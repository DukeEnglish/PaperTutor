Does Data Contamination Detection Work (Well) for LLMs?
A Survey and Evaluation on Detection Assumptions
YujuanVelvinFu1,ÖzlemUzuner2,MelihaYetis¸gen1,FeiXia1
1UniversityofWashington,2GeorgeMasonUniversity
{velvinfu, melihay, fxia}@uw.edu
ouzuner@gmu.edu
Abstract inflatedperformanceassessments(Balloccuetal.,
2024;Sainzetal.,2023a;LiandFlanigan,2024),
Largelanguagemodels(LLMs)havedemon-
andcreatesamisleadingperceptionoftheircapa-
stratedgreatperformanceacrossvariousbench-
bilities. Therefore,multipledetectionapproaches
marks, showing potential as general-purpose
have been developed to identify data contamina-
tasksolvers. However,asLLMsaretypically
tion in LLMs, and these approaches can also be
trainedonvastamountsofdata, asignificant
concernintheirevaluationisdatacontamina- deployedtoidentifytheuseofcopyrightedorsen-
tion,whereoverlapbetweentrainingdataand sitivecontentinLLMtraining(Xuetal.,2024).
evaluationdatasetsinflatesperformanceassess- Allexistingapproachesfordetectingdatacon-
ments. Whilemultipleapproacheshavebeen taminationinlanguagemodels(LMs)relyonspe-
developedtoidentifydatacontamination,these
cificassumptionsregardingtheLMsanddatasets,
approaches rely on specific assumptions that
which may not be universally applicable across
maynotholduniversallyacrossdifferentset-
different settings2. While previous surveys have
tings. Tobridgethisgap,wesystematicallyre-
view47papersondatacontaminationdetection, focused on detection and mitigation techniques,
categorizetheunderlyingassumptions,andas- toourbestknowledge,thereiscurrentlynocom-
sess whether they have been rigorously vali- prehensiveanalysisthatsurveysandvalidatesthe
dated. Weidentifyandanalyzeeightcategories assumptionsunderlyingtheseapproaches(Xuetal.,
ofassumptionsandtestthreeofthemascase
2024;Ishihara,2023;Huetal.,2022).
studies. Our analysis reveals that when clas-
Tobridgethisgap,we(1)systematicallyreview
sifying instances used for pretraining LLMs,
47papersondatacontaminationdetectionforLMs,
detectionapproachesbasedonthesethreeas-
(2)presenttheformal,mathematicaldefinitionsfor
sumptionsperformclosetorandomguessing,
suggesting that current LLMs learn data dis- differentlevelsofdatacontamination,(3)catego-
tributions rather than memorizing individual rizetheunderlyingrequirementsandassumptions
instances. Overall,thisworkunderscoresthe associatedwitheachapproachandcriticallyassess
importanceofapproachesclearlystatingtheir
whether these assumptions have been rigorously
underlyingassumptionsandtestingtheirvalid-
validated,and(4)demonstratethatsomeunverified
ityacrossvariousscenarios1.
assumptions can be wrong in multiple scenarios
1 Introduction throughthreecasestudies.
Largelanguagemodels(LLMs)haveachievedre- 2 LiteratureEvaluation
markableperformanceacrossvariousbenchmarks,
Tosystematicallyinvestigateapproachesfordata
signaling their potential to revolutionize numer-
contamination detection, we implement a three-
oustechnicaldomainsasgeneral-purposeproblem
step literature review process with 3 sources of
solvers(Achiametal.,2023;MetaAI,2024). How-
papers: (1)threekeysurveypapers(Xuetal.,2024;
ever,akeyconcerninaccuratelyevaluatingthose
Ishihara,2023;Huetal.,2022)andpapersaccepted
LLMs is the possibility of data contamination,
inthe1stWorkshoponDataContaminationatACL
where the LLM’s training data overlaps with the
20243;(2)relevantpaperscitedbythepapersfrom
evaluation dataset (Balloccu et al., 2024). Evalu-
atingLLMsoncontaminatedbenchmarksleadsto 2This work surveys data contamination detection ap-
proachesforLMsofallsizes,notlimitedtoLLMs.
1Wewillreleasethedataandcodeassociatedwiththis 3https://conda-workshop.GitHub.io/. Threepapers
workuponacceptanceofthiswork. fromthisworkshopwereexcludedfromtheanalysisasthey
1
4202
tcO
42
]LC.sc[
1v66981.0142:viXraSource (1); and (3) relevant papers cited by the candeterminewhatinstancesareconsideredtobe
papersfromSource(2). thesamebydefiningb(x,x′)accordingly.
Fromtheabovethreesources,wecollect78rel-
Definition D1. Instance-Level contamination:
evantpapersondatacontamination. Weaddition-
Let D be the training data of an LM M. The
M
ally filter these papers according to the inclusion
binaryfunctionf(M,x)isdefinedasfollows:
criterion: thepapermustproposeand/orevaluate
detectionapproachesfordatacontaminationintext ß 1 if∃x′ ∈ D ,b(x,x′) = 1
datasetsandLMs. 8studiessolelydiscussingrisks f(M,x) = M
0 if∀x′ ∈ D ,b(x,x′) = 0
andmitigationstrategiesfordatacontaminationdo M
(1)
notmeettheinclusioncriteriaandareexcluded.
We define an instance x to be seen by M, or
Consequently,ourreviewincludesatotalof47
M is contaminated by x, ifff(M,x) = 1. Con-
papers. Among them, we systematically summa-
versely,wedefineaninstancexascleanorunseen
rizethosedetectionapproachesandpresentformal
byM,ifff(M,x) = 0.
mathematicalrepresentationsfortheirunderlying
Thedetectionofinstance-levelcontaminationis
requirementsandassumptions. Wethenevaluate
commonlyreferredtoasmembershipinference
whethertheseunderlyingassumptionsaretrueun-
attack (MIA). The goal of MIA is to determine
derdifferentscenarios,asdescribedbelow.
theprobabilityofaninstancebeingusedtotrainan
3 LevelsofDataContamination
LM,namely,f(cid:98)(M,x)(Huetal.,2022).
Datacontaminationcanoccuratinstanceordataset 3.2 Dataset-LevelContamination
levels and the detection approaches for them can
Priorresearchimplicitlyreferstodataset-levelcon-
bedifferent. Tofacilitatethediscussion,wewould
taminationattwodegrees: partialdatasetcontami-
liketofirstprovideformalmathematicaldefinitions
nationandfulldatasetcontamination.
ofdatacontaminationattheselevels,basedonthe
descriptivedefinitionsfrompreviousresearch(Xu DefinitionD2. FullDatasetContamination: A
etal.,2024;Balloccuetal.,2024;Ishihara,2023). datasetD isfullycontaminated(fullyseen)byan
LM,ifeveryinstancewithinthisdatasetiscontam-
3.1 Instance-LevelContamination inated. Namely,∀x ∈ D,f(M,x) = 1.
In this study, we focus on text datasets and de- When creating benchmarks for detecting data
finealanguageinstancexasasequenceofword contamination,previousworktypicallygenerates
tokens. Originally,directinstance-levelcontami- the fully contaminated split. For example, Maini
nationisdefinedasthepresenceofaninstancex et al. (2024b) created contaminated and clean
within an LM M’s train set, D M, i.e. x ∈ D M datasets,respectivelyfromthetrainandvalidation
(Xuetal.,2024). However,LMsoftendonotpub- splits of the LM’s pretraining corpus. Shi et al.
lish their exact train corpus, but instead refer to (2023)focusedonLMswhichdisclosedthatthey
multiple datasets, as subsets of D M (Zhao et al., usedWikipediaeventdatafortraining,andcreated
2023). These datasets typically undergo various thecontaminateddatasetfromtheWikipediaevent
pre-processingsteps,suchasde-duplication,filter- datawhichwerepublishedbeforetheLMs’release.
ing,masking,andremovingnoise. Consequently,
DefinitionD3. PartialDatasetContamination:
LMsaretrainedonslightlydifferentversionsofthe
A dataset D is partially contaminated (paritially
same dataset (Palavalli et al., 2024). Meanwhile,
seen)byanLMM,ifatleastoneinstancewithin
thereisalsoindirectinstance-levelcontamination
D isseen. Namely,∃x ∈ D,f(M,x) = 1.
fromgreatervariationsofthedataset,suchasma-
chineparaphrasing(Yangetal.,2023). Inpractice,especiallywhenreportingcontami-
To account for such minor differences and in- nationfrombenchmarkdatasets(Dongetal.,2024)
directcontamination,inourDefinitionD1below, or detecting copyrighted content (Karamolegkou
we introduce a Binary Indicator Function for etal.,2023;Changetal.,2023),peoplefocusmore
Instance-Level Contamination, b(x,x′), which onevaluatingpartialdatasetcontamination.
outputsTrue(1)iftwoinstancesareconsideredto
DefinitionD4. Unseen/CleanDataset: Adataset
bethesameandFalse(0)otherwise. Researchers
isclean(unseen)byanLM,ifnoneofitsinstances
werenotpubliclyavailableduringourresearch. iscontaminated. Namely,∀x ∈ D,f(M,x) = 0.
2Detection Require- Assump- Critiqueson
DetectionRearch
Approach ments(ID) tions(ID) ThoseApproaches
Dodgeetal.(2021),Elangovanetal.(2021),
Disclose
Instance Li(2023b),Riddelletal.(2024),
(R1)
Similarity None Dengetal.(2024),Yangetal.(2023),
&Release
(9papers) Piktusetal.(2023),Leeetal.(2022),
(R2)
MaroneandVanDurme(2023)
SongandShmatikov(2019), Dekonincketal.(2024),
Absolute Shietal.(2023),Li(2023a), Duanetal.(2024),
None
Prob.(A1) Mainietal.(2024b),Weietal.(2024), Mainietal.(2024b),
Srivastavaetal.(2023) Caoetal.(2024)
Prob. Perturbed Ref. Matternetal.(2023),Mainietal.(2024b) Duanetal.(2024),
Analysis Instance(R3) Prob.(A2) Orenetal.(2024) Mainietal.(2024b)
(16papers) Dekonincketal.(2024),
Carlinietal.(2021),Mainietal.(2024b),
Ref. Ref. Duanetal.(2024),
Mireshghallahetal.(2022),
LM(R4) Prob.(A3) Caoetal.(2024),
Zanella-Béguelinetal.(2020)
Mainietal.(2024b)
Other Other Jagannathaetal.(2021),Zhangetal.(2023)
Carlinietal.(2022),Kandpaletal.(2022),
Verbatim MagarandSchwartz(2022),Duarteetal.(2024)*,
None
Mem.(A4) Tirumalaetal.(2022),Schwarzschildetal.(2024),
GolchinandSurdeanu(2023a)*
Instance
Dengetal.(2024),Ranaldietal.(2024),
Gen.
Key Key Changetal.(2023),Panetal.(2020),
&
Info. Info. Carlinietal.(2021),Carlinietal.(2019),
Instance
(R5) Gen.(A5) Liuetal.(2024),GolchinandSurdeanu(2023b),
Select.
GolchinandSurdeanu(2023a)
(20papers)
Gen.
None Dongetal.(2024)
Variation(A6)
Metadata Metadata Sainzetal.(2023b)*
Dekonincketal.(2024)
(R6) Mem.(A7) Karamolegkouetal.(2023)*
Answer Liuetal.(2024)*,Mehrbakhshetal.(2024)*,
Instance Answer
Mem. Yimetal.(2024)*,Zongetal.(2023)*,
Perturb.(R7) Change(A8)
(5papers) Razeghietal.(2022)*
Table1: Existingdetectionapproachesfordirectdatacontamination, theirrequirementsandassumptions, and
Critiques they received. Some papers cover multiple detection approaches with different assumptions. Most
detectionmethodsapplytobothinstance-anddataset-levelcontamination,while*denotesthoselimitedtodataset-
levelcontamination. Ourcasestudiesshowthattheunderlinedrequirements/assumptionsarenotalwayssatisfied.
4 DetectionofDirectDataContamination Theperformanceofadetectionmethoddepends
onhowwellitsrequirementsaremetandtherelia-
Directdatacontaminationisthemostcommonand bilityofitsassumptions. Therefore,wegroupthe
well-researchedtypeofdatacontamination. Inthis detection approaches based on their assumptions
section, we categorize the existing detection ap- andrequirements.
proaches,theirrequirements,assumptions,andthe
critiquestheyreceived(seeTable1). Therequire- 4.1 InstanceSimilarity
mentsaredefinedasthepreliminaryconditionsnec- WhenD isknown,detectionapproachesbased
M
essaryforconductingcertaindetectionapproaches. oninstancesimilaritydirectlydeployEquation1,
Theassumptionsarewhattheauthorsofdetection byproposingasimilarityfunctiontomeasureb(·,·)
approachesassumetobetrue;theassumptionsei- andcomparinganewinstancewitheveryx ∈ D .
M
therareexplicitlystatedbytheauthorsorcanbe Previousresearchfocusesondevelopingabetter
inferredfromthedetectionapproaches. ormoreefficientsimilarityfunction. Examplesof
Mostdetectionmethodologiesfordirectcontam- similaritycalculationcanbeconductedthroughex-
inationareprimarilydevelopedtoaddressinstance- actmatch(Dodgeetal.,2021),fuzzymatch(Piktus
levelcontamination;however,theycanbeadapted etal.,2023;Leeetal.),automaticNLGevaluation
to account for dataset-level contamination. Con- metrics(Elangovanetal.,2021;Dengetal.,2024),
sequently,unlessspecifiedotherwise,thissection and another LM (Yang et al., 2023). Tools have
willconcentrateoninstance-levelcontamination. alsobeendevelopedtoallowefficientcomparison
3toalargeD ,suchasDataPortraits(Maroneand However, Maini et al. (2024b) and Duan et al.
M
VanDurme,2023)andROOTSSearchTool(Piktus (2024)havedemonstratedthattheperplexityand
etal.,2023). Min TOP p probabilities are close to random in
Althoughthisapproachdoesnotrelyonunder- detectingdirectinstance-leveldatacontamination
lyingassumptions,ithastworequirements: across different splits of the Pile dataset. Maini
etal.(2024b)suggeststhatshiftsinperplexityand
RequirementR1. D needstobedisclosed.
M
infrequentwordprobabilitiesmaybeattributedto
RequirementR2. D mustbeaccessible,which
M temporaleventsonplatformslikeWikipedia,rather
isoftenhinderedbylegal,privacyconstraints,and
than contamination. Similarly, Cao et al. (2024)
expiredwebsitelinks.
highlightedthatperplexityandtokenprobabilityap-
CaseStudy: Toexaminehowoftenthesetwore- proachesareineffectiveforcodegenerationtasks.
quirementsaremet,weanalyzedthetop10LMson
4.2.2 ReferenceProbabilitybyAnInstance
theVellumLLMleaderboard4. Wefoundthatnone
Insteadofassumingtheprobabilitiesofalltheseen
of the LMs fulfilled the most basic, requirement
instances are higher than the probabilities of all
R1,letaloneR2(seeAppendixA.3fordetails).
theunseeninstances,thisapproachcomparesthe
4.2 ProbabilityAnalysis probabilitiesofsimilarinstances.
WhenthetrainingdatasetD isunavailable,but RequirementR3. Thereexistsanalgorithmwhich,
M
theLMM’soutputtokenprobabilitiesareknown, givenaninstancexandanLMM, canautomati-
probabilityanalysishasbeenusedtodetectpoten- callygenerateasimilarinstance,unseenx′.
tialinstance-levelcontamination. Wegroupthose AssumptionA2. Ifxandx′aresimilarandM has
detectionapproachesbytheirassumptions,andun- seen x but not x′ , the probability of x should be
lessotherwisespecified,theyhavenorequirements. muchhigherthanthatofx′ basedonM:
4.2.1 AbsoluteProbability ß ≫ P (x′) iff(M,x) = 1
P (x) M (3)
Givenaninstancex,probabilityanalysismeasures M ̸≫ P (x′) iff(M,x) = 0
M
instance-levelcontaminationthroughP (x),the
M
probabilityoftheinstancexbasedonanLMM. Utilizingthisassumption,Matternetal.(2023)
construct the similar, reference instance x′ by
AssumptionA1. Seeninstanceswillhaveahigher
replacing individual words in x with their syn-
probability than unseen ones, and there exists a
onyms. However,inpractice,theobservationthat
threshold, ξ , that separates seen instances from
p
P (x) ≥ P (x′)mightresultfromreplacement
unseenones: M M
ß withrarewords. Thisassumptionhasbeenproven
≥ ξ iff(M,x) = 1
P M(x)
<
ξp
iff(M,x) = 0
(2) false by Maini et al. (2024b) on different splits
p
of the Pile dataset (Gao et al., 2020). In addition
PreviousresearchmeasuresP (x)throughper- to this synonym-based perturbation, Maini et al.
M
plexity(Carlinietal.,2021;Li,2023a)orapproxi- (2024b) demonstrate the ineffectiveness of other
matesitthroughLMloss(Weietal.,2024),which perturbation approaches, including white space,
canbeimpactedbytheinstancedomainandsim- characters,randomdeletion,andcasechanges.
plicity. Toimproveuponthisassumption,Shietal. Anotherstudy,Orenetal.(2024),constructsthe
(2023)onlyevaluatestheaverageprobabilitiesof referenceinstancebyrandomlyshuffling(exchang-
topp%leastlikelytokensinaninstance(MinTOP ing) the order of sentences. They make another
p Prob), assuming that unseen instances contain assumptionoftheexchangeability,positingthatall
morelow-probabilityoutliersinWikipediaevents orderingsofanexchangeablebenchmarkshouldbe
andbooks. Similarly,SongandShmatikov(2019) equallylikelyifuncontaminated. Thisassumption
assessesprobabilitiesofthekmostfrequenttokens. mightnotbevalidforcodingandreasoningtasks.
Likewise,Srivastavaetal.(2023)andWeietal.
4.2.3 ReferenceProbabilitybyAnLM
(2024)proposedinsertingspecialstringsaswater-
Thistypeofapproachcomparestheprobabilityof
marksintothetrainingdata,usingtheprobability
aninstancebasedontwoLMs.
ofthesewatermarkstodetectdatacontamination.
Requirement R4. Given an instance x, we can
4https://www.vellum.ai/llm-leaderboard#model
-comparison.AccessedonOct6,2024. findanotherLMM′,suchthatxisunseenbyM′.
4Assumption A3. If x is seen by M but not M′, begenerated(memorized)byM,withtheinputx ,
p
thenP (x)shouldbemuchhigherthanP (x): throughgreedydecoding.
M M′
ß
= x iff(M,x) = 1
P
M(x)ß ̸≫≫ PP M′( (x x)
)
i if fff( (M M, ,x x)
)
=
=
1
0
(4)
M g(x p)
̸=
xs
s iff(M,x) = 0
(5)
M′
Instance-level contamination does not always
Previous research has utilized the zlib entropy leadtoverbatimmemorization. Utilizinginstance
(Carlinietal.,2021)andanotherLM(Carlinietal., generation, Kandpal et al. (2022), Carlini et al.
2021;Mireshghallahetal.,2022)asthereference (2019), Carlini et al. (2022), and Tirumala et al.
model. However, Maini et al. (2024b) and Duan (2022)demonstratethatverbatimmemorizationre-
etal.(2024)havedemonstratedthatthosereference quiresrepeatedexposurestothisinstancexduring
models perform closeto random guessing across training,andalargerLMandlongerinputlengthx
p
variousdomains. Caoetal.(2024)alsoshowthe canresultinbettermemorization. Schwarzschild
zlib entropy does not work for code generation etal.(2024)usedtheminimumlengthofx needed
p
tasks. togeneratethedesiredoutputx ,todefinethede-
s
Similarlytothisassumptionatthesentencelevel, greeofmemorization.
Zanella-Béguelinetal.(2020)deployareference Similarly,Kandpaletal.(2022)andCarlinietal.
modelforbothindividualtokenprobabilityandits (2021)studyarelaxedversionofthisassumption,
probabilityrankwithinthevocabulary. where the LM can generate x through different
s
sampling strategies in decoding, such as top-k or
4.3 InstanceGenerationandInstance
top-p(Nucleus)sampling(Holtzmanetal.,2020).
Selection
Theyreachasimilarconclusionthatdatacontami-
Inthissection,weinvestigateunderlyingrequire-
nationdoesnotnecessarilyleadtomemorization.
ments and assumptions for detection approaches
Forinstanceselectionapproaches,Duarteetal.
based on instance generation and instance selec-
(2024)andGolchinandSurdeanu(2023a)define
tion.
x assentencesorpassages,andcreatesimilarin-
s
Instance generation detects contamination by
stancestox byparaphrasingx usinganotherLM.
s s
treating x as a prefix-suffix pair, x = (x ,x ).
p s They assume that the contaminated LM will be
TheseapproachesevaluatetheLMM’sgenerated
morelikelytoselecttheverbatimoption.
output, M(x ), conditioned on x . If M(x ) is
p p p
similar or identical to x , x will be predicted as 4.3.2 KeyInformationGeneration
s
seen. Based on this core intuition, instance gen- Thistypeofapproachassumesthat,ifanLMhas
eration approaches vary in their assumptions re- seenaninstance,itcangeneratex’skeyinforma-
gardinginput-outputpairsandlanguagegeneration tionbasedonitscontext.
approaches. Unless otherwise mentioned, those
Requirement R5. An instance x can be para-
approachesbelowfocusoninstancegeneration.
phrased into a slot-filling, context-key pair, x =
For instance selection, instead of directly gen-
(x ,x ). The key x is usually a representative
c k k
erating answers, the LM is tasked with selecting
sub-spanofx,suchasdatesandnames. Therest
themostlikelyx fromasetofcandidateoptions
s tokensinxcomposethecontext,x .
c
in a multi-choice format. However, detection ap-
Assumption A5. If x is seen, M will be able to
proachesrelyingoninstanceselectionfaceafun-
producesimilaroutputtox whengivenx .
damental limitation: even if x is unseen, the LM k c
mightstillchoosethecorrectx s byaccident. Con- S(M(x ),x )ß ≥ τ s iff(M,x) = 1 (6)
sequently,theseapproachesaregenerallynotem- c k < τ iff(M,x) = 0
s
ployedtodetectinstance-levelcontaminationbut
rathertoassesstheprobabilityoffulldatasetcon- Here, M(x c) denotes the output of the LM M
tamination. through a certain decoding method. S(·,·) is a
text similarity function, and τ is the contamina-
s
4.3.1 VerbatimMemorization
tionthreshold. Onecanusethesimilarityfunctions
ThistypeofapproachassumesLMscanmemorize
describedinSection4.1.
theirtrainingdata,tocertainextent.
Leveragingthisassumption,priorstudieshave
AssumptionA4. Givenanprefix-suffixpairx = maskedkeyinformationwithinspecificdatasets,in-
(x ,x ), if x has been seen by an LM M, x can cludinginputquestionsinNLPbenchmarks(Deng
p s s
5et al., 2024; Golchin and Surdeanu, 2023b; Liu phasedoesnotpreservethelinkagebetweenD’s
et al., 2024), column names in SQL code gener- metadataandinstances(Dekonincketal.,2024).
ation questions (Ranaldi et al., 2024), character
4.4 AnswerMemorization
namesinbooks(Changetal.,2023),andlabelsin
NLIandSSTtasks(MagarandSchwartz,2022). Answermemorizationisusuallyconductedatthe
dataset level. It introduces perturbations to the
4.3.3 GenerationVariation
originaldataset,measurestheLM’sperformance
This type of approach investigates how an LM’s
change,andaimstodetectiftheLM’shighperfor-
outputsvaryifithasseenaninstance.
manceisduetomemorizingitsanswer.
Assumption A6. Suppose an instance x can be
RequirementR7. GivenanLMM andaevalua-
represented as a prefix-suffix pair, x = (x ,x ).
p s tiondatasetD,onecangenerateasimilardataset
If an LM M has seen x, then given x p, M will forthesametask,D′,bymodifyingeveryx ∈ D.
generatesomethingidenticalorsimilartox under
s AssumptionA8. SupposedatasetsD andD′ are
differentsamplingstrategies:
similarandanLMM hasseenD butnotD′,M’s
ß
≥ ξ iff(M,x ) = 1 performance on D (Eval(M,D)) will be much
Var({M(x )}) v p (7)
· p < ξ iff(M,x ) = 0 higherthanitsperformanceonD′ (Eval(M,D′)).
v p
whereVar({M ·(x p)})measuresthevariationsof ß ̸≫ Eval(M,D′) ifD isunseen
Eval(M,D)
outputs for M under diverse, different sampling ≫ Eval(M,D′) Otherwise
strategieswhengivenx ;ξ isathreshold,based (9)
p v
ontheVar(·)andthetypeofinputx . Previous research evaluates answer memoriza-
p
Dong et al. (2024) defines the metric Var(·) tioninmultiple-choice(MC)tasksbyintroducing
as ‘Contamination Detection via output Distribu- variationssuchasalteringnumbersinmathematical
tion’(CDD),andutilizesthisassumptiontodetect tasks(Mehrbakhshetal.,2024),changingtheorder
memorizationincodingandreasoningbenchmarks. ofMCoptions,etc. (Yimetal.,2024;Zongetal.,
However,thisassumptioncanleadtofalsepositives 2023). Razeghi et al. (2022) show that multiple
forothertasks,suchasmultiplechoices,wherethe LMsperformbetteronnumericalreasoningtasks
outputismoreconstrainedandhaslessvariation. involvingfrequentlyoccurringnumericalvariables
intheirpretrainingdata. Similartothisassumption,
4.3.4 Metadata-basedMemorization
Liuetal.(2024)detectsiftheLMcanstillpredict
ThistypeofapproachdetermineswhetheranLM
thecorrectanswer,afterremovingallMCoptions.
hasseenadatasetD byusingD’smetadata.
RequirementR6. GivenadatasetD,wecancon- 5 OtherTypesofContamination
structaninputpromptx includingD’smetadata
m Besides direct data contamination, previous re-
m,suchasdatasetname,split,andformat.
searchalsoinvestigatesindirectdatacontamination
AssumptionA7. IfanLMM hasseenadatasetD, (6papers),andtaskcontamination(5papers).
when given D’s metadata, M is able to generate
instancesthatareverysimilartosomex ∈ D. 5.1 IndirectDataContamination
Indirect data contamination occurs when an in-
ß
∀x ∈ D,S(M(x ),x) < τ ifD isunseen
m m stancexissimilartoaninstancex′ ∈ D ,butwith
∃x ∈ D,S(M(x ),x) ≥ τ Otherwise M
m m arelativelylowerdegreeofsimilaritycomparedto
(8)
direct contamination. For instance, x might be a
Here,M(x )mightgeneratemultipleinstances
m paraphraseorsummaryofx′ (Yangetal.,2023).
when given D’s metadata m; S(M(x ),x) rep-
m
In practice, indirect data contamination often
resents the highest similarity between x and an
originatesfromsourcesthatarenotexplicitlyrec-
instancex′ asasubsequenceofM(x );τ isthe
m m
ognizedaspartoftheoriginaldatasetcomplicating
contaminationthresholdforS(·,·).
the tracking and tracing process (Balloccu et al.,
Sainzetal.(2023b)andGolchinandSurdeanu
2024). Forexample,OpenAIusesonlineusercon-
(2023b) utilized this assumption to demonstrate
versations for training, which could include vari-
that OpenAI systems memorized many instances
ationsofbenchmarkdatasets5. Anotherexample
fromwidelyusedbenchmarks. However,thisap-
proachcanhavefalsenegativesiftheLM’straining 5https://help.openai.com/en/articles/572248
6involvesknowledgedistillation,whereanLMuti- Asavariationofthisassumption,Aiyappaetal.
lizes instances x generated by another LM M′ (2023) find improved performance on the same
k
during training, and these instances x may re- benchmarkwithmodelupdates.
k
sembleinstancesfromthetrainingsetD ofM′
M′
(Veselovskyetal.,2023). 6 CaseStudy
Besides the case study in Section 4.1, we aim to
5.1.1 DetectionApproachesforIndirectData
evaluateiftheassumptionsoutlinedinTable1are
Contamination
universallyapplicableacrossdifferentdomains,for
Compared to direct contamination, indirect data
directandinstance-leveldatacontamination.
contaminationismuchmorechallengingtodetect.
Dekonincketal.(2024)andCaoetal.(2024)show 6.1 AssumptionstoEvaluate
thatmanyprobability-baseddetectionapproaches
Becausesomeassumptionshavespecificrequire-
areineffectiveforindirectdatacontamination.
mentsandtheirapplicabilitydependsonhowwell
However,threeapproachesmaystillbeapplica-
theserequirementsaremet,weselectthethreeas-
ble: (1)theinstancesimilaritymeasuredbyanother
sumptions without any requirements to evaluate.
LM (Yang et al., 2023), (2) the CDD metric pro-
Thus,werestricttheconfoundingfactorsandleave
posedbyDongetal.(2024),whichleveragesAs-
thetestingofotherassumptionsforfutureresearch.
sumptionA6bymeasuringoutputvariationsrather
AssumptionA1: AbsoluteProbability. Inthe
than directly comparing with original instances,
assumption that seen instances will have a lower
and (3) directly tracking the disclosed usage of
perplexity and higher Min Top p Probability, we
datasets. For example, Balloccu et al. (2024) re-
measureperplexitybyaninstance’sfirstk tokens
viewedthedatasetsevaluatedusingOpenAIAPIs.
(PPL_k) (Carlini et al., 2021); Min Top p Prob
by the average of the least likely tokens, whose
5.2 TaskContamination
cumulativeprobabilitiesarep%(Shietal.,2023).
Task contamination occurs when any instance of AssumptionA4: VerbatimMemorization. We
thesametaskisseenbyanLM(LiandFlanigan, expandthisassumptionfromtheinstancelevelto
2024). Detectingtaskcontaminationiscrucialfor thetokenlevel,assumingseeninstanceswillmem-
assessinganLM’sgeneralizabilitytounseentasks orizemoretokens. Wemeasurethepercentageof
(Chung et al., 2024). Tasks can include applica- tokensinaninstancerankedasthek mostlikelyin
tionssuchasmachinetranslation,summarization, casuallanguagemodeling(Memk). Thek value
and mathematical calculation. Task contamina- of1representsgreedydecoding,andlargerthan1
tionisabroaderconceptthandatacontamination: simulatesthedecodingwithtopk tokensampling.
if a dataset is partially contaminated, the associ-
Assumption A6: Generation Variation. We
atedtaskiscontaminated,buttaskcontamination
evaluatetherelatedassumptiontogenerationvari-
doesn’talwaysimplythedatasethasbeenseen.
ationthatseeninstanceswillhavebettercertainty
TaskcontaminationgenerallyevaluatesanLM’s and lower entropy. We measure entropy over the
performanceonaparticulartaskatthedatasetlevel. top k most likely tokens (Entropy k) (see Ap-
TheideaisthatifanLMhaspreviouslyseenthe pendixA.4fordetails).
task, its performance will be much higher com-
paredtounseentasksofsimilardifficulty. 6.2 ExperimentDesign
For example, Ranaldi et al. (2024) and Li and
WeutilizethewidelyusedPythiaLMsofdifferent
Flanigan(2024)findthatOpenAImodelsperform sizes6(Bidermanetal.,2023),whicharepretrained
significantlybetteronbenchmarksreleasedbefore onthe825GiBPiledataset7 (Gaoetal.,2020).
themodel’sreleasethanonthosereleasedlater. To
BecausethePileiscomposedof22smallersub-
ensurefaircomparisonsacrosstasks,LiandFlani-
setsfromdifferentdomains,werandomlysample
gan(2024)controltaskdifficultyusingabaseline
11domainsforourexperiment.Withineachdomain,
system. However,Caoetal.(2024)alsonotethat
werandomlyselect1kinstancesfromthetrainsplit
LMs do not necessarily perform worse on more
(seen),and1kinstancesfromthetestsplit(unseen).
recentcodegenerationbenchmarks.
6https://huggingface.co/EleutherAI/pythia-70m
6-how-your-data-is-used-to-improve-model-perfo 7https://Pile.eleuther.ai/,https://huggingfac
rmance e.co/datasets/ArmelR/the-Pile-splitted
7Free- PubMed Enron PubMed Open- OpenWeb- Youtube- Hacker-
Github ArXiv Pile-CC
Assumptions&Metric Law Central Emails Abstracts Subtitles Text2 Subtitles News
(GH) (AX) (PC)
(FL) (PC) (EE) (PA) (OS) (OWT) (YS) (HN)
PPL_50 49.7 48.9 50.7 49.4 49.6 49.7 47.9 50.3 50.8 48.4 49.8
PPL_100 50.8 49.4 51.1 50.1 50.5 48.8 48.1 48.9 50.3 47.3 50.4
PPL_200 51.1 49.4 50.9 50.2 51.4 49.9 47.8 48.5 49.5 50.7 52.1
A1
Min5%token 51.5 49.8 51.3 48.5 50.0 48.5 49.8 49.7 49.0 47.8 50.9
Min15%token 51.2 49.8 51.3 49.2 50.1 49.4 49.9 49.2 49.2 49.6 51.1
Min25%token 51.0 49.4 51.4 49.5 50.0 49.6 50.2 49.0 49.2 49.8 51.1
Mem1 49.2 48.9 50.5 47.1 49.8 50.7 49.6 50.6 49.4 48.3 49.7
A4 Mem3 49.7 50.0 51.0 46.5 50.1 49.8 51.5 50.5 48.2 49.3 48.7
Mem5 49.7 48.0 50.8 48.2 49.7 52.7 50.8 50.2 49.1 50.0 49.4
Entropy5 51.9 49.2 52.4 48.9 50.3 48.0 49.4 51.9 48.6 49.0 50.4
A6 Entropy10 52.2 49.4 52.3 49.3 50.4 48.2 48.7 52.1 48.6 49.0 50.6
Entropy25 52.1 49.5 50.9 49.5 50.7 48.5 48.1 51.0 48.1 49.6 50.9
Seen 3.0±5.6 3.9±1.4 5.6±2.4 5.6±2.1 7.8±10.6 8.7±4.4 9.5±3.4 10.8±9.2 11.0±13.2 12.1±5.9 13.9±11.8
PPL
Unseen 3.0±6.2 4.0±3.0 5.7±2.1 5.6±2.2 8.0±10.4 8.6±4.3 9.6±3.6 10.6±8.2 10.8±13.2 11.8±5.5 13.9±10.5
Table2: AveragecontaminationdetectionAUCforthePythia-6.9bmodel,underdifferentPILEdomains. ‘PPL’
representstheaverageperplexity±STD,whereinstancesaretruncatedbythemodelmaxlengthof2048tokens.
Thecolumns(domains)aresortedbytheaveragePPLamongtheseeninstances.
Weevaluatethedetectionperformancethroughthe
areaunderthecurve(AUC),attheinstancelevel.
6.3 Results
6.3.1 Within-DomainDetection
Table 2 shows the AUC for each contamination
detectionmethodonPythia-6.9b. Withinthesame
domain,thesimilaraveragePPLbetweenseenand
unseen instances indicates that they have similar
underlyingdistributions,butalsoahighvariation
(STD).However,PPLdiffersalotacrossdomains.
We also observed all metrics perform close to
random guessing, with AUC close to 50. Our re-
sults for Assumption A1 are consistent with cri-
tiques they received (see Section 4.2.1). We con-
clude that the LM learns underlying data distri- Figure1: AveragecontaminationdetectionAUCforthe
butionsratherthanmemorizingspecificinstances. Pythia-6.9bmodelwithPPL_200,whentheseenand
unseeninstancesarefromdifferentdomains.
Among different sizes of Pythia models, we find
that larger LMs have a lower average PPL in the 7 Conclusion
same domain. However, all metrics have similar,
near-randomperformance(seeAppendixA.5.1). Inthisstudy,wepresentacomprehensivesurveyof
47studiesfocusedondatacontaminationdetection
6.3.2 Cross-DomainsDetection and their underlying assumptions. Our analysis
AsTable2suggestsdomaindifferencegreatlyim- revealsthattheseassumptionsmaynotapplycon-
pactsPPL,wefurtherevaluatethescenariowhere sistentlyacrossdifferentcontexts. Ourcasestudies
seen and unseen instances are from different do- confirmedthat3outofthe8assumptionsarenot
mains,withthesameinstancesinSection6.2. universallyapplicableacrossalltrainingdomains.
We present the AUC with PPL_200 in Figure We conclude that many assumptions measure
1. The AUC is high in the top-right corner when anLM’sgoodnessoffit,whichisnotnecessarily
seeninstanceshavehighaveragePPLandunseen the result of instance memorization due to data
instances have low average PPL. Conversely, the contamination. Therefore, detecting direct data
bottom-leftcorner haslowAUCs. This indicates contaminationremainschallenging.
thatthePPL_200detectsdomainshifts,insteadof Moregenerally,thisstudyhighlightstheimpor-
datacontamination. Weadditionallyvisualizethe tanceofresearchersclearlystatingandvalidating
PPL_200distributionwithinandacrossdomainsin theassumptionsbehindtheirproposedapproaches,
AppendixA.5.2. Asimilartrendisobservedwith becauseapproachesbuiltonshakyassumptionsare
othermetrics(seeAppendixA.5.2&A.5.3). unlikelytoperformwell.
88 Limitations Anthropic.2024b. Introducingclaude3.5sonnet. Ac-
cessedonOct6,2024.
Inthisstudy,wereviewed47papersondatacon-
taminationdetection,buttheremaybeadditional Anthropic.2024c. Introducingthenextgenerationof
relevant studies not captured by our collection claude. AccessedonOct6,2024.
methods. We primarily focus on data contamina-
SimoneBalloccu,PatríciaSchmidtová,MateuszLango,
tiondetectionapproachesforEnglishLMs. Other
andOndˇrejDušek.2024. Leak,cheat,repeat: Data
detection approaches for non-English LMs, data contaminationandevaluationmalpracticesinclosed-
modalities beyond text, and other machine learn- sourcellms. InProceedingsofthe18thConferenceof
theEuropeanChapteroftheAssociationforCompu-
ingtechniquesmayexistandcouldpotentiallybe
tationalLinguistics(Volume1: LongPapers),pages
transferabletoEnglishLMs.
67–93.
OurcasestudydeployedthePythiamodelfam-
ily, which is trained for no longer than 2 epochs StellaBiderman,HaileySchoelkopf,QuentinAnthony,
for each domain, and has therefore seen each in- Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-
hammadAflahKhan,ShivanshuPurohit,USVSNSai
stance for at most twice. In this case, although
Prashanth,EdwardRaff,etal.2023. Pythia: asuite
an instance is contaminated, it is difficult for the
foranalyzinglargelanguagemodelsacrosstraining
LMtomemorizetheinstance. Thoseassumptions andscaling. InProceedingsofthe40thInternational
mightperformverydifferentlyforotherLMsand ConferenceonMachineLearning,pages2397–2430.
otherdatasets.
Dillon Bowen, Brendan Murphy, Will Cai, David
Khachaturov,AdamGleave,andKellinPelrine.2024.
9 EthicalConsiderations
Scaling laws for data poisoning in llms. Preprint,
arXiv:2408.02946.
Inthiswork,weemployedthePythiamodelfamily
(Biderman et al., 2023) and their pretraining cor-
JialunCao,WuqiZhang,andShing-ChiCheung.2024.
pus, the 825 GiB Pile dataset (Gao et al., 2020). Concernedwithdatacontamination? assessingcoun-
Since the Pile includes content crawled from the termeasuresincodelanguagemodel. arXivpreprint
Internet,itmaycontainsensitiveand/oridentifiable arXiv:2403.16898.
information. Therefore,weonlydownloadedthe
NicholasCarlini,DaphneIppolito,MatthewJagielski,
necessary instances and published the numerical
KatherineLee,FlorianTramer,andChiyuanZhang.
resultsfromourexperiments. Uponacceptanceof 2022. Quantifyingmemorizationacrossneurallan-
our work, we will release our code for data sam- guagemodels. InTheEleventhInternationalConfer-
enceonLearningRepresentations.
pling and contamination detection approaches to
ensurereproducibility,butwewillnotpublishany
NicholasCarlini,ChangLiu,ÚlfarErlingsson,Jernej
actualdatainstancesorlanguagemodels. Wead-
Kos,andDawnSong.2019. Thesecretsharer: Eval-
vise the community to check data regulations be- uatingandtestingunintendedmemorizationinneu-
foredeployingthePythiamodelfamily(Biderman ralnetworks. In28thUSENIXSecuritySymposium
(USENIXSecurity19),pages267–284,SantaClara,
etal.,2023)andthePiledataset(Gaoetal.,2020)
CA.USENIXAssociation.
forotherpurposes.
Nicholas Carlini, Florian Tramer, Eric Wallace,
Matthew Jagielski, Ariel Herbert-Voss, Katherine
References
Lee,AdamRoberts,TomBrown,DawnSong,Ulfar
Erlingsson,etal.2021. Extractingtrainingdatafrom
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama
large language models. In 30th USENIX Security
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Symposium(USENIXSecurity21),pages2633–2650.
DiogoAlmeida,JankoAltenschmidt,SamAltman,
ShyamalAnadkat,etal.2023. Gpt-4technicalreport.
Kent Chang, Mackenzie Cramer, Sandeep Soni, and
arXivpreprintarXiv:2303.08774.
DavidBamman.2023. Speak,memory:Anarchaeol-
RachithAiyappa,JisunAn,HaewoonKwak,andYong- ogyofbooksknowntochatgpt/gpt-4. InProceedings
yeolAhn.2023. CanwetrusttheevaluationonChat- ofthe2023ConferenceonEmpiricalMethodsinNat-
GPT? InProceedingsofthe3rdWorkshoponTrust- uralLanguageProcessing,pages7312–7327.
worthy Natural Language Processing (TrustNLP
2023),pages47–54,Toronto,Canada.Association HyungWonChung,LeHou,ShayneLongpre,Barret
forComputationalLinguistics. Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi
Wang,MostafaDehghani,SiddharthaBrahma,etal.
Anthropic.2024a. Claude3haiku: ourfastestmodel 2024. Scalinginstruction-finetunedlanguagemodels.
yet. AccessedonOct6,2024. JournalofMachineLearningResearch,25(70):1–53.
9Jasper Dekoninck, Mark Niklas Müller, Maximilian HongshengHu,ZoranSalcic,LichaoSun,GillianDob-
Baader, Marc Fischer, and Martin Vechev. 2024. bie,PhilipSYu,andXuyunZhang.2022. Member-
Evadingdatacontaminationdetectionforlanguage shipinferenceattacksonmachinelearning: Asurvey.
modelsis(too)easy. ACMComputingSurveys(CSUR),54(11s):1–37.
ChunyuanDeng,YilunZhao,XiangruTang,MarkGer-
ShotaroIshihara.2023. Trainingdataextractionfrom
stein, and Arman Cohan. 2024. Investigating data
pre-trainedlanguagemodels: Asurvey. InProceed-
contaminationinmodernbenchmarksforlargelan-
ings of the 3rd Workshop on Trustworthy Natural
guagemodels. InProceedingsofthe2024Confer-
LanguageProcessing(TrustNLP2023),pages260–
enceoftheNorthAmericanChapteroftheAssoci-
275.
ation for Computational Linguistics: Human Lan-
guageTechnologies(Volume1: LongPapers),pages
AlonJacovi,AviCaciularu,OmerGoldman,andYoav
8698–8711.
Goldberg. 2023. Stop uploading test data in plain
text: Practicalstrategiesformitigatingdatacontami-
Jesse Dodge, Maarten Sap, Ana Marasovic´, William
nationbyevaluationbenchmarks. InProceedingsof
Agnew,GabrielIlharco,DirkGroeneveld,Margaret
the2023ConferenceonEmpiricalMethodsinNatu-
Mitchell, and Matt Gardner. 2021. Documenting
ralLanguageProcessing,pages5075–5084,Singa-
largewebtextcorpora: Acasestudyonthecolossal
pore.AssociationforComputationalLinguistics.
clean crawled corpus. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
AbhyudayJagannatha,BhanuPratapSinghRawat,and
guageProcessing,pages1286–1305.
HongYu.2021. Membershipinferenceattacksuscep-
Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, and tibilityofclinicallanguagemodels. arXivpreprint
GeLi.2024. Generalizationormemorization: Data arXiv:2104.08305.
contaminationandtrustworthyevaluationforlarge
languagemodels. arXivpreprintarXiv:2402.15938. Albert Q Jiang, Alexandre Sablayrolles, Antoine
Roux,ArthurMensch,BlancheSavary,ChrisBam-
MichaelDuan,AnshumanSuri,NiloofarMireshghallah, ford,DevendraSinghChaplot,DiegodelasCasas,
Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Emma Bou Hanna, Florian Bressand, et al. 2024.
Tsvetkov,YejinChoi,DavidEvans,andHannaneh Mixtralofexperts. arXivpreprintarXiv:2401.04088.
Hajishirzi.2024. Domembershipinferenceattacks
workonlargelanguagemodels? InConferenceon NikhilKandpal,EricWallace,andColinRaffel.2022.
LanguageModeling(COLM). Deduplicatingtrainingdata mitigatesprivacy risks
inlanguagemodels. InInternationalConferenceon
André Vicente Duarte, Xuandong Zhao, Arlindo L.
MachineLearning,pages10697–10707.PMLR.
Oliveira, and Lei Li. 2024. DE-COP: Detecting
copyrightedcontentinlanguagemodelstrainingdata.
Antonia Karamolegkou, Jiaang Li, Li Zhou, and An-
InForty-firstInternationalConferenceonMachine
dersSøgaard.2023. Copyrightviolationsandlarge
Learning.
languagemodels. InProceedingsofthe2023Con-
Aparna Elangovan, Jiayuan He, and Karin Verspoor. ferenceonEmpiricalMethodsinNaturalLanguage
2021. Memorization vs. generalization: Quantify- Processing,pages7403–7412,Singapore.Associa-
ingdataleakageinnlpperformanceevaluation. In tionforComputationalLinguistics.
Proceedingsofthe16thConferenceoftheEuropean
Chapter of the Association for Computational Lin- ArielLee, ColeHunter, andNatanielRuiz. Platypus:
guistics: MainVolume,pages1325–1335. Quick,cheap,andpowerfulrefinementofllms. In
NeurIPS2023WorkshoponInstructionTuningand
LeoGao,StellaBiderman,SidBlack,LaurenceGold- InstructionFollowing.
ing,TravisHoppe,CharlesFoster,JasonPhang,Ho-
raceHe, AnishThite, NoaNabeshima, etal.2020. Katherine Lee, Daphne Ippolito, Andrew Nystrom,
The pile: An 800gb dataset of diverse text for lan- ChiyuanZhang,DouglasEck,ChrisCallison-Burch,
guagemodeling. arXivpreprintarXiv:2101.00027. andNicholasCarlini.2022. Deduplicatingtraining
datamakeslanguagemodelsbetter. InProceedings
Shahriar Golchin and Mihai Surdeanu. 2023a. Data
of the 60th Annual Meeting of the Association for
contamination quiz: A tool to detect and estimate
ComputationalLinguistics(Volume1: LongPapers),
contamination in large language models. CoRR,
pages8424–8445.
abs/2311.06233.
Changmao Li and Jeffrey Flanigan. 2024. Task con-
Shahriar Golchin and Mihai Surdeanu. 2023b. Data
tamination: Languagemodelsmaynotbefew-shot
contamination quiz: A tool to detect and estimate
anymore. In Proceedings of the AAAI Conference
contamination in large language models. CoRR,
onArtificialIntelligence,volume38,pages18471–
abs/2311.06233.
18480.
AriHoltzman,JanBuys,LiDu,MaxwellForbes,and
YejinChoi.2020. Thecuriouscaseofneuraltextde- Yucheng Li. 2023a. Estimating contamination via
generation. InInternationalConferenceonLearning perplexity: Quantifying memorisation in language
Representations. modelevaluation. arXivpreprintarXiv:2309.10677.
10Yucheng Li. 2023b. An open source data contamina- modelsusingmembershipinferenceattacks. InPro-
tionreportforllamaseriesmodels. arXivpreprint ceedingsofthe2022ConferenceonEmpiricalMeth-
arXiv:2310.17589. ods in Natural Language Processing, pages 8332–
8347,AbuDhabi,UnitedArabEmirates.Association
Chuang Liu, Renren Jin, Mark Steedman, and Deyi forComputationalLinguistics.
Xiong. 2024. Evaluating Chinese large language
modelsondisciplineknowledgeacquisitionviamem- Eric Mitchell, Yoonho Lee, Alexander Khazatsky,
orization and robustness assessment. In Proceed- ChristopherDManning,andChelseaFinn.2023. De-
ings of the 1st Workshop on Data Contamination tectgpt: Zero-shotmachine-generatedtextdetection
(CONDA),pages1–12,Bangkok,Thailand.Associa- using probability curvature. In International Con-
tionforComputationalLinguistics. ferenceonMachineLearning,pages24950–24962.
PMLR.
InbalMagarandRoySchwartz.2022. Datacontamina-
tion:Frommemorizationtoexploitation. InProceed- OpenAI. Models-openaiapi. AccessedonOct6,2024.
ingsofthe60thAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume2: ShortPapers),
OpenAI.2024a. Gpt-4omini: advancingcost-efficient
pages157–165.
intelligence. AccessedonOct6,2024.
Pratyush Maini, Zhili Feng, Avi Schwarzschild,
OpenAI. 2024b. Hello gpt-4o. Accessed on Oct 6,
ZacharyC.Lipton,andJ.ZicoKolter.2024a. Tofu:
2024.
Ataskoffictitiousunlearningforllms.
Yonatan Oren, Nicole Meister, Niladri S. Chatterji,
Pratyush Maini, Hengrui Jia, Nicolas Papernot, and
FaisalLadhak,andTatsunoriHashimoto.2024. Prov-
AdamDziedzic.2024b. Llmdatasetinference: Did
ing test set contamination in black-box language
youtrainonmydataset? The1stWorkshoponData
models. In The Twelfth International Conference
Contamination(CONDA).
onLearningRepresentations.
MarcMaroneandBenjaminVanDurme.2023. Data
MedhaPalavalli,AmandaBertsch,andMatthewGorm-
portraits: Recordingfoundationmodeltrainingdata.
ley. 2024. A taxonomy for data contamination in
InAdvancesinNeuralInformationProcessingSys-
large language models. In Proceedings of the 1st
tems,volume36,pages15121–15135.CurranAsso-
WorkshoponDataContamination(CONDA),pages
ciates,Inc.
22–40,Bangkok,Thailand.AssociationforCompu-
tationalLinguistics.
JustusMattern, FatemehsadatMireshghallah,Zhijing
Jin, Bernhard Schoelkopf, Mrinmaya Sachan, and
Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang.
TaylorBerg-Kirkpatrick.2023. Membershipinfer-
2020. Privacy risks of general-purpose language
enceattacksagainstlanguagemodelsvianeighbour-
models. In2020IEEESymposiumonSecurityand
hoodcomparison. InFindingsoftheAssociationfor
Privacy(SP),pages1314–1331.
ComputationalLinguistics: ACL2023,pages11330–
11343.
Sundar Pichai and Demis Hassabis. 2024. Our next-
RThomasMcCoy,PaulSmolensky,TalLinzen,Jian- generationmodel: Gemini1.5. AccessedonOct6,
fengGao,andAsliCelikyilmaz.2023. Howmuchdo 2024.
languagemodelscopyfromtheirtrainingdata?evalu-
atinglinguisticnoveltyintextgenerationusingraven. AleksandraPiktus,ChristopherAkiki,PauloVillegas,
TransactionsoftheAssociationforComputational Hugo Laurençon, Gérard Dupont, Sasha Luccioni,
Linguistics,11:652–670. Yacine Jernite, and Anna Rogers. 2023. The roots
searchtool: Datatransparencyforllms. InProceed-
Behzad Mehrbakhsh, Dario Garigliotti, Fernando ingsofthe61stAnnualMeetingoftheAssociation
Martínez-Plumed,andJoseHernandez-Orallo.2024. for Computational Linguistics (Volume 3: System
Confoundersininstancevariationfortheanalysisof Demonstrations),pages304–314.
datacontamination. InProceedingsofthe1stWork-
shoponDataContamination(CONDA),pages13–21, Federico Ranaldi, Elena Sofia Ruzzetti, Dario Ono-
Bangkok,Thailand.AssociationforComputational rati,LeonardoRanaldi,CristinaGiannone,Andrea
Linguistics. Favalli,RanieroRomagnoli,andFabioMassimoZan-
zotto. 2024. Investigating the impact of data con-
Meta AI. 2024. Introducing meta llama 3: The most tamination of large language models in text-to-sql
capableopenlyavailablellmtodate. https://ai.m translation. arXivpreprintarXiv:2402.08100.
eta.com/blog/meta-llama-3/. Accessed: 2024-
04-18. YasamanRazeghi, RobertLLoganIV,MattGardner,
andSameerSingh.2022. Impactofpretrainingterm
Fatemehsadat Mireshghallah, Kartik Goyal, Archit frequencies on few-shot numerical reasoning. In
Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. FindingsoftheAssociationforComputationalLin-
2022. Quantifyingprivacyrisksofmaskedlanguage guistics: EMNLP2022,pages840–854.
11MartinRiddell, AnsongNi, andArmanCohan.2024. ShuoYang,Wei-LinChiang,LianminZheng,JosephE.
Quantifyingcontaminationinevaluatingcodegener- Gonzalez,andIonStoica.2023. Rethinkingbench-
ationcapabilitiesoflanguagemodels. arXivpreprint markandcontaminationforlanguagemodelswith
arXiv:2403.04811. rephrasedsamples. Preprint,arXiv:2311.04850.
Oscar Sainz, Jon Campos, Iker García-Ferrero, Julen
Wen-wai Yim, Yujuan Fu, Asma Ben Abacha, and
Etxaniz,OierLopezdeLacalle,andEnekoAgirre.
MelihaYetisgen.2024. Toerrishuman,howabout
2023a. Nlp evaluation in trouble: On the need to
medical large language models? comparing pre-
measurellmdatacontaminationforeachbenchmark.
trainedlanguagemodelsformedicalassessmenter-
In Findings of the Association for Computational
rorsandreliability. InProceedingsofthe2024Joint
Linguistics: EMNLP2023,pages10776–10787.
InternationalConferenceonComputationalLinguis-
tics, Language Resources and Evaluation (LREC-
OscarSainz,JonAnderCampos,IkerGarcía-Ferrero,
COLING2024),pages16211–16223,Torino,Italia.
JulenEtxaniz,andEnekoAgirre.2023b. Didchatgpt
ELRAandICCL.
cheatonyourtest? Accessed: 2024-09-09.
Avi Schwarzschild, Zhili Feng, Pratyush Maini, Santiago Zanella-Béguelin, Lukas Wutschitz, Shruti
ZacharyCLipton,andJZicoKolter.2024. Rethink- Tople, Victor R"¨uhle, Andrew Paverd, Olga Ohri-
ingllmmemorizationthroughthelensofadversarial menko,BorisK"¨opf,andMarcBrockschmidt.2020.
compression. The1stWorkshoponDataContamina- Analyzinginformationleakageofupdatestonatural
tion(CONDA). languagemodels. InProceedingsofthe2020ACM
SIGSAC conference on computer and communica-
Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo tionssecurity,pages363–375.
Huang,DaogaoLiu,TerraBlevins,DanqiChen,and
LukeZettlemoyer.2023. Detectingpretrainingdata
Chiyuan Zhang, Daphne Ippolito, Katherine Lee,
fromlargelanguagemodels. InNeurIPS2023Work-
MatthewJagielski,FlorianTramèr,andNicholasCar-
shoponRegulatableML.
lini. 2023. Counterfactual memorization in neural
languagemodels. AdvancesinNeuralInformation
CongzhengSongandVitalyShmatikov.2019. Audit-
ProcessingSystems,36:39321–39362.
ing data provenance in text-generation models. In
Proceedingsofthe25thACMSIGKDDInternational
ConferenceonKnowledgeDiscovery& DataMin- Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
ing,KDD’19,page196–206,NewYork,NY,USA. XiaoleiWang,YupengHou,YingqianMin,Beichen
Zhang, Junjie Zhang, Zican Dong, et al. 2023. A
AssociationforComputingMachinery.
survey of large language models. arXiv preprint
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, arXiv:2303.18223.
AbuAwalMdShoeb,AbubakarAbid,AdamFisch,
AdamRBrown,AdamSantoro,AdityaGupta,Adrià KunZhou,YutaoZhu,ZhipengChen,WentongChen,
Garriga-Alonso, et al. 2023. Beyond the imitation Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong
game: Quantifying and extrapolating the capabili- Wen, and Jiawei Han. 2023. Don’t make your llm
tiesoflanguagemodels. TransactionsonMachine an evaluation benchmark cheater. arXiv preprint
LearningResearch. arXiv:2311.01964.
KushalTirumala,AramMarkosyan,LukeZettlemoyer,
WenhongZhu,HongkunHao,ZhiweiHe,Yun-ZeSong,
andArmenAghajanyan.2022. Memorizationwith-
JiaoYueyang,YumengZhang,HanxuHu,YiranWei,
outoverfitting: Analyzingthetrainingdynamicsof
RuiWang,andHongyuanLu.2024. CLEAN–EVAL:
largelanguagemodels. AdvancesinNeuralInforma-
Clean evaluation on contaminated large language
tionProcessingSystems,35:38274–38290.
models. InFindingsoftheAssociationforCompu-
tationalLinguistics: NAACL2024,pages835–847,
Veniamin Veselovsky, Manoel Horta Ribeiro, and
MexicoCity,Mexico.AssociationforComputational
RobertWest.2023. Artificialartificialartificialintel-
Linguistics.
ligence: Crowdworkerswidelyuselargelanguage
models for text production tasks. arXiv preprint
arXiv:2306.07899. YongshuoZong,TingyangYu,BingchenZhao,Ruchika
Chavhan, and Timothy Hospedales. 2023. Fool
JohnnyWei,RyanWang,andRobinJia.2024. Proving your (vision and) language model with embar-
membershipinLLMpretrainingdataviadatawater- rassingly simple permutations. arXiv preprint
marks. In Findings of the Association for Compu- arXiv:2310.01651.
tationalLinguisticsACL2024,pages13306–13320,
Bangkok,Thailandandvirtualmeeting.Association
A Appendix
forComputationalLinguistics.
ChengXu, ShuhaoGuan, DerekGreene, MKechadi, A.1 TableofNotations
et al. 2024. Benchmark data contamination of
Wepresentthenotationsusedinthismanuscriptin
large language models: A survey. arXiv preprint
arXiv:2406.04244. Table3.
12Notation Definition MeetRequire.
Model Citation
x Anlanguageinstance,asaseriesoftokens. 1 2
Aprefix-suffixinstancepair,whichisa Claude3.5Sonnet Anthropic(2024b) No -
Claude3Opus Anthropic(2024c) No -
(xp,xs) commondataformatinnatural
Gemini1.5Pro PichaiandHassabis(2024) No -
languagegeneration(NLG)tasks.
GPT-4 Achiametal.(2023) No -
(xc,xk) A tasc ko sn ,t ae sx dt- ek fie ny ei dns intan Rc ee qup ia ri er mfr eo nm tRsl 5o .t-filling L Cl la am uda e3 3I Hns at ir ku uct-70B M Ane tt ha rA opI i( c2 (0 22 04 2)
4a)
N No
o
-
-
M Alanguagemodel. GPT-3.5 OpenAI No -
M’soutputrespecttoaninputx,given Mixtral8x7B Jiangetal.(2024) No -
adecodingsetup·.If·isnotspecified, GPT-4o OpenAI(2024b) No -
M·(x)
weconsideritasafixed,butunknown
GPT-4omini OpenAI(2024a) No -
decodingstate.
Table5: Noneofthetop10LMs,intheLLMLeader-
D Adataset,asasetoflanguageinstances.
DM M’strainset. boardbyVellummeettherequirementsofdisclosing
Binaryindicatorfunctionforinstance-level pre-trainingcorpora(Requirement1).
contamination,whichtakestheinputastwo
b(x,x′)
instancesxandx′andoutputsFalse(0)or
True(1),basedontheinstancesimilarity. A.4 EntropyCalculation
Afunctionaccessingthesimilaritybetween
S(x,x′) twoinstances,xandx′andoutputsand Incasuallanguagemodeling,weconsideranLM
outputsarealvalue. M withaprefixsequencex . Wedeploythetoken
p
Goldstandardforinstance-level
f(M,x) entropytomeasurethecertaintyofthenexttoken
contamination,definedbyEquation1.
Theprobabilityoftheinstancexgiven generation. Wefirstfindthenexttokenprobability
PM(x)
anLMM.
distributionoverthevocabulary. GiveneachPythia
Theaverageprobabilitiesoftopp%
MinTOP leastlikelytokensinaninstancex,for modelhasmorethan50ktokensinthevocabulary
pProb
agivenLMM. (Biderman et al., 2023), the entropy is measured
τ· Thecontaminationthresholdforfunctions.
overthetopk mostlikelytokens(Entropyk). We
Themeasureofvariationsofoutputs
Var({M·(xp)}) forMunderdiverse,differentsampling firstnormalizetheprobabilityofhek tokens:
strategies.givenxp
Entropyk dT eh fie ne en dtr bo ypy Eqo uf ath tie onto 1p 1k . mostlikelytokens, P′(M(x )) = P i(M(x p)) (10)
TheevaluationresultofanLMMona
i p (cid:80)k
P (M(x ))
Eval(M,D) i=1 i p
datasetD.
Theentropyisthencalculatedoverthenormal-
Table3: Tableofnotations.
izedprobabilityamongthek tokens.
Entropy (M,x )
A.2 RisksandMitigationApproachesfor k p
DataContamination k
(cid:88)
= − P′(M(x ))logP′(M(x ))
i p i p
During our paper collection, we identify the rel-
i=1
evant research on the risks and mitigation strate- (11)
gies for data contamination, which does not in-
volveproposingorevaluatingexistingdetectionap- A.5 MoreCaseStudyResults
proachesfordatacontamination. Whileexcluding A.5.1 Within-DomainDetectionwith
thosepapersfromthemaintextofthismanuscript, DifferentLMs
wepresenttheminTable4inthisAppendix.
Inthissection,wepresentthedetectionAUCsfor
other sizes of Pythia models: Pythia-70m (Table
A.3 CaseStudyforInstanceSimilarity
6), Pythia-160m (Table 7), Pythia-410m (Table
To assess the applicability of instance similarity- 8), Pythia-1.4b (Table 9), Pythia-2.8b (Table 10),
baseddetectionapproaches,weanalyzedhowfre- Pythia-12b(Table11). SimilartotheresultsinTa-
quentlytheirrequirementsaremet. Wereviewed ble2,weobservedclose-to-randomperformance
the top 10 models from the Vellum LLM leader- inthedetectionAUCs.
board8. As demonstrated in Table 5, none of the
models fulfilled the most basic, requirement 1.
However,somemodelsdisclosetheircut-offdate
for collecting training data (Achiam et al., 2023;
OpenAI).
8https://www.vellum.ai/llm-leaderboard#model
-comparison.AccessedonOct6,2024.
13Citation Content
Zhouetal.(2023) Impactofdirectdatacontaminationontestperformance
Dekonincketal.(2024) Impactofindirectdatacontaminationontestperformance
Jacovietal.(2023) Strategiestopreventcontaminationinbenchmarkdatasets
Zhuetal.(2024) Strategiestomitigatecontaminationinbenchmarkdatasets
McCoyetal.(2023) EvaluatingthenoveltyofLM-generatedtext
Mainietal.(2024a) StudyingunlearningmethodstomakeLMsforgetspecifictrainingdata
Bowenetal.(2024) Studyingcontributingfactorsbehinddatapoisoning,withcorruptedormalicioustrainingdata.
Mitchelletal.(2023) Differentiateshumanvs.machine-generatedtextusingprobabilitycurvature
Table4: Seletedrelevantworktorisksandmitigationapproachesfordatacontamination.
Free- PubMed Enron PubMed Open- OpenWeb- Youtube- Hacker-
Assumptions&Metric Github ArXiv Pile-CC
Law Central Emails Abstracts Subtitles Text2 Subtitles News
PPL_50 49.7 48.9 50.7 49.4 49.6 49.7 47.9 50.3 50.8 48.4 49.8
PPL_100 50.8 49.4 51.1 50.1 50.5 48.8 48.1 48.9 50.3 47.3 50.4
PPL_200 51.1 49.4 50.9 50.2 51.4 49.9 47.8 48.5 49.5 50.7 52.1
A1
Min5%token 51.5 49.8 51.3 48.5 50.0 48.5 49.8 49.7 49.0 47.8 50.9
Min15%token 51.2 49.8 51.3 49.2 50.1 49.4 49.9 49.2 49.2 49.6 51.1
Min25%token 51.0 49.4 51.4 49.5 50.0 49.6 50.2 49.0 49.2 49.8 51.1
Mem1 49.2 48.9 50.5 47.1 49.8 50.7 49.6 50.6 49.4 48.3 49.7
A4 Mem3 49.7 50.0 51.0 46.5 50.1 49.8 51.5 50.5 48.2 49.3 48.7
Mem5 49.7 48.0 50.8 48.2 49.7 52.7 50.8 50.2 49.1 50.0 49.4
Entropy5 51.9 49.2 52.4 48.9 50.3 48.0 49.4 51.9 48.6 49.0 50.4
A6 Entropy10 52.2 49.4 52.3 49.3 50.4 48.2 48.7 52.1 48.6 49.0 50.6
Entropy25 52.1 49.5 50.9 49.5 50.7 48.5 48.1 51.0 48.1 49.6 50.9
Seen 10.8±25.2 13.1±5.4 17.2±9.2 16.0±7.6 137.4±1682.7 29.1±15.6 26.2±7.5 43.3±49.5 37.3±46.5 42.0±24.1 50.0±44.6
PPL
Unseen 10.5±19.6 13.7±13.4 18.6±34.0 15.8±7.4 38.9±44.7 29.2±22.7 26.1±8.7 41.7±31.5 36.9±37.1 40.8±21.0 49.8±36.1
Table6: AveragecontaminationdetectionAUCforthePythia-70mmodel,underdifferentPILEdomains. ‘PPL’
representstheaverageperplexity±STD,whereinstancesaretruncatedbythemodelmaxlengthof2048tokens.
A.5.2 MetricDistributioninHistogram
Inthissection,wepresentthedistributionsofdif-
ferent metrics both within domain and across do-
mains.
We compare between the GitHub and Pile-CC
domains. As shown in Figure 2 & 3, when the Figure3: ThedistributionofPPL_200whenbothseen
seen and unseen instances are from the same do- andunseeninstancesarefromthePile-CCdomain.
main,theirPPL_200distributionsareverysimilar.
However,asshowninFigure4&5,whentheseen
and unseen instances are from different domains,
theirPPL_200distributionsareverydifferent. This
indicatesthatthePPL_200relatesmoretodomain
shifts,insteadofthecontaminationstatusofindi-
vidualinstances.
Figure4: ThedistributionofPPL_200whentheseen
Figure2: ThedistributionofPPL_200whenbothseen instancesarefromtheGithubdomain,andtheunseen
andunseeninstancesarefromtheGithubdomain. instancesarefromthePile-CCdomain.
14Free- PubMed Enron PubMed Open- OpenWeb- Youtube- Hacker-
Assumptions&Metric Github ArXiv Pile-CC
Law Central Emails Abstracts Subtitles Text2 Subtitles News
PPL_50 49.7 48.9 50.7 49.4 49.6 49.7 47.9 50.3 50.8 48.4 49.8
PPL_100 50.8 49.4 51.1 50.1 50.5 48.8 48.1 48.9 50.3 47.3 50.4
PPL_200 51.1 49.4 50.9 50.2 51.4 49.9 47.8 48.5 49.5 50.7 52.1
A1
Min5%token 51.5 49.8 51.3 48.5 50.0 48.5 49.8 49.7 49.0 47.8 50.9
Min15%token 51.2 49.8 51.3 49.2 50.1 49.4 49.9 49.2 49.2 49.6 51.1
Min25%token 51.0 49.4 51.4 49.5 50.0 49.6 50.2 49.0 49.2 49.8 51.1
Mem1 49.2 48.9 50.5 47.1 49.8 50.7 49.6 50.6 49.4 48.3 49.7
A4 Mem3 49.7 50.0 51.0 46.5 50.1 49.8 51.5 50.5 48.2 49.3 48.7
Mem5 49.7 48.0 50.8 48.2 49.7 52.7 50.8 50.2 49.1 50.0 49.4
Entropy5 51.9 49.2 52.4 48.9 50.3 48.0 49.4 51.9 48.6 49.0 50.4
A6 Entropy10 52.2 49.4 52.3 49.3 50.4 48.2 48.7 52.1 48.6 49.0 50.6
Entropy25 52.1 49.5 50.9 49.5 50.7 48.5 48.1 51.0 48.1 49.6 50.9
Seen 7.1±15.6 8.5±3.4 11.8±5.9 11.1±5.0 25.1±34.8 19.0±10.3 20.0±7.2 27.3±23.9 25.2±29.9 28.5±15.2 33.1±29.8
PPL
Unseen 7.4±18.8 8.9±9.7 12.1±5.1 11.0±5.0 25.5±30.8 18.8±11.3 19.9±6.4 26.9±19.3 24.8±25.2 27.9±14.2 32.9±24.3
Table7: AveragecontaminationdetectionAUCforthePythia-160mmodel,underdifferentPILEdomains. ‘PPL’
representstheaverageperplexity±STD,whereinstancesaretruncatedbythemodelmaxlengthof2048tokens.
Free- PubMed Enron PubMed Open- OpenWeb- Youtube- Hacker-
Assumptions&Metric Github ArXiv Pile-CC
Law Central Emails Abstracts Subtitles Text2 Subtitles News
PPL_50 49.7 48.9 50.7 49.4 49.6 49.7 47.9 50.3 50.8 48.4 49.8
PPL_100 50.8 49.4 51.1 50.1 50.5 48.8 48.1 48.9 50.3 47.3 50.4
PPL_200 51.1 49.4 50.9 50.2 51.4 49.9 47.8 48.5 49.5 50.7 52.1
A1
Min5%token 51.5 49.8 51.3 48.5 50.0 48.5 49.8 49.7 49.0 47.8 50.9
Min15%token 51.2 49.8 51.3 49.2 50.1 49.4 49.9 49.2 49.2 49.6 51.1
Min25%token 51.0 49.4 51.4 49.5 50.0 49.6 50.2 49.0 49.2 49.8 51.1
Mem1 49.2 48.9 50.5 47.1 49.8 50.7 49.6 50.6 49.4 48.3 49.7
A4 Mem3 49.7 50.0 51.0 46.5 50.1 49.8 51.5 50.5 48.2 49.3 48.7
Mem5 49.7 48.0 50.8 48.2 49.7 52.7 50.8 50.2 49.1 50.0 49.4
Entropy5 51.9 49.2 52.4 48.9 50.3 48.0 49.4 51.9 48.6 49.0 50.4
A6 Entropy10 52.2 49.4 52.3 49.3 50.4 48.2 48.7 52.1 48.6 49.0 50.6
Entropy25 52.1 49.5 50.9 49.5 50.7 48.5 48.1 51.0 48.1 49.6 50.9
Seen 4.8±11.4 5.9±2.2 8.3±4.7 7.9±3.3 16.1±22.9 13.0±7.0 14.9±4.0 17.7±14.4 17.0±21.4 19.5±10.8 22.4±20.7
PPL
Unseen 4.7±8.7 6.1±6.0 8.5±3.6 7.9±3.4 16.2±19.5 12.9±7.3 14.9±4.5 17.5±13.4 16.6±18.4 19.2±10.3 22.3±17.3
Table8: AveragecontaminationdetectionAUCforthePythia-410mmodel,underdifferentPILEdomains. ‘PPL’
representstheaverageperplexity±STD,whereinstancesaretruncatedbythemodelmaxlengthof2048tokens.
Figure5: ThedistributionofPPL_200whentheseen Figure7:ThedistributionofMin25%Probwhenboth
instancesarefromthePile-CCdomain,andtheunseen seenandunseeninstancesarefromthePile-CCdomain.
instancesarefromtheGithubdomain.
We observe a similar trend for other metrics:
Min25%Prob(Figure6,7,8,9),Mem5(Figure
10,11,12,13),Entropy25(Figure14,15,16,17).
Figure8: ThedistributionofMin25%Probwhenthe
Figure6:ThedistributionofMin25%Probwhenboth seen instances are from the Github domain, and the
seenandunseeninstancesarefromtheGithubdomain. unseeninstancesarefromthePile-CCdomain.
15Free- PubMed Enron PubMed Open- OpenWeb- Youtube- Hacker-
Assumptions&Metric Github ArXiv Pile-CC
Law Central Emails Abstracts Subtitles Text2 Subtitles News
PPL_50 49.7 48.9 50.7 49.4 49.6 49.7 47.9 50.3 50.8 48.4 49.8
PPL_100 50.8 49.4 51.1 50.1 50.5 48.8 48.1 48.9 50.3 47.3 50.4
PPL_200 51.1 49.4 50.9 50.2 51.4 49.9 47.8 48.5 49.5 50.7 52.1
A1
Min5%token 51.5 49.8 51.3 48.5 50.0 48.5 49.8 49.7 49.0 47.8 50.9
Min15%token 51.2 49.8 51.3 49.2 50.1 49.4 49.9 49.2 49.2 49.6 51.1
Min25%token 51.0 49.4 51.4 49.5 50.0 49.6 50.2 49.0 49.2 49.8 51.1
Mem1 49.2 48.9 50.5 47.1 49.8 50.7 49.6 50.6 49.4 48.3 49.7
A4 Mem3 49.7 50.0 51.0 46.5 50.1 49.8 51.5 50.5 48.2 49.3 48.7
Mem5 49.7 48.0 50.8 48.2 49.7 52.7 50.8 50.2 49.1 50.0 49.4
Entropy5 51.9 49.2 52.4 48.9 50.3 48.0 49.4 51.9 48.6 49.0 50.4
A6 Entropy10 52.2 49.4 52.3 49.3 50.4 48.2 48.7 52.1 48.6 49.0 50.6
Entropy25 52.1 49.5 50.9 49.5 50.7 48.5 48.1 51.0 48.1 49.6 50.9
Seen 3.7±8.0 4.7±1.7 6.7±2.9 6.5±2.5 11.2±14.9 10.3±5.4 12.2±3.3 13.6±11.5 13.5±16.5 15.1±8.1 17.3±15.3
PPL
Unseen 3.7±6.9 4.9±4.2 6.8±2.7 6.5±2.7 11.4±14.8 10.2±5.4 12.2±3.7 13.5±11.6 13.0±14.9 14.8±7.4 17.2±13.2
Table9: AveragecontaminationdetectionAUCforthePythia-1.4bmodel,underdifferentPILEdomains. ‘PPL’
representstheaverageperplexity±STD,whereinstancesaretruncatedbythemodelmaxlengthof2048tokens.
Free- PubMed Enron PubMed Open- OpenWeb- Youtube- Hacker-
Assumptions&Metric Github ArXiv Pile-CC
Law Central Emails Abstracts Subtitles Text2 Subtitles News
PPL_50 49.7 48.9 50.7 49.4 49.6 49.7 47.9 50.3 50.8 48.4 49.8
PPL_100 50.8 49.4 51.1 50.1 50.5 48.8 48.1 48.9 50.3 47.3 50.4
PPL_200 51.1 49.4 50.9 50.2 51.4 49.9 47.8 48.5 49.5 50.7 52.1
A1
Min5%token 51.5 49.8 51.3 48.5 50.0 48.5 49.8 49.7 49.0 47.8 50.9
Min15%token 51.2 49.8 51.3 49.2 50.1 49.4 49.9 49.2 49.2 49.6 51.1
Min25%token 51.0 49.4 51.4 49.5 50.0 49.6 50.2 49.0 49.2 49.8 51.1
Mem1 49.2 48.9 50.5 47.1 49.8 50.7 49.6 50.6 49.4 48.3 49.7
A4 Mem3 49.7 50.0 51.0 46.5 50.1 49.8 51.5 50.5 48.2 49.3 48.7
Mem5 49.7 48.0 50.8 48.2 49.7 52.7 50.8 50.2 49.1 50.0 49.4
Entropy5 51.9 49.2 52.4 48.9 50.3 48.0 49.4 51.9 48.6 49.0 50.4
A6 Entropy10 52.2 49.4 52.3 49.3 50.4 48.2 48.7 52.1 48.6 49.0 50.6
Entropy25 52.1 49.5 50.9 49.5 50.7 48.5 48.1 51.0 48.1 49.6 50.9
Seen 3.3±6.6 4.2±1.5 6.1±2.8 6.0±2.2 9.4±13.3 9.3±4.8 10.8±3.2 11.9±9.4 12.1±15.3 13.1±6.6 15.4±13.8
PPL
Unseen 3.3±6.5 4.3±3.5 6.2±2.5 6.0±2.4 9.5±12.4 9.2±4.8 10.8±3.5 11.8±9.0 11.7±14.2 12.9±6.0 15.2±11.4
Table10: AveragecontaminationdetectionAUCforthePythia-2.8bmodel,underdifferentPILEdomains. ‘PPL’
representstheaverageperplexity±STD,whereinstancesaretruncatedbythemodelmaxlengthof2048tokens.
Figure9: ThedistributionofMin25%Probwhenthe Figure11: ThedistributionofMem5whenbothseen
seeninstancesarefromthePile-CCdomain, andthe andunseeninstancesarefromthePile-CCdomain.
unseeninstancesarefromtheGithubdomain.
Figure12: ThedistributionofMem5whentheseen
Figure10: ThedistributionofMem5whenbothseen instancesarefromtheGithubdomain,andtheunseen
andunseeninstancesarefromtheGithubdomain. instancesarefromthePile-CCdomain.
16Free- PubMed Enron PubMed Open- OpenWeb- Youtube- Hacker-
Assumptions&Metric Github ArXiv Pile-CC
Law Central Emails Abstracts Subtitles Text2 Subtitles News
PPL_50 49.7 48.9 50.7 49.4 49.6 49.7 47.9 50.3 50.8 48.4 49.8
PPL_100 50.8 49.4 51.1 50.1 50.5 48.8 48.1 48.9 50.3 47.3 50.4
PPL_200 51.1 49.4 50.9 50.2 51.4 49.9 47.8 48.5 49.5 50.7 52.1
A1
Min5%token 51.5 49.8 51.3 48.5 50.0 48.5 49.8 49.7 49.0 47.8 50.9
Min15%token 51.2 49.8 51.3 49.2 50.1 49.4 49.9 49.2 49.2 49.6 51.1
Min25%token 51.0 49.4 51.4 49.5 50.0 49.6 50.2 49.0 49.2 49.8 51.1
Mem1 49.2 48.9 50.5 47.1 49.8 50.7 49.6 50.6 49.4 48.3 49.7
A4 Mem3 49.7 50.0 51.0 46.5 50.1 49.8 51.5 50.5 48.2 49.3 48.7
Mem5 49.7 48.0 50.8 48.2 49.7 52.7 50.8 50.2 49.1 50.0 49.4
Entropy5 51.9 49.2 52.4 48.9 50.3 48.0 49.4 51.9 48.6 49.0 50.4
A6 Entropy10 52.2 49.4 52.3 49.3 50.4 48.2 48.7 52.1 48.6 49.0 50.6
Entropy25 52.1 49.5 50.9 49.5 50.7 48.5 48.1 51.0 48.1 49.6 50.9
Seen 2.8±5.4 3.8±1.3 5.4±2.6 5.4±1.9 6.8±8.9 8.2±4.0 8.6±3.5 10.0±7.9 10.3±12.6 11.1±5.3 13.1±11.1
PPL
Unseen 2.9±6.0 3.8±2.7 5.4±2.0 5.4±2.1 7.0±9.4 8.1±3.9 8.6±3.7 9.9±8.0 10.1±12.5 10.8±4.8 13.0±9.7
Table11: AveragecontaminationdetectionAUCforthePythia-12bmodel,underdifferentPILEdomains. ‘PPL’
representstheaverageperplexity±STD,whereinstancesaretruncatedbythemodelmaxlengthof2048tokens.
Figure13: ThedistributionofMem5whentheseen Figure 16: The distribution of Entropy 25 when the
instancesarefromthePile-CCdomain,andtheunseen seen instances are from the Github domain, and the
instancesarefromtheGithubdomain. unseeninstancesarefromthePile-CCdomain.
Figure 17: The distribution of Entropy 25 when the
seeninstancesarefromthePile-CCdomain, andthe
unseeninstancesarefromtheGithubdomain.
Figure14: ThedistributionofEntropy25whenboth
seenandunseeninstancesarefromtheGithubdomain.
Figure15: ThedistributionofEntropy25whenboth
seenandunseeninstancesarefromthePile-CCdomain.
17A.5.3 Cross-DomainDetectionwithDifferent
Metrics
Inthissection,wepresentAUCresultswithother
metrics,whenseenandunseeninstancesarefrom
differentdomains,forthePythia-6.9bmodel. The
metrics include Min Top 5% Prob (Figure 18),
Mem 1 (Figure 19), and Entropy 5 (Figure 20).
AllmetricsexhibithigherAUCvaluesinthetop-
right corner and lower values in the bottom-left,
whilediagonalpointsapproachrandomguessing.
Figure20: AveragecontaminationdetectionAUCfor
thePythia-6.9bmodelwiththemetric,Entropy5,when
the seen and unseen instances are from different do-
mains.TheabbreviationsrepresentthedomainsinTable
2.
Figure18: AveragecontaminationdetectionAUCfor
the Pythia-6.9b model with the metric, Min Top 5%
Prob, when the seen and unseen instances are from
differentdomains. Theabbreviationsrepresentthedo-
mainsinTable2.
Figure19: AveragecontaminationdetectionAUCfor
thePythia-6.9bmodelwiththemetric,Mem1,whenthe
seenandunseeninstancesarefromdifferentdomains.
TheabbreviationsrepresentthedomainsinTable2.
18