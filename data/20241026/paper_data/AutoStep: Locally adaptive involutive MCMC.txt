AutoStep: Locally adaptive involutive MCMC
Tiange Liu Nikola Surjanovic Miguel Biron-Lattes
University of British Columbia University of British Columbia University of British Columbia
Alexandre Bouchard-Cˆot´e Trevor Campbell
University of British Columbia University of British Columbia
Abstract (Rossky et al., 1978), and Hamiltonian Monte Carlo
(HMC) (Duane et al., 1987; Neal, 1996)—often involve
a scalar step size parameter θ ≥ 0 that governs the
Many common Markov chain Monte Carlo
distance of the proposed next state from the current
(MCMC) kernels can be formulated using a
state. Too large a choice of θ results in distant pro-
deterministic involutive proposal with a step
posals that are often rejected, while too small a choice
size parameter. Selecting an appropriate step
leads to nearby proposals that do not explore the state
size is often a challenging task in practice;
spacequickly; eithercaseresultsinslowconvergenceof
andforcomplexmultiscaletargets, theremay
thechain. Forcertainmultiscaletargets(e.g. Bayesian
not be one choice of step size that works well
posteriors with scale priors (Polson and Scott, 2012))
globally. In this work, we address this prob-
there may not even be a single good choice of step size
lem with a novel class of involutive MCMC
throughout the whole state space.
methods—AutoStep MCMC—that selects an
appropriatestepsizeateachiterationadapted Existing methods for selecting step size parameters
to the local geometry of the target distribu- fall generally into three categories: adaptive MCMC,
tion. We prove that AutoStep MCMC is π- discrepancy minimization, and locally-adaptive ker-
invariant and has other desirable properties nels. Adaptive MCMC algorithms (Haario et al., 2001;
under mild assumptions on the target distri- Atchad´e,2006;AndrieuandThoms,2008;Marshalland
bution π and involutive proposal. Empirical Roberts,2012)tunetheproposaldistributionusingpre-
results examine the effect of various step size viousdrawsfromthechain,oftentargetingaparticular
selection design choices, and show that Au- acceptance rate derived from high-dimensional asymp-
toStep MCMC is competitive with state-of- totics (Roberts et al., 1997; Roberts and Rosenthal,
the-art methods in terms of effective sample 1998). Obtaining theoretical guarantees on estimates
size per unit cost on a range of challenging produced by adaptive MCMC targeting a distribution
target distributions. π is technically difficult and often requires strict condi-
tions on the adaptation process, such as increasingly
infrequent adaptation (Chimisov et al., 2018). Discrep-
1 INTRODUCTION
ancy minimization (Neklyudov et al., 2018; Coullon
etal.,2023)involvestuningusingadivergencebetween
MarkovchainMonteCarlo(MCMC)(Metropolisetal., the empirical distribution of draws and the target π,
1953; Hastings, 1970) is an effective tool for approx- whichrequiresmultipleMCMCrunstoestimatethedi-
imating integrals arising in Bayesian inference prob- vergence for each candidate step size. Both approaches
lems. The performance of MCMC is often sensitive also identify only a single step size value, which may
to the choice of tuning parameters in the Markov not be appropriate for the whole state space.
kernel. In particular, methods that propose a new
Locally-adaptive kernels, in contrast, select a value for
state followed by an accept/reject step—e.g., random-
the step size at each iteration based on the current
walk Metropolis–Hastings (RWMH) (Hastings, 1970),
state. Because the step size depends on the current
the Metropolis-adjusted Langevin algorithm (MALA)
state, these kernels can adapt to the local shape of the
target π; and because they depend only on the current
The code, data, and instructions needed to reproduce the state, they are Markovian and one can use standard
main experimental results: toolstoproveπ-invariance. Therearemanyapproaches
https://github.com/Julia-Tempering/soft-auto-mev to locally adaptive step size selection in the literature.
4202
tcO
42
]OC.tats[
1v92981.0142:viXraAutoStep: Locally adaptive involutive MCMC
Mixture kernels with state-dependent weights (Maire we draw z ∼m and the proposal
t
and Vandekerkhove, 2022) and delayed-rejection (Tier-
ney and Mira, 1999; Green and Mira, 2001) are both x′ t+1,z t′ +1 =f θ(x t,z t). (1)
general approaches, but each requires a prespecified
We set the next state to x =x′ with probability
maximum number of step sizes to consider at each t+1 t+1
iteration. There are also numerous methods specific
min{1,exp(ℓ(x ,z ,θ))}, (2)
t t
to certain samplers, e.g., HMC and MALA (Girolami
and Calderhead, 2011; Nishimura and Dunson, 2016; where
Kleppe, 2016; Modi et al., 2023; Biron-Lattes et al.,
(cid:18)π(x′ )m(z′ ) (cid:19)
2024; Turok et al., 2024), RWMH (see Livingstone, ℓ(x ,z ,θ)=log t+1 t+1 |∇f (x ,z )| ,
2021), or slice sampling (Neal, 2003). Of these, the t t π(x t)m(z t) θ t t
methodmostrelatedtothepresentworkisautoMALA
and otherwise set it to x =x . The sequence x is
(Biron-Lattes et al., 2024), which chooses a step size in t+1 t t
a Markov chain and has stationary distribution π if f
MALAusingadoubling/halvingprocedurethattargets θ
is continuously differentiable (Tierney, 1998, Thm. 2).
a Metropolis–Hastings acceptance ratio in a random-
ized range (a,b)⊂[0,1]. While the method was shown Choosing different families of involutions {f } and
θ
to be π-invariant, it includes a hard reversibility check auxiliary distributions m yields different MCMC al-
that can prevent the chain from moving in challeng- gorithms. For example, random walk Metropolis–
ing areas of the state space. Furthermore, important Hastings(RWMH)(Hastings,1970)withstepsizeθ >0
theoretical properties beyond invariance—especially ir- and mass matrix M is obtained by setting
reducibility and aperiodicity—were not demonstrated.
f (x,z)=(x+θz,−z) m=N(0,M).
θ
In this work, we develop a method for locally-adaptive
step size selection in the broad class of involutive The Metropolis-adjusted Langevin algorithm (MALA)
MCMC methods (Tierney, 1998; Andrieu et al., 2020; (Rossky et al., 1978) with step size θ > 0 and mass
Neklyudov et al., 2020). We show that these Markov matrix M is obtained by setting
kernelsareπ-invariant,irreducible,andaperiodicunder
mild assumptions on the target distribution, and in- f θ(x,z)=(x′,−z′) m=N(0,M),
vestigate bounds on energy jump distances. Empirical
where x′,z′ are computed via the leapfrog map
resultsdemonstratethatthestepsizeselectionmethod
performs well across a wide range of examples. Proofs
θ
of all theoretical results are provided in Appendix A. z 1/2 ←z+ 2∇logπ(x)
x′ ←x+θM−1z
1/2
2 BACKGROUND θ
z′ ←z + ∇logπ(x′).
1/2 2
Let π be a given target probability distribution on an
Finally,HamiltonianMonteCarlo(HMC)(Duaneetal.,
open subset X ⊂Rd. With a slight abuse of notation,
1987; Neal, 1996) with step size θ >0, mass matrix M,
weassumethatπ admitsadensityπ(x)withrespectto
and path length L is obtained by setting
the Lebesgue measure on Rd, and that we can evaluate
a function γ(x)∝π(x) pointwise so that f (x,z)=(x′,−z′) m=N(0,M),
θ
γ(x) where x′,z′ are computed via L leapfrogs.
π(x)= (cid:82) ,
γ(x)dx
ManyinvolutiveMCMCmethods—includingtheabove
(cid:82) three examples—have a positive scalar tuning param-
where γ(x)dx is the unknown normalizing constant.
eter θ >0 that can be interpreted as a form of “step
Involutive Markov Chain Monte Carlo (Tierney, 1998; size”: larger values result in more distant proposals,
Andrieu et al., 2020; Neklyudov et al., 2020) is an while smaller values result in nearby proposals. Too
MCMC method that uses involutions, i.e., functions large a choice of θ results in many rejected proposals,
f where f−1 = f, to generate new proposals. While while too small a choice results in proposals that are
therearemanypossiblevariationsofinvolutiveMCMC, accepted but explore the state space slowly. Further-
in this work we use the following formulation. Fix a more,theremaynotbeasinglechoiceofθ thatapplies
distribution m on an open subset Z ⊂Rp with density globally, e.g., in the case of multiscale targets (Polson
m(z) with respect to the Lebesgue measure, and a and Scott, 2012). This work resolves this problem by
familyofdifferentiableinvolutionsf :X×Z →X×Z selecting an appropriate θ at each iteration depending
θ
parametrized by θ ∈Θ. Then, starting from a state x , on the local behaviour of the augmented target π·m.
tAlgorithm 1 One iteration of AutoStep MCMC 1. Auxiliary refreshment: Draw
Require: Initial value x with π(x)>0, target π, aux-
z ∼m and (a ,b )∼Unif(∆).
iliary distribution m, step size distribution η, invo- t t t
lutions {f }
θ θ∈Θ
2. Tuning parameter refreshment: Draw
1: z ∼m ▷ auxiliary refreshment
2: (a,b)∼Unif(∆) ▷ soft acceptance bounds
θ ∼η(dθ |x ,z ,a ,b ).
t t t t t
3: θ ∼η(dθ |x,z,a,b) ▷ refresh tuning parameter
4: s←(x,z,a,b,θ) ▷ augmented state
3. Proposal: Set s =(x ,z ,a ,b ,θ ) and
5: s′ ←f(s) ▷ involutive proposal t t t t t t
6: U ∼Unif[0,1] s′ =f(s )=(x′ ,z′ ,a′ ,b′ ,θ′ ).
(cid:110) (cid:111) t+1 t t+1 t+1 t+1 t+1 t+1
7: if U ≤min 1,
π(s′)J(s)
then
π(s)
8: return x′ ▷ accept 4. Accept: Set x t+1 =x′ t+1 with probability
9: else (cid:26) π(s′ ) (cid:27)
10: return x ▷ reject min 1, t+1 J(s ) ,
π(s ) t
11: end if t
and otherwise set x =x .
t+1 t
3 AUTOSTEP MCMC
We can recover standard involutive MCMC by setting
η(dθ|x,z,a,b) = δ (dθ) for some fixed θ ∈ Θ. Fur-
Inthissection,wedevelopAutoStepMCMC,afamilyof θ0 0
ther, choosingdifferentinvolutionfamilies{f :θ ∈Θ}
modifiedinvolutiveMCMCmethodsthatautomatically θ
and auxiliary distributions m on Z recovers variants of
select appropriate tuning parameter values at each
common algorithms, e.g., RWMH, MALA, and HMC.
iteration. The key technique in developing AutoStep
Themajorimprovementisthatthechainmaydrawthe
MCMC is to formulate the sampler on an augmented
tuningparameterθ ∈Θautomaticallyateachstepina
space that includes the tuning parameter θ ∈Θ as well
mannerthatdependsonthecurrentstate(x ,z ,a ,b ).
as other auxiliary quantities. For a given family of t t t t
The key design choice, then, is to select a conditional
continuously differentiable involutions {f :θ ∈Θ} on
θ
tuning refreshment distribution η that yields values
X ×Z, define the augmented state space S as
of θ that are well-adapted to the local shape of the
S =X ×Z ×∆×Θ, target π¯. In this work, we focus on the design of the
conditional tuning refreshment distribution η in the
where X ×Z is the original augmented space for invo- case where θ is a step size parameter (with Θ=R ).
lutive MCMC, ∆:=(cid:8) a,b∈(0,1)2 :a<b(cid:9) is a set of +
However, the AutoStep MCMC method described pre-
acceptance ratio thresholds (a,b), and Θ is the set of
viously has the correct stationary distribution for more
tuning parameters θ for the involutions. We assume
general parameter spaces Θ (see Proposition 4.2).
Θ is a standard Borel space, such that S is standard
Borel as well. Let f : S → S denote the augmented
3.1 Step size selection
involution f : for a point s=(x,z,a,b,θ)∈S, define
θ
We now focus on the design of the tuning refreshment
f(s)=(f (x,z),a,b,θ) J(s)=|∇f (x,z)|.
θ θ
distribution η when θ is a step size parameter with
Notethatf isitselfaninvolutiononS. Wethendefine Θ=R +. Intuitively, a good choice of θ should yield a
the augmented target density proposal for which the acceptance ratio exp(ℓ(x,z,θ))
of the original involutive method is not too close to
π¯(s)=2π(x)·m(z)·1 ∆(a,b)·η(θ |x,z,a,b), either 0 (θ is too large) or 1 (θ is too small). Critically,
this should also be true for exp(−ℓ(x,z,θ)), which is
where we assume that π(x) and m(z) are with respect
the acceptance ratio in the reverse direction
to the Lebesgue measure on X ×Z, that both m and
η admit i.i.d. draws, and that there exists a σ-finite
ℓ(f (x,z),θ)=−ℓ(x,z,θ).
θ
measure dθ on Θ such that for all x ∈ X, z ∈ Z and
(a,b) ∈ ∆, η(· | x,z,a,b) is a density with respect to Toavoidsettingarbitraryfixedboundsonℓ,weusethe
dθ, but otherwise may depend arbitrarily on x,z,a,b. random a,b as thresholds and ensure that |ℓ| roughly
The X-marginal of π¯ is the target of interest, π. See tries to fall in the range (|log(b)|,|log(a)|). We also
concurrent work of Bou-Rabee et al. (2024b) and Bou- randomize(orjitter)thestepsizeθtoavoidpotentially
Rabee et al. (2024a) for a related augmentation. fragile deterministic choices.
Given this setup, starting from x ∈ X, AutoStep More precisely, given a fixed initial step size θ > 0
t 0
MCMC (Algorithm 1) consists of the following steps: and jitter variance σ2 ≥0, we propose simulating theAutoStep: Locally adaptive involutive MCMC
Algorithm 2 Step size selector µ
Require: state x,z,a,b, initial step size θ .
0
1: θ ←θ
0
2: ℓ←ℓ(x,z,θ)
3: v ←1{|ℓ|<|logb|}−1{|ℓ|>|loga|}
4: j =0 ▷ number of doublings/halvings
5: if v =0 then
6: return j
7: end if
8: while true do
9: j ←j+v
10: θ ←θ 0·2j Figure 1: Symmetric acceptance thresholds help the
11: ℓ←ℓ(x,z,θ) sampler explore modal regions. Here, autostep RWMH
12: if v =1 and |ℓ|≥|logb| then starts at the leftmost dot and proposes moving to the
13: return j−1 ▷ Require final halving right. The shaded area indicates the region where the
14: else if v =−1 and |ℓ|≤|loga| then stepsizeselectorstopsdoubling. Ifoneusesasymmetric
15: return j thresholds (top row) (Biron-Lattes et al., 2024), since
16: end if ℓ>logb,AutoStepRWMHcontinuesdoublingthestep
17: end while size until ℓ<log(b), which often leads to overshooting
themode. Ourproposedsymmetricthresholds(bottom
row)basedon|ℓ|doublethestepsizeuntil|ℓ|<|log(b)|,
step size θ from the following conditional refreshment
enabling termination at higher density values.
distribution η(dθ|x,z,a,b):
δ ∼N(µ(x,z,a,b),σ2) θ =θ ×2δ, (3)
0
the sampler to effectively visit higher density regions
where µ is the step size selector function
(see Figure 1 for an illustration). Second, we include
µ(x,z,a,b)= thestepsizeθ asanaugmentationofourstatevariable,
(cid:40) min{j ∈Z+ :|ℓ(x,z,θ 2j)|≥|logb|}−1, |ℓ |<|logb| whichsimplifiestheoreticalanalyses(comparetheproof
0 0 of Proposition 4.2 in Appendix A with the proof of
max{j ∈Z− :|ℓ(x,z,θ 2j)|≤|loga|}, |ℓ |>|loga|
0 0
0, otherwise, Theorem 3.4 in Biron-Lattes et al. (2024)) and allows
for randomized step size proposals that avoid the hard
and ℓ = ℓ(x,z,θ ). Therefore, when σ2 > 0,
0 0 reversibility checks of autoMALA.
η(θ|x,z,a,b) is a density with respect to the Lebesgue
measure on R ; but we also allow σ2 = 0, which for-
+ 3.2 Round-based tuning
mally indicates that δ =µ(x,z,a,b) almost surely, in
which case η(θ|x,z,a,b) is a density with respect to
TheAutoStepMCMCmethodhastwofreeparameters:
the counting measure on {θ 0×2j :j ∈Z}. theinitialstepsizeθ andthestepsizejitterσ2. While
0
The pseudocode for computing µ(x,z,a,b) is given the method should be somewhat insensitive to θ 0 and
in Algorithm 2. If the initial step size θ yields an smallvaluesofσ2,itisstillhelpfultotunetheseparam-
0
acceptable |ℓ |, the function simply returns j = 0. eterstominimizethenumberofdoubling/halvingsteps.
0
If the initial step size is too large (|ℓ | > |loga|), j Furthermore, many involutive MCMC methods—e.g.,
0
is decreased until |ℓ | ≤ |loga|. And if the initial RWMH, MALA, and HMC—have a preconditioner, or
0
step size is too small (|ℓ | < |logb|), j is increased mass matrix M that needs to be tuned.
0
until |ℓ |>|logb|, and then finally decreased by 1 to
0 Inthiswork,weusearound-basedproceduretotuneθ ,
0
avoid poor proposals. Note that this function does σ2,andM (Algorithm3). Eachroundcorrespondstoa
not guarantee that |logb|≤|ℓ|≤|loga| precisely, but
blockofiterationsduringwhichthesethreeparameters
finds a good trade-off by approximately targeting that
are held constant. We use θ =1, M =I, and σ =0.5
0
range while avoiding the need for expensive methods
for the first round. At the end of each round, we
to find values exactly within the bounds. use: (1)theaverageselectedstepsizeθ 2µ(x,z,a,b) from
0
The step size refreshment was inspired by that of au- the current round as the new θ 0; (2) the inverse of a
toMALA (Biron-Lattes et al., 2024), but has two im- geometric mix of the diagonal of the sample covariance
portantdifferences. First, weusesymmetricthresholds matrix and the identity for M; and (3) half the mean
that check |logb| ≤ |ℓ| ≤ |loga|, instead of checking absolute difference d=|µ(x′,z′,a,b)−µ(x,z,a,b)| of
logb ≤ ℓ ≤ loga. This is crucial for ensuring irre- theforwardandreversestepsizeselectionsforσ,which
ducibility of the method (see Section 4), and enables controls the ratio of step size proposals
η(θ|x′,z′,a,b).
η(θ|x,z,a,b)Algorithm 3 Round-based AutoStep MCMC Rosenthal (2004)): intuitively, the chain has a positive
Require: Initial state x , number of rounds R, target probability of eventually visiting any measurable A⊆
0
π, auxiliary distribution m, step size distribution X withπ(A)>0,anditdoesnotvisitvarioussetsina
η, involutions {f } . repeatingpattern. Wewilldemonstrateπ-irreducibility
θ θ∈Θ
1: θ ←1 and aperiodicity simultaneously by showing that the
0
2: m←N(0,I ) X-component of the chain can reach any measurable
d
3: σ ←0.5 set A ⊂ X in a single step with positive probability
4: for r in 1, 2, ..., R do (one-stepirreducibility). Thefirstassumptionneededis
5: T ←2r ▷ Number of iterations thatforanyfixedθ ∈Θ,theX-marginalkernelP θ(x,·)
6: η ← (Eq. (3)) based on (θ ,σ) of the original involutive MCMC algorithm given by
0
7: for t in 1, 2, ..., T do Eqs. (1) and (2) can do so as well.
8: ξ ∼ 1 3δ 0+ 1 3δ 1+ 1 3Beta(1,1) Assumption 4.3. For all x∈X, θ ∈Θ, and A⊆X
▷ Random mixing of the preconditioner such that π(A)>0, the X-marginal kernel P of invo-
θ
9: Σ(cid:98) i− ,i1/2 ←ξΣ− i,i1/2+(1−ξ) lutive MCMC (Eqs. (1) and (2)) satisfies P θ(x,A)>0.
10: m←N(0,Σ(cid:98)−1)
The second assumption needed is that there is a non-
▷ see definition of d ,µ in Section 3.2
t t null set of parameters θ ∈Θ that can be selected and
11: x ,µ ,d ←AutoStep(x ,π,m,η,{f } )
t t t 0 θ θ∈Θ resultinanacceptedmovefromanypointx,z ∈X×Z
12: end for
13: θ
0
←θ 0T−1(cid:80)T t=12µt Win eth ene co or dig ein tha il sa uu sg im ngen tt he ed ps op sa itc ie vio tf yi on fvo thlu eti fv ue ncM tiC onMC.
14: σ
←0.5T−1(cid:80)T
d
t=1 t
15: x ←x γ(x,z,θ)=
0 T
16:
Σ←diag(cid:18)(cid:104) V(cid:100)ar[x( tj)]T t=1(cid:105)d (cid:19) (cid:90)
min{η(θ |x,z,a,b),η(θ |f (x,z),a,b)}1 (d(a,b)).
j=1 θ ∆
17: end for
18: return {x }T
t t=1
Assumption 4.4. There exists a B ⊆ Θ such that
(cid:82)
dθ >0 and for all x∈X, m-a.e. z ∈Z, and θ ∈B,
B
4 THEORETICAL ANALYSIS
γ(x,z,θ)>0.
The marginal sequence x on X of AutoStep MCMC
t Theseassumptionsyieldthedesiredresult,whichholds
is itself a Markov chain because each step redraws
for general parameter spaces Θ and distributions η.
z ,a ,b ,θ independently of their previous value con-
t t t t
ditioned on x . In this section we establish various Proposition 4.5. If both Assumptions 4.3 and 4.4
t
properties of the X-marginal Markov chain. hold, then AutoStep MCMC is one-step irreducible,
and hence irreducible and aperiodic.
4.1 Invariance
We now apply Proposition 4.5 to the case where θ is
a step size parameter and we use η from Section 3.1.
First, we show that AutoStep MCMC is π¯-invariant on
In this setting, Assumption 4.4 can be simplified to
the augmented space S, and hence π-invariant on X.
conditions on the target π and jitter σ2. Recall that
The result is a straightforward application of Tierney
σ2 =0 formally indicates that η is a Dirac delta.
(1998, Theorem 2) on the augmented space S. Note
that while this work focuses on step size parameters Corollary 4.6. Suppose Θ=(0,∞), Assumption 4.3
θ ∈ R , Proposition 4.2 below holds for general pa- holds, and we use η from Section 3.1. Then, AutoStep
+
rameter spaces Θ and tuning refreshment distributions MCMC is irreducible and aperiodic if σ2 >0, or if for
η(dθ|x,z,a,b). all x∈X and m-a.e. z ∈Z, |ℓ(x,z,θ 0)|∈/ {0,∞}.
Assumption4.1. Foreachθ ∈Θ,f isacontinuously
θ We show in Lemmas A.1 and A.2 that Assumption 4.3
differentiable involution.
holds for both RWMH and MALA under weak con-
Proposition 4.2. Under Assumption 4.1, AutoStep ditions, and hence the irreducibility and aperiodicity
MCMC is π-invariant, and hence the X-marginal is of AutoStep RWMH and MALA follows from either
π-invariant. σ2 >0 or |ℓ(x,z,θ )|∈/ {0,∞}.
0
4.2 Irreducibility and aperiodicity 4.3 Step size selector termination
Next, we establish that the X-marginal of AutoStep We now establish that under mild conditions, the step
MCMC is π-irreducible and aperiodic (see Roberts and size selector function µ can be computed in finite-time.AutoStep: Locally adaptive involutive MCMC
For starting state s = (x,z,a,b) and initial step size 5 EXPERIMENTS
θ >0, let τ(s,θ )≥1 be the number of iterations of
0 0
the while loop in Algorithm 2. In this section we present an empirical evaluation of
three AutoStep MCMC variants: RWMH, MALA, and
Assumption 4.7. π(x) and m(z) are continuous, and
HMC.Wefirstcomparetheperformanceofthesemeth-
both lim π(x)=0 and lim m(z)=0.
|x|→∞ |z|→∞
ods to their corresponding standard fixed-step-size
Assumption 4.8. For any (x,z)∈X ×Z, we have
methods. We then examine the effects of random-
ization andsymmetric thresholdson step sizeselection,
lim f (x,z)=(x,z) lim |∇f (x,z)|=1
θ θ
θ→0+ θ→0+ in comparison to previously used deterministic step
lim ∥f (x,z)∥=+∞ limsup|∇f (x,z)|<+∞. sizes without symmetric thresholds. Finally, we inves-
θ θ
θ→∞ θ→∞ tigate the effect of tuning the step size selection noise
on performance and stability.
Proposition 4.9. Let θ > 0 and suppose Assump-
0
tions 4.7 and 4.8 hold. Then τ(s,θ 0)<∞, π¯-a.s. Throughout, we measure the efficiency of each sampler
in terms of effective sample size (ESS) (Flegal et al.,
4.4 Energy jump distance 2008) per unit cost. The ESS we report (minESS) is
the minimum of the standard estimate based on auto-
For s = (x,z,a,b,θ) ∼ π, s′ = f(s) =(x′,z′,a′,b′,θ′),
correlation (Gelman et al., 2013, p. 286-287) and the
and U ∼Unif[0,1], define the energy jump distance
estimate with known first and second moments (Galin
(cid:20) π(s′) (cid:21) L Jones and Neath, 2006, p. 1539-1541) across each
D =|ℓ(x,z,θ)|1 U ≤ J(s) , target dimension, with moments obtained via a sepa-
π(s)
rate long run of parallel tempering with Pigeons.jl
whichweusetoencodethechangeinlog(π(x)m(z))af- (Surjanovic et al., 2023). Because we compare gradient
ter one iteration of AutoStep MCMC. Proposition 4.10 andnon-gradientmethodscodedinmultiplelanguages,
shows that any involutive MCMC method—both tra- weuseanimplementation-independentmeasureofcost,
ditional and AutoStep methods—have an expected N ℓ+αN g,whereN ℓ isthenumberoflog-targetdensity
energy jump distance bounded above by a simple ex- function calls, N g is the number of log-target gradient
pression via the tuning parameter proposal density calls, and α is the ratio of the average time for a gra-
ratio η, dient and density call, estimated using 1,000 random
samplesfromeachtarget. SeeAppendixBfortheαes-
η = esssup
η(θ |f θ(x,z),a,b)
(under π).
timates. Unless otherwise specified, all results include
η(θ |x,z,a,b) 30 independent trials, algorithms are run until the re-
x,z,a,b,θ
ported minESS exceeds 100, and we use round-based
adaptation for all parameters in AutoStep MCMC.
Proposition 4.10. Under Assumption 4.1,
ED ≤2ηmax(cid:8) e−1,ηlogη(cid:9) . 5.1 Comparison with existing methods
We first assess the performance of the three vari-
In particular, for traditional involutive MCMC with
ants of AutoStep MCMC with the corresponding
a fixed parameter θ = θ , or for AutoStep MCMC
0 standard fixed-step-size methods by comparing their
with σ2 =0, or AutoStep MCMC with a fixed-width
mean energy jump distance (D from Section 4.4)
uniform distribution for η, we have that η ≤1, so
and minESS per cost on a set of benchmark mod-
ED ≤2e−1 ≈0.736. els. For the fixed step size methods, we considered
step sizes {0.1,0.25,1.0,4.0,10.0}× the tuned θ from
0
The step size selector presented in Section 3.1 is a AutoStep RWMH, and used round-based adaptive tun-
computationally efficient method to make |ℓ(x,z,θ)| ing for the preconditioner. The benchmarks include
fall roughly in the range (|logb|,|loga|), where a,b∼ two synthetic distributions—Neal’s funnel and the ba-
Unif(∆). Therefore, as a heuristic, we expect nana distribution—and two real Bayesian posteriors—
logistic regression with a horseshoe prior (Carvalho
0.5=E|logb|≲E|ℓ(x,z,θ)|≲E|loga|=1.5,
et al., 2009) on the sonar dataset (Sejnowski and Gor-
man,1988),andanmRNAtransfectionmodel(Ballnus
with departures from exactness arising due to the dis-
et al., 2017).
crete doubling/halving procedure (as opposed to an
exactrootfinder). Inotherwords,thestepsizeselector Fig. 2 shows the results for RWMH and AutoStep
in this work creates proposals with mean energy jump RWMH; see Appendix B for the same set of results
distance roughly targeting the maximum ≈0.736 for a for MALA and HMC. This figure demonstrates that
broad class of involutive MCMC methods. the AutoStep method provides a consistently high en-Figure2: Averageenergyjumpdistance(left)andminESSperunitcost(right)forAutoStepRWMHandstandard
RWMH with the fixed step size set to {0.1,0.25,1.0,4.0,10.0}× the tuned θ from AutoStep RWMH.
0
ergy jump distance and minESS per unit cost across observation is that across a wide range of targets, the
all benchmarks; it is competitive with the manually jitter standard deviation tends to converge to ≈ 0.1;
tunedmethodinallcases. NotethatAutoStepRWMH we leave a deeper investigation of this observation as
requires no manual tuning and thus presents favorable an open question.
performance with a minimum of user tuning effort.
5.2 Effect of jitter and symmetric thresholds 5.4 Effect of round-based jitter tuning
Figure 3 presents the effect of jitter and the use of Figure 5 shows an investigation of the effect of fixed
symmetric step size selector thresholds for AutoStep (σ =0,0.1,0.5,2) and tuned jitter standard deviation
HMC, MALA and RWMH. Each sampler is tested in AutoStep MCMC. It is evident from Figure 5 that
with four different configurations: the original setting the choice of jitter distribution significantly impacts
(with asymmetric thresholds used in Biron-Lattes et al. the efficiency of the sampler. Lower σ (0.1 and 0.5)
(2024)), with symmetric thresholds, with auto-tuned generally yield higher minESS per cost, particularly
jitter, and with both symmetric thresholds and auto- for more challenging targets. The auto tuned jitter
tuned jitter. Simulations were conducted on several seems to prevent catastrophic failures, while any other
synthetic targets including the banana distribution, fixed value of σ has cases where performance suffers
Neal’s funnel distribution, and a multivariate Gaussian dramatically. Based on these results, the tuned jitter
distribution (Appendix B), each with different dimen- is recommended as a default, as it provides reliable
sions and scales. The plots demonstrate consistent performance without the need for manual tuning.
performance across all sampler variants, with occa-
sional improvements observed when using symmetric
thresholds versus asymmetric thresholds (particularly
6 CONCLUSION
for RWMH). Based on the consistent performance but
improved theoretical guarantees, it is generally recom-
mended to use AutoStep MCMC with both symmetric InthispaperwepresentedAutoStepMCMC,ageneral
thresholds and auto-tuned jitter. locally-adaptive step size selection method for involu-
tive MCMC. We proved that AutoStep MCMC kernels
5.3 Stability of round-based jitter tuning are π-invariant, irreducible, and aperiodic under mild
conditions. We also provided upper bounds on the
We examine the convergence of the jitter standard mean energy jump distance for a general class of invo-
deviation σ during round-based tuning. The three lutive MCMC methods. We demonstrated empirically
plots in Figure 4 show the evolution of the jitter stan- that AutoStep MCMC is competitive with well-tuned
dard deviation across sampling rounds for the three standard involutive methods, and that the proposed
AutoStep MCMC algorithms applied to five different methods are stable and reliable. One promising direc-
targets. Figure 4 shows that in early tuning rounds tionforfutureworkisarigoroustheoreticalanalysisof
the jitter standard deviation varies due to the use of thesamplingefficiencyofAutoStepMCMCintermsof
relatively small sample sizes, but eventually stabilizes expected squared jump distance, asymptotic variance,
and converges as the rounds progress. An interesting and total variation distance.AutoStep: Locally adaptive involutive MCMC
(a) AutoStep HMC (b) AutoStep MALA (c) AutoStep RWMH
Figure 3: Comparison of different versions of AutoStep MCMC on the funnel, banana, and Gaussian distributions
with varying dimensions and scales, denoted “model(dimension, scale)”. We use “jitter” to indicate that the
sampler uses an auto-tuned jitter, and “symmetric” to denote that the sampler uses symmetric thresholds.
(a) AutoStep HMC (b) AutoStep MALA (c) AutoStep RWMH
Figure 4: Convergence of the jitter standard deviation σ across sampling rounds for AutoStep MCMC.
(a) AutoStep HMC (b) AutoStep MALA (c) AutoStep RWMH
Figure 5: Comparison of tuned and fixed jitter. For the banana and funnel distributions, we set dim = 2 and
scale = 1.Acknowledgements Duane, S., Kennedy, A., Pendleton, B. J., and Roweth,
D. (1987). Hybrid Monte Carlo. Physics Letters B,
ABC and TC acknowledge the support of an NSERC
195(2):216–222.
Discovery Grant. TL acknowledges the support of
Flegal, J. M., Haran, M., and Jones, G. L. (2008).
the UBC Advanced Machine Learning Training Net-
Markov chain Monte Carlo: Can we trust the third
work. NSacknowledgesthesupportofaVanierCanada
significant figure? Statistical Science, 23(2).
Graduate Scholarship. We additionally acknowledge
use of the ARC Sockeye computing platform from the Galin L Jones, Murali Haran, B. S. C. and Neath, R.
University of British Columbia. (2006).Fixed-widthoutputanalysisforMarkovchain
Monte Carlo. Journal of the American Statistical
References Association, 101(476):1537–1547.
Ge, H., Xu, K., and Ghahramani, Z. (2018). Turing:
Andrieu,C.,Lee,A.,andLivingstone,S.(2020). Agen-
A language for flexible probabilistic inference. In
eral perspective on the Metropolis–Hastings kernel.
International Conference on Artificial Intelligence
arXiv:2012.14881.
and Statistics, pages 1682–1690.
Andrieu, C. and Thoms, J. (2008). A tutorial on adap-
Gelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari,
tive MCMC. Statistics and Computing, 18(4):343–
A., and Rubin, D. (2013). Bayesian Data Analysis.
373.
CRC Press, 3rd edition.
Atchad´e, Y. F. (2006). An adaptive version for the
Girolami, M. and Calderhead, B. (2011). Riemann
Metropolis adjusted Langevin algorithm with a trun-
manifold Langevin and Hamiltonian Monte Carlo
cated drift. Methodology and Computing in Applied
methods. Journal of the Royal Statistical Society:
Probability, 8(2):235–254.
Series B (Statistical Methodology), 73(2):123–214.
Ballnus, B., Hug, S., Hatz, K., Go¨rlitz, L., Hasenauer,
Green, P. and Mira, A. (2001). Delayed rejection in
J., and Theis, F. J. (2017). Comprehensive bench-
reversible jump Metropolis–Hastings. Biometrika,
marking of Markov chain Monte Carlo methods for
88(4):1035–1053.
dynamical systems. BMC Systems Biology.
Haario,H.,Saksman,E.,andTamminen,J.(2001). An
B´elisle, C., Romeijn, E., and Smith, R. (1993). Hit-
adaptive Metropolis algorithm. Bernoulli, 7(2):223–
and-run algorithms for generating multivariate dis-
242.
tributions. Mathematics of Operations Research,
Hastings, W. K. (1970). Monte Carlo sampling meth-
18(2):255–266.
ods using Markov chains and their applications.
Biron-Lattes, M., Surjanovic, N., Syed, S., Campbell, Biometrika, 57(1):97–109.
T., and Bouchard-Cˆot´e, A. (2024). autoMALA: Lo-
Hoffman, M. D. and Gelman, A. (2014). The No-U-
cally adaptive Metropolis-adjusted Langevin algo-
Turn Sampler: Adaptively setting path lengths in
rithm. In International Conference on Artificial
Hamiltonian Monte Carlo. The Journal of Machine
Intelligence and Statistics, volume 238, pages 4600–
Learning Research, 15(1):1593–1623.
4608.
Kleppe, T. S. (2016). Adaptive step size selection for
Bou-Rabee, N., Carpenter, B., Kleppe, T. S., and
Hessian-based manifold Langevin samplers. Scandi-
Marsden, M. (2024a). Incorporating local step-size
navian Journal of Statistics, 43(3):788–805.
adaptivity into the No-U-Turn Sampler using Gibbs
Livingstone, S. (2021). Geometric ergodicity of the
self tuning. arXiv: 2408.08259.
random walk Metropolis with position-dependent
Bou-Rabee, N., Carpenter, B., and Marsden, M.
proposal covariance. Mathematics, 9(4).
(2024b). GIST:Gibbsself-tuningforlocallyadaptive
Maire, F. and Vandekerkhove, P. (2022). Markov ker-
Hamiltonian Monte Carlo. arXiv:2404.15253.
nelslocalaggregationfornoisevanishingdistribution
Carvalho,C.M.,Polson,N.G.,andScott,J.G.(2009). sampling. SIAM Journal on Mathematics of Data
Handlingsparsityviathehorseshoe. InInternational Science, 4(4):1293–1319.
Conference on Artificial Intelligence and Statistics,
Marshall, T. and Roberts, G. (2012). An adaptive
volume 5, pages 73–80.
approach to Langevin MCMC. Statistics and Com-
Chimisov, C., Latuszynski, K., and Roberts, G. (2018). puting, 22.
Air Markov chain Monte Carlo. arXiv:1801.09309.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N.,
Coullon,J.,South,L.,andNemeth,C.(2023). Efficient Teller, A. H., and Teller, E. (1953). Equation of
and generalizable tuning strategies for stochastic state calculations by fast computing machines. The
gradientMCMC. Statistics and Computing,33(3):66. Journal of Chemical Physics, 21(6):1087–1092.AutoStep: Locally adaptive involutive MCMC
Modi, C., Barnett, A., and Carpenter, B. (2023). De-
layed rejection Hamiltonian Monte Carlo for sam-
pling multiscale distributions. Bayesian Analysis,
pages 1–28.
Neal, R. M. (1996). Bayesian Learning for Neural
Networks, volume 118 of Lecture Notes in Statistics.
Springer New York, New York, NY, 1 edition.
Neal, R. M. (2003). Slice sampling. The Annals of
Statistics, 31(3):705–767.
Neklyudov, K., Shvechikov, P., and Vetrov, D. (2018).
Metropolis-Hastings view on variational inference
and adversarial training. arXiv:1810.07151.
Neklyudov, K., Welling, M., Egorov, E., and Vetrov,
D. (2020). Involutive MCMC: A unifying framework.
In International Conference on Machine Learning.
Nishimura, A. and Dunson, D. (2016). Variable
length trajectory compressible hybrid Monte Carlo.
arXiv:1604.00889.
Polson, N. G. and Scott, J. G. (2012). On the half-
Cauchy prior for a global scale parameter. Bayesian
Analysis, 7(4):887–902.
Roberts, G., Gelman, A., and Gilks, W. (1997). Weak
convergence and optimal scaling of random walk
Metropolisalgorithms. AnnalsofAppliedProbability,
7(1):110–120.
Roberts, G. and Rosenthal, J. (2004). General state
space Markov chains and MCMC algorithms. Proba-
bility Surveys, 1:20–71.
Roberts, G. O. and Rosenthal, J. S. (1998). Optimal
scaling of discrete approximations to Langevin diffu-
sions. Journal of the Royal Statistical Society. Series
B (Statistical Methodology), 60(1):255–268.
Rossky, P. J., Doll, J. D., and Friedman, H. L. (1978).
BrowniandynamicsassmartMonteCarlosimulation.
The Journal of Chemical Physics, 69(10):4628–4633.
Sejnowski, T. and Gorman, R. (1988). Connection-
ist Bench (Sonar, Mines vs. Rocks). UCI Machine
Learning Repository.
Surjanovic, N., Biron-Lattes, M., Tiede, P., Syed, S.,
Campbell, T., and Bouchard-Cˆot´e, A. (2023). Pi-
geons.jl: Distributed sampling from intractable dis-
tributions. arXiv:2308.09769.
Tierney, L. (1998). A note on Metropolis–Hastings ker-
nelsforgeneralstatespaces. TheAnnalsofStatistics,
8(1):1–9.
Tierney, L. and Mira, A. (1999). Some adaptive Monte
Carlo methods for Bayesian inference. Statistics in
Medicine, 18:2507–2515.
Turok, G., Modi, C., and Carpenter, B. (2024).
Sampling from multiscale densities with delayed
rejection generalized Hamiltonian Monte Carlo.
arXiv:2406.02741.Supplementary Materials
A Proofs
Proof of Proposition 4.2. The auxiliary refreshment and tuning parameter refreshment steps in AutoStep MCMC
(Steps 1. and 2.) resample (z,a,b,θ) jointly from their conditional distribution given x under π. This move is
well-known to be π¯-invariant, and so it remains only to show that the Metropolis-corrected involutive proposal
(Steps 3. and 4.) is π¯-invariant. The kernel for the proposal on the augmented space S is
Q(s,ds′)=δ (ds′),
f(s)
and the acceptance probability α:S2 →R is given by
+
(cid:26) π(s′) (cid:27)
α(s,s′)=min 1, J(s) .
π(s)
In the notation of Tierney (1998, Theorem 2), define the measure µ(ds,ds′)=π(ds)δ (ds′); because f is an
f(s)
involution, we have that the symmetric set R and density ratio r :R→R are given by
+
π(ds)δ (ds′)
R={(s,s′)∈S2 :f(s)=s′, π(s)>0, π(s′)>0}, r(s,s′)= f(s) .
π(ds′)δ (ds)
f(s′)
Note that condition (i) of Tierney (1998, Theorem 2) holds by definition of R and α. Suppose for the moment
that r(s,s′)= π(s)J(s′); then condition (ii)—and hence π-invariance—holds because
π(s′)
(cid:26) π(s′) (cid:27) π(s) (cid:26) π(s) (cid:27)
α(s,s′)r(s,s′)=min 1, J(s) J(s′)=min J(s′),J(s′)J(s) =α(s′,s),
π(s) π(s′) π(s′)
which follows because J(s)J(s′)=1 µ-a.e. on R. To demonstrate that r(s,s′) has the required form, consider a
test function g :S2 →R:
(cid:90) (cid:90)
g(s,s′)π(ds)δ (ds′)= g(s,f(s))π(ds)
f(s)
(cid:90)
= g((x,z,a,b,θ),(f (x,z),a,b,θ))π(x,z,a,b,θ)dxdzd(a,b)dθ.
θ
Since f is a continuously differentiable involution, we can transform variables x′,z′ = f (x,z) by including a
θ θ
Jacobian term J(s)=|∇f (x,z)| in the integrand and by noting x,z =f (x′,z′):
θ θ
(cid:90)
= g((f (x′,z′),a,b,θ),(x′,z′,a,b,θ))π(f (x′,z′),a,b,θ)|∇f (x′,z′)|dx′dz′d(a,b)dθ
θ θ θ
(cid:90)
= g(f(s′),s′)π(f(s′))J(s′)ds′
(cid:90) π(f(s′))
= g(f(s′),s′) J(s′)π(ds′)
π(s′)
(cid:90) π(s)
= g(s,s′) J(s′)π(ds′)δ (ds).
π(s′) f(s′)
Examining the first and last integral expressions, the density ratio has the form
π(ds)δ (ds′) π(s)
r(s,s′)= f(s) = J(s′).
π(ds′)δ (ds) π(s′)
f(s′)AutoStep: Locally adaptive involutive MCMC
Proof of Proposition 4.5. Let K(x,·) denote the Markov kernel for the X-marginal process of AutoStep MCMC.
Since for u,v ≥0, min{1,uv}≥min{1,u}min{1,v}, we have that for s=(x,z,a,b,θ),
(cid:26) π(f(s)) (cid:27) (cid:110) (cid:111) (cid:26) η(θ |f (x,z),a,b)(cid:27)
min 1, J(s) ≥min 1,eℓ(x,z,θ) min 1, θ .
π(s) η(θ |x,z,a,b)
Therefore
(cid:90) (cid:110) (cid:111) (cid:26) η(θ |f (x,z),a,b)(cid:27)
K(x,A)≥ 1[f (x,z)∈A×Z]min 1,eℓ(x,z,θ) min 1, θ η(dθ |x,z,a,b)m(dz)1 (d(a,b))
θ η(θ |x,z,a,b) ∆
(cid:90) (cid:110) (cid:111)
= 1[f (x,z)∈A×Z]min 1,eℓ(x,z,θ) γ(x,z,θ)m(dz)dθ,
θ
where
(cid:90)
γ(x,z,θ)= min{η(θ |x,z,a,b),η(θ |f (x,z),a,b)}1 (d(a,b)).
θ ∆
(cid:82)
By Assumption 4.4, for all x∈X, m-a.e. z ∈Z, and for all θ ∈B where dθ >0, γ(x,z,θ)>0. Therefore
B
(cid:90) (cid:110) (cid:111)
1[f (x,z)∈A×Z]min 1,eℓ(x,z,θ) γ(x,z,θ)m(dz)1[θ ∈B]dθ >0
θ
(cid:90) (cid:110) (cid:111)
⇐⇒ 1[f (x,z)∈A×Z]min 1,eℓ(x,z,θ) m(dz)1[θ ∈B]dθ >0.
θ
The proof concludes by noting that
(cid:90) (cid:110) (cid:111) (cid:90)
1[f (x,z)∈A×Z]min 1,eℓ(x,z,θ) m(dz)1[θ ∈B]dθ = P (x,A)1[θ ∈B]dθ,
θ θ
where P (x,A) is the one-step probability of transitioning into A from x for the original involutive chain with
θ
parameter θ, and then by applying Assumption 4.3.
Proof of Corollary 4.6. The proof involves verifying Assumption 4.4. When σ2 >0, for all x∈X, z ∈Z, η is a
normaldistributionandhasfullsupport,andsoγ(x,z,θ)>0. Forthelattercase,withoutlossofgeneralitywecan
assume σ2 =0 (since if σ2 >0, the previous case applies). Therefore, dθ is concentrated on {θ =θ ×2j :j ∈Z}.
0
Consider setting B ={θ }. Assumption 4.4 holds if for all x∈X and m-a.e. z ∈Z,
0
(cid:90)
1[µ(x,z,a,b)=θ =µ(f (x,z),a,b)]1 (d(a,b))>0.
0 θ0 ∆
That is, if there is a nonzero probability of choosing the default parameter θ at any point (x,z). Note that
0
µ(x,z,a,b)=θ =µ(f (x,z),a,b)
0 θ0
⇐⇒ log(a)<ℓ(x,z,θ )<log(b) or log(a)<−ℓ(x,z,θ )<log(b).
0 0
By assumption, for all x ∈ X and m-a.e. z ∈ Z, we have ℓ(x,z,θ ) ∈/ {−∞,0,∞}. If ℓ(x,z,θ ) > 0, then
0 0
when a < exp(−ℓ(x,z,θ )) < b we have the condition hold. This has positive measure under 1 (d(a,b)).
0 ∆
If ℓ(x,z,θ ) < 0, then when a < exp(ℓ(x,z,θ )) < b, the above condition holds. This set also has positive
0 0
measure.
Lemma A.1. For the AutoStep RWMH kernel with any fixed θ >0, x∈X, and A⊂X with π(A)>0, we have
P (x,A)>0, provided that π(x)>0 for all x∈X.
θ
Proof of Lemma A.1. Fix θ > 0, A ⊂ X with π(A) > 0 and x ∈ X. Because π ≪ λ, we have λ(A) > 0. By
translationpropertiesoftheLebesguemeasure,foranyx∈Rd,λ(A)=λ(A−x)>0,whereA−x={x˜−x:x˜∈A}.
Here, (x′,z′)=f (x,z)=(x+z,−z) and so |∇f (x,z)|=1. Also, since z ∼m where m=N(0,I), we have
θ θ
(cid:18) π(x′)(cid:19)
ℓ(x,z,θ)=log .
π(x)Then,
(cid:90) (cid:26) π(x+z)(cid:27)
P (x,A)≥ 1[z ∈A−x]min 1, m(z)λ(dz)>0,
θ π(x)
because for all x,z we have λ(A−x)>0, m(z)>0, and min{1,π(x+z)/π(x)}>0.
Lemma A.2. For the AutoStep MALA kernel with any fixed θ >0, x∈X, differentiable π, positive definite M,
and A⊂X with π(A)>0, we have P (x,A)>0, provided that π(x)>0 for all x∈X.
θ
Proof of Lemma A.2. Fix θ > 0, A ⊂ X with π(A) > 0 and x ∈ X. Because π ≪ λ, we have λ(A) > 0. As in
Biron-Lattes et al. (2024), we combine the updates on (x,z) into one step, so that f (x,z)=(x′(θ),z′(θ)), where
θ
θ2 (cid:18) θ θ (cid:19)
x′(θ)=x+θM−1z+ M−1∇logγ(x), z′(θ)=− z+ ∇logγ(x)+ ∇logγ(x′(θ)) .
2 2 2
Now, x′(θ)∈A if
(cid:26) (cid:27)
M(x˜−x) θ
z ∈A := − ∇logγ(x):x˜∈A .
x θ 2
By translation and scaling properties of the Lebesgue measure, for any x∈Rd, λ(A )>0. It is a standard result
x
that the leapfrog integrator satisfies |∇f (x,z)|=1. We have
θ
(cid:18) π(x′)m(z′)(cid:19)
ℓ(x,z,θ)=log .
π(x)m(z)
Then,
(cid:90) (cid:26) π(x′)m(z′)(cid:27)
P (x,A)≥ 1[z ∈A ]min 1, m(z)λ(dz)>0.
θ x π(x)m(z)
because for all x,z we have λ(A )>0, m(z)>0, and the acceptance ratio is positive.
x
Proof of Proposition 4.9. We generalize the step size termination proof of Theorem 3.3 by Biron-Lattes et al.
(2024). We consider two cases: v =1 on Line 3 of Algorithm 2, and v =−1. Fix any (x,z)∈X ×Z and θ >0.
0
We assume that π(x)>0 and m(z)>0, which holds π¯-almost surely.
If v =−1, we have |ℓ |>|log(a)|, which implies either ℓ <log(a) or −ℓ <log(a). This means that the initial
0 0 0
step size θ is too large in either the forward or the reverse direction. In this scenario, we start the halving
0
procedure. Now,
(cid:18) π(x′(θ))m(z′(θ)) (cid:19)
ℓ(x,z,θ)=log |∇f (x,z)| ,
π(x)m(z) θ
where (x′(θ),z′(θ))=f (x,z). The step size selection procedure terminates as soon as |ℓ(θ)|≤|log(a)| for some
θ
θ =θ ×2j, j ∈Z−1. By Assumptions 4.7 and 4.8, lim ℓ(x,z,θ)=0, and so there is a 0<θ′ <θ with the
0 θ→0+ 0
property that |ℓ(θ)|≤|log(a)| for all 0<θ ≤θ′. Provided that |log(a)|>0, we have τ(s,θ )<∞ in this case.
0
In the case where v = 1, we have |ℓ | < |log(b)|, which implies ℓ > log(b) and −ℓ > log(b). This means that
0 0 0
the initial step size θ is too small in both the forward and the reverse direction. Here, we start the doubling
0
procedure. The step size selection procedure terminates as soon as |ℓ(θ)|≥|log(b)| for some θ =θ ×2j, j ∈Z−.
0
By Assumptions 4.7 and 4.8, lim ℓ(x,z,θ) = −∞. Therefore, there is a θ′ > θ with the property that
θ→∞ 0
|ℓ(θ)|≥|log(b)| for all θ ≥θ′. Provided that |log(b)|<∞, we have τ(s,θ )<∞ in this case, as well.
0
Proof of Proposition 4.10. The expected jump distance is
(cid:90) (cid:26) η(θ|f (x,z),a,b)(cid:27)
ED = |ℓ(x,z,θ)|min 1,eℓ(x,z,θ) θ π(dx)m(dz)(21 (d(a,b)))η(dθ |x,z,a,b).
η(θ|x,z,a,b) ∆AutoStep: Locally adaptive involutive MCMC
Let A = {x,z,a,b,θ : exp(ℓ(x,z,θ)) ≤ η(θ|x,z,a,b)/η(θ|f (x,z),a,b)}, and define A ,A ,A similarly by
≤ θ < ≥ >
replacing the inequality in the definition. We begin by splitting the integral into regions A ,A :
≤ >
(cid:90)
ED = |ℓ(x,z,θ)|π(dx)m(dz)(21 (d(a,b)))η(dθ |x,z,a,b)
∆
A>
(cid:90)
+ |ℓ(x,z,θ)|eℓ(x,z,θ)η(θ|f (x,z),a,b)π(dx)m(dz)(21 (d(a,b)))dθ.
θ ∆
A≤
Next we perform the transformation ofvariables x′,z′ =f (x,z) onthe A integral. Recall that ℓ(x,z,θ) satisfies
θ >
ℓ(f (x,z),θ)=−ℓ(x,z,θ),
θ
and note that this transformation yields the integration region A , such that
<
(cid:90)
ED = |ℓ(x,z,θ)|eℓ(x,z,θ)η(θ|f (x,z),a,b)π(dx)m(dz)(21 (d(a,b)))dθ
θ ∆
A<
(cid:90)
+ |ℓ(x,z,θ)|eℓ(x,z,θ)η(θ|f (x,z),a,b)π(dx)m(dz)(21 (d(a,b)))dθ.
θ ∆
A≤
The upper bound follows by combining the two terms and bounding the η ratio:
(cid:90) η(θ|f (x,z),a,b)
ED ≤2 |ℓ(x,z,θ)|eℓ(x,z,θ) θ η(dθ|x,z,a,b)π(dx)m(dz)(21 (d(a,b)))
η(θ|x,z,a,b) ∆
A≤
≤2η sup |y|eyπ(A )
≤
y≤logη
≤2η sup |y|ey
y≤logη
=2ηmax(cid:8) e−1,ηlogη(cid:9)
.B Additional Experiments
B.1 Synthetic and real data models
We present the synthetic and real data models used in Section 5. In the following, N(µ,σ2) represents a normal
distribution with mean µ and variance σ2. We write T (µ,τ) to represent the t-distribution with n degrees of
n
freedomandspecifiedlocationandscale,suchthatifx∼T (µ,τ)then(x−µ)/τ followsaStudent’st-distribution
n
with n degrees of freedom. We use T (µ,τ;b ,b ) to represent the truncated t-distribution with location and
n L U
scale with a lower bound b and an upper bound b . Similarly, we use N(µ,σ2;b ,b ) to denote a truncated
L U L U
normal distribution with bounds (b ,b ).
L U
• The Neal’s funnel with d dimensions (d≥2) and scale parameter τ >0, denoted by funnel(d,τ), is given by
iid
X ∼N(0,9), X ,...,X |X =x ∼ N(0,exp(x /τ)).
1 2 d 1 1 1
• The banana distribution with d dimensions and scale parameter τ >0, denoted by banana(d,τ), is given by
X ∼N(0,10), X ,...,X |X =x i ∼id N(x2,τ2/10).
1 2 d 1 1 1
• The d-dimensional normal distribution with precision parameter τ >0, denoted by normal(d,τ), is given by
iid
X ,...,X ∼ N(0,1/τ).
1 d
• The horseshoe model on the sonar dataset with a binary response vector y ∈{0,1}n and matrix of predictors
x∈Rn×d is given by
τ ∼Cauchy(0,1)
λ ∼Cauchy(0,1), j =1,...,d
j
β ∼T (0,1)
0 3
β ∼N(0,τ2Λ), Λ=diag(λ2,...,λ2)
1 d
 
1
y
i
|β 0,β,x
i
∼Bernoulli
(cid:16)
(cid:17), i=1,...,n.
1+exp −(β
+(cid:80)d
x β )
0 j=1 i,j j
• The mRNA model with N observations, the time t∈RN, and the observed outcomes y ∈RN is given by
log (t )∼Uniform(−2,1)
10 0
log (k )∼Uniform(−5,5)
10 0
log (β)∼Uniform(−5,5)
10
log (δ)∼Uniform(−5,5)
10
log (σ)∼Uniform(−2,2)
10

0, t −t ≤0

exp(−β·(t −t ))−exp(−δ·(t −t ))
i 0
µ = k · i 0 i 0 , if δ ̸=β ,i=1,...,N
i 0 δ−β
k
·(t −t ), if δ =β
0 i 0
y |µ ,σ ∼N(µ ,σ2), i=1,...,N.
i i i
• The kilpisjarvi model with the predictors x∈RN, the observed responses y ∈RN, and additional parameters
µ ,µ ,σ ,σ , is given by
α β α β
α∼N(µ ,σ2)
α α
β ∼N(µ ,σ2)
β β
σ ∼N(0,1;0,∞)
y |α,β,σ,x ∼N(α+βx ,σ2), i=1,...,N.
i i iAutoStep: Locally adaptive involutive MCMC
Model Dimension α Model Dimension α
horseshoe 127 35.67 banana(2,10) 2 3.431
mRNA 5 5.767 banana(4,0.3) 4 4.569
kilpisjarvi 3 5.9561 banana(4,1) 4 4.591
funnel(2,1) 2 4.047 banana(128,1) 128 58.94
funnel(2,10) 2 3.572 banana(128,10) 128 57.06
funnel(4,0.3) 4 4.712 normal(2,1) 2 5.674
funnel(4,1) 4 3.847 normal(2,10) 2 5.301
funnel(128,1) 128 65.44 normal(20,1) 20 14.19
funnel(128,10) 128 62.34 normal(128,1) 128 54.15
banana(2,1) 2 3.652 normal(128,10) 128 54.63
Table 1: The scaling constant α for each model, as mentioned in Section 5.
B.2 Estimation of scaling factors
AsmentionedinSection5,weuseαasatarget-dependentscalingfactortobalancethecostofgradientanddensity
evaluations. To determine α, we ran three separate Markov chains using Pigeons to obtain draws, benchmarked
the time required for gradient computations, and benchmarked the time for log density evaluations. The scaling
factor α is then obtained as the ratio of the total time spent on gradient evaluations to the total time spent on
log density evaluations. The chains are long enough such that the estimate error (in absolute difference) is within
2%. The α values used in all experiments are presented in Table 1.
There seems to be a positive correlation between α and the dimension of the problems. This correlation is likely
due to the auto-differentiation system used to compute gradients. While auto-differentiation avoids the need for
manual differentiation, it appears to incur a computational cost that increases with dimensionality.
B.3 Number of leapfrog steps in AutoStep HMC
GivenanumberL ∈N,thenumberofleapfrogstepstakenissampledateachiterationviaL∼Unif{1:L },
max max
independent of other variables. The algorithm is initialized with L =1, which means that it exactly matches
max
AutoStep MALA at this stage. Then, at each round we compute the sample autocorrelation ρ of V(x):=logπ(x).
If ρ > 0.99, we double the value of L . On the other hand, if ρ < 0.95, we set L ← max{1,L /2}.
max max max
Otherwise, we leave this parameter unchanged for the next round.
B.4 Comparison of AutoStep and fixed-step-size HMC and MALA
To complement the RWMH results shown in Section 5.1, we present the same results for MALA and HMC in
Fig. 6. The performance of the AutoStep methods is again consistent across all models in terms of high energy
jump distances. In terms of minESS per unit cost, the fixed-step-size methods are less sensitive to changes in step
size, and it is around a factor of 2 lower for AutoStep HMC and AutoStep MALA compared to methods with a
fixed step size. This is because the AutoStep methods require multiple evaluations of the gradient during each
iteration, which increases the computational cost. Despite this, the AutoStep methods give reliable performance
without the need for manual tuning. On the challenging mRNA transfection problem, MALA fails when the
step size is increased to 10 times the tuned value due to a rejection rate close to one (see Appendix B.5 for an
empirical validation).
B.5 Acceptance probability of AutoStep and fixed-step-size methods
We compare the average acceptance probability of AutoStep methods and fixed-step-size methods. Fig. 7 shows
theaverageacceptanceprobabilityforAutoStepandfixed-step-sizemethodsonthesamebenchmarksetdescribed
in Section 5.1. All samplers are run for 19 tuning rounds, and the average acceptance probabilities are collected
from the final round. AutoStep methods consistently target a reasonable acceptance probability. Conversely,
the acceptance probabilities for fixed-step-size methods are highly sensitive to changes in step size. Small step
sizes lead to acceptance probabilities near one, and large step sizes lead to acceptance probabilities near zero.
Specifically, MALA using 10.0×θ yields an acceptance probability of zero on the mRNA transfection problem.
0(a) Average energy jump distance (b) minESS per cost
Figure 6: Average energy jump distance (left) and minESS per unit cost (right) for AutoStep HMC (top) and
MALA (bottom). The fixed step sizes of the standard methods are set to {0.1,0.25,1.0,4.0,10.0} × the tuned θ
0
from AutoStep methods.
(a) HMC (b) MALA (c) RWMH
Figure 7: Average acceptance probabilities for HMC, MALA, and RWMH and their corresponding AutoStep
methods across six models for the last tuning round. The fixed step sizes of the standard methods are set to
{0.1,0.25,1.0,4.0,10.0} × the tuned θ from AutoStep methods.
0
B.6 Comparing performance in terms of minESS per second (instead of unit cost)
In Section 5, we reported the experimental results in terms of minESS per unit cost. In this section, we present
the same results expressed in terms of minESS per second. Fig. 8 (corresponds to Fig. 2 and Fig. 6) presents the
minESS per second for both AutoStep and fixed-step-size methods. Despite the fact that AutoStep methods
dynamically adjust the step size at each iteration, they consistently achieve minESS per second values that are
comparable to those of the fixed-step-size methods. Fig. 9 (corresponds to Fig. 3) presents the effect of jitter andAutoStep: Locally adaptive involutive MCMC
(a) HMC (b) MALA (c) RWMH
Figure 8: minESS per second for HMC, MALA, and RWMH and their corresponding AutoStep methods across
six models. The fixed step sizes of the standard methods are set to {0.1,0.25,1.0,4.0,10.0} × the tuned θ from
0
AutoStep methods.
(a) AutoStep HMC (b) AutoStep MALA (c) AutoStep RWMH
Figure 9: Comparison of each AutoStep MCMC method on the funnel, banana, and Gaussian distributions with
varying dimensions and scales. The plots display the minESS per second for each method. We use the same
model notation as in Fig. 3.
(a) AutoStep HMC (b) AutoStep MALA (c) AutoStep RWMH
Figure 10: Comparison of auto-tuned jitter and fixed jitter for each AutoStep MCMC method. The plots display
the minESS per second for each method. The legend distinguishes between the jitter tuning methods: “auto”
represents auto-tuned jitter, and fixed jitter is denoted by a number, which corresponds to the jitter standard
deviation σ.
the use of symmetric step size selector thresholds on minESS per second. It can be seen that the performance is
consistent across all sampler variants. Fig. 10 (corresponds to Fig. 5) shows the effect of jitter tuning on the
minESSpersecondforHMC,MALAandRWMH.Theautotunedjitterseemstoprovideconsistentlysatisfactory
performance. Notice that we use d=2 and τ =1 for both the funnel and banana distributions.(a) AutoStep HMC (b) AutoStep MALA (c) AutoStep RWMH
Figure 11: Comparison of each AutoStep MCMC method on the funnel, banana, and Gaussian distributions with
varying dimensions and scales. The boxplots show the average energy jump distance. We use the same model
notation as in Fig. 3.
(a) AutoStep HMC (b) AutoStep MALA (c) AutoStep RWMH
Figure12: Comparisonofauto-tunedjitterandfixedjitterforeachAutoStepMCMCmethod. Theboxplotsshow
the average energy jump distance for the different variants of jitter. The legend distinguishes between the jitter
tuning methods: “auto” represents auto-tuned jitter, and fixed jitter is denoted by a number, which corresponds
to the jitter standard deviation σ.
B.7 Effect of jitter and symmetric thresholds in terms of energy jump distance (instead of
minESS)
InadditiontotheminESS,wereporttheaverageenergyjumpdistanceforeachexperimentmentionedinSection5.
Fig. 11 presents the effect of jitter and the use of symmetric step size selector thresholds on average energy jump
distance. The performance is consistent across all sampler variants, except that RWMH benefits significantly by
using symmetric thresholds and random jitter. Fig. 12 shows the effect of jitter tuning on average energy jump
distance for AutoStep methods. The average energy jump distance appears to be sensitive to the jitter standard
deviation, with larger σ generally leading to lower results. The auto tuned jitter method provides consistently
satisfactory performance compared to well-tuned fixed jitter variants. As before, we use d=2 and τ =1 for both
the funnel and banana distributions.
B.8 Comparison with existing adaptive samplers
We compare the performance of AutoStep methods to two existing state-of-the-art adaptive samplers: the
No-U-Turn sampler (Hoffman and Gelman, 2014) from Turing.jl (Ge et al., 2018), and the hit-and-run slice
sampler (B´elisle et al., 1993; Neal, 2003). For all AutoStep methods, we use symmetric acceptance thresholds and
auto-tuned jitter as suggested in Section 5. The hit-and-run slice sampler uses univariate slice sampling (via
doubling, as described in Figure 4 of Neal, 2003) within the hit-and-run algorithm. Concretely, at each point
x with π(x) > 0, a direction z ∼ N(0,I) is sampled independently of other variables. Then, slice sampling is
used to sample from the restriction of π to the line {x+sz :s∈R} with initial point s=0. The slicer uses an
initial window size of 10 and a maximum number of 20 doublings. The sampler undergoes no adaptation in our
round-based scheme. We simulate on six benchmark models, which include two funnels, two banana distributions,
as well as the mRNA (Ballnus et al., 2017) and the logistic regression (Sejnowski and Gorman, 1988; CarvalhoAutoStep: Locally adaptive involutive MCMC
(a) minESS per cost (b) minESS per second (c) Average energy jump distance
Figure 13: Performance comparison across six benchmarks for NUTS, the hit-and-run slice sampler, and three
AutoStep methods.
et al., 2009) problems.
Fig. 13 demonstrates that AutoStep samplers generally offer competitive performance compared to both NUTS
and the hit-and-run slice sampler in terms of minESS per unit cost and average energy jump distance. In terms
of minESS per second, both AutoStep RWMH and the hit-and-run slice sampler offer fast sampling, whereas
NUTS is significantly slower by comparison. However, we note that the minESS is not always a reliable measure
of performance. For instance, although NUTS sometimes yields reasonable minESS values, it does not explore the
samplespaceaseffectivelyastheothermethods. ThisisevidentinFig.14,whereweshowpairplotsof217 samples
generated by the five samplers on the mRNA problem. As seen in the figure, NUTS exhibits erratic behaviour
compared to other methods. Furthermore, we should note that AutoStep RWMH shows greater variability in
performance when applied to real data problems.
B.9 Symmetric thresholds in AutoStep RWMH
We also empirically investigate the influence of symmetric thresholds when using AutoStep RWMH to ensure
irreducibility. Fig. 1 illustrates a hypothetical scenario where AutoStep RWMH is applied to a 1D Gaussian
distribution, which shows that symmetric thresholds help the sampler to explore modal regions. In support
of Fig. 1, Fig. 15 displays the scatter plots for 213 samples obtained from AutoStep RWMH with and without
symmetric thresholds applied to a 2D Gaussian distribution. Without symmetric thresholds, AutoStep RWMH
rarely visits the mode of the distribution. However, AutoStep MALA and HMC with asymmetric thresholds do
not experience this gap problem, because the use of an augmented space and projections onto the sample space
mitigate the risk of overshooting.(a) AutoStep HMC (b) AutoStep MALA
(c) AutoStep RWMH (d) Hit-and-run slice sampler
(e) NUTS
Figure 14: Pairplots of samples for different samplers on the mRNA model. Note that NUTS shows erratic
behavior on this target when compared to other samplers.AutoStep: Locally adaptive involutive MCMC
(a) Without symmetric acceptance thresholds (b) With symmetric acceptance thresholds
Figure 15: Scatter plots of samples obtained by AutoStep RWMH with no jitter on a 2D Gaussian target. Note
that without the symmetric acceptance thresholds, there is a visible hole in the middle of the scatterplot for this
target distribution.