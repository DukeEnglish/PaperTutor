[
    {
        "title": "From Zero to Hero: Cold-Start Anomaly Detection",
        "authors": "Tal ReissGeorge KourNaama ZwerdlingAteret Anaby-TavorYedid Hoshen",
        "links": "http://arxiv.org/abs/2405.20341v1",
        "entry_id": "http://arxiv.org/abs/2405.20341v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20341v1",
        "summary": "When first deploying an anomaly detection system, e.g., to detect\nout-of-scope queries in chatbots, there are no observed data, making\ndata-driven approaches ineffective. Zero-shot anomaly detection methods offer a\nsolution to such \"cold-start\" cases, but unfortunately they are often not\naccurate enough. This paper studies the realistic but underexplored cold-start\nsetting where an anomaly detection model is initialized using zero-shot\nguidance, but subsequently receives a small number of contaminated observations\n(namely, that may include anomalies). The goal is to make efficient use of both\nthe zero-shot guidance and the observations. We propose ColdFusion, a method\nthat effectively adapts the zero-shot anomaly detector to contaminated\nobservations. To support future development of this new setting, we propose an\nevaluation suite consisting of evaluation protocols and metrics.",
        "updated": "2024-05-30 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20341v1"
    },
    {
        "title": "Xwin-LM: Strong and Scalable Alignment Practice for LLMs",
        "authors": "Bolin NiJingCheng HuYixuan WeiHouwen PengZheng ZhangGaofeng MengHan Hu",
        "links": "http://arxiv.org/abs/2405.20335v1",
        "entry_id": "http://arxiv.org/abs/2405.20335v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20335v1",
        "summary": "In this work, we present Xwin-LM, a comprehensive suite of alignment\nmethodologies for large language models (LLMs). This suite encompasses several\nkey techniques, including supervised finetuning (SFT), reward modeling (RM),\nrejection sampling finetuning (RS), and direct preference optimization (DPO).\nThe key components are as follows: (1) Xwin-LM-SFT, models initially finetuned\nwith high-quality instruction data; (2) Xwin-Pair, a large-scale, multi-turn\npreference dataset meticulously annotated using GPT-4; (3) Xwin-RM, reward\nmodels trained on Xwin-Pair, developed at scales of 7B, 13B, and 70B\nparameters; (4) Xwin-Set, a multiwise preference dataset in which each prompt\nis linked to 64 unique responses generated by Xwin-LM-SFT and scored by\nXwin-RM; (5) Xwin-LM-RS, models finetuned with the highest-scoring responses\nfrom Xwin-Set; (6) Xwin-LM-DPO, models further optimized on Xwin-Set using the\nDPO algorithm. Our evaluations on AlpacaEval and MT-bench demonstrate\nconsistent and significant improvements across the pipeline, demonstrating the\nstrength and scalability of Xwin-LM. The repository\nhttps://github.com/Xwin-LM/Xwin-LM will be continually updated to foster\ncommunity research.",
        "updated": "2024-05-30 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20335v1"
    },
    {
        "title": "CausalQuest: Collecting Natural Causal Questions for AI Agents",
        "authors": "Roberto CeraoloDmitrii KharlapenkoAmélie ReymondRada MihalceaMrinmaya SachanBernhard SchölkopfZhijing Jin",
        "links": "http://arxiv.org/abs/2405.20318v1",
        "entry_id": "http://arxiv.org/abs/2405.20318v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20318v1",
        "summary": "Humans have an innate drive to seek out causality. Whether fuelled by\ncuriosity or specific goals, we constantly question why things happen, how they\nare interconnected, and many other related phenomena. To develop AI agents\ncapable of addressing this natural human quest for causality, we urgently need\na comprehensive dataset of natural causal questions. Unfortunately, existing\ndatasets either contain only artificially-crafted questions that do not reflect\nreal AI usage scenarios or have limited coverage of questions from specific\nsources. To address this gap, we present CausalQuest, a dataset of 13,500\nnaturally occurring questions sourced from social networks, search engines, and\nAI assistants. We formalize the definition of causal questions and establish a\ntaxonomy for finer-grained classification. Through a combined effort of human\nannotators and large language models (LLMs), we carefully label the dataset. We\nfind that 42% of the questions humans ask are indeed causal, with the majority\nseeking to understand the causes behind given effects. Using this dataset, we\ntrain efficient classifiers (up to 2.85B parameters) for the binary task of\nidentifying causal questions, achieving high performance with F1 scores of up\nto 0.877. We conclude with a rich set of future research directions that can\nbuild upon our data and models.",
        "updated": "2024-05-30 17:55:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20318v1"
    },
    {
        "title": "ANAH: Analytical Annotation of Hallucinations in Large Language Models",
        "authors": "Ziwei JiYuzhe GuWenwei ZhangChengqi LyuDahua LinKai Chen",
        "links": "http://arxiv.org/abs/2405.20315v1",
        "entry_id": "http://arxiv.org/abs/2405.20315v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20315v1",
        "summary": "Reducing the `$\\textit{hallucination}$' problem of Large Language Models\n(LLMs) is crucial for their wide applications. A comprehensive and fine-grained\nmeasurement of the hallucination is the first key step for the governance of\nthis issue but is under-explored in the community. Thus, we present\n$\\textbf{ANAH}$, a bilingual dataset that offers $\\textbf{AN}$alytical\n$\\textbf{A}$nnotation of $\\textbf{H}$allucinations in LLMs within Generative\nQuestion Answering. Each answer sentence in our dataset undergoes rigorous\nannotation, involving the retrieval of a reference fragment, the judgment of\nthe hallucination type, and the correction of hallucinated content. ANAH\nconsists of ~12k sentence-level annotations for ~4.3k LLM responses covering\nover 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the\nfine granularity of the hallucination annotations, we can quantitatively\nconfirm that the hallucinations of LLMs progressively accumulate in the answer\nand use ANAH to train and evaluate hallucination annotators. We conduct\nextensive experiments on studying generative and discriminative annotators and\nshow that, although current open-source LLMs have difficulties in fine-grained\nhallucination annotation, the generative annotator trained with ANAH can\nsurpass all open-source LLMs and GPT-3.5, obtain performance competitive with\nGPT-4, and exhibits better generalization ability on unseen questions.",
        "updated": "2024-05-30 17:54:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20315v1"
    },
    {
        "title": "S3D: A Simple and Cost-Effective Self-Speculative Decoding Scheme for Low-Memory GPUs",
        "authors": "Wei ZhongManasa Bharadwaj",
        "links": "http://arxiv.org/abs/2405.20314v1",
        "entry_id": "http://arxiv.org/abs/2405.20314v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20314v1",
        "summary": "Speculative decoding (SD) has attracted a significant amount of research\nattention due to the substantial speedup it can achieve for LLM inference.\nHowever, despite the high speedups they offer, speculative decoding methods\noften achieve optimal performance on high-end devices or with a substantial GPU\nmemory overhead. Given limited memory and the necessity of quantization, a\nhigh-performing model on a high-end GPU can slow down by up to 7 times. To this\nend, we propose Skippy Simultaneous Speculative Decoding (or S3D), a\ncost-effective self-speculative SD method based on simultaneous multi-token\ndecoding and mid-layer skipping. When compared against recent effective\nopen-source SD systems, our method has achieved one of the top\nperformance-memory ratios while requiring minimal architecture changes and\ntraining data. Leveraging our memory efficiency, we created a smaller yet more\neffective SD model based on Phi-3. It is 1.4 to 2 times faster than the\nquantized EAGLE model and operates in half-precision while using less VRAM.",
        "updated": "2024-05-30 17:54:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20314v1"
    }
]