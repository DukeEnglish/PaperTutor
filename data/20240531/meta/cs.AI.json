[
    {
        "title": "OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving",
        "authors": "Lening WangWenzhao ZhengYilong RenHan JiangZhiyong CuiHaiyang YuJiwen Lu",
        "links": "http://arxiv.org/abs/2405.20337v1",
        "entry_id": "http://arxiv.org/abs/2405.20337v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20337v1",
        "summary": "Understanding the evolution of 3D scenes is important for effective\nautonomous driving. While conventional methods mode scene development with the\nmotion of individual instances, world models emerge as a generative framework\nto describe the general scene dynamics. However, most existing methods adopt an\nautoregressive framework to perform next-token prediction, which suffer from\ninefficiency in modeling long-term temporal evolutions. To address this, we\npropose a diffusion-based 4D occupancy generation model, OccSora, to simulate\nthe development of the 3D world for autonomous driving. We employ a 4D scene\ntokenizer to obtain compact discrete spatial-temporal representations for 4D\noccupancy input and achieve high-quality reconstruction for long-sequence\noccupancy videos. We then learn a diffusion transformer on the spatial-temporal\nrepresentations and generate 4D occupancy conditioned on a trajectory prompt.\nWe conduct extensive experiments on the widely used nuScenes dataset with Occ3D\noccupancy annotations. OccSora can generate 16s-videos with authentic 3D layout\nand temporal consistency, demonstrating its ability to understand the spatial\nand temporal distributions of driving scenes. With trajectory-aware 4D\ngeneration, OccSora has the potential to serve as a world simulator for the\ndecision-making of autonomous driving. Code is available at:\nhttps://github.com/wzzheng/OccSora.",
        "updated": "2024-05-30 17:59:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20337v1"
    },
    {
        "title": "CoSy: Evaluating Textual Explanations of Neurons",
        "authors": "Laura KopfPhiline Lou BommerAnna HedströmSebastian LapuschkinMarina M. -C. HöhneKirill Bykov",
        "links": "http://arxiv.org/abs/2405.20331v1",
        "entry_id": "http://arxiv.org/abs/2405.20331v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20331v1",
        "summary": "A crucial aspect of understanding the complex nature of Deep Neural Networks\n(DNNs) is the ability to explain learned concepts within their latent\nrepresentations. While various methods exist to connect neurons to textual\ndescriptions of human-understandable concepts, evaluating the quality of these\nexplanation methods presents a major challenge in the field due to a lack of\nunified, general-purpose quantitative evaluation. In this work, we introduce\nCoSy (Concept Synthesis) -- a novel, architecture-agnostic framework to\nevaluate the quality of textual explanations for latent neurons. Given textual\nexplanations, our proposed framework leverages a generative model conditioned\non textual input to create data points representing the textual explanation.\nThen, the neuron's response to these explanation data points is compared with\nthe response to control data points, providing a quality estimate of the given\nexplanation. We ensure the reliability of our proposed framework in a series of\nmeta-evaluation experiments and demonstrate practical value through insights\nfrom benchmarking various concept-based textual explanation methods for\nComputer Vision tasks, showing that tested explanation methods significantly\ndiffer in quality.",
        "updated": "2024-05-30 17:59:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20331v1"
    },
    {
        "title": "4DHands: Reconstructing Interactive Hands in 4D with Transformers",
        "authors": "Dixuan LinYuxiang ZhangMengcheng LiYebin LiuWei JingQi YanQianying WangHongwen Zhang",
        "links": "http://arxiv.org/abs/2405.20330v1",
        "entry_id": "http://arxiv.org/abs/2405.20330v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20330v1",
        "summary": "In this paper, we introduce 4DHands, a robust approach to recovering\ninteractive hand meshes and their relative movement from monocular inputs. Our\napproach addresses two major limitations of previous methods: lacking a unified\nsolution for handling various hand image inputs and neglecting the positional\nrelationship of two hands within images. To overcome these challenges, we\ndevelop a transformer-based architecture with novel tokenization and feature\nfusion strategies. Specifically, we propose a Relation-aware Two-Hand\nTokenization (RAT) method to embed positional relation information into the\nhand tokens. In this way, our network can handle both single-hand and two-hand\ninputs and explicitly leverage relative hand positions, facilitating the\nreconstruction of intricate hand interactions in real-world scenarios. As such\ntokenization indicates the relative relationship of two hands, it also supports\nmore effective feature fusion. To this end, we further develop a\nSpatio-temporal Interaction Reasoning (SIR) module to fuse hand tokens in 4D\nwith attention and decode them into 3D hand meshes and relative temporal\nmovements. The efficacy of our approach is validated on several benchmark\ndatasets. The results on in-the-wild videos and real-world scenarios\ndemonstrate the superior performances of our approach for interactive hand\nreconstruction. More video results can be found on the project page:\nhttps://4dhands.github.io.",
        "updated": "2024-05-30 17:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20330v1"
    },
    {
        "title": "$\\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving",
        "authors": "Nan HuangXiaobao WeiWenzhao ZhengPengju AnMing LuWei ZhanMasayoshi TomizukaKurt KeutzerShanghang Zhang",
        "links": "http://arxiv.org/abs/2405.20323v1",
        "entry_id": "http://arxiv.org/abs/2405.20323v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20323v1",
        "summary": "Photorealistic 3D reconstruction of street scenes is a critical technique for\ndeveloping real-world simulators for autonomous driving. Despite the efficacy\nof Neural Radiance Fields (NeRF) for driving scenes, 3D Gaussian Splatting\n(3DGS) emerges as a promising direction due to its faster speed and more\nexplicit representation. However, most existing street 3DGS methods require\ntracked 3D vehicle bounding boxes to decompose the static and dynamic elements\nfor effective reconstruction, limiting their applications for in-the-wild\nscenarios. To facilitate efficient 3D scene reconstruction without costly\nannotations, we propose a self-supervised street Gaussian\n($\\textit{S}^3$Gaussian) method to decompose dynamic and static elements from\n4D consistency. We represent each scene with 3D Gaussians to preserve the\nexplicitness and further accompany them with a spatial-temporal field network\nto compactly model the 4D dynamics. We conduct extensive experiments on the\nchallenging Waymo-Open dataset to evaluate the effectiveness of our method. Our\n$\\textit{S}^3$Gaussian demonstrates the ability to decompose static and dynamic\nscenes and achieves the best performance without using 3D annotations. Code is\navailable at: https://github.com/nnanhuang/S3Gaussian/.",
        "updated": "2024-05-30 17:57:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20323v1"
    },
    {
        "title": "Improving the Training of Rectified Flows",
        "authors": "Sangyun LeeZinan LinGiulia Fanti",
        "links": "http://arxiv.org/abs/2405.20320v1",
        "entry_id": "http://arxiv.org/abs/2405.20320v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20320v1",
        "summary": "Diffusion models have shown great promise for image and video generation, but\nsampling from state-of-the-art models requires expensive numerical integration\nof a generative ODE. One approach for tackling this problem is rectified flows,\nwhich iteratively learn smooth ODE paths that are less susceptible to\ntruncation error. However, rectified flows still require a relatively large\nnumber of function evaluations (NFEs). In this work, we propose improved\ntechniques for training rectified flows, allowing them to compete with\nknowledge distillation methods even in the low NFE setting. Our main insight is\nthat under realistic settings, a single iteration of the Reflow algorithm for\ntraining rectified flows is sufficient to learn nearly straight trajectories;\nhence, the current practice of using multiple Reflow iterations is unnecessary.\nWe thus propose techniques to improve one-round training of rectified flows,\nincluding a U-shaped timestep distribution and LPIPS-Huber premetric. With\nthese techniques, we improve the FID of the previous 2-rectified flow by up to\n72% in the 1 NFE setting on CIFAR-10. On ImageNet 64$\\times$64, our improved\nrectified flow outperforms the state-of-the-art distillation methods such as\nconsistency distillation and progressive distillation in both one-step and\ntwo-step settings and rivals the performance of improved consistency training\n(iCT) in FID. Code is available at https://github.com/sangyun884/rfpp.",
        "updated": "2024-05-30 17:56:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20320v1"
    }
]