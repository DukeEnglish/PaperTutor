[
    {
        "title": "Disentangling and Mitigating the Impact of Task Similarity for Continual Learning",
        "authors": "Naoki Hiratani",
        "links": "http://arxiv.org/abs/2405.20236v1",
        "entry_id": "http://arxiv.org/abs/2405.20236v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20236v1",
        "summary": "Continual learning of partially similar tasks poses a challenge for\nartificial neural networks, as task similarity presents both an opportunity for\nknowledge transfer and a risk of interference and catastrophic forgetting.\nHowever, it remains unclear how task similarity in input features and readout\npatterns influences knowledge transfer and forgetting, as well as how they\ninteract with common algorithms for continual learning. Here, we develop a\nlinear teacher-student model with latent structure and show analytically that\nhigh input feature similarity coupled with low readout similarity is\ncatastrophic for both knowledge transfer and retention. Conversely, the\nopposite scenario is relatively benign. Our analysis further reveals that\ntask-dependent activity gating improves knowledge retention at the expense of\ntransfer, while task-dependent plasticity gating does not affect either\nretention or transfer performance at the over-parameterized limit. In contrast,\nweight regularization based on the Fisher information metric significantly\nimproves retention, regardless of task similarity, without compromising\ntransfer performance. Nevertheless, its diagonal approximation and\nregularization in the Euclidean space are much less robust against task\nsimilarity. We demonstrate consistent results in a permuted MNIST task with\nlatent variables. Overall, this work provides insights into when continual\nlearning is difficult and how to mitigate it.",
        "updated": "2024-05-30 16:40:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20236v1"
    },
    {
        "title": "The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof",
        "authors": "Derek LimMoe PuttermanRobin WaltersHaggai MaronStefanie Jegelka",
        "links": "http://arxiv.org/abs/2405.20231v1",
        "entry_id": "http://arxiv.org/abs/2405.20231v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20231v1",
        "summary": "Many algorithms and observed phenomena in deep learning appear to be affected\nby parameter symmetries -- transformations of neural network parameters that do\nnot change the underlying neural network function. These include linear mode\nconnectivity, model merging, Bayesian neural network inference, metanetworks,\nand several other characteristics of optimization or loss-landscapes. However,\ntheoretical analysis of the relationship between parameter space symmetries and\nthese phenomena is difficult. In this work, we empirically investigate the\nimpact of neural parameter symmetries by introducing new neural network\narchitectures that have reduced parameter space symmetries. We develop two\nmethods, with some provable guarantees, of modifying standard neural networks\nto reduce parameter space symmetries. With these new methods, we conduct a\ncomprehensive experimental study consisting of multiple tasks aimed at\nassessing the effect of removing parameter symmetries. Our experiments reveal\nseveral interesting observations on the empirical impact of parameter\nsymmetries; for instance, we observe linear mode connectivity between our\nnetworks without alignment of weight spaces, and we find that our networks\nallow for faster and more effective Bayesian neural network training.",
        "updated": "2024-05-30 16:32:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20231v1"
    },
    {
        "title": "Randomized Exploration for Reinforcement Learning with Multinomial Logistic Function Approximation",
        "authors": "Wooseong ChoTaehyun HwangJoongkyu LeeMin-hwan Oh",
        "links": "http://arxiv.org/abs/2405.20165v1",
        "entry_id": "http://arxiv.org/abs/2405.20165v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20165v1",
        "summary": "We study reinforcement learning with multinomial logistic (MNL) function\napproximation where the underlying transition probability kernel of the Markov\ndecision processes (MDPs) is parametrized by an unknown transition core with\nfeatures of state and action. For the finite horizon episodic setting with\ninhomogeneous state transitions, we propose provably efficient algorithms with\nrandomized exploration having frequentist regret guarantees. For our first\nalgorithm, $\\texttt{RRL-MNL}$, we adapt optimistic sampling to ensure the\noptimism of the estimated value function with sufficient frequency and\nestablish that $\\texttt{RRL-MNL}$ is both statistically and computationally\nefficient, achieving a $\\tilde{O}(\\kappa^{-1} d^{\\frac{3}{2}} H^{\\frac{3}{2}}\n\\sqrt{T})$ frequentist regret bound with constant-time computational cost per\nepisode. Here, $d$ is the dimension of the transition core, $H$ is the horizon\nlength, $T$ is the total number of steps, and $\\kappa$ is a problem-dependent\nconstant. Despite the simplicity and practicality of $\\texttt{RRL-MNL}$, its\nregret bound scales with $\\kappa^{-1}$, which is potentially large in the worst\ncase. To improve the dependence on $\\kappa^{-1}$, we propose\n$\\texttt{ORRL-MNL}$, which estimates the value function using local gradient\ninformation of the MNL transition model. We show that its frequentist regret\nbound is $\\tilde{O}(d^{\\frac{3}{2}} H^{\\frac{3}{2}} \\sqrt{T} + \\kappa^{-1} d^2\nH^2)$. To the best of our knowledge, these are the first randomized RL\nalgorithms for the MNL transition model that achieve both computational and\nstatistical efficiency. Numerical experiments demonstrate the superior\nperformance of the proposed algorithms.",
        "updated": "2024-05-30 15:39:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20165v1"
    },
    {
        "title": "A Geometric Unification of Distributionally Robust Covariance Estimators: Shrinking the Spectrum by Inflating the Ambiguity Set",
        "authors": "Man-Chung YueYves RychenerDaniel KuhnViet Anh Nguyen",
        "links": "http://arxiv.org/abs/2405.20124v1",
        "entry_id": "http://arxiv.org/abs/2405.20124v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20124v1",
        "summary": "The state-of-the-art methods for estimating high-dimensional covariance\nmatrices all shrink the eigenvalues of the sample covariance matrix towards a\ndata-insensitive shrinkage target. The underlying shrinkage transformation is\neither chosen heuristically - without compelling theoretical justification - or\noptimally in view of restrictive distributional assumptions. In this paper, we\npropose a principled approach to construct covariance estimators without\nimposing restrictive assumptions. That is, we study distributionally robust\ncovariance estimation problems that minimize the worst-case Frobenius error\nwith respect to all data distributions close to a nominal distribution, where\nthe proximity of distributions is measured via a divergence on the space of\ncovariance matrices. We identify mild conditions on this divergence under which\nthe resulting minimizers represent shrinkage estimators. We show that the\ncorresponding shrinkage transformations are intimately related to the\ngeometrical properties of the underlying divergence. We also prove that our\nrobust estimators are efficiently computable and asymptotically consistent and\nthat they enjoy finite-sample performance guarantees. We exemplify our general\nmethodology by synthesizing explicit estimators induced by the\nKullback-Leibler, Fisher-Rao, and Wasserstein divergences. Numerical\nexperiments based on synthetic and real data show that our robust estimators\nare competitive with state-of-the-art estimators.",
        "updated": "2024-05-30 15:01:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20124v1"
    },
    {
        "title": "Near Optimal Decentralized Optimization with Compression and Momentum Tracking",
        "authors": "Rustem IslamovYuan GaoSebastian U. Stich",
        "links": "http://arxiv.org/abs/2405.20114v1",
        "entry_id": "http://arxiv.org/abs/2405.20114v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20114v1",
        "summary": "Communication efficiency has garnered significant attention as it is\nconsidered the main bottleneck for large-scale decentralized Machine Learning\napplications in distributed and federated settings. In this regime, clients are\nrestricted to transmitting small amounts of quantized information to their\nneighbors over a communication graph. Numerous endeavors have been made to\naddress this challenging problem by developing algorithms with compressed\ncommunication for decentralized non-convex optimization problems. Despite\nconsiderable efforts, the current results suffer from various issues such as\nnon-scalability with the number of clients, requirements for large batches, or\nbounded gradient assumption. In this paper, we introduce MoTEF, a novel\napproach that integrates communication compression with Momentum Tracking and\nError Feedback. Our analysis demonstrates that MoTEF achieves most of the\ndesired properties, and significantly outperforms existing methods under\narbitrary data heterogeneity. We provide numerical experiments to validate our\ntheoretical findings and confirm the practical superiority of MoTEF.",
        "updated": "2024-05-30 14:51:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20114v1"
    }
]