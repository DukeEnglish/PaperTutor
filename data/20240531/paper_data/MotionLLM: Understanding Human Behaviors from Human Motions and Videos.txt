MotionLLM: Understanding Human Behaviors
from Human Motions and Videos
Ling-HaoChen∗1,3,ShunlinLu∗2,3,
AilingZeng3,HaoZhang3,4,BenyouWang2,RuimaoZhang2,LeiZhang†3
{thu.lhchen, shunlinlu0803}@gmail.com
1TsinghuaUniversity
2SchoolofDataScience,TheChineseUniversityofHongKong,Shenzhen(CUHK-SZ)
3InternationalDigitalEconomyAcademy(IDEAResearch)
4TheHongKongUniversityofScienceandTechnology
Projectpage: https://lhchen.top/MotionLLM
Please describe the motion.
… … The motion starts from preparing for the kick, moving
through the actual kick with left leg extended high in the
air, and ending with the follow-through after the kick.
Video Input What action does it start from?
OR preparing for the kick. (In-context learning)
What motion might be performing? What capabilities does
the performer need?
It might be kungfu or taekwondo. The performer must have good
physical coordination and balance.
Motion Input (reasoning)
(a)OurMotionLLMtakesmotionsorvideosasinputstounderstandhumanbehaviors.
… …
1. Begin in a supine position with arms at shoulder width. 2. Engage your core as you lift your
legs to a 45 degree angle. 3. Bring your feet hip width apart. 4. Slide your hands down your
MotionLLM as your thighs until your hands are on the floor. 5. Put down your feet to return to the supine position.
fitness coach
(b)Anapplication:MotionLLMasyourfitnesscoachbasedonitscaptioncapability.
Figure1: IntroductingMotionLLM.(a)TheinputandoutputofMotionLLM.(b)MotionLLMhas
broadapplicationscenarios,suchasanintelligentfitnesscoach.
Abstract
Thisstudydelvesintotherealmofmulti-modality(i.e.,videoandmotionmodal-
ities) human behavior understanding by leveraging the powerful capabilities of
LargeLanguageModels(LLMs). DivergingfromrecentLLMsdesignedforvideo-
onlyormotion-onlyunderstanding,wearguethatunderstandinghumanbehavior
necessitatesjointmodelingfrombothvideosandmotionsequences(e.g.,SMPL
sequences)tocapturenuancedbodypartdynamicsandsemanticseffectively. In
lightofthis,wepresentMotionLLM,astraightforwardyeteffectiveframeworkfor
humanmotionunderstanding,captioning,andreasoning. Specifically,MotionLLM
adoptsaunifiedvideo-motiontrainingstrategythatleveragesthecomplementary
advantagesofexistingcoarsevideo-textdataandfine-grainedmotion-textdatato
gleanrichspatial-temporalinsights. Furthermore,wecollectasubstantialdataset,
MoVid, comprising diverse videos, motions, captions, and instructions. Addi-
tionally, we propose the MoVid-Bench, with carefully manual annotations, for
betterevaluationofhumanbehaviorunderstandingonvideoandmotion. Extensive
experimentsshowthesuperiorityofMotionLLMinthecaption,spatial-temporal
comprehension,andreasoningability.
∗Equalcontribution,randomlistingorder.WorkdonebyLing-HaoChen,ShunlinLu,andHaoZhangduring
internshipatIDEAResearch.
†Correspondence:LeiZhang.
Preprint.
4202
yaM
03
]VC.sc[
1v04302.5042:viXraContents
1 Introduction 3
2 RelatedWork 4
2.1 LLM-basedVideoUnderstanding . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 HumanMotionUnderstanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3 Methodology 4
3.1 PreliminariesandNotations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.2 MotionLLM:UnderstandingHumanMotionsandVideos . . . . . . . . . . . . . 5
3.3 MoVid: HumanMotionandVideoUnderstandingDataset . . . . . . . . . . . . . 6
3.4 MoVid-Bench: MotionsandVideosUnderstandingBenchmark . . . . . . . . . . . 7
4 Experiments 7
4.1 ExperimentalSetting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.2 QuantitativeResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.3 QualitativeResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.4 AblationStudy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
5 ConclusionandDiscussion 12
A MoreComparisonsontheMoVid-Bench 19
B TechnicalDetails 20
B.1 ImplementationDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.2 EvaluationDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C DatasetConstructionofMoVid 21
C.1 ConstructingH3DQAandMotion-XQADataset. . . . . . . . . . . . . . . . . . . 21
C.2 Motion-XRecaptionUsingGPT-4V . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.3 MoVidDatasetOverallAnnotationProcessSummary . . . . . . . . . . . . . . . . 23
C.4 MoVidDatasetSamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.5 DetailsandDesignPrinciplesofMoVid-Bench . . . . . . . . . . . . . . . . . . . 29
D VisualizationComparisononDatasetUsage 32
D.1 ComparisononWhethertoUseMotionData . . . . . . . . . . . . . . . . . . . . 32
D.2 ComparisononWhethertoUseVideoData . . . . . . . . . . . . . . . . . . . . . 34
21 Introduction
Understandinghumanbehavior,suchasfine-grainedcaptioningandanalysis,iscrucialintherealm
ofhuman-centricmulti-modalityintelligence[21,25,93]andcanbenefitembodiedintelligencefrom
human-computerinteractionandroboticstohealthcareandsecurity[68,69,52,74,78,54,32,31].
Recently,therehasbeennotableprogressingeneral-purposevisualunderstanding[35,85,8,26,45,
72,70,87,37],owingtotheemergenceofLargeLanguageModels(LLMs)[50,66,12].Nevertheless,
therestillremainsasignificantgapinobtainingagoodunderstandingofhumanbehaviorsonspatial-
temporaldynamics,fine-grainedsemantics,behaviorreasoning,etc.
Human behaviors can be mainly represented by extracted human motions (e.g., via 3D human
parametricmodelSMPL[41,46]orskeletonsequences [57])orvideos[84,48,83,7]. Although
extractedhumanmotionisakindoflow-redundant, appearance-invariance, andprivacy-friendly
representation,directlyobtaininghigh-qualitymotionsneedsexpensivemotion-captureprocesses[67,
5, 47, 46], resulting in the scarcity. Besides, deficiencies in the motion-environment interaction
of motion data will lead to insufficient understanding of behaviors. In contrast, human-centric
videosareeasytoobtainandcontainrichhuman-environmentinteractionvisualcues,whichhelps
semanticmotionunderstandingandreasoningholistically. Forexample,playinggolf andsweeping
theflooraresimilarmotionsbutarequitedifferentinvideocontexts. However,videosinevitably
havehighcomputationcosts,raiseprivacyconcerns,andcontainexcessivelyredundantelementsand
ambiguitiesinsteadoffocusingonhumans.
Consideringthecomplementarycombinationofcompactmotionsandrich-contextvideos,weargue
that jointly modeling them is essential to pursue a more accurate, robust, and contextually rich
understandingofthedynamicsandsemanticsofmotions. Nevertheless,existingworkseitheruse
motions [56, 76, 20, 64, 25, 93] or videos [35, 85, 8, 26, 45, 72, 70, 37, 62, 9, 53, 81] as inputs
separatelytoconducthuman-centricmotionoractionunderstandingwithLLMs. Weattributethe
challengesofthisproblemtotwocriticalpoints: 1)limitedhigh-qualityvideo-motion-textpairsand
theinstructiontuningdata;2)under-exploredproblemtointegratemotionandvideounderstanding
intoaunifiedsystemduetolackofdataandincompleteharmonizationamongtext, motion, and
videomodalities.
Toaddresstheaforementionedchallenges,thisworkattemptstolaythefoundationofhuman-centric
motion-video-textpaireddataandaunifiedunderstandingframework. Firstly, weintroducethe
MoViddataset,comprisingdiversevideos,motions,captions,andinstructions. Thetextscontain
captionsandinstructionquestion-answers(QAs)tosupportdifferenttasksandtrainingstages.Motion
dataissourcedfromexistinglarge-scaledatasets,includingAMASS[46](captionsavailablefrom
HumanML3D[19])andMotion-X[36](accompaniedwithvideos). Regardingvideo-textdata,we
useGPT-4V[1,73,79]toannotate24kvideocaptionsfromMotion-X,employinga15×down-
samplingrateforkeyframesalongsidemeticulouslydesignedprompts. Formotion-textdata, we
augment the manually annotated captions of HumanML3D via GPT-4 [1], resulting in 272k QA
pairsservingasinstructions. Tofacilitateeffectiveinstructiontuning,theseinstructionsencompass
diversespatial-temporalquestions,in-contextQApairs,andreasoningdata. Similarly,weobtain
200kinstructionsforMotion-X.Secondly,weproposeMotionLLMtounderstandhumanbehaviors
withmotionandvideosinonesystem(Figure1a). Technically,toprojectmotionsandvideosinto
the linguistic space via trainable motion/video translators as V-L translators in the first stage. It
enablestheunificationofhumanbehaviorswithvariousmodalitiesastranslatedlanguages,thereby
leveragingthereasoningabilityinherentinLLMs[12]. Inthesecondstage,wefine-tunetheLLM
andV-Ltranslatorsthroughmotion-videojointinstructiontuning. Bysharingtheknowledgefrom
bothmodalitiesinthelinguisticspaceofLLM,MotionLLMcantakeadvantageofthecompatibility
oftwomodalities.
For a fair and thorough evaluation of both motion and video understanding, we present a bench-
mark, namely MoVid-Bench, to evaluate model performance on sequential dynamics, body-part
semantics,directionawareness,reasoningability,androbustnessagainsthallucinationusingdiverse
metrics. Thereferenceanswersundergometiculoushumanannotationandverification. Compared
toMotionGPT[25]andVideo-LLaVA[35],MotionLLMdemonstratesaverageimprovementsof
38%and15%inmotionandvideounderstanding,respectively. Inourablationstudy,integrating
fine-grainedmotiongainsanaverage15%enhancementinvideo-basedunderstanding,whilevisual
contentcuesfromvideosimprovemotion-basedunderstandingby29%. Ourextensiveevaluation
yieldsvaluableinsightsintohumanbehaviorunderstandingtocommunitydevelopmentandfollow-up
3research. Lastly, empowered with superior understanding capabilities from human motions and
videos,MotionLLMexhibitsversatilityacrossvariousdownstreamapplications,suchasservingasa
fitnesscoachforsocialgoods(Figure1b),particularlycateringtothevisuallyimpairedcommunity.
Beforedeliveringintodetail,wesummarizeourkeycontributionsasfollows.
• Torelievethescarcityofdataissues,weintroduceMoVidwithdiversecaptionandinstruc-
tion annotations from motion/video datasets for training holistic spatial-temporal under-
standingandfine-grainedbehaviors.
• Tobridgethegapbetweenvideoandmotionmodalities,wepropose amodelwithaunified
video-motiontrainingstrategyforhumanbehaviorunderstanding,captioning,andreasoning.
• Forbetterevaluationoffine-grainedunderstanding,wecarefullyconstructaMoVid-Bench
benchmarkconsideringmanymotion-relatedaspects.
2 RelatedWork
2.1 LLM-basedVideoUnderstanding
Videounderstandingplaysapivotalroleinnumerousapplicationsacrossvariousdomainsduetoits
abilitytoextractmeaningfulinsightsandinformationfromvisualdata. Previousattempts[60,82,30,
77]trytogeneratecaptionsofvideocontentwithdeeplearningmodels. Thedefectsofthesemethods
aremainlyrelatedtopoorreasoningandunderstandingabilities. WiththenotablesuccessofLarge
LanguageModels(LLMs),thereemergesaseriesofvision-basedormultimodalLLMs[38,29,86,
35,39,71]andthecorrespondingbenchmarks[34,49]. Recently,thesemethods[35,85,8,26,45,
72,70,37,91]explorethegeneral-purposeunderstandingofvideocontentsandthereasoningability
aboutthevideos.Specifically,Video-LLaVAenablesanLLMtoperformvisualreasoningcapabilities
onimagesandvideossimultaneously. Itlearnsaunitedvisualrepresentationbyaligningimagesand
videosbeforeprojectingthemintothelanguagefeaturespace. However,duetopaireddatalimitation
andignoringthedifferencesinmotionrepresentationsfromimages,therestillexistsasignificant
gap in understanding human-centric behaviors in the videos, especially dynamic movements for
fine-grainedbodysemantics.
2.2 HumanMotionUnderstanding
Humanmotionunderstanding[56,76,20,25,93]aimstoextractthesemanticsofhumanmotions.
Itisquitefundamentalandpromisingforautonomoustextualannotationandanalysisforhuman
motions, pavingthepathtobuildupmoredatafortext-alignedmotiongeneration[14,2,55,89,
11,19,3,17,88,27,94,13,42,18,75,61,23,10,43,92,54,40,59]. Takanoetal.[64]takesthe
earlyattempttogenerateatextualdescriptionofamotionviastatisticalmethods. PoseScript[15]is
proposedtodescribethesingle-framepose,whichenjoysgoodperformanceonthespatialmotion
understandingbutignoresthetemporalmotionunderstanding. Besides,[56,76,20]proposeddeep
models to perform motion captioning. Recently, some works [25, 93] have introduced LLMs to
understandhumanposesormotions. However,theseattemptsmainlyfocusonmotioncaptioning
andarenotequippedwithdetailedspatial-temporalawarenessandreasoningabilities. Asanalyzed
in[25], duetothelimitedmotionandinstructiontuningdata, theseworksarenotcapableofthe
reasoningabilityandarehardtoadapttolargerLLMs,e.g.Llama[66]orVicuna[12]. Besides,the
all-in-onesystemofmotiongenerationandunderstandingisakindofcompromisingunification.
Instead,inMotionLLM,weprojectthemotionandvideodataintothelinguisticspacetoobtaina
betterunderstandingofmotionsandvideos. Besides,withthereasoningabilityofLLMs,wecan
takeadvantageofthecompatibilityonbothmodalities.
3 Methodology
3.1 PreliminariesandNotations
We begin by clarifying the preliminaries and notations in MotionLLM. MotionLLM takes vi-
sual prompts P = M ∨ V (a motion M or a video V) as input, and outputs the text sequence
z={z1,z2,··· ,zL}∈{0,1}L×|S| thatfollowstheprompts,whereSdenotesthevocabularyset.
4Output Language Tokens
LLM
Large Language Model (LLM)
V-L Translator .
Vision Encoder
Input Language Tokens
V-L Translator (1) Stage 1: V-L Translation
… … LLM
Vision Encoder
LoRA
V-L Translator .
Visual Input P Vision Encoder
(2) Stage 2: Instruction Tuning
(a)MotionLLMArchitecture. (b)Two-stageTuning.
Figure2: SystemoverviewofMotionLLM.(a)MotionLLMtakesvideosorhumanmotionsas
visual input V. It first processes the visual input with a vision encoder and translates the vision
embeddingsintolinguisticspaceviaaV-Ltranslator. (b)MotionLLMistrainedintwostages. In
thefirststage,wetraintheV-Ltranslatortolearnthemodalitytranslation. Inthesecondstage,we
fine-tunetheLLMandtheV-Ltranslatorviainstructiontuningdata.
Specifically,amotionMiscomposedwithF-frameposesequencesM={m1,m2,··· ,mF}and
avideocomposedwithT key-frameimagesequencesV={v1,v2,··· ,vT}. Thetextgeneration
problemcanbeformulatedasanauto-regressiveproblem:z=F(z |P,z ),whereF(·)isMotion-
l <ℓ
LLM.ThetrainingprocessofMotionLLMusesacross-entropylossL=−(cid:80)L
F (z |P,z ).
ℓ=1 ℓ <ℓ
3.2 MotionLLM:UnderstandingHumanMotionsandVideos
Systemoverview. AsshowninFigure2a,MotionLLMtakesvideosorhumanmotionsasvisual
promptsP. MotionLLMfirstprocessesthevisualpromptsPwithavisionencoderandtranslates
thevisionembeddingsintolinguisticspaceviaaV-Ltranslator. Notethatweonlytakeonevideoor
motiondataasinput. Withawell-trainedMotionLLM,weoutputthelanguagesinanauto-regressive
fashion,i.e. z = F(z | P,z ). ThetrainingofMotionLLMcanbedividedintotwostages. As
l <ℓ
showninFigure2b,inthefirststage,MotionLLMlearnsatranslationlayer(V-Ltranslator,f (·))
T
betweenvisionembeddingsandLLMforbridgingthemodalitygap. Here,thevisionembeddings
areobtainedbyvisionencodersf (·). Inthesecondstage, MotionLLMfine-tunesboththeV-L
E
translator and LLM parts, i.e. f (·), via instruction tuning data. The whole MotionLLM can be
L
treatedasacompositefunctionofF =f ◦f ◦f . Wedetailbothtrainingpartsasfollows.
E T L
Modality translation (Stage 1). As there exists a modality gap between visual contents with
languages,wetrainamodalitytranslator(V-Ltranslator)tobridgethegapinthefirststage. Wename
thistrainingstagemodalitytranslationbecausethetargethereistoprojectthevisionpromptsinto
thelinguisticspace. Tokeepgoodcompressionknowledgeofmotionencoderandvideoencoder,we
freezebothencodersandtheLLMinthisstageandthetrainablepartistwoV-Ltranslatorsonly. The
motiontranslatorisalinearprojectionlayer,andthevideotranslatorisatwo-layerMLPduetothe
highercomplexityofvideodata. Inthismodalitytranslationstage,thetrainingdatawetakeisthe
motioncaptioningandvideocaptioningdata,whichwillbedescribedinSection4.
To detail the soundness of our technical design, we compare our MotionLLM with two similar
VisionLLMs(VLLM),LLaVA[38]andVideo-LLaVA[35]respectively. AsshowninFigure3a,
LLaVAonlytakestheimagesasinputwithoutotherexternalmodalities. DifferentfromLLaVA,
Video-LLaVAtakesimagesandvideosasinput. AscanbeseeninFigure3b,Video-LLaVAuses
different vision encoders for images and videos, respectively. As there is a small modality gap
betweenimagesandvideos,Video-LLaVAenjoysgoodperformancewiththesharedV-Ltranslator.
However,inFigure3c,motiondataisakindofstructuralskeleton-baseddata,whichisquitedifferent
frompixel-levelvideodata. Thislargermodalitygapindicatesthesharedmodalitytranslatorisno
longerawisechoiceforourtask. Therefore,inMotionLLM,wetakedifferentV-Ltranslatorsfor
motionsandvideosrespectively. Inthisfashion,twomodalitiescanenjoybettermodalitytranslation
capabilitiesrespectively.
Motion-video unified instruction tuning (Stage 2). In the second stage, MotionLLM needs to
respondtomorediverseinstructionsofhumaninputs.Here,thevisualencodersofbothmodalitiesare
frozen,andtheV-Ltranslatorsarestilltrainable. Differentfromthetrainingstrategyinthemodality
5
ytiladom eciohc oediV
noitoM
rotalsnarT
rotalsnarT
oediV
niotoM
redocnE
redocnELLM LLM LLM
V-L Translator . V-L Translator . m co hd oa icli ety
modality Motion Translator . Video Translator .
choice
Image Encoder
Image Encoder Video Encoder Motion Encoder Video Encoder
Smaller Modality Gap Larger Modality Gap
Image Image Video Motion Video
(a)LLaVAArchitecture. (b)Video-LLaVAArchitecture. (c)MotionLLMArchitecture.
Figure3: TechnicalcomparisonswithotherVLLMs. (a)LLaVA[38]takestheimagesasinput
only. (b)Video-LLaVA[35]sharesaunifiedV-Ltranslatorforimagesandvideosduetothesmall
modalitygapbetweenthetwomodalities. (c)Tobridgethelargermodalitygapbetweenmotionand
videos,wetaketwoseparatedV-Ltranslatorsforbettermodalitytranslations.
translation,theLLMpartf (·)isalsotrainabletoobtainabetterunderstandingofvisualcontent. To
L
keeptheoriginalknowledgeoftheLLM,wetraintheLLMpartinaparameter-efficientfine-tuning
fashion(PEFT),likeLoRA[22]. Here,withsharedparametersintheLLMpart,knowledgeofthe
twomodalitiesisinteractiveandsharedinthelinguisticspaceandbenefitseachother. Exceptfor
thecarefultechnicaldesignofMotionLLM,wealsoconstructtheunifiedinstructiontuningdataset,
especiallymotion-video-textdatainpairs,whichwillbeintroducedinSection3.3.
3.3 MoVid: HumanMotionandVideoUnderstandingDataset
AsdiscussedinSection1andSection3.2,weexpandexistingmotionandvideodatatoaunified
dataset(MoVid)forfine-grainedhumanbehaviorunderstanding. Forbothmotionandvideoparts,we
constructcorrespondingtextwithcaptionsandinstructions. Thedetailsofconstructionandstatistics
arediscussedasfollows.
Motion-textdatasetconstructionofMoVid. Inthe prompts
motionpartofMoVid,ourmethodmainlyfocuseson
The person is performing a series of kungfu motions. Start
detailedspatial-temporalmotionunderstandingand in a half squat with the left hand above your head, then
turn around and squat down in a defensive movement.
reasoningability. AsshowninFigure4,weaugment Motion Caption GPT-4
thecaptionofHumanML3D[19](a.k.a. H3D)mo- What is the starting position of the movement? Q
tiondatatodialogueQAsviaGPT-4[1],including A Start in a half squat position with your left hand
raised above your head.
272k QA pairs in total. The generated QAs cover And then? Q
diversespatial-temporalquestions,in-contextQAs, A Turn around and then squat down in a defensive
posture. generated
and reasoning data, which are used for instruction What are the benefits of completing the motion? Q QAs
tuning. The detailed prompts and more in-context A Completing this movement can enhance lower body
strength, improve flexibility and balance, and is also a
examples[90]areshownintheAppendix. Similarly good practice for self-de .f .e .n s .e . .skills.
toH3DQA,wealsointroducetheMotion-XQAin- Figure 4: QAofmotioncaptioningdatacon-
struction tuning dataset, whose caption annotation structionexample.WeintroduceGPT-4togen-
processwillbedetailedinthenextvideo-textdataset eratediverseQApairs,includingin-context
constructionpart. TheMotion-XQAcomesupwith examples (e.g. “And then?”), and reason-
200kQApairsintotal. Differentfromtheprevious ing QAs (e.g. “What are the benefits
motioninstructiontuningdataset[25]highlyrelated of completing the motion?”).
tomotioncaptioning,ourinstructiontuningdatasetismorecomplexanddiverse,includingin-context
examplesandreasoningdata.
Video-textdatasetconstructionofMoVid.Asthere
arelimitedvideo-textdatasetshighlyrelatedtohu- T ...
prompts
manbehaviors,ourmaineffortsmainlyfocusonan-
notatinghuman-centricvideos. AlthoughMotion-X
i
i
ts
hs
ew
no
ait nth nind ofi tv
o
ae
tr
imr os ne atm
piv
ro oet ci
e
eo
n
sn
o
s- uv isgid
h
se h.o
oT
wp oa ni rr
e
is ns, oi Ft ls
v
iget ue tx
rh
et ia
s
5n ,in
s
wso euta
e
rt
,
eio
la
an
s
-
1/15
frame
sampling
GPT-4V
belthecaptionofMotion-X[36]viaGPT-4Vatfirst. Relabelling videos and corresponding the motion part.
Wetakethe15×down-samplingratetoextractthe A man picks up something and points it,
holding in the air. After that, the man walks generated
keyframesofavideoandcontactthemintotheGPT- away happily and swaggeringly. cm apo tt ii oo nn s
4Vmodelwithsomecarefullydesignedprompts(de- Figure5: GPT-4Vannotationpipeline.
tailedinAppendix). Wechecktheannotatedvideocaptionsandfindthemaccuratelyannotatedon
6Dataset motion video type #pairs annotator Motion
H3DQA (cid:33) (cid:37) QA 272k GPT-4 Type All Body. Seq. Dir. Rea. Hall.
#count 700 205 171 140 148 36
Motion-XCaption (cid:33) (cid:33) Caption 24k GPT-4V
Video
Motion-XQA (cid:33) (cid:33) QA 200k GPT-4 Type All Body. Seq. Dir. Rea. Hall.
#count 650 167 216 43 185 39
Table1:Datasetstatistics. MoViddatasetincludesnewcaptiondatafor
Motion-XandQApairsforH3DQA. Table2:BenchmarkstatisticsofVideoandMotion.
Stage1 Stage2 Stage1 Stage2
Data H3D Motion-X H3DQA Motion-XQA BABEL-QA Data Valley Motion-XQA Video-ChatGPT
Capion
#count 23k 24k 272k 200k 2k #count 702k 200k 100k
(a)MoVidmotiondatasetstatistics. (b)MoVidvideodatasetstatistics.
Table3: MoViddatasetstatistics(DatainMoVidisbolded).
humanmotions. Thankstothepairwisevideo-motiondatainMotion-X[36],withthewell-annotated
videocaptiondata,wealsorelabelthecaptionofthemotionpartinMotion-X.Therefore,wecan
obtain 24k pairwise motion-video data with the same textual caption, which will provide more
modalityalignmentintheinstructiontuningstage. WiththeobtainedannotatedMotion-Xcaption
data,wegenerateaMotion-XQAinstructiontuningdatasetwithmulti-roundQAstoempowerthe
reasoningabilityofMotionLLM.ThepipelineofMotion-XQAannotationissimilartothepipeline
ofH3DQA(Figure4),generatedbyGPT-4[1]. WeleavemoredetailsofMotion-XQAconstruction
andgeneratedexamplesintheAppendix.
Datasetstatistics.Wesummarizethedatasetweconstructedforbothmodalities.AsshowninTable1,
ourMoviddatasetincludesnewcaptiondataforMotion-XandQApairsforH3DQAandMotion-
XQA.TheH3DQAsubsetofMoVidcomesupwith272kQApairs. ForMotion-X,weobtainnew
24kcaptionsofMotion-XwithGPT-4Vand200kQApairswithGPT-4. Wedetailmoredetailsabout
annotatedsamplesintheAppendix.
3.4 MoVid-Bench: MotionsandVideosUnderstandingBenchmark
Forabettercomparisonofthefine-grainedhumanbehaviorunder-
standing,weconstructabenchmarkforevaluatingtheperformance, W h -i Ach ) Rle igg h d tid the man use to kick? Q
named MoVid-Bench. As shown in Table 2, MoVid-Bench eval- - B) Left
- C) Both
uatesthehumanbehaviorunderstandingabilitiesonmotionsand A B) Left - D) None Body-part
videos. FollowingthepreviousVLLMbenchmark[34]evaluation
Which direction does the person turn to Q
volumes,MoVid-Benchcomesupwith1,350datapairs,including after walking?
A He turns 180 degrees and walks back
700formotionsand650forvideos. Forthemotionpart,thedatais the way he came. Direction
asubsetoftheH3DQAtestset,whereallQAsarecarefullychecked After clapping, what does the person do? Q
andrevisedbyhumans. Similarly, thevideobenchmarkdataisa A The person to sit down. Sequential
subsetoftheMotion-XQAtestset,whereallQAsarealsocarefully
Could this motion sequence be a part of any Q
exercise or is it just random motion?
checked and revised by humans. Besides, we design the evalu-
Without additional context, it's
ation of model performance on five aspects, including body-part A d mi off vic eu mlt e t no t d se et qe ur em ni cn ee iw sh pe at rh te or ft h ai ns Hallucination
exercise routine or simply random
motionawareness(Body.),sequentialanalysisability(Seq.),direc- movements.
tionawareness(Dir.),reasoningability(Rea.)[63],androbustness Can we induce whether the person walking Q
on a flat surface or inclined plane?
againsthallucination(Hall.)[24],respectively. Allthesefiveaspects A Yes, he is. Reasoning
arecategorizedmanually. AsthemovementtrajectoryinMotion-X Figure6: AnexampleofQA
videosisshort,suchas“sitting while playing guitar”,the categorization.
annotationsonthedirectionpartarelimited. Besides,asthevideos
aregivenasreferencesandthehallucinationdoesnothappenfrequently,wedonotevaluatethiswith
toomanyexamples,whichismorerelatedtothenaturallanguageprocessingcategory. Wetakean
exampleinFigure6toshowourdesignprincipleonhowtocategorizethesefivetypes. Weleave
moredetailsanddesignprinciplesintheAppendix. InSection4,wewillintroduceourevaluation
metricsontheMoVid-Bench.
4 Experiments
4.1 ExperimentalSetting
Trainingdataset. Formotion,asshowninTable3a,wetakeHumanML3D(a.k.a. H3D)andour
Motion-X Caption (a subsetof MoVid)data as our training data. In the instruction tuning stage,
7Body. Seq. Dir. Rea. Hall. All
MoVid-Bench-Motion
Acc. Score Acc. Score Acc. Score Acc. Score Acc. Score Acc. Score
GT 100.00 5.00 100.00 5.00 100.00 5.00 100.00 5.00 100.00 5.00 100.00 5.00
GPT-3.5[6] 24.51 2.04 30.41 2.25 27.14 2.19 39.19 2.64 58.33 3.22 31.33 2.31
MotionGPT[25] 31.22 3.98 42.69 3.16 44.29 3.50 35.81 3.06 16.66 2.25 36.86 3.11
MotionLLM 50.49 3.55 36.84 3.14 58.57 3.76 52.70 3.58 55.56 3.39 49.50↑38% 3.49↑12%
+12.64 +0.38
Body. Seq. Dir. Rea. Hull. All
MoVid-Bench-Video
Acc. Score Acc. Score Acc. Score Acc. Score Acc. Score Acc. Score
GT 100.00 5.00 100.00 5.00 100.00 5.00 100.00 5.00 100.00 5.00 100.00 5.00
GPT-3.5[6] 2.40 1.23 1.39 1.00 4.65 1.09 5.41 1.65 0.00 0.94 3.03 1.26
Video-LLAVA[35] 33.53 2.76 25.46 2.72 41.86 2.84 52.97 3.28 58.83 1.89 42.53 2.70
MotionLLM 34.13 2.93 32.87 2.92 44.18 3.14 63.20 3.55 70.59 2.30 49.00↑15% 2.97↑10%
+6.47 +0.27
Table4: ComparisonontheMoVid-Bench. Thetoptableisformotionandthebottomtableisfor
video. Thelargertheaccuracyandscore,thebettertheresult.
exceptforourconstructedH3DQAandMotion-XQA,weadditionallytake2ksizeBABEL-QA[16]
asourtrainingdata. Forvideo, inTable3b, asweonlyneedtolearnaV-Ltranslatorinthefirst
stage,wetaketheValley[44]videocaptioningdatasettotrainourprojectionlayer. Inthesecond
stage, we take the Motion-XQA as a part of the training data to empower the comprehension of
humanbehaviors. TopreservethegeneralVQAcapability,weusetheVideo-ChatGPTdataduring
ourinstructiontuning.
Evaluationdataset. Formotionunderstandingtasks,weevaluatemotioncomprehensionabilityon
ourMoVid-Bench. WealsotestourperformanceontheBABEL-QA[16]testsetforcomparison
with some expert models. For video-based tasks, we evaluate our model on three benchmarks,
MVBench[34](zero-shot),ActivityNet-QA[83](zero-shot),andMoVid-Bench. Specifically,forthe
MVBench,sincewedonotfocusonscenesandobjectsandforthefairevaluationofhumanbehavior
understanding,weperformacomparisonof7humanbehavior-relatedsub-tasks,whichare1)Action
Localization,2)ActionPrediction,3)ActionSequence,4)EgocentricNavigation,5)Fine-grained
Action,6)Fine-grainedPose,and7)UnexpectedAction,respectively.
Evaluationmetrics. IntermsofourMoVid-Bench,followingtheevaluationprotocolinprevious
research[35],weutilizeGPT-3.5-turboforevaluation.Technically,theevaluationinvolvescomparing
themodelanswerwiththegroundtruthanswertoprovideevaluationaccuracyandassignascore
rangingfrom0to5. InourapproachtoBABEL-QAbenchmark[16],followingtheoriginalsetting,
we use prediction accuracy for evaluation. For MVBench video understanding evaluation, we
answerthemulti-choicequestionsandselectthebestoptionsguidedbytheanswerprompt“Best
option:(”,following[34]. Inthisway,ourmodelcouldfollowtheinstructionswellandchoosethe
bestoneamongthegivenoptions. IntermsofActivityNet-QA[83]andourMoVid-Bench,weadopt
theevaluationprotocolusedin[35,33]byutilizingGPT-3.5-turbo,whichissimilartotheevaluation
protocolonourMoVid-Bench.
Implementationdetails. Weusethelit-gptframework[4]andextendittomulti-modalinput. We
applythepre-trainedLanguageBind[95]toencodevideoandapre-trainedVQ-VAE[88]encoderto
encodemotiondata. Vicuna-7B[12]isusedasourbaseLLMmodel. Formotion,weuseaone-layer
lineartransformationasthemotiontranslatortoperformmodalitytranslation. Forvideo,weusea
two-layerMLPasthevideotranslatorandencodevideoswith8-frameimages. Whentraining,inthe
firststage,thevideoencoder,motionVQ-VAEencoder,andtheLLMarefrozen. wetrainthemotion
andvideotranslatorwithalearningrateof1×10−3. Inthesecondstage,thevideoencoderand
motionVQ-VAEencoderarestillfrozen;wetrainthevideoandmotiontranslatorswithalearning
rateof2×10−5. TheLLMistunedbyLoRA[22]withalearningrateof2×10−4andtherankas
64. Intheevaluationstage,wetake8videoframesandthewholemotionsasthemodelinput. We
leavemoretrainingandtestingdetailsintheAppendix.
4.2 QuantitativeResults
Weshowquantitativeresultsformotionandvideounderstandingofhumanbehaviorsonexisting
benchmarksandMoVid-Bench.
EvaluationmotionunderstandingcapabilityonMoVid-Bench. WecompareMotionLLMwith
baselinesonMoVid-Bench(motionpart)onfiveaspects: body-partawareness,sequentially,direction
analysis,reasoningability,andhallucination,respectively.TheevaluationfollowspreviousLLMeval-
uationmetrics[35,33,26]onaccuracyandscores. Wecompareourmethodwithtext-onlyGPT-3.5
8Model Pred.type Overall↑ Action↑ Direction↑ BodyPart↑ Before↑ After↑ Other↑
2s-AGCN-M[58] cls. 0.355 0.384 0.352 0.228 0.331 0.264 0.295
2s-AGCN-R[58] cls. 0.357 0.396 0.352 0.194 0.337 0.301 0.285
MotionCLIP-M[65] cls. 0.430 0.485 0.361 0.272 0.372 0.321 0.404
MotionCLIP-R[65] cls. 0.420 0.489 0.310 0.250 0.398 0.314 0.387
MotionLLM gen. 0.372 0.396 0.417 0.154 0.329 0.353 0.338
MotionLLM* gen. 0.436 0.517 0.354 0.154 0.427 0.368 0.529
Table5: ComparisonofdifferentmethodsonBABEL-QAtestset. The“*”denotesfinallyfine-
tunedonBABEL-QA.“Pred. type”denotesthepredictiontype,includingclosedsetclassification
(cls.) andopenvocabularygeneration(gen.). “-M”and“-R”denoteMLPandRNN,respectively.
MotionLLMshowscomparableperformancewithclose-setregressionexpertmodels.
Model LLM Frames AL AP AS EN FA FP UA Avg.
Otter-V[28] Llama-7B 16 23.5 23.0 23.0 23.5 27.0 22.0 29.5 24.5
mPLUG-Owl-V[80] Llama-7B 16 23.0 28.0 22.0 26.0 29.0 24.0 29.0 25.8
VideoChatGPT[45] Vicuna-7B 100 20.0 26.0 23.5 29.5 22.5 29.0 26.5 25.2
VideoLLaMA[85] Vicuna-7B 16 22.5 25.5 27.5 30.0 29.0 32.5 39.0 29.4
VideoChat[33] Vicuna-7B 16 27.0 26.5 33.5 23.5 33.5 26.5 40.5 30.1
Video-LLaVA[35] Vicuna-7B 8 22.5 25.5 29.5 29.0 24.5 28.5 24.5 26.3
MotionLLM Vicuna-7B 8 33.0 29.5 32.5 29.0 31.5 28.5 37.5 31.6↑5%
+1.5
Table6: Comparisonwithdifferentvideo-basedLLMsonMV-Bench. MotionLLMoutperforms
baselinesonoverallaveragemetric.
answerresultsandMotionGPTresults. AsshowninTable4,ourmodelperformsthebestresultswith
baselinesonoverallaccuracyandscores.AstheGPT-3.5baselinecannotbecompatiblewithmotions,
itcannotunderstandhumanmotionaccurately. Specifically,MotionGPTshowslimitedreasoning
androbustnessagainsthallucination. MotionGPTistrainedontheHumanML3Ddatasetonly,and
the instruction tuning dataset mainly focuses on the motion caption task, like “Describe the
motion represented by <Motion> in plain English.” or “What does the <Motion>
communicate? Please describe it in language.”. Thisinstruction-tuningdatasetmakes
ithardtofollowcomplexinstructions,likereasoningorfine-grainedspatial-temporalunderstanding.
MotionLLMrelievestheseissues,benefitingfromourcarefullydesignedinstructiontuningdataset.
EvaluationonBABEL-QA.Weadditionallyshowthespatial-temporalcapacityofMotionLLM
onBABEL-QA,whichincludesdiversespatial-temporalquestions. Following[16],wecompare
MotionLLMwithseveralbaselines. 1)2s-AGCN,anend-to-endapproachusing2s-GCNtoextract
motion features and predict the answer with an MLP (-M) or an RNN (-R). 2) MotionCLIP, a
transformer-basedmethodusedforextractingmotionfeaturesandpredictingtheanswerwithan
MLP(-M)oranRNN(-R).NotethatthesebaselinesanswerthequestionsofBABEL-QAinaclosed
vocabularyset. WetaketwostagesMotionLLMmodelforcomparison. Here,wesettheprediction
accuracyastheevaluationmetric. Astheevaluationistheexactstringmatching,wesetupabaseline
MotionLLM*fine-tunedonBABEL-QA.FromTable5,althoughourmethodisanopenvocabulary
multi-modalitylanguagegenerationmodel,MotionLLMstillenjoyscomparableperformancewith
expertmodels.Astheexactstringmatchingisnotreasonableforourfinalmodel,wealsocompareour
finalMotoinLLMwithbaselinesviaGPTevaluation,similartothesettingintroducedinSection4.1.
AsshowninTable5,MotionLLMalsoshowscomparableaccuracywithbaselines. Theperformance
dropinthefirststagemainlycomesfromthemodalityandtaskcompromise.
EvaluationvideounderstandingresultsonMoVid-Bench. Table4showstheevaluationresults
onMoVid-Bench(videopart)acrossfivekeyareasdefinedintheMoVid-Benchconstructionpart.
Thisevaluationadherestotheestablishedmetricsusedinpreviouslargelanguagemodel(LLM)
evaluation[35,33,26]. AscanbeseeninTable4,withoutvideogrounds,itishardforGPT-3.5to
answerthequestionswell. OurMotionLLMsignificantlyoutshinesthepreviouslyleadingvideo
model, Video-LLaVA[35], ineachevaluatedaspect. Specifically, MotionLLMachievesa↑15%
improvementinaverageaccuracyanda↑10%enhancementinaveragescoreoverVideo-LLaVA.The
performanceofVideo-LLaVAisweakerinsequentiality,reasoning,andhallucinationtasksduetoits
lackofjointtrainingwithmotiondata. Conversely,MotionLLMmakesreasonableuseofmotion
dataalongsideacarefullydesignedinstructiontuningdataset,leadingtoenhancedperformance. We
providemorediscussiononmorevideo-basedLLMsonourMoVid-BenchintheAppendix.
9GT: Blowing a balloon during walking
https://motionx.deepdataspace.com/visualization/idea400/subset_000
0/Blowing_A_Balloon_During_Walking.mp4
: Explain the motion illustrated in <motion> : Build a narrative description that matches the
using language. stated series of human motion cues.
TM2T : the person is move something. MotionGPT: a person is waving around his arms. : a person holdsself & turning around.
https://motionx.deepdataspace.com/visualization/hu https://motionx.deepdataspace.com/visualization/huma
manml/001130.mp4 nml/001130.mp4
: While hopping, does the individual change the position?
: Generate a textual description corresponding to the given sequence of : Indeed, the individual shifts laterally while hopping.
human motion.
: Does the individual remain still during these movements?
: a person crawls forward on their hands then stands up.
: No, the individual is not stationary. They are hopping in a fixed location.
: What is the direction? : Can you elaborate on how the individual's motion resembles that of a spring?
: Comparing it to a spring-like motion suggests that the individual's actions
: The person crawled forward, got up and started walking in a are likely lively and buoyant. They leap from side to side, rebounding each
counterclockwise direction. time – akin to the movement of a spring being compressed and released.
Figure7: Examplesofmotionscomprehension. TheresultsdemonstratetheproficiencyofMotion-
LLMincaptioning,spatial-temporalunderstanding,andreasoning. ComparisonwithTM2T[20]and
MotionGPT[25]underscorestheeffectivenessofMotionLLMinhandlingunseenmotions.
Zero-shotvideomulti-choiceQAonMVBench. AsshowninTable6,weconductazero-shot
evaluationofvideoquestion-answeringcapabilitiesonMVbench[34]with7differenthumanmotion-
relatedsub-tasks.Despiteprocessingmerelyeightframespervideo,ourMotionLLMmodelsurpasses
existingvideo-basedLLMbaselinesintermsofaverageaccuracy. Notably,MotionLLMachieves
a↑5%higheraverageaccuracythanitsclosestcompetitor,VideoChat[33]. IntheareasofAction
LocalizationandActionPrediction,ourmodeldemonstratessuperiorperformanceoverallcompeti-
tors,highlightingitsexceptionalabilitytounderstandtemporaldynamics. Remarkably,MotionLLM
leadsVideoChatby↑22%inActionLocalization,underscoringitseffectivenessincompensatingfor
thelimitationsofavideoencoderthatonlyprocesseseightframes. Moreover,theperformanceon
othersub-tasksisonparwiththatofotherbaselines,indicatingthatourmodelmaintainsthespatial
relationshipsandreasoningcapabilities.
Zero-shot open-vocabulary video QA on
Methods Acc. Score
ActivityNet-QA.Tobetterevaluateourmodel FrozenBiLM 24.7 -
on human behaviors for long videos, we con- VideoChat - 2.2
ductzero-shotevaluationsonActivityNet-QA, LLaMA-Adapter 34.2 2.7
Video-LLaMA 12.4 1.1
inTable7. Notethatourmodelisnottrained
Video-ChatGPT 35.2 2.7
with any ActivityNet data. Our MotionLLM Video-LLaVA 45.3 3.3
surpasses the leading model by ↑9% on accu- Video-chat2 49.1 3.3
racyand↑7%onscoremetrics,showinggood MotionLLM 53.3↑9% 3.52↑7%
+4.2 +0.22
sequentialunderstandingandgeneralizablehu- Table7:ResultsonActivityNet-QA.
man behavior comprehension of the video content. This indicates the promising application of
MotionLLMintherealworld.
4.3 QualitativeResults
Qualitativeresultsonmotion-basedcomprehension. FortheunderstandingofMotionLLMon
themotionmodality, weprovidemorevisualizationresultsinFigure7. Forthefirstcomparison
withTM2T[20]andMotionGPT[25],wechoosetheMotionLLMmodelcheckpointtrainedwithout
Motion-Xdataforcomparison. ThecomparisonisconductedontheIDEA-400subsetofMotion-
X[36]inazero-shottestsetting. AscanbeseeninFigure7,MotionLLMenjoysmoregeneralization
abilityonsomein-the-wildscenariosandhasthepotentialtoworkasanautomatictextannotatorfor
motiondata. Wefurthertakesomein-contextexamplestoshowthespatialawarenessofMotionLLM.
10(Caption Ability)
: Is the man prone or standing?
: Yes, the man is prone and doing the Ginastica, which involves being prone on the ground and lifting and lowering the leg using the arms.
(Spatial-temporal Comprehension) (Comparing with baselines)
: In what shape does the man walk around? A circle or a square?
: The man walks around in a circle.
: Did the man walk backward?
(Reasoning Ability)
: Yes, the man in : Yes, the man walked backwards : No, the man
the video walked as he stood up and spoke to the does not walk
backward. camera while the other two men backward.
were working in the office.
: w D ali kd ft oh rwe a m rda ?n : Did the man walk forward? : w D ai ld k ft oh re w am rda ?n
: Yes, the video shows the man
: If you are a robot and you watch and obey the human's gesture : Yes, the man in walking forward. It is unclear if : Yes, the man
command. What does the man's gesture mean in the video? the video walked he continues to walk forward is walking
backward. forward.
after the camera cuts away.
: The man's gestures indicate that he is asking me to come. Video-Chat Video-LLaVA MotionLLM (Ours)
Figure8: Videoscomprehensionofmodels. TheresultsshowgoodperformanceofMotionLLMon
captioning,spatial-temporalcomprehension,andreasoning. ThecomparisonwithVideo-Chat[33]
andVideo-LLaVA[35]showsgoodsequentialityanddirectioncomprehensionofMotionLLM.
For the final case, MotionLLM not only understands the motion patterns but also enjoys good
reasoningcapabilitiesformotioncomprehension.
Qualitativeresultsonvideo-basedcomprehension. Toanalyzeourlanguageoutputsonunder-
standinghumanbehaviorsfromvideos,wetakesomerepresentativeexamplestoexplorethehuman
behaviorunderstandingcapabilityofMotionLLMinFigure8. Expectthebasiccaptioningcapacity.
Our MotionLLM also enjoys good spatial-temporal comprehension, such as “walks around a
circle”. Moreover, thanks to the basic reasoning ability of LLM, MotionLLM can also induce
theintentionofhumanbehaviors,suchasinducingthe“indicate that he is asking me to
come”intentionfromthebeckingmotion,showingthepotentialofapplicationinembodiedintelli-
gencescenarios. WeadditionallyperformacomparisonwithVideo-Chat[33]andVideo-LLaVA[35]
onthetemporalunderstandingability. AlthoughVideo-Chatcananswerthefirstquestioncorrectly,
itssecondansweriscontradictory, failingtoobtaingoodin-contextlearningcapability. Besides,
Video-LLaVAfailsinthefirstquestionandalwaysanswersthe“Yes”. Differentfromthesemethods,
MotionLLMenjoysbetterin-contextlearningcapabilityandtemporalcomprehensionthanbaselines.
4.4 AblationStudy
Here, we conduct ablation studies on different modality modeling strategies and show results of
instructiontuningusingunpaireddatafromH3DQA,BabelQA,andVideo-ChatGPTinstructiondata
andpaireddataMotion-XQAdiscussedabove. Notethatthe“paired”datastatementheredenotesthe
Motion-XQAsubsetofMoVid,includingmotion-video-texttriplepairs. Thisdatasetdesignaimsto
maketheunderstandingofvideoandmotionbenefitfromeachother. Theperformanceistestedon
ourbenchmarkMoVid-Bench.
Ablationonmotionunderstanding. AsseeninthetoppartoftheTable8,theusageofvideodata
helpstoimprovethemotionunderstandingoverall,especiallyonthebodydescription,reasoning
abilities,andhallucinationreduction. Withthehelpofvideos,theoverallperformanceimprovedby
28.6%onaverageaccuracy. Weattributethistothefactthatvideoprovidesmorereferenceinfor-
mation,suchashuman-environmentinteractioninformationformotionmodality. Wheninstruction
tuning with unpaired video-motion data, the abilities of all five aspects are improved, indicating
theadvantagesofthejointtrainingstrategywithvideos. Moreover,basedonthis,theusageofour
paireddataMotionX-QAbooststheperformancefurtherinmostaspects,exceptforthesequential
perceptionability. Wearguethisisduetothelimitationofthevideoencodercompressioncapacity,
whichcanonlyencode8frames,losingtoomuchinformation. Therefore,whentrainingwithmore
11MoVid-Bench-Motion Body. Seq. Dir. Rea. Hall. All
Motion Video Unpair Pair Acc. Score Acc. Score Acc. Score Acc. Score Acc. Score Acc. Score
(cid:33) (cid:37) - - 35.29 2.77 39.18 2.90 53.57 3.25 34.45 2.77 11.11 2.00 38.48 2.86
(cid:33) (cid:33) (cid:33) (cid:37) 47.55 3.54 46.20 3.26 46.43 3.49 53.38 3.63 44.44 3.08 48.07 3.50
(cid:33) (cid:33) (cid:33) (cid:33) 50.49 3.55 36.84 3.14 58.57 3.76 52.70 3.58 55.56 3.39 49.50↑29% 3.49↑8%
+11.02 +0.63
MoVid-Bench-Video Body. Seq. Dir. Rea. Hall. All
Motion Video Unpair Pair Acc. Score Acc. Score Acc. Score Acc. Score Acc. Score Acc. Score
(cid:37) (cid:33) - - 33.53 2.76 25.46 2.72 41.86 2.84 52.97 3.28 58.83 1.89 42.53 2.70
(cid:33) (cid:33) (cid:33) (cid:37) 31.74 2.80 28.70 2.69 32.56 2.78 49.73 3.21 64.71 2.29 41.49 2.75
(cid:33) (cid:33) (cid:33) (cid:33) 34.13 2.93 32.87 2.92 44.18 3.14 63.20 3.55 70.59 2.30 48.94↑15% 2.97↑10%
+6.41 +0.27
Table8: Ablationstudiesformodelingdifferentdatasetsandmodalities. Thetoptableisfor
motion and the bottom table is for video. Unpair refers to using unpaired instruction datasets,
includingH3DQA,BabelQA,Video-ChatGPTinstructiondatasets,whilePairmeansusingMotion-
XQAtodoinstructiontuning.
videosusingourMotionX-QA,themotionbranchwillcompromisetothislimitationandbeaffected.
Recentprogress[51]invideocompressionmightbeapromisingfashiontorevealthis.
Ablationonvideounderstanding. AscanbeseeninthebottompartoftheTable8,incorporating
unpairedmotiondatasetssuchasH3DQAandBABEL-QAhasproventoenhancethesequential
perceptioncapabilitiesforthevideobranchsignificantly. Theimprovementofothercapabilitiesis
limited. ThemodesteffectismainlyduetothelimitedamountofH3DQAandBABEL-QAdata.
Uponconductingadditionalinstructiontuningwithourpaireddataset,Motion-XQA,weobserved
holistic enhancements in all five aspects, culminating in a notable 17% improvement in overall
accuracy. Itindicatestheeffectivenessofjointtrainingwithpairedmotion-videodata,enablingthe
modeltomoreadeptlyutilizemotioncuesandenhancetheintegrationbytransferringinformation
acrossthedifferentmodalities.
5 ConclusionandDiscussion
Conclusion. Inthiswork,wehavepresentedMotionLLM,aunifiedframeworkforhumanbehavior
understanding,focusingonhumanmotionandvideomodalities. MotionLLMintroducesanLLM-
based framework to bridge the gap among motions, videos, and languages. To empower good
spatial-temporalunderstandingandreasoningcapabilities,weconstructedaMoViddatasettoinclude
diversequestion-answerpairsofmotionsandvideosonspatial-temporalunderstandingandreasoning.
WealsodevelopedaMoVid-Benchtoevaluatetheunderstandingcapabilityofmodelsonhuman
behaviors. Experimentsshowtheeffectivenessofbothourmethodsanddatasetsonfine-grained
humanbehaviorunderstanding.
LimitationandImpactStatement. Thisworksuffersfromthelimitedcapacityofthevideoencoder.
Futureworkmayconsiderimprovingthecapacityofvideoencoders. MotionLLMispromisingto
serveasanAIassistantinmanyscenarios,likeafitnesscoachforsocialgoods,especiallyforthe
visuallyimpairedcommunity. Forthenegativeimpact,thedevelopmentofLLMsmightraisethe
possibilityofnegativeuseofourmodel,suchasnegativecontentonsocialmedia.
Acknowledgement
The author team would like to deliver many thanks to many people. Qing Jiang helps a lot with
somepartsofmanualannotationonMoVidBenchandresolvessomeethicsissuesofMotionLLM.
Jingcheng Hu provided some technical suggestions for efficient training. Shilong Liu and Bojia
ZiprovidedsomesignificanttechnicalsuggestionsonLLMtuning. JialeLiu,WenhaoYang,and
ChenlaiQianprovidedsomesignificantsuggestionsforustopolishthepaper. HongyangLihelped
usalotwiththefiguredesign. YirenPangprovidedGPTAPIkeyswhenourkeysweretemporarily
outofquota.
12References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, and Songhwai Oh. Text2action:
Generativeadversarialsynthesisfromlanguagetoaction. InICRA,2018.
[3] ChaitanyaAhujaandLouis-PhilippeMorency. Language2pose: Naturallanguagegrounded
poseforecasting. In3DV,2019.
[4] LightningAI. Lit-gpt. https://github.com/Lightning-AI/lit-gpt,2023.
[5] Bobby Bodenheimer, Chuck Rose, Seth Rosenthal, and John Pella. The process of motion
capture: Dealingwiththedata. InEGWorkshop,pages3–18.Springer,1997.
[6] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. NeurIPS,33:1877–1901,2020.
[7] FabianCabaHeilbron,VictorEscorcia,BernardGhanem,andJuanCarlosNiebles. Activitynet:
Alarge-scalevideobenchmarkforhumanactivityunderstanding. InCVPR,pages961–970,
2015.
[8] GuoChen,Yin-DongZheng,JiahaoWang,JilanXu,YifeiHuang,JuntingPan,YiWang,Yali
Wang,YuQiao,TongLu,etal.Videollm:Modelingvideosequencewithlargelanguagemodels.
arXivpreprintarXiv:2305.13292,2023.
[9] JunChen,DeyaoZhu,KilichbekHaydarov,XiangLi,andMohamedElhoseiny. Videochatcap-
tioner: Towardstheenrichedspatiotemporaldescriptions. arXivpreprintarXiv:2304.04227,
2023.
[10] Ling-HaoChen,JiaweiZhang,YewenLi,YirenPang,XiaoboXia,andTongliangLiu. Hu-
manmac: Masked motion completion for human motion prediction. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,pages9544–9555,2023.
[11] XinChen,BiaoJiang,WenLiu,ZilongHuang,BinFu,TaoChen,JingyiYu,andGangYu.
Executingyourcommandsviamotiondiffusioninlatentspace. CVPR,2023.
[12] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna:
Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality,March2023.
[13] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian Theobalt.
Mofusion: A framework for denoising-diffusion-based motion synthesis. In CVPR, pages
9760–9770,2023.
[14] WenxunDai, Ling-HaoChen, JingboWang, JinpengLiu, BoDai, andYansongTang. Mo-
tionlcm: Real-timecontrollablemotiongenerationvialatentconsistencymodel. arXivpreprint
arXiv:2404.19759,2024.
[15] GingerDelmas,PhilippeWeinzaepfel,ThomasLucas,FrancescMoreno-Noguer,andGrégory
Rogez. Posescript: 3dhumanposesfromnaturallanguage. InECCV,pages346–362.Springer,
2022.
[16] MarkEndo,JoyHsu,JiamanLi,andJiajunWu. Motionquestionansweringviamodularmotion
programs. ICML,2023.
[17] AninditaGhosh,NoshabaCheema,CennetOguz,ChristianTheobalt,andPhilippSlusallek.
Synthesisofcompositionalanimationsfromtextualdescriptions. InICCV,2021.
[18] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask:
Generativemaskedmodelingof3dhumanmotions. CVPR,2024.
13[19] ChuanGuo,ShihaoZou,XinxinZuo,SenWang,WeiJi,XingyuLi,andLiCheng. Generating
diverseandnatural3dhumanmotionsfromtext. InCVPR,2022.
[20] ChuanGuo,XinxinZuo,SenWang,andLiCheng. Tm2t: Stochasticandtokenizedmodeling
forthereciprocalgenerationof3dhumanmotionsandtexts.InECCV,pages580–597.Springer,
2022.
[21] FangzhouHong,LiangPan,ZhongangCai,andZiweiLiu. Versatilemulti-modalpre-training
forhuman-centricperception. InCVPR,pages16156–16166,2022.
[22] EdwardJHu,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,Weizhu
Chen,etal. Lora: Low-rankadaptationoflargelanguagemodels. InICLR,2021.
[23] YukunHuang,JiananWang,AilingZeng,HeCao,XianbiaoQi,YukaiShi,Zheng-JunZha,and
LeiZhang. Dreamwaltz: Makeascenewithcomplex3danimatableavatars. NeurIPS,36,2024.
[24] ZiweiJi,NayeonLee,RitaFrieske,TiezhengYu,DanSu,YanXu,EtsukoIshii,YeJinBang,
AndreaMadotto,andPascaleFung. Surveyofhallucinationinnaturallanguagegeneration.
ACMComputingSurveys,55(12):1–38,2023.
[25] BiaoJiang,XinChen,WenLiu,JingyiYu,GangYu,andTaoChen. Motiongpt: Humanmotion
asaforeignlanguage. NeurIPS,36,2024.
[26] PengJin,RyuichiTakanobu,CaiwanZhang,XiaochunCao,andLiYuan. Chat-univi: Unified
visualrepresentationempowerslargelanguagemodelswithimageandvideounderstanding.
arXivpreprintarXiv:2311.08046,2023.
[27] TaeryungLee,GyeongsikMoon,andKyoungMuLee. Multiact: Long-term3dhumanmotion
generationfrommultipleactionlabels. InAAAI,volume37,pages1231–1239,2023.
[28] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,JingkangYang,andZiweiLiu. Otter: A
multi-modalmodelwithin-contextinstructiontuning. arXivpreprintarXiv:2305.03726,2023.
[29] ChunyuanLi,CliffWong,ShengZhang,NaotoUsuyama,HaotianLiu,JianweiYang,Tristan
Naumann,HoifungPoon,andJianfengGao. Llava-med: Trainingalargelanguage-and-vision
assistantforbiomedicineinoneday. NeurIPS,36,2024.
[30] HongxiangLi,MengCao,XuxinCheng,ZhihongZhu,YaoweiLi,andYuexianZou.Generating
templatedcaptionforvideogrounding. arXivpreprintarXiv:2301.05997,2023.
[31] HongyangLi,JiehongLin,andKuiJia. Dcl-net: Deepcorrespondencelearningnetworkfor6d
poseestimation. InECCV,pages369–385.Springer,2022.
[32] HongyangLi,HaoZhang,ZhaoyangZeng,ShilongLiu,FengLi,TianheRen,andLeiZhang.
Dfa3d: 3ddeformableattentionfor2d-to-3dfeaturelifting. InICCV,pages6684–6693,2023.
[33] KunChangLi,YinanHe,YiWang,YizhuoLi,WenhaiWang,PingLuo,YaliWang,LiminWang,
andYuQiao. Videochat: Chat-centricvideounderstanding. arXivpreprintarXiv:2305.06355,
2023.
[34] KunchangLi,YaliWang,YinanHe,YizhuoLi,YiWang,YiLiu,ZunWang,JilanXu,Guo
Chen,PingLuo,etal.Mvbench:Acomprehensivemulti-modalvideounderstandingbenchmark.
CVPR,2024.
[35] BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava: Learningunited
visualrepresentationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
[36] JingLin,AilingZeng,ShunlinLu,YuanhaoCai,RuimaoZhang,HaoqianWang,andLeiZhang.
Motion-x: Alarge-scale3dexpressivewhole-bodyhumanmotiondataset. NeurIPS,36,2024.
[37] KevinLin,FaisalAhmed,LinjieLi,Chung-ChingLin,EhsanAzarnasab,ZhengyuanYang,Jian-
fengWang,LinLiang,ZichengLiu,YumaoLu,etal. Mm-vid: Advancingvideounderstanding
withgpt-4v(ision). arXivpreprintarXiv:2310.19773,2023.
14[38] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.NeurIPS,
36,2024.
[39] ShilongLiu,HaoCheng,HaotianLiu,HaoZhang,FengLi,TianheRen,XueyanZou,Jianwei
Yang,HangSu,JunZhu,etal. Llava-plus: Learningtousetoolsforcreatingmultimodalagents.
arXivpreprintarXiv:2311.05437,2023.
[40] Yunze Liu, Changxi Chen, and Li Yi. Interactive humanoid: Online full-body motion re-
action synthesis with social affordance canonicalization and forecasting. arXiv preprint
arXiv:2312.08983,2023.
[41] MatthewLoper,NaureenMahmood,JavierRomero,GerardPons-Moll,andMichaelJ.Black.
SMPL:Askinnedmulti-personlinearmodel. TOG,34(6):248:1–248:16,October2015.
[42] ShunlinLu,Ling-HaoChen,AilingZeng,JingLin,RuimaoZhang,LeiZhang,andHeung-
Yeung Shum. Humantomato: Text-aligned whole-body motion generation. arXiv preprint
arXiv:2310.12978,2023.
[43] ShunlinLu,Ling-HaoChen,AilingZeng,JingLin,RuimaoZhang,LeiZhang,andHeung-
Yeung Shum. Humantomato: Text-aligned whole-body motion generation. arXiv preprint
arXiv:2310.12978,2023.
[44] RuipuLuo,ZiwangZhao,MinYang,JunweiDong,MinghuiQiu,PengchengLu,TaoWang,
andZhongyuWei. Valley: Videoassistantwithlargelanguagemodelenhancedability. arXiv
preprintarXiv:2306.07207,2023.
[45] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan. Video-chatgpt:
Towardsdetailedvideounderstandingvialargevisionandlanguagemodels. arXivpreprint
arXiv:2306.05424,2023.
[46] NaureenMahmood,NimaGhorbani,NikolausFTroje,GerardPons-Moll,andMichaelJBlack.
Amass: Archiveofmotioncaptureassurfaceshapes. InICCV,2019.
[47] ThomasBMoeslund,AdrianHilton,andVolkerKrüger. Asurveyofadvancesinvision-based
humanmotioncaptureandanalysis. CVIU,104(2-3):90–126,2006.
[48] Naila Murray, Luca Marchesotti, and Florent Perronnin. Ava: A large-scale database for
aestheticvisualanalysis. InCVPR,pages2408–2415.IEEE,2012.
[49] MunanNing,BinZhu,YujiaXie,BinLin,JiaxiCui,LuYuan,DongdongChen,andLiYuan.
Video-bench:Acomprehensivebenchmarkandtoolkitforevaluatingvideo-basedlargelanguage
models. arXivpreprintarXiv:2311.16103,2023.
[50] OpenAI. ChatGPTbyopenai,2022.
[51] OpenAI. Videogenerationmodelsasworldsimulators,2024.
[52] LiangPan,JingboWang,BuzhenHuang,JunyuZhang,HaofanWang,XuTang,andYangang
Wang. Synthesizingphysicallyplausiblehumanmotionsin3dscenes. 3DV,2024.
[53] YulinPan, XiangtengHe, BiaoGong, YiliangLv, YujunShen, YuxinPeng, andDeliZhao.
Scanning only once: An end-to-end framework for fast temporal grounding in long videos.
ICCV,2023.
[54] Xiaogang Peng, Yiming Xie, Zizhao Wu, Varun Jampani, Deqing Sun, and Huaizu Jiang.
Hoi-diff: Text-drivensynthesisof3dhuman-objectinteractionsusingdiffusionmodels. arXiv
preprintarXiv:2312.06553,2023.
[55] MathisPetrovich,MichaelJBlack,andGülVarol. Temos: Generatingdiversehumanmotions
fromtextualdescriptions. InECCV,2022.
[56] MatthiasPlappert,ChristianMandery,andTamimAsfour. Learningabidirectionalmapping
betweenhumanwhole-bodymotionandnaturallanguageusingdeeprecurrentneuralnetworks.
RAS,109:13–26,2018.
15[57] AmirShahroudy,JunLiu,Tian-TsongNg,andGangWang. Nturgb+d: Alargescaledataset
for3dhumanactivityanalysis. InCVPR,2016.
[58] LeiShi,YifanZhang,JianCheng,andHanqingLu. Two-streamadaptivegraphconvolutional
networksforskeleton-basedactionrecognition. InCVPR,pages12026–12035,2019.
[59] XuShi,ChuanchenLuo,JunranPeng,HongwenZhang,andYunlianSun. Generatingfine-
grainedhumanmotionsusingchatgpt-refineddescriptions. arXivpreprintarXiv:2312.02772,
2023.
[60] YayaShi,HaiyangXu,ChunfengYuan,BingLi,WeimingHu,andZheng-JunZha. Learning
video-textalignedrepresentationsforvideocaptioning. TMM,19(2):1–21,2023.
[61] Yukai Shi, Jianan Wang, CAO He, Boshi Tang, Xianbiao Qi, Tianyu Yang, Yukun Huang,
ShilongLiu,LeiZhang,andHeung-YeungShum. Toss: High-qualitytext-guidednovelview
synthesisfromasingleimage. InICLR,2023.
[62] EnxinSong,WenhaoChai,GuanhongWang,YuchengZhang,HaoyangZhou,FeiyangWu,
XunGuo,TianYe,YanLu,Jenq-NengHwang,etal. Moviechat: Fromdensetokentosparse
memoryforlongvideounderstanding. CVPR,2024.
[63] JiankaiSun,ChuanyangZheng,EnzeXie,ZhengyingLiu,RuihangChu,JianingQiu,Jiaqi
Xu,MingyuDing,HongyangLi,MengzheGeng,etal. Asurveyofreasoningwithfoundation
models. arXivpreprintarXiv:2312.11562,2023.
[64] WataruTakanoandYoshihikoNakamura. Statisticalmutualconversionbetweenwholebody
motionprimitivesandlinguisticsentencesforhumanmotions. IJRR,34(10):1314–1328,2015.
[65] GuyTevet,BrianGordon,AmirHertz,AmitHBermano,andDanielCohen-Or. Motionclip:
Exposinghumanmotiongenerationtoclipspace. InECCV,2022.
[66] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[67] DanielVlasic,RolfAdelsberger,GiovanniVannucci,JohnBarnwell,MarkusGross,Wojciech
Matusik,andJovanPopovic´.Practicalmotioncaptureineverydaysurroundings.TOG,26(3):35–
es,2007.
[68] JingboWang,YuRong,JingyuanLiu,SijieYan,DahuaLin,andBoDai. Towardsdiverseand
naturalscene-aware3dhumanmotionsynthesis. InCVPR,pages20460–20469,2022.
[69] JingboWang,YeYuan,ZhengyiLuo,KevinXie,DahuaLin,UmarIqbal,SanjaFidler,and
SamehKhamis. Learninghumandynamicsinautonomousdrivingscenarios. InICCV,pages
20796–20806,2023.
[70] JunkeWang,DongdongChen,ChongLuo,XiyangDai,LuYuan,ZuxuanWu,andYu-Gang
Jiang. Chatvideo: A tracklet-centric multimodal and versatile video understanding system.
arXivpreprintarXiv:2304.14407,2023.
[71] WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,Zhuoyi
Yang,LeiZhao,XixuanSong,etal. Cogvlm: Visualexpertforpretrainedlanguagemodels.
arXivpreprintarXiv:2311.03079,2023.
[72] ZhanyuWang,LongyueWang,ZhenZhao,MinghaoWu,ChenyangLyu,HuayangLi,Deng
Cai,LupingZhou,ShumingShi,andZhaopengTu. Gpt4video: Aunifiedmultimodallarge
languagemodelforlnstruction-followedunderstandingandsafety-awaregeneration. arXiv
preprintarXiv:2311.16511,2023.
[73] YangWu,ShilongWang,HaoYang,TianZheng,HongboZhang,YanyanZhao,andBingQin.
Anearlyevaluationofgpt-4v(ision). arXivpreprintarXiv:2310.16534,2023.
[74] Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai, Dahua Lin, and
JiangmiaoPang. Unifiedhuman-sceneinteractionviapromptedchain-of-contacts. ICLR,2024.
16[75] YimingXie,VarunJampani,LeiZhong,DeqingSun,andHuaizuJiang. Omnicontrol: Control
anyjointatanytimeforhumanmotiongeneration. ICLR,2024.
[76] TatsuroYamada,HiroyukiMatsunaga,andTetsuyaOgata. Pairedrecurrentautoencodersfor
bidirectionaltranslationbetweenrobotactionsandlinguisticdescriptions. RA-L,3(4):3441–
3448,2018.
[77] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan
Laptev,JosefSivic,andCordeliaSchmid. Vid2seq: Large-scalepretrainingofavisuallanguage
modelfordensevideocaptioning. InCVPR,pages10714–10726,2023.
[78] JieYang, BingliangLi, FengyuYang, AilingZeng, LeiZhang, andRuimaoZhang. Boost-
ing human-object interaction detection with text-to-image diffusion model. arXiv preprint
arXiv:2305.12252,2023.
[79] ZhengyuanYang,LinjieLi,KevinLin,JianfengWang,Chung-ChingLin,ZichengLiu,and
LijuanWang. Thedawnoflmms: Preliminaryexplorationswithgpt-4v(ision). arXivpreprint
arXiv:2309.17421,9(1):1,2023.
[80] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,
Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large
languagemodelswithmultimodality. CVPR,2024.
[81] EnYu,LiangZhao,YanaWei,JinrongYang,DongmingWu,LingyuKong,HaoranWei,Tiancai
Wang,ZhengGe,XiangyuZhang,andWenbingTao. Merlin: Empoweringmultimodalllms
withforesightminds. arXivpreprintarXiv:2312.00589,2023.
[82] HuanyuYu,ShuoCheng,BingbingNi,MinsiWang,JianZhang,andXiaokangYang. Fine-
grainedvideocaptioningforsportsnarrative. CVPR,pages6006–6015,2018.
[83] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao.
Activitynet-qa: A dataset for understanding complex web videos via question answering.
InAAAI,volume33,pages9127–9134,2019.
[84] RujingYue,ZhiqiangTian,andShaoyiDu. Actionrecognitionbasedonrgbandskeletondata
sets: Asurvey. Neurocomputing,2022.
[85] HangZhang,XinLi,andLidongBing.Video-llama:Aninstruction-tunedaudio-visuallanguage
modelforvideounderstanding. arXivpreprintarXiv:2306.02858,2023.
[86] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang,
JianfengGao,LeiZhang,ChunyuanLi,etal. Llava-grounding: Groundedvisualchatwith
largemultimodalmodels. arXivpreprintarXiv:2312.02949,2023.
[87] HaoZhang,HongyangLi,XingyuLiao,FengLi,ShilongLiu,LionelMNi,andLeiZhang.
Da-bev: Depthawarebevtransformerfor3dobjectdetection. arXivpreprintarXiv:2302.13002,
2023.
[88] JianrongZhang,YangsongZhang,XiaodongCun,ShaoliHuang,YongZhang,HongweiZhao,
HongtaoLu,andXiShen. T2m-gpt: Generatinghumanmotionfromtextualdescriptionswith
discreterepresentations. CVPR,2023.
[89] MingyuanZhang,ZhongangCai,LiangPan,FangzhouHong,XinyingGuo,LeiYang,and
ZiweiLiu. Motiondiffuse: Text-drivenhumanmotiongenerationwithdiffusionmodel. arXiv
preprintarXiv:2208.15001,2022.
[90] ShaokunZhang,XiaoboXia,ZhaoqingWang,Ling-HaoChen,JialeLiu,QingyunWu,and
TongliangLiu. Ideal: Influence-drivenselectiveannotationsempowerin-contextlearnersin
largelanguagemodels. arXivpreprintarXiv:2310.10873,2023.
[91] Yue Zhao, Long Zhao, Xingyi Zhou, Jialin Wu, Chun-Te Chu, Hui Miao, Florian Schroff,
HartwigAdam,TingLiu,BoqingGong,etal. Distillingvision-languagemodelsonmillionsof
videos. arXivpreprintarXiv:2401.06129,2024.
17[92] WenyangZhou,ZhiyangDou,ZeyuCao,ZhouyingchengLiao,JingboWang,WenjiaWang,
YuanLiu,TakuKomura,WenpingWang,andLingjieLiu. Emdm: Efficientmotiondiffusion
modelforfast,high-qualitymotiongeneration. arXivpreprintarXiv:2312.02256,2023.
[93] Zixiang Zhou, Yu Wan, and Baoyuan Wang. Avatargpt: All-in-one framework for motion
understanding,planning,generationandbeyond. CVPR,2024.
[94] ZixiangZhouandBaoyuanWang. Ude: Aunifieddrivingengineforhumanmotiongeneration.
InCVPR,pages5632–5641,2023.
[95] BinZhu, BinLin, MunanNing, YangYan, JiaxiCui, HongFaWang, YatianPang, Wenhao
Jiang,JunwuZhang,ZongweiLi,etal. Languagebind: Extendingvideo-languagepretraining
ton-modalitybylanguage-basedsemanticalignment. ICLR,2024.
18Appendix for
MotionLLM: Understanding Human Behaviors
from Human Motions and Videos
A MoreComparisonsontheMoVid-Bench
WecompareMotionLLMwithmorevideo-basedLLMsinthissection. Duetothepagelimits,we
leavemorecompassionresultsonMoVid-Benchintheappendix. Alltheseresultsaretheaverageof
threeevaluations.
Overall,MotionLLMobtainsstate-of-the-artresultsonourMoVid-Benchvideopartindicatingthe
effectivenessofourmodelandarchitecturedesign. Inaddition,VideoChat2[34]couldachievethe
bestonthebodydescriptionpart,whileourMotionLLMcouldachievethebestonotherparts. Due
to our joint training with motion data, our model could get substantial improvement in direction
perceptionandreasoningaspects.
Body. Seq. Dir. Rea. Hall. All
Acc. Score Acc. Score Acc. Score Acc. Score Acc. Score Acc. Score
GT 100.00 5.00 100.00 5.00 100.00 5.00 100.00 5.00 100.00 5.00 100.00 5.00
GPT-3.5[6] 2.40 1.23 1.39 1.00 4.65 1.09 5.41 1.65 0.00 0.94 3.03 1.26
Video-LLAVA[35] 33.53 2.76 25.46 2.72 41.86 2.84 52.97 3.28 58.83 1.89 42.53 2.70
Video-LLaMA[85] 32.90 2.81 28.20 2.81 41.87 2.95 59.46 3.42 53.95 1.89 43.28 2.77
VideoChat[33] 41.21 2.93 28.21 2.81 32.73 2.78 46.34 3.15 62.13 2.21 42.12 2.79
VideoChat2[34] 43.11 3.11 31.60 2.91 34.88 3.05 48.65 3.22 64.71 2.12 44.59 2.88
MotionLLM 34.13 2.93 32.87 2.92 44.18 3.14 63.20 3.55 70.59 2.30 49.00 2.97
Table9: MorecomparisonsontheMoVid-Bench(videopart). Thelargertheaccuracyandscore,
thebettertheresult.
19B TechnicalDetails
B.1 ImplementationDetails
Model training. During the first stage of modality translation, we trained both the motion and
videotranslatorsontheNVIDIATeslaA100-80GBGPU,utilizingtheAdamWoptimizerwitha
weight decay of 0.01. The motion translator underwent training for 40k iterations, whereas the
videotranslatorwastrainedfor70kiterationstoaccommodatethevaryingamountsofdataintheir
respectivedatasets. Inthesecondstage,forthemotion-videounifiedinstructiontuning,wetrained
theLoRAandthetwotranslatorsontheNVIDIAA100-80GBGPUwithabatchsizeof2oneach
GPUforasingleepoch,requiring96hours. ThetrainingstillemployedtheAdamWoptimizerwitha
weightdecayof0.01. Fortrainingonunpaireddatasets,wesampledonlyonemodalityperbatch,
ensuringthatallsampleswithinabatchbelongedtothesamemodality. Conversely,forthepaired
datasetstraining(specificallyforMotionX-QA),eachbatchcontainedonemotioninstructionQA
andonevideoinstructionQA.
Modelinference. AlltestingandinferencetaskswereperformedonasingleNVIDIAA100-80GB
GPU.Werepeatedallteststhreetimestocalculatethemeanresults.
B.2 EvaluationDetails
Our MoVid-Bench evaluation extends the evaluation protocol of previous multi-modality LLMs
evaluations[35]. TheevaluationofGPTwillreturnadictionaryofpredictionsandascore. The
detailsoftheevaluationpromptareshowninTable10.
Input: question,answer,prediction
LLMevaluationprompts:
Youareanintelligentchatbotdesignedforevaluatingthecorrectnessofgenerativeoutputsfor
question-answerpairs.
Yourtaskistocomparethepredictedanswerwiththecorrectansweranddetermineiftheymatch
meaningfully.Here’showyoucanaccomplishthetask:
——
##INSTRUCTIONS:
-Focusonthemeaningfulmatchbetweenthepredictedanswerandthecorrectanswer.
-Considersynonymsorparaphrasesasvalidmatches.
-Evaluatethecorrectnessofthepredictioncomparedtotheanswer.
Pleaseevaluatethefollowingvideo-basedquestion-answerpair:
Question:{question}
CorrectAnswer:{answer}
PredictedAnswer:{prediction}
Provideyourevaluationonlyasayes/noandscorewherethescoreisanintegervaluebetween0and
5,with5indicatingthehighestmeaningfulmatch.
PleasegeneratetheresponseintheformofaPythondictionarystringwithkeys’pred’and’score’,
wherevalueof’pred’isastringof’yes’or’no’andvalueof’score’isinINTEGER,notSTRING.
DONOTPROVIDEANYOTHEROUTPUTTEXTOREXPLANATION.OnlyprovidethePython
dictionarystring.
Forexample,yourresponseshouldlooklikethis:{’pred’:’yes’,’score’:4.8}.
Table10: GPTevaluationprompts.
20C DatasetConstructionofMoVid
C.1 ConstructingH3DQAandMotion-XQADataset
PromptsandResponse. WedetailatemplatepromptweusetogeneratetheH3DQA,andMotion-
XQAinstructiontuningdataset. AsshowninTable11,ourpromptcomesupwithourrequirements
andsomein-contextexamples. TheMotion-XQAinstructiontuningdatasetconstructionissimilar
totheannotationprocessofH3DQA.
Constructionpipelineanddatasetpost-processing. ToobtaintheQAsaccordingtotheprompt
in Table 11, we concatenate the prompt and the motion caption as input for GPT-4. For post-
processingtheobtainedresponsestring,weprocessthestringtoquestion-answerpairsviaalanguage
parser. Weignoretheseveryfewcasesforresponsesnotalignedwiththepromptcommand. The
wholeannotationprocessisdetailedinAlgorithm1.
C.2 Motion-XRecaptionUsingGPT-4V
AsthetextualannotationoftheexistingMotion-Xdatasetistoocoarse,werelabelthecaptionof
theMotion-Xdataset. AsvideosintheMotion-Xdatasetarepairwisewithmotions,therelabelled
caption can also used as the motion captions. As shown in Table 12, our prompt comes up with
ourrequirements,wherethegeneral_descriptionisthevanillacoarsecaptionofMotion-Xdata.
Besides,theimagesfromthevideowitha15×down-samplingratearealsofedintotheGPT-4V.
TheannotationprocessisshowninAlgorithm2.
Algorithm1GPT-4InstructionTuningDatasetConstruction.
Input: prompts,motion_captions.
Output: QApairsanswer.
input_string=prompts+motion_captions
completion=openai.ChatCompletion.create(
model=“gpt-4”,
messages=[
{“role”: “user”,“content”: input_string},
]
)
answer_string=completion.choices[0].message["content"]
answer=parser(answer_string)
Returnanswer
Algorithm2GPT-4RecaptionontheMotion-X
Input: prompts,images.
Output: caption.
content=[]
content.append(“type”: “text”,“text’: prompts)
content.append(“type”: “image_url”, “text”: “image_url”: {“url”:
f“data:image/jpeg;base64,{resize_image(down_sample(images),width,height)”}})
messages=[{
“role”: “user”,
“content”: content
}]
payload={
“model”: “gpt-4-vision-preview”,
“messages”: messages,
}
response = requests.post(f“api_base/chat/completions”, headers=headers, json=payload,
proxies=proxies)
caption=response.json()[“choices”][0][“message”][“content”]
Returncaption
21Promptexample:
ThisismultipledescriptionsofONEmotionsequence,eachlineisadescription,andthelasttwo
numbersarethestartingtime.PleaseconstructseveralQApairsbasedonthis.Iftheyareall0.0,it
representstheentiresequence,otherwise,itcorrespondstothecorrespondingstartingtime.
!Notethatthestartingtimeofeachrowonlycorrespondstothedescriptionofthisrow,andhas
nothingtodowiththedescriptionsofotherrows.Avoidambiguity.
!Notethatyoucanonlyseetheaction,notthedescriptionsinadvance.!Therefore,youcanNOTask
oranswersomethinglike“thefirst/thirddescription”
!NotethatDon’tsayanythinglike‘sure’or‘hereisxxx’,justreturnQAtomedirectly.
Don’trigidlyimitatethetemplateeither
**HEREISANEXAMPLE:**
[GIVENDESCRIPTION]
apersonwalkstowardthefront,turnstotheright,bouncesintoasquat,andplacesbotharmsinfront
ofchestbeforeplacingthemontheknees.#0.0#5.0
personwalksupandsquatsslightlytoposeaposition#0.0#0.0
hemovedforwardthenstretchedhisbody,movinghishandandtouchinghiskneeswithhandsand
nowheturnedtotheleftside.#0.0#0.0
apersonwalks,turnsslightlytotheright,squats,putshandonbothkneeswhilesquatting,andthen
squatsagain.#0.0#8.0
[SystemOutput]:
**Q**Whatisthemotivationbehindthepersonperformingthesemotions?
**A**Withoutadditionalcontext,itisnotpossibletodeterminetheexactmotivation.Thesequence
couldbepartofanexerciseroutine,amovie,oraspecificfunctionalmovement.
**Q**TrueorFalse:Thepersontouchestheirkneeswiththeirhandsimmediatelyafterwalking.
**A**False.Hestretcheshisbodybeforetouchinghisknees.
**Q**Whatdoesthemandoafterwalkingforward?
**A**Theguyturnstotheright.
**Q**Andthen?
**A**Hebouncesintoasquat.
**Q**Howmanytimesdoesthemandothis?
**A**twice.
**Q**Canyoudescribetheinitialmovementofthepersoninthissequenceandthetimingassociated
withit?Isthereanyrepetitioninthesequence,andhowdoesthisaffecttheoverallflow?
**A**Initially,thepersonbeginsthesequencebywalkingforward.Thetimingforthisinitial
movementismarkedas0.0,indicatingthatitisthebeginningofthesequence.Theanswertothe
secondquestionisYES.Thereisarepetitionofthesquattingmovementwithinthesequence.This
repetitionemphasizesthesquatasacoreelementofthesequence,reinforcingitsimportance.Italso
addsarhythmicaspecttothemovement,creatingapatternthatcouldbeaestheticallypleasingor
functionallysignificantintermsofexerciseordanceroutines.
**Q**Couldyouelaborateonthearmmovementsofthepersonduringthesquatandtheir
significanceinthesequence?
**A**Whileinthesquatposition,thepersonexecutesadetailedarmmovement.Initially,theyplace
botharmsinfrontoftheirchest,whichcouldbeinterpretedasapreparatoryorbalancingaction.
Subsequently,theyshifttheirarmstorestontheirknees.Thistransitioninarmpositioningiscrucial
asitdemonstratescoordinationandcontrol,integratingtheupperbodyintothesequence,whichwas
initiallyfocusedonlowerbodymovements.
Table11: Anexampleofinstructiontuningdataset(H3DQA,andMotion-XQA)construction.
Ourpromptcomesupwithourrequirementsandsomein-contextexamples.
Promptexample:
You’llbeshowndifferentframes,whichareuniformlysampledfromonehumanmotionvideo.
Youwillalsobegivenageneraldescriptionofthevideo.Inthisvideo,thegeneraldescriptionis:
{general_description}.
Pleasetellmewhatthepersoninthevideodoinganditsdetaileddescription.
Notice:youneedtocombinealltheframesofthesamemotionvideoandthegivengeneral
descriptionasifyouareactuallyseeingthevideo.
Andgiveadescriptionofthetemporaljointmovementsandspatialbodypartmovements.Your
descriptionshouldfocusmoreontheatomicactionofdifferentbodyparts.Itisnecessarytodescribe
thetemporalsequenceofdifferentbodyparts,suchas:Amantookastepforward,andatthesame
time,thrustforward,thenreturnedtotheoriginalposition.Tellmewhatthepersondoinginlessthan
50words,pleasedon’tberedundant.
Table12: AnexampleofMotion-Xre-captionannotationprocess. Ourpromptcomesupwith
ourrequirements.
22C.3 MoVidDatasetOverallAnnotationProcessSummary
WesummarizethewholeannotationprocessoftheMoViddatasetinFigure9. Asstatedinthemain
paper,weregenerateMotion-XcaptionswithGPT-4VatfirstandaugmentthemasQAsviaGPT-4.
TheH3DQAisaugmentedbasedonHumanML3DcaptionsviaGPT-4,too.
HumanML3D (H3D) H3DQA
caption QAs
GPT-4
Motion-X video Motion-X recaption Motion-XQA
down-sampling images GPT-4V caption QAs
MoVid
Figure9: ThewholeannotationprocessofMoVid.
C.4 MoVidDatasetSamples
WeprovidesomeexamplesofthegeneratedMoViddataset,includingH3DQA,Motion-Xcaption,
andMotion-XQA.
H3DQAdatasamples. Here,wepresentsomesamplesofH3DQAgeneratedbyGPT-4[1]. The
H3DQA includes multi-round question-answer pairs, related to captioning, in-context QAs, and
reasoningdata. PleaserefertoFigure10andFigure11.
23Is the person moving anywhere during this sequence? Q
A No, the person appears to be standing still while raising
their hands.
What is the person's primary action in the described
sequence? Q
- A) Running
- B) Sitting
- C) Laying
- D) Standing
A D) Standing
What is the position of the person's hands in this
sequence? Q
- A) By their side
- B) Crossed in front of them
- C) Above their head
- D) In their pockets
A C) Above their head
Q
How would you describe the movement of the person?
A The person extends their arms upwards above
the head, it could be seen as a form of stretching
or perhaps an expressive gesture.
Figure10: H3DQAsample(example1).
24What does the person do with his left arm in this sequence? Q
A The person raises his left arm as if he is checking the
time on a watch.
What is the person likely checking on his wrist?
- A) A bracelet Q
- B) A watch
- C) A wrist injury
- D) Nothing
A B) A watch
Does the motion sequence involve any other action Q
apart from raising the left hand and checking the time?
A No, the motion sequence only involves raising the left hand
as if checking the time.
Q
How will you describe the sequence in terms of
pacing and rhythm?
A Without additional context, it is not possible to accurately
speak to the pacing and rhythm of the sequence.
Figure11: H3DQAsample(example2).
25Motion-XrecaptionandMotion-XQAdatasamples. WepresentsomesamplesofMotion-Xnew
captiongeneratedbyGPT-4V[1]andMotion-XQAdatageneratedbyGPT-4[1]. TheMotion-X
new caption data shows more high-quality motion caption data than previous annotations. The
Motion-XQAincludesmulti-roundquestion-answerpairs,relatedtocaptioning,in-contextQAs,and
reasoningdata. PleaserefertoFigure12,Figure13,andFigure14.
Vinilla caption: Battle motion Tame Whirlwind kick motion
GPT-4V annotation: The person is performing a sequence of motions to catch a ball.
Starting from a stance with knees bent, the individual leaps, twists mid-air to face the
opposite direction, then lands, preparing to catch. Immediately after landing, the ball is
caught with both hands and brought downward in a controlled motion.
Q
What is the sequence of movements the person is performing in the motion?
A The person starts from a stance with bent knees, leaps, twists mid-air
to face the opposite direction, lands, and then prepares to catch a ball.
Immediately after landing, the ball is caught with both hands and
brought downward in a controlled motion.
What is the first action the person does in the motion? Q
A The person starts by bending their knees for a leap.
What happens after the person twists mid-air in the motion? Q
A After twisting mid-air, the person lands and prepares to catch a ball.
Q
Why does the person twist mid-air in this motion?
A The person twists mid-air to face the opposite direction and
prepare for catching the ball.
Figure12: Motion-XreceptionandMotion-XQA(sample1).
26Vinilla caption: Ways to Wake Up Where Am I
GPT-4V annotation: The person is in bed, gradually waking up with a confused
expression, shifting from a relaxed state to a more alert posture.
Q
What body posture does the person switch to from a relaxed state?
A The person switches from a relaxed state to a more alert posture.
How does the person's posture change over the course of the motion? Q
A The person's posture changes from a relaxed, lying-down state to
a more alert and upright position.
What might cause a confused expression upon waking up? Q
A Waking up from deep sleep, dreaming, or being woken abruptly
could result in a confused expression.
Q
What might the person do next after becoming alert?
A After becoming alert, the person may get out of bed to start
their day, which could involve routine activities such as
brushing teeth, eating breakfast, or getting dressed.
Q
How can waking up gradually benefit the person?
A Waking up gradually can help the individual feel more rested
and avoids the stress induced by sudden alarms. It also
allows the body's biological processes to naturally prepare
for waking, making the transition from sleep to wakefulness
smoother.
Figure13: Motion-XreceptionandMotion-XQA(sample2).
27Vinilla caption: Play Cello
GPT-4V annotation: The person is playing the cello, using her right arm to move the
bow across the strings while her left hand adjusts the pitch by pressing on different
positions of the cello's fingerboard.
Q
What motion is being demonstrated in the video?
A The person is playing the cello.
What role does the right arm play in the motion sequence? Q
A The right arm is used to move the bow across the
strings of the cello.
How does the left hand contribute to the motion sequence? Q
A The left hand adjusts the pitch by pressing on different positions of
the cello's fingerboard.
Q
What is the function of the bow in playing the cello?
A The bow is used to vibrate the strings when it's drawn across
them, producing sound from the cello.
Q
Why does the strings' vibration produce sound in the cello?
A When the bow glides over the strings, it makes them vibrate.
These vibrations resonate within the body of the cello
creating audible sound waves, which we hear as music.
Figure14: Motion-XreceptionandMotion-XQA(sample3).
28C.5 DetailsandDesignPrinciplesofMoVid-Bench
WedetailourdesignprinciplesofMoVid-Bench. Asstatedinthemainpaper,ourMoVid-Bench
benchmarkmainlyfocusesonevaluatingbody-partmotionawareness(Body.),sequentialanalysis
ability (Seq.), direction awareness (Dir.), reasoning ability (Rea.) [63], and robustness against
hallucination(Hall.)[24],respectively. Thebody-partmotionawarenessanddirectionawarenessaim
toevaluatethespatialunderstandingabilityofhumanmotions. Thesequentialanalysisfocuseson
thetemporalcomprehensionabilityofthemodel. Thereasoningabilityisabasicevaluationofthe
LLMsandanalysisoftheintelligenceofmodels. ThehallucinationisarevelationofLLM-based
models,whichmainlyrelyonthecapabilityofbasedLLMs.
Notethatallexampleshavebeenmanuallyannotatedandcheckedcarefully,whichensuresthequality
andfairnessofourevaluation.WeprovidesomeexamplesoftheseaspectsofmotionpartinFigure15,
Figure16, Figure17, Figure18, andFigure19, respectively. ThevideopartofMoVid-Benchis
designedsimilarlytothemotionpart.
True or False: The person only moves their Q
right arm during the sequence.
A True.
In what way does the person move their head Q
as part of the performance?
A The person moves their head around in a circle as
part of the performance.
Figure15: MoVid-Benchsamples(body-partmotionawareness).
29How many steps does the person take to complete the circle? Q
A The person takes eight steps to complete the circle.
After clapping, what does the person do? Q
A The person to sit down.
Figure16: MoVid-Benchsamples(sequentialanalysisability).
What is the repeated pattern in the person's movements? Q
A The pattern of movement is a jog to the right, a jog to
the left, then back to the middle.
Which direction does the person turn to after walking? Q
A He turns 180 degrees and walks back the way he came.
Figure17: MoVid-Benchsamples(directionawareness).
30Is the person walking on a flat surface or Q
inclined plane?
A Yes.
What can be inferred about the portrayed Q
motion sequence primarily involving?
A The portrayed motion sequence primarily involves defensive
techniques like kicking, punching, hopping between foots,
and dodging to left side.
Figure18: MoVid-Benchsamples(reasoningability).
Could this motion sequence be a part of Q
any exercise or is it just random movements?
A Without additional context, it's difficult to determine
whether this movement sequence is part of an
exercise routine or simply random movements.
What does the person appear to put a top on? Q
A It do not provide specific details on what the
person puts a top on.
Figure19: MoVid-Benchsamples(robustnessagainsthallucination).
31D VisualizationComparisononDatasetUsage
Wecomparetheeffectofdatausageonbothmodalitiesviavisualizationresults. Toexplorehow
motionandvideoshelpeachothertoboosthumanbehaviors,weshowvisualizationcomparisons
besidesnumericcomparisonsinthemainpaper.
D.1 ComparisononWhethertoUseMotionData
Weexplorehowmotiondatahelpsvideounderstandingonhumanbehaviorsviavisualizationresults.
AsshowninTable13andTable14,motiondataalleviatestheproblemofmodelhallucinationwhile
describing the content of motions more accurately. This is mainly due to the dense information
densityofmotiondata.
Human:describethemotion
<VIDEO>
<VIDEO>
MotionLLM(w/o Motion data):Themaninthevideoisseenmovinghisarmsandlegs
aroundinafluidmotion,suggestingthatheisperformingadanceroutineoramartialartsroutine.
(a)MotionLLMw/oMotiondatatraining(videoonly).
Human:describethemotion
<VIDEO>
<VIDEO>
MotionLLM(w/o Motion data):Themanishittingrecurringpadswithhishandsandfeet,
kickingandthrowinghisbodys.Hejumpsandswingshisbodybackandforth.Witheachhit,theman
seemstobegettingfasterandmoreprecise.
(b)MotionLLMw/Motiondatatraining(jointtraining).
Table13: Comparisononwhetherusingmotiondata. Thebluetexthighlightsacomprehensive
descriptionofthemotionandspeedinthegivenvideo,afeatureabsentintrainingthatlacksmotion
data. Trainingwithoutmotiondataprovidesonlyabasicoverview.
32Human:describethebodymovementofthewoman
<VIDEO>
<VIDEO>
MotionLLM(w/o Motion data):Thewomaninthevideoperformsaworkoutroutineona
matusingherlegstosupportherbodybeneathher.Shebeginsherroutinebystretchingherlegsupthe
wallwhilelyingonherback.Shethenproceedstoperformlegraisesandlegsit-upsonhermat.After
finishingherroutine,thewomanstopsandstretchesbeforewalkingawaywithasmileonherface.
(a)MotionLLMw/oMotiondatatraining(videoonly).
Human:describethebodymovementofthewoman
<VIDEO>
<VIDEO>
MotionLLM(w/ Motion data):Thewoman’sbodymovementsareslowandcontrolled,with
herliftingherupperbodyoffthematandherlowerbodyfollowingthemovement.Shemaintainsa
straightbackandneckthroughoutthemovement.
(b)MotionLLMw/Motiondatatraining(jointtraining).
Table14: Comparisononwhetherusingmotiondataforhallucinations.Thebluetextspecifically
detailsthewoman’sbodilymovements,whereasthepinktextmerelydescribeshallucinations,which
significantlydifferfromtheactualcontentofthevideo.
33D.2 ComparisononWhethertoUseVideoData
Weexplorehowvideodatahelpsmotionunderstandingofhumanbehaviorsviavisualizationresults.
As shown in Table 15 and Table 16, motion data can better reason about the environment and
interactioncontentinwhichmotionsoccur. ThisismainlyduetothefactthatMotionLLMcanobtain
interactivemotiongroundsfromalargenumberofvideos.
Human:Formawrittendescriptionthatcorrelateswiththeseriesofhumanmotiontokensprovided.
<MOTION>
<MOTION>
MotionLLM(w/o Video data):Themaniswalkingforward.
Human:Wherehemightbewalking?
MotionLLM(w/o Video data):Forward.
(a)MotionLLMw/ovideodatatraining(motiononly).
Human:Formawrittendescriptionthatcorrelateswiththeseriesofhumanmotiontokensprovided.
<MOTION>
<MOTION>
MotionLLM(w/o Video data):amanwalkingforwardatthesameplace.
Human:Wherehemightbewalking?
MotionLLM(w/o Video data):Onatreadmill.
(b)MotionLLMw/videodatatraining(jointtraining).
Table15: Comparisononwhetherusingvideodata. Videodatahelpstoinfertheenvironment
contentofthe“treadmill”duetothelargenumberofvideogroundsintraining.
34Human:Translatethegivenhumanmotionintoacorrespondingtextualdescription.
<MOTION>
<MOTION>
MotionLLM(w/o Video data):thepersonstandsthereandraisestheirhandsalmosttotheir
face.
Human:Whatmightbethemotivationofit?
MotionLLM(w/o Video data):It’shardtodetermine.
(a)MotionLLMw/ovideodatatraining(motiononly).
Human:Translatethegivenhumanmotionintoacorrespondingtextualdescription.
<MOTION>
<MOTION>
MotionLLM(w/o Video data):Themanraisestherighthand.
Human:Whatmightbethemotivationofit?
MotionLLM(w/o Video data):Itmightbelookingatthewatchontherightwrist.
(b)MotionLLMw/videodatatraining(jointtraining).
Table16: Comparisononwhetherusingvideodata. Theseconddialoguehelpsthecomprehension
andreasoningofthecontentof“lookingwatchontherightwrist”,whichismoreaccurate.
35