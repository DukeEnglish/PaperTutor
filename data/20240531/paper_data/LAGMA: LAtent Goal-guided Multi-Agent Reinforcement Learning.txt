LAGMA: LAtent Goal-guided Multi-Agent Reinforcement Learning
HyunghoNa1 Il-ChulMoon12
Abstract asGoogleResearchFootball(GRF)(Kurachetal.,2020);
learningoptimalpolicytakeslongtime,andtrainedmod-
Incooperativemulti-agentreinforcementlearning
elsevenfailtoachieveacommongoal,suchasdestroying
(MARL),agentscollaboratetoachievecommon
allenemiesinSMACorscoringagoalinGRF.Thus,re-
goals, such as defeating enemies and scoring a
searchers focus on sample efficiency to expedite training
goal. However,learninggoal-reachingpathsto-
(Zhengetal.,2021)andencouragecommittedexploration
wardsuchasemanticgoaltakesaconsiderable
(Mahajanetal.,2019;Yangetal.,2019;Wangetal.,2019).
amountoftimeincomplextasksandthetrained
modeloftenfailstofindsuchpaths. Toaddress Toenhancesampleefficiencyduringtraining,statespace
this,wepresentLAtentGoal-guidedMulti-Agent abstractionhasbeenintroducedinbothmodel-based(Jiang
reinforcementlearning(LAGMA),whichgener- etal.,2015;Zhuetal.,2021;Hafneretal.,2020)andmodel-
atesagoal-reachingtrajectoryinlatentspaceand free settings (Grzes´ & Kudenko, 2008; Tang & Agrawal,
providesalatentgoal-guidedincentivetotransi- 2020;Lietal.,2023). Suchsampleefficiencycanbemore
tions toward this reference trajectory. LAGMA importantinsparserewardsettingssincetrajectoriesinare-
consistsofthreemajorcomponents:(a)quantized playbufferrarelyexperiencepositiverewardsignals. How-
latentspaceconstructedviaamodifiedVQ-VAE ever,suchmethodshavebeenstudiedwithinasingle-agent
forefficientsampleutilization,(b)goal-reaching taskwithoutexpandingtomulti-agentsettings.
trajectorygenerationviaextendedVQcodebook,
Toencouragecommittedexploration,goal-conditionedrein-
and(c)latentgoal-guidedintrinsicrewardgenera-
forcementlearning(GCRL)(Kaelbling,1993;Schauletal.,
tiontoencouragetransitionstowardsthesampled
2015;Andrychowiczetal.,2017)hasbeenwidelyadopted
goal-reachingpath. Theproposedmethodiseval-
inasingleagenttask,suchascomplexpathfindingwitha
uatedbyStarCraftIIwithbothdenseandsparse
sparse reward(Nasiriany etal., 2019; Zhang et al., 2020;
reward settings and Google Research Football.
Chane-Saneetal.,2021;Kimetal.,2023;Leeetal.,2023).
Empiricalresultsshowfurtherperformanceim-
However, GCRLconcepthasalsobeenlimitedlyapplied
provementoverstate-of-the-artbaselines.
tomulti-agentreinforcementlearning(MARL)taskssince
there are various difficulties: 1) a goal is not explicitly
known,onlyasemanticgoalcanbefoundduringtrainingby
1.Introduction
rewardsignal;2)partialobservabilityanddecentralizedex-
Centralizedtraininganddecentralizedexecution(CTDE) ecutioninMARLmakesimpossibletoutilizepathplanning
paradigm(Oliehoeketal.,2008;Guptaetal.,2017)espe- with global information during execution, only allowing
ciallywithvaluefactorizationframework(Sunehagetal., suchplanningduringcentralizedtraining;3)mostMARL
2017;Rashidetal.,2018;Wangetal.,2020a)hasshown tasksseeknottheshortestpath,butthecoordinatedtrajec-
itssuccessonvariouscooperativemulti-agenttasks(Lowe tory,whichrenderssingle-agentpathplanninginGCRLbe
et al., 2017; Samvelyan et al., 2019). However, in more toosimplisticinMARLtasks.
complex tasks with dense reward settings, such as super
Motivatedbymethodsemployedinsingle-agenttasks,we
hardmapsinStarCraftIIMulti-agentChallenge(SMAC)
considerageneralcooperativeMARLproblemasfinding
(Samvelyanetal.,2019)orinsparserewardsettings,aswell
trajectoriestowardsemanticgoalsinlatentspace.
1Korea Advanced Institute of Science and Tech-
Contribution. This paper presents LAtent Goal-guided
nology (KAIST), Daejeon 34141, Republic of Korea.
Multi-Agentreinforcementlearning(LAGMA).LAGMA
2summary.ai, Daejeon, Republic of Korea. Correspondence
to: Hyungho Na <gudgh723@gmail.com>, Il-Chul Moon generatesagoal-reachingtrajectoryinlatentspaceandpro-
<icmoon@kaist.ac.kr>. videsalatentgoal-guidedincentivetotransitiontowardthis
referencetrajectoryduringcentralizedtraining.
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by • ModifiedVQ-VAEforquantizedembeddingspace
theauthor(s). construction: As one measure of efficient sam-
1
4202
yaM
03
]AM.sc[
1v89991.5042:viXraLAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
ple utilization, we use Vector Quantized-Variational states,particularlythoseinproximity. Inaddition,thanks
Autoencoder(VQ-VAE)(VanDenOordetal.,2017) tothediscretizedembeddings,thecount-basedestimation
whichprojectsstatestoaquantizedvectorspacesothat canbeadoptedtoestimatethevalueofstatesprojectedto
acommonlatentcanbeusedasarepresentativefora eachdiscretizedembedding. Then,wegenerateareference
widerangeofembeddingspace. However, statedis- orgoal-reachingtrajectorybasedonthisevaluationinquan-
tributionsinhighdimensionalMARLtasksarequite tizedvectorspaceandprovideanincentivefortransitions
limitedtosmallfeasiblesubspaceunlikeimagegener- thatoverlapwiththisreference.
ationtasks,whoseinputsorstatesoftenutilizeafull
IntrinsicincentiveinRL Inreinforcementlearning,bal-
statespace. Insuchacase,onlyafewquantizedvec-
ancing exploration and exploitation during training is a
torsareutilizedthroughouttrainingwhenadoptingthe
paramount issue (Sutton & Barto, 2018). To encourage
originalVQ-VAE.Tomakequantizedembeddingvec-
a proper exploration, researchers have presented various
torsdistributedproperlyovertheembeddingspaceof
formsofmethodsinasingle-agentcasesuchasmodified
feasiblestates,weproposeamodifiedlearningframe-
count-based methods (Bellemare et al., 2016; Ostrovski
workforVQ-VAEwithanovelcoverageloss.
etal.,2017;Tangetal.,2017),predictionerror-basedmeth-
• Goal-reachingtrajectorygenerationwithextended
ods (Stadie et al., 2015; Pathak et al., 2017; Burda et al.,
VQcodebook: LAGMAconstructsanextendedVQ
2018;Kimetal.,2018),andinformationgain-basedmeth-
codebooktoevaluatethestatesprojectedtoacertain
ods(Mohamed&JimenezRezende,2015;Houthooftetal.,
quantizedvectorandgenerateagoal-reachingtrajec-
2016). Inmostcases,anincentiveforexplorationisintro-
torybasedonthisevaluation. Specifically,duringtrain-
ducedasanadditionalrewardtoaTDtargetinQ-learningor
ing, we store various goal-reaching trajectories in a
aregularizertooveralllossfunctions. Recently,diverseap-
quantizedlatentspace. Then,LAGMAusesthemasa
proachesmentionedearlierhavebeenadoptedinthemulti-
referencetofollowduringcentralizedtraining.
agentenvironmenttopromoteexploration(Mahajanetal.,
• Latentgoal-guidedintrinsicrewardgeneration: To 2019;Wangetal.,2019;Jaquesetal.,2019;Mgunietal.,
encourage coordinated exploration toward reference 2021). Asanexample,EMC(Zhengetal.,2021)utilizes
trajectoriessampledfromtheextendedVQcodebook, episodiccontrol(Lengyel&Dayan,2007;Blundelletal.,
LAGMApresentsalatentgoal-guidedintrinsicreward. 2016)asregularizationforthejointQ-learning,inaddition
Theproposedlatentgoal-guidedintrinsicrewardaims to a curiosity-driven exploration by predicting individual
toaccuratelyestimateTD-targetfortransitionstoward Q-values. Learningwithintrinsicrewardsbecomesmore
goal-reachingpaths,andweprovideboththeoretical importantinsparserewardsettings. However,thisintrinsic
andempiricalsupport. rewardcanadverselyaffecttheoverallpolicylearningifit
is not properly annealed throughout the training. Instead
2.RelatedWorks of generating an additional reward signal solely encour-
agingexploration,LAGMAgeneratesanintrinsicreward
StatespaceabstractionforRL Stateabstractiongroups thatguaranteesamoreaccurateTD-targetforQ-learning,
stateswithsimilarcharacteristicsintoasinglecluster,and yieldingadditionalincentivetowardagoal-reachingpath.
ithasbeeneffectiveinbothmodel-basedRL(Jiangetal.,
Additional related works regarding goal-conditioned re-
2015;Zhuetal.,2021;Hafneretal.,2020)andmodel-free
inforcement learning (GCRL) and subtask-conditioned
settings(Grzes´&Kudenko,2008;Tang&Agrawal,2020).
MARLarepresentedinAppendixC.
NECSA (Li et al., 2023) adopted the abstraction of grid-
based state-action pair for episodic control and achieved
state-of-the-art (SOTA) performance in a general single- 3.Preliminaries
RL task. This approach could relax the limitations of in-
DecentralizedPOMDP Ageneralcooperativemulti-agent
efficient memory usage in the conventional episodic con-
task with n agents can be formalized as the Decentral-
trol,butthisrequiresanadditionaldimensionalityreduction
izedPartiallyObservableMarkovDecisionProcess(Dec-
technique,suchasrandomprojection(Dasgupta,2013)in
POMDP)(Oliehoek&Amato,2016). DecPOMDPconsists
high-dimensionaltasks. Recently,EMU(Naetal.,2024)
ofatupleG=⟨I,S,A,P,R,Ω,O,n,γ⟩,whereI isthefi-
presentedasemanticembeddingforefficientmemoryuti-
nitesetofnagents;s∈Sisthetruestateintheglobalstate
lization, but it still resorts to the episodic buffer, which
spaceS;Aistheactionspaceofeachagent’sactiona form-
requiresstoringboththestatesandtheembeddings. This i
ingthejointactiona ∈ An; P(s′|s,a)isthestatetransi-
additional memory usage could be burdensome in tasks
tionfunctiondeterminedbytheenvironment;Risareward
withlargestatespace. Incontrasttopreviousresearch,we
functionr =R(s,a,s′)∈R;Oistheobservationfunction
employVQ-VAEforstateembeddingandestimatetheover-
generatinganindividualobservationfromobservationspace
all value of abstracted states. In this manner, a sparse or
Ω,i.e.,o ∈ Ω;andfinally,γ ∈ [0,1)isadiscountfactor.
delayedrewardsignalcanbeutilizedbyabroadrangeof i
2LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
(a) VQ-VAE (e) Standard CTED framework
Train 𝑓 𝜙and 𝑓 𝜓via Eq. (6)
Replay
Environment
buffer
Mixing Network
𝑠 𝑡 𝑓 𝜙 𝑓 𝜓 𝑠Ƹ 𝑡 𝜏 𝑠𝑡 𝑄 𝑡𝑜𝑡=𝑓(𝑄 1,𝑄 2,⋯,𝑄 𝑛;𝜃)
Mixing Gradients
𝜏
Coverage 𝜒𝑡
Loss 𝑓 𝜙 Controller
𝑄(∙|𝑜;𝜃)
𝑖 𝑖
𝑅 𝑡 𝑥 𝑡 𝑥 𝑞,𝑡 𝑥 𝑞,𝑡 intrinsic reward 𝑟𝐼
VQ Codebook Goal-reaching trajectory generation Intrinsic reward generation for
𝒟 𝑉𝑄 𝒟 𝑠𝑒𝑞 with VQ Codebook desirable transition via Eq. (8)
𝐶 𝑞⋮ ,𝑡 𝑥⋮ 𝑞,𝑡 𝑧 𝑡 𝒟⋮ 𝐶𝑞,𝑡 𝒟 𝜏⋮ 𝜒𝑡 𝑥 𝑞,𝑡 𝜏 𝜒∗ 𝑡~𝒟 𝜏𝜒𝑡 𝜏 𝜒∗ 𝑡
⋮ ⋮ ⋮ ⋮
(b) VQ Codebook generation (c) Goal-reaching trajectory generation (d) Intrinsic reward generation
Figure1: OverviewofLAGMAframework.(a)VQ-VAEconstructsquantizedvectorspacewithcoverageloss,while(b)
VQcodebookstoresgoal-reachingsequencesfromagivenx . Then,(c)thegoal-reachingtrajectoryiscomparedwiththe
q,t
currentbatchtrajectorytogenerate(d)intrinsicreward. MARLtrainingisdoneby(e)thestandardCTDEframework.
InageneralcooperativeMARLtask,anagentacquiresits asemanticgoal,suchasdefeatingallenemiesinSMACor
localobservationo ateachtimestep,andtheagentselects scoringagoalinGRF.Thus,wedefinegoalstatesandthe
i
anactiona ∈Abasedono . P(s′|s,a)determinesanext goal-reachingtrajectoryincooperativeMARLasfollows.
i i
states′foragivencurrentstatesandthejointactiontaken
byagentsa. Foragiventupleof{s,a,s′},Rprovidesan Definition3.1. (GoalStateandGoal-ReachingTrajectory)
ForagiventaskdependentR andanepisodicsequence
identicalcommonrewardtoallagents.Toovercomethepar- max
T := {s ,a ,r ,s ,a ,r ,...,s }, when ΣT−1r =
tialobservabilityinDecPOMDP,eachagentoftenutilizesa 0 0 0 1 1 1 T t=0 t
R for r ∈ T, we define such an episodic sequence
localaction-observationhistoryτ ∈ T ≡ (Ω×A)forits max t
i asagoal-reachingsequenceanddenoteasT∗. Then,for
policyπ (a|τ ),whereπ :T ×A→[0,1](Hausknecht&
i i ∀s ∈ T∗, τ∗ := {s ,s ,...s } is a goal-reaching tra-
Stone,2015;Rashidetal.,2018). Additionally,wedenotea t st t t+1 T
jectoryandwedefinethefinalstateofτ∗ asagoalstate
grouptrajectoryasτ =<τ 1,...,τ
n
>.
denotedbys∗.
st
T
Centralized Training with Decentralized Execution
(CTDE) In fully cooperative MARL tasks, under the
4.Methodology
CTDEparadigm,valuefactorizationapproacheshavebeen
introduced by (Sunehag et al., 2017; Rashid et al., 2018; ThissectionintroducesLAtentGoal-guidedMulti-Agent
Son et al., 2019; Rashid et al., 2020; Wang et al., 2020a) reinforcementlearning(LAGMA)(Figure1). Wefirstex-
andachievedstate-of-the-artperformanceincomplexmulti- plainhowtoconstructaproper(1)quantizedembeddings
agent tasks such as SMAC (Samvelyan et al., 2019). In viaVQ-VAE.Tothisend,weintroduceanovellossterm
valuefactorizationapproaches,thejointaction-valuefunc- calledcoveragelosstodistributequantizedembeddingvec-
tion Qtot parameterized by θ is trained to minimize the torsacrosstheoverallembeddingspace. Then,weelaborate
θ
followinglossfunction. onthedetailsof(2)goal-reachingtrajectorygeneration
with extended VQ codebook. Finally, we propose (3) a
L(θ)=E [(cid:0) rext+γVtot(τ′)−Qtot(τ,a)(cid:1)2 ] latentgoal-guidedintrinsicreward whichguaranteesa
τ,a,rext,τ′∈D θ− θ
(1) betterTD-targetforpolicylearningandthusyieldsabetter
Here,Vtot(τ′)=max Qtot(τ′,a′)bydefinition;Drep- convergenceonoptimalpolicy.
θ− a′ θ−
resentsthereplaybuffer;rextisanexternalrewardprovided
bytheenvironment;Qtotisatargetnetworkparameterized 4.1.StateEmbeddingviaModifiedVQ-VAE
θ−
by θ− for double Q-learning(Hasselt, 2010; Van Hasselt
Inthispaper,weadoptVQ-VAEasadiscretizationbottle-
et al., 2016); and Qtot and Qtot include both mixer and
θ θ− neck(VanDenOordetal.,2017)toconstructadiscretized
individualpolicynetwork.
low-dimensional embedding space. Thus, we first define
Goal State and Goal-Reaching Trajectory In general n -trainable embedding vectors (codes) e ∈ RD in the
c j
cooperative multi-agent tasks, undiscounted reward sum, codebookwherej ={1,2,...,n }. Anencodernetworkf
c ϕ
i.e.,R =ΣT−1r ,ismaximizedasR ifagentsachieve inVQ-VAEprojectsaglobalstatestowardD-dimensional
0 t=0 t max
3LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
(a) TrainingwithoutL (λ =0.0). (b) TrainingwithLall (λ =0.2). (c) TrainingwithL (λ =0.2).
cvr cvr cvr cvr cvr cvr
Figure2: VisualizationofembeddingresultsviaVQ-VAE.UnderSMAC5m vs 6mtask,thesizeofcodebookn =64,
c
thelatentdimensionD =8;thisillustratesembeddingsattrainingtimeatT=1.0M.Coloreddotsrepresentχ,whichisa
statepresentationbeforequantization,andgraydotsarequantizedvectorrepresentationsbelongingtoVQcodebookderived
fromthestaterepresentations. Colorsfromredtopurple(rainbow)representfromsmalltolargetimestepwithinepisodes.
vector,x=f (s)∈RD. Insteadofadirectusageoflatent bufferD,denotedasχ = {x ∈ RD : x = f (s),s ∈ D},
ϕ ϕ
vectorx,weuseadiscretizedlatentx bythequantization leavingonlyafeweclosetoxwithinanepisode.Toresolve
q
processwhichmapsanembeddingvectorxtothenearest thisissue,weintroducethecoveragelosswhichminimizes
embeddingvectorinthecodebookasfollows. theoveralldistancebetweenthecurrentembeddingxand
allvectorsinthecodebook,i.e.,e forallj ={1,2,...,n }.
j c
x q =e z,wherez =argmin j||x−e j|| 2 (2) 1 (cid:88)nc
Then, thequantizedvectorx isusedasaninputtoade-
La cl vl r(e)=
n
||sg[f ϕ(s)]−e j||2
2
(4)
q c j=1
coderf whichreconstructstheoriginalstates. Totrain
ψ
anencoder,adecoder,andembeddingvectorsinthecode- AlthoughLall couldleadembeddingvectorstowardχ,all
cvr
book,weconsiderthefollowingobjectivesimilarto(Van quantizedvectorstendtolocatethecenterofχratherthan
DenOordetal.,2017;Islametal.,2022;Leeetal.,2023). densely covering whole χ space. Thus, we consider a
timestepdependentindexingJ(t)whencomputingthecov-
L (ϕ,ψ,e)=||f ([x=f (s)] )−s||2
VQ ψ ϕ q 2 erageloss. ThepurposeofintroducingJ(t)istomakeonly
+λ ||sg[f (s)]−x ||2+λ ||f (s)−sg[x ]||2 sequentiallyselectedquantizedvectorsclosetothecurrent
vq ϕ q 2 commit ϕ q 2
(3) embeddingx sothatquantizedembeddingsareuniformly
t
Here, [·] and sg[·] represent a quantization process and distributedacrossχaccordingtotimesteps. Then,thefinal
q
stopgradient,respecitvely. λ andλ arescalefactor formofcoveragelosscanbeexpressedasfollows.
vq commit
for correponsding terms. The first term in Eq. (2) is the
reconstructionloss,whilethesecondtermrepresentsVQ- L (e)= 1 (cid:88) ||sg[f (s)]−e ||2
objectivewhichmakesanembeddingvectoremovetoward cvr |J(t)| ϕ j 2 (5)
j∈J(t)
x=f (s).Thelasttermcalledacommitmentlossenforces
ϕ
an encoder to generate f ϕ(s) similar to x q and prevents We defer the details of J(t) construction to Appendix E.
itsoutputfromgrowingsignificantly. Toapproximatethe ByconsideringthecoveragelossinEq. (5), notonlythe
gradientsignalforanencoder,weadoptastraight-through nearestquantizedvectorbutalsoallvectorsinthecodebook
estimator(Bengioetal.,2013). movetowardsoveralllatentspaceχ. Inthisway,χcanbe
wellcoveredbyquantizedvectorsinthecodebook. Thus,
WhenadoptingVQ-VAEforstateembedding,wefoundthat
weconsidertheoveralllearningobjectiveasfollows.
onlyafewquantizedvectorseinthecodebookareselected
throughoutanepisode,whichmakesithardtoutilizesucha
Ltot (ϕ,ψ,e)=L (ϕ,ψ,e)+λ L (e) (6)
methodformeaningfulstateembedding. Wepresumedthat VQ VQ cvr cvr
thereasonisthenarrowprojectedembeddingspacefrom
whereλ isascalefactorforL .
feasiblestatescomparedtoawholeembeddingspace,i.e., cov cvr
RD. Thus,mostrandomlyinitializedquantizedvectorse Figure2presentsthevisualizationofembeddingsbyprinci-
locatefarfromthelatentspaceofstatesinthecurrentreplay palcomponentanalysis(PCA)(Woldetal.,1987).InFigure
4LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
2,thetrainingwithoutL leadstoquantizedvectorsthat f (τ ), respectively. Then, latent sequence after quanti-
cvr ϕ st
aredistantfromχ. zation process can be expressed as τ = [f (τ )] =
χt ϕ xt q
{x ,x ,x ,...,x }. Toevaluatethevaluetheof
In addition, embedding space χ itself distributes q,t q,t+1 q,t+2 q,T
trajectoryτ ,weuseC valueinthecodebookD of
around a few quantized vectors due to the commit- χt q,t VQ
aninitialquantizedvectorx inτ .
ment loss in Eq. (3). Considering Lall makes q,t χt
cvr
quantized vectors close to χ but they majorly locate Toencouragedesiredtransitionscontainedinτ withhigh
χt
around the center of χ rather than distributed properly. C value,weneedtokeepasequencedataofτ . Fora
q,t χt
Ontheotherhand,thepro- givenstartingnodex ,wekeeptop-ksequencesinD
q,t seq
posedL resultsinwell- basedontheirC . Thus,D consistsoftwoparts;D
cvr q,t seq τχt
distributedquantizedvec- storestop-ksequencesofτ andD storestheircorre-
χt Cq,t
tors over χ space so that spondingC values. Updatingalgorithmforasequence
q,t
they can properly repre- bufferD andstructuraldetailsofD arepresentedin
seq seq
sentlatentspaceofs∈D. AppendixD.
Figure 3 presents the oc-
AsinDefinition3.1,thehighestreturnincooperativemulti-
currenceofrecalledquan-
agenttaskscanonlybeachievedwhenthesemanticgoalis
tizedvectorsforstateem-
satisfied. Thus,onceagentshaveachievedacommongoal
Figure 3: Histogram of re- beddings in Fig. 2. We
duringtraining,goal-reachingtrajectoriesstartingfromvar-
calledquantizedvector. canseethattrainingwith
iousinitialpositionsarestoredinD . Afterweconstruct
λ guaranteesquantized seq
cvr D ,areferencetrajectoryτ∗ canbesampledoutofD .
vectorswelldistributedacrossχ. AppendixEpresentsthe seq χt τχt
Foragiveninitialpositionx inthequantizedlatentspace,
trainingalgorithmfortheproposedVQ-VAE. q,t
werandomlysampleareferencetrajectoryorgoal-reaching
trajectoryfromD .
seq
4.2.Goal-ReachingTrajectoryGenerationwith
ExtendedVQCodebook
4.3.IntrinsicRewardGeneration
Afterconstructingquantizedvectorsinthecodebook,we
Withagoal-reachingtrajectoryτ∗ fromthecurrentstate,
need to properly estimate the value of states projected to χt
wecandeterminethedesiredtransitionsthatleadtoagoal-
each quantized vector. Note that the estimated value of
reachingpath,simplybycheckingwhetherthequantized
eachquantizedvectorisusedwhengeneratinganadditional
latent x at each timestep t is in τ∗ . However, before
incentivetodesiredtransitions,i.e.,transitiontowardagoal- q,t χt
quantizedvectorseinthecodebookwellcoverthelatent
reachingtrajectory. Thankstothequantizedvectorsinthe
distribution χ, only a few e vectors are selected and thus
codebook,wecanresorttocount-basedestimationforthe
the same x will be repeatedly obtained. In such a case,
valueestimationofagivenstate. Foragivens , acumu- q
t stayinginthesamex willbeencouragedbyintrinsicre-
lative return from s denoted as R = ΣT−1γi−tr , and q
t t i=t i ward if x ∈ τ∗ . To prevent this, we only provide an
x q,t = [x t = f ϕ(s t)] q,thevalueofx q,t canbecomputed incentiveq totheχ dt esiredtransitiontowardx suchthat
viacount-basedestimationas q,t+1
x ∈ τ∗ and x ̸= x . Aremainingproblemis
1
N (cid:88)xq,t hoq, wt+ m1 uchχ wt eincenq ti, vt+ iz1 esuchq, at desiredtransition. Instead
C q,t(x q,t)=
N
R tj(x q,t) (7) ofanarbitraryincentive,wewanttodesignanadditionalre-
xq,t
j=1 wardtoguaranteeabetterTD-target,toconvergeonoptimal
Here,N isthevisitationcountonx . However,asan policy.
xq,t q,t
e bn ec twod ee er nn ae stw peo cr ik ficf ϕ sti as teup sd aa nt ded xd =uri fn ϕg (str )a cin ai nng b, reth ake .m Ta ht uc sh
,
P trr ao jep co ts oi rt yio an nd4.1 s.
′
∈Pro τv ∗id ,ed anth ia nt triτ nχ∗
st
icis rea wag ro dal r- Ire (a s′c )hi :n =g
itbecomeshardtoaccuratelyestimatethevalueofsviathe γ(C (s′)−max Q
χt
(s′,a′))tothecurrentTD-target
q,t a′ θ−
count-basedvisitonx q,t.Toresolvethis,weadoptamoving y = r(s,a)+γV θ−(s′) guarantees a true TD-target as
averagewithabuffersizeofmwhencomputingC q,t(x q,t) y∗ =r(s,a)+γV∗(s′),whereV∗(s′)isatruevalueofs′.
andstoretheupdatedvalueintheextendedcodebook,D .
VQ
AppendixDpresentsstructuraldetailsofD .
VQ Proof. PleaseseeAppendixA.
AfterconstructingD ,nowweneedtodetermineagoal-
VQ
reaching trajectory τ∗, defined in Definition 3.1, in the AccordingtoProposition4.1,whenτ∗ isagoal-reaching
latent space. This tras jt ectory is considered as a reference trajectory and s′ ∈ τ∗ , we can setχ at true TD-target by
χt
trajectory to incentivize desired transitions. Let the state addinganintrinsicrewardrI(s′)tothecurrentTD-targety,
sequencefroms anditscorrespondinglatentsequencepro- yieldingabetterconvergenceonanoptimalpolicy. Inthe
t
jected by f as τ = {s ,s ,s ,...,s } and τ = casewhenareferencetrajectoryτ∗ isnotagoal-reaching
ϕ st t t+1 t+2 T xt χt
5LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Voronoi-cell
𝑟𝐼
𝑟𝐼
𝜏 𝑥𝑡 𝜏 𝜒𝑡 𝜏 𝜒𝑡
𝜏
𝑥𝑡
𝜏 𝜒∗
𝑡
𝑒
(a) Trajectoryembedding. (b) Trajectoryinquantizedlatentspace. (c) Intrinsicrewardgeneration.
Figure4: Intrinsicrewardgenerationbycomparingthecurrenttrajectoryinquantizedlatentspace(τ )withasampled
χt
goal-reachingtrajectory(τ∗ ).
χt
trajectory,rI(s′)incentivizesthetransitiontowardthehigh- 4.4.OverallLearningObjective
returntrajectoryexperiencedsofar. Thus,wedefinealatent
ThispaperadoptsaconventionalCTDEparadigm(Oliehoek
goal-guidedintrinsicrewardrI asfollows.
etal.,2008;Guptaetal.,2017),andthusanyformofmixer
structure can be used for value factorization. We use the
rI(s )=γ(C (s )−max Q (s ,a′)), mixer structure presented in QMIX (Rashid et al., 2018)
t t+1 q,t t+1 a′ θ− t+1 (8)
ifx ∈τ∗ andx ̸=x similarto(Yangetal.,2022;Wangetal.,2021;Jeonetal.,
q,t+1 χt q,t+1 q,t 2022)toconstructthejointQ-value(Qtot)fromindividual
Q-functions. Byadoptingthelatentgoal-guidedintrinsic
NotethatrI(s )isaddedtoy =r +γV noty . In rewardrI toEq. (1),theoveralllossfunctionforthepolicy
t t+1 t t θ− t+1
addition,wecanmakesurethatrI becomesnon-negativeso learningcanbeexpressedasfollows.
thataninaccurateestimateofC (s′)intheearlytraining
q,t
phasedoesnotadverselyaffecttheestimationofV θ−. Al- L(θ)=(cid:0) rext+rI +γmax a′Qt θo −t(s′,a′)−Qt θot(s,a)(cid:1)2
gorithm1summarizestheoverallmethodforgoal-reaching (9)
trajectoryandanintrinsicrewardgeneration. Figure4illus-
NotethathererI doesnotincludeanyscalefactortocon-
tratestheschematicdiagramofquantizedtrajectoryembed-
trolitsmagnitude. ForanindividualpolicyviaQ-function,
dingsτ viaVQ-VAEandintrinsicrewardgenerationby
χt GRUsareadoptedtoencodealocalaction-observationhis-
comparingitwithagoal-reachingtrajectory,τ∗ .
χt tory τ to overcome the partial observability in POMDP
similartomostMARLapproaches(Sunehagetal.,2017;
Algorithm1Goal-reachingTrajectoryandIntrinsicReward Rashid et al., 2018; Son et al., 2019; Rashid et al., 2020;
Generation Wangetal.,2020a). However,inEq. (9),weexpressthe
Given: Sequences of the current batch [τi ]B , a se- equationwithsinsteadofτ fortheconcisenessandcoher-
quencebufferD ,anupdateintervaln χ ft ori= τ1 ∗ ,and encewiththemathematicalderivation. Theoveralltraining
VQ-VAEcodebos oe kq
D
freq χt
algorithmforbothVQ-VAEtrainingandpolicylearningis
VQ
fori=1toBdo presentedinAppendixE.
ComputeRi
t
fort=0toT do 5.Experiments
Getindexz ←τi
t χt Inthissection,wepresentexperimentsettingsandresults
ifmod(t,n )then
freq to evaluate the proposed method. We have designed our
RunAlgorithm2toupdateDzt withRi
seq t experimentswiththeintentionofaddressingthefollowing
Sampleareferencetrajectoryτ∗ fromDzt
χt seq inquiriesdenotedasQ1-3.
else
ifz Gt e∈ tCτ χ q∗ ,t ta ←nd Dz t Vzt̸= Q.z Ct q− ,t1then • Q sta1 t. e-T oh f-e thp ee -r af ro tr Mm Aan Rc Le o frf amLA ewG oM rkA sii nn bc oo tm hp da er ni ss eon anto
d
(r tI −1)i ←γmax(C q,t−max a′Q θ−(s t,a′),0) sparserewardsettings
endif
• Q2. Theimpactoftheproposedembeddingmethodon
endif
overallperformance
endfor
endfor • Q3. The efficiency of latent goal-guided incentive
comparedtootherrewarddesign
6LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Figure5: PerformancecomparisonofLAGMAagainstbaselinealgorithmsontwoeasyandhardSMACmaps: 1c3s5z,
5m vs 6m,andtwosuperhardSMACmaps: MMM2,6h vs 8z. (Denserewardsetting)
Figure6: PerformancecomparisonofLAGMAagainstbaselinealgorithmsonfourmaps: 3m,8m,2s3z,and2m vs 1z.
(Sparserewardsetting)
We consider complex multi-agent tasks such as SMAC to dense reward settings, LAGMA shows the best perfor-
(Samvelyan et al., 2019) and GRF (Kurach et al., 2020) manceinsparserewardsettingsthankstothelatentgoal-
asbenchmarkproblems. Inaddition,asbaselinealgorithm, guidedincentive. Sparserewardhardlygeneratesareward
we consider various baselines in MARL such as QMIX signalinexperiencereplay,thustrainingwiththeexperience
(Rashidetal.,2018),RODE(Wangetal.,2021)andLDSA oftheexactsamestatetakesalongtimetofindtheoptimal
(Yangetal.,2022)adoptingaroleorskillconditionedpolicy, policy. However,LAGMAconsidersthevalueofsemanti-
MASER(Jeonetal.,2022)presentingagent-wiseindivid- callysimilarstatesprojectedontothesamequantizedvector
ual subgoals from replay buffer, and EMC (Zheng et al., during training, so its learning efficiency is significantly
2021)adoptingepisodiccontrol. AppendixBpresentsfur- increased.
therdetailsofexperimentsettingsandimplementations,and
AppendixGillustratestheresourceusageandthecompu-
tationalcostrequiredfortheimplementationandtraining
ofLAGMA.Inaddition,additionalgeneralizabilitytestsof
LAGMAarepresentedinAppendixF.Ourcodeisavailable
at: https://github.com/aailabkaist/LAGMA.
5.1.PerformanceevaluationonSMAC
Denserewardsettings Fordenserewardsettings,wefol-
lowthedefaultsettingpresentedin(Samvelyanetal.,2019).
Figure 7: Performance comparison of LAGMA against
Figure 5 illustrates the overall performance of LAGMA.
baseline algorithms on two GRF maps: 3 vs 1WK and
Thankstoquantizedembeddingandlatentgoal-guidedin-
CounterAttack(CA) easy. (Sparserewardsetting)
centive,LAGMAshowssignificantperformanceimprove-
mentcomparedtothebackbonealgorithm,i.e.,QMIX,and
otherstate-of-the-art(SOTA)baselinealgorithms,especially 5.2.PerformanceevaluationonGRF
insuperhardSMACmaps.
Here,weconductexperimentsonadditionalsparsereward
Sparserewardsettings Forasparserewardsetting,we tasksinGRFtocompareLAGMAwithbaselinealgorithms.
followtherewarddesigninMASER(Jeonetal.,2022). Ap- Forexperiments,wedonotutilizeanyadditionalalgorithm
pendixBenumeratesthedetailsofrewardsettings. Similar forsampleefficiencysuchasprioritizedexperiencereplay
(Schauletal.,2015)forallalgorithms.
7LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Figure8: QualitativeanalysisonSMACMMM2(redteamsareRL-agents). Purplestarsrepresentquantizedembeddingsof
goalstatesinreplaybufferD. Yellowdotsindicatethequantizedembeddingsinasampledgoal-reachingtrajectorystarting
fromaninitialstatedenotedbyagreendot. GraydotsandtransparentdotsarethesameasFigure2. Blueandreddots
indicateterminalembeddingsoftwotrajectories,respectively.
EMC(Zhengetal.,2021)showscomparableperformance LAGMA(CL-All)trainedwithλall consideringallquan-
cvr
byutilizinganepisodicbuffer, whichbenefitsingenerat- tizedvectorsateachtimestepandLAGMA(No-CL)trained
ing a positive reward signal via additional episodic con- withoutcoverageloss.
trol term. However, LAGMA with a modified VQ code-
Figure9illustratestheeffectoftheproposedcoverageloss
bookcouldguideascoringpolicywithoututilizinganaddi-
inmodifiedVQ-VAEontheoverallperformance. Asshown
tionalepisodicbufferasbeingrequiredinEMC.Therefore,
in Fig. 9, the performance decreases when the model is
LAGMAachievesasimilarorbetterperformancewithless
trainedwithoutcoveragelossortrainedwithλall insteadof
memoryrequirement. cvr
λ . Theresultsimplythat,withouttheproposedcoverage
cvr
loss,quantizedlatentvectorsmaynotcoverχproperlyand
5.3.Ablationstudy
thusx canhardlyrepresenttheprojectedstates.Asaresult,
q
In this subsection, we conduct ablation studies to see agoal-reachingtrajectorythatconsistsofafewquantized
the effect of the proposed embedding method and latent vectorsyieldsnoincentivesignalinmosttransitions.
goal-guidedincentiveonoverallperformance. Wecom-
In addition, we conduct an ablation study on reward de-
pare LAGMA (ours) with ablated configurations such as
sign. Weconsiderasumofundiscountedrewards,C =
q0
ΣT−1r ,fortrajectoryvalueestimationinsteadofC ,de-
t=0 t q,t
notedasLAMGA(Cq0). WealsoconsidertheLAMGA
configurationwithgoal-reachingtrajectorygenerationonly
attheinitialstatedenotedbyLAMGA(Cqt-No-Upd). Fig-
ure 10 illustrates the results. Figure 10 implies that the
reward design of C shows a more stable performance
q,t
thanbothLAMGA(Cq0)andLAMGA(Cqt-No-Upd).
5.4.Qualitativeanalysis
Figure9: Ablationstudyconsideringthecoverageloss(CL)
onfourSMACmaps:3mand2s3z.(Sparserewardsetting) In this section, we conduct a qualitative analysis to ob-
servehowthestatesinanepisodeareprojectedontoquan-
tizedvectorspaceandreceivelatentgoal-guidedincentive
comparedtogoal-reachingtrajectorysampledfromD .
seq
Figure8illustratesthequantizedembeddingsequencesof
two trajectories: one denoted by a blue line representing
abattle-wontrajectoryandtheotherdenotedbyaredline
representingalosingtrajectory. InFig. 8,alosingtrajectory
initiallyfollowedtheoptimalsequencedenotedbyyellow
dots but began to bifurcate at t = 20 by losing Medivac
Figure10: Performancecomparisonofgoal-guidedincen-
and two more allies. Although the losing trajectory still
tivewithotherrewarddesignchoicesontwoSMACmaps:
passedthroughgoal-reachingsequencesduringanepisode,
3mand2s3z. (Sparserewardsetting)
8LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
itultimatelyreachedaterminalstatewithoutachanceto A., Leibo, J. Z., Rae, J., Wierstra, D., and Hass-
defeattheenemiesatt=40,asindicatedbytheabsenceof abis, D. Model-free episodic control. arXiv preprint
apurplestar. Ontheotherhand,atrajectorythatachieved arXiv:1606.04460,2016.
victoryfollowedthegoal-reachingpathandreachedagoal
stateattheend,asindicatedbypurplestars. Sinceonlytran- Burda, Y., Edwards, H., Storkey, A., andKlimov, O. Ex-
sitionstowardthesequencesonthegoal-reachingpathare plorationbyrandomnetworkdistillation. arXivpreprint
incentivized,LAGMAcanefficientlylearnagoal-reaching arXiv:1810.12894,2018.
policy,i.e.,theoptimalpolicyincooperativeMARL.
Chane-Sane, E., Schmid, C., and Laptev, I. Goal-
conditionedreinforcementlearningwithimaginedsub-
6.Conclusions
goals. InInternationalConferenceonMachineLearning,
pp.1430–1440.PMLR,2021.
This paper presents LAGMA, a framework to generate a
goal-reaching trajectory in latent space and a latent goal-
Dasgupta,S. Experimentswithrandomprojection. arXiv
guidedincentivetoachieveacommongoalincooperative
preprintarXiv:1301.3849,2013.
MARL.Thankstothequantizedembeddingspace,theex-
perienceofsemanticallysimilarstatesissharedbystates
Ellis, B., Cook, J., Moalla, S., Samvelyan, M., Sun, M.,
projectedontothesamequantizedvector,yieldingefficient
Mahajan, A., Foerster, J., and Whiteson, S. Smacv2:
training. Theproposedlatentgoal-guidedintrinsicreward
Animprovedbenchmarkforcooperativemulti-agentre-
encourages transitions toward a goal-reaching trajectory.
inforcementlearning. AdvancesinNeuralInformation
Experimentsandablationstudiesvalidatetheeffectiveness
ProcessingSystems,36,2024.
ofLAGMA.
Ghosh,D.,Gupta,A.,andLevine,S. Learningactionable
Acknowledgements representations with goal-conditioned policies. arXiv
preprintarXiv:1811.07819,2018.
This research was supported by AI Technology Develop-
mentforCommonsenseExtraction,Reasoning,andInfer- Grzes´,M.andKudenko,D. Multigridreinforcementlearn-
encefromHeterogeneousData(IITP)fundedbytheMin- ingwithrewardshaping. InInternationalConferenceon
istryofScienceandICT(2022-0-00077). ArtificialNeuralNetworks,pp.357–366.Springer,2008.
ImpactStatement Gupta,J.K.,Egorov,M.,andKochenderfer,M.Cooperative
multi-agent control using deep reinforcement learning.
ThispaperprimarilyfocusesonadvancingthefieldofMa- InInternationalconferenceonautonomousagentsand
chineLearningthroughmulti-agentreinforcementlearning. multiagentsystems,pp.66–83.Springer,2017.
Whiletherecouldbevariouspotentialsocietalconsequences
ofourwork,noneofwhichwebelievemustbespecifically Hafner, D., Lillicrap, T., Norouzi, M., and Ba, J. Mas-
highlightedhere. teringatariwithdiscreteworldmodels. arXivpreprint
arXiv:2010.02193,2020.
References
Hasselt,H. Doubleq-learning. Advancesinneuralinforma-
Andrychowicz,M.,Wolski,F.,Ray,A.,Schneider,J.,Fong, tionprocessingsystems,23,2010.
R.,Welinder,P.,McGrew,B.,Tobin,J.,PieterAbbeel,O.,
Hausknecht,M.andStone,P. Deeprecurrentq-learningfor
andZaremba,W. Hindsightexperiencereplay. Advances
partiallyobservablemdps. In2015aaaifallsymposium
inneuralinformationprocessingsystems,30,2017.
series,2015.
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T.,
Saxton, D., and Munos, R. Unifying count-based ex- Houthooft,R.,Chen,X.,Duan,Y.,Schulman,J.,DeTurck,
plorationandintrinsicmotivation. Advancesinneural F., andAbbeel, P. Vime: Variationalinformationmax-
informationprocessingsystems,29,2016. imizing exploration. Advances in neural information
processingsystems,29,2016.
Bengio,Y.,Le´onard,N.,andCourville,A. Estimatingor
propagatinggradientsthroughstochasticneuronsforcon- Islam,R.,Zang,H.,Goyal,A.,Lamb,A.,Kawaguchi,K.,
ditionalcomputation. arXivpreprintarXiv:1308.3432, Li, X., Laroche, R., Bengio, Y., and Combes, R. T. D.
2013. Discrete factorial representations as an abstraction for
goalconditionedreinforcementlearning. arXivpreprint
Blundell, C., Uria, B., Pritzel, A., Li, Y., Ruderman, arXiv:2211.00247,2022.
9LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Jaques,N.,Lazaridou,A.,Hughes,E.,Gulcehre,C.,Ortega, Lowe,R.,Wu,Y.,Tamar,A.,Harb,J.,Abbeel,P.,andMor-
P., Strouse, D., Leibo, J.Z., andDeFreitas, N. Social datch,I. Multi-agentactor-criticformixedcooperative-
influence as intrinsic motivation for multi-agent deep competitiveenvironments. NeuralInformationProcess-
reinforcementlearning. InInternationalconferenceon ingSystems(NIPS),2017.
machinelearning,pp.3040–3049.PMLR,2019.
Mahajan,A.,Rashid,T.,Samvelyan,M.,andWhiteson,S.
Jeon, J., Kim, W., Jung, W., and Sung, Y. Maser: Multi- Maven: Multi-agentvariationalexploration. Advancesin
agent reinforcement learning with subgoals generated NeuralInformationProcessingSystems,32,2019.
fromexperiencereplaybuffer. InInternationalConfer-
Mguni, D. H., Jafferjee, T., Wang, J., Perez-Nieves, N.,
ence on Machine Learning, pp. 10041–10052. PMLR,
Slumbers, O., Tong, F., Li, Y., Zhu, J., Yang, Y., and
2022.
Wang, J. Ligs: Learnable intrinsic-reward genera-
Jiang,N.,Kulesza,A.,andSingh,S. Abstractionselection tion selection for multi-agent learning. arXiv preprint
inmodel-basedreinforcementlearning. InInternational arXiv:2112.02618,2021.
ConferenceonMachineLearning,pp.179–188.PMLR,
Mohamed, S. and Jimenez Rezende, D. Variational in-
2015.
formation maximisation for intrinsically motivated re-
inforcement learning. Advances in neural information
Kaelbling,L.P. Learningtoachievegoals. InIJCAI,vol-
processingsystems,28,2015.
ume2,pp.1094–8.Citeseer,1993.
Na, H., Seo, Y., andMoon, I.-c. Efficientepisodicmem-
Kim,H.,Kim,J.,Jeong,Y.,Levine,S.,andSong,H.O.Emi:
oryutilizationofcooperativemulti-agentreinforcement
Exploration with mutual information. arXiv preprint
learning. arXivpreprintarXiv:2403.01112,2024.
arXiv:1810.01176,2018.
Nasiriany, S., Pong, V., Lin, S., and Levine, S. Planning
Kim, J., Seo, Y., Ahn, S., Son, K., and Shin, J. Imitat-
withgoal-conditionedpolicies. AdvancesinNeuralInfor-
inggraph-basedplanningwithgoal-conditionedpolicies.
mationProcessingSystems,32,2019.
arXivpreprintarXiv:2303.11166,2023.
Oliehoek, F.A.andAmato, C. Aconciseintroductionto
Kulkarni, T. D., Narasimhan, K., Saeedi, A., and Tenen-
decentralizedPOMDPs. Springer,2016.
baum,J. Hierarchicaldeepreinforcementlearning: In-
tegrating temporal abstractionand intrinsicmotivation. Oliehoek,F.A.,Spaan,M.T.,andVlassis,N. Optimaland
Advancesinneuralinformationprocessingsystems,29, approximateq-valuefunctionsfordecentralizedpomdps.
2016. JournalofArtificialIntelligenceResearch,32:289–353,
2008.
Kurach,K.,Raichuk,A.,Stanczyk,P.,Zajkc,M.,Bachem,
O.,Espeholt,L.,Riquelme,C.,Vincent,D.,Michalski, Ostrovski,G.,Bellemare,M.G.,Oord,A.,andMunos,R.
M., Bousquet, O., et al. Google research football: A Count-basedexplorationwithneuraldensitymodels. In
novelreinforcementlearningenvironment. InProceed- Internationalconferenceonmachinelearning,pp.2721–
ings of the AAAI Conference on Artificial Intelligence, 2730.PMLR,2017.
volume34,pp.4501–4510,2020.
Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T.
Lee,S.,Cho,D.,Park,J.,andKim,H.J. Cqm: Curriculum Curiosity-driven exploration by self-supervised predic-
reinforcement learning with a quantized world model. tion. InInternationalconferenceonmachinelearning,
arXivpreprintarXiv:2310.17330,2023. pp.2778–2787.PMLR,2017.
Lengyel,M.andDayan,P. Hippocampalcontributionsto Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G.,
control: thethirdway. Advancesinneuralinformation Foerster,J.,andWhiteson,S. Qmix: Monotonicvalue
processingsystems,20,2007. functionfactorisationfordeepmulti-agentreinforcement
learning. InInternationalconferenceonmachinelearn-
Li,Z.,Zhu,D.,Hu,Y.,Xie,X.,Ma,L.,Zheng,Y.,Song,Y., ing,pp.4295–4304.PMLR,2018.
Chen,Y.,andZhao,J. Neuralepisodiccontrolwithstate
abstraction. arXivpreprintarXiv:2301.11490,2023. Rashid, T., Farquhar, G., Peng, B., and Whiteson, S.
Weighted qmix: Expanding monotonic value function
Liu,Y.,Li,Y.,Xu,X.,Dou,Y.,andLiu,D. Heterogeneous factorisationfordeepmulti-agentreinforcementlearning.
skilllearningformulti-agenttasks. AdvancesinNeural Advancesinneuralinformationprocessingsystems,33:
InformationProcessingSystems,35:37011–37023,2022. 10199–10210,2020.
10LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Samvelyan, M., Rashid, T., De Witt, C. S., Farquhar, G., Wang, T., Gupta, T., Mahajan, A., Peng, B., Whiteson,
Nardelli, N., Rudner, T. G., Hung, C.-M., Torr, P. H., S., andZhang, C. Rode: Learningrolestodecompose
Foerster,J.,andWhiteson,S. Thestarcraftmulti-agent multi-agent tasks. In Proceedings of the International
challenge. arXivpreprintarXiv:1902.04043,2019. ConferenceonLearningRepresentations(ICLR),2021.
Schaul,T.,Horgan,D.,Gregor,K.,andSilver,D. Universal Wold, S., Esbensen, K., and Geladi, P. Principal compo-
valuefunctionapproximators.InInternationalconference nentanalysis. Chemometricsandintelligentlaboratory
onmachinelearning,pp.1312–1320.PMLR,2015. systems,2(1-3):37–52,1987.
Son, K., Kim, D., Kang, W.J., Hostallero, D.E., andYi, Yang,J.,Borovikov,I.,andZha,H.Hierarchicalcooperative
Y. Qtran: Learningtofactorizewithtransformationfor multi-agentreinforcementlearningwithskilldiscovery.
cooperativemulti-agentreinforcementlearning. InInter- arXivpreprintarXiv:1912.03558,2019.
nationalconferenceonmachinelearning,pp.5887–5896.
PMLR,2019. Yang, M., Zhao, J., Hu, X., Zhou, W., Zhu, J., andLi, H.
Ldsa: Learningdynamicsubtaskassignmentincooper-
Stadie,B.C.,Levine,S.,andAbbeel,P. Incentivizingex- ative multi-agent reinforcement learning. Advances in
plorationinreinforcementlearningwithdeeppredictive NeuralInformationProcessingSystems,35:1698–1710,
models. arXivpreprintarXiv:1507.00814,2015. 2022.
Sunehag,P.,Lever,G.,Gruslys,A.,Czarnecki,W.M.,Zam- Zhang, T., Guo, S., Tan, T., Hu, X., and Chen, F. Gen-
baldi,V.,Jaderberg,M.,Lanctot,M.,Sonnerat,N.,Leibo, erating adjacency-constrained subgoals in hierarchical
J. Z., Tuyls, K., et al. Value-decomposition networks reinforcementlearning. AdvancesinNeuralInformation
for cooperative multi-agent learning. arXiv preprint ProcessingSystems,33:21579–21590,2020.
arXiv:1706.05296,2017.
Zheng, L., Chen, J., Wang, J., He, J., Hu, Y., Chen, Y.,
Sutton,R.S.andBarto,A.G. Reinforcementlearning: An
Fan, C., Gao, Y., and Zhang, C. Episodic multi-agent
introduction. MITpress,2018.
reinforcementlearningwithcuriosity-drivenexploration.
AdvancesinNeuralInformationProcessingSystems,34:
Tang,H.,Houthooft,R.,Foote,D.,Stooke,A.,XiChen,O.,
3757–3769,2021.
Duan, Y., Schulman, J., DeTurck, F., andAbbeel, P. #
exploration: Astudyofcount-basedexplorationfordeep
Zhu,D.,Chen,J.,Shang,W.,Zhou,X.,Grossklags,J.,and
reinforcementlearning. Advancesinneuralinformation
Hassan,A.E. Deepmemory: model-basedmemorization
processingsystems,30,2017.
analysisofdeepneurallanguagemodels. In202136th
IEEE/ACMInternationalConferenceonAutomatedSoft-
Tang, Y. andAgrawal, S. Discretizingcontinuous action
wareEngineering(ASE),pp.1003–1015.IEEE,2021.
spaceforon-policyoptimization. InProceedingsofthe
aaaiconferenceonartificialintelligence,volume34,pp.
5981–5988,2020.
VanDenOord,A.,Vinyals,O.,etal. Neuraldiscreterep-
resentation learning. Advances in neural information
processingsystems,30,2017.
VanHasselt,H.,Guez,A.,andSilver,D. Deepreinforce-
mentlearningwithdoubleq-learning. InProceedingsof
theAAAIconferenceonartificialintelligence,volume30,
2016.
Wang, J., Ren, Z., Liu, T., Yu, Y., and Zhang, C. Qplex:
Duplexduelingmulti-agentq-learning. arXivpreprint
arXiv:2008.01062,2020a.
Wang, T., Wang, J., Wu, Y., and Zhang, C. Influence-
based multi-agent exploration. arXiv preprint
arXiv:1910.05512,2019.
Wang,T.,Dong,H.,Lesser,V.,andZhang,C. Roma:Multi-
agentreinforcementlearningwithemergentroles. arXiv
preprintarXiv:2003.08039,2020b.
11LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
A.MathematicalProof
Here,weprovidetheomittedproofofProposition4.1.
Proof. Lety =r(s,a)+γV bethecurrentTD-targetwiththetargetnetworkparameterizedwithθ−. Addinganintrinsic
θ−
rewardrI(s′)toyyieldsy′ =y+rI(s′). Now,weneedtocheckwhethery′accuratelyestimatesy∗ =r+γV∗(s′).
E[y′]=E[r(s,a)+γV +rI(s′)]
θ−
=E[r(s,a)+γmax Q (s′,a′)+γ(C (s′)−max Q (s′,a′))]
a′ θ− q,t a′ θ−
=E[r(s,a)+γ(C (s′))]
q,t
T (cid:88)−1 (10)
=E[r(s,a)+γ(E[ γi−(t+1)r ])]
i
i=t+1
=r(s,a)+γE[r +γr +···+γT−t−2r ]
t+1 t+2 T−1
=r(s,a)+γV∗(s′)
ThelastequalityinEq. (10)holdssinces′isonagoal-reachingtrajectory,i.e.,s′ ∈τ∗ whosereturnismaximized,and
E[r +γr +···]isanunbiasedMonte-CarloestimateofV∗(s′).
χ0
t+1 t+2
B.ExperimentDetails
In this section, we present details of SMAC (Samvelyan et al., 2019) and GRF (Kurach et al., 2020), and we also list
hyperparemetersettingsofLAGMAforeachtask. Tables1and2presentthedimensionsofstateandactionspacesandthe
maximumepisodiclength.
Table1: DimensionofthestatespaceandtheactionspaceandtheepisodiclengthofSMAC
Task Dimensionofstatespace Dimensionofactionspace Episodiclength
3m 48 9 60
8m 168 14 120
2s3z 120 11 120
2m vs 1z 26 7 150
1c3s5z 270 15 180
5m vs 6m 98 12 70
MMM2 322 18 180
6h vs 8z 140 14 150
Table2: DimensionofthestatespaceandtheactionspaceandtheepisodiclengthofGRF
Task Dimensionofstatespace Dimensionofactionspace Episodiclength
3 vs 1WK 26 19 150
CA easy 30 19 150
Inaddition,Table3presentsthetask-dependenthyperparametersettingsforallexperiments. AsseenfromTable3,weused
similarhyperparametersacrossvarioustasks. Foranupdateintervaln inAlgorithm1,weusethesamevaluen =5
freq freq
forallexperiments. ϵ representsannealingtimeforexplorationrateofϵ-greedy,from1.0to0.05.
T
Aftersomeparametricstudies,adjustinghyperparameterforVQ-VAEtrainingsuchasncd andnvq ,insteadofvaryingλ
freq freq
valueslistedasλ ,λ ,andλ ,providesmoreefficientwayofsearchingparametricspace. Thus,weprimarilyadjust
vq commit cvr
ncd andnvq accordingtotasks,whilekeepingtheratiobetweenλvaluesthesame.
freq freq
12LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Forhyperparametersettings,werecommendtheefficientboundsforeachhyperparameterbasedonourexperimentsas
follows:
• Numberofcodebook,n : 64-512
c
• UpdateintervalforVQ-VAE,nvq : 10-40(underncd =10)
freq freq
• Updateintervalforextendedcodebook,ncd : 10-40(undernvq =10)
freq freq
• Numberofreferencetrajectory,k: 10-30
• Scalefactorofcoverageloss,λ : 0.25-1.0(underλ =1.0andλ =0.5)
cvr vq commit
Notethatlargervaluesofn andk,andsmallervaluesofnvq andncd willincreasethecomputationalcost.
c freq freq
Table3: Hyperparametersettingsforexperiments.
task n D λ λ λ ϵ ncd nvq
c vq commit cvr T freq freq
3m 256 8 2.0 1.0 1.0 50K 10 40
8m 256 8 2.0 1.0 1.0 50K 20 10
SMAC(sparse)
2s3z 256 8 2.0 1.0 1.0 50K 10 40
2m vs 1z 256 8 2.0 1.0 1.0 500K 20 10
1c3s5z 64 8 1.0 0.5 0.5 50K 40 10
5m vs 6m 64 8 1.0 0.5 0.5 50K 40 10
SMAC(dense)
MMM2 64 8 1.0 0.5 0.5 50K 40 10
6h vs 8z 256 8 2.0 1.0 1.0 500K 40 10
3 vs 1WK 256 8 2.0 1.0 1.0 50K 20 10
GRF(sparse)
CA easy 256 8 2.0 1.0 1.0 50K 10 20
Table4presentstherewardsettingsforSMAC(sparse)whichfollowsthesparserewardsettingsfrom(Jeonetal.,2022).
Table4: RewardsettingsforSMAC(sparse)
Condition Sparsereward
Allenemiesdie(Win) +200
Eachenemydies +10
Eachallydies -5
13LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
C.AdditionalRelatedWorks
Goal-conditionedRLvs. SubtaskconditionedMARL Inasingleagentcase,Goal-conditionedRL(GCRL)which
aimstosolvemultipletaskstoreachgiventarget-goalshasbeenwidelyadoptedinvarioustasksincludingtaskswitha
sparsereward(Kaelbling,1993;Schauletal.,2015;Andrychowiczetal.,2017). GCRLoftenutilizesthegivengoalas
anadditionalinputtoactionpolicyinadditiontostates(Schauletal.,2015). Especially,goal-conditionedhierarchical
reinforcementlearning(HRL)(Kulkarnietal.,2016;Zhangetal.,2020;Chane-Saneetal.,2021)adoptshierarchicalpolicy
structurewhereanupper-tierpolicydeterminessubgoalorlandmarkandalower-tierpolicytakesactionbasedonbothstate
andselectedasubgoalorlandmark.
Asonetechnique,reachingtosubgoalsgeneratesarewardsignalviahindsightexperiencereplay(Andrychowiczetal.,
2017),andthusthegoal-conditionedpolicylearnpolicytoreachthefinalgoalwiththehelpoftheseintermediatesignals.
Thus,manyresearchers(Nasirianyetal.,2019;Zhangetal.,2020;Chane-Saneetal.,2021;Kimetal.,2023;Leeetal.,
2023)havestudiedonhowtogenerateintermediatesubgoalstoreachfinalgoals.
InthefieldofMARL,asubtask(Yangetal.,2022),role(Wangetal.,2020b;2021)orskill(Yangetal.,2019;Liuetal.,2022)
conditionedpolicyadoptedinahierarchicalMARLstructurehasastructuralcommonalitywithagoal-conditionedRLin
thatlower-tierpolicynetworkusedesignatedsubtaskbytheupper-tiernetworkasanadditionalinputwhendetermining
individualaction. InMARLtasks,suchsubtasks,roles,orskillsareabitdifferentfromsubgoalsinGCRL,astheyare
adoptedtodecomposeactionspaceforefficienttrainingorforsubtask-dependentcoordination. Anothermajordifferenceis
thatinageneralMARLtask,thefinalgoalisnotdefinedexplicitlyunlikeagoal-conditionedRL.MASER(Jeonetal.,
2022)adoptsthesubgoalgenerationschemefromgoal-conditionedRLwhenitgeneratesanintrinsicrewardbasedonthe
Euclideandistancebetweenactionablerepresentations(Ghoshetal.,2018)ofthecurrentandsubgoalobservation. However,
thissignaldoesnotguaranteetheconsistencywithlearningsignalforthejointQ-function. IncontrasttoMASER,weadopt
alatentgoal-guidedincentiveduringacentralizedtrainingphasebasedonwhethervisitingonthepromisingsubgoalsor
goalsinthelatentspace. Also,thegeneratedincentivebyLAGMAtheoreticallyguaranteesabetterTD-target,yielding
betterconvergenceontheoptimalpolicy.
D.StructureofExtendedVQCodebook
TocomputeC viaamovingaverage,dataisstoredinaFIFO(Firstin,FirstOut)styletothecodebookD ,similar
q,t VQ
to a replay buffer D. After computing C (x ) with the current R , we update the value of C (x ) in D as
q,t q,t t q,t q,t VQ
Dzt .C ←C (x )wherez isanindexofaquantizedvectorx .
VQ q,t q,t q,t t q,t
Extended VQ Codebook
𝒟
𝑉𝑄 𝒟 𝒟 𝒟
𝑠𝑒𝑞 𝐶𝑞,𝑡 𝜏𝜒𝑡
𝑅 𝑡(1,1) ⋮ 𝑅 𝑡(1,𝑚) 𝑒 1 𝒟 𝑠1 𝑒𝑞 𝐶 𝑞1 ,𝑡 =𝐶 𝑞m ,𝑡in 𝑥 𝑞1 ,𝑡 𝑥 𝑞1 ,𝑡+1 ⋮
𝑅 𝑡(2,1) ⋮ 𝑅 𝑡(2,𝑚) 𝑒 2 𝒟 𝑠2 𝑒𝑞 ⋮ ⋮ ⋮ ⋮
⋮ ⋮ ⋮ ⋮ 𝐶 𝑞∗ ,𝑡 ⋮ ⋮ ⋮ ~𝜏 𝜒∗ 𝑡
𝐶 𝑞𝑧 ,𝑡 𝑅 𝑡(𝑧,1) ⋮ 𝑅 𝑡(𝑧,𝑚) 𝑒 𝑧 𝑥 𝑞 𝒟 𝑠𝑧 𝑒𝑞 ={𝒟 𝐶𝑞,𝑡,𝒟 𝜏𝜒𝑡}
⋮ ⋮ ⋮ ⋮
⋮ ⋮ ⋮ ⋮
𝑅 𝑡(𝑛𝑐,1) ⋮ 𝑅 𝑡(𝑛𝑐,𝑚) 𝑒 𝑛𝑐 𝒟 𝑠𝑛 𝑒𝑐 𝑞 𝐶 𝑞𝑘 ,𝑡 =𝐶 𝑞m ,𝑡ax 𝑥 𝑞𝑘 ,𝑡 𝑥 𝑞𝑘 ,𝑡+1 ⋮
Figure11: VQcodebookstructure.
InAlgorithm2,heap pushandheap replaceadopttheconventionalheapspacemanagementrule,withacomputa-
tionalcomplexityofO(logk). ThedifferenceisthatweadditionallystorethesequenceinformationinD accordingto
τχt
theirC values.
q,t
14LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Algorithm2UpdateSequenceBufferD
seq
1: D seq keeptopktrajectorysequencesbasedontheirC q,t
2: Input: AtotalrewardsumR tandasequenceτ χt
3: Getaninitialindexz t ←τ χt[0]
4: GetD Cq,t,D τχt fromD sz et q
5: if|D Cq,t|<kthen
6: heap push(D Cq,t,D τχt,C q,t,τ χt)
7: else
8: C qm ,i tn ←D Cq,t[0]
9: ifR t >C qm ,i tnthen
10: heap replace(D Cq,t,D τχt,R t,τ χt)
11: endif
12: endif
E.ImplementationDetails
Inthissection,wepresentfurtherdetailsoftheimplementationforLAGMA.Algorithm3presentsthepseudo-codefora
timestepdependentindexingJ(t)usedinEq. (5). ThepurposeofatimestepdependentindexingJ(t)istodistributethe
quantizedvectorsthroughoutanepisode. Thus,Algorithm3triestouniformlydistributequantizedvectorsaccordingtothe
maximumbatchtimeofanepisode. Byconsideringthemaximumbatchtimeofeachepisode,Algorithm3canadaptively
distributequantizedvectors.
Algorithm3ComputeJ(t)
1: Input: ForgiventhemaximumbatchtimeT,thenumberofcodebookn c,andthecurrenttimestept
2: ift==0then
3: d=⌊n c/T⌋
4: r =n c modT
5: i s =d n×T
6: Keepthevaluesofd,r,i suntiltheendoftheepisode
7: endif
8: ifd≥1then
9: J(t)=d×t:1:d×(t+1)
10: ift<rthen
11: AppendJ(t)withi s+t
12: endif
13: else
14: J(t)=⌊t×n c/T⌋
15: endif
16: returnJ(t)
Forgiventhemaximumbatchtimet andthenumberofcodebookn ,Line#4andLine#5inAlgorithm3compute
max c
thequotientandremainder,respectively. Line#8computeanarraywithincreasingorderstartingfromtheindexd×tto
d×(t+1). Line#10additionallydesignatetheremainingquantizedvectorstotheearlytimeofanepisode.
Algorithm4presentstrainingalgorithmtoupdateencoderf ,decoderf ,andquantizedembeddingseinVQ-VAE.In
ϕ ψ
Algorithm4,wealsopresentaseparateupdateforD ,whichestimatesthevalueofeachquantizedvectorinVQ-VAE.In
VQ
addition,theoveralltrainingalgorithmincludingtrainingforVQ-VAEispresentedinAlgorithm5.
15LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Algorithm4TrainingalgorithmforVQ-VAEandD
VQ
Parameter: learningrateαandbatch-sizeB
Input: Bsampletrajectories[T]B fromreplaybufferD,thecurrentepisodenumbern ,anupdateintervalnvq for
i=1 epi freq
VQ-VAEandncd forD updateinterval.
freq VQ
fort=0toT do
fori=1toBdo
ifmod(n ,ncd )then
epi freq
GetRi andupdateD withEq. (7).
t VQ
endif
ifmod(n ,nvq )then
epi freq
Getcurrentstates ∼[T] andcomputeJ(t)viaAlgorithm3
t i=1
ComputeLtot viaEq. (6)withf ,f ,ande.
VQ ϕ ψ
endif
endfor
endfor
ifmod(n ,nvq )then
epi freq
∂Ltot ∂Ltot ∂Ltot
Updateϕ←ϕ−α VQ,ψ ←ψ−α VQ,e←e−α VQ
∂ϕ ∂ψ ∂e
endif
Algorithm5TrainingalgorithmforLAGMA.
1: Parameter: BatchsizeBandthemaximumtrainingtimeT env
2: Input: Qi
θ
isindividualQ-networkofnagents,replaybufferD,extendedVQcodebookD VQ,andsequencebuffer
D
seq
3: Initializenetworkparametersθ,ϕ,ψ,e
4: whilet env ≤T env do
5: Interactwiththeenvironmentviaϵ-greedypolicybasedon[Qi]n andgetatrajectoryT
θ i=1
6: AppendT toD
7: GetBsampletrajectories[T]B ∼D
i=1
8: Foragiven[T]B ,runMARLtrainingalgorithmandAlgorithm1toupdateθ withEq. (9),andAlgorithm4to
i=1
updateϕ,ψ,ewithEq. (6)
9: endwhile
F.GeneralizabilityofLAGMA
F.1.Policyrobustnesstest
Toassesstherobustnessofpolicylearnedbyourmodel,wedesignedtaskswiththesameunitconfigurationbuthighlyvaried
initialpositions,onesthatagentshadnotencounteredduringtraining,i.e.,unseenmaps. Withthesesettings,opponent
agentswillalsoexperiencetotallydifferentrelativepositionsandthuswillmakedifferentdecisions. Wesetthedifferent
initialpositionsforthisevaluationasfollows.
AsillustratedinFigure12,theinitialpositionofeachtaskissignificantlymovedfromthenominalpositionexperienced
duringthetrainingphase.
Forthecomparison,weconductthesameexperimentforotherbaselines,suchasQMIX(Rashidetal.,2018)andLDSA
(Yangetal.,2022). ThemodelbyeachalgorithmistrainedforT =1M innominalMMM2map(denotedasNominal)
env
andthenevaluatedundervariousproblemsettings,suchasNW(hard),NW,SW,andSW(hard).Eachevaluationisconducted
for5differentseedswith32testsandTable5showsthemeanandvarianceofwinrateofeachcase.
InTable5, LAGMAshowsnotonlythebestperformancebutalsotherobustperformanceinvariousproblemsettings.
The fast learning of LAGMA is attributed to the latent goal-guided incentive, which generates accurate TD-target by
utilizingvaluesofsemanticallysimilarstatesprojectedtothesamequantizedvector. BecauseLAGMAutilizesthevalueof
semanticallysimilarstatesratherthanthespecificstateswhenlearningQ-values,differentyetsemanticallysimilarstatestend
tohavesimilarQ-values,yieldinggeneralizablepolicies. Inthismanner,LAGMAwouldenablefurtherexploration,rather
16LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Figure12: Problemsettingsforpolicyrobustnesstest. Team1representstheinitialpositionofRLagents,whileTeam2is
theinitialpositionofopponents.
Table5: PolicyrobustnesstestonSMACMMM2(superhard). AllmodelsaretrainedforT =1M. Thepercentage(%)
env
inparenthesesrepresentstheratiocomparedtoanominalmeanvalue.
NW(hard) NW Nominal SW SW(hard)
LAGMA 0.275±0.064(28.2%) 0.500±0.104(51.3%) 0.975±0.026 0.556±0.051(57.1%) 0.394±0.042(40.4%)
QMIX 0.050±0.036(13.1%) 0.138±0.100(36.1%) 0.381±0.078 0.194±0.092(50.8%) 0.156±0.058(41.0%)
LDSA 0.000±0.000(0.0%) 0.081±0.047(18.3%) 0.444±0.107 0.063±0.049(14.1%) 0.081±0.028(18.3%)
thansolelyenforcingexploitationofanidentifiedstatetrajectory. Thus,eventhoughthetransitiontowardagoal-reaching
trajectoryisencouragedduringtraining,thepolicylearnedbyLAGMAdoesnotoverfittospecifictrajectoriesandexhibits
robustnesstounseenmaps.
F.2.Scalabilitytest
LAGMAcanbeadoptedtolarge-scaleproblemswithoutanymodifications. VQ-VAEtakesaglobalstateasaninputto
projectthemintoquantizedlatentspace. Thus,inlarge-scaleproblems,onlytheinputsizewilldifferfromtaskswitha
smallnumberofagents. Inaddition,manyMARLtasksincludehigh-dimensionalglobalinputsizeaspresentedinTable1
inthemanuscript. ToassessthescalablityofLAGMA,weconductadditionalexperimentsin27m vs 30mSMACtask,
whosestatedimensionis1170. Figure13illustratestheperformanceofLAGMA.InFigure13,LAGMAmaintainsefficient
learning performance even when applied to large-scale problems, using identical hyperparameter settings as those for
small-scaleproblemssuchas5m vs 6m.
(a) Learningcurve.
Figure13: Performanceonlarge-scaleproblem(27m vs 30mSMAC).
17LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
F.3.Generalizabilitytesttoproblemswithdiversesemanticgoals
To show the generalizability of LAGMA further, we conducted additional experiments on another benchmark such as
SMACv2(Ellisetal.,2024),whichincludesdiversityininitialpositionsandunitcombinationswithintheidenticaltask.
Thus,fromtheperspectiveoflatentspace,SMACv2tasksmayencompassdistinctmultiplegoals,evenwithinthesametask.
Forevaluation,weadoptthesamehyperparametersasthosefor3 vs 1WKpresentedinTable3inthemanuscript,except
forD =4andncd =40.
freq
(a) protoss 5 vs 5 (b) protoss 10 vs 11
Figure14: PerformanceevaluationonSMACv2.
InFigure14,LAGMAshowscomparableorbetterperformancethanbaselinealgorithms,butitdoesnotexhibitdistinctively
strongperformance,unlikeotherbenchmarkproblems. Wedeemthatthisresultstemsfromcharacteristicsofthecurrent
LAGMAcapturingreferencetrajectoriestowardsasimilargoalintheearlytrainingphase.
Multi-objective(ormultiplegoals)tasksmayrequireadiversereferencetrajectorygeneration. ThecurrentLAGMAonly
considersthereturnofatrajectorywhenstoringreferencetrajectoriesinD . Thus,whentrajectoriestowarddifferentgoals
seq
bifurcatefromthesamequantizedvector,i.e.,semanticallysimilarstates,theymaynotbecapturedbythecurrentversionof
LAGMAalgorithmiftheirreturnisrelativelylowcomparedtothatofotherreferencetrajectoriesalreadystoredinD .
seq
Thus,LAGMAmaynotexhibitstrongeffectivenessinsuchtasksuntilvariousreferencetrajectoriestowarddifferentgoals
arestoredforagivenquantizedvector.
Toimprove,onemayalsoconsiderthediversityofatrajectorywhenstoringareferencetrajectoryinD . Inaddition,goal
seq
orstrategy-dependentagent-wiseexecutionwouldenhancecoordinationinsuchproblemcases,butitmayleadtodelayed
learningineasytasks. Thestudyregardingthistrade-offwouldbeaninterestingdirectionforfutureresearch.
18LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
G.Computationalcostanalysis
G.1.Resourceusage
TheintroductionofanextendedVQcodebookinLAGMArequiresadditionalmemoryusagetoanoverallMARLframework.
Memoryusagedependsonthecodebooknumber(n ),thenumberofareferencetrajectory(indexsequence)tosave(k)in
c
sequencebufferD ,itsbatchtimelength(T),thetotalnumberofdatasavedformovingaveragecomputation(m),and
seq
datatype. MemoryusageofD andD arecomputedasfollows.
τχt VQ
• D :byte(dtype)×n ×k×T
τχt c
• D :byte(dtype)×n ×m
VQ c
Forexample,whenm=100,n =64,k =30,T =T =150,i.e.,themaximumtimestepdefinedbytheenvironment,
c max
and D and D use data type int64 and float32, respectively, resource usages by introducing extended VQ
τχt VQ
codebookarecomputedasfollows:
• (D ) : 8(int64)×64×30×150=2.19MiB
τχt max
• D : 4(float32)×64×100=25.6KiB
VQ
Here,(D ) valuerepresentsthepossiblemaximumvalueandtheactualvaluemayvarybasedonthegoal-reaching
τχt max
trajectoryofeachtask. Wecanseethatresourcerequirementduetotheintroductionoftheextendedcodebookismarginal
comparedtothatofthereplaybufferandtheGPU’smemorycapacity,suchas24GiBinGeForceRTX3090. Notethatany
ofthesememoryusagesdonotdependonthedimensionofstatessinceonlytheindex(z)ofthequantizedvector(x )ofa
q
sequenceisstoredinD .
τχt
G.2.Trainingtimeanalysis
InLAGMA,weneedtoconductanadditionalupdateforVQ-VAEandtheextendedcodebook. Thus,theupdatefrequency
ofVQ-VAEandtheextendedcodebookwouldaffecttheoveralltrainingtime. Inthemanuscript,weutilizetheidentical
updateintervalnvq = 10,indicatingtrainingonceevery10MARLtrainingiterationsforbothVQ-VAEandcodebook
freq
update. Table6representstheoveralltrainingtimetakenbyvariousalgorithmsfordiversetasks. GeForceRTX3090is
usedfor5m vs 6mandGeForceRTX4090for8m(sparse)andMMM2. Inthecaseof8m(sparse)task,thetraining
timevariesaccordingtowhetherthelearnedmodelfindspolicyachievingacommongoal. Thus,thetrainingtimeofthe
successfulcaseispresentedforeachalgorithm.
Table6: TrainingtimeforeachmodelinvariousSMACmaps(inhours).
Model 5m vs 6m(2M) 8m(sparse)(3M) MMM2(3M)
EMC 8.6 11.8 12.0
MASER 12.7 13.4 20.5
LDSA 5.6 11.0 9.8
LAGMA 10.5 12.6 17.7
Here,numbersinparenthesisrepresentthemaximumtrainingtime(T )accordingtotasks. InTable6,wecanseethat
env
trainingofLAGMAdoesnottakemuchtimecomparedtoexistingbaselinealgorithms. Therefore,wecanconcludethatthe
introductionofVQ-VAEandtheextendedcodebookinLAGMAimposesanacceptablecomputationalburden,withonly
marginalincreasesinresourcerequirements.
19