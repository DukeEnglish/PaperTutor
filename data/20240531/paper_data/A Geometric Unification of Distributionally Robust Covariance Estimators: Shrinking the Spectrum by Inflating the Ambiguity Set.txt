A Geometric Unification
of Distributionally Robust Covariance Estimators:
Shrinking the Spectrum by Inflating the Ambiguity Set
MAN-CHUNGYUE,YVESRYCHENER,DANIELKUHN,VIETANHNGUYEN
Abstract. Thestate-of-the-artmethodsforestimatinghigh-dimensionalcovariancematricesallshrinkthe
eigenvalues of the sample covariance matrix towards a data-insensitive shrinkage target. The underlying
shrinkage transformation is either chosen heuristically—without compelling theoretical justification—or op-
timallyinviewofrestrictivedistributionalassumptions. Inthispaper,weproposeaprincipledapproachto
constructcovarianceestimatorswithoutimposingrestrictiveassumptions. Thatis,westudydistributionally
robustcovarianceestimationproblemsthatminimizetheworst-caseFrobeniuserrorwithrespecttoalldata
distributionsclosetoanominaldistribution,wheretheproximityofdistributionsismeasuredviaadivergence
onthespaceofcovariancematrices. Weidentifymildconditionsonthisdivergenceunderwhichtheresulting
minimizers represent shrinkage estimators. We show that the corresponding shrinkage transformations are
intimatelyrelatedtothegeometricalpropertiesoftheunderlyingdivergence. Wealsoprovethatourrobust
estimatorsareefficientlycomputableandasymptoticallyconsistentandthattheyenjoyfinite-sampleperfor-
manceguarantees. Weexemplifyourgeneralmethodologybysynthesizingexplicitestimatorsinducedbythe
Kullback-Leibler, Fisher-Rao, and Wasserstein divergences. Numerical experiments based on synthetic and
realdatashowthatourrobustestimatorsarecompetitivewithstate-of-the-artestimators.
1. Introduction
The covariance matrix Σ of a random vector ξ ∈ Rp is a fundamental summary statistic that captures
0
the dispersion of ξ. Together with the mean vector µ , it characterizes a unique member of the family of
0
Gaussian distributions, which occupies the central stage in statistics and probability theory. Hence, any
probabilistic model involving Gaussian distributions requires an estimate of Σ as an input. For example,
0
Gaussian distributions are ubiquitous in finance (e.g., in portfolio theory [41]), in statistical learning (e.g.,
in linear and quadratic discriminant analysis [20, § 4.3]) or control and signal processing (e.g., in Kalman
filtering[25]). Inaddition,Σ isintimatelyrelatedtothecorrelationmatrix,includingthePearsoncorrelation
0
coefficients [48], and it permeates medical statistics [60] and correlation network analysis [13, 40] etc.
If the distribution P of ξ is known, then the mean vector µ
0
= E P[ξ] and the covariance matrix Σ
0
=
E P[(ξ−µ 0)(ξ−µ 0)⊤]canbeobtainedbyevaluatingtherelevantintegralswithrespecttoP—eitheranalytically
or via numerical integration quadratures. If P is unknown, however, one typically has to estimate µ and Σ
0 0
from n independent samples ξ(cid:98)1,...,ξ(cid:98)n ∼ P. Arguably the simplest estimators for µ
0
and Σ
0
are the sample
meanµ
(cid:98)SA
= n1 (cid:80)n i=1ξ(cid:98)i andthesamplecovariancematrixΣ(cid:98)SA = n−1 1(cid:80)n i=1(ξ(cid:98)i−µ (cid:98)SA)(ξ(cid:98)i−µ (cid:98)SA)⊤,respectively.
AnelementarycalculationshowsthatΣ(cid:98)SAisunbiased. Uptoscaling,Σ(cid:98)SAfurthercoincideswiththemaximum
likelihood estimator for Σ provided that P constitutes a normal distribution. In 1975, much to the surprise
0
ofstatisticians,CharlesSteinshowedthatonecanstrictlyreducethemeansquarederrorofΣ(cid:98)SA byshrinking
it towards a constant matrix independent of the data [23, 57]. Even though it improves the mean squared
error, Stein’s shrinkage transformation suffers from two major shortcomings, that is, it may alter the order
of the estimator’s eigenvalues and may even render some eigenvalues negative [51]. Nonetheless, since Stein’s
surprising discovery, the study of shrinkage estimators embodies an important research area in statistics.
Date:May31,2024.
The authors are with the University of Hong Kong (mcyue@hku.hk), the Ecole Polytechnique F´ed´erale de Lausanne
(yves.rychener, daniel.kuhn@epfl.ch),andtheChineseUniversityofHongKong(nguyen@se.cuhk.edu.hk).
1
4202
yaM
03
]LM.tats[
1v42102.5042:viXraA GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 2
NotealsothatΣ(cid:98)SA isill-conditionedifp≲nandevensingularifp>n[63]. Indeed,asΣ(cid:98)SA isunbiasedand
asthemaximumeigenvaluefunctionisconvexonthespaceofsymmetricmatrices,Jensen’sinequalityensures
thatthelargesteigenvalueofΣ(cid:98)SA exceeds,inexpectation,thelargesteigenvalueofΣ 0. Similarly,thesmallest
eigenvalue of Σ(cid:98)SA undershoots, in expectation, the smallest eigenvalue of Σ 0. Hence, the condition number
of Σ(cid:98)SA, defined as the ratio of its largest to its smallest eigenvalue, tends to exceed the condition number
of Σ . This effect is most pronounced if Σ is (approximately) proportional to the identity matrix I and is
0 0 p
exacerbated with increasing dimension p. A simple and effective method to improve the condition number
is to construct a linear shrinkage estimator by forming a convex combination of Σ(cid:98)SA and a data-insensitive
shrinkage target such as p1Tr[Σ(cid:98)SA]I
p
[32]. Other popular shrinkage targets include the constant correlation
model [31], that is, a modified sample covariance matrix under which all pairwise correlations are equalized,
thesingleindexmodel[30], thatis, thesumofarank-oneandadiagonalmatrixrepresentingsystematicand
idiosyncratic risk factors as in Sharpe’s single index model [56], and the diagonal matrix model [61], that is,
thediagonalmatrixthatcontainsallsampleeigenvaluesonitsmaindiagonal. TheshrinkageweightofΣ(cid:98)SA is
usuallytunedtominimizetheFrobeniusrisk, thatis,theexpectedsquaredFrobeniusnormdistancebetween
the estimator and Σ . Linear shrinkage estimators can be computed highly efficiently, improve the condition
0
number of the sample covariance matrix, and are guaranteed to have full rank even if p>n.
Intheremainderofthepaper,wefocusoncovarianceestimatorsthatdependonthesamplesonlyindirectly
throughthesamplecovariancematrix. Thisassumptionisunrestrictive. Indeed,itissatisfiedbyallcommonly
used covariance estimators. Moreover, it comes at no loss of generality if P is a normal distribution, in which
caseΣ(cid:98)SA constitutesasufficientstatisticforΣ 0. WithoutpriorinformationabouttheeigenvectorsofΣ 0,itis
natural to restrict attention to rotation equivariant estimators. Rotation equivariance means that evaluating
theestimatorΣ(cid:98) ontherotateddataset{Rξ(cid:98)i}N I=1isequivalenttoevaluatingtherotatedestimatorRΣ(cid:98)R⊤onthe
theoriginaldataset{ξ(cid:98)i}n i=1foranyrotationmatrixR. OnecanshowthatanyrotationequivariantestimatorΣ(cid:98)
commutes with the sample covariance matrix Σ(cid:98)SA, that is, Σ(cid:98)SA and Σ(cid:98) share the same eigenvectors, and the
spectrum of Σ(cid:98) can be viewed as a transformation of the spectrum of Σ(cid:98)SA[49, Lemma 5.3]. Such spectral
transformations are referred to as a shrinkage transformations. Note that the linear shrinkage estimators
discussed above are rotation equivariant only if the shrinkage target commutes with Σ(cid:98)SA.
If P is governed by a spiked covariance model, that is, if P is Gaussian, p and n tend to infinity at an
asymptoticallyconstantratioandΣ constitutesafixed-rankperturbationoftheidentitymatrix,thenonecan
0
useresultsfromrandommatrixtheorytoconstructthebestrotationequivariantestimatorsinclosedformfor
a broad range of different loss functions [12]. Nonlinear shrinkage estimators that are asymptotically optimal
with respect to the Frobenius loss can also be constructed in the absence of any normality assumptions, and
theycansignificantlyimproveonlinearshrinkageestimatorsiftheeigenvaluespectrumofΣ isdispersed[33,
0
35]. Similarly,onecanconstructoptimalshrinkageestimatorsfortheinversecovariancematrixΣ−1,whichis
0
usuallytermedtheprecisionmatrix; see[8,36]. However,theavailablestatisticalguaranteesforallshrinkage
estimators described above are asymptotic and depend on assumptions about the structure of P and/or the
convergence properties of the spectral distribution of Σ(cid:98)SA, which may be difficult to check in practice.
Inthispaper,weproposeaflexibleandprincipledapproachtoestimatethecovariancematrixΣ byusing
0
ideas from distributionally robust optimization (DRO). Specifically, our approach generates a rich family of
covariance matrix estimators corresponding to different ambiguity sets that can encode prior distributional
information. All emerging estimators are rotation equivariant and thus represent nonlinear shrinkage estima-
tors. In addition, they all improve the condition number of the sample covariance matrix, are invertible, and
preserve the order of the sample eigenvalues. They also offer finite sample guarantees on the prediction loss
and are asymptotically consistent. These appealing properties are not enforced ad hoc but emerge naturally
from the solution of a principled distributionally robust estimation model. We emphasize that our results
do not rely on any restrictive assumptions such as the requirement that P is Gaussian or that the spectral
distribution of Σ(cid:98)SA converges to a well-defined limit as p and n tend to infinity at a constant ratio.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 3
To develop the distributionally robust estimation model to be studied in this paper, we first express the
unknown true covariance matrix Σ as the minimizer of a stochastic optimization problem involving the
0
unknown probability distribution P. Specifically, adopting the standard assumption that µ
0
= E P[ξ] = 0
[32, 33, 34, 36] and noting that the squared Frobenius norm is strictly convex, we obtain
{Σ 0}=Argmin ∥X−Σ 0∥2
F
=Argmin Tr[X2]−2Tr[XΣ 0]=Argmin Tr[X2]−2Tr[XE P[ξξ⊤]].
X∈Sp X∈Sp X∈Sp
+ + +
Ifwecouldsolvethestochasticoptimizationproblemontheright-handsideoftheaboveexpression,wecould
preciselyrecovertheidealestimatorX⋆ =Σ . Thisisimpossible,however,becausethedistributionPneeded
0
to evaluate the stochastic optimization problem’s objective function is unknown. Nevertheless, replacing P
with a nominal distribution P (cid:98) constructed from the n training samples yields the nominal estimation model
min Tr[X2]−2E (cid:2) ξ⊤Xξ(cid:3) , (1)
X∈Sp
P(cid:98)
+
which requires no unavailable inputs. An elementary calculation shows that (1) is uniquely solved by Σ(cid:98) =
E P(cid:98)[ξξ⊤], which is the covariance matrix of ξ under the nominal distribution P (cid:98), provided that µ (cid:98)=E P(cid:98)[ξ]=0.
Ofcourse,characterizingΣ(cid:98) asaminimizerof (1)hasnoconceptualorcomputationalbenefitsbecausewehave
to compute the integral E [ξξ⊤] already to evaluate the objective function of (1). Nevertheless, the nominal
P(cid:98)
estimation problem (1) is useful because it allows us to construct a broad range of nonlinear shrinkage
estimators in a principled and systematic manner by robustifying the prediction loss.
Any nominal distribution P (cid:98) constructed from a finite dataset must invariably differ from the true data-
generating distribution P. Estimation errors in P (cid:98) are conveniently captured by an ambiguity set of the form
¶ ©
U ε(P (cid:98))= Q:Q∼(0,Σ), D(Σ,Σ(cid:98))≤ε , (2)
whereQ∼(0,Σ)indicatesthatξ hasmean0andcovariancematrixΣunderQ,andDrepresentsadivergence
on the space of positive semidefinite matrices. Divergences are general distance-like functions that are non-
negative and satisfy the identity of indiscernibles (that is, they satisfy D(Σ,Σ(cid:98)) = 0 if and only if Σ = Σ(cid:98)).
However,divergencesmayfailtobesymmetricandmayviolatethetriangleinequality. Intuitively,U ε(P (cid:98))can
beviewedasadivergenceballofradiusε≥0aroundP (cid:98) inthespaceofprobabilitydistributions. Robustifying
the nominal estimation problem (1) against all distributions in U ε(P (cid:98)) yields the following DRO problem.
min sup Tr[X2]−2E Q(cid:2) ξ⊤Xξ(cid:3) (3)
X∈Sp
+ Q∈U ε(P(cid:98))
Problem (3) seeks an estimator X that minimizes the worst-case expected prediction loss across all distribu-
tionsinU ε(P (cid:98)). Notethatifε=0,thentheDROproblem(3)collapsestothenominalestimationproblem(1)
because the divergence D satisfies the identity of indiscernibles, which ensures that U 0(P (cid:98))={P (cid:98)}. Hence, (3)
embeds (1) into a family of estimation models parametrized by D and ε. Moreover, DRO models naturally
bridgeoptimizationandstatisticsinthattheyofferanintuitivewaytoderivegeneralizationbounds. Indeed,
if ε is tuned to ensure that U ε(P (cid:98)) contains the data-generating distribution P with high confidence 1−β,
then the optimal value of the DRO problem (3) provides a (1−β)-upper confidence bound on the prediction
loss of its unique minimizer X⋆ under P [42]. Stronger generalization bounds that do not require P to belong
toU ε(P (cid:98))areprovidedin[7,15]. EveniftheambiguitysetdoesnotcontainP,DROmodelstendtoyieldhigh-
quality solutions because there is a deep connection between robustification and regularization [16, 53, 54].
This connection may also explain the empirical success of DRO in statistical estimation [6, 27, 59].
The flexibility to choose the divergence D underlying the ambiguity set U ε(P (cid:98)) is both a blessing and a
curse. On the one hand, D can encode prior distributional information and thus lead to better estimators.
Ontheotherhand,thefamilyofdivergencesisvast. Hence,thechoiceofasuitableinstancecouldoverwhelm
the modeler. Given the statistical estimation task at hand, it makes sense to restrict attention to diver-
gences that admit a statistical interpretation. Many popular divergences on the space of covariance matrices
are obtained by restricting a divergence on the space of probability distributions to the family of normal
distributions. For example, the Kullback-Leibler divergence, the 2-Wasserstein distance, or the Fisher-RaoA GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 4
distancebetweenzero-meannormaldistributionsalladmitclosed-formformulasintermsofthedistributions’
covariance matrices. These ‘Gaussian’ divergences are popular because they are conducive to tractable DRO
models in risk management [17, 44], ethical machine learning [10, 66], likelihood evaluation [46, 47], Kalman
filtering [71, 55] and control [58] etc. In addition, the shrinkage estimator for the inverse covariance matrix
proposed in [43] also leverages a ‘Gaussian’ divergence. Nonetheless, the approach proposed in this paper
does not rely on the assumption that P is Gaussian.
The main contributions of this paper can be summarized as follows.
• Weproposearichfamilyofdistributionallyrobustcovariancematrixestimators. Eachestimatorisdefined
as a solution of (3) for a particular ambiguity set of the form (2). Here, the nominal covariance matrix Σ(cid:98)
characterizes the center, the divergence D determines the geometry, and the radius ε determines the size
of the ambiguity set. We demonstrate that all such estimators are well-defined, unique and efficiently
computable under natural structural assumptions on D and mild regularity conditions on Σ(cid:98) and ε.
• We prove that our distributionally robust covariance matrix estimators constitute nonlinear shrinkage
estimators, that is, they have the same eigenbasis as Σ(cid:98), and their eigenvalues are obtained by shrinking
thespectrumofΣ(cid:98) towards0byusinganonlinearshrinkagetransformationdependingonDandashrinkage
intensity depending on ε. We further prove that these estimators improve the condition number of Σ(cid:98).
• Weidentifyvariousdivergencescommonlyusedinstatistics,machinelearningandinformationtheorythat
satisfy the requisite regularity conditions. To this end, we generalize Sion’s classic minimax theorem from
EuclideanspacestoRiemannianmanifolds,whichcouldbeofindependentinterest. Wealsoexemplifyour
framework by deriving explicit analytical formulas for the distributionally robust covariance estimators
induced by the Kullback-Leibler divergence, the 2-Wasserstein distance and the Fisher-Rao distance.
• We prove that, if ε scales with the sample size n as O(n−1 2), then the proposed estimators are strongly
consistent and enjoy finite-sample performance guarantees at a fixed confidence level. Numerical exper-
iments based on synthetic as well as real data for portfolio optimization and binary classification tasks
suggest that our robust estimators are competitive with state-of-the-art estimators from the literature.
Thefirstrobustnessinterpretationofashrinkageestimatorwasdiscoveredinthecontextofinversecovari-
ance matrix estimation [43]. Specifically, it was shown that a particular nonlinear shrinkage estimator can be
obtained by robustifying the maximum likelihood estimator for Σ−1 across all Gaussian distributions of the
0
trainingsampleswithinaprescribedWassersteinball. Thisresultcriticallyreliesontherestrictiveassumption
that the unknown data-generating distribution, the nominal distribution as well as all other distributions in
the Wasserstein ball are Gaussian. In addition, this result has not been extended to more general ambiguity
sets based on other divergences beyond the 2-Wasserstein distance, thus limiting the modeler’s flexibility.
In this paper we show that a broad spectrum of shrinkage estimators for Σ can be obtained from a
0
versatile DRO model that does not rely on restrictive normality assumptions. That is, we seek the most
generalconditionsontheDROmodelunderwhichashrinkageeffectemerges. Inaddition,weuncoveradeep
connectionbetweenthegeometryoftheambiguityset,whichisdeterminedbythechoiceofthedivergenceD,
and the nonlinear shrinkage transformation of the corresponding distributionally robust estimator.
Notation. We use R=R∪{+∞} as a shorthand for the extended real line. The space of p-dimensional
realvectorsanditssubsetsof(entry-wise)non-negativeandpositivevectorsaredenotedbyRp,Rp,andRp ,
+ ++
respectively. Similarly, thespaceofsymmetricmatricesinRp×p, aswellasitssubsetsofpositivesemidefinite
andpositivedefinitematrices,aredenotedbySp,Sp,andSp ,respectively. Thegroupoforthogonalmatrices
+ ++
in Rp×p is denoted by O , and I stands for the identity matrix in Rp×p. For any x∈Rp, we use x↓ and x↑
p p
to denote the vectors obtained by rearranging the entries of x in non-increasing and non-decreasing order,
respectively. The trace of a matrix S ∈ Sp is defined as Tr[S] = (cid:80)p S . Finally, ∥M∥ = sup ∥Mv∥
i=1 ii ∥v∥2=1 2
and ∥M∥
F
=Tr[M⊤M]21 stand for the spectral norm and the Frobenius norm of M, respectively.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 5
2. Overview of Main Results
The distributionally robust estimation problem (3) perturbs—and thereby hopefully improves—the nom-
inal estimator Σ(cid:98) in view of the divergence D. We now derive a simple reformulation of (3) as a standard
minimization problem, and we informally outline the main properties of the corresponding optimal solution,
whichwillbeestablishedrigorouslyintheremainderofthepaper. Fromnowon, thenominalcovariancema-
trix Σ(cid:98) can be viewed as any na¨ıve initial estimator for the covariance matrix Σ 0. The construction of Σ(cid:98) from
the samples ξ(cid:98)1,...,ξ(cid:98)n is immaterial for most of our discussion. As the loss function underlying problem (3)
is quadratic in ξ and as E Q[ξ] = 0, its expected value depends on Q only indirectly through the covariance
matrix Σ=E Q[ξξ⊤]. Thus, the DRO problem (3) is equivalent to the robust covariance estimation problem
min max Tr[X2]−2Tr[ΣX] (4)
X∈Sp
+ Σ∈Bε(Σ“)
with uncertainty set
¶ ©
B ε(Σ(cid:98))= Σ∈Sp
+
:D(Σ,Σ(cid:98))≤ε . (5)
WestressthatthedivergencefunctionDmayfailtobesymmetric,thatis,D(X,Y)maydifferfromD(Y,X).
It is therefore important to remember the convention that Σ(cid:98) is the second argument of D in the definition
of B ε(Σ(cid:98)). Note also that B ε(Σ(cid:98)) grows with the size parameter ε and collapses to the singleton {Σ(cid:98)} for ε=0.
Therobustestimationproblem(4)constitutesazero-sumgamebetweenthestatistician,whomovesfirstand
chooses the estimator X, and nature, who moves second and chooses the covariance matrix Σ. The following
dual estimation problem is obtained by interchanging the order of minimization and maximization in (4).
max min Tr[X2]−2Tr[ΣX] (6)
Σ∈Bε(Σ“)
X∈Sp
+
From now on, we denote by X⋆ and Σ⋆ the optimal solutions of the primal and dual estimation problems (4)
and (6), respectively. In Section 3.1 below, we will identify mild conditions on D and Σ(cid:98) under which X⋆
and Σ⋆ are indeed guaranteed to exist and to be unique. If the uncertainty set B ε(Σ(cid:98)) is convex and compact,
then strong duality prevails (that is, (4) and (6) share the same optimal value) by Sion’s classic minimax
theorem. Asseveralpopulardivergencefunctionsarenon-convexintheirfirstargumentandthusinduceanon-
convexuncertaintysetB ε(Σ(cid:98));however,wewilldevelopageneralizedminimaxtheoremthatguaranteesstrong
duality under significantly more general conditions. Whenever strong duality holds, (X⋆,Σ⋆) constitutes a
Nash equilibrium of the zero-sum game between the statistician and nature [52, Lemma 36.2].
A cursory glance at its first-order optimality condition reveals that the inner minimization problem in (6)
is solved by X = Σ. Hence, the inner minimum evaluates to −Tr[Σ2] = −∥Σ∥2 . Eliminating the factor −1
F
further shows that Σ⋆ solves the maximization problem (6) if and only if it solves the minimization problem
¶ ©
Σm ∈i Sn
p
∥Σ∥2
F
: D(Σ,Σ(cid:98))≤ε . (P Mat)
+
Thus,nature’sNashstrategyΣ⋆ canbecomputedbysolving(P )insteadof (6). Bythedefiningproperties
Mat
of Nash strategies, the statistician’s Nash strategy X⋆ must be a best response to Σ⋆, that is, X⋆ must solve
theinnerminimizationproblemin(6)forΣ=Σ⋆. However,theuniqueoptimalsolutionofthisminimization
problem is Σ⋆. In summary, this reasoning implies that if strong duality holds, then the Nash strategies X⋆
andΣ⋆ ofthestatisticianandnaturecoincideandarebothgivenbytheuniqueminimizerofproblem(P ).
Mat
Problem (P ) is reminiscent of a ridge regression problem [21, 64], which seeks an estimator that mini-
Mat
mizesaweightedsumofaleastsquaresfidelitytermandaFrobeniusnormregularizationterm. Indeed,prob-
lem(P )seeksacovariancematrixΣwithminimumFrobeniusnormandafidelityerrorofatmostε,where
Mat
the fidelity of Σ with respect to the nominal covariance estimator Σ(cid:98) is measured by the divergence D(Σ,Σ(cid:98)).A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 6
Divergence function D(Σ,Σ(cid:98)) Domain
Ä ä
Kullback-Leibler / Stein [28] 1 Tr[Σ(cid:98)−1Σ]−p+logdet(Σ(cid:98)Σ−1) Sp ×Sp
2 ++ ++
Wasserstein [18] Tr[Σ+Σ(cid:98) −2(cid:0) ΣΣ(cid:98)(cid:1)1 2] Sp ×Sp
+ +
(cid:13) (cid:13)2
Fisher-Rao [3] (cid:13) (cid:13)log(Σ(cid:98)− 21ΣΣ(cid:98)−1 2)(cid:13)
(cid:13)
Sp ++×Sp
++
F
Ä ä
Inverse Stein [28] 1 Tr[Σ−1Σ(cid:98)]−p+logdet(ΣΣ(cid:98)−1) Sp ×Sp
2 ++ ++
Ä ä
Symmetrized Stein / Jeffreys divergence [24] 1 Tr[ΣΣ(cid:98)−1+Σ(cid:98)Σ−1]−2p Sp ×Sp
2 ++ ++
Quadratic / Squared Frobenius Tr[(Σ−Σ(cid:98))2] Sp ×Sp
+ +
Weighted quadratic Tr[(Σ−Σ(cid:98))2Σ(cid:98)−1] Sp ×Sp
+ ++
Table 1. Popular divergence functions and their domains. We adopt the convention from
convex analysis that each divergence evaluates to +∞ outside of its domain.
Wenowinformallystateourkeyresult,whichapplies,amongothers,toalldivergencefunctionsofTable1.
Theorem 1 (Distributionally robust estimator (informal)). If D is any divergence functions from Table 1,
the nominal covariance matrix Σ(cid:98) satisfies a regularity condition, and ε > 0 is not too large, then the distri-
butionally robust estimator X⋆ exists, is unique, and can be computed efficiently via the following procedure.
(1) Compute the eigenvalues and the eigenvectors of the nominal covariance matrix Σ(cid:98).
(2) Construct the inverse shrinkage intensity γ⋆ by solving a univariate nonlinear equation that depends
only on the spectrum of Σ(cid:98).
(3) Shrink the eigenvalues of Σ(cid:98) by applying a nonlinear transformation that depends only on γ⋆.
(4) Construct X⋆ by combining the eigenvectors found in step (1) with the eigenvalues found in step (3).
The estimator X⋆ constructed in this manner preserves the eigenvectors of Σ(cid:98), shrinks the eigenvalues of Σ(cid:98),
and reduces the condition number of Σ(cid:98). Thus, it represents a nonlinear shrinkage estimator.
Theorem1revealsthatawiderangeofnonlinearshrinkageestimatorsadmitarobustnessinterpretationin
the sense that they correspond to solutions of the distributionally robust estimation problem (3) for different
divergence functions. This insight is of interest from a statistical point of view because it relates nonlinear
shrinkageestimatorstodistributionalambiguitysets,whichcanbeusedtoderivenewgeneralizationbounds.
Theorem 1 also implies that the distributionally robust estimation problem (3) can be solved efficiently by
diagonalizing Σ(cid:98) and solving a univariate nonlinear equation, both of which are computationally cheap.
3. Distributionally Robust Covariance Shrinkage Estimators
Thissectionformallyintroducesourdistributionallyrobustestimationframework. Specifically,Section3.1
details all technical assumptions needed throughout the paper, Section 3.2 formally states the main result,
and Section 3.3 describes several desirable properties of the emerging distributionally robust estimators.
3.1. Assumptions
The uncertainty set B ε(Σ(cid:98)) is non-convex for some choices of the divergence function D. In these cases,
we cannot use Sion’s minimax theorem to establish strong duality between the primal and dual estimationA GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 7
problems (4) and (6), respectively. Instead, we will have to develop a more nuanced minimax theorem. For
now, we assume that such a minimax theorem is readily available.
Assumption 1 (Minimax property). The minimum of the primal estimation problem (4) coincides with the
maximum of the dual estimation problem (6).
We will later see that Assumption 1 is satisfied for all divergence functions listed in Table 1. In addition,
we require D to constitute a spectral divergence in the sense of the following assumption.
Assumption 2 (Spectral divergence). The divergence function D : Sp × Sp → R is non-negative, and
+ +
satisfies the identity of indiscernibles, that is, for any (X,Y) ∈ dom(D) we have D(X,Y)= 0 if and only if
X =Y. In addition, D satisfies the following structural conditions.
(a) (Orthogonal equivariance) For any X,Y ∈Sp and V ∈O(p) we have D(X,Y)=D(VXV⊤,VYV⊤).
+
(b) (Spectrality) There exists a function d:R ×R →R such that
+ +
p
(cid:88)
D(Diag(x),Diag(y))= d(x ,y ) ∀x,y ∈Rp
i i +
i=1
and d(a,b) is continuous1 in a for every b>0. In the following, we refer to d as the generator of D.
(c) (Rearrangement property) For any x,y ∈Rp and V ∈O(p) we have
+
D(cid:0)
V
Diag(x↑)V⊤,Diag(y↑)(cid:1) ≥D(cid:0) Diag(x↑),Diag(y↑)(cid:1)
.
If its left side is finite, this inequality becomes an equality if and only if Diag(x↑)=V Diag(x↑)V⊤.
Assumptions 2(a) and 2(b) imply that if X and Y are simultaneously diagonalizable, then the divergence
of X with respect to Y depends only on the spectra of X and Y and the generator d. Specifically, we have
p
(cid:88)
D(X,Y)=D(V Diag(x)V⊤,V Diag(y)V⊤)=D(Diag(x),Diag(y))= d(x ,y ), (7)
i i
i=1
where the entries of the vectors x and y represent the eigenvalues and where the columns of the orthonormal
matrix V represent the (common) eigenvectors of X and Y, respectively. Note that the last two equalities
in (7) readily follow from 2(a) and 2(b). Assumption 2(b) further implies that if D is a spectral divergence
on Sp, then its generator d is a spectral divergence on R . Indeed, restricting x and y to multiples of the
+ +
vector of all ones reveals via Assumption 2(b) that dom(d) = {(a,b) ∈ R2 : (aI ,bI ) ∈ dom(D)} and
+ d d
that d inheritscontinuity, non-negativity andthe identity ofindiscerniblesfrom D. Orthogonalequivariance,
spectrality, and the rearrangement inequality are trivially satisfied in the one-dimensional case. Finally, we
pointoutthatAssumption2(c)isreminiscentoftheHardy-Littlewood-Polyakrearrangementinequality[19],
which asserts that (x↑)⊤y↓ ≤x⊤y ≤(x↑)⊤y↑ for any vectors x,y ∈Rp.
Our results also require the following assumptions about the eigenvalues xˆ ,...,xˆ of the nominal covari-
1 p
ance matrix Σ(cid:98) as well as about the radius ε of the uncertainty set B ε(Σ(cid:98)).
Assumption 3 (Regularity of input parameters). The following hold.
(a) For any i=1,...,p we have (xˆ ,xˆ )∈dom(d).
i i
(b) The radius ε of the uncertainty set satisfies 0<ε<ε¯, where
ε¯=(cid:80)p
d(0,xˆ ).
i=1 i
Together with Assumptions 2(a) and 2(b), Assumption 3(a) ensures that the nominal covariance matrix Σ(cid:98)
is feasible in problem (P Mat). Indeed, inserting X = Y = Σ(cid:98) into (7) implies that D(Σ(cid:98),Σ(cid:98)) = 0. This implies
that (Σ(cid:98),Σ(cid:98))∈dom(D) and, more importantly, that the feasible region of problem (P Mat) is non-empty. This
assumptionisnotentirelyinnocentbecausesomedivergencefunctionsfromTable1havedomainSp ×Sp .
++ ++
In all these cases, Assumption 3(a) requires that Σ(cid:98) has full rank and, if Σ(cid:98) is the sample covariance matrix,
that the sample size n is at least as large as the dimension p. Assumption 3(b) ensures that the radius ε>0
1Byconvention,acontinuousextendedreal-valuedfunctionmusttendto∞whenapproachingtheboundaryofitsdomain.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 8
is small enough for the feasible region of the reformulated dual estimation problem (P ) not to contain 0.
Mat
Otherwise, problem (P ) would trivially be solved by the nonsensical estimator X⋆ =0.
Mat
Assumption 4 (Smoothness and convexity of the generator d). For any b>0, the function d(·,b) is twice
continuously differentiable throughout R and convex on the interval [0,b].
++
Assumption 4 implies that the domain of d(·,b) contains R for every b>0. Hence, d(a,b) can evaluate
++
to +∞ only at a = 0, which means that the domain of d(·,b) is either R or R . We emphasize that the
+ ++
convexity of d(·,b) on the interval [0,b] does not imply that problem (P ) is convex. However, we will see
Mat
below that this restricted convexity assumption helps us to reduce problem (P ) to a convex program.
Mat
3.2. Construction of the Distributionally Robust Estimator
We need the following notation to restate Theorem 1 rigorously. We denote the i-th smallest eigenvalue
of a symmetric matrix S ∈ Sp by λ (S), and we use λ(S) = (λ (S),...,λ (S)) as a shorthand for the
i 1 p
spectrum of S. We also reserve the symbols xˆ
i
= λ i(Σ(cid:98)) and vˆ
i
for the non-negative eigenvalues and the
corresponding orthonormal eigenvectors of the nominal covariance matrix Σ(cid:98). In addition, we use xˆ = λ(Σ(cid:98))
and V“=(vˆ 1,...,vˆ p) to denote the nominal spectrum and the orthogonal matrix of the nominal eigenvectors,
respectively. The nominal covariance matrix thus admits the spectral decomposition Σ(cid:98) =V“Diag(xˆ)V“⊤. We
also define the auxiliary function s:R2 →R corresponding to a divergence function with generator d via
+
(cid:40)
the unique solution a⋆ ≥0 of the equation 0=2a⋆+γ∂d(a⋆,b) if b>0 and γ >0,
s(γ,b)= ∂a (8)
0 if b=0 or γ =0.
In the remainder of the paper, we refer to s as the eigenvalue map. We will see below that it is well-defined
under Assumption 4, which implies that the nonlinear equation in (8) has a unique solution whenever b>0.
We will also prove that s(γ,b) ≤ b for every γ,b ≥ 0, which means that it can be viewed as a shrinkage
transformation that maps any input eigenvalue b≥0 to a smaller output eigenvalue s(γ,b) for every fixed γ.
Given these conventions, we are now ready to restate Theorem 1 formally.
Theorem 1 (Distributionally robust estimator (formal)). If Assumptions 1–4 hold, then the distributionally
robust estimator X⋆ exists and is unique. If, additionally, γ⋆ is the unique positive root of the equation
p
(cid:88)
d(s(γ,xˆ ),xˆ )−ε=0,
i i
i=1
then the distributionally robust estimator admits the spectral decomposition X⋆ =V“Diag(x⋆)V“⊤ with eigen-
values x⋆ =s(γ⋆,xˆ ), i=1,...,p, where 0<x⋆ <xˆ whenever xˆ >0 and x⋆ =0 whenever xˆ =0.
i i i i i i i
Theorem1providesaquasi-closedformexpressionfortheoptimalcovarianceestimatorX⋆ thatsolvesthe
robust estimation problem (4) as well as its dual reformulation (P ). In particular, it shows that X⋆ has
Mat
the same eigenvectors as Σ(cid:98) and that all positive eigenvalues of X⋆ can be computed by solving a nonlinear
equation parametrized by γ⋆. Remarkably, this nonlinear equation admits a closed-form solution for all
divergences listed in Table 1. In addition, we will see that γ⋆ can be computed efficiently by bisection. All of
thisimpliesthatthecomplexityofcomputingX⋆ islargelydeterminedbythecomplexityofdiagonalizingΣ(cid:98).
In addition, we will see that x⋆ =s(γ⋆,xˆ ) decreases with γ⋆. Thus, X⋆ and γ⋆ are naturally interpreted as
i i
a nonlinear shrinkage estimator and inverse shrinkage intensity, respectively.
We now outline the high-level structure of the proof of Theorem 1; see Figure 1 for a visualization. The
proofisdividedintothreestepsthatgiverisetothreepropositions. Proposition1belowfirstshowsthatthere
isaone-to-onerelationshipbetweentheminimizersoftherobustestimationproblem(4)andproblem(P ).
Mat
Proposition 1 (Dual characterization of X⋆). If Assumption 1 holds, then the primal and dual robust
estimation problems (4) and (6) are equivalent to problem (P ) in the following sense.
MatA GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 9
(i) If Σ⋆ solves (P ), then X⋆ =Σ⋆ solves (4), and Σ⋆ solves (6).
Mat
(ii) If X⋆ solves (4) and Σ⋆ solves (6), then X⋆ coincides with Σ⋆ and solves (P ).
Mat
TheproofofProposition1followsimmediatelyfromthediscussioninSection2andisthusomitted. Next,
we show that problem (P ), which optimizes over all matrices in the positive semidefinite cone Sp, is
Mat +
equivalent to problem (P ) below, which optimizes over all vectors in the non-negative orthant Rp:
Vec +
(cid:40) p (cid:41)
min ∥x∥2 : (cid:88) d(x ,xˆ )≤ε . (P )
x∈Rp 2 i i Vec
+ i=1
We henceforth use x⋆ to denote the unique minimizer of problem (P ) if it exists.
Vec
Problem (4) X⋆ =Σ⋆ Problem (P Mat)
{X⋆}≜Argmin max Tr[X2]−2Tr[ΣX] {Σ⋆}≜Argmin¶ ∥Σ∥2 : D(Σ,Σ(cid:98))≤ε©
Proposition 1 F
X∈Sp
+
Σ∈Bε(Σ“) Σ∈Sp
+
Σ⋆ =V“Diag(x⋆) V“⊤
Theorem 1 X⋆ =V“Diag(s(γ⋆,x (cid:98)i))V“⊤ Proposition 2 x⋆ =λ (Σ⋆)∀i
i i
First-order condition Problem (P )
Vec
γ⋆ >0 unique solution of x⋆ i =s(γ⋆,x (cid:98)i) ∀i (cid:40) p (cid:41)
(cid:88)p
d(s(γ,x (cid:98)i),x (cid:98)i)−ε=0 Proposition 3
{x⋆}≜A xr ∈g Rm
p
+in ∥x∥2
2
: (cid:88) i=1d(x i,x (cid:98)i)≤ε
i=1
Figure 1. Structure of the proof of Theorem 1. An arc indicates that the solution to the
problematthearc’stailcanbeusedtoconstructasolutionfortheproblematthearc’shead.
Proposition2(Equivalenceof (P )and(P )). IfAssumption2holds, thenproblem (P )isequivalent
Mat Vec Mat
to problem (P ) in the following sense.
Vec
(i) If x⋆ solves (P Vec), then V“Diag(x⋆)V“⊤ solves (P Mat).
(ii) If Σ⋆ solves (P ), then λ(Σ⋆) solves (P ).
Mat Vec
In the third and last step, we solve problem (P ) in quasi-analytical form. To this end, we denote
Vec
the Lagrange multiplier associated with the divergence constraint (cid:80)p d(x ,xˆ ) ≤ ε by γ⋆. The following
i=1 i i
proposition characterizes the unique solution of problem (P ) through an explicit function of γ⋆ and shows
Vec
that (P ) can be computed by solving a single nonlinear equation.
Vec
Proposition 3 (Solution of (P )). If Assumptions 2, 3 and 4 hold, then problem (P ) admits a unique
Vec Vec
optimal solution x⋆ with components x⋆ = s(γ⋆,x ), i = 1,...,p, where γ⋆ is the unique positive root of the
i (cid:98)i
equation(cid:80)p d(s(γ,xˆ ),xˆ )−ε=0. Wealsohave0<x⋆ <xˆ wheneverxˆ >0andx⋆ =0wheneverxˆ =0.
i=1 i i i i i i i
In summary, Proposition 3 provides a simple characterization of γ⋆ and shows how one can use γ⋆ to
construct a unique solution x⋆ for problem (P ). Proposition 2 reveals how x⋆ can be used to construct
Vec
a unique solution X⋆ for problem (P ), and Proposition 1 guarantees that X⋆ is uniquely optimal in the
Vec
robust estimation problem (4). Taken together, Propositions 1, 2 and 3 therefore prove Theorem 1.
3.3. Properties of the Distributionally Robust Estimator
We now highlight several desirable characteristics of the distributionally robust covariance estimator X⋆.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 10
3.3.1. Efficient Computation
We have seen that X⋆ can be constructed from x⋆, which can be constructed from γ⋆. In addition, we
have seen that the Lagrange multiplier γ⋆ is the unique positive root of the equation F(γ) = 0, where the
function F :R →R is defined through F(γ)=(cid:80)p d(s(γ,xˆ ),xˆ )−ε. The following proposition suggests
+ i=1 i i
that this root-finding problem can be solved highly efficiently by bisection or Newton’s method.
Proposition 4 (Structural properties of F). If Assumptions 2, 3 and 4 hold, then the function F is differ-
entiable and strictly decreasing on R . In addition, we have lim F(γ)>0 and lim F(γ)<0.
++ γ→0 γ→∞
Suppose now that we have access to an a priori upper bound γ¯ > 0 on the Lagrange multiplier γ⋆. Note
that γ¯ is guaranteed to exist under the assumptions of the proposition. Section 4 shows that γ¯ can be
constructed explicitly for several popular divergence functions. The structural properties of F established in
Proposition 4 allow us to estimate the number of function evaluations needed to compute γ⋆. For example,
γ⋆ can be computed via bisection to within an absolute error of δ > 0 using log (γ¯/δ) function evaluations.
2
Under additional mild conditions, γ⋆ can also be computed via Newton’s method to within an absolute error
of δ >0 using merely O(log log (γ¯/δ)) function and derivative evaluations [11, Theorem 2.4.3].
2 2
3.3.2. Shrinkage Properties
Proposition 3 asserts that if Assumptions 2, 3 and 4 hold, then the optimal solution x⋆ of problem (P )
Vec
is unique and can thus be seen as a function x⋆(ε) of the radius ε ∈ (0,ε¯) of the uncertainty set, where ε¯is
defined as in Assumption 3(b). In fact, x⋆(ε) can naturally be extended to a function on [0,ε¯]. As d satisfies
the identity of indiscernibles, we can define x⋆(0) = xˆ as the unique solution of problem (P ) for ε = 0.
Vec
In addition, we may define x⋆(ε¯) = 0. One can then show that each component of x⋆(ε) monotonically
decreasesto0on[0,ε¯]. ByTheorem1,thedistributionallyrobustestimatorX⋆ =V“Diag(x⋆)V“⊤ inheritsthe
eigenbasisfromthenominalcovariancematrixΣ(cid:98). Hence,X⋆ andΣ(cid:98) commute,andX⋆ isrotationequivariant.
In summary, these insights imply that X⋆ essentially shrinks the eigenvalues of Σ(cid:98) towards zero.
Proposition 5 (Shrinkage estimator). If Assumptions 2, 3 and 4 hold, then x⋆(ε) is non-increasing on [0,ε¯]
i
and satisfies lim x⋆(ε)=0 for every i=1,...,p. If additionally Assumption 1 holds, then X⋆ constitutes
ε↑ε¯ i
a shrinkage estimator, that is, it has the same eigenvectors as Σ(cid:98) and satisfies 0⪯X⋆ ⪯Σ(cid:98).
Proposition5assertsthattheeigenvaluesofX⋆ areboundedabovebythecorrespondingnominaleigenval-
ues. This shrinkage property persists across a remarkably broad class of estimators. The shrinkage effects of
robustificationwerefirstdiscoveredinadistributionallyrobustinverse covarianceestimationproblemwitha
Wassersteinambiguityset[43]. Theresultspresentedherearesignificantlymoregeneral. Indeed, theyreveal
that a broad class of divergence functions gives rise to diverse shrinkage estimators.
3.3.3. Improvement of the Condition Number
The condition number κ(X) of a positive definite matrix X ∈Sp is defined as the ratio of its largest to
++
its smallest eigenvalue. It is well known that unless n ≫ p, the sample covariance matrix Σ(cid:98)SA tends to be
ill-conditioned, that is, κ(Σ(cid:98)SA) ≫ 1 [63]. Therefore, most shrinkage estimators are designed to improve the
condition number of an ill-conditioned baseline estimator Σ(cid:98). Below we will show that the distributionally
robust estimator X⋆ is also guaranteed to improve the condition number of Σ(cid:98) whenever the generator d of
the divergence D satisfies a second-order differential inequality.
Assumption 5 (Differential inequality). The generator d of the divergence function D is twice continuously
differentiable on R2 and satisfies the following differential inequality for all a,b∈R with a<b.
++ ++
∂2 ∂2 ∂
a d(a,b)+b d(a,b)≥ d(a,b)
∂a2 ∂a∂b ∂aA GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 11
Assumption 5 may be difficult to check. In Theorem 2 below, we will show, however, that it is satisfied by
all divergence functions of Table 1. We can now prove that robustification improves the condition number.
Proposition 6 (Improved condition number). If Assumptions 1–5 hold and Σ(cid:98) ∈Sp , then κ(X⋆)≤κ(Σ(cid:98)).
++
The proof of Proposition 6 exploits a generalized monotonicity property of the eigenvalue map s(γ,b).
Lemma 1 (Generalized monotonicity property of the eigenvalue map s). If Assumptions 2, 4 and 5 hold,
then we have s(γ,b )/s(γ,b )≤b /b for all γ >0 and b ,b ∈R with b ≥b .
2 1 2 1 1 2 ++ 2 1
Recall from Theorem 1 that x⋆ = s(γ⋆,xˆ ) for all i = 1,...,p and that γ⋆ > 0. Therefore, Proposition 6
i i
follows immediately from Lemma 1.
3.3.4. Statistical Guarantees
We finally show that the distributionally robust estimator is consistent and enjoys a finite-sample per-
formance guarantee. To this end, we make the dependence on n explicit, that is, we let X⋆ be the unique
n
solutionof (4), wherethenominalestimatorisanycovarianceestimatorΣ(cid:98)n constructedfromni.i.d.training
samples, and where the radius is set to a non-negative number ε that may depend on n ∈ N. We say a
n
covariance estimator is strongly consistent if it converges almost surely to Σ as n tends to infinity.
0
Proposition 7 (Consistency). Suppose that Assumptions 1–4 hold and that d is continuous on R ×R .
+ ++
If Σ(cid:98)n is a strongly consistent estimator and ε
n
converges to 0 as n grows, then X n⋆ is strongly consistent.
Proposition 7 is intuitive because the uncertainty set is assumed to shrink with n, and the nominal covari-
ancematrixatitscenterisassumedtobeconsistent. Astheuncertaintysetisdefinedasagenericdivergence
ball,however,theproofisperhapssurprisinglytedious. Thestandardexampleofaconsistentnominalcovari-
ance estimator Σ(cid:98)n is the sample covariance matrix. Next, we establish finite-sample performance guarantees,
thatis,weshowthattheuncertaintysetofradiusε
n
∝n− 21 aroundthesamplecovariancematrixconstitutes
a confidence region for Σ . In the following we say that the probability distribution P is sub-Gaussian if
0
there exists σ2 ≥0 with E P[exp(z⊤ξ)]≤exp(1 2σ2∥z∥2 2) for every z ∈Rp. As both sides of this inequality are
differentiable and coincide at z =0, one can show that any sub-Gaussian distribution P must have mean 0.
Proposition 8 (Finite-sample performance guarantee). Suppose that P is sub-Gaussian with covariance
matrix Σ
0
∈Sp ++, and let Σ(cid:98)n be the sample covariance matrix corresponding to n i.i.d. samples from P. For
any divergence function D from Table 1 there exist n min(η)=O(logη−1) and ε min(n,η)=O(n− 21(logη−1)1 2)
that may depend on P such that Pn[Σ
0
∈B ε(Σ(cid:98)n)]≥1−η for every n≥n min(η) and ε≥ε min(n,η).
Proposition8impliesthatifn≥n (η)andε≥ε (n,η),thentheoptimalvalueoftherobustcovariance
min min
estimationproblem(4)providesa(1−η)-upperconfidenceboundontheactualestimationerrorwithrespect
tothetruecovariancematrixΣ . Weemphasizethatthedependenceofn (η)andε (n,η)onthesample
0 min min
size n and significance level η is sometimes substantially better than the worst-case dependence reported in
Proposition 8. Explicit formulas for n (η) and ε (n,η) tailored to different divergence functions can be
min min
found in the proof of Proposition 8 in the appendix.
4. A Zoo of New Covariance Shrinkage Estimators
Inthissection,wefirstshowthattheassumptionsofTheorem1aresatisfiedbyabroadspectrumofdiver-
gence functions commonly used in statistics, information theory, and machine learning. Next, we explicitly
construct the shrinkage estimators corresponding to three popular divergence functions.
Theorem 2 (Validation of assumptions). All divergences in Table 1 satisfy Assumptions 1, 2, 4 and 5.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 12
We emphasize that the uncertainty sets corresponding to the Fisher-Rao and inverse Stein divergences
fail to be convex, in which case one cannot use standard minimax results to prove Assumption 1. However,
perhaps surprisingly, in Appendix C.2, we show that the uncertainty sets corresponding to these divergences
are geodesically convex with respect to a particular Riemannian geometry on the space of positive definite
matrices. Moreover, we prove a Riemannian minimax theorem, which requires geodesic convexity instead of
ordinary convexity and, therefore, significantly generalizes the classic Euclidean minimax results; see Theo-
rem 3 in Appendix C.3. This new theorem enables us to prove the desired minimax property even for robust
estimation problems based on the Fisher-Rao and inverse Stein divergences.
To showcase the richness of our framework, we now focus on three popular divergence functions and
analyze the corresponding robust covariance estimators. Specifically, we will derive the optimal solutions of
problem (P ) in quasi-closed form for the Kullback-Leibler, Wasserstein, and Fisher-Rao divergences. In
Vec
doing so, we develop a general recipe for the other divergence functions listed in Table 1.
4.1. The Kullback-Leibler Covariance Shrinkage Estimator
Table 1 defines the Kullback-Leibler (KL) divergence between two matrices Σ ,Σ ∈Sp as
1 2 ++
D (Σ ,Σ )= 1(cid:0) Tr[Σ−1Σ ]−p+logdet(Σ Σ−1)(cid:1) .
KL 1 2 2 2 1 2 1
This KL divergence between matrices is intimately related to the KL divergence between distributions.
Definition 1 (KL divergence). If P and P are two probability distributions on Rp, and P is absolutely
1 2 1
continuous with respect to P 2, then the KL divergence from P
1
to P
2
is KL(P 1∥P 2)=(cid:82) Rplog( dd PP 21(x))dP 2(x).
The following lemma shows that the KL divergence between two non-degenerate zero-mean Gaussian
distributions coincides with the KL divergence between their positive definite covariance matrices.
Lemma 2 (KL divergence between Gaussian distributions [28]). The KL divergence from P =N(0,Σ ) to
1 1
P =N(0,Σ ) with Σ ,Σ ∈Sp is given by KL(P ∥P )=D (Σ ,Σ ).
2 2 1 2 ++ 1 2 KL 1 2
Lemma 2 justifies our terminology of referring to D as the KL divergence and suggests that D
KL KL
inherits many properties of the KL divergence between distributions. For example, it is easy to verify
that D satisfies the identity of indiscernibles but fails to be symmetric. Indeed, for any Σ ∈ Sp we
KL ++
have D (Σ,2Σ) = p(1−log(2)) ≈ 0.15p, whereas D (2Σ,Σ) = p(log(2)− 1) ≈ 0.1p. An elementary
KL 2 KL 2 2
calculation further reveals that the generator d corresponding to the KL divergence D can be expressed as
KL
1(cid:16)a (cid:16)a(cid:17)(cid:17)
d(a,b)= −1−log .
2 b b
The following corollary of Theorem 1 characterizes the eigenvalue map and the inverse shrinkage intensity
corresponding to the KL divergence, which determines the KL covariance shrinkage estimator.
Corollary 1 (KL covariance shrinkage estimator). If D is the KL divergence, Σ(cid:98) ∈ Sp and ε > 0, then
++
problem (4) is uniquely solved by the KL covariance shrinkage estimator X⋆ = V“Diag(x⋆)V“⊤ with shrunk
eigenvalues x⋆ =s(γ⋆,xˆ ), i=1,...,p. The underlying eigenvalue map is given by
i i
(cid:112)
−γ+ γ2+16b2γ
s(γ,b)= , (9a)
8b
and the inverse shrinkage intensity γ⋆ ∈(0,γ ] is the unique positive solution of the nonlinear equation
KL
2ε+p+(cid:88)p ï −s(γ⋆,xˆ i) +logs(γ⋆,xˆ i)ò
=0, (9b)
xˆ xˆ
i i
i=1
where
4xˆ2exp(−4ε/p)
p
γ = >0.
KL 1−exp(−2ε/p)A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 13
4.2. The Wasserstein Covariance Shrinkage Estimator
Table 1 defines the Wasserstein divergence between two matrices Σ ,Σ ∈Sp as
1 2 +
(cid:0) (cid:1)1
D (Σ ,Σ )=Tr[Σ +Σ −2 Σ Σ 2].
W 1 2 1 2 1 2
Inthefollowing,wewillshowthattheWassersteindistancebetweenmatricesiscloselyrelatedtothesquared
2-Wasserstein distance between distributions, where the transportation cost is defined via the 2-norm.
Definition 2 (Wasserstein distance). The 2-Wasserstein distance between two probability distributions P
1
and P on Rp with finite second moments is defined as
2
Å (cid:90) ã1
2
W (P ,P )= inf ∥x −x ∥2dπ(x ,x ) ,
2 1 2 1 2 2 1 2
π∈Π(P 1,P 2) Rp×Rp
where Π(P ,P ) denotes the set of probability distributions on Rp×Rp with marginals P and P , respectively.
1 2 1 2
OnecanshowthatWassersteindistanceW isametriconthespaceofprobabilitydistributionswithfinite
2
second moments [65, § 6]. However, the squared Wasserstein distance W2 is only a divergence as it fails to
2
satisfy the triangle inequality. The following lemma shows that the squared 2-Wasserstein distance between
twozero-meanGaussiandistributionsmatchestheWassersteindivergencebetweentheircovariancematrices.
Lemma 3 (Squared Wasserstein distance between Gaussian distributions [18]). The squared 2-Wasserstein
distance between P =N(0,Σ ) and P =N(0,Σ ) evaluates to W (P ,P )2 =D (Σ ,Σ ).
1 1 2 2 2 1 2 W 1 2
Lemma 3 justifies our terminology of referring to D as the Wasserstein divergence and suggests that
W
D inherits many properties from the Wasserstein distance between distributions. Note that D remains
W W
well-definedevenifΣ orΣ arerank-deficient. ThegeneratordoftheWassersteindivergenceD isgivenby
1 2 W
√
d(a,b)=a+b−2 ab.
The following corollary of Theorem 1 characterizes the eigenvalue map and inverse shrinkage intensity corre-
sponding to the Wasserstein divergence, which determines the Wasserstein covariance shrinkage estimator.
Corollary 2 (Wasserstein covariance shrinkage estimator). If D is the Wasserstein divergence, Σ(cid:98) ∈ Sp
+
and ε ∈ (0,Tr[Σ(cid:98)]), then problem (4) is uniquely solved by the Wasserstein covariance shrinkage estimator
X⋆ =V“Diag(x⋆)V“⊤ with eigenvalues x⋆
i
=s(γ⋆,xˆ i), i=1,...,p. The underlying eigenvalue map is given by
Ñ é2
®
γ
Ç√ …
2
å´1
3 γ
®
γ
Ç√ …
2
å´−1
3
s(γ,b)= b+ b+ γ − b+ b+ γ (10a)
4 27 6 4 27
and the inverse shrinkage intensity γ⋆ ∈(0,γ ] is the unique positive solution of the nonlinear equation
W
p
(cid:88)(cid:16)(cid:112) » (cid:17)2
ε− xˆ − s(γ⋆,xˆ ) =0, (10b)
i i
i=1
»
where γ =2 pxˆ3/ε>0.
W p
The requirement that ε be strictly smaller than Tr[Σ(cid:98)] is equivalent to Assumption 3(b). It is needed
to prevent problem (P ) from admitting the trivial solution x⋆ = 0. To see this, note that the condition
Vec
ε≥Tr[Σ(cid:98)]isequivalentto(cid:80)p
i=1d(0,xˆ i)≤ε,whichinturnimpliesthat0isfeasibleandevenoptimalin(P Vec).
In this case, the trivial (and essentially nonsensical) estimator X⋆ =0 would be optimal in problem (4).A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 14
4.3. The Fisher-Rao Covariance Shrinkage Estimator
Table 1 defines the Fisher-Rao divergence between two matrices Σ ,Σ ∈Sp as
1 2 ++
D (Σ ,Σ )=∥log(Σ− 21 Σ Σ−1 2)∥2.
FR 1 2 2 1 2 F
TheFisher-RaodivergencecanbeinterpretedastheFisher-Raodistanceonaparticularstatisticalmanifold.
Definition 3 (Fisher-Rao distance). Consider a family of probability density functions {f (ξ)} whose
θ θ∈Θ
parameter θ ranges over a Riemannian manifold Θ with metric I =(cid:82) f (ξ)∇ log(f (ξ))∇ log(f (ξ))⊤dξ.
θ Ξ θ θ θ θ θ
The geodesic distance FR(θ ,θ ) on Θ induced by this metric is referred to as the Fisher-Rao distance.
1 2
Note that I represents the Fisher information matrix corresponding to the parameter θ. Next, we show
θ
that the squared Fisher-Rao distance between two non-degenerate zero-mean Gaussian probability density
functions is proportional to the Fisher-Rao divergence between their positive definite covariance matrices.
Lemma 4 (Fisher-Rao distance between positive definite covariance matrices [3]). Let {f (ξ)} be the
θ θ∈Θ
family of all non-degenerate zero-mean Gaussian probability density functions encoded by their covariance
matrices θ =Σ, which range over the Riemannian manifold Θ=Sp equipped with the Fisher-Rao distance.
++
If θ =Σ and θ =Σ belong to Sp , then FR(θ ,θ )2 = 1D (Σ ,Σ ).
1 1 2 2 ++ 1 2 2 FR 1 2
Lemma 4 justifies our terminology of referring to D as the Fisher-Rao divergence. As D is propor-
FR FR
tional to the squared Fisher-Rao distance FR2, it fails to satisfy the triangle inequality and is indeed only
a divergence. Moreover, Example 1 in Appendix C.1 reveals that D is neither convex nor quasi-convex.
FR
However, it is geodesically convex. The generator d corresponding to D can be expressed as
FR
d(a,b)=(log(a/b))2.
The following corollary of Theorem 1 characterizes the eigenvalue map and inverse shrinkage intensity corre-
sponding to the Fisher-Rao divergence, which characterizes the Fisher-Rao covariance estimator.
Corollary 3 (Fisher-Rao covariance shrinkage estimator). If D is the Fisher-Rao divergence, Σ(cid:98) ∈ Sp
++
and ε > 0, then problem (4) is uniquely solved by the Fisher-Rao covariance shrinkage estimator X⋆ =
V“Diag(x⋆)V“⊤ with eigenvalues x⋆
i
=s(γ⋆,xˆ i), i=1,...,p. The underlying eigenvalue map is given by
Å ã
s(γ,b)=bexp
−1
W
(cid:0) 2b2/γ(cid:1)
, (11a)
0
2
and W denotes the principal branch of the Lambert-W function. In addition, the inverse shrinkage intensity
0 √
γ⋆ ∈(0,γ FR] with γ
FR
=∥Σ(cid:98)∥2 F/ ε>0 is the unique positive solution of the nonlinear equation
p
(cid:88) W2(cid:0) 2xˆ2/γ(cid:1)
=4ε. (11b)
0 i
i=1
4.4. Other Covariance Shrinkage Estimators
Theorem 2 ensures that all divergence functions from Table 1 satisfy Assumptions 1, 2, 4 and 5 and thus
induceviaTheorem1adistributionallyrobustcovarianceshrinkageestimator. Thegeneratorsandeigenvalue
maps corresponding to all these divergences can be derived by using similar techniques as in Corollaries 1, 2,
and 3. Details are omitted for brevity. All generators and eigenvalue maps are provided in Table 2.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 15
Divergence d(a,b) dom(d) s(γ,b)forb>0
√
Kullback-Leibler/Stein 1 2(cid:0)a b −1−loga b(cid:1) R ++×R ++ −γ+ γ 82 b+16b2γ
Wasserstein a+b−2√ ab R +×R + Ç (cid:16) γ 4(cid:16)√ b+» b+ 22 7γ(cid:17)(cid:17)1 3 − γ 6(cid:16) γ 4(cid:16)√ b+» b+ 22 7γ(cid:17)(cid:17)−1 3å2
Fisher-Rao (loga b)2 R ++×R ++ bexp(−1 2W0(2b2/γ))
InverseStein 1 2Ä ab −1−log abä R ++×R ++ 31/3(cid:0) 6√ (cid:0)√3γ 32 γ(2 27 (2b 72 b+ 2γ +) γ+ )9 +γ 9b γ(cid:1) b2 (cid:1)/ 13 /− 332/3γ
S Jey ffm rm eye str di iz ve ed rgS et ne cin e/ 1 2Ä ab + a b −2ä R ++×R ++
11 2(cid:16) +b(cid:0)
(cid:0)2 21 16 6γ γb4 b4+ +1 12
2√
√3 3(1
(10γ 082
8(γ (γ2 2b8 b8− −3 3(γ (γb) b4 )4)− )−γ γ3
3(cid:1)
(cid:1)
−
γ(cid:17)
b b
Quadratic/ (a−b)2 R +×R
+
γ+b
b
SquaredFrobenius
Weightedquadratic (a− bb)2 R +×R ++ γγ +b b
Table 2. Generators and eigenvalue maps of the divergences from Table 1.
5. Numerical Experiments
We now compare our distributionally robust covariance estimators against the linear shrinkage estimator
with shrinkage target n1 Tr[Σ(cid:98)]I
n
[32] as well as a state-of-the-art nonlinear shrinkage estimator proposed by
LedoitandWolf[35],henceforthreferredtoastheNLLW estimator. Theperformanceofthelinearshrinkage
estimator depends on the choice of the mixing parameter α∈[0,1], which we calibrate via cross-validation.
We first study the dependence of our estimators on the radius ε of the uncertainty set. Using synthetic
data, we assess their Frobenius risk as a function of the sample size. Using real data, we further test the
performance of minimum variance portfolios constructed from our estimators. In addition, we illustrate the
use of covariance estimators in the context of linear and quadratic discriminant analysis. The code for all
experiments as well as an implementation of our methods can be found on GitHub.2
5.1. Dependence on the Radius of the Uncertainty Set
WefirststudythedecayoftheeigenvaluesandtheconditionnumberoftheKullback-Leibler,Wasserstein,
and Fisher-Rao covariance shrinkage estimators with the radius ε of the uncertainty set. To this end, we
set p = 3 and consider a nominal covariance matrix with eigenvalue spectrum λ(Σ(cid:98)) = [1,2,3]. Figure 2
visualizestheeigenvaluesofX⋆asafunctionofε. InagreementwithProposition5,weobservethatX⋆shrinks
the eigenvalues of the underlying nominal estimator Σ(cid:98) towards 0 as ε grows. Recall from Assumption 3(b)
and the subsequent discussion that X⋆ =0 whenever ε≥(cid:80)p d(0,xˆ ). As the generator of the Wasserstein
i=1 i
divergence satisfies d(0,b)=b, the eigenvalues of the Wasserstein covariance shrinkage estimator thus vanish
for any ε ≥ Tr[Σ(cid:98)]. In contrast, the eigenvalues of the Kullback-Leibler and Fisher-Rao covariance shrinkage
estimators remain strictly positive for all ε. We further observe that, for small values of ε, the Wasserstein
and Fisher-Rao covariance shrinkage estimators primarily shrink the large eigenvalues of Σ(cid:98) and keep the
small ones constant. Figure 3 visualizes the condition number κ(X⋆) as a function of ε. As predicted by
Proposition 6, κ(X⋆) is at most as large as κ(Σ(cid:98)). Note also that κ(X⋆) is undefined for ε ≥ (cid:80)p i=1d(0,xˆ i).
Figure 3 indicates that the condition number of X⋆ decreases monotonically as ε tends to (cid:80)p d(0,xˆ ).
i=1 i
2https://github.com/yvesrychener/covariance_DROA GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 16
 ( L J H Q Y D O X H  3 D W K  & R Q G L W L R ( Q L J  1 H X Q P Y D E O X H H U   3 3 D D W W K K  & R Q G L W L R ( Q L J  1 H X Q P Y D E O X H H U   3 3 D D W W K K  & R Q G L W L R Q  1 X P E H U  3 D W K
              
    x1         x1         x1
    
    x2     x2     x2
x3      x3      x3     
                        
                        
   
            
   
    
              
           
              
                        
                                                                                                        
(a) Kullback-Leibler (b) Wasserstein (c) Fisher-Rao
Figure 2. Eigenvalues of three different distributionally robust covariance estimators as a
function of the radius ε for λ(Σ(cid:98))=[1,2,3].
 ( L J H Q Y D O X H  3 D W K  & R Q G L W L R ( Q L J  1 H X Q P Y D E O X H H U   3 3 D D W W K K  & R Q G L W L R ( Q L J  1 H X Q P Y D E O X H H U   3 3 D D W W K K  & R Q G L W L R Q  1 X P E H U  3 D W K
              
    x1         x1         x1
    
    x2     x2     x2
x3      x3      x3     
                        
                        
   
            
   
    
              
           
              
                        
                                                                                                        
(a) Kullback-Leibler (b) Wasserstein (c) Fisher-Rao
Figure 3. Conditionnumberofthreedifferentdistributionallyrobustcovarianceestimators
as a function of the radius ε for λ(Σ(cid:98))=[1,2,3].
5.2. Frobenius Error
In the first experiment, we use synthetic data to analyze the Frobenius risk of different covariance esti-
mators. Specifically, we construct a diagonal covariance matrix Σ ∈ S100 with 90 eigenvalues equal to 1
0 ++
and 10 ‘spiking’ eigenvalues equal to M ∈ {10,100,500}. Thus, we have κ(Σ 0) = M. Next, we let Σ(cid:98) be
the sample covariance matrix constructed from n∈{100,200,500} independent samples from P=N(0,Σ ).
0
This experimental setup captures the small to medium sample size regime with n≳p, in which we expect Σ(cid:98)
to provide a poor approximation for Σ 0. We thus compare Σ(cid:98) against the Kullback-Leibler, Wasserstein, and
Fisher-Rao covariance shrinkage estimators as well as against the linear shrinkage estimator with shrinkage
target n1 Tr[Σ(cid:98)]I
n
andagainsttheNLLWestimator. Figure4visualizestheFrobeniuslossofallestimatorsasa
functionoftheunderlyinghyperparameters,thatis,theradiusεoftheuncertaintysetforthedistributionally
robust estimators and the mixing weight α for the linear shrinkage estimator. The NNLW estimator and the
sample covariance matrix involve no hyperparameters and are thus visualized as horizontal lines. Figure 4
shows both the means (solid lines) as well as the areas within one standard deviation of the means (shaded
areas) of the Frobenius loss based on 10 independent training sets for all possible combinations of M and n.
As ε tends to 0, all distributionally robust estimators approach the sample covariance matrix. Thus, they
(cid:80)p
overfit the data and display a high variance. As ε tends to d(0,xˆ ), on the other hand, all distribution-
i=1 i
allyrobustestimatorscollapseto0andthusdisplayahighbias. Wethusfaceaclassicbias-variancetrade-off.
Figure 4 reveals that the Frobenius loss of the distributionally robust estimators is minimal at intermediate
values of ε. We observe that the linear shrinkage estimator is competitive with the distributionally robust
estimators for well-conditioned covariance matrices (small M, top row). As the covariance matrix becomes
more ill-conditioned (large M, middle and bottom rows), the linear shrinkage estimator is dominated by the
 H X O D Y Q H J L (
 U H E P X 1  Q R L W L G Q R &
 H X O D Y Q H J L (
 H X O D Y Q H J L (
 U H E P X 1  Q R L W L G Q R &
 U H E P X 1  Q R L W L G Q R &
 H X O D Y Q H J L (
 H X O D Y Q H J L (
 U H E P X 1  Q R L W L G Q R &
 U H E P X 1  Q R L W L G Q R &
 H X O D Y Q H J L (
 U H E P X 1  Q R L W L G Q R &A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 17
   
   
   
                                                           
  R U    R U    R U 
(a) n=100,M =10 (b) n=200,M =10 (c) n=500,M =10
           
   
   
       
   
                                                           
  R U    R U    R U 
(d) n=100,M =100 (e) n=200,M =100 (f) n=500,M =100
       
   
   
   
   
   
                                                           
  R U    R U    R U 
(g) n=100,M =500 (h) n=200,M =500 (i) n=500,M =500
Figure 4. Frobenius loss of the Kullback-Leibler (blue), Wasserstein (orange), and Fisher-
Rao (green) covariance shrinkage estimators and of the linear shrinkage estimator (red) as a
function of the underlying hyperparameter (radius ε or mixing weight α) for different spike
sizes M and sample sizes n. The sample covariance matrix (gray) and the NLLW estimator
(purple) involve no hyperparameters; thus, their Frobenius error is constant.
distributionally robust estimators, which attain a significantly smaller Frobenius loss. The advantage of the
distributionallyrobustestimatorsrelativetothenominalsamplecovariancematrixdiminisheswithincreasing
sample size n. The NLLW estimator is designed to be asymptotically optimal and, therefore, dominates the
other estimators for large sample sizes. However, it is suboptimal if training samples are scarce.
The insights of this synthetic experiment can be summarized as follows. Linear shrinkage estimators
are suitable for well-conditioned covariance matrices and small sample sizes, while the NLLW estimator is
preferableforlargesamplesizes, irrespectiveoftheconditionnumber. Thedistributionallyrobustestimators
perform better when the covariance matrix is ill-conditioned and training samples are scarce.
5.3. Minimum Variance Portfolio Selection
We consider the problem of constructing the minimum variance portfolio of p risky assets by solving the
convex program min w∈Rp{w⊤Σ 0w : w⊤1 = 1} [22], where 1 denotes the p-dimensional vector of ones, and
Σ ∈ Sp stands for the covariance matrix of the asset returns over the investment horizon. The unique
0 ++
optimal solution of this problem is given by w⋆ = Σ−11/1⊤Σ−11. In practice, however, the distribution
0 0
of the asset returns is unknown, and thus the covariance matrix Σ needs to be estimated from historical
0
data. If the chosen covariance estimator Σ(cid:98) is invertible, then it is natural to use w (cid:98)⋆ = Σ(cid:98)−11/1⊤Σ(cid:98)−11 as
 V V R O  V X L Q H E R U )
 V V R O  V X L Q H E R U )
 V V R O  V X L Q H E R U )
 V V R O  V X L Q H E R U )
 V V R O  V X L Q H E R U )
 V V R O  V X L Q H E R U )
 V V R O  V X L Q H E R U )
 V V R O  V X L Q H E R U )
 V V R O  V X L Q H E R U )A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 18
0.25 2.E0m0pirical 16Empirical Empirical
0.20
1.L N7i5 Ln Le WarShrinkage 14L Ni Ln Le WarShrinkage L Ni Ln Le WarShrinkage
1.W50asserstein 12Wasserstein Wasserstein
0.15 1.K F2i5u shll eb ra -c Rk aL oeibler 10K Fiu shll eb ra -c Rk aL oeibler K Fiu shll eb ra -c Rk aL oeibler
0.10 1.00 8
0.75
6
0.05 0.50
4
2 4 6 8 10 12 2 4 6 8 10 12 2 4 6 8 10 12
k[months] k[months] k[months]
(a) Sharperatio (b) Meanreturn (c) Standarddeviationofreturn
Figure 5. Sharpe ratios, means, and standard deviations induced by different covariance
estimators on the “48 Industry Portfolios” depending on the length k of an updating period.
an estimator for the minimum variance portfolio. This approach seems reasonable, provided that the asset
return distribution is stationary over the (past) estimation window and the (future) investment horizon.
Inthenextexperiment,weassesstheminimumvarianceportfoliosinducedbyseveralcovarianceestimators
onthe“48IndustryPortfolios”datasetfromtheFama-Frenchonlinelibrary,3whichcontainsmonthlyreturns
of 48 portfolios grouped by industry. Specifically, we adopt the following rolling horizon procedure from
January 1974 to December 2022. First, we estimate Σ from the historical asset returns within a rolling
0
estimation window of 50 months and construct the corresponding minimum variance portfolio. We then
compute the returns of this portfolio over the k months immediately after the estimation window. Finally,
thecovarianceestimatorsarerecalibratedbasedonanewestimationwindowshiftedaheadbyk months,and
the procedure starts afresh. Some covariance estimators involve a hyperparameter, which we calibrate via
leave-one-out cross-validation on the 50 return samples in each estimation window. To this end, we assume
that the mixing weight α of the linear shrinkage estimator with shrinkage target n1 Tr[Σ(cid:98)]I
n
ranges from 10−5
to 1, whereas the radius ε of the uncertainty set ranges from 10−5 to 102 for the Kullback-Leibler and
Fisher-Rao covariance shrinkage estimators and from 10−10 to 108 for the Wasserstein covariance shrinkage
estimator. Wediscretizetheseparameterrangesinto50logarithmicallyspacedcandidatevaluesandselectthe
onethatinducesthesmallestportfoliovariance. Giventheselectedhyperparameter,thecovarianceestimator
corresponding to the current estimation window is computed using all 50 data points. In the following, we
measurethequalityofagivencovarianceestimatorbySharperatioandthemeanandthestandarddeviation
of the portfolio returns generated by the above rolling horizon procedure over the backtesting period.
Figure 5 displays the Sharpe ratios, means, and standard deviations corresponding to different covariance
estimatorsasafunctionofthelengthkofanupdatingperiod. Allshrinkageestimatorsproducelowerstandard
deviations and higher Sharpe ratios than the sample covariance matrix. Even though the mean portfolio
returns of the sample covariance matrix are—on average—similar to those of the shrinkage estimators, they
change rapidly with k, which is troubling for investors who need to select k before seeing the results of
the backtest. The distributionally robust estimators proposed in this paper outperform the other shrinkage
estimators in terms of mean returns and Sharpe ratios for most choices of k, and the Wasserstein covariance
shrinkage estimator results in the globally highest Sharpe ratio. However, the Kullback-Leibler and Fisher-
Rao covariance shrinkage estimators result in slightly higher means and standard deviations.
5.4. Linear and Quadratic Discriminant Analysis
Quadratic discriminant analysis (QDA) seeks to predict a label y ∈ {0,1} from a feature vector z ∈ Rp
under the assumption that z|y ∼ N(µ ,Σ ) for every y ∈ {0,1}. If the mean µ , the covariance matrix Σ
y y y y
as well as the marginal class probability p are known for all y ∈ {0,1}, then the Bayes-optimal classifier
y
predictsyasasolutionofmin (z−µ )Σ−1(z−µ )+logdet(Σ )−2log(p ). Lineardiscriminantanalysis
y∈{0,1} y y y y y
3https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html
oitareprahS
]%[naem
]%[noitaiveddradnatsA GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 19
Table 3. Mean (standard error) of the LDA and QDA accuracy based on 100 independent
permutations of the underlying dataset
Dataset Empirical Linear NLLW Wasserstein Kullback-Leibler Fisher-Rao
Banknote 0.9751(0.0005) 0.9754(0.0005) 0.9510(0.0011) 0.9761(0.0005) 0.9763(0.0005) 0.9759(0.0005)
LDA
Cancer 0.9520(0.0011) 0.9365(0.0015) 0.8902(0.0015) 0.9520(0.0011) 0.8874(0.0043) 0.9515(0.0013)
Banknote 0.9854(0.0005) 0.9839(0.0005) 0.9877(0.0004) 0.9854(0.0005) 0.9853(0.0005) 0.9854(0.0005)
QDA
Cancer 0.9418(0.0012) 0.8945(0.0027) 0.6320(0.0052) 0.9418(0.0012) 0.9451(0.0013) 0.9414(0.0016)
(LDA) operates under the additional assumption that Σ = Σ . The decision boundaries of the resulting
0 1
LDA and QDA classifiers are thus given by linear hyperplanes and quadratic hypersurfaces, respectively [20].
In the last experiment, we use LDA and QDA to address the breast cancer detection [68] and banknote
authentication [39] problems from the UCI Machine Learning Repository. As the distribution governing y
and z is unobservable, we replace the unknown class probabilities p and class means µ by the empirical
y y
frequencies and sample average estimators, respectively, and we use different shrinkage estimators for the
unknown covariance matrices Σ . All tested shrinkage estimators use the debiased empirical covariance
y
matrix as the nominal estimator. QDA constructs a separate covariance estimator for each class y that only
uses class-y samples, whereas LDA pools all samples to construct a single joint covariance estimator.
We use 50% of each dataset for training and the rest for testing. The hyperparameters ε (for the distribu-
tionally robust shrinkage estimators) and α (for the linear shrinkage estimator) are selected by the holdout
method with a validation set comprising 20% of the training data. The quality of a covariance estimator is
then measured by the accuracy (i.e., the proportion of correct predictions) of the resulting LDA and QDA
classifiers. Table 3 reports the means and standard errors of the accuracy achieved by different covariance
estimators. We observe that shrinking the empirical covariance estimator can improve the performance of
LDA and QDA, and that nonlinear shrinkage methods outperform the linear shrinkage method across all
experiments. The Kullback-Leibler covariance shrinkage estimator consistently performs well. QDA based
on the NLLW estimator attains the highest accuracy on the banknote authentication dataset but performs
poorly on the breast cancer dataset. On the other hand, the distributionally robust covariance estimators
are consistently on par with or better than the empirical and the linear shrinkage estimator. Note that the
best-performing distributionally robust shrinkage estimator changes with the dataset. This highlights the
usefulness of our approach, which results in a zoo of complementary covariance shrinkage estimators.
Acknowledgments. This research was supported by the Hong Kong Research Grants Council under
the GRF project 15304422, by the Swiss National Science Foundation under the NCCR Automation, grant
agreement 51NF40 180545, and by CUHK through the ‘Improvement on Competitiveness in Hiring New
Faculties Funding Scheme’ and the CUHK Direct Grant with project number 4055191.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 20
Appendix
Theappendixisorganizedasfollows. InAppendixA,weproveTheorem1andderivebasicpropertiesofγ⋆
andx⋆,whichwillbeusedinAppendixBtoestablishthecomputational,structuralandstatisticalproperties
i
ofthedistributionallyrobustestimators. AppendicesCandDverifyAssumptions1and2foralldivergences
in Table 1, respectively. As a byproduct, we derive a Riemannian generalization of Sion’s minimax theorem.
The insights of Appendices C and D are used in Appendix E to prove the results of Section 4.
Appendix A. Proof of Theorem 1
A.1. Proof of Proposition 2
We first reduce problem (P ), which optimizes over matrices, to an equivalent problem of the form
Mat
inf
∥x∥2
x∈Rp 2
+
p
s.t. (cid:88) d(x i,xˆ i)≤ε (P↑ Vec)
i=1
x ≤···≤x ,
1 p
whichmerelyoptimizesovervectors. Here, xˆ∈Rp denotesasusualthevectorwhosei-thentryxˆ isthei-th
+ i
smallest eigenvalue of the nominal covariance matrix Σ(cid:98). We therefore have xˆ↑ = xˆ. To simplify the subse-
quent discussions, for any minimization problem designated by “P,” say, we use “Min(P),” “Argmin(P)” and
“Fea(P)”todenoteitsminimum/infimum,thesetofitsoptimalsolutionsanditsfeasibleregion,respectively.
The following proposition illuminates the relationship between problems (P ) and (P↑ ).
Mat Vec
Proposition 9 (Reduction of (P ) to (P↑ )). If D is a spectral divergence in the sense of Assumption 2,
Mat Vec
then the following assertions hold.
(i) Problem (P ) is feasible if and only if problem (P↑ ) is feasible.
Mat Vec
(ii) Fea(P ) is compact if and only if Fea(P↑ ) is compact.
Mat Vec
(iii) For any x⋆ ∈Argmin(P↑ Vec), the matrix V“Diag(x⋆)V“⊤ is an optimal solution to problem (P Mat).
(iv) For any Σ⋆ ∈Argmin(P ), the vector λ(Σ⋆) is an optimal solution to problem (P↑ ).
Mat Vec
(v) Min(P )=Min(P↑ ).
Mat Vec
Proof of Proposition 9. Select any Σ ∈ Fea(P ), and use Σ = V Diag(λ(Σ))V⊤ to denote its eigenvalue
Mat Σ Σ
decomposition. By our notational conventions, we have 0≤λ (Σ)≤···≤λ (Σ). We then obtain
1 p
p
(cid:88) Ä ä
d(λ i(Σ),xˆ i)=D(Diag(λ(Σ)),Diag(xˆ))≤D V“⊤V ΣDiag(λ(Σ))V Σ⊤V“,Diag(xˆ)
(12)
i=1
Ä ä
=D V ΣDiag(λ(Σ))V Σ⊤,V“Diag(xˆ)V“⊤ =D(Σ,Σ(cid:98))≤ε,
where the first equality follows from Assumption 2(b), the first inequality follows from Assumption 2(c), and
the second equality follows from Assumption 2(a). This implies that λ(Σ)∈Fea(P↑ ).
Vec
Next, select any x∈Fea(P↑ ) such that V“Diag(x)V“⊤ ∈Sp. By Assumptions 2(a) and (b), we thus have
Vec +
p
(cid:88)
D(V“Diag(x)V“⊤,Σ(cid:98))=D(V“Diag(x)V“⊤,V“Diag(xˆ)V“⊤)=D(Diag(x),Diag(xˆ))= d(x i,xˆ i)≤ε, (13)
i=1
where the three equalities follow from the eigenvalue decomposition of Σ(cid:98), Assumption 2(a) and Assump-
tion 2(b), respectively. This implies that V“Diag(x)V“⊤ ∈Fea(P Mat). In summary, we have thus shown that
problem (P ) is feasible if and only if problem (P↑ ) is feasible. This establishes assertion (i).
Mat Vec
The first part of the proof of assertion (i) actually implies that λ(Fea(P )) ⊆ Fea(P↑ ). Conversely,
Mat Vec
the second part of the proof of assertion (i) implies that Fea(P↑ )⊆λ(Fea(P )). To see this, recall that
Vec MatA GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 21
any x feasible in (P↑ Vec) satisfies x=λ(V“Diag(x)V“⊤). As V“Diag(x)V“⊤ is feasible in (P Mat), we may indeed
conclude that x∈λ(Fea(P )). As the eigenvalue map λ is continuous [4, Corollary VI.1.6], λ(Fea(P ))
Mat Mat
is compact if and only if Fea(P↑ ) is compact. This observation establishes assertion (ii).
Vec
As for assertion (iii), assume that Argmin(P↑ ) ̸= ∅ for otherwise the claim is trivial. Choose then any
Vec
x⋆ ∈Argmin(P↑ Vec),andnotethatV“Diag(x⋆)V“⊤ ∈Fea(P Mat)byvirtueof (13). Itremainstobeshownthat
V“Diag(x⋆)V“⊤ ∈Argmin(P Mat). Suppose, for the sake of contradiction, that there is Σ′ ∈Fea(P Mat) with
(cid:13) (cid:13)2
∥Σ′∥2 <(cid:13)V“Diag(x⋆)V“⊤(cid:13) ,
F (cid:13) (cid:13)
F
and let Σ′ =V′Diag(λ(Σ′))V′⊤ be the eigenvalue decomposition of Σ′ for some V′ ∈O(p). By (12), we then
have λ(Σ′)∈Fea(P↑ ), which contradicts the optimality of x⋆ in problem (P↑ ) because
Vec Vec
(cid:13) (cid:13)2
∥λ(Σ′)∥2 =∥Σ′∥2 <(cid:13)V“Diag(x⋆)V“⊤(cid:13) =∥x⋆∥2 .
2 F (cid:13) (cid:13) 2
F
Therefore, V“Diag(x⋆)V“⊤ ∈Argmin(P Mat). This proves assertion (iii).
As for assertion (iv), assume that Argmin(P ) ̸= ∅ for otherwise the claim is trivial. Choose then any
Mat
Σ⋆ ∈ Argmin(P ), and note that λ(Σ⋆) ∈ Fea(P↑ ) by virtue of (12). It remains to be shown that
Mat Vec
λ(Σ⋆)∈Argmin(P↑ ). Suppose, for the sake of contradiction, that there is x′ ∈Fea(P↑ ) with
Vec Vec
∥x′∥2 <∥λ(Σ⋆)∥2
.
2 2
By (13), we then have V“Diag(x′)V“⊤ ∈Fea(P Mat), which contradicts the optimality of Σ⋆ in (P Mat) because
(cid:13) (cid:13)2
(cid:13)V“Diag(x′)V“⊤(cid:13) =∥x′∥2 <∥λ(Σ⋆)∥2 =∥Σ⋆∥2 .
(cid:13) (cid:13) 2 2 F
F
Therefore, λ(Σ⋆)∈Argmin(P↑ ). This proves assertion (iv).
Vec
Finally, in order to prove assertion (v), we need to show that any Σ∈Fea(P ) corresponds to some x∈
Mat
Fea(P↑ ) with the same objective function value and vice versa. However, this follows in a straightforward
Vec
manner from the proof of assertion (i). Details are omitted for brevity. □
Proposition9(iii)andthediscussionafterAssumption1implythatifx⋆ ∈Argmin(P↑ ),thenthematrix
Vec
V“Diag(x⋆)V“⊤isoptimalin(4). Wecanthuscomputerobustcovarianceestimatorsbysolvingproblem(P↑ ).
Vec
Although problem (P↑ ) optimizes over a significantly lower-dimensional search space than (P ), it
Vec Mat
still involves p−1 ordering constraints x ≤ ··· ≤ x . This is undesirable because each of these ordering
1 p
constraints necessitates a separate dual variable. We now show that problem (P↑ ) is in fact equivalent to
Vec
problem (P ), which suppresses all ordering constraints, and which is repeated below for convenience.
Vec
inf
∥x∥2
x∈Rp 2
+
p (P )
(cid:88) Vec
s.t. d(x ,xˆ )≤ε.
i i
i=1
More precisely, we will show that relaxing the ordering constraints preserves the optimal value and even the
minimizers of problem (P↑ )—up to simple rearrangements. Our proof will critically rely on Assumption 2
Vec
and on the following lemma, which shows that if x feasible is in (P ), then x↑ is feasible in (P↑ ).
Vec Vec
Lemma 5. If Assumption 2 holds, then we have
p p
(cid:88) (cid:88)
d(x↑,y↑)≤ d(x ,y↑) ∀x,y ∈Rp.
i i i i +
i=1 i=1
If the right-hand side is finite, then the above inequality collapses to an equality if and only if x↑ =x.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 22
Proof of Lemma 5. Let P ∈O(p) be a permutation matrix with Diag(x)=P Diag(x↑)P⊤. We then have
x x x
p
(cid:88) d(x↑,y↑)=D(cid:0) Diag(x↑),Diag(y↑)(cid:1)
i i
i=1 p
≤D(cid:0) P Diag(x↑)P⊤,Diag(y↑)(cid:1) =D(cid:0) Diag(x),Diag(y↑)(cid:1) =(cid:88) d(x ,y↑),
x x i i
i=1
where the first and last equalities follow from Assumption 2(b), the inequality follows from Assumption 2(c),
and the second equality from the definition of P . This proves the first claim.
x
Assumenowthattheright-handsideoftheaboveexpressionisfinite. Hence,D(P Diag(x↑)P⊤,Diag(y↑))
x x
isalsofinite. Assumption2(c)thenimpliesthattheequalityholdsifandonlyifDiag(x↑)=P Diag(x↑)P⊤ =
x x
Diag(x), that is, if and only if x↑ =x. This proves the second claim. □
We are now ready to prove the equivalence of (P↑ ) and (P ).
Vec Vec
Proposition10(Equivalenceof (P↑ )and(P )). IfAssumption2holds, thenproblems (P↑ )and (P )
Vec Vec Vec Vec
share the same optimal value as well as the same set of minimizers.
Proof. WeonlyshowthatArgmin(P↑ )=Argmin(P ). Thisreadilyimpliesthattheoptimalvaluesmatch.
Vec Vec
To prove the inclusion Argmin(P↑ ) ⊆ Argmin(P ), select any x⋆ ∈ Argmin(P↑ ). Hence, x⋆ ∈
Vec Vec Vec
Fea(P ). Suppose now for the sake of argument that there exists x′ ∈ Fea(P ) with ∥x′∥2 < ∥x⋆∥2 . By
Vec Vec 2 2
Lemma 5, which applies because xˆ=xˆ↑, we may thus conclude that x′↑ ∈Fea(P↑ ). However, this contra-
Vec
dictstheoptimalityofx⋆ inproblem(P↑ )because∥x′↑∥2 =∥x′∥2 <∥x⋆∥2 . Therefore, x⋆ ∈Argmin(P ).
Vec 2 2 2 Vec
To prove the reverse inclusion Argmin(P↑ ) ⊇ Argmin(P ), select any x⋆ ∈ Argmin(P ). We claim
Vec Vec Vec
that x⋆↑ = x⋆. Suppose for the sake of contradiction that x⋆↑ ̸= x⋆. This readily guarantees that not all
components of x⋆ are equal. Since x⋆ ∈Fea(P ) and xˆ=xˆ↑, Lemma 5 then implies that
Vec
p p
(cid:88) d((x⋆↑ ) ,xˆ )<(cid:88) d(x⋆,xˆ )≤ε,
i i i i
i=1 i=1
that is, x⋆↑ satisfies the constraint in (P ) strictly. In the following we use e to denote the p-th standard
Vec p
basisvectorinRp. Asx⋆ ̸=x⋆↑ ∈Rp isnotconstant,thereisj ∈{1,...,p}withx⋆ >0. Thisreadilyimplies
+ j
that(x⋆↑) ≥x⋆ >0. AsdiscontinuousbyvirtueofAssumption2(b)andasx⋆↑ strictlysatisfiestheexplicit
p j
constraint in (P ), the perturbed vector x⋆↑−ϵe remains feasible in (P ) for all sufficiently small ϵ>0.
Vec p Vec
In addition, ∥x⋆↑−ϵe ∥ < ∥x⋆↑∥ = ∥x⋆∥ for all sufficiently small ϵ > 0. This contradicts the optimality
p 2 2 2
of x⋆. Hence, we mayconclude that x⋆ =x⋆↑. Therefore, x⋆ is feasible in(P↑ ). Since(P )is arelaxation
Vec Vec
of problem (P↑ ) (with the same objective function), we may thus conclude that x⋆ ∈Argmin(P↑ ). □
Vec Vec
With these results in place, the proof of Proposition 2 is now immediate.
Proof of Proposition 2. The proof is an immediate consequence of Propositions 9 and 10. □
A.2. Proof of Proposition 3
The next lemma shows that any solution of problem (P ) shrinks xˆ towards the origin. This will imply
Vec
that our proposed distributionally robust estimators constitute shrinkage estimators.
Lemma 6 (Eigenvalue shrinkage). If Assumptions 2 and 3(a) hold and x⋆ solves problem (P ), then we
Vec
have x⋆ ∈dom(d(·,xˆ )) and x⋆ ≤xˆ for all i=1,...,p.
i i i iA GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 23
Proof of Lemma 6. Selectanyx⋆ ∈Argmin(P ). Asx⋆ ∈Fea(P ), itisclearthatx⋆ ∈dom(d(·,xˆ ))for
Vec Vec i i
all i=1,...,p. Next, suppose that x⋆ >xˆ for some j =1,...,p, and define x˜∈Rp through
j j +
(cid:40)
xˆ if i=j,
j
x˜ =
i
x⋆ if i̸=j.
i
Recall now that if Assumption 2(b) holds, then d constitutes a spectral divergence on R . Assumption 3(a)
+
further implies that (xˆ ,xˆ ) ∈ dom(d). Hence, d(xˆ ,xˆ ) = 0 < d(x⋆,xˆ ), which ensures that x˜ ∈ Fea(P ).
j j j j j j Vec
However, from the construction of x˜ it is evident that ∥x˜∥2 < ∥x⋆∥2 , which contradicts the optimality of x⋆
2 2
in (P ). Thus, we have x⋆ ≤xˆ for all i=1,...,p. This observation completes the proof. □
Vec i i
Lemma 6 allows us to prove the existence and uniqueness of the proposed robust covariance estimators.
Proposition 11 (Existence and uniqueness of optimal solutions). If Assumptions 2, 3 and 4 hold, then
problems (P ), (P↑ ) and (P ) admit a unique optimal solution. In addition, if Assumptions 1, 2, 3
Vec Vec Mat
and 4 hold, then there exists a unique distributionally robust estimator that solves problem (3).
Proof of Proposition 11. Suppose first that only Assumptions 2, 3 and 4 hold. Lemma 6 then implies that
problem (P ) has the same set of optimal solutions as the following variant of (P ) with box constraints
Vec Vec
inf
∥x∥2
x∈Rp 2
p
s.t. (cid:88) d(x i,xˆ i)≤ε (P′ Vec)
i=1
0≤x ≤xˆ ∀i=1,...,p.
i i
Notethatproblem(P′ )isfeasibleduetoAssumption3(a),whichpositsthatd(xˆ ,xˆ )=0foralli=1,...,p.
Vec i i
Next, we show that the feasible region of (P′ ) is compact. To this end, note that d(x ,xˆ ) is continuous
Vec i i
in x on the interval [0,xˆ ] for every i = 1,...,p. Indeed, continuity trivially holds if xˆ = 0, in which
i i i
case [0,xˆ ] collapses to a point. Otherwise, if xˆ > 0, then continuity follows from Assumption 2(b). This
i i
readily implies that the feasible region of (P′ ) is closed and—thanks to the box constraints—also compact.
Vec
The solvability of problem (P′ ) thus follows from Weierstrass’ maximum theorem, which applies because
Vec
the objective function is continuous. Assumption 4 further implies that d(x ,xˆ ) is convex in x on [0,xˆ ] for
i i i i
all i = 1,...,p, which implies that the feasible region of (P′ ) is convex. The uniqueness of the optimal
Vec
solution x⋆ thus follows from the strong convexity of the objective function. This shows that problem (P )
Vec
has a unique optimal solution. The other claims immediately follow from Propositions 1, 9 and 10. □
From now on we use d (·) as a notational shorthand for the function d(·,b) for any fixed b≥0.
b
Proposition 12 (Solutionofproblem(P )). If Assumptions 2, 3 and 4 hold, then the unique minimizer x⋆
Vec
of problem (P ) has the following properties. If xˆ =0, then x⋆ =0, and if xˆ >0, then x⋆ ∈(0,xˆ ) and
Vec i i i i i
0=2x⋆+γ⋆d′ (x⋆), (14)
i xˆi i
where γ⋆ is a solution of the nonlinear equation (cid:80)p d(s(γ⋆,xˆ ),xˆ )−ε=0.
i=1 i i
Thefollowinglemmashowsthatd isstrictlydecreasingon[0,b],whichwillbeusedtoproveProposition12.
b
Lemma 7 (Derivative of d ). If Assumptions 2 and 4 hold, then we have
b
d(a,b) d(a,b)
d′(a)≤− <− <0 ∀a∈(0,b), ∀b>0.
b b−a b
Proof of Lemma 7. Selectanyb>0. Asd(·,b)isfiniteandconvexon[0,b]thankstoAssumption4,wehave
0=d(b,b)≥d(a,b)+(b−a)d′(a) ∀a∈(0,b).
b
The desired inequality then follows from an elementary rearrangement. □A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 24
Proof of Proposition 12. Lemma 6 allows us to rewrite problem (P ) equivalently as
Vec
min
∥x∥2
2
x∈C
(cid:88)p (P′ V′ ec)
s.t. d(x ,xˆ )≤ε,
i i
i=1
where C = C ×···×C with C = [0,xˆ ]∩dom(d ) for each i = 1,...,p. Note that the objective and the
1 p i i xˆi
constraintfunctionadoptfinitevaluesonC. ByProposition11,problem(P′′ )hasauniqueminimizerx⋆,and
Vec
byLemma6wehavex⋆ =0foralliwithxˆ =0. Forsuchindicesi,d(0,0)=d(xˆ ,xˆ )=0byAssumption3(a).
i i i i
By removing the corresponding decision variables from (P′′ ) and focusing on the optimization problem in
Vec
the remaining variables, we can therefore assume without loss of generality that xˆ > 0 for all i = 1,...,p.
i
Hence, problem (P′′ ) can be viewed as an ordinary convex program in the sense of [52, Section 28].
Vec
Following [52, Section 28], we define the Lagrangian L:R×Rp →R of problem (P′′ ) through
Vec

∥x∥2 2+γ((cid:80)p i=1d(x i,xˆ i)−ε) if x∈C,γ ≥0,
L(γ,x)= −∞ if x∈C,γ <0,
+∞
if x̸∈C.
By [52, Corollary 28.2.1 and Theorem 28.3], problem (P′′ ) is thus equivalent to the minimax problem
Vec
minsup L(γ,x)=maxmin L(γ,x).
x∈Rpγ∈R γ∈R x∈Rp
Specifically, the dual maximization problem on the right-hand side is solvable, and every maximizer γ⋆ ≥ 0
gives rise to a saddle point (γ⋆,x⋆) of the minimax problem. Next, we prove that γ⋆ > 0. Suppose for the
sake of contradiction that γ⋆ = 0. Note first that x⋆ ∈ C for otherwise problem (P′′ ) would be infeasible,
Vec
thuscontradictingthefeasibilityofxˆ. Hence,wefindL(γ⋆,x⋆)=L(0,x⋆)=∥x⋆∥2. Ifx⋆ >0forsomei,then
2 i
0<∥x⋆∥2 =L(0,x⋆)≤L(0,x)=∥x∥2 ∀x∈C,
2 2
wherethesecondinequalityholdsbecause(0,x⋆)isasaddlepoint. However,thediscussionafterAssumption4
implies that dom(d ) either equals R or R for every i=1,...,p. Hence, we have (cid:81)p (0,xˆ ]⊆C, that
xˆi + ++ i=1 i
is, C contains points that are arbitrarily close to 0. This leads to the contradiction
0= inf ∥x∥2 ≥∥x⋆∥2 >0.
2 2
x∈C
We may thus conclude that if γ⋆ = 0, then x⋆ = 0 for all i, that is, x⋆ = 0. However, this contradicts
i
Assumption 3(b), which implies that 0̸∈Fea(P ). In summary, this shows that γ⋆ >0.
Vec
Next, we note that for any dual optimal solution γ⋆ >0, the minimization problem
(cid:32) p (cid:33)
(cid:88)
minL(γ⋆,x)=min∥x∥2+γ⋆ d(x ,xˆ )−ε (15)
x∈Rp x∈C 2 i i
i=1
admits a unique optimal solution, and by [52, Corollary 28.1.1] this minimizer must coincide with the unique
optimal solution x⋆ of problem (P′′ ). Given γ⋆, we can thus solve (15) instead of (P′′ ). This is attractive
Vec Vec
from a computational point of view because C is rectangular, whereby problem (15) can be simplified to
p p
−εγ⋆+(cid:88) min (cid:8) x2+γ⋆d(x ,xˆ )(cid:9) =−εγ⋆+(cid:88) min (cid:8) x2+γ⋆d(x ,xˆ )(cid:9) .
i i i i i i
i=1xi∈Ci i=1xi∈[0,xˆi]
Therefore, it suffices to solve the following simple univariate minimization problem for each i=1,...,p.
min x2+γ⋆d(x ,xˆ ) (16)
i i i
xi∈[0,xˆi]
If xˆ =0, then (0,0)∈dom(d) by Assumption 3(a), and hence d(0,0)=d(xˆ ,xˆ )=0. In this case, x⋆ =0 is
i i i i
the only feasible—and thus unique optimal—solution of (16). Assume next that xˆ >0. In this case we need
i
to prove that x⋆ falls within the open interval (0,xˆ ) and satisfies (14). We will first show that x⋆ >0. From
i i i
the discussion after Assumption 4 we know that d x⋆
i
can evaluate to +∞ only at 0. If d xˆi(0)=+∞, then weA GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 25
triviallyhavex⋆ >0. Assumenextthatd (0)<+∞. ByAssumption2(b),d iscontinuousandd (0)>0.
i xˆi xˆi xˆi
There exists a threshold δ > 0 such that d (a) ≥ δ for all sufficiently small a ∈ [0,xˆ ]. In addition, as the
xˆi i
function a2+γ⋆d(a,xˆ ) is convex and differentiable in a by virtue of Assumption 4, we have
i
02+γ⋆d(0,xˆ )≥a2+γ⋆d(a,xˆ )+(2a+γ⋆d′ (a))(0−a)
i i xˆi
aγ⋆d(a,xˆ )
>a2+γ⋆d(a,xˆ )−2a2+ i
i
xˆ
i
aγ⋆δ
≥a2+γ⋆d(a,xˆ )−2a2+
i
xˆ
i
for all sufficiently small a ≥ 0. Here, the second inequality follows from Lemma 7, and the third inequality
holds because d (a)≥δ for all sufficiently small a≥0. This reasoning implies that
xˆi
aγ⋆δ
γ⋆d(0,xˆ )>a2+γ⋆d(a,xˆ )−2a2+ >a2+γ⋆d(a,xˆ ) (17)
i i i
xˆ
i
for all sufficiently small a≥0. Thus, small a>0 are strictly preferable to 0, that is, x⋆ >0.
i
Next,weprovethatx⋆ <xˆ . Asthedifferentiablefunctiond (a)isnon-negativeandattainsitsminimum0
i i b
at a=b, we may conclude that its derivative d′(a) converges to 0 as a tends to b. For any a<b sufficiently
b
close to b we thus have (b−a)(2a+γ⋆d′(a))>0. As a2+γ⋆d(a,b) is convex in a on [0,b], this ensures that
b
b2+γ⋆d(b,b)≥a2+γ⋆d(a,b)+(b−a)(2a+γ⋆d′(a))>a2+γ⋆d(a,b).
b
Hence, any a<b sufficiently close to b is strictly preferable to b. Setting b=xˆ , we thus find x⋆ <xˆ .
i i i
Finally, note that since x⋆ ∈ (0,xˆ ), the constraints of problem (16) are not binding at optimality. Thus,
i i
the minimizer of (16) is uniquely determined by the problem’s first-order optimality condition (14).
It remains to be shown that γ⋆ is unique. As 0 ̸∈ Fea(P ) thanks to Assumption 3(b), there exists at
Vec
leastonei=1,...,pwithx⋆ >0, andhencexˆ >0. Sinced isdifferentiableonR , equation(14)implies
i i xˆi ++
2x⋆
γ⋆ =− i .
d′ (x⋆)
xˆi i
Hence, γ⋆ is unique because x⋆ is unique. Note also that γ⋆ is the Lagrange multiplier associated with the
i
constraint (cid:80)p d(x ,xˆ )≤ε in problem (P′′ ). As strong duality holds and γ⋆ >0, we have
i=1 i i Vec
p
(cid:88)
d(x⋆,xˆ )−ε=0
i i
i=1
by complementary slackness. Using the definition (8) of the eigenvalue map s, we then obtain
p
(cid:88)
d(s(γ⋆,xˆ ),xˆ )−ε=0.
i i
i=1
This observation completes the proof. □
A.2.1. Properties of s and γ⋆
We first provide a detailed analysis of the nonlinear equation that defines the eigenvalue map s.
Lemma 8 (Properties of s). If Assumptions 2 and 4 hold, then the the following hold.
(i) If γ >0 and b>0, then the equation 0=2a+γd′(a) admits a unique solution in (0,b). Hence, the
b
eigenvalue map s(γ,b) is well-defined on R2.
+
(ii) If b>0, then s (γ)=s(γ,b) is continuous and strictly increasing on R and differentiable on R .
b + ++
(iii) If b>0, then lim s (γ)=0 and lim s (γ)=b.
γ↓0 b γ→∞ b
Recallthat,forandfixedγ >0,thefunctions (b)shrinkstheinputbinthesensethats (b)≤b. Lemma8
γ γ
further shows that, for any fixed b > 0, s (γ) strictly increases from 0 to b as γ grows. Therefore, we can
b
interpret γ as an inverse shrinkage intensity.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 26
Proof of Lemma 8. Assertion (i) follows directly from the proof of Proposition 12 and is thus not repeated.
Next,weproveassertion(ii). RecallfromAssumption4thatd istwicecontinuouslydifferentiableonR .
b ++
Thus, the function H(γ,a) = 2a + γd′(a) is continuously differentiable on R2 . Assumption 4 further
b ++
stipulates that d is convex on [0,b]. Hence, H(γ,a) is strictly increasing in a in the sense that
b
∂H(γ,a)
=2+γd′′(a)≥2>0 ∀a∈(0,b].
∂a b
As s (γ) ∈ (0,b) by assertion (i), the implicit function theorem ensures that s (γ) is differentiable (and in
b b
particular continuous) at any γ > 0. It remains to be shown that s (γ) is continuous at 0. Given any ϵ > 0
b
and as s (0)=0 by definition, we thus need to show that there is δ >0 such that s (γ)≤ϵ for all γ ∈[0,δ].
b b
As s (γ) ∈ (0,b) for all γ,b > 0, we may assume without loss of generality that ϵ ∈ (0,b). By Lemma 7, we
b
have d′(ϵ)<0, which guarantees that δ =−2ϵ/d′(ϵ) is positive. For any γ ∈[0,δ], we thus obtain
b b
γd′(s (γ)) ϵd′(s (γ))
s (γ)=− b b ≤ b b ,
b 2 d′(ϵ)
b
where the equality follows from the definition of s in (8), and the inequality follows from the definition
b
of δ. This confirms that s (γ) ≤ ε. Suppose to the contrary that s (γ) > ϵ. Then the above inequality
b b
implies d′(s (γ)) < d′(ϵ). As d′ is non-decreasing by virtue of the convexity of d , this in turn leads to the
b b b b b
contradiction s (γ)>ε. Thus, s (γ)≤ε for all γ ∈[0,δ]. We conclude that s (γ) is indeed continuous at 0.
b b b
To show that s (γ) is strictly increasing on R , recall that s (γ) is differentiable on R . We may thus
b ++ b ++
differentiate both sides of the equation 0=2s (γ)+γd′(s (γ)) with respect to γ to obtain
b b b
0=2s′(γ)+d′(s (γ))+γd′′(s (γ))s′(γ).
b b b b b b
Rearranging terms then yields
d′(s (γ))
s′(γ)=− b b , (18)
b 2+γd′′(s (γ))
b b
whichisstrictlypositivebecaused′(s (γ))<0thankstoLemma7andd′′(s (γ))≥0thankstotheconvexity
b b b b
of d on [0,b]. Hence, s (γ) is strictly increasing on R . This completes the proof of assertion (ii).
b b +
It remains to prove assertion (iii). The continuity of s (γ) at γ = 0 has already been established in
b
assertion (ii). As s (γ) ∈ (0,b) is strictly increasing in γ, it is clear that, as γ tends to infinity, s (γ) has a
b b
well-defined limit not larger than b. By the definition of s in (8), we further have
b
2s (γ)
b +d′(s (γ))=0 ∀γ >0.
γ b b
Driving γ to infinity and recalling that s (γ)∈(0,b) for all γ >0 thus shows that
b
Å ã
0= lim d′(s (γ))=d′ lim s (γ) ,
b b b b
γ→∞ γ→∞
where the second equality follows from the continuity of d′ on R . Note that lim s (γ) exists and falls
b ++ γ→∞ b
within the interval (0,b] because s is a strictly increasing function mapping R to (0,b). These arguments
b +
imply that the limit must be a root of d′ within (0,b]. Lemma 7 implies that d′ has no root in the open
b b
interval (0,b). We may thus conclude that lim s (γ) must coincide with b. As a sanity check, one readily
γ→∞ b
verifies that 0=d′(b) because d (a) attains its minimum of 0 at a=b. Thus, assertion (iii) follows. □
b b
We now prove that the function F(γ)
=(cid:80)p
d(s(γ,xˆ ),xˆ )−ε has one and only one root. By the proof
i=1 i i
of Proposition 12, this root must coincide with the unique optimal solution γ⋆ of the problem dual to (P ).
Vec
Lemma 9. If Assumptions 2, 3 and 4 hold, then the equation F(γ)=0 has a unique root, which is positive.
Proof of Lemma 9. Recall that s(γ,0) = 0 by the definition of s in (8). Recall also that if xˆ = 0, then
i
d(s(γ,xˆ ),xˆ ) = d(0,0) = 0 by virtue of Assumptions 2 and 3(a). Therefore, vanishing components of xˆ do
i i
not contribute to the function F(γ). In addition, Assumption 3(b) ensures that there exists at least one
i∈{1,...,p} with x⋆ >0 and hence also with xˆ >0. For these reasons, we henceforth assume without loss
i iA GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 27
of generality that xˆ > 0 for all i = 1,...,p. By Lemma 8(ii), s(γ,xˆ ) constitutes a continuous real-valued
i i
function of γ ∈ R . Similarly, by Assumption 2(b), d(x ,xˆ ) constitutes a continuous extended real-valued
+ i i
functionofx ∈R . Therefore,theextendedreal-valuedfunctionF(γ)iscontinuousonR . Assumption3(b)
i + +
implies that F(0) =
(cid:80)p
d(0,xˆ )−ε > 0. Recall now from Lemma 8(iii) that s(γ,xˆ ) converges to xˆ as γ
i=1 i i i
tends to infinity. By the continuity of d(x ,xˆ ) in x we thus have
i i i
p
(cid:88)
lim F(γ)= d(xˆ ,xˆ )−ε=−ε<0.
i i
γ→∞
i=1
All of this implies that the equation F(γ)=0 has at least one positive root. In the remainder we prove that
this root is unique. As xˆ >0, Lemma 8 implies that s(γ,xˆ ) strictly increases from 0 (at γ =0) to xˆ (as γ
i i i
tends to infinity). Lemma 7 further implies that d is strictly decreasing on [0,xˆ ]. Thus, the composite
xˆi i
functiond(s(γ,xˆ ),xˆ )isstrictlydecreasinginγ foreveryi. ThisreadilyshowsthatF(γ)isstrictlydecreasing
i i
in γ throughout R , thus implying that the equation F(γ)=0 has only one root. □
+
We are now ready to prove Proposition 3.
Proof of Proposition 3. TheproofisadirectconsequenceofPropositions11and12andLemmas8and9. □
Appendix B. Proofs of Section 3.3
Proof of Proposition 4. InviewoftheproofofLemma9,itonlyremainstobeshownthatF(γ)isdifferentiable
at any γ >0. Towards that end, recall that vanishing components of xˆ do not contribute to F(γ) such that
p p
(cid:88) (cid:88)
F(γ)= d(s(γ,xˆ ),xˆ )−ε= d(s(γ,xˆ ),xˆ )−ε.
i i i i
i=1 i=1:
xˆi>0
For any fixed xˆ > 0, s(γ,xˆ ) is differentiable with respect to γ ∈ R by Lemma 8(ii), and d(x,xˆ ) is
i i ++ i
differentiable with respect to x∈R by Assumption 4. Therefore, F(γ) is differentiable at any γ >0. □
++
FromtheproofofProposition12weknowthattheproblemdualto(P )hasauniqueoptimalsolutionγ⋆.
Vec
Thus, γ⋆ can be viewed as a function γ⋆(ε) of the radius ε>0 of the divergence ball (2).
Lemma 10 (Monotonicity of γ⋆). If Assumptions 2, 3 and 4 hold, then γ⋆(ε) is non-increasing on (0,ε¯).
Proof of Lemma 10. The proof of Proposition 12 implies that γ⋆(ε) is the unique maximizer of the problem
dual to (P ). By inverting its objective function, this problem can be recast as the minimization problem
Vec
min εγ+G(γ), (19)
γ>0
where the function G:R →R is defined through
++
p p
G(γ)=− (cid:88) min (cid:8) x2+γd(x ,xˆ )(cid:9) =− (cid:88) Ä (s (γ))2 +γd (s (γ))ä .
i=1:
xi∈[0,xˆi]
i i i
i=1:
xˆi xˆi xˆi
xˆi>0 xˆi>0
Notealsothatthenon-negativityconstraintonγ in(19)isstrictbecauseγ =0cannotbeoptimal,or,dually,
because the constraint in (P ) must be binding at optimality for ε<ε¯. By construction, G(γ) constitutes
Vec
a pointwise maximum of multiple linear functions and is, therefore, convex. Next, select ε ,ε ∈ (0,ε¯] with
1 2
0 < ε < ε , and introduce the notational shorthands γ = γ⋆(ε ) and γ = γ⋆(ε ). By the optimality of γ
1 2 1 1 2 2 1
andγ inproblem(19)atε andε ,thereexistsubgradientsg ∈∂G(γ )andg ∈∂G(γ )satisfyingthefirst-
2 1 2 1 1 2 2
order optimality conditions ε +g =0 and ε +g =0, respectively. Since G(γ) is convex, its subdifferential
1 1 2 2
ismonotone, whereby(γ −γ )(g −g )≥0. Togetherwiththefirst-orderoptimalityconditions, thisimplies
2 1 2 1
that (γ −γ )(ε −ε )≥0. As ε <ε , we may thus conclude that γ ≤γ . Hence, the claim follows. □
2 1 1 2 1 2 2 1A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 28
Proof of Proposition 5. Note that x⋆(ε)=s(γ⋆(ε),xˆ ) for every ε∈(0,ε¯) thanks to Proposition 3, and recall
i i
that x⋆(ε¯)=0 by definition. We aim to show that x⋆(ε) is non-increasing on [0,ε¯] and that lim x⋆(ε)=0.
i i ε↑ε¯ i
Tothisend,notefirstthatbothclaimsaretriviallysatisfiedifxˆ =0,inwhichcasex⋆(ε)=0forallε∈(0,ε¯)
i i
thankstoProposition3andourconventionsthatx⋆(0)=xˆ andx⋆(ε¯)=0. Assumenextthatxˆ >0. Recall
i i i i
that γ⋆(ε) is non-increasing on (0,ε¯) thanks to Lemma 10, while s (x ) = s(x ,xˆ ) is strictly increasing
xˆi i i i
onR thankstoLemma8(ii),whichappliesbecausexˆ >0. Therefore,x⋆(ε)=s(γ⋆(ε),xˆ )isnon-increasing
+ i i i
on (0,ε¯). We also have x⋆(ε)∈(0,xˆ ) for all ε∈(0,ε¯) thanks to Proposition 3, and we have x⋆(0)=xˆ and
i i i i
x⋆(ε¯)=0bydefinition. Allofthisreadilyimpliesthatx⋆(ε)isnon-increasingon[0,ε¯]. Inordertoprovethat
i i
lim x⋆(ε)=0,notefirstthatlim x⋆(ε)mustexistbecausex⋆(ε)isnon-negativeaswellasnon-increasing
ε↑ε¯ i ε↑ε¯ i i
in ε. Next, recall from Lemma 7 that the function d (x )=d(x ,xˆ ) is strictly decreasing on (0,xˆ ). In fact,
xˆi i i i i
this monotonicity property extends to [0,xˆ ] because d is continuous thanks to Assumption 2(b). We then
i xˆi
choose an arbitrary tolerance δ >0 and assume without loss of generality that δ is smaller than the smallest
non-vanishing component of xˆ. Next, consider a vector x ∈ Rp defined through x = 0 if xˆ = 0 and x = δ
+ i i i
(cid:80)p
if xˆ >0, i=1,...,p, and set ε= d(x ,xˆ ). By construction, we have
i i=1 i i
p p
(cid:88) (cid:88)
ε= d(x ,xˆ )< d(0,xˆ )=ε¯,
i i i
i=1 i=1
wherethestrictinequalityholdsbecausexˆhasatleastonestrictlypositivecomponentandbecaused(x ,xˆ )<
i i
d(0,xˆ ) whenever xˆ > 0 thanks to the monotonicity properties of d established above. Hence, x is feasible
i i
inP ,andεisconsistentwithAssumption3(b). Inaddition,onereadilyverifiesthattheobjectivefunction
Vec
value of x satisfies ∥x∥2 ≤pδ2. By the optimality of x⋆(ε) in P , we thus find
2 Vec
x⋆(ε)2 ≤∥x⋆(ε)∥2 ≤pδ2 ∀i=1,...,p.
i 2
√
Thus,foranysufficientlysmallδ >0thereexistsε>0withx⋆(ε)≤ pδ. Asx⋆(ε)isnon-increasingon[0,ε¯],
i i
this implies indeed that lim x⋆(ε)=0. It remains to be shown that X⋆ constitutes a shrinkage estimator.
ε↑ε¯ i
This is now evident, however, because Σ(cid:98) =V“Diag(xˆ)V“⊤ =V“Diag(x⋆(0))V“⊤. □
Proof of Lemma 1. Throughout this proof we fix any γ >0. We first aim to show that the function
1∂d(s(γ,b),b)
K(b)=
b ∂a
is non-decreasing on R . To this end, note that d(a,b) is twice continuously differentiable on R2 by
++ ++
Assumption 5. Using the implicit function theorem as in Lemma 8, one can thus show that s(γ,b) is differ-
entiable with respect to b and that s(γ,b)∈(0,b) for every b>0. Recall also that −2s(γ,b)= ∂d(s(γ,b),b)
γ ∂a
by the definition of s in (8). Differentiating both sides of this equation with respect to b then yields
2∂s(γ,b) d Å ∂d(s(γ,b),b)ã ∂2d(s(γ,b),b) ∂2d(s(γ,b),b)∂s(γ,b)
− = = + . (20)
γ ∂b db ∂a ∂a∂b ∂a2 ∂b
This in turn implies that
∂s(γ,b) Å 2 ∂2d(s(γ,b),b)ã−1 ∂2d(s(γ,b),b)
=− + , (21)
∂b γ ∂a2 ∂a∂b
which is well-defined because γ >0 and d(·,b) is convex by Assumption 4. We then find
Å ã
dK(b) 1 ∂d(s(γ,b),b) 1 d ∂d(s(γ,b),b)
=− + .
db b2 ∂a bdb ∂a
The second term on the right hand side of the above expression satisfies
1 d Å ∂d(s(γ,b),b)ã 2 ∂s(γ,b) 2 Å 2 ∂2d(s(γ,b),b)ã−1 ∂2d(s(γ,b),b)
=− = +
bdb ∂a bγ ∂b bγ γ ∂a2 ∂a∂b
(cid:32) ∂2d(s(γ,b),b) (cid:33) (cid:32) ∂d(s(γ,b),b)∂2d(s(γ,b),b) (cid:33)
2 1
= ∂a∂b = ∂a ∂a∂b ,
b 2− 2s(γ,b) ∂2d(s(γ,b),b) b ∂d(s(γ,b),b) −s(γ,b)∂2d(s(γ,b),b)
∂d(s(γ,b),b) ∂a2 ∂a ∂a2
∂aA GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 29
wherethefirstandthesecondequalitiesfollowfrom(20)and(21),respectively,andthethirdequalityfollows
from the defining equation of s in (8). Combining the last two equations finally yields
dK(b) 1
∂d(s(γ,b),b)(cid:32) b∂2d(s(γ,b),b) (cid:33)
=− 1− ∂a∂b .
db b2 ∂a ∂d(s(γ,b),b) −s(γ,b)∂2d(s(γ,b),b)
∂a ∂a2
Recallnowthat ∂d(a,b) <0foreverya∈(0,b)thankstoLemma7andthats(γ,b)∈(0,b)thankstoLemma8.
∂a
This implies that the derivative of K(b) is non-negative if and only if
∂2d(s(γ,b),b) 1Å ∂d(s(γ,b),b) ∂2d(s(γ,b),b)ã
≥ −s(γ,b) . (22)
∂a∂b b ∂a ∂a2
Assumption 5 guarantees that (22) holds indeed for all b>0. Hence, K(b) is a non-decreasing function.
We now prove the desired inequality. By the defining equation of s in (8) we have
∂d(s(γ,b ),b ) ∂d(s(γ,b ),b )
−2γb s(γ,b )=b 2 2 ≥b 1 1 =−2γb s(γ,b )
1 2 1 2 2 1
∂a ∂a
foranyb ≥b >0, whereandinequalityfollowsfromthemonotonicityofK establishedabove. Thisimplies
2 1
that s(γ,b )/s(γ,b )≤b /b for all b ,b ∈R with b ≥b . Hence, the claim follows. □
2 1 2 1 1 2 ++ 2 1
Proof of Proposition 7. Throughout the proof we use the shorthands x⋆
i,n
=λ i(X n⋆) and x
(cid:98)i,n
=λ i(Σ(cid:98)n) for all
i = 1,...,p and n ∈ N. By the strong consistency assumption, Σ(cid:98)n converges almost surely to Σ 0. Fix now
temporarily a particular realization of the uncertainties, for which Σ(cid:98)n converges deterministically to Σ 0. In
this case, x converges to λ (Σ ) because the eigenvalue map λ is continuous [4, Corollary VI.1.6], and the
(cid:98)i,n i 0 i
sequence {x⋆ i,n} n∈N is bounded by Lemma 6. Thus, any convergent subsequence {x⋆ i,nk} k∈N satisfies
lim x⋆ ∈[0, lim xˆ ]=[0, lim xˆ ]=[0,λ (Σ )].
k→∞
i,nk
k→∞
i,nk
n→∞
i,n i 0
In addition, we have
p
(cid:88) Ä ä
d(x⋆ i,nk,x (cid:98)i,nk)≤ d(x⋆ j,nk,xˆ j,nk)=D X n⋆ k,Σ(cid:98)nk ≤ε
nk
∀k ∈N,
j=1
where the first equality holds because of Assumptions 2(a) and 2(b) and because X n⋆
k
and Σ(cid:98)nk share the
same eigenvectors. The second inequality follows from Proposition 1(ii), which ensures that X⋆ is feasible
nk
in problem (P ). As ε converges to 0 and as d is continuous on R ×R , the above implies that
Mat nk + ++
d( lim x⋆ ,λ (Σ ))=d( lim x⋆ , lim x )= lim d(x⋆ ,x )=0.
k→∞
i,nk i 0
k→∞
i,nk k→∞(cid:98)i,nk
k→∞
i,nk (cid:98)i,nk
Recall now from Assumption 2 that d satisfies the identity of indiscernibles. Thus we find lim x⋆ =
k→∞ i,nk
λ i(Σ 0). This shows that every convergent subsequence of the bounded sequence {x⋆ i,n} n∈N must have the
same limit λ (Σ ). By [1, Exercise 2.5.5], the eigenvalue x⋆ therefore converges to λ (Σ ). This reasoning
i 0 i,n i 0
applies to every uncertainty realization under which Σ(cid:98)n converges to Σ 0. As Σ(cid:98)n converges almost surely
to Σ , we have thus shown that x⋆ converges almost surely to λ (Σ ). This in turn implies that
0 i,n i 0
Ä ä
P[ lim ∥X n⋆−Σ 0∥
F
=0]≥P[ lim ∥X n⋆−Σ(cid:98)n∥ F+∥Σ(cid:98)n−Σ 0∥
F
=0]
n→∞ n→∞
Ä ä
=P[ lim ∥x⋆ n−xˆ n∥ 2+∥Σ(cid:98)n−Σ 0∥
F
=0]
n→∞
Ä ä
≥P[ lim ∥x⋆ n−λ(Σ 0)∥ 2+∥λ(Σ 0)−xˆ n∥ 2+∥Σ(cid:98)n−Σ 0∥
F
=0]=1,
n→∞
where both inequalities hold thanks to the triangle inequality, the first equality follows from Theorem 1,
which ensures that X n⋆ and Σ(cid:98)n share the same eigenvectors, and the second equality exploits the almost sure
convergence of x⋆
n
and xˆ
n
to λ(Σ 0) established above and the almost sure convergence of Σ(cid:98)n to Σ 0. This
shows that X⋆ converges almost surely to Σ and therefore completes the proof. □
n 0A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 30
From now on we use ∥X∥ to denote the nuclear norm of X ∈ Sp (i.e., the sum of all singular values
∗
of X), which is the norm dual to the operator norm ∥X∥ (i.e., the largest singular value of X). The proof of
Proposition 8 relies on the following well-known result from high-dimensional statistics.
Lemma 11 ([67,Theorem6.5]). Under the assumptions of Proposition 8, there exists a constant c that only
0
depends on the dimension p of the random vector ξ and satisfies
î ó η
Pn ∥Σ(cid:98)n−Σ 0∥≤ρ(n,η) ≥1−
2
for every n∈N and η ∈(0,1), where
Ç … å
logη−1 logη−1
ρ(n,η)=c σ2 + .
0
n n
Proof of Proposition 8. For any divergence function D from Table 1 we will prove that there exist a con-
stant c>0 and a function n (η)=O(logη−1) that may depend on P such that
min
î ó η
Pn D(Σ 0,Σ(cid:98)n)≤c∥Σ 0−Σ(cid:98)n∥ ≥1− (23)
2
foreveryn≥n (η)andη ∈(0,1). Indeed,assumingthatsuchaninequalityholds,Lemma11andtheunion
min
bound imply that Pn[D(Σ 0,Σ(cid:98)n)≤cρ(n,η)]≥1−η. The claim then follows by setting ε min(n,η)=cρ(n,η).
Stein, Inverse Stein and Symmetrized Stein Divergences: Note that the sum of the Stein and
inverse Stein divergences equals twice the symmetrized Stein divergence. Recall also that all divergences are
non-negative. Thus, iftheballofradiusεwithrespecttothesymmetrizedSteindivergencecontainsΣ with
0
probability at least 1−η, then the ball of radius 2ε with respect to the Stein or inverse Stein divergence con-
tainsΣ withprobabilityatleast1−η. ItthussufficestofocusonthesymmetrizedSteindivergence. Suppose
0
now that the smallest eigenvalue of Σ(cid:98)n is no smaller than half of the smallest eigenvalue of Σ 0. As Σ
0
≻ 0,
this implies in particular that Σ(cid:98)n is positive definite and that Σ(cid:98)− n1 exists. Rewriting the symmetrized Stein
divergence as 1 2Tr[(Σ− 01−Σ(cid:98)− n1)(Σ(cid:98)n−Σ 0)], we may then use the matrix H¨older’s inequality to obtain
Tr[(Σ− 01−Σ(cid:98)− n1)(Σ(cid:98)n−Σ 0)]≤∥Σ 0−Σ(cid:98)n∥∥Σ− 01−Σ(cid:98)− n1∥ ∗.
In the following we use x
i
= λ i(Σ 0) and xˆ
i,n
= λ i(Σ(cid:98)n) to denote i-th smallest population and sample
eigenvalues for i=1,...,p, respectively. By the definitions of the nuclear and operator norms, we then have
∥Σ− 01−Σ(cid:98)− n1∥
∗
≤p∥Σ− 01−Σ(cid:98)− n1∥
¶ ©
=pmax λ p(Σ− 01−Σ(cid:98)− n1),−λ 1(Σ− 01−Σ(cid:98)− n1)
¶ ©
≤pmax λ p(Σ− 01)−λ 1(Σ(cid:98)− n1),λ p(Σ(cid:98)− n1)−λ 1(Σ− 01)
ß ™
1 1 1 1
=pmax − , −
x xˆ xˆ x
1 p,n 1,n p
ß ™
1 1 2p
≤pmax , ≤ ,
x xˆ x
1 1,n 1
where the first equality holds because the singular values of a symmetric matrix coincide with the absolute
values of the eigenvalues of that matrix. The second inequality follows from a classic result by Weyl, which
assertsthatλ (A+B)≤λ (A)+λ (B)≤λ (A+B)foranyA,B ∈Sp,andthesecondequalityholdsbecause
1 1 p p
λ (A−1)=1/λ (A) for any i=1,...,p and A∈Sp . The third inquality exploits our assumption that
i p−i+1 ++
allpopulationandsampleeigenvaluesarestrictlypositive,andthelastinequalityfollowsfromtheassumption
that xˆ
1,n
≥x 1/2. We have thus shown that if xˆ
1,n
≥x 1/2, then D(Σ 0,Σ(cid:98)n)≤ xp 1∥Σ 0−Σ(cid:98)n∥. Hence, we find
(cid:104) p (cid:105) (cid:104) x (cid:105)
Pn D(Σ 0,Σ(cid:98)n)≤ ∥Σ 0−Σ(cid:98)n∥ ≥Pn xˆ
1,n
≥ 1 .
x 2
1
As xˆ
1,n
≥x 1−∥Σ 0−Σ(cid:98)n∥ by virtue of Weyl’s inequality and by Lemma 11, the last probability satisfies
(cid:104) x (cid:105) (cid:104) x (cid:105) (cid:104) (cid:105) η
Pn xˆ
1,n
≥ 1 ≥Pn ∥Σ 0−Σ(cid:98)n∥≤ 1 ≥Pn ∥Σ 0−Σ(cid:98)n∥≤ρ(n,η) ≥1− (24)
2 2 2A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 31
whenever x /2≥ρ(n,η). By the definition of ρ(n,η), a sufficient condition for this inequality to hold is
1
Ç å2
n≥n (η)= c2 0σ4 (cid:112) logη−1+ logη−1+ 2x 1 logη−1 .
min x2 c σ2
1 0
The above estimates imply that (23) holds for all n≥n (η) and η ∈(0,1) if we set c=p/x . In addition,
min 1
the minimal sample size and the minimal radius of the uncertainty set satisfy n (η)=O(logη−1) and
min
Ç … å
p logη−1 logη−1
ε min(n,η)=cρ(n,η)= c 0σ2 + =O(n− 21 (logη−1)1 2),
x n n
1
where the last equality holds because n ≥ O(logη−1). This establishes the claim for the Stein, the inverse
Stein and the symmetrized Stein divergences.
Wasserstein Divergence: From the proof of [44, Theorem 4] we know that if xˆ
1,n
≥ x 21, then
1 4
D(Σ 0,Σ(cid:98)n)≤
(xˆ +x
)2∥Σ 0−Σ(cid:98)n∥2 ≤ 9x2∥Σ 0−Σ(cid:98)n∥2.
1,n 1 1
We also know from (24) that Pn[xˆ
1,n
≥ x 21]≥1− η
2
for all n≥O(logη−1). Thus, we have
ï 4 ò (cid:104) x (cid:105) η
Pn D(Σ 0,Σ(cid:98)n)≤ 9x2∥Σ 0−Σ(cid:98)n∥2 ≥Pn xˆ
1,n
≥ 21 ≥1−
2
(25)
1
for all n≥O(logη−1). Lemma 11 further implies that
î ó î ó η
Pn ∥Σ 0−Σ(cid:98)n∥≤1 ≥Pn ∥Σ 0−Σ(cid:98)n∥≤ρ(n,η) ≥1− , (26)
2
whenever
Ç … å
logη−1 logη−1
1≥ρ(n,η)=c σ2 + .
0
n n
Asufficientconditionforthisinequalitytoholdisthatn≥O(logη−1). Combiningtheestimates(25)and(26)
and using the union bound implies that there is a function n (η) that grows at most as O(logη−1) with
min
ï ò
4
Pn D(Σ 0,Σ(cid:98)n)≤ 9x2∥Σ 0−Σ(cid:98)n∥ ≥1−η
1
for all n ≥ n (η). Thus, (23) holds for all n ≥ n (η) and η ∈ (0,1) if we set c = 4/(9x2). Similar calcu-
min min 1
lations as in the last part of the proof reveal that ε min(n,η) = cρ(n,η) grows at most as O(n− 21(logη−1)21).
This establishes the claim for the Wasserstein divergence.
√
Quadratic Divergence: Since ∥A∥ ≤ p∥A∥ for all A∈Sp, we have
F
D(Σ 0,Σ(cid:98)n)=∥Σ 0−Σ(cid:98)n∥2
F
≤p∥Σ 0−Σ(cid:98)n∥2.
From (26) we already know that Pn[∥Σ 0−Σ(cid:98)n∥≤1]≥1−η for all n≥O(logη−1). Thus, there is a function
n (η) = O(logη−1) such that (23) holds for all n ≥ n (η) and η ∈ (0,1) if we set c = p. As usual, one
min min
verifies that ε min(n,η)=cρ(n,η)=O(n−1 2(logη−1)21). This proves the claim for the quadratic divergence.
Weighted Quadratic Divergence: As Tr[AB]≤∥A∥∥B∥ ≤p∥A∥∥B∥ for all A,B ∈Sp, we have
∗
p 2p
D(Σ 0,Σ(cid:98)n)=Tr[(Σ 0−Σ(cid:98)n)2Σ(cid:98)− n1]≤p∥(Σ 0−Σ(cid:98)n)2∥∥Σ(cid:98)− n1∥≤
xˆ
∥Σ 0−Σ(cid:98)n∥2 ≤
x
∥Σ 0−Σ(cid:98)n∥2
1,n 1
whenever xˆ
1,n
≥ x 21. Recall also that Σ(cid:98)n is indeed invertible under this assumption. Together with (24)
and (26), the above inequality implies that there exists a function n (η)=O(logη−1) such that
min
ï ò
2p
Pn D(Σ 0,Σ(cid:98)n)≤ ∥Σ 0−Σ(cid:98)n∥ ≥1−η,
x
1
for all n ≥ n (η). Thus, (23) holds for all n ≥ n (η) and η ∈ (0,1) if we set c = 2p/x . As usual, one
min min 1
verifies that ε min(n,η)=O(n− 21(logη−1)1 2). This proves the claim for the weighted quadratic divergence.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 32
Fisher-Rao Divergence: As log2x≤x−2+x−1 for all x>0, the Fisher-Rao divergence satisfies
p p Å ã
(cid:88) (cid:88) 1
D(X,Y)= log2λ (XY−1)≤ λ (XY−1)−2+ =Tr[XY−1]−2p+Tr[YX−1]
i i λ (XY−1)
i
i=1 i=1
forallX,Y ∈Sp , wherethelastexpressionequalstwicethesymmetrizedSteindivergenceof X andY. We
++
have already shown that (23) holds for symmetrized Stein divergence for all n ≥ n (η) = O(logη−1) and
min
η ∈(0,1) provided that c= p . Thus, (23) must also hold for the Fisher-Rao divergence if c= 2p. As usual,
x1 x1
one readily verfies that ε
min
=O(n−1 2(logη−1)1 2). This proves the claim for the Fisher-Rao divergence. □
Appendix C. Verification of the Minimax Property
Proposition 13. All the divergences listed in Table 1 satisfy Assumption 1.
Proof of Proposition 13. Our goal is to prove the minimax equality
min max Tr[X2]−2⟨Σ,X⟩= max min Tr[X2]−2⟨Σ,X⟩. (27)
X∈Sp
+ Σ∈Bε(Σ“) Σ∈Bε(Σ“)
X∈Sp
+
If D is the Kullback-Leibler, Fisher-Rao, inverse Stein, symmetrized Stein or weighted quadratic divergence
and if Σ(cid:98) is singular, then (Σ,Σ(cid:98))̸∈dom(D) for every Σ∈Sp +. In this case, the uncertainty set B ε(Σ(cid:98))={Σ∈
Sp :D(Σ,Σ(cid:98))≤ε}isempty,andtheminimaxequality (27)holdstriviallybecausebothsidesof (27)evaluate
+
to ∞. Thus, we may always assume that Σ(cid:98) ∈Sp for these divergences.
++
TheobjectivefunctionTr[X2]−2⟨Σ,X⟩oftheminimaxproblem(27)isconvexandcontinuousinX forany
fixedΣ∈B ε(Σ(cid:98)),anditisconcaveandcontinuousinΣforanyfixedX ∈Sp +. IfB ε(Σ(cid:98))isconvexandcompact,
then (27) follows readily from Sion’s classic minimax theorem. We will argue below that this is true for
the Kullback-Leibler, Wasserstein, symmetrized Stein, quadratic, and weighted quadratic divergences. The
uncertainty sets associated with the quadratic and weighted quadratic divergences constitute ellipsoids and
are,therefore,triviallyconvexandcompact. Inaddition,theconvexityandcompactnessoftheuncertaintyset
inducedbytheWassersteindivergencefollowfrom[45,LemmaA.6]. WenextshowthattheKullback-Leibler
and symmetrized Stein divergences also induce convex and compact uncertainty sets.
Kullback-Leibler Divergence: For any fixed Σ(cid:98) ∈Sp , the Kullback-Leibler divergence D(Σ,Σ(cid:98)) consti-
++
tutes a continuous extended real-valued function of Σ. Indeed, one can show that D(Σ,Σ(cid:98)) tends to infinity
asΣapproachestheboundaryofSp
+
andΣ(cid:98) ∈Sp
++
iskeptfixed. Therefore,theuncertaintysetB ε(Σ(cid:98))isclosed
as a sublevel set of a continuous function. As t−1−logt≥0 for every t>0, any Σ∈B ε(Σ(cid:98)) satisfies
p
1(cid:88)Ä ä 1Ä ä
ε≥D(Σ,Σ(cid:98))= λ i(Σ(cid:98)−1Σ)−1−logλ i(Σ(cid:98)−1Σ) ≥ λ p(Σ(cid:98)−1Σ)−1−logλ p(Σ(cid:98)−1Σ) .
2 2
i=1
Notethatthefunctiont−1−logtgrowsindefinitelyasttendstoinfinity. Consequently,theaboveinequality
implies that there exists λ>0 with λ p(Σ(cid:98)−1Σ)≤λ for all Σ∈B ε(Σ(cid:98)). Recall now that the operator norm of
any positive definite matrix coincides with its maximum eigenvalue. For any Σ∈B ε(Σ(cid:98)) we thus have
∥Σ∥=∥Σ(cid:98)1 2Σ(cid:98)−1 2ΣΣ(cid:98)−1 2Σ(cid:98)1 2∥≤∥Σ(cid:98)−1 2ΣΣ(cid:98)− 21∥∥Σ(cid:98)∥=λ p(ΣΣ(cid:98)−1)λ p(Σ(cid:98))≤λλ p(Σ(cid:98)),
where the second equality holds because ∥Σ(cid:98)− 21ΣΣ(cid:98)−1 2∥ = λ p(Σ(cid:98)− 21ΣΣ(cid:98)−1 2) and because ΣΣ(cid:98)−1 has the same
eigenvalues as Σ(cid:98)− 21ΣΣ(cid:98)−1 2. This shows that B ε(Σ(cid:98)) is bounded and thus compact. Finally, note that D(Σ,Σ(cid:98))
is convex in Σ because Tr[Σ(cid:98)−1Σ] is linear and logdet(Σ(cid:98)Σ−1) is convex in Σ. Hence, B ε(Σ(cid:98)) is convex.
Symmetrized Stein Divergence: For any fixed Σ(cid:98) ∈Sp , the symmetrized Stein divergence D(Σ,Σ(cid:98)) is
++
continuous in Σ. Thus, the corresponding uncertainty set B ε(Σ(cid:98)) is closed. Also, any Σ∈B ε(Σ(cid:98)) satisfies
p
1(cid:88)Ä ä 1Ä ä
ε≥D(Σ,Σ(cid:98))=
2
λ i(Σ(cid:98)−1Σ)+λ−
i
1(Σ(cid:98)−1Σ)−2 ≥
2
λ p(Σ(cid:98)−1Σ)+λ− p1(Σ(cid:98)−1Σ)−2 ,
i=1A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 33
where the second inequality holds because all eigenvalues of Σ(cid:98)−1Σ are positive. Note that t+t−1−2 grows
indefinitely as t tends to infinity. Hence, there exists λ>0 with λ p(Σ(cid:98)−1Σ)≤λ for all Σ∈B ε(Σ(cid:98)). By using a
similar reasoning as for the Kullback-Leibler divergence, we can thus show that B ε(Σ(cid:98)) is compact. To prove
convexity,weneedtoshowthatD(Σ,Σ(cid:98))isaconvexfunctionofΣ. Butthisfollowsfrom[9,Exercise3.18(a)].
The uncertainty sets induced by the Fisher-Rao and inverse Stein divergences fail to be convex in the
standardEuclideansense;seeSectionC.1. Wewillshow,however,thattheseuncertaintysetsaregeodesically
convex with respect to a certain Riemannian geometry on the cone Sp . This will allow us to prove the
++
minimax equality (27) by appealing to Theorem 3, which establishes a generalized version of Sion’s minimax
theorem for geodesic quasi-convex-quasi-concave minimax problems on Hadamard manifolds.
In order to apply Theorem 3, we embed the feasible set Sp of the minimization problem in (27) into Sp
+
equipped with the usual Euclidean geometry. Recall from Example 3 that Sp can be viewed as a Hadamard
manifold and that the associated geodesic convexity coincides with the usual Euclidean convexity. Thus, the
feasible set Sp constitutes a convex subset of the Hadamard manifold Sp. In addition, we embed the feasible
+
set B ε(Σ(cid:98)) of the maximization problem in (27) into Sp ++. Recall from Example 4 that Sp
++
also constitutes
a Hadamard manifold. The objective function Tr[X2]−2⟨Σ,X⟩ of (27) is ostensibly convex and continuous
in X. Similarly, by Lemma 13, the objective function is geodesically concave and continuous in Σ. Hence,
Theorem 3 applies, and the desired minimax equality (27) follows if we can prove that B ε(Σ(cid:98)) is geodesically
convex as well as compact with respect to the metric topology induced by the Riemannian geometry on Sp .
++
ByRemark1,however,thisnotionofcompactnessisequivalenttotheusualcompactnessnotionwithrespect
to the Euclidean space Sp. Therefore, it suffices to show that B ε(Σ(cid:98)) is compact in the usual sense.
AsfortheFisher-Raodivergence,thecompactnessandgeodesicconvexityofB ε(Σ(cid:98))followfromLemma12.
It thus remains to prove the desired properties of B ε(Σ(cid:98)) for the inverse Stein divergence.
Inverse Stein Divergence: For any fixed Σ(cid:98) ∈ Sp , the inverse Stein divergence D(Σ,Σ(cid:98)) is continuous
++
in Σ. Therefore, the corresponding uncertainty set B ε(Σ(cid:98)) is closed. In addition, any Σ∈B ε(Σ(cid:98)) satisfies
p
1(cid:88)Ä ä 1Ä ä
ε≥D(Σ,Σ(cid:98))= λ i(Σ−1Σ(cid:98))−1−logλ i(Σ−1Σ(cid:98)) ≥ λ 1(Σ−1Σ(cid:98))−1−logλ 1(Σ−1Σ(cid:98)) ,
2 2
i=1
wherethesecondinequalityholdsbecauset−1−logt≥0forallt>0. Ast−1−logtgrowsindefinitelywhent
tends to 0, the above inequality implies that there exists λ > 0 with λ 1(Σ−1Σ(cid:98)) ≥ λ for all Σ ∈ B ε(Σ(cid:98)). This
in turn implies that λ p(Σ(cid:98)−1Σ)=λ− 11(Σ−1Σ(cid:98))≤λ−1 for all Σ∈B ε(Σ(cid:98)). We may thus conclude that B ε(Σ(cid:98)) is
Ä ä
compact. Finally, sinceD(Σ,Σ(cid:98))= 1 Tr[Σ−1Σ(cid:98)]−p+logdetΣ−logdetΣ(cid:98) , D(Σ,Σ(cid:98))isageodesicallyconvex
2
function of Σ thanks to Lemmas 13(ii) and 13(iii). Therefore, B ε(Σ(cid:98)) is a geodesically convex set by virtue of
Proposition 14. □
C.1. Inapplicability of Sion’s Minimax Theorem
We now show through counterexamples that if D(Σ,Σ(cid:98)) is the Fisher-Rao or inverse Stein divergence, then
¶ ©
the corresponding uncertainty set B ε(Σ(cid:98))= Σ∈Sp
+
:D(Σ,Σ(cid:98))≤ε fails to be a convex subset of Sp. Hence,
for these divergences, we cannot appeal to Sion’s classic minimax theorem to prove (27). More precisely, we
will show that D(Σ,Σ(cid:98)) fails to be quasi-convex and thus has non-convex sublevel sets.
Definition 4 (Quasi-convex function). A function ψ : Sp → R is quasi-convex if for any Σ ,Σ ∈ Sp and
+ 1 2 +
λ∈[0,1], we have ψ(λΣ +(1−λ)Σ )≤max{ψ(Σ ),ψ(Σ )}.
1 2 1 2
Example 1 (Non-convexityoftheFisher-Raouncertaintyset). The function D(Σ,Σ(cid:98))=∥log(Σ(cid:98)−1 2ΣΣ(cid:98)−1 2)∥2
F
is not quasi-convex in Σ for any fixed Σ(cid:98) ∈S3 ++. To see this, assume first that Σ(cid:98) =I 3. Setting
Ö è Ö è
33 −5 −10 6 −4 5
Σ = −5 6 3 and Σ = −4 11 −2 ,
1 2
−10 3 4 5 −2 18A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 34
one readily verifies that Σ ,Σ ≻0, while D(Σ ,I )=16.4501 and D(Σ ,I )=16.2111. In addition, we find
1 2 1 3 2 3
D(1Σ + 1Σ ,I )=18.6796>max{16.4501, 16.2111}=max{D(Σ ,I ),D(Σ ,I )}.
2 1 2 2 3 1 3 2 3
This shows that D(Σ,I 3) fails to be quasi-convex in Σ. For a generic Σ(cid:98) ∈S3 ++, we define Σ′
1
=Σ(cid:98)21Σ 1Σ(cid:98)21 and
Σ′
2
=Σ(cid:98)1 2Σ 2Σ(cid:98)21. The above inequality then immediately implies that
D(1Σ′ + 1Σ′,Σ(cid:98))>max{D(Σ′,Σ(cid:98)),D(Σ′,Σ(cid:98))}.
2 1 2 2 1 2
Consequently, the function D(Σ,Σ(cid:98)) fails to be quasi-convex in Σ irrespective of Σ(cid:98) ∈S3 .
++
Example 2 (Non-convexity of the inverse Stein uncertainty set). The function D(Σ,Σ(cid:98))= 1(Tr[Σ−1Σ(cid:98)]−3+
2
logdet(ΣΣ(cid:98)−1)) is not quasi-convex in Σ for any fixed Σ(cid:98) ∈S3 ++. Indeed, if Σ(cid:98) =I 3, we may set
Ö è Ö è
30 13 23 27 13 23
Σ = 13 12 9 and Σ = 13 10 14 .
1 2
23 9 20 23 14 30
It can be verified that Σ ,Σ ≻0, while D(Σ ,I )=4.0427 and D(Σ ,I )=4.3020. In addition, we find
1 2 1 3 2 3
D(1Σ + 1Σ ,I )=4.3262>max{4.0427, 4.3020}=max{D(Σ ,I ),D(Σ ,I )}.
2 1 2 2 3 1 3 2 3
This shows that D(Σ,I 3) fails to be quasi-convex in Σ. For a generic Σ(cid:98) ∈S3 ++, we define Σ′
1
=Σ(cid:98)21Σ 1Σ(cid:98)21 and
Σ′
2
=Σ(cid:98)1 2Σ 2Σ(cid:98)21. The above inequality then immediately implies that
D(1Σ′ + 1Σ′,Σ(cid:98))>max{D(Σ′,Σ(cid:98)),D(Σ′,Σ(cid:98))}
2 1 2 2 1 2
that is, the function D(Σ,Σ(cid:98)) fails to be quasi-convex in Σ irrespective of Σ(cid:98) ∈S3 .
++
C.2. Riemannian Geometry and Geodesic Convexity
In order to keep this paper self-contained, we now briefly review some basic concepts from Riemannian
geometry. For a more comprehensive survey of this topic, we refer to the excellent textbooks [29, 37].
Definition 5 (Riemannian manifold). A Riemannian manifold is a pair (M,{⟨·, ·⟩ } ) consisting of a
u u∈M
differentiable manifold M and a smooth family of inner products {⟨·, ·⟩ } defined on the tangent spaces
u u∈M
T M of M. That is, for any u∈M, ⟨·, ·⟩ represents a symmetric, positive definite bilinear map on T M.
u u u
The family {⟨·, ·⟩ } of inner products is called a Riemannian metric on M.
u u∈M
Throughout this paper we will restrict attention to Hadamard manifolds.
Definition 6 (Hadamard manifolds). A Hadamard manifold is a complete, simply connected Riemannian
manifold that has everywhere non-positive sectional curvature.
Intuitively, the sectional curvature of a Riemannian manifold is non-positive at a point u if and only if the
areaofanysmalltwo-dimensionaldisccenteredatuislargerorequaltotheareaofadiscwiththesameradius
in flat space. For a formal definition see [29, p. 236] or [37, p. 154]. All piecewise continuously differentiable
curves on a Riemannian manifold—and, in particular, on a Hadamard manifold—can be assigned a length.
Definition 7 (Length of a curve). The length of a continuously differentiable curve c : [0,1] → M on a
Riemannian manifold (M,{⟨·, ·⟩ } ) is defined as
u u∈M
(cid:90) 1»
L(c)= ⟨c˙(t),c˙(t)⟩ dt.
c(t)
0
If c is piecewise continuously differentiable, then its length is defined as the sum of the lengths of its pieces.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 35
The Riemannian distance between two points u ,u ∈M is defined as d (u ,u )=min L(c), where the
1 2 M 1 2 c
minimum is over all continuously differentiable curves c with constant speed (⟨c˙(t),c˙(t)⟩ c(t))21 that connect
u and u . For complete and connected Riemannian manifolds, the minimum is guaranteed to exist, and
1 2
any minimizer is a geodesic. Moreover, by the Hopf-Rinow theorem [29, 37], any two points on a Hadamard
manifoldareconnectedbyauniquegeodesic. Thisgreatlysimplifiesthestudyofconvexityonsuchmanifolds.
Definition 8 (Geodesically convex sets). If (M,{⟨·, ·⟩ } ) is a Hadamard manifold, then U ⊆ M is
u u∈M
geodesically convex if, for any u ,u ∈U, the image of the geodesic connecting u and u lies within U.
1 2 1 2
Definition 9 (Geodesically (quasi-)convex function). If (M,{⟨·, ·⟩ } ) is a Hadamard manifold and
u u∈M
U ⊆M is geodesically convex, then the function ψ : U →R is geodesically (quasi-)convex if the composition
ψ◦c:[0,1]→R is (quasi-)convex function in the usual Euclidean sense for every geodesic c connecting two
arbitrary points in U. In addition, ϕ is geodesically (quasi-)concave if −ϕ is geodesically (quasi-)convex.
Definition9makessensebecauseageodesicisalwaysparametrizedproportionallytoarclength. Itreadily
implies that all sublevel sets of a geodesically quasi-convex function are geodesically convex.
Proposition 14 ([62, Theorem 3.4]). If (M,{⟨·, ·⟩ } ) is a Hadamard manifold and ψ : M → R is
u u∈M
geodesically quasi-convex, then the sublevel set {u∈M:ψ(u)≤α} is geodesically convex for any α∈R.
The examples below are useful for our theoretical development and used in the proof of Proposition 13.
Example 3. The Euclidean spaces Rp and Sp equipped with their usual inner products constitute Hadamard
manifolds. In both cases, geodesic convexity (of sets as well as functions) reduces to Euclidean convexity.
Example 4. The cone of positive definite matrices Sp represents a differentiable manifold [5,29]. The tan-
++
gent space T Sp at Σ∈Sp is naturally identified with Sp, that is, all tangent vectors constitute symmetric
Σ ++ ++
matrices. We can assign every Σ∈Sp an inner product ⟨·, ·⟩ :Sp×Sp →R defined through
++ Σ
⟨Σ ,Σ ⟩ =Tr[Σ−1Σ Σ−1Σ ] ∀Σ ,Σ ∈Sp.
1 2 Σ 1 2 1 2
By [29, Theorem XII 1.2], Sp equipped with the inner products ⟨·, ·⟩ , Σ∈Sp , is a Hadamard manifold.
++ Σ ++
Remark 1. By definition, any Hadamard manifold (M,{⟨·, ·⟩ } ) is simply connected and therefore,
u u∈M
in particular, connected. Hence, [38, Theorem 13.29] implies that the metric topology on M induced by the
Riemannian distance d coincides with the manifold topology. For instance, the metric topology on the
M
Hadamard manifold Sp from Example 4 coincides with the subspace topology on Sp inherited from the
++ ++
ambient vector space Sp, which is the standard (Euclidean norm) topology used for matrices.
In the following lemmas, we treat Sp as a Hadamard manifold in the sense of Example 4.
++
Lemma 12 (Compactness and convexity [46, Theorem 2.5]). For any fixed Σ′ ∈Sp , the set
++
(cid:110) Σ∈Sp : ∥log(Σ′− 21 ΣΣ′− 21 )∥2 ≤ε2(cid:111)
++ F
constitutes a compact and geodesically convex subset of Sp .
++
We now show that several popular matrix functions are geodesically convex. Here, we adopt the standard
terminology whereby a function that is both geodesically convex and concave is called geodesically linear.
Lemma 13 (Geodesic convexity of popular matrix functions). The following hold.
(i) g(Σ)=Tr[XΣ] is geodesically convex on Sp for every X ∈Sp.
++ +
(ii) g(Σ)=Tr[XΣ−1] is geodesically convex on Sp for every X ∈Sp.
++ +
(iii) g(Σ)=logdetΣ is geodesically linear on Sp .
++A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 36
Proof of Lemma 13. We can prove assertion (i) by showing that, for every fixed Σ ∈ Sp , the Riemannian
++
Hessian of the function g(Σ)=Tr[XΣ] is positive semidefinite on the tangent space T ΣSp
++
∼ =Sp [2, 62]. To
this end, note first that the Euclidean gradient of g is given by ∇g(Σ) = X and that the Euclidean Hessian
∇2g(Σ) coincides with the zero map from Sp to Sp. By [14, § 4.2], the Riemannian Hessian of g thus satisfies
1
Hessg(Σ)[S]= (SXΣ+ΣXS) ∀S ∈Sp.
2
This implies that
(cid:10) Hessg(Σ)[S],S(cid:11) =Tr[SXSΣ−1]≥0 ∀S ∈Sp,
Σ
where the inequality holds because SXS ∈ Sp and Σ−1 ∈ Sp . Thus, the Riemannian Hessian of g is
+ ++
positive semidefinite on the tangent space T ΣSp
++
∼ = Sp. As Σ ∈ Sp
++
was chosen freely, this shows via [62,
Theorem 6.2] that g is geodesically convex throughout Sp .
++
Assertions(ii)and(iii)areprovedsimilarly. Asforassertion(ii),notethatthegradientofg(Σ)=Tr[XΣ−1]
is given by ∇g(Σ)=−Σ−1XΣ−1 [50, § 2.2]. Also, the Hessian of g is a linear operator on Sp satisfying
∇2g(Σ)[S]=
d∇g(Σ+tS)(cid:12)
(cid:12) (cid:12) =−
d(Σ+tS)−1(cid:12)
(cid:12) (cid:12) XΣ−1−Σ−1X
d(Σ+tS)−1(cid:12)
(cid:12) (cid:12)
dt (cid:12) dt (cid:12) dt (cid:12)
t=0 t=0 t=0
=Σ−1SΣ−1XΣ−1+Σ−1XΣ−1SΣ−1,
where the third equality exploits [50, § 2.2]. By [14, § 4.2], the Riemannian Hessian of g thus satisfies
1
Hessg(Σ)[S]= (SΣ−1X+XΣ−1S) ∀S ∈Sp.
2
This implies that
(cid:10) Hessg(Σ)[S],S(cid:11) = 1 Tr[Σ−1(SΣ−1X+XΣ−1S)Σ−1S]=Tr[SΣ−1SΣ−1XΣ−1]≥0 ∀S ∈Sp,
Σ 2
where the inequality holds because SΣ−1S and Σ−1XΣ−1 are positive semidefinite. Thus, the Riemannian
Hessian of g is positive semidefinite on the tangent space T ΣSp
++
∼ =Sp, and the claim follows.
Asforassertion(iii), thegradientofg(Σ)=logdetΣisgivenby∇g(Σ)=−Σ−1, andtheHessianofg isa
linearoperatoronSp satisfying∇2g(Σ)[S]=Σ−1SΣ−1 [50,§2.2]. By[14,§4.2],theRiemannianHessianofg
thus satisfies Hessg(Σ)[S]=0 for all S ∈Sp. Hence, g is both geodesically convex and concave on Sp . □
++
C.3. A Riemannian Generalization of Sion’s Minimax Theorem
WenowproveageneralizationofSion’sminimaxtheoremforgeodesicallyconvex-concavesaddlefunctions
on Hadamard manifolds. This result appears to be new and may be of independent interest.4
Theorem 3 (Sion’s minimax theorem for geodesically convex-concave saddle problems). Let U and V be
geodesicallyconvexsubsetsoftwoHadamardmanifolds,andassumethatU iscompact. Also,letψ :U×V →R
beafunctionwithψ(u, ·)beinguppersemi-continuousandgeodesicallyquasi-concaveonV foranyfixedu∈U
and with ψ(·,v) being lower semi-continuous and geodesically quasi-convex on U for every fixed v ∈V. Then,
minsup ψ(u,v)=supmin ψ(u,v).
u∈Uv∈V v∈Vu∈U
The proof of Theorem 3 parallels that of Sion’s minimax theorem as presented in [26], with appropriate
modifications to account for the manifold setting. The following two lemmas are instrumental for the proof.
Lemma 14. If all conditions of Theorem 3 hold, v ,v ∈ V and α < min max{ψ(u,v ),ψ(u,v )}, then
1 2 u∈U 1 2
there exists v ∈V with α<min ψ(u,v ).
0 u∈U 0
4While finalizing this paper, we discovered a concurrent work describing a result akin to Theorem 3 [70]. A preliminary
versionofourpaper—includingTheorem3—waspresentedattheRobustOptimizationWebinaron24June,2021.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 37
Proof of Lemma 14. Fix any v ,v ∈V and α<min max{ψ(u,v ),ψ(u,v )}, and suppose for the sake of
1 2 u∈U 1 2
contradiction that α≥min ψ(u,v) for all v ∈V. Next, choose any β with
u∈U
α<β <minmax{ψ(u,v ),ψ(u,v )}.
1 2
u∈U
Let c:[0,1]→V be the unique geodesic from v to v , and denote by [v ,v ]=c([0,1]) its image. Also, for
1 2 1 2
any threshold ζ ∈R and point v ∈[v ,v ] on the geodesic, we denote the sublevel set of ψ(·,v) at level ζ as
1 2
L (ζ)={u∈U :ψ(u,v)≤ζ}.
v
Note that L (α) and L (β) are non-empty for all v ∈V because of our assumption that α≥min ψ(u,v).
v v u∈U
In addition, L (α) and L (β) are closed because ψ(u,v) is lower semi-continuous in u. Suppose now that
v v
there is u¯∈L (β)∩L (β) such that ψ(u¯,v )≤β and ψ(u¯,v )≤β. By the choice of β and u¯, we thus have
v1 v2 1 2
β <minmax{ψ(u,v ),ψ(u,v )}≤max{ψ(u¯,v ),ψ(u¯,v )}≤β,
1 2 1 2
u∈U
which is a contradiction. Hence, L (β)∩L (β)=∅. As ψ(u, ·) is geodesically quasi-concave on V for every
v1 v2
fixed u∈U, the composition ψ(u,c(·)) is quasi-concave in the classical sense on [0,1]. Therefore, we find
ψ(u,v)=ψ(u,c(t ))≥min{ψ(u,c(0)),ψ(u,c(1))}=min{ψ(u,v ),ψ(u,v )}
v 1 2
for every u ∈ U and v ∈ [v ,v ], where t ∈ [0,1] is the pre-image of v under the geodesic map c, that
1 2 v
is, t is the unique solution of the equation c(t ) = v. This implies that L (β) ⊆ L (β)∪L (β). By
v v v v1 v2
Proposition 14, which applies because ψ(·,v) is geodesically quasi-convex for every v ∈ [v ,v ] ⊆ V, the
1 2
set L (α) is geodesically convex and hence connected. In summary, we have shown that, for any v ∈[v ,v ],
v 1 2
the connected set L (α)⊆L (β) is covered by the union of L (β) and L (β), which are mutually disjoint.
v v v1 v2
Hence, exactly one of the following two inclusions holds:
L (α)⊆L (β)⊆L (β) or L (α)⊆L (β)⊆L (β). (28)
v v v1 v v v2
Next, define I = {t ∈ [0,1] : L (α) ⊆ L (β)} and J = {t ∈ [0,1] : L (α) ⊆ L (β)}. Since α < β,
c(t) v1 c(t) v2
c(0)=v andc(1)=v ,itisclearthat0∈I and1∈J,thatis,bothsetsarenon-empty. By (28),wefurther
1 2
have I ∩J =∅ and I ∪J =[0,1]. We will now show that I is closed. To this end, let {tk} k∈N be a sequence
in I converging t∞ ∈ [0,1]. To prove that I is closed, we must show that t∞ ∈ I. Define v = c(t∞), and
select any u∈L (α). By construction, we have ψ(u,v)≤α<β. Furthermore, by the upper semi-continuity
v
of ψ(u, ·) on V and the continuity of c, we therefore obtain
limsupψ(u,c(tk))≤ψ(u, lim c(tk))=ψ(u,v)≤α<β.
k→∞ k→∞
This implies that there is k′ ∈ N such that v′ = c(tk′ ) satisfies ψ(u,v′) < β, that is, u ∈ L v′(β). Since
tk′ ∈ I, we know from the definition of I that L v′(α) ⊆ L v1(β). However, in view of the dichotomy (28),
this is only possible if L v′(β) ⊆ L v1(β). Thus, u ∈ L v1(β). Since u ∈ L v(α) was chosen arbitrarily, we have
L (α)⊆L (β). As v =c(t∞), we thus have t∞ ∈I, proving that I is closed. Similarly, we can show that J
v v1
is closed, too. However, as I and J form a partition of [0,1], they cannot be simultaneously closed. This
contradiction implies that our initial assumption was false, that is, we have indeed α<min ψ(u,v ). □
u∈U 0
Lemma 15. If all conditions of Theorem 3 hold, v ,...,v ∈ V and α < min max ψ(u,v ) for
1 n u∈U 1≤i≤n i
some n∈N, then there exists v ∈V with α<min ψ(u,v ).
0 u∈U 0
Proof of Lemma 15. The statement trivially holds if U = ∅. In the remainder we may thus assume without
loss of generality that U ≠ ∅. We prove the claim by induction on n. The base step corresponding to n = 1
is trivial. As for the induction step, fix any n>1, and assume that the claim corresponding to n−1 is true.
Next, define the sublevel set U ={u∈U :ψ(u,v )≤α}, which is geodesically convex and closed thanks to
n n
our assumptions about ψ and U. In addition, U inherits compactness from U. We then have
n
α<min max ψ(u,v )≤ min max ψ(u,v )= min max ψ(u,v ),
i i i
u∈U1≤i≤n u∈Un1≤i≤n u∈Un1≤i≤n−1A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 38
where the second inequality follows from the inclusion U ⊆ U, and the equality holds because any u ∈ U
n n
satisfies ψ(u,v )≤α, which implies that i=n never attains the maximum. As the sets U and V as well as
n n
the restriction of ψ to U ×V satisfy all conditions of Theorem 3, we may invoke the induction hypothesis
n
to conclude that there exists v′ ∈ V with α < min ψ(u,v′). Hence, for any u ∈ U, we have either
0 u∈Un 0
α<ψ(u,v′) (if u∈U ) or α<ψ(u,v ) (if u∈U \U ). In other words, we have shown that
0 n n n
α<minmax{ψ(u,v′),ψ(u,v )}.
0 n
u∈U
By Lemma 14, we may conclude that α<min ψ(u,v ) for some v ∈V. This completes the proof. □
u∈U 0 0
The proof of Theorem 3 also relies on the following elementary topological lemma.
Lemma 16. Let {X } be a non-empty family of compact subsets of a Hausdorff topological space with
a a∈A
∩ X =∅. Then, there exist finitely many indices a ,...,a ∈A with ∩n X =∅.
a∈A a 1 n i=1 ai
Proof of Lemma 16. Fixanarbitraryindexa ∈A, anddefineY =X \X foreverya∈A. NotethatX
0 a a0 a a0
is Hausdorff because it constitutes a subspace of a Hausdorff space. Recall also that X is compact and
a0
that any compact subset of a Hausdorff space is closed. Therefore, Y is open with respect to the subspace
a
topology on X . By de Morgan’s laws, we further have
a0
(cid:91) (cid:92)
Y =X \ X =X \∅=X .
a a0 a a0 a0
a∈A a∈A
Thus, {Y } constitutesanopencoverofX . AsX iscompact, thereisafinitesub-cover{Y }n with
a a∈A a0 a0 ai i=1
n n
(cid:91) (cid:92)
X = Y =X \ X ,
a0 ai a0 ai
i=1 i=1
where the second equality follows again from de Morgan’s laws. We have thus shown that ∩n X =∅. □
i=0 ai
We are now armed to prove Theorem 3.
Proof of Theorem 3. By the max-min inequality, we have
supmin ψ(u,v)≤minsup ψ(u,v).
v∈Vu∈U u∈Uv∈V
It thus suffices to prove the reverse inequality. To this end, select any α<min sup ψ(u,v), and define
u∈U v∈V
U = {u ∈ U : ψ(u,v) ≤ α} for every v ∈ V. As ψ(·,v) is lower semi-continuous, U is a closed subset of U
v v
and thus compact. Suppose now that there exists u∈∩ U . By the definitions of u and U , we then find
v∈V v v
supψ(u,v)≤α,
v∈V
which contradicts the selection of α. We may thus conclude that ∩ U = ∅, which implies via Lemma 16
v∈V v
that there exist finitely many indices v ,...,v ∈V with ∩n U =∅. This in turn implies that
1 n i=1 vi
α<min max ψ(u,v ).
i
u∈U1≤i≤n
Lemma 15 then guarantees the existence of a point v ∈ V satisfying α < min ψ(u,v ). Therefore, we
0 u∈U 0
have α<sup min ψ(u,v). As α<min sup ψ(u,v) was chosen arbitrarily, we finally obtain
v∈V u∈U u∈U v∈V
minsup ψ(u,v)≤supmin ψ(u,v).
u∈Uv∈V v∈Vu∈U
This observation completes the proof. □A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 39
Appendix D. Verification of the Rearrangement Property
Proposition 15. All the divergences listed in Table 1 satisfy Assumption 2(c).
Proof of Proposition 15. Let D be the Kullback-Leibler, Fisher-Rao, inverse Stein or symmetric Stein diver-
gence. Ineithercase,ifxory containsanyvanishingentry,thenbothsidesoftherearrangementinequalityin
Assumption 2(c) evaluate to +∞; see the definitions in Table 1. Thus, Assumption 2(c) is trivially satisfied.
Itthereforesufficestoprovetheinequalityforx,y ∈Rp . Next, letD betheweightedquadraticdivergence.
++
Hence, if y contains any vanishing entry, then both sides of the rearrangement inequality evaluate again
to +∞, and Assumption 2(c) is trivially satisfied. It therefore suffices to assume that y ∈ Rp . With these
++
assumptions in place, both sides of the rearrangement inequality are guaranteed to be finite.
Thesubsequentproofrequiresadditionalnotation. Weuseσ (S)todenotethei-thsmallestsingularvalue
i
ofthematrixS ∈Sp. Thevectorσ(S)∈Rp isthendefinedthrough(σ(S)) =σ (S)foralli=1,...,p. Any
+ i i
univariate function g :R→R naturally induces multivariate functions g :Rp →Rp and g :Sp →Sp, which,
by slight abuse of notation, are represented by the same symbol g. Specifically, for any x ∈ Rp, we define
g(x)∈Rp through(g(x)) =g(x )foralli=1,...,p. Similarly,foranyS ∈Sp witheigenvaluedecomposition
i i
S =V Diag(λ(S))V⊤ with V ∈O , we define g(S)∈Sp through g(S)=V Diag(g(λ(S)))V⊤.
S S S p S S
Observe now that all divergences listed in Table 1 are representable as
p p
(cid:88)(cid:0) (cid:1) (cid:88) (cid:0) 1 1 (cid:1)
D(X,Y)= h 1(λ i(X))+h 2(λ i(Y)) + f λ i(g 2(Y 2)g 1(X)g 2(Y 2)) (29)
i=1 i=1
for some functions f, h , h , g and g from R to R as specified in Table 4.
1 2 1 2
Divergence h (t) h (t) g (t) g (t) f(t) tf′(t)
1 2 1 2
Kullback-Leibler −1 −1 t 1 1(t−logt) 1(t−1)
4 4 t 2 2
√ √
Wasserstein t t t t −2 t − t
Fisher-Rao 0 0 t 1 (logt)2 2logt
t
Inverse Stein −1 −1 t 1 1(cid:0)1 +logt(cid:1) 1(1− 1)
4 4 t 2 t 2 t
Symmetrized Stein −1 −1 t 1 1(cid:0) t+ 1(cid:1) 1(t− 1)
2 2 t 2 t 2 t
Quadratic t2 t2 t t −2t −2t
Weighted quadratic −2t t t2 1 t t
t
Table 4. Functions h , h , g , g and f in the representation (29) of the divergences of Table 1.
1 2 1 2
As the spectrum of any matrix is invariant under conjugation with an orthogonal matrix V ∈O(p), we have
p p
(cid:88)(cid:0) h (λ (V Diag(x↑)V⊤))+h (λ (Diag(y↑)))(cid:1) =(cid:88)(cid:0) h (λ (Diag(x↑)))+h (λ (Diag(y↑)))(cid:1)
1 i 2 i 1 i 2 i
i=1 i=1
for all x,y ∈Rp. In view of the representation (29) and the above identity, it remains to be shown that
p p
(cid:88) fÄ λ (Diag(√ y↑ )Vg (Diag(x↑))V⊤g (Diag(√ y↑ )))ä ≥(cid:88) f(cid:0) λ (g (Diag(x↑))g (Diag(y↑)))(cid:1) (30)
i 1 2 i 1 2
i=1 i=1
for all x,y ∈Rp and V ∈O(p). Table 4 shows that always either of the following two conditions holds:
+A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 40
• t(cid:55)→tf′(t) is strictly increasing, g is strictly increasing and g is is strictly decreasing;
1 2
• t(cid:55)→tf′(t) is strictly decreasing, and g and g are both strictly increasing.
1 2
The desired inequality (30) then follows from [69, Theorem 3]. Inspecting the proofs of [69, Theorem 3 and
Lemma1]furtherrevealsthat(30)holdsifandonlyifVg (Diag(x↑))V⊤ =g (Diag(x↑)), whichisequivalent
1 1
to V Diag(x↑)V⊤ =Diag(x↑) because g is strictly increasing. This observation completes the proof. □
1
Appendix E. Proofs of Section 4
Proof of Theorem 2. We prove the assumptions one by one. Note first that, by Proposition 13, every diver-
gence D in Table 1 satisfies the minimax property specified in Assumption 1.
Assumption2requiresDtobeaspectraldivergence. ToshowthatDisorthogonallyequivariant,recallthat
thespectrumofamatrixispreservedundersimilaritytransformations. Asthetraceandthedeterminantare
spectralfunctions,theorthogonalequivarianceofalldivergencesinTable1iseasilyverifiedusingelementary
rulesofmatrixalgebra. ItisalsostraightforwardtoverifythateverydivergenceD inTable1isspectralwith
generator d as specified in Table 2. In addition, the domain of d contains a point (a,b) with b > 0, and d is
ostensibly continuous throughout its domain. The rearrangement property holds thanks to Proposition 15.
Assumption4followsimmediatelyfromdefinitionsofthegeneratorsinTable2. Forexample,itisclearthat
the generator d (·) = d(·,b) = (log(·/b))2 of the Fisher-Rao divergence is twice continuously differentiable
b
on R for any fixed b>0. In addition, we have d′′(a)=2(1−log(a/b))/a2 >0 for any a∈(0,b] and b>0,
++ b
which shows that d is convex on [0,b]. Similarly, one can prove Assumption 4 for all other divergences.
b
It remains to be shown that all generators in Table 2 satisfy the differential inequality of Assumption 5.
For example, the generator d(a,b)=(log(a/b))2 of the Fisher-Rao divergence satisfies
∂ 2 a ∂2 2 (cid:16) a(cid:17) ∂2 2
d(a,b)= log , d(a,b)= 1−log and d(a,b)=− ∀a,b∈R .
∂a a b ∂a2 a2 b ∂b∂a ab ++
Therefore, we obtain
∂2 ∂2 ∂ 2(cid:16) a(cid:17) 2 2 a 4 a
a d(a,b)+b d(a,b)− d(a,b)= 1−log − − log =− log >0
∂a2 ∂a∂b ∂a a b a a b a b
for all for any b>a>0. Hence, Assumption 5 holds for the Fisher-Rao divergence. Similarly, Assumption 5
can be proved for all other divergences using the basic rules of calculus. □
WenowproveCorollaries1,2and3,whichcharacterizetheeigenvaluemapaswellastheinverseshrinkage
intensity of the KL, Wasserstein and Fisher-Rao covariance shrinkage estimators, respectively.
Proof of Corollary 1. The generator of the KL divergence is given by d(a,b)= 1(a −1−loga); see Table 2.
2 b b
Note that Assumptions 1, 2, 4 and 5 hold by Theorem 2, Assumption 3(a) holds because Σ(cid:98) ∈ Sp , and
++
Assumption 3(b) holds because d(0,b) = +∞ for any b > 0. Therefore, Theorem 1 applies, which implies
that problem (4) is uniquely solved by X⋆ = V“Diag(x⋆)V“⊤, where x⋆
i
= s(γ⋆,xˆ i) for every i = 1,...,p.
Next, we construct the eigenvalue map s defined in (8). If b>0, then s(γ,b) is the unique solution a⋆ ≥0 of
Å ã
∂ γ 1 1
0=2a⋆+γ d(a⋆,b)=2a⋆+ − .
∂a 2 b a⋆
We thus obtain
(cid:112)
−γ+ γ2+16b2γ
s(γ,b)= .
8b
It remains to find a formula for γ⋆. By Theorem 1, γ⋆ is the unique positive root of the equation
(cid:88)p
d(s(γ⋆,xˆ ),xˆ )−ε=0 ⇐⇒
2ε+p+(cid:88)p ï −s(γ⋆,xˆ i) +logs(γ⋆,xˆ i)ò
=0.
i i
xˆ xˆ
i i
i=1 i=1A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 41
To show that γ provides an upper bound on γ⋆, note that the above equation implies that
KL
0=2ε+p+(cid:88)p ï −s(γ⋆,xˆ i) +logs(γ⋆,xˆ i)ò ≥2ε+(cid:88)p logs(γ⋆,xˆ i) ≥2ε+plogs(γ⋆,xˆ p)
.
xˆ xˆ xˆ xˆ
i i i p
i=1 i=1
Here, the two inequalities follow from Lemmas 8 and 1, which imply that s(γ,b)<b for all γ,b>0 and that
s(γ,b)/b is non-increasing in b, respectively. Rearranging the above inequality yields xˆ pe−2 pε ≥s(γ⋆,xˆ p). As
s(γ,xˆ ) is strictly increasing in γ by virtue of Lemma 8(ii), the unique solution γ of the equation
p KL
»
−γ + γ 2+16xˆ2γ
xˆ
pe−2 pε
=s(γ KL,xˆ p)=
KL KL p KL
8xˆ
p
provides an upper bound on γ⋆. The desired formula for γ is obtained by solving this equation. □
KL
√
Proof of Corollary 2. The generator of the Wasserstein divergence is given by d(a,b) = a+b−2 ab; see
Table 2. Assumptions 1, 2, 4 and 5 hold by Theorem 2, Assumption 3(a) holds because Σ(cid:98) ∈ Sp, and
+
Assumption3(b)holdsbecauseε∈(0,Tr[Σ(cid:98)]),whichimpliesthat(cid:80)p
i=1d(0,xˆ
i)=(cid:80)p
i=1xˆ
i
=Tr[Σ(cid:98)]>ε. Thus,
Theorem 1 applies. Recall now from (8) that if γ >0, then s(γ,b) is defined as the unique solution a⋆ ≥0 of
Ç … å
∂ b
0=2a⋆+γ d(a⋆,b)=2a⋆+γ 1− .
∂a a⋆
√
Solving a cubic equation in a⋆ thus reveals that s(γ,b) is given by (10a). Theorem 1 further implies that
theinverseshrinkageintensityγ⋆ istheuniquepositiverootoftheequation(10b). Toshowthatγ provides
W
an upper bound on γ⋆, let i′ ∈{1,...,p} be the smallest index i with xˆ >0. As s(γ⋆,0)=0, (10b) implies
i
p p (cid:32) (cid:33)2
0=ε−(cid:88)(cid:16)(cid:112)
xˆ
−»
s(γ⋆,xˆ
)(cid:17)2
≥ε−xˆ
(cid:88)
1−
s(γ⋆,xˆ i)
i i p
xˆ
i
i=i′ i=i′
Ç å2
≥ε−pxˆ 1−
s(γ⋆,xˆ p) =ε−p(cid:16)(cid:112)
xˆ
−»
s(γ⋆,xˆ
)(cid:17)2
, (31)
p p p
xˆ
p
where the first inequality holds because xˆ ≤ xˆ , and the second inequality follows from Lemmas 1 and 8,
i p
which imply that s(γ,b)/b is non-increasing in b and that 0 < s(γ,b) < b for all γ,b > 0, respectively. The
defining equation for s(γ⋆,xˆ ) further implies that
p
(cid:16)(cid:112)
xˆ
−»
s(γ⋆,xˆ
)(cid:17)2
=
4s(γ⋆,xˆ p)3
. (32)
p p γ⋆2
Substituting (32) into (31) yields
4ps(γ⋆,xˆ )3 4pxˆ3 pxˆ3
0≥ε− p ≥ε− p ⇐⇒ γ⋆ ≤2 p =γ .
γ⋆2 γ⋆2 ε W
This observation completes the proof. □
Proof of Corollary 3. The generator of the Fisher-Rao divergence is d(a,b)=(loga)2; see Table 2. Assump-
b
tions 1, 2, 4 and 5 hold by Theorem 2, Assumption 3(a) holds because Σ(cid:98) ∈Sp , and Assumption 3(b) holds
++
becaused(0,b)=+∞foranyb>0. Thus,Theorem1applies. Ifb>0,s(γ,b)istheuniquesolutiona⋆ ≥0of
0=2a⋆+γ ∂ d(a⋆,b)=2a⋆+ 2γ loga⋆ ⇐⇒ 2(a⋆)2 e2(a γ⋆)2 = 2b2 .
∂a a⋆ b γ γ
Recall now that, for any t>−e−1, the principal branch of the Lambert W-function is defined as the unique
solution W (t) of the equation WeW =t. Identifying W with 2(a⋆)2/γ and t with 2b2/γ >0, we thus find
0
… Å ã
s(γ,b)=
γ
W
Ä 2b2ä
=bexp
−1
W
(2b2
) , (33)
2 0 γ 2 0 γA GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 42
where the second equality holds because W 0(t) = te−W0(t). This proves (11a). Theorem 1 further implies
that the inverse shrinkage intensity γ⋆ is the unique positive root of the equation (11b). It remains to prove
that γ upper bounds γ⋆. Recalling that 0≤W (t)=texp(−W (t))≤t for any t≥0, (11b) implies that
FR 0 0
Ã
(cid:88)p Å 2xˆ2ã (cid:88)p 4xˆ4 (cid:88)p xˆ4 √
4ε= W 02 γ⋆i ≤ γ⋆2i =⇒ γ⋆ ≤ εi ≤∥Σ(cid:98)∥2
F
ε=γ FR.
i=1 i=1 i=1
This observation completes the proof. □
References
[1] S. Abbott,Understanding Analysis,Springer,2015.
[2] P.-A.Absil,R.Mahony,andR.Sepulchre,Optimization Algorithms on Matrix Manifolds,PrincetonUniversityPress,
2009.
[3] C. Atkinson and A. F. Mitchell, Rao’s distance measure, Sankhy¯a: The Indian Journal of Statistics, Series A, (1981),
pp.345–365.
[4] R. Bhatia,Matrix Analysis,Springer,1997.
[5] R. Bhatia,Positive Definite Matrices,PrincetonUniversityPress,2007.
[6] J. Blanchet, K. Murthy, and V. A. Nguyen, Statistical analysis of Wasserstein distributionally robust estimators, in
EmergingOptimizationMethodsandModelingTechniqueswithApplications,INFORMS,2021,pp.227–254.
[7] J. Blanchet, K. Murthy, and N. Si, Confidence regions in Wasserstein distributionally robust estimation, Biometrika,
109(2021),pp.295–315.
[8] T.Bodnar,A.K.Gupta,andN.Parolya,Directshrinkageestimationoflargedimensionalprecisionmatrix,Journalof
MultivariateAnalysis,146(2016),pp.223–236.
[9] S. Boyd and L. Vandenberghe,Convex Optimization,CambridgeUniversityPress,2004.
[10] N. Bui, D. Nguyen, M.-C. Yue, and V. A. Nguyen, Coverage-validity-aware algorithmic recourse, arXiv preprint
arXiv:2311.11349,(2023).
[11] J. E. Dennis Jr and R. B. Schnabel, Numerical Methods for Unconstrained Optimization and Nonlinear Equations,
SIAM,1996.
[12] D.Donoho,M.Gavish,andI.Johnstone,Optimalshrinkageofeigenvaluesinthespikedcovariancemodel,TheAnnals
ofStatistics,46(2018),pp.1742–1778.
[13] V. M. Eguiluz, D. R. Chialvo, G. A. Cecchi, M. Baliki, and A. V. Apkarian, Scale-free brain functional networks,
PhysicalReviewLetters,94(2005),p.018102.
[14] O. P. Ferreira, M. S. Louzeiro, and L. Prudente, Gradient method for optimization on Riemannian manifolds with
lower bounded curvature,SIAMJournalonOptimization,29(2019),pp.2517–2541.
[15] R. Gao, Finite-sample guarantees for Wasserstein distributionally robust optimization: Breaking the curse of dimension-
ality,OperationsResearch,71(2023),pp.2291–2306.
[16] R. Gao, X. Chen, and A. J. Kleywegt, Wasserstein distributionally robust optimization and variation regularization,
OperationsResearch,(2022).
[17] L.E.Ghaoui,M.Oks,andF.Oustry,Worst-casevalue-at-riskandrobustportfoliooptimization: Aconicprogramming
approach,OperationsResearch,51(2003),pp.543–556.
[18] C. R. Givens and R. M. Shortt, A class of Wasserstein metrics for probability distributions., Michigan Mathematical
Journal,31(1984),pp.231–240.
[19] G. H. Hardy, J. E. Littlewood, and G. Po´lya,Inequalities,CambridgeUniversityPress,1952.
[20] T. Hastie, R. Tibshirani, and J. H. Friedman,The Elements of Statistical Learning,Springer,2009.
[21] A. E. Hoerl and R. W. Kennard, Ridge regression: Biased estimation for nonorthogonal problems, Technometrics, 12
(1970),pp.55–67.
[22] R. Jagannathan and T. Ma, Risk reduction in large portfolios: Why imposing the wrong constraints helps, The Journal
ofFinance,58(2003),pp.1651–1683.
[23] W. James and C. Stein,Estimation with quadratic loss,inBreakthroughsinStatistics,Springer,1992,pp.443–460.
[24] H. Jeffreys, An invariant form for the prior probability in estimation problems, Proceedings of the Royal Society of
LondonA,186(1946),pp.453–461.
[25] R.E.Kalman,Anewapproachtolinearfilteringandpredictionproblems,JournalofBasicEngineering,82(1960),pp.35–
45.
[26] H. Komiya,Elementary proof for Sion’s minimax theorem,KodaiMathematicalJournal,11(1988),pp.5–7.
[27] D. Kuhn, P. Mohajerin Esfahani, V. A. Nguyen, and S. Shafieezadeh-Abadeh, Wasserstein distributionally robust
optimization: Theory and applications in machine learning,inOperationsResearch&ManagementScienceintheAgeof
Analytics,INFORMS,2019,pp.130–166.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 43
[28] S. Kullback,Information Theory and Statistics,CourierCorporation,1997.
[29] S. Lang,Fundamentals of Differential Geometry,Springer,2012.
[30] O. Ledoit and M. Wolf, Improved estimation of the covariance matrix of stock returns with an application to portfolio
selection,JournalofEmpiricalFinance,10(2003),pp.603–621.
[31] ,Honey, I shrunk the sample covariance matrix,TheJournalofPortfolioManagement,30(2004),pp.110–119.
[32] ,Awell-conditionedestimatorforlarge-dimensionalcovariancematrices,JournalofMultivariateAnalysis,88(2004),
pp.365–411.
[33] , Nonlinear shrinkage estimation of large-dimensional covariance matrices, The Annals of Statistics, 40 (2012),
pp.1024–1060.
[34] , Nonlinear shrinkage of the covariance matrix for portfolio selection: Markowitz meets Goldilocks, The Review of
FinancialStudies,30(2017),pp.4349–4388.
[35] , Analytical nonlinear shrinkage of large-dimensional covariance matrices, The Annals of Statistics, 48 (2020),
pp.3043–3065.
[36] ,Quadratic shrinkage for large covariance matrices,Bernoulli,28(2022),pp.1519–1547.
[37] J. M. Lee,Riemannian Manifolds: An Introduction to Curvature,Springer,2006.
[38] ,Introduction to Smooth Manifolds,Springer,2013.
[39] V.Lohweg,Banknoteauthenticationdataset.UCIMachineLearningRepository,2013.https://doi.org/10.24432/C55P57.
[40] R. N. Mantegna,Hierarchical structure in financial markets,TheEuropeanPhysicalJournalB,11(1999),pp.193–197.
[41] H. Markowitz,Portfolio selection,TheJournalofFinance,7(1952),pp.77–91.
[42] P. Mohajerin Esfahani and D. Kuhn, Data-driven distributionally robust optimization using the Wasserstein metric:
Performance guarantees and tractable reformulations,MathematicalProgramming,171(2018),pp.115–166.
[43] V.A.Nguyen,D.Kuhn,andP.MohajerinEsfahani,Distributionallyrobustinversecovarianceestimation: TheWasser-
stein shrinkage estimator,OperationsResearch,70(2022),pp.490–515.
[44] V.A.Nguyen,S.Shafieezadeh-Abadeh,D.Filipovic´,andD.Kuhn,Mean-covariance robust risk measurement,arXiv
preprintarXiv:2112.09959,(2021).
[45] V. A. Nguyen, S. Shafieezadeh-Abadeh, D. Kuhn, and P. Mohajerin Esfahani, Bridging Bayesian and minimax
mean square error estimation via Wasserstein distributionally robust optimization, Mathematics of Operations Research,
48(2023),pp.1–37.
[46] V. A. Nguyen, S. Shafieezadeh-Abadeh, M.-C. Yue, D. Kuhn, and W. Wiesemann,Calculating optimistic likelihoods
using (geodesically) convex optimization,inAdvancesinNeuralInformationProcessingSystems,2019,pp.13920–13931.
[47] V. A. Nguyen, S. Shafieezadeh Abadeh, M.-C. Yue, D. Kuhn, and W. Wiesemann,Optimistic distributionally robust
optimization for nonparametric likelihood approximation, in Advances in Neural Information Processing Systems, 2019,
pp.13942–13953.
[48] K. Pearson, Note on regression and inheritance in the case of two parents, Proceedings of the Royal Society of London,
58(1895),pp.240–242.
[49] M. Perlman,STAT 542: Multivariate Statistical Analysis,LectureNotes,UniversityofWashington,Seattle,(2007).
[50] K. B. Petersen,The Matrix Cookbook,2012.
[51] B.RajaratnamandD.Vincenzi,AtheoreticalstudyofStein’scovarianceestimator,Biometrika,103(2016),pp.653–666.
[52] R. Rockafellar,Convex Analysis,PrincetonUniversityPress,1997.
[53] S.Shafieezadeh-Abadeh,L.Aolaritei,F.Do¨rfler,andD.Kuhn,Newperspectivesonregularizationandcomputation
in optimal transport-based distributionally robust optimization,arXivpreprintarxiv.2303.03900,(2023).
[54] S. Shafieezadeh-Abadeh, D. Kuhn, and P. Mohajerin Esfahani, Regularization via mass transportation, Journal of
MachineLearningResearch,20(2019),pp.1–68.
[55] S. Shafieezadeh-Abadeh, V. A. Nguyen, D. Kuhn, and P. Mohajerin Esfahani, Wasserstein distributionally robust
Kalman filtering,inAdvancesinNeuralInformationProcessingSystems31,2018,pp.8483–8492.
[56] W. F. Sharpe,A simplified model for portfolio analysis,ManagementScience,9(1963),pp.277–293.
[57] C. Stein, Estimation of a covariance matrix, Rietz Lecture, in 39th Annual Meeting IMS, Institute of Mathematical
Statistics,1975.
[58] B.Ta¸skesen,D.Iancu,C¸.Koc¸yig˘it,andD.Kuhn,Distributionallyrobustlinearquadraticcontrol,inAdvancesinNeural
InformationProcessingSystems,2023,pp.18613–18632.
[59] B. Taskesen, M.-C. Yue, J. Blanchet, D. Kuhn, and V. A. Nguyen, Sequential domain adaptation by synthesizing
distributionally robust experts,inInternationalConferenceonMachineLearning,PMLR,2021,pp.10162–10172.
[60] R.Taylor,Interpretationofthecorrelationcoefficient: Abasicreview,JournalofDiagnosticMedicalSonography,6(1990),
pp.35–39.
[61] A. Touloumis,Nonparametric Stein-type shrinkage covariance matrix estimators in high-dimensional settings,Computa-
tionalStatistics&DataAnalysis,83(2015),pp.251–261.
[62] C. Udriste,Convex Functions and Optimization Methods on Riemannian Manifolds,Springer,2013.
[63] H. R. van der Vaart, On certain characteristics of the distribution of the latent roots of a symmetric random matrix
under general conditions,TheAnnalsofMathematicalStatistics,32(1961),pp.864–873.A GEOMETRIC UNIFICATION OF DISTRIBUTIONALLY ROBUST COVARIANCE ESTIMATORS 44
[64] W. N. van Wieringen,Lecture Notes on Ridge Regression,arXivpreprintarXiv:1509.09169,(2015).
[65] C. Villani,Optimal Transport: Old and New,Springer,2008.
[66] H.Vu,T.Tran,M.-C.Yue,andV.A.Nguyen,Distributionallyrobustfairprincipalcomponentsviageodesicdescents,
inProceedingsofthe10thInternationalConferenceonLearningRepresentations,2022.
[67] M. J. Wainwright,High-Dimensional Statistics: A Non-Asymptotic Viewpoint,CambridgeUniversityPress,2019.
[68] W.Wolberg,O.Mangasarian,N.Street,andW.Street,BreastcancerWisconsin(diagnostic)dataset.UCIMachine
LearningRepository,1992.https://doi.org/10.24432/C5DW2B.
[69] M.-C. Yue, A matrix generalization of the Hardy-Littlewood-Po´lya rearrangement inequality and its applications, arXiv
preprintarXiv:2006.08144,(2020).
[70] P. Zhang, J. Zhang, and S. Sra, Minimax in geodesic metric spaces: Sion’s theorem and algorithms, arXiv preprint
arXiv:2202.06950,(2022).
[71] M. Zorzi, Robust Kalman filtering under model perturbations, IEEE Transactions on Automatic Control, 62 (2017),
pp.2902–2907.