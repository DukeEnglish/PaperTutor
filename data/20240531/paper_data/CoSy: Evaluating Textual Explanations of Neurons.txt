CoSy
: Evaluating Textual Explanations of Neurons
Laura Kopf1,2 Philine Lou Bommer1,3 Anna Hedström1,3,4 Sebastian Lapuschkin4
Marina M.-C. Höhne1,2 Kirill Bykov1,3,5
1UMI Lab, ATB Potsdam, Germany 2University of Potsdam, Germany 3TU Berlin, Germany
4Fraunhofer Heinrich-Hertz-Institute, Germany 5BIFOLD, Germany
{lkopf,pbommer,ahedstroem,mhoehne,kbykov}@atb-potsdam.de
sebastian.lapuschkin@hhi.fraunhofer.de
Abstract
A crucial aspect of understanding the complex nature of Deep Neural Networks (DNNs) is the ability
to explain learned concepts within their latent representations. While various methods exist to con-
nect neurons to textual descriptions of human-understandable concepts, evaluating the quality of these
explanation methods presents a major challenge in the field due to a lack of unified, general-purpose
quantitative evaluation. In this work, we introduce CoSy (Concept Synthesis)—a novel, architecture-
agnostic framework to evaluate the quality of textual explanations for latent neurons. Given textual
explanations,ourproposedframeworkleveragesagenerativemodelconditionedontextualinputtocre-
ate data points representing the textual explanation. Then, the neuron’s response to these explanation
data points is compared with the response to control data points, providing a quality estimate of the
given explanation. We ensure the reliability of our proposed framework in a series of meta-evaluation
experimentsanddemonstratepracticalvaluethroughinsightsfrombenchmarkingvariousconcept-based
textual explanation methods for Computer Vision tasks, showing that tested explanation methods sig-
nificantly differ in quality. We provide an open-source implementation on GitHub1.
1 Introduction
One of the key obstacles to the wider adoption of Machine Learning methods in various areas is the inher-
ent opacity of modern Deep Neural Networks (DNNs)—in simple terms, we do not understand why these
machines make the predictions they do. To address this problem, the field of Explainable AI (XAI) [1, 2]
has emerged, to reveal the decision-making processes of DNNs in a human-understandable fashion. XAI
has broadened its focus from explaining the decision-making of DNNs locally, i.e., for specific inputs using
saliencymaps[3,4,5,6],toexplainingtheglobal behaviorofthemodelsbyanalyzingindividualmodelcom-
ponents and their functional purpose [7]. Following the latter global explainability approach, often referred
to as mechanistic interpretability [8, 9, 10], there are methods that aim to describe the specific concepts
neurons have learned to detect [11, 12, 13, 14, 15, 16], enabling analysis of how these high-level concepts
influence network predictions.
A popular approach for explaining the functionality of latent representations of a network is to label neu-
rons using human-understandable textual concepts. A textual description is assigned to a neuron based on
the concepts that the neuron has learned to detect or is significantly activated by. Over time, these meth-
ods have evolved from providing label-specific descriptions [11] to more complex compositional [12, 16] and
open-vocabularyexplanations[13,15]. However,asignificantchallengeremains: thelackofauniversallyac-
ceptedquantitativeevaluationmeasureforopen-vocabularyneurondescriptions. Asaconsequence,different
methodsdevisedtheirownevaluationcriteria,makingitdifficulttoperformgeneral-purpose,comprehensive
cross-comparisons.
With our work, we aim to bridge this gap by introducing a novel quantitative evaluation framework named
CoSy,forevaluatingopen-vocabularyexplanationsforneuronsinComputerVision(CV)models(illustrated
1https://github.com/lkopf/cosy
1
4202
yaM
03
]GL.sc[
1v13302.5042:viXraCoSy: Evaluating Textual Explanations of Neurons Kopf et al.
Figure 1: A schematic illustration of the CoSy evaluation framework for Neuron 80 in ResNet18’s avgpool layer.
The current challenge lies in the absence of dataset- and architecture-agnostic evaluation measures to benchmark
textual explanations of neurons. To address this, we propose CoSy, a framework consisting of three steps: first, a
generative model translates textual concepts into the visual domain, creating synthetic images for each explanation
usingatext-to-imagemodel. Then,inferenceisperformedonsyntheticimages,alongwithacontrolimagedataset,to
collectneuronactivations. Finally,bycomparingtheactivationsonsyntheticimageswithactivationsonthecontrol
dataset,wecanquantitativelyassessthequalityofthetextualexplanationandcomparetheresultsbetweendifferent
explanation methods. The implementation details of this example can be found in Appendix A.2.
in Figure 1). Our approach builds on recent advancements in Generative AI, which enable the generation
of synthetic images that align with provided concept-based textual explanations. We use a set of available
text-to-image models to synthesize data points that are prototypical for specific target explanations. These
data points allow us to evaluate how neurons differentiate between concept-related images and those from a
control dataset. We summarize our contributions as below:
(C1) Weprovidethefirstgeneral-purpose,quantitativeevaluationframeworkCoSy(Section3)thatenables
the evaluation of individual or a set of textual explanation methods for CV models.
(C2) In a series of meta-evaluation experiments (Section 4), we analyze the choice of generative models and
prompts for synthetic image generation, demonstrating framework reliability.
(C3) Webenchmarkexistingexplanationmethods(Section5)andextractnovelinsights, revealingsubstan-
tial variability in the quality of explanations. Generally, textual explanations for lower layers are less
accurate compared to those for higher layers.
2 Related Work
Activation Maximization Activation Maximization is a commonly used methodology to understand
what a neuron has learned to detect [17]. Such methods work by identifying input signals that trigger the
highestactivationinaneuron. Thiscanbeachievedsynthetically,whereanoptimizationprocessisemployed
to create the optimal input that maximizes the neuron’s activation [18, 19, 20], or naturally, by finding
such inputs within a data corpus [21]. Activation Maximization has been employed for explaining latent
representations of models [22, 23], including probabilistic models [24], detection of backdoor attacks [25]
and spurious correlations [26]. Recently, it was shown that Activation Maximization methods could be
manipulated to illustrate predetermined signals without significantly affecting performance [27, 28, 29].
However, one of the key limitations of this methodology lies in its inability to scale; its scalability is limited
due to its dependency on users to manually audit maximization signals.
2CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
Table 1: Comparison of characteristics of concept-based textual explanation methods. The columns (from left to
right)representtheexplanationmethodused,itstextualoutputtype(fixed-label,compositional,oropen-vocabulary),
the type of neuron targeted for analysis (convolutional, scalar, or predetermined), the target metric the method
optimizes(IoU,WPMI,AUC,etc.),whetherthemethodreliesonauxiliaryblack-boxmodelsforfindingorgenerating
explanations(img2txt,CLIP),andwhethertheexplanationmethodisarchitecture-agnostic,meaningitcanbeapplied
to any CV model. For a more detailed description of each method, refer to Appendix A.1.
Method Explanation NeuronType Target Black-BoxDependency Architecture-Agnostic
NetDissect[11] fixed-label conv. IoU — ✓
CompExp[12] compositional conv. IoU — ✓
MILAN[13] open-vocabulary conv. WPMI img2txtmodel ✓
INVERT[16] compositional scalar AUC — ✓
CLIP-Dissect[15] open-vocabulary scalar SoftWPMI CLIP ✓
FALCON[14] open-vocabulary predetermined avg. CLIPscore CLIP —
Automatic Neuron Interpretation Amorescalablealternativeapproachinvolveslinkingneuronswith
human-understandable concepts through textual descriptions. Network Dissection [11] (NetDissect) is a pi-
oneering method in this field, associating convolutional neurons with a concept based on the Intersection
over Union (IoU) of neuron activation maps and ground truth segmentation masks. Building on this, Com-
positional Explanations of Neurons (CompExp) [12] enhanced the detail of the explanations by allowing
compositional concepts (i.e., concepts constructed using logical operators). MILAN [13] further expanded
this by allowing for open-vocabulary explanations, permitting the generation of descriptions beyond pre-
defined labels. INVERT [16] adopted a compositional concept approach, enabling explanations for general
neuron types without relying on segmentation masks, and assigns compositional labels based on a neuron’s
ability to distinguish concepts using the Area Under the Receiver Operating Characteristic Curve (AUC).
FALCON [14] and CLIP-Dissect [15] compute image-text similarity with a CLIP model [30] for the most
activating images and their corresponding captions or concept sets. An overview of the different techniques
is illustrated in Table 1. More detailed descriptions of these methods can be found in Appendix A.1.
Prior Methods for Evaluation While significant effort has been made towards developing approaches
and tools for evaluating local explanations [31, 32, 33], there has been relatively limited focus on evaluating
global methods. Currently, to the best of our knowledge, there is no unified approach that allows for
benchmarking across models and explanation methods. In their respective papers, the INVERT and CLIP-
Dissectexplanationmethodsevaluatedtheaccuracyoftheirexplanationsbycomparingthegeneratedneuron
labels with ground truth descriptions provided for neurons in the output layer of a network. CLIP-Dissect
additionallyevaluatesthequalityofexplanationsbycomputingthecosinesimilarityinasentenceembedding
space between the ground truth class name for each neuron and the explanation generated by the method.
FALCONemploysahumanstudyconductedonAmazonMechanicalTurktoevaluatetheconceptsgenerated
by the method. Participants are tasked with selecting the best explanation for each target feature from a
selection of explanation methods, considering a given set of highly and lowly activating images. MILAN
evaluatestheperformanceofneuronlabelingmethodsrelativetohumanannotationsusingBERTScores[34].
3 Method
In the following section, we introduce CoSy—a first automatic evaluation procedure for open-vocabulary
textual explanations for neurons. We first define preliminary notations in Section 3.1, then describe CoSy
formally in Section 3.2.
3CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
3.1 Preliminaries
ConsideraDeepNeuralNetwork(DNN)representedbythefunctiong :X →Z,whereX ⊂Rh×w×c denotes
the input image domain and Z ⊂ Rl represents the model’s output domain. We can view the model as a
composition of two functions, F : X → Y, and L : Y → Z, such that g = L◦F. Here Y ⊂ Rk×w∗×h∗,
where k ∈ N is the number of neurons in the layer, and w∗,h∗ ∈ N represent the width and height of the
feature map, respectively. The function F, which we refer to as the feature extractor, can be chosen based
on the layer of the model we aim to inspect. This could be an existing layer within the model or a concept
bottleneck layer [35]. We refer to the i-th neuron within the layer as f (x) = F (x) : X → Rw∗×h∗. Within
i i
the scope of this paper, we refer to explanation method as an operator E that maps a neuron to the textual
descriptions=E(f )∈S,whereS isasetofpotentialtextualexplanations. Thespecificsetofexplanations
i
depends on the implementation of the particular method.
3.2 CoSy: Evaluating Open-Vocabulary Explanations
Agoodtextualexplanationforaneuronshouldbeahuman-understandabledescriptionofaninputthatyields
a high level of activation in the neuron. However, modern methods for explaining the functional purpose
of neurons often provide open-vocabulary textual explanations, complicating the quantitative collection of
natural data that represents the explanation. To address this issue, CoSy leverages recent advancements in
generative models to synthesize data points that correspond to the textual explanation. The response of a
neuron to a set of synthetic images is measured and compared to the neuron’s activation on a set of control
natural images representing random concepts. This comparison allows for a quantitative evaluation of the
alignment between the explanation and the target neuron.
Parameters of the proposed method include a control dataset X = {x0,...,x0} ⊂ X,n ∈ N – containing
0 1 n
naturalimagesthatrepresenttheconceptsthemodelwasoriginallytrainedon,agenerativemodelp ,that
M
isusedforsynthesizingimages,andanumberofgeneratedimagesm∈N.Givenaneuronf andexplanation
s ∈ S, CoSy evaluates the alignment between the explanation and a neuron in 3 consecutive steps, which
are illustrated in Figure 1.
1. GenerateSyntheticData. Thefirststepinvolvesgeneratingsyntheticimagesforagivenexplanation
s∈S, which we use as a prompt to a generative model p to create a collection of synthetic images,
M
denoted as X = {x1,...,x1 } ∼ p (x | s). This collection consists of m ∈ N images, where m is
1 1 m M
adjustable as a parameter of the evaluation procedure.
2. CollectNeuronActivations. GiventhecontroldatasetX andthesetofgeneratedsyntheticimages
0
X , we collect activations as follows:
1
A ={σ(f(x0)),...,σ(f(x0))}∈Rn,A ={σ(f(x1)),...,σ(f(x1 ))}∈Rm, (1)
0 1 n 1 1 m
where σ is an aggregation function for multi-dimensional neurons. Within the scope of our paper, we
use Average Pooling as aggregation function
σ(y)= 1 (cid:88) (cid:88) y , y ∈Y ⊂Rw∗×h∗ . (2)
w∗h∗ i,j
i∈[1,w∗]i∈[1,h∗]
3. Score Explanations. Thefinalstepoftheproposedmethodreliesontheevaluationofthedifference
betweenneuronactivationsonthecontroldatasetA andneuronactivationsgiventhesyntheticdataset
0
A .Toquantifythisdifference,weutilizeascoringfunction Ψ:Rn×Rm →Rtomeasurethedifference
1
between the distributions of activations.
In the context of our paper, we employ the following scoring functions:
4CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
• Area Under the Receiver Operating Characteristic (AUC)
AUC is a widely used non-parametric evaluation measure for assessing the performance of binary
classification. Inourmethod, AUCmeasurestheneuron’sabilitytodistinguishbetweensyntheticand
control data points
(cid:80) (cid:80)
1[a<b]
Ψ (A ,A )= a∈A 0 b∈A 1 . (3)
AUC 0 1 |A |·|A |
0 1
• Mean Activation Difference (MAD)
MADisaparametricmeasurethatquantifiesthedifferencebetweenthemeanactivationoftheneuron
on synthetic images and the mean activation on control data points
1 (cid:88) 1 (cid:88)
Ψ (A ,A )= b− a. (4)
MAD 0 1 m n
b∈A
1
a∈A
0
These two chosen metrics complement each other. AUC, being non-parametric and stable to outliers, evalu-
ates the classifier’s ability to rank synthetic images higher than control images (with scores ranging from 0
to 1, where 1 represents a perfect classifier and 0.5 is random). On the other hand, MAD allows us to para-
metrically measure the extent to which images corresponding to explanations maximize neuron activation.
4 Meta-Evaluation Analysis
Meta-evaluation is the practice of evaluating the evaluation method itself [36]. This process is crucial to
ensure the reliability of our proposed evaluation measure. In this section, we analyze the following: (1)
whichgenerativemodelsandpromptsprovidethebestsimilaritytonaturalimages, (2)whetherthemodel’s
behavioronsyntheticandnaturalimagesdiffersforthesameconcept,and(3)validatingthatCoSyprovides
appropriate evaluation scores for true and random explanations, given known ground truth concept for the
neuron.
4.1 Synthetic Image Reliability
One of the key features of CoSy is its reliance on generative models to translate textual explanations
of neurons into the visual domain. Thus, it is essential that the generated images reliably resemble the
textual concepts. In the following section, we present an experiment where we varied several parameters of
the generation procedure and evaluated the visual similarity between generated images and synthetic ones,
focusing on concepts for which we have a collection of natural images.
Forouranalysis,weusedonlyopen-sourceandfreelyavailabletext-to-imagemodels,namelyStableDiffusion
XL 1.0-base (SDXL) [37] and Stable Cascade (SC) [38]. We also varied the prompts for image generation.
To measure the similarity between synthetic images and natural images corresponding to the same concept,
we employed cosine similarity (CS) in the CLIP embedding space with the CLIP-ViT-B/32 model [30]. We
select a set of 10 random concepts from the 1,000 classes in the ImageNet validation dataset [39]. For each
[concept]weuse5differentpromptsandemploythemwithSDXLandSCmodels, generating50imagesper
concept. We then measure the CS between image pairs of the same class.
Figure 2 illustrates the comparison across all generative models and prompts in terms of CS of generated
imagestonaturalimagesofthesameclass. Theresultsindicatethatwhenusingprompt5asinputtoSDXL,
the synthetic images show the highest similarity to natural images. The performance is generally best with
the most detailed prompt (5) and closely aligns with prompts 1, 3, and 4. Moreover, SDXL appears to be
slightly more effectively realizing detailed prompts than SC. As anticipated, the poorly constructed prompt
(2)resultsinthelowestsimilaritytonaturalimagesforbothmodels. Ifnotstatedotherwise,forallfollowing
experiments, prompt 5 together with SDXL model was employed for image generation.
5CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
Figure 2: An overview of the impact of varying the prompt on the similarity between natural and synthetic images,
using two text-to-image models. Left: average Cosine similarity (CS) across all natural and synthetic images over
all classes are reported. Higher CS values are better, indicating greater similarity between the images. Right: an
illustration of the visual differences produced by the SDXL and SC models in response to diverse prompts for the
explanation concept “submarine”, and natural images from the ImageNet validation set. Our results show that SDXL
and SC generate similar images, with SDXL generally being more closely aligned with natural images than SC.
4.2 Do Models Respond Differently to Synthetic and Natural Images?
Giventhevisualsimilaritybetweennaturalandsyntheticimagesofthesameconcepts,weinvestigatewhether
CVmodelsresponddifferentlytothesegroupsandiftheactivationdifferencesindicateadversarialbehavior.
To this end, we employed four different models pre-trained on ImageNet: ResNet18 [40], DenseNet161 [41],
GoogleNet [42], and ViT-B/16 [43]. For each model, we randomly selected 10 output classes and generated
50 images per class using the class descriptions. We pass both synthetic and natural images through the
models, collecting the activations of the output neuron corresponding to each class.
Figure 3 (left) illustrates the distributions of the MAD between synthetic and natural images for the same
class across the 10 classes. Generally, we observe that the activation of synthetic images is slightly higher
than that of natural images of the same class. However, this difference is small, given the 0 value lies within
1 standard deviation. We also illustrated (Figure 3, right) the activations of neuron 504 in the ResNet18
output layer, corresponding to the “coffee mug” class. The results indicate a strong overlap in the neural
response to both synthetic and natural images. While synthetic images activate the neuron slightly more,
Figure 3: An overview of analyses performed to study the similarity between natural and synthetic images. From
lefttoright: (1)anoverviewofMADscoresbetweensyntheticandnaturalimageactivationsoftheoutputneuron’s
ground truth classes for each model studied in this work, (2) activations collected for neuron 504 in ResNet18 for
the class “coffee mug”, showcasing the difference between the natural and synthetic distributions and (3) examples
of natural versus synthetic images. In both analyses, we observe a substantial overlap in the activations of synthetic
and natural images, suggesting that the models respond similarly to both types of images.
6CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
Table 2: Comparison of true and random explanations on output neurons with known ground truth labels. This
tablepresentsthequalityestimatesfortrueexplanations,derivedfromtargetclasslabels,andrandomexplanations,
derivedfromrandomlyselectedsyntheticimageclasses(excludingthetargetclass),acrossfourmodelspre-trainedon
ImageNet. Higher values are better. Our results consistently show high scores for true explanations and low scores
for random ones.
AUC (↑) MAD (↑)
Model
True Random True Random
ResNet18 0.94±0.20 0.48±0.23 15.95±6.88 -0.53±2.00
DenseNet161 0.95±0.17 0.52±0.24 15.27±5.88 -0.13±1.55
GoogLeNet 0.95±0.16 0.40±0.19 8.98±3.48 -0.41±0.57
ViT-B/16 0.97±0.11 0.52±0.16 7.55±2.69 -0.00±0.21
thisdoesn’tconstituteanartifactualbehaviororaffectourframework,whichwedemonstrateinthefollowing
experiment.
4.3 Sanity Check
A robust evaluation metric should reliably discern between random explanations resulting in low scores
and non-random explanations resulting in high scores. To assess our evaluation framework regarding this
requirement, we evaluated the results of the CoSy evaluation by comparing the scores of ground truth
explanations with those of randomly selected explanations.
Following the experimental setup in Section 4.2, we selected a set of 10 output neurons and compared
the CoSy scores of the ground truth explanations, given by the neuron label, with those of randomly
selected explanations. The results, presented in Table 2, consistently demonstrate high scores for true
explanationsandlowscoresforrandomexplanations. Thisexperimentprovidesfurtherevidencesupporting
the correctness of the proposed evaluation procedure. Additional experiments, including an analysis of the
robustness of the evaluation measure, can be found in Appendix A.6.
5 Evaluating Explanation Methods
Within the scope of this section, we produce a comprehensive cross-comparison of various methods for the
textualexplanationsofneurons. Forthiscomparison,weemployedmodelstrainedondifferentdatasets,and
we conducted our analysis on the latent layers of the models, where no ground truth is known.
5.1 Benchmarking Explanation Methods
In this section, we evaluated three recent textual explanation methods, namely MILAN, INVERT, and
CLIP-Dissect. Our analysis involves four distinct models: two pre-trained on the ImageNet dataset [39]
(ResNet18 [40], ViT-B/16 [43]) and two pre-trained on the Places365 dataset [44] (DenseNet161 [41],
ResNet50 [40]). The ImageNet dataset focuses on objects, whereas the Places365 dataset is designed for
scenerecognition. Consequently,wecustomizedourpromptsaccordingly: Prompt5performsbestforobject
recognition, while for scene recognition, we found that Prompt 4 is more effective. Therefore, Prompt 4 was
utilized in the Places365 experiment. For generating explanations with the explanation methods, we use a
subset of 50,000 images from the training dataset on which the models were trained. For evaluation with
CoSy, we use the corresponding validation datasets the models were pre-trained on as the control dataset.
Additionally,forCLIP-Dissect,wedefineconceptlabelsasacombinationofthe20,000mostcommonEnglish
wordsandthecorrespondingdatasetlabels. FormoredetailsonComputeResourcesrefertoAppendixA.3.
7CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
Table3: Benchmarkingofexplanationmethods,explainingneuronsonthesecondtolastlayersfordifferentmodels.
Explanations are generated with respect to a randomly selected set of 50 neurons where both AUC and MAD are
reported. Higher values are better.
Dataset Model Layer Method AUC (↑) MAD (↑)
MILAN 0.61±0.23 0.62±1.23
ResNet18 Avgpool INVERT 0.93±0.11 2.88±1.54
CLIP-Dissect 0.93±0.11 3.45±1.71
ImageNet
MILAN 0.53±0.19 0.08±0.45
ViT-B/16 Features INVERT 0.89±0.17 0.97±0.46
CLIP-Dissect 0.78±0.19 0.76±0.62
MILAN 0.56±0.28 0.08±0.27
DenseNet161 Features INVERT 0.85±0.16 0.39±0.94
CLIP-Dissect 0.82±0.21 0.41±1.04
Places365
MILAN 0.65±0.28 0.35±0.53
ResNet50 Avgpool INVERT 0.94±0.08 1.04±0.74
CLIP-Dissect 0.92±0.11 1.05±0.75
Results of the evaluation can be found in Table 3. Overall, INVERT achieves the highest AUC scores
acrossallmodelsanddatasets, exceptfortheResNet18appliedtoImageNetwhereCLIP-Dissectachievesa
similar score. Also across other models and datasets, CLIP-Dissect demonstrates consistently good results.
Since INVERT optimizes AUC in explanation generation, it may be biased towards AUC in our evaluation,
leading to higher scores. MILAN generally performs poorly, with an average AUC below 0.65 across all
tasks, indicating performance close to random guessing. This is somewhat expected since MILAN works
with convolutional neurons. MILAN tends to generate highly abstract explanations, such as “white areas”,
“nothing” or “similar patterns”. These abstract concepts are particularly challenging for a text-to-image
model to generate accurately, likely contributing significantly to the low scores of MILAN. Contrary to
the AUC scores, the MAD scores suggest that CLIP-Dissect outperforms INVERT for convolutional neural
networks applied to both datasets. Nonetheless, in these cases, INVERT concepts also achieve consistently
high scores. Otherwise, we find similar outcomes for both metrics Ψ, with MILAN achieving poor scores in
all experimental settings.
5.2 Explanation Methods Struggle to Explain Lower Layer Neurons
Inadditiontothegeneralbenchmarking,weaimedtostudythequalityofexplanationsforneuronsindifferent
layers of a model. Since it is well known that lower-layer neurons usually encode lower-level concepts [45],
it is interesting to see whether explanation methods can capture the concepts these neurons detect. To
investigate this, we examined the quality of explanations across layers 1 to 4 and the output layer of an
ImageNet pre-trained ResNet18. In addition to three prior explanation methods, we included the FALCON
method in our analysis. For more details on the implementation of FALCON and FALCON-original see
Appendix A.1.4. For each layer, we randomly selected 50 neurons for analysis.
In Figure 4 we present the AUC (left) and MAD (right) results for all explanation methods across layers
1 to 4 and the output layer of ResNet18. While less pronounced for the AUC metric, in general, we find
increasingscoresforlaterlayersacrossallmethodsandbothmetricsΨ,whichsuggesthigherconceptquality
in later layers. Furthermore, we find that similar to the benchmarking experiments, MILAN achieves lower
scores across metrics. However, here, FALCON scores the lowest, not even surpassing random performance
indicated by its AUC results (AUC<0.5).
8CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
Figure 4: A comparison of how different explanation methods vary in their quality, as measured by AUC (left) and
MAD (right), across different layers in ResNet18. INVERT and CLIP-Dissect similarly high AUC and MAD scores
acrossalllayers,whileMILAN,FALCON,andFALCON-originalscoresarecomparablylow. Generally,allmethods
perform increasingly worse on lower layers.
5.3 What are Good Explanations?
In our approach, we assume that testing visual representations of textual explanations on neurons can
provideinsightsintowhatconstitutesgoodexplanations. Basedonthisassumption,weobserveconsistently
high results from CLIP-Dissect and INVERT. The qualitative examples in Figure 5 demonstrate that their
explanations share visually similar concepts (neurons 155 and 459) or even identical concepts (neuron 221)
while both achieving high AUC and MAD scores. It is important to note that although INVERT performs
slightly better in several tasks, its applicability is limited to input data labels. In contrast, CLIP-Dissect
can generate labels from a broader selection of concepts, though its reliance on a black-box model reduces
interpretability compared to INVERT.
Thereareinstances,suchasneuron260inFigure5,whereallexplanationsvarysignificantly. Inthesecases,
we find that the explanation activation distributions of FALCON and MILAN often overlap with or even
matchthecontroldataset,providingtheuserwithnearlyrandomexplanations. Thisobservationalignswith
our overall findings: both the AUC and MAD scores consistently indicate the low performance of FALCON
and MILAN explanations in CoSy evaluation. Also, neurons 459 and 155 demonstrate the gap between
consistentlyhigherandlower-performingexplanationmethods,withAUCscoresof0.5andbelowsuggesting
that these explanations are essentially random guesses.
6 Conclusion
In this work, we propose the first automatic evaluation framework for concept-based textual explanations of
neurons. Unlike existing ad-hoc evaluation methods, we can now quantitatively compare different concept-
based textual explanation methods against each other and test, whether the given explanation describes the
neuron accurately, based on the neurons’s activations. We can evaluate the quality of individual neuron
explanations by examining how accurately they align with the generated concept data points, without
requiring human involvement.
Our comprehensive meta-evaluation demonstrates that CoSy guarantees a reliable explanation evaluation.
Inseveralexperiments,weshowthatconcept-basedtextualexplanationmethodsaremostapplicableforthe
last layers, where high-level concepts are learned. In these layers, INVERT and CLIP-Dissect provide high-
quality neuron concepts, whereas MILAN and FALCON explanations have lower quality and can present
close to random concepts, which might lead to wrong conclusions about the network. Thus, the results
highlight the importance of evaluation when using concept-based textual explanation methods.
9CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
Figure 5: A qualitative example, of neuron explanations across 4 neurons. The first four panels include the textual
explanationacrossINVERT,FALCON,CLIP-Dissect,andMILANalongsidethreecorrespondinggeneratedimages.
TherespectiveAUCandMADscoresarereportedbeloweachpanel. Thelastpanelshowstheactivationdistributions
across 50 generated images for each method and the distribution of the control data.
Limitations While we can present promising results, one of the key limitations of CoSy is the generative
model. For example, the text-to-image model training might not include the generated concepts. This
absence leads to worsened generative performance but could be circumvented by an analysis of pre-training
datasets and model performance. Moreover, the model’s capabilities of generating highly abstract concepts
like “white objects” are limited. In both cases, exploring more sophisticated, specialized, or constrained
models could help.
Future Work Evaluationofnon-localexplanationmethodsisstillalargelyneglectedresearcharea,where
CoSyplaysanimportantyetpreliminarypart. Inthefuture,weneedadditional,complementarydefinitions
of explanation quality that extend our precise definition of AUC and MAD, e.g., that involve humans to
assessplausibility[46]orevaluateexplanationqualityviathesuccessofadownstreamtask[47]. Furthermore,
we plan to extend the application of our evaluation framework to additional domains including NLP and
healthcare. In particular, it would be interesting to analyze the quality of more recent autointerpretable
explanation methods given by highly opaque, large language models (LLMs) [48, 9]. Also, we believe that
applying CoSy to healthcare datasets, where the quality of the explanation really matter, is an impactful
next step.
Acknowledgements
This work was partly funded by the German Ministry for Education and Research (BMBF) through the
project Explaining 4.0 (ref. 01IS200551). Additionally, this work was supported by the European Union’s
HorizonEuroperesearchandinnovationprogramme(EUHorizonEurope)asgrantTEMA(101093003); the
European Union’s Horizon 2020 research and innovation programme (EU Horizon 2020) as grant iToBoS
(965221); andthestateofBerlinwithintheinnovationsupportprogrammeProFIT(IBB)asgrantBerDiBa
(10174498).
10CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
References
[1] WojciechSamekandKlaus-RobertMüller.“Towardsexplainableartificialintelligence”.In:Explainable
AI: interpreting, explaining and visualizing deep learning (2019), pp. 5–22.
[2] Feiyu Xu et al. “Explainable AI: A brief survey on history, research areas, approaches and challenges”.
In:Natural Language Processing and Chinese Computing: 8th CCF International Conference, NLPCC
2019, Dunhuang, China, October 9–14, 2019, Proceedings, Part II 8. Springer. 2019, pp. 563–574.
[3] Sebastian Bach et al. “On pixel-wise explanations for non-linear classifier decisions by layer-wise rele-
vance propagation”. In: PloS one 10.7 (2015), e0130140.
[4] K Simonyan, A Vedaldi, and A Zisserman. “Deep inside convolutional networks: visualising image
classification models and saliency maps”. In: Proceedings of the International Conference on Learning
Representations (ICLR). ICLR. 2014.
[5] RamprasaathRSelvarajuetal.“Grad-cam:Visualexplanationsfromdeepnetworksviagradient-based
localization”. In: Proceedings of the IEEE international conference on computer vision. 2017, pp. 618–
626.
[6] DanielSmilkovetal.“Smoothgrad:removingnoisebyaddingnoise”.In:arXivpreprintarXiv:1706.03825
(2017).
[7] Chris Olah et al. “Zoom in: An introduction to circuits”. In: Distill 5.3 (2020), e00024–001.
[8] Kevin Ro Wang et al. “Interpretability in the Wild: a Circuit for Indirect Object Identification in
GPT-2 Small”. In: The Eleventh International Conference on Learning Representations. 2022.
[9] Steven Bills et al. Language models can explain neurons in language models. https://openaipublic.
blob.core.windows.net/neuron-explainer/paper/index.html. 2023.
[10] Neel Nanda et al. “Progress measures for grokking via mechanistic interpretability”. In: The Eleventh
International Conference on Learning Representations. 2022.
[11] David Bau et al. “Network Dissection: Quantifying Interpretability of Deep Visual Representations”.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). July
2017.
[12] Jesse Mu and Jacob Andreas. “Compositional Explanations of Neurons”. In: Advances in Neural In-
formation Processing Systems. Ed. by H. Larochelle et al. Vol. 33. Curran Associates, Inc., 2020,
pp. 17153–17163. url: https://proceedings.neurips.cc/paper_files/paper/2020/file/
c74956ffb38ba48ed6ce977af6727275-Paper.pdf.
[13] EvanHernandezetal.“NaturalLanguageDescriptionsofDeepFeatures”.In:InternationalConference
on Learning Representations. 2022. url: https://openreview.net/forum?id=NudBMY-tzDr.
[14] Neha Kalibhat et al. “Identifying Interpretable Subspaces in Image Representations”. In: Proceedings
of the 40th International Conference on Machine Learning. Ed. by Andreas Krause et al. Vol. 202.
Proceedings of Machine Learning Research. PMLR, 23–29 Jul 2023, pp. 15623–15638. url: https:
//proceedings.mlr.press/v202/kalibhat23a.html.
[15] Tuomas Oikarinen and Tsui-Wei Weng. “CLIP-Dissect: Automatic Description of Neuron Representa-
tions in Deep Vision Networks”. In: International Conference on Learning Representations (2023).
[16] Kirill Bykov et al. “Labeling Neural Representations with Inverse Recognition”. In: Thirty-seventh
Conference on Neural Information Processing Systems. 2023.
[17] Dumitru Erhan et al. “Visualizing higher-layer features of a deep network”. In: University of Montreal
1341.3 (2009), p. 1.
[18] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. “Feature visualization”. In: Distill 2.11
(2017), e7.
[19] AnhNguyenetal.“Synthesizingthepreferredinputsforneuronsinneuralnetworksviadeepgenerator
networks”. In: Advances in neural information processing systems 29 (2016).
11CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
[20] Thomas Fel et al. “Unlocking Feature Visualization for Deep Network with MAgnitude Constrained
Optimization”. In: Thirty-seventh Conference on Neural Information Processing Systems. 2023. url:
https://openreview.net/forum?id=J7VoDuzuKs.
[21] JudyBorowskietal.“Naturalimagesaremoreinformativeforinterpretingcnnactivationsthanstate-
of-the-art synthetic feature visualizations”. In: NeurIPS 2020 Workshop SVRHM. 2020.
[22] Gabriel Goh et al. “Multimodal neurons in artificial neural networks”. In: Distill 6.3 (2021), e30.
[23] Naoya Yoshimura, Takuya Maekawa, and Takahiro Hara. “Toward understanding acceleration-based
activity recognition neural networks with activation maximization”. In: 2021 International Joint Con-
ference on Neural Networks (IJCNN). IEEE. 2021, pp. 1–8.
[24] Dennis Grinwald et al. “Visualizing the Diversity of Representations Learned by Bayesian Neural
Networks”. In: Transactions on Machine Learning Research (2023).
[25] Stephen Casper et al. “Red teaming deep neural networks with feature synthesis tools”. In: Thirty-
seventh Conference on Neural Information Processing Systems. 2023.
[26] Kirill Bykov et al. “DORA: Exploring Outlier Representations in Deep Neural Networks”. In: Trans-
actions on Machine Learning Research (2023).
[27] Dilyara Bareeva et al. “Manipulating Feature Visualizations with Gradient Slingshots”. In: arXiv
preprint arXiv:2401.06122 (2024).
[28] RobertGeirhosetal.“Don’ttrustyoureyes:onthe(un)reliabilityoffeaturevisualizations”.In:arXiv
preprint arXiv:2306.04719 (2023).
[29] Jonathan Marty, Eugene Belilovsky, and Michael Eickenberg. “Adversarial Attacks on Feature Visu-
alization Methods”. In: NeurIPS ML Safety Workshop. 2022. url: https://openreview.net/forum?
id=J51K0rszIjr.
[30] Alec Radford et al. “Learning transferable visual models from natural language supervision”. In: In-
ternational conference on machine learning. PMLR. 2021, pp. 8748–8763.
[31] Chirag Agarwal et al. “OpenXAI: Towards a Transparent Evaluation of Model Explanations”. In:
Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.
2022. url: https://openreview.net/forum?id=MU2495w47rz.
[32] Anna Hedström et al. “Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural
NetworkExplanationsandBeyond”.In:Journal of Machine Learning Research 24.34(2023),pp.1–11.
[33] Anna Hedström et al. “Sanity Checks Revisited: An Exploration to Repair the Model Parameter
Randomisation Test”. In: XAI in Action: Past, Present, and Future Applications. 2023. url: https:
//openreview.net/forum?id=vVpefYmnsG.
[34] Tianyi Zhang et al. “BERTScore: Evaluating Text Generation with BERT”. In: International Confer-
ence on Learning Representations. 2019.
[35] Mert Yuksekgonul, Maggie Wang, and James Zou. “Post-hoc Concept Bottleneck Models”. In: The
Eleventh International Conference on Learning Representations. 2022.
[36] Anna Hedström et al. “The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estima-
tors with MetaQuantus”. In: (2023). doi: 10.48550/ARXIV.2302.07265. url: https://arxiv.org/
abs/2302.07265.
[37] DustinPodelletal.“SDXL:ImprovingLatentDiffusionModelsforHigh-ResolutionImageSynthesis”.
In: The Twelfth International Conference on Learning Representations. 2023.
[38] Pablo Pernias et al. “Würstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion
Models”. In: The Twelfth International Conference on Learning Representations. 2023.
[39] OlgaRussakovskyetal.“ImageNetLargeScaleVisualRecognitionChallenge”.In:International Jour-
nal of Computer Vision (IJCV) 115.3 (2015), pp. 211–252. doi: 10.1007/s11263-015-0816-y.
[40] KaimingHeetal.“Deepresiduallearningforimagerecognition”.In:ProceedingsoftheIEEEconference
on computer vision and pattern recognition. 2016, pp. 770–778.
12CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
[41] Gao Huang et al. “Densely connected convolutional networks”. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. 2017, pp. 4700–4708.
[42] Christian Szegedy et al. “Going deeper with convolutions”. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. 2015, pp. 1–9.
[43] Alexey Dosovitskiy et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at
Scale”. In: International Conference on Learning Representations. 2020.
[44] Bolei Zhou et al. “Places: A 10 million Image Database for Scene Recognition”. In: IEEE Transactions
on Pattern Analysis and Machine Intelligence (2017).
[45] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. “Deep learning”. In: nature 521.7553 (2015),
pp. 436–444.
[46] David Cheng-Han Chiang and Hung-yi Lee. “A Closer Look into Using Large Language Models for
Automatic Evaluation”. In: Findings of the Association for Computational Linguistics: EMNLP 2023,
Singapore, December 6-10, 2023. Ed. by Houda Bouamor, Juan Pino, and Kalika Bali. Association for
Computational Linguistics, 2023, pp. 8928–8942.
[47] Satyapriya Krishna et al. “Post Hoc Explanations of Language Models Can Improve Language Mod-
els”. In: Thirty-seventh Conference on Neural Information Processing Systems. 2023. url: https:
//openreview.net/forum?id=3H37XciUEv.
[48] NicholasKroegeretal.“AreLargeLanguageModelsPostHocExplainers?” In:CoRR abs/2310.05797
(2023).
[49] ChristophMolnar.InterpretableMachineLearning.AGuideforMakingBlackBoxModelsExplainable.
2nd ed. 2022. url: https://christophm.github.io/interpretable-ml-book.
[50] Kelvin Xu et al. “Show, attend and tell: Neural image caption generation with visual attention”. In:
International conference on machine learning. PMLR. 2015, pp. 2048–2057.
[51] Thomas H Cormen et al. Introduction to algorithms. MIT press, 2022.
[52] Sepp Hochreiter and Jürgen Schmidhuber. “Long short-term memory”. In: Neural computation 9.8
(1997), pp. 1735–1780.
[53] Christoph Schuhmann et al. “LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text
Pairs”. In: NeurIPS Workshop Datacentric AI, online (online), 14 Dec 2021 - 14 Dec 2021. Dec. 14,
2021, 5 p. url: https://juser.fz-juelich.de/record/905696.
[54] Tuomas Oikarinen and Tsui-Wei Weng. “CLIP-Dissect: Automatic Description of Neuron Representa-
tions in Deep Vision Networks”. In: The Eleventh International Conference on Learning Representa-
tions. 2022.
[55] George A Miller. “WordNet: a lexical database for English”. In: Communications of the ACM 38.11
(1995), pp. 39–41.
13CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
A Appendix
A.1 Concept-based Textual Explanation Methods
Concept-based textual explanation methods aim to provide insights into human-understandable concepts
learned by DNNs, enabling a deeper understanding of their decision-making mechanisms. These methods
provide textual descriptions for neurons in CV models. This creates a connection between the abstract
representation of a concept by the neural network and a human interpretation. In general, a concept can
be any abstraction, such as a color, an object, or even an idea [49]. Concept-based textual descriptions of a
neuron f can originate from various spaces depending on their generation process.
i
As defined in Section 3.1, we refer to explanation method as an operator E that maps a neuron to the
textual description s = E(f ) ∈ S, where S is a set of potential textual explanations. The specific set of
i
explanations depends on the implementation of the particular method. We define the following subsets of
textual descriptions s∈S:
• C represents the space of individual concepts,
• L represents the space of logical combinations of concepts,
• N represents the space of open-ended natural language concept descriptions.
These textual descriptions serve as explanations for f generated by explanation methods.
i
Examples for such explanation methods are MILAN [13], FALCON [14], CLIP-Dissect [15], and INVERT
[16]. Figure 6 shows the general principle of how E works. In Table 4 we outline the origin of textual
descriptions and their corresponding set memberships for each E.
Concept s
Explanation
Method E “purple flowers”
Neuron
E = MILAN,
FALCON, INVERT,
CLIP-Dissect etc.
Figure 6: Concept-Based Textuaĺ Explanation Methods. A neural representation f is selected, and a concept-based
i
textual explanation method E is applied to generate a textual description s explaining f .
i
A.1.1 NetDissect
Network Dissection (NetDissect) [11] is a method designed to explain individual neurons of DNNs, particu-
larlyconvolutionalneuralnetworks(CNNs)withinthedomainofCV.Thisapproachsystematicallyanalyzes
the network’s learned concepts by aligning individual neurons with given semantic concepts. To perform
this analysis, annotated datasets with segmentation masks are required, where these masks label each pixel
in an image with its corresponding object or attribute identity. The Broadly and Densely Labeled Dataset
(Broden)[11]combinesasetofdenselylabeledimagedatasetsthatrepresentbothlow-levelconcepts,suchas
colors, and higher-level concepts, such as objects. It provides a comprehensive set of ground truth examples
forabroadrangeofvisualconceptssuchasobjects,scenes,objectparts,textures,andmaterialsinavariety
of contexts.
Aconcepts∈C ⊂S isdefinedasavisualconceptinNetDissectandisprovidedbythepixel-levelannotated
Brodendataset. GivenaCNNandtheBrodendatasetasinput,NetDissectexplainsaneuralrepresentation
14CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
Table4: SetMembershipandOriginofDescriptions. Generatedtextualdescriptionsshavevaryingsetmembership
andoriginacrossallE. Thesedescriptionscanoriginatefromdistinctspaces: individualconceptsC,logicalcombina-
tionsofconceptsL,andopen-endednaturallanguageconceptdescriptionsN. Alabeleddatasetreferstoacollection
of images paired with individual concept labels. Generated captions are produced by image-to-text models, such as
Show-Attend-Tell [50]. An image caption dataset consists of image-caption pairs. A concept set consists of textual
concept labels.
Method Set Origin
NetDissect C labeled dataset
CompExp L labeled dataset
MILAN N generated caption
FALCON N image caption dataset
CLIP-Dissect C concept set
INVERT L labeled dataset
f by searching for the highest similarity between concept image segmentation masks and neuron activation
i
masks. Concept image segmentation masks are provided by the Broden dataset B (xxx)∈{0,1}H×W, where
s
a value of 1 signifies the pixel-level presence of s, and 0 denotes its absence. Neuron activation masks are
obtainedbythresholdingthecontinuousneuronactivationsoff intobinarymasksA(xxx)∈{0,1}H×W. Then
i
the similarity δ between image segmentation masks and binary neuron masks can be evaluated using the
IoU
Intersection over Union score (IoU) for an individual neuron within a layer:
(cid:80)
1(B (xxx)∩A(xxx))
δ (f ,s)= xxx∈XXX s . (5)
IoU i (cid:80) 1(B (xxx)∪A(xxx))
xxx∈XXX s
TheNetDissectmethodisoptimizedtoidentifytheconceptthatyieldsthehighestIoUscorebetweenbinary
masks and image segmentation masks. This can be formalized as:
E (f )=argmaxδ (f ,s). (6)
NetDissect i IoU i
s∈C⊂S
NetDissect is constrained to segmentation datasets, relying on pixel-level annotated images with segmenta-
tion masks. Moreover, its labeling capabilities are confined to concepts provided within a labeled dataset.
Furthermore, only individual concepts can be associated with each neuron.
A.1.2 CompExp
Toovercomethelimitationofexplainingneuronswithonlyasingleconcept,theCompositionalExplanations
ofNeurons(CompExp)methodwaslaterintroduced[12],enablingthelabelingofneuronswithcompositional
concepts. The method obtains its explanations by merging individual concepts into logical formulas using
composition operators AND, OR, and NOT. The formula length L∈N is defined beforehand. The initial
stage of explanation generation is similar to NetDissect, a set of images is taken as input, and convolutional
neuronactivationsareconvertedintobinarymasks. Theexplanationsareconstructedthroughabeamsearch
algorithm [51], beginning with individual concepts and gradually building them into more complex logical
formulas. Throughout the beam search stages, the existing formulas in the beam are combined with new
concepts. These new formulas are measured by the IoU. The maximization of the IoU score is desired to get
a high explanation quality.
The approach for obtaining δ is the same as in Equation 5. In contrast to NetDissect, the explanations
IoU
can be a combination of concepts, where s ∈ L ⊂ S. The procedure of finding the best neuron description
can be formalized as:
E (f )=argmaxδ (f ,s). (7)
CompExp i IoU i
s∈L⊂S
15CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
SimilartoNetDissect,CompExprequiresdatasetscontainingsegmentationmasksandisprimarilyapplicable
to convolutional neurons.
A.1.3 MILAN
MILAN [13] is a method that aims to describe neural representations within a DNN through open-ended
natural language descriptions. First, a dataset of fine-grained human descriptions of image regions (Milan-
notations) is collected. These descriptions can be defined as concepts that are open-ended natural language
descriptions, where s ∈ N ⊂ S. Given a DNN and input images xxx ∈ XXX, neuron masks M(xxx) ∈ RH×W×C
are collected of highly activated image regions for f .
i
Two distributions are then derived: the probability p(s|M(xxx)) that a human would describe an image
region with s, and the probability p(s) that a human would use the description s for any neuron. The
probability p(s|M(xxx)) is approximated with the Show-Attend-Tell [50] image-to-text model trained on the
Milannotations dataset. Additionally, p(s) is approximated with a two-layer LSTM language model [52]
trained on the Milannotations dataset.
These distributions are then utilized to find a description that has high pointwise mutual information with
M(xxx). Ahyperparameter λ∈R adjusts the significanceof p(s) during thecomputation of pointwise mutual
information (PMI) between descriptions s and M(xxx) sets, where the similarity δ is weighted PMI
WPMI
(WPMI). The objective for WPMI is given by:
δ (s)=logp(s|M(xxx))−λlogp(s). (8)
WPMI
MILANaimstooptimizehighpointwisemutualinformationbetweensandM(xxx)tofindthebestdescription
for f :
i
E (f )=argmaxδ (f ,s). (9)
MILAN i WPMI i
s∈N⊂S
Therequirementofcollectingthecuratedlabeleddataset,Milannotations,limitsMILAN’scapabilitieswhen
applied to tasks beyond this specific dataset. Additionally, another drawback is the requirement for model
training.
A.1.4 FALCON
The FALCON [14] explainability method has a similar approach to MILAN. Initially, it gathers the most
highly activating images corresponding to a neural representation. GradCam [5] is subsequently applied
to identify highlighted features in these images, which are then cropped to focus on these regions. These
cropped images, along with large captioning dataset LAION-400m [53] with concepts s∈N ⊂S, are input
toCLIP(ContrastiveLanguage-ImagePre-training)[30], whichcomputestheimage-textsimilaritybetween
the text embeddings of captions and the input cropped images. The top 5 captions are then extracted.
Conversely, the least activating images are collected, and concepts are extracted and removed from the
top-scoring concepts, ultimately yielding the explanation of the neural representation.
Thesimilarityδ isobtainedbycalculatingtheCLIPconfidencematrix,whichisessentiallyacosine
CLIPScore
similarity matrix. The aim is to find the maximum image-text similarity score between image embeddings
and their closest text embeddings from a large captioning dataset:
E (f )=argmaxδ (f ,s). (10)
FALCON i CLIPScore i
s∈N⊂S
This restriction significantly narrows down the range of models suitable for analysis, setting it apart consid-
erably from other explanation methods.
16CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
FALCON Implementation In its original implementation, FALCON restricts the set of “explainable
neurons” based on specific parameters. These include the parameter α ∈ N, which determines the set of
highly activating images for a given feature by requiring α>10. Additionally, it employs a threshold γ ∈R
for CLIP cosine similarity, with a set value of γ >0.8.
These parameter settings significantly restrict the number of explainable neurons, resulting to fewer than
50 explainable neurons. This constraint prevents the necessary randomization for comparison with other
methods. To address this, we set α = 0 and γ = 0. However, for FALCON-original, we retain the original
settings of α and γ and calculate Ψ across all “explainable neurons.” In our experiments on ResNet18,
FALCON can only be applied to layers 2 to 4.
A.1.5 CLIP-Dissect
CLIP-Dissect[54]isanexplanationmethodthatdescribesneuronsinvisionDNNswithopen-endedconcepts,
eliminatingtheneedforlabeleddataorhumanexamples. ThismethodintegratesCLIP[30],whichefficiently
learnsdeepvisualrepresentationsfromnaturallanguagesupervision. Itutilizesboththeimageencoderand
text encoder components of a CLIP model to compute the text embedding for each concept s∈C ⊂S from
aconceptdatasetandtheimageembeddingsfortheprobingimagesinthedataset, subsequentlycalculating
a concept-activation matrix.
Theactivationsofatargetneuronf arethencomputedacrossallimagesintheprobingdatasetXXX. However,
i
as this process is designed for scalar neural representations, these activations are summarized by a function
that calculates the mean of the activation map over spatial dimensions. The concept corresponding to the
target neuron is determined by identifying the most similar concept s based on its activation vector. The
most highly activated images are denoted asXXX ⊂XXX.
s
SoftWPMI is a generalization of WPMI where the probability p(xxx∈XXX ) denotes the chance an image xxx
s
belongs to the example set XXX . Standard WPMI corresponds to cases where p(xxx ∈XXX ) is either 0 or 1 for
s s
allxxx∈XXX, while SoftWPMI relaxes this binary setting to real values between 0 and 1. The function can be
formalized as:
δ (s)=logE[p(s|XXX )]−λlogp(s). (11)
SoftPMI s
The similarity function δ aims to identify the highest pointwise mutual information between the
SoftWPMI
most highly activated imagesXXX and a concept s. This optimization search is expressed as:
s
E (f )=argmaxδ (f ,s). (12)
CLIP-Dissect i SoftWPMI i
s∈C⊂S
AdrawbackofCLIP-Dissectliesinitsinterpretability;descriptionsaregeneratedbytheCLIPmodel,which
itself is challenging to interpret.
A.1.6 INVERT
LabelingNeuralRepresentationswithInverseRecognition(INVERT)[16]sharesthecapabilityofconstruct-
ing complex explanations like CompExp [12] but with the added advantage of not relying on segmentation
masks and only needing labeled data. The method obtains its explanations by merging individual concepts
into logical formulas using composition operators AND, OR, and NOT. It also exhibits greater versatility
inhandlingvariousneurontypesandiscomputationallylessdemandingcomparedtopreviousmethodssuch
as NetDissect [11] and CompExp [12]. Additionally, INVERT introduces a transparent metric for assessing
the alignment between representations and their associated explanations. The non-parametric Area Under
the Receiver Operating Characteristic (AUC) measure evaluates the relationship between representations
andconceptsbasedontherepresentation’sabilitytodistinguishthepresencefromtheabsenceofaconcept,
withstatisticalsignificance. TheprobingdatasetwiththeconceptpresentislabeledasXXX ,whilethedataset
1
without the concept is labeled asXXX .
0
17CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
ThegoalofINVERTistoidentifytheconcepts∈L⊂S thatmaximizesδ withtheneuralrepresentation
AUC
f . Here, s can be a combination of concepts. The optimization process resembles that of CompExp,
i
employing beam search [51] to find the optimal compositional concept. The top-performing concepts are
iteratively selected until the predefined compositional length L∈N is reached.
The similarity measure δ is defined as:
AUC
(cid:80) (cid:80)
1[f (xxx )<f (xxx )]
δ (f )= xxx0∈XXX0 xxx1∈XXX1 i 0 i 1 . (13)
AUC i |XXX |·|XXX |
0 1
The objective of INVERT is to maximize the similarity δ between a concept s and the neural represen-
AUC
tation f , which can be described as:
i
E (f )=argmaxδ (f ,s). (14)
INVERT i AUC i
s∈L⊂S
INVERT is constrained by the requirement of a labeled dataset and is computationally more expensive
compared to CLIP-Dissect.
A.2 Schematic Illustration of CoSy Implementation Details
In the example shown in Figure 1, we used the default settings of the explanation methods to generate
explanations for neuron 80 in the avgpool layer of ResNet18. For CLIP-Dissect, we used the 20,000 most
common English words as the concept dataset and the ImageNet validation dataset [39] as the probing
dataset . We employed Stable Diffusion XL 1.0-base (SDXL) [37] as the text-to-image model, using the
prompt “realistic photo of a close up of [concept]” to generate concept images, with [concept] being
replacedbythetextualexplanationfromthemethods. Wegenerated50imagesperconceptfor50randomly
chosen neurons from the avgpool layer of ResNet18. For evaluation, we also used the ImageNet validation
dataset as the control dataset.
A.3 Compute Resources
For running the task of image generation for CoSy we use distributed inference across multiple GPUs with
PyTorch Distributed, enabling image generation with multiple prompts in parallel. We run our script on
three Tesla V100S-PCIE-32GB GPUs in an internal cluster. Generating 50 images for 3 prompts in parallel
takes approximately 12 minutes.
A.4 Concept Broadness
While CoSy focuses on measuring the explanation quality, another open question is how broad or abstract
are the concepts provided as textual explanations. This question of how specific or general an individual
neuronisdescribedbytheexplanation,mightberelevanttodifferentXAIapplications. Forexample,research
fields where the user aims to deploy the same network for multiple tasks with varying image domains. In
this case, describing a neuron’s more general concept such as “a round object” might be more informative
than a more (domain-)specific concept such as “a tennis ball” for the network assessment. In an effort to
provide insight on the broadness of concepts, we assessed whether the similarity between images generated
based on the same concept changes for more general to more specific concepts.
In our experiment, we define the broadness of a concept based on the number of hypernyms in the WordNet
hierarchy [55]. The more specific a concept the larger the number of hypernyms. We choose two ImageNet
classes(“ladybug”,“pug”)andgenerate50imagesforeachconceptaswellaseachhypernymofbothconcepts
(withthemostgeneralconceptbeing“entity”.) Then,wemeasurethecosinesimilarityofallimagesgenerated
based on the same concept. The box plot of the cosine similarity across both concepts and all hypernyms,
in Figure 7 indicates that we do not find a correlation. Thus, we hypothesize that the chosen temperature
18CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
Figure7: Thefiguredemonstratestheindependenceoftheconceptbroadnessmeasuredbythenumberofhypernyms
as defined in WordNet [55] to the inter-image similarity of corresponding generated images.
of the diffusion model has a stronger effect on image similarity than the broadness of the prompt used for
image generation.
A.5 Intraclass Image Similarity
In addition to comparing natural and synthetic images as in Section 4.1, we also analyze the intraclass
distance to compare the similarity among synthetic images. Intraclass distance refers to the degree of
diversity or dissimilarity observed within a set of images of the same class. It quantifies how much the
individual images deviate from the average or central tendency of the image set. In this context, intraclass
distance is desirable, reflecting how visual concepts can appear in natural images. Higher similarity scores
indicate greater divergence from natural occurrences of concepts.
Cosine similarity (CS) and “Euclidean distance” (ED) are commonly used metrics for measuring image
similarity because they capture different aspects of similarity and complement each other. We compute
the average CS and ED for each class and determine the overall class average. Table 5 provides a detailed
overviewoftheresultsquantifyingthesimilaritywithinsyntheticimagesusingCS andED.Whenevaluating
these results, it is important to note that high scores do not necessarily indicate optimal outcomes, as
they suggest nearly identical images, which may lack intraclass distance. Conversely, very low scores imply
significantdifferencesamongimages,whichmightnotcapturetheessenceoftheconceptadequately. Ideally,
we aim for somewhat similar yet slightly varied images representing the same class. The results show that
theStableCascade(SC)modelconsistentlyachieveshigherscoresacrossallpromptscomparedtotheStable
DiffusionXL1.0-base(SDXL)model. Notably,itobtainsthehighestscoreforthetwomostelaborateprompts
(4,5). ThisindicatesthattheSCmodeltendstoofferlessintraclassdistanceinvisuallyrepresentingconcepts.
A.6 Model Stability
In this experiment, our goal is to evaluate the stability of the image generation method employed, aiming
to ensure consistent results within our CoSy framework. We achieve this by varying the seed of the image
generator and observing the impact on image generation. We anticipate consistent image representations
across different model initializations, thus ensuring the stability of our framework.
For our analysis, we utilize ResNet18 and focus on its output neurons, as the ground-truth labels associated
19CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
Table 5: Intraclass Image Similarity. This table illustrates the impact of varying parameters within our CoSy
framework. We evaluate 5 different prompts as input to 2 different text-to-image models. A random selection of 10
ImageNet classes is made, with each class name used as input to the prompt, denoted as [concept], resulting in 50
images generated per prompt using a text-to-image model. We compute the average intraclass similarity across all
classes. HigherCS andlowerED valuesindicategreatersimilaritybetweentheimages. Inintraclassimagesimilarity,
neither excessively high nor excessively low scores are desirable.
Prompt Text-to-image CS (↑) ED (↓)
1. “a [concept]” SDXL 0.83±0.07 5.85±1.41
SC 0.92±0.03 4.03±1.00
2. “a painting of [concept]” SDXL 0.87±0.05 4.94±1.13
SC 0.92±0.03 3.95±0.88
3. “photo of [concept]” SDXL 0.81±0.07 6.13±1.36
SC 0.90±0.04 4.46±1.05
4. “realistic photo of [concept]” SDXL 0.86±0.06 5.41±1.34
SC 0.93±0.03 3.79±0.85
5. “realistic photo of a close up of [concept]” SDXL 0.88±0.05 5.09±1.29
SC 0.93±0.03 3.95±0.92
with these neurons are known. We randomly select six classes s from the ImageNet validation dataset [39]
and examine the corresponding f class output neurons using CoSy. Here, we exclude the s class from A
i 0
andletA representthesclass. Toensurerobustness,weinitializethetext-to-imagemodelacrossarandom
1
setof10seeds. Ouranalysisinvolvescalculatingthefirst(mean)andsecondmoment(STD)usingΨ ,as
AUC
well as evaluating the intraclass image similarity (refer to Section A.5) within each synthetic ground truth
class.
The results for our experiment, as shown in Table 6, demonstrate remarkably high AUC scores, indicating
near-perfect detection of synthetic ground truth classes across all image model initializations. Furthermore,
the standard deviation is exceptionally low, suggesting consistent image generation regardless of the chosen
seed. Theintraclasssimilarityvaluesindicateacertaindegreeofdistanceinthegeneratedimages,indicating
high similarity yet distinctiveness. This intraclass distance is desirable, ensuring that the images are not
identical but share common characteristics.
These findings underscore the reliability and consistency of our image generation pipeline within our CoSy
framework. The high stability of text-to-image generation across different seeds and the diversity of image
similarity contribute to the robustness of our approach.
Table 6: Model Stability. A comparison of various model initializations across 10 random seeds using SDXL.
Concept AUC (↑) CS (↑) ED (↓)
bulbul 0.9996±0.0002 0.91±0.03 3.99±0.66
china cabinet 0.9999±0.0001 0.89±0.04 5.00±0.90
leatherback turtle 0.9994±0.0001 0.91±0.04 4.65±0.87
beer bottle 0.9919±0.0038 0.80±0.08 6.79±1.41
half track 0.9998±0.0000 0.88±0.04 5.12±0.91
hard disc 1.0000±0.0001 0.90±0.05 4.64±1.17
Overall Mean 0.9984±0.0007 0.88±0.02 5.03±0.26
20CoSy: Evaluating Textual Explanations of Neurons Kopf et al.
A.7 Prompt and Text-to-Image Model Comparison
Figure 8 showcases additional examples of synthetically generated images using both SDXL and SC across
various prompts, highlighting the diversity and accuracy of concept representation.
Figure8: Exampleimagesfor“coffeemug” generatedbythetext-to-imagemodelsSDXLandSCacrossvariousprompts.
(1)and(3)presentexamplesofsyntheticimageswithrelativelylowintraclasssimilarityandrelativelyhighnatural-
to-synthetic similarity scores. (2) shows examples of synthetic images with the lowest similarity to natural images.
(4) illustrates examples of synthetic images with the highest similarity to other synthetic images within the same
class. (5)showcasesexamplesofsyntheticimageswiththehighestsimilaritytonaturalimages. (6)displaysexamples
of natural images from the ImageNet validation dataset [39] belonging to the class “coffee mug”.
21