3
S Gaussian: Self-Supervised Street Gaussians for
Autonomous Driving
NanHuang1,2,∗ XiaobaoWei2 WenzhaoZheng1,3,† PengjuAn2 MingLu2
WeiZhan1 MasayoshiTomizuka1 KurtKeutzer1 ShanghangZhang2,‡
https://wzzheng.net/S3Gaussian
1UCBerkeley 2PekingUniversity 3TsinghuaUniversity
wenzhao.zheng@outlook.com; shanghang@pku.edu.cn
Abstract
Photorealistic 3D reconstruction of street scenes is a critical technique for de-
velopingreal-worldsimulatorsforautonomousdriving. Despitetheefficacyof
NeuralRadianceFields(NeRF)fordrivingscenes,3DGaussianSplatting(3DGS)
emergesasapromisingdirectionduetoitsfasterspeedandmoreexplicitrepresen-
tation. However,mostexistingstreet3DGSmethodsrequiretracked3Dvehicle
boundingboxestodecomposethestaticanddynamicelementsforeffectiverecon-
struction,limitingtheirapplicationsforin-the-wildscenarios. Tofacilitateefficient
3Dscenereconstructionwithoutcostlyannotations,weproposeaself-supervised
streetGaussian(S3Gaussian)methodtodecomposedynamicandstaticelements
from4Dconsistency. Werepresenteachscenewith3DGaussianstopreservethe
explicitness and further accompany them with a spatial-temporal field network
tocompactlymodelthe4Ddynamics. Weconductextensiveexperimentsonthe
challengingWaymo-Opendatasettoevaluatetheeffectivenessofourmethod. Our
S3Gaussiandemonstratestheabilitytodecomposestaticanddynamicscenesand
achievesthebestperformancewithoutusing3Dannotations. Codeisavailableat:
https://github.com/nnanhuang/S3Gaussian/.
1 Introduction
Autonomousdrivinghasmadesignificantprogressinrecentyearsanddevelopedvarioustechniques
in each stage of its pipeline including perception [29, 67, 23, 56], prediction [18, 16, 31], and
planning[11,9,10]. Withtheemergenceofend-to-endautonomousdrivingwhichdirectlyoutputs
the control signal from sensor inputs [19, 20, 24], open-loop evaluation of autonomous driving
systems ceases to be effective and thus requires pressing improvement [65, 30]. As a promising
solution, real-world closed-loop evaluation requires sensor inputs for controllable views, which
motivatesthedevelopmentofhigh-qualityscenereconstructionmethods[53,59].
Despitenumerouseffortsonphoto-realisticreconstructiononsmall-scalescenes[35,36,7,25,55],
thelarge-scaleandhighlydynamiccharacteristicsofdrivingscenariosposenewchallengestothe
effective modeling of 3D scenes. To accommodate these, most existing works adopt tracked 3D
boundingboxestodecomposestaticanddynamicelements[60,58,53]. Still,thecostlyannotations
of 3D tracklets limit their applications for 3D modeling from in-the-wild data. EmerNerf [61]
addressedthisbysimultaneouslylearningthesceneflowandusingittoconnectcorrespondingpoints
in the 4D NeRF field for multi-frame reconstruction, enabling the emergence of decomposition
betweenstaticanddynamicobjectswithoutexplicitboundingboxes. However,3Ddrivingscene
modelinghasbeenundergoingashiftfromNeRF-basedreconstructionto3DGaussianSplattingdue
*WorkdoneduringaninternshipatUCBerkeley.†Projectleader.‡Correspondingauthor.
Preprint.
4202
yaM
03
]VC.sc[
1v32302.5042:viXraGT RGB
3DGS
MARS
EmerNeRF
Ours
Front Left Front Front Right Front Left Front Front Right
Figure1: QualitativecomparisonoverWaymo-NOTRDatasets. Ontheleft,weshowcaseresults
fromnovelviewsynthesis;ontheright,resultsfromdynamicscenereconstructionaredisplayed.
Withtheproposedspatial-temporalnetworkfortheself-supervisedscenedecomposition,ourmethod
S3Gaussianproducesthebestrenderingqualitywithhighfidelityandsharpdetails.
toitsdesireforlowlatencyandexplicitrepresentation. ThoughEmerNerfdemonstratedpromising
results,itcanonlybeusedforNeRF-basedscenemodeling,whichtakesalongtimefortrainingand
rendering. Itisstillunclearhowtoachieve3DGaussianSplattingforurbanscenereconstruction
withoutexplicit3Dsupervision.
Toaddresstheaboveissues,weproposeaSelf-SupervisedStreetGaussiansnamedS3Gaussian,
offeringarobustsolutionfordynamicstreetsceneswithoutrequiring3Dsupervision. Specifically,to
handlethecomplexspatial-temporaldeformationsinherentindrivingscenes,S3Gaussianintroduces
a cutting-edge spatial-temporal field for scene decomposition in a self-supervised manner. This
spatial-temporalfieldincorporatesamulti-resolutionHexplanestructureencoderalongsideacompact
multi-headGaussiandecoder. TheHexplaneencoderisdesignedtodecomposethe4Dinputgridinto
multi-resolution,learnablefeatureplanes,efficientlyaggregatingtemporalandspatialinformation
from the dynamic street scenes. During the optimization process, the multi-resolution Hexplane
structureencodereffectivelyseparatestheentirescene,achievingacanonicalrepresentationforeach
scene. Dynamic-relatedfeaturesarestoredwithinthespatial-temporalplane,whilestatic-related
featuresareretainedinthespatial-onlyplane. Leveragingthedenselyencodedfeatures,themulti-
headGaussiandecoderscalculatethedeformationoffsetsfromthecanonicalrepresentations. These
deformationsarethenaddedtotheoriginal3DGaussians’attributes,includingpositionandspherical
harmonics,allowingforadynamicalterationofthescenerepresentationconditionedontimeseries.
Ourmaincontributionsaresummarizedasfollows:
• WeproposeS3Gaussian,thefirstself-supervisedmethodthatmanagestodecomposethe
dynamicandstatic3DGaussiansinstreetsceneswithoutextramanuallyannotateddata.
• Tomodelthecomplexchangesindrivingscenes,weintroduceanefficientspatial-temporal
decompositionnetworktoautomaticallycapturethedeformationof3DGaussians.
• We conduct comprehensive experiments on challenging datasets, including NOTR and
Waymo. ResultsdemonstratethatS3Gaussianachievesstate-of-the-artrenderingqualityon
scenereconstructionandnovelviewsynthesistasks.
2 RelatedWork
3DGaussianSplatting. Recentbreakthroughsin3DGaussianSplatting(3DGS)[26]haverevo-
lutionizedscenemodelingandrendering. Harnessingthepowerofexplicit3DGaussians,3DGS
achievesoptimaloutcomesinnovelviewsynthesisandreal-timerenderingwhilealsosubstantially
reducingparametercomplexitycomparedtoconventionalrepresentationssuchasmeshesorvoxels.
2Thistechniqueseamlesslyintegratestheprinciplesofpoint-basedrendering[1]andsplatting[70],
facilitatingrapidrenderinganddifferentiablecomputationthroughsplat-basedrasterization.
Whiletheoriginal3DGSmodelisdesignedforstaticscenerepresentation,severalresearchershave
extendeditsapplicabilitytodynamicobjectsandscenes. Forinstance,Yangetal.[63]introduces
adeformationnetworkaimedatcapturingGaussianmotionfromaseriesofdynamicmonocular
images. Anotherapproach,detailedby[57],establishesconnectionsbetweenneighboringGaussians
using a HexPlane, thereby enabling real-time rendering. By optimizing point clouds containing
semanticlogitsand3DGaussiansfornoveldynamicscenerepresentation,Yanetal.[60]achieves
improvementsintrainingandrenderingspeed. SimilarlytoNeRF’smethodology,Zhouetal.[68]
differentiatesstaticbackgroundsanddynamicobjectswithinthesceneandreconstructseachusing
distinctGaussianSplattingmethods. However,existingapproachesareconstrainedastheycanmodel
onlystaticordynamicscenesindividuallyandrequiresupervisedclassificationofscenetypes. Our
objectiveistoautonomouslylearnthedecompositionofstaticanddynamicscenesinaself-supervised
manner,therebyeliminatingtherelianceonrealannotations,suchasdynamicobjectboundingboxes.
StreetSceneReconstructionforAutonomousDrivingSimulation. Numerouseffortshavebeen
put into reconstructing scenes from autonomous driving data captured in real scenes. Existing
self-drivingsimulationenginessuchasCARLA[12]orAirSim[45]sufferfromcostlymanualeffort
tocreatevirtualenvironmentsandthelackofrealisminthegenerateddata. Therapiddevelopment
of Novel View Synthesis (NVS) techniques, including NeRF [35] and 3DGS [26], has attracted
considerableattentionwithinthearenaofautonomousdriving.Manystudies[8,17,33,34,40,43,42,
49,51,53,52,58,60,62,68]haveinvestigatedtheapplicationofthesemethodsforreconstructing
street scenes. Block-NeRF [49] and Mega-NeRF [52] propose segmenting scenes into distinct
blocksforindividualmodeling. UrbanRadianceField[42]enhancesNeRFtrainingwithgeometric
informationfrom LiDAR, whileDNMP [34]utilizes apre-traineddeformable meshprimitiveto
representthescene. Streetsurf[17]dividesscenesintoclose-range,distant-view,andskycategories,
yielding superior reconstruction results for urban street surfaces. For modeling dynamic urban
scenes,NSG[39]representsscenesasneuralgraphs,andMARS[58]employsseparatenetworksfor
modelingbackgroundandvehicles,establishinganinstance-awaresimulationframework. Withthe
introductionof3DGS[26],DrivingGaussian[68]introducesCompositeDynamicGaussianGraphs
andincrementalstaticGaussians,whileStreetGaussian[60]optimizesthetrackedposeofdynamic
Gaussiansandintroduces4Dsphericalharmonicsforvaryingvehicleappearancesacrossframes.
Theaforementionedmethodsnotonlysufferfromprolongedtrainingdurationsandsluggishrendering
speedsbutalsofailtoqualifytheabilitytodividedynamicandstaticscenesautomatically. Therefore,
we propose S3Gaussian to differentiate between dynamic and static scenes in a self-supervised
mannerwithouttheneedforadditionalannotations,andperformhigh-fidelityandreal-timeneural
renderingofdynamicurbanstreetscenes,whichiscrucialforautonomousdrivingsimulation.
3 ProposedApproach
Weaimtolearnaspatial-temporalrepresentationofthedynamicenvironmentofthestreetfroma
sequenceofimagescapturedbymovingvehicles. However,duetothelimitednumberofobservation
viewsandthehighcostofobtaininggroundtruthannotationsfordynamicandstaticobjects,weaim
tolearnthescenedecompositionofbothstaticanddynamiccomponentsinafullyself-supervised
manner,avoidingthesupervisionofextraannotationsincludingboundingboxesfordynamicobjects,
segmentationmasksforthescenedecomposition,andopticalflowforthemotionperception.
To achieve these objectives, we propose a novel scene representation named S3Gaussian. First,
in Sec. 3.1, we lift 3D Gaussians to 4D to better represent dynamic and complex scenes. Then,
in Sec. 3.2, we introduce a novel Spatial-temporal Field Network to integrate high-dimensional
spatial-temporalinformationanddecodethemtotransform4DGaussians. Finally,inSec.3.3,we
describetheentireoptimizationprocess,eliminatingextraannotations.
3.1 4DGaussianRepresentations
AsdepictedinFigure2,ourscenerepresentationsinclude3DGaussians[26]GandaSpatial-temporal
FieldNetworkF. Todepictstaticscenes,3DGaussiansarecharacterizedbyacovariancematrix
ΣandapositionvectorX,referredtoasthegeometricattributes. Forastableoptimization,each
3Space-only Plane Space-time Plane Bilinear Multi-scale feature
Interpolation
𝓓 ∆𝑆𝐻
𝑺𝑯
∆𝑥,
𝑷 𝒚𝒛 𝑷 𝒙𝒚 𝑷 𝒚𝒕 𝑷 𝒙𝒕 𝓓 𝒙 ∆ ∆𝑦 𝑧,
𝑷
𝑷 𝒙𝒛 𝒛𝒕 𝝓 𝒎 𝓓 𝒔 𝑓 !
Dynamic Scene SurroundingNovel ViewSynthesis
Rendering Reconstruction
𝒙,𝒚,𝒛,𝒕
Dynamic and StaticObject Decomposition
GaussianPrimitives Original Static Dynamic
Figure2: PipelineofS3Gaussian. Totacklethechallengesinself-supervisedstreetscenedecomposi-
tion,ourmethodconsistsofaMulti-resolutionHexplaneStructureEncodertoencode4Dgridinto
featureplanesandamulti-headGaussianDecodertodecodethemintodeformed4DGaussians. The
entirepipelineisoptimizedwithoutextraannotationsinaself-supervisedmanner,leadingtosuperior
scenedecompositionabilityandrenderingquality.
covariancematrixisfurtherfactorizedintoascalingmatrixS andarotationmatrixR:
Σ=RSSTRT (1)
Inadditiontothepositionandcovariancematrices,eachGaussianisalsoassignedanopacityvalue
α∈RandcolorC ∈R3(k+1)2,definedbysphericalharmonic(SH)coefficients,wherekrepresents
thedegreesofSHfunctions.
TheSpatial-temporalFieldNetworktakesthepositionofeachGaussianX andthecurrenttimestep
tasinput,producingspatial-temporalfeaturesf. Afterdecodingthesefeatures,thenetworkcan
predictthedisplacement△G ofeachpointrelativetocanonicalspacewhilealsoobtainingsemantic
informationf throughthesemanticfeaturedecoderD . WedetailitinSec.3.2.
s s
Following[64],weutilizeadifferentiable3DGaussiansplattingrendererRtoprojectthedeformed
3DGaussiansG′ =△G+G into2D[69]. Here,thecovariancematrixΣ′incameracoordinatesis:
Σ′ =JWΣWTJT (2)
whereJ istheJacobianmatrixoftheperspectiveprojection,andW istheviewingtransformmatrix.
ThecolorofeachpixeliscalculatedbyN orderedpointsusingα-blending:
i−1
(cid:88) (cid:89)
C = c α (1−α ) (3)
i i i
i∈N j=1
Here,α andc representtheopacityandcolorofonepoint,computedbyanoptimizableper-point
i i
opacityandSHcolorcoefficientswiththeviewdirection. Thesemanticmapcanberenderedsimply
bychangingthecolorcinEq.3tothesemanticfeaturef .
s
3.2 Spatial-temporalFieldNetwork
Theprimaryfocusofvanilla3DGaussiansSplattingisontasksinstaticscenes. However,thereal
worldisdynamic,especiallyincontextslikeautonomousdriving. Thismakesthetransitionfrom
3DGSto4Dacrucialandchallengingendeavor. Firstly,indynamicscenarios,theviewscapturedby
eachmovingcameraateachtimesteparesparserthaninstaticscenes,makingindividualmodeling
4ofeachtimestepexceptionallydifficultduetothissparsity. Therefore, itbecomesimperativeto
considerinformationsharingacrosstimesteps[14].
Moreover, modeling all Gaussian points in space and time is impractical for large-scale or long-
durationscenarioslikeautonomousdrivingduetosignificantmemoryoverhead. Hence,wepropose
leveraging an efficient Gaussian-based spatial-temporal network to model 3D Gaussian motion.
ThisnetworkcomprisesaMulti-resolutionHexplaneStructureEncoderandaminimalMulti-head
GaussianDecoder.Itonlyneedstomaintainasetofcanonical3DGaussiansandmodeladeformation
fieldforeachtimestep. Thisfieldpredictsdisplacementandcolorchangesrelativetothecanonical
space3DGaussians,thuscapturingGaussianmotion[57]. Additionally,weincorporateasimple
semanticfieldtoassistinautomaticallydecomposingstaticanddynamicGaussians.
Multi-resolution Hexplane Structure Encoder. To efficiently aggregate temporal and spatial
informationacrosstimesteps, consideringthatadjacentGaussiansoftensharesimilarspatialand
temporalcharacteristics,weemploytheMulti-resolutionHexplaneStructureEncoderE withatiny
MLPϕ torepresentdynamic3Dsceneseffectivelyinspiredby[6,13,14,46]. Specifically,the
m
HexPlanedecomposesthe4Dspatial-temporalgridintosixmulti-resolutionlearnablefeatureplanes
spanningeachpairofcoordinateaxes,eachendowedwithanorthogonalaxis. Thefirstthreeplanes
P , P , P represent spatial-only dimensions, while the latter three P , P , P represent
xy xz yz xt yt zt
spatial-temporalvariations. Thisdecouplingoftimeandspaceisbeneficialforseparatingstaticand
dynamicelements. Dynamicobjectsbecomedistinctlyvisibleonthespatial-temporalplane,while
staticobjectssolelymanifestonthespatial-onlyplane.
Additionally,topromotespatialsmoothnessandcoherencewhilecompressingthemodelandreducing
thenumberoffeaturesstoredatthehighestresolution,inspiredbyInstant-NGP’smulti-scalehash
encoding [? ], our hexplane encoder comprises multiple copies of different resolutions. This
representationeffectivelyencodesspatialfeaturesatvariousscales. Therefore,ourformulationis:
Pρ ∈Rd×ρri×ρrj,(i,j)∈{(x,y),(x,z),(y,z),(x,t),(y,t),(z,t)},ρ∈{1,2} (4)
ij
wheredisthehiddendimensionoffeatures,ρstandsfortheupsamplingscale,andrequalstothe
basicresolution. Givinga4Dcoordinate(x,y,z,t),wethenobtaintheneuralvoxelfeaturesand
mergeallthefeaturesusingatinyMLPϕ asfollows:
m
(cid:91)(cid:89)
f(x,y,z,t)=ϕ ( π(Pρ,ψρ(x,y,z,t))) (5)
m ij ij
ρ
whereψρ projects4Dcoordinate(x,y,z,t)ontothecorrespondingplane,andπdenotesbilinear
ij
interpolation,usedforqueryingvoxelfeatureslocatedatthefourvertices. Wemergetheplanesusing
Hadamardproducttoproducespatiallylocalizedsignals,asdiscussedin[14].
Multi-headGaussianDecoder. WeuseseparateMLPheadsD = (D ,D ,D )todecodethe
SH x s
featuresobtainedinSec.3.2.Specifically,weemployasemanticfeaturedecodertocomputesemantic
featuresf =D (f(x,y,z,t)). Consideringthatmostautonomousdrivingscenariosinvolverigid
s s
motion,weonlyconsiderdeformationinthepositionoftheGaussians,thus△x=D (f(x,y,z,t)).
x
Additionally,consideringfactorslikeillumination,theappearanceofthescenevarieswithitsglobal
positionandtime. Therefore,wealsointroduceanSHcoefficientheadtomodelthe4Ddynamic
appearancemodel△SH =D (f(x,y,z,t)). Finally,ourdeformed4DGaussiansareformulated
SH
as: G′ ={X +△X,C+△C,s,r,σ,f }.
s
3.3 Self-supervisedOptimization
LiDARPriorInitialization. Toinitializethepositionsofthe3DGaussians,weleveragetheLiDAR
pointcloudcapturedbythevehicleinsteadofusingtheoriginalSFM[44]pointcloudtoprovide
abettergeometricstructure. Toreducemodelsize,wealsodownsampletheentirepointcloudby
voxelizingitandfilteringoutpointsoutsidetheimage. Forcolors,weinitializethemrandomly.
OptimizationObjective. Thelossfunctionofourmethodconsistsofsevenparts,andwejointly
optimizeourscenerepresentationandSpatial-temporalfieldusingit. L istheL1lossbetween
rgb
renderedandgroundtruthimagesandL measuresthesimilaritybetweenthem. L istheL2
ssim depth
lossbetweentheestimateddepthmapfromtheLiDARpointcloudandtherendereddepthmap,used
tosupervisetheexpectedpositionoftheGaussians[61,68]. Therendereddepthiscomputedusing
thepositionsoftheGaussians. L istheL2lossofsemanticfeature. Following[13,47,14],we
feat
5Final Warm-up Initial LiDAR Points Initial Warm-up Final
Figure3: Illustrationoftheoptimizationprocess. WiththeLiDARpointsinitializationandthestatic
3DGaussianWarm-upstrategy,ourmodelachieveshigh-quality4DGaussianrepresentationsofthe
complexdynamicscenes.
alsointroduceagrid-basedtotal-variationallossL . Giventhatmostelementsinthescenearestatic,
tv
weintroduceregularizationconstraintsintothespatial-temporalnetworktoenhancetheseparation
ofstaticanddynamiccomponents. WeachievethisbyminimizingtheexpectationofE(△X)and
E(△C),whichencouragesthenetworkonlytoproduceoffsetvalueswhennecessary. Then,thetotal
lossfunctioncanbeformulatedasfollows:
L=λ L +λ L +λ L +λ L +λ L +λx Lx +λy Lc (6)
rgb rgb depth depth feat feat ssim ssim tv tv reg reg reg reg
where λ = 1.0, λ = 0.1, λ = 0.1, λ = 0.1, λ = 0.1, λx = 0.01, and
rgb depth feat ssim tv reg
λy =0.01aretheweightsassignedtoeachlosscomponent.
reg
4 Experiments
Inthissection,weprimarilydiscusstheexperimentalmethodologyusedtoevaluatetheperformance
ofourS3Gaussian. Detailsofthedatasetsettings,baselinemethods,andimplementationspecifics
areprovidedinSec.4.1. InSec.4.2,wecompareourapproachwithstate-of-the-art(SOTA)methods
acrossvarioustasks. FurtherablationstudiesandanalysisaredetailedinSec.4.3.
4.1 ExperimentalSetup
Datasets.NOTRdatasetisasubsetoftheWaymoOpendataset[48]curatedby[61],whichcomprises
manychallengingdrivingscenarios: ego-static,high-speed,exposuremismatch,dusk/dawn,gloomy,
rainy, and night scenes. In contrast, many public datasets with LiDAR data suffer from a severe
imbalance, eg. nuScenes [4] and nuPlan [5], predominantly featuring simple scenes with few
dynamic objects. Therefore, we utilize NOTR’s dynamic32 (D32) and static32 (S32) datasets,
totaling64scenes,toobtainabalancedanddiversestandardforevaluatingourstaticanddynamic
reconstruction. Furthermore,sincemostbaselinemethodsareNeRF-based,toensureafairevaluation
ofourmethod’sperformance,weconductcomparisonswiththecurrentstate-of-the-artGaussian-
basedmethod,StreetGaussian[60]. WeadheretothedatasetconfigurationusedbyStreetGaussian,
employingthesixscenesselectedfromtheWaymoOpendataset[48],whicharecharacterizedby
complexenvironmentsandsignificantobjectmotion.
BaselineMethods.Weevaluateourapproachagainststate-of-the-artmethods,includingNeRF-based
modelsand3DGS-basedmodels. MARS[58]isamodular[50]simulatorbasedonNeRF,utilizing
2DboundingboxestotrainNeRFforstaticanddynamicobjectsrespectively. NSG[40]learnslatent
codestomodelmovingobjectswithashareddecoder. EmerNeRF[61]alsobuildsuponNeRFbut
self-supervisesthemodelingofdynamicscenesbyoptimizingflowfields,representingthecurrent
SOTAinself-supervisedlearningfordynamicdrivingscenerepresentations. The3DGS[26]model
employsanisotropic3DGaussianellipsoidsasanexplicit3Dscenerepresentation,achievingthe
strongestperformanceacrossvarioustasksinstaticscenes. StreetGaussian[60],thelatestGaussian-
basedmethod,introducestimeintoSHcoefficients,reachingSOTAperformanceaswell,albeitalso
utilizing2Dtrackedboxes. Forafaircomparison,wealsoapplyLiDARpointcloudinitializationto
3DGS,anddepthregularizationto3DGSandMARS,mirroringourapproach.
ImplementationDetails. Wetrainourmodelfor50,000iterationsusingtheAdamoptimizer[27],
followingthelearningrateconfigurationsof3DGaussians[26]. Additionally,weemploy5,000steps
ofpurestatic3DGaussiantraining[26]asawarm-upforthescene[57],asillustratedinFigure3.
Forthereconstructionoflongsequencescenes,wedividethesceneintomultipleclips. Specifically,
weuse50framesperclip, wheretheoptimizedSpatial-temporalfieldservesastheinitialization
fortheSpatial-temporalfieldofthenextsequencewith50steps. Thisapproachmaintainsspatial
and temporal consistency across sequences within the same scene. The basic resolution for our
6Table1: OverallperformanceofourmethodswithexistingSOTAapproachesontheWaymo-NOTR
dataset[61]. "PSNR*"and"SSIM*"denotethePSNRandSSIMofdynamicobjectsrespectively.
The best andthe secondbest resultsaredenotedbypinkandblue.
SceneReconstruction NovelViewSynthesis
Data Metrics
3DGS MARS EmerNeRF Ours 3DGS MARS EmerNeRF Ours
PSNR↑ 28.47 28.24 28.16 31.35 25.14 26.61 25.14 27.44
SSIM↑ 0.876 0.866 0.806 0.911 0.813 0.796 0.747 0.857
D32 LPIPS↓ 0.136 0.252 0.228 0.106 0.165 0.305 0.313 0.137
PSNR*↑ 23.26 23.37 24.32 26.02 20.48 22.21 23.49 22.92
SSIM*↑ 0.716 0.701 0.682 0.783 0.753 0.697 0.660 0.680
PSNR↑ 29.42 28.31 30.00 30.73 26.82 27.63 28.89 27.05
S32 SSIM↑ 0.891 0.879 0.834 0.883 0.836 0.848 0.814 0.825
LPIPS↓ 0.118 0.196 0.201 0.116 0.134 0.193 0.212 0.142
Table2: QuantitativeresultsonStreetGaussiandatasets[60]. Westrictlyfollowtheexperimental
settingofitandborrowresultsfromitsinceithasnotbeenopen-sourced.
Metrics 3DGS NSG MARS EmerNeRF StreetGaussian Ours
PSNR↑ 29.64 28.31 31.37 32.34 34.96 34.61
SSIM↑ 0.918 0.862 0.904 0.886 0.945 0.95z0
LPIPS↓ 0.117 0.346 0.246 0.142 0.068 0.050
PSNR*↑ 16.48 19.55 23.07 25.71 25.46 25.78
multi-resolutionHexPlaneencoderissetto64,thenupsampledby2and4as[57]. Thelearningrate
ofitissetas1.6×10−3,decayedto1.6×10−4attheendoftrainging.Eachdecoderinthemulti-head
decoderisasmallMLPwiththesamelearningrateastheHexPlaneencoder. Otherhyperparameters
arekeptconsistentwith3DGS[26]. IntheexperimentsconductedontheWaymo-NOTRdataset,we
strictlyadheredtotheexperimentalsettingsofEmerNeRF[61]. Similarly,fortheWaymo-Street
dataset,ourexperimentalsetupcloselyfollowedStreetGaussian[60].
4.2 ComparisonswiththeState-of-the-art
TheresultsontheWaymo-NOTRdatasetdemonstratethatourapproachconsistentlyoutperforms
othermethodsinscenereconstructionandnovelviewsynthesis,asshowninTable1. Forthestatic32
dataset,weutilizePSNR,SSIM,andLPIPS[66]asmetricstoevaluaterenderingquality. Forthe
dynamic32dataset,weadditionallyincludePSNR*andSSIM*metricsfocusingondynamicobjects.
Specifically,weprojectthe3Dboundingboxesofdynamicobjectsontothe2Dimageplaneand
calculatepixellossonlywithintheprojectedboxesas[61,60]. Ourmetricsoutperformthoseof
otherexistingmethods,indicatingthesuperiorperformanceofourapproachinmodelingdynamic
objects. Moreover,althoughstaticscenerepresentationisnotourprimaryfocus,ourmethodalso
performsexceptionallywellinthisaspect. Thus,ourapproachismoreversatileandgeneral.
We also conducted qualitative comparisons, as shown in Figure 1. We emphasized regions with
significantdifferencestoprovideaclearerdemonstration.Fromthefigure,itisevidentthatourmethod
surpassesthestate-of-the-art(SOTA)inboththesynthesisofnewviewpoints(leftsideofFigure1)
and reconstruction (right side of Figure 1) of static and dynamic scenes. Although 3DGS [26]
faithfullyreconstructsstaticobjects,itfailswhendealingwithdynamicobjectsandstruggleswith
reconstructingdistantskies. ThereconstructionqualityofMARS[58]ispoor,beingeffectiveonly
forveryshortsequences,anditstrugglestoreconstructfast-movingobjects. WhileEmerNeRF[61]
can self-supervise the reconstruction of static and dynamic objects, the reconstruction quality is
unsatisfactory, with issues such as ghosting, loss of plant texture details, missing lane markings,
andblurrydistantscenes. Fornovelviewsynthesis,ourmethodcangeneratehigh-qualityrendered
imagesandensureconsistencybetweenmultiplecameraviews. Indynamicscenereconstruction,we
accuratelysimulatedynamicobjectsinlarge-scalescenes,particularlydistantdynamicobjects,and
mitigateissuessuchasloss,ghosting,orblurrinessassociatedwiththesedynamicelements.
Table 2 presents the results on the dataset collected by StreetGaussian [60]. StreetGaussian is a
state-of-the-artmethodforGaussian-baseddynamicobjectrepresentation. Ourapproachperforms
7GT RGB Ours StreetGaussian EmerNeRF MARS 3DGS
Figure 4: Qualitative comparison over Waymo-Street Datasets [60]. All results are from novel
viewsynthesis. ComparedtoStreetGaussian[60], ourmethoddemonstratesastrongerabilityto
self-supervisedlyreconstructdistantdynamicobjectsandismoresensitivetochangesinscenedetails.
Table3: QuantitativeablationstudiesonWaymo-NOTRdynamic32datasets.
Task Metrics w/oPρ w/oD w/oD w/oD w/oWarm-up Ours
ij x SH s
PSNR↑ 18.702 29.861 31.458 31.605 31.390 32.135
Scene SSIM↑ 0.4793 0.8871 0.9157 0.9174 0.9173 0.9355
Reconstruct PSNR*↑ 16.800 24.626 26.420 26.556 26.628 27.046
SSIM*↑ 0.3627 0.7521 0.8162 0.8182 0.8213 0.8284
PSNR↑ 17.245 25.850 27.959 27.981 27.955 28.417
SSIM↑ 0.4499 0.8174 0.8616 0.8624 0.8641 0.8641
NVS
PSNR*↑ 15.613 21.385 21.385 23.402 23.681 23.974
SSIM*↑ 0.3118 0.6386 0.6386 0.7138 0.7117 0.7175
similarlytoStreetGaussian,butwiththedistinctionthatStreetGaussianusesadditionalbounding
boxestomodeldynamicobjects,whereasourapproachdoesnotrequireanyexplicitsupervision. As
showninFigure4,comparedtoStreetGaussian[60]whichusesexplicitsupervision,ourmethod
excelsinself-supervisedreconstructionofdistantdynamicobjects. Additionally,ourmethodismore
sensitivetochangesinscenedetails,suchasvariationsintrafficlights. Furthermore,StreetGaussian
exhibitsnoiseinthesky,resultinginadecreaseinrenderingquality.
4.3 AblationandAnalysis
Weinvestigatetheeffectivenessofourmethodanditsvariouscomponents. Duetotimeconstraints,
we select 20 sequences from NOTR dynamic32 [61] for analysis, and all models are trained for
a shorter duration of 30,000 iterations. Table 3 presents the quantitative results, while Figure 5
showcasesthevisualcomparisonresults.
Multi-resolutionHexplaneStructureEncoder. Comparedtopurelyexplicitmethods,theproposed
HexPlane encoder Pρ allows for memory savings and enables retention of different dimensions
ij
ofspatial-temporalinformationinthescenethroughvariousresolutions. Discardingthismodule
and relying solely on a shallow MLP ϕ fails to accurately establish spatial-temporal fields and
m
cannotsimulateGaussiandeformations. BothTable3andFigure5demonstratethis,withoutthis
module,ourrenderingqualitysharplydeclines. Wealsoprovidevisualizationsofthefeaturesofthis
encoder,asshowninFigure6. Asanexplicitmodule,wecaneasilyoptimizeallGaussianfeatures
onasinglevoxelplane. FromFigure6,itisevidentthatthevoxelplanefeaturesmainlyconcentrate
onthemovingpartsofthescene. Thetrajectoriesofmovingvehiclesinthesceneextendfromthe
bottom-righttothetop-rightcorner. Asaresult,spatialplanefeaturesareprimarilyconcentratedin
thebottom-rightcorner,whereastemporalplanefeaturesarepredominantlyobservedontheright
side. Thesepatternsdemonstratethatourencodersuccessfullycapturesbothspatialandtemporal
information. Thiscapabilityallowsustoeffectivelyself-supervisethedecompositionofstaticand
dynamiccomponents,asillustratedinFigure6andFigure2.
Multi-head Gaussian Decoder. Our proposed multi-head Gaussian decoder can decode voxel
features. AsindicatedinTable3,disablingthiscomponentwouldimpactrenderingqualitygreatly.
Additionally,asshowninFigure5,disablingtheD decoderandonlytrainingGaussianincanonical
x
8Ours Complete Model W/O Semantic Decoder 𝒟&’
W/O 3D-only Warm-up Strategy W/O SH Decoder 𝒟%
W/O Hexplane Encoder 𝒫 !#
"
W/O Position Decoder 𝒟$
Figure5: VisualablationresultsontheWaymo-NOTRdynamic32dataset.
Rendered View 1 Rendered View 2
Temporal Grid Spatial-only Grid Static and Dynamic Object Decomposition
Figure6: VisualizationofHexPlanevoxelgrids,showcasingitscapabilitytodecomposestaticand
dynamicelements. Spatial-onlygridreferstothespatialvoxelparameters,whilethetemporalgrid
referstoitstimefeatures.
spacewouldintroducesignificantnoise. ThenoisestemsfromGaussianpointsinitializedbyLiDAR
pointclouds,resultinginaseriesofGaussianpointsalongamovingvehicle’strajectory. Ifthese
pointsarenotdeformed,itbecomeschallengingtooptimizethemafterward. Furthermore,omitting
thesemanticfeaturedecoderD andcolordeformationdecoderD primarilyaffectsrendering
s SH
details. Forexample,thegeometricstructureofthetruckbecomesblurrierwithoutthesecomponents.
StaticGaussianWarm-up. AccordingtoFigure5,wefoundthatdirectlytrainingthe4DGaussians
withoutfirstoptimizing3DGaussiansforwarm-upnotonlyreducesconvergencespeedbutalso
affectsthefinalrenderingquality. Asshowninthe3,performingawarm-upstepalreadyyieldsbasic
staticscenereconstruction,whichalleviatesthepressureonthe4Dspatial-temporalnetworktolearn
large-scalescenesandallowsthenetworktofocusmoreondynamicparts. Additionally,itstabilizes
thenetworkbyavoidingearly-stagenumericalerrors[57].
5 Conclusion
Inthispaper,weproposeS3Gaussian,thefirstself-supervisedstreetGaussianmethodtodifferentiate
dynamicandstaticelementsincomplexdrivingscenes. S3GaussianemploysaSpatial-temporal
FieldNetworktoachievethescenedecompositionautomatically,whichconsistsofaMulti-resolution
HexplaneStructureEncoderandaMulti-headGaussianDecoder. Givena4Dgridinglobalspace,
theproposedHexplaneencoderaggregatesfeaturesintodynamicorstaticplanes. Thenwedecode
these features into the deformed 4D Gaussians. The entire pipeline is optimized without any
extraannotations. ExperimentsonchallengingdatasetsincludingNOTRandWaymoimprovethat
S3Gaussian show superior scene decomposition ability and obtain the state-of-the-art rendering
qualityacrossdifferenttasks. Abundantquantitativeresultsareimplementedtoshedlightonthe
effectivenessofeachcomponentinS3Gaussian.
9A Appendix
A.1 AdditionalImplementationDetails
Datasets Details. Our Waymo-NOTR dataset follows the setup of [61]. For camera images, we
utilizethreefrontalcameras: FRONTLEFT,FRONT,andFRONTRIGHT,adjustedtoaresolution
of640×960fortrainingandevaluation. Thelengthofallsequencesissetto100frames. Weselect
every10thframefromthesequencesasthetestframesandusetheremainingframesfortraining.
ForourWaymo-Streetdataset,consistentwith[60],weusefrontalcamerasanddownscaletheinput
imagesto1066×1600forevaluatingmonocularreconstructionandnovelviewsynthesiscapabilities.
ThelengthofallsequencesstrictlyfollowsthedatasetsettingreleasedbyStreetGaussian[60],with
eachsequenceapproximately100frameslong. Weselectevery4thframefromthesequencesasthe
testframesandusetheremainingframesfortraining.
FeatureExtraction. WeemploytheDINOv2[38]checkpointandthefeatureextractorimplementa-
tionby[2]. Specifically,weusetheViT-B/14variantandadjusttheimagedimensionsto644×966
withastrideof7. Giventhelargesizeofthefeaturemaps,following[61]weusePCAdecomposition
toreducethefeaturedimensionfrom768to3andnormalizethesefeaturestothe[0,1]range.
A.2 MoreRelatedWork(AdvancesinNeuralRadianceFields)
Inrecentyears,therehasbeenasurgeofinterestamongresearchersinleveragingneuralrendering
techniques for scene modeling. Among these techniques, Neural Radiance Fields (NeRF) have
garneredparticularattention. NeRF[35]utilizesdifferentiablevolumerenderingmethods,facilitating
the generation of novel scenes from a mere collection of planar images accompanied by their
respectivecameraposes. Moreover,NeRFdemonstratesthecapabilitytosegregatestreetviewsinto
staticanddynamicscenesbytrackingtheboundingboxesofvehicles. Despitetheextensiveresearch
effortsaimedatenhancingNeRF’sfunctionalities,whichhaveledtonotableadvancementsintraining
speed [14, 15, 37], pose optimization [3, 32, 54], scene editing [28, 43], object generation [21],
anddynamicscenerepresentation[22,41],challengespersist,particularlyregardingtrainingand
renderingspeed. ThesechallengesposesignificantobstaclestothewidespreadadoptionofNeRF
inautonomousdrivingscenarios. ComparedtoNeRF-basedmethods,S3Gaussiansproposed4D
Gaussianrepresentationsfordynamicscenes,significantlyboostingrenderingspeed.
A.3 Limitations
Similartoothermethods[61,58],oursceneencountersdifficultyinmodelingobjectsmovingathigh
speeds. Wesuspectthismaybeduetothedeformationfield’shighvariance, renderingitunable
tomodeltheirrapidmovementsaccurately. Moreover,viewsofrapidlymovingdynamicobjects
aretypicallysparse,withonlyafewviewsavailableforcapture,makingreconstructionevenmore
challenging. Howtoreconstructthesechallengingsceneswillbeafocusofourfutureresearch.
References
[1] Kara-AliAliev,DmitryUlyanov,andVictorS.Lempitsky. Neuralpoint-basedgraphics. ArXiv,
abs/1906.08240,2019.
[2] ShirAmir,YossiGandelsman,ShaiBagon,andTaliDekel. Deepvitfeaturesasdensevisual
descriptors. arXivpreprintarXiv:2112.05814,2(3):4,2021.
[3] WenjingBian,ZiruiWang,KejieLi,JiawangBian,andVictorAdrianPrisacariu. Nope-nerf:
Optimisingneuralradiancefieldwithnoposeprior. 2023IEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),pages4160–4169,2022.
[4] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu,
Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal
datasetforautonomousdriving.In2020IEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),2020.
[5] Holger Caesar, Juraj Kabzan, KokSeang Tan, FongWhye Kit, EricM. Wolff, AlexH. Lang,
LukeFletcher,OscarBeijbom,andSammyOmari. nuplan: Aclosed-loopml-basedplanning
10benchmarkforautonomousvehicles. arXiv: ComputerVisionandPatternRecognition,arXiv:
ComputerVisionandPatternRecognition,2021.
[6] AngCaoandJustinJohnson. Hexplane: Afastrepresentationfordynamicscenes. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages130–141,
2023.
[7] AnpeiChen,ZexiangXu,AndreasGeiger,JingyiYu,andHaoSu. Tensorf: Tensorialradiance
fields. InEuropeanConferenceonComputerVision,pages333–350.Springer,2022.
[8] YuruiChen,ChunGu,JunzheJiang,XiatianZhu,andLiZhang. Periodicvibrationgaussian:
Dynamicurbanscenereconstructionandreal-timerendering. ArXiv,abs/2311.18561,2023.
[9] JieCheng,YingbingChen,QingwenZhang,LuGan,ChengjuLiu,andMingLiu. Real-time
trajectoryplanningforautonomousdrivingwithgaussianprocessandincrementalrefinement.
InICRA,pages8999–9005,2022.
[10] Jie Cheng, Xiaodong Mei, and Ming Liu. Forecast-MAE: Self-supervised pre-training for
motionforecastingwithmaskedautoencoders. ICCV,2023.
[11] DanielDauner,MarcelHallgarten,AndreasGeiger,andKashyapChitta. Partingwithmiscon-
ceptionsaboutlearning-basedvehiclemotionplanning. InCoRL,2023.
[12] AlexeyDosovitskiy,GermánRos,FelipeCodevilla,AntonioM.López,andVladlenKoltun.
Carla: Anopenurbandrivingsimulator. InConferenceonRobotLearning,2017.
[13] JieminFang,TaoranYi,XinggangWang,LingxiXie,XiaopengZhang,WenyuLiu,Matthias
Nießner, andQiTian. Fastdynamicradiancefieldswithtime-awareneuralvoxels. InSIG-
GRAPHAsia2022ConferencePapers,pages1–9,2022.
[14] SaraFridovich-Keil,GiacomoMeanti,FrederikRahbækWarburg,BenjaminRecht,andAngjoo
Kanazawa. K-planes: Explicitradiancefieldsinspace,time,andappearance. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages12479–12488,
2023.
[15] StephanJ.Garbin,MarekKowalski,MatthewJohnson,JamieShotton,andJulienP.C.Valentin.
Fastnerf: High-fidelityneuralrenderingat200fps. 2021IEEE/CVFInternationalConference
onComputerVision(ICCV),pages14326–14335,2021.
[16] JunruGu,ChenxuHu,TianyuanZhang,XuanyaoChen,YilunWang,YueWang,andHang
Zhao. Vip3d: End-to-end visual trajectory prediction via 3d agent queries. arXiv preprint
arXiv:2208.01582,2022.
[17] JianfeiGuo,NianchenDeng,XinyangLi,YeqiBai,BotianShi,ChiyuWang,ChenjingDing,
DongliangWang,andYikangLi. Streetsurf: Extendingmulti-viewimplicitsurfacereconstruc-
tiontostreetviews. ArXiv,abs/2306.04988,2023.
[18] AnthonyHu,ZakMurez,NikhilMohan,SofíaDudas,JeffreyHawke,VijayBadrinarayanan,
RobertoCipolla,andAlexKendall. Fiery: Futureinstancepredictioninbird’s-eyeviewfrom
surroundmonocularcameras. InICCV,2021.
[19] ShengchaoHu,LiChen,PenghaoWu,HongyangLi,JunchiYan,andDachengTao. St-p3:
End-to-endvision-basedautonomousdrivingviaspatial-temporalfeaturelearning. InECCV,
2022.
[20] YihanHu,JiazhiYang,LiChen,KeyuLi,ChonghaoSima,XizhouZhu,SiqiChai,Senyao
Du,TianweiLin,WenhaiWang,etal. Planning-orientedautonomousdriving. InCVPR,pages
17853–17862,2023.
[21] NanHuang,TingZhang,YuhuiYuan,DongChen,andShanghangZhang. Customize-it-3d:
High-quality3dcreationfromasingleimageusingsubject-specificknowledgeprior,2024.
[22] XinHuang,QiZhang,FengYing,HongdongLi,XuanWang,andQingWang. Hdr-nerf: High
dynamicrangeneuralradiancefields. 2022IEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR),pages18377–18387,2021.
11[23] YuanhuiHuang,WenzhaoZheng,YunpengZhang,JieZhou,andJiwenLu. Tri-perspective
viewforvision-based3dsemanticoccupancyprediction. InCVPR,pages9223–9232,2023.
[24] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang,
Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for
efficientautonomousdriving. arXivpreprintarXiv:2303.12077,2023.
[25] BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis. 3dgaussian
splattingforreal-timeradiancefieldrendering. ACMTransactionsonGraphics,42(4):1–14,
2023.
[26] BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis. 3dgaussian
splattingforreal-timeradiancefieldrendering. ACMTransactionsonGraphics,42(4),2023.
[27] DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization,2017.
[28] YuanLi,ZhiLin,DavidW.Forsyth,Jia-BinHuang,andShenlongWang. Climatenerf: Extreme
weather synthesis in neural radiance field. 2023 IEEE/CVF International Conference on
ComputerVision(ICCV),pages3204–3215,2022.
[29] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, and
JifengDai. Bevformer: Learningbird’s-eye-viewrepresentationfrommulti-cameraimagesvia
spatiotemporaltransformers. InECCV,2022.
[30] ZhiqiLi,ZhidingYu,ShiyiLan,JiahanLi,JanKautz,TongLu,andJoseMAlvarez. Isego
statusallyouneedforopen-loopend-to-endautonomousdriving? InCVPR,2024.
[31] MingLiang,BinYang,WenyuanZeng,YunChen,RuiHu,SergioCasas,andRaquelUrtasun.
Pnpnet: End-to-endperceptionandpredictionwithtrackingintheloop. InCVPR,2020.
[32] Chen-HsuanLin,Wei-ChiuMa,AntonioTorralba,andSimonLucey. Barf: Bundle-adjusting
neuralradiancefields. 2021IEEE/CVFInternationalConferenceonComputerVision(ICCV),
pages5721–5731,2021.
[33] JeffreyYunfanLiu,YunChen,ZeYang,JingkangWang,SivabalanManivasagam,andRaquel
Urtasun. Real-timeneuralrasterizationforlargescenes. 2023IEEE/CVFInternationalConfer-
enceonComputerVision(ICCV),pages8382–8393,2023.
[34] Fan Lu, Yan Xu, Guang-Sheng Chen, Hongsheng Li, Kwan-Yee Lin, and Changjun Jiang.
Urbanradiancefieldrepresentationwithdeformableneuralmeshprimitives. 2023IEEE/CVF
InternationalConferenceonComputerVision(ICCV),pages465–476,2023.
[35] BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoor-
thi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis.
CommunicationsoftheACM,65(1):99–106,2021.
[36] ThomasMüller,AlexEvans,ChristophSchied,andAlexanderKeller. Instantneuralgraphics
primitiveswithamultiresolutionhashencoding. ACMtransactionsongraphics(TOG),41(4):
1–15,2022.
[37] ThomasMüller,AlexEvans,ChristophSchied,andAlexanderKeller. Instantneuralgraphics
primitiveswithamultiresolutionhashencoding. ACMTransactionsonGraphics,page1–15,
2022.
[38] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal.Dinov2:Learning
robustvisualfeatureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023.
[39] JulianOst,FahimMannan,NilsThuerey,JulianKnodt,andFelixHeide. Neuralscenegraphs
fordynamicscenes. 2021IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),pages2855–2864,2020.
[40] JulianOst,FahimMannan,NilsThuerey,JulianKnodt,andFelixHeide.Neuralscenegraphsfor
dynamicscenes. In2021IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),2021.
12[41] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf:
Neuralradiancefieldsfordynamicscenes. 2021IEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR),pages10313–10322,2020.
[42] KonstantinosRematas,AnLiu,PratulP.Srinivasan,JonathanT.Barron,AndreaTagliasacchi,
ThomasA.Funkhouser,andVittorioFerrari.Urbanradiancefields.2022IEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR),pages12922–12932,2021.
[43] ViktorRudnev,MohamedA.Elgharib,WilliamH.B.Smith,LingjieLiu,VladislavGolyanik,
andChristianTheobalt.Nerfforoutdoorscenerelighting.InEuropeanConferenceonComputer
Vision,2021.
[44] JohannesL.SchonbergerandJan-MichaelFrahm. Structure-from-motionrevisited. In2016
IEEEConferenceonComputerVisionandPatternRecognition(CVPR),2016.
[45] S.Shah,DebadeeptaDey,ChrisLovett,andAshishKapoor. Airsim: High-fidelityvisualand
physicalsimulationforautonomousvehicles. InInternationalSymposiumonFieldandService
Robotics,2017.
[46] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and Yebin Liu.
Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and
rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages16632–16642,2023.
[47] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast
convergenceforradiancefieldsreconstruction. In2022IEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),2022.
[48] PeiSun,HenrikKretzschmar,XerxesDotiwalla,AurelienChouard,VijaysaiPatnaik,PaulTsui,
JamesGuo,YinZhou,YuningChai,BenjaminCaine,VijayVasudevan,WeiHan,JiquanNgiam,
HangZhao,AlekseiTimofeev,ScottEttinger,MaximKrivokon,AmyGao,AdityaJoshi,Sheng
Zhao,ShuyangCheng,YuZhang,JonathonShlens,ZhifengChen,andDragomirAnguelov.
Scalabilityinperceptionforautonomousdriving: Waymoopendataset,2020.
[49] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P.
Srinivasan, Jonathan T. Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene
neuralviewsynthesis.2022IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),pages8238–8248,2022.
[50] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang, Alexan-
derKristoffersen, JakeAustin, KamyarSalahi, AbhikAhuja, DavidMcallister, JustinKerr,
andAngjooKanazawa. Nerfstudio: Amodularframeworkforneuralradiancefielddevelop-
ment. InSpecialInterestGrouponComputerGraphicsandInteractiveTechniquesConference
ConferenceProceedings.ACM,2023.
[51] Adam Tonderski, Carl Lindstrom, Georg Hess, William Ljungbergh, Lennart Svensson,
and Christoffer Petersson. Neurad: Neural rendering for autonomous driving. ArXiv,
abs/2311.15260,2023.
[52] HaithemTurki,DevaRamanan,andMahadevSatyanarayanan. Mega-nerf: Scalableconstruc-
tionoflarge-scalenerfsforvirtualfly-throughs. 2022IEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),pages12912–12921,2021.
[53] HaithemTurki,JasonYZhang,FrancescoFerroni,andDevaRamanan. Suds: Scalableurban
dynamicscenes. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages12375–12385,2023.
[54] ZiruiWang,ShangzheWu,WeidiXie,MinChen,andVictorAdrianPrisacariu. Nerf-: Neural
radiancefieldswithoutknowncameraparameters. ArXiv,abs/2102.07064,2021.
[55] XiaobaoWei,RenruiZhang,JiaruiWu,JiamingLiu,MingLu,YandongGuo,andShanghang
Zhang. Noc: High-qualityneuralobjectcloningwith3dliftingofsegmentanything. arXiv
preprintarXiv:2309.12790,2023.
13[56] YiWei,LinqingZhao,WenzhaoZheng,ZhengZhu,JieZhou,andJiwenLu. Surroundocc:
Multi-camera3doccupancypredictionforautonomousdriving. InICCV,pages21729–21740,
2023.
[57] GuanjunWu,TaoranYi,JieminFang,LingxiXie,XiaopengZhang,WeiWei,WenyuLiu,Qi
Tian,andXinggangWang. 4dgaussiansplattingforreal-timedynamicscenerendering. ArXiv,
abs/2310.08528,2023.
[58] Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen, Hongmin Xiao, Chao Hou,
Haozhe Lou, Yuantao Chen, Runyi Yang, Yuxin Huang, Xiaoyu Ye, Zike Yan, Yongliang
Shi,YiyiLiao,andHaoZhao. Mars: Aninstance-aware,modularandrealisticsimulatorfor
autonomousdriving. CICAI,2023.
[59] ZiyangXie,JungeZhang,WenyeLi,FeihuZhang,andLiZhang. S-nerf: Neuralradiancefields
forstreetviews. arXivpreprintarXiv:2303.00749,2023.
[60] YunzhiYan,HaotongLin,ChenxuZhou,WeijieWang,HaiyangSun,KunZhan,Xianpeng
Lang, Xiaowei Zhou, and Sida Peng. Street gaussians for modeling dynamic urban scenes.
ArXiv,abs/2401.01339,2024.
[61] JiaweiYang,BorisIvanovic,OrLitany,XinshuoWeng,SeungWookKim,BoyiLi,TongChe,
DanfeiXu,SanjaFidler,MarcoPavone,andYueWang. Emernerf: Emergentspatial-temporal
scenedecompositionviaself-supervision,2023.
[62] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce
Yang,andRaquelUrtasun. Unisim: Aneuralclosed-loopsensorsimulator. 2023IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),pages1389–1399,2023.
[63] Ziyi Yang, Xinyu Gao, Wenming Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin.
Deformable3dgaussiansforhigh-fidelitymonoculardynamicscenereconstruction. ArXiv,
abs/2309.13101,2023.
[64] WangYifan,FeliceSerena,ShihaoWu,CengizÖztireli,andOlgaSorkine-Hornung. Differen-
tiablesurfacesplattingforpoint-basedgeometryprocessing. ACMTransactionsonGraphics,
38(6):1–14,2019.
[65] Jiang-Tian Zhai, Ze Feng, Jinhao Du, Yongqiang Mao, Jiang-Jiang Liu, Zichang Tan, Yifu
Zhang,XiaoqingYe,andJingdongWang. Rethinkingtheopen-loopevaluationofend-to-end
autonomousdrivinginnuscenes. arXivpreprintarXiv:2305.10430,2023.
[66] RichardZhang,PhillipIsola,AlexeiA.Efros,EliShechtman,andOliverWang. Theunreason-
ableeffectivenessofdeepfeaturesasaperceptualmetric. In2018IEEE/CVFConferenceon
ComputerVisionandPatternRecognition,2018.
[67] YunpengZhang,ZhengZhu,WenzhaoZheng,JunjieHuang,GuanHuang,JieZhou,andJiwen
Lu. Beverse:Unifiedperceptionandpredictioninbirds-eye-viewforvision-centricautonomous
driving. arXivpreprintarXiv:2205.09743,2022.
[68] XiaoyuZhou,ZhiweiLin,XiaojunShan,YongtaoWang,DeqingSun,andMing-HsuanYang.
Drivinggaussian: Compositegaussiansplattingforsurroundingdynamicautonomousdriving
scenes. ArXiv,abs/2312.07920,2023.
[69] MatthiasZwicker,HanspeterPfister,JeroenVanBaar,andMarkusGross. Surfacesplatting. In
Proceedingsofthe28thannualconferenceonComputergraphicsandinteractivetechniques,
pages371–378,2001.
[70] MatthiasZwicker,HanspeterPfister,JeroenvanBaar,andMarkusH.Gross. Ewasplatting.
IEEETrans.Vis.Comput.Graph.,8:223–238,2002.
14