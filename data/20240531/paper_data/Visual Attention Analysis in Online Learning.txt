Visual Attention Analysis in Online Learning
Miriam Navarro∗, A´lvaro Becerra∗, Roberto Daza∗, Ruth Cobos∗, Aythami Morales∗, Julian Fierrez∗
∗School of Engineering, Universidad Autonoma de Madrid, Spain
{miriam.navarroclemente, alvaro.becerra, roberto.daza, ruth.cobos, aythami.morales, julian.fierrez}@uam.es
Abstract—In this paper, we present an approach in the those in need of additional support. Consequently, instructors
Multimodal Learning Analytics field. Within this approach, we can intervene and offer targeted assistance to learners who
have developed a tool to visualize and analyze eye movement
may benefit from it.
data collected during learning sessions in online courses. The
The second one, M2LADS, facilitates the integration and
toolisnamedVAAD—anacronymforVisualAttentionAnalysis
Dashboard—. These eye movement data have been gathered visualization of multimodal data captured during learning
using an eye-tracker and subsequently processed and visualized sessionswithinaMOOC,presentedthroughWeb-basedDash-
for interpretation. The purpose of the tool is to conduct a boards.
descriptive analysis of the data by facilitating its visualization,
The capture of data throughout learning sessions is carried
enabling the identification of differences and learning patterns
out via the edBB platform, tailored for remote education,
among various learner populations. Additionally, it integrates a
predictivemodulecapableofanticipatinglearneractivitiesduring which captures both biometric and behavioral data [11], [12].
a learning session. Consequently, VAAD holds the potential to With the main aim to complement and provide analysis of
offer valuable insights into online learning behaviors from both some data managed by M2LADS, we have developed a tool
descriptive and predictive perspectives.
designed to visualize eye movement data collected during a
IndexTerms—biometrics,dashboard,eye-tracker,learningan-
learning session (LS) in online learning. The name of this
alytics, machine learning, multimodal learning, online learning
tool is VAAD, which stands for Visual Attention Analysis
Dashboard.
I. INTRODUCTION In this article we present the tool VAAD. The structure
The progression of technology has played a crucial role in of this article is as follows: In the following section we
fosteringinnovativeeducationwithinonlinesettings,alongside present related works and the motivation for the approach
various other noteworthy advancements. A notable instance proposed. Then, in Section III we provide a description of
of this is the growing prominence of Massive Open Online the context and dataset. In Section IV, a detailed explanation
Courses (MOOCs). Known for their diversity, MOOCs pri- of the proposed approach is presented. Finally, in Section V
oritize inclusivity and accessibility, as they do not necessitate the article concludes with conclusions and future work.
specificprerequisitesforenrollment.Thiswidearrayofsubject
II. RELATEDWORKSANDMOTIVATIONFORTHE
matter makes them a valuable source of knowledge, as they
APPROACHPROPOSED
are recognized by official educational institutions [1], [2].
Educatorscannowuploadlearningmaterials,suchasvideos Capturing and analyzing biometric multimodal data can
and other educational resources, through these online courses, be an effective method for gaining insight into a course’s
making it easy for learners to access this information. This dynamics and learner engagement [13]. For instance, audio
enables learners from all over the world to access numerous data can be used to create a network of learner interactions;
courses uploaded by different universities globally. visualandeye-trackingdatacanofferinsightsintovisualatten-
While the popularity of MOOCs continues to grow, so do tion;sensorscanmeasurepsychologicalresponses.Ifproperly
dropout rates. Despite being an appealing learning tool, few interpreted, these data can help understand interactions that
learners successfully complete these courses [3]. This is why contribute to educational success [14].
educators are worried about this trend, emphasizing the need Biometric multimodal data can be collected from various
to understand the underlying causes. devices[15],includingwebcams,electroencephalogrambands
This concern has led to the creation of Learning Analyt- (EEG) for brain waves and attention levels, smartwatches for
ics (LA) tools, which provide valuable insights into online heart rate and EDA, or eye-trackers for pupil diameter and
educational environments [4]. It is the case of Universidad visual attention, including fixation gaze and saccades.
Auto´noma de Madrid (UAM), with the development of such Regarding visual attention, earlier studies have shown that
systems, like edX-LIMS [5]–[8] or M2LADS [9], [10]. eye-tracking techniques can give a deeper understanding of
The first one, edX-LIMS, gives learners feedback on their learners’ performance, regardless of their backgrounds [16].
interaction with the MOOC, assisting them throughout their Employingthisapproachcouldenhanceattentionandthequal-
learning journey and addressing their problems, thereby keep- ity of learning in MOOCs, due to the established correlation
ing them engaged. They also provide instructors with a web- between learner performance and visual attention [17].
basedDashboardtomonitortheirlearners,givingthemabetter In [16], learners’ visual attention was examined, and feed-
understanding of their progress and helping them identify back on their visual performance led to a consistent improve-
4202
yaM
03
]VC.sc[
1v19002.5042:viXrament of their performance by 1% every minute. Additionally, In this study, the monitored learners engaged in various
research in [18] revealed that listeners who focused more activities such as watching videos, reading documents, and
on visual references made by an interlocutor had higher then completing assignments on the LS content, which was
comprehension levels, while research in [19] found a strong about HTML language. Prior to the gathering of multimodal
correlation between fixation eye movements and learning data from the LS, learners underwent a pretest to determine
outcomes. their initial level of knowledge on HTML. At the end of the
To better understand visual data, visualization is crucial. LS, the assignments that had been completed by the learners
In the LA field, the frequent use of dashboards [20], [21] in the MOOC served as the posttest items.
aids learners and instructors in decision-making by providing Learners were monitored throughout the LS via the edBB
a clear insight into the ongoing learning process [22]. platform. All multimodal biometric data captured by their
Research cited in [23] highlights the effectiveness of dash- sensors are synchronised by M2LADS [9], [10]. The sensors
boards in the aftermath of processing and synthesizing visual used to obtain the different multimodal data are as follows:
·
attention data, providing analysts with precise and succinct Video data: Acquired from overhead, front and side
information. The study presents a dashboard designed to cameras,consistingof2webcamsand1IntelRealSense.
organize eye-tracking data from a range of learners, including Additionally, video from the screen monitors was also
information on eye fixations, saccades, and pupil diameter. recorded.
·
Moreover, it successfully predicted with a 76.4% accuracy Electroencephalogram(EEG)data:AcquiredfromaNeu-
whether a learner was fatigued or not employing predictive roSky EGG band [26], [27].
·
algorithms. Heart rate data: Acquired from a Huawei Watch 2 pul-
The eye-tracking technology is an integral part of all the someter feature [28].
·
research cited. This is why in the case of the tool VAAD Visual attention data: Acquired from a Tobii Pro Fusion
the focus remains on eye movement data. This tool focuses that contains 2 eye-tracking cameras.
on fixation and saccade events, as prior research has revealed
As previously mentioned, VAAD centers on managing all
thateyemovementscanindicatepatternsandvisualbehaviors
multimodal data linked to visual attention, which is why the
associated with different areas of interest (AOI) for learners,
Tobii Pro Fusion eye-tracking device remains the focal point.
highlighting areas where they focus the most [24]. Moreover,
ThedataforVAADarisesfromtheprocessingmodule,which
this research has shown that studying saccade movements can
handles the eye-tracking data produced by the eye-tracker, as
successfully help identify AOIs.
wellasadditionaldatageneratedbyM2LADS.Thisstudyhas
VAAD’s goal is to extract essential information about fixa- received approval from the university’s ethics committee, and
tion and saccade events. This tool then presents visual data in all biometric multimodal data are anonymized.
two views: a global one, offering insight into the session and
MOOC activities, and an individual view displaying learners’ A. Eye-tracking Data
AOIs through a heat map. Consequently, VAAD is equipped
The eye-tracker estimates a wide variety of multimodal
with a descriptive analysis module that includes the option
data. However, for the development of VAAD, the processing
foranANOVAtest,enablingittoidentifycorrelationsamong
module only works with specific metrics, which are listed in
different learner populations as seen in prior studies [25].
Table I.
However, VAAD is not limited to a descriptive module; it
also features a predictive analysis module that can identify B. M2LADS Data
what a learner is doing at a specific moment during the
AdditionaldataregardinglearnerpopulationandtheLSare
LS. This allows VAAD to provide insights using both de-
obtainedfromtheM2LADSdatabase.Theprocessingmodule
scriptive and predictive approaches, enabling analysts to draw
exclusively operates with specific data, which are listed in
conclusions and gain a better understanding of the cognitive
Table II.
processes related to eye movement.
Prior to the monitorization, learners were categorized into
three different groups (40 learners per group), which deter-
III. CONTEXTANDDATASET
mined the frequency of interruptions they encountered during
In order to test the approach proposed, we have conducted the LS.
a study in the School of Engineering at our institution,
IV. APPROACHPROPOSED
where 120 learners from the school were monitored in our
laboratory while they attended and interacted with a MOOC The VAAD tool is composed of three different modules:
subunit during a 30-minute LS. The chosen MOOC was titled a processing data module, a visualization module, and a
”IntroductiontoDevelopmentofWebApplication”(WebApp), prediction module. As seen in Fig. 1, the processing module
which is available on the edX MOOC platform1 and offered manages the data from the eye-tracker and M2LADS, and
by our university. is interconnected with both the visualization and prediction
modules, which use the processed data for their respective
1https://www.edx.org/ tasks.TABLEI
MULTIMODALDATAEXTRACTEDFROMTHETOBIIPROFUSION
EYE-TRACKINGDEVICE
Variable Description Units
ParticipantID AnonymizedID -
Recordingdate - YYYY-MM-DD
Recording start - HH:MM:SS:mmm
time
Recording times- Timestamp counted from the Milliseconds
tamp startoftherecording(t0=0)
GazepointX Horizontal coordinate of the Pixels
averaged left and right eye
gazepoint
GazepointY Vertical coordinate of the av- Pixels
eraged left and right eye gaze
point
Eye movement Typeofeyemovement Fixation,
type Saccade,
Fig.1. VAADArchitecture/Modules
Unclassified,
EyesNotFound
GazeEventdura- The duration of the currently Milliseconds
tion activeeyemovement thus enabling differentiation of populations based on these
Eye movement Count is an auto-increment Number factors. Subsequently, four key parameters were determined
typeindex number starting with 1 for
for each learner: average saccades, average fixations, average
eacheyemovementtype
saccade time, and average fixation time. These parameters
were not only assessed for the overall session but also for
TABLEII
DATAEXTRACTEDFROMM2LADS each activity individually. The culmination of this data pro-
cessing is a final database that profiles each learner, including
Variable Description
their respective group and sex, and provides details on the
ParticipantID AnonymizedIDoftheparticipant
Activityidentifier Activity of the MOOC in which the four aforementioned parameters for every activity within the
learnerisinvolved MOOC, as well as for the entire session.
Initialtime Initialtimestampoftheactivity
Regarding the individual visual overview, data were gen-
Finaltime Finaltimestampoftheactivity
Sex Participantsex erated for each available learner, which included information
Group Participantgroup about their saccade and fixation events, along with their gaze
HTMLlevel InitialknowledgelevelonHTML point parameters for the X and Y coordinates and the activity
Academicbackground Universitydegree
within the MOOC.
Learningscore Difference between the posttest and
pretestgrade Concerning the predictive analysis, data were utilized to
collect various metrics subsequently employed for prediction.
Detailed information on these metrics is gathered in Table III.
A. Processing Data Module
Throughout the LS, VAAD has specifically focused on TABLEIII
capturing saccade and fixation event information provided by METRICSUSEDINTHEPREDICTIONMODULE
theeye-tracker,inadditiontotimestampsandeventdurations,
Feature Description
to gather insights into these eye movements. Sex Participantsex
Before the processing stage, the eye-tracking data of learn- Label Theactivitytobepredictedwithinthe
MOOC
ers were thoroughly inspected to identify errors or anomalies.
Saccadenumber Averagenumberofsaccademovements
Learners displaying a notable frequency of ”Eyes Not Found”
Velocity Averagesaccadevelocity
values for the Eye movement type variable were excluded to VelocityX Averagehorizontalsaccadevelocity
ensure the integrity of the final data utilized by VAAD. VelocityY Averageverticalsaccadevelocity
Maxvelocity Maximumsaccadevelocity
The objective of the processing module was to synchronize
Minvelocity Minimumsaccadevelocity
the eye-tracking data with the M2LADS data, ensuring that Deviation Saccadevelocitytypicaldeviation
each learner’s eye-tracking data were accurately tagged with DeviationX Saccade horizontal velocity standard
deviation
thecorrespondingactivitieswithintheMOOCforeveryframe
DeviationY Saccadeverticalvelocitytypicaldevia-
captured. This approach enabled us to determine precisely
tion
when each eye event occurred during the LS, utilizing the Kurtosis Saccadevelocitykurtosis
available timestamps and activities. By incorporating this KurtosisX Saccadehorizontalvelocitykurtosis
KurtosisY Saccadeverticalvelocitykurtosis
tagging process, we were able to generate the necessary data
Skew Saccadevelocityskew
for the visualization module and the prediction module. SkewX Saccadehorizontalvelocityskew
From a general visualization perspective, data were com- SkewY Saccadeverticalvelocityskew
piledtoincludealllearners,incorporatingtheirsexandgroup,Fig.2. Screenshotfromanexampleoverviewofglobalanalysis
Fig.3. Screenshotfromanexampleoverviewofindividualanalysis
B. Visualization Module fixations observed within the region of the screen where the
video was displayed. Analysts can select a specific learner
The visualization charts crafted by VAAD provide a thor-
and customize their visual screen heat map by choosing
oughexaminationofthesession,providingadeepunderstand-
from various options, such as different activities within the
ingofbothglobaltrendsandindividuallearners’engagement.
MOOC session. This approach offers valuable insights into
These charts are available in both English and Spanish.
each learner’s focus during different activities, assisting in the
The general session overview is presented through interac-
identification of engaging materials and assessing the impact
tive box plot charts, providing a visual representation of the
of diverse learning sources on learners’ performance.
four parameters mentioned earlier, namely average saccades,
average saccade time, average fixations, and average fixation
C. Prediction Module
time. These charts can be filtered by different demographic
categories, such as groups or sex, allowing for a detailed The goal of the prediction module is to determine the
exploration of the data. Fig. 2 (a) illustrates an example of activity which the learner is doing. The identifier metric may
average saccade movements during reading, filtered by sex, indicate either video watching or course content reading. The
revealing a greater distribution among females. The same prediction module utilizes the metrics outlined in Table III to
pattern is observed for video watching in Fig. 2 (b). Ana- makeabinarypredictionbetweenreadingandvideowatching.
lysts have the option to visualize specific activities within For the prediction task, we worked with learners belonging
the MOOC or view the entire session at once, with each to groups 2 and 3 (80 learners), excluding those from group
task differentiated by a different color for enhanced clarity. 1 due to the high frequency of interruptions experienced by
Moreover, an ANOVA test is conducted for each of the four learnersinthisgroupduringtheLS.Additionally,weexcluded
parameters, identifying significant variations among learners some data from the reading category to balance our training
from different populations. dataset, ensuring an equal number of data points for each
Another interactive chart presents individual learners’ data category (524 samples for each category).
through a heat map, providing a visual representation of their WeusedtheRandomForestalgorithmandaneuralnetwork
on-screen attention. As depicted in the example shown in Fig. to test against the testing data. The neural network tested is
3, it is possible to identify precisely where the learner was a perceptron with one hidden layer (32 neurons and ReLU
looking during the video task, with a higher concentration of activation)andoneoutputlayer(sigmoidactivation).Thelossfunction chosen was Mean Squared Error (MSE) with Adam duringthelearningprocess.ThedatamanagedbyVAADalso
optimizer (default learning rate of 0.001). offer the opportunity to detect the tasks performed by online
We adopted two distinct approaches. Initially, we divided learners during the learning session, which can be valuable
the learners from groups 2 and 3 into 75% for training data information for instructors.
and25%fortestingdata.Subsequently,weusedbothRandom This tool has the potential to significantly enhance the
Forest and the neural network for analysis. (See Table IV) analysis of online learning behaviors and provide valuable
insights for educational practitioners, and is currently being
TABLEIV usedbytheMOOCinstructors’teamtogatherinformationon
RESULTSOFTESTINGTWOALGORITHMSONTHETESTINGDATAWITH learner visual behaviour while learning.
75%TRAININGAND25%TEST
Finally,futureworkwillfurtherexplorecurrentandemerg-
RandomForest NeuralNetwork ing predictive methods to ascertain the most suitable model
Accuracytest 0.72 0.73
for task prediction and investigate other indicators that have
Precision 0.62 0.67
Videowatching Recall 0.88 0.75 proven to be useful in predicting activities such as eyeblink
F1-Score 0.73 0.70 [29],[30],eyepupilsize[31],keystroking[32],amongothers.
Precision 0.88 0.80
Reading Recall 0.61 0.74
F1-Score 0.72 0.77 ACKNOWLEDGMENT
Support by projects: HumanCAIC (TED2021-131787B-I00
The second approach consisted of evaluating both methods
MICINN), SNOLA (RED2022-134284-T), e-Madrid-CM
using the Leave-One-Out Cross-Validation (LOOCV) tech-
(S2018/TCS-4307, a project which is co-funded by the
nique.
European Structural Funds, FSE and FEDER), IndiGo!
Followingthetestingofbothapproachesforbothpredictive
(PID2019-105951RB-I00), TEA360 (PID2023-150488OB-
methods, the results obtained after applying the LOOCV
technique were more promising as we don´t have a lot of I00, SPID202300X150488IV0) and BIO-PROCTORING
(GNOSS Program, Agreement Ministerio de Defensa-UAM-
data. The results obtained from the LOOCV technique are
FUAM dated 29-03-2022).Roberto Daza is supported by a
presented in Table V.
FPI fellowship from MINECO/FEDER.
TABLEV
RESULTSOFTESTINGTWOALGORITHMSONTHETESTINGDATAWITH REFERENCES
LOOCV
[1] F.J.Garc´ıa-Pen˜alvo,A´.Fidalgo-Blanco,andM.L.Sein-Echaluce,“Los
RandomForest NeuralNetwork mooc:Unana´lisisdesdeunaperspectivadelainnovacio´ninstitucional
Accuracytest 0.76 0.74 universitaria,”Tech.Rep.,2017.
Precision 0.76 0.74 [2] L. Ma and C. S. Lee, “Investigating the adoption of moocs: A
Videowatching Recall 0.79 0.76 technology–user–environment perspective,” Journal of Computer As-
F1-Score 0.77 0.75 sistedLearning,vol.35,no.1,pp.89–98,2019.
Precision 0.78 0.75 [3] R.Cobos,A.Wilde,andE.Zaluska,“Predictingattritionfrommassive
Reading Recall 0.75 0.73 open online courses in futurelearn and edx,” in Proceedings of the
F1-Score 0.76 0.74 7thInternationalLearningAnalyticsandKnowledgeConference,Simon
FraserUniversity,Vancouver,BC,Canada,2017,pp.13–17.
ForLOOCV,intermsofaccuracy,bothmethodsshowclose [4] C.Lang,G.Siemens,A.Wise,andD.Gasevic,Handbookoflearning
analytics. SOLAR,SocietyforLearningAnalyticsandResearchNew
similarity, with Random Forest slightly edging ahead. This
York,2017.
trend persists across the other metrics, with Random Forest [5] R.Cobos,“Self-regulatedlearningandactivefeedbackofmooclearners
consistently showing a slight advantage. supported by the intervention strategy of a learning analytics system,”
Electronics,vol.12,no.15,p.3368,2023.
Overall, while both models share similarities, Random For-
[6] R. Cobos and J. C. Ruiz-Garcia, “Improving learner engagement in
est shows a slight superiority. moocs using a learning intervention system: A research study in en-
gineeringeducation,”ComputerApplicationsinEngineeringEducation,
V. CONCLUSIONSANDFUTUREWORK vol.29,no.4,pp.733–749,2021.
[7] R. Cobos and J. Sobero´n, “A proposal for monitoring the intervention
In this paper, we introduce VAAD (Visual Attention Anal-
strategy on the learning of mooc learners,” in CEUR Conference
ysis Dashboard), an innovative tool designed to visualize Proceedings.LASI-Spain,2020.
biometric data related to visual attention gathered from an [8] I.PascualandR.Cobos,“Aproposalforpredictingandinterveningon
online learning session. This tool enables analysts to gain mooclearners’performanceinrealtime,”2022.
[9] A´.Becerra,R.Daza,R.Cobos,A.Morales,andJ.Fierrez,“Userexpe-
a deeper understanding of learner behavior by filtering and
riencestudyusingasystemforgeneratingmultimodallearninganalytics
visualizing different components of the session. dashboards,” in Proceedings of the XXIII International Conference on
It offers valuable insights into learners’ focus and en- HumanComputerInteraction,2023,pp.1–2.
[10] A´.Becerra,R.Daza,R.Cobos,A.Morales,M.Cukurova,andJ.Fier-
gagement through the analysis of eye movements. Moreover,
rez, “M2lads: A system for generating multimodal learning analytics
it provides the flexibility to filter data by various learner dashboards,” in 2023 IEEE 47th Annual Computers, Software, and
demographics, enabling a more detailed exploration of the ApplicationsConference(COMPSAC). IEEE,jun2023,pp.1564–1569.
[11] J. Hernandez-Ortega, R. Daza, A. Morales, J. Fierrez, and J. Ortega-
data. Additionally, the tool facilitates ANOVA tests, allowing
Garcia,“edbb:Biometricsandbehaviorforassessingremoteeducation,”
for the identification of significant differences among learners arXivpreprintarXiv:1912.04786,2019.[12] R. Daza, A. Morales, R. Tolosana, L. F. Gomez, J. Fierrez, and ference on Pervasive Technologies Related to Assistive Environments,
J. Ortega-Garcia, “edbb-demo: biometrics and behavior analysis for 2015,pp.1–8.
online educational platforms,” in Proceedings of the AAAI Conference [32] A. Morales, J. Fierrez, M. Gomez-Barrero, J. Ortega-Garcia, R. Daza,
onArtificialIntelligence,vol.37,no.13,2023,pp.16422–16424. J. V. Monaco, J. Montalva˜o, J. Canuto, and A. George, “KBOC:
[13] M. Saqr, O. Viberg, J. Nouri, and S. Oyelere, “Multimodal temporal Keystroke Biometrics Ongoing Competition,” in Proc. Intl. Conf. on
network analysis to improve learner support and teaching,” in 2020 BiometricsTheory,ApplicationsandSystems,2016,pp.1–6.
CrossMMLA in Practice: Collecting, Annotating and Analyzing Multi-
modalDataAcrossSpaces,CrossMMLA2020,24March2020. CEUR-
WS,2020,pp.30–33.
[14] D. Spikol, E. Ruffaldi, G. Dabisias, and M. Cukurova, “Supervised
machinelearninginmultimodallearninganalyticsforestimatingsuccess
in project-based learning,” Journal of Computer Assisted Learning,
vol.34,no.4,pp.366–377,2018.
[15] M. Giannakos, D. Spikol, D. Di Mitri, K. Sharma, X. Ochoa, and
R. Hammad, The multimodal learning analytics handbook. Springer
Nature,2022.
[16] K.Sharma,H.S.Alavi,P.Jermann,andP.Dillenbourg,“Agaze-based
learninganalyticsmodel:in-videovisualfeedbacktoimprovelearner’s
attentioninmoocs,”inProceedingsofthesixthinternationalconference
onlearninganalytics&knowledge,2016,pp.417–421.
[17] K. Sharma, S. D’Angelo, D. Gergle, and P. Dillenbourg, “Visual aug-
mentationofdeicticgesturesinmoocvideos.” Singapore:International
SocietyoftheLearningSciences,2016.
[18] D.C.Richardson,R.Dale,andN.Z.Kirkham,“Theartofconversation
is coordination,” Psychological science, vol. 18, no. 5, pp. 407–413,
2007.
[19] R.E.Mayer,“Uniquecontributionsofeye-trackingresearchtothestudy
oflearningwithgraphics,”Learningandinstruction,vol.20,no.2,pp.
167–171,2010.
[20] K.Verbert,E.Duval,J.Klerkx,S.Govaerts,andJ.L.Santos,“Learn-
ing analytics dashboard applications,” American Behavioral Scientist,
vol.57,no.10,pp.1500–1509,2013.
[21] K.Verbert,S.Govaerts,E.Duval,J.L.Santos,F.VanAssche,G.Parra,
and J. Klerkx, “Learning dashboards: an overview and future research
opportunities,”PersonalandUbiquitousComputing,vol.18,pp.1499–
1514,2014.
[22] R. Martinez Maldonado, J. Kay, K. Yacef, and B. Schwendimann,
“An interactive teacher’s dashboard for monitoring groups in a multi-
tabletop learning environment,” in Intelligent Tutoring Systems: 11th
InternationalConference,ITS2012,Chania,Crete,Greece,June14-18,
2012.Proceedings11. Springer,2012,pp.482–492.
[23] J.Andreu-Perez,C.Solnais,andK.Sriskandarajah,“Ealab(eyeactivity
lab):amatlabtoolboxforvariableextraction,multivariateanalysisand
classificationofeye-movementdata,”Neuroinformatics,vol.14,pp.51–
67,2016.
[24] S.Goodwin,A.Prouzeau,R.Whitelock-Jones,C.Hurter,L.Lawrence,
U. Afzal, and T. Dwyer, “Veta: Visual eye-tracking analytics for the
explorationofgazepatternsandbehaviours,”VisualInformatics,vol.6,
no.2,pp.1–13,2022.
[25] K. Sharma, V. Chavez-Demoulin, and P. Dillenbourg, “An application
of extreme value theory to learning analytics: predicting collaboration
outcomefromeye-trackingdata,”AvailableatSSRN2831445,2016.
[26] R.Daza,D.DeAlcala,A.Morales,R.Tolosana,R.Cobos,andJ.Fierrez,
“ALEBk: Feasibility Study of Attention Level Estimation Via Blink
DetectionAppliedtoe-learning,”inProc.AAAIWorkshoponArtificial
IntelligenceforEducation,2022.
[27] R.Daza,L.F.Gomez,A.Morales,J.Fierrez,R.Tolosana,R.Cobos,and
J.Ortega-Garcia,“MATT:MultimodalAttentionLevelEstimationfore-
learning Platforms,” in Proc. AAAI Workshop on Artificial Intelligence
forEducation,2023.
[28] J.Hernandez-Ortega,R.Daza,A.Morales,J.Fierrez,andR.Tolosana,
“Heart Rate Estimation from Face Videos for Student Assessment:
Experiments on edBB,” in Proc. Annual Computers, Software, and
ApplicationsConference(COMPSAC),2020,pp.172–177.
[29] R. Daza, A. Morales, J. Fierrez, and R. Tolosana, “mEBAL: A Multi-
modal Database for Eye Blink Detection and Attention Level Estima-
tion,”inProc.Intl.Conf.onMultimodalInteraction,2020,pp.32–36.
[30] R. Daza, A. Morales, J. Fierrez, R. Tolosana, and R. Vera-Rodriguez,
“mebal2 database and benchmark: Image-based multispectral eyeblink
detection,”PatternRecognitionLetters,vol.182,pp.83–89,2024.
[31] S.Rafiqi,C.Wangwiwattana,J.Kim,E.Fernandez,S.Nair,andE.C.
Larson, “PupilWare: Towards Pervasive Cognitive Load Measurement
usingCommodityDevices,”inProc.ofthe8thACMInternationalCon-