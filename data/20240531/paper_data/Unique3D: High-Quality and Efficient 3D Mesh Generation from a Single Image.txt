Unique3D: High-Quality and Efficient 3D Mesh
Generation from a Single Image
KailuWu FangfuLiu ZhihanCai RunjieYan
TsinghuaUniversity TsinghuaUniversity TsinghuaUniversity TsinghuaUniversity
HanyangWang YatingHu YueqiDuan KaishengMa†
TsinghuaUniversity AVARInc. TsinghuaUniversity TsinghuaUniversity
Figure1: GalleryofUnique3D.High-fidelityanddiversetexturedmeshgeneratedbyUnique3D
fromsingle-viewwildimageswithin30seconds.
Abstract
In this work, we introduce Unique3D, a novel image-to-3D framework for ef-
ficientlygeneratinghigh-quality3Dmeshesfromsingle-viewimages,featuring
state-of-the-artgenerationfidelityandstronggeneralizability. Previousmethods
basedonScoreDistillationSampling(SDS)canproducediversified3Dresultsby
†CorrespondingAuthor
Preprint.Underreview.
4202
yaM
03
]VC.sc[
1v34302.5042:viXradistilling3Dknowledgefromlarge2Ddiffusionmodel,buttheyusuallysuffer
fromlongper-caseoptimizationtimewithinconsistentissues. Recentworksad-
dresstheproblemandgeneratebetter3Dresultseitherbyfinetuningamulti-view
diffusionmodelortrainingafastfeed-forwardmodel. However, theystilllack
intricatetexturesandcomplexgeometriesduetoinconsistencyandlimitedgener-
atedresolution. Tosimultaneouslyachievehighfidelity,consistency,andefficiency
in single image-to-3D, we propose a novel framework Unique3D that includes
a multi-view diffusion model with a corresponding normal diffusion model to
generatemulti-viewimageswiththeirnormalmaps,amulti-levelupscaleprocess
toprogressivelyimprovetheresolutionofgeneratedorthographicmulti-views,as
wellasaninstantandconsistentmeshreconstructionalgorithmcalledISOMER,
which fully integrates the color and geometric priors into mesh results. Exten-
siveexperimentsdemonstratethatourUnique3Dsignificantlyoutperformsother
image-to-3Dbaselinesintermsofgeometricandtexturaldetails. Projectpage:
https://wukailu.github.io/Unique3D/.
1 Introduction
Automaticallygeneratingdiverseandhigh-quality3Dcontentfromsingle-viewimagesisafunda-
mental task in 3D Computer Vision [1, 2, 3, 4, 5], which can facilitate a wide range of versatile
applications[6,7],includinggaming,architecture,art,andanimation. However,thistaskischalleng-
ingandill-posedduetotheunderlyingambiguityof3Dgeometryinasingleview.
Recently,therapiddevelopmentofdiffusionmodels[8,9,10]hasopenedupnewperspectivesfor
3Dcontentcreation. Poweredbythestrongpriorof2Dimagediffusionmodels,DreamFusion[11]
proposesScoreDistillationSampling(SDS)toaddressthelimitationof3Ddatabydistilling3D
knowledgefrom2Ddiffusions[12],inspiringtheprogressofSDS-based2Dliftingmethods[13,
14,15,16,17]. Despitetheirdiversifiedcompellingresults,theyusuallysufferfromlongper-case
optimizationtimeforhours,poorgeometry,andinconsistentissues(e.g.,,Janusproblem[11]),thus
notpracticalforreal-worldapplications. Toovercometheproblems,aseriesofworksleveragelarger-
scaleopen-world3Ddatasets[18,19,20]eithertofine-tuneamulti-viewdiffusionmodel[3,21,22]
andrecoverthe3Dshapesfromthegeneratedmulti-viewimagesortrainalargereconstructionmodel
(LRM)[2, 23, 4, 5]by directlymappingimage tokensinto3D representations(e.g., , triplaneor
3DGaussian[24]). However,duetolocalinconsistencyinmeshoptimization[3,25]andlimited
resolutionofthegenerativeprocesswithexpensivecomputationaloverhead[2,5],theystruggleto
produceintricatetexturesandcomplexgeometricdetailswithhighresolution.
Inthispaper,wepresentanovelimage-to-3Dframeworkforefficient3Dmeshgeneration,coined
Unique3D,toaddresstheabovechallengesandsimultaneouslyachievehigh-fidelity,consistency,
andgeneralizability. Givenaninputimage,Unique3Dfirstgeneratesorthographicmulti-viewimages
fromamulti-viewdiffusionmodel. Thenweintroduceamulti-levelupscalestrategytoprogressively
improvetheresolutionofgeneratedmulti-viewimageswiththeircorrespondingnormalmapsfroma
normaldiffusionmodel. Finally,weproposeaninstantandconsistentmeshreconstruction(ISOMER)
algorithmtoreconstructhigh-quality3DmeshesfromthemultipleRGBimagesandnormalmaps,
whichfullyintegratesthecolorandgeometricpriorsintomeshresults. Bothdiffusionmodelsare
trained on a filtered version of the Objaverse dataset [18] with ∼ 50k 3D data. To enhance the
qualityandrobustness,wedesignaseriesofstrategiesintoourframework,includingthenoiseoffset
channelinthemulti-viewdiffusiontrainingprocesstocorrectthediscrepancybetweentrainingand
inference[26], astricterdatasetfilteringpolicy, andanexpansionregularizationtoavoidnormal
collapseinmeshreconstruction. Overall,ourmethodcangeneratehigh-fidelity,diverse,andmulti-
viewconsistentmeshesfromsingle-viewwildimageswithin30seconds,asshowninFigure1.
Weconductextensiveexperimentsonvariouswild2Dimageswithdifferentstyles. Theexperiments
verifytheefficacyofourframeworkandshowthatourUnique3Doutperformsexistingmethodsfor
highfidelity,geometricdetails,highresolution,andstronggeneralizability.
Insummary,ourcontributionsare:
• Weproposeanovelimage-to-3DframeworkcalledUnique3Dthatholisticallyarchivesaleading
levelofhigh-fidelity,efficiency,andgeneralizabilityamongcurrentmethods.
2• Weintroduceamulti-levelupscalestrategytoprogressivelygeneratehigher-resolutionRGBimages
withthecorrespondingnormalmaps.
• Wedesignanovelinstantandconsistentmeshreconstructionalgorithm(ISOMER)toreconstruct
3DmesheswithintricategeometricdetailsandtexturefromRGBimagesandnormalmaps.
• Extensiveexperimentsonimage-to-3Dtasksdemonstratetheefficacyandgenerationfidelityofour
method,unlockingnewpossibilitiesforreal-worlddeploymentinthefieldof3DgenerativeAI.
2 RelatedWork
MeshReconstruction. Despitethesignificantadvancementsinvarious3Drepresentations(e.g.,,
SDF[27,28],NeRF[29,30],3DGaussian[24]),meshesremainthemostwidelyused3Dformatin
popular3Dengines(e.g.,,Blender,Maya)withamaturerenderingpipeline. Reconstructinghigh-
quality3Dmeshesefficientlyfrommulti-vieworsingle-viewimagesisadauntingtaskingraphics
and3Dcomputervision. Earlyapproachesusuallyadoptalaboriousandcomplexphotogrammetry
pipelinewithmultiplestages,withtechniqueslikeStructurefrommotion(SfM)[31,32,33],Multi-
View Stereo (MVS) [34, 35], and mesh surface extraction [36, 37]. Powered by deep learning
andpowerfulGPUs, recentworks[38,39,40,41,2,4,23]havebeenproposedtopursuehigher
efficiencyandqualitywithgradient-basedmeshoptimizationoreventrainingalargefeed-forward
reconstructionnetwork. However,theirpipelinestillsuffersfromheavycomputationalcostsand
strugglestoadapttocomplexgeometry. Tobalanceefficiencyandquality,weproposeanovelinstant
andhigh-qualitymeshreconstructionalgorithminthispaperthatcanreconstructcomplex3Dmeshes
withintricategeometricdetailsfromsparseviews.
ScoreDistillationfor3DGeneration. Recently,data-drivenlarge-scale2Ddiffusionmodelshave
achievednotablesuccessinimageandvideogeneration[10,12,42,43]. However,transferringit
to3Dgenerationisnon-trivialduetocuratinglarge-scale3Ddatasets. PioneeringworksDreamFu-
sion[11]proposesScoreDistillationSampling(SDS)(alsoknownasScoreJacobianChaining[44])
todistill3Dgeometryandappearancefrompretrained2Ddiffusionmodelswhenrenderedfrom
different viewpoints. The following works continue to enhance various aspects such as fidelity,
promptalignment,consistency,andfurtherapplications[13,14,15,17,45,46,47]. However,such
optimization-based2Dliftingmethodsarelimitedbylongper-caseoptimizationtimeandmulti-face
problem[48]duetolackofexplicit3Dprior. AsZero123[49]provesthatStableDiffusion[10]
canbefinetunedtogeneratenovelviewsbyconditioningonrelativecameraposes,one-2-3-45[50]
directlyproduceplausible3DshapesfromgeneratedimagesinZero123. Thoughitachieveshigh
efficiency,thegeneratedresultsshowpoorqualitywithalackoftexturedetailsand3Dconsistency.
Multi-viewDiffusionModelsfor3DGeneration. Toachieveefficientand3Dconsistentresults,
someworks[3,21,51,22,48]fine-tunethe2Ddiffusionmodelswithlarge-scale3Ddata[18]to
generatemulti-viewconsistentimagesandthencreate3Dcontentsusingsparseviewreconstruction.
Forexample,SyncDreamer[21]leveragesattentionlayerstoproduceconsistentmulti-viewcolor
imagesandthenuseNeuS[52]forreconstruction. Wonder3D[3]explicitlyencodesthegeometric
informationinto3Dresultsandimprovesqualitybycross-domaindiffusion. Althoughthesemethods
generatereasonableresults,theyarestilllimitedbylocalinconsistencyfrommulti-viewsgenerated
byout-domaininputimagesandlimitedgeneratedresolutionfromthearchitecturedesign,producing
coarseresultswithouthigh-resolutiontexturesandgeometries. Incontrast,ourmethodcangenerate
higher-qualitytextured3Dmesheswithmorecomplexgeometricdetailswithinjust30seconds.
3 Method
Inthissection,weintroduceourframework,i.e.,Unique3D,forhigh-fidelity,efficient,andgeneral-
izable3Dmeshgenerationfromasinglewildimage. Givenaninputimage,wefirstgeneratefour
orthographicmulti-viewimageswiththeircorrespondingnormalmapsfromamulti-viewdiffusion
model and a normal diffusion model. Then, we lift them to high-resolution space progressively
(Sec3.1). Givenhigh-resolutionmulti-viewRGBimagesandnormalmaps,wefinallyreconstruct
high-quality 3D meshes with our instant and consistent mesh reconstruction algorithm ISOMER
(Sec3.2). AnoverviewofourframeworkisdepictedinFigure2.
3×N ×N 20482 ×N 20482
2562
Input Image Multi-view Images Upscaled Multi-view Images Multi-view Normal Maps
Textured Mesh Refined Mesh Initialized Mesh ISOMER
Figure 2: Pipeline of our Unique3D. Given a single wild image as input, we first generate four
orthographicmulti-viewimagesfromamulti-viewdiffusionmodel. Then,weprogressivelyimprove
theresolutionofgeneratedmulti-viewsthroughamulti-levelupscaleprocess. Givengeneratedcolor
images,wetrainanormaldiffusionmodeltogeneratenormalmapscorrespondingtomulti-view
images and utilize a similar strategy to lift it to high-resolution space. Finally, we reconstruct
high-quality3Dmeshesfromhigh-resolutioncolorimagesandnormalmapswithourinstantand
consistentmeshreconstructionalgorithmISOMER.
3.1 High-resolutionMulti-viewGeneration
Wefirstexplainthedesignofourhigh-resolutionmulti-viewgenerationmodelthatgeneratesfour
orthographicviewimagesfromasingleinputimage.Insteadofdirectlytrainingahigh-resolution(2K)
multi-viewdiffusionthatwouldconsumeexcessivecomputationalresources,weadoptamulti-level
generationstrategytoupscalethegeneratedresolutionprogressively.
High-resolutionMulti-viewImageGeneration. Insteadoftrainingfromscratch,westartwiththe
initializationofthepre-trained2DdiffusionmodelusingthecheckpointofStableDiffusion[53]
and encode multi-view dependencies to fine-tune it to obtain a multi-view diffusion model that
is able to generate four orthographic view images (256 resolution) from a single wild image. It
is worth noting that the images generated in this step have relatively low resolution and suffer
frommulti-viewinconsistencyinout-of-the-domaindata. Thissignificantlylimitsthequalityof
recentworks[23,4,5,3,51]. Incontrast,weaddressthemulti-viewconsistencyissueduringthe
reconstructionphase(Sec3.2).Giventhegeneratedfourorthographicviewimages,wethenfinetunea
multi-viewawareControlNet[54]toimprovetheresolutionofimages. Thismodelleveragesthefour
collocatedRGBimagesascontrolinformationtogeneratecorrespondingclearerandmoreprecise
multi-viewresults. Itenhancesthedetailsandamelioratesunclearregions,leadingtheresolutionof
imagesfrom256to512. Finally,weemployasingle-viewsuper-resolutionmodel[55]tofurther
upscaletheimagebyafactoroffour,achievingaresolutionof2048thatofferssharperedgesand
detailswithoutdisruptingthemulti-viewconsistency.
High-resolutionNormalMapPrediction. UsingpureRGBimagesmakesitextremelyhardto
reconstructcorrectgeometry. Toeffectivelycapturetherichsurfacedetailsofthetarget3Dshape,we
finetuneanormaldiffusionmodeltopredictnormalmapscorrespondingtomulti-viewcolorimages.
Similartotheabovehigh-resolutionimagegenerationstage,wealsoemploythesuper-resolution
model[55]toquadruplethenormalresolution,whichenablesourmethodtorecoverhigh-fidelity
geometricdetails,especiallytheaccuracyoftheedges.
Toenhancethecapabilityoftheimagegenerationmodelandthestandardnormalpredictionmodel
inproducinghigh-qualityimageswithuniformbackgrounds,weadoptachannel-wisenoiseoffset
strategy[56]. ThiscanalleviatetheproblemcausedbythediscrepancybetweentheinitialGaussian
noiseduringsamplingandthenoisiesttrainingsample.
4
weiv-itluM noisuffiD
hseM
eziroloC
weiv-itluM
elacspU
tnetsisnoC
RS
x4
tcurtsnoceR
hseM
enifeR
&
lamroN noisuffiD
RS
x4
hseM
ezilaitinI3.2 ISOMER:AnEfficientMethodforDirectMeshReconstruction
Despiteimpressiveresultsgeneratedbyrecentpopularimage-to-3Dmethods[3,57,5,2,4]that
followthefield-basedreconstruction[38,39,58],theyhavelimitedpotentialforhigher-resolution
applicationsastheircomputationalloadisproportionaltothecubeofthespatialresolution.Incontrast,
wedesignanovelreconstructionalgorithmdirectlybasedonmesh,wherethecomputationalload
scaleswithonlythesquareofthespatialresolutionandrelatestothenumberoffaces,thusachieving
afundamentalimprovement. Thisenablesourmodeltoefficientlyreconstructmesheswithtensof
millionsoffaceswithinseconds.
We now move to introduce our instant and consistent mesh reconstruction algorithm (ISOMER),
whichisarobust,accurate,andefficientapproachfordirectmeshreconstructionfromhigh-resolution
multi-viewimages. Specifically,theISOMERconsistsofthreemainsteps: (a)estimatingtherough
topological structure of the 3D object and generating an initial mesh directly; (b) employing a
coarse-to-finestrategytofurtherapproximatethetargetshape;(c)explicitlyaddressinginconsistency
across multiple views to reconstruct high-fidelity and intricate details. Notably, the entire mesh
reconstructionprocesstakesnomorethan10seconds.
InitialMeshEstimation.Unlikepopularreconstructionmethodsbasedonsigneddistancefields[59]
oroccupancyfields[29],mesh-basedreconstructionmethods[60,61]strugglewithchangingtopo-
logical connectivity during optimization, which requires correct topological construction during
initialization. AlthoughinitialmeshestimationcanbeobtainedbyexistingmethodslikeDMTet[38],
theycannotaccuratelyreconstructprecisedetails(e.g.,,smallholesorgaps). Toaddresstheproblem,
weutilizefrontandbackviewstodirectlyestimatetheinitialmesh,whichisfastforaccuraterecovery
ofalltopologicallyconnectedcomponentsvisiblefromthefront.Specifically,weintegratethenormal
mapfromthefrontalviewtoobtainadepthmapby
(cid:90) i i
(cid:88)
d(i,j)= ⃗n(x)·d⃗x≈ n (t). (1)
x
0 t=0
Althoughthediffusionprocessgeneratespseudonormalmaps,thesemapsdonotyieldarealnormal
fieldwhichisirrotational. Toaddressthis,weintroducearandomrotationtothenormalmapbefore
integration. Theprocessisrepeatedseveraltimes,andthemeanvalueoftheseintegrationsisthen
utilizedtocalculatethedepth,providingareliableestimation. Subsequently,wemapeachpixelto
itsrespectivespatiallocationusingtheestimateddepth,creatingmeshmodelsfromboththefront
andbackviewsoftheobject. ThetwomodelsareseamlesslyjoinedthroughPoissonreconstruction,
whichguaranteesasmoothconnectionbetweenthem. Finally,wesimplifytheminto2000fewer
facesforourmeshinitialization.
Coarse-to-FineMeshOptimization. Buildingupontheresearchininverserendering[62,63,64],
weiterativelyoptimizethemeshmodeltominimizealossfunction. Duringeachoptimizationstep,
themeshundergoesdifferentiablerenderingtocomputethelossandgradients,followedbyvertex
movementaccording tothe gradients. Finally, themesh iscorrected afteriteration throughedge
collapse,edgesplit,andedgefliptomaintainauniformfacedistributionandreasonableedgelengths.
Afterseveralhundredcoarse-to-fineiterations,themodelconvergestoaroughapproximationofthe
targetobject’sshape. Thelossfunctionforthispartincludesamask-basedloss
L
=(cid:88)(cid:13)
(cid:13)Mˆ
−Mpred(cid:13) (cid:13)2
, (2)
mask (cid:13) i i (cid:13)
2
i
where Mˆ is the rendered mask under view i and Mpred is the predicted mask from previous
i i
subsectionunderviewi. Themask-basedlossregulatesthemeshcontour. Additionally,itincludesa
normal-basedloss
L =(cid:88) Mpred⊗(cid:13) (cid:13)Nˆ −Npred(cid:13) (cid:13)2 , (3)
normal i (cid:13) i i (cid:13)
2
i
concerningtherenderednormalmapNˆ oftheobjectandthepredictednormalmapNpred,optimizing
i i
thenormaldirectioninthevisibleareas,where⊗denoteselement-wiseproduction. Wecomputethe
finallossfunctionas:
L =L +L . (4)
recon mask normal
To address potential surface collapse issues under limited-view normal supervision as shown in
Figure3-(b),weemployaregularizationmethodcalledExpansion. Ateachstep,verticesaremoved
asmalldistanceinthedirectionoftheirnormals,akintoweightdecay.
5ExplicitTargetOptimizationforMulti-viewInconsistencyandGeometricRefinement. Dueto
inherentinconsistenciesingeneratedmulti-viewimagesfromout-of-distribution(OOD)wildinput,
nosolutioncanperfectlyalignwitheveryviewpoint. Aftertheabovesteps,wecanonlyreconstruct
amodelthatroughlymatchestheshapebutlacksdetail,fallingshortofourpursuitofhigh-quality
mesh. Therefore,wecannotusethecommonmethodthatminimizesdifferencesinallviews,which
wouldleadtosignificantwave-patternflaws,asshowninFigure3-(a). Toovercomethischallenge,
findingamoresuitableoptimizationtargetbecomescrucial. Undersingle-viewsupervision,although
acompletemodelcannotbereconstructed,themeshshapewithinthevisibleareaofthatviewcan
meetthesupervisionrequirementswithhighlydetailedstructures. Basedonthis,weproposeanovel
methodthatassignsauniqueoptimizationtargetforeachvertextoguidetheoptimizationdirection.
Incontrasttotheconventionalimplicituseofmulti-viewimagesasoptimizationtargets,weexplicitly
define the optimization target with better robustness. We call this explicit optimization target as
ExplicitTargetanddeviseitasfollows:
(ExplicitTarget). LetP(v,i) : (R3,N+) → R2 betheimagespacecoordinatesofaspatialpoint
v in view i, Col(p,I ) : (R2,RH×W×3) → R3 denote the color of point p in image I , and
m m
V (v,i) : (N+,N+) → {0,1} represent the visibility of vertex v in mesh M under view i. We
M
computetheExplicitTargetET ofmeshM asamappingfunctionfromthesetofverticesinM toa
setofcolors:
(cid:40)(cid:80) i∈IVM(v,i)WM(v,i)2Col(P(v,i),I m(i))
,if
(cid:80)
V (v,I)>0
ET M(I,I m,v)= (cid:80) i∈IVM(v,i) i∈I M (5)
0 ,otherwise,
where I refer to the set of sampling views, I are images corresponding to the views, and
m
W (v,i) = −cos(N(M),N(view)) is a weighting factor, where N(M) is the vertex normal of
M v i v
vinmeshM,andN(view)istheviewdirectionofviewi.
i
InthefunctionET (I,I ),theresultforvertexviscomputedastheweightedsumofsupervised
M m
views,withweightsdeterminedbythesquareofcosineangles. Thisisbecausetheprojectedarea
isdirectlyproportionaltothecosinevalue,andthepredictionaccuracyisalsopositivelycorrelated
withthecosinevalue. TheobjectlossfunctionforExplicitTargetisdefinedas
L =(cid:88) Mpred⊗(cid:13) (cid:13)Nˆ −NET(cid:13) (cid:13)2 , (6)
ET i (cid:13) i i (cid:13)
2
i
where NET is the rendering result of mesh M with {ET (I,Npred,v)|v ∈ M} under the i-th
i M
viewpoint. Thefinaloptimizationlossfunctionis
L =L +L . (7)
refine mask ET
Towardsthisend,wefinishtheintroductionoftheISOMERreconstructionprocess,whichincludes
threestages: Initialization,Reconstruction,andRefinement.
Upongeneratingprecisegeometricstructures,itisnecessarytocolorizethembasedonmulti-view
images. Giventheinconsistenciesacrossmulti-viewimages,thecolorizingprocessadoptsthesame
methodusedintherefinementstage. Specifically,thecolorsofmeshM is{ET (I,Ipred,v)|v ∈
M rgb
M}. Moreover, certain regions of the model may remain unobservable from the multi-view per-
spective,necessitatingthecoloringoftheseinvisibleareas. Toaddressthis,weutilizeanefficient
smoothingcoloringalgorithmtocompletethetask.Moredetailedandspecificalgorithmicprocedures
canbefoundintheAppendix.
4 Experiments
4.1 ExperimentalSetting
Dataset: UtilizingasubsetoftheObjaversedatasetasdelineatedbyLGM[65],weapplyarigorous
filtrationprocesstoexcludescenescontainingmultipleobjects,low-resolutionimagery,andunidirec-
tionalfaces,leadingtoarefineddatasetofapproximately50kobjects. Toaddresssurfaceswithout
thickness,werendereightorthographicprojectionsaroundeachobjecthorizontally. Byexamining
theepipolarlinescorrespondingtoeachhorizontalray,weidentify13kinstancesofillegitimatedata.
Forrendering,weemployrandomenvironmentmapsandlightingtoaugmentthedataset,thereby
6Figure3: QualitativeComparison. Ourapproachprovidessuperiorgeometryandtexture.
enhancingthemodel’srobustness. Toensurehigh-qualitygeneration,allimagesarerenderedata
resolutionof2048×2048pixels.
NetworkArchitecture: Theinitiallevelofimagegenerationisinitializedwiththeweightofthe
Stable Diffusion Image Variations Model [53], while the subsequent level employs an upscaled
versionfine-tunedfromControlNet-Tile[54]. Thefinalstageusesthepre-trainedReal-ESRGAN
model[55].Similarly,theinitialstageofnormalmappredictionisinitializedfromtheaforementioned
StableDiffusionImageVariations. DetailsofthesenetworksareprovidedintheAppendix.
7
egamI
tupnI
sruO
MRG
hseMtnatsnI
MRC
remaerDcnyS
MRLnepO
D3rednoW
54-3-2-enOInput Image Ours InstantMesh CRM OpenLRM
Figure 4: Detailed Comparison. We compare our model with InstantMesh [5], CRM [4] and
OpenLRM[2]. Ourmodelsgeneratesaccurategeometryanddetailedtexture.
Reconstruction Details: The preliminary mesh structure is inferred from a normal map with a
resolutionof256×256,whichisthensimplifiedtoameshcomprising2,000faces.Thereconstruction
processinvolves300iterationsusingtheSGDoptimizer[66],withalearningrateof0.3. Theweight
ofexpansionregularizationissetto0.1. Subsequentrefinementtakes100iterations,maintainingthe
sameoptimizationparameters.
Training Details: The entire training takes around 4 days on 8 NVIDIA RTX4090 GPUs. The
primarylevelofmultiviewimagegenerationuses30ktrainingiterationswithabatchsizeof1,024.
Thesecondleveloftraininginvolves10kiterationswithabatchsizeof128. Normalmapprediction
istrainedfor10kiterationsatabatchsizeof128. Additionaltrainingspecificsareaccessibleinthe
Appendix.
4.2 Comparisons
QualitativeComparison: Tohighlighttheadvantagesofourmethodology,weperformacompre-
hensivecomparisonwithexistingworks,includingCRM[4],one-2-3-45[50],SyncDreamer[21],
Wonder3D [3], OpenLRM [2], InstantMesh [5], and GRM [23]. For a fair quality comparison,
wechoosetopresentsamplespreviouslyselectedinthereferencedpapers,originatingfromWon-
der3D[3],SyncDreamer[21],CRM[4],andInstantMesh[5]. TheresultsareshowninFigure3. Our
resultsclearlysurpasstheexistingworksinbothgeometricandmaterialquality,therebyemphasizing
thebenefitsofourapproachinachievinghighresolutionandintricatedetailsinbothgeometryand
material. Inadditiontotheaboveoverallqualitycomparison,wefurthershowthecomparisonofthe
detailsinFigure4,highlightingtheadvantageofourmethodinhighresolution. Thereconstruction
processofISOMERiscompletedinunder10seconds,whiletheentireprocedurefromtheinput
imagetohigh-precisionmeshisaccomplishedinlessthan30secondsonanRTX4090.
QuantitativeComparison: Inlinewithpreviouswork,weevaluateourresultsusingtheGoogle
ScannedObjects(GSO)[68]dataset. Werandomlyselect30objectsandrenderfrontalviewsata
resolutionof1024×1024asinputforallmethods. Allgeneratedmeshresultsarenormalizedto
theboundingbox[−0.5,0.5]toensurealignment. Thegeometricqualityisassessedbycalculating
thedistancetothegroundtruthmeshusingmetricssuchasChamferDistance(CD),VolumeIoU,
andF-Score. Concurrently,werender24viewsaroundtheobject,selectingoneof[0,15,30]for
elevationanglesand8evenlydistributedazimuthanglesspanningafull360-degreerotation. We
8Table1: Quantitativecomparisonresultsformeshvisualandgeometryquality. Wereportthemetrics
ofPSNR,SSIM,LPIPSandClip-Similarity[67],ChamferDistance(CD),VolumeIoUandF-score
onGSO[68]dataset.
Method PSNR↑ SSIM↑ LPIPS↓ Clip-Sim↑ CD↓ Vol.IoU↑ F-Score↑
One-2-3-45[50] 13.19 0.7231 0.3795 0.7792 0.0246 0.4647 0.6025
OpenLRM[2] 14.73 0.7722 0.3294 0.8607 0.0229 0.4452 0.5907
SyncDreamer[21] 13.13 0.7262 0.4036 0.7828 0.0207 0.4220 0.5689
Wonder3D[3] 15.66 0.7898 0.3006 0.8612 0.0184 0.5199 0.6513
InstantMesh[5] 15.30 0.7809 0.3028 0.8998 0.0164 0.5141 0.6421
GRM[23] 14.50 0.7676 0.3332 0.8820 0.0161 0.4768 0.6178
CRM[4] 16.45 0.7965 0.2750 0.8936 0.0156 0.5379 0.6697
Ours 16.48 0.8007 0.2624 0.9096 0.0145 0.5538 0.6845
(a) Input w/o Explicit Target w/ Explicit Target (b) Input w/o Expansion w/ Expansion
Figure 5: Ablation Study on ISOMER. (a) Without ExplicitTarget, the output mesh result has
obviousdefects. (b)Withoutexpansionregularization,theoutputresultcollapsesinsomecases.
employPSNR,SSIM,LPIPS,andClip-Similarity[67]toevaluatethevisualquality. Theresultsare
presentedinTable1. Asevidencedintable,bothourgeometricandmaterialqualitysignificantly
outperformthoseofexistingmethods.
4.3 AblationStudyandDisscussion
Ablation Study: We analyze the importance of ExplicitTarget and expansion regularization in
ISOMER.WecomparesampleswithandwithoutExplicitTargetandExpansionRegularizationin
figure 5. We clearly show the improvement of ExplicitTarget for geometry and the necessity of
expansionregularizationforreconstruction. ExplicitTargetnotablyimprovesreconstructionresultsin
challengingcases,whileexpansionregularizationavoidssomepossiblecollapses.
5 Conclusion
Inthispaper,weintroduceUnique3D,apioneeringimage-to-3Dframeworkthatefficientlygenerates
high-quality3Dmeshesfromsingle-viewimageswithunprecedentedfidelityandconsistency. By
integratingadvanceddiffusionmodelsandthepowerfulreconstructionmethodISOMER,Unique3D
generatesdetailedandtexturedmesheswithin30seconds,significantlyadvancingthestate-of-the-art
in3Dcontentcreationfromsingleimages.
Limitation and Future Works. Our method, while capable of generating high-fidelity textured
meshesrapidly,faceschallenges. Themulti-viewpredictionmodelmayproducelesssatisfactory
predictionsforskewedornon-perspectiveinputs. Furthermore,thegeometriccoloringalgorithm
currently does not support texture maps. In the future, we aim to enhance the robustness of the
multi-viewpredictionmodelbytrainingonamoreextensiveanddiversedataset.
9References
[1] HeewooJunandAlexNichol. Shap-e: Generatingconditional3dimplicitfunctions. arXivpreprint
arXiv:2305.02463,2023.
[2] YicongHong,KaiZhang,JiuxiangGu,SaiBi,YangZhou,DifanLiu,FengLiu,KalyanSunkavalli,Trung
Bui,andHaoTan. LRM:largereconstructionmodelforsingleimageto3d. CoRR,abs/2311.04400,2023.
[3] XiaoxiaoLong,Yuan-ChenGuo,ChengLin,YuanLiu,ZhiyangDou,LingjieLiu,YuexinMa,Song-Hai
Zhang,MarcHabermann,ChristianTheobalt,etal. Wonder3d:Singleimageto3dusingcross-domain
diffusion. arXivpreprintarXiv:2310.15008,2023.
[4] ZhengyiWang,YikaiWang,YifeiChen,ChendongXiang,ShuoChen,DajiangYu,ChongxuanLi,Hang
Su,andJunZhu. CRM:singleimageto3dtexturedmeshwithconvolutionalreconstructionmodel. CoRR,
abs/2403.05034,2024.
[5] JialeXu,WeihaoCheng,YimingGao,XintaoWang,ShenghuaGao,andYingShan.Instantmesh:Efficient
3dmeshgenerationfromasingleimagewithsparse-viewlargereconstructionmodels. arXivpreprint
arXiv:2404.07191,2024.
[6] JianLiu,XiaoshuiHuang,TianyuHuang,LuChen,YuenanHou,ShixiangTang,ZiweiLiu,WanliOuyang,
WangmengZuo,JunjunJiang,etal. Acomprehensivesurveyon3dcontentgeneration. arXivpreprint
arXiv:2402.01166,2024.
[7] ZifanShi,SidaPeng,YinghaoXu,YiyiLiao,andYujunShen. Deepgenerativemodelson3drepresenta-
tions:Asurvey. CoRR,abs/2210.15663,2022.
[8] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InHugoLarochelle,
Marc’AurelioRanzato,RaiaHadsell,Maria-FlorinaBalcan,andHsuan-TienLin,editors,Advancesin
NeuralInformationProcessingSystems33:AnnualConferenceonNeuralInformationProcessingSystems
2020,NeurIPS2020,December6-12,2020,virtual,2020.
[9] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. In9thInter-
nationalConferenceonLearningRepresentations,ICLR2021,VirtualEvent,Austria,May3-7,2021.
OpenReview.net,2021.
[10] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10684–10695,2022.
[11] BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall.Dreamfusion:Text-to-3dusing2ddiffusion.
InTheEleventhInternationalConferenceonLearningRepresentations,ICLR2023,Kigali,Rwanda,May
1-5,2023.OpenReview.net,2023.
[12] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyL.Denton,SeyedKam-
yar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho,
DavidJ.Fleet,andMohammadNorouzi. Photorealistictext-to-imagediffusionmodelswithdeeplanguage
understanding. InNeurIPS,2022.
[13] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,XiaohuiZeng,XunHuang,KarstenKreis,
SanjaFidler, Ming-YuLiu,andTsung-YiLin. Magic3d: High-resolutiontext-to-3dcontentcreation.
CoRR,abs/2211.10440,2022.
[14] GuochengQian,JinjieMai,AbdullahHamdi,JianRen,AliaksandrSiarohin,BingLi,Hsin-YingLee,
IvanSkorokhodov,PeterWonka,SergeyTulyakov,etal. Magic123:Oneimagetohigh-quality3dobject
generationusingboth2dand3ddiffusionpriors. arXivpreprintarXiv:2306.17843,2023.
[15] ZhengyiWang,ChengLu,YikaiWang,FanBao,ChongxuanLi,HangSu,andJunZhu. Prolificdreamer:
High-fidelityanddiversetext-to-3dgenerationwithvariationalscoredistillation. CoRR,abs/2305.16213,
2023.
[16] FangfuLiu,DiankunWu,YiWei,YongmingRao,andYueqiDuan. Sherpa3d: Boostinghigh-fidelity
text-to-3dgenerationviacoarse3dprior. arXivpreprintarXiv:2312.06655,2023.
[17] RuiChen,YongweiChen,NingxinJiao,andKuiJia. Fantasia3d:Disentanglinggeometryandappearance
forhigh-qualitytext-to-3dcontentcreation. CoRR,abs/2303.13873,2023.
[18] MattDeitke,DustinSchwenk,JordiSalvador,LucaWeihs,OscarMichel,EliVanderBilt,LudwigSchmidt,
KianaEhsani,AniruddhaKembhavi,andAliFarhadi. Objaverse:Auniverseofannotated3dobjects. In
IEEE/CVFConferenceonComputerVisionandPatternRecognition,CVPR2023,Vancouver,BC,Canada,
June17-24,2023,pages13142–13153.IEEE,2023.
[19] AngelXChang,ThomasFunkhouser,LeonidasGuibas,PatHanrahan,QixingHuang,ZimoLi,Silvio
Savarese,ManolisSavva,ShuranSong,HaoSu,etal. Shapenet:Aninformation-rich3dmodelrepository.
arXivpreprintarXiv:1512.03012,2015.
10[20] MattDeitke,RuoshiLiu,MatthewWallingford,HuongNgo,OscarMichel,AdityaKusupati,AlanFan,
ChristianLaforte,VikramVoleti,SamirYitzhakGadre,etal. Objaverse-xl:Auniverseof10m+3dobjects.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[21] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang.
Syncdreamer: Generating multiview-consistent images from a single-view image. arXiv preprint
arXiv:2309.03453,2023.
[22] PengWangandYichunShi. Imagedream:Image-promptmulti-viewdiffusionfor3dgeneration. CoRR,
abs/2312.02201,2023.
[23] YinghaoXu,ZifanShi,WangYifan,HanshengChen,CeyuanYang,SidaPeng,YujunShen,andGordon
Wetzstein.GRM:largegaussianreconstructionmodelforefficient3dreconstructionandgeneration.CoRR,
abs/2403.14621,2024.
[24] BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis. 3dgaussiansplattingfor
real-timeradiancefieldrendering. ACMTrans.Graph.,42(4):139:1–139:14,2023.
[25] FanYang,JianfengZhang,YichunShi,BowenChen,ChenxuZhang,HuichaoZhang,XiaofengYang,
JiashiFeng,andGuoshengLin. Magic-boost:Boost3dgenerationwithmutli-viewconditioneddiffusion.
arXivpreprintarXiv:2404.06429,2024.
[26] ShanchuanLin,BingchenLiu,JiashiLi,andXiaoYang. Commondiffusionnoiseschedulesandsample
stepsareflawed. InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision,
pages5404–5411,2024.
[27] Wikipedia. Signed distance function — Wikipedia, the free encyclopedia. http://en.wikipedia.
org/w/index.php?title=Signed%20distance%20function&oldid=1189894340,2024. [Online;
accessed05-May-2024].
[28] JeongJoonPark,PeterFlorence,JulianStraub,RichardA.Newcombe,andStevenLovegrove. Deepsdf:
Learningcontinuoussigneddistancefunctionsforshaperepresentation. InIEEEConferenceonComputer
VisionandPatternRecognition,CVPR2019,LongBeach,CA,USA,June16-20,2019,pages165–174.
ComputerVisionFoundation/IEEE,2019.
[29] BenMildenhall,PratulP.Srinivasan,MatthewTancik,JonathanT.Barron,RaviRamamoorthi,andRen
Ng. Nerf:representingscenesasneuralradiancefieldsforviewsynthesis. Commun.ACM,65(1):99–106,
2022.
[30] ThomasMüller,AlexEvans,ChristophSchied,andAlexanderKeller. Instantneuralgraphicsprimitives
withamultiresolutionhashencoding. ACMTrans.Graph.,41(4):102:1–102:15,2022.
[31] SameerAgarwal, YasutakaFurukawa, NoahSnavely, IanSimon, BrianCurless, StevenMSeitz, and
RichardSzeliski. Buildingromeinaday. CommunicationsoftheACM,54(10):105–112,2011.
[32] JohannesLSchonbergerandJan-MichaelFrahm. Structure-from-motionrevisited. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition,pages4104–4113,2016.
[33] NoahSnavely,StevenMSeitz,andRichardSzeliski. Phototourism:exploringphotocollectionsin3d. In
ACMsiggraph2006papers,pages835–846.2006.
[34] YasutakaFurukawa,CarlosHernández,etal. Multi-viewstereo:Atutorial. FoundationsandTrends®in
ComputerGraphicsandVision,9(1-2):1–148,2015.
[35] Johannes L Schönberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view
selectionforunstructuredmulti-viewstereo. InComputerVision–ECCV2016:14thEuropeanConference,
Amsterdam,TheNetherlands,October11-14,2016,Proceedings,PartIII14,pages501–518.Springer,
2016.
[36] PatrickPérez,MichelGangnet,andAndrewBlake.Poissonimageediting.ACMTrans.Graph.,22(3):313–
318,2003.
[37] WilliamELorensenandHarveyECline. Marchingcubes: Ahighresolution3dsurfaceconstruction
algorithm. InSeminalgraphics:pioneeringeffortsthatshapedthefield,pages347–353.1998.
[38] TianchangShen,JunGao,KangxueYin,Ming-YuLiu,andSanjaFidler. Deepmarchingtetrahedra: a
hybridrepresentationforhigh-resolution3dshapesynthesis. InMarc’AurelioRanzato,AlinaBeygelzimer,
YannN.Dauphin,PercyLiang,andJenniferWortmanVaughan,editors,AdvancesinNeuralInformation
ProcessingSystems34: AnnualConferenceonNeuralInformationProcessingSystems2021,NeurIPS
2021,December6-14,2021,virtual,pages6087–6101,2021.
[39] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan
Gojcic,SanjaFidler,NicholasSharp,andJunGao. Flexibleisosurfaceextractionforgradient-basedmesh
optimization. ACMTrans.Graph.,42(4):37:1–37:16,2023.
[40] Po-HanHuang,KevinMatzen,JohannesKopf,NarendraAhuja,andJia-BinHuang. Deepmvs:Learning
multi-viewstereopsis. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages2821–2830,2018.
11[41] YaoYao,ZixinLuo,ShiweiLi,TianFang,andLongQuan. Mvsnet: Depthinferenceforunstructured
multi-viewstereo.InProceedingsoftheEuropeanconferenceoncomputervision(ECCV),pages767–783,
2018.
[42] LingYang,ZhilongZhang,YangSong,ShendaHong,RunshengXu,YueZhao,WentaoZhang,BinCui,
andMing-HsuanYang. Diffusionmodels:Acomprehensivesurveyofmethodsandapplications. ACM
ComputingSurveys,56(4):1–39,2023.
[43] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn,SongyangZhang,QiyuanHu,HarryYang,
OronAshual,OranGafni,etal. Make-a-video:Text-to-videogenerationwithouttext-videodata. arXiv
preprintarXiv:2209.14792,2022.
[44] HaochenWang, XiaodanDu, JiahaoLi, RaymondA.Yeh, andGregShakhnarovich. Scorejacobian
chaining:Liftingpretrained2ddiffusionmodelsfor3dgeneration. CoRR,abs/2212.00774,2022.
[45] WeiyuLi,RuiChen,XuelinChen,andPingTan. Sweetdreamer:Aligninggeometricpriorsin2ddiffusion
forconsistenttext-to-3d. arXivpreprintarXiv:2310.02596,2023.
[46] JiaxiangTang,JiaweiRen,HangZhou,ZiweiLiu,andGangZeng. Dreamgaussian:Generativegaussian
splattingforefficient3dcontentcreation. arXivpreprintarXiv:2309.16653,2023.
[47] JunliangYe,FangfuLiu,QixiuLi,ZhengyiWang,YikaiWang,XinzhouWang,YueqiDuan,andJunZhu.
Dreamreward:Text-to-3dgenerationwithhumanpreference. arXivpreprintarXiv:2403.14613,2024.
[48] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view
diffusionfor3dgeneration. CoRR,abs/2308.16512,2023.
[49] RuoshiLiu,RundiWu,BasileVanHoorick,PavelTokmakov,SergeyZakharov,andCarlVondrick. Zero-
1-to-3: Zero-shotoneimageto3dobject. InIEEE/CVFInternationalConferenceonComputerVision,
ICCV2023,Paris,France,October1-6,2023,pages9264–9275.IEEE,2023.
[50] MinghuaLiu,ChaoXu,HaianJin,LinghaoChen,MukundVarmaT,ZexiangXu,andHaoSu. One-2-
3-45:Anysingleimageto3dmeshin45secondswithoutper-shapeoptimization. AdvancesinNeural
InformationProcessingSystems,36,2024.
[51] RuoxiShi,HanshengChen,ZhuoyangZhang,MinghuaLiu,ChaoXu,XinyueWei,LinghaoChen,Chong
Zeng,andHaoSu. Zero123++: asingleimagetoconsistentmulti-viewdiffusionbasemodel. CoRR,
abs/2310.15110,2023.
[52] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus:
Learningneuralimplicitsurfacesbyvolumerenderingformulti-viewreconstruction. arXivpreprint
arXiv:2106.10689,2021.
[53] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InIEEE/CVFConferenceonComputerVisionandPattern
Recognition,CVPR2022,NewOrleans,LA,USA,June18-24,2022,pages10674–10685.IEEE,2022.
[54] LvminZhangandManeeshAgrawala. Addingconditionalcontroltotext-to-imagediffusionmodels.
CoRR,abs/2302.05543,2023.
[55] XintaoWang,LiangbinXie,ChaoDong,andYingShan. Real-esrgan:Trainingreal-worldblindsuper-
resolutionwithpuresyntheticdata.InIEEE/CVFInternationalConferenceonComputerVisionWorkshops,
ICCVW2021,Montreal,BC,Canada,October11-17,2021,pages1905–1914.IEEE,2021.
[56] ShanchuanLin,BingchenLiu,JiashiLi,andXiaoYang. Commondiffusionnoiseschedulesandsample
stepsareflawed. InIEEE/CVFWinterConferenceonApplicationsofComputerVision,WACV2024,
Waikoloa,HI,USA,January3-8,2024,pages5392–5399.IEEE,2024.
[57] FangfuLiu,HanyangWang,WeiliangChen,HaowenSun,andYueqiDuan. Make-your-3d: Fastand
consistentsubject-driven3dcontentgeneration. arXivpreprintarXiv:2403.09625,2024.
[58] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan
Gojcic,SanjaFidler,NicholasSharp,andJunGao. Flexibleisosurfaceextractionforgradient-basedmesh
optimization. ACMTrans.Graph.,42(4):37:1–37:16,2023.
[59] FuqiangZhao,YuhengJiang,KaixinYao,JiakaiZhang,LiaoWang,HaizhaoDai,YuhuiZhong,Yingliang
Zhang, Minye Wu, Lan Xu, and Jingyi Yu. Human performance modeling and rendering via neural
animatedmesh. ACMTrans.Graph.,41(6):235:1–235:17,2022.
[60] ShubhamGoel,GeorgiaGkioxari,andJitendraMalik. Differentiablestereopsis:Meshesfrommultiple
viewsusingdifferentiablerendering.InIEEE/CVFConferenceonComputerVisionandPatternRecognition,
CVPR2022,NewOrleans,LA,USA,June18-24,2022,pages8625–8634.IEEE,2022.
[61] MarkusWorchel,RodrigoDiaz,WeiwenHu,OliverSchreer,IngoFeldmann,andPeterEisert. Multi-view
meshreconstructionwithneuraldeferredshading. InIEEE/CVFConferenceonComputerVisionand
PatternRecognition,CVPR2022,NewOrleans,LA,USA,June18-24,2022,pages6177–6187.IEEE,
2022.
12[62] MarioBotschandLeifKobbelt. Aremeshingapproachtomultiresolutionmodeling. InJean-Daniel
BoissonnatandPierreAlliez,editors,SecondEurographicsSymposiumonGeometryProcessing,Nice,
France,July8-10,2004,volume71ofACMInternationalConferenceProceedingSeries,pages185–192.
EurographicsAssociation,2004.
[63] WernerPalfinger. Continuousremeshingforinverserendering. Comput.Animat.VirtualWorlds,33(5),
2022.
[64] SamuliLaine,JanneHellsten,TeroKarras,YeonghoSeol,JaakkoLehtinen,andTimoAila. Modular
primitivesforhigh-performancedifferentiablerendering. ACMTrans.Graph.,39(6):194:1–194:14,2020.
[65] JiaxiangTang,ZhaoxiChen,XiaokangChen,TengfeiWang,GangZeng,andZiweiLiu. LGM:large
multi-viewgaussianmodelforhigh-resolution3dcontentcreation. CoRR,abs/2402.05054,2024.
[66] Léon Bottou. Large-scale machine learning with stochastic gradient descent. In Yves Lechevallier
andGilbertSaporta,editors,19thInternationalConferenceonComputationalStatistics,COMPSTAT
2010, Paris, France, August 22-27, 2010 - Keynote, Invited and Contributed Papers, pages 177–186.
Physica-Verlag,2010.
[67] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learning
transferablevisualmodelsfromnaturallanguagesupervision. InMarinaMeilaandTongZhang,editors,
Proceedingsofthe38thInternationalConferenceonMachineLearning,ICML2021,18-24July2021,
VirtualEvent,volume139ofProceedingsofMachineLearningResearch,pages8748–8763.PMLR,2021.
[68] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann,
Thomas Barlow McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset
of3dscannedhouseholditems. In2022InternationalConferenceonRoboticsandAutomation,ICRA
2022,Philadelphia,PA,USA,May23-27,2022,pages2553–2560.IEEE,2022.
[69] DiederikP.KingmaandJimmyBa. Adam:Amethodforstochasticoptimization. InYoshuaBengioand
YannLeCun,editors,3rdInternationalConferenceonLearningRepresentations,ICLR2015,SanDiego,
CA,USA,May7-9,2015,ConferenceTrackProceedings,2015.
[70] HuYe,JunZhang,SiboLiu,XiaoHan,andWeiYang. Ip-adapter:Textcompatibleimagepromptadapter
fortext-to-imagediffusionmodels. CoRR,abs/2308.06721,2023.
13Input Image Generated Textured Mesh Input Image Generated Textured Mesh
Figure6: Moregeneratedresultsofourmethodfromasingleimage.
A MoreResults
WeprovidemoregenerationresultsofourmethodfromasingleimageinFigure6.
14B NetworkArchitectureandTrainingDetails
Multi-viewImageGeneration: Inthispart,wedevelopamodelbasedonthearchitectureofStable
DiffusionImageVariation[53]withtwomainmodifications: (1). Theuseofaclassembedding,
whichtakesanintegerfrom0to3asinput,indicatingthecorrespondingviewindexes. (2). The
simultaneousforwardoffourperspectives,wheretheyareconcatenatedintheself-attentionlayersto
achievemulti-viewconsistency.
Forthetrainingofthenetwork,weutilizethefollowingparameters:
• Alearningrateof10−4.
• Abatchsizeof1024.
• Anoiseoffsetof0.1.
• AnSNRgammaof5.0.
• An8-bitAdamoptimizer[69]withbetassetto(0.9,0.999).
• AnAdamweightdecayof0.01.
• AnAdamepsilonof10−8.
• Gradientclippingwithanormof1toensuretrainingstability.
Multi-viewImageUpscale: Inthispart,weaimtoupscalemergedlow-resolutionfour-viewimages
to a high resolution of 1024 pixels. To achieve this, we fine-tune the ControlNet-Tile network,
leveragingStableDiffusion1.5asitsbackbone. Unliketraditionalmethods,wedonotusetextinput;
instead,wefeedanemptytext. Concurrently,wepasstheinputimagethroughanIP-Adapter[70].
Thisapproachallowsthenetworktobeguidedinenhancingthemulti-viewdetailsandachievingthe
desiredresolution.
Forthetrainingofthisnetwork,weusethefollowingparameters:
• Alearningrateof5×10−6.
• Abatchsizeof128.
• Anoiseoffsetof0.1.
• AnSNRgammaof5.0.
• An8-bitAdamoptimizer[69]withbetassetto(0.9,0.999).
• AnAdamweightdecayof0.01.
• AnAdamepsilonof10−8.
• Gradientclippingwithanormof1toensuretrainingstability.
• FreezeparametersexceptfortheControlNet.
NormalPredictionDiffusion: Inthispart, wetrainadiffusionmodelthattakesanRGBimage
asinputandproducesitscorrespondingnormalmapasoutput. ThismodelisbasedontheStable
DiffusionImageVariation[53],withonekeymodification: areferenceU-Nethasbeenincorporated,
whichhasanidenticalnetworkstructureandinitializationastheoriginalnetwork. Thisreference
U-Netprovidespixel-wisereferenceattentiontothemainnetworkexclusivelyatanewattention
layerthataddedtotheself-attention.
Forthetrainingofthenetwork,weutilizethefollowingparameters:
• Alearningrateof10−4formainnetwork.
• Alearningrateof10−5forreferencenetwork.
• Abatchsizeof128.
• Anoiseoffsetof0.1.
• AnSNRgammaof5.0.
• An8-bitAdamoptimizer[69]withbetassetto(0.9,0.999).
• AnAdamweightdecayof0.01.
15• AnAdamepsilonof10−8.
• Gradientclippingwithanormof1toensuretrainingstability.
• Freezeparametersexceptfortheself-attentioninreferenceattention.
• Trainallparametersinthemainnetwork.
C EfficientInvisibleRegionColorCompletionAlgorithm
Inourapproachtomeshcoloringusingmultipleviewpoints,weencounteraminoryetnoteworthy
challenge: theneedtocolorregionsthatarenotdirectlyvisible. Althoughtheseregionsaretypically
sparseandinconspicuous. Infield-basedrepresentationssuchasSignedDistanceFields[27],they
oftenarethecolorofneighboringvisibleareasuponcompletionofthefieldoptimization. Toaddress
this,weoptforastraightforwardyetefficientalgorithmthatseamlesslyspreadsthecolorsofnearby
visibleregionsintotheinvisibleones.
Ourmethodologyemploysastraightforward,multi-stepcolorpropagationalgorithm,whichstandsout
foritssimplicity,swiftexecution,andreliabilityindeliveringareasonablydetailedandnuancedcolor
complement.Thisapproachoutperformsmorecomplex,resource-intensive,andlessstabletechniques
likeusingpre-trainedinpaintingdiffusionmodels. Thealgorithmleveragesthesurroundingcolorsto
gentlyfillintheinvisibleregions,withthedetailedprocessoutlinedinAlgorithmC.
Acriticalaspectofthealgorithmtoconsideristhepotentialforastarkcolordemarcationlineifthe
processishaltedimmediatelyafterallnodeshavebeencolored. Forexample,inaone-dimensional
scenario,ifredisontheleftandblueisontheright,separatedbyanuncoloredsection,stopping
immediatelywillresultinahigh-contrastboundary. Tomitigatethis,weextendthecolorpropagation
processthroughanumberofiterationstoensureasmoothcolorgradient. Thisallowsthecolorsto
graduallypermeatethroughouttheentireconnectedcomponentofthemeshthatrequirescoloring,
thusachievingaharmoniousandvisuallycoherentresult.
Algorithm1ColorCompletionAlgorithm
Input:MeshM,listofinvisibleverticesInv,listofcolorofallverticesC
Output:ThecompletedcolorlistC
1: cnt←0
2: stage2←False
3: colored←∅
4: forallverticesvinM do
5: ifv∈/ Invthen
6: AppendTruetovisible_vertices
7: else
8: AppendFalsetovisible_vertices
9: endif
10: endfor
11: whilestage2==Falseorcnt>0do
12: foralliinInvdo
13: colored_neighbors←listofverticesdirectlyconnectedtoiinM thathavecolored==True
14: ifcolored_neighbors!=∅ then
15: colored[i]←True
16: C[i]←mean(C[colored_neighbors])
17: else
18: colored[i]←False
19: endif
20: endfor
21: ifallelementsofcoloredareTruethen
22: stage2←True
23: cnt←cnt−1
24: else
25: cnt←cnt+1
26: endif
27: endwhile
28: returnC
16Algorithm2ExplicitTargetAlgorithm
Input: Multi-viewimagelistimgs,initialmeshmodelM
Output: ModelM′withvertexcolorssettoExplicitTarget
1: M′ ←M
2: SetthecolorofM′tothevertexnormalsofM′
3: forallverticesvinthevertexsetofM′do
4: tot_weight←0
5: tot_color ←⃗0 ▷Initializetozerovector
6: forallimagesiminimgsdo
7: ifvertexvisnotvisibleintheviewpointofimthen
8: continue
9: endif
10: ci←thecolorofvertexvinimageim
11: wi←thesquareofthecosineoftheanglebetweenthevertexnormalofvandtheview
directionfromimtov
12: tot_weight←tot_weight+wi
13: tot_color ←tot_color+wi·c⃗i
14: endfor
15: iftot_weight>0then
16: SetthecolorofvertexvinM′totot_color/tot_weight
17: endif
18: endfor
19: returnM′
D ExplicitTargetalgorithm
Cosine of Angles
Figure7: Correlationbetweenpredictionvalueandpredictionerrors.
In Algorithm 2, we demonstrate the detailed computation of ExplicitTarget. Specifically, we set
anoptimizationtargetforeachvertex,whichisaweightedsumofthesupervisedsignalsfromthe
visibleviewsofthevertex. Theweightsaredeterminedbytwofactors: theprojectedareaofthe
nearbysurfaceandtheconfidencelevelintheaccuracyofthenormals,whichareusedtocalculate
theweights. InFigure7,weshowtherelationshipbetweenthenormalresultspredictedbymultiView
diffusionandtheaccuracyofthepredictionsontheObjaverse[18]validationset. Theresultsindicate
thattheclosertheanglebetweenthepredictednormalandtheverticaltothecurrentviewpointis,the
lowertheaccuracyoftheprediction. Thereisanegativecorrelationbetweenthesetwofactors,witha
Pearsoncorrelationcoefficientof-0.304.
17
noitciderP
eht
fo
rorrE
noitceriDE AblationStudyonMeshInitialization
Input Image Our Initialization Ball Initialization
Figure8: AblationsonMeshInitialization. Wecomparetheresultsofusingourfastinitialization
method,versususingasphereasaninitialization.
Wecomparethedifferentmeshinitializationmethodsandtheirresults. Oneisourproposedfast
initializationmethod,andtheotherisusingspheresasinitializationobjects,acommonpracticein
mesh-basedreconstructiontechniques. FigureEillustratestheproblemofthemeshreconstruction
methodthatfailstomodifyitstopologicalstructure. Forexample,inthefirstrow,themodelcannot
achieveahollowstructurebydirectoptimizationbecausetheirtopologiesareinherentlydifferent.
However, asshowninthesecondrow, eventhoughthetopologiesaredifferent, theoptimization
methodcanstillprovideapproximateresults. Forinstance,thesphere-basedinitializationcanshape
thehandleontherightside,eventhoughthehandleisincomplete. Thesphere-basedinitialization
cansometimesproduceevenmoreaccurateresultsthanourproposedmethod,asseeninthethird
row. Theseexperimentsdemonstratethatourmethodisrobusttoinitialization. Aimingforabetter
abilitytogeneralize,wechosetouseourfastinitialization.
F UserStudy
Foruserstudy,werender360-degreevideosofsubject-driven3Dmodelsandshoweachvolunteer
with five samples of rendered video from a random method. They can rate in four aspects: 3D
consistency,subjectfidelity,promptfidelity,andoverallqualityonascaleof1-10,withhigherscores
indicatingbetterperformance. Wecollectresultsfrom30volunteersshowninTable2. Wefindour
methodissignificantlypreferredbyusersovertheseaspects.
G SocialImpact
PositiveImpacts: TheUnique3Dframeworkcandemocratize3Dcontentcreation,makingiteasier
for artists and designers to produce 3D models from single images, which can lead to increased
innovationandasurgeincreativeapplicationsacrossvariousindustriesincludinggaming,film,and
education.
NegativeImpacts: Ontheflipside,theeaseofgeneratinghigh-quality3Dmodelsraisesconcerns
aboutpotentialmisuse,suchascreatingdeepfakes,andcouldleadtojobdisplacementfortraditional
3Dmodelers. Additionally,theremaybechallengesrelatedtointellectualpropertyandprivacyifthe
technologyisusedirresponsibly.
18Table2: Quantitativecomparisonresultsonthemulti-viewconsistency,subjectfidelity(relatedto
geometricandtexturedetails),promptfidelity(relatedtothealignmentofinputsingleimage),and
overallqualityscoreinauserstudy,ratedonarangeof1-10,withhigherscoresindicatingbetter
performance.
Method Multi-viewConsistency SubjectFidelity PromptFidelity OverallQuality
One-2-3-45[50] 5.46 4.78 6.93 5.79
OpenLRM[2] 6.72 7.16 6.92 7.15
SyncDreamer[21] 5.71 7.52 4.06 5.92
Wonder3D[3] 8.67 7.80 7.39 8.14
InstantMesh[5] 8.31 7.68 7.91 8.43
GRM[23] 6.93 7.42 6.02 7.38
CRM[4] 7.95 8.53 8.03 8.25
Ours 9.26 8.74 8.52 9.02
H LicensesforUsedAssets
StableDiffusion[53]isunderCreativeMLOpenRAILMLicense.
Objaverse[18]isunderODC-Byv1.0license.
19