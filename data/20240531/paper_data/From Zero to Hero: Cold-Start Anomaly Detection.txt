From Zero to Hero: Cold-Start Anomaly Detection
TalReiss1 GeorgeKour2 NaamaZwerdling2 AteretAnaby-Tavor2 YedidHoshen1
1TheHebrewUniversityofJerusalem 2IBM
https://github.com/talreiss/ColdFusion
Abstract 2024; Zhou et al., 2024) uses descriptions of the
normalclassesanddoesnotrequiretrainingdata.
When first deploying an anomaly detection
While zero-shot methods can be used for freshly
system,e.g.,todetectout-of-scopequeriesin
chatbots, there are no observed data, making deployedsystems,theyresultinreducedaccuracy
data-drivenapproachesineffective. Zero-shot as the descriptions often fail to properly express
anomalydetectionmethodsofferasolutionto thedistributionofrealdata.
such"cold-start"cases,butunfortunatelythey
Weexplorethecold-startsettingwhichprovides
areoftennotaccurateenough. Thispaperstud-
two types of guidance: i) a textual description of
ies the realistic but underexplored cold-start
eachnormalclass,servingasinitialguidance,such
setting where an anomaly detection model is
as predefined topic names in chatbot systems; ii)
initialized using zero-shot guidance, but sub-
sequentlyreceivesasmallnumberofcontami- astreamoftcontaminatedobservations(thatmay
natedobservations(namely,thatmayinclude includeanomalies),e.g.,realuserqueries. Itispar-
anomalies). Thegoalistomakeefficientuse ticularlyrelevantinreal-worldapplicationswhere,
ofboththezero-shotguidanceandtheobser- shortly after deployment, a short stream of user
vations. We propose ColdFusion, a method
queriesbecomesavailablebutthequeriesarenot
that effectively adapts the zero-shot anomaly
labeledintointenttypesandsomeofthemareout-
detectortocontaminatedobservations. Tosup-
of-scope. To our knowledge, the only work that
portfuturedevelopmentofthisnewsetting,we
proposeanevaluationsuiteconsistingofevalu- deals with a similar setting (Jeong et al., 2023)
ationprotocolsandmetrics. assumespriorknowledgeofanomalies,thatobser-
vationscomefromasinglenormalclassandthat
1 Introduction
theyarenotcontaminatedbyanomalies.
Anomaly detection methods aim to flag data that Totacklethecold-startsetting,wepresentCold-
violateacceptednorms. Forexample,acustomer Fusion, a method for adapting a zero-shot model
supportchatbotmaybedesignedtoanswerqueries given the distribution of a limited observation
about particular intents (in-scope) but not about stream. Our method is very effective, achieving
other intents (out-of-scope). Unlike related tasks considerablybetterresultsthanpurezero-shotand
suchasout-of-scopeintentdiscoveryandclassifi- observation-based methods. To encourage future
cation, whichrelyonlargelabeledin-scopedata, researchonthispromisingnewsetting,weprovide
anomaly detection approaches relax the labeling evaluationprotocolsandmetrics.
assumption and treat the problem as a one-class Ourcontributionsare:
classification task (Lin et al., 2020; Zhang et al.,
1. Proposing the new setting of cold-start
2021b;Mouetal.,2022;Zhengetal.,2020;Zhan
anomalydetection.
etal.,2021;LinandXu,2019;Zengetal.,2021;
Zhangetal.,2021a;Xuetal.,2020). Mostanomaly 2. PresentingColdFusionfortacklingthesetting.
detection methods (Reiss et al., 2021; Qiu et al.,
3. Introducingadedicatedevaluationsuitecon-
2021; Zhang et al., 2023) require previous obser-
sistingofevaluationprotocolsandmetrics.
vations for training and are effective when many
pastobservationsareavailable. Suchmethodsare
2 Cold-StartAnomalyDetection
noteffectiveforsystemsjustafterdeployment,as
they lack access to any past observations. Zero- Task Definition. In the cold-start setting, a
shotanomalydetection(Jeongetal.,2023;Lietal., modelhasaccesstoK classdescriptionsD =
prior
4202
yaM
03
]GL.sc[
1v14302.5042:viXraAlgorithm1:ColdFusion
Input: D ,D ,p,queryx.
prior t
Output: AnomalyscoreS (x).
adapt
Step1: Encodeclassdescriptionsand
observations: ϕ(D ),ϕ(D );
prior t
Step2: Assignobservationstoclasses
basedonnearestclassdescriptor:
a(x) = argmin {d(ϕ(x),ϕ(c ))}K ;
k k k=1
Step3: Adaptclassembeddings:
z = median(ϕ(c ),{ϕ(x)|a(x) = k});
k k
Figure1: ColdFusionassignseachofthetobservations
Step4: Computeanomalyscoreforx:
to their nearest class, then adapts the embeddings of
S (x) = min {d(ϕ(x),z )}K ;
eachclasstowardstheassignedobservations. adapt k k k=1
{c ,c ,...,c } and a stream of t observations
1 2 K
D = {x ,x ,...,x }, where t is small. We de- allowedintents. Thenadeepencoderextractsthe
t 1 2 t
notethepercentageofanomalousobservationsas embeddingsofthetargetuserqueryandintentde-
thecontaminationratior%. Anobservationxei- scriptions. Finally,themethodlabelstheuserquery
thercomesfromoneoftheK normalclassesoris asOOSifitisfarfromallallowedintentnames.
anomalous,butwedonothaveaccesstotheclass 3.2 LimitationsofExistingMethods
or anomaly label. The task is to learn a model S
Inpractice,itisimpossibletoprovideperfectclass
tomapeachtrainingsamplextoananomalyscore
descriptions,andthereforezero-shotanomalyde-
suchthathighvaluesindicateanomaloussamples.
tectionoftendoesnotachievesufficientaccuracy.
Applicationtochatbots. Ourpracticalmotiva-
Ontheotherhand,ifthenumberofobservationsis
tionisidentifyingout-of-scopequeriesinarecently
limited,observation-basedanomalydetectionmeth-
deployedchatbot. Weobserveastreamofqueries
ods,suchasK-nearestneighbors,struggleforthree
sent to the chatbot, as well as descriptions of all
keyreasons: i)theobservationsmaynotincludeall
allowed intents. At time step t+1, we leverage
in-scopeclasses; ii)itishardtoestimatethetrue
both D and D to classify a given query x
t prior t+1
distributionofnormaldatafromafewsamples;iii)
asin-scope(INS)orout-of-scope(OOS).
theobservationsmaybecontaminatedbyanoma-
3 Method lies. Empirically, observation-based methods un-
derperformZSmethodsforsmallt(seeTab.1and
3.1 Recap: Zero-ShotAnomalyDetection
Fig.2).
Zero-shot(ZS)anomalydetectionmapseachdata
point x to an anomaly score S(x). Notably, ZS 3.3 OurMethod: ColdFusion
methodsdonotrequirepastdata,insteadtheyare To bridge the gap between ZS and observation-
guided by a set of distinct normal class names basedmethods,weproposeColdFusion(illustrated
{c 1,c 2,...,c K}providedbytheuser. Apre-trained in Fig. 1), a method for cold-start anomaly de-
featureextractorϕmapseachoftheclassnamesc k, tection using domain adaptation. It improves ZS
andobservationsx t todeepembeddingsϕ(c k)and anomalydetectionusingthetobservationsintwo
ϕ(x t). Itthencomputesthedistanced(oftenL 2 or key stages: i) assigning observations to classes;
Cosine) between the embeddings of the example ii)adaptingZSclassembeddingsbasedontheas-
and each of the class names. The final anomaly signedobservations.
scoreisgivenbythedistancetothenearestclass: Assignment. We assign each of the t observa-
S (x) = min{d(ϕ(x),ϕ(c ))}K (1) tionstothenearestclassasmeasuredinthefeature
zs k k=1
k spaceϕ. Wedenotetheclassassignmentofobser-
Highanomalyscoresserveasindicatorsofanoma-
vationxasa(x). Moreformally:
lies. The anomaly score can be converted to a
binary label by choosing a threshold α such that a(x) = argmin{d(ϕ(x),ϕ(c ))}K (2)
k k=1
k
y = 0ifS(x) < αandy = 1ifS(x) ≥ α.
Zero-shot anomaly detection can be used for We further define C , the set of all observations
k
OOS query detection by first specifying a set of assignedtoclassk asC = {ϕ(x)|a(x) = k}.
kAUC2 AUC2 AUC2 AUC2
10% 25% 50% 100%
Method
B77 C-Bank C-Cards B77 C-Bank C-Cards B77 C-Bank C-Cards B77 C-Bank C-Cards
ZS 78.9 83.1 81.8 78.9 83.1 81.8 78.9 83.1 81.8 78.9 83.1 81.8
DN2 76.7 64.8 70.0 76.2 76.0 75.6 75.9 80.2 79.6 75.3 82.2 80.2
ColdFusion 81.7 82.3 84.8 81.8 87.0 87.3 81.9 88.6 88.7 82.3 89.2 89.0
ZS 81.8 82.7 80.1 81.8 82.7 80.1 81.8 82.7 80.1 81.8 82.7 80.1
DN2 78.3 69.7 69.8 78.2 78.9 76.9 77.6 82.3 80.9 76.3 83.6 81.1
ColdFusion 83.3 84.4 84.1 82.8 87.8 86.0 82.8 88.8 87.8 83.0 89.4 88.3
Table1: AUC2results,withcontaminationofr =5%. Bestresultsareinbold.
t˜
AUC2 AUC2 queries, categorized into 77 fine-grained intents
10% 25%
Method B77 C-Bank C-Cards B77 C-Bank C-Cards withintheonlinebankingdomain. Amongthese,
K-means 80.0 79.2 83.7 78.9 84.0 87.0 50intentsarein-scope,whiletheremaining27are
Mean 81.6 80.8 84.7 81.7 86.4 87.5
OOS queries. CLINC-OOS (Larson et al., 2019;
MI 81.6 82.3 84.8 81.8 87.0 87.1
Median 81.7 82.3 84.8 81.8 87.0 87.3 Zhang et al., 2021c), derived from the broader
CLINC dataset, consists of two domains: "Bank-
Table2: AUC2 t˜results,withcontaminationofr =5% ing"and"Creditcards",eachfeaturing10in-scope
usingtheGTEmodel. MIreferstomultipleiterations
and5OOSintents. Thetrainingsetsforeachdo-
withmedianadaptation. Bestresultsareinbold.
maininclude500in-scopequeries,whilethetest
sets contain 850 queries, with 350 designated as
Adaptation. We now adapt each class embed- OOS instances. Notably, our setting is unsuper-
dingbyconsideringboththeinitialclassdescrip- visedi.e.,observationsdonotincludeintentlabels
tionandtheassignedobservations. Concretely,the fortraining. FurtherdetailsareinApp.B.1.
adaptedcodeforeachclassisthemedianoftheset Featureextractor&classencoding. Weexplored
containingtheembeddingoftheclassdescriptions twofeatureencoders,namelytheGTEmodel(Li
andtheembeddingsofallassignedobservations: etal.,2023)andMPNET(Songetal.,2020),both
pre-trained on a large corpus of text pairs across
z k = median({ϕ(c k)}∪C k) (3) variousdomains. Wefoundthatdirectlyencoding
intent topic names using these encoders did not
Wechosethemedianandnotmeanforcontamina-
meet our performance expectations (See Sec. 5).
tionrobustness. Notethatthisstepwillnotmodify
Toovercomethischallenge,weleverageChatGPT
theembeddingofclasseswithnoobservations.
to generate a query corresponding to each topic
Anomaly scoring. ColdFusion uses the same andutilizethesegeneratedqueriesasclassdescrip-
anomalyscoringasZSexceptthattheclasscodes tionsinsteadoftheintenttopicnames. Forfurther
are the adapted {z k}K k=1 instead of the encod- details,pleaserefertoApp.A.
ing of the original description i.e., S (x) =
adapt Baselines. We compare ColdFusion (Sec. 3.3)
min {d(ϕ(x ),z )}K .
k t+1 k k=1 to several baselines. These include the zero-shot
model(ZS),detailedinSec.3.1,whichreliessolely
4 Experiments
onthegeneratednormalclassdescriptions. Addi-
Experimentalsetting. Ourexperimentssimulate tionally, we consider DN2, an observation-based
the deployment of an OOS query detection sys- anomaly detection method proposed by (Reiss
tem. We first randomly sort the queries so that et al., 2021). DN2 computes the anomaly score
eachqueryhasauniquetimet,modelingaquery of an observation by its deep 1-nearest neighbor
stream. Ateachtimet,wetrainamodelusingthet distanceversusthepreviousobservationsD t. For
availableobservationsandtheK classnames,and implementationdetailsrefertoApp.B.2.
evaluatethemodelontheentiretestset. Evaluationmetrics. Weproposeanewmetricto
Datasets. We use three evaluation datasets, evaluate the cold-start setting, which emphasizes
Banking77-OOS and CLINC-OOS segmented high-accuracyshortlyafterdeployment(lowt). At
into CLINC-Banking and CLINC-Credit_Cards. each time step t, we evaluate the performance of
Banking77-OOS (Casanueva et al., 2020; Zhang theanomalyscoresmodelusingtheAreaUnderthe
etal.,2021c)consistsof13,083customerservice ReceiverOperationCharacteristic(AUROC)curve.
redocnE
ETG
TENPM(a)Banking77-OOS (b)CLINC-Banking (c)CLINC-Credit_Cards
Figure 2: Performance trends with contamination r = 5% using the GTE model over time demonstrate the
superiorityofourColdFusionmethodoverotherbaselineapproaches.
WeobtainanAUROCscoreforeverytimestep,and Method B77 C-Bank C-Cards
wedenotethemas{AUC(t)}T . Wesummarize
t=1
Naive 76.9 60.7 69.8
thistvs. AUROCcurvebytheareaunderitupto
timet. This isdenoted asAUC2 =
(cid:80)t t′=1AUC(t′)
,
Generated 78.9 83.1 81.8
t˜ t
wheret˜= t,thefractionofthetrainingsetused. Naive 79.8 69.6 73.7
T
TheAUC2 metricprovidesaconcisesummaryof Generated 81.8 82.7 80.1
t˜
themodel’saccuracyfreshlyafterdeployment.
Table3: ComparisonofZSmodelsintermsofAUROC.
4.1 Results AsZSmodelsmaintainconstantperformanceovertime
andarenotexposedtodata,AUC2andcontaminations
WepresentourresultsinTab.1andFig.2. ColdFu- t˜
areirrelevant. Bestresultsareinbold.
sionconsistentlyoutperformsallbaselinesacross
theevaluateddatasetsbyalargemargin. Particu-
larly,weseethatDN2performspoorly,especially andadaptation,eachtimeassigningtotheadapted
withsmallt,andthezero-shotbaseline(ZS)main- center,failstooutperformColdFusion. Thesingle
tainsconstantperformanceovertime. Conversely, iterationofColdFusionispreferred,sincemultiple
ourapproachperformswellevenforlowtvalues, iterations increase the computational cost. Addi-
andimprovesovertime. Thepresenceofanomalies tionally, the results in Tab. 2 show that median
inthedatastreamposesachallengeforDN2,asit adaptationisslightlybetterthanusingthemeanon
solelyreliesontheobservedcontaminatedstream. theevaluateddatasets.
This reliance often leads to occasional decreases
Effectivenessofgeneratedqueries. InTab.3,we
inperformanceforDN2, highlightingthevulner-
examinetheimpactofanaiveZSdetectorthatsim-
abilityofmethodsthatexclusivelydependonthe
plyencodestheintentnames,comparedtoourZS
observeddatawithoutconsideringtheunderlying
approach,whichusesChatGPTtogenerateaquery
anomalies. Furthermore,ourmethod’srobustness
for each intent and then encodes the generated
todifferentfeatureencoders,asevidencedbycon-
queryastheclassembedding. Theresultshighlight
sistenttrendsinboththeGTEandMPNETmodels,
that naive encoding of intent names alone yields
suggests that it is not reliant on a single feature
subpar performance, whereas our pre-processing
extractor. Resultsfordifferentcontaminationare
procedureconsiderablyimprovesresults.
inApp.C.
6 Conclusion
5 AblationStudy
We introduced the new setting of cold-start
Classembeddingadaptationmethod. Weinves-
anomaly detection, modeling freshly deployed
tigateseveralvariationsoftheadaptationmethod,
anomaly detection systems. Our proposed solu-
showninTab.2. i)Replacingourassignmentand
tion, ColdFusion, is a method for adapting zero-
adaptation stages with K-means notably reduces
shotanomalydetectiontoalignwithanobservation
performance, mainly due to its less effective ran-
stream. Weintroducedanevaluationprotocoland
dominitializationmethodvs. ourdescriptorinitial-
metricsforcomparingfuturemethods.
ization; ii) Iterating multiple steps of assignment
ETG
TENPMLimitations 2023. Winclip: Zero-/few-shot anomaly classifi-
cation and segmentation. In Proceedings of the
Our proposed method has several limitations. i) IEEE/CVFConferenceonComputerVisionandPat-
Not all deployed anomaly detection systems en- ternRecognition(CVPR),pages19606–19616.
counterthegeneralizedcold-startproblemandin-
Ian Jolliffe. 2011. Principal component analysis.
deedinthecasewheretherearemanyobservations Springer.
andveryfewanomalies,itissometimesbetterto
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron
useobservation-drivenmethodse.g.,DN2(Reiss
Sarna, Yonglong Tian, Phillip Isola, Aaron
etal.,2021). However,webelievethatitisacom- Maschinot, CeLiu, andDilipKrishnan.2020. Su-
monissue,particularlyindomainslikechatbots;ii) pervised contrastive learning. Advances in neural
Ourapproachreliesonuser-providedguidancefor informationprocessingsystems,33:18661–18673.
zero-shotdetection,whichisnotalwaysavailable; Stefan Larson, Anish Mahendran, Joseph J Peper,
iii) We assume a low contamination ratio; if this Christopher Clarke, Andrew Lee, Parker Hill,
ratioissignificantlyhigher,theeffectivenessofour JonathanKKummerfeld, KevinLeach, MichaelA
Laurenzano, Lingjia Tang, et al. 2019. An evalua-
methodmaydecrease.
tiondatasetforintentclassificationandout-of-scope
prediction. arXivpreprintarXiv:1909.02027.
EthicsStatement
Longin Jan Latecki, Aleksandar Lazarevic, and
Cold-startanomalydetectioncanhaveseveralbene- Dragoljub Pokrajac. 2007. Outlier detection with
fitsforsociety,includingimprovingonlineservices kerneldensityfunctions. InInternationalWorkshop
securitybyidentifyingfraudulentactivities,unau- on Machine Learning and Data Mining in Pattern
Recognition,pages61–75.Springer.
thorizedaccessandenhancingchatbotfunctionality
byfilteringoutirrelevantqueriesearly. However,it XuruiLi,ZimingHuang,FengXue,andYuZhou.2024.
mayalsohavesomelesspositiveusecases,suchas Musc: Zero-shot industrial anomaly classification
and segmentation with mutual scoring of the unla-
beingusedforsurveillanceorprofilingindividuals
beledimages. InInternationalConferenceonLearn-
withouttheirconsent. Ourresearchisnotgeared
ingRepresentations.
towardstheselesspositiveusecases.
ZehanLi,XinZhang,YanzhaoZhang,DingkunLong,
Acknowledgements Pengjun Xie, and Meishan Zhang. 2023. Towards
generaltextembeddingswithmulti-stagecontrastive
Thisresearchwaspartiallysupportedbyfunding learning. arXivpreprintarXiv:2308.03281.
fromIBM,theIsraeliScienceFoundationandthe
Ting-En Lin and Hua Xu. 2019. Deep unknown in-
IsraeliCouncilforHigherEducation. tent detection with margin loss. arXiv preprint
arXiv:1906.00434.
Ting-En Lin, Hua Xu, and Hanlei Zhang. 2020. Dis-
References
coveringnewintentsviaconstraineddeepadaptive
Iñigo Casanueva, Tadas Temcˇinas, Daniela Gerz, clustering with cluster refinement. In Proceedings
MatthewHenderson,andIvanVulic´.2020. Efficient of the AAAI Conference on Artificial Intelligence,
intentdetectionwithdualsentenceencoders. arXiv volume34,pages8360–8367.
preprintarXiv:2003.04807.
Yutao Mou, Keqing He, Yanan Wu, Zhiyuan Zeng,
EleazarEskin,AndrewArnold,MichaelPrerau,Leonid HongXu,HuixingJiang,WeiWu,andWeiranXu.
Portnoy,andSalStolfo.2002. Ageometricframe- 2022. Disentangledknowledgetransferforoodin-
workforunsupervisedanomalydetection. InAppli- tentdiscoverywithunifiedcontrastivelearning. In
cationsofdataminingincomputersecurity,pages Proceedings of the60th Annual Meeting of the As-
77–101.Springer. sociationforComputationalLinguistics(Volume2:
ShortPapers),pages46–53.
Michael Glodek, Martin Schels, and Friedhelm
Schwenker.2013. Ensemblegaussianmixturemod- Chen Qiu, Timo Pfrommer, Marius Kloft, Stephan
elsforprobabilitydensityestimation. Computational Mandt, and Maja Rudolph. 2021. Neural transfor-
Statistics,28(1):127–138. mationlearningfordeepanomalydetectionbeyond
images. In International Conference on Machine
DanHendrycks,MantasMazeika,SauravKadavath,and Learning,pages8703–8714.PMLR.
Dawn Song. 2019. Using self-supervised learning
can improve model robustness and uncertainty. In Tal Reiss, Niv Cohen, Liron Bergman, and Yedid
NeurIPS. Hoshen.2021. Panda: Adaptingpretrainedfeatures
for anomaly detection and segmentation. In Pro-
Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing ceedingsoftheIEEE/CVFConferenceonComputer
Zhang, Avinash Ravichandran, and Onkar Dabeer. VisionandPatternRecognition,pages2806–2814.TalReiss,NivCohen,EliahuHorwitz,RonAbutbul,and XuanZhang,ShiyuLi,XiLi,PingHuang,JiulongShan,
Yedid Hoshen. 2022. Anomaly detection requires andTingChen.2023. Destseg: Segmentationguided
betterrepresentations. InEuropeanConferenceon denoisingstudent-teacherforanomalydetection. In
ComputerVision,pages56–68.Springer. ProceedingsoftheIEEE/CVFConferenceonCom-
puterVisionandPatternRecognition(CVPR),pages
TalReissandYedidHoshen.2023. Mean-shiftedcon- 3914–3923.
trastivelossforanomalydetection. InProceedings
of the AAAI Conference on Artificial Intelligence, YinheZheng,GuanyiChen,andMinlieHuang.2020.
volume37,pages2155–2162. Out-of-domain detection for natural language un-
derstanding in dialog systems. IEEE/ACM Trans-
Lukas Ruff, Nico Gornitz, Lucas Deecke, actionsonAudio,Speech,andLanguageProcessing,
Shoaib Ahmed Siddiqui, Robert Vandermeulen, 28:1198–1209.
Alexander Binder, Emmanuel Müller, and Marius
Kloft.2018. Deepone-classclassification. InICML. QihangZhou,GuansongPang,YuTian,ShiboHe,and
JimingChen.2024. Anomalyclip: Object-agnostic
Bernhard Scholkopf, Robert C Williamson, Alex J promptlearningforzero-shotanomalydetection.
Smola,JohnShawe-Taylor,andJohnCPlatt.2000.
Support vector method for novelty detection. In
NIPS.
KaitaoSong, XuTan, TaoQin, JianfengLu, andTie-
YanLiu.2020. Mpnet: Maskedandpermutedpre-
training for language understanding. Advances in
NeuralInformationProcessingSystems,33:16857–
16867.
David MJ Tax and Robert PW Duin. 2004. Support
vectordatadescription. Machinelearning.
HongXu,KeqingHe,YuanmengYan,SihongLiu,Zi-
jun Liu, and Weiran Xu. 2020. A deep generative
distance-basedclassifierforout-of-domaindetection
withmahalanobisspace. InProceedingsofthe28th
InternationalConferenceonComputationalLinguis-
tics,pages1452–1460.
ZhiyuanZeng,KeqingHe,YuanmengYan,ZijunLiu,
YananWu,HongXu,HuixingJiang,andWeiranXu.
2021. Modelingdiscriminativerepresentationsfor
out-of-domaindetectionwithsupervisedcontrastive
learning. arXivpreprintarXiv:2105.14289.
Li-MingZhan,HaowenLiang,BoLiu,LuFan,Xiao-
MingWu,andAlbertLam.2021. Out-of-scopein-
tentdetectionwithself-supervisionanddiscrimina-
tivetraining. arXivpreprintarXiv:2106.08616.
HanleiZhang,HuaXu,andTing-EnLin.2021a. Deep
open intent classification with adaptive decision
boundary. InProceedingsoftheAAAIConference
onArtificialIntelligence,volume35,pages14374–
14382.
Hanlei Zhang, Hua Xu, Ting-En Lin, and Rui Lyu.
2021b. Discoveringnewintentswithdeepaligned
clustering. InProceedingsoftheAAAIConference
onArtificialIntelligence,volume35,pages14365–
14373.
Jianguo Zhang, Kazuma Hashimoto, Yao Wan, Zhi-
wei Liu, Ye Liu, Caiming Xiong, and Philip S Yu.
2021c. Are pretrained transformers robust in in-
tent classification? a missing ingredient in evalua-
tionofout-of-scopeintentdetection. arXivpreprint
arXiv:2106.04564.A Zero-ShotAnomalyDetection
Algorithm2:Zero-ShotDetector
Input: D ,ϕ,queryx.
Zero-shot (ZS) anomaly detection assigns an prior
Output: AnomalyscoreS (x).
anomalyscoreS(x)toeachdatapointxwithout zs
Step1: Generateuserqueriesusing
relyingonpastdata. Instead,itisguidedbyasetof
ChatGPTandD : {q }K ;
class names {c 1,c 2,...,c K} provided by the user. prior k k=1
Step2: Encodegeneratedqueries:
Totacklethischallenge,weleverageChatGPTto
{ϕ(q )}K andinputquery: ϕ(x);
generateauserquerycorrespondingtoeachclass k k=1
Step3: Computeanomalyscoreforx:
topicname. Weusethesegeneratedqueriesasclass
S (x) = min {dϕ(x),ϕ(q ))K ;
descriptionsinsteadoftheintenttopicnames. zs k k k=1
QueryGeneration: UtilizingChatGPT-3.5,we
generateauserqueryforeachtopictoserveasour
classdescriptions. Here,[DOMAIN]representsthe CLINC-OOS. CLINC-OOS (Larson et al.,
chatbotdomain(e.g.,"Banking"). Weemploythe 2019; Zhang et al., 2021c) emanates from the
followingtemplate: "Generatequeriesthatsome- broader CLINC dataset, encompassing 15 intent
onewouldaskachatbotin[DOMAIN].Generate classesacross10differentdomains,withintegrated
one-sentencequeriesforeachofthefollowingtop- out-of-scopeexamples. Forourevaluation,wefo-
ics: {c ,c ,...,c }." This process yields a set of cusontwodomains: "Banking"and"Creditcards".
1 2 K
K userqueries,denotedby{q }K . Each domain is characterized by 5 in-scope and
k k=1
Apre-trainedfeatureextractorϕmapseachgen- 10out-of-scopeintents. Thetrainingsetforeach
erated class name q and observation x to deep domaincomprises500in-scopeuserqueries,while
k
embeddings ϕ(q ) and ϕ(x). Subsequently, we the test set includes 850 user queries, with 350
k
computetheL distancebetweentheexampleem- designatedasout-of-scopeinstances.
2
beddingsandeachgenerateduserquery. Thefinal
B.2 ImplementationDetails&Baselines
anomalyscoreisdeterminedbythedistancetothe
nearestclass:
Ourimplementationreliesontwofeatureencoders:
S (x) = min{d(ϕ(x),ϕ(c ))}K theGTEmodel(Lietal.,2023)andMPNET(Song
zs k k=1
k
et al., 2020), both pre-trained on a large corpus
Alg.2outlinesourzero-shotmodel.
of text pairs across various domains. We use the
A comparison between naive class names and HuggingFacelibraryforbothmodels. Specifically,
generatedqueriesispresentedinTab.3.
fortheGTEmodel,weemploythe"thenlper/gte-
large" model checkpoint, while for MPNET, we
B ExperimentalDetails
usethe"sentence-transformers/all-mpnet-base-v2"
B.1 Datasets modelcheckpoint. It’snoteworthythatallbaselines
areusingthesamefeatureencodersinourcompar-
Weemploythreewidelyuseddatasets,Banking77-
isons. We use L as a distance metric. For DN2
OOSandCLINC-OOS(whichissplitintoCLINC- 2
(Reiss et al., 2021), the implementation involves
Banking and CLINC-Credit_Cards), to evaluate
encodingD andthetargetqueryxwithourfeature
ouranomalydetectionapproach. t
encoder ϕ, followed by computing the 1-nearest-
Banking77-OOS.Banking77-OOS(Casanueva
neighbordistancetoϕ(D ). Weemploythefaiss
etal.,2020;Zhangetal.,2021c)isanannotatedin- t
libraryfornearest-neighbordistancecomputations.
tentclassificationdatasetdesignedforonlinebank-
ingqueries. Comprising13,083customerservice InourColdFusioninordertoberobusttoanoma-
lies,weexcludedobservationsassignedtoclassk
queries,eachqueryislabeledwithoneof77fine-
butarefurtherthanτ. Formally,wedefineC ,as
grained intents within the banking domain. The k
thesetofallobservationsassignedtoclassk as:
datasetfocusesonfine-grained,single-domainin-
tent detection. Of these 77 intents, Banking77-
OOS incorporates 50 in-scope intents, while the C k = {ϕ(x)|a(x) = k,d(ϕ(x),ϕ(c k)) ≤ τ}
out-of-scope(OOS)queriesareconstructedbased
on 27 held-out in-scope intents. The training set Wesetτ byfirstcomputingthedistancesbetween
consistsof5,095in-scopeuserqueries,andthetest allsamplesandtheirassignedcenters,sortingthem,
setcomprises3,080userqueries,including1,080 andchoosingτ asthe90%percentile. Anablation
OOSinstances. studyonthisparameterisinApp.C.AUC2 AUC2 AUC2 AUC2
10% 25% 50% 100%
Method
B77 C-Bank C-Cards B77 C-Bank C-Cards B77 C-Bank C-Cards B77 C-Bank C-Cards
ZS 78.9 83.1 81.8 78.9 83.1 81.8 78.9 83.1 81.8 78.9 83.1 81.8
DN2 74.6 71.2 71.6 77.5 79.4 79.1 78.8 82.9 82.9 79.2 84.7 85.7
ColdFusion 79.0 85.1 85.2 80.9 86.9 87.6 81.8 87.7 88.7 82.3 89.1 89.1
ZS 81.8 82.7 80.1 81.8 82.7 80.1 81.8 82.7 80.1 81.8 82.7 80.1
DN2 76.6 74.7 70.7 79.1 82.4 78.5 80.1 85.7 82.7 80.5 86.9 84.8
ColdFusion 80.6 87.0 85.2 81.7 89.0 87.6 82.5 89.5 89.0 83.2 90.0 89.1
Table4: AUC2results,withcontaminationofr =2.5%. Bestresultsareinbold.
t˜
AUC2 AUC2 AUC2 AUC2
10% 25% 50% 100%
Method
B77 C-Bank C-Cards B77 C-Bank C-Cards B77 C-Bank C-Cards B77 C-Bank C-Cards
ZS 78.9 83.1 81.8 78.9 83.1 81.8 78.9 83.1 81.8 78.9 83.1 81.8
DN2 70.6 67.2 71.3 72.5 77.5 78.1 73.4 80.5 81.1 73.8 80.8 81.9
ColdFusion 77.4 83.4 86.4 78.9 87.0 87.1 79.9 88.4 87.9 80.8 88.9 88.2
ZS 81.8 82.7 80.1 81.8 82.7 80.1 81.8 82.7 80.1 81.8 82.7 80.1
DN2 72.3 72.8 70.7 74.5 82.4 78.0 75.3 84.8 80.9 75.3 84.1 80.9
ColdFusion 79.9 85.5 85.4 81.1 88.1 86.9 81.8 88.9 87.8 82.6 89.2 88.0
Table5: AUC2results,withcontaminationofr =7.5%. Bestresultsareinbold.
t˜
C MoreResults&Analysis plored semi-supervised clustering using labeled
in-domain data. Methods such as pre-training a
Contamination Ratios. We extend our analysis
BERTencoderwithcross-entropyloss(Linetal.,
byconsideringadditionalcontaminationratiosof
2020; Zhang et al., 2021b) and utilizing similar-
r% = 2.5 and r% = 7.5, as shown in Tables 4
ity constrained or supervised contrastive losses
and5,respectively. Additionally,wepresentvisual
(Khoslaetal.,2020)tolearndiscriminativefeatures
insights into ColdFusion’s adaptive performance
(Mouetal.,2022)aimtotransferintentrepresenta-
over time through the figures presented in Fig. 3,
tions. However,theseapproachesfacechallenges
Fig. 4, Fig. 5, and Fig. 6. Across all contamina-
related to in-domain overfitting, where represen-
tion ratios, ColdFusion consistently outperforms
tations learned from in-scope data may not gen-
all baselines by a significant margin, reinforcing
eralize well to OOS data. In contrast to this line
ourapproach’srobustnessandeffectiveness. These
ofwork,ourapproachfocusesondetectingOOS
supplementaryresultsfurthersupportthestability
intentsratherthandiscoveringthem. Notably,our
andreliabilityofColdFusion’sperformancetrends
settinginvolvesunlabeledin-scopeintents,andour
observedinthemainanalysis.
model’spriorknowledgeislimitedtointentnames.
Effectofτ. Table6providesanablationanalysis
ofdifferentτ parametersasdefinedinEq.B.2. We
Out-of-scopeintentclassification. OOSintent
observethatselectingthe50%and75%percentiles
classificationiscategorizedbasedontheuseofex-
yieldssuboptimalperformancecomparedtousing
tensivelabeledOOSintentsamplesduringtraining.
the90%and100%percentiles. Thesepercentiles
ThefirstcategoryinvolvesmethodsthatuseOOS
involve minimal filtering. Interestingly, there is
samplesduringtraining,treatingOOSintentclassi-
a slight improvement in performance when em-
ficationasa(n+1)-classclassificationtask(Zheng
ployingthe90%percentilecomparedtothe100%
etal.,2020;Zhanetal.,2021). Incontrast,thesec-
percentile.
ondcategoryaimstominimizeintra-classvariance
and maximize inter-class variance to widen the
D RelatedWorks
marginbetweenin-scopeandOOSintents(Linand
Out-of-scope intent discovery. Out-of-scope Xu, 2019; Zeng et al., 2021). Some approaches
(OOS) intent discovery involves clustering new, (Zhang et al., 2021a; Xu et al., 2020; Zeng et al.,
unknownintentstoidentifypotentialdevelopment 2021) incorporate Gaussian distribution into the
directionsandexpandthecapabilitiesofdialogue learnedintentfeaturestoaidOOSdetection. Our
systems. Priorworks(Linetal.,2020;Zhangetal., work stands apart from this line of research as it
2021b; Mou et al., 2022) in this domain have ex- specificallyaddressesOOSintents,wherein-scope
redocnE
ETG
TENPM
redocnE
ETG
TENPMintents(topics)lacklabels, andthemodelhasno tionhasbeenproposedby(Hendrycksetal.,2019).
informationorexposuretoanyOOSintents. Itwasrecentlyestablished(Reissetal.,2021)that
Classicalanomalydetectionmethods. Detect- givensufficientlypowerfulrepresentations,asim-
ing anomalies in images has been researched for plecriterionbasedonthekNNdistancetothenor-
several decades. The methods follow three main maltrainingdataachievesstrongperformance. The
paradigms: i)Reconstruction-thisparadigmfirst bestperformingmethods(Reissetal.,2021,2022;
attempts to characterize the normal data by a set ReissandHoshen,2023)combinepre-trainingon
ofbasisfunctionsandthenattemptstoreconstruct externaldatasetsandasecondfinetuningstageon
a new example using these basis functions, typ- the provided normal samples in the training set,
ically under some constraint such as sparsity or but they require many data observations and as-
weights with a small norm. Samples with high sume that the observations are not contaminated.
reconstruction errors are atypical of normal data Toourknowledge,theonlyworkthatdealswitha
distribution and anomalous. Some notable meth- similar setting (Jeong et al., 2023) assumes prior
odsinclude: principalcomponentanalysis(Jolliffe, knowledge of anomalies, that observations come
2011)andK-nearestneighbors(kNN)(Eskinetal., from a single normal class and that they are not
2002); ii) Density estimation - another paradigm contaminatedbyanomalies.
is to first estimate the density of normal data. A
newtestsampleisdenotedasanomalousifitsesti-
mateddensityislow. Parametricdensityestimation
methodsincludeEnsemblesofGaussianMixture
Models (EGMM) (Glodek et al., 2013), and non-
parametricmethodsincludekNN(whichisalsoa
reconstruction-basedmethod)aswellaskernelden-
sity estimation (Latecki et al., 2007). Both types
ofmethodshaveweaknesses: parametricmethods
aresensitivetoparametricassumptionsaboutthe
natureofthedatawhereasnon-parametricmethods
sufferfromthedifficultyofaccuratelyestimating
density in high-dimensions; iii) One-class classi-
fication (OCC) - this paradigm attempts to fit a
parametricclassifiertodistinguishbetweennormal
training data and all other data. The classifier is
then used to classify new samples as normal or
anomalous. Suchmethodsincludeone-classsup-
port vector machine (OCSVM) (Scholkopf et al.,
2000)andsupportvectordatadescription(SVDD)
(TaxandDuin,2004).
Deeplearningforanomalydetection. Thisline
ofworkisbasedontheideaofinitializinganeural
networkwithpre-trainedweightsandthenobtain-
ingstrongerperformancebyfurtheradaptationof
the training data. DeepSVDD (Ruff et al., 2018)
suggestedtofirsttrainanauto-encoderonthenor-
mal training data, and then using the encoder as
theinitialfeatureextractor. Moreover,sincetheen-
coderfeaturesarenotspecificallyfittedtoanomaly
detection,DeepSVDDadaptstotheencodertrain-
ing data. However, this naive training procedure
leads to catastrophic collapse. An alternative di-
rection is to use features learned from auxiliary
tasksonlarge-scaleexternaldatasets. Transferring
pre-trained features for out-of-distribution detec-(a)Banking77-OOS (b)CLINC-Banking (c)CLINC-Credit_Cards
Figure3: Performancetrendswithcontaminationr =2.5%usingtheMPNETmodelovertime.
(a)Banking77-OOS (b)CLINC-Banking (c)CLINC-Credit_Cards
Figure4: Performancetrendswithcontaminationr =5%usingtheMPNETmodelovertime.
(a)Banking77-OOS (b)CLINC-Banking (c)CLINC-Credit_Cards
Figure5: Performancetrendswithcontaminationr =7.5%usingtheMPNETmodelovertime.
(a)Banking77-OOS (b)CLINC-Banking (c)CLINC-Credit_Cards
Figure6: Performancetrendswithcontaminationr =7.5%usingtheGTEmodelovertime.AUC2 AUC2 AUC2 AUC2
10% 25% 50% 100%
τ
B77 C-Bank C-Cards B77 C-Bank C-Cards B77 C-Bank C-Cards B77 C-Bank C-Cards
τ =perc(ϕ(Dt),50%) 80.1 80.4 80.7 80.6 83.6 83.0 80.7 85.1 84.6 81.3 86.5 85.4
τ =perc(ϕ(Dt),75%) 81.8 81.9 83.0 81.9 85.5 84.8 81.8 87.4 86.7 82.1 88.2 87.7
τ =perc(ϕ(Dt),100%) 82.0 81.1 85.0 82.1 86.0 86.7 81.8 88.0 88.0 82.3 89.0 88.5
τ =perc(ϕ(Dt),90%) 81.7 82.3 84.8 81.8 87.0 87.3 81.9 88.6 88.7 82.3 89.2 89.0
Table6: AUC2resultsusingtheGTEmodel,withcontaminationofr =5%. Bestresultsareinbold.
t˜