The Empirical Impact of Neural Parameter
Symmetries, or Lack Thereof
DerekLim∗ MoePutterman∗
MITCSAIL UCBerkeley
dereklim@mit.edu moeputterman@berkeley.edu
RobinWalters HaggaiMaron StefanieJegelka
NortheasternUniversity Technion,NVIDIA TUMunich,MIT
Abstract
Manyalgorithmsandobservedphenomenaindeeplearningappeartobeaffected
byparametersymmetries—transformationsofneuralnetworkparametersthat
donotchangetheunderlyingneuralnetworkfunction. Theseincludelinearmode
connectivity,modelmerging,Bayesianneuralnetworkinference,metanetworks,
and several other characteristics of optimization or loss-landscapes. However,
theoreticalanalysisoftherelationshipbetweenparameterspacesymmetriesand
thesephenomenaisdifficult. Inthiswork,weempiricallyinvestigatetheimpactof
neuralparametersymmetriesbyintroducingnewneuralnetworkarchitecturesthat
havereducedparameterspacesymmetries. Wedeveloptwomethods,withsome
provableguarantees,ofmodifyingstandardneuralnetworkstoreduceparameter
spacesymmetries. Withthesenewmethods,weconductacomprehensiveexperi-
mentalstudyconsistingofmultipletasksaimedatassessingtheeffectofremoving
parametersymmetries. Ourexperimentsrevealseveralinterestingobservations
ontheempiricalimpactofparametersymmetries;forinstance,weobservelinear
modeconnectivitybetweenournetworkswithoutalignmentofweightspaces,and
we find that our networks allow for faster and more effective Bayesian neural
networktraining.
1 Introduction
Neuralnetworkshavefoundprofoundempiricalsuccess,buthavemanyassociatedbehaviorsand
phenomenathataredifficulttounderstand. Oneimportantpropertyofneuralnetworksisthatthey
generallyhavemanyparameterspacesymmetries—foranysetofparameters,therearetypically
manyotherchoicesofparametersthatcorrespondtothesameexactneuralnetworkfunction[23].
Forinstance,permutationsofhiddenneuronsinamulti-layerperceptron(MLP)inducepermutations
ofweightsthatleavetheoverallinput-outputrelationshipunchanged. Theseparametersymmetries
areatypeof(not-necessarilydetrimental)redundancyintheparameterizationofneuralnetworks,
thataddsmuchnon-Euclideanstructuretoparameterspace.
Parameterspacesymmetriesappeartoinfluenceseveralphenomenaobservedinneuralnetworks.
For example, when linearly interpolating between the parameters of two independently trained
networkswiththesamearchitecture,theintermediatenetworkstypicallyperformpoorly[57,12].
However, if we first align the two networks via a permutation of parameters that does not affect
the network function, then the intermediate networks can perform just as well as the unmerged
networks[57,1]. Insomesense,thissuggeststhatneuralnetworklosslandscapesaremoreconvex
∗Equalcontribution
Preprint.Underreview.
4202
yaM
03
]GL.sc[
1v13202.5042:viXraStandard -Asymmetric -Asymmetric
Trained FiGLU
Fixed
Figure1: (Left)StandardMLP.Thehiddennodes(greyhatches)canbefreelypermuted,which
inducespermutationparametersymmetries. Blackedgesdenotetrainableparameters. (Middle)Our
W-AsymmetricMLP,whichfixescertainweightstobeconstantanduntrainable(coloreddashed
lines) to break parameter symmetries. (Right) Our σ-Asymmetric MLP, which uses our FiGLU
nonlinearityinvolvingafixedmatrixF(coloreddashedlines)tobreakparametersymmetries.
orwell-behavedafterremovingpermutationsymmetries. Otherareasthatparametersymmetries
playaroleinincludeinterpretabilityofneurons[19],optimization[46,80,76],modelmerging[58],
learned equivariance [6], Bayesian deep learning [33], loss landscape geometry [52], processing
neuralnetworkweightsasinputdatausingmetanetworks[38],andgeneralizationmeasures[47,10].
Torigorouslystudytheeffectofparametersymmetries,westudytheeffectofremovingthem. In
particular,weintroducetwowaysofmodifyingneuralnetworkarchitecturestoremoveparameter
spacesymmetries(seeFigure1):
(1) W-Asymmetricnetworksfixcertainelementsofeachlinearmaptobreaksymmetriesin
thecomputationgraph.
(2) σ-Asymmetricnetworksuseanewnonlinearity(FiGLU)thatdoesnotactelementwise,and
hencedoesnotinducesymmetriessuchaspermutations.
Thesetwoapproachesareinspiredbypreviouswork,whichshowsthatbothsymmetriesofcomputa-
tiongraphs[38]andequivariancesofnonlinearities[19]induceparametersymmetriesinstandard
neuralnetworks. Wetheoreticallyprovethatbothofourapproachesremoveparametersymmetries
undercertainconditions. OurAsymmetricnetworksaresimilarstructurallytostandardnetworks
andcanbetrainedwithstandardbackpropagationandfirst-orderoptimizationalgorithmslikeAdam.
Thus,theyareareasonable“counterfactual”systemforstudyingneuralnetworkswithoutparameter
symmetries.
With our Asymmetric networks, we run a suite of experiments to study the effects of removing
parametersymmetriesonseveralbasearchitectures,includingMLPs,ResNets,andgraphneural
networks. We investigate linear mode connectivity, Bayesian deep learning, metanetworks, and
monotoniclinearinterpolation. Throughthelensesoflinearmodeconnectivityandmonotoniclinear
interpolation, we see that the loss landscapes of our Asymmetric networks are remarkably more
well-behavedandclosertoconvexthanthelosslandscapesofstandardneuralnetworks. Whenusing
ourAsymmetricnetworksasthebasemodelinaBayesianneuralnetwork,wefindfastertraining
andbetterperformancethanusingstandardneuralnetworksthathavemanyparametersymmetries.
Whenusingmetanetworkstopredictpropertiessuchastestaccuracyofaninputneuralnetwork,we
seethatalltestedmetanetworksmoreaccuratelypredicttheaccuracyofAsymmetricnetworksthan
standardnetworks. Overall,ourAsymmetricnetworksprovidevaluableinsightsforempiricalstudy
andholdpromiseforadvancingourunderstandingoftheimpactofneuralparametersymmetries.
2 BackgroundandDefinitions
LetΘbethespaceofparametersofafixedneuralnetworkarchitecture. Foranychoiceofparameters
θ ∈Θ,wehaveaneuralnetworkfunctionf :X →Y fromaninputspaceX toanoutputspaceY.
θ
Wecallafunctionϕ:Θ→Θaparameterspacesymmetryiff (x)=f (x)forallinputsxand
θ ϕ(θ)
parametersθ ∈Θ(i.e. iff andf arealwaysthesamefunction).
θ ϕ(θ)
Forinstance,consideratwo-layerMLPwithnobiases,parameterizedbymatricesθ =(W ,W )
2 1
withanelementwisenonlinearityσ. Thenf (x) = W σ(W x). LetP beapermutationmatrix,
θ 2 1
2andletϕ(θ)=(W P⊤,PW ). Thenforanyinputx,
2 1
f (x)=W P⊤σ(PW x)=W P⊤Pσ(W x)=W σ(W x)=f (x), (1)
ϕ(θ) 2 1 2 1 2 1 θ
so ϕ is a parameter space symmetry. A key step is the second equality, which holds because
Pσ(x)=σ(Px):anyelementwisenonlinearityσispermutationequivariant. Anyotherequivariance
ofσalsoinducesaparametersymmetry;forinstance,ifσ(x)=max(0,x)istheReLUfunction,then
ασ(x)=σ(αx)foranyα>0,sothereisapositive-scaling-basedparametersymmetry[47,10,19].
3 RelatedWork
Characterizingparameterspacesymmetries. Whilemanyworksspanningseveraldecadeshave
notedspecificparameterspacesymmetriesinneuralnetworks[23,59], someworkstakeamore
systematicapproachtoderivingparameterspacesymmetries. Godfreyetal.[19]characterizeall
global linear symmetries induced by the nonlinearity for two-layer multi-layer perceptrons with
pointwisenonlinearities. Zhaoetal.[74]studyseveraltypesofsymmetries,andderivenonlinear,
data-dependentparameterspacesymmetries. Limetal.[38]showthatgraphautomorphismsofthe
computationgraphofaneuralnetworkinducepermutationparametersymmetries,whichcaptures
hiddenneuronpermutationsinMLPsandhiddenchannelpermutationsinCNNs.
Constraintsandpost-processingtobreakparameterspacesymmetries. Severalworksdevelop
methods for constraining or post-processing the weights of a single neural network to remove
ambiguitiesfromparameterspacesymmetries. Thisincludesmethodstoremovescalingsymmetries
induced by normalization layers or positively-homogeneous nonlinearities such as ReLU [5, 53,
52,35],methodstoremovepermutationsymmetries[53,52,69,35],andmethodstoremovesign
symmetriesinducedbyoddactivationfunctions[69]. Unlikethesepreviousworks,wedevelopneural
networkarchitecturesthathavereducedparameterspacesymmetries. Ourmodelsareoptimized
usingstandardunconstrainedgradient-descentbasedmethodslikeAdam. Hence,ournetworksdonot
requireanynon-standardoptimizationalgorithmssuchasmanifoldoptimizationorprojectedgradient
descent[5,53],nordotheyrequirepost-training-processingtoremovesymmetriesorspecialcare
duringanalysisofparameters(suchasgeodesicinterpolationinaRiemannianweightspace[52]).
Aligning multiple networks for relative invariance to parameter symmetries. One way to
reducetheimpactofparametersymmetriesincertainsettings,especiallyformodelmerging,isto
aligntheparametersofonenetworktoanother. Severalmethodshavebeenproposedforchoosing
permutationsthataligntheparametersoftwoneuralnetworksofthesamearchitecture,viaefficient
heuristics or learned methods [3, 67, 61, 12, 1, 51, 45, 65]. Other approaches relax the exact
permutation-parameter-symmetry constraint or do additional postprocessing to achieve effective
merging of models in parameter space [57, 27, 28, 58, 54]. As our Asymmetric networks have
removed parameter symmetries, they can often be successfully merged and linearly interpolated
betweenwithoutanyalignment.
4 AsymmetricNetworks
Wedeveloptwomethodsofparameterizingneuralnetworkarchitectureswithoutparametersym-
metries, both of which are justified by theoretical results. We first focus on the case of fully-
connected MLPs with no biases, which take the form f (x) = W σ(W ···σ(W x)) for
θ L L−1 1
weightsθ =(W ,...,W )andnonlinearityσ. TheninSection4.3,wediscusshowweusethese
L 1
approachestoremoveparametersymmetriesinotherarchitecturesaswell.
4.1 ComputationGraphApproach(W-AsymmetricNetworks)
Ourfirstapproachtodevelopingneuralnetworkswithgreatlyreducedparameterspacesymmetries
reliesontheircomputationgraph. Inparticular,wecanwriteafeedforwardneuralnetworkarchi-
tectureasaDAGG=(V,E)withneuronsasnodesV andconnectionsbetweenthemasedgesE.
Forachoiceofparametersθ ∈R|E|,wegetafunctionf frominputneuronspacetooutputneuron
θ
space[18,47,38]
Limetal.[38]showedthatneuralDAGautomorphismsϕ,whicharegraphautomorphismsofthe
DAGGthatpreservetypesofnodesandweight-sharingconstraints,inducepermutationparameter
3Standard -Asymmetric
Linear
-Asymmetric -Asymmetric
Standard (Fix entries) (Fix filters)
Convolution
Figure2: DepictionofourW-Asymmetricapproachtoremovingparametersymmetries. Entries
withablackoutlineareuntrained. NotethattheW-Asymlinearmaphas2nonzerosperrow,the
W-Asymconvolutionwithfixedentrieshas8fixedentriesforitssingleoutputchannel, andthe
W-Asymconvolutionwithfixedfiltershasasingleinputfilterfixed. Weoftenuseaconstantnumber
offixedentriesperroworoutputchannelinourexperiments.
symmetriesϕthatleavethefunctionunchanged: f =f . Thus,anyfeedforwardneuralnetwork
θ ϕ(θ)
architecture that has no permutation parameter symmetries must necessarily have a computation
graphwithnonontrivialneuralDAGautomorphisms.
To modify MLPs so they have no nontrivial neural DAG automorphisms, we mask edges in the
computationgraph,bysettingcertainedgeweightstoconstantvaluesthatarenotupdatedduring
training. For an MLP, we can do this by enforcing that every linear layer T : Rd1 → Rd2 takes
theformofamatrixW ∈Rd2×d1,whereeachrowhasauniquepatternofuntrainedweights. To
achievethis,defineamaskM ∈{0,1}d2×d1 suchthatW
ij
isatrainableparameterifandonlyif
M
ij
= 1, andtherowsofM arepairwisedistinctbinaryvectorsin{0,1}d1. Wecallanyneural
networkwithlinearmapsmaskedassuchaW-Asymmetricneuralnetwork. InAppendixB.1,we
showthatmaskingtheseentriessothattheyarenottrainedissufficienttoremoveallnontrivialneural
DAGautomorphisms.
Theorem1. IfeachmaskmatrixM hasuniquenonzerorows,thenW-AsymmetricMLPswithfixed
entriessettozerohavenonontrivialneuralDAGautomorphisms.
Inpractice,wegenerateabinarymaskM byrandomlyselectingasubsetofn fixedelementsfor
fix
eachrow. Forthefixedentries,wesamplethemfromanormaldistributionN(0,κI)withstandard
deviationκ>0thatwetune. Ourasymmetriclinearlayercanbewrittenas
W′ =M ⊙W+(1−M)⊙F, (2)
whereW∈Rd2×d1 isamatrixoftrainableparameters,andF∈Rd2×d1 isamatrixoffixedelements,
sampledfromN(0,κI).TheonlytrainableparametersaretheunmaskedentriesofM⊙W,ofwhich
thereared ·(d −n ). Weempiricallyfindthathavingκbesignificantlylargerthanthestandard
2 1 fix
deviationoftypicalinitializationsforweightmatrices(e.g. κ=1whilethetrainedcoefficientshave
√
standarddeviationabout1/ 1000)isimportantforbreakingparametersymmetries.
4.2 NonlinearityApproach(σ-AsymmetricNetworks)
Another approach for removing parameter symmetries is to change the nonlinearity. As studied
by Godfrey et al. [19], equivariances of the nonlinearity induce parameter symmetries in MLPs
withelementwise nonlinearities. Recall thatanelementwise nonlinearityactsby usingthe same
function on each coordinate of the input; σ : Rd → Rd is elementwise if it takes the form
σ(x)=(σ (x ),...,σ (x ))forsomerealfunctionσ :R→R. Anyelementwisenonlinearityis
1 1 1 d 1
permutationequivariant,andhenceinducesapermutationparametersymmetry.
Thus, in contrast to most neural network architectures, for Asymmetric networks we must use
a nonlinearity that does not act elementwise. Likewise, the nonlinearity cannot have any linear
4symmetryitself,sinceifσ◦A=B◦σforA,B ∈GL(d),thenforatwo-layernetwork:
W ◦σ◦W =W B−1B◦σ◦W =W B−1◦σ◦AW . (3)
2 1 2 1 2 1
So(W ,W )and(W B−1,AW )givethesameneuralnetworkfunction. Thus,inordertodefine
2 1 2 1
amodelclasswithoutparametersymmetries,itisnecessaryforσtohavenolinearequivariances,
i.e. wedesirethatifσ◦A=B◦σforA,B ∈GL(d),thenA=B =I. Fortwo-layerMLPswith
squareinvertibleweights,thisisinfactsufficienttoremoveallparametersymmetries: weprovethis
inAppendixB.2.
Proposition1. LettheparameterspaceΘbeallpairsofsquareinvertiblematricesθ =(W ,W )
2 1
for W ,W ∈ GL(d), and let f (x) = W σ(W x). If σ has no linear equivariances, then
2 1 θ 2 1
f =f ifandonlyifθ =θ . Inotherwords,therearenonontrivialparameterspacesymmetries.
θ1 θ2 1 2
4.2.1 FiGLU:theFixedGatedLinearUnitNonlinearity
MotivatedbyProposition1,wedefineanon-elementwisenonlinearitythatdoesnothavetheequiv-
ariancesofstandardnonlinearities. Lettingηbethesigmoidfunctionη(x)= 1 ,wedefineour
1+e−x
nonlinearityas
σ(x)=η(Fx)⊙x, (4)
forarandomlysampled,untrainedmatrixF. Inthiswork,wesampleFasani.i.dGaussianmatrix
withvariancethatwetune. ThisnonlinearityissimilartoSwish/SiLU[55,25]withanadditional
matrixFtomixfeaturedimensions(tobreakpermutationequivariance),anditisalsosimilartoa
gatedlinearunit(GLU)withnotrainableparameters[8]. Thus,wecallournonlinearityFiGLU:the
FixedGatedLinearUnit.
In Appendix B.2.1, we prove that FiGLU does not have permutation equivariances or diagonal
equivariances,whicharetheonlyequivariancesformostelementwisenonlinearities[19].
Proposition2. Withprobability1overthesamplingofF,FiGLUhasnopermutationequivariances
ordiagonalequivariances.
Wecallanynetworkwithoursymmetry-breakingFiGLUnonlinearityaσ-AsymmetricNetwork.
4.3 ExtensiontoOtherArchitectures
Thegraph-basedapproach(W-AsymmetricNetworks)worksnaturallyforneuralnetworkarchi-
tectureswith“channel”dimensions,suchasconvolutionalneuralnetworks(CNNs),graphneural
networks(GNNs)[21], Transformers[64], andequivariantneuralnetworksbasedonequivariant
linearmaps[15]. Inthesetypesofnetworks,permutationsofentirechannelsinducepermutation
parametersymmetries[38]. Forsuchnetworks,wethusmaskentireconnectionsbetweenchannels,
e.g. entirefiltersinCNNs. ForCNNs,wealsoexperimentwithrandomlymaskingsomenumberof
entriesineachfilter(insteadofmaskingentirefilters),andfindthatthisalsoworkswellinremoving
parametersymmetries. Forneuralnetworkswithlinearlayersthatincludebiasterms, wedonot
modifythebiasesinanyway,astheydonotintroducenewcomputationgraphautomorphisms[38].
Thenonlinearity-basedapproach(σ-AsymmetricNetworks)canbestraightforwardlyappliedtomany
generalarchitecturesaswell. Though,thefixedmatrixFmayhavetobechangedtoastructured
linearmap;forinstance,inCNNswetakeFtobea1Dconvolution.
4.4 UniversalApproximation
Our two approaches remove parameter symmetries from standard neural networks, but still intu-
itivelyretainmuchofthestructureofstandardnetworks. Oneimportantpropertyofwidely-used
neuralnetworkarchitecturesisuniversalapproximation—foranytargetfunctionofacertaintype,
thereexistsaneuralnetworkofthegivenarchitecturethatapproximatesthetargettoanarbitrary
accuracy[7,24,41,72]. InAppendixB.3,weshowthatW-AsymmetricMLPsretainthisproperty:
Theorem2(Informal). Forn
fix
∈o(n1 4),wherenisthehiddendimension,W-AsymmetricMLPs
areUniversalApproximatorswithprobability1overthechoiceofhardwiredentries.
5MLP MNIST ResNet 8x width CIFAR-10 GNN ogbn-arXiv
0.40
Standard Standard 2.75 Standard
0.35 Rebasin 3.0 Rebasin 2.50 Rebasin
0.30 A As sy ym m W 2.5 A As sy ym m W 2.25 A As sy ym m W
0.25 2.0 2.00
0.20 1.5 1.75
0.15 1.0 1.50
0.10 1.25
0.5
0.05 1.00
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Interpolation Interpolation Interpolation
Figure 3: Linear mode connectivity: test loss curves along linear interpolations between trained
networks. (Left)MLPonMNIST.(Middle)ResNetwith8×widthonCIFAR-10. (Right)GNN
onogbn-arXiv. W-Asymmetricnetworksinterpolatethebest,followedbynetworksalignedwith
Git-Rebasin,thenσ-Asymmetricnetworks,andfinallystandardnetworks.
5 Experiments
5.1 LinearModeConnectivitywithoutPermutationAlignment
Background. Manyworkshavestudiedlinearmodeconnectivity,whichiswhenallnetworkson
thelinesegmentinparameterspacebetweentwowell-performingtrainednetworksarealsowell-
performing[17,42,12]. Whenthetwonetworksarerandomlyinitializedandtrainedindependently,
linearmodeconnectivitygenerallydoesnothold[12,1]. However,ifoneofthetwonetworksis
permutedwithaparametersymmetrythatdoesnotchangeitsfunction,butthatalignsitsparameters
withtheothernetwork,thenlinearmodeconnectivityempiricallyandtheoreticallyholdsformany
moremodel/taskcombinations[12,1,75,13]. Infact,Entezarietal.[12]conjecturesthatifall
permutationsymmetriesareaccountedfor,thenlinearmodeconnectivitygenerallyholds. Sinceour
Asymmetricnetworksremoveparameterspacesymmetries,wemayexpectlinearmodeconnectivity
tohold,withoutanypost-processingoralignmentstep.
Hypothesis. Asymmetricnetworksaremorelinearlymodeconnectedthanstandardnetworks,and
donotrequirepost-processingoralignmentofpairsofnetworksbeforemerging.
ExperimentalSetup. Weconsiderseveralnetworksandtasks: MLPsonMNIST,ResNets[22]on
CIFAR-10,andGraphNeuralNetworks[71]onogbn-arXiv[26]. Foreacharchitectureandtask,
wecomputethemidpointtestlossbarrier: L(1θ + 1θ )− 1(L(θ )+L(θ )). Thismeasureshow
2 1 2 2 2 1 2
muchworsetheinterpolatednetworkwithparameters 1θ + 1θ isthantheoriginalnetworkswith
2 1 2 2
parametersθ andθ . Wemeasurethisbarrierforstandardnetworks,pairsofnetworksalignedwith
1 2
Git-Rebasin[1],andnetworkswithourtwoapproaches(σ-AsymandW-Asym)applied.
Results. Figure3plotsinterpolationcurvesandTable1displaysmidpointtestlossbarriersofvarious
methods. Ourσ-Asymmetricapproachlowersthetestlossbarriercomparedtostandardnetworks,
butfallsshortofthealignmentapproachofGit-Rebasin. Ontheotherhand,ourW-Asymmetric
approachachievesstrong(andsometimesperfect)interpolation,andinterpolatesbetterthanstandard
networksalignedviaGit-ReBasin. ThismaybecausedbyfailureoftheGit-ReBasinapproaches
to find the optimal permutations, importance of other parameter symmetries besides layer-wise
permutations,orotherpropertiesofW-Asymmetricnetworks.
Table1: Testlossinterpolationbarriersatmidpoint: L(1θ + 1θ )− 1(L(θ )+L(θ )). Weuse
2 1 2 2 2 1 2
differentmethodsofbreakingsymmetriesineachcolumn;fromlefttoright: nosymmetrybreaking,
Git-Rebasin[1],ourσ-Asymapproach,andourW-Asymapproach. Wereportmeanandstandard
deviationofthebarrieracrossatleast5pairsofnetworks,andboldlowestbarriers.
Standard Git-ReBasin σ-Asym(ours) W-Asym(ours)
MLP(MNIST) 0.188±.12 −.006±.00 0.117±.01 −0.012±.00
ResNet(CIFAR-10) 3.287±.32 2.041±.21 2.521±.46 0.934±.72
ResNet8xwidth(CIFAR-10) 2.640±.24 0.509±.45 1.492±.15 0.031±.05
GNN(ogbn-arXiv) 1.475±.24 0.269±.02 0.901±.11 0.095±.03
6
ssoL
tseT
ssoL
tseT
ssoL
tseT5.2 BayesianNeuralNetworks
Background. Bayesiandeeplearningisapromisingapproachtoimproveseveraldeficitsofmain-
streamdeeplearningmethods,suchasuncertaintyquantificationandintegrationofpriors[29,49].
However,parametersymmetriesareproblematicinBayesianneuralnetworks,astheyareamajor
sourceofstatisticalnonidentifiability[29]. Parametersymmetriesintroducemodesintheposterior
p(θ|D)thatmaketheposteriorhardertoapproximate[2,33,70],samplefrom[48,69],andother-
wiseanalyze[35]. Forinstance,onecommontechniquefortrainingBayesianneuralnetworksis
variationalinferenceviafittingaGaussiandistributiontothetrueposteriorp(θ|D). Thisapproach
suffersbecausetheGaussiandistributionhasonlyonemode,whereasthetrueposteriorhasatleast
onemodeforeveryparametersymmetry.
Table2: Bayesianneuralnetworkresults. Allresults(exceptforlastcolumn)areafter50epochs
oftraining. W-Asymmetricnetworkstendtoimproveovertheirstandardcounterparts,especially
earlyintraining. 16-layerMLPsfailtotrain,but16-layerW-AsymmetricMLPssuccessfullytrain.
StandardorAsymmetricnetworksbetterthantheircounterpartbyastandarddeviationarebolded.
Model TrainLoss↓ TestLoss↓ ECE↓ TestAcc↑ TestAcc(25Epochs)↑
MLP-8 1.34±.00 1.24±.01 .039±.009 56.37±.31 52.87±0.2
W-AsymMLP-8 1.31±.01 1.22±.01 .042±.009 57.08±.50 54.15±0.2
MLP-16 2.29±.02 2.28±.03 .026±.017 13.54±2.0 13.34±2.7
W-AsymMLP-16 1.39±.01 1.27±.01 .045±.009 55.16±.44 51.42±0.3
ResNet20 .596±.01 .535±.03 .045±.007 81.98±1.2 72.37±1.0
W-AsymResNet20 .600±.02 .535±.01 .044±.004 81.94±0.6 73.64±1.5
ResNet110 .803±.08 .706±.08 .052±.007 75.71±2.8 59.85±3.9
W-AsymResNet110 .745±.07 .658±.06 .049±.004 77.40±2.4 63.20±3.0
ResNet20(BN) 1.68±.03 1.57±.02 .078±.004 56.83±.62 46.80±0.9
W-AsymResNet20(BN) 1.62±.02 1.50±.03 .076±.006 58.40±.62 49.29±0.4
ResNet20(LN) 1.97±.02 1.88±.02 .090±.007 50.02±.54 37.24±1.1
W-AsymResNet20(LN) 1.91±.03 1.82±.02 .086±.006 51.20±.47 39.03±1.0
Hypothesis. UsingAsymmetricnetworksasthebasemodelimprovesBayesianneuralnetworks,as
theposteriorwillhavelessmodes.
Experimentalsetup. WetrainStandardBayesianandAsymmetricBayesianNetworksforimage
classificationusingvariationalinference. Weusethemethodof[62]forvariationalinference,which
fitsaGaussianapproximateposteriorwithadiagonalpluslow-rankcovariance. Wetrain10instances
of each model and then report train loss, test loss, test accuracy, and Expected Calibration Error
(ECE)[43],whichisameasureofcalibration.
Results. SeetrainingcurvesinFigure4,andquantiativeresultsinTable2. UsingW-Asymmetric
networks as a base for Bayesian deep learning improves training speed and convergence. Most
strikingly,BayesianMLPsofdepth16cannottrainatall,whileW-AsymmetricBayesianMLPs
trainwell. Ingeneral,theW-Asymmetricapproachimprovestrainingandtestaccuracyacrossthe
severalmodels(MLPs,ResNetsofvaryingsizes,andResNetswitheitherbatchnormorlayernorm).
MLP MNIST ResNet110 CIFAR-10 ResNet20 BN CIFAR-100
100 Standard 3×100 Standard Standard
W-Asym W-Asym 4×100 W-Asym
2×100
3×100
101
100 2×100
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
Epoch Epoch Epoch
Figure 4: Bayesian neural network training loss over time for depth 8 MLPs on MNIST (left),
ResNet110 on CIFAR-10 (middle), and ResNet20 with BatchNorm on CIFAR-100 (right). W-
Asymmetricnetworkstrainmorequickly,andachievelowertrainingloss.
5.3 Metanetworks
Background. Metanetworks [38] — also referred to as deep weight-space networks [44, 56],
meta-models[34],orneuralfunctionals[77,78,79]—areneuralnetworksthattakeasinputsthe
7
01-RAFIC
01-RAFIC
001-RAFIC
ssoL
niarT
ssoL
niarT
ssoL
niarTTable3: MetanetworkperformanceforpredictingthetestaccuracyofsmallResNetsandourW-
AsymResNets. Eachrowisadifferentmetanetwork. ReportedareR2andKendallτ onthetestset
—higherisbetter.
ResNet W-AsymResNet
R2 τ R2 τ
MLP −.171±.11 .311±.02 .594±.12 .864±.01
DMC[11] .950±.01 .787±.02 .967±.01 .911±.01
DeepSets[73] .855±.01 .617±.03 .936±.00 .858±.00
StatNN[63] .976±.00 .866±.00 .978±.00 .935±.01
Standard ResNet -Asymmetric ResNet W-Asymmetric ResNet
5 2.5
8
4 2.0
6
3 1.5
4
2 1.0
1 2
0.5
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Interpolation Interpolation Interpolation
Figure5: Trainlossagainstinterpolationcoefficientαfortheinterpolation(1−α)θ +αθ between
0 T
initialparametersθ andtrainedparametersθ . Trajectoriesforthe20(θ ,θ )pairsoflowesttrain
0 T 0 T
lossforeacharchitectureareplotted. ThetrajectoriesforAsymmetricResNetsappearsignificantly
moremonotonicandconvex.
parametersofotherneuralnetworks. Recentworkhasfoundthatmakingmetanetworksinvariantor
equivarianttoparameter-spacesymmetriesoftheinputneuralnetworkscansubstantiallyimprove
metanetworkperformance[44,77,38,31].
Hypothesis. Asymmetricnetworksareeasiertotrainmetanetworksonbecausetheydonothaveto
explicitlyaccountforsymmetries.
Experimentalsetup. WeexperimentwithmetanetworksonthetaskofpredictingtheCIFAR-10test
accuracyofaninputimageclassifier,whichmanymetanetworkshavebeentestedon[63,11,77,38].
WeusemetanetworksbasedonsimpleMLPs,1D-CNNmetanetworks[11],andmetanetworksthat
areexactlyinvariant topermutationparametersymmetries: DeepSets[73]and StatNN[63]. We
traintwoseparatedatasetsof10,000imageclassifiers: onedatasetofsmallResNetmodels,andone
datasetofW-AsymmetricResNetmodels. Moreinformationonthedata,metanetworks,andtraining
detailsareinAppendixE.3.
Results. InTable3,weseethatmetanetworksaresignficantlybetteratpredictingtheperformance
ofourW-AsymmetricResNetsthanstandardResNets. Interestingly,simpleMLPmetanetworks,
whichviewtheinputparametersasaflattenedvector,canpredictthetestaccuracyofAsymmetric
Networksquitewell,butfailonstandardnetworks. Also,thepermutationequivariantmetanetworks
(DeepSetsandStatNN)bothimproveonW-AsymResNetscomparedtoonResNets,eventhough
the permutation symmetries of standard ResNets do not affect these metanetworks; thus, it may
bepossiblethatothersymmetriesinstandardResNets(butnotAsym-ResNets)harmmetanetwork
performance,ortheymaybeotherfactorsbesidessymmetriesthatimprovemetanetworkperformance
forAsym-ResNets.
5.4 MonotonicLinearInterpolation
Background. One common method of studying the loss landscapes of neural networks is by
studyingtheone-dimensionallinesegmentofparametersattainedbylinearinterpolationbetween
parametersatinitializationandparametersaftertraining. Manyworkshaveobservedmonotonic
linear interpolation (MLI), which is when the training loss monotonically decreases along this
linesegment[20,16,40,66]. Losslandscapesofconvexproblemshavethispropertyaswell,so
presenceofthemonotoniclinearinterpolationpropertyhasbeenusedasaroughmeasureofhow
well-behavedthelosslandscapeis. However,withmanytypesofmodels,tasks,orhyperparameter
settings,monotoniclinearinterpolationdoesnothold[40,66],orthereisalargeplateauwherethe
8
ssoL
niarT
ssoL
niarT
ssoL
niarTTable 4: Monotonic linear interpolation: properties of linear interpolations between 300 pairs
of initialization and trained parameters. Arrows denote behavior that is more similar to convex
optimization,e.g. thereisadownarrow(↓)nextto∆becauseconvexobjectiveshavenonpositive
∆,whilenonconvexcanhavepositive∆. ForbothtypesofAsymmetricnetworks,alldifferences
fromStandardResNetsarestatisticallysignificant(p<.001)underatwo-sidedT-test: Asymmetric
networkshavesignificantlymoremonotonicandconvexlinearinterpolationsfrominitialization.
∆↓ PercentMonotonic↑ LocalConvexity↑ GlobalConvexity↑
StandardResNet .079±.109 26.3% .548±.139 .823±.229
σ-AsymResNet .004±.047 87.3% .675±.143 .976±.098
W-AsymResNet −.027±.026 100% .769±.165 1.00±.000
lossbarelychangesformuchofthelinesegment[16,68];neitherofthesepropertiescanhappenfor
convexobjectivestrainedtocompletion. Tothebestofourknowledge,therehasbeenlittleworkon
theroleofparametersymmetries—orlackthereof—inmonotoniclinearinterpolation(besides
oneminorexperimentin[40]AppendixC.9). Nonetheless,sinceremovingparametersymmetries
substantiallyimproveslinearinterpolationbetweentrainednetworks(Section5.1),onemayexpect
removingparametersymmetriestoimprovemonotoniclinearinterpolation.
Hypothesis. Thetraininglossalongthelinesegmentbetweeninitializationandtrainedparametersis
moremonotonicandconvexforAsymmetricnetworks.
Experimental setup. For the learning task, we follow the setup used for creating the dataset
of image classifiers in Section 5.3. In particular, we train 300 standard ResNets and 300 W-
AsymmetricResNetswithvaryinghyperparameterssampledfromthesamedistributionsasused
forthedatasetofimageclassifiers(seeAppendixTable8). Foreachofthesenetworks,welinearly
interpolatebetweenitsinitialparametersθ anditsfinaltrainedparametersθ : (1−α)θ +αθ
0 T 0 T
for 25 uniformly spaced values 0 = α < α < ... < α = 1. To measure monotonicity, we
1 2 25
record the maximum increase between adjacent networks ∆ = max(L(α )−L(α )), and the
i+1 i
percentageofnetworksthathave∆≤0i.e. thepercentageofnetworksthatsatisfymonotoniclinear
interpolation. Tomeasureconvexity,weconsideralocalconvexitymeasure(theproportionofα
i
wherethecentereddifferencesecondderivativeapproximationisnonnegative)andaglobalconvexity
measure(theproportionofα suchthatL(α )liesbelowthelinesegmentbetweentheendpoints,i.e.
i i
L(α )≤(1−α )L(0)+α L(1)).
i i i
Results. Table4showsthemeasuresofmonotonicityandconvexityforstandard,σ-Asymmetric,and
W-AsymmetricResNets. Remarkably,everysingleoneofthe300W-AsymmetricResNetssatisfies
monotoniclinearinterpolationandhasatrajectorythatliesunderneaththelinesegmentbetween
theendpoints. Qualitatively,wecanseeinFigure5thatW-AsymmetricResNetsdonothaveany
clearlossbarriersfrominitialization,noranylossplateausthatindicatenonconvexity. Incontrast,the
majorityofstandardResNetshavenon-monotonictrajectories,andthemonotonictrajectoriesseemto
bemorenonconvex. σ-Asymmetricnetworktrajectoriesaresignficantlymoreconvexandmonotonic
thanstandardnetworktrajectories,buttherearesomenon-monotonicornonconvextrajectoriesstill.
5.5 OtherOptimizationandLossLandscapeProperties
InAppendixA,wenoteotherinterestingdifferencesinoptimizationandlosslandscapepropertiesof
Asymmetricandstandardneuralnetworks. Thesecanbesummarizedas:
1. Even though Asymmetric networks interpolate much better than standard networks, the
parametersoftrainedAsymmetricnetworksareoftenbasicallythesamedistanceawayfrom
eachotherinweightspaceasstandardnetworks.
2. Asymmetricnetworksdonottendtooverfitasmuch: thedifferenceintrainperformance
andtestperformancecanbesubstantiallylowerthanthatofstandardnetworks.
3. Asymmetricnetworkscantakelongertotrain,especiallywhenchoosinghyperparameters
thatmakethemmoredissimilartostandardnetworks.
96 Discussion
WhilemanypropertiesofAsymmetricnetworksareinlinewithourhypothesesandintuitionabout
theimpactofremovingparametersymmetries,therearemanyunexpectedeffectsandunanswered
questions that are promising to further investigate. For instance, we did not extensively explore
Asymmetricnetworksinthecontextofmodelinterpretability,generalizationmeasuresinweight
spaces,oroptimizationimprovements,allofwhichareknowntobeinfluencedtosomeextentby
parametersymmetries. FurtherstudyingthepropertiesinSection5.5,thedependenceofbehavioron
thechoicesofAsymmetry-inducinghyperparameters,andotherdesignchoicesinmakingnetworks
asymmetriccouldalsobringmoreinsightsintoparameterspacesymmetries. Webelievethatfuture
theoreticalandempiricalstudyofAsymmetricnetworkscouldgarnermanyinsightsintotheroleof
parametersymmetriesindeeplearning.
Acknowledgements
WewouldliketothankKwangjunAhn,BenjaminBanks,NimaDehmamy,NikosKaralias,Jinwoo
Kim,MarcLaw,HannahLawrence,ThienLe,JonathanLorraine,JamesLucas,BehroozTahmasebi,
and Logan Weber for discussions at various points of this project. DL is supported by an NSF
GraduateFellowship. RWissupportedinpartbyNSFaward2134178. HMistheRobertJ.Shillman
Fellow,andissupportedbytheIsraelScienceFoundationthroughapersonalgrant(ISF264/23)and
anequipmentgrant(ISF532/23). ThisresearchwassupportedinpartbyOfficeofNavalResearch
grantN00014-20-1-2023(MURIML-SCOPE),NSFAIInstituteTILOS(NSFCCF-2112665),NSF
award2134108,andtheAlexandervonHumboldtFoundation.
References
[1] SamuelAinsworth,JonathanHayase,andSiddharthaSrinivasa. Gitre-basin: Mergingmodels
modulo permutation symmetries. In The Eleventh International Conference on Learning
Representations,2023. URLhttps://openreview.net/forum?id=CQsmMYmlP5T.
[2] LaurenceAitchison,AdamYang,andSebastianWOber.Deepkernelprocesses.InInternational
ConferenceonMachineLearning,pages130–140.PMLR,2021.
[3] StephenAshmoreandMichaelGashler. Amethodforfindingsimilaritybetweenmulti-layer
perceptronsbyforwardbipartitealignment. In2015InternationalJointConferenceonNeural
Networks(IJCNN),pages1–7.IEEE,2015.
[4] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint
arXiv:1607.06450,2016.
[5] VijayBadrinarayanan,BamdevMishra,andRobertoCipolla. Understandingsymmetriesin
deepnetworks. arXivpreprintarXiv:1511.01029,2015.
[6] GeorgBökmanandFredrikKahl. InvestigatinghowreLU-networksencodesymmetries. In
Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:
//openreview.net/forum?id=8lbFwpebeu.
[7] GeorgeCybenko. Approximationbysuperpositionsofasigmoidalfunction. Mathematicsof
control,signalsandsystems,2(4):303–314,1989.
[8] YannNDauphin,AngelaFan,MichaelAuli,andDavidGrangier. Languagemodelingwith
gatedconvolutionalnetworks. InInternationalconferenceonmachinelearning,pages933–941.
PMLR,2017.
[9] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural
networkswithcutout. arXivpreprintarXiv:1708.04552,2017.
[10] LaurentDinh,RazvanPascanu,SamyBengio,andYoshuaBengio.Sharpminimacangeneralize
fordeepnets. InInternationalConferenceonMachineLearning,pages1019–1028.PMLR,
2017.
[11] Gabriel Eilertsen, Daniel Jönsson, Timo Ropinski, Jonas Unger, and Anders Ynnerman.
Classifying the classifier: dissecting the weight space of neural networks. arXiv preprint
arXiv:2002.05688,2020.
10[12] RahimEntezari,HanieSedghi,OlgaSaukh,andBehnamNeyshabur. Theroleofpermutation
invariance in linear mode connectivity of neural networks. In International Conference on
LearningRepresentations,2022. URLhttps://openreview.net/forum?id=dNigytemkL.
[13] DamienFerbach,BaptisteGoujaud,GauthierGidel,andAymericDieuleveut. Provinglinear
modeconnectivityofneuralnetworksviaoptimaltransport. InInternationalConferenceon
ArtificialIntelligenceandStatistics,pages3853–3861.PMLR,2024.
[14] MatthiasFeyandJanEricLenssen. Fastgraphrepresentationlearningwithpytorchgeometric.
arXivpreprintarXiv:1903.02428,2019.
[15] MarcFinzi,MaxWelling,andAndrewGordonWilson. Apracticalmethodforconstructing
equivariantmultilayerperceptronsforarbitrarymatrixgroups. InInternationalconferenceon
machinelearning,pages3318–3328.PMLR,2021.
[16] JonathanFrankle. Revisiting"qualitativelycharacterizingneuralnetworkoptimizationprob-
lems",2020.
[17] JonathanFrankle,GintareKarolinaDziugaite,DanielRoy,andMichaelCarbin. Linearmode
connectivityandthelotterytickethypothesis.InInternationalConferenceonMachineLearning,
pages3259–3269.PMLR,2020.
[18] Cedric Gegout, Bernard Girau, and Fabrice Rossi. A mathematical model for feed-forward
neuralnetworks: theoreticaldescriptionandparallelapplications. PhDthesis,Laboratoirede
l’informatiqueduparallélisme,1995.
[19] CharlesGodfrey,DavisBrown,TeganEmerson,andHenryKvinge. Onthesymmetriesofdeep
learningmodelsandtheirinternalrepresentations. AdvancesinNeuralInformationProcessing
Systems,35:11893–11905,2022.
[20] Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural
networkoptimizationproblems. ICLR,2015.
[21] WilliamLHamilton. Graphrepresentationlearning. Morgan&ClaypoolPublishers,2020.
[22] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage
recognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages770–778,2016.
[23] RobertHecht-Nielsen. Onthealgebraicstructureoffeedforwardnetworkweightspaces. In
AdvancedNeuralComputers,pages129–135.Elsevier,1990.
[24] RobertHecht-Nielsen. Theoryofthebackpropagationneuralnetwork. InNeuralnetworksfor
perception,pages65–93.Elsevier,1992.
[25] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415,2016.
[26] WeihuaHu,MatthiasFey,MarinkaZitnik,YuxiaoDong,HongyuRen,BowenLiu,Michele
Catasta,andJureLeskovec. Opengraphbenchmark: Datasetsformachinelearningongraphs.
Advancesinneuralinformationprocessingsystems,33:22118–22133,2020.
[27] MoritzImfeld,JacopoGraldi,MarcoGiordano,ThomasHofmann,SotirisAnagnostidis,and
SidakPalSingh. Transformerfusionwithoptimaltransport. arXivpreprintarXiv:2310.05719,
2023.
[28] KellerJordan,HanieSedghi,OlgaSaukh,RahimEntezari,andBehnamNeyshabur. REPAIR:
REnormalizingpermutedactivationsforinterpolationrepair. InTheEleventhInternational
ConferenceonLearningRepresentations,2023. URLhttps://openreview.net/forum?
id=gU5sJ6ZggcX.
[29] LaurentValentinJospin,HamidLaga,FaridBoussaid,WrayBuntine,andMohammedBen-
namoun. Hands-on bayesian neural networks—a tutorial for deep learning users. IEEE
ComputationalIntelligenceMagazine,17(2):29–48,2022.
[30] DiederikPKingmaandJimmyBa. Adam:Amethodforstochasticoptimization. arXivpreprint
arXiv:1412.6980,2014.
[31] MiltiadisKofinas, BorisKnyazev, YanZhang, YunluChen, GertjanJBurghouts, Efstratios
Gavves,CeesGMSnoek,andDavidWZhang. Graphneuralnetworksforlearningequivariant
representationsofneuralnetworks. arXivpreprintarXiv:2403.12143,2024.
11[32] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages,
2009.
[33] RichardKurle,RalfHerbrich,TimJanuschowski,YuyangBernieWang,andJanGasthaus. On
thedetrimentaleffectofinvariancesinthelikelihoodforvariationalinference. Advancesin
NeuralInformationProcessingSystems,35:4531–4542,2022.
[34] Lauro Langosco, Neel Alex, William Baker, David John Quarel, Herbie Bradley, and
David Krueger. Towards meta-models for automated interpretability, 2024. URL https:
//openreview.net/forum?id=fM1ETm3ssl.
[35] Olivier Laurent, Emanuel Aldea, and Gianni Franchi. A symmetry-aware exploration of
bayesian neural network posteriors. In The Twelfth International Conference on Learning
Representations,2024. URLhttps://openreview.net/forum?id=FOSBQuXgAq.
[36] GuillaumeLeclerc,AndrewIlyas,LoganEngstrom,SungMinPark,HadiSalman,andAlek-
sanderMa˛dry. Ffcv: Acceleratingtrainingbyremovingdatabottlenecks. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages12011–12020,
2023.
[37] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
appliedtodocumentrecognition. ProceedingsoftheIEEE,86(11):2278–2324,1998.
[38] DerekLim,HaggaiMaron,MarcT.Law,JonathanLorraine,andJamesLucas. Graphmetanet-
worksforprocessingdiverseneuralarchitectures. InTheTwelfthInternationalConferenceon
LearningRepresentations,2024. URLhttps://openreview.net/forum?id=ijK5hyxs0n.
[39] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InInternational
ConferenceonLearningRepresentations,2018.
[40] JamesLucas,JuhanBae,MichaelRZhang,StanislavFort,RichardZemel,andRogerGrosse.
Analyzingmonotoniclinearinterpolationinneuralnetworklosslandscapes. arXivpreprint
arXiv:2104.11044,2021.
[41] HaggaiMaron,EthanFetaya,NimrodSegol,andYaronLipman.Ontheuniversalityofinvariant
networks. InInternationalconferenceonmachinelearning,pages4363–4371.PMLR,2019.
[42] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, and Hassan
Ghasemzadeh. Linear mode connectivity in multitask and continual learning. In Interna-
tional Conference on Learning Representations, 2021. URL https://openreview.net/
forum?id=Fmg_fQYUejf.
[43] MahdiPakdamanNaeini,GregoryCooper,andMilosHauskrecht. Obtainingwellcalibrated
probabilities using bayesian binning. In Proceedings of the AAAI conference on artificial
intelligence,volume29,2015.
[44] AvivNavon,AvivShamsian,IdanAchituve,EthanFetaya,GalChechik,andHaggaiMaron.
Equivariantarchitecturesforlearningindeepweightspaces. InInternationalConferenceon
MachineLearning,pages25790–25816.PMLR,2023.
[45] Aviv Navon, Aviv Shamsian, Ethan Fetaya, Gal Chechik, Nadav Dym, and Haggai Maron.
Equivariantdeepweightspacealignment. arXivpreprintarXiv:2310.13397,2023.
[46] Behnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized
optimizationindeepneuralnetworks. Advancesinneuralinformationprocessingsystems,28,
2015.
[47] BehnamNeyshabur,RyotaTomioka,andNathanSrebro. Norm-basedcapacitycontrolinneural
networks. InConferenceonlearningtheory,pages1376–1401.PMLR,2015.
[48] Theodore Papamarkou, Jacob Hinkle, M Todd Young, and David Womble. Challenges in
markovchainmontecarloforbayesianneuralnetworks. StatisticalScience,37(3):425–442,
2022.
[49] Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan
Arbel,DavidDunson,MaurizioFilippone,VincentFortuin,PhilippHennig,AliaksandrHubin,
et al. Position paper: Bayesian deep learning in the age of large-scale ai. arXiv preprint
arXiv:2402.00809,2024.
12[50] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,
TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal. Pytorch: Animperative
style, high-performance deep learning library. Advances in neural information processing
systems,32,2019.
[51] FidelAGuerreroPeña,HeitorRapelaMedeiros,ThomasDubail,MasihAminbeidokhti,Eric
Granger,andMarcoPedersoli. Re-basinviaimplicitsinkhorndifferentiation. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages20237–20246,
2023.
[52] FabrizioPittorino,AntonioFerraro,GabrielePerugini,ChristophFeinauer,CarloBaldassi,and
RiccardoZecchina. Deepnetworksontoroids:removingsymmetriesrevealsthestructureofflat
regionsinthelandscapegeometry. InInternationalConferenceonMachineLearning,pages
17759–17781.PMLR,2022.
[53] AryaAPourzanjani,RichardMJiang,andLindaRPetzold. Improvingtheidentifiabilityof
neuralnetworksforbayesianinference.InNIPSworkshoponbayesiandeeplearning,volume4,
page31,2017.
[54] XingyuQuandSamuelHorvath. Rethinkmodelre-basinandthelinearmodeconnectivity.
arXivpreprintarXiv:2402.05966,2024.
[55] PrajitRamachandran,BarretZoph,andQuocV.Le. Searchingforactivationfunctions,2017.
[56] AvivShamsian,AvivNavon,DavidWZhang,YanZhang,EthanFetaya,GalChechik,and
HaggaiMaron. Improvedgeneralizationofweightspacenetworksviaaugmentations. arXiv
preprintarXiv:2402.04081,2024.
[57] SidakPalSinghandMartinJaggi. Modelfusionviaoptimaltransport. AdvancesinNeural
InformationProcessingSystems,33:22045–22055,2020.
[58] GeorgeStoica,DanielBolya,JakobBrandtBjorner,PratikRamesh,TaylorHearn,andJudy
Hoffman. Zipit! merging models from different tasks without training. In The Twelfth
InternationalConferenceonLearningRepresentations,2024. URLhttps://openreview.
net/forum?id=LEYUkvdUhq.
[59] Héctor J Sussmann. Uniqueness of the weights for minimal feedforward nets with a given
input-outputmap. Neuralnetworks,5(4):589–593,1992.
[60] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonShlens,andZbigniewWojna. Re-
thinkingtheinceptionarchitectureforcomputervision. InProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition,pages2818–2826,2016.
[61] NormanTatro,Pin-YuChen,PayelDas,IgorMelnyk,PrasannaSattigeri,andRongjieLai. Op-
timizingmodeconnectivityvianeuronalignment. AdvancesinNeuralInformationProcessing
Systems,33:15300–15311,2020.
[62] MarcinTomczak,SiddharthSwaroop,andRichardTurner. Efficientlowrankgaussianvaria-
tionalinferenceforneuralnetworks.InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,and
H.Lin,editors,AdvancesinNeuralInformationProcessingSystems,volume33,pages4610–
4622. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_
files/paper/2020/file/310cc7ca5a76a446f85c1a0d641ba96d-Paper.pdf.
[63] Thomas Unterthiner, Daniel Keysers, Sylvain Gelly, Olivier Bousquet, and Ilya Tolstikhin.
Predictingneuralnetworkaccuracyfromweights. arXivpreprintarXiv:2002.11448,2020.
[64] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformation
processingsystems,30,2017.
[65] NehaVermaandMahaElbayad. Mergingtexttransformermodelsfromdifferentinitializations.
arXivpreprintarXiv:2403.00986,2024.
[66] TiffanyJ.VlaarandJonathanFrankle. Whatcanlinearinterpolationofneuralnetworkloss
landscapestellus? ICML,2022.
[67] HongyiWang,MikhailYurochkin,YuekaiSun,DimitrisPapailiopoulos,andYasamanKhazaeni.
Federatedlearningwithmatchedaveraging. arXivpreprintarXiv:2002.06440,2020.
13[68] XiangWang,AnnieN.Wang,MoZhou,andRongGe. Plateauinmonotoniclinearinterpo-
lation—a”biased”viewoflosslandscapefordeepnetworks. InTheEleventhInternational
ConferenceonLearningRepresentations,2023. URLhttps://openreview.net/forum?
id=z289SIQOQna.
[69] JonasGregorWiese,LisaWimmer,TheodorePapamarkou,BerndBischl,StephanGünnemann,
andDavidRügamer.Towardsefficientmcmcsamplinginbayesianneuralnetworksbyexploiting
symmetry. InJointEuropeanConferenceonMachineLearningandKnowledgeDiscoveryin
Databases,pages459–474.Springer,2023.
[70] TimZXiao,WeiyangLiu,andRobertBamler. Acompactrepresentationforbayesianneural
networksbyremovingpermutationsymmetry. arXivpreprintarXiv:2401.00611,2023.
[71] KeyuluXu,WeihuaHu,JureLeskovec,andStefanieJegelka. Howpowerfularegraphneural
networks? In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=ryGs6iA5Km.
[72] ChulheeYun,SrinadhBhojanapalli,AnkitSinghRawat,SashankReddi,andSanjivKumar.
Aretransformersuniversalapproximatorsofsequence-to-sequencefunctions? InInternational
ConferenceonLearningRepresentations,2020. URLhttps://openreview.net/forum?
id=ByxRM0Ntvr.
[73] ManzilZaheer,SatwikKottur,SiamakRavanbakhsh,BarnabasPoczos,RussRSalakhutdinov,
andAlexanderJSmola. Deepsets. Advancesinneuralinformationprocessingsystems,30,
2017.
[74] BoZhao,IordanGanev,RobinWalters,RoseYu,andNimaDehmamy.Symmetries,flatminima,
andtheconservedquantitiesofgradientflow. arXivpreprintarXiv:2210.17216,2022.
[75] BoZhao,NimaDehmamy,RobinWalters,andRoseYu. Understandingmodeconnectivityvia
parameterspacesymmetry. InUniReps: theFirstWorkshoponUnifyingRepresentationsin
NeuralModels,2023.
[76] BoZhao,RobertM.Gower,RobinWalters,andRoseYu. Improvingconvergenceandgener-
alizationusingparametersymmetries. InTheTwelfthInternationalConferenceonLearning
Representations,2024. URLhttps://openreview.net/forum?id=L0r0GphlIL.
[77] AllanZhou,KaienYang,KayleeBurns,AdrianoCardace,YidingJiang,SamuelSokota,JZico
Kolter, and Chelsea Finn. Permutation equivariant neural functionals. Advances in Neural
InformationProcessingSystems,36,2023.
[78] AllanZhou,KaienYang,YidingJiang,KayleeBurns,WinnieXu,SamuelSokota,JZicoKolter,
andChelseaFinn. Neuralfunctionaltransformers. AdvancesinNeuralInformationProcessing
Systems,36,2023.
[79] AllanZhou,ChelseaFinn,andJamesHarrison. Universalneuralfunctionals. arXivpreprint
arXiv:2402.05232,2024.
[80] LiuZiyin.Symmetryleadstostructuredconstraintoflearning.arXivpreprintarXiv:2309.16932,
2023.
14Number of Epochs until 70%
70
11 10 11 12 11
60
11 12 15 19 22
50
11 19 25 31 38
40
11 24 36 44 53 30
20
11 30 46 60 72
0 ~10000 ~20000 ~30000 ~40000
Num Fixed
Figure6: Epochsuntilreaching70%trainingaccuracyonCIFAR-10whenvaryingthehyperparame-
tersofW-AsymmetricResNets;wevarynumberoffixedentriesn andstandarddeviationκofthe
fix
fixedentriesF. Entriesfurthertothebottomandrightaremoreasymmetric,whiletheentriesfurther
tothetopandleftaremorelikestandardnetworks(theleftmostcolumnareallstandardnetworks).
Weseethatmore-asymmetricnetworksneedmoretimetotrain.
A AdditionalObservationsonAsymmetricNetworks
Thereareseveralotherinterestingdifferencesintheoptimizationandlosslandscapepropertiesof
Asymmetricandstandardneuralnetworks. Forone,eventhoughAsymmetricnetworksgenerally
interpolatesignificantlybetterthanstandardnetworks,thiscannotbeseenbymeasuringdistances
in parameter space. For instance, in GNN experiments following the setup of Section 5.1, pairs
ofstandardGNNshaveadistanceperparameterof.000174onaverage,whereasW-Asymmetric
GNNshave.000159,whichisonlyslightlylower. However,theaveragetestlossbarrieris1.448for
standardGNNswhileitisonly0.069forW-AsymmetricGNNs. Likewise,inourdatasetsof10,000
standard and W-Asymmetric ResNets, the average distance per parameter between the weights
oftrainedstandardclassifiersis.0034,whichisactuallylowerthanthedistanceperparameterof
.0051 for W-Asymmetric ResNets (estimated on 20,000 pairs of networks). Thus, although we
sometimesimaginewell-interpolatingpairsofnetworkstolieinthesamelocalbasinofparameter
space,W-Asymmetricnetworksareactuallyratherfarapartinparameterspace,butnonetheless
havelinearsegmentsoflowlossbetweenthem.
We also find that Asymmetric networks often do not overfit as much as standard networks. For
instance,intheGNNsetupofSection5.1,standardGNNshaveamaxtrainingaccuracyof84.6%on
average,withavalidationaccuracyof71.6%. Ontheotherhand,σ-AsymGNNshave70.8%/70.1%
train/validationaccuracy,whileW-AsymGNNshave70.7%/70.06%train/validationaccuracy. This
differencedoesnotshowasmuchinourdatasetsof10,000standardResNetsandW-AsymResNets,
possibly because of the substantial regularization (data augmentation, weight decay, and label
smoothing)usedfortraining(standardgets74.8%/73.8%train/testaccuracywhileW-Asymmetric
gets64.0%/64.0%).
Further, in Figure 6, we see that training speed is slower for W-Asymmetric ResNets when we
increase the amount of asymmetry (by increasing the number of fixed entries and the standard
deviation of the fixed entries). While standard ResNets take on average 11 epochs to reach 70%
trainingaccuracyonCIFAR-10,W-AsymmetricResNetswiththemostextremehyperparameters
takeupto72epochs.
15
noitaiveD
dradnatS
0.0
57.0
5.1
52.2
0.3B ProofsofTheoreticalResults
B.1 Graph-basedapproach
Here,weprovethataslongaseachmaskmatrixM inourW-AsymmetricMLPswithfixedentriesset
tozerohasuniquenonzerorows,thenourarchitecturehasnonontrivialneuralDAGautomorphisms.
Inpractice,wefindthatsettingthestandarddeviationκofthefixedentriesFtobepositive(and
infactordersofmagnitudelargerthanthestandarddeviationthatwetypicallyinitializetrainable
weightswith)isimportanttoachievepropertiessuchaslinearmodeconnectivitythatAsymmetric
networkshavebutstandardnetworksdonothave. Whenκ = 0(i.e. whenfixedentriesaresetto
zero),wecandirectlyworkintheframeworkofLimetal.[38]thatconnectsparametersymmetries
tocomputationgraphautomorphisms. Toworktowardsgeneralizingourresulttoκ>0,wewould
havetomodifythedefinitionsandresultsofLimetal.[38];forinstance,wewouldneedtoaddedges
associatedtountrainableparametersinthecomputationgraph,andredefinetheconceptofneural
DAGautomorphisms. Weleavesuchexplorationtofuturework.
Theorem3. IfeachmaskmatrixM hasuniquenonzerorows,thenW-AsymmetricMLPswithκ=0
havenonontrivialneuralDAGautomorphisms.
Proof. ConsideranL-layerW-AsymmetricMLPwithfixedentriessettozero. Denoteitsweights
asW ,...,W andthecorrespondingbinarymasksasM ,...,M . Theforwardpassofsucha
L 1 L 1
networkonaninputxisthen
[W ⊙M ]σ(···σ([W ⊙M ]x)···), (5)
L L 1 1
forsomeelementwisenonlinearityσ. ThedimensionofW isd ×d . IntheframeworkofLim
i i i−1
etal.[38],thisisafeedforwardneuralnetworkwithacomputationgraphdefinedasfollows.
ThenodesetisV ×V ×...×V ,whereV hasd nodes,andnonodesaresharedbetweendifferent
0 1 L i i
V . IfanodevisinV ,thenwesaythatlayer(v)=i. V containstheinputnodesandV contains
i i 0 L
theoutputnodes. Theadjacencymatrixcanbewrittenas
 
0
M 1 0 
 
A= M 2 . (6)



...
0



M 0
L
Everyblockbesidestheonescontainingmasksiszero. ThereareL+1×L+1blocks,andthe(i,j)
blockisofsized ×d .
i j
RecallthataneuralDAGautomorphismisarelabellingofnodesτ :V →V suchthatτ isbijective,
(i,j)∈E ifandonlyif(τ(i),τ(j))∈E,andeveryinputnodeandoutputnodeisafixedpointofτ.
Now,letτ :V →V beaneuralDAGautomorphism.Further,letP bethecorrespondingpermutation
matrix. Wewillshowthatτ istheidentity,i.e. thatP =I. ByLemma1,weknowthatτ preserves
layernumberofnodes,meaninglayer(τ(i)) = layer(i). Thus,P isablockdiagonalpermutation
matrix:
 
P
0
 P 1 
P = 

...   , (7)
P
L
whereP isd ×d . Morever,P =I andP =I becauseinputnodesandoutputnodesarefixed
i i i 0 L
points. Applyingthistotheadjacencymatrix,weseethat
 
0
τ(A)=PAP⊤
=

P 1M 1P 0⊤ 0
...

 . (8)
 
P M P⊤ 0
L L L−1
Sinceτ isaneuralDAGautomorphism,wehavethatτ(A)=A. Equatingblocks,thismeansthat
P M P⊤ =M . AsP =I,wehaveP M =M . ButM hasuniquerows,soP =I aswell.
1 1 0 1 0 1 1 1 1 1
16Fortheinductivestep,assumeP = I forsomei. ThenP M P⊤ = P M = M ,so
i i+1 i+1 i i+1 i+1 i+1
sinceM hasuniquerows,wehavethatP =I. Asthisholdsforanyibyinduction,thismeans
i+1 i+1
thatP =I,soτ isatrivialneuralDAGautomorphismandwearedone.
Lemma1. NeuralDAGautomorphismspreservelayernumberinW-AsymmetricMLPsthathave
maskswithnonzerorows.
Proof. Let τ be a neural DAG automorphism. This means that PAP⊤ = A, where P is the
permutationmatrixassociatedtoτ. Then,usingPAP⊤ =Aforthefirstequalityandthedefinition
ofP inthesecond,wehavethat
A =(PAP⊤) =A . (9)
τ(i),τ(j) τ(i),τ(j) i,j
Weproceedbyinductiononlayernumberl. Foranyinputnodeiweknowthatτ(i)=i,soofcourse
layer(τ(i))=layer(i).
Now,supposethatlayer(τ(i)) = layer(i)foranyiinlayerl ≥ 1. Ifnodej isinlayerl+1,then
thereis somei inlayer l suchthat (i,j) ∈ E because M hasno nonzerorows. Wehavethat
l+1
A =A ,soτ(i)isconnectedtoτ(j). Asweknowthatτ(i)isinlayerl,wehavethatτ(j)
τ(i),τ(j) i,j
isinlayerl+1.
B.2 SymmetryBreakingviaNonlinearities
Proposition3. LettheparameterspaceΘbeallpairsofsquareinvertiblematricesθ =(W ,W )
2 1
for W ,W ∈ GL(d), and let f (x) = W σ(W x). If σ has no linear equivariances, then
2 1 θ 2 1
f =f ifandonlyifθ =θ . Inotherwords,therearenonontrivialparameterspacesymmetries.
θ1 θ2 1 2
Proof. Ifθ = θ ,thenclearlyf = f . Fortheotherdirection,supposef = f ,anddenote
1 2 θ1 θ2 θ1 θ2
θ
1
=(W 2,W 1)andθ
2
=(W(cid:102)2,W(cid:102)1). Thenforanyinputz ∈Rn,wehave
W 2σ(W 1z)=W(cid:102)2σ(W(cid:102)1z) (10)
W(cid:102) 2−1W 2σ(W 1z)=σ(W(cid:102)1z). (11)
Now,chooseanarbitraryx∈Rn. Weletzintheaboveequation(11)beW−1x,sowehave
1
W(cid:102) 2−1W 2σ(x)=σ(W(cid:102)1W 1−1x). (12)
Thisholdsforanyx,soW(cid:102) 2−1W 2◦σ =σ◦W(cid:102)1W 1−1,i.e. wehavefoundalinearequivarianceofσ.
Sinceσhasnolinearequivariances,
W(cid:102) 2−1W
2
=I =W(cid:102)1W 1−1, (13)
meaningthatW(cid:102)2 =W 2andW(cid:102)1 =W 1,i.e. θ
1
=θ 2,sowearedone.
B.2.1 FiGLUnonlinearityproofs(Proposition2)
Now,westudythepropertiesofourFiGLUnonlinearityσ(x)=η(Fx)⊙x,whereηisthesigmoid
functionη(x) = 1 . ForprovingProposition2,wewanttoprovethatwithprobability1over
1+e−x
samplesofF,σhasnopermutationordiagonalequivariances.
WesaythatσhasnopermutationequivariancesifwheneverP ◦σ =σ◦P forpermutationmatrices
2 1
P andP ,thenP =P =I. Likewise,wesaythatσhasnodiagonalequivariancesifwhenever
1 2 1 2
B◦σ =σ◦AforinvertiblediagonalmatricesAandB,thenA=B =I.
WewillshowthatthesetwopropertiesholdforanyFthathasnopermutationsymmetriesandno
zeroentries. WesaythatFhasnopermutationsymmetriesifP FP =Fforpermutationmatrices
2 1
P andP impliesthatP =P =I. NotethatifFhasdistinctentries,thenithasnopermutation
1 2 1 2
symmetries. Thus,Fsatisfiesbothoftheseconditionswithprobability1,sincethesetofmatrices
withnondistinctentriesorwithatleastonezeroentryareofLebesguemeasurezero,sotheyhave
zeroprobabilityundertheGaussiandistribution. Wenowproceedtoshowthatσhasnopermutation
ordiagonalequivariancesundertheseconditionsonF.
17Proposition4. IfFisasquarematrixwithnopermutationsymmetries,thenσ(x)=η(Fx)⊙xhas
nopermutationequivariances.
Proof. Supposeσ◦P =P ◦σforpermutationmatricesP ,P . WewillshowthatP =P =I.
1 2 1 2 1 2
Foranyinputx,wehave
(cid:2) (cid:3)
η(FP x)⊙P x=P η(Fx)⊙x (14)
1 1 2
P⊤(cid:2)
η(FP x)⊙P
x(cid:3)
=η(Fx)⊙x (15)
2 1 1
η(P⊤FP x)⊙P⊤P x=η(Fx)⊙x, (16)
2 1 2 1
whereweusedpermutationequivarianceofη,whichactselementwise. Letx = e ,thestandard
i
basisvectorthatis1intheithcoordinateand0elsewhere. Ifiisnotafixedpointofthepermutation
P⊤P ,thenletj betheindexthatitismappedto. Thenequation(16)givesthat
2 1
η(P⊤FP e )⊙P⊤P e =η(Fe )⊙e (17)
2 1 i 2 1 i i i
η(P⊤FP e )⊙e =η(Fe )⊙e . (18)
2 1 i j i i
Intheithcoordinateofthisequalityofvectors,weseethatη(Fe )=0,whichisimpossible,since
i
η isthesigmoidfunction. Thus,icannotbeafixedpointofP⊤P ,soP⊤P = I istheidentity
2 1 2 1
permutation. Now,letxbeanarbitraryvectorwithnozeroentries. Equation(16)givesthat
η(P⊤FP x)⊙x=η(Fx)⊙x. (19)
2 1
Sincexisnonzero,wecandividebyx intheithcoordinateofthisvectorequalityforeachitoget
i
that
η(P⊤FP x)=η(Fx). (20)
2 1
Asηisbijective,
P⊤FP x=Fx. (21)
2 1
Becausethisholdsforallxwithnozeroentries(andinparticularforabasisoftheinputspace),we
knowthat
P⊤FP =F (22)
2 1
asmatrices. ButsinceFhasnopermutationsymmetries, wehavethatP = P = I, soweare
1 2
done.
Proposition5. IfFisasquarematrixwithnozeroentries,thenσ(x)=η(Fx)⊙xhasnodiagonal
equivariances.
Proof. Let A = Diag(α) and B = Diag(β) be invertible diagonal matrices, and suppose that
σ◦A=B◦σ. WewillshowthatA=B =I. Foranyinputx,wehave
(cid:2) (cid:3)
η(F[α⊙x])⊙(α⊙x)=β⊙ η(Fx)⊙x . (23)
Letx=ce ,wheree istheithstandardbasisvectorandc̸=0isanynonzeronumber. Then
i i
(cid:2) (cid:3)
η(Fcα e )⊙cα e =β⊙ η(cFe )⊙ce . (24)
i i i i i i
Attheithcoordinateofthisequality,wehave
η(Fcα e ) cα =β η(cFe ) c (25)
i i i i i i i
α η(cFe )
i = i i (26)
β η(α cFe )
i i i i
Thus,therighthandsideisconstantinc. Wemusthavethatα >0,becauseifnot,thenincreasingc
i
wouldincreaseeitherthenumeratorordenominatoranddecreasetheother,hencecontradictingthe
equality(hereweusethatFhasnozeroentries,socFe isnonzeroineveryentry). Thus,letting
i
c→∞,weseethat α βii =1,soα i =β i. PluggingthisbackintoEquation(26),wehave
η(cFe )
1= i i (27)
η(α cFe )
i i i
η(α cFe ) =η(cFe ) (28)
i i i i i
α c(Fe ) =c(Fe ) (29)
i i i i i
α =1, (30)
i
whereinthethirdlineweusedthefactthatηisinvertible. Wehaveshownthatα =β =1foreach
i i
i,soA=B =I andwearedone.
18WenotethattheproofsofthesetworesultsaboutFiGLUarereminiscentofsomeprooftechniques
fromGodfreyetal.[19],suchasthoseusedintheiranalysisofGELUnonlinearities.
B.3 ProofsforUniversalApproximation
Here,weprovetheuniversalapproximationresultforourW-AsymmetricMLPs.
Theorem4. Letη beanynonpolynomialelementwisenonlinearitywithη(x)−η(−x) = x(e.g.
ReLU,GELU,swish),letΩ⊆RD beacompactdomain,andletf :Ω→Rbeacontinuous
target
targetfunction. Fixε>0andδ >0.
Thereexistsawidthn′suchthatforalln>n′,withprobability1−δ,forarandomlysampled4-layer
W-AsymmetricMLPf withηnonlinearity,hiddendimensions24n→n→24n,andn ∈o(n1/4)
fix
hardwiredentriesperneuron,therewillexistθ ∈ΘsuchthattheW-AsymmetricMLPf :Rn →R
approximatesf toε:
target
(cid:13) (cid:13)
(cid:13)f θ([x;0])−f target(x)(cid:13)<εforallx∈Ω. (31)
Importantly,werequirethattheinputtof bepaddedwithn−Dzeroes,so[x;0]∈Rn.
θ
B.3.1 Proofsketch
Toapproximatef toε>0,wewillleveragetheuniversalapproximationforstandardMLPs
target
with nonlinearity η to first obtain a standard 2-layer MLP that approximates f to within ε,
target
(cid:13) (cid:13)
meaning(cid:13)f target(x)−f MLP(x)(cid:13)<εforallx∈Ω. Thenwewillexactlyrepresentf MLPusingan
AsymmetricNetworkf.
ThiswillbedonebyapproximatingeachlinearmapW off bytwolayersofanAsymmetric
MLP
network: W′ ◦η◦W′ =W forAsymmetriclinearmapsW′ andW′. Forthesakeofexposition,
2 1 2 1
wewillshowhowtodothisfirstwhenbothW′ andW′ havenoAsymmetricmask(i.e. fittinga
2 1
linearmapW usingastandardtwo-layerη-MLP),thenwhenonlyW′ hasanAsymmetricmask,and
2
finallywhenbothW′ andW′ haveanAsymmetricmask.
2 1
B.3.2 FittingaLinearMapwithaTwo-LayerStandardMLP
LetW ∈ Rn×n bethetargetlinearmap,andletB ∈ R2n×n andA ∈ Rn×2n beparametersofa
two-layerMLP,definedbyf (x)=Aη(Bx). WewillchooseAandBsuchthatf (x)=Wx
A,B A,B
forallx∈Rn.
DenotetheithrowofW byW ,sothat
i
   
W W ·x
0 0
W = . . , Wx= . .  (32)
 .   . 
W W ·x
n−1 n−1
WesetAandBasfollows,whereI isthen×nidentitymatrix:
n
 
W
0
 1 −1   −W 0 
 W 
A=I n⊗(cid:2) 1 −1(cid:3) =    1 − ..1 . ...     B =     −W . . .1 1     . (33)
1 −1  
 W 
n−1
−W
n−1
Thenwecanseethatf exactlycomputesthelineartransformationWx.
A,B
   
η(W ·x)−η(−W ·x) W ·x
0 0 0
Aη(Bx)= . . = . . =Wx. (34)
 .   . 
η(W ·x)−η(−W ·x) W ·x
n−1 n−1 n−1
19B.3.3 FittingaLinearMapwithOneAsymmetricandOneStandardLinearMap
Letn >0andleteachrowofN∈{0,1}n×6nhaven entriesequalto0,selectedatrandom. Let
fix fix
B ∈R6n×nandA∈Rn×6n.DefineA′tobeanAsymmetriclinearmap:A′ =A⊙N+(1−N)⊙P,
where N is a randomly sampled binary mask, and P a randomly sampled Gaussian matrix. We
consider a two-layer network with one Asymmetric and one standard linear map: f (x) =
A,B
A′η(Bx). Wewantf (x)=Wxforallx. Fortheremainderofthisproof,wewillassumethatN
A,B
neverhasthreeconsecutiveentriesinarowsettozero;wewilllatershowthatthisholdswithhigh
probabilityoverthesamplingofN.
First,wedefineBinasimilarwaytothepurelylinearsetting,butwithadditionalcopiesofentriesto
allowforerrorcorrectionoftherandomnoisyentriesfixedinA′.
 
W
0
W
 0 
 W 
 0 
 −W 
 0 
 −W 
 0 
 −W 0 
 
B = . . . (35)
 . 
 
 W n−1 
 W 
 n−1 
 W 
 n−1 
−W 
 n−1
−W 
n−1
−W
n−1
RecallthateachrowA′ ofA′hasn entriesthatarerandomlyhardwiredtoconstants. Ideally,we
i fix
wouldwantA′ topickoutη(W ·x)−η(−W ·x)=W ·x,butbecauseofthehardwiredconstants,
i i i i
A′ mightrandomlyaddc∗η(W ·x). However,sincetherearethreecopiesofη(W ·x)inη(Bx),as
i j j
longasnotallthreecorrespondingentriesinA′ arefixed,oneoftheun-fixedcopiescanbechanged
i
suchthatthecoefficientssumto0. SincebyourassumptionNneverhasthreeconsecutiveentriesall
setto0,thecoefficientsofAcanbepickedsuchthatA′η(Bx)=Wx. Forexample,apossibleA′
matrixwouldbe
 
1 0 0 −1 0 0 | 1.1 .37 −1.47 0 0 0 |... | 0 0 0 0 0 0
A′=.89 −.89 0 0 0 0 | .37 .63 0 −1 0 0 |... | 0 0 0 0 0 0
0 0 0 0 0 0 | 0 0 0 0 0 0 |... | 1 0 0 −1 0 0
Thus we have shown that under the assumption that N never has three consecutive en-
tries equal to 0, A can be picked such that A′η(Bx) = Wx. We will now show that
P(Nneverhasthreeconsecutiveentriesequalto0)canbemadearbitrarilysmallbyincreasingthe
widthnwhilekeepingn tobeo(n1/3).
fix
TheprobabilitytherearethreeconsecutiveentriesinagivenrowofNthatarezeroisO(n3
fix). By
n2
theunionbound,theprobabilitythatanyrowhas3consecutivehardwiredentriesisO(n3
fix). Forany
n
n
fix
∈o(n1/3),thistendstowards0. Thus,withprobability≥1−O(n n3 fix),Acanbepickedsuch
thatA′η(Bx)=Wx.
B.3.4 FittingaLinearMapwithTwoAsymmetricLinearMaps
Onceagainletn >0,andleteachrowofMhaven entriesequalto0,selectedatrandom. Let
fix fix
B ∈R24n×nandA∈Rn×24n. Further,defineAsymmetricmaps
A′ =A⊙N+(1−N)⊙P, B′ =B⊙M+(1−M)⊙Q, (36)
whereN,Marerandomlysampledmasks,andP,Qarenormalmatrices. Thenweletf (x)=
A,B
A′η(B′(x)),andweonceagaindesirechoicesofparametersAandBsuchthatf (x)=Wx.
A,B
20ConstructingB
ConsidertherandomlydrawnmaskM∈{0,1}24n×n,anddenotetheithrowbyM .
i
 
M
0
 M 1 
M= .  (37)
 . 
 . 
M
24n−1
whereM ∈{0,1}n. WepartitionM’srowsintonblocksof24rows. β ={M ...M },...β =
i 1 0 23 i
{M ...M }. Now,considerβ ,thefirst24rowsofM.
24i 24i+23 1
DefinitionB.1. WesaytworowsM , M areintersectingifthereissomecolumnindexαsuch
j k
thatM =M =0. Thatis,tworowsofareintersectingiftheysharea0atthesameindex.
j,α k,α
NotethatforanytwogivenrowsofM,theprobabilitythattheysharea0inthesamelocationis
≤
n2
fix.
n
Weassumethatβ containsnomorethanonepairofintersectingrows;later,weshowthistohold
i
withhighprobability. Then,everyβ canbebrokenintotwodisjointsetsof12rows,β andβ ,
i i,0 i,1
suchthatneithersetof12containsasinglepairofintersectingrows. Intuitively,thismeansthateach
rowinBcorrespondingtoβ willhaveuniquefixedindices.
i,0
Ourgoalwillbefortherowsinβ tomimictherowW . Wewillshowhowtodothisforeachi. Fix
i i
anarbitraryindexi∈{0,...,n−1}.
Withoutlossofgenerality,assumeβ andβ arecontinguous,soβ =M andβ =
i,0 i,1 i,0 24i:24i+11 i,1
M . By our assumption, for j,k ∈ β (i.e. j,k ∈ {0,...,11}), the mask rows
24i+11:24i+23 i,0
M and M are never 0 in the same two column indices. Similarly, for j,k ∈ β (i.e.
24i+j 24i+k i,1
j,k ∈ {12,...,23}), the mask rows M and M are never 0 in the same two column
24i+j 24i+k
indices.
Next,wedefinec asthedifferencebetweenB′andBinthe(24i+j)throw:
i,j
c =B′ −B . (38)
i,j 24i+j 24i+j
Inparticular,wehavethat
c =−B ⊙(1−M )+(1−M )⊙Q . (39)
i,j 24i+j 24i+j 24i+j 24i+j
Lemma2. Foranyindicesj ̸=ksuchthatj,k ∈β orj,k ∈β ,wehavethat
i,0 i,1
c ⊙M =c . (40)
i,j 24i+k i,j
Proof. Bythedefinitionofc ,weknowthatc isonlynonzeroatindiceswhereM isequal
i,j i,j 24i+j
to zero. Since j,k are either both in β or β , we know that M cannot also be zero at
i,0 i,1 24i+k
indiceswhereM iszero. Thus,M isequalto1ateveryindexwherec isnonzero,so
24i+j 24i+k i,j
c ⊙M =c asdesired.
i,j 24i+k i,j
Next,weconstructB,byconstructingthisblockof24rows. Let[c ,c ,c ,c ]becontinguous
i,0 i,1 i,2 i,3
sumsoflength-3segmentsofc :
i,:
c =c +c +c (41)
i,0 i,0 i,1 i,2
c =c +c +c (42)
i,1 i,3 i,4 i,5
c =c +c +c (43)
i,2 i,6 i,7 i,8
c =c +c +c (44)
i,3 i,9 i,10 i,11
Weassignthefirst12rowsofBasfollows.
(0≤j <3)→B =W +c −c +c −c −c (45)
24i+j i i,0 i,1 i,2 i,3 ij
(3≤j <6)→B =−W −c +c −c +c −c (46)
24i+j i i,0 i,1 i,2 i,3 ij
(6≤j <9)→B =+c −c +c −c −c (47)
24i+j i,0 i,1 i,2 i,3 ij
(9≤j <12)→B =−c +c −c +c −c (48)
24i+j i,0 i,1 i,2 i,3 ij
21Definingc=c −c +c −c ,wehavetheniceproperty:
i,0 i,1 i,2 i,3
(0≤j <3)→B′ =W +c (49)
24i+j i
(3≤j <6)→B′ =−W −c (50)
24i+j i
(6≤j <9)→B′ =+c (51)
24i+j
(9≤j <12)→B′ =−c (52)
24i+j
Bytheconstructionabove,B′ =−B′ ,andB′ =−B′ . Thismeansthat
24i 24i+3 24i+6 24i+9
η(B′ ·x)−η(B′ ·x)=B′ ·x=(W +c)·x (53)
24i 24i+3 24i i
andlikewisethat
η(B′ ·x)−η(B′ ·x)=c·x (54)
24i+6 24i+9
Sothatasimplelinearmapgivesourdesiredoutput:
η(B′ ·x)−η(B′ ·x)−[η(B′ ·x)−η(B′ ·x)]=(W +c−c)·x (55)
24i 24i+3 24i+6 24i+9 i
=W ·x. (56)
i
Inthenextpart,wewillconstructA′ tocomputethislinearmap,whichwillfollowthemethodof
AppendixB.3.3(becauseA′hascertainfixedentries).
WhatremainsistodefinetherowsofBcorrespondingtoβ inanerrorcorrectiblemanner. This
i,1
canbedoneeasilybydefining
23
(cid:88)
d= c (57)
ij
j=12
andthendefining
(12≤j <24)→B =d−c (58)
24i+j ij
Bysimilarreasoningtobefore,thismeansthat
(12≤j <24)→B′ =d. (59)
24i+j
RecallthatweconstructedB′undertheassumptionthatnoβ hasatmostonepairofintersecting
i
rows. Wenowshowthattheβ eachhaveatmostonepairofintersectingrowswithhighprobability.
i
Withinthe24rowsofanygivenβ ,theprobabilitythatmorethanonepairofrowsareintersectingis
i
≤Cn n4 f 2ix forsomeconstantC. So,bytheunionbound,theprobabilityoverM thatanyoftheβ ihave
morethanonepairofintersectingrowsis≤Cn4
fix. Thus,wecanconstructB inthismannerwith
n
probability≥1−Cn n4 fix. Forsufficientlylargenandn
fix
∈o(n1/4),thisprobabilityapproaches1.
22ConstructionofA
Withouraboveconstruction,eachblockofthe24rowsinβ ofη(B′x)isoftheform
i
 
η((W +c)·x)
i
 η((W i+c)·x) 
 η((W +c)·x) 
 i 
η((−W −c)·x)
 i 
η((−W −c)·x)
 i 
η((−W i−c)·x)
 
 η(c·x) 
 η(c·x) 
 
 η(c·x) 
 
 η(−c·x) 
 
 η(−c·x) 
 
 η(−c·x) 
  (60)
 η(d·x) 
 η(d·x) 
 
 η(d·x) 
 
 η(d·x) 
 
 η(d·x) 
 
 η(d·x) 
 
 η(d·x) 
 η(d·x) 
 
 η(d·x) 
 
 η(d·x) 
 
 η(d·x) 
η(d·x)
Importantly, each row here is n wide. Recall that A′ ∈ Rn×24n. Denote the ith row of A′ by
A′ ∈ R24n with A′ ∈ R24n. If A′ had 0 hardwired entries, then setting A = 1 −1 −
i i i 24i 24i+3
(1 −1 )wouldgiveA η(B′·x)=W ·x,bythesameargumentasinAppendixB.3.2.
24i+6 24i+9 i i
Unfortunatelythisisnotthecase,sowehavetousetheconstructioninAppendixB.3.3. Recall,that
A′hasn fixedentriesineachrow. ThismeansthatN hasn entriesequalto0. Sinceeveryentry
fix i fix
ofB′xhasthreecopies,aslongasN doesnothavethreeelementssetto0inarow,A′ canbemade
i i
equivalenttoA =1 −1 −(1 −1 ). Thisisbecause,asinAppendixB.3.3,ifat
i 24i 24i+3 24i+6 24i+9
most2elementsoutofany3threecopiesarehardwired,thenthethirdcanbechangedarbitrarilyto
offsetthehardwiring.
Further,justasinAppendixB.3.3,theprobabilitythatagivenrowofA′hasthreeitemshardwired
in a row is O(n3 fix). Thus, by the union bound, the probability that some row of A′ has three
n2
items hardwired in a row is
O(n3
fix). So, with large enough width n, A can be chosen such that
n
A′η(B′x)=Wx.
Similarly,anylinearmapinRn˜×nforn˜ <ncanalsobefitusingthismethod.
Conclusion
WehaveshownthataW-AsymmetricMLPwithhiddendimension24ncanexactlyfitann×n
linearmapwithhighprobabilityoverthechoiceofAsymmetricmasksM. Itisknownby[7]thatfor
anycontinuousfunctionf : Ω ⊆ RD → Randanyε > 0,thereexistsawidthk′ suchthat
target
2-layerMLPsofwidthk′canapproximatef towithinε.
Let k be sufficiently big so that the probability that the masks do not satisfy the conditions of
AppendixB.3.4islessthanδ. Suchakexistsaslongasn
fix
∈o(k1 4). Letm≥max(k,k′,D).
Importantly,ifa2-layerMLPofwidthk′canapproximatef towithinε,a2-layerMLPofwidth
target
mwithm≥k′canalsoapproximatef towithinε. Letf beawidthmMLPthatapproximates
MLP
f towithinε.
target
23We now pad the input x to f , with m−D zeros. This allows us to define a new function
target
f0 :Rm →Rbyf0 ([x;0])=f(x). Clearlyf0 canalsobeapproximatedbyawidthm
target target target
MLP.
Letf0 denotethewidthmMLPthatapproximatesf towithinε. Now,f0 hasdimensions
MLP 0 MLP
m → m → 1,withcorrespondinglinearmapsW ∈ Rm×m, W ∈ R1×m. Eachofthesemaps
1 2
canbeexactlyfitusinga2-layerW-AsymmetricMLP,sincetheircorrespondingmatriceshaveat
leastasmanycolumnsasrows. ConcatenatingthesetwoexactfitsyieldsanasymmetricMLPwhose
outputexactlymatchesf0 andthusapproximatesf towithinε.
MLP 0
Thus, setting n′ = m, there exists a width n′ such that for all n > n′, with probability 1−δ,
for a randomly sampled 4-layer W-Asymmetric MLP f with η nonlinearity, hidden dimensions
24n→n→24n,andn ∈o(n1/4)hardwiredentriesperneuron,therewillexistθ ∈Θsuchthat
fix
theW-AsymmetricMLPf :Rn →Rapproximatesf toε.
target
Onparametersymmetriesofthe4-layerW-AsymmetricNetwork
Asanaside,thisprocedureofmapping2-layerstandardMLPsto4-layerW-AsymmetricMLPs
impliesthatthese4-layerW-AsymmetricMLPshaveatleastasmanysymmetriesas2layerstandard
MLPs. Tofixthis,wemaywanttoconsideranonlinearityηsuchthatη(x)−η(−x)̸=x.
C Limitations
AlthoughourW-Asymmetricandσ-Asymmetricnetworksaremotivatedbyremovingparameter
space symmetries, their distinct empirical behavior may be caused by other factors besides just
parameterspacesymmetries. Forinstance,thefixedentriesFfortheW-Asymmetricapproachare
takentobemuchlargerthanthestandardinitializationoflinearmaps,whichcouldcauseseveral
changestooptimizationandlosslandscapesbesidesjustparametersymmetrybreaking.
Also,ourtheoreticalresultscouldbestrengthenedbyfutureworkinseveralways. Forinstance,for
theσ-Asymmetricapproach,Proposition1onlygivesaguaranteeofnoparametersymmetriesinthe
two-layernetworkcasewithsquareinvertibleweights. Futureworkcouldalsogivetighteranalysis
oftherequiredwidthanddepthforuniversalapproximationusingourW-Asymmetricarchitecture.
D BroaderImpacts
Thisworkdoesnotfocusonanyparticularapplicationarea. Instead,westudyfundamentalphenom-
enaandtheoryofdeeplearningingeneral. Ourworkhaspotentialtoimproveknowndeficitsof
neuralnetworks: bymakingneuralnetworklosslandscapesmoresimilartoconvexlandscapes,we
canimproveourunderstandingofthem,andbyimprovingBayesianneuralnetworksweadvance
oneparadigmforbetteringuncertaintyquantificationinneuralnetworks. However,unlikestandard
neuralnetworks,whichhavemillionsofpapersstudyingthem,wehaveonlyscratchedthesurface
of Asymmetric networks. Important properties such as generalization, robustness to distribution
shifts,andadversarialrobustnesshavenotbeenextensivelystudiedforAsymmetricnetworks,and
theinteractionofparametersymmetrieswiththesepropertiesisnotclear. Futureresearchshould
furtherexploretheseimportantproperties.
E ExperimentalDetails
E.1 LinearModeConnectivityExperimentalDetails
E.1.1 ImageClassifierInterpolation
Fortheimageclassificationexperiments,weusetwotypesofmodels.
1. ResNetWetrainResNet20swithLayerNormofwidth64and8·64. Weuseabatchsizeof
128andalearningratethatwarmsupfrom.0001to.01over20epochs. Inthewidth8×
multipliercasewetrainfor50epochs,andinthewidth1×multipliercasewetrainfor100.
Forσ-AsymmetricResNets,wewarmuptoalearningrateof.001insteadof.01dueto
traininginstability.
242. MLPWetrainMLPswith4layers,LayerNorm,andwidth512. ForMNISTwetunedthe
hyperparameters(epochs,learningrate,weightdecay)ofboththeAsymmetricandStandard
modelstominimizelossbarrier. Weuseabatchsizeof64.
ForMNISTweusenodataaugmentation,andforCIFAR-10weuserandomcroppingandhorizontal
flipping. For the Git-ReBasin tests, we use the weight matching algorithm from [1]. For MLPs
on MNIST, we used the Asymmetry hyperparameters in Table 5. Table 6 gives the Asymmetric
hyperparametersforResNet20onCIFAR-10,andTable7liststhesameforResNet20with8xlarger
width.
Table5: W-Asymmetricnetworkhyperparametersfordepth4MLPs. n referstothenumberof
fix
weightswerandomlyfixperneuron. κreferstothestandarddeviationofthenormaldistributionthat
thefixedentriesFaredrawnfrom.
Layer n κ
fix
Linear-1 64 1
Linear-2 64 1
Linear-3 64 1
2
Linear-4 256 1
4
Table6: W-AsymmetricnetworkhyperparametersforResNet20swithwidthmultiplier1. n refers
fix
tothenumberofweightswerandomlyfixperoutputchannel(forconvolutionallayers)orneuron
(forlinearlayers). κreferstothestandarddeviationofthenormaldistributionthatthefixedentriesF
aredrawnfrom.
Block n κ
fix
FirstConv 12 2
Block1-Conv 36 2
Block1-Skip 4 2
Block2-Conv 54 2
Block2-Skip 6 2
Block3-Conv 72 2
Block3-Skip 8 2
Linear 8 2
Table7: W-AsymmetricnetworkhyperparametersforResNet20swithwidthmultiplier8onCIFAR-
10. Weuse3timesmorefixedentriesperoutputchannelorneuronthanforTable6.
Block n κ
fix
FirstConv 27 2
Block1-Conv 108 2
Block1-Skip 12 2
Block2-Conv 162 2
Block2-Skip 18 2
Block3-Conv 216 2
Block3-Skip 24 2
Linear 24 2
E.1.2 GraphNeuralNetworkInterpolation
FortheGNNexperiments,weuseaGNNarchitecturesimilartoGIN[71]withmeanaggregation.
ThebaseGNNhasthreemessagepassinglayersandahiddendimensionof256,whichgives176,424
trainableparameters. Thedatasetisogbn-arXiv[26],whichisacitationnetworkofcomputerscience
arXivpaperswith169,343nodesand1,166,243edges. Thetaskistransductivenodeclassification,
wherethelabelofeachpapernodeistheprimarysubjectareaofthepaper.
Asiscommonintransductivenodeclassificationonmodestlysizedgraphs,wetraineachnetwork
withfull-batchgradientonthewholegraph. Thus,therandomnessintrainingispurelyfromthe
25initialization—thereisnonoisefromminibatchselectioninSGD.WeusetheAdamoptimizer[30]
withapeaklearningrateof.001. Thelearningrateislinearlywarmedupfor25epochstothepeak,
andthenisheldconstant. Eachnetworkistrainedfor500epochs.
For the Git-ReBasin alignment, we implement the activation matching approach. For the σ-
Asymmetric GNN, we take σ to be FiGLU, in which we randomly initialize each fixed matrix
√
F as a standard normal matrix with standard deviation .01/ d where d is the number of hidden
channels;wefoundthathavingsmallstandarddeviationhelpedwithtrainingandinterpolation. For
theW-AsymmetricGNN,wefix6constantsineachrowofeachlinearmap,andrandomlyinitialize
theseconstantsfromanormaldistributionwithstandarddeviation.5.
E.2 BayesianNeuralNetworkExperimentalDetails
FortrainingBayesianneuralnetworks,weusethevariationalinferenceapproachofTomczaketal.
[62],whichfitsanapproximateposteriorthatisGaussianwithadiagonalplusrank-4covariance
matrixstructure. FortheW-AsymResNettests,wetrainResNet20swiththesameAsymmetric
hyperparameters as in Table 6, though with κ = .5. For the CIFAR-100 experiments, we use a
standard linear layer instead of hardwiring weights for the last fully-connected linear layer. On
CIFAR-100wealsouseawidthmultiplierof2forourResNets. FortheResNetexperiments,weuse
alearningrateof.001. Wetrainwithabatchsizeof250for50epochs.
FortheMLPexperiments, weuseκ = .5, 8hardwiredentriesperneuron, andalearningrateof
.0005. Abatchsizeof250isusedfor50epochsagain.
Weusestandarddataaugmentation(horizontalflipsandrandomcrops)onCIFAR-10andCIFAR-100,
andnodataaugmentationsforMNIST.AlltrainingisdonewiththeAdamoptimizer[30].
E.3 MetanetworkExperimentalDetails
E.3.1 DatasetDetails
WetrainedtwodatasetsofimageclassifiersonCIFAR-10:oneconsistingof10,000smallResNet-like
convolutionalneuralnetworks,andoneconsistingof10,000networkswithasimilararchitecture,
thatuseourgraph-basedapproachtoremovingparametersymmetries. Forfasttrainingofmany
imageclassifiers,weusetheFFCVpackage[36]. Inparticular,weusetheirCIFAR-10samplescript
https://github.com/libffcv/ffcv/tree/main/examples/cifar,whichincludesdataaug-
mentation(randomhorizontalflips,randomtranslations,andCutout[9]),labelsmoothing[60],anda
linearlearningratewarmupanddecay. Intotal,trainingall20,000classifierstakesjustunder400
GPUhours(about2GPU-weeks)onNVIDIARTX2080TiGPUs.
SeeTable8forthehyperparametersandrangesthatwevariedacrossthenetworksinourdatasets. In
eachdataset,thetrainednetworksallhavethesamearchitecture.
Each ResNet has 78,042 trainable parameters, and each W-Asym ResNet has 60,634 trainable
parameters. Bothhavethesamearchitecture,excepttheW-AsymResNethascertainfiltersthatare
fixedtoconstantstobreaktheparametersymmetries. TheResNetseachhave8convolutionlayers,
LayerNorm[4],andafinalfully-connectedlinearclassificationlayerafteraveragepoolingacross
spatialdimensions.
Table8: Hyperparametersanddistributionswesampledfromforthedatasetsofimageclassifiersthat
wetrainedonCIFAR-10. Unif(a,b)istheuniformdistributionover[a,b],andRandInt(a,b)isthe
uniformdistributionoverintegersin[a,b](inclusiveofendpoints).
Hyperparameter Distribution
Learningrate .5·10−Unif(0,2)
Weightdecay 10−Unif(1,5)
Labelsmoothing Unif(0,.2)
Epochs RandInt(10,40)
26Table9: LearningrateandnumberofparametersforeachtypeofmetanetworktrainedinTable3.
ResNet W-AsymResNet
LR #Params LR #Params
MLP 10−4 4,994,945 10−4 3,880,833
DMC[11] 10−3 105,357 5·10−3 105,357
DeepSets[73] 10−2 8,897 5·10−3 8,897
StatNN[63] 10−3 119,297 10−2 119,297
E.3.2 MetanetworkDetails
Wetrainedseveraltypesofmetanetworksforourexperiments. Allofthesemetanetworksaretrained
for50epochsusingtheAdamWoptimizer[39]. Foreachmetanetwork,oneachdataset,wechoose
thelearningratein{10−5,10−4,5·10−4,10−3,5·10−3,10−2}thatgivesthebestvalidationR2
performanceononetrainingrun. Thenweruntraineachtypeofmetanetwork5timesoneachdataset,
andreportthemeanandstandarddeviationforeachmetricinTable3.
E.4 MonotonicLinearInterpolationExperimentalDetails
Forthemonotoniclinearinterpolationexperiments,weusedthesamesetupasinthetrainingofthe
datasetsofCIFAR-10imageclassifiersinSection5.3. Foreacharchitecture,wesample300sets
ofhyperparametersfromthedistributionsinTable8, andtrainonenetworkforeachsetofthese
sampledhyperparameters. Whenevaluatingtrainingloss,weincludethelabelingsmoothingterm.
√
For the σ-Asymmetric networks, we initialize the FiGLU F with a standard deviation of 1/ d,
wheredisthenumberofchannelsinthelayer. Notethatthisisconsiderablylargerthanthestandard
√
deviationof.01/ dusedintheGNNexperimentsofSection5.1;wefoundthissettingtotrainbetter
(notethatthisinitializationisinlinewithstandardinitializationsoftrainableparameters). Further,
fortheσ-Asymmetricnetworks,24outofthe300networksdivergedduringtraining(givingNaNs),
so we exclude them from the computation of statistics in Table 4. From manual inspection, this
divergenceseemstohappenwhenthelearningrateishigh(greaterthan.1). Incontrast,noneofthe
standardorW-Asymmetricnetworksdiverged.
E.5 MiscellaneousExperimentalDetails
ThedatasetsweuseareMNIST[37],CIFAR-10[32],CIFAR-100[32],andogbn-arXiv[26],which
areallwidelyusedinmachinelearningresearch. Thefirstthreeappeartonothavelicensesandare
opentouse,whilethelastdatasetisfromtheOpenGraphBenchmark,whichhasanMITLicensein
theGithubrepository.
WeusesoftwarepackagesincludingPyTorch[50](forallneuralnetworkexperiments),FFCV[36]
(forbuildingourdatasetinSection5.3),andPyTorchGeometric[14](forGNNexperiments).
WeranourexperimentsonseveraltypesofNVIDIAGPUsandcomputesystems,including2080Ti,
3090Ti,4090Ti,andV100GPUs. EverytrainingrunwasconductedonatmostoneGPU.
27