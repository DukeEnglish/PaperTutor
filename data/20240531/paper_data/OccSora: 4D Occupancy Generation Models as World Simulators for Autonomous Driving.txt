OccSora: 4D Occupancy Generation Models as
World Simulators for Autonomous Driving
LeningWang1,2,∗,† WenzhaoZheng2,3,∗,‡ YilongRen1 HanJiang1
ZhiyongCui1 HaiyangYu1 JiwenLu3
https://wzzheng.net/OccSora
1StateKeyLabofIntelligentTransportationSystem,BeihangUniversity,China
2EECS,UCBerkeley,UnitedStates 3DepartmentofAutomation,TsinghuaUniversity,China
leningwang@buaa.edu.cn; wenzhao.zheng@outlook.com
Abstract
Understandingtheevolutionof3Dscenesisimportantforeffectiveautonomous
driving. Whileconventionalmethodsmodelscenedevelopmentwiththemotionof
individualinstances,worldmodelsemergeasagenerativeframeworktodescribe
thegeneralscenedynamics. However,mostexistingmethodsadoptanautoregres-
siveframeworktoperformnext-tokenprediction,whichsufferfrominefficiencyin
modelinglong-termtemporalevolutions. Toaddressthis,weproposeadiffusion-
based4Doccupancygenerationmodel,OccSora,tosimulatethedevelopmentof
the3Dworldforautonomousdriving. Weemploya4Dscenetokenizertoobtain
compact discrete spatial-temporal representations for 4D occupancy input and
achievehigh-qualityreconstructionforlong-sequenceoccupancyvideos. Wethen
learnadiffusiontransformeronthespatial-temporalrepresentationsandgenerate
4Doccupancyconditionedonatrajectoryprompt. Weconductextensiveexperi-
mentsonthewidelyusednuScenesdatasetwithOcc3Doccupancyannotations.
OccSoracangenerate16s-videoswithauthentic3Dlayoutandtemporalconsis-
tency,demonstratingitsabilitytounderstandthespatialandtemporaldistributions
ofdrivingscenes. Withtrajectory-aware4Dgeneration,OccSorahasthepotential
toserveasaworldsimulatorforthedecision-makingofautonomousdriving. Code
isavailableat: https://github.com/wzzheng/OccSora.
1 Introduction
Asapromisingapplicationofartificialintelligencetechnology,autonomousdrivinghasgarnered
widespreadattentionandresearchinrecentyears[16,7,46]. Establishingtherelationshipbetween
perception[27,3,4,29],prediction[14,11,24],andplanning[32,19,20,41]inautonomousdriving
iscrucialforacomprehensiveunderstandingofthefield.
Conventionalautonomousdrivingmodels[16]relyonthemotionoftheegovehicleinstancesto
modelthedevelopmentofscenes,unabletodevelopaprofoundunderstandingofsceneperception
andvehiclemotioncontrolcomparabletohumanunderstanding. Theemergenceofworldmodels
[12]offersnewpossibilitiesforadeeperunderstandingofthecomprehensiverelationshipbetween
autonomousdrivingscenesandvehiclemotion. Basedonstrongimagepretrainedmodels,image-
basedworldmodels[15,42]cangeneratehigh-qualitydriving-sceneimageswithconditionsof3D
boundingboxes. OccWorld[53]furtherlearnsaworldmodelinthe3Doccupancyspace,whichcan
bebetterleveragedfor3Dreasoningforautonomousdriving. However,mostexistingmethodsadopt
anautoregressiveframeworktomodelthedynamics(e.g.,imagetokens,boundingboxes,occupancy)
ofa3Dscene,whichhinderstheirabilitytoefficientlyproducelong-termvideosequences.
*Equalcontribution.†WorkdoneduringaninternshipatUCBerkeley.‡Projectleader.
Preprint.
4202
yaM
03
]VC.sc[
1v73302.5042:viXraConventional Methods Existing World Models OccSora
Input Input TrajectoryPrompt
A
Map 3D Boxes ObservationatT otu
r 4DWorldModel
e
g
Prediction 2D/3DWorldModel r
e
InstanceTrajectory ObservationatT+1
ss
evi 4DSequences
Figure1: Comparisonswithexistingmethods. Itcancomprehendtheintricaterelationshipbetween
scenesandtrajectoriesandgeneratelong-term,physicallyconsistent4Doccupancy.
Toaddressthis,weproposea4DworldmodelOccSoratodirectlygeneratespatial-temporalrepre-
sentationswithdiffusionmodelsasshowninFigure1,motivatedbyOpenAI’s2Dvideogeneration
modelSora[1]. Toaccuratelyunderstandandrepresent4Dscenes,wedesign4Dscenediscretization
tocapturethedynamiccharacteristicsofscenesandproposeadiffusion-basedworldmodeltoachieve
controllable scene generation following physical laws. Specifically, in the 4D occupancy scene
tokenizer,wefocusonextractingandcompressingreal4Dscenestoestablishanunderstandingofthe
worldmodelenvironment.Inthediffusion-basedworldmodel,weemploymultidimensionaldiffusion
techniquestopropagateaccuratespatiotemporal4Dinformationandrealizetrajectory-controllable
scenegenerationbyincorporatingrealegocartrajectoriesascondition,therebyachievingadeeper
understandingbetweenautonomousdrivingscenesandvehiclemotioncontrol. Throughtrainingand
testing,OccSoracangenerateautonomousdriving4Doccupancyscenesthatadheretophysicallogic
andachievecontrollablescenegenerationbasedondifferenttrajectories. Theproposedautonomous
driving4Dworldmodelopensupnewpossibilitiesforunderstandingdynamicscenechangesin
autonomousdrivingandthephysicalworld.
2 RelatedWork
3DOccupancyPrediction. 3Doccupancyfocusesonpartitioningspaceintovoxelsandassigning
specificsemantictypestoeachvoxel. Itisconsideredacrucialmeansofrepresentingreal-world
scenes,following3Dobjectdetection[29,28,48]andBird’sEyeView(BEV)perception[47,52,
40,51],forautonomousdrivingperceptiontasks. Earlyresearchonthistaskprimarilyfocusedon
semantically classifying discrete points from LiDAR [55, 36, 25, 56]. In fact, due to the camera
containingsemanticinformationfarexceedingthatofLiDARandtheirlowcost. Thus,utilizing
images for depth estimation or employing end-to-end methods for 3D scene perception research
iscurrentlythemainstreamapproach[18,23,44,17]. Consideringtheadvantagesofmulti-sensor
systems,somestudiesresearchmulti-modalfusionfor3Doccupancyprediction[43,49].
Inadditiontoutilizingtypicalsensordevicesfor3Doccupancyprediction,somestudiesfocuson
othertasksinvolvingoccupancy. Forinstance,OccWorld[53]proposesaspatiotemporalgenerative
transformer to predict subsequent scene tokens and the vehicle token, thereby predicting future
occupancy and vehicle trajectory. On the other hand, GenOcc [39] utilizes generative models to
accomplishoccupancyprediction. DriveWorld[31]introducesaworld-model-basedframeworkfor
learning in autonomous driving from 2D images and videos, addressing tasks such as 3D object
detection,onlinemapcreation,andoccupancyprediction. Althoughprogresshasbeenmadein3D
occupancypredictionandcontinuous4Dprediction,thescopeofthesestudiesremainslimited. They
usuallyuseautoregressivemodelsinconjunctionwithsceneinformationfromprecedingframesto
carryoutsubsequentoccupancytasks,therebynecessitatingpriorsceneor3Dboundingboxinputs.
Consequently,theylackagenuineunderstandingofthefundamentalrelationshipsbetweensceneand
motion,andthereforedonotconstituteworldmodelsconditionedonactions.
GenerativeModel. Generativemodelshavegarneredwidespreadattentionrecentlyduetotheir
powerfulcapabilities. Bylearningtheprobabilitydistributionofdata,generativemodelscantrain
modelscapableofgeneratingnewsamples. FromtheemergenceofGenerativeAdversarialNetworks
(GAN)[10]totherecentadventofdiffusionmodelslikeVariationalAutoencoders(VAE)[37],the
tasksofgenerativemodelshavegraduallyexpandedfrominitialimagegenerationtaskstoin-depth
studiesonvideos[45]. TaskssuchasimagegenerationbasedontheDITmodel[34]delveintoand
utilizetheirgenerativecapabilities. TheSoravideogenerationmodel[1]furtherdemonstratesthe
abilitytoproducehigh-qualityvideoswithrealistictransitionsbetweenframesincontinuousscenes.
Similarly, in the field of autonomous driving, controllable image generation can provide various
drivingscenariostoserveperception,planning,control,anddecision-makingtasks. Forinstance,
24D Occupancy Scene Tokenizer
Class Tokenizer
Encoder Quantizer Decoder
GT 4D Occupancy Reconstruction 4D Occupancy
Token
OccSora Diffusion-based World Model Trajectory Control Generate
Diffusion
Go Straight Turning Right
Noise
Any Trajectory
Motionless Accelerate
Figure2: ThepipelineofOccSora. The4Doccupancyscenetokenizerachievescompressionand
restorationofrealinformation. Thecompressedinformationandvehicletrajectoriesaresimultane-
ouslyusedasinputsforthediffusion-basedworldmodel. Aftertraining,thediffusion-basedworld
modelutilizesrandomnoiseandarbitrarytrajectoriestogeneratecontrollabletokens,whicharethen
decodedinto4Doccupancymapsinthe4Doccupancyscenetokenizerstage.
MagicDriver[8]generatesvideosdepictingvariousweatherscenariosbylearningfromvideosof
autonomous driving vehicles and incorporating labels such as object detection boxes and maps.
DriveDreamer[42]proposesaworldmodelthatisentirelyderivedfromreal-worlddrivingscenes,
enablingadeepunderstandingofstructuredtrafficconstraintsandtherebyachievingpreciseand
controllablevideogeneration. However,forautonomousdrivingscenarios,obtainingthe3Doccu-
pancyofscenesismoreimportantcomparedto2Dinformation[50,30,35]. Somestudies[22,26]
proposeathree-dimensionaldiffusionmodelsuitableforgeneratingoutdoorrealscenes,which,by
utilizingdiffusionmethods,accomplishesscalableseamlessscenegenerationtasks. Whilesome
previous studies have generated 2D static images and extended them to the temporal dimension
throughautoregression,andothershaveachievedstaticgenerationof3Doccupancyscenes,boththe
2Dimagesgeneratedbasedon3Dobjectboundingboxesandthestaticlarge-scalescenesaredifficult
todirectlyapplytoautonomousdrivingtasks[41,54]. Incontrast,ourproposedOccSoraestablishes
adynamic4Doccupancyworldmodelthatadaptstoscenechangeswithvehicletrajectories,without
theneedforanypriorobjectdetectionboxesorsceneinformation,representingthefirstgenerative
4Doccupancyworldmodelforautonomousdriving.
3 ProposedApproach
3.1 WorldModelforAutonomousDriving
4Doccupancycancomprehensivelycapturethestructural,semantic,andtemporalinformationofa3D
sceneandeffectivelyfacilitateweaksupervisionorself-supervisedlearning,whichcanbeappliedto
visual,LiDAR,ormultimodaltasks. Basedontheseprinciples,werepresenttheworldmodelχas4D
occupancyR. Figure2illustratestheoverallframeworkofOccSora. Weconstructeda4Doccupancy
scenetokenizertocompressreal4DoccupancyR ∈RB×D×H×W×T inboththetemporalT and
in
spatialD×H×W dimensions,capturingtherelationshipsandevolutionpatternsin4Dautonomous
drivingscenes. Thisresultsincompressedhigh-leveltokensR ∈RB×c×h×w×tandreconstructed
mi
4DoccupancydataR ∈ RB×D×H×W×T. Wedesignedadiffusion-basedworldmodelthatuses
o
trajectoryinformationR ∈RB×T×2ascontrolunits,trainingthemsupervisedbythecompressed
tr
tokensR togeneratehigh-dimensionalscenerepresentationtokensT ∈RB×c×h×w×t. Theyare
mi o
thendecodedbythe4DoccupancyscenetokenizertoconsistentanddynamicallycontrollableR .
o
3.2 4DOccupancySceneTokenizer
Thegoalof4Doccupancypredictionistodeterminethesemantictypeatspecificlocationsovertime.
Wediscretizeandencodethereal4DoccupancysceneR intoanintermediatelatentspaceR
in mi
toobtainatruerepresentationofthe4Doccupancyscene,asshowninFigure3. Theformulaisas
(cid:8) (cid:9)
follows:R =ζ τ (R ) .Here,ζ representstheencodedcodebook,andτ denotes
mi token en in token en
thedesigned3Dencodernetworkandcategoryembedding.This3Doccupancyrepresentationdivides
34D Occupancy Scene Tokenizer
Encoded Tensor Codebook
Class Tokenizer 3D Embedded Tensor Encoder
Conv 3D Attention Quantizer
Loss Design
Down sample Down sample Down sample Down sample Down sample
Conv 3D Conv 3D Conv 3D Conv 3D Conv 3D
Batch Batch Batch Batch Batch
Normalization Normalization Normalization Normalization Normalization
Reconstruction Tensor
Decoder
Reshape Conv 3D Attention Token
Reshape
Figure3: Thestructureofthe4Doccupancyscenetokenizer. Theproposedmethodencodesand
compresses4Dscenestoextracthigh-dimensionalfeatures,whicharethendecodedtoretrievethe
spatiotemporalphysicalcharacteristicsofthescenes.
the3DspacearoundthevehicleintovoxelsrT = N ∈ RH×W×D, whereeachvoxelpositionis
assignedatypelabelN,indicatingwhetheritisoccupiedandthesemanticsoftheobjectoccupyingit.
Unliketraditionalmethods,weincorporateandcompresstemporalinformationwithinthesamescene,
reshapingthetensortoR . Thisapproachallowsforunifiedlearningofbothspatialandtemporal
in
evolutionpatternsandthephysicalrelationshipsofrealscenes,comparedtopreviousautoregressive
methods. Afterpassingthroughtheτ 3Dencodernetworkwithcategoryembeddingandtheζ
en token
encodedcodebook,thetensoristransformedintoR representsthepotentialspaces. Thisreshaping
mi
ensuresacomprehensiverepresentationofthetemporaldynamicsof4Doccupancy.
CategoryEmbeddingandTokenizer. Toaccuratelycapturethespatialinformationoftheoriginal
parameters,wefirstperformanembeddingoperationontheinputR .Weassignalearnablecategory
in
embeddingb ∈ Rc′ foreachcategoryinR tolabelthecategoriesofcontinuous3Doccupancy
in
scenes. Thepositioninformationisembeddedastokensthatrepresentthecategories. Then,these
embeddingsareconcatenatedalongthefeaturedimension. Tofacilitatesubsequent3Dencodingwith
compressioninspecificdimensions,wefurtherreshapeR intoR′ ∈RB×(Dc′)×T×H×W.
in in
3DVideoEncoder. Toeffectivelylearndiscretelatenttokens,wefurtherperformeddownsampling
ontheembeddedpositionalinformationofthe4DoccupancyR′ toextracthigh-dimensionalfeatures.
in
The designed encoder architecture comprises a series of 3D downsampling convolutional layers,
whichperform3Ddownsamplinginboththetimedimension(T)andspatialdimensions(H×W),
increasingthefusiondimensiontoD×c′.WeinitiallydownscaledtheinputR′ threetimestoobtain
in
R i′′ n ∈ RB×(8×Dc′)×T 8×H 8×W 8 ,andintroduceddropoutlayersafterthefeedforwardandattention
block layers for regularization. Considering the relationships between consecutive frames, we
introducedcross-channelattentionafterdownsampling,segmentingR′′ alongthe8×Dc′dimension
in
andthenperformingcross-channelattentionbetweenthesegmentedparts. Thisoperationenhanced
themodelabilitytocapturerelationshipsbetweenfeaturesalongdifferentaxes,andsubsequently
reshapedthembacktotheoriginalshapetoobtaintheoutputtensorR .
mi
CoodbookandTrainingObjective. Toachieveamorecondensedrepresentation,wesimultaneously
learnacodebookζ ∈RN×D containingNcodes. Eachcodeb∈Rc′ inthecodebookencodes
token
ahigh-levelconceptofthescene,suchaswhetherthecorrespondingpositionisoccupiedbyacar.
(cid:91)
ζ tokenrepresentstheencodedcodebook. WequantizeeachspatialfeatureR m(ij i)inR(cid:100) mibymapping
(cid:91)
ittothenearestcodeN(R(ij),B):
mi
(cid:91) (cid:91)
R(ij) =N(R(ij),ζ )= min ||R(ij)−b|| , (1)
mi mi token mi 2
b∈ζtoken
(cid:91)
where||·|| representstheL2norm. Subsequently,weintegratethequantizedfeaturesR(ij)toobtain
2 mi
thefinalscenerepresentationR .
mi
3D Video Decoder. To reconstruct R from the learned scene representation R , we design a
o mi
decoderconsistingof3Ddeconvolutionlayers. Incontrasttotheencoder,thedecoderarchitecture
includescross-channelattention,residualblocks,andaseriesof3Dconvolutions,enablingupsam-
plinginbothtemporalandspatialdimensions. ThisgradualupsamplingprocesstransformsR to
mi
itsoriginaloccupancyresolutionR . Thedecoderthensplitstheresultalongthechanneldimension
o
4Diffusion-based World Model Loss
Random Noise
Token
d n
a
ren
iLep
ah
seR
m
roN
reyaL
Train Token
Embedding Generate Token
Ego Car Train Frozen
Trajectory
Generate Train
Time Step Multi-Head Self-Attention
Any Pointwise Feedforward
Trajectory Scale Shift
Random Layer Norm
Noise
Figure4: Thestructureofthediffusion-basedworldmodel. Themodelinvolvesutilizingthe
optimalcodebookobtainedfromtrainingthe4Doccupancyscenetokenizertoconvert4Doccupancy
intoasequenceoftokens. Thesetokens,alongwiththeegovehicletrajectoryandrandomnoise,are
thencombinedasinputfordenoisingtrainingtoacquirethegeneratedtoken.
toreconstructthetemporaldimension,yieldingoccupancyvaluesforeachvoxel. Duringtraining,
weaccomplishedthetrainingoftheencoder,decoderparameters,andtheencodingcodebook. The
designed network enables us to simultaneously encode the input 4D occupancy information and
compressitintomultipletokens,therebylearningthephysicalcorrelationsofworldmodelsunder
spatiotemporalfusion. Additionally,werestoretheinformationduringthedecodingprocess.
3.3 Diffusion-basedWorldModel
Inspiredbythediffusionmethod[33],weusescenetokensR containingspatiotemporalinforma-
mi
tionfeaturesasinputsforthegenerativemodel. Additionally,weconductdenoisingtrainingand
trajectory-controllablegenerationtasksunderthecontrolofvehicletrajectoriesR ,asshownin
tr
Figure4.
TokenEmbedding. Toefficientlyandaccuratelyutilizethetransformer[38],weflattentheinput
datatokensR intoR . Simultaneously,consideringthesignificanceofpositionalinformationfor
mi re
spatiotemporalcompression,weperformpositionalembeddingontheinput. Wedesignthefollowing
function,whichutilizessinandcosfunctionstoencodepositionalindices:
R(emb) =embd+R ,R ∈RB×c×(hwt). (2)
re i re re
Itoperatesontwomainparameters: C,representingtheembeddingoutputdimensionalityofeach
position,andi=hwt,representingthenumberoftokensenumeratingthepositionstobeencoded.
TheresultingoutputfollowsamatrixstructureofdimensionsC×i,andembconstructsthepositional
embeddingrepresentationusingsinandcosfunctions. Theseembeddingsencapsulatethepositional
attributesofthetokens,enhancingthemodelunderstandingofpositionswithintheinput. Weadd
thepositionalencodingE totheinputR ,yieldingR(emb),whichrepresentsthetokensafter
mb re re
positionalencoding.
Trajectory Conditioning Embedding. The transformation relationship between scenes and tra-
jectoriesisacrucialaspectofautonomousdriving. Generatingdiverse4Doccupancyscenesthat
alignwithcontroltrajectoriesisessential. Therefore,weusetheegovehicletrajectoryT asinput
r
togeneratecontrollable4Doccupancy. Firstly,theegovehicletrajectoryT ∈RB×t×2 isusedas
r
oneofthecontrolinputs,wheretdenotesthecontinuoustimedimension,andthethirddimension
representsthevehiclepositionsalongthexandyaxesoftheabsolutecoordinatesystem. Toachieve
trajectoryembeddingandencoding,wereshapethevehicletrajectorytoT ∈RB×(t×2)andlearn
r
andencodeitasfollows:
g =ν(t)+δ(T ),δ(T )∈RB×c×(hwt),ν ∈RB×c×(hwt), (3)
r r
whereν ∈RB×c×(hwt)representsthetimestepembedding,andδdenotestheMultilayerPerceptron
(MLP)networkthatextractstrajectoryinformation. Itisthenembeddedgintotheinputsequenceof
thediffusiontransformerandprocessedtogetherwiththetokeninformationR(emb).
re
DiffusionTransformer. Wedevelopedadiffusion-basedworldmodeltolearnfromandgenerate
withinthelatentspaceR ,whileintegratingtrajectorylabelsT anddenoisingtimestepsν ascon-
mi r
5trolconditions. Inthemodeldiffusionlearningprocess,weconstructedaforwardnoiseprocessthat
graduallyintroducesnoisetothelatentspaceR : q(cid:0) Rg |R (cid:1) =N(cid:16) Rg ;√ σgR ,(cid:0) 1−σg(cid:1) I(cid:17) ,
mi re re re re
where the constant g represents the embedding of trajectories and time steps. Utilizing the repa-
√ √
rameterizationtrick,wecansample: Rg = σgR + 1−σgϵg,whereϵg ∼ N(0,I). The4D
re re
occupancydiffusionmodelistrainedtolearnthereversepropagationprocess. Toinverttheforward
processcorruption:
p (cid:16) Rg−1|Rg (cid:17) =N(cid:0) µ (Rg ),Σ (Rg )(cid:1) , (4)
θ re re θ re θ re
where neural networks predict the statistical properties of p. The reverse process model is
trained with the variational lower bound of x , which simplifies to: L(θ) = −p(cid:0) R0 |R1 (cid:1) +
0 re re
(cid:80) D (cid:16) q∗(cid:0) Rg−1|Rg ,R0 (cid:1)(cid:17) ||p (cid:0) Rg−1|Rg (cid:1) ,whichexcludingirrelevantadditionaltermsdur-
g KL re re re θ re re
ingtraining. Asbothqandparegaussiandistributions,theKullback-Leibler(KL)divergencecan
beevaluatedusingthemeansandcovariancesofthetwodistributions. Byreparameterizingasa
noisepredictionnetwork,themodelcanbetrainedusingthesimplemeansquarederrorbetweenthe
predictednoiseRˆg andthesampledgaussiannoiseRg : L (θ)= 1(Rˆg −Rg )2.However,to
re re simple 2 re re
trainthediffusionmodelwithlearnedreverseprocesscovariance,thefullKLdivergencetermneeds
tobeoptimized. Wefollowdiffusionmodelsapproach[6]: trainfirstwithL (θ),thenwiththe
simple
fullL. Oncepistrained,newtokencanbesampledbyinitializingRg ∼ N(0,I)andsampling
re
Rg−1 ∼p(Rg−1|Rg )usingthereparameterizationtrick.
re re re
Overall,tokensR processedintheinitialstageasR arepassedtoaseriesoftransformerblocks
mi re
forfurtherrefinement. Theseblockseffectivelycapturetherelationshipsbetweentrajectoryinforma-
tionandtokens. Regardingnoisyimageinputprocessing,thediffusiontransformeremploysspecific
attentionmechanismsandlossfunctionstominimizetheimpactofnoiseonmodelperformance,
ensuringrobustoperationinnoisyenvironments. ToincorporatetrajectorylabelsT anddenoising
r
timestepsν asadditionalcontrolconditions,wefeedthemassupplementaryinputsalongsidetoken
embeddingsintothetransformerblocks. Thisenablesthemodeltodynamicallyadjustitsprocessing
basedontheseconditions,therebybetteradaptingtovarioustrajectorycontrolrequirements. Inthe
end,thetraineddiffusion-basedworldmodelsuccessfullytransformspurenoiseandtrajectorylabels
T intoT ∈RB×c×h×w×t,whichareeventuallydecodedintoR throughthe3Ddecoder.
r o o
4 Experiments
As a 4D occupancy world model in the field of autonomous driving, OccSora offers a deeper
understandingoftherelationshipbetweenautonomousdrivingscenesandvehicletrajectorieswithout
requiringanyinputof3Dboundingboxes,maps,orhistoricalinformation. Itcanconstructalong-
timesequenceworldmodelthatadherestophysicallaws. Wehaveconductedaseriesofquantitative
experimentsandvisualizationstoillustratethis.
4.1 ImplementationDetails
WeconductedexperimentsonthewidelyusednuScenes-Occupancydataset[2],whichiscurrently
oneofthemostmainstreamandstandarddatasets,supportingmanywell-knownresearchstudies
[16,43]. FortheOccSoramodel,weappliedthreeroundsofcompressionto32consecutiveframes
andincreaseditschanneldimensionto128. Subsequently,weconductedfurthercomparativeand
ablation experiments under different components and trajectory scenarios. We trained using the
AdamWoptimizerwithaninitiallearningratesetto1×10−5andaweightdecayof0.01. Using8
NVIDIAGeForceA100GPUs,wesetabatchsizeof2perGPU.Forthetrainingofthe4Doccupancy
scenetokenizer,weneededabout42GBofmemoryperGPUtotrainfor150epochs,whichtook
50.6hours. Forthediffusion-basedworldmodel,weneededabout47GBofmemoryperGPUto
trainfor1,200,000steps,whichtook108hours.
4.2 4DOccupancyReconstruction
Thecompressionandreconstructionof4Doccupancyareessentialforlearningthelatentspatiotem-
poralcorrelationsandfeaturesnecessaryforimagegeneration. Unliketraditionalmodelsforvideo
andimageprocessing,OccSoraoperatesonedimensionhigherthanoccupancyforsingleframes
6Table1: Thequantitativeanalysisof4Doccupancyreconstruction. Despiteacompressionrate32
timesgreaterthanOccWorld,OccSoramaintainsoverhalfofitsreconstructionaccuracycompared
toOccWorld.
Method RatioIoUmIoU
OccWorld[53] 16 62.2 65.7 45.072.269.668.269.444.470.774.867.654.165.482.778.469.766.452.843.7
OccSora 512 37.0 27.4 11.722.6 0.0 34.629.016.6 8.7 11.5 3.5 20.129.061.338.736.531.112.018.4
T0 T1 T2 T3 T4 T5
Ground Truth
Reconstruction
Figure5: Visualizationofreconstructionofthe4Doccupancyscenetokenizer.
Table2: ComparisonsofOccSorawithothermodelsinitsgenerationcapability. Tothebest
of our knowledge, we are the first 4D occupancy generation model. Therefore, we only provide
comparisonswithothergenerativemodelsondifferentdatasetsanddataformats.
Method Type Dimension Dataset FID
DiT[34] Image 2D ImageNet[5] 12.03
MagicDriver[8] Video 3D nuScenes[2] 14.46
DriveDreamer[42] Video 3D nuScenes[2] 14.9
DriveGAN[21] Video 3D nuScenes[2] 27.8
SemCity[22] Occupancy 3D KITTI[9] 40.63
OccSora OccupancyVideo 4D nuScenes[2] 8.348
Diffusion-Frame T=0 T=0 T=0 T=0 T=0 T=1 T=2
Diffusion-step Step=10000 Step=20000 Step=100000 Step=1000000 Step=1200000 Step=1200000 Step=1200000
SoraOcc-Base
Figure 6: The visualization of the progressive generation of accurate scenes as the model
undergoesiterativetraining.
andtwodimensionshigherthanimages. Therefore,achievingefficientcompressionandaccurate
reconstructionisparamount. Figure5depictsthegroundtruthandreconstructionoftheoccupancy.
Wealsoconductedaquantitativeanalysisof4Doccupancyreconstruction,asshowninTable1. The
tableindicatesthatevenwithOccSoraachievingacompressionratio32timesgreaterthanthatof
OccWorld,itstillmaintainsnearly50%mIoUoftheoriginalOccWorldmodel. Thisunifiedtemporal
compressioneffectivelycapturesthedynamicchangesofvariouselements,improvinglong-sequence
modelingcapabilitiescomparedtoprogressiveautoregressivemethods.
4.3 4DOccupancyGeneration
Inthediffusion-basedworldmodelforthe4Doccupancygenerationtask,weusedtokensgenerated
bytheOccSoramodel,trainedwith32frames,asinputforourgenerationexperiments. InFigure
6,wepresentthevisualresultsofacrosstrainingiterations,from10,000to1,200,000steps. These
visualresultsindicatethatasthenumberoftrainingiterationsincreases,theaccuracyoftheOccSora
modelcontinuouslyimproves,demonstratingthegenerationofcoherentscenes.
WecomparedandquantitativelyevaluatedourproposedOccSoramodelagainstothergeneration
models. Asthefirst4Doccupancyworldmodelforautonomousdriving,weonlycompareditagainst
conventional image generation, 2D video generation, and static 3D occupancy scene generation
methods. As shown in Table 2, our model achieves similar performance in terms of the Fréchet
InceptionDistance(FID)[13],demonstratingtheeffectivenessoftheproposedmethod.
7
srehtO reirrab elcycib
sub rac
.hev.tsnoc elcycrotom nairtsedep enoccfifart
reliart kcurt
.fus.evird taflrehto klawedis
niarret
edamnam noitategevT=0 T=5 T=10 T=15 T=20 T=25
Scene 1
Go Straight
FID: 8.902
Position (x, y) (0.966,0.033) (0.801,0.201) (0.653,0.352) (0.502,0.507) (0.340,0.669) (0.173,0.834)
Scene 1
Turning Right
FID: 10.886
Position (x, y) (0.003,0.008) (0.071,0.238) (0.196,0.631) (0.226,0.697) (0.255,0.750) (0.501,0.982)
Scene 1
Motionless
FID: 5.257
Position (x, y) (0.000,0.000) (0.000,0.000) (0.000,0.000) (0.000,0.000) (0.000,0.000) (0.000,0.000)
Figure7:4Doccupancygenerationunderdifferentinputtrajectories.Fromtoptobottom,thereis
gostraight,turningright,andmotionless,witheachscenegenerationcorrespondingtothetrajectory,
ensuringlogicalcoherenceandcontinuity.
T=0 T=5 T=10 T=0 T=5 T=10
Go Straight
2
enecS
3
enecS
Turning Right
2
enecS
3
enecS
Motionless
2
enecS
3
enecS
Figure8: Generatingdiversecontinuousscenesundertrajectorycontrol. Thegeneratedscenes
exhibitdiversitywhilemaintainingthestabilityoftheoriginaltrajectorycontrol.
Table3: Resultsofablativeevaluationondifferentcomponents. Wequantitativelyevaluatedthe
impactofdifferentcompressionrates,components,andchanneldimensionsonthereconstruction
andgenerationresultsthroughcontrolledvariables.
InputSizeR TokenSizeR Channel Class Tembed. Trajectory IoU mIoU FID
in mi
32x200x200 128x4x25x25 8 ✓ ✓ ✓ 37.03 27.42 8.34
32x200x200 128x4x25x25 8 ✓ ✗ ✓ 37.03 27.42 87.26
32x200x200 128x4x25x25 8 ✓ ✓ ✗ 37.03 27.42 17.48
32x200x200 128x4x25x25 4 ✓ ✓ ✓ 29.67 23.21 34.24
32x200x200 128x8x50x50 8 ✓ ✓ ✓ 32.91 24.4 72.32
12x200x200 64x3x50x50 8 ✓ ✓ ✓ 26.73 14.12 187.78
12x200x200 64x3x25x25 8 ✓ ✓ ✓ 22.42 9.27 270.23
12x200x200 32x3x25x25 8 ✓ ✓ ✓ 13.60 3.85 465.18
TrajectoryVideoGeneration. OccSorahasthecapabilitytogeneratevariousdynamicscenesbased
ondifferentinputtrajectories,thuslearningtherelationshipbetweenegovehicletrajectoriesand
sceneevolutioninautonomousdriving. AsshowninFigure7,weinputdifferentvehicletrajectory
motionpatternsintothemodel,demonstratingthe4Doccupancyforgostraight,turningright,and
motionless. Weconductedexperimentsatdifferentscalesforgeneratingtrajectories,revealingthat
theFIDscoreislowestforstationaryscenesandhigherforcurvedscene,indicatingthecomplexity
ofcontinuouslymodelingcurvedmotionscenesandthesimplicityofmodelingstationaryscenes.
SceneVideoGeneration. Diversityinscenesiscrucialunderreasonabletrajectorycontrol. We
tested the reconstruction of 4D occupancy scenes for different scenarios under three trajectories
to verify the generalization performance of generating scenes under controllable trajectories. In
Figure8,theleftandrightpartsrespectivelydemonstratethecapabilitytogeneratedifferentscenes
under the same trajectory. In the reconstructed scenes, surrounding trees and road environments
exhibitrandomvariationswhilestillmaintainingthelogicoftheoriginaltrajectory,showcasingthe
abilitytomaintainrobustnessingeneratingscenescorrespondingtotheoriginaltrajectoryandits
generalizationacrossdifferentscenarios.
8Table4: Thequantitativeanalysisofdifferentscalesregardingdenoisingstepsanddenoising
rates. Denoisingstepshavearelativelyminorimpactonthemodel,whereasdenoisingratesand
modelscalessignificantlyaffectthequalityofthegeneratedoutputs.
FID
Step InputSizeR TokenSizeR
in mi
10% 30% 50% 70% 90% 100%
10 32x200x200 128x4x25x25 49863 34927 17630 339 42 9.1
100 32x200x200 128x4x25x25 53297 29521 19471 1084 72 10.08
1000 32x200x200 128x4x25x25 32171 10284 5924 591 17 8.94
10 12x200x200 64x3x50x50 71293 54625 5644 7416 742 431
100 12x200x200 64x3x50x50 81274 53431 45346 3161 456 446
1000 12x200x200 64x3x50x50 43631 33415 17431 4366 379 353
10 % 30 % 50 % 70 % 90 % 100 %
Go Straight
1000 Step
Go Straight
100 Step
Motionless
100 Step
Figure9: Effectofthedenoisingratiosunderdifferenttrajectoriesordenoisingsteps. Denoising
stepsandtrajectorieshaveminorimpactswhiledenoisingratioshaveasignificanteffect.
4.4 AblationandAnalysis
AnalysisoftheTokenizerandEmbeddings.Weconductedanablationoftheproposedcomponents
includingdifferentcompressionscales,thenumberofclasstokenizerdiscretizations,time-stepembed-
dings,andvehicletrajectoryembeddings,asshowninTable3. Whenthenumberofclasstokenizer
discretizationswasreducedfrom8to4,thereconstructionaccuracydroppedbyapproximately18%.
TheFIDscorealsodeclinedafterremovingthetime-stepembeddings. Withoutpositionembeddings,
thegeneratedsceneslackedmotioncontrolanddisplayedalmostlinearmovementpatternsinfluenced
bythedatadistribution. Additionally,atlowercompressionratios,althoughthereconstructionper-
formancewasbettercomparedtohighercompressionratios,thelackofhigher-dimensionalfeature
correlationspreventedthegenerationofeffectivescenes.
AnalysisoftheGenerationSteps. Thetotalnumberofdenoisingstepsandthedenoisingratecan
affectthegenerationqualitytosomeextent. AsshowninFigure9,asthedenoisingrateincreases,
thegeneratedscenesbecomeprogressivelyclearer. AccordingtothequantitativeresultsinTable4,
increasingthetotalnumberofdenoisingstepscanimprovegenerationaccuracytoacertainextent.
However, the generation quality is much more significantly influenced by the token size and the
numberofchannelsthanbythetotalnumberofdenoisingsteps.
5 ConclusionandLimitations
Inthispaper,wehaveintroducedaframeworkforgenerating4Doccupancytosimulate3Dworld
developmentinautonomousdriving. Usinga4Dscenetokenizer,weobtaincompactrepresentations
forinputandachievehigh-qualityreconstructionforlong-sequenceoccupancyvideos.Then,welearn
adiffusiontransformeronthespatiotemporalrepresentationsandgenerate4Doccupancyconditioned
onatrajectoryprompt. ThroughexperimentsonthenuScenesdataset,wedemonstrateaccuratescene
evolution. Inthefuture,wewillinvestigatemorerefined4Doccupancyworldmodelsandexplore
thepossibilitiesofend-to-endautonomousdrivingunderclosed-loopsettings.
Limitations. Theadvantageofa4Doccupancyworldmodelliesinestablishinganunderstandingof
therelationshipbetweenscenesandmotion. However,duetolimitationsinthegranularityofvoxel
data,wecannotconstructmorefinelydetailed4Dscenes. Thegenerativeresultsalsodemonstrate
inconsistentdetailsformovingobjects,possiblyduetothesmallsizeofthetrainingdata.
9References
[1] TimBrooks,BillPeebles,ConnorHolmes,WillDePue,YufeiGuo,LiJing,DavidSchnurr,JoeTaylor,
TroyLuhman,EricLuhman,ClarenceNg,RickyWang,andAdityaRamesh. Videogenerationmodelsas
worldsimulators. 2024.
[2] HolgerCaesar,VarunBankiti,AlexHLang,SourabhVora,VeniceErinLiong,QiangXu,AnushKrishnan,
YuPan,GiancarloBaldan,andOscarBeijbom. nuscenes:Amultimodaldatasetforautonomousdriving.
InInCVPR,pages11621–11631,2020.
[3] ChengChang,JiaweiZhang,KunpengZhang,WenqinZhong,XinyuPeng,ShenLi,andLiLi. Bev-
v2x:cooperativebirds-eye-viewfusionandgridoccupancypredictionviav2x-baseddatasharing. IEEE
TransactionsonIntelligentVehicles,2023.
[4] XuanyaoChen,TianyuanZhang,YueWang,YilunWang,andHangZhao. Futr3d:Aunifiedsensorfusion
frameworkfor3ddetection. InInCVPR,pages172–181,2023.
[5] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. InInCVPR,pages248–255.Ieee,2009.
[6] PrafullaDhariwalandAlexNichol. Diffusionmodelsbeatgansonimagesynthesis,2021.
[7] DaochengFu,XinLi,LichengWen,MinDou,PinlongCai,BotianShi,andYuQiao. Drivelikeahuman:
Rethinkingautonomousdrivingwithlargelanguagemodels. InInWACV,pages910–919,2024.
[8] RuiyuanGao,KaiChen,EnzeXie,LanqingHong,ZhenguoLi,Dit-YanYeung,andQiangXu.Magicdrive:
Streetviewgenerationwithdiverse3dgeometrycontrol. arXivpreprintarXiv:2310.02601,2023.
[9] AndreasGeiger,PhilipLenz,andRaquelUrtasun. Arewereadyforautonomousdriving?thekittivision
benchmarksuite. InInCVPR,pages3354–3361.IEEE,2012.
[10] IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,Aaron
Courville,andYoshuaBengio.Generativeadversarialnetworks.CommunicationsoftheACM,63(11):139–
144,2020.
[11] JunruGu,ChenxuHu,TianyuanZhang,XuanyaoChen,YilunWang,YueWang,andHangZhao. Vip3d:
End-to-endvisualtrajectorypredictionvia3dagentqueries. arXivpreprintarXiv:2208.01582,2022.
[12] DavidHaandJürgenSchmidhuber. Worldmodels. 2018.
[13] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter. Gans
trainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium,2018.
[14] AnthonyHu,ZakMurez,NikhilMohan,SofíaDudas,JeffreyHawke,VijayBadrinarayanan,Roberto
Cipolla,andAlexKendall. Fiery:Futureinstancepredictioninbird’s-eyeviewfromsurroundmonocular
cameras. InICCV,2021.
[15] AnthonyHu,LloydRussell,HudsonYeo,ZakMurez,GeorgeFedoseev,AlexKendall,JamieShotton,
and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving. arXiv preprint
arXiv:2309.17080,2023.
[16] YihanHu,JiazhiYang,LiChen,KeyuLi,ChonghaoSima,XizhouZhu,SiqiChai,SenyaoDu,Tianwei
Lin,WenhaiWang,etal. Planning-orientedautonomousdriving. InInCVPR,pages17853–17862,2023.
[17] Yuanhui Huang, Wenzhao Zheng, Borui Zhang, Jie Zhou, and Jiwen Lu. Selfocc: Self-supervised
vision-based3doccupancyprediction. arXivpreprintarXiv:2311.12754,2023.
[18] YuanhuiHuang,WenzhaoZheng,YunpengZhang,JieZhou,andJiwenLu. Tri-perspectiveviewfor
vision-based3dsemanticoccupancyprediction. InInCVPR,pages9223–9232,2023.
[19] ZhiyuHuang,HaochenLiu,JingdaWu,andChenLv. Differentiableintegratedmotionpredictionand
planningwithlearnablecostfunctionforautonomousdriving. IEEEtransactionsonneuralnetworksand
learningsystems,2023.
[20] XiaosongJia,PenghaoWu,LiChen,JiangweiXie,ConghuiHe,JunchiYan,andHongyangLi. Think
twicebeforedriving:Towardsscalabledecodersforend-to-endautonomousdriving. InInCVPR,pages
21983–21994,2023.
[21] SeungWookKim,JonahPhilion,AntonioTorralba,andSanjaFidler. Drivegan:Towardsacontrollable
high-qualityneuralsimulation. InInCVPR,pages5820–5829,June2021.
[22] JuminLee,SebinLee,ChanghoJo,WoobinIm,JuhyeongSeon,andSung-EuiYoon. Semcity:Semantic
scenegenerationwithtriplanediffusion. arXivpreprintarXiv:2403.07773,2024.
[23] YimingLi,ZhidingYu,ChristopherChoy,ChaoweiXiao,JoseMAlvarez,SanjaFidler,ChenFeng,and
AnimaAnandkumar.Voxformer:Sparsevoxeltransformerforcamera-based3dsemanticscenecompletion.
InInCVPR,pages9087–9098,2023.
10[24] MingLiang,BinYang,WenyuanZeng,YunChen,RuiHu,SergioCasas,andRaquelUrtasun. Pnpnet:
End-to-endperceptionandpredictionwithtrackingintheloop. InCVPR,2020.
[25] XinhaoLiu,MoonjunGong,QiFang,HaoyuXie,YimingLi,HangZhao,andChenFeng. Lidar-based4d
occupancycompletionandforecasting. arXivpreprintarXiv:2310.11239,2023.
[26] YuhengLiu,XinkeLi,XuetingLi,LuQi,ChongshouLi,andMing-HsuanYang. Pyramiddiffusionfor
fine3dlargescenegeneration. arXivpreprintarXiv:2311.12085,2023.
[27] ZhijianLiu,HaotianTang,AlexanderAmini,XinyuYang,HuiziMao,DanielaLRus,andSongHan.
Bevfusion:Multi-taskmulti-sensorfusionwithunifiedbird’s-eyeviewrepresentation. InInICRA,pages
2774–2781.IEEE,2023.
[28] XinzhuMa, WanliOuyang, AndreaSimonelli, andElisaRicci. 3dobjectdetectionfromimagesfor
autonomousdriving:asurvey. IEEETransactionsonPatternAnalysisandMachineIntelligence,2023.
[29] JiagengMao,ShaoshuaiShi,XiaogangWang,andHongshengLi. 3dobjectdetectionforautonomous
driving:Acomprehensivesurvey. InternationalJournalofComputerVision,131(8):1909–1963,2023.
[30] LarsMescheder,MichaelOechsle,MichaelNiemeyer,SebastianNowozin,andAndreasGeiger.Occupancy
networks:Learning3dreconstructioninfunctionspace. InInCVPR,pages4460–4470,2019.
[31] ChenMin,DaweiZhao,LiangXiao,JianZhao,XinliXu,ZhengZhu,LeiJin,JianshuLi,YulanGuo,
JunliangXing,etal. Driveworld:4dpre-trainedsceneunderstandingviaworldmodelsforautonomous
driving. arXivpreprintarXiv:2405.04390,2024.
[32] Sajjad Mozaffari, Omar Y Al-Jarrah, Mehrdad Dianati, Paul Jennings, and Alexandros Mouzakitis.
Deeplearning-basedvehiclebehaviorpredictionforautonomousdrivingapplications:Areview. IEEE
TransactionsonIntelligentTransportationSystems,23(1):33–47,2020.
[33] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint
arXiv:2212.09748,2022.
[34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In In ICCV, pages
4195–4205,2023.
[35] ChonghaoSima,WenwenTong,TaiWang,LiChen,SileiWu,HanmingDeng,YiGu,LeweiLu,Ping
Luo,DahuaLin,andHongyangLi. Sceneasoccupancy. 2023.
[36] Gurpreet Singh, Soumyajit Gupta, Matthew Lease, and Clint Dawson. Range-net: A high precision
streamingsvdforbigdataapplications. arXivpreprintarXiv:2010.14226,2020.
[37] AaronVanDenOord,OriolVinyals,etal. Neuraldiscreterepresentationlearning. InNeurIPS,30,2017.
[38] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. InNeurIPS,30,2017.
[39] GuoqingWang,ZhongdaoWang,PinTang,JilaiZheng,XiangxuanRen,BailanFeng,andChaoMa.
Occgen: Generative multi-modal 3d occupancy prediction for autonomous driving. arXiv preprint
arXiv:2404.15014,2024.
[40] PengqinWang,MeixinZhu,HongliangLu,HuiZhong,XiandaChen,ShaojieShen,XuesongWang,and
YinhaiWang. Bevgpt:Generativepre-trainedlargemodelforautonomousdrivingprediction,decision-
making,andplanning. arXivpreprintarXiv:2310.10357,2023.
[41] TianqiWang,SukminKim,JiWenxuan,EnzeXie,ChongjianGe,JunsongChen,ZhenguoLi,andPing
Luo. Deepaccident:Amotionandaccidentpredictionbenchmarkforv2xautonomousdriving. InInAAAI,
volume38,pages5599–5606,2024.
[42] XiaofengWang,ZhengZhu,GuanHuang,XinzeChen,andJiwenLu. Drivedreamer:Towardsreal-world-
drivenworldmodelsforautonomousdriving. arXivpreprintarXiv:2309.09777,2023.
[43] XiaofengWang,ZhengZhu,WenboXu,YunpengZhang,YiWei,XuChi,YunYe,DalongDu,Jiwen
Lu,andXingangWang. Openoccupancy:Alargescalebenchmarkforsurroundingsemanticoccupancy
perception. InInICCV,pages17850–17859,2023.
[44] YiWei,LinqingZhao,WenzhaoZheng,ZhengZhu,JieZhou,andJiwenLu. Surroundocc:Multi-camera
3doccupancypredictionforautonomousdriving. InInICCV,pages21729–21740,2023.
[45] WilsonYan,YunzhiZhang,PieterAbbeel,andAravindSrinivas. Videogpt:Videogenerationusingvq-vae
andtransformers,2021.
[46] Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou Zhu, Zhaoxiang Zhang, Gao Huang,
HongyangLi,YuQiao,LeweiLu,etal. Bevformerv2:Adaptingmodernimagebackbonestobird’s-eye-
viewrecognitionviaperspectivesupervision. InInCVPR,pages17830–17839,2023.
[47] LeiYang,KaichengYu,TaoTang,JunLi,KunYuan,LiWang,XinyuZhang,andPengChen. Bevheight:
Arobustframeworkforvision-basedroadside3dobjectdetection. InInCVPR,pages21611–21620,2023.
11[48] HaibaoYu,YingjuanTang,EnzeXie,JileiMao,PingLuo,andZaiqingNie. Flow-basedfeaturefusionfor
vehicle-infrastructurecooperative3dobjectdetection. InNeurIPS,36,2024.
[49] JiZhangandYiranDing.Occfusion:Depthestimationfreemulti-sensorfusionfor3doccupancyprediction.
arXivpreprintarXiv:2403.05329,2024.
[50] YunpengZhang,ZhengZhu,andDalongDu. Occformer: Dual-pathtransformerforvision-based3d
semanticoccupancyprediction. InInICCV,pages9433–9443,2023.
[51] Yunpeng Zhang, Zheng Zhu, Wenzhao Zheng, Junjie Huang, Guan Huang, Jie Zhou, and Jiwen Lu.
Beverse:Unifiedperceptionandpredictioninbirds-eye-viewforvision-centricautonomousdriving,2022.
[52] YuanZhao,LuZhang,JiajunDeng,andYanyongZhang. Bev-radar:bidirectionalradar-camerafusionfor
3dobjectdetection. JUSTC,54(1):0101–1,2024.
[53] WenzhaoZheng,WeiliangChen,YuanhuiHuang,BoruiZhang,YueqiDuan,andJiwenLu. Occworld:
Learninga3doccupancyworldmodelforautonomousdriving. arXivpreprintarXiv:2311.16038,2023.
[54] WenzhaoZheng,RuiqiSong,XiandaGuo,andLongChen. Genad:Generativeend-to-endautonomous
driving. arXivpreprintarXiv:2402.11502,2024.
[55] ZixiangZhou, YangZhang, andHassanForoosh. Panoptic-polarnet: Proposal-freelidarpointcloud
panopticsegmentation. InInCVPR,pages13194–13203,2021.
[56] Sicheng Zuo, Wenzhao Zheng, Yuanhui Huang, Jie Zhou, and Jiwen Lu. Pointocc: Cylindrical tri-
perspectiveviewforpoint-based3dsemanticoccupancyprediction. arXivpreprintarXiv:2308.16896,
2023.
12A Appendix
A.1 MoreVisualizations
Toprovideresultsforlongertimeseries,wepresentthegeneratedscenesunderdifferentegovehicle
trajectorycontrols, namelyGoStraight, TurningRight, Motionless, andAccelerateinFigure10.
Additionally,wealsoshowcasetheequivalentcontrolmethodsunderdifferentscenesinFigure11.
Generation theLong 4D Occupancy of OccSora (Scene 1)
Go Straight
Turning Right
Motionless
Accelerate
Figure10: Theabilitytogeneratelongtime-series4Doccupancyunderdifferenttrajectory
controls. Fromtoptobottom,wepresentlong-termcontinuousscenesgeneratedunderfourtypesof
egovehicletrajectories: GoStraight,TurningRight,Motionless,andAccelerate.
13Generation theLong 4D Occupancy of OccSora (Scene 2)
Go Straight
Turning Right
Motionless
Accelerate
Figure 11: The generalization ability to generate different scenes under fixed ego vehicle
trajectories. Fromtoptobottom,weshowthecapabilitiesofgeneratingdifferentscenesunderthe
fourvehicletrajectories: GoStraight,TurningRight,Motionless,andAccelerate.
14