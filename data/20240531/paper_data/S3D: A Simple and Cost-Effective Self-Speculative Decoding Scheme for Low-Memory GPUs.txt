S3D: A Simple and Cost-Effective Self-Speculative
Decoding Scheme for Low-Memory GPUs
WeiZhong Manasa Bharadwaj
wei.zhong@lge.com manasa.bharadwaj@lge.com
LG Electronics,TorontoAILab
Abstract
105
Speculativedecoding(SD)hasattractedasig-
nificantamountofresearchattentionduetothe 104
s feu rb es nta cn e.tia Hl osp we ee vd eu rp
,
dit ec span iteac th hi eev he igf hor sL peL eM dui pn s- Self-Spec Medusa EAGLE MCSD Sequoia CLLM Ours
they offer, speculative decoding methods of- 2.0
7B model
ten achieve optimalperformanceon high-end M speed
15
devices or with a substantial GPU memory 1.5 VRAM
overhead. Given limited memory and the 10
1.0
necessity of quantization, a high-performing
model on a high-end GPU can slow down 0.5 5
by up to 7 times. To this end, we propose
0.0 0
(S ok rip Sp 3y D)S ,i am cu olt sa tn -ee fo fu ecs tivS epe sc eu lfl -a st piv ee cuD lae tic vo ed Sin Dg Self-Spec Medusa EAGLE MCSD Sequoia CLLM Ours
methodbasedonsimultaneousmulti-tokende-
Figure 1: Training efficiency, inference efficiency per
coding and mid-layer skipping. When com-
memory unit, and load-time VRAM evaluated for dif-
paredagainstrecenteffectiveopen-sourceSD
ferent models on MT-Bench. From left to right: The
systems, our method has achieved one of the
mostrecentopen-sourceSDsystemsorderedbyrelease
top performance-memory ratios while requir-
dates. All systems use 7B target models with 8-bit
ingminimalarchitecturechangesandtraining
quantization.Ourmodel(S3D)standsoutinbothtrain-
data. Leveraging our memory efficiency, we
ingefficiencyandmemory-speedtrade-offs.
createdasmalleryetmoreeffectiveSDmodel
basedonPhi-3. Itis1.4to2timesfasterthan
the quantized EAGLE model and operates in
2024;Lietal.,2024;Chenetal.,2024)addacon-
half-precisionwhileusinglessVRAM.
siderable amount of memory, e.g., due to the ex-
tra modules or a large token trees used for draft-
1 Introduction ing. When models are deployed at scale, even a
minor memory overhead can largely increase the
Speculative decoding (SD) (Stern et al., 2018; cost of inference, given the current high cost of
Zhangetal.,2024;Xiaetal.,2024)canaccelerate using HBM VRAM in inference. On the other
LLMinference without sacrificing thequality. As hand,high-performingSDcanachieveremarkable
a result, it is becoming one of the most common speedupsusingalargemodel(Zhangetal.,2023a;
optimization techniques inLLMs. Atahighlevel, Zhaoetal.,2024;Yangetal.,2024)oronhigh-end
typical speculative decoding (SD) works by draft- GPUs(Zhangetal.,2023a;Chenetal.,2024;Kou
ingtokensatarelativelyfasterspeed,andthenver- et al., 2024; Elhoushi et al., 2024). However, we
ifyingtheguessed tokensattheendofaniteration noticethatthesespeedups becomelimitedoreven
usingafullforwardpass. Thespeedupisbasedon start underperforming when a smaller model or
theassumptionthattheacceptedtokensinonefor- low-memory device is used where quantization is
ward pass during the verification step will offset necessary. Surprisingly, the state-of-the-art open-
thecostofthedraftingsteps. sourceSDmodel(inspeedups)mayslowdownby
However, greater speedups are not always free. up to 7 times when applying quantization under
Ononehand,somepopularSDsystems(Caietal., constrainedGPUmemory,highlightingthesignifi-
1
4202
yaM
03
]LC.sc[
1v41302.5042:viXra
deeps
dezilamron-meM
atad
gniniarT
%
esaercni
MARV
ledoMcantoverheadsfromquantization(Linetal.,2024). astranslationandgrammarerrorcorrection,where
In such cases, we question the cost-effectiveness significant speedups are easily achieved. These
ofexisting SDmethods, eveniftheyshow greater methodsarecharacterized bysingle-branch specu-
speedups onhigh-end GPUs. lationusingadditionalmodules(Sternetal.,2018)
Although recently Chen et al. (2024) designed or an independent draft model (Xia et al., 2023).
a GPU-agnostic optimization by pre-profiling the Byusingspeculative sampling(Chenetal.,2023),
GPUtimesforthedraftandverifystages,theirap- SD can also sample tokens from target model dis-
proach relies on the hard assumption of fixed ac- tribution (Leviathan et al., 2023). In contrast, our
ceptance rates among different levels of the draft work targets general domain tasks and focuses on
token tree, making it less applicable to various greedy1 andnon-batching decodingviasimultane-
SDmethods. Additionally, the optimal trees have ous multi-token prediction. We save memory and
fewer differences for smaller models or slower gaintraining efficiencythrough layer-skipping.
GPUs,limitingtheirpotential onlow-enddevices.
2.1 Multi-TokenPredictions
Inthiswork,weintroduceSkippySimultaneous
Speculative Decoding (or S3D) to achive fast in- Since Stern et al. (2018), predicting the next k to-
ference, low VRAM costs, and high training effi- kenssimultaneouslyhasproveneffective,butitre-
ciency. Ourkeycontributions arelistedbelow: quires adding k feed-forward decoder layers from
the last encoder state. Cai et al. (2024) popular-
EffectiveSelf-speculative SD:Wepropose asim-
ized this idea using Medusa heads, additionally
ple and memory-saving self-speculative decoding
predicting multiple token branches using tree at-
scheme named S3D for low-memory GPUs. Our
tention(Miaoetal.,2024;SpectorandRe,2023).
scheme features mid-layer skipping and simulta-
IntheSpecDecmethod(Xiaetal.,2023),multi-
neous multi-token predictions, offering no added
token prediction is done by unmasking future to-
VRAM costs and high training efficiency. Com-
kensfrommultipledecoderheadsattendingtodif-
paredtoZhangetal.(2023a), S3Dovercomes the
ferent encoder states, utilizing distinct attention
limitedspeedups insmallermodels.
queries for predicting different tokens. However,
Optimal hyper-parameters: Instead of relying SpecDecrequiresfullmodelfine-tuningforalllay-
on statistical optimization, we formalize the re- ers as the decoder-only target model has not been
lationship between the number of skipped layers pretrained on multi-token unmasking tasks. More
andspeedupinself-speculativedecoding,asprevi- recently, Bhendawade et al. (2024) predict multi-
ously studied in Zhang et al. (2023a) empirically. ple tokens by adding streaming embeddings ini-
Basedonourformalization, wecanalsoverifythe tialized from upper layers, with the token tree re-
optimalnumberoftokenpredictors,asempirically duced by early exiting. However, they made ma-
observedbyGloeckleetal.(2024). jor changes to the original architecture, and their
speedups areconsidered limited.
Optimal speed-memory ratio: Our SD method
Multi-tokenpredictioncanalsobeimplemented
demonstrates optimalperformance-memory ratios
auto-regressively(Yangetal.,2024;Lietal.,2024;
amongrecentopen-sourceSDmodels. Byexploit-
Ankner et al., 2024), which takes multiple steps
ingthememoryefficiency,wecanavoidthesignif-
to predict the next draft token conditioned on pre-
icantquantizationoverheadsundercertainVRAM
viously drafted tokens in one iteration. To miti-
constraintsandoutperformthepreviousfastestSD
gate the substantial overheads incurred by multi-
method, i.e., EAGLE(Lietal.,2024), under 8-bit
step drafting within a single iteration, the draft
quantization by up to 3.9x in speedups on A10G
overhead should be minimal while ensuring it re-
GPU. Moreover, by switching to a smaller target
tains the capability to generate acceptable tokens.
model,wehavecreatedamoreeffectiveSDmodel
In the case of EAGLE (Li et al., 2024), this is
basedonPhi-3,whichdecodes1.4to2timesfaster
achieved by efficiently concatenating the target
thanEAGLEonA10GwhileusinglessVRAM.
model’s high-level features with the embeddings
2 RelatedWork or hidden states of the next tokens for regression
viaanadditional layerofTransformerdecoder.
Early work in speculative decoding (SD) using
1Our approach can be easily extended to support sam-
Transformers (Stern et al., 2018; Sun et al., 2021;
pling; we focus on greedy decoding as it is orthogonal to
Xiaet al., 2023) focused on in-domain tasks such speculativesampling.
2Another line of work to generate multiple draft of the target model are used for drafting tokens.
tokensinparallelisbasedonJacobiiterationmeth- These approaches mitigate the common memory
ods, treating auto-regressive decoding in LLM as overheads of SD by incorporating minimal or no
anon-linear system ofequations, orJacobi decod- extramodulesforthedraftstage. Specifically,they
ing (Song et al., 2021; Santilli et al., 2023). In either entail no additional training (Zhang et al.,
practice, however, an LLM may obtain marginal 2023a) oronlynecessitate training forlearning an
speedupsfromJacobidecodingasitcanrarelypro- adaptive early exit threshold (Bae et al., 2023; El-
duce an accepted token if a previous token in the houshi et al., 2024) with a potential trade-off in
trajectory is predicted incorrectly. Lookahead de- quality. In Elhoushi et al. (2024), they reuse KV-
coding (Fu et al., 2024) attempts to address this cachefromdraftstages,reducing thecomputation
issue by introducing memory costs and caching neededfortheremaininglayersintheverifystage.
n-gram tokens from previous Jacobi trajectories. However, their approach involves training Layer-
Inspired by the Consistency Model (Song et al., Drop(Fanetal.,2019)andrequirescomplexearly
2023), CLLMs (Kou et al., 2024) additionally exitthresholds duringinference.
traintheirtargetmodelstominimizethedistances A concurrent work by Gloeckle et al. (2024)
between Jacobi trajectories and the fixed point, combines self-speculative decoding with multi-
leading to faster convergence and thus greater token predictions. However, their approach trains
speedups. Compared to regular SD methods, Ja- additional independent heads comprising entire
cobidecodingdoesnothaveaseparatedraftphase. Transformerlayers,potentially addingmoremem-
ory cost compared toEAGLE(Lietal., 2024). In
2.2 LayerSkipping
contrast, our SD scheme imposes no extra model
Layerskippingisatypeofstructuredpruningtech- load cost and has minimal training requirements
nique (Anwar et al., 2017; Louizos et al., 2018; through applying mid-layerskipping.
Xia et al., 2022) that reduces a model by only us-
ing a subset of its layers. Structured pruning is 3 Preliminaries
particularly intriguing forLLMoptimizations due
Given a Transformer decoder-only model (Rad-
to its compatibility with GPU acceleration. This
fordetal.,2018)M ,itsgenerated next-token dis-
is because it enables immediate gains in mem- p
tribution p(t ) given the current context tokens
ory and compute by discarding substructures en- ≤i
t = t ,t ,...,t can be expressed in terms of
tirely(Ouderaaetal.,2024). ≤i 1 2 i
Transformerlayers. Forlayerℓ = 1,2,...,L,
Various layer skipping schemes explored for
Transformer models are discussed next. Early
(0)
h =Emb(t ) (1)
Exiting (Dehghani et al., 2019; Teerapittayanon i i
etal.,2017)utilizesearlylayersandskipstherest. h(ℓ) =T(ℓ)(h(ℓ−1) ,Pos ) (2)
i ≤i ≤i
LayerDrop (Fan et al., 2019) randomly drops lay-
(L)
t ∼p(t ) = LM-Head(h ) (3)
ers during training and skips layers during infer- i+1 ≤i i
ence. Lagunas etal. (2021); Ouderaa etal.(2024)
where Emb represents the embedding transforma-
haveidentifiedsub-layerstructures (e.g.,attention
tion, and T(ℓ) denotes the Transformer layer at
heads orweightblocks) tobepruned during train-
level l, which receives the context hidden states
ing. Sajjad et al. (2022) propose symmetric drop-
(ℓ−1)
from the previous layer h , associated with
ping of complete top and bottom layers, based ≤i
their position information Pos . The LM-Head
on the observation that middle layers of a Trans- ≤i
mapsthehiddenspacetoavocabularydistribution
formerarelesscritical. Thisobservation hasbeen
pforthenexttokensampling.
validated by Ma et al. (2023); Wu and Tu (2024)
The decoder-only language model is typically
on larger LLMs and for KV-cache as well. Re-
trainedusingthenext-tokenpredictiontask,where
cently,Raposoetal.(2024)havetrainedadditional
training involves employing cross entropy loss
parameterstodynamically skiplayers.
across tokens in parallel. Given a sample of se-
Combining layer skipping with SD leads to an
quential tokenst ,i = 1,2,...,N, thelossis
intriguing way to save memory, known as self- i
speculative decoding (Bae et al., 2023; Zhang
N−1
et al., 2023a; Liu et al., 2024a; Elhoushi et al., 1
2024),whereastaticoradaptivenumberoflayers
L =
N −1
X −logp(t ≤i) ti+1 (4)
i=1
3During a SD iteration, a more efficient draft currentdraftstatesfromtoplayersarekeptforde-
model M is often used to predict the next γ to- coding. Additionally, we do not necessarily skip
q
ken(s)fromthetargetmodelM throughsampling lower layers due to the adjustments required in
p
t ∼ q(t )wherej = 0,1,2,...,γ−1. To lower-levelrepresentationsforskippedmiddlelay-
i+j+1 ≤i+j
produce tokens as if they were sampled from the ers,anotionexploredsimilarlybyMaetal.(2023)
target distribution, Leviathan et al. (2023) show andWuandTu(2024). Wewillfurtherjustifythis
that we can verify drafted tokens by comparing skipping schemeinSection5.3.
p(t )withq(t )successively, andaccepteach
i+j i+j Training: Our training objective is to accurately
token with a probability of min(1, p q(( tt ii ++ jj )) ). Upon uncover masked tokens while preserving the orig-
completion, one more last token t i+γ+1 can be inal next-token prediction capability. To this end,
sampled from the target distribution p. On rejec- we train the draft model to decode both the next
tion, sampling isdone fromanormalized distribu- token right after i and its following masked to-
tionofmax(0,p−q). Ingreedydecoding,thispro- kens. Assume the masked tokens are located at
cess is equivalent to accepting only the matched i+1,i+2,...,i+γ −1,ourtraining lossis
tokensproduced frompandq.
1
L(S3D) =
|D|
X−logq(t ≤j)
tj+1
(9)
4 S3D
j∈D
We propose a self-speculative SD scheme called wheredecoding setD = {i,i+1,...,i+γ−1}.
Skippy Simultaneous Speculative Decoding (or During training, we freeze the skipped layers
S3D).InS3D,thedraftmodelM usespartiallay- to preserve the target model distribution. Instead
q
ersfromtargetmodelM . of predicting next tokens sequentially, we assign
p
To adhere to the Transformer decoder architec- masked tokens randomly so that training samples
ture and circumvent the need for auxiliary mod- can be processed in one batch, utilizing the paral-
ules, we opt to emulate the Masked Language lelism ofTransformer. Anillustration ofourmod-
Modeling (MLM) task commonly employed in elingisshowninFigure2.
Transformer encoder training. This involves in- Predictingspeedup: Giventarget ratioβ ∈ [0,1],
serting aspecial mask token, denoted as <M>, into which represents the ratio of target model param-
future inputs to predict the next γ tokens concur- eters that the draft model uses during decoding,
rently. Specifically, the acceptance rate α of the frist drafted token
should be a function of β. Naturally, in the self-
t ,...,t ∼q(t ,<M>,...,<M>) (5)
i+1 i+γ ≤i speculative case, α(1) = 1 and α → 0 when
| γ{−z1 } β → 0.
Inthiswork,wehavehypothesizedafunctionto
wherethedraftmodelM usesallprevioushidden
q
(ℓ) estimate draft token acceptance rate as a function
statesofthetargetmodelh ,ℓ = 1,2,...,L:
≤i ofmodelsize(parameterized byU):
h(ℓ) = T(ℓ)(h(ℓ−1) ,Pos ) (6) 1−Uβ
i ≤i ≤i α(β;U) = . (10)
1−U
Different from Xia et al. (2023), the simultane-
We will show in Section 5.3 that the above func-
ouslygeneratedtokensatj = i+1,i+2,...,i+γ
tionalignswellwithempiricalobservations.
require only propagating through lower and top
In multi-token predictions, assume the true ac-
layers, skipping middle m-th to n-th layers of the
ceptancerateatthek-thdrafttoken,i.e.,α (β),is
k
targetmodel:
discounted by k in a discrete function (which we
(n−1) (m) mayreadilyestimatefromempiricaldata). Follow-
h =h (7)
j j ing the notation in Li et al. (2024), the expected
h( jℓ′) =T(ℓ′)(h ≤(ℓ j′−1) ,Pos ≤j) (8) newlygenerated tokensτ is2
where non-skipping layers ℓ′ = 1,2,...,m,n,n +
γ+1 n−1
τ(γ,β) = Xn· Yα k(β)·z n(β) (11)
1,...,L. Unlike Zhang et al. (2023a), a non-
n=1 k=1
skippinglayerℓisabletoutilizepreviousstatesof
thetarget
model,i.e.,h(ℓ−1)
. Furthermore, incon-
2Whendraftingasinglenexttoken,i.e.,α k(β)=α 1(β),
≤i Eq.11isacappedgeometricseriesandcanbefurthersimpli-
trast to early exiting in Zhang et al. (2023a), the fiedtoaformulagivenbyLeviathanetal.(2023).
4Figure2:AnillustrationofS3Dbasedonsimultaneouspredictionsofthelastγtokens(γ =2).Amasktoken<M>
isaddedintovocabularypriortotraining,andapartialmodelistrainedtopredictthenexttokenssimultaneously.
Tree attention is adopted to verify multiple branches of predictions give top candidates of the k-th draft token.
Unlikeotherself-speculativedecodingmethodsbasedonfully-skippedlayers,weonlyskipthemiddlelayerson
topofthedrafttokenssothatthedraftmodelcanaccesshigh-levelfeaturesfromtoplayersaswellastheprevious
statesverifiedbythecompletetargetmodel.
where the shorthand notation z (β) = 1−α (β) for effectiveness evaluation. We report accuracy
n n
ifn 6= γ+1and1otherwise. scoresforHuman-EvalandRouge-1andRouge-L
When the number of drafting tokens is not sig- scores(Lin,2004)forCNN-Daily.
nificant(whichcommonlyhappensonlow-endde-
Baselines A fair comparison is conducted by run-
vices), itisreasonable toassume thetimecost for
ning all systems on identical samples and hard-
afull-model forward pass to be a constant C. We
ware using a uniform evaluation framework. To
also assume there isa fixedoverhead H for each
0 this end, our model is compared to open-source
iteration, proportionally to C, i.e., H = δ · C,
0 SD systems including: Self-Spec (Zhang et al.,
then deriving the decoding speed by taking out C
2023a), Medusa (Cai et al., 2024), EAGLE (Li
leavesustheImprovementFactor(Leviathanetal.,
et al., 2024), MCSD (Yang et al., 2024), Se-
2023)forself-speculative decoding, i.e.,
quoia (Chen et al., 2024), Ouroboros (Zhao et al.,
τ(γ,β) 2024), and CLLMs (Kou et al., 2024). Self-
IF(γ,β) = , (12)
δ+β+1 Specisatraining-freeself-speculativemethodthat
predicts the next single token via layer skipping.
assumingthattheforwardtimeforapartialmodel
Medusa is a popular SD method that adds paral-
scaleslinearly withthenumberofitsparameters.
lel decoder heads to predict multiple next tokens.
NotethatEq.12represents adifferent improve-
EAGLE concatenates target model late-layer hid-
mentfactorcomparedtotheoneinauto-regressive
den states with the last token embedding to pre-
draftingschemes(Leviathanetal.,2023),whereγ
dictthenext5tokensauto-regressivelyviaanaddi-
predictions areperformedsequentially.
tionalTransfomerdecoder layer. Andrecentwork
5 Experiments MCSD,Sequoia, andOuroboros generate draftto-
kens through a separate draft model. In partic-
5.1 ExperimentalSetup
ular, Sequoia constructs an optimal draft token
Datasets We consider datasets commonly used tree from profiling the underlying hardware. In
in SD evaluations, including MT-Bench (Zheng these three systems, we adopt the 68M JackFram
et al., 2023) for multi-turn conversation, Human- LLaMA (Miao et al., 2024) as the draft model,
Eval (Chen et al., 2021) for coding, and CNN- whichisalso thedefault and mostefficient option
Daily (Hermann et al., 2015) for summarization. for their LLaMA target models. Lastly, CLLMs
For CNN-Daily, we only use the 1,000 samples is considered as the latest development in the di-
similar to Zhang et al. (2023a), while the com- rection of Jacobi or Lookahead decoding (Santilli
plete datasets are used for the others. For MT- etal.,2023;Fuetal.,2024).
Bench, we use Gemini Pro (Team et al., 2023)
5Table1: Thecost-effectivenesscomparisonsonA10GGPUconsideringpeakVRAMcosts. Allmodelsare8-bit
quantizedandarebasedonthe7BLLaMA-v2targetmodelexceptmentionedotherwiseinparentheses.Thelargest
3numbersineachcolumnarehighlightedinitalicsorbold. “Peak”denotesthepeakVRAMusageinGiB.The
overall(averaged)resultscountforbothM speedandtherelativeeffectivenessmetricscomparedtothebaseline.
MT-Bench Human-Eval CNN-Daily Overall
Model\Metric
Peak↓ Tok/s M Score Peak↓ Tok/s M Acc.% Peak↓ Tok/s M R-1 R-L M Eff.
Baseline 8.53 7.02 1.00 7.05 8.43 7.02 1.00 6.71 8.96 6.53 1.00 0.19 0.13 1.00 1.00
Self-Spec. 7.77 5.00 0.78 7.08 7.46 4.89 0.79 6.10 8.09 5.01 0.85 0.19 0.14 0.81 0.97
Medusa(Vicuna) 9.09 9.27 1.24 4.98 8.94 10.69 1.44 7.93 9.36 7.62 1.12 0.24 0.14 1.26 1.03
EAGLE 9.58 13.03 1.65 6.98 9.45 15.22 1.93 5.49 9.86 12.43 1.73 0.19 0.13 1.77 0.94
MCSD 7.76 7.72 1.21 6.79 7.40 7.77 1.26 9.76 8.10 6.74 1.14 0.20 0.14 1.21 1.16
Sequoia 8.44 8.64 1.24 6.46 8.35 9.01 1.30 3.05 8.57 7.99 1.28 0.18 0.12 1.27 0.77
Ouroboros 7.95 5.47 0.84 7.08 7.61 5.83 0.92 8.54 8.30 4.91 0.81 0.18 0.13 0.86 1.09
CLLM 7.51 11.75 1.90 5.31 7.37 16.29 2.66 3.66 7.53 8.06 1.47 0.20 0.14 2.01 0.79
Ours
S3D 7.79 12.39 1.93 5.68 7.60 13.85 2.19 6.71 8.80 9.58 1.49 0.28 0.19 1.87 1.09
S3D(Phi-3,fp16) 8.14 25.31 3.78 7.04 7.92 28.13 4.27 20.12 8.87 17.35 2.69 0.25 0.18 3.58 1.77
Implementation details All implementations use FlashAttention-2 (Dao,2023) withabatch sizeof
eager-mode decoding base onHuggingface Trans- 64 for one epoch on A10G GPUs. Please refer to
formers (Wolf et al., 2020) and we adopt the na- AppendixAfordetailed trainingdescriptions.
tiveBNB8-bitquantization(Dettmersetal.,2022)
5.2 MainResults
for quantized models. We configure each system
with greedy decoding and keep the other SD con- Initially, we discuss the cost-effectiveness of our
figurations default for different systems. A non- model. In Table 1 and 2, we observe that S3D
speculative implementation is used as the com- models and CLLM exhibit the highest overall M
mon baseline to calculate speedups and relative speeds among the evaluated systems, which re-
memory costs as it shares the same instructional mainsconsistent acrossdifferent GPUs.
LLaMA-v2 (Touvron et al., 2023) backbone for Importantly, models producing high speedups
mostofourevaluated systems. arenotnecessarily themostcost-effective, asseen
We mostly consider the 7B target model size. in the case of Medusa and EAGLE, where the
In exception to this, we also train a 3.8B Phi-3 cost of extra draft module(s) must be considered.
Mini (Abdin et al., 2024) target model and a 13B We also observed discrepancies in the effective-
LLaMA-v2targetmodeltodemonstratethegener- ness scores of models when using the original
alization ability of our method. Unless specified target model for token verification, likely due to
otherwise, our S3D implementations use the opti- implementation issues or numerical errors. How-
malhyper-parameters suggested bySection5.3. ever,ourmodelgenerallymaintainsbaselineeffec-
tivenessandachievesthehighest overalleffective-
Costeffectiveness metricWepropose amemory-
ness among systems requiring target model train-
normalized speed metric M, which divides a
ing (i.e., Self-Spec., CLLM, and ours). By using
speedup by the relative memory usage compared
layeradapterslikeLoRA(Huetal.,2021),wecan
tothebaselinemodel:
easily enable lossless decoding at the expense of
v m v v
1 1 1 0
M = / = / (13) efficiencycosts(seeanalysis inAppendix B).
v m m m
0 0 1 0
Interestingly, thevanilla self-speculative decod-
where v and m are the generation speed and
0 0 ing method, i.e., Self-Spec, underperforms the
memory consumption of the baseline model, and
baseline in terms of speed for the 7B target
v and m are the generation speed and memory
1 1 model. Thishighlights thelimitation ofnaiveself-
consumption oftheevaluating model. Thismetric
speculative decoding (γ = 1) in smaller models,
quantifies the generation speed per memory unit,
wherethepartialdraftmodelbecomesfurthercon-
ensuringafairandmemory-awarecomparisonfor
strained and unable to propose good draft tokens.
targetmodelsofthesamesize.
In contrast, we alleviate this issue by allowing
TrainingSimilartoMedusa, EAGLE,CLLMs,et the draft model to attend to previous target model
al.(Cai et al., 2024; Li et al., 2024; Kou et al., statesandtrainingthemodeltopredictmultipleto-
2024), we train our models on the ShareGPT kens (γ > 1), thereby enhancing theeffectiveness
dataset. All training is conducted using bf16 and ofsmallerself-speculative models.
65.0
Ours (fp16) dep h 0
70 EAGLE (8-bit quantized) 4.5 0.6 dep h 1
EAGLE (fp16) dep h 2
60 0.5
Our speedup 4.0
50 0.4
3.5
40 3.0 0.3
0.2
30 2.5
0.1
20 2.0
0.0
1 00
2x
RTX
3060
RTX
4090 A10G A100
11 .. 05 L11 → LMH L10 → L31 L9 → L30 LaL7
y
e→
r
L s28 kipL5
p
→
in
gL2 6 scL h3 e→
m
L e24 L1 → L22 L0 → L21 Emb → 20
Figure 4: The overall acceptance rates and individual
Figure 3: Speed comparison between ours (S3D) and acceptance rates at different drafting depths (w/ only
EAGLEondifferentGPUdevices(MT-Benchsamples, a single branch of future tokens). L, LMH, and Emb
7BLLaMAtargetmodel). Thedashedbarsrepresents stand forregularlayer, LM heads, andthe embedding
the full speed potentials of the EAGLE model with- layer, respectively. Skipping the middle layers sym-
out memory restrictions. However, when constrained metricallyhasshownbetteracceptanceratesingeneral.
witha VRAM limitof16 GiB, the quantizedEAGLE Notethatwedistinguishembeddinglayerandlm_head
model(indicatedbyredbars)suffersfromseverespeed herealthoughinpracticetheymayhavetiedweights.
degradation, highlightingthe significant overheadsas-
sociatedwithquantization.
5.3 OptimalHyper-Parameters
We first study the optimal layer skipping choices.
Admittedly, the LLaMA-based S3D model ties To this end, we have empirically explored three
closely to EAGLE and underperforms CLLM different schemes by skipping a fixed number of
in overall M speed, primarily due to the high original layers of LLaMA 7B target model: (1)
speedupsandoptimalmemoryefficiencyachieved Skipping asymmetrically from the middle, includ-
byEAGLEandCLLM,respectively. However, as ing early existing and using only late layers while
shown in Table 1 and 2, we are able to exploit skipping all early layers. (2) Skipping symmetric
ourmemoryefficiencyandoutperform EAGLEin layers from the middle, i.e., layer 5 → 26 or skip-
both efficiency and effectiveness while using less ping the middle 20 layers. (3) Alternate evenly
amountofVRAMbyswitchingtoanon-quantized betweenskipping andnon-skipping layers.
Phi-3target model. 3 Evenwithoutswitching toa LookingatFigure4(andAppendixFigure9for
differenttargetmodel,wedemonstrateinFigure3 training efficiencies), skipping symmetricallayers
that ourLLaMA-based S3D model canoperate in from the middle performs best, achieving higher
half-precision within a VRAM limit of 16 GiB, overall acceptance rates and optimal training effi-
and outperform EAGLE by up to 3.9 times when ciency. In contrast, skipping from layer 11 to the
EAGLE needs to be quantized. This underscores top (LM-head) layer and skipping from the bot-
thecritical importanceofmemoryefficiency. tom (embedding) layer to layer 20 have the worst
performance, highlighting the importance of both
On the other hand, we find that the training ob-
early and late layers. Lastly, skipping symmetric
jectives of CLLM may encourage repeating pat-
middlelayersoralternatingevery3layershassim-
terns in its outputs, leading to degraded effective-
ilartraining efficiencies.
nessscores, asseeninTable1andadditional case
To answer what is the optimal number of lay-
studies inAppendix D.Incontrast, ourmodelcan
ers and what is the best number of tokens to be
preserve effectiveness scores more robustly while
predicted in parallel, wetrain different number of
achievingtheoptimalspeed-memory ratios.
layers (β) skipped symmetrically from the mid-
dle layer, each trained model is evaluated for dif-
ferent γ values up to 5. We run different mod-
3We conducted ablations in Appendix C to understand els on MT-Bench for 50 samples, and linearly
thesignificantimprovementsinPhi-3S3D.Foralarger13B
interpolate the acceptance rate discount function
model,ourcost-effectivenesscanbemaintainedaswell(see
AppendixTable3). α k(β) (detailed in Appendix E). As summarized
7
.ces
rep
snekoT
pudeepS
e
ar
ecna
peccA1.00
0.75 11
EAGLE (ex ra 1 layer)
0.50
10 Ours (layer 11 → lm_head)
0.25 predicted Ours (layer 5 → 26)
0.00 actual 9 Ours (embed_layer → 20)
0.0 0.2 0.4 0.6 0.8 1.0 8
7
γ=1
1.8 γ=2 6
γ=3
γ=4 5
1.6 γ=5
4
1.4 3
0 50 100 150 200 250 300
Step
1.2
Figure6:TraininglosscomparisonsbetweenEAGLE’s
1.0 classificationloss(Lietal.,2024)andour(S3D)train-
ing loss in Eq. 9. EAGLE requires training an extra
0.0 0.2 0.4 0.6 0.8 1.0
β layerofTransformerwithadditionallinearmappings.
Figure 5: Upper: The predicted (in dashes) and sam-
pledacceptancerates(interpolatedorangedots)ofvar-
ing method used in S3D inherently lowers train-
ious draft modelsizes (β). Lower: The predicted (in
ing costs, as the training task leverages the exist-
curves)andsampled(indots)speedsofdifferentdraft
ing model weights. In light of this, we compare
modelsizes anddifferentnumberof guesses(γ). Our
predictioncurvesjustifytheoptimalityofusingaround and train both the S3D and EAGLE models us-
halfthenumberofparametersandγ = 4,asobserved ing 20,000 data rows (the original EAGLE was
individually and respectively in Zhang et al. (2023a) trainedon68,000datarows).
andGloeckleetal.(2024). AsshowninFigure6,S3Dconsistentlyexhibits
lowertraininglosses,evenwhenconsideringmore
layers and including far-future tokens, which are
in Figure 5, our proposed formula for predicting
generally challenging to predict. In the worst-
self-speculative acceptance rates in Eq. 10 mostly
performingcase,whereearlyexitingorearlylayer
matches with the empirical results except for the
skipping is used, our loss values remain mostly
lowest β value, and this outlier may be explained
below those of EAGLE. Additionally, S3D train-
by the less predictability in training a small par-
ing loss shows less variance and converges more
tialmodel(< 1.5B).Addtionally, thespeedup for-
steadily.
mulainEq.12successfullypredictsboththetrend
and the sweet spot in speedups. Higher γ values
6 Conclusion
align less with the prediction because the accep-
tanceratesforfar-futuretokenshavehigheruncer- We have proposed S3D, a self-speculative SD
taintyandvariance asreflectedbyFigure4. methodbasedonsimultaneousmulti-tokenpredic-
In addition to the findings from Zhang et al. tions and mid-layer skipping. S3D demonstrates
(2023a); Gloeckle et al. (2024), we have unified one of the best cost-effectiveness among recent
multi-token predictions with layer skipping. Our openSDsystems,whilealsoexhibitinghightrain-
prediction in Eq. 12 has also justified their find- ingefficiencyandmaintainingtheeffectivenessof
ingsthattheoptimalspeedisachievedbyskipping the original model. We have also verified the op-
around half of the layers (β → 0.5) and the opti- timal hyper-parameters for our proposed method
malγ is4(asshowninFigure5,ahigherγresults in a principled manner, without requiring any
in an almost diminished speedup, offset by fewer black-box optimizations beforehand. By leverag-
acceptedtokens). ing memory efficiency, S3D can avoid quantiza-
tion and surpass the speed of quantized EAGLE
5.4 TrainingEfficiency whena16GiBVRAMlimitisimposed. Addition-
ally,S3D,basedonthesmallerPhi-3targetmodel,
In addition to its cost-effectiveness, S3D also
decodes 1.4 to 2 times faster than quantized EA-
demonstrates greatertrainingefficiencycompared
GLE on A10G, with reduced VRAM usage and
to other effective SD models (see Figure 1). So
bettereffectiveness.
we hypothesize that the self-speculative decod-
8
setar
ecnatpeccA
deepS
ssoLAcknowledgments Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu
Peng, Jason D. Lee, Deming Chen, and Tri Dao.
We extend our gratitude to Touqir Sajed for 2024. Medusa: SimpleLLMinferenceacceleration
his suggestions and brainstorming throughout the frameworkwithmultipledecodingheads. Preprint,
arXiv:2401.10774.
preparation of this paper. Additionally, we thank
Kevin Ferreira, Yipeng (Penny) Ji, and Paria Ne- Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,
jatfortheirsupportinarranging computational re- Jean-Baptiste Lespiau, Laurent Sifre, and John
sources. Jumper. 2023. Accelerating large language model
decoding with speculative sampling. Preprint,
arXiv:2302.01318.
References
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
NguyenBach,AmitBahree,ArashBakhtiari,Harki-
Krueger,MichaelPetrov,HeidyKhlaaf, GirishSas-
rat Behl, Alon Benhaim, Misha Bilenko, Jo-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
han Bjorck, Sébastien Bubeck, Martin Cai, Caio
NickRyder,MikhailPavlov,AletheaPower,Lukasz
César Teodoro Mendes, Weizhu Chen, Vishrav
Kaiser, Mohammad Bavarian, Clemens Winter,
Chaudhary, Parul Chopra, Allie Del Giorno, Gus-
Philippe Tillet, Felipe Petroski Such, Dave Cum-
tavo de Rosa, Matthew Dixon, Ronen Eldan,
mings, Matthias Plappert, Fotios Chantzis, Eliza-
Dan Iter, Amit Garg, Abhishek Goswami, Suriya
beth Barnes, Ariel Herbert-Voss, William Hebgen
Gunasekar, Emman Haider, Junheng Hao, Rus-
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
sell J. Hewett, Jamie Huynh, Mojan Javaheripi,
Tang,IgorBabuschkin,SuchirBalaji,ShantanuJain,
Xin Jin, Piero Kauffmann, Nikos Karampatziakis,
William Saunders, Christopher Hesse, Andrew N.
Dongwoo Kim, Mahoud Khademi, Lev Kurilenko,
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen
Morikawa, Alec Radford, Matthew Knight, Miles
Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush
Brundage, Mira Murati, Katie Mayer, Peter Welin-
Madan,ArindamMitra,HardikModi,AnhNguyen,
der, Bob McGrew, Dario Amodei, Sam McCan-
BrandonNorick,BarunPatra,DanielPerez-Becker,
dlish, Ilya Sutskever, andWojciech Zaremba.2021.
Thomas Portet, Reid Pryzant, Heyang Qin, Marko
Evaluating large language models trained on code.
Radmilac, Corby Rosset, Sambudha Roy, Olatunji
Preprint,arXiv:2107.03374.
Ruwase, Olli Saarikivi, Amin Saied, Adil Salim,
Michael Santacroce, Shital Shah, Ning Shang,
Zhuoming Chen, Avner May, Ruslan Svirschevski,
Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin
Yuhsun Huang, Max Ryabinin, Zhihao Jia, and
Wang,RachelWard, GuanhuaWang,PhilippWitte,
Beidi Chen. 2024. Sequoia: Scalable, robust, and
MichaelWyatt, Can Xu, JiahangXu, SonaliYadav,
hardware-aware speculative decoding. Preprint,
Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong
arXiv:2402.12374.
Zhang,CyrilZhang,JianwenZhang,LiLynaZhang,
Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren TriDao.2023. Flashattention-2: Faster attentionwith
Zhou.2024. Phi-3technicalreport: A highlycapa- better parallelism and work partitioning. Preprint,
blelanguagemodellocallyonyourphone. Preprint, arXiv:2307.08691.
arXiv:2404.14219.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Zachary Ankner, Rishab Parthasarathy, Aniruddha JakobUszkoreit, andŁukasz Kaiser. 2019. Univer-
Nrusimha, Christopher Rinard, Jonathan Ragan- saltransformers. Preprint,arXiv:1807.03819.
Kelley, and William Brandon. 2024. Hydra: Se-
quentially-dependentdraftheadsformedusadecod- Tim Dettmers, Mike Lewis, Younes Belkada, and
ing. Preprint,arXiv:2402.05109. Luke Zettlemoyer. 2022. LLM.int8(): 8-bit matrix
multiplication for transformers at scale. Preprint,
Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. arXiv:2208.07339.
2017. Structuredpruningofdeepconvolutionalneu-
ralnetworks. JETC. Mostafa Elhoushi, Akshat Shrivastava, Diana
Liskovich, Basil Hosmer, Bram Wasti, Liangzhen
Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se- Lai,AnasMahmoud,BilgeAcun,SaurabhAgarwal,
Young Yun. 2023. Fast and robust early-exiting Ahmed Roman, Ahmed A Aly, Beidi Chen, and
frameworkforautoregressivelanguagemodelswith Carole-Jean Wu. 2024. LayerSkip: Enabling
synchronizedparalleldecoding. InEMNLP. early exit inference and self-speculative decoding.
Preprint,arXiv:2404.16710.
Nikhil Bhendawade, Irina Belousova, Qichen Fu,
Henry Mason, Mohammad Rastegari, and Mah- AngelaFan,EdouardGrave,andArmandJoulin.2019.
yar Najibi. 2024. Speculative streaming: Fast Reducing transformer depth on demand with struc-
LLM inferencewithoutauxiliarymodels. Preprint, tureddropout. Preprint,arXiv:1909.11556.
arXiv:2402.11131.
9Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023.
2024. Break the sequential dependency of LLM LLM-Pruner:Onthestructuralpruningoflargelan-
inference using lookahead decoding. Preprint, guagemodels. Preprint,arXiv:2305.11627.
arXiv:2402.02057.
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xin-
FabianGloeckle,BadrYoubiIdrissi,BaptisteRozière, hao Cheng, Zeyu Wang, Zhengxin Zhang, Rae
DavidLopez-Paz,andGabrielSynnaeve.2024. Bet- Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang
ter & faster large language models via multi-token Shi, ChunanShi,ZhuomingChen,DaiyaanArfeen,
prediction. Preprint,arXiv:2404.19737. Reyna Abhyankar, and Zhihao Jia. 2024. SpecIn-
fer:Acceleratinglargelanguagemodelservingwith
KarlMoritzHermann,TomášKocˇiský,EdwardGrefen- tree-basedspeculativeinferenceandverification. In
stette,LasseEspeholt,WillKay,MustafaSuleyman, ASPLOS.
andPhilBlunsom.2015. Teachingmachinestoread
andcomprehend. Preprint,arXiv:1506.03340. Tycho Ouderaa, Markus Nagel, Mart van Baalen,
YukiM.Asano,andTijmenBlankevoort.2024. The
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan LLMsurgeon. Preprint,arXiv:2312.17244.
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank AlecRadford,KarthikNarasimhan,TimSalimans,Ilya
adaptation of large language models. Preprint, Sutskever, et al. 2018. Improving language under-
arXiv:2106.09685. standingbygenerativepre-training.
Siqi Kou, LanxiangHu, ZhezhiHe, Zhijie Deng, and Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
Hao Zhang. 2024. CLLMs: Consistency large lan- and Yuxiong He. 2020. Zero: Memory opti-
guagemodels. mizationstowardtrainingtrillionparametermodels.
Preprint,arXiv:1910.02054.
François Lagunas, Ella Charlaix, Victor Sanh, and
AlexanderM.Rush.2021. Blockpruningforfaster David Raposo, Sam Ritter, Blake Richards, Timothy
transformers. Preprint,arXiv:2109.04838. Lillicrap,PeterConwayHumphreys,andAdamSan-
toro.2024. Mixture-of-depths:Dynamicallyallocat-
Yaniv Leviathan, Matan Kalman, and Yossi Matias. ingcomputein transformer-basedlanguagemodels.
2023. Fast inference from transformers via specu- Preprint,arXiv:2404.02258.
lativedecoding. Preprint,arXiv:2211.17192.
Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and
Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Preslav Nakov. 2022. On the effect of dropping
Zhang.2024. Eagle: Speculativesamplingrequires layers of pre-trained transformermodels. Preprint,
rethinkingfeatureuncertainty. InICML. arXiv:2004.03844.
Chin-Yew Lin. 2004. ROUGE: A package for auto- Andrea Santilli, Silvio Severino, Emilian Postolache,
maticevaluationofsummaries. ACL. Valentino Maiorca, Michele Mancusi, Riccardo
Marin, and Emanuele Rodola. 2023. Accelerating
YujunLin, HaotianTang, ShangYang, ZhekaiZhang,
transformerinferencefortranslationviaparallelde-
Guangxuan Xiao, Chuang Gan, and Song Han.
coding. InACL.
2024. QServe: W4A8KV4 quantization and sys-
tem co-design for efficient LLM serving. Preprint, Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
arXiv:2405.04532. Sutskever. 2023. Consistency models. Preprint,
arXiv:2303.01469.
Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng
Ni, Kai Han, and Yunhe Wang. 2024a. Kangaroo: Yang Song, Chenlin Meng, Renjie Liao, and Ste-
Lossless self-speculativedecodingvia doubleearly fano Ermon. 2021. Accelerating feedforwardcom-
exiting. Preprint,arXiv:2404.18911. putation via parallel nonlinear equation solving.
Preprint,arXiv:2002.03629.
James Liu, Guangxuan Xiao, Kai Li, Jason D. Lee,
SongHan,TriDao,andTianleCai.2024b. Bitdelta: Benjamin Spector and Chris Re. 2023. Accelerating
Yourfine-tunemayonlybeworthonebit. Preprint, LLM inference with staged speculative decoding.
arXiv:2402.10193. Preprint,arXiv:2308.04623.
Ilya Loshchilov and Frank Hutter. 2019. De- Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit.
coupled weight decay regularization. Preprint, 2018. Blockwiseparalleldecodingfordeepautore-
arXiv:1711.05101. gressivemodels. Preprint,arXiv:1811.03115.
Christos Louizos, Max Welling, and Diederik P. Xin Sun, Tao Ge, Furu Wei, and Houfeng Wang.
Kingma. 2018. Learning sparse neural net- 2021. Instantaneous grammatical error correction
works through l0 regularization. Preprint, withshallowaggressivedecoding. InACL.
arXiv:1712.01312.
10Gemini Team, Rohan Anil, Sebastian Borgeaud, Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun
Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Chen. 2024. Multi-candidate speculative decoding.
Radu Soricut, Johan Schalkwyk, Andrew M Dai, Preprint,arXiv:2401.06706.
AnjaHauth,etal.2023. Gemini:Afamilyofhighly
capablemultimodalmodels. ChenZhang,ZhuoruiLiu,andDaweiSong.2024. Be-
yond the speculative game: A survey of specula-
Surat Teerapittayanon, Bradley McDanel, and H. T. tive execution in large language models. Preprint,
Kung. 2017. BranchyNet: Fast inference via arXiv:2404.14897.
early exiting from deep neural networks. Preprint,
arXiv:1709.01686. JunZhang,JueWang,HuanLi,LidanShou,KeChen,
Gang Chen, and Sharad Mehrotra. 2023a. Draft
Hugo Touvron, Louis Martin, Kevin Stone, Peter & Verify: Lossless large language model accel-
Albert, Amjad Almahairi, Yasmine Babaei, Niko- eration via self-speculative decoding. Preprint,
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, arXiv:2309.08168.
Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull, Qingru Zhang, Minshuo Chen, Alexander Bukharin,
David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Nikos Karampatziakis, Pengcheng He, Yu Cheng,
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Weizhu Chen, and Tuo Zhao. 2023b. AdaLoRA:
Naman Goyal, Anthony Hartshorn, Saghar Hos- Adaptive budget allocation for parameter-efficient
seini, RuiHou,HakanInan,MarcinKardas,Viktor fine-tuning. Preprint,arXiv:2303.10512.
Kerkez, Madian Khabsa, Isabel Kloumann, Artem
Weilin Zhao,YuxiangHuang,Xu Han,ChaojunXiao,
Korenev, PunitSingh Koura, Marie-AnneLachaux,
ZhiyuanLiu, and Maosong Sun. 2024. Ouroboros:
ThibautLavril,JenyaLee,DianaLiskovich,Yinghai
Speculative decoding with large model enhanced
Lu, YuningMao,XavierMartinet,TodorMihaylov,
drafting. Preprint,arXiv:2402.13720.
PushkarMishra, IgorMolybog,YixinNie, Andrew
Poulton,JeremyReizenstein,RashiRungta,Kalyan
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Saladi, Alan Schelten, Ruan Silva, Eric Michael
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Smith, Ranjan Subramanian, Xiaoqing Ellen Tan,
ZhuohanLi, DachengLi, Eric P. Xing, Hao Zhang,
Binh Tang, Ross Taylor, Adina Williams, Jian Xi-
JosephE.Gonzalez,andIonStoica.2023. Judging
ang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,
LLM-as-a-judgewithMT-BenchandChatbotArena.
Yuchen Zhang, Angela Fan, Melanie Kambadur,
Preprint,arXiv:2306.05685.
SharanNarang,AurelienRodriguez,RobertStojnic,
SergeyEdunov,andThomasScialom.2023. Llama
2: Open foundation and fine-tuned chat models.
Preprint,arXiv:2307.09288.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi, Pier-
ric Cistac, Tim Rault, Rémi Louf, MorganFuntow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Huggingface’stransformers:State-of-the-artnatural
languageprocessing. Preprint,arXiv:1910.03771.
HaoyiWuandKeweiTu.2024. Layer-CondensedKV
Cacheforefficientinferenceoflargelanguagemod-
els. Preprint,arXiv:2405.10637.
HemingXia,TaoGe,PeiyiWang,Si-QingChen,Furu
Wei, and ZhifangSui. 2023. Speculative decoding:
Exploiting speculative execution for accelerating
seq2seqgeneration. Preprint,arXiv:2203.16487.
Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang,
YongqiLi,TaoGe,TianyuLiu,WenjieLi,andZhi-
fang Sui. 2024. Unlocking efficiency in large lan-
guagemodelinference: Acomprehensivesurveyof
speculativedecoding. Preprint,arXiv:2401.07851.
MengzhouXia,ZexuanZhong,andDanqiChen.2022.
Structured pruning learns compact and accurate
models. InACL.
11Appendix From Table 4, it is observed that avoiding
quantizationgenerallyonlyenhancesspeed,while
A Training Configurations
thesignificant improvements incost-effectiveness
We train 7B models with a per-device batch stem from two main factors: 1) transitioning to
size of 2, a max sequence length of 2048, and Phi-3 Mini as the target model, and 2) imple-
acc_grad=8 with 4 GPUs. For 13B models, we menting the S3D method. However, the latter is
use a per-device batch size of 1, a max sequence crucial for surpassing the state-of-the-art SD in
lengthof1024,andacc_grad=8with8GPUspar- both speedups and cost-effectiveness under lim-
allelizedinZero-3usingDeepSpeedwithparame- itedmemory.
teroffloading(Rajbhandari etal.,2020).
D IssuesofCLLMs
We uniformly apply a learning rate of 3·10−5
with50linearwarm-upstepsandafusedAdamW Similar to our method, CLLMs(Kou et al., 2024)
(Loshchilov and Hutter, 2019) kernel from Py- incurs no additional memory costs to the original
Torch for optimization. During our multi-token model. It iterates and verifies multiple tokens in
prediction training, we sample 20,000 data rows parallel using Jacobi iterations with random ini-
andmaskout15%tokens randomly. tial guess tokens. Consequently, in Table 1 and 2,
CLLM has achieved higher speed-memory ratios
B S3D using LoRA
comparable toours.
UsinganS3Dmodelfine-tuned withadapters can However, we have discovered that the CLLM
easily guarantee the same output as the original model we have evaluated is prone to generating
target model, thus maintaining the original model repetitive patterns, presumably due to its train-
quality. With this intention, we have trained S3D ing objectives, which encourage the target model
models using LoRA (Hu et al., 2021) with the to shorten the Jacobi trajectories before reaching
sameamountofdataasfullfine-tuning. the fixed point. While this can accelerate infer-
We apply LoRA to the optimal layer skipping ence convergence and mitigate the issue of rela-
schemes (5 → 26 and 7 → 24) and vary ma- tively slow speedups from vanilla Jacobi decod-
trix ranks r from 12 to 128, and α from 32 to ing,wehaveobservedrelatively loweffectiveness
128. However, as shown by Figure 8, the speed scores from the officially trained 7B model using
of a LoRA-based S3D model is notably penal- ShareGPT data (shown in Table 1). In contrast,
ized by the overheads from the attached linear S3D preserves most of the effectiveness of the
adapters, and the acceptance rate underperforms original model.
comparedtothatofafullyfine-tunedmodel. Asa In Figure 7, we present a comparison between
result, weobserve inferior speed whenusing S3D two example outputs of CLLM. The one with
with LoRA adapters. We have also experimented repetitive patterns can generate tokens 2x faster
AdaLoRA(Zhangetal.,2023b)buthaveobserved than its counterpart, raising questions about the
similarnegativeresultsforend-to-end speeds. speed optimization of CLLM in terms of preserv-
Nevertheless, we have seen that the BitDelta ingtheeffectiveness oftheoriginalmodel.
method (Liu et al., 2024b) successfully com-
E Prediction FormulaDetails
pressed fine-tuned adapter weights by more than
10x. However, BitDelta requires large training re- Our prediction for both acceptance rates and
sources andneeds toloadthree models simultane- speeds shown in Fig 5 can be captured by only a
ously. Despite this, we still see great potential to few hyper-parameters. For Eq. 10, we use U =
achievesimilarefficiencytoafullyfine-tunedS3D 0.01. For SD iteration speed (Eq. 12), we set
model with guaranteed original model outputs us- δ = 0.04. Forthediscount function ofthek-thto-
ingadapters, butweleavethistofuturework. ken acceptance rate, we use a linear interpolation
ofreal data from experiments shown inFigure10.
C Phi-3 S3D AblationStudy
Specifically,
To comprehend the high cost-effectiveness of the
k
Phi-3 based S3D model, we conducted an abla- σ (β) = (1− )·σ(β) (14)
k
4
tion study, dissecting the contributions of the tar-
getmodel,quantization, andtheS3Dmethod.
12Table 2: The cost-effectivenesscomparisonson a low-endRTX 3060 GPU considering peak VRAM costs. All
modelsare8-bitquantizedandarebasedonthe7BLLaMA-v2targetmodelexceptmentionedotherwiseinparen-
theses. The largest three numbers in each column are highlighted in italics or bold. “Peak” denotes the peak
VRAMusageinGiB.EffectivenessmetricsareomittedhereastheymirrorthoseinTable1.
MT-Bench Human-Eval CNN-Daily Overall
Model\Metric
Peak↓ Tok/s M Peak↓ Tok/s M Peak↓ Tok/s M M
Baseline 8.42 8.11 1.00 8.35 7.82 1.00 8.90 7.42 1.00 1.00
Self-Spec. 7.81 5.71 0.76 7.46 5.46 0.78 8.08 5.68 0.84 0.79
Medusa(Vicuna) 9.09 10.32 1.18 8.94 11.97 1.43 9.36 7.99 1.02 1.21
EAGLE 9.59 15.18 1.64 9.45 17.39 1.97 9.86 13.82 1.68 1.76
MCSD 7.78 8.51 1.14 7.42 8.46 1.22 8.07 7.01 1.04 1.13
Sequoia 8.44 10.29 1.27 8.35 10.48 1.34 8.57 9.01 1.26 1.29
Ouroboros 7.94 6.28 0.82 7.60 6.79 0.95 8.23 5.62 0.82 0.86
CLLM 7.47 14.14 1.97 7.39 19.19 2.77 7.53 9.28 1.48 2.07
Ours
S3D 7.81 13.99 1.86 7.58 14.99 2.11 8.72 10.41 1.43 1.80
S3D(Phi-3,fp16) 8.15 29.88 3.23 7.92 33.70 3.18 8.81 19.49 2.65 3.23
Table 3: The cost-effectivenesscomparison consideringpeak VRAM costs for selected models using 13B 8-bit
quantizedLLaMAasthetargetmodel(onA10G).Thelargestnumberineachcolumnarehighlightedinitalicsor
bold. “Peak”denotesthepeakVRAMusageinGiB.Theoverall(averaged)resultscountforbothM speedand
therelativeeffectivenessmetricscomparedtothebaseline.
MT-Bench Human-Eval CNN-Daily Overall
Model\Metric
Peak↓ Tok/s M Score Peak↓ Tok/s M Acc.% Peak↓ Tok/s M R-1 R-L M Eff.
Baseline(13B) 15.39 5.60 1.00 7.36 15.03 5.51 1.00 6.10 15.88 5.09 1.00 0.21 0.15 1.00 1.00
EAGLE(13B) 16.96 10.67 1.73 7.40 16.82 12.58 2.04 4.27 17.29 9.82 1.77 0.21 0.15 1.85 0.90
S3D(13B) 14.39 10.70 2.04 6.18 14.08 12.23 2.37 6.71 15.80 7.87 1.55 0.25 0.17 1.99 1.04
Table 4: Ablations on the cost-effectiveness of S3D models based on Phi-3 Mini (on A10G), considering peak
VRAM costs. The largest two numbers in each column are highlighted in italics or bold. “Peak” denotes the
peak VRAM usage in GiB. Although using non-quantized fp16 generally improves model speeds, it does not
neccessarilyaddcost-effectivenessduetotheadditionalmemoryoverheads.
MT-Bench Human-Eval CNN-Daily Overall
Models\Metrics
Peak↓ Tok/s M Peak↓ Tok/s M Peak↓ Tok/s M M
LLaMA(8-bit) 8.53 7.02 1.00 8.43 7.02 1.00 8.96 6.53 1.00 1.00
Phi-3(8-bit) 4.64 8.91 2.33 3.78 8.82 2.80 5.49 7.99 2.00 2.38
Phi-3(fp16) 7.99 16.43 2.50 8.22 15.97 2.33 8.80 13.59 2.12 2.32
Phi-3(S3D,8-bit) 4.79 14.39 3.65 5.11 26.71 6.28 5.53 11.11 2.76 4.23
Phi-3(S3D,fp16) 8.19 26.00 3.86 8.56 48.23 6.77 8.88 16.95 2.62 4.42
13USER:tellmeafewinteresting factsaboutthesunandthemoon.
ASSISTANT:1. Thesunisapproximately4.6millionmilesawayfromEarth,whilethemoon
is about 238,900 miles away. 2. The sun is about 864,000 miles in diameter, while the
moonisonlyabout2,159milesindiameter. 3. Thesunismadeupofhydrogen andhelium,
while themoonisprimarily composed ofsilicon and iron. 4. Thesunisaspherical object,
whilethemoonhasaslightly irregularshape duetoitsformation process. 5. Thesuntakes
approximately 24 days to complete one rotation, while the moon takes approximately 27.3
days to complete one rotation around Earth. 6. The sun is approximately4.6 billion years
old, while the moon is estimated to be around4.5 billion years old. 7. The sun’s surface
temperature is approximately 5,500°F (3,000°C), while the moon’s surface temperature
rangesfrom-273°F(-170°C)duringthenightto260°F(127°C)duringtheday. 8. Thesun
isastar,whilethemoonisanaturalsatelliteofEarth. 9. Thesunisthesourceofallenergy
onEarth,whilethemoonhasnointrinsicenergysource. 10. Thesunisthelargestobjectin
thesolarsystem,whilethemoonisthelargestnaturalsatellite inthesolarsystem.
USER:tellmeafewinteresting factsaboutthesunandthemoon.
ASSISTANT: 1. The sun is a gigantic ball of plasma, while the moonis a rocky body. 2.
Thesun isapproximately 4.6billion years old, whilethemoon isapproximately 4.5billion
yearsold. 3. The sun is approximately 109 times larger thanthe moon, and the moon is
approximately 300 timessmaller than thesun. 4. Thesun isapproximately 100 timesmore
massivethanthemoon. 5. Thesunisapproximately93millionmiles(150millionkilometers)
awayfromEarth,whilethemoonisapproximately238,900miles(384,400kilometers)away.
6. Thesunisapproximately10,000degreesFahrenheit(5,538degreesCelsius)atitssurface,
while the moon’s surface temperature rangesfrom -273 to -233 degrees Fahrenheit (120 to
-150 degrees Celsius). 7. Thesun is approximately 864,000 miles(1,392,000 kilometers) in
diameter, whilethemoonisapproximately 2160miles(3474kilometers) indiameter. 8. The
sunisapproximately 109timesmoreluminous thanthemoon. 9. Thesunisapproximately
10,000timesmoremassivethanthemoon. 10. Thesunisapproximately 10,000 timesmore
luminous thanthe moon. 11. Thesunisapproximately 10,000 timesmoremassive than the
moon. 12. Thesunisapproximately10,000timesmoreluminousthanthemoon. 13. Thesun
is approximately 10,000times more massive than the moon. 14. The sun is approximately
10,000 times more luminous than the moon. 15. The sun is approximately 10,000 times
moremassive than the moon. 16. The sun is approximately 10,000 times more luminous
than the moon. 17. The sun is approximately 10,000 times more massive thanthe moon.
18. The sun is approximately 10,000 times more luminous than the moon. 19. Thesun is
approximately 10,000 times more massive than the moon. 20. The sun is approximately
10,000times moreluminousthanthemoon. ...(repetitivepatterncontinues)
Figure 7: Examples of CLLM outputs (on a RTX 3060 GPU with 8-bit quantization). The implementation is
non-deterministicduetopotentialnumericalerrors. Asaresult,weareabletoshowtwodifferentinputsfromthe
same prompt. Upper: A goodexamplewhichhas a speed of 10.85tokensper second. Lower: A bad example
whichshowsrepetitivepatternsattheend,havinga2xhigherspeedof21.98tokenspersecond.
14Full Fine-Tuning LoRA
1.0 15 1.0 15
Acce tance rate
14 S eed 14
0.9 0.9
13 13
0.8 12 0.8 12
0.7 11 0.7 11
10 10
0.6 0.6
9 9
0.5 8 0.5 8
full full all all all all vko vko vko
fine- fine- r=12 r=64 r=128 r=64 r=64 r=128 r=64
tuning tuning α=32 α=64 α=128 α=64 α=64 α=128 α=64
5→26 7→24 5→26 5→26 5→26 7→24 5→26 5→26 7→24
Figure8:S3Dfullfine-tuningcomparedwithusingLoRAadapters(MT-Bench).LoRAsettingsincludeattaching
adapterstoalllinearlayers(all)oronlyattachingtovalue,key,andoutputprojectionlayers(vko).Giventhesame
layer skipping scheme, vko and low-rank LoRA have lower inference overheads but achieve lower acceptance
rates(atdepth-0). Overall,LoRAdoesnotoffersimilarspeedscomparedtofullfine-tuning,althoughitreliably
maintainstheoriginalmodeloutput.
11
layer 11 → lm_head
10 layer 10 → 31
layer 9 → 30
9 layer 7 → 28
layer 5 → 26
8 every 3 layers
layer 3 → 24
7 layer 1 → 22
layer 0 → 21
6 embed_layer → 20
5
4
3
0 50 100 150 200 250 300 350 400
Step
Figure9:Thetraininglosswhenfine-tuningdifferentskippingschemes.Wealtertheskippedlayerswhilekeeping
thetotalnumberofusedlayers(12layersorβ =0.375)unchanged.
mt-bench human-eval cnn-daily
EAGLE
0.8
0.8 0.8 Ours
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0 1 2 3 0 1 2 3 0 1 2 3
Depth Depth Depth
Figure10:Acceptanceratescomparisonatdifferentdrafttokentreedepths,comparedbetweenours(S3D)andthe
state-of-the-artopen-sourcemodelEAGLE in differentdatasets. Because we predicttokenssimultaneously,our
acceptanceratesdropmoreatfuturepositions. However,weachievesimilaracceptanceratesatthedraftrootand
areabletooutperformEAGLEcosteffectivelywhileusingmuchlesstrainingdata.
15
etar
ecnatpeccA
etar
ecnatpeccA
ssoL
etar
ecnatpeccA
etar
ecnatpeccA
dee
S