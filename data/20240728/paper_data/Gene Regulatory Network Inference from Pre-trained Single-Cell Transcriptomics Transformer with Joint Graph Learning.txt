Gene Regulatory Network Inference from Pre-trained Single-Cell
Transcriptomics Transformer with Joint Graph Learning
SindhuraKommu1 YizhiWang2 YueWang2 XuanWang1
Abstract One such promising application is the inference of gene
Inferringgeneregulatorynetworks(GRNs)from regulatorynetworks(GRNs)whichrepresentthecomplex
single-cellRNAsequencing(scRNA-seq)datais interplaybetweentranscriptionfactors(TFs)andtheirdown-
acomplexchallengethatrequirescapturingthe streamtargetgenes(Akers&Murali,2021;Cramer,2019).
intricate relationships between genes and their ApreciseunderstandingofGRNsiscrucialforunderstand-
regulatoryinteractions. Inthisstudy, wetackle ingcellularprocesses,molecularfunctions,andultimately,
thischallengebyleveragingthesingle-cellBERT- developingeffectivetherapeuticinterventions(Biswasetal.,
basedpre-trainedtransformermodel(scBERT), 2021).
trainedonextensiveunlabeledscRNA-seqdata,
However,inferringGRNsfromscRNA-seqdataischalleng-
toaugmentstructuredbiologicalknowledgefrom
ingduetocellheterogeneity(Wagneretal.,2016),cellcycle
existingGRNs. Weintroduceanoveljointgraph
effects(Buettneretal.,2015),andhighsparsitycausedby
learningapproachscTransNetthatcombinesthe
dropoutevents(Kharchenkoetal.,2014),whichcanimpact
rich contextual representations learned by pre-
accuracyandrobustness. Additionally,theavailabilityof
trainedsingle-celllanguagemodelswiththestruc-
labeledscRNA-seqdatacorrespondingtoaGRNislimited,
turedknowledgeencodedinGRNsusinggraph
making it challenging to train models from scratch. Tra-
neural networks (GNNs). By integrating these
ditionalunsupervisedorself-supervisedmodels,whilenot
twomodalities,ourapproacheffectivelyreasons
reliant on label information, often struggle to effectively
over both the gene expression level constraints
handlethenoise,dropouts,highsparsity,andhighdimen-
provided by the scRNA-seq data and the struc-
sionalitycharacteristicsofscRNA-seqdata(Moermanetal.,
tured biological knowledge inherent in GRNs.
2019; Matsumoto et al., 2017; Zeng et al., 2023). Super-
We evaluate scTransNet on human cell bench-
visedmethodsarealsoproposedforGRNreconstruction
markdatasetsfromtheBEELINEstudywithcell
(Zhaoetal.,2022;Shuetal.,2022;KCetal.,2019;Chen&
type-specificgroundtruthnetworks. Theresults
Liu,2022a)butstruggletohandlebatcheffectsandfailto
demonstrate superior performance over current
leveragelatentgene-geneinteractioninformationeffectively
state-of-the-artbaselines,offeringadeeperunder-
limitingtheirgeneralizationcapabilities.
standingofcellularregulatorymechanisms.
Recentadvancementsinlargelanguagemodels(LLMs)and
thepre-trainingfollowedbyfine-tuningparadigm(Devlin
1.Introduction etal.,2019;OpenAI,2023)havesignificantlycontributedto
thedevelopmentoftransformer-basedarchitecturestailored
Single-cellRNAsequencing(scRNA-seq)hastransformed
forscRNA-seqdataanalysis(Yangetal.,2022;Cuietal.,
theexplorationofgeneexpressionpatternsattheindividual
2024;Chenetal.,2023;Theodorisetal.,2023). Thesemod-
celllevel(Jovicetal.,2022),offeringanunprecedentedop-
elseffectivelyleveragevastamountsofunlabeledscRNA-
portunitytounraveltheintricateregulatorymechanismsgov-
seq data to learn contextual representations and capture
erningcellularidentityandfunction(Pratapaetal.,2020).
intricatelatentinteractionsbetweengenes. Toaddressthe
limitationsofthecurrentmethods,weeffectivelyleverage
1DepartmentofComputerScience, VirginiaPolytechnicIn-
stitute and State University, Blacksburg, VA, United States one of these large-scale pre-trained transformer models,
2Department of Electrical and Computer Engineering, Vir- namely scBERT (Yang et al., 2022), which has been pre-
giniaPolytechnicInstituteandStateUniversity,Arlington,VA, trainedonlarge-scaleunlabelledscRNA-seqdatatolearn
United States. Correspondence to: Sindhura Kommu <sind-
domain-irrelevantgeneexpressionpatternsandinteractions
hura@vt.edu>,YizhiWang<yzwang@vt.edu>,YueWang<yue-
fromthewholegenomeexpression. Byfine-tuningscBERT
wang@vt.edu>,XuanWang<xuanw@vt.edu>.
onuserspecificscRNA-seqdatasets,wecanmitigatebatch
ICML2024AIforScienceworkshop,Vienna,Austria.PMLR235, effectsandcapturelatentgene-geneinteractionsfordown-
2024.Copyright2024bytheauthor(s).
1
4202
luJ
52
]GL.sc[
1v18181.7042:viXrascTransNetforGRNInferencefromPre-trainedscRNA-seqTransformerwithJointGraphLearning
streamtasks. Supervisedmethods,includingDGRNS(Zhaoetal.,2022),
convolutional neural network for co-expression (CNNC)
We propose an innovative knowledge-aware supervised
(Yuan&Bar-Joseph,2019),andDeepDRIM(Chenetal.,
GRN inference framework, scTransNet (see Figure 1),
2021),havebeendevelopedtoaddresstheincreasingscale
which integrates pre-trained single-cell language models
and inherent complexity of scRNA-seq data. Compared
withstructured knowledgeof GRNs. Ourapproachcom-
withunsupervisedlearning,supervisedmodelsarecapable
binesgenerepresentationslearnedfromscBERTwithgraph
ofdetectingmuchmoresubtledifferencesbetweenpositive
representationsderivedfromthecorrespondingGRNs,cre-
andnegativepairs(Yuan&Bar-Joseph,2019).
atingaunifiedcontext-awareandknowledge-awarerepre-
sentation(Fengetal.,2020). Thisjointlearningapproach DGRNS(Zhaoetal.,2022)combinesrecurrentneuralnet-
enablesustosurpasstheaccuracyofcurrentstate-of-the-art works(RNNs)forextractingtemporalfeaturesandconvolu-
methodsinsupervisedGRNinference. Byharnessingthe tionalneuralnetworks(CNNs)forextractingspatialfeatures
power of pre-trained transformer models and incorporat- toinferGRNs. CNNC(Yuan&Bar-Joseph,2019)converts
ingbiologicalknowledgefromdiversedatasources,such theidentificationofgeneregulationintoanimageclassifi-
asgeneexpressiondataandgeneregulatorynetworks,our cationtaskbytransformingtheexpressionvaluesofgene
approachpavesthewayformorepreciseandrobustGRN pairs into histograms and using a CNN for classification.
inference. Ultimately, this methodology offers deeper in- However,theperformanceofCNNC(Yuan&Bar-Joseph,
sightsintocellularregulatorymechanisms,advancingour 2019)ishinderedbytheissueoftransitiveinteractions. To
understandingofgeneregulation. addressthis,DeepDRIM(Chenetal.,2021)considersthe
informationfromneighboringgenesandconvertsTF–gene
pairsandneighboringgenesintohistogramsasadditional
2.RelatedWork
inputs,therebyreducingtheoccurrenceoftransitiveinter-
SeveralmethodshavebeendevelopedtoinferGRNsfrom actions to some extent. Despite their success there exist
scRNA-seqdata,broadlycategorizedintounsupervisedand certainlimitationstotheemploymentofCNNmodel-based
supervisedmethods. approachesforGRNreconstruction. Firstofall,thegenera-
tionofimagedatanotonlygivesrisetounanticipatednoise
Unsupervised methods primarily include information
butalsoconcealscertainoriginaldatafeatures. Addition-
theory-based, model-based, and machine learning-based
ally,thisprocessistime-consuming,andsinceitchangesthe
approaches. Information theory-based methods, such as
formatofscRNA-seqdata,thepredictionsmadebythese
mutualinformation(MI)(Margolinetal.,2006),Pearson
CNN-based computational approaches cannot be wholly
correlationcoefficient(PCC)(Sallehetal.,2015;Raza&
explained.
Jaiswal,2013),andpartialinformationdecompositionand
context(PIDC)(Chanetal.,2017),conductcorrelationanal- In addition to CNN-based methods, there are also other
ysesundertheassumptionthatthestrengthofthecorrelation approaches such as GNE (Kc et al., 2019) and GRN-
betweengenesisispositivelycorrelatedwiththelikelihood Transformer(Shuetal.,2022). GNE(genenetworkembed-
ofregulationbetweenthem. Model-basedapproaches,such ding)(Kcetal.,2019)isadeeplearningmethodbasedon
asSCODE(Matsumotoetal.,2017),involvefittinggeneex- multilayerperceptron(MLP)forGRNinferenceappliedto
pressionprofilestomodelsthatdescribegenerelationships, microarraydata.Itutilizesone-hotgeneIDvectorsfromthe
whicharethenusedtoreconstructGRNs(Shuetal.,2021; genetopologytocapturetopologicalinformation,whichis
Tsaietal.,2020). ofteninefficientduetothehighlysparsenatureoftheresult-
ingone-hotfeaturevector. GRN-Transformer(Shuetal.,
Machine learning-based unsupervised methods, like GE-
2022)constructsaweaklysupervisedlearningframework
NIE3(Huynh-Thuetal.,2010)andGRNBoost2(Moerman
basedonaxialtransformertoinfercell-type-specificGRNs
etal.,2019), utilizetree-basedalgorithmstoinferGRNs.
fromscRNA-seqdataandgenericGRNsderivedfromthe
ThesemethodsareintegratedintotoolslikeSCENIC(Aibar
bulkdata.
etal.,2017;VandeSandeetal.,2020),employingtreerules
to learn regulatory relationships by iteratively excluding Morerecently,graphneuralnetworks(GNNs)(Wuetal.,
onegeneatatimetodetermineitsassociationswithother 2020),whichareeffectiveincapturingthetopologyofgene
genes. Despite not requiring labeled data, these unsuper- networks,havebeenintroducedintoGRNpredictionmeth-
visedmethodsoftenstrugglewiththenoise,dropouts,high ods. Forinstance,GENELink(Chen&Liu,2022b)treats
sparsity,andhighdimensionalitytypicalofscRNA-seqdata. GRNinferenceasalinkpredictionproblemandusesgraph
Additionally,thecomputationalexpenseandscalabilityis- attentionnetworkstopredicttheprobabilityofinteraction
sues of these tree-based methods, due to the necessity of betweentwogenenodes. However,existingmethodsoften
segmentinginputdataanditerativelyestablishingmultiple sufferfromlimitationssuchasimproperhandlingofbatch
models,presentfurtherchallengesforlargedatasets. effects,difficultyinleveraginglatentgene-geneinteraction
2scTransNetforGRNInferencefromPre-trainedscRNA-seqTransformerwithJointGraphLearning
information,andmakingsimplisticassumptions,whichcan TheconnectionsofthismodelwithBERTaregivenasfol-
impairtheirgeneralizationandrobustness. lows. First,scBERTfollowsBERT’srevolutionarymethod
toconductself-supervisedpre-training(Devlinetal.,2019)
andusesTransformerasthemodelbackbone(Choromanski
3.Approach
etal.,2022). Second,thedesignofembeddingsinscBERT
Asshownin(Figure1),ourapproachcontainsfourparts: is similar to BERT in some aspects while having unique
BERT encoding, Attentive Pooling, GRN encoding with featurestoleveragegeneknowledge. Fromthisperspective,
GNNsandOutputlayer. TheinputscRNA-seqdatasetsare thegeneexpressionembeddingcouldbeviewedastheto-
processedintoacell-by-genematrix, X ∈ RN×T, where kenembeddingofBERT.Asshufflingthecolumnsofthe
eachelementrepresentsthereadcountofanRNAmolecule. input does not change its meaning (like the extension of
Specifically,forscRNA-seqdata,theelementdenotesthe BERTtounderstandtabulardatawithTaBERT(Yinetal.,
RNA abundance for gene t ∈ {0,1,...,T} in cell n ∈ 2020)), absolute positions are meaningless for gene. In-
{0,1,...,N}. Insubsequentsections,wewillrefertothis steadgene2vecisusedtoproducegeneembeddings,which
matrixastherawcountmatrix. Letusdenotethesequence couldbeviewedasrelativeembeddings(Duetal.,2019)
ofgenetokensas{g ,...,g },whereTisthetotalnumber thatcapturethesemanticsimilaritiesbetweenanyoftwo
1 T
ofgenes. genes. Third,Transformerwithglobalreceptivefieldcould
effectivelylearnglobalrepresentationandlong-rangede-
pendencywithoutabsolutepositioninformation,achieving
excellentperformanceonnon-sequentialdata(suchasim-
Attention Scores
Pooled gene ages,tables)(Parmaretal.,2018;Yinetal.,2020).
Gene
Cell 1 gene
embeddings (Zg)
Es nc cB oE dR eT r C ee mm el bb l 2 ee dd g dd e ii n nn e gg ss g g1
2
I hn owsp ti ote uo tf ilt ih zee tg he ene trae nm sb cre id pd tii on ng, leth ve er le oi fs ea al cso ha gec nh ea ,ll wen hg ice ho in s
Input (scRNA-seq) Cell n gene gT actuallyasinglecontinuousvariable. Thegeneexpression
embeddings
couldalsobeconsideredastheoccurrenceofeachgenethat
hasalreadybeenwell-documentedinabiologicalsystem.
Node Features
Drawing from bag-of-words (Zhang et al., 2010) insight, Multi-hop Message Passing
theconventionallyusedterm-frequency-analysismethodis
Graph ⍺ g1 Final Output
Encoder 12 Layer appliedthatdiscretizesthecontinuousexpressionvariables
⍺
Gene regulatory Network
g2 13 g3
Link Prediction
bybinning,andconvertstheminto200-dimensionalvectors,
whicharethenusedastokenembeddingsforthescBERT
model.
Figure1.OverviewofscTransNetframeworkforsupervised
GRNinferencewithBERTEncodingLayer(topleft;Section3.1), Foreachtokeng inacell,weconstructitsinputrepresen-
t
AttentivePooling(topright; Section3.2), GRNencodingwith tationas:
GNNs(bottomleft;Section3.3)andFinalOutputlayer(bottom
right;Section3.4). Itaugmentstheoutputfromgraphencoder h0
t
=emb gene2vec(g t)+emb expr(g t) (1)
(forknowledgeunderstanding)withscBERTencoder(forcontex-
where emb (g ) represents gene2vec embedding
tualunderstanding)toinferregulatoryinterdependenciesbetween gene2vec t
(Duetal.,2019)ofgeneg analogoustopositionembedding
genes. t
inBERTandemb (g )representsexpressionembedding
expr t
ofthegeneexpressionofg analogoustotokenembedding
t
3.1.BERTEncodingLayer inBERT.
SuchinputrepresentationsarethenfedintoLsuccessive
(Yang et al., 2022; Cui et al., 2024; Chen et al., 2023;
Transformerencoderblocks,i.e.,
Theodoris et al., 2023) show that pre-trained transformer
modelshaveastrongunderstandingofgene-geneinterac- hl =Transformer(hl−1),l=1,2,...,L, (2)
t t
tionsacrosscellsandhaveachievedstate-of-the-artresults
onavarietyofsingle-cellprocessingtasks. WeusescBERT so as to generate deep, context-aware representations for
(Yangetal.,2022)asthebackbone,whichisasuccessful genes. The final hidden states {hL t}T t=1 are taken as the
pre-trained model with the advantage of capturing long- output of this layer (Devlin et al., 2019; Vaswani et al.,
distance dependency as it uses Performer (Choromanski 2023).
etal.,2022)toimprovethescalabilityofthemodeltotoler-
ateover16,000geneinputs. 3.2.AttentivePooling
ThescBERTmodeladoptstheadvancedparadigmofBERT AfterextractingtheBERTencodingswefurtherutilizethe
andtailorsthearchitecturetosolvesingle-celldataanalysis. attention scores across cells from the model to select the
3
lleC
…
gnilooP
evitnettA
noitatneserpeR
edoN
…scTransNetforGRNInferencefromPre-trainedscRNA-seqTransformerwithJointGraphLearning
mostrepresentativecellsforpoolingofeachgenerepresen- embedding such that the updated matrix is of the form
tation. Foreachinputgenetokeng wegettheembeddings Z ∈ RT×d, where d is the dimension of the output gene
t
forallcellsdenotedas{hL }N ,whereNisthenumber embedding.
t(n) n=1
ofcells. Z [t]=⊕N hL ·an (6)
g i=1 t(n) t
The quadratic computational complexity of the BERT
3.3.GRNencodingwithGNNs
model,withtheTransformerasitsfoundationalunit,does
notscaleefficientlyforlongsequences. Giventhatthenum- Inthismodule,weuserawcountmatrixasthefeaturesof
berofgenesinscRNA-seqdatacanexceed20,000,thislim- the genes. Subsequently, we utilize graph convolutional
itationbecomessignificant. Toaddressthisissue,scBERT network(GCN)-basedinteractiongraphencoderstolearn
employsamatrixdecompositionvariantoftheTransformer, genefeaturesbyleveragingtheunderlyingstructureofthe
knownasPerformer(Choromanskietal.,2022),tohandle geneinteractiongraph.
longersequencelengths. InaregularTransformer,thedot-
LetusdenotethepriornetworkasG={V,E},whereVis
productattentionmechanismmapsQ,K,andV,whichare
thesetofnodesandEisthesetofedges. Toperformthe
theencodedrepresentationsoftheinputqueries,keys,and
reasoningonthispriorgeneregulatorynetworkG,ourGNN
valuesforeachunit. Thebidirectionalattentionmatrixis
module builds on the graph attention framework (GAT)
formulatedasfollows:
(Velickovicetal.,2017),whichinducesnoderepresentations
Att(Q,K,V)=D−1(QKT)V, via iterative message passing between neighbors on the
(3)
D =diag(QKT1 ) graph. IneachlayerofthisGNN,thecurrentrepresentation
L ofthenodeembeddings{vl−1,...,vl−1}isfedintothelayer
1 T
where Q = W X, K = W X, V = W X are linear to perform a round of information propagation between
q K V
transformationsoftheinputX;W ,W andW arethe nodesinthegraphandyieldpre-fusednodeembeddingsfor
Q K V
weightmatricesasparameters;1 istheall-onesvectorof eachnode:
L
length L; and diag(.) is a diagonal matrix with the input
{v˜l,...,v˜ l}=GNN({vl−1,...,vl−1})
vectorasthediagonal. 1 T 1 T (7)
for l=1,...,M
TheattentionmatrixinPerformerisdescribedasfollows:
Aˆtt(Q,K,V)=Dˆ−1(Q′ ((K′ )TV)), Specifically,foreachlayerl,weupdatetherepresentation
Dˆ =diag(Q′ ((K′ )T1 )) (4) v˜ tl ofeachnodeby
L
(cid:88)
whereQ′ = ϕ(Q),K′ = ϕ(K),andthefunctionϕ(x)is v˜ tl =f n( α stm st)+v tl−1 (8)
definedas:
c
vs∈ηvt∪{vt}
ϕ(X)= √ f(ωTX) (5)
m whereη vt representstheneighborhoodofanarbitrarynode
v ,m denotesthemessageoneofitsneighborsv passes
t st s
wherecisapositiveconstant,ωisarandomfeaturematrix, tov ,α isanattentionweightthatscalesthemessagem ,
t st st
andmisthedimensionalityofthematrix. andf isa2-layerMLP.Themessagesm betweennodes
n st
allowentityinformationfromanodetoaffectthemodel’s
The attention weights can be obtained from equation 3,
representation of its neighbors, and are computed in the
modifiedbyreplacingVwithV0,whereV0containsone-
followingmanner:
hot indicators for each position index. All the attention
matricesareintegratedintoonematrixbytakinganelement-
r =f (˜r ,u ,u ) (9)
st r st s t
wise average across all attention matrices in multi-head
multi-layerPerformers. Inthisaverageattentionmatrixfor m =f (v(l−1),u ,r ) (10)
eachcell,A(i,j)representshowmuchattentionfromgene st m s s st
iwaspaidtogenej. Tofocusontheimportanceofgenes
whereu ,u arenodetypeembeddings,˜r isarelationem-
to each cell n, the attention matrix is summed along the s t st
bedding for the relation connecting v and v , f is a 2-
columnsintoanattention-sumvectora ,anditslengthis s t r
n layerMLP,andf isalineartransformation. Theattention
equal to the number of genes. These attention scores of m
weightsα scalethecontributionofeachneighbor’smes-
geneg areobtainedacrosscellsandnormalizeddenotedas st
t sagebyitsimportance,andarecomputedasfollows:
{an}N
t n=1
These normalized scores are used for weighted aggrega- q s =f q(v( sl−1),u s) (11)
tionofgeneembeddingsacrosscells. Weaggregateeach
cell’sgenerepresentationstogetherintoonegene-levelcell k =f (v(l−1),u ,r ) (12)
t k t t st
4scTransNetforGRNInferencefromPre-trainedscRNA-seqTransformerwithJointGraphLearning
exp(γ ) q⊺k
α st = (cid:80) vs∈ηvt∪{vt}s et xp(γ st),γ st = √s Dt (13) p lea ci tr es d. fM roo mre to rav ie nr i, n1 g0 s% amo pf leT sF f- og re vn ae lip da ai tr is ona .re Thra en rd eo mm al iy nis ne g-
positivepairsformedthepositivetestset. Negativesamples
wheref andf arelineartransformationsandu ,u ,r weregeneratedusingthefollowingstrategies: 1)Unlabeled
q k s t st
aredefinedthesameasabove. interactions: AllunobservedTF-geneinteractionsoutside
the labeled files were considered negative instances. 2)
3.4.FinalOutputLayer Hardnegativesampling: Toenhancemodellearningduring
training,weemployedauniformlyrandomnegativesam-
In the final output layer, we concatenate the input gene
plingstrategywithinthetrainingset. Thisinvolvedcreating
representationsZ fromtheBERTencodinglayerwiththe
g ”hard negative samples” by pairing each positive sample
graphrepresentationofeachgenefromGNNtogetthefinal
(g1,g2)withanegativesample(g1,g3),wherebothshare
geneembedding.
thesamegeneg1. Thisapproachinjectsmorediscrimina-
Weinputthesefinalembeddingsofpairwisegenesiandj tive information and accelerates training. 3) Information
intotwochannelswiththesamestructure. Eachchannelis leakageprevention: Negativetestsampleswererandomly
composedofMLPstofurtherencoderepresentationstolow- selected from the remaining negative instances after gen-
dimensionalvectorswhichservefordownstreamsimilarity erating the training and validation sets. This ensured no
measurementorcausalinferencebetweengenes. informationleakagefromthetestsettothetrainingprocess.
Thepositive-to-negativesampleratioineachdatasetwas
adjustedtoreflectthenetworkdensityi.e.
4.ExperimentalSetup
4.1.BenchmarkscRNA-seqdatasets Positive NetworkDensity
= (14)
Negative 1−NetworkDensity
TheperformanceofscTransNetisevaluatedontwohuman
celltypesusingsingle-cellRNA-sequencing(scRNA-seq)
Model Training To account for the class imbalance, we
datasets from the BEELINE study (Pratapa et al., 2020):
adoptedtwoperformancemetrics: AreaUndertheReceiver
humanembryonicstemcells(hESC(Yuan&Bar-Joseph,
OperatingCharacteristicCurve(AUROC)andAreaUnder
2019))andhumanmaturehepatocytes(hHEP(Campetal.,
thePrecision-RecallCurve(AUPRC).Thesupervisedmodel
2017)). Thecell-type-specificChIP-seqground-truthnet-
wastrainedfor100iterationswithalearningrateof0.003.
worksareusedasareferenceforthesedatasets.ThescRNA-
TheGraphNeuralNetwork(GNN)architecturecomprised
seq datasets are preprocessed following the approach de-
two layers with hidden layer sizes of 256 and 128 units,
scribedinthe(Pratapaetal.,2020),focusingoninferring
respectively.
interactionsoutgoingfromtranscriptionfactors(TFs). The
most significantly varying genes are selected, including EvaluationAllreportedresultsarebasedsolelyonpredic-
all TFs with a corrected P-value (Bonferroni method) of tionsfromtheheld-outtestset. Toensureafaircomparison,
variance below 0.01. Specifically, 500 and 1000 of the identical training and validation sets were utilized for all
mostvaryinggenesarechosenforgeneregulatorynetwork evaluated supervised methods. This approach eliminates
(GRN)inference. ThescRNA-seqdatasetscanbeaccessed potentialbiasintroducedbydifferentdatasplits.
fromtheGeneExpressionOmnibus(GEO)withaccession
numbersGSE81252(hHEP)andGSE75748(hESC).The 4.3.BaselineMethods
evaluationcomparestheinferredgeneregulatorynetworks
ToassesstheeffectivenessofourmodelinpredictingGRNs,
toknownChIP-seqground-truthnetworksspecifictothese
wecompareourmodelscTransNetagainsttheexistingbase-
celltypes.
line methods commonly used for inferring GRNs, as fol-
lows:
4.2.ImplementationandTrainingdetails
Data Preparation We utilized the benchmark networks • GNNLink(Maoetal.,2023)isagraphneuralnetwork
that containing labeled directed regulatory dependencies modelthatusesaGCN-basedinteractiongraphencoder
betweengenepairs. Thesedependencieswereclassifiedas tocapturegeneexpressionpatterns.
positivesamples(labeled1)ifpresentinthenetwork,and
• GENELink(Chen&Liu,2022b)proposesagraphatten-
negativesamples(labeled0)ifabsent. Duetotheinherent
tionnetwork(GAT)approachtoinferpotentialGRNsby
network density, the number of negative samples signifi-
leveragingthegraphstructureofgeneregulatoryinterac-
cantlyoutnumberedpositivesamples. Toaddresstheclass
tions.
imbalance,knowntranscriptionfactor(TF)-genepairsare
splitintotraining(2/3),andtest(1/3)sets. Positivetraining • GNE (gene network embedding) (Kc et al., 2019) pro-
sampleswererandomlyselectedfromtheknownTF-gene posesamultilayerperceptron(MLP)approachtoencode
5scTransNetforGRNInferencefromPre-trainedscRNA-seqTransformerwithJointGraphLearning
mer
A)
Ours G N
NLink GENELink
G NE C N N
C DeepDRIM
GRN-Transfor
PCC MI SC O
DE GRNBoost2 GENIE3
TFs with most-varying hESC 0.88 0.85 0.79 0.67 0.68 0.627 0.51 0.47 0.51 0.5 0.49 0.5
500 genes hHEP 0.87 0.82 0.83 0.8 0.64 0.52 0.49 0.49 0.5 0.47 0.52 0.54
TFs with most-varying hESC 0.87 0.8 0.78 0.68 0.72 0.56 0.67 0.47 0.51 0.51 0.48 0.49
1000 genes hHEP 0.87 0.84 0.85 0.81 0.66 0.63 0.58 0.49 0.49 0.48 0.52 0.54
mer
B)
Ours G N
NLink GENELink
G NE C N N
C DeepDRIM GRN-Transfor
PCC MI SC O
DE GRNBoost2 GENIE3
TFs with most-varying hESC 0.6 0.52 0.48 0.34 0.25 0.13 0.15 0.14 0.15 0.15 0.15 0.15
500 genes hHEP 0.78 0.75 0.69 0.65 0.46 0.39 0.35 0.35 0.35 0.33 0.38 0.38
TFs with most-varying hESC 0.59 0.51 0.5 0.34 0.27 0.19 0.16 0.14 0.15 0.15 0.14 0.15
1000 genes hHEP 0.78 0.78 0.7 0.66 0.49 0.46 0.53 0.34 0.34 0.33 0.37 0.38
Low High
Figure2.SummaryoftheGRNpredictionperformanceofscTransNetinthe(A)AUROCmetric(top)(B)andtheAUPRCmetric(bottom).
Ourevaluationisconductedontwohumansingle-cellRNAsequencing(scRNA-seq)datasets,withacell-type-specificground-truth
network.ThescRNA-seqdatasetsconsistofsignificantlyvaryingtranscriptionfactors(TFs)andthe500or1000most-varyinggenes.
bothgeneexpressionprofilesandnetworktopologyfor • GENIE3 (Huynh-Thu et al., 2010) is a random forest-
predictinggenedependencies. based machine learning method that constructs GRNs
based on regression weight coefficients, and won the
• CNNC (Yuan & Bar-Joseph, 2019) proposes inferring
DREAM5InSilicoNetworkChallengein2010.
GRNsusingdeepconvolutionalneuralnetworks(CNNs).
Thesemethodsrepresentadiverserangeofapproaches,in-
• DeepDRIM(Chenetal.,2021)isasuperviseddeepneural
cluding traditional statistical methods, machine learning
networkthatutilizesimagesrepresentingtheexpression
techniques, and deep learning models, for inferring gene
distributionofjointgenepairsasinputforbinaryclassifi-
regulatory networks from various types of data, such as
cationofregulatoryrelationships,consideringbothtarget
bulkandsingle-cellRNA-seq,aswellasincorporatingad-
TF-genepairsandpotentialneighborgenes.
ditionalinformationlikenetworktopologyandchromatin
• GRN-transformer (Shu et al., 2022) is a weakly super- accessibility.
visedlearningmethodthatutilizesaxialtransformersto
infercelltype-specificGRNsfromsingle-cellRNA-seq 5.Results
dataandgenericGRNs.
5.1.Performanceonbenchmarkdatasets
• Pearsoncorrelationcoefficient(PCC)(Sallehetal.,2015;
The results (see Figure 2) demonstrate that scTransNet
Raza&Jaiswal,2013)isatraditionalstatisticalmethod
outperforms state-of-the-art baseline methods across all
formeasuringthelinearcorrelationbetweentwovariables,
fourbenchmarkdatasets,achievingsuperiorperformance
oftenusedasabaselineforGRNinference.
intermsofbothAUROCandAUPRCevaluationmetrics.
• Mutual information (MI) (Margolin et al., 2006) is an Notably, scTransNet’s AUROC values are approximately
information-theoreticmeasureofthemutualdependence 5.4%and7.4%higheronaveragecomparedtothesecond-
betweentworandomvariables,alsousedasabaselinefor best methods, namely GNNLink (Mao et al., 2023) and
GRNinference. GENELink(Chen&Liu,2022b),respectively.Similarly,sc-
TransNet’sAUPRCvaluesshowanimpressiveimprovement
• SCODE (Matsumoto et al., 2017) is a computational ofapproximately7.4%and16%onaverageoverGNNLink
method for inferring GRNs from single-cell RNA-seq andGENELink,respectively.
datausingaBayesianframework.
To gain further insights, we analyzed scTransNet’s final
• GRNBoost2(Moermanetal.,2019)isagradientboosting- generegulatorynetwork(GRN)predictionsandcompared
basedmethodforGRNinference. them with those from GENELink. Our analysis revealed
6scTransNetforGRNInferencefromPre-trainedscRNA-seqTransformerwithJointGraphLearning
Table1.ComparisonofaverageAUROCandAUPRCevaluationmetricsonhumanbenchmarkdatasets,validatingtherolesoftheGNN
encoder,scBERTencoder,andAttentivePoolingusingthecell-type-specificChIP-seqnetworkforourproposedmethod.
w/oGNNencoder w/oscBERTencoder w/oAttentivePooling scTransNet
Dataset AUROC AUPRC AUROC AUPRC AUROC AUPRC AUROC AUPRC
hESC 0.842 0.544 0.853 0.572 0.860 0.569 0.880 0.595
hHEP 0.830 0.725 0.854 0.753 0.862 0.683 0.870 0.780
thatscTransNeteffectivelycapturedallthegeneregulatory
interactionspredictedbyGENELink. Thisfindingsuggests
that by incorporating joint learning, scTransNet does not OTX2
KIF23
introduce additional noise to the predictive power of the
graphrepresentations. Instead, itenhancesthepredictive FAM117B
RBPJ AR1D4B
capabilitythroughthescBERTencoderinitsarchitecture.
Figure3providesavisualizationofapartialsubgraphof
SOX2 HNRNPD NANOG
thegroundtruthGRN,highlightingthepredictionsmade
byscTransNetthatwerenotcapturedbyGENELink,which NUP35
CCNB2
solelyreliesongraphsforpredictinggene-geneinteractions.
Additionally, thefigurevisualizesthegroundtruthlabels HSD17B1 ISL1
thatscTransNetfailedtocapture. Insummary,thecompara- EOMES
tiveanalysisdemonstratesthatscTransNeteffectivelycap-
turesalltheregulatoryinteractionspredictedbyGENELink Edge detected by scTransNet Transcription Factor
whileleveragingjointlearningtoimprovepredictiveperfor- Edge not detected by scTransNet Target Gene
mance. Thevisualizationillustratestheadditionalinterac-
tionsscTransNetcouldpredictbeyondGENELink,aswell
Figure3.GRNpredictionperformanceofscTransNetonapartial
asthegroundtruthinteractionsitmissed,providinginsights
groundtruthsubgraph.Solidlineedgesdepictgroundtruthregu-
intothestrengthsandlimitationsoftheproposedmethod.
latoryinteractionscorrectlypredictedbyscTransNetbutmissed
bythebaselineGENELinkmethod,whichreliessolelyongraph
5.2.DiscussionandAblations representations.Notably,scTransNeteffectivelyidentifiedallreg-
ulatorylinkspredictedbyGENELink(notvisualized).Dottedline
Toevaluatetheeffectivenessofjointlylearningfrompre-
edgesrepresentgroundtruthinteractionsthatscTransNetfailed
trained scRNA-seq language models (Yang et al., 2022),
tocapturerevealitslimitationsandprovidinginsightsforfurther
which capture rich contextual representations, and Gene
improvement. Overall, thishighlightsscTransNet’sstrengthin
RegulatoryNetworks(GRNs),whichencodestructuredbi- leveragingjointlearningtouncoveradditionaltrueregulatoryin-
ologicalknowledge,wecomparetheaverageAreaUnder teractionsbeyondgraphs.
theReceiverOperatingCharacteristicCurve(AUROC)and
AreaUnderthePrecision-RecallCurve(AUPRC)metrics
withandwithouttheseencodersacrossthefourhumancell
theGraphNeuralNetwork(GNN)encodercomponentin
typebenchmarkdatasets(Pratapaetal.,2020). Theaver-
theproposedmethod. WiththeGNNencoder,theaverage
ageAUROCandAUPRCscoresarecalculatedacrossboth
AUROC value across all the human cell type datasets is
the TFs+500highly variablegenes and TFs+1000 highly
87.5%,andtheaverageAUPRCvalueis68.7%. Incontrast,
variablegenesdatasetsforeachhumancelldatatype(i.e,
without the GNN encoder, the average AUROC drops to
hESC (human embryonic stem cells) and hHEP (human
83.6%,andtheaverageAUPRCdecreasesto63.4%. The
maturehepatocytes)). Additionally,wevalidatetheimpor-
inclusionoftheGNNencoderleadstoanimprovementof
tanceofincorporatingAttentivePooling(Section3.2)by
4.6%intheaverageAUROCandanotable8.3%increasein
contrastingtheresultswhenusingaveragepoolingofgene
theaverageAUPRC.Theseresultshighlighttheconsistent
embeddingsacrosscellsinsteadofattentivepooling.Consis-
performanceenhancementprovidedbytheGNNencoder
tentparametersettingsareemployedacrossallfourhuman
acrossbothAUROCandAUPRCmetricsforthehumancell
cellbenchmarkdatasets,withCell-type-specificChIP-seq
typebenchmarkdatasets. TheGNNencoderplaysacrucial
networkdataservingasthegroundtruth.
roleinthearchitectureasthetaskisformulatedasasuper-
Effect of Graph Neural Network Component: The re- visedGeneRegulatoryNetwork(GRN)inferenceproblem,
sults demonstrate the significant impact of incorporating aimingtoidentifypotentialgeneregulatorydependencies
7scTransNetforGRNInferencefromPre-trainedscRNA-seqTransformerwithJointGraphLearning
givenpriorknowledgeoftheGRN.TheGNNmodelsthe learnedforeachcell. Consequently,attentivepoolinghelps
regulatoryinteractionsasagraph,learningnoderepresen- toeffectivelyfocusonhigh-qualitycelldatabyremoving
tations that effectively encode the network topology and noise. Byemployinganattentivepoolingmechanism,sc-
geneinterdependenciespresentintheGRN,whichserves TransNetselectivelyfocusesonthemostinformativecells
astheprimarysourceofbiologicalknowledge. Theresults foreachgene,mitigatingnoiseandfilteringoutirrelevant
inTable1justifytheuseofthisstructuralgraphrepresenta- information,therebyenhancingthequalityoftheinputdata
tionforunderstandingthecomplexregulatorynetworksin usedforGRNinference.
single-celltranscriptomicsdata.
EffectofPre-trainedSingle-CellTranscriptomicsTrans- 6.ConclusionandFutureWork
former: The removal of the scBERT encoder also leads
Inthiswork, weproposescTransNet, ajointgraphlearn-
to a drop in performance, with the average AUROC de-
ing inference framework that integrates prior knowledge
creasing from 87.5% to 85.3%, and the average AUPRC
fromknownGeneRegulatoryNetworks(GRNs)withcon-
decliningfrom68.7%to66.2%acrossbothcelltypes(see
textual representations learned by pre-trained single-cell
Table1).TheinclusionofscBERTrepresentationsimproves
transcriptomicsTransformers. Ourapproachaimstoeffec-
theAUROCby2.6%andtheAUPRCby3.8%. Whilethe
tivelyboostGRNpredictionbyleveragingthecomplemen-
improvementislesssignificantcomparedtotheGNNen-
tarystrengthsofstructuredbiologicalknowledgeandrich
coder, this is expected as the contextual representations
contextualrepresentations. Weevaluateourmethodonfour
fromscRNA-seqdataarelearnedthroughpre-trainingon
humancellscRNA-seqbenchmarkdatasetsanddemonstrate
millionsofunlabeledsinglecellsandthenfine-tunedforthe
consistentimprovementsovercurrentbaselinesinpredict-
specificcelltype. Inadditiontorichcontextualrepresenta-
inggene-generegulatoryinteractions. Ourframeworkcom-
tions,scBERTcaptureslong-rangedependenciesbetween
prises four key modules: a GNN encoder to capture the
genesbyleveragingself-attentionmechanismsandpretrain-
networktopologyfromknownGRNs,ascBERTencoderto
ingonlarge-scaleunlabeledscRNA-seqdata(Pratapaetal.,
learncontextualrepresentationsfromscRNA-seqdata,an
2020). This comprehensive understanding of gene-gene
AttentivePoolingmechanismtofocusoninformativecells,
interactionsandsemanticrelationshipsallowsforeffective
and a Final Output layer for prediction. The synergistic
modelingofcomplex,non-lineargeneregulatorypatterns
combinationofthesemodulesisverifiedtobeeffectivein
thatextendbeyondimmediateneighborsinthegeneregula-
accuratelyinferringgeneregulatorydependencies.
torynetwork.
Moving forward, we plan to incorporate the knowledge
Thecontextualrepresentationslearnedbythepre-trained
integrationprocessdirectlyintothefine-tuningoftheTrans-
Transformerfacilitatetheidentificationofintricateregula-
formermodel,aimingtofuseinformationacrosslayersmore
toryrelationshipsthatmightbeoverlookedbytraditional
effectively. Additionally,wewillevaluateourapproachon
methodsfocusedonlocalneighborhoodsorpredefinedgene
variousotherdatasets,includingsimulateddatasets,tofur-
sets. Theabilitytocaptureglobalcontextandlong-range
ther validate its robustness and generalizability. Beyond
dependenciesisakeyadvantageofpre-trainedsingle-cell
GRNinference,weintendtoinvestigatetheadvantagesof
Transformermodelsfordecipheringtheintricategeneregu-
jointlylearningsingle-cellTransformersandstructuredbio-
latorymechanismsgoverningcellularstatesandidentities.
logicalknowledgeforothercell-relatedtasks. Thesetasks
TheimprovementshowninTable1justifiestheeffectiveness
include cell type annotation, identifying echo archetypes
ofthisapproach.
(Luca et al., 2021), and enhancing the interpretability of
EffectofAttentivePoolingMechanism: Theimpactof single-cell models. By leveraging the complementary
incorporatingAttentivePoolingisevaluatedbycomparing strengthsofcontextualrepresentationsandstructuredknowl-
theAUROCandAUPRCmetricswithandwithoutattentive edge,weaimtoadvancetheunderstandingandanalysisof
poolingacrossfourdatasets. AsshowninTable1,thein- complexcellularprocessesandregulatorymechanisms.
clusionofattentivepoolingresultsinaslightimprovement,
with a 1.6% increase in the average AUROC and a 9.6%
Acknowledgements
increase in the average AUPRC. While the improvement
is not significant, the experiments confirm that attentive OurworkissponsoredbytheNSFNAIRRPilotandPSC
pooling offers some support for the gene regulation task. Neocortex,CommonwealthCyberInitiative,Children’sNa-
Webelievethatthesignificanceofattentivepoolingwillbe tionalHospital,FralinBiomedicalResearchInstitute(Vir-
morepronouncedwhenscalingthemethodtolargerdatasets. ginia Tech), Sanghani Center for AI and Data Analytics
Thecelltypedataissparseandoflowquality. However, (VirginiaTech), VirginiaTechInnovationCampus, anda
theattentionweightslearnedfromscBERT(Pratapaetal., generousgiftfromtheAmazon+VirginiaTechCenterfor
2020)demonstratethatthemarkergenesareautomatically EfficientandRobustMachineLearning.
8scTransNetforGRNInferencefromPre-trainedscRNA-seqTransformerwithJointGraphLearning
ImpactStatement networktoreconstructcell-type-specificgeneregulatory
networkusingsingle-cellrna-seqdata. Briefingsinbioin-
“Thispaperpresentsworkwhosegoalistoadvancethefield
formatics,22(6):bbab325,2021.
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be Chen, J., Xu, H., Tao, W., Chen, Z., Zhao, Y., and
specificallyhighlightedhere.” Han, J.-D. J. Transformer for one stop interpretable
cell type annotation. Nature Communications, 14(1):
References 223, Jan 2023. ISSN 2041-1723. doi: 10.1038/
s41467-023-35923-4. URLhttps://doi.org/10.
Aibar,S.,Gonza´lez-Blas,C.B.,Moerman,T.,Huynh-Thu, 1038/s41467-023-35923-4.
V.A.,Imrichova,H.,Hulselmans,G.,Rambow,F.,Ma-
rine,J.-C.,Geurts,P.,Aerts,J.,etal. Scenic: single-cell Choromanski,K.,Likhosherstov,V.,Dohan,D.,Song,X.,
regulatorynetworkinferenceandclustering.Naturemeth- Gane,A.,Sarlos,T.,Hawkins,P.,Davis,J.,Mohiuddin,
ods,14(11):1083–1086,2017. A.,Kaiser,L.,Belanger,D.,Colwell,L.,andWeller,A.
Rethinkingattentionwithperformers,2022.
Akers, K. and Murali, T. Gene regulatory network infer-
enceinsingle-cellbiology. CurrentOpinioninSystems Cramer,P.Organizationandregulationofgenetranscription.
Biology,26:87–97,2021. Nature,573(7772):45–54,2019.
Cui,H.,Wang,C.,Maan,H.,Pang,K.,Luo,F.,Duan,N.,
Biswas, S., Manicka, S., Hoel, E., and Levin, M. Gene
andWang,B. scgpt: towardbuildingafoundationmodel
regulatory networks exhibit several kinds of memory:
forsingle-cellmulti-omicsusinggenerativeai. Nature
quantificationofmemoryinbiologicalandrandomtran-
Methods, Feb 2024. ISSN 1548-7105. doi: 10.1038/
scriptional networks. iScience, 24(3):102131, March
s41592-024-02201-0. URLhttps://doi.org/10.
2021.
1038/s41592-024-02201-0.
Buettner,F.,Natarajan,K.N.,Casale,F.P.,Proserpio,V.,
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K. Bert:
Scialdone, A., Theis, F. J., Teichmann, S. A., Marioni,
Pre-training of deep bidirectional transformers for lan-
J.C.,andStegle,O.Computationalanalysisofcell-to-cell
guageunderstanding,2019.
heterogeneityinsingle-cellrna-sequencingdatareveals
hiddensubpopulationsofcells. Naturebiotechnology,33
Du, J., Jia, P., Dai, Y., Tao, C., Zhao, Z., and Zhi, D.
(2):155–160,2015.
Gene2vec: distributedrepresentationofgenesbasedon
co-expression. BMCgenomics,20:7–15,2019.
Camp, J. G., Sekine, K., Gerber, T., Loeffler-Wirth, H.,
Binder,H.,Gac,M.,Kanton,S.,Kageyama,J.,Damm, Feng,Y.,Chen,X.,Lin,B.Y.,Wang,P.,Yan,J.,andRen,X.
G., Seehofer, D., et al. Multilineage communication Scalablemulti-hoprelationalreasoningforknowledge-
regulateshumanliverbuddevelopmentfrompluripotency. awarequestionanswering,2020.
Nature,546(7659):533–538,2017.
Huynh-Thu,V.A.,Irrthum,A.,Wehenkel,L.,andGeurts,P.
Chan, T. E., Stumpf, M. P., and Babtie, A. C. Gene reg- Inferringregulatorynetworksfromexpressiondatausing
ulatory network inference from single-cell data using tree-basedmethods. PloSone,5(9):e12776,2010.
multivariate information measures. Cell systems, 5(3):
251–267,2017. Jovic,D.,Liang,X.,Zeng,H.,Lin,L.,Xu,F.,andLuo,Y.
Single-cell RNA sequencing technologies and applica-
Chen, G. and Liu, Z.-P. Graph attention network for tions: Abriefoverview. Clin.Transl.Med.,12(3):e694,
link prediction of gene regulations from single-cell March2022.
RNA-sequencing data. Bioinformatics, 38(19):4522–
4529, 08 2022a. ISSN 1367-4803. doi: 10.1093/ KC, K., Li, R., Cui, F., Yu, Q., and Haake, A. R.
bioinformatics/btac559. URL https://doi.org/ Gne: a deep learning framework for gene network in-
10.1093/bioinformatics/btac559. ference by aggregating biological information. BMC
Systems Biology, 13(2):38, Apr 2019. doi: 10.1186/
Chen, G. and Liu, Z.-P. Graph attention network for s12918-019-0694-y. URL https://doi.org/10.
linkpredictionofgeneregulationsfromsingle-cellrna- 1186/s12918-019-0694-y.
sequencing data. Bioinformatics, 38(19):4522–4529,
Kc, K., Li, R., Cui, F., Yu, Q., and Haake, A. R. Gne:
2022b.
adeeplearningframeworkforgenenetworkinference
Chen, J., Cheong, C., Lan, L., Zhou, X., Liu, J., Lyu, A., by aggregating biological information. BMC systems
Cheung,W.K.,andZhang,L. Deepdrim: adeepneural biology,13:1–14,2019.
9scTransNetforGRNInferencefromPre-trainedscRNA-seqTransformerwithJointGraphLearning
Kharchenko, P. V., Silberstein, L., and Scadden, D. T. Raza, K. and Jaiswal, R. Reconstruction and analysis of
Bayesianapproachtosingle-celldifferentialexpression cancer-specificgeneregulatorynetworksfromgeneex-
analysis. Naturemethods,11(7):740–742,2014. pressionprofiles. arXivpreprintarXiv:1305.5750,2013.
Luca, B. A., Steen, C. B., Matusiak, M., Azizi, A., Salleh, F. H. M., Arif, S. M., Zainudin, S., and Firdaus-
Varma, S., Zhu, C., Przybyl, J., Esp´ın-Pe´rez, A., Raih,M. Reconstructinggeneregulatorynetworksfrom
Diehn, M., Alizadeh, A. A., van de Rijn, M., Gen- knock-outdatausinggaussiannoisemodelandpearson
tles, A. J., and Newman, A. M. Atlas of clinically correlationcoefficient. Computationalbiologyandchem-
distinct cell states and ecosystems across human solid istry,59:3–14,2015.
tumors. Cell, 184(21):5482–5496.e28, 2021. ISSN
0092-8674. doi: https://doi.org/10.1016/j.cell.2021.09. Shu,H.,Zhou,J.,Lian,Q.,Li,H.,Zhao,D.,Zeng,J.,and
014. URLhttps://www.sciencedirect.com/ Ma,J. Modelinggeneregulatorynetworksusingneural
science/article/pii/S0092867421010618. networkarchitectures. NatureComputationalScience,1
(7):491–501,2021.
Mao,G.,Pang,Z.,Zuo,K.,Wang,Q.,Pei,X.,Chen,X.,and
Liu,J. Predictinggeneregulatorylinksfromsingle-cell Shu, H., Ding, F., Zhou, J., Xue, Y., Zhao, D., Zeng, J.,
RNA-seq data using graph neural networks. Briefings andMa,J. Boostingsingle-cellgeneregulatorynetwork
inBioinformatics,24(6):bbad414,112023. ISSN1477- reconstructionviabulk-celltranscriptomicdata.Briefings
4054. doi: 10.1093/bib/bbad414. URLhttps://doi. inBioinformatics,23(5):bbac389,2022.
org/10.1093/bib/bbad414.
Theodoris, C. V., Xiao, L., Chopra, A., Chaffin, M. D.,
Margolin, A., Nemenman, I., Basso, K., Wiggins, C., Al Sayed, Z. R., Hill, M. C., Mantineo, H., Brydon,
Stolovitzky,G.,Favera,R.,andandCalifano,A. Aracne: E.M., Zeng, Z., Liu, X.S., andEllinor, P.T. Transfer
Analgorithm forthereconstructionofgeneregulatorynet- learningenablespredictionsinnetworkbiology. Nature,
worksinamammaliancellularcontext. BMCbioinformat- 618(7965):616–624, Jun 2023. ISSN 1476-4687. doi:
ics,7,2006. 10.1038/s41586-023-06139-9. URL https://doi.
org/10.1038/s41586-023-06139-9.
Matsumoto, H., Kiryu, H., Furusawa, C., Ko, M. S., Ko,
S. B., Gouda, N., Hayashi, T., and Nikaido, I. Scode: Tsai,M.-J.,Wang,J.-R.,Ho,S.-J.,Shu,L.-S.,Huang,W.-
anefficientregulatorynetworkinferencealgorithmfrom L.,andHo,S.-Y. Grema: modellingofemulatedgene
single-cellrna-seqduringdifferentiation. Bioinformatics, regulatorynetworkswithconfidencelevelsbasedonevo-
33(15):2314–2321,2017. lutionaryintelligencetocopewiththeunderdetermined
problem. Bioinformatics,36(12):3833–3840,2020.
Moerman,T.,AibarSantos,S.,BravoGonza´lez-Blas,C.,
Simm,J.,Moreau,Y.,Aerts,J.,andAerts,S. Grnboost2 Van de Sande, B., Flerin, C., Davie, K., De Waegeneer,
and arboreto: efficient and scalable inference of gene M., Hulselmans, G., Aibar, S., Seurinck, R., Saelens,
regulatorynetworks. Bioinformatics,35(12):2159–2161, W.,Cannoodt,R.,Rouchon,Q.,etal. Ascalablescenic
2019. workflowforsingle-cellgeneregulatorynetworkanalysis.
Natureprotocols,15(7):2247–2276,2020.
OpenAI,R. Gpt-4technicalreport. ArXiv,2303,2023.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L.,
L.,Gomez,A.N.,Kaiser,L.,andPolosukhin,I.Attention
Shazeer, N., Ku, A., and Tran, D. Image trans-
isallyouneed,2023.
former. In Dy, J. and Krause, A. (eds.), Proceed-
ings of the 35th International Conference on Machine Velickovic, P., Cucurull, G., Casanova, A., Romero, A.,
Learning,volume80ofProceedingsofMachineLearn- Lio’, P., and Bengio, Y. Graph attention networks.
ing Research, pp. 4055–4064. PMLR, 10–15 Jul 2018. ArXiv, abs/1710.10903, 2017. URL https://api.
URLhttps://proceedings.mlr.press/v80/ semanticscholar.org/CorpusID:3292002.
parmar18a.html.
Wagner,A.,Regev,A.,andYosef,N. Revealingthevectors
Pratapa, A., Jalihal, A. P., Law, J. N., Bharadwaj, of cellular identity with single-cell genomics. Nature
A., and Murali, T. M. Benchmarking algorithms biotechnology,34(11):1145–1160,2016.
for gene regulatory network inference from single-
cell transcriptomic data. Nature Methods, 17(2):147– Wu,Z.,Pan,S.,Chen,F.,Long,G.,Zhang,C.,andPhilip,
154, Feb 2020. ISSN 1548-7105. doi: 10.1038/ S.Y. Acomprehensivesurveyongraphneuralnetworks.
s41592-019-0690-6. URL https://doi.org/10. IEEEtransactionsonneuralnetworksandlearningsys-
1038/s41592-019-0690-6. tems,32(1):4–24,2020.
10scTransNetforGRNInferencefromPre-trainedscRNA-seqTransformerwithJointGraphLearning
Yang,F.,Wang,W.,Wang,F.,Fang,Y.,Tang,D.,Huang,
J.,Lu,H.,andYao,J. scbertasalarge-scalepretrained
deeplanguagemodelforcelltypeannotationofsingle-
cell rna-seq data. Nature Machine Intelligence, 4(10):
852–866, Oct 2022. ISSN 2522-5839. doi: 10.1038/
s42256-022-00534-z. URLhttps://doi.org/10.
1038/s42256-022-00534-z.
Yin, P., Neubig, G., Yih, W.-t., and Riedel, S. TaBERT:
Pretraining for joint understanding of textual and tab-
ular data. In Jurafsky, D., Chai, J., Schluter, N., and
Tetreault,J.(eds.),Proceedingsofthe58thAnnualMeet-
ingoftheAssociationforComputationalLinguistics,pp.
8413–8426,Online,July2020.AssociationforCompu-
tational Linguistics. doi: 10.18653/v1/2020.acl-main.
745. URL https://aclanthology.org/2020.
acl-main.745.
Yuan, Y. and Bar-Joseph, Z. Deep learning for inferring
generelationshipsfromsingle-cellexpressiondata. Pro-
ceedingsoftheNationalAcademyofSciences,116(52):
27151–27158,2019.
Zeng, Y., He, Y., Zheng, R., and Li, M. Inferring single-
cellgeneregulatorynetworkbynon-redundantmutual
information. BriefingsinBioinformatics,24(5):bbad326,
2023.
Zhang, Y., Jin, R., and Zhou, Z.-H. Understanding bag-
of-wordsmodel: astatisticalframework. International
journal of machine learning and cybernetics, 1:43–52,
2010.
Zhao,M.,He,W.,Tang,J.,Zou,Q.,andGuo,F. Ahybrid
deep learning framework for gene regulatory network
inferencefromsingle-celltranscriptomicdata. Briefings
inbioinformatics,23(2):bbab568,2022.
11