Self-Training with Direct Preference Optimization
Improves Chain-of-Thought Reasoning
TianduoWang† , ShichenLi‡ , WeiLu†
†StatNLPResearchGroup,SingaporeUniversityofTechnologyandDesign
‡SoochowUniversity
{tianduo_wang,luwei}@sutd.edu.sg, scli_21@outlook.com
https://github.com/tianduowang/dpo-st
Abstract
GSM8K Performance v.s. Compute cost
Flan-T5-L
Effective training of language models (LMs) (Ours)
40
for mathematical reasoning tasks demands T5-11B T5-3B
high-quality supervised fine-tuning data. Be- 35 T5-L
sides obtaining annotations from human ex-
perts,acommonalternativeissamplingfrom 30
larger and more powerful LMs. However,
Flan-T5-11B
this knowledge distillation approach can be 25 T5-3B
Codex distillation
costlyandunstable,particularlywhenrelying (Fu et al., 2023)
on closed-source, proprietary LMs like GPT- 20 T5-L Flan-T5-3B P (MaL aM gi sd ti es rt il ela t t aio l.n , 2023)
Flan-T5-L Calcformer
4(OpenAI,2023),whosebehaviorsareoften (Kadlík et al., 2023)
15
unpredictable. In this work, we demonstrate 1e+18 3e+18 1e+19 3e+19 1e+20
Compute Cost (FLOPs)
thatthereasoningabilitiesofsmall-scaleLMs
can be enhanced through self-training, a pro-
Figure1: Ourapproachdemonstratessuperiorperfor-
cesswheremodelslearnfromtheirownoutputs.
mance on the GSM8K benchmark while minimizing
Wealsoshowthattheconventionalself-training
therequiredcomputecost,includingbothtrainingand
canbefurtheraugmentedbyapreferencelearn-
inference. Computecostcalculationsarebasedonthe
ing algorithm called Direct Preference Opti- methodologyoutlinedbyYuanetal.(2023).1
mization (DPO) (Rafailov et al., 2023). By
integratingDPOintoself-training,weleverage
preferencedatatoguideLMstowardsmoreac- Recentstudies(Fuetal.,2023;Magisteretal.,
curateanddiversechain-of-thoughtreasoning. 2023; Li et al., 2023a) demonstrate that the rea-
Weevaluateourmethodacrossvariousmath- soning capabilities of smaller LMs can be signif-
ematical reasoning tasks using different base
icantly enhanced through learning from the out-
models. Our experiments show that this ap-
puts of larger and more advanced LMs, such as
proachnotonlyimprovesLMs’reasoningper-
Codex (Chen et al., 2021), PaLM (Chowdhery
formancebutalsooffersamorecost-effective
et al., 2022), and GPT-4 (OpenAI, 2023). While
andscalablesolutioncomparedtorelyingon
largeproprietaryLMs. this method is straightforward to implement, the
associatedcostscanbesubstantial. Thecomputa-
1 Introduction tional demand, measured in floating-point opera-
tions(FLOPs),increasesconsiderablywhenusing
Making language models (LMs) perform math-
large LMs. Additionally, the reliance on propri-
ematical reasoning is a valuable, yet challeng-
etarylargeLMsfordataannotationnotonlyincurs
ing research objective (Hendrycks et al., 2021;
high economic costs but also raises concerns re-
Cobbe et al., 2021). Recent efforts have focused
garding the sustainability and scalability of such
on enhancing large-scale LMs’ reasoning abili-
practices. Forinstance,Hoetal.(2023)highlighted
tiesthroughvariousmethods,includingchain-of-
thatwhileemployinglargeLMsasannotatorscan
thoughtprompting(Weietal.,2022b;Kojimaetal.,
largelyenhancetheperformanceofsmallerLMs,
2022), continual pretraining (Azerbayev et al.,
it introduces a clear trade-off between economic
2024), and adding external verifiersq (Li et al.,
costsandperformancegains.
2023b). However, the research question of how
to enhance the reasoning capabilities of smaller-
1Allmethodspresentedhereareintegratedwithanexternal
sizedLMsremainsrelativelyunder-explored. calculatorexceptfortheCodexdistillationbyFuetal.(2023).
4202
luJ
52
]LC.sc[
1v84281.7042:viXra
)%(
ycaruccAAnother line of research focuses on explor- Algorithm1Self-trainingforCoTreasoningtasks
ingenhancementsthroughself-improvementmeth-
Input: pre-trainedlanguagemodelf
θ
ods(Zelikmanetal.,2022;Gulcehreetal.,2023;
Input: labeleddatasetL = {(xi,yi,ai)}l
i=1
Singh et al., 2023). These methods diverge from
Input: unlabeleddatasetU = {(xi,ai)}u
i=1
usingoutputsfromlargermodels,insteadencour-
Output: fine-tunedmodelf
θ′
agingLMstolearnfromtheirowngenerateddata.
The effectiveness of these techniques is evident, 1: Fine-tunef θ onLtogetf θ′
yet their success largely depends upon the inher- 2: repeat
entcapabilities ofthebase models. Forexample, 3: Buildpseudo-labeleddatasetS:
S = {(xi,yˆi,aˆi)}s
Zelikmanetal.(2022)initiatedself-improvement i=1
where xi ∼ U and yˆi,aˆi ∼ f (·|xi)
by few-shot prompting GPT-J (Wang and Ko- θ′
matsuzaki, 2021), a relatively large LM which 4: SelectSα ⊂ S when aˆi = ai
has 6 billion parameters, to generate rationales – 5: UpdateL ← Sα∪L
an emergent ability typically reserved for large 6: Trainf θ onLtogetanewf θ′
models (Wei et al., 2022a). However, the ex- 7: untilconvergenceormaxiterationisreached
tenttowhichsmall-scaleLMscangainfromself-
improvementremainsuncertain.
2 Background
In this work, we introduce a novel enhance-
menttotheconventionalself-trainingframework Math word problem solving The math word
by incorporating Direct Preference Optimization problem solving task (Cobbe et al., 2021) can be
(DPO) (Rafailov et al., 2023). This integration formulatedasasequence-to-sequencetaskwhere
specificallytargetsperformanceobjectiveswithin the input x is a question asking for an unknown
chain-of-thoughtreasoning,withaparticularfocus valueandtheoutputyisarationalethatleadstothe
on mathematical reasoning. The clear-cut nature answera. Normally,theanswerscanbeextracted
ofmathematicalsolutionsenablesstraightforward fromtherationalesviasomerule-basedmethods,
validationofamodel’soutputs,facilitatingthecre- suchasregularexpressions. Ageneratedrationale
ation of a preference dataset for DPO. Our em- yˆis regarded as correct if the extracted answer aˆ
piricalresultsindicatethatthismethodnotablyen- matchesthegoldanswera. Formally,thelabeled
hancesthereasoningcapabilitiesofLMswhilealso datasetforamathwordproblemsolvingtaskwith
reducingcomputationaloverhead. Wevisualizethe linstancescanberepresentedas:
relationship between the GSM8K (Cobbe et al.,
L = {(xi,yi,ai)}l . (1)
2021)performanceandcomputationalcostacross i=1
various specialized models in Figure 1. It can be
AcommonwayforspecializingaLMf towards
θ
observedthatourmethodnotonlyachievesstrong
mathreasoningwiththelabeleddatasetLissuper-
performance, but also reduces computational de-
vised fine-tuning (SFT). It optimizes f by mini-
θ
mandsbyeffectivelyutilizingself-generateddata
mizingthenegativeloglikelihoodlossL (θ):
SFT
forlearning. Overall,themaincontributionofthis
workcanbesummarizedasfollows: T
(cid:104)(cid:88) (cid:105)
E − logf (y |x,y ) , (2)
• We propose a novel extension to the classic θ t 1:t−1
(x,y)∼L
t=1
self-training framework by integrating Direct
PreferenceOptimization,demonstratingitsef- whereT isthelengthoftherationaley andweuse
fectivenessacrossvariousmathreasoningtasks. y t torepresentthet-thtokeniny.
Self-training Self-trainingisoneoftheearliest
• Ourmethodsignificantlyenhancesthereason-
approachesinsemi-supervisedlearning(Scudder,
ingabilitiesoflanguagemodelswhilerequiring
1965; Fralick, 1967) that has risen in popularity
minimal computational resources, optimizing
recently(Heetal.,2020;Aminietal.,2022). This
bothperformanceandefficiency.
method first regards a base model trained with a
• Wepresentanefficientmethodforintegrating labeled dataset L as teacher, and uses it to build
LMs with external tools, which significantly apseudo-labeleddatasetS byannotatinganunla-
boosts downstream task performance without beleddatasetU. Then,astudentmodelistrained
notablycompromisinginferencespeed. onacombinationofLandS thatareexpectedtooutperformtheteachermodel. Suchaframework Algorithm2DPO-augmentedself-training
has been shown effective across a wide range of
Input: pre-trainedlanguagemodelf
θ
naturallanguageprocessingtasks,includingnatu-
Input: labeleddatasetL = {(xi,yi,ai)}l
i=1
ral language understanding (Vu et al., 2021) and
Input: unlabeleddatasetU = {(xi,ai)}u
i=1
generation(Heetal.,2020). Aformaldescription
Output: fine-tunedmodelf
θ′
of a self-training algorithm for chain-of-thought
#Warm-upstage
(CoT)reasoningtasksisprovidedinAlgorithm1.
Previousstudieshavedemonstratedthatthequal- 1: Fine-tunef θ onLtogetf θ′
ity of the pseudo-labels largely impacts the over- 2: repeat
#DPOstep
allperformanceoftheself-trainingalgorithm(He
etal.,2020;Aminietal.,2022). Forexample,Gul- 3: GenerateDPOdatasetD:
D = {(xi, yi , yi )}N
cehreetal.(2023)proposedtoselecthigh-quality w l i=1
where xi ∼ U and yi , yi ∼ f (·|xi)
pseudo-labelswithalearnedrewardfunction. Ze- w l θ′
likman et al. (2022) filtered the generated ratio- 4: Tunef θ′ withL DPO onD togetf θd
#SFTstep
nalestoincludeonlytheonesthatleadtocorrect
answers. Although many methods are proposed 5: Buildpseudo-labeleddatasetS:
S = {(xi,yˆi,aˆi)}s
to select pseudo-labels, few works discuss how i=1
where xi ∼ U and yˆi,aˆi ∼ f (·|xi)
to improve the fine-tuned model f
θ′
so that more θd
high-quality pseudo-labels can be generated. In 6: SelectSα ⊂ S when aˆi = ai
thispaper,wepresentamethodtoenhancef in 7: UpdateL ← Sα∪L
θ′
eachiterationsothathigher-qualitypseudo-labeled 8: Trainf θ onLtogetanewf θ′
datacanbegenerated. 9: untilconvergenceormaxiterationisreached
Direct Preference Optimization The Rein-
whichcanbedefinedasfollows:
forcement Learning from Human Feedback
(RLHF) methods align LMs with human prefer- (cid:20) (cid:16) (cid:17)(cid:21)
E −logσ r(y |x)−r(y |x) , (5)
ence (Ouyang et al., 2022; Bai et al., 2022). The w l
(x,yw,y l)∼D
standardpipelineofRLHFrequirestofirsttraina
rewardmodelfromhumanpreferencedata. Then, π (·|x)
wherer(·|x) = βlog θ andβ isacoefficient
the reward model is used to fine-tune language π ref(·|x)
thatcontrolsπ ’sdeviationfromπ .
θ ref
modelsviareinforcementlearningobjective,e.g.,
Proximal Policy Optimization (Schulman et al., 3 Method
2017). A recent study propose Direct Preference
In this section, we first describe the proposed ap-
Optimization(DPO)(Rafailovetal.,2023)toavoid
proach. Then, we demonstrate how we integrate
explicitlytrainingarewardmodelsothatlanguage
an external calculator into the model’s decoding
models can be directly tuned with human prefer-
processwhichsignificantlyimprovesLMs’perfor-
encedata.
manceonthedownstreamtasks.
TheDPOpipelinecanbedescribedasfollows.
First,givensomepromptx,wesampleseveralcom-
3.1 DPO-augmentedSelf-Training
pletionsfromthereferencemodelπ (normallyit
ref Our approach starts with a warm-up stage, and
isthemodelaftersupervisedfine-tuning):
thenfollowedbyaniterativeprocess,whereeach
iterationiscomposedoftwosub-steps: DPOstep
y ,y ∼ π (·|x). (3)
1 2 ref and SFT step. The iterative process ends when
the model performance converges or reaches the
Next,constructtheDPOdatasetD fromthecom-
maximum iteration. A formal description of the
pletionsbasedonthehumanpreference:
proposedmethodisillustratedinAlgorithm2. An
illustrationofourmethodispresentedinFigure2.
D = {(xi, yi , yi )}N , (4)
w l i=1
Warm-up stage Like classic self-training, we
where yi and yi represent the winning and los- startbyfine-tuningthebasemodelf tooptimize
w l θ
ing completions respectively. Then, we optimize L (θ)onthelabeleddataL,resultinginanup-
SFT
thelanguagemodelπ tominimizeL (π ;π ) datedmodelf . Afterthisstage,weassumethat
θ DPO θ ref θ′Human-labeled
SFT data SFT data Pseudo-labeled data
Deduplication
Preference data
Sampling
&
filtering
Sampling
Pre-trained
SFT model DPO model
model Supervised DPO
fine-tuning training
Iteration n
Figure2: AnillustrationoftheDPO-augmentedSelf-Trainingframework. Traditionalself-trainingmethoduses
theSFTmodeltogeneratethepseudo-labelsforsubsequentiterations. Incontrast,ourmethodenhancestheSFT
modelwithDirectPreferenceOptimization(DPO),usingtheoptimizedDPOmodeltoproducethepseudo-labels.
Q: James writes a 3-page letter to 2
f is capable of solving certain math problems.
θ′
different friends twice a week. How many
Specifically,givenamathquestionx,f willgen-
θ′
pages does he write a year?
eratearationaleyˆwithansweraˆ.
A: He writes each friend
3*2=<<3*2=6>>6 pages a week.
Iterativestep1: DPOstep Inthisstep,wefirst
So he writes
samplerationalesyˆfromthefine-tunedmodelf
θ′
6*2=<<6*2=12>>12 pages every week.
given some questions x from U. For each ques-
That means he writes
tionx,wegeneratemultiplerationalestobuildthe
12*52=<<12*52=624>>624 pages a year.
DPOtrainingdatasetD. Asmentioned,formath
#### 624
problemsolvingtasks,itiseasytoknowwhethera
generatedrationaleyˆcanbeconsideredascorrect. Figure3: AnexamplefromtheGSM8Kdataset. The
We label rationales with correct answers as win- calculation annotations are highlighted in blue. All
ning completions, while consider rationales with calculation steps are wrapped within special tokens
<<...>>. Duringdecoding,thecalculatorwillbetrig-
incorrectanswersaslosingcompletions. Then,we
geredwhensuchpatternsexistandthemodel’soutput
train f on D to optimize the objective function
θ′
tokenswillbeoverriddenbythecalculatorresults. Fol-
L andgetaDPOmodelf intheend.
DPO θd lowingCobbeetal.(2021),thecalculationisperformed
withthein-builtpythonfunctioneval().
Iterativestep2: SFTstep Afterobtainingf ,
θd
weuseittogenerateanewpseudo-labeleddataset
S forthenext-roundsupervisedfine-tuning: 3.2 BatchDecodingwithCalculator
Empirical observations indicate that while large
S = {(x,yˆ)|x ∼ U,yˆ∼ f (·|x)} (6)
θd LMs, such as those described in Brown et al.
(2020),demonstratesuperiorproficiencyinbasic
Aftergeneration,wecleanS byeliminatingra- arithmeticcalculations,smallerLMslikeFlan-T5-
tionaleswithincorrectanswersandremovingdu- Largetendtostrugglewithsimilararithmetictasks.
plicates. Therefore,thepseudo-labeleddatasetwe This limitation significantly affects their perfor-
obtainedintheendisasubsetoftheoriginalone, mance in math reasoning tasks. To address this,
i.e.,Sα ⊂ S. Thefinaltrainingdatasetisthecom- various studies (Parisi et al., 2022; Schick et al.,
bination of the original labeled dataset L and the 2023;Kadlcˇíketal.,2023)haveexploredaugment-
newly-generated pseudo-labeled dataset Sα. No- ingsmall-scalemodelswithanexternalcalculator
ticethatduringthisprocess,oncewecollectanew to boost their arithmetic capabilities. However,
dataset, wetrainfromtheoriginalbasemodelf many of these existing methods are limited to a
θ
insteadofcontinuallyfine-tuningf toavoidover- batchsizeofoneduringdecoding. Thisconstraint
θ′
fitting,followingpreviouspractice(Zelikmanetal., substantiallyreducestheinferencespeedandlimits
2022;Singhetal.,2023). theirpracticalapplication.Dataset Split #Data
20 Calcformer 19.9x
Ours w/ calculator GSM8K(Cobbeetal.,2021) Train 6,705
Ours w/o calculator
15 13.9x Validation 0,768
12.3x Test 1,319
10 9.5x
MultiArith(RoyandRoth,2015) Test 0,600
6.9x
5.8x
5 ASDiv(Miaoetal.,2020) Test 2,096
SVAMP(Pateletal.,2021) Test 1,000
1.0x
0
Table 1: Statistics of the datasets used in our experi-
1 8 16 32 ments. TheoriginalGSM8Kdatasetonlycontainstrain
Batch Size
andtestsplit.Werandomlyselect768trainingexamples
Figure 4: Inference speed comparison between toconstructthevalidationdatasetinourexperiments.
our methods (w/ and w/o calculator) and Cal-
cformer(Kadlcˇíketal.,2023)withvaryingbatchsizes.
4.1 Setup
TheresultsaremeasuredonasingleNVIDIAA40GPU.
Basemodels WeemployFlan-T5models(Chung
etal.,2024)asourprimarybasemodels. Specifi-
To address this challenge, we propose a sim-
cally, we consider two variants from the Flan-T5
ple yet efficient method that allows for using
family: Flan-T5-BaseandFlan-T5-Large. Wese-
largerbatchsizesduringinferencewithanexternal
lect Flan-T5 over the original T5 models (Raffel
calculator. Our approach leverages the calcula-
et al., 2019) as our backbone models based on
tor annotations provided in the original GSM8K
theevidencefrompreviousresearch(Chungetal.,
dataset(Cobbeetal.,2021). Figure3demonstrates
2024; Fu et al., 2023), which demonstrates that
an example of this annotation and describes how
instruction-tunedmodelslikeFlan-T5outperform
suchannotationscanbeusedduringdecoding. For
theirpre-trainedcounterpartsinmathematicalrea-
optimalutilizationoftheseannotations,webuild
soningtasks. Tobroadenouranalysis,wealsoin-
our models with the Transformers library (Wolf
cludeLlamamodels(Touvronetal.,2023a,b;Meta,
et al., 2020). During inference, we employ a
2024)asadditionalbasemodelsforcomparison.
customized LogitsProcessor2–available in the
Transformersdocumentation–toadjustthemodel’s
Datasets ThelabeleddatasetLusedinourexper-
generationprocess. ThisLogitsProcessoractsas
imentscomesfromthetrainingsplitoftheGSM8K
aninterface,allowingmodificationstotheoutputs
dataset. OurunlabeleddatasetU isalsobuiltupon
of the model during generation and thereby en-
GSM8K’strainingdatabyremovingitsannotated
ablingefficientmanagementoflargerbatchsizes.
rationales. For evaluation, we consider three ad-
To demonstrate the efficiency of the proposed
ditionalcommonlyusedmathreasoningtasksbe-
solution, we compare the inference speed of our
sides GSM8K: MultiArith, ASDiv, and SVAMP.
methods(w/andw/ocalculator)basedonFlan-T5-
Table1providesthestatisticsinformationofeach
Large against an open-source tool-using method,
dataset. Following previous practice (Fu et al.,
Calcformer (Kadlcˇík et al., 2023) based on T5-
2023), we fine-tune our base models exclusively
Large,inFigure4. Wefindthatwhenthebatchsize
on the GSM8K training data while utilizing the
equals1,allthreemethodshaveasimilarinference
restthreedatasetstoevaluateourmodels’out-of-
speedofaround40tokenspersecond. However,as
domainperformanceastheydonothaveanofficial
theinferencebatchsizeincreases,thespeedupof
in-domaintrainingsplit.
ourmethodsincreasessignificantly.
4.2 ImplementationDetails
4 Experiments
Inthewarm-upstage,wefine-tunethebasemodels
Inthissection,wefirstoutlineourexperimentsetup onthetrainingsetofGSM8K(Cobbeetal.,2021)
andimplementationdetails,thenpresentourmod- with the original human-labeled annotations and
els’performanceonvariousmathreasoningtasks obtaintheinitialSFTmodel. ForsubsequentDPO
againstcompetitivebaselines. Finally,weanalyze steps, we first sample rationales from SFT mod-
theeffectivenessofourmethodempirically. els to build the preference dataset. We sample 5
rationales per question with a temperature of 0.7.
2https://huggingface.co/docs/transformers/
internal/generation_utils#logitsprocessor Generated rationales yˆcontaining the correct an-
pudeepSMethod BaseModel GSM8K MultiArith ASDiv SVAMP
SupervisedFine-Tuning Flan-T5-Base 18.1 54.2 26.2 19.5
Self-Training Flan-T5-Base 25.9 73.8 28.2 24.2
DPO-augSelf-Training(Ours) Flan-T5-Base 27.2 74.3 29.2 22.6
SupervisedFine-Tuning Flan-T5-Large 30.8 77.2 38.1 33.6
Self-Training Flan-T5-Large 35.6 86.2 42.5 34.8
DPO-augSelf-Training(Ours) Flan-T5-Large 37.4 89.0 42.8 36.8
Table2: Overallaccuracies(%)overfourmathwordproblemsolvingtasks. Inspiredbythepreviouspractice(Fu
etal.,2023),allthemodelsinthistableareonlytrainedwiththeGSM8Ktrainingset(Cobbeetal.,2021). Hence,
wereportthein-distributionperformanceforGSM8K,whilereportingtheout-of-distributionperformanceforthe
otherthreedatasets,i.e.,MultiArith,ASDiv,andSVAMP.
swerareclassifiedaswinningonesy ,whilethe
w Flan-T5-Base
restareconsideredlosingonesy l. Wesetβ = 0.1 30.0 ST
intheDPOlearningobjectiveL DPO. Forthesub- 27.5 Ours
26.0
26.6
25.9
27.2
sequent SFT steps, we generate 3 rationales per 25.0 24.2 24.6
questionfromtheDPO-tunedmodelf ,alsowith 22.5
θd
a temperature of 0.7. Only the correct generated 20.0
18.1 18.1
rationales yˆwill be selected to build the pseudo- 17.5
labeleddataset. ForbothDPOandSFTsteps,we 15.0
performsimplededuplicationbasedontheJaccard iter 0 iter 1 iter 2 iter 3
similarity scores with a threshold of 0.7. Addi-
Flan-T5-Large
tionalimplementationdetailscanbefoundinAp-
38 ST 37.4
pendixA. Ours
36 35.6 35.6
35.1
Baselines Wemainlyconsidertwobaselinemeth- 34.1
34
32.9
odstocomparewithourmethod: SupervisedFine-
32
Tuning (SFT) and Self-Training (ST). The SFT 30.8 30.8
baselinecorrespondstothemodelafterthewarm- 30
upstage. TheSelf-Trainingbaselineadherestothe iter 0 iter 1 iter 2 iter 3
procedureoutlinedinAlgorithm1. Toensureafair
Figure5: Theperformanceoftheproposedmethodon
comparisonbetweenourproposedmethodandthe
GSM8Koverthreeiterations.For“iter0”,wereportthe
STbaseline,weusethesamesetofhyperparame-
performanceoftheSFTbaselines,whichareobtained
tersforbothmethodsateachiteration.
afterthewarm-upstage.
4.3 MainResults
Effect of iterative training Figure 5 demon-
Comparisonwithbaselines Table2showsthe stratestheimpactofiterativetrainingonFlan-T5-
performance of our method compared with the Base and Flan-T5-Large models, comparing our
baselines using two base models, Flan-T5-Base method to the ST baseline. Initially, both meth-
and Flan-T5-Large, across four datasets. The re- ods start with a warm-up stage and have similar
sultsclearlyshowthatboththeSTbaselineandour accuracies at iteration 0. As training progresses,
proposed DPO-augmented Self-Training method ourmethodconsistentlyoutperformsSTacrossit-
outperformtheSFTbaselinebyalargemargin,in- erations for both models. For Flan-T5-Base, the
dicatingtheeffectivenessoftheself-trainingframe- accuracyimprovementplateausbyiteration3,sug-
workingeneral. AlthoughtheSTbaselinesmake gesting convergence. In contrast, Flan-T5-Large
significantimprovementsovertheSFTbaselines, shows a clear and steady improvement, with our
ourDPO-augmentedSelf-Trainingmodelsdemon- methodachievingsignificantlyhigheraccuracyby
strate enhanced performance on both in-domain iteration 3. This underscores the effectiveness of
(GSM8K)andout-of-domain(MultiArith,ASDiv, our iterative training process, particularly in en-
andSVAMP)tasks. hancingperformanceoflargermodels.
)%(
ycaruccA
)%(
ycaruccAMethod BaseModel #Annotations Annotator Tools Acc.
Supervisedfine-tuning
CoT(Shridharetal.,2023) GPT-2-Large 007K Human (cid:37) 14.1
Self-consistency(Khalifaetal.,2023) Flan-T5-Large 007K Human (cid:33) 33.3
GRACE(Khalifaetal.,2023) Flan-T5-Large 007K Human (cid:33) 36.3
Calcformer(Kadlcˇíketal.,2023) T5-Large 030K Human (cid:33) 34.2
KnowledgeDistillation
SocraticCoT(Shridharetal.,2023) GPT-2-Large 007K GPT-3175B (cid:37) 21.1
CoTfromCodeX(Fuetal.,2023) Flan-T5-Large 100K CodeX (cid:37) 20.2
CoTfromPaLM(Magisteretal.,2023) T5-Large 006K PaLM540B (cid:33) 22.2
Ours
DPO-augSelf-Training(K=3) Flan-T5-Large 007K Human (cid:33) 37.4
DPO-augSelf-Training(K=5) Flan-T5-Large 007K Human (cid:33) 39.1
DPO-augSelf-Training(K=10) Flan-T5-Large 007K Human (cid:33) 40.0
Table3: DetailedcomparisonamongexistingmethodswithcomparablemodelsizesontheGSM8Ktestset. The
“Annotator”columnindicateshowtherationalesofthetrainingdataaregenerated. Inthiscolumn,“Human”refers
tothelabelsfromtheoriginalGSM8Kdataset(Cobbeetal.,2021)thatarewrittenbyhumanannotators. The
“Tools”columnindicateswhetherexternalcalculatorsareappliedduringinference.
4.4 ComparisonwithExistingMethods ciesof44.7%and54.7%respectively. Furthermore,
ourmodelemployingthelatestLlama-3-8b(Meta,
Inthissection,wecompareourmethodswithex-
2024)matchesorexceedstheperformanceofear-
isting approaches. To enhance our method, we
lier models with knowledge distillation, demon-
increasethenumberofsampledpseudo-labelsper
stratingasignificantaccuracyof68.8%.
questiontobuildamorediverseandrobustpseudo-
labeldataset. WedenotethishyperparameterasK
followingYuanetal.(2023). Method BaseModel Acc.
Table 3 presents a detailed comparison be- Closed-sourcemodels
tween our method and exisiting methods using Claude-3-Opus(Anthropic,2024) - 95.0
a simialr base model size. The base models we Claude-2(Anthropic,2023) - 88.0
GPT-4(OpenAI,2023) - 92.0
considered include GPT-2-Large (Radford et al.,
Flan-PaLM-2(Aniletal.,2023) - 84.7
2019), T5-Large (Raffel et al., 2019), and Flan-
Open-sourcemodelsw/knowledgedistillation
T5-Large(Chungetal.,2024),eachwithapprox- MAmooTH(Yueetal.,2023)♡ Llama-2-7b 53.6
imately770millionparameters. AsshowninTa- LEMA(Anetal.,2023) Llama-2-7b 54.1
ble 3, our approach not only outperforms other WizardMath(Luoetal.,2023) Llama-2-7b 54.9
MetaMath(Yuetal.,2024) Llama-2-7b 66.5
methods on the GSM8K benchmark, but also
MuggleMath(Lietal.,2023a) Llama-2-7b 68.4
demonstrates remarkable label efficiency by ex-
ToRA(Gouetal.,2024)♡ Llama-2-7b 68.8
clusively using the annotations from the original
Open-sourcemodelsw/oknowledgedistillation
GSM8Kdataset.
SFT(Yuanetal.,2023) Llama-1-7b 35.9
InTable4,wefurtherevaluatetheeffectiveness SFTw/Calculator♡ Llama-1-7b 40.0
RFT(K=100)(Yuanetal.,2023) Llama-1-7b 41.7
oftheproposedmethodwiththeLlamamodelfam-
SFT(Yuanetal.,2023) Llama-2-7b 41.6
ily(Touvronetal.,2023a,b;Meta,2024),compar-
SFTw/Calculator♡ Llama-2-7b 45.1
ing it with several state-of-the-art closed-source RFT(K=100)(Yuanetal.,2023) Llama-2-7b 47.5
modelsaswellassimilarlysizedopen-sourcemod- SFTw/Calculator♡ Llama-3-8b 61.0
els. Weobserveasubstantialperformancegapbe- Ours
tweenproprietaryandopen-sourcemodels. Among DPO-ST(K=10)♡ Llama-1-7b 44.7
DPO-ST(K=10)♡ Llama-2-7b 54.7
theopen-sourcemodels,thoseutilizingknowledge
DPO-ST(K=10)♡ Llama-3-8b 68.8
distillationgenerallyoutperformtheircounterparts
without such enhancement. Notably, our models Table 4: Comparison with the state-of-the-art pro-
using Llama-1-7b and Llama-2-7b base models prietary models and Llama-based open-source mod-
surpassotheropen-sourcealternativesthatdonot els(Touvronetal.,2023a,b;Meta,2024). ♡: models
employ knowledge distillation, achieving accura- augmentedwithexternaltools.# generated
Pass@1 66 Pass@10 CoT pseudo-labels 50 w/ calculator
w/o calculator
39 64.8 3000 2940 43.9 44.8
40.5
40
36.1 36.5 64 36.7 36
62.9
2700 30
33 62
2495 20 16.3 17.1 17.8 18.0
30 60 2400
10
Before DPO step After DPO step iter 0 iter 1 iter 2 iter 3
Figure 6: Effects of the DPO step. Left: we report Figure7: GSM8KdevelopmentsetaccuracyofFlan-
thegreedydecodingresultsforPass@1. Middle: For T5-Largewithandwithouttheuseofanexternalcalcu-
Pass@10,thesolutionsaresampledwithtemperature latorduringinference.
0.7. Right: Wecountthenumberofgeneratedpseudo-
labelsafterdeduplication.
weconductanablationstudybyomittingthecal-
culator and present the findings in Figure 7. Our
4.5 EffectsoftheDPOStep
resultsindicatethatdecodingwithoutthecalcula-
Asmentionedearlier,themaindifferencebetween
tormarkedlyreducesaccuracyacrossalliterations.
theproposedmethodandtheclassicself-training
Webelievethatthisisbecausemodelswillgenerate
istheDPOstepineveryiterativeprocess. Wenow
largeamountoffalsepositivepseudo-labelswith-
analyzehowtheDPOstepsimproveself-training.
outcalculator,thatis,thegeneratedpseudo-labels
Figure6comparestheperformanceofmodelsbe-
mayhavecorrectfinalanswersbutmakeerrorsin
foreandaftertheDPOstepinthefirstiterationon
theintermediatereasoningsteps.
thePass@Kmetrics. Pass@Kmeasurestheproba-
bilitythatatleastoneoftheK generatedsolutions
foraproblemiscorrect, whichservesasagauge 5 RelatedWork
for both the quality and the variety of the model-
generated solutions. The models we investigate Learningfrompseudo-labels Supervisedfine-
herearefine-tunedfromtheFlan-T5-Large. tuning (SFT) is prevalent technique employed to
AsshowninFigure6,theDPOstepyieldsonly enhancetheperformanceofpre-trainedlanguage
marginalimprovementsovertheSFTmodelinthe models on specific downstream tasks (Ouyang
Pass@1performanceonthedevelopmentset. How- et al., 2022; Chung et al., 2024). However, this
ever,theperformancesignificantlyimproveswhen methodheavilydependsontheavailabilityofhigh-
multiplerationales,i.e.,10solutionsperquestion, qualitylabeleddata,whichcanbebothexpensive
aresampledwithtemperature0.7(measuredwith andlabor-intensivetoprocure(Brownetal.,2020).
thePass@10metric). ThisindicatesthattheDPO Toaddressthislimitation,variousstrategieshave
trainingobjectivemakeslanguagemodelsinclined been developed to generate high-quality pseudo-
togeneraterationalesofbothhighqualityanddi- labelsusingeitherunlabeledorsyntheticdatafor
versity. Wealsocomparethenumberofgenerated a wide range of applications, including text clas-
rationales on the training set L for models with sification (Xie et al., 2020), sentence representa-
andwithouttheDPOstep. Figure6(right)clearly tion learning (Wang and Lu, 2022), instruction
showsthatthemodelaftertheDPOstepcanpro- tuning (Honovich et al., 2022), and math reason-
ducemoreSFTdataforthenextiteration. ing (Wang and Lu, 2023). Recent advancements
inthisareaprimarilyfocusontwodirections: self-
4.6 EffectsofExternalCalculator
training and knowledge distillation. The key dif-
Driven by the observation that small-scale LMs ference between these methods lies in the source
frequently make basic calculation errors, we de- ofthepseudo-labels: self-trainingusesthemodel’s
velopasimpleyetefficientmethodthatintegrates own predictions on unlabeled data, while knowl-
an external calculator into the models’ decoding edge distillation utilizes the insights from larger,
process. Toevaluatetheimpactofthisintegration, morepowerfulmodels.
)%(
ycarucca
veD
)%(
ycarucca
veD
)%(
ycarucca
veDSelf-traininginlanguagemodel Recently,we 6 Conclusion
have witnessed a large number of works focus-
ingonself-trainingalgorithmsforlanguagemod- We present an effective and resource-efficient
els (He et al., 2020; Zelikman et al., 2022; Yuan method called DPO-augmented Self-Training
et al., 2023). Most of such methods are built (DPO-ST), which augments the original Self-
upon the classic self-training framework (Scud- Training algorithm with Direct Preference Opti-
der, 1965). He et al. (2020) empirically studied mization(Rafailovetal.,2023). Unlikeprevious
the effectiveness of self-training in natural lan- studiesthatimprovesmall-scalelanguagemodels’
guage generation tasks, e.g., summarization and reasoningabilitiesbydistillingalargerandmore
translation. Zelikman et al.(2022) proposedself- powerfulmodel,wearguethatsmallmodelsthat
taughtreasoner(STaR),whichdemonstratedthat are trained merely on the limited human-labeled
languagemodelscanbeiterativelyimprovedfrom datacanimprovethemselvessignificantly. Wealso
its own generation, even there are no gold ratio- empiricallyfindthatmodelstrainedwithDPOloss
nales provided. Yuan et al. (2023) proposed re- cangeneratepseudo-labeleddatawithhigherqual-
jectionsamplingfine-tuningtoimprovelanguage ityanddiversity. Ourexperimentsdemonstratethat
models’mathreasoningabilities. Thismethodcan the proposed method not only outperforms exist-
be interpreted as only executing one iteration of ingmethodswithcomparablemodelsizesonthe
theself-trainingalgorithm. Singhetal.(2023)pro- GSM8Kbenchmark,butalsoachievesremarkable
posedReSTEM,aself-improvingalgorithmbased
resourceefficiencyintermsofbothcomputational
on expectation-maximization framework. This costandtherequirementsofhuman-labeleddata.
methoddemonstratessignificantimprovementsin
problem-solving tasks, e.g., math reasoning and
Limitations
codegeneration.
Useofunlabeleddata Ourmethodisbuiltupon
Knowledge distillation from LLMs Many of
theclassicself-trainingalgorithm,whichprovides
therecentresearcheffortsdemonstratedlargelan-
an effective semi-supervised learning framework
guagemodels(LLMs)arecapableofperforming
capableofutilizingunlabeleddataefficiently. How-
mathreasoning(Weietal.,2022b;Gaoetal.,2022;
ever, this work doesn’t explore the use of unla-
OpenAI,2023;Aniletal.,2023). Asaresult,there
beleddataexplicitly. Futureresearcheffortscanbe
isgrowinginterestinenhancingthereasoningabil-
madetoexplorehowtocollecthigh-qualityunla-
itiesofsmallerlanguagemodelsbydistillingchain-
beleddataformathwordproblemsolving. Inother
of-thought pseudo-labels from LLMs. (Ho et al.,
words,weneedtofindanefficientmethodforcol-
2023; Magister et al., 2023; Fu et al., 2023). For
lecting unlabeled data U = {(x ,a )}u that for
example, Luo et al. (2023) proposed Reinforce- i i i=1
each math question x , there is a corresponding
mentLearningfromEvol-InstructFeedbackbuilt i
ground-truth answer a , ensuring the data’s rele-
upontheEvol-Instructframework(Xuetal.,2023), i
vanceandutilityforenhancingmodeltraining.
whichrequiresChatGPTtoprovidethetrainingsig-
nals. Anetal.(2023)demonstratedthatlanguage
modelscaneffectivelylearnfromthemistakesthat Generalization to other tasks One of the lim-
canbecorrectedbyLLMsduringsupervisedfine- itations of this work is the narrow scope of our
tuning. Althoughthesemethodsareshowntohave experiments,whichwereexclusivelyconductedon
promisingexperimentalresults,theyarecostlyto mathreasoningtasks. Theprimaryreasonforthis
implementaslargemodelscostmoreFLOPsdur- limitation is the lack of appropriate training data
inginference. Ourworkdemonstratesthatsmall- forotherreasoningtasks. Asourmethodrequires
scalelanguagemodelscaneffectivelylearnfrom asetoftrainingdatawithchain-of-thoughtlabels,
their own generations, offering a more resource- many existing reasoning tasks lack such annota-
efficientalternativetoknowledgedistillation. Since tions,makingitchallengingtoextendourexperi-
our method is conceptually orthogonal to knowl- ments beyond the current scope. Future research
edgedistillationtechniques,aninterestingavenue mayfocusonidentifyinganddevelopingsuitable
forfutureresearchwouldbetoexploreintegrating datasets for a wider range of reasoning tasks to
knowledge distillation into our iterative training fullyevaluatetheapplicabilityandeffectivenessof
processtofurtherenhancemodelperformance. ourmethodacrossdifferentreasoningtasks.Acknowledgements Neelakantan,PranavShyam,GirishSastry,Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
This work was done when Shichen Li was a vis- Gretchen Krueger, et al. 2020. Language models
iting student at the StatNLP Research Group of arefew-shotlearners. InProceedingsofNeurIPS.
SUTD.Wewouldliketothanktheanonymousre-
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
viewers,ourmeta-reviewer,andseniorareachairs
Yuan,HenriquePondedeOliveiraPinto,JaredKa-
for their constructive comments and support on plan, HarriEdwards, YuriBurda, NicholasJoseph,
this work. This research/project is supported by Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
Ministry of Education, Singapore, under its Aca-
arXiv:2107.03374.
demicResearchFund(AcRF)Tier2Programme
(MOEAcRFTier2AwardNo: MOET2EP20122- AakankshaChowdhery,SharanNarang,JacobDevlin,
0011), the National Research Foundation Singa- MaartenBosma,GauravMishra,AdamRoberts,Paul
Barham,HyungWonChung,CharlesSutton,Sebas-
pore and DSO National Laboratories under the
tianGehrmann,etal.2022. Palm: Scalinglanguage
AISingaporeProgram(AISGAwardNo: AISG2-
modelingwithpathways. JournalofMachineLearn-
RP-2020-016),andMinistryofEducation,Singa- ingResearch.
pore,underitsTier3Programme(TheAwardNo.:
HyungWonChung,LeHou,ShayneLongpre,Barret
MOET320200004). Any opinions, findings and
Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi
conclusionsorrecommendationsexpressedinthis
Wang,MostafaDehghani,SiddharthaBrahma,etal.
materialarethoseoftheauthorsanddonotreflect 2024. Scalinginstruction-finetunedlanguagemodels.
theviewsofthefundingagencies. JournalofMachineLearningResearch,25(70):1–53.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
MarkChen,HeewooJun,LukaszKaiser,Matthias
References
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano,etal.2021. Trainingverifierstosolvemath
Massih-Reza Amini, Vasilii Feofanov, Loïc
wordproblems. arXivpreprintarXiv:2110.14168.
Pauletto, Emilie Devijver, and Yury Maximov.
2022. Self-training: A survey. arXiv preprint
StanleyC.Fralick.1967. Learningtorecognizepatterns
arXiv:2202.12040.
withoutateacher. IEEETrans.Inf.Theory.
ShengnanAn,ZexiongMa,ZeqiLin,NanningZheng,
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and
Jian-Guang Lou, and Weizhu Chen. 2023. Learn-
Tushar Khot. 2023. Specializing smaller language
ingfrommistakesmakesllmbetterreasoner. arXiv
modelstowardsmulti-stepreasoning. InProceedings
preprintarXiv:2310.20689.
ofICML.
RohanAnil,AndrewMDai,OrhanFirat,MelvinJohn-
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
son, Dmitry Lepikhin, Alexandre Passos, Siamak
PengfeiLiu, YimingYang, JamieCallan, andGra-
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
ham Neubig. 2022. Pal: Program-aided language
Chen, et al. 2023. Palm 2 technical report. arXiv
models. arXivpreprintarXiv:2211.10435.
preprintarXiv:2305.10403.
Anthropic.2023. Claude2. https://www.anthropic. ZhibinGou,ZhihongShao,YeyunGong,YujiuYang,
com/news/claude-2. Accessed: 2024-05-06. MinlieHuang,NanDuan,WeizhuChen,etal.2024.
Tora: A tool-integrated reasoning agent for mathe-
Anthropic. 2024. The claude 3 model family: Opus, maticalproblemsolving. InProceedingsofACL.
sonnet,haiku. Accessed: 2024-05-06.
Caglar Gulcehre, Tom Le Paine, Srivatsan Srini-
ZhangirAzerbayev,HaileySchoelkopf,KeiranPaster, vasan,KseniaKonyushkova,LotteWeerts,Abhishek
MarcoDosSantos,StephenMcAleer,AlbertQJiang, Sharma, Aditya Siddhant, Alex Ahern, Miaosen
JiaDeng,StellaBiderman,andSeanWelleck.2024. Wang, Chenjie Gu, et al. 2023. Reinforced self-
Llemma: Anopenlanguagemodelformathematics. training(rest)forlanguagemodeling. arXivpreprint
InProceedingsofICLR. arXiv:2308.08998.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda JunxianHe,JiataoGu,JiajunShen,andMarc’Aurelio
Askell, AnnaChen, NovaDasSarma, DawnDrain, Ranzato. 2020. Revisiting self-training for neural
StanislavFort,DeepGanguli,TomHenighan,etal. sequencegeneration. InProceedingsofICLR.
2022. Trainingahelpfulandharmlessassistantwith
reinforcementlearningfromhumanfeedback. arXiv DanHendrycks,CollinBurns,SauravKadavath,Akul
preprintarXiv:2204.05862. Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021. Measuring mathematical
Tom Brown, Benjamin Mann, Nick Ryder, Melanie problemsolvingwiththemathdataset. InProceed-
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind ingsofNeurIPS.NamgyuHo,LauraSchmid,andSe-YoungYun.2023. Miller,MaddieSimens,AmandaAskell,PeterWelin-
Large language models are reasoning teachers. In der,PaulFrancisChristiano,JanLeike,andRyanJ.
ProceedingsofACL. Lowe. 2022. Training language models to follow
instructionswithhumanfeedback. InProceedingsof
OrHonovich,ThomasScialom,OmerLevy,andTimo NeurIPS.
Schick. 2022. Unnatural instructions: Tuning lan-
guagemodelswith(almost)nohumanlabor. arXiv AaronParisi,YaoZhao,andNoahFiedel.2022. Talm:
preprintarXiv:2212.09689. Tool augmented language models. arXiv preprint
arXiv:2205.12255.
MarekKadlcˇík,MichalŠtefánik,OndˇrejSotoláˇr,and
VlastimilMartinek.2023. Calc-xandcalcformers: Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
Empoweringarithmeticalchain-of-thoughtthrough 2021. AreNLPmodelsreallyabletosolvesimple
interactionwithsymbolicsystems. InProceedings mathwordproblems? InProceedingsofNAACL.
ofEMNLP.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Muhammad Khalifa, Lajanugen Logeswaran, Moon- DarioAmodei,andIlyaSutskever.2019. Language
taeLee,HonglakLee,andLuWang.2023. Grace: modelsareunsupervisedmultitasklearners. OpenAI
Discriminator-guidedchain-of-thoughtreasoning. In blog.
FindingsofEMNLP.
RafaelRafailov,ArchitSharma,EricMitchell,Stefano
TakeshiKojima,ShixiangShaneGu,MachelReid,Yu- Ermon,ChristopherDManning,andChelseaFinn.
takaMatsuo,andYusukeIwasawa.2022. Largelan- 2023. Directpreferenceoptimization:Yourlanguage
guagemodelsarezero-shotreasoners. InProceed- modelissecretlyarewardmodel. InProceedingsof
ingsofNeurIPS. NeurIPS.
ChengpengLi,ZhengYuan,GuantingDong,Keming ColinRaffel,NoamM.Shazeer,AdamRoberts,Kather-
Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and ine Lee, Sharan Narang, Michael Matena, Yanqi
ChangZhou.2023a. Queryandresponseaugmenta- Zhou,WeiLi,andPeterJ.Liu.2019. Exploringthe
tioncannothelpout-of-domainmathreasoninggen- limitsoftransferlearningwithaunifiedtext-to-text
eralization. arXivpreprintarXiv:2310.05506. transformer. JournalofMachineLearningResearch.
YifeiLi,ZeqiLin,ShizhuoZhang,QiangFu,BeiChen, SubhroRoyandDanRoth.2015. Solvinggeneralarith-
Jian-GuangLou,andWeizhuChen.2023b. Making meticwordproblems. InProceedingsofEMNLP.
language models better reasoners with step-aware
verifier. InProceedingsofACL. TimoSchick,JaneDwivedi-Yu,RobertoDessì,Roberta
Raileanu,MariaLomeli,LukeZettlemoyer,Nicola
Ilya Loshchilov and Frank Hutter. 2019. Decoupled Cancedda,andThomasScialom.2023. Toolformer:
weightdecayregularization. InProceedingsofICLR. Languagemodelscanteachthemselvestousetools.
InProceedingsofNeurIPS.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-
guangLou,ChongyangTao,XiuboGeng,Qingwei John Schulman, Filip Wolski, Prafulla Dhariwal,
Lin,ShifengChen,andDongmeiZhang.2023. Wiz- Alec Radford, and Oleg Klimov. 2017. Proxi-
ardmath: Empoweringmathematicalreasoningfor malpolicyoptimizationalgorithms. arXivpreprint
large language models via reinforced evol-instruct. arXiv:1707.06347.
arXivpreprintarXiv:2308.09583.
H.J.Scudder.1965. Probabilityoferrorofsomeadap-
Lucie Charlotte Magister, Jonathan Mallinson, Jakub tivepattern-recognitionmachines. IEEETrans.Inf.
Adamek, Eric Malmi, and Aliaksei Severyn. 2023. Theory.
Teachingsmalllanguagemodelstoreason. InPro-
Kumar Shridhar, Alessandro Stolfo, and Mrinmaya
ceedingsofACL.
Sachan.2023. Distillingreasoningcapabilitiesinto
Meta. 2024. Llama 3. https://llama.meta.com/ smallerlanguagemodels. InFindingsofACL.
llama3/. Accessed: 2024-06-01.
AviSingh,JohnDCo-Reyes,RishabhAgarwal,Ankesh
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. Anand, Piyush Patil, Peter J Liu, James Harri-
2020. Adiversecorpusforevaluatinganddeveloping son, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al.
englishmathwordproblemsolvers. InProceedings 2023. Beyond human data: Scaling self-training
ofACL. for problem-solving with language models. arXiv
preprintarXiv:2312.06585.
OpenAI.2023. Gpt-4technicalreport. arXivpreprint
arXiv:2303.08774. HugoTouvron,ThibautLavril,GautierIzacard,Xavier
Martinet,Marie-AnneLachaux,TimothéeLacroix,
LongOuyang,JeffWu,XuJiang,DiogoAlmeida,Car- BaptisteRozière,NamanGoyal,EricHambro,Faisal
rollL.Wainwright,PamelaMishkin,ChongZhang, Azhar, et al. 2023a. Llama: Open and effi-
SandhiniAgarwal,KatarinaSlama,AlexRay,John cient foundation language models. arXiv preprint
Schulman, Jacob Hilton, Fraser Kelton, Luke E. arXiv:2302.13971.Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wen-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay hao Huang, Huan Sun, Yu Su, and Wenhu Chen.
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti 2023. Mammoth: Buildingmathgeneralistmodels
Bhosale, et al. 2023b. Llama 2: Open founda- through hybrid instruction tuning. arXiv preprint
tion and fine-tuned chat models. arXiv preprint arXiv:2309.05653.
arXiv:2307.09288.
EricZelikman,YuhuaiWu,JesseMu,andNoahGood-
man.2022. Star: Bootstrappingreasoningwithrea-
Tu Vu, Minh-Thang Luong, Quoc Le, Grady Simon,
soning. InProceedingsofNeurIPS.
andMohitIyyer.2021. STraTA:Self-trainingwith
task augmentation for better few-shot learning. In
ProceedingsofEMNLP. A AdditionalImplementationDetails
Our models are trained using the AdamW opti-
Ben Wang and Aran Komatsuzaki. 2021. GPT-J-
6B: A 6 Billion Parameter Autoregressive Lan- mizer(LoshchilovandHutter,2019)withaweight
guageModel. https://github.com/kingoflolz/ decayof0.01andgradientclippingof1.0. Weem-
mesh-transformer-jax.
ployacosinelearningrateschedulewithwarm-up.
During training, the maximum sequence lengths
TianduoWangandWeiLu.2022. Differentiabledata
augmentationforcontrastivesentencerepresentation are set to 500 for T5 models and 640 for Llama
learning. InProceedingsofEMNLP. models. BothT5andLlamamodelsundergoDPO-
STforthreeiterations,usingthesamesetofhyper-
TianduoWangandWeiLu.2023. Learningmulti-step
parametersforeachiterationasdetailedinTable5.
reasoningbysolvingarithmetictasks. InProceed-
ForeachDPOstep,wesample5pseudo-labelsper
ingsofACL.
question from the SFT model to build the DPO
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, trainingdata,andsetβ = 0.1duringDPOtraining.
Barret Zoph, Sebastian Borgeaud, Dani Yogatama, In SFT steps, the number of model-generated so-
MaartenBosma,DennyZhou,DonaldMetzler,etal.
lutions per question can be varied and controlled
2022a. Emergentabilitiesoflargelanguagemodels.
bythehyperparameterK. Whensamplingpseudo-
TransactionsonMachineLearningResearch.
labels,welimitthemaximumgeneratedtokensto
JasonWei,XuezhiWang,DaleSchuurmans,Maarten 300anduseatemperatureof0.7.
Bosma,EdChi,QuocLe,andDennyZhou.2022b.
Chainofthoughtpromptingelicitsreasoninginlarge
Flan-T5 LLaMA
languagemodels. InProceedingsofNeurIPS.
Hyperparameters SFT DPO SFT DPO
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Batchsize 96 96 128 128
Chaumond,ClementDelangue,AnthonyMoi,Pier-
Epochs 8 - 2 -
ric Cistac, Tim Rault, et al. 2020. Transformers:
Maxsteps - 150 - 100
State-of-the-artnaturallanguageprocessing. InPro-
Learningrate 3e-4 7e-7 2e-5 3e-7
ceedingsofEMNLP.
Warm-upratio 0.1 0.1 0.03 0.03
QizheXie,ZihangDai,EduardHovy,ThangLuong,and
Table 5: Training details of SFT and DPO steps for
QuocLe.2020. Unsuperviseddataaugmentationfor
consistencytraining. InProceedingsofNeurIPS. Flan-T5andLlamamodels.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
PuZhao,JiazhanFeng,ChongyangTao,andDaxin
Jiang. 2023. Wizardlm: Empowering large lan-
guagemodelstofollowcomplexinstructions. arXiv
preprintarXiv:2304.12244.
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,
ZhengyingLiu,YuZhang,JamesTKwok,Zhenguo
Li, Adrian Weller, and Weiyang Liu. 2024. Meta-
math: Bootstrapyourownmathematicalquestions
forlargelanguagemodels. InProceedingsofICLR.
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting
Dong, ChuanqiTan, andChangZhou.2023. Scal-
ing relationship on learning mathematical reason-
ing with large language models. arXiv preprint
arXiv:2308.01825.