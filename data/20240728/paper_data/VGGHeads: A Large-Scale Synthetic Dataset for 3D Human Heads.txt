VGGHeads: A Large-Scale Synthetic Dataset for 3D
Human Heads
OrestKupyn
UniversityofOxford
Pi√±ataFarmsAI
EugeneKhvedchenia ChristianRupprecht
UkrainianCatholicUniversity UniversityofOxford
Abstract
Humanheaddetection,keypointestimation,and3Dheadmodelfittingareimpor-
tanttaskswithmanyapplications. However,traditionalreal-worlddatasetsoften
suffer from bias, privacy, and ethical concerns, and they have been recorded in
laboratoryenvironments,whichmakesitdifficultfortrainedmodelstogeneralize.
Here, we introduce VGGHeads- a large scale synthetic dataset generated with
diffusionmodelsforhumanheaddetectionand3Dmeshestimation. Ourdataset
comprises over 1 million high-resolution images, each annotated with detailed
3Dheadmeshes, faciallandmarks, andboundingboxes. Usingthisdatasetwe
introduceanewmodelarchitecturecapableofsimultaneousheadsdetectionand
headmeshesreconstructionfromasingleimageinasinglestep.Throughextensive
experimentalevaluations,wedemonstratethatmodelstrainedonoursyntheticdata
achieve strong performance on real images. Furthermore, the versatility of our
datasetmakesitapplicableacrossabroadspectrumoftasks,offeringageneraland
comprehensiverepresentationofhumanheads. Additionally,weprovidedetailed
informationaboutthesyntheticdatagenerationpipeline,enablingittobere-used
forothertasksanddomains. Thedataset,code,andmodelareavailablehere.
1 Introduction
Inrecentyears,thedemandforhigh-qualitydatasetshassurgedacrossvariousdomainsofcomputer
vision,particularlyfortasksinvolvinghumanheadrepresentation. Accurateheadmodelingiscrucial
forapplicationsrangingfromfacialrecognition[10]andanimation[59]toaugmentedrealityand
medicalimaging. Traditionaldatasetsoftenfallshort,eitherfocusingnarrowlyonspecificaspects
like facial landmarks [69] or offering limited resolution and annotation types. Historically, head
processingtypicallybeginswithfacedetection,followedbylandmarkdetectionineachdetected
face box. However, face bounding box detection, while useful for certain applications, does not
generalizewellto3Dreconstructiontasks. Thisisbecauseboundingboxmethodstypicallyoperate
onalimitedrangeofposes,failingtocapturethefullvariabilityandcomplexityofheadorientations
andstructuresneededforaccurate3Dmodeling. Moreover,real-worlddatasetscomewithsignificant
challengesrelatedtoprivacy,ethics,andconsent,makingitdifficulttocompileandusesuchdata
withoutlegalandethicalconcerns. Forexample,someofthewell-establishedbenchmarks,suchas
MS-Celeb[16]havebeenwithdrawntolicensingissues. Arealisticsyntheticheaddatasetcanhelp
mitigateethicalandprivacyrisks.
Inthiswork,weaddressthemainbottleneckinhumanheadunderstandingresearchbyconstructing
alarge-scalediversedatasetwiththehelpoflatentdiffusionmodels. Combinedwiththepseudo
groundtruth3Dmorphablemodel(3DMM)headparameters,thedatasetpresentsageneralhuman
Preprint.Underreview.
4202
luJ
52
]VC.sc[
1v54281.7042:viXraFigure1: DatasetExamples. Thedatasetcontainsawidevarietyofscenes,numbersofpeople,and
providesrichannotationsforeveryhumanhead.
headrepresentationthatincludes3Dheadposeandshape,expression,headandfaceboundingboxes,
and2Dfaciallandmarks. Wedemonstratethatconditioningthegenerationon2Dbodyskeletons
extractedfromalargedatasetofrealimagesissufficientbothtoremovetheprivateinformationand
tomodelawidevarietyofin-the-wildscenes. Ourcontributionsareasfollows:
Large-Scale Synthetic Dataset: We introduce a synthetic dataset with over 1 million images,
witheachheadinanimageannotatedwithadetailed3Dheadmesh. Thisdatasetprovidesarich
and diverse representation for training and evaluating models in various head-related tasks. Our
experimentsvalidatethequality,accuracy,andreal-worldgeneralizationofoursyntheticdata.
Novel Model: We introduce a novel architecture and model trained on this dataset, capable of
reconstructingmulti-headmeshesfromasingleinputimage.Thismodelleveragestherichannotations
inourdatasettosimultaneouslyoptimizeboundingboxes,3Dvertices,rotations,and2Dlandmarks
toachieverobustandaccuratereconstructions.
GeneralizedRepresentation: Weconductanextensiveexperimentalevaluationtodemonstratethe
accuracyandeffectivenessofourdatasetandmodel. Firstly,weshowthatmodelstrainedondata
fromdiffusionmodelsachievestrongperformanceonrealimages,indicatingthegeneralizabilityof
oursyntheticdataset. Themodeltrainedonourdatarecoversageneralheadrepresentationthatis
applicabletomultipledownstreamtasks. Weillustratethisversatilitybyapplyingthemodeltoawide
rangeoftasks. Themodeltrainedonourdatasetachievescomparableorsuperiorperformanceto
state-of-the-artmethodsin3DHeadPoseEstimation,3DMeshEstimation,FaceandHeadDetection,
HeadAlignment,andothers.
2 RelatedWork
HeadDetection: Thesimplestwaytorepresentaheadiswithafaceboundingbox. Initially,face
detectors used hand-crafted features [9; 51]. However, the rise of deep learning, particularly its
effectivenessinenhancingobjectdetectionaccuracy[44],hassignificantlyreshapedthefield.Current
methodologiescanbebroadlycategorizedbasedontheirutilizationofregionproposalnetworksinto
single-stage[23;33]andtwo-stagemethods[44]. Single-stageapproaches,suchasS3FD[64],or
PyramidBox[49],introduceaframeworktodetectsmallerfacialstructuresefficiently. Two-stage
methodologies,basedonFasterR-CNN[44]andR-FCN[8],integratemulti-scaletechniquesand
novelpoolingstrategiestoenhancedetectionperformance[62;53]. Theseapproachesare,however,
limitedtodetectingonlyafacialareaandarenotrobustonsideandout-of-distributionposes.
HollywoodHeads[52]extendsthetasktofullheaddetection,releasingadatasetwithmoviescenes
andamethodtodetectthehumanheadacrossvariousposes. Similarly,SCUT-Head[41]introduces
adatasetforheaddetectionof4405imagessampledfromvideosofclassroomsandacascadesingle-
stagedetectionmodel. Yet,bothdatasetsarelimitedtospecificscenes,notreflectingareal-world
distribution.
Recently,RetinaFace[11]improveddetectionaccuracybyproposingmulti-tasklearningofbounding
boxandfaciallandmarks,makingafirststeptowardspredictingamoregeneralheadrepresentation.
img2pose[3]directlyregress6DoFfacepose,recoveringbothfaceboundingboxand3Dheadpose
with a small set of parameters. Still, these methods only extend the detection task to predicting
2headposeorasmallsetoffaciallandmarks,whichmightnotbesufficientforawiderangeofhead
modelingmethods.
3DMorphableModel: Early3DMorphableFaceModels(3DMM)providedageneralpurposehead
representation[4]. A3DMMisastatisticalmodelof3Dfacialshapesandtexturesbuiltfromscanned
faces,allowingrealistic3Dfacialmodelstobecreatedandmanipulatedwithafewparameters. The
BaselFaceModel(BFM)[40]usesmatrixdecompositiontorepresentfaceswithparametersderived
from200scans. Morerecentmodels,suchasFLAME[29],coverthefull3Dheadmeshandare
trainedonamuchlargerdatasetofscans.
Recently,severalmethodshaveaddressedregressing3DMMparametersfromheadcrops. RingNet
[47]estimates3Dfaceshapefromasingleimagewithoutdirect3Dsupervision,tacklingthelackof
groundtruthdata. 3DDFA[14]usesaCascadedCNNtopredictadense3DMMfromafacialimage,
while3DDFA-v2[15]improvesthiswithmeta-jointoptimization. DAD-3DHeads[37]introduces
thefirst3Dheaddatasetinthewildandamodelfor3DFLAMEmeshreconstructionwithdirect3D
supervision.
Althoughthesemethodspredictrichheadrepresentations,theyrequiretightlycroppedheadimages
andcanbecomputationallyexpensive,especiallywhenappliedserially. RetinaFace[11]isthefirst
torecover3DFacialLandmarkinanend-to-endmanner. Yet,themethodislimitedtothefacial
areaonlyandutilizesonly5ground-truth2Dfaciallandmarkstogeneratepseudoground-truth3D
information,lackingconsistencyonhardsamples.
Thelackoflarge-scaledatasetsfordirectheadmeshregressioninthewildandthecomplexityofthe
taskduetothehigh-dimensionalrepresentationlimitstheprogressinheadmodeling. VGGHeads
efficientlyaddressestheseissuesbyintroducingalarge-scalesyntheticdatasetwithdense3Dhead
annotationsand,tothebestofourknowledge,thefirstmodeltodirectlyrecovermulti-personhead
meshesfromanimage.
SyntheticDataGeneration: Earlysyntheticdatagenerationmethodsused3Drenderingenginesto
address2Dvisionproblems. Still,theseapproacheswereconstrainedbythedomainof3Dmodels
andrequiredmodificationsforeachdatasetandsubtask. Forexample,VirtualKITTI[5]islimited
tostreetdrivingscenes. Morerecentmethodsusinggenerativeadversarialnetworks(GANs)[58]
offergreaterflexibilityandgeneralizationtoreal-worldimages. However,theseapproachesprimarily
samplefromtheoriginaldatasetdistribution,limitingtheirabilitytoincorporatenewinformation.
RecentmethodsbasedondiffusionmodelssuchasDiffumask[57]andDatasetDM[56]generate
synthetic images and annotations for tasks such as semantic segmentation and depth prediction.
InstanceAugmentation[25]generateseparateobjectsinanimage,providingaframeworktoaugment
andanonymizedatasets. However,thesemethodsstillrequirefine-tuningforspecificdatasetsand
havelimitedapplicationtotaskswithalowamountofgroundtruthdata.
SyntheticFaceData: Thesyntheticfacegenerationmethodstypicallyutilizerenderingengines.
FakeItTillYouMakeIt[54]releasesthefirstlarge-scalesyntheticdatasetcombiningaprocedurally
generatedparametric3Dfacemodelwithacomprehensivelibraryofhand-craftedassetstorender
trainingimageswithcorresponding3Dheadmeshand2Dfaciallandmarks. Yet,thedatasetconsists
of only tight face crops and requires manually generating mesh textures, limiting its scalability.
Manipulating2Dimagescanbeanalternativetousinga3Dgraphicspipeline. [69]fita3DMM
tofaceimagesandwarpthemtoaugmenttheheadpose. [39]compositehandimagesontofaces
toimprovefacedetection. Theseapproachescanonlymakeminoradjustmentstoexistingimages,
limitingtheiruse. Incontrast,VGGHeadsdirectlysamplesfromawidedistributionoflargediffusion
modelsandeasilyscalestoanarbitrarynumberoftrainsamplesandtypesofscenes.
3 Dataset
Ourgoalistocreateadatasetconsistingofimage-labelpairs,wherethelabelforeachimageisaset
ofheadboundingboxesand3Dheadmodelparameters. Weutilizelargepre-trainedlatentdiffusion
models[42]togenerateimages,abinaryheaddetectionmodeltrainedongeneratedsamples,andan
off-the-shelf3Dheadalignmentmodel[37]topredict3DMMparametersforeachgeneratedhead.
3Figure2: DataGeneration: Thepredicted2Dbodypose[2]andscenedescription[28]condition
theimagegenerationprocess. Binarydetectionmodelpredictsheadboundingboxesand3DMM
regressor[37]generatesfinalannotationforeachheadcrop.
3.1 GenerationPipeline
Thedatasetgenerationprocessconsistsofthefollowingstages: (1)wegenerateimageswithalatent
diffusionmodelconditionedonalargereal-worlddataset, (2)asmallsubsetofdataismanually
labeledwithheadboundingboxestotrainabinarydetectoronsyntheticdata,(3)foreachdetected
head in generated images we predict the 3D head model parameters and (4) the final dataset is
automatically filtered to remove extra noise and privacy sensitive samples. The full pipeline is
illustratedinFig.2.
Image Generation: Latent diffusion models have achieved remarkable progress in high-quality
imagegeneration[42;20]. However,multi-objectgenerationiscurrentlystillachallenge[57]. To
generalizewelltoin-the-wildsettings,alarge-scaledatasetofhumanimagesshouldcontainvarious
scenes. Thisincludestheabilitytomodelmultiplecomplexbackgroundswithdifferentamountsof
peopleandinteractionsbetweenobjectswithinascene. Toovercomethislimitation,wegenerate
imagesviaanintermediaterepresentation: 2Dhumanposes. Theposeencodespositions,poses,and
scalesofhumanbodies. Weconditionimagegenerationonthepose[38],constrainingthetask,which
makesgenerationsmuchmorereliableandoffersadegreeofcontrol. Wepredictposes[2]andimage
descriptionsfromasubsetoftheLAIONdataset[48]tomodelawidevarietyofreal-worldscenes.
Thegroundtruthimagedescriptionsinthedatasetcontainmuchprivacy-sensitiveinformation,so
instead,weuseBLIP-VQA[28]topredictsubject-neutralscenedescriptions. Wedemonstratethat
thisapproachallowsustosignificantlyreduceethnic,gender,andagebiasesbymodifyingprompts
toensuresamplingfromawiderdistribution. TheT2I-Adaptermodel[38]injectsskeletonfeature
mapsintothedecoderblocksoftheSDXL[42]modeltogeneratehigh-resolutionsyntheticdata.
Annotations: Availablefacedetectionmodels[11]aretrainedtodetectonlythefacialareaandare
limitedtomainlyfrontalposes; thus, theydonotgeneralizewelltoawiderangeofheadangles,
whichiscrucialformanyheadmodelingtasks. Inaddition,labelingfacesonsideposesisambiguous
duetoocclusionsandtheabsenceofclearvisualmarkers.Tocreateconsistentlabelsonsyntheticdata
acrossallposeranges,wemanuallylabel10,000imageswithheadboundingboxes‚Äîthesmallest
axis-parallel rectangles including all visible pixels of the head. While annotating the subset, we
manuallyremovedpotentiallyharmfulorprivacy-sensitivecontent. Theoverallannotationprocess
took56hours.
Inasecondstage,wetrainRT-DETR[36]asabinaryheaddetectoronthesmallsetofannotated
imagestobeabletodetectheadsaccuratelyonthesyntheticdatasetforfutureself-labeling. The
modelachieves0.925mAPonavalidationsubsetof2000syntheticimages.
Weemploythetrainedbinarydetectortolocateandcropallheadsinthedatasettogeneratethefinal
annotations. Werepresentthehumanheadwitha3Dmorphablemodel,FLAME[29]. TheFLAME
modelencodesthe3Dheadmeshwithasmallsetofdisentangledmodelparameters, effectively
combiningheadshape,expression,andpose.Itconsistsofasetof5023vertices,allowingittobeused
asabaserepresentationforavarietyofdownstreamtasks. Weusetheoff-the-shelfstate-of-the-art
FLAMEregressor[37]topredictavectorof413FLAMEparametersforeachheadcrop.
DataFiltering: Despitethehighqualityoftheaveragegeneratedsamples,thelatentconditional
diffusionmodelcanstillfailtogeneratecomplexscenesincaseofout-of-distributionbodyposesor
poseestimationfailcases. Itiscrucialtofilteroutpotentialwronggenerationstodecreasethenoise
levelinthedatasetandguaranteethestrongperformanceofthemodelstrainedonit. Weutilizethe
4pre-trainedbinaryheaddetectionmodelfromtheprevioussteptoremovenoisysamplesoverseveral
stages: (1)imageswithnoheadsdetectedareremoved;(2)wedetecttheheadsontheoriginaland
flippedimageandremovesampleswherethenumberofthedetectedheadsdiffers;(3)weutilizethe
factthattheheaddetectorshouldalsodetecteveryface. Wepredictthefacelocationsontheimage
withRetinaFace[11]andremovetheimageswherethedetectedfacehasnooverlapwiththedetected
headboundingboxes;(4)wesplittheimageverticallybyhalfandfilterthesampleswherethetotal
numberofdetectedheadsinbothcropsisnotequaltotheoriginalimage. Wefoundthishelpfulin
removingimageswithmanytinyheads,whichmightoftenbedeformedduetolimitedresolution.
3.2 Safety&Privacy
Manydatasetshavebeenscrapedfromtheinternetwithoutprivacyandcopyrightconsiderations.
Large-scalewebdatasetsmayalsoincludeharmfulcontentthatcannotalwaysbemanuallydetected
atscale. Ourpurelysyntheticdatasetcanlargelymitigatetheseissuesandcanbeefficientlyscaledto
anarbitrarysize.
ContentSafety:Thelatentdiffusionmodelshavebeentrainedonbillionsofimagesfromtheinternet
andthusmightbepronetogeneratingharmfulorpotentiallysensitivecontent. Eventhoughmodern
generativemodelsuseextensivedatafilteringtoexcludesuchcases,wealsoincludeseveraladditional
stagesoffilteringtoremoveanypotentialNSFWcontentproducedbytheLDM.Initially,becausethe
pipelinecangeneratenumerousimages,weremovethosewithoriginalgroundtruthlabelsindicating
theycontainNSFWorpotentiallyNSFWcontent. Inthegenerationpipeline,weutilizeaCLIP[43]
basedNSFWfilter[1]toflagandremoveharmfulcontent. Wealsoaddtheembeddingsofvarious
keywordsthatmighttriggerNSFWcontentgenerationwithanegativeweight. Asthelaststep,we
classifyandfilterthegeneratedimageswithanopen-sourceNSFWclassificationmodel[26].
ContentPrivacy: Weinvestigatewhetherdiffusionmodelscanfullyreproducetrainingsamples,
potentiallyleakingpersonaldata. [6]showthatalatentdiffusionmodelcanmemorizesomeofits
trainingsetandreproduceitssamples. StableDiffusion[45]isshowntobetheleastsensitiveto
trainingsamplememorizationand, inmanycases, thealgorithmonlyrecoversimagessimilarat
thelevelofscenecompositionwhileignoringfinerdetails. Thesefinerdetails,however,arecrucial
to recover a person‚Äôs identity. Most of the recovered training samples in [6] are photographs of
publicfiguresforwhichtherearemultipletrainingsamples. Moreover,theassociatedtextalsooften
containstheirname. We hypothesizethatmostof theidentifiablevisualfeaturesof apersonare
storedwithinthetextencoderandareencodedwiththenameofaperson. Toverifythishypothesis,
wefine-tunedaStableDiffusionmodel[42]onasubsetfromtheRaFDdataset[27]. Thedataset
containsmultiplehigh-resolutionphotosof61peoplecapturingfacialexpressionsandsideposes.
Toevaluatememorization, wegeneratefaceswithpromptspredictedfromthetrainsamplesand
try to re-identify the generated people in the training dataset. Interestingly, the re-id model [10]
failstoidentifythepeopleintheoriginaldataset. Weobservethatthemodel(a)oftenmixesthe
facialfeaturesofdifferentpeoplefromtheoriginaldatasetand(b)doesnotlearntorecoverfiner
facialdetailsthatcancaptureaperson‚Äôsidentity. However,fine-tuningasecondmodelonasingle
personresultsinstrongmemorization,whichhintsthatlargediffusionmodelsmightstoretheidentity
informationofpersonsgivenintheirnamesintextTomitigatethis,weemploytheGliNER[61]
modeltoautomaticallydetect(firstandlast)namesinthepromptandremovesuchsamplesfromthe
dataset. Additionally,weuseasimpleBLIPmodel[28]todescribethescenecomposition,ignoring
thegroundtruthpromptthatmightcontainprivacy-sensitiveinformation,suchasnames. Moreover,
asanadditionalfilteringstage,weconstructthedatabaseofthefaceembeddingsfromtheCeleb-A
[35]andremoveallthedatasampleswithfacesre-identifiedinthecelebritiesdatabase.Thisapproach
significantlymitigatesprivacyconcerns,allowingthedatasettobeusedforawiderangeoftasks.
3.3 DatasetStatistics
We sample 1.7 million images from LAION-FACE [65], a subset of LAION [48] dataset that
containsimagesofpeople. Wefilterout20.6%ofthedatathatmightcontainprivacy-sensitiveor
NSFWcontent. Weonlyusethehumanposepredictionandthepredictedimagedescriptionfrom
theseimages,discardingallvisualdata. Wethengeneratedthe1.3millionsyntheticimagesupto
1280x1280pxsize. Furtherdatapostprocessingremoved19.9%ofimageswithadditionalNSFWand
noisefilters. Thefinaldatasetconsistsof1,022,944imageswith2,219,146headsannotatedwith
boundingboxesand3Dmodelparameters. Theoverallgenerationprocesstook4000GPUhours.
5Table1: ModelComparison. Ourmodelrecoversawiderheadrepresentationspace.
Model FaceBox HeadBox 2DLandmarks 3DLandmarks 3DPose 3DShape Expression
PyramidBox[49] ‚úì ‚úó 0 0 ‚úó ‚úó ‚úó
Img2Pose[3] ‚úì ‚úó 0 0 ‚úó ‚úó ‚úó
RetinaFace[11] ‚úì ‚úó 5 1000 ‚úì ‚úó ‚úó
VGGHeads(Ours) ‚úì ‚úì 2470 2470 ‚úì ‚úì ‚úì
Figure3: ModelArchitecture. VGGHeadsextendsYOLO-NAS[2]architecturetopredictthe3D
MorphableModelparametersalongwiththeheadboundingboxesfromthemulti-scalefeaturemaps.
4 Method
Our rich ground truth annotations allow us to train a model that estimates a compact 3D Head
representation of multiple people from an RGB image in a single forward pass. For each head,
wepredictavectorof3DMMparametersdisentangledintoshape,expression,andpose. Asevery
vertex can be mapped to a face part, both head, face and other head parts bounding box can be
recoveredfromthereprojectedheadmesh. Comparedtoothermethods,thissetupencodesamore
generalrepresentationthatservesasabaseforotherdownstreamheadmodelingtasksTable1. The
architecture is illustrated in Fig. 3. The model is based on the YOLO-NAS [2] architecture for
objectdetection. Thebasearchitectureisextendedwithsixseparatemodulespredictingahuman
face‚Äôsshape, expression, jaw, pose, translation, andscaleparameters, achievinganextralevelof
disentanglement. ThearchitecturedetailsarepresentedinAppendixB.
4.1 ObjectiveFunctions
Weintroduceamulti-componentlossfunctionfortheend-to-endtrainingofthemodeltoprovide
supervisionfordifferentsetsofparameters. Thelossfunctionconsistsoffivedifferentparts. Two
commonlyusedlosscomponentsoptimizetheboundingboxdetection: BoundingBoxRegression
Loss (L ) and Classification Loss (L ). Additionally, we extend the objective function with
bbox C
three loss components, each optimizing different properties of the 3D head model. 3D Vertices
Lossmeasuresgoodness-of-fitofthe3DHeadShape(L ),RotationLoss(L )optimizethe3D
3D R
HeadPoseandtheReprojectionLoss(L )fitscaleandtranslation. Thedetailedablationstudies
proj
Table7showtheimportanceoftheintroducedcomponents.
ReprojectionLoss: Wemeasurethediscrepancybetweenthereprojectedverticesandgroundtruth
2Dkeypointcoordinates. ThecommonapproachistousetheL1norm,whichcalculatestheabsolute
differencebetweenthepredictedandgroundtruthkeypointscoordinates:
N
L reproj(vp,vgt) = N1 (cid:88)(cid:12) (cid:12)v pi ‚àív gi t(cid:12) (cid:12)
i=1
whereN isthenumberofkeypoints.
3D Vertices Loss: Following DAD-3D [37] we calculate the L2 Loss over the normalized and
unrotated3DHeadVertices. Theglobalrotationpredictionsaresettozerotoevaluatethediscrepancy
6Table2: DatasetEvaluation: ThemodeltrainedonVGGHeadsshowsbetterperformanceon3D
HeadAlignmentandPoseEstimationtasks.
DAD-3D AFLW BIWI FDDB
Model Dataset
NME‚Üì Z Accuracy‚Üë CD‚Üì PoseError‚Üì MAE‚Üì MAE‚Üì AP50‚Üë
5
VGGHeads WIDER 5.03 0.87 5.83 0.38 9.83 5.17 96.6
VGGHeads VGGHeads 2.92 0.93 4.00 0.18 3.67 3.58 96.1
betweenourpredictionsandthegroundtruthin3D.The3Dverticesarecomputedfromthe3DMM
parametersusingadifferentiableFLAMElayer. Wesubsamplethesetofverticesbyremovingears,
eyeballs,andneckparts. Thegroundtruthandthepredictedmeshcandifferinscaleandlocation,so
wenormalizebothtofitintotheunitcubeaftersubsampling.
Thefinallosstermmeasuresthediscrepancybetweennormalizedsubsampledvertices:
N
L 3D(v p,v gt)= N1 (cid:88)(cid:12) (cid:12)v pi ‚àív gi t(cid:12) (cid:12)
2
i=1
RotationLoss:Themodeldirectlypredictsanover-parametrized6Drotationrepresentation,enabling
theminimizationofthedistancebetweenrotationmatricesdirectly. InsteadofusingtheFrobenius
norm,whichdoesnotalignwiththeSO(3)manifoldgeometry,weemploythegeodesicdistanceto
measuretheshortestpathbetweentwo3Drotations. LetR andR betheestimatedandground
p gt
truthrotationmatrices,respectively,bothbelongingtoSO(3). Thegeodesicdistancebetweenthese
tworotationmatricesisdefinedas:
(cid:32) (cid:33)
tr(R RT)‚àí1
L (R ,R )=cos‚àí1 p gt
R p gt 2
Detection Losses: The focal loss L [32], which addresses class imbalance by assigning higher
c
weightstohard-to-classifyexamples,andCompleteIntersectionoverUnion(CIoU)L [66],which
reg
incorporatesIoU,centerpointdistance,andaspectratioforboundingboxregression,arestandard
componentsofourdetectionmodel
Thefinallossiscalculatedasfollows:L=Œ± ‚àóL +Œ± ‚àóL +Œ± ‚àóL +Œ± ‚àóL +Œ± ‚àóL .
1 3D 2 R 3 reproj 4 c 5 reg
4.2 ImplementationDetails
WeimplementedourmodelsusingPyTorch. Themodelisinitializedusingthepre-trainedweights
onCOCO[30]. ThedifferentiableFLAMElayeriskeptfixedduringthetraining. Thenumberof
learnableheadshapeandexpressionparametersaresetto300and100,respectively. Allthemodels
aretrainedusing4RTXA6000GPU,withabatchsizeof80. Thetrainingtakes7daystoconverge.
To preserve the scale ratio and shape of the head, images are padded to the square size and then
resizedto640x640. Tofurtherreducethedomaingapbetweenthegeneratedandthereal-world
images,weapplyextensivedataaugmentationssuchasblurring,noising,compressing,andimage
manipulatingcolorandimagecontrast.
5 ExperimentalEvaluation
Weextensivelyevaluatethedatasetandtheproposedmethodonvarioushead-relatedtasks.
5.1 HeadPoseEstimation
We evaluate the accuracy of 3D Head Pose Estimation on AFLW2000-3D [68] and BIWI [13]
datasets. TheAFLW2000-3DDataset[68]comprisesthefirst2,000subjectsfromtheAFLWdataset,
re-annotated to include image-level 68 3D landmarks and detailed pose annotations. The BIWI
Dataset[13]wascollectedinalaboratoryenvironmentbyrecordingRGB-Dvideoofvarioussubjects
usingaKinectv2device. Itincludesframeswithheadrotationsof¬±75¬∞foryaw,¬±60¬∞forpitch,and
7Table3: AFLW:VGGHeadsachievesstate-of-the-artperformancecomparingtootherend-to-end
methods.
Model EndtoEnd 3DMM MAE‚Üì PitchMAE‚Üì RollMAE‚Üì YawMAE‚Üì
RetinaFace[11] ‚úì ‚úó 6.22 9.64 3.92 5.10
Img2Pose[3] ‚úì ‚úó 3.91 5.03 3.28 3.43
VGGHeads ‚úì ‚úì 3.76 4.91 3.37 3.00
Table4: BIWI:VGGHeadsachievessuperiorperformancetothecompetitorsinyawandrollMAE
whilebeingslightlyinferioronthesampleswithextremepitchangle.
Model EndtoEnd 3DMM MAE‚Üì PitchMAE‚Üì RollMAE‚Üì YawMAE‚Üì
RetinaFace[11] ‚úì ‚úó 4.49 6.42 2.97 4.07
Img2Pose[3] ‚úì ‚úó 3.79 3.55 3.24 4.57
VGGHeads ‚úì ‚úì 3.79 5.24 2.65 3.47
¬±50¬∞forroll. Eachsubject‚Äôspointcloudwasfittedwitha3Dmodel,andheadrotationsweretracked
togeneratetheposeannotations.
Results: WereporttheresultsinTable4,Table3. VGGHeadsoutperformsmostendtoendpose
prediction methods [11; 3]. The full results are presented in Table 9 and Table 10. VGGHeads
achievessuperiorresultstomost3DMMestimatorsanddirectposeregressionmethodsbyalarge
marginandshowscomparableperformancetootherstate-of-the-artmethodsforheadposeestimation,
eventhoughnotbeingoptimizedsolelyforthisproblemandnotusinganyrealimagesduringthe
training.
5.2 3DHeadAlignment
WeutilizetheDAD-3DHeadsBenchmark[37]toevaluatethe3DDenseHeadAlignmentfroman
imageandrobustnesstoextremeposes. Thebenchmarkconsistsof2746imagesannotatedwith
themeshesinFLAMEtopologyandmeasurestheposefitting,aswellasbothfaceandheadshape
matching. Itincludesheadsinextremeposes,illumination,occlusions,andotherchallengingcases.
Astheimagesmighthavemorethanasinglehead,weextracttheheadwiththehighestoverlapwith
thegroundtruth-boundingbox.
Results: VGGHeads significantly outperforms RingNet [47] and 3DDFA-v2 [15], though these
methodsweretrainedonrealimagesandoptimizedtopredictthe3Dheadmodelfromtightcrops.
We also achieve comparable results to DAD-3D [37] even though this model was trained on the
datasetfromthesamedistributionasthebenchmark. Theresultsdemonstratethatourmethodcanfit
pose,headshape,andexpressiononrealimageseveninchallengingconditions.
5.3 FaceDetection
Thelaststepistovalidatethemodel‚Äôsabilitytodetectheadsandfacesunderdifferentconditions.
Thoughthemodelwastrainedtodetectheadboundingboxlocations,the3Drepresentationalso
encodesfaceboxes,allowingustoevaluatetheapproachonFaceDetectionbenchmarks[22;60]. To
computeboundingboxeswecalculateminimumboundingrectangleofthesubsetoffacevertices
Table5: DAD3D-Heads: VGGHeadsachievessuperiorperformancetoRingNetand3DDFA-V2,
whilebeingonlyslightlyinferiortothemethodsthatwastrainedonthebenchmark.
Model EndtoEnd NME‚Üì Z Accuracy‚Üë ChamferDistance‚Üì PoseError‚Üì
5
3DDFA-V2[15] ‚úó 3.580 - 6.170 0.527
RingNet[47] ‚úó 8.757 0.880 5.166 0.438
DAD-3DNet[37] ‚úó 2.302 0.954 3.178 0.138
VGGHeads ‚úì 2.917 0.933 4.002 0.179
8Table 6: Face Detection: The model trained on VGGHeads data is not optimized for tiny faces
detectionbutwithadditionalfinetuningitshowscomparableresultstostate-of-the-artdetectorswhile
recoveringmorecompleterepresentation.
FDDB WIDERVal
Model Dataset
AP50‚Üë EasyAP50‚Üë MediumAP50‚Üë HardAP50‚Üë
RetinaFace[11] WIDER 96.2 94.6 93.0 80.4
img2pose[3] WIDER 96.1 86.5 82.9 61.3
VGGHeads VGGHeads 96.1 56.3 51.0 29.2
VGGHeads VGGHeads+WIDER 96.6 92.6 88.9 70.3
Table7: Ablation. Removinganylossreducesperformance.
Table8: DataGenerationAblation
VGGHeadsVal AFLW FDDB
Model Model Condition FID‚Üì IS‚Üë
NME‚Üì FR‚Üì MAE‚Üì MAE‚Üì AP50‚Üë
SD1.5ControlNet 7.86 13.59
VGGHeads 1.975 0.122 2.470 5.03 88.5
SDXL ControlNet 4.18 15.09
w/oRotationLoss 2.052 0.113 2.863 6.27 87.9
SDXL T2I 3.22 14.37
w/o3DVerticesLoss 2.050 0.128 2.600 5.13 87.9
foreachdetectedheadandfilteroutdetectionswhere|yaw|> œÄ,sothatthefaceisnotvisible. In
2
contrasttoothermethods,wedonotfocusondetectingtinyfacesintheimagesincethe3DMM
parametersareambiguousincaseswheretheboundingboxareaissmallerthantheamountofvertices.
Still,VGGHeadsachievesacomparableperformancewithstate-of-the-artfacedetectorsonFDDB
[22].
5.4 AblationStudy
Weverifytheseparatecomponents‚Äôefficiencyforthedatagenerationandmodeltrainingpipeline.
Data Generation: We test the impact of using the larger diffusion models to generate data and
differentconditioningmechanismsbygenerating20,000imagesineachsetup. WecalculateFID
[19] and Inception Score [46] for every setup to measure the generated samples‚Äô diversity and
realism. The experiment shows that a more advanced diffusion model [42] plays a key role in
generatinghigh-qualitydata,significantlyoutperformingtheanalogoussetupwithStableDiffusion
1.5[45]. Additionally,wecompareControlNet[63]toT2I-Adapter[38]astheconditioningmodule
(Table 8). The metrics between the two setups are comparable, but we find that the ControlNet
pipelinefrequentlygeneratesdeformedbodies,soweusetheT2I-Adapterinthefinalpipeline.
ModelTraining: Westartfromamodeloptimizedwithonlya2Dreprojectionloss. Addingrich
supervisionofthenormalized3Dverticeslocationsenablesthemodeltobetterreconstructthehead
shapein3D,improvingthe3Dheadposefitting. Augmentingthepipelinefurtherwiththegeodesic
rotationlossprovidesdirectposesupervisionand,yetagain,improvesallmetricsandreducesthe3D
headposeerrorsignificantly.
6 Conclusion
Wehavedevelopedandreleasedacomprehensivedatasetforhumanheaddetectionand3Dmesh
estimation, created using diffusion models. It overcomes the limitations of traditional datasets,
whichoftengrapplewithissuesofbias,privacy,andethicalconcerns. Additionally,weproposed
anovelarchitectureandmodel, trainedexplicitlyonthissyntheticdataset, capableofaccurately
reconstructingmultipleheadmeshesfromasingleimage. Ourextensiveexperimentalevaluations
revealthatmodelstrainedwithoursyntheticdataperformexceptionallywellonrealimages,proving
thedataset‚Äôsrobustnessandversatilityacrossvariousapplications. Ourresearchdemonstratesthat
syntheticdatageneratedviadiffusionmodelscaneffectivelyclosethegapbetweensyntheticand
real-worldapplications,facilitatingfutureprogressin3Dheadmodelingandbeyond. Furthermore,
9the detailed synthetic data generation pipeline we describe can be adapted for other tasks and
domains. Thedataset,code,andmodelarepubliclyavailable,therebysupportingfurtherresearch
anddevelopmentinthefield.
Acknowledgements. WewouldliketothankTetianaMartyniukandIroLainaforpaperproofreading
andvaluablefeedback.WealsothanktheArmedForcesofUkraineforprovidingsecuritytocomplete
thiswork.
10A Re-IdentificationonSyntheticData
Figure4providesvisualexamplesofgeneratedsamplesthatwerematchedwithpublicfiguresfrom
Celeb-A[35]dataset. Eventhoughthedatagenerationincludethepromptanonymization,someparts
oftheprompts,suchasthemovienamestillmightencodeidentityoffamouspeople. Identifyingand
removingsuchsamplesimprovetheprivacyaspect. Fig.5showsanexampleofanimagegenerated
bydiffusionmodelthatwasfinetunedonRaFD[27]dataset,and2nearestneighborsfromthedataset
itwastrainedon. Eventhoughthemodellearnsfeaturesfromtheoriginalimages,inmostcasesit
doescombinefeaturesfrommultiplepeopleandfailstorecoverfinerdetailsthatencodetheidentity.
B ModelDetails
The model is based on the YOLO-NAS [2] architecture for object detection. YOLO-NAS use a
neuralarchitecturesearchenginetoenhancetheYOLOfamilyofmodelsbyoptimzingthesizesand
structuresofstages,blocktypes,thenumberofblocks,andthenumberofchannelsineachstage. We
employtheYOLO-NAS-Lbackbone,thoughthemodelisagnostictothechoiceofencoder. The
‚Äúneck‚Äùisusedtofusethefeaturesgeneratedbythebackbone. Thevisualfeaturesfromtheencoder
mapsneckarefusedbytheSpatialPyramidPooling[17]moduleatdifferentscalesandprocessedby
FeaturePyramidNetwork[31]togeneratefeaturesatdifferentsemanticlevels. Similarlytoother
YOLOmodels,weadoptananchor-basedmulti-scaledetectionscheme. PathAggregationNetwork
(PAN)[34]transferspositioningfeaturesbottom-up. WecombinethemwiththefeaturesfromFPN
toobtainabetterfeaturefusioneffectandthendirectlyusethemulti-scalefusionfeaturemapsin
thePANfordetection. Thus,thedetectionheadspredictboundingboxesand3DMMparameterson
differentscales,ensuringhighaccuracyondifferentobjectsizes.
ThedetectionheadintheYOLO-NASmodelpredictstheoffsetoftheboundingboxpositionandthe
scalingoftheheightandwidth,aswellastheconfidenceoftheprediction. Weextendthedetection
head to also predict the 3DMM parameters by introducing six separate 3D parameter prediction
modules,eachconsistingoftwoRepVGGblocks[7]andafinal1√ó1convolutionthatpredictsthe
finalsetofparameters. EachRepVGGblockconsistsofthreebranches:a3√ó3Convolutionfollowed
by BatchNorm [21], a branch of a 1x1 Convolution with bias and a residual branch. Predicting
different3DDMparameterscomponentsseparatelyachievesanextralevelofdisentanglement.
TheweightsofthelosscomponentsaresettoŒ± =50,Œ± =1,Œ± =1,Œ± =0.5,Œ± =2.5inthe
1 2 3 4 5
finalversion.
C HeadAlignment
Thenon-obviousadvantageofFLAMErepresentationforheadalignmentisthatthezerocoordinate
in FLAME space is located in the geometrical center of the 3D head model. This allows us to
implementconsistentalignmentandnormalizationofthehumanheadacrossthewiderangeofposes
sothatthecenterofthe‚Äúskull‚ÄùwillbealwayslocatedinthecenterofthecroppedimageFig.6. In
Figure 4: Re-ID on Celeb-A. We automatically detect and removed samples where face in the
generatedimageismatchedwithoneofthefacesfromCeleb-Adataset[35].
11Figure 5: Preserving Identity. With subject neutral prompts diffusion Model blends feature of
differentpeoplefromthesetitwastrainedon.
Figure6: HeadAlignment. VGGHeadsintroducemoreconsistentalignmentacrossvariousposes
byensuringthecenteroftheheadin3Disreprojectedtothecenterofthealignedimage. Toprow-
VGGHeads,Bottomrow-RetinaFace[11].
contrasttofacealignmentmethods,itpreservesthescaleanddoesnotcroppartsoftheheadwhichis
crucialforpreprocessingmethodsofthedetailed3Dheadmodellingtasks.
D HeadPoseEstimation
ThefullresultsonAFLWandBIWIdatasetsarepresentedinTable9,Table10.
Table9: AFLW
Model EndtoEnd 3DMM MAE‚Üì PitchMAE‚Üì RollMAE‚Üì YawMAE‚Üì
Dlib[24] ‚úó ‚úó 13.29 12.60 9.00 18.27
HopeNet[12] ‚úó ‚úó 6.16 6.56 5.44 6.47
6DRepNet[18] ‚úó ‚úó 3.61 4.58 2.98 3.27
RingNet[47] ‚úó ‚úì 8.27 4.39 13.51 6.92
3DDFA-V2[15] ‚úó ‚úì 7.56 8.48 9.89 4.30
3DDFA[14] ‚úó ‚úì 7.39 8.53 7.39 5.40
DAD-3DHeads[37] ‚úó ‚úì 3.66 4.76 3.15 3.08
SynergyNet[55] ‚úó ‚úì 3.35 4.09 2.55 3.42
RetinaFace[11] ‚úì ‚úó 6.22 9.64 3.92 5.10
Img2Pose[3] ‚úì ‚úó 3.91 5.03 3.28 3.43
VGGHeads ‚úì ‚úì 3.76 4.91 3.37 3.00
12Table10: BIWI
Model EndtoEnd 3DMM MAE‚Üì PitchMAE‚Üì RollMAE‚Üì YawMAE‚Üì
Dlib(68points)[24] ‚úó ‚úó 12.25 13.80 6.19 16.76
HopeNet[12] ‚úó ‚úó 4.90 6.61 3.27 4.81
WHENet[67] ‚úó ‚úó 3.81 4.39 3.06 3.99
6DRepNet[18] ‚úó ‚úó 3.78 5.32 2.78 3.23
MNN[50] ‚úó ‚úó 3.66 4.61 2.39 3.98
3DDFA[14] ‚úó ‚úì 19.07 12.25 8.78 36.18
3DDFA-V2[15] ‚úó ‚úì 8.81 12.08 7.54 6.80
RingNet[47] ‚úó ‚úì 7.34 5.37 7.82 8.82
DAD-3DNet[37] ‚úó ‚úì 3.98 5.24 2.92 3.79
RetinaFace[11] ‚úì ‚úó 4.49 6.42 2.97 4.07
Img2Pose[3] ‚úì ‚úó 3.79 3.55 3.24 4.57
VGGHeads ‚úì ‚úì 3.79 5.24 2.65 3.47
E ControllableGeneration
Figure7: T2IAdapterwithVGGHeads. The3Dconditionprovidesastrongdegreeofcontrolfor
thegenerativemodel,preservingshape,poseandexpressionoftheinputimage.
Conditioningtheimagegenerationonfullheadmeshhelpstopreservetheheadshapeandexpression
whichiscrucialformanyARapplications. WetrainedtheT2I-Adapter[38]forSDXL[42]model
thatisconditionedonmeshesrecoveredbyourmodel. ThemeshesarerenderedbymappingtoRGB
spacewithProjectedNormalizedCoordinateCode(PNCC)[69],withthe3Dcoordinateofeach
vertexofthenormalizedheadmeshencodedasRGB(NCCx=R,NCCy=G,NCCz=B).Theheads
onthegeneratedimagespreservethepose,expressionandshapeoftheoriginalphotoFig.7.
F QualitativeResults
WeincludemorevisualresultsonAFLW[68]Fig.10,BIWI[13]Fig.11,DAD-3D[37]Fig.12,
WIDER[60]Fig.8andFDDB[22]benchmarksFig.9.
G LimitationsandBroaderImpact
ThedatasetannotationsarebasedonDAD-3D[37]sowedon‚Äôtaimtomodelneck,earsandeyeball
verticesthatareapartoftheFLAMEtopology. Thegenerationpipelinestillcanproducedeformed
small faces due to the limited resolution so we don‚Äôt label and predict the 3D model parameters
of the tiny faces. Also, the more advanced filtering methods and nsfw detection methods are a
suitablevenueforfutureexplorationsasonthelargescaleitisnotfeasibletoguaranteetheabsolute
correctnessofthegeneratedsamples, evenbyaddingthehumanevaluationintotheprocess. By
leveragingsynthetic data generatedthrough diffusion models, we reducethe privacy, ethics, and
13Figure8: QualitativeEvaluation. VGGHeadsisabletoaccuratelyrecover3Dheadmodelson
variouscomplexscenesfromWIDERFace[60]dataset.
Figure9: QualitativeEvaluationonFDDB.
14Figure10: QualitativeEvaluationonAFLW.
safetyissuesinhumansubjectresearch,asnorealpersonaldataisusedsothatprivacyandethical
standardsareupheld. Furthermore,thesyntheticdataset‚Äôshighresolutionanddetailedannotations
15Figure11: QualitativeEvaluationonBIWI.
providearobustandversatileresourcefordevelopingandtestingnewmodels. Thisapproachnot
onlyenhancesthegeneralizabilityandaccuracyofmodelstrainedonthisdatabutalsopromotes
ethicalresearchpracticesbyeliminatingtheneedforrealhumansubjects. Theabilitytogenerate
16Figure12: QualitativeEvaluationonDAD-3D
large-scale synthetic datasets paves the way for safer and more inclusive research, free from the
constraintsandrisksassociatedwithreal-worlddatacollection. Thus,ourworkpromotesethicalAI
practicesandsetsastandardforfutureresearchinthisarea.
References
[1] Machinevision&learninggrouplmu.safetycheckermodelcard. https://huggingface.
co/CompVis/stable-diffusion-safety-checker. Accessed: 2023-11-16. 5
[2] Shay Aharon, Louis-Dupont, Ofri Masad, Kate Yurkova, Lotem Fridman, Lkdci, Eugene
Khvedchenya,RanRubin,NatanBagrov,BorysTymchenko,TomerKeren,AlexanderZhilko,
andEran-Deci. Super-gradients,2021. URLhttps://zenodo.org/record/7789328. 4,6,
11
[3] VitorAlbiero,XingyuChen,XiYin,GuanPang,andTalHassner. img2pose: Facealignment
anddetectionvia6dof,faceposeestimation. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages7617‚Äì7627,2021. 2,6,8,9,12,13
[4] Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In
ConferenceonComputergraphicsandinteractivetechniques,pages187‚Äì194,1999. 3
[5] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint
arXiv:2001.10773,2020. 3
[6] NicolasCarlini,JamieHayes,MiladNasr,MatthewJagielski,VikashSehwag,FlorianTramer,
BorjaBalle,DaphneIppolito,andEricWallace. Extractingtrainingdatafromdiffusionmodels.
In32ndUSENIXSecuritySymposium(USENIXSecurity23),pages5253‚Äì5270,2023. 5
[7] XiangxiangChu,LiangLi,andBoZhang. Makerepvgggreateragain: Aquantization-aware
approach. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume38,pages
11624‚Äì11632,2024. 11
[8] JifengDai,YiLi,KaimingHe,andJianSun. R-fcn: Objectdetectionviaregion-basedfully
convolutionalnetworks. Advancesinneuralinformationprocessingsystems,29,2016. 2
17[9] NavneetDalalandBillTriggs. Histogramsoforientedgradientsforhumandetection. In2005
IEEE computer society conference on computer vision and pattern recognition (CVPR‚Äô05),
volume1,pages886‚Äì893.Ieee,2005. 2
[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular
marginlossfordeepfacerecognition. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages4690‚Äì4699,2019. 1,5
[11] JiankangDeng,JiaGuo,EvangelosVerveras,IreneKotsia,andStefanosZafeiriou. Retinaface:
Single-shotmulti-levelfacelocalisationinthewild.InProceedingsoftheIEEE/CVFconference
oncomputervisionandpatternrecognition,pages5203‚Äì5212,2020. 2,3,4,5,6,8,9,12,13
[12] BardiaDoosti,ShujonNaha,MajidMirbagheri,andDavidJ.Crandall. Hope-net: Agraph-
basedmodelforhand-objectposeestimation. InIEEEConferenceonComputerVisionand
PatternRecognition(CVPR),June2020. 12,13
[13] Gabriele Fanelli, Thibaut Weise, Juergen Gall, and Luc Van Gool. Real time head pose
estimation from consumer depth cameras. In Joint pattern recognition symposium, pages
101‚Äì110.Springer,2011. 7,13
[14] JianzhuGuo,XiangyuZhu,andZhenLei. 3ddfa. https://github.com/cleardusk/3DDFA,
2018. 3,12,13
[15] Jianzhu Guo, Xiangyu Zhu, Yang Yang, Fan Yang, Zhen Lei, and Stan Z Li. Towards fast,
accurate andstable 3ddense facealignment. In EuropeanConference onComputer Vision
(ECCV),2020. 3,8,12,13
[16] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A
datasetandbenchmarkforlarge-scalefacerecognition. InComputerVision‚ÄìECCV2016: 14th
EuropeanConference,Amsterdam,TheNetherlands,October11-14,2016,Proceedings,Part
III14,pages87‚Äì102.Springer,2016. 1
[17] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Spatialpyramidpoolingindeep
convolutional networks for visual recognition. IEEE transactions on pattern analysis and
machineintelligence,37(9):1904‚Äì1916,2015. 11
[18] ThorstenHempel,AhmedAAbdelrahman,andAyoubAl-Hamadi. Towardrobustanduncon-
strainedfullrangeofrotationheadposeestimation. IEEETransactionsonImageProcessing,
33:2377‚Äì2387,2024. 12,13
[19] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter.
Ganstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. Advancesin
neuralinformationprocessingsystems,30,2017. 9
[20] JonathanHo,ChitwanSaharia,WilliamChan,DavidJFleet,MohammadNorouzi,andTim
Salimans. Cascaded diffusion models for high fidelity image generation. The Journal of
MachineLearningResearch,23(1):2249‚Äì2281,2022. 4
[21] SergeyIoffeandChristianSzegedy. Batchnormalization: Acceleratingdeepnetworktraining
byreducinginternalcovariateshift. InInternationalconferenceonmachinelearning,pages
448‚Äì456.pmlr,2015. 11
[22] ViditJainandErikLearned-Miller. Fddb: Abenchmarkforfacedetectioninunconstrained
settings. Technicalreport,UMassAmhersttechnicalreport,2010. 8,9,13
[23] PeiyuanJiang, DajiErgu, FangyaoLiu, YingCai, andBoMa. Areviewofyoloalgorithm
developments. Procediacomputerscience,199:1066‚Äì1073,2022. 2
[24] DavisE.King. Dlib-ml: Amachinelearningtoolkit. JournalofMachineLearningResearch,
10:1755‚Äì1758,2009. 12,13
[25] OrestKupynandChristianRupprecht. Datasetenhancementwithinstance-levelaugmentations,
2024. 3
18[26] GantLaborde. Deepnnfornsfwdetection. URLhttps://github.com/GantMan/nsfw_
model. 5
[27] Oliver Langner, Ron Dotsch, Gijsbert Bijlstra, Daniel HJ Wigboldus, Skyler T Hawk, and
ADVanKnippenberg. Presentationandvalidationoftheradboudfacesdatabase. Cognition
andemotion,24(8):1377‚Äì1388,2010. 5,11
[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-
imagepre-trainingforunifiedvision-languageunderstandingandgeneration. InInternational
ConferenceonMachineLearning,pages12888‚Äì12900.PMLR,2022. 4,5
[29] TianyeLi,TimoBolkart,MichaelJ.Black,HaoLi,andJavierRomero. Learningamodelof
facialshapeandexpressionfrom4dscans. ACMTrans.Graph.,36(6),2017. 3,4
[30] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,Piotr
Doll√°r,andCLawrenceZitnick. Microsoftcoco: Commonobjectsincontext. InComputer
Vision‚ÄìECCV2014: 13thEuropeanConference,Zurich,Switzerland,September6-12,2014,
Proceedings,PartV13,pages740‚Äì755.Springer,2014. 7
[31] Tsung-YiLin,PiotrDoll√°r,RossGirshick,KaimingHe,BharathHariharan,andSergeBelongie.
Feature pyramid networks for object detection. In Proceedings of the IEEE conference on
computervisionandpatternrecognition,pages2117‚Äì2125,2017. 11
[32] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,andPiotrDoll√°r. Focallossfordense
object detection. In Proceedings of the IEEE international conference on computer vision,
pages2980‚Äì2988,2017. 7
[33] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,andPiotrDoll√°r. Focallossfordense
object detection. In Proceedings of the IEEE international conference on computer vision,
pages2980‚Äì2988,2017. 2
[34] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for
instancesegmentation. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages8759‚Äì8768,2018. 11
[35] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes
(celeba)dataset. RetrievedAugust,15(2018):11,2018. 5,11
[36] WenyuLv, ShangliangXu, YianZhao, GuanzhongWang, JinmanWei, ChengCui, Yuning
Du,QingqingDang,andYiLiu. Detrsbeatyolosonreal-timeobjectdetection. arXivpreprint
arXiv:2304.08069,2023. 4
[37] Tetiana Martyniuk, Orest Kupyn, Yana Kurlyak, Igor Krashenyi, JiÀári Matas, and Viktoriia
Sharmanska. Dad-3dheads: A large-scale dense, accurate and diverse dataset for 3d head
alignment from a single image. In Proceedings of the IEEE/CVF Conference on computer
visionandpatternrecognition,pages20942‚Äì20952,2022. 3,4,6,8,12,13
[38] ChongMou,XintaoWang,LiangbinXie,JianZhang,ZhongangQi,YingShan,andXiaohuQie.
T2i-adapter: Learningadapterstodigoutmorecontrollableabilityfortext-to-imagediffusion
models. arXivpreprintarXiv:2302.08453,2023. 4,9,13
[39] BehnazNojavanasghari,CharlesEHughes,TadasBaltru≈°aitis,andLouis-PhilippeMorency.
Hand2face: Automaticsynthesisandrecognitionofhandoverfaceocclusions. In2017Seventh
International Conference on Affective Computing and Intelligent Interaction (ACII), pages
209‚Äì215.IEEE,2017. 3
[40] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter. A
3d face model for pose and illumination invariant face recognition. In IEEE International
Conference on Advanced Video and Signal Based Surveillance, pages 296‚Äì301, 2009. doi:
10.1109/AVSS.2009.58. 3
[41] DezhiPeng,ZikaiSun,ZirongChen,ZiruiCai,LeleXie,andLianwenJin. Detectingheads
usingfeaturerefinenetandcascadedmulti-scalearchitecture. arXivpreprintarXiv:1803.09256,
2018. 2
19[42] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,JonasM√ºller,Joe
Penna,andRobinRombach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXivpreprintarXiv:2307.01952,2023. 3,4,5,9,13
[43] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,
pages8748‚Äì8763.PMLR,2021. 5
[44] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time
objectdetectionwithregionproposalnetworks. Advancesinneuralinformationprocessing
systems,28,2015. 2
[45] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBj√∂rnOmmer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages10684‚Äì10695,2022. 5,9
[46] TimSalimans,IanGoodfellow,WojciechZaremba,VickiCheung,AlecRadford,andXiChen.
Improvedtechniquesfortraininggans. Advancesinneuralinformationprocessingsystems,29,
2016. 9
[47] SoubhikSanyal,TimoBolkart,HaiwenFeng,andMichaelJBlack. Learningtoregress3dface
shapeandexpressionfromanimagewithout3dsupervision. InIEEEConferenceonComputer
VisionandPatternRecognition(CVPR),pages7763‚Äì7772,2019. 3,8,12,13
[48] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,
MehdiCherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-
5b: Anopenlarge-scaledatasetfortrainingnextgenerationimage-textmodels. Advancesin
NeuralInformationProcessingSystems,35:25278‚Äì25294,2022. 4,5
[49] XuTang,DanielKDu,ZeqiangHe,andJingtuoLiu. Pyramidbox: Acontext-assistedsingle
shotfacedetector. InProceedingsoftheEuropeanconferenceoncomputervision(ECCV),
pages797‚Äì813,2018. 2,6
[50] RobertoValle,Jos√©MBuenaposada,andLuisBaumela. Multi-taskheadposeestimationin-
the-wild. IEEETransactionsonPatternAnalysisandMachineIntelligence,43(8):2874‚Äì2881,
2020. 13
[51] Paul Viola and Michael J Jones. Robust real-time face detection. International journal of
computervision,57:137‚Äì154,2004. 2
[52] Tuan-HungVu,AntonOsokin,andIvanLaptev. Context-awarecnnsforpersonheaddetection.
InProceedingsoftheIEEEInternationalConferenceonComputerVision,pages2893‚Äì2901,
2015. 2
[53] YitongWang,XingJi,ZhengZhou,HaoWang,andZhifengLi. Detectingfacesusingregion-
basedfullyconvolutionalnetworks. arXivpreprintarXiv:1709.05256,2017. 2
[54] ErrollWood,TadasBaltru≈°aitis,CharlieHewitt,SebastianDziadzio,ThomasJCashman,and
JamieShotton. Fakeittillyoumakeit: faceanalysisinthewildusingsyntheticdataalone. In
ProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages3681‚Äì3691,
2021. 3
[55] Cho-YingWu,QiangengXu,andUlrichNeumann. Synergybetween3dmmand3dlandmarks
foraccurate3dfacialgeometry. arXivpreprintarXiv:2110.09772,2021. 12
[56] WeijiaWu,YuzhongZhao,HaoChen,YuchaoGu,RuiZhao,YefeiHe,HongZhou,MikeZheng
Shou,andChunhuaShen. Datasetdm: Synthesizingdatawithperceptionannotationsusing
diffusionmodels. arXivpreprintarXiv:2308.06160,2023. 3
[57] WeijiaWu,YuzhongZhao,MikeZhengShou,HongZhou,andChunhuaShen. Diffumask:
Synthesizingimageswithpixel-levelannotationsforsemanticsegmentationusingdiffusion
models. arXivpreprintarXiv:2303.11681,2023. 3,4
20[58] ZhenyuWu,LinWang,WeiWang,TengfeiShi,ChenglizhaoChen,AiminHao,andShuoLi.
Syntheticdatasupervisedsalientobjectdetection.InProceedingsofthe30thACMInternational
ConferenceonMultimedia,pages5557‚Äì5565,2022. 3
[59] YuelangXu,BenwangChen,ZheLi,HongwenZhang,LizhenWang,ZerongZheng,andYebin
Liu. Gaussianheadavatar:Ultrahigh-fidelityheadavatarviadynamicgaussians. arXivpreprint
arXiv:2312.03029,2023. 1
[60] Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Wider face: A face detection
benchmark. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages5525‚Äì5533,2016. 8,13,14
[61] UrchadeZaratiana,NadiTomeh,PierreHolat,andThierryCharnois. Gliner: Generalistmodel
fornamedentityrecognitionusingbidirectionaltransformer,2023. 5
[62] ChangzhengZhang, XiangXu, andDandanTu. Facedetectionusingimprovedfasterrcnn.
arXivpreprintarXiv:1802.02142,2018. 2
[63] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-image
diffusionmodels. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages3836‚Äì3847,2023. 9
[64] ShifengZhang,XiangyuZhu,ZhenLei,HailinShi,XiaoboWang,andStanZLi. S3fd: Single
shot scale-invariant face detector. In Proceedings of the IEEE international conference on
computervision,pages192‚Äì201,2017. 2
[65] YinglinZheng,HaoYang,TingZhang,JianminBao,DongdongChen,YangyuHuang,LuYuan,
Dong Chen, Ming Zeng, and Fang Wen. General facial representation learning in a visual-
linguisticmanner. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages18697‚Äì18709,2022. 5
[66] ZhaohuiZheng,PingWang,WeiLiu,JinzeLi,RongguangYe,andDongweiRen. Distance-iou
loss: Faster and better learning for bounding box regression. In Proceedings of the AAAI
ConferenceonArtificialIntelligence,volume34,pages12993‚Äì13000,2020. 7
[67] YijunZhouandJamesGregson. Whenet: Real-timefine-grainedestimationforwiderange
headpose. arXivpreprintarXiv:2005.10353,2020. 13
[68] XiangyuZhu,ZhenLei,XiaomingLiu,HailinShi,andStanZLi. Facealignmentacrosslarge
poses:A3dsolution.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),
pages146‚Äì155,2016. 7,13
[69] XiangyuZhu,ZhenLei,XiaomingLiu,HailinShi,andStanZLi. Facealignmentacrosslarge
poses: A3dsolution. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages146‚Äì155,2016. 1,3,13
21