[
    {
        "title": "Strategic Cost Selection in Participatory Budgeting",
        "authors": "Piotr FaliszewskiŁukasz JaneczkoAndrzej KaczmarczykGrzegorz LisowskiPiotr SkowronStanisław Szufa",
        "links": "http://arxiv.org/abs/2407.18092v1",
        "entry_id": "http://arxiv.org/abs/2407.18092v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18092v1",
        "summary": "We study strategic behavior of project proposers in the context of\napproval-based participatory budgeting (PB). In our model we assume that the\nvotes are fixed and known and the proposers want to set as high project prices\nas possible, provided that their projects get selected and the prices are not\nbelow the minimum costs of their delivery. We study the existence of pure Nash\nequilibria (NE) in such games, focusing on the AV/Cost, Phragm\\'en, and Method\nof Equal Shares rules. Furthermore, we report an experimental study of\nstrategic cost selection on real-life PB election data.",
        "updated": "2024-07-25 15:00:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18092v1"
    },
    {
        "title": "Principal-Agent Reinforcement Learning",
        "authors": "Dima IvanovPaul DüttingInbal Talgam-CohenTonghan WangDavid C. Parkes",
        "links": "http://arxiv.org/abs/2407.18074v1",
        "entry_id": "http://arxiv.org/abs/2407.18074v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18074v1",
        "summary": "Contracts are the economic framework which allows a principal to delegate a\ntask to an agent -- despite misaligned interests, and even without directly\nobserving the agent's actions. In many modern reinforcement learning settings,\nself-interested agents learn to perform a multi-stage task delegated to them by\na principal. We explore the significant potential of utilizing contracts to\nincentivize the agents. We model the delegated task as an MDP, and study a\nstochastic game between the principal and agent where the principal learns what\ncontracts to use, and the agent learns an MDP policy in response. We present a\nlearning-based algorithm for optimizing the principal's contracts, which\nprovably converges to the subgame-perfect equilibrium of the principal-agent\ngame. A deep RL implementation allows us to apply our method to very large MDPs\nwith unknown transition dynamics. We extend our approach to multiple agents,\nand demonstrate its relevance to resolving a canonical sequential social\ndilemma with minimal intervention to agent rewards.",
        "updated": "2024-07-25 14:28:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18074v1"
    },
    {
        "title": "Stochastic Games with Minimally Bounded Action Costs",
        "authors": "David Mguni",
        "links": "http://arxiv.org/abs/2407.18010v1",
        "entry_id": "http://arxiv.org/abs/2407.18010v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18010v1",
        "summary": "In many multi-player interactions, players incur strictly positive costs each\ntime they execute actions e.g. 'menu costs' or transaction costs in financial\nsystems. Since acting at each available opportunity would accumulate\nprohibitively large costs, the resulting decision problem is one in which\nplayers must make strategic decisions about when to execute actions in addition\nto their choice of action. This paper analyses a discrete-time stochastic game\n(SG) in which players face minimally bounded positive costs for each action and\ninfluence the system using impulse controls. We prove SGs of two-sided impulse\ncontrol have a unique value and characterise the saddle point equilibrium in\nwhich the players execute actions at strategically chosen times in accordance\nwith Markovian strategies. We prove the game respects a dynamic programming\nprinciple and that the Markov perfect equilibrium can be computed as a limit\npoint of a sequence of Bellman operations. We then introduce a new Q-learning\nvariant which we show converges almost surely to the value of the game enabling\nsolutions to be extracted in unknown settings. Lastly, we extend our results to\nsettings with budgetory constraints.",
        "updated": "2024-07-25 13:04:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18010v1"
    },
    {
        "title": "Limited Voting for Better Representation?",
        "authors": "Maaike Venema-LosZoé ChristoffDavide Grossi",
        "links": "http://arxiv.org/abs/2407.17973v1",
        "entry_id": "http://arxiv.org/abs/2407.17973v1",
        "pdf_url": "http://arxiv.org/pdf/2407.17973v1",
        "summary": "Limited Voting (LV) is an approval-based method for multi-winner elections\nwhere all ballots are required to have a same fixed size. While it appears to\nbe used as voting method in corporate governance and has some political\napplications, to the best of our knowledge, no formal analysis of the rule\nexists to date. We provide such an analysis here, prompted by a request for\nadvice about this voting rule by a health insurance company in the Netherlands,\nwhich uses it to elect its work council. We study conditions under which LV\nwould improve representation over standard approval voting and when it would\nnot. We establish the extent of such an improvement, or lack thereof, both in\nterms of diversity and proportionality notions. These results help us\nunderstand if, and how, LV may be used as a low-effort fix of approval voting\nin order to enhance representation.",
        "updated": "2024-07-25 12:08:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.17973v1"
    },
    {
        "title": "Very Large-Scale Multi-Agent Simulation in AgentScope",
        "authors": "Xuchen PanDawei GaoYuexiang XieZhewei WeiYaliang LiBolin DingJi-Rong WenJingren Zhou",
        "links": "http://arxiv.org/abs/2407.17789v1",
        "entry_id": "http://arxiv.org/abs/2407.17789v1",
        "pdf_url": "http://arxiv.org/pdf/2407.17789v1",
        "summary": "Recent advances in large language models (LLMs) have opened new avenues for\napplying multi-agent systems in very large-scale simulations. However, there\nremain several challenges when conducting multi-agent simulations with existing\nplatforms, such as limited scalability and low efficiency, unsatisfied agent\ndiversity, and effort-intensive management processes. To address these\nchallenges, we develop several new features and components for AgentScope, a\nuser-friendly multi-agent platform, enhancing its convenience and flexibility\nfor supporting very large-scale multi-agent simulations. Specifically, we\npropose an actor-based distributed mechanism as the underlying technological\ninfrastructure towards great scalability and high efficiency, and provide\nflexible environment support for simulating various real-world scenarios, which\nenables parallel execution of multiple agents, centralized workflow\norchestration, and both inter-agent and agent-environment interactions among\nagents. Moreover, we integrate an easy-to-use configurable tool and an\nautomatic background generation pipeline in AgentScope, simplifying the process\nof creating agents with diverse yet detailed background settings. Last but not\nleast, we provide a web-based interface for conveniently monitoring and\nmanaging a large number of agents that might deploy across multiple devices. We\nconduct a comprehensive simulation to demonstrate the effectiveness of the\nproposed enhancements in AgentScope, and provide detailed observations and\ndiscussions to highlight the great potential of applying multi-agent systems in\nlarge-scale simulations. The source code is released on GitHub at\nhttps://github.com/modelscope/agentscope to inspire further research and\ndevelopment in large-scale multi-agent simulations.",
        "updated": "2024-07-25 05:50:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.17789v1"
    }
]