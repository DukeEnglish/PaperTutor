[
    {
        "title": "Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image",
        "authors": "Kailu WuFangfu LiuZhihan CaiRunjie YanHanyang WangYating HuYueqi DuanKaisheng Ma",
        "links": "http://arxiv.org/abs/2405.20343v1",
        "entry_id": "http://arxiv.org/abs/2405.20343v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20343v1",
        "summary": "In this work, we introduce Unique3D, a novel image-to-3D framework for\nefficiently generating high-quality 3D meshes from single-view images,\nfeaturing state-of-the-art generation fidelity and strong generalizability.\nPrevious methods based on Score Distillation Sampling (SDS) can produce\ndiversified 3D results by distilling 3D knowledge from large 2D diffusion\nmodels, but they usually suffer from long per-case optimization time with\ninconsistent issues. Recent works address the problem and generate better 3D\nresults either by finetuning a multi-view diffusion model or training a fast\nfeed-forward model. However, they still lack intricate textures and complex\ngeometries due to inconsistency and limited generated resolution. To\nsimultaneously achieve high fidelity, consistency, and efficiency in single\nimage-to-3D, we propose a novel framework Unique3D that includes a multi-view\ndiffusion model with a corresponding normal diffusion model to generate\nmulti-view images with their normal maps, a multi-level upscale process to\nprogressively improve the resolution of generated orthographic multi-views, as\nwell as an instant and consistent mesh reconstruction algorithm called ISOMER,\nwhich fully integrates the color and geometric priors into mesh results.\nExtensive experiments demonstrate that our Unique3D significantly outperforms\nother image-to-3D baselines in terms of geometric and textural details.",
        "updated": "2024-05-30 17:59:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20343v1"
    },
    {
        "title": "MotionLLM: Understanding Human Behaviors from Human Motions and Videos",
        "authors": "Ling-Hao ChenShunlin LuAiling ZengHao ZhangBenyou WangRuimao ZhangLei Zhang",
        "links": "http://arxiv.org/abs/2405.20340v1",
        "entry_id": "http://arxiv.org/abs/2405.20340v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20340v1",
        "summary": "This study delves into the realm of multi-modality (i.e., video and motion\nmodalities) human behavior understanding by leveraging the powerful\ncapabilities of Large Language Models (LLMs). Diverging from recent LLMs\ndesigned for video-only or motion-only understanding, we argue that\nunderstanding human behavior necessitates joint modeling from both videos and\nmotion sequences (e.g., SMPL sequences) to capture nuanced body part dynamics\nand semantics effectively. In light of this, we present MotionLLM, a\nstraightforward yet effective framework for human motion understanding,\ncaptioning, and reasoning. Specifically, MotionLLM adopts a unified\nvideo-motion training strategy that leverages the complementary advantages of\nexisting coarse video-text data and fine-grained motion-text data to glean rich\nspatial-temporal insights. Furthermore, we collect a substantial dataset,\nMoVid, comprising diverse videos, motions, captions, and instructions.\nAdditionally, we propose the MoVid-Bench, with carefully manual annotations,\nfor better evaluation of human behavior understanding on video and motion.\nExtensive experiments show the superiority of MotionLLM in the caption,\nspatial-temporal comprehension, and reasoning ability.",
        "updated": "2024-05-30 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20340v1"
    },
    {
        "title": "Visual Perception by Large Language Model's Weights",
        "authors": "Feipeng MaHongwei XueGuangting WangYizhou ZhouFengyun RaoShilin YanYueyi ZhangSiying WuMike Zheng ShouXiaoyan Sun",
        "links": "http://arxiv.org/abs/2405.20339v1",
        "entry_id": "http://arxiv.org/abs/2405.20339v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20339v1",
        "summary": "Existing Multimodal Large Language Models (MLLMs) follow the paradigm that\nperceives visual information by aligning visual features with the input space\nof Large Language Models (LLMs), and concatenating visual tokens with text\ntokens to form a unified sequence input for LLMs. These methods demonstrate\npromising results on various vision-language tasks but are limited by the high\ncomputational effort due to the extended input sequence resulting from the\ninvolvement of visual tokens. In this paper, instead of input space alignment,\nwe propose a novel parameter space alignment paradigm that represents visual\ninformation as model weights. For each input image, we use a vision encoder to\nextract visual features, convert features into perceptual weights, and merge\nthe perceptual weights with LLM's weights. In this way, the input of LLM does\nnot require visual tokens, which reduces the length of the input sequence and\ngreatly improves efficiency. Following this paradigm, we propose VLoRA with the\nperceptual weights generator. The perceptual weights generator is designed to\nconvert visual features to perceptual weights with low-rank property,\nexhibiting a form similar to LoRA. The experimental results show that our VLoRA\nachieves comparable performance on various benchmarks for MLLMs, while\nsignificantly reducing the computational costs for both training and inference.\nThe code and models will be made open-source.",
        "updated": "2024-05-30 17:59:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20339v1"
    },
    {
        "title": "OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving",
        "authors": "Lening WangWenzhao ZhengYilong RenHan JiangZhiyong CuiHaiyang YuJiwen Lu",
        "links": "http://arxiv.org/abs/2405.20337v1",
        "entry_id": "http://arxiv.org/abs/2405.20337v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20337v1",
        "summary": "Understanding the evolution of 3D scenes is important for effective\nautonomous driving. While conventional methods mode scene development with the\nmotion of individual instances, world models emerge as a generative framework\nto describe the general scene dynamics. However, most existing methods adopt an\nautoregressive framework to perform next-token prediction, which suffer from\ninefficiency in modeling long-term temporal evolutions. To address this, we\npropose a diffusion-based 4D occupancy generation model, OccSora, to simulate\nthe development of the 3D world for autonomous driving. We employ a 4D scene\ntokenizer to obtain compact discrete spatial-temporal representations for 4D\noccupancy input and achieve high-quality reconstruction for long-sequence\noccupancy videos. We then learn a diffusion transformer on the spatial-temporal\nrepresentations and generate 4D occupancy conditioned on a trajectory prompt.\nWe conduct extensive experiments on the widely used nuScenes dataset with Occ3D\noccupancy annotations. OccSora can generate 16s-videos with authentic 3D layout\nand temporal consistency, demonstrating its ability to understand the spatial\nand temporal distributions of driving scenes. With trajectory-aware 4D\ngeneration, OccSora has the potential to serve as a world simulator for the\ndecision-making of autonomous driving. Code is available at:\nhttps://github.com/wzzheng/OccSora.",
        "updated": "2024-05-30 17:59:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20337v1"
    },
    {
        "title": "RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text",
        "authors": "Jiaben ChenXin YanYihang ChenSiyuan CenQinwei MaHaoyu ZhenKaizhi QianLie LuChuang Gan",
        "links": "http://arxiv.org/abs/2405.20336v1",
        "entry_id": "http://arxiv.org/abs/2405.20336v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20336v1",
        "summary": "In this work, we introduce a challenging task for simultaneously generating\n3D holistic body motions and singing vocals directly from textual lyrics\ninputs, advancing beyond existing works that typically address these two\nmodalities in isolation. To facilitate this, we first collect the RapVerse\ndataset, a large dataset containing synchronous rapping vocals, lyrics, and\nhigh-quality 3D holistic body meshes. With the RapVerse dataset, we investigate\nthe extent to which scaling autoregressive multimodal transformers across\nlanguage, audio, and motion can enhance the coherent and realistic generation\nof vocals and whole-body human motions. For modality unification, a\nvector-quantized variational autoencoder is employed to encode whole-body\nmotion sequences into discrete motion tokens, while a vocal-to-unit model is\nleveraged to obtain quantized audio tokens preserving content, prosodic\ninformation, and singer identity. By jointly performing transformer modeling on\nthese three modalities in a unified way, our framework ensures a seamless and\nrealistic blend of vocals and human motions. Extensive experiments demonstrate\nthat our unified generation framework not only produces coherent and realistic\nsinging vocals alongside human motions directly from textual inputs but also\nrivals the performance of specialized single-modality generation systems,\nestablishing new benchmarks for joint vocal-motion generation. The project page\nis available for research purposes at https://vis-www.cs.umass.edu/RapVerse.",
        "updated": "2024-05-30 17:59:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20336v1"
    }
]