Don’t drop your samples! Coherence-aware training benefits Conditional diffusion
NicolasDufour1,2,VictorBesnier3,VickyKalogeiton2,DavidPicard1
1LIGM,ÉcoledesPonts,UnivGustaveEiffel,CNRS,Marne-la-Vallée,France 2LIX,CNRS,ÉcolePolytechnique,IPParis 3Valeo.ai,Prague
Baseline Filtered Weighted CAD(Ours)
Prompt:Avibrant,multicoloredfurrywolfwithneonhighlightsplayinganelectricguitaronstage;trendingonartstation
Prompt:AshantyversionofTokyo,newrusticstyle,boldcolorswithallcolorspalette,videogame,genshin,tribe,fantasy,overwatch
Figure1.ImagesgeneratedwithaRINmodeltrainedwithdifferenthandlingofthemisalignmentbetweentheimageanditsassociatedtext
attraining.Comparedtodoingnothing(baseline),removingmisalignedsamples(filtering)orweightingthelossaccordingly(weighted),our
Coherence-AwareDiffusiontraining(CAD)generatesmorevisuallypleasingimageswhilebetteradheringtotheprompt’ssubject.
Abstract modelonboththeconditionalinformationandthecoherence
score. Inthisway,themodellearnstoignoreordiscountthe
conditioningwhenthecoherenceislow. WeshowthatCAD
Conditionaldiffusionmodelsarepowerfulgenerativemodels
istheoreticallysoundandempiricallyeffectiveonvarious
thatcanleveragevarioustypesofconditionalinformation,
conditionalgenerationtasks. Moreover,weshowthatlever-
suchasclasslabels,segmentationmasks,ortextcaptions.
agingcoherencegeneratesrealisticanddiversesamplesthat
However,inmanyreal-worldscenarios,conditionalinfor-
respectconditionalinformationbetterthanmodelstrained
mationmaybenoisyorunreliableduetohumanannotation
oncleaneddatasetswheresampleswithlowcoherencehave
errors or weak alignment. In this paper, we propose the
beendiscarded. Codeandweightscanbefoundhere.
Coherence-AwareDiffusion(CAD),anovelmethodthatin-
tegratescoherenceinconditionalinformationintodiffusion
models,allowingthemtolearnfromnoisyannotationswith-
1.Introduction
out discarding data. We assume that each data point has
an associated coherence score that reflects the quality of Conditional Diffusion models excel in image generation
theconditionalinformation. Wethenconditionthediffusion whileaffordinggreaterusercontroloverthegenerationpro-
1
4202
yaM
03
]VC.sc[
1v42302.5042:viXracessbyintegratingadditionalinformation[1,47]. Thisextra involvevarioustypesofconditioning: textfortext-to-image
dataenablesthemodeltoguidethegeneratedimagetowards generation, labelsforclass-conditionedimagegeneration,
aspecifictarget,leadingtoimprovedvariousapplications andsemanticmapsforpaint-by-wordimagegeneration. In
includinghigh-qualitytext-to-imagegeneration[46],aswell textconditioning,weusetheCLIPscore[42]toestimatethe
asothermodalitiessuchasdepthorhumanbodypose[62]. coherencebetweentheimageanditsaccompanyingcaption.
Furthermore, theaccessibilityofopen-sourcemodelslike Forclass-conditionalgeneration,weemployanoff-the-shelf
StableDiffusionhasdemocratizedtheuseofthistechnology, confidence estimator to gauge the coherence between the
alreadycausingsignificantshiftsinvariousdomainssuchas imageanditslabel. Concerningsemanticmaps,wederive
design,art,andmarketing. pixel-levelcoherencescoreseitherbyautomaticallygenerat-
Trainingconditionaldiffusionmodelsrequiressubstantial ingthembasedonclassboundariesorbyusinganoff-the-
volumesofpaireddatacomprisingthetargetimageandits shelfconfidenceestimator. Ourevaluationsspanmultiple
correspondingcondition. Intext-to-imagegeneration,this datasetssuchasCOCO[33]forzero-shottext-to-imagegen-
pairinginvolvesanimageandadescriptivecaptionthatchar- eration,ImageNet[11]forclass-conditionedgeneration,and
acterizesboththecontentandthestyleoftheimage. Simi- ADE-20K [63] for semantic maps. Our results show that
larly,forclassconditionalgeneration,thepairconsistsofan includingthecoherencescoreinthetrainingprocessallows
imageanditscorrespondingclasslabel. Besidesthetechni- trainingdiffusionmodelswithbetterimagequalityandthat
calchallengesassociatedwiththeacquisitionofextremely aremorecoherentwithwhattheyareprompted.
largequantitiesofpaireddata,ensuringaccuratealignment Insummary,ourcontributionscanbeoutlinedasfollows:
betweenimageandtextconditionsisstillanopenresearch ✓ InnovativeTrainingApproach: Wepresentcoherence-
questioninthecommunity,asattestedbythelargeamount awarediffusion(CAD),anovelmethodfortrainingcon-
ofrecentworkinthedomain[32,60]. Inpractice,largeweb- ditionaldiffusionmodelsinthepresenceofannotation
scraped datasets, such as LAION-5B [49] or CC12M [4], imperfections. By incorporating a coherence score be-
containabundantnoisypairsduetotheircollectingprocess. tweenthetargetimageanditsassociatedcondition,our
Tocleanthepairs,henceensuringalignmentofhigherqual- modelcanadaptandfine-tunetheinfluenceofthecondi-
ity,theprevailingstrategyfiltersoutsamplesthatfailtomeet tiononthegenerationprocess.
anarbitrarilychosencriterion,oftendonethroughtechniques ✓ FlexibleInferenceScheme: Weintroduceaversatilein-
like thresholding the CLIP-score [40–42]. This approach, ferencescheme,inwhichmanualtuningofthecoherence
however, has two main drawbacks: first, it is challenging scoreduringthegenerationenablesthemodulationofthe
to adjust the criterion accurately and more importantly, it condition’simpactonimagegeneration.Additionally,we
discardsmanyhigh-qualitysamplesthatcouldpotentially refinetheclassifier-freeguidancemethodunderthisnew
enhancegenerationqualityirrespectiveofthecondition. For inferencescheme,resultinginenhancedimagequality.
instance,outofthe50Binitially-collectedtext-imagepairs, ✓ Wide Applicability: Demonstrating the versatility of
only10%wereleftinLAION-5B[49],thusdiscarding90% CAD,weevaluateitacrossthreediversetasksinvolving
ofthesamples,i.e. 45Bimages. different types of conditions (text, class labels, and se-
manticmaps). CADproducesvisuallypleasingresults
Insteadofdiscardingthevastmajorityoftrainingsam-
acrossalltasks,emphasizingitsgenericapplicability.
ples,inthiswork,weleveragethemtolearnsimultaneously
conditionalandunconditionaldistributions. Specifically,we
2.RelatedWork
introduceanovelapproachthatestimateswhatwecallthe
coherence score, which evaluates how well the condition Conditional generation. Previous attempts to condition
corresponds to its associated image. We incorporate this generative models were focused on GANs [16]. Class-
coherencescoreintothetrainingprocessbyembeddingit conditional[35]wasthefirstwaytointroduceconditioning
intoalatentvector,whichissubsequentlymergedwiththe in generative models. This is a simple global condition-
condition. Thisadditionalinformationenablesthediffusion ing. However,itlackscontrolovertheoutput. Toincrease
modeltodeterminetheextenttowhichtheconditionshould thecontrolpowerofconditioning,morelocalconditionings
influencethegenerationofatargetimage. Duringinference, were proposed such as drawings, maps, or segmentation
ourmethodhastheflexibilitytotakeasinputthecoherence maps [26]. Segmentation maps conditioning [14, 38, 64]
score,therebyallowinguserstovarytheimpactofthecon- proposethemostcontroltotheuser. Indeed,theusercan
ditiononthegenerationprocess,asillustratedinFigure1. notonlyspecifytheshapeoftheobjectsbutalsoper-object
Inaddition,tofurtherimprovethegeneratedimagequality, classinformation. Semanticmasksarehowevertediousto
werefinetheClassifier-Free-Guidancemethod(CFG)intro- draw,whichimpactsusability. Text-conditionedmodels[61]
ducedin[22]byleveragingthegapbetweenhighandlow offeracompromise. Theycanprovidebothglobalandlocal
coherencescores. conditioningandareeasytoworkwith. Recently,diffusion
Weevaluateourapproachacrossthreedistincttasksthat modelshavemadegreatadvancesinthisdomain.
226.0
79
25.5
78
25.0
77
24.5 CLIPScore 76
FID
24.0
75
23.5
74
23.0 73
1 0.86 0.71 0.57 0.43 0.29 0.14 0
0 Coherencescore 1
Coherence
(a) (b)
Figure2.(a)Examplesofimagesgeneratedwiththeinputcoherencescorebetweenthepromptandthetargetimage.Thescorevariesfrom
0(nocoherence)to1(maximumcoherence).Highercoherencescorestendtogenerateimagesthatadheremoreeffectivelytotheprompt.
Topprompt: “araccoonwearinganastronautsuit. Theracoonislookingoutofthewindowatastarrynight;unrealengine,detailed,
digitalpainting,cinematic,characterdesignbypixarandhayaomiyazaki,unreal5,daz,hyperrealistic,octanerender”,bottomprompt:“An
armchairintheshapeofanavocado”(b)Increasingthecoherencefrom0to1,CLIPScoreincreasesandFIDdecreases.
Diffusionmodels. Diffusionmodels[23,51–53]havere- sivelypositivelabelsaccompaniedbyconfidencescores. To
centlyattractedtheattentionofresearchinimagegeneration. bringamorepracticalperspective,[2]introducedinstance-
ComparedtoGANs,theyhavebettercoverageoverthedata dependent noise scored by confidence, where this score
distribution,areeasiertotrainandoutperformtheminterms alignswiththeprobabilityoftheassignedlabel’saccuracy.
ofimagequality[12]. Architecture-wise,diffusionmodels Thenegativeimpactofnoisylabelshasbeenmitigatedwith
relymostlyonmodifiedversionsofaU-Net[12,23,52].Re- changesinarchitecture[8,15],intheloss[45]orfilteringthe
centworkshavehowevershownthatotherarchitecturesare noisysamples[19].Morerecently,[28]proposeasimilarap-
possible[24,39]. Inparticular,RIN[27]proposesamuch proachtooursbyconditioninganimagecaptionermodelby
simplerarchitecturethantheU-Netachievingmoreefficient theCLIP-scoretomitigatetheimpactoftextmisalignment.
training. Theyrecentlyhavebeenalotofworks[1,43,47] Instead,wefocusonimagesynthesis,wherewecondition
scalingupthesemodelsonhugetext-to-imagedatasets[49]. thediffusionmodelwithacoherencescore.
StableDiffusion[46],StableDiffusionXL[41],Paella[44]
orWuerstchen[40]haveprovidedopen-sourceweightsfor 3.Coherence-AwareDiffusion(CAD)
their networks, which has allowed an explosion in image
generation.ControlNet[62]hasshownthatfine-tuningthese Inthiswork,wewanttoimprovethetrainingofthediffu-
modelsallowsforveryfine-grainedcontrolovertheoutput sionmodelinthepresenceofmisalignedconditioning. We
withlotsofdifferentconditioningmodalities. Recently,con- maketheassumptionthatforeachtrainingsample,acoher-
sistencymodels[34,54]haveshownthatbytrainingmore encescoremeasureshowcoherenttheconditioningiswith
withadifferentloss,inferencecanbedoneinsmallamounts respecttothedata. Weproposetoconditionthediffusion
ofsteps(2-4steps). Allthesetext-to-imagenetworkshave model on this coherence score in addition to the original
beentunedonverynoisyweb-scrappeddata. Wearguein condition. Bydoingso,themodellearnstodiscardthelow-
thispaperthatthisnoisecauseslimitationsinthetraining. coherenceconditionsandfocusonthehigh-coherenceones.
Concurrentworks[3,6,17,50]proposetotacklethistask Consequently,ourmodelcanbehaveaseitheraconditional
throughre-captioning,butthisrequireslotsofresourcesto oranunconditionalmodel. Low-coherencesamples, lead
trainagoodcaptionerthatoutputsdetailedcaptionswithout tounconditionalsampling, whilehigh-coherencesamples
hallucinatingdetails. Asshownby[3],italsorequiresbridg- leadtoconditionalsamples. Buildingonthis,weredesign
ingthegapbetweentrainandtesttimeprompts. Instead,our classifier-free guidance to rely on coherence conditioning
approachismuchsimplerincurrenttrainingsetups. insteadofdroppingouttheconditioningrandomly.
Learning with noisy conditioning has been widely ex-
3.1.ConditionalDiffusionModels
ploredwhenconsideringclassification. Forbinaryclassifi-
cation, [37] study machine learning robustness when con- Wefirstprovideanoverviewofconditionaldiffusionmodels.
frontedwithnoisylabels,while[25]trainaDNNwithexclu- Thesemodelslearntodenoiseatargetatvariousnoiselev-
3
erocSPILC
DIFels. Bydenoisingsufficientlystrongnoises,weeventually conditionaldiffusionmodelrelyingonattention,thereisno
denoisepurenoise,whichcanthenbeusedtogenerateim- guaranteethatthelabelisactuallyused.
ages. Eachdiffusionprocessisassociatedwithanetworkϵ , Tostrengthentheuseofthelabel,weproposeamodifi-
θ
whichperformsthedenoisingtask. Totrainsuchanetwork, cationtotheClassifierFreeGuidance(CFG)method[22]
wehaveX animageandyitsassociatedconditioningcom- thatleveragesthecoherence. CFGusesbothaconditional
ingfromp thedatadistribution. Weuseanoisescheduler and unconditional model to improve the quality of gener-
data
γ(t),whichdefinesX whichistheinputimagecorrupted atedsamples. Tolearnsuchmodels,aconditionaldiffusion
t
with Gaussian noise at the t-th step of diffusion. such as modelisusedandtheconditioningisdroppedoutforapor-
(cid:112) (cid:112)
X = γ(t)X+ 1−γ(t)ϵ,whereϵ∼N(0,1)thenoise tionofthetrainingsamples. TheoriginalCFGformulation
t
wewanttopredictandt∈[0,1]thediffusiontimestep. Dur- isasfollows:
ingtraining,conditioningisprovidedtoϵ . Theobjectiveof
θ
thediffusionmodelistominimizethefollowingloss: ϵˆ θ(x t,y)=ϵ θ(x t,y)+ω(ϵ θ(x t,y)−ϵ θ(x t,∅)) , (3)
L =E [∥ϵ−ϵ (X ,y,t)∥] , (1) withωtheguidancerate. Instead,weproposeacoherence-
simple (X,y)∼pdata,t∼U[0,1] θ t
awareversionofCFG(CA-CFG):
where∥·∥denotestheL norm.
2
One observation is that the conditioning is implicitly ϵˆ (x ,y)=ϵ (x ,y,1)+ω(ϵ (x ,y,1)−ϵ (x ,y,0)) .
θ t θ t θ t θ t
learnedbythediffusionmodel,asthediffusionlossisonly (4)
enforced on the image and not on the conditioning itself. Thismodificationremovestheneedtodropoutthecondition-
Thismotivatesourhypothesisthatremovingdatawithlow ing. Instead,wedirectlyusethenoiseintheconditioningto
labelcoherencecanharmthetrainingofthediffusionmodel. drivetheguidance.
Eveniftheconditioningisnotwellaligned,theimagestill
belongstothedistributionthatweaimtolearn. Bydiscard- 4.Experiments
ingsuchdata,weweakenthedistributionestimator.
In this section, we will analyze 3 tasks: text, class, and
3.2.Integratinglabelinformationintothediffusion semanticallyconditionedimagegeneration. Wedescribethe
model experimentalsetup,andanalyzequantitativeandqualitative
resultstobetterunderstandtheinnerworkingsofCoherence-
Weassumethatforeverydatapoint(X,y)wehaveanasso-
awarediffusion.
ciatedc,thecoherencescoreofywherec∈[0,1]. Ourgoal
is to incorporate label coherence into the diffusion model
4.1.ExperimentalsetupandMetrics
todiscardonlytheconditioningthatcontainslowlevelsof
coherencewhilecontinuingtotrainontheimage. Avalue Experimentalsetup. Fortext-conditionalimagegenera-
ofc=1indicatesthatyisthebestpossibleannotationforX, tion,weuseamodifiedversionofRIN[27]. Tomapthetext
whilec=0suggeststhatyisapoorannotationforX. toanembeddingspace,weuseafrozenFLAN-T5XL[9].
Toachievethis,wemodifytheconditioningofthediffusion Wethenmaptheembeddingwith2self-attentiontransformer
modelϵ toincludebothyandc,usingthefollowingloss: layersinitializedwithLayerScale[56]. Wefinallyaddthe
θ
conditioningtothelatentbranchofRINateachRINBlock
L simple =E (X,y,c)∼pdata,t∼U[0,1][∥ϵ−ϵ θ(X t,y,c,t)∥] . withacross-attentionlayer. Themodelhas188Mparame-
(2) tersandwetrainitfor240Ksteps. Wetrainthesemodels
We refer to this kind of models as coherence coherence- onamixofdatasetscomposedofCC12M[5]andLAION
awarediffusion(CAD)models. Byinformingthediffusion Aesthetics 6+ [49]. To estimate the coherence score, we
modelofthecoherencescoreassociatedwithsamples,we use MetaCLIP H/14 [58] that we then bin into 8 equally
avoidfilteringoutlow-confidentsamplesandletthemodel distributeddiscretebins. Wethenusethenormalizedindex
learnbyitselfwhatinformationtotakeintoaccount. Avoid- between0and1asthecoherencescore. Wecompareour
ingthefilteringallowsustostilllearnXeveninthepresence methodto3baselines: "Baseline"isamodelwherewejust
ofnoisylabels. trainwithoutcoherence,"Filtered"correspondstoamodel
wherewediscardthe3lesscoherentbins,and"Weighted"
3.3.Test-timeprompting
corresponds to a model where we weight the loss of the
After training a model with different levels of coherence, modelbythenormalizedcoherenceofthesample. When
we can thus prompt it with varying degrees of coherence. usingCoherentAwareprompting,wesamplearandomset
When we prompt with minimal coherence, we obtain an ofcharactersthatweusewithcoherence-scoreofzeroasthe
unconditionalmodel. Ontheotherhand,whenweprompt negativeprompt.
withmaximalcoherence,wegetamodelthatisverycon- Fortheclass-conditionalimagegenerationexperiments,we
fident about the provided label. However, like any other relyonRIN[27]andusethesamehyperparametersasthe
4COCO-10K 1.0 140
130
Method ω FID↓ CLIPScore↑ P↑ R↑ D↑ C↑ 0.8 120
Baseline 10 91.9 25.96 0.281 0.047 0.181 0.222 0.6 110
100
Weighted 5 98.3 25.15 0.192 0.046 0.111 0.155 0.4
90
CAF Dilte (Ore ud rs) 11 50 8 65 9. .8 4 2 26 6. .5 12 6 0 0. .2 38 71 3 00 .. 00 76 81 00 .. 21 67 55 00 .. 32 13 53 00 .. 02
Image
quaC B laA itsD yeline TexF W til et ce ig ore h hd te ed
rence
78 00
22
C BaA sD el 2in 3eModel
C2 L4
IF W Pil et Se ig cre h od t re ed
25 26
(a) (b) (c)
Figure3.Text-to-imagegenerationresults.(a)Quantitativeresultsfortext-to-imagegeneration.WeshowthatCADachievessignificantly
lowerFID,precision,recall,densityandcoveragewhilekeepingsimilarCLIPscore.(b)Userstudyresults.Usershadtoindicatethehighest
qualityimageandthemostadheringtothepromptamongpairsofimagescorrespondingtoourCADmethodandoneofbaseline,filtered
orweightedmethod. (c)FIDversusCLIPonthetext-to-imagetaskforvaryingdegreesofguidanceω. WeshowthatCADachievesa
significantlybettertrade-offwithamuchlowerFIDforthesameCLIPscore.
authors. Weexperimentonconditionalimagegenerationfor a10KsamplessubsetofCOCO[33]inazero-shotsetting.
CIFAR-10 [29] and Imagenet-64 [11] for which we artifi- Forclass-conditional,wecomputetheInceptionScore[48]
ciallynoisethelabel. Weextractthecoherencescorefrom (IS).WealsoaddtheAccuracy(Acc)metricaimingateval-
pre-trainedclassifiersinthefollowingway: Were-sample uatinghowwelltheimagegeneratortakesintoaccountthe
withsometemperatureβ anewlabelfromthelabeldistribu- conditioninganddefinedasAcc(g)=E [1 ],
c∈Cat(N) f(g(c))=c
tionpredictedbytheclassifier. Wethenconsidertheentropy whereg(.)isthegeneratorwewanttoevaluate,f(.)isaclas-
ofthedistributionasthecoherencescore. After,weusea sifier,andCat(N)isthecategoricaldistributionofN labels.
sinusoidalpositionalembedding[57]thatwemapwithan For CIFAR-10, we use a Vision Transformer [13] trained
MLP.Weaddthiscoherencetokeninthelatentbranchof onCIFAR-10,andforImageNet,weuseaDeiT[55]. For
RIN,similartotheclasstoken. segmentation,insteadofAccuracy,wecomputethemean
Forsemanticsegmentationconditionedexperiments,weuse IntersectionoverUnion(mIoU)insteadoftheAccuracy.
ControlNet [62] to condition a pre-trained text-to-image
Stable-Diffusion[46]modelwithbothsemanticandcoher- 4.2.Analysis
ence maps concatenated. The training and evaluation of
Coherenceconditioning. Here,weexplorethebehaviorof
ourmethodareperformedontheADE20Kdataset[63],a
ourproposedcoherence-awarediffusionmodelattesttime.
large-scalesemanticsegmentationdatasetcontainingover
Forthetextconditionalsetting,weobservefromFigure2b
20,000 images with fine-detailed labels, covering diverse
thatthecoherenceandthequalityofthegeneratedimage
scenesandclasses. Sincecaptionsarenotavailableforthis
increaseasthecoherenceincreases. Indeed,FIDdecreases
dataset, we use BLIP2 [31] to generate captions for each
andtheCLIPScoreincreases. Intheclass-conditionalsetup,
imageinthedatasetsimilarlyto[62]. Weuseapre-trained
wepromptaCADmodeltrainedonresampledImageNetand
Maskformer[7]ontheCOCODataset[33],toextractthe
reportresultsinTable5(a). Similartotheprevioussetting,
segmentationmapanditsassociatedconfidence(MCP[20])
foreachimageintheADE20kdataset1. Weuseconfidence when the model has very high coherence, it achieves the
bestFIDandaccuracy. However,whenthecoherencescore
asourcoherencescore. Moredetailsabouttheexperimental
decreases, theaccuracydecreasesaswellanddropsto 1
setupareavailableinthesupplementary. N
whenthecoherencegoesto0. Thisvalidatesourhypothesis
thatinthepresenceoflow-coherencesamples,ourproposed
Metrics. To evaluate image generation for all condition modelbehaveslikeanunconditionalmodel. Furthermore,
types,weusetheFrechetInceptionDistance[21](FID)that even if the FID increases, it remains close to the FID of
evaluatesimagequality. WealsousePrecision[30](P),Re- theconditionalmodel,whichimpliesthatourCADsamples
call [30] (R), Density [36] (D) and Coverage [36] (C) as imagesareclosetothetrainingdistribution.
manifoldmetrics,allowingustoevaluatehowwelltheman-
Qualitatively,fortext-to-imagegeneration,wepromptthe
ifoldofthegeneratedimagesoverlapswiththemanifoldof
modelwithvaryingcoherencescoresfrom0to1anddisplay
therealimages. Fortext-conditional,wealsocomputethe
resultsinFigure2a. Weobservethatwhenthecoherence
CLIPScore[42]andevaluatemetricsonCLIPfeatureson
increases,theoutputsareclosetotheprompt. Forinstance,
inthebottomfigure,thegeneratedimagedisplaysanavo-
1ItisworthnotingthatusingaMaskformertrainedonthesamedataset
cadoarmchair,whereavocadoandarmchairaresuccessfully
wouldresultinhighconfidencemapeverywhereduetoitshighperformance
onthetrainingset[18] mixed. Evenamorecomplexprompt, liketheraccoonat
5
ecnereferP
resU
DIFω=0.0 ω=1.0 ω=5.0 ω=20.0
Figure4.Coherence-AwareClassifier-freeGuidanceforclassesMalamuteandIceCreamwithguidanceratesω∈{0,1,5,20}.
thetop,followscloselythetextualdescription. Theraccoon we compute different coherence scores. In the plot, high
doeswearanastronautsuitandislookingthroughthewin- coherencetranslatestolowtransparency. Weobservethatas
dowatastarrynight. Similarly,asthecoherencedecreases, thecoherencedecreases,theembeddingsofallclassestend
theimagesstarttodivergefromtheoriginalprompt. The toconvergeintothesameembedding(center). Thiscorrob-
avocado chair starts to first lose the "avocado" traits until oratesourhypothesisthatthemodelusesthecoherenceto
thereisonlyachairandattheendanobjectthatdoesnot learnbyitselfhowmuchtorelyonthelabel.
looklikeanavocadoorachair. Atthetop,wefirstlosethe Coherence-awareclassifier-freeguidance. Here, weex-
window, then the raccoon. We note that contrary to class aminetheimpactofthecoherence-awareclassifierguidance.
conditional(asseenbelow),wedonotconvergetoatotally Forthis,wefirstcomputedifferentguidanceratesranging
randomimage. Instead,somefeaturesfromthepromptare from0to30with250stepsofDDIM,andthenweplotthe
preserved, such as the astronaut suit and the starry night. FIDvstheCLIPScoretheclassifieraccuracyfordifferent
ThisishighlylinkedtotheCLIPnetworkbiases,whichmay rates. Figure5(b)illustratesthis. Weobserveatrade-off
paylessattentiontolesssalientpartsofanimagesuchas betweenclassifieraccuracyandFID.Specifically,themore
thebackground,andaremoresensitivetothemainsubject. weincreasetheguidance,themoretheaccuracyincreases,
Similarly,forclass-conditional,wepromptaCADmodel butatthecostofhigherFID.
trainedonresampledImageNet,withdifferentcoherences Qualitatively,thisbehaviourisalsopresentinFigure4:at
andclasses. WesamplewithDDPMbutweseedthesam- alowerguidancerate,theimagesaremorediversebutatthe
plingtohavethesamenoisewhensamplingdifferentclasses. costofloweraccuracywithrespecttotheclass. Thispattern
Table 5 (c) illustrates the results, where we observe that isbestshownintheMalamuteexamplewhenω =20(first
when prompted with high coherence, CAD samples have partinFigure4),whereallmalamuteshaveasimilarpose
thedesiredclass. However,asthecoherencedecreases,the withtheirtongueshangingandsimilarly,andinthethirdpart
samples get converted into samples from random classes. whereallice-creamslooksimilar,i.e.,onewhiteice-cream
Furthermore,samplesthatusethesamesamplingnoisecon- scoopwithredfillings.
vergetowardsthesameimageinthelowcoherenceregime. Interestingly,wealsoobservethatsomeguidanceleads
Thisshowsthatwhenthelabelcoherenceislow,CADdis- tooptimalresults. InFigure5,whenω = 1,theFIDisat
cardstheconditioningandinsteadsamplesunconditionally. itslowestpoint,andtheaccuracyishigherthanthedefault
Tobetterunderstandtheunderlyingmechanism,wede- modelthathasω =0. ThisisalsoshowninFigure4,where,
sign the following experiment. We modify the proposed whenω =1,samplesbestcombinediversityandfidelity.
modelsothatinsteadofaddingbothaclassandacoherence
4.3.Results
tokentotheRINnetwork,wemergethemintoasingletoken
withanMLP.Figure5(e)displaysthet-SNEplotsofthe In this section, we report image generation results condi-
outputoftheMLPforeveryclassinCIFAR-10forwhich tionedontext,class,andsemanticmaps.
6Coherence
22 Acc with 30
101 20 0.2 25 0.2 0.5 0.8
0.5
18 0.8 20
16 FID with
102 0.2 15
14 0.5
0.8 10
12
1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8
Coherence Accuracy
(a) (b) (c)
ImageNet CIFAR-10
β Method FID IS Acc P R D C FID IS Acc P R D C
Conditional 7.56 34.26 0.475 0.595 0.610 0.768 0.706 8.97 10.13 0.954 0.630 0.573 0.817 0.758
Baseline 14.38 20.46 0.168 0.539 0.579 0.595 0.505 5.66 9.79 0.507 0.659 0.614 0.940 0.809
0.5 Filtered 10.20 26.59 0.338 0.573 0.608 0.707 0.645 9.48 9.53 0.634 0.679 0.547 1.021 0.789
CAD 9.11 25.97 0.327 0.571 0.610 0.714 0.633 4.75 9.69 0.906 0.688 0.588 1.059 0.821
(d) (e)
Figure5. Impactofcoherenceonthemodel. Top: (a)ImpactonFIDandAccuracyofpromptingamodel(CADonImageNetwith
β =0.5).(b)FIDvsAccuracy(usingCA-CFG).Wevarytheguidanceratefrom0to25.(c)Impactofpromptingwithdifferentcoherence
scoresonimagegeneration.Lowcoherenceindicatesconvergencetowardsanunconditionalmodel.Bottom:(d)Quantitativeresultsfor
class-conditionalimagegeneration. Ourcoherenceawarediffusion(CAD)iscomparedtoabaselinemodelandatrainingsetfiltering
strategyfordifferentlevelsoflabelnoiseβ.WeshowthatCADachieveshigherfidelityandbetteraccuracy.(e)TSNEofamixedembedding
oftheclasslabelandthecoherencescoreonCIFAR-10.Eachcolordenotesaclassandthetransparencyshowsthecoherencelevel:the
moretransparent,thelessconfidence.
Qualitativeresultsfortext-conditionedimagegeneration. methodoutperformsothermethodsonallmetricsexceptfor
InFigure1,weobservethatcoherence-awarediffusionfor theCLIPScore. Inparticular,ourmethodachievesabetter
textualconditioningallowsforbetterpromptadherenceand FIDby15pointsthanthesecond-bestmethod,i.e. thefil-
better-lookingimages. Inthefirstrow,weobservethatCAD teredmodel. Wecorroboratetheseresultswithauserstudy
istheonlymethodthatcapturesthedetailsoftheprompt, in Figure 3 (b), where we generate images for randomly
such as having the wolf play the guitar. Indeed, the base- sampledcaptionsinCOCO.Foreachmethod,wegenerate
lineandfilteredmodelsoutputawolf,butmostgenerations 10pairsofimagesandweask36userstovotefortheimage
displayonlyahead. Theweightedmodelperformsslightly withthebestquality,andfortheonewiththemostcoherence
betterbutitlacksquality. Furthermore,ourmodeldisplays totheprompt. Ourmethodisoverwhelminglypreferredto
higher diversity in the output styles. For instance, this is othermethods. Inparticular,usersprefertheimagequality
visibleinthebottomrowwhereourmodeldisplaysavariety ofourimages95%ofthecasesandfindourimagesbetter
ofstreetimageswhereastheothermethodstendtohavea alignedwiththepromptsby89%. Notably,theuserstudy
collapsedoutput. Forthisprompt,ourmodelalsodisplays revealsawell-establishedlimitationinsuchmodels,i.e. FID
betterimagequalitycomparedtoothermethods. vs CLIPScore tradeoffs do not necessarily correlate well
withhumanperception,asshownalsoinSD-XL[41].
Quantitativeresultsfortext-conditionedimagegenera-
tion. InFigure3(c),wecompareourmethodtoaclassical Quantitative results for class-conditional image gener-
diffusionmodel(baseline),toamethodwhereallsamples ation. In Table 5 (d), we compare to a baseline where
withCLIPScorelowerthan0.41arefilteredasiscommonly we do not use the coherence score, and a filtered model,
done with LAION-5B (Filtered), and one where the dif- wherewefilterallsampleswithcoherencescoreslowerthan
fusion loss is wheighted by the CLIPScore of the sample 0.5. Whenfiltering,weobserveonCIFARthatthemodel’s
(Weighted). WevarythecoherenceandplottheFIDwith performancedramaticallydrops. FIDsareworsethanthe
respect to the CLIPScore. We observe that our method baseline, showing that dropping images prevents generat-
achievessignificantlybetterFID/CLIPtradeoffthantheother inghigh-qualityimages. CADdisplaysimprovedAccuracy
methods. InFigure3(a),wereportthemetricsfortheguid- overthebaselinewhilehavingbetterimagequalitythanthe
anceparameterthatgivesthebestFID.Weobservethatour filteredbaseline.
7
ycaruccA
DIF DIF
lebal
ssalC𝑐=1 Canny edges Softmax 𝜎 𝜎+ edit 𝜎+ edit 𝜎+ edit
ADE20k COCO
Metric Baseline CAD Baseline CAD
Semantic map FID 33.67 30.88 20.1 18.1
mIoU 22.6 23.7 35.1 35.3
P 0.785 0.844 0.7876 0.8404
R 0.757 0.824 0.6760 0.8060
D 1.029 1.0755 1.0811 1.0687
C 0.904 0.934 0.8956 0.9304
(b)QuantitativeresultsonADE20kand
𝑝=“A room with exposed brick wall and a radiator” 𝑝+ “and a cat 𝑝+ “and two MSCOCO.WereportFID,mIoU,Pre-
in a box” tiny children
cision(P),Recall(R),Diversity(D)and
(a)Examplesofimagegenerationconditionedonasemanticmap. Coverage(C).
Figure6.(a)Imagegenerationconditionedonasemanticmap.Theimagesgeneratedforagivenpromptp(shownbelow)areshown
withrespecttodifferentpixel-levelcoherencescoresc(shownabove).CoherencescoresareobtainedeithersyntheticallyusingCannyedge
detectiononthesemanticmap,orfromthemaximumofthesoftmaxprobabilityσofapre-trainedmodel.Theycanthenbeeditedeither
manually(σ+edit)orbyblittingothercoherencemaps(rightmost).(b)QuantitativeresultsonADE20kandMSCOCO.
Qualitativeresultsforsemanticconditioning. InFigure togeneraterealisticcontentinsteadofstrictlyadheringto
6(a),wepresentgeneratedimagesderivedfromthesame thesegmentationmap.
semanticmap,obtainedthroughthepredictionofasegmen-
tationnetworkonanimagefromtheADE20Kdataset[63].
Notably, we vary the prompts and coherence maps in our 5.Conclusions
experiments. Whenusingauniformcoherencemapsetat
c = 1, the generated image aligns correctly with the se- We proposed a novel method for training conditional
manticmapbutlackssemanticinformation,resultinginan diffusion models with additional coherence information.
imagethatmaynotappearmeaningful. Tointroducesyn- By incorporating coherence scores into the conditioning
theticcoherencemaps,weemployCannyedgedetectionon process, our approach allows the model to dynamically
thesemanticmap,creatingregionsoflowcoherenceatclass adjust its reliance on the conditioning. We also extend
boundaries. Thisapproachgivesthemodelmoreflexibility the classifier-free guidance, enabling the derivation of
inadjustingtheshapeofdifferentobjects,leadingtoamore conditionalandunconditionalmodelswithouttheneedfor
realisticimage(secondcolumn). dropout during training. We have demonstrated that our
Additionally,wemanuallyeditthecoherencemapbyin- method,calledcondition-awarediffusion(CAD),produces
troducingalow-coherenceregionintheformofasquare.As more diverse and realistic samples on various conditional
depictedinthe"σ+edit"columnsofFigure6(a),themodel generation tasks, including classification on CIFAR10,
tendstoadheretotheshapedefinedbythecoherencemap. ImageNetandsemanticsegmentationonADE20k.
Itstrategicallyemploysthelocationofthislow-coherence
regiontoincorporateobjectsthatarespecifiedintheprompt Limitations. The main limitation of CAD lies in the
butabsentfromthesemanticmap. Thisobservationispar- extraction of coherence scores, as unreliable coherence
ticularlyhighlightedinthefinalimageinFigure6(a),where scores can lead to biases in the model. Future research
weoverlaythecoherencescoresoftwochildrenobtained includes focusing on more robust and reliable methods
fromanotherimageontothemanipulatedcoherencemap, for obtaining coherence scores to further enhance the
while correspondingly adjusting the prompt. The model effectivenessandgeneralizabilityofourapproach.
adeptlyusesthedegreesoffreedomandshapeinformation
providedbythelow-coherenceregionofthecoherencemap
6.Acknowledgments
toseamlesslyinsertthechildrenintotheimage.
ThisworkwassupportedbyANRprojectTOSAIANR-20-
Quantitativeresultsforsemanticconditioning. InFig- IADJ-0009,andwasgrantedaccesstotheHPCresources
ure6(b), wedemonstratethatincorporatingboththeseg- ofIDRISundertheallocation2023-AD011014246madeby
mentationandthecoherencemapleadstoadecreaseinFID GENCI.WewouldliketothankVincentLepetit, Romain
forbothscenarios,withandwithoutthetextinput,indicat- Loiseau, RobinCourant, MathisPetrovich, TeodorPoncu
ingthesuperiorvisualqualityofthegeneratedimages. This andtheanonymousreviewersfortheirinsightfulcomments
behaviorisexpectedasourmodelpossessesgreaterfreedom andsuggestion.
8References [15] Jacob Goldberger and Ehud Ben-Reuven. Training deep
neural-networksusinganoiseadaptationlayer. ICLR,2016.
[1] YogeshBalaji,SeungjunNah,XunHuang,ArashVahdat,Ji-
3
amingSong,KarstenKreis,MiikaAittala,TimoAila,Samuli
[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Laine,BryanCatanzaro,etal. ediffi:Text-to-imagediffusion
Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,and
modelswithanensembleofexpertdenoisers. arxiv,2022. 2,
YoshuaBengio. Generativeadversarialnets. NeurIPS,2014.
3
2
[2] Antonin Berthon, Bo Han, Gang Niu, Tongliang Liu, and [17] JiataoGu,ShuangfeiZhai,YizheZhang,JoshSusskind,and
Masashi Sugiyama. Confidence scores make instance- NavdeepJaitly. Matryoshkadiffusionmodels,2023. 3
dependent label-noise learning possible. Int. Conf. Mach.
[18] ChuanGuo,GeoffPleiss,YuSun,andKilianQWeinberger.
Lear.,2021. 3
Oncalibrationofmodernneuralnetworks. Int.Conf.Mach.
[3] James Betker, Gabriel Goh, Li Jing, † TimBrooks, Jian- Lear.,2017. 5
fengWang,LinjieLi,†LongOuyang,†JuntangZhuang,† [19] BoHan,QuanmingYao,XingruiYu,GangNiu,MiaoXu,
JoyceLee,†YufeiGuo,†WesamManassra,†PrafullaDhari- WeihuaHu,IvorTsang,andMasashiSugiyama.Co-teaching:
wal, † CaseyChu, † YunxinJiao, and Aditya Ramesh. Im- Robusttrainingofdeepneuralnetworkswithextremelynoisy
provingimagegenerationwithbettercaptions. arxiv,2023. labels. NeurIPS,2018. 3
3 [20] DanHendrycksandKevinGimpel. Abaselinefordetect-
[4] SoravitChangpinyo,PiyushSharma,NanDing,andRadu ingmisclassifiedandout-of-distributionexamplesinneural
Soricut. Conceptual12m:Pushingweb-scaleimage-textpre- networks. ICLR,2017. 5
trainingtorecognizelong-tailvisualconcepts. CVPR,2021. [21] MartinHeusel,HubertRamsauer,ThomasUnterthiner,Bern-
2 hardNessler,andSeppHochreiter. Ganstrainedbyatwo
[5] SoravitChangpinyo,PiyushSharma,NanDing,andRadu time-scaleupdateruleconvergetoalocalnashequilibrium.
Soricut. Conceptual12M:Pushingweb-scaleimage-textpre- NeurIPS,2017. 5
trainingtorecognizelong-tailvisualconcepts. CVPR,2021. [22] Jonathan Ho and Tim Salimans. Classifier-free diffusion
4 guidance. NeurIPS,2022. 2,4
[6] JunsongChen,JinchengYu,ChongjianGe,LeweiYao,Enze [23] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-
Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, sionprobabilisticmodels. NeurIPS,2020. 3
HuchuanLu,etal. Pixart-alpha: Fasttrainingofdiffusion [24] EmielHoogeboom,JonathanHeek,andTimSalimans.simple
transformerforphotorealistictext-to-imagesynthesis. arXiv, diffusion: End-to-enddiffusionforhighresolutionimages.
2023. 3 arxiv,2023. 3
[7] BowenCheng,AlexanderG.Schwing,andAlexanderKir- [25] TakashiIshida,GangNiu,andMasashiSugiyama. Binary
illov. Per-pixelclassificationisnotallyouneedforsemantic classificationfrompositive-confidencedata. NeurIPS,2018.
segmentation. NeurIPS,2021. 5,12 3
[26] PhillipIsola,Jun-YanZhu,TinghuiZhou,andAlexeiAEfros.
[8] Lele Cheng, Xiangzeng Zhou, Liming Zhao, Dangwei Li,
Image-to-imagetranslationwithconditionaladversarialnet-
HongShang,YunZheng,PanPan,andYinghuiXu. Weakly
works. CVPR,2017. 2
supervisedlearningwithsideinformationfornoisylabeled
[27] AllanJabri,DavidFleet,andTingChen. Scalableadaptive
images. ECCV,2020. 3
computationforiterativegeneration. Int.Conf.Mach.Lear.,
[9] HyungWonChung,LeHou,ShayneLongpre,BarretZoph,
2022. 3,4,11
YiTay,WilliamFedus,YunxuanLi,XuezhiWang,Mostafa
[28] Wooyoung Kang, Jonghwan Mun, Sungjun Lee, and
Dehghani, Siddhartha Brahma, et al. Scaling instruction-
ByungseokRoh. Noise-awarelearningfromweb-crawled
finetunedlanguagemodels. arXiv,2022. 4
image-textdataforimagecaptioning. arxiv,2022. 3
[10] TimothéeDarcet,MaximeOquab,JulienMairal,andPiotrBo-
[29] AlexKrizhevsky. Learningmultiplelayersoffeaturesfrom
janowski. Visiontransformersneedregisters. arXivpreprint
tinyimages. arxiv,2009. 5
arXiv:2309.16588,2023. 11
[30] TuomasKynkäänniemi,TeroKarras,SamuliLaine,Jaakko
[11] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLi
Lehtinen, and Timo Aila. Improved precision and recall
Fei-Fei. Imagenet:Alarge-scalehierarchicalimagedatabase.
metricforassessinggenerativemodels. NeurIPS,2019. 5
CVPR,2009. 2,5
[31] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-
[12] PrafullaDhariwalandAlexanderNichol. Diffusionmodels 2: Bootstrappinglanguage-imagepre-trainingwithfrozen
beatgansonimagesynthesis. NeurIPS,2021. 3 imageencodersandlargelanguagemodels. arXiv,2023. 5
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, [32] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.BLIP-
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, 2: bootstrapping language-image pre-training with frozen
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- imageencodersandlargelanguagemodels. Int.Conf.Mach.
vainGelly,etal. Animageisworth16x16words:Transform- Lear.,2023. 2,12
ersforimagerecognitionatscale. arxiv,2020. 5 [33] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
[14] NicolasDufour,DavidPicard,andVickyKalogeiton. Scam! PietroPerona,DevaRamanan,PiotrDollár,andCLawrence
transferring humans between images with semantic cross Zitnick. Microsoftcoco:Commonobjectsincontext. ECCV,
attentionmodulation. ECCV,2022. 2 2014. 2,5,12
9[34] SimianLuo,YiqinTan,LongboHuang,JianLi,andHang Principledrecaptioningimprovesimagegeneration. arXiv,
Zhao. Latent consistency models: Synthesizing high- 2023. 3
resolutionimageswithfew-stepinference. arxiv,2023. 3 [51] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,
[35] MehdiMirzaandSimonOsindero. Conditionalgenerative and Surya Ganguli. Deep unsupervised learning using
adversarialnets. NeurIPS,2014. 2 nonequilibrium thermodynamics. Int. Conf. Mach. Lear.,
[36] MuhammadFerjadNaeem,SeongJoonOh,YoungjungUh, 2015. 3
YunjeyChoi,andJaejunYoo. Reliablefidelityanddiversity [52] Yang Song and Stefano Ermon. Generative modeling by
metricsforgenerativemodels. Int.Conf.Mach.Lear.,2020. estimatinggradientsofthedatadistribution. NeurIPS,2019.
5 3
[37] NagarajanNatarajan,InderjitSDhillon,PradeepKRaviku- [53] Yang Song and Stefano Ermon. Improved techniques for
mar,andAmbujTewari.Learningwithnoisylabels.NeurIPS, trainingscore-basedgenerativemodels. NeurIPS,2020. 3
2013. 3 [54] YangSong,PrafullaDhariwal,MarkChen,andIlyaSutskever.
[38] TaesungPark,Ming-YuLiu,Ting-ChunWang,andJun-Yan Consistencymodels. arxiv,2023. 3
Zhu. Semanticimagesynthesiswithspatially-adaptivenor- [55] HugoTouvron,MatthieuCord,MatthijsDouze,Francisco
malization. CVPR,2019. 2 Massa,AlexandreSablayrolles,andHervéJégou. Training
[39] WilliamPeeblesandSainingXie. Scalablediffusionmodels data-efficientimagetransformers&distillationthroughatten-
withtransformers. ICCV,2022. 3 tion. Int.Conf.Mach.Lear.,2021. 5,11
[40] PabloPernias,DominicRampas,andMarcAubreville. Wuer- [56] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,
stchen:Efficientpretrainingoftext-to-imagemodels. arXiv, Gabriel Synnaeve, and Hervé Jégou. Going deeper with
2023. 2,3 imagetransformers. ICCV,2021. 4
[41] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann, [57] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
TimDockhorn,JonasMüller,JoePenna,andRobinRombach. reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
Sdxl:Improvinglatentdiffusionmodelsforhigh-resolution Polosukhin. Attentionisallyouneed. NeurIPS,2017. 5
imagesynthesis. arXiv,2023. 3,7 [58] HuXu,SainingXie,XiaoqingEllenTan,Po-YaoHuang,Rus-
[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya sellHowes,VasuSharma,Shang-WenLi,GargiGhosh,Luke
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Zettlemoyer,andChristophFeichtenhofer. Demystifyingclip
AmandaAskell,PamelaMishkin,JackClark,etal. Learning data. arxiv,2023. 4
transferablevisualmodelsfromnaturallanguagesupervision. [59] YangYou, JingLi, SashankReddi, JonathanHseu, Sanjiv
Int.Conf.Mach.Lear.,2021. 2,5 Kumar,SrinadhBhojanapalli,XiaodanSong,JamesDemmel,
[43] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu, KurtKeutzer,andCho-JuiHsieh. Largebatchoptimization
andMarkChen. Hierarchicaltext-conditionalimagegenera- fordeeplearning:Trainingbertin76minutes. ICLR,2019.
tionwithcliplatents. arxiv,2022. 3 11
[44] Dominic Rampas, Pablo Pernias, Elea Zhong, and Marc [60] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and
Aubreville. Fasttext-conditionaldiscretedenoisingonvector- LucasBeyer. Sigmoidlossforlanguageimagepre-training,
quantizedlatentspaces. arXiv,2022. 3 2023. 2
[45] MengyeRen,WenyuanZeng,BinYang,andRaquelUrtasun. [61] HanZhang,TaoXu,HongshengLi,ShaotingZhang,Xiao-
Learningtoreweightexamplesforrobustdeeplearning. Int. gangWang,XiaoleiHuang,andDimitrisNMetaxas. Stack-
Conf.Mach.Lear.,2018. 3 gan: Text to photo-realistic image synthesis with stacked
[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, generativeadversarialnetworks. CVPR,2017. 2
Patrick Esser, and Björn Ommer. High-resolution image [62] LvminZhang,AnyiRao,andManeeshAgrawala. Adding
synthesiswithlatentdiffusionmodels. CVPR,2022. 2,3,5, conditionalcontroltotext-to-imagediffusionmodels. ICCV,
11 2023. 2,3,5,12
[47] ChitwanSaharia,WilliamChan,SaurabhSaxena, LalaLi, [63] BoleiZhou,HangZhao,XavierPuig,SanjaFidler,AdelaBar-
JayWhang,EmilyLDenton,KamyarGhasemipour,Raphael riuso,andAntonioTorralba. Sceneparsingthroughade20k
GontijoLopes,BurcuKaragolAyan,TimSalimans,etal.Pho- dataset. CVPR,2017. 2,5,8,12
torealistictext-to-imagediffusionmodelswithdeeplanguage [64] PeihaoZhu,RameenAbdal,YipengQin,andPeterWonka.
understanding. NeurIPS,2022. 2,3 Sean:Imagesynthesiswithsemanticregion-adaptivenormal-
[48] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki ization. CVPR,2020. 2
Cheung,AlecRadford,andXiChen. Improvedtechniques
fortraininggans. NeurIPS,2016. 5
[49] ChristophSchuhmann,RomainBeaumont,RichardVencu,
CadeGordon,RossWightman,MehdiCherti,TheoCoombes,
Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.
Laion-5b:Anopenlarge-scaledatasetfortrainingnextgener-
ationimage-textmodels. NeurIPS,2022. 2,3,4
[50] EyalSegalis, DaniValevski, DannyLumen, YossiMatias,
andYanivLeviathan. Apictureisworthathousandwords:
10Similarly,duringinferenceourarchitecturetakesasinput
a noisy version of the image, the timestep and coherence
tokens,andatextconditioningthatismappedwithafrozen
FLAN T5 XL encoder. It then predicts the noise that has
been added to the clean image. Output latents from one
denoisingsteparealsoforwardedtothenextdenoisingstep.
Figure7. ArchitectureoftheRINBlockmodifiedtoreceiveas
B.Implementationdetails
inputthecoherence.
In this section, we present the implementation details for
A.CADarchitecture eachexperimentaswellasthetrainingsetup.
Inthissection,wedescribethebuildingblocksofthepro- B.1.TextConditining
posedCADarchitecture.
Forthetextconditional,weusetheLAMBoptimizer[59]
A.1.RINarchitectureforclassconditionalCAD withaweightdecayof0.01. Weusealearningrateof0.001.
The batch size is 1024. We use a linear warmup of the
WefirstexplainouradaptationoftheRINarchitecture[27],
learning rate for the first 10k steps and then use a cosine
thatweuseinourexperimentsonclassconditionalgenera-
decay. Wetrainallmodelsfor300ksteps. WeuseanEMA
tioninthecontextofcoherenceawarediffusion. Asshown
decayof0.9999forthelast50ksteps.
in Figure 7, the RIN block is composed of two branches:
WeusetheStablediffusionVAEencoder[46]andperform
onewiththelatentsandonewiththepatches. Weconcate-
thediffusionprocessinitsembeddingspaceinwhichthe
natethetimestep,coherenceandconditioningembeddings
image tokens have dimension 32x32x4. We have 4 RIN
to the latents (olive, green and orange blocks), with the
blocks,eachhaving4self-attentionunits. Thedatatokens
coherenceembedding beingan additionof ourmethod to
dimensionis256andthelatenttokendimensionis768. The
theoriginalRINarchitecture. Then,first,thelatentsgather
inputdataisreducedto256datatokensbyusingapatch-size
informationfromtheinputpatchesviaCross-Attention. Sec-
of2.
ond,thelatentsareprocessedwithNself-attentionlayers.
Finally,thepatchesareupdatedfromthelatentsviaCross-
B.2.ClassConditioning
Attention.TheRINarchitectureconsistsofstackingmultiple
RINblocks,wherethenextRINblockreceivestheupdated Fortheclassconditionalexperiments,wefollowthehyper-
latentsandpatches. parameters provided by the authors of RIN. We use the
Duringinference,RINtakesasinputanoisyversionof LAMBoptimizerwithweightdecayof0.01. Weusealinear
the image, a class, a timestep, and a coherence token to warmupofthelearningrateforthefirst10kstepsandthen
predictthenoisethathasbeenaddedtothecleanversionof useacosinedecay. Wetrainallmodelsfor150ksteps. We
theimage. Toimprovethesampling,outputlatentsfroma useanEMAdecayof0.9999.
givenstepareforwardedasinputtothenextdenoisingstep. ForCIFAR-10,wehave3RINblocks,eachhaving2pro-
Formoredetails,see[27]. cessing units. The data tokens dimension is 256 and the
latenttokendimensionis512. Weuseapatch-sizeof2. We
A.2.CADwithtextconditioning
usealearningrateof0.003. Thebatch-sizeis256.
To enable text-conditioned image generation, we propose For ImageNet-64, we have 4 RIN blocks, each having 4
a modification to the RIN architecture, coined Text RIN
Block(seeFigure8). First,thetexttokensaremappedwith
thecoherencewith2xself-attentionlayersinitializedwith
LayerScale[55](toppartinthefigure)and16registers[10].
ThismappingisthesameforeveryTextRINblock. Note
that unlike the class conditional RIN block (Figure 7), in
our proposed Text RIN Block, the latent branch contains
onlythelatents;therearenoconcatenatedtokensanymore.
Instead, the mapped text tokens, coherence, and timestep
embeddingsprovideinformationtothelatentbranchwitha
cross-attentionlayeratthebeginningoftheTextRINblock.
Then, the rest of the architecture is the same as the RIN Figure8. ArchitectureoftheproposedTextRINBlockusedin
block. CAD.
11processingunits. Thedatatokensdimensionis512andthe Table1.QuantitativeresultsonADE20Kwhenconditioningon
latenttokendimensionis768. Weuseapatch-sizeof8. We semanticmaps.‘CADbin’encodesthecoherenceinto5equally
distributed discrete bins. ‘CAD scalar’ uses a scalar coherence
usealearningrateof0.002. Thebatch-sizeis1024.
scoreforthewholeimage. CADachievesbetterFIDduetoits
enhancedabilitytogeneraterealisticobjectsinlowcoherencere-
B.3.Semanticmapconditioning
gionsandsuperiormIoUastheleakedspatialinformationfromthe
coherencemapandthecaptionassistittogeneratebettersamples
Generationsconditionedonsemanticmapswereobtained
(seeSectionC.1formoredetails).
bytrainingaControlNet[62]. TotraintheControlNet,we
createdadatasetofimagesselectedfromtheADE20K[63]
ADE20K
andMSCOCO[33]datasets. Thisdatasetcontainstuples ControlNet FID IS mIoU P R D C
oftheform(image,caption,semanticmap,coherencemap) Baseline 33.67 14.82 22.6 0.785 0.757 1.029 0.904
CAD 30.88 14.79 23.7 0.844 0.824 1.0755 0.934
thatweregeneratedfromtheoriginalimages. Thecaptions
Baselinew/otext 74.37 5.88 5.25 0.657 0.351 0.789 0.515
areobtainedwithBLIP2[32],animagecaptioninglanguage CADw/otext 60.21 7.93 11.8 0.619 0.536 0.789 0.682
model. We utilize a Maskformer [7] trained either on the CADbin 63.47 7.97 10.8 0.5875 0.4925 0.757 0.663
CADscalar 74.69 6.15 3.11 0.6495 0.347 0.858 0.536
MS COCO dataset or ADE20k to generate the semantic
maps. To extract coherence values, weuse the maximum
class probability obtained from the softmax output of the
Maskformermodel. WeemployaMaskFormertrainedon C.1.Quantitativeresults
ADE20K to generate the coherence map for MS COCO,
Here,wequantitativelyevaluatetheeffectivenessofincor-
and conversely, use a MaskFormer trained on MS COCO
porating coherence maps for semantic segmentation. For
toobtainthecoherencemapforADE20K.Thisapproach
this, we experiment on two of the most popular datasets
helpsmitigatetheproblemofoverconfidenceinpredictions
withsegmentation: ADE20K[63]andMSCOCO[33]. To
onthetrainingset,reducingthetendencytohaveonlyhigh
evaluateourresults,weemploytheFrechetInceptionDis-
coherencescoresacrossallpixels. Weusedabatchsizeof
tance(FID)andInceptionScore(IS)forevaluatingimage
16,usingtheAdamoptimizerwithalearningrateof1e-5.
quality. Additionally,Precision(P),Recall(R),Density(D),
and Coverage (C) serve as manifold metrics, enabling an
B.4.Computationalcost
evaluationoftheoverlapbetweenthegeneratedandrealim-
Ourmethodaddsnegligibletrainingandinferencetimebe- agemanifolds. Finally,wecalculatethemeanIntersection
cause we either modify existing architectures or add non- overUnion(mIoU)byutilizingapre-trainedMaskFormer
computationally expensive components. Specifically, in to predict a segmentation map from the generated image
terms of architecture, we are replacing one of the latent andcompareitwiththeoriginalsemanticmap. Thishelps
tokens with an embedded coherence score. For the text- illustratethefidelityofthegeneratedimagestotheground
conditional,wedoaddanewcross-attentionlayer,butmost truth.
ofthecomputeisstillintheself-attentionblocks.
Inthisproject,wehaveusedapproximately25,553V100 Methodcomparison. WecomparetheresultsofourCAD
hours for preliminary experiments including the CIFAR- methodwithabaselineapproachthatexcludescoherence
10experimentsand29,489A100hoursforImageNetand informationinbothTable1andTable2,withthecomplete
text-conditionalexperiments. EachGPUhouraccountsfor results. Weconductexperimentsintwosettings: withtext
roughly 259 Wh for a total of 14,255kWh. For semantic (firsttworows)andwithouttext(lastfourrows). Addition-
segmentation,thetrainingofthedifferentControlNetswas ally, we compare against two CAD variations. Similar to
performedusingabout1,800hoursofNvidiaA100GPUs thebinningstrategyintext-to-imagegeneration,‘CADbin’
in total, or about 470kWh. The training process required encodescoherenceinto5equallydistributeddiscretebins.
approximately 100 GPU hours for each model trained on Furthermore,‘CADscalar’utilizesasingularscalarcoher-
ADE20k[63]and200GPUhoursforMSCOCO[33]. encescorefortheentireimage,equivalenttothemeanof
theoriginalcoherencemap.
C.Imagefromsemanticmapadditionalexperi-
ments Results. InTable1andTable2,showthecompleteresults
ofourmethodonADE20kandCOCOwedemonstratethat
Inthissection,weexaminetheeffectivenessofincorporating usingboththesegmentationmapsandthecoherencemaps
coherencemapsforsemanticsegmentation. Wequalitatively leadtoadecreaseinFIDforbothscenarios,includingornot
andquantitativelycomparetheresultsofourCADmethod thetextinput. Thisbehaviorisexpectedasourmodelpos-
againstabaselineapproachnotusingthecoherenceinforma- sessesgreaterfreedomtogeneraterealisticcontentinstead
tion. ofstickingtothesegmentationmapuniquely(seee.g.,the
12Table2. QuantitativeresultsonMSCOCOwhencondition- totherealimage.
ingonsemanticmaps.‘CADbin’encodesthecoherenceinto5
equallydistributeddiscretebins.‘CADscalar’usesascalarcoher- C.3.Promptgeneralization
encescoreforthewholeimage(seeSectionC.1formoredetails).
In this subsection, we demonstrate the sensitivity of our
COCO methodtothecaptioninput. WeobserveinFigure10that
ControlNet FID IS mIoU P R D C
our CAD method can successfully generalize to different
Baseline 20.1 32.6 35.1 0.7876 0.6760 1.0811 0.8956
CAD 18.1 32.0 35.3 0.8404 0.8060 1.0687 0.9304 typesofcaptions,suchas‘diningtable’,‘billiard’,or‘ping
Baselinew/otext 54.93 15.40 8.36 0.4884 0.4402 0.5297 0.5260 pongtable’andadjustthesceneaccordingly.Moreover,even
CADw/otext 37.06 18.04 12.53 0.6222 0.6502 0.7599 0.7052
whenthetableisnotexplicitlymentionedinthecaption(as
CADbin 44.63 16.39 9.76 0.5636 0.5614 0.7075 0.6402
CADscalar 55.82 15.92 8.10 0.5139 0.4382 0.5294 0.5341 seenontherightmostsideofthefigure),ourmethodexhibits
stronggeneralizationcapabilitiesandsuccessfullygenerates
thetable.
4thcolumnofFigure9). Furthermore,theimprovementin C.4.CoherenceInterpolation
themIoUscorecanbeattributedtotwofactors. First,when
theinputsegmentationmapisoflowquality,thebaseline InFigure11,wedemonstratethesignificanceofthecoher-
methodfailstocaptureimportantsceneinformation. Incon- ence map in the conditioning of the ControlNet. In this
trast,ourmethodbenefitsfromadditionalinformationfrom experiment,wemakeaninterpolationofthecoherencemap
thecoherencemap. Secondly,ourmethodbetterleverages fromthemaximumcoherencescoreeverywhere(left)tovery
thecaptioninthelowcoherenceregion,mitigatingthelimi- lowcoherence(right). Whentheinputhashighcoherence
tationofthesegmentationmap’slimitednumberofclasses throughout,thegeneratedimagelacksthepresenceofaping
(asseenin4throwinFigure10,thereisnoping-pongtable pongtableasitisnotpresentinthesemanticmap. However,
classintheCOCOdataset). asthecoherencescoredecreases,ourmethodsrecognizethe
shapeofthepingpongtableandsuccessfullygeneratesitin
C.2.AdditionalVisualizations theimage. Itisworthnotingthatevenatalowscaleofthe
coherencescore(secondcolumncorrespondsto1e-4times
WeshowadditionalresultsinFigure9,where,fromtheleft theoriginalvalue),ourmethodisstillabletoreconstructthe
columntotherightone,wehighlightthesegmentationinput, table,evenifitisnotinthesegmentationinput. Whenwe
thecoherencemap,theimagegeneratedbythebaseline,the artificiallyreducethecoherence(twotimeslesscoherence,
imagegeneratedbyourmethodsandthereferenceimage. in the last column), our method is still able to generate a
Thecoherencemaprevealsspatial/shapedetailsofthescene. consistentscenewithoutanyartifacts.
Forinstance,whencomparingourmethodtoaControlNet
trained solely with the segmentation map, our approach,
D.Classconditionalexperiments
whichincorporatesbothsegmentationandcoherence,accu-
ratelyreconstructsthecurtain’sshapeandthewindowsin D.1.Simulatingannotationnoise
thefirstrow,orreconstructsacloudinthebackoftheplane
Ourapproachforclass-conditionalimagegenerationrelies
in the second row. Moreover, the efficacy of our method
on the assumption that the dataset comes with annotated
becomesevenmoreapparentwhenthesegmentationmapis
coherencescores. However,suchscoresarenotalwaysavail-
ofpoorqualityandthecoherencescoreislow. Forinstance,
ablefortraditionalimage-generationdatasets. Toaddress
asshowninthethirdrow,thebasicControlNetattemptsto
this issue, we propose to simulate annotation noise by re-
adhere to the limited information provided by the flawed
samplingfromadatasetwithcleanannotations.
segmentationmap,resultinginascenewithmultiplearms
Weassociateanerrorprobabilityαwitheachlabel. We
displayed(thirdcolumn). Incontrast,ourapproachbenefits
assumethatwhentheannotatoriswrong,theymisclassify
fromtheflexibilitygivenbythecoherencemap,allowingfor
uniformlyoverallclasses,whereN isthetotalnumberof
moreconsistentimagegeneration. Interestingly,ourmethod
classes. Thisleadstothefollowingmodel:
alsoexhibitslocalizedself-correction,asshownonthefourth
row. Inparticular,ourmodelisrefrainingfromgenerating (cid:40)
the hand region due to its low coherence in the input. Fi- y¯∼p withp (Y¯ =k)= 1−α ify =k, (5)
nally,wedemonstratethemodel’sresponsivenesstotextual y,α y,α α otherwise.
N−1
inputinthelastrow. Wepresentinthelastrowanexample
wheretheoriginalimagedoesnotcontainsnow, buthave Wealsodefineastrategytoremaptheentiredatasetusinga
highcoherenceinthesky. Bothmodelsgeneratesomesnow normalizedentropy-basedcoherencemeasure. Weusethe
basedonthecaption,butinlinewiththecoherencemap,our normalizedentropysothat0mapstonocoherenceatalland
modeldoesnotgeneratesnowinthesky,thusbeingcloser 1 to total coherence in the label. We define the following
13Caption:Anhotelroomwithabed,chairandtable:
Caption:Awhiteairplaneontherunway:
Caption:Amansittinginfrontofaslotmachine:
Caption:Alargeroomwithping-pongtablesandpeopleplaying:
Caption:Adrillingriginthemiddleofasnowyfield:
Segmentation Coherence Baseline Ours RealImage
Figure9.QualitativeresultsonADE20K:Examplesofimagesgeneratedconditionallytotextandsemanticmaps.
14Figure10. Captiongeneralization: Ourmethodsdemonstrateremarkablecapabilityinleveragingthecoherencemaptogeneralizeto
diversepromptinputs.
High coherence Low coherence
Figure11.Coherenceinterpolation:Inthefirstcolumn,weartificiallyprovideourControlNetwithacoherencemaphavingthemaximum
valueeverywhere.Indeed,ourmodelsdonotgeneratethepingpongtables.But,assoonaswedecreasethecoherencetowarditsoriginal
value,thepingpongtablesstarttoappear.Finally,inthelastcolumn,weprovidethemodelwithacoherencemapthatishalfasconfidentas
theoriginalvalueanddemonstratethatwecangenerateanimagewithoutartifacts
coherencefunction: whereκrepresentsathresholdandβ representstheentropy
−1 (cid:18) (cid:18) α (cid:19)(cid:19) atthisthreshold. Thisfunctionconstructiondefinesalow
E(α)= (1−α)log(1−α)+αlog entropyregionbeforethethresholdandahighentropyregion
log(N) N −1
afterthethreshold.
(cid:20) (cid:21)
N −1
forα∈ 0, . (6) Finally,foreachsampleinthedataset(X,y),wesamplet∈
N
U[0,1],andassociateatargetentropyu. Wethencompute
Toensurethatthedatasethassampleswithvaryinglevels theassociatederrorprobabilityα=E−1(u),andresample
ofconfidence,wedefineatargetentropycumulativedistri- according to p to obtain the tuple (X,y¯,1−u)2. This
y,α
bution. Toachievethis,weuseapiecewise-linearfunction: process allows us to generate synthetic data points with
(cid:40) varyingdegreesofannotationnoiseandcoherence.
tβ ift<κ,
E (t)= κ (7)
β,th 1+(t−1)1−β otherwise. 2Thecoherencegoesintheoppositedirectionoftheentropy.
1−κ
15D.2.QuantitativeResults PropositionE.1. Lipschitzcontinuousconditionalneural
diffusionmodelsthatleveragecoherenceconsistentembed-
InthissectionweaddmoreresultsonImageNetwithdif-
dingsfortheconditioningareequivalenttounconditional
ferent levels of noise. We observe in Table 3 results on
modelsatlowcoherence.
ImageNetforβ ∈{0.2,0.5,0.8}. Wefirstobservetheless
coherentthelabelsaretheworsetheresultsgetinbothim- Proof. Wehavetoprovethefollowingequivalentstatement:
agequality(FID)butalsoinaccuracy(Acc). However,our Letϵ : x ,t,h(y,c) (cid:55)→ ϵˆ beaLipschitzcontinuousneu-
θ t t
methodCADmanagestoachievebetterresultsthanother raldiffusionmodelthatpredictsthenoiseϵˆ attimetfrom
t
methodsinthecontextofun-coherentlabels. Thisisfurther the noisy sample x with the help of the condition y em-
t
amplifiedwhenleveragingcoherenceawareguidance beddedusingthecoherenceconsistentembeddinghunder
coherencec. Then,∀η >0and∀x ,t,y ̸=y ,thereexists
t 1 2
Table3.Quantitativeresultsforclass-conditionalimagegeneration. C >0suchthatforall0<c≤C,wehave
Ourcoherenceawarediffusion(CAD)iscomparedtoabaseline
modelandatrainingsetfilteringstrategyfordifferentlevelsof ∥ϵ (x ,t,h(y ,c))−ϵ (x ,t,h(y ,c))∥2 <η. (9)
θ t 1 θ t 2
label noise β. We show that CAD achieves higher fidelity and
betteraccuracy. By Lipschitz property of ϵ , we have
θ
∥ϵ (x ,t,h(y ,c)) − ϵ (x ,t,h(y ,c))∥2 ≤
θ t 1 θ t 2
ImageNet L2∥h(y ,c) − h(y ,c)∥2. From the coherence con-
1 2
β Method FID IS Acc P R D C sistent property, there exists C > 0 such that for all
√
Conditional 7.56 34.26 0.475 0.595 0.610 0.768 0.706 0<c≤C,∥h(y ,c)−h(y ,c)∥< η/L.
1 2
Baseline 11.09 23.54 0.264 0.562 0.598 0.670 0.594
Filtered 8.53 29.27 0.389 0.591 0.609 0.756 0.688 Thefollowingcontrapositivenecessaryconditiononthe
0.2
CAD 8.17 27.75 0.367 0.585 0.615 0.736 0.658 coherencedirectlyfollowsfromthisproposition:
CA-CFGω=1 5.95 68.95 0.679 0.742 0.477 1.203 0.812
CorollaryE.2. Lipschitzcontinuousconditionalneuraldif-
Baseline 14.38 20.46 0.168 0.539 0.579 0.595 0.505
Filtered 10.20 26.59 0.338 0.573 0.608 0.707 0.645 fusion models that leverage coherence aware embeddings
0.5
CAD 9.11 25.97 0.327 0.571 0.610 0.714 0.633 requirehighcoherencetobehavelikeconditionalmodels.
CA-CFGω=1 5.95 68.95 0.679 0.742 0.477 1.203 0.812
Inpractice, weshowintheexperimentsthatthecoher-
Baseline 20.10 17.28 0.100 0.502 0.535 0.526 0.417
enceconsistencypropertytendstonaturallyemergeduring
Filtered 12.00 24.55 0.292 0.420 0.712 0.647 0.605
0.8 training and that consequently coherence aware diffusion
CAD 11.39 22.08 0.248 0.558 0.590 0.682 0.574
providesatunablepromptparametertosamplefromuncon-
CA-CFGω=1 6.70 44.27 0.523 0.695 0.483 1.035 0.743
ditionaltoconditionalmodels.
F.AdditionalQualitativeResults
E.Theoreticalanalysis
In this section, we provide additional samples from our
Inthissection,wemotivatetheuseofcoherenceasanaddi-
method. Most of the prompts are sampled from the Lex-
tionalconditioningfordiffusionmodels. Underassumptions
ica.artwebsite.
thatareverifiedempirically,weshowthatcoherenceaware
diffusion can transition from an unconditional model to a
conditionalmodelsimplybyvaryingthecoherencepassed
tothemodel. First,wedefineaconsistencypropertyofthe
coherenceembeddingasfollows:
Definition E.1. We denote coherence consistent a condi-
tional embedding h(y,c) of the condition y ∈ Y under
coherencec∈[0,1],if∀y ,y ∈Y wehave
1 2
lim∥h(y ,c)−h(y ,c)∥=0. (8)
1 2
c→0
In other words, an embedding is coherence consistent
ifittendstoproducethesamevectorasthecoherenceap-
proaches0. Thispropertyisasufficientconditiontocon-
strainthebehaviorofthediffusionmodel. Indeed,thefol-
lowingpropositioniseasilyderivedfromit:
16Figure12.SamplesfromourCADmodelat512resolutionwithassociatedcaption
Anold-worldgalleonnavigatingthroughtur- anoilpaintingofrainatatraditionalChinese portraitphotoofaasiaoldwarriorchief tribal
bulentoceanwavesunderastormysky litby town panthermakeup blueonred sideprofile look-
flashesoflightning ingaway seriouseyes 50mmportraitphotogra-
phy hardrimlightingphotography
a blue jay stops on the top of a helmet of Acutelittlemattelowpolyisometriccherry
Underwatercathedral
Japanesesamurai backgroundwithsakuratree blossomforestisland waterfalls lighting soft
shadowstrendingonArtstation3drendermon-
umentvalley fezvideogame.
Acozygingerbreadhousenestledinadusting ateddybearwearingblueribbontakingselfie Pirateshiptrappedinacosmicmaelstromneb-
ofpowderedsugarsnow adornedwithvibrant inasmallboatinthecenterofalake ula renderedincosmicbeachwhirlpoolengine
candycanesandshimmeringgumdrops volumetriclighting spectacular ambientlights
lightpollution cinematicatmosphere artnou-
veaustyleillustrationartartworkbySenseiJaye
intricatedetail.
17Figure13.SamplesfromourCADmodelat512resolution
18Figure14.SamplesfromourCADmodelat512resolution
19Figure15.SamplesfromourCADmodelat512resolution
20Figure16.SamplesfromourCADmodelat512resolution
21