Vision-based Manipulation from Single Human Video
with Open-World Object Graphs
YifengZhu1,ArisreiLim1,PeterStone1,2,YukeZhu1
1TheUniversityofTexasatAustin2SonyAI
Abstract: We present an object-centric approach to empower robots to learn
vision-based manipulation skills from human videos. We investigate the prob-
lemofimitatingrobotmanipulationfromasinglehumanvideointheopen-world
setting, where a robot must learn to manipulate novel objects from one video
demonstration. Weintroduce ORION,analgorithmthattacklestheproblemby
extractinganobject-centricmanipulationplanfromasingleRGB-Dvideoandde-
rivingapolicythatconditionsontheextractedplan.Ourmethodenablestherobot
tolearnfromvideoscapturedbydailymobiledevicessuchasaniPadandgener-
alize the policies to deployment environments with varying visual backgrounds,
cameraangles,spatiallayouts,andnovelobjectinstances.Wesystematicallyeval-
uateourmethodonbothshort-horizonandlong-horizontasks,demonstratingthe
efficacy of ORION in learning from a single human video in the open world.
Videoscanbefoundintheprojectwebsite.
Keywords: RobotManipulation,ImitationFromHumanVideos
1 Introduction
Acriticalsteptowardbuildingrobotautonomyisdevelopingsensorimotorskillsforperceivingand
interacting with unstructured environments. Conventional methods for acquiring skills necessitate
manualengineeringand/orcostlydatacollection[1,2,3,4,5]. Apromisingalternativeisteaching
robotsthroughhumanvideosofmanipulationbehaviorssituatedineverydayscenarios.Thesemeth-
odshavegreatpotentialtotapintothereadilyavailablesourceofInternetvideosthatencompassa
widedistributionofhumanactivities,pavingthegroundforscalingupskilllearning.
Prior work on learning from human videos has focused on pre-training representations and value
functions[6,7,8,9,10]. However,theydonotexplicitlycaptureobjectstatesandtheirinteractions
in3Dspacewhererobotmotionsaredefined.Consequently,theyrequireseparateteleoperationdata
for each set of objects in each location and even for each possible change in visual background,
e.g.,thescenebackgroundorlightingconditions[11]. Incontrast,ourgoalisforarobottolearnto
performataskrobustlyinthe“openworld”,i.e.,undervaryingvisualandspatialconditionsfroma
singlehumanvideo,withoutpriorknowledgeoftheobjectmodelsorthebehaviorsshown.Sinceour
policy construction process uses actionless videos that are equivalent to state-only demonstrations
intheproblemof“ImitationfromObservation”[12],werefertoourproblemsettingasopen-world
imitationfromobservation.
Developing a method in this setting is only possible due to the recent advances in vision founda-
tionmodels[13,14]. Thesemodels,pre-trainedonInternet-scalevisualdata,excelatunderstand-
ingopen-vocabularyvisualconceptsandenablerobotstorecognizeandlocalizeobjectsinnatural
videoswithoutknownobjectcategoriesoraccesstophysicalstates. Thisworkmarksthefirststep
towardachievingourvisionofopen-worldimitationfromobservation, wherearobotlearnstoin-
teract with objects given a single video while generalizing to environments with different visual
backgroundsandunseenspatialconfigurationsduringdeployment. Inthiswork,weconsiderusing
RGB-Dvideodemonstrationswhereapersonmanipulatesasmallsetoftask-relevantobjectswith
their single hand, recorded with a stationary camera. These videos are actionless or state-only, as
theydonotcomewithanyground-truthactionlabelsfortherobot.
4202
yaM
03
]OR.sc[
1v12302.5042:viXraSingle Video ORION Diverse Test
Demonstration Environments
Open-world
Object Graphs
Vision Foundation Generalizable
(OOGs)
Models Policy
Figure1: Overview. WeintroduceORIONfortacklingtheproblemoflearningmanipulationbehaviorsfrom
singlehumanvideodemonstrations. ORIONfirstextractsasequenceofOpen-WorldObjectGraphs(OOGs),
where each OOG models a keyframe state with task-relevant objects and hand information. Then ORION
leveragestheOOGsequencetoconstructamanipulationpolicythatgeneralizesacrossvariedinitialconditions,
specificallyinfouraspects:visualbackground,camerashifts,spatiallayouts,andnovelinstancesfromthesame
objectcategories.
We introduce our method ORION, short for Open-woRld video ImitatiON. Figure 1 visualizes a
high-leveloverviewofORION.Thecoreinnovationliesincreatinganobject-centricspatiotemporal
abstraction that effectively bridges the observational gap between human demonstration and robot
execution. The design of ORION stems from our insight that manipulation tasks center around
object interaction, and task completion depends on whether specific intermediate states, so-called
subgoals, are reached. To capture the object-centric information in the video, we design a graph-
based,object-centricrepresentation,calledOpen-worldObjectGraphs(OOGs),tomodelthestates
oftask-relevantobjectsandtheirrelationships. AnOOGhasatwo-levelhierarchy. Thehighlevel
consistsoftheobjectnodesandahandnode,whereobjectnodesidentifyandlocalizetherelevant
objects by leveraging outputs from vision foundation models, while the hand node encodes the
interactioninformationbetweenthehandandobjects,suchaswheretograsp.Thelowlevelconsists
of point nodes, which correspond to object keypoints, and the node features detail the motions of
objectkeypointsinthe3Dspace.
ORION extracts a manipulation plan from the video as a sequence of OOGs and uses the plan to
construct a generalizable policy. Experiments indicate that ORION constructs manipulation poli-
cies that arerobust to conditions vastly different fromthe one in the video. Using only an iPhone
oraniPadrecordingofahumanperformingthetaskineverydayenvironments(e.g.,anofficeora
kitchen),theresultingpoliciessucceedinworkspaceswithdrasticallydifferentvisualbackgrounds,
camera angles, and spatial arrangements, and even generalize to manipulating unseen object in-
stancesofthesamecategories.
Insummary,ourcontributionisthree-fold: 1)Weposetheproblemoflearningvision-basedrobot
manipulation from a single human video in the open-world setting; 2) We introduce Open-world
Object Graphs (OOGs), a graph-based, object-centric representation for modeling the states and
relationsoftask-relevantobjects;and3)WepresentORION,analgorithmthatusesasinglevideo
to construct a manipulation policy, which generalizes to conditions that differ in four key ways:
visualbackgrounds,cameraperspectives,spatialconfigurations,andnewobjectinstances.
2 ProblemFormulation
Inthispaper,weconsideravision-based,tabletopmanipulationtask,formulatedasafinite-horizon
MarkovDecisionProcess(MDP)describedbyatuple , , ,H,R,µ ,where isthestatespace
⟨S A P ⟩ S
ofrawsensorydataincludingRGB-Dimagesandrobotproprioception, istheactionspaceoflow-
A
levelrobotcommands, : isthetransitiondynamics,H isthemaximaltaskhorizon,
P S ×A (cid:55)→ S
R is the sparse reward function, and µ is the initial state distributions of a task. In this work, we
consider the case where task reward functions are defined based on the contact relations between
2a small set of task-relevant objects. For example, a mug is placed on top of a coaster, or a spoon
isputinsideabowl. Arewardfunctionreturns1ifallobjectrelationsofataskaresatisfiedand0
otherwise.Theprimaryobjectiveofsolvingamanipulationtaskistofindavisuomotorpolicyπthat
maximizestheexpectedtasksuccessratefromawiderangeofinitialconfigurations,characterized
byµ,wherethestatesvaryacrossthefollowingfourdimensions: 1)changingvisualbackgrounds,
2) different camera angles, 3) different object instances from the same categories, and 4) varied
spatiallayoutsofthetask-relevantobjects.
We assume a robot does not have direct access to the ground-truth task reward or the physical
statesoftask-relevantobjects. Weconsiderasettingwhereasingleactionlessvideo[15,16]V is
providedasastate-onlydemonstration. WeassumeV tobeavideostreamofapersonmanipulating
the task-relevant objects with their single hand, captured as a sequence of RGB-D images using
a stationary camera. V is an arbitrarily long video that involves a manipulation sequence where
the contact relations among task-relevant objects and the hand change (e.g., an object is grasped
or an object is placed on top of another). To avoid the inherent ambiguities of videos due to the
distractionofirrelevantobjects, eachV isaccompaniedbyacompletelistofEnglishdescriptions
ofthetask-relevantobjects, uniquelydefiningtheobjectinstancesinV. Suchalistisrepresented
as a comma-separated list; an example is “[‘small red block’, ‘boat body’]” for the task
shown in Figure 2. In this scenario, however, the robot is not pre-programmed to have access to
ground-truthcategoriesandlocationsofthetask-relevantobjectsinV. Werefertothischallenging
settingas“open-world”[17],astherobotmustimitatefromV whilenotpre-programmedortrained
tointeractwiththeobjectsinV.Toallowarobottooperateinthis“open-world”setting,weassume
access to common sense knowledge through large models pre-trained on internet-scale data, i.e.,
foundationmodels.
Forevaluation,weadoptthefollowingprocedure. GivenasinglevideoV thataccomplishesatask
instancedrawnfromµ,theperformanceofanapproachisquantifiedbytheaveragerewardsreceived
whenevaluatingnewtaskinstancesdrawnfromthesameµ.
3 Method
In this section, we describe our method ORION (Open-woRld video ImitatiON). ORION is an
algorithm that allows a robot to mimic how to perform a manipulation task given a single human
video,V. Toeffectivelyconstructapolicyπ fromV, ORION employsalearningobjectivebased
on an object-centric prior. The goal is to create a policy π that directs the robot to move objects
along3DtrajectoriesthatmimicthedirectionalandcurvaturepatternsobservedinV,relativetothe
objects’initialandfinalpositions. Thisobjectiveisbasedontheobservationthatobjectsarelikely
toachievetargetconfigurationsbymovingalongtrajectoriessimilartothoseinV. KeytoORION
is generating a manipulation plan from V, which serves as the spatiotemporal abstraction of the
videothatguidestherobottoperformatask. Aplanisasequenceofobject-centrickeyframesthat
eachspecifiesaninitialorasubgoalstatecapturedinV. Wefirstintroduceourformulationofthe
object-centric representation of a state, Open-world Object Graph (OOG), used in ORION, and
thendescribethealgorithmthatconstructsarobotpolicygivenahumanvideo.
3.1 Open-worldObjectGraph
At the core of our approach is a graph-based, object-centric representation, Open-world Object
Graphs (OOGs). OOGs use open-world vision models that model the visual scenes with task-
relevantobjectsandthehandsuchthattheynaturallyexcludethedistractingfactorsinvisualdata
andlocalizethetask-relevantobjectsregardlessoftheirspatiallocations(seeSection3.2).
WedenoteanOOGas . Atthehighlevel,eachobjectnodecorrespondstoatask-relevantobject
G
fromtheresultofopen-worldvisionmodels.Everyobjectnodecomeswithnodefeatures,consisting
ofcolored3DpointcloudsderivedfromRGB-Dobservations.Thisnodefeatureindicatesbothwhat
and where objects are and also represents their geometry information. Additionally, to inform the
robotwheretointeractwithobjects(e.g.wheretograsp),weintroducethespecialized“handnode”,
3RGB-DVideo Plan Generation From Video
1 2
[“sm “ba oll a r te bd o b dl yo ”c ]k”, G<latexit sha1_base64="q4eI1N2+f4W77PFocpovZEQ/6+Y=">AAAB+HicbVDLSsNAFL2pr1ofjbp0M1gEVyWRoi6LLnRZwT6gDWEynbZDJ5MwMxFqyJe4caGIWz/FnX/jpM1CWw8MHM65l3vmBDFnSjvOt1VaW9/Y3CpvV3Z29/ar9sFhR0WJJLRNIh7JXoAV5UzQtmaa014sKQ4DTrvB9Cb3u49UKhaJBz2LqRfisWAjRrA2km9XByHWE4J5epv5qZP5ds2pO3OgVeIWpAYFWr79NRhGJAmp0IRjpfquE2svxVIzwmlWGSSKxphM8Zj2DRU4pMpL58EzdGqUIRpF0jyh0Vz9vZHiUKlZGJjJPKZa9nLxP6+f6NGVlzIRJ5oKsjg0SjjSEcpbQEMmKdF8ZggmkpmsiEywxESbriqmBHf5y6ukc153L+qN+0ateV3UUYZjOIEzcOESmnAHLWgDgQSe4RXerCfrxXq3PhajJavYOYI/sD5/AOzbk0Y=</latexit> 0 G<latexit sha1_base64="STmu5LFvMYnmDxzPQNM89tzP2BU=">AAAB+HicbVDLSsNAFL2pr1ofjbp0M1gEVyWRoi6LLnRZwT6gDWEynbZDJ5MwMxFqyJe4caGIWz/FnX/jpM1CWw8MHM65l3vmBDFnSjvOt1VaW9/Y3CpvV3Z29/ar9sFhR0WJJLRNIh7JXoAV5UzQtmaa014sKQ4DTrvB9Cb3u49UKhaJBz2LqRfisWAjRrA2km9XByHWE4J5epv5qZv5ds2pO3OgVeIWpAYFWr79NRhGJAmp0IRjpfquE2svxVIzwmlWGSSKxphM8Zj2DRU4pMpL58EzdGqUIRpF0jyh0Vz9vZHiUKlZGJjJPKZa9nLxP6+f6NGVlzIRJ5oKsjg0SjjSEcpbQEMmKdF8ZggmkpmsiEywxESbriqmBHf5y6ukc153L+qN+0ateV3UUYZjOIEzcOESmnAHLWgDgQSe4RXerCfrxXq3PhajJavYOYI/sD5/AO5gk0c=</latexit> 1 G<latexit sha1_base64="HwwpqZH/QX0nfc75g1jdSFHlkuY=">AAAB+HicbVDLSsNAFL2pr1ofrbp0M1gEVyURUZdFF7pwUcE+oA1hMp20QyeTMDMRasiXuHGhiFs/xZ1/46TNQlsPDBzOuZd75vgxZ0rb9rdVWlldW98ob1a2tnd2q7W9/Y6KEklom0Q8kj0fK8qZoG3NNKe9WFIc+px2/cl17ncfqVQsEg96GlM3xCPBAkawNpJXqw5CrMcE8/Qm89K7zKvV7YY9A1omTkHqUKDl1b4Gw4gkIRWacKxU37Fj7aZYakY4zSqDRNEYkwke0b6hAodUuekseIaOjTJEQSTNExrN1N8bKQ6Vmoa+mcxjqkUvF//z+okOLt2UiTjRVJD5oSDhSEcobwENmaRE86khmEhmsiIyxhITbbqqmBKcxS8vk85pwzlvnN2f1ZtXRR1lOIQjOAEHLqAJt9CCNhBI4Ble4c16sl6sd+tjPlqyip0D+APr8wcXdpNi</latexit> L
Identify keyframes using keypoint trajectories Generate OOG sequence
Text description
1 Object & Keypoint Tracking 2 Building Open-world Object Graph (OOG)
h<latexit sha1_base64="rTY8A/41fKd2STNcY2eiXTDuu2Y=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOl5qhfrrhVdw6ySrycVCBHo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ+6LqXVZrzVqlfpPHUYQTOIVz8OAK6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8f0OeM9Q==</latexit>
[“small red block”,
“boat body”] Reconstruct hand Thumb-index distance Hand node
from images & Interaction points h<latexit sha1_base64="rTY8A/41fKd2STNcY2eiXTDuu2Y=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOl5qhfrrhVdw6ySrycVCBHo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ+6LqXVZrzVqlfpPHUYQTOIVz8OAK6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8f0OeM9Q==</latexit>
o<latexit sha1_base64="ahaNgsmAq1yKTVkcYvW9FwytZSw=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvgqexKUY9FLx4r2g9ol5JNs21oNlmSrFCW/gQvHhTx6i/y5r8xbfegrQ8GHu/NMDMvTAQ31vO+UWFtfWNzq7hd2tnd2z8oHx61jEo1ZU2qhNKdkBgmuGRNy61gnUQzEoeCtcPx7cxvPzFtuJKPdpKwICZDySNOiXXSg+r7/XLFq3pz4FXi56QCORr98ldvoGgaM2mpIMZ0fS+xQUa05VSwaamXGpYQOiZD1nVUkpiZIJufOsVnThngSGlX0uK5+nsiI7Exkzh0nTGxI7PszcT/vG5qo+sg4zJJLZN0sShKBbYKz/7GA64ZtWLiCKGau1sxHRFNqHXplFwI/vLLq6R1UfUvq7X7WqV+k8dRhBM4hXPw4QrqcAcNaAKFITzDK7whgV7QO/pYtBZQPnMMf4A+fwAA6I2g</latexit> 1 o<latexit sha1_base64="05cyP/VQUxlxbyGI57HGndoS9Kc=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvgqeyWoh6LXjxWtLXQLiWbZtvQbLIkWaEs/QlePCji1V/kzX9j2u5BWx8MPN6bYWZemAhurOd9o8La+sbmVnG7tLO7t39QPjxqG5VqylpUCaU7ITFMcMlallvBOolmJA4FewzHNzP/8Ylpw5V8sJOEBTEZSh5xSqyT7lW/1i9XvKo3B14lfk4qkKPZL3/1BoqmMZOWCmJM1/cSG2REW04Fm5Z6qWEJoWMyZF1HJYmZCbL5qVN85pQBjpR2JS2eq78nMhIbM4lD1xkTOzLL3kz8z+umNroKMi6T1DJJF4uiVGCr8OxvPOCaUSsmjhCqubsV0xHRhFqXTsmF4C+/vEratap/Ua3f1SuN6zyOIpzAKZyDD5fQgFtoQgsoDOEZXuENCfSC3tHHorWA8plj+AP0+QMCbI2h</latexit> 2 o<latexit sha1_base64="ahaNgsmAq1yKTVkcYvW9FwytZSw=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvgqexKUY9FLx4r2g9ol5JNs21oNlmSrFCW/gQvHhTx6i/y5r8xbfegrQ8GHu/NMDMvTAQ31vO+UWFtfWNzq7hd2tnd2z8oHx61jEo1ZU2qhNKdkBgmuGRNy61gnUQzEoeCtcPx7cxvPzFtuJKPdpKwICZDySNOiXXSg+r7/XLFq3pz4FXi56QCORr98ldvoGgaM2mpIMZ0fS+xQUa05VSwaamXGpYQOiZD1nVUkpiZIJufOsVnThngSGlX0uK5+nsiI7Exkzh0nTGxI7PszcT/vG5qo+sg4zJJLZN0sShKBbYKz/7GA64ZtWLiCKGau1sxHRFNqHXplFwI/vLLq6R1UfUvq7X7WqV+k8dRhBM4hXPw4QrqcAcNaAKFITzDK7whgV7QO/pYtBZQPnMMf4A+fwAA6I2g</latexit> 1
o<latexit sha1_base64="05cyP/VQUxlxbyGI57HGndoS9Kc=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvgqeyWoh6LXjxWtLXQLiWbZtvQbLIkWaEs/QlePCji1V/kzX9j2u5BWx8MPN6bYWZemAhurOd9o8La+sbmVnG7tLO7t39QPjxqG5VqylpUCaU7ITFMcMlallvBOolmJA4FewzHNzP/8Ylpw5V8sJOEBTEZSh5xSqyT7lW/1i9XvKo3B14lfk4qkKPZL3/1BoqmMZOWCmJM1/cSG2REW04Fm5Z6qWEJoWMyZF1HJYmZCbL5qVN85pQBjpR2JS2eq78nMhIbM4lD1xkTOzLL3kz8z+umNroKMi6T1DJJF4uiVGCr8OxvPOCaUSsmjhCqubsV0xHRhFqXTsmF4C+/vEratap/Ua3f1SuN6zyOIpzAKZyDD5fQgFtoQgsoDOEZXuENCfSC3tHHorWA8plj+AP0+QMCbI2h</latexit> 2
Back-project to Object nodes
Object point clouds
Segment objects point clouds
Nocontact
Incontact
Keypoint trajectories Point nodes
Belonging
Track object motions Track motions in 3D Nodefeatures
via sampled keypoints through back-projection
Figure2:OverviewofplangenerationinORION.ORIONgeneratesamanipulationplanfromagivenvideo
V inorderforsubsequentpoliciestosynthesizeactions. ORIONfirsttrackstheobjectsandkeypointsacross
thevideoframes. Thenkeyframesareidentifiedbasedonthevelocitystatisticsofthekeypointtrajectories.
ThenORIONgeneratesanOpen-worldObjectGraph(OOG)foreverykeyframe,resultinginasequenceof
OOGsthatservesasthespatiotemporalabstractionofthevideo.Thefigureisviewedbestincolor.
whichstorestheinteractioncuessuchascontactpointsandthegripstatus(openorclosed)thatcan
be directly mapped to the robot end-effector during execution. At the low level, each point node
correspondstoakeypointthatbelongstoatask-relevantobject. Everypointnodecomeswiththe
feature,namelythe3Dmotiontrajectories. Thefeatureexplicitlymodelshowanobjectshouldbe
movedduringamanipulationtask. Intherestofthepaper,bymotionfeaturesofapointnodeinG ,
l
wemean3Dtrajectorybetweenkeyframelandl+1.
InanOOG,alltheobjectnodesandthehandnodearefullyconnected,reflectingreal-worldspatial
relationships. Additionally, the edges are augmented with a binary attribute, indicating whether
two objects or objects and the hand are in contact to represent their pairwise contact relations.
Thisattributeallowsourdevelopedalgorithmtocheckthesetofcontactrelationsthataresatisfied,
retrievingthematchedOOGfromthegeneratedplan(seeSection3.2). Asforthelow-levelpoint
nodes, they are connected to their respective object node, indicating a belonging relationship. In
therestofthepaper,wedenotenodeentitiesfromhumanvideoswithasuperscriptV,anddenote
theonesfromtherobotrolloutwithasuperscriptRo. Table1intheappendixalsosummarizesthe
variablesneededtodefineanOOG.
3.2 ManipulationPlanGenerationFromV
WedescribethefirstpartofORION(seeFigure2),whereourmethodautomaticallyannotatesthe
videoandgeneratesamanipulationplanfromagivenhumanvideo,V.Inthispaper,amanipulation
planisaspatiotemporalabstractionofV thatcentersaroundtheobjectstatesandtheirmotionsover
time,demonstratinghowataskshouldbecompleted.Ourcoreinsightisthatwecancost-effectively
modelataskwithobjectlocationsatsomekeyframestateswherethesetofsatisfiedcontactrelations
ischanged,andabstractamajorityofintermediatestatesinto3Dmotionsofobjects. Concretely,a
planisrepresentedasasequenceofOOGs, L whichcorrespondstoL+1keyframesinV,
{Gl }l=0
with representingtheinitialstate.
0
G
4Trackingtask-relevantobjects. ORIONfirstlocalizestask-relevantobjectsinthevideoV. Given
V and the list of object descriptions mentioned in Section 2, ORION uses an open-world vision
model,Grounded-SAM[18],toannotatevideoframeswithsegmentationmasksofthetask-relevant
objects. In practice, open-world vision models are computationally demanding, so we reduce the
computationbyexploitingobjectpermanencetotracktheobjects. Specifically, ORION annotates
thefirstvideoframewithGrounded-SAM,andthenpropagatesthesegmentationtotherestofthe
videousingaVideoObjectSegmentationModel,Cutie[19].
Discoveringkeyframes. Afterannotatingthelocationsoftask-relevantobjects,wetracktheirmo-
tionsacrossthevideotodiscoverthekeyframesbasedonthevelocitystatisticsofobjectmotions.
This design is based on the observation that changes in object contact relations due to manipu-
lation are often accompanied by sudden changes in object motions (e.g., transitioning from free
spacemotiontograspinganobject). However,keepingfulltrackofobjectpointmotionusingtech-
niqueslikeopticalflowestimationrequiresheavycomputationandthetrackingqualityissuscepti-
bletonoisyobservations,largelyduetoocclusionsduringmanipulation. WeuseaTrack-Any-Point
(TAP)model,namelyCoTracker[20],totrackasubsetofpointsinalong-termvideowithexplicit
occlusion modeling, which has been successfully applied to track object motions in robot manip-
ulation[21,22]. Specifically, wefirstsamplekeypointswithintheobjectsegmentationofthefirst
frameandtrackthetrajectoriesacrossthevideo. Thechangesinvelocitystatisticsarestraightfor-
ward to detect based on the TAP trajectories, where we discover the keyframes using a standard
unsupervisedchangepointdetectionalgorithm[23].
GeneratingOOGsfromV. OnceORIONdiscoversthekeyframes,itgeneratesanOOGateach
keyframe to model the state of task-relevant objects and the human hand in V. The creation of
OOGnodescanreusetheresultsfromtheannotationprocess: forobjectnodes,thepointcloudsfor
nodefeaturesareobtainedbyback-projectingtheobjectsegmentationwithdepthdata;forthepoint
nodes,eachnodecorrespondstothesampledkeypoints,andtheirmotionfeatures,3Dtrajectories,
are back-projected from the TAP trajectories using depth data. Additionally, hand information is
requiredtospecifytheinteractionpointswithtask-relevantobjectsandthegripstatustobemapped
totherobotgripper. Weuseahand-reconstructionmodel,HaMeR[24],whichgivesareconstructed
handmeshthatpinpointsthehandlocationsateachkeyframe. Thedistancesbetweenthefingertips
ofthemeshhelpdeterminethegripstatus,i.e.,whetheritisopenorclosed.
Withallthenodeinformation, ORION establishestheedgeconnectionsbetweennodesinOOGs,
representingcontactrelations.Sinceallobjectandhandlocationsarecomputedinthecameraframe
while the camera extrinsic of V is unknown, there is ambiguity when deciding the spatial rela-
tionsbetweenobjects. Weexploittheassumptionoftabletopmanipulation,whereatableisalways
presentwithitsnormaldirectionalignedwiththez-axisoftheworldcoordinatesystem.SoORION
estimatesthetransformationmatrixofthetableplaneandtransformsallthepointcloudfeaturesin
OOGs to align with the xy plane of the world coordinate (Full details appear in Appendix A.2).
Then, the contact relations in each state can be determined based on the spatial relations and the
computeddistancesbetweenpointclouds. Therelationsallow ORION tomatchthetest-timeob-
servationswithakeyframestatefromtheplanandsubsequentlydecidewhichobjecttomanipulate
(seeSection3.3). Intheend,ORIONgeneratesacompleteOOGforeachdiscoveredkeyframe.
3.3 RobotPolicyToSynthesizeActions
Givenamanipulationplan,ORIONconstructsapolicythatsynthesizesactions,detailedinFigure3.
Themanipulationpolicyisderivedbasedontheaforementionedlearningobjectivetoachieveobject
motionsimilarities. Theactionsynthesiscomprisesthreemajorsteps: identifyakeyframefromthe
planthatmatchesthecurrentobservation,predictobjectmotions,andusethepredictionstooptimize
therobotactionsfortherobotcontrollertoexecute. Thepolicyrepeatsthesethreestepsuntilatask
iscompletedorfails,detailedinAppendixC.
Retrieving OOGs from the plan. ORION identifies the keyframe and retrieves OOGs to help
decide what next actions to take. At test-time, ORION localizes objects in the new observations
5Object point clouds ⌧<latexit sha1_base64="77cVee9eLHd2h2lp5+BCAyMJaRE=">AAACAnicbVBNS8NAEN34WetX1ZN4CRbBU0mkqMeiF49V7Ac0tWy203bpZhN2J2IJwYt/xYsHRbz6K7z5b9y2OWjrg4HHezPMzPMjwTU6zre1sLi0vLKaW8uvb2xubRd2dus6jBWDGgtFqJo+1SC4hBpyFNCMFNDAF9Dwh5djv3EPSvNQ3uIognZA+5L3OKNopE5h30Ma3yU3YdpJPIQHTJCqPmCadgpFp+RMYM8TNyNFkqHaKXx53ZDFAUhkgmrdcp0I2wlVyJmANO/FGiLKhrQPLUMlDUC3k8kLqX1klK7dC5UpifZE/T2R0EDrUeCbzoDiQM96Y/E/rxVj77ydcBnFCJJNF/ViYWNoj/Owu1wBQzEyhDLFza02G1BFGZrU8iYEd/bleVI/KbmnpfJ1uVi5yOLIkQNySI6JS85IhVyRKqkRRh7JM3klb9aT9WK9Wx/T1gUrm9kjf2B9/gDbU5hi</latexit> Ro ORION Policy
from robot observation target
o<latexit sha1_base64="lCXOeLb1UD7gD61ltKBDD/A+w04=">AAAB+nicbVBNS8NAEN3Ur1q/Uj16CRbBU0lE1GPRi8cK9gPaEDbbSbt088HuRC0xP8WLB0W8+ku8+W/ctjlo64OBx3szzMzzE8EV2va3UVpZXVvfKG9WtrZ3dvfM6n5bxalk0GKxiGXXpwoEj6CFHAV0Ewk09AV0/PH11O/cg1Q8ju5wkoAb0mHEA84oaskzq7GX9REeMUMqh4B57pk1u27PYC0TpyA1UqDpmV/9QczSECJkgirVc+wE3YxK5ExAXumnChLKxnQIPU0jGoJys9npuXWslYEVxFJXhNZM/T2R0VCpSejrzpDiSC16U/E/r5dicOlmPEpShIjNFwWpsDC2pjlYAy6BoZhoQpnk+laLjaikDHVaFR2Cs/jyMmmf1p3z+tntWa1xVcRRJofkiJwQh1yQBrkhTdIijDyQZ/JK3own48V4Nz7mrSWjmDkgf2B8/gBr+pTE</latexit>target
o<latexit sha1_base64="7reDcwboKWiTEcNrWTVIXrlp4Gg=">AAAB9XicbVDLSgNBEJyNrxhfUY9eBoPgKexKUI9BLx4jmAcka5id9CZDZh/M9Kph2f/w4kERr/6LN//GSbIHTSxoKKq66e7yYik02va3VVhZXVvfKG6WtrZ3dvfK+wctHSWKQ5NHMlIdj2mQIoQmCpTQiRWwwJPQ9sbXU7/9AEqLKLzDSQxuwIah8AVnaKT7qJ/2EJ4wVeBnWb9csav2DHSZODmpkByNfvmrN4h4EkCIXDKtu44do5syhYJLyEq9REPM+JgNoWtoyALQbjq7OqMnRhlQP1KmQqQz9fdEygKtJ4FnOgOGI73oTcX/vG6C/qWbijBOEEI+X+QnkmJEpxHQgVDAUU4MYVwJcyvlI6YYRxNUyYTgLL68TFpnVee8WrutVepXeRxFckSOySlxyAWpkxvSIE3CiSLP5JW8WY/Wi/VufcxbC1Y+c0j+wPr8AX1nkys=</latexit> ref
Robotobservation Localize task-relevant
(RGB-D) object at test time ⌧<latexit sha1_base64="wrmz/QE0izvMO3xwt+3f5QrhiJA=">AAACAXicbVBNS8NAEN3Ur1q/ol4EL8EieCqJFPVY9OKxgv2AJpbNdtou3XywOxFLiBf/ihcPinj1X3jz37htc9DWBwOP92aYmefHgiu07W+jsLS8srpWXC9tbG5t75i7e00VJZJBg0Uikm2fKhA8hAZyFNCOJdDAF9DyR1cTv3UPUvEovMVxDF5AByHvc0ZRS13zwEWa3KXNrJu6CA+YIpUDwCzrmmW7Yk9hLRInJ2WSo941v9xexJIAQmSCKtVx7Bi9lErkTEBWchMFMWUjOoCOpiENQHnp9IPMOtZKz+pHUleI1lT9PZHSQKlx4OvOgOJQzXsT8T+vk2D/wkt5GCcIIZst6ifCwsiaxGH1uASGYqwJZZLrWy02pJIy1KGVdAjO/MuLpHlacc4q1ZtquXaZx1Ekh+SInBCHnJMauSZ10iCMPJJn8krejCfjxXg3PmatBSOf2Sd/YHz+AAoKl+0=</latexit> V
target
o<latexit sha1_base64="tk2JDTL0lqbcv4K/aYuuCXp67aA=">AAACAHicbVBNS8NAEN3Ur1q/oh48eAkWwVNJpKjHohePFewHNCFsttN26eaD3YlYQi7+FS8eFPHqz/Dmv3Hb5qCtDwYe780wMy9IBFdo299GaWV1bX2jvFnZ2t7Z3TP3D9oqTiWDFotFLLsBVSB4BC3kKKCbSKBhIKATjG+mfucBpOJxdI+TBLyQDiM+4IyilnzzyB1RzOLcz1yER8yQyiFgnvtm1a7ZM1jLxClIlRRo+uaX249ZGkKETFCleo6doJdRiZwJyCtuqiChbEyH0NM0oiEoL5s9kFunWulbg1jqitCaqb8nMhoqNQkD3RlSHKlFbyr+5/VSHFx5GY+SFCFi80WDVFgYW9M0rD6XwFBMNKFMcn2rxUZUUoY6s4oOwVl8eZm0z2vORa1+V682ros4yuSYnJAz4pBL0iC3pElahJGcPJNX8mY8GS/Gu/Exby0Zxcwh+QPj8wdgfpeR</latexit>ˆtarget o<latexit sha1_base64="7D5TiES2CVCPzzyAG6SOU3rnac8=">AAAB/XicbVDJSgNBEO1xjXEbl5uXwSB4CjMS1GPQi8cIZoHMMPR0apImPQvdNWIcBn/FiwdFvPof3vwbO8tBEx8UPN6roqpekAqu0La/jaXlldW19dJGeXNre2fX3NtvqSSTDJosEYnsBFSB4DE0kaOATiqBRoGAdjC8Hvvte5CKJ/EdjlLwItqPecgZRS355qE7oJgnhZ+7CA+YSwiLwjcrdtWewFokzoxUyAwN3/xyewnLIoiRCapU17FT9HIqkTMBRdnNFKSUDWkfuprGNALl5ZPrC+tEKz0rTKSuGK2J+nsip5FSoyjQnRHFgZr3xuJ/XjfD8NLLeZxmCDGbLgozYWFijaOwelwCQzHShDLJ9a0WG1BJGerAyjoEZ/7lRdI6qzrn1dptrVK/msVRIkfkmJwSh1yQOrkhDdIkjDySZ/JK3own48V4Nz6mrUvGbOaA/IHx+QPhqpYp</latexit>ˆref
Object point clouds
fM roa mn ip hu ul mat aio nn
v
p idla en
o
G<latexit sha1_base64="eexiU8MUrWXXvwzpzJTTxrMxHU4=">AAAB+HicbVDLSsNAFL2pr1ofjbp0M1gEVyWRoi6LLnRZwT6gDWEynbZDJ5MwMxFqyJe4caGIWz/FnX/jpM1CWw8MHM65l3vmBDFnSjvOt1VaW9/Y3CpvV3Z29/ar9sFhR0WJJLRNIh7JXoAV5UzQtmaa014sKQ4DTrvB9Cb3u49UKhaJBz2LqRfisWAjRrA2km9XByHWE4J5epv5Kc98u+bUnTnQKnELUoMCLd/+GgwjkoRUaMKxUn3XibWXYqkZ4TSrDBJFY0ymeEz7hgocUuWl8+AZOjXKEI0iaZ7QaK7+3khxqNQsDMxkHlMte7n4n9dP9OjKS5mIE00FWRwaJRzpCOUtoCGTlGg+MwQTyUxWRCZYYqJNVxVTgrv85VXSOa+7F/XGfaPWvC7qKMMxnMAZuHAJTbiDFrSBQALP8Apv1pP1Yr1bH4vRklXsHMEfWJ8/SBaTgg==</latexit> l G<latexit sha1_base64="L4ce2Qt2FZEXxh8atdrgWWp2SX0=">AAAB+nicbVDLSsNAFL2pr1pfqS7dDBZBEEoiRV0WXeiygn1AG8JkOmmHTh7MTJQS8yluXCji1i9x5984abPQ1gMDh3Pu5Z45XsyZVJb1bZRWVtfWN8qbla3tnd09s7rfkVEiCG2TiEei52FJOQtpWzHFaS8WFAcep11vcp373QcqJIvCezWNqRPgUch8RrDSkmtWBwFWY4J5epO5KT+1M9esWXVrBrRM7ILUoEDLNb8Gw4gkAQ0V4VjKvm3FykmxUIxwmlUGiaQxJhM8on1NQxxQ6aSz6Bk61soQ+ZHQL1Ropv7eSHEg5TTw9GQeVC56ufif10+Uf+mkLIwTRUMyP+QnHKkI5T2gIROUKD7VBBPBdFZExlhgonRbFV2CvfjlZdI5q9vn9cZdo9a8KuoowyEcwQnYcAFNuIUWtIHAIzzDK7wZT8aL8W58zEdLRrFzAH9gfP4AKdKT8g==</latexit> l+1 from OOGs
Retrieve the OOGs Global registration of Predict trajectories using global Optimize SE(3)
object point clouds registration transformations action sequence
Figure 3: Overview of the ORION Policy. ORION first localizes task-relevant objects at test time and
retrievesthematchedOOGfromthegeneratedmanipulationplan. ThenORIONusestheretrievedOOGsto
predicttheobjectmotionsbyfirstcomputingglobalregistrationofobjectpointcloudsandthentransforming
theobservedkeypointtrajectoriesfromvideointotheworkspace. Thepredictedtrajectoriesarethenusedto
optimizetheSE(3)actionsequenceoftherobotendeffector,whichissubsequentlyusedtocommandtherobot.
and estimates contact relations using the same vision pipeline as described in Section 3.2. Then
ORION retrieves the OOG that has the same set of relations as the current state, allowing us to
identifyapair( , ),where istheretrievedgraphand thegraphofthenextkeyframe.
l l+1 l l+1
G G G G
Thispairofgraphsprovidessufficientinformationtodecidewhichobjecttomanipulatenext,termed
thetargetobject,andwedenoteitspointcloudatkeyframelasoˆ ,anditskeypointtrajectories
target
asτV . Atargetobjectistheoneinmotionduetomanipulationbetweentwokeyframes,anditis
target
determinedbycomputingtheaveragevelocityper-objectusingmotionfeaturesinG . Atthesame
l
time,anotherobject,calledthereferenceobject,isinvolvedinchangingcontactstaterelationsfrom
to andservesasaspatialreferenceforthetargetobject’smovement. Weusethepointcloud
l l+1
G G
ofthereferenceobjectatnextkeyframel+1,asthereferenceobjectmighthavelocationchanges
due to object interactions and using the updated information from the next keyframe gives us an
accurate prediction of the trajectories. Once the target and reference objects are determined, we
canlocalizethecorrespondingobjectsinthenewobservationsandtheirpointcloudsaredenotedas
o ando ,respectively.
target ref
Predictingobjectmotions. Giventhetargetandreferenceobjectsfromkeyframesl,andl+1,we
predictthemotionofthetargetobjectinthecurrentstatebywarpingthekeypointtrajectoriesesti-
matedfromV.Towarpthetrajectories,wefirstidentifytheinitialandgoallocationsofkeypointsin
thenewconfigurationbyleveraginginformationgivenbytheOOGpair. Weuseglobalregistration
of point clouds [25] to align oˆ with o and oˆ with o , giving us two transformations to
target target ref ref
computethenewstartingandgoalpositionsoftargetobjectkeypointsconditionedonwheretheref-
erenceobjectis. ThenwenormalizeτV withitsstartingandgoallocations,obtainingτˆ . τˆ
target target target
onlycontainsthedirectionalandcurvaturepatternsthatareindependentoftheabsolutelocationof
the initial and the goal keypoints. Then we scale it back to the workspace coordinate frame using
thenewstartingandgoallocations,resultinginnewkeypointtrajectoriesofthetargetobjectτRo .
target
Optimizingrobotactions. OnceweobtainτRo ,weoptimizeforasequenceofSE(3)transforma-
target
tions that guide the robot end-effector to move. The SE(3) transformations are optimized to align
thekeypointlocationsfrompreviousframestothenextframesalongthepredictedtrajectories:
tl+(cid:88)1−tl
min (τRo (i+1) T τRo (i)) (1)
T0,T1,...,Ttl+1−tl
i=0
target − i target
whereτRo (i)(0 i t t )representsthekeypointlocationsattimestepialongthetrajec-
target ≤ ≤ l+1 − l
tory. Thisoptimizationprocessnaturallyallowsgeneralizationsoverspatialvariations,astheaction
sequencealwaysconditionsonanewlocationinsteadofoverfittingtoabsolutelocations. Tofurther
specifywherethegrippershouldinteractwiththeobjectandwhetheritshouldbeopenorclosed,
weaugmenttheresultingSE(3)sequencewiththeinteractioninformationstoredinthehandnode
6TaskNames
Mug-on- Chip- Simple- Succulents-in- Rearrange- Complex- Prepare-
coaster on-plate boat-assembly llama-vase mug-box boat-assembly breakfast
HumanVideos
WordDescriptions
‘mug’, ‘chip bag’, ‘red block’, ‘llama ‘mug’,‘coaster’, ‘red block’, ‘mug’,
‘coaster’ ‘plate’ ‘boat body’ white vase’, ‘creamcheese ‘boat chimney’, ‘squaremat’,
‘succulents’ box’ , ’plate’ ‘boat body’ ‘plate’,‘can’,
‘creamcheese box’
PolicyEvaluation
Results 93.3% Successrate Missedgrasping
66.7% 66.7% Missedtracking Unsatisfiedcontacts
60% 60% 60% 60%
Figure 4: The upper part of the figure illustrates the following items: the initial and final frames of human
videosforeverytask,thelistofworddescriptionsprovidedalongwiththevideo,andtheexampleimagesof
initialstatesforpolicyevaluation.ThelowerpartofthefigureshowstheoverallevaluationofORIONoverall
seventasks,includingthesuccessratesandthequantificationoffailedtrials,separatedbyfailuremode.
h. Weimplementacombinationofinversekinematics(IK)andjointimpedancecontroltoachieve
preciseandcompliantexecution.
TheresultingORIONpolicyisrobusttovisualvariationsduetotheuseofopen-worldvisionmod-
els.Italsogeneralizestodifferentspatiallocationsduetoourchoiceofrepresentingobjectlocations
inobject-centricframesandtheoptimizationprocessthatisnotconstrainedtospecificpositions.
4 Experiments
In this section, we report on experiments to answer the following questions regarding the effec-
tivenessof ORION andtheimportantdesignchoices. 1)Is ORION effectiveatconstructingma-
nipulation policies given a single human video in the open-world setting? 2) To what extent does
the object-centric abstraction improve the policy performance? 3) How critical is it to model the
objectmotionswithkeypointsandtheTAPformulation? 4)Howconsistentistheperformanceof
ORION’spolicygivenvideostakenindifferentconditions? 5)HoweffectivelydoesORIONscale
tolong-horizonmanipulationtasks?
4.1 ExperimentSetup
Task descriptions. We design the following five tasks to evaluate the policy performance: 1)
Mug-on-coaster: placing a mug on the coaster; 2) Simple-boat-assembly: putting
a small red block on a toy boat; 3) Chips-on-plate: placing a bag of chips on the
plate; 4) Succulents-in-llama-vase: inserting succulents into the llama vase; 5)
Rearrange-mug-box: placing a mug on a coaster and placing a cream cheese box on a plate
consecutively; 6) Complex-boat-assembly: placing both a small red block and a chimney-
like part on top of a boat. 7) Prepare-breakfast: placing a mug on a coaster and putting a
foodboxandcanontheplate. Thefirstfourare“short-horizon”tasks,andthelastthreeare“long-
horizon” tasks. In the context of this paper, “short-horizon” refers to tasks that only require one
contact relation between two objects, while “long-horizon” refers to those that require more than
onecontactrelation. DetailedsuccessconditionsofalltasksaredescribedinAppendixC.
7ORION Successrate
Unsatisfiedcontacts
Hand-Motion-Imitation
Missedtracking
66.7% 66.7% Dense-Correspondence 11/15 Missedgrasping
60% 10/15
9/15
20% 20%
0%
Mug-on-coaster Simple-boat-assembly OriginalSetting DiverseSetting1 DiverseSetting2
(a) (b)
Figure 5: (a) Experimental comparison between ORION and the two baselines, namely HAND-MOTION-
IMITATION and DENSE-CORRESPONDENCE. (b)Ablationstudyonusingdifferentvideosofthesametask.
WeselectthetaskMug-on-coasterforconductingthisablation.Wedisplaythenumberofsuccessfultrials
outof15totaltrialsonthebarplotsforeachsetting. Figure6inAppendixDvisualizesthedifferentsettings
inthisexperiment.
Experimentalsetup. Wedesignexperimentstofullytesttheefficacyofourmethodbyproviding
the robot with videos captured in everyday scenarios, which naturally encompass visual back-
grounds and camera setups that are different from the one for the robot. Specifically, we record
an RGB-D video of a person performing each of the five tasks in everyday scenarios, such as an
officeorakitchen. WeuseaniPadforrecording,whichcomeswithaTrueDepthCamera,andwe
fixitonacamerastand. Thevideoscanbefoundinthesupplementarymaterials. Duringtesttime,
the robot receives visual data through a single RGB-D camera, Intel Realsense435, and performs
manipulation in its workstation to evaluate policies. We use the 7DoF Franka Emika Panda robot
foralltheexperiments.
Evaluation protocol. As we describe in the experimental setup, the videos naturally include
various visual backgrounds and camera perspectives that are significantly different from the robot
workspace. Therefore, we only intentionally vary two dimensions before evaluating each trial of
robot execution, namely the spatial layouts and the new object instances. Furthermore, the new
object generalizations are included in the tasks Mug-on-coaster and Chips-on-plate as
mugs and chip bags have many similar instances. As for the other three tasks, there are no novel
objectsinvolved,butweextensivelyvarythespatiallayoutsoftask-relevantobjectsforevaluation.
Thepolicyperformanceofataskistheaveragedsuccessratesover15real-worldtrials. Asidefrom
the success rates, we also group the failed executions into three types: Missed tracking of objects
due to failure of the vision models, Missed grasping of objects during execution, and Unsatisifed
contactswherethetargetobjectconfigurationsarenotachievedforreasonsotherthantheprevious
twofailuretypes.
Baselines. Tounderstandthemodelcapacityandvalidateourdesignchoices,wecompareORION
withbaselines. Sincenopriorworkexiststhatmatchestheexactsettingofourapproach,weadopt
themostimportantcomponentsfrompriorworksandtreatthemasbaselinestoourmodel. Specifi-
cally,weimplementthefollowingtwobaselines:1)HAND-MOTION-IMITATION[9,26]isabaseline
that predicts robot actions by learning from the hand trajectories. The rest of the parts remain the
same as ORION. We use this baseline to show whether it is critical to compute actions centering
around objects. 2) DENSE-CORRESPONDENCE [15, 27] is a baseline that replace the TAP model
in ORION with a dense correspondence model, optical flows. This baseline is used to evaluate
whetherourchoiceofTAPmodelisabetterdesign. Forthisablativestudy,weconductexperiments
on Mug-on-coaster and Simple-boat-assembly to validate our model design, covering
thedistributionofcommondailyobjectsandassemblymanipulationthatrequiresprecisecontrol.
4.2 ExperimentalResults
OurevaluationsarepresentedinFigures41and 5.Weanswerquestion(1)byshowingthesuccessful
deployment of the ORION policies, while no other methods are designed to be able to operate in
1Individualsinimageshavebeendigitallyblurredtoensureanonymityinaccordancewiththerequirements
ofthedouble-blindreviewprocess.
8our setting. Furthermore, ORION yields an average of 69.3% success rates, which validates our
modeldesigninlearningfromasinglehumanvideointheopen-worldsetting.
We then answer question (2), showing the comparison results in Figure 5 against the baseline,
HAND-MOTION-IMITATION, which yields low success rates in both tasks. Concretely, HAND-
MOTION-IMITATION typically succeeds in trials where the initial spatial layouts are similar to the
one in V. Its major failure mode is not being able to reach the target object configuration, e.g.,
misplacingthemugonthetablewhilenotachievingcontactwiththecoaster. Theseresultsimply
that learning from human hand motion from V results in poor generalization abilities of policies,
supportingthedesignchoiceofORIONwhichfocusesontheobject-centricinformation.
Wefurtheranswerquestion(3)bycomparingtheperformancebetween ORION andthebaseline,
DENSE-CORRESPONDENCE, shown in Figure 5(a). We observe that the optical flow baseline
performs drastically worse on Simple-boat-assembly than on Mug-on-coaster. With
our further investigation, we find that the optical flow baseline discovers keyframes in the middle
ofsmoothtransitionsasopposedtochangesinobjectcontactrelations,resultinginamanipulation
plan that computes completely wrong actions. This finding further supports our choice of using
TAPkeypointstodiscoverthekeyframes.
To answer question (4), we conduct controlled experiments using the task Mug-on-coaster.
Specifically, we record two additional videos of the same task in very different visual condi-
tions and spatial layouts (see details in Appendix D) and construct a policy from each video.
Then, we compare the two policies against the original one and test them using the same set
of evaluation conditions. The results in Figure 5(b) shows that there is no statistically sig-
nificant difference in the performance, demonstrating that ORION is robust to videos taken
under very different visual conditions. Finally, we show that ORION is effective in scaling
to long-horizon tasks. This conclusion is supported by the performance among the pairs of
Mug-on-coaster versus Rearrange-mug-box, and Simple-boat-assembly versus
Complex-boat-assembly. In these two pairs, both the short-horizon tasks are subgoals of
their long-horizon counterparts, yet we do not see any performance drop between the two. This
result indicates that ORION excels at scaling to long-horizon tasks without a significant drop in
policyperformance.
5 RelatedWork
Learning Manipulation From Human Videos. Human videos offer a rich repertoire of ob-
ject interaction behaviors, making them an invaluable data source for manipulation. A large
body of work has explored how to leverage human video data for learning robot manipula-
tion[9,10,28,29,30,31,32,33],eitherthroughpre-trainingasinglelatentrepresentation[7,9,33],
learninganimplicitrewardfunction[6,8],orlearninggenerativemodelsthatin-painthumanmor-
phologies[15,26,28,34]. However,theyeitherrequireadditionalrobotdatafromthetargettasks
orpaireddatabetweenhumansandrobots. Ourapproachtakesanoveldirectionbytacklinghowa
robotcanimitateorlearnfromasinglehumanvideoonly: therobotdoesnotrelyonpre-existing
data,models,orground-truthannotationsinsceneswherevideorecordingandrobotevaluationtake
place. Werefertosuchasettingasopen-worldimitationfromobservation, wheretherobotisnot
programmedortrainedtointeractwiththeobjectsinthevideoaprioriandthevideodatadoesnot
come with any robot actions. Our setting is closely related to the problem of “Imitation Learning
fromObservation”[12],wherestate-onlydemonstrationsareusedtoconstructpoliciesforphysical
interaction. However, this line of prior work assumes simulators of demonstrated tasks exist and
physicalstatesoftheagentsorobjectsareknown[35,36,37,38,39]. Incontrast,oursettingdoes
not assume the digital replica of real-world tasks, and all the object information is only perceived
throughRGB-Dvideos.
LearningManipulationFromaSingleDemonstration. Studieshavedelvedintolearningmanip-
ulationpoliciesfromonedemonstration.Anotableframeisone-shotimitationlearningwithinmeta-
learningframeworkproposedbyDuanetal.[40]. Whilepriorworksonone-shotimitationlearning
9haveshownarobotperformingnewtasksfromonedemonstration,theyrequireextensivein-domain
dataandawell-curatedsetofmeta-trainingtasksbeforehand,leadingtosignificantdatacollection
costsandrestrictedpolicygeneralizationattesttimeduetothetailorednatureofthetraining.
Analternativeapproachinvolvesusingasingledemonstrationforinitialguidance,refiningthepol-
icy through real-world self-play [41, 42, 43, 44, 45]. However, this approach mainly applies to
reset-freetasksandstruggleswithscalingtomulti-stagetaskswhereresettingtothetaskinitialcon-
ditionsdoesnotcomefree. Ourworkalignswiththesestudiesinusingasingledemonstrationfor
learningmanipulation,butstandsoutbynotneedingpriordataorself-play. Withjustonesinglehu-
manvideo,ourmethodconstructsapolicythatsuccessfullycompletesthetask,adaptingtovarious
visualandspatialdifferencesfromthetaskinstanceofvideodemonstration.
Object-Centric Representation for Learning Robot Manipulation. The concept of object-
centricrepresentationhaslongbeenrecognizedforitspotentialtoenhanceroboticperceptionand
manipulation by focusing on the objects within a scene. Prior works have shown effectiveness
of such representation in downstream manipulation tasks by factorizing visual scenes into disen-
tangled object concepts [46, 47, 48, 49, 50], but these works are typically confined to known ob-
jectcategoriesorinstances. Recentdevelopmentsinfoundationmodelsallowrobotstoaccessthe
open-world object concepts through pre-trained vision models [13, 14], enabling a wide range of
abilitiessuchasimitationoflong-horizontabletopmanipulation[5,51]ormobilemanipulationin
the wild [52]. Building upon these advances, our work focuses on leveraging open-world, object-
centricconceptsinimitatingmanipulationbehaviorsfromactionlesshumanvideos. Weproposea
graph-based representation called Open-world Object Graph (OOG), which allows a robot to im-
itate from a human video by leveraging the object-centric concepts. This proposed representation
sharesasimilarveinwithpriorworksthatfactorizesceneortask-relevantvisualconceptsintoscene
graphs[29,53,54,55,56]. However, ourrepresentationistailoredtointegrateopen-worldobject
conceptsandenablegeneralizationacrossdifferentembodiments,specificallyahumanandarobot.
6 Conclusions
Inthispaper,weinvestigatetheproblemoflearningrobotmanipulationfromasinglehumanvideo
in the open-world setting, where a robot must learn to manipulate novel objects from one video
demonstration. Totacklethisproblem,weintroduce ORION,analgorithmbuiltonobject-centric
priors. Ourresultsshowthatgivenasinglehumanvideo,ORIONisabletoconstructapolicythat
generalizesoverthefollowingfourdimensions: visualbackgrounds,cameraangles,spatiallayouts,
andthepresenceofnewobjectinstances.
Limitations: We consider the task goals to be described by contact states so that we naturally
avoid the ambiguities introduced when considering spatial relations, such as placing items next to
anobject.Howtoinferhumanintentionswhileclearingtheinherentambiguitiesinvideosisafuture
directiontoexplore.
Wehavealsoassumedtwoconstraintsonhowvideosarecaptured:thecameraneedstobestationary
andincludeRGB-Ddata. Inreality,mostvideosineverydayscenariosaretakenwhilecamerasare
moving ande in RGB. Thus, a promising future direction is to investigate how to build a model
thatcanreconstructthedynamicscenesfromamovingRGBcamera,wherethedesiredmodelcan
estimatethegeometryofbothstaticandmovingobjectsinthesceneswhilemakingsurethescale
ofreconstructedscenesandobjectsmatchestherealworld.
Furthermore,ORIONestablishesthecorrespondencebetweenobjectsfromdemonstrationandroll-
outusingglobalregistrationofthepointclouds. Suchcorrespondencereliessolelyonthegeometry
ofobjects,whichmaysufferfromambiguitieswhentheobjectshapesaresymmetricandthecorre-
spondencereliesonthetextureinformation. Afuturedirectionistoincorporatebothsemanticand
geometricinformationoftheobjectstoestablishobjectcorrespondence.
10Acknowledgments
We would like to thank Rutav Shah, Jiayuan Mao, and Fangchen Liu for the helpful discussions.
ThisworkhastakenplaceintheRobotPerceptionandLearningGroup(RPL)andLearningAgents
ResearchGroup(LARG)atUTAustin. RPLresearchhasbeenpartiallysupportedbytheNational
Science Foundation (FRR2145283, EFRI-2318065) and the Office of Naval Research (N00014-
22-1-2204). LARG research is supported in part by NSF (FAIN-2019844, NRT-2125858), ONR
(N00014-18-2243), ARO (E2061621), Bosch, Lockheed Martin, and UT Austin’s Good Systems
grandchallenge. PeterStoneservesastheExecutiveDirectorofSonyAIAmericaandreceivesfi-
nancialcompensationforthiswork.Thetermsofthisarrangementhavebeenreviewedandapproved
bytheUniversityofTexasatAustininaccordancewithitspolicyonobjectivityinresearch.
References
[1] M. Dalal, D. Pathak, and R. R. Salakhutdinov. Accelerating robotic reinforcement learning
viaparameterizedactionprimitives. AdvancesinNeuralInformationProcessingSystems,34:
21847–21859,2021.
[2] A. Mandlekar, D. Xu, R. Mart´ın-Mart´ın, S. Savarese, and L. Fei-Fei. Learning to general-
izeacrosslong-horizontasksfromhumandemonstrations. arXivpreprintarXiv:2003.06085,
2020.
[3] S.Nasiriany,H.Liu,andY.Zhu. Augmentingreinforcementlearningwithbehaviorprimitives
fordiversemanipulationtasks. In2022InternationalConferenceonRoboticsandAutomation
(ICRA),pages7477–7484.IEEE,2022.
[4] R. Zhang, S. Lee, M. Hwang, A. Hiranaka, C. Wang, W. Ai, J. J. R. Tan, S. Gupta, Y. Hao,
G.Levine,etal. Noir: Neuralsignaloperatedintelligentrobotsforeverydayactivities. arXiv
preprintarXiv:2311.01454,2023.
[5] Y.Zhu,A.Joshi,P.Stone,andY.Zhu. Viola:Imitationlearningforvision-basedmanipulation
withobjectproposalpriors. arXivpreprintarXiv:2210.11339,2022.
[6] A. S.Chen, S. Nair, and C.Finn. Learninggeneralizable roboticreward functionsfrom” in-
the-wild”humanvideos. arXivpreprintarXiv:2103.16817,2021.
[7] S.Nair,A.Rajeswaran,V.Kumar,C.Finn,andA.Gupta. R3m: Auniversalvisualrepresen-
tationforrobotmanipulation. arXivpreprintarXiv:2203.12601,2022.
[8] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang. Vip: Towards
universal visual reward and representation via value-implicit pre-training. arXiv preprint
arXiv:2210.00030,2022.
[9] C.Wang,L.Fan,J.Sun,R.Zhang,L.Fei-Fei,D.Xu,Y.Zhu,andA.Anandkumar.Mimicplay:
Long-horizonimitationlearningbywatchinghumanplay. arXivpreprintarXiv:2302.12422,
2023.
[10] H.Xiong, Q.Li, Y.-C.Chen, H.Bharadhwaj, S.Sinha, andA.Garg. Learningbywatching:
Physicalimitationofmanipulationskillsfromhumanvideos. In2021IEEE/RSJInternational
ConferenceonIntelligentRobotsandSystems(IROS),pages7827–7834.IEEE,2021.
[11] Y. Zhu, Z. Jiang, P. Stone, and Y. Zhu. Learning generalizable manipulation policies with
object-centric3drepresentations. In7thAnnualConferenceonRobotLearning,2023.
[12] F. Torabi. Imitation Learning from Observation. PhD thesis, University of Texas at Austin,
2021. PhDThesis.
[13] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead,
A.C.Berg,W.-Y.Lo,etal. Segmentanything. arXivpreprintarXiv:2304.02643,2023.
11[14] M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khalidov,P.Fernandez,D.Haz-
iza,F.Massa,A.El-Nouby,etal.Dinov2:Learningrobustvisualfeatureswithoutsupervision.
arXivpreprintarXiv:2304.07193,2023.
[15] P.-C. Ko, J. Mao, Y. Du, S.-H. Sun, and J. B. Tenenbaum. Learning to act from actionless
videosthroughdensecorrespondences. arXivpreprintarXiv:2310.08576,2023.
[16] C.Wen,X.Lin,J.So,K.Chen,Q.Dou,Y.Gao,andP.Abbeel. Any-pointtrajectorymodeling
forpolicylearning. arXivpreprintarXiv:2401.00025,2023.
[17] K.Joseph,S.Khan,F.S.Khan,andV.N.Balasubramanian. Towardsopenworldobjectdetec-
tion. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pages5830–5840,2021.
[18] S.Liu,Z.Zeng,T.Ren,F.Li,H.Zhang,J.Yang,C.Li,J.Yang,H.Su,J.Zhu,etal.Grounding
dino: Marryingdinowithgroundedpre-trainingforopen-setobjectdetection. arXivpreprint
arXiv:2303.05499,2023.
[19] H.K.Cheng,S.W.Oh,B.Price,J.-Y.Lee,andA.Schwing. Puttingtheobjectbackintovideo
objectsegmentation. arXivpreprintarXiv:2310.12982,2023.
[20] N.Karaev,I.Rocco,B.Graham,N.Neverova,A.Vedaldi,andC.Rupprecht. Cotracker: Itis
bettertotracktogether. arXivpreprintarXiv:2307.07635,2023.
[21] M.Vecerik,C.Doersch,Y.Yang,T.Davchev,Y.Aytar,G.Zhou,R.Hadsell,L.Agapito,and
J. Scholz. Robotap: Tracking arbitrary points for few-shot visual imitation. arXiv preprint
arXiv:2308.15975,2023.
[22] B.Wen, W.Lian, K.Bekris, andS.Schaal. Youonlydemonstrateonce: Category-levelma-
nipulationfromsinglevisualdemonstration. arXivpreprintarXiv:2201.12716,2022.
[23] R. Killick, P. Fearnhead, and I. A. Eckley. Optimal detection of changepoints with a linear
computational cost. Journal of the American Statistical Association, 107(500):1590–1598,
2012.
[24] G.Pavlakos,D.Shan,I.Radosavovic,A.Kanazawa,D.Fouhey,andJ.Malik. Reconstructing
handsin3dwithtransformers. arXivpreprintarXiv:2312.05251,2023.
[25] S.Choi,Q.-Y.Zhou,andV.Koltun. Robustreconstructionofindoorscenes. InProceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition,pages5556–5565,2015.
[26] H. Bharadhwaj, A. Gupta, S. Tulsiani, and V. Kumar. Zero-shot robot manipulation from
passivehumanvideos. arXivpreprintarXiv:2302.02011,2023.
[27] N.Heppert,M.Argus,T.Welschehold,T.Brox,andA.Valada.Ditto:Demonstrationimitation
bytrajectorytransformation. arXivpreprintarXiv:2403.15203,2024.
[28] S. Bahl, A. Gupta, and D. Pathak. Human-to-robot imitation in the wild. arXiv preprint
arXiv:2207.09450,2022.
[29] S.Kumar,J.Zamora,N.Hansen,R.Jangir,andX.Wang.Graphinversereinforcementlearning
fromdiversevideos. InConferenceonRobotLearning,pages55–66.PMLR,2023.
[30] Y. Liu, A. Gupta, P. Abbeel, and S. Levine. Imitation from observation: Learning to imitate
behaviorsfromrawvideoviacontexttranslation. In2018IEEEInternationalConferenceon
RoboticsandAutomation(ICRA),pages1118–1125.IEEE,2018.
[31] P. Sharma, D. Pathak, and A. Gupta. Third-person visual imitation learning via decoupled
hierarchicalcontroller. AdvancesinNeuralInformationProcessingSystems,32,2019.
12[32] L.Smith,N.Dhawan,M.Zhang,P.Abbeel,andS.Levine. Avid: Learningmulti-stagetasks
viapixel-leveltranslationofhumanvideos. arXivpreprintarXiv:1912.04443,2019.
[33] M.Xu,Z.Xu,C.Chi,M.Veloso,andS.Song. Xskill: Crossembodimentskilldiscovery. In
ConferenceonRobotLearning,pages3536–3555.PMLR,2023.
[34] H.Bharadhwaj,A.Gupta,V.Kumar,andS.Tulsiani. Towardsgeneralizablezero-shotmanip-
ulationviatranslatinghumaninteractionplans. arXivpreprintarXiv:2312.00775,2023.
[35] B.S.Pavse,F.Torabi,J.Hanna,G.Warnell,andP.Stone. Ridm:Reinforcedinversedynamics
modelingforlearningfromasingleobserveddemonstration. IEEERoboticsandAutomation
Letters,5(4):6262–6269,2020.
[36] H. Karnan, F. Torabi, G. Warnell, and P. Stone. Adversarial imitation learning from video
usingastateobserver. In2022InternationalConferenceonRoboticsandAutomation(ICRA),
pages2452–2458.IEEE,2022.
[37] F.Torabi,G.Warnell,andP.Stone.Imitationlearningfromvideobyleveragingproprioception.
In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pages
3585–3591,2019.
[38] F. Torabi, G. Warnell, and P. Stone. Generative adversarial imitation from observation. In
Imitation,Intent,andInteraction(I3)WorkshopatICML2019,June2019.
[39] F.Torabi, G.Warnell, andP.Stone. Behavioralcloningfromobservation. InProceedingsof
the27thInternationalJointConferenceonArtificialIntelligence,pages4950–4957,2018.
[40] Y.Duan,M.Andrychowicz,B.Stadie,O.JonathanHo,J.Schneider,I.Sutskever,P.Abbeel,
and W. Zaremba. One-shot imitation learning. Advances in neural information processing
systems,30,2017.
[41] N.DiPaloandE.Johns. Learningmulti-stagetaskswithonedemonstrationviaself-replay. In
ConferenceonRobotLearning,pages1180–1189.PMLR,2022.
[42] S.Haldar,V.Mathur,D.Yarats,andL.Pinto. Watchandmatch: Superchargingimitationwith
regularizedoptimaltransport. InConferenceonRobotLearning,pages32–43.PMLR,2023.
[43] S. Haldar, J. Pari, A. Rai, and L. Pinto. Teach a robot to fish: Versatile imitation from one
minuteofdemonstrations. arXivpreprintarXiv:2303.01497,2023.
[44] E.Johns. Coarse-to-fineimitationlearning: Robotmanipulationfromasingledemonstration.
In2021IEEEinternationalconferenceonroboticsandautomation(ICRA),pages4613–4619.
IEEE,2021.
[45] E. Valassakis, G. Papagiannis, N. Di Palo, and E. Johns. Demonstrate once, imitate imme-
diately (dome): Learning visual servoing for one-shot imitation learning. In 2022 IEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems(IROS),pages8614–8621.IEEE,
2022.
[46] J.Tremblay,T.To,B.Sundaralingam,Y.Xiang,D.Fox,andS.Birchfield.Deepobjectposees-
timationforsemanticroboticgraspingofhouseholdobjects.arXivpreprintarXiv:1809.10790,
2018.
[47] S.Tyree,J.Tremblay,T.To,J.Cheng,T.Mosier,J.Smith,andS.Birchfield. 6-dofposeesti-
mationofhouseholdobjectsforroboticmanipulation: Anaccessibledatasetandbenchmark.
arXivpreprintarXiv:2203.05701,2022.
[48] T.MigimatsuandJ.Bohg. Object-centrictaskandmotionplanningindynamicenvironments.
IEEERoboticsandAutomationLetters,5(2):844–851,2020.
13[49] D. Wang, C. Devin, Q.-Z. Cai, F. Yu, and T. Darrell. Deep object-centric policies for au-
tonomous driving. In 2019 International Conference on Robotics and Automation (ICRA),
pages8853–8859.IEEE,2019.
[50] C.Devin,P.Abbeel,T.Darrell,andS.Levine. Deepobject-centricrepresentationsforgener-
alizablerobotlearning. In2018IEEEInternationalConferenceonRoboticsandAutomation
(ICRA),pages7111–7118.IEEE,2018.
[51] J.Shi,J.Qian,Y.J.Ma,andD.Jayaraman. Plug-and-playobject-centricrepresentationsfrom
“what”and“where”foundationmodels.
[52] A.Stone,T.Xiao,Y.Lu,K.Gopalakrishnan,K.-H.Lee,Q.Vuong,P.Wohlhart,B.Zitkovich,
F.Xia,C.Finn,etal. Open-worldobjectmanipulationusingpre-trainedvision-languagemod-
els. arXivpreprintarXiv:2303.00905,2023.
[53] K.Mo,P.Guerrero,L.Yi,H.Su,P.Wonka,N.Mitra,andL.J.Guibas. Structurenet: Hierar-
chicalgraphnetworksfor3dshapegeneration. arXivpreprintarXiv:1908.00575,2019.
[54] Y. Huang, A. Conkey, and T. Hermans. Planning for multi-object manipulation with graph
neuralnetworkrelationalclassifiers. In2023IEEEInternationalConferenceonRoboticsand
Automation(ICRA),pages1822–1829.IEEE,2023.
[55] A.H.Qureshi,A.Mousavian,C.Paxton,M.C.Yip,andD.Fox. Nerp: Neuralrearrangement
planningforunknownobjects. arXivpreprintarXiv:2106.01352,2021.
[56] Y.Zhu,J.Tremblay,S.Birchfield,andY.Zhu. Hierarchicalplanningforlong-horizonmanip-
ulationwithgeometricandsymbolicscenegraphs. In2021IEEEInternationalConferenceon
RoboticsandAutomation(ICRA),pages6541–6548.IEEE,2021.
[57] C.Truong,L.Oudre,andN.Vayatis. Selectivereviewofofflinechangepointdetectionmeth-
ods. SignalProcessing,167:107299,2020.
[58] Q.-Y.Zhou,J.Park,andV.Koltun. Open3d: Amodernlibraryfor3ddataprocessing. arXiv
preprintarXiv:1801.09847,2018.
[59] R.B.Rusu,N.Blodow,andM.Beetz. Fastpointfeaturehistograms(fpfh)for3dregistration.
In2009IEEEinternationalconferenceonroboticsandautomation,pages3212–3217.IEEE,
2009.
[60] Z.TeedandJ.Deng.Tangentspacebackpropagationfor3dtransformationgroups.InProceed-
ingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition, pages10338–
10347,2021.
14A AdditionalTechnicalDetails
A.1 DataStructureofanOOG.
Foreasyreproducibilityoftheproposedmethod,wepresentatablethatexplainsthedatastructure
ofanOOG.
Node/Edge Type Attributes
.vo ObjectNode 3Dpointcloudofanobject.
i
G
Handmeshandlocationsofthethumbandindex
.vh HandNode
G finger.
A trajectory of a TAP keypoint between two
.vp PointNode
G ij keyframes,recordedinxyzpositions.
.eo Object-ObjectEdge Abinaryvalueofcontactornot.
ik
G
.eh Object-HandEdge Abinaryvalueofcontactornot.
i
G
Thepresenceofanedgerepresentsthebelonging
.ep Object-PointEdge
G ij relation,andnospecificfeatureisattached.
Table1:DataStructureofanOOG.ForagivenOOGG =(V,E),ithasV ={G.vo }∪{G.vh}∪{G.vp },
i ij
andE ={G.eo }∪{G.eh }∪{G.ep }.
ik i ij
A.2 ImplementationDetails
Changepointdetections. Weusechangepointdetectiontoidentifychangesinvelocitystatisticsof
TAPkeypoints. Specifically,weuseakernel-basedchangepointdetectionmethodandchooseradial
basis function [23]. The implementation of this function is directly based on an existing library
Ruptures[57].
Plane estimation. In Section 3.2, we mentioned using the prior knowledge of tabletop manipula-
tion scenarios and transforming the point clouds by estimating the table plane. Here, we explain
how the plane estimation is computed. Concretely, we rely on the plane estimation function from
Open3D [58], which gives an equation in the form of ax + by + cz = d. From this estimated
plane equation, we can infer a normal vector of the estimated table plane, (a,b,c), in the camera
coordinateframe. Then,wealignthisplanewithxyplaneintheworldcoordinateframe,wherewe
computeatransformationmatrixthatdisplacesthenormalvector(a,b,c)tothenormalizedvector
(0,0,1)alongthez-axisoftheworldcoordinateframe. Thistransformationmatrixisusedtotrans-
formpointcloudsineveryframesothattheplaneofthetablealwaysalignswiththexyplaneofthe
worldcoordinate.
Objectlocalizationattesttime. Whenwelocalizeobjectsattesttime,therecouldbesomefalse
positivesegmentationofdistractingobjects. Suchvisionfailureswillpreventtherobotpolicyfrom
successfullyexecutingactions. Toexcludesuchfalsepositiveobjectsegmentaiton,weuseSegmen-
tation Correspondence Model (SCM) from GROOT [11], where SCM filters out the false positive
segmentationoftheobjectsbycomputingtheaffinityscoresbetweenmasksusingDINOv2features.
Globalregistration.Inthispaper,weuseglobalregistrationtocomputethetransformationbetween
observed object point clouds from videos and those from rollout settings. We implement this part
using a RANSAC-based registration function from Open3D [58]. Specifically, given two object
pointclouds,wefirstcomputetheirfeaturesusingFast-PointFeatureHistograms(FPFH)[59],and
thenperformaglobalRANSACregistrationontheFPFHfeaturesofthepointclouds[25].
Implementation of SE(3) optimization. We parameterize each homogeneous matrix T into a
i
translation variable and a rotation variable and randomly initialize each variable using the normal
distribution. Wechoosequaternionsastherepresentationforrotationvariables, andwenormalize
therandomlyinitializedvectorsforrotationsothattheyremainunitquaternions. Withsuchparam-
eterization,weoptimizetheSE(3)end-effectortrajectoriesT ,T ,...,T overtheObjective
0 1 tl+1−tl
15(1).However,jointlyoptimizingbothtranslationandrotationfromscratchtypicallyresultsintrivial
solutions,wheretherotationvariablesdonotchangemuchfromtheinitializationduetothevanish-
ing gradients. To avoid trivial solutions, we implement a two-stage process. In the first stage, we
onlyoptimizetherotationvariableswith200gradientsteps. Then,theoptimizationproceedstothe
secondstage,whereweoptimizeboththerotationandtranslationvariablesforanother200gradient
steps. In this case, we prevent the optimization process from getting stuck in trivial solutions for
rotationvariables. WeimplementtheoptimizationprocessusingLietorch[60].
B SystemSetup
Details of camera observations. As mentioned in Section 4, we use an iPad with a TrueDepth
cameraforcollectinghumanvideodemonstrations. WeuseaniOSapp, Record3D,thatallowsus
to access the depth images from the TrueDepth camera. We record RGB and depth image frames
in sizes 1920 1080 and 640 480, respectively. To align the RGB images with the depth data,
× ×
we resize the RGB frames to the size 640 480. The app also automatically records the camera
×
intrinsicsoftheiPhonecamerasothattheback-projectionofpointcloudsismadepossible.
Tostreamimagesattesttime, weuseanIntelRealsenseD435i. Inourrobotexperiments, weuse
RGBanddepthimagesinthesize640 480or1280 720invariedscenarios,allcoveredinour
× ×
evaluations.Evaluatingondifferentimagesizesshowcasesthatourmethodisnottailoredtospecific
cameraconfigurations,supportingthewideapplicabilityofconstructedpolicy.
Implementation of real robot control. In our evaluation, we reset the robot to a default joint
positionbeforeobjectinteractioneverytime.Thenweuseareachingprimitivefortherobottoreach
theinteractionpoints. Resettingtothedefaultjointpositionenablesanunoccludedobservationof
task-relevant objects at the start of each decision-making step. Note that the execution of object
interaction does not necessarily require resetting. To command the robot to interact with objects,
weconverttheoptimizedSE(3)actionsequencetoasequenceofjointconfigurationsusinginverse
kinematics and control the robot using joint impedance control. We use the implementation of
Deoxys[5]forthejointimpedancecontrollerthatoperatesat500Hz. Toavoidabruptmotionand
makesuretheactionsaresmooth,wefurtherinterpolatethejointsequencefromtheresultofinverse
kinematics.Specifically,wechoosetheinterpolationsothatthemaximaldisplacementforeachjoint
doesnotexceed0.5radianbetweentwoadjacentwaypoints.
C Successconditionsoftasks
Wedescribethesuccessconditionsforeachofthetasksindetail:
• Mug-on-coaster: Amugisplaceduprightonthecoaster.
• Simple-boat-assembly: Aredblockisplacedintheslotclosesttothebackofthe
boat. Theblockneedstobeuprightintheslot.
• Chips-on-plate: Abagofchipsisplacedontheplate,andthebagdoesnottouchthe
table.
• Succulents-in-llama-vase:Apotofsucculentsisinsertedintoawhitevaseinthe
shapeofallama.
• Rearrange-mug-box: Themugisplaceduprightonthecoaster,andthecreamcheese
boxisplacedontheplate.
• Complex-boat-assembly: The chimney-like part is placed in the slot closest to the
frontoftheboat. Theredblockisplacedintheslotclosesttothebackoftheboat. Both
blocksneedtobeuprightintheslots.
• Prepare-breakfast: Themugisplacedontopofacoaster,thecreamcheeseboxis
placedinthelargeareaoftheplate,andthefoodcanisplacedonthesmallareaasshown
inthevideodemonstration.
16In practice, we record the success and failure of a rollout as follows: If the program in ORION
policy returns true when matching the observed state with the final OOG from a plan, we mark a
trialassuccessaslongasweobservethattheobjectstateindeedsatisfiesthesuccessconditionof
a task as described above. Otherwise, if the robot generates dangerous actions (bumping into the
table)ordoesnotachievethedesiredsubgoalafterexecutingthecomputedtrajectory,weconsider
therolloutasafailureandwemanuallyrecordthefailure.
D AdditionalDetailsonExperiments
Diversevideorecordingsusedintheablationstudy.Figure6showsthethreevideostakeninvery
differentscenarios: kitchen,office,andoutdoor. Thevideotakeninkitchenscenarioisusedinthe
majorquantitativeevaluation,termed“Originalsetting”. Theothertwosettingsaretermed“Diverse
setting1”and“Diversesetting2.”Weconductanablationstudywherewecomparepoliciesimitated
from these three videos, which inherently involve varied visual scenes, camera perspectives. The
resultoftheablationstudyisshowninFigure5.
Kitchen Office Outdoor
(Original setting) (Diverse setting 1) (Diverse setting 2)
Figure 6: This figure visualizes the initial and final frames of the three videos of the same
taskMug-on-coaster.
17