Near Optimal Decentralized Optimization with
Compression and Momentum Tracking
Rustem Islamov∗ Yuan Gao∗ Sebastian U. Stich
University of Basel CISPA† CISPA†
rustem.islamov@unibas.ch yuan.gao@cispa.de stich@cispa.de
Abstract
Communication efficiency has garnered significant attention as it is considered the main
bottleneck for large-scale decentralized Machine Learning applications in distributed and federated
settings. Inthisregime,clientsarerestrictedtotransmittingsmallamountsofquantizedinformation
to their neighbors over a communication graph. Numerous endeavors have been made to address
thischallengingproblembydevelopingalgorithmswithcompressedcommunicationfordecentralized
non-convex optimization problems. Despite considerable efforts, the current results suffer from
various issues such as non-scalability with the number of clients, requirements for large batches,
or bounded gradient assumption. In this paper, we introduce MoTEF, a novel approach that
integrates communication compression with Momentum Tracking and Error Feedback. Our
analysis demonstrates that MoTEF achieves most of the desired properties, and significantly
outperformsexistingmethodsunderarbitrarydataheterogeneity. Weprovidenumericalexperiments
to validate our theoretical findings and confirm the practical superiority of MoTEF.
1 Introduction
Decentralized machine learning approaches are increasingly popular in numerous applications such as
the internet-of-things (IoT) and networked autonomous systems [Marvasti et al., 2014, Savazzi et al.,
2020], primarily due to their scalability to larger datasets and systems, as well as their respect for data
locality and privacy concerns. In this work, we focus on decentralized optimization techniques that
operatewithoutacentralcoordinator,relyingsolelyonon-devicecomputationandlocalcommunication
withneighboringdevices. ThisencompassestraditionalscenariosliketrainingMachineLearningmodels
in large data centers, as well as emerging applications where computations occur directly on devices.
Such a setting is preferred over centralized topology which often poses a significant bottleneck on the
central node in terms of communication latency, bandwidth, and fault tolerance.
Considering the enormous size of modern Machine Learning models, classic single-node training is
often impossible. Moreover, the training of large models requires a huge amount of data that does not
fit the memory of a single machine. Therefore, modern training techniques heavily rely on distributed
computations over a set of computation nodes/clients [Shoeybi et al., 2019, Wang et al., 2020, Ramesh
et al., 2021, 2022]. One of the instances of distributed training is Federated Learning (FL) [Konecny`
et al., 2016, Kairouz et al., 2021] which has recently gathered a lot of attention. In this setting, clients,
suchashospitalsorownersofedgedevices,collaborativelytrainamodelontheirdeviceswhileretaining
their data locally.
∗Equalcontribution.
†CISPAHelmholtzCenterforInformationSecurity
1
4202
yaM
03
]GL.sc[
1v41102.5042:viXraOne of the key issues in distributed optimization is the communication bottleneck [Seide et al.,
2014, Ström, 2015] that limits the scaling properties of distributed deep learning training [Seide
et al., 2014, Alistarh et al., 2017, Stich, 2018, Stich et al., 2021]. One of the remedies to decrease
communication expenses involves communication compression, where only quantized messages (with
fewer bits) are exchanged between clients using compression operators. When used appropriately,
contractive compressors (see Definition 1), such as Top-K, are often empirically preferable. However,
the naive application of contractive compression operators might lead to divergence [Beznosikov et al.,
2023]. To make compression suitable for distributed training, the Error Feedback (EF) mechanism
[Seide et al., 2014, Stich et al., 2018] is widely used in practice. It plays a crucial role in achieving high
compression ratios.
However,mostoftheworksanalyzedEFmechanisminthecentralizedsetting[StichandKarimireddy,
2019, Gorbunov et al., 2020, Stich, 2020]. Recent research achievements [Gao et al., 2024a, Fatkhullin
et al., 2024] demonstrate that in this regime properly constructed EF mechanism can handle both client
drift [Mishchenko et al., 2019, Karimireddy et al., 2020b] and stochastic noise from the gradients, and
can achieve near-optimal convergence rates. In the more challenging decentralized setting, a series of
studies [Zhao et al., 2022, Yan et al., 2023] introduced algorithms capable of effectively managing the
drift but fail to achieve a linear acceleration in parallel training, i.e. increasing the number of devices
used for training does not lead to a decrease in the training time. Yau and Wai [2022] partially solved
this issue under stronger assumptions, achieving linear speed-up using variance reduction, but with
worse dependency on the variance of the noise
Designing a method that addresses client drift while preserving linear acceleration in decentralized
training has been challenging due to the complex interplay between client drift, Error Feedback
mechanism and the communication topology. In our study, we introduce MoTEF, a novel method that
tackles these challenges concurrently. Our primary contributions can be outlined as follows.
• We propose a novel method MoTEF that incorporates momentum tracking with compression and
Error Feedback, and provably works under standard assumptions (i) without imposing any data
heterogeneity bounds, (ii) without any impractical assumptions such as large batches, (iii) with
arbitrary contractive compressor, and (iv) achieves linear speed-up with the number of clients n. We
provide convergence guarantees for the general class of non-convex functions, and for the structured
class of non-convex functions satisfying the Polyak-Łojasiewicz (PŁ) condition.
• We propose MoTEF-VR, a momentum-based STORM-type [Cutkosky and Orabona, 2019] variance-
reduced variant of our base method that improves further the asymptotic rate of convergence.
• Finally, we provide an extensive numerical study of MoTEF demonstrating the superiority of the
proposed method in practice and supporting theoretical claims.
1.1 Related works
Decentralized optimization and gradient tracking. First works in the field studied gossip
averaging procedures that are typically used to reach consensus [Kempe et al., 2003, Xiao and Boyd,
2004]. Nevertheless, direct use of gossip averaging might be sub-optimal as it often results in slow
convergence [Nedic and Ozdaglar, 2009, Koloskova et al., 2020b]. Gradient tracking [Qu and Li, 2017,
Nedic et al., 2017, Koloskova et al., 2021] is one the most popular remedies to this issue. It has been
widely applied to obtain faster decentralized algorithms [Sun et al., 2020, Xin et al., 2022, 2021, Li
et al., 2022, Zhao et al., 2022, Liu et al., 2024]. In this work, we follow a similar approach but perform
a tracking step on momentum term instead of gradients.
Momentum in distributed training. Lately, the utilization of momentum [Polyak, 1964] has
attracted attention in distributed optimization. Several works empirically showed that momentum can
2improve performance in distributed setting [Wang et al., 2019a, Karimireddy et al., 2020a, Das et al.,
2022]. Besides, it has recently been shown that the use of momentum improves convergence guarantees
[Yau and Wai, 2022, Fatkhullin et al., 2024, Cheng et al., 2024, Huang et al., 2024, Gao et al., 2024b]
fully removing dependencies on data heterogeneity bounds. In this work, we follow this approach and
apply the momentum technique to the more challenging decentralized setting.
Short history of Error Feedback. Initially, the Error Feedback mechanism was introduced as a
heuristic [Seide et al., 2014] and was subsequently analyzed within a simple single-node framework
[Stich et al., 2018, Karimireddy et al., 2019]. The first findings in the distributed context were
achieved under strong assumptions such as IID data distributions [Karimireddy etal., 2019] or bounded
gradients [Cordonnier, 2018, Alistarh et al., 2018, Koloskova et al., 2019, 2020a]. EF21 [Richtárik
et al., 2021] was proven to operate with any contractive compressors and under arbitrary heterogeneity,
albeit failing to converge when clients are limited to using only stochastic gradients [Fatkhullin et al.,
2024]. Subsequently, EF21 was extended to diverse practical scenarios [Fatkhullin et al., 2021] and
decentralized training [Zhao et al., 2022] improving the dependencies on some problem parameters.
Recent advancements [Gao et al., 2024a, Fatkhullin et al., 2024] have demonstrated that a carefully
designed EF mechanism (through the control of feedback signal strength or the use of momentum)
results in nearly optimal convergence guarantees in a centralized setting.
IssuesofErrorFeedbackindecentralizedsetting. Despitehavingbeenstudiedinthecentralized
setting extensively, EF-based algorithms in the decentralized regime still fail to achieve desirable
properties.
• Strong assumptions. Many earlier theoretical results for EF require strong assumptions, such
as either the bounded gradient assumption [Koloskova et al., 2019, 2020a] or global heterogeneity
bound [Lian et al., 2017, Tang et al., 2019, Lu and De Sa, 2021, Singh et al., 2021].
• Mega batches. Convergence of the BEER algorithm [Zhao et al., 2022] requires large batches that
can be costly or even infeasible in some applications. For example, in medical applications [Rieke
et al., 2020] or Reinforcement Learning [Khodadadian et al., 2022, Jin et al., 2022, Mitra et al., 2023]
sampling large batches is often intractable. Moreover, it has been shown that training with small
batch sizes improves generalization and convergence [Wilson and Martinez, 2003, Keskar et al., 2016,
Sekhari et al., 2021].
• Suboptimal rates. The stochastic term of several algorithms does not improve with n the number
of clients [Zhao et al., 2022, Yan et al., 2023], while the opposite is often desirable, and can be
achieved in the centralized training setting [Fatkhullin et al., 2024, Gao et al., 2024a]. Other work
achievesspeed-upwithn, butrequiresstrongersmoothnessassumptionsandhasaworsedependency
on the noise variance [Yau and Wai, 2022]. Moreover, [Koloskova et al., 2019, 2020a] do not achieve
the standard O(1/ε2) convergence rate in the noiseless regime.
• Necessity of unbiased compression. Finally, early works analyzed decentralized algorithms only
for a more restricted class of unbiased compressors [Tang et al., 2018a, Kovalev et al., 2021]. Huang
and Pu [2023] modify any contractive compressor using an additional unbiased compressor following
the results of [Horváth and Richtárik, 2020]. This approach enables the creation of a better sequence
of gradient estimators, albeit with twice the per-iteration communication cost.
In Table 1, we provide a summary of known theoretical results in decentralized training with
compression. We highlight the main issues of existing algorithms.
3Table 1: Summary of convergence guarantees for decentralized methods supporting contractive compressors.
nCVX = supports non-convex functions; PŁ = supports functions satisfying PŁ condition. We present the
convergence in terms of E(cid:2) ∥∇f(x )∥2(cid:3) ≤ε2 and E[f(x )−f⋆]≤ε in PŁ regimes for specifically chosen
out out
x . Here F0 :=E(cid:2) f(x0)−f∗(cid:3) , L and ℓ are smoothness constants, ρ is a spectral gap, and σ2 is stochastic
out
variance bound.
Asymptotic Complexity Extra
Method Large Batches?
nCVX PŁ Assumptions?
[KolosC koh vo aco e- tSG alD .,2019] LF nε0 4σ2 ✗ ✗ EB (cid:2) ∥o ∇un fd i(e xd ,G ξ)r ∥a 2d (cid:3)ie ≤nt Gs 2
Batchsizeof
BEER LF0σ2 LF0 ✗
[Zhaoetal.,2022] α2ρ3ε4 µ2α2ρ3ε order σ2
αε2
CEDAS LF0σ2 ✗ ✗ AdditionalUnbiased
[HuangandPu,2023] nε4 Compressor
[TaD ne ge ep tSq au l.e ,e 2z 0e
19]
LF nε0 4σ2 ✗ ✗ n−1(cid:80)Bo iu ∥n ∇d fe id (xH )e −te ∇ro fg (e xn )e ∥it 2y
≤ζ2
DoCoM ℓF0σ3 ℓF0σ3 ✗ ✗
[YauandWai,2022] nε3 µ2nε
CDProxSGT LF0σ2 ✗ ✗ ✗
[Yanetal.,2023] α2ρ2ε4
MoTEF LF0σ2 LF0σ2 ✗ ✗
[This work] nε4 µ2nε
MoTEF-VR ℓF0σ2 ✗ ✗ ✗
[This work] nε3
2 Problem setup
Formally, we consider the following optimization problem
(cid:40) n (cid:41)
1 (cid:88)
min f(x):= f (x) , (1)
x∈Rd n i
i=1
where n is the number of clients participating in the training, x are the parameters of a model, f(x)
is the global objective, and f (x) := E [f (x,ξ )] is the local objective over local dataset D .
i ξi∼Di i i i
Throughout this work, we assume that the global function f is bounded below by f⋆ >−∞.
Inthesettingofdecentralizedcommunication, theclientsarerestrictedtocommunicatingwiththeir
neighbors only over a certain undirected communication graph G([n],E). Each vertex in [n] represents
a client, and each edge in E represents a communication link between clients. Besides, we assign a
positiveweighttow ifthereisanedge (i,j)∈E, andw =0 if(i,j)∈/ E.Weightsw formamixing
ij ij ij
matrix W∈Rn×n (sometimes also called gossip or interaction matrix). The mixing matrix W should
satisfy the following standard assumption.
Assumption 1. We assume that W is symmetric (W=W⊤) and doubly stochastic (W1=1,1⊤W=
1⊤) matrix with eigenvalues 1=|λ (W)|>|λ (W)|≥···≥|λ (W)|. We denote the spectral gap of
1 2 n
W as
ρ:=1−|λ (W)|∈(0,1]. (2)
2
The spectral gap is typically used to measure the influence of network topology in the training
[Aldous and Fill, 2002, Nedić et al., 2018].
In our work, we consider algorithms combined with compressed communication. Formally, we
analyze methods utilizing practically useful contractive compression operators.
4Definition 1. We say that a (possibly randomized) mapping C: Rd →Rd is a contractive compression
operator if for some constant 0<α≤1 it holds
E(cid:2) ∥C(x)−x∥2(cid:3) ≤(1−α)∥x∥2. (3)
One of the classic examples of compressors satisfying (3) is Top-K [Stich et al., 2018]. It acts on the
input by preserving K largest by magnitude entries while zeroing the rest. The class of contractive
compressorsincludeswell-knownsparsification[Alistarhetal.,2018,Stichetal.,2018]andquantization
[Wen et al., 2017, Bernstein et al., 2018, Horváth et al., 2022] operators. We refer to [Beznosikov et al.,
2023, Safaryan et al., 2022, Islamov et al., 2023] for more examples of contractive compressors.
Indecentralizedtraining, typically,eachclientreceivesthemessagesfromitsneighborsandtransfers
back to them the aggregated information. We highlight that, contrary to many prior works, our
analysis supports an arbitrarily heterogeneous setting, i.e. it does not require any assumptions on the
heterogeneity level, which means that local data distributions might be distant from each other. Next,
we provide standard assumptions on the function class and noise model.
Assumption 2. We assume that each local function f is L-smooth, i.e. for all x,y∈Rd, and i∈[n]
i
it holds
∥∇f (x)−∇f (y)∥≤L∥x−y∥. (4)
i i
Next, we assume that each client has access to an unbiased gradient estimator with bounded
variance.
Assumption 3. We assume that we have access to a gradient oracle gi(x): Rd →Rd for each local
function f such that for all x∈Rd and i∈[n] it holds
i
E(cid:2) gi(x)(cid:3) =∇f (x), E(cid:2) ∥gi(x)−∇f (x)∥2(cid:3) ≤σ2. (5)
i i
It is important to mention that mini-batches are allowed as well, effectively reducing the variance
by the local batch size. Nevertheless, there is no requirement for any specific (minimal) batch size, and
for simplicity, we consistently assume a batch size of one.
Finally, we consider the structural class of non-convex functions satisfying Polyak-Łojasiewicz
condition[Polyak,1963]. ThisassumptionisoneoftheweakestconditionsunderwhichvanillaGradient
Descent converges linearly [Karimi et al., 2016].
Assumption 4. We assume that the global function f is µ-PŁ for some µ>0, i.e. for all x∈Rd it
holds
∥∇f(x)∥2 ≥2µ(f(x)−f⋆). (6)
NotethatthePŁconditionisarelaxationofstrongconvexity,i.e. ifstrongconvexitywithparameter
µ implies µ-PŁ condition.
3 Theoretical analysis
In this section, we list the main results of our work. Our base method MoTEF is summarized
in Algorithm 1, and our variance-reduced variant is summarized in Algorithm 2. We defer the proofs of
all theoretical claims to the appendix; see Appendices A and B.
5Algorithm 1 MoTEF Algorithm 2 MoTEF-VR
1: Input: X0 =x01⊤,G0,H0,V0,γ,η,λ, 1: Input: X0 =x01⊤,G0,H0,V0,γ,η,λ,
2: C α 2: C α
3: for t=0,1,2,... do 3: for t=0,1,2,... do
4: Xt+1 =Xt+γHt(W−I)−ηVt 4: Xt+1 =Xt+γHt(W−I)−ηVt
5: Q ht+1 =C α(Xt+1−Ht) 5: Qt h+1 =C α(Xt+1−Ht)
6: Ht+1 =Ht+Qt+1 6: Ht+1 =Ht+Qt+1
h h
7: Mt+1 =(1−λ)Mt+λ∇(cid:101)F(Xt+1) 7: Mt+1 =∇(cid:101)F(Xt+1,Ξt+1)
8: Vt+1 =Vt+γGt(W−I)+Mt+1−Mt 8: +(1−λ)(Mt−∇(cid:101)F(Xt,Ξt+1))
9: Q gt+1 =C α(Vt+1−Gt) 9: Vt+1 =Vt+γGt(W−I)+Mt+1−Mt
10: Gt+1 =Gt+Qt g+1 10: Qt g+1 =C α(Vt+1−Gt)
11: Gt+1 =Gt+Qt+1
g
3.1 Notation
Before going into details, we introduce a notation that we use throughout the paper. We stack the local
parameters xt stored at each clients into a matrix Xt :=[xt,...,xt]∈Rn×d, and denote the average
i 1 n
model x¯t := 1Xt1, where 1 is a vector of ones. Other quantities are defined similarly. To track local
n
gradients, we define ∇F(Xt) := [∇f 1(xt 1),...,∇f n(xt n)] ∈ Rd×n. Similarly we write ∇(cid:101)F(Xt) as the
collection of local stochastic gradients. Finally, C (X) denotes the contractive compression operator C
α α
applied column-wise on a matrix X, i.e. C (X):=[C(x ),...,C(x )]∈Rd×n.
α 1 n
3.2 Convergence of MoTEF
Now we are ready to present convergence guarantees for MoTEF. Below we summarize the convergence
guarantees for Algorithm 1 in general non-convex and PŁ settings. Our analysis relies on the Lyapunov
function of the form
c c τ c L c τ c L c τ
Φt :=Ft+ 1 Gˆt+ 2 G(cid:101)t+ 3 Ωt + 4 Ωt + 5 Ωt + 6 Ωt, (7)
n2L nL ρ3nτ 1 ρnL 2 ρ3nτ 3 ρnL 4
where{c }6 areabsoluteconstantsdefinedin(34)1,Ft :=E[f(x¯t)−f⋆]representsthesub-optimality
k k=1
function gap, and the error terms are defined as follows
Gˆt :=E(cid:2) ∥∇F(Xt)1−Mt1∥2(cid:3) , G(cid:101)t :=E(cid:104)(cid:13) (cid:13)∇F(Xt)−Mt(cid:13) (cid:13)2(cid:105) , Ωt :=E(cid:104)(cid:13) (cid:13)Ht−Xt(cid:13) (cid:13)2(cid:105)
F F 1 F
Ωt :=E(cid:104)(cid:13) (cid:13)Gt−Vt(cid:13) (cid:13)2(cid:105) , Ωt :=E(cid:104)(cid:13) (cid:13)Xt−x¯t1T(cid:13) (cid:13)2(cid:105) (8)
2 F 3 F
Ωt :=E(cid:104)(cid:13) (cid:13)Vt−v¯t1T(cid:13) (cid:13)2(cid:105) , Ωt :=E(cid:104)(cid:13) (cid:13)v¯t(cid:13) (cid:13)2(cid:105) .
4 F 5
Our theory relies on the descent of the Lyapunov function Φt introduced above.
Lemma 1 (DescentoftheLyapunovfunction). LetAssumptions1and3hold. Thenthereexistabsolute
constants c ,c ,c , and τ ≤1 such that if we set stepsizes γ =c αρ,λ=c αρ3τ,η =c L−1αρ3τ such
γ λ η γ λ η
1TofindasuitablechoiceofconstantsweuseSymbolicMathToolboxinMATLAB[Inc.,2023]. Ourcodecanbe
foundathttps://github.com/mlolab/MoTEF.git.
6that the Lyapunov function Φt decreases as
Φt+1 ≤ Φt− c ηαρ3τ E(cid:2) ∥∇f(x¯t)∥2(cid:3) + c2 λc 1α2ρ6 ·τ2σ2
2L nL
(cid:18) 6c2c αρ5 2c2c α2ρ6 6c c2αρ4(cid:19)
+ λ 4 + λ 2 + 6 λ τ3σ2. (9)
L L c L
γ
Using the above descent of the Lyapunov function, we demonstrate the convergence guarantees for
MoTEF.
Theorem 1 (ConvergenceofMoTEF). Let Assumptions 1 to 3 hold. Then there exist absolute constants
c ,c ,c , and some τ ≤ 1 such that if we set stepsizes γ = c αρ,λ = c αρ3τ,η = c L−1αρ3τ, and
γ λ η γ λ η
choosing the initial batch size B ≥⌈LF0⌉, then after at most
init σ2
(cid:18) σ2 σ σ σ 1 (cid:19)
T =O + + + + LF0 (10)
nε4 αρ2ε3 α1/2ρ3/2ε3 αρ5/2ε3 αρ3ε2
iterations of Algorithm 1 it holds E(cid:2) ∥∇f(x )∥2(cid:3) ≤ ε2, where x is chosen uniformly at random
out out
from {x¯ ,...,x¯ }, and O suppresses absolute constants.
0 T−1
Remark 2. Note that using a large initial batch size B is not required for convergence of MoTEF. If
int
we set B =1, the above theorem still holds by replacing F0 by Φ0.
init
We observe that the use of momentum in MoTEF allows us to improve convergence guarantees over
BEER. Indeed, Algorithm 1 achieves optimal asymptotic complexity2 with a desirable linear speed-up
with the number of clients n. Moreover, MoTEF provably converges for any batch size in contrast to
BEER. To the best of our knowledge, MoTEF is the first decentralized algorithm supporting contractive
compressors that achieves optimal asymptotic complexity under Assumptions 2 and 3 without data
heterogeneity restrictions.
Now we derive convergence guarantees of MoTEF for the class of functions satisfying Assumption 4.
Theorem 2 (ConvergenceofMoTEF). Let Assumptions 1 to 4 hold. Then there exist absolute constants
c ,c ,c , and some τ ≤ 1 such that if we set stepsizes γ = c αρ,λ = c αρ3τ,η = c L−1αρ3τ, and
γ λ η γ λ η
choosing the initial batch size B ≥⌈LF0⌉, then after at most
init σ2
(cid:18) Lσ2 Lσ Lσ Lσ L (cid:19)
T =O(cid:101) + + + + (11)
µ2nε αρ2µ3/2ε1/2 αρ5/2µ3/2ε1/2 αρ2µ3/2ε1/2 µαρ3
iterations of Algorithm 1 it holds E(cid:2) f(xT)−f∗(cid:3) ≤ε, and O(cid:101) suppresses absolute constants and poly-
logarithmic factors.
Remark 3. Note that using a large initial batch size B is not required for convergence of MoTEF. If
int
we set B =1, the above theorem still holds by replacing F0 by Φ0, which is hidden in the logarithmic
init
terms.
Contrary to BEER, we demonstrate that the asymptotic rate of MoTEF in the PŁ setting improves
with n and does not require large batches. To the best of our knowledge, MoTEF is the first decen-
tralized algorithm that supports contractive compressors and achieves optimal asymptotic complexity
under Assumptions 2 to 4. Moreover, we highlight that in the noiseless regime MoTEF converges
linearly as expected. Another momentum-based algorithm DoCom was analyzed under more restricted
Assumption 5 only. Therefore, its applicability in this setting is not known. Moreover, DoCoM achieves
linear speed-up with n, but with sub-optimal dependency on the noise variance σ2.
2Thismeanstheregimewhenε→0.
73.3 Convergence of MoTEF-VR
Though MoTEF achieves optimal asymptotic complexity under Assumptions 2 and 3, we consider
strengthening of Assumption 2, the mean-squared-smoothness assumption, under which further acceler-
ation on the stochastic term might be achieved via variance reduction. In this section, we introduce
MoTEF-VR, our variance-reduced algorithm for decentralized optimization with compression. First, we
introduce the mean-squared-smoothness assumption.
Assumption 5. We assume that each local function f is ℓ-mean-squared-smooth, i.e. for all x,y∈
i
Rd,i∈[n], it holds
E (cid:2) ∥∇f (x,ξ)−∇f (y,ξ)∥2(cid:3) ≤ℓ2∥x−y∥. (12)
ξ i i
Note that this also implicitly assumes that the clients can take the same randomness for different
points x and y. Assumption 5 is the standard assumption made for variance reduction techniques, and
is the key assumption for circumventing existing lower bounds on stochastic methods [Fang et al., 2018,
Cutkosky and Orabona, 2019, Tran-Dinh et al., 2022, Wang et al., 2019b, Xu and Xu, 2022].
In MoTEF-VR, instead of a simple momentum term, each client now maintains a momentum-based
variancereductionterm,similartotheSTORMestimator[CutkoskyandOrabona,2019]. Thealgorithm
also maintains a momentum parameter λ, and it turns out that the additional variance reduction terms
and Assumption 5 allow us to set the momentum parameter more aggressively, leading to an improved
convergence rate. For the analysis of MoTEF-VR, we introduce the following Lyapunov function of the
form
d d d ℓ d d ℓ d
Ψt :=Ft+ 1 Gˆt+ 2G(cid:101)t+ 3 Ωt + 4 Ωt + 5 Ωt + 6 Ωt, (13)
αρ3nτℓ nℓ ρ3nτ 1 ρnℓ 2 ρ3nτ 3 ρnℓ 4
where {d }6 are absolute constants defined in (49), and other error terms are defined similarly as
k k=1
before. Again, we present the descent lemma on the Lyapunov function Ψt.
Lemma 4 (Descent of the Lyapunov function). Let Assumptions 1, 3 and 5 hold. Then there exists
absolute constants c ,c ,c and τ < 1 such that if we set stepsizes γ = c αρ,λ = c n−1α2ρ6τ2,η =
γ λ η γ λ
c ℓ−1αρ3τ then the Lyapunov function Ψt decreases as
η
Ψt+1 ≤ Ψt− c ηαρ3 τE(cid:2) ∥∇f(x¯t)∥2(cid:3) + 2c 1c2 λα3ρ9τ3σ2
2ℓ n2ℓ
(cid:18) 2c c2α4ρ12 12c c2α3ρ11 6c c2α3ρ10(cid:19)
+ 2 λ + 4 λ + 6 λ τ4σ2. (14)
n2ℓ n3ℓ n3ℓ
Remark 5. Compared to Lemma 1, in Lemma 4, the leading stochastic term has a cubic dependence
on τ, whereas in Lemma 1 the dependence is quadratic. The improved dependence on τ is the key
ingredient to the speed-up for variance reduction type methods.
Theorem 3 (Convergence of MoTEF-VR). Let Assumptions 1, 3 and 5 hold. Then there exists absolute
constantsc ,c ,c andsomeτ <1suchthatifwestepsizesγ =c αρ,λ=c n−1α2ρ6τ2,η =c ℓ−1αρ3τ,
γ λ η γ λ η
and initial batch size B ≥⌈ σ2 ⌉, then after at most
init LF0αρ3
(cid:18) σ σ2/3 σ2/3 σ2/3 1 (cid:19)
T =O + + + + ℓF0 (15)
nε3 n2/3ε8/3 nα1/3ρ1/3ε8/3 nα1/3ρ2/3ε8/3 αρ3ε2
(cid:104) (cid:105)
iterations of Algorithm 2 it holds E ∥∇f(x )∥2 ≤ ε2, where x is chosen uniformly at random
out out
from {x¯ ,··· ,x¯ }, and O suppresses absolute constants and poly-logarithmic factors.
0 T−1
82 nodes ¸=1
4 nodes ¸=0:5
10−2 8 nodes 10−2 ¸=0:05
16 nodes ¸=0:005
10−3 10−3
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Iterations Iterations
(a) (b)
Figure 1: Performance of MoTEF (a) with different number of clients n; (b) varying momentum parameter
λ. In all cases, we set d=20,ζ =10,σ =10, and apply Top-K compressor with α=K/d=0.1. We fix the
parameters γ =0.1,η=0.0005,λ=0.005, and n=16, if the opposite is not stated.
³=20 ³=60 ³=100
100 100 100
10−1 10−1 10−1
MoTEF MoTEF MoTEF
BEER BEER BEER
10−2 CHOCO-SGD 10−2 CHOCO-SGD 10−2 CHOCO-SGD
100 102 103 104 105 100 102 103 104 105 100 102 103 104 105
Iterations Iterations Iterations
Figure 2: Performance of MoTEF, BEER and CHOCO-SGD with varying data heterogeneity ζ and fixed noise
level σ=5. We set d=20,n=4 and apply Top-K compressor with α=K/d=0.1. We set the target error to
be 0.01.
Remark 6. Note that using a large initial batch size B is not required for convergence of MoTEF-VR.
init
If we set B =1, the above theorem still holds replacing F0 by Ψ0.
init
Compared to MoTEF, MoTEF-VR achieves an improved asymptotic rate. Moreover, all stochastic
terms (the ones with σ) have a speed-up with n in contrast to the convergence of DoCoM, where only
asymptotic term improves with n.
WepointoutthatMoTEF-VRappliestheSTORMmechanismlocallytoachievethevariancereduction
effect. STORMisspecificallydesignedfornon-convexoptimizationproblems,anditsconvergenceratein
themorestructuredclassoffunctionssatisfyingAssumption4isstillunclearintheliterature[Cutkosky
and Orabona, 2019, Xu and Xu, 2022] even for the simplest centralized SGD setting. In this work, we
also do not consider the rate of MoTEF-VR under the PŁ condition.
4 Numerical experiments
Inthissection, wecomplementthetheoreticalresultsontheconvergenceofAlgorithm1withnumerical
evaluations. We run our experiments on AMD EPYC 9554 64-Core Processor.
4.1 Synthetic least squares problem
We first consider a simple synthetic least squares problem to demonstrate some of the important
theoretical properties of Algorithm 1. This problem is designed by Koloskova et al. [2020b] and studied
in [Gao et al., 2024a]. For each client i, f i(x) := 1 2∥A ix−b i∥2, where A2
i
:= i2/n·I
d
and each b
i
9
2 k⋆x
¡tx
k
2
⋆x
x t
k
¡
k
2 k⋆x
¡tx
k
2
⋆x
x t
k
¡
k
2 k⋆x
¡tx
k80 100 100
100 80
60 BEER BEER
M C Dh So o GT cE DoF -SGD 10¡1 46 00 M C Dh So o GT cE DoF -SGD 10¡1
40 D2
20
D2 10¡2
20
0 2 4 6
10¡2
0 1 2 3 0 5 10 15 20
10¡3
0 2 4 6 8
Communicated bits, 107 Communicated bits, 108 Communicated bits, 107 Communicated bits, 108
£ £ £ £
(a) a9a (b) a9a (c) w8a (d) w8a
Figure 3: Comparison of MoTEF, BEER, Choco-SGD, DSGD, D2 in terms of communication complexity on
logistic regression with non-convex regularization on ring topology with batch size 5 and gsgd compressor.
b
is sampled from N(0,ζ2/i2I d) for some parameter ζ which controls the gradient dissimilarity of the
problem [Koloskova et al., 2020b]. It is easy to see that when ζ =0,∇f (x⋆)=0,∀i. We add Gaussian
i
noise to the gradients to control the stochastic level σ2 of the gradient. We use the ring network
topologyforthesyntheticexperiment. Thecodestoreproduceoursyntheticexperimentcanbeaccessed
here.
Increasing the number of nodes. In Figure 1-(a) we study the effect of increasing the number of
nodes on the convergence of Algorithm 1. A crucial property of Algorithm 1 is that its convergence
rate provably improves linearly with the number of nodes, which BEER does not possess. Here we fix a
small stepsize and investigate the error that Algorithm 1 achieves with an increasing number of nodes.
We observe that the error decreases linearly with the number of nodes, which is consistent with the
theoretical results.
Effect of the momentum parameter. In Figure 1-(b) we investigate the effect of the momentum
parameter λ. In particular, how it affects the convergence in the noisy regime. Our theoretical analysis
suggests that the momentum parameter λ∝η is crucial for the convergence of MoTEF. We observe
that the error increases as the momentum parameter increases. Note that when λ = 1, we recover
BEER which is known to not converge with the presence of noise in the local gradients, which our
experiment confirms.
Effect of changing heterogeneity. In Figure 2 we investigate the effect of changing data hetero-
geneity ζ on the performance of MoTEF, BEER, and Choco-SGD. The hyperparameters were tuned; the
detailed description is given in Appendix C.1. We observe that MoTEF outperforms other algorithms
andisnotaffectedbythechangingζ. BEERisalsonotaffectedbythechangingζ, whileCHOCO-SGD’s
performance degrades as ζ increases. This is consistent with the theoretical results.
4.2 Non-convex logistic regression
Following [Khirirat et al., 2023, Makarenko et al., 2023, Islamov et al., 2024] we compare algorithms on
logistic regression problem with non-convex regularization3
min
1 (cid:88)n
f
(x)+λ(cid:88)d x2
j , f (x):=
1 (cid:88)m
log(1+exp(−b a⊤x)), (16)
x∈Rd n
i=1
i
j=1
1+x2
j
i m
j=1
ij ij
where {b ,a }m is a local dataset. We set λ=0.05,n=100 and use LibSVM datasets [Chang and
ij ij j=1
Lin, 2011]. We do not shuffle datasets to have a more heterogeneous setting. Besides, each dataset is
equally distributed among all clients. In all experiments on logistic regression, we use gsgd compressor
b
[Alistarh et al., 2017] with b=5. More details of this experiments are given in Appendix C.
3Ourimplementationisbasedonopen-sourcecodefrom[Zhaoetal.,2022]https://github.com/liboyue/beer. Our
codecanbeaccessedathttps://github.com/mlolab/MoTEF.git.
10
ycaruccA
tseT
mroN
tneidarG
ycaruccA
tseT
mroN
tneidarG68 00 101 ¡0 10 R S G
E ER
Rti rn a i
,
,dg r
p p= =0 0: :2
5
1 680 000 101 ¡0 10 R S G
E ER
Rti rn a i
,
,dg r
p p= =0 0: :2
5
40
R Stin ag
r
10¡2
40 R Stin ag
r
10¡2
G ERri ,d p=0:2 10¡3 20 G ERri ,d p=0:2 10¡3
20
0 5 10
15ER, p=0:5
20
10¡3
0 2 4 6 8 10 0 2 4 6
E 8R, p=0:5
10 0 1 2 3 4
Communicated bits, 106 Communicated bits, 107 Communicated bits, 106 Communicated bits, 108
£ £ £ £
(a) a9a (b) a9a (c) w8a (d) w8a
Figure 5: Performance of MoTEF changing of network topology tested on logistic regression with non-convex
regularization. We set n=40,λ=0.05, and batch size 100.
Comparison against other methods. We 101
compare BEER [Zhao et al., 2022], Choco-SGD 80
[Koloskova et al., 2019], DSGD [Alistarh et al., 60
100
2017], and D2 [Tang et al., 2018b] algorithms 40 C Dh So Gc Do-SGD
D2
w tioit nh isM go ivT eE nF inon Apri pn eg ndto ixpo Cl .o 3g .y F. oD re et aa ci hle ad lgd oe rs itc hri mp-
,
20
0 Commu0: n5 icated bit1 s,
B ME oE 1TR E 0F
10 1:5
10¡1
0 Com0: m5 unic1 ated 1 b:5 its, 2 1010 2:5
£ £
we fine-tune all stepsizes to achieve better con- (a) MLP (b) MLP
vergence. According to the results in Figure 3,
we observe that MoTEF outperforms other algo- Figure 4: Comparison of MoTEF, BEER, Choco-SGD,
rithms in terms of communication complexity in DSGD, D2 in terms of communication complexity on
both cases, when the convergence is measured by training MLP with 1 hidden layer.
training gradient norm and test accuracy.
Robustness to communication topology. Next, we study the effect of the network topology on
the convergence of MoTEF. We run experiments for ring, star, grid, Erdös-Rènyi (p=0.2 and p=0.5)
topologies. Note the spectral gaps of these networks 0.012,0.049,0.063,0.467,0.755 correspondingly.
The hyperparameters of algorithms are given in Appendix C.2. Despite the convergence of Algorithm 1
is affected by the spectral gap ρ, in Figure 5 we demonstrate that convergence MoTEF is not affected
much by the change in spectral gap. These results demonstrate the robustness of MoTEF to the change
of network topology.
Training of MLP. Finally, we consider training MLP with 1 hidden layer of size 32. We present
the results in Figure 4. We observe that MLP trained with MoTEF and BEER achieve similar gradient
norm, but MoTEF is much faster in accuracy metric showing the advantage from using momentum
tracking.
5 Conclusion, limitations, and future work
In this work, we proposed new efficient algorithms, MoTEF and its variance-reduced version MoTEF-VR,
for decentralized training with compressed communication that incorporates momentum tracking and
Error Feedback. We provide theoretical convergence of our algorithms in general non-convex regimes.
Besides, we extend the convergence of MoTEF to the class of functions satisfying the PŁ condition. In
the non-convex regime, we achieve an optimal asymptotic complexity without imposing any assumption
on batch sizes, bounded gradients or data heterogeneity. We support our theoretical findings with an
extensive experimental study.
We believe that it might be possible to improve the dependency on the spectral gap via a more
careful choice of the Lyapunov function. However, this might require computer assistance with the
search of the parameters. In our study, we focus only on compressed communication while there are
manyapproachessuchasperformingseverallocalsteps[Stich,2018,Mishchenkoetal.,2022b,Gorbunov
11
ycaruccA
tseT
mroN
tneidarG
ycaruccA
tseT
ycaruccA
tseT
mroN
tneidarG
mroN
tneidarGet al., 2021, Jiang et al., 2024] or asynchronous communication [Islamov et al., 2024, Mishchenko
et al., 2022a, Ghadikolaei et al., 2021] that might be useful. We also note that some recent works
attempt to improve the dependencies on the smoothness parameters for variants of Error Feedback
algorithms [Richtárik et al., 2024], where each local objective is assumed to be L -smooth, and a more
i
careful analysis of the method gives a dependency on the average-smoothness L¯ =n−1(cid:80)n L instead
i=1 i
of the maximum smoothness L = max L . Therefore, combining the aforementioned research
i∈[n] i
directions with our proof techniques might lead to more improved results. We defer the exploration of
these possible extensions to future research endeavors.
References
David Aldous and James Allen Fill. Reversible markov chains and random walks on graphs. Unfin-
ished monograph, recompiled 2014, 2002. URL http://www.stat.berkeley.edu/$\sim$aldous/RWG/
book.html.
(Citedonpage4)
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communication-
efficient sgd via gradient quantization and encoding. Advances in neural information processing
systems, 30, 2017.
(Citedonpages2,10,and11)
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and Cédric
Renggli. The convergence of sparsified gradient methods. Advances in Neural Information Processing
Systems, 31, 2018.
(Citedonpages3and5)
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signsgd:
Compressedoptimisationfornon-convexproblems. InInternational Conference on Machine Learning,
2018.
(Citedonpage5)
Aleksandr Beznosikov, Samuel Horváth, Peter Richtárik, and Mher Safaryan. On biased compression
for distributed learning. Journal of Machine Learning Research, 2023.
(Citedonpages2and5)
Chih-ChungChangandChih-JenLin. Libsvm: alibraryforsupportvectormachines. ACMtransactions
on intelligent systems and technology (TIST), 2011.
(Citedonpage10)
Ziheng Cheng, Xinmeng Huang, Pengfei Wu, and Kun Yuan. Momentum benefits non-iid federated
learning simply and provably. In The Twelfth International Conference on Learning Representations,
2024. URL https://openreview.net/forum?id=TdhkAcXkRi.
(Citedonpage3)
Jean-Baptiste Cordonnier. Convex optimization using sparsified stochastic gradient descent with
memory. 2018.
(Citedonpage3)
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd.
Advances in neural information processing systems, 32, 2019.
(Citedonpages2,8,and9)
Rudrajit Das, Anish Acharya, Abolfazl Hashemi, Sujay Sanghavi, Inderjit S Dhillon, and Ufuk Topcu.
Faster non-convex federated learning via global and local momentum. In Uncertainty in Artificial
Intelligence, 2022.
(Citedonpage3)
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex
optimization via stochastic path-integrated differential estimator. Advances in neural information
processing systems, 31, 2018.
(Citedonpage8)
Ilyas Fatkhullin, Igor Sokolov, Eduard Gorbunov, Zhize Li, and Peter Richtárik. Ef21 with bells &
whistles: Practical algorithmic extensions of modern error feedback, 2021.
(Citedonpage3)
12Ilyas Fatkhullin, Alexander Tyurin, and Peter Richtárik. Momentum provably improves error feedback!
Advances in Neural Information Processing Systems, 2024.
(Citedonpages2and3)
Yuan Gao, Rustem Islamov, and Sebastian U Stich. EControl: Fast distributed optimization with
compression and error control. In The Twelfth International Conference on Learning Representations,
2024a. URL https://openreview.net/forum?id=lsvlvWB9vz.
(Citedonpages2,3,and9)
Yuan Gao, Anton Rodomanov, and Sebastian U Stich. Non-convex stochastic composite optimization
with polyak momentum. arXiv preprint arXiv:2403.02967, 2024b.
(Citedonpage3)
Hossein Shokri Ghadikolaei, Sebastian Stich, and Martin Jaggi. Lena: Communication-efficient
distributed learning with self-triggered gradient uploads. In International Conference on Artificial
Intelligence and Statistics, pages 3943–3951. PMLR, 2021.
(Citedonpage12)
Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richtárik. Linearly converging error
compensated sgd. Advances in Neural Information Processing Systems, 2020.
(Citedonpage2)
Eduard Gorbunov, Filip Hanzely, and Peter Richtárik. Local sgd: Unified theory and new efficient
methods. In International Conference on Artificial Intelligence and Statistics, 2021.
(Citedonpage11)
Samuel Horváth and Peter Richtárik. A better alternative to error feedback for communication-efficient
distributed learning. arXiv preprint arXiv:2006.11077, 2020.
(Citedonpage3)
SamuelHorváth,Chen-YuHo,LudovitHorvath,AtalNarayanSahu,MarcoCanini,andPeterRichtárik.
Natural compression for distributed deep learning. In Mathematical and Scientific Machine Learning,
2022.
(Citedonpage5)
Kun Huang and Shi Pu. Cedas: A compressed decentralized stochastic gradient method with improved
convergence. arXiv preprint arXiv:2301.05872, 2023.
(Citedonpages3and4)
Xinmeng Huang, Ping Li, and Xiaoyun Li. Stochastic controlled averaging for federated learning with
communication compression. In The Twelfth International Conference on Learning Representations,
2024. URL https://openreview.net/pdf?id=jj5ZjZsWJe.
(Citedonpage3)
The MathWorks Inc. Symbolic math toolbox version: 23.2 (r2023b), 2023. URL https://
www.mathworks.com.
(Citedonpage6)
Rustem Islamov, Xun Qian, Slavomír Hanzely, Mher Safaryan, and Peter Richtárik. Distributed
newton-type methods with communication compression and bernoulli aggregation. Transactions on
Machine Learning Research, 2023.
(Citedonpage5)
Rustem Islamov, Mher Safaryan, and Dan Alistarh. AsGrad: A sharp unified analysis of asynchronous-
SGD algorithms. In Proceedings of The 27th International Conference on Artificial Intelligence and
Statistics, 2024.
(Citedonpages10and12)
Xiaowen Jiang, Anton Rodomanov, and Sebastian U Stich. Federated optimization with doubly
regularized drift correction. arXiv preprint arXiv:2404.08447, 2024.
(Citedonpage12)
HaoJin, YangPeng, WenhaoYang, ShusenWang, andZhihuaZhang. Federatedreinforcementlearning
with environment heterogeneity. In International Conference on Artificial Intelligence and Statistics,
2022.
(Citedonpage3)
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. Foundations and trends® in machine learning, 2021.
(Cited
onpage1)
13Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition. In Machine Learning and Knowledge Discovery in
Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016,
Proceedings, Part I 16, 2016.
(Citedonpage5)
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback fixes
signsgd and other gradient compression schemes. In International Conference on Machine Learning,
2019.
(Citedonpage3)
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U
Stich, and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in federated
learning. arXiv preprint arXiv:2008.03606, 2020a.
(Citedonpage3)
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International conference on machine learning. PMLR, 2020b.
(Citedonpage2)
David Kempe, Alin Dobra, and Johannes Gehrke. Gossip-based computation of aggregate information.
In 44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings., 2003.
(Citedonpage2)
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
(Citedonpage3)
SaritKhirirat,EduardGorbunov,SamuelHorváth,RustemIslamov,FakhriKarray,andPeterRichtárik.
Clip21: Error feedback for gradient clipping. arXiv preprint: arXiv 2305.18929, 2023.
(Cited on
page10)
Sajad Khodadadian, Pranay Sharma, Gauri Joshi, and Siva Theja Maguluri. Federated reinforcement
learning: Linear speedup under markovian sampling. In International Conference on Machine
Learning, 2022.
(Citedonpage3)
Anastasia Koloskova, Sebastian Stich, and Martin Jaggi. Decentralized stochastic optimization and
gossipalgorithmswithcompressedcommunication. InInternational Conference on Machine Learning,
2019.
(Citedonpages3,4,and11)
Anastasia Koloskova, Tao Lin, Sebastian U Stich, and Martin Jaggi. Decentralized deep learning with
arbitrary communication compression. In International Conference on Learning Representations,
2020a. URL https://openreview.net/forum?id=SkgGCkrKvH.
(Citedonpage3)
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified
theoryofdecentralizedsgdwithchangingtopologyandlocalupdates. InProceedings of International
Conference on Machine Learning, pages 5381–5393. PMLR, 2020b.
(Citedonpages2,9,and10)
Anastasiia Koloskova, Tao Lin, and Sebastian U Stich. An improved analysis of gradient tracking for
decentralized machine learning. Advances in Neural Information Processing Systems, 2021.
(Citedon
page2)
Jakub Konecny`, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint
arXiv:1610.05492, 2016.
(Citedonpage1)
14Dmitry Kovalev, Anastasia Koloskova, Martin Jaggi, Peter Richtarik, and Sebastian Stich. A linearly
convergent algorithm for decentralized optimization: Sending less bits for free! In International
Conference on Artificial Intelligence and Statistics, 2021.
(Citedonpage3)
Boyue Li, Zhize Li, and Yuejie Chi. Destress: Computation-optimal and communication-efficient
decentralized nonconvex finite-sum optimization. SIAM Journal on Mathematics of Data Science,
2022.
(Citedonpage2)
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic
gradient descent. Advances in neural information processing systems, 2017.
(Citedonpage3)
Yue Liu, Tao Lin, Anastasia Koloskova, and Sebastian U Stich. Decentralized gradient tracking with
local steps. Optimization Methods and Software, pages 1–28, 2024.
(Citedonpage2)
Yucheng Lu and Christopher De Sa. Optimal complexity in decentralized training. In International
Conference on Machine Learning, 2021.
(Citedonpage3)
Maksim Makarenko, Elnur Gasanov, Abdurakhmon Sadiev, Rustem Islamov, and Peter Richtárik.
Adaptive compression for communication-efficient distributed training. Transactions on Machine
Learning Research, 2023.
(Citedonpage10)
Amin Kargarian Marvasti, Yong Fu, Saber DorMohammadi, and Masoud Rais-Rohani. Optimal
operation of active distribution grids: A system of systems framework. IEEE Transactions on Smart
Grid, 2014.
(Citedonpage1)
Konstantin Mishchenko, Eduard Gorbunov, Martin Takáč, and Peter Richtárik. Distributed learning
with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
(Citedonpage2)
Konstantin Mishchenko, Ahmed Khaled, and Peter Richtárik. Random reshuffling: Simple analysis
with vast improvements. Advances in Neural Information Processing Systems, 2020.
(Citedonpage28)
Konstantin Mishchenko, Francis Bach, Mathieu Even, and Blake E Woodworth. Asynchronous sgd
beats minibatch sgd under arbitrary delays. Advances in Neural Information Processing Systems,
2022a.
(Citedonpage12)
KonstantinMishchenko, GrigoryMalinovsky, SebastianStich, andPeterRichtárik. Proxskip: Yes! local
gradient steps provably lead to communication acceleration! finally! In International Conference on
Machine Learning, 2022b.
(Citedonpage11)
Aritra Mitra, George J Pappas, and Hamed Hassani. Temporal difference learning with compressed
updates: Error-feedback meets reinforcement learning. arXiv preprint arXiv:2301.00944, 2023.
(Cited
onpage3)
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization.
IEEE Transactions on Automatic Control, 2009.
(Citedonpage2)
Angelia Nedic, Alex Olshevsky, and Wei Shi. Achieving geometric convergence for distributed optimiza-
tion over time-varying graphs. SIAM Journal on Optimization, 2017.
(Citedonpage2)
Angelia Nedić, Alex Olshevsky, and Michael G Rabbat. Network topology and communication-
computation tradeoffs in decentralized optimization. Proceedings of the IEEE, 2018.
(Cited on
page4)
15Boris T Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathe-
matics and Mathematical Physics, 1963.
(Citedonpage5)
BorisTPolyak. Somemethodsofspeedinguptheconvergenceofiterationmethods. Ussr computational
mathematics and mathematical physics, 1964.
(Citedonpage2)
Guannan Qu and Na Li. Harnessing smoothness to accelerate distributed optimization. IEEE
Transactions on Control of Network Systems, 2017.
(Citedonpage2)
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine
learning, 2021.
(Citedonpage1)
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.
(Cited on
page1)
Peter Richtárik, Igor Sokolov, and Ilyas Fatkhullin. Ef21: A new, simpler, theoretically better, and
practically faster error feedback. Advances in Neural Information Processing Systems, 2021.
(Citedon
page3)
PeterRichtárik,ElnurGasanov, andKonstantinPavlovichBurlachenko. Errorfeedbackreloaded: From
quadratic to arithmetic mean of smoothness constants. In The Twelfth International Conference on
Learning Representations, 2024. URL https://openreview.net/forum?id=Ch7WqGcGmb.
(Citedon
page12)
Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi Albarqouni, Spyridon
Bakas, Mathieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al. The future of digital health
with federated learning. NPJ digital medicine, 2020.
(Citedonpage3)
MherSafaryan,RustemIslamov,XunQian,andPeterRichtarik. FedNL:MakingNewton-typemethods
applicable to federated learning. In Proceedings of the 39th International Conference on Machine
Learning, 2022.
(Citedonpage5)
Stefano Savazzi, Monica Nicoli, and Vittorio Rampa. Federated learning with cooperating devices: A
consensus approach for massive iot networks. IEEE Internet of Things Journal, 2020.
(Citedonpage1)
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its
application to data-parallel distributed training of speech dnns. In Fifteenth annual conference of the
international speech communication association, 2014.
(Citedonpages2and3)
Ayush Sekhari, Karthik Sridharan, and Satyen Kale. Sgd: The role of implicit regularization, batch-size
and multiple-epochs. Advances In Neural Information Processing Systems, 2021.
(Citedonpage3)
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan
Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism.
arXiv preprint arXiv:1909.08053, 2019.
(Citedonpage1)
Navjot Singh, Deepesh Data, Jemin George, and Suhas Diggavi. Squarm-sgd: Communication-efficient
momentum sgd for decentralized optimization. IEEE Journal on Selected Areas in Information
Theory, 2021.
(Citedonpage3)
SebastianStich,AmirkeivanMohtashami,andMartinJaggi. Criticalparametersforscalabledistributed
learning with large batches and asynchronous updates. In International Conference on Artificial
Intelligence and Statistics, pages 4042–4050. PMLR, 2021.
(Citedonpage2)
16Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767,
2018.
(Citedonpages2and11)
Sebastian U Stich. On communication compression for distributed optimization on heterogeneous data.
arXiv preprint arXiv:2009.02388, 2020.
(Citedonpage2)
Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for
sgd with delayed gradients and compressed communication. arXiv preprint arXiv:1909.05350, 2019.
(Citedonpage2)
Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified sgd with memory. Advances
in neural information processing systems, 31, 2018.
(Citedonpages2,3,and5)
Nikko Ström. Scalable distributed dnn training using commodity gpu cloud computing. 2015.
(Citedon
page2)
Haoran Sun, Songtao Lu, and Mingyi Hong. Improving the sample and communication complexity
for decentralized non-convex optimization: Joint gradient estimation and tracking. In International
conference on machine learning, 2020.
(Citedonpage2)
Hanlin Tang, Shaoduo Gan, Ce Zhang, Tong Zhang, and Ji Liu. Communication compression for
decentralized training. Advances in Neural Information Processing Systems, 2018a.
(Citedonpage3)
Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. D2: Decentralized training over
decentralized data. In International Conference on Machine Learning, 2018b.
(Citedonpage11)
Hanlin Tang, Xiangru Lian, Shuang Qiu, Lei Yuan, Ce Zhang, Tong Zhang, and Ji Liu. Deepsqueeze:
Decentralizationmeetserror-compensatedcompression. arXiv preprint arXiv:1907.07346,2019.
(Cited
onpages3and4)
QuocTran-Dinh, NhanHPham, DzungTPhan, andLamMNguyen. Ahybridstochasticoptimization
framework for composite nonconvex optimization. Mathematical Programming, 191(2):1005–1071,
2022.
(Citedonpage8)
JianyuWang,VinayakTantia,NicolasBallas,andMichaelRabbat. Slowmo: Improvingcommunication-
efficient distributed sgd with slow momentum. arXiv preprint arXiv:1910.00643, 2019a.
(Citedon
page3)
Meng Wang, Weijie Fu, Xiangnan He, Shijie Hao, and Xindong Wu. A survey on large-scale machine
learning. IEEE Transactions on Knowledge and Data Engineering, 2020.
(Citedonpage1)
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum: Faster
variance reduction algorithms. Advances in Neural Information Processing Systems, 32, 2019b.
(Cited
onpage8)
WeiWen,CongXu,FengYan,ChunpengWu,YandanWang,YiranChen,andHaiLi.Terngrad: Ternary
gradients to reduce communication in distributed deep learning. Advances in neural information
processing systems, 2017.
(Citedonpage5)
D Randall Wilson and Tony R Martinez. The general inefficiency of batch training for gradient descent
learning. Neural networks, 2003.
(Citedonpage3)
Lin Xiao and Stephen Boyd. Fast linear iterations for distributed averaging. Systems & Control Letters,
2004.
(Citedonpage2)
17Ran Xin, Usman A Khan, and Soummya Kar. A fast randomized incremental gradient method for
decentralized nonconvex optimization. IEEE Transactions on Automatic Control, 2021.
(Citedon
page2)
Ran Xin, Usman A Khan, and Soummya Kar. Fast decentralized nonconvex finite-sum optimization
with recursive variance reduction. SIAM Journal on Optimization, 2022.
(Citedonpage2)
Yangyang Xu and Yibo Xu. Momentum-based variance-reduced proximal stochastic gradient method
for composite nonconvex stochastic optimization, 2022.
(Citedonpages8and9)
YongguiYan,JieChen,Pin-YuChen,XiaodongCui,SongtaoLu,andYangyangXu. Compresseddecen-
tralized proximal stochastic gradient method for nonconvex composite problems with heterogeneous
data. In International Conference on Machine Learning, 2023.
(Citedonpages2,3,and4)
Chung-Yiu Yau and Hoi-To Wai. Docom: Compressed decentralized optimization with near-optimal
sample complexity. arXiv preprint arXiv:2202.00255, 2022.
(Citedonpages2,3,and4)
HaoyuZhao,BoyueLi,ZhizeLi,PeterRichtárik,andYuejieChi. Beer: Fasto(1/t)ratefordecentralized
nonconvexoptimizationwithcommunicationcompression. AdvancesinNeuralInformationProcessing
Systems, 2022.
(Citedonpages2,3,4,10,11,19,and22)
18A Missing proof for MoTEF
We recall the notation we use to prove convergence of MoTEF:
Gˆt :=E(cid:104)(cid:13) (cid:13)∇F(Xt)1−Mt1(cid:13) (cid:13)2(cid:105)
n
G(cid:101)t :=(cid:88) E(cid:104)(cid:13) (cid:13)∇f i(xt i)−mt i(cid:13) (cid:13)2(cid:105) =E(cid:104)(cid:13) (cid:13)∇F(Xt)−Mt(cid:13) (cid:13)2 F(cid:105)
i=1
Ωt :=E(cid:104)(cid:13) (cid:13)Ht−Xt(cid:13) (cid:13)2(cid:105)
1 F
Ωt :=E(cid:104)(cid:13) (cid:13)Gt−Vt(cid:13) (cid:13)2(cid:105)
2 F
Ωt :=E(cid:104)(cid:13) (cid:13)Xt−x¯t1T(cid:13) (cid:13)2(cid:105)
3 F
Ωt :=E(cid:104)(cid:13) (cid:13)Vt−v¯t1T(cid:13) (cid:13)2(cid:105)
4 F
Ωt :=E(cid:104)(cid:13) (cid:13)v¯t(cid:13) (cid:13)2(cid:105) .
5
Moreover, Ft :=E[f(x¯t)]−f⋆. LetusdefineΩt :=[Gˆt,G(cid:101)t,Ωt,Ωt,Ωt,Ωt]⊤. Inaddition, wedenote
1 2 3 4
∇(cid:101)F(Xt):=[g1(xt),...,gn(xt)]∈Rd×n a matrix that contains local stochastic gradients. We denote
i n
C :=σ2 (W−I)≤4.
max
Lemma 7 (Lemma B.2 from [Zhao et al., 2022]). Let W be a mixing matrix with a spectral gap ρ.
Then for any matrix X∈Rd×n and x¯ = 1X1 we have
n
∥XW−x¯1⊤∥2 ≤(1−ρ)∥X−x¯1⊤∥2. (17)
F F
Moreover, for any γ ∈(0,1] the matrix W(cid:102) =I+γ(W−I) has a spectral gap at least γρ.
Lemma 8. The iterates of Algorithm 1 satisfy
1
v¯t+1 = Mt+11, (18)
n
and
η
x¯t+1 =x¯t− Mt1. (19)
n
Proof. By induction, we can show that v¯t = 1Mt1, if we initialize V0 =M0. Indeed, we have
n
1
v¯t+1 = Vt+11
n
1 1 1
= Vt1+ γGt(W−I)1+ (Mt+1−Mt)1
n n n
1 1
= Vt1+ (Mt+1−Vt)1
n n
1
= Mt+11.
n
Therefore, we have
γ η
x¯t+1 = x¯t+ Ht(W−I)1− Vt1
n n
η
= x¯t−ηv¯t =x¯t− Mt1.
n
19A.1 General non-convex setting.
Lemma 9. Assume that Assumption 2 holds. Then we have the following descent on Ft
F
t+1
≤F t− η 2E(cid:2) ∥∇f(x¯t)∥2(cid:3) + nη 2Gˆt+ ηL n2 Ωt 3−(−η/2−η2L/2)Ωt 5. (20)
Proof. Using smoothness we get
F ≤F
−ηE(cid:2)(cid:10) ∇f(x¯t),v¯t(cid:11)(cid:3)
+
η2L
E(cid:2) ∥vt∥2(cid:3)
t+1 t 2
=F t−
η 2E(cid:2) ∥∇f(x¯t)∥2(cid:3)
+
η 2E(cid:2) ∥∇f(x¯t)−v¯t∥2(cid:3) −(−η/2−η2L/2)E(cid:2) ∥v¯t∥2(cid:3)
=F t− η 2E(cid:2) ∥∇f(x¯t)∥2(cid:3) + η
2E(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)n1 ∇F(x¯t)1− n1
Mt1(cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:35)
−(−η/2−η2L/2)E(cid:2) ∥v¯t∥2(cid:3)
≤F t− η 2E(cid:2) ∥∇f(x¯t)∥2(cid:3)
+ηE(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)n1 ∇F(Xt)1− n1
Mt1(cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:35)
+ηE(cid:34)(cid:13)
(cid:13) (cid:13)1 ∇F(x¯t)1− 1
∇F(Xt)1(cid:13)
(cid:13)
(cid:13)2(cid:35)
−(−η/2−η2L/2)E(cid:2) ∥v¯t∥2(cid:3)
(cid:13)n n (cid:13)
≤F t− η 2E(cid:2) ∥∇f(x¯t)∥2(cid:3) + nη 2Gˆt+ η nL2 E(cid:104)(cid:13) (cid:13)Xt−x¯t1⊤(cid:13) (cid:13)2(cid:105) −(−η/2−η2L/2)Ωt
5
=F t− η 2E(cid:2) ∥∇f(x¯t)∥2(cid:3) + nη 2Gˆt+ η nL2 Ωt 3−(−η/2−η2L/2)Ωt 5.
Lemma 10. Assume that Assumptions 2 and 3 hold. Then we have the following descent on Gˆt
Gˆt+1 ≤(1−λ)E(cid:104)(cid:13) (cid:13)∇F(Xt)1−Mt1(cid:13) (cid:13)2(cid:105) + (1−λ)2nL2 E(cid:104)(cid:13) (cid:13)Xt−Xt+1(cid:13) (cid:13)2(cid:105) +λ2nσ2. (21)
λ F
Proof. Using the update rules of Algorithm 1 we get
Gˆt+1 = E(cid:104)(cid:13) (cid:13)∇F(Xt+1)1−Mt+11(cid:13) (cid:13)2(cid:105)
(cid:20)(cid:13) (cid:13)2(cid:21)
= E (cid:13)∇F(Xt+1)1−(1−λ)Mt1−λ∇(cid:101)F(Xt+1)1(cid:13)
(cid:13) (cid:13)
(cid:20)(cid:13) (cid:13)2(cid:21)
= E (cid:13)(1−λ)(∇F(Xt)−Mt)1+λ(∇F(Xt+1)−∇(cid:101)F(Xt+1))1+(1−λ)(∇F(Xt+1)−∇F(Xt))1(cid:13)
(cid:13) (cid:13)
≤ (1−λ)2E(cid:104)(cid:13) (cid:13)(∇F(Xt)−Mt)1+(∇F(Xt+1)−∇F(Xt))1(cid:13) (cid:13)2(cid:105) +λ2nσ2
≤ (1−λ)E(cid:104)(cid:13) (cid:13)∇F(Xt)1−Mt1(cid:13) (cid:13)2(cid:105) + (1−λ)2nL2 E(cid:104)(cid:13) (cid:13)Xt−Xt+1(cid:13) (cid:13)2(cid:105) +λ2nσ2,
λ F
(cid:104) (cid:105)
whereinthefirstinequalityweusethefactthatE ∇(cid:101)F(Xt+1) =∇F(Xt+1)andAssumption3,andin
the second inequality we use ∥a+b∥2 ≤(1+β)∥a∥2+(1+β−1)∥b∥2 for any vectors a,b and constant
b.
Lemma 11. Assume that Assumptions 2 and 3 hold. Then we have the following descent on G(cid:101)t
G(cid:101)t+1 ≤λ2σ2n+ (1−λ)2L2 E(cid:104)(cid:13) (cid:13)Xt+1−Xt(cid:13) (cid:13)2(cid:105) +(1−λ)G(cid:101)t. (22)
λ F
20Proof.
G(cid:101)t+1 = E(cid:104)(cid:13) (cid:13)∇F(Xt+1)−Mt+1(cid:13) (cid:13)2(cid:105)
F
(cid:20)(cid:13) (cid:13)2(cid:21)
= E (cid:13)∇F(Xt+1)−(1−λ)Mt−λ∇(cid:101)F(Xt+1)(cid:13)
(cid:13) (cid:13)
F
≤
λ2σ2n+(1−λ)2E(cid:104)(cid:13) (cid:13)∇F(Xt+1)−Mt(cid:13) (cid:13)2(cid:105)
F
≤
λ2σ2n+(1−λ)2(1+β−1)E(cid:104)(cid:13) (cid:13)∇F(Xt+1)−∇F(Xt)(cid:13) (cid:13)2(cid:105)
1 F
+(1−λ)2(1+β 1)E(cid:104)(cid:13) (cid:13)Mt−∇F(Xt)(cid:13) (cid:13)2 F(cid:105)
≤ λ2σ2n+ (1−λ)2 E(cid:104)(cid:13) (cid:13)∇F(Xt+1)−∇F(Xt)(cid:13) (cid:13)2(cid:105) +(1−λ)E(cid:104)(cid:13) (cid:13)Mt−∇F(Xt)(cid:13) (cid:13)2(cid:105)
λ F F
≤ λ2σ2n+ (1−λ)2L2 E(cid:104)(cid:13) (cid:13)Xt+1−Xt(cid:13) (cid:13)2(cid:105) +(1−λ)G(cid:101)t.
λ F
where we choose β = λ .
1 (1−λ)
Lemma 12. Let C be any contractive compressor with parameter α. Then we have the following
α
descent on Ωt
1
Ωt 1+1 ≤(1−α/2)E(cid:104)(cid:13) (cid:13)Ht−Xt(cid:13) (cid:13)2 F(cid:105) + α2 E(cid:104)(cid:13) (cid:13)Xt−Xt+1(cid:13) (cid:13)2 F(cid:105) . (23)
Proof. We have
Ωt+1 =E(cid:104)(cid:13) (cid:13)Ht+1−Xt+1(cid:13) (cid:13)2(cid:105)
1 F
=E(cid:104)(cid:13) (cid:13)Ht+C α(Xt+1−Ht)−Xt+1(cid:13) (cid:13)2 F(cid:105)
≤(1−α)E(cid:104)(cid:13) (cid:13)Ht−Xt+1(cid:13) (cid:13)2(cid:105)
F
≤(1−α/2)E(cid:104)(cid:13) (cid:13)Ht−Xt(cid:13) (cid:13)2(cid:105)
+
2 E(cid:104)(cid:13) (cid:13)Xt−Xt+1(cid:13) (cid:13)2(cid:105)
.
F α F
Lemma 13. Let C be any contractive compressor with parameter α. Then we have the following
α
descent on Ωt
2
Proof. The proof is similar to the one of Lemma 12
Ωt+1 =E(cid:104)(cid:13) (cid:13)Gt+1−Vt+1(cid:13) (cid:13)2(cid:105)
2 F
≤(1−α/2)E(cid:104)(cid:13) (cid:13)Gt−Vt(cid:13) (cid:13)2(cid:105)
+
2 E(cid:104)(cid:13) (cid:13)Vt−Vt+1(cid:13) (cid:13)2(cid:105)
.
F α F
Lemma 14. We have the following descent on Ωt
3
Ωt 3+1 ≤(1−γρ/2)Ωt 3+(1+2/γρ)2γ2CΩt 1+(1+2/γρ)2η2Ωt 4. (24)
21Proof.
Ωt+1 =E(cid:104)(cid:13) (cid:13)Xt+1−x¯t+11⊤(cid:13) (cid:13)2(cid:105)
3 F
=E(cid:104)(cid:13) (cid:13)Xt+γHt(W−I)−ηVt−x¯t1T +ηv¯t1T(cid:13) (cid:13)2(cid:105)
F
(cid:20)(cid:13) (cid:13)2(cid:21)
=E (cid:13)XtW(cid:102) −x¯t1T +γ(Ht−Xt)(W−I)−ηVt+ηv¯t1⊤(cid:13)
(cid:13) (cid:13)
F
≤(1+β)(1−γρ)E(cid:104)(cid:13) (cid:13)Xt−x¯t1⊤(cid:13) (cid:13)2(cid:105) +(1+β−1)(2γ2E(cid:104)(cid:13) (cid:13)(Ht−Xt)(W−I)(cid:13) (cid:13)2(cid:105)
F F
+
2η2E(cid:104)(cid:13) (cid:13)Vt−v¯t1⊤(cid:13) (cid:13)2(cid:105)
)
F
≤(1−γρ/2)E(cid:104)(cid:13) (cid:13)Xt−x¯t1⊤(cid:13) (cid:13)2(cid:105) +(1+2/γρ)(2γ2CE(cid:104)(cid:13) (cid:13)Ht−Xt(cid:13) (cid:13)2(cid:105)
F F
+
2η2E(cid:104)(cid:13) (cid:13)Vt−v¯t1⊤(cid:13) (cid:13)2(cid:105)
)
F
=(1−γρ/2)Ωt 3+(1+2/γρ)2γ2CΩt 1+(1+2/γρ)2η2Ωt 4.
whereβ = γρ/2 andwedefineW(cid:102) :=I+γ(W−I)whichhasaspectralgapatleastγρbyLemma7.
1−γρ
Lemma 15. We have the following descent on Ωt
4
Ωt 4+1 ≤ (1−γρ/2)E(cid:104)(cid:13) (cid:13)Vt−v¯t1T(cid:13) (cid:13)2 F(cid:105)
+(1+2/γρ)(cid:16) 2γ2CE(cid:104)(cid:13) (cid:13)Gt−Vt(cid:13) (cid:13)2(cid:105) +2E(cid:104)(cid:13) (cid:13)Mt+1−Mt(cid:13) (cid:13)2(cid:105)(cid:17) . (25)
F F
Proof.
Ωt+1 =E(cid:104)(cid:13) (cid:13)Vt+1−v¯t1T +v¯t1T −v¯t+11T(cid:13) (cid:13)2(cid:105)
4 F
=E(cid:104)(cid:13) (cid:13)Vt+1−v¯t1T(cid:13) (cid:13)2(cid:105) −nE(cid:104)(cid:13) (cid:13)v¯t−v¯t+1(cid:13) (cid:13)2(cid:105)
F
≤E(cid:104)(cid:13) (cid:13)Vt+1−v¯t1T(cid:13) (cid:13)2(cid:105)
F
=E(cid:104)(cid:13) (cid:13)Vt+γGt(W−I)+Mt+1−Mt−v¯t1T(cid:13) (cid:13)2(cid:105)
F
(cid:20)(cid:13) (cid:13)2(cid:21)
=E (cid:13)VtW(cid:102) −v¯t1T +γ(Gt−Vt)(W−I)+Mt+1−Mt(cid:13)
(cid:13) (cid:13)
F
≤(1−γρ/2)E(cid:104)(cid:13) (cid:13)Vt−v¯t1T(cid:13) (cid:13)2(cid:105) +(1+2/γρ)(2γ2CE(cid:104)(cid:13) (cid:13)Gt−Vt(cid:13) (cid:13)2(cid:105)
F F
+
2E(cid:104)(cid:13) (cid:13)Mt+1−Mt(cid:13) (cid:13)2(cid:105)
).
F
Lemma 16 (Lemma B.4, Eq. (18) from [Zhao et al., 2022]). We have the following control of the
iterates at iterations t and t+1
E(cid:104)(cid:13) (cid:13)Xt+1−Xt(cid:13) (cid:13)2(cid:105) ≤3γ2CΩt +3γ2CΩt +3η2Ωt +3η2nΩt. (26)
F 1 3 4 5
Lemma 17. Assume Assumptions 2 and 3 hold. Then we have the following control of the momentum
at iterations t and t+1
E(cid:104)(cid:13) (cid:13)Mt+1−Mt(cid:13) (cid:13)2(cid:105) ≤λ2nσ2+2λ2E(cid:104)(cid:13) (cid:13)∇F(Xt)−Mt(cid:13) (cid:13)2(cid:105) +2λ2L2E(cid:104)(cid:13) (cid:13)Xt−Xt+1(cid:13) (cid:13)2(cid:105) . (27)
F F F
22Proof.
E(cid:104)(cid:13) (cid:13)Mt+1−Mt(cid:13) (cid:13)2 F(cid:105) =λ2E(cid:20)(cid:13) (cid:13) (cid:13)∇(cid:101)F(Xt+1)−Mt(cid:13) (cid:13) (cid:13)2 F(cid:21)
(cid:20)(cid:13) (cid:13)2(cid:21)
=λ2E (cid:13)∇(cid:101)F(Xt+1)−∇F(Xt+1)+∇F(Xt+1)−Mt(cid:13)
(cid:13) (cid:13)
F
≤λ2nσ2+λ2E(cid:104)(cid:13) (cid:13)∇F(Xt+1)−Mt(cid:13) (cid:13)2(cid:105)
F
≤λ2nσ2+2λ2E(cid:104)(cid:13) (cid:13)∇F(Xt)−Mt(cid:13) (cid:13)2(cid:105) +2λ2L2E(cid:104)(cid:13) (cid:13)Xt−Xt+1(cid:13) (cid:13)2(cid:105)
.
F F
Lemma 18. We have the following control of the gradient estimator Vt at iterations t and t+1
E(cid:104)(cid:13) (cid:13)Vt+1−Vt(cid:13) (cid:13)2(cid:105) ≤3γ2CΩt +3γ2CΩt +3E(cid:104)(cid:13) (cid:13)Mt+1−Mt(cid:13) (cid:13)2(cid:105) . (28)
F 2 4 F
Proof.
E(cid:104)(cid:13) (cid:13)Vt+1−Vt(cid:13) (cid:13)2(cid:105) =E(cid:104)(cid:13) (cid:13)γGt(W−I)+Mt+1−Mt(cid:13) (cid:13)2(cid:105)
F F
=E(cid:104)(cid:13) (cid:13)γ(Gt−Vt)(W−I)+γ(Vt−v¯t1T)(W−I)+Mt+1−Mt(cid:13) (cid:13)2(cid:105)
F
≤3γ2CE(cid:104)(cid:13) (cid:13)Gt−Vt(cid:13) (cid:13)2(cid:105) +3γ2CE(cid:104)(cid:13) (cid:13)Vt−v¯t1T(cid:13) (cid:13)2(cid:105)
F F
+
3E(cid:104)(cid:13) (cid:13)Mt+1−Mt(cid:13) (cid:13)2(cid:105)
F
=3γ2CΩt +3γ2CΩt +3E(cid:104)(cid:13) (cid:13)Mt+1−Mt(cid:13) (cid:13)2(cid:105) .
2 4 F
Theorem 1 (ConvergenceofMoTEF). Let Assumptions 1 to 3 hold. Then there exist absolute constants
c ,c ,c , and some τ ≤ 1 such that if we set stepsizes γ = c αρ,λ = c αρ3τ,η = c L−1αρ3τ, and
γ λ η γ λ η
choosing the initial batch size B ≥⌈LF0⌉, then after at most
init σ2
(cid:18) σ2 σ σ σ 1 (cid:19)
T =O + + + + LF0 (10)
nε4 αρ2ε3 α1/2ρ3/2ε3 αρ5/2ε3 αρ3ε2
iterations of Algorithm 1 it holds E(cid:2) ∥∇f(x )∥2(cid:3) ≤ ε2, where x is chosen uniformly at random
out out
from {x¯ ,...,x¯ }, and O suppresses absolute constants.
0 T−1
Proof. From Lemma 17 and Lemma 16 we get
E(cid:104)(cid:13) (cid:13)Mt+1−Mt(cid:13) (cid:13)2(cid:105) ≤λ2nσ2+2λ2G(cid:101)t+6λ2γ2L2CΩt +6λ2γ2L2CΩt +6λ2η2L2Ωt
F 1 3 4
+6λ2η2L2nΩt. (29)
5
Using the above and Lemma 18 we get
E(cid:104)(cid:13) (cid:13)Vt+1−Vt(cid:13) (cid:13)2(cid:105) ≤3λ2nσ2+6λ2G(cid:101)t+18λ2γ2L2CΩt +3γ2CΩt +18λ2γ2L2CΩt
F 1 2 3
+(3γ2C+18λ2η2L2)Ωt +18λ2η2L2nΩt. (30)
4 5
23Using (29), (30), and Lemma 10 we get the following descent on Gˆt
3L2nγ2C 3L2nγ2C 3L2nη2 3L2n2η2
Gˆt+1 ≤(1−λ)Gˆt+ Ωt + Ωt + Ωt + Ωt +λ2nσ2.
λ 1 λ 3 λ 4 λ 5
Using (29), (30), and Lemma 11 we get the following descent on G(cid:101)t
3L2γ2C 3L2γ2C 3L2η2 3L2nη2
G(cid:101)t+1 ≤(1−λ)G(cid:101)t+
λ
Ω 1+
λ
Ωt 3+
λ
Ωt 4+
λ
Ωt 5+λ2nσ2.
Using (29), (30), and Lemma 12 we get the following descent on Ωt
1
(cid:18) α 6γ2C(cid:19) 6γ2C 6η2 6η2n
Ωt+1 ≤ 1− + Ωt + Ωt + Ωt + Ωt.
1 2 α 1 α 3 α 4 α 5
Using (29), (30), and Lemma 13 we get the following descent on Ωt
2
(cid:18) α 6γ2C(cid:19) 6λ2 36λ2γ2L2C 36λ2γ2L2C
Ωt+1 ≤ 1− + Ωt + G(cid:101)t+ Ωt + Ωt
2 2 α 2 α α 1 α 3
(cid:18) 6γ2C 36η2λ2L2(cid:19) 36η2λ2L2n 6λ2n
+ + Ωt + Ωt + σ2.
α α 4 α 5 α
Using (29), (30), and Lemma 14 we get the following descent on Ωt
3
γρ 6γC 6η2
Ωt+1 ≤(1− )Ωt + Ωt + Ωt. (31)
3 2 3 ρ 1 γρ 4
Finally, using (29), (30), and Lemma 15 we get the following descent on Ωt:
4
γρ 36η2λ2L2 12λ2 36γλ2L2C 6γC 36γλ2L2C
Ωt+1 ≤(1− + )Ωt + G(cid:101)t+ Ωt + Ωt + Ωt
4 2 γρ 4 γρ ρ 1 ρ 2 ρ 3
36η2γL2n 6λ2n
+ Ωt + σ2.
ρ 5 γρ
Now we can gather all together

1−λ 0
3L2nγ2C
0
3L2nγ2C 3L2nη2 
λ λ λ
 0 1−λ 3L2γ2C 0 3L2γ2C 3L2η2 
 λ λ λ 
Ωt+1 ≤    0 0 60 λ2 1− 36λα 2 2γ+ 2L6 2γ Cα2C 1− α0 + 6γ2C 36λ6 2γ γα2 2C L2C 6γ2C +6 αη 32 6λ2η2L2    Ωt
 α α 2 α α α α 
 0 0 6γC 0 1− γρ 6η2 
 ρ 2 γρ 
0 12λ2 36γλ2L2C 6γC 36γλ2L2C 1− γρ + 36η2λ2L2
γρ ρ ρ ρ 2 γρ
(cid:124) (cid:123)(cid:122) (cid:125)
:=A
 3L2n2η2 
 
λ n
 3L2nη2 
 λ  2n
+    36η6 2η λα2 2n L2n   Ωt
5
+    60 n   λ2σ2. (32)
  α 0     α 0  
  6n
36η2γL2n
γρ
ρ (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) :=(cid:123) b(cid:122)
1
(cid:125) :=b2
24We remind that the Lyapunov function Φt has the following form
c c τ c L c τ c L c τ
Φt :=Ft+ 1 Gˆt+ 2 G(cid:101)t+ 3 Ωt + 4 Ωt + 5 Ωt + 6 Ωt =Ft+c⊤Ωt,
n2L nL ρ3nτ 1 ρnL 2 ρ3nτ 3 ρnL 4
where {c }6 are absolute constants. Let
k k=1
(cid:18)
c c τ c L c τ c L c τ
(cid:19)⊤
c:= 1 , 2 , 3 , 4 , 5 , 6 .
n2L nL ρ3nτ ρnL ρ3nτ ρnL
Therefore, the descent on Φt for is the following
Φt+1 =Ft+1+c⊤Ωt
≤F t− η 2E(cid:104)(cid:13) (cid:13)∇f(x¯t)(cid:13) (cid:13)2(cid:105) + nη 2Gˆt+ η nL2 Ωt 3−(η/2−η2L/2)Ωt
5
+ c⊤(AΩt+Ωtb +λ2σ2b )
5 1 2
=Ft− η 2E(cid:104)(cid:13) (cid:13)∇f(x¯t)(cid:13) (cid:13)2(cid:105) +c⊤Ωt+(q⊤+c⊤A−c⊤)Ωt−(η/2−η2L/2−c⊤b 1)Ωt
5
+ c⊤b λ2σ2
2
=Φt− η 2E(cid:104)(cid:13) (cid:13)∇f(x¯t)(cid:13) (cid:13)2(cid:105) +(q⊤+c⊤A−c⊤)Ωt−(η/2−η2L/2−c⊤b 1)Ωt
5
+ c⊤b λ2σ2,
2
where q := (η/n2,0,0,0,ηL2/n,0)⊤. We need coefficients next to Ωt and Ωt
5
to be negative. This is
equivalent to finding c such that
(cid:20) I−A⊤(cid:21) (cid:20) q (cid:21)
c≥ . (33)
−b⊤ η2L − η
1 2 2
We make the following choice of stepsizes
c αρ3τ
λ:=c αρ3τ, γ :=c αρ, η := η .
λ γ L
with the following choice of constants:
1 1 1
c = ,c = ,c = , and,
λ 200 γ 200 η 100000
1 13 1 1 9 1
c = ,c = ,c = ,c = ,c = ,c = . (34)
1 500 2 200000 3 20 4 400000 5 100 6 200000
The system of inequalities (33) are satisfied when τ ≤1.
Given the complexity of the inequalities and the choices of the parameters, we do not attempt to
write down a proof for the correctness of the choices manually, instead, we verify these choices using
the Symbolic Math Toolbox in MATLAB. We also perform such verification for our parameters and
constants choices for MoTEF in PŁ case and MoTEF-VR. The code performing all the verification can
be found at here. We also note that, when c ,c and c are fixed, we can search for a feasible {c }
λ γ η i i∈[6]
efficiently using the Linear Program solver with MATLAB as well. But searching for a feasible set of
choices for c ,c and c is very much a trial-and-error process.
λ γ η
25Note that this choice gives us both λ and γ smaller than 1. This choice of constants gives the
following result
Φt+1 ≤ Φt− c ηαρ3τ E(cid:2) ∥∇f(x¯t)∥2(cid:3) + c 1 ·nc2α2ρ6τ2σ2
2L n2L λ
c τ
+ 2 ·2nc2α2ρ6τ2σ2
nL λ
c τ 6n
+ 4 · c2α2ρ6τ2σ2
ρnL α λ
c τ 6n
+ 6 · c2α2ρ6τ2σ2
ρnL c αρ λ
γ
= Φt− c ηαρ3τ E(cid:2) ∥∇f(x¯t)∥2(cid:3) + c2 λc 1α2ρ6 ·τ2σ2
2L nL
(cid:18) 6c2c αρ5 2c2c α2ρ6 6c c2αρ4(cid:19)
+ λ 4 + λ 2 + 6 λ τ3σ2. (35)
L L c L
γ
By this, we proved Lemma 1. Let us define constants
c αρ3
B := η ,
2L
c2c α2ρ6
C := λ 1 ,
nL
(cid:18) 6c2c αρ5 2c2c α2ρ6 6c c2αρ4(cid:19)
D := λ 4 + λ 2 + 6 λ ,
L L c L
γ
E := 1.
Using τ ≤E and unrolling (35) for T iterations we get
1 T (cid:88)−1 E(cid:2) ∥∇f(x¯t)∥2(cid:3) ≤ Φ0 + C τσ2+ D τ2σ2.
T τBT B B
t=0
(cid:26) (cid:16) (cid:17)1/2 (cid:16) (cid:17)1/3(cid:27)
So we need to choose τ =min 1, Φ0 , Φ0 and we get the following rate
E CTσ2 DTσ2
1 T (cid:88)−1 E(cid:2) ∥∇f(x¯t)∥2(cid:3)
≤
O
Φ0E +(cid:18) CΦ0σ2(cid:19)1/2
+(cid:32)√ DΦ0σ(cid:33)2/3

T BT B2T B3/2T
t=0
(cid:32) LΦ0 (cid:18) LΦ0σ2(cid:19)1/2
= O +
αρ3T nT
(cid:32)(cid:112) (cid:33)2/3
αρ5+α2ρ6+αρ4LΦ0σ
+ 
α3/2ρ9/2T
(cid:32) LΦ0 (cid:18) LΦ0σ2(cid:19)1/2 (cid:18) (ρ1/2+α1/2ρ+1)LΦ0σ(cid:19)2/3(cid:33)
= O + + ,
T nT αρ7/2T
that translates to the rate in terms of ε to
1 T (cid:88)−1 E(cid:2) ∥∇f(x¯t)∥2(cid:3) ≤ε2 ⇒T =O(cid:18) LΦ0 + LΦ0σ2 + LΦ0σ + LΦ0σ + LΦ0σ (cid:19) .
T αρ3ε2 nε4 αρ2ε3 α1/2ρ3/2ε3 αρ5/2ε3
t=0
26Note that with the choice V0 =G0 =M0 =∇(cid:101)F(X0),H0 =X0 =x01⊤, we get
Gˆ0 ≤σ2n, G(cid:101)0 ≤σ2n, Ω0 =Ω0 =Ω0 =Ω0 =0.
1 2 3 4
c c τ
Φ0 ≤F0+ 1 σ2n+ 2 σ2n. (36)
n2L nL
If we choose the initial batch size B ≥⌈ σ2 ⌉, we get
init LF0
1 σ2 1 σ2
Φ0 ≤F0+ + ≤3F0. (37)
nLB LB
init init
A.2 PŁ setting.
Theorem 4 (ConvergenceofMoTEF). Let Assumptions 1 to 4 hold. Then there exist absolute constants
c ,c ,c , and some τ ≤ 1 such that if we set stepsizes γ = c αρ,λ = c αρ3τ,η = c L−1αρ3τ, and
γ λ η γ λ η
choosing the initial batch size B ≥⌈LF0⌉, then after at most
init σ2
(cid:18) Lσ2 Lσ Lσ Lσ L (cid:19)
T =O(cid:101) + + + + (38)
µ2nε αρ2µ3/2ε1/2 αρ5/2µ3/2ε1/2 αρ2µ3/2ε1/2 µαρ3
iterations of Algorithm 1 it holds E(cid:2) f(xT)−f∗(cid:3) ≤ε, and O(cid:101) suppresses absolute constants and poly-
logarithmic factors.
Proof. The only change in the proof is the descent of the Lyapunov function. In PŁ case, the descent
on Φt becomes
Φt+1 =Ft+1+b⊤Ωt
≤F t− η 2E(cid:104)(cid:13) (cid:13)∇f(x¯t)(cid:13) (cid:13)2(cid:105) + nη 2Gˆt+ η nL2 Ωt 3−(η/2−η2L/2)Ωt
5
+ b⊤(AΩt+Ωtb +λ2σ2b )
5 1 2
≤(1−ηµ)Ft+(1−ηµ)b⊤Ωt+(q⊤+b⊤A−(1−ηµ)b⊤)Ωt
− (η/2−η2L/2−b⊤b 1)Ωt 5+b⊤b 2λ2σ2
=(1−ηµ)Φt+(q⊤+b⊤A−(1−ηµ)b⊤)Ωt−(η/2−η2L/2−b⊤b 1)Ωt 5+b⊤b 2λ2σ2,
where in the second inequality we use PŁ condition. Similar to the proof of Theorem 1, we need to
satisfy
(cid:20) (1−µη)I−A⊤(cid:21) (cid:20) q (cid:21)
b≥
−b⊤ η2L − η
1 2 2
for some coefficients b. We set the stepsizes such that
c αρ3τ
λ:=c αρ3τ, γ :=c αρ, η := η ,
λ γ L
and
(cid:18)
b b τ b L b τ b L b τ
(cid:19)⊤
b:= 1 , 2 , 3 , 4 , 5 , 6
n2L nL ρ3nτ ρnL ρ3nτ ρnL
27with the choice
1 1 1
c = ,c = ,c = ,
λ 200000 γ 200000 η 100000000
and
1 13 1 1 1
b = ,b = ,b = ,b = ,b =2,b = ,
1 250 2 200000 3 20 4 400000 5 6 200000
gives the following descent on Φt (note that both γ and λ are smaller than 1 with this choice of
constants)
(cid:18) c αρ3τµ(cid:19) c2b α2ρ6
Φt+1 ≤ 1− η Φt+ λ 1 ·τ2σ2
L nL
(cid:18) 6c2b αρ5 2c2b α2ρ6 6b c2αρ4(cid:19)
+ λ 4 + λ 2 + 6 λ τ3σ2. (39)
L L c L
γ
Let us define constants
c αρ3µ
B := η ,
2L
c2c α2ρ6
C := λ 1 ,
nL
(cid:18) 6c2c αρ5 2c2c α2ρ6 6c c2αρ4(cid:19)
D := λ 4 + λ 2 + 6 λ ,
L L c L
γ
E := 1.
Unrolling (39) for T iterations we get
C D C D
ΦT ≤(1−Bτ)TΦ0+ τ2σ2+ τ3σ2 =(1−Bτ)TΦ0+ σ2+ τ3σ2
Bτ Bτ B Bτ
where we use the fact that
m (cid:88)−1 1−(1−Bτ)m 1
(1−Bτ)l = ≤ .
1−(1−Bτ) Bτ
l=0
(cid:110) (cid:16) (cid:17) (cid:16) (cid:17)(cid:111)
Choosing τ =min 1, 1 log Φ0B2T , 1 log Φ0B3T2 leads to the following rate
E BT Cσ2 BT Dσ2
(cid:18) (cid:18) B (cid:19) Cσ2 Dσ2 (cid:19)
ΦT ≤O(cid:101) exp − T Φ0+ + .
E B2T B3T2
We refer to [Mishchenko et al., 2020] for a more detailed derivation (proof of Corollary 1, page 20). To
achieve FT ≤ε, we need to perform
(cid:32) √ (cid:33)
E Cσ2 Dσ
T = O(cid:101) + +
B B2ε B3/2ε1/2
(cid:18) L Lσ2 Lσ Lσ Lσ (cid:19)
= O(cid:101) + + + +
µαρ3 µ2nε αρ2µ3/2ε1/2 αρ5/2µ3/2ε1/2 αρ2µ3/2ε1/2
iterations.
28B Missing proofs for MoTEF-VR
In this section, we provide the proof of convergence of Algorithm 2. Note that in this case Lemma 9
remains unchanged.
Lemma 19. Let Assumptions 3 and 5 hold. Then we have the following descent on Gˆt
Gˆt+1 ≤ (1−λ)Gˆt+2λ2σ2n+ℓ2E(cid:2) ∥Xt+1−Xt∥2(cid:3) . (40)
F
Proof. We have
Gˆt+1 = E(cid:104)(cid:13) (cid:13)Mt+11−∇F(Xt+1)1(cid:13) (cid:13)2(cid:105)
(cid:20)(cid:13) (cid:13)2(cid:21)
= E (cid:13)[∇(cid:101)F(Xt+1,Ξt+1)+(1−λ)(Mt−∇(cid:101)F(Xt,Ξt+1)−∇F(Xt+1)]1(cid:13)
(cid:13) (cid:13)
(cid:104)(cid:13)(cid:16)
= E (cid:13) λ(∇(cid:101)F(Xt+1,Ξt+1)−∇F(Xt+1))
(cid:13)
+(1−λ)(∇(cid:101)F(Xt+1,Ξt+1)−∇F(Xt+1)+∇F(Xt)−∇(cid:101)F(Xt,Ξt+1))
+(1−λ)(Mt−∇F(Xt))(cid:1) 1(cid:13) (cid:13)2(cid:105)
≤ (1−λ)2E∥(Mt−∇F(Xt))1∥2
+2λ2E∥(∇(cid:101)F(Xt+1,Ξt+1)−∇F(Xt+1))1∥2
+2(1−λ)2E∥(∇(cid:101)F(Xt+1,Ξt+1−∇F(Xt+1)+∇F(Xt)−∇(cid:101)F(Xt,Ξt+1))1∥2
≤ (1−λ)Gˆt+2λ2σ2n
+2E∥(∇(cid:101)F(Xt+1,Ξt+1)−∇F(Xt+1)+∇F(Xt)−∇(cid:101)F(Xt,Ξt+1))1∥2. (41)
For the last term above we continue as follows
(cid:104) (cid:105)
E ∥(∇(cid:101)F(Xt+1,Ξt+1)−∇F(Xt+1)+∇F(Xt)−∇(cid:101)F(Xt,Ξt+1))1∥2
(cid:13)
(cid:13)(cid:88)n
(cid:13) (cid:13)2
= E (cid:13) (cid:13) ∇f i(xt i+1,ξ it+1)−∇f i(xt i+1)+∇f i(xt i)−∇f i(xt i,ξ it+1)(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1
n
= (cid:88) E(cid:104)(cid:13) (cid:13)∇f i(xt i+1,ξ it+1)−∇f i(xt i+1)+∇f i(xt i)−∇f i(xt i,ξ it+1)(cid:13) (cid:13)2(cid:105)
i=1
n
≤ (cid:88) E(cid:104)(cid:13) (cid:13)∇f i(xt i+1,ξ it+1)−∇f i(xt i,ξ it+1)(cid:13) (cid:13)2(cid:105)
i=1
≤ ℓ2E(cid:2) ∥Xt+1−Xt∥2(cid:3) . (42)
F
Therefore, from (41) we get
Gˆt+1 ≤ (1−λ)Gˆt+2λ2σ2n+ℓ2E(cid:2) ∥Xt+1−Xt∥2(cid:3) . (43)
F
Lemma 20. Assume Assumptions 3 and 5 hold. Then we have the following descent on Gˆt
G(cid:101)t+1 ≤ (1−λ)G(cid:101)t+2λ2σ2n+ℓ2E(cid:2) ∥Xt+1−Xt∥2(cid:3) . (44)
F
Proof. The proof is similar to the one of Lemma 19.
29Note that Lemmas 12 to 16 and 18 do not change in this setting, thus, we do not repeat them.
Lemma 21. Assume Assumptions 3 and 5 hold. Then we have the following control of momentum at
iterations t and t+1
E(cid:2) ∥Mt+1−Mt∥2(cid:3) ≤λ2G(cid:101)t+2λ2nσ2+2ℓ2E(cid:2) ∥Xt+1−Xt∥2(cid:3) . (45)
F F
Proof. Using the update of Mt we have
E(cid:2) ∥Mt+1−Mt∥2(cid:3) =E(cid:104) ∥∇(cid:101)F(Xt+1,Ξt+1)+(1−λ)(Mt−∇(cid:101)F(Xt,Ξt+1))−Mt∥2(cid:105)
F F
(cid:104) (cid:105)
=E ∥∇(cid:101)F(Xt+1,Ξt+1)−λMt−(1−λ)∇(cid:101)F(Xt,Ξt+1)∥2
F
(cid:104)(cid:13)
=E (cid:13)λ(∇F(Xt)−Mt)+λ(∇(cid:101)F(Xt,Ξt+1)−∇F(Xt))
(cid:13)
(cid:13)2(cid:21)
+(∇(cid:101)F(Xt+1,Ξt+1)−∇(cid:101)F(Xt,Ξt+1))(cid:13)
(cid:13)
F
(cid:104)(cid:13)
=λ2G(cid:101)t+E (cid:13)λ(∇(cid:101)F(Xt,Ξt+1)−∇F(Xt))
(cid:13)
(cid:13)2(cid:21)
+ (∇(cid:101)F(Xt+1,Ξt+1)−∇(cid:101)F(Xt,Ξt+1))(cid:13)
(cid:13)
F
≤λ2G(cid:101)t+2λ2nσ2+2ℓ2E(cid:2) ∥Xt+1−Xt∥2(cid:3)
.
F
Theorem 3 (Convergence of MoTEF-VR). Let Assumptions 1, 3 and 5 hold. Then there exists absolute
constantsc ,c ,c andsomeτ <1suchthatifwestepsizesγ =c αρ,λ=c n−1α2ρ6τ2,η =c ℓ−1αρ3τ,
γ λ η γ λ η
and initial batch size B ≥⌈ σ2 ⌉, then after at most
init LF0αρ3
(cid:18) σ σ2/3 σ2/3 σ2/3 1 (cid:19)
T =O + + + + ℓF0 (15)
nε3 n2/3ε8/3 nα1/3ρ1/3ε8/3 nα1/3ρ2/3ε8/3 αρ3ε2
(cid:104) (cid:105)
iterations of Algorithm 2 it holds E ∥∇f(x )∥2 ≤ ε2, where x is chosen uniformly at random
out out
from {x¯ ,··· ,x¯ }, and O suppresses absolute constants and poly-logarithmic factors.
0 T−1
Proof. From Lemmas 16 and 21 we get
E(cid:2) ∥Mt+1−Mt∥2(cid:3) ≤λ2G(cid:101)t+2λ2nσ2+2ℓ2(3γ2CΩt +3γ2CΩt +3η2Ωt +3η2nΩt)
F 1 3 4 5
=2λ2nσ2+λ2G(cid:101)t+6Cγ2ℓ2Ωt +6Cγ2ℓ2Ωt +6η2ℓ2Ωt +6nη2ℓ2Ωt. (46)
1 3 4 5
From the above inequality (46) and Lemma 18 we get
E(cid:2) ∥Vt+1−Vt∥2(cid:3) ≤3γ2CΩt +3γ2CΩt
F 2 4
(cid:16) (cid:17)
+ 3 2λ2nσ2+λ2G(cid:101)t+6Cγ2ℓ2Ωt +6Cγ2ℓ2Ωt +6η2ℓ2Ωt +6nη2Ωt
1 3 4 5
=6λ2nσ2+3λ2G(cid:101)t+18Cγ2ℓ2Ωt +3Cγ2Ωt +18Cγ2ℓ2Ωt
1 2 3
+ (3Cγ2+18η2ℓ2)Ωt +18nη2ℓ2Ωt. (47)
4 5
From Lemmas 16 and 19 we get the following descent on Gˆt
Gˆt+1 ≤(1−λ)Gˆt+2λ2σ2n+ℓ2(3γ2CΩt +3γ2CΩt +3η2Ωt +3η2nΩt)
1 3 4 5
=2λ2σ2n+(1−λ)Gˆt+3Cγ2ℓ2Ωt +3Cγ2ℓ2Ωt +3η2ℓ2Ωt +3nη2ℓ2Ωt.
1 3 4 5
30Similarly, from Lemmas 16 and 20 we get the following descent on G(cid:101)t
G(cid:101)t+1 ≤2λ2σ2n+(1−λ)G(cid:101)t+3Cγ2ℓ2Ωt +3Cγ2ℓ2Ωt +3η2ℓ2Ωt +3nη2ℓ2Ωt.
1 3 4 5
From Lemmas 12 and 16 we get the following descent on Ωt
1
Ωt 1+1 ≤(1−α/2)E(cid:2) ∥Ht−Xt∥2 F(cid:3) + α2 (3γ2CΩt 1+3γ2CΩt 3+3η2Ωt 4+3η2nΩt 5)
6Cγ2 6η2 6nη2
=(1−α/2+6Cγ2/α)Ωt 1+
α
Ωt 3+
α
Ωt 4+
α
Ωt 5.
From Lemma 13 and (47) we get the following descent on Ωt
2
(cid:18)
2
Ωt 2+1 ≤(1−α/2)Ωt 2+
α
6λ2nσ2+3λ2G(cid:101)t+18Cγ2ℓ2Ωt 1+3Cγ2Ωt 2+18Cγ2ℓ2Ωt
3
(cid:19)
+ (3Cγ2+18Cη2ℓ2)Ωt +18nη2ℓ2Ωt.
4 5
12nλ2 6λ2 36Cγ2ℓ2 36Cγ2ℓ2
=
α
σ2+
α
G(cid:101)t+
α
Ωt 1+(1−α/2+6Cγ2/α)Ωt 2+
α
Ωt
3
2 36nη2ℓ2
+ (3Cγ2+18η2ℓ2)Ωt + Ωt.
α 4 α 5
The descent on Ωt (31) from the proof of MoTEF remains unchanged
3
γρ 6γC 6η2
Ωt+1 ≤(1− )Ωt + Ωt + Ωt.
3 2 3 ρ 1 γρ 4
From Lemma 15 and (46) we get the following descent on Ωt
4
Ωt 4+1 ≤(1−γρ/2)Ωt 4+2γ2C(1+2/γρ)Ωt
2
+ 2(1+2/γρ)(2λ2nσ2+λ2G(cid:101)t+6Cγ2ℓ2Ωt 1+6Cγ2ℓ2Ωt 3+6η2ℓ2Ωt 4+6nη2Ωt 5)
6nλ2 3λ2 18Cγℓ2 6Cγ 18Cγℓ2
≤
γρ
σ2+
γρ
G(cid:101)t+
ρ
Ωt 1+
ρ
Ωt 2+
ρ
Ωt 3+(1−γρ/2+18η2ℓ2/γρ)Ωt
4
18nη2ℓ2
+ Ωt.
γρ 5
31We remind that Ω=(Gˆt,G(cid:101)t,Ω,Ωt,Ωt,Ωt)⊤. Now we can gather all inequalities together
1 2 3 4
 1−λ 0 3Cγ2ℓ2 0 3Cγ2ℓ2 3η2ℓ2 
 0 1−λ 3Cγ2ℓ2 0 3Cγ2ℓ2 3η2ℓ2 
 
 0 0 1− α + 6Cγ2 0 6Cγ2 6η2 
Ωt+1 ≤   

0
0
6 α 0λ2 362 6C γαγ C2ℓ2α 1− α 2 0+ 6C αγ2 3 16C −α αγ2 γℓ ρ2 6C αγ2 + 6α η236η α2ℓ2     Ωt
 ρ 2 γρ 
0 3λ2 18Cγℓ2 6Cγ 18Cγℓ2 1− γρ + 18η2ℓ2
γρ ρ ρ ρ 2 γρ
(cid:124) (cid:123)(cid:122) (cid:125)
:=A
3nη2ℓ2  
2n
+        3 36n 6 nnη α α 0ηη2 22ℓ ℓ2 2       Ωt 5+      12 α0 02n n      λ2σ2. (48)
18nη2ℓ2 6n
γρ
γρ
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
:=b1
:=b2
Now we consider the following choice of stepsizes
c α2ρ6τ2 c αρ3τ
λ:= λ , γ :=c αρ, η := η ,
n γ ℓ
and constants
(cid:18)
d d d ℓ d d ℓ d
(cid:19)⊤
d:= 1 , 2, 3 , 4 , 5 , 6 ,
αρ3nτℓ nℓ ρ3nτ ρnℓ ρ3nτ ρnℓ
where
1 1 1
c = ,c = ,c = ,
λ 200 γ 200 η 100000
c =0.0020,c =0.000065,c =0.005,c =0.0000025,c =0.01,c =0.000005 (49)
1 2 3 4 5 6
Note that choosing τ ≤ 1 makes the system of inequalities (48) hold. Using this choice, we get the
following descent on Ψt =Ft+d⊤Ωt
Ψt+1 ≤ Ψt− c ηαρ3τ E(cid:2) ∥∇f(x¯t)∥2(cid:3) + c 1 ·2nc2α4ρ12n−2τ4σ2
2ℓ αρ3nτℓ λ
c τ
+ 2 ·2nc2α4ρ12n−2τ4σ2
nℓ λ
c 12n
+ 4 · c2α4ρ12n−2τ4σ2
ρnℓ α λ
c 6n
+ 6 · c2α4ρ12τ4n−2σ2
ρnℓ c αρ λ
γ
= Ψt− c ηαρ3 τE(cid:2) ∥∇f(x¯t)∥2(cid:3) + 2c 1c2 λα3ρ9τ3σ2
2ℓ n2ℓ
(cid:18) 2c c2α4ρ12 12c c2α3ρ11 6c c2α3ρ10(cid:19)
+ 2 λ + 4 λ + 6 λ τ4σ2. (50)
n2ℓ n3ℓ n3ℓ
32By this, we proved Lemma 4. Let us define constants
c αρ3
B := η ,
2ℓ
2c c2
C := 1 λα3ρ9,
n2ℓ
(cid:18) 2c c2α4ρ12 12c c2α3ρ11 6c c2α3ρ10(cid:19)
D := 2 λ + 4 λ + 6 λ ,
n2ℓ n3ℓ n3ℓ
E := 1.
Unrolling (50) for T iterations we get
1 T (cid:88)−1 E(cid:2) ∥∇f(x¯t)∥2(cid:3) ≤ Φ0 + C τ2σ2+ D τ3σ2.
T τBT B B
t=0
(cid:26) (cid:16) (cid:17)1/3 (cid:16) (cid:17)1/4(cid:27)
Choosing τ =min 1, Ψ0 , Ψ0 gives the following rate
E Cσ2T Dσ2T
1 T (cid:88)−1 E(cid:2) ∥∇f(x¯t)∥2(cid:3)
≤
EΨ0 +(cid:32)√ CΨ0σ(cid:33)2/3 +(cid:18) D1/3Ψ0σ2/3(cid:19)3/4
T BT B3/2T B4/3T
t=0
(cid:32) ℓΨ0 (cid:18) ℓΨ0σ(cid:19)2/3
= O +
αρ3T nT
(cid:18) (n−2/3+α−1/3ρ−1/3n−1+α−1/3ρ−2/3n−1)ℓΨ0σ2/3(cid:19)3/4(cid:33)
+ ,
T
that translates into the rate in terms of ε to
1 T (cid:88)−1 E(cid:2) ∥∇f(x¯t)∥2(cid:3) ≤ε2 ⇒ = O(cid:18) ℓΨ0 + ℓΨ0σ + ℓΨ0σ2/3 + ℓΨ0σ2/3 (cid:19)
T αρ3ε2 nε3 n2/3ε8/3 α1/3ρ1/3nε8/3
t=0
ℓΨ0σ2/3 (cid:19)
+ .
α1/3ρ2/3nε8/3
Note that with the choice V0 =G0 =M0 =∇(cid:101)F(X0),H0 =X0 =x01⊤, we get
Gˆ0 ≤σ2n, G(cid:101)0 ≤σ2n, Ω0 =Ω0 =Ω0 =Ω0 =0.
1 2 3 4
d d
Ψ0 ≤F0+ 1 σ2n+ 2σ2n. (51)
αρ3nτℓ nℓ
If we choose the initial batch size B ≥⌈ σ2 ⌉, we get
init LF0αρ3
1 σ2 1 σ2
Ψ0 ≤F0+ + ≤3F0. (52)
αρ3ℓB ℓB
init init
33C Experiment details
C.1 Effect of changing heterogeneity
We perform a grid search for the parameters γ from {0.1,0.01,0.001}, η from the log space from 10−4
to 10−1 and the log space from 5×10−4 to 5×10−1. For MoTEF we search the momentum parameter
λ from the same log space as η as well.
C.2 Robustness to communication topology.
Next,westudytheeffectofthenetworktopologyontheconvergenceofMoTEF.Wesetn=40,λ=0.05,
choose batch size 100, and run experiments for ring, star, grid, Erdös-Rènyi (p = 0.2 and p = 0.5)
topologies. For all topologies, we use η = 0.05,γ = 0.5,λ = 0.01 for a9a dataset and η = 0.05,γ =
0.5,λ = 0.01 for w8a. Note that the spectral gaps of these networks 0.012,0.049,0.063,0.467,0.755
correspondingly.
C.3 Hyperparameters for section 4.2
ForMoTEFwetunestepsizeasfollowsη ∈{0.001,0.01,0.05},γ ∈{0.1,0.2,0.5,0.9},λ∈{0.005,0.01,0.05,0.1}.
For BEER we tune the stepsizes in the range η ∈{0.001,0.01,0.05},γ ∈{0.1,0.2,0.5,0.9}. For Choco-
SGD we tune the stepsizes in the range η ∈{0.01,0.05},γ ∈{0.1,0.5,0.9}. Finally, for DSGD and D2
we choose the stepsize η =0.01.
34