Thisworkhasbeenacceptedforpublicationin2024InternationalSymposiumonWirelessCommunicationSystems(ISWCS)
Soft Partitioning of Latent Space for Semantic
Channel Equalization
Toma´s Huttebraucker, Mohamed Sana, Emilio Calvanese Strinati
CEA-Leti, Universite´ Grenoble Alpes, F-38000 Grenoble, France
Email : tomas.huttebraucker; mohamed.sana; emilio.calvanese-strinati @cea.fr
{ }
Abstract—Semantic channel equalization has emerged as a
solution to address language mismatch in multi-user semantic SourceLanguage
communications. This approach aims to align the latent spaces Environment o ∈O λs: O→− X xs∈X T:
of an encoder and a decoder which were not jointly trained X→− X
and it relies on a partition of the semantic (latent) space into
atoms based on the the semantic meaning. In this work we
exploretheroleofthesemanticspacepartitioninscenarioswhere P π(o ol ,i Tcy
)
xˆt∈X
the task structure involves a one-to-many mapping between
the semantic space and the action space. In such scenarios, Targetlanguage
partitioning based on hard inference results results in loss a ∈A γt: X→− A yˆt∈X Channel
of information which degrades the equalization performance. p(yx)
|
We propose a soft criterion to derive the atoms of the
partition which leverages the soft decoder’s output and offers
a more comprehensive understanding of the semantic space’s
structure. Through empirical validation, we demonstrate that
Fig. 1. Proposed communication scenario shared with [10]. A distributed
soft partitioning yields a more descriptive and regular partition
control problem is explored, where the language of the encoder does not
of the space, consequently enhancing the performance of the
matchthelanguageofthedecoder.Usingacodebookoftransformationsand
equalization algorithm. aselectionpolicy,semanticchannelequalizationisperformed.
I. INTRODUCTION
Semantic communication, as introduced by Weaver [1]
in its prelude to Shannon’s seminal paper [2], is a
solve a given task will likely dynamically change depending
paradigm where communication serves as a means to
on geographical factors and resource availability. In such
solve a given task rather than an end in itself. Semantic
cases, constantly re-learning a shared language proves to be
communication systems can significantly reduce overall
a resource-intensive endeavor, which is infeasible in networks
network rate requirements by extracting and transmitting only
characterized by constrained energy and bandwidth resources.
task-relevant information from the data. Recently, they have
Conversely, when the language between agents is not shared,
been identified as a key enabler of future communication
semantic mismatch arises, and task performance significantly
systems [3] [4] [5]. The main driver behind the recent
drops[8].Tosolvethis,SemanticChannelEqualization(SCE)
popularity of semantic communications is the success of
was introduced [9]. This framework models language as a
Artificial Intelligence (AI), particularly Machine Learning
partition of the communication space into multiple atoms,
(ML), for automatic task solving. ML can be utilized to
with each atom associated with a distinct semantic meaning.
learn a language (communication protocol) that enables
By doing so, SCE facilitates effective semantic translation
effectivecommunicationandcollaborationbetweenconnected
among different languages with a low complexity algorithm.
agents . These communication protocols are well-suited for
The efficacy of this approach in various domains, including
futuretechnologieslikesmartcitiesandautonomousvehicles,
image classification [9] and reinforcement learning scenarios
wheremultipleintelligentagentscommunicateandcollaborate
[10] was extensively shown. While previous studies showcase
to solve downstream tasks. Consequently, the design and
the empirical effectiveness of the SCE framework, they do
development of such protocols are of paramount importance.
not delve into the crucial role of the semantics captured by
The development of ML semantic protocols is an active
the languages.
area of research. Studies have shown that the bandwidth
or energy consumption of wireless networks can be greatly In this work, we aim to elucidate how different partitions
reduced while still successfully completing downstream tasks of the latent space capture diverse semantic meanings and
withoutanyperformanceloss[6][7].However,mostliterature how these variations impact the equalization algorithm. Our
assumes that the language between transmitter and receiver is primary contribution lies in introducing a novel methodology
shared as a result of a joint learning procedure, which might for partitioning the latent space, which captures richer
not hold in practical scenarios. In future networks, the set semanticmeaningandconsequentlyenhancestheperformance
of agents participating in communication and collaboration to of SCE.
4202
yaM
03
]GL.sc[
1v58002.5042:viXraII. SYSTEMMODEL atoms.Eachatomcorrespondstoadistinctsemanticmeaning,
with all semantic representations within an atom reflecting
Consider the distributed inference problem illustrated in
observations that share that particular meaning. We denote
Fig. 1. Here, an encoder λ: transforms observations
O →X the set of chosen atoms as a partition of the semantic space,
o from the environment into a semantic representation
∈ O represented as P = P ,P ,...,P , where P denotes the
x . This semantic representation x encapsulates the { 0 1 N } i
∈ X i-th atom of the partition.
relevant information present in o, which the decoder γ :
Different semantic space partitions can capture different
interprets to select an action a (we will
X → A ∈ A levels of semantics. For example, in an image classification
assume to be discrete). In this study, we focus on the
A task, each atom of the semantic space can be associated with
scenario where the communication between the encoder and
a label of the images, which is a high level description of
the decoder is through a noisy channel over multiple time
semantics. However, if the encoder is descriptive enough,
steps. It is worth noting that this general formulation includes
lower-level descriptions are also possible, and features such
image classification, which can be seen as a specialized case
as the colors or shapes can be captured with the appropriate
withonlyonetime-step.Followingtheterminologyintroduced
partitioning. As an example, see Fig. 2 where two different
in SCE [9], we refer to λ as the language generator and
partitions of the semantic space are shown for a generator
to γ as the language interpreter. Furthermore, we denote
trained to solve the MNIST classification task. Points in the
, , and as the observation space, semantic space, and
O X A latent (semantic) space are partitioned according to different
action space, respectively and we denote µ as the probability
o
criteria.Onepartitioncapturesthedigitinformationpresentin
distribution of observations. We assume that the language
the semantic symbols while other possible partition captures
(communication strategy) between λ and γ is a result of a
the parity of this symbols.
joint learning process. Through this process, the agents learn
Q(a,o), an approximation of the true action-value function
B. Language mismatch in multi-user communication
Q∗(a,o), which is an indicator of how “good” it is to play
When the language generator and the language interpreter
actionawhenoisobserved.Basedonthelearnedaction-value
are not trained jointly, it is unlikely that they employ the
function Q, the agents make decisions following a greedy
same language and a semantic mismatch arises even if
policy: γ(λ(o))=argmax Q(a,o).
a∈A the training procedures follow the same architectures, data
and objective functions [11]. In SCE [9], the semantic
0 2 4 6 8 mismatcharisingbetweenalanguagegeneratorandalanguage
1 3 5 7 9 Even Odd
interpreter was modeled as a misalignment of the atoms
Digit Partition Parity Partition of their corresponding partitions. More precisely, when the
2 2
source generator (transmitter) sends a message x = λ (o),
s s
1 1 it will not be interpreted correctly at the target interpreter
(receiver) if it does not fall in the corresponding target atom
0 0 of x t = λ t(o). We denote the source (λ s,γ s, s, s, )
X A O
and target (λ ,γ , , , ) languages which were trained
t t t t
X A O
1 1 independently on the same observation space and we will
explore the case where λ and γ communicate.
s t
2 2
C. Compensating for language mismatch via Semantic
2 1 0 1 2 2 1 0 1 2
Channel Equalization
To deal with the language mismatch, the SCE algorithm
Fig.2. Twopossiblepartitionsforthelatentspaceofalanguagegenerator. leverages a codebook of linear transformations between
The generator was trained with the task of MNIST classification. Different individual atoms andT a selection policy to operate the
ways to partition the semantic space depend on the criteria used (digit
codebook. For a source Ps = Ps,Ps,...,Ps and target
classificationordigitparityclassification) { 0 1 Js}
Pt = Pt,Pt,...,Pt partition of the semantic space, each
{ 0 1 Jt}
transformation T : is learned using optimal
s t
A. Semantic space partitioning and language effectiveness X → X ∈ T
transport and aims to maximize the transported volume for a
A language generator λ defines a method for encoding given pair of source and target atoms:
information into the semantic space. When the generator
µ (T (Ps) Pt)
o fep ae tr ua rt ee ss oe fffe ec at civ hely o, bsi et rve an tis ou nres
o
t ah ra et ath pe prota ps rk ia-r tee ll yeva en nt cod da et da ρ P is →−P jt(T)= Tλs µ Tλs(i P is∩ ) i . (1)
in the semantic representation x = λ(o). The resulting Here µ is the post-transformation distribution on the
Tλs
semantic space encapsulates all pertinent information in a semanticspace,whichdependsontheobservationdistribution
structured manner, where observations with shared semantic µ and, if λ and T are injective, can be written as µ =
o Tλs
characteristics are encoded similarly. This structure can be µ λ−1 T−1.Thecodebook hasasmanytransformations
o ◦ s ◦ T
exploitedtopartitionthespaceintomultiplesubspacestermed as the total pairs of source and target atoms, i.e. =J J .
s t
|T| ·Fig.3. Differentwaystopartitionthesemanticspacecapturedifferentsemantics.Whenusingtheharddecisionoutcometodefinethepartition,thestructure
ofthetaskandtherelationshipbetweenactionsislost.Whenusingthesoftdecisionvalues,allthetask-relevantinformationisexploitedforthepartition.
The operation policy selects a transformation T from the semanticspacegeneratedbyaMNISTclassificationmodel.If
codebook of transformation following theobjectiveistoclassifydigits,onlytheleftpartitioncaptures
(cid:34) (cid:35) the necessary semantics and aids the receiver effectively.
(cid:88)Js
(cid:88)
π =argmax µ (Ps o) ρ (T) (2) Conversely, if the task involves classifying data parity, both
sem
T∈T
λs i| P is →−P jt
partitionswillconveytherequiredsemantics.However,opting
i=1 j∈κ(i)
for a more detailed partition comes with a trade-off—a more
where µ is the source language distribution (which can be
λs intricate equalization algorithm due to the increased number
written as µ =µ λ−1 if λ is injective) on the semantic
λs o ◦ s s of atoms. This study aims to identify the optimal approach
space and κ(i) is a (problem dependent) mapping function
to partitioning the semantic space, considering the underlying
between source and target atoms. The policy π aims to
sem task structure.
perfectly align source and target atoms according to their
semantic meaning without regards to the downstream task
III. THEIMPACTOFSEMANTICSPACEPARTITIONINGON
performance. On [10], a new equalization policy which aims
EQUALIZATIONPERFORMANCE
to maximize downstream task performance was proposed as
(cid:34) (cid:88)Js (cid:88)Jt (cid:35) A. Hard partitioning
π =argmax µ (Ps o) ρ (T)Q (a ,o) .
eff
T∈T
λs i| P is →−P jt t j To partition the semantic space, previous work considers
i=1 j=1
hard partitioning [9], [10]. This approach defines an atom P
(3) i
of a partition as the set of semantic symbols (i.e., states being
Where Q (a,o) is the target language’s estimation of the true
t
mapped) in the semantic space that result in the same action
action-value function. Here, it is implicitly assumed that each
a :
target atom should correspond to a unique action, this is the i
∈A
assumption we challenge in this work. The policy π eff aims to P =(cid:8) x x=λ(o); (4)
maximizeperformanceratherthanperfectsemanticalignment, i ∈X |
(cid:9)
which is not required to complete the task. We call π sem γ(x)=a i =argmaxQ(a,o), ∀o ∈O
a∈A
and π the semantic and effectiveness equalization policies
eff
respectively. This approach is based on one assumption: there exists a
The effectiveness of SCE heavily relies on how the one-to-one relationship between the atoms of the partition
semantic space is partitioned. Essentially, this partitioning and the actions. Yet, in many control tasks, different possible
servesasameansofcompressingtheinformationintendedfor actions may exist for a given observed state, thus leading to
transmission. It groups distinct observations sharing the same action ambiguities. Action ambiguities can be detrimental to
semantic meaning into atoms. As SCE aligns these atoms, equalization performance when ignored. Indeed, when action
only the information captured by the partition is transmitted. ambiguity is present, hard partitioning assigns the semantic
Therefore, the selection of the language partition is a critical symbol x to the output of the decision γ(x), ignoring its
aspect of SCE as it determines the relevant information to truesemanticmeaning.Weshowlaterthatthisapproachleads
be conveyed. If the chosen partition isn’t suitable for the to irregular atom shapes, which are hard to equalize. As an
downstream task, the equalization process will likely fail. For alternative, we propose a soft-value based partitioning, which
instance, consider Fig. 2, showcasing two partitions of the leverages the action-values Q(,o) R|A|.
· ∈B. Proposed solution via soft-values based partitionning
On Fig. 3 the main idea behind soft values semantic space
partitioning is shown. Using the information present in the
soft decision process of the interpreter allows for a more
descriptive partition of the semantic space. Following the
figure, if hard decisions were to be used for partitioning, the
state will fall withing the semantic atom corresponding to the
the estimated optimal action. However, there are two actions
that lead to optimal behaviour. The learned Q values scores
capturethisbyassigninghighvaluetotheseactions,wherethe
difference between them is only due to noise in the learning
process. Using the information provided by Q, the partition
of the semantic space could be able to differentiate between
states in which only one action is optimal and states where
multiple actions are. Not only this is a better description of
Fig.4. Projectionoftheaction-valuespaceofdimensionna=4intothefirst
the semantic space, but also boosts the performance of the two data maximum variance directions for the source language. Each point
equalization algorithm, since, as we will show, the resulting correspondstoanobservation.Colorsareshownaccordingtotheactionthat
maximizedthevalueforeachobservation.
atoms will be more regular.
We propose then to build the partition on the action-value
space to build the atoms. For each observation o , we normalization technique, where a normalization constant τ is
c Ra |n A|r .ep Ur se is ne gnt st th ae ndf au rn dct cio lun stQ er( in·, go) tea cs hna iqv ue ec st ,or itin ist∈ h pe oO ss sp ia bc le
e
c do em nop tu ested theas nτ oi n-= norη m· aτ lii z− e1 d+ se( m1 a− ntiη c) | s|x y∗ i m|| b2 o. lH (e xr ie, =x∗ i
x∗
i∈ /τX
i)
to divide the action-value space and translate this into the chosen at training step i, with η = 0.1 representing the
semantic space. More precisely, using a clustering algorithm momentumvalue.Duringtesting,thevalueofτ isfixedasthe
C : R|A| 0,1,...,n 1 which maps each point in finalvalueobtainedduringtraining.Subsequently,thedecoder
c
the action
→ valu{
e space
R|A−
|
in}
to an index value indicating γ processes the noisy version of the transmitted symbol and
belongingtooneof n atoms,wecandefineapartitionofthe selects an action from = right, down, left, up , which the
c A { }
semantic space agent executes. An episode concludes either when the agent
reaches the treasure or when the maximum number of steps
P = {P 0,...,P nc−1
}
(5) (150) is attained. We operate under the assumption that the
encoder decoder pairs (λ ,γ ) and (λ ,γ ) are provided and
where each atom is constructed following s s t t
have each undergone a joint training in a centralized manner
P = λ(o)o and C(Q(,o))=i . (6) utilizingRLtechniques,allwhileadheringtothesamereward
i
{ | ∈O · }
signalandaSignaltoNoiseRatio(SNR)of5dB.Fortraining
The choice of the clustering algorithm is not simple and it
purposes, we employ Deep Q-Learning (DQN) [13], which
is most likely problem dependent. In our work we choose to
is well-suited for discrete action spaces. Additionally, we set
use the well known k-means algorithm [12]. This algorithm
= = and = = .
s t s t
requires to define the number of atoms n beforehand so we X X X A A A
c
test multiple atom numbers in our experiments. While we A. Semantic space partitioning
are aware of the limitations of the algorithm with respect We first explore the possible semantic space partitions for
to convergence, the need to define the number of clusters thelearnedlanguages.Weonlyshowtheresultsforthesource
beforehand and also the clustering criteria, we chose k-means language for lack of space. We first show that the information
both for its simplicity and popularity. Finding the optimal included in the action-value space is more descriptive of
clustering criteria is beyond the scope of this paper, our the task than the hard-decision clustering. On Fig. 4 the
objective is rather to show the influence of different latent projection of the four dimensional action-value space for the
space partition on SCE. source language is shown. It is clear that, even if the total
amount of actions is four, the action-value space hints that
IV. NUMERICALRESULTS
different partitions are possible. It is easy to see at least
We evaluate the proposed system using a language eightclusters,fourcorrespondingtouniqueactions(originated
generated through Reinforcement Learning (RL) techniques, from states where only one action is optimal) and four others
aiming to address the environment depicted in Fig. 1. In correspondingtopairsofactions(originatedfromstateswhere
this case, the observation space represents the state of a more than one action is optimal, as such shown on Fig. 3).
O
grid world featuring an agent and a treasure. The encoder λ On Fig. 5 we show the resulting partitions on the semantic
maps each observation into = R2. During training, the space when using k-means on the action-value space for
X
average output power of the encoder is standardized to 1. multiple choices of number of atoms n . In general, we can
c
Thisisachievedthroughtheimplementationofarollingmean observe that the resulting atoms are more regular in shapeRRiigghhtt DDoowwnn LLeefftt UUpp
4 atoms 6 atoms 8 atoms
2.0 2.0 2.0
5: Up 7: Right - Down
1.5 3: Right - Down1.5 1.5
6: Up
1.0 1.0 4: Down 1.0
5: Down
0.5 2: Left - Up 0.5 0.5
3: Left - Up 4: Left
0.0 0.0 0.0
2: Right - Down 3: Right - Up
0.5 1: Right - Up 0.5 0.5
2: Down - Left
1.0 1.0 1: Down - Left 1.0
1: Left - Up
1.5 0: Down - Left 1.5 1.5
0: Right - Up 0: Right
2.0 2.0 2.0
2 1 0 1 2 2 1 0 1 2 2 1 0 1 2
10 atoms 12 atoms 15 atoms
2.0 9: Right 2.0 11: Right - Up 2.0 14: Right - Up
1.5 8: Left 1.5 10: Right - Down1.5 11 23 :: DRi og wh nt - Up
1.0 7: Down - Left 1.0 9: Right - Down 1.0 11: Right - Down
8: Down - Left 10: Down - Left
0.5 6: Right - Down 0.5 7: Down 0.5 9: Left - Up
5: Left - Up 6: Right 8: Down
0.0 0.0 0.0 7: Right
4: Right - Down 5: Left - Up 6: Down
0.5 3: Down - Left 0.5 4: Left 0.5 5: Left
3: Right - Up 4: Up
1.0 2: Right - Up 1.0 2: Down - Left 1.0 3: Down - Left
1.5 1: Up 1.5 1: Down 1.5 12 :: LR eig fth -t U- U pp
2.0 0: Down 2.0 0: Up 2.0 0: Right - Down
2 1 0 1 2 2 1 0 1 2 2 1 0 1 2
Fig.5. Differentpartitionsofthesemanticspaceusingthek-meansalgorithmwithvaryingnumberofatoms.Theactions(hardpartitioning)arevisualized
asdifferentshapesforeachpartition.Thecolorofthepointscorrespondstoagivenatomandtheactionsassociatedwitheachatomareshownnexttoitin
thecolorlegend.
compared to the hard partitioning (which is indicated by
the shape of the plotted points). Different partitions capture sem - Hard Paritioning sem - Soft Partitioning - nc=10
sem - Soft Partitioning - nc=4 sem - Soft Partitioning - nc=12
different semantic descriptions of the task. In particular, for sem - Soft Partitioning - nc=6 sem - Soft Partitioning - nc=15
n c = 8 the eight atoms of the semantic space correspond sem - Soft Partitioning - nc=8
to the observed clusters in the action-value space on Fig. 4.
However, for four and six atoms, the semantic meaning for
102
someindividualactionsislost,aswewillshownext,thiswill
be detrimental for the performance of the equalization.
B. Performance of proposed solution
Leveraging the partitions of the semantic space shown on
Fig. 5, we implement both equalization policies π and π
sem eff
introduced by the SCE framework as described on Eq. (2) 101
and Eq. (3) respectively. When using k-means based soft
partitioning, we replace µ by the normalized inverse of the
λs
distances to the k-means centers and κ(i) as the index of the -20 -10 0 10 20 30 40 50
targetatomwhosecenterintheaction-valuespaceliesclosest SNR [dB]
totheonefromPs.Tocomputetheactionvalueofeachtarget
i
atom(equivalenttoQ
t
inEq.(3))weusetheaverageQ-value Fig. 6. Performance of the policy πsem as a function of SNR for hard
for the actions in it, i.e.
partitioningandsoftpartitioningwithdifferentnumbersofatomsnc.
1 (cid:88)
Q (Pt,o)= Q (γ (x ),o). (7)
t i Pt t t j
| i|x∈P it the equalization is worse than when using the four action
The results for π are shown on Fig. 6 and for π hard partitioning. To understand why, it suffices to look at
sem eff
on Fig. 7. The performance of both policies depends on the resulting partitions on Fig. 5. For the case of four and
the partition of the semantic space and we note that soft six atoms, all unique actions do not have a corresponding
partitioningisnotalwaysbeneficial.Forexample,whenusing atom, as shown by the atom labels. For four atoms, no unique
soft partitioning with four and six atoms, the performance of action is captured, and for six atoms, unique actions “left”
htgnel
edosipEjustifywhypartitioningthesemanticspaceaccordingtooutput
eff - Hard Paritioning eff - Soft Partitioning - nc=10 actions (hard partitioning) is sub-optimal in cases where there
eff - Soft Partitioning - nc=4 eff - Soft Partitioning - nc=12
areactionambiguities,i.e.multipleoptimalactions.Toaddress
eff - Soft Partitioning - nc=6 eff - Soft Partitioning - nc=15
eff - Soft Partitioning - nc=8 this problem, we propose to use soft partitioning, which
leverages the estimates action-values to define the semantic
102
atoms. We show that, using soft partitioning, the resulting
partition of the space is more regular, which improves the
equalization performance. Moreover, we show that richer
semanticsoftheproblemcanbecapturedbysoftpartitioning,
whichisapromisingresultthatcanbeappliedtomultitasking
equalization.
ACKNOWLEDGEMENTS
101
The present work was supported by the EU Horizon 2020
Marie Skłodowska-Curie ITN Greenedge (GA No. 953775),
by “6G-GOALS”, an EU-funded project, and by the French
-20 -10 0 10 20 30 40 50
projectfundedbytheprogram“PEPRNetworksoftheFuture”
SNR [dB]
of France 2030.
Fig. 7. Performance of the policy π eff as a function of SNR for hard REFERENCES
partitioningandsoftpartitioningwithdifferentnumbersofatomsnc.
[1] W. Weaver, “Recent contributions to the mathematical theory of
communication,” ETC: a review of general semantics, pp. 261–281,
and “right” are not captured. This degrades the performance, 1953.
[2] C. E. Shannon, W. Weaver, and N. Wiener, “The mathematical theory
as the equalization algorithm can not transmit the message of
ofcommunication,”PhysicsToday,vol.3,no.9,p.31,1950.
taking only a singular action, which in the particular task is [3] E.CalvaneseStrinatiandS.Barbarossa,“6Gnetworks:Beyondshannon
necessary to reach the goal. However, when the soft partition towards semantic and goal-oriented communications,” Computer
Networks,vol.190,p.107930,2021.
allows to capture all the relevant information of the task, the
[4] M. Kountouris and N. Pappas, “Semantics-empowered communication
performance is greatly improved. From our results, we can for networked intelligent systems,” IEEE Communications Magazine,
conclude that the best number of atoms for our problem is vol.59,no.6,pp.96–102,2021.
[5] E.C.Strinati,P.DiLorenzo,V.Sciancalepore,A.Aijaz,M.Kountouris,
eight, since it is the smallest number of atoms for which we
D.Gu¨ndu¨z,P.Popovski,M.Sana,P.A.Stavrou,B.Soret,etal.,“Goal-
obtain the best performance. This observation reinforces our OrientedandSemanticCommunicationin6GAI-NativeNetworks:The
intuitionontheoptimalnumberofatoms,inwhicheachatom 6G-GOALSApproach,”arXivpreprintarXiv:2402.07573,2024.
[6] E. Bourtsoulatze, D. B. Kurka, and D. Gu¨ndu¨z, “Deep joint source-
should be associated to all singular actions and all possible
channelcodingforwirelessimagetransmission,”IEEETransactionson
action ambiguities, which, for this particular example, gives CognitiveCommunicationsandNetworking,vol.5,no.3,pp.567–579,
us a total of eight atoms. 2019.
[7] T.-Y. Tung, S. Kobus, J. P. Roig, and D. Gu¨ndu¨z, “Effective
C. On the descriptiveness of soft partitioning for multi-task communications: A joint learning and communication framework for
multi-agentreinforcementlearningovernoisychannels,”IEEEJournal
equalization
on Selected Areas in Communications, vol. 39, no. 8, pp. 2590–2603,
Soft partitioning offers a more nuanced depiction of the 2021.
[8] M. Sana and E. C. Strinati, “Learning semantics: An opportunity for
semantic space, enabling a richer understanding of semantics
effective 6G communications,” in 2022 IEEE 19th Annual Consumer
thatcanbebeneficialformulti-taskequalization.Forinstance, Communications & Networking Conference (CCNC), pp. 631–636,
consider a scenario where the decoder is expanded to IEEE,2022.
[9] M. Sana and E. Calvanese Strinati, “Semantic Channel
accommodateeightactions,includingdiagonalmoves,instead
Equalizer: Modelling Language Mismatch in Multi-User Semantic
of the original four horizontal and vertical ones. In this case, Communications,” in GLOBECOM 2023 - 2023 IEEE Global
hard partitioning may hinder the encoder’s ability to transmit CommunicationsConference,pp.2221–2226,2023.
[10] T. Huttebraucker, M. Sana, and E. Calvanese Strinati, “Pragmatic
semantic meanings associated with diagonal actions. On the
Goal-OrientedCommunicationsUnderSemantic-EffectivenessChannel
other hand, soft partitioning the semantic space into eight Errors,” in 2024 IEEE 21st Consumer Communications & Networking
atoms (as illustrated in Fig. 5) could empower the decoder to Conference(CCNC),pp.683–689,2024.
[11] L. Moschella, V. Maiorca, M. Fumero, A. Norelli, F. Locatello, and
interpretatomscontaininginformationaboutdoubleactionsas
E. Rodola`, “Relative representations enable zero-shot latent space
diagonalmoves.Thisstraightforwardexampleunderscoresthe communication,”inTheEleventhInternationalConferenceonLearning
potential of soft clustering for multi-task equalization. Further Representations,2022.
[12] M.Ahmed,R.Seraj,andS.M.S.Islam,“Thek-meansAlgorithm:A
exploration of this aspect is left for future research.
ComprehensiveSurveyandPerformanceEvaluation,”Electronics,vol.9,
no.8,2020.
V. CONCLUSIONS
[13] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Motivated by the recent advancement on Semantic Channel Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
et al., “Human-level control through deep reinforcement learning,”
Equalization, in this work we address the role of the semantic nature,vol.518,no.7540,pp.529–533,2015.
space partitioning on the equalization performance. We first
htgnel
edosipE