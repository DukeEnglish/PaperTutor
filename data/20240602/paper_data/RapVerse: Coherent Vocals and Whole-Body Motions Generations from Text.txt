RapVerse: Coherent Vocals and Whole-Body
Motions Generations from Text
JiabenChen1∗ XinYan2 YihangChen3 SiyuanCen1 QinweiMa4
HaoyuZhen5 KaizhiQian6 LieLu7 ChuangGan1
Abstract
Inthiswork,weintroduceachallengingtaskforsimultaneouslygenerating3D
holisticbodymotionsandsingingvocalsdirectlyfromtextuallyricsinputs,advanc-
ingbeyondexistingworksthattypicallyaddressthesetwomodalitiesinisolation.
Tofacilitatethis,wefirstcollecttheRapVersedataset,alargedatasetcontaining
synchronousrappingvocals,lyrics,andhigh-quality3Dholisticbodymeshes.With
theRapVersedataset,weinvestigatetheextenttowhichscalingautoregressivemul-
timodaltransformersacrosslanguage,audio,andmotioncanenhancethecoherent
and realistic generation of vocals and whole-body human motions. For modal-
ityunification,avector-quantizedvariationalautoencoderisemployedtoencode
whole-bodymotionsequencesintodiscretemotiontokens,whileavocal-to-unit
modelisleveragedtoobtainquantizedaudiotokenspreservingcontent,prosodic
informationandsingeridentity. Byjointlyperformingtransformermodelingon
these three modalities in a unified way, our framework ensures a seamless and
realisticblendofvocalsandhumanmotions. Extensiveexperimentsdemonstrate
that our unified generation framework not only produces coherent and realistic
singingvocalsalongsidehumanmotionsdirectlyfromtextualinputs,butalsorivals
theperformanceofspecializedsingle-modalitygenerationsystems,establishing
newbenchmarksforjointvocal-motiongeneration. Theprojectpageisavailable
forresearchpurposesathttps://vis-www.cs.umass.edu/RapVerse.
1 Introduction
Intheevolvinglandscapeofmulti-modalcontentgenerationintermsofsoundandmotion,significant
strideshavebeenmadeinindividualmodalities,includingtext-to-music[54,1,21],text-to-vocal
[32],text-to-motion[13,69,4,23,34],andaudio-to-motion[68,15,31]generation. Thesedevelop-
mentshavepavedthewayforcreatingmoredynamicandinteractivedigitalcontent. Despitethese
advancements,existingworkspredominantlyoperateinsilos,addressingeachmodalityinisolation.
However,there’sstrongpsychologicalevidencethatforhumanbeings,thegenerationofsoundand
motionarehighlyrelatedandcoupled[28]. Aunifiedsystemforjointgenerationallowsforamore
expressiveandnuancedcommunicationofemotions,intentions,andcontext,wherethegenerationof
onemodalitycouldguideandassisttheotherinacoherentandefficientway.
Inthispaper,wetackleacrucialproblem: canamachinenotonlysingwithemotionaldepthbutalso
performwithhuman-likeexpressionsandmotions? Weproposeanoveltaskforgeneratingcoherent
singingvocalsandwhole-bodyhumanmotions(includingbodymotions,handgestures,andfacial
expressions)simultaneously,seeFig. 1. Thisendeavorholdspracticalsignificanceinfosteringmore
immersiveandnaturalisticdigitalinteractions,therebyelevatingvirtualperformances,interactive
gaming,andtherealismofvirtualavatars.
∗1UniversityofMassachusettsAmherst2WuhanUniversity3 ZhejiangUniversity4TsinghuaUniversity
5ShanghaiJiaoTongUniversity6MIT-IBMWatsonAILab7DolbyLaboratories
Preprint.Underreview.
4202
yaM
03
]VC.sc[
1v63302.5042:viXraGenerate a holistic body motionand the corresponding singing vocalsfrom the followinglyrics:
The way I sing on these tracksnow I been back now digging but broke for a minuteI been up since the evening
Generated holistic body motion:
Generated singing vocals:
Figure1: RapVerse. Wepresentaunifiedtext-conditionedmulti-modalitygenerationframework,for
jointlygeneratingholisticbodymotionsandsingingvocalsfromtextuallyricsinputsonly. Notethat
thecorrespondingvideoframesarejustshownforreference.
Animportantquestionnaturallyarises: whatconstitutesagoodmodelforunifiedgenerationofsound
andmotion? Firstly,weconsidertextuallyricsastheproperformofinputsfortheunifiedsystem,
sincetextprovidesahighlyexpressive,interpretableflexiblemeansofconveyinginformationby
human beings, and could serve as a bridge between various modalities. Previous efforts explore
scores[32],actioncommands[69,4,23],oraudiosignals[68]asinputs,whichareinferiortotextual
inputsintermsofsemanticrichness,expressivenessandflexibleintegrationofdifferentmodalities.
Secondly,wereckonthatajointgenerationsystemthatcouldproducemulti-modaloutputssimulta-
neouslyisbetterthanacascadedsystemthatexecutesthesingle-modalgenerationsequentially. A
cascadedsystem,combiningatext-to-vocalmodulewithavocal-to-motionmodule,risksaccumu-
latingerrorsacrosseachstageofgeneration. Forinstance,amisinterpretationinthetext-to-vocal
phase can lead to inaccurate motion generation, thereby diluting the intended coherence of the
output. Furthermore,cascadedarchitecturesnecessitatemultipletrainingandinferencephasesacross
differentmodels,substantiallyincreasingcomputationaldemands.
Tobuildsuchajointgenerationsystem,theprimarychallengesinclude:1)thescarcityofdatasetsthat
providelyrics,vocals,and3Dwhole-bodymotionannotationssimultaneously;and2)theneedfora
unifiedarchitecturecapableofcoherentlysynthesizingvocalsandmotionsfromtext. Inresponse
to these challenges, we have curated RapVerse, a large-scale dataset featuring a comprehensive
collectionoflyrics,singingvocals,and3Dwhole-bodymotions. Despitetheexistenceofdatasets
available for text-to-vocal [32, 22, 8, 55], text-to-motion [44, 35, 13, 30], and audio-to-motion
[3,15,12,9,5,65],thelandscapelacksaunifieddatasetthatencapsulatessingingvocals,whole-
body motion, and lyrics simultaneously. Most notably, large text-to-vocal datasets [22, 70] are
predominantly in Chinese, limiting their applicability for English language research and lacking
anymotiondata. Andtext-to-motiondatasets[44,13,30]typicallyfocusontextdescriptionsof
specificactionspairedwithcorrespondingmotionswithoutaudiodata,oftennotcoveringwhole
bodymovements. Moreover,audio-to-motiondatasets[32,33]focusprimarilyonspeechratherthan
singing. AcomparisonofexistingrelateddatasetsisdemonstratedinTable. 1. TheRapVersedataset
isdividedintotwodistinctivepartstocatertoabroadrangeofresearchneeds: 1)aRap-Vocalsubset
containingalargenumberofpairsofvocalsandlyrics,and2)aRap-Motionsubsetencompassing
vocals, lyrics, and human motions. The Rap-Vocal subset contains 108.44 hours of high-quality
Englishsingingvoiceintherapgenrewithoutbackgroundmusic.Pairedlyricsandvocalsarecrawled
fromtheInternetfrom32singers, withcarefulcleaningandpost-processing. Ontheotherhand,
theRap-Motionsubsetcontains26.8hoursofrapperformancevideoswith3Dholisticbodymesh
annotationsinSMPL-Xparameters[42]usingtheannotationpipelineofMotion-X[30],synchronous
singingvocalsandcorrespondinglyrics.
With the RapVerse dataset, we explore how far we can push by simply scaling autoregressive
multimodaltransformerswithlanguage,audio,andmotionforacoherentandrealisticgeneration
of vocals and whole-body human motions. To this end, we unify different modalities as token
2representations. Specifically, three VQVAEs [63] are utilized to compress whole-body motion
sequencesintothree-leveldiscretetokensforhead,body,andhand,respectively.Forvocalgeneration,
previous works [37, 7, 32, 37] share a common paradigm, producing mel-spectrograms of audio
signalsfrominputtextualfeaturesandadditionalmusicscoreinformation,followingwithavocoder
[40, 62, 67] to reconstruct the phase. We draw inspiration from the speech resynthesis domain
[45],andlearnaself-superviseddiscreterepresentationtoquantizerawaudiosignalintodiscrete
tokenswhilepreservingthevocalcontentandprosodicinformation. Then, withalltheinputsin
discreterepresentations,weleverageatransformertopredictthediscretecodesofaudioandmotion
inanautoregressivefashion. Extensiveexperimentsdemonstratethatthisstraightforwardunified
generationframeworknotonlyproducesrealisticsingingvocalsalongsidehumanmotionsdirectly
fromtextualinputsbutalsorivalstheperformanceofspecializedsingle-modalitygenerationsystems.
Tosumup,thispapermakesthefollowingcontributions:
•WereleaseRapVerse,alargedatasetfeaturingsynchronoussingingvocals,lyrics,andhigh-quality
3DholisticSMPL-Xparameters.
•Wedesignasimplebuteffectiveunifiedframeworkforthejointgenerationofsingingvocalsand
humanmotionsfromtextwithamulti-modaltransformerinanautoregressivefashion.
•Tounifyrepresentationsofdifferentmodalities,weemployavocal-to-unitmodeltoobtainquantized
audiotokensandutilizecompositionalVQVAEstogetdiscretemotiontokens.
•Experimentalresultsshowthatourframeworkrivalstheperformanceofspecializedsingle-modality
generationsystems,settingnewbenchmarksforjointgenerationofvocalsandmotion.
2 RelatedWork
2.1 TexttoVocalGeneration
Text-to-audioDataset. Thereexistsseveralsingingvocaldatasets,yettheyeachhasconstraints. For
instance,PopCS[32]andOpenSinger[22]arelimitedtoChinese,whileNUS-48E[8]andNHSS
[55]haveonlyafewhoursofsongs. Nevertheless,JVS-MuSiC[58]andNUS-48E[8]offerafew
hoursofsongsfromdozensofsingers,whereasOpenSinger[22]providesamoreextensivecollection
withtensofhoursfromasinglesinger. Notably,ourdatasetrepresentsthefirsttobespecifically
curatedforrapsongsofmultiplesingerswith108hours.
Text-to-vocalModels. Recentadvancementsintext-to-speech(TTS)models,includingWavNet[40],
FastSpeech1and2[52,51],andEATS[7],havesignificantlyimprovedthequalityofsynthesized
speech. However, singing voice synthesis (SVS) presents a greater challenge due to its reliance
on additional musical scores and lyrics. Recent generation models [32, 16, 16, 74, 26] perform
excellently in generating singing voices. However, their performance tends to deteriorate when
encounteringout-of-distributiondata. Tohandlethis,StyleSinger[73]introducesaResidualStyle
AdaptorandanUncertaintyModelingLayerNormalizationtohandlethis.
2.2 TexttoMotionGeneration
Text-to-motionDataset. Currenttext-motiondatasets, suchasKIT[44], AMASS[35], andHu-
manML3D[13],areconstrainedbylimiteddatacoverage,typicallyspanningonlytensofhoursand
lackingwhole-bodymotionrepresentation. Toovercometheseshortcomings,Motion-X[30]was
developed,providinganextensivedatasetthatencompasseswhole-bodymotions.
Text-to-motion Models. Text-to-motion models [43, 71, 14, 13, 34, 4] have gained popularity
for their user convenience. Recent advancements have focused on diffusion models [71, 25, 61],
which, unlike deterministic generation models [2, 11], enable finer-grained and diverse output
generation. Chenetal. [4]proposedamotionlatent-baseddiffusionmodeltoenhancegenerative
qualityandreducecomputationalcosts. However,themisalignmentbetweennaturallanguageand
humanmotionspresentschallenges. Luetal. [34]introducedtheText-alignedWhole-bodyMotion
generationframeworktoaddresstheseissuesbygeneratinghigh-quality,diverse,andcoherentfacial
expressions,handgestures,andbodymotionssimultaneously.
2.3 AudiotoMotionGeneration
Audio-to-motionDataset. Co-SpeechDatasetscanbeclassifiedinto1: pseudo-labeled(PGT)and2:
motion-captured(mocap). PGTdatasets[3,15,12,68]utilizedisconnected2Dor3Dkeypointsto
3Dataset Language #Singers Hours Dataset Modality Whole† Hours
NUS-48E[8] English 12 1.91 KIT-ML[44] Text ✗ 11.2
NHSS[55] English 10 7 AMASS[35] Text ✗ 40.0
JVS-MuSiC[58] Japanese 100 2.28 HumanML3D[13] Text ✗ 28.6
TohokuKiritan[39] Japanese 1 1 Motion-X[30] Text ✓ 127.1
PopCS[32] Chinese 1 5.89 HA2G[32] Audio ✗ 33
OpenSinger[22] Chinese 66 50 Yoonet.al[33] Audio ✗ 30
Opencpop[64] Chinese 1 5.25 Speech2Gesture[12] Audio ✗ 144
M4Singer[70] Chinese 20 29.77 Talkshow[68] Audio ✓ 27.0
Ours English 32 108.44 Ours Text-Audio ✓ 26.8
(a) AudioDatasets (b) MotionDatasets
Table1: ComparisonofAudioandMotionDatasets. (a)comparesourRap-VocalSubsetwith
existingsingingvocaldatasets. (b)comparesourRap-MotionSubsetwithexistinghumanmotion
datasets. †: “Whole”meanswhole-bodymotionincludingface,body,andhand.
representthebody,allowingextractingatalowercostbutwithlimitedaccuracy. Ontheotherhand,
mocapdatasetsprovideannotationsforlimitedbodyparts,withsomefocusingsolelyonthehead
[9,5,65]orthebody[57,10]. Differingfromthem,BEATX[31]containsmeshdataofbothhead
andbody. However,thesedatasetstypicallyfocusonspeechtomotiongeneration. OurRapVerse,in
contrast,containspairedtext-vocal-motiondata,enablingsimultaneousmotionandvocalgeneration.
Audio-to-motion Models. Audio-to-motion models [59, 56, 72, 31, 68] aim to produce human
motionfromaudioinnputs. Recognizingtheintricaterelationshipbetweenaudioandhumanface,
TalkSHOW[68]separatelygeneratesfacepartsandotherbodyparts. However,thisapproachexhibits
limitations,suchastheabsenceofgeneratinglowerbodyandglobalmotion. EMAGE[31]utilizes
maskedgesturestogetherwithvocalsforageneration. Ourmodel,however,goesastepfurtherby
beingthefirsttogeneratepairedaudio-motiondatadirectlyfromtext.
Rap-Vocal Subset
Data Audio + Lyrics Vocal and Vocal + Lyrics Vocal Data Paired
Crawling Background Music Process Vocal&Lyrics
Separation
Rap-Motion Subset
Audio Audio Data Vocal Clips Lyric
Process Generation
Synchronized
Data Vocal&Lyrics
Crawling & Motion clips
Video Data Motion
Videos Process Video Clips Annotation
Figure2: RapVersedatasetcollectionpipeline. Therearetwopathwaysforrap-vocalandrap-motion
subsets,respectively.
3 RapVerseDataset
Inthissection,weintroduceRapVerse,alargerapmusicmotiondatasetcontainingsynchronized
singingvocals,textuallyricsandwhole-bodyhumanmotions. Acomparisonofourdatasetwith
existingdatasetsisshownatTable. 1. TheRapVersedatasetisdividedintotwosubsetstocatertoa
broadrangeofresearchneeds: aRap-VocalsubsetandaRap-Motionsubset. Theoverallcollection
pipelineofRapVerseisshownatFig. 2.
3.1 Rap-VocalSubset
TheRap-Vocalsubsetcontains108.44hoursofhigh-qualityEnglishsingingvoiceintherapgenre
withpairedlyrics. Wewillintroduceeachstepindetail.
DataCrawling. Inabidtoobtainalargenumberofrapsongsandcorrespondinglyricsfromthe
Internet,weutilizeSpotdlandSpotipytocollectsongs,lyrics,andmetadataofdifferentrapsingers.
Toensurethequalityofthedataset,weperformcleaningonthecrawledsongsbyremovingsongs
withmisalignedlyricsandfilteringoutsongsthataretoolongortooshort.
4VocalandBackgroundMusicSeparation. Sincethecrawledsongsaremixedwithrappingvocals
andbackgroundmusic,andweaimtosynthesizesingingvocalsfromseparatedcleandata,weutilize
Spleeter[18],thestate-of-the-artopen-sourcevocal-musicsourceseparationtooltoseparateand
extractrapvocalvoicesandaccompanyingbackgroundmusicfromthecollectedsongs. Following
[53],wenormalizetheloudnessofthevocalvoicestoafixedloudnesslevel.
VocalDataProcessing. TherawcrawledlyricsfromtheInternetareininconsistentformats,we
conductdatacleaningonthelyricsbyremovingmetainformation(singer,composer,songname,
bridgingwords,andspecialsymbols). Toensurethatthelyricsarealignedwiththesingingvocals,
wecollectlyricsonlywiththecorrecttimestampsofeachsentence,andweseparateeachsonginto
around10-secondto20-secondsegmentsformodeltraining.
3.2 Rap-MotionSubset
TheRap-Motionsubsetcontains26.8hoursofrapperformancevideoswith3Dholisticbodymesh
annotationsinSMPL-Xparameters[42],synchronoussingingvocals,andcorrespondinglyrics. We
introducethecollectionpipelineofthissubsetasfollows.
DataCrawling. Wecrawledover1000studioperformancevideosfromYouTubeundertheCommon
CreativeLicense. Wefilteroutlow-qualityvideosmanuallytoensurethevideosmeetthefollowing
criteria: stablecamerawork,performerscenteredintheframe,clearvisibilityoftheperformer’s
entirebodytocapturedetailedmotiondata,andhigh-qualityaudioforaccuratevocalanalysis.
AudioDataProcessing. SimilartotheRap-Vocalsubset,weleverageSpleeter[19]toisolatesinging
vocalsfromaccompanyingmusic. GiventhatYouTubevideostypicallylackpairedlyrics,weemploy
anASRmodel,Whisper[47],toaccuratelytranscribevocalsintocorrespondingtext.
VideoDataProcessing. Toensurethecollectionofhigh-qualityvideoclipsformotionannotation,
weimplementedasemi-automaticprocesstofilteroutundesirablecontent,suchasadvertisements,
transitionframes,changesinshots,andflashinglights. Initially,weappliedYOLO[50]forhuman
detectiontodiscardframeswherenohumansweredetected. Subsequently,weutilizedRAFT[60]to
assessthemotionmagnitude,employingathresholdtoeliminateframesaffectedbycamerainstability.
Wethenperformmeticulousmanualcurationontheextractedclips,retainingonlythoseofthehighest
quality. Finally,wefollowthepipelineoftheoptimized-basedmethodMotion-X[30]toextract3D
whole-bodymeshesfrommonocularvideos. Specifically,weadopttheSMPL-X[42]formotion
representations,givenaT-framevideoclip,thecorrespondingposestatesMarerepresentedas:
M={M ,M ,M ,ζ,ϵ} (1)
f b h
where M ∈ RT×3,M ∈ RT×63 and M ∈ RT×90 denotes jaw poses, body poses and hand
f b h
poses,respectively. ζ ∈RT×100andϵ∈RT×3arethefacialexpressionandglobaltranslation.
4 Method
Givenapieceoflyricstext,ourgoalistogeneraterap-stylevocalsandwhole-bodymotions,including
bodymovements,handgestures,andfacialexpressionsthatresonatewiththelyrics. Withthehelp
ofourRapVersedataset,weproposeanovelframeworkthatnotonlyrepresentstexts,vocals,and
motionsasunifiedtokenformsbutalsointegratestokenmodelinginaunifiedmodel. Asillustrated
inFig.3,ourmodelconsistsofmultipletokenizersformotion(Sec.4.2)andvocal(Sec.4.3)token
conversions,aswellasageneralLargeText-Motion-AudioFoundationModel(Sec.4.4)thattargets
foraudiotokensynthesizeandmotiontokencreation,basedonraplyrics.
4.1 ProblemFormulation
LetLbetheraplyric, suchas“Igoingtomakeitaflashbacksetapicturefromwayback.”, our
modelwillcomposethetext-relatedvocalV ∈Rt×dv andwhole-bodymotionM∈Rt×dm,wheret,
d ,d denotethetemporallengthandthefeaturedimensionsforvocalandmotionineverytimeunit,
v m
respectively. ModalitytokenizerencodersϕX(T | X)anddecodersφX(X | T)willbridgeeach
modalitywiththetokenformatT utilizingdifferenttokenizers,whereX ∈{L,V,M}. Thelarge
foundationmodelψX willincorporatetokensinunifiedmodeling. Thewholeproblemofcoherent
vocalandmodelgenerationvialyrictextcanbeformulatedasfollows:
Θ∗ =argmaxP (V,M|L), (2)
Θ
Θ
5Motion
Vocoder Synthesized Decoder
Motion
Synthesized
Vocal
MultimodalityTokensDecoupling
LargeText-Motion-Audio Foundation Model
T5Encoder
Multimodality TokensOrganizing
Body Code
Texttokens Hubert Code Face Code
Pitch Code Hand Code
Vocal Body/Face/Hand
T5Tokenizer
Tokenizer Encoder
Body Face Hand
I going to make it a flashback
set a picture from way back
TextInput: Lyric Vocal Audio Input Whole-Body Motion Sequence
Figure3: Pipelineoverview. Wefirstpre-trainalltokenizersonvocal-onlyandmotion-onlydata.
Afterwehavepretrainedthemodalitytokenizers,wecanunifytext,vocal,andmotioninthesame
tokenspace. Weadoptamixingorganizingalgorithmforinputtokenstoalignviathetemporalaxis.
ThesemixedinputtokensarefedintothelargeText-Motion-Audiofoundationmodeltotrainon
tokenpredictiontasks,guidedbytheencodedfeaturesfromtextualinput.
whereΘdenotesthewholemodelparametersandP (·)denotesthemodeldistributions.
Θ
4.2 MotionVQ-VAETokenizer
Similartopriorarts,amotionVQ-VAEisadoptedtoconvert3Dhumanmotionsintodiscretetokens,
wherethetokenizerencoderϕgeneratesmotiontokensenrichedwithhigh-frequencyinformation,
while the decoder φ seamlessly reconstructs discrete tokens into continuous motion sequences.
Specifically, the tokenizer will maintain a learnable codebook C, which firstly encodes the input
motion features M to vectors zM, and then the motion quantizer QM will look up the nearest
neighborincodebookCM = {cM}dc forthemotionrepresentation. Thequantizedcodecanbe
k k=1
calculatedas
Q(zM;CM)=argmin∥z−cM∥ . (3)
k 2
k
Thetokenizerdecoderφwillprojectthequantizedcodebackintothehumanmotionafterquantization.
Thereconstructionobjectiveinthetokenizertrainingstagecanberepresentedas
E [logφ (M|T)]. (4)
T∼ϕM(T|M) M
Asthewhole-bodymotionMcanbedecomposedintofaceM ,bodyM ,andhandM ,where
f b h
M∈Rt×d,d +d +d =d ,webuildthreetokenizerstoobtaintokensforthesethreeparts.
f b h m
4.3 Vocal2unitAudioTokenizer
Overall,weleveragetheself-supervisedframework[45]inspeechresynthesisdomaintolearnvocal
representationsfromtheaudiosequences. Specifically,wetrainaVocal2unitaudiotokenizertobuild
adiscretetokenizedrepresentationforthehumansingingvoice. Thevocaltokenizerconsistsofthree
encodersandavocoder. Theencodersincludethreedifferentparts: (1)thesemanticencoder;(2)the
F0encoder;and(3)thesingerencoder. Wewillintroduceeachcomponentofthemodelseparately.
SemanticEncoder. Inordertoextractthesemanticinformationoftheaudiointodiscretetokens,we
useapre-trainedHubertencoder[20]asthemaincomponentofoursemanticencodernetworkEh.
Givenarawaudiosequencev,theoutputoftheHubertencoderiscontinuousrepresentation. Then
weapplyaK-meansalgorithmontherepresentationstofinallygeneratediscreteunitszS ={zS}Ls
i i=1
wherezS ∈{1,2,··· ,K}. L isthelengthofhuberttokensequence,andK representsthenumber
i s
ofclustersinK-meansalgorithm.
6F0Encoder. WeusetheYAAPTalgorithm[24]toextracttheF0fromtheinputaudiosignalv,which
isthenfedintoanencoderE togeneratedlow-frequencyF0pitchrepresentation. TheencoderEp
p
is trained using a VQ-VAE framework with Exponential Moving Average updates similar to [6],
maintainingalearnablecodebookCp ={cp}dc ,whered isthenumberofcodesinthecodebook.
k k=1 c
TherawF0sequenceisfirstpreprocessedaccordingtothesinger,convertedintoasequenceoflatent
vectors,thenmappedtoitsnearestneighborsinthecodebookcp,whichcouldberepresentedusing
integernumberk ∈{1,2,··· ,d }asitsindexinthecodebook. Theindexsequencewillbefurther
c
formatted as the token <pitch_k> to produce pitch token sequence zP = {zP}Lp , where L is
i i=1 p
thelengthofthepitchtokensequence. Finally,thedecoderwillreconstructtheF0representation
leveragingthepitchtokensequence.
SingerEncoder. Asthelastcomponentofourencoders,thesingerencoderE from[17,45]isused
si
toextracttheMel-spectrogramfromtherawaudiosequenceandtooutputasingerrepresentation
zI ∈R256. Thesingerembeddingonlydependsonthesingerandistrainedglobally.
Vocoder. Inordertodecodethevocalsignalfromthediscretetokens,weadoptamodifiedversion
oftheHiFi-GANneuralvocodersimilartotheonein[45],consistingofageneratorandmultiple
discriminators. ThegeneratortakestheencodeddiscreterepresentationzS andzP,andthesinger
embeddingvectorzI asinputs. Afteralookuptableandaseriesofblockscomposedoftransposed
convolution and a residual block with dilated layers, it finally outputs waveform audio. The dis-
criminatorsincludemultiplesub-discriminators,analyzingondifferentscalesanddifferentperiodic
structures.
4.4 GeneralAuto-regressiveModeling
ModelArchitectures. Thewholemodelofourgeneralmodelingpipelineconsistsofseveralpre-
trainedvocalandmotiontokenizersandalargeText-Motion-Audiofoundationmodel. Thevocaland
motiontokenizersarebasedonVQ-VAEarchitectures,enablingustorepresentthesetwomodalities
asdiscretetokens. WealsoadoptaT5-Tokenizer[48]toconvertlyrictextintotokens,sothatall
threemodalitiesX ∈ {L,V,M}areunitedinthetokenspaceTX = {tX}NX,whereNX isthe
k k=1
tokensequencelengthforeachmodality. Thelargefoundationmodel,atemporaltransformerbased
onthedecoder-onlyarchitecture,performsthenext-token-predictiontaskbasedontextualfeatures
encodedbytheT5-Encoder.
Specifically, eachtokenisgeneratedafterthestarttokenbytheprobabilitydistributionfromthe
foundation model p (tX;hL) = (cid:81) p (tX | tX ;h ). During the training period, we calculate
δ δ i <i L
i
thecross-entropylosstomeasurethelikelihoodbetweengroundtruthtokensandpredictedtoken
probabilitiesas
NX
(cid:88)
− logp (tX;hL). (5)
δ i
i=1
Afteroptimizingviathistrainingobjective,ourmodellearnstopredictthenexttoken,whichcanbe
decodedintodifferentmodalityfeatures. Thisprocessissimilartotextwordgenerationinlanguage
models, while the “word” in our method such as <face_0123>, does not have explicit semantic
information,butcanbedecodedintocontinuousmodalityfeatures.
MultimodalityTokensOrganization. Sincewehavedesignedacoherentvocalandmotiongen-
eration pipeline with multiple token types in our method, including hubert and pitch tokens in
rap vocals, and face, body, hand in whole-body motions, the organization method of all these
tokens in one sentence is important for training. Similar to previous works [34, 23], we adopt
aninterleavedstylewithineachmodality. Specifically,withtheinputwhole-bodymotiontokens
TMs ={tMs}NMs,wheres=f,b,hwhichdenotesface,bodyandhandrespectively,andNMs
k k=1
denotes the token length for each part. Therefore the motion token sequence can be formulated
asTM = {tMf,tMb,tMh,tMf,...}. Similarly,thevocaltokensconsistofthehuberttokenstVh
1 1 1 2
andpitchtokenstVp,whichcanbeformulatedasTV = {tVh,tVp,tVh,tVp,...}. Thisinterleaved
1 1 2 2
wayoforganizationallowsustoalignthetimestepsofeachpart. Whenperformingcoherentvocal
andmotiongeneration,wemixthesetwomodalitysequencessequentiallywithmodality-wisestart
tokenasT ={tV ,TV,tM ,TM}. Notethatweputvocalsequencesbeforemotionsequences
start start
because1)thevocalsaremorerelatedtothelyrics,and2)motiontokenshavearelationshipwith
7Method FID↓ DIV↑ BC↑ MSE↓ LVD↓
Text-to-Motion
T2M-GPT[69] 18.42 11.75 - - -
Method MOS↑†
MLD[4] 24.52 12.15 - - -
GT 4.45±0.06
Audio-to-Motion
Reconstruction 4.02±0.08
Habibieetal.[15] 32.14 10.08 0.476 2.11 9.54
Talkshow[68] 21.10 13.14 0.487 1.93 9.20 FastSpeech2[51] 3.41±0.18
DiffSinger[32] 3.72±0.12
CascadedResult 23.42 12.87 0.479 2.05 9.38
Ours 3.64±0.15
Text-to-Audio+Motion
Ours 18.65 12.19 0.481 2.03 7.23 (b) VocalGeneration
(a) MotionGeneration
Table2: Quantitativeresultsofgeneratedmotionandvocal. (a)compareswithdifferentmotion
generationbaselines,bestresultisshowninboldandthesecond-bestresultisshowninunderline.
(b)compareswithtext-to-vocalgenerationbaselines. †: TheMOSiscalculatedwith95%confidence
intervalsofsongsamples.
previouslygeneratedvocals, e.g. facemotionswithlipsmovementaredirectlyrelatedtovocals.
Jointtrainingofmixedmodalitiesallowsthetransformertocapturethetemporaldependenciesand
dynamicsinherentinvocalandmotiondata.
InferenceandDecoupling. Intheinferencestage,weusedifferentstarttokenstospecifywhich
modalitytogenerate. Thetextualinputisencodedasfeaturestoguidetokeninference. Wealsoadopt
atop-kalgorithmtocontrolthediversityofthegeneratedcontentbyadjustingthetemperature,as
generatingvocalsandmotionsbasedonlyricsisacreationprocesswithmultiplepossibleanswers.
Aftertokenprediction,adecouplingalgorithmisusedtoprocessoutputtokenstomakesuretokens
fromdifferentmodalitiesareseparatedandtemporallyaligned. Thesediscretetokenswillbefurther
decodedintotext-alignedvocalsandmotions.
5 Experiments
Inthissection,weevaluateourproposedmodelonourproposedbenchmarkdesignedforjointvocal
andwhole-bodymotiongenerationfromtextualinputs.
5.1 ExperimentalSetup
Metrics. Toevaluatethegenerationqualityofsingingvocals,weutilizetheMeanOpinionScore
(MOS) to gauge the naturalness of the synthesized vocal. For motion synthesis, we evaluate the
generationqualityofthebodyhandgesturesandtherealismoftheface,respectively. Specifically,for
gesturegeneration,weuseFrechetInceptionDistance(FID)basedonafeatureextractorfrom[13]to
evaluatethedistanceoffeaturedistributionsbetweenthegeneratedandrealmotions,andDiversity
(DIV)metrictoassessthemotionsdiversity. Forfacegeneration,wecomparethevertexMSE[66]
andthevertexL1differenceLVD[68]. Finally,weadoptBeatConstancy(BC)[29]tomeasurethe
synchronyofgeneratedmotionandsingingvocals.
Baselines. Wecomparethevocalgenerationqualitywiththestate-of-the-artvocalgenerationmethod
DiffSinger[32]. Andwealsoadaptthetext-to-speechmodelFastSpeech2[51]forvocalgeneration.
For motion generation, we compare our method with both text-to-motion methods and audio-to-
motionmethods. Fortext-to-motionmethods,sincethereisnoexistingopen-sourcedworkfortext
towhole-bodymotiongeneration,wecomparewithtransformer-basedT2M-GPT[69]andMLD
[4]forbodygeneration. Fortheaudio-to-motiongeneration,wecomparewithHabibieetal. [15]
andtheSOTAmodelTalkshow[68]. WereportalltheresultsonRapVersewithan85%/7.5%/7.5%
train/val/testsplit.
5.2 MainResultsAnalysis
Evaluationsonjointvocalandwhole-bodymotiongenerations. Wecomparedbothtext-driven
andaudio-drivenmotiongenerationbaselinesinTable. 2(a). Tobenoted,oursettingisdifferent
fromallexistingmethodsinthefollowingways. First,weuseraplyricsasourtextualinputinstead
ofmotiontextualdescriptions,whichcontaindirectactionpromptwords,suchaswalkandjump;
8Secondweusetexttojointlygeneratebothaudioandmotion,insteadofusingaudiotogenerate
motionasaudio-drivenmethodsdid. Asisdemonstrated,ourmodelrivalswithbothtext-to-motion
andaudio-to-motionmethodsintermsofmetricsmeasuringbodymotionqualityandfacemotion
accuracy.
Furthermore,thecornerstoneofourapproachliesinthesimultaneousgenerationofvocalsandmotion,
aimingtoachievetemporalalignmentbetweenthesemodalities. Thisobjectiveissubstantiatedby
our competitive results on the BC metric, which assesses the synchrony between singing vocals
andcorrespondingmotions, underscoringoursuccessincloselysynchronizingthegenerationof
thesetwomodalities. Forthecascadedsystem,weintegratethetext-to-vocalmodelDiffSingerwith
the audio-to-motion model Talkshow. Compared with the cascaded system, our joint-generation
pipelinedemonstratessuperioroutcomeswhilealsoreducingcomputationaldemandsduringboth
trainingandinferencephases. Inthecascadedarchitectures,errorstendtoaccumulatethrougheach
stage. Specifically, ifthetext-to-vocalmoduleproducesunclearvocals, itsubsequentlyhampers
theaudio-to-motionmodel’sabilitytogenerateaccuratefacialexpressionsthatalignwiththevocal
content.
Evaluationsonvocalgenerations. Wehavealsocomparedourmethodagainstotherstate-of-the-art
text-to-vocalgenerationbaselinesinTable.2(b).Whileourunifiedmodelistrainedtosimultaneously
generatevocalsandmotion,ataskconsiderablymorecomplexthangeneratingvocalsalone,itsvocal
generationcomponentstillmanagestoachieveresultscomparabletothosesystemsdesignedsolely
forvocalgenerations.
Method FGD↓ DIV↑ BC↑ MSE↓ LVD↓
GT 0 12.53 0.499 0 0
PretrainedLLM 48.25 10.05 0.462 3.28 12.15
Singlemotiontoken 19.15 11.29 0.477 2.18 10.12
Ours 18.65 12.19 0.481 2.03 7.23
Table3: Ablationstudy. Wecomparewithcommondesignsinmotiongenerationframeworks.
5.3 AblationStudy
WepresenttheoutcomesofourablationstudyinTable. 3. Initially,weexploredtheintegrationofa
pre-trainedlargelanguagemodel[48]formulti-modalitygeneration,akintotheapproachin [23].
However,theefficacyofutilizingpre-trainedlanguagemodelssignificantlylagsbehindourtailored
design, underscoring that pre-training primarily on linguistic tokens does not facilitate effective
predictionacrossmultiplemodalitieslikevocalandmotion. Additionally,westudytheimpactofour
compositionalVQ-VAEsonmotiongeneration. Incontrast,abaselineemployingasingleVQVAE
forthejointquantizationoffacial,body,andhandmovementswasimplemented. Thisapproachled
toanoticeabledegradationinperformance,particularlymarkedbya-2.89decreaseinLVD.This
declinecanbeattributedtothepreponderanceoffacialmovementsinasinger’sperformance. Using
asingleVQ-VAEmodelforfull-bodydynamicscompromisesthedetailedrepresentationoffacial
expressions,whicharecrucialforrealisticandcoherentmotionsynthesis.
6 Conclusion
Inthiswork,wepresentanewframeworkforthesimultaneousgenerationof3Dwhole-bodymotions
andsingingvocals, directlyfromtextuallyrics. Toaddressthischallengingtask, wefirstcollect
RapVerse,alargedatasetcontainingsynchronousrapvocals,alongsidelyricsand3Dwhole-body
motions. UtilizingRapVerse,wedemonstratethatsimplyscalingautoregressivetransformersacross
language,audio,andmotionyieldsacoherentgenerationofsingingvocalsand3Dholistichuman
motions. Weanticipatethatthisworkwillinspirenovelavenuesinthejointmodelingoftext,audio,
andmotion.
LimitationsandFutureWorks. OnelimitationofRapVerseisthatitiscurrentlyconstrainedtothe
rapmusicgeneration,excludingothermusicgenres. However,wewanttoemphasizethatourmethod
isageneralframeworkthatcouldbeflexiblyutilizedinotherscenariosforjointaudioandmotion
generationifprovidedwiththedatasets.Anotherimportantfuturedirectionresidesinmulti-performer
audioandmotiongeneration,whichcouldbeutilizedincontemporarymusicperformancessuchas
virtuallivebands.
9References
[1] AndreaAgostinelli,TimoIDenk,ZalánBorsos,JesseEngel,MauroVerzetti,AntoineCaillon,Qingqing
Huang,ArenJansen,AdamRoberts,MarcoTagliasacchi,etal. Musiclm: Generatingmusicfromtext.
arXivpreprintarXiv:2301.11325,2023.
[2] ChaitanyaAhujaandLouis-PhilippeMorency. Language2pose:Naturallanguagegroundedposeforecast-
ing. In2019InternationalConferenceon3DVision(3DV),pages719–728.IEEE,2019.
[3] ZheCao,TomasSimon,Shih-EnWei,andYaserSheikh. Realtimemulti-person2dposeestimationusing
partaffinityfields. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages7291–7299,2017.
[4] XinChen,BiaoJiang,WenLiu,ZilongHuang,BinFu,TaoChen,andGangYu.Executingyourcommands
viamotiondiffusioninlatentspace. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages18000–18010,2023.
[5] DanielCudeiro,TimoBolkart,CassidyLaidlaw,AnuragRanjan,andMichaelJBlack. Capture,learning,
andsynthesisof3dspeakingstyles. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages10101–10111,2019.
[6] PrafullaDhariwal,HeewooJun,ChristinePayne,JongWookKim,AlecRadford,andIlyaSutskever.
Jukebox:Agenerativemodelformusic. arXivpreprintarXiv:2005.00341,2020.
[7] JeffDonahue, SanderDieleman, MikołajBin´kowski, ErichElsen, andKarenSimonyan. End-to-end
adversarialtext-to-speech. arXivpreprintarXiv:2006.03575,2020.
[8] ZhiyanDuan,HaotianFang,BoLi,KheChaiSim,andYeWang. Thenussungandspokenlyricscorpus:
Aquantitativecomparisonofsingingandspeech. In2013Asia-PacificSignalandInformationProcessing
AssociationAnnualSummitandConference,pages1–9.IEEE,2013.
[9] GabrieleFanelli,JuergenGall,HaraldRomsdorfer,ThibautWeise,andLucVanGool. A3-daudio-visual
corpusofaffectivecommunication. IEEETransactionsonMultimedia,12(6):591–598,2010.
[10] YlvaFerstlandRachelMcDonnell. Investigatingtheuseofrecurrentmotionmodellingforspeechgesture
generation.InProceedingsofthe18thInternationalConferenceonIntelligentVirtualAgents,pages93–98,
2018.
[11] AninditaGhosh,NoshabaCheema,CennetOguz,ChristianTheobalt,andPhilippSlusallek. Synthesis
ofcompositionalanimationsfromtextualdescriptions. InProceedingsoftheIEEE/CVFinternational
conferenceoncomputervision,pages1396–1406,2021.
[12] ShiryGinosar,AmirBar,GefenKohavi,CarolineChan,AndrewOwens,andJitendraMalik. Learning
individualstylesofconversationalgesture. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages3497–3506,2019.
[13] ChuanGuo,ShihaoZou,XinxinZuo,SenWang,WeiJi,XingyuLi,andLiCheng. Generatingdiverseand
natural3dhumanmotionsfromtext. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages5152–5161,2022.
[14] ChuanGuo,XinxinZuo,SenWang,andLiCheng. Tm2t: Stochasticandtokenizedmodelingforthe
reciprocalgenerationof3dhumanmotionsandtexts. InEuropeanConferenceonComputerVision,pages
580–597.Springer,2022.
[15] Ikhsanul Habibie, Weipeng Xu, Dushyant Mehta, Lingjie Liu, Hans-Peter Seidel, Gerard Pons-Moll,
MohamedElgharib,andChristianTheobalt.Learningspeech-driven3dconversationalgesturesfromvideo.
InProceedingsofthe21stACMInternationalConferenceonIntelligentVirtualAgents,pages101–108,
2021.
[16] JinzhengHe,JinglinLiu,ZhenhuiYe,RongjieHuang,ChenyeCui,HuadaiLiu,andZhouZhao.Rmssinger:
Realistic-music-scorebasedsingingvoicesynthesis. arXivpreprintarXiv:2305.10686,2023.
[17] GeorgHeigold,IgnacioMoreno,SamyBengio,andNoamShazeer. End-to-endtext-dependentspeaker
verification.In2016IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),
pages5115–5119.IEEE,2016.
[18] RomainHennequin,AnisKhlif,FelixVoituret,andManuelMoussallam. Spleeter: afastandefficient
musicsourceseparationtoolwithpre-trainedmodels. JournalofOpenSourceSoftware,5(50):2154,2020.
[19] RomainHennequin,AnisKhlif,FelixVoituret,andManuelMoussallam. Spleeter: afastandefficient
musicsourceseparationtoolwithpre-trainedmodels. JournalofOpenSourceSoftware,5(50):2154,2020.
DeezerResearch.
[20] Wei-NingHsu,Yao-HungHubertTsai,BenjaminBolte,RuslanSalakhutdinov,andAbdelrahmanMo-
hamed. Hubert: Howmuchcanabadteacherbenefitasrpre-training? InICASSP2021-2021IEEE
InternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages6533–6537.IEEE,
2021.
10[21] QingqingHuang,DanielSPark,TaoWang,TimoIDenk,AndyLy,NanxinChen,ZhengdongZhang,
ZhishuaiZhang,JiahuiYu,ChristianFrank,etal. Noise2music:Text-conditionedmusicgenerationwith
diffusionmodels. arXivpreprintarXiv:2302.03917,2023.
[22] RongjieHuang,FeiyangChen,YiRen,JinglinLiu,ChenyeCui,andZhouZhao. Multi-singer: Fast
multi-singersingingvoicevocoderwithalarge-scalecorpus.InProceedingsofthe29thACMInternational
ConferenceonMultimedia,pages3945–3954,2021.
[23] BiaoJiang,XinChen,WenLiu,JingyiYu,GangYu,andTaoChen. Motiongpt: Humanmotionasa
foreignlanguage. AdvancesinNeuralInformationProcessingSystems,36,2024.
[24] KavitaKasiandStephenAZahorian. Yetanotheralgorithmforpitchtracking. In2002ieeeinternational
conferenceonacoustics,speech,andsignalprocessing,volume1,pagesI–361.IEEE,2002.
[25] JihoonKim, JiseobKim, andSungjoonChoi. Flame: Free-formlanguage-basedmotionsynthesis&
editing. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume37,pages8255–8263,
2023.
[26] SungjaeKim,YewonKim,JewooJun,andInjungKim. Muse-svs:Multi-singeremotionalsingingvoice
synthesizerthatcontrolsemotionalintensity. IEEE/ACMTransactionsonAudio,Speech,andLanguage
Processing,2023.
[27] KushalLakhotia,EugeneKharitonov,Wei-NingHsu,YossiAdi,AdamPolyak,BenjaminBolte,Tu-Anh
Nguyen,JadeCopet,AlexeiBaevski,AbdelrahmanMohamed,etal. Ongenerativespokenlanguage
modelingfromrawaudio. TransactionsoftheAssociationforComputationalLinguistics,9:1336–1354,
2021.
[28] GeorgeLakoffandMarkJohnson. MetaphorsWeLiveBy. UniversityofChicagoPress,2003edition,
1980. FirstpublishedJanuary1,1980.
[29] RuilongLi,ShanYang,DavidARoss,andAngjooKanazawa. Aichoreographer:Musicconditioned3d
dancegenerationwithaist++. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages13401–13412,2021.
[30] JingLin,AilingZeng,ShunlinLu,YuanhaoCai,RuimaoZhang,HaoqianWang,andLeiZhang.Motion-x:
Alarge-scale3dexpressivewhole-bodyhumanmotiondataset.AdvancesinNeuralInformationProcessing
Systems,36,2024.
[31] HaiyangLiu,ZihaoZhu,GiorgioBecherini,YichenPeng,MingyangSu,YouZhou,NaoyaIwamoto,
BoZheng,andMichaelJBlack. Emage:Towardsunifiedholisticco-speechgesturegenerationviamasked
audiogesturemodeling. arXivpreprintarXiv:2401.00374,2023.
[32] JinglinLiu,ChengxiLi,YiRen,FeiyangChen,andZhouZhao. Diffsinger:Singingvoicesynthesisvia
shallowdiffusionmechanism. InProceedingsoftheAAAIconferenceonartificialintelligence,volume36,
pages11020–11028,2022.
[33] ShuhongLu,YoungwooYoon,andAndrewFeng.Co-speechgesturesynthesisusingdiscretegesturetoken
learning. arXivpreprintarXiv:2303.12822,2023.
[34] ShunlinLu,Ling-HaoChen,AilingZeng,JingLin,RuimaoZhang,LeiZhang,andHeung-YeungShum.
Humantomato:Text-alignedwhole-bodymotiongeneration. arXivpreprintarXiv:2310.12978,2023.
[35] NaureenMahmood,NimaGhorbani,NikolausFTroje,GerardPons-Moll,andMichaelJBlack. Amass:
Archiveofmotioncaptureassurfaceshapes. InProceedingsoftheIEEE/CVFinternationalconferenceon
computervision,pages5442–5451,2019.
[36] BrianMcFee,ColinRaffel,DawenLiang,DanielPWEllis,MattMcVicar,EricBattenberg,andOriol
Nieto. librosa:Audioandmusicsignalanalysisinpython. InSciPy,pages18–24,2015.
[37] DongchanMin,DongBokLee,EunhoYang,andSungJuHwang. Meta-stylespeech: Multi-speaker
adaptivetext-to-speechgeneration. InInternationalConferenceonMachineLearning,pages7748–7759.
PMLR,2021.
[38] TomohiroNakatani,ShigeakiAmano,ToshioIrino,KentaroIshizuka,andTadahisaKondo. Amethodfor
fundamentalfrequencyestimationandvoicingdecision:Applicationtoinfantutterancesrecordedinreal
acousticalenvironments. SpeechCommunication,50(3):203–214,2008.
[39] ItsukiOgawaandMasanoriMorise. Tohokukiritansingingdatabase:Asingingdatabaseforstatistical
parametricsingingsynthesisusingjapanesepopsongs.AcousticalScienceandTechnology,42(3):140–145,
2021.
[40] AaronvandenOord,SanderDieleman,HeigaZen,KarenSimonyan,OriolVinyals,AlexGraves,Nal
Kalchbrenner,AndrewSenior,andKorayKavukcuoglu. Wavenet: Agenerativemodelforrawaudio.
arXivpreprintarXiv:1609.03499,2016.
[41] VassilPanayotov,GuoguoChen,DanielPovey,andSanjeevKhudanpur. Librispeech:anasrcorpusbased
onpublicdomainaudiobooks. In2015IEEEinternationalconferenceonacoustics,speechandsignal
processing(ICASSP),pages5206–5210.IEEE,2015.
11[42] GeorgiosPavlakos, VasileiosChoutas, NimaGhorbani, TimoBolkart, AhmedAAOsman, Dimitrios
Tzionas,andMichaelJBlack. Expressivebodycapture:3dhands,face,andbodyfromasingleimage. In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages10975–10985,
2019.
[43] MathisPetrovich,MichaelJBlack,andGülVarol.Temos:Generatingdiversehumanmotionsfromtextual
descriptions. InEuropeanConferenceonComputerVision,pages480–497.Springer,2022.
[44] MatthiasPlappert,ChristianMandery,andTamimAsfour. Thekitmotion-languagedataset. Bigdata,
4(4):236–252,2016.
[45] AdamPolyak,YossiAdi,JadeCopet,EugeneKharitonov,KushalLakhotia,Wei-NingHsu,Abdelrah-
manMohamed,andEmmanuelDupoux. Speechresynthesisfromdiscretedisentangledself-supervised
representations. arXivpreprintarXiv:2104.00355,2021.
[46] AlecRadford,JongWookKim,TaoXu,GregBrockman,ChristineMcLeavey,andIlyaSutskever. Robust
speechrecognitionvialarge-scaleweaksupervision,2022.
[47] AlecRadford,JongWookKim,TaoXu,GregBrockman,ChristineMcLeavey,andIlyaSutskever. Robust
speechrecognitionvialarge-scaleweaksupervision. InInternationalConferenceonMachineLearning,
pages28492–28518.PMLR,2023.
[48] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.
Journalofmachinelearningresearch,21(140):1–67,2020.
[49] AliRazavi,AaronVandenOord,andOriolVinyals.Generatingdiversehigh-fidelityimageswithvq-vae-2.
Advancesinneuralinformationprocessingsystems,32,2019.
[50] JosephRedmonandAliFarhadi. Yolov3:Anincrementalimprovement. arXivpreprintarXiv:1804.02767,
2018.
[51] YiRen,ChenxuHu,XuTan,TaoQin,ShengZhao,ZhouZhao,andTie-YanLiu. Fastspeech2:Fastand
high-qualityend-to-endtexttospeech. arXivpreprintarXiv:2006.04558,2020.
[52] YiRen,YangjunRuan,XuTan,TaoQin,ShengZhao,ZhouZhao,andTie-YanLiu. Fastspeech: Fast,
robustandcontrollabletexttospeech. Advancesinneuralinformationprocessingsystems,32,2019.
[53] YiRen,XuTan,TaoQin,JianLuan,ZhouZhao,andTie-YanLiu. Deepsinger:Singingvoicesynthesis
withdataminedfromtheweb. InProceedingsofthe26thACMSIGKDDInternationalConferenceon
KnowledgeDiscovery&DataMining,pages1979–1989,2020.
[54] FlavioSchneider,OjasvKamal,ZhijingJin,andBernhardSchölkopf.Mo\ˆusai:Text-to-musicgeneration
withlong-contextlatentdiffusion. arXivpreprintarXiv:2301.11757,2023.
[55] BidishaSharma,XiaoxueGao,KarthikaVijayan,XiaohaiTian,andHaizhouLi. Nhss: Aspeechand
singingparalleldatabase. SpeechCommunication,133:9–22,2021.
[56] ShuaiShen,WenliangZhao,ZibinMeng,WanhuaLi,ZhengZhu,JieZhou, andJiwenLu. Difftalk:
Craftingdiffusionmodelsforgeneralizedtalkingheadsynthesis. arXivpreprintarXiv:2301.03786,2023.
[57] KentaTakeuchi,SouichirouKubota,KeisukeSuzuki,DaiHasegawa,andHiroshiSakuta. Creatinga
gesture-speechdatasetforspeech-basedautomaticgesturegeneration. InHCIInternational2017–Posters’
ExtendedAbstracts:19thInternationalConference,HCIInternational2017,Vancouver,BC,Canada,July
9–14,2017,Proceedings,PartI19,pages198–202.Springer,2017.
[58] Hiroki Tamaru, Shinnosuke Takamichi, Naoko Tanji, and Hiroshi Saruwatari. Jvs-music: Japanese
multispeakersinging-voicecorpus. arXivpreprintarXiv:2001.07044,2020.
[59] SarahTaylor, TaehwanKim, YisongYue, MosheMahler, JamesKrahe, AnastasioGarciaRodriguez,
JessicaHodgins,andIainMatthews. Adeeplearningapproachforgeneralizedspeechanimation. ACM
TransactionsOnGraphics(TOG),36(4):1–11,2017.
[60] ZacharyTeedandJiaDeng. Raft: Recurrentall-pairsfieldtransformsforopticalflow. InComputer
Vision–ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,PartII
16,pages402–419.Springer,2020.
[61] GuyTevet,SigalRaab,BrianGordon,YonatanShafir,DanielCohen-Or,andAmitHBermano. Human
motiondiffusionmodel. arXivpreprintarXiv:2209.14916,2022.
[62] Jean-MarcValinandJanSkoglund.Lpcnet:Improvingneuralspeechsynthesisthroughlinearprediction.In
ICASSP2019-2019IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),
pages5891–5895.IEEE,2019.
[63] AaronVanDenOord,OriolVinyals,etal. Neuraldiscreterepresentationlearning. Advancesinneural
informationprocessingsystems,30,2017.
12[64] YuWang,XinshengWang,PengchengZhu,JieWu,HanzhaoLi,HeyangXue,YongmaoZhang,LeiXie,
andMengxiaoBi. Opencpop:Ahigh-qualityopensourcechinesepopularsongcorpusforsingingvoice
synthesis. arXivpreprintarXiv:2201.07429,2022.
[65] Cheng-hsinWuu,NingyuanZheng,ScottArdisson,RohanBali,DanielleBelko,EricBrockmeyer,Lucas
Evans,TimothyGodisart,HyowonHa,XuhuaHuang,etal. Multiface:Adatasetforneuralfacerendering.
arXivpreprintarXiv:2207.11243,2022.
[66] JinboXing,MenghanXia,YuechenZhang,XiaodongCun,JueWang,andTien-TsinWong. Codetalker:
Speech-driven3dfacialanimationwithdiscretemotionprior. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages12780–12790,2023.
[67] GengYang,ShanYang,KaiLiu,PengFang,WeiChen,andLeiXie. Multi-bandmelgan:Fasterwaveform
generationforhigh-qualitytext-to-speech. In2021IEEESpokenLanguageTechnologyWorkshop(SLT),
pages492–498.IEEE,2021.
[68] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and
MichaelJBlack. Generatingholistic3dhumanmotionfromspeech. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages469–480,2023.
[69] JianrongZhang,YangsongZhang,XiaodongCun,ShaoliHuang,YongZhang,HongweiZhao,HongtaoLu,
andXiShen. T2m-gpt:Generatinghumanmotionfromtextualdescriptionswithdiscreterepresentations.
arXivpreprintarXiv:2301.06052,2023.
[70] LichaoZhang,RuiqiLi,ShoutongWang,LiqunDeng,JinglinLiu,YiRen,JinzhengHe,RongjieHuang,
JiemingZhu,XiaoChen,etal.M4singer:Amulti-style,multi-singerandmusicalscoreprovidedmandarin
singingcorpus. AdvancesinNeuralInformationProcessingSystems,35:6914–6926,2022.
[71] MingyuanZhang,ZhongangCai,LiangPan,FangzhouHong,XinyingGuo,LeiYang,andZiweiLiu.
Motiondiffuse:Text-drivenhumanmotiongenerationwithdiffusionmodel. IEEETransactionsonPattern
AnalysisandMachineIntelligence,2024.
[72] WenxuanZhang,XiaodongCun,XuanWang,YongZhang,XiShen,YuGuo,YingShan,andFeiWang.
Sadtalker: Learningrealistic3dmotioncoefficientsforstylizedaudio-drivensingleimagetalkingface
animation. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages8652–8661,2023.
[73] YuZhang,RongjieHuang,RuiqiLi,JinZhengHe,YanXia,FeiyangChen,XinyuDuan,BaoxingHuai,
andZhouZhao. Stylesinger: Styletransferforout-of-domainsingingvoicesynthesis. arXivpreprint
arXiv:2312.10741,2023.
[74] ZewangZhang,YibinZheng,XinhuiLi,andLiLu. Wesinger:Data-augmentedsingingvoicesynthesis
withauxiliarylosses. arXivpreprintarXiv:2203.10750,2022.
A Appendix
The supplementary material is organized as follows: Sec. A.1 provides a website demo to show
additionalqualitativeresults;Sec. A.2presentsadditionaldetailsofthenetworkarchitectures;Sec.
A.3introducesevaluationmetrics; Sec. A.4presentsadditionalablationstudies; Sec. A.5shows
additionalqualitativeresults;Sec. A.6discussesbroadersocietalimpactsofthework.
A.1 WebsiteDemo
Inordertoprovidemorevividandclearqualitativeresults,wemakeasupplementalwebsitedemoto
demonstratethegenerationqualityofourproposedsystem. Weencouragereaderstoviewtheresults
athttps://vis-www.cs.umass.edu/RapVerse/.
A.2 ImplementationDetails
MotionTokenizer. WetrainthreeseparateVectorQuantizedVariationalAutoencoders(VQ-VAE)
forface,bodyandhand,respectively. WeadoptthesameVQ-VAEarchitecturebasedon[14,69,23].
For loss functions optimizing the motion tokenizers, we use the L1 smooth reconstruction loss,
the embedding loss, and the commitment loss. The commitment loss weight is set to 0.02. In
line with [14, 23], strategies such as the exponential moving average and the codebook reset
technique[49]areimplementedtooptimizecodebookefficacythroughoutthetrainingprocess. We
take 512 for the codebook size and set the dimension of each code to 512. We set the temporal
down-samplingrateto4. WetraintheVQ-VAEswithabatchsizeof256andasequencewindow
13lengthof72.WeadoptAdamwithβ =0.9,β =0.99,andalearningrateof0.0002astheoptimizer.
1 2
Vocal Tokenizer. For the semantic encoder, we adopt a BASE 12 transformer of HuBERT [20]
pre-trained on the 969-hour LibriSpeech corpus [41]. Following [27, 45], we derive the feature
activationsfromitssixthlayer. ThisprocessallowstheHuBERTmodeltotransforminputaudiointo
a768-dimensionalvectorspace. Subsequently,weemploythek-meansalgorithmwith500centroids
togetquantizeddiscretecontentcodes. FortheF0encoder, aVQ-VAEframeworkisutilizedto
discretizetheF0signalintoquantizedF0tokens. WeadopttheExponentialMovingAverageupdates
during training the VQ-VAE following [6, 45]. We set the codebook size of the VQ-VAE to 20
entries. Moreover,astheoriginalworkdirectlynormalizestheextractedF0valuesforeachsinger
respectively,wedon’texplicitlyusethesinger’sstatisticsbutadoptawindowedconvolutionallayer
withboththeaudioinput(slicedintothewindowsize)andsinger’sembeddingasinput. Finally,we
adoptasimilararchitectureas[17]forthesingerencoder.
General Auto-regressive Model. The auto-regressive model consists of a T5 Embedder and a
Foundation Model. We use a T5-Large Encoder as our Embedder, with 24 layers and 16 heads.
TheEmbedderisfreezedduringfoundationmodeltraining. Thefoundationmodelisbasedonthe
Decoder-onlytransformerarchitecture,whichhas12layersand8heads. WeuseAdamoptimizer
withβ =0.9,β =0.99,andalearningrateof0.0002. Wedonotusedropoutinourtraining. Our
1 2
trainingbatchsizeis384for100epochs.
A.3 EvaluationMetrics
Toevaluatethemotiongenerationquality,weutilizethefollowingmetrics:
1. Frechet Inception Distance (FID): This metric measures the distribution discrepancy
betweenthegroundtruthandgeneratedmotionsofbodyandhandgestures. Specifically,
wetrainanautoencoderbasedon[14]asthemotionfeatureextractor.
2. Diversity(DIV):DIVevaluatesthediversityofgeneratedmotions,wherewecalculatethe
variancefromtheextractedmotionfeatures.
3. Vertex MSE: Following [66], we compute the mean L2 error of lip vertices between
generatedfacemotionsandgroundtruth.
4. LandmarkVelocityDifference(LVD):Introducedby[68],LVDcalculatesthevelocity
differenceofgeneratedfaciallandmarksandgroundtruth.
5. BeatConstancy(BC)[29]: BCevaluatesthesynchronyofgeneratedmotionsandsinging
vocalsbycalculatingthesimilaritybetweentherhythmofgesturesandaudiobeat. Specifi-
cally,weextractaudiobeatsusinglibrosa[36],andwecomputethekinematicbeatsasthe
localminimaofthevelocityofjoints. Thenthealignmentscoreisderivedfromthemean
proximityofmotionbeatstothenearestaudiobeat.
Fortheevaluationofsingingvocalgenerationquality,theMeanOpinionScore(MOS)isemployed.
Itreflectstheperceivednaturalnessofthesynthesizedvocaltones,withhumanevaluatorsratingeach
sampleonascalefrom1to5,therebyofferingasubjectivemeasureofvocalsynthesisfidelity.
A.4 AdditionalAblationStudies
AblationonMotionTokenizer. Westudydifferentdesignsofourmotiontokenizerbycomparing
thereconstructionresults. Specifically,weexploreVQ-VAEswithdifferentcodebooksizes,and
studytheeffectofusingasingleVQ-VAEtomodelfull-bodymotionsinsteadofmultipleVQ-VAEs
fordifferentbodyparts. AsisdemonstratedinTable. 4,wefindthatusingseparateVQ-VAEsfor
face,bodyandhandshaslowerreconstructionerror. Andweselectacodebooksizeof512forour
finalmodel.
AblationonVocalTokenizer. Wealsostudydifferentdesignsforouraudiotokenizerbycomparing
thereconstructionresults. Specifically,weexploredifferentcodebooksizesforthesemanticencoder
bychangingtheK-Meansnumber. WealsocomparetheeffectwithoursingerembeddinginF0
valuepostprocessing. Weusethefollowingmetricstomeasurethereconstructionqualityofthevocal
tokenizer:
14Design MPJPE↓ PAMPJPE↓ ACCL↓
K=256 63.4 39.1 9.14
K=512 60.9 35.4 8.90
K=1024 62.2 36.8 9.08
Single 65.2 38.8 9.32
Ours 60.9 35.4 8.90
Table4: Evaluationofourmotiontokenizer. Wefollow[34]toevaluatethemotionreconstruction
errorsofourmotionVQVAEmodelV . MPJPEandPAMPJPEaremeasuredinmillimeters. ACCL
m
indicatesaccelerationerror.
1. CharacterErrorRate(CER):WeuseWhisper[46]totranscribethegroundtruthand
synthesizedaudios,andthentakethecorrespondinggroundtruthlyricsasthereferenceto
calculatetheCERofthesynthesizedaudios.
2. GrossPitchError(GPE):Thepercentageofpitchestimatesthathaveadeviationofmore
than20%aroundthetruepitch. Onlyframesconsideredpitchedbyboththegroundtruth
andthesynthesizedaudioareconsidered.
3. VoicingDecisionError(VDE)[38]: Theportionofframeswithvoicingdecisionerror,
thatis,theresultsusinggroundtruthandsynthesizedaudiotodeterminewhethertheframe
isvoicedaredifferent.
Method CER↓ GPE↓ VDE↓
GT 41.88 - -
K=100 76.39 2.64 11.98
K=500(Ours) 69.21 2.29 8.84
K=2000 67.46 2.54 9.08
OriginalF0post-process 68.93 2.71 9.19
Table5: Evaluationofourunit2wavmodel. Wefollow[45]toevaluatethespeechresynthesis
errorsofourunit2wavmodel. WER,GPE,andVDE,expressedaspercentages,indicatethecharacter
errorrate,thegrandpitcherrorandthevoicingdecisionerror.
TheanalysisoftheresultsindicatesthateventheoriginalaudioexhibitsahighCER,whichcould
beattributedtotherapidspeechrateassociatedwithrapping. Insomeinstances,thelyricsmaynot
bedistinctlyrecognizableevenbyhumanlisteners. Uponcomparingdifferentcodebooksizes,itis
observedthattheyachievecomparableGPEvalues. ThissimilarityinGPEisexpectedsincethe
sameF0modelisemployedacrossallcodebooksizes. TheCER,whichservesasadirectmeasureof
thesemanticinformationpreservedinthecode,suggeststhatlargercodebookstendtoretainmore
semanticinformation. However,thedifferenceinCERbetweencodebooksizesofK = 500and
K =2000isminimal. GiventhatK =500demonstratesbetterGPEandVDE,weselectK =500.
Additionally, we ablate a design without the singer embedding in the F0 preprocessing, instead
normalizingtheF0valuesforeachsinger. Itshowsthatthisapproachresultedinsignificantlyinferior
performance, particularlyinpitchprediction, comparedtothemodifiedversionthatincludesthe
singerembedding.
A.5 AdditionalQualitativeResults
WeshowadditionalqualitativeresultsinFig. 4. Ourmodeladeptlygeneratescomprehensivewhole-
bodymotionsthatembodytheessenceoftheinputlyrics. Theseincludeauthenticgesturemovements
thatresonatewiththesong’srhythmandsynchronizedlipmotionsthatarticulatethelyrics.
A.6 BroaderImpacts
Thisresearchcontributestoadvancementsingeneratingsynchronizedvocalsandhumanmotion
fromtextuallyrics,aimingtoenhancevirtualagents’abilitytoprovideimmersiveandinteractive
15When I get around you it’s a lie
The way I sing on these tracks now I been back now digging but broke for a minute I been up since the evening
I feel like I feeling fine without you in my wedding I would never think to look back here I going straight
Figure4: Additionalqualitativeresults. Ourmethodcangeneratediversewhole-bodymotions
frominputlyrics.
experiencesindigitalmedia. Thepotentialpositiveimpactofthisworkliesinitsabilitytocreate
more lifelike and engaging virtual performances, such as in virtual concerts and gaming, where
characterscanperformandreactinwaysthataredeeplyresonantwithhumanexpressions. Thiscould
significantlyenhanceuserengagementinvirtualrealitysettings,andprovideinnovativesolutionsin
entertainmentindustries.
However,thiscapabilitycarriesinherentrisksofmisuse. Thetechnology’sabilitytogeneraterealistic
human-likeactionsandsingingvocalsfrommeretextraisesconcernsaboutitspotentialtocreate
misleading or deceptive content. For example, this could be exploited to produce fake videos or
deepfakeswhereindividualsappeartosingandperformthatneveractuallyoccurred,whichcouldbe
usedtospreadmisinformationorharmreputations. Recognizingtheserisks,itiscrucialtoadvocate
forethicalguidelinesandrobustframeworkstoensuretheresponsibleuseofsuchtechnologies.
16