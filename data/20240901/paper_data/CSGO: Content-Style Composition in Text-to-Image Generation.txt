Preprint.
CSGO: CONTENT-STYLE COMPOSITION IN TEXT-TO-
IMAGE GENERATION
PengXing∗12,HaofanWang∗2,YanpengSun1,QixunWang2,
XuBai2,HaoAi2,RenyuanHuang2,ZechaoLi(cid:66)1
NanjingUniversityofScienceandTechnology,InstantXTeam
{xingp ng, zechao.li}@njust.edu.cn,haofanwang.ai@gmail.com
Content Image Style Image StyleID Ours ContentImage Style Image StyleID Ours
StyleImage ”Adog” ”A man” ”Apanda” ”A rocket” ”Abuilding” ContentImage Originalprompt: Newprompt
a small
house with a
sheep statue
on top of it Style Image
Figure 1: (1) Comparison of the style transfer results of the proposed method with the recent
state-of-the-artmethodStyleIDChungetal.(2024). (2)OurCSGOachieveshigh-qualitytext-driven
stylizedsynthesis. (3)OurCSGOachieveshigh-qualitytextediting-drivenstylizedsynthesis.
ABSTRACT
Thediffusionmodelhasshownexceptionalcapabilitiesincontrolledimagegen-
eration,whichhasfurtherfueledinterestinimagestyletransfer. Existingworks
mainly focus on training free-based methods (e.g., image inversion) due to the
scarcity of specific data. In this study, we present a data construction pipeline
forcontent-style-stylizedimagetripletsthatgeneratesandautomaticallycleanses
stylizeddatatriplets. Basedonthispipeline,weconstructadatasetIMAGStyle,
thefirstlarge-scalestyletransferdatasetcontaining210kimagetriplets,available
forthecommunitytoexploreandresearch. EquippedwithIMAGStyle,wepro-
poseCSGO,astyletransfermodelbasedonend-to-endtraining,whichexplicitly
decouplescontentandstylefeaturesemployingindependentfeatureinjection. The
unifiedCSGOimplementsimage-drivenstyletransfer,text-drivenstylizedsynthe-
sis,andtextediting-drivenstylizedsynthesis. Extensiveexperimentsdemonstrate
theeffectivenessofourapproachinenhancingstylecontrolcapabilitiesinimage
generation. Additionalvisualizationandaccesstothesourcecodecanbelocated
ontheprojectpage: https://csgo-gen.github.io/.
1 INTRODUCTION
Recentadvancementsindiffusionmodelshavesignificantlyimprovedthefieldoftext-to-imagegener-
ationSongetal.(2020);Hoetal.(2020). ModelssuchasSDRombachetal.(2022)excelatcreating
1
4202
guA
92
]VC.sc[
1v66761.8042:viXra
(1)
Image-Driven
Style
Transfer
(2)
Text-Driven
Stylized
Synthesis
esuoh
llams
a
esuoh
llams
a
ekib
a
htiw
Stylized
Synthesis
(3)TextEditing-DrivenPreprint.
visuallyappealingimagesbasedontextualprompts,playingacrucialroleinpersonalizedcontent
creationRuizetal.(2023);Xuetal.(2024). Despitenumerousstudiesongeneralcontrollability,
imagestyletransferremainsparticularlychallenging.
Imagestyletransferaimstogenerateaplausibletargetimagebycombiningthecontentofoneimage
withthestyleofanother,ensuringthatthetargetimagemaintainstheoriginalcontent’ssemantics
whileadoptingthedesiredstyleJingetal.(2019);Dengetal.(2020). Thisprocessrequiresfine-
grainedcontrolovercontentandstyle,involvingabstractconceptsliketexture,color,composition,
andvisualquality,makingitacomplexandnuancedchallengeChungetal.(2024).
Asignificantchallengeinstyletransferisthelackofalarge-scalestylizeddataset,whichmakes
it impossible to train models end-to-end and results in suboptimal style transfer quality for non-
end-to-end methods. Existing methods typically rely on training-free structures, such as DDIM
inversionSongetal.(2020)orcarefullytunedfeatureinjectionlayersofpre-trainedIP-AdapterYe
etal.(2023). MethodslikePlug-and-PlayTumanyanetal.(2023),VCTChengetal.(2023),and
the state-of-the-art StyleID Chung et al. (2024) employ content image inversion and sometimes
styleimageinversiontoextractandinjectimagefeaturesintospecificallydesignedlayers. However,
invertingcontentandstyleimagessignificantlyincreasesinferencetime,andDDIMinversioncanlose
criticalinformationMokadyetal.(2023),leadingtofailures,asshowninFigure1.InstantStyleWang
etal.(2024a)employsthepre-trainedIP-Adapter. However,itstruggleswithaccuratecontentcontrol.
AnotherclassofmethodsreliesonasmallamountofdatatotrainLoRAandimplicitlydecouple
contentandstyleLoRAs,suchasZipLoRAShahetal.(2023)andB-LoRAFrenkeletal.(2024),
whichcombinestyleandcontentLoRAstoachievecontentretentionandStyletransfer. However,
eachimagerequiresfine-tuning,andimplicitdecouplingreducesstability.
To overcome the above challenges, we start by constructing a style transfer-specific dataset and
then design a simple yet effective framework to validate the beneficial effects of this large-scale
dataset on style transfer. Initially, we propose a dataset construction pipeline for Content-Style-
StylizedImageTriplets(CSSIT),incorporatingbothadatagenerationmethodandanautomated
cleaning process. Using this pipeline, we construct a large-scale stylized dataset, IMAGStyle,
comprising210Kcontent-style-stylizedimagetriplets. Next,weintroduceanend-to-endtrained
styletransferframework,CSGO.Unlikepreviousimplicitextractions,itexplicitlyusesindependent
contentandstylefeatureinjectionmodulestoachievehigh-qualityimagestyletransformations. The
frameworksimultaneouslyacceptsstyleandcontentimagesasinputsandefficientlyfusescontentand
stylefeaturesusingwell-designedfeatureinjectionblocks. Benefitingfromthedecoupledtraining
framework,oncetrained,CSGOrealizesanyformofarbitrarystyletransferwithoutfine-tuningatthe
inferencestage,includingsketchornatureimage-drivenstyletransfer,text-driven,textediting-driven
stylizedsynthesis. Finally,weintroduceaContentAlignmentScore(CAS)toevaluatethequalityof
styletransfer,effectivelymeasuringthedegreeofcontentlosspost-transfer. Extensivequalitativeand
quantitativestudiesvalidatethatourproposedmethodachievesadvancedzero-shotstyletransfer.
2 RELATED WORK
Text-to-ImageModel. Inrecentyears,diffusionmodelshavegarneredsignificantattentioninthe
text-to-imagegenerationcommunityduetotheirpowerfulgenerativecapabilitiesdemonstratedby
earlyworksDhariwal&Nichol(2021);Rameshetal.(2022). Owingtolarge-scaletrainingSchuh-
mannetal.(2022),improvedarchitecturesRadfordetal.(2021);Peebles&Xie(2023),andlatent
spacediffusionmechanisms,modelslikeStableDiffusionhaveachievednotablesuccessintext-to-
imagegeneration.Rameshetal.(2022). Thefocusoncontrollabilityintext-to-imagemodelshas
growninresponsetopracticaldemands. PopularmodelssuchasControlNetZhangetal.(2023a),
T2IadapterMouetal.(2024),andIP-AdapterYeetal.(2023)introduceadditionalimageconditions
toenhancecontrollability. Thesemodelsusesophisticatedfeatureextractionmethodsandintegrate
these features into well-designed modules to achieve layout control. In this paper, we present a
styletransferframework,CSGO,basedonanimage-conditionalgenerationmodelthatcanperform
zero-shotstyletransfer.
Style Transfer. Style transfer has garnered significant attention and research due to its practical
applications in art creation Gatys et al. (2016). Early methods, both optimization-based Gatys
etal.(2016)andinference-basedChenetal.(2017);Dumoulinetal.(2016),arelimitedbyspeed
constraintsandthediversityofstyletransfer. TheAdaINapproachHuang&Belongie(2017),which
2Preprint.
separatescontentandstylefeaturesfromdeepfeatures,hasbecomearepresentativemethodforstyle
transfer,inspiringaseriesoftechniquesusingstatisticalmeanandvarianceChenetal.(2021);Hertz
etal.(2024). Additionally,transformer-basedmethodssuchasStyleFormerWuetal.(2021)and
StyTR2Dengetal.(2022)improvecontentbias. However,thesemethodsprimarilyfocusoncoloror
stroketransferandfacelimitationsinarbitrarystyletransfer.
Currently,inversion-basedStyleTransfer(InST)Zhangetal.(2023b)isproposedtoobtaininversion
latent of style image and manipulate attention maps to edit generated Images. However, DDIM
(Denoising Diffusion Implicit Models) inversion results in content loss and increased inference
timeSongetal.(2020). Hertzetal. exploreself-attentionlayersusingkeyandvaluematricesfor
styletransferHertzetal.(2024). DEADiffQietal.(2024)andStyleShotJunyaoetal.(2024)are
trainedthroughatwo-stagestylecontrolmethod. However,itiseasytolosedetailedinformation
withinthecontrolthroughsparselines.InstantStyleWangetal.(2024a;b)toachievehigh-qualitystyle
controlthroughpre-trainedpromptadapterYeetal.(2023)andcarefullydesignedinjectionlayers.
However,thesemethodsstrugglewithachievinghigh-precisionstyletransferandfacelimitations
relatedtocontentpreservation. Somefine-tuningapproaches,suchasLoRAHuetal.(2021),DB-
LoRARyu,Zip-LoRAShahetal.(2023),andB-LoRAFrenkeletal.(2024),enablehigher-quality
style-controlledgenerationbutrequirefine-tuningfordifferentstylesandfacechallengesinachieving
styletransfer. OurproposedmethodintroducesanovelstyletransferdatasetanddevelopstheCSGO
framework,achievinghigh-qualityarbitraryimagestyletransferwithouttheneedforfine-tuning.
3 DATA PIPELINE
Inthissection,wefirstintroducetheproposedpipelineforconstructingcontent-style-stylizedimage
triplets. Then,wedescribetheconstructedIMAGStyledatasetindetail.
3.1 PIPELINEFORCONSTRUCTINGCONTENT-STYLE-STYLIZEDIMAGETRIPLETS
Thelackofalarge-scaleopen-sourcedatasetof Content Image Style Image Failure Case 1 Failure Case 2
content-style-stylized image pairs (CSSIT) in
thecommunityseriouslyhinderstheresearchon
styletransfer. Inthiswork,weproposeadata
construction pipeline that automatically con-
structsandcleanstoobtainhigh-qualitycontent-
style-stylizedimagetriplets,givenonlyarbitrary
contentimagesandstyleimages. Thepipeline
containstwosteps: (1)stylizedimagegenera-
tionand(2)stylizedimagecleaning.
Figure 2: Failure cases in step (1), which fail to
Stylizedimagegeneration. Givenanarbitrary
maintainthespatialstructureofthecontentimage.
contentimageC andanarbitrarystyleimageS,
thegoalistogenerateastylizedimageT that Algorithm1PipelineofConstructingCSSIT
preservesthecontentofC whileadoptingthe
Input: contentimagesSetcontent,styleimagesSetstyle
styleofS. WeareinspiredbyB-LoRAFrenkel Output: Content-style-stylizedimagetripletsSet
et al. (2024), which finds that content LoRA 1: foreachC∈Setcontentdo
andstyleLoRAcanbeimplicitlyseparatedby 2 3: : C CL Lcoo onR RtA Aen← t←Tra Si en pL aro aR teA cofo nr teC ntLoRAinCLoRA
SD-trained LoRA, preserving the original im- 4: foreachS∈Setstyledo
age’scontentandstyleinformation,respectively. 5: SLoRA←TrainLoRAforS
Therefore,wefirsttrainalargenumberofLo- 6: S Lst oy Rle A←SeparatestyleLoRAinSLoRA
RAs with lots of content and style imges. To
7: CSLoRA←CombineC Lco on Rt AentandS Lst oy Rle
A
ensurethatthecontentofthegeneratedimage 8: T and= C{ ST L1 o, RT A2,...,Tn}←GeneratenimagesbySDXL
T isalignedtoC asmuchaspossible,theloRA 9: CAS1,CAS2,...,CASn ←ComputeCASforeach
forC istrainedusingonlyonecontentimage generatedimagebasedonEqu.(1)
10: i←ObtaintheindexoftheminimumvalueofallCAS
C. Then,EachtrainedloRAisdecomposedinto 11: Set.append([C,S,Ti])
acontentLoRAandastyleLoRAthroughim- 12: endfor
13: endfor
plicitseparatementionedbyworkFrenkeletal. 14: return Set
(2024). Finally,thecontentLoRAofimageCis
combinedwiththestyleLoRAofStogeneratethetargetimagesT ={T ,T ,...,T }usingthebase
1 2 n
model. However,theimplicitseparateapproachisunstable,resultinginthecontentandstyleLoRA
3Preprint.
notreliablyretainingcontentorstyleinformation. Thismanifestsitselfintheformofthegenerated
imageT ,whichdoesnotalwaysagreewiththecontentofC,asshowninFigure2. Therefore,itis
i
necessarytofilterT,samplingthemostreasonableT asthetargetimage.
i
Stylizedimagecleaning. Slowmethodsofcleaningdatawithhumaninvolvementareunacceptable
fortheconstructionoflarge-scalestylizeddatatriplets. Tothisend,wedevelopanautomaticcleaning
methodtoobtaintheidealandhigh-qualitystylizedimageT efficiently. First,weproposeacontent
alignmentscore(CAS)thateffectivelymeasuresthecontentalignmentofthegeneratedimagewith
thecontentimage. Itisdefinedasthefeaturedistancebetweenthecontentsemanticfeatures(without
styleinformation)ofthegeneratedimageandtheoriginalcontentimage. Itisrepresentedasfollows:
CAS =∥Ada(ϕ(C))−Ada(ϕ(T ))∥2, (1)
i i
whereCAS denotesthecontentalignmentscoreofgeneratedimageT ,ϕ(·)denotesimageencoder.
i i
WecomparethemainstreamfeatureextractorsandtheclosesttohumanfilteringresultsisDINO-
V2Lietal.(2023). Ada(F)representsafunctionoffeatureF toremovestyleinformation. We
followAdaINHuang&Belongie(2017)toexpressstyleinformationbymeanandvariance. Itis
representedasfollows:
F −µ(F)
Ada(F)= , (2)
ρ(F)
where µ(F) and ρ(F) represent the mean and variance of feature F. Obviously, a smaller CAS
indicatesthatthegeneratedimageisclosertothecontentoftheoriginalimage. InAlgorithm1,we
provideapseudo-codeofourpipeline.
3.2 IMAGSTYLEDATASETDETAILS
Content Images. To ensure that the content images have clear semantic information and fa-
cilitate separating after training, we employ the saliency detection datasets, MSRA10K Cheng
et al. (2015; 2013) and MSRA-B Jiang et al. (2013), as the content images. In addi-
tion, for sketch stylized, we sample 1000 sketch images from ImageNet-Sketch Wang et al.
(2019) as content images. The category distribution of content images is shown in Figure 3.
We use BLIP Li et al. (2023) to generate a caption for each
Toys
content image. A total of 11,000 content images are trained 6%3%4%
8%
Architecture
andusedascontentLoRA. Transportation
6% Food
Style Images. To ensure the richness of the style diversity, 22% 4% Clothing
Tools
we sample 5000 images of different painting styles (history 9%
Natural Landscapes
painting,portrait,genrepainting,landscape,andstilllife)from Furniture
the Wikiart dataset Saleh & Elgammal (2016). In addition, 13% 17% Terrestrial Animals
Aquatic Animals
wegenerated5000imagesusingMidjourneycoveringdiverse 8%
Others
styles,includingClassical,Modern,Romantic,Realistic,Sur-
Figure 3: Distribution of content
real, Abstract, Futuristic, Bright, Dark styles etc. A total of
images.
10,000styleimagesareusedtotrainstyleLoRA.
Dataset. Based on the pipeline described in Section 3.1, we construct a style transfer dataset,
IMAGStyle,whichcontains210Kcontent-style-stylizedimagetripletsastrainingdataset. Further-
more,wecollect248contentimagesfromthewebcontainingimagesofrealscenes,sketchedscenes,
faces,andstylescenes,aswellas206styleimagesofdifferentscenesastestingdataset. Fortesting,
eachcontentimageistransferredto206styles. Thisdatasetwillbeusedforcommunityresearchon
styletransferandstylizedsynthesis.
4 APPROACH
4.1 CSGOFRAMEWORK
Theproposedstyletransfermodel,CSGO,showninFigure4,aimstoachievearbitrarystylization
of any image without fine-tuning, including sketch and natural image-driven style transfer, text-
drivenstylizedsynthesis,andtextediting-drivenstylizedsynthesis. Benefitingfromtheproposed
IMAGStyledataset,theproposedCSGOsupportsanend-to-endstyletransfertrainingparadigm. To
ensureeffectivestyletransferandaccuratecontentpreservation,wecarefullydesignthecontentand
4Preprint.
CSGO IMAGStyle Dataset
Text Embedding
Text Prompt: “a cat” Content Image Style Image TargetImage
ℱ(𝑆)
Cross ℱ(𝑆)! Style Image
𝐶 Attention Projection Encoder
ControlNet
ℱ(𝐶)
Image Content ℱ(𝐶)! Cross Cross 𝑆
Encoder Projection Attention Attention
Content Control Style Control
UNet Output
Figure4: (a)Left: Overviewoftheproposedend-to-endstyletransferframeworkCSGO.(b)Right:
SamplesfromourIMAGStyledataset.
stylecontrolmodules. Inaddition,toreducetheriskthatthecontentimageleaksstyleinformationor
thestyleimageleakscontent,thecontentcontrolandstylecontrolmodulesareexplicitlydecoupled,
andthecorrespondingfeaturesareextractedindependently. Tobemorespecific,wecategorizeour
CSGOintotwomaincomponentsanddescribethemindetail.
ContentControl. Thepurposeofcontentcontrolistoensurethatthestylizedimageretainsthe
semantics,layout,andotherfeaturesofthecontentimage.Tothisend,wecarefullydesignedtwoways
ofcontentcontrol. First,weimplementcontentcontrolthroughpre-trainedControlNetZhangetal.
(2023a),whoseinputisthecontentimageandthecorrespondingcaption. Weleveragethecapabilities
of the specific content-controllable model(Tile ControlNet) to reduce the data requirements and
computationalcostsoftrainingcontentretentionfromscratchFollowingtheControlNet,theoutput
ofControlNetisdirectlyinjectedintotheup-samplingblocksofthebasemodel(pre-trainedUNet
in SD) to obtain fusion output D′ = D +δ ×C , D denotes the output of i-th block in the
i i c i i
basemodel,C denotestheoutputofi-thblockinControlNet,δ representsthefusionweight. In
i c
addition,toachievecontentcontrolinthedown-samplingblocksofthebasemodel,weutilizean
additionallearnablecross-attentionlayertoinjectcontentfeaturesintodownblocks. Specifically,we
usepre-trainedCLIPimageencoderRadfordetal.(2021)andalearnableprojectionlayertoextract
the semantic feature F(C)′ of the content image. Then, we utilize an additional cross-attention
layertoinjecttheextractedcontentfeaturesintothedown-samplingblocksofthebasemodel,i.e.,
D′ =D+λ ×D ,Ddenotestheoutputofinthebasemodel,D denotestheoutputofcontent
C c C C
IP-Adapter,λ representsthefusionweightYeetal.(2023). Thesetwocontentcontrolstrategies
c
ensuresmallcontentlossduringthestyletransfer.
StyleControl. ToensurethattheproposedCSGOhasstrongstylecontrolcapability,wealsodesign
twosimpleyeteffectivestylecontrolmethods. Generally,wefeedthestyleimagesintoapre-trained
imageencodertoextracttheoriginalembeddingF(S)∈Ro×dandmapthemtothenewembedding
F(S)′ ∈ Rt×d through the Style Projection layer. Here, o and t represent the token number of
originalandnewembeddings,ddenotesthedimensionofF(S). Forstyleprojection,weemploythe
PerceiverResamplerstructureAlayracetal.(2022)toobtainmoredetailedstylefeatures. Then,we
utilizeanadditionalcross-attentionlayertoinjectthenewembeddingintotheup-samplingblocksof
thebasemodel. Furthermore,wenotethatrelyingonlyontheinjectionoftheup-samplingblocksof
thebasemodelweakensthestylecontrolsinceControlNetinjectionsinthecontentcontrolmayleak
styleinformationofthecontentimageC. Forthisreason,weproposetouseanindependentcross
attentionmoduletosimultaneouslyinjectstylefeaturesinto,andthefusionweightisλ ,asshownin
s
Figure4. Theinsightofthisistopre-adjustthestyleofthecontentimageusingstylefeaturesmaking
theoutputoftheControlnetmodelretainthecontentwhilecontainingthedesiredstylefeatures.
Insummary,theproposedCSGOframeworkexplicitlylearnsseparatefeatureprocessingmodules
thatinjectstyleandcontentfeaturesintodifferentlocationsofthebasemodel,respectively. Despite
itssimplicity,CSGOachievesstate-of-the-artstyletransferresults.
5
Base
Model
Content
Block
Style
BlockPreprint.
Content Image Style Image Ours Instantstyle StyleShot_lienart StyleShot StyleID styTR2 StyleAligned
Figure5: Comparisonofimage-drivenstyletransferresults. Zoomedinforthebestviewing.
4.2 MODELTRAININGANDINFERENCE.
Training. Based on the proposed dataset, IMAGStyle, our CSGO is the first implementation of
end-to-endstyletransfertraining. GivenacontentimageC,acaptionP ofthecontentimage,astyle
imageS,andatargetimageT,wetrainastyletransfernetworkbasedonapre-traineddiffusion
model. OurtrainingobjectiveistomodeltherelationshipbetweenthestyledimageT andGaussian
noiseundercontentandstyleimageconditions,whichisrepresentedasfollows:
(cid:104) (cid:105)
L=E ∥ϵ−ϵ (z ,t,C,S,P)∥2 , (3)
z0,t,P,C,S,ϵ∼N(0,1) θ t
whereεdenotestherandomsampledGaussiannoise,ε denotesthetrainableparametersofCSGO,
θ
trepresentsthetimestep. Notethatthelatentlatentz isconstructedwithastylimageT during
√ √ t
training, z = α¯ ψ(T)+ 1−α¯ ε, where ψ(·) mapping the original input to the latent space
t t t
function,α¯ isconsistentwithdiffusionmodelsSongetal.(2020);Hoetal.(2020). Werandomly
t
dropcontentimageandstyleimageconditionsinthetrainingphasetoenableclassifier-freeguidance
intheinferencestage.
Inference. Duringtheinferencephase,weemployclassifier-freeguidance. Theoutputoftimestept
isindicatedasfollows:
ϵˆ (z ,t,C,S,P)=wϵ (z ,t,C,S,P)+(1−w)ϵ (z ,t), (4)
θ t θ t θ t
wherewrepresentstheclassifier-freeguidancefactor(CFG).
5 EXPERIMENTS
5.1 EXPERIMENTALSETUP
Setup. FortheIMAGstyledataset,duringthetrainingphase,wesuggestusing‘a[vcp]’asaprompt
forcontentimagesand‘a[stp]’asapromptforstyleimages. Therankissetto64andeachB-loRA
istrainedwith1000steps. Duringthegenerationphase,wesuggestusing‘a[vcp]in[stv]style’as
theprompt. FortheCSGOframework,weemploystabilityai/stable-diffusion-xl-base-1.0asthebase
model,pre-trainedViT-Hasimageencoder,andTTPlanet/TTPLanet SDXL Controlnet Tile Realistic
asControlNet. weuniformlysettheimagesto512×512resolution. Thedroprateoftext,content
image,andstyleimageis0.15. Thelearningrateis1e-4. Duringtrainingstage,λ =λ =δ =1.0.
c s c
Duringinferencestage,wesuggestλ =λ =1.0andδ =0.5. Ourexperimentsareconductedon
c s c
8NVIDIAH800GPUs(80GB)withabatchsizeof20perGPUandtrained80000steps.
6Preprint.
Table1: Comparisonofstylesimilarity(CSD)andcontentalignment(CAS)withrecentstate-of-the-
artmethodsonthetestdataset.
StyTR2 Style-Aligned StyleID InstantStyle StyleShot StyleShot-lineart CSGO
Dengetal.(2022) Hertzetal.(2024) Chungetal.(2024) Wangetal.(2024a) Junyaoetal.(2024) Junyaoetal.(2024) Ours
CSD(↑) 0.2695 0.4274 0.0992 0.3175 0.4522 0.3903 0.5146
CAS(↓) 0.9699 1.3930 0.4873 1.3147 1.5105 1.0750 0.8386
DatasetsandEvaluation. WeusetheproposedIMAGStyleasatrainingdatasetanduseitstesting
datasetasanevaluationdataset. WeusetheCSDscoreSomepallietal.(2024)asanevaluationmetric
toevaluatethestylesimilarity. Meanwhile,weemploytheproposedcontentalignmentscore(CAS)
asanevaluationmetrictoevaluatethecontentsimilarity.
Style Image A InstantStyle StyleShot StyleAligned DEADiff Ours InstantStyle StyleShot StyleAligned DEADiff Ours
a
monkey
Style Image B girl playing
with a ball
in the
playground
a church
in the
mountain
a man
reading
a book
a robot
near the
house
Figure6: Comparisonofgenerationresultsfortext-drivenstylizedsynthesiswithrecentmethods.
StyleImage “A giraffe” “A man” “A house” “A computer” ContentImage StyleImage wS ir zc arP dr o wm itp ht a: a b op oai kn ati nn dg ao f h a a t sua n p ga lain st si en sg w o if t ha aw biz oa ord k w ane gar ain hg at a pa win et ain rg in o gf a a h w ai tzard
SrcPrompt:arafed view of a arafed view of a building arafed view of a building
building with a sky background andplantswith a sky andacarwith a sky
and a cloudy sky background and a cloudy sky background
Figure7: Generatedresultsoftheproposed Figure8: ThegeneratedresultsoftheproposedCSGO
CSGOintext-drivenstylizedsynthesis. intextediting-drivenstylizedsynthesis.
Baselines. We compare recent advanced inversion-based StyleID Chung et al. (2024),
StyleAligned Hertz et al. (2024) methods, and StyTR2 Deng et al. (2022) based on the Trans-
formerstructure. Inaddition,wecompareInstantstyleWangetal.(2024a)andStyleShot(andtheir
fine-grainedcontrolmethodStyleShot-lineart)Junyaoetal.(2024)thatintroduceControlNetand
IPAdapterstructuresasbaselines. Fortext-drivenstylecontroltask,wealsointroduceDEADiffQi
etal.(2024)asabaseline.
5.2 EXPERIMENTALRESULTS
Image-DrivenStyleTransfer. InTable1,wedemonstratetheCSDscoresandCASoftheproposed
method with recent advanced methods for the image-driven style transfer task. In terms of style
control, our CSGO achieves the highest CSD score, demonstrating that CSGO achieves state-of-
the-artstylecontrol. Duetothedecoupledstyleinjectionapproach,theproposedCSGOeffectively
extractsstylefeaturesandfusesthemwithhigh-qualitycontentfeatures. AsillustratedinFigure5,
OurCSGOpreciselytransfersstyleswhilemaintainingthesemanticsofthecontentinnatural,sketch,
face,andartscenes.
7Preprint.
ContentImage StyleImage Con( t1 en) tW C/ oO ntrol W/O s( t2 y) l eW i nC jeo cn tt ie on nt iC n o Cn otr no tl rolNet (3) Ours ContentImage StyleImage (1) w/o resampler (2)wresampler
Figure9: Ablationstudiesofcontentcontrolandstylecon-Figure10: Ablationstudiesofstyleim-
trol. ageprojection.
In terms of content retention, it can be observed that StyleID Chung et al. (2024) and
StyleAlignedHertzetal.(2024),whicharebasedoninversion,maintaintheoriginalcontenttoo
stronglyinsketchstyletransferscenarios(CASisverylow). However. theyareunabletoinject
styleinformationsinceCSDscoreislow. InstantStyleWangetal.(2024a)andStyleShotJunyao
etal.(2024)(includingLineart),whichuselinestocontrolthecontent,areaffectedbythelevelof
detailofthelines,andhavedifferentdegreesoflossofcontent(suchasfacescenes). Theproposed
CSGOdirectlyutilizesalltheinformationofthecontentimage,andcontentpreservationisoptimal.
ThequantitativeresultsinTable1alsoshowthattheproposedCSGOmaintainshigh-qualitycontent
retentionwithprecisestyletransfer.
ContentImage Style Image 𝑡=4 𝑡=12 𝑡=16 𝑡=32 Style Image 𝑡=32 𝑡=16 𝑡=4
Prompt:“ahelicopter”
Prompt:“acat”
Figure 11: Ablation studies of style token number t. Left: Image style transfer results. Right:
Text-drivenstylizedsynthesisresults.
Text-DrivenStylizedSynthesis. Theproposedmethodenablestext-drivenstylecontrol,i.e.,givena
textpromptandastyleimage,generatingimageswithsimilarstyles. Figure6showsthecomparison
ofthegenerationresultsoftheproposedCSGOwiththestate-of-the-artmethods. Inasimplescene,
itisintuitivetoobservethatourCSGOobeystextualpromptsmore. Thereasonforthisisthatthanks
totheexplicitdecouplingofcontentandstylefeatures,styleimagesonlyinjectstyleinformation
without exposing content. In addition, in the complex scene, thanks to the well-designed style
featureinjectionblock,CSGOenablesoptimalstylecontrolwhileconvertingthemeaningoftext. As
illustratedinFigure7,wedemonstratedmoreresults.
Textediting-DrivenStylizedSynthesis. TheproposedCSGOsupportstextediting-drivenstyle
control. AsshowninFigure8, inthestyletransfer, wemaintainthesemanticsandlayoutofthe
originalcontentimageswhileallowingsimpleeditingofthetextualprompts. Theaboveexcellent
resultsdemonstratethattheproposedCSGOisapowerfulframeworkforstylecontrol.
5.3 ABLATIONSTUDIES.
Contentcontrolandstylecontrol. Wediscusstheimpactofthetwofeatureinjectionmethods,
as shown in Figure 9. The content image must be injected via ControlNet injection to maintain
the layout while preserving the semantic information. Content features only retain semantic in-
formation if injected into the CSGO framework only through IP-Adapter injection (Figure 9(1)).
8Preprint.
Content Image Style Image wControlNet w/o ControlNet
After introducing the ControlNet in-
jection, the quality of content reten-
tionimproved,asshowninFigure12.
However, ifthestylefeaturesarein-
jected into base UNet only without
ControlNetinjection,thisweakensthe
styleofthegeneratedimages,which
canbeobservedinthecomparisonof Figure12: AblationstudiesofControlNet.
Figure 9(2) and (3). Therefore, the
proposedCSGOpre-injectsstylefeaturesintheControlNetbranchtofurtherfusethestylefeatures
toenhancethetransfereffect.
Styleimageprojectionlayer. Thestyleimageprojectionlayercaneffectivelyextractstylefeatures
fromtheoriginalembedding. WeexplorethenormallinearlayerandtheResamplerstructure,andthe
experimentalresultsareshowninFigure10. UsingtheResamplerstructurecapturesmoredetailed
stylefeatureswhileavoidingcontentleakage.
Token number. We explore the effect of the number of token t in the style projection layer on
theresultsofstyletransferandtext-drivenstylesynthesis. Theexperimentalresultsareshownin
Figure10,wherethestylecontrolbecomesprogressivelybetterastincreases. Thisisinlinewithour
expectationthattinfluencesthequalityoffeatureextraction. Alargertmeansthattheprojection
layerisabletoextractricherstylefeatures.
(1)Fix𝑪𝑭𝑮=𝟏𝟎.𝟎,𝝀𝒔=𝟏.𝟎,𝝀𝒄=𝟏.𝟎 (4) Fix 𝑪𝑭𝑮=𝟏𝟎.𝟎,𝜹𝒄=𝟎.𝟓, 𝝀𝒔=𝟏.𝟎
Content Image 𝜹𝒄=𝟎.𝟏 𝜹𝒄=𝟎.𝟑 𝜹𝒄=𝟎.𝟓 𝜹𝒄=𝟎.𝟕 𝜹𝒄=𝟎.𝟗 𝜹𝒄=𝟏.𝟐 𝝀#=𝟎.𝟏
Style Image 𝑪𝑭𝑮 = 1.0 𝑪𝑭𝑮 = 10.0 𝑪𝑭𝑮 =20.0 𝝀𝒔=𝟎.𝟓 𝝀𝒔=𝟏.𝟎 𝝀𝒔=𝟏.𝟓 𝝀#=𝟏.𝟎
(2)Fix𝜹𝒄=𝟎.𝟓 ,𝝀𝒔=𝟏.𝟎,𝝀𝒄=𝟏.𝟎 (3)Fix𝑪𝑭𝑮=𝟏𝟎.𝟎,𝜹𝒄=𝟎.𝟓,𝝀𝒄=𝟏.𝟎
ContentImage 𝜹𝒄=𝟎.𝟏 𝜹𝒄=𝟎.𝟑 𝜹𝒄=𝟎.𝟓 𝜹𝒄=𝟎.𝟕 𝜹𝒄=𝟎.𝟗 𝜹𝒄=𝟏.𝟐 𝝀#=𝟎.𝟏
StyleImage 𝑪𝑭𝑮 =1.0 𝑪𝑭𝑮 =10.0 𝑪𝑭𝑮 = 20.0 𝝀𝒔=𝟎.𝟓 𝝀𝒔=𝟏.𝟎 𝝀𝒔=𝟏.𝟓 𝜹𝒄=𝟏.𝟎
Figure13: Ablationstudiesofcontentscaleδ ,CFG,contentscaleλ ,andstylescaleλ .
c c s
The impact of content scale δ . As shown in Figure 13, when δ is small, the content feature
c c
injectionisweak,andCSGOobeysthetextualpromptsandstylemore. Asδ increases,thequality
c
ofcontentretentionbecomessuperior. However,wenoticethatwhenδ islarge(e.g.,0.9and1.2),
c
thestyleinformationisseverelyweakened.
TheimpactofCFGscale. Classifier-freeguidanceenhancesthecapabilitiesofthetext-to-image
model.TheproposedCSGOissimilarlyaffectedbythestrengthofCFGscale.AsshowninFigure13,
theintroductionofCFGenhancesthestyletransfereffect.
The impact of style scale λ and content scale λ . The style scale affects the degree of style
s c
injection. Figure13showsthatifthestylescaleislessthan1.0,thestyleofthegeneratedimageis
severelyweakened. Wesuggestthatthestylescaleshouldbebetween1.0and1.5. Contentcontrolin
thedown-samplingblocksutilizesthesemanticinformationofthecontentimagetoreinforcethe
accurateretentionofcontent. Figure13showsthatλ ismosteffectivewhenitisnear1.0.
c
9Preprint.
6 CONCLUSION.
Wefirstproposeapipelinefortheconstructionofcontent-style-stylizedimagetriplets. Basedon
thispipeline,weconstructthefirstlarge-scalestyletransferdataset,IMAGStyle,whichcontains
210Kimagetripletsandcoversawiderangeofstylescenarios. TovalidatetheimpactofIMAGStyle
onstyletransfer,weproposeCSGO,asimplebuthighlyeffectiveend-to-endtrainingstyletransfer
framework,andweverifythattheproposedCSGOcansimultaneouslyperformimagestyletransfer,
text-driven style synthesis, and text editing-driven style synthesis tasks in a unified framework.
ExtensiveexperimentsvalidatethebeneficialeffectsofIMAGStyleandCSGOforstyletransfer. We
hopethatourworkwillinspiretheresearchcommunitytofurtherexplorestylizedresearch.
Futurework. Althoughtheproposeddatasetandframeworkachieveveryadvancedperformance,
there is still room for improvement. Due to time and computational resource constraints, we
constructedonly210Kdatatriplets. Webelievethatbyexpandingthesizeofthedataset,thestyle
transferqualityofCSGOwillbeevenbetter. Meanwhile,theproposedCSGOframeworkisabasic
version,whichonlyverifiesthebeneficialeffectsofgenerativestylizeddatasetsonstyletransfer. We
believethatthequalityofstyletransfercanbefurtherimprovedbyoptimizingthestyleandcontent
featureextractionandfusionmethods.
REFERENCES
Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,Karel
Lenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguage
modelforfew-shotlearning. InNeurIPS,2022.
DongdongChen,LuYuan,JingLiao,NenghaiYu,andGangHua. Stylebank: Anexplicitrepresenta-
tionforneuralimagestyletransfer. InCVPR,2017.
Haibo Chen, Zhizhong Wang, Huiming Zhang, Zhiwen Zuo, Ailin Li, Wei Xing, Dongming Lu,
etal. Artisticstyletransferwithinternal-externallearningandcontrastivelearning. NeurIPS,34:
26561–26573,2021.
BinCheng,ZuhaoLiu,YunboPeng,andYueLin. Generalimage-to-imagetranslationwithone-shot
imageguidance. InICCV,2023.
Ming-MingCheng,JonathanWarrell,Wen-YanLin,ShuaiZheng,VibhavVineet,andNigelCrook.
Efficientsalientregiondetectionwithsoftimageabstraction. InIEEEICCV,pp.1529–1536,2013.
Ming-MingCheng,NiloyJ.Mitra,XiaoleiHuang,PhilipH.S.Torr,andShi-MinHu. Globalcontrast
basedsalientregiondetection. IEEETPAMI,37(3):569–582,2015.
JiwooChung,SangeekHyun,andJae-PilHeo. Styleinjectionindiffusion: Atraining-freeapproach
foradaptinglarge-scalediffusionmodelsforstyletransfer. InCVPR,2024.
YingyingDeng,FanTang,WeimingDong,WenSun,FeiyueHuang,andChangshengXu. Arbitrary
styletransferviamulti-adaptationnetwork. InACMMM,2020.
YingyingDeng,FanTang,WeimingDong,ChongyangMa,XingjiaPan,LeiWang,andChangsheng
Xu. Stytr2: Imagestyletransferwithtransformers. InCVPR,pp.11326–11336,2022.
PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. NeurIPS,
2021.
VincentDumoulin,JonathonShlens,andManjunathKudlur. Alearnedrepresentationforartistic
style. arXiv,2016.
YardenFrenkel,YaelVinker,ArielShamir,andDanielCohen-Or. Implicitstyle-contentseparation
usingb-lora. arXiv,2024.
LeonAGatys,AlexanderSEcker,andMatthiasBethge. Imagestyletransferusingconvolutional
neuralnetworks. InCVPR,2016.
10Preprint.
AmirHertz,AndreyVoynov,ShlomiFruchter,andDanielCohen-Or. Stylealignedimagegeneration
viasharedattention. InCVPR,2024.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InNeurIPS,
2020.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
andWeizhuChen. Lora: Low-rankadaptationoflargelanguagemodels. arXiv,2021.
XunHuangandSergeBelongie. Arbitrarystyletransferinreal-timewithadaptiveinstancenormal-
ization. InICCV,pp.1501–1510,2017.
HuaizuJiang, JingdongWang, ZejianYuan, YangWu, NanningZheng, andShipengLi. Salient
objectdetection: Adiscriminativeregionalfeatureintegrationapproach. InCVPR,2013.
YongchengJing,YezhouYang,ZunleiFeng,JingwenYe,YizhouYu,andMingliSong. Neuralstyle
transfer: Areview. IEEETVCG,26(11):3365–3385,2019.
GaoJunyao,LiuYanchen,SunYanan,TangYinhao,ZengYanhong,ChenKai,andZhaoCairong.
Styleshot: Asnapshotonanystyle. arXiv,2024.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. InICML,pp.19730–19742.
PMLR,2023.
RonMokady,AmirHertz,KfirAberman,YaelPritch,andDanielCohen-Or. Null-textinversionfor
editingrealimagesusingguideddiffusionmodels. InCVPR,2023.
Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan.
T2i-adapter: Learningadapterstodigoutmorecontrollableabilityfortext-to-imagediffusion
models. InAAAI,2024.
WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InICCV,2023.
TianhaoQi,ShanchengFang,YanzeWu,HongtaoXie,JiaweiLiu,LangChen,QianHe,andYong-
dongZhang. Deadiff: Anefficientstylizationdiffusionmodelwithdisentangledrepresentations.
InCVPR,pp.8693–8702,2024.
AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh, GabrielGoh, SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InICML,2021.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditionalimagegenerationwithcliplatents. arXiv,2022.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,2022.
NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman.
Dreambooth: Finetuningtext-to-imagediffusionmodelsforsubject-drivengeneration. InCVPR,
2023.
SimoRyu. Low-rankadaptationforfasttext-to-imagediffusionfine-tuning.2022. URLhttps://github.
com/cloneofsimo/lora.
BabakSalehandAhmedElgammal. Large-scaleclassificationoffine-artpaintings: Learningthe
rightmetricontherightfeature. InternationalJournalforDigitalArtHistory,(2),2016.
ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,Mehdi
Cherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-5b: An
openlarge-scaledatasetfortrainingnextgenerationimage-textmodels. NeurIPS,2022.
VirajShah,NatanielRuiz,ForresterCole,ErikaLu,SvetlanaLazebnik,YuanzhenLi,andVarun
Jampani. Ziplora: Anysubjectinanystylebyeffectivelymergingloras. arXiv,2023.
11Preprint.
Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas
Geiping,AbhinavShrivastava,andTomGoldstein. Measuringstylesimilarityindiffusionmodels.
InECCV,2024.
JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. InICLR,
2020.
NarekTumanyan,MichalGeyer,ShaiBagon,andTaliDekel. Plug-and-playdiffusionfeaturesfor
text-drivenimage-to-imagetranslation. InCVPR,2023.
HaofanWang,QixunWang,XuBai,ZekuiQin,andAnthonyChen. Instantstyle: Freelunchtowards
style-preservingintext-to-imagegeneration. arXiv,2024a.
HaofanWang,PengXing,RenyuanHuang,HaoAi,QixunWang,andXuBai. Instantstyle-plus:
Styletransferwithcontent-preservingintext-to-imagegeneration. arXiv,2024b.
HaohanWang,SongweiGe,ZacharyLipton,andEricPXing. Learningrobustglobalrepresentations
bypenalizinglocalpredictivepower. InNeurIPS,2019.
XiaoleiWu,ZhihaoHu,LuSheng,andDongXu. Styleformer: Real-timearbitrarystyletransfervia
parametricstylecomposition. InICCV,pp.14618–14627,2021.
JiazhengXu,XiaoLiu,YuchenWu,YuxuanTong,QinkaiLi,MingDing,JieTang,andYuxiaoDong.
Imagereward: Learningandevaluatinghumanpreferencesfortext-to-imagegeneration. NeurIPS,
2024.
HuYe,JunZhang,SiboLiu,XiaoHan,andWeiYang. Ip-adapter: Textcompatibleimageprompt
adapterfortext-to-imagediffusionmodels. arXivpreprintarXiv:2308.06721,2023.
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image
diffusionmodels. InICCV,pp.3836–3847,2023a.
YuxinZhang,NishaHuang,FanTang,HaibinHuang,ChongyangMa,WeimingDong,andChang-
sheng Xu. Inversion-based style transfer with diffusion models. In CVPR, pp. 10146–10156,
2023b.
12