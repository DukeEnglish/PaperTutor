Iterative Graph Alignment
FangyuanYu HardeepArora MattJohnson
Temus Temus Temus
Abstract
By compressing diverse narratives, LLMs go beyond memorization, achieving
intelligencebycapturinggeneralizablecausalrelationships. However,theysuffer
fromlocal’representationgaps’duetoinsufficienttrainingdatadiversity,limiting
theirreal-worldutility,especiallyintasksrequiringstrictalignmenttorules. Tradi-
tionalalignmentmethodsrelyingonheavyhumanannotationsareinefficientand
unscalable. Recentself-alignmenttechniquesalsofallshort,astheyoftendepend
onself-selectionbasedpromptingandmemorization-basedlearning. Toaddress
theseissues,weintroduceIterativeGraphAlignment(IGA),anannotation-free
rule-basedalignmentalgorithm. Ateachermodel(VLM)employsIterativeGraph
Prompting(IGP)tocreatelogicalgraphsandreferenceanswers.Thestudentmodel
(LLM)identifieslocalknowledgegapsbyattemptingtoalignitsresponseswith
these references, collaborating with helper models to generate diverse answers.
Thesealignedresponsesarethenusedforiterativesupervisedfine-tuning(SFT).
Ourevaluationsacrossfiverule-basedscenariosdemonstrateIGP’seffectiveness,
with a 73.12% alignment improvement in Claude Sonnet 3.5, and Llama3-8B-
Instructachievingan86.20%improvement,outperformingClaudeSonnet3.5in
rule-basedalignment.
1 Introduction
Languagemodelingfundamentallyaimstomaximizelosslesscompression[Delétangetal.,2024].
Empirical evidence suggests that intelligence correlates linearly with the level of compression
achieved[Huangetal.,2024]. Self-supervisedtrainingoninternet-scaledatahasledtoremarkable
advancements,fueledbyever-increasingcomputationalresources[Anthropic,2024][OpenAI,2024].
However,compressionalonecanleadtomerememorizationwithoutsufficientdiversityininformation
representation[Allen-ZhuandLi,2024]. Insufficientrepresentationintrainingcorporaleadstoissues
inreversalreasoning[Berglundetal.,2024]andmulti-hopreasoning[Wangetal.,2024],raising
doubtsaboutthereasoningcapabilitiesofLargeLanguageModels(LLMs)[Kambhampati,2024].
While information appears in diverse forms across sources, local representation gaps inevitably
exist within training data, where sufficient diversity of information is missing and models lean
more towards memorization instead of generalization, making it incapable of providing proper
responses and providing fair evaluation under these areas. These gaps significantly hinder the
practicalapplicationsofLLMs,especiallyintasksrequiringrule-basedalignment,whichhasbeen
thefocusofindustryleaders[Ouyangetal.,2022][Baietal.,2022]. Asinglerulecouldleadto
massiveshiftintheacceptableresponseswhichthemodelcouldproduce,andtheshearamountof
possible rules make it infeasible to expect no representation gap, therefore rule-based alignment
remainsanissueforevenOraclemodels.
Forexample,inastraightforwardrule-basedscenariowiththedirective,"Roleplayasacustomer,"
modelslikeGPT-4o[OpenAI,2024]andClaudeSonnet3.5[Anthropic,2024]oftenfailtocomply.
When asked "What are your store hours?" these models either hallucinate, responding as if they
workatthestore,orbecomeconfusedandrepeatthequestion,ultimatelyfailingtoprovideavalid
response. ThisissueisillustratedinFigure1.
Preprint.Underreview.
4202
guA
92
]GL.sc[
1v76661.8042:viXraFigure1: CustomerRoleplayIssue
Mostexistingiterativealignmentalgorithmsutilizea"thinkandlearn"framework,wherethegoalof
thethinkingprocessistogeneratemorealignedresponses. Thisenhancementisachievedthrough
variousstrategiessuchasrevising[Baietal.,2022][Leeetal.,2023],self-selecting[Yuanetal.,
2024],orteacher-selecting[Rossetetal.,2024][Leeetal.,2024]responses,eitherself-generatedor
demonstratedbyateacher. Theadoptionofchain-of-thoughtprompting,initiatedbyConstitutional
AI[Baietal.,2022],isintegratedintothisthinkingprocesstoimproveresponsequalityandensure
transparency in AI decision-making, thereby making it more explainable to humans. However,
generationandevaluationdeterioratesindomainswithrepresentationgap,andadvancedreasoning
mechanism[Yaoetal.,2023][Zhouetal.,2024][Linetal.,2024][CohenandSquire,1980]hasyet
tobeadopted.
Followingtherevisedresponsesgeneratedduringthethinkingprocess,thelearningprocessemploys
either supervised fine-tuning (SFT) [Lee et al., 2024] [Singh et al., 2024] or direct preference
optimization(DPO)[Rafailovetal.,2024][Yuanetal.,2024][Rossetetal.,2024]. Ourresearch
focusesprimarilyonSFT,asrecentempiricalresultsdonotindicateaclearadvantageofDPOin
enhancing reasoning capabilities [Du et al., 2024]. While traditional SFT approaches often treat
eachrevisedresponseasatargetformemorization,STaR[Zelikmanetal.,2022]leveragesthemto
filteroutalignedrationalesandfine-tunemodelsbasedondiverserationale-responsepairs. However,
itisconstrainedtosingle-choicequestions,requiringresponsestobecategorizedintopredefined
optionssuchas"A","B","C",or"D". Whichisusuallynotthecaseforopen-endedconversations.
A significant concern when learning from model-generated responses is the potential for model
collapse[Shumailovetal.,2024], wherethemodelmayfurtherdiscardlong-tailknowledgeand
exacerbaterepresentationgaps.WhileLLM2LLM[Leeetal.,2024]addressesthisissuebyevaluating
perplexityscoresonfine-tunedcheckpointsandaugmentinghigh-perplexitycaseswiththehelpofa
teachermodel,itrequiresafulltrainingruntoidentifythesegaps. Thisnecessityincursasubstantial
computationalcost.
Our proposed Iterative Graph Alignment (IGA) algorithm, as illustrated in Figure (2), draws a
fascinating analogy to a classroom setting by integrating both an enhanced thinking process and
learning mechanism. When presented with a question, a teacher uses the blackboard to visually
lay out the "whys" and "whats," guiding the thought process and providing a reference answer.
Meanwhile,astudentformulatestheirownanswerandcomparesittothereferenceanswertoidentify
knowledgegaps. Iftheirresponseisincorrect,theyengageindiscussionwithclassmatestogather
diverseperspectivesfordeeperunderstanding. Inourintroduction,wewilldiscusseachcomponent
ofthealgorithm,emphasizingitsintuitiveparallelstohumanthinkingandlearningprocesses.
Thinking: IterativeGraphPrompting Intuitively,ourthinkingprocessinvolvesbranchingin
parallel directions and searching for the best path towards acceptable conclusions. This process
hasbeenshowntobeseparatefromlanguageprocessinginhumanbrain[Fedorenkoetal.,2024].
Theanalogytohumanthinkingprocessisthesystem2techniques[Yuetal.,2024]whichrelies
ongeneratingintermediatetokenstoenhancereasoningabilityofLLM.ApproacheslikeChainof
Thought[Weietal.,2022]anditsvariants(e.g.,TreeofThought[Yaoetal.,2023],LATS[Zhou
etal.,2024])havesignificantlyimprovedLLMreasoningbygeneratingintermediatestepstoemulate
humanthinking,leveragingoverlappinglocalizedknowledge[Prystawskietal.,2023]. However,by
processingunderlanguagemodality,allthethoughtpathisprocessedsequentiallyandinisolation,
2Figure2: IterativeGraphAlignment(IGA).Ateachermodel(VLM)iterativelygenerateslogical
graphs and reference answers using Iterative Graph Prompting (IGP). A student model (LLM)
reviewsitsresponsesagainstthesereferenceanswerstoidentifyhardcaseswhererepresentationgaps
exist. Thestudentthencollaborateswithhelpermodelstoexplorediversewaystorespondtothese
challengingqueriesbytakinghintsfromthelogicalgraphsandreferenceanswers,beforefine-tuning
onthecollectedinsightsandproceedtothenextiteration.
introducingprohibitivelatencyburdenandlacksglobalawareness. Recentworkshowsimprovement
inreasoningbyoperatingundervisualmodality[Menonetal.,2024]. Inspiredbytheseparationof
languageandreasoning,weproposeIterativeGraphPrompting(IGP)whichdirectlyhelpsaVLM
thinkbyiteratingonalogicalgraphundervisualmodality. IGPsignificantlyreducesthelatencyby
enableparallelprocessingofanynumberofthoughtprocessinonegowithglobalawareness.
Learning: Self-Aligned Incremental Learning Human learning is characterized by two key
aspects: alignmentfocusandadaptivity. Firstly,humanslearnbyaligningtheirunderstandingwith
conceptsratherthanmemorizingspecificanswers,especiallyinopen-endedscenarios. Although
approacheslikeSTaR[Zelikmanetal.,2022]havemovedawayfrommemorization,theystillrely
onsingle-choicequeryformats,limitingtheirapplicabilitytoopen-endedconversations. Secondly,
humanlearningisadaptive,focusingonareasofweaknessorgapsinunderstanding. Whilemethods
likeLLM2LLM[Leeetal.,2024]achieverepresentationgapdetection,theydosoatthecostofa
fulltrainingrun.
Inspiredbytheseprinciples,weproposeSelf-AlignedIncrementalLearning(SAIL).SAILimple-
mentsa’proposeandcheck’mechanismthattreatsannotatedanswersasalignmenttargetsratherthan
memorizationtargets,alignedresponsegetsaddedtothetrainingdataset,whileunalignedresponse
indicaterepresentationgap. Toadaptivelyaddresschallengingcases,theincrementalmulti-agent
curriculumbuildingprocessinSAILemploysaseriesofiterative’proposeandcheck’units,witheach
stageprovidingescalatinglevelsofsupport—fromdirecttohintedtoguidedpropositions. Questions
that remain unsolved are augmented and advanced to the next stage for further refinement. This
structuredthree-stageapproach,combinedwiththeinvolvementofmultiplehelperLLMs,ensures
the collection of a comprehensive and diverse set of responses. This process effectively targets
representationgaps,therebyenhancingthemodel’sadaptabilityandrobustness. SAILiteratively
augmentdataandperformSFTtoupdatethestudentmodel.
Iterative Graph Alignment We present a novel self-alignment algorithm, the Iterative Graph
Alignment(IGA),whichcombinestheenhancedthinkingandlearningalgorithmproposedabove. In
theIGAframework,ateacherVision-LanguageModel(VLM)employsIterativeGraphPrompting
to generate a logical graph and an answer for each question. The logical graph, embodying the
"whys,"andtheanswer,representingthe"what,"areusedtoinstructthestudentmodelthroughthe
Self-AlignedIncrementalLearning(SAIL)methodology. Inthiscontext,thetextualrepresentation
ofthelogicalgraphactsasahint,whiletheanswerprovidesguidanceandservesasthealignment
target. Thissetupsupportsanadaptive,alignment-basedlearningprocess,ensuringthatthestudent
modelnotonlyunderstandsthe"what"butalsotheunderlying"whys"ofeachanswer.
We curated a rule-alignment dataset of 1.5K queries with annotated responses to assess the per-
formance of Iterative Graph Prompting (IGP) and Iterative Graph Alignment (IGA) across five
3rule-based scenarios. Our empirical evaluations reveal significant enhancements: When applied
toanOracleVision-LanguageModel(VLM)suchasClaudeSonnet3.5[Anthropic,2024], IGP
improvedrulealignmentby73.2%relativetobaselineprompting. Furthermore,fine-tuningwithIGA,
asexemplifiedbyLlama3-Instruct,showedanimprovementof86.2%overthebaseline,matchingthe
performanceofClaudeSonnet3.5withthehelpofIGP.IGApresentsapromisingavenueforthe
self-improvementofLargeLanguageModels(LLMs),significantlynarrowingtherepresentationgap
andpromotingthedevelopmentofmorerobustandadaptablelanguagemodels.
Ourcontributionsareasfollows:
• WeintroduceIterativeGraphPrompting(IGP),whichharnessesvisuallanguagemodelsto
initializeandupdatelogicalreasoningwithinagraphstructure,enablingparallelprocessing
withglobalawareness.
• We present Self-Adaptive Incremental Learning (SAIL), a method that employs labeled
answersasalignmenttargetsratherthanmemorizationtargets,facilitatingbetterlearningin
open-endedconversations.
• Wedevelopanincrementalmulti-agentcurriculumbuildingprocessthatenablestraining-
timerepresentationgapdetectionandincrementalaugmentationofmorechallengingcases,
customizingthetrainingdatasetforeachmodel.
• Wepioneerthefirsttechniqueallowingpre-trainedlargelanguagemodelsachieveiterative
alignmentthroughgraph-basedreasoning.
• WecreateRuleAlign,adatasetcoveringfiverule-basedscenarioswith1.5Kdatapoints,and
demonstratethatIGPandIGAsignificantlyenhancerule-basedalignmentandperformance
overbaselineandproprietarymodels.
2 BackgroundandRelatedwork
AlignmentAlgorithmandPreferenceOptimization RLHF[Ouyangetal.,2022]requiresenor-
moushumanannotationstobuildrewardmodelswhichisusedtoalignLLMwithreinforcement
learning. Constitution AI [Bai et al., 2022] partially replaces human annotations with a hybrid
AI-humanapproachtobuildtheirpreferencemodels,butstillreliesheavilyonhumaninput. DPO
[Rafailovetal.,2024]andlatervariantssuchasSimPO[Mengetal.,2024]eliminatetheneedfora
separaterewardmodelbutoffernoclearadvantageovermethodslikeSTaR[Zelikmanetal.,2022]
especiallyundersituationsrequiringreasoning,accordingtoempiricalresultsin[Duetal.,2024].
Iterativeself-improvingLLMsystem Recentresearchhasexploredself-improvingsystemsthat
enhancemodelcapabilitieswithoutextensivehumanannotation. Thesesystemscombinea"thinking"
processthatgeneratessuperiorresponseswitha"learning"processtointegratetheseimprovements
iteratively. Self-rewardingLLMs[Yuanetal.,2024]useself-evaluationtoselectthebestresponses
forDirectPreferenceOptimization(DPO),whileMeta-rewardingLLMs[Wuetal.,2024]introducea
meta-judgetorefinetheevaluationlayeritself. DirectNashOptimization(DNO)[Rossetetal.,2024]
employsanoraclemodeltoassistinbothgenerationandselection,avoidingperformancesaturation
moreeffectively. However,topreventmodelcollapse[Shumailovetal.,2024],thesesystemsoften
resettothebasemodelforeachiteration,effectivelyturningtheprocessintoadataaugmentation
pipeline. Specialfocuson"hardcases"asdemonstratedbymorecapableLLMsisfirsthighlighted
byLLM2LLM[Leeetal.,2024].
Thinking mechanism Chain of thought (CoT) [Wei et al., 2022] improves reasoning ability
of language model through connecting overlapping localized knowledge in its pre-traing corpus
[Prystawskietal.,2023]. AsCoTinnatelyrequiressearching[Yeetal.,2024],explicittreesearch
mechanism could further enhance it [Yao et al., 2023]. Language agent tree search (LATS) also
includesfeedback,self-evaluation,andMCTS[Zhouetal.,2024],achievingbetterperformanceby
deliberatelygoingthrougheachthreadofthoughts. However,theysuffersfromsignificantlatency
increase,andshowssignificantlimitationwhenfacingproblemwhichareeasyundervisualmodality
[Menonetal.,2024],planningwithgraph-likestructure[Linetal.,2024]hasalsoshownadvantage,
despiterelyingstillonlanguagemodality.
4EfficientAlignmentinLLMs RecentresearchhasfocusedonenhancingLLMs’alignmentthrough
varioussupervisedfine-tuningtechniques. Theseapproachesgenerallyaimtoaugmentsupervision
datasetsusingLLM-generateddata. Forinstance,[Zelikmanetal.,2022]leveragesprovidedanswers
toiterativelygeneratediversereasoningpaths,selectingthosethatleadtocorrectoutcomes,[Lietal.,
2022]extendsthistodistilltherationalesofferedfromateachermodelintoastudentmodels. [Lee
etal.,2024]takesatargetedapproachbyidentifyingcaseswhereLLMsproduceincorrectoutputs
andconcentratingaugmentationeffortsontheseinstances. Thesestudiessupportourhypothesisthat
forspecifictasks,asetofkeyassociationsisrequired,andfocusedaugmentationcombinedwith
supervisedlearningcansignificantlyenhanceLLMs’reasoningabilities.
3 Methodology
WebootstrapreasoningabilityofastrongVLMwithexplicitgraphreasoning,logicaloperationsare
carriedoutwithinthegraphtofurtherunderstandtheproblem. Thisistheadaptivereasoningwith
graphsection. Suchgraphisusedtodetecttheweaknessofthelanguagemodel,andtheniteratively
enhanceitsunderstandingofthecurrentproblem.
3.1 IterativeGraphPrompting
Figure3: IterativeGraphPrompting(IGP)
Givenarules, aqueryx, weuseaVLMtoobtainananswery, togetherwithalogicalgraphG
whichcontainsthethoughtprocessofprovidingtherule-alignedresponse.
WeasktheVLMtogeneratenarratedversionofagraphGwhichisessentiallyaJSONformattedlist
of(entity,relation,entity)triplets. WecouldalsoplotsuchgraphGintoanimageΦ(G),whichcan
beprompteddirectlyintotheVLMagain.
SelfEvaluation Webeginwithaself-check,evaluatingtheLLM’sinitialresponseforconsistency
withitsownjudgement:
y ∼π(s,x), π (s,x,y) (1)
eval
where s is the instruction, x the query, y the answer, π the LLM’s response function, and π
eval
indicatestheevaluationoftheLLMonitsownresponse.
Forquerywhichthemodelfailstoprovideasatisfactoryresponse,weproceedwithiterativegraph
constructionprocess.Giventheinstructionandquery,weasktheVLMtoprovideitslogicalreasoning
processintheformof(entity,relation,entity)triplets,whichweusetoinitializeagraphG .
1
IterativeRefinement Fori=1tokiterations:
G =π (s,x,ϕ(G )) (2)
i refine i−1
whereϕ(G )isthevisualrepresentationofthegraphfromthepreviousiteration,andπ isthe
i−1 refine
LLM’sgraphrefinementfunction.
5VisualPrompting Weleveragethegraphforvisualprompting,enhancingtheLLM’sreasoning
capabilities:
y ∼π(s,x,ϕ(G)) (3)
WeobserveempiricallythatstrongLLMs(GPT-4andSonnet-3.5)performbettergiventhegraphas
animageΦ(G)comparedtodescriptivetextG.
WeprovideIterativeGraphPrompting(IGP)inAlgorithm(3.1)anditsdiagraminFigure(3)
Algorithm1IterativeGraphPrompting
Require: s(rule),x(query),visionlanguagemodelπ
Ensure: y
1: y ∼π(s,x)
2: ifπ (s,x,y)then ▷NaiveInferenceisaccepted
eval
3: returnr
4: else
5: G 1 ←π initialize(s,x) ▷InitializeGraph
6: i←1
7: whileG i ̸=G i−1do
8: G i+1 ←π refine(s,x,Φ(G i)) ▷IterateonGraph
9: i←i+1
10: endwhile
11: y ∼π(s,x,ϕ(G i))
12: returny
13: endif
3.2 Self-AlignedIncrementalLearning
Webeginwiththreecoreintuitionsaboutthecurrentprocessoftraininglanguagemodels,arguing
theneedforaparadigmshift. Firstly,whengivenanannotatedanswer,alanguagemodelshould
recognizethatany’aligned’answerisacceptable,ratherthanbeingconstrainedtobelievethisisthe
solecorrectresponse. Traditionalfine-tuningapproaches,however,treattheannotatedanswerasa
definitivetarget,aimingtominimizethelikelihoodofdeviatingfromit.
Oursecondintuitionpositsthatlanguagemodelsshouldlearnthethoughtprocessesbehindgenerating
answers,notjusttheanswersthemselves. Thisapproachisakintoteachingamantofish,which
sustainshimforalifetime,ratherthangivinghimafish,whichfeedshimforaday.
Thirdly,thelearningprocessshouldbeindividuallytailoredtoeachmodel,akintocustomizingedu-
cationfordifferentstudents. Trainingshouldfocusincrementallyonscenariosthatposechallenges,
ensuringthateachstepiscalibratedtoaddressandovercomethespecificdifficultiesencounteredby
themodel. Thismethodensuresthatlearningisnotonlyadaptivebutalsoprogressivelytargetsareas
ofweakness,fosteringamorerobustandcomprehensiveunderstandingovertime.
Figure4: Self-AlignedIncrementalLearning(SAIL)
6Algorithm2Propose&Check(PAC)
Require: Modelπ,solvedcasesD,unsolvedcasesD ,stagenumbern
unsolved stage
Ensure: UpdatedDandD
unsolved
1: foreach(x,y,G)inD do
unsolved
2: ifn =1then
stage
3: y
propose
∼π propose(x) ▷Directproposition
4: elseifn =2then
stage
5: y ∼π (x,G) ▷Hintedproposition
propose propose
6: else
7: y
propose
∼π propose(x,y,G) ▷Guidedproposition
8: endif
9: e∼π check(y propose,y) ▷Checkalignment
10: ife=Truethen
11: D ←D∪{(x,y )}
propose
12: D ←D \{(x,y,G)}
unsolved unsolved
13: endif
14: endfor
15: returnD,D
unsolved
Algorithm3Self-AlignedIncrementalLearning(SAIL)
Require: InitialdatasetD , baseLLMπ , helperLLMs{π ,...,π }, numberofiterationsT,
0 0 h1 hm
repetitionfactorsn ,n
2 3
Ensure: TrainedLLMπ
T
1: D ←∅
2: D unsolved ←D 0
3: fort=1 to T do
4: D,D
unsolved
← PAC(π t−1,D,D unsolved,1)
5: D
←(cid:83)n2
D ▷Augmentmoderatedifficultcase
unsolved i=1 unsolved
6: D,D
unsolved
← PAC(π t−1,D,D unsolved,2)
7: foreachπ hin{π h1,...,π hm}do
8: D,D
unsolved
← PAC(π h,D,D unsolved,2)
9: endfor
10: D
←(cid:83)n3
D ▷Augmentmostchallengingcase
unsolved i=1 unsolved
11: D,D
unsolved
← PAC(π t−1,D,D unsolved,3)
12: foreachπ hin{π h1,...,π hm}do
13: D,D
unsolved
← PAC(π h,D,D unsolved,3)
14: endfor
15: π t ← SFT(π 0,D) ▷SupervisedFine-Tuning
16: endfor
17: returnπ T
Self-AlignedIncrementalLearning(SAIL)(seeFigure3)introducesanovelparadigmforaligning
LargeLanguageModels(LLMs). Atitscore,SAILemploysa’proposeandcheck’methodology(see
Algorithm2),whichgeneratesarangeofthoughtsandanswers,thencheckseachagainstannotated
answersforalignment. Thisself-alignmentprocessisseamlesslyintegratedintoSupervisedFine-
Tuning(SFT),movingbeyonduniformtrainingwithstaticquestion-answerpairs.SAILincrementally
selectsandaugmentscaseswithinthetrainingdatasetbasedondifficultyandthemodel’scurrent
capabilitiesthroughamulti-agentincrementalcurriculumbuildingprocess.
Thisprocessunfoldsinthreeprogressivestages,eachofferingincreasinglevelsofsupport. Inthe
directpropositionstage,thestudentmodelindependentlyproposessolutions. Unsolvedcasesare
thenaugmentedandadvancedtothehintedpropositionstage,wherethemodelreceivesadditional
contexttoencouragebuildinguponexistingknowledge. Finally, intheguidedpropositionstage,
remainingchallengingcasesarepresentedwithbothhintsandannotatedanswers,promptingthe
modeltoproposesimilarsolutionswhilegraspingtheunderlyingreasoning.
To enhance the diversity of responses and effectively patch representation gaps, SAIL employs
multiple helper models in the hinted and guided stages. These helper models, with their varied
7capabilitiesandknowledgebases,contributetoamorecomprehensiveexplorationofsolutionspaces.
Thiscollectiveintelligenceapproachensuresarich,diversesetofresponses,helpingtoaddressblind
spotsinthestudentmodel’sunderstanding. SAILispresentedinAlgorithm(3).
3.3 IterativeGraphAlignment
IterativeGraphAlignment(IGA)aimstocombinethepowerofIGPandSAILtoachieveefficient
alignmentwithminimalhumansupervision. IGPnaturallyprovidesalogicalgraphG,togetherwith
athoughtfulresponser,wecouldnarratethegraphtoformthehintϕ(G). Thisprovidesannotations
withoutanyhumaninput. Optionally,extrahumaneditingcouldbeleveragedtofurtherenhancethe
qualityofsuchadataset. Thealgorithmisdisplayedin3.3
Algorithm4IterativeGraphAlignment(IGA)
Require: TeacherVLMπ ,studentLLMπ ,helperLLMs{π ,...,π },queries{x }N ,rule
vlm 0 h1 hm i i=1
r,numberofiterationsT
Ensure: TrainedLLMπ
T
D ←∅ ▷Initializedataset
fori=1toN do
(y ,G )←IGP(s,x ,π ) ▷IterativeGraphPrompting
i i i vlm
4: D ←D∪{(x i,y i,G i)}
endfor
π ←SAIL(π ,{π ,...,π },D,T) ▷Self-AlignedIncrementalLearning
T 0 h1 hm
returnπ
T
4 ExperimentalSetupandResults
To evaluate Iterative Graph Alignment (IGA), we developed a test suite ’RuleAlign’ consists of
fiverule-basedscenarios,eachcomprising100testand200trainingqueries. Thesequeries,which
mix in-domain and out-of-domain examples, are designed to assess rule-alignment capacity of a
LLM.Eachqueryisopen-endedtobettermimicpracticalscenarios. Inordertoevaluateaproposed
response,wecheckwhetheritalignswiththereferenceanswercollectedfromhuman. Thecomplete
datasetandevaluationsuiteisavailableatRuleAlign.
PromptingTechniquesComparison Toachieveannotation-freerule-basedalignment,ahighlevel
ofruleadherencefromIterativeGraphPrompting(IGP)iscritical,asitwillbeusedtoteachthe
studentLLM,alongwiththeobtainedlogicalgraph. WecompareIGPagainstnaivepromptingand
chain-of-thought(CoT)prompting[Kojimaetal.,2023],whichisappliedincurrentself-alignment
algorithms[Yuanetal.,2024,Rossetetal.,2024,Baietal.,2022].
WeconductourexperimentswithClaudeSonnet-3.5[Anthropic,2024],withresultspresentedin
Table1. Inpractice,weuse2iterationsforIGPtobalanceperformanceandspeed. WefoundIGP
achievesanaverage73.12%relativeimprovementinrule-basedalignmentcomparedtothebaseline.
Interestingly,weobserveddegradedruleadherencefromCoTinroleplayscenarios. Wesuspectthis
isduetothe’AIAssistantself-identification’biasintheseinstructmodels,whichisretrievedwith
CoT,aligningwithinsightsfrom[Prystawskietal.,2023].
Inroleplayscenarios,wefoundthatthemodeltendstorepeatthesamequestionwhenpresentedwith
anatypicalquery(e.g.,"Whatarethestorehours?"askedtoanLLMroleplayingasacustomer).
By inspecting the two logical graphs generated through IGP, we identified that while the model
recognizesthatacustomertypicallyaskssuchquestions, itfailstodeducethat, asacustomer, it
should decline to answer rather than ask the question itself. By explicitly including prompts in
thegraphrefinementprocess,weeffectively’engineeritsthoughtprocess’,instructingittodecide
whetherto’answer’or’reject’thequestionafteridentifyingitsrole. Thissuggestspotentialinthe
areaof’thoughtflowengineering’.
Learning Techniques Comparison Although Iterative Graph Alignment (IGA) requires zero
humanannotation,unlikeSupervisedFine-Tuning(SFT)andSelf-TaughtReasoner(STaR)[Zelikman
etal.,2022],wecompareIGAwiththesemethodsusingteacherVLMannotationscollectedthrough
8IGPtoensureafaircomparison. ForSelf-AlignedIncrementalLearning(SAIL),SFT,andSTaR,
wechoseLlama3-8B-Instruct[llama3teams,2024]asourstudentmodelsandappliedLoRA[Hu
etal.,2022],whichismorecommonlyadoptedinresource-limitedscenarios. Weusedthesame
hyper-parametersforLoRAacrossallthreemethods.
Table2presentsourcomparisonresults. Onaverage,IGAachievesarelativeimprovementof86.20%
overthebaseline. Notably,anIGAfine-tuned8BmodeloutperformsClaudeSonnet-3.5,matching
itsIGP-enhancedresponsesinrule-basedalignment. ShowingtheeffectivenessofIGAinpatching
representationgapandachievingannotation-freerule-basedalignment.
Table1: Comparisonofeffectivenessacrossdifferentpromptingstrategies(Claude).
Scenario Naive CoT IGP
Donottalkaboutelephant 76.0% 82.5% 97.0%
Roleplayascustomer 51.6% 42.5% 96.6%
Roleplayassalesperson 54.2% 38.2% 98.2%
Roleplayaspatient 43.1% 34.6% 97.2%
Connectusertohumanwhenrequested 65.4% 76.2% 94.2%
RelativeimprovementoverNaivebaseline(%)
Donottalkaboutelephant - 8.55% 27.63%
Roleplayascustomer - -17.64% 87.21%
Roleplayassalesperson - -29.52% 81.18%
Roleplayaspatient - -19.72% 125.52%
Connectusertohumanwhenrequested - 16.51% 44.04%
Averagerelativeimprovement - -8.36% 73.12%
Table2: Comparisonofeffectivenessacrossdifferentpromptingstrategies(Llama3).
Scenario Naive CoT SFT STaR IGA
Donottalkaboutelephant 73.0% 81.2% 53.6% 84.6% 96.5%
Roleplayascustomer 52.4% 10.2% 4.4% 65.8% 98.0%
Roleplayassalesperson 53.2% 12.6% 3.2% 74.2% 96.8%
Roleplayasapatient 36.0% 10.6% 4.0% 68.4% 97.4%
Connectusertohumanwhenrequested 61.4% 74.2% 48.2% 80.8% 97.8%
RelativeimprovementoverNaivebaseline(%)
Donottalkaboutelephant - 11.23% -26.58% 15.89% 32.19%
Roleplayascustomer - -80.53% -91.60% 25.57% 87.02%
Roleplayassalesperson - -76.32% -93.98% 39.47% 81.95%
Roleplayasapatient - -70.56% -88.89% 90.00% 170.56%
Connectusertohumanwhenrequested - 20.85% -21.50% 31.60% 59.28%
Averagerelativeimprovement - -39.07% -64.51% 40.51% 86.20%
5 LimitationsandFutureWork
MaintainingtheaccuracyofevaluationsiscriticalforSAIL.Ourresearchindicatesthatself-alignment
checksoftenfavorthemodel’sowngeneratedresponses,potentiallyleadingtothemisidentificationof
complexcases. Thisbiascanskewthedistributionoftrainingdata,resultinginsuboptimaloutcomes.
Additionally,althoughourapproachexcelsatdetectingrepresentationgaps,itprimarilyfocuseson
augmentingresponseswithdiversitytoindividualquestions. Thisstrategyisfrequentlyinsufficient
fordisciplineslikemathematicsandcoding,whichalsorequireawiderangeofquestiontypes. We
advocateforfutureresearchtoemphasizeautomaticdataaugmentationbasedon’meta-logic’to
improvetheresilienceandversatilityofourtechniques.
Ultimately, weaspiretodevelopanidealversionoftheIntegratedGrowthandReasoning(IGA)
systemasafullyautonomousself-enhancementframeworkforVision-LanguageModels(VLMs).
ThissystemwouldintegratereasoningenhancementsfromIntegratedGrowthProcesses(IGP)into
themodel’slearningseamlessly. Moreover,itwouldusevisualgroundingthroughfeedbackfrom
9virtualenvironmentstotacklethechallengesinevaluationandaugmentation,particularlyinlightof
theextensiveonlineinteractiondata(see[Puttaetal.,2024]). Byincorporatingtheseinnovations,we
aimtoconstructamoreadvancedandself-optimizingIGAsystemcapableofadaptingandevolving
withouthumanoversight.
References
Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage
andextraction. arXivpreprintarXiv:2309.14316,2024. URLhttps://arxiv.org/abs/2309.
14316.
Anthropic. Claude 3.5 sonnet. Blog, 2024. URL https://www.anthropic.com/news/
claude-3-5-sonnet.
YuntaoBai,SauravKadavath,SandipanKundu,AmandaAskell,JohnKernion,AndyJones,Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson,
ChristopherOlah,DannyHernandez,DawnDrain,DeepGanguli,DustinLi,EliTran-Johnson,
EPerez,JamieKerr,JaredMueller,JeffLadish,JLandau,KamalNdousse,Kamile˙ Luko,Liane
Lovitt,MichaelSellitto,NelsonElhage,NicholasSchiefer,Noem’iMercado,NovaDassarma,
RobertLasenby, RobinLarson, SamRinger, ScottJohnston, ShaunaKravec, SheerElShowk,
StanislavFort,TameraLanham,TimothyTelleen-Lawton,TomConerly,TomHenighan,Tristan
Hume,SamBowman,ZacHatfield-Dodds,BenjaminMann,DarioAmodei,NicholasJoseph,Sam
McCandlish, TomB.Brown, andJaredKaplan. Constitutionalai: Harmlessnessfromaifeed-
back. ArXiv,abs/2212.08073,2022. URLhttps://api.semanticscholar.org/CorpusID:
254823489.
LukasBerglund,MegTong,MaxKaufmann,MikitaBalesni,AsaCooperStickland,TomaszKorbak,
andOwainEvans. Thereversalcurse: LLMstrainedon"aisb"failtolearn"bisa". arXivpreprint
arXiv:2309.12288,2024.
NealJ.CohenandLarryR.Squire. Preservedlearningandretentionofpattern-analyzingskillin
amnesia: Dissociationofknowinghowandknowingthat. Science,210(4466):207–210,1980.
GrégoireDelétang,AnianRuoss,Paul-AmbroiseDuquenne,ElliotCatt,TimGenewein,Christopher
Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus
Hutter,andJoelVeness. Languagemodelingiscompression. arXivpreprintarXiv:2309.10668,
2024. URLhttps://arxiv.org/abs/2309.10668.
YuqingDu,AlexanderHavrilla,SainbayarSukhbaatar,PieterAbbeel,andRobertaRaileanu. Astudy
onimprovingreasoninginlanguagemodels. ICan’tBelieveIt’sNotBetterWorkshop: Failure
ModesintheAgeofFoundationModels,2024. URLhttps://openreview.net/forum?id=
tCZFmDyPFm.
Evelina Fedorenko, Steven Piantadosi, and Edward Gibson. Language is primarily a tool for
communicationratherthanthought. Nature,630,2024.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
andWeizhuChen.LoRA:Low-rankadaptationoflargelanguagemodels.InternationalConference
onLearningRepresentations,2022.
YuzhenHuang,JinghanZhang,ZifeiShan,andJunxianHe. Compressionrepresentsintelligence
linearly. arXiv:2404.09937,2024.
Subbarao Kambhampati. Can large language models reason and plan? Annals of the New York
Academy of Sciences, 2024. ISSN 1749-6632. URL http://dx.doi.org/10.1111/nyas.
15125.
TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusukeIwasawa. Large
languagemodelsarezero-shotreasoners. arXivpreprintarXiv:2205.11916,2023. URLhttps:
//arxiv.org/abs/2205.11916.
10HarrisonLee,SamratPhatale,HassanMansoor,ThomasMesnard,JohanFerret,KellieLu,Colton
Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlaif: Scaling
reinforcementlearningfromhumanfeedbackwithaifeedback. arXivpreprintarXiv:2309.00267,
2023. URLhttps://arxiv.org/abs/2309.00267.
NicholasLee,ThanakulWattanawong,SehoonKim,KarttikeyaMangalam,ShengShen,Gopala
Anumanchipali,MichaelW.Mahoney,KurtKeutzer,andAmirGholami. LLM2LLM:Boosting
LLMswithnoveliterativedataenhancement. arXivpreprintarXiv:2403.15042,2024.
ShiyangLi, JianshuChen, YelongShen, ZhiyuChen, XinluZhang, ZekunLi, HongWang, Jing
Qian,BaolinPeng,YiMao,WenhuChen,andXifengYan. Explanationsfromlargelanguage
models make small reasoners better. arXiv preprint arXiv:2210.06726, 2022. URL https:
//arxiv.org/abs/2210.06726.
FangruLin,EmanueleLaMalfa,ValentinHofmann,ElleMichelleYang,AnthonyCohn,andJanetB.
Pierrehumbert. Graph-enhancedlargelanguagemodelsinasynchronousplanreasoning. arXiv
preprintarXiv:2402.02805,2024. URLhttps://arxiv.org/abs/2402.02805.
Meta llama3 teams. The llama 3 herd of models. arXiv preprint: 2024.21783, 2024. URL
https://arxiv.org/abs/2407.21783.
YuMeng,MengzhouXia,andDanqiChen. Simpo: Simplepreferenceoptimizationwithareference-
free reward. arXiv preprint arXiv:2405.14734, 2024. URL https://arxiv.org/abs/2405.
14734.
SachitMenon,RichardZemel,andCarlVondrick. Whiteboard-of-thought: Thinkingstep-by-step
acrossmodalities. arXiv,2024. URLhttps://arxiv.org/abs/2406.14562.
OpenAI. Gpt-4o. Blog,2024. URLhttps://openai.com/index/gpt-4o-system-card.
LongOuyang,JeffWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,JohnSchulman,JacobHilton,FraserKelton,
LukeMiller,MaddieSimens,AmandaAskell,PeterWelinder,PaulChristiano,JanLeike,and
RyanLowe. Traininglanguagemodelstofollowinstructionswithhumanfeedback. arXivpreprint
arXiv:2203.02155,2022. URLhttps://arxiv.org/abs/2203.02155.
BenPrystawski,MichaelY.Li,andNoahD.Goodman. Whythinkstepbystep? reasoningemerges
fromthelocalityofexperience. arXiv,2023. URLhttps://arxiv.org/abs/2304.03843.
PranavPutta,EdmundMills,NamanGarg,SumeetMotwani,ChelseaFinn,DivyanshGarg,and
RafaelRafailov. Agentq: Advancedreasoningandlearningforautonomousaiagents. arXiv,2024.
URLhttps://arxiv.org/abs/2408.07199.
RafaelRafailov,ArchitSharma,EricMitchell,StefanoErmon,ChristopherD.Manning,andChelsea
Finn. Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel. arXiv
preprintarXiv:2305.18290,2024. URLhttps://arxiv.org/abs/2305.18290.
Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and
TengyangXie. Directnashoptimization: Teachinglanguagemodelstoself-improvewithgeneral
preferences. arXiv preprint arXiv:2404.03715, 2024. URL https://arxiv.org/abs/2404.
03715.
I.Shumailov,Z.Shumaylov,Y.Zhao,etal. Aimodelscollapsewhentrainedonrecursivelygenerated
data. Nature,631:755–759,2024.
AviSingh,JohnD.Co-Reyes,RishabhAgarwal,AnkeshAnand,PiyushPatil,XavierGarcia,PeterJ.
Liu,JamesHarrison,JaehoonLee,KelvinXu,AaronParisi,AbhishekKumar,AlexAlemi,Alex
Rizkowsky,AzadeNova,BenAdlam,BerndBohnet,GamaleldinElsayed,HanieSedghi,Igor
Mordatch,IsabelleSimpson,IzzeddinGur,JasperSnoek,JeffreyPennington,JiriHron,Kathleen
Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi,
NoahConstant,RomanNovak,RosanneLiu,TrisWarkentin,YundiQian,YaminiBansal,Ethan
Dyer,BehnamNeyshabur,JaschaSohl-Dickstein,andNoahFiedel. Beyondhumandata: Scaling
self-trainingforproblem-solvingwithlanguagemodels. arXivpreprintarXiv:2312.06585,2024.
URLhttps://arxiv.org/abs/2312.06585.
11BoshiWang, XiangYue, YuSu, andHuanSun. Grokkedtransformersareimplicitreasoners: A
mechanisticjourneytotheedgeofgeneralization. arXivpreprintarXiv:2405.15071,2024.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,
QuocV.Le,andDennyZhou. Chain-of-thoughtpromptingelicitsreasoninginlargelanguage
models. Proceedings of the 36th International Conference on Neural Information Processing
Systems,2022.
TianhaoWu,WeizheYuan,OlgaGolovneva,JingXu,YuandongTian,JiantaoJiao,JasonWeston,
and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with
llm-as-a-meta-judge. arXivpreprintarXiv:2407.19594,2024. URLhttps://arxiv.org/abs/
2407.19594.
ShunyuYao, DianYu, JeffreyZhao, IzhakShafran, ThomasL.Griffiths, YuanCao, andKarthik
Narasimhan. Treeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels. arXiv
preprintarXiv:2305.10601,2023. URLhttps://arxiv.org/abs/2305.10601.
TianYe,ZichengXu,YuanzhiLi,andZeyuanAllen-Zhu. Physicsoflanguagemodels: Part2.1,
grade-school math and the hidden reasoning process. arXiv preprint arXiv:2407.20311, 2024.
URLhttps://arxiv.org/abs/2407.20311.
Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv:
2407.06023,2024.
WeizheYuan,RichardYuanzhePang,KyunghyunCho,XianLi,SainbayarSukhbaatar,JingXu,and
JasonWeston. Self-rewardinglanguagemodels. arXivpreprintarXiv:2401.10020,2024. URL
https://arxiv.org/abs/2401.10020.
Eric Zelikman, Yuhuai Wu, Jiasi Mu, and Noah Goodman. Star: Bootstrapping reasoning with
reasoning. AdvancesinNeuralInformationProcessingSystems,35:15476–15488,2022.
AndyZhou,KaiYan,MichalShlapentokh-Rothman,HaohanWang,andYu-XiongWang. Language
agenttreesearchunifiesreasoningactingandplanninginlanguagemodels. ICML2024,2024.
12