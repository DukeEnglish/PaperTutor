[
    {
        "title": "SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners",
        "authors": "Ziyu GuoRenrui ZhangXiangyang ZhuChengzhuo TongPeng GaoChunyuan LiPheng-Ann Heng",
        "links": "http://arxiv.org/abs/2408.16768v1",
        "entry_id": "http://arxiv.org/abs/2408.16768v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16768v1",
        "summary": "We introduce SAM2Point, a preliminary exploration adapting Segment Anything\nModel 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point\ninterprets any 3D data as a series of multi-directional videos, and leverages\nSAM 2 for 3D-space segmentation, without further training or 2D-3D projection.\nOur framework supports various prompt types, including 3D points, boxes, and\nmasks, and can generalize across diverse scenarios, such as 3D objects, indoor\nscenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple\n3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight\nthe robust generalization capabilities of SAM2Point. To our best knowledge, we\npresent the most faithful implementation of SAM in 3D, which may serve as a\nstarting point for future research in promptable 3D segmentation. Online Demo:\nhttps://huggingface.co/spaces/ZiyuG/SAM2Point . Code:\nhttps://github.com/ZiyuGuo99/SAM2Point .",
        "updated": "2024-08-29 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16768v1"
    },
    {
        "title": "How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models",
        "authors": "Jiyue JiangLiheng ChenPengan ChenSheng WangQinghang BaoLingpeng KongYu LiChuan Wu",
        "links": "http://arxiv.org/abs/2408.16756v1",
        "entry_id": "http://arxiv.org/abs/2408.16756v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16756v1",
        "summary": "The rapid evolution of large language models (LLMs) has transformed the\ncompetitive landscape in natural language processing (NLP), particularly for\nEnglish and other data-rich languages. However, underrepresented languages like\nCantonese, spoken by over 85 million people, face significant development gaps,\nwhich is particularly concerning given the economic significance of the\nGuangdong-Hong Kong-Macau Greater Bay Area, and in substantial\nCantonese-speaking populations in places like Singapore and North America.\nDespite its wide use, Cantonese has scant representation in NLP research,\nespecially compared to other languages from similarly developed regions. To\nbridge these gaps, we outline current Cantonese NLP methods and introduce new\nbenchmarks designed to evaluate LLM performance in factual generation,\nmathematical logic, complex reasoning, and general knowledge in Cantonese,\nwhich aim to advance open-source Cantonese LLM technology. We also propose\nfuture research directions and recommended models to enhance Cantonese LLM\ndevelopment.",
        "updated": "2024-08-29 17:54:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16756v1"
    },
    {
        "title": "Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning of Large Language Models",
        "authors": "Alec Solway",
        "links": "http://arxiv.org/abs/2408.16753v1",
        "entry_id": "http://arxiv.org/abs/2408.16753v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16753v1",
        "summary": "Reinforcement learning is used to align language models with human preference\nsignals after first pre-training the model to predict the next token of text\nwithin a large corpus using likelihood maximization. Before being deployed in a\nspecific domain, models are often further fine-tuned on task specific data.\nSince human preferences are often unavailable for the last step, it is\nperformed using likelihood maximization as that is the typical default method.\nHowever, reinforcement learning has other advantages besides facilitating\nalignment to a human derived reward function. For one, whereas likelihood\nmaximization is a form of imitation learning in which the model is trained on\nwhat to do under ideal conditions, reinforcement learning is not limited to\ndemonstrating actions just for optimally reached states and trains a model what\nto do under a range of scenarios as it explores the policy space. In addition,\nit also trains a model what not to do, suppressing competitive but poor\nactions. This work develops a framework for last-mile fine-tuning using\nreinforcement learning and tests whether it garners performance gains. The\nexperiments center on abstractive summarization, but the framework is general\nand broadly applicable. Use of the procedure produced significantly better\nresults than likelihood maximization when comparing raw predictions. For the\nspecific data tested, the gap could be bridged by employing post-processing of\nthe maximum likelihood outputs. Nonetheless, the framework offers a new avenue\nfor model optimization in situations where post-processing may be less\nstraightforward or effective, and it can be extended to include more complex\nclasses of undesirable outputs to penalize and train against, such as\nhallucinations.",
        "updated": "2024-08-29 17:49:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16753v1"
    },
    {
        "title": "A Gradient Analysis Framework for Rewarding Good and Penalizing Bad Examples in Language Models",
        "authors": "Yi-Lin TuanWilliam Yang Wang",
        "links": "http://arxiv.org/abs/2408.16751v1",
        "entry_id": "http://arxiv.org/abs/2408.16751v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16751v1",
        "summary": "Beyond maximum likelihood estimation (MLE), the standard objective of a\nlanguage model (LM) that optimizes good examples probabilities, many studies\nhave explored ways that also penalize bad examples for enhancing the quality of\noutput distribution, including unlikelihood training, exponential maximizing\naverage treatment effect (ExMATE), and direct preference optimization (DPO). To\nsystematically compare these methods and further provide a unified recipe for\nLM optimization, in this paper, we present a unique angle of gradient analysis\nof loss functions that simultaneously reward good examples and penalize bad\nones in LMs. Through both mathematical results and experiments on\nCausalDialogue and Anthropic HH-RLHF datasets, we identify distinct functional\ncharacteristics among these methods. We find that ExMATE serves as a superior\nsurrogate for MLE, and that combining DPO with ExMATE instead of MLE further\nenhances both the statistical (5-7%) and generative (+18% win rate)\nperformance.",
        "updated": "2024-08-29 17:46:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16751v1"
    },
    {
        "title": "Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge",
        "authors": "Beidi DongJin R. LeeZiwei ZhuBalassubramanian Srinivasan",
        "links": "http://arxiv.org/abs/2408.16749v1",
        "entry_id": "http://arxiv.org/abs/2408.16749v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16749v1",
        "summary": "The United States has experienced a significant increase in violent\nextremism, prompting the need for automated tools to detect and limit the\nspread of extremist ideology online. This study evaluates the performance of\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformers (GPT) in detecting and classifying online domestic\nextremist posts. We collected social media posts containing \"far-right\" and\n\"far-left\" ideological keywords and manually labeled them as extremist or\nnon-extremist. Extremist posts were further classified into one or more of five\ncontributing elements of extremism based on a working definitional framework.\nThe BERT model's performance was evaluated based on training data size and\nknowledge transfer between categories. We also compared the performance of GPT\n3.5 and GPT 4 models using different prompts: na\\\"ive, layperson-definition,\nrole-playing, and professional-definition. Results showed that the best\nperforming GPT models outperformed the best performing BERT models, with more\ndetailed prompts generally yielding better results. However, overly complex\nprompts may impair performance. Different versions of GPT have unique\nsensitives to what they consider extremist. GPT 3.5 performed better at\nclassifying far-left extremist posts, while GPT 4 performed better at\nclassifying far-right extremist posts. Large language models, represented by\nGPT models, hold significant potential for online extremism classification\ntasks, surpassing traditional BERT models in a zero-shot setting. Future\nresearch should explore human-computer interactions in optimizing GPT models\nfor extremist detection and classification tasks to develop more efficient\n(e.g., quicker, less effort) and effective (e.g., fewer errors or mistakes)\nmethods for identifying extremist content.",
        "updated": "2024-08-29 17:43:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16749v1"
    }
]