[
    {
        "title": "Auricular Vagus Nerve Stimulation for Enhancing Remote Pilot Training and Operations",
        "authors": "William J. Tyler",
        "links": "http://arxiv.org/abs/2408.16755v1",
        "entry_id": "http://arxiv.org/abs/2408.16755v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16755v1",
        "summary": "The rapid growth of the drone industry, particularly in the use of small\nunmanned aerial systems (sUAS) and unmanned aerial vehicles (UAVs), requires\nthe development of advanced training protocols for remote pilots. Remote pilots\nmust develop a combination of technical and cognitive skills to manage the\ncomplexities of modern drone operations. This paper explores the integration of\nneurotechnology, specifically auricular vagus nerve stimulation (aVNS), as a\nmethod to enhance remote pilot training and performance. The scientific\nliterature shows aVNS can safely improve cognitive functions such as attention,\nlearning, and memory. It has also been shown useful to manage stress responses.\nFor safe and efficient sUAS/UAV operation, it is essential for pilots to\nmaintain high levels of vigilance and decision-making under pressure. By\nmodulating sympathetic stress and cortical arousal, aVNS can prime cognitive\nfaculties before training, help maintain focus during training and improve\nstress recovery post-training. Furthermore, aVNS has demonstrated the potential\nto enhance multitasking and cognitive control. This may help remote pilots\nduring complex sUAS operations by potentially reducing the risk of impulsive\ndecision-making or cognitive errors. This paper advocates for the inclusion of\naVNS in remote pilot training programs by proposing that it can provide\nsignificant benefits in improving cognitive readiness, skill and knowledge\nacquisition, as well as operational safety and efficiency. Future research\nshould focus on optimizing aVNS protocols for drone pilots while assessing\nlong-term benefits to industrial safety and workforce readiness in real-world\nscenarios.",
        "updated": "2024-08-29 17:53:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16755v1"
    },
    {
        "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
        "authors": "Zhifei XieChangqiao Wu",
        "links": "http://arxiv.org/abs/2408.16725v1",
        "entry_id": "http://arxiv.org/abs/2408.16725v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16725v1",
        "summary": "Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research.",
        "updated": "2024-08-29 17:18:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16725v1"
    },
    {
        "title": "VMC: A Grammar for Visualizing Statistical Model Checks",
        "authors": "Ziyang GuoAlex KaleMatthew KayJessica Hullman",
        "links": "http://arxiv.org/abs/2408.16702v1",
        "entry_id": "http://arxiv.org/abs/2408.16702v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16702v1",
        "summary": "Visualizations play a critical role in validating and improving statistical\nmodels. However, the design space of model check visualizations is not well\nunderstood, making it difficult for authors to explore and specify effective\ngraphical model checks. VMC defines a model check visualization using four\ncomponents: (1) samples of distributions of checkable quantities generated from\nthe model, including predictive distributions for new data and distributions of\nmodel parameters; (2) transformations on observed data to facilitate\ncomparison; (3) visual representations of distributions; and (4) layouts to\nfacilitate comparing model samples and observed data. We contribute an\nimplementation of VMC as an R package. We validate VMC by reproducing a set of\ncanonical model check examples, and show how using VMC to generate model checks\nreduces the edit distance between visualizations relative to existing\nvisualization toolkits. The findings of an interview study with three expert\nmodelers who used VMC highlight challenges and opportunities for encouraging\nexploration of correct, effective model check visualizations.",
        "updated": "2024-08-29 16:56:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16702v1"
    },
    {
        "title": "Fostering Creative Visualisation Skills Through Data-Art Exhibitions",
        "authors": "Jonathan C. Roberts",
        "links": "http://arxiv.org/abs/2408.16479v1",
        "entry_id": "http://arxiv.org/abs/2408.16479v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16479v1",
        "summary": "Data-art exhibitions offer a unique and real-world setting to foster creative\nvisualisation skills among students. They serve as real-world platform for\nstudents to display their work, bridging the gap between classroom learning and\nprofessional practice. Students must develop a technical solution, grasp the\ncontext, and produce work that is appropriate for public presentation. This\nscenario helps to encourage innovative thinking, engagement with the topic, and\nhelps to enhance technical proficiency. We present our implementation of a\ndata-art exhibition within a computing curriculum, for third-year degree-level\nstudents. Students create art-based visualisations from selected datasets and\npresent their work in a public exhibition. We have used this initiative over\nthe course of two academic years with different cohorts, and reflect on its\nimpact on student learning and creativity.",
        "updated": "2024-08-29 12:16:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16479v1"
    },
    {
        "title": "Human and LLM-Based Voice Assistant Interaction: An Analytical Framework for User Verbal and Nonverbal Behaviors",
        "authors": "Szeyi ChanShihan FuJiachen LiBingsheng YaoSmit DesaiMirjana PrpaDakuo Wang",
        "links": "http://arxiv.org/abs/2408.16465v1",
        "entry_id": "http://arxiv.org/abs/2408.16465v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16465v1",
        "summary": "Recent progress in large language model (LLM) technology has significantly\nenhanced the interaction experience between humans and voice assistants (VAs).\nThis project aims to explore a user's continuous interaction with LLM-based VA\n(LLM-VA) during a complex task. We recruited 12 participants to interact with\nan LLM-VA during a cooking task, selected for its complexity and the\nrequirement for continuous interaction. We observed that users show both verbal\nand nonverbal behaviors, though they know that the LLM-VA can not capture those\nnonverbal signals. Despite the prevalence of nonverbal behavior in human-human\ncommunication, there is no established analytical methodology or framework for\nexploring it in human-VA interactions. After analyzing 3 hours and 39 minutes\nof video recordings, we developed an analytical framework with three\ndimensions: 1) behavior characteristics, including both verbal and nonverbal\nbehaviors, 2) interaction stages--exploration, conflict, and integration--that\nillustrate the progression of user interactions, and 3) stage transition\nthroughout the task. This analytical framework identifies key verbal and\nnonverbal behaviors that provide a foundation for future research and practical\napplications in optimizing human and LLM-VA interactions.",
        "updated": "2024-08-29 11:54:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16465v1"
    }
]