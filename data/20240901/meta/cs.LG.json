[
    {
        "title": "A Score-Based Density Formula, with Applications in Diffusion Generative Models",
        "authors": "Gen LiYuling Yan",
        "links": "http://arxiv.org/abs/2408.16765v1",
        "entry_id": "http://arxiv.org/abs/2408.16765v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16765v1",
        "summary": "Score-based generative models (SGMs) have revolutionized the field of\ngenerative modeling, achieving unprecedented success in generating realistic\nand diverse content. Despite empirical advances, the theoretical basis for why\noptimizing the evidence lower bound (ELBO) on the log-likelihood is effective\nfor training diffusion generative models, such as DDPMs, remains largely\nunexplored. In this paper, we address this question by establishing a density\nformula for a continuous-time diffusion process, which can be viewed as the\ncontinuous-time limit of the forward process in an SGM. This formula reveals\nthe connection between the target density and the score function associated\nwith each step of the forward process. Building on this, we demonstrate that\nthe minimizer of the optimization objective for training DDPMs nearly coincides\nwith that of the true objective, providing a theoretical foundation for\noptimizing DDPMs using the ELBO. Furthermore, we offer new insights into the\nrole of score-matching regularization in training GANs, the use of ELBO in\ndiffusion classifiers, and the recently proposed diffusion loss.",
        "updated": "2024-08-29 17:59:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16765v1"
    },
    {
        "title": "UV-free Texture Generation with Denoising and Geodesic Heat Diffusions",
        "authors": "Simone FotiStefanos ZafeiriouTolga Birdal",
        "links": "http://arxiv.org/abs/2408.16762v1",
        "entry_id": "http://arxiv.org/abs/2408.16762v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16762v1",
        "summary": "Seams, distortions, wasted UV space, vertex-duplication, and varying\nresolution over the surface are the most prominent issues of the standard\nUV-based texturing of meshes. These issues are particularly acute when\nautomatic UV-unwrapping techniques are used. For this reason, instead of\ngenerating textures in automatically generated UV-planes like most\nstate-of-the-art methods, we propose to represent textures as coloured\npoint-clouds whose colours are generated by a denoising diffusion probabilistic\nmodel constrained to operate on the surface of 3D objects. Our sampling and\nresolution agnostic generative model heavily relies on heat diffusion over the\nsurface of the meshes for spatial communication between points. To enable\nprocessing of arbitrarily sampled point-cloud textures and ensure long-distance\ntexture consistency we introduce a fast re-sampling of the mesh spectral\nproperties used during the heat diffusion and introduce a novel\nheat-diffusion-based self-attention mechanism. Our code and pre-trained models\nare available at github.com/simofoti/UV3-TeD.",
        "updated": "2024-08-29 17:57:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16762v1"
    },
    {
        "title": "Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning of Large Language Models",
        "authors": "Alec Solway",
        "links": "http://arxiv.org/abs/2408.16753v1",
        "entry_id": "http://arxiv.org/abs/2408.16753v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16753v1",
        "summary": "Reinforcement learning is used to align language models with human preference\nsignals after first pre-training the model to predict the next token of text\nwithin a large corpus using likelihood maximization. Before being deployed in a\nspecific domain, models are often further fine-tuned on task specific data.\nSince human preferences are often unavailable for the last step, it is\nperformed using likelihood maximization as that is the typical default method.\nHowever, reinforcement learning has other advantages besides facilitating\nalignment to a human derived reward function. For one, whereas likelihood\nmaximization is a form of imitation learning in which the model is trained on\nwhat to do under ideal conditions, reinforcement learning is not limited to\ndemonstrating actions just for optimally reached states and trains a model what\nto do under a range of scenarios as it explores the policy space. In addition,\nit also trains a model what not to do, suppressing competitive but poor\nactions. This work develops a framework for last-mile fine-tuning using\nreinforcement learning and tests whether it garners performance gains. The\nexperiments center on abstractive summarization, but the framework is general\nand broadly applicable. Use of the procedure produced significantly better\nresults than likelihood maximization when comparing raw predictions. For the\nspecific data tested, the gap could be bridged by employing post-processing of\nthe maximum likelihood outputs. Nonetheless, the framework offers a new avenue\nfor model optimization in situations where post-processing may be less\nstraightforward or effective, and it can be extended to include more complex\nclasses of undesirable outputs to penalize and train against, such as\nhallucinations.",
        "updated": "2024-08-29 17:49:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16753v1"
    },
    {
        "title": "A Gradient Analysis Framework for Rewarding Good and Penalizing Bad Examples in Language Models",
        "authors": "Yi-Lin TuanWilliam Yang Wang",
        "links": "http://arxiv.org/abs/2408.16751v1",
        "entry_id": "http://arxiv.org/abs/2408.16751v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16751v1",
        "summary": "Beyond maximum likelihood estimation (MLE), the standard objective of a\nlanguage model (LM) that optimizes good examples probabilities, many studies\nhave explored ways that also penalize bad examples for enhancing the quality of\noutput distribution, including unlikelihood training, exponential maximizing\naverage treatment effect (ExMATE), and direct preference optimization (DPO). To\nsystematically compare these methods and further provide a unified recipe for\nLM optimization, in this paper, we present a unique angle of gradient analysis\nof loss functions that simultaneously reward good examples and penalize bad\nones in LMs. Through both mathematical results and experiments on\nCausalDialogue and Anthropic HH-RLHF datasets, we identify distinct functional\ncharacteristics among these methods. We find that ExMATE serves as a superior\nsurrogate for MLE, and that combining DPO with ExMATE instead of MLE further\nenhances both the statistical (5-7%) and generative (+18% win rate)\nperformance.",
        "updated": "2024-08-29 17:46:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16751v1"
    },
    {
        "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
        "authors": "Zhifei XieChangqiao Wu",
        "links": "http://arxiv.org/abs/2408.16725v1",
        "entry_id": "http://arxiv.org/abs/2408.16725v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16725v1",
        "summary": "Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research.",
        "updated": "2024-08-29 17:18:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16725v1"
    }
]