Explainable Post hoc Portfolio Management
Financial Policy of a Deep Reinforcement
Learning agent
Alejandra de la Rica Escudero, Eduardo C. Garrido-Merch´an,
Mar´ıa Coronado-Vaca
Universidad Pontificia Comillas, Madrid, Spain
201906490@alu.comillas.edu, ecgarrido@comillas.edu,
mcoronado@comillas.edu
Abstract. Financialportfoliomanagementinvestmentpoliciescomputed
quantitativelybymodernportfoliotheorytechniquesliketheMarkowitz
model rely on a set on assumptions that are not supported by data in
highvolatilitymarketssuchasthetechnologicalsectororcryptocurren-
cies.Hence,quantitativeresearchersarelookingforalternativemodelsto
tacklethisproblem.Concretely,portfoliomanagementisaproblemthat
has been successfully addressed recently by Deep Reinforcement Learn-
ing (DRL) approaches. In particular, DRL algorithms train an agent
by estimating the distribution of the expected reward of every action
performed by an agent given any financial state in a simulator, also
called gymnasium. However, these methods rely on Deep Neural Net-
works model to represent such a distribution, that although they are
universalapproximatormodels,capableofrepresentingthepreviousdis-
tribution over time, they cannot explain its behaviour, given by a set of
parametersthatarenotinterpretable.Critically,financialinvestorspoli-
ciesrequirepredictionstobeinterpretable,toassesswhethertheyfollow
areasonablebehaviour,soDRLagentsarenotsuitedtofollowaparticu-
larpolicyorexplaintheiractions.Inthiswork,drivenbythemotivation
ofmakingDRLexplainable,wedevelopedanovelExplainableDeepRe-
inforcement Learning (XDRL) approach for portfolio management, in-
tegrating the Proximal Policy Optimization (PPO) deep reinforcement
learning algorithm with the model agnostic explainable machine learn-
ing techniques of feature importance, SHAP and LIME techniques to
enhance transparency in prediction time. By executing our methodol-
ogy,wecaninterpretinpredictiontimetheactionsoftheagenttoassess
whether they follow the requisites of an investment policy or to assess
the risk of following the agent suggestions. To the best of our knowl-
edge, our proposed approach is the first explainable post hoc portfolio
management financial policy of a DRL agent. We empirically illustrate
our methodology by successfully identifying key features influencing in-
vestment decisions, which demonstrate the ability to explain the agent
actions in prediction time.
Keywords: DeepReinforcementLearning;PortfolioManagement;Explainable
AI
4202
luJ
91
]EC.sc[
1v68441.7042:viXra2 Alejandra de la Rica Escudero et al.
1 Introduction
Financial portfolio management is a critical task in the investment domain
[1], traditionally guided by modern portfolio theory techniques such as the
Markowitz model [2]. These models, which optimize the trade-off between risk
and return, are grounded in a set of assumptions about market behavior [3].
However, for example, in high-volatility markets like the technological sector
or cryptocurrencies, these assumptions often fail to hold true [4]. This discrep-
ancyhasledquantitativeresearcherstoseekalternativemethodsthatcanbetter
handle the dynamic and unpredictable nature of such markets [5].
TheapplicationofDeepReinforcementLearning(DRL)[6]toportfolioman-
agementhasgainedpopularityinrecentyears.Concretely,DRLalgorithmstrain
agents to maximize expected returns by learning optimal actions through inter-
action with a simulated environment [7], also known as gymnasium [8]. These
agents use Deep Neural Networks models (DNNs) [9] to approximate the dis-
tribution of expected rewards for different actions in varying financial states.
Despite their capability as universal function approximators [10], DNNs suffer
as they lack interpretability [11]. This means a challenge in financial contexts,
where decision-making transparency is crucial for investors to trust and adopt
automated strategies [12].
The need for explainability in financial decision-making drives the develop-
ment of Explainable Artificial Intelligence (XAI) techniques [13,14,15]. In the
context of DRL, incorporating XAI methods enhance the high performance of
DRLagentsandthenecessityfortransparent,interpretableinvestmentpolicies.
But,as wewill showin section2 (sateofthe art),despitethe growingliterature
thatanalyzestheapplicationofDRLtoportfoliomanagement,theliteratureon
the explainability of DRL algorithms applied to portfolio management is very
scarce and underdeveloped, with only four recent studies [16,17,18,19], to the
bestofourknowledge.Moreover,thesefourpublishedDRLexplainabilitymeth-
ods in portfolio management, only offer explanations of the model in training
time,notbeingabletomonitorthepredictionsdonebytheagentinthetrading
time.
Driven by this motivation, and to respond to this research gap in the litera-
ture,inourwork,weintroduceanovelExplainableDeepReinforcementLearning
(XDRL) framework for portfolio management. Concretely, our approach com-
bines the popular Proximal Policy Optimization (PPO) algorithm [20], a state-
of-the-art DRL technique, with model-agnostic XAI methods such as feature
importance[21],SHAP(SHapleyAdditiveexPlanations)[22],andLIME(Local
Interpretable Model-agnostic Explanations) [23]. By doing so, we obtain inter-
pretability of DRL trained agents. The three explainability techniques can be
implemented independently or jointly, being able to explain the DRL agent pre-
dictions in trading time, being able to track throughout the time whether the
policyisactingasitisexpectedornot,whatisanadvantagewithrespecttothe
restoffourpublishedDRLexplainabilitymethodsjustmentioned.Ourworking
hypothesis is that the predictions of DRL agents can be explained in a post-hocXDRL for Financial Portfolio Management 3
fashion, offering an interpretation that can be tested with respect to financial
investment policies.
Our paper contributes to the underdeveloped literature on the applications
of XDRL models in the realm of portfolio management in two ways: We add to
this emerging literature of only four previous studies but with promising results
and, our research novelty entails, to the best of our knowledge, the first-ever
application of an explainable post-hoc portfolio management policy of a DRL
agent.
The rest of the paper is organised as follows. First, we begin with a state-
of-the-art section where we will show the work that is currently done on DRL
andXDRLinfinancialportfoliomanagement.Afterwards,wewillcontinuewith
a methodology section where we will explain the fundamental details of deep
reinforcement learning applied to financial portfolio management, the PPO al-
gorithm and the explainability techniques that are going to be used to explain
the predictions of the agents. We continue our work with an experiments sec-
tionwherewegiveempiricalevidenceabouttheusefulnessofourapproachthat
supports our claim that DRL predictions can be explained and hence compared
with a particular financial policy. Finally, we illustrate conclusions and further
research lines in this useful and significant topic of explaining DRL agent be-
haviour to obtain transparency about the financial investment policy that is
being executed by an agent.
2 State of the art
The application of DRL in financial portfolio management is gaining popularity
intherecentyears,mainlyduetotheriseofcomputingpowerandarchitectures
that enables a reasonable estimation of the rewards distribution of the actions
with respect to the states given by the training process of the agents with re-
spect to financial data [24]. But despite the growing literature that analyzes the
applicationofDRLtoportfoliomanagement,theliteratureontheexplainability
ofDRLalgorithmsappliedtoportfoliomanagementisveryscarceandunderde-
veloped,withonlyfourrecentstudies[16,17,18,19],tothebestofourknowledge.
In this section, we show a detailed state-of-the-art of DRL and XDRL applied
to financial portfolio management to show the research gap in the literature to
which our work responds.
Multiple DRL algorithms have been proposed recently, which has motivated
their application in our area of interest. Liu et al. [25] classify the state-of-art
DRLalgorithmsintothreecategories:1)value-basedalgorithms:thosebasedon
Deep Q-Networks (DQN) [26]; 2) policy-based algorithms: directly update the
parameters of a policy through policy gradient (PG) [27]; and 3) Actor-Critic
based algorithms, such as Advantage Actor Critic (A2C) [26], Proximal Pol-
icyOptimization(PPO)[20],DeepDeterministicPolicyGradient(DDPG)[28],
Soft Actor-Critic (SAC) [29], or Twin Delayed Deep Deterministic Policy Gra-
dient (TD3) [30]. And all of them have been used to maximize portfolio returns
whileminimizingrisk;forexample,[31]applyDDPGforstocktrading;[32]apply4 Alejandra de la Rica Escudero et al.
DQN,DDPG,PPO,A2C,TD3,SACforautomatedstocktradinginquantitative
finance; [25] implement PPO, A2C, DDPG and TD3 for portfolio management.
Liang et al. [33] have applied successfully to financial portfolio management
three state-of-the-art popular DRL algorithms that can deal with continuous
valuedactions,namelyPPO,DDPGandPG.Specifically,theauthorsconducted
comprehensive experiments on the China stock market, examining various set-
tings, including learning rates, objective functions, and feature combinations,
to derive insights for parameter tuning, feature selection, and data preparation.
Additionally, driven by the usefulness of DRL in high volatility markets, some
authorshaveappliedthismethodologyincryptocurrencyportfoliomanagement
[33,34,35,36,25].Zhangetal.[37]applyDRLtotradefuturescontractsandvar-
ious authors implement hedging strategies with DRL (deep hedging) [38,39,40].
More concretely, Pham et al. [41] focus on multi-agent DRL to automatically
construct hedging strategies. And other authors also apply multi-agent DRL in
portfolio management [42,43,44]. [45] propose a cost-sensitive portfolio selection
with DRL, being, thus, the first authors to incorporate transaction costs. While
all these DRL approches for portfolio management only consider price changes
of assets, but without considering the relation between companies, [46] propose
a new DRL framework for portfolio management based on GCN (Graph Con-
volutional Network) to take into account the relational features of the portfolio
(relations between assets and their corresponding companies). [47] incorporate
ensembletechniquesandfuzzyextensioninadditiontoexistingDRLalgorithms
and use them for portfolio management. [48] propose a novel DRL approach for
portfolio optimization that combines the MPT and a DL approach (specifically,
they solve the multimodal problem on a dataset of 28 USA stocks through the
Tucker decomposition of a model with the input of technical analysis and stock
return covariates). Some authors incorporate sentiment analysis in DRL to also
perceivemarketsentimentinportfolioallocation[49,50,51,52].Hamblyetal.[24]
provide a review of the recent developments and use of RL and DRL in finance,
including portfolio management.
But despite the growing literature that analyzes the application of DRL to
portfolio management (as we have just reviewed above), the literature on the
explainabilityofDRLalgorithmsappliedtoportfoliomanagementisveryscarce
and underdeveloped, with only four recent studies [16,17,18,19], to the best of
our knowledge. Guan and Liu (2021) [18] provide an empirical approach of ex-
plainable DRL for the portfolio management task in response to the challenge
ofunderstandingaDRL-basedtradingstrategybecauseoftheblack-boxnature
of deep neural networks. Specifically, they use a linear layer in hindsight as the
referencemodelandtheyfindtherelationshipbetweenthereward(theportfolio
return) and the input (the features) by using integrated gradients. Since the
linear model (a regression) is interpretable given that the coefficients are inter-
pretable, then the methodology is interpretable. The neural network’s capacity
as a universal approximator dramatically exceeds that of regression. However,
they use a neural network and then make a kind of compensation between the
networkandthecoefficientstoseehowthelinearregression”approximates”theXDRL for Financial Portfolio Management 5
network. If it approximates well, then you can trust of the coefficients. But you
lose explainability if the approximation is poor. Additionally, as they are using
linear interpretation, correlations may not explain complex patterns. Moreover,
their approach differs from ours since it is an explainability approach depen-
dent on the model, while ours is agnostic of the model, it is a post-hoc one.
Bougie and Ichise (2020) [17] present a method to combine external knowledge
and interpretable reinforcement learning in portfolio management. They derive
a rule-based variant version of the Sarsa algorithm ([7], p.140), that is, a neu-
rosymbolic. This way, you can thus add a priori rules and data augmentation to
”explain” your policy, since you are ”injecting” it a priori. It is not a post-hoc
approach like ours but rather an a priori one. While we explain the agent’s pre-
dictions, they inject the agent with a policy in the form of rules before training.
In fact, as we state in section 5 (Conclusions and further research) a line of fu-
ture work could be to hybridize their approach with ours and see how training
modifies the rules injected a priori. Shi et al. (2021) [19] add explainability to
their DRL methods in portfolio managememt through this approach: they use
a temporal neural network to extract significant features that explain the pat-
ternsasafunctionoftime,thatis,amodelthatmanagestime,comparedtoour
CNN. Then they apply a regularization technique to simplify them and finally,
toexplainthem,theyuseclassactivationmapping(CAM),awaytoexplainthe
features of the neural network that they have used in the model, not in predic-
tiontime,likeourproposedapproach.Thus,theyexplainthemodel,thatis,the
policy trained during the training period, but we explain the predictions of the
model during trading time. Wang et al. (2019) [16] offer an interpretable DRL
investment strategy using interpretable deep attention networks. A ranking of
featuresisobtainedthroughtwoneuralnetworksthatseemtoexplainthetrain-
ing time data in the best possible way and subsequently a sensitivity analysis
is performed to determine the best ones. Their interpretation analysis results
reveal that their strategy selects assets by following a principle as “selecting
the stocks as winners with high long-term growth, low volatility, high intrinsic
value, and being undervalued recently”. Once again, this approach differs from
ours since it is useful for explanatory purposes, while we make explanations of
predictions, for predictive purposes.
Thus, to the best of our knowledge, our study is the first to propose an
explainableposthocportfoliomanagementfinancialpolicyofaDeepReinforce-
mentLearningagent.Theotheronlyfourexistingstudiesinthisunderdeveloped
strandofliteratureofferanexplainable”apriori”investmentstrategyusingDRL
models.
3 Methodology
We now introduce the methodological details of our proposed approach to post-
hoc explainable deep reinforcement learning applied to financial portfolio man-
agement. First, we will explain the fundamentals of deep reinforcement learning
applied to finance, then, we will illustrate the explainable artificial techniques6 Alejandra de la Rica Escudero et al.
that we have chosen and, finally, we will show how we can integrate those tech-
niques into the deep reinforcement learning method to explain the predictions
of the agent.
3.1 Fundamentals of Deep Reinforcement Learning applied to
financial portfolio management
We will first introduce objections to our methodology for portfolio management
and arguments that answer to those objections. Then, we will describe the fun-
damentalsofdeepreinforcementlearningandhowwecanapplythesealgorithms
to financial portfolio management.
AlthoughDRLhaspotentialforportfoliomanagementduetoitscompetence
in capturing nonlinear features, low prior assumptions, and high similarities
with human investing, there are characteristics worth paying attention to as
pointed out by Liang et al. [33]: First, a financial market is both highly volatile
and non-stationary, totally different to games or robot control [53,54] which
are the main sectors where DRL has been experimented. Second, traditional
Reinforcement Learning (RL) aims to maximize rewards over an infinite period,
while portfolio management focuses on maximizing returns within a finite time.
Third, in finance, it’s crucial to test strategies on separate data sets to evaluate
their performance, unlike in games or robotics. Lastly, the stock market has
an explicit expression for portfolio value; therefore, approximating the value
function is useless and can even deteriorate the agent´s performance.
However, deep neural networks are able to approximate any function given
enough data and a particular architecture of the network, being universal ap-
proximator functions. Regarding maximizing returns within a finite time, we
can tune the DRL algorithm via the γ hyperparameter to consider high future
rewards. Concretely, γ ∈ [0,1] controls the focus of the agent in immediate or
far rewards as a function of time where a value near to one focus on maximiz-
ing returns on a long time period, being γ the same as a discount rate for all
DRL algorithms. Next, we can assume that an immediate future behaviour of
the stock market is explained technically and by past information, being also
DRL suited in this scenario. The agent can be retrained in a constant fashion
after its predictions happen in the real-time scenario with the new information.
Lastly,althoughMarkowitzmodelassumesthatportfoliosareonlyafunctionof
expected reward and risk, if those assumptions, like normal distributed returns,
arenotmet,then,thefunctionexplainingtheoptimalportfolioisablack-boxof
anenormoussetoffeaturesliketechnicalindicators,fundamentalratiosorsocial
networks, that DRL algorithms can handle due to neural scaling laws. In this
work, we assume that the market can be perfectly explained by technical data,
focusing hence only in this kind of data. However, any source of data can also
be integrated into the space state of the agent, even multimodal data, so this
assumption is not an issue in real-case scenarios. To sum up, we consider that
DRLcanbesuccessfullyappliedtothefinancialportfoliomanagementproblem,
and now explain the fundamental concepts of this methodology.XDRL for Financial Portfolio Management 7
DRL is a class of methods that combines reinforcement learning algorithms
[7,24]withdeepneuralnetworkmodels[9]totackleanycomplexdecision-making
task. In DRL, an agent interacts with an environment defined by a state space
S and an action space A. Critically, these spaces can be a bounded continuous
domain, S ∈ Rd and A ∈ Rd where d is the number of features that the agent
perceives in each time step t, and not limited to discrete spaces, as in reinforce-
ment learning. In particular, deep reinforcement learning will encode the policy
π(a |s )learntbytrailanderrorwiththeenvironmentinthetrainingprocessin
t t
thedeepneuralnetworkthatwillmapadistributionofstatestoactionsS →A,
with the purpose of selecting the action that maximizes an expected reward in
a given time period by a γ ∈[0,1] hyperparameter. Concretely, at each discrete
timestept,theagentobservesastates ∈S andselectsanactiona ∈Abased
t t
onthelearntpolicyπ(a |s ),thatactsastheconditionalprobabilitydistribution
t t
of actions given states being estimated to maximize the expected reward. The
environment responds to the action by transitioning to a new state s and
t+1
providing a reward r as an effect of making action a given state s . Following
t t t
an iterative process in a simulator, the agent can learn a policy π(a |s ) by trail
t t
and error that may generalize outside of the simulator if the assumptions made
by the deep neural network model are met by the prediction data.
Moreformally,thepurposeofthelearningprocessistolearnapolicyπ that
maximizes the expected cumulative reward, which can be defined with a return
function in every time step R :
t
K
(cid:88)
R = γkr , (1)
t t+k
k=0
whereγ ∈[0,1]isthepreviouslymentioneddiscountfactorthatbalancesim-
mediateandfuturerewardsandK istheendoftheepisode,ordesiredprediction
period. Consequently, once that we have estimated the policy that maximizes
the expected return function R , that can be personalized for any problem, we
t
candefine,foreverytimestepandforeveryactionandstate,aq-valuefunction
Q(s,a)thatrepresentsthecompletedistributionoftheexpectedreturnoftaking
any action a∈A in any state s∈S and following policy π(a |s ) as:
t t
Qπ(s,a)=E [R |s =s,a =a], (2)
π t t t
that is the probability distribution learnt by a DRL algorithm and approxi-
mated in the used deep neural network Q(s,a|θ) by its set of parameters θ. We
illustrate this framework in Figure 1.
One of these algorithms, used in our work as its popularity and being a
lightweight version ofa more expensive algorithm,is Proximal Policy Optimiza-
tion (PPO) [20], that aims to update policies without making big changes at
once to avoid outliers in the training process that can make the estimation of
the policy worse. This makes the learning process more stable. This is achieved
by creating a ”trust region,” ensuring that new policies don’t deviate too much
from old ones. In order to do so, PPO introduces a clipped objective function8 Alejandra de la Rica Escudero et al.
Fig.1.Deepreinforcementlearningmaincomponentsthatenabletheestimationofthe
expectedrewardofanyactionoftheactionspaceoftheagentconditionedtoanystate
perceivedbytheagent.Thelearntpolicyfunctionisencodedbyadeepneuralnetwork,
enabling continuous-valued actions and spaces and any complexity of its mapping.
(clippedprobabilityratios)thatpreventstheupdatingstepfrommovingthenew
policy too far from the old policy. This clipping mechanism modifies the policy
optimization by clipping the probability ratio between the new and old policies,
keeping it within a specified range. By maximizing the clipped objective, PPO
finds a balance between trying new strategies (exploration) and sticking with
known good ones (exploitation).
3.2 Explainable artificial intelligence techniques
As the purpose of our work is to enhance the DRL framework by integrating
explainability of financial features to make the decision-making process of the
DRL model transparent and understandable, we briefly describe in this section
some of the techniques that we have integrated in the DRL framework to make
it interpretable for financial experts.
HumandecisionmakersusepredictionsmadebyMLmodelsintheirprocess,
but the usability of those predictions is limited if the human is unable to justify
andunderstandtheirtrustinsaidpredictions.Explanationisawayofobtaining
such understanding, by selecting “what” must be communicated and “how”
that information is presented. Moreover,according to [?] XAI is necessary for
regulatory issues, and thus, there is also a legal component to be considered.
The EU General Data Protection Regulation (GDPR) aims to ensure the ‘right
to explanation’ concerning automated decision-making models.
Interpretabilityalsohelpsdevelopersunderstandandimprovethemodelper-
formance. It can aid in troubleshooting and debugging, as well as in detecting
potentialbiasesintheAIsystemandprovideinsightsintohowthesystemwould
reactunderdifferentcircumstances.ThisunderstandingcanalsoprovideinsightsXDRL for Financial Portfolio Management 9
that go beyond predictions, uncovering underlying patterns and relationships
within the data.
Vouros [55] provides a comprehensive review of state-of-the-art for explain-
ableDRLmethods,categorizingtheminclassesaccordingtotheparadigmthey
follow, the interpretable models they use, and the surface representation of ex-
planations provided. In fact, it is the unique existing literature review focusing
on XDRL. In our work, we have used three explainability techniques:the SHap-
ley Additive exPlanations (SHAP) [56], the Local Interpretable Model-agnostic
Explanations (LIME) methodology [57], and feature importance methods.
TheSHAPmethodoffersaunifiedwaytoexplaineachinstance´spredictions
[56].Imagineyou’replayingateamsport,andattheend,youwanttoknowwho
contributed most to the win. It’s not enough to say ”everyone did their best”;
you want specifics, like who scored the most goals. SHAP does this for machine
learning. It breaks down a prediction to show the impact of each feature—like
how a player’s actions affect the game’s outcome. Here is how SHAP works in
very simple terms: 1. Contribution: It looks at each feature (like a player in a
game) of the data the model uses and asks, ”What’s the contribution of this
feature to the final prediction?”. 2. Fair Distribution: Then, SHAP uses a fair
method for distributing the ”credit” of the outcome. It makes sure that the
contributions of all features sum up to the total prediction. This is like making
sure that all the individual scores from players add up to the final score of the
game. 3. Individual Impact: SHAP values can tell you the impact of a single
feature on the prediction. For example, just as you might wonder how much
scoring that one goal early on helped the team win, SHAP can show how much
changing one piece of data can change the prediction. 4. Teamwork: It also
considers how features affect the prediction when they work together, kind of
like how players might pass the ball to each other. 5. Comparisons: It even lets
you compare the importance of different features. Like after a match, you might
debate who the most valuable player was, based on their contributions.
LIME is a tool designed to explain the predictions of any classifier in a way
thatisunderstandableandaccurate[57].Itworksbylearningasimplifiedmodel
around the prediction, using perturbations of uniformly sampled features. This
allowsforabetterunderstandingoftherelationshipbetweeninputfeaturesand
model responses. The goal is to ensure local fidelity, explaining how the model
behavesaroundthespecificinstancebeingpredicted.WhileLIMEwasoriginally
designed to inspect models using binary vectors, for instance representation, it
also offers solutions for outcome explanation, as it highlights the importance
of features in a local context only. LIME is a universal method that enhances
both local and global model interpretability. In the context of DRL, it can be
used to explain any models, such as local decisions for action selection based on
interpretable state feature representation, even if it wasn’t specifically designed
for that task. Functions can provide binary vectors, for instance representation,
whileanyinterpretablemodelcantheoreticallybeusedforoutcomeexplanation,
like a linear model or a decision tree. Visual means are used to provide surface
representations of the instance’s interpretable representations for model inspec-10 Alejandra de la Rica Escudero et al.
tion, as well as explanation logic for local and model explainability. Although
Ludenberg et al. [56] showed that LIME is a subset of SHAP and that SHAP
outperformed LIME, LIME is still a useful tool, since it’s faster compared to
SHAP and thus, LIME can be practical in cases where efficiency is important.
Feature importance methods provide insights into which features are most
influential in the model’s overall decision-making process. By analyzing the im-
portance of each feature, we can understand how the model prioritizes differ-
ent aspects of the input data when making predictions. This helps identify key
driversofthemodel’sbehavior,enhancingtransparencyandtrustinthemodel’s
decisions.
3.3 Integrating explainability in deep reinforcement learning
financial predictions
In this section we describe how we integrate the explainability techniques men-
tioned in the previous subsection with the DRL methodology illustrated before.
Concretely, our work builds on the existing framework from the GitHub repos-
itory ”Reinforcement learning in portfolio management” (DeepCrypto, 2018)
https://github.com/deepcrypto/Reinforcement-learning-in-portfolio-management-/
tree/master?tab=readme-ov-file. The goal is to enhance this framework by
integrating explainability features to make the decision-making process of the
DRL model transparent and understandable.
Our starting point is the already developed PPO-based model in the frame-
work, which has demonstrated effectiveness in portfolio management tasks. The
PPO algorithm is chosen for its balance between exploration and exploitation,
making it well-suited for dynamic and unpredictable environments like financial
markets.
WedesignatrainingphaseconsistingonfinancialdatadownloadedfromYa-
hoo Finance with OHCLV (Open, High, Close, Low, and Volume) information
about several tickers to build a portfolio in a certain period of time. Once the
agent is ready, after a preprocessing phase to ensure the data is clean and ready
for analysis. This involves normalizing values, handling missing data and struc-
turing the data so that the model can work with it. Then, the training phase
starts on the financial market loaded, the agent interacts with the environment,
learns from the data, and refines its policy to improve decision-making.
To implement the explainability techniques in the already analyzed frame-
work, we will use post-hoc interpretability methods. This requires saving the
state-action pairs for all steps during the training process. These state-action
pairs will be used later to create explainability models. To achieve our objec-
tive, we implement explainability techniques such as SHAP, LIME, and feature
important methods. These methods reveal which features and inputs are most
influential in the agent decision-making process, providing insights both at a
global level and for specific predictions, leading to an interpretable DRL model.XDRL for Financial Portfolio Management 11
4 Experiments
In this section, we will describe the different experiments that we have per-
formed to show the usefulness of our explainable deep reinforcement learning
methodology. For reproducibility and transparency, we have uploaded all the
code of these experiments in the following Github repository https://github.
com/aleedelarica/XDRL-for-finance.
For our portfolio management experiment we have considered a training pe-
riodfrom2015to2017andatradingperiodfrom2017to2018.Wehaveconsid-
ered the OHCL information of five different technological assets, having a total
of 20 features in the state space of the agent. We have considered a PPO DRL
algorithmtolearntheweightsofthedeepneuralnetworkwithdefaulthyperpa-
rameters and 100 epochs across all the financial data downloaded from Yahoo
finance. Having all that information and performing the training period of the
neural network, we interpret the predictions of the deep neural network in the
trading period using feature importance, SHAP and LIME methods as we will
describe in this section.
We begin with experiments that show how the feature importance can be
extracted for the predictions of the agent during the trading period. In order to
do so, we configure a portfolio of several technological assets and measure the
importance of their OHCL features in the trading period, information that we
illustrateinFigure2.WewillthenillustratehowcanSHAPandLIMEmethods
offer interpretability and explainability of the actions performed by the agent in
such a scenario.
Figure3showstheaverageimportanceacrossallfeaturesforeachstock.Ap-
ple(AAPL)remainsthemostsignificant,followedbyVisa(V),Alibaba(BABA),
Adobe (ADBE), and Sony (SNE). This means the DRL agent in the model pri-
oritizesApple’sdatainordertomaketheinvestingdecisions,asitisconsistently
seeninthepreviousexperiment.Mostcritically,wecanseehow,foreveryasset,
itisnotclearwhichoftheOHCListhemostimportantone,asitvariesinevery
asset. Consequently, it is wise to use them all to make the agent more robust to
different portfolios.
However, as we believe that it is important to know whether the OCHL
information is important, we have made a plot of the average importance of
each technical indicator for all the assets of the portfolio during all the trading
period, obtaining the results plotted on Figure 4, where we can clearly see that
although close and open are the most critical indicators, the extreme results of
theday,highandlow,arefeaturesthathaveaswellimportanceinthepredictions
of the DRL agent.
To explain the model’s predictions with SHAP, we use SHAP’s force plot.
Our model is a multi-output model (6 actions, one for each asset and 1 for the
cash risk-free asset), consequently, SHAP can generate one force plot for each
asset weight allocation through all the samples. We can observe the force plot
for Apple’s weight allocation in Figure 5.
The SHAP values represent the contribution of each feature to the model’s
output for the specific sample. Positive SHAP values (red) push the prediction12 Alejandra de la Rica Escudero et al.
Fig.2.ImportanceofthefeaturesusedasthestatespaceoftheDRLagentforfinancial
portfoliomanagementexperiments.WecanseehowtheAPPLEclosevalueisthemost
important for the estimated policy of the DRL agent.
higher, while negative SHAP values (blue) push the prediction lower. The base
value, represented by the horizontal baseline, is the average model output (logit
in this case) over the entire dataset. Conversely, we can do the same process
with only one feature, to test how and whether it impacts on the portfolio,
as we illustrate in Figure 6, where we can see how the red and blue segments
still indicate positive and negative contributions, respectively, but they are now
isolated into a single feature (Apple’s closing price value contribution, in this
case).
This plot helps in understanding both the specific impact of this particular
feature through the entirety of the dataset and in one specific prediction with-
out the interference of others. For instance, imagine an investor who wants to
understand why the model lowered the weight allocation of Apple on a certainXDRL for Financial Portfolio Management 13
Fig.3. Feature importance of the state space of the DRL agent sorted by assets.
instance. The investor could analyze the impact of the different features to that
prediction, realizing that the closing price of Apple stock lowered significantly
this weight allocation prediction. The combination of the investor’s knowledge
and the model´s explanation gives a complete understanding of the prediction.
Continuing with the storyline, let’s say the investor also examines another in-
stance where the model increased the weight allocation of Apple. By analyzing
this specific prediction, the investor notices a red segment indicating that the
BABA open L1 value had apositive impact. Thismightinitially seem puzzling,
asBABA open L1referstotheopeningpriceofAlibabastockfromtheprevious
day.However,theinvestorrecallsthatonthisparticularday,positivenewsabout
Alibaba’s strong quarterly performance had a ripple effect on the tech sector,
boosting overall market sentiment and indirectly benefiting Apple’s stock price.
The model’s sensitivity to such interconnected market dynamics is reassuring
to the investor. It demonstrates that the model doesn’t just consider isolated
stockmovementsbutalsounderstandsbroadermarkettrendsandtheirimpacts
on individual assets. This interconnected understanding is crucial for making
informed allocation decisions in a diversified portfolio. This comprehensive ex-
plainability method enhances the investor’s trust in the model, ensuring they
can confidently rely on its predictions to guide their investment strategies.
Finally, we comment how can the LIME method be used to interpret the
predictionsoftheDRLagent.ByusingLIMEexplanations,wecangaininsights14 Alejandra de la Rica Escudero et al.
Fig.4. Mean feature importance of the different financial indicators across all the
assets of the portfolio.
into the feature contributions for each asset’s weight allocation at a specific
instance, as we can see in the example illustrated on Figure 7.
The predicted weight allocation is at its maximum of 0.21. This high alloca-
tion is driven by several features.The positive contributors include V high L1,
BABA close L1,AAPL open L1,ADBE close L1,andBABA low L1.Thesefea-
tures collectively push the prediction higher. Specifically, the feature V high L1
with a value of 1.02 and BABA close L1 at 0.96 significantly influence the
model’s decision to allocate a larger portion to Apple. Notably, there are no
significant negative contributors, indicating a strong overall positive sentiment
for Apple’s allocation based on these metrics.
Moving to Adobe (Fig 8), the predicted allocation is 0.16 within a range of
0.15 to 0.25. Positive contributions come from V low L1 and BABA open L1.
ThesemetricsindicatethatlowervaluesforVisaandopeningvaluesforAlibaba
push the allocation towards Adobe higher. On the negative side, for instance
metrics from Apple, particularly AAPL low L1 and AAPL high L1, reduce the
allocation. This insight suggests that while Adobe benefits from positive condi-
tions in Visa and Alibaba, certain Apple metrics dampen its allocation.
ThisLIMEexplanationscanassistinvestorsbyofferingexplicitexplanations
for each forecast, allowing them to observe how different factors interact and
influencetheDRLagentdecisions.Forexample,ifaninvestorfindsthatspecific
measuresfromVisaandAlibabacontinuouslyincreaseallocationstotechstocks
such as Apple and Adobe, they may opt to constantly monitor these metrics.XDRL for Financial Portfolio Management 15
Fig.5. SHAP force plot for AAPL weight allocation with all features contribution.
Fig.6. SHAP force plot for AAPL weight allocation with only Apple’s closing price
value contribution.
This actionable insight can help investors make more informed and proactive
decisions.
Understanding why the model cuts allocations in response to specific traits
canalsoassistinvestorsinreducingrisks.IfApple’spoormetricsroutinelyreduce
allocations to other stocks, the investor may investigate this link further and
change their portfolio to balance potential drawbacks.
As we have shown throughout the section, the three explainability tech-
niques can be implemented independently or jointly, being able to explain the
DRL agent predictions in trading time, being able to track throughout the time
whether the policy is acting as it is expected or not, what is an advantage with
respect to the rest of published DRL explainability methods, that only offer
explanations of the model in training time, not being able to monitor the pre-
dictions done by the agent in the trading time.
5 Conclusions and Further Work
In this work, we successfully used SHAP, LIME and feature importance to ex-
plain the decisions made by our DRL model in portfolio management tasks.
These methods provided clear and detailed insights into why the model made
specificinvestmentchoices,affirmingourhypothesisthatitispossibletoexplain
DRL predictions in portfolio management with explainability techniques. Also,
our prediction explainability analysis with SHAP and LIME revealed that the16 Alejandra de la Rica Escudero et al.
Fig.7. LIME explanation for Apple’s weight allocation prediction at a particular in-
stance.
Fig.8. LIME explanation for Adobe’s weight allocation prediction at a particular
instance.
importanceoffeaturesvariedacrossdifferentmarketconditionsandspecificpre-
dictions.Moreover,ourfeatureimportanceanalysisshowedthatcertainfeatures
consistently influenced the model’s investment decisions more than others.
We believe that several future directions could enhance and expand the im-
pact of this work. First, it would be interesting to conduct several studies to
evaluate how real investors interpret and react to the explanations provided by
the model, which could yield valuable reinforcement learning from human feed-
back. For a better understanding of non-technical investors, it would be nice to
develop an intuitive user interface that presents the model’s decisions and their
explanations in a user-friendly manner would improve its practicality. Also, it
would be interesting to deal with a wider set of explainable techniques and dif-
ferentmarketstoassesstheusefulnessofthemethodologyindifferentscenarios.
Finally, a future line of work would be to hybridize one of the a priori XDRL
approachesofBougieandIchise[17]orShietal.[19]withourproposedposthoc
approach (the only one in the literature of XDRL for portfolio management);
thatis,explainingboththemodel(thepolicytrainedduringthetrainingperiod)
astheseauthorsdo,andthepredictionsofthemodelduringtradingtime(aswe
do), and see how training modifies the rules injected a priori [17] or the policy
trained [19].XDRL for Financial Portfolio Management 17
References
1. PamelaPetersonDrakeandFrankJFabozzi.Thebasicsoffinance:Anintroduction
tofinancialmarkets,businessfinance,andportfoliomanagement,volume192.John
Wiley & Sons, 2010.
2. Myles E Mangram. A simplified perspective of the markowitz portfolio theory.
Global journal of business research, 7(1):59–70, 2013.
3. D Sykes Wilford. True markowitz or assumptions we break and why it matters.
Review of Financial Economics, 21(3):93–101, 2012.
4. JoanneMHill. Thedifferentfacesofvolatilityexposureinportfoliomanagement.
The Journal of Alternative Investments, 15(3):9, 2013.
5. AkhterMohiuddinRather,VNSastry,andArunAgarwal.Stockmarketprediction
and portfolio selection models: a survey. Opsearch, 54:558–579, 2017.
6. Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony
Bharath. Deep reinforcement learning: A brief survey. IEEE Signal Processing
Magazine, 34(6):26–38, 2017.
7. RichardSSuttonandAndrewGBarto. Reinforcement learning: An introduction,
2nd ed. MIT press, Cambridge, MA, 2020.
8. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John
Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint
arXiv:1606.01540, 2016.
9. Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature,
521(7553):436–444, 2015.
10. Yulong Lu and Jianfeng Lu. A universal approximation theorem of deep neural
networks for expressing probability distributions. Advances in neural information
processing systems, 33:3094–3105, 2020.
11. DavideCastelvecchi.CanweopentheblackboxofAI?NatureNews,538(7623):20,
2016.
12. Louis Lowenstein. Financial transparency and corporate governance: you manage
what you measure. Columbia Law Review, 96(5):1335–1362, 1996.
13. Rich Caruana, Scott Lundberg, Marco Tulio Ribeiro, Harsha Nori, and Samuel
Jenkins. Intelligibleandexplainablemachinelearning:Bestpracticesandpractical
challenges. In Proceedings of the 26th ACM SIGKDD international conference on
knowledge discovery & data mining, pages 3511–3512, 2020.
14. Plamen P Angelov, Eduardo A Soares, Richard Jiang, Nicholas I Arnold, and
Peter M Atkinson. Explainable artificial intelligence: an analytical review. Wi-
leyInterdisciplinaryReviews:DataMiningandKnowledgeDiscovery,11(5):e1424,
2021.
15. MayuriMehta,VasilePalade,andIndranathChatterjee. Explainable AI: Founda-
tions, methodologies and applications. Springer, 2023.
16. Jingyuan Wang, Yang Zhang, Ke Tang, Junjie Wu, and Zhang Xiong. Alphas-
tock: A buying-winners-and-selling-losers investment strategy using interpretable
deepreinforcementattentionnetworks. InProceedings of the 25th ACM SIGKDD
international conference on knowledge discovery & data mining, pages1900–1908,
2019.
17. Nicolas Bougie and Ryutaro Ichise. Towards interpretable reinforcement learning
withstateabstractiondrivenbyexternalknowledge. IEICETRANSACTIONSon
Information and Systems, 103(10):2143–2153, 2020.
18. Mao Guan and Xiao-Yang Liu. Explainable deep reinforcement learning for port-
folio management: an empirical approach. In Proceedings of the second ACM in-
ternational conference on AI in finance, pages 1–9, 2021.18 Alejandra de la Rica Escudero et al.
19. Si Shi, Jianjun Li, Guohui Li, Peng Pan, and Ke Liu. XPM: An explainable deep
reinforcement learning framework for portfolio management. In Proceedings of
the 30th ACM international conference on information & knowledge management,
pages 1661–1670, 2021.
20. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
21. Vaishak Belle and Ioannis Papantonis. Principles and practice of explainable ma-
chine learning. Frontiers in big Data, 4:688969, 2021.
22. Benedek Rozemberczki, Lauren Watson, P´eter Bayer, Hao-Tsung Yang, Oliv´er
Kiss, Sebastian Nilsson, and Rik Sarkar. The shapley value in machine learning.
arXiv preprint arXiv:2202.05594, 2022.
23. Giorgio Visani, Enrico Bagli, Federico Chesani, Alessandro Poluzzi, and Davide
Capuzzo. Statistical stability indices for lime: Obtaining reliable explanations for
machine learning models. Journal of the Operational Research Society, 73(1):91–
101, 2022.
24. Ben Hambly, Renyuan Xu, and Huining Yang. Recent advances in reinforcement
learning in finance. Mathematical Finance, 33(3):437–503, 2023.
25. Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang. Finrl:
Deep reinforcement learning framework to automate trading in quantitative fi-
nance.InProceedingsofthesecondACMinternationalconferenceonAIinfinance,
pages 1–9, 2021.
26. VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiARusu,JoelVeness,
Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg
Ostrovski,etal. Human-levelcontrolthroughdeepreinforcementlearning. nature,
518(7540):529–533, 2015.
27. RichardSSutton,DavidMcAllester,SatinderSingh,andYishayMansour. Policy
gradient methods for reinforcement learning with function approximation. Ad-
vances in neural information processing systems, 12, 1999.
28. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom
Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
29. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-
critic: Off-policy maximum entropy deep reinforcement learning with a stochastic
actor. In International conference on machine learning, pages 1861–1870. PMLR,
2018.
30. Stephen Dankwa and Wenfeng Zheng. Twin-delayed ddpg: A deep reinforcement
learning technique to model a continuous movement of an intelligent robot agent.
In Proceedings of the 3rd international conference on vision, image and signal
processing, pages 1–5, 2019.
31. Xiao-Yang Liu, Zhuoran Xiong, Shan Zhong, Hongyang Yang, and Anwar Walid.
Practical deep reinforcement learning approach for stock trading. arXiv preprint
arXiv:1811.07522, 2018.
32. Xiao-YangLiu,HongyangYang,QianChen,RunjiaZhang,LiuqingYang,Bowen
Xiao, and Christina Dan Wang. Finrl: A deep reinforcement learning library for
automatedstocktradinginquantitativefinance. arXivpreprintarXiv:2011.09607,
2020.
33. Zhipeng Liang, Hao Chen, Junhao Zhu, Kangkang Jiang, and Yanran Li. Ad-
versarial deep reinforcement learning in portfolio management. arXiv preprint
arXiv:1808.09940, 2018.XDRL for Financial Portfolio Management 19
34. Zhengyao Jiang and Jinjun Liang. Cryptocurrency portfolio management with
deep reinforcement learning. In 2017 Intelligent systems conference (IntelliSys),
pages 905–913. IEEE, 2017.
35. Jonathan Sadighian. Deep reinforcement learning in cryptocurrency market mak-
ing. arXiv preprint arXiv:1911.08647, 2019.
36. Otabek Sattarov, Azamjon Muminov, Cheol Won Lee, Hyun Kyu Kang, Ryum-
duck Oh, Junho Ahn, Hyung Jun Oh, and Heung Seok Jeon. Recommending
cryptocurrencytradingpointswithdeepreinforcementlearningapproach. Applied
Sciences, 10(4):1506, 2020.
37. Zihao Zhang, Stefan Zohren, and Stephen Roberts. Deep reinforcement learning
for trading. arXiv preprint arXiv:1911.10107, 2019.
38. Hans Buehler, Lukas Gonon, Josef Teichmann, Ben Wood, Baranidharan Mohan,
and Jonathan Kochems. Deep hedging: hedging derivatives under generic market
frictions using reinforcement learning. Swiss Finance Institute Research Paper,
(19-80), 2019.
39. Jay Cao, Jacky Chen, John Hull, and Zissis Poulos. Deep hedging of derivatives
using reinforcement learning. arXiv preprint arXiv:2103.16409, 2021.
40. AlexandreCarbonneau.Deephedgingoflong-termfinancialderivatives.Insurance:
Mathematics and Economics, 99:327–340, 2021.
41. Uyen Pham, Quoc Luu, and Hien Tran. Multi-agent reinforcement learning ap-
proach for hedging portfolio problem. Soft Computing, 25(12):7877–7885, 2021.
42. Johann Lussange, Ivan Lazarevich, Sacha Bourgeois-Gironde, Stefano Palminteri,
andBorisGutkin. Modellingstockmarketsbymulti-agentreinforcementlearning.
Computational Economics, 57(1):113–147, 2021.
43. ZhenhanHuangandFumihideTanaka. Mspm:Amodularizedandscalablemulti-
agentreinforcementlearning-basedsystemforfinancialportfoliomanagement.Plos
one, 17(2):e0263689, 2022.
44. Jinho Lee, Raehyun Kim, Seok-Won Yi, and Jaewoo Kang. Maps: Multi-
agent reinforcement learning-based portfolio management system. arXiv preprint
arXiv:2007.05402, 2020.
45. Yifan Zhang, Peilin Zhao, Qingyao Wu, Bin Li, Junzhou Huang, and Mingkui
Tan. Cost-sensitive portfolio selection via deep reinforcement learning. IEEE
Transactions on Knowledge and Data Engineering, 34(1):236–248, 2020.
46. Si Shi, Jianjun Li, Guohui Li, Peng Pan, Qi Chen, and Qing Sun. Gpm: A graph
convolutional network based reinforcement learning framework for portfolio man-
agement. Neurocomputing, 498:14–27, 2022.
47. Zheng Hao, Haowei Zhang, and Yipu Zhang. Stock portfolio management by
using fuzzy ensemble deep reinforcement learning algorithm. Journal of Risk and
Financial Management, 16(3):201, 2023.
48. JunkyuJangandNohYoonSeong. Deepreinforcementlearningforstockportfolio
optimization by connecting with modern portfolio theory. Expert Systems with
Applications, 218:119556, 2023.
49. Prahlad Koratamaddi, Karan Wadhwani, Mridul Gupta, and Sriram G Sanjeevi.
Market sentiment-aware deep reinforcement learning approach for stock portfo-
lio allocation. Engineering Science and Technology, an International Journal,
24(4):848–859, 2021.
50. Xinyi Li, Yinchuan Li, Yuancheng Zhan, and Xiao-Yang Liu. Optimistic bull or
pessimistic bear: Adaptive deep reinforcement learning for stock portfolio alloca-
tion. arXiv preprint arXiv:1907.01503, 2019.20 Alejandra de la Rica Escudero et al.
51. ParaskeviNousi,LoukiaAvramelou,GeorgiosRodinos,MariaTzelepi,Theodoros
Manousis, Konstantinos Tsampazis, Kyriakos Stefanidis, Dimitris Spanos, Em-
manouil Kirtas, Pavlos Tosidis, et al. Leveraging deep learning and online source
sentiment for financial portfolio management. arXiv preprint arXiv:2309.16679,
2023.
52. Akhil Raj Azhikodan, Anvitha GK Bhat, and Mamatha V Jadhav. Stock trading
bot using deep reinforcement learning. In Innovations in Computer Science and
Engineering: Proceedings of the Fifth ICICSE 2017, pages 41–49. Springer, 2019.
53. RongrongLiu,FlorentNageotte,PhilippeZanne,MicheldeMathelin,andBirgitta
Dresp-Langley. Deep reinforcement learning for the control of robotic manipula-
tion: a focussed mini-review. Robotics, 10(1):22, 2021.
54. KunShao,ZhentaoTang,YuanhengZhu,NannanLi,andDongbinZhao. Asurvey
of deep reinforcement learning in video games. arXiv preprint arXiv:1912.10944,
2019.
55. George A Vouros. Explainable deep reinforcement learning: state of the art and
challenges. ACM Computing Surveys, 55(5):1–39, 2022.
56. Scott M Lundberg and Su-In Lee. A unified approach to interpreting model pre-
dictions. Advances in neural information processing systems, 30, 2017.
57. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”Why should I trust
you?”explainingthepredictionsofanyclassifier. InProceedingsof the22nd ACM
SIGKDD international conference on knowledge discovery and data mining,pages
1135–1144, 2016.