Catastrophic Goodhart: regularizing RLHF with KL divergence does not
mitigate heavy-tailed reward misspecification
ThomasKwa12 DrakeThomas3 AdriàGarriga-Alonso4
Abstract etal.,2017;Ziegleretal.,2020,RLHF)isaverypopular
method to induce desirable behavior in language models.
Whenapplyingreinforcementlearningfromhu-
RLHFstarts withabase pre-trained model, thenlearnsa
manfeedback(RLHF),therewardislearnedfrom
rewardfunctionfromhumanannotatordata. Next,ittrains
data and, therefore, always has some error. It
anRLpolicytomaximizethisreward,whilepenalizinghigh
is common to mitigate this by regularizing the
KLdivergencefromthepolicytothebasemodel. RLHF
policy with KL divergence from a base model,
usesanon-policyalgorithmandhasaccurateadvantages,
withthehopethatbalancingrewardwithregular-
buttherewardfunctionisalwayssomewhatmisspecified
ization will achieve desirable outcomes despite
comparedtodesiredbehavior,duetoinsufficientdata,hu-
thisrewardmisspecification. Weshowthatwhen
manbiases,andotherfactors.
the reward function has light-tailed error, opti-
mal policies under less restrictive KL penalties The main purpose of the KL penalty in RLHF is to limit
achievearbitrarilyhighutility. However,iferror theconsequenceofrewardmodelingerrorsbykeepingthe
is heavy-tailed, some policies obtain arbitrarily policywithinadistributionsimilartothatonwhichitwas
highrewarddespiteachievingnomoreutilitythan trained. Ideally,inthelow-KLregimetherewardmodel’s
the base model—a phenomenon we call catas- errorsaresmallenoughthatitprovidescorrectupdatesto
trophicGoodhart. Weadaptadiscreteoptimiza- thebasemodel. Gaoetal.(2023)empiricallysupportsthis
tionmethodtomeasurethetailsofrewardmodels, view: iftheKLdivergenceinRLHFisallowedtogrowtoo
findingthattheyareconsistentwithlight-tailed much,withamisspecifiedreward,themodel’sperformance
error. However,thepervasivenessofheavy-tailed onthetrueutilitystartstodecrease.
distributionsinmanyreal-worldapplicationsin-
Weask: canweobtaingoodoutcomesfrommisspecified
dicates that future sources of RL reward could
rewardinRLHFbycontrollingtheKLdivergence? That
haveheavy-tailederror,increasingthelikelihood
is,ifthereissomeerrorbetweenthetruerewardV andthe
ofrewardhackingevenwithKLregularization.
proxy reward U, can the KL help us to still optimize V?
Usingmathematicalproof,weanswerthequestioninthe
negativeforheavy-tailederrors: thereexistpolicieswhich
1.Introduction
haveinfiniteproxyrewardU,butwhoseKLwiththebase
modelvanishes(thesehaveundeterminedV). Wetermthis
Kullback-Leibler(KL)divergenceconstraintsinreinforce-
phenomenon“catastrophicGoodhart”,afterGoodhart’slaw.
mentlearning(RL)areemployedtostayinregimeswhere
theobjectiveisaccurateenough. Someon-policy(Schul- If the misspecification errors are independent and light-
manetal.,2015;2017)andmanyoff-policy(Abdolmaleki tailed, the KL divergence does suffice to guarantee good
etal.,2018;Jaquesetal.,2019)policygradientalgorithms outcomes. Theremayalsobeguaranteesunderweakeras-
employKLconstraintsorpenaltiesduringoptimizationto sumptions,butassumptionsthatintuitivelyseemsufficient
preventthepolicyfromdeviatingtoomuchfromthedata areoftennot(seeSection6).
collectiondistribution. Thisensuresthatestimatesofeach
Possibly, other regularization schemes would guarantee
action’sadvantagearereliableenoughtoupdatethepolicy
goodoutcomesforheavy-tailederrors,butthisisnotjust
inahelpfulway.
aproblemofKL.Weshowthatoptimizingbycondition-
Reinforcementlearningfromhumanfeedback(Christiano ingonlargerewardU hassimilaroutcomesinlight-and
heavy-tailedregimes.
1Independent2FARLabs3Anthropic4FARAI.Correspondence
to: Thomas Kwa <kwathomas0@gmail.com>, Adrià Garriga- Empirically, open-source language reward models seem
Alonso<adria@far.ai>.
tobelight-tailed,whichdoesnotimplylight-tailederrors
butsuggestsit(Section5). However,theerrorsarelikely
1
4202
luJ
91
]GL.sc[
1v30541.7042:viXraCatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
notindependentand,giventheprevalenceofheavy-tailed adistributionovertext(generatedbyanLLM),wegettwo
distributionsintherealworld,errorinfuturerewardmodels scalarrandomvariables,whichwecallU forproxyreward
mayalsobeheavy-tailed.Inanycase,thepresentsuccessof andV fortruereward/utility. Thenwecandefinetheerror
RLHFwithmisspecifiedrewardscannotbeexplainedsolely intheproxyrewardasX ≜ U −V,sothatU = X +V.
bytheKLregularizationinitsobjective. Framedthisway,optimizationforaproxyrewardU isamix
ofdesirableoptimizationforV andundesirableoptimiza-
tionforX.ThejointdistributionofV andXdeterminesthe
2.Background
limitingvalueofV asweapplymoreoptimization. When
2.1.KLdivergenceandKLregularization wesaythatrewardmisspecificationcanhavenegativeef-
fects,wemeanthattoomuchvarianceinX can"redirect"
RecallthatKLdivergencebetweentwodistributionsPand
(cid:16) (cid:17) theoptimizationpressurefromV toX,andpreventutility
QisdefinedasD KL(P∥Q)=(cid:80) x∈X P(x)log QP( (x x) ) . gainfromoptimization.
Ifwehavetwopoliciesπ,π ,wedefineD (π∥π )asthe Rewardmisspecificationisalsostudiedby(Lambert&Ca-
0 KL 0
KLdivergencebetweenthedistributionsofactionstaken landra,2024),(Laidlawetal.,2024),andothers. Laidlawet
onthestatesintrajectoriesreachedbyπ. Thatis,ifTr(π) alshowthataKLpenaltybetweenactiondistributionscan
is the distribution of trajectories taken by π, we penalize beineffective,andproposeinsteadregularizingstateoccu-
D (π∥π )≜E [D (π(s)∥π (s))]. pancymeasure. Ourresultsshowaninherentweaknessof
KL 0 s∈T,T∼Tr(π) KL 0
KLdivergence,includingwhenappliedtostateoccupancy
In RLHF, it is common to use the regularization term
measure.
βD (π∥π )topreventthelearnedpolicyfromdeviating
KL 0
toomuchfromthebasepolicy,whichcanpreventunstable
3.Theoreticalresults
behaviororoverfittingtotherewardmodel. Ifourreward
modelgivesrewardU,thentheoptimalpolicyforRLHF
WhenapplyingKLregularization,thetrainedmodelisreg-
withaKLpenaltyisargmax E[U(π)]−βD (π∥π ).
π KL 0 ularizedtowardssomebasepolicyπ . Onewouldhopethat
0
Often the regularization parameter β is dynamically ad- aKLpenaltycanproducegoodoutcomeseveninthecase
justed to keep the D near some target value (Ziegler of reward misspecification; that is, if the reward U is the
KL
etal.,2020). sumoftrueutilityV andanerrortermX,wewouldhope
that optimal policies under a KL penalty achieve high V
2.2.Heavy-taileddistributions evenifthemagnitudeofX islarge. Weshowthatthisisnot
alwaysthecase: Corollary1ofTheorems1,2,and3estab-
AdistributionP overRwithcumulativedistributionfunc-
lishesthatwhenX(π )isheavy-tailed,therearearbitrarily
tion(CDF)F P isheavy-tailedifitstailfunctionF¯ P(x)≜ well-performing poli0 cies π with E [V] ≈ E [V]. How-
1−F (x) satisfies lim etxF¯(x) = ∞ for all t > 0. π π0
P x→∞ ever,Theorem4showsthatwhenerrorislight-tailedand
Heavy-tailed distributions are well-known in statistics to
independentofV,theoptimalpolicyunderaKLpenalty
have a higher probability of producing a single extreme
resultsinV >0,andV canbemadearbitrarilylarge. Thus,
value. Forexample,ifthesumoftwoindependentvariables
thetailsoftheerrordistributionarecrucialindetermining
fromheavy-taileddistributionsislarge,itismostlikelydue
howmuchutilitywillresultfromoptimizationtowardsan
tooneextremesampleratherthantwoequallylargesamples.
imperfectproxy.
(Wierman,2013)
Theorems 5 and 8 (Section B of the appendix) show that
therelationshipofcatastrophicGoodharttoheavy-taileder-
2.3.RewardmisspecificationandGoodhart’sLaw
rorisnotjustaquirkofKLdivergencebyusingadifferent
Rewardmisspecificationhascausedlow-utilityoutcomes modelofoptimizationbasedonconditioningonhighreward
in practice; for example, in (Clark & Amodei, 2016), an values. Underthismodel(andgivenadditionalregularity
RLagenttrainedtoplayaracingvideogameaccordingto conditions),itisalsotruethatheavy-tailederrorresultsin
amisspecifiedrewardfunctionachievesahighscorewhile catastrophicGoodhart,andlight-tailederrorplusindepen-
failingtocompletethecourse. denceresultsinarbitrarilylargeutility. Allproofsareinthe
appendix.
Gaoetal.(2023)introducetheconceptof“overoptimiza-
tion”: optimizing for a proxy objective decreases perfor-
3.1.Heavy-taileddistributions
manceaccordingtothetrueobjective. Thisraisestheques-
tion: ingeneral,whenRLHFrewardismisspecified,when
Theorem1. Givenanyheavy-tailedreferencedistribution
doestheoptimalpolicyproducehighutility? Q over R with mean µ , and any M,ϵ > 0, there is a
Q
Byapplyingtheproxyrewardandtruerewardfunctionsto distributionP withmeanµ P >M andD KL(P∥Q)<ϵ.
2CatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
Outlineofproof(seeappendixforfullproof): WLOGtake 3.3.IfVislight-tailed,E [V]−E [V]→0asD →0
P Q KL
µ = 0. IfwesetP toupweighttheprobabilitymassof
Q t Theorem 3. If V is light-tailed and d = D (P∥Q) is
Pr (X >t)toc/tforsomec,t,thenthemeanofP will KL
Pt t bounded,thenE [V]isbounded,andE [V]−E [V]→0
beapproximatelyatleastc. Ast→∞,theKLdivergence P P Q
asd→0.
D (P ∥Q)willshrinktozero.
KL t
Corollary1. Theorems2and3implythatwhenutilityis
3.2.RLHFwithKLpenaltyunderheavy-tailedreturn
light-tailed,rewardmodelingerrorsmaketheproxyreward
distribution
heavy-tailed,andapolicyπisregularizedseverelyenough
Wenowadaptourresulttothecasewherethepolicyisa tohaveKLdivergencevaluesapproachingzero,thereward
languagemodelandwearetrainingitusingRLHF.Weare E[U(π)]cangotoinfinitywhileutilityE[V(π)]approaches
nowapplyingKLdivergenceoverthepoliciesratherthan avaluenohigherthanthebasepolicy.
thereturndistributions. Wefirstformallydefinetheprop-
ertiesofRLHFonlanguagemodelsthatcausetheresultto 3.4.Light-tailed+independenceimplyE[V]→∞
hold: namely,whenwhenconsideredasaMarkovdecision
process(MDP),environmentaltransitionsaredeterministic Theorem 4. If U = X + V with X and V both
andreturndependsonlyonthefinalstatereached. light-tailed, and the distribution of U is continuous,
and π∗(β) ≜ argmax E[U(π)] − βD (π,π ), then
π KL 0
Definition:Adeterministic-transitionMDPwithMarkovian lim E[V(π∗(β))]=∞.
β→0+
returns(DMRMDP)isanMDP(S,A,P,R)suchthat:
4.ExperimentalMethodology
• ThetransitionfunctionP :S×A→Sisdeterministic,
Our theoretical results now raise the question of whether
i.e.,foreachstates∈S andactiona∈A,thereexists
auniquestates′ ∈S suchthatP(s′|s,a)=1. theerrorinrewardmodelsisheavy-tailedorlight-tailedin
practice. 1 Ifweobservetherewarddistributiontobelight-
InRLHF: thetransition isappending thegenerated
tailed, this is a strong indication that error is light-tailed.
tokenatothecontexts.
2
• There is a set of sink states E ⊆ S that terminate a Toempiricallytestwhethertherewardisheavy-tailed,we
trajectory,whichisdisjointfromthesetofstartstates. considertwolinesofevidence: examiningthedistributions
In RLHF: The sink states are sequences ending in directlythroughrandomsamplingandtemperature-1sam-
<EOS>oraboveacertainlength. pling,andfindingadversarialtokensequencesthatgethigh
rewards. Weexamineonesmallandonemediumreward
model that performed reasonably well on RewardBench
• Returns are Markovian; that is, for any two trajec-
tories τ = (s ,a ,...,s ),τ′ = (s′,a′,...,s′ ), if (Lambertetal.,2023). ThesmallmodelisanOpenAssis-
1 1 n 1 1 n
s = s′ ,thenτ andτ′ haveidenticalreturndistribu- tantmodelbasedonPythia1.4B,andthemediummodelis
n n Starling7B-alpha(Zhuetal.,2023)3.
tions. Equivalently,forthetrajectoryrandomvariable
T =(S 1,A 1,...)distributedaccordingtoanypolicy, For random sampling, we sample 30000 length-1024 se-
withreturnG,G⊥⊥(S <i,A <i)|S iforanyi≥1. quencesofuniformlyrandomtokensandobservethedistri-
InRLHF:thereturnonlydependsonthefullgener- butionofrewardsassignedbybothPythia1.4BandLlama
atedstring,whichisthefinalstate. 7B-chat. We also use Llama 7B-chat to generate 16000
length-133 sequences and observe the distribution of re-
wardsassignedbyStarling7B-alpha.
Thelanguagemodelstochasticallyoutputsthenexttoken
agivens,andcorrespondstothepolicy. ADMRMDPis Becausesamplingisinefficientatprobingtheextremetail,
thereforeagoodmodelofRLHF. wealsofindtokensequencesthatoptimizeStarling7B-alpha
for reward. We considered Greedy Coordinate Gradient
Theorem 2. Let W = (S,A,P,R) be a deterministic-
transitionMDPwithMarkovianreturns.GivenW wedefine 1Notethatdistributionsoverafinitesetareboundedandcannot
thefunctionthattakespoliciestotrajectoriesTr : (S → be heavy-tailed in a technical sense, and models with a finite
∆A) → ∆(S ×A)∗, and the average return function g : contextwindowhaveafiniteinputspace.Wesaythatadistribution
(S×A)∗ →R,whichinducesafunctionG:∆(S×A)∗ → ofrewardorerrorisheavy-tailedifitiswell-modeledbyaheavy-
∆R. Letπ :S →∆Abesomebasepolicy. IfG◦Tr(π ) taileddistributiononitssupport.
0 0 2ItispossibleforU tobelight-tailedwhileXandV areboth
isheavy-tailedwithfinitemeanµ ,thenforanyM,ϵ>0,
Q heavy-tailed,butthisisunusualandwedonotexpectittohappen
thereisapolicyπwithmeanreturnE[U|U ∼G◦Tr(π)]> inpractice.
M andE s∈T,T∼Tr(π)[D KL(π(s)∥π 0(s))]<ϵ. 3https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha
3CatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
Figure1.Plotsofthedistributionofrewardfrom30000random Figure2.Plots of the reward distribution from 16000 token se-
length-1024 token sequences to Starling 7B-alpha. Clockwise quencesgeneratedbyLlama7B-chatoflength≤ 133,starting
fromtopleft:Thehistogramshowsaunimodaldistributionwitha withfiverandomtokens. Clockwisefromtopleft: Ahistogram
slightrightskew. Thenormalprobabilityplotindicatesthedata showstherewarddistributionhasaleftskew. Thenormalprob-
areheavier-tailedthannormal.TheHillestimator(errorbarsare ability plot suggests reward is approximately normal and thus
standarderror)appearstobe0.20forhighervaluesbutfluctuates light-tailed.TheHillestimatorplotshouldstabilizeifthedistribu-
for lower values. The exponential probability plot of the right tionisheavy-tailed,butitdoesnot;thus,thereisnoevidencethe
halfofthedistributionisconsistentwitheitherlightorheavytails distributionisheavy-tailed.Theexponentialprobabilityplotalso
(underheavytails,theslopewouldgotoinfinity). indicateslighttails,becausethecurveisbendingdownwards.
(cid:16) (cid:17)
(GCG)from(Zouetal.,2023),amethodusedtofindad- [(1−α)q(x)+α]log (1−α)q(x)+α + (1 − α)log(1 −
q(x)
versarialsuffixesthatcircumventjailbreaking,butdecided
α)(1−q(x)).
onafasterversionofGCGcalledAcceleratedCoordinate
Gradient(ACG)from(HaizeLabs,2024). SeeTableB.1.1 Whenαissmallbutmuchlargerthanq(x),weapproximate
(cid:16) (cid:17)
forACGhyperparameters. thistofirstorderasD (P∥Q)≈αlog α . InTheo-
KL q(x)
Generating plots took about 5 GPU-hours on 1x Nvidia rems1and2,weprovethatwhentheerrorissufficiently
H100,andrunningACGtookafurther8hours. heavy-tailed, a policy that gets extremely large reward a
small fraction of the time will achieve high expected re-
ward with low KL divergence. This is not the case here
5.Results
becausetherewardsachievedthroughACGweresmalland
Whensamplingtokensequences,boththePythiamodelon the log-probabilities extremely negative. For example, a
randominputs(Figure5)andStarling7B-alphaonLlama- policythatmatchesLlama2-chat’sbasereward99%ofthe
generatedinputs(Figure2)appearapproximatelynormal timeandusesthehighest-rewardinputgeneratedbyACG
and,therefore,light-tailed. Starlingonrandominputs(Fig- α =1%ofthetimewillhaveKLdivergencefromLlama
ure1isambiguous,withtheexponentialQ-Qplothavingan 2-chat of α(log(α)−1339.70) = 13.35 nats, but reward
outlierthatcouldindicateaheavy-taileddistribution,butthe onlyaboutα∗(2.2377−0.3329)=0.02571greaterthan
Hillestimatorisconsistentwithalight-taileddistribution. thebasemodel,farlessthancanbeobtainedwiththesame
BecauseLlama-7B-chatisamorereasonablebasemodel KLdivergencebyconditioning.
thanacompletelyrandompolicy,webelievethatStarling
7B-alphaismorelikelytobelight-tailedforthepurposesof 6.DiscussionandLimitations
ourtheoreticalresults.
6.1.HowlikelyiscatastrophicGoodhart?
The ACG results need some interpretation. The KL di-
vergence between two distributions P and Q if P is the The low-KL policies that result in catastrophic Good-
sameasQafraction1−αofthetime,butissomevalue hart are not a unique optimal policy, just one family of
x a fraction α of the time is given by D (P∥Q) = high-performing policies. When optimizing E[U(π)] −
KL
4CatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
βD (π,π ), the outcome depends on RL training dy- 6.3.Strongeroptimizationmethods
KL 0
namics; it could be that D → 0 causing catastrophic
KL
Wedidnotsearchtheentirespaceoftokensequences,so
Goodhart, but more likely both terms will go to infinity,
wecannotruleoutthattherewardisheavy-tailedenough
potentiallyallowingV → ∞. CatastrophicGoodhartcan
tocausecatastrophicGoodhartinsomesituations. Whileit
bepreventedbyusingalight-tailedorboundedrewardfunc-
isintractabletosearchthemorethan102000possibletoken
tion.
sequences, future work could get more evidence through
Evenso,catastrophicGoodhartislikelytooccurinmany morepowerfuloptimizationmethods.
scenarioswhereKLregularizationisnaivelyemployedin
anattempttoavoidGoodhart’sLaw: 6.4.Relationtopreviousoveroptimizationwork
Gaoetal.(2023)foundthatoptimizingtherewardofsmall
• If we maximize σ(E[U]) + D (Tr(π)∥Tr(π )),
KL 0 rewardmodelscausesoveroptimization:adecreaseinutility
whereσisaboundedfunction(e.g. sigmoid),allnear-
withincreasingoptimization. However,weobservedthat
optimalpolicieswillhaveV ≈0. Sincewecanonly
rewardmodelsarelight-tailed,and(Theorem4)thatinde-
obtainsomuchrewardfromσ(E[U]),itpaystomake
pendencecombinedwithlight-tailederrorpreventsoverop-
theKL(andthusV)gotozero.
timization. Wethinkthisdiscrepancyisexplainedbyde-
pendencebetweenerrorandutility. Policiesoptimizedfor
• IfwecapKLtoafinitevalue(ordynamicallyadjust
higherrormayactivatefeaturesintheproxyrewardmodels
theKLpenaltytotargetafiniteKL,asdoneinZiegler
that are undesirable according to the true utility function.
et al. (2020), then E[V] is also upper bounded by a
5 More research is needed to understand why high-error
finitevalue(seeTheorem3),andwethinkitislikely
completionshavelowutilityandtodesignrewardmodels
thatE[V]≈0. ConsideratoymodelwhereanAIcan
thatdonotsufferfromthisproblem;Perhapsitispossible
adjustthreeparameters: truequalityV ofresponses,
toconstructrewardmodelswhoseerrorsareindirections
frequencyofrewardhacking(producingactionswith
orthogonaltohumanpreferences,sothatthelarge-reward
extremelyhighX),andseverityofhacking(valueof
completionsdonothavelowerutility.
Xonthoseactions). Adjustingthepolicytoincrease
E[U] without increasing KL increase the severity of
7.Conclusion
hackingwhiledecreasingeitherfrequencyofhacking
orqualityofresponses. WhenE[U]isalreadylarge,
WehavearguedthatthepurposeoftheKLdivergenceregu-
decreasing quality has much better returns than de-
larizationinRLHFistomitigaterewardmisspecification.
creasingfrequency. ThisissimilartoTheorems5,8
However,wehavealsoproventhatwhenerrorsinthereward
abouthard-thresholdoptimization.
functionareheavy-tailed,itcannotservethispurpose: even
withzeroKLdivergence,therearepoliciesthatachievevery
• AnywaywemaximizeE[U(π)]−βD (π,π )re-
KL 0 highmisspecifiedrewardandnoactualreward.
sultsinverylargevaluesofE[U(π)],andtherearea
numberofargumentsthatextremeoptimizationforan When errors are light-tailed and independent, the KL di-
imperfectproxycanresultindecreasedutilitydueto vergencecanmitigatemisspecification,butwhentheyare
tradeoffsbetweenX andV; e.g., theconstrainedre- dependent,thismaynotbepossible. Thus,wemustlook
sourcescenarioin(Zhuang&Hadfield-Menell,2021). toplacesotherthantheKLobjectivetoexplainthecurrent
success of RLHF and ensure its continued success in the
future.
6.2.Independenceassumptions
Theorems2and3donotrequireanyindependenceassump- ImpactStatement
tion, but Theorems 4, 5, and 6 require that error X and
utility V are independent, which seems to be violated in AsthisworkaimstoimprovethesafetyoffutureMLsys-
practice. Future work could weaken this assumption, al- temsbycharacterizingapossiblefailuremodeofreward
thoughintuitivelyobviouswaystoweakenitresultinthe misspecification in RLHF, we hope the social impact is
statementbeingfalse. 4 positive. Weseenoparticularethicalissuestodiscuss.
4SupposethaterrorXislight-tailedconditionalonanyvalue 5Thereareotherexplanationspossible. Perhapsbetteropti-
ofV,butourproxyismerelyunbiased(E[X|V =v]=0forall mizationmethodswouldfindheavy-tailedrewardinopenreward
v).ThenthelimitofV underoptimizationforX+V stilldepends models;orOpenAI’srewardmodelshaveheavy-tailederror(and
ontherelationshipbetweenX andV. Iftheyareindependent, theirresultsarestraightforwardlyexplainedbyourTheorem1),
Theorem 6 says that lim E[V|X +V ≥ t] = ∞. But if whileopenrewardmodelshavelight-tailederror.
t→∞
V ∼N(0,1),andX|V ∼N(0,4)whenV ∈[−1,1],otherwise
X =0,thenlim E[V|X+V ≥t]=0.
t→∞
5CatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
References Wierman,A. Catastrophes,conspiracies,andsubexponentialdis-
tributions (part ii). https://rigorandrelevance.
Abdolmaleki,A.,Springenberg,J.T.,Tassa,Y.,Munos,R.,Heess,
wordpress.com/2013/12/17/
N.,andRiedmiller,M. Maximumaposterioripolicyoptimisa-
catastrophes-conspiracies-and-subexponential-distributions-part-ii/,
tion. CoRR,2018. URLhttp://arxiv.org/abs/1806.
2013. Accessed:2024-06-26.
06920v1.
Zhu, B., Frick, E., Wu, T., Zhu, H., and Jiao, J. Starling-7B:
Christiano,P.F.,Leike,J.,Brown,T.,Martic,M.,Legg,S.,and Improvingllmhelpfulness&harmlessnesswithrlaif,November
Amodei,D. Deepreinforcementlearningfromhumanprefer- 2023.
ences. InGuyon,I.,Luxburg,U.V.,Bengio,S.,Wallach,H.,
Fergus,R.,Vishwanathan,S.,andGarnett,R.(eds.),Advances Zhuang, S. and Hadfield-Menell, D. Consequences of mis-
inNeuralInformationProcessingSystems,volume30.Curran alignedAI. AdvancesinNeuralInformationProcessingSys-
Associates, Inc., 2017. URL https://proceedings. tems, 33:15762–15773, 2021. doi: 10.48550/arXiv.2102.
neurips.cc/paper_files/paper/2017/file/ 03896. URLhttps://arxiv.org/abs/2102.03896.
d5e2c0adad503c91f91df240d0cd4e49-Paper. arXiv:2102.03896v1[cs.AI].
pdf.
Ziegler,D.M.,Stiennon,N.,Wu,J.,Brown,T.B.,Radford,A.,
Clark, J. and Amodei, D. Faulty reward functions in the wild, Amodei, D., Christiano, P., and Irving, G. Fine-tuning lan-
December 2016. URL https://openai.com/index/ guagemodelsfromhumanpreferences,2020. URLhttps:
faulty-reward-functions/. Accessed:2024-07-07. //arxiv.org/abs/1909.08593. arXiv:1909.08593v2
[cs.CL].
Foss, S., Korshunov, D., and Zachary, S. An In-
troduction to Heavy-Tailed and Subexponential Distribu- Zou,A.,Wang,Z.,Kolter,J.Z.,andFredrikson,M. Universal
tions. Springer, 2 edition, 2013. doi: 10.1007/ andtransferableadversarialattacksonalignedlanguagemodels,
978-1-4614-7101-1. URL https://link.springer. 2023.
com/book/10.1007/978-1-4614-7101-1.
Gao, L., Schulman, J., and Hilton, J. Scaling laws for reward
modeloveroptimization. InKrause,A.,Brunskill,E.,Cho,K.,
Engelhardt,B.,Sabato,S.,andScarlett,J.(eds.),Proceedings
of the 40th International Conference on Machine Learning,
volume 202 of Proceedings of Machine Learning Research,
pp.10835–10866.PMLR,23–29Jul2023. URLhttps://
proceedings.mlr.press/v202/gao23h.html.
Haize Labs. Making a sota adversarial attack on llms 38x
faster.https://blog.haizelabs.com/posts/acg/,
March2024. Accessed:2024-05-22.
Jaques, N., Ghandeharioun, A., Shen, J. H., Ferguson, C.,
Lapedriza, A., Jones, N., Gu, S., and Picard, R. Way off-
policy batch deep reinforcement learning of implicit human
preferencesindialog. arXivpreprintarXiv:1907.00456,2019.
Laidlaw,C.,Singhal,S.,andDragan,A. Preventingrewardhack-
ingwithoccupancymeasureregularization. arXiv,2024. URL
https://www.arxiv.org/abs/2403.03185. Ac-
cessed:2024-07-07.
Lambert,N.andCalandra,R. Thealignmentceiling: Objective
mismatchinreinforcementlearningfromhumanfeedback,feb
2024.
Lambert,N.,Pyatkin,V.,Morrison,J.,Miranda,L.,Lin,B.Y.,
Chandu, K., Dziri, N., Kumar, S., Zick, T., Choi, Y., Smith,
N. A., and Hajishirzi, H. Rewardbench: Evaluating reward
modelsforlanguagemodeling,2023.URLhttps://arxiv.
org/abs/2403.13787. 40pages,19figures,12tables.
Schulman,J.,Levine,S.,Moritz,P.,Jordan,M.I.,andAbbeel,P.
Trustregionpolicyoptimization. CoRR,2015. URLhttp:
//arxiv.org/abs/1502.05477v5.
Schulman,J.,Wolski,F.,Dhariwal,P.,Radford,A.,andKlimov,
O.Proximalpolicyoptimizationalgorithms.CoRR,2017.URL
http://arxiv.org/abs/1707.06347v2.
6CatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
Figure3.Ast→∞,themeanofX (bluebar)growswithoutboundwhileKLdivergenceD (P ∥Q)(orangebar)goesto0. The
KL t
basedistributionQisaStudentt-distributionwithdf =3.Inthiscase,highvaluesofXareupweightedto1/t0.8;upweightingthemto
1/twouldcauseE[X]toconvergeto 1whileKLdivergencegoestozerofaster.
A.Proofs
A.1.Theorem1
Restatement of Theorem 1. Given any heavy-tailed reference distribution Q over R with mean µ , and any M,ϵ > 0, there is a
Q
distributionP withmeanµ >M andD (P∥Q)<ϵ.
P KL
Intuitively,inaheavy-taileddistribution,eventswithextremelyhighxarenotveryrare,soyoudon’tpaymuchofaKLpenaltyto
upweightthemsotheyhappenabout1/xofthetime.ThisisvisuallyillustratedinFigureA.1.
Proof. WLOGletµ = 0. Weconstructasequenceofdistributions{P }suchthatlim E [X] ≥ cforanyconstantc, and
lim D (P ∥QQ ) = 0. We define P for any t > c thusly. Writingt F (x) for thet→ C∞ DFP Pt r (X ≤ x) and F¯ (x) for
t→∞ KL t t Pt X∼Pt Pt
1−F (x),welet
Pt
(cid:40) 1− 1−c/tF (x) x≤t
F¯_{P_t}(x)= FQ(t) Q
c/t F¯ (x) x>t
F¯ Q(t) Q
Intuitively,werescalethepartofthedistributiontotherightoftevenlytohavetotalprobabilityc/t,whichislessthan1becauset>c.
Wemustcheckthatlim E [X]=c.Wecanwrite
t→∞ Pt
E [X]=F (t)E [X|X ≤t]+F¯ (t)E [X|X >t]
Pt Pt Pt Pt Pt
=F (t)E [X|X ≤t]+F¯ (t)E [X|X >t]
Pt Q Pt Q
=F (t)E [X|X ≤t]+F¯ (t)E [X|X >t]+
Q Q Q Q
(F (t)−F (t))E [X|X ≤t]+(F¯ (t)−F¯ (t))E [X|X >t]
Pt Q Q Pt Q Q
=E [X]+(F¯ (t)−F¯ (t))(E [X|X >t]−E [X|X ≤t])
Q Pt Q Q Q
WeknowthatE [X|X >t]>tbecauseitisanintegralofvaluesstrictlygreaterthant.BecauseE [X]=0isaweightedaverageof
Q Q
E [X|X >t]andE [X|X ≤t],andE [X|X >t]>0,weknowE [X|X ≤t]<0.SoE [X|X >t]−E [X|X ≤t]>t.We
Q Q Q Q Q Q
alsoknowthatforsufficientlylarget,(F (t)−F (t))>0.Intuitively,startingfromQ,whichhasmean0,P movesaprobability
Pt Q t
massapproaching c frommean<0tomean>t.
t
Nowwecansay
lim E [X]> lim (cid:2)E [X]+(F¯ (t)−F¯ (t))(t−0)(cid:3) = lim (cid:16)c −F¯ (t)(cid:17) t= lim c−tF¯ (t)
t→∞ Pt t→∞ Q Pt Q t→∞ t Q t→∞ Q
BecauseQhasafinitemean,lim tF¯ (t)=0,andsolim E [X]≥c.
t→∞ Q t→∞ Pt
Nowwecheckthatlim D (P ∥Q)=0:
t→∞ KL t
7CatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
(cid:90) P (dx)
D (P ∥Q)= log t P (dx)
KL t Q(dx) t
R
(cid:90) P (dx) (cid:90) P (dx)
= log t P (dx)+ log t P (dx)
Q(dx) t Q(dx) t
x≤t x>t
F (t) F¯ (t)
=F (t)log Pt +F¯ (t)log Pt sincebothratiosareconstant
Pt F (t) Pt F¯ (t)
Q Q
1−c/t F¯ (t)
=F (t)log +F¯ (t)log Pt
Pt F (t) Pt F¯ (t)
Q Q
Sinceboth1−c/tandF (t)goto1ast→∞,thelefttermgoesto0,andso
Q
F¯ (t)
lim D (P ∥Q)≤0+ lim F¯ (t)log Pt
t→∞ KL t t→∞ Pt F¯ Q(t)
c c c 1
= lim log ≤ lim log
t→∞ t tF¯ Q(t) t→∞ t F¯ Q(t)
c
= lim − logF¯ (t) sincet>c
t→∞ t Q
Qisheavy-tailed,sobydefinitionlim eatF¯ (t)=∞ foralla>0.Thisimpliesthatforeverya>0thereisasufficientlylarge
t→∞ Q
t sothatforallt>t ,F¯ (x)>e−at,whichmeansthatlogF¯ (t)>−at.
c c Q Q
Thereforeforeverya>0,lim D (P ∥Q)≤lim −c/tlogF¯ (t)<lim −−act =ac,whichsinceKLdivergenceis
nonnegativemeansthatlim t→ D∞ (K PL ∥Q)t =0asdesit r→ ed∞ .■ Q t→∞ t
t→∞ KL t
A.2.Theorem2
RestatementofTheorem2. LetW =(S,A,P,R)beadeterministic-transitionMDPwithMarkovianreturns.GivenW,wedefinethe
functionthattakespoliciestotrajectoriesTr:(S →∆A)→∆(S×A)∗,andtheaveragereturnfunctiong:(S×A)∗ →Rwhich
inducesafunctionG:∆(S×A)∗ →∆R.Letπ :S →∆Abesomebasepolicy.IfG◦Tr(π )isheavy-tailedwithfinitemeanµ ,
0 0 Q
thenforanyM,ϵ>0,thereisapolicyπwithmeanreturnE[U|U ∼G◦Tr(π)]>M andE [D (π(s)∥π (s))]<ϵ.
s∈T,T∼Tr(π) KL 0
Proof:WewillexhibitadistributionoftrajectoriesρsuchthatD (ρ∥Tr(π ))<ϵandE[G(ρ)]>M,andthenconstructapolicyπ
KL 0
withTr(π)=ρ.Notethatthisproofappliesforcontinuousactionspacesiftrajectoriesarereplacedwithmeasurablesets,butthiswould
makeithardertoread.
Letρ =Tr(π ).Wehaveaheavy-taileddistributionofreturnQ≜G(ρ )overR,sowecanapplyTheorem1.Buttodefineρ,we
π0 0 π0
canconstructP intheproofofTheorem1inaparticularway.Foranyt>c,weneedaP thatuniformlyupweightsvaluesofmean
t t
returnsuchthatF¯ (t)=c/t.Wecandefineρ suchthatanytrajectoryτ isupweightedbyafactordependingonlyonitsmeanreturn:
Pt t
(cid:40)1−c/tρ
(τ) g(τ)≤t
ρ (τ)= FQ(t) π0
t c/t ρ (τ) g(τ)>t
F¯ Q(t) π0
ThenwecanletP ≜ G◦ρ andtherestoftheproofofTheorem1applies. Therefore,applyingthetheorem,wecanletρ = ρ
t t t
forsufficientlylarget,andthenµ > M andD (G◦ρ,G◦ρ ) < ϵ. BythechainruleforKLdivergence,D (ρ,ρ ) =
D (G◦ρ,G◦ρ )+E [G D◦ρ (ρ(T)|G(T)K =L γ ∥ρ (T)|Gπ0 (T) = γ)]. SinceweconstructedρsothatthepK roL babilitπ i0 esof
KL π0 γ∼G◦ρ KL π0
eachτ conditionalonitsreturnbeingγareequal,thesecondtermiszero,andwealsohaveD (ρ,ρ )<ϵ.
KL π0
Finally,sincetheKLdivergencebetweentrajectorydistributionsisthesumofKLdivergencebetweenpoliciesateachactioninthe
trajectory,andeachtrajectoryhasatleastoneaction,E [D (π(s)∥π (s))] ≤ E (cid:80) [D (π(s)∥π (s))] =
s∈T,T∼Tr(π) KL 0 T∼Tr(π) s∈T KL 0
D (ρ∥ρ )<ϵasdesired.
KL π0
TodefineπsuchthatTr(π)=ρ,weletπ(s,a)=Pr(a =a|τ =(...,s,a ,...)∼ρ).
i i
Then,theprobabilitythatanytrajectoryτ =(s ,a ,...,a )issampledis:
1 1 n
8CatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
n
(cid:89)
Tr(π)(τ)= π(s ,a ) (1)
i i
i=1
n
=(cid:89) Pr(a =a′|τ′ =(...,s,a′,...)∼ρ) (2)
i i i
i=1
n
=(cid:89) Pr(a =a′|τ′ =(s′,a′,...,s,a′,...)∼ρ,s =s′ ,a =a′ ) (3)
i i 1 1 i <i <i <i <i
i=1
=ρ(τ) (4)
In(2),returnsareMarkovian,soalltrajectoryprefixesendinginstateshavethesamedistributionofreturnsunderanypolicy.Inthe
constructionofρ,alltrajectorieswiththesamemeanreturnhaveequalmeasure.Therefore,conditioningonearlierstatesandactionsofτ
doesnotchangethemeasure,sowecanwrite(3).SoTr(π)=ρasdesired.■
A.3.Theorem3
RestatementofTheorem3. IfV islight-tailed,E [V]iszero,andd=D (P∥Q)isbounded,thenE [V]isbounded,andE [V]→0
Q KL P P
asd→0.
Proof.UsingLagrangemultipliers,wefindthatwhenKLdivergenceisminimized,wehaveP(V)[λ logP(V) +λ −X]=0forsome
1 Q(V) 2
constantsλ ,λ ,so
1 2
P(V) V −λ
log = 2 (5)
Q(V) λ
1
(cid:18) (cid:19)
V −λ
P(V)=Q(V)exp 2 =Q(V) (6)
λ
1
eV/λ_1e−λ_2/λ_1 =CQ(V)eV/λ_1 (7)
Thatis,thenewPDFisanexponentialtiltingoftheoldPDF.Now,whatisE P[V]?It’sjust(cid:82) −∞ ∞CVeV/λ1Q(X)dV.Ifthedistribution
ofVisheavy-taileddistribution,thisis∞;ifitislight-tailed,thisissomefinitevalue.
Whend=0,P andQareidentical,andE[V]=0.Sobyacontinuityargument,E [V]→0asd→0.■
P
A.4.Theorem4
RestatementofTheorem4. IfU = X +V withX andV bothlight-tailed,andthedistributionofUiscontinuous, andπ∗(β) ≜
argmax E[U(π)]−βD (π,π ),thenlim E[V(π∗(β))]=∞.
π KL 0 β→0+
Proof.Fixsomeβ.UsingLagrangemultipliers,wefindthatforanyeventS,Pr (S)=Pr (S)eλU(S).Letc(β)bethemedianvalueof
π π0
U underthepolicyπ∗(β);thatis,Pr(U >c(β)|U ∼G◦Tr(π∗(β)))= 1.ThisexistsbecauseU hasacontinuousdistribution.Then:
2
1 1
E[V|π]= E[V|π,U <c]+ E[V|π,U ≥c]
2 2
1 1
≥ E[V|π,U <c]+ E[V|π]
2 2
1 1
lim E[V|π]≥ lim E[V|π,U <c]+ lim E[V|π]
β→0+ β→0+ 2 β→0+ 2
Thelefttermisc,whiletherighttermis∞,sotheoveralllimitis∞.
B.Conditioningasalternatemodelofoptimization
AlthoughwethinkaKLdivergencepenaltyorcapisthemostrealisticsettingforRLHF,itisnottheonlymodelofoptimization
whereheavy-tailednessoftheerrordetermineswhethercatastrophicGoodhartoccurs.Consideranothermodelofoptimizationwhere
U =X+V asbefore,butwesimplyconditiononU beinghigherthansomethresholdt.6 Inthiscase,weareinterestedinthequantity
6Thiscouldmodelasatisficingagentthattakesrandomacceptableactions.
9CatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
Region WhyitseffectonE[V|c]issmall Explanation
r =(−∞,−h(t)] P[V ∈r |c]istoolow Inthisregion,|V|>h(t)andX >t+h(t),both
1 1
ofwhichareunlikely.
r =(−h(t),h(t)) E[V|V ∈r ,c]≈E[V|V ∈r ] ThetaildistributionofXistooflattochangethe
2 2 2
shapeofV ’sdistributionwithinthisregion.
r =[h(t),t−h(t)] P[V ∈r |c]islow,andV <t. Thereareincreasingreturnstoeachbitofoptimiza-
3 3
tionforX,soit’sunlikelythatbothXandVhave
moderatevalues. 8
r =(t−h(t),∞) P[V ∈r |c]istoolow X is heavier-tailed than V, so the condition that
4 4
V >t−h(t)ismuchlesslikelythanX >t−h(t)
inr .
2
Table1. AsummaryoftheproofstrategyforTheorem5.
lim E[V|X+V ≥t].Ifweslightlystrengthentheheavy-tailednessandlight-tailednessassumptions,heavy-tailederrorresultsin
t→∞
catastrophicGoodhart,whilelight-tailederrorresultsinarbitrarilyhighexpectedutility.
B.1.Conditioningwithheavy-tailederrorproducescatastrophicGoodhart
Theorem5. LetXandV betwoindependentrandomvariableswithCDFsF andF andtailfunctionsF¯ ≜1−F ,F¯ ≜1−F
X V V V X X
suchthat
• V hasafinitemean.
• Xissubexponential;thatis,lim Pr(X1+X2>x) =2ifX ,X aretwoindependentsamplesfromX.Thisisaslightlystronger
x→∞ Pr(X>x) 1 2
propertythanbeingheavy-tailed.
• ThetailofV issufficientlylighterthanthetailofXsuchthatlim tpF¯ V(t) =0forsomep>1.
t→∞ F¯ X(t)
Thenlim E[V|X+V ≥t]=E[V];thatis,catastrophicGoodhartoccursinthelimitofoptimizationforU =X+V.
t→∞
Theproofrequiresexpressingtheconditionalexpectationinquestionas (cid:82) (cid:82)−∞ −∞∞ ∞v ff VV (( vv )) PP rr (( XX >> tt −− vv )) ,thenpartitioningtheinterval(−∞,∞)
intofourregionsandboundingtheintegrandinthenumeratorabovebyadifferentquantityineachregion.
Inadditiontotheworkscitedinthemainpaper,wemakereferencetothetextbook(Fossetal.,2013)throughouttheproof.Manysimilar
resultsaboutrandomvariablesarepresentinthetextbook.
B.1.1.PROOFSKETCHANDINTUITIONS
TheconditionalexpectationE[V|X +V > t]isgivenby (cid:82) (cid:82)−∞ −∞∞ ∞v ff VV (( vv )) PP rr (( XX >> tt −− vv )) ,7 andwedividetheintegralinthenumeratorinto
4regions,showingthateachregion’seffectontheconditionalexpectationofVissimilartothatofthecorrespondingregioninthe
unconditionalexpectationE[V].
Theregionsaredefinedintermsofaslow-growingfunctionh(t):R→R suchthatthefiddlyboundsondifferentpiecesoftheproof
≥0
workout.Roughly,wewantittogotoinfinitysothat|V|islikelytobelessthanh(t)inthelimit,butgrowslowlyenoughthattheshape
ofV’sdistributionwithintheinterval[−h(t),h(t)]doesn’tchangemuchafterconditioning.
InTableB.1.1,weabbreviatetheconditionX+V >tasc.
Notethatuptoaconstantverticalshiftofnormalization,thegreencurveisthepointwisesumoftheblueandorangecurves.
7We’llgenerallyomitdxanddvtermsintheinterestsofcompactnessandconciseness;theimplieddifferentialsshouldbeprettyclear.
10CatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
Figure4.Adiagramshowingtheregionboundariesat−h(t),h(t),andt−h(t)inanexamplewheret=25andh(t)=4,alongwitha
negativelogplotoftherelevantdistribution:
B.1.2.DEFINITIONS
Tobemoreprecise,we’regoingtomakethefollowingdefinitionsandassumptions:
Letf (v)bethePDFofV atthevaluev.Weassumeforconveniencethatf exists,isintegrable,etc,thoughwesuspectthatthisisn’t
V V
necessary,andthatonecouldworkthroughasimilarproofjustreferringtothetailsofV.Wewon’tmakethisassumptionforX.Let
F (x)=Pr(X ≤x)andF¯ (x)=Pr(X >x),similarlyforF andF¯ .Assumethat
X X V V
• V
hasafinitemean:(cid:82)∞
vf (v)dvconvergesabsolutely.
−∞ V
• Xissubexponential.
Formally,thismeansthatlim Pr(X1+X2>x) = 2. ThisoccursroughlywheneverX hastailsthatareheavierthane−cx forany
x→∞ Pr(X>x)
candisreasonablywell-behaved;counterexamplestotheclaim"long-tailedimpliessubexponential"exist,butthey’renontrivialto
exhibit. Examplesofsubexponentialdistributionsincludelog-normaldistributions,anythingthatdecayslikeapowerlaw,thePareto
distribution,anddistributionswithtailsasymptotictoe−xa
forany0<a<1.
WerequireforV thatitstailfunctionissubstantiallylighterthanX’s,namelythatlim tpF¯ V(t) =0forsomep>1.(Thisimplies
t→∞ F¯ X(t)
thatF¯ (t)=O(F¯ (t)/t).)
V X
Withthesedefinitionsandassumptions,wecanmoveontotheproof.
TheunnormalizedPDFofV conditionedonX+V ≥tisgivenbyf (v)F¯ (t−v).Itsexpectationisgivenby (cid:82) −∞ ∞vfV(v)F¯ X(t−v) .
V X (cid:82) −∞ ∞fV(v)F¯ X(t−v)
Meanwhile,theunconditionalexpectationofVisgivenby(cid:82)∞
vf (v).
−∞ V
We’dliketoshowthatthesetwoexpectationsareequalinthelimitforlarget. Todothis,we’llintroduceQ(v) = F¯ X(t−v). (More
F¯ X(t)
pedantically,thisshouldreallybeQ (v),whichwe’lloccasionallyusewhereit’shelpfultorememberthatthisisafunctionoft.)
t
Foragivenvalueoft,Q(v)isjustascaledversionofF¯ X(t−v),sotheconditionalexpectationofV isgivenby (cid:82) (cid:82)−∞ −∞∞ ∞v ff VV (( vv )) QQ (( vv )) .
ButbecauseQ(0)=1,thenumeratoranddenominatorofthisfractionare(forsmallv)closetotheunconditionalexpectationand1,
respectively.
(cid:12) (cid:12)
We’ll aim to show that for all ϵ > 0, we have for sufficiently large t that (cid:12)(cid:82)∞ vf (v)Q (v)−(cid:82)∞ vf (v)(cid:12) < ϵ and
(cid:12) −∞ V t −∞ V (cid:12)
(cid:82)∞
f (v)Q (v) ∈ [1−ϵ,1+ϵ], which implies (exercise) that the two expectations have limiting difference zero. But first we
−∞ V t
needsomelemmas.
11CatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
B.1.3.LEMMAS
Lemma6. Thereish(t)dependingonF suchthat:
X
(a) lim h(t)=∞
x→∞
(b) lim t−h(t)=∞
t→∞
(c) lim F¯ X(t−h(t)) =1
t→∞ F¯ X(t)
(d) lim sup |Q(v,t)−1|=0.
t→∞ |v|≤h(t)
Proof. Lemma2.19from(Fossetal.,2013)impliesthatifXislong-tailed(whichitis,becausesubexponentialimplieslong-tailed),then
thereish(t)suchthatcondition(a)holdsandF¯ ish-insensitive;byProposition2.20wecantakehsuchthath(t)≤t/2forsufficiently
X
larget,implyingcondition(b).Conditions(c)and(d)followfrombeingh-insensitive.
Lemma7. SupposethatF iswhole-linesubexponentialandhischosenasinLemma1. AlsosupposethatF¯ (t)=O(F¯ (t)/t).
X V X
ThenPr[X+V >t, V >h(t), X >h(t)]=o(F¯ (t)/t).
X
Proof. Thisisaslightvariationonlemma3.8from(Fossetal.,2013),andfollowsfromtheproofofLemma2.37.Lemma2.37statesthat
Lemma2.37.LethbeanyincreasingfunctiononR+suchthath(x)→∞.Then,foranydistributionsF ,F ,G ,andG
1 2 1 2
onR,
P{ξ +η >x,ξ >h(x),η >h(x)} F (x) G (x)
limsup 1 1 1 1 ≤limsup 1 ·limsup 1 ,
x→∞ P{ξ 2+η 2 >x,ξ 2 >h(x),η 2 >h(x)} x→∞ F 2(x) x→∞ G 2(x)
whereξ ,ξ ,η ,andη areindependentrandomvariableswithrespectivedistributionsF ,F ,G andG .
1 2 1 2 1 2 1 2
butitisactuallyprovedthat
P{ξ +η >x,ξ >h(x),η >h(x)}≤
1 1 1 1
F (z) G (z)
sup 1 · sup 1 ·P{ξ +η >x,ξ >h(x),η >h(x)}. (8)
2 2 2 2
F (z) G (z)
z>h(x) 2 z>h(x) 2
IfweletF =F ,F =G =G =F ,thenweget
1 V 2 1 2 X
P{X+V >t,X >h(t),V >h(t)}
≤ sup F¯ V(z) sup F¯ X(z) P(cid:8) X+X′ >t,X >h(t),X′ >h(t)(cid:9)
F¯ (z) F¯ (z)
z>h(t) X z>h(t) X
= sup F¯ V(z) P(cid:8) X+X′ >t,X >h(t),X′ >h(t)(cid:9)
F¯ (z)
z>h(t) X
(9)
whereX,X′ ∼F .Multiplyingbyt,wehave
X
tP{X+V >t,X>h(t),V >h(t)} ≤ sup tF¯ V(z) P(cid:8) X+X′ >t,X>h(t),X′>h(t)(cid:9) , (10)
F¯ (z)
z>h(t) X
andbecauseh(t) → ∞ast → ∞andF¯ (t) = O(F¯ (t)/t), wecansaythatforsomec < ∞, lim sup tF¯ V(z) < c.
V X t→∞ z>h(t) F¯ X(z)
ThereforeforsufficientlylargetP{X+V >t,X >h(t),V >h(t)}≤ cP{X+X′ >t,X>h(t),X′>h(t)}.
t
ByTheorem3.6,P{X+X′ >t,X>h(t),X′>h(t)}iso(F¯ (t)),sotheLHSiso(F¯ (t)/t)asdesired.
X X
12CatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
B.1.4.BOUNDSONTHENUMERATOR
(cid:12) (cid:12)
We want to show, for arbitrary ϵ > 0, that (cid:12)(cid:82)∞ vf (v)Q(v)−(cid:82)∞ vf (v)(cid:12) < ϵ in the limit as t → ∞. Since
(cid:12) −∞ V −∞ V (cid:12)
(cid:12) (cid:12)
(cid:12)(cid:82)∞ vf (v)Q(v)−(cid:82)∞ vf (v)(cid:12) ≤ (cid:82)∞ |vf (v)(Q(v)−1)| = (cid:82)∞ |v|·f (v)·|Q(v)−1| it will suffice to show that the
(cid:12) −∞ V −∞ V (cid:12) −∞ V −∞ V
latterquantityislessthanϵforlarget.
We’regoingtoshowthat(cid:82)∞
|v|·f (v)·|Q(v)−1|issmallbyshowingthattheintegralgetsarbitrarilysmalloneachoffourpieces:
−∞ V
(−∞,−h(t)],(−h(t),h(t)),[h(t),t−h(t)],and(t−h(t),∞).
We’llhandlethesecasebycase(they’llgetmonotonicallytrickier).
Region1: (−∞,−h(t)] Since(cid:82)∞ vf (v)isabsolutelyconvergent,forsufficientlylargetwewillhave(cid:82)−h(t)|v|f (v) < ϵ,
−∞ V −∞ V
sinceh(t)goestoinfinitybyLemma1(a).
SinceQ(v)ismonotonicallyincreasingandQ(0)=1,weknowthatinthisinterval|Q(v)−1|=1−Q(v).
Sowehave(cid:82)−h(t)|v|·f (v)·|Q(v)−1|=(cid:82)−h(t)|v|f (v)(1−Q(v))<(cid:82)−h(t)|v|f
(v)<ϵasdesired.
−∞ V −∞ V −∞ V
Region 2: (−h(t),h(t)) By lemma 1(d), h is such that for sufficiently large t, |Q(v)−1| < (cid:82) −∞ ∞|vϵ |fV(v) on the interval
[−h(t),h(t)].(NotethatthevalueofthisupperbounddependsonlyonV
andϵ,notontorh.)Sowehave(cid:82)h(t)
|v|f (v)|Q(v)−1|<
−h(t) V
(cid:82) −∞
∞|vϵ |fV(v)(cid:82) −h h(t () t)|v|f V(v)<
(cid:82) −∞
∞|vϵ |fV(v)(cid:82) −∞ ∞|v|f V(v)=ϵ.
Region3: [h(t),t−h(t)] Forthethirdpart,we’dliketoshowthat(cid:82)t−h(t)vf (v)(Q(v)−1)<ϵ.Since(cid:82)t−h(t)vf (v)(Q(v)−
h(t) V h(t) V
1)<(cid:82)t−h(t)tf (v)Q(v)= t (cid:82)t−h(t)f (v)F¯ (t−v)itwouldsufficetoshowthatthelatterexpressionbecomeslessthanϵfor
larget,h o(t r) equivaV lentlythat(cid:82)t−F¯ X h(( tt )) fh( (t v) )F¯ (V t−v)X =o(cid:16) F¯ X(t)(cid:17) .
h(t) V X t
The LHS in this expression is the unconditional probability that X +V > t and h(t) < V < t−h(t), but this event implies
X+V >t,V >h(t),andX >h(t).Sowecanwrite
(cid:90) t−h(t)
f (v)F¯ (t−v)=Pr[X+V >t, h(t)<V <t−h(t)]
V X
h(t)
<Pr[X+V >t, V >h(t), X >h(t)]=o(F¯ (t)/t)
X
byLemma2.
Region4: (t−h(t),∞) Forthefourthpart,we’dliketoshowthat(cid:82)∞ vf (v)Q(v)→0forlarget.
t−h(t) V
SinceQ(v)= F¯ X(t−v) < 1 ,itwouldsufficetoshow(cid:82)∞ vf (v)=o(F¯ (t)).Butnotethatsincelim F¯ X(t−h(t)) =1by
Lemma1(c),thisF¯ iX s( et q) uivaleF¯ nX tt( ot) (cid:82)∞ vf (v)=o(F¯ (tt −−h h(t () t)))V ,which(byLX emma1(b))isequivalentto(cid:82)t→ ∞∞ vf (F v¯ X )( =t) o(F¯ (t)).
t−h(t) V X t V X
Notethat(cid:82)∞vf (v)=t(cid:82)∞f (v)+(cid:82)∞(v−t)f (v)=tF¯ (t)+(cid:82)∞F¯ (v),soitwillsufficetoshowthatbothtermsinthissum
t V t V t V V t V
areo(F¯ (t)).
X
ThefirsttermtF¯ (t)iso(F¯ (t))becauseweassumedlim tpF¯ V(t) =0forsomep>1.
V X t→∞ F¯ X(t)
Forthesecondterm,wehaveforthesamereason(cid:82)∞F¯ (v)<(cid:82)∞ F¯ X(v) =F¯ (t)(cid:82)∞v−p = t1−pF¯ (t)=o(F¯ (t)).
t V t vp X t p−1 X X
B.1.5.BOUNDSONTHEDENOMINATOR
Forthedenominator,wewanttoshowthatlim
(cid:82)∞
f (v)Q
(v)=1=(cid:82)∞
f
(v),soit’llsufficetoshow|(cid:82)∞
f (v)(Q (v)−
t→∞ −∞ V t −∞ V −∞ V t
1)|=o(1)ast→∞.Again,we’llbreakupthisintegralintopieces,thoughthey’llbemorestraightforwardthanlasttime.We’lllookat
(−∞,−h(t)),[−h(t),h(t)],and(h(t),∞).
•
|(cid:82)−h(t)f (v)(Q(v)−1)|=(cid:82)−h(t)f (v)(1−Q(v))<(cid:82)−h(t)f
(v).
−∞ V −∞ V −∞ V
– Butsinceh(t)goestoinfinity,thislefttailoftheintegralwillcontainlessandlessofV ’sprobabilitymassastincreases.
•
|(cid:82)h(t)
f
(v)(Q(v)−1)|≤(cid:82)h(t)
f (v)|Q(v)−1|
−h(t) V −h(t) V
13CatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
Figure5.HistogramandnormalprobabilityplotofrewardassignedbyPythiaRMtorandomlength-1024tokensequences.TheQ-Qplot
suggeststhedistributionisapproximatelynormal,whichismuchlighter-tailedthanexponential.
• ≤sup
|Q(v,t)−1|(cid:82)h(t)
f (v)≤sup |Q(v,t)−1|
|v|≤h(t) −h(t) V |v|≤h(t)
– ByLemma1(d)weknowthatthisgoestozeroforlarget.
•
|(cid:82)∞
f
(v)(Q(v)−1)|=(cid:82)∞
f
(v)(Q(v)−1)<(cid:82)∞
f (v)Q(v).
h(t) V h(t) V h(t) V
Butforsufficientlylargetwehaveh(t)>1,soweobtain(cid:82)∞
f
(v)Q(v)<(cid:82)∞
vf
(v)Q(v)<(cid:82)∞
vf (v)Q(v)=o(1)bythe
h(t) V h(t) V −∞ V
resultsoftheprevioussection.Thiscompletestheproof.
B.2.Conditioningwithlight-tailederrorproducesarbitrarilyhighutility
Theorem8. LetX,V beindependentrandomvariablessuchthatlim F¯ X(t+1) =0.(ThisimpliesthatXhastailsthataredominated
t→∞ F¯ X(t)
bye−cxforanyc,thoughit’saslightlystrongerclaimbecauseitrequiresthatXnothavelargejumpsinthedecayofitstails.)Thenfor
anyVwithafinitemeanwhichhasnoupperbound,lim E[V|X+V >t]=∞.
t→∞
Theorem8generalizesaconsequenceofthe"RegressionalGoodhartIdentity"in(Gaoetal.,2023).
Proof. LetPr(V >c+1)=p>0,whichexistsbyourassumptionthatV isunbounded.
LetE[V|V <c]=q.(Ifthisisundefinedbecausetheconditionalhasprobability0,we’llhavethedesiredresultanywaysincethenV
wouldalwaysbeatleastc.)
Observethatforallt,E[V|V <c,X+V >t]≥q(assumingitisdefined),becausewe’reconditioning(V|V <c)onaneventwhich
ismorelikelyforlargerv(sinceXandV areindependent).
First,let’sseethatlim P(V<c|X+V≥t) =0.Thisratioofprobabilitiesisequalto
t→∞ P(V>c+1|X+V≥t)
(cid:82) (cid:82)− cc
∞
+∞ 1ff VV (( vv )) FF ¯¯ XX (( tt −− vv )) ≤
(cid:82)
c(cid:82)
∞
+−c 1∞ fVfV (v( )v F¯) XF¯ X (t( −t− c−c)
1)
=
F¯
XF¯ X (t( −t− c−c)
1)
· (cid:82) (cid:82)− cc
∞
+∞ 1ff VV (( vv ))
= F¯ X(t−c) · Pr(V<c) ≤ F¯ X(t−c) · 1
F¯ X(t−c−1) Pr(V>c+1) F¯ X(t−c−1) p
which,byourassumptionthatlim F¯ X(t+1) =0,willgetarbitrarilysmallastincreasesforanypositivep.
t→∞ F¯ X(t)
Now,considerE[V|X+V ≥t].WecanbreakthisupasthesumacrossoutcomesZofE[V|Z,X+V ≥t]·Pr(Z|X+V ≥t)for
thethreedisjointoutcomesV <c,c≤V ≤c+1,andV >c+1. Notethatwecanlowerboundtheseexpectationsbyq,c,c+1
respectively.Butthenoncetislargeenoughthat Pr(V<c|X+V≥t) < 1 ,thisweightedsumofconditionalexpectationswilladdto
Pr(V>c+1|X+V≥t) c−q
morethanc.
C.Additionalfigures
Seefigures5,6.
14CatastrophicGoodhart:regularizingRLHFwithKLdivergencedoesnotmitigateheavy-tailedrewardmisspecification
Figure6. Rewardandlog-probabilityforACG-optimizedinputstoStarling7B-alpha.
Table2. HyperparametersforACG
Parameter Value
Contextlength 133
Iterations 1000
Candidatesperseq. position(k) 3
Annealingstartingvalue 9
Annealingendingvalue 2
D.HyperparametersforACG
Seetable2.
E.Assets
Weusethreemodelsforourexperiments:Starling7B-alpha,Llama27B-chat,andPythia-1.4B.StarlingwasdevelopedbyBerkeley,
andPythiabyEleutherAI.StarlingandPythiamodelsarelicensedunderApache-2.0.9 10 Llama2modelsweredevelopedbyMetaand
licensedunderalicensepublishedbyMeta.11
9https://twitter.com/NexusflowX/status/1770532630645420474
10https://huggingface.co/EleutherAI/pythia-1.4b
11https://ai.meta.com/llama/license/
15