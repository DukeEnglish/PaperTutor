JOURNALOFLATEXCLASSFILES 1
MSCT: Addressing Time-Varying Confounding with
Marginal Structural Causal Transformer for
Counterfactual Post-Crash Traffic Prediction
Shuang Li, Ziyuan Pu, Member, IEEE, Nan Zhang, Duxin Chen, Member, IEEE, Lu Dong, Member, IEEE,
Daniel J. Graham, Yinhai Wang, Fellow, IEEE,
Abstract—Traffic crashes profoundly impede traffic efficiency
and pose economic challenges. Accurate prediction of post-
crash traffic status provides essential information for evaluating
traffic perturbations and developing effective solutions. Previous
studies have established a series of deep learning models to
predict post-crash traffic conditions, however, these correlation-
based methods cannot accommodate the biases caused by time-
varying confounders and the heterogeneous effects of crashes.
The post-crash traffic prediction model needs to estimate the
counterfactual traffic speed response to hypothetical crashes
under various conditions, which demonstrates the necessity of
understanding the causal relationship between traffic factors.
Therefore, this paper presents the Marginal Structural Causal
Transformer (MSCT), a novel deep learning model designed
for counterfactual post-crash traffic prediction. To address the
issue of time-varying confounding bias, MSCT incorporates a
structureinspiredbyMarginalStructuralModelsandintroduces
a balanced loss function to facilitate learning of invariant causal
features.Theproposedmodelistreatment-aware,withaspecific
focusoncomprehendingandpredictingtrafficspeedunderhypo-
thetical crash intervention strategies. In the absence of ground-
truth data, a synthetic data generation procedure is proposed
to emulate the causal mechanism between traffic speed, crashes, Fig.1. Illustrationofcounterfactualtrafficspeedundertheimpactofcrashes.
and covariates. The model is validated using both synthetic and
real-worlddata,demonstratingthatMSCToutperformsstate-of-
the-art models in multi-step-ahead prediction performance. This
study also systematically analyzes the impact of time-varying TRAFFIC crashes always cause severe travel delays and
confoundingbiasanddatasetdistributiononmodelperformance, substantial economic losses. According to the World
contributing valuable insights into counterfactual prediction for
Health Organisation (WHO), the cost of road traffic injuries
intelligent transportation systems.
in the global macroeconomy is estimated to be 1.8 trillion
Index Terms—Causal deep learning, counterfactual traffic dollars [1]. In addition, the risk of crashes during post-crash
prediction, traffic crash, time-varying confounding bias.
duration is six times greater than that of primary crashes [2].
The ability to accurately predict post-crash traffic conditions
I. INTRODUCTION
is essential for Intelligent Transportation Systems (ITS), not
This work has been supported by the Postgraduate Research&Practice only to help traffic managers improve traffic management and
Innovation Program of Jiangsu Province (project number: KYCX22 0271) control strategies but also to ensure the safety of road users.
andtheMinistryofTransportofPRCKeyLaboratoryofTransportIndustry
ofComprehensiveTransportationTheory(NanjingModernMultimodalTrans- Post-crash traffic prediction is a counterfactual prediction
portation Laboratory) (project number: MTF2023002)(Crossponding author: taskthatrequiresthedevelopmentofatreatment-awaremodel,
ZiyuanPu)
enabling it to predict the traffic conditions under hypothetical
ShuangLiarewithSchoolofTransportation,SoutheastUniversity,Nanjing,
211189, China, and Department of Civil and Environmental Engineering, intervention(crash)basedonobservationaldata[3].However,
ImperialCollegeLondon,London,SW72AZ,UK(shuangli seu@seu.edu.cn) many existing machine-learning models are data-driven, fun-
ZiyuanPuiswithSchoolofTransportation,SoutheastUniversity,Nanjing,
damentally focusing on learning correlations within the data
211189,China(ziyuanpu@seu.edu.cn)
Daniel J. Graham, and Nan Zhang are with Department of Civil and ratherthanestablishingcausalrelationships.Since”correlation
Environmental Engineering, Imperial College London, London, SW7 2AZ, doesnotimplycausation,”data-drivenmethodsoftenoverlook
UK(d.j.graham@imperial.ac.uk;nan.zhang16@imperial.ac.uk)
selection biases introduced by confounders—variables that
Duxin Chen is with the School of Mathematics, Southeast University,
Nanjing211189,China(chendx@seu.edu.cn) causally influence both the treatment and the outcomes. This
LuDongiswiththeSchoolofCyberScienceandEngineering,Southeast oversightcanleadtounforeseenbiasesandincreasedgeneral-
University,Nanjing211189,China(ldong90@seu.edu.cn)
izationerrorswhenpredictingcounterfactualtrafficconditions
YinhaiWangiswiththeDepartmentofCivilandEnvironmentalEngineer-
ing,UniversityofWashington,Seattle,WA98195USA(yinhai@uw.edu) [4].
4202
luJ
91
]GL.sc[
1v56041.7042:viXraJOURNALOFLATEXCLASSFILES 2
Fig.1illustratestheproblemofpost-crashtrafficprediction. as a domain adaptation problem and proposed a Balancing
AsshowninFig.1(a),attheendofthepeakperiod,arealcrash Neural Network (BNN) for estimating Individual Treatment
occurred at time t. Following the crash, there was a slight in- Effects (ITE). Subsequently, various types of causal DL
creaseinspeed(solidline).Data-drivenmodelstrainedonthis models have been established based on balancing rules and
observed data might mistakenly learn that crashes contribute domain-invariant learning [20]–[23]. To predict the treatment
to a slight speed increase. However, the counterfactual speed response under time-varying confounding situations, several
(dottedline)wouldhavebeenhigherifnocrashhadoccurred. causal DL models have been proposed in the medical field,
Here, the time acts as a time-varying confounder, potentially including recurrent marginal structural networks (RMSNs)
preventingmodelsfromaccuratelycapturingthetruetemporal [24],counterfactualrecurrentnetwork(CRN)[25],G-Net[26],
causal mechanism. Fig. 1(b) shows the heterogeneous effects and causal transformer (CT) [27]. These models utilize the
ofhypotheticalcrashesatdifferenttimepoints.Theprediction DL network to capture long-term temporal dependencies of
model needs to estimate the speed after a crash that did not patients’ states. However, traffic conditions tend to exhibit
actually occur, thereby highlighting the imperative to compre- morecomplexanddynamicallyfluctuatingcharacteristics,and
hendthecausaleffectsofcrashesacrossdiversecircumstances. the occurrence of crashes is more difficult to predict than
Therefore,post-crashtrafficpredictionshouldbeconsideredas therapeutic strategies, making it more difficult to infer the
a counterfactual prediction task, which means estimating the evolution of post-crash conditions.
outcomesresponsetohypotheticalinterventionortreatmentin As it is impossible to simultaneously observe traffic states
various conditions. under both crash and non-crash conditions, a dataset with a
Based on causal theory, the crash indicator and the post- known internal causal mechanism is necessary to evaluate the
crash traffic speed are usually defined as Treatment and performanceofcounterfactualprediction.Despitethecapabil-
Outcome. To address post-crash traffic prediction, three major ity of traffic simulation software to produce data for diverse
challengesremaintobesolved:1)Theissueofselectionbiases traffic scenarios, the intricate nature of scene modeling and
inobservationaldata,particularlythetime-varyingconfound- parametercalibrationposeschallengesincomprehensivelyun-
ing biases resulting from time-varying confounders, needs to derstanding causal mechanisms. Therefore, a Data Generation
beaddressed.[5],[6];2)Counterfactualpredictionshouldbe Process (DGP) that directly reveal the causal relationship is
treatment-aware, namely make a causal estimation given the necessary. However, the challenge of simulating traffic trends,
hypotheticalinterventionunderdifferentcircumstances;and3) such as the temporal dependency, peak and off-peak patterns,
Due to the unobservable nature of counterfactual outcomes, andthedissipationofcongestion,remainsanunresolvedissue.
thereisnogroundtruthtovalidatethepredictionperformance. Thisstudyintroducesanoveldeeplearningmodel,Marginal
In previous research, a multitude of factors have been Structural Causal Transformer (MSCT), that aims to address
demonstrated to impact traffic crashes and post-crash condi- the problem of counterfactual post-crash traffic prediction.
tions significantly, and numerous Deep Learning (DL) models Different from traditional DL prediction models that are
havebeenproposedforpredictingpost-crashtrafficconditions mainly dedicated to fusing multi-source data and capturing
[7]–[10]. These studies adopt data-driven approaches to in- spatial-temporal dependencies, our model additionally pays
corporate a comprehensive set of relevant variables into the more focus on the causal relationship and is specifically
model, leveraging the learning capabilities of DL for accu- tailored to address interventions related to traffic crashes. Our
rate predictions. However, these data-driven models rely on contributions are summarized as follows:
correlations rather than causation [11], which may introduce • Inspired by causal theory, we develop a sequence-to-
unexpectederrorswhenattemptingtomakecausalestimations sequence causal deep learning model that applies the
[12]. Transformer for counterfactual prediction. To deal with
Since randomized controlled trials (the best method for the time-varying confounding bias, an MSMs-inspired
causal estimations) for traffic crashes are unfeasible and modelstructureisbuilt,andthebalancedlossisproposed
ethically untenable, the estimation from observational data tolearninvariantfeaturesbydomaingeneralizedtraining
has garnered considerable attention among scholars [13]– procedure.
[15]. Traditional methods, such as propensity score matching • The proposed DL model is treatment-aware, meaning
and doubly robust, are mainly designed for single treatment it can predict heterogeneous traffic speed response to
rule and cross-section data. Considering traffic conditions are hypothetical crash intervention strategies. To the best of
significantly affected by time-varying covariates, the methods our knowledge, this is the first attempt to introduce the
designed for time-varying treatments, like Marginal Structural conceptoftreatmentresponsepredictionintotrafficcrash
Models (MSMs) [16] and G-estimation [17], are thus more scenarios by deep learning techniques.
suitable post-crash traffic speed prediction. However, these • Due to the absence of ground-truth data, we propose
linear regression-based models are limited in fitting extensive a synthetic data generation procedure that can emulate
traffic data and capturing long-term temporal dependencies trafficspeed,consideringtheoccurrenceoftrafficcrashes.
[11]. Ourmodeliscomparedwithstate-of-the-artmodelsusing
GiventhestrongcapabilitiesofDLtechnologiesinhandling synthetic and real-world data, and it achieves superior
large and complex data, the integration of DL networks performance. Systematic and empirical analyses have
with causal theory has emerged as a prevailing trend [18]. been conducted to investigate the impact of the degree
Johansson et al. [19] initially framed counterfactual inference of time-varying confounding bias and the imbalancedJOURNALOFLATEXCLASSFILES 3
distribution of the dataset on model performance. lack the capability to predict accurately. On the other hand,
DL has contributed largely to prediction when applied to
II. LITERATUREREVIEW big data, but these models are black boxes and are more
interested in correlations than causality between inputs and
A. Traffic Speed Prediction under Incident Conditions
outputs, resulting in pool reliability for decision-making [34].
Extensivemethodshavebeenproposedforestimatingtraffic
Toimprove theability ofDL modelsto answercounterfactual
speed. Early prediction models, such as ARIMA and VAR,
questions, causal models have been widely combined for
are mainly parametric models. With the development of ML
counterfactual outcome prediction.
technologies,variousmodelshavebeenappliedforspeedpre-
ThemethodsintroducingcausalmodelsintoDLarediverse.
diction [28]. Recently, deep learning models have performed
Johansson et al. combined domain adaptation and represen-
superiorly in capturing complicated and dynamic features of
tation learning for causal effects inference [19]. Alaa et al.
trafficconditions.Alargeamountofdeeplearningmodelsfor
built a DCN-PD model that utilizes a propensity-dropout
traffic speed prediction have emerged. These new models are
regularization scheme to alleviate selection bias in the obser-
mainly based on one or several combinations of state-of-the-
vational data [20]. Louizos et al proposed CEVAE based on
art neural networks, such as RNN, LSTM, CNN, GCN, and
VariationalAutoencoderswhichfollowsthecausalstructureof
Attention.
inference [21]. Yoon et al. incorporated generative adversarial
There are also several studies exploring the impact of
model architecture into individual treatment effect inference,
incidentsontrafficconditions.Milleretal.suggestedasystem
which makes GAN popular in the field of causal estimation
to forecast the cost and effect of highway incidents using
[22]. Li et al. proposed a conditional invariant deep domain
machine models [7]. Their system allows for predicting the
generalization approach for domain-invariant representation
duration of incidents and the cost of delay. Li and Chen
learning. The gradient reversal is adopted to eliminate spu-
collected the features of highway traffic conditions with non-
rious correlations [35]. Shi et al. developed a Dragonnet that
recurrent congestion and developed a travel time prediction
applies propensity score to adjust the confounding bias [23].
model by using MLP [29]. Pan et al. proposed a model to
However, these models are designed for cross-sectional data,
predict when and how travel time delays will occur in road
which cannot be applied for time-varying treatment response
networksduetotrafficincidents[8].Theirmodelisbuiltbased
prediction.
ontheattributesofhistoricalcases,andaclusteringalgorithm
Due to the greater importance of longitudinal data in the
is applied for new scenarios to determine initial propagation
medicalfield,severalscholarsexploredtheadjustmentoftime-
behavior.Javidetal.developedaframeworktoestimatetravel
varying confounding bias by DL. Lim et al. first applied a
time variability caused by traffic incidents by using a series
recurrent neural network to predict the inverse probability
of robust regression methods [30]. Xie et al. propose a deep
of treatment weights (IPTWs), and built recurrent marginal
learningmodeltoquantifytheimpactofurbantrafficincidents
structural networks (RMSN) [24]. Subsequently, Bica et al.
ontrafficflows[10].Thetrafficincidentdataareprocessedby
proposed counterfactual recurrent networks (CRN) which ap-
a classifier to extract latent features which then be introduced
ply domain adversarial training to balancing representations
into the prediction model. Shafiei et al. built a simulation-
[25]. Li et al. designed a sequential DL framework for G-
basedframeworkforincidentimpactanalysis[31].MLmodels
computation (G-Net) that enables counterfactual predictions
are applied in their framework for incident classification and
underdynamictreatmentstrategies[26].Melnychuketal.also
demand prediction, which enhance traffic micro-simulation.
developed a domain adversarial representations-based model
Accordingtotheexistingliterature,clusteringorclassifying
withmulti-inputtransformerarchitecture(CT)[27].According
thetrafficincidentsfirstisacommonprocedureforprediction,
to these state-of-art examples, two main ideas are applied for
which helps recognize the impact level of a new incident.
counterfactualoutcomespredictionovertime:oneisdesigning
However, these methods are based on correlation of variables.
a network to implement causal models, such as MSMs, and
Whenitcomestounderstandingtheimpactmechanismoftraf-
G-computation; the other is applying adversarial training to
fic crashes, a causal relationship is necessary to avoid biased
generate representations. To capture the temporal dependency,
estimation. Therefore, different from the previous studies, this
RNN, transformer, and sequence-to-sequence structure are
studyaimstoaddressthecausalproblemforpost-crashtraffic
the main methods. However, these models are designed for
speed prediction and provide a more effective solution for
medicaldata,whichpossessdistinctcharacteristicsfromtrans-
multi-step-ahead prediction given the intervention strategies.
portationdata.Therefore,toeffectivelyadaptthesemodelsfor
transportation-related applications, we need to conduct more
B. Counterfactual Outcomes Prediction with Deep Learning
specific and tailored research to account for the time-varying
In recent years, the DL community’s interest in causality treatmenteffectsandconfoundingbiasinherentintrafficdata.
has greatly increased. Traditionally, causal models have been
primarily used for estimating average treatment effects in the
C. Causal Traffic Prediction
policy research field, with the potential outcome framework
(also known as Neyman-Rubin Causal Model) being a widely Causal effects estimating models have been widely applied
employed approach [32], [33]. However, there is a growing in the traffic safety field to explore the causal effects of
interestinindividualtreatmenteffects(i.e.,counterfactualout- behavioralelements[36],trafficpolicy[13],roadconstruction
comes), for which traditional linear regression-based models [37], etc. These studies mainly focus on estimating averageJOURNALOFLATEXCLASSFILES 4
treatment effects which are useful to support traffic decision-
making. Recently, causal machine learning methods have also
applied to traffic fields, such as generalized random forest
[38],doublemachinelearning[39],anddoublyrobustlearning
[40], because of their outstanding performance in estimating
heterogeneous treatment effects. These studies contributed
greatlytosolvingtheconfoundingbiasoftrafficdatabycausal
models; however, they are mainly based on cross-sectional
data.
Additionally,someresearchershaveexploredtheintegration
of causal discovery models into prediction models for traffic
time-series data. For instance, Li et al. introduced a Lasso
method that applies Granger causality to uncover potential
Fig.2. Causalgraphforfactorsinteractingmechanismovertime.
dependencies among big data, leading to more robust traffic
flow predictions [41]. Similarly, Gao et al. integrated an itera-
tivecausaldiscoveryalgorithmintothetemporalconvolutional
each vector depends on the given time-sequence length of
network,enhancingpredictivemodelperformancethroughthe
traffic states. Let τ ≥ 1 denote the prediction horizon for
graph adjacency matrix of variables [42]. Meanwhile, He et
a τ-step-ahead prediction. Then, we can artificially define the
al.proposedaGNN-basedframework,usingaspatial-temporal
hypothetic intervention vector of treatments as T˜ =
Granger causality graph (STGC-GNNs) for traffic prediction (t,t+τ−1)
[T˜,...,T˜ ]duringτ timestepsandthepotentialoutcomes
[43].Whilethesestudiesapplycausalmodelstoextractcausal t t (cid:16)+τ−1 (cid:17)
graphs between variables, their primary focus remains on are Y t+τ T˜ (t,t+τ−1) . Therefore, for a given traffic history
traditionalpredictiontasksratherthancounterfactualoutcomes and crash intervention, we aim to learn a function f(•) for
prediction. As a result, these models may not be suitable the expected traffic speed over τ prediction horizons. The
for estimating traffic conditions under specific interventions, estimated formula can be represented as follows:
such as crashes, due to the potential selection bias present E(Y (T˜ )|H¯ )=f(τ,T˜ ,H¯ ) (1)
t+τ (t,t+τ−1) t (t,t+τ−1) t
in observational data, which can lead to errors. Therefore,
The learning of function f(•) is a counterfactual task,
embedding confounding bias adjustment into deep learning
because we can only observe one type of outcome for each
models for time-series traffic data prediction becomes crucial
unitateachtimestep,andthentheotherunobservedoutcomes
for more accurate and reliable results.
arereferredtoascounterfactualoutcomes.Forexample,ifwe
observe a crash at time t for unit i, then the traffic speed
III. PROBLEMSTATEMENT at next time step is Y (T =1), and it is impossible
i,t+1 i,t
to have knowledge about Y (T =0). Therefore, when
To estimate the causal effect of traffic crashes on speed i,t+1 i,t
estimatingthecounterfactualoutcomesbasedonobservational
over time, we describe the problem built upon the time-
data, the confounding bias should be adjusted. Specifically,
varying extension of the potential outcome framework[26].
confounding bias may be raised because the covariates af-
Given that traffic data consists of long-term continuous time
fect outcomes and treatments simultaneously (red arrows in
series, considering each road segment as an individual would
Fig.2(b)), which cannot be tackled by traditional machine
result in excessively long observation periods. Therefore, to
learning. Additionally, to ensure the potential outcome is
address this issue, we employ a specific length of time
identifiable under time-varying treatment settings, we assume
sequencetoextractsubsetsoftimeseriesdatabasedontraffic
three standard assumptions as follows:
location information, creating multiple time blocks that serve
as the unit. For each unit i at time step t, we have the Assumption 1. Sequential consistency: Y(T¯) = Y(T¯∗) if
following: d x-dimension time-varying covariates X
i,t
∈ Rdx T¯ = T¯∗, where T¯∗denotes the measured treatment history.
potential outcome (traffic speed) Y , and treatment (crash) This means that the traffic speed under the given sequence of
i,t
T ∈ [0,1]. If a crash occurs at time t for unit i, then crash treatments corresponds to the observed speed of units
i,t
T = 1, otherwise, T = 0. Further, each unit i has d - treated by the same crash treatments.
i,t i,t s
dimensionstaticfeaturesS i ∈Rds,suchasmilepost,weather, Assumption 2. Sequential positivity: If P(H¯ t =h¯ t) ̸= 0,
and day of the week. Because we study the traffic states in P(T t|H¯ t =h¯ t) > 0, where h¯ t denotes the factual history
5-minintervals,theweatheranddayoftheweekarerelatively of traffic conditions. This suggests that there is a non-zero
static.Fornotationalsimplicity,weomitthesubscriptiunless probability of a crash occurring or not occurring for all
needed. instances in the historical data over time.
Fig.2 illustrates the time-varying causal mechanism of Assumption 3. Sequential exchangeability:
traffic speed under crash treatment. The traffic speed and (Y (1),Y (0))⊥T |H¯ . This indicates that the likelihood
t+1 t+1 t t
crashes are effected by historical covariates. Here, we denote of a crash happening at the current time is independent of
H¯ = X¯ ,T¯ ,Y¯ ,S as the history of traffic conditions, potential outcomes, conditioned on the observed history. In
t t t−1 t
where X¯ = [X ,...,X ], T¯ = [T ,...,T ], and other words, there are no hidden confounding factors between
t 1 t t−1 1 t−1
Y¯ = [Y ,...,Y ]. For each unit, the maximum length of treatments and outcomes.
t 1 tJOURNALOFLATEXCLASSFILES 5
Fig.3. ThearchitectureofMSCT.
IV. METHODS problems. Robins [45] has proved that when using SW, the
A. Marginal Structural Models bias from time-varying confounders can be removed when
P(T |H¯ ) = P(T |T¯ ) . Therefore, let Tk denote the k-
Before predicting the counterfactual traffic speed, we need t t t t−1 t
th possible treatment (i.e., domain) at time t, a representation
to briefly introduce the basic methods for time-varying
Φ of history that could remove the time-varying bias should
treatment response estimation: Marginal Structural Models
have the same distribution across the different possible treat-
(MSMs), which are widely applied for estimating causal
ments: P(Φ(H¯ )|T =T1)=···=P(Φ(H¯ )|T =Tk) [25],
effects with IPTW from observational data [16], [44], and the t t t t t t
[35]. Because this study only considers crash and non-crash
main form can be expressed as follows:
scenarios, there are two domains in this problem.
E(Y |T¯ ,S)=β +β T¯ +β S⊗T¯ +β S (2)
t+τ t−1 0 1 t 2 t 3
C. Marginal Structural Causal Transformer
where β is the vector of parameters to be estimated, in which
β are the causal effects of treatments, and β are the causal Inspired by the above methods, we propose a novel trans-
1 2
effect of treatments conditioned on S. former model called Marginal Structural Causal Transformer
Underthethreeassumptionsmentionedabove,theunbiased (MSCT)toestimatethecounterfactualspeedundercrashtreat-
estimates of causal parameters can be obtained by fitting the ment. To predict the traffic speed at multiple time horizons,
model with the stabilized weights (SW): themodelappliessequence-to-sequencearchitecture,whichis
depicted in Fig.3. The encoders receive all historical features
t+τ
(cid:81) pr(T |T¯ ) and conduct one-step-ahead prediction. As for decoders, each
t t−1
SW(t,τ)= t=1 (3) ofthemwilloutputthepredictionsequentially.Forsimplicity,
t (cid:81)+τ pr(T |H¯ ) this figure only shows the last encoder and the first decoder.
t t The inputs of MSCT include historical traffic statements H¯
t=1
and the intervention strategies of treatment T˜ (i.e., as-
where pr(·) is the probabilities of treatments conditioned on t,t+τ−1
sumedcrasheswithinaspecifictimeperiod).Whenpredicting
the covariates.
Commonly, the numerator pr(T |T¯ ) is referred to as thefuturehorizon,thepredictionsofoutcomesYˆ (t+1:t+τ) are
propensity score and the denominatt or pt− r(1 T |H¯ ) is historical fed to the model autoregressively starting from the predicted
t t time step.
propensity score. This method creates a pseudo population
Each MSCT block contains two parallel neural network
by SW to balance the covariates, thus, we could build two
pathways, one consists of LSTMs and the other consists
networks to fit the propensity score and outcome respectively
of Transformers. LSTMs pathway is constructed to predict
as in [23].
propensity score Tˆ = pr(T|T¯). The Transformers pathway
ps
is constructed to predict outcome Yˆ and historical propensity
B. Domain Adaptation for Counterfactual Prediction score Tˆ =pr(T|H¯). To enhance generalization and reduce
hps
Although adjusting by SW can remove bias, the predic- computational complexity, we adopt Variational RNN [46],
tion bias increases with the increase in the dimension of a regularization technique that applies variational dropout to
covariates. Scholars have found that counterfactual prediction stochastically set neuron outputs to zero during training. At
problems can also be transformed into a domain adaptation the last block of the encoder, the outputs of LSTMs willJOURNALOFLATEXCLASSFILES 6
be processed by linear networks and then be input into the
decoder as the memory states.
Encoder: The encoder aims to learn a good representation
of current traffic states that captures the key information of
historical statements. This module will predict one-step-ahead
traffic speed Yˆ given the observational previous traffic
t+1
conditions and factual crashes. For time step t, the inputs of
theencoderareH¯ whicharefirstlyconcatenatedandmapped
t
to a d -dimension hidden state space h by a fully-connected
h t
linearlayer.Then,thehiddenstatesforalltimestepsareinput
to the first transformer encoder blocks, and the subsequent
transformer blocks receive the outputs of the previous blocks
as the inputs. Finally, the outputs from the last block are
further processed by a fully connected layer followed by an
exponentiallinearunit(ELU)activationfunctionbeforebeing
taken as a representation. The formalization is as follows:
h =Linear(Concat(X¯ ,T¯ ,Y¯ ,S )) (4)
t t t−1 t t Fig.4. Thearchitectureoftransformerblocks.
h0 =[h ,...,h ]T (5)
t 1 t
hl =TransEncoderBlock (hl−1) (6)
t l t can be described as mapping queries Q, keys K, and values V
Φ t =ELU(Linear(hl t)) (7) toanoutput.Thehiddenstateshl areprocessedbythreelinear
where hl ∈Rt×dh denotes the hidden states processed by the layerstod a-dimensionQ,K,andV,respectively.Asdesigned
t in [47] (called scaled dot-product attention), we compute i-th
l-layer block, l ∈ {1,2,...,L}, L denotes the total number
of blocks, and Φ
t
∈ Rt×dh is the representation of historical head of output as follows:
conditions.
QiKi⊤
Attentioni(Qi,Ki,Vi)=softmax( √ )V (12)
Decoder: The following future horizons Yˆ t+τ (τ ≥ 2) are d a
predictedbythedecodergiventherepresentationlearnedfrom Then,themulti-headattentionprocessesconcatenatedheads
the encoder. The inputs of the decoder contain the historical by linearly projection:
treatment T¯ , historical outcomes Y¯ , static features S, and
t−1 t
the predict outcomes Yˆ that are autoregressively fed to the MultiHead(Q,K,V)=Concat(head ,...,head )WO
t+1 1 i
model. Different from the encoder, the representation from (13)
the encoder is input into the decoder blocks together with the head =Attentioni(QiWQ,KiWK,ViWV) (14)
i i i i
transformedhiddenstateh .Then,topredictthecounterfactual
t
outcomes, the intervention strategies T˜ (t,t+τ−1) and outputs where W iQ ∈ Rdh×da, W iK ∈ Rdh×da,W iV ∈ Rdh×da, and
of the last blocks are input into a linear layer to predict the WO ∈Ridh×da are parameters for training.
outcomes: In addition, multi-head attention is used in the encoder and
h =Linear(Concat(T ,Y ,S,Yˆ )) (8) decoder by different ways. As shown in Fig. 4, the encoder
t+τ t+τ−2 t+τ−2 t+τ−1
h0 =[h ,...,h ]T (9) onlycontainsself-attentionwhichonlytakesthehiddenstates
t+τ t+1 t+τ
from the same subnetwork as inputs. The decoder contains an
hl =TransDecoderBlock (hl−1,Φ ) (10)
t+τ l t+τ t extra cross-attention layer that receives the queries from the
Φ t+τ =ELU(Linear(hl t+τ)) (11) previous decoder block and keys and values from the output
of the encoder as inputs. Therefore, self-attention and cross-
Output Heads: There are three heads for outputs, i.e.,
attention can be formalized as follows:
Outcome Head D (Φ(H¯ ),T˜ ), Propensity Score Head
y t (t+τ−1)
(PS)D (T¯ ),andHistoricalPropensityScoreHead(HPS)
ps t−1 Self(Q,K,V)=[Q,K,V]
D (Φ(H¯ )). Each head consists of two fully connected
hps t =[Linear(hl−1),Linear(hl−1),Linear(hl−1)]
linear layers with an ELU activation in between.
(15)
Transformer Block: Transformers are deep neural net-
works used for sequential data that commonly employ a Cross(Q,K,V)=[Q,K,V]
specialized attention mechanism [47]. The Transformer archi- =[Linear(hl−1),Linear(Φ),Linear(ϕ)]
tecture is based on the mechanism of self-attention, which (16)
allows the model to weigh the importance of different values
in a sequence when processing it. The key components of a To prevent future information flows in the current state,
Transformermodelincludemulti-headattention,position-wise mask matrixes are used in scaled dot-product attention to set
feed-forward layer, and Add&Norm layer (Shown as Fig. 4). the future input values of softmax to −∞.
(1) Multi-head attention is a module that runs through an at- (2)APosition-wisefeed-forwardlayerisappliedaftereach
tentionheadseveraltimesinparallel.Anattentionmechanism attention and takes the outputs of multi-head attention asJOURNALOFLATEXCLASSFILES 7
inputs.Thislayerconsistsoftwofullyconnectedlinearlayers where λ is the hyperparameter controlling the trade-off be-
with a ReLU activation in between: tween domain confusion and outcome prediction.
We realized MSCT by PyTorch Lightning and used Adam
FFL(α)=Linear(ReLU(Linear(α))) (17)
to optimize (∆ ) the loss with learning rate η [50]. The
where α denotes the outputs of the sub-layer in the blocks.
overall training procedure consists of three steps, and we
(3) Add&Norm layer functions as the connection between
describe the training process by pseudocode in Algorithm 1.
eachlayer(attention,feed-forward)inallencoderanddecoder
blocks.Ithasaresidualconnectionandisfollowedbyalayer- Algorithm 1 Training Procedure
normalization step [48]. The computation can be formalized Input: Initialize dataset H¯ ={X¯ ,T¯ ,Y¯ ,S}, intervention
t t t−1 t
as follows: T˜ , and hyperparameters a.
(t,t+τ−1)
α=LayerNorm(α+Sublayer(α)) (18) Output: Optimized parameters: Θ ,Θ
E D
Positional Encoding: To provide the sequence information Step 1: Training Encoder: Optimize parameters
ofhiddenstatesforthetransformer,absolutepositionalencod- Θ =(θ ,θ ,θ ,θ ,θ )
E R Y T PS HPS E
ing is utilized to the linear-transformed inputs right before the for each epoch do
first transformer block [47]. The fixed weights are applied to for batch B in dataset do
encode each time step t by sine and cosine function: (θ ,θ ,θ ) ← (θ ,θ ,θ ) −
R T PS E R T PS E
PE
(t,2d)
=sin(t/10002d/dh) (19) η∆( B1 (cid:80) B(cid:80) t(L t,y+λL t,ps))
(θ ,θ ,θ ) ← (θ ,θ ,θ ) −
PE
(t,2d+1)
=cos(t/10002d/dh) (20) η∆R (1Y (cid:80)H (cid:80)PS (E
L +λL ))
R Y HPS E
B B t t,y t,hps
where d is the dimension. Thus, each dimension of the end for
positional encoding corresponds to a sinusoid, enabling linear end for
transformations to be performed between neighboring time return Encoder network: Encoder(H¯ ;Θ )
i,t E
steps and time-delta shifts. Step2:Computetheencoderrepresentationsandmemorystates
Φ,M
D. Training Procedure
for unit i=1 to N do
Thekeyprocedureforcounterfactualpredictionisremoving for t=1 to T do
the time-varying confounding bias by training. The aim of (Φ ,M ,Yˆ )=Encoder(H¯ ;Θ )
i,t i,t i,t+1 i,t E
our models is: (a) predict the outcome by representations, (b) end for
balance the historical treatments by fitting propensity score, end for
and (c) remove the correlation between representations and
Step 3: Training Decoder: Optimize parameters Θ =
D
treatments. For objective (a), the parameters of networks for
(θ ,θ ,θ ,θ ,θ )
outcomes and representations can be fitted by the true next R Y T PS HPS D
for epochs do
outcome via minimizing the mean squared loss (MSE):
for batch B in dataset {T ,Y ,S}∪
L t,y(θ Y,θ R)=∥Y t+1−D y(Φ t(H¯ t;θ R);θ Y)∥2 (21) {Φ t,M t,Yˆ t+1} do
t:t+τmax−1 t+2:t+τmax
(θ ,θ ,θ ) ← (θ ,θ ,θ ) −
For objective (b), we need to fit the propensity score R T PS D R T PS D
η∆(1 (cid:80) (cid:80)τmax(L +λL ))
network according to the next true treatment by binary cross B B t=2 t,y t,ps
(θ ,θ ,θ ) ← (θ ,θ ,θ ) −
entropy (BCE) loss: R Y HPS D R Y HPS D
η∆(1 (cid:80) (cid:80)τmax(L +λL ))
K B B t=2 t,y t,hps
L (θ ,θ )=−(cid:88) I logD (L(T¯ ;θ );θ ) end for
t,ps T PS {Tt=T tk} ps t−1 T PS end for
k=1 (22) return Encoder network: Decoder(H¯ i,t;Θ D)
where I is the indicator function, and L(•) represents the
LSTMs pathway in MSCT blocks. aHyperparametersinthismodelincludenumberofepochs,batchsize,drop
rate,sizeofhiddenstates,numberofheads,andnumberoflayers.
Objective (c) requires learning a representation that cannot
beusedtopredicttreatment,thus,adomaingeneralizationloss
function is applied [49]:
V. EXPERIMENTS
K
(cid:88) 1
L (θ ,θ )=− logD (Φ (H¯ ;θ );θ )
t,hps R HPS K hps t t R HPS Counterfactual outcome prediction is different from tradi-
k=1
(23) tional prediction tasks because of the unobserved potential
Bycombiningalltheabovelossfunctions,thetotallosscan outcomes,whichmeansthereisnogroundtruthforreal-world
be obtained as follows: data. Therefore, we propose a way to generate a synthetic
dataset based on common sense and the causal mechanism of
trafficfeatures.Wealsouseareal-worlddatasetabouthighway
R (θ ,θ ,θ ,θ ,θ )
t R Y T PS HPS traffic crashes. Due to no access to the true counterfactuals,
=L (θ ,θ )+λ(L (θ ,θ )+L (θ ,θ ))
t,Y R Y t,ps T PS t,hps R HPS we only present the performance of factual outcomes which
(24)
can also prove the efficiency of our model to some degree.JOURNALOFLATEXCLASSFILES 8
A. Synthetic Data Generation 2) The second part is Base(t), which simulates the basic
trends of daily changes in traffic speed based on the normal
In this section, A DGP is proposed for traffic speed consid-
distribution. We assume the formalization of basic trends is:
eringtheimpactofcrashes.Here,weholdthebeliefthattraffic
speedY isinfluencedbycrashesT andsomeotherfactors(rep- exp(−(ϕ(t)−µ)2/2σ2
Base(t)=Ψ−λ× √ (28)
resented by X), and the occurrence of crashes is also affected
σ 2π
by these factors. Therefore, the procedure of synthetic data
generation considers the basic traffic temporal tendency (i.e., where Ψ is the assumed maximum basic speed, λ is the am-
the peak and non-peak hours), the temporal dependency, the plitude,µisthemean,andσ isthevariance.Theseparameters
dissipation of congestion, and the time-varying confounding can control the change trends. Here, we set Ψ = 80, µ = 6,
effects. For each unit, we generate data iteratively as follows: and σ = 1 .Thus, ϕ(t) ∈ {1,2,...,12}. Because normal
distribution is an unimodal curve that can simulate one peak
Y(t)=(β 1X t−β 2T t+ε)×Y(t−1) for traffic conditions, we apply this function to simulate the
+Base(t)×(Y(t−1)/Base(t−1))g(d) (25) traffic speed trends for half a day. To generate the data by 5-
min interval for a whole day (i.e., 720 values of time for one
This formula can be divided into three parts: day),wecansetϕ(t)=t%360/30,wheret∈{1,2,...,720}.
1) The first part (β 1X t−β 2T t+ε)×Y(t−1) simulates 3) The third part (Y(t−1)/Base(t−1))g(d) reflects the
the time-varying causal effects of treatments and covariates. dissipation of crash influence. When the speed at time t-1 is
X t ∼ N(0,1) is the confounder for treatment and outcome. affected by crashes, then the speed at time step t will also be
Here, we use a one-dimension covariate for confounding and influenced, but to a lesser extent. Therefore, the function g(d)
assume its coefficient β 1 = 0.1. T t is the treatment indicator represents the impact duration of crashes. This study assumes
fortimestept (T t =1indicatesacrashoccurs,whileT t =0 that the crash will affect the next 5 time steps, then:
represents no crash), and it is affected by X . Therefore, we
t (cid:40)
generate the treatment by the following formalization: 1.25−0.25d if 1≤d=(t−t )≤5
g(d)= c (29)
(cid:40) ←− ←− 0 others
1 if X ≥ x
T
t
= ←−t ←−10th (26)
0 if X < x where t denotes the time when the crash occurs. To illustrate
t 10th c
thesynthetictrafficspeeddataaffectedbycrashes,wegenerate
t
←− 1 (cid:88)
X = X (27) threesampleswith30-time-stepssequentialdatashowninFig.
t,ω min(t,ω) t
5.
max(0,t−ω)
To test the performance of counterfactual prediction, for
←−
whereX t,ω denotestheaverageofhistorical-time-stepcovari- each unit in the test set and each time step, we simulate
ates, which reflects the degree of time-varying confounding several counterfactual speeds under the assumed intervention
bias. ← x− denotes the 10-th percentile value of ← X− for all strategies T˜ . For one-step-ahead prediction, we sim-
10th t (t,t+τ−1)
sample points. ulate both crash and non-crash speed Y . For multi-step-
t+1
Considering the varying levels of crash impact, the coeffi- aheadprediction,a“singleslidingtreatment”settingisapplied
cients β are determined based on the following assumptions: wherestrategiesinvolveonly asingletreatmentbutiteratively
2
there are three types of crashes and their respective causal move it over a window ranging from t to t+τ −1 [25].
max
effects are β = [0.2,0.4,0.8]. For each type of crash, the In addition, we also consider non-crash strategies (i.e., full
2
probabilityofoccurrenceisp =[0.6,0.3,0.1].Bythissetting, zero vector). Therefore, the number of all potential outcomes
c
the probability of the most serious crash occurring is 10%. grows exponentially. For example, when τ = 2, for each
max
Fig.5. Synthetictrafficspeeddatasamples.Reddashedlinedenotesthetimeofcrashoccurrence.JOURNALOFLATEXCLASSFILES 9
time step in each unit, we will simulate three counterfactual performance. For the time step after crash, the calculation of
outcomes(i.e., T˜ ={[1,0],[0,1],[0,0]}). CRMSE can be formulated as follows:
(t,t+1)
(cid:118)
It should be noted that, during the training procedure, only (cid:117) N
(cid:117) 1 (cid:88)
onetypeofpotentialoutcome(i.e.observedoutcomes)canbe CRMSE t =(cid:116) N (c τ−cˆ τ)2
input into the model. During the test procedure, all generated i=1
(cid:118)
counterfactual outcomes for each unit should be input to test (cid:117) N
(cid:117) 1 (cid:88)
the performance. This ensures that the model has learned the =(cid:116) ((Y1 −Y0 ))−(Y1 −Y0 ))2
N true,τ true,τ pred,τ pred,τ
ability to predict counterfactual outcomes from observational i=1
(30)
data.
Wherec andcˆ denotethetrueandpredictedcrashcausal
τ τ
effects,respectively.Y1 andY0 denotethetrafficspeedunder
conditions of a crash occurring and not, respectively.
B. Real-world Data
The real-world data used in this study was collected from VI. RESULTS
the main road of Interstate 5 (I5) in Washington from Nov. A. Results of synthetic data
2019 to May. 2021, and the milepost range from 139 to Wegenerated1000samplestotrainourmodel,100samples
178. The traffic data consists of traffic speed, occupancy, and forvalidation,and100fortesting.Foreachsample,thelength
volume, and the incident data set collected by the Highway of the sequence is 60 time steps, and τ for the single
max
Safety Information System provides information on crashes sliding treatment setting is five. Therefore, we conducted a
including time, location, crash type, etc. We matched all data 6-step-ahead prediction of counterfactual outcomes (the first
according to the time, location, and direction and aggregated step is predicted by the encoder, and the others are predicted
them in one-mile and 5-minute intervals. by the decoder). It should be noted that each sample in the
We use average speed as the outcome and the indicator validation and testing sets was extended to include its coun-
of crash occurrence as the treatment for each time step. Due terfactual outcomes for evaluating the model’s performance.
to the varying effects of different types of crashes, the value Specifically, during the one-step-ahead prediction conducted
of treatment is 0 to 3, corresponding to four scenarios: no by the encoder, we generated non-crash data for each crashed
crash, crash to objects (OBJ), sideswipe (WIPE), and rear- sample. For the multi-step-ahead prediction conducted by the
end (REAR). This treatment variable is then encoded into a decoder, when predicting the target speed from time t+1 to
one-hot format when fed into the model. The time-varying timet+5,thetreatmentindicatorsfromtimettotimet+4were
confounders including time, the max speed difference between manipulated according to the single sliding treatment settings.
lanes, occupancy, volume, and congestion index (real speed Thus, for each current time step, we generated six different
divided by speed limit). Moreover, considering the impacts futureoutcomesforeverysample.Theresultsarepresentedin
of other locations, we also incorporate the average speed TABLE I. According to the findings, our model outperformed
and congestion index at one and two miles upstream and other models, especially as the prediction horizon increased,
downstream of the crash location. The static features are suggesting that our model demonstrates superior performance
milepost, direction, day of week, and weather. Because the in longer forecasting horizons.
trafficdataarecontinuouslongtimeseries,weutilizeasliding
window to generate the data sequence from the traffic flow TABLEI
data. RMSERESULTSFORSYNTHETICTRAFFICDATA(ω=5)
τ =1 τ =2 τ =3 τ =4 τ =5 τ =6
LSTM 8.92 12.61 16.88 20.45 24.59 28.07
C. Baseline Models and Evaluation Metrics BiLSTM 8.66 10.88 12.92 14.15 15.26 16.38
GRU 9.38 11.43 13.49 14.77 15.72 22.14
To compare the performance of counterfactual predic- RNN 8.73 10.57 12.61 13.87 14.55 14.75
tion between causal-based and non-causal-based models, we MSMs 10.14 12.47 14.86 16.81 18.12 18.92
RMSN 8.27 11.58 12.83 13.10 12.85 12.78
first selected four traditional temporal deep learning models,
CRN 8.28 11.63 12.5 12.93 13.29 13.60
namely RNN, LSTM, Bi-LSTM, and GRU, to serve as base-
G-Net 8.31 11.08 13.2 14.07 14.54 14.68
line models. In addition, we also chose several causal models
CT 8.09 10.98 12.84 12.65 12.78 13.02
in the state-of-the-art literature.These are: RMSNs [24], CRN
MSCT 7.96 10.47 11.77 11.99 11.86 11.86
[25], G-Net [26], and CT [27]. All these models are realized
by Pytorch Lightning (version 1.4.5) on a GPU-equipped Fig. 6. illustrates the variation in prediction results for
workstation,andtheirhyper-parametersaretunedbyRaytune. differenttime-varyingconfoundingcoefficientsω.Theoverall
To evaluate the performance of the trained model, the trend of the prediction error is decreasing as ω increases.
Root Mean Square Error (RMSE) is calculated for each test
This phenomenon can be attributed to the fact that when
dataset. In addition, because we know the true factual and
the occurrence of a crash is influenced by a longer historical
counterfactual speed for synthetic data, we can calculate the
trueandpredictedcausaleffectsforeachcrash,andtheRMSE period, its outcome becomes more predictable, allowing for a
for causal effects (CRMSE) can be calculated to evaluate the betterestimationofcounterfactualoutcomes.Forinstance,ifaJOURNALOFLATEXCLASSFILES 10
Fig.6. Theimpactofdifferentdegreesoftime-varyingconfoundingonRMSE
crash is caused by prolonged congestion, both the occurrence B. Results of real-world data
of the crash and the post-crash traffic conditions are more
TABLE III demonstrates the prediction performance based
likely to be anticipated. In contrast, unexpected crashes may
on real-world data. We randomly selected 3000 samples from
be influenced by short-term factors, making their outcomes
real-world datasets, with 30% of the samples containing
more challenging to predict.
crashes. Each sample is a time series with 60 lengths, rep-
The above errors are calculated by all time-step speeds
resenting 300 minutes. As we do not have access to actual
includingcrashandnon-crashconditions.Totesttheabilityof
counterfactual data, all the results are validated based on
models for causal effects estimation, we further focus on the
real-world data only. The results consistently show that our
time steps after the crash occurred. The CRMSE for causal
model exhibits better performance compared to other models
effects prediction is outlined in TABLE II. Because we set
in overall predictions.
τ =5inthisstudy,wecanonlycalculatethecausaleffects
max Considering the influence of the ratio of crash samples in
for up to five time steps after the crash. According to the
the dataset, we further explore the prediction performance
results, causal-based deep learning methods exhibit superior
changing with the crash ratio. Fig.8 shows that for 1-step-
performance compared to traditional temporal deep learning
ahead prediction, there is no consistent relationship between
models. In addition, MSCT consistently performs well across
prediction accuracy and crash ratio. As for other time-step-
nearlyall-timesteps,showinglowerCRMSEvaluescompared
ahead predictions, there is a slight increase as the crash ratio
to other models, indicating better capability of heterogeneous
increases. This reflects the good adaptable capability of these
causal effects estimation.
TABLEII
CRMSERESULTSFORSYNTHETICTRAFFICDATA(ω=5)
τ =1 τ =2 τ =3 τ =4 τ =5
LSTM 15.33 14.95 12.67 12.22 10.49
BiLSTM 15.81 17.34 16.74 19.00 25.17
GRU 17.31 16.60 12.03 10.87 8.49
RNN 14.74 15.49 13.51 13.18 15.12
MSMs 16.57 15.01 12.4 11.15 10.11
RMSN 14.84 17.26 9.29 6.72 4.08
CRN 14.58 15.32 8.26 5.48 4.38
G-Net 16.17 18.79 13.96 11.26 10.35
CT 16.01 19.48 19.48 10.81 3.62
MSCT 13.62 14.70 8.05 5.38 4.67
Fig.7 shows the prediction results of MSCT for a unit
under six intervention strategies, with Fig.7(a)-(e) under crash
conditions, and no crash occurs in Fig.7(f). According to
the figure, a noticeable decline is easily observed following
crashes, and speeds gradually recover over time. This further Fig. 7. Prediction performance of MSCT for different treatment strategies.
illustrates the crash-aware capability of the proposed model. Reddashedlinedenotesthefirsttimestepaftercrashes.JOURNALOFLATEXCLASSFILES 11
Fig.8. TheImpactofCrashRatioinReal-worldDatasetonRMSE
TABLEIII
RMSERESULTSFORREAL-WORLDTRAFFICDATA(CRASH
RATIO=0.3)
τ =1 τ =2 τ =3 τ =4 τ =5 τ =6
LSTM 0.92 8.37 8.71 8.63 8.70 8.83
BiLSTM 1.11 8.03 8.56 8.70 8.74 8.76
GRU 1.01 7.63 8.34 8.57 8.72 8.83
RNN 2.64 5.15 6.26 7.06 7.48 7.32
MSMs 1.35 2.98 4.40 5.88 7.52 9.20
RMSN 1.17 2.87 3.64 4.2 4.74 5.32
CRN 0.98 2.92 3.84 4.57 5.13 5.58
G-Net 3.4 4.23 4.85 5.42 5.97 6.47
CT 0.95 2.74 3.66 4.28 4.76 5.16
MSCT 0.83 2.68 3.56 4.15 4.62 5.04
counterfactual models. Because the different distributions of
the dataset may cause different selection biases, counterfac-
tual prediction models should be robust for the imbalanced
observational dataset. In addition, our proposed model still
achieves state-of-the-art performance.
Fig.9 illustrates the comparison of predicted results by
MSCT across different crash types. The four subgraphs
Fig.9. ComparisonoforiginalandpredictedspeedbyMSCTacrossdifferent
demonstrate the various types of crashes: no crash, OBJ, crashtypes.Thereddottedlineindicatesthemomentofcrash
WIPE, and REAR. The 1-step-ahead and 2-step-ahead speeds
predicted by the encoder and decoder are compared with the
original speed. This figure reveals the varying degrees of • w/obl:removebalancedloss(i.e.,PSlossandHPSloss)
impact,anditcanbeobservedthatthepredictedvaluesclosely from final loss. Consequently, LSTM pathway, PS head,
match the original data for all crash types, demonstrating the and HPS head are also removed.
ability of MSCT to handle heterogeneous scenarios. • w/ gr: replace domain confusion training by gradient
reversal training [35], [51].
C. Ablation Study TABLE IV summarizes the ablation study results. Overall,
MSCT outperforms all its variants in predicting the speed of
To evaluate the effectiveness of each component inside
almost all time steps. When replacing the transformer with
MSCT,theablationtestisconductedusingsyntheticdata.The
LSTMnetwork,althoughtheperformanceof1-step-aheadpre-
setting of the ablation study is shown as follows:
diction is better than others, the other results predicted by the
• w/o transf: replace the transformer module with LSTM decoder are much worse. This indicates that the transformer
network. improves the performance for sequence-to-sequence structure
• w/ops:removethePSlossfromfinalloss.Consequently, andismoresuitableforautoregressiveprediction.Thevariants
LSTM pathway in MSCT block and PS head are also without PS loss and balance loss present poorer ability than
removed. MSCT, which proves the effectiveness of propensity scoreJOURNALOFLATEXCLASSFILES 12
and historical propensity score for eliminating time-varying [6] V. Karwa, A. B. Slavkovic´, and E. T. Donnell, “Causal inference in
confounding bias. When using gradient reversal instead of transportation safety studies: Comparison of potential outcomes and
causal diagrams,” The Annals of Applied Statistics, vol. 5, no. 2B, pp.
domainconfusion,thereisalmostnodifferenceforshort-term
1428–1455,Jun.2011.
prediction, however, the performance becomes worse when [7] M. Miller and C. Gupta, “Mining traffic incidents to forecast impact,”
predicting long-term speed. In all, the results highlight the inProceedingsoftheACMSIGKDDInternationalWorkshoponUrban
Computing. Beijing China: ACM, Aug. 2012, pp. 33–40. [Online].
importance of transformer, propensity score adjustment, and
Available:https://dl.acm.org/doi/10.1145/2346496.2346502
domain confusion training in the MSCT model. [8] B. Pan, U. Demiryurek, C. Gupta, and C. Shahabi, “Forecasting spa-
tiotemporal impact of traffic incidents for next-generation navigation
systems,”KnowledgeandInformationSystems,vol.45,no.1,pp.75–
TABLEIV
104,Oct.2015.
ABLATIONRESULTSFORSYNTHETICTRAFFICDATA(ω=5)
[9] J.Wang,Q.Gu,J.Wu,G.Liu,andZ.Xiong,“TrafficSpeedPrediction
and Congestion Source Exploration: A Deep Learning Method,” in
τ =1 τ =2 τ =3 τ =4 τ =5 τ =6 2016 IEEE 16th International Conference on Data Mining (ICDM).
w/otransf 7.88 11.96 12.96 13.34 13.36 13.4 Barcelona, Spain: IEEE, Dec. 2016, pp. 499–508. [Online]. Available:
w/ops 7.98 10.89 11.97 12.24 12.66 12.55 http://ieeexplore.ieee.org/document/7837874/
[10] Q. Xie, T. Guo, Y. Chen, Y. Xiao, X. Wang, and B. Y. Zhao, “How
w/obl 8.04 10.97 12.12 12.46 12.51 12.38
do urban incidents affect traffic speed? A Deep Graph Convolutional
w/gr 7.98 10.68 11.97 12.15 12.02 12.05 Network for Incident-driven Traffic Speed Prediction,” Dec. 2019,
MSCT 7.96 10.47 11.77 11.99 11.86 11.86 arXiv:1912.01242.[Online].Available:http://arxiv.org/abs/1912.01242
[11] F. L. Mannering, C. R. Bhat, V. Shankar, and M. Abdel-Aty, “Big
data,traditionaldataandthetradeoffsbetweenpredictionandcausality
in highway-safety analysis,” Analytic Methods in Accident Research,
VII. CONCLUSIONS
vol.25,p.100113,Mar.2020.
This study has presented a novel causal deep learn- [12] F. Mannering, “Temporal instability and the analysis of highway
accident data,” Analytic Methods in Accident Research, vol. 17, pp.
ing model, named Marginal Structural Causal Transformer
1–13, Mar. 2018. [Online]. Available: https://www.sciencedirect.com/
(MSCT), for counterfactual traffic speed prediction under the science/article/pii/S2213665717300271
interventionoftrafficcrashes.Theproposedmodeleffectively [13] H.Li,D.J.Graham,andA.Majumdar,“TheImpactsofSpeedCameras
on Road Accidents: An Application of Propensity Score Matching
addresses the challenge of time-varying confounding bias
Methods,”Accident;analysisandprevention,Jan.2013.
in time-series traffic data and the heterogeneous effects of [14] H.LiandD.J.Graham,“Quantifyingthecausaleffectsof20mphzones
crashes.Duetothelackofground-truthdata,wealsopropose on road casualties in London via doubly robust estimation,” Accident
Analysis&Prevention,vol.93,pp.65–74,Aug.2016.
a synthetic traffic data generation procedure that mimics
[15] F.L.Mannering,V.Shankar,andC.R.Bhat,“Unobservedheterogeneity
the causal relationship between crashes and traffic speed. andthestatisticalanalysisofhighwayaccidentdata,”AnalyticMethods
Our experimental results demonstrate that the MSCT outper- inAccidentResearch,vol.11,pp.1–16,Sep.2016.[Online].Available:
https://www.sciencedirect.com/science/article/pii/S2213665716300100
forms existing models both in synthetic data and real-world
[16] J. M. Robins, M. A. Herna´n, and B. Brumback, “Marginal structural
data, particularly in longer prediction horizons. The findings models and causal inference in epidemiology,” Epidemiology (Cam-
highlight the importance of incorporating causal theory into bridge,Mass.),vol.11,no.5,pp.550–560,Sep.2000.
[17] M.A.Herna´nandJ.Rubins,CausalInference:WhatIf. BocaRaton:
deep learning models for counterfactual outcome prediction
Chapman&Hall/CRC,2020.
in transportation research. In the future, we plan to explore [18] J. Berrevoets, K. Kacprzyk, Z. Qian, and M. van der Schaar, “Causal
spatialcausalrelationshipsaswell,consideringthattrafficdata DeepLearning,”Mar.2023,arXiv:2303.02186[cs].[Online].Available:
http://arxiv.org/abs/2303.02186
from the road network exhibits spatial dependencies. Besides,
[19] F. D. Johansson, U. Shalit, and D. Sontag, “Learning Representations
the model architecture will be further refined to explore for Counterfactual Inference,” Jun. 2018, arXiv:1605.03661 [cs, stat].
the model’s applicability in different transportation scenarios. [Online].Available:http://arxiv.org/abs/1605.03661
[20] A. Alaa, M. Weisz, and M. Schaar, “Deep
Overall, MSCT represents the first attempt at counterfactual
Counterfactual Networks with Propensity-Dropout,” ArXiv,
prediction by deep learning under traffic crash conditions and Jun. 2017. [Online]. Available: https://www.semanticscholar.
shows great potential for improving traffic management and org/paper/Deep-Counterfactual-Networks-with-Alaa-Weisz/
dc3d7783fb12824b02fb5ae26117b3470d0d9e3c
decision-making processes in real-world scenarios.
[21] C. Louizos, U. Shalit, J. Mooij, D. Sontag, R. Zemel, and
M. Welling, “Causal Effect Inference with Deep Latent-Variable
REFERENCES Models,” Nov. 2017, arXiv:1705.08821 [cs, stat]. [Online]. Available:
http://arxiv.org/abs/1705.08821
[1] “Globalstatusreportonroadsafety2023,”WorldHealthOrganization, [22] J. Yoon, J. Jordon, and M. v. d. Schaar, “GANITE: Estimation
Geneva,Tech.Rep.,2023. of Individualized Treatment Effects using Generative Adversarial
[2] S.A.Tedesco,V.Alexiadis,W.R.Loudon,R.Margiotta,andD.Skinner, Nets,”Feb.2018.[Online].Available:https://openreview.net/forum?id=
“Developmentofamodeltoassessthesafetyimpactsofimplementing ByKWUeWA-
ivhsuserservices,”inMovingTowardDeployment.Proceedingsofthe [23] C. Shi, D. M. Blei, and V. Veitch, “Adapting Neural Networks for
IVHS America Annual Meeting. 2 VolumesIVHS America, no. Volume theEstimationofTreatmentEffects,”Oct.2019,arXiv:1906.02120[cs,
1,1994. stat].[Online].Available:http://arxiv.org/abs/1906.02120
[3] J.Pearl,“Causalinferenceinstatistics:Anoverview,”StatisticsSurveys, [24] B. Lim, “Forecasting Treatment Responses Over Time Using
vol.3,pp.96–146,Jul.2009. Recurrent Marginal Structural Networks,” in Advances in Neural
[4] A.AlaaandM.Schaar,“LimitsofEstimatingHeterogeneousTreatment Information Processing Systems, vol. 31. Curran Associates, Inc.,
Effects: Guidelines for Practical Algorithm Design,” in Proceedings 2018. [Online]. Available: https://proceedings.neurips.cc/paper files/
of the 35th International Conference on Machine Learning. PMLR, paper/2018/hash/56e6a93212e4482d99c84a639d254b67-Abstract.html
Jul. 2018, pp. 129–138, iSSN: 2640-3498. [Online]. Available: [25] I. Bica, A. M. Alaa, J. Jordon, and M. van der Schaar, “Estimating
https://proceedings.mlr.press/v80/alaa18a.html Counterfactual Treatment Outcomes over Time Through Adversarially
[5] F. L. Mannering, “Selectivity bias in models of discrete and Balanced Representations,” Feb. 2020, arXiv:2002.04083 [cs, stat].
continuous choice: An empirical analysis,” Transportation Research [Online].Available:http://arxiv.org/abs/2002.04083
Record, no. 1085, 1986, iSBN: 9780309041058. [Online]. Available: [26] R.Li,S.Hu,M.Lu,Y.Utsumi,P.Chakraborty,D.M.Sow,P.Madan,
https://trid.trb.org/view/283758 J.Li,M.Ghalwash,Z.Shahn,andL.-w.Lehman,“G-Net:aRecurrentJOURNALOFLATEXCLASSFILES 13
Network Approach to G-Computation for Counterfactual Prediction [44] M. A. Herna´n, B. Brumback, and J. M. Robins, “Marginal Structural
Under a Dynamic Treatment Regime,” in Proceedings of Machine Models to Estimate the Joint Causal Effect of Nonrandomized Treat-
Learning for Health. PMLR, Nov. 2021, pp. 282–299, iSSN: 2640- ments,” Journal of the American Statistical Association, vol. 96, no.
3498.[Online].Available:https://proceedings.mlr.press/v158/li21a.html 454,pp.440–448,Jun.2001.
[27] V. Melnychuk, D. Frauen, and S. Feuerriegel, “Causal Transformer [45] J.M.Robins,“Association,Causation,andMarginalStructuralModels,”
for Estimating Counterfactual Outcomes,” in Proceedings of the Synthese, vol. 121, no. 1/2, pp. 151–179, 1999, publisher: Springer.
39th International Conference on Machine Learning. PMLR, Jun. [Online].Available:https://www.jstor.org/stable/20118224
2022, pp. 15293–15329, iSSN: 2640-3498. [Online]. Available: [46] J. Chung, K. Kastner, L. Dinh, K. Goel, A. Courville, and
https://proceedings.mlr.press/v162/melnychuk22a.html Y. Bengio, “A Recurrent Latent Variable Model for Sequential
[28] Z. Zhou, Z. Yang, Y. Zhang, Y. Huang, H. Chen, and Z. Yu, “A Data,” Apr. 2016, arXiv:1506.02216 [cs]. [Online]. Available: http:
comprehensivestudyofspeedpredictionintransportationsystem:From //arxiv.org/abs/1506.02216
vehicletotraffic,”iScience,vol.25,no.3,p.103909,Mar.2022. [47] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
[29] C.-S. Li and M.-C. Chen, “Identifying important variables for A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention Is All
predicting travel time of freeway with non-recurrent congestion You Need,” Jul. 2023, arXiv:1706.03762 [cs]. [Online]. Available:
with neural networks,” Neural Computing and Applications, vol. 23, http://arxiv.org/abs/1706.03762
no. 6, pp. 1611–1629, Nov. 2013. [Online]. Available: http: [48] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer Normalization,”
//link.springer.com/10.1007/s00521-012-1114-z Jul. 2016, arXiv:1607.06450 [cs, stat]. [Online]. Available: http:
[30] R. J. Javid and R. Jahanbakhsh Javid, “A framework for travel //arxiv.org/abs/1607.06450
time variability analysis using urban traffic incident data,” IATSS [49] E.Tzeng,J.Hoffman,T.Darrell,andK.Saenko,“SimultaneousDeep
Research, vol. 42, no. 1, pp. 30–38, Apr. 2018. [Online]. Available: Transfer Across Domains and Tasks,” Oct. 2015, arXiv:1510.02192
https://www.sciencedirect.com/science/article/pii/S0386111216300395 [cs].[Online].Available:http://arxiv.org/abs/1510.02192
[31] S. Shafiei, A.-S. Mihaita, H. Nguyen, C. Bentley, and C. Cai, “Short- [50] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic
Term Traffic Prediction Under Non-Recurrent Incident Conditions Optimization,” Jan. 2017, arXiv:1412.6980 [cs]. [Online]. Available:
Integrating Data-Driven Models and Traffic Simulation,” Jan. 2020. http://arxiv.org/abs/1412.6980
[Online].Available:https://opus.lib.uts.edu.au/handle/10453/138721 [51] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
F. Laviolette, M. Marchand, and V. Lempitsky, “Domain-Adversarial
[32] D. B. Rubin, “Bayesian Inference for Causal Effects: The Role of
Randomization,”TheAnnalsofStatistics,vol.6,no.1,pp.34–58,Jan. TrainingofNeuralNetworks,”May2016,arXiv:1505.07818[cs,stat].
[Online].Available:http://arxiv.org/abs/1505.07818
1978.
[33] J. Splawa-Neyman, D. M. Dabrowska, and T. P. Speed, “On the
Application of Probability Theory to Agricultural Experiments. Essay
onPrinciples.Section9,”StatisticalScience,vol.5,no.4,pp.465–472,
Nov.1990.
[34] S.Chakraborty,R.Tomsett,R.Raghavendra,D.Harborne,M.Alzantot,
F.Cerutti,M.Srivastava,A.Preece,S.Julier,R.M.Rao,T.D.Kelley,
D.Braines,M.Sensoy,C.J.Willis,andP.Gurram,“Interpretabilityof
deeplearningmodels:Asurveyofresults,”in2017IEEESmartWorld,
UbiquitousIntelligence&Computing,Advanced&TrustedComputed,
ScalableComputing&Communications,Cloud&BigDataComputing,
InternetofPeopleandSmartCityInnovation,Aug.2017,pp.1–6.
[35] Y.Li,X.Tian,M.Gong,Y.Liu,T.Liu,K.Zhang,andD.Tao,“Deep
DomainGeneralizationviaConditionalInvariantAdversarialNetworks,”
in Computer Vision – ECCV 2018, ser. Lecture Notes in Computer
Science, V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss, Eds.
Cham:SpringerInternationalPublishing,2018,pp.647–663.
[36] A. P. Afghari, E. Papadimitriou, F. Pilkington-Cheney, A. Filtness,
T. Brijs, K. Brijs, A. Cuenen, B. De Vos, H. Dirix, V. Ross, G. Wets,
A.Lourenc¸o,andL.Rodrigues,“Investigatingtheeffectsofsleepiness
intruckdriversontheirheadway:Aninstrumentalvariablemodelwith
groupedrandomparametersandheterogeneityintheirmeans,”Analytic
MethodsinAccidentResearch,vol.36,p.100241,Dec.2022.
[37] Z.Zhang,B.Akinci,andS.Qian,“Inferringthecausaleffectofwork
zonesoncrashes:Methodologyandacasestudy,”AnalyticMethodsin
AccidentResearch,vol.33,p.100203,Mar.2022.
[38] Y.Zhang,H.Li,andG.Ren,“Estimatingheterogeneoustreatmentef-
fectsinroadsafetyanalysisusinggeneralizedrandomforests.”Accident
Analysis&Prevention,vol.165,p.106507,Nov.2021.
[39] X.Liu,S.Qian,H.-H.Teo,andW.Ma,“Estimatingandmitigatingthe
congestioneffectofcurbsidepick-upsanddrop-offs:Acausalinference
approach,”2023.
[40] S. Li, Z. Pu, Z. Cui, S. Lee, X. Guo, and D. Ngoduy, “Inferring
heterogeneoustreatmenteffectsofcrashesonhighwaytraffic:Adoubly
robustcausalmachinelearningapproach,”TransportationResearchPart
C:EmergingTechnologies,vol.160,p.104537,2024.
[41] L. Li, X. Su, Y. Wang, Y. Lin, Z. Li, and Y. Li, “Robust causal
dependence mining in big data network and its application to
traffic flow predictions,” Transportation Research Part C: Emerging
Technologies, vol. 58, pp. 292–307, Sep. 2015. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0968090X15000820
[42] T. Gao, R. K. Marques, and L. Yu, “GT-CausIn: a novel causal-based
insight for traffic prediction,” Dec. 2022, arXiv:2212.05782 [cs].
[Online].Available:http://arxiv.org/abs/2212.05782
[43] S.He,Q.Luo,R.Du,L.Zhao,andH.Li,“STGC-GNNs:AGNN-based
traffic prediction framework with a spatial-temporal Granger causality
graph,” Physica A: Statistical Mechanics and its Applications, vol.
623,p.128913,Aug.2023,arXiv:2210.16789[cs].[Online].Available:
http://arxiv.org/abs/2210.16789