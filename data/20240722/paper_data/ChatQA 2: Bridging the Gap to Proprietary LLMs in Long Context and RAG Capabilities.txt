ChatQA 2: Bridging the Gap to Proprietary LLMs in
Long Context and RAG Capabilities
PengXu∗, WeiPing∗, XianchaoWu, ZihanLiu,MohammadShoeybi, BryanCatanzaro
NVIDIA
∗{pengx, wping}@nvidia.com
Abstract
Inthiswork,weintroduceChatQA2,aLlama3-basedmodeldesignedtobridge
thegapbetweenopen-accessLLMsandleadingproprietarymodels(e.g.,GPT-4-
Turbo)inlong-contextunderstandingandretrieval-augmentedgeneration(RAG)
capabilities.ThesetwocapabilitiesareessentialforLLMstoprocesslargevolumes
of information that cannot fit into a single prompt and are complementary to
eachother,dependingonthedownstreamtasksandcomputationalbudgets. We
present a detailed continued training recipe to extend the context window of
Llama3-70B-basefrom8Kto128Ktokens,alongwithathree-stageinstruction
tuningprocesstoenhancethemodel’sinstruction-following,RAGperformance,
and long-context understanding capabilities. Our results demonstrate that the
Llama3-ChatQA-2-70B model achieves accuracy comparable to GPT-4-Turbo-
2024-0409onmanylong-contextunderstandingtasksandsurpassesitontheRAG
benchmark. Interestingly,wefindthatthelong-contextretrievercanalleviatethe
top-kcontextfragmentationissueinRAG,furtherimprovingRAG-basedresultsfor
long-contextunderstandingtasks. Wealsoprovideextensivecomparisonsbetween
RAGandlong-contextsolutionsusingstate-of-the-artlong-contextLLMs.
1 Introduction
TheopenLLMcommunityhasmadesignificantprogressinadvancingthecapabilitiesofopen-access
large language models (LLMs), including Llama-3-70B-Instruct (Meta-AI, 2024), QWen2-72B-
Instruct(Alibaba-QWen,2024),Nemotron-4-340B-Instruct(Nvidiaetal.,2024),andMixtral-8x22B-
Instruct-v0.1(Mistral,2024). However,performancegapscomparedtofrontierproprietarymodels,
e.g.,GPT-4-Turbo(OpenAI,2023),stillexistinmanydomains. Additionally,open-accessmodels
focusedonkeydomainshavebeendeveloped,suchasDeepSeek-Coder-V2(Zhuetal.,2024)for
coding and math, ChatQA 1.5 (Liu et al., 2024) for conversational QA and retrieval-augmented
generation(RAG),andInternVL1.5(Chenetal.,2024)forvision-languagetasks,whichcanbeon
parwithGPT-4-Turbo-2024-04-09(OpenAI,2023)inthecertaindomains.
In recent developments, the trend of extending the context window length in LLMs has gained
remarkabletractionwithinboththeindustrialandresearchcommunities. Allleadingproprietary
LLMssupportverylargecontextwindow,allowingthemtoaccommodateseveralhundredpagesof
textinasingleprompt. Forexample,GPT-4Turbo(OpenAI,2023)andClaude3.5Sonnetoffera
128Kand200Kcontextwindow,respectively. Meanwhile,Gemini1.5Pro(Gemini-Team,2024)
impressivelysupportsuptoa10Mcontext. Open-accessLLMshavealsomadesignificantstrides
tokeepup(01.AIetal.,2024;Alibaba-QWen,2024). Forinstance,QWen2-72B-Instruct(Alibaba-
QWen,2024)andYi-34B(01.AIetal.,2024)support128Kand200Kcontextwindows,respectively.
However,thetrainingdataandtechnicaldetailsforthesemodelsaremissing,makingreproduction
challenging. Inaddition,thesemodelshavemostlybeenevaluatedonsynthetictasks,likeNeedlein
aHaystack(Kamradt,2023)test,whichdoesnotaccuratelyrepresentreal-worlddownstreamtask
performance. Forexample, previousstudiesshowsanoticeablegapbetweenopen-accessLLMs
4202
luJ
91
]LC.sc[
1v28441.7042:viXraandleadingproprietarymodelsonreal-worldlongcontextunderstandingtasks(Zhangetal.,2024a;
Hsiehetal.,2024). Inthiswork,wefocusonbridgingthegapbetweentheopen-accessLlama-3and
proprietaryGPT-4Turboonreal-worldlongcontextunderstandingtasks.
Thelong-contextcapabilityofLLMsissometimesconsideredarivaltechniquetoretrieval-augmented
generation(RAG).However,fromapragmaticperspective,thesetechniquescomplementeachother.
AnLLMwithalongcontextwindowcaneitherprocesslargevolumesoftextasapromptorutilize
retrievalmethodstoefficientlyextractrelevantinformationfromtheextensivetext,dependingon
thedownstreamtasksandaccuracyvs. efficiencytrade-offs. RAGhasefficiencyadvantagesand
caneasilyretrieverelevantcontextsforquery-basedtasks(e.g,QA)frombillionsoftokens,afeat
thatlongcontextmodelscannotachieve. Meanwhile,longcontextmodelsaregoodattaskssuchas
summarizingentiredocuments,whereRAGmaynotperformaswell. Asaresult,thestate-of-the-art
LLMneedstoexcelatbothcapabilities,providingoptionsfordifferentdownstreamtasksbasedon
accuracyandefficiencyrequirements. Inapreviousstudy,theopen-sourceChatQA1.5(Liuetal.,
2024)modelcansurpassGPT-4-TurboonRAGtasks. Inthiswork,wepresentChatQA2,which
possessesbothGPT-4-TurbolevellongcontextunderstandingcapabilityandRAGperformance.
Xuetal.(2024)extendedthecontextwindowofLlama2(Touvronetal.,2023b)to16Kand32K
tokens,andstudiedtheinterplaybetweenRAGandlong-contextLLMs. Itdemonstratesthatthe
RAGmethodcanimprovethegenerationaccuracyandinferenceefficiencyofaGPT-3.5-turbo-16k
level long context model for QA and query-based summarization tasks. In this work, we extend
thisstudybypushingthelongcontextLLMtoGPT-4-Turbolevelwith128Kcontextwindowand
combiningitwithastate-of-the-artlong-contextretrieverforRAG.
Specifically,wemakethefollowingcontributions:
1. Wepresentatwo-stepapproachtoestablishthelongcontextcapabilityofLlama3-70B.First,
weextendLlama3-70Bbase’scontextwindowfrom8Kto128Kbycontinuallypretraining
it on a mix of SlimPajama (Soboleva et al., 2023) with upsampled long sequences (Fu
etal.,2024). Then, weapplyathree-stageinstructiontuningprocessoncurateddatasets
toenhancetheinstruction-following,RAG,andlongcontextunderstandingcapabilitiesat
eachrespectivestage. Wefindthatstage-wiseinstruction-tuning,byincorporatingprevious
datasets,simplifiesexperimentationandhyperparametertuning. Thisapproachenhanceslong
contextcapabilitieswhilemaintainingRAGperformance.
2. We demonstrate that the resulting Llama3-ChatQA-2-70B-128K can be on par or slightly
worsethanGPT-4-Turbo-2024-04-09onmanyreal-worldlongcontextunderstandingtasks.
Inaddition,itoutperformsGPT-4-Turbo-2024-04-09onRAGandconversationalQAtasks.
3. ThecurrentRAGpipelinehaslimitationsthatcanunderminedownstreamtaskaccuracy: i)
top-kchunk-wiseretrievalintroducesfragmentationofcontext;andii)asmalltop-kleads
tolowrecall,whilealargerkintroducestoomuchirrelevantcontexttotheLLM(e.g.,see
theanalysisinFigure1ofYuetal.,2024). Wefindthatthestate-of-the-artlong-context
retriever(Wangetal.,2023c;Leeetal.,2024)canlargelyalleviatetheseissuesandfurther
improvetheRAG-basedresultsforlong-contextunderstandingtasks.
4. InthecomparisonbetweenRAGandlong-contextresults,wefindthattheGPT-4-Turbolevel
long-contextmodel(includingourLlama3-ChatQA-2-70B)outperformstheRAGon32K
benchmarksbutstillunderperformscomparedtoRAGmethodsonreal-world128Ktasks.
We organize the rest of the paper as follows. We discuss related work in § 2. We introduce the
continued pretraining for context window extension in § 3 and the three-stage instruction tuning
in§4. Wereportresultsin§7andconcludethepaperin§8.
2 RelatedWork
2.1 LongContextLLM
ThetrendofextendingthecontextwindowinLLMstartsbyClaudewith100Ktokencontext(An-
thropic, 2023). Although the underlying long context techniques behind proprietary models are
unclear,theopenLLMandresearchcommunityhasdevelopedmanymethodstoextendthecontext
windowofLLMsthroughcontinuedtrainingorfine-tuning(Kaiokendev,2023;Nijkampetal.,2023;
Chenetal.,2023a;Tworkowskietal.,2023;Chenetal.,2023b;Pengetal.,2023;Xiongetal.,2023;
2Fuetal.,2024),especiallyforopen-accessLLMs(Touvronetal.,2023a,b)basedonrotaryposition
embedding(RoPE)(Suetal.,2024).
TherearetwopopularapproachestoadaptRoPEforlong-contextinputs:positioninterpolation(Chen
etal.,2023a)andincreasingthebasefrequencyθofRoPE(Xiongetal.,2023;Liuetal.,2023b).
Recently,Yi-34B(01.AIetal.,2024)waspretrainedwithasequencelengthof4K,anditscontext
window was extended to 200K by increasing the RoPE θ from 10,000 to 5M during continued
pretraining. Qwen2-72B-Instruct(Alibaba-QWen,2024)wastrainedon32K-lengthcontextsand
extrapolated to a 128K context length using YaRN (Peng et al., 2023). Instead of extending the
context window of the base model then applying instruction tuning, GradientAI (2024) directly
fine-tunestheLlama-3-Instruct, whichusesNTK-awareinterpolation(Pengetal.,2023)andthe
formulafromLiuetal.(2023b)toscaleupθ.
2.2 Retrieval-augmentedGeneration(RAG)
Retrievalwithastandaloneretriever(e.g.,Karpukhinetal.,2020;Wangetal.,2022;Linetal.,2023;
Leeetal.,2024)isalong-standingsolutionforhandlinglongtextsthatcannotfitintothecontext
windowoflanguagemodels. Inpreviouswork,variousretrieval-augmentedlanguagemodelshave
beenproposed(Nakanoetal.,2021;Borgeaudetal.,2022;Wangetal.,2023b,a;Guuetal.,2020;
Izacard&Grave,2021;Izacardetal.,2022;Lewisetal.,2020;Huangetal.,2023;Khandelwaletal.,
2019;Liuetal.,2024).
Previous dense-embedding-based retrievers only supported limited context windows (e.g., 512
tokens) (e.g., Karpukhin et al., 2020; Wang et al., 2022; Lin et al., 2023). In top-k chunk-wise
retrieval,theshortchunksizeincreasescontextfragmentation. Asaresult,extendingthecontext
windowofretrievershasbecomepopular. Forexample,JinaEmbeddings2(Güntheretal.,2023)and
NomicEmbed(Nussbaumetal.,2024)support8Ktokens,whileE5-mistral-7B(Wangetal.,2023c)
andNV-EmbedLeeetal.(2024)support32Ktokens.
3 ExtendingContextWindowto128K
Inthissection,wepresentthemethodtoextendthecontextwindowfrom8Kto128KforLlama3.We
prepareourlongcontextpretrainingcorpusfromtheSlimpajama(Sobolevaetal.,2023)following
Fuetal.(2024). Weupsamplelong-contextdocumentswiththehyperparametersetas0.1toproduce
10billiontokenswithsequencelengthof128k. SinceLlama3ispretrainedwithamuchhigherRoPE
basefrequencyof500,000comparedtoLlama2,weincreasedtheRoPEbasefrequencyto150M
accordingly. Wesetthebatchsizeto32tofit4milliontokensinabatchandusealearningrateof
3e-5totrain2000steps(8Btokensintotal).
Interestingly,wefounditmoreeffectivetoseparatedifferentdocumentsusingspecialcharacters,such
as"<s>",ratherthanthereservedbeginningandendingtokens<BOS>and<EOS>. Wehypothesize
thatthe<BOS>and<EOS>tokensinLlama3signalthemodeltoignorepreviouschunksoftext
afterpretraining,whichisnothelpfulfortheLLMstoadaptforlongercontextinputs.
4 Instruction-TuningwithLongContextData
In this section, we present the instruction tuning method designed to enhance both long context
understandingcapabilityandRAGperformance.
Specifically, weimplementthreestagesofinstruction-tuning. Forthefirsttwostages, wefollow
ChatQA1.5(Liuetal.,2024),wherethemodelisinitiallytrainedon128khigh-qualityinstruction-
followingdatasets, andthentrainedonablendofconversationalQAdatawithprovidedcontext.
However,thesetwostagesinvolverelativelyshortcontexts,withamaximumsequencelengthof
only4Ktokens. Toenhanceourmodel’scapabilitytohandleverylongcontextsequencesupto128k
tokens,wecollectalongSFTdataset.
This dataset is collected through two categories: 1) For SFT data sequences less than 32k: We
leverageexistinglong-contextdatasetsfromLongAlpaca12k,GPT-4samplesfromOpenOrca1,and
1https://huggingface.co/datasets/Open-Orca/OpenOrca
3chunk-size 300 300 300 600 1200 1200
top-k 10 20 40 10 5 10
totaltokens 3000 6000 12000 6000 6000 12000
Avg. 46.31 46.28 46.96 46.88 47.08 47.13
Table 1: Ablation of Llama3-ChatQA-2-70B with RAG given different top-k = {5, 10, 20, 40}
retrieval, and chunk-size = {300, 600, 1200} on medium-long context benchmarks within 32K
tokens (see Section 6.2 for more details). Given the same budget of total tokens in the context
window(e.g.,6000),largerchunk-size(e.g.,1200)givesbetterresultsthansmallchunk-size(e.g.,
300and600). Theaccuracycanalsoimprovewithlargertotaltokensinthecontextwindow.
LongDataCollections2. 2)Forsequencelengthsbetween32kand128k: Sinceitischallengingto
collectsuchSFTsamples,werelyonsyntheticdatasets. WeutilizeNarrativeQA,whichcontains
boththegroundtruthsummaryandsemanticallyrelatedparagraphs. Weassemblealltherelated
paragraphsandrandomlyinsertthegroundtruthsummarytosimulateareallongdocumentforits
QApairs. BoththefulllongSFTdatasetandtheshortSFTdatasetfromthefirsttwostagesarethen
blendedfortraining. Wesetthelearningrateat3e-5andthebatchsizeat32.
5 LongContextRetrievermeetsLongContextLLM
Aswementionedinprevioussection,thecurrentRAGpipelineforLLMhasthefollowingissues:
i)Thetop-kchunk-wiseretrievalintroducesnon-negligiblefragmentationofcontextforgenerating
accurateanswers. Forexample,previousstate-of-the-artdense-embeddingbasedretrievers(e.g.,Li
etal.,2023;Linetal.,2023)onlysupport512tokens. ii)Smalltop-k(e.g.,5or10)usuallyleadsto
relativelylowrecall,whilemuchlargerk(e.g.,100)canleadtoworsegeneration(seeTable5inXu
etal.(2024))asthepreviousLLMscouldnotutilizetoomanychunkedcontextverywell(Liuetal.,
2023a). Toaddresstheissue,weproposetousethemostrecentlong-contextretriever(Wangetal.,
2023c;Leeetal.,2024),whichcansupportthousandsoftokens. Inoursetting,weusetheE5-mistral
embeddingmodel(Wangetal.,2023c)astheretriever.
Table1comparesdifferentchunksizesfortop-kretrievalandthetotalnumberoftokensinthecontext
window. Comparingtotaltokensfrom3000to12000,wefoundthatmoretokensconsistentlyyield
betterresults,confirmingthestronglong-contextcapabilityofourmodel. Wealsofoundthat6000
totaltokensofferagoodtrade-offbetweencostandperformance. Withthetotalnumberoftokensset
to6000,wediscoveredthatlargerchunksizesgivebetterresults. Therefore,weuseachunksizeof
1200andtop-5chunksasthedefaultinourexperiments.
6 EvaluationBenchmarks
We compare our model against SOTA long context models: 1) GPT-4-Turbo-2024-04-09 (128K
contextwindow)(OpenAI,2023),2)Qwen2-72B-Instruct(128Kcontextwindow)(Alibaba-QWen,
2024),and(3)Llama-3-70B-Instruct-Gradient-262k(GradientAI,2024).
Togiveacomprehensivestudyofdifferentcontextlengths,ourevaluationbenchmarkscoversthree
categories,1)longcontextbenchmarksbeyond100Ktokens,2)medium-longcontextbenchmarks
within32Ktokens,and3)shortcontextbenchmarkswithin4Ktokens. WealsoapplyRAGwhenit
isapplicabletothedownstreamtasks.
6.1 LongContextBenchmarksBeyond100KTokens
InfiniteBench(Zhangetal.,2024b)isproposedtoevaluatethelongcontextcapabilityofLLMsover
100Ksequencelength. Aswefocusonreal-worldenglishtasks,weonlytakethefourrelatedtasks
fromtheInfiniteBench,i.e. longbooksummarization(En.Sum),longbookqa(En.QA),longbook
multiplechoice(En.MC),andlongbookdialogue(En.Dia). En.Sumisataskthatrequiresmodelsto
generateaconcisesummaryofthegivennovelandisevaluatedusingtheROUGE-L-Summetric(Lin,
2https://huggingface.co/datasets/togethercomputer/Long-Data-Collections
42004). En.QA is annotated by a pipeline that ensures the questions’ necessitating of long-range
dependencies and reasoning, beyond simple short passage retrieval. Aggregation reasoning and
filteringreasoningarethetwoprimaryreasoningcategories. F1scoreisusedtoevaluatethequality
oftheanswer. En.MCisannotatedwiththesamepipelineofEn.QAexceptthatfouranswerchoices
areprovidedandexactmatchingscoresarereported. En.Dialeveragesmovieanddramascriptsfrom
a designated online database 3 with long, multi-role dialogues. In this task, random instances of
characternameswithinascriptaremaskedandtheobjectiveistocorrectlyidentifythesemasked
names. Exactmatchingscoreisusedagaintoevaluatethepredictionaccuracy.
6.2 Medium-LongContextBenchmarkswithin32KTokens
We use the long context datasets (except NarrativeQA as it is included in our training) from Xu
etal.(2024)asourbenchmarkformedium-longdatasetswithin32K.Therearesixdatasetsintotal,
whereQMSum(QM),Qasper(QASP),QuALITY(QLTY)aretokenfromSCROLLSandHotpotQA
(HQA)MuSiQue(MSQ),MultiFieldQA-en(MFQA)aretokenfromLongBench. Followingthe
officialmetrics,wereportthegeometricmeanofROUGEscores(i.e.,ROUGE1/2/L)(Lin,2004)for
QM,theexactmatching(EM)scoreforQLTY,andF1scoresfortheremainingfourdatasetsQASP,
MSQ,HQAandMFQA.
6.3 ShortContextwithin4KTokens
WeuseChatRAGBench(Liuetal.,2024)asourbenchmarkforshortcontextwithin4k. ChatRAG
benchconsistsof10datasetsandweexcludeHDialasitisvasincludedinourtraining. Followingthe
setupofLiuetal.(2024),forDoc2Dial(D2D),QuAC,andQReCCtaskwithlongdocuments,each
documentisdividedintosegmentsofroughly300words. Thetop5relevantchunksarethenretrieved
as context for each user question. For TopiOCQA and INSCIT, top-20 chunks were retrieved to
obtainsimilarcontextlengthtothefirstthreedatasets. TheotherfourdatasetsareCoQA,DoQA,
ConvFinQA(CFQA),andSQA,whichcoverawiderangeofdomainslikefinance,children’sstories,
literature,mid/highschoolexams,news,Wikipediaandetc. WeuseF1scoreasthemetrictoevaluate
thegenerationsandreporttheaveragescorewithoutHDialasitisafairzero-shotcomparisonsover
differentmodels.
7 Results
In this section, we present the results and comparisons from extensive benchmark evaluations.
We begin with the synthetic Needle in a Haystack test, then focus on real-world long context
understandingandRAGtasks.
7.1 NeedleInAHaystack
WeevaluateourLlama3-ChatQA-2-70BmodelontheNeedleInAHaystacktest(Kamradt,2023).
Thissynthetictaskispopularfortestingthelong-contextcapabilityofLLMs,andcanbeconsidered
asathresholdlevelevaluation. Figure1demonstratestheperformanceofourmodelwithupto128K
tokens, showingthat our modelachieves 100%accuracy. This testconfirms our model’sperfect
long-contextretrievalcapability.
7.2 LongContextEvaluationBeyond100KTokens
Inthissubsection,weevaluatethelongcontextcapabilitybeyond100Ktokensonthereal-world
tasksfromInfiniteBench(Zhangetal.,2024a). Table2showsthatourmodel(34.11)outperforms
manyexistingstate-of-the-artmodels,suchasGPT4-Turbo-2024-04-09(33.16),GPT4-1106preview
(28.23), Llama-3-70B-Instruct-Gradient-262k (32.57) and Claude 2 (33.96). Additionally, our
modelisveryclosetothehighestscoreof34.88achievedbyQwen2-72B-Instruct,confirmingthe
competitivelong-contextcapabilityofourmodel.
3https://imsdb.com
5Figure1: NeedleinAHaystacktestforLlama3-ChatQA-2-70Bupto128Kcontextwindow.
Model Avg. En.Sum En.QA En.MC En.Dia
GPT-4-1106preview 28.23 14.73 22.44 67.25 8.50
Claude2 33.96 14.50 11.97 62.88 46.50
Kimi-Chat 29.62 17.96 16.52 72.49 11.50
Yi-34B-200K <15.15 <5 12.17 38.43 <5
Llama-3-70B-Instruct-Gradient-262k 32.57 14.27 29.52 69.00 17.50
GPT-4-Turbo-2024-04-09 33.16 17.62 19.29 77.73 18.00
w/RAG N/A N/A 17.69 77.29 N/A
Qwen2-72B-Instruct 34.88 14.84 21.50 81.66 21.50
w/RAG N/A N/A 16.48 76.86 N/A
Llama3-ChatQA-2-70B 34.11 16.10 44.22 64.63 11.50
w/RAG N/A N/A 41.00 70.74 N/A
Table2: EvaluationresultsonInfiniteBenchincludesreal-worldlong-contextunderstandingtasks
beyonda100Kcontextwindow. ForRAG,weusetop-5retrievedchunks,eachwith1200tokens
fromE5-mistralretriever(Wangetal.,2023c).
7.3 Medium-LongContextEvaluationwithin32KTokens
In this subsection, we evaluate the medium-long context capability within 32K tokens. Table 3
showsthatGPT-4-Turbo-2024-04-09achievesthehighestscoreof51.93amongallmodels. Our
modelscores47.37, whichishigherthanLlama-3-70B-Instruct-Gradient-262kbutislowerthan
Qwen2-72B-Instruct. Thisdifferencecanbeattributedtotheextensive32Kpretrainingimplemented
byQwen2-72B-Instruct,whileweusedamuchsmallercontinuedpretrainingcorpus. Additionally,
wefoundthatalltheRAGsolutionsperformworsethanthelongcontextsolution,whichsuggestall
theseSOTAlongcontextLLMscanreallyhandle32Ktokenswithintheircontextwindow.
7.4 CHATRAGBENCH: ShortContextEvaluationwithin4KTokens
Inthissubsection,weevaluatethemodelsontheshortcontexttaskswithin4KtokensfromCHATRAG
BENCH(Liuetal.,2024).Ourmodelachievestheaveragescoreof54.81.Eventhoughitisworsethan
Llama3-ChatQA-1.5-70B,itstilloutperformsGPT-4-Turbo-2024-04-09andQwen2-72B-Instruct.
This confirms that extending short context models to long context is not a free lunch. How to
effectively extend the context window to even larger scale (e.g., million tokens in Gemini 1.5
Pro (Gemini-Team, 2024)) without any degradation on regular short context tasks is an exciting
researchdirection.
6Model Avg. QM QASP QLTY MSQ HQA MFQA
GPT-4-Turbo-2024-04-09 51.93 16.37 38.96 88.45 44.88 70.65 52.26
w/RAG 49.84 16.07 36.18 85.85 42.17 67.85 50.94
Qwen2-72B-Instruct 49.94 17.06 34.84 84.85 46.80 65.98 50.12
w/RAG 48.08 17.63 35.19 83.05 39.92 64.58 48.10
Llama3-ChatQA-2-70B 47.37 15.20 33.77 81.45 37.27 62.69 53.84
w/RAG 47.08 14.71 33.25 80.45 39.45 61.04 53.58
Llama-3-70B-Instruct-Gradient-262k 40.51 20.72 30.64 74.35 20.20 45.82 51.33
w/RAG 40.57 20.04 30.68 72.35 22.19 46.85 51.31
Table3: Evaluationresultsonthemedium-longcontextbenchmarkswithin32Ktokens. ForRAG,
weusetop-5retrievedchunks,eachwith1200tokensfromE5-mistralretriever(Wangetal.,2023c).
Avg.w/o
Models D2D QuAC QReCC CoQA DoQA CFQA SQA TCQA INSCIT
HDial
Llama2-Chat-70B 44.64 36.87 32.47 49.40 80.41 38.97 46.85 37.62 44.31 34.88
Llama3-Instruct-70B 52.95 37.88 36.96 51.34 76.98 41.24 76.60 69.61 49.72 36.23
CommandR+ 51.40 33.51 34.16 49.77 69.71 40.67 71.21 74.07 53.77 35.76
GPT-3.5-Turbo-0613 50.69 34.83 37.17 50.46 79.33 41.11 73.15 60.63 44.30 35.27
GPT-4-0613 54.35 34.16 40.29 52.01 77.42 43.39 81.28 79.21 45.09 36.34
Llama3-ChatQA-1.5-70B 57.14 41.26 38.82 51.40 78.44 50.76 81.88 83.82 55.63 32.31
GPT-4-Turbo-2024-04-09 54.72 35.35 40.10 51.46 77.73 41.60 84.16 79.98 48.32 33.75
Llama-3-70B-Instruct-Gradient-262k 45.20 34.30 24.01 49.60 73.45 25.76 54.70 63.80 46.30 34.89
Qwen2-72B-Instruct 54.06 35.76 38.48 51.21 85.04 33.89 77.52 77.06 51.64 35.90
Llama3-ChatQA-2-70B 54.81 40.76 38.99 47.12 72.44 51.21 78.52 78.15 55.75 30.35
Table4: EvaluationresultsonCHATRAGBENCHwith9datasets. Following(Liuetal.,2024),we
excludeHDialasitisincludedintheinstructiontuningdatasets. Themaximumcontextlengthsare
4Ktokens.
Top-k 5 10 20 30 LongContext
Llama3-ChatQA-2-70B 47.08 47.13 47.18 47.19 47.37
Table5: WecompareourLlama3-ChatQA-2-70BusingRAGvs. directlongcontextevaluationon
benchmarkswithmaximum32Ktokensinputs. RAGcanbeslightlyworsethandirectlongcontext
solutionevenwhenweincreasetop-kchunksto30.
RAG(top-k) LongContext
Llama3-ChatQA-2-70B 56.36(5) 54.43
Qwen2-72B-Instruct 52.95(20) 51.58
Table6: WecompareRAGvs. longcontextevaluationusingourLlama3-ChatQA-2-70Bontasks
beyond100k. HereweuseaverageaccuracyofEn.QAandEn.MCtasksthatcanapplyRAG.The
RAG-basedresultisstillbetterthanlongcontextevaluation.
7.5 RAGvs. LongContext
InTable5andTable6,wecompareRAGvs. longcontextsolutionsunderdifferentcontextlengths.
Forsequencelengthbeyond100k,weonlyreporttheaveragescoreofEn.QAandEn.MCasthe
RAGsettingisnotdirectlyapplicableforEn.SumandEn.Dia. Wefoundthatfordownstreamtasks
within32ksequencelength,ourlongcontextsolutionisbetterthanRAG.ThismeansusingRAGcan
savethecostbuttheaccuracywilldropabit. Ontheotherhand,wefoundthatforcontextlengths
beyond100K,RAG(usingtop-5forourLlama3-ChatQA-2-70B,andtop-20forQwen2-72B-Instruct)
outperforms the full long-context solution. This indicates that even state-of-the-art long-context
LLMs may struggle to effectively understand and reason over 128K tokens. In such scenarios,
RAGisrecommendedforbetteraccuracyandlowerinferencecost,provideditisapplicabletothe
downstreamtasks.
78 Conclusion
WeintroduceLlama3-ChatQA-2-70B,alongcontextmodelthatpossessGPT-4Turbo-levelcapabili-
tiesforbothunderstandingupto128Klongcontextsandutilizingretrievedcontextsforgeneration.
Thisprovidestheflexibleoptionsfordifferentdownstreamtaskswithspecificaccuracyandefficiency
requirements. Wepresentadetailedandreproducibletechnicalrecipeforbuildingandevaluating
the model, including the methods, training data, and evaluation benchmarks. In particular, we
evaluateChatQA2onRAGandshort-contextbenchmark(ChatRAG)(within4Ktokens),medium-
contexttasksfromSCROLLSandLongBench(within32Ktokens),andlong-contexttasksfrom
InfiniteBench(beyond100Ktokens). WedemonstratethattheLlama3-ChatQA-2-70Bcanachieve
GPT-4-Turbo-2024-0409levelaccuracyonthesebenchmarks.
References
01.AI, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,
JiangchengZhu,JianqunChen,JingChang,etal. Yi: Openfoundationmodelsby01.ai. arXiv
preprintarXiv:2403.04652,2024.
Alibaba-QWen.Qwen2technicalreport.2024.URLhttps://qwenlm.github.io/blog/qwen2/.
Anthropic. Introducing 100k context windows. https://www.anthropic.com/index/
100k-context-windows,2023.
SebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMillican,
George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.
Improvinglanguagemodelsbyretrievingfromtrillionsoftokens. InICML,2022.
ShouyuanChen,ShermanWong,LiangjianChen,andYuandongTian. Extendingcontextwindowof
largelanguagemodelsviapositionalinterpolation. arXivpreprintarXiv:2306.15595,2023a.
YukangChen,ShengjuQian,HaotianTang,XinLai,ZhijianLiu,SongHan,andJiayaJia. Longlora:
Efficientfine-tuningoflong-contextlargelanguagemodels. arXivpreprintarXiv:2309.12307,
2023b.
ZheChen,WeiyunWang,HaoTian,ShenglongYe,ZhangweiGao,ErfeiCui,WenwenTong,Kongzhi
Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial
multimodalmodelswithopen-sourcesuites. arXivpreprintarXiv:2404.16821,2024.
YaoFu,RameswarPanda,XinyaoNiu,XiangYue,HannanehHajishirzi,YoonKim,andHaoPeng.
Dataengineeringforscalinglanguagemodelsto128kcontext. arXivpreprintarXiv:2402.10171,
2024.
Gemini-Team. Gemini1.5:Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext.
arXivpreprintarXiv:2310.07713,2024.
GradientAI. Scalingrotationalembeddingsforlong-contextlanguagemodels. https://gradient.
ai/blog/scaling-rotational-embeddings-for-long-context-language-models,
2024.
Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Moham-
mad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, et al. Jina
embeddings2: 8192-tokengeneral-purposetextembeddingsforlongdocuments. arXivpreprint
arXiv:2310.19923,2023.
KelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang. REALM:Retrieval
augmentedlanguagemodelpre-training. InICML,2020.
Cheng-PingHsieh, SimengSun, SamuelKriman, ShantanuAcharya, DimaRekesh, FeiJia, and
BorisGinsburg. Ruler: What’stherealcontextsizeofyourlong-contextlanguagemodels? arXiv
preprintarXiv:2404.06654,2024.
8JieHuang,WeiPing,PengXu,MohammadShoeybi,KevinChen-ChuanChang,andBryanCatanzaro.
Raven: In-contextlearningwithretrievalaugmentedencoder-decoderlanguagemodels. arXiv
preprintarXiv:2308.07922,2023.
GautierIzacardandÉdouardGrave. Leveragingpassageretrievalwithgenerativemodelsforopen
domainquestionanswering. InEACL,2021.
GautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with
retrievalaugmentedlanguagemodels. arXivpreprintarXiv:2208.03299,2022.
Kaiokendev. ThingsI’mlearningwhiletrainingSuperHOT. https://kaiokendev.github.io/
til#extending-context-to-8k,2023.
GregoryKamradt. Needleinahaystack-pressuretestingllms. https://github.com/gkamradt/
LLMTest_NeedleInAHaystack,2023.
VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,Danqi
Chen,andWen-tauYih. Densepassageretrievalforopen-domainquestionanswering. InEMNLP,
2020.
UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMikeLewis. Generalization
through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172,
2019.
ChankyuLee,RajarshiRoy,MengyaoXu,JonathanRaiman,MohammadShoeybi,BryanCatanzaro,
andWeiPing. Nv-embed: Improvedtechniquesfortrainingllmsasgeneralistembeddingmodels.
arXivpreprintarXiv:2405.17428,2024.
PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,
HeinrichKüttler,MikeLewis,Wen-tauYih,TimRocktäschel,etal. Retrieval-augmentedgenera-
tionforknowledge-intensivenlptasks. NeurIPS,2020.
ZehanLi,XinZhang,YanzhaoZhang,DingkunLong,PengjunXie,andMeishanZhang. Towards
generaltextembeddingswithmulti-stagecontrastivelearning. arXivpreprintarXiv:2308.03281,
2023.
Chin-YewLin. ROUGE:Apackageforautomaticevaluationofsummaries. InTextSummarization
BranchesOut,pp.74–81,Barcelona,Spain,July2004.AssociationforComputationalLinguistics.
URLhttps://aclanthology.org/W04-1013.
Sheng-ChiehLin,AkariAsai,MinghanLi,BarlasOguz,JimmyLin,YasharMehdad,Wen-tauYih,
andXilunChen. Howtotrainyourdragon: Diverseaugmentationtowardsgeneralizabledense
retrieval. arXivpreprintarXiv:2302.07452,2023.
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,
andPercyLiang. Lostinthemiddle: Howlanguagemodelsuselongcontexts. arXivpreprint
arXiv:2307.03172,2023a.
XiaoranLiu,HangYan,ShuoZhang,ChenxinAn,XipengQiu,andDahuaLin. Scalinglawsof
rope-basedextrapolation. arXivpreprintarXiv:2310.05209,2023b.
Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan
Catanzaro. ChatQA: Surpassing GPT-4 on conversational QA and RAG. arXiv preprint
arXiv:2401.10225,2024.
Meta-AI. Llama3modelcard. 2024. URLhttps://github.com/meta-llama/llama3/blob/
main/MODEL_CARD.md.
Mistral. Mixtral8x22b. 2024. URLhttps://mistral.ai/news/mixtral-8x22b/.
ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,LongOuyang,ChristinaKim,Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted
question-answeringwithhumanfeedback. arXivpreprintarXiv:2112.09332,2021.
9ErikNijkamp,HiroakiHayashi,TianXie,CongyingXia,BoPang,CongyingXia,andetal. Long
sequencemodelingwithXGen: A7bLLMtrainedon8kinputsequencelength. https://blog.
salesforceairesearch.com/xgen/,2023.
ZachNussbaum,JohnXMorris,BrandonDuderstadt,andAndriyMulyar. Nomicembed: Traininga
reproduciblelongcontexttextembedder. arXivpreprintarXiv:2402.01613,2024.
Nvidia, Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H Anh, Pallab Bhattacharya, Annika
Brundyn,JaredCasper,BryanCatanzaro,SharonClay,JonathanCohen,etal. Nemotron-4340b
technicalreport. arXivpreprintarXiv:2406.11704,2024.
OpenAI. GPT-4 turbo with 128k context. https://openai.com/blog/
new-models-and-developer-products-announced-at-devday,2023.
BowenPeng,JeffreyQuesnelle,HongluFan,andEnricoShippole. Yarn: Efficientcontextwindow
extensionoflargelanguagemodels. arXivpreprintarXiv:2309.00071,2023.
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel
Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and
deduplicated version of RedPajama. https://www.cerebras.net/blog/
slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama,
2023. URLhttps://huggingface.co/datasets/cerebras/SlimPajama-627B.
JianlinSu,MurtadhaAhmed,YuLu,ShengfengPan,WenBo,andYunfengLiu.Roformer:Enhanced
transformerwithrotarypositionembedding. Neurocomputing,568:127063,2024.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix, BaptisteRozière, NamanGoyal, EricHambro, FaisalAzhar, etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023a.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023b.
Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski,
andPiotrMiłos´. Focusedtransformer: Contrastivetrainingforcontextscaling. arXivpreprint
arXiv:2307.03170,2023.
Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, and Bryan
Catanzaro. InstructRetro: Instructiontuningpostretrieval-augmentedpretraining. arXivpreprint
arXiv:2310.07713,2023a.
BoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,
OleksiiKuchaiev,BoLi,ChaoweiXiao,etal. Shallwepretrainautoregressivelanguagemodels
withretrieval? acomprehensivestudy. arXivpreprintarXiv:2304.06762,2023b.
LiangWang,NanYang,XiaolongHuang,BinxingJiao,LinjunYang,DaxinJiang,RanganMajumder,
andFuruWei. Textembeddingsbyweakly-supervisedcontrastivepre-training. arXivpreprint
arXiv:2212.03533,2022.
LiangWang,NanYang,XiaolongHuang,LinjunYang,RanganMajumder,andFuruWei. Improving
textembeddingswithlargelanguagemodels. arXivpreprintarXiv:2401.00368,2023c.
WenhanXiong,JingyuLiu,IgorMolybog,HejiaZhang,PrajjwalBhargava,RuiHou,LouisMartin,
RashiRungta,KarthikAbinavSankararaman,BarlasOguz,etal. Effectivelong-contextscalingof
foundationmodels. arXivpreprintarXiv:2309.16039,2023.
PengXu,WeiPing,XianchaoWu,LawrenceMcAfee,ChenZhu,ZihanLiu,SandeepSubramanian,
EvelinaBakhturina,MohammadShoeybi,andBryanCatanzaro. Retrievalmeetslongcontext
largelanguagemodels. InICLR,2024.
YueYu,WeiPing,ZihanLiu,BoxinWang,JiaxuanYou,ChaoZhang,MohammadShoeybi,and
Bryan Catanzaro. Rankrag: Unifying context ranking with retrieval-augmented generation in
LLMs. arXive-prints,pp.arXiv–2407,2024.
10XinrongZhang,YingfaChen,ShengdingHu,ZihangXu,JunhaoChen,MooKhaiHao,XuHan,
ZhenLengThai,ShuoWang,ZhiyuanLiu,andMaosongSun. ∞bench: Extendinglongcontext
evaluationbeyond100ktokens,2024a.
XinrongZhang,YingfaChen,ShengdingHu,ZihangXu,JunhaoChen,MooKhaiHao,XuHan,
ZhenLengThai,ShuoWang,ZhiyuanLiu,andMaosongSun. ∞bench: Extendinglongcontext
evaluationbeyond100ktokens,2024b. URLhttps://arxiv.org/abs/2402.13718.
Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li,
HuazuoGao,ShirongMa,etal. Deepseek-coder-v2: Breakingthebarrierofclosed-sourcemodels
incodeintelligence. arXivpreprintarXiv:2406.11931,2024.
11