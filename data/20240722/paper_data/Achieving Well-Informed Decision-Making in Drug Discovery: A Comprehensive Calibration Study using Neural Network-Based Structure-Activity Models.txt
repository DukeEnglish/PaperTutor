ACHIEVING WELL-INFORMED DECISION-MAKING IN DRUG
DISCOVERY: A COMPREHENSIVE CALIBRATION STUDY
USING NEURAL NETWORK-BASED STRUCTURE-ACTIVITY
MODELS
HannahRosaFriesacher OlaEngkvist
ESATSTADIUS DepartmentofComputerScienceandEngineering
KULeuven ChalmersUniversityofTechnology
Leuven,3000,Belgium Gothenburg,41296,Sweden
and and
MolecularAI,DiscoverySciences,R&D MolecularAI,DiscoverySciences,R&D
AstraZenecaGothenburg AstraZenecaGothenburg
Gothenburg,43183,Sweden Gothenburg,43183,Sweden
rosa.friesacher@kuleuven.be ola.engkvist@astrazeneca.com
LewisMervin YvesMoreau
MolecularAI,DiscoverySciences,R&D ESATSTADIUS
AstraZenecaCambridge KULeuven
Cambridge,CB20AA,UK Leuven,3000,Belgium
lewis.mervin1@astrazeneca.com yves.moreau@esat.kuleuven.be
AdamArany
ESATSTADIUS
KULeuven
Leuven,3000,Belgium
adam.arany@esat.kuleuven.be
ABSTRACT
In the drug discovery process, where experiments can be costly and time-consuming, com-
putational models that predict drug-target interactions are valuable tools to accelerate the
developmentofnewtherapeuticagents. Estimatingtheuncertaintyinherentintheseneural
network predictions provides valuable information that facilitates optimal decision-making
whenriskassessmentiscrucial. However,suchmodelscanbepoorlycalibrated,whichresults
inunreliableuncertaintyestimatesthatdonotreflectthetruepredictiveuncertainty.Inthisstudy,
wecomparedifferentmetrics,includingaccuracyandcalibrationscores,usedformodelhyper-
parametertuningtoinvestigatewhichmodelselectionstrategyachieveswell-calibratedmodels.
Furthermore,weproposetouseacomputationallyefficientBayesianuncertaintyestimation
method named BayesianLinear Probing (BLP), which generatesHamiltonian Monte Carlo
(HMC)trajectoriestoobtainsamplesfortheparametersofaBayesianLogisticRegressionfitted
tothehiddenlayerofthebaselineneuralnetwork. WereportthatBLPimprovesmodelcalibra-
tionandachievestheperformanceofcommonuncertaintyquantificationmethodsbycombining
thebenefitsofuncertaintyestimationandprobabilitycalibrationmethods. Finally,weshow
thatcombiningposthoccalibrationmethodwithwell-performinguncertaintyquantification
approachescanboostmodelaccuracyandcalibration.
4202
luJ
91
]GL.sc[
1v58141.7042:viXra1 Introduction
Thedevelopmentofsafeandeffectivedrugsisachallengingtask,whichisassociatedwithhighdevelopment
costs,ahighriskofadverseeffectsorlackofefficacyleadingtothefailureofadrugcandidate,andlongapproval
processesuntiladrugcanbebroughttothemarket[1,2]. Machinelearningmodelshaveemergedasavaluable
tool,revolutionizingthedrugdiscoveryanddevelopmentprocessbyshiftingtoamoretime-andresource-efficient
pipeline[3,4,5].
Asaconsequenceoftheincreasingavailabilityofcomputationalresourcesanddata,recentmachinelearning
modelsperformwellinpredictiontasks,whichisreflectedinhighaccuracyscoresandlowclassificationerrors.
Estimatingtheuncertaintyinherenttosuchapredictioncanprovideavaluablesourceofinformationinvarious
applicationsbesidesdrugdesign[6,7,8,9,10,11,12]. Moreover,accurateuncertaintyestimatescanbeleveraged
toimprovedecisionsaboutwhichcandidatestopursueacrossacandidateportfolio.
Evenwhenpredictionaccuracyisstrong,neuralnetworksoftenfailtogiverealisticestimatesofhowuncertain
theyareaboutaprediction.Thesemodelsarecalledpoorlycalibrated,whichimpliesthatthepredictiveuncertainty
doesnotreflectthetrueprobabilityofmakingapredictionerror. However,thereliabilityofuncertaintyestimates
iscrucialtoguaranteethereliabilityofmachinelearningmodels. Thisisparticularlyimportantforhigh-stakes
decisionprocesseslikethedrugdiscoverypipelinewhereexperimentscanbecostlyandpoordecisionsinevitably
leadtoanincreaseinrequiredtimeandresources.
Predictiveuncertaintycancomefromvarioussources. Whilemanydifferentcategorizationsofthesesources
canbefoundinliterature,acommononeisthedistinctionbetweenaleatoricandepistemicuncertainty[13,14].
Aleatoric or data uncertainty is the uncertainty related to data and data acquisition, including systematic and
unsystematicerrors,suchasmeasurementerrors. Aleatoricuncertaintyisalsooftencalledirreducibleuncertainty,
asitcannotbedecreasedbyaddingmoredatasamplestothecurrentmodel. Bycontrast,epistemic,ormodel
uncertaintycanbereducedbyaddingknowledge. Epistemicuncertaintycanhaveseveralcauses,includingmodel
overfittingordistributionshiftsbetweentrainingandtestdata.
Inclassification,themodeloutputisusuallyaprobability-likescore,reflectingtheuncertaintyofaprediction,if
thenetworkiswellcalibrated. Thepredictiveuncertaintyshouldsummarizethetotaluncertaintyassociatedwith
theprediction,consideringallsourcesofuncertainty. However,theseprobabilitieshavebeenreportedtodiverge
fromtheirgroundtruthpreventingareliableriskassessment[15,16]. In2017,Guoetal.[15]drewattentiontothe
lackinabilityofmodernneuralnetworkstoestimateuncertaintiesofpredictions. Theyreportedthatdespitetheir
highaccuracy,largeneuralnetworksarepoorlycalibrated,resultingininaccurateprobabilityestimates.
Intheirpaper,Guoandhiscolleagueslinkedpoorprobabilitycalibrationtomodeloverfitting,leadingtoincreased
probabilisticerrorsratherthanaffectingthemodel’sabilitytocorrectlyclassifytestinstances. Furthermore,they
concludedthatmodelcalibrationandmodelaccuracyarealsolikelytobeoptimizedbydifferenthyperparameter
settings[15]. Wangliststhreemajorfactorsdiminishingtheprobabilitycalibrationofamodel,includinglarge
modelsizeandover-parametrizationofmodels,lackofmodelregularizationanddataqualityandquantity,aswell
asimbalancedlabeldistributioninclassification[17]. Inaddition,thedistributionoftrainingandtestdatawas
reportedtoimpactmodelcalibration. Acalibratedmodelwillbemoreuncertainthemorethedistributionofthe
testinstancesdivergesfromthedistributionofthetrainingdata. Currentneuralnetworksareoftenoverconfident,
sothatprobabilitycalibrationdeteriorateswithincreasingdistributionshift[18,19].
Thisisparticularlyproblematicwhendevelopingnewtherapeuticagents,whichrequiresexploringthechemical
spacebyshiftingthefocusduringinferencetochemicalstructuresthatarenewandunknowntothemodel. As
a consequence, there is a pressing need for methods that can reliably support the drug discovery process by
estimatingthetrueriskassociatedwithadecision.
This paper focuses specifically on drug-target interaction modeling to explore the effects of different model
selectionstrategiesanduncertaintyestimationapproachestomodelcalibration. Toourknowledge,thereisno
studyinvestigatingtheimpactofdifferenthyperparameter(HP)optimizationmetricsonthecalibrationproperties
of bioactivity prediction models, and we are aiming to close this gap by contributing an analysis of how to
trainmodelswhenaimingforgooduncertaintyestimates. Furthermore,wecomparetheuncalibratedbaseline
modelwiththreecommoncalibrationmethodsandweproposealimitedcomputationalcomplexityBayesian
approach,whichallowstheretrievalofsamplesfromtheposteriordistributionofthelastlayerweights. Finally,
weinvestigateifcombiningtheposthoccalibrationapproachPlattscalingwithotheruncertaintyquantification
methodsbenefitsmodelcalibration.
21.1 RelatedWorkandBackground
1.1.1 PosthocCalibrationMethods
Plattscaling. Since1999,Plattscaling[20]hasbeenwidelyusedforcalibratingprobabilities[15,17,16]. Itis
isaparametriccalibrationmethodthatfitsalogisticregressionmodeltothelogitsofthemodelpredictionsto
counteractover-orunderconfidentmodelpredictions. Usually,aseparatedataset,calledcalibrationdataset,is
usedforthiscalibrationstep. SincePlattscalingisaposthoccalibrationmethod,itisversatileandcanbeusedin
combinationwithotheruncertaintyquantificationtechniques,includingBayesianapproaches.
1.1.2 Calibration-FreeUncertaintyQuantificationMethods
Incontrasttoposthocscalingmethods,thefollowinguncertaintyquantificationapproachestakeacalibration-free
approachtoestimatepredictiveuncertainty. Themainideaofthesetechniquesistoaccountforuncertaintyin
themodelbytreatingthemodelparametersasrandomvariableswithassociatedprobabilitydistributions. Bayes’
theoremallowsaccesstotheseposteriordistributionsp(θ|D)overmodelparametersθ. Subsequently,aposterior
distributionofthepredictedlabelcorrespondingtothetestinstancexcanbederivedbymarginalizingoverθ:
(cid:90)
p(y|x,D)= p(y|x,θ)p(θ|D)dθ. (1)
θ
However,themodelposteriordistributionsareusuallycomplexandtheiranalyticalformisoftennotavailable
because of intractable marginal likelihood terms needed to solve the Bayesian inference. The majority of
the uncertainty quantification approaches are Bayesian or apply heuristics motivated by Bayesian statistical
principles. Theseincludevarioussampling-basedapproachesthatdrawsamplesθ ∼p(θ|D)fromthiscomplex
i
posteriordistribution. Anuncertaintyestimateforatestinstancecanbeobtainedbyaveragingoverthesamples
p(y|x,D) ≈ 1 (cid:80)M p(y|x,θ ). The following sections provide a short introduction to the uncertainty
M m=0 m
quantificationmethodsusedinthisstudy. Inaddition,anoverviewoftheapproachesisalsoprovidedinFigure3.
MonteCarlodropout.In2016,GalandhiscolleaguesintroducedMonteCarlo(MC)dropoutasanapproximation
toBayesianinference[21]. InMCdropout, stochasticityisintroducedbyapplyingdropoutduringinference.
Samplesaregeneratedbyperformingmultipleforwardpasses,ineachofwhichanewsetofrandomlyselected
neuronsissettozero. Subsequently,thesamplesareaveragedtoobtainanuncertaintyestimateforatestinstance.
SinceforMCdropout,thetrainingofonlyonemodelisnecessary,thiscalibrationmethodisefficientintermsof
computationalcostandtime.
Deepensembles. Anotherapproachthathasbeenshowntoproducewell-calibratedpredictionsisthegeneration
ofdeepensembles[22]. Multiplebaseestimatorsaretrained,startingfromdifferentweightinitializationofthe
network. Itisassumedthatbecauseofthestrongnon-convexnatureoftheerrorlandscape,mostofthesemodels
reachdifferentlocalminima. Thebaseestimatorsareusedtoobtainpredictions,whicharesubsequentlyaveraged
toretrieveasingleprobability. EnsemblescanbeinterpretedasheuristicapproximationsofaBayesianprocedure.
Thefoundminimacorrespondtodifferentmodesoftheposterior. Itisthereforeexpectedthatsuchsetsofbase
estimatorsrepresentthemostimportantregionsoftheposterior. Deepensemblesareeasytoimplement. Theycan
howeverbecomputationallyexpensiveastheyinvolvethegenerationofmultiplemodels.
SamplingwithHamiltonianMonteCarlo. HamiltonianMonteCarlo(HMC)isaMarkovChainMonteCarlo
(MCMC)method,whichallowsdrawingsamplesdirectlyfromtheposteriordistributionoftheparameters[23].
MCMCmethodsgeneratesamplesbyconstructingaMarkovchaininwhichtheproposaldistributionofthenext
sampledependsonthecurrentsample. WhencomparingittootherMCMCmethodsthatusearandomwalk
approach,HMCstandsoutbecauseofitsabilitytoproposenewsamplesinaninformedway. TheHMCsampler
usesHamiltonianDynamicstoefficientlymovethroughthenegativelogspaceoftheunnormalizedposterior
by following Hamiltonian trajectories. Simply put, the sampling procedure can be intuitively imagined as a
particleslidingalongthespace. Thisparticleisstoppedaftersometimetorecordthecurrentstateasasampleof
theMarkovChain. TheparticlemovesalongspecifictrajectoriesobtainedbynumericallysolvingHamilton’s
equation. Toaccountforaccumulatederror,anadditionalMetropolis-Hastingsstepisrequiredafterdrawingthe
sample,inwhicherroneoussamplescanberejected. Foramoredetailedexplanationofthemathematicaland
physicaldetailsofHMCwereferto[23,24].
Becauseitsinformedapproachtoproposingnewsamples,HMCisgenerallybetteratgeneratingwell-mixing
chainsthanmethodsusingrandomwalktechniques. Furthermore,themixingabilityofthechainwilldependon
thelengthofthetrajectorydeterminedbythenumberofstepsLandthestepsizeϵ. Ifachainismixingpoorly,
3thechainwillgetstuckinoneareaofthenegativelogprobabilityspace,resultinginhighlycorrelatedsamples.
Ifthisisthecase,thetrajectorycanbelengthenedbyeitherincreasingϵorL. However,thesehyperparameters
needtobetunedcarefully,sincehighϵcanleadtoincreasedrejectionoftheproposedsamplesbecauseoflarger
accumulatederror,andtheincreaseofLisoftenconnectedtoproblematiccomputationalcosts.
Becauseofitshighcomputationaldemand,theapplicationofHMCtoafullBayesianneuralnetworkischallenging.
In2021,Izmailovetal.[25]generatedtrulyBayesianneuralnetworksbytrainingmodernarchitecturesusing
full-batchHMC.DespitegivinghighlyinterestinginsightsintothenatureofBayesianneuralnetworks,theauthors
concludedthatHMCisanimpracticalmethodbecauseofthehighcomputationaldemand.
In our work, we propose to use HMC in a computationally-feasible way by sampling only from the weight
posteriorofthelastlayer. WecallourmethodBayesianLinearProbing(BLP),whichfitsaBayesianlogistic
regressiontothelasthiddenlayerofthenetwork. Thenamereferstotheanalogywithlinearclassifierprobes
proposedbyAlainandBengio[26].
2 Methods
2.1 Datasets
Table1: SummaryoftheassaydataextractedfromtheChEMBLdataset. Datafromthreeassaysofvaryingsizes
andpositiveratioswereextracted.
ChEMBL-IDs Target #Results ActiveRatio pIC50Threshold
CHEMBL1951 MonoamineoxidaseA 2917 0.259170 5.5
CHEMBL340 CytochromeP4503A4 7619 0.252658 5.5
CHEMBL240 hERG 9558 0.079828 6.5
Extraction of target specific data from ChEMBL. Target-specific bioactivity data was extracted from the
ChEMBLdatabase(version: 29)[27]. Togeneratesingle-taskmodels,compoundactivitiesfromthreedifferent
targets,namelyMonoamineoxidaseA(MAOA),CytochromeP4503A4(CYP3A4),andhERG,wereextracted.
Table1summarizesthepropertiesoftheusedtargetdata. BioactivitieswereconvertedtopIC50,andthresholds
forassigningbioactivitylabelswerechosenforeachtarget, respectively. ForMAOAandCYP3A4, apIC50
valueof5.5waschosenasthreshold,whichresultedinactiveratiosofapproximately25%forbothtargets. To
investigateifourconclusionswerealsovalidforsmalleractiveratios,wechoseastricterthresholdof6.5pIC50
fortheremainingtargethERGleadingtoanactiveratioof7%inthisdataset. Extendedconnectivityfingerprints
(ECFPs)(size=32k,radius=3),wereobtainedusingRDKitVersion2022.03.2[28].
Foldgenerationviaclustering. Thedatawassplitintofivedifferentfoldstoenablecross-validation. Figure1
illustratesthegenerationanduseofthedatasetsplits. Thevalidationfoldusedforthemodel´squalityassessment
duringhyperparametertuningandforearlystoppingwasexcludedfromthetrainingdataset. Toobtainthefolds,
weusedtheprocedureoffoldgenerationdescribedindetailinSimmetal.[29]. Inshort,Tanimotosimilarity
computedontheabove-describedECFPfeaturesisusedtomeasurethechemicalsimilarityofthecompound,
whichisthenusedtoassignthecompoundstoclusters. Next,theentireclusterscontainingsimilarcompounds
wererandomlyassignedtofolds. Thisprocedureensuresthattrainingandtestdatasetsconsistofcompoundsfrom
divergentchemicalspace,mimickingthereal-worldscenario,inwhichthemodelisusedtopredictbioactivities
forchemicallyunfamiliarcompounds. Testingthemodeloncompoundstothoseonwhichitwastrainedwould
resultinoveroptimisticresultsduringmodelperformanceassessment.
2.2 Single-TaskModelGeneration
Modelarchitecture. ModelswereimplementedandtrainedusingPyTorchVersion2.1.0[30]. Theopen-source
hamiltorchpackage[31]wasusedtoobtainMLP-BLPmodelsusingaHMCsampler[32]. Single-taskMLPs
weregeneratedandusedasthebaseline. Subsequently,thebaselinemodelswereextendedtocomparedifferent
calibration approaches. Figure 2 illustrates the architecture of the baseline MLPs and the HP optimization
workflow. Thebaselinemodel(MLP)iscomprisedoftwolayers,withaReLUfunctionandadropoutlayerin
between. Probability-likescoreswereobtainedbyapplyingasigmoidfunction. Thesizeofthehiddenlayerand
thedropoutrateweretunedinagridsearchasdescribedinthenextsection.
Modeltuning. Avalidationdatasetwasusedforearlystoppingduringtrainingandtooptimizethehyperparam-
eters(HPs)ofthemodels. Anexhaustivegridsearchinparameterspacewasperformedtotunethesizeofthe
4Figure 1: Overview of the dataset generation. The chemical structures were extracted from ChEMBL, and
subsequently filtered and clustered. The clusters were assigned to five folds, which were used to set up a
training,validation,andtestfold. ThetrainingfoldswereusedforMLPtraining. Thevalidationsetwasusedfor
hyperparametertuning,aswellasforfittingthelogisticregressionmodelsforthedeepensemblemodel(MLP-E),
andtochoosethepriorforBayesianLinearProbingmodel(MLP-BLP),respectively.
hiddenlayer,andthedropoutrateofthemodel,aswellasthelearningrateandtheweightdecayusedduring
modeltrainingasshowninFigure2. Toensurerepeatability,theHPmetricwasaveragedovertenrepeatsforeach
HPsetting. Binarycrossentropyloss(BCEloss),adaptivecalibrationerror(ACE),accuracy,andareaunderthe
ROCCurve(AUC)wereusedformodelselectiontoassesstheimpactonprobabilitycalibration.
Modelassessmentandevaluation. Allperformancemetricswerecalculatedfrompredictionsonatestdataset
foreachmodelandprobabilitycalibrationmethod. AUC↑scoreswereobtainedtoassessthemodel´sabilityto
correctlyclassifysamples. Inaddition,weassessedwhetherthegeneratedmodelswerecapableofproducing
calibratedprobabilitypredictions. WeusedBCEloss↓,theBrierscore(BS)↓,andtwotypesofcalibrationerrors
(CEs)↓tomeasuretheprobabilitycalibrationofthemodels. TheBrierscore(BS)[33]measurestheperformance
ofamodelbyobtainingthemeansquarederrorbetweenthepredictedprobabilitiesyˆandthetruelabelsy:
N
1 (cid:88)
BS = (yˆ−y)2. (2)
N
n=1
When decomposing the BS, it can be shown that different metrics contribute to the final score, including an
AUC-relatedterm(Refinement)andcalibration-relatedterm(Reliability)[34,35]. Theexpectedcalibrationerror
(ECE)↓iscommonlyusedinliteraturetoassessifamodeliscalibrated. Inaddition,wealsousedtheadaptive
calibrationerror(ACE)↓,whichhassomedesirablepropertiesmakingitmorerobusttowardsskeweddistributions
of the predictions, as described below. Both CE types estimate the true calibration error by discretizing the
probabilityintervalofthepredictionsintobinsandtakingaweightedaverageoftheerrorsoverallbins[36,37].
Forbinaryclassificationtasks,theerrorineachbinbisobtainedbycalculatingtheabsolutedifferencebetween
themeanoftheprobabilitypredictions(confidence)andtheratioofobservedpositivesamplesplacedinthisbin
(accuracy). Theresultingcalibrationerroristheaverageofthiserroroverallbins,weightedbythebinsizen :
b
B
1 (cid:88)
CE = n |Accuracy(b)−Confidence(b)|. (3)
N b
b=1
TheECEandACEdifferonlyinthewayinwhichthebinsareformed. WhilefortheECE,theprobabilityinterval
isdividedintoequally-spacedbinsofafixedwidth,theACEformsbinswiththesamenumberofsampledin
eachbin[37]. Ingeneral,theACEisconsideredmorerobust,sincetheconstantbinsizepreventssamplesfrom
contributingmoretotheerrorthanothers. Incontrast,thisbehaviorcanbedetectedintheECEasaresultof
thefixedbinningleadingtodifferentlypopulatedbinsandanincreasedvarianceoftheerrorestimateinbins
withfewersamples. Note,thattheCEsusedinthispaperareimproperscoringrules[38]inthesensethatthe
predictionswithzerocalibrationerrorarenotnecessarilygoodpredictions. Amodelthatalwayspredictsthe
overallratioofclassinstancesinthedatasetwillbeperfectlycalibrated,despiteitspooraccuracy. Contrarytothe
calibrationerrorsusedinthispaper,theBCElossandtheBSareproperscoringrules[38],whichmeansthatthe
resultswiththebestscorewillalsocorrespondtothebestprediction.
52.3 Experiments
Forthesakeofrepeatabilityandestimationofthestandarddeviationofthepredictions,wegeneratedtenrepeats
foreachmodeltypeandaveragedtheresultingrepeats. Notethat,forcomputationalreasons,onlyfiverepeats
weregeneratedforensemblemodels,resultingin250trainingsessionspertarget. Thestatisticalsignificanceof
thebestresultswastestedineachexperimentbyperformingatwo-sidedt-testwithathresholdofp=0.05. Paired
t-testswereusedwhencomparingthebaselinemodelwithmodificationsofthisspecificmodelorbetweenthese
modifications,includingMLPplusPlattscaling(MLP+P),MCdropout(MLP-D),MLPplusBayesianLinear
Probing(MLP-BLP)andMLP-BLPplusPlattscaling(MLP-BLP+P).Inallothercases,anunpairedt-testwas
used.
Modelselectionstudy. Sinceitisassumedthatmodeloverfittingaffectsprobabilitycalibration,weassessed
theimpactofdifferentHPoptimizationmetrics(HP-metrics)onmodelcalibration. Todoso,wecomparedthe
calibrationerrorsofmodelswithHPseithermaximizingaccuracyortheAUCvalueorminimizingtheBCEloss
ortheACE.WeassessedhowtheAUC,theCEs,andtheBSwereaffectedbyvaryingHPmetrics. Furthermore,
wecomparediftheresultsofthisanalysisvaryacrosstargets.
Figure2: OverviewofthearchitectureoftheMLPbaselinemodelandtheHPtuningworkflow. Thesizeofthe
hiddenlayerandthedropoutrate,aswellastheweightdecayandlearningrateusedduringtraining,weretuned
inagridsearchusingavalidationdataset. FourdifferentHPoptimizationmetrics(HPmetrics)wereusedandthe
performancesoftherespectivemodelswerecomparedinamodelselectionstudy.
Model calibration study. We studied the ability of four uncertainty estimation approaches to achieve better
uncertaintyestimates. Figure3givesanoverviewofthemethodsassessedinthisexperiment. Inallcases,we
buildonanuncalibratedfullyconnectednetwork,whichwewillrefertoasthebaselinemodelinthispaper. For
theposthoccalibrationmethod,Plattscaling,thevalidationdatasetwasusedtofitalogisticregressiontothe
generatedscores. Inthefollowingsections,wewillrefertothismodelasMLP+P.Moreover,weassessedtwo
uncertaintyestimationapproaches: ensemblemodels(MLP-E)andMCdropout(MLP-D).Forthegenerationof
MLP-Emodels,50baseestimatorsweretrainedwithrandominitialization,whereasfortheMLP-Dapproach,
100predictionsweregeneratedusingdropoutduringtheforwardpasses. Inbothapproaches,thepredictionswere
averagedtoobtainapredictionforatestinstance. Finally,weusedourproposedmethodBayesianLinearProbing
(MLP-BLP),byremovingthelastlayerofthebaselineMLPandreplacingitbyaBayesianLogisticRegression
model. Theparametersforthelogisticregressionmodelweresampledfromtheirtrueposteriordistributionusing
HamiltonianMonteCarlo(HMC).NotethatsimilarlytoMLP-EandMLP-DthetrainingoftheBLPmodelis
carriedoutonthetrainingsetanddoesnotuseacalibrationset. However,thevalidationsetwasusedtotune
theprecisionoftheGaussianpriorofthemodelweights. Theselectionofthissinglescalarparameterhasan
analogouseffectasregularizationandpositionsthemethodbetweenthetwomaintypesofmethodsdiscussedso
far. Again,CEs,BS,andAUCscoreswereusedtocomparetheperformanceacrosscalibrationapproachesand
targets.
Asasecondstep,posthoccalibrationanduncertaintyquantificationmethodswerecombinedtoassesswhether
themodel’sprobabilitycalibrationwouldbenefitfromfirstquantifyingtheuncertaintyinherentinthepredictions
andsubsequentlycalibratingtheuncertaintyestimates. Thearchitectureofthecombinedmodelsisillustrated
6Figure3: Overviewofmodelsandprobabilitycalibrationapproachesassessedinthemodelcalibrationstudy. The
baselinemodel(MLP)wascomparedtotheposthoccalibrationmethodPlattscaling(MLP+P)andtheBayesian
approachesMCdropout(MLP-D)anddeepensembles(MLP-E).Furthermore,thenewlyproposedBayesian
approachBayesianLinearProbing(MLP-BLP)wasincludedintheanalysis. Themodelsweretrainedonthe
trainingdataset. Fortheposthoccalibrationapproach(Plattscaling),thevalidationdatasetwasusedtofitthe
logisticregressionmodel.
inFigure4. WeappliedPlattscalingtotheMLP-EandMLP-BLPmodel,byfittingasigmoidfunctiontothe
logitscoresofthepredictions,whichresultedinthePlatt-scaleduncertaintyquantificationmodelsMLP-E+P
andMLP-BLP+P.SincePlattscalingdoesnotaffecttheAUCscoresofthepredictions,onlyCEsandBSwere
calculatedtocomparethemodels’performancewiththeircalibration-freecounterparts.
Figure 4: Architecture of the combined models MLP-E + P and MLP-BLP + P. For generating Platt-scaled
uncertaintyquantificationmethods,asigmoidwasfittothelogitsofthedeepensemble(MLP-E)andBayesian
LinearProbing(MLP-BLP)model. Forthecalibrationstep,anadditionalcalibrationdatasetwasused.
73 ResultsandDiscussion
Reliableuncertaintyestimatesarecrucialforassessingthecostsandbenefitsofexperimentsinthedrugdiscovery
process. Theycansupporttheidentificationofcompoundsthataremorelikelytobeactiveagainstatargetof
interestandonwhichfurtherexperimentalanalysisshouldbefocused. Inthefollowingsectionsweaddressthis
issuebycomparingvariousHPtuningstrategiesandprobabilitycalibrationapproachestoidentifypracticesthat
allowthegenerationofbetter-calibratedmachinelearningmodels.
3.1 ModelSelectionStudy
Figure5: ResultsofthemodelselectionstudyfortargetCYP3A4. Theperformanceoftenmodelrepetitionsis
shown. [A]Comparisonofthecalibrationerrorsofmodelsoptimizingaccuracy(ACC),AUCROCscore(AUC),
BCEloss,andtheexpected(ECE)andadaptive(ACE)calibrationerrors. [B]ACEvs. AUCofmodelstuned
usingdifferentHPmetrics. Modelsintheleftuppercorner(correspondingtohighAUCandlowACE)perform
best.
WeinvestigatedthecalibrationofbaselinemodelswithfourdifferentHPsettings,eachofwhichwasselectedto
optimizeaccuracy,AUC,BCEloss,andACE.
Calibrationofmodelselectionstrategies: exampleCYP3A4. Figure5[A]showstheECEandACEoften
modelrepetitionscomputedonatestsetforCYP3A4. ThefigureshowsthatthemodelsforwhichBCElossand
ACEwerechosenasHPmetricsperformedbetterintermsofCEthanthosetunedonaccuracyorAUCvalues.
Overall,theACEwassmallerthantheECEforallmodels,whichcouldresultfromthehighvarianceofthemean
predictionsinlesspopulatedbinscontributingconsiderablymoretotheECEthanotherbinsandleadingtoan
overestimationoftheCE.TheperformanceoftheindividualmodelsontheCYP3A4datasetshowninTable2
illustratesthatthemodeloptimizingtheACEandBCEonavalidationsetperformsbestintermsofCEswhile
optimizationbasedonAUCvaluesleadstotheworstresults. Sincegoodcalibrationdoesnotautomaticallyimply
goodclassifyingandrankingabilitiesofamodel,wealsocalculatedtheAUCvaluesforthetestsetpredictions.
TheresultsareplottedinFigure5[B].Again,themodelsoptimizedbasedonBCElossandACEyieldedthebest
AUCvalueswiththeformerbeingsignificantlybetter(p=0.006)asshowninTable2. Inaddition,theBSwas
calculatedforallmodelpredictions,whichsummarizestheperformanceofboth,thecalibrationofthemodel
aswellasitsabilitytocorrectlyrankpredictions. SinceBCElossandACEasHPmetricsperformedbestwith
respecttoCEsandAUC,thesetwometricsalsoresultedinthelowestBS,asexpected,withBCElossperforming
significantlybetter(p=0.0171).
Calibrationofmodelselectionstrategiesacrosstargets. Table2liststheresultsofthemodelselectionstudy
foralltargets. TheoveralltrendsdetectedintheanalysisoftheCYP3A4datasetcouldbeobservedwiththe
othertwotargets. Ingeneral,optimizingtheHPswithregardtoBCElossandACEresultedinmorecalibrated
modelsthanoptimizationbasedonaccuracyorAUC.Indetail,optimizingACEwasalwaysthebestchoicewhen
lookingattheCEsofthetargetswithhigheractiveratios. WhenlookingatthehERGdata,whichexhibitsamuch
8smalleractiveratio,HPsselectedtominimizeBCElossresultedinthesignificantlybestCEs. SincebothCEsare
improperscoresandcangivegoodresultsforhighlyinaccuratemodels,itisadvisabletoalsoconsiderproper
scoreswhenanalyzingmodelcalibration. TheresultsinTable2showthattheBSsandtheAUCvaluessupport
theoutcomesobtainedintheCEanalysis,favoringHPselectionstrategiesbasedonBCElossorACEratherthan
accuracyorAUC.TheresultsforMAOAshowedthatoptimizationwithACEleadstosignificantlybetterresults
(p<0.001forallmetrics),whiletheresultsforhERGreportedBCElosstobesignificantlybetterintermsofall
metrics.
TheresultsofthisanalysiscouldbeexplainedbyreducedmodeloverfittingwhenchoosingACEorBCElossas
HPmetric. SinceaccuracyandAUCscoresdonotaccountforprobabilitycalibration,modelstunedtooptimize
thesescoresonthevalidationdatasetsareexpectedtoperformworseintermsofmetricsthatincludeprobability
calibration. Surprisingly,modelstunedtooptimizeAUCvalueonavalidationdatasetalsoshowedworseAUC
performanceonthetestsetwithHPsoptimizingBCElossorACE.
Insummary,modelswithHPsselectedtooptimizeBCElossandACEshowedsimilarresultsacrossthemajority
ofscoresandHPmetrics.Inthesubsequentsectionsofthispaper,wewillfocusonmodelswithHPsthatminimize
BCEloss,sincethesemodelsachievedthebestBSintwotargetsandthesecond-bestBSfortheMAOAdataset.
Table2: Resultsofthemodelselectionstudyforalltargets. CEs,BSs,andAUCvaluesareshownacrosstargets
forallHPtuningstrategies. Resultsareaveragedovertenmodelrepetitions,exceptforthedeepensemblemodels,
forwhichfivemodelrepeatswerecomputed. Foreachperformancemetric, theresultsofthebestmodelare
boldandunderlined. Allotherboldresultsarestatisticallyindistinguishablefromthebestresultasreportedina
two-sidedt-test(p<0.05)
.
Target HPMetric ECE ACE BS AUC
CYP3A4 ACC 0.1488±0.0031 0.1435±0.0036 0.1759±0.0015 0.7798±0.0055
(ChEMBL240) AUC 0.1799±0.0111 0.1709±0.0121 0.1951±0.0103 0.7682±0.0087
BCEloss 0.0698±0.0056 0.0663±0.005 0.1506±0.0014 0.7975±0.0028
ACE 0.068±0.0062 0.0635±0.0073 0.1548±0.0039 0.7872±0.0072
MAO-A ACC 0.2207±0.0232 0.2136±0.0252 0.2391±0.0224 0.7082±0.0287
(ChEMBL1951) AUC 0.2379±0.0185 0.2281±0.017 0.252±0.016 0.695±0.0134
BCEloss 0.1696±0.0116 0.1663±0.0122 0.212±0.006 0.7219±0.0057
ACE 0.0999±0.0076 0.0962±0.009 0.1808±0.0021 0.7461±0.0035
hERG ACC 0.1028±0.0991 0.1019±0.0997 0.0792±0.0324 0.6928±0.0806
(ChEMBL340) AUC 0.0763±0.0061 0.0721±0.0085 0.079±0.0052 0.761±0.0139
BCEloss 0.0289±0.0079 0.0254±0.0063 0.0541±0.0018 0.8061±0.0031
ACE 0.0328±0.0109 0.0317±0.0111 0.0586±0.0033 0.7742±0.0348
3.2 ModelCalibrationStudy
Wecomparedthreecommonprobabilitycalibrationapproaches,includingtheposthocmethodPlattscalingand
thecalibration-freeuncertaintyestimationtechniquesdeepensemblesandMCdropout. Forthesakeofaclear
andstraightforwardcomparison,weonlyconsideredmodelswithHPsselectedtominimizetheBCElossinthis
probabilitycalibrationstudy. TheresultsofmodelsoptimizingotherHPmetricsarelistedintheappendix.
Performanceofuncertaintyestimationstrategies:examplehERG.Figure6depictstheresultsofthecalibration
studyforhERG.Tocomparecalibrationandrankingabilitiesacrossthemodels,AUCvaluesareplottedagainst
ACEforeverycalibrationapproach. Interestingly,onlyPlattscalingandourproposedmethodBLPachievebetter
calibrationthanthebaselinewithBLPsignificantlyoutperformingallothermodelsintermsofECE(p<0.001)
andACE(p<0.001). TheAUCvalueoftheBLPmodelwasslightlyworsethanthebaselinewithadifferenceof
about0.07. Whiletheensembleapproachdidnotachievealowercalibrationerror,itledtoaslightincreasein
AUC.
Performance of uncertainty estimation strategies across targets. Table 7 lists the results for every target
acrossallprobabilitycalibrationmethods. ApartfromMCdropout(MLP-D),allcalibrationmethodsachieve
lowerCEsthanthebaselinemodelforCYP3A4andMAOA.ForhERG,themoreunbalancedtargetintermsof
activeratiointhedataset,similarresultswereobtained,exceptfortheensemblemodel,whichdidnotimprove
probability calibration in this specific case. The analysis of the AUC scores showed only minor differences
betweenthecalibrationmethodsacrossalltargets. Basedontheseresults,wecanconcludethattheanalyzed
methodsimprovetheprobabilitycalibrationofthebaselinemodelwhilehavinglimitedimpactontheaccuracyof
9Figure6: ResultsoftheprobabilitycalibrationstudyfortargethERG.Comparisonofthebaselinemodel(MLP)
withthreeprobabilitycalibrationmethods,includingPlattscaling(MLP+P),ensemblemodeling(MLP-E),and
Bayesianlinearprobing(MLP-BLP).Resultswereaveragedovertenmodelrepeats.
therankingabilitiesofthemodel. Inthiscontext,interestingfindingswerereportedbyRothandBajorath,who
investigatedtherelationshipbetweenaccuracyandcalibrationinavarietyofclassificationmodels[39]. Similar
toourconclusions,theauthorsreportedthatdespitetheirlargedifferencesinthecalibratingperformance,the
modelsoverallproducedstableandaccuratepredictions.
Whencomparingtheindividualprobabilitycalibrationapproaches,Plattscaling(MLP+P)andBayesianLinear
Probing(MLP-BLP)aremostpowerfulintermsofprobabilitycalibrationasshowninTable7,withthelatter
resultinginthebestcalibrationerrorfortheMAOAdatasetandhERG.ForthehERGdataset,MLP-BLPperformed
significantlybetterthanallotherapproaches. BayesianLinearProbing(MLP-BLP)consistentlyoutperformedall
othercalibration-freeuncertaintyestimationapproachesacrossallperformancemetrics,exceptforAUC,where
theensemblemodel(MLP-E)achievedthebestresultsintwotargets.
TheseresultsshowthatBLPreachestheCEperformanceofstate-of-the-artuncertaintyquantificationmethods
and is only outperformed in one dataset by Platt scaling. Furthermore, common calibration-free uncertainty
quantificationmethods,suchasdeepensemblesorMCdropout,donotreachtheperformanceofBLPorPlatt
scalingintermsofCEsandBS.ApossibleexplanationforthismightbethattherathersimpleMLParchitecture
leadstoalesscomplexlosslandscapeandfewerlocalminimaresultinginsimilarbaseestimatorsoftheensemble
model. Furthermore, deep ensembles and MC Dropout were reported to lead to less confident predictions,
anddonotnecessarilyimproveprobabilitycalibration[40]. Asaconsequence,wehypothesizethatensembling
techniquesareonlybeneficialifthebaseestimatorsareoverconfident,whiletheyareineffectiveforunderconfident
orwell-calibratedmodelsandcanevenimpairthequalityoftheuncertaintyestimates. Giventhatthemodelsused
inthisstudywererathershallowandtheircalibrationpropertieswereprioritizedduringhyperparametertuning,
theresultingbaseestimatorsmightnotsufferfromoverconfidenceinthesameseverityasitisknownforlarger
deepneuralnetworks.
Posthoccalibrationofuncertaintyquantificationmethods. Plattscalingisaposthoccalibrationmethod,
whichmakesitversatileasitcanbeappliedtoanymodelaftertraining. WecombinedPlattscalingwithMLP-E
andMLP-BLPtoassessifcalibratinguncertaintyestimatesobtainedfromensemblemodelingorBayesianLinear
Probingenhancesmodelcalibration. TheresultsforMLP-E+PandMLP-BLP+PareshowninTable4. Since
Plattscalingdoesnotchangetherankingofthepredictions,andthereforedoesnotaffecttheAUCscores,onlyCEs
andBSarereported. Ingeneral,thePlatt-scaledmodelsMLP+P,MLP-E+PandMLP-BLP+Poutperformed
allcalibration-freeapproachesacrossalltargets. TheonlymodelthatmatchedtheperformanceofthePlatt-scaled
approacheswastheMLP-BLPmodel,whichwasonlysignificantlyoutperformedbyitsPlatt-scaledcounterpart
intermsofCEsontheMAOAdataset. Thecombinedmethodswerereportedtosignificantlyimprovecalibration
onlyinoneofthethreeassays(MAOA).Inthiscase,themodifiedBayesianLinearProbingmodelMLP-BLP
+ P was best calibrated while all other models performing significantly worse in terms of CEs. In addition,
10Table3: Resultsoftheprobabilitycalibrationstudyforalltargets. CEs,BSsandAUCvaluesareshownacross
targetsforallprobabilitycalibrationmethods. Resultsareaveragedovertenmodelrepetitions,exceptforthe
deepensemblemodels,forwhichfivemodelrepeatswerecomputed. Theresultsofthebestmodelareboldand
underlinedforeachperformancemetricacrossallmodels. Allotherboldresultsarestatisticallyindistinguishable
fromthebestresultasreportedinatwo-sidedt-test(p<0.05).
Target Model ECE ACE BS AUC
CYP3A4 MLP 0.0698±0.0056 0.0663±0.005 0.1506±0.0014 0.7975±0.0028
(ChEMBL240) MLP+P 0.0373±0.0036 0.039±0.0046 0.1469±0.0007 0.7975±0.0028
MLP-E 0.0674±0.0012 0.0611±0.001 0.1496±0.0004 0.8004±0.0008
MLP-D 0.1476±0.0246 0.1523±0.0225 0.1783±0.0095 0.7797±0.0077
MLP-BLP 0.0585±0.0333 0.0604±0.0327 0.1521±0.0079 0.7966±0.0029
MAO-A MLP 0.1696±0.0116 0.1663±0.0122 0.212±0.006 0.7219±0.0057
(ChEMBL1951) MLP+P 0.0473±0.0084 0.0455±0.0061 0.1838±0.0017 0.7219±0.0057
MLP-E 0.1729±0.0016 0.1701±0.0007 0.2124±0.0006 0.7212±0.0007
MLP-D 0.1268±0.0061 0.1259±0.0067 0.2019±0.0043 0.7142±0.0081
MLP-BLP 0.0465±0.0061 0.0439±0.0037 0.1851±0.0018 0.7254±0.0054
hERG MLP 0.0289±0.0079 0.0254±0.0063 0.0541±0.0018 0.8061±0.0031
(ChEMBL340) MLP+P 0.0148±0.0023 0.0204±0.0017 0.0547±0.0003 0.8061±0.0031
MLP-E 0.0294±0.0012 0.0285±0.001 0.0541±0.0002 0.8083±0.0002
MLP-D 0.0815±0.0186 0.0792±0.0191 0.0607±0.0043 0.8061±0.0072
MLP-BLP 0.0111±0.0037 0.0112±0.0066 0.0534±0.0009 0.7991±0.0053
MLP-BLP+PalsogeneratedthesmallestBS,withnosignificantdifferencetotheotherPlatt-scaledmodels
MLP+P,MLP-E+Pandthecalibration-freemodelMLP+BLP.TheresultsfortargetCYP3A4showthat,
again,allPlatt-scaledmodelsaswellasMLP+BLPperformedbest,withMLP-E+Presultinginthelowest
CEsandBS.TheonlyexceptionwastheMLP-BLPmodel,whichconsistentlymatchedtheperformanceofthe
calibratedmodels. TheMLP-BLPmodelalsoresultedinthebestperformanceofthehERGmodel,significantly
outperforming Platt-scaledmodels interms of CEs. Interestingly, Plattscaling ofthe ensemble modelled to
improvedCEsacrossalltargets,whilethedifferencebetweentheMLP-BLPandMLP-BLP+Pmodelswasmuch
smaller. Insomecases,PlattscalingoftheMLP-BLPmodelsdidnotleadtoanyimprovementsatall. Apossible
explanationforthisdifferencebetweenthetwouncertaintyquantificationmethodscouldbethattuningthesingle
hyperparameterneededforMLP-BLPmodelgenerationalreadyhasacalibratingeffect. Hence,MLP-BLPmodels
arealreadybettercalibratedthantheMLP-Emodelsanddonotneedanadditionalcalibrationstep. Intheprevious
section,ensemblingfailedtoproducebetter-calibratedpredictionsinmostcases,however,theirpredictionswere
nowcalibratedafterthePlattscalingstep.
4 Conclusion
Inthispaper,weprovidedasystematicstudythatcomparedvariousmodelselectionanduncertaintyestimation
strategiestoachievewell-calibratedmodelsusingbioactivitydataofthreetargetsextractedfromtheChEMBL
database[27]. First,wereportedthattheselectionofthemetricusedforhyperparametertuningsubstantially
affectsmodelperformance. Weobservedthatusingmetricsthattookintoaccounttheprobabilitycalibrationof
themodel,notonlyresultedinsmallercalibrationerrorsbutalsoinimprovedAUCscores. Second,wecompared
thebaselinemodelwiththreecommonuncertaintyestimationapproaches,includingtheprobabilitycalibration
methodPlattscaling,aswellasthecalibration-freeuncertaintyquantificationtechniquesdeepensemblesand
MCdropout. Inaddition,weinvestigatedthecalibrationperformanceofthebaselinemodelcombinedwitha
Bayesianlogisticregressiontakingasinputtheoutputofthepenultimatelayer,whichwecalledBayesianLinear
Probing (MLP-BLP). A Hamiltonian Monte Carlo sampler was used to retrieve samples from the parameter
posterior. TheresultsshowedthatMLP-BLPwastheonlycalibration-freeapproachthatsuccessfullyimproved
theprobabilitycalibrationoverthebaselineandcouldmatchoroutperformother,commonuncertaintyestimation
approaches. Furthermore,theBLP-MLPapproachisagoodcompromisebecauseofitsreducedcomputational
complexitycomparedtothefullBayesiantreatmentoftheweights. Surprisingly,othercalibration-freeuncertainty
quantificationmethodsfailedtoproducebetter-calibratedpredictions,whichmightbearesultofthepreviously
reportedinabilityoftheseapproachestocalibratenon-overconfidentmodels[40]. Third,weexaminedwhether
applyingaposthoccalibratortodifferentuncertaintyestimationmethodsimprovedmodelperformance. Todo
so,weusedPlattscalingbyfittingalogisticregressiontothelogitsofthedeepensembleandtheMLP-BLP
model predictions. Interestingly, Platt scaling did not always improve model calibration. In general, CEs of
11Table4: ResultsforcombiningPlattscalingwithuncertaintyquantificationmethods. Resultsforthebaseline
MLP,thecalibratedbaselineMLP+P,andthecalibration-freeandcalibrateduncertaintyquantificationmodels
(MLP-EandMLP-E+Pforensemblemodeling,MLP-BLPandMLP-BLP+PforBayesianLinearProbing)are
shown. CEsandBSsareshownacrosstargetsforallprobabilitycalibrationmethods. Resultsareaveragedover
tenmodelrepetitions,exceptforthedeepensemblemodels,forwhichfivemodelrepeatswerecomputed. The
resultsofthebestmodelareboldandunderlinedforeachperformancemetricacrossallmodels. Allotherbold
resultsarestatisticallyindistinguishablefromthebestresultasreportedinatwo-sidedt-test(p<0.05).
Target Model ECE ACE BS
CYP3A4 MLP 0.0698±0.0056 0.0663±0.005 0.1506±0.0014
(ChEMBL240) MLP+P 0.0373±0.0036 0.039±0.0046 0.1469±0.0007
MLP-E 0.0674±0.0012 0.0611±0.001 0.1496±0.0004
MLP-E+P 0.0366±0.0018 0.0375±0.001 0.1461±0.0003
MLP-BLP 0.0585±0.0333 0.0604±0.0327 0.1521±0.0079
MLP-BLP+P 0.0393±0.0063 0.0398±0.0048 0.1473±0.0008
MAO-A MLP 0.1696±0.0116 0.1663±0.0122 0.212±0.006
(ChEMBL1951) MLP+P 0.0473±0.0084 0.0455±0.0061 0.1838±0.0017
MLP-E 0.1729±0.0016 0.1701±0.0007 0.2124±0.0006
MLP-E+P 0.0428±0.0011 0.0446±0.0044 0.1838±0.0002
MLP-BLP 0.0465±0.0061 0.0439±0.0037 0.1851±0.0018
MLP-BLP+P 0.0355±0.0061 0.0318±0.0049 0.1838±0.0017
hERG MLP 0.0289±0.0079 0.0254±0.0063 0.0541±0.0018
(ChEMBL340) MLP+P 0.0148±0.0023 0.0204±0.0017 0.0547±0.0003
MLP-E 0.0294±0.0012 0.0285±0.001 0.0541±0.0002
MLP-E+P 0.0133±0.0006 0.021±0.0004 0.0545±0.0001
MLP-BLP 0.0111±0.0037 0.0112±0.0066 0.0534±0.0009
MLP-BLP+P 0.0133±0.001 0.02±0.002 0.055±0.0004
thecalibratedmodelsweresmallerwhenthecalibration-freeuncertaintyestimationmodelcouldnotimprove
probabilitycalibration,whichwasoftenthecasefortheresultsoftheensemblemodel. Forthebetter-calibrated
MLP-BLPmodel,PlattscalingproducedsmallerCEsonlyinoneoutofthreetargets,anditfailedtoretrieve
significantlybetterBSforallthreedatasets. Intheframeworkofthedrugdiscoveryprocess,ourworkprovides
importantinsightintohowtoachievereliableuncertaintyestimates,facilitatingwell-informeddecision-making
andaresource-andtime-efficientpipelineforthedevelopmentofnewtherapeuticagents.
Declarations
4.1 Acknowledgements
This study was partially funded by the European Union’s Horizon 2020 research and innovation programme
undertheMarieSkłodowska-CurieActionsgrantagreement“AdvancedmachinelearningforInnovativeDrug
Discovery(AIDD)”No. 956832. YM,AA,andHFareaffiliatedtoLeuven.AIandreceivedfundingfromthe
FlemishGovernment(AIResearchProgram). Computationalresourcesandservicesusedinthisworkwerepartly
providedbytheVSC(FlemishSupercomputerCenter),fundedbytheResearchFoundation-Flanders(FWO)and
theFlemishGovernment–departmentEWI.
4.2 Availabilityofdataandmaterials
Inthisstudy,weusepubliclyavailabledatasources,whichwecitewiththecorrespondingversionsinsection2.1.
ThecleanedChEMBLdatacanbedownloadedfromhttps://doi.org/10.5281/zenodo.12663462.
4.3 Codeavailability
Weonlyusedthird-partyopen-sourcesoftwarecitedinsection2.2. ThecodeisavailableonGitHub: https:
//github.com/hannahrosafriesacher/CalibrationStudy.
125 Appendix
5.1 ProbabilityCalibrationStudy
Table5: Resultsoftheprobabilitycalibrationstudyforalltargets. Themodelhyperparametersweretunedto
optimizetheAUCscoreonavalidationset. CEs,BSsandAUCvaluesareshownacrosstargetsforallprobability
calibrationmethods. Resultsareaveragedover10modelrepetitions,exceptforthedeepensemblemodels,for
which5modelrepeatswerecomputed.
Target Model ECE ACE BS AUC
CYP3A4 MLP 0.1799±0.0111 0.1709±0.0121 0.1951±0.0103 0.7682±0.0087
(ChEMBL240) MLP+P 0.0415±0.0072 0.0438±0.0068 0.1586±0.0048 0.7682±0.0087
MLP-E 0.1142±0.0017 0.1054±0.0024 0.1602±0.0002 0.7957±0.0015
MLP-Do 0.2262±0.0964 0.2262±0.0948 0.2395±0.0539 0.6968±0.0276
MLP-E+P 0.0551±0.0046 0.0542±0.004 0.1503±0.0002 0.7957±0.0015
MLP-BLP 0.0452±0.0114 0.0448±0.0103 0.1589±0.0046 0.7684±0.0083
MLP-BLP+P 0.0452±0.0114 0.0448±0.0103 0.1589±0.0046 0.7684±0.0083
MAO-A MLP 0.2379±0.0185 0.2281±0.017 0.252±0.016 0.695±0.0134
MLP+P 0.0574±0.0125 0.0619±0.0129 0.1926±0.0058 0.695±0.0134
MLP-E 0.1544±0.0053 0.1437±0.0029 0.2099±0.0016 0.7187±0.0028
MLP-Do 0.0902±0.0322 0.0942±0.0337 0.2063±0.0123 0.6562±0.0196
MLP-E+P 0.0643±0.0044 0.0578±0.0101 0.1861±0.0011 0.7187±0.0028
MLP-BLP 0.0607±0.0148 0.0647±0.0128 0.1959±0.0047 0.6824±0.0139
MLP-BLP+P 0.0607±0.0148 0.0647±0.0128 0.1959±0.0047 0.6824±0.0139
hERG MLP 0.0763±0.0061 0.0721±0.0085 0.079±0.0052 0.761±0.0139
(ChEMBL340) MLP+P 0.0102±0.0024 0.015±0.0023 0.0576±0.0005 0.761±0.0139
MLP-E 0.0539±0.0033 0.0535±0.0031 0.0662±0.0008 0.8088±0.0032
MLP-Do 0.3175±0.1577 0.3172±0.1578 0.1875±0.1368 0.6205±0.0526
MLP-E+P 0.0126±0.0007 0.0199±0.0026 0.0559±0.0002 0.8088±0.0032
MLP-BLP 0.0126±0.0007 0.0199±0.0026 0.0559±0.0002 0.8088±0.0032
MLP-BLP+P 0.0126±0.0007 0.0199±0.0026 0.0559±0.0002 0.8088±0.0032
13Table6: Resultsoftheprobabilitycalibrationstudyforalltargets. Themodelhyperparametersweretunedto
optimizetheaccuracyonavalidationset. CEs,BSsandAUCvaluesareshownacrosstargetsforallprobability
calibrationmethods. Resultsareaveragedover10modelrepetitions,exceptforthedeepensemblemodels,for
which5modelrepeatswerecomputed.
Target Model ECE ACE BS AUC
CYP3A4 MLP 0.1488±0.0031 0.1435±0.0036 0.1759±0.0015 0.7798±0.0055
MLP+P 0.037±0.0053 0.0371±0.004 0.1536±0.0018 0.7798±0.0055
MLP-E 0.1369±0.0031 0.1333±0.0032 0.1703±0.0003 0.7877±0.0008
MLP-Do 0.0453±0.0123 0.0457±0.0096 0.1559±0.0021 0.7781±0.0049
MLP-E+P 0.0345±0.0012 0.0342±0.0012 0.1517±0.0001 0.7877±0.0008
MLP-BLP 0.0373±0.0075 0.046±0.0051 0.158±0.0023 0.7741±0.0067
MLP-BLP+P 0.0373±0.0075 0.046±0.0051 0.158±0.0023 0.7741±0.0067
MAO-A MLP 0.2207±0.0232 0.2136±0.0252 0.2391±0.0224 0.7082±0.0287
MLP+P 0.0544±0.0159 0.0604±0.015 0.188±0.0107 0.7082±0.0287
MLP-E 0.139±0.0028 0.1345±0.0023 0.2005±0.0011 0.728±0.0038
MLP-Do 0.1247±0.0284 0.1275±0.0256 0.2126±0.0088 0.66±0.0292
MLP-E+P 0.0568±0.0044 0.0628±0.0059 0.1826±0.001 0.728±0.0038
MLP-BLP 0.0563±0.0166 0.0638±0.0137 0.1897±0.0078 0.7039±0.0292
MLP-BLP+P 0.0563±0.0166 0.0638±0.0137 0.1897±0.0078 0.7039±0.0292
hERG MLP 0.1028±0.0991 0.1019±0.0997 0.0792±0.0324 0.6928±0.0806
MLP+P 0.0091±0.0016 0.0205±0.0045 0.0591±0.0024 0.6928±0.0806
MLP-E 0.0851±0.0135 0.0766±0.0127 0.0626±0.0023 0.741±0.0064
MLP-Do 0.2654±0.0859 0.2645±0.0866 0.138±0.0486 0.6281±0.078
MLP-E+P 0.0089±0.0016 0.0217±0.003 0.0566±0.0003 0.741±0.0064
MLP-BLP 0.0302±0.0128 0.0304±0.0128 0.0595±0.0036 0.6917±0.0835
MLP-BLP+P 0.0302±0.0128 0.0304±0.0128 0.0595±0.0036 0.6917±0.0835
Table7: Resultsoftheprobabilitycalibrationstudyforalltargets. CEs,BSsandAUCvaluesareshownacross
targetsforallprobabilitycalibrationmethods. ThemodelhyperparametersweretunedtooptimizetheACEona
validationset. Resultsareaveragedover10modelrepetitions,exceptforthedeepensemblemodels,forwhich5
modelrepeatswerecomputed.
Target Model ECE ACE BS AUC
CYP3A4 MLP 0.068±0.0062 0.0635±0.0073 0.1548±0.0039 0.7872±0.0072
MLP+P 0.0419±0.0118 0.0429±0.0119 0.1522±0.0031 0.7872±0.0072
MLP-E 0.0457±0.0026 0.0415±0.0007 0.1451±0.0004 0.8043±0.0007
MLP-Do 0.2551±0.0158 0.2542±0.0142 0.2437±0.0075 0.6717±0.0267
MLP-E+P 0.0423±0.0035 0.0435±0.0037 0.1451±0.0004 0.8043±0.0007
MLP-BLP 0.0461±0.0062 0.047±0.0047 0.1536±0.003 0.7872±0.0078
MLP-BLP+P 0.0461±0.0062 0.047±0.0047 0.1536±0.003 0.7872±0.0078
MAO-A MLP 0.0999±0.0076 0.0962±0.009 0.1808±0.0021 0.7461±0.0035
MLP+P 0.0642±0.0044 0.0677±0.0069 0.1744±0.0011 0.7461±0.0035
MLP-E 0.085±0.0025 0.0921±0.0012 0.1797±0.0004 0.7469±0.0007
MLP-Do 0.0671±0.0064 0.0677±0.0098 0.1771±0.0018 0.7396±0.0051
MLP-E+P 0.0713±0.0027 0.0703±0.0015 0.1743±0.0002 0.7469±0.0007
MLP-BLP 0.0638±0.006 0.0848±0.0073 0.1786±0.0017 0.746±0.0035
MLP-BLP+P 0.0638±0.006 0.0848±0.0073 0.1786±0.0017 0.746±0.0035
hERG MLP 0.0328±0.0109 0.0317±0.0111 0.0586±0.0033 0.7742±0.0348
MLP+P 0.0112±0.0051 0.0166±0.0046 0.0569±0.0015 0.7742±0.0348
MLP-E 0.0157±0.002 0.0165±0.0031 0.0533±0.0003 0.8154±0.0024
MLP-Do 0.4349±0.1518 0.4343±0.1531 0.2737±0.1126 0.5771±0.0763
MLP-E+P 0.012±0.0013 0.0195±0.0014 0.0542±0.0002 0.8154±0.0024
MLP-BLP 0.0188±0.0083 0.0224±0.0096 0.0573±0.0027 0.7797±0.0268
MLP-BLP+P 0.0188±0.0083 0.0224±0.0096 0.0573±0.0027 0.7797±0.0268
14References
[1] MichaelSchlander,KarlaHernandez-Villafuerte,Chih-YuanCheng,JorgeMestre-Ferrandiz,andMichael
Baumann. Howmuchdoesitcosttoresearchanddevelopanewdrug? asystematicreviewandassessment.
PharmacoEconomics,39:1–27,2021. doi: 10.1007/s40273-021-01065-y.
[2] OlivierJ.Wouters,MartinMcKee,andJeroenLuyten. EstimatedResearchandDevelopmentInvestment
NeededtoBringaNewMedicinetoMarket,2009-2018. JAMA,323(9):844–853,2020. doi: 10.1001/jama.
2020.1166.
[3] HongmingChen,OlaEngkvist,YinhaiWang,MarcusOlivecrona,andThomasBlaschke. Theriseofdeep
learningindrugdiscovery. DrugDiscoveryToday,23(6):1241–1250,2018. doi: 10.1016/j.drudis.2018.01.
039.
[4] Hyunho Kim, Eunyoung Kim, Ingoo Lee, Bongsung Bae, Minsu Park, and Hojung Nam. Artificial
intelligenceindrugdiscovery: Acomprehensivereviewofdata-drivenandmachinelearningapproaches.
BiotechnologyandBioprocessEngineering,25:895–930,2020. doi: 10.1007/s12257-020-0049-y.
[5] ChaynaSarkar,BiswadeepDas,VikramSinghRawat,JulieBirdieWahlang,ArvindNongpiur,Iadarilang
Tiewsoh, NariM.Lyngdoh, DebasmitaDas, ManjunathBidarolli, andHannahTheresaSony. Artificial
intelligenceandmachinelearningtechnologydrivenmoderndrugdiscoveryanddevelopment. International
JournalofMolecularSciences,24(3),2023. doi: 10.3390/ijms24032026.
[6] MoloudAbdar,FarhadPourpanah,SadiqHussain,DanaRezazadegan,LiLiu,MohammadGhavamzadeh,
Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U. Rajendra Acharya, Vladimir Makarenkov, and Saeid
Nahavandi. Areviewofuncertaintyquantificationindeeplearning: Techniques,applicationsandchallenges.
InformationFusion,76:243–297,2021. doi: 10.1016/j.inffus.2021.05.008.
[7] LewisH.Mervin,SimonJohansson,ElizavetaSemenova,KathrynA.Giblin,andOlaEngkvist. Uncertainty
quantification in drug design. Drug Discovery Today, 26(2):474–489, 2021. ISSN 1359-6446. doi:
10.1016/j.drudis.2020.11.027.
[8] Edmon Begoli, Tanmoy Bhattacharya, and Dimitri Kusnezov. The need for uncertainty quantification
in machine-assisted medical decision making. Nature Machine Intelligence, 1(1):20 – 23, 2019. doi:
10.1038/s42256-018-0004-1.
[9] VineetEdupuganti,MortezaMardani,ShreyasVasanawala,andJohnPauly. Uncertaintyquantificationin
deepmrireconstruction. IEEETransactionsonMedicalImaging,40(1):239–250,2021. doi: 10.1109/TMI.
2020.3025065.
[10] Ryutaro Tanno, Daniel E. Worrall, Enrico Kaden, Aurobrata Ghosh, Francesco Grussu, Alberto Bizzi,
StamatiosN.Sotiropoulos,AntonioCriminisi,andDanielC.Alexander. Uncertaintymodellingindeep
learningforsaferneuroimageenhancement: Demonstrationindiffusionmri. NeuroImage,225:117366,
2021. doi: 10.1016/j.neuroimage.2020.117366.
[11] Txus Blasco, J. Salvador Sánchez, and Vicente García. A survey on uncertainty quantification in deep
learningforfinancialtimeseriesprediction. Neurocomputing,576:127339,2024. doi: 10.1016/j.neucom.
2024.127339.
[12] RhiannonMichelmore,MatthewWicker,LucaLaurenti,LucaCardelli,YarinGal,andMartaKwiatkowska.
Uncertaintyquantificationwithstatisticalguaranteesinend-to-endautonomousdrivingcontrol. In2020
IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages7344–7350,2020. doi: 10.1109/
ICRA40945.2020.9196844.
[13] CorneliaGruber,PatrickOliverSchenk,MalteSchierholz,FraukeKreuter,andGöranKauermann. Sources
ofuncertaintyinmachinelearning–astatisticians’view,2023. Preprintathttps://arxiv.org/abs/
2305.16703.
[14] EykeHüllermeierandWillemWaegeman. Aleatoricandepistemicuncertaintyinmachinelearning: anintro-
ductiontoconceptsandmethods. MachineLearning,110(3):457–506. doi: 10.1007/s10994-021-05946-3.
[15] ChuanGuo,GeoffPleiss,YuSun,andKilianQWeinberger. Oncalibrationofmodernneuralnetworks. In
Internationalconferenceonmachinelearning,pages1321–1330.PMLR,2017.
[16] LewisH.Mervin,AvidM.Afzal,OlaEngkvist,andAndreasBender. Comparisonofscalingmethodsto
obtaincalibratedprobabilitiesofactivityforprotein–ligandpredictions. JournalofChemicalInformation
andModeling,60(10):4546–4559,2020. doi: 10.1021/acs.jcim.0c00476.
[17] ChengWang. Calibrationindeeplearning: Asurveyofthestate-of-the-art. Preprintathttps://arxiv.
org/abs/2308.01222.
15[18] YanivOvadia, EmilyFertig, JieRen, ZacharyNado, DavidSculley, SebastianNowozin, JoshuaDillon,
BalajiLakshminarayanan,andJasperSnoek. Canyoutrustyourmodel’suncertainty? evaluatingpredictive
uncertaintyunderdatasetshift. Advancesinneuralinformationprocessingsystems,32,2019.
[19] MatthiasMinderer,JosipDjolonga,RobRomijnders,FrancesHubis,XiaohuaZhai,NeilHoulsby,Dustin
Tran,andMarioLucic.Revisitingthecalibrationofmodernneuralnetworks.AdvancesinNeuralInformation
ProcessingSystems,34:15682–15694,2021.
[20] JohnPlatt. Probabilisticoutputsforsupportvectormachinesandcomparisonstoregularizedlikelihoodmeth-
ods. Adv.LargeMarginClassif.,10,061999. URLhttps://api.semanticscholar.org/CorpusID:
56563878.
[21] YarinGalandZoubinGhahramani. Dropoutasabayesianapproximation: Representingmodeluncertainty
in deep learning. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd
InternationalConferenceonMachineLearning,volume48ofProceedingsofMachineLearningResearch,
pages1050–1059,NewYork,NewYork,USA,20–22Jun2016.PMLR.
[22] BalajiLakshminarayanan,AlexanderPritzel,andCharlesBlundell. Simpleandscalablepredictiveuncer-
taintyestimationusingdeepensembles. Advancesinneuralinformationprocessingsystems,30,2017.
[23] SteveBrooks,AndrewGelman,GalinJones,andXiao-LiMeng. Handbookofmarkovchainmontecarlo.
CRCpress,NewYork,2011.
[24] Michael Betancourt. A conceptual introduction to hamiltonian monte carlo, 2018. Preprint at https:
//arxiv.org/abs/1701.02434.
[25] Pavel Izmailov, Sharad Vikram, Matthew D Hoffman, and Andrew Gordon Gordon Wilson. What are
bayesianneuralnetworkposteriorsreallylike? InInternationalconferenceonmachinelearning,pages
4629–4640.PMLR,2021.
[26] GuillaumeAlainandY.Bengio. Understandingintermediatelayersusinglinearclassifierprobes, 2017.
Preprintathttps://arxiv.org/abs/1610.01644.
[27] DavidMendez,AnnaGaulton,APatríciaBento,JonChambers,MarleenDeVeij,EloyFélix,MaríaPaula
Magariños,JuanFMosquera,PrudenceMutowo,MichałNowotka,MaríaGordillo-Marañón,FionaHunter,
Laura Junco, Grace Mugumbate, Milagros Rodriguez-Lopez, Francis Atkinson, Nicolas Bosc, Chris J
Radoux,AldoSegura-Cabrera,AnneHersey,andAndrewRLeach. ChEMBL:towardsdirectdepositionof
bioassaydata. NucleicAcidsResearch,47(D1):D930–D940,2018. doi: 10.1093/nar/gky1075.
[28] GregLandrum. Rdkit: Open-sourcecheminformatics,2006.
[29] Jaak Simm, Günter Klambauer, Adam Arany, Marvin Steijaert, Jörg Kurt Wegner, Emmanuel Gustin,
VladimirChupakhin,YolandaT.Chong,JorgeVialard,PeterBuijnsters,IngridVelter,AlexanderVapirev,
ShantanuSingh,AnneE.Carpenter,RoelWuyts,SeppHochreiter,YvesMoreau,andHugoCeulemans.
Repurposinghigh-throughputimageassaysenablesbiologicalactivitypredictionfordrugdiscovery. Cell
ChemicalBiology,25(5):611–618.e3,2018. doi: 10.1016/j.chembiol.2018.01.015.
[30] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,
ZemingLin,NataliaGimelshein,LucaAntiga,etal. Pytorch: Animperativestyle,high-performancedeep
learninglibrary. Advancesinneuralinformationprocessingsystems,32,2019.
[31] AdamDCobb,AtılımGünes¸Baydin,andBrianJalaian. hamiltorch,2023. URLhttps://github.com/
AdamCobb/hamiltorch.
[32] AdamDCobbandBrianJalaian. Scalinghamiltonianmontecarloinferenceforbayesianneuralnetworks
withsymmetricsplitting. InUncertaintyinArtificialIntelligence,pages675–685.PMLR,2021.
[33] GlennW.Brier. Verificationofforecastsexpressedintermsofprobability. MonthlyWeatherReview,78(1):1
–3,1950. doi: 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2.
[34] AllanH.Murphy. Anewvectorpartitionoftheprobabilityscore. JournalofAppliedMeteorologyand
Climatology,12(4):595–600,1973. doi: 10.1175/1520-0450(1973)012<0595:ANVPOT>2.0.CO;2.
[35] MorrisH.DeGrootandStephenE.Fienberg. Thecomparisonandevaluationofforecasters. Journalofthe
RoyalStatisticalSociety.SeriesD(TheStatistician),32(1-2):12–22,1983. doi: 10.2307/2987588.
[36] MahdiPakdamanNaeini,GregoryCooper,andMilosHauskrecht. Obtainingwellcalibratedprobabilities
usingbayesianbinning. ProceedingsoftheAAAIConferenceonArtificialIntelligence,29(1),2015. doi:
10.1609/aaai.v29i1.9602.
16[37] Jeremy Nixon, Michael W. Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring
calibrationindeeplearning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR)Workshops,June2019.
[38] TilmannGneitingandAdrianERaftery. Strictlyproperscoringrules,prediction,andestimation. Journalof
theAmericanStatisticalAssociation,102(477):359–378,2007. doi: 10.1198/016214506000001437.
[39] JannikPRothandJürgenBajorath. Relationshipbetweenpredictionaccuracyanduncertaintyincompound
potencypredictionusingdeepneuralnetworksandcontrolmodels. ScientificReports,14(1):6536,2024.
doi: 10.1038/s41598-024-57135-6.
[40] Rahul Rahaman et al. Uncertainty quantification and deep ensembles. Advances in neural information
processingsystems,34:20063–20075,2021.
17