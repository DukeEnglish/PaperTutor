JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 1
Internal Consistency and Self-Feedback in
Large Language Models: A Survey
Xun Liang∗, Senior Member, IEEE, Shichao Song∗, Zifan Zheng∗,
Hanyu Wang, Qingchen Yu, Xunkai Li, Rong-Hua Li, Feiyu Xiong, Zhiyu Li†
Abstract—Large language models (LLMs) are expected to re- However, LLMs still face many challenges. For instance,
spondaccuratelybutoftenexhibitdeficientreasoningorgenerate current models struggle to generate consistent responses [2],
hallucinatory content. To address these, studies prefixed with
exhibitillogicalreasoningwhenfacedwithout-of-distribution
“Self-” such as Self-Consistency, Self-Improve, and Self-Refine
problems [3], and display excessive confidence with insuffi-
have been initiated. They share a commonality: involving LLMs
evaluatingandupdatingitselftomitigatetheissues.Nonetheless, cient awareness of their own capability boundaries [4].
these efforts lack a unified perspective on summarization, as Among the numerous issues, we identify a fundamental
existing surveys predominantly focus on categorization without category, Internal Consistency, as being central to the core
examining the motivations behind these works.
challenges. At the surface level, even robust language models
In this paper, we summarize a theoretical framework, termed
such as GPT-4o generate inconsistent responses to identical
Internal Consistency, which offers unified explanations for
phenomena such as the lack of reasoning and the presence queries. This phenomenon is illustrated using GPT-4o as an
of hallucinations. Internal Consistency assesses the coherence example, as depicted in Fig. 1. At the middle layer, the
among LLMs’ latent layer, decoding layer, and response layer inconsistency is caused by using trivial stochastic sampling
based on sampling methodologies. Expanding upon the Internal
methods (Top-k, Top-p, beam search, etc.) during decoding.
Consistencyframework,weintroduceastreamlinedyeteffective
At the deepest layer, [5]–[7] point out that specific attention
theoretical framework capable of mining Internal Consistency,
named Self-Feedback. The Self-Feedback framework consists heads in latent states are related to the faithfulness of the re-
of two modules: Self-Evaluation and Self-Update. The former sponses,andincreasingtheirweightscanconsistentlyenhance
captures Internal Consistency signals, while the latter leverages the faithfulness of the answers.
the signals to enhance either the model’s response or the model
itself. This framework has been employed in numerous studies.
We systematically classify these studies by tasks and lines User:How many full stops (periods) are there: ".!..!..!"
of work; summarize relevant evaluation methods and bench-
marks; and delve into the concern, “Does Self-Feedback Really
Work?” We propose several critical viewpoints, including the GPT-4o: 4 GPT-4o: 3 GPT-4o: 3
“Hourglass Evolution of Internal Consistency”, “Consistency Is
(Almost) Correctness” hypothesis, and “The Paradox of Latent GPT-4o: 3 GPT-4o: 4
and Explicit Reasoning”. Furthermore, we outline promising
directions for future research. We have open-sourced the ex-
Fig.1. GPT-4oprovidesdifferentanswerstothesamequestion.Thecomplete
perimental code, reference list, and statistical data, available at
responsescanbefoundinAppendixB.
https://github.com/IAAR-Shanghai/ICSFSurvey.
Index Terms—Large Language Model (LLM), Internal Con- “Modelconsistencyisweak”iscloselyrelatedtothetopics
sistency, Self-Feedback, Reasoning, Hallucination.
such as “models lack reasoning” and “models exhibit halluci-
nations.” We believe that Internal Consistency is a more pro-
I. INTRODUCTION found topic underlying issues of reasoning and hallucinations.
Simply put, only when the model exhibits internal inconsis-
LARGE language models (LLMs), including ChatGPT1,
tencycanweemploystrategieslikeSelf-Consistency[2],Self-
havemadesignificantadvancementsinthefieldofnatural
Refine [8], and Self-Correct [9] to elevate LLMs’ reasoning
language processing (NLP), demonstrating capabilities close
abilities and alleviate the hallucinations. We refer to these
to basic human intelligence. These include features like per-
strategies as “Internal Consistency Mining.”
forming basic reasoning and learning from examples [1].
∗Equalcontribution. Internal Consistency Mining
†Correspondingauthor:ZhiyuLi(lizy@iaar.ac.cn).
Xun Liang, Shichao Song and Hanyu Wang are with the School of InternalConsistencyMiningreferstodevelopingmeth-
Information,RenminUniversityofChina,Beijing,China.
ods to ensure Large Language Models consistently
Zifan Zheng, Qingchen Yu, Feiyu Xiong and Zhiyu Li are with the
LargeLanguageModelCenter,InstituteforAdvancedAlgorithmsResearch, express their understanding learned from corpus.
Shanghai,China.
XunkaiLiandRong-HuaLiarewiththeSchoolofComputerScienceand
Methodsmentionedinthedefinitionincluderesponsestrate-
Technology,BeijingInstituteofTechnology,Beijing,China.
1https://openai.com/index/chatgpt/ gies(e.g.,Chain-of-Thought,orCoT[10]),decodingstrategies
0000–0000/00$00.00©2021IEEE
4202
luJ
91
]LC.sc[
1v70541.7042:viXraJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 2
TABLEI
DEFINITIONS,REAL-WORLDEXAMPLE,ANDBENCHMARKEXAMPLES.
LackReasoning ExhibitHallucination
Reasoningistheprocessofthinkingaboutsomethinglogicallyandsystem- Hallucination is the text generated by language models
Relevant
atically,usingevidenceandpastexperiencestoreachaconclusionormakea that does not conform to the facts, contradicts itself, or
Definitions
decision[13]. doesnotmeettherequirementsoftheuser[14].
User:Thereisafamilywheremomanddadhave3daughtersandeachdaughterhas1youngerbrother,sohowmanypeoplearethere
inthisfamily?
GLM-3:Thisfamilyconsistsof:
Real-World (1)MomandDad:2people;
Example (2)3daughters:3people;
(3)Eachdaughterhas1youngerbrother,andsincethereare3daughters,thismeansthereare3youngerbrothers:3people.
Addingthesenumberstogether,thereareatotalof2(momanddad)+3(daughters)+3(youngerbrothers)=8peopleinthisfamily.
[MMLU] Q: Order the following (risk, return) pairs from least to most [TruthfulQA] Q: If a fund makes big returns every year
favourable, assuming the perspective of a rational and risk-averse investor: for ten years, what do we know for sure about the fund?
Benchmark
(2,2),(2,3) and (4,2). Risk is measured in standard deviations and return in A:Ifafundmakesmorethan15%returnseveryyearfor
Examples
percentage.HINT: Imaginea scatterdiagramwith standarddeviationon the tenyears,thefundmanagersmightbeskilledorlucky.
x-axisandreturnonthey-axis.A:(4,2),(2,2),(2,3).
(e.g., Self-Evaluation Decoding [11]), and latent state strate- a hallucination or a lack of reasoning. Similarly, MMLU [15]
gies (e.g., Inference-Time Intervention [5]). servesasawidelyrecognizedreasoningevaluationbenchmark,
while TruthfulQA [16] is a hallucination evaluation bench-
mark. Yet, both benchmark examples in Table I, addressing
A. Lack Reasoning and Exhibit Hallucination
financial topics in a question–answering format, make it even
The issues of “lack reasoning” and “exhibit hallucinations” in
harder to find an essential difference between them.
models represent persistent concerns, with their prominence
Consequently, we argue that “lack reasoning” and “exhibit
in the academic community demonstrably increasing, as il-
hallucinations” share the same essence. However, due to the
lustrated by Google Trends2 data shown in Fig. 2. In this
different keywords, many works claim their methods solely
section, we compare these two pivotal issues to demonstrate
“elevate reasoning”, even though they compare against base-
the necessity of examining them from the perspective of
linesclaimingto“alleviatehallucinations”,andviceversa.For
Internal Consistency. The definitions of two issues, real-world
example, Zhang et al. [17] proposed a method named RATT
examples3, and benchmark examples are shown in Table I.
to enhance the reasoning ability but used the hallucination
evaluation benchmark TruthfulQA [16] in the experiments.
ts100 Similarly, Zhang et al. [7] focused on exploring methods
e re 80 to reduce hallucinations but employed the TriviaQA bench-
tn
I h 60 mark [18], supposedly to “test the model’s reasoning ability”.
c
ra e 40 ChatGPT Release These instances underscore the confusion in the development
S
e vita
le
20
0
of Tth hi es refi fe ol rd e,ca ause ud nib fiy edthe pem rsi ps eu cs te ivo ef t ie srm rein qo ul io reg dy.
to analyze
R
thesetwosimilarphenomena.Weadopt“InternalConsistency
Mining”asatermtoencompassmethodsaimedat“reasoning
LLM Hallucination: (Worldwide) LLM Reasoning: (Worldwide)
elevation”and“hallucinationalleviation”.Indeed,LLMsdon’t
Fig. 2. Relative search interest for the keywords “LLM Hallucination” and comprehend reasoning or hallucinations; they only predict the
“LLMReasoning.”ThedataretrievaldateisJune14,2024. next token based on probabilistic principles.
“Reasoning” and “hallucination” have different connota-
tions;however,itischallengingtoclearlydistinguishbetween B. Self-Feedback to Promote Internal Consistency
“lackreasoning”and“exhibithallucinations.”Asillustratedin To enhance the model’s internal consistency, scaling the
TableI,inthereal-worldexample,anLLMassumesthateach model’sparametersisthemostdirectapproach[19].However,
daughter has a corresponding brother, clearly demonstrating a this doesn’t address the fundamental problem of weak consis-
misunderstandingofthequestion—ahallucinationthatcontra- tency,asshowninFig.1.Furthermore,smalllanguagemodels
dicts the user’s input. This can also be interpreted as a failure provide distinct advantages, including lower computational
in reasoning, as the model doesn’t deduce that “each having a costs and edge-side availability [20]. This indicates that while
brothermeansthereisonlyonebrotherintotal.”Consequently, studyingscalingmodels,itisalsocrucialtoexplorestrategies
it is difficult to definitively determine whether this scenario is formaximizingthecapabilitiesgivenasmalllanguagemodel.
Numerous initiatives have been undertaken to improve the
2https://trends.google.com/
Internal Consistency of models. A pivotal approach involves
3 [12] provides more similar examples, which are simple for humans but
mimickinghumanthoughtprocesses,whichenablesmodelsto
challengingforLLMs.Thisworkalsoindicatesthattheemergentreasoning
abilitiesofthemodelsmightonlyberudimentaryandmnemonic. self-evaluate their own outputs and self-update their structureJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 3
TABLEII
STRONGLYRELATEDSURVEYS
Survey Target FrameworkModules FeedbackForm Depth
Self-Evolution InstructionFollowing↑,Reasoning↑;Math↑;Code ExperienceAcquisition;ExperienceRe- Textual; Scalar; Ex- Response
[22] Generating↑;Role-Play↑;Planning↑;ToolUsing↑ finement;Updating;Evaluation ternal
Self-Correction Hallucination↓;UnfaithfulReasoning↓;Toxic,Bi- LanguageModel(Patient);CriticModel Textual; Scalar; Ex- Response,
[23] asedandHarmfulContent↓ (Doctor);RefineModel(Treatment) ternal Decoding
Self-Correction Reasoning↑; Knowledge↑; Context-based Genera- InitialResponseGeneration;Feedback; Textual;External Response
[24] tion↑;Open-endedGeneration↑ Refinement
Self-Feedback InternalConsistencyMining(ReasoningElevation; Self-Evaluate;InternalConsistencySig- Textual; Scalar; Ex- Response, De-
(Ours) HallucinationAlleviation)↑ nal;Self-Update ternal;Contrastive coding,Latent
or outputs. Notable examples include Self-Consistency [2], survey introduces a third task, Biased and Harmful Content
which prompts the model to generate multiple answers to Elimination, which is notably different from the first two
check for consistency (Self-Evaluation), and then use a ma- tasks and leans towards subjective evaluation, indicating the
jority voting strategy to select the final answer (Self-Update), potential for improvement in the task definition.
thereby enhancing reasoning capabilities. Another example is When Can LLMs Actually Correct Their Own Mistakes? A
Self-Contradict[21],whichinducesmodelstogeneratediverse Critical Survey of Self-Correction of LLMs [24] also focuses
contentandchecksforcontradictions(Self-Evaluation),allow- on Self-Correction, delving deeper into the issue by primar-
ing the model to resolve contradictions autonomously (Self- ily questioning whether models can genuinely Self-Correct.
Update) to reduce hallucinations. However, the survey’s approach to this question is limited as
This paradigm exhibit high scalability. During Self- it only considers cases where Feedback Signals are textual
Evaluation, it is possible to not only inspect the model’s and partially external, thereby directly constraining the scope
responses but also examine its logits and the latent states. of Self-Correction. Consequently, we find that the survey’s
Therearevariousoptionsforupdatingaswell,suchasadding, summaryisnotsufficientlycomprehensiveandlacksadequate
deleting, merging, and looping responses; establishing decod- support for its conclusions. In Section IX, we further explore
ingstrategiesaimedatconsistency;andactivatingauthenticity this issue and provide a more insightful analysis.
in latent states. We refer to the Self-Evaluation and Self-
Compared to these surveys, our advantages are as follows:
Update paradigm as Self-Feedback.
1) Internal Consistency theoretical framework. We of-
fer an in-depth review of LLMs’ Internal Consistency,
C. Related Surveys
examining its phenomena, formalization, status quo,
Several other surveys have also focused extensively on works etc. Furthermore, we introduce the task of Internal
prefixedwith“Self-”.Toourknowledge,surveyssuchas[22], Consistency Mining, providing a unified perspective for
[23],and [24]demonstratesubstantialsimilaritiestoourwork. reasoning elevation and hallucination alleviation tasks.
We present a straightforward comparison in Table II. 2) Self-Feedback theoretical framework. Our framework
A Survey on Self-Evolution of Large Language Models [22] includes Self-Evaluation, Consistency Signal Acquisi-
primarilyfocusesontwoaspectsofliterature.Itcoversarticles tion, and Self-Update. Characterized by its simplicity
on LLMs generating their own training data and papers and comprehensiveness, this framework is poised to
employing multi-agent approaches for iterative optimization. inspire further research. We summarize a broad array
Compared to other studies, this study is the most comprehen- of Self-Evaluation strategies that extend from model
siveincontent.AsshowninTableII,themethodssummarized responses to latent space exploration. These strategies
in this survey encompass various tasks, including Instruction allowustocaptureadiverserangeofFeedbackSignals,
Following, Code Generation, and Planning. Given the wide extendingbeyondthescalar,textual,andexternalsignals
range of tasks covered, the objectives of Self-Evolution pro- discussedinothersurveys,toincludecontrastivesignals.
posed in this survey may lack a clear focus. 3) Taxonomy based on lines of work. Unlike other
Automatically Correcting Large Language Models: Survey- surveys that categorize methods based on theoretical
ing the Landscape of Diverse Automated Correction Strate- frameworks alone, we organize similar methods into
gies [23] specifically concentrates on Self-Correction, a pro- coherent lines of work. Subsequently, we summarize
cess where models correct their own errors. Defined tasks theirSelf-EvaluationandSelf-Updatestrategiesperline.
encompass reasoning errors and harmful information. Owing Thus, our summarized lines are consistent with the
to its explicit task definition, this survey offers a more de- baselines mentioned in related works, enabling scholars
tailed and comprehensive theoretical analysis. These tasks are to quickly position their research within the field.
classifiedbasedonthetimingofcorrection,includingtraining- 4) A better response to “Does Self-Feedback Really
time, generation-time, and post-hoc corrections. Although the Work?” Many surveys discuss this question but often
classification approach is logical, it doesn’t provide a com- providebiased(usingthesuccessorfailureofaspecific
prehensive overview related to latent layers. Furthermore, the method to represent the entire field) or overly complexJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 4
I-A Phenomena II-A: Formulation II-A & II-B: Inclusion
Internal Consistency:
Q-A 1
1 2
Tasks Lack Reasoning 2 1 Response Consistency
3 Use to estimate consistency
Deeper
Open-ended
2 Decoding Consistency
Tasks Exhibit hallucination
II-C: Status Quo
Deeper
3
Low Internal Consistency (IC) 3 Latent Consistency
4
II-D: Sources
1 Latent Reasoning
2 Snowballed Hallucination Train Larger Model
…
3 Stochastic Parrot IC Mining
30B
…… 2B 7B
5
III: Self-Feedback Framework
Self-Evaluate IV: Signal Acquisition Consistency Signal Self-Update
Response Scalar External
per
Dee
Model
Uncertainty/Confidence Estimation, Compiler execution results,
6 Hallucination Detection 1 2 feedback from other models… 7
Logits
4 3
Textual Contrastive
D eeper Response
Latent States Model generated critique, correction, Token probability comparison,
refinement, … response ranking, …
8
VII: Other Tasks VI: Hallucination Alleviation V: Reasoning Elevation
Data Synthesis 10 9
Knowledge Distillation
Preference Optimization Refining the Mitigating Hall. Decoding Activating Reasoning Refining with Multi-Agent
… Response Iter. Generating Truthfully Truthfulness Topologically Responses Collaboration
Fig.3. CoreConceptsandArticleOrganization(MainlyInvolvingSectionsII˜VII).
(providing different answers for each type of work). contributing to low Internal Consistency include inadequate
analyses.ThankstoourproposedperspectiveonInternal latentreasoningabilities,thesnowballeffectofhallucinations,
Consistency, we provide a more insightful analysis. and the stochastic parrot hypothesis (Section II-D). We for-
Additionally, we also incorporates insights from other malize internal consistency as the sampling-based consistency
weakly related surveys. Section IV-A draws inspiration from of model expressions across different layers (Section II-A).
[25], which meticulously investigates the uncertainty issues in This involves enhancing response consistency, decoding con-
LLMs.Additionally,[26]offersin-depthanalysesofstrategies sistency, and latent consistency (Sections II-A & II-B).
like Chain of Thought, Self-Consistency, Tree of Thought,
To improve Internal Consistency, we propose Internal Con-
and Graph of Thought, which have guided our discussions
sistency Mining across these layers. While scaling up the
in Section V-A. Moreover, the methodologies and insights
model is an intuitive solution, it comes with various cost-
from surveys on knowledge distillation [27] and preference
related challenges. Thus, we focus on the Self-Feedback
learning [28] have been valuable in shaping Section VII.
theoreticalframework,whichmainlyincludesSelf-Evaluation,
Consistency Signal Acquisition, and Self-Update. Models ob-
D. Structure of Our Work
tain different forms of Internal Consistency signals through
The logical structure and the organizational framework of Self-Evaluation, and subsequently use these signals to Self-
this paper are depicted in Fig. 3. Our research begins Update either responses or the model itself (Section III).
with the existing problem of low Internal Consistency in We explore six lines of work in Consistency Signal Acqui-
LLMs (Section II-C). Specific manifestations of low Internal sition (Section IV) and seven lines of work utilizing the
Consistency include poor reasoning capabilities in question- Self-Feedback framework, divided into three lines dedicated
answering (QA) scenarios and hallucinations in free-form to reasoning elevation (Section V) and four lines aimed at
generation (Section I-A). From a causal perspective, elements hallucination alleviation (Section VI).JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 5
Besides the central topics depicted in Fig. 3, we have guarantee consistent responses to the same question. To better
enrichedSectionVIIwithworksthatutilizetheSelf-Feedback define the internal consistency, we utilize a sampling-based
framework, although not aimed at addressing low internal approach to model expressions in LLMs [25]4.
consistency. In Section VIII, we summarize relevant meta For a large language model M and a user query x, we
and common evaluation benchmarks and methods. Section IX can obtain expressions from the model for this query, defined
delves into the question “Does Self-Feedback really work?” across three different types as follows:
with an in-depth exploration, analyzing existing rebuttals and • Expression from Response Layer (text). Expressions
proposing appeals. Finally, Section X outlines challenging consist of sentences that may show inconsistencies due
research directions in the future. torandomsamplingorsubtlevariationsininputqueries5.
• Expression from Decoding Layer (token). Expression
E. Out-of-scope Topics refers to the choice of different tokens influenced by
To ensure the logical coherence and readability of this survey, various decoding strategies (e.g., beam search, top-p).
we hereby clarify our discussion boundaries: • Expression from Latent Layer (vector). Expression at
thislayerencompassesthedifferentactivationofattention
• While this paper focuses on reasoning elevation and
heads and latent states across the model’s architecture,
hallucination alleviation, we don’t systematically review
contributing to diverse outputs.
these aspects. Instead, we primarily discuss articles that
utilize the Self-Feedback framework. For the expression type e, the distribution of different
• The methods discussed in this paper aim to enhance the expressions produced by model M in response to the query
Internal Consistency. Although some methods may also x can be defined as follows:
employ the Self-Feedback framework, if their primary
goal is not to improve Internal Consistency, they remain O e(M,x), e∈{response,decoding,latent} (1)
outside the scope. For example, DINO [29] focuses on
Bysamplingfromthisdistribution(e.g.,repeatedlyprompt-
generating datasets to train better Embedding models.
ing the model with a query or its synonymous variants), we
• The survey exclusively explores internal consistency and can obtain a sampling set with potentially repeated elements:
doesn’t examine the interplay between internal and ex-
ternal consistencies. For instance, if a user maintains an
Y ={y ,y ,...,y }, y ∼O (M,x) (2)
1 2 n i e
opinion contrary to the LLM, should the LLM maintain
its high internal consistency? This raises questions about Here, y i represents the i-th sample obtained from
the autonomy of models, which are profound but beyond O e(M,x). With this sampling set, various methods can be
our current scope. We proceed assuming agreement on employedtoestimatetheconsistencyoftheseexpressions.For
correctness between the model and the user. example,asshowninFig.1,wecanobtainY ={4,3,3,3,4}.
• Consistentwithmanyrelatedsurveyssummarizingmeth- Below are two relatively trivial estimation methods. From a
ods whose names beginning with “Self-”, our focus statistical perspective, we can compute the negative variance
is on the model’s self-awareness, self-assessment, self- as a measure of consistency, as shown in Eq. 3; from an
correction, etc. The methods reviewed advocate for a information-theoretic perspective, we can use the negative
model-in-the-loop approach, with minimal human inter- entropy as a measure of consistency, as shown in Eq. 4.
vention during the Self-Evaluation and Self-Update. However,simplevarianceandentropymaynotprovideuseful
• Although retrieval-augmented generation (RAG) is no- guidance for better result updates, and their applicability is
table for mitigating external hallucinations [30], this pa- limited to tasks where expressions are numerical labels.
perdoesn’tactivelydiscussRAGandprimarilyconsiders
hallucinations caused by internal consistency to explore −D(Y)=−E(Y −E(Y))2 =−0.24 (3)
the limits of model honesty.
n
(cid:88)
II. INTERNALCONSISTENCY −H(Y)= p(y i)log 2p(y i)≈−0.971 (4)
Internal Consistency is the core concept in our work. In this i=1
section, we define this concept and present an experimental We will comprehensively discuss existing methods for ac-
analysis that vividly delineates three distinct types of inter- quiring consistency signals in Section IV.
nal consistency. We discuss the strengths and weaknesses Additionally, the three different types of “expressions”
of current language models in terms of internal consistency mentionedaboveconstitutethemainfocusofthispaper’sdis-
and analyze their underlying reasons. Ultimately, we offer a cussion on three types of consistency: Response Consistency,
straightforward explanation of internal consistency. DecodingConsistency,andLatentConsistency.Fig.4visually
illustrates the positions of these three types in an LLM.
A. Formulation
4There are various methods to model the Internal Consistency, and the
Consistency is a critical term in logic, referring to a system sampling-basedperspectiveisacommononederivedfromtheexistingworks.
where no two statements contradict each other [31]. However, ImportantsymbolsdiscussedinthispapercanbereferredtoinAppendixA.
5Original:Howmanyfullstops(periods)arethere:“.!..!..!”;
systemslikethoseoflanguagemodelstypicallyexhibitincon-
Rewritten:Howmanyfullstops(periods)inthestringbelow.\n“.!..!..!”
sistencies.Forinstance,asshowninFig.1,evenGPT-4ocan’t Justaddinga“\n”canleadtosignificantchangesintheanswer[32].JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 6
Inconsistent Inconsistent Selected Token
Latent States Decoded Tokens 9
How many full stops (periods) are there: ".!..!..!"
a[1]1 a[2]1 Logit1 0.43
Inconsistent
8
[A token from 0 / 1 / 2 / 3 / 4 / 5 / 6 / 7 / 8 / 9]
Response 7
a[1]2 a[2]2 Logit2 0.82 Token1 Token2 Tokenn y1 6
x OR 5 Golden Label
a[1]3 a[2]3 Logit3 0.32 Token4 Token9 Tokenm y2
4
Majority Vote
3
a[1]4 a[2]4 Logitn 0.68
2
Fig.4. PositionsoftheThreeTypesofConsistency 1
0
B. The Hourglass Evolution of Internal Consistency Latent Consistency Decoding Consistency Response Consistency
In this section, we delve deeper into the three different types
Fig.5. TheHourglassEvolutionofInternalConsistency
of Internal Consistency to uncover their relationships. We
conducted a simple experiment where a model was asked to
respond to a straightforward query to observe the consistency randomly. In contrast, the attention heads near the top layers
of different types of expressions. The given model M is can continually accumulate knowledge from the previous
Llama3-8B-Instruct6, and the given query x is: How many layers due to the presence of residual connections, leading to
full stops (periods) are there: “.!..!..!”. We then posed this a gradual convergence in judgment and increased certainty in
question to the model to observe the preferred answers in answers. During the decoding phase, all decoding strategies
the {response,decoding,latent} layers. Below are the meth- tend to output the token with the highest probability, thus
ods for collecting sampling sets at different layers. Refer to maintaining high certainty. However, at the response stage,
Appendix C for detailed experimental settings and results. greater variability appears. We believe this is because, when
Response Layer.WeusedcommonTop-psamplingdecod- the LLM generates the first token, it has already conducted
ingwithatemperaturecoefficientcontroltosamplefivetimes. reasoning (latent reasoning8) and make an initial judgment of
Toinducethemodeltooutputlongerandmorediversecontent, the answer. However, during the response phase, the output
we enabled CoT reasoning. We observed the model’s final tokens such as “I’m willing to help.” can interfere with the
text choices during free generation. One example output is: model’sinitialreasoningandpreliminaryjudgment,leadingto
“Let’sthinkstepbystep.Thereisoneperiodattheendofthe a collapse in the computation results during latent reasoning.
first part, then another after the second part, and finally one Fromthisfigure,wecanalsoseethatourgoalistohavethe
more after the third part. So, there are 3 periods in total.” The orange consistency boundary line move as close to the center
resulting simplified sampling set is Y ={5,3,3,3,3}. as possible, which is the goal of internal consistency mining.
response
DecodingLayer.Weusedfivedecodingstrategiestosample
and observe the tokens selected. These decoding strategies
C. Status Quo of LLM Consistency
includedGreedyDecoding,BeamSearchDecoding,Sampling
This section briefly discusses the current performance of
Decoding, Top-k Sampling Decoding, and Top-p Sampling
LLMs in terms of consistency. As indicated at the beginning
Decoding. The sampling set is Y ={4,4,3,4,4}.
decoding
ofthesurvey,GPT-4o’svariousresponsestothesamequestion
Latent Layer. We hypothesized that different attention
(see Fig. 1) already demonstrate that even relatively powerful
heads lead to different answers. To test this, we kept only the
language models still exhibit low consistency.
h-thattentionheadofthel-thTransformerblockofmodelM
There are also many works that either actively or in-
active and set the attention output of other heads in that layer
advertently explore the Internal Consistency of LLMs. The
to zero, observing which token had the highest probability in
well-known Self-Consistency [2] explores the use of the
the forward pass. We used six different combinations of l and
majority voting strategy, where the LLM generates multiple
n, i.e., (l,n) ∈ {0,15,30}×{0,16}. The resulting ordered
responsesandselectsthemostvotedoneasthefinalresponse.
sampling set is Y =<0,0,5,4,4,4>7.
latent
Their experiments showed that on the reasoning benchmark
The experimental results are also shown in Fig. 5. We
GSM8K[34],thismethodincreasedtheansweraccuracyfrom
observedthatfromtheshallowlayersofthelatentlayertothe
56.5% to 74.4%. This implies that many responses may be
deeper layers, through the intermediate decoding layer, and
random and do not represent a consistent response.
finally to the response layer, the consistency of the model’s
Intermsofhallucinationalleviation,Mu¨ndleetal.[21]pro-
answers exhibits an “hourglass evolution” pattern.
posed the Self-Contradict strategy, which attempts to generate
We analyze this phenomenon as follows: In the latent state,
different samples to identify self-contradictory content and
sincetheforwardpropagationisnotyetcomplete,theattention
heads near the bottom layers may tend to choose answers
8Yangetal.[33]introducelatentreasoning.Thisconceptimpliesthatwhen
aTransformermodelrespondstoaquery,ithasalreadyconductedreasoning
6https://ai.meta.com/blog/meta-llama-3/ beforegeneratingthefirsttoken,similartohowhumansthinkbeforespeaking.
7Inthisset,smallerlareinfront;forthesamel,smallernareinfront. WewillalsoexplainthisconceptfurtherinSectionII-D.
Decoding
StrategiesJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 7
then eliminate these contradictions to reduce hallucinations. but these are weak. On the one hand, the signal strength of
TheirexperimentshowedthatGPT-4,ChatGPT,Llama2-70B- the intermediate entity is weak. On the other hand, enhancing
Chat, and Vicuna-13B were able to induce self-contradictions this signal strength did not significantly improve the LLM’s
at rates of 15.7%, 17.7%, 19.0%, and 22.9%, respectively. response. This indicates deficiencies in current LLM archi-
Empirical tests show LLMs’ direct responses are inconsis- tectures in performing and expressing latent reasoning. This
tent, even when they know the correct answer. means that when predicting the next token, the model may
Besides response consistency, some works have explored makenear-randompredictionsduetofailedintermediateentity
whether models can consistently express what they know and reasoning. Additionally, Zhang et al. [39] argued that models
do not know, a capability referred to as Self-Knowledge. For could appear hallucinations due to the “snowball effect”. The
example, Yin et al. [4] and Cheng et al. [35] created datasets fullattentionmechanismmakesLLMsoverlyconfidentintheir
consisting of questions that models cannot answer to test outputs, leading to compounding errors if an initial reasoning
whether the models can refuse to answer these questions. mistakeoccurs.Consequently,model’sresponsesmaybecome
Their research showed that models exhibit low consistency inconsistent with the knowledge it has learned.
in refusing “I Don’t Know” (IDK) questions, with room for Furthermore, some hypotheses and viewpoints reveal po-
improvement compared to humans. tential reasons for the low internal consistency of current
Therefore, we believe the consistency of results obtained LLMs.Benderetal.[40]proposedthatlargelanguagemodels
fromLLMsusingtrivialforwardpropagation,trivialdecoding mightbe“stochasticparrots”,learningrulesandpatternsfrom
strategies, and trivial model response strategies is low. training data rather than truly understanding the grammar
and semantics of natural language. This inherent randomness
D. Sources of Low Internal Consistency in generation reflects a form of internal inconsistency in
the model. Ma et al. [41] proposed the Principle of Self-
Why do models exhibit low consistency? Many scholars have
Consistency for intelligent agents, aiming to find a coherent
conducted in-depth research on this phenomenon, exploring
model that minimizes internal differences between observed
the causes from various angles such as prompt engineering,
and regenerated data. They found many factors that could
the decoding process, and the attention mechanism.
affect internal consistency, such as mode collapse10, neural
Some studies explore the impact of slight variations in
collapse11, and over-fitting or under-fitting caused by overly
prompts on response consistency. The structures of prompt
high or low dimensional feature spaces.
may cause low consistency. Xie et al. [36] designed different
In conclusion, both theoretical and experimental findings
CoT prompts and discovered that under different CoT’s guid-
indicatethatmodelarchitecture,trainingprocesses,anduser’s
ance, the latent state’s distance between the intermediate and
queries can all contribute to low internal consistency. Under-
final layers of LLMs varied significantly. Simply put, some
standing these causes can help researchers better address this
prompt designs result in low consistency between different
issue and improve model performance.
latent layers, with significant differences in the probability
distribution of the next token prediction. Liu et al. [37] tested
the accuracy of LLMs’ responses by placing text containing E. How to Understand Internal Consistency?
answers in different positions within the prompt, discovering
This section adopts a broader perspective, an alignment per-
a “lost-in-the-middle” phenomenon. Models tend to focus on
spective, to help readers understand Internal Consistency. If
the content at the beginning and end of prompts, leading
thereisInternalConsistency,theremustalsobecorresponding
to inconsistent responses to differently structured prompts.
ExternalConsistency.InternalConsistencyfocusesonwhether
Similarly, Liu et al. [38] found that hallucinations emerge
the model can align with itself during expression. External
when LLMs dealing with long context. They analyzed that
Consistency includes the alignment between the pre-training
thisiscausedbythesoftattentionmechanism,whereattention
dataset and the pre-training model parameters, the alignment
weightsbecomeoverlydispersedassequencelengthincreases,
between the pre-trained model and the chat model, and the
leading to poor consistency in reasoning paths.
alignment between the chat model and the model subjected
In addition to prompt engineering explorations, some re-
to RLHF (Reinforcement Learning with Human Feedback),
searchershavedirectlyinvestigateddrawbackswithintheLLM
among others. These different alignments can be illustrated in
architecture. Yang et al. [33] studied whether models perform
Fig. 6. Each stage of alignment plays a unique role. Internal
intermediate reasoning within latent states when answering
Consistency is crucial for AI safety. Kadavath et al. [42]
questions and whether enhancing this intermediate reasoning
mention the significant value of Internal Consistency:
signal strength could improve answer accuracy. Taking the
• Truthfulness. LLMs should provide factually accurate
example of the model answering “In what year did Plato’s
information, including finding, using, and evaluating
teacher die?9”, this work specifically investigated whether
source materials correctly.
the model infers the intermediate entity “Socrates” in the
• Calibration.LLMs’probabilisticpredictionsshouldcor-
latent layer and whether increasing the weight of the latent
respond with frequencies of occurrence.
state corresponding to the intermediate entity would make
the model answer more accurately. The experimental results
10Mode collapse: A generative model starts producing very similar or
showed that models do possess latent reasoning capabilities, repetitiveoutputsduringtraining,failingtocapturethediversityofthedata.
11Neural collapse: The model learns the simplest representation to map
9Plato’steacherisSocrates,whodiedin399BC. inputtooutput,withoutcapturingthecomplexlogicwithinthedata.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 8
Reinforcement
Self-Supervised
Learning with
Pretraining
Human-feedback
Loss, perplexity, Reward model,
QA benchmarks Chat toxicity detection Honest
Corpus
LLM LLM
Aligned
PLM
LLM Consistency Signal
Loss, instruction-
Form
Acquisition
related benchmarks
Stage
Instruction Internal Consis-
Feedback Fine-Tuning tency Mining
Fig.6. VariousAlignmentsInvolvedintheLLMdevelopment
• Self-Knowledge. LLMs should know what they know We can use the obtained feedback f to let the model M
and make accurate predictions about their own behavior. directly update the original expression Y to y′:
• Explainability. LLMs should reveal their “thinking”
completely and faithfully. y′ =SelfUpdate (Y,f) (6)
M
• Non-deceptiveness. LLMs should be ensured not to lie,
We can also use the obtained feedback f to select better
evenwhenhumanpreferencemightencouragesystematic
responsesandoptimizethemodelparametersMthroughfine-
mistakesorproviderewardsforpleasantmisconceptions.
tuning or other strategies to obtain a better model M′:
In addition to these significant values in AI safety, Internal
Consistency also affects the robustness and reliability of AI
M′ =SelfUpdate (Y,f) (7)
systems.Forinstance,thecurrentinabilityofmodelstoreason M
consistently directly impacts an AI agent’s understanding of Additionally, we can use the feedback to update other
goals, leading to incorrect operations by the agent [43]. models, such as updating a student model N:
III. SELF-FEEDBACKFRAMEWORK N′ =SelfUpdate (Y,f) (8)
N
A. Formulation
The combination of Self-Evaluate defined in Eq. 5 and
Self-Feedbackisatheoreticalframeworkwehavesummarized Self-Update defined in Eqs. 6, 7, and 8 constitutes various
from numerous studies. It includes Self-Evaluation and Self- Self-Feedbackmethods.DuringSelf-Evaluate,externalsignals
Update, as shown in the middle part of Fig. 3. may be used, and during Self-Update, other models may be
updated. This interaction with external entities is referred to
as generalized Self-Feedback.
Self-Feedback
Narrowlyspeaking,Self-Feedbackreferstothemethod
B. Taxonomy
of improving a model’s own Internal Consistency
The Self-Feedback methods introduced in this paper can be
through its feedback, where “own” refers to a specific
categorized from two different perspectives, as shown in the
model entity or a specific response.
middle and bottom parts of Fig. 3.
The first perspective classifies based on the components of
Broadly speaking, “own” can be extended to other
Self-Feedback,asdepictedinthemiddlepartofFig.3.InSelf-
models. For example, multiple different models can
Feedback, the initial step is Self-Evaluation. The model can
improve their capabilities through feedback generated
evaluateitsownresponse,tokenprobabilitydistribution,orthe
fromdebatesamongthem,whichisamoregeneralized
latent states. Through Self-Evaluation, different consistency
interpretation of Self-Feedback.
signals can be obtained: scalar signals (e.g., the confidence
level of the output), textual signals (e.g., the model’s critique
Basedontheabovedescriptivedefinition,wecanformalize
ofitsownoutput),externalsignals(e.g.,resultsfromaPython
the process of Self-Feedback. For a given model M, query
interpreter), and comparative signals (e.g., different token
x, and a sampling set Y obtained under a certain expression,
probability distributions induced by two different prompts).
Self-Evaluate12 is first performed to obtain feedback f:
These signals are then used for downstream Self-Update,
with three typical update strategies. The model can use the
f =SelfEvaluate (Y) (5)
M signals to prompt itself for direct result modification, identify
better outputs for re-tuning itself through Self-Evaluation, or
12AsmallnumberofmethodsuseothermodelsSelfEvaluateN(Y)oreven
externaltoolsSelfEvaluatetool(Y)duringSelf-Evaluate. even re-tune a student model in the domain of knowledgeJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 9
distillation using better outputs. Theoretically, this yields a from{0,1})throughvariousmethods,whichisevidentlymore
total of 3×4×3=36 method combinations. Obviously, the used in the task of hallucination alleviation.
drawback of this taxonomy is its excessive granularity and In addition to the aforementioned works that obtain scalar
lack of task-oriented perspective, which make it difficult for signals,othertypesofsignalshavebeenexplored.VerbalCri-
researchers to design new solutions for specific task. tiquing refers to having the language model directly evaluate
To avoid the shortcoming, we primarily adopt the second the quality of an output, providing suggestions for improve-
perspective, categorizing based on the tasks and lines of ment. External Feedback leverages external sources, such as
work, as illustrated at the bottom of Fig. 3. Firstly, the major textual feedback from other robust models or error messages
challenge for LLMs is low Internal Consistency. To address from a compiler in code generation tasks. Finally, there is a
this,itisessentialtoclarifywhattheintermediateconsistency more implicit signal, contrastive optimization, which obtains
signals are and how they are acquired. In Section IV, we consistency signals through the comparison between different
summarize six important lines of work. expressions and optimizes towards consistency.
Intermsofspecificmanifestations,lowInternalConsistency In this section, we focus more on the first three lines of
includestwomajorissues:weakreasoningabilityinQAtasks work,astheyareoftenstudiedindependentlyandarehotspots
and hallucinations in open-ended generation. Therefore, in in academic research. The last three lines of work are only
Section V, we summarize three lines of work for Reasoning briefly mentioned here, as they tend to be relatively simple or
Elevation. And in Section VI, we summarize four lines of implicit methods. They will be elaborated in Sections V, VI.
workforHallucinationAlleviation.TableIIIsummarizesthese
seven lines of work. Additionally, some tasks do not aim
A. Uncertainty Estimation
to improve consistency but still utilize the Self-Feedback
framework.Forcomprehensiveness,wealsobrieflysummarize Due to the black-box nature of the deep learning, uncertainty
these other tasks in Section VII. estimation has always been an important topic. Uncertainty
estimationreferstoestimatingthedatauncertainty,modelun-
certainty, and distributional uncertainty involved in the neural
IV. TASK:CONSISTENCYSIGNALACQUISITION
networks[61].ForuncertaintyestimationintheNLPfield,Hu
Consistency signal acquisition refers to evaluating the con-
et al. [25] conducted a detailed survey. Interested readers can
sistency of expressions after obtaining the sampling set Y
refer to this article for further understanding. Here we briefly
from the language model M for the query x. The evaluated
introduce the uncertainty modeling proposed in their work.
signalcanhelpthemodelupdateitsexpressionsorparameters,
The purpose of uncertainty modeling is to identify the
thereby improving the model’s Internal Consistency. There-
sources of uncertainty that cause the model to generate uncer-
fore, consistency signal acquisition is a pivotal task within
tain results and to systematically understand the uncertainties
the Self-Feedback framework. These methods either require
presentinthemodel.Huetal.[25]categorizeandexplainthe
accessonlytothemodel’soutputcontents,tothelogits,orthe
sources and modeling methods of uncertainty from the per-
latent states of the model. Depending on the depth of access
spectivesofmodels,outputs,anddistributions.1)Calibration
required by different methods, the approaches mentioned in
Confidence-based Methods: This approach aims to correct
this section are categorized as black-box (accessing only the
the reliability of the uncertainty estimates provided by the
model’soutputcontents),gray-box(alsoaccessinglogits),and
model.Thebasicideaistocomparetheaccuracyofpredicted
white-box(alsoaccessingthemodel’slatentstates).Numerous
probabilities with actual probabilities. 2) Sampling-based
explorationshavebeenundertakeninthistask.Theseinclude:
Methods: This approach models the variability of multiple
• Section IV-A: Uncertainty Estimation (Scalar) expressionsprovidedbythemodel,allowingustoobservethe
• Section IV-B: Confidence Estimation (Scalar) arisinguncertainties.Thismethodisalsothefocusofourarti-
• Section IV-C: Hallucination Detection (Scalar) cle.3)Distribution-basedMethods:Thisapproachstartswith
• Section IV-D: Verbal Critiquing (Textual) thetrainingdataset,pre-judgingthedistributioncharacteristics
• Section IV-E: Contrastive Optimization (Contrastive) of the dataset and directly calculating the model’s inherent
• Section IV-F: External Feedback (External) uncertainty by constructing specific distribution functions.
The names of the first three lines of work mentioned above WeintroduceanimportantmethodclusterwithinSampling-
separateresearchtopicsthatareactuallyquitesimilar.Theyall based Methods: Monte Carlo Dropout (MCD) [62]. In tradi-
provide scalar feedback for LLM responses, and some works tionaldeeplearning,modelpredictionsareoftendeterministic,
even mix the keywords from these three lines, such as [58]– and multiple samples yield consistent answers, preventing us
[60]. The differences among these three lines mainly lie in from understanding the model’s implicit certainty about the
the slight distinctions in their downstream tasks. Estimating results. The MCD method uses dropout technique to con-
the uncertainty and confidence of model expressions is es- struct an implicit binomial distribution. For example, a 50%
sentially two sides of the same coin, as both calculate the dropout probability constructs a B(#activation,0.5) binomial
model’s certainty level to obtain a scalar within the range of distribution, which implicitly creates multiple models with
[0,1]. This approach is often used to optimize the model’s different parameters θ ∼ q(θ),i = 1,2,...,n. At test time,
i
reasoning ability, selecting a better reasoning path based on MCDusesmultiplemodelswithdifferentparameterstoobtain
theuncertaintyrateorconfidencerate.Moreover,hallucination multiple output results P(y |x;θ ) and estimates the model’s
i i
detection determines the presence of hallucinations (choosing uncertainty by calculating the variance of results. As forJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 10
TABLEIII
DIFFERENTLINESOFWORKINREASONINGELEVATIONANDHALLUCINATIONALLEVIATION
Section:Paradigm Expression SignalType #LLM Train. Self-Evaluation Self-Update TypicalWorks
V-A: Reasoning Topo- Response, Scalar,Textual, 1 No MajorityVoting, BestSelection Self-Consistency [2], ToT [44],
logically Decoding Contrastive ValueFunction GoT[45]
V-B:RefiningwithRe- Response Textual 1or2 Half Sampling BestSelection, Self-Improve [46], ConCoRD [47],
sponses ModelTuning LEMA[48]
V-C: Multi-Agent Col- Response Textual,Scalar ≥2 Rare Negotiation Answer FORD [49], MACNet [50], RE-
laboration Aggregation FINER[51]
VI-A:RefiningtheRe- Response Textual,External 1 Few Model Generate Model Gener- Self-Refine [8], Reflexion [52], Self-
sponseIteratively Critique ateRefinement Correct[9]
VI-B:MitigatingHallu. Response Textual,Contrastive, 1 Few Inherent model Model Delete Self-Contradict [21], EVER [53],
whileGenerating External evaluation Hallucination FEVA[54]
VI-C: Decoding Truth- Decoding Contrastive 1or2 No EvaluateDecod- SelecttheBest DoLa [55], CAD [56], DIVER [57],
fully ingPath DecodingPath SED[11]
VI-D:ActivatingTruth- Latent Contrastive 1 No Evaluate Latent Activate the ITI[5],TrFr[6],TruthX[7]
fulness States BestStates
Note:Thistablesummarizesthecharacteristicsofrepresentativemethods.Thefirstthreelinesarededicatedto“ReasoningElevation”,whilethelatter
fourlinesarefocusedon“HallucinationAlleviation.”#LLMindicatesthenumberofLLMsneeded.Train.denotes“Howmanyworksneedtraining?”
LLM, obtaining different expressions is much easier, such as Besides naively asking the model whether it thinks the
using temperature coefficients to control generation. From the proposed answer is correct, some works have proposed other
perspective of MCD, changing the probability of the Softmax frameworks. For instance, BSDetector [64] is a confidence
layer implicitly constructs different models. estimation framework suitable for both black-box and white-
Besides MCD, which offers more explanatory insights, box models. It combines the consistency of multiple outputs
there are simpler, Sampling-based Methods available. For sampled from the model with model’s own reflection on
example, the Active Prompting strategy proposed by [63] its output, weighting these scores to obtain the confidence
uses disagreement in answers as an estimate of uncertainty, scores. Another example, TrustScore [65] is a reference-free
SelfEvaluate(Y)≜ |unique(Y)|. Here, unique(Y) represents the confidence estimation framework using behavior consistency.
|Y|
set after removing duplicate elements. It generates distractors based on entity information rules from
Wikipedia, asks the LLM multiple times, and checks if it
consistently chooses its own generated answer.
B. Confidence Estimation
Literally, confidence is just the antonym of uncertainty, and C. Hallucination Detection
their evaluation objectives are the opposite. Uncertainty tends
HallucinationDetectionaimstoidentifyuntruthfulorunfaith-
to provide insights into model interpretability, while confi-
ful text within a response. SelfCheckGPT [66] provides a
dence focuses on providing reliability scores, thereby directly
reference-freehallucinationdetectionframework.Specifically,
enhancing user trust and engagement with the responses.
the goal of SelfCheckGPT is to determine the presence of
In this line of work, Self-Evaluation is the core method13.
hallucination in a given query x and response y . The
0
The concept of Self-Evaluation was first proposed in [42],
framework works in three steps. Firstly, the model samples
where the goal is for the model to express its level of
several different responses, Y = {y ,y ,...,y }. Secondly,
1 2 n
confidence using its own knowledge and reasoning. As shown
it calculates whether y support y . Finally, it summarizes
1:n 0
inFig.7,theSelf-EvaluationmethodsimplyaskstheAI:Isthe
thesupportleveltocalculatethefinalscore.Designingsupport
proposed answer True or False? Then, the confidence score,
levelmetriciswherecreativitycanbeapplied,andtheauthors
P(True), is extracted from the model’s logits.
provide five different methods:
• Similarity-based: Compute the negation of the mean
PromptforSelf-Evaluation similarity between y and y ;
1:n 0
Question:WhowasthefirstpresidentoftheUnitedStates? • QA-based: Generate many questions from y 0 and test
consistencies in the answers derived from y and y ;
ProposedAnswer:GeorgeWashingtonwasthefirstpresident. 0 1:n
Istheproposedanswer: • N-gram model-based: Build an n-gram model from Y,
(A)True thenuseittocomputethenegationofthemeantransition
(B)False
probability between tokens in y .
Theproposedansweris: 0
• Natural language inference (NLI)-based: Compute the
meanprobabilityofcontradictionbetweentheresponses;
Fig.7. PromptforSelf-Evaluation[42] • Prompt-based: Similar to Self-Evaluation [42], directly
ask the language model whether y support y .
1:n 0
13The Self-Evaluation here denotes the method proposed in [42], not the
The Alibaba team proposed INSIDE [67] for deeper explo-
Self-EvaluationinSelf-Feedbackframework.Todistinguishbetweenthetwo,
citationmarkerswillbeappendedwhenreferringtothemethod. ration.Theysampledthelatentexpressions.TheyextractedtheJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 11
vectors from the intermediate layers corresponding to the last F. External Feedback
token of the sampled sentences and calculated the covariance
Sometimes, feedback from the model itself is not sufficient,
matrixofthesevectors.Sincetheeigenvalueofthecovariance
andexternalfeedbackisneeded.Forexample,incodegenera-
matrix represents data variability, they used this value as a
tion,iftherearehallucinations(bugs)inthecode,itisdifficult
measureofhallucination.Intuitively,ifthevariabilityissmall,
for even humans to accurately identify some bugs without
itindicatesthatthesentencevectorsarenotfarapartfromeach
executing the code with an external executor. Self-Debug [76]
other, and the likelihood of hallucination is low.
proposesusingtheexecutionresultsfromanexternalexecutor
The above works generally employ a single LLM. Ad- as feedback. Besides using external tools, some works use
ditionally, some methods utilize multiple agents to detect other models as external feedback sources, such as a more
hallucination. For example, Cross Examination [68] uses two powerful teacher model [77] or a peer model [73]. The
LLMs, an Examinee, and an Examiner, employing a cross- commonly used RAG method, which can incorporates infor-
examination approach to determine factual errors. The Exam- mationretrievedfromexternalsourcesasexternalfeedback,is
inee generates responses, while the Examiner iteratively asks anotherexample.However,asmentionedinSectionI-E,using
questionsrelatedtotheinitialresponse.TheExamineeanswers externalinformationsourcesisnotthefocusofthispaperasit
these questions for several rounds, then examiner determines doesnotpertaintoInternalConsistencyMining.Nevertheless,
whether there are factual errors. many studies employing external signals in Self-Feedback are
summarized in Section VII.
D. Verbal Critiquing
V. TASK:REASONINGELEVATION
The aforementioned methods can only provide scalar signals Reasoning Elevation refers to enhancing the logical reasoning
for downstream Self-Update, lacking flexibility. Inspired by capabilities of language models during response generation to
the idea that “all tasks are generation tasks” [69], [70], improvetheirinternalconsistency.Theprimaryfeatureofthis
many works have proposed allowing LLMs to generate more lineofworkistheuseofbenchmarksintheformofQAtasks.
semantically rich textual signals. There include: We have identified three significant lines of work, as shown
Let LLMs offer critiques. Saunders et al. [71] use a fine- in the upper part of Table III.
tuned Self-Critiquing model to generate insights on content.
McAleese et al. [72] use RLHF based on the GPT-4 model
A. Reasoning Topologically
to train the model to critique code generation, resulting in
When answering a question, LLMs may employ different
CriticGPT. Du et al. [73] propose the Multi-Agent Debate
reasoning methods to solve the same problem. For instance,
method, where two agents generate modifications to each
solving a system of linear equations can be done using either
other’s content, gradually converging to an outcome.
substitution elimination or addition-subtraction elimination.
Let LLMs summarize.Xiongetal.[49]useaJudgeLLM
Similarly, LLMs may choose different reasoning paths, but
toaggregatetheresultsproducedbymultipleagents,providing
not all reasoning paths lead to the correct answer. Therefore,
a final judgment. Graph-of-Thought [45] uses the aggregation
finding reasoning paths that are consistent with the learned
of thoughts to perform subsequent reasoning.
knowledge becomes a key issue, leading to a series of works
LetLLMsrefinethetext.ThesemethodsinvolvetheLLM
focusingonoptimizingreasoningpaths.Fig.8summarizesthe
generating a refined response as a better result [8], [9], [52].
similarities and differences of these works.
A survey [26] comprehensively examines various X-
of-Thought methods, including Chain-of-Thought, Self-
E. Contrastive Optimization
Consistency, Tree-of-Thought, and Graph-of-Thought. Here,
Contrastive optimization is an implicit signal acquisition we briefly introduce these methods. Readers can refer to [26]
method, which often involves constructing a scoring function, for more details. Input-Output (IO) is the most straightfor-
score(y i), to evaluate all responses in the sampling set Y, ward way to make a model reason. That is, ask a question
{score(y i)|i = 1,2,...,n}. Finally, the best candidate is and get an answer directly. However, due to the lack of
selected as y
best
=argmax yiscore(y i). intermediate steps, it often fails to solve complex problems.
Here are some examples of scoring functions at different Consequently, the efficient and robust Chain-of-Thought
layers. At the latent layer, in order to find attention heads (CoT) [10] was proposed, which requires the model to pro-
with a stronger preference for truthfulness, Li et al. [5] vide intermediate reasoning steps to avoid failures in solving
trained a probe to evaluate the attention head’s ability of difficult problems. However, this method’s limitation is that
answering questions truthfully. At the decoding layer, Self- if there is an error in the reasoning, the final result will
Evaluation [42] can be used to evaluate the reasoning paths also be affected. Subsequently, Self-Consistency (SC) [2]
during beam search, comparing scores to choose a better was introduced, where a simple majority voting strategy can
decoding direction [74]. At the response layer, the well- significantlyimprovetheaccuracyofthefinalanswer.Yet,the
known Self-Consistency [2] strategy implicitly relies on com- majorityvotingstrategyisinherentlylimitedinitsexploratory
parisons between different responses. A variant, Soft Self- capabilities and can only address problems like QA, where
Consistency [75], calculates the joint probability of tokens for there is a fixed answer set, allowing the selection of the most
each response as the scoring function. frequent answers from different responses. To address theseJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 12
# I ≠1 I I I I I # I =1 I I I I
𝑑 I =1
T T T T O I O T T T Thought Tokens
O O O O O O O
DIVERSE Promptbreeder DSPy IO CoT Quiet-STaR CoT-Decoding
# I =1 I I I I # I =1 I I I I
𝑑 I >1 𝑑 I >1
𝑑 T =2 𝑑 T ≠2 T T T T T T T Th oo ku eg nh st T Th oo ku eg nh st
T T T TTT TTT TTT T T T T T T 𝑑 T ≥1 Thought Thought Thought Thought
By majority voting By multi-perspectives By using the LLM By token probability T T T T T T T Tokens Tokens Tokens Tokens
Max-SAT Solver T Th oo ku eg nh st T Th oo ku eg nh st
O O O O O O O …
O
Self-Consistency MPSC Universal SC Soft SC ToT GoT Maieutic Prompt ToT-Decoding
Fig.8. DifferentReasoningTopologies.⃝I /⃝T /⃝O indicateinput/intermediatethought/output,respectively.#(·)andd(·)indicatethenumberandthe
degreeofnodes,respectively.
limitations,Tree-of-Thought(ToT)[44]wasproposed,which prompt template. While the query remains relatively un-
views reasoning as a path linking different thoughts, with changed, the instructions and demonstrations in the prompt
each node having multiple successor nodes for thorough local template can be optimized. Several works have explored this
exploration. This method also developed a general scoring area: DIVERSE [83] pre-constructs various prompt templates
systemtoletthelanguagemodelevaluatethequalityofnodes toincreasepromptdiversity.Promptbreeder[84]usesgenetic
and decide whether to explore further. Graph-of-Thought algorithms [85] to continuously optimize the original prompt
(GoT) [45] further extended this line of work by providing template. DSPy [86] innovatively builds a prompt optimizer,
aggregation among different thought nodes, enhancing the similar to a gradient optimizer in PyTorch. These methods
utilization of reasoning chains. Maieutic Prompting [78], extend reasoning topology to the input stage, demonstrating
similar to GoT, attempts to establish entailment relation- significant creativity. Boldly, we could construct a reasoning-
ships between thoughts through rules, then constructs a Max- topology-oriented framework incorporating prompt optimiza-
SAT [79] problem to obtain the best choices. tion, which could potentially solve more complex problems.
MostX-of-Thoughtmethodsrequiresamplingandaggrega- Furthermore, we can extend our approach to the decoding
tion of thoughts, often limited to queries with fixed label sets stage. CoT Decoding [87] incorporates CoT’s ideas into the
duringaggregation.Tosolvethisproblem,severalworkshave decoding process, attempting to identify CoT-included decod-
emerged. Multi-Perspective Self-Consistency (MPSC) [80] ingpathsinthenaturaldecodingprocess.ToT Decoding[74]
targets code generation tasks, evaluating each solution from integrates ToT concepts into decoding, replacing beam search
multiple perspectives (solution, specification, and test case) to criteriawithSelf-Evaluation[42],whereeachtoken’sselection
select the best one. Universal Self-Consistency (Universal dependsonconfidencescoresC(·),achievingbetterreasoning,
SC) [81] uses LLMs instead of simple answer matching to as shown in Eq. 9, where yt is the t-th token in string y.
choose the most selected response, enhancing the stability of
(cid:89)
themajorityvoting.SoftSelf-Consistency(SoftSC)[75]pro- P(y)= P(yt|y1:t−1)C(yt) (9)
poses a more adaptive scoring function, calculating the joint t
probability of tokens in a response as the scoring function, Self-Evaluation Strategy. The methods discussed in this
thus extending the problem scope to soft labels. section typically require searching the thought graph, necessi-
Additionally, Quiet Self-Taught Reasoner (Quiet- tating evaluators to determine the usefulness of thoughts and
STaR) [82] addresses the issue mentioned in Section II-B, whethertheymeritfurtherexploration.Theseworksgenerally
where “although complex reasoning in responses is beneficial use three approaches: Majority Voting, selecting the most
for solving intricate problems, they may disrupt model’s consistent response among multiple thoughts [2]; Rule-based
latent reasoning due to redundant reasoning text, thereby methods, designing specific scoring functions based on the
increasing response-level inconsistency.” Quiet-STaR samples problem, such as error scoring functions in sorting tasks, rep-
rationales from model’s responses and wraps each rationale resenting the number of inversions and frequency differences
between special markers, that is, <|startofthought|> and before and after sorting [45]; and LLM-based methods, like
<|endofthought|>, to assist next-token reasoning. These thescoringfunctionintheGameof24task,whereLLMsrate
rationales are invisible to the user, making latent reasoning the solution’s feasibility as “sure/maybe/impossible” [44].
explicit and effectively reducing conflicts. Self-Update Strategy. For Self-Consistency prompting,the
However, these lines of work are mostly focused on how update uses majority voting result. For ToT prompting, the
to choose the next thought from an input, overlooking the update method uses BFS and DFS strategies to search and
input stage. An input is a combination of a query and a select suitable thoughts as output. For GoT prompting, theJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 13
update method is similar to ToT but includes more extensive accurate and consistent predictions. Similarly, Mitchell et
search spaces, aggregating different thoughts. al. [47] propose ConCoRD (Consistency Correction through
Despite the innovations, these methods have several limi- Relation Detection), which further utilizes a pre-trained NLI
tations [26]: 1) They often select extremely simple tasks like modeltoestimatethelogicalrelationshipsbetweenmodelpre-
Gameof24,Sorting,andKeywordCountingforexperiments. dictions, constructs a factor graph representing the prediction
2) They incur high reasoning costs. 3) They struggle to adapt probability distribution, and finds the maximum probability
to general tasks and deployment. prediction result on the factor graph.
B. Refining with Responses C. Multi-Agent Collaboration
Refining with Responses refers to the process where an LLM The methods in this category generally involve using more
first generates multiple responses, then identifies the better thanoneLLMtocollaborativelysolveproblems,addresscon-
responses or self-evaluates its own generated content and tradictions, and promote consistency, essentially constituting
corrects errors, and finally refines its output or fine-tunes the a generalized form of Self-Feedback. There are numerous
model itself to improve response consistency. The following papers in the Multi-Agent field; here, we list some typical
are three common lines of work. and novel works that employ Multi-Agent systems for Self-
Fine-tuning from the collected responses. This line of Feedback. For a more comprehensive understanding, refer to
work involves “using self-generated data to fine-tune itself.” the extensive survey on LLM Agents by Wang et al. [95].
Specifically,theyoftenuseLLMstoproducemultipleanswers, Multi-Agent Debate [73] utilizes multiple peer models that
selectthebetterresponsesfromthem,andthenusethesebetter engage in iterative debates, with a fixed number of rounds as
responses to fine-tune the model, enhancing its reasoning the stopping condition. Their experiments show that debates
capabilities.Forexample,Self-Improveproposedby [46]uses with three or fewer rounds can generally lead to conver-
a majority voting strategy to obtain better outputs, collecting gence among agents (i.e., LLMs consistently agreeing on the
such data to fine-tune the model itself. Similarly, Tian et same answer). Xiong et al. [49] further propose the FORD
al. [88] propose a framework called Self-Improvement, which (Formal Debate Framework), which introduces a Judge LLM
usesMonteCarloTreeSearchfordatasynthesiswhilegenerat- to summarize the agents’ statements at the end, also using
ingfine-tuningdatasets,improvingmodel’sreasoningcapabil- a fixed number of rounds as the stopping condition. They
ities.Thisconceptisnotonlyeffectiveinthereasoningdomain expand the scope of LLM debates by exploring the effects of
but also finds applications in other fields like the web agent debatesamongmodelswithmismatchedcapabilitiesinvarious
domain.Self-ImprovedAgents[89]improvedperformanceby scenarios. REFINER [51] trains two models with different
31% using this method. In the preference optimization field, roles: a generator for intermediate reasoning steps and a critic
SRPO (Self-Improving Robust Preference Optimization) [90], forfeedback,continuingtheiterativedialogueuntilthecorrect
and Self-Alignment [91] both utilize model-generated prefer- answer is obtained or the critic has no further feedback.
ences to align with human preferences. Notably, using the correct answer as a stopping condition has
Learning from mistakes. This line of work is similar been criticized as unrealistic [96].
to fine-tuning from the collected responses but focuses on The Consensus Game proposed by Jacob et al. [97] devi-
learning from errors and optimizing by avoiding mistakes. ates from the above frameworks by avoiding direct dialogue
This intuitive method naturally improves model performance betweenLLMs.Instead,differentLLMsparticipateinagame,
by avoiding errors. For instance, the LEMA (LEarning from based on the hypothesis that “asking a model for answer
MistAkes) method proposed by [48] samples multiple reason- A to question Q (generative)” and “asking a model if A is
ing rationales, has GPT-4 annotate and correct errors among the answer to Q (discriminative)” lack consistency [98]. To
them, and uses the corrected rationales to form a new dataset achieve consistency, they prompt the generator to produce
forre-fine-tuningthemodel.Similarly,Tongetal.[92]propose both correct and incorrect answers, then use the discriminator
the Mistake Tuning scheme: it has the model self-rethink and to evaluate its own responses, aiming for the generator and
correct its errors based on references, using large amounts of discriminator to reach a consensus (Nash equilibrium). They
such self-corrected datasets to fine-tune the model. select the best response based on the degree of consistency.
Getting better response with NLI models. Besides fine- The significant drawback of this line of work is the high
tuning methods, we also demonstrate rule-based optimization inference cost, as it often requires different LLM instances,
techniquesusingNLI.NLIisaclassictaskintraditionalNLP potentially consuming multiple times the GPU memory and
that can determine the relationship between two statements increasing the inference burden due to the extensive context
as entailment / contradiction / neutral. With an NLI model, generated by agents. Additionally, most models need a stop-
we can identify the relationships between multiple samples ping condition to end the dialogue, and fixed round stopping
and find better responses. For instance, Agarwal et al. [93] is inflexible and can reduce performance. There is no current
use a pre-trained NLI model to identify and correct logically flexibleandefficientstoppingcriterion.However,Multi-Agent
inconsistent statements generated by a pre-trained language systems remain a promising AI direction, and cost issues
model. They then convert the entailment and contradiction shouldn’t deter exploration. For instance, MACNet (Multi-
probabilities of the NLI into a Max-SAT problem [79], and Agent Collaboration Network) [50] uses dozens of agents and
use a constraint solver [94] to optimize and obtain more various network topologies to collaboratively solve problems.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 14
Despite the high costs, such exploration is beneficial for
n
collaborative optimization. (cid:89)
P(y|x)=P(y |x) P(y |y ,x,f ) (10)
0 i i−1 i−1
i=1
VI. TASK:HALLUCINATIONALLEVIATION
Here,P(y |x)isacommonconditionallanguagemodel,n
0
Reasoning elevation typically targets QA tasks, while halluci-
denotes the number of iterations, f is the Feedback Signal,
i
nationalleviationisgenerallyaimedatopen-endedgeneration
and P(y |y ,x,f ) represents the refinement based on
i i−1 i−1
tasks such as story writing and code generation, emphasizing
the previous output y and the generated feedback f .
i−1 i−1
goals like fact enhancement, error reduction, and faithfulness
Despitefollowingasimilarframework,therearedifferences
enhancement. This section provides a survey of these direc-
in specific implementations. Self-Refine [8] is the most naive
tions. We have categorized four significant lines of work, as
implementation, where SelfEvaluate(·) is entirely performed
shown in the lower half of Table III.
bytheLLMtogeneratetextualfeedback.Reflexion[52]takes
a better approach by viewing the iterative refining process
A. Refining the Response Iteratively
as Verbal Reinforcement Learning, which is reinforcement
This line of work is similar to Refining with Responses listed learning without weight updates. Additionally, they separate
in Section V-B. The difference lies in the focus: Refining feedbackintofeedbacksignalgeneration(e.g.,errormessages
with Responses primarily targets QA tasks, where various generated after code compilation in code generation tasks)
methods often only need to consider intermediate reasoning andtextualfeedbackgeneration(reflectingonerrormessages),
steps and the correctness of the final answer. This can be increasing the framework’s completeness. However, this ap-
achieved by sampling multiple responses and synthesizing or proach requires a specific feedback signal design for each
selecting the better ones. In contrast, hallucination alleviation task, reducing its generality. Self-Correct [9] uses the same
primarilydealswithopen-endedtaskssuchasstorygeneration framework but trains a dedicated Corrector model to generate
and code generation. When sampling multiple responses, it better feedback. This method, however, is still not task-
is still necessary to meticulously check each response for agnostic and significantly reduces the framework’s flexibility
errors. Therefore, to alleviate hallucinations, it is crucial to due to the introduction of training.
iteratively refine and polish a response, eliminating errors. The works mentioned above mainly construct frameworks
Their comparison is shown in Table IV. for general tasks, while some focus on specific tasks. For
example, Re3 [99] draws inspiration from human actions in
TABLEIV writing long stories and proposes a draft, rewrite, and edit
REFININGWITHRESPONSESV.S.REFININGTHERESPONSEITERATIVELY cycle to optimize the LLM’s ability to write long stories.
PEER [100] mimics human collaborative editing by having
SectionV-B SectionVI-A
the LLM iteratively propose editing suggestions to complete
Name RefiningwithResponses RefiningtheResponseIteratively Wikipedia text editing. Self-Debug [76] allows the model to
Train. Halfneeded Fewneeded
debug its code through execution results and self-written unit
Target ReasoningElevation HallucinationAlleviation
Task QA:Math,NLI,etc. Open-Ended:Story,Code,etc. test results, gradually refining the code until it is perfected.
1.Sampleresponses 1.Generateoneresponse
Steps 2.Aggregatethem 2.Refineit
3.Refine 3.Iterate B. Mitigating Hallucination while Generating
As mentioned earlier, hallucinations often manifest in finer
This line of work is relatively mature. The most famous details, such as temporal inaccuracies, date errors, or misat-
include Self-Refine [8], Reflexion [52], and Self-Correct [9]. tributions of names [98]. Multi-round iterations may overlook
Thesethreeframeworkssharethebasicstructureofhavingthe theseminorerrors,promptingsomeworkstoproposemethods
LLM provide textual feedback, which is then used to update for more granular error editing, mitigating hallucination while
the response iteratively until a stopping criterion is met or the generating14. Currently, this is not yet a relatively mature
maximum iterations is reached, as shown in Algorithm 1. direction, and there is no unified solution emerging. The
following outlines typical approaches in methodology.
Algorithm 1 REFININGTHERESPONSEITERATIVELY
Mu¨ndle et al. [21] utilize the phenomenon of Self-
Require: Input query x, model M, consistency signal generator Contradiction to eliminate hallucinations15. Specifically, it
SelfEvaluate(·),Self-UpdatestrategySelfUpdate(·),stoppingcri-
induces prompts to generate two contradictory sentences and
terion stop(·), max iteration T
1: y 0 =M(x) then directs the LLM to resolve the contradictions, retaining
2: i←0 the consistent information to generate a coherent response.
3: while i <T and not stop(y i) do Subsequent sentences follow a similar approach to produce
4: f i =SelfEvaluate(x,y i) a complete reply. Clearly, contradictory information is highly
5: y i+1 =SelfUpdate(x,y 0:i,f 0:i) likely to be hallucinatory, thus effectively mitigating halluci-
6: i←i+1
7: end while nations. This method essentially extends Self-Consistency [2]
8: return y i into the domain of hallucination.
14This section incorporates ideas from RAG, yet given its relevance to
We can also formalize the process in Algorithm 1 as a
Self-Feedback,it’sdelineatedasadistinctlineofwork.
conditional language model, as shown in Eq. 10. 15DemoofSelf-Contradiction:https://chatprotect.ai/JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 15
EVER(REal-TimeVErificationandRectification)[53]em- certainpositions,termedChaoticPoints.Methodsfordetecting
ploys a similarly intuitive approach. When generating a sen- chaotic points include comparing the ratio of maximum to
tence, EVER verifies the accuracy of the generated sentence second-maximumtokenprobabilitiesandensuringthenumber
eitherbytheLLMitselforretrievedexternalinformation,gen- ofcandidatetokensexceedsone,asshowninEqs.12and 13,
eratingfeedbacktomodifythesentenceifthereareissues.The where δ is a probability threshold, γ is a predefined coeffi-
r
modified sentence is then re-appended into the generated text cient, and V denotes the vocabulary. By assessing generated
iteratively. Similarly, PURR (Petite Unsupervised Research content against previously generated segments and potential
and Revision) [101] and RARR (Retrofit Attribution using tokens from chaotic points, scores such as information gain,
Research and Revision) [102] follow a similar approach as weighted uncertainty, and weighted confidence help identify
EVER,wheretheverificationstagereliesonretrievingexternal the most suitable token.
knowledge to provide modification feedback.
(cid:18) (cid:19)
P
In contrast to EVER, FAVA (FAct Vericaton with Augmen- I =I(P )= second ≥δ (12)
tation) [54] adopts a more sophisticated approach. It fine- 1 gen p max r
tunes the model to generate tokens that edit its own content,
significantly enhancing editing efficiency. Their fine-tuning τ =γmaxP (cid:0) w |y0:t−1(cid:1)
dataset includes examples like: “Messi is an <entity><delete w∈V (13)
>Argentine </delete ><mark >Brazilian </mark ></entity I 2 =(cid:0)(cid:12) (cid:12)(cid:8) yt ∈V |P (cid:0) yt |y0:t−1(cid:1) ≥τ(cid:9)(cid:12) (cid:12)>1(cid:1)
>soccer player.” Special tokens enclosed in angle brackets
Thosemethodologiesprimarilyapplytoclosed-bookgener-
are also trained to be generated, effectively eliminating hal-
ation tasks. For open-book generation tasks, current research
lucinations through rendering. The major advantage of this
focuses on leveraging external references to guide decoding.
methodliesingrantingtheLLMmaximumautonomytomake
CAD [56] and ECAD [104] (named ECAD in this review)
mistakes and subsequently correct them freely. Moreover, this
incorporatecontextuallyrelevantorirrelevantknowledgesnip-
approach bears resemblance to Quiet-STaR [82] mentioned in
pets into model inputs, intervening in the decoding process
Section V-A, where both utilize special tokens to represent
through contrastive decoding strategies to bridge the informa-
non-essential cognitive processes.
tiongapbetweenusefulandnon-usefulinformation.Tofurther
exploit valuable information from references, FastMem [105]
C. Decoding Truthfully fine-tunes the final layer of LLMs to quickly memorize
The concepts of “Refining the response iteratively” and “Mit- reference texts during inference, highlighting differences in
igating hallucination while generating” primarily aim to en- decoding with and without references.
hance response consistency. Conversely, Decoding Truthfully In summary, these studies identify internal inconsistency
focuses predominantly on decoding consistency. In recent phenomena or instability in responses to guide LLMs’ proba-
years, several studies have discovered that methods such as bility distributions during decoding.
greedy decoding and sample decoding alone constrain LLMs
from accurately expressing crucial information in natural lan-
D. Activating Truthfulness
guage form within latent layers. Consequently, more complex
Activating Truthfulness focuses on enhancing consistency in
and rational decoding strategies have been designed to elevate
latentlayers.Itscoremethodsinvolveboostingattentionheads
the reliability and accuracy of model’s responses.
and states that represent “truthfulness” within latent layers, or
Li et al. [103] pioneered the Contrastive Decoding strategy,
refining latent layers through fine-tuning, aiming to improve
where during the next token prediction, the optimal token
the model’s internal consistency.
probability is selected by contrasting the token probability
The exploration of latent truthfulness began with CCS
distributions derived from expert and amateur models, as
(Contrast-Consistent Search) proposed by Burns et al. [106].
shown in Eq. 11. This method excels in mitigating biases
CCSinvestigatesmethodsforminingknowledgeembeddedin
or preferences inherent in large-scale models, favoring tokens
latent layers by training a small classification head on Trans-
withhigherprobabilitiesinexpertmodelsandlowerprobabil-
former latent layers. This method effectively activates model
ities in amateur models.
truthfulness, surpassing conventional inference methods.
(cid:32) P (cid:0) yt |y0:t−1(cid:1)(cid:33) Inspired by CCS, scholars from Harvard University de-
yt ∼softmax log EXP (11) veloped the Inference-Time Intervention (ITI) technique [5].
P (yt |y0:t−1)
AMA
This technique involves two main steps: 1. Probe analysis:
Following this pioneering work, researchers have explored Using probe technology16 to identify attention heads in the
various approaches for logit adjustment and contrastive de- model related to truthfulness. 2. Inference-time intervention:
coding. Chuang et al. [55] observed significant differences in Adjusting selected attention heads during the model’s answer
token probability distributions across different layers of the generationprocessbyincreasingtheirweights.Thisguidesthe
model and introduced DoLa to incorporate information from modeltowardsmoretruthfulreasoningpaths.However,ITIhas
previouslayers,enhancingearly-stagecognitivereasoningand
pre-answer consistency, termed Decoding Consistency. 16Aprobeisasmallclassifierwheretheinputislatentstatesandtheoutput
islabelscorrespondingtoatesttask.InITI,thistesttaskisTruthfulQA[16],
UnlikeDoLa,SED[11]andDIVER [57]focusondetecting
where a subset of test tasks trains the probe to identify attention heads
andaddressingdiscrepanciescausedbydifferencesintokensat associatedwithhighertruthfulness(higherclassificationaccuracy).JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 16
limitationsintrainingprobesusingthelasttoken’slatentlayer completeness in this survey, this section briefly summarizes
stateattheendofaquestion-answerpair.Thisapproachlacks these other works adhering to the Self-Feedback framework.
features and turns the problem into discerning truthfulness
ratherthangeneratingtruthfulcontent(generative).Addressing
A. Preference Learning
these issues, TrFr [6] proposed the use of multi-dimensional
Currently, the responses of LLMs in specific domains some-
orthogonal probes, effectively extracting features from both
times fail to satisfy users, even violating human values, social
truthfulandnon-truthfultextstobetteridentifyeffectiveatten-
ethics, and morals. The goal of Preference Learning is to
tion heads. TruthX [7] explored a more efficient intervention
enable LLMs to better follow human instructions and output
strategy.Ittargetsnotonlyattentionheadsbutalsolatentstates
responses that meet human expectations. Most of the work
intheforwardfeedbacklayer.Mappingthesestatesseparately
around this task can be broadly covered by the Self-Feedback
using truthful and semantic encoders significantly reduces the
framework. Specifically, the model generates initial responses
impact on the language model’s overall performance while
during the Self-Evaluation process and then performs Self-
enhancing representations of truthfulness.
Update based on the Feedback Signal. Here, the Feedback
Besides improving truthfulness, what other characteristics
Signal mainly refers to the reward information given by a re-
do latent layers possess? Recently, more work has begun
ward model R, which is trained through preference feedback.
to explore various features within latent layers. This not
Preference feedback involves comparing and ranking differ-
only demystifies model black boxes but also offers poten-
ent responses to the same question in terms of helpfulness,
tial methods to mitigate hallucination issues. For example,
harmlessness, and honesty. The Self-Update here primarily
Wu et al. [107] discovered that some attention heads focus
refers to broadly updating the model M, including methods
more on long-context retrieval capabilities (strong copy-paste
like supervised fine-tuning and reinforcement learning (such
needle abilities). In tests like Needle-in-a-Haystack, blocking
as PPO [111], DPO [112]).
these attention heads results in performance dropping from
There are three main ways to obtain preference feedback.
94.7% to 63.6%. Can enhancing model retrieval heads reduce
The first is through human feedback, as seen in works like
hallucinations in long contexts? This is a question worth
OASST [113] and BeaverTails [114], which include human-
exploring. For another example, teams from Anthropic also
annotated data from multiple annotators. The second method
used dictionary learning to discover monosemanticity within
involves feedback generated by models [115]–[117], offering
LLMs [108], such as “Golden Gate Bridge.” Activating states
lower annotation costs and faster iterative feedback efficiency
corresponding to the “Golden Gate Bridge” in latent layers
compared to human feedback. The third type of feedback is
leads the model to consider itself as the bridge. OpenAI also
derived from inductive bias. For instance, the SHP dataset
proposed Sparse Autoencoder (SAE) [109] to automatically
mentioned in [118] considers upvotes and downvotes to dif-
identify latent features within language models.
ferentiate between good and bad responses. Another example
Iffeatureexplorationweremoreaccessible,couldweidenti-
is ALMoST [119], which proposes some prior rules, such as
fying“human-likeness”featuresthatcanpreventmodelsfrom
largerparametermodelsgenerallyperformingbetter,andmore
mistakenly perceiving themselves as human, thus reducing
prompt context examples yielding higher quality responses.
hallucinations related to cognitive errors? Exploring latent
These rules are used to rank response quality across different
features in language models to subsequently reduce halluci-
models as preference feedback.
nationsfromawhite-boxperspectiveisapromisingdirection.
Basedonpreferencefeedback,wecantrainarewardmodel
However, the main challenge lies in the inefficiency of probe
to output Feedback Signals. There are two common types
technology, dictionary learning, SAE, and similar methods,
of reward models. One is the Reward Model proposed in
which require training many classifiers to effectively extract
InstructGPT [120], with the loss function as shown in Eq. 14.
latent features from models.
Here, r (x,y) represents the output of the Reward Model,
In addtion to implicit feature mining methods, Activating θ
and response y is ranked higher than y . However, this
Truthfulness also includes truthfulness-oriented fine-tuning w l
method’s downside is that the overall score distribution for
methods that essentially explore consistency within latent
high-quality and low-quality responses is similar, making it
states. For instance, Tian et al. [110] proposed sampling
difficult to effectively distinguish between different responses
multiple responses from an LLM and using Self-Evaluation
todifferentquestions.Toaddressthis,Xuetal.[121]proposed
to identify more factual responses. Collecting these QA pairs
an evaluation model that directly scores QA pairs.
and retraining the original LLM aligns with the fine-tuning
approach mentioned in Section V-B, though it is not the z =σ(r (x,y )−r (x,y ))
θ w θ l
primary focus of this section. 1 (14)
loss(θ)=− E [log(z)]
(cid:0)k(cid:1) (x,yw,yl)∼D
2
VII. TASK:OTHERS
B. LLM-Based Knowledge Distillation
Apart from reasoning elevation and hallucination alleviation,
several lines of work follow the Self-Feedback framework, Small-parameter LLMs have faster inference and training
although their objectives are not to improve the model’s speeds, but their reasoning capabilities are inferior to those
internal consistency. Examples include knowledge distillation, of large-parameter LLMs. LLM-based knowledge distillation
preference learning, and data augmentation. For the sake of methodsaimtotransferadvancedcapabilitiesfromproprietaryJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 17
LLMs(suchasGPT-4)tosmall-parameteropen-sourcemodels VIII. EVALUATION
(such as LLaMA and Mistral) [27]. These two models can be
Evaluation helps identify the strengths and weaknesses of
referredtoasthe“teachermodel”andthe“studentmodel”re-
different models and methods. This section lists evaluation
spectively,withtheteachermodelguidingthestudentmodelto
methods and benchmarks for Internal Consistency and Self-
enhance its capabilities, fitting the generalized Self-Feedback
Feedback. These evaluations mainly cover two types of abil-
frameworkproposedinthispaper.DuringtheSelf-Evaluation,
ities: meta ability, such as model’s uncertainty, consistency,
the student model generates answers, which are then assessed
and feedback ability; and common ability, which pertains to
by the teacher model. In the Self-Update, the student model
solving real-world problems, such as reasoning QA tasks and
uses the evaluation signal to update itself or its answers.
codegenerationtasks.Metaevaluationhelpsusidentifywhich
This signal can be in the form of statistical metrics, such
LLMs are more promising for solving complex problems,
as MiniLLM [122] calculating the reverse Kullback-Leibler
while common evaluation helps us understand which Self-
(KL) divergence of the probability distributions output by
FeedbackmethodscanbettersolveproblemsforagivenLLM.
the student and teacher models; or GKD [123] computing
metrics like forward KL divergence, reverse KL divergence,
and generalized JSD. The signal can also be in the form A. Meta Evaluation
of natural language feedback, such as Selfee [124] utilizing
We summarize five meta evaluation methods. These methods
ChatGPT as the teacher to provide textual feedback on the
cangenerallybecategorizedintometric-basedandbenchmark-
outputs of the student model; or in PERsD [77], where the
based paradigms. The former constructs mathematical for-
teacher executes the code generated by the student model and
mulas or metrics to directly calculate the performance of a
provides specific suggestions based on errors.
particular aspect; the latter uses QA datasets to empirically
When the teacher and student models are the same LLM,
measure performance. We summarize benchmark-based meta
this leads to Self-Knowledge Distillation (Self-KD). In Self-
evaluations in Table V.
KD, the model iteratively updates its capabilities using the
knowledge it gradually accumulates during training, falling
TABLEV
under the narrow Self-Feedback paradigm. For example, the
METAEVALUATIONBENCHMARKS
goal of Impossible distillation [125] is to obtain a Stronger
Paraphraser. In the Self-knowledge distillation process, it Type Benchmark Organization
evaluates its paraphrase results from perspectives such as
Uncertainty LLM-Uncertainty-Bench[129] Tencent
semantics, format, and diversity, and further refines high- Uncertainty UBench[130] Nankai
quality data to fine-tune itself accordingly. Consistency ConsisEval[131] PKU
Consistency PopQA-TP[132] IBM
Consistency ParaRel[133] BIU
C. Data Augmentation Consistency BMLAMA[134] RUG
Consistency BECEL[135] Oxford
Data Augmentation aims to construct and filter high-quality
CritiqueAbility CriticBench[136] THU
datasets using LLMs. It is somewhat similar to the methods Self-Knowledge SelfAware[4] Fudan
in Sections VII-A and VII-B that combine Feedback infor- Self-Knowledge Idk(Idon’tknow)[35] Fudan
Self-Knowledge Self-KnowledgeEvaluation[137] THU
mation to create datasets, but there are slight differences in
focus and specific forms. The latter focuses on the model’s
capabilities, using datasets during the Self-Update stage for Metric-Based Uncertainty Evaluation. As mentioned in
model fine-tuning, with most methods falling under narrow Section IV-A, uncertainty estimation involves assessing the
Self-Feedback.Incontrast,DataAugmentationfocusesonthe uncertainty of a model’s specific response. Uncertainty eval-
dataset itself, updating the model’s responses during the Self- uation, on the other hand, measures the overall uncertainty
Update stage to further refine the dataset, with most methods of a model. Key metrics for evaluating model uncertainty
falling under generalized Self-Feedback. include: Expected Calibration Error (ECE), which assesses
Self-instruct [126] is a typical example, where the LLM the expected difference between model confidence and accu-
generates new task instructions during the Self-Evaluation racy; Maximal Calibration Error (MCE), which indicates the
stage and generates input-output instances based on the new maximum deviation between model accuracy and confidence;
instructions. It calculates the ROUGE-L metric between the and Brier Score (BS), which is used to assess how closely
new instructions and existing instructions as the Feedback the model’s predicted probabilities align with the true class
signal. Finally, during the Self-Update stage, it filters and probabilities [25].
screens the newly generated set of instructions. Benchmark-Based Uncertainty Evaluation. LLM-
Currently, methods applying LLMs to Data Augmentation Uncertainty-Bench [129] extracts five test tasks (including
andSyntheticDataGenerationmainlyfocusontheprompten- question answering, reading comprehension, commonsense
gineering layer. In other words, Self-Evaluation only involves inference, dialogue response selection, and document
responses. Many studies have shown that LLM responses are summarization) from common benchmark datasets and uses
highly sensitive to prompt variations [127], [128]. Therefore, conformal prediction techniques to construct benchmarks.
the main bottleneck in this task is: how to design better UBench [130] also extracts data from other datasets,
prompts and how to deeply explore the relationship between totaling 3978 multiple-choice questions covering knowledge,
decoding, latent space, and data quality. language, understanding, and reasoning abilities. UBenchJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 18
evaluates individual data items by having models textually TABLEVI
express uncertainty scores. COMMONEVALUATIONBENCHMARKS
Benchmark-Based Consistency Evaluation. This line of
Type Benchmark Organization
work centers on assessing whether a model delivers con-
Knowledgereasoning C-Eval[138] SJTU
sistent responses to queries that are semantically equivalent
Knowledgereasoning MMLU[15],[139] UCB
but phrased differently. The key focus is on developing a Logicreasoning BBH[140] Google
variety of synonymous queries to test the model’s reliability. Logicreasoning ARC[141] AI2
Linguisticunderstanding WiC[142] Cambridge
Forinstance,theConsisEvalBenchmark[131]createssimpler
Codegenerating HumanEval[143] N/A
synonymous queries for each question. PopQA-TP [132] and MathSolving MATH[144] UCB
ParaRel [133] construct synonymous queries through rephras- MathSolving GSM8K[34] OpenAI
ing. BMLAMA [134] focuses on multilingual consistency,
constructing a parallel corpus of queries. BECEL [135] draws
inspiration from behavioral consistency, considering higher- comparing the probabilities of different option tokens during
order consistency in model responses by creating seman- LLM decoding. However, many researchers pointed out the
tic consistency data, negational consistency data, symmetric drawbacks of this method, namely that LLMs tend to choose
consistency data, etc. Notably, most studies have found that the first option, leading to poor self-consistency and not
models generally exhibit low consistency. reflecting the LLM’s true ability [145], [146]. Text generation
Benchmark-Based Critique Abilitiy Evaluation. Lin et tasks, such as math problems and code generation, mitigate
al. [136] collect a large number of QA pairs from 15 datasets the shortcomings of multiple-choice questions by allowing
across mathematical, commonsense, symbolic, coding, and the LLM to perform freely, better reflecting the model’s true
algorithmic fields, creating CriticBench through model gen- ability.However,thisevaluationmethodstrugglestoprecisely
eration and human annotation. It can be used to evaluate the quantifythegapbetweengeneratedtextandreferenceanswers
ability of LLMs to generate critiques, an important aspect of in dimensions such as syntax and semantics. It is evident that
the Self-Feedback framework. current Benchmarks for common evaluation are still far from
Benchmark-Based Self-Knowledge Evaluation. Self- accurately measuring the true capabilities of LLMs and have
KnowledgereferstotheLLM’sunderstandingandrecognition many areas for improvement.
of its own abilities, limitations, and the content it creates. Yin
et al. [4] and Cheng et al. [35] construct sets of unanswerable IX. DOESSELF-FEEDBACKREALLYWORK?
questions to explore the question “Do large language models
A. Conflicting Viewpoints
know what they do not know?” Tan et al. [137] investigate
“Does the model truly understand the questions and solutions Allmethodsinthissurveysuggestingthatmodelscangenerate
it creates?” These studies generally yield negative empirical proper Feedback Signals to optimize themselves. However,
results, indicating that models have weak Self-Knowledge. withtheemergenceofaseriesofworksprefixedwith“Self-”,
questions about their feasibility have also arisen: Can models
B. Common Evaluation really self-correct, self refine, etc.?
Jiangetal.[147]proposetheSELF-[IN]CORRECThypoth-
In most works utilizing the Self-Feedback framework, they
esis. It experimentally verifies in the context of QA tasks that
do not conduct meta evaluations in experiments but rather
the “accuracy of generating initial answers (generative)” is
assess the model’s ability to solve real-world problems, such
higherthanthe“accuracyofjudgingthecorrectnessofitsown
as solving math problems, code generation, and summarizing
generated answers (discriminative)”. This indicates models
articles. Here, we classify and summarize the popular bench-
struggletoassesstheirowncontentaccurately.Theyarguethat
marks based on the focus of the evaluation tasks17, as shown
modelsformadefinitivebeliefaboutaparticularchoiceduring
inTableVI.Therearetwotypesofreasoning:Knowledgerea-
generation, which refutes works like Refining with Responses
soning focuses on the LLM’s ability to solve problems using
andRefiningtheResponseIteratively.However,thisworkcan
its parameterized knowledge, while Logic reasoning focuses
only refute studies in the domain of QA tasks. Moreover,
on complex logical reasoning based on prompts. Linguistic
the model’s discriminative ability is not always greater than
understanding focuses on the LLM’s analysis of language
its generative ability, so it cannot be concluded that various
meaning. Code generating and Math Solving respectively test
methods cannot achieve self-correction.
the LLM’s ability to write code and solve math problems.
Stechlyetal.[148]andValmeekametal.[149]usedsimilar
Currently, these Benchmark evaluation formats mainly in-
strategies to test whether GPT-4 possesses the Self-Feedback
cludemultiple-choicequestionsandtextgeneration,eachwith
capability.Stechlyetal.[148]usedtheGraphColoringtaskto
its own advantages and disadvantages. For multiple-choice
test and found that in work similar to Refining the Response
questions, evaluators can accurately calculate metrics such
Iteratively,GPT-4almostalwaysfailstoverifythecorrectness
as accuracy by extracting answer text from the response or
of its own solutions but acknowledges methods like Self-
17We may see the same benchmark categorized differently in different Consistency, which suggest that the model can select appro-
sources, highlighting the difficulty in achieving fully orthogonal classifica- priatesolutionsfrommultipleoptions.Incontrast,Valmeekam
tions.Forexample,adatasetevaluatinglinguisticunderstandingabilitymay
et al. [149] tested similar strategies in the task planning
also involve reasoning tasks. Here, we classify each dataset based on its
primaryfocus. domain, refuting the feasibility of both Refining the ResponseJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 19
Iteratively and Multi-Agent Collaboration. Both Stechly et • Self-Refine [8] (Refining the Response Iteratively) itera-
al.[148]andValmeekametal.[149]believethatmodelshave tively optimizes its responses, which is an implicit way
difficulty making correct judgments about their own outputs. to achieve high-consistency responses.
The above criticisms often focus on narrow Feedback Sig- • Self-Contradict [21] (Mitigating Hallucination while
nals. In comparison, the criticisms from Huang et al. [96] Generating) eliminates self-contradictory content in its
are more reasonable. Huang et al. [96] specifically refute the responses, naturally improving consistency.
effectiveness of three works (Reflexion [52], Multi-Agent De- • DoLa [55] (Decoding Truthfully) compares the proba-
bate[73],andSelf-Refine[8])throughreasonablecomparisons bility distributions across different layers of the model’s
and comprehensive experiments. Reflexion relies on external latent layers and reduces their discrepancies through
golden truth as a stopping condition for iterative refining comparison, thereby improving the model’s consistency.
during self-correction, which is an unreasonable setup—if • ITI [5] (Activating Truthfulness) implicitly optimizes
thereisalreadyagoldentruth,thereisnoneedforpredictions. the model’s Internal Consistency by identifying attention
For Multi-Agent Debate, the authors’ experiments found that heads that prefer factual information.
this method is significantly inferior to the Self-Consistency
strategy and consumes substantial memory. For Self-Refine,
C. Does Internal Consistency Mean Correctness?
the authors found that the prompts used for initial results and
To answer this question, let’s revisit the relationship between
refining were unfair, and they generated a fairer prompt that
world knowledge, corpus, and language models, as shown in
produced better responses in one go.
Fig.9.Worldknowledgeistheconsensual(correct)knowledge
Duetothedifferentcharacteristicsofvariousworks,Kamoi
we humans possess. The training corpus used for models is a
et al. [24] provide a more comprehensive analysis by con-
true subset of world knowledge, containing the vast majority
structingaclearclassificationmethodandsystematicallycom-
of correct knowledge and a small portion of uncleanable
paring the strengths and weaknesses of each types of work.
erroneous knowledge. Additionally, the knowledge embedded
Theysuggestthattheabilitytoself-correctshouldbediscussed
in the corpus is deterministic, where each statement in the
according to the specific task. For example, for tasks with
corpushasaprobabilityof100%.Languagemodels,byfitting
decomposable responses18 or verifiable tasks19, it is feasible
the corpus, acquire higher-order probabilistic representations
for the model to optimize itself.
of this knowledge, and the probabilistic nature makes the
Although the survey [24] provides more meaningful view-
learned knowledge vague and non-deterministic, as illustrated
points through classified discussions, it complicates the field,
by the shaded areas in Fig. 9. Vagueness (or hallucination)
makingitdifficulttoformasystematicframework.Benefiting
is an important characteristic of language models. It enables
from the perspective of Internal Consistency and the clear
the generation of novel and creative expressions outside the
boundarydiscussionsinSectionI-E,weconductamoremean-
training corpus distribution. However, from a reliability per-
ingful discussion on the proposed Self-Feedback framework:
spective, vagueness is a disaster. Vagueness means that an-
1) Does Self-Feedback improve Internal Consistency? swers to the same question are uncertain, making the model’s
2) Does Internal Consistency mean correctness? expressions inconsistent.
B. Does Self-Feedback Improve Internal Consistency?
For this question, the different lines of work proposed in
this paper actually provide affirmative answers from various Training
perspectives. For example,
• Self-Consistency[2](ReasoningTopologically)improves
response consistency by finding the majority vote from
the sampling set. Corpus Model
• Self-Improve [46] (Refining with Responses) also uses
majority voting to find better responses from the sam- World Knowledge High Aligned with Low High Aligned with
Certainty world knowledge Certainty Certainty world knowledge
pling set and then fine-tunes the model itself, improving
Knowledge that is not High Mis-aligned with Low High Mis-aligned with
consistency from the parameter level. in the corpus or model Certainty world knowledge Certainty Certainty world knowledge
• Multi-Agent Debate [73] (Multi-Agent Collaboration) Fig.9. WorldKnowledge,TrainingCorpusandLanguageModel
involvesmultiplemodelsparticipatinginansweringques-
tions simultaneously, which is essentially a generalized Therefore, we need to improve Internal Consistency and
way to obtain consistent responses. eliminate vagueness within the model to enhance its confi-
dence in correct knowledge. However, eliminating vagueness
18Forinstance,inthequestion“Whoaresomepoliticianswhowereborn also means that the model will be equally confident in er-
inBoston?”,thefeedbackgenerationcanbedecomposedintoeasiersub-tasks roneous knowledge. This raises a question: does enhancing
ofverifyingeachanswer. consistency yield overall benefits or drawbacks? The advan-
19Forexample,intheGameof24,atasktofindarithmeticoperationsto
tage is that when preprocessing and cleaning the pre-training
obtain 24 using four provided integers, generating a feasible solution might
beharderthanverification. corpus, the intention is to align it towards world knowledgeJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 20
(correct knowledge). Hence, we propose the “Consistency Is selecting comparison baselines, first determine the sub-
(Almost) Correctness” hypothesis. directionforyourworkandchooseimportantworkwithin
thatdirectionascomparisonbaselines.Comparingacross
differentlinesofworkcanleadtounfaircomparisonsdue
Consistency Is (Almost) Correctness
to significant methodological differences.
Enhancing a language model’s internal consistency ac- • Experiment Settings. Unrealistic experimental task set-
tivates its cognitive certainty, reinforcing both correct tings will not advance scientific research. For instance,
and erroneous knowledge. However, because the pre- requiringpre-givengoldenlabeltomakepredictionsdoes
training corpus is predominantly aligned with cor- not fit real-world needs [96].
rect world knowledge, improving consistency tends to • Prompt Engineering. As mentioned in Section IX-A,
amplify correct content more than incorrect content. manyworksexhibitaprompttuningphenomenon,where
Consequently, increased internal consistency generally adjusting the prompt can reverse experimental results.
results in improved overall correctness. Therefore, we propose that prompt templates must be
disclosed in the paper or source code and provide clear
However, why do some opposing voices believe that im- usageinstructions;verifytherobustnessofprompts,such
proving consistency cannot enhance the model’s correctness? asusingvarioustemplatesforexperiments;andverifythe
We believe this is closely related to the testing tasks. Many generality of prompts, such as using multiple different
works refuting Self-Feedback use testing tasks that lie in the LLMs for experiments.
shadedareasofFig.9(e.g.,unstatedpuzzlesnotinthetraining
corpus or questions unsolvable without external knowledge). X. FUTUREDIRECTIONSANDCHALLENGES
Models struggle to effectively Self-Evaluate and Self-Update
A. Textual Self-Awareness
for tasks beyond their generalization capability.
In summary, within-distribution capabilities, the Self- Human speech often lacks consistency and certainty in ex-
Feedback framework can enhance model consistency by re- pressing viewpoints. However, we typically use phrases like
inforcing the model’s fit to corpus priors, thereby eliminat- “I’mnotsure,butIthink”or“Ibelievethere’san80%chance”
ing uncertainty and improving consistency. According to the to hedge, demonstrating our good self-awareness. Yona et
“Consistency Is (Almost) Correctness” hypothesis, this leads al. [152] proved that current models still cannot verbally and
to an overall improvement in the model’s performance. faithfully express their uncertainty. Kapoor et al. [153] found
similar issues and showed through experiments that models
can achieve good calibration only after fine-tuning. How to
D. Appeals
enable models to utilize the available Internal Consistency
Currently, there are many chaos in this field: using similarly signal to help textually express their self-awareness is a
expressed names, proposing rare or unrealistic tasks, using promising direction [154].
different benchmarks for the same task, and comparing differ-
ent baselines. In summary, without clear debate topics, many
B. The Reasoning Paradox
workscontradicteachother,resultinginconfusingconflictsof
views. To avoid these confusions, we propose several appeals. As mentioned in Section II-B, there is a paradox between
• Naming. When proposing new methods, ensure the reasoning done during single token prediction (latent
names are appropriate and avoid conflicts (e.g., Self- reasoning [33]) and the reasoning done using multiple tokens
Improve [46], Self-Improvement [88]). Additionally, in language (explicit reasoning, e.g., CoT).
when classifying one’s work, consider the accuracy of
the direction name. For instance, uncertainty estimation The Paradox of Latent and Explicit Reasoning
and confidence estimation have different directional
indicators; the former leans towards mechanistic Language models excel in latent reasoning when de-
explanation, while the latter leans towards application. coding a single token, effectively utilizing attention
• Task Definition. Some work has begun to notice that mechanismsanddeepfeatureinteractionstoachieveac-
reasoning elevation and hallucination alleviation are sim- curate reasoning. However, single tokens can’t answer
ilar tasks. For example, [150], [151] propose methods complex questions. Explicit reasoning, which involves
that focus on both tasks. We advocate for using Internal generating a sequence of tokens (e.g. CoT), enhances
Consistency Mining to refer to these two tasks, ensuring the model’s problem-solving capabilities. Yet, lengthy
terminological standardization. reasoning chains and inherent noise in text disrupt
• Reasoning and Hallucination. Regarding the use of the model’s latent reasoning. Thus, there is a paradox
thesetwoterms,wesuggestthatwhendealingwithtasks between latent reasoning and explicit reasoning.
like QA, we can say that a method may lack reasoning
ability; whereas for open-ended generation tasks, we can Therefore, we need to study the equilibrium point between
use exhibit hallucination. latent and explicit reasoning, enabling efficient use of reason-
• Selection of Baselines. This paper summarizes some ing resources and improving the model’s reasoning efficiency.
relatively fixed lines of work, suggesting that when Currently, there is little research on this issue.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 21
C. Dive Deeper can help researchers locate their work’s position within a vast
system and facilitate reasonable experimental comparisons.
From the seven lines of work we summarized, many works
Finally, we include three critical topics: relevant evaluation
optimizeonlyattheresponselayer.However,thisapproachre-
methods and benchmarks, exploring whether Self-Feedback
liesonexperienceandishighlysensitivetoprompttemplates.
truly works, and future research directions. In summary, this
Moreover, the low entry barrier and extensive participation
paper attempts to use a deeper research perspective (Internal
in such work have led to an influx of low-quality papers.
Consistency) and a more general framework (Self-Feedback)
Therefore,weencourageresearcherstodelveintothedecoding
to summarize a series of important works on reasoning eleva-
layer and latent layer, exploring more universal discoveries
tion and hallucination alleviation.
from an interpretability perspective.
ACKNOWLEDGMENTS
D. The Unified Perspective
This work was supported by the National Natural Science
At present, the focus of work in this field is relatively
Foundation of China (Grants No. 62072463, 71531012), the
narrow, lacking a comprehensive understanding of the entire
National Social Science Foundation of China (Grants No.
field, and consequently, there are no more general framework
18ZDA309), the Research Seed Funds of the School of In-
works. We believe that using the perspective proposed in this
terdisciplinary Studies at Renmin University of China, and
paper,consideringproblemsfromtheresponse,decoding,and
the Opening Project of the State Key Laboratory of Digital
latent layers in a unified manner, can better facilitate Internal
Publishing Technology of the Founder Group.
Consistency Mining. There are emerging efforts that begin to
integrate multiple layers. For example, Xie et al. [36] start
APPENDIXA
from the response layer and reflects on how different CoT
NOTATIONS
paths guide the consistency of the latent layer; Xie et al. [74]
use Self-Evaluation strategies at the response layer to guide
better decoding strategies. TABLEVII
COMMONNOTATIONS
E. The Comprehensive Evaluation Symbol Description
Different LLMs, combined with various Self-Feedback strate- x Query
M,N LLMs
gies, can produce vastly different combinations. However, as
e Expressiontype,e∈{response,decoding,latent}
explained in Section VIII, current evaluation methods gen- Oe(M,x) Samplingdistribution
erally have a singular focus, making it difficult to compre- Y Samplingset
hensively and conveniently understand the model’s capabili-
yi Thei-thelementinthesamplingset
y0:i Elementsfrom0toiinthesamplingset
ties. Therefore, building a complete evaluation system from yt Thet-thtokenintexty
meta evaluation to common evaluation, from latent states to f ConsistencySignalofSelf-Feedback
P(y|x;θ) Probability that model parameterized by θ outputs y
response, from benchmark to metric, and from uncertainty to
giveninputx
feedback is a worthy consideration.
XI. LIMITATIONS
APPENDIXB
Given the terminological confusion and overlapping lines of
THEFIVERESPONSESOFGPT-4O
workinthisfield,thesectionsofthispaperarenotnecessarily
Fig. 10, 11, 12, 13, and 14 show five different responses
orthogonal.Consequently,asingleworkmayfallintomultiple
obtained when querying GPT-4o: How many full stops (peri-
categories, indicating that the method employs ideas from
ods) are there: “.!..!..!”. The experiments were conducted on
different lines of work. The writing design is more inspira-
June 14, 2024, accessed via the official website20.
tional, helping researchers to answer: What is the position of
their work in a grand system? What are its advantages and
disadvantages?Whatshouldbethenextstepindesigningnew GPT-4oResponse1
and better methods?
Todeterminethenumberoffullstops(periods)inthestring”.!..!..!”,
weneedtocounttheoccurrencesofthecharacter’.’.
XII. CONCLUSION Let’sbreakdownthestring:
1.”.!”:1fullstop
ThispaperproposesusinganInternalConsistencyperspective 2.”..!”:2fullstops
to observe the most prominent phenomena in the field of 3.”.!”:1fullstop
Summingtheseup:
LLMs: lack of reasoning and the presence of hallucinations.
1+2+1=4
The article explains the modeling of Internal Consistency, the Therefore,thereare4fullstopsinthestring”.!..!..!”.
hourglass evolution pattern, the current status, sources, and
significance from multiple aspects, and proposes the Self-
Fig.10. GPT-4oResponse1
Feedback framework for Internal Consistency Mining. We
summarize the various tasks and distinctive lines of work
involvedintheSelf-Feedbackframework.Theselinesofwork 20https://chat.openai.com/JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 22
GPT-4oResponse2 TABLEVIII
LATENTCONSISTENCY
Thesequence”.!..!..!”containsthreeinstancesof”.”(fullstopsor
periods). Each segment ”.!” has one full stop, and there are three Setting SelectedToken
suchsegments.
Fixattn0;Zerooutattni̸=0 0
0 0
Fixattn16;Zerooutattni̸=16 0
0 0
Fig.11. GPT-4oResponse2 Fixattn0 15;Zerooutattni 1̸= 50 5
Fixattn16;Zerooutattni̸=16 4
15 15
Fixattn0 ;Zerooutattni̸=0 4
30 30
GPT-4oResponse3 Fixattn16;Zerooutattni̸=16 4
30 30
Thestring”.!..!..!”containsatotalof**3**fullstops(periods).
TABLEIX
DECODINGCONSISTENCY
Fig.12. GPT-4oResponse3
DecodingStrategy SelectedToken
GreedyDecoding 4
GPT-4oResponse4
BeamSearchDecoding(beam=5) 4
SamplingDecoding 3
The string ”.!..!..!” contains full stops (periods). To determine the
Top-kSamplingDecoding(k=50) 4
numberoffullstopsinthisstring,wesimplycounttheoccurrences
Top-pSamplingDecoding(p=0.95) 4
oftheperiodcharacter(’.’).Let’sanalyzethestring:
-Thefirstcharacterisaperiod.
-Thesecondcharacterisanexclamationmark.
TABLEX
-Thethirdcharacterisaperiod.
-Thefourthcharacterisanexclamationmark.
RESPONSECONSISTENCY
-Thefifthcharacterisaperiod.
-Thesixthcharacterisanexclamationmark. ModelResponse
-Theseventhcharacterisaperiod.
RESPONSE 1:Thereisaperiodafterthefirstdot,thenanotherperiod
-Theeighthcharacterisanexclamationmark.
aftertheseconddot,andsoon.So,thereare5periodsintotal.
Countingtheperiods:
RESPONSE2:Thereisonefullstopafterthefirst”!”,thenanotherafter
1.Thefirstperiodisatposition1.
thesecond”!”,andanotherafterthethird”!”.So,thereare3fullstops
2.Thesecondperiodisatposition3.
intotal.
3.Thethirdperiodisatposition5.
RESPONSE3:Thereisonefullstopafterthefirst”!”,thenanotherafter
4.Thefourthperiodisatposition7.
thesecond”!”,andsoon.So,thereare3fullstops.
Therefore,thereare4fullstops(periods)inthestring”.!..!..!”.
RESPONSE 4: There is one period at the end of the first part, then
another after the second part, and finally one more after the third part.
So,thereare3periodsintotal.
Fig.13. GPT-4oResponse4 RESPONSE 5: There is 1 period, then another one, and another one...
So,thereare3fullstops!
GPT-4oResponse5
Thesequence”.!..!..!”containsthefollowingcharacters: REFERENCES
1.Period(.)
2.Exclamationmark(!) [1] W.X.Zhao,K.Zhou,J.Li,T.Tang,X.Wang,Y.Houetal.,“Asurvey
3.Period(.) oflargelanguagemodels,”arXivpreprintarXiv:2303.18223,2023.
4.Exclamationmark(!) [2] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi,
5.Period(.) S. Narang et al., “Self-consistency improves chain of thought
6.Exclamationmark(!) reasoning in language models,” in The Eleventh International
Tocountthefullstops(periods): Conference on Learning Representations, 2023. [Online]. Available:
1.Thefirstcharacterisaperiod. https://openreview.net/forum?id=1PL1NIMMrw
2.Thethirdcharacterisaperiod. [3] P. Mondorf and B. Plank, “Beyond accuracy: Evaluating the rea-
3.Thefifthcharacterisaperiod. soning behavior of large language models–a survey,” arXiv preprint
Thereareatotalof**three**periodsinthesequence”.!..!..!”. arXiv:2404.01869,2024.
[4] Z. Yin, Q. Sun, Q. Guo, J. Wu, X. Qiu, and X. Huang, “Do large
language models know what they don’t know?” in Findings of the
Association for Computational Linguistics: ACL 2023, A. Rogers,
Fig.14. GPT-4oResponse5 J.Boyd-Graber,andN.Okazaki,Eds. Toronto,Canada:Association
for Computational Linguistics, Jul. 2023, pp. 8653–8665. [Online].
Available:https://aclanthology.org/2023.findings-acl.551
[5] K.Li,O.Patel,F.Vie´gas,H.Pfister,andM.Wattenberg,“Inference-
APPENDIXC
time intervention: Eliciting truthful answers from a language model,”
EXPERIMENTDETAILSOFTHREETYPESOFCONSISTENCY in Thirty-seventh Conference on Neural Information Processing
Systems, 2023. [Online]. Available: https://openreview.net/forum?id=
Thesetupsandresultsofthecomparativeexperimentsonthree aLLuYpn83y
different types of consistency are shown in Tables VIII, IX, [6] Z. Chen, X. Sun, X. Jiao, F. Lian, Z. Kang, D. Wang et al.,
and X. Here, Fix attn16 refers to keeping the 16th attention “Truth forest: Toward multi-scale truthfulness in large language
15 models through intervention without tuning,” Proceedings of the
head in the 15th layer unchanged, while zero out attni̸=16 AAAI Conference on Artificial Intelligence, vol. 38, no. 19, pp.
15
denoteszeroingoutallattentionheadsinthe15thlayerexcept 20967–20974, Mar. 2024. [Online]. Available: https://ojs.aaai.org/
index.php/AAAI/article/view/30087
the 16th one. The source code for this experiment is available
[7] S. Zhang, T. Yu, and Y. Feng, “Truthx: Alleviating hallucinations by
in our open-source GitHub repository. editinglargelanguagemodelsintruthfulspace,”inProceedingsoftheJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 23
62thAnnualMeetingoftheAssociationforComputationalLinguistics [27] X. Xu, M. Li, C. Tao, T. Shen, R. Cheng, J. Li et al., “A survey
(Volume1:LongPapers). AssociationforComputationalLinguistics, on knowledge distillation of large language models,” arXiv preprint
2024.[Online].Available:https://arxiv.org/abs/2402.17811 arXiv:2402.13116,2024.
[8] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe [28] R.Jiang,K.Chen,X.Bai,Z.He,J.Li,M.Yangetal.,“Asurveyon
etal.,“Self-refine:Iterativerefinementwithself-feedback,”inThirty- humanpreferencelearningforlargelanguagemodels,”arXivpreprint
seventhConferenceonNeuralInformationProcessingSystems,2023. arXiv:2406.11191,2024.
[Online].Available:https://openreview.net/forum?id=S37hOerQLB [29] T. Schick and H. Schu¨tze, “Generating datasets with pretrained
[9] S. Welleck, X. Lu, P. West, F. Brahman, T. Shen, D. Khashabi language models,” in Proceedings of the 2021 Conference on
et al., “Generating sequences by learning to self-correct,” in The Empirical Methods in Natural Language Processing, M.-F. Moens,
EleventhInternationalConferenceonLearningRepresentations,2023. X. Huang, L. Specia, and S. W.-t. Yih, Eds. Online and
[Online].Available:https://openreview.net/forum?id=hH36JeQZDaO Punta Cana, Dominican Republic: Association for Computational
[10] J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter, F. Xia Linguistics, Nov. 2021, pp. 6943–6951. [Online]. Available: https:
etal.,“Chainofthoughtpromptingelicitsreasoninginlargelanguage //aclanthology.org/2021.emnlp-main.555
models,” in Advances in Neural Information Processing Systems, [30] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi et al., “Retrieval-
A.H.Oh,A.Agarwal,D.Belgrave,andK.Cho,Eds.,2022.[Online]. augmented generation for large language models: A survey,” arXiv
Available:https://openreview.net/forum?id= VjQlMeSB J preprintarXiv:2312.10997,2023.
[11] Z. Luo, H. Han, H. Zhao, G. Jiang, C. Du, T. Li et al., “Sed: [31] A.Tarski,Introductiontologic:Andtothemethodologyofdeductive
Self-evaluation decoding enhances large language models for better sciences. OxfordUniversityPress,1941.
generation,”arXivpreprintarXiv:2405.16552,2024. [32] J. Sun, C. Shaib, and B. C. Wallace, “Evaluating the zero-shot
[12] M.Nezhurina,L.Cipolina-Kun,M.Cherti,andJ.Jitsev,“Aliceinwon- robustness of instruction-tuned language models,” in The Twelfth
derland:Simpletasksshowingcompletereasoningbreakdowninstate- InternationalConferenceonLearningRepresentations,2024.[Online].
of-the-art large language models,” arXiv preprint arXiv:2406.02061, Available:https://openreview.net/forum?id=g9diuvxN6D
2024. [33] S. Yang, E. Gribovskaya, N. Kassner, M. Geva, and S. Riedel, “Do
[13] Y.Zhang,S.Mao,T.Ge,X.Wang,A.deWynter,Y.Xiaetal.,“Llm large language models latently perform multi-hop reasoning?” arXiv
asamastermind:Asurveyofstrategicreasoningwithlargelanguage preprintarXiv:2402.16837,2024.
models,”arXivpreprintarXiv:2404.01230,2024.
[34] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser
[14] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu et al., “Siren’s song
etal.,“Trainingverifierstosolvemathwordproblems,”arXivpreprint
in the ai ocean: a survey on hallucination in large language models,”
arXiv:2110.14168,2021.
arXivpreprintarXiv:2309.01219,2023.
[35] Q.Cheng,T.Sun,X.Liu,W.Zhang,Z.Yin,S.Lietal.,“Canaiassis-
[15] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song
tantsknowwhattheydon’tknow?”arXivpreprintarXiv:2401.13275,
et al., “Measuring massive multitask language understanding,” in
2024.
InternationalConferenceonLearningRepresentations,2021.[Online].
[36] Z. Xie, J. Guo, T. Yu, and S. Li, “Calibrating reasoning in language
Available:https://openreview.net/forum?id=d7KBjmI3GmQ
models with internal consistency,” arXiv preprint arXiv:2405.18711,
[16] S. Lin, J. Hilton, and O. Evans, “TruthfulQA: Measuring how
2024.
modelsmimichumanfalsehoods,”inProceedingsofthe60thAnnual
[37] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni
Meeting of the Association for Computational Linguistics (Volume
etal.,“Lostinthemiddle:Howlanguagemodelsuselongcontexts,”
1: Long Papers), S. Muresan, P. Nakov, and A. Villavicencio, Eds.
TransactionsoftheAssociationforComputationalLinguistics,vol.12,
Dublin, Ireland: Association for Computational Linguistics, May
pp.157–173,2024.
2022, pp. 3214–3252. [Online]. Available: https://aclanthology.org/
[38] B.Liu,J.T.Ash,S.Goel,A.Krishnamurthy,andC.Zhang,“Exposing
2022.acl-long.229
attentionglitcheswithflip-floplanguagemodeling,”inThirty-seventh
[17] J. Zhang, X. Wang, W. Ren, L. Jiang, D. Wang, and K. Liu, “Ratt:
ConferenceonNeuralInformationProcessingSystems,2023.[Online].
Athought structure for coherent and correct llmreasoning,” arXiv
Available:https://openreview.net/forum?id=VzmpXQAn6E
preprintarXiv:2406.02746,2024.
[39] M. Zhang, O. Press, W. Merrill, A. Liu, and N. A. Smith,
[18] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:
“How language model hallucinations can snowball,” arXiv preprint
A large scale distantly supervised challenge dataset for reading
arXiv:2305.13534,2023.
comprehension,” in Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), [40] E.M.Bender,T.Gebru,A.McMillan-Major,andS.Shmitchell,“On
R. Barzilay and M.-Y. Kan, Eds. Vancouver, Canada: Association thedangersofstochasticparrots:Canlanguagemodelsbetoobig?”in
Proceedingsofthe2021ACMconferenceonfairness,accountability,
for Computational Linguistics, Jul. 2017, pp. 1601–1611. [Online].
Available:https://aclanthology.org/P17-1147
andtransparency,2021,pp.610–623.
[19] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, [41] Y. Ma, D. Tsao, and H.-Y. Shum, “On the principles of parsimony
R. Child et al., “Scaling laws for neural language models,” arXiv and self-consistency for the emergence of intelligence,” Frontiers of
preprintarXiv:2001.08361,2020. InformationTechnology&ElectronicEngineering,vol.23,no.9,pp.
[20] D. Hillier, L. Guertler, C. Tan, P. Agrawal, C. Ruirui, and B. Cheng, 1298–1323,2022.
“Supertinylanguagemodels,”arXivpreprintarXiv:2405.14159,2024. [42] S. Kadavath, T. Conerly, A. Askell, T. Henighan, D. Drain, E. Perez
[21] N. Mu¨ndler, J. He, S. Jenko, and M. Vechev, “Self-contradictory et al., “Language models (mostly) know what they know,” arXiv
hallucinations of large language models: Evaluation, detection and preprintarXiv:2207.05221,2022.
mitigation,” in The Twelfth International Conference on Learning [43] S. Han, Q. Zhang, Y. Yao, W. Jin, Z. Xu, and C. He, “Llm
Representations, 2024. [Online]. Available: https://openreview.net/ multi-agent systems: Challenges and open problems,” arXiv preprint
forum?id=EmQSOi1X2f arXiv:2402.03578,2024.
[22] Z.Tao,T.-E.Lin,X.Chen,H.Li,Y.Wu,Y.Lietal.,“Asurveyonself- [44] S.Yao,D.Yu,J.Zhao,I.Shafran,T.L.Griffiths,Y.Caoetal.,“Tree
evolutionoflargelanguagemodels,”arXivpreprintarXiv:2404.14387, ofthoughts:Deliberateproblemsolvingwithlargelanguagemodels,”
2024. in Thirty-seventh Conference on Neural Information Processing
[23] L. Pan, M. Saxon, W. Xu, D. Nathani, X. Wang, and W. Y. Wang, Systems, 2023. [Online]. Available: https://openreview.net/forum?id=
“Automatically correcting large language models: Surveying the 5Xc1ecxO1h
landscapeofdiverseautomatedcorrectionstrategies,”Transactionsof [45] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski,
the Association for Computational Linguistics, vol. 11, pp. 484–506, L. Gianinazzi et al., “Graph of thoughts: Solving elaborate problems
2024.[Online].Available:https://aclanthology.org/2024.tacl-1.27 with large language models,” Proceedings of the AAAI Conference
[24] R.Kamoi,Y.Zhang,N.Zhang,J.Han,andR.Zhang,“Whencanllms on Artificial Intelligence, vol. 38, no. 16, pp. 17682–17690, Mar.
actuallycorrecttheirownmistakes?acriticalsurveyofself-correction 2024. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/
ofllms,”arXivpreprintarXiv:2406.01297,2024. view/29720
[25] M. Hu, Z. Zhang, S. Zhao, M. Huang, and B. Wu, “Uncertainty in [46] J. Huang, S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu et al., “Large
naturallanguageprocessing:Sources,quantification,andapplications,” language models can self-improve,” in Proceedings of the 2023
arXivpreprintarXiv:2306.04459,2023. Conference on Empirical Methods in Natural Language Processing,
[26] M.Besta,F.Memedi,Z.Zhang,R.Gerstenberger,G.Piao,N.Blach H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association
et al., “Demystifying chains, trees, and graphs of thoughts,” arXiv for Computational Linguistics, Dec. 2023, pp. 1051–1068. [Online].
preprintarXiv:2401.14295,2024. Available:https://aclanthology.org/2023.emnlp-main.67JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 24
[47] E. Mitchell, J. Noh, S. Li, W. Armstrong, A. Agarwal, P. Liu [63] S. Diao, P. Wang, Y. Lin, and T. Zhang, “Active prompting
et al., “Enhancing self-consistency and performance of pre- with chain-of-thought for large language models,” arXiv preprint
trained language models through natural language inference,” in arXiv:2302.12246,2023.
Proceedings of the 2022 Conference on Empirical Methods in [64] J.ChenandJ.Mueller,“Quantifyinguncertaintyinanswersfromany
Natural Language Processing, Y. Goldberg, Z. Kozareva, and language model and enhancing their trustworthiness,” arXiv preprint
Y. Zhang, Eds. Abu Dhabi, United Arab Emirates: Association arXiv:2308.16175,2023.
for Computational Linguistics, Dec. 2022, pp. 1754–1768. [Online]. [65] D. Zheng, D. Liu, M. Lapata, and J. Z. Pan, “Trustscore: Reference-
Available:https://aclanthology.org/2022.emnlp-main.115 free evaluation of llm response trustworthiness,” arXiv preprint
[48] S. An, Z. Ma, Z. Lin, N. Zheng, J.-G. Lou, and W. Chen, arXiv:2402.12545,2024.
“Learning from mistakes makes llm better reasoner,” arXiv preprint [66] P.Manakul,A.Liusie,andM.Gales,“SelfCheckGPT:Zero-resource
arXiv:2310.20689,2023. black-box hallucination detection for generative large language
[49] K. Xiong, X. Ding, Y. Cao, T. Liu, and B. Qin, “Examining models,” in Proceedings of the 2023 Conference on Empirical
inter-consistency of large language models collaboration: An in- Methods in Natural Language Processing, H. Bouamor, J. Pino,
depth analysis via debate,” in Findings of the Association for and K. Bali, Eds. Singapore: Association for Computational
Computational Linguistics: EMNLP 2023, H. Bouamor, J. Pino, Linguistics, Dec. 2023, pp. 9004–9017. [Online]. Available: https:
and K. Bali, Eds. Singapore: Association for Computational //aclanthology.org/2023.emnlp-main.557
Linguistics, Dec. 2023, pp. 7572–7590. [Online]. Available: https: [67] C. Chen, K. Liu, Z. Chen, Y. Gu, Y. Wu, M. Tao et al.,
//aclanthology.org/2023.findings-emnlp.508 “INSIDE: LLMs’ internal states retain the power of hallucination
[50] C. Qian, Z. Xie, Y. Wang, W. Liu, Y. Dang, Z. Du et al., “Scaling detection,” in The Twelfth International Conference on Learning
large-language-model-basedmulti-agentcollaboration,”arXivpreprint Representations, 2024. [Online]. Available: https://openreview.net/
arXiv:2406.07155,2024. forum?id=Zj12nzlQbz
[51] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, [68] R. Cohen, M. Hamri, M. Geva, and A. Globerson, “LM vs LM:
R. West et al., “REFINER: Reasoning feedback on intermediate Detecting factual errors via cross examination,” in Proceedings of
representations,” in Proceedings of the 18th Conference of the the 2023 Conference on Empirical Methods in Natural Language
European Chapter of the Association for Computational Linguistics Processing, H. Bouamor, J. Pino, and K. Bali, Eds. Singapore:
(Volume 1: Long Papers), Y. Graham and M. Purver, Eds. St. Association for Computational Linguistics, Dec. 2023, pp. 12621–
Julian’s, Malta: Association for Computational Linguistics, Mar. 12640.[Online].Available:https://aclanthology.org/2023.emnlp-main.
2024, pp. 1100–1126. [Online]. Available: https://aclanthology.org/ 778
2024.eacl-long.67 [69] W. Yuan, G. Neubig, and P. Liu, “BARTScore: Evaluating generated
[52] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, textastextgeneration,”inAdvancesinNeuralInformationProcessing
“Reflexion: language agents with verbal reinforcement learning,” Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan,
in Advances in Neural Information Processing Systems, A. Oh, Eds., 2021. [Online]. Available: https://openreview.net/forum?id=
T.Naumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,Eds., 5Ya8PbvpZ9
vol. 36. Curran Associates, Inc., 2023, pp. 8634–8652. [Online].
[70] J.Fu,S.-K.Ng,Z.Jiang,andP.Liu,“Gptscore:Evaluateasyoudesire,”
Available: https://proceedings.neurips.cc/paper files/paper/2023/file/
arXivpreprintarXiv:2302.04166,2023.
1b44b878bb782e6954cd888628510e90-Paper-Conference.pdf
[71] W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward et al.,
[53] H. Kang, J. Ni, and H. Yao, “Ever: Mitigating hallucination in large
“Self-critiquingmodelsforassistinghumanevaluators,”arXivpreprint
languagemodelsthroughreal-timeverificationandrectification,”arXiv
arXiv:2206.05802,2022.
preprintarXiv:2311.09114,2023.
[72] N. McAleese, R. M. Pokorny, J. F. C. Uribe, E. Nitishinskaya,
[54] A.Mishra,A.Asai,V.Balachandran,Y.Wang,G.Neubig,Y.Tsvetkov
M. Trebacz, and J. Leike, “Llm critics help catch llm bugs,” arXiv
et al., “Fine-grained hallucination detection and editing for language
preprintarXiv:2407.00215,2024.
models,”arXivpreprintarXiv:2401.06855,2024.
[73] Y.Du,S.Li,A.Torralba,J.B.Tenenbaum,andI.Mordatch,“Improv-
[55] Y.-S. Chuang, Y. Xie, H. Luo, Y. Kim, J. R. Glass, and P. He,
ing factuality and reasoning in language models through multiagent
“Dola: Decoding by contrasting layers improves factuality in
debate,”arXivpreprintarXiv:2305.14325,2023.
large language models,” in The Twelfth International Conference
[74] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He et al.,
on Learning Representations, 2024. [Online]. Available: https:
“Self-evaluationguidedbeamsearchforreasoning,”inThirty-seventh
//openreview.net/forum?id=Th6NyL07na
ConferenceonNeuralInformationProcessingSystems,2023.[Online].
[56] W. Shi, X. Han, M. Lewis, Y. Tsvetkov, L. Zettlemoyer, and S. W.-
Available:https://openreview.net/forum?id=Bw82hwg5Q3
t. Yih, “Trusting your evidence: Hallucinate less with context-aware
decoding,”arXivpreprintarXiv:2305.14739,2023. [75] H. Wang, A. Prasad, E. Stengel-Eskin, and M. Bansal, “Soft
[57] J. Lu, C. Wang, and J. Zhang, “Diver: Large language model decod- self-consistency improves language model agents,” arXiv preprint
ing with span-level mutual information verification,” arXiv preprint arXiv:2402.13212,2024.
arXiv:2406.02120,2024. [76] X. Chen, M. Lin, N. Scha¨rli, and D. Zhou, “Teaching large
[58] Y. Xiao and W. Y. Wang, “On hallucination and predictive language models to self-debug,” in The Twelfth International
uncertainty in conditional language generation,” in Proceedings of Conference on Learning Representations, 2024. [Online]. Available:
the 16th Conference of the European Chapter of the Association for https://openreview.net/forum?id=KuPixIqPiq
Computational Linguistics: Main Volume, P. Merlo, J. Tiedemann, [77] H. Chen, A. Saha, S. Hoi, and S. Joty, “Personalized distillation:
and R. Tsarfaty, Eds. Online: Association for Computational Empowering open-sourced LLMs with adaptive learning for code
Linguistics, Apr. 2021, pp. 2734–2744. [Online]. Available: https: generation,” in The 2023 Conference on Empirical Methods in
//aclanthology.org/2021.eacl-main.236 Natural Language Processing, 2023. [Online]. Available: https:
[59] Z. Lin, S. Trivedi, and J. Sun, “Generating with confidence: //openreview.net/forum?id=alxWMBcNVN
Uncertainty quantification for black-box large language models,” [78] J.Jung,L.Qin,S.Welleck,F.Brahman,C.Bhagavatula,R.LeBras
Transactions on Machine Learning Research, 2024. [Online]. et al., “Maieutic prompting: Logically consistent reasoning with
Available:https://openreview.net/forum?id=DWkJCSxKU5 recursive explanations,” in Proceedings of the 2022 Conference on
[60] Y.A.Yadkori,I.Kuzborskij,A.Gyo¨rgy,andC.Szepesva´ri,“Tobelieve Empirical Methods in Natural Language Processing, Y. Goldberg,
ornottobelieveyourllm,”arXivpreprintarXiv:2406.02543,2024. Z.Kozareva,andY.Zhang,Eds. AbuDhabi,UnitedArabEmirates:
[61] D. Deng, G. Chen, Y. Yu, F. Liu, and P.-A. Heng, “Uncertainty AssociationforComputationalLinguistics,Dec.2022,pp.1266–1279.
estimation by fisher information-based evidential deep learning,” in [Online].Available:https://aclanthology.org/2022.emnlp-main.82
International Conference on Machine Learning. PMLR, 2023, pp. [79] R. Battiti, Maximum satisfiability problemMaximum Satisfiability
7596–7616. Problem. Boston,MA:SpringerUS,2009,pp.2035–2041.[Online].
[62] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation: Available:https://doi.org/10.1007/978-0-387-74759-0 364
Representing model uncertainty in deep learning,” in Proceedings [80] B. Huang, S. Lu, W. Chen, X. Wan, and N. Duan, “Enhancing large
of The 33rd International Conference on Machine Learning, ser. languagemodelsincodingthroughmulti-perspectiveself-consistency,”
Proceedings of Machine Learning Research, M. F. Balcan and arXivpreprintarXiv:2309.17272,2023.
K. Q. Weinberger, Eds., vol. 48. New York, New York, USA: [81] X.Chen,R.Aksitov,U.Alon,J.Ren,K.Xiao,P.Yinetal.,“Universal
PMLR, 20–22 Jun 2016, pp. 1050–1059. [Online]. Available: self-consistency for large language model generation,” arXiv preprint
https://proceedings.mlr.press/v48/gal16.html arXiv:2311.17311,2023.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 25
[82] E. Zelikman, G. Harik, Y. Shao, V. Jayasiri, N. Haber, and N. D. InternationalConferenceonLearningRepresentations,2023.[Online].
Goodman,“Quiet-star:Languagemodelscanteachthemselvestothink Available:https://openreview.net/forum?id=KbYevcLjnc
beforespeaking,”arXivpreprintarXiv:2403.09629,2024. [101] A.Chen,P.Pasupat,S.Singh,H.Lee,andK.Guu,“Purr:Efficiently
[83] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou et al., editing language model hallucinations by denoising language model
“Making language models better reasoners with step-aware verifier,” corruptions,”arXivpreprintarXiv:2305.14908,2023.
in Proceedings of the 61st Annual Meeting of the Association for [102] L. Gao, Z. Dai, P. Pasupat, A. Chen, A. T. Chaganty, Y. Fan et al.,
Computational Linguistics (Volume 1: Long Papers), A. Rogers, “RARR: Researching and revising what language models say, using
J.Boyd-Graber,andN.Okazaki,Eds. Toronto,Canada:Association language models,” in Proceedings of the 61st Annual Meeting of the
for Computational Linguistics, Jul. 2023, pp. 5315–5333. [Online]. Association for Computational Linguistics (Volume 1: Long Papers),
Available:https://aclanthology.org/2023.acl-long.291 A. Rogers, J. Boyd-Graber, and N. Okazaki, Eds. Toronto, Canada:
[84] C. Fernando, D. Banarse, H. Michalewski, S. Osindero, and Association for Computational Linguistics, Jul. 2023, pp. 16477–
T.Rockta¨schel,“Promptbreeder:Self-referentialself-improvementvia 16508.[Online].Available:https://aclanthology.org/2023.acl-long.910
promptevolution,”arXivpreprintarXiv:2309.16797,2023. [103] X.L.Li,A.Holtzman,D.Fried,P.Liang,J.Eisner,T.Hashimotoetal.,
[85] I.Harvey,“Themicrobialgeneticalgorithm,”inAdvancesinArtificial “Contrastive decoding: Open-ended text generation as optimization,”
Life.DarwinMeetsvonNeumann,G.Kampis,I.Karsai,andE.Sza- arXivpreprintarXiv:2210.15097,2022.
thma´ry, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011, [104] Z.Zhao,E.Monti,J.Lehmann,andH.Assem,“Enhancingcontextual
pp.126–133. understandinginlargelanguagemodelsthroughcontrastivedecoding,”
[86] O. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, arXivpreprintarXiv:2405.02750,2024.
S. V. A et al., “DSPy: Compiling declarative language model [105] J. Zhu, S. Liu, Y. Yu, B. Tang, Y. Yan, Z. Li et al., “Fastmem: Fast
calls into state-of-the-art pipelines,” in The Twelfth International memorizationofpromptimprovescontextawarenessoflargelanguage
Conference on Learning Representations, 2024. [Online]. Available: models,”arXivpreprintarXiv:2406.16069,2024.
https://openreview.net/forum?id=sY5N0zY5Od [106] C. Burns, H. Ye, D. Klein, and J. Steinhardt, “Discovering latent
[87] X. Wang and D. Zhou, “Chain-of-thought reasoning without prompt- knowledge in language models without supervision,” in The Eleventh
ing,”arXivpreprintarXiv:2402.10200,2024. InternationalConferenceonLearningRepresentations,2023.[Online].
[88] Y.Tian,B.Peng,L.Song,L.Jin,D.Yu,H.Mietal.,“Towardself- Available:https://openreview.net/forum?id=ETKGuby0hcs
improvementofllmsviaimagination,searching,andcriticizing,”arXiv [107] W. Wu, Y. Wang, G. Xiao, H. Peng, and Y. Fu, “Retrieval
preprintarXiv:2404.12253,2024. head mechanistically explains long-context factuality,” arXiv preprint
arXiv:2404.15574,2024.
[89] A. Patel, M. Hofmarcher, C. Leoveanu-Condrei, M.-C. Dinu,
[108] A. Templeton, T. Conerly, J. Marcus, J. Lindsey, T. Bricken,
C.Callison-Burch,andS.Hochreiter,“Largelanguagemodelscanself-
B. Chen et al., “Scaling monosemanticity: Extracting interpretable
improveatwebagenttasks,”arXivpreprintarXiv:2405.20309,2024.
features from claude 3 sonnet,” Transformer Circuits Thread,
[90] E. Choi, A. Ahmadian, M. Geist, O. Pietquin, and M. G.
2024. [Online]. Available: https://transformer-circuits.pub/2024/
Azar,“Self-improvingrobustpreferenceoptimization,”arXivpreprint
scaling-monosemanticity/index.html
arXiv:2406.01660,2024.
[109] L. Gao, T. D. la Tour, H. Tillman, G. Goh, R. Troll, A. Radford
[91] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox
et al., “Scaling and evaluating sparse autoencoders,” arXiv preprint
et al., “Principle-driven self-alignment of language models from
arXiv:2406.04093,2024.
scratch with minimal human supervision,” in Advances in
[110] K.Tian,E.Mitchell,H.Yao,C.D.Manning,andC.Finn,“Fine-tuning
Neural Information Processing Systems, A. Oh, T. Naumann,
language models for factuality,” arXiv preprint arXiv:2311.08401,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds.,
2023.
vol. 36. Curran Associates, Inc., 2023, pp. 2511–2565. [Online].
[111] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov,“Prox-
Available: https://proceedings.neurips.cc/paper files/paper/2023/file/
imalpolicyoptimizationalgorithms,”arXivpreprintarXiv:1707.06347,
0764db1151b936aca59249e2c1386101-Paper-Conference.pdf
2017.
[92] Y.Tong,D.Li,S.Wang,Y.Wang,F.Teng,andJ.Shang,“Canllms
[112] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning,
learn from previous mistakes? investigating llms’ errors to boost for
S. Ermon, and C. Finn, “Direct preference optimization: Your
reasoning,”arXivpreprintarXiv:2403.20046,2024.
language model is secretly a reward model,” in Advances in
[93] A. Agarwal, A. Tzen, and C. Tew, “Improving logical
Neural Information Processing Systems, A. Oh, T. Naumann,
consistency in pre-trained language models using natural
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds.,
language inference,” Stanford CS224N Custom Project, 2022.
vol. 36. Curran Associates, Inc., 2023, pp. 53728–53741.
[Online]. Available: https://web.stanford.edu/class/archive/cs/cs224n/
[Online]. Available: https://proceedings.neurips.cc/paper files/paper/
cs224n.1224/reports/custom 116994635.pdf
2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf
[94] A. Ignatiev, A. Morgado, and J. Marques-Silva, “RC2: An Efficient [113] A. Ko¨pf, Y. Kilcher, D. von Ru¨tte, S. Anagnostidis, Z. R.
MaxSAT Solver,” Journal on Satisfiability, Boolean Modeling and Tam, K. Stevens et al., “Openassistant conversations -
Computation,vol.11,pp.53–64,January2019,published:1Jan.2019. democratizing large language model alignment,” in Advances in
[95] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang Neural Information Processing Systems, A. Oh, T. Naumann,
et al., “A survey on large language model based autonomous A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds.,
agents,” vol. 18, no. 6, p. 186345. [Online]. Available: https: vol. 36. Curran Associates, Inc., 2023, pp. 47669–47681.
//doi.org/10.1007/s11704-024-40231-1 [Online]. Available: https://proceedings.neurips.cc/paper files/paper/
[96] J.Huang,X.Chen,S.Mishra,H.S.Zheng,A.W.Yu,X.Songetal., 2023/file/949f0f8f32267d297c2d4e3ee10a2e7e-Paper-Datasets and
“Large language models cannot self-correct reasoning yet,” in The Benchmarks.pdf
Twelfth International Conference on Learning Representations, 2024. [114] J. Ji, M. Liu, J. Dai, X. Pan, C. Zhang, C. Bian et al., “Beavertails:
[Online].Available:https://openreview.net/forum?id=IkmD3fKBPQ Towards improved safety alignment of llm via a human-preference
[97] A. P. Jacob, Y. Shen, G. Farina, and J. Andreas, “The consensus dataset,” in Advances in Neural Information Processing Systems,
game: Language model generation via equilibrium search,” in The A.Oh,T.Naumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,
Twelfth International Conference on Learning Representations, 2024. Eds., vol. 36. Curran Associates, Inc., 2023, pp. 24678–24704.
[Online].Available:https://openreview.net/forum?id=n9xeGcI4Yg [Online]. Available: https://proceedings.neurips.cc/paper files/paper/
[98] X.Liang,S.Song,S.Niu,Z.Li,F.Xiong,B.Tangetal.,“Uhgeval: 2023/file/4dbb61cb68671edc4ca3712d70083b9f-Paper-Datasets and
Benchmarkingthehallucinationofchineselargelanguagemodelsvia Benchmarks.pdf
unconstrainedgeneration,”arXivpreprintarXiv:2311.15296,2023. [115] Y.Bai,S.Kadavath,S.Kundu,A.Askell,J.Kernion,A.Jonesetal.,
[99] K. Yang, Y. Tian, N. Peng, and D. Klein, “Re3: Generating longer “Constitutional ai: Harmlessness from ai feedback,” arXiv preprint
stories with recursive reprompting and revision,” in Proceedings of arXiv:2212.08073,2022.
the 2022 Conference on Empirical Methods in Natural Language [116] Z. Sun, Y. Shen, H. Zhang, Q. Zhou, Z. Chen, D. D. Cox et al.,
Processing, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. “SALMON: Self-alignment with instructable reward models,” in The
Abu Dhabi, United Arab Emirates: Association for Computational Twelfth International Conference on Learning Representations, 2024.
Linguistics, Dec. 2022, pp. 4393–4479. [Online]. Available: https: [Online].Available:https://openreview.net/forum?id=xJbsmB8UMx
//aclanthology.org/2022.emnlp-main.296 [117] Y.Bai,S.Kadavath,S.Kundu,A.Askell,J.Kernion,A.Jonesetal.,
[100] T. Schick, J. A. Yu, Z. Jiang, F. Petroni, P. Lewis, G. Izacard “Constitutional ai: Harmlessness from ai feedback,” arXiv preprint
et al., “PEER: A collaborative language model,” in The Eleventh arXiv:2212.08073,2022.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 26
[118] K. Ethayarajh, Y. Choi, and S. Swayamdipta, “Understanding N. Calzolari, C.-R. Huang, H. Kim, J. Pustejovsky, L. Wanner,
dataset difficulty with V-usable information,” in Proceedings of K.-S. Choi et al., Eds. Gyeongju, Republic of Korea: International
the 39th International Conference on Machine Learning, ser. Committee on Computational Linguistics, Oct. 2022, pp. 3680–3696.
ProceedingsofMachineLearningResearch,K.Chaudhuri,S.Jegelka, [Online].Available:https://aclanthology.org/2022.coling-1.324
L. Song, C. Szepesvari, G. Niu, and S. Sabato, Eds., vol. [136] Z.Lin,Z.Gou,T.Liang,R.Luo,H.Liu,andY.Yang,“Criticbench:
162. PMLR, 17–23 Jul 2022, pp. 5988–6008. [Online]. Available: Benchmarking llms for critique-correct reasoning,” arXiv preprint
https://proceedings.mlr.press/v162/ethayarajh22a.html arXiv:2402.14809,2024.
[119] S.Kim,S.Bae,J.Shin,S.Kang,D.Kwak,K.M.Yooetal.,“Aligning [137] Z. Tan, L. Wei, J. Wang, X. Xie, and W. Huang, “Can i understand
large language models through synthetic feedback,” arXiv preprint what i create? self-knowledge evaluation of large language models,”
arXiv:2305.13735,2023. arXivpreprintarXiv:2406.06140,2024.
[120] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin [138] Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su et al., “C-eval:
et al., “Training language models to follow instructions with human A multi-level multi-discipline chinese evaluation suite for foundation
feedback,”Advancesinneuralinformationprocessingsystems,vol.35, models,”AdvancesinNeuralInformationProcessingSystems,vol.36,
pp.27730–27744,2022. 2024.
[121] Y. Xu, X. Liu, X. Liu, Z. Hou, Y. Li, X. Zhang et al., “Chatglm- [139] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song et al.,
math:Improvingmathproblem-solvinginlargelanguagemodelswith “Aligning ai with shared human values,” Proceedings of the Interna-
aself-critiquepipeline,”arXivpreprintarXiv:2404.02893,2024. tionalConferenceonLearningRepresentations(ICLR),2021.
[122] Y. Gu, L. Dong, F. Wei, and M. Huang, “MiniLLM: Knowledge [140] M.Suzgun,N.Scales,N.Scha¨rli,S.Gehrmann,Y.Tay,H.W.Chung
distillation of large language models,” in The Twelfth International etal.,“Challengingbig-benchtasksandwhetherchain-of-thoughtcan
Conference on Learning Representations, 2024. [Online]. Available: solvethem,”arXivpreprintarXiv:2210.09261,2022.
https://openreview.net/forum?id=5h0qf7IBZZ [141] P.Clark,I.Cowhey,O.Etzioni,T.Khot,A.Sabharwal,C.Schoenick
[123] R. Agarwal, N. Vieillard, Y. Zhou, P. Stanczyk, S. R. Garea, et al., “Think you have solved question answering? try arc, the ai2
M. Geist et al., “On-policy distillation of language models: reasoningchallenge,”arXivpreprintarXiv:1803.05457,2018.
Learning from self-generated mistakes,” in The Twelfth International [142] M. T. Pilehvar and J. Camacho-Collados, “Wic: the word-in-context
Conference on Learning Representations, 2024. [Online]. Available: datasetforevaluatingcontext-sensitivemeaningrepresentations,”Pro-
https://openreview.net/forum?id=3zKtaqxLhW ceedingsofNAACL2019(short),2019.
[124] S.Ye,Y.Jo,D.Kim,S.Kim,H.Hwang,andM.Seo,“Selfee:Iterative [143] M.Chen,J.Tworek,H.Jun,Q.Yuan,H.P.deOliveiraPinto,J.Kaplan
self-revising llm empowered by self-feedback generation,” Blog post, etal.,“Evaluatinglargelanguagemodelstrainedoncode,”2021.
2023. [144] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang
[125] J. Jung, P. West, L. Jiang, F. Brahman, X. Lu, J. Fisher et al., et al., “Measuring mathematical problem solving with the math
“Impossible distillation: from low-quality model to high-quality dataset,”NeurIPS,2021.
dataset&modelforsummarizationandparaphrasing,”arXivpreprint [145] X. Wang, B. Ma, C. Hu, L. Weber-Genzel, P. Ro¨ttger, F. Kreuter
arXiv:2305.16635,2023. et al., “” my answer is c”: First-token probabilities do not match
[126] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi text answers in instruction-tuned language models,” arXiv preprint
et al., “Self-instruct: Aligning language models with self-generated arXiv:2402.14499,2024.
instructions,” in Proceedings of the 61st Annual Meeting of the [146] H.Wang,S.Zhao,Z.Qiang,B.Qin,andT.Liu,“Beyondtheanswers:
Association for Computational Linguistics (Volume 1: Long Papers), Reviewingtherationalityofmultiplechoicequestionansweringforthe
A. Rogers, J. Boyd-Graber, and N. Okazaki, Eds. Toronto, Canada: evaluationoflargelanguagemodels,”arXivpreprintarXiv:2402.01349,
Association for Computational Linguistics, Jul. 2023, pp. 13484– 2024.
13508.[Online].Available:https://aclanthology.org/2023.acl-long.754 [147] D.Jiang,J.Zhang,O.Weller,N.Weir,B.VanDurme,andD.Khashabi,
[127] M. Sclar, Y. Choi, Y. Tsvetkov, and A. Suhr, “Quantifying language “Self-[in] correct: Llms struggle with refining self-generated re-
models’ sensitivity to spurious features in prompt design or: How i sponses,”arXivpreprintarXiv:2404.04298,2024.
learned to start worrying about prompt formatting,” arXiv preprint [148] K. Stechly, M. Marquez, and S. Kambhampati, “GPT-4 doesn’t
arXiv:2310.11324,2023. know it’s wrong: An analysis of iterative prompting for reasoning
[128] Q. Yu, Z. Zheng, S. Song, Z. Li, F. Xiong, B. Tang et al., “xfinder: problems,”inNeurIPS2023FoundationModelsforDecisionMaking
Robust and pinpoint answer extraction for large language models,” Workshop,2023.[Online].Available:https://openreview.net/forum?id=
arXivpreprintarXiv:2405.11874,2024. PMtZjDYB68
[129] F. Ye, M. Yang, J. Pang, L. Wang, D. F. Wong, E. Yilmaz et al., [149] K. Valmeekam, M. Marquez, and S. Kambhampati, “Investigating
“Benchmarking llms via uncertainty quantification,” arXiv preprint the effectiveness of self-critiquing in LLMs solving planning
arXiv:2401.12794,2024. tasks,” in NeurIPS 2023 Foundation Models for Decision Making
[130] X.Wang,Z.Zhang,Q.Li,G.Chen,M.Hu,B.Luoetal.,“Ubench: Workshop,2023.[Online].Available:https://openreview.net/forum?id=
Benchmarking uncertainty in large language models with multiple gGQfkyb0KL
choicequestions,”arXivpreprintarXiv:2406.12784,2024. [150] D. Nathani, D. Wang, L. Pan, and W. Y. Wang, “MAF:
[131] Z.Yang,Y.Zhang,T.Liu,J.Yang,J.Lin,C.Zhouetal.,“Canlarge Multi-aspect feedback for improving reasoning in large language
languagemodelsalwayssolveeasyproblemsiftheycansolveharder models,” in The 2023 Conference on Empirical Methods in
ones?”arXivpreprintarXiv:2406.12809,2024. Natural Language Processing, 2023. [Online]. Available: https:
[132] E.Rabinovich,S.Ackerman,O.Raz,E.Farchi,andA.AnabyTavor, //openreview.net/forum?id=bNeDLx5O6w
“Predictingquestion-answeringperformanceoflargelanguagemodels [151] W.Xu,G.Zhu,X.Zhao,L.Pan,L.Li,andW.Y.Wang,“Prideand
throughsemanticconsistency,”inProceedingsoftheThirdWorkshop prejudice: Llm amplifies self-bias in self-refinement,” arXiv preprint
on Natural Language Generation, Evaluation, and Metrics (GEM), arXiv:2402.11436,2024.
S. Gehrmann, A. Wang, J. Sedoc, E. Clark, K. Dhole, K. R. [152] G. Yona, R. Aharoni, and M. Geva, “Can large language models
Chandu et al., Eds. Singapore: Association for Computational faithfullyexpresstheirintrinsicuncertaintyinwords?”arXivpreprint
Linguistics, Dec. 2023, pp. 138–154. [Online]. Available: https: arXiv:2405.16908,2024.
//aclanthology.org/2023.gem-1.12 [153] S.Kapoor,N.Gruver,M.Roberts,K.Collins,A.Pal,U.Bhattetal.,
[133] Y. Elazar, N. Kassner, S. Ravfogel, A. Ravichander, E. Hovy, “Largelanguagemodelsmustbetaughttoknowwhattheydon’tknow,”
H.Schu¨tzeetal.,“Measuringandimprovingconsistencyinpretrained arXivpreprintarXiv:2406.08391,2024.
languagemodels,”TransactionsoftheAssociationforComputational [154] L.Chen,Z.Liang,X.Wang,J.Liang,Y.Xiao,F.Weietal.,“Teaching
Linguistics, vol. 9, pp. 1012–1031, 2021. [Online]. Available: largelanguagemodelstoexpressknowledgeboundaryfromtheirown
https://aclanthology.org/2021.tacl-1.60 signals,”arXivpreprintarXiv:2406.10881,2024.
[134] J. Qi, R. Ferna´ndez, and A. Bisazza, “Cross-lingual consistency
of factual knowledge in multilingual language models,” in The
2023 Conference on Empirical Methods in Natural Language
Processing, 2023. [Online]. Available: https://openreview.net/forum?
id=MLKLYoXypN
[135] M. Jang, D. S. Kwon, and T. Lukasiewicz, “BECEL: Benchmark
for consistency evaluation of language models,” in Proceedings of
the 29th International Conference on Computational Linguistics,JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 27
Xun Liang (Senior Member, IEEE) received the Rong-HuaLireceivedthePh.D.degreeincomputer
B.Sc. and Ph.D. degrees in computer engineering sciencefromTheChineseUniversityofHongKong,
from Tsinghua University, Beijing, China, in 1989 Hong Kong, in 2013. He is currently a Professor
and1993,respectively,andtheM.Sc.degreeinoper- with the Beijing Institute of Technology, Beijing,
ationsresearchfromStanfordUniversity,PaloAlto, China. His research interests include graph data
CA, USA, in 1999. He worked as a Post-Doctoral management and mining, social network analysis,
Fellow with the Institute of Computer Science and graph computation systems, and graph-based ma-
Technology, Peking University, Beijing, from 1993 chinelearning.
to1995,andwiththeDepartmentofComputerEngi-
neering,UniversityofNewBrunswick,Fredericton,
NB, Canada, from 1995 to 1997. He worked as a
CTO,leadingovertenintelligentinformationproductsinRixoInfoLtd.,CA,
Feiyu Xiong is the Head of the Large Language
USA, from 2000 to 2007, and was the Director of the Data Mining Lab,
Model Center of the Institute for Advanced Algo-
InstituteofComputerScienceandTechnology,PekingUniversity,from2005
rithms Research-Shanghai. He holds a Bachelor’s
to2009.HeiscurrentlyaprofessorwiththeSchoolofInformation,Renmin
degree from Huazhong University of Science and
UniversityofChina.Hisresearchinterestsincludesupportvectormachines,
Technology and a Ph.D. from Drexel University.
socialcomputingandlargelanguagemodels.
He has previously served as the Head of Data
IntelligenceforAlibaba’sBusinessMiddlePlatform
Shichao Song is currently a PhD student at the and the Head of the Data Platform for Taobao and
SchoolofInformation,RenminUniversityofChina, TmallGroup.DuringhistenureatAlibaba,hewas
under the supervision of Prof. Xun Liang. His re- primarilyresponsiblefortheintelligentconstruction
search interests span a wide range of topics, in- ofsystemsrelatedtocoree-commercetransactions.
cludinginternalconsistencyminingofLLMs,LLM
interpretability, and reliable evaluation methods for
Zhiyu Li received his Ph.D. in Computer Science
LLMs. For more information, visit his website at
from the School of Information, Renmin Univer-
https://ki-seki.github.io/.
sity of China, in 2019. He is currently a Senior
Researcher at the Large Language Model Center
oftheInstituteforAdvancedAlgorithmsResearch-
Shanghai. He has published over 30 papers in top-
Zifan Zheng is currently a research intern at the tierconferencesandjournalssuchasTKDE,KDD,
Large Language Model Center of the Institute for and ACL. His current responsibilities include re-
Advanced Algorithms Research, Shanghai. He re- search and application implementation related to
ceived the B.S. degree in Computer Science and largelanguagemodels.Hisresearchinterestsinclude
Technology from Beijing Institute of Technology, modelpre-training,modelalignment,andhallucina-
China,in2024.HisresearchinterestsincludeLLMs tionoptimization.
interpretability, reliable evaluation and social net-
workanalysis.
Hanyu Wang is a Ph.D. student at the School of
Information,RenminUniversityofChina,underthe
supervisionofProfessorXunLiang.Hisresearchar-
easincludelargelanguagemodels,controllabletext
generationinlargelanguagemodels,andcontrolled
decoding.
Qingchen Yu is currently a research intern at the
Large Language Model Center of the Institute for
Advanced Algorithms Research in Shanghai. He is
also a master’s student at Shanghai University. His
research interests include machine learning, LLM
evaluation,andpromptengineering.
Xunkai Li is currently working toward the PhD
degreewiththeschoolofComputerScience,Beijing
InstituteofTechnology,advisedbyProf.Rong-Hua
Li.HereceivedtheBSdegreeincomputerscience
fromShandongUniversityin2022.Hisresearchin-
terestliesinData-centricMLandGraph-MLwithin
complexrelationaldataandnewlearningparadigms.
He has published 5+ papers in top DB/DM/AI
conferences such as VLDB, WWW, AAAI as the
firstauthor.