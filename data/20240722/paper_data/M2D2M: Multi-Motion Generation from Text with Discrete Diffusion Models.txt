M2D2M: Multi-Motion Generation from Text
with Discrete Diffusion Models
Seunggeun Chi1,2∗ , Hyung-gun Chi2∗ , Hengbo Ma‡, Nakul Agarwal1,
Faizan Siddiqui1, Karthik Ramani2† , and Kwonjoon Lee1†
1Honda Research Institute USA 2Purdue University
{chi65, chi45}@purdue.edu,
ramani@purdue.edu, kwonjoon_lee@honda-ri.com
Abstract. We introduce the Multi-Motion Discrete Diffusion Models
(M2D2M), a novel approach for human motion generation from textual
descriptions of multiple actions, utilizing the strengths of discrete dif-
fusion models. This approach adeptly addresses the challenge of gener-
ating multi-motion sequences, ensuring seamless transitions of motions
and coherence across a series of actions. The strength of M2D2M lies in
its dynamic transition probability within the discrete diffusion model,
which adapts transition probabilities based on the proximity between
motion tokens, encouraging mixing between different modes. Comple-
mentedbyatwo-phasesamplingstrategythatincludesindependentand
joint denoising steps, M2D2M effectively generates long-term, smooth,
and contextually coherent human motion sequences, utilizing a model
trainedforsingle-motiongeneration.Extensiveexperimentsdemonstrate
that M2D2M surpasses current state-of-the-art benchmarks for motion
generationfromtextdescriptions,showcasingitsefficacyininterpreting
language semantics and generating dynamic, realistic motions.
Keywords: Text-to-Motion · Multi-Motion Generation · VQ-Diffusion
1 Introduction
Thegenerationofhumanmotionisarapidlyadvancingfieldwithprofoundappli-
cations in areas such as animation [3,5], VR/AR [27,28], and human-computer
interaction [33,62]. Particularly, the ability to accurately convert textual de-
scriptions into realistic, fluid human motions is not just a remarkable technical
achievement but also a crucial step towards more immersive digital experiences.
Recent progress in human motion generation has seen a surge in the use of
diffusionmodels[11,32,54,63].Theseadvancementshavebeencriticalinaligning
textual descriptions with corresponding human motions.
Previousresearchonhumanmotiongenerationhavemainlyfocusedonsingle-
motionsequences(i.e.,sequencesthatfeatureasingleaction),buttheabilityto
generate multi-motion sequences, which involve a series of actions, from a set of
∗ Co-first authors. † Senior authors. ‡ Work done at Honda Research Institute.
4202
luJ
91
]VC.sc[
1v20541.7042:viXra2 S. Chi et al.
Fig.1: Qualitative Comparison of Multi-Motion Sequences.Inthetransitions
highlighted by the green boxes, our model shows a consistent and gradual progression
of poses compared to others. This indicates that our model not only produces more
realistic and smooth motions but also maintains the fidelity of each motion segment,
aligning accurately with the corresponding action descriptions on top.
actiondescriptions,whichisillustratedinFig.1,iscrucialformanyapplications.
This capability is particularly important in scenarios where a series of actions
must be depicted in a continuous and coherent manner, such as in storytelling,
interactive gaming, or complex training simulations. However, generating such
sequences presents unique challenges where models often struggle to maintain
continuity and coherence throughout a series of actions. Previous methods [14,
23,30,49,52,63], which generate motion for each action description separately
and then attempt to connect them, frequently result in motions with abrupt
transitions at action boundaries or distorted individual motions, lacking fidelity
to textual descriptions for individual motion.
Totacklethischallenge,weintroducetheM2D2M(MultiMotionDiscrete
Diffusion Models), a novel approach for human motion generation from tex-
tual descriptions of multiple actions. We devise a novel sampling mechanism
to generate coherent and faithful multi-motion sequences using discrete diffu-
sion models [17] trained on single-motion sequences. Furthermore, to encourage
mixing between different modes (especially at multi-motion boundaries), we in-
troduce a unique transition probability mechanism that considers the proximity
between motion tokens.
A key contribution of our work is the introduction of a dynamic transition
probability model within the discrete diffusion framework. This model adjusts
the transition probabilities based on principles of exploration and exploitation.M2D2M: Multi-Motion Discrete Diffusion Models 3
Initially,itemphasizesbroadexplorationofdiversemotionsbychoosingelements
far apart in the codebook. As the process progresses, the focus shifts to select-
ing closer elements, refining the probabilities to facilitate convergence towards
accurate individual motions, embodying the principle of exploitation.
Our Two-Phase Sampling (TPS) strategy represents another key contribu-
tion, addressing the challenge of generating extended human motion sequences.
This method starts by sketching a coarse outline of multi-motion sequences
through joint sampling, and then it refines each motion with independent sam-
pling. We posit that the exploration of diverse motions is crucial in the initial
stageofTPS,whereweestablishtheroughlayoutofmulti-motionsequences.Our
ablation results (Table 5) demonstrate that the synergy between the dynamic
transition probability model and TPS is essential for converging to optimal so-
lutions. TPS allows for the generation of multi-motion using models trained on
single-motion generation without additional training for multi-motion genera-
tion, which is particularly advantageous given the scarcity of datasets contain-
ing multiple actions.TPSenhancesthenaturalflowofthemotion,ensuringthat
transitions between actions are both smooth and realistic.
Toquantifythetransitionbehaviorofdifferentmulti-motiongenerationmod-
els (as seen in Fig. 1), we introduce a novel evaluation metric, Jerk, to measure
thesmoothnessofmulti-motionsequencesatthetransitionsbetweenactions.Al-
thoughsimilarmetricshavebeenemployedinvariousfields[18,41,48,60],tothe
best of our knowledge, our work is the first to use Jerk for evaluating transition
smoothness in multi-motion generation, establishing a specialized benchmark
for this task. Experiments show the effectiveness of our approach in enhanc-
ing transition smoothness, while maintaining fidelity to individual motions
within motion boundaries.
The contribution of our work is summarized as follows:
– We present a two-phase sampling method for creating multi-motion se-
quences from text. This method enables multi-motion generation without
additional training and offers a better tradeoff between the fidelity of indi-
vidual motions and the smoothness of transitions between actions [49,63].
– We introduce a dynamic transition probability for the discrete diffusion
model, specifically designed for human motion generation from text.
– We introduce a new evaluation metric, Jerk, designed to assess the smooth-
ness of generated motions at action boundaries.
– Extensive experiments confirm that our methods establish state-of-the-art
performance in both single-motion generation and multi-motion generation.
2 Related Works
2.1 Human Motion Generation from Text
Generating3Dhumanmotionfromtextualdescriptionsisagrowingareawithin
humanmotiongeneration[1,2,4,16,31,39,40,58,59,65–67].Recentadvancements
include using CLIP [45] for text encoding and aligning text and motion embed-
dings [42,53] alongside the development of motion-language datasets [19,43,44].4 S. Chi et al.
Fig.2: Overview of M2D2M. We train a (a) VQ-VAE to obtain motion tokens,
whichissubsequentlyusedtotraina(b)DenoisingTransformerforthediscretediffu-
sionmodel.Ingeneratinghumanmotion,wefollowthe(c)standarddenoisingprocess
for single-motion generation and (d) employ Two-Phase Sampling (TPS) for multi-
motion generation. A <MASK> token is denoted as ‘M’ in the figure.
Fig.3: Comparison of Multi-Motion Generation Algorithms. Unlike heuristic
post-processing methods for combining independent motions such as Handshake [49]
andSLERP[6],TPSisasingle-stagealgorithmforamulti-motiongenerationthatdoes
not require completed individual motions or a hyper-parameter for transition length.
Notably, diffusion-based models [11,54,64–66] have been increasingly used for
text-to-motion generation.
Multi-motion generation is crucial for creating realistic and continuous hu-
man motion sequences. Recent work, such as “motion in-betweening" [14,22,23,
30,52], addresses this by interpolating between motions. TEACH [6] enhanced
this approach with an unfolding method using SLERP interpolation. Addition-
ally, PriorMDM [49] introduces a handshake algorithm for smoother transitions
inlongsequences.Despitetheireffectivenessinbridginggaps,thesemethodsre-
quire an additional stage for multi-motion generation to merge independently
generated motions. As illustrated in Fig. 3, these methods not only modify
the length of individual motions but also require an extra hyper-parameter for
transition length to achieve smoothness, affecting the outcomes and evaluation
metrics. VAE-based methods like Multi-Act [36] and TEACH [6] attempt to
generate motion conditioned on a previous motion and a text but face limi-
tations in long-term generation due to their iterative process. FineMoGen [66]
utilzes spatial-temporal attention for fine-grained text description to generate
multi-motion. Our method overcomes these challenges by maintaining fidelity
and smoothness in the generated motion, significantly enhancing multi-motion
generation without the trade-offs inherent in traditional approaches.M2D2M: Multi-Motion Discrete Diffusion Models 5
2.2 Discrete Diffusion Models
Diffusion models [50], defined by forward and reverse Markov processes, are
integral to advancements in generative models. These models, known for trans-
forming data into increasingly noisy variables and subsequently denoising them,
benefit from stability and rapid sampling capabilities. Enhanced by neural net-
works learning the reverse process [13,17,24,51], they are particularly effective
in continuous spaces like images. Latent diffusion models [47], operating in a
latentspacebeforereturningtotheoriginaldataspace,adeptlyhandlecomplex
data distributions.
Indiscretespacessuchastext,diffusionmodelsalsoexcel.D3PM[7]andVQ-
Diffusion [17] have introduced methods like structured categorical corruption
andmask-and-replacetominimizeerrorsiniterativemodels.Thisshowcasesthe
broad applicability of diffusion models. Drawing inspiration from recent studies
demonstrating VQ-VAE’s [56] effectiveness in human pose modeling in discrete
spaces[32,63],weapplydiscretediffusiontohumanmotiongeneration.Similarto
our approach Kong et al [32] also introduce discrete diffusion for human motion
generation from the text. However, different from this work, we introduce a
new transition matrix considering the relationship between action tokens and is
specifically tailored for multi-motion generation.
3 Preliminary: Discrete Diffusion Models
Discretediffusionmodels[7,17,26]areaclassofdiffusionmodelswhichworkby
graduallyaddingnoisetodataandlearningtoreversethisprocess.Unlikecontin-
uous models like latent diffusion models [47] which operate on data represented
in a continuous space, discrete diffusion models work with data representation
in discrete state spaces.
Forward Diffusion Process. Since the first introduction of the discrete diffu-
sion model [50], VQ-Diffusion [17] has improved the approach by incorporating
amask-and-replacestrategy.VQ-Diffusionentailsaforwarddiffusionprocessby
transitioning from one token to another token. The forward Markov diffusion
process for step t−1 to t is given by:
q(z |z )=v⊤(z )Q v(z ), (1)
t t−1 t t t−1
where v(z )∈R(K+1)×1 denotes the one-hot encoded vector for the token index
t
of z , and Q [i,j] is the transition probability from a token z to z at diffusion
t t i j
step t. The transition probability matrix Q ∈R(K+1)×(K+1) is written as:
t
(cid:20) Qˆ 0 (cid:21)
Q = t , where Qˆ =α I+β 11⊤. (2)
t γ ·1⊤ 1 t t t
t
Here, I is the identity matrix, 1 is a column vector of ones, β represents the
t
probability of transitioning between the different tokens, γ denotes the prob-
t
ability of transitioning to a <MASK> token, and α =1−Kβ −γ . Due to the
t t t6 S. Chi et al.
Markov property, the probabilities of z at arbitrary diffusion time step can be
t
derived q(z |z ) = v⊤(z )Q v(z ), where Q = Q Q ···Q . The matrix is
t 0 t t 0 t t t−1 1
constructed such that the <MASK> token always maintains its original state so
that z converges to <MASK> token with sufficiently large t.
t
Conditional Denoising Process. The conditional denoising process through
aneuralnetworkp .Thisnetworkpredictsthenoiselesstokenz whenprovided
θ 0
with a corrupted token and its corresponding condition, such as a language
token. For training the network p , beyond the denoising objective, the training
θ
incorporates the standard variational lower bound objective [50], denoted as
Lvlb. The training objective with a coefficient for the denoising loss λ is:
L=Lvlb+λE zt∼q(zt|z0)[−logp θ(z 0|z t,y)], (3)
Here, the reverse transition distribution can be written as follow:
p (z |z
,y)=(cid:80)K
q(z |z ,z˜ )p (z˜ |z ,y). (4)
θ t−1 t z˜0=1 t−1 t 0 θ 0 t
By iteratively denoising tokens from T down to 1, we can obtain the gener-
ated token z conditioned on y. The tractable posterior distribution of discrete
0
diffusion can be expressed as:
q(z |z ,z )q(z |z )
q(z |z ,z )= t t−1 0 t−1 0
t−1 t 0 q(z |z )
t 0
(cid:0) v⊤(z )Q v(z )(cid:1)(cid:0) v⊤(z )Q v(z )(cid:1)
= t t t−1 t−1 t−1 0 . (5)
v⊤(z )Q v(z )
t t 0
4 Multi-Motion Discrete Diffusion Model
We introduce the M2D2M (Multi Motion Discrete Diffusion Model), as il-
lustrated in Fig. 2. M2D2M is a discrete diffusion model designed specifically
forgeneratinghumanmotionfromtextualdescriptionsofmultipleactions.This
modelutilizesaVQ-VAE-baseddiscreteencoding[56]whichhasproveneffective
in representing human motion [29,32,63]. The following sections will provide a
detailed overview of our approach.
4.1 Motion VQ-VAE
To establish a codebook for discrete diffusion, we first trained a VQ-VAE model
[56]. Our training approach and the model’s architectural design closely fol-
low [63]. The model comprises of an encoder E(·), a decoder D(·), and a quan-
tizerQ(·)(seeFig.2(a)).Theencoderprocesseshumanmotion,representedby
x∈RL×D, converting it into motion tokens, z=E(x)∈RL 4×D. Here, L signifies
thelengthofthemotionsequenceandDdenotesthedimensionalityofeachcode-
book.Thequantizer’sroleistomapthemotiontokenatanytimeframeτ tothe
nearest codebook entry, determined by z [τ]=Q(z[τ])=argmin ||z[τ]−c || .
q ci∈C i 2
Here, C = {c ,...,c } represents the codebook, where K signifies the total
1 KM2D2M: Multi-Motion Discrete Diffusion Models 7
(a) LatentCodebookRepresentation (b) TransitionProbability
Fig.4: (a) PCA plot representing motion tokens from the codebook of Motion VQ-
VAE visualized in 3D space. (b) Plot of the dynamic transition probability function
β(t,d) across various diffusion steps t.
number of codebooks. Subsequently, the decoder utilizes these motion tokens to
reconstruct the human motion as xˆ=D(z). We train the motion VQ-VAE with
LVQ =||x−xˆ|| 2+||z q−sg[z]|| 2+λVQ||sg[z q]−z|| 2, (6)
where sg[·] represents stop gradient and λVQ is coefficient for commitment loss.
4.2 Dynamic Transition Probability
In the VQ-Diffusion model [17], the transition matrix Q employs a uniform
t
transition probability β across different tokens, as described in Eq. (8). This
t
method overlooks the varying proximity between motion tokens, which is essen-
tial for capturing the context of human motion. To address this, we introduce
a dynamic transition probability that accounts for the distance between tokens.
Duringtheinitialstagesofdiffusion,whenthediffusionsteptislarge,ourmodel
adoptsanexploratoryapproach,allowingforawiderangeoftransitionstofoster
diversity. As t progresses towards 0, the model gradually reduces the favorabil-
ity of transitions between more distantly related tokens, eventually becoming
uniform, identical to the original VQ-Diffusion model [17], as shown in Fig. 4.
The strategy is to begin with broad exploration and progressively narrow the
focus as diffusion steps decrease, thereby improving the precision and coherence
in generating extended motion sequences.
Our approach mathematically formulates the transition probability at each
diffusionsteptasβ(t,d),wheredsignifiesthedistancebetweencodebooktokens.
The transition probability is defined by the following equation:
(cid:18) (cid:19)
t d
β(t,d)=(1−γ −α )·softmax η· · , (7)
t t d T K
where η is a scale factor that modulates the influence of the softmax function
on the relative distances between tokens. The essence of this equation lies in its
softmaxfunctionoverdistances,whichprogressivelyassignshigherprobabilities
to greater distances between tokens as the diffusion step t advances. This allo-
cation adheres to the transition probability constraint γ +α
+(cid:80)K
β(t,d)=1.
t t d=18 S. Chi et al.
Algorithm 1 Two-Phase Sampling (TPS).
Given: Action sentences a={a1,...,aN}
Hyperparameter: T
s
1: z ∼p(z )
T T
2: y←CLIP-TextEncoder(a)
3: for t=T,T−1,...,T +1 do ▷ Joint Sampling
s
4: z ∼p (z |z ,y)
t−1 θ t−1 t
5: end for
6: for t=T ,T −1,...,1 do ▷ Independent Sampling
s s
7: for i=1,...,N do ▷ Parallel Process
8: zi ∼p (zi |zi,yi)
t−1 θ t−1 t
9: end for
10: end for
11: z ←Concat({z1,...,zN})
0 0 0
12: xˆ=D(z )
0
13: return xˆ
Thedistance-basedmodulation,scaledbyη· t · d,ensuresthatasthediffusion
T K
process unfolds, the selection of token transitions is increasingly influenced by
the distance metric. This approach boosts early stage exploration of denoising
byfavoringtransitionstodistanttokens.Thetransitionmatrixisdefinedas
(cid:20) Qˆ 0 (cid:21)
Q = t , where Qˆ =α I+β 11⊤. (8)
t γ ·1⊤ 1 t t (t,di,j)
t
In this matrix, each element d represents the distance d(z ,z ) for indices
i,j i j
i,j ranging from 1 to K, where i and j denote the row and column indices,
respectively. Here, d(·,·) is a distance metric specifically chosen as the rank
index of codebook entries, which are sorted by their L2 distances. The dynamic
and context-sensitive nature of this matrix formulation allows for an adaptive
approach to the diffusion process, modifying transition probabilities in response
to the evolving state of the diffusion and the relative distances between tokens.
4.3 Sampling for Multi-Motion Generation
We introduce a Two-Phase Sampling (TPS) method for the discrete diffusion
model, designed to generate long-term human motion sequences from a series
of action descriptions a = a1,...,aN. This approach enables the creation of
multi-motionsequencesusingamodeltrainedforsingle-motiongeneration.The
overview of TPS is presented in Algorithm 1 and visually depicted in Fig. 2
(d).Inthealgorithm,subscriptsrepresentdiffusionsteps,whilesuperscriptsde-
note action indices. TPS effectively overcomes the challenge of ensuring smooth
transitions between distinct actions, while preserving the distinctiveness of each
motion segment as per its action description.
The denoising process begins by outlining the basic contours of the entire
action sequence, subsequently refining these outlines to achieve semantic coher-
ence with the textual descriptions. Inspired by this approach, our two-phaseM2D2M: Multi-Motion Discrete Diffusion Models 9
sampling starts with joint sampling, where mask tokens from different actions
aremergedandcollectivelydenoisedusingadenoisingTransformer.Thisallows
self-attention mechanism within the Transformer to integrate contextual infor-
mation from action descriptions, ensuring that tokens influence each other to
achieve seamless motion transitions. This step is followed by independent sam-
pling, wherein each action is individually denoised within its designated bound-
aries to align accurately with its specific description.
The number of joint denoising steps, denoted by T , is carefully adjusted to
s
achieve smooth transitions without losing the distinctiveness of each action.
4.4 Denoising Transformer
Motivated by the work of VQ-Diffusion [17], we design a denoising transformer
that estimates the distribution p (z˜ |z ,y) using the Transformer architecture
θ 0 t
[57].AnoverviewofourproposedmodelisdepictedinFig.2(b).Toincorporate
thediffusionsteptintothenetwork,weemploytheadaptivelayernormalization
(AdaLN) [9,35]. The action sentence a is encoded into the action token y using
the CLIP [45] text encoder. The Transformer’s cross-attention mechanism then
integrates this action information with motion, providing a nuanced condition-
ing with the action sentence. To enhance the human motion generation of our
Transformer architecture, we added the following features:
Relative Positional Encoding. One of our primary objectives is the gen-
eration of long-term motion sequences. During the training phase, models ex-
clusively trained on single-motion struggle to generate longer sequences. This
limitation is observed when using traditional absolute positional encodings [57]
that assign a static vector to each position, confining the model’s capability to
the maximum sequence length encountered during its training. By leveraging
Relative Positional Encoding (RPE) [46], we equip our models with the ability
to extrapolate beyond the sequence lengths experienced in training, thus signif-
icantly enhancing their proficiency in generating extended motion sequences.
Classifier-Free Guidance. We adopt classifier-free guidance [25]. This ap-
proach facilitates a balance between diversity and fidelity, allowing both con-
ditional and unconditional sampling from the same model. For unconditional
sampling, a learnable null token, denoted as ∅, may be substituted for the ac-
tion token y. The action token y is replaced by ∅ with a probability of 10%.
When inference, the denoising step is defined using s as follows:
logp (z |z ,y)=(s+1)logp (z |z ,y)−slogp (z |z ,∅). (9)
θ t−1 t θ t−1 t θ t−1 t
5 Experiments
Our experiments are designed to assess the capabilities of our model on two
tasks: 1) multi-motion generation (Sec. 5.2) and 2) single-motion generation
(Sec.5.3).Theseexperimentsareconductedusingthefollowingmotion-language
datasets. HumanML3D [20] is the largest dataset in the domain of language-
annotated 3D human motion, boasting 14,616 sequences of human motion, each10 S. Chi et al.
meticulously aligned to a standard human skeleton template and recorded at
20 FPS. Accompanying these sequences are 44,970 textual descriptions, with an
average length of 12 words. Notably, each motion sequence is associated with
a minimum of three descriptive texts. KIT-ML [43] comprises 3,911 human
motionsequences,eachannotatedwithonetofournaturallanguagedescriptions,
culminating in a total of 6,278 descriptions, averaging 8 words in length. The
datasetcollatedmotionsequencessourcedfromboththeKIT[38]andCMU[12]
datasets,witheachsequencedownsampledto12.5FPS.Eachmotionclipinthis
collection is paired with one to four corresponding textual descriptions.
In our experiments, we perform 10 evaluations for each model, providing
a comprehensive analysis of their performance. We report both the mean and
standard deviation of these evaluations, along with a 95% confidence interval to
ensure statistical robustness. Detailed information about our model implemen-
tation can be found in Appendix A.
5.1 Evaluation of Generated Motions
Single-Motions. To evaluate generated single-motions, we adopt evaluation
metrics from previous works [19,32]. 1) R-Top3 measures the model’s precision
in correlating motion sequences with their corresponding textual descriptions,
highlighting the importance of accurate retrieval from a range of options. 2)
FID (Frechet Inception Distance) evaluates the realism of generated motions
by comparing the distribution of generated data with real data. 3) MM-Dist
(Multi-Modal Distance) gauges the average closeness between features of gen-
erated motions and the text features of their respective descriptions, ensuring
effective synchronization across different modalities. 4) Diversity assesses the
rangeofgeneratedmotions,reflectingthenaturalvariationinhumanmovement.
5) MModality (Multi-Modality) examines the model’s capacity to produce a
wide array of plausible motions from a single text prompt, a key aspect for
versatile motion generation.
Multi-Motions. To evaluate the fidelity of each individual motion within our
generated multi-motion sequences, we compute key metrics used to evaluate
single-motion: R-Top3, FID, MM-Dist, and Diversity. This is measured by
segmenting the generated multi-motion into distinct parts, each corresponding
to a specific action description. Through this process, we can precisely analyze
the alignment between each motion segment and its corresponding action. To
evaluatethetransitionpartformulti-motiongeneration,FIDandDiversityare
evaluatedfor40framesofmotionaroundboundaries,followingthemethodology
explained by [49]. Additionally, we defined a metric for evaluating the smooth-
ness of multi-motion generation near motion boundaries. Jerk [15,18,41,48] is
based on the derivative of acceleration with respect to time. Similar to [18], we
usethelogarithmofadimensionlessjerktoachievescaleinvariancewithrespect
to velocity. Contrary to the Jitter described in the prior study [61], which mea-
suresthesmoothnessofindividualmotions,wecalculatetheintegralofthetime
derivative for each joint in a transitional motion. We then average these valuesM2D2M: Multi-Motion Discrete Diffusion Models 11
(a) Velocity (b) Jerk
Fig.5: Plots for average motion transition produced by our approach versus a
concatenation of two randomly selected real motions: (a) Velocity. (b) Jerk (time
derivative of acceleration normalized by peak velocity).
to derive a single statistic that represents the smoothness of the transition.
(cid:88) 1 (cid:90) t2 d
Jerk = ln ∥ a (t)∥2dt, (10)
v2 dt p 2
p p,peak t1
where p denotes each joint, v p,peak is the maximum speed of the joint p and
[t ,t ] is the time interval for the motion transition. The metric’s intuitive sig-
1 2
nificancebecomesclearwhencomparingasingle-motiontransitiongenerated
by our method to the concatenation of two arbitrarily chosen real motions. As
depicted in Fig. 5, directly joining different real motions results in considerable
jerk. Conversely, our method, which integrates two motions seamlessly using a
two-phase sampling strategy, greatly diminishes jerk. This method facilitates a
smooth transition between motions, resulting in minimal jerk.
5.2 Multi-Motion Generation
The objective of multi-motion generation is to produce continuous human mo-
tion sequences from a series of action descriptions. To achieve this, we adapt
our model, initially trained for single-motion generation, to handle extended se-
quences. This adaptation is facilitated by our two-phase sampling strategy, as
elaborated in Sec. 4.3. To assess the model’s effectiveness in multi-motion gen-
eration, we created a test set by randomly combining N action sentences in the
testset.Thisprocessyieldedatotalof1,448testinstancesfortheHumanML3D
and 532 instances for the KIT-ML for N =4 scenario. We provide further de-
tails about test set generation for multi-motion generation in Appendix B. This
approach ensures a thorough evaluation of the model’s capability to generate
coherent long-term motion sequences from multiple action descriptions.
Baselines. We selected PriorMDM [49] and T2M-GPT [63] as baseline models
for the multi-motion generation task. For the PriorMDM model, we adhered to
theoriginalconfigurationwithoutmakinganymodifications.FortheT2M-GPT,
we extend the approach by concatenating the codebook for each motion and
feeding it into the decoder to accommodate the auto-regressive nature of GPT12 S. Chi et al.
Table 1: Multi-motiongenerationperformanceonHumanML3D.‘IndividualMotion’
denotes individual motions within our motion boundaries. For transitions, ground
truth(single)motionsareindependentlysampledtomatchthetransitionlength,while
ground truth (concat) involves concatenating ground truth motions sampled from the
same textual condition with generated motions.
IndividualMotion Transition(40frames)
Methods
R-Top3↑ FID↓ MMdist↓ Div→ FID↓ Div→ Jerk→
GroundTruth(Single) 0.791±.002 0.002±.000 2.707±.008 9.820±.065 0.003±.002 9.574±.054 1.192±.005
GroundTruth(Concat) - - - - - - 1.371±.004
PriorMDM[49] 0.586±.003 0.832±.017 5.901±.021 9.543±.005 3.351±.034 8.801±.098 0.476±.004
T2M-GPT[63] 0.719±.003 0.342±.019 3.512±.014 9.692±.003 3.412±.027 8.716±.135 1.321±.005
M2D2M 0.733±.003 0.253±.016 3.165±.019 9.806±.005 3.276±.024 8.599±.154 1.238±.008
Table 2: Multi-motion generation performance on KIT-ML.
IndividualMotion Transition(40frames)
Methods
R-Top3↑ FID↓ MMdist↓ Div→ FID↓ Div→ Jerk→
GroundTruth(Single) 0.775±.008 0.034±.004 2.779±.019 11.055±.122 0.041±.005 10.434±.044 1.231±.002
GroundTruth(Concat) - - - - - - 1.469±.003
PriorMDM[49] 0.292±.217 3.311±.106 5.451±.045 10.842±.067 21.231±.844 7.281±.045 0.594±.002
T2M-GPT[63] 0.667±.006 0.907±.059 3.421±.026 10.587±.089 14.494±.547 7.059±.042 1.388±.003
M2D2M 0.711±.006 0.817±.058 3.272±.021 10.337±.122 15.843±.742 7.156±.048 1.351±.003
models. Implementation details of these models can be found in Appendix A.
Importantly, we opted not to include iterative multi-motion generation models
such as TEACH [6] and Multi-Act [36] in our comparison. This decision was
basedonthefactthatthesemodelsrelyontherelationshipsbetweensubsequent
actions, which are not represented in the HumanML3D and KIT-ML datasets
and would require additional annotations for proper evaluation.
Results.Inourcomparativeanalysis,detailedinTables1and2,weestablished
the number of actions for generation at N = 4. Our model demonstrates su-
perior performance in terms of FID, R-Top3, and MMdist. It exhibits a Jerk
value smaller than that of concatenated real motions and approaches the value
of individual real motions. These results suggest that our model is capable of
generatingmulti-motionwithhighfidelitywhilemaintainingcontinuityandcon-
sistency throughout the entire motions. In Table 1, PriorMDM falls short in
FID and R-precision metrics. It also tends to oversmoothens at the transition,
resulting in Jerk metric lower than both real single-motion and concatenated
real motions. This indicates that the motions generated by PriorMDM lack the
depth and subtle characteristics of real motions. A qualitative comparison in
Fig. 1 further demonstrates our model’s superiority, showcasing more natural,
continuous motion compared to T2M-GPT. This quantitative and qualitative
evidence underscores our model’s advanced capability in producing realistic, co-
herent long-term human motions.
5.3 Single-Motion Generation
Results. The task of single-motion generation involves generating human mo-
tionfromindividualactiondescriptions.InTables3and4,wepresentacompara-M2D2M: Multi-Motion Discrete Diffusion Models 13
Table 3: Single-motion generation performance on HumanML3D. The figures high-
lighted in bold and blue denote the best and second-best results, respectively.
Methods R-Top3↑ FID↓ MM-Dist↓ Diversity→ MModality↑
GroundTruth 0.797±.002 0.002±.000 2.974±.008 9.503±.065 -
VQ-VAE(reconstruction) 0.785±.002 0.070±.001 3.072±.009 9.593±.079 -
TEMOS[42] 0.722±.002 3.734±.028 3.703±.008 0.725±.071 0.368±.018
TM2T[21] 0.729±.002 1.501±.017 3.467±.011 8.973±.076 2.424±.093
MotionDiffuse[64] 0.782±.001 0.630±.001 3.113±.001 9.410±.049 1.553±.042
MLD[11] 0.736±.002 1.087±.021 3.347±.008 8.589±.083 2.219±.074
T2M-GPT[63] 0.775±.002 0.116±.004 3.118±.011 9.761±.081 1.856±.011
AttT2M[67] 0.786±.006 0.112±.006 3.038±.007 9.700±.090 2.452±.051
M2DM[32] 0.763±.003 0.352±.005 3.134±.010 9.926±.073 3.587±.072
M2D2M(w/βt) 0.796±.002 0.115±.006 3.036±.008 9.680±.074 2.193±.077
M2D2M(w/β(t,d)) 0.799±.002 0.087±.004 3.018±.008 9.672±.086 2.115±.079
Table 4: Single-motion generation performance on KIT-ML. The figures highlighted
in bold and blue denote the best and second-best results, respectively.
Methods R-Top3↑ FID↓ MM-Dist↓ Diversity→ MModality↑
GroundTruth 0.779±.006 0.031±.004 2.788±.012 11.08±.097 -
VQ-VAE(reconstruction) 0.740±.006 0.472±.011 2.986±.027 10.994±.120 -
TEMOS[42] 0.687±.002 3.717±.028 3.417±.008 10.84±.004 0.532±.018
TM2T[21] 0.587±.005 3.599±.051 4.591±.019 9.473±.100 3.292±.034
MotionDiffuse[64] 0.739±.004 1.954±.062 2.958±.005 11.10±.143 0.730±.013
MLD[11] 0.734±.007 0.404±.027 3.204±.027 10.80±.117 2.192±.071
T2M-GPT[63] 0.737±.006 0.717±.041 3.053±.026 10.862±.094 1.912±.036
AttT2M[67] 0.751±.006 0.870±.039 3.309±.021 10.96±.123 2.281±.047
M2DM[32] 0.743±.004 0.515±.029 3.015±.017 11.417±.097 3.325±037
M2D2M(w/βt) 0.743±.006 0.404±.022 3.018±.019 10.749±.102 2.063±.066
M2D2M(w/β(t,d)) 0.753±.006 0.378±.023 3.012±.021 10.709±.121 2.061±.067
tive analysis of single-motion generation performance against selected baselines.
We present comprehensive comparison in Appendix D. Our approach outper-
forms current state-of-the-art methods on both the HumanML3D and KIT-ML
datasets, particularly excelling in FID and R-Top 3 metrics. While not leading
but closely competitive in other metrics such as MM-Distance, Diversity our
methoddemonstratesrobustnear-bestperformance.Intermsofmulti-modality,
our model exhibits lower performance compared to the top-performing models.
However, there appears to be a trade-off between multi-modality and FID, sug-
gesting that models with higher FID scores may achieve better multi-modality,
as observed in the case of M2DM [32] and TM2T [21].
5.4 Ablation Studies
We conduct ablation studies to assess the effects of Dynamic Transition Proba-
bilityandTwo-PhaseSamplingonourmodel.Duetospaceconstraints,wehave
included further ablation studies in Appendix C.
Dynamic Transition Probability.Wefirstinvestigatetheimpactofdynamic
transition probability presented in wcrefsec:transition. In Table 5, we compare
the performance of our model with a dynamic token transition probability, de-
noted as β(t,d), as opposed to a static one, β . Dynamic transition probability
t
substantially enhances our model’s performance, particularly in terms of FID.14 S. Chi et al.
Table 5: Ablation studies on multi-motion generation performance on HumanML3D.
‘Individual Motion’ denotes individual motions within our motion boundaries.
IndividualMotion Transition(40frames)
Methods
R-Top3↑ FID↓ MMdist↓ Div→ FID↓ Div→ Jerk→
GroundTruth(Single) 0.791 0.002 2.707 9.820 0.003 9.574 1.192
GroundTruth(Concat) - - - - - - 1.371
βt,Ts=100 0.749 0.212 3.015 9.990 3.324 8.681 1.248
βt,Ts=90 0.738 0.253 3.164 9.822 3.483 8.625 1.265
β(t,d),Ts=100 0.751 0.196 3.012 9.894 3.340 8.751 1.248
β(t,d),Ts=90 0.733 0.253 3.165 9.806 3.276 8.599 1.238
Table 6: Multi-motion generation on different smoothing methods on HumanML3D.
IndividualMotion Transition(40frames)
Methods
R-Top3↑ FID↓ MMdist↓ Div→ FID↓ Div→ Jerk→
GroundTruth(Single) 0.791 0.002 2.707 9.820 0.003 9.574 1.192
GroundTruth(Concat) - - - - - - 1.371
Handshake[49] 0.635 1.279 4.182 8.939 3.039 8.566 1.097
SLERP[6] 0.549 1.402 4.679 8.535 4.873 7.912 −3.554
TPS(Ours) 0.733 0.254 3.165 9.806 3.276 8.599 1.238
In addition, the table shows that it enhances the smoothness proven by the
lowest jerk when it is combined with two-phase sampling. This improvement
underscores the importance of the synergistic effect of TPS and β(t,d).
Two-PhaseSampling(TPS).Tocomparemulti-motiongenerationalgorithms
illustrated in Fig. 3, we evaluate the performance of these algorithms on VQ-
DiffusionmodelinTable6,highlightingtheeffectivenessofTPSinmulti-motion
generation. TPS significantly improves the smoothness of long-term motion se-
quences while maintaining fidelity to individual motions, as evidenced by the
improvedJerkandFIDmetrics.However,theresultsforHandshakeandSLERP
showanover-smoothingeffectwhencomparedtotheoriginaldataset,withtheir
Jerk values being lower than that of the single ground truth. Notably, SLERP
even exhibits a negative Jerk value, indicating excessive smoothing.
6 Conclusion & Discussion
We present M2D2M, a model designed for generating multi-motion sequences
from a set of action descriptions. Incorporating a Dynamic Transition Matrix
andTwo-PhaseSampling,M2D2Machievesstate-of-the-artperformanceingen-
erating human motion from text tasks. For multi-motion generation, a ground
truthisabsentaswecreateextendedsequencesfrommultipleactiondescriptions.
Consequently, we introduce a new evaluation metric to assess the smoothness of
the motion. However, these metrics do not comprehensively evaluate the gener-
ated multi-motion sequences, as they do not account for all possible scenarios.
Addressing this limitation remains for future work. Additionally, our research
aimstoenhancevirtualrealityandassistivetechnologiesbutcouldraiseprivacy
andsecurityconcerns,requiringstrictdatapoliciesandtransparentmonitoring.M2D2M: Multi-Motion Discrete Diffusion Models 15
Acknowledgements
WeacknowledgeFeddersenChairFundsandtheUSNationalScienceFoundation
(FW-HTF 1839971, PFI-TT 2329804) for Dr. Karthik Ramani. Any opinions,
findings,andconclusionsexpressedinthismaterialarethoseoftheauthorsand
do not necessarily reflect the views of the funding agency. We sincerely thank
the reviewers for their constructive suggestions.
References
1. Ahn, H., Ha, T., Choi, Y., Yoo, H., Oh, S.: Text2action: Generative adversarial
synthesis from language to action. In: 2018 IEEE International Conference on
Robotics and Automation (ICRA). pp. 5915–5920. IEEE (2018)
2. Ahuja, C., Morency, L.P.: Language2pose: Natural language grounded pose fore-
casting.In:2019InternationalConferenceon3DVision(3DV).pp.719–728.IEEE
(2019)
3. Alexanderson,S.,Nagy,R.,Beskow,J.,Henter,G.E.:Listen,denoise,action!audio-
driven motion synthesis with diffusion models. ACM Transactions on Graphics
(TOG) 42(4), 1–20 (2023)
4. Aliakbarian, S., Saleh, F.S., Salzmann, M., Petersson, L., Gould, S.: A stochastic
conditioning scheme for diverse human motion prediction. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5223–
5232 (2020)
5. Ao, T., Zhang, Z., Liu, L.: Gesturediffuclip: Gesture diffusion model with clip
latents. arXiv preprint arXiv:2303.14613 (2023)
6. Athanasiou, N., Petrovich, M., Black, M.J., Varol, G.: Teach: Temporal action
compositions for 3d humans. In: International Conference on 3D Vision (3DV)
(September 2022)
7. Austin, J., Johnson, D.D., Ho, J., Tarlow, D., Van Den Berg, R.: Structured de-
noising diffusion models in discrete state-spaces. Advances in Neural Information
Processing Systems 34, 17981–17993 (2021)
8. Azadi, S., Shah, A., Hayes, T., Parikh, D., Gupta, S.: Make-an-animation:
Large-scale text-conditional 3d human motion generation. arXiv preprint
arXiv:2305.09662 (2023)
9. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint
arXiv:1607.06450 (2016)
10. Bhattacharya,U.,Rewkowski,N.,Banerjee,A.,Guhan,P.,Bera,A.,Manocha,D.:
Text2gestures:Atransformer-basednetworkforgeneratingemotivebodygestures
for virtual agents. In: 2021 IEEE virtual reality and 3D user interfaces (VR). pp.
1–10. IEEE (2021)
11. Chen,X.,Jiang,B.,Liu,W.,Huang,Z.,Fu,B.,Chen,T.,Yu,G.:Executingyour
commandsviamotiondiffusioninlatentspace.In:ProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.18000–18010(2023)
12. CMU Graphics Lab: Motion capture database. http://mocap.cs.cmu.edu (2016)
13. Dhariwal,P.,Nichol,A.:Diffusionmodelsbeatgansonimagesynthesis.Advances
in neural information processing systems 34, 8780–8794 (2021)
14. Duan, Y., Shi, T., Zou, Z., Lin, Y., Qian, Z., Zhang, B., Yuan, Y.: Single-shot
motion completion with transformer. arXiv preprint arXiv:2103.00776 (2021)16 S. Chi et al.
15. Flash,T.,Hogan,N.:Thecoordinationofarmmovements:anexperimentallycon-
firmed mathematical model. Journal of neuroscience 5(7), 1688–1703 (1985)
16. Ghosh,A.,Cheema,N.,Oguz,C.,Theobalt,C.,Slusallek,P.:Synthesisofcompo-
sitional animations from textual descriptions. In: Proceedings of the IEEE/CVF
international conference on computer vision. pp. 1396–1406 (2021)
17. Gu, S., Chen, D., Bao, J., Wen, F., Zhang, B., Chen, D., Yuan, L., Guo, B.:
Vectorquantizeddiffusionmodelfortext-to-imagesynthesis.In:Proceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.10696–
10706 (2022)
18. Gulde,P.,Hermsdörfer,J.:Smoothnessmetricsincomplexmovementtasks.Fron-
tiers in Neurology 9, 615 (09 2018). https://doi.org/10.3389/fneur.2018.
00615
19. Guo, C., Zou, S., Zuo, X., Wang, S., Ji, W., Li, X., Cheng, L.: Generating di-
verseandnatural3dhumanmotionsfromtext.In:ProceedingsoftheIEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 5152–5161 (2022)
20. Guo, C., Zou, S., Zuo, X., Wang, S., Ji, W., Li, X., Cheng, L.: Generating di-
verseandnatural3dhumanmotionsfromtext.In:ProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR).pp.5152–5161
(June 2022)
21. Guo, C., Zuo, X., Wang, S., Cheng, L.: Tm2t: Stochastic and tokenized model-
ing for the reciprocal generation of 3d human motions and texts. In: European
Conference on Computer Vision. pp. 580–597. Springer (2022)
22. Harvey,F.G.,Pal,C.:Recurrenttransitionnetworksforcharacterlocomotion.In:
SIGGRAPH Asia 2018 Technical Briefs, pp. 1–4 (2018)
23. Harvey, F.G., Yurick, M., Nowrouzezahrai, D., Pal, C.: Robust motion in-
betweening. ACM Transactions on Graphics (TOG) 39(4), 60–1 (2020)
24. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in
neural information processing systems 33, 6840–6851 (2020)
25. Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598 (2022)
26. Hoogeboom, E., Nielsen, D., Jaini, P., Forré, P., Welling, M.: Argmax flows and
multinomial diffusion: Learning categorical distributions (2021), https://arxiv.
org/abs/2102.05379
27. Huang, G., Qian, X., Wang, T., Patel, F., Sreeram, M., Cao, Y., Ramani, K.,
Quinn, A.J.: Adaptutar: An adaptive tutoring system for machine tasks in aug-
mented reality. In: Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems. pp. 1–15 (2021)
28. Ipsita, A., Li, H., Duan, R., Cao, Y., Chidambaram, S., Liu, M., Ramani, K.:
Vrfromx: from scanned reality to interactive virtual experience with human-in-
the-loop. In: Extended Abstracts of the 2021 CHI Conference on Human Factors
in Computing Systems. pp. 1–7 (2021)
29. Jiang,B.,Chen,X.,Liu,W.,Yu,J.,Yu,G.,Chen,T.:Motiongpt:Humanmotion
as a foreign language. arXiv preprint arXiv:2306.14795 (2023)
30. Kaufmann,M.,Aksan,E.,Song,J.,Pece,F.,Ziegler,R.,Hilliges,O.:Convolutional
autoencodersforhumanmotioninfilling.In:2020InternationalConferenceon3D
Vision (3DV). pp. 918–927. IEEE (2020)
31. Komura, T., Habibie, I., Holden, D., Schwarz, J., Yearsley, J.: A recurrent vari-
ational autoencoder for human motion synthesis. In: The 28th British Machine
Vision Conference (2017)M2D2M: Multi-Motion Discrete Diffusion Models 17
32. Kong,H.,Gong,K.,Lian,D.,Mi,M.B.,Wang,X.:Priority-centrichumanmotion
generationindiscretelatentspace.In:ProceedingsoftheIEEE/CVFInternational
Conference on Computer Vision. pp. 14806–14816 (2023)
33. Kucherenko,T.,Hasegawa,D.,Henter,G.E.,Kaneko,N.,Kjellström,H.:Analyz-
inginputandoutputrepresentationsforspeech-drivengesturegeneration.In:Pro-
ceedingsofthe19thACMInternationalConferenceonIntelligentVirtualAgents.
pp. 97–104 (2019)
34. Lee, H.Y., Yang, X., Liu, M.Y., Wang, T.C., Lu, Y.D., Yang, M.H., Kautz, J.:
Dancing to music. Advances in neural information processing systems 32 (2019)
35. Lee,K.,Chang,H.,Jiang,L.,Zhang,H.,Tu,Z.,Liu,C.:ViTGAN:TrainingGANs
withvisiontransformers.In:InternationalConferenceonLearningRepresentations
(2022), https://openreview.net/forum?id=dwg5rXg1WS_
36. Lee, T., Moon, G., Lee, K.M.: Multiact: Long-term 3d human motion generation
frommultipleactionlabels.In:AAAIConferenceonArtificialIntelligence(AAAI)
(2023)
37. Lin, A.S., Wu, L., Corona, R., Tai, K., Huang, Q., Mooney, R.J.: Generating an-
imated videos of human activities from natural language descriptions. Learning
2018(1) (2018)
38. Mandery, C., Terlemez, Ö., Do, M., Vahrenkamp, N., Asfour, T.: The kit whole-
body human motion database. In: 2015 International Conference on Advanced
Robotics (ICAR). pp. 329–336. IEEE (2015)
39. Mao,W.,Liu,M.,Salzmann,M.:Historyrepeatsitself:Humanmotionprediction
via motion attention. In: Computer Vision–ECCV 2020: 16th European Confer-
ence, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16. pp. 474–489.
Springer (2020)
40. Mao,W.,Liu,M.,Salzmann,M.,Li,H.:Learningtrajectorydependenciesforhu-
manmotionprediction.In:ProceedingsoftheIEEE/CVFinternationalconference
on computer vision. pp. 9489–9497 (2019)
41. Mobini, A., Behzadipour, S., Foumani, M.: Test–retest reliability of kinect’s mea-
surementsfortheevaluationofupperbodyrecoveryofstrokepatients.Biomedical
engineering online 14, 75 (08 2015). https://doi.org/10.1186/s12938-015-
0070-0
42. Petrovich, M., Black, M.J., Varol, G.: Temos: Generating diverse human motions
fromtextualdescriptions.In:EuropeanConferenceonComputerVision.pp.480–
497. Springer (2022)
43. Plappert,M.,Mandery,C.,Asfour,T.:Thekitmotion-languagedataset.Bigdata
4(4), 236–252 (2016)
44. Punnakkal,A.R.,Chandrasekaran,A.,Athanasiou,N.,Quiros-Ramirez,A.,Black,
M.J.:Babel:Bodies,actionandbehaviorwithenglishlabels.In:Proceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.722–731
(2021)
45. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
naturallanguagesupervision.In:Internationalconferenceonmachinelearning.pp.
8748–8763. PMLR (2021)
46. Raffel,C.,Shazeer,N.,Roberts,A.,Lee,K.,Narang,S.,Matena,M.,Zhou,Y.,Li,
W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text
transformer. The Journal of Machine Learning Research 21(1), 5485–5551 (2020)
47. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 10684–10695 (2022)18 S. Chi et al.
48. Roren,A.,Mazarguil,A.,Vaquero-Ramos,D.,Deloose,J.B.,Vidal,P.P.,Nguyen,
C.,Rannou,F.,Wang,D.,Oudre,L.,lefevrecolau,m.m.:Assessingsmoothnessof
armmovementswithjerk:Acomparisonoflaterality,contractionmodeandplane
of elevation. a pilot study. Frontiers in Bioengineering and Biotechnology 9 (01
2022). https://doi.org/10.3389/fbioe.2021.782740
49. Shafir, Y., Tevet, G., Kapon, R., Bermano, A.H.: Human motion diffusion as a
generative prior. In: The Twelfth International Conference on Learning Represen-
tations (2024), https://openreview.net/forum?id=dTpbEdN9kr
50. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-
visedlearningusingnonequilibriumthermodynamics.In:Internationalconference
on machine learning. pp. 2256–2265. PMLR (2015)
51. Song,Y.,Ermon,S.:Improvedtechniquesfortrainingscore-basedgenerativemod-
els. Advances in neural information processing systems 33, 12438–12448 (2020)
52. Tang, X., Wang, H., Hu, B., Gong, X., Yi, R., Kou, Q., Jin, X.: Real-time con-
trollable motion transition for characters. ACM Transactions on Graphics (TOG)
41(4), 1–10 (2022)
53. Tevet, G., Gordon, B., Hertz, A., Bermano, A.H., Cohen-Or, D.: Motionclip: Ex-
posing human motion generation to clip space. In: European Conference on Com-
puter Vision. pp. 358–374. Springer (2022)
54. Tevet,G.,Raab,S.,Gordon,B.,Shafir,Y.,Cohen-or,D.,Bermano,A.H.:Human
motion diffusion model. In: The Eleventh International Conference on Learning
Representations (2023), https://openreview.net/forum?id=SJ1kSyO2jwu
55. Tulyakov, S., Liu, M.Y., Yang, X., Kautz, J.: Mocogan: Decomposing motion and
contentforvideogeneration.In:ProceedingsoftheIEEEconferenceoncomputer
vision and pattern recognition. pp. 1526–1535 (2018)
56. Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning.
Advances in neural information processing systems 30 (2017)
57. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
Ł., Polosukhin, I.: Attention is all you need. Advances in neural information pro-
cessing systems 30 (2017)
58. Yan,S.,Li,Z.,Xiong,Y.,Yan,H.,Lin,D.:Convolutionalsequencegenerationfor
skeleton-based action synthesis. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 4394–4402 (2019)
59. Yan,X.,Rastogi,A.,Villegas,R.,Sunkavalli,K.,Shechtman,E.,Hadap,S.,Yumer,
E., Lee, H.: Mt-vae: Learning motion transformations to generate multimodal hu-
man dynamics. In: Proceedings of the European conference on computer vision
(ECCV). pp. 265–281 (2018)
60. Yi,X.,Zhou,Y.,Xu,F.:Transpose:real-time3dhumantranslationandposeesti-
mation with six inertial sensors. ACM Trans. Graph. 40(4) (jul 2021). https:
//doi.org/10.1145/3450626.3459786, https://doi.org/10.1145/3450626.
3459786
61. Yi, X., Zhou, Y., Xu, F.: Transpose: Real-time 3d human translation and pose
estimationwithsixinertialsensors.ACMTransactionsOnGraphics(TOG)40(4),
1–13 (2021)
62. Yin, T., Hoyet, L., Christie, M., Cani, M.P., Pettré, J.: The one-man-crowd: Sin-
gle user generation of crowd motions using virtual reality. IEEE Transactions on
Visualization and Computer Graphics 28(5), 2245–2255 (2022)
63. Zhang, J., Zhang, Y., Cun, X., Huang, S., Zhang, Y., Zhao, H., Lu, H., Shen,
X.: T2m-gpt: Generating human motion from textual descriptions with discrete
representations.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition (CVPR) (2023)M2D2M: Multi-Motion Discrete Diffusion Models 19
64. Zhang, M., Cai, Z., Pan, L., Hong, F., Guo, X., Yang, L., Liu, Z.: Motiondif-
fuse: Text-driven human motion generation with diffusion model. arXiv preprint
arXiv:2208.15001 (2022)
65. Zhang, M., Guo, X., Pan, L., Cai, Z., Hong, F., Li, H., Yang, L., Liu, Z.: Re-
modiffuse: Retrieval-augmented motion diffusion model. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 364–373 (2023)
66. Zhang, M., Li, H., Cai, Z., Ren, J., Yang, L., Liu, Z.: Finemogen: Fine-grained
spatio-temporal motion generation and editing. Advances in Neural Information
Processing Systems 36 (2024)
67. Zhong, C., Hu, L., Zhang, Z., Xia, S.: Attt2m: Text-driven human motion
generation with multi-perspective attention mechanism. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 509–519 (2023)Supplementary Material of M2D2M:
Multi-Motion Generation from Text with
Discrete Diffusion Models
Seunggeun Chi1,2∗, Hyung-gun Chi2∗, Hengbo Ma‡, Nakul Agarwal1,
Faizan Siddiqui1, Karthik Ramani2†, and Kwonjoon Lee1†
1Honda Research Institute USA 2Purdue University
{chi65, chi45}@purdue.edu,
ramani@purdue.edu, kwonjoon_lee@honda-ri.com
In the supplementary material, we offer additional details and experiments
thatarenotincludedinthemainpaperduetothepagelimit.Thisincludesim-
plementation specifics and architectural design, along with baseline implemen-
tation methodologies (Appendix A). Additionally, we describe the generation of
testsetsforthemulti-motiongenerationtaskinAppendixB,presentfurtherab-
lationstudiesinAppendixC,reportcomprehensivecomparisoninAppendixD,
and provide in-depth analysis of our work in Appendix E. Lastly, we include
extra qualitative results in Appendix F.
A Additional Details
A.1 M2D2M
Motion VQ-VAE. In developing the Motion VQ-VAE, we adopt the architec-
tureproposedbyZhangetal.[63].Weconstructboththeencoderanddecoderof
theMotionVQ-VAEusingaCNN-basedarchitecture,specificallyemploying1D
convolutions.Additionally,weadheretothesamehyperparametersandtraining
procedures as outlined in their study.
Denoising Transformer. The denoising transformer configuration is specified
asfollows:12layers,16attentionheads,512embeddingdimensions,2048hidden
dimensions, and a dropout rate of 0. Also, we designed action sentence condi-
tioningforthedenoisingtransformertoenablethemulti-motiongenerationtask
with the HumanML3D dataset and KIT-ML dataset. We focus on the action
verbs within a sentence (i.e., ‘walk’, ‘turn around’) of datasets, because they
offer clear information about the type of motion involved. Therefore, we further
break down the sentence using action verbs and then enrich them to form a
complete action description, like ‘a person walking,’ which serves as the basis
for conditioning the motion generation as illustrated in Fig. 1. For a joint sam-
pling of Two-Phase Sampling (TPS), which aims to create a seamless motion
sequence, we concatenate action tokens from successive actions for condition-
ing. This forms a compound condition that infuses the motion generation with
∗ Co-first authors. † Senior authors. ‡ Work done at Honda Research Institute.2 S. Chi et al.
Fig.1: Overview of action sentence conditioning of M2D2M. We initially decompose
sentencestoextractactionverbsandsubsequentlyutilizetheseverbstoconstructnew
sentences.Thesenewlyformedsentencesthenserveasconditionsforgeneratinghuman
motion sequences.
contextual information, ensuring the resulting sequence is both cohesive and
reflective of the intended actions.
Implementation Details.Ourmodeladherestothehyper-parametersettings
ofVQ-Diffusion[17]unlessotherwisestated,encompassingtheconfigurationsfor
the transition matrix parameters, namely α¯ and γ¯ . We linearly increase the γ¯
t t t
anddecreasetheα¯ .Thelosscoefficientissetatλ=5.0×10−4asperEq.(3),and
t
the diffusion process is defined over T =100 timesteps. Optimization is carried
out using the AdamW optimizer with a learning rate of 2.0×10−4, β = 0.9,
1
β = 0.99, and weight decay 4.5×10−2. We trained the model for 110 epochs,
2
and the learning rate decayed to 2.0×10−5 at the 100th epoch. We use the
guidance scale of s=4 for single motion generation, and s=2 for multi-motion
generation. When generating multi-motion sequence, we use T = 90 for TPS.
s
For generating single motions, we apply a Dynamic Transition Probability scale
factor of η = 0.5, and for multi-action generation, we adjust the scale factor to
η =0.25.
A.2 Baselines for Multi-Motion Generation
We evaluated the baseline methods of T2M-GPT‡ [63] and PriorMDM§ [49]
for the task of multi-motion generation based on the code provided from the
original papers. For a fair comparison with T2M-GPT, we modified the model
to produce codebooks matching the specified ground truth length by disabling
theend-tokenoutput.Thesecodebookswerethenconcatenatedforeachmotion
and fed into the decoder. In the case of PriorMDM and Handshake [49], we set
the hyper-parameter to match the illustration of Fig. 3 in the main paper for
the fair comparison, employing a handshake size of 40 and transition margins
‡ https://github.com/Mael-zys/T2M-GPT § https://githubwcom/priorMDM/priorMDMSupplementary Material of M2D2M 3
Table 1: Comparison table for Multi-motion generation performance with different
classifier-free scales on HumanML3D dataset.
Classifier-free IndividualMotion Transition(40frames)
GuidanceScale(s) R-Top3↑ FID↓ MMdist↓ Div→ FID↓ Div→ Jerk→
GroundTruth(Single) 0.791±.002 0.002±.000 2.707±.008 9.820±.065 0.003±.002 9.574±.054 1.192±.005
GroundTruth(Concat) - - - - - - 1.371±.004
1.0 0.628±.005 0.350±.021 3.836±.019 9.573±.156 3.299±.152 8.395±.142 1.246±.006
1.5 0.705±.004 0.254±.017 3.063±.017 9.777±.170 3.293±.177 8.545±.115 1.242±.009
2.0 0.733±.003 0.254±.016 3.165±.019 9.806±.158 3.276±.173 8.599±.154 1.238±.008
2.5 0.746±.006 0.262±.025 3.063±.017 9.844±.148 3.321±.178 8.622±.124 1.252±.009
3.0 0.751±.006 0.270±.020 3.042±.023 9.795±.147 3.400±.194 8.648±.130 1.263±.007
of 20. For the other hyper-parameters, we follow the setup of PriorMDM [49].
For the SLERP algorithm, unlike the TEACH [6] setup, we first independently
generate individual motions with half-transition length shorter than the given
ground truth length, then apply SLERP as illustrated in Fig. 3 of the main
paper. We computed the FID score based on their prescribed method, for both
individual motions and transitions.
B Multi-Motion Generation Test Set
Due to the absence of distinct motion boundaries in multi-action verb annota-
tionswithintheHumanML3DandKIT-MLdatasetsusedinourexperiments,we
opted for test sets that exclusively consist of single action verbs. In the curated
testsets,eachsentenceincludesonlyoneactionverb,suchas‘walk’or‘run’.We
thenrandomlyselectedN actiondescriptionsfromthispoolofsingle-actionverb
sentences, ensuring no overlap, to create our test set for the multi-motion gen-
eration task. Specifically, for N =4, the test set from the HumanML3D dataset
comprises1448motions,eachassociatedwithasingle-verbannotation.Similarly,
thetestsetfromtheKIT-MLdatasetincludes532motions,allcharacterizedby
single action verb annotations.
C Additional Ablation Studies
In this section, we present a series of additional ablation studies that were not
included in Sec. 5.4 of the main paper due to the page limit. It includes 1)
exploringdifferentclassifier-freeguidancescales(AppendixC.1),2)assessingour
model’sperformancewithvaryingnumbersofactionsinmulti-motiongeneration
tasks(AppendixC.2),3)examiningthesmoothness-fidelitytrade-offatdifferent
independent sampling steps in TPS (Appendix C.4), and finally, 4) evaluating
the Dynamic Transition Probability scale η (Appendix C.5).
C.1 Classifier-free Guidance Scale
We first focus on the effect of different classifier guidance scales s, which is de-
scribed in Eq. (9). To evaluate the performance of our model in multi-motion
generation and single-motion generation, we utilize the HumanML3D dataset,4 S. Chi et al.
Table2:Single-motiongenerationperformanceonthedifferentclassifier-freeguidance
scale on HumanML3D.
Classifier-free
GuidanceScale(s)
R-Top3↑ FID↓ MM-Dist↓ Diversity→
GroundTruth 0.797±.002 0.002±.000 2.974±.008 9.503±.065
0.0 0.686±.003 0.107±.005 3.690±.008 9.580±.088
1.0 0.786±.003 0.146±.002 3.084±.008 9.897±.088
2.0 0.804±.003 0.139±.004 2.995±.008 9.886±.082
3.0 0.803±.002 0.107±.003 2.980±.006 9.815±.089
4.0 0.799±.002 0.087±.003 3.018±.008 9.672±.086
5.0 0.787±.002 0.127±.007 3.089±.007 9.439±.086
Table 3: Single motion generation performance on different distance functions for
d(·,·) on Human3D dataset.
Methods R-Top3↑ FID↓ MM-Dist↓ Diversity→ MModality↑
L2 0.798±.002 0.098±.005 3.018±.008 9.623±.085 2.115±.079
L2Rank 0.799±.002 0.087±.004 3.018±.008 9.672±.086 2.132±.073
Cosine 0.801±.002 0.092±.004 3.011±.008 9.670±.084 2.137±.084
CosineRank 0.797±.002 0.099±.005 3.026±.008 9.669±.085 2.125±.069
and provide results presented in Table 2. This experiment reveals that the op-
timal balance between accuracy and fidelity for these metrics is achieved at a
classifier guidance scale of s=4 for single-motion generation, and best smooth-
ness at s=2 for multi-motion generation.
C.2 Number of Action in Multi-Motion Generation
In order to explore our model’s effectiveness in generating long-term motion, we
evaluate the performance of our model by progressively increasing the number
of actions (N) using the HumanML3D dataset. The results of these evaluations
are detailed in Table 4. We found that as N increases, R-Top3 and FID scores
of individual motion demonstrate a decline, indicating a reduction in fidelity
with more actions. Despite this, it’s noteworthy that our model’s performance
on the transition part remains comparably effective to that of real single mo-
tions, even at N =32, a considerably long motion sequence. This highlights our
model’s proficiency in generating long-term motion with smooth and coherent
transitions.
C.3 Different Distance metrics for Dynamic Transition Probability
InTable3,weconductacomparativeanalysisofdifferentdistancefunctionsfor
d(·,·),utilizedindefiningthecodebookdistanceforEq.(8).Specifically,weeval-
uate the performance of L2 and Cosine Distance, focusing on their effectiveness
as distance functions. Our findings indicate that the L2 Rank distance function
yields the best FID score, highlighting its superiority in this context.
C.4 Effect of Two-Phase Sampling
In Table 5, we explore the impact of Two-Phase Sampling. Our analysis also in-
cludes adjustments in the ratio of independent denoising steps (T ) to the total
sSupplementary Material of M2D2M 5
Table 4: Multi-motiongenerationperformanceonthedifferentnumberofactions(N)
on HumanML3D.
Thenumber IndividualMotion Transition(40frames)
ofactions(N) R-Top3↑ FID↓ MMdist↓ Div→ FID↓ Div→ Jerk→
GroundTruth(Single) 0.791±.002 0.002±.000 2.707±.008 9.820±.065 0.003±.002 9.574±.054 1.192±.005
GroundTruth(Concat) - - - - - - 1.371±.004
N=1 0.751±.008 0.196±.003 3.012±.018 9.894±.057 3.340±.219 8.751±.005 1.248±.005
N=2 0.737±.007 0.198±.025 3.127±.031 9.870±.064 3.430±.431 8.497±.121 1.244±.013
N=4 0.733±.003 0.254±.016 3.165±.019 9.806±.158 3.276±.173 8.599±.154 1.238±.008
N=8 0.733±.005 0.307±.027 3.153±.028 9.624±.137 3.343±.092 8.675±.121 1.255±.010
N=16 0.725±.004 0.312±.031 3.193±.018 9.557±.066 3.380±.109 8.455±.165 1.245±.011
N=32 0.731±.005 0.350±.040 3.192±.023 9.555±.069 3.336±.145 8.537±.182 1.248±.013
Table 5: Multi-motion generation performance across a different number of indepen-
dent denoising steps (T ) of Two-Phase Sampling on HumanML3D.
s
IndividualMotion Transition(40frames)
Methods
R-Top3↑ FID↓ MMdist↓ Div→ FID↓ Div→ Jerk→
GroundTruth(Single) 0.791±.002 0.002±.000 2.707±.008 9.820±.065 0.003±.002 9.574±.054 1.192±.005
GroundTruth(Concat) - - - - - - 1.371±.004
w/oTPS 0.755±.007 0.173±.010 3.015±.024 9.950±.076 3.455±.142 8.554±.081 1.402±.005
Ts=100 0.751±.008 0.196±.003 3.012±.018 9.894±.057 3.340±.219 8.751±.005 1.248±.005
Ts=95 0.737±.004 0.232±.028 3.105±.017 9.772±.167 3.289±.243 8.643±.132 1.253±.007
Ts=90 0.733±.003 0.254±.016 3.165±.019 9.806±.158 3.276±.173 8.599±.154 1.238±.008
Ts=80 0.725±.006 0.284±.024 3.194±.029 9.767±.129 3.338±.129 8.691±.114 1.247±.007
Ts=50 0.709±.006 0.371±.034 3.315±.018 9.665±.125 3.282±.263 8.595±.144 1.254±.010
Table 6: Multi-motion generation on different smoothing methods with MDM on
HumanML3D.
IndividualMotion Transition(40frames)
Methods
R-Top3↑ FID↓ MMdist↓ Div→ FID↓ Div→ Jerk→
GroundTruth(Single) 0.791 0.002 2.707 9.820 0.003 9.574 1.192
GroundTruth(Concat) - - - - - - 1.371
MDM[54]+Handshake[49] 0.586 0.832 5.901 9.543 3.351 8.801 0.476
MDM[54]+TPS(Ours) 0.640 0.582 5.287 9.321 3.376 8.070 0.634
numberofdenoisingsteps(T).Thisexaminationrevealsacleartrade-offinmo-
tiongenerationbetweensmoothnessandfidelity.AsdiscussedinSec.4.3,phases
ofindependentsamplingenhancethefidelityofindividualmotions,whilephases
of joint sampling improve the fidelity and smoothness of transitions between
motions. Implementing the Two-Phase Sampling algorithm and reducing the
number of independent sampling steps (T ) tends to improve smoothness met-
s
rics (e.g., Jerk), but simultaneously, fidelity metrics such as R-Top3 and FID
begintodeteriorate.Thisobservationemphasizestheintrinsictrade-offbetween
smoothnessandfidelityinmotiongeneration,identifyinganoptimalT =90for
s
the smoothness metric being identified.
In Table 6, we evaluate multi-motion generation algorithms on non-latent
diffusion models. We applied Handshake [49] and TPS to MDM [54], a diffusion
model operating in Cartesian space with 3D skeletal coordinates. We observe
that the effectiveness of TPS is not confined to its designed latent space; it
also functions effectively in the Cartesian domain. The results show that TPS
achieves better FID and R-Precision for individual motions, albeit with reduced6 S. Chi et al.
Table 7: Multi-motion generation performance across a different number of indepen-
dent denoising steps (T ) of Two-Phase Sampling on HumanML3D.
s
TransitionProbability IndividualMotion Transition(40frames)
Methods R-Top3↑ FID↓ MMdist↓ Div→ FID↓ Div→ Jerk→
GroundTruth(Single) 0.791±.002 0.002±.000 2.707±.008 9.820±.065 0.003±.002 9.574±.054 1.192±.005
GroundTruth(Concat) - - - - - - 1.371±.004
β(t) - 0.738±.009 0.253±.002 3.164±.021 9.822±.051 3.483±.029 8.625±.044 1.265±.005
β(d,t) η=1.00 0.730±.005 0.264±.026 3.152±.028 9.808±.162 3.315±.225 8.654±0.064 1.252±.007
β(d,t) η=0.50 0.733±.003 0.244±.016 3.156±.029 9.830±.160 3.278±.138 8.586±.127 1.250±.008
β(d,t) η=0.33 0.732±004 0.245±.010 3.150±.173 9.815±.152 3.312±.171 8.675±.134 1.246±.009
β(d,t) η=0.25 0.734±.003 0.253±.016 3.165±.019 9.806±.158 3.276±.017 8.599±.154 1.238±.008
β(d,t) η=0.20 0.724±005 0.254±.010 3.194±.026 9.803±.152 3.330±.205 8.519±.162 1.247±.008
Fig.2: PCA plot representing motion tokens from the codebook of Motion VQ-VAE,
visualized in 2D (Left) and 3D (Right) space.
diversity. For the transition part, TPS demonstrates comparable FID results
while exhibiting improved smoothness as measured by Jerk.
C.5 The scale of Transition Probability Matrix
We investigated the impact of dynamic transition probability on the generation
of multiple motions by conducting an ablation study that varied the transition
probabilityscale,η.InTable7,wenotedthatthedynamictransitionprobability,
β(d,t), outperforms the traditional method of β(t). Additionally, the results
indicate a trend where the smoothness metric (Jerk) becomes closer to ground
truth single motion as η is reduced.
D Comprehensive Comparison of Single-Motion
Generation
In Tables 8 and 9, we present a comprehensive comparison of single-motion
generation results to demonstrate the effectiveness of our method.Supplementary Material of M2D2M 7
Table 8: Single-motion generation performance on HumanML3D. The figures high-
lighted in bold and blue denote the best and second-best results, respectively.
Methods R-Top3↑ FID↓ MM-Dist↓ Diversity→ MModality↑
GroundTruth 0.797±.002 0.002±.000 2.974±.008 9.503±.065 -
VQ-VAE(reconstruction) 0.785±.002 0.070±.001 3.072±.009 9.593±.079 -
Seq2Seq[37] 0.396±.002 11.75±.035 5.529±.007 6.223±.061 -
J2LP[2] 0.486±.002 11.02±.046 5.296±.008 6.223±.058 -
Text2Gesture[10] 0.345±.002 5.012±.030 6.030±.008 7.676±.071 -
Hier[16] 0.552±.004 6.532±.024 5.012±.018 6.409±.042 -
MoCoGAN[55] 0.106±.001 94.41±.021 9.643±.006 8.332±.008 0.019±.000
Dance2Music[34] 0.097±.001 66.98±.016 8.116±.006 0.462±.011 0.043±.001
TEMOS[42] 0.722±.002 3.734±.028 3.703±.008 0.725±.071 0.368±.018
TM2T[21] 0.729±.002 1.501±.017 3.467±.011 8.973±.076 2.424±.093
MLD[11] 0.736±.002 1.087±.021 3.347±.008 8.589±.083 2.219±.074
Guoetal.[19] 0.772±.002 0.473±.013 3.196±.010 9.175±.082 2.413±.079
MDM[54] 0.611±.007 0.544±.044 5.566±.027 9.724±.086 2.799±.072
MotionDiffuse[64] 0.782±.001 0.630±.001 3.113±.001 9.410±.049 1.553±.042
T2M-GPT[63] 0.775±.002 0.116±.004 3.118±.011 9.761±.081 1.856±.011
AttT2M[67] 0.786±.006 0.112±.006 3.038±.007 9.700±.090 2.452±.051
MAA[8] 0.675±.002 0.774±.007 - 8.230±.064 -
M2DM[32] 0.763±.003 0.352±.005 3.134±.010 9.926±.073 3.587±.072
M2D2M(w/βt) 0.796±.002 0.115±.006 3.036±.008 9.680±.074 2.193±.077
M2D2M(w/β(t,d)) 0.799±.002 0.087±.004 3.018±.008 9.672±.086 2.115±.079
Table 9: Single-motion generation performance on KIT-ML. The figures highlighted
in bold and blue denote the best and second-best results, respectively.
Methods R-Top3↑ FID↓ MM-Dist↓ Diversity→ MModality↑
GroundTruth 0.779±.006 0.031±.004 2.788±.012 11.08±.097 -
VQ-VAE(reconstruction) 0.740±.006 0.472±.011 2.986±.027 10.994±.120 -
Seq2Seq[37] 0.241±.006 24.86±.348 7.960±.031 6.744±.106 -
J2LP[2] 0.483±.005 6.545±.072 5.147±.030 9.073±.100 -
Text2Gesture[10] 0.338±.005 12.12±.183 6.964±.029 9.334±.079 -
Hier[16] 0.531±.007 5.203±.107 4.986±.027 9.563±.072 -
MoCoGAN[55] 0.063±.003 82.69±.242 10.47±.012 3.091±.043 0.250±.009
Dance2Music[34] 0.086±.003 115.4±.240 10.40±.016 0.241±.004 0.062±.002
TEMOS[42] 0.687±.002 3.717±.028 3.417±.008 10.84±.004 0.532±.018
TM2T[21] 0.587±.005 3.599±.051 4.591±.019 9.473±.100 3.292±.034
Guoetal.[19] 0.681±.007 3.022±.107 3.488±.028 10.72±.145 2.052±.107
MLD[11] 0.734±.007 0.404±.027 3.204±.027 10.80±.117 2.192±.071
MDM[54] 0.396±.004 0.497±.021 9.191±.022 10.847±.109 1.907±.214
MotionDiffuse[64] 0.739±.004 1.954±.062 2.958±.005 11.10±.143 0.730±.013
T2M-GPT[63] 0.737±.006 0.717±.041 3.053±.026 10.862±.094 1.912±.036
AttT2M[67] 0.751±.006 0.870±.039 3.309±.021 10.96±.123 2.281±.047
M2DM[32] 0.743±.004 0.515±.029 3.015±.017 11.417±.097 3.325±037
M2D2M(w/βt) 0.743±.006 0.404±.022 3.018±.019 10.749±.102 2.063±.066
M2D2M(w/β(t,d)) 0.753±.006 0.378±.023 3.012±.021 10.709±.121 2.061±.067
E Analysis
E.1 Codebook visualization
To examine relationships within the codebook, which inspired our design of
dynamic transition probabilities as detailed in Sec. 4.2, we have visualized the
tokensfromtheMotionVQ-VAE’scodebookinFig.2.Thisvisualizationreveals
that certain tokens are more closely correlated, as evidenced by their clustering
or alignment along implicit lines. Unlike the uniform transition strategy used in8 S. Chi et al.
Fig.3: (Left)PlotofMeanVelocityand(Right)plotofMeanJerkofalltransitions
(40frames)acrossalltestsetsinMulti-MotionGenerationwithN =4.‘GT’represents
concatenated real single motions.
Fig.4: Inference time scaling with action sequence length. Measured with a single
NVIDIA RTX A6000 GPU.
the VQ-Diffusion model, our method starts with a broad, exploratory range of
transitions to encourage diversity by considering token proximity. These results
justify our design of transition probabilities for the discrete diffusion.
E.2 Mean Velocity & Jerk Plot of Generated Multi-Motion
ToassessthesmoothnessofourM2D2Mmodel,weplottedthemeanvelocityof
thegeneratedmulti-motionsequencesacrossalltestsetsformulti-motiongener-
ation, as shown in Fig. 3). In this figure, concatenated real single motions serve
asthegroundtruth(GT).Itisevidentthatthe GTdemonstratesdiscretetran-
sitions between motions, while our M2D2M model (OURS) achieves smoother
transitions with reduced jerk in the transitional phases.
E.3 Running time
We calculate inference time based on the number of actions and visualize the
results in Fig. 4. This illustration demonstrates that our method is practical for
generating multi-motion sequences with reasonable computational cost. We set
each action to have 196 frames; thus, 256 text prompts generate 50,176 frames.
The gradient of the plotted line is nearly linear, as the joint sampling step is
limitedtoT ,allowingmostotherstepstobeexecutedinparallelwithinabatch.
sSupplementary Material of M2D2M 9
F Additional Qualitative Results of Generated
Multi-Motion from M2D2M
FurtherqualitativeresultsshowcasingthecapabilitiesofM2D2Minmulti-motion
generation, akin to the examples in Fig. 1 in the main paper, are provided as
animations (GIFs) in the supplementary materials.