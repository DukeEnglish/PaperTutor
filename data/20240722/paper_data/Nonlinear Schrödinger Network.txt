Nonlinear Schro¨dinger Network
Yiming Zhou1, Callen MacPhee1, Tingyi Zhou1, Bahram Jalali1
1Electrical and Computer Engineering Department, University of
California, Los Angeles, 420 Westwood Plaza, Los Angeles, 90095, CA,
United States.
Abstract
Deepneuralnetworks(DNNs)haveachievedexceptionalperformanceacrossvar-
ious fields by learning complex nonlinear mappings from large-scale datasets.
However, they encounter challenges such as high computational costs and lim-
ited interpretability. To address these issues, hybrid approaches that integrate
physics with AI are gaining interest. This paper introduces a novel physics-
based AI model called the “Nonlinear Schro¨dinger Network”, which treats the
Nonlinear Schr¨odinger Equation (NLSE) as a general-purpose trainable model
for learning complex patterns including nonlinear mappings and memory effects
from data. Existing physics-informed machine learning methods use neural net-
works to approximate the solutions of partial differential equations (PDEs). In
contrast, our approach directly treats the PDE as a trainable model to obtain
general nonlinear mappings that would otherwise require neural networks. As a
physics-inspired approach, it offers a more interpretable and parameter-efficient
alternative to traditional black-box neural networks, achieving comparable or
betteraccuracyintimeseriesclassificationtaskswhilesignificantlyreducingthe
numberofrequiredparameters.Notably,thetrainedNonlinearSchr¨odingerNet-
workisinterpretable,withallparametershavingphysicalmeaningsasproperties
of a virtual physical system that transforms the data to a more separable space.
This interpretability allows for insight into the underlying dynamics of the data
transformation process. Applications to time series forecasting have also been
explored. While our current implementation utilizes the NLSE, the proposed
method of using physics equations as trainable models to learn nonlinear map-
pingsfromdataisnotlimitedtotheNLSEandmaybeextendedtoothermaster
equations of physics.
1
4202
luJ
91
]GL.sc[
1v40541.7042:viXra1 Introduction
Deep neural networks (DNNs) have emerged as a powerful tool for learning complex
nonlinearmappingsfromdata,achievingstate-of-the-artperformanceinvariousfields
including computer vision, natural language processing, and scientific computing. By
leveraging their deep hierarchical structure along with billions or even trillions of
trainable parameters, DNNs have demonstrated remarkable capability to learn com-
plex patterns from vast amounts of data. However, the success of DNNs comes with
challenges, such as the reliance on large-scale datasets, high computational and mem-
ory costs, and limited interpretability. To address these challenges, there is a growing
interest in hybrid approaches that integrate physics with AI to enable better infer-
ence, prediction, vision, control and more [1, 2]. For example, physical principles can
be leveraged as prior knowledge to design neural networks with special architectures
andlossfunctions,enablingmoreefficientmodelingofcomplexphysicalsystems[3,4].
Moreover, physical systems can act as analog computers to perform specialized com-
putations or transform the data in a manner that reduces the burden on digital
processors [5–7].
One example from our own work is the nonlinear Schr¨odinger kernel [7], a hard-
ware acceleration technique that was originally proposed to bridge the latency gap
between ultrafast data acquisition in optical systems and neural network inference on
digitalcomputers.ThenonlinearSchr¨odingerkerneloperatesbymodulatingtheinput
data onto the spectrum of a femtosecond laser pulse and then propagating the pulse
through a nonlinear optical medium, where complex nonlinear interactions governed
by the Nonlinear Schr¨odinger Equation (NLSE) lead to a transformation of the data
that makes it more separable by a simple linear classifier. Notably, operating in the
opticaldomainreduceslatencybyordersofmagnitudecomparedtoutilizingnumerical
kernels like radius basis function (RBF). The benefits are gained without an increase
ofdatadimensionality.Ourfollow-upworkhasdemonstratedthatwhilethenonlinear
optical system cannot be directly tuned, the nonlinear transformation can be effec-
tively engineered by varying the phase of the input data via spectral phase encoding
with an optimization feedback loop [8]. This further improves the performance and
generalization of the nonlinear Schr¨odinger kernel.
While the nonlinear Schr¨odinger kernel technique has shown promising results in
terms of classification accuracy and speed, its physical implementation presents cer-
tain limitations, such as the fixed nature of a given optical medium and the need for
specialized hardware. However, its underlying principles, particularly the use of non-
linear transformation stemming from the Nonlinear Schr¨odinger Equation (NLSE) to
improve data separability, can potentially be adapted to fully digital algorithms. In
thispaper,weproposethe“NonlinearSchr¨odingerNetwork”,anovelphysics-inspired
trainable model that operates entirely in the digital domain. This approach treats
the NLSE as a trainable model to learn the complex patterns from data. By opti-
mizing the coefficients of the NLSE using backpropagation and stochastic gradient
descent (SGD), nonlinear mappings can be learned to transform the data adaptively
for classification tasks. Unlike existing work on using neural networks to approximate
the solution of a partial differential equation (PDE) [3, 4], our proposed approach
2directly learns the coefficients of the PDE itself to obtain general nonlinear map-
pings that would otherwise require neural networks. This physics-inspired approach
provides a more interpretable and parameter-efficient model compared to black-box
neural networks. By leveraging the low-dimensional nature of physical laws, the Non-
linearSchr¨odingerNetworkrepresentscomplexdatatransformationswithsignificantly
fewer parameters while achieving accuracy comparable to traditional neural networks
on nonlinear classification tasks.
2 Related Work
2.1 Nonlinear Schr¨odinger Kernel Computing
Despitetheincreasingspeedofdigitalprocessors,theexecutiontimeofAIalgorithms
are still orders of magnitude slower than the time scales in ultrafast optical imaging,
sensing, and metrology. To address this problem, a new concept in hardware acceler-
ation of AI that exploits femtosecond pulses for both data acquisition and computing
called nonlinear Schr¨odinger kernel has been demonstrated [7]. This method modu-
lates data onto a supercontinuum laser spectrum, which then undergoes a nonlinear
optical transformation analogous to a kernel operation, enhancing data classification
accuracy.Itisshownthatthenonlinearopticalkernelcanimprovethelinearclassifica-
tion results similar to a traditional numerical kernel (such as the radial-basis-function
(RBF))butwithordersofmagnitudelowerlatency.TheinferencelatencyoftheRBF
kernel with a linear classifier is on the order of 10−2 to 10−3 second, while the nonlin-
ear Schr¨odinger kernel achieves a substantially reduced latency on the order of 10−5
second and better classification accuracy. In the original work [7], the performance is
data-dependent due to the limited degrees of freedom and the unsupervised nature of
theopticalkernel.Thefollow-upworkhasdemonstratedthatbymodulatingthespec-
tral phase of the input data, the nonlinear optical interactions within the kernel can
be effectively engineered [8]. Incorporating this phase encoding scheme within a digi-
tal optimization feedback loop allows the optical kernel to be trained in a data-driven
manner, minimizing the classification error of the digital backend.
2.2 Neural Networks as PDE Solvers
An emerging trend in blending physics and AI is the use of neural networks as com-
putationallyefficientsurrogatesforsolvingpartialdifferentialequations(PDEs).This
approach has the potential to significantly accelerate forward simulations and inverse
design used in the optimization of engineered physical systems. For example, neural
networks can compute the transmission spectra of metasurfaces orders of magni-
tude faster than numerically solving the full Maxwell’s equations [9, 10]. Similarly,
neural networks trained on real-world data can model intricate fluid flows for appli-
cationslikeweatherforecastingandaerodynamics,circumventingtheneedtodirectly
solve the Navier-Stokes equations [11]. More ambitious attempts have been made to
develop general neural network architectures that can learn different types of PDEs.
Physics-informed neural network (PINN) [3] is a pioneering example which utilizes a
physics-driven loss derived from the equation itself to solve one instance of the PDE
3Fig.1 SchematicrepresentationoftheNonlinearSchr¨odingerNetwork.Theinputdataxistreatedas
aninputEin(t)propagatingthroughavirtualmedium.ThenetworkconsistsofMcascadedNonlinear
Schr¨odingerLayers,eachcomprisingalineartransformationparameterizedbyαandβ2,followedby
a nonlinear transformation parameterized by γ. These layers transform the input according to the
NonlinearSchr¨odingerEquation(NLSE),resultinginthetransformedoutputEout(t).Theoutputis
then mapped to the predicted label yˆ. The trainable parameters {α,β2,γ} across all the layers are
optimizedthroughbackpropagationandgradientdescenttominimizethelossbetweenthepredicted
andtruelabels.
with a fixed set of parameters and conditions. Fourier Neural Operator (FNO) [4]
takes a step further by learning the nonlinear mapping of an entire family of PDEs.
3 Nonlinear Schr¨odinger Network
The Nonlinear Schr¨odinger Equation (NLSE) is one of the canonical equations of
physics,accountingforthreeprimaryeffects:attenuation,dispersion,andnonlinearity.
These effects, in the limit of constant group delay dispersion (GDD), are represented
by parameters α, β , and γ, respectively, as illustrated in Equation 1. The NLSE
2
has demonstrated significant success in describing systems with complex memory and
nonlinear effects. In this paper, we treat the NLSE as a trainable model by dividing a
virtualmediumitintolayers,whereineachlayerhaslearnablenonlinearanddispersive
properties that can be trained with backpropagation and gradient descent.
∂E(t,z) α β ∂2E(t,z)
=− E(t,z)−i 2 +iγ|E(t,z)|2E(t,z) (1)
∂z 2 2 ∂t2
Figure 1 presents a schematic diagram of the Nonlinear Schr¨odinger Network, a
novelphysics-inspiredtrainablemodel.TheinputdataxistreatedasaninputE (t)
in
that propagates through a virtual medium, wherein the propagation over a short
distance is represented by a Nonlinear Schr¨odinger Layer with two main components:
a linear transformation D(i) parameterized by α(i) and β(i), followed by a nonlinear
2
transformation parameterized by γ(i), as described below.
4(cid:40) (cid:32) (cid:33)(cid:41)
α(i) β(i)
D(i)(E(t))=F−1 F{E(t)}·exp − δz−i 2 ω2δz (2)
2 2
(cid:16) (cid:17)
N(i)(E(t))=E(t)·exp iγ(i)|E(t)|2δz (3)
Thisstructureresemblesasingleblockinacommonneuralnetwork,wherealinear
layer is typically followed by a nonlinear activation function. By cascading several
Nonlinear Schr¨odinger Layers, we construct a “Nonlinear Schr¨odinger Network”, a
model capable of learning nonlinear mappings from the input data to the desired
output.
In Equation 2 and 3, E(t) is a 1D vector with ω being its corresponding frequency
vector. α(i), β(i), and γ(i) are trainable scalars in each layer, and δz is a constant
2
scalar. F and F−1 represent 1D Fourier transform and 1D inverse Fourier transform,
respectively. As the input data traverses the layers, it undergoes a series of transfor-
mations governed by the NLSE, resulting in the transformed output E (t). Such an
out
outputisthenmappedtothepredictedlabelyˆ,whichiscomparedwiththetruelabel
y to compute the loss L. With all the operations being differentiable, backpropaga-
tion and stochastic gradient descent are utilized to update the trainable parameters
{α(i),β(i),γ(i)}M across all the layers.
2 i=1
Remarkably, each Nonlinear Schr¨odinger Layer only introduces three trainable
parameters, yet they can parameterize the nonlinear transformation more efficiently
than conventional neural networks, as demonstrated later in this work. Moreover,
since the trainable parameters are coefficients of a physics equation, the resulting
modelisinterpretablebythephysics.ThisuniquepropertydistinguishestheNonlinear
Schr¨odinger Network from traditional black-box neural networks.
It is worth noting that we modulate the original 1D data onto a super-Gaussian
pulse with zero padding for more accurate spectral representations during the com-
putation [12]. The computation across all the Nonlinear Schr¨odinger Layers doesn’t
change the dimension of data, and the desired output y may have different dimen-
sions compared to E (t), especially in classification tasks. To align with the output
out
dimension, a densely connected linear layer will be added.
4 Results
4.1 Time Series Classification
We demonstrate the performance of the proposed Nonlinear Schr¨odinger Network on
three time series classification datasets from different domains and compare it with
conventional neural networks.
• Ford Engine Dataset [13]: This dataset consists of time series representing mea-
surements of engine noise. The classification problem is to diagnose whether a
specific symptom exists or not in an automotive subsystem, which is a binary
classification task.
5Fig. 2 Model architectures of (a) baseline, (b) Multi-Layer Perceptron (MLP), (c) Convolutional
NeuralNetwork(CNN),and(d)NonlinearSchro¨dingerNetwork.Theevolutionsofdatadimensions
arevisualizedinbluerectangles.The“embedding”layersarehighlightedwiththereddashedbox.
• Starlight Curve Dataset[14]:Eachtimeseriesinthisdatasetrecordsthebright-
nessofacelestialobjectasafunctionoftime.Theclassificationtaskistoassociate
the light curves with three different sources.
• Spoken Digits Dataset [15]: This dataset contains recordings of digits (0-9) at
8kHz from different speakers. The classification task is to classify the recording as
one of the ten digits.
Table1presentstheaccuracyandthenumberoftrainableparametersamongfour
different models across the three datasets. The model architectures are visualized in
Figure2.Thebaselineisalinearclassifierconsistingofasingledenselyconnectedlayer.
Multi-Layer Perceptron (MLP) is a feedforward neural network with several densely
connectedhiddenlayersandnonlinearactivations.The1DConvolutionalNeuralNet-
work(CNN)includesafewconvolutionallayerscascadedwithdenselyconnectedlinear
layers. In our proposed Nonlinear Schr¨odinger Network, we add a max pooling layer
after the cascaded Nonlinear Schr¨odinger Layers to mitigate the dimension increase
causedbythezero-padding,andadenselyconnectedlinearlayertomatchtheoutput
dimension to the number of classes. All models are trained with the Adam optimizer
using the cross-entropy loss. As shown in Figure 2, we divide the model architecture
into two parts: the “embedding” layers (red dashed box) that maps the original data
intoamorelinearlyseparablespace,andalinearclassifierthatmakesthefinaldecision
on the class to which the transformed data belongs. Note that there is no embedding
layer in the baseline model.
The baseline results obtained using only a linear classifier demonstrate poor per-
formance, particularly on linearly non-separable datasets such as Ford Engine and
SpokenDigits.Thishighlightstheinherentlimitationsoflinearclassifiersincapturing
complex patterns and relationships in the data. In contrast, our proposed Nonlinear
6Table 1 Classificationaccuracyandnumberoftrainableparametersofdifferentmodels.
Starlight Ford Engine Spoken Digits
Metrics Param# Acc(%) Param# Acc(%) Param# Acc(%)
Baseline 3,072 84.6(0.6) 500 49.1(0.5) 51,200 15.6(1.1)
MLP 139,264+192 85.8(0.7) 72,385+64 87.7(0.7) 663,754+640 23.1(1.6)
CNN 65,715+96 92.0(1.9) 32,153+32 89.5(1.8) 327,866+320 61.9(8.4)
Ours 18+3,072 92.1(1.5) 18+500 86.2(3.3) 18+51,200 70.1(2.5)
Baseline:asinglelayerlinearclassifier.MLP:Multi-LayerPerceptron.CNN:1DConvolutionalNeural
Network.Ours:NonlinearSchr¨odingerNetwork.Thenumberoftrainableparametersinthe“embed-
ding”layersarehighlightedinred.Allreportedaccuraciesareaveragedover10randomseeds,with
standarddeviationinparentheses.
Schr¨odinger Network significantly improves the classification accuracy by incorporat-
ing several Nonlinear Schr¨odinger Layers as embedding layers preceding the linear
classifier. Each added layer only introduces 3 extra parameters {α(i),β(i),γ(i)} to the
2
modelasdiscussedearlier,efficientlyparameterizingthenonlineartransformationthat
projects the original data into a more linearly separable space, thereby improving
the performance of the same linear classifier used in the baseline model. The results
reported here for the Nonlinear Schr¨odinger Network utilize 6 layers for all three
datasets.Therefore,theNonlinearSchr¨odingerNetworkintroducesonly18additional
trainableparameterscomparedtothebaselinemodelashighlightedintheTable.This
is a notable advantage of the Nonlinear Schr¨odinger Network, as it can achieve high
performance with a minimal increase in model complexity.
We further compare the Nonlinear Schr¨odinger Network with conventional neu-
ral networks, including Multilayer Perceptrons (MLPs) and 1D Convolutional Neural
Networks (CNNs). As shown in Table 1, the Nonlinear Schr¨odinger Network achieves
comparable or better accuracy while maintaining significantly fewer parameters.
Notably,thenumberoftrainableparametersintheembeddinglayersoftheNonlinear
Schr¨odinger Network is orders of magnitude fewer than MLP and CNN, as high-
lighted in red. These results demonstrate the effectiveness of the proposed Nonlinear
Schr¨odinger Layers in capturing the complex nonlinear dynamics of time series data
while maintaining a compact model size. This could be a potential solution to the
MemoryWall,whichisthegapbetweentheperformanceoftheprocessorandmemory
in computer architecture. More importantly, the trained Nonlinear Schr¨odinger Net-
work is an interpretable model with all the parameters having physical meanings as
propertiesofavirtual physicalsystem.This interpretabilityisa significantadvantage
over traditional black-box neural networks, as it allows us to gain deeper insights into
the underlying dynamics of the data and the decision-making process of the model.
4.2 Learning Curve and Parameter Convergence
Figure 3 illustrates the learning curves of the Nonlinear Schr¨odinger Network trained
on the Starlight dataset. As the training progresses, the loss decreases steadily while
the accuracy improves for both the training and validation datasets. This indicates
7Fig. 3 LearningcurvesoftheNonlinearSchr¨odingerNetworktrainedontheStarlightdataset.The
progression of loss (left) and accuracy (right) for both training and validation datasets over the
training epochs are demonstrated here. The curves have been smoothed using a weighted moving
averagetoenhancevisualclarity.
Fig. 4 The convergence of α,β2, and γ in the first four layers of the Nonlinear Schr¨odinger Net-
workduringtrainingontheStarlightdataset.(a),(b),and(c)depicttheconvergenceofα,β2,and
γ, respectively. All parameters start with zero and gradually converge to values that optimize the
network’sclassificationperformance.
successful optimization of the network using backpropagation, demonstrating the
network’s ability to learn and generalize from the training data.
Figure4presentstheconvergenceofthekeyparametersα,β ,andγinthefirstfour
2
layers of the Nonlinear Schr¨odinger Network during training on the Starlight dataset.
Initially, all parameters are set to zero. As the training progresses, these parameters
gradually converge to values that optimize the network’s classification performance.
Notably, they can be interpreted as the properties of a virtual physical system that
transforms the original data to another space for improved classification accuracy.
84.3 Ablation Study on the Impact of Dispersion and
Nonlinearity
In this section, we conduct an ablation study to investigate the effects of the linear
operations (D), as shown in Equation 2, and the nonlinear operations (N), as shown
inEquation3,withintheNonlinearSchr¨odingerNetwork.Thisstudyaimstoprovide
insightsintothenetwork’sunderlyingmechanismsandhowthesecomponentsinteract
to achieve improved performance.
To accomplish this, we first train the network using only the linear operations
(D) by setting γ(i) = 0 and updating only {α(i) and β(i)} across all layers during
2
the training process. Similarly, we then train the network using only the nonlinear
operations (N) by setting {α(i) =0,β(i) =0} and updating only γ(i) across all layers
2
during training. The results of these ablated models are compared with the baseline
(a linear classifier only) and the full Nonlinear Schr¨odinger Network (with both linear
and nonlinear effects enabled) in Table 2.
Table 2 Ablationstudyoflinearandnonlinear
operationsintheNonlinearSchro¨dingerLayer.
Model FordEngineAcc(%) SpokenDigitsAcc(%)
Baseline 49.1(0.5) 15.6(1.1)
N Only 51.7(1.2) 19.9(1.9)
D Only 79.5(6.2) 60.3(5.5)
Full 86.2(3.3) 70.1(2.5)
Baseline:modelwithalinearclassifier.N Only:Nonlinear
Schr¨odingerNetworkwithonlynonlinearoperationsineach
layer. D Only: Nonlinear Schr¨odinger Network with only
linear operations in each layer. Full: Both linear and non-
linearoperationsexist.
The results demonstrate that the model with only linear operations (D) achieves
significant improvement over the baseline. In contrast, the model with only nonlin-
ear operations (N) shows barely any improvement over the baseline. The full model,
incorporating both linear and nonlinear components, yields the best results overall.
These findings can be attributed to the data processing method employed in the
NonlinearSchr¨odingerNetwork.Theinputdataisamplitudemodulatedontothereal
input E (t). After propagation through the network, a complex output E (t) is
in out
obtained,anditsamplitude|E (t)|issubsequentlyfedtothelinearclassifier.When
out
onlythenonlinearoperations(N)arepresent,theself-phasemodulationdoesnotalter
the amplitude, resulting in |E (t)|=E (t). Consequently, the nonlinear operations
out in
alonedonotsignificantlycontributetothenetwork’sperformance.Ontheotherhand,
when only linear operations (D) are present, all transformations are linear, except for
thefinalstepoftakingtheamplitude.Thisallowsthelinearoperationstocapturesome
underlying patterns in the data, leading to improved performance over the baseline.
The true potential of the Nonlinear Schr¨odinger Network is realized when both
linear and nonlinear components are combined. The nonlinear effect of N in layer i
9comesintoplaywhenfollowedbyalinearoperationDinlayeri+1asthedataremains
in the complex domain. This coupled interaction between the nonlinear and linear
components enables the network to capture complicated patterns and relationships in
the data more effectively, resulting in the best overall performance.
5 Discussion
While the Nonlinear Schr¨odinger Network demonstrates impressive performance, it is
essentialtoacknowledgethatitsgeneralizabilitymightbelessthanthatoftraditional
DNNs. The model’s design is heavily influenced by the specific physical principles
encodedintheNLSE.Asaresult,itmaynotbeasflexibleinhandlingdatathatdonot
conformtotheunderlyingphysicalassumptions.However,thistrade-offbetweeninter-
pretability and generalizability is not necessarily a disadvantage. In many practical
applications, a more physically consistent and interpretable model may be preferred,
even if it sacrifices some degree of generalizability.
6 Conclusion
In this paper, we have presented the Nonlinear Schr¨odinger Network, a novel physics-
inspired trainable model that leverages the principles of the Nonlinear Schr¨odinger
Equation(NLSE)tolearncomplexpatternsandnonlinearmappingsfromdata.Unlike
conventional neural networks, which often rely on large numbers of parameters, our
approachdirectlyutilizesthepartialdifferentialequation(PDE)thatgovernsphysical
systems in a trainable and digital form, resulting in an interpretable and parameter-
efficient model that can be optimized with backpropagation and gradient descent.
The experiments on various time series classification datasets have demonstrated
that the Nonlinear Schr¨odinger Network achieves comparable or superior accu-
racy with orders of magnitude fewer parameters than conventional models such as
Multi-Layer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). This
highlights the efficiency of our approach in capturing complex dynamics with min-
imal model complexity. Furthermore, the transformations learned by the Nonlinear
Schr¨odinger Network can be explicitly formulated and interpreted through the lens of
a partial differential equation, with each parameter having a clear physical meaning.
This contrasts sharply with the black-box nature of typical neural networks, offering
insightsintothedatatransformationprocessandmakingthemodelmoretransparent
and understandable. While the current implementation uses the NLSE, the proposed
method of using a physics equation as a trainable model to learn nonlinear mappings
fromdataisnotrestrictedtotheNLSEbutmaybeimplementedwithotherequations.
References
[1] Karniadakis,G.E.,Kevrekidis,I.G.,Lu,L.,Perdikaris,P.,Wang,S.,Yang,L.:Physics-
informed machine learning. Nature Reviews Physics 3(6), 422–440 (2021)
[2] Jalali, B., Zhou, Y., Kadambi, A., Roychowdhury, V.: Physics-ai symbiosis. Machine
Learning: Science and Technology 3(4), 041001 (2022)
10[3] Raissi,M.,Perdikaris,P.,Karniadakis,G.E.:Physics-informedneuralnetworks:Adeep
learningframeworkforsolvingforwardandinverseproblemsinvolvingnonlinearpartial
differential equations. Journal of Computational physics 378, 686–707 (2019)
[4] Li,Z.,Kovachki,N.,Azizzadenesheli,K.,Liu,B.,Bhattacharya,K.,Stuart,A.,Anand-
kumar, A.: Fourier neural operator for parametric partial differential equations. arXiv
preprint arXiv:2010.08895 (2020)
[5] Lin, X., Rivenson, Y., Yardimci, N.T., Veli, M., Luo, Y., Jarrahi, M., Ozcan, A.: All-
optical machine learning using diffractive deep neural networks. Science 361(6406),
1004–1008 (2018)
[6] Mohammadi Estakhri, N., Edwards, B., Engheta, N.: Inverse-designed metastructures
that solve equations. Science 363(6433), 1333–1338 (2019)
[7] Zhou, T., Scalzo, F., Jalali, B.: Nonlinear schro¨dinger kernel for hardware acceleration
of machine learning. Journal of Lightwave Technology 40(5), 1308–1319 (2022)
[8] Zhou, T., Jalali, B.: Low latency computing for time stretch instruments. Journal of
Physics: Photonics 5(4), 045004 (2023)
[9] Liu,Z.,Zhu,D.,Rodrigues,S.P.,Lee,K.-T.,Cai,W.:Generativemodelfortheinverse
design of metasurfaces. Nano letters 18(10), 6570–6576 (2018)
[10] Lim, J., Psaltis, D.: Maxwellnet: Physics-driven deep neural network training based on
maxwell’s equations. Apl Photonics 7(1) (2022)
[11] Kutz,J.N.:Deeplearninginfluiddynamics.JournalofFluidMechanics814,1–4(2017)
[12] Agrawal, G.P.: Nonlinear fiber optics. In: Nonlinear Science at the Dawn of the 21st
Century, pp. 195–211. Springer, ??? (2000)
[13] Dau,H.A.,Bagnall,A.,Kamgar,K.,Yeh,C.-C.M.,Zhu,Y.,Gharghabi,S.,Ratanama-
hatana,C.A.,Keogh,E.:Theucrtimeseriesarchive.IEEE/CAAJournalofAutomatica
Sinica 6(6), 1293–1305 (2019)
[14] Rebbapragada, U., Protopapas, P., Brodley, C.E., Alcock, C.: Finding anomalous peri-
odic time series: An application to catalogs of periodic variable stars. Machine learning
74, 281–313 (2009)
[15] Jackson,Z.,Souza,C.,Flaks,J.,Pan,Y.,Nicolas,H.,Thite,A.:Jakobovski/free-spoken-
digit-dataset: v1. 0.8. Zenodo, August (2018)
11