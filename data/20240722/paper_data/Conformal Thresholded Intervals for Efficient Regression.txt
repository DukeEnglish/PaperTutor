CONFORMAL THRESHOLDED INTERVALS FOR EFFICIENT REGRESSION
Rui Luo and Zhixin Zhou
Abstract. This paper introduces Conformal Thresholded Intervals (CTI), a novel conformal regres-
sionmethodthataimstoproducethesmallestpossiblepredictionsetwithguaranteedcoverage. Unlike
existingmethodsthatrelyonnestedconformalframeworkandfullconditionaldistributionestimation,
CTIestimatestheconditionalprobabilitydensityforanewresponsetofallintoeachinterquantileinter-
valusingoff-the-shelfmulti-outputquantileregression. CTIconstructspredictionsetsbythresholding
theestimatedconditionalinterquantileintervalsbasedontheirlength, whichisinverselyproportional
to the estimated probability density. The threshold is determined using a calibration set to ensure
marginal coverage. Experimental results demonstrate that CTI achieves optimal performance across
variousdatasets.
1. Introduction
Conformal prediction is a powerful framework for constructing prediction intervals with finite-sample
validity guarantees. By leveraging exchangeability of the data, conformal methods can convert the
output of any machine learning algorithm into a set-valued prediction satisfying the required coverage
level, without assumptions on the data distribution. This paper develops a novel conformal prediction
methodforregressionthataimstoproducethesmallestpossiblepredictionsetwithguaranteedcoverage.
Most existing conformal methods for regression either directly predict the lower and upper endpoints
of the interval using quantile regression models [30, 17, 32, 11] or first estimate the full conditional
distribution of the response and then invert it to obtain prediction sets [13, 4]. While these approaches
perform well in many situations, they may produce sub-optimal intervals if the conditional distribution
isskewed. Conformalizedquantileregressiontypicallyyieldsequal-tailedintervals,buttheshortestvalid
interval may be unbalanced. On the other hand, density-based methods can adapt to skewness but
typically involve many tuning parameters and more difficult interpretation, which can be complex for
practitioners.
Toaddresstheselimitations,weproposeconformalthresholdedintervals(CTI),aconformalinference
method that seeks the smallest possible prediction set. Instead of relying on an estimate of the full
conditional distribution, we use off-the-shelf multi-output quantile regression and construct prediction
set by thresholding the estimated conditional interquantile intervals. Compared with conformal his-
togram regression (CHR) [33], which first partitions the response space into bins, CTI directly trains a
multi-output quantile regression model that uses equiprobable quantiles. This allows us to estimate the
conditional probability density for a new response to fall into each interquantile interval, without the
need for explicitly binning the response space.
For each sample in the calibration set, we obtain the interquantile interval that its response falls
into and find the corresponding probability density estimate. We compute the non-conformity scores
based on these estimates. Intuitively, the non-conformity score is higher for a sample that falls into
a long interquantile interval and lower for a sample that falls into a short interquantile interval. By
adopting a similar thresholding idea as in conformal classification [31, 23], we threshold the intervals
according to their length, the inverse of which corresponds to the probability density estimate. At test
time, the threshold, i.e., the quantile for non-conformity scores, is used in constructing prediction sets
for test samples. Specifically, the interquantile intervals are sorted in ascending order of length, and the
first ones shorter than or equal to the threshold are kept. We show that the prediction sets generated
fromthresholdinginterquantileintervalsguaranteemarginalcoverageandcanachievedesiredconditional
coverageaswellasthesmallestexpectedpredictionintervallengthifthemulti-outputquantileregression
model produces true conditional probability density estimates.
Therestofthispaperisorganizedasfollows. WediscussrelatedworkinSection2. Section3describes
the proposed CTI method in detail. Section 4 presents a theoretical analysis of CTI. Section 5 provides
numerical experiments comparing CTI to existing conformal regression methods on both simulated and
real data. Finally, Section 6 concludes with a discussion of the main results and future directions.
1
4202
luJ
91
]GL.sc[
1v59441.7042:viXra2 CONFORMAL THRESHOLDED INTERVALS
2. Related Work
Quantileregression,introducedby[18],estimatestheτ-thconditionalquantilefunctionbyminimizing
the check function loss:
n
(cid:88)
(1) min ρ (y −q (x )),
τ i τ i
fτ
i=1
where
®
τr if r >0
(2) ρ (r)=
τ
−(1−τ)r otherwise
is the check function representing the absolute loss.
Quantile regression has been widely used to construct prediction intervals by estimating conditional
quantile functions at specific levels, such as the 5% and 95% levels for 90% nominal coverage [12, 36, 25,
35, 34]. This approach adapts to local variability, even for highly heteroscedastic data.
[5] showed that simultaneous estimation of multiple quantiles is asymptotically more efficient than
separate estimation of individual regression quantiles or ignoring within-subject dependency. However,
this approach does not guarantee non-crossing quantiles, which can affect the validity of the predictions
and introduce critical issues in certain scenarios. To address this limitation, research on non-crossing
multiplequantileregressionhasgainedattentioninrecentyears,withseveralmethodsproposedtoensure
non-crossing quantile estimates, including stepwise approaches [21], non-parametric techniques [3], and
deep learning-based models [27, 2].
However, the validity of the produced intervals is only guaranteed for specific models under certain
regularityandasymptoticconditions[34,35,25]. Manyrelatedmethodsforconstructingvalidprediction
intervalscanbeencompassedwithinthenestedconformalpredictionframework,whereanestedsequence
ofpredictionsetsisgeneratedbythresholdingnonconformityscoresderivedfromvariousapproaches,such
as residual-based methods [29, 1, 19], quantile regression [30, 17, 32, 4], density estimation [14, 33, 15],
andtheircombinationswithensemblemethods[11]andlocalizedmethods[28,6,22]. However,asnoted
by [20], the optimal conditionally-valid prediction regions are level sets of conditional densities, which
need not be intervals, suggesting that constructing possibly non-convex prediction sets might lead to
more efficient conformal predictors.
Our proposed method for constructing non-convex prediction sets is related to the work of [15], who
introduceaprofiledistancetomeasurethesimilaritybetweenfeaturesandconstructpredictionsetsbased
on neighboring samples. In contrast, our method directly estimates the conditional probability density
for a new response to fall into each interquantile interval based on a multi-output quantile regression
model. By thresholding the interquantile intervals based on their length, which is inversely proportional
to the estimated probability density, we can construct efficient prediction sets that adapt to the local
densityofthedata. Thisapproachallowsustogeneratepredictionsetsthatarenotrestrictedtointervals
and can potentially achieve better coverage and efficiency compared to interval-based methods.
Anotherrelatedapproach[10]convertsregressiontoaclassificationproblemandemploysaconditional
distribution with a smoothness-enforcing penalty. This method is orthogonal to our approach and can
be potentially combined with our multi-output quantile regression framework to further improve the
efficiency of the constructed prediction sets.
3. Proposed Method
3.1. Problem Setup. We consider a general regression problem with a dataset {(x ,y )}n , where
i i i=1
x ∈ X ⊆ Rd is the input feature vector and y ∈ Y ⊆ R is the corresponding continuous response
i i
variable. The dataset is split into three parts: a training set D , a calibration set D , and a test set
train cal
D . The corresponding indices set are denoted by I ,I and I respectively. We assume that
test train cal test
the examples in these sets are exchangeable.
Our goal is to construct a conformal predictor that outputs a prediction set C(X) ⊆ Y for each test
input X such that the true response value Y is included in C(X) with a probability of at least 1−α,
where α∈(0,1) is a user-specified significance level. Formally, we aim to achieve the following marginal
coverage guarantee:
P(Y ∈C(X))≥1−α
for joint distribution for X and Y. The probability in this statement is marginal, being taken over all
the samples in D and D .
cal testCONFORMAL THRESHOLDED INTERVALS 3
Whileachievingvalidcoverage,weaimtoconstructpredictionsetsthatareasinformativeaspossible.
Specifically, we seek to minimize the expected length of the prediction sets:
(cid:90)
E[µ(C(X))]= µ(C(x))dP(x),
X
where µ denotes the Lebesgue measure on R and P(x) is the marginal distribution of the input features.
3.2. Our Method. First, we apply quantile regression on the training set D to predict the τ-th
train
quantile of the conditional distribution Y|X = x for every x ∈ X, where τ takes values from 0 to 1 in
increments of 1/K. The estimated quantile for τ =k/K is denoted by
(3) q (x) for k =0,1,...,K.
(cid:98)k
We then define the interquantile intervals as
(4) I (x)=(q (x), q (x)] for k = 1,...,K.
k (cid:98)k−1 (cid:98)k
Assuming the quantile regression provides sufficiently accurate estimations, each interval should have
approximately the same probability, 1/K, of covering the true label Y. To minimize the size of the
prediction set, it is more efficient to include intervals with smaller sizes. This strategy leads us to define
the confidence set as:
(cid:91)
(5) C(x)= {I (x):µ(I (x))≤t,k =1,...,K},
k k
wheretisathresholddeterminedinamarginalsense,meaningitisindependentofx. Todeterminet,we
utilizethecalibrationset. Wewantttosatisfytheconditionthaty ∈C(x )foratleast⌈(1+|I |)(1−α)⌉
i i cal
instances in the calibration set, where i ∈ I . We define t as the smallest value that satisfies this
cal
condition:
(6) t=⌈(1+|I |)(1−α)⌉-th smallest value of µ(I (x )) for i∈I ,
cal k(yi) i cal
wherek(y)istheindexthatoftheintervalthaty belongs, i.e., y ∈I (x). Bypluggingtbackinto(5),
k(y)
we obtain the prediction set for every x∈X.
Algorithm 1 Conformal thresholding intervals
1: Input: labeled data {(x i,y i)} i∈I, unlabel test data {x i} Itest, a data split ratio, black-box learning
algorithm B, level α∈(0,1), number of interquantile intervals K
2: Randomly split the indices I into I train and I cal.
3: Train B on samples in I train, and obtain quantile estimation functions q (cid:98)k for k =0,1,...,K.
4: For every i∈I cal∪I test, evaluate q (cid:98)k(x i) for k =0,1,...,K.
5: Foreveryi∈I cal∪I test,definetheinterquantileintervalsI k(x i)=(q (cid:98)k−1(x i),q (cid:98)k(x i)]fork =1,...,K.
6: t←⌈(1+|I cal|)(1−α (cid:83))⌉-th smallest value of µ(I k(yi)(x i)) for i∈I cal.
7: For i∈I test,C(x i)= {I k(x i):µ(I k(x i))≤t,k =1,...,K}.
8: Output: C(x i) for i∈I test.
Remark 3.3. The procedure we propose is generic and can be applied to any multi-output quantile
regression method. In Section 5, we present results based on quantile regression methods using both a
neural network and a random forest. In some cases, quantile regression algorithms may not support τ
values of exactly 0 or 1. To overcome this limitation, we can replace these extreme values with values
very close to 0 or 1, respectively. For example, we might use τ =0.001 instead of τ =0, and τ =0.999
instead of τ =1. For the sake of notational simplicity, we will continue to refer to these values as 0 and
1 throughout the discussion, unless otherwise specified.
Remark 3.4. Our approach can also be considered in terms of conformity scores. Using the definition
of the prediction set in equation (5), the value of label y is contained within a small interval. More
formally, let k(y) be the index such that y ∈I (x). We can then define the conformity score function
k(y)
for our proposed method as:
s(x,y)=µ(I (x)).
k(y)
This conformity score function assigns a score to each label y based on the size of the interval I (x)
k(y)
in which it falls. A smaller score indicates that the label y is more likely to be the true label for the input
x. In the context of conformal prediction, labels with smaller conformity scores are given priority for
inclusion in the prediction set.4 CONFORMAL THRESHOLDED INTERVALS
4. Theoretical Analysis
Inthecontextoftheentirepopulation,CTIsharesaverysimilarformulationwiththeLeastAmbigu-
ous Set method used for classification, as described in [31]. If we assume that our quantile regression
model is sufficiently accurate, CTI has the potential to achieve the optimal size for prediction sets when
considering the marginal distribution. To understand this better, let’s first take a look at the Neyman-
Pearson Lemma:
Lemma 4.1 (Neyman-Pearson). Let f and g be two nonnegative measurable functions. Then the opti-
mizer of the problem
(cid:90) (cid:90)
min g subject to f ≥1−α,
C C C
is given by C ={x:f(x)/g(x)≥t′} if there exists t such that (cid:82) f =1−α.
f/g≥t′
To formalize the problem of minimizing the expected length of the prediction set subject to 1−α
coverage, we can write the problem as:
(cid:90) (cid:90) (cid:90) (cid:90)
min 1dµ(y)dP(x) subject to f(y|x)dµ(y)dP(x)≥1−α.
C(x) X C(x) X C(x)
The Neyman-Pearson Lemma implies that the optimal solution for C(x) has the form:
(7) C(x)={y :f(y|x)≥t′}
for some suitable threshold t′. Indeed, this threshold can be defined as
(8) t′ =inf{t∈R:P(f(Y|X)≥t)≥1−α}.
which will be shown in Lemma A.2. Our algorithm is an empirical construction of such an interval.
Suppose the quantile regression approximates q well. In that case, we have:
(cid:98)τ
(cid:90)
f(y|x)dµ(y)=P(Y ∈I (X))≈1/K.
k
y∈Ik(x)
AsKapproachesinfinity,µ(I (x))tendsto0. Iff(y|x)issufficientlysmooth,thenf(y|x)≈1/(Kµ(I (x))).
k k
The threshold on the length of intervals µ(I (x)) ≤ t in equation (5) approximately implies f(y|x) ≥
k
1/(Kt), which is optimal in the sense of the Neyman-Pearson Lemma. This means that our algorithm,
whichconstructspredictionsetsbasedonthethresholdonintervallengths,isanempiricalapproximation
of the optimal solution prescribed by the Neyman-Pearson Lemma.
The demonstration of the coverage probability for CTI follows the same reasoning as the traditional
proof used in the general conformal prediction framework.
Theorem 4.2 (Coverage Probability). Suppose the samples in {(X ,Y )} } are exchangeable,
i i i∈Ical∪Itest
then for (X,Y) in the test set, the coverage probability
P(Y ∈C(X))≥1−α.
Proof. The score function s(X ,Y ) for i∈I ∪I are also exchangeable. For any (X,Y) in the test
i i cal test
set, the rank of s(X,Y) is smaller than t defined in (6) with probability ⌈(1+|Ical|)(1−α)⌉ ≥1−α. □
1+|Ical|
The upcoming proposition will demonstrate that the threshold t, which is defined in equation (6) for
the length of interquantile intervals, results in a suitable threshold for the distribution ofthe conditional
density f(Y|X), as shown in equation (8).
Proposition 1 (Threshold Consistency). Suppose the interquantile intervals satisfy
(cid:12) (cid:12)
(cid:12) (cid:12)
(9) sup(cid:12) (cid:12) (cid:12)F f(Y|X)(t′)− I1 (cid:88) 1(cid:8) µ(I k(yi)(x i))> K1 t′(cid:9)(cid:12) (cid:12) (cid:12)≤ϵ
t′ (cid:12) cal i∈Ical (cid:12)
Let t be as defined in (6) and let 1−α′ =⌈(1+|I |)(1−α)⌉/|I |, then F (1/(Kt))≥α′−ϵ.
cal cal f(Y|X)
Proof. Let t′ =1/(Kt) in the assumption (9), then by the definition of t,
(cid:88) 1(cid:8) µ(I (x ))> 1 (cid:9) =|I |−⌈(1+|I |)(1−α)⌉=:|I |α′.
k(yi) i Kt′ cal cal cal
i∈Ical
Under the assumption, F (t′)≥α′−ϵ. □
f(Y|X)CONFORMAL THRESHOLDED INTERVALS 5
This theorem establishes that if the value of ϵ is sufficiently small and the size of the calibration set is
large enough, then the value t′ = 1/(Kt), where t is the threshold defined in equation (6), will be close
to the α-th quantile of the distribution f(Y|X).
Lastly, the theorem presented below demonstrates that the size of the prediction set obtained using
CTI will not significantly exceed the size of the theoretically optimal prediction set, which is defined in
equation (7).
Theorem 4.3 (Prediction Set). Suppose for x∈X,
(cid:90) 1−δ(x)
(10) P(Y ∈I (x)|X =x)= f(y|x)dy ≥ ,
k K
y∈Ik(x)
and suppose f(y|x) has a Lipschitz constant L(x), then
ß 1−δ (x) L(x)t™
C (x)⊆ y :f(y|x)≥ k − .
1−α Kt 2
Proof. In the construction of C (x), we consider the intervals with lengthµ(I (x)) ≤ t. Given the
1−α k
assumption (10) in the theorem, and combine it with Lemma A.1, we have
1−δ (x) L(x)t
minf(y|x)≥ k − =:t′.
y∈Ik Kt 2
By the construction of C (x), we have the conclusion of the theorem. □
1−α
The previous proposition demonstrates that the value 1/(Kt), where t is the threshold defined in
equation (6), is slightly smaller than the α-th quantile of f(Y|X), which represents the theoretically
optimal threshold as shown in equation (8). This theorem further illustrates that the prediction set
obtained using CTI will include values that are even more conservative. However, we understand that
1/Kt, where t is the threshold defined in equation (6), serves as a relatively stable estimate of the
quantile,whichisasymptoticallyequivalenttoaconstantvalue. Asthenumberofinterquantileintervals
K approaches infinity, the threshold t converges to 0. If we additionally assume that our quantile
regressionmodelisaccurate, meaningthattheerrortermδ (x)issmall, thenthepredictionsetC (x)
k 1−α
obtained using CTI will be close to the theoretically optimal prediction set defined in equation (5).
5. Experiments
This section presents experiments that evaluate the performance of prediction sets generated by Split
Conformal[19],CQR[30],CHR[33],andourproposedCTI,onvarioussimulatedandreal-worlddatasets
as in [30, 17, 32]. For a detailed introduction of the datasets, please refer to [32]. We have released our
code on GitHub, which is built upon TorchCP [37], CHR [33], and QRF [16].
Following similar procedures, we standardize the response variables Y for all datasets. We randomly
hold out 20% of the samples for testing and divide the remaining observations into two disjoint sets,
I and I , for training the multi-output quantile regression model and computing the non-conformity
1 2
scores, respectively. We use 70% of the samples for training, which achieves a reasonable compromise
for all datasets [32]. We repeat all experiments 10 times, starting from the initial data splitting. We
evaluate the performance of the generated prediction intervals in terms of coverage and efficiency.
AsshowninTable1,CTIachievesoptimalperformanceacrossalldatasetsexceptforthestardataset,
whichhasarelativelysmallsamplesize(n=2161,d=39). Thelimitednumberofsamplesinthisdataset
may hinder the performance of the multi-output quantile regression model, as it requires sufficient data
to accurately capture the underlying relationships between the features and the response variable. We
also notice a similar trend in the relative performance comparison of CTI based on random forest and
CTIbasedonneuralnetwork,aswellasCHRbasedonrandomforestandCHRbasedonneuralnetwork.
This suggests that the efficiency of the conformal prediction sets depends on the quality of the multi-
output quantile regression. The choice of the underlying model plays a crucial role in the performance
of the conformal prediction methods.
To further analyze the superior performance of CTI, we compare the interval lengths between the
response intervals (i.e., intervals containing the actual responses) and all intervals generated by the
multi-output quantile regression model for the MEPS-19 dataset. Figure 1 presents the distribution of
interval lengths for both the response intervals (blue histogram) and all intervals (red histogram) on the
test set. The difference in means between the two distributions is -0.0415, indicating that the response
intervals have a smaller average length compared to all intervals.
This finding suggests that the multi-output quantile regression model effectively captures the under-
lying structure of the data, producing tighter intervals around the true responses. By estimating the6 CONFORMAL THRESHOLDED INTERVALS
conditionalquantilefunctionssimultaneously,themodelcanleveragethedependenciesbetweendifferent
quantilelevelsandgeneratemorepreciseintervals. Themulti-outputapproachallowsthemodeltoshare
information across different quantile levels, leading to improved interval estimates.
Consequently, CTI can construct more efficient prediction sets by thresholding the intervals based on
theirestimatedprobabilitydensity. Byselectingintervalswithhigherprobabilitydensity,CTIfocuseson
regions of the feature space where the true response is more likely to occur. This adaptive thresholding
approach leads to improved coverage and shorter interval lengths compared to other methods that rely
on fixed thresholds or do not consider the local density of the data.
Dataset Metric CTI(RF) CTI(NN) CHR(RF) CHR(NN) CQR Split
Coverage 0.898(0.007) 0.899(0.007) 0.898(0.010) 0.900(0.006) 0.906(0.009) 0.899(0.008)
bike
Size 1.032(0.029) 0.720 (0.028) 1.124(0.028) 0.758(0.047) 1.599(0.054) 1.345(0.053)
Coverage 0.910(0.002) 0.900(0.004) 0.902(0.004) 0.902(0.003) 0.940(0.009) 0.910(0.006)
blog
Size 0.709 (0.031) 1.003(0.024) 1.567(0.074) 1.737(0.154) 3.259(0.327) 1.453(0.113)
Coverage 0.909(0.018) 0.908(0.021) 0.903(0.015) 0.905(0.021) 0.889(0.024) 0.902(0.024)
community
Size 1.611(0.088) 1.275 (0.095) 1.637(0.096) 1.588(0.100) 1.680(0.078) 2.132(0.188)
Coverage 0.903(0.018) 0.910(0.017) 0.907(0.021) 0.897(0.018) 0.901(0.016) 0.910(0.024)
star
Size 0.186(0.006) 0.197(0.009) 0.182(0.005) 0.204(0.009) 0.181 (0.005) 0.181(0.008)
Coverage 0.900(0.005) 0.900(0.006) 0.899(0.005) 0.895(0.007) 0.898(0.006) 0.897(0.005)
homes
Size 0.640(0.011) 0.515 (0.008) 0.682(0.012) 0.535(0.010) 0.851(0.052) 0.825(0.072)
Coverage 0.909(0.003) 0.899(0.003) 0.901(0.004) 0.900(0.004) 0.945(0.009) 0.903(0.002)
facebook1
Size 0.766 (0.033) 0.780(0.023) 1.595(0.088) 1.379(0.086) 2.627(0.329) 2.252(0.208)
Coverage 0.911(0.002) 0.900(0.001) 0.899(0.002) 0.899(0.002) 0.943(0.006) 0.904(0.002)
facebook2
Size 0.735 (0.017) 0.773(0.023) 1.533(0.053) 1.382(0.057) 2.661(0.272) 2.100(0.108)
Coverage 0.900(0.004) 0.902(0.004) 0.899(0.005) 0.900(0.004) 0.900(0.003) 0.901(0.004)
bio
Size 1.295 (0.018) 1.474(0.030) 1.450(0.023) 1.576(0.012) 2.005(0.016) 1.961(0.039)
Coverage 0.908(0.024) 0.900(0.031) 0.899(0.022) 0.900(0.023) 0.901(0.024) 0.896(0.021)
concrete
Size 0.967(0.035) 0.473 (0.050) 0.933(0.041) 0.505(0.144) 0.692(0.051) 0.619(0.029)
Coverage 0.907(0.008) 0.902(0.007) 0.901(0.007) 0.902(0.004) 0.932(0.007) 0.902(0.010)
meps19
Size 1.760 (0.087) 1.795(0.061) 2.388(0.195) 2.602(0.128) 2.923(0.170) 3.092(0.377)
Coverage 0.904(0.004) 0.901(0.007) 0.901(0.007) 0.901(0.006) 0.927(0.009) 0.902(0.005)
meps20
Size 1.883 (0.067) 1.921(0.091) 2.376(0.105) 2.594(0.140) 2.925(0.193) 3.154(0.217)
Coverage 0.906(0.005) 0.900(0.008) 0.900(0.006) 0.898(0.004) 0.928(0.007) 0.905(0.004)
meps21
Size 1.832 (0.089) 1.866(0.076) 2.510(0.167) 2.609(0.145) 2.971(0.179) 3.046(0.199)
Table 1. Coverage and Size of Different Methods
6. Discussion and Future Work
Ourproposedconformalpredictionmethodforregressiontaskshasdemonstratedpromisingresultsin
achieving high performance. However, there are several areas where further research and improvements
can be made.
Computing prediction intervals as opposed to prediction sets. There might be concerns re-
garding constructing disjoint prediction sets instead of prediction intervals, as it may be more desirable
to have a single contiguous interval for interpretability and practicality. To address this issue, we can
adopt ideas similar to those in [7], which deals with the problem of non-contiguous prediction sets for
ordinal classification tasks. The authors propose training the predictor with a unimodal posterior over
classes, allowing the construction of contiguous prediction sets. In the context of regression, we can
imposeunimodalityconstraintsontheestimatedconditionaldensityorquantilefunctionstoensurethat
the resulting prediction sets are contiguous intervals. This approach would provide a more interpretable
and user-friendly output while maintaining the advantages of our proposed method.
Weighted aggregation of score functions. In this work, we implement the concept of weighted
aggregation of score functions for conformal classification [24] by applying it to multi-output quantile
regressionmodels. Specifically,wecombinetheresultsfromaneuralnetworkmodelandarandomforest
model. To ensure the combined results form a valid non-conformity score and accurately reflect predic-
tions from both models, we use a harmonic mean for aggregation. This approach allows us to leverage
the strengths of different models and improve the overall performance of the conformal prediction sets.CONFORMAL THRESHOLDED INTERVALS 7
Difference in Mean: -0.0459
400 18000
Response Intervals
All Intervals 16000
350
14000
300
12000
250
10000
200
8000
150
6000
100
4000
50 2000
0 0
0.0 0.1 0.2 0.3 0.4 0.5
Interval Length
Figure 1. Comparison of interval lengths between response intervals and all intervals
for the MEPS-19 dataset. The blue histogram represents the distribution of interval
lengthsfortheintervalsthatcontaintheactualresponses,whiletheredhistogramshows
thedistributionofallintervalsgeneratedbythemulti-outputquantileregressionmodel.
Bothallresultsonthetestset. Thedifferenceinmeansbetweenthetwodistributionsis
-0.0415, indicating that the response intervals have a smaller average length compared
to all intervals.
Future research can explore alternative aggregation methods and investigate the optimal combination of
models for different datasets and problem settings.
Extension to other regression settings. Our current work focuses on the standard regression set-
ting with continuous response variables. However, there are various other regression settings, such as
multivariate regression or functional regression [8, 26, 9], where conformal prediction methods could be
beneficial. Adapting our multi-output quantile regression approach to these settings would require ad-
dressing the specific challenges and characteristics of each problem. For example, in multivariate regres-
sion, the construction of prediction sets would involve estimating joint quantile functions and handling
the dependencies between multiple response variables. In functional regression, the response variable is
a function rather than a scalar, requiring the development of appropriate non-conformity measures and
prediction set construction techniques.
7. Conclusion
Conformal Thresholded Intervals (CTI) is a novel conformal prediction method for regression that
constructs the smallest possible prediction set with guaranteed coverage. By leveraging multi-output
quantile regression and thresholding the estimated conditional interquantile intervals based on their
length, CTI adapts to the local density of the data and generates non-convex prediction sets. Exper-
imental results demonstrate the effectiveness of CTI in achieving optimal performance across various
datasets. However,theperformanceofCTIdependsonthequalityoftheunderlyingmulti-outputquan-
tile regression model, emphasizing the importance of model selection in conformal prediction methods.
Futureresearchdirectionsincludeoptimizingmodelselectionaswellasmodelaggregationandextending
CTI to other types of regression problems.
References
[1] Balasubramanian, V., Ho, S.-S., and Vovk, V. Conformal prediction for reliable machine learning: theory,
adaptations and applications.Newnes,2014.
[2] Brando,A.,Center,B.S.,Rodríguez-Serrano,J.,Vitrià,J.,etal.Deepnon-crossingquantilesthroughthe
partialderivative.InInternationalConferenceonArtificialIntelligenceandStatistics (2022),PMLR,pp.7902–7914.
[3] Cannon,A.J.Non-crossingnonlinearregressionquantilesbymonotonecompositequantileregressionneuralnetwork,
withapplicationtorainfallextremes.Stochasticenvironmentalresearchandriskassessment32,11(2018),3207–3225.
[4] Chernozhukov, V., Wüthrich, K., and Zhu, Y.Distributionalconformalprediction.ProceedingsoftheNational
Academy of Sciences 118,48(2021),e2107794118.
[5] Cho, H., Kim, S., and Kim, M.-O. Multiple quantile regression analysis of longitudinal data: Heteroscedasticity
andefficientestimation.Journal of Multivariate Analysis 155 (2017),334–343.
[6] Colombo, N. On training locally adaptive cp. In Conformal and Probabilistic Prediction with Applications (2023),
PMLR,pp.384–398.
)slavretnI
esnopseR(
ycneuqerF
)slavretnI
llA(
ycneuqerF8 CONFORMAL THRESHOLDED INTERVALS
[7] DEY, P., Merugu, S., and Kaveri, S. R. Conformal prediction sets for ordinal classification. In Thirty-seventh
Conference on Neural Information Processing Systems (2023).
[8] Diquigiovanni, J., Fontana, M., and Vantini, S. Conformal prediction bands for multivariate functional data.
Journal of Multivariate Analysis 189 (2022),104879.
[9] Feldman, S., Bates, S., and Romano, Y. Calibrated multiple-output quantile regression with representation
learning.Journal of Machine Learning Research 24,24(2023),1–48.
[10] Guha, E. K., Natarajan, S., Möllenhoff, T., Khan, M. E., and Ndiaye, E. Conformal prediction via
regression-as-classification.InThe Twelfth International Conference on Learning Representations (2024).
[11] Gupta, C., Kuchibhotla, A. K., and Ramdas, A.Nestedconformalpredictionandquantileout-of-bagensemble
methods.Pattern Recognition 127 (2022),108496.
[12] Hunter, D. R., and Lange, K.Quantileregressionviaanmmalgorithm.JournalofComputationalandGraphical
Statistics 9,1(2000),60–77.
[13] Izbicki, R., Shimizu, G., and Stern, R.Flexibledistribution-freeconditionalpredictivebandsusingdensityesti-
mators.InInternational Conference on Artificial Intelligence and Statistics (2020),PMLR,pp.3068–3077.
[14] Izbicki,R.,Shimizu,G.,andStern,R.Flexibledistribution-freeconditionalpredictivebandsusingdensityestima-
tors.InProceedingsoftheTwentyThirdInternationalConferenceonArtificialIntelligenceandStatistics(26–28Aug
2020), S. Chiappa and R. Calandra, Eds., vol. 108 of Proceedings of Machine Learning Research, PMLR, pp. 3068–
3077.
[15] Izbicki, R., Shimizu, G., and Stern, R. B.Cd-splitandhpd-split: Efficientconformalregionsinhighdimensions.
Journal of Machine Learning Research 23,87(2022),1–32.
[16] Johnson, R. A. quantile-forest: A python package for quantile regression forests. Journal of Open Source Software
9,93(2024),5976.
[17] Kivaranovic,D.,Johnson,K.D.,andLeeb,H.Adaptive,distribution-freepredictionintervalsfordeepnetworks.
InInternational Conference on Artificial Intelligence and Statistics (2020),PMLR,pp.4346–4356.
[18] Koenker, R.Quantile regression,vol.38.Cambridgeuniversitypress,2005.
[19] Lei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., and Wasserman, L.Distribution-freepredictiveinference
forregression.Journal of the American Statistical Association 113,523(2018),1094–1111.
[20] Lei, J., Robins, J., and Wasserman, L. Distribution-free prediction sets. Journal of the American Statistical
Association 108,501(2013),278–287.
[21] Liu, Y., and Wu, Y.Stepwisemultiplequantileregressionestimationusingnon-crossingconstraints.Statistics and
its Interface 2,3(2009),299–310.
[22] Luo, R., and Colombo, N. Conformal load prediction with transductive graph autoencoders. arXiv preprint
arXiv:2406.08281(2024).
[23] Luo, R., and Zhou, Z. Trustworthy classification through rank-based conformal prediction sets. arXiv preprint
arXiv:2407.04407(2024).
[24] Luo, R., and Zhou, Z.Weightedaggregationofconformityscoresforclassification,2024.
[25] Meinshausen, N.Quantileregressionforests.Journal of Machine Learning Research 7 (2006),983–999.
[26] Messoudi, S., Destercke, S., and Rousseau, S. Ellipsoidal conformal inference for multi-target regression. In
Conformal and Probabilistic Prediction with Applications (2022),PMLR,pp.294–306.
[27] Moon, S. J., Jeon, J.-J., Lee, J. S. H., and Kim, Y.Learningmultiplequantileswithneuralnetworks.Journal
of Computational and Graphical Statistics 30,4(2021),1238–1248.
[28] Papadopoulos, H., Gammerman, A., and Vovk, V.Normalizednonconformitymeasuresforregressionconformal
prediction.InProceedings of the IASTED International Conference on Artificial Intelligence and Applications (AIA
2008) (2008),pp.64–69.
[29] Papadopoulos,H.,Proedrou,K.,Vovk,V.,andGammerman,A.Inductiveconfidencemachinesforregression.
InMachineLearning: ECML2002: 13thEuropeanConferenceonMachineLearningHelsinki,Finland,August19–23,
2002 Proceedings 13 (2002),Springer,pp.345–356.
[30] Romano,Y.,Patterson,E.,andCandès,E.Conformalizedquantileregression.InAdvancesinNeuralInformation
Processing Systems (2019),pp.3538–3548.
[31] Sadinle,M.,Lei,J.,andWasserman,L.Leastambiguousset-valuedclassifierswithboundederrorlevels.Journal
of the American Statistical Association 114,525(2019),223–234.
[32] Sesia, M., and Candès, E.Acomparisonofsomeconformalquantileregressionmethods.Stat 9,1(2020).
[33] Sesia, M., and Romano, Y. Conformal prediction using conditional histograms. Advances in Neural Information
Processing Systems 34 (2021),6304–6315.
[34] Steinwart, I., and Christmann, A.Estimatingconditionalquantileswiththehelpofthepinballloss.
[35] Takeuchi, I., Le, Q. V., Sears, T. D., Smola, A. J., and Williams, C. Nonparametric quantile estimation.
Journal of machine learning research 7,7(2006).
[36] Taylor, J. W. A quantile regression neural network approach to estimating the conditional density of multiperiod
returns.Journal of Forecasting 19,4(2000),299–311.
[37] Wei, H., and Huang, J. Torchcp: A library for conformal prediction based on pytorch. arXiv preprint
arXiv:2402.12683(2024).
Appendix A. Ancillary Lemmas
Lemma A.1. For a Lipschitz function f :R→R with Lipschitz constant L, if (cid:82)b f(x)dµ(x)=c, then
a
f(x)∈[ c − L(b−a), c + L(b−a)] for all x∈[a,b].
b−a 2 b−a 2
Proof. Suppose f(x′)=d for some x′ ∈[a,b]. Then, by the Lipschitz condition, we have:
|f(x)−f(x′)|≤L|x−x′| =⇒ f(x)≤d+L|x−x′|CONFORMAL THRESHOLDED INTERVALS 9
for all x∈[a,b]. Integrating both sides over [a,b], we get:
(cid:90) b (cid:90) b
f(x)dµ(x)≤ (d+L|x−x′|)dµ(x)
a a
(cid:90) b
=d(b−a)+L |x−x′|dµ(x)
a
1
≤d(b−a)+ L(b−a)2,
2
wherethelastinequalityfollowsfromthefactthat(cid:82)b |x−x′|dµ(x)≤ 1(b−a)2. Since(cid:82)b f(x)dµ(x)=c,
a 2 a
we have:
1
c≤d(b−a)+ L(b−a)2
2
1 Å 1 ã
=⇒ d≥ c− L(b−a)2
b−a 2
c L(b−a)
= − .
b−a 2
Since x′ was arbitrary, this lower bound holds for all x ∈ [a,b]. The upper bound can be proved
analogously. □
Lemma A.2. Let C(x)={y :f(y|x)≥t′}. Then the smallest t′ satisfying
(cid:90) (cid:90)
f(y|x)dµ(y)dP(x)≥1−α
X C(x)
is given by
t′ =inf{t∈R:P(f(Y|X)≥t)≥1−α}.
Proof. By direct calculation.
(cid:90) (cid:90) (cid:90) (cid:90)
P(f(Y|X)≥t)= 1{f(y|x)≥t}dP(y|x)dP(x)= f(y|x)dµ(y)dP(x)
X Y X {f(y|x)≥t}
The proof is complete. □