Check-Eval: A Checklist-based Approach for
Evaluating Text Quality
Jayr Pereira1,2 and Roberto Lotufo2,3
1 Universidade Federal do Cariri (UFCA), Juazeiro do Norte-CE, Brazil
jayr.pereira@ufca.edu.br
2 Universidade Estadual de Campinas (UNICAMP), Campinas-SP, Brazil
3 NeuralMind.ai, Campinas-SP, Brazil
Abstract. Evaluating the quality of text generated by large language
models(LLMs)remainsasignificantchallenge.Traditionalmetricsoften
fail to align well with human judgments, particularly in tasks requiring
creativity and nuance. In this paper, we propose Check-Eval, a novel
evaluationframeworkleveragingLLMstoassessthequalityofgenerated
textthroughachecklist-basedapproach.Check-Evalcanbeemployed
asbothareference-freeandreference-dependentevaluationmethod,pro-
viding a structured and interpretable assessment of text quality. The
framework consists of two main stages: checklist generation and check-
list evaluation. We validate Check-Eval on two benchmark datasets:
Portuguese Legal Semantic Textual Similarity and SummEval. Our re-
sults demonstrate that Check-Eval achieves higher correlations with
human judgments compared to existing metrics, such as G-Eval and
GPTScore, underscoring its potential as a more reliable and effective
evaluation framework for natural language generation tasks. The code
forourexperimentsisavailableathttps://anonymous.4open.science/
r/check-eval-0DB4.
Keywords: NaturalLanguageGeneration·EvaluationMetrics·Large
Language Models · Text Quality Assessment
1 Introduction
Evaluating the quality of text generated by large language models (LLMs) re-
mains an open problem within the field of natural language generation (NLG).
Traditional evaluation metrics, such as BLEU [6], ROUGE [4], and METEOR
[1], often show limited correlation with human judgments, especially in tasks
that require creativity and diversity, such as dialogue generation and summa-
rization.DespiteadvancementsinLLMs,whichcanproducehigh-quality,fluent
textsthatcloselymirrorhumanwriting,thechallengeliesinaccuratelyassessing
these outputs quality.
Recent approaches utilizing LLMs as text quality evaluators have shown
promise,yettheystillfallshortinachievingreliablealignmentwithhumanjudg-
ments [5,3]. This ongoing challenge underscores the necessity for more effective
4202
luJ
91
]LC.sc[
1v76441.7042:viXra2 Jayr Pereira and Roberto Lotufo
and scalable evaluation frameworks that can bridge the gap between automated
metrics and human evaluators, ensuring that the outputs of NLG systems meet
the desired standards of coherence, relevance, and overall quality [5].
In this paper, we propose Check-Eval, a text evaluation framework that
leverages the strengths of LLMs to assess the quality of generated text. The
proposed method instructs the LLM to generate a checklist of key points that
should be present in a candidate text for it to be considered high-quality. This
checklistisderivedfromthesourceorreferencetext,providingastructuredand
interpretable reference for evaluating the candidate. By comparing the candi-
date text to the generated checklist, Check-Eval can provide a nuanced and
comprehensive assessment of text quality, capturing essential elements such as
content consistency, coherence, and relevance.
WeevaluateCheck-Evalintwomainscenarios,bothbasedonhumanjudg-
ments:(1)thePortugueseLegalSemanticTextualSimilaritydataset[7],abench-
mark dataset for evaluating the semantic similarity of legal texts in Portuguese,
and(2)theSummEvaldataset[2],abenchmarkdatasetfortextsummarization
evaluation.OurexperimentsdemonstratethatCheck-Evalachieveshighercor-
relations with human judgments compared to existing metrics, such as G-Eval
and GPTScore, highlighting its potential as a more reliable and effective eval-
uation framework for NLG tasks. Additionally, we show that Check-Eval can
identify specific areas of improvement in the generated summaries, providing
valuable feedback for model development and refinement.
The main contributions of this paper are:
– Introduction of Check-Eval: A novel evaluation framework leveraging
LLMs for text quality assessment.
– Comprehensive Evaluation: Demonstrating the superior performance of
Check-Eval on two benchmark datasets.
– Insightful Feedback: Highlighting the ability of Check-Eval to identify
specific areas of improvement, aiding in model development and refinement.
The remainder of this paper is organized as follows: Section 2 provides an
overview of related work. Section 3 introduces the Check-Eval framework,
detailing the checklist generation and evaluation stages. Section 4 describes
the experimental settings and dataset used to evaluate Check-Eval. Section
5 presents the results of the experiments, comparing Check-Eval to existing
metrics. Finally, Section 6 concludes the paper and discusses future directions
for research.
2 Related Work
The evaluation of automatically generated text has been a persistent challenge
in the NLG field. Traditional metrics such as BLEU, ROUGE, and METEOR
have been extensively used but have shown limitations in aligning with human
judgment, particularly in tasks requiring creativity and nuance [2]. In recent
years, more sophisticated evaluation frameworks leveraging LLMs have beenCheck-Eval: A Checklist-based Approach for Evaluating Text Quality 3
proposed to address these shortcomings. Two recent methods are GPTScore
[3] and G-Eval [5].
GPTScore isaframeworkthatutilizesgenerativepre-trainedtransformers
(GPTs)andotherlanguagemodelstoevaluateNLGoutputswithoutrelyingon
reference texts. The core idea is to assess the probability that the LLM assigns
to the generated text, under the assumption that higher probabilities indicate
higherquality.Fuetal.(2023)[3]demonstratedthatGPTScorecouldachieve
better correlations with human judgments compared to traditional metrics, es-
pecially in open-ended tasks such as dialogue generation and creative writing.
However, despite its advancements, the method lacks interpretability and may
be biased towards texts similar to those seen during the model’s training phase.
G-Eval is another recent approach that leverages the capabilities of LLMs,
specifically GPT-4, to improve NLG evaluation. Proposed by Liu et al. (2023)
[5], G-Eval introduces a chain-of-thought (CoT) [8] paradigm where the eval-
uation process is guided by detailed intermediate steps generated by the LLM.
This method has shown improvements in correlation with human evaluations,
particularly in tasks such as text summarization and dialogue generation. To
address issues with score distribution and variability, G-Eval employs a self-
consistency strategy. Specifically, it generates multiple samples (n = 20) using
different decoding parameters and averages the evaluation scores across these
samples. This approach helps mitigate two key problems: the dominance of a
single score (such as 3 on a 1-5 scale) and the tendency of LLMs to output in-
teger scores, even when decimal values are requested. By using self-consistency,
G-Eval captures more subtle differences between generated texts and provides
a more reliable assessment of text quality.
While both GPTScore and G-Eval represent significant advancements in
automatictextqualityevaluation,Check-Evalaimstoaddresstheirlimitations
through a novel checklist-based approach. Unlike GPTScore, which relies on
the probabilistic output of LLMs, Check-Eval generates a checklist of key
pointsderivedfromthesourcetext.Thischecklistservesasaconcretereference
against which the generated summary is evaluated, providing a more structured
and interpretable assessment of content consistency, coherence, and relevance.
Furthermore, Check-Eval builds upon the structured evaluation paradigm
of G-Eval but simplifies the evaluation process by focusing on key content
pointsratherthangeneratingcomprehensiveCoTsteps.Thisapproachnotonly
reduces the potential for bias towards LLM-generated texts but also enhances
the scalability and applicability of the evaluation framework to a wider range of
NLGtasks.Byprovidingactionablefeedback,Check-Evalcanmoreeffectively
guide model improvement and refinement, ultimately leading to the generation
of higher-quality text.
3 Check-Eval
In this section, we introduce Check-Eval, an LLM-based evaluation frame-
work designed to assess the quality of automatically generated text. Check-4 Jayr Pereira and Roberto Lotufo
Fig.1. Illustration of the Check-Eval methodology.
Eval leverages the capabilities of large language models (LLMs) to generate
and evaluate a checklist of key points derived from the reference document and
thecandidatetext.Theprimaryaimof Check-Evalistoprovideamorestruc-
tured, interpretable, and comprehensive assessment of text quality.
The Check-Eval framework consists of two main stages: checklist gener-
ation and checklist evaluation, as illustrated in Figure 1. The framework has
three main variations: (1) Reference-Guided, (2) Candidate-Guided, and (3)
Criterion-Guided evaluation. The Reference-Guided variation uses a reference
texttogeneratethechecklist,whichisthenusedtoevaluatethecandidatetext.
The Candidate-Guided variation works similarly, but the checklist is generated
fromthe candidate textand usedto evaluate thereferencetext. Thesetwovari-
ations function as recall and precision evaluations, respectively.
The Criterion-Guided variation uses specific evaluation criteria to generate
the checklist, which is then used to evaluate the candidate text against the
referencetext.Itisusefulwhennoreferencetextisavailable,suchasinthecase
of text summarization tasks. Deciding which variation to use depends on the
evaluationscenarioandthecharacteristicsofthedataset.Thefollowingsections
describe the checklist generation and evaluation stages in detail.Check-Eval: A Checklist-based Approach for Evaluating Text Quality 5
[System]
Your task is to write a checklist of the elements a relevant text summary
should have.
Relevance definition: selection of important content from the source.
The summary should include only important information from the source
document. Annotators were instructed to penalize summaries that contained
redundancies and excess information.
You must follow the following rules:
1. The checklist should contain yes/no questions.
2. Checklist items must be self-contained.
3. Focus on the main concepts and avoid including minor details.
4. Each item should represent a unique concept or element from the text.
5. Avoid repetition and overlapping of concepts in the checklist items.
6. The checklist should be comprehensive but not exhaustive, aiming for
clarity and brevity.
7. Generate as many items as you think are necessary
[User]
{source text}
Fig.2. Prompt used to generate the checklist from a source text. The blue text is the
definitionoftheevaluationcriteria,whichisavariablethatcanbechangedaccording
to the desired evaluation criteria (e.g., consistency, coherence, relevance and fluency).
3.1 Checklist Generation
The first stage of Check-Eval involves generating a checklist of key points
to evaluate the quality of the candidate text. Below, we describe the checklist
generation process for the framework’s variations.
Reference-Guided and Candidate-Guided Checklist Generation For
thisstep,let’sconsiderthereferenceandthecandidatetextusedinthereference-
guided and candidate-guided variations, respectively, as the source document.
The LLM is prompted to extract the essential information from the source doc-
umentandcreateanevaluationchecklistbasedonpredefinedevaluationcriteria.
The checklist serves as a structured reference for the key points that should be
present in the text to be evaluated.
Figure 2 shows the prompt used to generate the checklist from a source text
given specific evaluation criteria. The criteria can be adjusted according to the
desired evaluation focus, such as consistency, coherence, relevance, and fluency,
which are generally used in text summarization tasks. Defining the evaluation6 Jayr Pereira and Roberto Lotufo
Source Document Excerpt: "Climate change refers to long-term shifts
and alterations in temperature and weather patterns. These changes may
be natural, such as through variations in the solar cycle. However, since the
1800s,humanactivitieshavebeenthemaindriverofclimatechange,primarily
due to the burning of fossil fuels like coal, oil, and gas."
Generated Checklist:
1. Does the summary mention that climate change refers to long-term shifts
in temperature and weather patterns?
2. Does the summary state that these changes can be natural, such as varia-
tions in the solar cycle?
3. Does the summary highlight that human activities have been the main
driver of climate change since the 1800s?
4. Doesthesummaryspecifythatburningfossilfuelslikecoal,oil,andgasis
a primary cause?
...
Fig.3. Example of a generated checklist based on a source document about climate
change.Thechecklistaimstocapturethekeypointsofthesourcedocumentandserves
as a reference for evaluating the candidate summary.
criteria is crucial to ensure that the generated checklist captures the essential
elementsthatshouldbepresentinahigh-qualitysummary.Thechecklistgener-
ationprocessisrepeatedforeachsourcedocumentandeachevaluationcriterion,
resulting in a set of reference checklists that can be used to evaluate candidate
texts. The prompt is designed to guide the LLM in generating a comprehen-
sive and relevant checklist that captures the main concepts and elements of the
source document, avoiding redundancies and minor details.
Theoutputofthechecklistgenerationstageisalistofyes/noquestionsthat
represent the key points extracted from the source document. Each question
correspondstoauniqueconceptorelementfromthetextthatshouldbepresent
in a high-quality candidate text.
To illustrate the interpretability power of Check-Eval evaluation, consider
the example of a generated checklist based on a source document about climate
changeshowninFigure3.Thisexampledemonstrateshow Check-Evalgener-
ates a structured and interpretable checklist that captures the key points of the
source document that should be present in a high-quality candidate text.
Criterion-Guided Checklist Generation In the criterion-guided variation,
the checklist is generated based on specific evaluation criteria, such as con-
sistency, coherence, relevance, and fluency. The LLM is prompted to create
a checklist of key points to consider when evaluating the quality of a text in
comparison to the reference text, based on the chosen evaluation criteria. The
promptusedforthisvariationissimilartotheoneusedforthereference-guidedCheck-Eval: A Checklist-based Approach for Evaluating Text Quality 7
[System]
Task: assess the consistency of a summary of a source text.
Consistency definition: the factual alignment between the summary and the
summarized source. A factually consistent summary contains only statements
that are entailed by the source document. Annotators were also asked to
penalize summaries that contained hallucinated facts.
Checklist:
{generated checklist}
[User]
{candidate summary}
Fig.4.Promptusedtoevaluateacandidatesummarybasedonthegeneratedchecklist.
The checklist is specific to the evaluation criteria, in this case, consistency.
and candidate-guided variations, but it only includes the evaluation criteria, as
shown in Figure 4. The checklist generated in this variation is used to evaluate
the candidate text against the reference text.
3.2 Checklist Evaluation
The second stage of the Check-Eval framework involves evaluating the candi-
date text based on the generated checklist. The LLM is prompted to compare
thecontentofthecandidatetexttothekeypointsinthechecklistanddetermine
the presence or absence of each key point.
Figure4showsthepromptusedtoevaluateacandidatetextbasedonthegen-
erated checklist for either the reference-guided or candidate-guided variations.
The prompt specifies the evaluation criteria—in this case, consistency—which
is an optional variable that can be changed according to the desired evaluation
focus. The LLM is instructed to assess the defined criteria of the text by com-
paring its content to the key points in the checklist and determining whether
each key point is present. The checklist ensures a structured and comprehensive
assessment.
For the criterion-guided variation, the prompt is similar to the one used for
the reference-guided and candidate-guided variations, but the reference and the
candidate text are passed together with the checklist generated based on the
chosen evaluation criteria. The LLM is prompted to evaluate the candidate text
against the reference text based on the checklist.
Theoutputofthechecklistevaluationstageisascorethatreflectsthequality
of the candidate text based on the presence or absence of key points in the
checklist. The score indicates how well the text captures the essential elements
of the source document according to specific evaluation criteria. For example,8 Jayr Pereira and Roberto Lotufo
in the case of evaluating consistency, the score reflects the factual alignment
betweenthereferenceandthecandidatetext,consideringthepresenceofentailed
statementsandtheabsenceofhallucinatedfacts.Thefinalscoreiscalculatedby
counting the number of key points present in the text, providing a quantitative
measure of the text’s quality.
4 Experiments
In this section, we describe the experiments conducted to evaluate the perfor-
mance of Check-Eval. We begin by detailing the datasets used, followed by
the experimental settings.
4.1 Portuguese Legal Semantic Textual Similarity
We evaluated Check-Eval using the Portuguese Legal Semantic Textual Sim-
ilarity [7] dataset, a benchmark dataset for evaluating the semantic similarity
of legal texts in Portuguese. The dataset consists of pairs of legal documents,
each annotated with a similarity score. The dataset provides two versions of the
annotations: one performed by legal experts on 32 pairs of legal documents and
another automated annotation using heuristics based on legal cases metadata.
The overall correlation between the human and automated annotations is 0.45
(Pearsoncorrelationcoefficient)and0.43(Spearmancorrelationcoefficient).We
experimented with the 32 pairs annotated by legal experts and a subset of 100
pairs annotated by the automated method randomly selected from the dataset.
Experimental Settings Weevaluatedthefirsttwovariationsof Check-Eval
(Reference-GuidedandCandidate-Guided)usingthePortugueseLegalSemantic
Textual Similarity dataset. As we consider the reference-guided variation as the
recall evaluation and the candidate-guided variation as the precision evaluation,
we also computed the F1 score as the harmonic mean of the recall and precision
scores. We used OpenAI’s GPT-4-turbo model to perform both checklist gen-
eration and checklist evaluation. The following steps outline the experimental
setup:
1. ChecklistGeneration:Foreachtextpair(text1andtext2)inthedataset,
wepromptedtheLLMtogenerateachecklistofkeypointsbasedonthecrite-
riaofrelevance,coherence,consistency,andfluency.Forthereference-guided
variation, the checklist was generated from text 1, and for the candidate-
guided variation, the checklist was generated from text 2. The prompt used
for checklist generation is a translation of the prompt used for the Sum-
mEval dataset, as shown in Figure 2.
2. Checklist Evaluation: Each text pair was evaluated based on the gener-
ated checklist. The LLM was prompted to assess the presence or absence of
eachkeypointintext2(candidatetext)comparedtotext1(referencetext).
Thepromptusedforchecklistevaluationisatranslationofthepromptused
for the SummEval dataset, as shown in Figure 4.Check-Eval: A Checklist-based Approach for Evaluating Text Quality 9
3. Scoring: The final score for each text pair was calculated by counting the
number of key points present in text 2, providing a quantitative measure of
its quality. The F1 score was computed as the harmonic mean of the recall
and precision scores.
4.2 SummEval Dataset
WealsoevaluatedCheck-Evalinareference-freeevaluationscenariousingthe
SummEval dataset [2]. The SummEval dataset consists of automatically gen-
eratedsummariesfornewsarticlesfromtheCNN/DailyMaildataset,alongwith
humanannotationsforthequalitydimensionsofcoherence,consistency,fluency,
and relevance. This dataset provides a comprehensive benchmark for compar-
ing the performance of different evaluation metrics against human judgments.
In this case, we evaluated the Criterion-Guided variation of Check-Eval using
the SummEval dataset.
Experimental Settings For our experiments, we used the GPT-4 model to
perform both checklist generation and checklist evaluation. We prompted GPT-
4 to generate a checklist of key points based on the evaluation criteria provided
intheSummEvalpaper[2].Thefollowingstepsoutlinetheexperimentalsetup:
1. ChecklistGeneration:Foreachoftheevaluationcriteria(consistency,rel-
evance,coherence,andfluency),wepromptedGPT-4togenerateachecklist
of key points based on the criteria definitions provided in the SummEval
paper [2].
2. ChecklistEvaluation:Eachcandidatesummarywasevaluatedagainstthe
generated checklist. GPT-4 was prompted to assess the presence or absence
of each key point in the summary. The prompt used for checklist evaluation
is shown in Figure 4.
3. Scoring: The final score for each candidate summary was calculated by
counting the number of key points present in the summary, providing a
quantitative measure of its quality.
5 Results
In this section, we present the results of our experiments evaluating the perfor-
mance of Check-Eval.
5.1 Portuguese Legal Semantic Textual Similarity
Table1summarizestheresultsof Check-EvalonthePortugueseLegalSeman-
ticTextualSimilaritydataset.ThetablepresentsthePearson(ρ)andSpearman
(ρ )correlationsforthereference-guidedandcandidate-guidedvariations,aswell
s
astheF1score,whichistheharmonicmeanoftherecall(reference-guided)and
precision(candidate-guided)scores.TheresultsdemonstratethatCheck-Eval10 Jayr Pereira and Roberto Lotufo
Table 1.Check-EvalresultsforPortugueseLegalSemanticTextualSimilarity.The
table presents (ρ) Pearson and (ρ ) Spearman correlations.
s
Method ConsistencyRelevanceCoherence Fluency AVG
ρ ρ ρ ρ ρ ρ ρ ρ ρ ρ
s s s s s
Reference-guided 0.45 0.52 0.58 0.63 0.49 0.57 0.160.1390.560.61
Candidate-guided0.55 0.62 0.58 0.59 0.55 0.55 0.25 0.14 0.620.62
F1 0.47 0.51 0.58 0.58 0.51 0.53 0.28 0.17 0.580.59
achieves correlation scores above the 0.45 (Pearson) and 0.43 (Spearman) of the
automatedannotationsfromthedataset,indicatingthatCheck-Evalprovides
a reliable and effective evaluation of text similarity in the legal domain.
We evaluated the performance of Check-Eval across four evaluation cri-
teria: consistency, relevance, coherence, and fluency. Additionally, we computed
the average correlation across all criteria to provide an overall assessment of
Check-Eval’sperformance.TheresultsshowthatCheck-Evalachievescom-
petitive correlations with human judgments across all criteria. The F1 score
also demonstrates the effectiveness of Check-Eval in capturing the essential
elements of the source text in the candidate summary.
Thecandidate-guidedvariationof Check-Evalachieveshighercorrelations
withhumanjudgmentscomparedtothereference-guidedvariation.However,for
this dataset, the F1 score provides a more balanced evaluation of text quality,
asbothcandidateandreferencetextsareequallyimportant.Overall,theresults
indicate that Check-Eval is a reliable and effective evaluation framework for
assessing the quality of text similarity in the legal domain.
We also evaluated Check-Eval using the automated annotations from the
dataset. Table 2 presents the results for a sub-sample of 100 pairs automatically
annotatedbytheheuristicsproposedin[7].TheresultsshowthatCheck-Eval
achieves higher correlations with the automated annotations compared to the
humanannotations.Forexample,thereference-guidedvariationshowsaPearson
correlationof0.53forconsistencywithautomatedannotations,whereasitshows
0.45 with human annotations. This suggests that the automated annotations
align more closely with the systematic evaluation approach of Check-Eval.
Table 2.Check-EvalresultsforPortugueseLegalSemanticTextualSimilarity.The
tablepresents(ρ)Pearsonand(ρ )Spearmancorrelationscomputedforasub-sample
s
of 100 pairs automatically annotated by the heuristics proposed in [7].
Method ConsistencyRelevanceCoherence AVG
ρ ρ ρ ρ ρ ρ ρ ρ
s s s s
Reference-guided 0,53 0,54 0,54 0,55 0,54 0,58 0,560,58
Candidate-guided0,52 0,51 0,56 0,57 0,57 0,58 0,580,57
F1 0,50 0,49 0,53 0,55 0,55 0,61 0,550,57Check-Eval: A Checklist-based Approach for Evaluating Text Quality 11
Table 3. Check-Eval results on SummEval. Summary-level (ρ ) Spearman and (τ)
s
Kendall-Tau correlations.
Method Consistency Relevance Coherence Fluency AVG
ρ τ ρ τ ρ τ ρ τ ρ τ
s s s s s
G-eval 0.51 0.42 0.550.430.58 0.46 0.45 0.38 0.51 0.42
GPTScore 0.45 0.38 0.43 0.40 0.41
Check-Eval0.60 0.57 0.50 0.42 0.56 0.46 0.490.440.620.49
The higher correlations with automated annotations could be attributed to
the nature of automated heuristics, which tend to be more consistent and sys-
tematic compared to human annotations that can be subjective and vary sig-
nificantly between annotators. Automated annotations might follow strict rules
that closely match the checklist-based approach of Check-Eval, resulting in
higher alignment.
Moreover, the fluency criterion was excluded from the evaluation with auto-
mated annotations as it showed negative correlations at early evaluation stages.
This exclusion might have contributed to the higher average correlations ob-
served with automated annotations, as fluency tends to be more subjective and
harder to evaluate consistently by both automated methods and LLMs.
5.2 SummEval Dataset
We compare Check-Eval against existing reference-free evaluation metrics,
including G-Eval and GPTScore, using the SummEval dataset. The pri-
maryevaluationmetricsusedareSpearman’srankcorrelationcoefficient(ρ)and
Kendall-Tau correlation coefficient (τ), which measure the correlation between
theautomatedevaluationscoresandhumanjudgments.Table3summarizesthe
resultsof Check-EvalcomparedtoG-EvalandGPTScoreacrossfoureval-
uation criteria: consistency, relevance, coherence, and fluency. Additionally, we
report the average correlation across all criteria.
Consistency Check-Eval achieves a Spearman correlation of 0.605 and a
Kendall-Tau correlation of 0.570 for the consistency criterion, outperforming
both G-Eval (0.507, 0.425) and GPTScore (0.449, -) significantly. This in-
dicates that Check-Eval’s checklist-based approach provides a more reliable
assessment of the factual alignment between the generated summaries and the
source documents.
Relevance For relevance, Check-Eval achieves a Spearman correlation of
0.502andaKendall-Taucorrelationof0.420.AlthoughG-Evalslightlyoutper-
forms Check-Eval in this criterion with a Spearman correlation of 0.547 and
Kendall-Tau correlation of 0.433, Check-Eval still shows competitive perfor-
mance, highlighting its robustness across different evaluation dimensions.12 Jayr Pereira and Roberto Lotufo
Coherence Check-Eval demonstrates strong performance in coherence, with
a Spearman correlation of 0.563 and a Kendall-Tau correlation of 0.461, com-
paredtoG-Eval’s0.582and0.457.AlthoughG-EvalslightlysurpassesCheck-
Eval in this criterion, the results indicate that Check-Eval’s structured ap-
proachtoevaluatingthelogicalflowandclarityofsummariesishighlyeffective.
Fluency Forfluency,Check-EvalachievesthehighestSpearmanandKendall-
Tau correlations (0.490 and 0.446, respectively), outperforming G-Eval (0.455,
0.378)andGPTScore(0.403).ThissuggeststhatCheck-Evalisparticularly
adept at assessing the grammatical and stylistic quality of the generated text.
Overall Performance Overall,Check-Evalachievesthehighestaveragecor-
relationsacrossallcriteria.Theseresultswerecomputedbyaveragingthehuman
annotationscoresforeachcriterionandcomparingthemtotheautomatedeval-
uation scores generated by Check-Eval, G-Eval, and GPTScore. Check-
EvaldemonstratessuperiorperformancewithanaverageSpearmancorrelation
of 0.623 and an average Kendall-Tau correlation of 0.493. These results demon-
stratethatCheck-Evalprovidesamorecomprehensiveandaccurateevaluation
of generated text quality compared to existing metrics.
Thesuperiorperformanceof Check-Evalcanbeattributedtoitschecklist-
based approach, which allows for a more detailed and structured evaluation of
key content points. Traditional metrics often struggle with subjective aspects of
text quality, such as coherence and relevance, due to their reliance on surface-
level comparisons. In contrast, Check-Eval systematically identifies and eval-
uates essential elements within the text, ensuring that all critical aspects are
considered.
6 Conclusions
In this paper, we introduced Check-Eval, a novel evaluation framework for
assessing the quality of automatically generated text. Check-Eval leverages
largelanguagemodels(LLMs)togenerateandevaluateachecklistofkeypoints
derivedfromthesourcedocumentandthecandidatesummary.Ourexperiments
demonstrated that Check-Eval significantly outperforms existing text evalu-
ation metrics, such as GPTScore and G-Eval, in terms of correlation with
human judgments across various dimensions of text quality, including consis-
tency, relevance, coherence, and fluency.
Oneofthekeystrengthsof Check-Evalisitsabilitytoprovideastructured
andinterpretableassessmentoftextquality.Byfocusingonkeypointsextracted
from the source document, Check-Eval offers a detailed evaluation framework
that aligns with human judgment. The checklist-based approach aims to ensure
that all essential elements of the source document are considered, leading to
a more comprehensive and focused evaluation. This way, this method tries to
mitigate the ambiguity and variability often associated with human judgmentCheck-Eval: A Checklist-based Approach for Evaluating Text Quality 13
by standardizing the evaluation criteria and providing a concrete framework for
assessment.
Additionally, Check-Eval reduces the potential for bias present in prob-
abilistic models like GPTScore, which may favor texts similar to those seen
during the model’s training phase. By concentrating on the presence of specific
content elements, Check-Eval provides a fairer and more objective evalua-
tion. This approach also offers actionable feedback by pinpointing specific areas
where the generated text deviates from expected quality standards, thus aiding
in targeted improvements and refinements.
We evaluated Check-Eval using the Portuguese Legal Semantic Textual
Similarity dataset, where it demonstrated higher correlation scores with human
judgments compared to automated annotations, indicating its reliability and
effectiveness in the legal domain. We also evaluated our method using the Sum-
mEval dataset. In this case, the results indicate that Check-Eval achieves
higher correlations with human judgments compared to traditional metrics and
other LLM-based evaluators. This superior performance underscores the poten-
tial of Check-Eval as a reliable and effective evaluation framework for NLG
tasks.
Despiteitsstrengths,ourstudyhassomelimitations.Firstly,theperformance
of Check-Eval is inherently tied to the capabilities of the underlying LLM,
which may introduce biases or errors in checklist generation and evaluation.
Secondly, the current implementation of Check-Eval may require significant
computational resources, making it less accessible for researchers with limited
resources. Lastly, while we demonstrated the effectiveness of Check-Eval in
the context of text summarization and legal text similarity, its performance in
other NLG tasks remains to be thoroughly evaluated.
Futureworkshouldfocusonaddressingtheselimitationsbyexploringwaysto
optimizethecomputationalefficiencyof Check-Evalandextendingitsapplica-
tiontoabroaderrangeofNLGtasks.Additionally,furtherresearchisneededto
refine the checklist generation process to minimize potential biases and improve
the robustness of evaluations.
Declaration of Generative AI and AI-assisted technologies
in the writing process
During the preparation of this paper, the authors used ChatGPT to check the
grammarandsemanticsofthehumanwrittentext.Afterusingthistool/service,
theauthorsreviewedandeditedthecontentasneededandtakefullresponsibility
for the content of the publication.
References
1. Banerjee, S., Lavie, A.: Meteor: An automatic metric for mt evaluation with im-
proved correlation with human judgments. In: Proceedings of the acl workshop on14 Jayr Pereira and Roberto Lotufo
intrinsic and extrinsic evaluation measures for machine translation and/or summa-
rization. pp. 65–72 (2005)
2. Fabbri,A.R.,Kryściński,W.,McCann,B.,Xiong,C.,Socher,R.,Radev,D.:Sum-
mEval: Re-evaluating Summarization Evaluation. Transactions of the Association
for Computational Linguistics 9, 391–409 (04 2021). https://doi.org/10.1162/
tacl_a_00373, https://doi.org/10.1162/tacl_a_00373
3. Fu, J., Ng, S.K., Jiang, Z., Liu, P.: Gptscore: Evaluate as you desire (2023)
4. Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. In: Text sum-
marization branches out. pp. 74–81 (2004)
5. Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., Zhu, C.: G-eval: Nlg evaluation using
gpt-4 with better human alignment (2023)
6. Papineni, K., Roukos, S., Ward, T.,Zhu, W.J.:Bleu: a method for automaticeval-
uation of machine translation. In: Proceedings of the 40th annual meeting of the
Association for Computational Linguistics. pp. 311–318 (2002)
7. da Silva Junior, D., dos Santos Corval, P.R., de Oliveira, D., Paes, A.: Datasets
for portuguese legal semantic textual similarity. Journal of Information and Data
Management 15(1), 206–215 (Apr 2024). https://doi.org/10.5753/jidm.2024.
3564, https://journals-sol.sbc.org.br/index.php/jidm/article/view/3564
8. Wei,J.,Wang,X.,Schuurmans,D.,Bosma,M.,ichter,b.,Xia,F.,Chi,E.,Le,Q.V.,
Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models.
In:Koyejo,S.,Mohamed,S.,Agarwal,A.,Belgrave,D.,Cho,K.,Oh,A.(eds.)Ad-
vancesinNeuralInformationProcessingSystems.vol.35,pp.24824–24837.Curran
Associates, Inc. (2022)