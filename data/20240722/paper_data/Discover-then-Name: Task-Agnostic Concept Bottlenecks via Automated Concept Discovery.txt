Discover-then-Name: Task-Agnostic Concept
Bottlenecks via Automated Concept Discovery
Sukrut Rao∗,1,2 , Sweta Mahajan∗,1,2 , Moritz Böhle1 , and Bernt Schiele1
1 Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken
2 RTG Neuroexplicit Models, Saarbrücken
{sukrut.rao,sweta.mahajan,mboehle,schiele}@mpi-inf.mpg.de
Abstract. ConceptBottleneckModels(CBMs)haverecentlybeenpro-
posed to address the ‘black-box’ problem of deep neural networks, by
firstmappingimagestoahuman-understandableconceptspaceandthen
linearly combining concepts for classification. Such models typically re-
quire first coming up with a set of concepts relevant to the task and
then aligning the representations of a feature extractor to map to these
concepts. However, even with powerful foundational feature extractors
like CLIP, there are no guarantees that the specified concepts are de-
tectable. In this work, we leverage recent advances in mechanistic in-
terpretability and propose a novel CBM approach — called Discover-
then-Name-CBM (DN-CBM) — that inverts the typical paradigm: in-
stead of pre-selecting concepts based on the downstream classification
task,weusesparseautoencoderstofirstdiscover conceptslearntbythe
model, and then name them and train linear probes for classification.
Our concept extraction strategy is efficient, since it is agnostic to the
downstream task, and uses concepts already known to the model. We
perform a comprehensive evaluation across multiple datasets and CLIP
architectures and show that our method yields semantically meaningful
concepts,assignsappropriatenamestothemthatmakethemeasytoin-
terpret, and yields performant and interpretable CBMs. Code available
at https://github.com/neuroexplicit-saar/discover-then-name.
Keywords: Inherent Interpretability · Concept Bottleneck Models
1 Introduction
Deep neural networks have been immensely successful for a variety of tasks, yet
their ‘black-box’ nature poses a risk for their use in safety-critical applications.
While attribution methods [5,47,52] have popularly been used to explain such
models post-hoc, they have been shown to often provide explanations unfaithful
tothemodel[2,3,46].Toaddressthis,inherently interpretable modelshavebeen
proposed [8,12,30] that constrain the model to yield more faithful and human-
understandable explanations in the form of heatmaps, concepts, or prototypes.
* Equal contribution.
4202
luJ
91
]VC.sc[
1v99441.7042:viXra2 S. Rao et al.
Concept Set
R Se ed a, t CS e ot dr l,i op S ue u rs fn u, gS l,l p a …sh se ere ss , , Au Nt ao mm ia nt ged
Top Activating Examples
Image
Colourful, 0.09
Image EC xo trn ac ce tip ot n Spheres, 0.06 ⇒ Ball pit
Fence, 0.09
⋮ ⋮
⓵ Automated Concept ⓶ Automated Concept ⓷ Concept Bottleneck
Discovery Naming Model
Fig.1:Automatedconceptextractionandnamingtoconstructtask-agnostic
concept bottlenecks. Our approach consists of three steps: (1) we use a sparse au-
toencoder to extract disentangled concepts from CLIP feature extractors, (2) auto-
maticallynameextractedconceptsbymatchingthedictionaryvectorswiththeclosest
text embedding in CLIP space from a concept set of texts, and (3) use this named
concept extractor layer as a concept bottleneck to create concept bottleneck models
for classification on different datasets. In the example shown, the concepts ‘colorful’,
‘spheres’, and ‘fence’ are extracted from the image with high strengths, resulting in a
prediction of ‘ball pit’. For details, see Fig. 2 and Sec. 3.
ConceptBottleneckModels(CBMs)[30,36,59]areaclassofinherentlyinter-
pretable models that express their prediction as a linear combination of simpler
but human-interpretable concepts detected from the input features. While typi-
callyconstrainedbytheneedofalabelledattributedatasetfortraining[30],re-
centCBMsleveragelarge-languagemodels(LLMs)suchasGPT-3[10]togener-
ateclass-specificconceptsandvision-languagemodels(VLMs)suchasCLIP[44]
to learn the mapping from inputs to concepts in an attribute-label-free man-
ner[34,36,41,58],andhavebeenshowntobeperformantevenonlargedatasets
suchasImageNet[16].However,suchmethodsstillrequirequeryingLLMsbased
on the classification task, and it is unclear if the concepts one wants the model
to detect can be detected at all; in fact, recent works have suggested that while
plausible, explanations from such CBMs may not be faithful [33,49].
Toaddressthis,inthiswork,weinvertthetypicalCBMparadigm,andaimto
discover concepts the model knows, name them, and then perform classification
(Fig. 1). We specifically use CLIP feature extractors to leverage vision-language
alignmentforautomatednamingofconcepts.Whilerawfeaturesofanetworkare
typically uninterpretable [18], sparse autoencoders (SAEs) have been shown to
be a promising tool in the context of language models wherein they disentangle
learnedrepresentationsintoasparsesetofhuman-understandableconcepts.This
is achieved by decomposing the representations into a sparse linear combination
ofasetoflearneddictionaryvectors[9,14].Weextendthistovisionandfinditto
be similarly promising, and surprisingly, find that the dictionary vectors appear
to align well with text embeddings of concepts they represent in CLIP space,
thusmakingtheircorrespondingconceptsnameable(Fig.3).Finally,weusethis
latent concept space as a concept bottleneck, and show that, once learnt, it can
befrozenandused‘asis’totrainclassifierstoconstructperformantCBMsforaTask-Agnostic Concept Bottlenecks via Automated Concept Discovery 3
varietyofdownstreamclassificationtasks.Ourapproachisalsocomputationally
efficientsinceitlearnsconceptbottlenecksinatask-agnosticmanner,eliminating
the need to make queries to external LLMs to find task-relevant concepts.
Insummary,our contributions are WeproposeDN-CBM,anovelCBM
•
that leverages sparse autoencoders (SAEs) to discover concepts learnt by CLIP.
WefindthatSAEslendthemselveswelltooursimpleandintuitiveapproachfor
automated concept discovery. We propose a novel approach to automatically
•
name the discovered concepts, by mapping concepts to text with embeddings
most similar to the corresponding dictionary vectors of the learned concept.
We find that this often yields names semantically consistent to the images ac-
tivating the concept (Fig. 3). We show that, once discovered and named, the
•
learntconceptmappingcanbeusedtotrainconceptbottleneckmodels(CBMs)
out-of-the-box for a variety of downstream classification tasks. Specifically, we
discoverconceptsusingCC3M[53]inatask-agnosticfashion,andthenconstruct
CBMs for a variety of downstream datasets: ImageNet [16], Places365 [62], CI-
FAR10[31],andCIFAR100[31].Importantly,thistask-agnosticconceptdiscov-
ery approach yields both performant (Tab. 1) and interpretable (Figs. 7 and 8)
classifiers.
2 Related Work
Concept-based Explanations (e.g. [1,28,30,37]) aim to express a model’s
decision via human-understandable concepts. Unlike popularly used post-hoc
attribution heatmaps (e.g. [5,32,47,52,54,55]) that only inform which regions
in the input is influential for the decision, such methods attempt to also answer
what high-levelconceptsareimportantforthemodel[1].Inourwork,wepropose
apipelineforautomaticallyextractingandnamingsuchconceptsfromCLIPand
using them to build interpretable models.
Concept Discovery [6,9,14,19–22,37–39,60] methods have been proposed to
betterunderstandmodelsbydiscoveringandextractingsemanticallymeaningful
conceptslearntbythem.Theytypicallyfocusonexplainingthefunctionofneu-
rons in a model [6,20,37,39], or on discovering features present in an input, and
have been shown to be useful for diagnosing model failures [20]. However, these
methodsassignconceptstoindividualneurons,whichmayoftenbepolysemantic
and not decodable to human-understandable concepts [18]. Recently, [9] showed
that sparse autoencoders (SAEs) can be effective to address the polysemantic-
ity and superposition problem in deep networks [18] and extract mono-semantic
concepts from language models (cf. [14]). We extend the setup of [9] to vision
and use sparse autoencoders to automatically extract concepts learnt by CLIP.
Explanations using Language [13,15,24,25,35,37,39,56,61] have become
populartoexpressamodel’slearntrepresentations[15,37,39]inaneasilyhuman-
interpretable manner. To decode concepts to language, such methods typically
use a large-language model (LLM) such as GPT-3 [10] and learn a mapping
from vision featuresto the LLM input. Morerecently, [37,39]leverageCLIP,by
aligningvisionfeaturesofthemodelbeingexplainedtotherepresentationspace4 S. Rao et al.
of CLIP and finding the closest aligned texts from a large concept set. Similar
to Concept Discovery methods, this line of work assigns concepts to individual
neurons as well. In contrast, we use sparse autoencoders (SAEs) [9] to first
disentangle the representation space into more human-interpretable concepts,
and then name each concept. In particular, we encode the CLIP features to a
high dimensional latent space, which we then pass through the SAE decoder
to reconstruct back the CLIP features. Surprisingly, when using CLIP feature
extractors, we find that the dictionary vectors of the SAE decoder can directly
be decoded into text by finding most similar text embeddings in CLIP space,
without needing to use LLMs as used by [15,35,56], and are effective in yielding
semantically meaningful and human-interpretable concepts (Figs. 3 and 4).
Concept Bottleneck Models (CBMs) [4,11,26,27,29,30,34,36,40,41,51,
57–59,63] are a recently popular class of inherently interpretable models (e.g.
[8,12]) that use a concept bottleneck layer (CBL) to extract named concepts
and then learn a (typically sparse) linear classifier that predicts by combining
such concepts, yielding highly interpretable explanations. While such methods
typically require a labelled concept dataset [30] to learn the concept bottleneck,
recentworksleverageLLMssuchasGPT-3[10]andVLMssuchasCLIP[44]to
learnsuchbottleneckswithoutneedingtheconceptlabels[36,40,41,58],making
themscaletolargedatasetssuchasImageNet[16]inaperformantmanner.Given
a classification task, such methods first query an LLM for concepts relevant to
thetask,anduseaVLMtolearnaconceptbottleneckwhereeachneuronaligns
tooneofthedesiredconcepts.However,itisunclearifthefeatureextractorcan
trulyrecognizeallsuchconceptswhenspecifiedapriori,sincetheymayoftenbe
non-visual[49,58]andtheirfaithfulnesshasalsobeencalledintoquestion[33,49].
Further, the CBL needs to be trained separately for each classification dataset.
Incontrast,wefliptheparadigmandfirstextractconceptsthataredetectedby
the model to train the concept bottleneck, using a dataset independent of the
downstream classification task. We then fix the concept bottleneck and train
linearclassifiersforseveraldatasetsandshowthatthisyieldshighlyperformant
and interpretable models. Similar to us, [26] also first discover concepts before
constructing a CBM; however, in contrast our method does not require any
external text annotations for the images and the concept discovery can even be
done using a dataset different from that of the downstream task.
Explaining CLIP. Several approaches have been proposed to specifically ex-
plain and understand CLIP [44] models [7,41,56]. Similar to [7], we disentangle
CLIPfeaturesintohumaninterpretableconcepts.However,incontrastto[7],we
donotoptimizeforasparseconceptrepresentationperimageusingapredefined
concept set, and instead first apply a general concept discovery framework [9]
for extracting human understandable concepts and then name them post hoc in
a task-agnostic manner to construct CBMs.Task-Agnostic Concept Bottlenecks via Automated Concept Discovery 5
⓵ Automated Concept Discovery ⓶ Automated Concept Naming
Sparse Autoencoder Text Embeddings
Concepts
❄ CLIP Features " " Reconstruction R Se uS Sd np e, g h aS le att er sr ei d sp s e,e , s s ,, ❄ EnC T ceL oxI dP t e r
EV ni cs oio dn e r EL ni cn oe da er r DL ei cn oe dar e r Colourful, … Cosine
⋮ Dictionary Vector Similarity
❄
ℒsparse
+
ℒrecon
⓷ Concept Bottleneck Model Concepts
Prediction + Explanation
❄ Features "
❄
EV nC i csL oiI o dP n
e r
Encoder L Li an ye ea rr
⋮
ℒclassify+λℒsparse
Fig.2:Overview.Ourapproachconsistsofthreesteps.(1)Wetrainasparseautoen-
coder to extract disentangled concepts from a CLIP vision backbone. The autencoder
istrainedonalargedatasetD toreconstructCLIPfeaturesusingalinearcom-
extract
binationofencodedconcepts,whichareoptimizedtobesparseusingL sparsity.The
1
weights of the decoder can be interpreted as dictionary vectors whose linear sum with
conceptstrengthsreconstructstheoriginalfeature(Sec.3.1).(2)Weusealargeconcept
set of texts V to name each extracted concept, by finding the text from the set whose
embedding has the highest cosine similarity to concept’s dictionary vector (Sec. 3.2).
(3)Weusetheextractedandnamedconceptsasaconceptbottlenecklayer,andtrain
linearclassifierstoconstructinherentlyinterpretableconceptbottleneckmodelsacross
downstream datasets D using the same bottleneck layer (Sec. 3.3).
classify
3 Constructing CBMs via Automated Concept Discovery
Inthissection,wedescribeourapproachwhichconsistsofthreestages:discover-
ing the concepts the CLIP model has learnt via a sparse autoencoder (Sec. 3.1),
naming those concepts in natural language by leveraging the CLIP text embed-
dings from a large vocabulary, (Sec. 3.2), and, lastly, training an interpretable
concept bottleneck model (CBM) based on the discovered concepts (Sec. 3.3).
3.1 Extracting Concepts Learned by the Model
Todiscovertheconceptslearnedbythemodel,weadaptthesparseautoencoder
(SAE) approach asdescribed by [9].Specifically, we aimto discoverconcepts by
representing the CLIP features in a high-dimensional, but very sparsely activat-
ing space. For language models, this has been shown to yield representations in
which individual neurons (dimensions) are more easily interpretable [9].
TheSparseAutoencoders(SAEs)proposedby[9]consistofalinearencoder
f() with weights W Rd×h, a ReLUnon-linearity ϕ, and alinear decoder g()
E
wi·
th weights W
Rh∈
×d. For a given input a, the SAE computes:
·
D
∈
SAE(a)=(g ϕ f)(a)=WT ϕ(cid:0) WTa(cid:1) . (1)
◦ ◦ D E6 S. Rao et al.
Importantly,thehiddenrepresentationf(a)isofsignificantlyhigherdimension-
ality than the CLIP embedding space (i.e. h d), but optimised to activate
≫
only very sparsely. Specifically, the SAE is trained with an L reconstruction
2
loss, as well as an L sparsity regularisation:
1
(a)= SAE(a) a 2+λ ϕ(f(a)) (2)
LSAE ∥ − ∥2 1 ∥ ∥1
with λ a hyperparameter. To discover a diverse set of concepts for usage in
1
downstream tasks, we train the SAE on a large dataset ; given the re-
extract
D
construction objective, no labels for this dataset are required.
Note that sparsity does of course not guarantee that individual neurons in
the hidden representation of the SAE align with human-interpretable concepts.
However, similar to [9], in our experiments we find that this is often the case,
and, as we discuss in the next section, can often even be automatically named.
WhySAEs?WhileSAEsarecertainlynottheonlyoptionforconceptdiscovery
in DNNs, recent work on language models suggests that they might be particu-
larlywellsuitedtodiscoverinterpretableconcepts,see[9,14],andexhibitcertain
properties that lend themselves well for automatically naming visual concepts.
Specifically,aswewillseeinthenextsection,byreconstructingtheoriginalfea-
ture space, we are able to leverage the dictionary vectors of the reconstruction
matrix W for assigning names to individual concepts. Moreover, in contrast
D
to dimensionality reduction techniques (e.g. PCA), SAEs are able to represent
more features than there are neurons, which was shown to be advantageous to
address the problem of polysemanticity [9].
3.2 Automated Concept Naming
Once we trained the SAE, we aim to automatically name the individual feature
dimensions in the hidden representation of the SAE. For this, we propose using
a large vocabulary of English words, say = v ,v ,... , which we embed via
1 2
V { }
the CLIP text encoder to obtain word embeddings = e ,e ,... .
1 2
T E { }
TonametheSAE’shiddenfeatures,weproposetoleveragethefactthateach
of the SAE neurons c is assigned a specific dictionary vector p , corresponding
c
to a column of the decoder weight matrix:
p =[W ] Rd . (3)
c D c
∈
If the SAE indeed succeeds to decompose image representations given by CLIP
into individual concepts, we expect the p to resemble the embeddings of par-
c
ticular words that CLIP has learnt to expect in a corresponding image caption.
Hence, to name the ‘concept’ neuron c of the SAE, we propose to assign it
the word s of the closest text embedding in :
c
E
s =argmin [cos(p , (v))] . (4)
c c
v∈V T
Note that this setting is equivalent to using the SAE to reconstruct a CLIP
feature when only the concept to be named is present. As CLIP was trained to
optimisecosinesimilaritiesbetweentextandimageembeddings,usingthecosine
similarity to assign names to concept nodes is a natural choice in this context.Task-Agnostic Concept Bottlenecks via Automated Concept Discovery 7
3.3 Constructing Concept Bottleneck Models
Thus far, we trained an SAE to obtain sparse representations (Sec. 3.1), and
named individual ‘neurons’ by leveraging the similarity between dictionary vec-
tors p to word embeddings obtained via CLIP’s text encoder (Sec. 3.2).
c
T
Such a sparse decomposition into named ‘concepts’ constitutes the ideal
startingpointforconstructinginterpretableConceptBottleneckModels(CBMs)
[30,36,59]: for a given labelled dataset , we can now train a linear trans-
probe
D
formation h() on the SAE’s sparse concept activations, yielding our CBM t():
· ·
t(x )=( h ϕ f )(x ). (5)
i i
(cid:124)(cid:123)(cid:122)(cid:125)◦ (cid:124)(cid:123)◦(cid:122)(cid:125) ◦(cid:124)(cid:123)I(cid:122)(cid:125)
Probe SAE CLIP
Here, x denotes an image from the probe dataset. The probe is trained using
i
the cross-entropy loss, and to increase the interpretability of the resulting CBM
classifier, we additionally apply a sparsity loss to the probe weights:
(x )=CE(t(x ),y )+λ ω (6)
probe i i i 2 1
L ∥ ∥
where, λ is a hyperparameter, y the ground truth label of x in the probe
2 i i
dataset, and ω denotes the parameters of the linear probe.
Importantly, note that the feature extractor, the dataset used for concept
discovery, and the vocabulary used for naming can be freely chosen. As such,
our approach is likely to benefit from advances in any of these directions.
4 Evaluation of Concept Discovery and Naming
Inthissection,weevaluatetheeffectivenessofusingSAEstodiscoverandname
concepts in CLIP vision encoders; see Sec. 5 for an evaluation of CBMs built on
the SAEs. In Sec. 4.1, we first evaluate the accuracy and task agnosticity of the
discovered concepts qualitatively and quantitatively, in Sec. 4.2, we discuss the
impact of the vocabulary towards the granularity of concept names, and in
V
Sec. 4.3, we evaluate how well semantically similar concepts group together.
Setup.WeuseaCLIP[44]ResNet-50[23]visionencoderforextractingfeatures,
andusethecorrespondingtextencoderforlabellingtheextractedconcepts.For
additional results using CLIP ViT-B/16 and ViT-L/14 [17], see Appendices C
and D. To extract concepts, we follow a setup similar to [9] and train SAEs
usingtheCC3Mdataset[53].Following[37],weusethesetof20kmostfrequent
English words as the vocabulary (Eq. (4)). For details, see Appendix B.1.
V
4.1 Task-Agnosticity and Accuracy of Concepts
In this section, we qualitatively and quantitatively evaluate the extracted and
named concepts for semantic consistency and accuracy.
Qualitative. To showcase the promise of our proposed approach, in Fig. 3 we
visualizethetopactivatingimagesacrossfourdatasetsforvariousconceptsthat8 S. Rao et al.
turquoise stripes sunglasses asleep
Index2031 Index1715 Index3703 Index371
ImageNet
CIFAR10
CIFAR100
Places365
pink fog smiling silhouette
Index7188 Index2911 Index1324 Index5221
ImageNet
CIFAR10
CIFAR100
Places365
Fig.3: Task-agnosticity of concept extraction.Weshowexamplesofnamedcon-
cepts(blocks)andtopimagesactivatingthemfromfourdatasets(rows).Wefindthat
the images activating the concept are highly consistent with the concept name across
datasets(e.g.the‘asleep’conceptyieldsimagesacrossdifferentspecies),despitenotus-
ingthesedatasetsforextractionandnaming,showingtherobustnessofourapproach.
were discovered and named as described in Secs. 3.1 and 3.2. For this, we se-
lect concepts c from the vocabulary with a high cosine similarity between p
c
and (s ), see Eqs. (3) and (4). In particular, we show examples for various
c
T
low-level concepts (turquoise, pink, striped), object and scene-specific concepts
(sunglasses, fog, silhouette), as well as higher-level concepts (asleep, smiling),
and find that the visualized concepts not only exhibit a high level of seman-
tic consistency, but also that the automatically chosen names for the concepts
accurately reflect the common feature in the images, despite coming from very
differentdatasets.ThishighlightsthepromiseoftheSAEfordisentanglingrepre-
sentationsintohumaninterpretableconceptsaswellasoftheproposedstrategy
for naming those concepts. Interestingly, as expected, we find that the accuracy
of the ascribed names correlates with the cosine similarity between the text em-
bedding (s ) and the dictionary vector p (cf. Eq. (3)), as we discuss next
c c
T
(see also Fig. 4). This indicates that our naming strategy could be significantly
improved with a larger vocabulary, as we also discuss in Sec. 4.2.
Quantitative. To not only rely on the visual assessment of a few selected sam-
ples, we perform quantitative evaluations to assess the concept consistency and
naming accuracy. This is generally challenging as only a few datasets include
concept labels, and, even if they do, might describe different concepts in the
imagethanthosethatwereextractedbyourtask-agnosticapproach.Toaddress
this, we perform a user study to evaluate concept accuracy. Specifically, we sort
conceptsbasedonhow welltheirdictionaryvector isalignedto thetextembed-
dings of the name assigned to them (Sec. 3.2), and sample concepts with high,
intermediate,andlowalignments.Wethenextractthetopactivatingimagesfor
each concept from three datasets (ImageNet, Places365, CC3M), and for each
concept, we ask two questions: (1) how semantically consistent the concept is,
i.e. if the top activating images map to some human interpretable concept, and
(2) how accurate the assigned name is, if so. To evaluate if our SAE yields moreTask-Agnostic Concept Bottlenecks via Automated Concept Discovery 9
Semantic Consistency of Concepts Consistency vs. Accuracy
5 Ours CLIP Features 5
4 4
3 3
2 2
1 1
High Intermediate Low n/a 1 2 3 4 5
Alignment Accuracy
Fig.4: User study on concept accuracy. Left: We evaluate the semantic consis-
tency of concepts for nodes with high, intermediate, and low alignment with the text
embeddingsofthenameassignedtothem,bothfornodesfromourSAE(green)andthe
CLIPfeatures(orange).WefindthattheconceptsfromtheSAEaresignificantlymore
semantically consistent than CLIP features, and the consistency increases with align-
ment.Thepoorperformanceofthe‘lowalignment’groupsuggeststhatsomenodesdo
notcorrespondtoaconsistenthumaninterpretableconcept.Right: Weplotthescores
forsemanticconsistencyagainstnameaccuracyfromhumanevaluators,bothfornodes
from our SAE (green) and the CLIP features (orange). We find that compared to the
baseline, our SAE nodes are generally more consistent and accurately named.
disentangled concepts, we also compare with the neurons from the CLIP image
features, named using CLIP-Dissect [37], as a baseline. For full details, see Ap-
pendixB.3.InFig.4(left),wereportthedistributionofconsistencyscoresboth
for our discovered concepts and the CLIP baseline each for the high, intermedi-
ate, and low aligned concepts, and find that our approach provides significantly
more human interpretable concepts. Interestingly, for both sets of concepts, the
consistency decreases as the alignment with the text embedding decreases, sug-
gesting that some concepts are not human interpretable. In Fig. 4 (right), we
evaluate the concept consistency against name accuracy, and find that our as-
signed names score highly in terms of accurately representing the concept (top
right) as compared to the baseline. Note that some concepts, despite being con-
sistent, are not named accurately, which could also be because of limitations in
the vocabulary used; for more discussion, see Sec. 4.2.
In addition to the human evaluation, we also perform a small quantitative
evaluation using the SUNAttributes dataset [42], following [41]. We use its la-
belled attributes as the vocabulary for naming the nodes in the SAE (Sec. 3.2)
to match discovered concepts to ground truth labels. To account for concepts
outside the labelled attribute set, we filter out nodes where the cosine similarity
betweenthedictionaryvectorandtheassignedtextembeddingisbelowathresh-
old, and merge concepts assigned to the same name. As a baseline, we compare
against images obtained using CLIP retrieval from the ground truth attributes.
We obtain a Jaccard index of 18.3, as compared to 22.0 for the CLIP retrieval
baseline (for comparison, [41] report a Jaccard index of 15.7 under a similar
setting) despite not optimizing the SAE to learn dataset-specific concepts.
gnitaR
resU
Consistency10 S. Rao et al.
ornaments←It nr de ee x7→446christmastree branches←Int dr ee xe 81→67treeinfield prague←bri Ind dg exe 4s 12→9archbridge lisbon←bridge Is nd→ex6s 8u 73spensionbridge
Fig.5: Impact of vocabulary. We show examples of pairs of concepts that, despite
beingassignedtothesamecoarsegrainedname(e.g.left:‘tree’),correspondtodistinct
fine-grainedconcepts.Betternamesthatcandistinguishingsuchconceptsareassigned
ifaddedtothevocabulary(e.g.‘christmastree’forthefirstconcept,and‘treeinfield’
for the second). On the other hand, removing the assigned name from the vocabulary
leads to worse names being assigned (e.g. ‘ornaments’ and ‘branches’), which shows
that the granularity of the vocabulary can impact name accuracy.
4.2 Impact of Vocabulary on Concept Name Granularity
As seen in Sec. 4.1 and Fig. 4, some of the SAE nodes may not map to human
interpretable concepts(Fig. 4,left), ormay notbenamed appropriately(Fig. 4,
right). The latter could be a result of limitations in the vocabulary: it being
finite and only consisting of single words, it is possible that even concepts that
the SAE discovers cannot be named accurately.
To explore this, in Fig. 5 we visualize examples of concept pairs that are
originallyassignedthesamename(e.g.right:‘bridges’),butvisuallycorrespond
to distinct modalities of the concept. We find that a more fine-grained name
is assigned to the concept when added to the vocabulary (e.g. ‘arch bridge’,
V
‘suspension bridge’). Conversely, removing the assigned name ‘bridge’ from the
vocabulary leads to worse names being assigned (e.g. ‘prague’, ‘lisbon’; inter-
estingly, note that the cities contain a prominent arch and suspension bridge,
respectively). This suggests that the granularity and size of the vocabulary can
significantly affect the name accuracy, and can also serve as a tool for practi-
tioners to control the granularity of assigned names depending on the use case.
4.3 Clustering Concept Vectors
To further measure semantic consistency, we also evaluate how well semanti-
cally related concepts cluster together in the latent concept space. To do this,
weperformK-Meansclusteringontheconceptrepresentationsacrossallimages
in the Places365 dataset, and visualize a random selection of clusters. For each
cluster, we compute the cluster centroid and then visualize the strongest con-
cepts. We find that semantically similar concepts and their associated images
cluster together in concept space (e.g. farming related concepts and images in
the right), showing that our concept-based (latent) representation does indeed
result in semantically meaningful and nameable similarities.Task-Agnostic Concept Bottlenecks via Automated Concept Discovery 11
SampledImagesfromCluster SampledImagesfromCluster SampledImagesfromCluster
pier cabinet vegetables
tahoe subcommittee fruit
walks suits fields
halfway legislative maize
duluth judicial leaves
beach ministerial farming
Fig.6: Extracting meaningful clusters from concept strength vectors. We
perform K-Means clustering over concept activation vectors on the Places365 dataset
toevaluatethesemanticconsistencyoftheselatentrepresentations.Weshowarandom
subset of clusters: each block represents a cluster, and we show top concepts from the
cluster centroid and randomly selected images assigned to the cluster. We find that
highly semantically consistent clusters of concepts emerge (e.g. right: concepts and
images from classes related to farming are grouped together).
5 Evaluation of DN-CBM
We now present results on the concept bottleneck models (DN-CBM) (Sec. 3.3)
builtonthediscoveredandnamedconcepts(Secs.3.1,3.2),evaluatingaccuracy
(Sec.5.1),interpretability(Sec.5.2),andeffectivenessofinterventions(Sec.5.3).
Setup. Similar to prior work [36,41], we train linear classifiers on top of the
extracted concepts on four datasets—ImageNet [16], CIFAR10 [31], CIFAR100
[31], and Places365 [62]—and evaluate them for accuracy and interpretability.
We train with various hyperparameters and pick the configurations based on
performance on a heldout set. We compare our CBMs with recently proposed
label-free approaches: LF-CBM [36], LaBo [58], DCLIP [34] and CDM [41], and
alsoreportthelinearprobeandzero-shotperformanceoftheCLIPmodelweuse
as a backbone for reference. We use the respective concept sets of each baseline
method, and for a fair comparison, the same feature extractor across methods.
5.1 Classification Performance
In Tab. 1, we show the classification performance of our DN-CBM on four
datasets and two feature extractors and compare them with the baselines. We
findthatDN-CBMishighlyperformantacrossdatasetsandbackbones.Despite
being task-agnostic, DN-CBM almost always outperforms the baselines, which
use concept sets optimized for the downstream task, showing the generality of
our approach. The highest gains are with Places365 (i.e. 52.70 53.53 pp. on
→
ResNet-50 and 52.58 55.11 pp. on ViT-B/16), which is a scene-classification
→
dataset rich in a wide variety of objects, which correspond to coarser, higher
level concepts than e.g. body parts of animals as in ImageNet or CIFAR10, and
are likely more well-represented in our concept space trained on CC3M.
5.2 Interpretability of DN-CBM
LocalExplanations(Image-Level). InFig.7,weshowqualitativeexamples
of local explanations from our DN-CBM, i.e., explanations of individual deci-
sions. For each image, we show the most contributing concepts along with their
stpecnoCpoT12 S. Rao et al.
Table1:PerformanceofourCBMincomparisontopriorwork.Wereportthe
classificationaccuracy(%)ofourCBMandbaselinesusingCLIPResNet-50andViT-
B/16featureextractors(ViT-L/14inAppendixC)onImageNet,Places365,CIFAR10,
andCIFAR100.WefindthatourCBMperformscompetitivelyandoftenoutperforms
priorwork,despiteusingacommonsetofconceptsacrossdatasets.‘*’indicatesresults
reportedfortherespectivebaselines,andzero-shotperformanceisasreportedby[44].
Model Task CLIP ResNet-50 CLIP ViT-B/16
Agnostic IMN Places Cif10 Cif100 IMN Places Cif10 Cif100
Linear Probe - 73.3* 53.4 88.7* 70.3* 80.2* 55.1 96.2* 83.1*
Zero Shot - 59.6* 38.7 75.6* 41.6* 68.6* 41.2 91.6* 68.7*
LF-CBM [36] ✗ 67.5 49.0 86.4* 65.1* 75.4 50.6 94.6 77.4
LaBo [58] ✗ 68.9 - 87.9* 69.1* 78.9 - 95.7 81.2
CDM [41] ✗ 72.2* 52.7* 86.5* 67.6* 79.3* 52.6* 95.3* 80.5*
DCLIP [34] ✗ 59.6 37.9 - - 68.0* 40.3* - -
DN-CBM (Ours) ✓ 72.9 53.5 87.6 67.5 79.5 55.1 96.0 82.1
contribution strengths. We find that the concepts used are intuitive and class-
relevant, thus aiding interpretability. The concepts used are also diverse, and
includeobjectsinthescenes(e.g.‘rocks’for‘swimminghole’,top-right),similar
features (e.g. ‘toaster’ given the corroded surface for ‘junkyard’, bottom-left),
and high-level concepts (e.g. ‘abandoned’ for ‘junkyard’, bottom-left). Interest-
ingly, we also find concepts associated with the class (e.g. ‘alps’ or ‘everest’ for
‘glacier’, top-left), which shows that the model’s decision is also based on what
ascenelooks like,akintoProtoPNets[12].Finally,weobservethattheconcepts
for predicting the same class change based on the contents of the image, e.g. in
the bottom row, we find that despite both images depicting a junkyard where
the most influential concept is ‘corrosion’, the second highest concepts are ‘car’
and ‘tractor’ respectively, reflecting the image contents.
In Fig. 8, we also compare explanations from DN-CBM with baselines (LF-
CBM, CDM) on the same images from Places365. Interestingly, we find that
our approach yields similarly convincing explanations as prior state-of-the-art
CBM models, despite the fact that it does not use a task-specific vocabulary
and extracts the concepts on a separate dataset (CC3M).
Global Explanations (Class-level). In Fig. 9,weshowqualitativeexamples
of global explanations from our DN-CBM, i.e., explanations of which concepts
contribute themostto aclass as a whole.Todo this,foreachclass,wecompute
the average contribution of all concepts for images from that class, and visu-
alize the set of top concepts. Qualitatively, we find this set to be semantically
consistent with what is contained in each class.
5.3 Effectiveness of Concept Interventions
In addition to understanding model decisions, explanations have also been used
to debug models [30] and fix models’ reasoning [43,45,48]. Specifically, concept
bottleneck models allow human interventions on individual concepts to controlTask-Agnostic Concept Bottlenecks via Automated Concept Discovery 13
PredictedClass:Glacier MostStronglyContributingConcepts PredictedClass:Swimminghole MostStronglyContributingConcepts
alps +2.54 rapids +1.22
snowy +1.36 canoeing +0.81
everest +1.35 rocks +0.78
antarctica+0.70 pond +0.64
mud +0.51 wetland +0.60
PredictedClass:Junkyard MostStronglyContributingConcepts PredictedClass:Junkyard MostStronglyContributingConcepts
corrosion +1.70 corrosion +1.52
car +1.12 tractor +0.77
toaster +0.83 paintball +0.70
van +0.63 carriage +0.65
abandoned +0.57 packard +0.57
Fig.7: Explaining decisions using our CBM. We show examples of images from
the Places365 dataset along with the top concepts contributing to the decision. We
find that our CBM classifies based on a diverse set of concepts present in the image,
including objects, similar features, higher level concepts, and things associated with
the class (e.g. similar locations), thus aiding interpretability.
Predicted:Swimminghole Ours LF-CBM CDM
rapids +1.22 astream +1.65 adivingboard +6.76
canoeing +0.81 thewaterishottoth+e1t.0o4uch canbeverylong +6.57
rocks +0.78 apaddle +0.94 clearbluewaters +6.13
pond +0.64 alakeorriver +0.83 mayberockyorforested +6.09
wetland +0.60 swimsuits+0.58 alarge,openareaofwater+5.97
Predicted:Raft Ours LF-CBM CDM
tubing +2.20 alifejacket +1.41 youngpeople +9.04
rapids +1.75 jettedorbubblingwater +1.16 chlorinatedwater +8.87
canoeing +0.94 akayak +1.07 aboat +8.85
waves +0.80 flotationdevices +0.87 thewaterishottothetouch +8.69
kayaking +0.76 fun +0.45 amooring +7.72
Fig.8: Comparing interpretability across CBMs. We show an example from
the Places365 dataset with explanations consisting of top contributing concepts using
our CBM, LF-CBM [36], and CDM [41]. We find that our approach yields similar
explanations despite not querying LLMs for concepts specific to the task and instead
using a single task-agnostic concept bottleneck layer that is named post hoc.
Class:Amusementarcade Class:Amusementpark Class:Apartmentbuildingoutdoor
daycare acrobat towers
display ore harlem
playground festival courthouse
tv clic windows
laserjet transmitter victorian
budgeting bridges mississauga
playstation carnival travelodge
holdem playground townhouse
scrabble disneyland bldg
arcade wheel condominiums
Fig.9: Class-wise explanations of CBMs. We show examples of classes from the
Places365datasetwiththetopcontributingconcepts.Foreachclass,weshowrandom
examplesofimagesbelongingtothatclassandselectconceptswiththehighestaverage
contribution across all images from the class in the validation set. We find that our
approach yields classifiers that use concepts highly semantically related to each class.
the models’ reliance on them. We assess the effectiveness of our DN-CBM with
interventions by training on the Waterbirds-100 [43,50] dataset. This contains
stpecnoCpoT14 S. Rao et al.
images of Landbirds and Waterbirds, with landbirds (waterbirds) on land (wa-
ter) backgrounds during training, but without any such correlation in the test
set. Following [43,46] we evaluate if intervening to (1) only keep bird related
concepts, and (2) only remove such concepts increases (respectively, decreases)
the performance on the worse group classification. To do this, we train a DN-
CBM model that uses only five concepts for each class. For full details, see
Appendix B.4.
In Tab. 2, we report the accuracy before and after the two interventions. We
find that keeping only bird related concepts significantly improves the overall
andworstgroup(LandbirdonWater,WaterbirdonLand)accuracies,withonly
a small drop in the other groups. Similarly, removing only such concepts leads
to a large drop in accuracies, showing the effectiveness of interventions.
Table 2: Performance before and after intervening on the concept bottle-
neckmodeltrainedfortheWaterbirds-100dataset.Wereporttheclassification
accuracy (%) on the full test set (‘Overall’) and each of the four groups (e.g. Land-
birdonWater,shownas‘L.Bird@W’)beforeandafterapplyinginterventions.Wefind
that intervening to only keep bird relevant concepts increases the overall and worst
group [50] (Landbird on Water, Waterbird on Land) accuracy significantly, and con-
versely removing exactly these concepts leads to a large drop in accuracy, without
adversely affecting performance on the groups in the training set (‘Training Groups’).
Worst Groups Training Groups
Model Overall
L.Bird@W W.Bird@L L.Bird@L W.Bird@W
BeforeIntervention 82.8 71.3 57.5 98.6 93.3
OnlyBirdConcepts 89.4 (+6.6) 86.6 (+15.3) 71.3 (+13.8) 96.8 (-1.8) 91.4 (-1.9)
60.8 28.5 28.8 95.0 85.8
NoBirdConcepts (-22.0) (-42.8) (-28.7) (-3.6) (-7.5)
6 Conclusion
In this work, we proposed Discover-then-Name CBM (DN-CBM), a novel CBM
approach that uses sparse autoencoders to discover and automatically name
concepts learnt by CLIP, and then use the learnt concept representations as a
conceptbottleneckandtrainlinearlayersforclassification.Wefindthatthissim-
pleapproachissurprisinglyeffectiveatyieldingsemanticallyconsistentconcepts
with appropriate names. Further, we find despite being task-agnostic, i.e. only
extracting and naming concepts once, our approach can yield performant and
interpretableCBMsacrossavarietyofdownstreamdatasets.Ourresultsfurther
corroborate the promise of sparse autoencoders for concept discovery. Training
a more ‘foundational’ sparse autoencoder with a much larger dataset (e.g. at
CLIP scale) and concept space dimensionality (with hundreds of thousands or
millions of concepts) to obtain even more general-purpose CBMs, particularly
for fine-grained classification, would be a fruitful area for future research.Task-Agnostic Concept Bottlenecks via Automated Concept Discovery 15
Acknowledgements
Funded in part by the Deutsche Forschungsgemeinschaft (DFG, German Re-
search Foundation) - GRK 2853/1 “Neuroexplicit Models of Language, Vision,
and Action” - project number 471607914.
References
1. Achtibat, R., Dreyer, M., Eisenbraun, I., Bosse, S., Wiegand, T., Samek, W.,
Lapuschkin, S.: From Attribution Maps to Human-Understandable Explanations
through Concept Relevance Propagation. Nature Machine Intelligence 5(9), 1006–
1019 (2023)
2. Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., Kim, B.: Sanity
Checks for Saliency Maps. In: NeurIPS. vol. 31 (2018)
3. Adebayo, J., Muelly, M., Abelson, H., Kim, B.: Post Hoc Explanations may be
Ineffective for Detecting Unknown Spurious Correlation. In: ICLR (2021)
4. Alukaev,D.,Kiselev,S.,Pershin,I.,Ibragimov,B.,Ivanov,V.,Kornaev,A.,Titov,
I.: Cross-Modal Conceptualization in Bottleneck Models. In: EMNLP (2023)
5. Bach, S., Binder, A., Montavon, G., Klauschen, F., Müller, K.R., Samek, W.: On
Pixel-wiseExplanationsforNon-LinearClassifierDecisionsbyLayer-wiseRelevance
Propagation. PloS one 10(7), e0130140 (2015)
6. Bau, D., Zhou, B., Khosla, A., Oliva, A., Torralba, A.: Network Dissection: Quan-
tifying Interpretability of Deep Visual Representations. In: CVPR. pp. 6541–6549
(2017)
7. Bhalla, U., Oesterling, A., Srinivas, S., Calmon, F.P., Lakkaraju, H.: Interpret-
ing CLIP with Sparse Linear Concept Embeddings (SpLiCE). arXiv preprint
arXiv:2402.10376 (2024)
8. Böhle, M., Fritz, M., Schiele, B.: B-cos Networks: Alignment is All We Need for
Interpretability. In: CVPR. pp. 10329–10338 (2022)
9. Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner,
N., Anil, C., Denison, C., Askell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer,
N.,Maxwell,T.,Joseph,N.,Hatfield-Dodds,Z.,Tamkin,A.,Nguyen,K.,McLean,
B., Burke, J.E., Hume, T., Carter, S., Henighan, T., Olah, C.: Towards Monose-
manticity: Decomposing Language Models With Dictionary Learning. Transformer
Circuits Thread (2023)
10. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:LanguageModelsareFew-Shot
Learners. In: NeurIPS. vol. 33, pp. 1877–1901 (2020)
11. Chauhan,K.,Tiwari,R.,Freyberg,J.,Shenoy,P.,Dvijotham,D.:InteractiveCon-
cept Bottleneck Models. In: AAAI (2023)
12. Chen,C.,Li,O.,Tao,D.,Barnett,A.,Rudin,C.,Su,J.K.:ThisLooksLikeThat:
Deep Learning for Interpretable Image Recognition. In: NeurIPS. vol. 32 (2019)
13. Chen, H., Yang, J., Vondrick, C., Mao, C.: Interpreting and Controlling Vision
FoundationModelsviaTextExplanations.arXivpreprintarXiv:2310.10591(2023)
14. Cunningham, H., Ewart, A., Riggs, L., Huben, R., Sharkey, L.: Sparse Autoen-
coders find Highly Interpretable Features in Language Models. arXiv preprint
arXiv:2309.08600 (2023)
15. Dani, M., Rio-Torto, I., Alaniz, S., Akata, Z.: DeViL: Decoding Vision Features
into Language. In: GCPR (2023)16 S. Rao et al.
16. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-
Scale Hierarchical Image Database. In: CVPR. pp. 248–255 (2009)
17. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:
AnImageisWorth16x16Words:TransformersforImageRecognitionatScale.In:
ICLR (2021)
18. Elhage,N.,Hume,T.,Olsson,C.,Schiefer,N.,Henighan,T.,Kravec,S.,Hatfield-
Dodds, Z., Lasenby, R., Drain, D., Chen, C., et al.: Toy Models of Superposition.
arXiv preprint arXiv:2209.10652 (2022)
19. Fel, T., Boutin, V., Béthune, L., Cadène, R., Moayeri, M., Andéol, L., Chalvidal,
M.,Serre,T.:AHolisticApproachtoUnifyingAutomaticConceptExtractionand
Concept Importance Estimation. In: NeurIPS. vol. 36 (2023)
20. Fel,T.,Picard,A.,Bethune,L.,Boissin,T.,Vigouroux,D.,Colin,J.,Cadène,R.,
Serre, T.: CRAFT: Concept Recursive Activation FacTorization for Explainability.
In: CVPR. pp. 2711–2721 (2023)
21. Ghorbani, A., Wexler, J., Zou, J.Y., Kim, B.: Towards Automatic Concept-Based
Explanations. In: NeurIPS. vol. 32 (2019)
22. Graziani, M., Nguyen, A.p., O’Mahony, L., Müller, H., Andrearczyk, V.: Concept
DiscoveryandDatasetExplorationwithSingularValueDecomposition.In:ICLRW
(2023)
23. He,K.,Zhang,X.,Ren,S.,Sun,J.:DeepResidualLearningforImageRecognition.
In: CVPR. pp. 770–778 (2016)
24. Hendricks, L.A., Akata, Z., Rohrbach, M., Donahue, J., Schiele, B., Darrell, T.:
Generating Visual Explanations. In: ECCV. pp. 3–19. Springer (2016)
25. Hernandez, E., Schwettmann, S., Bau, D., Bagashvili, T., Torralba, A., Andreas,
J.: Natural Language Descriptions of Deep Visual Features. In: ICLR (2022)
26. Jeyakumar,J.V.,Dickens,L.,Garcia,L.,Cheng,Y.H.,Echavarria,D.R.,Noor,J.,
Russo, A., Kaplan, L., Blasch, E., Srivastava, M.: Automatic Concept Extraction
forConceptBottleneck-basedVideoClassification.arXivpreprintarXiv:2206.10129
(2022)
27. Kazmierczak,R.,Berthier,E.,Frehse,G.,Franchi,G.:CLIP-QDA:AnExplainable
Concept Bottleneck Model. arXiv preprint arXiv:2312.00110 (2023)
28. Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., et al.: Inter-
pretabilityBeyondFeatureAttribution:QuantitativeTestingwithConceptActiva-
tion Vectors (TCAV). In: ICML. pp. 2668–2677 (2018)
29. Kim, E., Jung, D., Park, S., Kim, S., Yoon, S.: Probabilistic Concept Bottleneck
Models. In: ICML (2023)
30. Koh, P.W., Nguyen, T., Tang, Y.S., Mussmann, S., Pierson, E., Kim, B., Liang,
P.: Concept Bottleneck Models. In: ICML. pp. 5338–5348 (2020)
31. Krizhevsky,A.,Hinton,G.,etal.:LearningMultipleLayersofFeaturesfromTiny
Images. Technical Report, Computer Science Department, University of Toronto
(2009)
32. Lundberg,S.M.,Lee,S.I.:AUnifiedApproachtoInterpretingModelPredictions.
NeurIPS 30 (2017)
33. Margeloiu, A., Ashman, M., Bhatt, U., Chen, Y., Jamnik, M., Weller, A.: Do
Concept Bottleneck Models Learn as Intended? In: ICLRW (2021)
34. Menon,S.,Vondrick,C.:VisualClassificationviaDescriptionfromLargeLanguage
Models. In: ICLR (2023)
35. Moayeri, M., Rezaei, K., Sanjabi, M., Feizi, S.: Text-to-Concept (and Back) via
Cross-Model Alignment. In: ICML. pp. 25037–25060 (2023)Task-Agnostic Concept Bottlenecks via Automated Concept Discovery 17
36. Oikarinen,T.,Das,S.,Nguyen,L.M.,Weng,T.W.:Label-FreeConceptBottleneck
Models. In: ICLR (2023)
37. Oikarinen,T.,Weng,T.W.:CLIP-Dissect:AutomaticDescriptionofNeuronRep-
resentations in Deep Vision Networks. In: ICLR (2023)
38. O’Mahony, L., Andrearczyk, V., Müller, H., Graziani, M.: Disentangling Neuron
Representations with Concept Vectors. In: CVPRW. pp. 3769–3774 (2023)
39. Panousis, K.P., Chatzis, S.: DISCOVER: Making Vision Networks Interpretable
via Competition and Dissection. In: NeurIPS (2023)
40. Panousis, K.P., Ienco, D., Marcos, D.: Hierarchical Concept Discovery Models: A
Concept Pyramid Scheme. arXiv preprint arXiv:2310.02116 (2023)
41. Panousis, K.P., Ienco, D., Marcos, D.: Sparse Linear Concept Discovery Models.
In: ICCVW. pp. 2767–2771 (2023)
42. Patterson, G., Xu, C., Su, H., Hays, J.: The SUN Attribute Database: Beyond
Categories for Deeper Scene Understanding. IJCV 108(1-2), 59–81 (2014)
43. Petryk, S., Dunlap, L., Nasseri, K., Gonzalez, J., Darrell, T., Rohrbach, A.: On
GuidingVisualAttentionwithLanguageSpecification.In:CVPR.pp.18092–18102
(2022)
44. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning Transferable Visual Models
from Natural Language Supervision. In: ICML. pp. 8748–8763 (2021)
45. Rao, S., Böhle, M., Parchami-Araghi, A., Schiele, B.: Studying How to Efficiently
and Effectively Guide Models with Explanations. In: ICCV. pp. 1922–1933 (2023)
46. Rao, S., Böhle, M., Schiele, B.: Towards Better Understanding Attribution Meth-
ods. In: CVPR. pp. 10213–10222 (2022)
47. Ribeiro,M.T.,Singh,S.,Guestrin,C.:"WhyShouldITrustYou?"Explainingthe
Predictions of any Classifier. In: KDD. pp. 1135–1144 (2016)
48. Ross, A.S., Hughes, M.C., Doshi-Velez, F.: Right for the Right Reasons: Training
DifferentiableModelsbyConstrainingtheirExplanations.In:IJCAI.pp.2662–2670
(2017)
49. Roth, K., Kim, J.M., Koepke, A.S., Vinyals, O., Schmid, C., Akata, Z.: Waffling
AroundforPerformance:VisualClassificationwithRandomWordsandBroadCon-
cepts. In: ICCV. pp. 15746–15757 (2023)
50. Sagawa,S.,Koh,P.W.,Hashimoto,T.B.,Liang,P.:DistributionallyRobustNeural
Networks. In: ICLR (2020)
51. Sawada, Y., Nakamura, K.: Concept Bottleneck Model with Additional Unsuper-
vised Concepts. IEEE Access 10, 41758–41765 (2022)
52. Selvaraju,R.R.,Cogswell,M.,Das,A.,Vedantam,R.,Parikh,D.,Batra,D.:Grad-
CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.
In: ICCV. pp. 618–626 (2017)
53. Sharma,P.,Ding,N.,Goodman,S.,Soricut,R.:ConceptualCaptions:ACleaned,
Hypernymed, Image Alt-Text Dataset for Automatic Image Captioning. In: ACL.
pp. 2556–2565 (2018)
54. Shrikumar, A., Greenside, P., Kundaje, A.: Learning Important Features through
Propagating Activation Differences. In: ICML. pp. 3145–3153 (2017)
55. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic Attribution for Deep Metworks.
In: ICML. pp. 3319–3328. PMLR (2017)
56. Tewel, Y., Shalev, Y., Schwartz, I., Wolf, L.: ZeroCap: Zero-Shot Image-to-Text
Generation for Visual-Semantic Arithmetic. In: CVPR. pp. 17918–17928 (2022)
57. Xu,X.,Qin,Y.,Mi,L.,Wang,H.,Li,X.:Energy-BasedConceptBottleneckMod-
els:UnifyingPrediction,ConceptIntervention,andConditionalInterpretations.In:
ICLR (2024)18 S. Rao et al.
58. Yang, Y., Panagopoulou, A., Zhou, S., Jin, D., Callison-Burch, C., Yatskar, M.:
Language in a Bottle: Language Model Guided Concept Bottlenecks for Inter-
pretable Image Classification. In: CVPR. pp. 19187–19197 (2023)
59. Yuksekgonul, M., Wang, M., Zou, J.: Post-hoc Concept Bottleneck Models. In:
ICLR (2023)
60. Zhang, R., Madumal, P., Miller, T., Ehinger, K.A., Rubinstein, B.I.: Invertible
Concept-based Explanations for CNN Models with Non-Negative Concept Activa-
tion Vectors. In: AAAI. pp. 11682–11690 (2021)
61. Zhao, C., Qian, W., Shi, Y., Huai, M., Liu, N.: Automated Natural Lan-
guage Explanation of Deep Visual Neurons with Large Models. arXiv preprint
arXiv:2310.10708 (2023)
62. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: A 10 million
Image Database for Scene Recognition. IEEE TPAMI (2017)
63. Zhou, B., Sun, Y., Bau, D., Torralba, A.: Interpretable Basis Decomposition for
Visual Explanation. In: ECCV. pp. 119–134 (2018)Task-Agnostic Concept Bottlenecks via Automated Concept Discovery 19
Discover-then-Name: Task-Agnostic Concept
Bottlenecks via Automated Concept Discovery
Appendix
Table of Contents
(A) Limitations and Broader Impact ....................20
In this section, we discuss the limitations of our work and
broader impact.
(B) Implementation Details ...............................21
In this section, we provide training and implementation details
for our evaluations.
(B.1) Training SAEs and DN-CBM
(B.3) User Study
(B.2) Concept Accuracy on SUNAttributes
(B.4) Applying Interventions on DN-CBM
(C) Additional Quantitative Results .....................26
Inthissection,weprovideadditionalquantitativeresultsacross
backbones and datasets.
(C.1) DN-CBM Classification Performance
(C.2) Sparsity of DN-CBM Explanations
(D) Additional Qualitative Results .......................29
In this section, we provide additional qualitative results across
backbones and datasets.
(D.1) Task Agnosticity of Concepts
(D.2) Clustering Concept Strength Vectors
(D.3) Local Explanations from DN-CBM
(D.4) Global Explanation from DN-CBM20 S. Rao et al.
A Limitations and Broader Impact
In this work, we proposed DN-CBM, as task-agnostic approach to discovering
concepts from CLIP [S16] using sparse autoencoders (SAEs) and then using
them to construct a concept bottleneck model across downstream classification
tasks. We find that this simple approach shows promise that using a higher
dimensional concept space and training SAEs on much larger datasets than
CC3M [S20] would provide a richer and more diverse concept representation.
We already find that concept discovery from our SAEs using a relatively small
dataset like CC3M can lead to performant and interpretable CBMs for datasets
such as Places365 [S23], leveraging concepts related to objects, colours, shapes,
locations, and associations to reach decisions in an interpretable manner. How-
ever,theseconstituterelativelycoarseconcepts,andgiventhenatureandsizeof
the CC3M dataset, we do not find highly fine-grained concepts (e.g. bird body
parts as used in the CUB dataset [S21]), and mitigating this by scaling up to a
“foundational” SAE could be a promising direction for future research. Further,
one could also explore using better vocabularies for concept naming, e.g. that
are more tailored to the type of dataset used for SAE training. Further, note
that spurious correlations learnt by CLIP would likely persist in our concept
discovery, leading to concepts being activated when features correlated with the
concept are present (e.g. see concept ‘plane’ activated for ‘Airport terminal’ in
the example in Fig. D5, (c)). Mitigating this is orthogonal to this work and a
fruitful direction for future research. Overall, we find that despite being simple,
our approach is surprisingly effective in finding meaningful concepts, assigning
them human interpretable names, and constructing performant classifiers in an
efficient and task-agnostic manner, and can further the goal of building more
interpretable models.Task-Agnostic Concept Bottlenecks via Automated Concept Discovery 21
B Implementation Details
Inthissection,weprovideadditionaldetailsforeachofourevaluationsinSec.4
and Sec. 5.
B.1 Training SAEs and DN-CBM
In this section, we describe the details for training the sparse autoencoders
(SAEs) used for discovering and naming concepts (Secs. 3.1 and 3.2), and then
forconstructingourconceptbottleneckmodels(DN-CBM)(Sec.3.3).Weimple-
mentourcodeforallourexperimentsusingPyTorch[S13]anduseCaptum[S7]
for visualization.
Feature Extractors. We use CLIP [S16] ResNet-50 [S5], ViT-B/16 [S4], and
ViT-L/14 [S4] pre-trained feature extractors from the official repository3. We
use the output features (after pooling) for discovering concepts using sparse
autoencoders.
Datasets. We train our sparse autoencoders on the CC3M dataset [S20], and
then train linear probes for classification on ImageNet [S3], Places365 [S23],
CIFAR10 [S8], and CIFAR100 [S8]. To speed up training, we pre-compute fea-
turesandconceptstrengthsrespectivelybeforetraining,anddonotperformany
augmentations.Performingsuchaugmentations(e.g.randomcropping,flipping,
etc.) would likely improve our classification performance further.
Training Sparse Autoencoders (SAEs). We train sparse autoencoders fol-
lowingthesetupof[S1],usingtheimplementationof[S2]4 (v1.3.0).Wetrainfor
200 epochs, and resample every 10 epochs. We perform hyperparameter sweeps
using a heldout set over the learning rate 1 10−5,5 10−5,1 10−4,5
{ × × × ×
10−4,1 10−3 ,L sparsitycoefficient(λ ) 3 10−5,1.5 10−4,3 10−4,1.5
1 1
× } { × × × ×
10−3,3 10−3 ,andexpansionfactors 2,4,8 .FortheCLIPResNet-50model,
× } { }
we choose the SAE with learning rate 5 10−4, L sparsity 3 10−5, expan-
1
× ×
sion factor 8 based on ImageNet zeroshot performance on reconstructions. See
also Fig. B1 for an evaluation of the impact of λ on reconstruction error and
1
sparsity.
Training Linear Probes. We train linear probes without bias on the learned
concept representations using the Adam optimizer [S6]. In addition to using the
cross entropy loss for classification, we apply a L sparsity constraint on the
1
weights, and train for 200 epochs. We perform hyperparameter sweeps using a
heldout set over the learning rate 1 10−4,1 10−3,1 10−2 , L sparsity
1
{ × × × }
coefficient (λ ) 0,0.1,1 . We train such probes over all trained SAEs and pick
2
{ }
the probes with the best top-1 validation accuracy for each dataset.
B.2 Concept Accuracy on SUNAttributes
In this section, we describe the process for quantitatively evaluating concept
accuracyontheSUNAttributesdataset[S14],asdiscussedinSec.4.1.Following
3 https://github.com/openai/CLIP
4 https://github.com/ai-safety-foundation/sparse_autoencoder22 S. Rao et al.
Fig.B1: Impact of sparsity parameter λ (Eq. (2)) on reconstruction. We
1
plot the reconstruction error versues the number of concepts used for various values
of the L sparsity constraint parameter λ . We find that higher λ leads to sparser
1 1 1
concept representations, but at the cost of a higher representation error.
[40], to quantify the agreement, we estimate the Jaccard index between the
ground truth (GT) concepts and the predicted concepts per image, which is
then averaged over all the images. The jaccard index is given by
M
Jaccard(GT, Predicted)= 11
M +M +M
11 10 01
where, M = true positives ; M = false negatives; M = false positives.
11 10 01
To evaluate the concept accuracy, we use the ground truth concepts of the
SUNAttributesdataset.Thereare102GTconceptsforSUNAttributes,whichis
of much smaller dimension than our SAE latent space. To bring the SAE latent
dimensiondowntothedimensionofGTconcepts,weclusterthedictionaryvec-
tors with the same concept name and treat it as a ‘compound’ node. From each
cluster, we filter the unactivated nodes and nodes with poor alignment between
their dictionary vector and the associated text embedding. The compound node
is then associated with the maximum concept strength of all the constituent
individual nodes. We then use a heldout set to dynamically find a per concept
threshold to binarize the concept strengths. For the CLIP retrieval baseline, we
follow the same procedure on the cosine similarities of the nodes with the text
embeddings.
B.3 User Study
Inthissection,wedescribethedetailsofouruserstudytoquantitativelymeasure
theconsistencyandaccuracyofourdiscoveredandnamedconcepts,asdiscussed
in Sec. 4.1.
Methods.WeevaluateonnodesfromourSAE(Sec.B.1)fortheCLIPResNet-
50model,assignedwithnamesfromournamingscheme(Sec.3.2).Asabaseline,
weusenodesfromtheimagefeaturevectorofthesameCLIPResNet-50model,
assigned names using CLIP-Dissect [S11] with the same vocabulary.
Selecting Nodes. To obtain a holistic view of the consistency and name ac-
curacy of the node concepts, we sort nodes based on the alignment with the
text embedding vector of the name assigned to them. Specifically, for the SAEs,
we sort based on the cosine similarity between the dictionary vector and the
text embedding, and for the CLIP features, we sort based on the CLIP-Dissect
similarity. We then uniformly at random sample nodes from three bins, whereTask-Agnostic Concept Bottlenecks via Automated Concept Discovery 23
Fig.B2:Examplesprovidedatthebeginningoftheuserstudy.Threeexamples
(two shown here) were provided at the beginning of the survey to help partipants
familiarize themselves with the task.
the alignment is the highest, intermediate, and lowest. Specifically, for our SAE
with 8192 nodes, we sample five nodes each from the top 2000, intermediate
4192, and bottom 2000 nodes. For the CLIP feature vector with 1024 nodes, we
scaledownandsamplethreenodeseachfromthetop250,intermediate724,and
bottom 250 nodes.
Question Structure.Foreachnode,weextractthetopfouractivatingimages
fromthreediversedatasets–ImageNet[S3],Places365[S23],andCC3M[S20]–
andcreateagridoftwelveimages.Togetherwiththeimage,weprovidetheword
that corresponds to the name assigned to the node. We then ask two questions
to evaluate: (1) if there is a semantically consistent and human interpretable
concept common among the top activating images, and (2) if the word assigned
accurately depicts such a concept, if any. For each question, participants are
invited to rate on a five point scale, from 1 (“Strongly Disagree”) to 5 (“Strongly
Agree”). For the second question, a “Not Applicable” option is also provided to
account for the case that no common concept may be found by the participant.
Fig. B3 shows an exmample of a question.
Survey Structure. We randomly order the 24 nodes (15 from our SAE, and 9
fromCLIP-Dissect)inthesurvey.Tohelpparticipants,wealsoprovideexamples
at the beginning of the survey, as shown in Fig. B2 We published the survey
internally, and received 22 responses.24 S. Rao et al.
Fig.B3: An example of a question in the user study.Foreachnode,weprovide
toptwelveactivatingimagesacrossthreedatasetsandthenameassignedtothenode,
and ask participants to rate for semantic consistency and name accuracy.Task-Agnostic Concept Bottlenecks via Automated Concept Discovery 25
B.4 Applying Interventions on DN-CBM
In this section, we describe the details of the evaluation by intervening on the
conceptbottleneckusingtheWaterbirds-100[S15,S19]datasetonourDN-CBM,
as discussed in Sec. 5.3.
Training Setup. We use our SAE for the CLIP ResNet-50 model, and train a
linear probe for the binary classification task of the Waterbirds-100 dataset (i.e.
Landbird versus Waterbird) to obtain a concept bottleneck model. Specifically,
we use a learning rate of 0.1, L sparsity coefficient of 10, and train for 200
1
epochs.Tofurtherimprovesparsity,weprunetheweightsforeachclass,leaving
only the five largest weights and replacing the rest with zeroes. This model
obtains an accuracy of 82.8% (see Tab. 2, ‘Before Interventions’).
Group-wise Evaluation. Following [S15,S18,S19], we also report the perfor-
manceatagroup-wiselevel,i.e.bothforthegroupsfoundintraining(‘Landbird
onLand’,‘WaterbirdonWater’)andnewgroupsonlyfoundinthetestset(also
known as the ‘Worst Groups’, i.e. ‘Landbird on Water’, ‘Waterbird on Land’).
Interventions.Foreachclass,welookattheassignednamesofthefiveconcepts
andmanuallyclassifythemas‘BirdConcept’and‘NotBirdConcept’;forthefull
list,seeTab.B1.Wefindthatthebirdconceptstypicallycorrespondtoexamples
of the type of bird (e.g. ‘sparrow’ for ‘Landbird’, ‘gull’ for ‘Waterbird’), which
we attribute to the granularity of examples in the CC3M dataset used for the
training the SAE and the granularity of the vocabulary. We perform two sets
of interventions: (1) keeping only the bird concepts, and (2) removing only the
bird concepts. For a full discussion on the results, see Sec. 5.3.
Table B1: Set of concepts in our evaluation for performing interventions on
DN-CBMs. For each class, we manually classify each of the five concepts as being a
bird or non-bird concept, for applying appropriate interventions.
Class Bird Concepts Non-Bird Concepts
Landbird sparrow, parrot, crow forest, clic
Waterbird gull, ducks landing, beach, canoeing26 S. Rao et al.
C Additional Quantitative Results
Inthissection,weprovideadditionalquantitativeresults.InSec.C.1weprovide
results for the performance of DN-CBM on additional backbones. In Sec. C.2,
wediscussthetradeoffbetweenaccuracyandsparsityofDN-CBMexplanations.
C.1 DN-CBM Classification Performance
Inthissection,weprovidefullquantitativeresultsacrossthethreebackbones,i.e.
CLIP [S16] ResNet-50 [S5], CLIP ViT-B/16 [S4], and CLIP ViT-L/14 [S4]. The
classification accuracies across the four datasets, i.e., ImageNet [S3], Places365
[S23], CIFAR10 [S8], and CIFAR100 [S8], for our DN-CBM and the baseline
methods for each of these backbones can be found in Tabs. C1 to C3.
Broadly,wefindthatourDN-CBMoutperformsallthebaselinesandalmost
completely bridges the gap in accuracy with linear probes
For our DN-CBM, in addition to the proposed task-agnostic setting, we also
additionally report accuracies by using the best configuration across SAE con-
figurations, based on classification performance on a held out validation set,
and call this DN-CBM . In other words, the performance reported under DN-
T
CBM constitutes the setting when our concept discovery is not task-agnostic,
T
i.e., when a separate SAE is selected for each dataset. We find, as expected,
that DN-CBM slightly outperforms the task-agnostic DN-CBM setting. How-
T
ever, interestingly, the performance difference is very small, showing that our
task-agnostic approach, while being more general, also yields highly performant
classifiers.
TableC1:Classificationaccuracy(%)onCLIPResNet-50.‘*’indicatesresults
reportedfortherespectivebaselines,andlinearprobeandzero-shotperformanceisas
reportedby[S16].‘-’indicatesconfigurationswhereresultswerenotreportedorwhich
couldnotbererunsinceconceptsetswerenotgeneratedbythebaselinemethods(e.g.
LaBowithPlaces365).DN-CBM referstoourmethod,whereinwechoosetheSAE
T
which achieves best validation accuracy depending on the downstream dataset.
CLIP ResNet-50
Model
ImageNet Places365 CIFAR10 CIFAR100
Linear Probe 73.3* 53.4 88.7* 70.3*
Zero Shot 59.6* 38.7 75.6* 41.6*
LF-CBM [S10] 67.5 49.0 86.4* 65.1*
LaBo [S22] 68.9 - 87.9* 69.1*
CDM [S12] 72.2* 52.7* 86.5* 67.6*
DCLIP [S9] 59.6 37.9 - -
DN-CBM (Ours) 72.9 53.5 87.6 67.5
DN-CBM (Ours) 73.2 53.9 88.6 69.2
TTask-Agnostic Concept Bottlenecks via Automated Concept Discovery 27
Table C2: Classification accuracy (%) on CLIP ViT-B/16.‘*’indicatesresults
reportedfortherespectivebaselines,andlinearprobeandzero-shotperformanceisas
reportedby[S16].‘-’indicatesconfigurationswhereresultswerenotreportedorwhich
couldnotbererunsinceconceptsetswerenotgeneratedbythebaselinemethods(e.g.
LaBowithPlaces365).DN-CBM referstoourmethod,whereinwechoosetheSAE
T
which achieves best validation accuracy depending on the downstream dataset.
CLIP ViT-B/16
Model
ImageNet Places365 CIFAR10 CIFAR100
Linear Probe 80.2* 55.1 96.2* 83.1*
Zero Shot 68.6* 41.2 91.6* 68.7*
LF-CBM [S10] 75.4 50.6 94.6 77.4
LaBo [S22] 78.9 - 95.7 81.2
CDM [S12] 79.3* 52.6* 95.3* 80.5*
DCLIP [S9] 68.0* 40.3* - -
DN-CBM (Ours) 79.5 55.1 96.0 82.1
DN-CBM (Ours) 79.5 55.1 95.7 82.1
T
Table C3: Classification accuracy (%) on CLIP ViT-L/14.‘*’indicatesresults
reported for the respective baselines. LF-CBM is not reported here as they take ViT-
B/16astheteachermodelandhencewedonotuseitwithaViT-L/14student.Linear
probe and zero-shot performance is as reported by [S16]. ‘-’ indicates configurations
where results were not reported or which could not be rerun since concept sets were
not generated by the baseline methods (e.g. LaBo with Places365). DN-CBM refers
T
to our method, where in we choose the SAE which achieves best validation accuracy
depending on the downstream dataset.
CLIP ViT-L/14
Model
ImageNet Places365 CIFAR10 CIFAR100
Linear Probe 83.9* 55.6 98.0* 87.5*
Zero Shot 75.3* 41.4 96.2* 77.9*
LaBo [S22] 84.0* - 97.8* 86.0*
CDM [S12] 83.4 55.2 98.0 86.4
DCLIP [S9] 75.0* 40.6* - -
DN-CBM (Ours) 83.6 55.6 98.1 86.0
DN-CBM (Ours) 83.6 55.6 97.9 87.4
T28 S. Rao et al.
ImageNet Places365
200 200
150 150
100 100
50 50
0 0
0.00 0.20 0.40 0.60 0.43 0.45 0.48 0.50 0.53
Accuracy Accuracy
Fig.C1: Accuracy vs. Sparsity Trade-off. For each dataset, we show the model
accuracy (x-axis) versus the average number of concepts needed to reach 0.9 of the
total logit value (y-axis). We find that while there exists a trade-off between accuracy
andsparsity,highlysparsedecisionscanbeachievedbyaverysmalldropinaccuracy.
C.2 Sparsity of DN-CBM Explanations
Inadditiontobeingaccurateandgeneral,onealsodesiresthattheexplanations
are sparse, since explanations with a large number of concepts are not very
human interpretable [S17]. We explore the relationship between sparsity and
accuracy of DN-CBM in Fig. C1 by evaluating models across SAE and classifier
hyperparameters on both metrics. To measure sparsity, we count the average
number of concepts required to reach 0.9 fraction of the original logit value for
eachimageacrossthedatasetandfind,quitenaturally,thatthereexistsatrade-
offbetweenthetwometrics acrossdatasets.However, interestingly,wefindthat
one can obtain models that require very few concepts per decision on average
by sacrificing only a small amount of accuracy.
desU
stpecnoC
fo
rebmuN
desU
stpecnoC
fo
rebmuNTask-Agnostic Concept Bottlenecks via Automated Concept Discovery 29
D Additional Qualitative Results
Inthissection,weprovideadditionalqualitativeresults.InSec.D.1,weprovide
additionalqualitativeexamplesoftaskagnosticityofourdiscoveredconcepts.In
Sec. D.2, we provide additional examples of clustering concept strength vectors.
In Sec. D.3 and Sec. D.4, we provide examples of local and global explanations
from our DN-CBM and discuss our findings.
D.1 Task Agnosticity of Concepts
Inthissection,weprovidefurtherqualitativeevidenceoftaskagnosticityofour
discoveredconceptsbyshowingexamplesofdiscoveredconceptsandtopactivat-
ing images for each concept from ImageNet [S3], Places365 [S23], CIFAR10 [S8]
and CIFAR100 [S8], using sparse autoencoders trained on each of the three vi-
sion backbones, i.e. CLIP [S16] ResNet-50 [S5] (Fig. D1), CLIP ViT-B/16 [S4]
(Fig. D2), and CLIP ViT-L/14 [S4] (Fig. D3).
Overall (Figs. D1 to D3), we find that across backbones, the discovered con-
cepts are highly semantically consistent and map well to the name that is auto-
matically assigned to them. The discovered concepts greatly vary in complexity,
from simple concepts such as colours (e.g. ‘maroon’, Fig. D2) to complex con-
cepts such as ‘conversation’ (Fig. D3). The images activating on each concept
are highly visually diverse, while still being semantically related to the concept.
For a more detailed discussion, please refer to the captions of Figs. D1 to D3.
D.2 Clustering Concept Strength Vectors
In this section, we provide additional qualitative evidence that our sparse au-
toencoder (SAE) is able to learn concepts which are semantically consistent, for
which we cluster the concept strengths in the SAE latent space and find that
the concept strengths are able segregate images well formed groups. Overall
(Fig. D4), we find that across backbones, the discovered clusters contain images
which are highly similar to each other visually. (Fig. D4) shows the clusters
formed for the ImageNet dataset. For a more detailed discussion, please refer to
the captions of (Fig. D4).30 S. Rao et al.
reflection snowy column bus
Index5629 Index2184 Index6946 Index6708
ImageNet
CIFAR10
CIFAR100
Places365
framed arch suits spheres
Index602 Index7115 Index2574 Index1934
ImageNet
CIFAR10
CIFAR100
Places365
Fig.D1: Task-agnosticity of concept extraction using CLIP ResNet-50. We
show examples of named concepts (blocks) and top images activating them from four
datasets (rows).
(1) We find overall that the images activating the concept are highly consistent with
the concept name across datasets and are from diverse scenarios. For example, the
concept ‘column’ (top row, middle right) is activated by images from a diverse set of
classes, while visually sharing the common feature of being a long and narrow object,
includinganunderwateranimal(row1,col3intheblock),ananimalhead(row2,col
4), a rocket (row 1, col 2), and an elongated version of a truck (row 2, col 2).
(2) Images activating a concept are highly visually diverse. For example, the concept
‘bus’ (top right) contains images depicting both exterior as well as interior views of
buses. Similarly, the concept ‘arch’ (bottom row, middle left), is activated by images
fromdifferentclassessuchasanimals,gates,bridges,rocks,allsimilarintheaspectthat
theirshaperesemblesanarch;andtheconcept‘spheres’(bottomright)isactivatedby
images of apples, snails, as well as the birds which look round in shape. The concept
‘suits’(bottomrow,middleright),interestinglyandaptly,isalsoactivatedbyanimage
of a dog wearing a tie.Task-Agnostic Concept Bottlenecks via Automated Concept Discovery 31
autumn doors bed maroon
Index1332 Index704 Index2061 Index1840
ImageNet
CIFAR10
CIFAR100
Places365
fences pupil dining tower
Index1526 Index3955 Index1149 Index736
ImageNet
CIFAR10
CIFAR100
Places365
Fig.D2: Task-agnosticity of concept extraction using CLIP ViT-B/16. We
show examples of named concepts (blocks) and top images activating them from four
datasets (rows).
(1) We find overall that the images activating the concept are highly consistent with
the concept name across datasets and are from diverse scenarios. For example, the
concept‘autumn’(topleft)includesimagesoftreeswithautumncolours,fallenleaves,
and pumpkins which are associated with autumn and Halloween.
(2) Concepts range from simple concepts (e.g. colours, such as ‘marooon’ in the top
right) to complex ones (e.g. ‘dining’, bottom row, middle right).
(3)Imagesactivatingaconceptarehighlyvisuallydiverse.Forexample,theconcepts
‘door’(toprow,middleleft)and‘dining’(bottomrow,middleright)includeimagesat
ofdoorsanddiningtablesrespectivelyatdifferentscales,viewpoints,ofdiversestyles
andcolours,andindiversescenarios(e.g.indooraswellasoutdoor).Similarly,thecon-
cept‘tower’(bottomright)includesadiversesetofimagesthatlookliketowers,such
aslighthouses,rockets,skyscrapers,andwatertowers;andtheconcept‘pupil’(bottom
row,middleleft)containsexamplesofeyesofadiversesetofspecies,includinghumans,
mammals, and insects, and also associations, such as images of an ophthalmologist’s
office.
(4)Whenimagesmatchingaparticularconceptarescarceinadataset(e.g.CIFAR10,
whichhasonlytenclassesandlikelyafarsmallersetofvisibleconcepts),thetopcon-
ceptsstillappeartobegoodapproximationsofandarereasonablysemanticallyaligned
tothenamedconcept.Forexample,theconcept‘tower’(bottomright)includesexam-
plesoffrontviewsofanostrichheadandanairplane,bothofwhichvisuallyresemble
towers. Similarly, the concept ‘pupil’ (bottom row, middle left) includes examples of
sunflowers, the top view of a flower pot, and a wheel, which have a similar shape and
appearance as eye pupils. This provides further evidence that the discovered concepts
activate for semantically meaningful and similar visual features.32 S. Rao et al.
bros aerial conversation circular
Index4053 Index5885 Index5944 Index2934
ImageNet
CIFAR10
CIFAR100
Places365
rainbow bridges underwater bw
Index5230 Index2455 Index2766 Index24
ImageNet
CIFAR10
CIFAR100
Places365
Fig.D3: Task-agnosticity of concept extraction using CLIP ViT-L/14. We
show examples of named concepts (blocks) and top images activating them from four
datasets (rows). We find overall that the images activating the concept are highly
consistent with the concept name across datasets and are from diverse scenarios.
(1) The concept ‘bros’ (top left) is activated by the presence of two entities in the
image, from humans to to dogs, rats, and cats.
(2) The concept ‘rainbow’ (bottom left) is activated by highly colourful birds, and an
image of bell peppers which are of different colors that are present in a rainbow. It
is also activated by a rainbow-coloured flag, by bio-luminescent fish, and drawings of
fish.
(3) Concepts representing viewpoints such as ‘aerial’ (top row, middle left) is also
discovered by the model, which shows farming lands, highways, water bodies, and
landmass.
(4)Theconcept‘conversation’(toprow,middleright),isactivatedbypeoplepresentin
acrowdfacingeachother,whicharetypicalimagesofpeopleengagedinconversations.
Italsoisactivatedbyphonesandmegaphoneswhicharetoolsforhavingconversations,
and objects which look visually similar when good matches may not be present in the
dataset, such as an image of an exhaust pipe from CIFAR10.Task-Agnostic Concept Bottlenecks via Automated Concept Discovery 33
SampledImagesfromCluster SampledImagesfromCluster SampledImagesfromCluster
crow fabrics sailing
ibis quilts tubing
female handmade tubing
chickens pillow kayaking
eagle crochet yacht
birds knit yacht
(a) CLIPResNet-50
SampledImagesfromCluster SampledImagesfromCluster SampledImagesfromCluster
after rosie snowboard
infants glasses tackling
cat tao festival
rabbits infant debut
bowie puppies bikes
dog dog qualifier
(b) CLIPViT-B/16
SampledImagesfromCluster SampledImagesfromCluster SampledImagesfromCluster
volvo curly clouds
penguins locus pile
universal beard wildlife
underwater joanna visitor
fishes pixel jaguar
wildlife dog birds
(c) CLIPViT-L/14
Fig.D4: Extracting meaningful clusters from concept strength vectors on
the ImageNet dataset. We perform K-Means clustering over concept activation
vectors on the ImageNet dataset to evaluate the semantic consistency of these latent
representations. We showa randomsubset ofclusters:eachblock representsa cluster,
and we show top concepts from the cluster centroid and randomly selected images
assignedtothecluster.Wefindthathighlysemanticallyconsistentclustersofconcepts
emerge.
(a)Thetoprow,showsclusterswhicharewellsegregatedintodifferentsemantics,the
first cluster shows close-up pictures of eagles, the second one shows knitted garments
and the third cluster represents ships sailing in the ocean.
(b) Similarly, in the second row, the first cluster represents infant rabbits, the second
cluster corresponds to infant dogs and the third one represents a person biking.
(c) The third row represents clusters corresponding to sea animal underwater, dogs
with a specific texture, and birds flying in the sky respectively.
stpecnoCpoT
stpecnoCpoT
stpecnoCpoT34 S. Rao et al.
D.3 Local Explanations from DN-CBM
In this section, we provide additional examples of local explanations of our DN-
CBM on the Places365 and ImageNet datasets, using all three vision backbones
(CLIP ResNet-50, CLIP ViT-B/16, and CLIP ViT-L/14). In Fig. D5, we show
examplesofPlaces365imagesmisclassifiedbythemodel,andanalyzeanddiscuss
themisclassificationsbasedontheprovidedexplanations.InFig.D6,weprovide
additionalexamplesofcorrectclassificationsonPlaces365.Finally,inFig.D7,we
provide examples of local explanations from ImageNet, including a misclassified
example.
Overall, we find that the provided explanations typically describe the input
imagewell,arehighlydiverse,andcanevenhelpbetterunderstandmisclassified
decisions by the model. For full details, please refer to the captions of Figs. D5
to D7.
D.4 Global Explanations from DN-CBM
Inthissection,weprovideadditionalqualitativeexamplesofglobalexplanations
on the Places365 dataset from our DN-CBM. This figure contains explanations
as concept names which contribute the most to the class. Our method is able
to explain the class with concepts which are highly relevant (Fig. D8) to it.
Overall, we find that across backbones, the classes are explained well with the
top-contributing concepts. For detailed discussion, please refer to the captions
of (Fig. D8).Task-Agnostic Concept Bottlenecks via Automated Concept Discovery 35
Predicted:Autoshowroom,GT:Raceway MostStronglyContributingConcepts Predicted:Fieldcultivated,GT:Orchard MostStronglyContributingConcepts
chevelle +1.61 farming +2.08
garage +0.98 volunteers +1.35
corvette +0.60 fields +0.71
mustang +0.55 leaves +0.61
lowered +0.52 plants +0.57
(a) (b)
Predicted:Airportterminal,GT:Busstationindoor MostStronglyContributingConcepts Predicted:Lakenatural,GT:Lagoon MostStronglyContributingConcepts
malls +1.85 canoeing +2.08
luggage +1.46
reflection +0.93
platform +0.87
lake +0.34
plane +0.66
exhibitor+0.48 clic +0.30
(c) (d)
Fig.D5: Examples of misclassifications on Places365 by DN-CBM. We show
examples of misclassified images from the Places365 dataset using a CLIP ResNet-50
backbone along with the top concepts contributing to the decision.
(a) An image of class ‘Raceway’ is misclassified as ‘Auto showroom’, classes which
both likely share similar car related concepts. The top five concepts in the decision
include three car model names (‘chevelle’, ‘corvette’, and ‘mustang’) which might be
correlated to cars found in images of auto showrooms. The image also prominently
shows a car with an open hood, likely causing the concepts of ‘garage’ and ‘lowered’
to be activated and causing the misclassification.
(b) An image of class ‘Orchard’ is misclassified as ‘Field cultivated’. In this example,
the image does appear to be visually more similar to the predicted class, with the top
concepts for the decision all being visually present in the image.
(c)Animageofclass‘Busstationindoor’ismisclassifiedas‘Airportterminal’.Again,
this example visually appears highly similar to the predicted class. However, the top
concepts also include those that are not present in the image (e.g. ‘plane’), suggesting
thatconceptsmayalsobespuriouslycorrelatedwithoneanother.Thismightevenbe
a consequence of CLIP having learnt such correlations, and investigating this further
andfindingwaystomitigatethemwouldbeaninterestingdirectionforfutureresearch.
(d)Animageofclass‘Lagoon’misclassifiedas‘Lakenatural’.Thisimagealsovisually
appearsbetteralignedwiththepredictedclass.Italsoservesasanotherexamplewhere
conceptsappeartobedetectedduetospuriouscorrelations—thetopdetectedconcept
is ‘canoeing’, despite it not being present in the image. It is probable that instances
of ‘canoeing’ are typically found in images of lakes, leading to this correlation being
learnt by CLIP or our DN-CBM.36 S. Rao et al.
PredictedClass:Trench MostStronglyContributingConcepts PredictedClass:Boathouse MostStronglyContributingConcepts
pathway +0.89 harbor +2.13
rocks +0.89
farmhouse +2.09
mud +0.88
sailing +1.71
forest +0.58
infantry +0.50 canoeing +0.87
(a) (b)
PredictedClass:Footballfield MostStronglyContributingConcepts PredictedClass:Foodcourt MostStronglyContributingConcepts
nfl +3.99 malls +1.32
soccer +1.44 restaurant +1.05
stadium +1.44 shoppers +0.66
districts +1.12 dining +0.40
crowd +0.88 coffee +0.38
(c) (d)
Fig.D6: Examples of local explanations on Places365. We show examples of
correctlyclassifiedimagesfromthePlaces365datasetalongwiththetopconceptscon-
tributing to the decision using CLIP ViT-B/16 (top) and CLIP ViT-L/14 (bottom)
vision backbones. We find that our DN-CBM classifies based on a diverse set of con-
cepts present in the image, including objects, similar features, higher level concepts,
andthingsassociatedwiththeclass(e.g.similarlocations),thusaidinginterpretability.
(a) An example of class ‘Trench’. The top concepts in the decision include elements
thatarevisuallypresentintheimage(e.g.‘mud’,‘forest’),elementsthatlookvisually
similar to features in the image (e.g.‘rocks’, ‘pathway’),and concepts associated with
the class (e.g. ‘infantry’).
(b) An example of class ‘Boathouse’. The top two concepts include elements similar
to those visually in the image, i.e. ‘harbor’, which is depicted via the visible pier, and
‘farmhouse’, which is visually similar to the depicted boathouse. The other top con-
cepts include actions depicted in the image, i.e. ‘sailing’ and ‘canoeing’.
(c) An example of class ‘Football field’. The top concepts are generally relevant to
the class being predicted, and include features visibly present in the image, such as
‘stadium’ and ‘crowd’. The top concept, ‘nfl’, also accurately describes the sport. In-
terestingly, the concept ‘soccer’ is also activated, which could be due to (potentially
spurious)associationslearntbythemodelbetweenthesportsofAmericanfootballand
soccer,whichisalsocommonlyreferredtoasfootball.Theconcept‘districts’,however
seems to have been activated incorrectly and does not appear to relate to any feature
have any association with the image.
(d)Anexampleofclass‘Foodcourt’.Thetopconceptsincluderelevantlocations(i.e.
‘malls’), associated locations that are visually similar (i.e. ‘restaurant’), actions (i.e.
‘dining’), and visible elements (i.e. ‘shoppers’ and ‘coffee’).Task-Agnostic Concept Bottlenecks via Automated Concept Discovery 37
PredictedClass:Lemon MostStronglyContributingConcepts PredictedClass:Drinkpitcher MostStronglyContributingConcepts
yellow +5.98 pottery +1.67
fruits +3.98 mug +1.55
balloons +2.73 coffee +1.28
alcoholic +2.72 perfumes +1.12
shakes +2.47 jaw +0.85
(a) (b)
PredictedClass:Academicgown MostStronglyContributingConcepts PredictedClass:Brassmemorialplaque MostStronglyContributingConcepts
graduating +2.18 funeral +6.30
universities +1.63 statue +2.49
black +1.12 truth +2.39
commencement +1.05 award +1.96
suits +0.83 sign +1.73
(c) (d)
Fig.D7: Examples of local explanations on ImageNet. We show examples of
correctly classified images from the ImageNet dataset along with the top concepts
contributing to the decision using the three vision backbones. We find that our DN-
CBMclassifiesbasedonadiversesetofconceptspresentintheimage,includingobjects,
similarfeatures,higherlevelconcepts,andthingsassociatedwiththeclass(e.g.similar
locations), thus aiding interpretability.
(a)Anexampleofclass‘Waterjug’misclassifiedasclass‘Lemon’.Theimageandthe
top concepts appear to clearly explain the misclassification: the lemon, being highly
salientintheimage,activates‘yellow’(colour),‘fruits’(category),andballoons(similar
shape) as the top concepts. The other concepts (i.e. ‘alcoholic’, ‘shakes’) might be
associated to drinks typically found in such jugs, but are outweighed by the presence
of the top three concepts. This shows that the explanations could be used to better
understand misclassifications.
(b) An example of class ‘Drink pitcher’. The top concepts include visually similar
elements, such as ‘pottery’ and ‘mug’. Interestingly, one of the top concepts is also
‘jaw’,whichisvisuallysimilartotheopeningsofthepitchersshown.Conceptsinvolving
contents that may be present in such pitchers are also activated, such as ‘coffee’ and
‘perfumes’.
(c)Anexampleofclass‘Academicgown’.Thetopconceptsarehighlydiverse,including
thecolourofthegownsintheimage(i.e.‘black’),whatthegownslookvisuallysimilar
to(i.e.‘suits’),thelocationinthebackground,thatistypicalforsuchceremonialgowns
(i.e. ‘universities’), and concepts related to the event where such gowns are worn (i.e.
‘graduating’ and ‘commencement’).
(d) An example of class ‘Brass memorial plaque’. The top concepts include what
plaques represent (i.e. ‘sign’) and locations where plaques are typically found (i.e.
‘statue’,‘award’).Thehighestactivatingconcept,‘funeral’,couldbeassociatedwiththe
semantic closeness specifically of memorial plaques to deceased individuals. However,
the concept ‘truth’ appears to have been detected incorrectly.38 S. Rao et al.
Class:Cleanroom Class:Cliff Class:Closet
milling rocks bedroom
dialysis canyon bedroom
utilities nevada enclosures
guests gibraltar doors
dialysis montserrat cleaned
scientist beach dressing
astronomy sedona finished
polymerase capri dorm
manufacturing cliffs shelves
(a) CLIPResNet-50
Class:Pizzeria Class:Playground Class:Playroom
wheel soccer settlers
pizza gym birthdays
motherboards classrooms rainbow
alcoholic swings after
desserts infant ironing
raphael garden infant
siena benches diecast
restaurant ladder bedroom
ramps park pooh
(b) CLIPViT-B/16
Class:Harbor Class:Hardwarestore Class:Hayfield
strait sears emergence
canterbury restaurant clouds
industries volunteers sunset
massachusetts spear chattanooga
pittsburgh vendor pittsburgh
canoeing newsroom yosemite
buildings servicing cows
beach assortment mountains
harbor fasteners northamptonshire
(c) CLIPViT-L/14
Fig.D8: Class-wise explanations using our DN-CBM. We show examples of
classes from the Places365 dataset with the top contributing concepts. For each class
(block),weshowrandomexamplesofimagesbelongingtothatclassandselectconcepts
withthehighestaveragecontributionacrossallimagesfromtheclassinthevalidation
set.
(1) We find that our approach learns classifiers that use concepts highly semantically
relatedtoeachclass.Fore.g.,theclass‘cliff’isassociatedwithconceptssuchasrocks,
canyon, beach and also with places like gibraltar, montserrat which could due to the
association the CLIP model has learnt from the training data.
(2) Note that the class name itself often appears in the list of top concepts (e.g. con-
cept ‘cliffs’ for class ‘cliff’, and concept ‘harbor’ for class ‘harbor’) as we use a generic
vocabularywhichconsistsofthemostusedEnglishwords.AsweassumeforourCBM
methodthatwedonothaveaccesstotheclasslabelsapriori,wedonotfilteroutthese
words, so as we are working with the last layer representation, the model sometimes
fires the concept corresponding to the class name to explain the same class.
(3) The ‘Playroom’ class is explained by concepts such as ‘rainbow’ which is usually
associatedwithcolors,andtheconcept‘pooh’,’birthdays’and‘infant’whichareclosely
associated with a child’s ‘Playroom’. The class ‘Playground’ is well described by con-
ceptssuchas‘ladder’,‘benches’,‘swings’,‘infant’,‘soccer’whichareusuallypresentin
achild’splayground,aswellasconceptslike‘park’,‘garden’aresimilarlookingplaces
andtheconcept‘gym’isactivatedasitcontainsgymequipmentswhicharesimilarto
the exercising equipments present in a playground or park.
stpecnoCpoT
stpecnoCpoT
stpecnoCpoTTask-Agnostic Concept Bottlenecks via Automated Concept Discovery 39
Supplementary References
1. Bricken,T.,Templeton,A.,Batson,J.,Chen,B.,Jermyn,A.,Conerly,T.,Turner,
N.,Anil,C.,Denison,C.,Askell,A.,Lasenby,R.,Wu,Y.,Kravec,S.,Schiefer,N.,
Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin, A., Nguyen, K., McLean,
B., Burke, J.E., Hume, T., Carter, S., Henighan, T., Olah, C.: Towards Monose-
manticity:DecomposingLanguageModelsWithDictionaryLearning.Transformer
Circuits Thread (2023)
2. Cooney, A.: Sparse Autoencoder Library. https://github.com/ai-safety-
foundation/sparse_autoencoder (2023)
3. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-
Scale Hierarchical Image Database. In: CVPR. pp. 248–255 (2009)
4. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T.,Dehghani,M.,Minderer,M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,Houlsby,N.:
AnImageisWorth16x16Words:TransformersforImageRecognitionatScale.In:
ICLR (2021)
5. He,K.,Zhang,X.,Ren,S.,Sun,J.:DeepResidualLearningforImageRecognition.
In: CVPR. pp. 770–778 (2016)
6. Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization. In: ICLR
(2015)
7. Kokhlikyan, N., Miglani, V., Martin, M., Wang, E., Alsallakh, B., Reynolds, J.,
Melnikov,A.,Kliushkina,N.,Araya,C.,Yan,S.,Reblitz-Richardson,O.:Captum:
A Unified and Generic Model Interpretability Library for PyTorch (2020)
8. Krizhevsky,A.,Hinton,G.,etal.:LearningMultipleLayersofFeaturesfromTiny
Images. Technical Report, Computer Science Department, University of Toronto
(2009)
9. Menon,S.,Vondrick,C.:VisualClassificationviaDescriptionfromLargeLanguage
Models. In: ICLR (2023)
10. Oikarinen,T.,Das,S.,Nguyen,L.M.,Weng,T.W.:Label-FreeConceptBottleneck
Models. In: ICLR (2023)
11. Oikarinen,T.,Weng,T.W.:CLIP-Dissect:AutomaticDescriptionofNeuronRep-
resentations in Deep Vision Networks. In: ICLR (2023)
12. Panousis, K.P., Ienco, D., Marcos, D.: Sparse Linear Concept Discovery Models.
In: ICCVW. pp. 2767–2771 (2023)
13. Paszke,A.,Gross,S.,Massa,F.,Lerer,A.,Bradbury,J.,Chanan,G.,Killeen,T.,
Lin,Z.,Gimelshein,N.,Antiga,L.,Desmaison,A.,Kopf,A.,Yang,E.,DeVito,Z.,
Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala,
S.: PyTorch: An Imperative Style, High-Performance Deep Learning Library. In:
NeurIPS (2019)
14. Patterson, G., Xu, C., Su, H., Hays, J.: The SUN Attribute Database: Beyond
Categories for Deeper Scene Understanding. IJCV 108(1-2), 59–81 (2014)
15. Petryk, S., Dunlap, L., Nasseri, K., Gonzalez, J., Darrell, T., Rohrbach, A.: On
GuidingVisualAttentionwithLanguageSpecification.In:CVPR.pp.18092–18102
(2022)
16. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning Transferable Visual Models
from Natural Language Supervision. In: ICML. pp. 8748–8763 (2021)
17. Ramaswamy, V.V., Kim, S.S., Fong, R., Russakovsky, O.: Overlooked Factors in
Concept-Based Explanations: Dataset Choice, Concept Learnability, and Human
Capability. In: CVPR. pp. 10932–10941 (2023)40 S. Rao et al.
18. Rao, S., Böhle, M., Parchami-Araghi, A., Schiele, B.: Studying How to Efficiently
andEffectivelyGuideModelswithExplanations.In:ICCV.pp.1922–1933(2023)
19. Sagawa,S.,Koh,P.W.,Hashimoto,T.B.,Liang,P.:DistributionallyRobustNeural
Networks. In: ICLR (2020)
20. Sharma,P.,Ding,N.,Goodman,S.,Soricut,R.:ConceptualCaptions:ACleaned,
Hypernymed, Image Alt-text Dataset for Automatic Image Captioning. In: ACL.
pp. 2556–2565 (2018)
21. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The Caltech-UCSD
Birds-200-2011 Dataset. Tech. Rep. CNS-TR-2011-001, California Institute of
Technology (2011)
22. Yang, Y., Panagopoulou, A., Zhou, S., Jin, D., Callison-Burch, C., Yatskar, M.:
Language in a Bottle: Language Model Guided Concept Bottlenecks for Inter-
pretable Image Classification. In: CVPR. pp. 19187–19197 (2023)
23. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: A 10 million
Image Database for Scene Recognition. IEEE TPAMI (2017)