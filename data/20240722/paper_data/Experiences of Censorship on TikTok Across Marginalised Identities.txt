Experiences of Censorship on TikTok Across Marginalised Identities
EddieL.Ungless,1
NinaMarkl,2
Bjo¨rnRoss1
1UniversityofEdinburgh
2UniversityofEssex
Correspondence:b.ross@ed.ac.uk
Abstract is deliberately suppressed by moderators of the platform
(Ko¨ver and Reuter 2019), a practice which seems to have
TikTok has seen exponential growth as a platform, fuelled
continued despite push-back (Biddle, Ribeiro, and Dias
by the success of its proprietary recommender algorithm
2020). The topic has also been explored through surveys
which serves tailored content to every user - though not
(Haimson et al. 2021) and interviews with users (Karizat
without controversy. Users complain of their content being
etal.2021;SimpsonandSemaan2021;Are2023;Lyuetal.
unfairly suppressed by “the algorithm”, particularly users
withmarginalisedidentitiessuchasLGBTQ+users.Together 2024a),whichsurfacedusers’beliefsabouthowmoderation
withcontentremoval,thissuppressionactstocensorwhatis isconducted.Whilstthesemaynotalignwiththeplatforms’
shared on the platform. Journalists have revealed biases in actual moderation system, they provide us with useful
automaticcensorship,aswellashumanmoderation.Wein- insight into the motivations behind users’ behaviours when
vestigateexperiencesofcensorshiponTikTok,acrossusers theyinteractwiththissystem(Karizatetal.2021).
marginalisedbytheirgender,LGBTQ+identity,disabilityor Understandingmarginalisedusers’experiencesofcensor-
ethnicity.Wesurvey627UK-basedTikTokusersandfindthat
shiponTikTokisvitalbecauseitisawidelyusedplatform
marginalised users often feel they are subject to censorship
thatservestoconnectmarginalisedcreatorswiththeircom-
for content that does not violate community guidelines. We
munitywhilstatthesametimepolicingwhattheyshare,as
highlight many avenues for future research into censorship
Simpson and Semaan (2021) highlight for LGBTQ+ users.
onTikTok,withafocusonusers’folktheories,whichgreatly
shapetheirexperiencesoftheplatform. ThisallowsTikToktoplayanunprecedentedroleinthecre-
ationofonlineandofflineidentities.Biasedcensorshipalso
alienatesalreadymarginalisedindividuals,reflectingoffline
Introduction
powerimbalances(Are2023).Surveyingusersallowsusto
In2024,TikTokisoneofthelargestsocialmedianetworks betterunderstandhowtheyinterpretthiscensorship.
in the world, having amassed over 1 billion users faster Weusecensorshipasanumbrellatermcoveringbothre-
thananyappeverbefore(Harwell2022).Itservesmachine- moval and suppression of content, by the recommender al-
learningcurated,user-generatedvideocontent;aformatthat gorithmandthemoderationsystem(s)(twosystems1 which
hasprovensosuccessfulthatothersocialplatformsarefol- are often conflated by TikTok users and referred to as “the
lowing suit (Harwell 2022; Frier 2022). For some younger algorithm”) and byhuman moderators, though our focus is
users, TikTok has taken the place of Google as a source on algorithmic censorship. Automated video removal now
of news and local information (Harwell 2022). Despite its accountsforthelargestproportionofremovedvideos:over
popularity,users’experiencesarefarfromuniversallyposi- 100Meveryquarter.2Suppressionreferstowhenacreator’s
tive:complaintsaboutcensorshipontheplatform,including content has its “reach” limited, for example by no longer
theremovalorsuppressionofcontentthatdoesnotseemto being served to viewers, though it is still on the creator’s
violate TikTok’s community guidelines, are common (e.g. profile.Thishasalsobeenreferredtoas“shadowbanning”
Brown(2021);Karizatetal.(2021);Are(2023)).Inpartic- whenithappenssystematically(Are2022,2023).Weaskre-
ular,hashtagsandtermsusedbyminoritygroupsappearto spondentsfortheirexperiencesofcontentremovalandsup-
beaffected(Ryan,Fritz,andImpiombato2020). pression,theirbeliefswithregardstoalgorithmiccensorship
(Perceived) censorship on TikTok of content by andhowthismightreflectordifferfromcommunityguide-
marginalised creators has received significant media lines, and the roles that human moderators might play. By
coverage (Brown 2021; Lorenz 2022; Kelion 2019; gathering detailed demographic data, we can explore how
Ohlheiser 2021; Ko¨ver and Reuter 2019; Biddle, Ribeiro, experiencesofcensorshipdifferacrossidentities.
andDias2020),includingaNetzpolitikarticlewhichshared
leaked documents which revealed that content featuring
1https://www.tiktok.com/transparency/en/content-moderation/
marginalised individuals such as those with disabilities and https://newsroom.tiktok.com/en-us/how-tiktok-recommends-
videos-for-you
Copyright©2024,Authors.ThisworkislicensedunderCCBY 2https://www.tiktok.com/transparency/en/community-
4.0. guidelines-enforcement-2023-2/
4202
luJ
91
]CH.sc[
1v46141.7042:viXraInthefollowing,wepresentthenecessarybackgroundto aswellasremoval:forexample,userstalkofcontentbeing
understanding our findings, including an overview of Tik- “takendown”bythealgorithm(Karizatetal.2021).Inthis
Tokandareviewoftheliteratureintoperceivedsocialme- paper, we discuss users’ attitudes towards this algorithmic
dia censorship. We present our survey methodology, then censorship.
presentdescriptivefindingsandstatisticalanalysesanddis-
cusstheseresults.Ourexploratoryworkhighlightsmanyav- Biasedmoderationonsocialmedia
enuesforfutureresearch.TheexplosiveincreaseinTikTok
Whilst ostensibly intended to protect communities, mod-
usage has left researchers and policy makers to catch up in
eration on social media can also enact harm. Studies into
ourunderstandingofuserexperiences,avitalstepinknow-
moderationonsocialmediasuggestthatmarginalisedusers
inghowtoprotectusersfromharmsrelatedtofairness,cen-
face additional censorship. Haimson et al. (2021) surveyed
sorshipandknowledgemanipulation.Ourquantitativeanal-
users of several social media platforms and noticed a trend
yses complement existing qualitative work on the topic of
whereby Black and trans users reported having content re-
marginalisedusers’experiencesonTikTok.Beyondexisting
moved that related to their marginalised experiences, even
quantitativework,weconsiderbothremovaland(suspected)
though the content followed site policies. “Conservative”
suppressionofcontent,thelatterbeingparticularlyrelevant
usersalsoreportedhighlevelsofremoval,butthiswastyp-
toaplatformwheretheuserexperienceissoheavilydriven
icallyrelatedtoviolationsoftheplatform’spoliciesonhate
byalgorithmicallycuratedcontent.Thisworkmakesanim-
speechormisinformation.Whilstallthreegroupsmayfeel
portantcontributiontoourunderstandingof(perceived)fair-
theplatformshowsabiasagainstthem,thecontentremoved
nessofcensorshipontheTikTok,onascalenotachievedby
from conservative users was often in violation of platform
existingwork.
policies – instances of true positives – whereas Black and
trans respondents reported high levels of false positives.
Background
Thereforethemoderationsystemsontheseplatformscanbe
IntroductiontoTikTok saidtobebiasedagainstBlackandtranscreators.
TikTokisaplatformforsharingshort-formvideocontent;it Further, moderation systems can be exploited by mali-
describesitselfas“theleadingdestination”forthiscontent.3 cious agents to enact harm on marginalised creators, for
Likemanysocialmediaplatforms,TikTokemploysanalgo- example by reporting content to get a creator silenced or
rithmtocuratecontentshowntousers,andusesautomated banned(ZengandKaye2022;Are2023).Are(2023)high-
moderation, including algorithms, to filter out inappropri- lightshowalreadymarginalisedcreatorssuchassexworkers
ate content.4 TikTok has published community guidelines andLGBTQ+individualscanfacetargetedcampaignsofre-
whichoutlinewhatcontentisinappropriatefortheplatform, portingasaformofharassment.
includingsexualcontentandcontentpromotingviolence.5
Content moderation is primarily presented as a way to Harmsof(biased)algorithmiccensorship
keep users safe, and as TikTok claim “to foster a fun
Considering briefly the negative impact automated censor-
and inclusive environment”.4 However, as Cobbe (2021)
ship on social media can have even if it were operating in
describes, automated moderation allows “unprecedented...
an unbiased manner, Cobbe (2021) argues that automated
control” over users’ public and private content. Zeng and
censorship allows privately owned social platforms’ com-
Kaye(2022)havereferredtothealgorithmicsuppressionof
mercialpriorities(thedesiretofeaturecontentthatappeals
content on TikTok as “visibility moderation”. The recom-
tothemainstream)tobe“inserted”intotheprivateandpub-
mender“ForYou”algorithmcanbeusedtoenforcemoder-
licconversationsofplatformusers,undermining“openand
ationdecisions,forexamplebynotservingcontentflagged
inclusivediscussion”.Thismaypreventusersfromexpress-
as “shocking... to a general audience”.6 The recommender
ingthemselvesinanauthenticmanner,astheyareprompted
system may also learn to suppress content that would oth-
toalignwhattheyexpresswiththecommercialgoalsofthe
erwise not be subject to moderation, for example by not
platform.
serving content from LGBTQ+ creators because it is sim-
A censorship algorithm that is (perceived to be) biased
ilartocontentthathaspreviouslyreceivedalotofnegative
can cause further harm at an individual and societal level.
interaction.Thustherecommender“ForYou”algorithmand
Discriminatorycensorshipcanleadtobothrepresentational
moderationsystems7,6 canbothbesaidtoconductalgorith-
and allocation harms against a community, following the
miccensorship,understandingcensorshiptoincludebothre-
distinctionmadebyBarocasetal.(2017).Ascenariowhere
movalandsuppressionofcontent.Indeed,TikTokusersof-
arepresentationalharmmightoccuriswhenthecensorship
tenseemtoconflatetheseintoasingleentityknownas“the
algorithm removes content featuring self-described fat cre-
algorithm”,responsibleforcontrollingthereachofcontent
ators(Clarketal.2021),reinforcingthefatphobicbeliefthat
3https://www.tiktok.com/about?lang=en onlythinbodiesshouldbeseen.Algorithmsregulate“what
4https://www.tiktok.com/transparency/en/community- becomesvisibleandwhatremainsoutofsight”(Velkovaand
guidelines-enforcement-2022-2/ Kaun 2021). An allocational harm might occur if a user’s
5https://www.tiktok.com/community-guidelines?lang=en contentbeingcensoredharmstheirincome,aswouldbethe
6https://newsroom.tiktok.com/en-us/how-tiktok-recommends- caseforthemanyinfluencersandsmallbusinesseswhorely
videos-for-you on social media. Further, the suppression and removal of
7https://www.tiktok.com/transparency/en/content-moderation/ postsrelatingto“BlackLivesMatter”,reportedbyusersofTikTok(Ghaffary2021),couldbearguedtodenyusers’the posted on TikTok, as we were interested in direct experi-
opportunitytocontributetoprotestsontheplatform. ences of censorship. 2,350 respondents completed our ini-
Ehsanetal.(2022)writeinreferencetoanunfairgrading tial prescreen, for a fee of £0.10 (equivalent to £10.59/hr).
algorithm that “algorithms can leave imprints on how peo- Ofthese, 777completedour mainstudy,for afeeof £1.70
plemakesenseofalgorithmicoperationsandinterprettheir (equivalentto£7.37/hr).45additionalrespondentswerere-
livedexperienceswiththealgorithm,carryingdeeppsycho- jectedinlinewithProlific’spolicies(i.e.failureofmultiple
logicalimpactontheirmentalwell-being.”.Havingcontent attention checks, answers contrary to pre-screening data).
unfairly censored by the app, or observing this happening, Next, we targeted LGBTQ+ respondents to “up-sample”
couldleadtofeelingsofalienationandlackofagency,feel- queerusersofTikTok.101LGBTQ+respondentscompleted
ingswhichmayremainafterdis-use(Ehsanetal.2022). our prescreen; 50 of these completed the main study. One
additionalrespondentwasrejected.Ofthe827respondents,
Folktheoriesofcensorship weuseddatafrom627respondentswhopassedallthreeat-
tention checks to ensure quality responses.8 This is a high
Karizatetal.(2021)foundthatusersofTikTokbelievedthat
rejectionrate,butasourattentioncheckswereverysimple
“the algorithm” suppressed content based on the creators’
wefeltfailureofevenonewouldindicatepoorqualitydata.
social identity, understood as referring to one’s member-
shipinacertainsocialgroup(BurkeandStets2009).More
ProcedureandMeasurements
specifically, users felt that content was suppressed based
on race and ethnicity, physical appearance including body We conducted a survey for an exploratory analysis of
size, disability and class status, LGBTQ+ identity and po- marginalised users’ experiences of censorship compared to
litical/ social justice group affiliations. Those belonging to non-marginalisedusers.Ethicsapprovalwasobtainedfrom
marginalised groups had their content suppressed, whereas the University of Edinburgh Informatics Research Ethics
others – those with “algorithmic privilege” (Karizat et al. Process, rt #6862. This study was conducted online using
2021)–benefittedfromhavingtheircontentfavouredbythe Qualtrics.com which has excellent security protocols, and
platform. This finding accords with work by Simpson and datawasanalysedonapassword-protectedcomputer.
Semaan (2021) who found that LGBTQ+ users of TikTok
Use of TikTok After giving informed consent, respon-
felt that TikTok unfairly censored content posted by them
dents were asked about their use of TikTok. If respondents
andfellowLGBTQ+creators,allwhilstpigeon-holingthem
indicated that they had stopped using TikTok, we asked
asbelongingto(normative)queeridentitiesintermsofthe
theirmotivations:optionswerebasedonGrandhi,Plotnick,
contenttheywereserved.Thealgorithmconstructedapro-
andHiltz(2019);VaterlausandWinter(2021);Lu,Lu,and
filefortheseusersbasedonanapproximationoftheirqueer
Liu (2020); Zhou, Yang, and Jin (2018). Most relevant to
identities,whichdeterminedwhattheycouldsee,andwhat
the present study, we asked whether “too much modera-
theycouldpost.
tion/censorship”wasamotivationforleaving.Respondents
Thebeliefthattheplatformcensorscontentbasedonso-
couldalsogiveothermotivations.
cial identity is referred to as The Identity Strainer Theory,
Respondents were asked how often they used TikTok to
an example of one of several folk theories Karizat et al.
view, and separately to post content, from “I have never
(2021) argue users hold about content curation and mod-
[posted/viewed]content”to“>3timesaday”(optionsbased
eration. Such folk theories shape the way users behave on
onLu,Lu,andLiu(2020)).Respondentswhoindicatedthat
social media platforms such as TikTok (Myers West 2018;
they had never posted content were removed from our fi-
Karizat et al. 2021; Are 2023), as well as their interactions
nal data. We asked about the types of content respondents
with other technologies such as smart devices (Frick et al.
viewed and, separately, posted on TikTok, with categories
2021)ortheirresponsetoalgorithmicgrading(Ehsanetal.
taken from Vaterlaus and Winter (2021). We confirmed re-
2022). Folk theories can develop as a result of “everyday
spondentsconsumedandpostedcontentinEnglishandtold
algorithm auditing”, whereby users detect problematic be-
themwewereonlyinterestedintheiruseofTikTokforEn-
haviour through their every day interactions with a system
glishlanguagecontent.
(Shen et al. 2021), as is the case when users of TikTok de-
liberatelyinteractwithcertaincontenttodeterminewhatthe Experience of Censorship We asked about respondents’
algorithm prioritises (Karizat et al. 2021; Simpson and Se- experiencesofcensorship.Westatedthatbycensorship“we
maan 2021); trial different text to determine what the app mean both when content is removed and when content is
censors(Brown2021);orobservewhichvideosreceivelim- suppressed.” We gave the example of a video getting very
itedviews(Lyuetal.2024b). fewviewsassomethingthatmightindicatesuppression,as
thisisreflectiveofwhatTikTokusersstatetheyuseas“evi-
Methodology dence”thattheyhavebeensuppressedor“shadowbanned”
Respondents (Lyu et al. 2024b). Removed content is no longer be visi-
bleontheplatform.Useoftheterm“censorship”mayhave
All respondents were recruited using Prolific.com, which
influencedourrespondents-seeLimitations.
pseudo-anonymises all data. All respondents were UK-
based. Prolific allows researchers to screen for use of Tik- 8Respondentswereaskedtoselect“yes”iftheywerepayingat-
Tok. Recruitment occurred from 2023-2024. We first con- tention.Twooftheratingquestionsincludedaninstructionto“se-
ducted a screening study to find respondents who had ever lectsomewhatdisagree”or“selectsomewhatagree”,respectivelyWe provided a list of 13 topics (henceforth “controver- Identity Marginalised:n,% Non-marg.:n,%
sial topics”9) and the option to supply “other”. The list of Female:377,60%
topics was derived from Haimson et al. (2021)’s paper on Gender Nonbinary:24,6% Male:224,36%
social media censorship, where users were invited to share Other:1,0%
what kind of content they felt was censored across differ-
Trans Yes:20,3% No:599,96%
ent social media platforms. Our list is as follows: Political
content;Contentsomemayfindoffensiveorinappropriate; Asexual:8,1%
Sex related content for a non-erotic purpose i.e. that is in- Bisexual:99,16%
Sexuality Straight:461,74%
tended to educate; Sex related content for an erotic pur- Gay:44,7%
pose; Covid-related content; Content insulting or criticiz- Other:4,1%
ing dominant group (e.g., men, white people); Content re- Disability Yes:73,12% No:542,87%
lating to a social justice movement, for example feminism
Asian:45,7%
or anti-racism; Content relating to minority identity expe-
Black:45,7%
rience i.e. queer content, content about Black experiences; Ethnicity White:511,82%
Multiple:21,3%
Hate speech; Curse words; Self-referential use of slur i.e.
Other:3,0%
d*ke by a lesbian; Content about violence that is not in-
tendedtoshockordisgusti.e.reportingonaviolentcrime;
Table1:Tableshowingcountandpercentageofrespondents
Contentaboutviolencethatisintendedtoshockordisgust.
of marginalised and non-marg(inalised) identities. Percent-
Byprovidingapre-definedlistwereducedthecognitiveef-
ageswillnotsumto100as“Prefernottosay”excluded.
fortforrespondentsprovidinganswers,andensuredwehad
consistentdataacrossrespondents.
Respondentswereaskedaboutwhichofthesetopictypes
- “prefer not to say”). Respondents were asked their age,
theyhadposted,andhowfrequently.Separately(seeLimita-
their gender identity (male, female, non-binary, other [text
tions)respondentscouldindicatewhethertheyhadhadcon-
entry]),theirsexuality(straight,gay,bisexual,asexual,other
tentremoved,andhowoftenthesedifferentkindsofcontent
[text entry]),10 and their ethnic group (topline categories
wereremovedona5-pointscaleof“never”to“always”.We
takenfromtheUK2021census(ONS2022)).Respondents
repeatedthisprocessforcontentsuppression.Wethenasked
wereaskedabouttheirtransstatusanddisabilitystatus(yes,
questionsrelevanttoatopicnotexploredinthispaper.
no). Respondents were also asked about political beliefs,
Beliefs about Algorithmic Censorship All respondents socio-economic status and religion, which we do not anal-
were then asked how strongly they agreed that the TikTok yseinthispaper.Respondentswereaskediftheybelonged
moderationalgorithmcensorsatleastsomepostsaboutthe to any other marginalised groups, and asked to give their
13 controversial topics, on a scale of 1-5 (“strongly dis- first language(s). Participants were asked additional ques-
agree”-“stronglyagree”). tionsnotrelevanttothisanalysis,thendebriefed.
BeliefsaboutCommunityGuidelines Respondentswere
askedhowstronglytheyagreedthattheTikTokcommunity Results
guidelinesdonotallowpostsaboutthe13controversialtop-
DemographicInformation Althoughdemographicques-
ics,onascaleof1-5.
tionswereaskedattheendofthesurveywepresentthisdata
We then explained that some users feel that content that
first as they contextualise the following results. The modal
seems to be in line with the community guidelines is cen-
age was 23, the mean was 30. The reported ages skewed
sored.Weaskedrespondentshowstronglytheyagreedona
heavilytowardsthe18-30range.ThisisreflectiveofTikTok
scale of 1-5 with the following statements about why con-
UKusertrends(HypeAuditor2022).Otherdemographicin-
tentiscensoredthatdoesnotgoagainstcommunityguide-
formation is given in Table 1. There being more females is
lines:“Otheruser(s)havereportedthecontent”,“Thealgo-
typicalofTikTok’sUKuserbase(HypeAuditor2022).The
rithm has not learned to follow the guidelines (it is not a
rateofLGB+identities(24.7%)ismuchhigherthanrecent
good algorithm)”, “TikTok has unpublished guidelines that
EnglandandWalescensusdatawouldanticipate(6.9%ONS
areusedtotrainthealgorithmwhicharestricter”,“TikTok (2022)),11evenbeforeweperformedupsamplingofLGBT+
hasunpublishedguidelinesthataregiventohumanmoder-
identities (21%). This may partly be due undercount in the
atorswhicharestricter”,“Thealgorithmhasmisunderstood
census (Guyan 2022)), but it also seems likely this is re-
the content / made a one-time mistake”, “Human modera-
flective of TikTok’s position as a prominent social network
torshavetheirownopinionsaboutwhatshouldbeallowed
forLGBTQ+youth,notedelsewhere(Ohlheiser2020).The
on the platform”. Myers West (2018) found human inter-
proportionofdisabledrespondentsislowerthancensusdata
vention and moderators having their own biases were two
forEnglandandWales(ONS2022):theaudio-visualnature
primaryreasonspeoplegaveforcontentmoderation.
oftheplatformwillhaveinfluencedthis,butitisworthnot-
Demographic Information Respondents were asked de- ing that TikTok has been shown to be biased against dis-
mographicquestions(alwayswiththeoptionnottoanswer
10Thesewerecategoriesofsexuality.Forthefulllistofoptions,
9Weusethistermasusershavepreviouslyreportedhavingsuch seeSupplementaryMaterial
contentremovedandthusitcanbethoughtofas“controversial” 11LatestScottishcensusdataunavailableattimeofwritingabled users which may have discouraged use by disabled answers included unwarranted concerns over minor safety
creators (Lyu et al. 2024b; Ko¨ver and Reuter 2019; Bid- (mostcommonreasongiven),copyrightinfringement,drugs
dle, Ribeiro, and Dias 2020). We were unable to establish oralcohol,andcontentincorrectlyidentifiedassexuallyin-
if ethnicitydata isreflective of thetypical UKuser base of appropriate. That “Content some may find offensive or in-
TikTok, but it is relatively reflective of the UK population, appropriate”wasthesecondmostcommonsuggeststhatre-
perEnglandandWalescensusdata(ONS2022).Weasked spondentswereaware(atleastretroactively)thatthecontent
respondentsiftheybelongedtoanyothermarginalisediden- could offend. The most likely content type to “Always” be
titiesandresponsesincluded“workingclass”,“neurodiver- removedwasviolence.
gent”and“ex-sexworker”. Forcontentsuppression,themostcommontypesofcon-
tentrespondentsbelievedhadbeensuppressedwere“Other”
AllRespondents (26.0% of those who reported suppression) then “Curse
words” (25.6%). Respondents who selected “Other” fre-
Wefirstpresenttheresultsacrossallrespondents,topainta
quently reported being unsure why their content was being
pictureoftypicaluse.Toensureourresultsacrossalldemo-
suppressed (∼ 1/3). Compared to content removal, reports
graphicsareindicativeofthetypicalTikTokpopulation,we
of suppression were more evenly distributed across topics.
userakingtoweightthesexualitygroups12 pertheirpreva-
Thissuggeststhatthecontroversialcontenttypesareallcon-
lence in the original recruitment drive,13 rather than after
sideredpossiblereasonsforsuppression,evenifremovalis
up-sampling of LGBTQ+ respondents. Typically, percent-
relatively uncommon for some e.g. Covid-related content.
agesaregivenasisappropriateforweighteddata,butwhere
The most consistently suppressed content was “Sex related
it improves clarity we also report raw n counts. We then
contentforanon-eroticpurpose”.
analyse results across each demographic axis in turn, with
Thehighnumberof fillintextanswerssuggeststhat the
afocusonhowtheexperiencesofmarginalisedusersdiffer
controversial topics we had identified through existing re-
fromnon-marginalisedusers.
searchexplainonlypartofTikTokusers’experiencesofcen-
Use of TikTok The vast majority (90.0%) reported hav- sorshipontheplatform(seeLimitations)
ing started using TikTok over a year ago. The majority
Beliefs about Algorithmic Censorship Respondents
(55.1%) of respondents used TikTok to view content over
most strongly agreed that “Content about violence that is
3 times a day. The modal total time spent on the app was
intendedtoshockordisgust”iscensored,selectingonaver-
between1-2hours.Thissuggestsourdataisrepresentative
age“Somewhatagree”(3.88).Respondentswereleastlikely
of“loyal”usersoftheapp.Themajority(74.1%)ofrespon-
to agree that “Covid-related content” is censored, selecting
dentsposted0-3timesamonth.Thissuggestsmostusersof
“Neitheragreenordisagree”onaverage(2.89).
theapparerelativelypassive,consumingcontentbutinfre-
Weinvitedrespondentstoaddanythingelsetheybelieve
quentlyposting.
about the TikTok moderation algorithm, and some themes
Averysmallnumber(3%)ofrespondentsreportedbeing
emerged.Tworespondents(rawcounts)suggestedthecom-
ex-usersoftheplatform.Ofthese,themostcommonreason
munityguidelineswerenotbeingfollowedbythealgorith-
forquittingwasbecausecontentwasnolongerentertaining
miccensorship–forexample,“Theydon’tfollowtheirown
(reportedby55%ofex-users).“Toomuchmoderation/cen-
communityguidelinesbecausetheybaninnocentpeopleall
sorship”wasreportedbyasingleex-user(rawcount),sug-
the time but keep people who break it for e.g. I always see
gestingthisisnotaprimarymotivatingfactorforleaving.
nudityonmyfyp”.Manyreportedthatcontentwasremoved
Experience of Censorship The least common content without reason or that sometimes the “wrong” content was
type that respondents posted was “hate speech” (3.15%). removed (n = 12). Some respondents (n = 5) shared
Themostcommoncontroversialcontenttypeswere“Curse thattheyfeltthecensorshipwasbiasedagainstmarginalised
words”(35.1%ofrespondents),followedby“Politicalcon- groups, for example saying “Minorities are disproportion-
tent”(19.3%). atelyaffected...Ablackpersoncanduetaracistpersonex-
A relatively small percentage of respondents reported plaining that they are wrong, and the black person’s video
havinghadcontentremoved(12.8%)orsuppressed(14.1%). will be removed... not the racist’s”. Several respondents
Around half of respondents who reported content suppres- (n = 3) felt that moderation was politically motivated, for
sionbelievedtheyhadhadcontentremoved,andviceversa. examplebecauseofitsfailuretoremovepropaganda.
Of those who report content removal, “Other” and “Con-
Beliefs about Community Guidelines There were sev-
tent some may find offensive” account for the majority
eral topics that users on average agreed were censored by
of reports of removal (reported by 38.1% and 33.5% re-
thealgorithm,butwhichtheythoughtdidnotgoagainstTik-
spectively;respondentscouldselectmultiplecontenttypes).
Tok community guidelines (or were unsure). This was true
Of the “Other” content which had been removed, written
fornon-eroticsexrelatedcontent,cursewordsandpolitical
content. Of particular relevance to the focus of this paper,
12The ANES raking variable selection algorithm determined
this was true for both content relating to minority identity
changestothesamplebasedongenderandtransstatuswerenegli-
gible experiences and content relating to a social justice move-
13ForlackofmoredetailedTikTokuserdemographicinforma- ment.Respondentssomewhatagreed(4.05)thatwhencon-
tionbeingavailable,wetakeouroriginalsampledistributiontobe tent was removed despite not violating community guide-
reasonablyaccurate lines,thiswasduetootherusersreportingthecontent.This100
Removal Suppression
90
Male −1.458(0.768) −0.469(0.593) 80
Straight −0.883∗(0.345) −0.608∗(0.352)
70
White – −0.658∗(0.281)
60
NotDisabled −0.850∗(0.331) −1.004∗(0.326)
50
Male+Straight +2.223∗(0.826) +1.320∗(0.664)
40
Constant −0.766∗(0.296) −0.302(0.389)
30
20
Table 2: Table showing coefficients and (standard errors) 10
172 40 41 130 34 42 19 7 7
in two logistic regressions predicting content removal, and 0
contentsuppression.∗p<.05. Women (377) Men (224) Nonbinary (24)
Posting Removed Suppressed
was the reason that respondents considered to be the most Figure 1: Chart showing the percentage of respondents by
likely. gender who posted “controversial” content, and reported
having content removed or suppressed. Data labels show
ExperienceofCensorshipbyDemographicGroup counts.Totalcountsaregivenafteridentitylabels.
We now consider the impact of identity on experiences of
censorship.Weconductedlogisticregressionstodetermine
ifidentitypredictscontentremovalandsuppression.Weex- findings.
cludethosewhoanswered“Prefernottosay”toanydemo-
By Gender Nonbinary respondents were the most likely
graphicquestion(n=25,4.0%ofdata).Wecreateddummy
toreportpostingoneofthecontroversialtopictypesatleast
variablesacrossgender,transstatus,sexuality,disabilityand
once, see Figure 1. Nonbinary people were more likely to
ethnicity(Marginalisedbygender=0,notmarginalisedby
postcontroversialcontentthanmen(p = .050)andwomen
gender = 1, etc). We include binary interaction effects e.g.
(p = .001). Nonbinary people were more likely to re-
male + straight. We use a stepwise algorithm to determine
port having content removed and suppressed than men and
thefinalmodel.Wefounddisabilitystatus,genderandsex-
uality all impact content removal, R2 = .05,p < .001: women, see Figure 1; the differences between nonbinary
people and women are significant (p = .014 for removal,
straightpeopleandpeoplewithoutdisabilitiesarelesslikely
p=.016forsuppression).
toexperienceremoval,butstraightmenspecificallyaremore
likelytoexperienceremoval(seeTable2).Wefounddisabil- Thetypesofcontentthatrespondentspostedandreported
itystatus,ethnicity,genderandsexualityallpredictcontent ascensoreddifferedacrossgenders.Menweresignificantly
suppression, R2 = .06,p < .001: white people, straight more likely to post political content than women, 28.6%,
peopleandthosewithoutdisabilitiesarelesslikelytoexpe- n = 64, vs. 14.1%, n = 53; p < .001, and more likely
riencesuppression,butstraightmenaremorelikely. to have it removed (11.76% of those who reported content
Identity clearly impacts experiences of censorship, and removal, n = 4, compared to n = 0 women, p = .019),
it is not always the case that those of marginalised identi- though a Chi-square test with history of posting political
tiesexperiencemorecensorship(i.e.straightmenaremore content as a layer (control) variable suggests this was en-
likely to report censorship). We now look at demographic tirelyaccountedforbyhistoryofposting.Despitethesedif-
attributesinturntobetterunderstandexperiencesofcensor- ferences,respondentsofdifferentgenderssharedsimilarbe-
shipacrossidentities. liefsaboutcensorshipofpoliticalcontent.Allgroupsneither
Giventheexploratorynatureofthiswork,wedidnotfor- agreed nor disagreed if it was subject to algorithmic cen-
mulate specific hypotheses, but we do perform some post-
sorship(3.15forwomen,3.09formen,3.25fornonbinary
hoc testing to suggest whether differences are substantive. people), and all disagreed that it went against community
We always exclude “Other” due to small n and lack of ho- guidelines (2.45 for women, 2.29 for men, 2.15 for nonbi-
mogeneity.WeprimarilyconductFisher’sExactTeststoes- nary people). Gender did not impact ratings, per Kruskal-
tablish the significance of the difference in rates of posting Wallistests.
controversial content or having it removed or suppressed, Men were much more likely to post content some may
betweengroups.Wereport2-sidedp-values14andmakesug- findoffensivecomparedtowomen(30.1%,n=68formen;
gestions as to whether the differences we report are likely 11.7%,n=44forwomen;p<.001),andmorelikelytore-
to be meaningful. Where relevant we include other statisti- porthavingthiscontentremoved(7.59%,n = 17formen;
calanalyses,notingthattypicallyonlylargeeffectsizescan 2.12, n = 8 for women; p = .002), though a Chi-square
bedetected.Tests wereselectedwhichweresuitableto the test suggests this was entirely accounted for by history of
lack of group size parity. This exploratory paper can indi- posting. Fifty percent of men who reported having content
cate fruitful lines of future enquiry to reify our descriptive removed said it included content some may find offensive
(n = 17).Itwasthemostcommontypeofcontentmenre-
14Allp-valuesarefromFisher’sExactTestsunlessspecified ported having had removed. This was also the most com-
%monly reported suppressed topic for men (28.6% of men 100
who reported suppression, n = 12). Similarly, men were
90
significantly more likely to post hate speech than women.
80
Over 5% of men (n = 14) reported posting hate speech
at least once (compared with 1.9% of women, n = 7; 70
p=.016).
60
Nonbinary respondents were more likely than women to
50
postcontentcriticisingadominantgroup(20.1%,n=5vs.
6.37%,n = 24;p = .022),relatedtoasocialjusticemove- 40
ment(30%,n = 8vs.15.6%,n = 59;p = .042)orabout 30
a minority experience (58%, n = 5 vs. 10.6%, n = 40;
20
p < .001). Nonbinary respondents were also significantly
morelikelytopostaboutminorityexperiencesthanmen(vs. 10 218 24
53 61 2 7 65 22 18 6 1 2
16.1%,n=48;p<.001). 0
Turning to beliefs about censorship, we find that men, Straight (461) Gay (44) Bisexual (99) Asexual (8)
women and nonbinary people differ in their beliefs about Posting Removed Suppressed
thecensorshipofcontentaboutmarginalisedidentities(crit-
icising a dominant group, related to a social justice move- Figure2:Chartshowingpercentageofrespondentsbysexu-
ment or about a minority experience). We find that nonbi- alitywhoposted“controversial”content,andreportedhav-
nary people agree more strongly on average that this con- ingcontentremovedorsuppressed.Datalabelsshowcounts.
tent is algorithmically censored compared to women, who Totalcountsaregivenafteridentities.
inturnbelievemorestronglyinalgorithmiccensorshipthan
men. We conducted Kruskal-Wallis tests (which are suit-
able for small sample sizes) and included pairwise com- lines (for social justice content: 2.58 for women, 2.60 for
parisons with Bonferroni correction. We find respondents men, 2.13 for nonbinary people; for minority experience
did not differ significantly in their beliefs about the algo- content:2.68forwomen,2.66formen,2.22fornonbinary
rithmic censorship of content criticising dominant groups, people). All three groups neither agreed nor disagreed if
but did so for content related to social justice movements content criticising a dominant group goes against commu-
(H(2) = 19.909,p < .001)andforcontentaboutminority nityguidelines(3.02forwomen,3.07formen,2.89fornon-
experiences (H(2) = 21.347,p < .001). Nonbinary peo- binarypeople).
ple were significantly more likely than men to agree social
By Trans Status Comparing trans and non-trans respon-
justice content (p = .001) and minority experience con-
dents, we found that trans respondents were much more
tent(p=.001)werealgorithmicallycensored;womenwere
likely to post controversial content (75% of trans respon-
also significantly more likely than men to agree social jus-
dents, n = 15, vs. 50.6%, n = 303; p = .040). However,
tice content (p = .001) and minority experience content
trans respondents were no more likely to have content re-
(p=.032)werealgorithmicallycensored;nonbinarypeople
movedorsuppressed(p=.163andp=.185).
weresignificantlymorelikelythanwomentoagreeminority
Lookingatdataonthetypesofcontentposted,onesignif-
experiencecontentwascensored(p=.002).
icantdifferenceisinthefrequencyofpostingaboutminor-
Ourresultssuggestthatthereisnotaclearrelationshipbe-
ityidentityexperienceswhichis(somewhatunsurprisingly)
tween history of censorship, and beliefs about algorithmic
much higher for trans compared to non-trans respondents
censorship. Nonbinary people did not differ significantly
(55%,n=11vs.12.7%,n=76;p<.001).Transrespon-
frommenintermsofpostingcontentrelatedtoasocialjus-
dentswerealsocomparativelymorelikelytoreporthaving
ticemovement,andtherewerenosignificantdifferencesin
this content removed (20% of those who report removal,
reported removal or suppression for this topic (per Fisher’s
n = 1 vs. 2.7%, n = 6; p = .094) or suppressed (60%
ExactTests,p = .209andp = .186respectively),yetnon-
of those who report suppression, n = 3 vs. 7.2%, n = 6;
binarypeopleweremorelikelytoagreethiscontentissub-
p=.002).Transrespondentsalsoagreedmorestronglythat
jecttoalgorithmiccensorship.Likewise,beliefsaboutcon-
thiscontentwassubjecttoalgorithmiccensorship(3.79vs.
tentsuppressiondidnotdirectlymirrorcontentremoval.For
3.31; p = .015 per a Mann-Whitney U Test), although the
example, 14.3% of men who reported having content sup-
twogroupsdidnotdiffersignificantlyinbelievingthatthis
pressedreportedthiscontentwasrelatedtoCovid(n = 6),
contentdoesnotgoagainstcommunityguidelines(2.40for
but less than 3% reported having Covid-related content re-
transrespondentsand2.67fornon-transrespondents).
moved(n=1,2.94%ofthosewhoreportcontentremoval).
There were further differences in the reported beliefs
Whilst respondents differed by gender in their beliefs
about censorship and community guidelines: for example,
aboutalgorithmiccensorshipofcontentaboutmarginalised
transrespondentsmorestronglyagreedthat“self-referential
identities, differences were not significant for beliefs about
use of slur” was censored by the algorithm (3.96 vs. 3.44;
whetherthiscontentgoesagainstcommunityguidelines(per
Mann-WhitneyUTest,z =2.385;p=.017).
Kruskall-Wallis tests). Women, men and nonbinary people
all disagreed that content related to a social justice move- BySexuality Countsforpostingcontroversialcontentand
mentorminorityexperiencegoesagainstcommunityguide- reports of removal and suppression suggest bisexual and
%asexualrespondentswerethemostlikelytoreporttheseex- Ethnicgroup Posted Removed Suppressed
periences, as shown in Figure 2. To establish whether like- Asian 48.9%(22) 13.3%(6) 20%(9)
lihoodofpostingcontroversialcontentdifferedsignificantly
Black 66.7%(30) 11.1%(5) 20%(9)
acrosssexualities,weperformedacross-tabulationanalysis
Multiple 61.9%(13) 28.6%(6) 33.3%(7)
withpost-hocz-testsacrossallsexualitiestoestablishlikely
significant differences between sexualities. This suggested White 49.7%(254) 12.3%(63) 12.5%(64)
thatbisexualrespondentsweremorelikelythanstraightre-
Table3:Percentageofrespondentsineachethnicgroupwho
spondents to post controversial content. We found this to
be significant per a Fisher’s Exact test (66.6%, n = 65 vs. reported posting controversial content, having content re-
47.3%,n=218;p=.001). movedorsuppressed.Countsgiveninbrackets.
Repeatingthisprocedureforcontentcensorship,wefind
bisexual respondents were significantly more likely than
straight respondents and gay respondents to report having ualandasexualrespondentsreportpostingthistypeofcon-
had content removed. Fisher’s Exact tests found these dif- tentonaverage.PeraFisher’sExactTestcomparingstraight
ferencestobesignificant(comparedtostraightrespondents: andgroupednon-straight(gay,bisexual,asexual)identities,
22.2%, n = 22 vs. 11.5, n = 53; p = .012; compared to this difference was significant p < .001. This was also the
gayrespondents:vs.4.54,n=2;p=.008). mostcommontypeofcontentforbisexualrespondentswho
Whilstbisexualrespondentsgreaterpostingofcontrover- reported content suppression: 33.3% (n = 6) of bisexuals
sialcontentseemstoexplainthehigherratesofcensorship
whoreportedexperiencingsuppression,equivalentto1/5of
compared to straight respondents, this does not explain the allwhoreportedpostingthistypeofcontent.Thiscontrasts
comparative “lack of censorship” of gay respondents. Bi- with the fact only two bisexuals reported having this con-
sexual respondents report much higher rates of content re-
tentremoved(9.10%ofbisexualswhoreportedcontentre-
movalcomparedtogayrespondents,althoughaChi-Square moval),reflectingadisconnectbetweenexperiencesofcon-
analysis with history of posting controversial content as a tentremovalandbeliefsaboutcontentsuppression.
layer variable did not find this to be significant (for those
By Ethnicity Experiences of censorship differed by eth-
with history of posting controversial content, χ2(1,89) =
nicgroup(SeeTable3).Blackrespondentsreportedposting
3.771,p = .084). Given the small sample sizes, we cau-
themostcontroversialcontent,andweresignificantlymore
tiouslysuggestthistestmayhavebeen“underpowered”and
likelytodosothanWhiterespondents(66.7%,n = 30vs.
thatthisresultmeritsfurtherinvestigation.
49.7%,n = 254;p = .030),butnomorelikelythanwhite
Aswithotheridentitygroups,wedidnotfindaclearlink
respondents to report having content removed given a his-
betweenreportsofcensorshipandbeliefsaboutalgorithmic
toryofpostingcontroversialcontent(peraChi-Squareanal-
censorship. For example bisexual respondents were more ysis,χ2(1,284)=0.120,p=.812,forthosewhopost).
likely to agree that “sex related content for an erotic pur-
Almost a third of respondents of mixed or multiple eth-
pose” was subject to algorithmic censorship, compared to
nic groups reported content removal (28.6%) and suppres-
straightrespondents(4.02forbisexualvs.3.70forstraight,
sion (33.3%); this is much higher compared to white re-
per Mann-Whitney U test, z = −2.349,p = .019). How-
spondents,evenwhencontrollingforhistoryofpostingcon-
ever,therewasnodifferenceinpostingratesbetweenthese
troversialcontent:aChi-squareanalysisfound respondents
twogroups,nordidbisexualrespondentsreporthigherrates
from mixed or multiple ethnic groups were significantly
ofcensorship,suggestingthisbeliefcomesfromobservation more likely to report content suppression (χ2(1,267) =
ofothers’ reportsofcensorship,or widerexperienceof the
4.399,p=.046,forthosewhopost).
policingofqueersexualities,ratherthandirectexperienceof
All marginalised (non-white) ethnicities report higher
censorshiponTikTok.
rates of content suppression compared to removal (∼ 5%
Straight respondents were most likely to report having
greater or more) (see Table 3). For Black respondents,
content some may find offensive removed by the platform
reports of content suppression were almost double those
(43.4%ofrespondentswhowerestraightandreportedhav-
of content removal, whereas for white respondents levels
ing content removed, n = 23), but this was reported by
were similar. People of colour may perceive themselves to
only two bisexual respondents (9.10% of bisexual respon-
face higher levels of censorship through suppression, even
dents who reported content being removed). A relatively
though censorship through removal is typically reported at
largenumber(n = 6)ofstraightrespondentsdonot report
similarratesacrossethnicities(barringmixedethnicity).
postingcontentthatsomemayfindoffensive,butdoreport
havingsuchcontentremoved,perhapsindicativeofabelief By Disability Respondents with disabilities were more
that the content they post is not offensive even if it is re- likely to have content removed and suppressed, even when
movedforbeingso.Acomparisonwithinthosewhoreport controllingfortheirgreaterlikelihoodtopostcontroversial
postingthistypeofcontentfoundbisexualswerelesslikely content (see Figure 3): Chi-square analyses with posting
tohaveitremovedthanstraightrespondents,butthisdiffer- controversial content as a layer variable found respondents
encewasnotsignificant(4.76%,n = 1vs.23.0%,n = 17; withdisabilitiestobesignificantlymorelikelytoreportcon-
p=.187). tentremoval(χ2(1,313) = 13.259,p < .001)andsuppres-
LGB+ respondents were much more likely to post about sion(χ2(1,313)=5.928,p=.022).
minorityidentityexperiences–around1/3ofallgay,bisex- Considering censorship of particular topics, as with100 dency to post “Telling jokes” content compared to women
90 and nonbinary people, which we also found, given the in-
herently subjective nature of humour. Meaney et al. (2022)
80
exploregenderdifferencesinratingonlinehumourandfind
70
thatmen“tolerate”offencemorein“humorous”content.It
60
isalsoworthexploringthesignificantlyhigherratesof(self-
50
reported)hatespeechpostingontheplatformamongstmen.
40
It may be that men are no more likely to post hate speech,
30 butaremorelikelytobehonestaboutit.Itmayevenbethat
20 men are less likely to take the topic seriously, so answered
10 yeswithoutmuchthought.Whateverthereason,ithighlights
46 19 19 267 59 67
0 thevalueofinvestigatinghatespeechcontentonTikTok,to
With disability (73) Without disability (542) understand who produces it and their attitudes towards of-
Posting Removed Suppressed fensivecontent.
AswithconservativeTikTokuserscomparedtotransand
Figure 3: Chart showing percentage of respondents by dis- Black users in Haimson et al. (2021), it may be that whilst
ability status who posted “controversial” content, and re- straightandmalerespondentsreportbeingcensoredatcom-
ported having content removed or suppressed. Data labels parable rates to marginalised respondents, they are hav-
showcounts.Totalcountsgivenafteridentities. ingcontentremovedthat(theythemselvesagree)isagainst
community guidelines, where marginalised creators have
contentremovedthatdoesnotclearlyviolatetheplatform’s
LGBTQ+ minorities, respondents with disabilities were standards.Beingcensoredforcontentthatisbelievedtobe
more likely to post about minority experiences compared in line with community guidelines will contribute towards
to those without (28.8%, n = 21 vs. 12.2%, n = 66; feelings of discrimination and alienation. Users may feel
p < .001). Respondents with disabilities were more likely theyarebeingsilencedforpostingcontentthatdirectlyper-
to report this content being removed (n = 3 vs. n = 0). tainstotheirlivedexperiencesasmarginalisedpeople.
A Chi-square analysis with posting about minority experi-
Direct experience isn’t everything We found beliefs
ences as a layer variable found respondents with disabil-
aboutalgorithmiccensorshipwerenotobviouslycorrelated
ities to be more likely to report removal of this content
with direct experience of censorship on the platform. For
(χ2(1,87)=9.765,p=.013forthosewhopost).Thismay
example, nonbinary respondents agreed more strongly that
explain why respondents with disabilities seemed to more
socialjusticecontentisalgorithmicallycensored,compared
strongly agree that this content was subject to algorithmic
to men, even though nonbinary respondents never reported
censorship (3.58 vs. 3.30), though this was not significant
having this content removed. Thus, TikTok users’ beliefs
per a Mann-Whitney U Test (z = 1.795; p = .088) – al-
about censorship are not always the result of direct expe-
though history of censorship did not always directly relate
rience,butlikelyformedthroughwhichtopicsthey(donot)
totypesoncontentposted.Forexample,ourresultssuggest
seeontheplatform;thecontenttheyconsumerelatedtooth-
that respondents with disabilities may agree more strongly
ers’experiencesof,andfolktheoriesabout,censorship;and
thatself-referentialslursarecensoredbytheplatform(3.67
censorshiponotherplatformsandin“reallife”.
vs.3.43)(thoughagainthiswasnotsignificantperaMann-
WhitneyUTest,z = 1.752;p = .080)thoughtherewasno Suspicious Minds We found there was a mismatch be-
differenceinratesofcensorship. tween reports of content removal, and content suppression.
Around half of respondents who reported content suppres-
DiscussionandFutureDirections sion reported no content removal. Many more topics were
reported as being subject to suppression compared to re-
Weidentifiedanumberofkeytrendsinthedata,whichwe
moval.Ratesofsuspectedsuppressionweretypicallyhigher
discussbelow.Wealsohighlightavenuesforfutureresearch
than rates of removal, across demographics. This suggests
to deepen our understanding of (perceived) biased censor-
that beliefs about what content is suppressed by the plat-
shiponTikTok.
formarenotdirectlyrelatedtoexperiencesofcontentbeing
Offenceisintheeyeofthebeholder Wefoundthatoften removed (where a justification is typically provided). This
contentsomemayfindoffensivewasoneofthemostcom- is particularly evident when comparing across respondents
monly censored content types for non-marginalised groups from different ethnic groups. Reports of removal and sup-
(i.e.menandstraightpeople).Forexample,straightpeople pressionwereatsimilarratesforwhiterespondents,butre-
reportedhigherratesofremovalthanbisexual,gayorasex- spondentsofcolourweremorelikelytosuspectsuppression
ual people, and it was the most commonly removed con- thanremoval.Wedonotattempttosaywhetherthesesuspi-
tenttype;menreportedhigherratesofcontentremovalthan cionsarefounded:itmaybethatusersofcolourarenomore
women and nonbinary people, and again it was the most subjecttosuppressionthanwhiteusers,despitetheirbeliefs.
commonly removed content type. Indeed, half of men who ItmayalsobethatTikTokfavourssuppressionoverremoval
reported content being removed said it was content some of content by marginalised creators, to avoid similar scan-
may find offensive. This could relate to their greater ten- dals to Brown (2021); Ghaffary (2021); Ohlheiser (2021)
%i.a. (whilst suppression of marginalised creators’ content inbothcasesalgorithmiccurationwouldbeaconfound.Our
has also received significant media attention (Kelion 2019; useoftheterm“censorship”mayhaveinfluencedresponses,
Ko¨verandReuter2019),itishardertoprovethandiffering as censorship has more negative connotations than for ex-
rates of content removal, account closures etc – though cf. ample“moderation”.Futureworkmightemploymoreneu-
Ryan, Fritz, and Impiombato (2020); Biddle, Ribeiro, and trallanguagei.e.onlyusingtheterms“removal”and“sup-
Dias(2020).ExperimentationinthestyleofKing,Pan,and pression”.Weaskedwhichtypesofcontentpeoplepostand
Roberts(2014)–whoattempttoreverseengineercensorship whichtypesarecensoredasseparatequestions,andsomere-
inChina–mayhelptoshedlightonwhichtopicsarecen- spondents gave conflicting answers. Mismatches may have
soredthroughsuppressionontheplatform.However,regard- arisen due to respondents preferring to select “Never” for
lessofthe“truth”,thatsomemarginalisedusersfeeltheyare thefrequencyofpostingatypeofcontenttheyhaveposted
subjecttoadditionalsuppressionisharmfulinandofitself. onlyonce.Orrespondentsmayhaveinterpretedthecontent
type descriptions differently in each instance, e.g. they do
Not So Innocent Mistakes We found respondents felt
not believe the content they post to be sexual, but this was
censorship of content did not always align with the plat-
the reason given for its removal. Greater clarity in our in-
form’s community guidelines. Respondents reported that
structionswouldhaveaddressedthisissue.
content was removed without reason or that sometimes the
Oursamplemaynotberepresentativeandwevehemently
“wrong” content was removed. We found that for several
discourageuseoftheworktoconcludethatspecificgroups
of the controversial topics (namely non-erotic sex related
do not experience harms from censorship, simply because
content, curse words, political content, content relating to
wedidnotfindevidence.
minorityidentityexperienceandcontentrelatingtoasocial
justicemovement),respondentsagreedthiscontentwassub-
Conclusion
jecttoalgorithmiccensorshipdespitenotbeingagainstcom-
munity guidelines. However, whilst respondents acknowl- Our exploratory analysis has revealed that whilst rates of
edged that algorithmic censorship could go against official censorship are not always higher for marginalised creators,
communityguidelines,whenaskedabouttheirbeliefsabout theyaremorelikelytobecensoredforcontentthatisgener-
why content might be removed or suppressed despite not allyregardedasinlinewithcommunityguidelines,through
violating guidelines, respondents most strongly agreed that removalandsuppression.Thiswillcontributetowardsfeel-
thiswasduetootherusersreportingthecontent.Thisaligns ings of discrimination and alienation on the platform. We
withpreviousresearchwhichfoundhumaninterventionwas have highlighted several avenues for future work into ex-
consideredaprimarycauseforcontentremoval(MyersWest periences of biased censorship on TikTok, to complement
2018).TikTokusershaveevenexpressedconcernsthisbeing investigativeworkalreadyconductedbyjournalists.Ween-
donetoharasscreators(ZengandKaye2022;Are2023). courageafocusonusers’beliefsoveranyattempttoidentify
the“truth”,asthesefolktheoriesofcensorshipplayapivotal
TheBisHaveIt Wefoundthebisexualrespondentsmay
roleinshapingusers’experiencesoftheplatform.
besubjecttoadditionalcensorshipcomparedtogayrespon-
dents,thoughwerepeatourcautionthatthisrequiresfurther
Acknowledgements
empiricalinvestigation.Thisadditionalcensorshipofbisex-
ual content may be best understood in the context of the EddieL.UnglessissupportedbytheUKRICentreforDoc-
findingofSimpsonandSemaan(2021)thattheTikTokalgo- toral Training in Natural Language Processing, funded by
rithmseemstofavourqueercontentthatalignswithnorms. theUKRI(grantEP/S022481/1)andtheUniversityofEdin-
Bisexual people face what is known as “double discrimi- burgh,SchoolofInformatics.
nation”,wherebytheyfacerejectionfrombothoutsideand
within the LGBTQ+ community for their plurisexual iden- References
tity (attraction to multiple genders) (Mereish, Katz-Wise,
Are,C.2022.TheShadowbanCycle:anautoethnographyof
and Woulfe 2017). It is possible that content related to bi- poledancing,nudityandcensorshiponInstagram. Feminist
sexualexperiencesfallsoutsidethenormthathasbeencon- MediaStudies,22(8):2002–2019.
structed on TikTok – by its developers and users – hence
Are, C. 2023. The assemblages of flagging and de-
bisexual respondents were subject to greater levels of cen-
platformingagainstmarginalisedcontentcreators. Conver-
sorshipthangayrespondents.
gence.
Limitations Barocas, S.; Crawford, K.; Shapiro, A.; and Wallach, H.
2017. Theproblemwithbias:fromallocativetorepresenta-
We did not format our questions around TikTok’s specific
tionalharmsinmachinelearning.SpecialInterestGroupfor
moderation guidelines (e.g. related to copyright music),
Computing. InformationandSociety(SIGCIS),2.
whichpartlyexplainsthehighnumberof“Other”answersto
typesofcensoredcontent.Thismayhavebiasedourfindings Biddle, S.; Ribeiro, P. V.; and Dias, T. 2020. Tik-
againsttopicsofparticularrelevancetoTikToksuchasmi- Tok told moderators: Suppress posts by the “ugly”
norsafety.Oursamplingmethodmeantweobtainedlimited and poor. https://theintercept.com/2020/03/16/tiktok-app-
datafromindividualswithexperienceofcensorship.Anal- moderators-users-discrimination/.
ternativemethodwouldbetotargetcreatorsonTikTokwho Brown, A. 2021. TikTok influencer of color
reportcensorship,orcreatevideostopromotethestudy,but faced ‘frustrating’ obstacle trying to add the word“black” to his creator Marketplace Bio. https: Kelion,L.2019. TikToksuppresseddisabledusers’videos.
//www.forbes.com/sites/abrambrown/2021/07/07/tiktok- https://www.bbc.co.uk/news/technology-50645345.
black-creators-creator-marketplace-black-lives-matter/.
King, G.; Pan, J.; and Roberts, M. E. 2014. Reverse-
Burke, P. J.; and Stets, J. E. 2009. Identity Theory. Ox- engineering censorship in China: Randomized experimen-
ford University Press. ISBN 978-0-19-538827-5. DOI: tationandparticipantobservation. Science.
10.1093/acprof:oso/9780195388275.001.0001.
Ko¨ver, C.; and Reuter, M. 2019. Discrimination:
Clark,O.;Lee,M.M.;Jingree,M.L.;O’Dwyer,E.;Yue,Y.; Tiktok Curbed Reach for people with disabilities.
Marrero, A.; Tamez, M.; Bhupathiraju, S. N.; and Mattei, https://netzpolitik.org/2019/discrimination-tiktok-curbed-
J. 2021. Weight Stigma and Social Media: Evidence and reach-for-people-with-disabilities/.
PublicHealthSolutions. FrontiersinNutrition,8.
Lorenz, T. 2022. Internet “Algospeak” is changing our
Cobbe, J. 2021. Algorithmic Censorship by Social Plat- language in real time, from “nip nops” to “le dollar
forms: Power and Resistance. Philosophy & Technology, bean”. https://www.washingtonpost.com/technology/2022/
34(4):739–766. 04/08/algospeak-tiktok-le-dollar-bean.
Ehsan,U.;Singh,R.;Metcalf,J.;andRiedl,M.2022. The Lu,X.;Lu,Z.;andLiu,C.2020. ExploringTikTokUseand
Algorithmic Imprint. In Proceedings of the 2022 ACM Non-usePracticesandExperiencesinChina. InMeiselwitz,
ConferenceonFairness,Accountability,andTransparency, G.,ed.,SocialComputingandSocialMedia.Participation,
FAccT ’22, 1305–1317. New York, NY, USA: Association User Experience, Consumer Experience, and Applications
forComputingMachinery. ISBN978-1-4503-9352-2. of Social Computing, Lecture Notes in Computer Science,
Frick, N. R.; Wilms, K. L.; Brachten, F.; Hetjens, T.; 57–70.Cham:SpringerInternationalPublishing.ISBN978-
Stieglitz,S.;andRoss,B.2021. Theperceivedsurveillance 3-030-49576-3.
of conversations through smart devices. Electronic Com- Lyu, Y.; Cai, J.; Callis, A.; Cotter, K.; and Carroll, J. M.
merceResearchandApplications,47:101046. 2024a.”IGotFlaggedforSupposedBullying,EvenThough
Frier, S. 2022. Why Facebook, Instagram look like Tik- It Was in Response to Someone Harassing Me About My
Tok. https://www.bloomberg.com/news/articles/2022-07- Disability.”:AStudyofBlindTikTokers’ContentModera-
27/why-facebook-instagram-look-like-tiktok. tionExperiences. InProceedingsoftheCHIConferenceon
HumanFactorsinComputingSystems,CHI’24.NewYork,
Ghaffary, S. 2021. How TikTok’s hate speech detec-
NY, USA: Association for Computing Machinery. ISBN
tion tool set off a debate about racial bias on the app.
9798400703300.
https://www.vox.com/recode/2021/7/7/22566017/tiktok-
black-creators-ziggi-tyler-debate-about-black-lives- Lyu, Y.; Cai, J.; Callis, A.; Cotter, K.; and Carroll, J. M.
matter-racial-bias-social-media?orgid=305. 2024b.“IGotFlaggedforSupposedBullying,EvenThough
It Was in Response to Someone Harassing Me About My
Grandhi,S.A.;Plotnick,L.;andHiltz,S.R.2019.DoIStay
Disability.”: A Study of Blind TikTokers’ Content Mod-
or Do I Go?: Motivations and Decision Making in Social
eration Experiences. In Proceedings of the CHI Con-
MediaNon-useandReversion. ProceedingsoftheACMon
ference on Human Factors in Computing Systems, 1–15.
Human-ComputerInteraction,3(GROUP):1–27.
ArXiv:2401.11663[cs].
Guyan,K.2022. QueerData:UsingGender,SexandSexu-
Meaney,J.A.;Wilson,S.R.;Chiruzzo,L.;andMagdy,W.
ality Data for Action. Bloomsbury Studies in Digital Cul-
2022. Don’tTakeItPersonally:AnalyzingGenderandAge
tures Ser. London: Bloomsbury Academic. ISBN 978-1-
DifferencesinRatingsofOnlineHumor. InHopfgartner,F.;
350-23072-9.
Jaidka, K.; Mayr, P.; Jose, J.; and Breitsohl, J., eds., Social
Haimson, O. L.; Delmonaco, D.; Nie, P.; and Wegner, A. Informatics, 20–33. Cham: Springer International Publish-
2021. Disproportionate Removals and Differing Content ing. ISBN978-3-031-19097-1.
ModerationExperiencesforConservative,Transgender,and
Mereish, E. H.; Katz-Wise, S. L.; and Woulfe, J. 2017.
BlackSocialMediaUsers:MarginalizationandModeration
Bisexual-Specific Minority Stressors, Psychological Dis-
GrayAreas. ProceedingsoftheACMonHuman-Computer
tress,andSuicidalityinBisexualIndividuals:theMediating
Interaction,5(CSCW2):1–35.
RoleofLoneliness. PreventionScience,18(6).
Harwell,D.2022. HowTikTokatetheinternet. TheWash-
MyersWest,S.2018. Censored,suspended,shadowbanned:
ingtonPost.
User interpretations of content moderation on social media
HypeAuditor. 2022. Distribution of TikTok audiences in platforms. NewMedia&Society,20(11):4366–4383.
the United Kingdom (UK) in 2021, by age and gender.
Ohlheiser, A. 2020. TikTok has become the soul of the
https://www.statista.com/statistics/1147635/distribution-of-
LGBTQInternet. WashingtonPost.
tiktok-influencer-audience-by-age-and-gender-uk/.
Ohlheiser, A. 2021. Welcome to TikTok’s end-
Karizat, N.; Delmonaco, D.; Eslami, M.; and Andalibi, N.
less cycle of censorship and mistakes. https:
2021. AlgorithmicFolkTheoriesandIdentity:HowTikTok
//www.technologyreview.com/2021/07/13/1028401/tiktok-
UsersCo-ProduceKnowledgeofIdentityandEngageinAl-
censorship-mistakes-glitches-apologies-endless-cycle/.
gorithmicResistance. ProceedingsoftheACMonHuman-
ComputerInteraction,5(CSCW2):305:1–305:44. ONS.2022. Census2021. https://www.ons.gov.uk/census.Ryan, F.; Fritz, A.; and Impiombato, D. 2020. TikTok and
WeChat-Curatingandcontrollingglobalinformationflows.
37/2020.
Shen, H.; DeVos, A.; Eslami, M.; and Holstein, K. 2021.
Everyday algorithm auditing: Understanding the power of
everyday users in surfacing harmful algorithmic behaviors.
Proceedings of the ACM on Human-Computer Interaction,
5(CSCW2):1–29.
Simpson,E.;andSemaan,B.2021. ForYou,orFor”You”?
Everyday LGBTQ+ Encounters with TikTok. Proceedings
of the ACM on Human-Computer Interaction, 4(CSCW3):
252:1–252:34.
Vaterlaus, J. M.; and Winter, M. 2021. TikTok: an ex-
ploratorystudyofyoungadults’usesandgratifications. The
SocialScienceJournal,1–20.
Velkova,J.;andKaun,A.2021.Algorithmicresistance:me-
diapracticesandthepoliticsofrepair.Information,Commu-
nication&Society,24(4):523–540.
Zeng, J.; and Kaye, D. B. V. 2022. From content modera-
tiontovisibilitymoderation:Acasestudyofplatformgov-
ernanceonTikTok. Policy&Internet,14(1):79–95.
Zhou,Z.;Yang,M.;andJin,X.-L.2018. Differencesinthe
ReasonsofIntermittentversusPermanentDiscontinuancein
SocialMedia:AnExploratoryStudyinWeibo. Proceedings
ofthe51stHawaiiInternationalConferenceonSystemSci-
ences,10.24/10/2023, 17:23 Qualtrics Survey Software
Use of TikTok
What is your Prolific ID?
Please note that this response should auto-fill with the correct ID
${e://Field/PROLIFIC_PID}
Do you use TikTok?
Yes I currently use TikTok
I previously used TikTok but am not a current user
I have never used TikTok
When did you first use TikTok?
More than three years ago
Between three and one years ago
Between 12 and 6 months ago
Between six and three months ago
Between three and one month ago
Less than one month ago
When did you stop using TikTok
More than three years ago
Between three and one years ago
Between 12 and 6 months ago
Between six and three months ago
Between three and one month ago
Less than one month ago
What were your reasons for stopping using TikTok?
To have more free time
The format of the app / content
Not enough of my friends were using it
The amount of advertising
To escape harassment / bullying
My account was deleted / banned
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibrar… 4/3224/10/2023, 17:23 Qualtrics Survey Software
Too much moderation / censorship
Too much data use / phone storage use
Concerns about security / privacy
It was negatively impacting my quality of life
A loved one asked me to stop using the app
The content wasn't entertaining anymore
The content was too negative
Other
On average, how often did/do you use TikTok to view content?
I have never viewed content
0-3 times a month
1-2 times a week
Multiple times a week
1-2 times a day
>3 times a day
On average, how often did/do you use TikTok to post content?
I have never posted content
0-3 times a month
1-2 times a week
Multiple times a week
1-2 times a day
>3 times a day
On average, how long did/do you spend on TikTok on the days that you use(d) the
app?
Less than 15 minutes a day
Between 15 and 30 minutes
Between 30 minutes and 1 hour
Between 1 and 2 hours
Between 2 and 4 hours
More than four hours a day
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibrar… 5/3224/10/2023, 17:23 Qualtrics Survey Software
What type of content have you viewed on TikTok (select all that apply)
Telling jokes Exercise
Telling stories Expose someone
Completing sound trend/ challenge Duet
Poster making fun of themselves Show off body (e.g. flexing, revealing
clothing)
Prank Sharing personal information
Dance Politics
Learn something Tribute to recently deceased person
Beauty/makeup Pets
Lip Sync Other (please describe)
Singing (real voice)
What type of content have you posted on TikTok (select all that apply)
Telling jokes Exercise
Telling stories Expose someone
Completing sound trend/ challenge Duet
Poster making fun of themselves Show off body (e.g. flexing, revealing
clothing)
Prank Sharing personal information
Dance Politics
Teach something Tribute to recently deceased person
Beauty/makeup Pets
Lip Sync Other (please describe)
Singing (real voice)
Did/do you view content in English on TikTok?
Yes, exclusively in English
Yes, in addition to other languages
No
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibrar… 6/3224/10/2023, 17:23 Qualtrics Survey Software
What languages do/did you view content on TikTok in? List in order of most to least
content viewed in that language.
Did/do you post content in English on TikTok?
Yes, exclusively in English
Yes, in addition to other languages
No
What languages do/did you post content on TikTok in? List in order of most to least
content posted in that language.
Please note for the remainder of this survey we are interested in your use of TikTok to
view and post English language content.
Removal and suppression of content
We are going to ask you about your experience of algorithmic censorship by TikTok.
On TikTok, some users are concerned about algorithmic censorship, which refers to
when a computer program or algorithm designed to moderate content removes or
suppresses the content they post i.e. when a video gets very few views and you
believe it is because it is being suppressed by the platform.
In this study, when we say censorship we mean both when content is removed and
when content is suppressed.
Have you had content removed by TikTok?
No
Yes
Please select which of the types of content you have had removed by the platform.
I have had posts removed that contained (you can select multiple):
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibrar… 7/3224/10/2023, 17:23 Qualtrics Survey Software
Content relating to minority identity experience i.e. queer content, content about
Black experiences
Content insulting or criticizing dominant group (e.g., men, white people)
Sex related content for a non-erotic purpose i.e. that is intended to educate
Curse words
Content some may find offensive or inappropriate
Content about violence that is not intended to shock or disgust i.e. reporting on a
violent crime
Sex related content for an erotic purpose
Hate speech
Covid-related content
Content about violence that is intended to shock or disgust
Self-referential use of slur i.e. d*ke by a lesbian
Political content
Content relating to a social justice movement, for example feminism or anti-racism
Other (please describe)
Please select how frequently your posts get removed when you post this kind of
content:
Never (this Always (all
kind of my content
content is about this
never About half Most of the topic is
removed) Sometimes the time time removed)
Content some may find
offensive or
inappropriate
Content about violence
that is intended to shock
or disgust
Political content
Hate speech
Content relating to
minority identity
experience i.e. queer
content, content about
Black experiences
Curse words
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibrar… 8/3224/10/2023, 17:23 Qualtrics Survey Software
Never (this Always (all
kind of my content
content is about this
never About half Most of the topic is
removed) Sometimes the time time removed)
Sex related content for a
non-erotic purpose i.e.
that is intended to
educate
Sex related content for
an erotic purpose
Covid-related content
Content relating to a
social justice movement,
for example feminism or
anti-racism
Content about violence
that is not intended to
shock or disgust i.e.
reporting on a violent
crime
Self-referential use of
slur i.e. d*ke by a lesbian
Content insulting or
criticizing dominant
group (e.g., men, white
people)
Other
Are you paying attention?
No
Yes
Do you believe you have had content suppressed by TikTok?
No
Yes
Please select which of the types of content you believe have been suppressed by the
platform.
I believe I have had posts suppressed that contained (you can select multiple):
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibrar… 9/3224/10/2023, 17:23 Qualtrics Survey Software
Content about violence that is not intended to shock or disgust i.e. reporting on a
violent crime
Content insulting or criticizing dominant group (e.g., men, white people)
Content some may find offensive or inappropriate
Self-referential use of slur i.e. d*ke by a lesbian
Hate speech
Sex related content for an erotic purpose
Covid-related content
Curse words
Content about violence that is intended to shock or disgust
Content relating to a social justice movement, for example feminism or anti-racism
Political content
Content relating to minority identity experience i.e. queer content, content about
Black experiences
Sex related content for a non-erotic purpose i.e. that is intended to educate
Other (please describe)
Please select how frequently you believe your posts get suppressed when you post
this kind of content:
Never (this Always (all
kind of my content
content is about this
never About half Most of the topic is
removed) Sometimes the time time removed)
Content about violence
that is intended to shock
or disgust
Hate speech
Content about violence
that is not intended to
shock or disgust i.e.
reporting on a violent
crime
Sex related content for
an erotic purpose
Covid-related content
Sex related content for a
non-erotic purpose i.e.
that is intended to
educate
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 10/3224/10/2023, 17:23 Qualtrics Survey Software
Never (this Always (all
kind of my content
content is about this
never About half Most of the topic is
removed) Sometimes the time time removed)
Content some may find
offensive or
inappropriate
Content relating to
minority identity
experience i.e. queer
content, content about
Black experiences
Curse words
Content relating to a
social justice movement,
for example feminism or
anti-racism
Self-referential use of
slur i.e. d*ke by a lesbian
Content insulting or
criticizing dominant
group (e.g., men, white
people)
Political content
Other
We are interested in how algorithmic censorship make you feel. Please rate the extent
to which you agree with the following statements. Algorithmic censorship makes me
feel:
Neither
Strongly Somewhat agree nor Somewhat Strongly
disagree disagree disagree agree agree
1 2 3 4 5
disgust, distaste,
revulsion
angry, irritated,
annoyed
amused, fun-loving,
silly
grateful,
appreciative,
thankful.
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 11/3224/10/2023, 17:23 Qualtrics Survey Software
Neither
Strongly Somewhat agree nor Somewhat Strongly
disagree disagree disagree agree agree
1 2 3 4 5
embarrassed, self-
conscious, blushing
glad, happy, joyful
scared, fearful,
afraid
awe, wonder,
amazement
ashamed,
humiliated,
disgraced
content, serene,
peaceful
.
Neither
Strongly Somewhat agree nor Somewhat Strongly
disagree disagree disagree agree agree
1 2 3 4 5
contemptuous,
scornful, disdainful
hopeful, optimistic,
encouraged
stressed, nervous,
overwhelmed
repentant, guilty,
blameworthy
love, closeness,
trust
inspired, uplifted,
elevated
proud, confident,
self-assured
sad, downhearted,
unhappy
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 12/3224/10/2023, 17:23 Qualtrics Survey Software
Neither
Strongly Somewhat agree nor Somewhat Strongly
disagree disagree disagree agree agree
1 2 3 4 5
interested, alert,
curious
hate, distrust,
suspicion
Awareness of obfuscation
Some TikTok users are concerned about algorithmic censorship, which as we stated
earlier refers to when an algorithm removes or suppresses the content they post. For
example, users are concerned that the moderation system may censor content which
includes words such as "lesbian" or "dead" in the text in the video or in the captions.
To avoid such censorship, some users employ different techniques to conceal what
they are writing about. For example, users might write "le$bean" instead of "lesbian".
This has been called "algospeak" and can be thought of as a kind of code, but we will
use the term "obfuscation" which is used in research. People obfuscate certain words
to conceal what they are saying to avoid algorithmic censorship.
Here are some examples of the techniques being used.
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 13/3224/10/2023, 17:23 Qualtrics Survey Software
User substitutes numbers and symbols for letters.
User substitutes a symbol for a letter. They also use the hashtag
#alphabetmafia instead of #lgbtcommunity
User has changed the spelling of "white" to "yt".
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 14/3224/10/2023, 17:23 Qualtrics Survey Software
User has substituted the word "sex" for "shrek's".
User has changed the word "motherfucker" with some sound changes and
repeated letters.
Researchers have identified 7 key obfuscation techniques used by English language
users (Calhoun & Fawcett, 2022). We will give specific examples of each technique
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 15/3224/10/2023, 17:23 Qualtrics Survey Software
below, taken from this research.
Users may also use these techniques for other reasons, such as making sure only
certain TikTok users can understand what they are saying. We will ask about these
motivations in the next section.
IMPORTANT: On mobile, please scroll right to see the explanations and examples
which will help you understand these techniques.
via GIPHY
Non-
Explanation / description Examples
technical Label
Numbers, symbols, diacritics
Gay >
[marks added to letters like for é ],
Use of non-letters smoke crack > sm o
emojis, spaces (including leaving
ke cr a ck
the word "blank")
Innovative
Combining words and meaningful dead > unalive [un +
subword
parts of words in new ways alive]
combinations
LGBTQ+ community
Word substitution Replacing words with those with > alphabet mafia
(meaning or similar meanings or which sound faggot > baguette
sound) similar Sex work > shrek
work
sexy > seggsy,
Spelling out a mispronounced sessy
Sound changes
version of the word drugs > droogs
faggot > fuhgoot
Spelling out the word with circumcised >
homonyms sircomesized
Spelling changes LGBT > leg booty
Spelling out an initialism/acronym LGBT > los
as words jibbities
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 16/3224/10/2023, 17:23 Qualtrics Survey Software
Replacing word with one with the
homophobic >
same number of syllables, with
Word substitution hookedonphonics
the emphasis put on matching
(structure) homophobia >
syllables, and with some similar
cornucopia
sounds
Sound changes mother fucker >
Changing consonants to match
with repetition mugga chugga
Replacing word with phrase
following the vowel pattern in "flip
Flip-flop version nipples > nip nops
flop", "hip hop", "tip top" (or "zig
zag", "tic tac" etc)
Which of these techniques have you seen others using?
Sound changes with repetition (mother
Use of non-letters (gay > )
fucker > mugga chugga)
Innovative subword combinations (unalive Flip-flop version (nipples > nip nops)
[un + alive])
Word substitution (meaning or sound) (sex Other (please describe)
work > shrek work)
Sound changes (sexy > seggsy) None
Spelling changes (LGBT > leg booty) Unsure
Word substitution (structure) (homophobia
> cornucopia)
How often do you understand what people's real intended meaning is when they use
these techniques?
Never
Sometimes
About half the time
Most of the time
Always
Which of these techniques have you used yourself when you have posted content?
Word substitution (structure) (homophobia
Use of non-letters (gay > )
> cornucopia)
Innovative subword combinations (unalive Sound changes with repetition (mother
[un + alive]) fucker > mugga chugga)
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 17/3224/10/2023, 17:23 Qualtrics Survey Software
Word substitution (meaning or sound) (sex Flip-flop version (nipples > nip nops)
work > shrek work)
Sound changes (sexy > seggsy) Other (please describe)
Spelling changes (LGBT > leg booty) None
How effective do you consider these techniques to be for avoiding algorithmic
censorship?
Not
effective at Slightly Moderately Very Extremely
all effective effective effective effective
1 2 3 4 5
Use of non-letters
(gay > )
Innovative subword
combinations
(unalive [un + alive])
Word substitution
(meaning or sound)
(sex work > shrek
work)
Sound changes
(sexy > seggsy)
Spelling changes
(LGBT > leg booty)
Word substitution
(structure)
(homophobia >
cornucopia)
Sound changes with
repetition (mother
fucker > mugga
chugga)
Flip-flop version
(nipples > nip nops)
Is there anything you would like to add? If you have mentioned any additional
techniques you have seen or used above, please indicate how effective you think they
are for evading algorithmic censorship.
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 18/3224/10/2023, 17:23 Qualtrics Survey Software
Feelings about obfuscation use
We are interested in how obfuscation techniques (when users conceal what they are
writing about) make you feel. Please rate the extent to which you agree with the
following statements.
Using or seeing obfuscation techniques makes me feel:
Neither
Strongly Somewhat agree nor Somewhat Strongly
disagree disagree disagree agree agree
1 2 3 4 5
glad, happy, joyful
scared, fearful,
afraid
ashamed,
humiliated,
disgraced
awe, wonder,
amazement
amused, fun-loving,
silly
angry, irritated,
annoyed
disgust, distaste,
revulsion
content, serene,
peaceful
embarrassed, self-
conscious, blushing
grateful,
appreciative,
thankful.
.
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 19/3224/10/2023, 17:23 Qualtrics Survey Software
Neither
Strongly Somewhat agree nor Somewhat Strongly
disagree disagree disagree agree agree
1 2 3 4 5
sad, downhearted,
unhappy
interested, alert,
curious
contemptuous,
scornful, disdainful
stressed, nervous,
overwhelmed
love, closeness,
trust
hate, distrust,
suspicion
hopeful, optimistic,
encouraged
repentant, guilty,
blameworthy
proud, confident,
self-assured
inspired, uplifted,
elevated
Motivations (own use)
We are going to ask you about the types of content you post. Please select how
frequently you post content about:
Never (I
never post Always (all
content About half Most of the my content
about this) Sometimes the time time is about this)
Content about
violence that is not
intended to shock or
disgust i.e. reporting
on a violent crime
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 20/3224/10/2023, 17:23 Qualtrics Survey Software
Never (I
never post Always (all
content About half Most of the my content
about this) Sometimes the time time is about this)
Sex related content
for an erotic purpose
Self-referential use of
slur i.e. d*ke by a
lesbian
Content insulting or
criticizing dominant
group (e.g., men,
white people)
Covid-related content
Curse words
Political content
Content relating to a
social justice
movement, for
example feminism or
anti-racism
Hate speech
Content about
violence that is
intended to shock or
disgust
Content relating to
minority identity
experience i.e. queer
content, content
about Black
experiences
Sex related content
for a non-erotic
purpose i.e. that is
intended to educate
Content some may
find offensive or
inappropriate
What other types of content do you post about? Please indicate how frequently you
post this type of content.
We are going to ask you about your use of the obfuscation techniques we introduced
you to earlier.
Please select how strongly you agree with each of the following statements relating to
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 21/3224/10/2023, 17:23 Qualtrics Survey Software
the types of content you are posting when you use obfuscation techniques.
I use obfuscation techniques when I post:
About half Most of the
Never Sometimes the time time Always
Political content
Content some may
find offensive or
inappropriate
Sex related content
for a non-erotic
purpose i.e. that is
intended to educate
Sex related content
for an erotic purpose
Covid-related content
Content insulting or
criticizing dominant
group (e.g., men,
white people)
Content relating to a
social justice
movement, for
example feminism or
anti-racism
Content relating to
minority identity
experience i.e. queer
content, content
about Black
experiences
Hate speech
Curse words
Self-referential use of
slur i.e. d*ke by a
lesbian
Content about
violence that is not
intended to shock or
disgust i.e. reporting
on a violent crime
Content about
violence that is
intended to shock or
disgust
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 22/3224/10/2023, 17:23 Qualtrics Survey Software
Are there any other types of content you post where you use obfuscation techniques?
Please indicate how frequently you use obfuscation techniques for this kind of content
We are interested in your motivations for using obfuscation techniques when posting
content. Please select how strongly you agree with the following statements.
I use obfuscation techniques:
Neither
Strongly Somewhat agree nor Somewhat Strongly
disagree disagree disagree agree agree
1 2 3 4 5
So only certain
TikTok users can
understand me
To show which
communities I
belong to
Because it's fun
So the algorithm
does not censor my
posts
To feel part of the
community of TikTok
users
To express my
creativity
So human
moderators can't
understand what I
am writing
To protest
algorithmic
moderation
Select somewhat
agree
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 23/3224/10/2023, 17:23 Qualtrics Survey Software
Is there anything else you would like to tell us about when and why you use
obfuscation techniques?
Beliefs about algorithmic censorship
Please select how strongly you agree with each of the following statements relating to
your perception of algorithmic censorship by the platform. Please rate to what extent
you agree with the following statements.
The TikTok moderation algorithm censors at least some posts about:
Neither
Strongly Somewhat agree nor Somewhat Strongly
disagree disagree disagree agree agree
1 2 3 4 5
Political content
Hate speech
Curse words
Content insulting or
criticizing dominant
group (e.g., men,
white people)
Content some may
find offensive or
inappropriate
Content relating to
minority identity
experience i.e. queer
content, content
about Black
experiences
Content relating to a
social justice
movement, for
example feminism or
anti-racism
Self-referential use
of slur i.e. d*ke by a
lesbian
Covid-related
content
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 24/3224/10/2023, 17:23 Qualtrics Survey Software
Neither
Sex related content
Strongly So mewhat agree nor Somewh at Strongly
for an erotic purpose
disagree disagree disagree agree agree
Content about 1 2 3 4 5
violence that is
intended to shock or
disgust
Content about
violence that is not
intended to shock or
disgust i.e. reporting
on a violent crime
Sex related content
for a non-erotic
purpose i.e. that is
intended to educate
Is there anything that you would like to add about what you believe about the TikTok
moderation algorithm?
Beliefs about community guidelines
Please select how strongly you agree with each of the following statements relating to
your knowledge of the TikTok community guidelines.
TikTok community guidelines do not allow posts about:
Neither
Strongly Somewhat agree nor Somewhat Strongly
disagree disagree disagree agree agree
1 2 3 4 5
Covid-related
content
Political content
Hate speech
Content some may
find offensive or
inappropriate
Sex related content
for a non-erotic
purpose i.e. that is
intended to educate
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 25/3224/10/2023, 17:23 Qualtrics Survey Software
Neither
Strongly Somewhat agree nor Somewhat Strongly
disagree disagree disagree agree agree
1 2 3 4 5
Content insulting or
criticizing dominant
group (e.g., men,
white people)
Sex related content
for an erotic purpose
Content relating to a
social justice
movement, for
example feminism or
anti-racism
Curse words
Content relating to
minority identity
experience i.e. queer
content, content
about Black
experiences
Content about
violence that is
intended to shock or
disgust
Content about
violence that is not
intended to shock or
disgust i.e. reporting
on a violent crime
Self-referential use
of slur i.e. d*ke by a
lesbian
Select somewhat
disagree
Is there anything that you would like to add about what you believe about the TikTok
community guidelines?
We will now ask you some questions about your thoughts on TikTok's moderation
process (both algorithmic and human moderation).
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 26/3224/10/2023, 17:23 Qualtrics Survey Software
Sometimes users feel that content that seems to be in line with the community
guidelines is censored. For example, the TikTok community guidelines do not disallow
content about LGBT+ individuals (so long as this content does not violate their other
policies such as the banning of sexually explicit content). However, LGBT+ creators
talk about their content being suppressed or removed (Haimson et al., 2022; Lorenz
2022; Zeng and Kaye, 2022). We are interested in your beliefs about what is occurring
when content that does not seem to violate the community guidelines is being
removed.
When content is suppressed or removed that does not go against the community
guidelines, this is because:
Neither
Strongly Somewhat agree nor Somewhat Strongly
disagree disagree disagree agree agree
1 2 3 4 5
TikTok has
unpublished
guidelines that are
used to train the
algorithm which are
stricter
TikTok has
unpublished
guidelines that are
given to human
moderators which
are stricter
The algorithm has
misunderstood the
content / made a
one-time mistake
Other user(s) have
reported the content
Human moderators
have their own
opinions about what
should be allowed
on the platform
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 27/3224/10/2023, 17:23 Qualtrics Survey Software
Neither
Strongly Somewhat agree nor Somewhat Strongly
disagree disagree disagree agree agree
1 2 3 4 5
The algorithm has
not learned to follow
the guidelines (it is
not a good algorithm
overall)
Do you think there are any other reasons why this occurs (content that is in line with
the community guidelines being removed by the algorithmic censorship system).
Demographic
What is your age? (Please give number of years)
What is your gender identity?
Male
Female
Non-binary
Other
Prefer not to say
Do you identify as trans?
Yes
No
Prefer not to say
What is your sexuality?
Straight / heterosexual
Gay / homosexual / lesbian
Bisexual / pansexual / bi+
Asexual / Acespec
Other
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 28/3224/10/2023, 17:23 Qualtrics Survey Software
Prefer not to say
Do you consider yourself to be disabled?
No
Yes
Prefer not to say
What is your ethnic group?
Asian or Asian British
Black, African, Black British or Caribbean
Mixed or multiple ethnic groups
White
Another ethnic group (please specify)
Prefer not to say
What religious group do you belong to?
Buddhist
Christian
Hindu
Jewish
Muslim
Sikh
Other (please specify)
No religion
Prefer not to say
Political beliefs can be thought of on a spectrum from left to right. Where would you
place yourself on this spectrum?
Left
Center left
Center
Center right
Right
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 29/3224/10/2023, 17:23 Qualtrics Survey Software
I'm not sure
Prefer not to say
Think of this ladder as representing where people stand in the UK.
At the top of the ladder are the people who are the best off – those who have the most
money, the most education, and the most respected jobs. At the bottom are the
people who are the worst off – those who have the least money, least education, the
least respected jobs, or no job. The higher up you are on this ladder, the closer you are
to the people at the very top; the lower you are, the closer you are to the people at the
very bottom.
Where would you place yourself on this ladder?
Please indicate with a number (1-10) on which rung you think you stand at this time in
your life relative to other people in the UK.
Do you identify as belonging to any other minority or marginalised group e.g. people
with HIV, sex workers, care leavers, from a Gypsy, Roma & Traveller community etc
"Social identity" has been used to describe aspects of your identity related "to
belonging to a certain social group, such as a certain race, gender or class" (Karizat et
al., 2021).
We are interested in how connected to your social identity communities you feel on
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 30/3224/10/2023, 17:23 Qualtrics Survey Software
TikTok.
On TikTok...
Neither
Strongly Somewhat agree nor Somewhat Strongly
disgree disagree disagree agree agree
1 2 3 4 5
I feel like I am part of
my communities
I feel a connection
with other members
of my communities
I feel accepted by
other members of
my community
I feel repsected by
other members of
my communities
I feel valued by other
members of my
communities
What is/are your first language(s)? (learned before age 5)
(cid:55)(cid:86)(cid:94)(cid:76)(cid:89)(cid:76)(cid:75)(cid:3)(cid:73)(cid:96)(cid:3)(cid:56)(cid:92)(cid:72)(cid:83)(cid:91)(cid:89)(cid:80)(cid:74)(cid:90)
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 31/3224/10/2023, 17:23 Qualtrics Survey Software
https://edinburghinformatics.eu.qualtrics.com/Q/EditSection/Blocks/Ajax/GetSurveyPrintPreview?ContextSurveyID=SV_aY4mdRl8ZkypgZ8&ContextLibra… 32/32