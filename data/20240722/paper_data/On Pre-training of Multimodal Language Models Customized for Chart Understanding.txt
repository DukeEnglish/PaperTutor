On Pre-training of Multimodal Language Models
Customized for Chart Understanding
Wan-CyuanFan1,3∗ Yen-ChunChen2 MengchenLiu2 LuYuan2 LeonidSigal1,3,4
1UniversityofBritishColumbia 2Microsoft
3VectorInstituteforAI 4CIFARAIChair
{wancyuan, lsigal}@cs.ubc.ca
{yen-chun.chen, mengcliu, luyuan}@microsoft.com
Abstract
RecentstudiescustomizingMultimodalLargeLanguageModels(MLLMs)for
domain-specific tasks have yielded promising results, especially in the field of
scientificchartcomprehension. Thesestudiesgenerallyutilizevisualinstruction
tuningwithspecializeddatasetstoenhancequestionandanswer(QA)accuracy
withinthechartdomain. However,theyoftenneglectthefundamentaldiscrepancy
betweennaturalimage-captionpre-trainingdataanddigitalchartimage-QAdata,
particularly in the models’ capacity to extract underlying numeric values from
charts. Thispapertacklesthisoversightbyexploringthetrainingprocessesneces-
sarytoimproveMLLMs’comprehensionofcharts. Wepresentthreekeyfindings:
(1) Incorporating raw data values in alignment pre-training markedly improves
comprehensionofchartdata.(2)Replacingimageswiththeirtextualrepresentation
randomlyduringend-to-endfine-tuningtransferthelanguagereasoningcapability
tochartinterpretationskills. (3)Requiringthemodeltofirstextracttheunderlying
chart data and then answer the question in the fine-tuning can further improve
theaccuracy. Consequently,weintroduceCHOPINLLM,anMLLMtailoredfor
in-depthchartcomprehension. CHOPINLLMeffectivelyinterpretsvarioustypes
ofcharts,includingunannotatedones,whilemaintainingrobustreasoningabilities.
Furthermore,weestablishanewbenchmarktoevaluateMLLMs’understanding
of different chart types across various comprehension levels. Experimental re-
sultsshowthatCHOPINLLMexhibitsstrongperformanceinunderstandingboth
annotatedandunannotatedchartsacrossawiderangeoftypes.
1 Introduction
Intoday’sdata-drivenworld,visualizationslikebarandpiechartsarecrucialfordecipheringcomplex
datasets. However,theincreasingdiversityandcomplexityofthesechartshighlightstheneedfor
advancedtoolstoenhancehumancapabilitiesindataanalysis.ArtificialIntelligence(AI),particularly
MultimodalLargeLanguageModels(MLLMs),isincreasinglyusedtoautomatetheunderstanding
of scientific charts, promising more efficient and accurate analysis. Robust benchmarks are also
essential,settingstandardsandmetricsthatdrivethedevelopmentandevaluationoftheseAItools.
Prior studies have introduced end-to-end neural models aimed at enhancing chart comprehen-
sion [29, 31, 67], such as masked table prediction [67], chart question answering [38], and chart
de-rendering[31]. Thesemodelseachisspecializedinhandlingonetaskwithinthedomainofchart
analysis. Furthermore,advancementsinMultimodalLargeLanguageModels(MLLMs),exemplified
byLLaVA[33,34]andminiGPT[68], haveshowcasedtheirversatilityinvision-languagetasks.
∗TheworkisdoneduringaninternshipatMicrosoft.
Preprint.Underreview.
4202
luJ
91
]VC.sc[
1v60541.7042:viXraPredictions by human Annotated chart images Predictions by MLLMs
GPT4V Gemini
A A B B C C
2021 2022 2021 2022 2021 2022
120 150 140 130 100 110 A A B B C C A A B B C C
2021 2022 2021 2022 2021 2022 2021 2022 2021 2022 2021 2022
120 150 140 130 100 110 120 140 105 130 85 110
ChartLlama Ours
A A B B C C A A B B C C
2021 2022 2021 2022 2021 2022 2021 2022 2021 2022 2021 2022
120 150 140 130 100 110 120 150 140 130 100 110
Can you extract the underlying
data for this chart?
Non-annotated chart images
GPT4V Gemini
A A B B C C A A B B C C
2021 2022 2021 2022 2021 2022 2021 2022 2021 2022 2021 2022
105 125 120 100 85 90 70 90 110 110 130 140
ChartLlama Ours
A A B B C C A A B B C C A A B B C C
000001_script_matplotlib_088.png 2021 20222021 2022 2021 2022 2021 2022 2021 2022 2021 2022 2021 2022 2021 2022 2021 2022
120 150 140 130 100 110
- - - - - - 120 150 140 130 100 110
Annotated chart Question Annotated chart
Extract the underlying GPT-4V
data for this chart Gemini A‘21 A’22 B’21 B‘22 C’21 C’22
ChartLlama 120 150 140 130 100 110
Predictions Predictions Ours
by human by MLLMs
Non-annotated chart Non-annotated chart
auxiliary line GPT-4V
A’21 A’22 B’21 B’22 C’21 C’22
Gemini 105 125 120 100 85 90
ChartLlama
A‘21 A’22 B’21 B‘22 C’21 C’22
Ours
ChartLlama 120 150 140 130 100 110
Figure1: Theunderlyingdatavaluescanbeinferredregardlessofwhetherthechartisannotated.
However,existingMLLMsrelyonannotationsandstrugglewithunannotatedcharts. Incontrast,our
modelbridgesthisfundamentaldiscrepancybetweennaturalimage-captionpre-trainingdataand
digitalchartimage-QAdata,enablingittoextractvaluesregardlessofwhetherthechartisannotated.
These generalist models undergo a two-stage training process: initially learning visual-language
alignmentthroughimage-captionpairs,followedbyend-to-endfine-tuningusingimage-QApairs.
ThistrainingnotonlyenablesLLMstointerpretvisualdatabutalsoretainstheirextensivepre-trained
knowledge,whichsupportstheirreasoningabilitiesandleadstostrongperformanceacrossdiverse
visuallanguageunderstandingtasks.
RecentadvancementshavefurtherignitedinterestintailoringMLLMstospecializeddomainssuch
asscientificchartunderstanding. Hanetal.[22],Liuetal.[32]haveexploredcollectinginstruction-
tunedchartdataandlow-rankadaptation[24]toenhanceMLLMs’proficiencywithuniquechart
characteristics,researchonthefundamental-trainingregimes–A namAely,Bpre-trBa ininCgtoCalignacross
modalities and comprehensive end-to-end fine-tuning – for2c0h21ar2t-0s2p2e2c0ifi21c2u0n2d2e2r0s2ta1n2d0i2n2g remains
scarce. AsshowninFig.1,existingMLLMsoftenstrGuPgTg4lVetoe12x0trac1t50the1u40nde1r3l0yin1g00dat1a1f0romcharts A A B B C C
Gemini 120 140 105 130 85 110 2021 2022 2021 2022 2021 2022
whennumericalvaluesarenotannotated. Wehypothesizethatthisissuestemsfromagapinvision-
ChartLlama 120 150 140 130 100 110 GPT4V 105 125 120 100 85 90
languagealignmentbetweennaturalimage-captionpaiOrsurasnddi1g2i0tal1c5h0art1-d40ata1p3a0irs1.0W0 it1h1o0uttargeted Gemini 70 90 110 110 130 140
pre-trainingforchart-dataalignment,modelsmayresorttorelyingona“shortcut”ofrecognizing ChartLlama todo todo todo todo todo todo
numericannotationsthroughOCRduringfine-tuningwithQApairs,ratherthantrulyunderstanding Ours 120 150 140 130 100 110
thevisualsubtletiesofdiversecharts.
Thispaperaddressestheaboveissuesbyconcentratingontheessentialtrainingmethodologiesfor
MLLMs,includingcross-modalfeaturealignmentpre-trainingandcomprehensiveend-to-endfine-
tuning. Ourresearchisguidedbythequestion,“HowsignificantlydoesfundamentalMLLMtraining
influence the enhancement of general MLLMs with chart-specific domain understanding?” Our
findingsindicatethat: (1)Rawdataextractionarepivotalinalignmentpre-trainingtobolsterchart
datacomprehension;(2)Substitutingsomechartimageswithpurelytextualdataduringend-to-end
fine-tuningnotonlypreservesLLM’stext-onlyreasoningabilitybutalsoaugmentschartinterpretation
capabilities;(3)AugmentingQAswithdataextractiontasksinthefine-tuningphaseallowsmodelto
achievethedatapromptingduringtesting,whereitfirstextractdataandthenanswertheQAs,further
improvingthecorrectnessofitsreasoningskills. Furthermore,existingchartbenchmarksarelimited
inchartandquestiontypes.Thispromptourintroductionofacomprehensivechartbenchmark,which
includes 18 chart types and three QA levels to better measure MLLM performance and advance
futureresearchinthisfield.
Ourkeycontributionsaresummarizedasfollows:
• WeintroduceCHOPINLLM,2aMultimodalLargeLanguageModeltailoredforcomprehensive
chartunderstanding. Thismodelexcelsatinterpretingvariouscharttypesincludingunannotated
ones,underpinnedbyourdetailedanalysisandtrainingguidancethatemphasizestheimportance
offoundationaltrainingforchart-specifictasks.
• Weproposeanoveldatagenerationpipelineusingtext-onlyLargeLanguageModelstoefficiently
producelarge-scalepairwisedata. Thisapproachsignificantlyreducesthecostsandcomplexity
ofdatagenerationforMLLMtraining.
• Weestablisharobustbenchmarkthatincludesadiversearrayofcharttypesandquestionanswering
levels,designedtorigorouslyevaluateMLLMs’abilitiesinchartunderstanding.
2ChartOrientedPretrainingIntegrationinLargeLanguageModels
22 Relatedworks
Largelanguagemodel LargeLanguageModels(LLMs)haveseenremarkableadvancementsin
recentyears,primarilydrivenbytransformers[54]thathavesignificantlyscaledinmodelsizeand
trainingdata[6,9,11,17,19,23,49,51]. Thesemodelsexcelingeneralizedreasoningandexhibit
robustchain-of-thoughtreasoning[55,57,65]acrossavarietyoftasks,largelyattributedtoextensive
pre-training[3,13,66]andfine-tuningstrategies[10,43,45]. Theavailabilityofusingpowerful
LLMswithspecializedcapabilities–rangingfromgeneralassistance[2,20,44,53]tocoding[21,
25,50]–hasfueleddiverseapplicationssuchasdataaugmentation[14],datageneration[46,60],
andprovidingtrainingtrainingguidance[28,61]. Thesedevelopmentshavemarkedlyaccelerated
researchandpracticalapplicationsinthefield.
Multimodallargelanguagemodel BuildingonthesuccessofLLMs,recentresearchhasexpanded
their application to multimodal tasks, including image [34, 35, 40, 64], video [7, 63], audio or
speech[4,12,18],mixed-modal[52],varioustoolandAPIusages[41,47,48],androbotics[5,62].
InextendingLLMstoimagemodalities,earlystudiescombinedLLMswithexternalvisionmodelsto
convertvisualinformationintotext,enhancingimagecomprehension[30,59]. Othershaveintegrated
visual encoders directly within LLM frameworks, developing end-to-end systems that transform
imagesintotextualtokens[1,8,16,33,34,68]. Whilemaintainingcapabilitieslikereasoningand
chain-of-thoughtprocessingacrossvarioustasks,thesemodelsoftenfallshortindomain-specific
taskslikechartanalysis[37,42]. Thispromptsfurtherresearchintospecializeddatacollectionand
fine-tuningfordistinctdomains.
Chartunderstanding Currentapproachestochartunderstandingfallintotwomaincategories:
modelsspecificallydesignedforchart-relatedtasks[29,31,36,38,67],andthosethatutilizepre-
trainedLLMsandMLLMs[22,32,39,58]. Thefirstgroupinvolvesmodelstrainedexclusivelyon
chart-specificdata,oftenlimitedbythescopeofthetrainingdatasetsthuscannotbeappliedtodiverse
chartscenarios. Thesecondgroup,whichinvolvesadaptingexistingLLMsandMLLMsthrough
fine-tuning[34]orintegrationwithexternalmodels[30],showspromisingversatilityacrossvarious
questionsandscenarios. Yet,thereisascarcityofresearchonMLLMs’pre-training,crucialfordeep
chartunderstandingandadaptabilitytomultiplecharttypesinpracticalsettings. Typically,chart
understandingmodelsareevaluatedagainstbenchmarksfocusedontaskslikedataextraction[26,37],
summarization[27],andbasicmathematicalreasoning[42],whichpredominantlyfeaturebasicchart
types(e.g.,bar,line,piecharts)andlacknuanceddifferentiationinQAlevelstothoroughlyassess
models’understandingcapabilities. Addressingthesegaps, ourworknotonlyexploreseffective
pre-trainingstrategiesforMLLMsonchartdatabutalsointroducesanewbenchmarkwithavariety
ofcharttypesanddifferentiatedQAlevels(e.g.,literal,inferential,reasoning)toevaluateMLLMs’
comprehensiveabilities.Concurrently,CharXiv[56]isproposedforevaluatinggeneralunderstanding
ofreal-worldscientificcharts,includingcomplexcompositionswithmultiplesubplots. Incontrast,
our benchmark focuses on single-plot chart images, evaluating the raw data understanding and
mathematicalreasoningofanMLLM.
3 Generatingdataforchartunderstanding
TobuildachartunderstandingMLLMandstudyitsfundamentaltrainingprocess,acomprehensive
dataset containing chart images paired with captions and raw data is essential for pre-training,
alongsidedifferenttypesofquestion-answerpairsforend-to-endfine-tuning. However,noexisting
dataset provides the necessary variety of chart types, topics, and styles. To bridge this gap, we
introduceanoveldatagenerationpipelineforlarge-scalechartdatageneration(Sec.3.1)andQAs
generation(Sec.3.2). Withthedataathand,wethenexplorevarioustrainingstrategiesinthelater
sections,includingfeaturealignmentpre-trainingandend-to-endfine-tuningforLLMs. Figure2
presentsanoverviewofourframework.
3.1 Efficientdatagenerationwithquadraticscaling
Ourdatagenerationleveragesthepromisingtextcontentgenerationandcodingabilitiesoflarge
languagemodels,e.g.,GPT-4,togeneratechartimagesanddata. Specifically,LLMsallowusto
synthesizerawdataforchartimages,andthenthegeneratedPythonscriptturnstherawdataintoa
3chaL ri tn te y pe T X Y D_i _ at tl aae axx: : ii“ {ssA __B ll aaC bb” ee ll :: ““ yx -- aa xx ii ss ”” Da Gta P e Tx -p 4ert cS ht aa rg t-e d e1 s: cF re ipa tt iou nr e p aa il ri sg , n cm hae rtn -t js p onre p- at ir rsaining
{“category”: “”, “value”: 0},
{“category”: “”, “value”: 0}, Chart topics Stage 2: End-to-end fine-tuning
}
Json template General QAs: summary QAs + description QAs +
JSON expert JSON QAs Sum. & Des. three-level QAs (literal, inferential, reasoning)
GPT-4 README Literal QAs Augmented QAs: JSON-only QAs, + data-driven QAs
JSON QAs Inferential QAs Stage 3: Downstream fine-tuning (LoRA)
Generated data Reasoning QAs high-quality downstream task instruction data
(b)
… LLM
Code expert
GPT-4
Generated codes ViT projector prompt
(a) (c)
Figure2: Overviewof(a)theproposeddatagenerationpipelineand(b)Trainingstrategiesof
CHOPINLLM.GeneratingcodeanddatapointsconformingtoasharedJSONtemplateenables
General QAs
quadraticscalingofthedatasizeC(hwar.tr .t.to#GPTcalls)O.utTpuhtes 3-stagetrainingequipsourmodelto
image
grasp the underlying data, thereby achieving a fundamental understanding of charts. (N and M
denotethenumberofgeneratedscriptsanddRaatwa,respectively.)
data
Augmented QAs
chartimage. Inthisway,wecanproduceimagedatawithoutaccessingcostlymultimodalLLMslike
GPT-4V.Unlikepreviousandconcurrentworks[22,58]thatpromptLLMstoiterativelygenerate
CSVdata,QAs,andPythonscriptforeachchartimage–aprocessthatiscostlytomassivelyscale–
ourpipelinefeaturesparallelcodeanddatagenerationthroughsharedtemplatesandREADMEsfor
consistentdefinitionsandformatsacrossthesamecharttypes. Mostimportantly,sinceallcodescript
anddatasharethesamestructure,ourgenerateddatacanbeuniversallyappliedtoanygenerated
codeandviceversa,significantlyenhancingscalabilitywithoutexhaustedlypromptingLLMs. We
detailthepipelinefurtherbelow.
SharedtemplateandREADME AsshowninFig.2(a),givenacharttype(e.g.,line)sampled
fromapredefinedcharttypedatabase,theJSONexpertGPT-4firstgeneratesaJSONtemplatefor
the given chart type, along with a README file. In detail, the JSON template contains general
informationforthechartimage,includingthetitle,x-axis,y-axisinformation,andrawdata. The
README contains the definition of the chart type and the meanings of the keys and values to
enhanceunderstandingoftheJSONtemplate. PleaserefertoAppendixFforsomeexamples. We
notethattheJSONtemplate,togetherwiththeREADME,ensurestheconsistencyofdatageneration
sothatfurtherdataandcodegenerationcanfollowtheexplicitformatanddefinitionguidanceofthe
templatedata. NotethatwechooseJSONasourprimarydatarepresentationformat,incontrastto
previousworks[22,37,42,58],whichusedCSV.TheJSONformatallowsustoincorporatenot
onlynumericaldatabutalsoadditionalchartinformation,suchastitlesandthescalesofxandy
axes,whichisbeneficialforpair-wisepre-trainingtasks. Moreover,JSONdataisstructured,and
whenpairedwithaREADMEfile,itminimizesambiguityindatadescriptions,whichisparticularly
valuableforcomplexcharttypes. Forinstance,incandlestickcharts,wecanclearlydefineadata
pointasadictionarycontaining“open”,“close”,“high”,and“low”values,ratherthanalistwhere
themeaningofeachnumbermightbeunclear.
Orthogonaldataandcodegeneration Withthetemplatefilesathand,wecangeneratedataand
code independently. For the data generation branch, to ensure the generated data covers diverse
topics,wejointlyinputtheproducedtemplatefiles(i.e.,JSONtemplateandREADME)andatopic
sampledfromapre-definedtopicset(e.g.,energyproductionandmarketshare)intoadataexpert
GPT-4module. Forthecompletetopiclist,pleaserefertoAppendixG.Werequirethedataexpert
GPT-4tofollowthedefinitionsinthetemplatefilesandgenerateM JSONdataalongwithdifferent
kindsofquestionsandanswers(e.g.,summaryQA)basedontherawdata. Asforcodegeneration,
anothercodeexpertGPT-4isutilizedtoproduceN Pythoncodebasedonthegivencharttype,data
template,andPythonlibrary. Notethattopreventgeneratingsimplecoderepeatedlyforthegiven
charttype,weexplicitlyaskthecodeexpertGPT-4tointroducevisualvariationsinaspectssuchas
color,legend,grid,font,andmarktexture,etc. Moredetailscanbefoundintheappendix.
4
…Chart Example 1 Literal QA
Tech Giant Quarterly Earnings Report Q: What was the closing earnings per share (EPS) for Q2-2022? Short A: 6.50 USD
Long A: The closing earnings per share (EPS) for Q2-2022 was 6.50 USD.
Inferential QA
Q: Was … closing EPS was lower than the opening EPS? Short A: Yes
Long A: Yes, in Q4-2022 … of 6.40 USD was lower than the opening EPS of 6.50 USD.
Reasoning QA
Q: How much … the lowest of Q1-2022 to the highest of Q1-2023? Short A: 1.80 USD
Quarter Long A: The EPS grew by 1.80 USD … Q1-2022 at 5.10 USD to …Q1-2023 at 6.90 USD.
Chart Example 2 Literal QA
Website Traffic vs Conversion Rate Analysis Q: What is the website traffic from Paid Advertisements? Short A: 150
Long A: According to … Y-axis in the bar chart, the … is 150 (in thousands).
Monthly Inferential QA
Website
Visitors Q: Does Social Media Referrals lead in conversion rate? Short A: Yes
Long A: Yes, Social … at 3.0%, the highest among all categories … line graph.
Monthly Reasoning QA
Conversion
Rate Q: What is the total website traffic from all sources? Short A: 660
Long A: The total … sum of … : 120,000 + 150,000 + 180,000 + 210,000 = 660,000.
Figure3: Examplesofgeneratedthree-levelQAswithlongandshortanswers,accessingthe
understandingofchartsfromvariousperspectives. Bestviewedincolor.
3.2 DiverseQAsynthesis
Basedontheparalleldatagenerationpipeline,weareabletocollectmassiveamountofchartimage
andJSONrawdatapairsforthefeaturealignmentpre-training. Now,wedetailshowwegenerate
differenttypesofQAsforend-to-endfine-tuning. Specifically,havingeachJSONdataasinput,we
usetext-onlyLLMtogeneratequestion-answer(QA)pairs. Tocovervariousquestion-anwserfor
chartdata,weincludegeneralQAs,containingnotonlydescriptionandsummaryQAbutalsothree
differentlevelofQAs: literalQAs, inferentialQAs, andreasoningQAs(asillustratedinFig.3).
Furthermore,toenhancethetrainingofchartunderstanding,weintroducetwoadditionalaugmented
QAs(fortrainingonly): text-onlyQAsanddata-drivenQAs. WedetaileachQAtypeasfollows:
• DescriptionQAs: Generateobjectivedescriptionsbasedonthechartdata.
• SummaryQAs: Summarizethechart,highlightingkeyfindings.
• LiteralQAs: Extractspecificvaluesdirectlyfromthedata.
• InferentialQAs: Inferglobalinsights,suchasidentifyingextremevalues.
• ReasoningQAs: Performcalculationstoderiveanswersfromchartdata.
• JSON-onlyQAs: ReplaceimageswithJSONrawdatatoaugmentedpreviousQAs.
• Data-drivenQAs: PromptthemodeltoextractJSONrawdatabeforeansweringthequestion.
These QAs encompass a range of questions for chart images, covering abilities from basic data
understanding and global concept comprehension to advanced reasoning, allowing us to further
assesstheabilitiesofMLLMs. Notethat,foreachQApair,weuseGPT-4togeneratebothlong
andshortanswers. Thelonganswer,generatedfirst,includesastep-by-stepexplanationtoderive
theanswer,whiletheshortanswer,generatedlater,containsonlythefinalanswerderivedfromthe
longexplanation. ShortanswerscontainonlynumericalvaluesorYes/Noresponseforconvenient
evaluationpurpose. FormoreexamplesofgeneratedchartandQAs,pleaserefertoAppendixJ.
Compositionforquadraticallyscaleddata AsshowninFig.3(a),weconsider18differentchart
types. Foreachcharttype, wecollect400differentPythoncodes(N = 400)and1000different
JSON data files (M = 1000) covering various topics. Note that we exclude bad data based on
predictedfilestructure’scorrectness,Pythoncodeexecutionerrors,andOCRtools. Pleaserefertothe
supplementarymaterialsfordetailedinformation. Afterfiltering,wehaveapproximately5million
images,withthedistributionforeachcharttypedisplayedinFig.3(a). Foreachchartimage,we
5
DSU
ni
)SPE(
erahs
rep
sgninraE
)sdnasuoht
ni(
cfifarT
etisbeW
)%(
etaR
noisrevnoCcollecttherawdatainJSONformat,asharedREADMEfile,thecorrespondingPythonscript,17
generalquestion-answer(QA)pairs: onedescriptionQA,onesummaryQA,fiveliteralQAs,five
inferentialQAs,fivereasoningQAs,2augmentedQAs: 1JSON-onlyQA,and1data-drivenQA.
3.3 Anewbenchmarkforcomprehensivechartunderstanding
Existing chart benchmarks [37, 42] contains only a limited range of chart types (e.g., line, bar,
andpiecharts)andlackofcomprehensiveQAstoaccessamodel’sunderstandingofchartsfrom
various perspectives, including raw data comprehension, inferential abilities, and mathematical
reasoningcapabilities. Tobridgethisgap,weproposeacomprehensivebenchmarkderivedfromthe
aforementionedsyntheticdataset. Itcovers18differentcharttypes,threedifferentlevelsofQAs
(literal, inferential, andreasoningQAs), andprovidesbothlongandshortanswers. Notably, the
chartimagesinthebenchmarkarenotallannotated,allowingassessmentofthemodel’sabilityto
understandtheunderlyingdataofachartashumansdo. Toensurethequalityoftheimagesinthe
benchmark,weemployedhumanevaluationstofilterthedataandobtainahigh-qualitytestset. The
evaluationsarebasedontwocriteria: Validity: Whethertheessentialcomponentsoftheimages(e.g.,
title,x,y-axis,labeling)areclearlyvisibleandnotmissing. Extractability: Whethertheevaluatorcan
extracttherawdatafromthegivenchartimage,andwhethertheextracteddatamatchesthegenerated
one. Afterhumanevaluationandfiltering,weobtainatestsetconsistingof≈300image-QAparis
for each chart type, resulting in a total of ≈ 5k pairs. Note that these QAs equally cover literal,
inferential,andreasoningquestionsformeasuringchartunderstandingofMLLMs.
4 Experimentsandmodelanalysis
4.1 Experimentalsetup
Benchmark Our evaluation utilizes four classical benchmarks to compare our model against
previousworks. WespecificallyusetheChartQAdataset[37],whichincludes1.5kchartimagesinits
testset,dividedintohuman-writtenandmachine-generatedquestionswith1.2kQApairseach. The
human-writtenquestionsoftenrequiremathematicalreasoning. ChartQAalsoprovidesCSVdatafor
eachimage,enablingustoconductaChart-to-Table(orChartExtraction)tasktoassesstheabilityof
MultimodalLargeLanguageModels(MLLMs)toextractrawdatafromcharts,followingprevious
studies[22,30]. Additionally,weusethePlotQAdataset[42]whereimagesgenerallylacknumerical
valueannotations,necessitatingvalueinferencerelativetotheY-axis. Forevaluatingthemodels’
capabilitytocaptureglobalconcepts,weassessontheChart-to-TexttaskusingthePewandStatista
splitsfromthedataset[27]. ThePewsplitcontains9kimagesaccompaniedbydescriptionswritten
byprofessionaleditors,whiletheStatistasplitincludes33kimagesthatoftenfeaturedescriptivetext
withinthechartsthemselves,makingitaneasiersplitthanPew.
Metrics ForChartQAandPlotQA,weadopttherelaxedaccuracymetricfornumericanswers,
allowinga5%marginoferrorfromtheexactvalue,anduseexactmatchfornon-numericanswersas
perthestandardinpreviousstudies[22,37]. IntheChart-to-Tabletask,wemeasureperformance
usingF1scoreofRelativeMappingSimilarity(RMS)andRelativeNumberSetSimilarity(RNSS)
toevaluatenumericaccuracyandtablesimilarity,respectively. FortheChart-to-Texttask,weuse
BLEU-4,anN-grammatchingscore,followingthepreviouswork[27].
A 3-stage training process Unlike previous approaches that convert a general MLLM into a
chart-specificexpertbyonlyapplyingLoRAfine-tuningonlimitedhigh-qualitydata[22],training
CHOPINLLMunfoldsinthreestages,illustratedinFig.2(b). The3-stagetrainingenablesourmodel
not only to understand chart QAs and downstream tasks but also to capture the underlying data,
therebyachievingafundamentalunderstandingofcharts. Intheinitialpre-trainingstage,wefixthe
ViTandLLMwhiletrainingtheprojectorfromscratchusingoriginalLLaVAdataalongsideournewly
generatedchart-descriptionandchart-jsonpairs. ThesecondstageinvolvesfreezingViTandjointly
fine-tuningtheprojectorandLLMwithbothoriginalLLaVAQApairsandourgeneratedchartQA
pairs,enablingtheLLMtocomprehendvisualtokensandfacilitatechartquestionanswering. Finally,
weapplyLoRAfine-tuningtoaligntheLLM’sresponsedistributionwiththetargetdownstream
dataset. Eachstageiscarefullystudiedandtheresultsarepresentedinthefollowingsubsections. In
thefollowingstudy,weablate1stageatatimeandusethefull-trainingsettingfortheother2stages.
6Table1: Ablationofstage-1training. Thisempiricallyverifiesthatpre-trainingbasicchartvisual
perception is still important, even with abundant stage-2 instruction fine-tuning data. Moreover,
learningtopredictJSONdataisbeneficial,evenontopofpre-trainingwithdescriptivecaptions.
ChartQA Ourbenchmark
Trainingdata
human augmented literal inferential reasoning
LLaVA-CC3M-Pretrainpairs[34] 44.80 83.92 41.45 34.09 22.31
+Chart-descriptionpairs 48.56 86.89 42.71 33.68 23.51
+Chart-JSONdatapairs 52.28 87.68 44.96 34.94 24.61
Table2: Ablationofstage-2training. Eachtypeofnewinstruction/QAdataimprovesthefinal
performanceconsistentlyacrossalmostallmetrics. BestresultishighlightedinBoldandthesecond
bestisunderlined. †denotesinferencetechniquewithoutextradata.
ChartQA Ourbenchmark
Trainingdata
human augmented literal inferential reasoning
LLaVA-Instruct-150KQAs 45.84 86.48 16.54 15.99 6.57
+descriptionandsummaryQAs 47.04 87.76 19.90 15.69 5.26
+Literal/infer. /reasoningQAs 48.96 87.52 40.55 33.33 21.30
+JSON-onlyQAs 49.60 87.36 41.45 34.84 22.36
+Data-drivenQAs 52.28 87.68 44.96 34.94 24.61
+DataPrompting† 56.96 87.60 52.00 41.75 31.90
4.2 Stage1: Pre-trainingforchartfeaturealignment
Inthefirsttrainingstage,thegoalistoalignvisualandlinguisticfeaturessothatvisualdatacanbe
seamlesslytranslatedintothetextualdomainforLLMcomprehension. Employingastrategyfrom
Liuetal.[34],weuseaprojectortotranslatevisualfeaturesfromViT[15]intothetextualdomain,
trainingitwithpairwiseimage-captiondatatoenhanceitscapabilitytocapturevisualinformation.
Weexplorethreeconfigurations: utilizingonlyLLaVACC3MPretrainingdata,3 combiningLLaVA
datawithchart-descriptionpairs,andusingLLaVAdatawithbothchart-descriptionandchart-raw
datapairs. Thedataforstagetwotrainingremainsconsistentacrossthesesettings,summaryQAs,
descriptionQAs,three-levelQAs,text-onlyQAs,anddata-drivenQAs,asdepictedinFig.2(b). In
stagethree,allmodelsundergoLoRAfine-tuningonthedownstreamdataset,usingLLaVA-7Bas
thebaselineforthiscomparison. ResultsaredetailedinTable1.
Densedataalignmentisbeneficialforbothchartdatacomprehensionandreasoning. Forchart
images,chart-descriptionpairsactasstandardimage-captionpairs. However,tomoreeffectively
bridgethevisual-textualgap,wealsoutilizechart-jsonpairsthatencompasstheunderlyingnumer-
icaldataanditsschemaofthecharts. Thisapproachnotonlyalignsvisualfeatureswithtextual
descriptionsbutalsosignificantlyenhancesmodelperformance,asdemonstratedbyimprovementsof
approximately2%inliteralQAsandabout1%inreasoningskills,accordingtoresultsinTable1.
4.3 Stage2: End-to-endfine-tuning
Thesecondstage,end-to-endfine-tuning,trainstheMLLMtoactuallyunderstandthealignedvisual
tokens so that it follows the user instruction and reason about the answer, on top of the inherent
languagecapabilityfromtheoriginalLLM.Weutilizeasignificantnumberofimage-QApairsto
jointlytunetheLLMandtheprojector. ToevaluatetheeffectivenessofincorporatingchartQAs
duringfine-tuning,weconductablationstudiesstartingwithabaselinethatusesonlyLLaVAInstruct-
150Kdata,4incrementallyaddingextraQApairs.Allmethodsleveragethesamepre-trainingweights,
derivedfromtrainingonLLaVAdatawithbothchart-descriptionandchart-rawdatapairs(thebest
3https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K
4https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K
7Table3: Comprehensiveevaluationacrossfourchartbenchmarks. CHOPINLLMachievesbest
QAresultsonboth(mostly)annotatedbenchmark,ChartQA,andnon-annotatedbenchmark,PlotQA.
H and A denote the human and augmented branch in ChartQA, respectively. Stat. represent the
statistasplit. †: ourreproductionusingtheofficialcode. Notethatforfaircomparison,wedon’tuse
chain-of-reasoningintheinference. ThebestresultishighlightedinBoldandthesecondunderlined.
ChartQA Chart-to-Table Chart-to-Text PlotQA∗
Method
H A Avg. F1 RNSS Pew Stat. v1 v2
Pix2struct[29] 30.50 81.60 56.00 - - 10.30 38.00 - -
Matcha[31] 38.20 90.20 64.20 - - 12.20 39.40 - -
Unichart[38] 43.92 88.56 66.24 52.71 - 12.48 38.21 - -
DePlot[30] - - - 87.22 94.28 - - - -
LLaVA †[34] 36.00 67.44 51.72 56.96 91.83 8.50 21.50 27.26 30.64
7B
LLaVA 37.68 72.96 55.32 48.95 - 7.16 24.65 - -
13B
LLaVA † 42.56 73.60 58.08 63.18 93.18 8.83 22.39 27.68 30.98
13B
ChartLlama [22] 48.96 90.36 69.66 89.84 94.65 14.23 40.71 29.76 29.93
13B
CHOPINLLM
7B
52.28 87.68 69.98 83.63 95.27 11.50 38.97 30.06 31.08
CHOPINLLM
13B
54.11 88.67 71.39 88.12 95.95 12.66 40.81 33.98 33.96
settinginSec.4.2. Instagethree,allmodelsundergoLoRAfine-tuningonthedownstreamdataset.
Table2presentstheresult.
JSON-onlyQAsallowtransferringpuretextreasoningabilitiestomultimodalchartunder-
standing. ThechartunderstandingofMLLMscanbeseenastwostages: visualandtextrawdata
alignment(whichisdoneinthetrainingofthefirststage)andquestionansweringwithreasoning
abilityontherawtextualdata(JSON).Thus,withawell-alignedfirststagetraining,wehypothesize
thatre-blendingsomepuretextualQAs,preservingtheabilityofreasoningontextrawdata,can
alsobenefitthereasoningabilitiesinvisual-textscenarios. AsdetailedinSec.3.2,forJSON-only
QAs,ratherthanutilizingchartimagesandQAs,wereplacethechartimagewithJSONdataanda
README,resultinginpurelytext-basedQAsfortraining. Table2demonstratestheeffectivenessof
eachQAtype. Wediscoverthatre-blendingJSON-onlydataduringtheend-to-endfine-tuningstage
improveschartreasoningskills,matchingtheassumption.
Data-driven QAs in the fine-tuning stage enable MLLMs to enhance prediction accuracy
throughdataprompting. AsdetailedinSec.3.2, data-drivenQAsaremulti-turnQAs, which
requiremodelstoextractrawdatabeforeansweringgivenquestions. Combinedwiththerawdata
reasoningabilities enhancedvia JSON-onlyQAs, the modelcan performdata promptingduring
inference,wheremodelsachievebetterreasoningrobustnessbyfirstextractingrawdataandthen
answering the given question based on the data. Please refer to Appendix I for some examples.
AsshowninTable2,data-drivenQAssignificantlyenhancethemodel’sabilitytocapturevisual
information.Furthermore,leveragingdatapromptingininferencesignificantlyimprovesperformance
acrossalldownstreamtasks.
4.4 Stage3: Downstreamfine-tuning
WebuildCHOPINLLMwiththebestsettingbasedonthepreviousobservation(thedatausedineach
stagecanbereferedtoFig.2(b)),andwecompareCHOPINLLMwithexistingchartunderstanding
approaches,includingPix2struct[29],Matcha[31],Unichart[38],Deplot[30],LLaVA[34],and
ChartLlama[22]. TheresultsareshowninTable3.
Classicalquestion-answeringonChartQA. WefindthatCHOPINLLMachievesthebestperfor-
manceonthehumansplitofChartQA,asshowninTable3,with≈5%improvement,whileachieving
comparableaccuracyontheaugmentedsplitwiththepreviousstate-of-the-artmodel,ChartLlama,
makingCHOPINLLMthebestmodelintermsofaverageaccuracy. Wenotethatthehumansplit
inChartQAismorechallengingthantheaugmentedsplit,asitcontainsmorereasoningquestions,
suggestingthatCHOPINLLMisbetteratperformingreasoningtasks.
8Gold table Ours ChartLlama LLaVA
Entity Value Entity Value Entity Value Entity Value
Portugal 84.0 Portugal 84.9 Portugal 84.0 Portugal 84.9
Italy 80.3 Italy 80.3 Italy 80.3 Italy 80.3
France 83.0 France 83.7 France 83.0 France 88.7
UK 915.0 UK 91.5 UK 915.0 UK 91.5
U.S 92.9 U.S 92.9 U.S 92.9 U.S 92.9
GT: In France, home to Europe’s largest Jewish population, reports of anti-Semitic incidents rose dramatically in 2018. There
were 541 cases reported last year – not as high as in some previous years, but a 74% increase from 2017, according to the
country’s Ministère de l’Intérieur. And already in 2019, there have been several new high-profile anti-Semitic incidents, including
swastikas being spray-painted on graves in a Jewish cemetery.
Ours: France has seen more anti-Semitic attacks than any other country in Europe, according to a 2018 report by the French
government. The report, which tracked reported “acts” – including hate crimes, hate speech and anti-Semitic acts by individuals
against people and property – found that 541 such acts occurred in 2018, up from 311 in 2017.
ChartLlama: The chart illustrates the number of reported anti-Semitic acts in France from 2011 to 2018. The data shows a
significant increase in the number of reported acts from 2011 to 2018. In 2011, there were 432 reported acts, which increased to
783 in 2018. The number of reported acts peaked in 2015 with 851, and then decreased to 651 in 2016. The chart also indicates a
slight increase in 2017 with 653 reported acts.
Figure 4: Qualitative comparison on Chart-to-text and Chart-to-Table task. CHOPINLLM
effectively captures the underlying data and generates accurate chart descriptions with correct
numericalvalues. Greendenotescorrectnumbers,whileredindicatesmistakes. Bestviewedincolor.
NotethatthegoldtableisthenoisygroundtruthprovidedinChartQAdataset.
ChartLlama
Can you please extract the raw data into csv format?
"Household Income","Income Distribution"\n "1000-2000",1500\n "2000-2500",4100\n
"2500-3500",5000\n "3500-4500",4100\n "4500-5500",2200\n "5500-7500",2000\n
"7500-10000",1200\n "10000-15000",900\n "15000+",2400
By how much does the frequency … 80k-100k income range?
5000, The calculation is as follows: 60k-80k households = 2200\n 80k-100k households =
1200, and 2200 - 1200 = 1000.\n 1000 is 5000 fewer households than the 60k-80k range, so
the decrease is 5000.
GT Ours
By how much does the frequency of Can you please extract the raw data into csv format?
households decrease when moving
from the 60k-80k income range to the Annual Household Income (Thousands USD),Number of households\n <20k%,1500\n
80k-100k income range? 20k-40k%,4700\n 40k-60k%,5000\n 60k-80k%,4300 \n 80k-100k%,3200 \n
100k-120k%,2100 \n 120k-140k%,1100\n >140k,900
The frequency of households
decreases by 1100 when moving from By how much does the frequency … 80k-100k income range?
the 60k-80k income range (4300
households) to the 80k-100k income The frequency of households decreases by 4300 - 3200 = 1100 households when moving from
range (3200 households). the 60k-80k income range to the 80k-100k income range.
Figure5:Qualitativecomparisonofmulti-turnchartquestion-answering.Greendenotesnumbers
thatmatchgroundtruthnumber,whileredindicatesmismatches. Bestviewedincolor.
Rawdataandglobalconceptunderstanding. AslistedinTable3,CHOPINLLMachievesthe
second-bestF1scoreandthehighestRNSSresult,indicatingthatCHOPINLLMcancapturenotonly
thestructurebutnumericalvaluesofrawdataofchartimages. Wenotethattheperformanceonthe
chart-to-tabletaskmayhavebeensaturated,astheimagesaremostlyannotated. Inthiscontext,this
primarilymeasurestheOCRcapabilityanddoesnotassesstheabilitytocapturetheunderlyingdata.
AsfortheChart-to-Text,showninTable3,CHOPINLLMperformscomparableintheglobalconcept
capturingandcancaptionchartimagewithmeaningfultexts.
Performanceonunannotatedchartimages. MostoftheimagesinChartQA[37]areannotated,
whichmeansthenumericalvaluesofdatapointsareexplicitlyshownontheimages. Weobservethat
existingchartMLLMs,suchasChartLlama[22],seemtoheavilyrelyonthisannotationforchart
understanding,whichisnotidealsincereal-worldchartsmaybeunannotated. Wefurtherevaluate
themusingthePlotQAdataset,andtheresultsareshowninthelastcolumnofTable3. Notably,
sincetrainingpreviousmodelslikeChartLlamaonPlotQAisinfeasible,weloadthemodelweights
asusedinChartQAandperformzero-shotpredictiononPlotQA.Theresultsshowthatourmodel
performssignificantlybetter(≈3%improvement)onunannotatedchartimages,suggestingthatour
methodswithfundamentaltrainingrelylessonnumericalannotationsonchartimages.
9
elbaT-ot-trahC
txeT-ot-trahCTable4: Performancecomparisonondifferentcharttypes. Overlappeddenotesthecharttypesthat
areinboththeChartLlamatrainingsetandourdataset.
Basic Overlapped
Method
Line Bar Pie Funnel Gantt Heatmap Scatter Box Candle.
LLaVA[34] 21.0 18.0 27.0 17.6 9.3 16.3 7.7 13.3 23.0
ChartLlama[22] 27.6 16.7 32.7 26.0 12.0 16.7 8.0 12.0 26.3
CHOPINLLM 48.0 37.0 62.7 52.6 24.0 30.0 24.6 25.7 41.3
4.5 Moremodelanalysis
Qualitativeexamples Weprovideaqualitativecomparisonofchart-to-textandchart-to-tabletasks,
withresultsdepictedinFig.4. Inthechart-to-tabletask,ourmodelaccuratelycapturesvaluesfrom
chartimages,unlikeLLaVAandChartLlama. Itisimportanttonotethatthegolddatatablesfor
ChartQAarenotalwaysdirectlyaccessible,leadingtotheuseofexistingmodelsorOCRtoolsfor
dataextraction. Thisprocesscanintroduceerrors,suchasmisreportingthevalue91.5fortheUKas
915.0,whichcanadverselyaffecttheperformanceofMLLMsfine-tunedonsuchdata. Despitethese
datasetinaccuracies,ourmodelremainsrobust,correctlyoutputtingvalueswhereChartLlamadoes
not. Inthechart-to-textcomparison,bothChartLlamaandourmodelgrasptheoverallconceptofthe
charts,butourmodelexcelsataccuratelycapturingandsummarizingexactnumericalvalues.
Additionally,asamultimodalchatbot,weemphasizepreservinghuman-likemulti-turnconversation
abilities. Figure5presentsaqualitativecomparisononchartimageswithmulti-turnQAs. Although
ChartLlamaextractsaccuratenumericalvalues,itfailstoprovidecoherentexplanationsorreasonable
text outputs. In contrast, CHOPINLLM not only extracts accurate data but also provides logical
reasoningandcoherentexplanations,showcasingtheeffectivenessofourtrainingapproach.
Performance across different chart types Our model, trained extensively across a variety of
charttypes,wasevaluatedtoassessitsperformanceagainstthepreviousstate-of-the-artmodelon
thesamecharttypes. Foranunbiasedcomparison,wefocusedontheshortanswerformatinQA
pairstoavoidvariationsinoutputpreference. Theresults,detailedinTable4,revealthatourmodel
consistentlyoutperformsthestate-of-the-artacrossbothoverlappingandbasiccharttypes. Notably,
ourbenchmark,whichfeaturesunannotatedimages,posesagreaterchallengethanChartQA.The
substantialperformanceimprovementindicatesthatourmodelisadeptatinferringdatadirectlyfrom
chartsanddemonstratessuperiorreasoningcapabilities.
5 Conclusion
Inthispaper,weexploretheimpactoffundamentaltrainingstrategiesinadaptinggeneralistMulti-
modalLargeLanguageModels(MLLMs)tochartunderstanding. Weofferpracticalguidancefor
optimizingfeaturealignmentpre-trainingandend-to-endfine-tuning. Leveragingtheseenhanced
trainingstrategies,weintroduceaspecializedchartMLLM,namedCHOPINLLM,capableofinter-
pretingdiversecharttypesindependentlyofnumericalannotations. Extensiveexperimentsconfirm
that CHOPINLLM surpassesthepreviousstate-of-the-artacrossfourbenchmarks, validatingour
framework’s effectiveness. Additionally, we present a new benchmark specifically designed to
evaluateMLLMs’comprehensionacrossvariouscharttypesandmultiplelevelsofunderstanding.
Acknowledgement
Thisworkwasfunded,inpart,bytheVectorInstituteforAI,CanadaCIFARAIChairs,NSERC
CRC,andNSERCDGs. Resourcesusedinpreparingthisresearchwereprovided,inpart,bythe
ProvinceofOntario,theGovernmentofCanadathroughCIFAR,theDigitalResearchAllianceof
Canada,5 companies sponsoring the Vector Institute,6 and Advanced Research Computing at the
UniversityofBritishColumbia. AdditionalhardwaresupportwasprovidedbyJohnR.EvansLeaders
FundCFIgrantandComputeCanadaundertheResourceAllocationCompetitionaward.
5alliance.can.ca
6https://vectorinstitute.ai/#partners
10Bibliography
[1] E.Aiello,L.Yu,Y.Nie,A.Aghajanyan,andB.Oguz. Jointlytraininglargeautoregressivemultimodal
models. arXivpreprintarXiv:2309.15564,2023. 3
[2] Anthropic. Theclaude3modelfamily:Opus,sonnet,haiku. TechnicalReport,2023. 3
[3] I.Beltagy,K.Lo,andA.Cohan. Scibert:Apretrainedlanguagemodelforscientifictext. arXivpreprint
arXiv:1903.10676,2019. 3
[4] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul,
D. Grangier, M. Tagliasacchi, et al. Audiolm: a language modeling approach to audio generation.
IEEE/ACMtransactionsonaudio,speech,andlanguageprocessing,2023. 3
[5] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,X.Chen,K.Choromanski,T.Ding,D.Driess,A.Dubey,
C.Finn,etal. Rt-2: Vision-language-actionmodelstransferwebknowledgetoroboticcontrol. arXiv
preprintarXiv:2307.15818,2023. 3
[6] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,G.Sastry,
A.Askell,etal. Languagemodelsarefew-shotlearners. NeurIPS,2020. 3
[7] G.Chen,Y.-D.Zheng,J.Wang,J.Xu,Y.Huang,J.Pan,Y.Wang,Y.Wang,Y.Qiao,T.Lu,etal. Videollm:
Modelingvideosequencewithlargelanguagemodels. arXivpreprintarXiv:2305.13292,2023. 3
[8] X. Chen, J. Djolonga, P. Padlewski, B. Mustafa, S. Changpinyo, J. Wu, C. R. Ruiz, S. Goodman,
X.Wang,Y.Tay,etal. Pali-x:Onscalingupamultilingualvisionandlanguagemodel. arXivpreprint
arXiv:2305.18565,2023. 3
[9] A.Chowdhery,S.Narang,J.Devlin,M.Bosma,G.Mishra,A.Roberts,P.Barham,H.W.Chung,C.Sutton,
S.Gehrmann,etal. Palm:Scalinglanguagemodelingwithpathways. JMLR,2023. 3
[10] H.W.Chung,L.Hou,S.Longpre,B.Zoph,Y.Tay,W.Fedus,Y.Li,X.Wang,M.Dehghani,S.Brahma,
etal. Scalinginstruction-finetunedlanguagemodels. JMLR,2024. 3
[11] Z.Dai,Z.Yang,Y.Yang,J.Carbonell,Q.V.Le,andR.Salakhutdinov.Transformer-xl:Attentivelanguage
modelsbeyondafixed-lengthcontext. arXivpreprintarXiv:1901.02860,2019. 3
[12] N.Das,S.Dingliwal,S.Ronanki,R.Paturi,D.Huang,P.Mathur,J.Yuan,D.Bekal,X.Niu,S.M.Jayanthi,
etal. Speechverse:Alarge-scalegeneralizableaudiolanguagemodel. arXivpreprintarXiv:2405.08295,
2024. 3
[13] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova. Bert:Pre-trainingofdeepbidirectionaltransformers
forlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018. 3
[14] B.Ding,C.Qin,R.Zhao,T.Luo,X.Li,G.Chen,W.Xia,J.Hu,A.T.Luu,andS.Joty.Dataaugmentation
usingllms:Dataperspectives,learningparadigmsandchallenges. arXivpreprintarXiv:2403.02990,2024.
3
[15] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,T.Unterthiner,M.Dehghani,M.Min-
derer,G.Heigold,S.Gelly,etal. Animageisworth16x16words:Transformersforimagerecognitionat
scale. arXivpreprintarXiv:2010.11929,2020. 7
[16] D.Driess,F.Xia,M.S.Sajjadi,C.Lynch,A.Chowdhery,B.Ichter,A.Wahid,J.Tompson,Q.Vuong,
T.Yu,etal. Palm-e:Anembodiedmultimodallanguagemodel. arXivpreprintarXiv:2303.03378,2023. 3
[17] N.Du,Y.Huang,A.M.Dai,S.Tong,D.Lepikhin,Y.Xu,M.Krikun,Y.Zhou,A.W.Yu,O.Firat,etal.
Glam:Efficientscalingoflanguagemodelswithmixture-of-experts. InICML,2022. 3
[18] Y.Fathullah,C.Wu,E.Lakomkin,J.Jia,Y.Shangguan,K.Li,J.Guo,W.Xiong,J.Mahadeokar,O.Kalinli,
etal. Promptinglargelanguagemodelswithspeechrecognitionabilities. InICASSP,2024. 3
[19] W.Fedus,B.Zoph,andN.Shazeer. Switchtransformers:Scalingtotrillionparametermodelswithsimple
andefficientsparsity. JMLR,2022. 3
[20] G.GeminiTeam. Gemini: Afamilyofhighlycapablemultimodalmodels.technicalreport. Technical
Report,2023. 3
[21] O.GitHub. Githubcopilot. URLhttps://github.com/features/copilot. 3
11[22] Y.Han,C.Zhang,X.Chen,X.Yang,Z.Wang,G.Yu,B.Fu,andH.Zhang. Chartllama:Amultimodalllm
forchartunderstandingandgeneration. arXivpreprintarXiv:2311.16483,2023. 2,3,4,6,8,9,10,17
[23] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.
Hendricks,J.Welbl,A.Clark,etal. Trainingcompute-optimallargelanguagemodels. arXivpreprint
arXiv:2203.15556,2022. 3
[24] E.J.Hu, Y.Shen, P.Wallis, Z.Allen-Zhu, Y.Li, S.Wang, L.Wang, andW.Chen. Lora: Low-rank
adaptationoflargelanguagemodels. arXivpreprintarXiv:2106.09685,2021. 2
[25] A.Q.Jiang,A.Sablayrolles,A.Mensch,C.Bamford,D.S.Chaplot,D.d.l.Casas,F.Bressand,G.Lengyel,
G.Lample,L.Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023. 3
[26] S.Kantharaj,X.L.Do,R.T.K.Leong,J.Q.Tan,E.Hoque,andS.Joty. Opencqa:Open-endedquestion
answeringwithcharts. arXivpreprintarXiv:2210.06628,2022. 3
[27] S.Kantharaj,R.T.K.Leong,X.Lin,A.Masry,M.Thakkar,E.Hoque,andS.Joty. Chart-to-text: A
large-scalebenchmarkforchartsummarization. arXivpreprintarXiv:2203.06486,2022. 3,6
[28] M.Kwon,S.M.Xie,K.Bullard,andD.Sadigh. Rewarddesignwithlanguagemodels. arXivpreprint
arXiv:2303.00001,2023. 3
[29] K.Lee,M.Joshi,I.R.Turc,H.Hu,F.Liu,J.M.Eisenschlos,U.Khandelwal,P.Shaw,M.-W.Chang,and
K.Toutanova. Pix2struct:Screenshotparsingaspretrainingforvisuallanguageunderstanding. InICML,
2023. 1,3,8
[30] F. Liu, J. M. Eisenschlos, F. Piccinno, S. Krichene, C. Pang, K. Lee, M. Joshi, W. Chen, N. Collier,
andY.Altun. Deplot: One-shotvisuallanguagereasoningbyplot-to-tabletranslation. arXivpreprint
arXiv:2212.10505,2022. 3,6,8
[31] F.Liu,F.Piccinno,S.Krichene,C.Pang,K.Lee,M.Joshi,Y.Altun,N.Collier,andJ.M.Eisenschlos.
Matcha:Enhancingvisuallanguagepretrainingwithmathreasoningandchartderendering. arXivpreprint
arXiv:2212.09662,2022. 1,3,8
[32] F.Liu,X.Wang,W.Yao,J.Chen,K.Song,S.Cho,Y.Yacoob,andD.Yu. Mmc:Advancingmultimodal
chartunderstandingwithlarge-scaleinstructiontuning. arXivpreprintarXiv:2311.10774,2023. 2,3
[33] H.Liu,C.Li,Y.Li,andY.J.Lee. Improvedbaselineswithvisualinstructiontuning. arXivpreprint
arXiv:2310.03744,2023. 1,3
[34] H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning. NeurIPS,2024. 1,3,7,8,10
[35] H.Lu,W.Liu,B.Zhang,B.Wang,K.Dong,B.Liu,J.Sun,T.Ren,Z.Li,Y.Sun,etal. Deepseek-vl:
towardsreal-worldvision-languageunderstanding. arXivpreprintarXiv:2403.05525,2024. 3
[36] A.MasryandE.Hoque. Integratingimagedataextractionandtableparsingmethodsforchartquestion
answering. InChartQuestionAnsweringWorkshop,inCVPR,2021. 3
[37] A.Masry,D.X.Long,J.Q.Tan,S.Joty,andE.Hoque. Chartqa:Abenchmarkforquestionanswering
aboutchartswithvisualandlogicalreasoning. arXivpreprintarXiv:2203.10244,2022. 3,4,6,9
[38] A. Masry, P. Kavehzadeh, X. L. Do, E. Hoque, and S. Joty. Unichart: A universal vision-language
pretrainedmodelforchartcomprehensionandreasoning. arXivpreprintarXiv:2305.14761,2023. 1,3,8
[39] A.Masry,M.Shahmohammadi,M.R.Parvez,E.Hoque,andS.Joty. Chartinstruct:Instructiontuningfor
chartcomprehensionandreasoning. arXivpreprintarXiv:2403.09028,2024. 3
[40] B. McKinzie, Z. Gan, J.-P. Fauconnier, S. Dodge, B. Zhang, P. Dufter, D. Shah, X. Du, F. Peng,
F.Weers,etal. Mm1: Methods,analysis&insightsfrommultimodalllmpre-training. arXivpreprint
arXiv:2403.09611,2024. 3
[41] MeetkAI. Functionary. URLhttps://functionary.meetkai.com/. 3
[42] N.Methani,P.Ganguly,M.M.Khapra,andP.Kumar. Plotqa:Reasoningoverscientificplots. InWACV,
2020. 3,4,6
[43] OpenAI. Chatgpt:Optimizinglanguagemodelsfordialogue. Technicalreport.,2023. 3
[44] OpenAI. Gpt-4technicalreport. TechnicalReport,2023. 3
12[45] L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin,C.Zhang,S.Agarwal,K.Slama,
A. Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 35:
27730–27744,2022. 3
[46] A. Patel, C. Raffel, and C. Callison-Burch. Datadreamer: A tool for synthetic data generation and
reproduciblellmworkflows. arXivpreprintarXiv:2402.10379,2024. 3
[47] S.G.Patil,T.Zhang,X.Wang,andJ.E.Gonzalez. Gorilla:Largelanguagemodelconnectedwithmassive
apis. arXivpreprintarXiv:2305.15334,2023. 3
[48] S.G.Patil, T.Zhang, V.Fang, R.Huang, A.Hao, M.Casado, J.E.Gonzalez, R.A.Popa, I.Stoica,
etal. Goex:Perspectivesanddesignstowardsaruntimeforautonomousllmapplications. arXivpreprint
arXiv:2404.06921,2024. 3
[49] J.W.Rae,S.Borgeaud,T.Cai,K.Millican,J.Hoffmann,F.Song,J.Aslanides,S.Henderson,R.Ring,
S.Young,etal. Scalinglanguagemodels: Methods,analysis&insightsfromtraininggopher. arXiv
preprintarXiv:2112.11446,2021. 3
[50] B.Roziere,J.Gehring,F.Gloeckle,S.Sootla,I.Gat,X.E.Tan,Y.Adi,J.Liu,T.Remez,J.Rapin,etal.
Codellama:Openfoundationmodelsforcode. arXivpreprintarXiv:2308.12950,2023. 3
[51] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye,
G.Zerveas,V.Korthikanti,etal. Usingdeepspeedandmegatrontotrainmegatron-turingnlg530b,a
large-scalegenerativelanguagemodel. arXivpreprintarXiv:2201.11990,2022. 3
[52] C.Team. Chameleon:Mixed-modalearly-fusionfoundationmodels. arXivpreprintarXiv:2405.09818,
2024. 3
[53] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,P.Bhargava,
S.Bhosale,etal. Llama2:Openfoundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,
2023. 3
[54] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,Ł.Kaiser,andI.Polosukhin.
Attentionisallyouneed. NeurIPS,2017. 3
[55] X.Wang,J.Wei,D.Schuurmans,Q.Le,E.Chi,S.Narang,A.Chowdhery,andD.Zhou. Self-consistency
improveschainofthoughtreasoninginlanguagemodels. arXivpreprintarXiv:2203.11171,2022. 3
[56] Z.Wang,M.Xia,L.He,H.Chen,Y.Liu,R.Zhu,K.Liang,X.Wu,H.Liu,S.Malladi,etal. Charxiv:
Chartinggapsinrealisticchartunderstandinginmultimodalllms. arXivpreprintarXiv:2406.18521,2024.
3
[57] J.Wei,X.Wang,D.Schuurmans,M.Bosma,F.Xia,E.Chi,Q.V.Le,D.Zhou,etal. Chain-of-thought
promptingelicitsreasoninginlargelanguagemodels. NeurIPS,35:24824–24837,2022. 3
[58] R.Xia,B.Zhang,H.Ye,X.Yan,Q.Liu,H.Zhou,Z.Chen,M.Dou,B.Shi,J.Yan,etal. Chartx&
chartvlm:Aversatilebenchmarkandfoundationmodelforcomplicatedchartreasoning. arXivpreprint
arXiv:2402.12185,2024. 3,4
[59] Z.Yang,Z.Gan,J.Wang,X.Hu,Y.Lu,Z.Liu,andL.Wang. Anempiricalstudyofgpt-3forfew-shot
knowledge-basedvqa. InAAAI,2022. 3
[60] Y.Yu,Y.Zhuang,J.Zhang,Y.Meng,A.J.Ratner,R.Krishna,J.Shen,andC.Zhang. Largelanguage
modelasattributedtrainingdatagenerator:Ataleofdiversityandbias. NeurIPS,2024. 3
[61] W.Yuan,R.Y.Pang,K.Cho,S.Sukhbaatar,J.Xu,andJ.Weston. Self-rewardinglanguagemodels. arXiv
preprintarXiv:2401.10020,2024. 3
[62] F.Zeng,W.Gan,Y.Wang,N.Liu,andP.S.Yu. Largelanguagemodelsforrobotics: Asurvey. arXiv
preprintarXiv:2311.07226,2023. 3
[63] H.Zhang,X.Li,andL.Bing. Video-llama:Aninstruction-tunedaudio-visuallanguagemodelforvideo
understanding. arXivpreprintarXiv:2306.02858,2023. 3
[64] P.Zhang, X.D.B.Wang, Y.Cao, C.Xu, L.Ouyang, Z.Zhao, S.Ding, S.Zhang, H.Duan, H.Yan,
etal. Internlm-xcomposer:Avision-languagelargemodelforadvancedtext-imagecomprehensionand
composition. arXivpreprintarXiv:2309.15112,2023. 3
[65] Z.Zhang,A.Zhang,M.Li,H.Zhao,G.Karypis,andA.Smola. Multimodalchain-of-thoughtreasoningin
languagemodels. arXivpreprintarXiv:2302.00923,2023. 3
13[66] W.X.Zhao,K.Zhou,J.Li,T.Tang,X.Wang,Y.Hou,Y.Min,B.Zhang,J.Zhang,Z.Dong,etal. A
surveyoflargelanguagemodels. arXivpreprintarXiv:2303.18223,2023. 3
[67] M.Zhou,Y.R.Fung,L.Chen,C.Thomas,H.Ji,andS.-F.Chang. Enhancedchartunderstandinginvision
andlanguagetaskviacross-modalpre-trainingonplottablepairs. arXivpreprintarXiv:2305.18641,2023.
1,3
[68] D.Zhu,J.Chen,X.Shen,X.Li,andM.Elhoseiny. Minigpt-4:Enhancingvision-languageunderstanding
withadvancedlargelanguagemodels. arXivpreprintarXiv:2304.10592,2023. 1,3
14Appendix
A Detailsofthedatesetgeneration 16
A.1 Datasetfiltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 DetailsofexpertGPT-4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B Implementationdetails 16
C Experimentalresults 17
D Socialimpact 18
E Limitation 18
F ExamplesforJSONtemplateandREADMEs 19
F.1 Example1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
F.2 Example2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
G Examplesforpre-definedtopics 20
H ExamplesofaugmentedQAs 21
H.1 JSON-onlyQA:example1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
H.2 JSON-onlyQA:example2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
H.3 Data-drivenQA:example1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
H.4 Data-drivenQA:example2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
I Examplesofdataprompting 23
I.1 Example1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
I.2 Example2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
J Examplesfromourbenchmark 24
J.1 Example1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
J.2 Example2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
J.3 Example3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
15A Detailsofthedatesetgeneration
A.1 Datasetfiltering
InSection3.1,weintroduceanoveldatagenerationpipelinethatleveragestext-onlyLLMs. This
pipelineenablesustocollectchartimagesalongwithvariousdataandQApairswithoutextensive
humaneffort,therebyreducingthecostofcreatingpairwisedata. However,LLMsarenotperfect
and can make mistakes in either data generation or code script generation. Thus, in this section,
wediscussthedatafilteringtechniquesweusetoimprovethequalityofthesyntheticdataset. The
generationpipelineissplitintothreeparts: sharedtemplategeneration,dataandQAgeneration,and
codescriptgeneration. Wenowdetailthefilteringprocessforeachpart.
SharedtemplateandREADME ThesharedtemplateandREADMEfileforeachcharttypeform
thecoreoftheentiredatagenerationprocess,asthesubsequentrawdata,QA,andPythonscriptare
basedonthesharedtemplate. Therefore,forthesharedtemplate,wedeployahumanchecktoensure
thetemplatecontainsnecessaryelementsforthechart(i.e.,title,x-axis,y-axis,data). Additionally,
humansarerequiredtoverifythecorrectnessofthecharttypedefinitionsintheREADME.Notethat
weconsider18differentcharttypes;thus,thereare18templateJSONfilesalongwiththeREADMEs
inourdataset.
DataandQAgeneration SinceallthedatashouldfollowthetemplateJSON,forthedatagenera-
tionpart,weapplyfilteringbasedontheJSONstructure. Specifically,weremovegenerateddatathat
deviatesfromthetemplatefilebycomparingtheelementsinthekeysoftheJSONdictionaryandthe
datatypesofallthevalues. AsforQAgeneration,wecheckthestructureoftheoutputdictionary. In
detail,thekeysoftheoutputQAdictionaryshouldcontainsummary,description,literal,inferential,
andreasoningQAs. WefilteroutQAswithmissingattributes.
Codescriptgeneration WepredefinedacodeexpertGPT-4tousefourdifferentPythonlibraries
toplotthechartimages: Matplotlib,Plotly,Pygal,andSeaborn. Theadvantageoftheselibrariesis
thatifthePythoncodeorinputdataisincorrectintermsofstructureorothererrors,thegenerated
imagewill eitherbe missingwith aPython erroror displaya "NoData" icon. Thus, we applya
two-stepfilteringprocess: (1)PythonErrorFiltering: IfthereisanerrorwhilerunningthePython
scripttogeneratetheimage,wewillremovethescriptandthecorrespondingJSONdata. (2)OCR
ToolFiltering: Iftheimageisgeneratedbutthereissomeothererror,theoutputimagewilldisplaya
"NoData"icononit. Tothisend,wefurtheruseanOCRtooltodetectwhetherthereisany"No
Data"iconintheimages. Ifso,wewillremovethedataandscriptaccordingly.
A.2 DetailsofexpertGPT-4
IntheGPT-4module,beforefeedingthepromptsforgeneration,wemustspecifythesystemmessage
forGPT-4. ThesesystemmessagesexplicitlyinformGPT-4abouttheenvironmentforthegeneration
andtheroleitmustplayinthistask,helpingthemodeltooutputpreciseresponsesthatmatchusers’
needs. Thus,inourdatagenerationpipeline,weencounterthreedifferentcircumstances,andwe
havethreedifferentsystemmessagesforGPT-4,tailoringittobecomeanexpertaccordingly. We
providethedetailsofthesesystemmessageinTable5forreference.
B Implementationdetails
Inthispaper,ifnotspecified,weemploythesameframeworkasLLaVA-7Bforourmodelarchitecture.
Fordetailsregardingthemodelarchitecture,hyperparameters,andthetypeofoptimizer,weadhereto
theconfigurationsettingsfoundintheofficialLLaVArepository.7 Regardingcomputationalresources
fortraining,weuse8*A100GPUsforboththepretrainingandfinetuningstages. Consideringthe
amountofdatautilizedinthesestages,ittakesapproximatelyonedayforpretrainingandfourdays
forfinetuningusing8*A100GPUs.
16Table5: SystemmessagesforGPT-4.
Model Systemmessages
JSONexpertGPT-4 YouareanAIchatbotdesignedtohelpusersgenerateaJSONtemplatefile
alongwithaREADMEforaspecificcharttype. Onceusersspecifythe
charttype,youwillhavetodeterminethenecessaryattributesforplottinga
chart,includingbutnotlimitedtothetitle,x-axis,y-axis,anddata.Thedata
partshouldhaveageneralstructurethatcoversbothsimpleandcomplex
examplesforthecharttype. AsfortheREADME,itshouldexplainthe
meaningandtypeofeachattributeorlabel,andalsothedefinitionforthis
chart type. The generated templateand README will then beused in
creatingrawdataandaPythonscriptforvisualizingthechart.
DataexpertGPT-4 Youareanexpertingeneratingquestionandanswerpairsbasedonraw
chart data. Your role involves carefully examining chart data, which is
presentedinJSONformat,andcreatingrelevantquestionandanswerpairs.
Thesepairswillbeusedforinstructionaltuningofavisionandlanguage
model. Alongwiththechartdata,youwillreceiveaJSONtemplateand
aREADMEfilethatprovidesadditionalinformationonthemeaningof
eachattributeintheJSONdata. Itisimportanttoreviewalltheprovided
materialsthoroughlytoensurethequestionandanswerpairsareaccurate
andusefulformodeltraining.
CodeexpertGPT-4 YouareaPythoncodeAIassistantandgoodatchartimageplotting. Now,
youareaskedtomodifythePythoncodetoageneralversionthatcantake
anyJSONdatamatchingthedefinitioninthetemplate.
Table6: Performancecomparisonwithsyntheticdatausingindifferenttrainingstages.
ChartQA Ourbenchmark
Idx Model Usesyn. datain
human augmented literal inferential reasoning
(1) LLaVA-7B - 36.00 67.44 14.69 18.50 7.32
(2) LLaVA-7B stage3 37.60 70.40 27.57 29.17 12.48
(3) LLaVA-7B stage1&2 52.28 87.68 45.91 34.74 23.01
C Experimentalresults
Effectivenessofsyntheticdataindifferenttrainingstages. Intheprevioussection,weshowcase
thatchartdatageneratedbytext-onlyLLMsenhancesMLLMlearninginchartunderstanding. Here,
wecomparetheperformanceofusingthesamedataamountinthethirdstageofLoRAdownstream
fine-tuning, similartoChartllama[22]. TheresultsareinTable6. Specifically, Model(1)isthe
baseline,trainedwithLLaVAdatainpre-trainingandfine-tuningstages,followedbyLoRAfine-
tuning on ChartQA. Model (2) is similar but includes synthetic data with ChartQA in the LoRA
fine-tuning stage. Model (3), our best 7B model, uses the same synthetic data as Model (2) but
incorporatesitinpretrainingandfine-tuningbeforeLoRAfine-tuningonChartQA.Asshownin
Table6,Model(2)showsan≈2%improvementoverModel(1),indicatingthebenefitofsynthetic
data. However,Model(3)showsasignificantimprovementoverModel(2),suggestingsyntheticdata
ismoreeffectiveinfundamentaltrainingratherthanfine-tuning. Weproposetwopossiblereasons
forthis. First,alignmentissuescannotbeeffectivelyresolvedviaLoRAtuning,asitonlyadjustsa
smallportionofthemodel’sparameters. Second,theoutputpreferenceofsyntheticdatamaydiffer
fromthatofthedownstreamdataset. Jointtuningmightshifttheoutputpreferenceawayfromthe
downstreamtask,resultinginlimitedperformanceimprovement.
7LLaVA:https://github.com/haotian-liu/LLaVA.
17D Socialimpact
Ourmodeliscapableofchartunderstandingandcaninterprettherawdataofachartlikeahuman,
withoutrelyingonannotations,whilealsoperformingvariouslevelsofQAtasks. Thus,ourmodel
can be used in many data analysis scenarios, such as market research, healthcare trend analysis,
and other data science areas. With the help of our model, humans can process large volumes of
chartdatamoreefficiently,makeinformeddecisions,andenhancereportingaccuracy. Whileour
modelprovidesbenefitsinchartunderstandingandanalysis,therearepotentialnegativeimpacts. For
instance,itcouldbeemployedtocreatemisleadingdatavisualizationsorgeneratefalsenarratives
whencombinedwithotherLLMtools. Thesefakechartsandpiecesofinformationcannegatively
affectdecision-makingprocesses.
E Limitation
In this paper, we propose an MLLM model for chart understanding, fundamentally trained on
syntheticdata. However,sincethesyntheticdatageneratedbyLLMscannotbeperfect,sometimes
incorrectdatacanbeintroducedintothedatasetandmaynotbefilteredoutbyourfilteringprocess.
Thesedatacanresultinmisalignmentsandincorrectmappingsduringpre-trainingandfine-tuning,
potentiallyleadingtoincorrectresponsesandhallucinations. Thus,theperformanceofourchart
MLLMsislimitedbytheLLMs’generationcapabilities. Wecanpotentiallyincludemoreadvanced
LLMsinthedatagenerationpipelinetoreducetheoccurrenceofincorrectdata. Moreover,another
limitationofourmodelisthatitcurrentlysupportsunderstandingonly18charttypes. However,
there are many more chart types in the real world. Developing an open-domain, versatile chart
understandingMLLMremainsataskforfuturework.
18F ExamplesforJSONtemplateandREADMEs
F.1 Example1
Template { "chart_title": "Chart Title", "x_axis_label": "X-Axis Label", "y_axis_label": "Y-Axis Label", "data": [
{ "category": "Category 1", "value": 0 }, { "category": "Category 2", "value": 0 }, { "category":
JSON
"Category 3", "value": 0 } ] }
The JSON template provided below is designed to generate data for a 'Bar Chart' chart. This template
includes all the necessary attributes and labels to ensure consistent data formatting for the chart.
- `chart_title`: This attribute represents the title or name of the bar chart. Please replace the placeholder
text with an appropriate title for your chart.
- `x_axis_label`: This attribute represents the label for the x-axis of the chart. Please replace the
placeholder text with an appropriate label.
- `y_axis_label`: This attribute represents the label for the y-axis of the chart. Please replace the
README placeholder text with an appropriate label.
- `data`: This attribute represents the data points for the chart. It is an array of objects, where each
object represents a category or group and its corresponding value.
For a simple example, the template can be filled as follows:
Definition of a 'Bar Chart' chart:
A bar chart, also known as a bar graph, is a visualization tool that uses rectangular bars to represent
data. Each bar represents a category or group, and the length or height of the bar corresponds to the
value it represents. Bar charts are commonly used to compare categorical data or show the distribution
of data across different categories.
F.2 Example2
Template { "chart": { "title": "Chart Title", "xAxisLabel": "X-axis Label", "yAxisLabel1": "Y-axis Label 1",
"yAxisLabel2": "Y-axis Label 2", "datasets": [ { "name": "Dataset 1", "type": "line", "data": [] }, {
JSON
"name": "Dataset 2", "type": "bar", "data": [] } ] } }
This JSON file template is designed for generating datasets for a 'Multi-axes Line Bar Chart' chart. It includes the following
attributes:
1. chart:
- title: (string) The title of the chart.
- xAxisLabel: (string) The label for the X-axis.
- yAxisLabel1: (string) The label for the Y-axis corresponding to the line chart.
- yAxisLabel2: (string) The label for the Y-axis corresponding to the bar chart.
2. datasets:
- name: (string) The name or label of the dataset.
- type: (string) The type of chart component for the dataset. Can be 'line' or 'bar'.
- data: (array) The array to store the data points for the dataset. Placeholder data should be added here.
README The template provides a structure to accommodate both simple and complex examples of a Multi-axes Line Bar Chart. Additional
datasets can be added within the "datasets" array.
Please ensure that the generated data adheres to the structure of this JSON template, including the attribute names and data types, to
ensure consistency when plotting the chart using Python.
Definition of a 'Multi-axes Line Bar Chart':
A multi-axes line bar chart is a type of chart that combines both line and bar charts in a single
visualization. It allows for the comparison of multiple datasets that have different scales or units of
measurement. This chart type uses multiple y-axes, one for each dataset, to display the corresponding
line and bar components.
19G Examplesforpre-definedtopics
Public Policy Gender and Diversity Transportation Labor and Employment Trends
Healthcare Systems Economic Development Weather and Climate Urban Development Disaster Relief and Emergency Response
Mental Health Artificial Intelligence and Robotics Sports and Recreation Sustainability and Green Initiatives LGBTQ+ Rights and Advocacy
Renewable Energy Consumer Spending Habits Entertainment and Media Education Policy and Reform Regional Economic Disparities
Water Resources Advertising and Marketing Food and Nutrition Healthcare Access and Equity Automation and Job Displacement
Manufacturing Cultural Trends Fashion and Lifestyle Clean Energy Initiatives Branding and Brand Loyalty
Retail Trends Philanthropy and Nonprofits Housing and Real Estate E-commerce Trends Subcultural Trends and Movements
Social Issues International Trade and Commerce Travel and Tourism Poverty and Homelessness Social Entrepreneurship and Impact
Population Dynamics Politics Crime and Safety Immigration and Migration Patterns Investing
Digital Media Consumption Business and Finance International Relations Internet and Social Media Usage Fair Trade and Ethical Consumption
Cryptocurrency and Blockchain Science and Research Religion and Beliefs Trends Import-Export Regulations and Tariffs
Humanitarian Aid and Development Agriculture History and Heritage Decentralized Finance (DeFi)
Weprovideawordcloudinthefigureabovetoshowthefrequencyofeachwordinthedefinedtopic
set. Acomprehensivelistofallthetopicsisalsoprovidedatthebottomofthisfigure.
20H ExamplesofaugmentedQAs
H.1 JSON-onlyQA:example1
Chart image JSON & README
{ "title": "Box Office Revenue Share by Genre", Title: The title of the pie chart, which gives an idea of the dataset's
"description": "This pie chart displays the distribution of box overall topic.
office revenues among five major movie genres in 2021,
illustrating the diversity in consumer preferences and the strategic Description: A short summary explaining what the pie chart data
positioning of movie studios.", represents and any additional information that might be useful to
"data": [{ understand the context.
"category": "Action Blockbuster",
"value": 350, Data: An array of objects where each object represents a slice of
"color": "#FF6347" the pie chart.
}, - category: A string representing the name of the category this slice
{ of the pie chart is about.
"category": "Animated Feature", - value: A numerical value indicating the size of the category in the
"value": 300, dataset. This determines the size of the pie slice.
"color": "#3CB371" - color: A string indicating the color of this slice. It can be a HEX
}, color code or a standard color name.
{
"category": "Gripping Thriller", The JSON template and README file are designed to ensure that
"value": 250, datasets for pie charts can be created with consistent structure and
"color": "#4682B4" clarity, accommodating both simple and complex use cases. Make
}, sure to replace placeholder text and values with actual data relevant
{ to the pie chart you are creating.
"category": "Sci-Fi Adventure",
"value": 200,
"color": "#DAA520"
},
{
"category": "Historical Drama",
"value": 150,
"color": "#A0522D"
}
]
}
H.2 JSON-onlyQA:example2
Chart image JSON & README
{ Chart Title: The title of the Radar Chart, which usually describes
"chartTitle": "Cybersecurity Domain Risk Analysis Trends", the overall data being represented.
"datasets": [ Datasets: An array of objects, each representing a different dataset
{ to be plotted on the Radar Chart.
"label": "2023 Risk Levels", Dataset Label: A descriptive name for the dataset. This could be the
"data": { name of a product, an individual's name, or any other identifier
"Network Security": 50, relevant to the data.
"Cloud Security": 75, Data: An object containing key-value pairs where the key is the
"Endpoint Security": 30, label for the axis (e.g., 'Communication', 'Battery Life') and the
"Application Security": 60 value is the data point associated with that axis. The number of
} axes can vary depending on the use case.
}, Axis Label: Placeholders for the actual labels of each axis on the
{ Radar Chart. Replace these placeholders with the appropriate
"label": "2022 Risk Levels", criteria or variables for your specific use case.
"data": { Options: An object containing additional settings and
"Network Security": 50, configurations for the Radar Chart.
"Cloud Security": 65,
"Endpoint Security": 45, Scale: Defines the range and increment of the chart's axes.
"Application Security": 60 Min: The minimum value of the scale (usually 0).
} Max: The maximum value of the scale, which should be set
} according to the data's range.
], StepSize: The interval between values on the scale.
"options": {
"scale": {
"min": 0,
"max": 100,
"stepSize": 10
}
}
}
21H.3 Data-drivenQA:example1
An example of data extraction QAs
Can you please extract the raw data of this chart?
{ "title": "Seasonal Wildlife Population Dynamics in Yellowstone", "xAxis": { "title":
"Species", "categories": [ "Elk", "Bison", "Wolves", "Bears" ] }, "yAxis": { "title": "Seasons",
"categories": [ "Spring", "Summer", "Autumn", "Winter" ] }, "colorScale": { "minValue": 0,
"maxValue": 100, "startColor": "#FFFFFF", "endColor": "#FF6347" }, "data": [ { "x": 0, "y": 0,
"value": 40 }, { "x": 0, "y": 1, "value": 70 }, { "x": 0, "y": 2, "value": 60 }, { "x": 0, "y": 3, "value": 30
}, { "x": 1, "y": 0, "value": 50 }, { "x": 1, "y": 1, "value": 80 }, { "x": 1, "y": 2, "value": 55 }, { "x": 1, "y":
3, "value": 35 }, { "x": 2, "y": 0, "value": 20 }, { "x": 2, "y": 1, "value": 60 }, { "x": 2, "y": 2, "value":
45 }, { "x": 2, "y": 3, "value": 25 }, { "x": 3, "y": 0, "value": 30 }, { "x": 3, "y": 1, "value": 50 }, { "x":
3, "y": 2, "value": 40 }, { "x": 3, "y": 3, "value": 20 } ] }
Which season generally shows the highest population densities across all species?
Reviewing the heatmap data, Summer has high values for all species:
Long answer Elk (70), Bison (80), Wolves (60), and Bears (50). Thus, Summer generally
shows the highest population densities.
Short answer Summer
H.4 Data-drivenQA:example2
An example of data extraction QAs
Can you please extract the raw data of this chart?
{ "chart": { "title": "Disaster Response Stages Efficiency", "description": "This funnel chart
maps the stages of disaster response from the initial alert to recovery, highlighting the
number of actions completed and the conversion rates between each stage. The cyclical
trend observed suggests repeated challenges in transitioning efficiently from one stage
to the next." }, "stages": [ { "stage": "Early Warning and Alert", "value": 1000,
"conversionRate": "85%" }, { "stage": "Immediate Response", "value": 850, "conversionRate":
"75%" }, { "stage": "Assessment", "value": 638, "conversionRate": "60%" }, { "stage": "Relief
Distribution", "value": 383, "conversionRate": "80%" }, { "stage": "Recovery and
Rehabilitation", "value": 306, "conversionRate": null } ] }
What is the sum of conversion rates for all stages where it is mentioned?
The sum of conversion rates for all stages mentioned is calculated by
Long answer adding the rates: 85% (Early Warning and Alert) + 75% (Immediate
Response) + 60% (Assessment) + 80% (Relief Distribution) = 300%.
Short answer 300%
22I Examplesofdataprompting
I.1 Example1
Without data extraction QA in the inference
What is the acceptance rate of manuscripts that have been revised?
50%
With data extraction QA in the inference
Retrieve the raw data from the image and organize it into a JSON dictionary.
{'chart': {'title': 'Research Paper Publication Funnel', 'xAxisLabel': 'Stage', 'yAxisLabel':
'Number of Papers'}, 'data': [{'stage': 'Submitted Manuscripts', 'count': 1200}, {'stage':
'Passed Initial Screening', 'count': 900}, {'stage': 'Peer Review Completed', 'count': 700},
{'stage': 'Revisions Submitted', 'count': 500}, {'stage': 'Accepted for Publication', 'count':
300}]}
What is the acceptance rate of manuscripts that have been revised?
60%
I.2 Example2
Without data extraction QA in the inference
What proportion of the total distribution is made up by 'Sustainable
Energy' and 'Healthcare Innovations' combined?
45%
With data extraction QA in the inference
Retrieve the raw data from the image and organize it into a JSON dictionary.
{'chart_title': 'Impact of Social Entrepreneurship Sectors on Investment Returns',
'x_axis_label': 'Sectors', 'y_axis_label_pie': 'Distribution of Sectors', 'y_axis_label_bar':
'Average Investment Return (%)', 'data': [{'category': 'Healthcare Innovation', 'value_pie':
35, 'value_bar': 12}, {'category': 'Sustainable Energy', 'value_pie': 25, 'value_bar': 15},
{'category': 'Education Technology', 'value_pie': 20, 'value_bar': 10}, {'category':
'Agricultural Development', 'value_pie': 15, 'value_bar': 8}, {'category': 'Financial Inclusion',
'value_pie': 5, 'value_bar': 20}]}
What proportion of the total distribution is made up by 'Sustainable
Energy' and 'Healthcare Innovations' combined?
60%
23J Examplesfromourbenchmark
J.1 Example1
JSON Data
{'chart': {'type': 'donut', 'title': 'AI & Robotics Investment Distribution 2023'}, 'data': {'labels': ['Machine
Learning', 'Natural Language Processing', 'Robotics', 'Computer Vision', 'Speech Recognition'], 'datasets':
[{'label': 'Investment Proportions', 'data': [35, 25, 15, 15, 10], 'backgroundColor': ['#6495ED', '#FFD700',
'#DC143C', '#32CD32', '#FF8C00']}]}}
Literal Question
Question: How much less investment did Speech Recognition receive compared to Natural
Language Processing?
Long Answer: Speech Recognition received 15% less investment than Natural Language
Processing, with Speech Recognition at 10% and Natural Language Processing
at 25%.
Short Answer: 15%
Inferential Question
Question: What two sectors together make up half of the total investment?
Long Answer: Machine Learning and Natural Language Processing together make up half of
the total investment, with percentages of 35% and 25% respectively.
Short Answer: Machine Learning and Natural Language Processing
Reasoning Question
Question: What fraction of the total investment is allocated to fields other than Machine
Learning?
Long Answer: Fields other than Machine Learning receive a combined total of 65% of the
investment, which is equivalent to the fraction 65/100 or 13/20 of the total
investment.
Short Answer: 13/20
Example 1
24J.2 Example2
JSON Data
{'chart': {'title': 'Quarterly Revenue and Profit Comparison', 'xAxisLabel': 'Quarter', 'yAxisLabel': 'Amount (in
million USD)'}, 'data': [{'category': 'Revenue', 'values': [{'x': 'Q1 2021', 'y': 120}, {'x': 'Q2 2021', 'y': 150}, {'x':
'Q3 2021', 'y': 130}, {'x': 'Q4 2021', 'y': 170}]}, {'category': 'Profit', 'values': [{'x': 'Q1 2021', 'y': 30}, {'x': 'Q2
2021', 'y': 50}, {'x': 'Q3 2021', 'y': 40}, {'x': 'Q4 2021', 'y': 60}]}]}
Literal Question
Question: What was the Revenue in Q3 2021?
Long Answer: In Q3 2021, the Revenue was 130 million USD as shown on the chart.
Short Answer: 130 million USD
Inferential Question
Question: Which quarter had the highest ratio of Profit to Revenue?
Long Answer: To determine the highest ratio of Profit to Revenue, we compare the ratios for
each quarter. The highest ratio is in Q2 2021, with Profit at 50 million USD and
Revenue at 150 million USD, giving a ratio of 1:3.
Short Answer: Q2 2021
Reasoning Question
Question: If Profit in Q1 2022 is expected to be 20% higher than Q4 2021, what would be
the expected Profit?
Long Answer: If Profit in Q1 2022 is expected to be 20% higher than Q4 2021's 60 million
USD, the expected Profit would be 60 * 1.20, which is 72 million USD.
Short Answer: 72 million USD
Example 2
25J.3 Example3
JSON Data
{'chart': {'title': 'Trends in Poverty and Homelessness Rates in Metropolis Over a Decade', 'xAxisLabel': 'Year',
'yAxisLabel': 'Percentage (%)'}, 'data': [{'category': 'Poverty Rate', 'values': [{'x': '2010', 'y': 15.0}, {'x': '2011',
'y': 15.5}, {'x': '2012', 'y': 16.0}, {'x': '2013', 'y': 15.8}, {'x': '2014', 'y': 15.6}, {'x': '2015', 'y': 14.7}, {'x': '2016',
'y': 14.0}, {'x': '2017', 'y': 13.5}, {'x': '2018', 'y': 13.0}, {'x': '2019', 'y': 12.5}, {'x': '2020', 'y': 14.0}]},
{'category': 'Homelessness Rate', 'values': [{'x': '2010', 'y': 0.9}, {'x': '2011', 'y': 1.0}, {'x': '2012', 'y': 1.1}, {'x':
'2013', 'y': 1.2}, {'x': '2014', 'y': 1.3}, {'x': '2015', 'y': 1.2}, {'x': '2016', 'y': 1.1}, {'x': '2017', 'y': 1.0}, {'x': '2018',
'y': 0.8}, {'x': '2019', 'y': 0.7}, {'x': '2020', 'y': 0.9}]}, {'category': 'Extreme Poverty Rate', 'values': [{'x': '2010',
'y': 2.0}, {'x': '2011', 'y': 2.2}, {'x': '2012', 'y': 2.4}, {'x': '2013', 'y': 2.5}, {'x': '2014', 'y': 2.3}, {'x': '2015', 'y':
2.1}, {'x': '2016', 'y': 2.0}, {'x': '2017', 'y': 1.9}, {'x': '2018', 'y': 1.7}, {'x': '2019', 'y': 1.5}, {'x': '2020', 'y':
1.8}]}]}
Literal Question
Question: In which year did the Poverty Rate reach its lowest value?
Long Answer: According to the chart data, the Poverty Rate reached its lowest value in 2019 at
12.5%.
Short Answer: 2019
Inferential Question
Question: Did any category show a consistent decline over the entire decade?
Long Answer: No category showed a consistent decline over the entire decade. While Poverty
Rate and Extreme Poverty Rate generally declined until 2019, they both
increased in 2020, and the Homelessness Rate fluctuated throughout the decade.
Short Answer: No
Reasoning Question
Question: What is the average annual decrease in the Poverty Rate from 2010 to 2019?
Long Answer: From 2010 to 2019, the Poverty Rate decreased from 15.0% to 12.5%. This is a
total decrease of 2.5 percentage points over 9 years, which gives an average
annual decrease of about 0.278 percentage points per year.
Short Answer: Approximately 0.278 percentage points per year
Example 32
26