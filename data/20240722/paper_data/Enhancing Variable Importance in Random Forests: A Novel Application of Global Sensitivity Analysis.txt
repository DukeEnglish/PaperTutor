ENHANCING VARIABLE IMPORTANCE IN RANDOM FORESTS: A
NOVEL APPLICATION OF GLOBAL SENSITIVITY ANALYSIS
GiuliaVannucci
DepartmentofElectricalEngineeringandInformationTechnology
PolytechnicandBasicSciencesSchool
UniversityofNaplesFedericoII
ViaClaudio,21
80125Napoli(NA),Italygiulia.vannucci@unina.it
RobertaSiciliano
DepartmentofElectricalEngineeringandInformationTechnology
PolytechnicandBasicSciencesSchool
UniversityofNaplesFedericoII
ViaClaudio,21
80125Napoli(NA),Italygiulia.vannucci@unina.it
AndreaSaltelli
UniversityPompeuFabra
BarcelonaSchoolofManagement
CarrerdeBalmes,132
08008,Barcelona,Spain
CentrefortheStudyoftheSciencesandtheHumanities
UniversityofBergen
Parkveien9,PB7805
5020,Bergen,Norwayandrea.saltelli@gmail.com
July22,2024
ABSTRACT
The present work provides an application of Global Sensitivity Analysis to supervised machine
learningmethodssuchasRandomForests. Thesemethodsactasblackboxes, selectingfeatures
in high–dimensional data sets as to provide accurate classifiers in terms of prediction when new
data are fed into the system. In supervised machine learning, predictors are generally ranked by
importancebasedontheircontributiontothefinalprediction. GlobalSensitivityAnalysisisprimarily
usedinmathematicalmodellingtoinvestigatetheeffectoftheuncertaintiesoftheinputvariables
ontheoutput. Weapplyithereasanovelwaytoranktheinputfeaturesbytheirimportancetothe
explainabilityofthedatageneratingprocess,sheddinglightonhowtheresponseisdeterminedby
thedependencestructureofitspredictors. Asimulationstudyshowsthatourproposalcanbeusedto
explorewhatadvancescanbeachievedeitherintermsofefficiency,explanatoryability,orsimplyby
wayofconfirmingexistingresults.
Keywords GlobalSensitivityAnalysis·ExplainableMachineLearning·RandomForests·VariableImportance
4202
luJ
91
]LM.tats[
1v49141.7042:viXraAPREPRINT-JULY22,2024
1 Introduction
Machine learning (ML) techniques are increasingly used in a number of scientific domains, such as engineering
andlifesciences,whereMLisalreadyanestablishedtool,andbehaviouralsocialsciences,whereMLiscurrently
spreading. AmongthevarietyofMLalgorithms,tree–basedmethodsareapopularclassofpredictivemodelsbecause
theyareconceptuallysimple,powerfulinbothpredictionandclassificationtasks,andabletodealintrinsicallywith
non-linearitiesandinteractions[1]. ThefamousCARTalgorithm[2]isthebasisofthemostwidelyusedensemble
algorithmssuchasRandomForests[3]. Notethatwhenmovingfromasingletree–basedmodeltoanensemblemodel,
thelossofinterpretabilityofthemodelbecomesaproblemwheneveroneisinterestedinunderstandingtherelationships
betweenfeatures. Infact,ensemblemethodsareusuallyregardedasblackboxes,veryusefulformakingpredictionsbut
lessabletoprovideinsightintohowthemethodarrivedatitsprediction[4]. Akeytooloftree–basedalgorithmsisthe
VariableImportance(VI)measure,whichcapturesthemagnitudeofthecontributionofeachpredictortotheresponsein
termsofthefinalprediction. However,thereareseveralexampleswherevariableimportanceisusedtogaininsightinto
thedependencebetweentheresponseandtheexplanatoryvariablesinagenerativesense[5,6]. Thecleardistinctionin
theliteraturebetweenthepredictiveandexplanatorypurposesofstatisticalmodelling[7,8,9]raisesthequestionof
whetheranMLmodelthatispredictivebydesigncanalsoprovidesomeinformationaboutthedatagenerationprocess.
InthecontextofML,interpretabilitymeansunderstandingwhyaparticularpredictionismadesothatdecisionscanbe
made,knowingwhichinputfeaturesdrivethepredictions,andmakingsenseofthepredictionitself[10]. Thereisa
lackofclarityintheliteratureaboutrigorousdefinitionsoftheconceptsofinterpretabilityandexplainability: although
theyaretworelatedterms,theyarenotidentical,astheseconceptsrefertodeepcognitiveprocessesrelatedtothe
socialsciencesandtheirdifferentfieldsofapplicationorscientificcommunities[10]. Moreover,arigorousdefinitionof
theseconceptsisrequiredbythelegislationondataprocessing: In2018,withtheintroductionoftheGeneralData
ProtectionRegulation(GDPR)bytheEuropeanParliament,whichforthefirsttimeintroduces,tosomeextent,aright
toexplanationforallindividualstoobtainmeaningfulexplanationsofthelogicinvolvedwhenautomateddecisionsare
taken[11],whichislinkedtoconceptssuchasfairnessandtransparency1.
Inthispaperwedistinguishbetweentwoconcepts. Werefertotheexplanatoryabilitywhenwefocusonunderstanding
whichpredictorsandhowtheyinteracttohaveadirecteffectorinfluenceonresponsevariation. Interpretability,on
theotherhand,focusesonunderstandingtheinnerworkingsoftheMLmodel. Alinearregressionmodelprovides
parameterestimatesandp-values,asingletreeprovidesaconnectedandorientedgraph. TheseMLmodelsaremore
interpretablethanensemblemethodssuchasrandomforests.
Explainabilityfocusesonexplainingthedecisionorpredictionresultandreferstotheabilitytojustifytheresultsofa
MLmodel. ExplainabilityinExplainableMLfocusesonmakingthecomplexdecisionsandoutputsofaMLmodel
understandabletousers,regardlessoftheirtechnicalexpertise. ExplainableMLincludestransparency,interpretability
andexplanatoryability.
GlobalSensitivityAnalysis(GSA)explorestherelativeinfluenceofuncertaininputfactorsindeterminingtheuncertainty
inthemodelprediction[12]. Abriefhistoryofthedisciplinecanbefoundin[13],whilerecentreviewscanbefound
in[14,15]. GSAhasbeenappliedtothestudyofvariableselectioninregression[16],suggestingasimilar,though
different,applicationtofeatureselectioninML.Whereasin[16]theemphasiswasonfindingamoreefficientway
ofidentifyingaknowndatageneratingprocess,heretheemphasisisontheinterpretabilityoftheresults. Notealso
thatwhenperformingaGSA,thequestionisnotonlywhichfactorsareinfluential,butalsohowtheyareinfluential,
forexamplewhetherinteractionsarerelevantandwhatistheeffectivedimensionoftheproblem[17]. GSAcanbe
usefullyappliedtoidentifynon–influentialfactorsandthusassistinmodelsimplification[18]. Considerableanalogies
havebeenidentifiedintheproblemsettingofGSAandML[14,10]: whereasinMLoneseeksafunctionthatcanmap
fromaninputspacetoanoutputofinterest,generallyindependentofanyunderlyingtheory,physicalorotherwise
(sometimesthefunctionactsasaclassifierfortheoutput),inGSAonedoesnotseektoidentifyaspecificfunction,but
tofindtherelativestrengthsofinput–outputrelationshipsviacoefficientssuchasSobol’indicesorShapleyeffects.
As noted in [14], there are differences between these two settings. In most cases, computational experiments, for
examplebasedonasystemofdifferentialequations,aredeterministic;thisisnotthecaseinML,whereobservational
datacomewithanerror. Again,computationalexperimentsgenerallyrequireatheory,whereastheeffectivenessofML
isoftenlinkedtoitsindependencefromanexistingtheory.MLrequireslargeinputdata,whereasnumericalexperiments
canandoftenareperformedinsituationswheredataarescarce. Finally,whenusingmathematicalmodelsinnumerical
experiments,onecandesigntheexperiment(wheretoselectdatapointsfrom,forexampleusingafractionalfactorialor
quasi–randomdesign),whereasthisisnotthecaseinML.
1AMLmodelistransparentifthefitoftheMLmodeltotrainingdataanditsgeneralisationtofreshdatacanbedescribedand
motivatedbytheanalystwithfairdocumentationthatallowsitsreproducibility.
2APREPRINT-JULY22,2024
Despitethesedifferences,thereareseveralexperimentswhereGSAisusedinconjunctionwithML.Ithasbeenshown
thatwhenanalysingtheimportanceoffeaturesinML,theso–calledtotalsensitivityindices[19]describedbeloware
closetothepermutation–basedimportancemeasurestraditionallyusedinML[20]. Thelastreferencenotonlyshows
theexactrelationshipbetweentheoverallsensitivityindexandthepermutation–basedfeatureimportance,butalso
demonstratestheuseofMLpriortoGSAasanefficientwaytoprunethenumberofvariablesbeforecomputingthe
sensitivityindices.
Can the GSA approach identify a variable importance ranking for understanding the true data generating process
underlyingthedata?
Inthispaper,weaimtoanswerthequestion:“CantheGSAapproachconstructavariableimportancemeasurethatcan
helpinunderstandingthetruedatageneratingprocessunderlyingthedata?”. Ratherthanproposinganewalgorithm
thatcanoutperformthealternatives, wewanttoproposeacombinedapproachinwhichourproposalcangivethe
generativeimportancerankingsofVIwithrespecttoothermethodsofcalculatingthepredictiveVI.Aswewillsee,
thediscrepancybetweenthetwointerpretationscanbeinformativefortheanalyst. Thisplacesourproposalinthe
contextofthebranchof“ExplainableML”thatinvolvesthedevelopmentofstrategiestopenetratethe“blackboxes”.
Specifically, we show the performance on ranking with VI computed with our approach, namely Random Forests
GlobalSensitivity–basedVariableImportance,andwith: theclassicalmeasuresproposedforCART[2]andRF[3];the
conditionalVIproposedby[21]forconditionalinferenceRF[22];theSobolMDAmeasureproposedby[23].
Thepaperisstructuredasfollows: Section2givesabriefoverviewofRTandRFmethods. Section3introducesGSA,
withareviewofsensitivitymeasuresinSection3.1andthegeneraldiscussionoftheGSAparadigminSection3.2. Our
proposalisdescribedinSection4. Theresultsofasimulationstudycomparingourproposalwithothersarediscussed
inSection5,whiletwoapplicationsarepresentedinSection6. Someconclusionsaredrawninthelastsection.
2 Tree–basedmethodsforRegression
Tree–basedmethodsareusedinmanyfieldsofapplications(medicine,genetics,finance,marketing,etc.)asasupervised
machinelearningmodelforcomplexdatastructure. Theyfollowanon–parametricdistributionfreeapproachtodeal
withnon–linearityintherelationshipbetweenasetofexplanatoryvariablesorpredictorsandatargetorresponse
variable. ClassificationandRegressionTrees(CART)isthebenchmarkmethodology[2].
Focusofthispaperistheregressionproblem. Let(Y,X)beamultivariaterandomvariablewhereXisthevectorofK
featuresplayingtheroleofinputorexplanatoryvariablestakingvaluesinΞ∈RK andY isthenumericaloutputor
responsetakingvalueintherealspaceR.
Asupervisedlearningmodelforregressionaimstoidentifyaclassifierorpredictord(x)onthebasisofalearningsample
L={(y ,x ),n=1,...N}takenfromthemultivariatedistributionof(Y,X)suchthatforeverymeasurementx,
n n
d(x)isequaltoavalueinR.
2.1 RegressionTrees
RegressionTree(RT)T(X)isatreestructuredpredictorthatisidentifiedstartingfromthebinarypartitionofX atthe
rootnodeintodisjointsubsetsandproceedingrecursivelyforeachofthemuptothefinalpartitionofX intoasetof
terminalnodesorleaveswhereisassignedaconstantresponsevalue. Ateachinternalnode,thebestsplitisfound
onthebasisofthebinarypartitionofoneofthepredictorsaccordingtoagoodnessofsplitcriterion. Regressiontree
growingattemptstoreducerecursivelythewithin–nodevariabilityoftheresponsevalues,finallyalsointheleaves.
TheRTmodelcanbeformalizedas
M
(cid:88)
T(X)=T(X;R ,µ)= µ I (1)
Y Rm (X∈Rm)
m=1
whereR =(R ,...,R )isabinaryrecursivepartitionofX : X =(cid:83)M R ,µ =E(Y|X ∈R ),themeans
ofY
witY hinthe1 terminalM
regions(ornodes),
arethevaluesthatminimim ze= t1 hem MSERm
ofthetree,
andIm
isthe
(X∈Rm)
indicatorvariableoftheelementswithinthepartition. Theconstructionofthetreemovesaroundthreeelements: the
splittingcriterion,thatreferstothechoiceofthesplitvariableX atthesplitpointssuchtodecreasetheMeanSquared
j
Error (MSE) of the tree MSE = E(Y −Tˆ(X))2; the stopping rule to declare a node to be terminal such as the
minimumvalueofobservations(i.e. 5);thepredictionrulethatlabelseachleavebytheaverageoftheresponsevalues
withinthatleave. Atreepathcanbeunderstoodasaproductionruledescribingtheinteractionamongthefeatures
3APREPRINT-JULY22,2024
yieldingtoaresponsevalue. AnytreestructureT(X)withafixednumberofleavescanbeveryusefulforexploratory
purposetounderstandandvisualizethedependencedatastructure,specificallytheinteractionamongthefeaturesthat
betterexplainthevariabilityreduction.
Asitconcernsthetreepredictord(x),thegeneralizationmeansquarederrorcanbedecomposedinthesumofthe
varianceandthesquaredbias. Aninductionstrategyisrequiredtoidentifythehonestsizetreethatcanbegeneralized
forfreshdata. Thisrequirescompromisingtwotypesoferrorsinthefamouswellknowntrade–offbiasversusvariance:
atoolargetreemightoverfitthetrainingdata(i.e.,highvariance)whereasatoosmalltreemaynotcapturethedata
generatingprocess(i.e.,highbias). Treesizeisthetuningparametergoverningthecomplexityofthemodel,i.e. the
VC–dimensionality,[24],andtheoptimaltreesizeshouldbeadaptivelychosenfromthedatausinganindependenttest
setorcross–validation(CV).
TheinductionprocedureprovidedbyCARTisintwosteps: cost–complexitypruningprocedureanddecisiontree
selection. ThefirstaimstoidentifyasequenceofnestedsubtreesT ⊂T ···⊂t where{t }istherootnodeof
max 1 1 1
thetreeandT isthemaximumexpandedtree,withtheassociatedsequenceoftreesizesfromthehighesttothe
max
lowest. ThefinaldecisiontreeisthenselectedminimizingtheMSEforregressionusinganindependenttestsample.
Alternatively,aresamplingproceduresuchasak–foldCVestimatecanbepreferred. Aone–stepvisualpruningand
treeselectionhasbeenrecentlyintroduced[25].
2.2 RandomForests
Moreeffectiveforpredictionaremultipleclassifiersorensemblemethods[26]. Theseconsistintheconstructionof
asetofclassifiers(weaklearners)byre–samplingthedataofthetrainingsampleandtheninclassifyingnewdata
pointsbyaveragingtheirtreepredictions. Anecessaryandsufficientconditionforanensembleofclassifierstowork
betterthananysingleclassifieristhattheclassifierstobeaggregatedmustbeaccurateanddiverse. Bagging[27]uses
bootstrapreplication,whereasBoosting[28]usesaweighted–bootstrapreplicationateachiterationsuchtoforcethe
algorithmtolearnbyitserrorsbecomingastronglearner.
Random Forests (RF) [3] provide an improvement over bagging. It is an ensemble method that combines several
individualregressiontreessuchthateachtreedependsonarandomvectorofunitssampledindependentlyandwiththe
samedistributionforalltreesintheforest. ThealgorithmofRFbeginswithsamplingwithreplacementn unitsfrom
h
theoriginallearningset: onlythesampledunitsareemployedforatreeconstruction. Eachrandomsamplereflectsthe
samedatageneratingprocess,butdiffersslightlyfromtheoriginaltrainingsamplebecauseofrandomvariation. Then,
thegrowthofH treesisdoneasinCART,withthepeculiaritythatonlyasubsetofdimensionl <pofvariablesis
consideredateachnodeoftheconstructionofthetree. Therandomforestspredictoris
H
1 (cid:88)
T (X)=T(X∗;R ,µ )= T(X∗;R ,µ )) (2)
rf h Yh,n∗ h,n∗ H h Yh,n∗ h,n∗
h=1
whereH isthetotalnumberoftreesintheforest,X∗denotestherandomsubsetofexplanatoryvariablessampledfor
h
eachnodeofthehthtreeintheensemble,R andµ arerespectivelytheregionsandthemeanonthebootstrapped
Yh h
samplethatdefinesthehthtree. Therefore,thenumberofrandomlypreselectedsplittingvariablesandthetotalnumber
oftreesintheforestareparametersthataffectthestabilityoftheresultsofRF.
Animportantfeatureofthismethodologyistheiruseofout–of–bagsamples,thebootstrappeddatawhicharenotused
tofitthetrees,fortheout–of–bagerrorestimates. TheOut–Of–Bagestimate(OOB)iscalculatedbypredictingthe
realvalueforeachobservationinthetrainset(X ,Y )trainbyusingonlythetreesforwhichthisobservationwasnot
i i
includedinthebootstrapsample. Asprovedby[27],theOOBestimatesareasaccurateasusingatestsetwithsize
equaltothetrainingset. ThereforetheuseoftheOOBestimatesremovestheneedofsplittingthedataintrainingand
testset.
2.3 VariableImportance
RTsandRFcanbeusedtorankthepredictiveimportanceofthevariables. ForRTs,[2]definedtheVImeasurethat
reflectstherelativeimportance,orcontribution,ofeachinputvariableinpredictingtheresponse. Ifitissimpletothink
aboutthecontributionofasplittingvariableliketherelativeimprovementinthedevianceofthemodel,moredifficult
canberankingthosevariablesthatneveroccurinthetreestructure. Withthisaim,theVIcanbecomputedwiththe
supportofthesurrogatesplit.
Howvariablesarerankedcanbecriticalforthosethatdonotreturnthebestsplit,butthesecondorthirdbestsplit. For
example,itmayhappenthatX neverentersthetreestructureifX ispresent. ByremovingthevariableX fromthe
1 2 2
4APREPRINT-JULY22,2024
model,X mayoccurprominentlyinthesplits,leadingtoatreeasaccurateasthepreviousone. Theanswerisinthe
1
usageofthesurrogatesplits. LetX˜ definethesurrogatevariable,whichisavariablethatmostaccuratelypredictsthe
j
actionofthebestsplitsonX selectedbytheCARTalgorithm. ThemeasureofVIofX isdefinedas
j j
(cid:88)
VI(X )= ∆Dev(T) (3)
j Xj∪X˜
j
m∈T
wheremaretheterminalnodesofthetreeT and∆Dev(T) representsthedecreaseofthedeviancedonebyX
Xj∪X˜
j
j
anditssurrogate. IfthereexistmorethanonesurrogatevariableforX atanynode,usetheonewithlarger∆Dev(T)
j
in (3). Themeasureofimportanceofvariablesgenerallyusedarenormalizedquantities,100VI(X )/max VI(X ),
j j j
sothemostimportantfeaturehasmeasure100,theothersareintherangefrom0to100.
ForRandomForests,theVI(RF_VI)iscomputedbytheMeanDecreaseAccuracy(MDA)[7]. LetX thevariable
j
forwhichtheimportancehastobeevaluated. TheMDA,reliestothepermutationprincipleandinvolvestheOOB
estimates. LetOOB betheout–of–bagsampleofthehthtree. LetX betheinputvariablevectorwherethejth
h j,perm
variablehasbeenpermuted,andletOOB beitscorrespondingout–of–bagsample. TheMDAforX isdefined
h,perm j
by:
H (cid:20) (cid:21) (cid:20) (cid:21)
1 (cid:88) 1 1
RF_VI(X )= (Y −T (X))2 − (Y −T (X ))2 (4)
j H |OOB| rf |OOB| rf j,perm
h h,perm
h=1
thatistheaveragedifferenceinaccuracyoftheout–of–bagversuspermutedout–of–bagobservationsovertheH trees.
ThisisthevariableimportancemeasureforX .
j
Forthescopeofthispaper,weconsidertwoothermeasuresofVIdevelopedin[21]and[23]: theConditionalVIand
theSobol–MDAVI.TheConditionalVI(CF–VI)isdefinedas:
H (cid:20) (cid:21) (cid:20) (cid:21)
1 (cid:88) 1 1
CF_VI(X )= (Y −T (X))2 − (Y −T (X ))2 (5)
j H |OOB| rf |OOB| rf j,perm|Z
h h,perm
h=1
whereX arethevaluesofthevariableX afterpermutingitsvalueswithinthegriddefinedbytheconditioning
j,perm|Z j
variablesZ.TherationaleforthechoicetoconditioningalsotoZ istodecreasetheimportanceofcorrelatedvariables,
whichisgenerallyoverestimatedintheRF_VI.TodeterminethevariablesZtobeconditionedon,themostconservative
choicewouldbetoincludeallothervariablesasconditioningvariables. Another,moreintuitive,choiceistoinclude
only those variables whose empirical correlation with the variable of interest exceeds a certain threshold. For the
moregeneralcaseofpredictorvariablesofdifferentscalesofmeasurementtheframeworkpromotedin[22]provides
p–valuesofconditionalinferencetestsasmeasuresofassociation.
TheSobol–MDAVI(S_MDA-VI)isdefinedas:
H (cid:20) (cid:21)
1 1 (cid:88) 1
S_MDA−VI(X )= (Y −T (X))2 −
j V(Y)H |OOB| rf
h
h=1 (6)
(cid:20) (cid:21)
1
(Y −Tproject(X ))2
|OOB| rf −j
h
whereTproject(X )istheprojectedforest,arandomforestsobtainedasasumofprojectedtrees: inthesekindof
rf −j
trees,thepartitionofthecovariatespaceobtainedwiththeterminalleavesoftheoriginaltreeisprojectedalongthe
j−thdirection,andtheoutputsofthecellsofthisnewprojectedpartitionarerecomputedwiththetrainingdata. This
allowstoobtaintheaccuracyoftheassociatedout–of–bagprojectedforestestimate,andbysubtractingitfromthe
originalaccuracyandnormalizingbythevarianceofY onobtaintheSobol–MDAVIforX .
j
3 GlobalSensitivityAnalysis
Inthissection,wefirstreviewSobol’sensitivityindices[29,30,19],andthenweintroducetheGSAparadigm.
3.1 Reviewofsensitivitymeasures
Sobol’sensitivityindicesdecomposethevarianceofthemodeloutputV(Y)intotermsofincreasingdimensionality:
k
(cid:88) (cid:88)(cid:88)
V(Y)= V + V +...+V , (7)
i ij 1,2,...,k
i=1 i i<j
5APREPRINT-JULY22,2024
where
(cid:2) (cid:3) (cid:2) (cid:3)
V =V E (Y|X ) V =V E (y|X ,X )
i Xi X∼i i ij Xi,Xj X∼i,j i j
(cid:2) (cid:3)
−V E (Y|X ) (8)
Xi X∼i i
(cid:2) (cid:3)
−V E (Y|X )
Xj X∼j j
andsimilarrelationsforthehigherorderterms[30]. V isthatpartofthevarianceV(Y)thatisonlyduetoX ,V the
i i ij
partduetoX andX onV(Y),andsoon. E (Y|X )indicatesameanofY thatistakenoverallinputsexceptX .
i j X∼i i i
Sobol’indicesarecomputedas
V V
S = i S = ij ... . (9)
i V(Y) ij V(Y)
whereS ,S arethefirstorderandsecondordertermsduerespectivelytoX and(X ,X ). S ,S ,... representthe
i ij i i j i ij
expectedreductioninvariancethatwouldbeachievebyfixingX ,(X ,X )respectively.
i i j
Animportantmeasureisthetotal–orderindexT ,thatincludesallterms–ofthefirstorderandhigher,thatincludeX
i i
[19]. T >S ,x flagsthepresenceofinteractionterm(s)involvingX . T isdefinedas
i i i i i
(cid:2) (cid:3) (cid:2) (cid:3)
V E (Y|X ) E V (Y|X )
T =1−
X∼i Xi ∼i
=
X∼i Xi ∼i
. (10)
i V(Y) V(Y)
AmongtheseveralestimatorsavailabletoestimateEquations9–10weusethoseindicatedin [31].
(cid:104) (cid:105)2
V(Y)− 1 (cid:80)N f(B) −f(A(i))
2N v=1 v B v
S = , (11)
i V(Y)
(cid:104) (cid:105)2
1 (cid:80)N f(A) −f(A(i))
2N v=1 v B v
T = . (12)
i V(Y)
Furtherdiscussionoftheseestimatorsisofferedin[31].
3.2 GlobalSensitivityAnalysisParadigm
Thesensitivityindexreviewedinthepreviousparagraphsandbasedonthedecompositionofthevarianceofthemodel
outputarethemaintoolsusedinGSA.Towhatextentcanonetalkofaglobalsensitivityanalysisparadigm? Itis
generallyacceptedthatinmodelvalidationoncanhavetwoparadigms,onemoretechnicalandonemoresocio–political
orparticipatory [32]. Theideahasaparallelinsociologyofquantification,whereonealsocallsforadoubledimension
ofqualityisstatisticalwork [33,34],againonetechnicalandonenormative. Aradicalvisionofsensitivityanalysis
isofferedby [35]forwhomtherealstrengthofthemodelsisin“sensitivityanalysis(whereonecouldexaminethe
responseofthemodeltoparametersorstructuresthatwerenotknownwithprecision."Econometricians [36]and [37]
alsoinsistonthenexttoexplorewidelyaroundtheassumptionofastudytoseeifitsinferenceisrobust. Morerecently
theseideashaveresurfacedinrelationtotheattempttoanswertheproblemofscarcereproducibilityofquantitative
analysis [38],withactualexperimentsperformedbyseveralteamsattemptingtoreplicatethesameanalysis [39]–
alineofinvestigationthathasrevealed“Auniverseofuncertaintyhidinginplainsight" [40]. Adoptingsensitivity
analysis–orbetterGSA [41]asaparadigm,impliestacklingthisuncertaintyheadon,bypropagatingallplausible
uncertaintiesandambiguities–bethesetechnicalornormative,throughtheassessment. Someinvestigatorscallthis
“multiverseanalysis" [42],whilewecalliteitherglobalsensitivityanalysisorinamoreverbosedefinition“modelling
ofthemodellingprocess" [43].
4 GlobalSensitivity–BasedRankingofVariablesforExplainableRandomForests
ToenhancetheexplainabilityofRFVI,weproposetousethetotalsensitivitymeasureofEquation12asanindicatorof
VImeasureinaGSAparadigm: theRF_GS-VI.ThetotalsensitivityindexinGSAisbasedonasystematicexploration
ofthespaceofthemodelinputtogettheinfluenceonthemodeloutput. WeestimateS usinganestimatoranda
Ti
structuredsampleconstructedasin[31]. Thiskindofestimatorisemployedalsoin[16]torankregressorsinterms
oftheirimportanceinaregressionmodel. Specifically,giventhesampledata{Y ,X }letusconsiderγ RFmodels.
i i
Letq(γ)beameasureofthemodelfit. SincetheBICandAICmeasuresusedby[16]arenotapplicabletomodelsas
RF,weproposedasq(γ)theRootMeanSquaredError(RMSE).InordertocomputetheS oneshouldbeableto
Ti
6APREPRINT-JULY22,2024
computeallq(γ)forallγ ∈Γ,whichcouldbepracticallyunfeasible. Therefore,generatearandomdrawofγ inΓ,
sayγ ;thenconsiderelementsγ(i)withallelementsequaltoγ exceptforthei–thcoordinatewhichisswitchedfrom
∗ ∗ ∗
0to1orvice–versa. ThisisusedtocalculatetheVI(γ)andapplytheestimatorof[31]. Thisprocessisdescribedin
Algorithm1.
Algorithm1:PseudocodeforRF_GS-VI
1 {Y i,X i},i=1,...,n,length(X i)=p
2 forl=1toLdo
3 Sampleγ linΓ∼DiscreteUniform(0,1);
4 FitY =T rf(X γl);
5 Evaluateq l=q(γ l);
6 repeat
7 Takethek−thelementofγ l,andswitchitto0ifitisequalto1,andto1ifitis0.
8 Denotethisnewvectorwithinvertedk−thelementasγ l(k);
9 Evaluateq kl=q(γ l(k));
10 untilk=p;
11 Sˆ Ti = σˆ VT ˆ2 i = σˆ VT2 ˆi == N4 11 −N 1(cid:80) (cid:80)N l= N l=1 1( (q qk ll −− q¯q )l 2)2 .
5 Simulationstudy
The simulation study is aimed to explore some data generating process (DGP) in terms of generative importance
betweenpredictorsandresponsevariable. Weexploredthreescenariosthatcoverdifferenttypeofregressionfunctions.
SCENARIO1
X =ϵ
1 1
X =aX +ϵ forj =2,...,4 (13)
j 1 j
Y =bX +bX +bX +ϵ .
2 3 4 Y
where the response Y depends directly on three intermediate variables X , X and X , and it is only indirectly
2 3 4
dependent on the background variable X . Therefore, Y⊥⊥X |X ,X ,X . We set a = b = 3, ϵ ∼ N(0,1),
1 1 2 3 4 j
j =1,...4,ϵ ∼N(0,1),Y =1,...n.
Y
SCENARIO2
X =ϵ
1 1
X =aX +ϵ
2 1 2
X =aX +ϵ forj =3,4 (14)
j 2 j
X =aX +ϵ
5 2 5
Y =bX +bX +ϵ .
3 4 Y
wheretheresponseY dependsdirectlyontwointermediatevariablesX andX ,anditisonlyindirectlydependenton
3 4
thebackgroundvariablesX andX .Moreover,thereisnodirectassociationbetweenY andX .Therefore,Y⊥⊥X |X ,
1 2 5 1 2
Y⊥⊥X |X ,X , Y⊥⊥X |X and Y⊥⊥X |X ,X . We set a = b = 3, ϵ ∼ N(0,1), j = 1,...5, ϵ ∼ N(0,1),
2 3 4 5 2 5 3 4 j Y
Y =1,...n.
SCENARIO3
X =ϵ
1 1
X =aX +ϵ
2 1 2
X =ϵ (15)
3 3
Y =bX +bX +ϵ
2 3 Y
X =cX +cY +ϵ
4 2 4
wheretheresponseY dependsdirectlyontwointermediatevariablesX andX ,anditisonlyindirectlydependenton
2 3
thebackgroundvariableX . Moreover,thecovariateX isdirectlydependenttotheresponseandtheintermediate
1 4
variable X −2. Therefore, Y⊥⊥X |X . We set a = b = 3 and c = 2, ϵ ∼ N(0,1), j = 1,...4, ϵ ∼ N(0,1),
1 2 j Y
7APREPRINT-JULY22,2024
X1
X1
a
X1 a
X2
a a a a X2 X3
a a
b
b
X2 X3 X4 X3 X4 X5
c Y
b b b
b b c
Y Y X4
(1) Scenario 1 (2) Scenario 2 (3) Scenario 3
Figure1: TheDirectAcyclicGraph(DAG)oftherecursiveregressionsystemsforequations13(1),14(2)and15(3).
Y =1 ,...n. Thisscenariorepresentsaninterestingcaseinwhichthereisavariableaffectedbytheresponse,butthe
researc herincludesallvariablesasexplanatoryvariables.
Figure1showstheDirectAcyclicGraph(DGP)foreachsystemofregressionequationsdepictedinthethreescenario,
whileTable5summarizestheresultingconditionalindependencesbetweentheresponseandtheexplanatoryvariables
andamongtheexplanatoryvariables.
CONDITIONALINDEPENDENCES
X ⊥⊥X |X ;X ⊥⊥X |X ;X ⊥⊥X |X ;
SCENARIO1 2 3 1 2 4 1 3 4 1
Y⊥⊥X 1|X 2,X 3,X
4
X 1⊥⊥X 3|X 2;X 1⊥⊥X 4|X 2;X 1⊥⊥Y|X 3,X 4;X 1⊥⊥X 5|X 2;
SCENARIO2 X 3⊥⊥X 4|X 2;X 3⊥⊥X 5|X 2;X 4⊥⊥X 5|X 2;
Y⊥⊥X 1|X 2;Y⊥⊥X 2|X 3,X 4;Y⊥⊥X 5|X 2;Y⊥⊥X 5|X 3,X
4
SCENARIO3 X 1⊥⊥X 3;X 1⊥⊥X 4|X2;X 2⊥⊥X 3;X 3⊥⊥X 4|X 2,Y;Y⊥⊥X 1|X
2
Table1: Conditionalindependencesbetweentheresponseandtheexplanatoryvariablesandamongtheexplanatory
variablesforthethreeDGPproposed.
AlltheresultsofthispaperwereobtainedbyRstatisticalsoftware(RCoreTeam2023,version4.2.3). Forevaluation
purposes,weconsideredasamplesizeofn=1000foreachscenario,andweinvestigated1000datasets.Wecompared:
• RF_GS-VI, our proposal, implemented as a user–written function with the usage of the function
randomForest;
• CART-VI, the VI measure of CART algorithm proposed by [2] and implemented in the package rpart,
functionrpart;
• RF-VI,theVImeasureofRFalgorithmproposedby[3]andimplementedinthepackagerandomForest,
functionrandomForest;
• CF-VI,theConditionalVIproposedby[21]andimplementedinthepackageparty,functionscforestand
varimp;
• S_MDA-VI,theVImeasureforRFproposedby[23]andimplementedinthepackageSobolMDAbasedon
thefastRFpackageranger,functionranger.
8APREPRINT-JULY22,2024
Scenario 1
0.9
0.6 Variable
X1
X2
X3
X4
0.3
0.0
Alg1: RF_GS−VI Alg2: CART−VI Alg3: RF−VI Alg4: CF−VI Alg5: S_MDA−VI
Figure2: ViolinplotofMonteCarlodistributionsofRF_GS-VI,CART-VI,RF-VI,CF-VIandS_MDA-VIforthe
Scenario1,n=1000.
InthissimulationstudywehaveexaminedtheMonteCarlodistributionsoftheaboveVI:theviolinplots(boxplot
anddensityplot)areshowninFigure2fortheDGPofscenario1,Figure3fortheDGPofscenario2andFigure4
fortheDGPofscenario3,whileTable5summarisesthesedistributionsintermsofMonteCarlomeansandstandard
deviations(inbrackets). Table5alsoshowstheproportionsofvariablescorrectlyrankedinthefirstposition. According
toscenario1,weconsideredacorrectrankinginthefirstpositionifthevariableisX ,X orX . Forscenario2,we
2 3 4
consideredacorrectrankingatthefirstpositionifthevariableisX orX . Forscenario3,weconsideredacorrect
3 4
rankingatthefirstpositionifthevariableisX orX .
2 3
Forscenario1,figure2(andtable5)showsthatwiththeproposedRF_GS-VI,themeanofthedistributionofVIs
islowerforthevariableX andhigherforthevariablesX ,X andX . AlsoforCF-VIandS_MDA-VIweobtain
1 2 3 4
alowermeanoftheVIforthevariableX withrespecttotheothervariables. Onthecontrary,forbothCART-VI
1
andRF-VI,thehighestvalueofthemeanisobtainedfromX . Fromtheproportionsofcorrectvariablesinthefirst
1
positionoftherankingoftheVIsinTable5,itcanbeseenthatwithourproposal,approximately80%oftheMonte
Carloreplicationshaveacorrectvariableinthefirstpositionoftheranking. ForRF-VI,thispercentagedecreasesto
about60%ofthecases,whileforCART-VI,X isthemostimportantvariableinalmostallMonteCarloreplications.
1
ForCF-VI,about88%oftheMonteCarloreplicationshaveacorrectvariableinthefirstpositionoftheranking,while
inthetotaloftheMonteCarloreplicationswithS_MDA-VI,acorrectvariableisobtainedinthefirstposition.
Forscenario2,figure3(andtable5)showsthatwiththeproposedRF_GS-VI,themeanofthedistributionoftheVIsis
lowerforX andX ,anditishigherforthevariablesX ,X andX ,withthemaximumvaluesofthemeanreached
1 5 2 3 4
bytheVIsofX andX . FortheCART-VIandRF-VImeans,theminimumvalueisobtainedfromX andX ,while
3 4 1 5
foralltheothervariablesthemeansarequitesimilar. ForCF-VI,thehighestvaluesofthemeansareobtainedfor
thevariablesX andX ,andthensmallervaluesforthevariablesX ,followedbyX andX . Thesameresultsare
3 4 2 5 1
obtainedfromS_MDA-VI.FromtheproportionsofcorrectvariablesinthefirstpositionoftherankingoftheVIsin
Table5,itcanbeseenthatwithourproposal,about70%oftheMonteCarloreplicationshaveacorrectvariableinthe
firstpositionoftheranking. Thispercentagedecreasestoabout56%ofthecasesforRF-VI,whileforCART-VIit
isabout70%. ForCF-VIandforthesumoftheMonteCarloreplications,acorrectvariableisobtainedinthefirst
position.
Forscenario3,figure4(andtable5)showsthatwithourproposal,themeanofthedistributionofVIsislowerfor
X andX ,anditishigherforthevariablesX andX ,withtheVIofX reachingthemaximumvalue. Fromthe
1 3 2 4 4
averagesofCART-VI,RF-VI,CF-VIandS_MDA-VI,theminimumvalueisreachedbyX ,whileforalltheother
3
variablestheaveragesarequitesimilar,withtheVIofX alwaysreachingthemaximumvalue. Fromtheproportions
4
ofcorrectvariablesinthefirstpositionoftherankingoftheVIsinthetable5,itcanbeseenthatwithourproposal
about26%oftheMonteCarloreplicationshaveacorrectvariableinthefirstpositionoftheranking. Forallother
algorithmsconsidered,thispercentageisabout0%ofthecases.
9
.pmi.raVAPREPRINT-JULY22,2024
Scenario 2
0.9
0.6 Variable
X1
X2
X3
X4
X5
0.3
0.0
Alg1: RF_GS−VI Alg2: CART−VI Alg3: RF−VI Alg4: CF−VI Alg5: S_MDA−VI
Figure3: ViolinplotofMonteCarlodistributionsofRF_GS-VI,CART-VI,RF-VI,CF-VIandS_MDA-VIforthe
Scenario2,n=1000.
Scenario 3
1.00
0.75
Variable
X1
0.50 X2
X3
X4
0.25
0.00
Alg1: RF_GS−VI Alg2: CART−VI Alg3: RF−VI Alg4: CF−VI Alg5: S_MDA−VI
Figure4: ViolinplotofMonteCarlodistributionsofRF_GS-VI,CART-VI,RF-VI,CF-VIandS_MDA-VIforthe
Scenario3,n=1000.
10
.pmi.raV
.pmi.raVAPREPRINT-JULY22,2024
X X X X X
1 2 3 4 5
SCENARIO1
RF_GS-VI 1.8 2.3 2.3 2.3
(1.2) (1.9) (1.9) (1.9)
CART-VI 705035.2 527604.2 527526.7 527820.5
(33004.9) (30279.9) (29849.3) (30674.84)
RF-VI 194304.5 186668.7 186681.3 186659.2
(12615.4) (12203.9) (12279.0) (12488.8)
CF-VI 8.0 9.8 9.9 9.8
(2.1) (1.2) (1.2) (1.2)
S_MDA-VI -0.002 0.011 0.011 0.011
(0) (0.001) (0.001) (0.001)
SCENARIO2
RF_GS-VI 0.1 1.6 2.0 2.1 0.7
(0.06) (1.8) (1.9) (2.0) (0.6)
CART-VI 2286063 2984104 2957274 2959452 2820692
(128474.4) (148527.5) (156767.9) (159303.7) (137696.6)
RF-VI 567700.4 675789.5 674645.8 675950.3 658084.8
(46294.1) (48511.0) (49407.6) (47864.7) (48150.01)
CF-VI 0 27.6 44.3 43.9 0.1
(0.004) (9.191) (7.935) (7.717) (0.200)
S_MDA-VI 0 0 0.003 0.003 0
(0) (0) (0) (0) (0)
SCENARIO3
RF_GS-VI 1.1 3.7 0.8 5.4
(0.7) (7.6) (1.5) (9.2)
CART-VI 66667.6 75420.7 9841.8 96045.5
(3845.5) (4024.7) (2452.3) (4273.3)
RF-VI 26470.3 31174.3 6196.0 35655.0
(1764.4) (1825.5) (600.6) (1865.1)
CF-VI 0 0.5 0.4 19.3
(0.005) (0.093) (0.177) (1.222)
S_MDA-VI 0 0 0.002 0.014
(0) (0) (0) (0.001)
Table2: MonteCarloaveragesandstandarddeviations(inparentheses)ofVImeasuresforsimulateddatagenerated
accordingtotheDAGofFigure1.
Fromthesimulationstudyitisclearthatitisveryeasytogetawrongrankingofvariableswhenusingtheclassical
VImeasuresofCARTandRFtoinfertheunderlyinggeneratingprocessofthedata. Scenario1representsacase
also studied in [44], where the role of induced correlations in greedy search algorithms such as CART and RF is
highlighted. Inthiscase,usingtheGSAalgorithmwiththeRFalgorithmcanhelptorestorethecorrectrankingof
variablesinthegenerativesense. NotethatevenusingVIfromConditionalVIforRandomForestsandSobol-MDAVI
thecorrectrankingisobtained. Lookingattheproportionsofcorrectvariablesinthefirstpositionoftherankingof
VIinthissimulationstudy,theproposedalgorithmranksoneofthecorrectvariablesinthefirstpositionalmost80%
ofthetime,whileRF-VIgivesacorrectvariableinthefirstpositioninabouthalfofthesimulationsandCART-VIis
practicallyalwayswrong. Scenario2representsamoredifficultDGP,wheretheVIsofCARTandRFonaveragedo
notdiscriminatewellbetweendirectandindirectrelationsbetweenthevariablesX ,X andX ,whiletheproposed
2 3 4
algorithmgivesacorrectsortingonaverage. TheConditionalVIforRandomForestsandtheSobol-MDAVIalsogive
correctrankings. LookingattheproportionsofcorrectvariablesatthefirstpositionoftheVIranking,theproposed
algorithmandtheCARTalgorithmgivethesameproportion,whilethereisagaininusingourproposalwithrespectto
theRFalgorithm. Interestingly,withCF-VIandS_MDA-VI,weobtainacorrectvariableinthefirstpositioninallthe
simulations. Finally,scenario3isaveryinterestingcasestudyalsointermsofcausalinference,andwhiletheVIs
ofCART,RF,ConditionalVIforRFandSobol-MDAVIalwaysputX asthemostimportantvariable,ourmethod
4
managestoreturnacorrectvariableinthefirstpositioninatleast30%ofthecases.
11APREPRINT-JULY22,2024
RF_GS-VI RF-VI CART-VI CF-VI S_MDA-VI
SCENARIO1 0.794 0.526 0.001 0.875 1
SCENARIO2 0.696 0.564 0.698 1 1
SCENARIO3 0.264 0.018 0 0 0
Table3: ProportionsofcorrectvariableatfirstpositionofrankingofVIforeachscenarioandalgorithm.
Inthissimulationstudy,theresultingVIdistributionsfromourproposalshowgreatervariabilitythanthoseobtained
withtheotheralgorithms. Thismaybehelpfultotheresearcher,sinceusingourmethodalongsidetheVIofmore
well–knownmethodssuchasCARTandRFmayshedlightonadiscrepancybetweentheorderingofthevariables,
which could be due precisely to a different predictive and generative ranking. Finally, it should be noted that the
Sobol–MDAproposalachievesexcellentperformanceforscenarios1and2,butwhenlookingatthevaluesoftheVIs,
theyarepracticallyallsimilarandcloseto0. Thismayleadtoerroneousconclusionsfortheresearcherlessfamiliar
withthetheoryunderlyingthemethodology.
6 Applications
In this section we compute the VI for the five approaches, RF_GS-VI, RF-VI, CART-VI, CF-VI and S_MDA-VI
fortwodatasets: theenergyefficiencydataset[45],andtheliverdisordersdataset[46],bothavailableontheUC
IrvineMachineLearningRepository(https://archive.ics.uci.edu). Theenergyefficiencystudyconcernsenergyanalysis
using12differentbuildingshapessimulatedinEcotect. Thebuildingsdifferintermsofglazingarea,glazingarea
distribution,orientation,height,wallarea,roofarea,floorareaandrelativecompactness. Aftersimulatingvarious
settingsasafunctionoftheabovecharacteristics,768buildingshapeswereobtained. Therefore,thedatasetconsistsof
n=768samplesandX =8explanatoryvariables,withtheaimofpredictingtworeal–valuedresponses,theheating
p
load(Y )andthecoolingload(Y ). Thestudyonliverdiseasesconcernsliverdiseasescausedbyexcessivealcohol
1 2
consumption. Thedatasetconsistsofn=345maleindividualandX =5explanatoryvariables,whichrepresents
p
bloodparametersthatarethoughttobesensitivetoliverdisorders. Specifically,thesebloodparametersarethemean
corpuscularvolume,thealkalinephosphotase,thealanineaminotransferase,theaspartateaminotransferaseandthe
gamma-glutamyltranspeptidase. Thetaskistopredictareal–valuedresponse,thenumberofhalf–pintequivalentsof
alcoholicbeveragesdrunkperdayY.
Weperforma5−foldCVonthedatasetandcomputeforourproposal,RF,ConditionalVIforRFandSobol-MDA
100MonteCarloreplicationswithinthefoldstoobtaintheVI.WethenaveragethevaluesoftheVIacrossthefolds
andobtaintheMonteCarlodistributionsoftheVIrelativestoeachexplanatoryvariable. FortheCARTalgorithm,we
obtaintheVIforeachexplanatoryvariableonlyfromthe5−foldsandaveragethevaluesacrossthesefolds.
Fortheenergyefficiencydataset,theplotsoftheMonteCarloreplicationsforRF_GS-VI,RF-VI,CF-VIandS_MDA-
VIareshowninFigures5and6forthemodelsforY andY respectively. ForCART-VI,theaverageofVIoverthe
1 2
5-foldsisgivenintable6. NotethatforCART-VItheVIforX isnotreportedasitisnotcalculatedbythealgorithm
6
inthemajorityoffolds. ForY ,RF_GSA-VIreportsX ,X ,X ,X andX asthevariableswiththehighestvalues
1 1 2 7 4 5
ofVI,bothRF-VIandCART-VIreportX ,X ,X andX asthevariableswiththehighestvaluesofVI,whileboth
1 2 4 5
CF-VIandS_MDA-VIindicateX ,X andX asthevariableswiththehighestvaluesofVI.Inthiscasewecan
7 3 2
concludethatourproposalcanhighlightthevariableX asapossibleinfluentialvariable,andthisisalsoconfirmedby
7
theconditionalVIforRFandSobol-MDA,whilewiththeCARTandRFVIsthisvariablewouldneverbeconsidered
asinfluential. ForY ,RF_GSA-VI,RF-VIandCART-VIgiveX ,X ,X andX asthevariableswiththehighest
2 1 2 4 5
valuesofVI,whileCF-VIgivesX ,X andX asthevariableswiththehighestvaluesofVI,andS_MDA-VIgives
2 3 7
X ,X andX asthevariableswiththehighestvaluesofVI.Therefore,forY wecanconcludethatourproposal
1 2 7 2
confirmstherankingoftheRFandCARTalgorithms,whiletheconditionalVIforRFandSobol–MDAalsohighlights
theimportanceofX ,whileitseemstobeabletoconsiderX andX aslessinfluential.
7 4 5
FortheLiverdisordersdataset,theplotsoftheMonteCarloreplicationsforRF_GS-VI,RF-VI,CF-VIandS_MDA-VI
are shown in Figure 7. For CART-VI, the average VI over the 5 replicates is shown in Table 6. From Figure 7,
RF_GSA-VIindicatesX andX asthevariableswiththehighestvaluesofVI,RF-VIindicatesX andX asthe
1 5 5 3
variableswiththehighestvaluesofVI,whilebothCF-VIandS_MDA-VIindicateX andX asthevariableswiththe
1 5
highestvaluesofVI.Fromthetable6,CART-VIshowsX andX asthevariableswiththehighestvaluesofVI.In
4 5
thiscase,wecanconcludethatallthetestedalgorithmshighlighttheimportanceofthevariableX ,whileourproposal
5
canhighlightthevariableX asapossibleinfluentialvariable,andthisisalsoconfirmedbytheconditionalVIforRF
1
andSobol–MDA,whilewiththeVIsofCARTandRFthisvariablewouldnotbeconsideredasinfluential.
12APREPRINT-JULY22,2024
A B
15000
1.0
10000
0.5
5000
Variable
0.0 0 X1
X2
RF_GS−VI RF−VI X3
C D X4
X5
7.5 X6
X7
0.04 X8
5.0
0.02
2.5
0.00
0.0
CF−VI S_MDA−VI
Figure5: Energydatasetanalysis: boxplotsof100MonteCarloreplicationsofVImeasuresforY : inAofRF_GS-VI;
1
inBofRF-VI;inCofCF-VI;inDofS_MDA-VI.
A B 15000
4
3 10000
2
5000
1
Variable
0
0 X1
X2
RF_GS−VI RF−VI X3
C 5 D X4
X5
0.03 X6
4 X7
X8
3
0.02
2
0.01
1
0 0.00
CF−VI S_MDA−VI
Figure6: Energydatasetanalysis: boxplotsof100MonteCarloreplicationsforY : inAofRF_GS-VImeasure;inB
2
ofRF-VImeasure;inCofCF-VImeasure;inDofS_MDA-VImeasure.
13
.I.V
.I.V
.I.V
.I.V
.I.V
.I.V
.I.V
.I.VAPREPRINT-JULY22,2024
CART-VI
Y Y
1 2
X 54699.203 48750.940
1
X 54699.203 48750.940
2
X 21666.449 18965.702
3
X 49429.970 44561.670
4
X 49429.970 44561.670
5
X - -
6
X 4052.775 1164.655
7
X 1730.266 508.153
8
Table4: Energydatasetanalysis: averageacrossthe5–foldCVofCART-VIforeachexplanatoryvariableandthetwo
responses.
A 1.00 B
0.75
550
0.50
500
0.25
Variable
RF_GS−VI RF−VI X1
X2
C D X3
1.00 X4
X5
0.050
0.75
0.50 0.025
0.25
0.000
0.00
CF−VI S_MDA−VI
Figure7:Liverdisordersdatasetanalysis:boxplotsof100MonteCarloreplicationsforY:inAofRF_GS-VImeasure;
inBofRF-VImeasure;inCofCF-VImeasure;inDofS_MDA-VImeasure.
CART-VI
X 355.670
1
X 222.046
2
X 382.772
3
X 465.039
4
X 442.684
5
Table5: Liverdisordersdatasetanalysis: averageacrossthe5-foldCVforCART-VIforeachexplanatoryvariableand
thetworesponsesofLiverdisordersdataset.
7 Conclusions
MachineLearning techniquesare increasinglyusedin anumber ofscientificdomains, by apopulation ofusersat
presentlargelyexceedingtherestrictcommunityofdataanalysispractitioners. Asaresult,theVariableImportance
featureofsupervisedalgorithmsmayhappentobeinterpretedintermsofgenerativeimportanceratherthaninthe
morepreciseandcircumscribedtermsofpredictiveimportance. InthispaperweapplyGlobalSensitivityAnalysisto
14
.I.V
.I.V
.I.V
.I.VAPREPRINT-JULY22,2024
RandomForeststoranktheinputfeaturesbytheirgenerativeimportance,withtheexpectationthatthiswillcontribute
totheexplainabilityofthemachinelearningmethodsingeneral.
Fromatheoreticalpointofview,ouralgorithmcanbeextendedtoallkindsofsupervisedlearningalgorithmswith
VIfeatures,i.e. toboostingtrees. Furthermore,theGSAparadigmpresentedherecanbeextendedtootherstatistical
models. [16]appliedthisparadigmtovariableselectioninregression. Fromthesimulationstudyandtheapplications,
ourresultsseemtoindicateglobalsensitivityanalysisinindeedeffectiveinelucidatingthedatageneratingprocess
fortheexamplewheretheitisknown,andthatthecasecanalsobemadefortheapplicationwhereitisnot. More
specifically, globalsensitivityanalysiscanonlyfailinthecasescenario3, wheretheresponseY dependsdirectly
ontwointermediatevariablesX andX ,anditisonlyindirectlydependentonthebackgroundvariableX . This
2 3 1
representsachallengingsituationforcausalinference,forwhichcurrentlynoneofthealgorithmsusedinthisarticle
managestodetectthequalitativedifferencebetweenX andX ,andifthelatterisaccidentallyinsertedamongthe
3 1
predictors,allthetree–basedalgorithmstestedchooseitasthemostimportantvariable. Onthecontrary,ourproposal
succeedsinthistaskabout30%ofthetime.
FurtherworkisneededtodeterminewhethertheuseofameasuresuchastherandommeansquarederrorRMSE
usedinourproposalcanbeimprovedupon. Alsotobeaddressedarethetheoreticalpropertiesofourmeasure,anda
theory–basedmeasureofitserror.
References
[1] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The elements of statistical
learning: datamining,inference,andprediction,volume2. Springer,2009.
[2] LeoBreiman,JeromeFriedman,CharlesJStone,andRichardAOlshen. Classificationandregressiontrees. CRC
press,1984.
[3] Leo Breiman et al. Statistical modeling: The two cultures (with comments and a rejoinder by the author).
Statisticalscience,16(3):199–231,2001.
[4] YannickRothacherandCarolinStrobl. Identifyinginformativepredictorvariableswithrandomforests. Journal
ofEducationalandBehavioralStatistics,page10769986231193327,2023.
[5] LiangyuanHu,BianLiu,JiayiJi,andYanLi. Tree-basedmachinelearningtoidentifyandunderstandmajor
determinantsforstrokeattheneighborhoodlevel. JournaloftheAmericanHeartAssociation,9(22):e016745,
2020.
[6] NeerajDhingra,RajBridgelall,PanLu,JosephSzmerekovsky,andBhavanaBhardwaj. Rankingriskfactors
in financial losses from railroad incidents: a machine learning approach. Transportation research record,
2677(2):299–309,2023.
[7] LBreiman. Randomforests. Machinelearning,45(1):5–32,2001.
[8] DavidRCox. [statisticalmodeling: Thetwocultures]: Comment. StatisticalScience,16(3):216–218,2001.
[9] GalitShmueli. ToExplainortoPredict? StatisticalScience,25(3):289–310,2010.
[10] BertrandIooss,RonKenett,andPiercesareSecchi. DifferentViewsofInterpretability,pages1–20. Springer
InternationalPublishing,Cham,2022.
[11] RiccardoGuidotti,AnnaMonreale,SalvatoreRuggieri,FrancoTurini,FoscaGiannotti,andDinoPedreschi. A
surveyofmethodsforexplainingblackboxmodels. ACMComputingSurveys(CSUR),51(5):93,2018.
[12] AndreaSaltelli. Sensitivityanalysisforimportanceassessment. Riskanalysis,22(3):579–590,2002.
[13] StefanoTarantola,FedericoFerretti,SamueleLoPiano,MariiaKozlova,AlessioLachi,RossanaRosati,Arnald
Puy, Pamphile Roy, Giulia Vannucci, Marta Kuc-Czarnecka, and Andrea Saltelli. An annotated timeline of
sensitivityanalysis. EnvironmentalModelling&Software,174:105977,2024.
[14] SamanRazavi, AnthonyJakeman, AndreaSaltelli, ClémentinePrieur, BertrandIooss, EmanueleBorgonovo,
ElmarPlischke,SamueleLoPiano,TakuyaIwanaga,WilliamBecker,StefanoTarantola,JosephH.A.Guillaume,
JohnJakeman,HoshinGupta,NicolaMelillo,GiovanniRabitti,VincentChabridon,QingyunDuan,XifuSun,
StefánSmith,RaziSheikholeslami,NasimHosseini,MasoudAsadzadeh,ArnaldPuy,SergeiKucherenko,and
HolgerR.Maier. Thefutureofsensitivityanalysis: Anessentialdisciplineforsystemsmodelingandpolicy
support. EnvironmentalModelling&Software,137:104954,2021.
[15] AndreaSaltelli,AnthonyJakeman,SamanRazavi,andQiongliWu. Sensitivityanalysis: Adisciplinecomingof
age. EnvironmentalModelling&Software,146:105226,2021.
15APREPRINT-JULY22,2024
[16] WilliamBecker,PaoloParuolo,andAndreaSaltelli.Variableselectioninregressionmodelsusingglobalsensitivity
analysis. JournalofTimeSeriesEconometrics,13(2):187–233,2021.
[17] ArnaldPuy,PierfrancescoBeneventano,SimonA.Levin,SamueleLoPiano,TommasoPortaluri,andAndrea
Saltelli. Modelswithhighereffectivedimensionstendtoproducemoreuncertainestimates. ScienceAdvances,
8(eabn9450),2022.
[18] AndreaSaltelliandStefanoTarantola.Ontherelativeimportanceofinputfactorsinmathematicalmodels.Journal
oftheAmericanStatisticalAssociation,97(459):702–709,2002.
[19] ToshimitsuHommaandAndreaSaltelli. Importancemeasuresinglobalsensitivityanalysisofnonlinearmodels.
ReliabilityEngineering&SystemSafety,52(1):1–17,1996.
[20] Anestis Antoniadis, Sophie Lambert-Lacroix, and Jean-Michel Poggi. Random forests for global sensitivity
analysis: Aselectivereview. ReliabilityEngineering&SystemSafety,206:107312,2021.
[21] Carolin Strobl, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, and Achim Zeileis. Conditional
variableimportanceforrandomforests. BMCbioinformatics,9:1–11,2008.
[22] Torsten Hothorn, Kurt Hornik, and Achim Zeileis. Unbiased recursive partitioning: A conditional inference
framework. JournalofComputationalandGraphicalstatistics,15(3):651–674,2006.
[23] ClémentBénard,SébastienDaVeiga,andErwanScornet. Meandecreaseaccuracyforrandomforests: inconsis-
tency,andapracticalsolutionviathesobol-mda. Biometrika,109(4):881–900,2022.
[24] V.N.VapnikandA.J.Chervonenkis. Thenecessaryandsufficientcondi-tionsforconsistencyofthemethodof
empiricalriskminimization. PatternRecognitionandImageAnalysis,1(3):284–305,1989.
[25] CarmelaIorio,MassimoAria,AntonioD’Ambrosio,andRobertaSiciliano. Informativetreesbyvisualpruning.
ExpertSystemswithApplications,127:228–240,2019.
[26] T.G.Dietterich. Ensemblemethodsinmachinelearning. InJ.KittlerandF.Roli,editors,InternationalWorkshop
onMultipleClassifierSystems,LectureNotesinComputerScience,pages1–15.Springer,2000.
[27] LeoBreiman. Baggingpredictors. MachineLearning,24(2):123–140,1996.
[28] YFreund,R.Shapire,andNAlbe. Ashortintroductiontoboosting. Journal-JapaneseSocietyForArtificial
Intelligence,14(5):771–780,1999.
[29] Il’yaMeerovichSobol’. Sensitivityanalysisfornonlinearmathematicalmodels: numericalexperience. Matem-
aticheskoeModelirovanie,7(11):16–28,1995.
[30] AndreaSaltelliandIlyaMSobol’. Abouttheuseofranktransformationinsensitivityanalysisofmodeloutput.
ReliabilityEngineering&SystemSafety,50(3):225–239,1995.
[31] Andrea Saltelli, Paola Annoni, Ivano Azzini, Francesca Campolongo, Marco Ratto, and Stefano Tarantola.
Variancebasedsensitivityanalysisofmodeloutput.designandestimatorforthetotalsensitivityindex. Computer
PhysicsCommunications,181(2):259–270,2010.
[32] SibelEker,ElenaRovenskaya,MichaelObersteiner,andSimonLangan.Practiceandperspectivesinthevalidation
ofresourcemanagementmodels. NatureCommunications,9(1):5359,December2018.
[33] RobertSalais. “Ladonnéen’estpasundonné”: Statistics,QuantificationandDemocraticChoice,page379–415.
: Utopia,EvidenceandDemocracy.PalgraveMacmillan,andreamennickenandrobersalaisedition,2022.
[34] AmartyaSen. Justice: Meansversusfreedoms. Philosophy&PublicAffairs,19(2):111–121,1990.
[35] CharlesA.S.Hall. SystemsEcologyandLimitstoGrowth: History, Models, andPresentStatus, page1–38.
Springer,Singapore,2020.
[36] EdwardELeamer. Sensitivityanalyseswouldhelp. TheAmericanEconomicReview,75(3):308–313,1985.
[37] PeterKennedy. Aguidetoeconometrics. Wiley-Blackwell;6edition,2008.
[38] AndrewGelmanandEricLoken. Thegardenofforkingpaths. WorkingPaperDepartmentofStatistics,Columbia
University,2013.
[39] Nate Breznau, Eike Mark Rinke, and Alexander Wuttke. Observing many researchers using the same data
and hypothesis reveals a hidden universe of uncertainty. Proceedings of the National Academy of Sciences,
119(44):e2203150119,November2022.
[40] PerEngzell. Auniverseofuncertaintyhidinginplainsight. ProceedingsoftheNationalAcademyofSciences,
120(2):e2218530120,January2023.
[41] AndreaSaltelli,S.Tarantola,andF.Campolongo. Sensitivityanaysisasaningredientofmodeling. Statistical
Science,15(4):377–395,November2000.
16APREPRINT-JULY22,2024
[42] SaraSteegen, FrancisTuerlinckx, AndrewGelman, andWolfVanpaemel. Increasingtransparencythrougha
multiverseanalysis. PerspectivesonPsychologicalScience,11(5):702–712,September2016.
[43] AndreaSaltelliandArnaldPuy. Whatcanmathematicalmodellingcontributetoasociologyofquantification?
HumanitiesandSocialSciencesCommunications,10(213),2023.
[44] Anna Gottard, Giulia Vannucci, and Giovanni Maria Marchetti. A note on the interpretation of tree-based
regressionmodels. BiometricalJournal,62(6):1564–1573,2020.
[45] AthanasiosTsanasandAngelikiXifara. Energy Efficiency. UCIMachineLearningRepository, 2012. DOI:
https://doi.org/10.24432/C51307.
[46] Richard S. Forsyth. Liver Disorders. UCI Machine Learning Repository, 1990. DOI:
https://doi.org/10.24432/C54G67.
17