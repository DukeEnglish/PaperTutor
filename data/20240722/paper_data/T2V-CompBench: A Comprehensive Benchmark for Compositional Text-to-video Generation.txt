T2V-CompBench: A Comprehensive Benchmark for
Compositional Text-to-video Generation
KaiyueSun1 KaiyiHuang1 XianLiu2 YueWu3 ZihanXu1 ZhenguoLi3 XihuiLiu1
1TheUniversityofHongKong 2TheChineseUniversityofHongKong
3HuaweiNoah’sArkLab
Project: https://t2v-compbench.github.io/ Code: https://github.com/KaiyueSun98/T2V-CompBench
(a) Prompt Suite (b) Evaluation Metrics (c) Benchmarking T2V Models
MLLM-based evaluation metrics
Describe the video.
The video shows...
CCoonnssiisstteenntt Evaluate the alignment with text.
GG nnee uunn mmee eerraa rraattii ccvv yyee aa bbtttt iinnrrii ddbb iiuu nntt ggee score:3, explanation: this video...
Detection-based evaluation metrics
DDyynnaammiicc
Physical
iinnttOO eerrbb aajj ccee ttcc iioott nnss aa bbtttt iinnrrii ddbb iiuu nntt ggee
Color
d
1
change d
MMoottiioonn SSppaattiiaall 2
bbiinnddiinngg rreellaattiioonnsshhiippss Object locations score Object depths (3D relationships)
AAccttiioonn Tracking-based evaluation metrics
bbiinnddiinngg
Foreground motion score Background motion
Figure1: OverviewofT2V-CompBench. WeproposeT2V-CompBench,acomprehensivecompo-
sitionaltext-to-videogenerationbenchmarkthatconsistsofsevencategories: consistentattribute
binding,dynamicattributebinding,spatialrelationships,actionbinding,objectinteractions,
andgenerativenumeracy. Weproposethreetypesofevaluationmetrics: MLLM-based,detection-
based,andtracking-basedmetrics. Webenchmarkvarioustext-to-videogenerationmodels.
Abstract
Text-to-video (T2V) generation models have advanced significantly, yet their
abilitytocomposedifferentobjects,attributes,actions,andmotionsintoavideo
remainsunexplored. Previoustext-to-videobenchmarksalsoneglectthisimportant
ability for evaluation. In this work, we conduct the first systematic study on
compositionaltext-to-videogeneration. WeproposeT2V-CompBench,thefirst
benchmarktailoredforcompositionaltext-to-videogeneration. T2V-CompBench
encompasses diverse aspects of compositionality, including consistent attribute
binding,dynamicattributebinding,spatialrelationships,motionbinding,action
binding,objectinteractions,andgenerativenumeracy. Wefurthercarefullydesign
evaluationmetricsofMLLM-basedmetrics,detection-basedmetrics,andtracking-
basedmetrics,whichcanbetterreflectthecompositionaltext-to-videogeneration
quality of seven proposed categories with 700 text prompts. The effectiveness
of the proposed metrics is verified by correlation with human evaluations. We
also benchmark various text-to-video generative models and conduct in-depth
analysisacrossdifferentmodelsanddifferentcompositionalcategories. Wefind
thatcompositionaltext-to-videogenerationishighlychallengingforcurrentmodels,
andwehopethatourattemptwillshedlightonfutureresearchinthisdirection.
Preprint.Underreview.
4202
luJ
91
]VC.sc[
1v50541.7042:viXra1 Introduction
Text-to-video(T2V)generationhasmadesignificantprogressinrecentyears[1,2,3,4,5,6,7,8,9,
10,11,12,13]. However,generatingvideosthataccuratelydepictmultipleobjects,attributes,and
motionsincomplexanddynamicscenesbasedonfine-grainedtextdescriptionsremainsachallenging
task. Inthiswork,weaimtoconductasystematicstudyoncompositionaltext-to-videogeneration.
Compositionaltext-to-image(T2I)generation,whichaimstocomposemultipleobjects,attributes,
andtheirrelationshipsintocomplexscenes,hasbeenwidelystudiedinpreviousmethods[14,15,
16]. Benchmarksforcompositionaltext-to-imagegeneration[17]havebeenacceptedasanimportant
evaluationdimensionfortext-to-imagefoundationmodels[18,19,20]. However,mostworksontext-
to-videogenerationfocusongeneratingvideoswithsimpletextprompts,neglectingthesignificance
ofcompositionaltext-to-videogeneration. Moreover,existingvideogenerationbenchmarks[21,22,
23]primarilyevaluatevideoquality,motionquality,andtext-videoalignmentwithsingle-objecttext
prompts,andbenchmarksforcompositionaltext-to-videogenerationhavenotbeensystematically
andextensivelyinvestigatedinpreviousliterature.
Tothisend,weproposeT2V-CompBench,acomprehensivebenchmarkdesignedforcompositional
text-to-videogeneration. Thisbenchmarkemphasizescompositionalitythroughmultipleobjectswith
attributes,quantities,actions,interactions,andspatial-temporaldynamics. Wedesignapromptsuite
composedof7categories,whereeachcategoryconsistsof100textpromptsforvideogeneration.
Whenconstructingtheprompts,weemphasizetemporaldynamicsandguaranteethateachprompt
containsatleastoneactionverb. Thesevencategoriesareasfollowsandexamplesareillustrated
in Figure 2: 1) Consistent attribute binding. This category includes prompts featuring at least
twodynamicobjects,eachwithadistinctattribute. Theattributesassociatedwitheachobjectare
consistentthroughoutthevideo. 2)Dynamicattributebinding. Thiscategoryincludesprompts
featuringattributebindingproblemswheretheattributeschangewithtime. 3)Spatialrelationships.
Inthiscategory,eachpromptmentionsatleasttwodynamicobjectswithspecifiedspatialrelationships.
4)Actionbinding. Promptsinthiscategorycontainatleasttwoobjectseachwithadistinctaction.
5)Motionbinding. Thepromptsinthiscategorycontainobjectswithspecifiedmovingdirections.
6)Objectinteractions. Thiscategoryteststhemodels’abilitiestounderstandandgeneratedynamic
interactions,includingphysicalinteractionsandsocialinteractions. 7)Generativenumeracy. The
textpromptsinthiscategoryincludeatleasttwoobjectswithquantitiesrangingfromonetoeight.
Anotherchallengeliesintheevaluationofcompositionaltext-to-videomodels. Commonlyused
metrics,suchasInceptionScore[24],FréchetInceptionDistance(FID)[25],FréchetVideoDistance
(FVD)[26],andCLIPScore[27],cannotfullyreflectthecompositionalityoftext-to-videogeneration
models. Evaluatingcompositionalityintext-to-videomodelsrequiresafine-grainedunderstandingof
notonlyobjectsandattributesineachframebutalsothedynamicsandmotionsbetweenframes. Itis
ordersofmagnitudemorecomplexthanevaluatingtext-to-imagemodels.
To address this challenge, we incorporate temporal dynamics across frames into evaluation and
designdifferentmetricstoevaluatedifferentcategoriesinthebenchmark. Specifically,wedesign
Multimodal Large Language Model (MLLM)-based metrics, including image-LLM and video-
LLM,toevaluatedynamicattributebinding,consistentattributebinding,actionbinding,andobject
interactions. We devise detection-based metrics to evaluate spatial relationships and generative
numeracy. Weproposetracking-basedmetricstoevaluatemotionbinding. Theeffectivenessofour
proposedmetricsisvalidatedbycomputingthecorrelationwithhumanevaluations. Weevaluate
previoustext-to-videogenerationmodelsonT2V-CompBenchandconductsystematicstudiesand
analysisoftheperformanceofcurrentmodelsondifferentcompositionalcategories.
Thecontributionsofourpaperarethree-fold. 1)Toourbestknowledge,wearethefirsttopropose
abenchmarkforcompositionaltext-to-videogeneration,featuringsevencategorieswith700text
prompts. 2)Weproposecomprehensiveevaluationmetricsforthesevencategoriesandverifytheir
effectivenessthroughhumancorrelation. 3)Webenchmarkvariousopen-sourceT2Vmodelsand
provideasystematicstudywithinsightfulanalysis,whichwillinspirefutureresearchinthisdirection.
2 Relatedwork
Text-to-videogeneration. Existingtext-to-videogenerationmodelscanberoughlycategorizedinto
twotypes,namelythelanguagemodel-based[8,28,29,30,31,32]andthediffusion-modelbased[3,
24,9,10,11,1,12,13]. Inthispaper,weevaluate20models,including13officiallyopen-sourced
models: ModelScope [13], ZeroScope [33], Latte [34], Show-1 [35], VideoCrafter2 [36], Open-
Sora1.1and1.2[37],Open-Sora-Planv1.0.0andv1.1.0[38],AnimateDiff[39],VideoTetris[40],
LVD[41],andMagicTime[42],aswellas7commercialmodels: Pika[43],Gen-2[44],Gen-3[45],
Dreamina[46],PixVerse[47],DreamMachine[48]andKling[49].
Compositionaltext-to-imagegeneration. Recentstudieshavedelvedintocompositionalityintext-
to-imagegeneration[14,15,50,51,17,52,53,16,54,41,55,51,56,57,58,59,60,61,62,63,64,
65]. T2I-CompBench[17]proposedthefirstcomprehensivebenchmarktoevaluatecompositionality
intext-to-imagemodels,focusingonattributesbinding,relationships,andcomplexcompositions.
Whiletheseevaluationsaretailoredexclusivelytotheimagedomain,videogenerationrequiresa
deeperconsiderationofspatio-temporaldynamics. ConcurrentworkVideoTetris[40]proposesa
frameworkofspatio-temporalcompositionaldiffusionthatenablescompositionalT2Vgeneration.
Ourworkpioneersthedevelopmentofbenchmarkingcompositionaltext-to-videogeneration.
Benchmarksfortext-to-videogeneration. ExistingworksevaluatetheFVDscoresoftext-to-video
modelsusingdatasetslikeUCF-101[66]andKinetics-400/600/700[67,68,69],whicharelimitedto
specifictopicsofhumanaction. Recentworksdesigntextpromptsandevaluationmetricstoevaluate
thevideoqualityandtext-videoalignmentinopendomains. Make-a-Video-Eval[4]contains300
promptsbutonlyconsidersspatialappearances. FETV[23]categorizespromptsbasedonmajor
content,controllableattributes,andpromptcomplexity. VBench[21]andEvalCrafter[22]propose
comprehensivebenchmarkstoevaluateT2Vmodelsfromvariousperspectives,butmostprompts
focusonsingle-objectratherthancompositionofmultipleobjects. AlthoughVBenchinvolvesspatial
relationshipsasasubcategory,theirprompts,suchas“aclockontheleftofavase”,donotreflectthe
dynamicsinvideos. Thereisalackofacomprehensivedefinitionofcompositionalityintext-to-video
generation. Weproposethefirstbenchmarkforcompositionaltext-to-videogeneration.
Evaluationmetricsfortext-to-videogeneration. Previousmethodsevaluatevideogeneratorsfrom
theperspectivesofvideoqualityandvideo-textalignment. Forvideoquality,thecommonlyused
metrics,suchasInceptionScore(IS)[24]andFréchetVideoDistance(FVD)[26]areadoptedto
evaluate the diversity and fidelity in video quality. For text-video alignment, CLIPScore [27] is
proposedtomeasurethesimilarityofthetextpromptandtheimage, usingthepre-trainedCLIP
model [70]. However, these metrics are not suitable for complex prompts in compositionality.
Weproposeevaluationmetricstailoredforourbenchmarkandvalidatetheireffectivenessthrough
extensivehumancorrelationstudies.
3 BenchmarkConstruction
3.1 PromptGeneration
Wedefinesevenpromptcategoriesforcompositionaltext-to-videogeneration.Eachcategoryincludes
100prompts, whicharegeneratedbypromptingGPT-4[71]withalistofobjectclasses, prompt
structures,andotherinformationforspecificpromptcategories. Dependingonthepromptcategory,
otherinformationprovidedmayincludeattributes,spatialrelationships,motiondirections,actions,
interactions,andquantities. Althoughnotallpromptcategoriesaredesignedtoevaluateactionsand
motions,weensureallpromptsinourbenchmarkcontainatleastoneactionverb,topreventtheT2V
modelfromgeneratingstaticvideos. GPT-4returnsbothpromptsandtheparsedmetainformation
ofpromptswhichfacilitatesevaluation. Theprompttemplatesforgeneratingthetextpromptsand
themetainformationaredetailedinAppendixA.Allgeneratedpromptsareverifiedbyhumansand
improperpromptsarefilteredout. ThewholeprocessisshowninFigure2.
3.2 PromptCategories
Consistentattributebinding. Consistentattributebindingsuchasrequiresattributestobeconsis-
tentlyassociatedwithcorrespondingobjectsthroughoutthegeneratedvideoframes. Toconstruct
the prompts, we define four attribute types including color, shape, texture, and human attributes.
GPT-4ispromptedtogeneratesentenceswithatleasttwoobjects,twoattributeswhereeachattribute
associateswithanobject,andoneactionverb. About30%ofthepromptscontaincolorattributes,
20%containshapeattributes,20%containtextureattributes,and30%containtwodifferenttypesof
3Category Information Take“Action Binding”
as an example
obG jee cn te sr .a Ete a p cr ho wm ip thts a f e na at tu ur ri an lg a t cw tio o n GPT-4 “A peP nr go um inp wts addles { " "o ab ctj ieM oc ne t t 11a "" : :d " "a aat a p pe enng guui inn ", Description Prom eaC p ctso h n w as i tt ti h rs i bt ae ut tn l eet a aA s stt s t t owr cib o ia u o tet be sje wB ct ii s tn h, d t awi nn o g o a bt jt eri cb tu .tes,
verb individually. Eligible object on the ice, a camel waddles on the ice",
vc ea ht ie cg leo , r aie rs ti fi an cc tl ,u fd oe o: d a , n ai nm da bl, u p ill da in nt g, . treks by” t r e " " ko a sb c btj ie yoc "nt 2 2" ": : " "aa c ca am me ell" , E Prx oa mm pp tl se A blue fc ea nr c d er i ov ne s a p sa us nt na y w dh ai yte . picket
}
Human verification Evaluation
obj. number >=2 Generated Video
action number >=2 By T2V Models
common/uncommon
objects coexistence Video LLM-based metric: Grid-LLaVA
Dynamic Attribute Binding Spatial Relationships Action Binding
Description P aro sm sop ct is a tw ei sth w o itn he a o dr ytw nao m o ib cj e ac ttt rs ib, ue ta ec .h Description Prompts con st pa ain tii an lg r ea lt a l te ioa ns st htw ipo s .objects with Description Prompts with mul ati cp tl ie o no sb .jects and multiple
Example A timelapse of a leaf transitioning from Example A toddler walking on the left of a dog in Example A penguin waddles on the ice,
Prompts green to bright red as autumn progresses. Prompts a park. Prompts a camel treks by.
Generated Video Generated Video Generated Video
By T2V Models By T2V Models By T2V Models
Image LLM-based metric: D-LLaVA Detection-based metric: G-Dino Video LLM-based metric: Grid-LLaVA
Motion Binding Object Interactions Generative Numeracy
Description Prompts s c po en ct ia fii cn i mng o vo in nge do ir r etw cto io o nb sj .ects with Description Prompts with dynamic object interactions. Description Prom qp uts a nw ti it th ie o s n re a no gr i ntw go f ro ob mje oc nt ec a tote eg io gr hie t.s with
E Px roa mm pp tl se A dee ar n r u on ws l l fe lif etw s a rir gd h t th wr ao ru dg ah b t oh ve e f .orest, E Px roa mm pp tl se Man teaches robot to play chess. E Px roa mm pp tl se Four sea lions aa n sd e ath sr ide ee . beach balls on
Image LLM-based metric: D-LLaVA
Generated Video Generated Video Generated Video
By T2V Models By T2V Models By T2V Models
Tracking-based metric: DOT Video LLM-based metric: Grid-LLaVA Detection-based metric: G-Dino
Figure2: Promptgenerationprocessandillustrationsofthesevencompositionalcategories.
attributestodescribethetwoobjects. Amongalltheprompts,80%arecommonlyseeninthereal
world,while20%areuncommononesthatrequireimagination.
Dynamicattributebinding. Thiscategoryfeaturesattributebindingproblemswheretheattributes
changeovertime. Forexample,“Greenavocadodarkenstoblackasthetomatobesideitripenstoa
deepred”.WepromptGPT-4togeneratedynamicattributessuchascolortransitionorstatetransition
forobjects. 80%ofthepromptsdescribeattributechangesthatarecommonlyseenintherealworld,
while20%ofthepromptsareuncommonandartificial.
Spatialrelationships. Thiscategoryrequiresthemodeltogenerateatleasttwoobjectswithcorrect
spatialrelationshipsacrossthedynamicvideo. Wedefinesixtypesofspatialrelationshipsbetween
twoobjectsinthe3-dimensionalspace: “ontheleftof”,“ontherightof”,“above”,“below”,“in
front of”, and “behind”. Around 35% prompts include left/right, 35% include above/below, and
theremaining30%includeinfrontof/behind. Forspatialrelationshipsincludingleftorright,we
constructsomecontrastivepromptsbyinvertingtherelationship.
Actionbinding. Thiscategoryteststhemodels’abilitiestobindactionstocorrespondingobjects
when there are multiple objects and multiple actions described. We prompt GPT-4 to generate
text prompts with two objects, each associated with an action verb. This category includes 80%
commonpromptsand20%uncommonprompts. Uncommonpromptscanbefurthercategorizedinto
uncommoncoexistenceofobjects,anduncommonaction-objectpairs.
Motionbinding. Thepromptsinthiscategorycontainoneortwoobjectswithspecifiedmoving
directions,aimingtotestthemodels’abilitiestobindspecificmotiondirectionswithobjects. We
define four types of moving directions: “leftwards”, “rightwards”, “upwards” and “downwards”.
Eachobjectinthepromptmovesinoneofthedirections. 60%ofthemotiondirectionsincludedin
thepromptsarehorizontal,and40%arevertical.
Objectinteractions. Thiscategoryteststhemodels’abilitiestounderstandandgeneratedynamic
interactions, including physical interactions causing motion change or state change and social
interactions. Promptsforphysicalinteractionsaccountfor50%prompts,including30%withstate
changesand20%withmotionchanges. Theother50%aresocialinteractions,with30%common
onesand20%uncommononesdepictinganthropopathicanimalsperformingsocialinteractions.
4Avg Adv.
Avg Adj.
Avg Verb
Avg Noun
Avg Length
0 2 4 6 8 10
Word Count
Figure 3: T2V-CompBench statistics. Left: Word cloud of the prompt suite. Right: Average
occurrencesofadverbs,adjectives,verbs,nouns,andpromptlengthperprompt.
Generativenumeracy. Thetextpromptsinthiscategoryincludeoneortwoobjectcategorieswith
quantitiesrangingfromonetoeight. About60%ofthepromptsdescribesingleobjectcategory,and
theother40%describetwoobjectcategories.
3.3 PromptSuiteStatistics
Thepromptsuitestatisticswithwordcloudvisualizationandpromptstatistics,aredepictedinFigure3.
T2V-CompBenchstandsoutforitsfocusonmultipleobjectsandtemporaldynamics: (1)Incontrast
topreviousbenchmarks,whichpredominantlyfocusonsingle-objectprompts,T2V-CompBench
featurespromptsinvolvingmorethantwoobjects(nouns)onaverage,witheachpromptcontaining
approximately3.2nouns. (2)T2V-CompBenchconsiderstemporaldynamics,withnearlyallprompts
containingactionverbs,averagingat1.1perprompt.
4 EvaluationMetrics
Weobservethattheevaluationmetricsforcompositionaltext-to-imagegeneration[17]cannotbe
directlyadoptedforevaluatingcompositionaltext-to-videogeneration,duetothelargenumberof
framesandcomplexspatial-temporaldynamicsinvideos. Mostvideogenerationmodelsgenerate
videos within 2-5 seconds. For a fair comparison, we evenly extract 6 frames for MLLM-based
evaluation,and16framesfordetection-basedevaluation,andsamplethevideostoaframerateof8
framespersecond(FPS)fortracking-basedevaluation.
4.1 MLLM-basedevaluationmetrics
Multimodal Large Language Models (MLLMs) have shown great capabilities in understanding
complexcontentsinimagesandvideos[72, 73, 71, 74]. Inspiredbytheireffectivenessinvideo
understanding,weexploitMLLMsasevaluatorsforcompositionaltext-to-videogeneration.
VideoLLM-basedmetricsforconsistentattributebinding,actionbinding,andobjectinterac-
tions. Tohandlethecomplexspatial-temporalinformationinvideos,weinvestigatevideoLLMs
such as Image Grid[75] and PLLaVA [74], which extends LLaVA [73] from single image input
to multi-frame input. We empirically find that Image Grid performs better than PLLaVA in our
compositionalcategories. Specifically,ImageGrid[75]uniformlysamples6framesfromthevideo
toformanimagegridastheinputtoLLaVA.Additionally,weboosttheabilityofvideoLLMsand
avoidhallucinationsbythechain-of-thoughtmechanism[76],wherewefirstaskMLLMtodescribe
thevideocontent,andthenaskittoscorethetext-videoalignment.(1)Toevaluateconsistentattribute
binding,weuseGPT-4toparsethepromptsintodifferentdisentangledphrases(e.g.,“Abluecar
drives past a white picket fence on a sunny day” is parsed into “a blue car” and “a white picket
fence”),andthenaskthevideoLLMtoprovideamatchingscorebetweentheeachdisentangled
promptandtheImageGrid. Thescoresforeachdisentangledphraseareaveragedtogetthefinal
score. (2)Foractionbinding,weuseGPT-4toextractobjectsandtheiractions. Forexample,given
theprompt“Adogrunsthroughafieldwhileacatclimbsatree”,weextractthephrases“adog”,“a
dogrunsthroughafield”,“acat”,and“acatclimbsatree”. WethenaskthevideoLLMtocheckthe
alignmentbetweeneachobject-actioncombinationandthevideotogetthefinalscore(3)Forobject
interactions,wepromptthevideoLLMtoscorethevideo-textalignment.
5Image LLM-based metrics for dynamic attribute binding. Evaluating the dynamic attribute
binding category such as “Bright green leaf wilts to brown” is challenging as it requires a deep
understanding of dynamic changes between frames. We find that current video LLMs achieve
unsatisfactoryperformanceinthiscategory,sowedesignaframe-by-frameevaluationmetricbased
onImageLLMsuchasLLaVA[73]. WeutilizeGPT-4toparsetheinitialstate(“brightgreenleaf”)
andfinalstate(“brownleaf”),andpromptLLaVA[73]toscorethealignmentbetweeneachframe
andeachofthetwostates. Basedonscoresbetween16framesand2states,wedefineascoring
functionwhereweencouragethefirstframetoalignwiththeinitialstate,thelastframetoalignwith
thefinalstate,andthemiddleframestobeinbetween. WedenotethismetricasD-LLaVA.
4.2 Detection-basedEvaluationMetricsforSpatialRelationshipsandNumeracy
Most previous vision-language models face difficulties with spatial relationships and numeracy-
relatedunderstandings. SoweintroducetheobjectdetectionmodelGroundingDINO(G-Dino)[77]
todetectobjectsforeachframe,filteroutduplicateboundingboxeswithhighintersection-over-union
(IoU),andthendefinerule-basedmetricsbasedontheobjectdetectionresults.
2Dspatialrelationships.For2Dspatialrelationshipsincluding“left”,“right”,“above”,and“below”,
wedefinerule-basedmetricsforeachframesimilartoT2I-CompBench[17]. Specifically,foreach
pairofobjects,wedenotetheircentersas(x ,y )and(x ,y ),respectively. Thefirstobjectison
1 1 2 2
theleftofthesecondobjectifx <x ,and|x −x |>|y −y |. Theruleissimilartootherspatial
1 2 1 2 1 2
relationships. Ifthereismorethanonepairofobjectsinaframe,weselectthemostprobableone
basedontheirIoUandconfidencescores. Theper-framescoreis(1−IoU)ifthereisobjectpairthat
satisfiestherelationship,or0ifnoobjectpairsatisfiestherelationship. Thevideo-levelscoreisthe
averageofper-framescoresoverallframes.
3Dspatialrelationships. 3Dspatialrelationships(“infrontof”,“behind”)cannotbeidentifiedby
2Dboundingboxlocations. Withthe2DobjectboundingboxesdetectedbyGroundingDINO[77],
wefurtherleveragetheSegmentAnything[78]topredictmasksofspecifiedobjects,andthenleverage
DepthAnything[79]topredictthedepthmaps. Thedepthofanobjectisdefinedastheaveragedepth
valuesofthepixelsinsidetheobjectmask. Wedefinetheframe-levelscoresbasedontheIoUand
relativedepthbetweentwoobjects,andthevideo-levelscoreistheaverageofframe-levelscores.
Generativenumeracy. Toevaluategenerativenumeracy,wecountthenumberofdetectedobjects
foreachobjectclass. Ifthedetectedquantitymatchesthenumberintextprompt,weassignascoreof
1forthespecifiedobjectclass. Otherwise,weassignascoreof0. Theframe-levelscoreiscalculated
astheaveragescoreofallobjectclasses,andthevideo-levelscoreistheaverageofallframes.
4.3 Tracking-basedEvaluationMetricsforMotionBinding
Theevaluationmetricformotionbindingshouldidentifythemovingdirectionofobjectsinthevideo.
Inmanyvideos,theobjectmotionsareentangledwithcameramotions,makingitchallengingto
determinetheactualmotiondirectionofobjects. Invideos,theactualmovingdirectionofanobject
istherelativemovingdirectionbetweentheforegroundobjectandthebackground. Therefore,we
introduceatracking-basedapproachtodeterminethemovingdirectionofbackgroundandforeground,
respectively. Inparticular,weuseGroundingSAM[80]toderivetheforegroundobjectmasksand
backgroundmask,andadoptDOT[81]totracktheforegroundobjectpointsandbackgroundpoints
inthevideo. Wecomputetheaveragemotionvectorsofforegroundobjectpointsandbackground
points,respectively,andthedifferencebetweentheobjectmotionvectorandthebackgroundmotion
vectoristheactualmovingdirectionoftheobject. Thefinalscorereflectswhethertheactualmoving
directionalignswiththemotiondescribedintextprompts.
5 Experiments
5.1 ExperimentalSetup
Evaluatedmodels. Weevaluatetheperformanceof13open-sourcetext-to-videomodelsand7com-
mercialmodelsonT2V-CompBench.Open-sourcemodelsincludeModelScope[13],ZeroScope[33],
Latte[34],Show-1[35],VideoCrafter2[36],Open-Sora1.1and1.2[37],Open-Sora-Planv1.0.0and
v1.1.0[38],AnimateDiff[39],VideoTetris[40],MagicTime[42]andLVD[41]. Commercialmodels
6Table1: Thecorrelationbetweenautomaticevaluationmetricsandhumanevaluation. Our
proposedmetricsshowenhancedperformanceinKendall’sτ andSpearman’sρ.
Consist-attr Dynamic-attr Spatial Motion Action Interaction Numeracy
Metric
τ(↑) ρ(↑) τ(↑) ρ(↑) τ(↑) ρ(↑) τ(↑) ρ(↑) τ(↑) ρ(↑) τ(↑) ρ(↑) τ(↑) ρ(↑)
CLIP 0.3667 0.4859 −0.0096 −0.01401 0.2395 0.3343 0.1381 0.1818 0.2796 0.3799 0.1085 0.1426 0.0560 0.0821
B-CLIP 0.2609 0.3562 0.2100 0.2917 0.1247 0.1647 −0.0582 −0.0889 0.0915 0.1246 0.1220 0.1720 0.0694 0.0829
B-VQA 0.5194 0.6964 − − − − − − − − − − − −
LLaVA 0.6626 0.8023 0.0830 0.1110 0.5297 0.6853 0.2535 0.3443 0.4899 0.6451 0.1866 0.2537 0.3212 0.4540
D-LLaVA − − 0.3147 0.4106 − − − − − − − − − −
Grid-LLaVA 0.7592 0.8679 0.1435 0.1678 0.4815 0.5763 0.1349 0.1619 0.5469 0.6795 0.3298 0.3962 0.2266 0.2809
PLLaVA 0.2715 0.3105 0.1845 0.2201 −0.1252 −0.1509 0.0401 0.0498 0.4326 0.5066 −0.0713 −0.0835 0.2253 0.3142
G-Dino − − − − 0.5769 0.7057 − − − − − − 0.4063 0.5378
DOT − − − − − − 0.4523 0.5366 − − − − − −
arePika[43],RunwayGen-2[44],RunwayGen-3[45],Dreamina[46],PixVerse[47],LumaDream
Machine[48],andKuaishouKling[49]. Amongthosemodels,LVD[41]isspecificallydesignedto
leverageLLM-guidedlayoutplanningforvideoswithmultipleobjects,andothersaretext-to-video
foundationmodels. VideoTetris[40]isproposedforcompositionalmultipleobjectsanddynamic
changesinobjectnumbers. Additionally,fordynamicattributebinding,weevaluateMagicTime[42],
whichistrainedfromAnimateDiff[39]anddesignedformetamorphictime-lapsevideogeneration.
Implementationdetails. WefollowtheofficialimplementationsoftheT2Vmodels,pleasereferto
AppendixBformoredetails.
5.2 EvaluationMetrics
Conventionalmetrics. Wecompareourproposedmetricswiththreemetricswidelyusedinprevious
studies: 1)CLIPScore[27](denotedasCLIP)calculatesthecosinesimilaritybetweenCLIPtext
and image embeddings. 2) BLIP-CLIP [16] (denoted as B-CLIP) applies BLIP [82] to generate
captionsforthegeneratedimages,andthencalculatestheCLIPtext-textcosinesimilaritybetween
twoprompts. 3)BLIP-VQA[17](denotedasB-VQA)appliesBLIP[82]toaskaquestiontoevaluate
thetext-imagealignment. Thevideo-levelscoresarecalculatedbyaveragingacrossallframes.
Our proposed metrics. As introduced in Sec. 4, the image-LLM-based metric, D-LLaVA, is
designedforevaluatingdynamicattributebinding,thedetection-basedmetric(G-Dino)isdesigned
forspatialrelationshipsandgenerativenumeracy,andthetracking-basedmetric(DOT)isdesigned
formotionbinding. Formoredetailsofourproposedevaluationmetrics,pleaserefertoAppendixC.
Additionally, we test video-LLM-based metrics (ImageGrid-LLaVA, PLLaVA) and image-LLM-
basedmetrics(LLaVA)forallcategories. Inthenextsubsection,wedeterminethebestmetricfor
eachcategorybycorrelationbetweenmetricsandhumanscores.
5.3 HumanEvaluationCorrelationAnalysis
Inthissection,weconducthumanevaluationsandcomputethecorrelationbetweenevaluationmetrics
andhumanscorestodeterminethebestmetricforeachcategory.
Humanevaluation. Forthehumanevaluationofeachcategory,werandomlyselect15prompts
out of 100 prompts and use different video generation models to generate a total of 90 videos.
Additionally,weinclude10groundtruthvideosforthedynamicattributebindingcategoryand16for
theobjectinteractionscategory. Thetotalnumberofvideosforhumanevaluationis656. Weemploy
AmazonMechanicalTurkandaskthreeannotatorstoscorethetext-videoalignmentscoreofeach
video. Weaverageacrossthreescoresforeachtext-videopairandcalculatethecorrelationbetween
humanscoresandautomaticevaluationscoreswithKendall’sτ andSpearman’sρ. Pleasereferto
AppendixDformorehumanevaluationdetails.
Comparisonsacrossevaluationmetrics. ThehumancorrelationresultsareillustratedinTable1.
Theresultsvalidatetheeffectivenessofourproposedevaluationmetrics,highlightedinbold,with
ImageGrid-LLaVAforconsistentattributebinding,actionbindingandobjectinteractions,LLaVAfor
dynamicattributebinding,G-Dinoforspatialrelationshipsandgenerativenumeracy,andDOTfor
motionbinding.
CLIPandB-CLIPshowrelativelylowhumancorrelations,indicatingtheycannotcapturethefine-
grainedattributesanddynamicsincomplexvideosandtextprompts. LLaVA,ImageGrid-LLaVA,
7Table2: T2V-CompBenchevaluationresultswithproposedmetrics. Ahigherscoreindicates
betterperformanceforacategory. Boldstandsforthebestscore, red indicatesthebestscoreacross
7commercialmodels, yellow indicatesthebestscoreacross13open-sourcedmodels.
Model Consist-attr Dynamic-attr Spatial Motion Action Interaction Numeracy
Metric Grid-LLaVA D-LLaVA G-Dino DOT Grid-LLaVA Grid-LLaVA G-Dino
ModelScope[13] 0.5483 0.1654 0.4220 0.2552 0.4880 0.7075 0.2066
ZeroScope[33] 0.4495 0.1086 0.4073 0.2319 0.4620 0.5550 0.2378
Latte[34] 0.5325 0.1598 0.4476 0.2187 0.5200 0.6625 0.2187
Show-1[35] 0.6388 0.1828 0.4649 0.2316 0.4940 0.7700 0.1644
VideoCrafter2[36] 0.6750 0.1850 0.4891 0.2233 0.5800 0.7600 0.2041
Open-Sora1.1[37] 0.6370 0.1762 0.5671 0.2317 0.5480 0.7625 0.2363
Open-Sora1.2[37] 0.6600 0.1714 0.5406 0.2388 0.5717 0.7400 0.2556
Open-Sora-Planv1.0.0[38] 0.5088 0.1562 0.4481 0.2147 0.5120 0.6275 0.1650
Open-Sora-Planv1.1.0[38] 0.7413 0.1770 0.5587 0.2187 0.6780 0.7275 0.2928
AnimateDiff[39] 0.4883 0.1764 0.3883 0.2236 0.4140 0.6550 0.0884
VideoTetris[40] 0.7125 0.2066 0.5148 0.2204 0.5280 0.7600 0.2609
LVD[41] 0.5595 0.1499 0.5469 0.2699 0.4960 0.6100 0.0991
MagicTime[42] - 0.1834 - - - - -
Pika[43] 0.6513 0.1744 0.5043 0.2221 0.5380 0.6625 0.2613
Gen-2[44] 0.7050 0.2100 0.5250 0.2169 0.5280 0.7300 0.3081
Gen-3[45] 0.7045 0.2078 0.5533 0.3111 0.6280 0.7900 0.2169
Dreamina[46] 0.8220 0.2114 0.6083 0.2391 0.6660 0.8175 0.4006
PixVerse[47] 0.7370 0.1738 0.5874 0.2178 0.6960 0.8275 0.3281
DreamMachine[48] 0.6900 0.2002 0.5387 0.2713 0.6400 0.7725 0.2109
Kling[49] 0.8045 0.2256 0.6150 0.2448 0.6460 0.8475 0.3044
andPLLaVAperformwellinactionbinding. However,thesemetricsfallshortwhenitcomesto
capturingdynamicchangesinvolvingattributesorstates(asindynamicattributebinding),aswellas
intasksrequiringunderstandingsofspatialrelationships,motiondirection,andnumeracy. Instead,
our proposed D-LLaVA significantly enhances the ability to capture dynamic attribute changes
invideos,resultinginahighercorrelationwithhumanevaluationsfordynamicattributebinding.
Additionally,detection-basedandtracking-basedmetricsshowimprovementsincapturingspatial
andcountingaspectsofcompositionality. ImageGrid-LLaVAsurpassesLLaVAintermsofconsistent
attributebindingandobjectinteractions,asitaccountsfortemporalsequencesratherthanjustastatic
frame.
5.4 QuantitativeEvaluation
Themodels’performancesonT2V-CompBenchareshowninTable2. Comparingagainstdifferent
models,weobservethefollowing: (1)Thecommercialmodelsoutperformtheopen-sourcemodelsin
sevencompositionalcategories.(2)Show-1[35],VideoCrafter2[36],VideoTetris[40],Open-Sora1.1
and1.2[37]andOpen-Sora-Planv1.1.0[38]exhibitbetteroverallperformances. (3)Open-Sora-Plan
v1.1.0showssignificantimprovementoverthepreviousversion,Open-Sora-Planv1.0.0,intermsof
7compositionalcategories,andleadsinconsistentattributebinding,actionbinding,andgenerative
numeracy.(4)Show-1[35]excelsinobjectinteractions,whileVideoTetris[40]issuperiorindynamic
attributebinding. (5)LVD[41]performswellinspatialrelationshipsandmotionbindingduetoits
design,whichleveragesLLM-guidedlayoutplanning.
5.5 QualitativeEvaluation
ThechallengingcasesforsevencompositionalcategoriesareillustratedinFig.4andFig.5,with
thelevelofchallengedecreasingfromtoptobottom. Fig.4showstheperformanceofopen-source
models,whileFig.5presentsthatofcommercialmodels.
Weobservethefollowing:(1)Themostchallengingonesaredynamicattributebindingandgenerative
numeracy(rows1-2inFig.4andFig.5),whichrequireafine-grainedunderstandingoftemporal
dynamicsoraccuratecounting. Inthesecategories,modelsoftenoverlooksomepartsofprompts.
Fordynamicattributebinding,theT2Vmodelsoftenfocusonkeywordsintheprompts,neglecting
thedynamicchangesinattributesorstates. Theytendtogenerateconsistentresultswithoutreflecting
therequiredtransitions. Fornumeracy,theT2Vmodelshandlenumeracywellwhenthenumbersare
smallerthanthree. However,theystrugglewhenthenumbersexceedthree.
8Show-1 VideoCrafter2 Open-Sora
A timelapse of a
leaf transitioning
from green to
bright red as
autumn progresses.
Six horses
graze in a field
under a clear
blue sky.
A gorilla sitting on
the left side of a
vending machine in
a forest.
A paper airplane
gliding to the Left
across a classroom.
A dog runs
through a field
while a cat
climbs a tree.
Two cars collide
at an intersection.
A blue car drives
past a white picket
fence on a sunny
day.
Figure 4: Qualitative comparisons of seven compositional categories among Show-1 [35],
VideoCrafter2[36],andOpen-Sora1.1[37].
Gen-3 Dreamina PixVerse
A timelapse of a
leaf transitioning
from green to
bright red as
autumn progresses.
Mx594riR0YZ
Six horses
graze in a field
under a clear
blue sky.
A gorilla sitting on
the left side of a
vending machine in
a forest.
A paper airplane
gliding to the Left
across a classroom.
A dog runs
through a field
while a cat
climbs a tree.
Two cars collide
at an intersection.
A blue car drives
past a white picket
fence on a sunny
day.
Figure5: QualitativecomparisonsofsevencompositionalcategoriesamongGen-3[45], Dream-
ina[46],andPixVerse[47].
(2) The second challenging categories include spatial relationships, motion binding, and action
binding(rows3-5inFig.4andFig.5). Forspatialrelationships,themodelsoftenconfuselocality
9termssuchas“left”and“right”. Theproblemworsenswithmovingdirectionsinmotionbinding,
suchas“sailtotheleft”or“flyingrighttowards”. Foractionbinding,themodelsfailtocorrectly
associatecertainactionswithobjects. Forexample,giventheprompts“Adogrunsthroughafield
whileacatclimbsatree”,themodelstendtogeneratebothanimalsrunninginsteadofperforming
theirrespectiveactions,ortheymayneglectoneobjectentirely.
(3)Thelastchallengingcategoriesareobjectinteractionsandconsistentattributebinding(rows6-7
inFig.4andFig.5). Inobjectinteractions,themodelsoftenproducenearlystaticvideos,neglecting
theentireinteractionprocess. Forconsistentattributebinding,themodelssometimesconfusethe
attributeswithspecificobjectsoroverlookcertainobject.
6 ConclusionandDiscussions
Weconductthefirstsystematicstudyoncompositionalityintext-to-videogeneration. Wepropose
T2V-CompBench, a comprehensive benchmark for compositional text-to-video generation, with
700 prompts belonging to 7 categories. We further design a suite of evaluation metrics for the 7
categories. Finally,webenchmark9text-to-videogenerationmodelsandprovideinsightfulanalysis
ofthecompositionalityofcurrenttext-to-videomodels. Compositionaltext-to-videogenerationis
highlychallengingforcurrentmodels,andwehopeourworkwillinspirefutureworkstoimprovethe
compositionalityoftext-to-videomodels. Alimitationofourworkisthelackofaunifiedevaluation
metricforallcategories,andwebelievethatthislimitationpointsoutnewchallengesformultimodal
LLMsorvideounderstandingmodels. Thecommunityshouldbeawareofthepotentialnegative
socialimpactofvideogenerationmodelsbeingusedtogeneratefakevideosthatmisleadpeople.
References
[1] Andreas Blattmann et al. “Align your latents: High-resolution video synthesis with latent
diffusionmodels”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition.2023,pp.22563–22575.
[2] AndreasBlattmannetal.“Stablevideodiffusion:Scalinglatentvideodiffusionmodelsto
largedatasets”.In:arXivpreprintarXiv:2311.15127(2023).
[3] JonathanHoetal.“Imagenvideo:Highdefinitionvideogenerationwithdiffusionmodels”.In:
arXivpreprintarXiv:2210.02303(2022).
[4] UrielSingeretal.“Make-a-video:Text-to-videogenerationwithouttext-videodata”.In:arXiv
preprintarXiv:2209.14792(2022).
[5] ChenfeiWuetal.“Godiva:Generatingopen-domainvideosfromnaturaldescriptions”.In:
arXivpreprintarXiv:2104.14806(2021).
[6] ChenfeiWuetal.“Nüwa:Visualsynthesispre-trainingforneuralvisualworldcreation”.In:
Europeanconferenceoncomputervision.Springer.2022,pp.720–736.
[7] WenyiHongetal.“Cogvideo:Large-scalepretrainingfortext-to-videogenerationviatrans-
formers”.In:arXivpreprintarXiv:2205.15868(2022).
[8] RubenVillegasetal.“Phenaki:Variablelengthvideogenerationfromopendomaintextual
descriptions”.In:InternationalConferenceonLearningRepresentations.2022.
[9] DaquanZhouetal.“Magicvideo:Efficientvideogenerationwithlatentdiffusionmodels”.In:
arXivpreprintarXiv:2211.11018(2022).
[10] Levon Khachatryan et al. “Text2video-zero: Text-to-image diffusion models are zero-shot
videogenerators”.In:ProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision.2023,pp.15954–15964.
[11] ZhengxiongLuoetal.“Videofusion:Decomposeddiffusionmodelsforhigh-qualityvideo
generation”.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.2023,pp.10209–10218.
[12] YingqingHeetal.“Latentvideodiffusionmodelsforhigh-fidelitylongvideogeneration”.In:
arXivpreprintarXiv:2211.13221(2022).
[13] Jiuniu Wang et al. “Modelscope text-to-video technical report”. In: arXiv preprint
arXiv:2308.06571(2023).
[14] NanLiuetal.“Compositionalvisualgenerationwithcomposablediffusionmodels”.In:ECCV.
2022.
10[15] WeixiFengetal.“Training-FreeStructuredDiffusionGuidanceforCompositionalText-to-
ImageSynthesis”.In:ICLR.2023.
[16] HilaCheferetal.“Attend-and-excite:Attention-basedsemanticguidancefortext-to-image
diffusionmodels”.In:ACMTrans.Graph.2023.
[17] KaiyiHuangetal.“T2i-compbench:Acomprehensivebenchmarkforopen-worldcomposi-
tionaltext-to-imagegeneration”.In:NeurIPS(2024).
[18] JunsongChenetal.“PixArt-alpha:FastTrainingofDiffusionTransformerforPhotorealistic
Text-to-ImageSynthesis”.In:ICLR.2024.
[19] JamesBetkeretal.“Improvingimagegenerationwithbettercaptions”.In:ComputerScience.
https://cdn.openai.com/papers/dall-e-3.pdf 2.3(2023),p.8.
[20] PatrickEsseretal.ScalingRectifiedFlowTransformersforHigh-ResolutionImageSynthesis.
2024.arXiv:2403.03206[cs.CV].
[21] ZiqiHuangetal.“Vbench:Comprehensivebenchmarksuiteforvideogenerativemodels”.In:
arXivpreprintarXiv:2311.17982(2023).
[22] YaofangLiuetal.“Evalcrafter:Benchmarkingandevaluatinglargevideogenerationmodels”.
In:arXivpreprintarXiv:2310.11440(2023).
[23] YuanxinLiuetal.“Fetv:Abenchmarkforfine-grainedevaluationofopen-domaintext-to-
videogeneration”.In:AdvancesinNeuralInformationProcessingSystems36(2024).
[24] TimSalimansetal.“Improvedtechniquesfortraininggans”.In:NeurIPS.2016.
[25] MartinHeuseletal.“Ganstrainedbyatwotime-scaleupdateruleconvergetoalocalnash
equilibrium”.In:NeurIPS.2017.
[26] ThomasUnterthineretal.“FVD:Anewmetricforvideogeneration”.In:(2019).
[27] JackHesseletal.“Clipscore:Areference-freeevaluationmetricforimagecaptioning”.In:
arXivpreprintarXiv:2104.08718(2021).
[28] HuiwenChangetal.“Muse:Text-to-imagegenerationviamaskedgenerativetransformers”.
In:arXivpreprintarXiv:2301.00704(2023).
[29] DanKondratyuketal.“Videopoet:Alargelanguagemodelforzero-shotvideogeneration”.
In:arXivpreprintarXiv:2312.14125(2023).
[30] Lijun Yu et al. “Magvit: Masked generative video transformer”. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.2023,pp.10459–10469.
[31] LijunYuetal.“LanguageModelBeatsDiffusion–TokenizerisKeytoVisualGeneration”.In:
arXivpreprintarXiv:2310.05737(2023).
[32] HuiwenChangetal.“Maskgit:Maskedgenerativeimagetransformer”.In:Proceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.2022,pp.11315–11325.
[33] Zeroscope. 2023. URL: https://huggingface.co/cerspense/zeroscope_%20v2_
576w.
[34] XinMaetal.“Latte:Latentdiffusiontransformerforvideogeneration”.In:arXivpreprint
arXiv:2401.03048(2024).
[35] DavidJunhaoZhangetal.“Show-1:Marryingpixelandlatentdiffusionmodelsfortext-to-
videogeneration”.In:arXivpreprintarXiv:2309.15818(2023).
[36] HaoxinChenetal.“Videocrafter2:Overcomingdatalimitationsforhigh-qualityvideodiffu-
sionmodels”.In:arXivpreprintarXiv:2401.09047(2024).
[37] hpcaitech.Open-sora:Democratizingefficientvideoproductionforall.2024.URL:https:
//github.com/hpcaitech/Open-Sora.
[38] PKU-Yuan Lab and Tuzhan AI etc. Open-Sora-Plan. Apr. 2024. DOI: 10.5281/zenodo.
10948109.URL:https://doi.org/10.5281/zenodo.10948109.
[39] YuweiGuoetal.“Animatediff:Animateyourpersonalizedtext-to-imagediffusionmodels
withoutspecifictuning”.In:arXivpreprintarXiv:2307.04725(2023).
[40] YeTianetal.VideoTetris:TowardsCompositionalText-to-VideoGeneration.2024.arXiv:
2406.04277[cs.CV].URL:https://arxiv.org/abs/2406.04277.
[41] LongLianetal.“Llm-groundedvideodiffusionmodels”.In:arXivpreprintarXiv:2309.17444
(2023).
[42] Shenghai Yuan et al. “MagicTime: Time-lapse Video Generation Models as Metamorphic
Simulators”.In:arXivpreprintarXiv:2404.05014(2024).
11[43] Pika.2023.URL:https://www.pika.art/.
[44] Gen-2.2024.URL:https://research.runwayml.com/gen2.
[45] Gen-3.2024.URL:https://runwayml.com/blog/introducing-gen-3-alpha/.
[46] Dreamina.2024.URL:https://dreamina.capcut.com/ai-tool/platform.
[47] PixVerse.2024.URL:https://app.pixverse.ai.
[48] LumaAI.2024.URL:https://lumalabs.ai/dream-machine.
[49] Kling.2024.URL:https://kling.kuaishou.com/.
[50] ZhihengLietal.“Stylet2i:Towardcompositionalandhigh-fidelitytext-to-imagesynthesis”.
In:CVPR.2022.
[51] QiuchengWuetal.“Harnessingthespatial-temporalattentionofdiffusionmodelsforhigh-
fidelitytext-to-imagesynthesis”.In:ICCV.2023.
[52] MaitreyaPateletal.“Eclipse:Aresource-efficienttext-to-imagepriorforimagegenerations”.
In:arXivpreprintarXiv:2312.04655(2023).
[53] XuantongLiuetal.“RefereeCanPlay:AnAlternativeApproachtoConditionalGeneration
viaModelInversion”.In:arXivpreprintarXiv:2402.16305(2024).
[54] DongHukParketal.“Benchmarkforcompositionaltext-to-imagesynthesis”.In:NeurIPS.
2021.
[55] Minghao Chen, Iro Laina, and Andrea Vedaldi. “Training-free layout control with cross-
attentionguidance”.In:WACV.2024.
[56] RuichenWangetal.“Compositionaltext-to-imagesynthesiswithattentionmapcontrolof
diffusionmodels”.In:arXivpreprintarXiv:2305.13921(2023).
[57] TunaHanSalihMeraletal.“CONFORM:ContrastisAllYouNeedForHigh-FidelityText-to-
ImageDiffusionModels”.In:arXivpreprintarXiv:2312.06059(2023).
[58] YunjiKimetal.“Densetext-to-imagegenerationwithattentionmodulation”.In:2023.
[59] RoyiRassinetal.“Linguisticbindingindiffusionmodels:Enhancingattributecorrespondence
throughattentionmapalignment”.In:NeurIPS(2024).
[60] HananGanietal.“Llmblueprint:Enablingtext-to-imagegenerationwithcomplexanddetailed
prompts”.In:arXivpreprintarXiv:2310.10640(2023).
[61] YuhengLietal.“Gligen:Open-setgroundedtext-to-imagegeneration”.In:ICCV.2023.
[62] AshkanTaghipouretal.“BoxIttoBindIt:UnifiedLayoutControlandAttributeBindingin
T2IDiffusionModels”.In:arXivpreprintarXiv:2402.17910(2024).
[63] ZhenyuWangetal.“DivideandConquer:LanguageModelscanPlanandSelf-Correctfor
CompositionalText-to-ImageGeneration”.In:arXivpreprintarXiv:2401.15688(2024).
[64] XiaohuiChenetal.“Reasonoutyourlayout:Evokingthelayoutmasterfromlargelanguage
modelsfortext-to-imagesynthesis”.In:arXivpreprintarXiv:2311.17126(2023).
[65] LingYangetal.“Masteringtext-to-imagediffusion:Recaptioning,planning,andgenerating
withmultimodalllms”.In:arXivpreprintarXiv:2401.11708(2024).
[66] KhurramSoomro,AmirRoshanZamir,andMubarakShah.“UCF101:Adatasetof101human
actionsclassesfromvideosinthewild”.In:arXivpreprintarXiv:1212.0402(2012).
[67] WillKayetal.“Thekineticshumanactionvideodataset”.In:arXivpreprintarXiv:1705.06950
(2017).
[68] JoaoCarreiraetal.“Ashortnoteaboutkinetics-600”.In:arXivpreprintarXiv:1808.01340
(2018).
[69] LucasSmairaetal.“Ashortnoteonthekinetics-700-2020humanactiondataset”.In:arXiv
preprintarXiv:2010.10864(2020).
[70] AlecRadfordetal.“Learningtransferablevisualmodelsfromnaturallanguagesupervision”.
In:ICML.2021.
[71] GPT-4TechnicalReport.2024.arXiv:2303.08774[cs.CL].
[72] DeyaoZhuetal.“Minigpt-4:Enhancingvision-languageunderstandingwithadvancedlarge
languagemodels”.In:arXivpreprintarXiv:2304.10592(2023).
[73] Haotian Liu et al. “Improved baselines with visual instruction tuning”. In: arXiv preprint
arXiv:2310.03744(2023).
[74] LinXuetal.“Pllava:Parameter-freellavaextensionfromimagestovideosforvideodense
captioning”.In:arXivpreprintarXiv:2404.16994(2024).
12[75] Wonkyun Kim et al. “An Image Grid Can Be Worth a Video: Zero-shot Video Question
AnsweringUsingaVLM”.In:arXivpreprintarXiv:2403.18406(2024).
[76] JasonWeietal.“Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels”.In:
NeurIPS.2022.
[77] ShilongLiuetal.“GroundingDINO:MarryingDINOwithGroundedPre-TrainingforOpen-
SetObjectDetection”.In:arXivpreprintarXiv:2303.05499(2023).
[78] AlexanderKirillovetal.“SegmentAnything”.In:arXiv:2304.02643(2023).
[79] LiheYangetal.“Depthanything:Unleashingthepoweroflarge-scaleunlabeleddata”.In:
arXivpreprintarXiv:2401.10891(2024).
[80] TianheRenetal.“Groundedsam:Assemblingopen-worldmodelsfordiversevisualtasks”.
In:arXivpreprintarXiv:2401.14159(2024).
[81] GuillaumeLeMoing,JeanPonce,andCordeliaSchmid.“Denseopticaltracking:Connecting
thedots”.In:arXivpreprintarXiv:2312.00786(2023).
[82] JunnanLietal.“Blip:Bootstrappinglanguage-imagepre-trainingforunifiedvision-language
understandingandgeneration”.In:ICML.2022.
13Appendix
A T2V-CompBenchDatasetConstruction
ThissectionprovidestheinstructionsusedtopromptGPT-4[71]togeneratethetextpromptsforT2V-
CompBenchandthecorrespondingmetadataforevaluation. ThetextpromptsinT2V-CompBench,
metadatageneratedforevaluation,andtheevaluationcodeareavailableatthislink.
Consistent attribute binding. Table 3 and Table 4 show the input templates for generating the
promptsinthecategoryofconsistentattributebinding. TemplatesinTable3areusedforgenerating
promptswithcolor,shape,ortextureattributes. TemplateinTable4isusedforgeneratingprompts
withhumanattributes. Table5isusedtogeneratethemetadataforevaluation.
Table3: Templatesforgeneratingcommonanduncommonconsistentattributebindingprompts
usingGPT-4. Theattributetypecanbecolor,shape,ortexture. Herewetakecolorasanexample.
Theinputattributetypeandexamplesaremarkedbycyan. Theexampleoutputpromptsaremarked
byorange.
Type Common Uncommon
User Generate30promptsfeaturingtwoobjects Generate10promptsfeaturingtwoobjects
with{color}attributes. with{color}attributes.
Ensuretheseobjectsarenaturaltocoexist, Thetwoobjectscanbeuncommontoco-
engaginginseparateorsharedactivities. exist,engaginginseparateorsharedactiv-
Objectsshouldbedescribedusingvibrant, ities.
active verbs, avoiding static actions like Objectsshouldbedescribedusingvibrant,
‘see’,‘rest’,or‘park’. active verbs, avoiding static actions like
Selectobjectsthatvaryin{color},speci- ‘see’,‘rest’,or‘park’.
fyingaunique{color}foreachwithinthe The{color}attributescanbeuncommon,
prompttoenrichthevisualnarrative. such as ‘Purple banana hanging from a
Eligibleobjectcategoriesinclude: animal, greenvine’or‘Blueapplebouncingnear
plant,vehicle,artifact,food,andbuilding. apinktree’.
Herearesomeexamples: Eligibleobjectcategoriesinclude: animal,
Abluecardrivespastawhitepicketfence plant,vehicle,artifact,food,andbuilding.
onasunnyday,
Yellowrubberduckfloatingnexttoablue
bathtowel.
GPT-4 A black dog sits beside a red vase on a Greencatchasingapurplemouse
table Orangepalmtreeswayingbesideasilver
Greenparrotperchingonabrownchair shed
Greentractorplowingnearawhitefarm- Pinkgiraffeeatingfromabrownbush
house ......
......
Dynamicattributebinding. Table6showstheinputtemplateforgeneratingthepromptsinthe
categoryofdynamicattributebinding. Table7isusedtogeneratethemetadataforevaluation.
Spatialrelationships. Table8showstheinputtemplateforgeneratingthepromptsinthecategoryof
spatialrelationships. Table9isusedtogeneratethemetadataforevaluation.
Actionbinding. Table10showstheinputtemplatesforgeneratingthepromptsinthecategoryof
actionbinding. Table11isusedtogeneratethemetadataforevaluation.
Motionbinding. Table12showstheinputtemplateforgeneratingthepromptsinthecategoryof
motionbinding. Table13isusedtogeneratethemetadataforevaluation.
Objectinteractions. Table14showstheinputtemplatesforgeneratingthepromptsinthecategory
ofobjectinteractions.
Generativenumeracy. Table15showstheinputtemplatesforgeneratingthepromptsinthecategory
ofgenerativenumeracy. Table16isusedtogeneratethemetadataforevaluation.
14Table4: TemplateforgeneratingconsistentattributebindingpromptsusingGPT-4. Theattribute
typeishumanattribute. Theexampleoutputpromptsaremarkedbyorange.
User Generate30promptsfeaturingtwoobjects,oneofwhichshouldbeahuman.
Theotherobjectcanbeahumanorfallintothecategoriesofanimal,plant,vehicle,
artifact,food,orbuilding.
Objectsshouldbedescribedusingvibrant,activeverbs,avoidingstaticactionslike
‘see’,‘rest’,or‘park’.
Considerattributesofhumanappearancesuchasheight,weight,hairstyle,age,facial
hair,glasses,orotheraccessories.
Forthenon-humanobject,focusonattributesofcolor,shape,ortexture. Choose
only one attribute for each object. Use adjectives like tall and short together for
comparison.
Herearesomeexamples:
Boywearingablueshirtplayingchesswithabeardedman,
Tallwomandiscussingplanswithashortwoman.
GPT-4 Girlwithcurlyhairridingaredbike
Womaninreddresspaintingwithamaninagreenshirt
Boywithacapplayingsoccerwithagirlwearingaredshirt
......
Table5: TemplateforgeneratingthemetadataofconsistentattributebindingpromptsusingGPT-4.
User Iwillprovideyouwithseveralprompts,eachonaseparateline. Eachpromptwill
describetwoobjects,eachassociatedwithanattribute. Foreachprompt,youare
tocreateaJSONobject. ThisJSONobjectshouldincludetwokeys: "prompt"and
"phrases". The"prompt"keyshouldsimplyreplicatetheoriginalprompttext. The
"phrases"keyshouldcontainthetwodescribedobjectsseparatedbyasemicolon.
Forinstance,fortheprompt"Abluecardrivespastawhitepicketfenceonasunny
day",thevalueunder"phrases"shouldbe"abluecar;awhitepicketfence". Below
arethepromptsforwhichyouneedtogeneratetheJSONobjects:
Abluecardrivespastawhitepicketfenceonasunnyday
Yellowrubberduckfloatingnexttoabluebathtowel
Greentractorplowingnearawhitefarmhouse
......
GPT-4 HerearetheJSONrepresentationsfortheprompts:
{"prompt": "Abluecardrivespastawhitepicketfenceonasunnyday","phrases":
"abluecar;awhitepicketfence"},
{"prompt":"Yellowrubberduckfloatingnexttoabluebathtowel","phrases":"yellow
rubberduck;abluebathtowel"},
{"prompt": "Green tractor plowing near a white farmhouse ", "phrases": "green
tractor;awhitefarmhouse"},
......
B ImplementationDetails
ModelScope [13] is a diffusion-based text-to-video generation model that evolves from a text-
to-image model by incorporating spatio-temporal blocks, ensuring consistent frame generation
andsmoothmovementtransitions. ZeroScope[33]isafoundationaltext-to-videolatentdiffusion
model. Latte[34]isavideogenerationmodelthatleveragesalatentdiffusiontransformer. Show-
1[35]isahigh-qualitytext-to-videodiffusionmodelthatinitiallyemployspixel-baseddiffusion
to create a low-resolution video, followed by a latent-based approach to upsample the video to
high resolution. VideoCrafter2 [36] is a video generation model that generates videos with high
visualqualityandprecisetext-videoalignmentwithoutrequiringhigh-qualityvideosOpen-Sora1.1
and1.2[37]andOpen-Sora-Planv1.0.0andv1.1.0[38]areopen-sourceprojectsfocusedonvideo
generation,aimingatcreatingawaytoreproduceSora. AnimateDiff[39]providesaframeworkfor
animatingpersonalizedT2Imodelswithamotionmodule. LVD[41]isavideodiffusionmodelthat
15Table6: TemplateforgeneratingdynamicattributebindingpromptsusingGPT-4. Herewetake
statechangeasanexample. Theinputexamplepromptsanddynamicchangetypearemarkedby
cyan. Theexampleoutputpromptsaremarkedbyorange.
User Generate50promptsfeaturingtwoobjects,eachofthemwithitsownstate,think
aboutsomecasesthatthestateischanging,fromaninitialstatetoafinalstate.
Eligibleobjectcategoriesinclude: human,animal,plant,vehicle,artifact,food,and
building.
Herearesomeexamples:
Atimelapseofanappledecayingovertime
Apumpkingrowingfromasmallbudintoalargefruit
GPT-4 Atimelapseofaflowerbudbloomingintoafullflower.
Asnowmanmeltingundertheafternoonsun.
......
Table7: TemplateforgeneratingthemetadataofdynamicattributebindingpromptsusingGPT-4.
User I will provide you with several prompts, each on a separate line. Each prompt
describesastateorcolorchangeofoneormultipleobjects. Foreachprompt,youare
tocreateaJSONobject. ThisJSONobjectshouldincludethefollowingkeys:
"prompt": simplyreplicatetheoriginalprompttext.
"state0": theinitialstateoftheobject(s).
"state1": thefinalstateoftheobject(s).
BelowarethepromptsforwhichyouneedtogeneratetheJSONobjects:
Atimelapseofaleaftransitioningfromgreentobrightredasautumnprogresses
Asilvercointarnishingfromshinytodull
......
GPT-4 HerearetheJSONrepresentationsforeachprompt:
{"prompt": "Atimelapseofaleaftransitioningfromgreentobrightredasautumn
progresses","state0": "Agreenleaf","state1": "Abrightredleaf"}
{"prompt": "Asilvercointarnishingfromshinytodull","state0": "Ashinysilver
coin","state1": "Adullsilvercoin"}
......
Table8: TemplateforgeneratingspatialrelationshipspromptsusingGPT-4. Thespatialrelationship
canbeleft,right,above,below,infrontof,andbehind. Herewetakeleftasanexample. Theinput
spatial relationship and example prompts are marked by cyan. The example output prompts are
markedbyorange.
User Generate30promptsfeaturingtwoobjectswhereoneis{ontheleftof}theother.
Thepromptmustcontainmovableobjectslikehumanoranimal,orwithdynamic
verbstoensurethesceneisdynamic.
Eligibleobjectcategoriesinclude: human,animal,plant,vehicle,artifact,food,and
building.
Herearesomeexamples:
Abirdflyingontheleftsideofahotairballoon
Atoddlerwalkingontheleftofadoginapark
Allamawalkingontheleftsideofamotorcycleinacitystreet
GPT-4 Adogrunningontheleftofabicycle
Apenguinstandingontheleftsideofacactusinadesert
Asheepgrazingontheleftofasurfboardonasandybeach
......
incorporatesLLMfordynamicscenelayouts,andthenenhancespromptunderstandingcapabilities
such as attributes and motion patterns. VideoTetris [40] is designed for compositional multiple
objectsanddynamicchangesinobjectnumbers. Additionally,fordynamicattributebinding,we
16Table9: TemplateforgeneratingthemetadataofspatialrelationshipspromptsusingGPT-4.
User Iwillprovideyouwithseveralprompts,eachonaseparateline. Foreachprompt,
youaretocreateaJSONobject.
ThisJSONobjectshouldincludethefollowingkeys:
"prompt": simplyreplicatetheoriginalprompttext.
"spatial": containthewordfromthepromptthatrepresentsthespatialrelationship.
Chooseonefromthefollowingsixoptions: left,right,above,below,infrontof,and
behind. Forexample,ifthepromptusestheword"under",setthevalueof"spatial"
to"below"becausethesewordshavethesamemeaning.
"object_1"and"object_2": Thesekeysrepresentthetwoobjectsmentionedinthe
prompt. Thevalueof"object_1"shouldbepositionedtotheleftof,rightof,above,
below,infrontof,orbehind"object_2"basedonthespatialwordusedintheprompt.
Here’sanexampletoillustratetheformat:
Prompt: "Atoddlerwalkingontheleftofadoginapark"
JSONobject: {"prompt": "Atoddlerwalkingontheleftofadoginapark","spatial":
"left","object_1": "toddler","object_2": "dog"}
BelowarethepromptsforwhichyouneedtogeneratetheJSONobjects:
Abirdflyingontheleftsideofahotairballoon
Atoddlerwalkingontheleftofadoginapark
Allamawalkingontheleftsideofamotorcycleinacitystreet
......
GPT-4 HerearetheJSONobjectsforeachofyourprompts,accuratelycapturingthespatial
relationship,object_1,andobject_2:
{"prompt": "A bird flying on the left side of a hot air balloon", "spatial": "left",
"object_1": "bird","object_2": "hotairballoon"},
{"prompt": "A toddler walking on the left of a dog in a park", "spatial": "left",
"object_1": "toddler","object_2": "dog"},
{"prompt":"Allamawalkingontheleftsideofamotorcycleinacitystreet","spatial":
"left","object_1": "llama","object_2": "motorcycle"},
......
evaluateMagicTime[42]thatisdesignedformetamorphictime-lapsevideogeneration. Thedetails
ofthevideosgeneratedbytheT2Vmodels,includingduration,FPS,andresolution,arepresentedin
Table17.
C AutomaticEvaluationDetails
VideoLLM-basedmetricsforconsistentattributebinding,actionbinding,andobjectinterac-
tions. WedetailthepromptsusedforvideoLLM-basedmetrics. Foreachcategory,weboostthe
abilityofvideoLLMbythechain-of-thoughtmechanism[76]. Specifically,Table18,Table20,and
Table21showtheprompttemplateforevaluatingconsistentattributebinding,actionbinding,and
objectinteractions,respectively.
Image LLM-based metrics for dynamic attribute binding. For evaluating dynamic attribute
bindinginavideoandtodisentanglethecomplextemporalsequence,weuseGPT-4toextractthe
initialandfinalstatesfromagivenprompt. Forinstance,giventheprompt“Atimelapseofaflower
budbloomingintoafullflower,”GPT-4identifiestheinitialstateas“Aflowerbud”andthefinalstate
as“Afullflower.”
Thevideoframesarelabeledas(F ,F ,...,F ). Weevaluatethefirstandlastframes(F andF )
0 1 T 0 T
inrelationtotheinitialandfinalstates(i.e.,theflowerbudandthefullflower),whileensuringthat
theintermediateframes(F ,F ,...,F )representthetransitionbetweenthesestates.
1 2 T−1
Specifically,wealigntheinitialframe(F )withboththeinitialandfinalstatesanddothesamefor
0
thefinalframe(F ). Fortheintermediateframes,weensuretheydepictthetransitionfromtheinitial
T
statetothefinalstate. ThedetailedpromptsarerepresentedinTable19. Theoverallevaluationscore
17Table10: TemplatesforgeneratingcommonanduncommonactionbindingpromptsusingGPT-4.
Theexampleinputpromptsaremarkedbycyan. Theexampleoutputpromptsaremarkedbyorange.
Type Common Uncommon
User Generate80promptsthatdescribeeither Generate20promptsthatdescribeeither
ananimalorahumanengaginginanactiv- ananimalorahumanengaginginanactiv-
ity,whileanotheranimalorhumanisalso ity,whileanotheranimalorhumanisalso
engagedinaseparateactivity. engagedinaseparateactivity.
Usedynamicverbsoractionssuchaswalk, You can choose to pair animals with an-
run,fly,swim,oreat. thropomorphicactivitiessuchasplayagui-
Avoid static actions like think, dream, tar,takingaselfie,listeningtomusicwith
watch,see,ormakeasound. headphones,orwearingVRgoggles.
Herearesomeexamples: Alternatively,youcanusetwoobjectsthat
A dog runs through a field while a cat areuncommontocoexist,suchas‘Apen-
climbsatree guinwaddlesontheice,acameltreksby’.
A panda eats bamboo while a monkey Usedynamicverbsoractionssuchaswalk,
swingsfrombranchtobranch run,fly,swim,oreat. Avoidstaticactions
Akangarooboundsacrosstheplainanda like think, dream, watch, see, or make a
cowgrazes sound.
GPT-4 Anelephantsprayswaterwithitstrunk,a Adogplaysguitarwhileacattakesaselfie
lionsittingnearby Akangarooboxesinaringwhileasheep
A deer grazes in the meadow as a rabbit jumpsrope
hopsby Afishswimsgracefullyinatankasahorse
A frog jumps from lily to lily, a swan gallopsoutside
swimsnearby ......
......
Table11: TemplateforgeneratingthemetadataofactionbindingpromptsusingGPT-4.
User Iwillprovideyouwithseveralprompts,eachonaseparateline. Foreachprompt,
yourtaskistoextractthepromptintoanounphraseandanounphrasewithanaction.
Forexample,giventheprompt"Adogrunsthroughafieldwhileacatclimbsatree",
theoutputshouldbe: {"prompt": "Adogrunsthroughafieldwhileacatclimbsa
tree","phrase_0": ["adog?","adogrunsthroughafield?"],"phrase_1": ["acat?",
"acatclimbsatree?"]}
BelowarethepromptsforwhichyouneedtogeneratetheJSONobjects:
Anelephantsprayswaterwithitstrunk,alionsittingnearby
Adeergrazesinthemeadowasarabbithopsby
Afrogjumpsfromlilytolily,aswanswimsnearby
......
GPT-4 Herearetheextractedphrasesforeachprompt:
{"prompt":"Anelephantsprayswaterwithitstrunk,alionsittingnearby","phrase_0":
["anelephant?","anelephantsprayswaterwithitstrunk?"],"phrase_1": ["alion?",
"alionsittingnearby?"]},
{"prompt":"Adeergrazesinthemeadowasarabbithopsby","phrase_0":["adeer?",
"adeergrazesinthemeadow?"],"phrase_1": ["arabbit?","arabbithopsby?"]},
{"prompt": "Afrogjumpsfromlilytolily,aswanswimsnearby","phrase_0": ["a
frog?","afrogjumpsfromlilytolily?"],"phrase_1": ["aswan?","aswanswims
nearby?"]},
......
calculatesthescoresofthefirstandlastframes, andgettheaveragescorewhileconsideringthe
intermediateframes’temporaldynamics.
18Table12: TemplateforgeneratingmotionbindingpromptsusingGPT-4. Themotiondirectioncan
beleftwards,rightwards,upwards,anddownwards. Herewetakeasingleobjectmovingleftwards
asanexample. Theinputmotiondirectionandexamplepromptsaremarkedbycyan. Theexample
outputpromptsaremarkedbyorange.
User Generate 30 prompts that describe an animal, human, or movable object moving
leftwards.
Herearesomeexamples:
Acatslinkingtotheleftsideofacozylivingroom
Agoldenretrieverscamperingleftwardsacrossagarden
Aboatsailstotheleftontheocean
GPT-4 Agoldenretrieverscamperingleftwardsacrossagarden
Asoccerballbeingkickedhardtotheleft
Apaperairplaneglidingtotheleftacrossaclassroom
......
Table13: TemplateforgeneratingthemetadataofmotionbindingpromptsusingGPT-4.
User I will provide you with several prompts, each on a separate line. Each prompt
describes the movement of one or two objects in different directions. For each
prompt, you are to create a JSON object. This JSON object should include the
followingkeys:
"prompt": simplyreplicatetheoriginalprompttext.
"object_1": thefirstobjectmentionedinthepromptthatmovesinadirection.
"d_1": motiondirectionfor"object_1"andcanonlybeoneofthefollowing: left,
right,up,ordown.
"object_2": If there is a second object mentioned in the prompt that moves in a
direction,thiskeywillcontainitsname. Otherwise,thevaluewillbeanemptystring.
"d_2": Ifthereis"object_2",thiskeywillrepresentitsmotiondirection. Otherwise,
thevaluewillbeanemptystring.
BelowarethepromptsforwhichyouneedtogeneratetheJSONobjects:
Arabbithopsfromlefttorightinagarden
Apaperairplaneglidingtotheleftacrossaclassroom
Aballrollsleftwhileapuppydashesright
......
GPT-4 HerearetheJSONrepresentationsforeachprompt:
{"prompt": "Arabbithopsfromlefttorightinagarden","object_1": "rabbit",d_1":
"right","object_2": "","d_2": ""}
{"prompt": "A paper airplane gliding to the left across a classroom", "object_1":
"paperairplane",d_1": "right","object_2": "","d_2": ""}
{"prompt": "Aballrollsleftwhileapuppydashesright","object_1": "ball",d_1":
"left","object_2": "puppy","d_2": "right"}
......
D HumanEvaluationDetails
Wefollow[17]toconducthumanevaluationsonAmazonMechanicalTurk(AMT).Specifically,we
asktheannotatorstoratethealignmentscorebetweenageneratedvideoandthetextpromptusedto
generatethevideo,basedontheprovidedexamplesandrules. Figure6,7,8,9,10,11,12showthe
interfacesforhumanevaluationoverthe7categories. Werandomlysample15promptsfromeach
categoryandusethegeneratedvideosfrom6models(ModelScope[13],Latte[34],Show-1[35],
VideoCrafter2[36],Open-Sora1.1[37],Open-Sora-Planv1.0.0[38]). Inaddition,weinclude10
groundtruthvideosforthedynamicattributebindingcategoryand16fortheobjectinteractions
category. Intotal,wegather656video-textpairsforhumanevaluationexperiments. Eachvideo-text
pairisratedby3humanannotatorsaccordingtothevideo-textalignment. Theestimatedhourly
wagepaidtoeachparticipantis9.6USD.Wespend195USDintotalonparticipantcompensation.
19Table14: TemplatesforgeneratingphysicalandsocialobjectinteractionspromptsusingGPT-4.
Theinputinteractiontypeandexamplepromptsaremarkedbycyan. Theexampleoutputprompts
aremarkedbyorange.
Type Physical Social
User Generate30promptsdescribingphysical Generate20promptsdescribingsocialin-
interactionbetweentwoobjectsthatcause teractionbetweentwoobjects.
motionstatechange, Eligibleobjectcategoriesinclude: human,
Eligibleobjectcategoriesinclude: human, animal,plant,vehicle,artifact,food,and
animal,plant,vehicle,artifact,food,and building.
building. Herearesomeexamples:
Herearesomeexamples: Manteachesrobottoplaychess
Twocarscollideatanintersection Girlreadsbedtimestorytoherstuffedbear
Footballplayersbumpintoeachotherdur-
ingagame
GPT-4 Childkicksasoccerball Childplayswithfamilydogatthebeach
Birdlandsonabranchcausingittodip Fishermanfeedsseagullsatthedock
...... ......
Table15: TemplateforgeneratinggenerativenumeracypromptsusingGPT-4. Theexampleinput
promptsaremarkedbycyan. Theexampleoutputpromptsaremarkedbyorange.
User Generate 40 prompts that describe a specific object class with a quantity ranging
fromonetoeight,alongwithanotherobjectclasswithaquantityrangingfromone
toeight,engaginginactivitieswithinascene. Theeligibleobjectcategoriesinclude
human,animal,plant,vehicle,artifact,food,andbuilding.
Herearesomeexamples:
Threedogsplayinthesnowwitharedball
Fourchildrenandthreedogshavingapicnicinapark
GPT-4 Threebeesandfivebutterfliesflyaroundabloominggarden
twohorsesandfourcowsgrazetogetheronasunnyhillside
......
20Table16: TemplateforgeneratingthemetadataofgenerativenumeracypromptsusingGPT-4.
User I will provide you with several prompts, each on a separate line. Each prompt
describesacertainnumberofobjectsengaginginanactivity,whereeachobjectis
associatedwithaspecificquantity. Forexample,thepromptmayincludephraseslike
"twocats"or"threedogs".
Foreachprompt,youaretocreateaJSONobject. ThisJSONobjectshouldinclude
thefollowingkeys:
"prompt": simplyreplicatetheoriginalprompttext.
"objects": Thiskeywillcontaintheobjectsmentionedintheprompt. Ifthereare
multipleobjects,theywillbeseparatedbycommas.
"numbers": Thiskeywillrepresentthecorrespondingnumbersassociatedwitheach
object. Thenumberswillbeconcatenatedwithcommas.
Whendescribingtheactiontheobjectsareperformingandtheenvironment,there
may be mentions of other objects, articles like "a park" or "the woods" or other
unnecessarydetails. Pleaseignoretheseandfocusonlyonextractingtheobjectsand
theirrespectivequantities.
BelowarethepromptsforwhichyouneedtogeneratetheJSONobjects:
threebeesandfivebutterfliesflyaroundabloominggarden
twohorsesandfourcowsgrazetogetheronasunnyhillside
......
GPT-4 HerearetheJSONrepresentationsforeachprompt:
{"prompt": "threebeesandfivebutterfliesflyingaroundabloominggarden","ob-
jects": "bee,butterfly","numbers": "3,5"},
{"prompt": "twohorsesandfourcowsgrazetogetheronasunnyhillside","objects":
"horse,cow","numbers": "2,4"},
......
Table17: Detailsaboutevaluationmodels. Thetableshowsduration,FPS,andresolutionforeach
model.
Model Duration(s) FPS Resolution
ModelScope[13] 2 8 256×256
ZeroScope[33] 3.6 10 576×320
Latte[34] 2 8 512×512
Show-1[35] 3.625 8 576×320
VideoCrafter2[36] 2.7 24 512×320
Open-Sora1.1[37] 2 8 512×512
Open-Sora1.2[37] 2.125 24 640×360
Open-Sora-Planv1.0.0[38] 2.7 24 512×512
Open-Sora-Planv1.1.0[38] 2.7 24 512×512
AnimateDiff[39] 2 8 384×256
VideoTetris[40] 1.6 10 512×320
LVD[41] 2 8 256×256
MagicTime[42] 2 8 512×512
Pika[43] 3 24 1280×720
Gen-2[44] 4 24 1408×768
Gen-3[45] 5.33 24 1280×768
Dreamina[46] 3 8 1280×720
PixVerse[47] 4.06 18 1408×768
DreamMachine[48] 5.25 24 1360×752
Kling[49] 5.1 30 1280×720
21Table18: PromptsdetailsforvideoLLM-basedevaluationmetricsonconsistentattributebinding.
Theprovidedimagearrangeskeyframesfromavideoinagridview.
Describe Describethevideowithin20words,carefullyexaminingthecharactersorobjects
throughouttheframesandtheirvisibleattributes.
Accordingtoyourpreviousdescription,pleaseselectoneanswerfromoptionsA1
toD1forthefirstmultiple-choicequestion,andselectoneanswerfromoptionsA2
toD2forthesecondone.
Question1:
A1: ’{phrase_1}’isclearlyportrayedthroughouttheframes.
B1: ’{phrase_1}’ispresentinsomeframes.
C1: ’{phrase_1}’isnotcorrectlyportrayed.
Predict D1: ’{phrase_1}’isnotpresent.
Question2:
A2: ’{phrase_2}’isclearlyportrayedthroughouttheframes.
B2: ’{phrase_2}’ispresentinsomeframes.
C2: ’{phrase_2}’isnotcorrectlyportrayed.
D2: ’{phrase_2}’isnotpresent.
SeparatethetwooptionsbyacommaandputitinJSONformatwiththefollowing
keys: option(e.g.,A1,B2),explanation(within20words).
Table19: PromptsdetailsforvideoLLM-basedevaluationmetricsondynamicattributebinding.
Describetheprovidedimagewithin20words,highlightalltheobjects’attributes
Describe
thatappearintheimage.
Accordingtotheimageandyourpreviousanswer,evaluateifthetext{initialstate}
or{finalstate}iscorrectlydescribedintheimage.
Giveascorefrom1to5,accordingtothecriteria:
5:theimageaccuratelydescribethetext.
4:theimageroughlydescribethetext,buttheattributeisalittledifferent.
First and
3:theimageroughlydescribethetext,buttheattributeistotallydifferent.
lastframes
2:theimagedonotdescribethetext.
1:theimagedidnotdepictanyelementsthatmatchthetext.
ProvideyouranalysisandexplanationinJSONformatwiththefollowingkeys:
score(e.g.,2),explanation(within20words).
Accordingtotheimage,evaluateiftheimageisalignedwiththetext{initialstate}
or{finalstate}.
Giveascorefrom0to2,accordingtothecriteria:
2:theimagematcheswiththetext{initialstate}.
Intermediate
1:theimagematcheswiththetext{finalstate}.
frames
0:theimageisnotalignedwiththetwotextstotally.
ProvideyouranalysisandexplanationinJSONformatwiththefollowingkeys:
score(e.g.,1),explanation(within20words).
22Table20: PromptsdetailsforvideoLLM-basedevaluationmetricsonactionbinding.
Theprovidedimagearrangeskeyframesfromavideoinagridview. Describethe
Describe videowithin20words,highlightallthecharactersorobjectsthatappearthroughout
theframesandindicatehowtheyact.
Accordingtothevideoandyourpreviousanswer,evaluateifthetext{prompt}is
correctlyportrayedinthevideo. Assignascorefrom0to5accordingtothecriteria:
5: Both{obj1}and{obj2}arepresent,and{obj1’saction},{obj2’saction}.
4: Both{obj1}and{obj2}arepresent,butonlyoneoftheactions(either{obj1’s
action}or{obj2’saction})isdepicted.
Predict 3: Both{obj1}and{obj2}arepresent,neitheroftheactionsaredepicted.
2: Onlyoneof{obj1}or{obj2}ispresent,anditsactionmatchesthetext.
1: Onlyoneof{obj1}or{obj2}ispresent,butitsactiondoesnotmatchthetext.
0: Neither{obj1}nor{obj2}appearsinthevideo.
ProvideyouranalysisandexplanationinJSONformatwiththefollowingkeys:
score(e.g.,2),explanation(within20words).
Table21: PromptsdetailsforvideoLLM-basedevaluationmetricsonobjectinteractions.
Theprovidedimagearrangeskeyframesfromavideoinagridview. Describethe
Describe videowithin20words,focusingontheinteractionsbetweencharactersorobjects
visiblethroughouttheframes.
Accordingtothevideoandyourpreviousanswer,evaluateifthetext{prompt}is
correctlyportrayedinthevideo. Assignascorefrom1to5accordingtothecriteria:
5: Alltheobjectsinvolvedintheinteractionarepresent,andtheinteractionis
depictedcorrectly.
4: Alltheobjectsinvolvedintheinteractionarepresent,andtheinteractionis
almostcorrect.
Predict
3: Alltheobjectsinvolvedintheinteractionarepresent,buttheinteractionis
notcorrect.
2: Someoftheobjectsinvolvedintheinteractionaremissing.
1: Noneoftheobjectsinvolvedintheinteractionarepresent.
ProvideyouranalysisandexplanationinJSONformatwiththefollowingkeys:
score(e.g.,2),explanation(within20words).
23Figure6: AMTInterfaceforthevideo-textalignmentevaluationonconsistentattributebinding.
24Figure7: AMTInterfaceforthevideo-textalignmentevaluationondynamicattributebinding.
25Figure8: AMTInterfaceforthevideo-textalignmentevaluationonspatialrelationships.
26Figure9: AMTInterfaceforthevideo-textalignmentevaluationonactionbinding.
27Figure10: AMTInterfaceforthevideo-textalignmentevaluationonmotionbinding.
28Figure11: AMTInterfaceforthevideo-textalignmentevaluationonobjectinteractions.
29Figure12: AMTInterfaceforthevideo-textalignmentevaluationongenerativenumeracy.
30