DEPICT: Diffusion-Enabled Permutation
Importance for Image Classification Tasks
Sarah Jabbour1 , Gregory Kondas1, Ella Kazerooni1 , Michael Sjoding1 ,
David Fouhey2∗ , and Jenna Wiens1∗
1University of Michigan, Ann Arbor, MI
2New York University, New York, NY
https://mld3.github.io/depict/
{sjabbour,wiensj}@umich.edu,david.fouhey@nyu.edu
Abstract. Weproposeapermutation-basedexplanationmethodforim-
age classifiers. Current image-model explanations like activation maps
are limited to instance-based explanations in the pixel space, making it
difficult to understand global model behavior. In contrast, permutation
based explanations for tabular data classifiers measure feature impor-
tancebycomparingmodelperformanceondatabeforeandafterpermut-
ingafeature.Weproposeanexplanationmethod for image-basedmod-
els that permutes interpretable concepts across dataset images. Given a
datasetofimageslabeledwithspecificconceptslikecaptions,wepermute
aconceptacrossexamplesinthetextspaceandthengenerateimagesvia
a text-conditioned diffusion model. Feature importance is then reflected
bythechangeinmodelperformancerelativetounpermuteddata.When
applied to a set of concepts, the method generates a ranking of feature
importance. We show this approach recovers underlying model feature
importance on synthetic and real-world image classification tasks.
Keywords: permutationimportance·explainableAI·diffusionmodels
1 Introduction
Understanding AI model predictions is often important for safe deployment.
However, explanation methods for image-based models are instance-based and
relyonheatmapsormasksinthepixelspace[21,28,32,42],andrecentworkhas
calledintoquestiontheirutility[1,2,13].Wehypothesizethatthesemethodsfall
shortinpartbecausetheyareinthepixelspaceratherthanintheconceptspace
(e.g., presence of an object), leading to an increase in the cognitive load placed
on a user. Furthermore, while useful for model debugging, it is often intractable
to look at instance-based explanations for every single image in a large test set.
We propose an approach for explaining image-based models that uses per-
mutationimportancetoproducedataset-levelexplanationsintheconceptspace.
In contrast to instance-based explanations, our method generates a ranking of
∗co-senior authors
4202
luJ
91
]VC.sc[
1v90541.7042:viXra2 S. Jabbour et al.
Original Concepts Generated Images Performance
Person Chair
1 1 Diffusion Black-box Baseline performance:
2 2 Model model High accuracy
3 3
Permute person
Person Chair
No drop in acc.
2 1 Diffusion Black-box Drop in acc.
or Does not
3 2 Model model Relies on person
rely on person
1 3
Fig.1: Text-conditioned diffusion enables permutation importance for images. Given
images captioned with concepts, we permute concepts across captions. Then, we gen-
erate images via text-conditioned diffusion models and measure classifier performance
relative to unpermuted data. If performance drops, the model relies on the concept.
feature importance by measuring the drop in model performance when one per-
mutes each concept across all instances in the test set. While widely used with
tabular data [4,5,33], it is unclear how permutation importance applies to im-
ages. For example, given a scene classifier, if we want to know to what extent
it relies on a concept like the presence of a chair, one cannot simply shuffle the
pixels of chairs across images in the dataset.
In light of these challenges, we present DEPICT, an approach that uses dif-
fusion models to enable permutation importance on image classifiers. Our main
insight is that while it is difficult to permute concepts in the pixel space, we can
permute concepts in the text space (Fig. 1). For example, we can simply shuffle
thepresenceofachairinthecaptions ofimages.Then,usingatext-conditioned
diffusion model, we bridge from text (captions) to pixel space (image), allow-
ing us to permute concepts across images. With the generated permuted and
unpermuted test set, we can apply permutation importance as usual.
Givenatargetmodel,animagetestsetcaptionedwithasetofconcepts,and
atext-conditioneddiffusionmodel,weshowthatDEPICTcangenerateconcept-
basedmodelexplanationsthatwouldotherwisebeintractablevialocalinstance-
based explanations. Through experiments on synthetic and real image data, we
show that our approach can more accurately capture the feature importance of
classifiers over commonly used instance-based explanation approaches.
2 Related Works
We introduce DEPICT, a diffusion-enabled permutation importance approach
to understand image-based classifiers. DEPICT lies at the intersection of ex-
plainable AI, generative models, and human-computer interaction.
Explainable AI. Explainable AI allows us to understand model behavior [35].
Global explanations allow us to do so as a whole. E.g., linear models that oper-
ate directly on the input space are explainable via their weights, which reflect
the importance of each input feature with respect to the model’s output [34].
The usefulness of these explanations depends in part on the interpretability of
the input space. If the input space is just pixels, such explanations are unlikelyDEPICT: Diffusion-Enabled Permutation Importance 3
Tabular Data Permutation Importance Diffusion-Enabled Image Permutation Importance 𝑔:Diffusion model
[ Original text [ [ Permute concept 𝑗in text [ 𝑓:Black-box classifier
“Person: 1, couch: 0, table: 1” “Person: 0, couch: 0, table: 1” ℎ:Concept classifier
𝑋= original data 𝑋!= permute column 𝑗 𝒄= “Person: 1, couch: 0, table: 0” 𝒄!= “Person: 1, couch: 0, table: 0” ℒ:Performance metric
person couch table person couch table “Person: 0, couch: 1, table: 1” “Person: 1, couch: 1, table: 1” 𝑋:Original images
1 0 1 0 0 1
𝑔 𝑔
1 0 0 1 0 0
0 1 1 1 1 1
𝑔(𝒄)= 𝑔(𝒄!)=
Permutation Importance Permutation Importance Check #1: effective generation Check #2: independent permutation
𝑋 𝑋! 𝑔(𝒄) 𝑔(𝒄!) 𝑋 𝑔(𝒄) 𝑋 𝑔(𝒄) 𝑔(𝒄) 𝑔(𝒄!)
𝑓 𝑓 𝑓 𝑓 𝑓 ℎ ℎ ℎ
ℒ(𝑓𝑋) − ℒ(𝑓(𝑋!)) ℒ(𝑓(𝑔(𝒄)))− ℒ(𝑓(𝑔(𝒄!))) ℒ(𝑓𝑋)−ℒ(𝑓(𝑔𝒄)) ℒℎ(𝑋)−ℒ(ℎ(𝑔𝒄)) ℒ(ℎ(𝑔𝒄)−ℒ(ℎ(𝑔(𝒄!)))
repeat for all features 𝑗 repeat for all concepts 𝑗 repeat for all concepts 𝑗
Fig.2: Approach overview. In tabular permutation importance (left), one obtains
feature importance by permuting each feature column and measuring the impact on
model performance. In diffusion-enabled image permutation importance (right), fea-
turesarepermutedinthediffusionmodel’sconditionedtextspaceandgeneratedataset
images for classifier evaluation. To validate results, one can check that the model can
accurately classify generated images, and only the permuted concept changed.
to be useful. More complex models like deep neural networks require extrinsic
explanation techniques. For tabular data, the input space corresponds to inter-
pretableconcepts,anddataset-specificfeatureimportancecanbecalculatedwith
permutation importance [5,10,37], which we describe in Section 3.1. Currently,
no global or even dataset-level explanation techniques that rank concepts exist
forimage-basedmodels.Instead,researcherstypicallyrelyoninstance-basedex-
planations in the form of activation maps or masks [21,28,32]. Our approach
helpsusunderstanddataset-levelbehaviorofimage-basedmodelsbypermuting
concepts across images using text-conditioned diffusion models.
Generative AI-enabled classifier explanations. Recent breakthroughs in
generative AI have helped researchers probe black-box models. For example,
generative models can produce counterfactual images that subsequently change
aclassifier’spredictions[8,25],andsuchchangescanbelinkedtoeitherchanges
in natural language text, concept annotations, or expert feedback to better un-
derstand why a model prediction might change. DEPICT is similar in that it
also relies on generative AI techniques to produce images with changed con-
cepts. However, in contrast, DEPICT generates a ranking of concepts based on
theireffectondownstreammodelperformance,ratherthantheireffectonmodel
predictions.
Concept bottleneck models. Concept bottleneck models (CBMs) are inter-
pretable models trained by learning a set of neurons that align with human-
specified concepts. They support interventions on concepts compared to end-to-
endmodels[18,20,22,24,36,38,40,41].OnecanperformpermutationonCBMsby
permuting concept predictions in the bottleneck layer. DEPICT differs by han-
dling a more common case of models. We cannot assume all models are CBMs:
many important networks are black-box, non-CBM models whose parameters
we do not have access to (e.g., proprietary/private data or training algorithms).4 S. Jabbour et al.
Image editing. Recent advances in text-to-image diffusion models [26,29,31]
allow for high-quality text-conditioned image synthesis, enabling easy manipu-
lationofimagesviatext-edits.DEPICTreliesongenerativemodelsconditioned
onnaturallanguagetextthatcanbemodifiedtoproduceanedited versionofan
image. Prior work on image editing has focused on limited types of edits (e.g.,
style transfer or inserting objects [17,44]). DEPICT is an application of these
techniques and advances in these areas of work would improve DEPICT.
3 Method
Overview. In our setting, we have a set of test images and a black-box model
f :I →Y that maps images in I to predictions in Y. In standard permutation
importance,onepermutesasinglefeatureacrossinstanceswhileholdingtheoth-
ers constant and examines the drop in model performance relative to baseline.
This does not yield meaningful explanations when permuting in pixel space. In-
stead,weassumethereisarelevantconcept-basedtextspaceT wherepermuting
conceptsiseasy(e.g.,imagecaptions).Givenatext-conditioneddiffusionmodel
g : T → I, we permute concepts in text space T, transform captions to image
space I with g, and use the generated images as a proxy for permutations in
image space.
Accurately estimating model feature importance via this approach requires
threetestableassumptions:(1)Permutableconcepts:wecanpermuteasetofrel-
evantconceptsinT;(2)Effectivegeneration:wecanobtainamappingg :T →I
suchthatf canaccuratelyclassifygeneratedinstances;(3)Independent Permu-
tation: while changing a concept for a set of instances, the other concepts in the
instances do not change. These assumptions require some algorithmic decisions
and data considerations that we discuss below and verify in our experiments.
3.1 Permutation Importance on Tabular Data
We begin by recounting how permutation importance is performed in tabular
data [5] to aid in describing our approach. For simplicity we focus on binary
classification, although permutation importance generalizes to multi-class clas-
sification and even regression. We assume: an input space T (e.g., Rd for d-
dimensional numerical tabular data); a classifier f :T →{0,1} that maps from
theinputspacetobinarydecisions;N labeledexamples{x ,y }N ;andalossL
i i i=1
evaluating performance (e.g., error). The reference performance of the classifier
on the unpermuted data is given by a= 1 (cid:80)N L(y ,f(x )) (Fig. 2).
N i=1 i i
In permutation importance, one permutes a single coordinate of the data
j for j = 1,...,d while holding the others fixed and measures the change in
performancerelativetotheoriginalmodelperformancea.Let{x¯j}N betheN
i i=1
examples with the jth coordinate permuted among the samples. One calculates
theperformanceoff onthepermutedtestset,asa = 1 (cid:80)N L(y ,f(x¯j)).The
j N i=1 i i
permutation importance of the jth coordinate for f is the difference between
the original accuracy and the accuracy while permuting j, or a − a . Given
jDEPICT: Diffusion-Enabled Permutation Importance 5
r: 0.92 (0.90 - 0.93) Model #1 Model #2 Model #3 Model #4
1.0 1.0
0.2 0.3
0.5 0.1 0.1 0.2 0.1 0.5
0.1
0.0 0.0 0.0 0.0 0.0 0.0
0.0 0.5 1.0
r: 0.08 (-0.00 - 0.16)
1.0 0.4 0.4 1.0 0.3 0.3
0.5 0.2 0.2 0.2 0.2 0.5
0.1 0.1
0.0 0.0 0.0 0.0 0.0 0.0
0.0 0.5 1.0
DEPICT GradCAM LIME Oracle
r: 0.73 (0.68 - 0.76)
1.0 0.75 0.6 0.75 1.0
0.5 0.50 0.4 0.5 0.50 0.5
0.25 0.2 0.25
0.0 0.00 0.0 0.0 0.00 0.0
0.0 0.5 1.0 BC GC RR GR RC BR GR RC RR BR GC BC BC GC BR GR RR RC BC GC GR RR RC BR
Feature Importance Concept Concept Concept Concept
Fig.3: Model feature importance across synthetic data models. We compare
the DEPICT ranking to GradCAM [32] and LIME [28]. Left: DEPICT has higher
correlationwiththestandardizedregressionweightscomparedtoGradCAMandLIME.
Right:rankinggeneratedfor4/100randomlychosenclassifiers.RC:redcircle;BC:blue
circle; GC: green circle; RR: red rectangle; BR: blue rectangle; GR: green rectangle.
the inherent randomness, this process is typically repeated many times and the
average importance value is used to rank the d variables.
Permutationimportanceisnotwithoutlimitation.Inparticular,highdegrees
ofcollinearityamonginputfeaturesmayleadtoincorrectbeliefsthataparticular
featureisnotrelevanttotheoutcomeorlabel [23,33].Thus,itsuseingenerating
hypotheses of associations is limited. However, we are primarily interested in
what the model is relying on and not the underlying relationships in the data
generating process. If there are two highly correlated features and the model is
only relying on one, permutation importance will correctly identify which one.
3.2 Permutation Importance on Image Data
Wenowextendpermutationimportancetoimages.Weassume:aspaceofimages
I; a classifier f : I → {0,1} mapping images to predictions; N labeled images
{x ,y }N ; and a performance metric L.
i i i=1
The crux of the method is a parallel concept text space T and functions for
movingbetweenT andI.Inparticular,weassumethereisaconcepttextspace
like scene image captions with D concepts (such as the presence of a chair) that
canbepermutedliketabulardataandturnedintotexteasily.Forsimplicity,we
also assume that we have corresponding concept labels {c }N for each input
i i=1
with each c ∈ T, where we can represent c ∈ {0,1}d, a d-dimensional binary
i i
vector indicating the presence of each concept. To move between the spaces,
we assume a generative model g : T → I that maps a concept vector to a
sampleimagematchingtheconcepts(Fig.2);wealsoassumeaconceptclassifier
h : I → T that can accurately detect whether a concept appears in an image.
For instance, g might be a diffusion model trained to map from a caption to an
TCIPED
MACdarG
EMIL
elcarO
elcarO
elcarO
egnahC
CORUA
UOI
UOI
Oracle
Oracle
Oracle6 S. Jabbour et al.
AUROC Top-K Accuracy AUROC Top-K Accuracy AUROC Top-K Accuracy
1 1 1 1 1 1
0 0 0 0 0 0
0 1 0 1 0 1 0 1 0 1 0 1
GradCAM GradCAM GradCAM GradCAM GradCAM GradCAM
1 1 1 1 1 1
0 0 0 0 0 0
0 1 0 1 0 1 0 1 0 1 0 1
LIME LIME LIME LIME LIME LIME
(a) Shapes (b) COCO-Primaryfeature (c) COCO-Mixedfeature
Fig.4: AUROC and top-k accuracy of methods across varying importance
thresholds. We plot DEPICT’s performance against GradCAM and LIME. Data-
points in the upper left half are DEPICT outperforming GradCAM and LIME, while
inthelowerhalfareDEPICTunderperforming.Acrossallthreesetsoftasks,DEPICT
outperformsbothGradCAMandLIMEintermsofAUROCandtop-kaccuracywhen
predicting important concepts across most thresholds.
image and h might be a classifier trained to recognize a set of concepts from an
image (e.g., if the image contains a couch).
Given the classifier, diffusion model, and concept classifier, we now set up
permutation importance for images. We start with the reference performance
on unpermuted generated data, a′ = 1 (cid:80)N L(y ,f(g(c ))). To test the impor-
N i=1 i i
tance of the jth concept, we permute the jth entry in the concept space across
text instances and map the text to new images, creating a new test set g(cj) for
eachpermutedconceptj.WerepeatthisprocessP timestogenerateadistribu-
tion of observed differences in performance between the original generated test
setandthepermutedtestset,a′−a ,wherea = 1 (cid:80)N L(y ,f(g(cj))).Large
j j N i=1 i i
performance drops indicate the model relied on the concept, while no drop in
performance suggests the concept is unimportant to the model and this partic-
ular dataset. We can then rank concepts by their average performance drop.
Importantly, the approach assumes effective generation, meaning that the
classifier f performs similarly on generated images from g conditioned on the
original dataset’s captions as it does the real images. To test whether this as-
sumption holds we do two tests. First, we measure the difference between a and
a′,wherea= 1 (cid:80)N L(y ,f(x )).Ifthedifferenceislarge,thenthisassumption
N i=1 i i
doesnothold.Ifthedifferenceissmall,welookformoregranulardifferencesby
computing concept classifier performance between the original images and the
generatedimages,i.e., 1 (cid:80)N L (y ,h(x ))− 1 (cid:80)N L (y ,h(g(c ))),whereL
N i=1 j i i N i=1 j i i j
istheconceptclassifierperformanceinpredictingconceptj.Adropineithertar-
get model performance overall or one concept via the concept classifier suggests
that the assumption of effective generation does not hold.
Finally,DEPICTassumesindependentpermutation.Ifchangingoneconcept
alsochangesotherconceptsintheimagespace,wecannottrustthepermutation
TCIPED
TCIPED
TCIPED
TCIPED
TCIPED
TCIPEDDEPICT: Diffusion-Enabled Permutation Importance 7
Concept
bed person couch dining table tv
Fig.5: Generated Images. Examples of generated images where each concept is
(upper) or is not (lower) in the caption used to generate the image. The generated
images reflect whether or not the concept is included in the caption.
importance results. Thus, after permuting concept j, we calculate the concept
classifier performance on the generated images before and after permutation.
For all non-permuted concepts k ̸= j, we expect concept classifier performance
to hold, and for permuted concept j, we expect performance to drop.
4 Experiments & Results
To validate DEPICT, we first consider a synthetic setting where generation is
easy, followed by two real-world datasets: COCO [19] and MIMIC-CXR [14,16].
4.1 Synthetic Dataset
In our synthetic dataset, images can contain any combination of six concepts
that each consist of a distinct colored geometric shape: {red, green blue} ×
{circle, rectangle}. Each image is generated according to an indicator variable
s∈{0,1}6 indicating whether each shape is present. s is drawn per-component
from a Bernoulli distribution with p = 0.5. We generate the image X from s
i
by placing shapes randomly, such that no two shapes overlap. We construct a
caption for each image by with descriptions of each shape joined by a comma
(e.g., a c-colored circle at (x,y) with radius r is described as “c circle (x,y) r”)
(full details are in supplementary 8).
Given images, we generate tasks and corresponding labels. Each task is de-
fined by a weight vector w ∈ R6 over the six indicator variables where each
component is drawn uniformly over [0,1]. Given the weight vector, the score of
an image with indicator vector s is given by w⊤s. We define a binary classifica-
tion task by thresholding image scores at the median of the dataset.
tpecnoc
tpecnoc
noitpac
ni
noitpac
ni
ton8 S. Jabbour et al.
r: 0.73 (0.59 - 0.83) Model # 1 Model # 2 Model # 3
1.0 0.4 0.3 0.2 1.0
0.3
0.2 0.5 0.2 0.1 0.5
0.1 0.1
0.0 0.0 0.0 0.0 0.0
0.0 0.5 1.0
r: -0.10 (-0.23 - 0.03)
1.0 0.8 0.8 0.8 1.0
0.6 0.6 0.6
0.5 0.4 0.4 0.4 0.5
0.2 0.2 0.2
0.0 0.0 0.0 0.0 0.0
0.0 0.5 1.0
r: 0.15 (-0.02 - 0.30)
1.0 0.6 0.4 0.6 1.0
0.4 0.4
0.5 0.2 0.2 0.2 0.5
0.0 0.0 0.0 0.0 0.0
0.0 0.5 1.0
Feature Importance
DEPICT LIME
GradCAM Oracle Concept Concept Concept
Fig.6:Modelfeatureimportanceacrossprimary feature models.Wecompare
the ranking produced by DEPICT, GradCAM [32] and LIME [28] to the oracle gen-
erated by permuting concepts at the bottleneck. Left: DEPICT has higher correlation
with the oracle compared to LIME and GradCAM. Right: ranking generated for 3 of
the 15 classifiers. DEPICT detects the primary concept in all classifiers as well as the
low importance of the non-primary concepts, while GradCAM and LIME do not.
Target models. We aim to generate concept-based explanations for a target
modelthatpredictsy .Weuseaconceptbottleneckmodel[18]forfullcontrol:we
i
first predict all concepts {c }N , by training a model to predict shape presence
i i=1
asavectorcˆ .Thetargetmodelisdefinedasaweightedsumofcˆ viatheweights
i i
generatedabove,yˆ =wTcˆ .Thisway,weknowtheexactmodelmechanismand
i i
consider the weight vector w as the true model feature importance.
Diffusionmodel.Wefine-tuneStableDiffusion[29]on50,000syntheticimages,
with captions describing the presence and location of each shape separated by
commas (full details are in supplementary 8).
Using DEPICT. To generate concept rankings, we permute each concept in
the text space 500 times. For each permutation, we generate a dataset using the
diffusion model and pass the images through the target model, measuring the
AUROC drop compared to the unpermuted generated dataset. Then, the mean
AUROC drop across all 500 permutations is used to rank concepts.
Oracle model feature ranking. We calculate standardized regression coeffi-
cientsastheoraclerankingoffeaturesbymultiplyingeachmodel’sweightvector
wbythestandarddeviationoftheconceptpredictionsontherealimages[6,27].
Wealsocomparetoanoraclethatpermutesconceptpredictionsoftherealdata
at the bottleneck of the network in supplementary 8. We note that DEPICT
does not assume access to model parameters needed to calculate such oracles.
Baselines. We compare the ranking produced by DEPICT to a ranking pro-
ducedbyGradCAM[32]andLIME[28],twocommonlyusedexplanationmeth-
ods for image-based classifiers. Since GradCAM and LIME generate instance-
TCIPED
MACdarG
EMIL
elcarO
elcarO
elcarO
egnahC
CORUA
UOI
UOI
nosrep puc lwob enohp
llec
riahc knis elbat
gninid
elttob vt deb potpal etomer koob nevo hcuoc vt potpal enohp
llec
riahc hcuoc lwob etomer elbat
gninid
elttob koob knis nosrep puc nevo deb potpal enohp
llec
lwob etomer nevo elbat
gninid
elttob koob vt hcuoc riahc knis puc deb nosrep
Oracle
Oracle
OracleDEPICT: Diffusion-Enabled Permutation Importance 9
Primary feature model relies on: person AUROC Primary feature model relies on: tv AUROC
0.54 0.68
(0.51–0.57) (0.64-0.71)
0.92 0.94
(0.91-0.94) (0.93-0.96)
0.91 0.94
(0.89-0.93) (0.92-0.96)
0.91 0.94
(0.89-0.93) (0.92-0.95)
Fig.7: Permutation examples. We show permutation examples for two primary
feature models that rely on either “person” or “tv” when predicting home or hotel.
Whenpermutingthemostimportantconcept,modelperformanceislow,whereaswhen
permutingconceptthatthemodeldoesnotrelyon,modelperformancedoesnotdrop.
basedexplanations,weextendtheseapproachestogeneratearankingbyrelying
onconceptannotationsandtheircorrespondingmask.Becausewehaveaccessto
the image generation process of the synthetic dataset, we generate an concept-
levelmaskforallconceptsineachimage.Then,foreachimage,wecalculatethe
intersection-over-union (IOU) between each concept-level mask and the Grad-
CAMorLIMEmaskgeneratedbytheclassifier(fulldetailsareinsupplementary
8).Then,werankconceptsbytheirmeanIOUacrosstheentiretestset.Wenote
that computing this ranking for GradCAM and LIME requires access to image-
level masks as well as the model parameters, while DEPICT does not. Because
GradCAM and LIME are generated via the real images, we only generate one
importance value for each concept in each image, compared to a distribution of
model feature importances generated by DEPICT.
Evaluation & Results. Evaluation consists of two parts. We quantitatively
and qualitatively compare to the oracle and baselines, and we validate our as-
sumptions of effective generation and independent permutation.
Model feature ranking evaluation. We plot the DEPICT, LIME and Grad-
CAM generated model feature importances against the oracle (standardized
weightvectorw)acrossall100modelsandmeasurethePearson’scorrelation[7],
with 95% bootstrapped confidence intervals. Methods that correctly rank con-
cepts will have high correlation with the oracle. We also show boxplots of each
method’s feature importances for a randomly chosen subset of models and com-
paretotheoracleranking.Additionally,weconsidereachmethod’spermutation
importance as a prediction task for which concepts are predicted to be impor-
tant. Welabel each concept as“important” or “notimportant” bybinarizing the
oracle model feature importances across all weight thresholds k, and calculate
the AUROC between the generated model feature importance and binarized
feature importance. Finally, we calculate the agreement in the top-k features
between the ground truth weights and each method. We consider k ∈ [1,6].
Results. DEPICT has the highest correlation with the oracle feature weights
of each model (0.92 [95%CI 0.90-0.93]), followed by LIME (0.73 [95%CI 0.68-10 S. Jabbour et al.
r: 0.35 (-0.05 - 0.66) workplace home or hotel cultural
1.0 0.06 1.0
0.075 0.02 0.04 0.050
0.5 0.5 0.00 0.02 0.025
0.000
0.0 0.00 0.0
0.0 0.5 1.0
r: 0.15 (-0.21 - 0.50)
1.0 1.00 1.00 1.00 1.0
0.75 0.75 0.75
0.5 0.50 0.50 0.50 0.5
0.25 0.25 0.25
0.0 0.00 0.00 0.00 0.0
0.0 0.5 1.0
r: 0.27 (0.01 - 0.52)
1.0 0.4 1.0
0.6 0.3 0.4 0.5 0.4 0.2 0.2 0.5
0.2 0.1
0.0 0.0 0.0 0.0 0.0
0.0 0.5 1.0
Feature Importance
DEPICT LIME
GradCAM Oracle Concept Concept Concept
Fig.8: Model feature importance across mixed feature models. We compare
therankingproducedbyDEPICT,GradCAM[32]andLIME[28]totheoraclegener-
atedbypermutingconceptsatthebottleneck.Left:DEPICThasthehighestcorrelation
with the oracle model feature importance. Right: We show the ranking generated by
DEPICT, GradCAM and LIME for three of the mixed feature models.
0.76]), and GradCAM (0.08 [95%CI 0.00-0.16]) (Fig. 3). Looking at individual
models, while both DEPICT and LIME produce feature importance rankings
which are highly correlated with the oracle, DEPICT better aligns with the
magnitude ofthegroundtruthfeatureimportances.Furthermore,DEPICTper-
formsonparorbetterthanbothGradCAMandLIMEintermsofAUROCand
top-kaccuracyacrossallweightthresholds(Fig.4a).Ifconsideringtheoracleas
permutingconceptsatthebottleneck,DEPICThasasignificantlyhighercorrela-
tion with the oracle (0.98 [0.97-0.98]) compared to GradCAM (0.07 [-0.01-0.15])
and LIME (0.72 [0.68-0.76]) (supplementary Fig. 11).
Validationofassumptions.Tocheckforeffectivegeneration,wemeasureAU-
ROCbetweenrealandgeneratedimagesonthetargetandconceptclassifier.To
check for independent permutation, we rely on a concept classifier that predicts
thepresenceofthesixshapesthatwearepermuting(supplementary8).Foreach
concept that is permuted across images (e.g., red circle), the concept classifier
should perform worse in classifying the permuted concept, while still classifying
the other concepts well.
Results. In terms of effective generation, the differences in AUROC between
real and generated images for all models was <= 0.12 for both the target mod-
els and concept classifiers (supplementary Tables 2, 3). Given that all AUROC
values were above 0.88, we consider this effective generation for this task. Fur-
thermore, each time a concept is permuted, the concept classifier is no longer
abletoclassifythespecificconcept,whilestillclassifyingtheotherconceptswell
(supplementaryFig.12).Thisvalidatesindependentpermutationforeachofthe
concepts.
TCIPED
MACdarG
EMIL
elcarO
elcarO
elcarO
egnahC
CORUA
UOI
UOI
elbat
gninid
lwob potpal hcuoc enohp
llec
elttob knis riahc vt koob etomer puc deb nosrep nevo knis elbat
gninid
lwob enohp
llec
riahc vt deb elttob koob potpal etomer hcuoc nevo puc nosrep elttob koob nosrep deb vt puc knis enohp
llec
elbat
gninid
lwob potpal etomer nevo riahc hcuoc
Oracle
Oracle
OracleDEPICT: Diffusion-Enabled Permutation Importance 11
4.2 Real Dataset
We evaluate DEPICT’s ability to generate concept-based explanations of image
classifiers on COCO [19]. We consider two settings reflecting different levels of
difficulty in ranking concepts, showing that DEPICT generates better rankings
compared to baselines.
Target models.Weconsidertwosetsofsceneclassifiers.Foralltargetmodels,
welearnaconceptbottleneckg(x)∈Rc wherecis15conceptsthattheclassifier
may rely on (see supplementary 9 for full list). Then, we learn a linear classifier
f(g(x))parameterizedbyw tomapconceptstoafinalprediction.Wetraintwo
sets of target classifiers:
Primary feature models. We first train binary tasks to classify images as
{home or hotel} or {not}. By design, these models each rely heavily on one of
15 concepts in the image: we resampled the training data such that there was a
1:1 correlation between a concept in the image (e.g., person or couch) and the
outcome, totalling 15 classifiers (full list in supplementary 9).
Mixed feature models. We also trained six scene classification tasks, where
a model classifies if an image is one of six scenes: (1) shopping and dining,
(2)workplace,(3)home or hotel,(4)transportation,(5)cultural,and(6)
sports and leisure. We did not resample the training data to encourage the
model to rely on specific concepts, but instead used the entire training set to let
the model rely on any set of concepts (see supplementary 9 for details).
Diffusion model.Wefine-tuneStableDiffusion[30]onCOCO[19]togenerate
images for our task (examples in Fig. 5). We use COCO concept annotations as
captions. E.g., if an image contains 2 persons and 1 couch, the corresponding
caption is “2 person, 1 couch.” We generate a scene label for each image using a
network trained on the Places 365 dataset [43] (full details in supplementary 9).
Using DEPICT. To generate model feature importances with DEPICT, we
permuteeachconceptinthetext-space25times.Foreachpermutation,wegen-
erateadatasetwiththediffusionmodelandpasstheseimagesthroughthetarget
model.TheAUROCdropcomparedtothedatasetgeneratedwithnon-permuted
text yields a distribution of model feature importance values per concept.
Oracle model feature ranking. We again calculate standardized regression
coefficients using the learned weight vector w. We also calculate an additional
oracle by permuting concepts at the bottleneck in the supplementary.
Baselines.WecompareDEPICTtoGradCAM[32]andLIME[28].Wemeasure
the IOU between the GradCAM and LIME masks using each object annotation
mask for each image in COCO (full details are in supplementary 9).
Evaluation & Results.WequantitativelyandqualitativelyevaluateDEPICT
on COCO just as we did in the synthetic setting, as well as validate the as-
sumptions of effective generation and independent permutation using a concept
classifiertrainedtopredicttheconceptsinCOCO(fulldetailsinsupplementary
9). Furthermore, for quantitative evaluation, we consider k∈ [1,15], asthere are
15 concepts to threshold over in the COCO models.
Primary feature model evaluation. DEPICT has higher correlation with the
oracle (0.73 [0.59-0.83]) compared to GradCAM (-0.10 [-0.23-0.03]) and LIME12 S. Jabbour et al.
Age < 60 Age > 60 BMI < 30 BMI > 30 Sex = F Sex = M
Fig.9: Generated X-rays. We show generated X-rays with patient age, body mass
index(BMI),andsexpermuted.Whiledifficulttopermutesuchconceptsinpixelspace,
a diffusion model can map permutations from text (e.g., “age>60") to pixel space.
(0.15 [-0.02-0.30]) (Fig. 6). We show rankings for three of 15 randomly chosen
classifiers in Fig. 6 as well as model performance on permuted datasets in Fig.
7. DEPICT also outperforms both GradCAM and LIME in terms of AUROC
andtop-kaccuracyacrossmostthresholds(Fig.4b).Ifconsideringtheoracleas
permutingconceptsatthebottleneck,DEPICThasahighercorrelationwiththe
oracle (0.90 [0.83-0.95]) compared to GradCAM (-0.05 [-0.17-0.10]) and LIME
(0.19 [0.03-0.35]) (supplementary Fig. 13).
Mixed feature model evaluation. DEPICT has higher correlation with the
oracle feature importance (0.35 [-0.05-0.66]) compared to GradCAM (0.15 [-
0.21-0.50]) and LIME (0.27 [0.01-0.52]) (Fig. 8). For individual scene classifiers,
DEPICTgeneratesmorereasonablerankingscomparedtoGradCAMandLIME.
DEPICT also outperforms both GradCAM and LIME in terms of AUROC and
top-k accuracy across most thresholds (Fig. 4c). If considering the oracle as
permutingconceptsatthebottleneck,DEPICThasahighercorrelationwiththe
oracle (0.49 [-0.01-0.79]) compared to GradCAM (0.17 [-0.18-0.50]) and LIME
(0.30 [0.04-0.53]) (supplementary Fig. 14).
Validationofassumptions.Fortheprimaryfeaturemodels,DEPICTachieves
botheffectivegenerationintargetmodelsandconceptclassifiers(<0.10AUROC
change between real and generated images) (supplementary Tables 4, 5) and in-
dependent permutation (minimal changes in concept classifier performance for
non-permutedconcepts)(supplementaryFig.15).Forthemixedfeaturemodels,
DEPICT achieves effective generation for three of the six scene classifiers (sup-
plementary Tables 6, 7) and independent permutation on all classifiers (supple-
mentary Fig. 16).
4.3 DEPICT in Practice: A Case Study in Healthcare
Until now, we have applied DEPICT to datasets in which all concepts that
a model might rely on can be permuted. However, depending on the diffusion
modeland/orourknowledgeofimportantconcepts,wemayonlyhavetheability
to permute on a subset of concepts on which the model relies. Here, we discussDEPICT: Diffusion-Enabled Permutation Importance 13
Table 1: DEPICT applied to MIMIC-CXR. We show AUROC and 95% boot-
strappedconfidenceintervalsonrealandgeneratedimagesforthemodelsthatrelyon
patient age, BMI, or sex. When permuting the concepts, model performance signifi-
cantly drops, showing that the models rely on each of the concepts in some way.
BMI Age Sex
Real Images 0.98 (0.97 - 0.98)0.89 (0.87 - 0.91)1.00 (1.00 - 1.00)
Generated Images0.97 (0.96 - 0.97)0.85 (0.83 - 0.87)1.00 (0.99 - 1.00)
DEPICT 0.70 (0.70 - 0.71)0.59 (0.59 - 0.59)0.53 (0.53 - 0.54)
how DEPICT can apply in such scenarios. Rather than generating a ranking of
all concepts, we ask the question: does the model rely on a specific concept?
We consider MIMIC-CXR [15,16], a dataset of paired X-rays and radiology
reports.Weconsiderthetaskofclassifyingpneumoniafromthepatient’schestX-
ray. We use patient demographics as concepts (Fig. 9): body mass index (BMI)
> 30, age > 60, sex = Female, and prepend them to the patient’s radiology
report, e.g., “Age: 1, BMI: 0, Sex: 1, Findings:...”, where “Findings:”
is the beginning of the report. The presence of the entirety of the radiology
report text allows the diffusion model to generate high quality images. Since
concept masks are not available, we cannot apply GradCAM and LIME.
Target models. We train three target models on MIMIC-CXR to predict the
presenceof pneumoniaonthechestX-ray.Bydesign,thesemodelsweretrained
such that they heavily rely on either the patient’s age, body mass index (BMI)
orsex.Toachievethis,weresampledthetrainingdatasuchthattherewasa1:1
correlation between each concept and the outcome of pneumonia. Furthermore,
the target model was a concept bottleneck constrained to 17 concepts: 13 radi-
ological findings on the chest X-rays, along with patient age, BMI, and sex (full
details in supplementary 10).
Diffusion model. We fine-tune Stable Diffusion [29] on MIMIC-CXR X-rays
and radiology reports prepended with concepts (details in supplementary 10).
Using DEPICT. To generate feature importances, we permute each concept
25 times. While permuting only a few concepts per classifier does not generate
a full ranking, a significant model performance drop on the permuted test set
reflects that the model relies on the concept in some way. We discuss validation
ofassumptionswhennotallconceptscanbepermutedinthesupplementary10.
Results. The difference in classification AUROC between real and generated
chest X-rays for all three target models as well as concept classifiers on the per-
mutableconceptsrangesfrom0.0to0.04(supplementaryTables8,9),suggesting
effective generation. For independent permutation, we observe some changes in
concept classifier performance after permutation when classifying concepts such
aslungopacityandlunglesion(supplementaryFig.18).Thus,onemustproceed
with caution about interpreting the importance of BMI, age, and sex, as they
may be confounded by changes to other concepts such as lung opacity or lung
lesion.14 S. Jabbour et al.
Forallthreetargetmodels,permutingpatientBMI,age,andsexresultsina
significantdropinmodelperformance(BMI:0.70[0.70-0.71]vs.0.97[0.96-0.97];
age: 0.59 [0.59-0.59] vs. 0.85 [0.83-0.87]; sex: 0.53 [0.53-0.54] vs. 1.00 [0.99-1.00])
(Table 1). We can conclude that the models rely on these concepts in some
way. DEPICT could allow model developers to probe models pre-deployment to
potentiallycatchwhenmodelsarerelyingonaconceptthattheyshouldnotbe.
5 Limitations
DEPICT’ssuccessreliesonthediffusionmodel’sabilitytopermuteconceptsef-
fectively and independently. In the experiments involving the synthetic dataset,
DEPICT’srankingwashighlycorrelatedwiththerankinggeneratedbydirectly
permuting concepts at the bottleneck (supplementary Fig. 11). Subsequently,
DEPICT’s ranking was also highly correlated with the ranking of the standard-
ized regressionweights(Fig. 3). Onthe otherhand, asDEPICT’s ranking’scor-
relation with the ranking generated by permuting at the bottleneck decreased
(supplementary Fig. 13, 14), so did its correlation with the logistic regression
weights (Fig. 6, 8).
Furthermore, when the diffusion model is conditioned on both permutable
andnon-permutabletext(e.g.,asinSection4.3),thediffusionmodelcouldstrug-
gle to permute concepts in the image space if there are mentions of permutable
concepts in the non-permutable text space (e.g., if one is trying to permute
the patient age, and the radiology report mentions the original age of the pa-
tient). While the concept classifier is used to ensure that the concept of interest
has been indeed permuted, this still limits the applicability of DEPICT. Mov-
ing forward, DEPICT’s success relies on good generative models that can map
permuted concepts in the text space to the image space effectively.
6 Conclusion
Understanding the reason behind AI model predictions can aid the safe de-
ployment of AI. To date, image-based model explanations have been limited to
instance-based explanations the pixel space [28,32], which are difficult to in-
terpret [1,2,12]. Instead, DEPICT generates image-based explanations at the
dataset-level in the concept space. While directly permuting concepts in pixel
space is difficult, DEPICT permutes concepts in the text space and then gen-
erates new images reflecting the permutations via text-conditioned diffusion.
DEPICT relies on a text-conditioned diffusion model that effectively generates
images and independently permutes concepts across images. While we have in-
cluded checks to verify these assumptions, we cannot guarantee that such a
diffusion model is available. However, given the rapid progress of the field, we
expect that the availability or the ability to train such models will improve, in-
creasing the feasibility of DEPICT.DEPICT: Diffusion-Enabled Permutation Importance 15
Acknowledgements
We thank Donna Tjandra, Fahad Kamran, Jung Min Lee, Meera Krishnamoor-
thy,MichaelIto,MohamedElBanani,ShengpuTang,StephanieShepard,Tren-
tonChangandWinstonChenfortheirhelpfulconversationsandfeedback.This
work was supported by grant R01 HL158626 from the National Heart, Lung,
and Blood Institute (NHLBI).
References
1. Adebayo, J., Muelly, M., Abelson, H., Kim, B.: Post hoc explanations may be
ineffectivefordetectingunknownspuriouscorrelation.In:Internationalconference
on learning representations (2021)
2. Adebayo,J.,Muelly,M.,Liccardi,I.,Kim,B.:Debuggingtestsformodelexplana-
tions. arXiv preprint arXiv:2011.05429 (2020)
3. Alsentzer, E., Murphy, J.R., Boag, W., Weng, W.H., Jin, D., Naumann, T.,
McDermott, M.: Publicly available clinical bert embeddings. arXiv preprint
arXiv:1904.03323 (2019)
4. Altmann, A., Toloşi, L., Sander, O., Lengauer, T.: Permutation importance: a
corrected feature importance measure. Bioinformatics 26(10), 1340–1347 (2010)
5. Breiman, L.: Random forests. Machine learning 45, 5–32 (2001)
6. Bring, J.: How to standardize regression coefficients. The American Statistician
48(3), 209–213 (1994)
7. Cohen, I., Huang, Y., Chen, J., Benesty, J., Benesty, J., Chen, J., Huang, Y.,
Cohen, I.: Pearson correlation coefficient. Noise reduction in speech processing
pp. 1–4 (2009)
8. DeGrave,A.J.,Cai,Z.R.,Janizek,J.D.,Daneshjou,R.,Lee,S.I.:Dissectionofmed-
icalaireasoningprocessesviaphysicianandgenerative-aicollaboration.medRxiv
(2023)
9. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: 2009 IEEE conference on computer vision
and pattern recognition. pp. 248–255. Ieee (2009)
10. Fisher, A., Rudin, C., Dominici, F.: All models are wrong, but many are useful:
Learning a variable’s importance by studying an entire class of prediction models
simultaneously. J. Mach. Learn. Res. 20(177), 1–81 (2019)
11. Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected
convolutionalnetworks.In:ProceedingsoftheIEEEconferenceoncomputervision
and pattern recognition. pp. 4700–4708 (2017)
12. Jabbour, S., Fouhey, D., Kazerooni, E., Sjoding, M.W., Wiens, J.: Deep learning
appliedtochestx-rays:Exploitingandpreventingshortcuts.In:MachineLearning
for Healthcare Conference. pp. 750–782. PMLR (2020)
13. Jabbour, S., Fouhey, D., Shepard, S., Valley, T.S., Kazerooni, E.A., Banovic, N.,
Wiens, J., Sjoding, M.W.: Measuring the impact of ai in the diagnosis of hos-
pitalized patients: a randomized clinical vignette survey study. JAMA 330(23),
2275–2284 (2023)
14. Johnson, A., Pollard, T., Mark, R., Berkowitz, S., Horng, S.: Mimic-cxr database
(version 2.0. 0). PhysioNet 10, C2JT1Q (2019)16 S. Jabbour et al.
15. Johnson, A., Bulgarelli, L., Pollard, T., Horng, S., Celi, L.A., Mark,
R.: Mimic-iv. PhysioNet. Available online at: https://physionet.
org/content/mimiciv/1.0/(accessed August 23, 2021) (2020)
16. Johnson, A.E., Pollard, T.J., Berkowitz, S.J., Greenbaum, N.R., Lungren, M.P.,
Deng, C.y., Mark, R.G., Horng, S.: Mimic-cxr, a de-identified publicly available
database of chest radiographs with free-text reports. Scientific data 6(1), 317
(2019)
17. Kim,G.,Kwon,T.,Ye,J.C.:Diffusionclip:Text-guideddiffusionmodelsforrobust
image manipulation. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 2426–2435 (2022)
18. Koh, P.W., Nguyen, T., Tang, Y.S., Mussmann, S., Pierson, E., Kim, B., Liang,
P.: Concept bottleneck models. In: International conference on machine learning.
pp. 5338–5348. PMLR (2020)
19. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision–
ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12,
2014, Proceedings, Part V 13. pp. 740–755. Springer (2014)
20. Losch, M., Fritz, M., Schiele, B.: Interpretability beyond classification output: Se-
mantic bottleneck networks. arXiv preprint arXiv:1907.10882 (2019)
21. Lundberg, S.M., Lee, S.I.: A unified approach to interpreting model predictions.
Advances in neural information processing systems 30 (2017)
22. Morales Rodríguez, D., Pegalajar Cuellar, M., Morales, D.P.: On the fusion of
soft-decision-trees and concept-based models. Available at SSRN 4402768 (2023)
23. Nicodemus, K.K., Malley, J.D.: Predictor correlation impacts machine learning
algorithms: implications for genomic studies. Bioinformatics 25(15), 1884–1890
(2009)
24. Oikarinen, T., Das, S., Nguyen, L.M., Weng, T.W.: Label-free concept bottleneck
models. arXiv preprint arXiv:2304.06129 (2023)
25. Prabhu,V.,Yenamandra,S.,Chattopadhyay,P.,Hoffman,J.:Lance:Stress-testing
visualmodelsbygeneratinglanguage-guidedcounterfactualimages.arXivpreprint
arXiv:2305.19164 (2023)
26. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
1(2), 3 (2022)
27. Rao,C.R.,Miller,J.P.,Rao,D.:Essentialstatisticalmethodsformedicalstatistics.
North Holland Amsterdam, The Netherlands (2011)
28. Ribeiro, M.T., Singh, S., Guestrin, C.: "why should i trust you?" explaining the
predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD interna-
tional conference on knowledge discovery and data mining. pp. 1135–1144 (2016)
29. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 10684–10695 (2022)
30. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR). pp. 10684–
10695 (June 2022)
31. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. Advances in Neural
Information Processing Systems 35, 36479–36494 (2022)DEPICT: Diffusion-Enabled Permutation Importance 17
32. Selvaraju,R.R.,Cogswell,M.,Das,A.,Vedantam,R.,Parikh,D.,Batra,D.:Grad-
cam: Visual explanations from deep networks via gradient-based localization. In:
ProceedingsoftheIEEEinternationalconferenceoncomputervision.pp.618–626
(2017)
33. Strobl,C.,Boulesteix,A.L.,Kneib,T.,Augustin,T.,Zeileis,A.:Conditionalvari-
able importance for random forests. BMC bioinformatics 9, 1–11 (2008)
34. Tibshirani, R.: Regression shrinkage and selection via the lasso. Journal of the
Royal Statistical Society Series B: Statistical Methodology 58(1), 267–288 (1996)
35. Tjoa, E., Guan, C.: A survey on explainable artificial intelligence (xai): Toward
medical xai. IEEE transactions on neural networks and learning systems 32(11),
4793–4813 (2020)
36. Wang, B., Li, L., Nakashima, Y., Nagahara, H.: Learning bottleneck concepts in
image classification. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 10962–10971 (2023)
37. Wei, P., Lu, Z., Song, J.: Variable importance analysis: A comprehensive review.
Reliability Engineering & System Safety 142, 399–432 (2015)
38. Wong,L.J.,McPherson,S.:Explainableneuralnetwork-basedmodulationclassifi-
cationviaconceptbottleneckmodels.In:2021IEEE11thAnnualComputingand
CommunicationWorkshopandConference(CCWC).pp.0191–0196.IEEE(2021)
39. Xiao,J.,Hays,J.,Ehinger,K.A.,Oliva,A.,Torralba,A.:Sundatabase:Large-scale
scene recognition from abbey to zoo. In: 2010 IEEE computer society conference
on computer vision and pattern recognition. pp. 3485–3492. IEEE (2010)
40. Yang, Y., Panagopoulou, A., Zhou, S., Jin, D., Callison-Burch, C., Yatskar, M.:
Languageinabottle:Languagemodelguidedconceptbottlenecksforinterpretable
image classification. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 19187–19197 (2023)
41. Yuksekgonul, M., Wang, M., Zou, J.: Post-hoc concept bottleneck models. arXiv
preprint arXiv:2205.15480 (2022)
42. Zhou,B.,Khosla,A.,Lapedriza,A.,Oliva,A.,Torralba,A.:Learningdeepfeatures
fordiscriminativelocalization.In:ProceedingsoftheIEEEconferenceoncomputer
vision and pattern recognition. pp. 2921–2929 (2016)
43. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: A 10 million
imagedatabaseforscenerecognition.IEEETransactionsonPatternAnalysisand
Machine Intelligence (2017)
44. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation
using cycle-consistent adversarial networks. In: Proceedings of the IEEE interna-
tional conference on computer vision. pp. 2223–2232 (2017)18 S. Jabbour et al.
7 Supplementary Materials Overview
Thissupplementarymaterialprovidesadditionaldetailsofthepaperalongwith
supplementaryresultsthatwereomittedfromthemainpaperduetospacecon-
straints. In Section 8 we present details and additional supplementary results of
thesyntheticdatasetexperiments.InSection9,wepresentdetailsandadditional
supplementary results of the real dataset (COCO [19]) experiments. Finally, in
Section 10, we present details and additional supplementary results of the case
study in healthcare (MIMIC-CXR [14,16]).
8 Synthetic Validation
8.1 Experiments
Dataset. Each image in the dataset is described by a set of concepts describing
distinctcoloredgeometricshapes:{red,greenblue}×{circle,rectangle}.Given
thevectorofindicatorvariabless ∈R6,weconstructtheimageX byrandomly
i i
placing each of the shapes in the image such that no two shapes overlap. The
captionfortheimageisthenastringdescribingeachoftheshapesintheimage,
separated by a comma. For instance, an image containing a red-colored circle
of radius 4 centered at (5,10) and a blue-colored rectangle with the top-left
corner at (20,30) and bottom-right corner at (50,60) would have the caption
“redcircle4(5,10),bluerectangle((20,30)(50,60))".Weshowexamplesofreal
and generated images of the synthetic shapes dataset in Fig. 10. We note that,
while the diffusion model does not generate the correct locations for the shapes,
thisdoesnotaffectdownstreamclassificationresultswhichdonotrelyonshape
locations.
Diffusion Model. A diffusion model initialized on Stable Diffusion [30] was
fine tuned for 105000 iterations on 107,000 images with a batch size of 16 at a
256x256 resolution and a learning rate of 1.0e-4. We fine-tuned only the U-Net
and text-encoder of the model.
Concept Classifier. The concept classifier g was a CNN with 5 layers, each
consisting of a convolution, batch norm, ReLU, and max pooling followed by a
3-layer multilayer perceptron that made six predictions for the presence of the
six shapes. The model was trained on 50,000 images for 15 epochs.
Baselines. We generated Grad-CAM [32] and LIME [28] explanations for the
predictedclassofeachimage.Theclasspredictionwasdeterminedbythreshold-
ing model predictions that maximized the true positive rate while minimizing
the false positive rate across the test set. Each GradCAM heatmap was first
converted to a binary mask by thresholding at the lowest non-zero value of the
Grad-CAMheatmap.5featureswereusedtogenerateeachLIMEmask.Forev-
eryshapeintheimage,wecalculatedtheintersectionoverunion(IOU)between
the shape and the explanation. Finally, we ranked shapes by their mean IOU
across the entire test set.DEPICT: Diffusion-Enabled Permutation Importance 19
Caption Real Generated
"green circle 7
(110,16) blue circle
23 (44,59) green
rectangle ((87, 70),
(175, 210)) blue
rectangle ((38,
104), (55, 183))"
"blue circle 27
(227,77) red
rectangle ((9, 58),
(112, 167)) green
rectangle ((126,
106), (178, 237))"
"red rectangle ((123,
22), (248, 87))"
Fig.10: Comparison between real and generated images for synthetic
dataset.Wecomparerealandgeneratedimagesfromthediffusionmodelconditioned
on the original captions. We find that the generated images look realistic and reflect
the shapes present in the captions.
Table 2: Effective generation validation for synthetic dataset models. We
reportAUROC(median,IQR)onbothrealandgeneratedimagesfor100targetmodels
on the synthetic dataset.
AUROC(median,IQR)
RealImages 0.99(0.98-1.0)
GeneratedImages 0.91(0.84-0.94)20 S. Jabbour et al.
Table 3: Effective generation validation for synthetic dataset concept clas-
sifier. We show AUROC on both real and generated images for the concept classifier
on the synthetic dataset across all six shapes. The concept classifier is able to detect
all six shapes from the generated images with high accuracy.
red circlegreen circleblue circlered squaregreen squareblue square
Real Images 1.00 0.99 1.00 1.00 1.00 1.00
Generated Images 0.97 0.95 0.88 0.96 0.95 0.93
r: 0.98 (0.97 - 0.98) Model #1 Model #2 Model #3 Model #4
1.0 1.0
0.2 0.3
0.5 0.1 0.1 0.2 0.1 0.5
0.1
0.0 0.0 0.0 0.0 0.0 0.0
0.0 0.5 1.0
r: 0.07 (-0.01 - 0.15)
1.0 0.4 0.4 1.0 0.3 0.3
0.5 0.2 0.2 0.2 0.2 0.5
0.1 0.1
0.0 0.0 0.0 0.0 0.0 0.0
0.0 0.5 1.0
DEPICT GradCAM LIME Oracle
r: 0.72 (0.68 - 0.76)
1.0 0.75 0.6 0.75 1.0
0.5 0.50 0.4 0.5 0.50 0.5
0.25 0.2 0.25
0.0 0.00 0.0 0.0 0.00 0.0
0.0 0.5 1.0 BC GC RR GR RC BR GR RC RR BR GC BC BC GC BR GR RR RC BC GC GR RC RR BR
Feature Importance Concept Concept Concept Concept
Fig.11: Model feature importance across synthetic data models with the
oracle generated by permuting concepts at the bottleneck. We compare the
DEPICT ranking to GradCAM [32] and LIME [28]. Left: DEPICT has higher cor-
relation with the standardized regression weights compared to GradCAM and LIME.
Right:rankinggeneratedfor4/100randomlychosenclassifiers.RC:redcircle;BC:blue
circle; GC: green circle; RR: red rectangle; BR: blue rectangle; GR: green rectangle.
TCIPED
MACdarG
EMIL
elcarO
elcarO
elcarO
egnahC
CORUA
UOI
UOI
Oracle
Oracle
OracleDEPICT: Diffusion-Enabled Permutation Importance 21
RC 46 1 1 0 0 0
GC 0 46 0 0 1 0
BC 0 2 39 0 0 0
RR 0 2 1 47 0 0
GR 0 2 2 0 46 0
BR 0 2 1 0 1 43
RC GC BC RR GR BR
Concept Classifier Targets
Fig.12: Independent permutation validation for synthetic dataset. We re-
port the average change in AUROC (unit = 0.01) of the concept classifier for the six
shapeswhenpermutingeachindividuallyforbothreal(oracle)andgeneratedimages.
We observe permutation independence: a large change in performance when classify-
ingpermutedconcepts,andminimalchangeinperformanceforunpermutedconcepts.
Colormap: 0 50. RC: red circle, BC: blue circle, GC: green circle, RR: red
rectangle, GR: green rectangle, BR: blue rectangle.
tpecnoC
detumreP22 S. Jabbour et al.
9 COCO
9.1 Experiments
Dataset. COCO [23] contains 117k training and 4.5k validation images anno-
tatedwith80objectcategories,whichweconsidertobeconcepts intheimages.
COCO also has 20k test images that are not labelled with object categories.
Instead, we randomly sampled 10k images from the training set to use for test
sets in downstream classification tasks, resulting in a final training set of 107k
images. To caption each image, we disregarded the natural language captions
correspondingtotheimages,andinsteadconstructednewcaptionsconsistingof
alltheconceptsintheimages.E.g.,ifanimagecontained2personsand1couch,
thecorrespondingcaptionis“2person,1couch.” The15conceptsusedwere:per-
son, bottle, cup, bowl, chair, couch, bed, dining table, tv, laptop, remote, cell
phone, oven, sink, and book. For downstream scene classification, we labelled
each of the images using a ResNet trained on Places365 [43]. We mapped the
scene label to one of six indoor labels from the MIT SUN Database [39]: shop-
ping and dining, workplace (office building, factory, lab, etc.), home or hotel,
transportation (vehicle interiors, stations, etc.), sports and leisure, and cultural
(art, education, religion, military, law, politics, etc.).
Diffusion Model. We fine-tuned a Stable Diffusion [30] model for 1.34 million
iterations with a batch size of 64 on COCO image-caption pairs at a 256x256
resolution and a learning rate of 1.0e-4. We fine-tuned only the U-Net and text-
encoder of the model.
Concept Classifier. We fine-tuned a DenseNet-121 [11] pretrained on Ima-
geNet [9] to predict the presence of the 80 objects in each image. The model
was trained using stochastic gradient descent with momentum minimizing bi-
nary cross-entropy loss with a learning rate of 1.0e-1, momentum of 0.8, weight
decay of 1.0e-4 and a batch size of 128. Early stopping based on validation loss
with a patience of 5 was used after at least 8 training epochs. During training,
imageswerereshapedsuchthattheirsmalleraxiswas256pixels,andthencenter
cropped along their longer axis to 256x256. Images were also randomly rotated
up to 45 degrees, and vertically flipped with probability 0.3. We used ImageNet
normalization across all experiments.
Primary feature models. We trained target classifiers on a binary task: home
or hotel or not. We only considered images labelled with one of these two
scene-level labels. Furthermore, for each of the target classifiers, we subsampled
thedatasuchthattherewasa1:1correlationbetweenthepresenceofaprimary
concept(e.g.,person)andtheoutcome.Wetrained15modelsusing15concepts
that were present in more than 5% of the data: person, bottle, cup, bowl, chair,
couch, bed, dining table, tv, laptop, remote, cell, phone, oven, sink, and book.
Modelsweretrainedwithmomentum0.8,weightdecay1.0e-4,andlearningrate
of 1.0e-1. The best model was chosen as the epoch with the lowest validation
loss.Duringtraining,imageswerereshapedsuchthattheirsmalleraxiswas256
pixels, and then center cropped along their longer axis to 256x256. Images wereDEPICT: Diffusion-Enabled Permutation Importance 23
also randomly rotated up to 45 degrees, and vertically flipped with probability
0.3. We used ImageNet normalization across all experiments.
Mixed feature models. We trained six total scene classifiers, where a model
classifies if an image is one of six indoor scenes: (1) shopping and dining, (2)
workplace, (3) home or hotel, (4) transportation, (5) cultural, and (6)
sports and leisure. Here, we do not resample the training data to encourage
the model to rely on specific concepts, but rather use the entire training set to
let the model rely on any combination of concepts. Models were trained with
momentum 0.8, weight decay 1.0e-4, and a learning rate of 1.0e-1. The best
model was chosen as the epoch with the lowest validation loss. During training,
imageswerereshapedsuchthattheirsmalleraxiswas256pixels,andthencenter
cropped along their longer axis to 256x256. Images were also randomly rotated
up to 45 degrees, and vertically flipped with probability 0.3. We used ImageNet
normalization across all experiments.
Baselines. We generated Grad-CAM [32] and LIME [28] explanations for the
predictedclassofeachimage.Theclasspredictionwasdeterminedbythreshold-
ing model predictions that maximized the true positive rate while minimizing
the false positive rate of the validation set. Each GradCAM heatmap was first
converted to a binary mask by thresholding at the lowest non-zero value of the
Grad-CAMheatmap.5featureswereusedtogenerateeachLIMEmask.Forev-
eryobjectintheimage,wecalculatedtheintersectionoverunion(IOU)between
the object mask and the explanation. Finally, we ranked objects by their mean
IOU across the entire test set.
Unconstrained primary feature models. In reality, we might want to ex-
plain a model that is not a concept bottleneck. Thus, we also trained primary
feature models end-to-end. When the model is not constrained to a specific set
of concepts, we want to observe that DEPICT still detects the primary feature
as the most important concept in a classifier’s decisions.
9.2 Results
Unconstrained primary feature models. We compare three randomly se-
lected unconstrained (trained end-to-end) primary feature model rankings gen-
erated by DEPICT to those generated by GradCAM and LIME in Fig. 17.
DEPICT identifies the primary feature in all cases as significantly more impor-
tant compared to the other concepts. While we do not have an oracle model to
compare to in the unconstrained setting (as the model is not a CBM, and thus
an oracle cannot be calculated), DEPICT’s results do align with the fact that
weresampledthetrainingdatatoencouragethemodelstofocusontheprimary
feature. Note that these models use the same data as the original primary fea-
ture models, so the validation of assumptions (effective generation, independent
permutation) hold for these models.24 S. Jabbour et al.
Table4: EffectivegenerationvalidationforCOCOprimaryfeaturemodels.
We show AUROC on both real and generated images for the primary feature models,
each with one primary feature. The models are able to classify generated images with
high AUROC and a maximum difference between the real and generated images of
0.09.
PrimaryFeatureModel
personbottlecupbowlchaircouchbeddiningtable tv laptopremotecellphoneovensinkbook
RealImages 0.97 0.90 0.89 0.93 0.87 0.94 0.95 0.93 0.95 0.96 0.89 0.82 0.99 0.99 0.86
GeneratedImages 0.91 0.82 0.80 0.87 0.79 0.86 0.89 0.88 0.95 0.92 0.81 0.80 0.95 0.91 0.80DEPICT: Diffusion-Enabled Permutation Importance 25
Table 5: Effective generation validation for COCO concept classifiers in
primaryfeaturemodels.WeshowAUROConrealandgeneratedimagesforconcept
classifiersonCOCOacrossallprimaryfeaturemodelsandallconceptclassifiertargets.
The concept classifier is able to classify generated images with high AUROC and a
maximum difference between the real and generated images of 0.07.
PrimaryFeatureModel
Person Bottle Cup Bowl Chair
ConceptClassifierTargetRealGenRealGenRealGenRealGenRealGen
Person 0.950.920.970.970.970.960.970.970.970.97
Bottle 0.860.830.870.860.870.810.870.820.860.82
Cup 0.860.840.830.840.840.810.850.850.860.86
Bowl 0.910.880.880.880.910.890.930.890.920.91
Chair 0.860.870.860.860.870.880.860.870.860.82
Couch 0.910.920.910.890.920.900.900.910.910.90
Bed 0.940.920.920.930.900.910.900.910.940.95
Diningtable 0.920.880.870.880.870.900.860.880.860.85
Tv 0.930.970.930.940.940.950.930.940.960.96
Laptop 0.970.960.970.970.970.960.970.960.960.97
Remote 0.860.860.910.890.910.870.890.860.910.87
Cellphone 0.880.870.880.880.890.880.870.880.840.88
Oven 0.980.920.980.940.980.920.980.950.970.95
Sink 0.970.920.980.960.970.950.970.940.980.97
Book 0.870.880.870.890.880.890.860.890.870.89
Couch Bed DiningTable Tv Laptop
ConceptClassifierTargetRealGenRealGenReal Gen RealGenRealGen
Person 0.970.970.970.960.97 0.97 0.970.960.970.97
Bottle 0.870.820.860.800.86 0.83 0.870.820.880.82
Cup 0.870.840.850.860.86 0.86 0.860.860.860.87
Bowl 0.880.890.880.900.91 0.89 0.870.870.890.87
Chair 0.870.860.860.860.88 0.88 0.880.880.870.87
Couch 0.930.900.890.900.92 0.90 0.930.910.920.91
Bed 0.920.940.970.950.90 0.92 0.910.940.910.93
Diningtable 0.860.890.870.870.93 0.89 0.860.870.860.90
Tv 0.940.950.940.940.94 0.95 0.960.960.950.95
Laptop 0.960.950.960.950.96 0.96 0.950.930.940.95
Remote 0.900.880.870.850.90 0.86 0.890.890.910.90
Cellphone 0.850.870.840.870.86 0.88 0.870.890.860.89
Oven 0.970.930.980.920.98 0.95 0.980.930.970.93
Sink 0.970.950.970.950.97 0.95 0.980.960.970.94
Book 0.870.890.860.880.87 0.88 0.880.870.880.90
Remote CellPhone Oven Sink Book
ConceptClassifierTargetRealGenReal Gen RealGenRealGenRealGen
Person 0.970.970.97 0.97 0.970.970.970.970.970.97
Bottle 0.870.820.89 0.83 0.880.830.870.800.880.83
Cup 0.880.870.87 0.87 0.860.850.860.820.870.86
Bowl 0.890.890.87 0.89 0.910.910.910.860.880.91
Chair 0.860.860.86 0.86 0.860.860.870.860.850.84
Couch 0.910.900.91 0.91 0.890.880.910.910.900.91
Bed 0.920.930.91 0.92 0.900.920.910.920.900.94
Diningtable 0.860.900.88 0.90 0.870.890.880.890.870.89
Tv 0.950.960.95 0.96 0.930.940.940.940.950.95
Laptop 0.960.960.97 0.96 0.970.950.970.960.970.97
Remote 0.860.850.89 0.87 0.900.890.910.890.920.90
Cellphone 0.850.880.83 0.85 0.860.880.870.890.860.89
Oven 0.970.910.97 0.93 0.980.980.990.960.980.93
Sink 0.980.950.98 0.95 0.970.950.990.960.970.95
Book 0.880.880.87 0.89 0.860.860.880.870.850.8526 S. Jabbour et al.
Table 6: Effective generation validation for COCO mixed feature models.
We show AUROC on both real and generated images for the mixed feature models.
ThedifferencesinclassificationAUROCbetweenrealandgeneratedimagesrangefrom
0.05 to 0.13 AUROC.
MixedFeatureModel
shoppinganddiningworkplacehomeorhoteltransportationsportsandleisurecultural
RealImages 0.89 0.74 0.87 0.89 0.82 0.74
GeneratedImages 0.78 0.69 0.78 0.76 0.71 0.66DEPICT: Diffusion-Enabled Permutation Importance 27
Table 7: Effective generation validation for COCO concept classifiers in
mixed feature models.WeshowAUROConrealandgeneratedimagesforconcept
classifiersonCOCOforthemixedfeaturemodelsandallconceptclassifiertargets.The
differencesinclassificationAUROCbetweenrealandgeneratedimagesrangefrom0.0
to 0.03 AUROC.
RealImagesGeneratedImages
Person 0.97 0.97
Bottle 0.87 0.84
Cup 0.89 0.87
Bowl 0.91 0.90
Chair 0.89 0.88
Couch 0.94 0.93
Bed 0.97 0.96
Diningtable 0.92 0.91
Tv 0.95 0.96
Laptop 0.97 0.96
Remote 0.95 0.92
Cellphone 0.89 0.88
Oven 0.98 0.96
Sink 0.98 0.95
Book 0.90 0.8828 S. Jabbour et al.
r: 0.90 (0.83 - 0.95) Model # 1 Model # 2 Model # 3
1.0 0.4 0.3 0.2 1.0
0.3
0.2 0.5 0.2 0.1 0.5
0.1 0.1
0.0 0.0 0.0 0.0 0.0
0.0 0.5 1.0
r: -0.05 (-0.17 - 0.10)
1.0 0.8 0.8 0.8 1.0
0.6 0.6 0.6
0.5 0.4 0.4 0.4 0.5
0.2 0.2 0.2
0.0 0.0 0.0 0.0 0.0
0.0 0.5 1.0
r: 0.19 (0.03 - 0.35)
1.0 0.6 0.4 0.6 1.0
0.4 0.4
0.5 0.2 0.2 0.2 0.5
0.0 0.0 0.0 0.0 0.0
0.0 0.5 1.0
Feature Importance
DEPICT LIME
GradCAM Oracle Concept Concept Concept
Fig.13: Model feature importance across primary feature models with the
oracle generated by permuting concepts at the bottleneck We compare the
ranking produced by DEPICT to GradCAM and LIME, with the oracle generated by
permutingconceptsatthebottleneck.DEPICThashighercorrelationwiththeoracle
compared to LIME and GradCAM.
TCIPED
MACdarG
EMIL
elcarO
elcarO
elcarO
egnahC
CORUA
UOI
UOI
nosrep puc knis enohp
llec
elbat
gninid
lwob riahc vt deb elttob potpal etomer koob nevo hcuoc vt riahc lwob enohp
llec
koob potpal elttob hcuoc elbat
gninid
nevo knis nosrep etomer puc deb potpal lwob enohp
llec
nevo elbat
gninid
elttob koob etomer vt hcuoc knis deb nosrep puc riahc
Oracle
Oracle
OracleDEPICT: Diffusion-Enabled Permutation Importance 29
r: 0.49 (-0.01 - 0.79) workplace home or hotel cultural
1.0 0.06 1.0
0.075 0.02 0.04 0.050
0.5 0.5 0.00 0.02 0.025
0.000
0.0 0.00 0.0
0.0 0.5 1.0
r: 0.17 (-0.18 - 0.50)
1.0 1.00 1.00 1.00 1.0
0.75 0.75 0.75
0.5 0.50 0.50 0.50 0.5
0.25 0.25 0.25
0.0 0.00 0.00 0.00 0.0
0.0 0.5 1.0
r: 0.30 (0.04 - 0.53)
1.0 0.4 1.0
0.6 0.3 0.4 0.5 0.4 0.2 0.2 0.5
0.2 0.1
0.0 0.0 0.0 0.0 0.0
0.0 0.5 1.0
Feature Importance
DEPICT LIME
GradCAM Oracle Concept Concept Concept
Fig.14: Model feature importance across mixed feature models with the
oracle generated by permuting concepts at the bottleneck. We compare the
ranking produced by DEPICT to GradCAM and LIME, with the generated by per-
muting concepts at the bottleneck. DEPICT has higher correlation with the oracle
compared to LIME and GradCAM.
TCIPED
MACdarG
EMIL
elcarO
elcarO
elcarO
egnahC
CORUA
UOI
UOI
elbat
gninid
potpal hcuoc lwob enohp
llec
knis elttob riahc vt koob etomer puc deb nevo nosrep knis elbat
gninid
riahc lwob enohp
llec
vt deb elttob koob etomer potpal hcuoc nevo puc nosrep elttob nosrep koob deb puc vt lwob knis enohp
llec
elbat
gninid
etomer potpal hcuoc riahc nevo
Oracle
Oracle
Oracle30 S. Jabbour et al.
Primary Concept: Person Primary Concept: Couch Primary Concept: Remote
person391 2 1 2 1 1 1 0 3 3 0 2 0 3 441 2 3 2 1 2 0 1 1 3 2 1 2 3 442 0 2 1 1 1 0 0 2 2 1 2 3 2
bottle 0181 0 0 0 0 0 0 1 2 1 3 0 1 1142 4 1 2 0 1 1 1 1 0 2 3 1 1153 2 0 1 0 1 1 1 1 0 3 2 0
cup 1 3151 0 0 0 2 0 1 2 0 2 1 1 1 2165 0 1 0 3 1 0 0 0 2 3 1 1 3173 0 2 0 3 0 1 1 1 2 3 1
bowl 1 0 1151 1 1 1 0 1 2 0 3 1 1 1 0 2171 2 0 2 1 0 1 0 1 1 1 1 0 3171 1 0 2 0 1 2 1 2 2 0
chair 0 0 1 1241 1 2 0 1 3 1 2 0 1 1 1 1 3180 0 3 1 1 1 0 3 1 2 1 1 2 1200 0 2 1 2 0 1 4 1 1
couch 1 0 0 0 1220 0 0 0 5 0 2 0 1 1 2 1 3 0150 0 1 0 3 0 3 1 2 1 2 1 1 1200 0 1 1 2 1 3 2 1
bed 1 0 1 0 1 1390 0 0 0 1 2 1 0 1 1 1 3 1 2350 1 1 0 1 1 2 1 1 2 2 1 1 2380 1 1 3 1 1 2 1
dining table 0 0 2 1 1 0 0170 1 2 2 2 0 0 2 1 4 7 0 1 0171 1 1 0 1 2 2 2 1 5 5 0 1 0180 1 1 0 2 3 1
tv 2 0 0 0 0 0 1 0321 2 0 3 0 1 1 2 1 2 1 1 0 0261 2 0 2 2 2 1 2 2 1 0 1 1 0281 3 1 3 2 1
laptop 0 1 1 0 0 0 1 1 0292 1 2 0 1 1 1 1 2 1 2 0 0 1300 0 0 2 2 1 1 2 0 0 1 0 1 1311 0 1 3 1
remote 0 0 1 0 1 1 1 1 0 0130 2 0 0 1 1 0 3 1 0 1 1 1 0170 1 3 1 1 1 1 2 1 1 0 0 0 1 6 1 2 2 0
cell phone 0 0 1 0 0 0 0 0 0 1 2182 0 0 1 2 1 2 1 2 0 0 1 1 0252 2 1 1 1 2 0 1 2 0 1 1 1 1262 3 0
oven 1 0 1 1 1 0 1 0 0 1 2 0210 0 1 1 0 4 1 2 1 0 1 1 1 1213 1 1 1 2 2 1 1 1 0 0 1 1 2213 0
sink 2 2 1 0 0 0 1 0 0 1 2 1 2221 1 1 1 4 0 2 1 1 1 1 0 1 1282 1 1 2 2 0 2 1 1 0 1 2 1 0291
book 0 0 0 0 0 1 1 1 0 1 2 0 2 018 1 2 0 3 0 1 0 1 0 1 0 1 1 317 1 1 2 1 0 1 0 1 0 1 1 1 2 317
Primary Concept: Bottle Primary Concept: Bed Primary Concept: Cell phone
person441 2 1 1 1 2 1 2 2 2 3 1 1 4 442 0 2 1 0 3 1 1 1 2 1 2 2 5 440 1 2 0 0 1 1 0 2 2 0 0 2 4
bottle 1131 2 0 1 0 0 2 2 0 2 2 2 2 1153 3 0 0 0 0 1 1 1 0 3 1 3 1162 3 0 0 1 0 1 1 0 1 2 1 1
cup 1 3143 0 1 0 2 1 2 0 1 1 1 2 0 2164 0 0 0 2 1 1 0 1 3 2 2 1 3174 0 0 1 3 0 1 0 2 1 2 1
bowl 1 1 2151 1 0 1 2 2 1 2 1 1 2 1 1 3171 1 1 1 1 1 1 1 2 1 3 1 0 3170 0 0 1 0 1 0 1 1 1 1
chair 1 1 1 0210 0 1 2 3 1 1 0 1 2 1 2 2 1211 1 2 1 1 1 1 4 0 4 2 0 1 2221 1 2 0 1 1 1 3 0 2
couch 1 1 0 0 1200 0 2 2 2 1 1 0 2 1 2 1 2 1230 1 1 0 2 1 3 1 3 1 1 0 2 1220 1 0 1 2 1 2 1 1
bed 1 1 0 1 1 2381 2 2 0 1 0 1 1 0 4 0 2 1 2381 1 0 2 0 1 1 1 1 0 1 2 1 1371 0 1 0 0 0 1 0
dining table 2 1 3 3 0 1 0172 2 1 2 1 1 2 2 1 5 5 0 0 0171 1 1 0 2 2 3 2 0 4 6 1 0 0181 1 0 0 1 1 1
tv 1 1 2 0 0 2 0 0282 2 2 1 1 2 1 3 2 1 0 1 1 1291 2 1 3 1 3 1 2 2 2 0 0 1 0291 2 1 2 1 1
laptop 1 0 1 0 0 0 0 1 2311 1 1 1 3 0 2 2 1 0 1 0 1 1321 1 1 2 3 1 1 1 1 0 0 0 2 1300 3 1 1 2
remote 1 0 0 1 1 1 1 1 2 1181 1 1 1 1 2 1 1 1 1 1 1 0 0191 2 2 2 1 1 1 2 0 0 1 2 0 0191 1 2 0
cell phone 1 1 0 0 0 1 0 1 2 2 0231 1 1 1 2 2 1 1 0 0 1 1 1 1242 2 2 1 1 1 1 0 0 0 1 1 1 0201 1 0
oven 1 1 0 2 0 1 1 1 2 2 1 0201 1 1 1 2 3 1 1 1 1 1 1 1 2201 2 1 0 1 3 0 0 1 1 1 1 1 0231 1
sink 1 5 1 1 0 1 0 0 1 2 0 1 1181 1 0 2 3 0 0 1 0 1 1 0 1 1293 2 1 1 2 0 0 2 0 0 1 0 1 0281
book 1 1 0 1 0 1 0 1 1 2 1 1 1 120 0 2 2 2 0 1 1 1 1 1 1 1 2 219 1 0 1 2 0 0 1 1 0 1 0 1 1 218
Primary Concept: Cup Primary Concept: Dining table Primary Concept: Oven
person443 2 2 0 1 1 2 1 1 1 3 1 1 4 440 0 2 0 1 1 0 1 2 1 1 1 3 3 441 0 2 0 2 1 1 1 1 3 2 2 3 2
bottle 1121 1 1 1 0 2 1 1 1 1 2 2 1 1163 1 0 1 0 1 1 1 1 0 0 3 0 1123 3 0 1 0 1 1 1 1 0 1 1 0
cup 1 1 8 2 1 1 1 4 1 0 1 1 2 1 1 1 4162 0 1 1 1 1 1 1 1 0 3 0 1 1144 1 2 0 4 1 1 1 0 1 1 0
bowl 1 2 0150 1 1 3 1 1 1 1 2 0 1 1 1 3141 1 0 1 1 1 1 0 1 1 1 1 1 2120 1 0 2 1 1 2 0 1 0 0
chair 1 2 1 0211 0 5 1 1 1 1 3 0 2 2 0 2 0211 1 2 1 2 1 1 0 1 1 1 1 1 2210 0 4 0 1 2 0 2 0 1
couch 1 3 2 1 2201 2 1 1 1 0 2 0 1 1 0 1 0 1191 1 1 1 2 1 0 2 1 1 2 0 3 2200 1 0 1 3 1 1 0 1
bed 1 3 3 0 0 1361 1 1 2 0 1 0 0 1 0 2 1 1 1362 1 1 1 1 1 2 0 1 1 1 2 0 2360 1 1 1 1 1 0 0
dining table 2 3 1 2 2 1 1201 1 0 1 2 1 1 2 1 5 3 1 1 1131 1 0 0 0 2 1 2 1 4 5 2 1 0180 1 2 0 1 0 1
tv 1 3 1 0 1 1 0 2281 1 1 2 1 2 1 1 2 0 1 1 0 1252 1 0 0 2 1 1 2 2 3 0 2 0 1311 2 0 1 0 1
laptop 1 3 4 1 1 1 1 2 1291 1 1 1 2 1 0 3 0 1 1 1 2 1301 0 1 2 1 1 1 2 2 1 1 1 0 1311 0 1 1 1
remote 1 3 2 1 0 1 0 1 1 0170 2 1 1 1 0 1 0 1 1 0 2 1 1151 0 3 0 1 1 1 2 0 1 0 0 0 0200 1 1 0
cell phone 1 4 2 0 1 1 0 1 1 1 1242 1 1 1 0 2 0 0 1 0 1 2 1 1251 2 0 1 1 1 2 1 1 0 0 1 1 1241 1 0
oven 1 2 2 1 1 1 0 2 1 1 1 0170 1 1 1 2 1 1 1 0 1 1 1 1 1184 1 1 0 0 3 1 2 1 1 1 0 1 0192 0
sink 1 0 0 1 1 1 0 2 0 1 1 0 1201 2 2 2 1 0 1 0 1 1 1 1 1 3231 1 1 1 2 0 2 0 1 1 1 1 1 2160
book 1 3 2 0 1 1 0 1 0 1 1 1 2 119 1 0 2 0 0 1 0 1 1 1 1 1 0 318 1 1 1 3 1 1 0 0 0 1 1 1 1 118
Primary Concept: Bowl Primary Concept: Tv Primary Concept: Sink
person451 1 0 0 0 0 0 1 2 1 3 1 3 5 431 0 1 0 1 3 1 0 1 2 1 0 4 2 432 2 0 0 1 1 0 1 1 3 1 2 2 2
bottle 2161 0 0 0 1 0 1 2 0 1 1 1 2 0163 2 0 2 1 0 1 1 0 1 1 4 0 1102 1 0 1 0 0 1 1 1 0 1 0 1
cup 1 2151 1 0 1 2 1 2 0 0 1 2 2 0 3164 1 2 0 2 1 0 1 2 1 5 1 1 1122 1 1 0 3 1 1 0 0 0 0 1
bowl 2 0 2110 0 1 1 1 2 0 1 1 1 2 1 1 3160 1 1 1 1 0 1 1 1 4 0 1 1 1110 0 0 2 1 1 2 0 0 0 1
chair 2 1 1 1211 2 2 1 2 1 0 1 1 3 1 1 2 1171 1 3 0 1 1 1 2 2 1 1 1 1 1211 0 4 1 1 2 1 1 1 2
couch 2 2 1 0 2222 0 1 1 2 0 1 1 2 0 2 1 2 1170 1 0 1 2 2 1 3 0 1 2 0 0 2210 1 1 1 3 1 0 0 1
bed 2 1 1 1 0 1351 1 2 0 0 0 1 1 0 1 2 2 0 2371 1 0 0 1 1 4 0 1 2 0 1 0 1370 1 1 0 1 0 0 0
dining table 3 1 3 1 2 0 2161 2 1 1 1 1 3 1 0 5 5 1 1 1150 0 1 1 1 5 0 2 2 2 3 2 0 0170 1 1 0 0 0 1
tv 2 2 1 0 0 0 1 0272 1 0 1 1 2 0 0 2 2 1 0 1 1240 3 1 2 3 1 1 2 1 1 0 1 0 0301 3 0 0 0 1
laptop 1 1 2 1 1 0 2 1 1311 0 0 1 3 0 1 2 0 0 1 0 1 1311 1 0 5 1 1 2 1 0 1 0 1 0 1321 0 1 1 2
remote 2 1 1 1 0 0 1 1 1 1180 1 2 1 1 1 1 1 0 2 1 1 0 0132 0 5 0 1 2 0 1 0 0 0 0 0 1200 0 0 0
cell phone 2 2 0 1 0 0 1 1 2 2 0241 1 1 0 1 2 1 0 2 1 1 1 0 1251 4 0 1 2 0 0 1 1 0 0 1 1 1250 0 0
oven 2 1 0 0 0 0 1 0 1 2 1 1152 2 0 0 2 3 0 2 1 1 1 0 1 2245 0 1 1 0 2 0 1 1 0 1 1 1 1181 0
sink 2 0 1 0 1 1 1 0 1 1 0 0 0202 1 1 2 2 1 2 1 0 0 0 0 2 1331 1 2 2 2 1 1 1 0 1 0 0 0 0140
book 1 1 0 1 1 0 1 1 1 2 1 0 1 220 0 1 2 1 0 1 1 1 0 0 0 2 1 616 1 2 0 1 1 0 0 1 1 1 1 1 0 019
Primary Concept: Chair Primary Concept: Laptop Primary Concept: Book
person440 1 3 4 1 3 3 0 2 0 1 0 3 3 441 0 1 1 0 2 1 1 3 3 1 0 3 3 441 2 3 2 1 3 1 1 2 4 3 3 2 3
bottle 1172 4 3 0 1 3 1 1 2 0 1 3 1 1143 1 1 1 0 1 1 1 1 0 1 3 0 1171 3 1 1 2 0 1 2 2 1 4 2 0
cup 1 4175 3 0 0 2 1 1 1 1 1 3 1 1 2173 0 0 0 4 1 1 2 1 1 3 1 1 2165 1 1 1 3 1 1 2 0 3 2 0
bowl 1 1 3173 0 1 1 1 1 1 1 0 2 1 1 0 4151 0 0 3 1 1 2 1 0 1 1 1 0 2182 1 1 2 1 1 3 1 2 1 2
chair 1 1 2 2150 1 3 0 1 1 1 1 2 3 1 1 2 0190 0 4 1 2 2 0 1 1 2 1 1 0 2191 1 1 0 2 3 0 4 0 2
couch 1 1 0 3 2210 2 1 1 0 2 1 2 1 1 2 1 0 1210 1 1 1 3 1 2 2 1 1 2 1 2 0221 1 1 1 4 0 3 2 1
bed 1 0 0 3 3 2333 1 1 2 0 0 2 1 1 1 2 0 1 1351 1 1 1 0 0 2 0 1 1 0 2 2 1381 1 1 2 0 2 1 1
dining table 2 0 6 5 2 0 1191 1 1 1 0 3 1 2 1 5 4 0 0 0211 1 2 0 0 2 1 2 1 4 6 1 1 1191 1 2 1 3 2 0
tv 1 1 1 3 2 1 1 3261 1 1 1 3 1 1 2 2 0 0 0 0 1262 4 1 1 2 1 1 2 1 2 1 0 2 1301 4 1 3 1 2
laptop 1 0 1 2 4 0 0 4 1311 1 0 3 2 1 1 1 1 0 0 0 1 1222 2 1 2 1 1 1 1 2 1 0 1 1 1323 1 2 2 0
remote 1 0 1 3 4 1 1 4 0 1141 0 3 1 1 2 2 0 1 0 1 0 1 1191 0 3 0 1 1 0 2 2 0 2 2 1 1210 2 2 0
cell phone 1 1 1 2 4 0 1 3 1 1 1260 2 1 1 1 2 0 1 1 0 0 2 1 1270 2 0 1 1 1 2 2 1 2 1 2 1 2273 2 0
oven 1 1 1 5 3 0 1 2 1 1 1 2213 1 1 1 2 1 1 0 1 1 1 1 2 1232 0 1 0 0 4 2 0 2 0 1 1 3 1211 1
sink 1 2 1 3 3 1 2 2 1 1 1 2 0241 1 1 2 1 0 1 1 2 1 1 2 2 1300 1 1 1 3 1 0 2 0 0 1 2 0 2290
book 1 0 1 3 3 0 1 3 0 1 1 2 0 319 1 1 2 1 0 0 0 0 1 1 2 1 1 317 1 1 1 3 1 0 2 1 1 2 3 0 2 213
Concept Classifier Targets
Fig.15: Independent permutation validation for COCO primary feature
models.WereporttheaveragechangeinAUROC(unit=0.01)oftheconceptclassi-
fierfortheCOCOprimaryfeaturemodelswhenpermutingeachconceptindependently.
We observe permutation independence: a large change in performance when classify-
ingpermutedconcepts,andminimalchangeinperformanceforunpermutedconcepts.
Colormap: 0 50.
tpecnoC
detumreP
nosrep elttob puc lwob riahc hcuoc deb elbat
gninid
vt potpal etomer enohp
llec
nevo knis koob nosrep elttob puc lwob riahc hcuoc deb elbat
gninid
vt potpal etomer enohp
llec
nevo knis koob nosrep elttob puc lwob riahc hcuoc deb elbat
gninid
vt potpal etomer enohp
llec
nevo knis koobDEPICT: Diffusion-Enabled Permutation Importance 31
person 42 1 0 1 1 0 2 0 0 1 2 0 3 2 1
bottle 0 18 2 0 0 1 0 0 0 1 0 0 1 1 0
cup 0 3 15 1 0 1 0 2 0 1 0 0 1 0 1
bowl 0 1 2 17 0 1 0 1 0 0 0 0 1 0 0
chair 1 1 1 0 19 0 1 2 0 1 0 1 1 1 1
couch 0 0 1 0 1 15 0 0 0 1 2 1 1 0 1
bed 0 0 1 0 0 1 30 0 0 1 0 0 1 0 0
dining table 1 1 3 1 1 1 0 15 0 1 0 0 1 1 1
tv 0 0 1 0 0 0 1 0 25 1 2 0 1 0 1
laptop 0 1 1 0 0 1 0 0 0 24 0 2 1 1 1
remote 0 0 0 0 0 0 0 0 0 0 13 1 1 1 0
cell phone 0 0 1 0 0 1 0 0 0 1 0 23 1 0 0
oven 0 1 1 0 0 1 1 0 0 0 0 1 24 1 0
sink 0 2 1 0 1 1 0 0 0 1 0 1 2 18 0
book 0 0 1 0 0 0 0 0 0 1 0 1 1 1 17
Concept Classifier Targets
Fig.16:IndependentpermutationvalidationforCOCOmixedfeaturemod-
els. We report the average change in AUROC (unit = 0.01) of the concept classifier
for the COCO mixed feature models when permuting each concept independently.
We observe permutation independence: a large change in performance when classify-
ingpermutedconcepts,andminimalchangeinperformanceforunpermutedconcepts.
Colormap: 0 50.
tpecnoC
detumreP
nosrep elttob puc lwob riahc hcuoc deb elbat
gninid
vt potpal etomer enohp
llec
nevo knis koob32 S. Jabbour et al.
person tv laptop
0.4
0.2
0.2 0.1
0.1
0.0 0.0 0.0
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.6 0.4
0.4 0.4
0.2 0.2
0.2
0.0 0.0 0.0
Feature Feature Feature
Fig.17: Unconstrained primary feature model rankings. We compare three
randomlyselectedunconstrained(trainedend-to-end)primaryfeaturemodelrankings
generatedbyDEPICTtothosegeneratedbyGradCAMandLIME.DEPICTidentifies
theprimaryfeatureinallcasesassignificantlymoreimportantcomparedtotheother
concepts.
TCIPED
MACdarG
EMIL
egnahC
CORUA
UOI
UOI
nosrep elttob puc lwob riahc hcuoc deb elbat
gninid
vt potpal etomer enohp
llec
nevo knis koob nosrep elttob puc lwob riahc hcuoc deb elbat
gninid
vt potpal etomer enohp
llec
nevo knis koob nosrep elttob puc lwob riahc hcuoc deb elbat
gninid
vt potpal etomer enohp
llec
nevo knis koobDEPICT: Diffusion-Enabled Permutation Importance 33
10 MIMIC-CXR
10.1 Experiments
Dataset. MIMIC-CXR [14,16] consists of 242,479 frontal chest X-rays with
corresponding radiology reports. We split the data into 193706/24549/24224
imagesfortraining,validation,andtestsets.Toconstructafinalcaptionforeach
image, we extracted demographic information corresponding to the patients’
body mass index (BMI), age, and sex at the time the chest X-ray was taken,
and prepended these information to the radiology report corresponding to the
chest X-ray. We subsampled the data for downstream tasks where we injected a
1:1 correlation between pneumonia and each primary features: bmi, age, or sex.
Diffusion Model. A diffusion model initialized on Stable Diffusion [30] with
the text encoder replaced with publicly available clinical BERT embeddings [3]
was fine tuned on chest X-ray/radiology report pairs for 295569 iterations on
a batch size of 16 at a 256x256 resolution with a learning rate of 1.0e-4. We
fine-tuned only the U-Net and text-encoder of the model.
Target Models. We trained target classifiers to predict the presence of pneu-
monia.Wetrainedtheclassifierontopoftheconceptclassifier.Duringtraining,
images were reshaped such that their smaller axis was 256 pixels, and then ran-
domly cropped along their longer axis to 256x256. Images were also randomly
rotated up to 15 degrees. We used ImageNet normalization across all experi-
ments.
Concept Classifier. We fine-tuned a DenseNet-121 [11] pretrained on Ima-
geNet [9] to learn the presence of radiological findings and the three permutable
concepts:bmi,age,sex,enlargedcardiomediastinum,cardiomegaly,lungopacity,
lung lesion, edema, consolidation, atelectasis, pneumothorax, pleural effusion,
pleural other, fracture, and support devices. The model was trained for three
epochs using stochastic gradient descent with momentum minimizing binary
cross-entropy loss with a learning rate of 1.0e-4, momentum of 0.8 and a batch
size of 32. During training, images were reshaped such that their smaller axis
was 256 pixels, and then randomly cropped along their longer axis to 256x256.
Images were also randomly rotated up to 15 degrees. We used ImageNet nor-
malization across all experiments.
Validationofassumptions.Foreffectivegeneration,wemeasurethedifference
in target model performance between real and generated images. If the target
modelperformswellongeneratedimages,thenwecanmeasureconceptclassifier
performance on specific concepts in the images that we wish to permute. For
concepts that we are not permuting, we might not need to validate effective
generation depending on the scenario: (1) Target model does not pass the checks
of effective generation. Consider a concept that we are not permuting in text
space(e.g.,PleuraleffusiononthechestX-ray).Ifthetargetmodelperformance
drops on the generated images, then we might want to investigate why. In this
setting,itwouldbeusefultolookatgranularchangesinmodelperformancevia
the concept classifier to know if specific concepts are not being generated well,
and thus contributing to poor target model performance. In any case, since the34 S. Jabbour et al.
targetmodeldoesnotpassthechecksofeffectivegeneration,wewouldnotapply
DEPICTsincethetargetmodeldoesnotpassthechecksofeffectivegeneration.
(2) Target model does pass the checks of effective generation. Again, consider
a concept that we are not permuting in text space (e.g., Pleural effusion on
the chest X-ray). If the target model passes the checks of effective generation,
but the concept classifier performs poorly in detecting a specific concept that
we are not permuting, we can still apply DEPICT. This is because we know
the model must not be relying on the non-permutable concept, since the target
model can still classify the generated images well. Furthermore, we do not need
togenerateareferenceperformanceforthenon-permutableconceptsinceweare
not permuting it, nor ranking it against other concepts.
Independent permutation. We note that it is still useful to measure indepen-
dent permutation on non-permutable concepts. This way, we can check to make
sure that when permuting a concept (such as age, bmi, or sex), any resulting
change in target model performance is not confounded by other changes on the
image.
Results. Here, we further discuss results of DEPICT on MIMIC-CXR.
Validationofassumptions.Allthreetargetmodelsareabletoaccuratelyclassify
thegeneratedimages(Table8).Similarly,theconceptclassifierperformswellon
bothrealandgeneratedimagesforallthreedemographicconcepts:bmi,age,and
sex (Table 9). We also measured concept classifier performance on radiological
findings, finding that the concept classifier performs well across most concepts
in the images (Table 9), but struggles on a few such as detecting lung lesions
(AUROC drop = 0.31) and pneumothorax (AUROC drop = 0.23). Again, we
note that we can still apply DEPICT to these settings, as we are not permuting
such concepts on the images and the target models still classify the generated
imageswell,evenwithoutbeingabletodetectconceptssuchaslunglesionsand
pneumothorax (target model AUROC > 0.85).
Intermsofindependentpermutation,whenpermutingage,bmi,andsex,we
observesomechangesinconceptclassifierperformancewhendetectingconcepts
such as lung opacity and lung lesion (Fig. 18). Thus, one must proceed with
caution when interpreting the results of DEPICT. When permuting one of the
threeconcepts,wecanconcludethatthemodelreliesoneachofthethreeprimary
featuresinsomeway-eitherdirectly,orbycorrelationwithotherconceptssuch
as lung opacity and lung lesion.
Table8: EffectivegenerationvalidationforMIMICmodels.WeshowAUROC
on both real and generated images for the MIMIC models. The differences in classifi-
cation AUROC between real and generated images range from 0.0 to 0.04 AUROC.
Primary Feature
BMIAge Sex
Real Images 0.89 0.98 1.0
Generated Images 0.85 0.96 1.0DEPICT: Diffusion-Enabled Permutation Importance 35
Table 9: Effective generation validation for MIMIC concept classifiers. We
show AUROC on real and generated images for concept classifiers on MIMIC across
all concept classifier targets.
Primary Feature
BMI Age Sex
Concept Classifier TargetRealGenRealGenRealGen
BMI 0.94 0.91 0.95 0.91 0.95 0.90
Age 0.98 0.95 0.98 0.97 0.98 0.96
Sex 1.00 1.00 1.00 1.00 1.00 1.00
Enlarged Cardiomediastinum 0.85 0.84 0.86 0.74 0.84 0.72
Cardiomegaly 0.91 0.85 0.92 0.86 0.91 0.82
Lung Opacity 0.70 0.66 0.79 0.71 0.69 0.60
Lung Lesion 0.88 0.68 0.92 0.78 0.95 0.64
Edema 0.95 0.82 0.95 0.84 0.93 0.83
Consolidation 0.91 0.81 0.91 0.83 0.91 0.85
Atelectasis 0.70 0.57 0.81 0.58 0.81 0.57
Pneumothorax 0.96 0.79 0.97 0.89 0.96 0.87
Fracture 0.88 0.68 0.85 0.69 0.87 0.7236 S. Jabbour et al.
BMI 37 1 0 3 1 5 2 1 4 6 4 3 0
Age 0 36 0 4 1 3 3 1 5 4 2 4 0
Sex 0 0 47 2 1 8 7 1 3 5 2 2 0
BMI Age Sex Enlarged Cardiomegaly Lung Lung Edema Consolidation AtelectasisPneumothorax Pleural No Finding
Cardiomediastinum Opacity Lesion Effusion
Concept Classifier Targets
Fig.18: Independent permutation validation for MIMIC-CXR. We report
the average change in AUROC (unit = 0.01) of the concept classifier for the MIMIC
conceptswhenpermutingeachoneindependently.Whenpermutingbmi,age,andsex,
weobservechangestotheconceptclassifier’sabilitytodetectotherradiologicalfindings
suchaslungopacityandlunglesion.Thus,theimportanceofthesethreedemographic
conceptscouldbedue,inpart,tochangesinthepresenceofotherfindingsonthechest
X-ray. Colormap: 0 50.
tpecnoC
detumreP