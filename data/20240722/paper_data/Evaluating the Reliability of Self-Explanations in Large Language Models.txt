Evaluating the Reliability of Self-Explanations in
Large Language Models
Korbinian Randl1[0000−0002−7938−2747], John
Pavlopoulos1,2,3[0000−0001−9188−7425], Aron Henriksson1[0000−0001−9731−1048],
and Tony Lindgren1[0000−0001−7713−1381]
1 Stockholm University, Department of Computer and Systems Sciences, Kista,
SE-164 07, Sweden {korbinian.randl,ioannis,aronhen,tony}@dsv.su.se
2 Athens University of Economics and Business, Patission 76, Athens 104 34, Greece
{annis}@aueb.gr
3 Archimedes/Athena RC
Abstract. Thispaperinvestigatesthereliabilityofexplanationsgener-
ated by large language models (LLMs) when prompted to explain their
previous output. We evaluate two kinds of such self-explanations – ex-
tractive and counterfactual – using three state-of-the-art LLMs (2B to
8B parameters) on two different classification tasks (objective and sub-
jective). Our findings reveal, that, while theseself-explanations can cor-
relatewithhumanjudgement,theydonotfullyandaccuratelyfollowthe
model’sdecisionprocess,indicatingagapbetweenperceivedandactual
modelreasoning.Weshowthatthisgapcanbebridgedbecauseprompt-
ingLLMsforcounterfactualexplanationscanproducefaithful,informa-
tive, and easy-to-verify results. These counterfactuals offer a promising
alternative to traditional explainability methods (e.g., SHAP, LIME),
provided that prompts are tailored to specific tasks and checked for va-
lidity.
Keywords: Large Language Models · Self-Explanations · Counterfac-
tuals.
1 Introduction
In recent years, large language models (LLMs) have made significant progress
in natural language processing tasks, exhibiting impressive capabilities across
various domains. Following their successes, these models have found their way
into people’s everyday lives, for example in the form of chatbots such as Chat-
GPT.Inlightofthisgreatimpactofthetechnology,andtheincreasingamounts
of trust placed on it, a critical question remains: How reliable are the expla-
nations these models provide for their own outputs and can they successfully
explain their own reasoning processes? Understanding the internal reasoning of
LLMs is crucial for building trust and transparency in their usage. This paper
investigates the reliability of self-explanations generated by prompting LLMs to
explain their previous outputs and provides the following contributions:
4202
luJ
91
]LC.sc[
1v78441.7042:viXra2 K. Randl et al.
1. We evaluate extractive self-explanations generated by three state-of-the-art
LLMs (2B to 8B parameters) on two different classification tasks (objective
andsubjective),andshowthat,whiletheseextractsmayoftenseemintuitive
forhumans(astheyshowhighcorrelationwithhumanassessment)theyare
notguaranteedtofullyandaccuratelydescribethemodel’sdecisionprocess.
2. We show that the gap highlighted in our first contribution can be bridged.
Specifically, our findings show that prompting the LLM for counterfactual
explanationscancreatefaithfulexplanationsthatcaneasilybevalidatedby
the model.
3. We provide an analysis of counterfactual self-explanations created by LLMs
and show that they can be highly faithful and similar to the original, but
need to be individually checked for validity.
2 Related Work
Local Explainability for Transformers: Inthescopeofthiswork,wedefine
LLMs as pre-trained text-to-text processing systems, based on the Transformer
architecture [31]. As such systems usually complete an input text by iteratively
predicting the next token, we use the following notation throughout this paper:
t =LLM(t ,...,t ) (1)
n+1 0 n
Specifically,LLMsconsistofanembeddinglayerh(0) =fIn(t ,i),computingthe
i i
inputembeddingh(0) =[h(0),...,h(0)],followedbyLtransformerblocksh(l+1) =
0 n
f(l)(h(l)),0 ≤ l < L, and a head t = fout(h(L)). Each of the transformer
n+1
blocks uses multi-head attention which we will explain in more detail later.
Modern Transformers can be divided into three sub-architectures: encoder-
only[7,17],encoder-decoder[23,12],anddecoder-only[29,2].Decoder-onlyLLMs
have demonstrated good classification abilities, even without additional fine-
tuning,using“in-contextlearning”:BysimplyaskingtheLLMtoclassifyasam-
ple text provided in the input text or “prompt” along with a list of possible
classes, LLMs can successfully solve many reasoning tasks. This method called
“zero-shotprompting” canbeextendedto“few-shotprompting” [5]formoredif-
ficult tasks by additionally including a small number of labeled samples in the
prompt. Recently, instruction-tuning improves further the performance [29,2].
In this paper, we focus on local explainability. This means we want to ex-
plain specific predictions of the Transformer rather than explain how the model
worksingeneral.Sincethefirstpublicationin2017,differentmethodsforgener-
ating such explanations for the classification output of Transformers, and there-
fore LLMs, have been proposed. These are heavily dependent on the classifi-
cation paradigm [36]: in general deep learning, which is often extendable to
Transformers,theliteratureshowsfeatureattributionapproaches(e.g.relevance-
propagation-based [4,20] or gradient-based [9,28]). For traditional applications
where the Transformer is fine-tuned to produce class probabilities via a task-
specific output layer, we are aware of attention-based [1] or mixed [6,16] ap-
proaches. In prompting, specifically for instruction-tuned models, the literatureEvaluating the Reliability of Self-Explanations in Large Language Models 3
mainly contains methods for generating textual explanations such as Chain-of-
Thought (CoT) [33,36]. Independent of the applied paradigm, surrogate-based
model-agnostic approaches, such as LIME [25] and SHAP [18], can be found in
literature [14,36]. Next, we introduce the most important types of explanations.
Attention-Based Explanations leverage the Transformer’s scaled dot prod-
uctattentionweightsA(l),generatedduringtheforwardpass,toexplaintheim-
pact of each input token to each output token. Given an input vector h(l) ∈Rn,
theself-attentionvariant,whichisappliedinallTransformersusedinthispaper,
is computed by feeding h(l) through three linear layers, computing the vectors
q(l), k(l), and v(l), and then:
(cid:32) q(l)·k(l)T(cid:33)
A(l) =softmax √ (2)
n
h′(l) =A(l)·v(l) (3)
This makes A(l), with all elements ∈ [0,1] a weight matrix connecting h(l) and
h′(l). As Transformers produce one matrix A per attention head (e.g. 12×12
heads in BERT [7], 28×16 heads in Gemma-7B [29]) extracting meaningful
base
explanations is not trivial: while naive approaches simply use the mean atten-
tion weights of the last layer, methods that follow the attention through the
whole Transformer have been shown to outperform them [1]. Attention-based
explanations can be improved by combining them with gradient-based methods
to estimate the importance of each head towards the prediction [6], the crucial
step lies in connecting attention weights in the last attention layer to the out-
put, as the residual connections within the Transformer keep input to output
association stable over multiple layers [16]. As there is a debate in the literature
about whether attention weights can be used as explanations, we also employ
gradient-based explanations.
Gradient-Based Explanations create saliency maps of the input by com-
puting the gradient ∂LLM(·),0 ≤ i ≤ n of the n+1th output of the LLM with
∂h(0)
i
regard to a specific input embedding h(0). In the simplest case, this gradient
i
itself can be the explanation [9], but the literature shows that computing the
Hadamard product with the input improves on it [26]. An often discussed prob-
lem of gradient-based approaches is the so-called saturation problem [26,28]: as
neural networks minimize the absolute gradient during training, gradients of a
well-fittednetworkwillbeclosetozero.Weargue,however,thattheambiguous
nature of natural language prevents overfitting and therefore also, to a certain
degree, gradient saturation of pre-trained multi-purpose LLMs. To support this
theory, we provide statistics on the gradients for each of our experiments.4 K. Randl et al.
Counterfactual Explanations are–simplyput–versionsofthemodelinput
that alter the model’s output. A good counterfactual should fulfill at least the
following two criteria [32]: (i) validity: the model output between the counter-
factualandtheoriginalinputshoulddifferatinferencetime.(ii) similaritythe
changes made to the original to produce the counterfactual should be minimal:
the more that is changed from the original, the less specific the counterfactual
becomes, eventually making it irrelevant as a local explanation. However, defin-
ingagooddistancemeasureforcomparingtwotextsisnon-trivial,andmaywell
requirethecombinationofbothsemanticandsyntacticsimilarity.Therefore,this
second point is sometimes overlooked in similar studies [19].
Rationale-Based Explanations can be described as textual excerpts or ab-
stractions of the model input that contribute to the model’s predictions [8]. In
contrast to feature attribution methods, these do not provide a measure for the
importance of each token, but rather a text that describes the influences. While
traditional methods rely on extraction or abstraction methods for generating
those texts [8], LLMs can be prompted to provide explanations. CoT generates
textual rationalesat inference time and even boosts reasoning performance[33],
while additional prompting for self-explanations has recently received increased
attention in the research community:
Huang et al. [10] prompt ChatGPT to yield feature importance scores for a
sentimentclassificationtaskbasedon100randomtextstakenfromtheStanford
Sentiment Treebank dataset [27]. Then, they evaluate the faithfulness of the
generated scores compared to LIME [25] and occlusion [13]. They conclude that
none of the three methods has a clear advantage over the others and that the
methodsoftendonotagreeonfeatureimportance.However,thisambiguitymay
be owed to their relatively small sample size. Furthermore, prompting an LLM
toproducenumericalimportancescoresforeachtokenisnotwhatthesemodels
are designed to produce.
Madsen et al. [19] instead prompt for the most important words, counter-
factuals, and redactions (i.e. asking the model to mask important tokens) for
classification tasks on different datasets using Llama2 [30], Falcon [3], and Mis-
tral [11]. They conclude that faithfulness of explanations is highly dependent
both on the choice of the LLM and on the data used. The authors, however, do
not compare the extracted self-explanations to established explainability meth-
ods or human annotations.
We extend the previously discussed work by addressing the following questions:
– RQ1: Do LLM self-explanations correlate well with human judgment?
– RQ2: Do LLM self-explanations correlate well with internal model dynam-
ics, represented by attention- and gradient-based explainability methods?
3 Method
To address our research questions, first we extract LLM self-explanations and
explanations from model-specific explainability gradient/attention methods.Evaluating the Reliability of Self-Explanations in Large Language Models 5
3.1 Self-Explanations
In all our experiments, we use zero-shot prompting using the chat-completion
format. A typical chat can be seen in Table 1: a) As a first step, we prompt
the LLM to perform the actual task: we study food hazard classification and
sentiment classification . Following this, we generate two different types of self-
explanations:b) extractive self-explanationsarecreatedbyaskingtheLLM
forthewords/phrases(weaskthemodeltoproducethesamenumberofphrases
as in the human ground truth) that were most important for its classification;
c) counterfactual self-explanations are generated by asking the LLM to
provide a version of the classified text, for which its decision would have been
different and which has as few words as possible changed from the original text.
These two kinds of explanations are generated in separate chats to avoid cross-
influencing the LLM’s output.
Table1.ExampleChatSequence:Ourapproachissplitintothreesteps:a)theactual
task,b)extractingaposthocself-explanation,andc)extractingacounterfactual.The
latter two are performed in independent chat sequences. This example was produced
using Llama 3-8B. The prompt is slightly adapted to fit different tasks and models.
a)user: What is the sentiment of the following review?
“Hints are made to the audience that this film could be a blast. Alas,
these are only hints.”
Assign one of the following labels: "negative" or "positive". Make sure
to answer only with the label.
assistant:Negative
b)user: What is the most important phrase influencing your assessment? Provide
only the phrase as a string.
assistant:“Alas, these are only hints.”
c) user: Provide a version of the review that would flip your assessment while
changing as few words in the original review as possible. Make sure to
answer with only the new version.
assistant:“Hints are made to the audience that this film could be a blast. And
indeed, these are more than just hints.”
3.2 Model-Specific Explanations
We compare the generated self-explanations to established explainability meth-
ods. In this paper, we do not consider explanation methods based on relevance
propagation [4,20], as we are not aware of any implementations that sufficiently
deal with the residual connections in the Transformer architecture. Instead,
we focus on gradient- and attention-based methods, as these remain close to
the internal information representation of the LLMs. Furthermore, we focus
on decoder-only Transformers, needed to generate self-explanations, which are
largely similar to encoder-only Transformers in architecture and therefore often6 K. Randl et al.
allow for the direct application of attention-based explainability methods tested
on BERT. In this work, we apply three simple analytic explanation methods:
(i) AGrad [16] for an output t is the product:
n+1
H
1 (cid:88) ∂LLM(·) A(L) , (4)
H ∂A(L) h,j,k
h=1 h,j,k
where A(L) is the attention weight with indices j,k in the hth head of the
h,j,k
last layer L. As the authors of the method show, backpropagating the produced
saliencytotheinputdoesnotgreatlyimprovefaithfulness,becauseofthetrans-
former’s residual connections. Therefore, we omit this step.
(ii) Gradient times Input (GradIn) [26]isasimplegradient-basedmethod,
computed by taking the product
∂LLM(·)
h(0), (5)
∂h(0) i
i
for an output token t and an input embedding h(0).
n+1 i
(iii) Inverted Gradient (IGrad)isourownapproachtogradient-basedcoun-
terfactual explanations, as we are not aware of existing methods in this domain.
While GradIn approximates the impact of a change in the input towards the
outputofthenetwork,IGradtakesacounterfactualapproachbyapproximating
the necessary change at the input in order to achieve a specific change of the
output. Starting from the first-order Taylor approximation of an LLM
(cid:12) (cid:16) (cid:17)
h(L) ≈h˜(L)+ J (h(0))(cid:12) · h(0)−h˜(0) ,
LLM (cid:12) h(0)=h˜(0)
wedefinetheimportanceasthepseudo-inverseoftheJacobianJ (h(0))eval-
LLM
uated at the input embedding h˜(0) and the corresponding output embedding
h˜(L). Therefore, the final explanation
(cid:16) (cid:12) (cid:17)−1 (cid:16) (cid:17)
h˜(0)−h(0) ≈ J (h(0))(cid:12) · h˜(L)−h(L) (6)
LLM (cid:12)
h(0)=h˜(0)
is a measure of how a specific change of the output maps to the LLM’s input.
4 Empirical Analysis
In this section, we first describe the tasks we consider, then the evaluation met-
rics, and lastly the results per task. We perform our experiments with Google’s
Gemma 1.1 Instruct [29] (2B and 7B to assess model size impact) and Meta’s
Llama 3 Instruct [2] (8B) from huggingface4 leveraging their chat formats. All
our experiments have been performed using 8 NVIDIA RTX A5500 graphics
cards with 24GB of memory each. Our code is publicly available on GitHub5.
4 https://huggingface.co
5 k-randl/self-explaining_llmsEvaluating the Reliability of Self-Explanations in Large Language Models 7
4.1 Tasks and Data
We assess the previously described methods on two different tasks:
1. Food hazard classification on the Food Recall Incidents dataset [24]. It
contains the titles of official food recalls released by government and non-
government organizations. We use expert annotations6, identifying the spe-
cificreasonforrecallingtheproduct,toextractmatchingspansinthetexts.
We then randomly selected 200 texts and ask the model to classify the re-
call into one of the following classes: “biological” (77 texts), “allergens” (53),
“chemical” (29),“foreignbodies” (20),“organolepticaspects” (12),or“fraud”
(9). The final set of texts has 95 characters on average (min: 51, max: 209).
2. Sentiment classification on an annotated version of the Movie Review
Polarity dataset v2 [21,35]. We use the validation split which contains 200
labeledmoviereviews(100positive,100negative)fromIMDB7 withhuman-
annotatedspanscarryinghighinformationtowardsthetask.Foreachreview,
we extract a random one- to three-sentence-long snippet including at least
one annotated important span, and ask the LLM to classify the sentiment
of the snippet. We only use excerpts of the reviews to keep the task close
to our first task, reduce the duration of the experiments, and increase the
interpretabilityoftextsandexplanationsforqualitativeinspection.Thefinal
set of texts has an average length of 344 characters (min: 48, max: 986) and
up to six annotated spans.
The main difference between the tasks is that the first task has a very confined
set of important words (one to two per text), while the second task has the
important words spread out over the whole sample.
4.2 Evaluation metrics
Asapreprocessingstep,enablingthecomparisonofself-explanationstoanalytic
explanations, we convert span-based explanations to saliency maps: for extrac-
tiveself-explanationsandhumanannotations,wefindthefirstoccurrenceinthe
input text and compute the token indices. We then assign a saliency of 1 to
all tokens in these indices. For counterfactual self-explanations, we find tokens
of the original text that have been changed in the counterfactual and assign
saliencyof1.Tokensnotaffectedbythepreviousstepsareassignedasaliencyof
1·10−9 to avoid zero division errors. Afterward, we normalize all saliency maps
(LLM-generated, gradient, and attention-based) through division by their sum.
To assess the ability of an LLM to explain itself on the above tasks, we employ
three quantitative evaluation metrics in addition to a qualitative assessment.
6 Experts from AgroKnow
7 https://www.imdb.com/8 K. Randl et al.
Faithfullness: A generally agreed upon measure of explanation quality is a
faithfulness test by means of perturbing the model input [1,6,16]. In our case,
we iteratively prompt the LLM after masking the input tokens from the most
importanttotheleastimportant(“hightolow”)andviceversa(“lowtohigh”)in
steps of 0.2·|tokens|. If several tokens have the same importance, we randomly
selectone.ForGemma,wemaskwiththe<unk>token.AsLlama3doesnothave
a pre-trained mask token, we mask with the token ###, which is often used for
obscuring texts and should therefore be understood by the LLM. Intuitively, for
a faithful explanation method, the class predicted by the LLM should change
very early in the high to low test, as the removal of important tokens should
alter the output, while it should change very late in the low to high test, as the
removal of unimportant tokens should not alter the LLM’s assessment.
Text similarity: As there are multiple dimensions to measure the similar-
ity between two texts, we employ several measures to assess it: (i) Two sim-
ple complementing measures of token-count-based similarity are BLEU [22] and
ROUGE[15].Bothcomparen-gramoverlapsbetweenthecandidateandtheref-
erence translations. BLEU, which also adjusts for length discrepancies, empha-
sisesPrecision(normalisestheoverlapagainstthegeneratedtext)whileROUGE
emphasises Recall (normalises against the reference text). As both of them ig-
noretoken/n-gramorderwealsoapplythesimilarityratioprovidedinPython’s
difflib.SequenceMatcher: this measure counts all matching tokens in the or-
der of the reference and computes the ratio towards the average token count of
reference and candidate. (ii) To measure semantic similarity of a candidate to-
wardsareferenceweemployBARTScore[34].Contrarytothepreviousmetrics,
this metric, based on the similarity of transformer embeddings, is defined on
a logarithmic scale and therefore produces scores between negative infinity (no
semantic similarity) to zero (high semantic similarity).
Similarityofsaliencymaps: Inordertocomparetwoexplanationsonfeature
importancelevel,wecomputePearson’sr per(input)textasameasureofcorre-
lation.Weprefercorrelationoverothermetrics(suchasaccuracy)ascorrelation
respects the order of tokens. As normal distributions cannot be automatically
assumed for these correlations, we present violin plots instead of average values.
4.3 Results
Food hazard classification: As shown in Table 2 all models show moderate,
butaboverandom,zero-shotperformanceonthefirsttask.AtleastforGemma-
7BandLlama3-8B,wegetgradientvaluescompletelydifferentfrom0,indicating
that the gradients are not saturating and that gradient-based explanations are
clearly distinguishable from noise.
Figure 1 a) shows the per-text-correlation of the human annotations and
the explainability methods. The figure shows a clear positive correlation of the
humanannotationswithextractiveself-explanationsandGradIn.Counterfactual
self-explanations, as well as AGrad, are positively correlated for Llama 3 butEvaluating the Reliability of Self-Explanations in Large Language Models 9
Table 2. Performance of LLMs in entity extraction and sentiment classification. The
min and max gradients are computed per input text per task, and averaged.
Food Movies
F Gradient F Gradient
1 1
macro min max macro min max
Gemma-2B 0.30 −0.03 0.03 0.89 −0.02 0.02
Gemma-7B 0.36 −0.28 0.21 0.90 −0.19 0.13
Llama 3-8B 0.34 −1.05 1.04 0.95 −0.64 0.58
Correlationwithhumanspans: Correlationwithextractiveself-expl.:
1 1
0 0
1 1
1 vs. vs. vs. 1 vvss.. vvss.. vs. vs. vs.
extractive counterfactual AGrad GhuramdaInn counItGerrafadctual AGrad GradIn IGrad
0 0
1 1
vs. vs. vs. vvss.. vvss.. vs. vs. vs.
extractive counterfactual AGrad GhuramdaInn counItGerrafadctual AGrad GradIn IGrad
self-explanations
Gemma - 2B Gemma - 7B Llama 3 - 8B
Fig.1. Per text Pearson’s r correlation with human annotations (left) and the LLM’s
extractive self-explanations (right).
uncorrelated for the Gemma models. IGrad shows no clear correlation for any
of the models. The correlations of extractive self-explanations and the analytic
methods show a similar picture: GradIn in is positively correlated in all LLMs
while AGrad only correlates for Llama 3 and IGrad show no clear trend.
Toassesswhetherthesecorrelationsarelinkedtothefaithfulnessofthemeth-
ods, we provide the perturbation curves (see Section 4.2) in Figure 2: while all
curves show a clear early drop during “high-to-low” perturbation, proving that
theexplanationssuccessfullyindicatethemostimportanttokens,onlyLlama3is
abletoretainmodeloutputwhenremovingunimportanttokens(“low-to-high”).
Furthermore, the explanations of the two larger LLMs eventually change the
model output for perturbations in both directions, while Gemma-2B’s expla-
nations only change the output in up to 70% of the cases (i.e., 0.3 or more,
vertically).Furtherinvestigationshows,thatGemma-2Bpredictsallcompletely
obscuredtextstothemostsupportedclass“biological”.Thisleadstonotswitch-
ing the label for all 30% of samples that were originally predicted to this class.
Gemma-7B classifies the majority (92%) of occluded texts to the much less sup-
ported class “foreign bodies”. Llama 3’s output for completely occluded texts is
more useful, stating a missing text in 83% of cases. This different behavior of
the two model families may be tied to the use of different occlusion tokens.
dooF
)a
seivoM
)b
r
s'nosraeP
r
s'nosraeP
r
s'nosraeP
r
s'nosraeP10 K. Randl et al.
Faithfullness - Food Hazard
Gemma - 2B Gemma - 7B Llama 3 - 8B
100 100
75 75
50 bbeetttteerr bbeetttteerr bbeetttteerr 50
25 25
0 0
100 100
75
bbeetttteerr bbeetttteerr bbeetttteerr
75
50 50
25 25
0 0
0 50 100 0 50 100 0 50 100
Masked Tokens [%] Masked Tokens [%] Masked Tokens [%]
AGrad GradIn IGrad extractive counterfactual human random
Fig.2.Faithfulnesstestforfoodhazardclassification.Human-annotatedspans(brown)
helpmeasurehoweasyitistoguesstokenimportancefromanexternalpointofview.
In general extractive self-explanations, human labels, and IGrad show the
highestfaithfulnessoverallmodels.ForLlama3counterfactualsshowhighfaith-
fulness while Gemma’s counterfactuals are amongst the least faithful explana-
tions. Comparing faithfulness to Figure 1, we cannot find a clear connection.
Table 3. Counterfactual quality for both tasks. The reported similarity metrics are
the mean over all (generated) texts that successfully change the model output.
BART
Validity Similarity ROUGE-1 BLEU-1 ROUGE-L
Score
Gemma-2B 0.11 0.95 0.77 0.75 0.75 −2.37
Gemma-7B 0.41 0.94 0.79 0.70 0.73 −2.41
Llama 3-8B 0.29 0.97 0.83 0.89 0.86 −2.46
Gemma-2B 0.39 0.53 0.32 0.44 0.25 −3.17
Gemma-7B 0.95 0.67 0.52 0.61 0.49 −2.55
Llama 3-8B 0.94 0.85 0.81 0.83 0.81 −1.69
Thelowperformanceofthecounterfactualsforthefoodhazardclassification
task with the Gemma models is also shown in the upper part of Table 3: the
generated explanations only change the LLM output in less than half of the
samples.However,counterintuitively,weseeasimilarlybadvalidityinLlama3.
Further qualitative assessment of this artifact yields that all three models are
.pmi
wol
ot
hgih
.pmi
hgih
ot
wol
dooF
eivoM
]%[
stuptuO
degnahC
]%[
stuptuO
degnahCEvaluating the Reliability of Self-Explanations in Large Language Models 11
abletoidentifyimportanttokens,butdon’tconsistentlyreplacethemwithcoun-
terfactual evidence. The Gemma models rather highlight these with markdown
(e.g. “**salmonella**” instead of “salmonella”), while Llama replaces them with
other hazards from the same class (e.g. “e. coli” instead of “salmonella”). While
in both cases the LLM fails to produce valid counterfactuals, these observations
implythatmorepreciseprompts,suggestingaclasstochangeto,couldimprove
validity. Additionally, as in Table 3, similarity for valid counterfactuals is high.
Sentiment classification: For our second task, we see much higher F -scores
1
(see Table 2) compared to the first task, increasing with model size. Contrary,
the average per-text-extrema of the gradients are much lower. This is expected,
however,asinthistasktheimportanceisnotfocusedonafewtokens,butspread
outoverthecompletesampletext.AtleastforGemma-7BandLlama3-8B,the
values are large enough to exclude gradient saturation.
Contrary to the first task, the correlation plots in Figure 1 b) show a very
clearpicture:whilethehuman-annotatedspansareuncorrelatedtoalltheother
explainabilitymethods,weseevaryingdegreesofcorrelationwiththeextractive
self-explanations, indicating that for some samples the LLM exactly reproduces
the ground truth while for others it produces spans not overlapping at all. As
in the first task, the correlation of extractive self-explanations with the ana-
lytic methods mimics the correlation of human annotations with the respective
method, resulting in no correlation for all three methods.
Theresultsofthefaithfulnesstest,showninFigure3,aremuchlessclearthan
forthefirsttask:ForGemma-7BandLlama3-8B,themostfaithfulmethodsare
counterfactual self-explanations and human explanations. The remaining meth-
ods perform comparably over all models and directions. The figure also shows
that for this binary task, completely obscured texts only flip the label in 50%
of the cases: While Gemma-2B assigns all completely obscured samples to the
“negative” class, there is no clear trend for the remaining two LLMs. Contrary
to the first task, Llama 3 only states a missing review in 19% of the samples.
The finding that, contrary to the first task, prompting for counterfactuals
seems to work well for the sentiment classification task is reinforced by the
lower part of Table 3: For the two larger models we see successful flipping of the
predictedclassinatleast94%ofthecases.Whilecounterfactualsoverallachieve
acceptable similarity, Llama 3-8B achieves the overall best semantic similarity
for its counterfactuals.
5 Discussion and Conclusion
In all scenarios, self-explanations (incl. extractive) correlation with human an-
notationsishigheronaveragecomparedtoanalyticexplanations.Therefore,our
answer to the research question RQ1 is clearly yes.
Focusing on faithfulness (Figures 2, 3), we show that the extractive self-
explanations and the human-annotated ground truth perform better in the “low
tohigh” test.Thisfinding,however,doesnotnecessarilymeanhigherfaithfulness12 K. Randl et al.
Faithfullness - Movies
Gemma - 2B Gemma - 7B Llama 3 - 8B
100 100
80 80
60
bbeetttteerr bbeetttteerr bbeetttteerr
60
40 40
100 100
80
bbeetttteerr bbeetttteerr bbeetttteerr
80
60 60
40 40
0 50 100 0 50 100 0 50 100
Masked Tokens [%] Masked Tokens [%] Masked Tokens [%]
AGrad GradIn IGrad extractive counterfactual human random
Fig.3. Faithfullness test for the sentiment classification task. The human annotated
spans (brown) provide a measure of how easy it is to guess token importance from an
external point of view.
compared to analytic gradient/attention methods. Such methods often assign
high importance to syntactically significant tokens (e.g., punctuation), crucial
for language understanding but not necessarily important for the task.
Regarding RQ2, our presented results show that self-explanations are not
generally correlated with the internal LLM dynamics. While we find correla-
tionsbetweenextractiveself-explanationsandanalyticexplanationsforobjective
tasks with clear token/task dependencies, such correlations are not guaranteed
forsubjectivetasksthatrequirereadingbetweenthelines.Thedifferentformats
of LLM-generated explanations and analytic methods may partly account for
this. Our food hazard classification task, however, shows that correlations are
possible but not the rule.
Combining our answers to RQ1 and RQ2, we argue that extractive self-
explanations can be seen as the model’s most probable explanations based on
itstrainingdata.Iftherearecorrelationswithanalyticmethods,theyappearin
taskswithacleardependencybetweenspecifictokensandthesupervisionsignal.
In such cases, the correlation between self-explanations and analytic methods is
indirect,withbothbeingdirectlycorrelatedwiththegroundtruthbutnoteach
other. Regardless of this finding, there is no advantage of analytic explanations
over self-explanations in terms of faithfulness, as is known in literature [10]. As,
human explanations are also found to be faithful, we argue that faithfulness of
self-explanations stems from self-explanations mimicking human assessment.
.pmi
wol
ot
hgih
.pmi
hgih
ot
wol
]%[
stuptuO
degnahC
]%[
stuptuO
degnahCEvaluating the Reliability of Self-Explanations in Large Language Models 13
Counterfactuals WeseethatLLM-generatedcounterfactualscanproducehighly
faithfulandinformativeexplanationsforthesentimentclassificationtask.While
these do not necessarily explain the LLM’s internal processes as previously dis-
cussed,theycanprovideinformationonwhethertheLLMcorrectly“understood”
the assigned task. We further argue, that because counterfactuals can be easily
validated by feeding them through the LLM, and our finding that valid coun-
terfactuals were highly similar to the original in our experiments, they present
an effective alternative to SHAP and LIME and an interesting field of further
study. We also find that prompt tuning for counterfactuals is essential for their
success. Further research could examine provding specific classes to which the
original text should be changed or redactions as suggested by [19].
Limitations (i) We chose to limit our methods for creating self-explanations to
post-hoc and text-based approaches, as we consider them a natural way to use
LLMs.Otherapproaches,forexamplecreatingtheexplanationsatinferencetime
likeCoT[33],orpromptingfornumericalwordimportance[10]existbutarenot
consideredhere.(ii)Inthispublication,wedonotexplorehowdifferentprompts
affect the quality of the self-explanations tested in our experiments. We argue,
that this is highly dependent on the LLM in question and hard to generalize.
In our experiments we used a prompt we found working for all our tasks and
models. (iii) Due to hardware limitations we only experiment on models up
to 8B parameters. We are aware that larger models exist and can be accessed
through APIs, but we need to be able to modify the LLMs source code in order
to implement our attention- and gradient-based methods.
Acknowledgments. ThisworkhasbeenpartiallysupportedbyprojectMIS5154714
oftheNationalRecoveryandResiliencePlanGreece2.0fundedbytheEuropeanUnion
under the NextGenerationEU Program. Funding for this research has also been pro-
vided by the European Union’s Horizon Europe research and innovation programme
EFRA(GrantAgreementNumber101093026).FundedbytheEuropeanUnion.Views
andopinionsexpressedarehoweverthoseoftheauthor(s)onlyanddonotnecessarily
reflect those of the European Union or European Commission-EU. Neither the Euro-
pean Union nor the granting authority can be held responsible for them. ⋆⋆ ⋆⋆ ⋆⋆ ⋆⋆ ⋆⋆ ⋆⋆
Disclosure of Interests. The authors declare no competing interests.
References
1. Abnar,S.,Zuidema,W.:Quantifyingattentionflowintransformers.arXivpreprint
arXiv:2005.00928 (2020)
2. AI@Meta: Llama 3 model card (2024)
3. Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah,
M.,ÉtienneGoffinet,Hesslow,D.,Launay,J.,Malartic,Q.,Mazzotta,D.,Noune,
B., Pannier, B., Penedo, G.: The falcon series of open language models (2023)
4. Bach, S., Binder, A., Montavon, G., Klauschen, F., Müller, K.R., Samek, W.: On
pixel-wise explanations for non-linear classifier decisions by layer-wise relevance
propagation. PLoS ONE 10(7) (2015)14 K. Randl et al.
5. Brown, T., Mann, B., Ryder, N.e.a.: Language models are few-shot learners. In:
Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Advances in
NeuralInformationProcessingSystems.vol.33,pp.1877–1901.CurranAssociates,
Inc. (2020)
6. Chefer,H.,Gur,S.,Wolf,L.:Transformerinterpretabilitybeyondattentionvisual-
ization. In: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition (CVPR). pp. 782–791 (2021)
7. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-
rectional transformers for language understanding (2019)
8. Gurrapu,S.,Kulkarni,A.,Huang,L.,Lourentzou,I.,Batarseh,F.A.:Rationaliza-
tion for explainable nlp: a survey. Frontiers in Artificial Intelligence (sep 2023)
9. Hechtlinger,Y.:Interpretationofpredictionmodelsusingtheinputgradient(2016)
10. Huang,S.,Mamidanna,S.,Jangam,S.,Zhou,Y.,Gilpin,L.H.:Canlargelanguage
models explain themselves? a study of llm-generated self-explanations (2023)
11. Jiang,A.Q.,Sablayrolles,A.,Mensch,A.,Bamford,C.,Chaplot,D.S.,delasCasas,
D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L.R., Lachaux,
M.A.,Stock,P.,Scao,T.L.,Lavril,T.,Wang,T.,Lacroix,T.,Sayed,W.E.:Mistral
7b (2023)
12. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoy-
anov,V.,Zettlemoyer,L.:BART:Denoisingsequence-to-sequencepre-trainingfor
natural language generation, translation, and comprehension. In: Jurafsky, D.,
Chai,J.,Schluter,N.,Tetreault,J.(eds.)Proceedingsofthe58thAnnualMeeting
of the Association for Computational Linguistics. pp. 7871–7880. Association for
Computational Linguistics, Online (Jul 2020)
13. Li, J., Monroe, W., Jurafsky, D.: Understanding neural networks through repre-
sentation erasure (2017)
14. Li, Z., Xu, P., Liu, F., Song, H.: Towards understanding in-context learning with
contrastive demonstrations and saliency maps. CoRR abs/2307.05052 (2023)
15. Lin, C.Y.: ROUGE: A package for automatic evaluation of summaries. In: Text
Summarization Branches Out. pp. 74–81. Association for Computational Linguis-
tics, Barcelona, Spain (Jul 2004)
16. Liu,S.,Le,F.,Chakraborty,S.,Abdelzaher,T.:Onexploringattention-basedex-
planationfortransformermodelsintextclassification.In:2021IEEEInternational
Conference on Big Data (Big Data). pp. 1193–1203 (2021)
17. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining
approach (2019)
18. Lundberg, S.M., Lee, S.I.: A unified approach to interpreting model predictions.
In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan,
S., Garnett, R.(eds.) Advances in Neural InformationProcessing Systems 30, pp.
4765–4774. Curran Associates, Inc. (2017)
19. Madsen, A., Chandar, S., Reddy, S.: Are self-explanations from large language
models faithful? (2024)
20. Montavon, G., Lapuschkin, S., Binder, A., Samek, W., Müller, K.R.: Explaining
nonlinear classification decisions with deep taylor decomposition. Pattern Recog-
nition 65, 211–222 (May 2017), http://dx.doi.org/10.1016/j.patcog.2016.11.008
21. Pang,B.,Lee,L.:Seeingstars:Exploitingclassrelationshipsforsentimentcatego-
rizationwithrespecttoratingscales.In:ProceedingsofACL.pp.115–124(2005)
22. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic
evaluation of machine translation. In: Isabelle, P., Charniak, E., Lin, D. (eds.)Evaluating the Reliability of Self-Explanations in Large Language Models 15
Proceedings of the 40th Annual Meeting of the Association for Computational
Linguistics.pp.311–318.AssociationforComputationalLinguistics,Philadelphia,
Pennsylvania, USA (Jul 2002)
23. Raffel,C.,Shazeer,N.,Roberts,A.,Lee,K.,Narang,S.,Matena,M.,Zhou,Y.,Li,
W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text
transformer. J. Mach. Learn. Res. 21(1) (jan 2020)
24. Randl,K.,Karvounis,M.,Marinos,G.,Pavlopoulos,J.,Lindgren,T.,Henriksson,
A.: Food recall incidents (Mar 2024)
25. Ribeiro, M.T., Singh, S., Guestrin, C.: "why should i trust you?": Explaining the
predictions of any classifier. p. 1135–1144. KDD ’16, Association for Computing
Machinery, New York, NY, USA (2016)
26. Shrikumar, A., Greenside, P., Kundaje, A.: Learning important features through
propagating activation differences. In: Proceedings of the 34th International Con-
ference on Machine Learning - Volume 70. p. 3145–3153. ICML’17, JMLR.org
(2017)
27. Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.D., Ng, A., Potts, C.:
Recursivedeepmodelsforsemanticcompositionalityoverasentimenttreebank.In:
Yarowsky,D.,Baldwin,T.,Korhonen,A.,Livescu,K.,Bethard,S.(eds.)Proceed-
ingsofthe2013ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.
pp. 1631–1642. Association for Computational Linguistics, Seattle, Washington,
USA (Oct 2013)
28. Sundararajan,M.,Taly,A.,Yan,Q.:Axiomaticattributionfordeepnetworks.In:
Proceedings of the 34th International Conference on Machine Learning - Volume
70. p. 3319–3328. ICML’17, JMLR.org (2017)
29. Team,G.,Mesnard,T.,Hardin,C.,etal.:Gemma:Openmodelsbasedongemini
research and technology (2024)
30. Touvron,H.,Martin,L.,Stone,K.,etal.:Llama2:Openfoundationandfine-tuned
chat models (2023)
31. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
L., Polosukhin, I.: Attention is all you need (2017)
32. Verma, S., Boonsanong, V., Hoang, M., Hines, K.E., Dickerson, J.P., Shah, C.:
Counterfactual explanations and algorithmic recourses for machine learning: A
review (2022)
33. Wei,J.,Wang,X.,Schuurmans,D.,Bosma,M.,Ichter,B.,Xia,F.,Chi,E.H.,Le,
Q.V., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language
models. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A.
(eds.)AdvancesinNeuralInformationProcessingSystems35:AnnualConference
onNeuralInformationProcessingSystems2022,NeurIPS2022,NewOrleans,LA,
USA, November 28 - December 9, 2022 (2022)
34. Yuan, W., Neubig, G., Liu, P.: Bartscore: Evaluating generated text as text gen-
eration.In:Ranzato,M.,Beygelzimer,A.,Dauphin,Y.,Liang,P.,Vaughan,J.W.
(eds.) Advances in Neural Information Processing Systems. vol. 34, pp. 27263–
27277. Curran Associates, Inc. (2021)
35. Zaidan, O., Eisner, J.: Modeling annotators: A generative approach to learning
fromannotatorrationales.In:Lapata,M.,Ng,H.T.(eds.)Proceedingsofthe2008
Conference on Empirical Methods in Natural Language Processing. pp. 31–40.
Association for Computational Linguistics, Honolulu, Hawaii (Oct 2008), https:
//aclanthology.org/D08-1004
36. Zhao, H., Chen, H., Yang, F., Liu, N., Deng, H., Cai, H., Wang, S., Yin, D., Du,
M.: Explainability for large language models: A survey 15(2) (feb 2024)