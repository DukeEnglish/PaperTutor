On Policy Evaluation Algorithms in
Distributional Reinforcement Learning
Julian Gerstenberg gerstenb@math.uni-frankfurt.de
Institute of Mathematics
Goethe University Frankfurt
Frankfurt am Main, Germany
Ralph Neininger neiningr@math.uni-frankfurt.de
Institute of Mathematics
Goethe University Frankfurt
Frankfurt am Main, Germany
Denis Spiegel spiegel@math.uni-frankfurt.de
Institute of Mathematics
Goethe University Frankfurt
Frankfurt am Main, Germany
Abstract
Weintroduceanovelclassofalgorithmstoefficientlyapproximatetheunknownreturndis-
tributionsinpolicyevaluationproblemsfromdistributionalreinforcementlearning(DRL).
The proposed distributional dynamic programming algorithms are suitable for underlying
Markov decision processes (MDPs) having an arbitrary probabilistic reward mechanism,
includingcontinuousrewarddistributionswithunboundedsupportbeingpotentiallyheavy-
tailed.
For a plain instance of our proposed class of algorithms we prove error bounds, both
within Wasserstein and Kolmogorov–Smirnov distances. Furthermore, for return distribu-
tions having probability density functions the algorithms yield approximations for these
densities; error bounds are given within supremum norm. We introduce the concept of
quantile-spline discretizations to come up with algorithms showing promising results in
simulation experiments.
While the performance of our algorithms can rigorously be analysed they can be seen
as universal black box algorithms applicable to a large class of MDPs. We also derive new
propertiesofprobabilitymetricscommonlyusedinDRLonwhichourquantitativeanalysis
is based.
Keywords: distributional reinforcement learning, distributional dynamic programming,
distributional Bellman operator, Markov decision process, probability metrics
1 Introduction
InReinforcementLearning(RL)agentstakeactionsinanenvironmentinordertomaximize
a cumulative random reward, called return, with respect to its expectation. It has long
been also of interest to optimize other characteristics of the probability distribution of the
return than its expectation. In recent years, a systematic study of the practical benefits
and theoretical foundations of optimizing the return distribution has been taken up; for
a comprehensive account of this direction, called Distributional Reinforcement Learning
(DRL), see the recently published book Bellemare et al. (2023). In the present paper we
©2024JulianGerstenberg,RalphNeiningerandDenisSpiegel.
License: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/.
4202
luJ
91
]LM.tats[
1v57141.7042:viXraGerstenberg, Neininger and Spiegel
focus on Distributional Dynamic Programming (DDP) algorithms for distributional policy
evaluation, see Chapter 5 of Bellemare et al. (2023), i.e., on the construction and analysis
of algorithms which yield approximations of the return distribution of a Markov decision
process (MDP) when a fixed policy is being considered.
A crucial ingredient of any MDP is its reward mechanism, which specifies the rewards
given to an agent for state-action-state transitions. Such a mechanism implies return dis-
tributions which are characterised as fixed-points of the Distributional Bellman Operator
(DBO), we recall key results in Section 2. DDP algorithms have typically been devel-
oped and analysed for MDPs for which all reward distributions are finitely supported. A
probability distribution µ on R is finitely supported if and only if it has a representation
µ = (cid:80)m p δ with m ∈ N, p ≥ 0 probabilities summing to 1, support points x ∈ R and
i=1 i xi i i
δ the Dirac measure in x ∈ R. Finitely supported distributions are also called empirical
x
in DRL literature. The state of the art for DDP algorithms with finitely supported reward
distributions is covered by Chapter 5 of Bellemare et al. (2023). Roughly, most of these
algorithms operate by iterations of the DBO together with a projection step, which maps a
distribution to a fixed-size (i.e., fixed m) finitely supported distribution, see also Bellemare
et al. (2017); Maddison et al. (2017); Dabney et al. (2018); Rowland et al. (2018).
The aim of the present paper is to develop and analyse DDP algorithms that are univer-
sally applicable for a wide class of MDPs, including cases of MDPs in which (some of the)
rewarddistributionsarenotfinitelysupported,includingdistributionsthathaveunbounded
support and/or are continuous and/or are heavy-tailed. Such MDPs are of interest in appli-
cations, for example, in pricing and trading in insurance and finance, see Krasheninnikova
et al. (2019), Kolm and Ritter (2020) and the recent survey Hambly et al. (2023).
To be specific, our DDP algorithms are applicable to any MDP for which cumulative
distributionfunctions(CDFs)aswellasquantilefunctions(QFs)ofitsrewarddistributions
can be evaluated efficiently, e.g., if all reward distributions are contained in well-known
distributionfamiliesforwhichmethodstoevaluateCDFandQFareefficientlyimplemented
in popular software packages, e.g., in R or the Python-package SciPy.
TheDDPalgorithmsweareproposinginSection3alsooperatebyiterationsoftheDBO
together with projection steps that map arbitrary distributions to finitely supported ones.
To take up an idea of Devroye and Neininger (2002), where fixed-points of related recursive
distributional equations were approximated, we allow varying projections, i.e. projections
which depend on the update step. In particular, as Devroye and Neininger (2002), we allow
the sizes of the finitely supported representations to increase with each iteration. In the
context of the approximation of certain perpetuities this idea has also been applied, see
Knape and Neininger (2008). Furthermore, we allow projections to depend on previously
calculated approximations, which also seems to generalise previous approaches. A general
framework for our family of algorithms is presented in Section 3.
From Section 4 on we investigate the complexity and performance of our algorithms.
First, in Section 4 bounds are derived in an abstract setting for our general class of DDP
algorithms where we also address how to choose our varying projections, in particular
how to let the number of support points grow. We quantify the quality of the calculated
approximations by use of analysis metrics, such as Wasserstein distances. Our setting and
analysis does not require the projections to be non-expansive with respect to the analysis
metrics, cf. Chapters 4 and 5 in Bellemare et al. (2023) and the discussion in our Section
2Numerical Policy Evaluation in Distributional Dynamic Programming
4.2. Later, in Section 6, which is also of independent interest, we explain how to extend
these bounds to the Kolmogorov–Smirnov distance and how to obtain approximations of
densities of return distributions when they exist together with uniform error bounds; we
also offer a sufficient criteria for the return distributions to have densities at all and discuss
the relation to nonuniform random number generation in Remark 22.
ConcreteuniversalDDPalgorithmsaredefinedandanalysedinSection5. Theyareuni-
versal in the sense that they apply to the whole class of MDPs for which the CDFs and QFs
of all reward distributions can be efficiently evaluated, regardless of their nature (discrete,
continuous, unbounded support, heavy-tailed, etc.). Our projections are constructed such
that they only require that CDFs of the reward distributions can be evaluated. We discuss
the trade-off between space-time complexity and approximation quality for a plain instance
of the algorithms in Section 5.1. This yields further evidence that one should let the finitely
supported representation sizes increase with each iteration. For practical purpose, we sug-
gest to use the universal DDP algorithms presented in Sections 5.2 and 5.3, which are based
onboundingquantilesofconvolutions,respextivelyapproximatingquantilesbylinear-spline
interpolations. We conclude with a small simulation study in Section 5.4 to demonstrate
that our algorithms outperform simple Monte-Carlo estimation techniques significantly.
Our algorithms are reasonably applicable for small finite state-action spaces. For larger
spaces methods based on Markov chains exploring the state space and temporal difference
learning have been developed, see Chapter 6 of Bellemare et al. (2023) and Morimura et al.
(2010a) for a popular particle smoothing approach within Markov chain techniques.
1.1 Notations
LetB(R)betheBorelσ-fieldonRandP(R)bethesetofprobabilitymeasureson(R,B(R)).
Let P (R) ⊊ P(R) be the subset of finitely supported distributions. Every µ ∈ P(R)
fin
is specified by its cumulative distribution function (CDF) F : R → [0,1] as well by its
µ
quantile function (QF) F−1 : (0,1) → R defined as
µ
F (x) = µ(cid:0) (−∞,x](cid:1) and F−1(u) = inf{x ∈ R | F (x) ≥ u},
µ µ µ
related by u ≤ F (x) ⇔ F−1(u) ≤ x. The set P(R) is closed under convex combinations,
µ µ
whichtransferstotheCDF:F = qF +(1−q)F holdsforallq ∈ [0,1],µ,ν ∈ P(R).
qµ+(1−q)ν µ ν
Convex combinations do not transfer to the QF. A distribution µ ∈ P(R) is said to be
continuous if F is continuous and is said to possess the probability density function (PDF)
µ
f : R → [0,∞] if F (x) = (cid:82)x f (y)dy holds for all x ∈ R.
µ µ −∞ µ
It is convenient to work with random variables: we write L(X) = P[X ∈ · ] ∈ P(R) for
the distribution of a real-valued random variable (RV) X defined on some probability space
(Ω,F,P). ItisL(X) = µifandonlyifF (x) = P[X ≤ x]forallx ∈ Randarandomvariable
µ
X withL(X) = µisgivenbyX := F−1(U),whereU isaRVcontinuousuniformon(0,1). If
µ
it exists, let E[X] ∈ [−∞,∞] be the expectation of X. If P[X ≥ 0] = 1, then E[X] ∈ [0,∞]
exists. For α ∈ (0,∞) let P (R) ⊊ P(R) be the set of distributions µ = L(X) with
α
finite α-moment E[|X|α] < ∞. Note that P (R) ⊊ P (R) and that β < α implies
fin α
P (R) ⊊ P (R). For an event F ∈ F with P[F] > 0 let L(X|F) = P[X ∈ · |F] ∈ P(R)
α β
be the conditional distribution and E[X|F] = E[X1 ]/P[F] the conditional expectation of
F
X given F has occurred.
3Gerstenberg, Neininger and Spiegel
Finally, for sets A and I it will be convenient to write elements of AI as [a : i ∈ I] ∈ AI.
i
For real numbers x,y ∈ R we write x∨y = max{x,y} and x∧y = min{x,y}. We use the
Bachmann–Landau big-O notation.
2 Distributional Bellman operator and dynamic programming
A Markov Decision Process (MDP) with a fixed stationary policy is a tupel
mdp := (S,A,p,π,γ,r)
with finite set of states S ∋ s,s¯, finite set of actions A ∋ a, state-action-state transition
probabilitiesp = [p(s¯|s,a) : (s,a,s¯) ∈ S×A×S], inwhichp(s¯|s,a) ∈ [0,1]istheprobability
to make a transition from s to s¯ when performing action a, a fixed stationary policy π =
[π(a|s) : (s,a) ∈ S×A]inwhichπ(a|s) ∈ [0,1]istheprobabilityoftheagentchoosingaction
awheninstates,afixeddiscountfactorγ ∈ (0,1)andr = [r( · |s,a,s¯) : (s,a,s¯) ∈ S×A×S]
an arbitrary probabilistic reward mechanism, in which
r( · |s,a,s¯) = L(R ) ∈ P(R)
sas¯
is a probability distribution modelling the distribution of the immediate and possibly ran-
dom R-valued reward R given to the agent for a transition from state s to s¯having used
sas¯
action a. Let F be the CDF and F−1 : (0,1) → R be the QF of r( · |s,a,s¯) = L(R ).
sas¯ sas¯ sas¯
The ingredients of mdp give rise to the following dynamical process: at time t = 0 the
agent is in a uniform random initial state S(0) ∈ S. If at time t ∈ N the agent is in state
0
S(t), conditionally on S(t) and the past, the agent chooses a random action A(t) distributed
as π( · |S(t)). The environment reacts with a random successor state S(t+1) distributed as
p( · |S(t),A(t)). Theagentreceivesanimmediatereal-valuedrandomrewardR(t) distributed
as r( · |S(t),A(t),S(t+1)). The resulting stochastic process
(S(t),A(t),R(t))
t∈N
0
is called full MDP, and the return random variable is defined as
(cid:88)∞
G∗ = γtR(t) = R(0)+γR(1)+γ2R(2)+...
t=0
which exists with probability one as a R-valued random variable under the assumption
(A1) E[log(1∨|R |)] < ∞ for all s,a,s¯,
sas¯
we refer to Gerstenberg et al. (2023) for a precise result. We assume (A1) holds throughout
the paper, but consider stronger moment assumptions on reward distributions later, cf. Re-
mark4. OfinterestindistributionalpolicyevaluationisthedistributionofG∗ whenstarting
the full MDP in a fixed but arbitrary given state s ∈ S. The (state-)return distribution
η∗ ∈ P(R)S is the S-indexed collection of all these conditional distributions:
η∗ = [η∗ : s ∈ S] with η∗ = L(G∗|S(0) = s).
s s
We call η∗ the s-th component of η∗. The return distribution η∗ can rarely by described
s
analyticallyandthetaskofdistributionalpolicyevaluationalgorithmsistoapproximateη∗.
We discuss algorithms that are applicable in practice under the following two assumptions
on the ingredients of mdp:
4Numerical Policy Evaluation in Distributional Dynamic Programming
(A2) F (x),F−1(u) can be calculated in constant time for all s,a,s¯and x ∈ R,u ∈ (0,1),
sas¯ sas¯
(A3) p(s¯|s,a),π(a|s) ∈ [0,1] are known for all s,a,s¯.
Algorithms to approximate η∗ can be derived from the fact that η∗ is the unique fixed-
point of the Distributional Bellman Operator (DBO), which is introduced next. In the fol-
lowing, let η = [η : s ∈ S] = [L(G ) : s ∈ S] ∈ P(R)S be an arbitrary S-indexed collection
s s
of distributions and assume that the three collections of random variables (G ) ,(R )
s s sas¯ sas¯
and the full MDP (S(t),A(t),R(t)) are defined on a common probability space and are
t
independent.
Definition 1 The DBO is the map T : P(R)S → P(R)S, η (cid:55)→ Tη = [T η : s ∈ S] defined
s
by its component functions T : P(R)S → P(R) via T η := L(R(0)+γG |S(0) = s).
s s S(1)
Theorem 2 Under (A1) it holds that
(a) η∗ is the unique fixed-point of T, that is for every η ∈ P(R)S it is η = η∗ if and only
if η solves the so-called distributional Bellman equation Tη = η,
(b) for every η ∈ P(R)S it holds T◦nη → η∗, where T◦n = T ◦ ··· ◦ T is the n-th
w
iteration of T and → denotes (component-wise) weak convergence of distributions.
w
Proof IncaseE[|R |] < ∞foralls,a,s¯thisresultifwell-knownandfollows, forexample,
sas¯
from Proposition 4.34 in Bellemare et al. (2023). Under the more general assumption (A1)
it follows from Theorem 1 in Gerstenberg et al. (2023).
Theorem 2(b) leads to a theoretical approximation algorithm for η∗: choose an initial
approximation η(0) and calculate a sequence η(n),n ∈ N of approximations by the inductive
update rule η(n) = Tη(n−1). However, this is not practical for applications, as we explain
next. First, by conditioning on {A(0) = a,S(1) = s¯}, the s-th component of Tη equals
(cid:88)
T η = π(a|s)p(s¯|s,a)L(R +γG ). (1)
s sas¯ s¯
(a,s¯)∈A×S
The convolutions L(R +γG ) also equal (measure theoretic) convex combinations:
sas¯ s¯
(cid:90) (cid:90)
L(R +γG ) = L(R +γz)dη (z) = L(y+γG )dr(y|s,a,s¯). (2)
sas¯ s¯ sas¯ s¯ s¯
R R
Now, assume all components of η are finitely supported with m ∈ N particles, say η =
s
(cid:80)m
w δ . Combining (1) and (2) gives
i=1 si zsi
(cid:88) (cid:88)m
T η = π(a|s)p(s¯|s,a)w L(R +γz ). (3)
s s¯i sas¯ s¯i
(a,s¯)∈A×S i=1
Classical DDP algorithms are developed and analysed under the assumption
(A2.Fin): r( · |s,a,s¯) ∈ P (R) is finitely supported for all s,a,s¯,
fin
5Gerstenberg, Neininger and Spiegel
whichimplies(A2). If(A2.Fin)holds,thatiseveryrewarddistributionisfinitelysupported,
say of size N ∈ N with L(R ) = (cid:80)N q δ , formula (3) yields
sas¯ l=1 sas¯l r sas¯l
(cid:88) (cid:88)m (cid:88)N
T η = π(a|s)p(s¯|s,a)w q δ , (4)
s
(a,s¯)∈A×S i=1 l=1
s¯i sas¯l r sas¯l+γzs¯i
which is finitely supported of size (at most) m·|A|·|S|·N and a representation can be
calculated with a space-time complexity of order O(m·|A|·|S|·N), see Algorithm 5.1 in
Bellemare et al. (2023). As a consequence, if η(0) has finitely supported components of size
m, inductively calculating the n-th approximation η(n) = Tη(n−1) by applying (4) to η =
η(n−1) is possible in theory, but has a space-time complexity of order O(m·|A|n·|S|n·Nn),
which is prohibitive for applications.
If only (A2) holds, the situation is even more complicated: (3) shows that T η can be an
s
intricate mixture involving continuous distributions and/or distributions with unbounded
support, in particular it is, in general, no longer finitely supported. However, (3) also shows
that the CDF of T η at x ∈ R is given by
s
(cid:88) (cid:88)m
F (x) = π(a|s)p(s¯|s,a)w F (x−γz ), (5)
Tsη
(a,s¯)∈A×S i=1
s¯i sas¯ s¯i
whichcanbecalculatedexplicitlyunder(A2)withatime-complexityoforderO(m·|A|·|S|).
However, calculating the CDF of even the second iterate T◦2η is no longer possible exactly.
The idea of existing DDP algorithms that work under (A2.FIN) is to prevent the above
mentionedspace-timecomplexityblow-upbychoosingaprojection map Π : P(R) → P(R)
that maps any µ ∈ P(R) to a finitely supported distribution Π(µ) having a fixed-size m,
and to use the update rule η(n) = Π¯Tη(n−1), where Π¯ is the component-wise extension of Π
to a map P(R)S → P(R)S. Assuming Π(µ) can be calculated for any finitely supported
µ within controllable space-time-complexity, this leads to useful algorithms, which are dis-
cussed and analysed in Chapter 5 of Bellemare et al. (2023). Many of these algorithms are
not directly applicable under (A2), as the used projections can often not easily be evaluated
fordistributionswhicharenotfinitelysupported. Ourapproachistofocusonaspecialclass
of (parameterised) projections Π(·,ξ) in which Π(µ,ξ) depends on µ only by a controllable
number of evaluations of its CDF.
Remark 3 (State-Action return distributions) The so-called state-action-return dis-
tribution is defined as the (S × A)-indexed collection ζ∗ = [ζ∗ : (s,a) ∈ S × A] with
sa
ζ∗ = L(G∗|S(0) = s,A(0) = a). For convenience, we only discuss state-return distributions,
sa
but everything could be formulated analogously for the state-action case. This restriction is
a recurring theme in the field of distributional policy evaluation, see Section 4 of Morimura
et al. (2010b) or Chapter 5 of Bellemare et al. (2023).
Remark 4 (Moment Assumptions) Each of the following (parameterised) moment as-
sumptions on reward distributions implies (A1):1
(A1.Pα), α ∈ (0,∞) : E[|R |α] < ∞ for all s,a,s¯,
sas¯
1. Inthenotations(A1.Pα),(A1.Eλ),(A1.B)thePindicatespolynomialtails,theEexponentialtailsand
B bounded support, where α and λ specify the degree and rate respectively.
6Numerical Policy Evaluation in Distributional Dynamic Programming
(A1.Eλ), λ ∈ (0,∞) : E[exp(λ|R |)] < ∞ for all s,a,s¯,
sas¯
(A1.B) : there is [r ,r ] ⊂ R with P(cid:2) r ≤ R ≤ r (cid:3) = 1 for all s,a,s¯.
min max min sas¯ max
Condition (A1.B) implies (A1.Eλ) for every λ, (A1.Eλ) implies (A1.Pα) for every α and
(A1.Pα) implies (A1). The bounded rewards case (A1.B) is frequently encountered in the
(distributional) reinforcement learning literature, as well as (A1.Pα) with α = 1. All of
these moment assumptions transfer to the return distributions, let η∗ = L(G∗): (A1.Pα)
s s
implies E[|G∗|α] < ∞ for all s, (A1.Eλ) implies E[exp(λ|G∗|)] < ∞ for all s and (A1.B) via
s s
bounding interval [r ,r ] ⊂ R implies P[v ≤ G∗ ≤ v ] = 1 for all s with bounding
min max min s max
interval [v ,v ] = [rmin, rmax]. Also note that (A2.Fin) implies (A1.B), hence η∗ has
min max 1−γ 1−γ s
compact support in this case. In case of unbounded support, the design and analysis of DDP
algorithms is more challenging.
3 A general DDP Framework
A parameterised projection is a map
Π : P(R)×Ξ −→ P(R), (µ,ξ) (cid:55)−→ Π(µ,ξ),
where Ξ ∋ ξ is a set of parameters. Parameters are chosen by a parameter algorithm A,
that can depend on ingredients of mdp, and maps as
A : P(R)S ×S ×N −→ Ξ, (η,s,k) (cid:55)−→ A(η,s,k).
Given an initial approximation η(0) ∈ P(R)S, the maps Π and A are used to calculate a
sequence
(η(n),ξ(n)) ∈ P(R)S ×ΞS, n ∈ N (6)
inductively over n ∈ N as follows:
1. calculate ξ(n) = [ξ(n) : s ∈ S] with ξ(n) = A(η(n−1),s,n),
s s
2. calculate η(n) = [η(n) : s ∈ S] with η(n) = Π(T η(n−1),ξ(n) ).
s s s s
All concrete parameterised projections we consider in this paper calculate finitely sup-
ported distributions in which the size of Π(µ,ξ) ∈ P (R) depends on the parameter ξ.
fin
Further, as we allow A to depend on the ingredients of mdp and T is a function of mdp, the
parameters ξ(n) calculated in the n-th update step may depend on (properties of) Tη(n−1),
see Section 5.3. Finally, the DDP framework considered in Chapter 5 of Bellemare et al.
(2023) is included in this setting by considering Ξ to be a one-point set, we elaborate on
this in Section 4.2.
Example 1 (Quantile Dynamic Programming, QDP) Let M : N → N be a function,
which is a hyper-parameter of the algorithm. Let Ξ = N and
(cid:18) (cid:19)
1 (cid:88)m 2i−1
Π : P(R)×N → P(R), Π(µ,m) = δ with x = F−1 ,
m i=1 xi i µ 2m
A : P(R)S ×S ×N → N, A(η,s,k) = M(k).
7Gerstenberg, Neininger and Spiegel
Let η(0) have finitely supported components. The iterative procedure to calculate η(n),n ∈ N
as in (6) specifies to
(cid:18) (cid:19)
1 (cid:88)M(n) 2i−1
η(n) = δ with x = F−1 . (7)
s M(n) i=1 xi i Tsη(n−1) 2M(n)
Following Chapter of 5 of Bellemare et al. (2023), who introduce and analyse this algorithm
for M(k) ≡ m ∈ N constant and under (A2.Fin), we call this the Quantile Dynamic
Programming (QDP) algorithm. To perform (7) in practice, quantiles of T η(n−1) need to
s
be accessible, which holds under (A2.Fin) and leads to Algorithm 5.4 in Bellemare et al.
(2023), which can be easily modified to work for arbitrary size functions M. By analysing
the trade-off between approximation quality and time complexity, we argue to choose M(k)
growing exponentially in k. To be precise, choosing M(k) = ⌈(1/θ)k⌉ for some θ ∈ [γc,1),
cf. Example 2.
QDPisdirectlyapplicableonlyincase(A2.Fin),becausethen,foranyfinitelysupported
η, it is T η finitely supported, see (4), and hence its QF F−1 can be explicitly evaluated.
s Tsη
Under the more general assumption (A2), only the CDF F can be evaluated, see (5),
Tsη
which is the CDF of an intricate mixture of continuous and/or unbounded distributions.
Inverting CDFs is, in general, only possible with numerical approximation methods, which
canbecostly. Similarproblemsarisewhenaimingtodesignrandomnumbergeneratorsthat
are universally applicable for distributions µ in which only their CDFs can be evaluated,
we refer to Section 7 of H¨ormann et al. (2004) and Chapter II.2 in Devroye (1986). Note
that many of these numerical inversion algorithms assume some sort of regularity on the
CDF, which, in general, does not hold under (A2).
Concrete DDP algorithms applicable under (A2) are introduced and analysed in Sec-
tion 5, which starts by introducing a particular parameterised projection, see Figure 2 for a
visualisation. In Sections 5.1, 5.2 and 5.3 we discuss concrete parameter algorithms for this
projection. The latter leads to an approximation of the QDP algorithm applicable under
(A2), where quantiles are approximated by linear splines, but this approximation is on the
level of parameters of the projection, not on the level of designing a projection.
4 Bounding the approximation error
Let d : P(R)×P(R) → [0,∞] be an analysis metric on P(R), which is allowed to assign
infinite distance d(µ,ν) = ∞ to pairs µ ̸= ν.2 By triangle inequality, d is a proper finite
metric restricted to P (R) = {µ ∈ P(R) : d(µ,δ ) < ∞} ⊆ P(R).3 Any such d is
d 0
extended to a metric d¯ on P(R)S by d¯(η,η′) = max d(η ,η′), which is a proper finite
s∈S s s
metric on P (R)S ⊆ P(R)S. With respect to the analysis metric d, the quality of an
d
approximation η(n) for η∗ is measured by d¯(η(n),η∗). Two popular parameterised families
of analysis metrics are the following:
2. dsatisfiesd(µ,ν)=0⇔µ=ν andd(µ,ν)=d(ν,µ)andd(µ,ν)≤d(µ,ζ)+d(ζ,ν)forallµ,ν,ζ ∈P(R).
3. InChapter5ofBellemareetal.(2023)itisfurtherrequiredthatanyµ∈P (R)hasfinitefirstmoment.
d
8Numerical Policy Evaluation in Distributional Dynamic Programming
The β-Wasserstein distances w ,β ∈ (0,∞], are defined as
β
(cid:82)1
|F−1(u)−F−1(u)|βdu, β ∈ (0,1),
  0 µ ν
w β(µ,ν) =
(cid:104)
(cid:82) 01 |F µ−1(u)−F
ν−1(u)|βdu(cid:105)1/β
, β ∈ [1,∞),


sup |F−1(u)−F−1(u)|, β = ∞.
u∈(0,1) µ ν
We have P (R) = P (R) for β ∈ (0,∞) and P (R) is the set of distributions with
w β β w∞
bounded support.
The Birnbaum–Orlicz average distances ℓ ,β ∈ (0,∞], are defined as
β
(cid:82)∞
|F (x)−F (x)|βdx, β ∈ (0,1),
  −∞ µ ν
ℓ β(µ,ν) =
(cid:104)
(cid:82) −∞ ∞|F µ(x)−F
ν(x)|βdx(cid:105)1/β
, β ∈ [1,∞),


sup |F (x)−F (x)|, β = ∞.
x∈R µ ν
It is ks := ℓ the Kolmogorov–Smirnov distance, ℓ is also known as Cram´er distance
∞ 2
and ℓ = w . A complete characterisation of all spaces P (R),β ∈ (0,∞], in terms of
1 1 ℓ
β
moment assumptions is challenging. However, for our purpose, it is enough to observe
the following: for β = 1 we have P (R) = P (R) = P (R), in case β = ∞ it is
ℓ1 w1 1
P (R) = P (R) = P(R), thatisksisametriconthewholeofP(R), andforβ ∈ (1,∞)
ℓ∞ ks
we have
(cid:92)
P (R) ⊊ P (R) ⊊ P (R), (8)
β1 ℓ
β 0<ε<1
β1−ε
β
which is proved in Proposition 25 in Appendix A.1.
4.1 The Accumulated Projection Error
The following theorem is the key tool to study d¯(η(n),η∗), but also shows that not all of
the above introduced metrics are equally admissible as analysis metrics. Let c ∈ (0,∞) be
a fixed constant.
Theorem 5 (Theorem 4.25 of Bellemare et al. (2023)) Letdbec-homogeneous, reg-
ular and convex4. Then T is a γc-contraction with respect to d¯, that is
d¯(cid:0) Tη,Tη′(cid:1) ≤ γc·d¯(η,η′) for all η,η′ ∈ P(R)S (9)
and, consequently, for all η ∈ P(R)S:
(i) d¯(T◦nη,η∗) ≤ γcn·d¯(η,η∗) for all n ∈ N ,
0
(ii) d¯(η,η∗) < ∞ =⇒ d¯(η,η∗) ≤ d¯(η,Tη) .
1−γc
Of the above introduced metrics, the following have the properties stated in Theorem 5:
- d = w ,β ∈ (0,∞] with c = min{1,β},
β
4. See Definition 26 in Appendix A.2, also refer to Definitions 4.23-25 in Bellemare et al. (2023).
9Gerstenberg, Neininger and Spiegel
- d = ℓ ,β ∈ [1,∞) with c = 1/β.
β
In particular, ks = ℓ is not covered by Theorem 5.
∞
Remark 6 The inequalities stated in Theorem 5 are of interest in case the distances ap-
pearing in the upper bounds are finite. We have that
η,η′,η∗ ∈ P (R)S, T(P (R)S) ⊆ P (R)S =⇒ d¯(η,η′),d¯(η,η∗),d¯(η,Tη) < ∞.
d d d
By triangle inequality, (9) and properties of d it can be shown that the moment assumption
(A1.d) : L(R ) ∈ P (R) for all s,a,s¯
sas¯ d
implies T(P (R)S) ⊆ P (R)S. This alone may, in general, not be sufficient to conclude
d d
η∗ ∈ P (R)S, but for the metrics we consider the following holds: for d = w ,β ∈ (0,∞), it
d β
is P (R) = P (R), hence (A1.w ) is equivalent to (A1.Pβ) and this assumption implies
w β β β
η∗ ∈ P (R)S, see Remark 4. Similarly, P (R) is the set of distributions with bounded
β w∞
support, hence (A1.w ) is equivalent to (A1.B), which implies that all components of η∗
∞
have bounded support. Finally, we have P (R) ⊂ P (R) for β ∈ [1,∞), see Proposi-
1/β ℓ β
tion 25, hence (A1.ℓ ) is implied by (A1.P1), which implies η∗ ∈ P (R)S ⊆ P (R)S.
β β 1/β ℓ β
The Kolmogorov–Smirnov distance ks = ℓ does not satisfy the properties stated in
∞
Theorem 5 and thus is not as easily admissible as, for example, Wasserstein distances.
However, we demonstrate in Section 6 how bounds on approximation errors d¯(η(n),η∗)
using d = w for some β ∈ (0,∞) lead to useful bounds for ks(η(n),η∗).
β
For now, let d be a c-homogeneous, regular and convex metric on P(R). Let η(0) be an
initial approximation and let (η(n),ξ(n)),n ∈ N be the sequence calculated inductively as in
(6). A possible approach to bound the approximation error d¯(η(n),η∗) is as follows: define
the k-th projection error(PE) and the n-th accumulated projection error(APE) by
PE(k,d) = d¯(η(k),Tη(k−1)),
(cid:88)n
APE(n,d) = γc(n−k)·PE(k,d).
k=1
Proposition 7 d¯(cid:0) η(n),η∗(cid:1) ≤ APE(n,d) + γcnd¯(η(0),η∗).
Proof By induction on n ∈ N : for n = 0 it is obvious. Assuming the inequality holds for
0
somen ∈ N andnoticingtherecursiveformulaAPE(n+1,d) = PE(n+1,d)+γcAPE(n,d)
0
it follows
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
d¯ η(n+1),η∗ ≤ d¯ η(n+1),Tη(n) +d¯ Tη(n),η∗
(cid:16) (cid:17)
= PE(n+1,d)+d¯ Tη(n),Tη∗
≤ PE(n+1,d)+γcd¯(η(n),η∗)
(cid:16) (cid:17)
≤ PE(n+1,d)+γc APE(n,d) + γcnd¯(η(0),η∗)
= APE(n+1,d) + γc(n+1)d¯(η(0),η∗),
10Numerical Policy Evaluation in Distributional Dynamic Programming
where the first inequality follows from triangle inequality, the equality from Tη∗ = η∗, the
second inequality from Theorem 5 and the third by induction hypothesis.
Bounds on projection errors yield bounds on the accumulated projection error:
Lemma 8 Let r ∈ (0,∞),θ ∈ (0,1),D > 0. Denote with Li (z) = (cid:80)∞ k−szk the polylog-
s k=1
arithm of order s and argument z. Then the following implications hold
D
∀k = 1,...,n : PE(k,d) ≤ D =⇒ APE(n,d) ≤ ,
1−γc
D·Li (γc)
∀k = 1,...,n : PE(k,d) ≤ D·k−r =⇒ APE(n,d) ≤ −r ·n−r,
γc

 D ·γcn, θ < γc,
 (γc/θ)−1
∀k = 1,...,n : PE(k,d) ≤ D·θk =⇒ APE(n,d) ≤ D·n·γcn, θ = γc,

  D ·θn, θ > γc.
1−γc/θ
Proof The first and third implications follow from bounding finite geometric series by
infinite geometric series, resp. explicit evaluation in case θ = γc. The second implication
follows from 1/(n−k) ≤ (k +1)/n for every k = 0,...,n−1, bounding finite by infinite
series and plugging in the definition of the polylogarithm.
Proposition7incombinationwithLemma8showthatifprojectionerrorsdecaytowards
zero fast enough as k → ∞, then the approximation error d¯(η(n),η∗) also decays to zero
as n → ∞ with a comparable rate of decay. This allows to bound the minimal number of
iterations required to calculate an approximation η(n) that is ε-close to η∗ with respect to
d. By investigating the trade-off between time complexity and approximation quality, we
make a case for constructing DDP algorithms such that PE(k,d) decays exponentially with
some rate θ ∈ [γc,1), corresponding to the last (two) parts of the previous lemma, see the
following Example 2 and Section 5.1.
Example 2 (Analysing QDP with respect to d = w ) Let η(n),n ∈ N be as in Exam-
1
ple 1 having used the size function M : N → N, that is η(n) = Π(T η(n−1),M(n)) with
s s
Π(µ,m) =
m−1(cid:80)m
δ . We consider the analysis metric d = w = ℓ , which is
i=1 Fµ−1(2i−1/2m) 1 1
c = 1-homogeneous. To analyse QDP, we note the following inequalities:
(cid:90) 1
w (Π(µ,m),µ) ≤
2m (cid:0) F−1(1−u)−F−1(u)(cid:1)
du, w (µ,ν) ≤ w (µ,ν).
1 µ µ 1 ∞
0
As discussed, QDP is practical in case (A2.Fin) holds, which implies that reward dis-
tributions are supported on some compact interval [r ,r ] ⊂ R. Let [v ,v ] =
min max min max
[r /(1−γ),r /(1−γ)]. If all components of η ∈ P(R)S are supported on [v ,v ],
min max min max
then all components of Tη are supported on [v ,v ] and the same holds for the compo-
min max
nents of η∗. Assume η(0) is finitely supported with M(0) particles supported on [v ,v ].
s min max
11Gerstenberg, Neininger and Spiegel
Figure 1: (Partially) overlapping curves T (cid:55)→ log(e(n(T))) for γ = 0.7 and different size
functions; M(k) = ⌈(1/θ)k⌉ with θ ∈ [0.75,0.9] and M(k) ≡ m constant with
m ∈ {50,51,...,2000}. For each choice of M the curve has a single color.
By bounding quantiles of compactly-supported distributions by the endpoints of their sup-
port, using the above inequalities and plugging in the definitions of PE,APE as well as using
Proposition 7 we obtain
PE(k,w ) ≤ v max−v min , APE(n,w ) ≤ (cid:88)n γn−kv max−v min , w (η(n),η∗) ≤ e(n),
1 1 1
2M(k) k=1 2M(k)
where the upper bound e(n) on the n-th approximation error is
e(n) =
(cid:88)n γn−k[v max−v min]
+γn[v −v ].5 (10)
max min
k=1 2M(k)
Treating the sizes of S,A as well as the number of particles of reward distributions as
constants, the time complexity to calculate η(n) is of order O(time(n)) with
(cid:88)n
time(n) = M(k)log(M(k)),
k=1
compare to page 137 in Bellemare et al. (2023). To visualise the effect of choosing different
size functions M, let
n(T) = max{n ∈ N |time(n) ≤ T}
0
be the number of iterations QDP has calculated up to (a global constant of) time T > 0.
Thus, e(n(T)) is a guaranteed bound for the approximation error after execution time T.
Note that for e(n(T)) → 0 as T → ∞ it is necessary that M(k) → ∞ as k → ∞. Figure 1
shows plots of functions T (cid:55)→ log(e(n(T))) for different choices of sizes functions M: we
compare M(k) = ⌈(1/θ)k⌉ for different θ with M(k) ≡ m for different m. The plot suggests
to prefer the exponential choice with θ ≈ γ+1.
2
(cid:104) (cid:105)
5. ifM(k)≡m∈Nisconstant,e(n)canbefurtherboundedby[v −v ]· 1 + γn ,cf.Lemma
max min 2m(1−γ)
5.30 of Bellemare et al. (2023).
12Numerical Policy Evaluation in Distributional Dynamic Programming
4.2 Fixed Non-Expansive Projection
In case Ξ contains one element, Π can be reduced to a function Π : P(R) → P(R).
Let Π¯ : P(R)S → P(R)S be the component-wise extension of Π. The n-th calculated
approximation η(n) then takes the form η(n) = (Π¯T)◦n(η(0)) and the parameter algorithm
A as well as the sequence ξ(n),n ∈ N, can be eliminated from the formalism of Section 3.
This corresponds to the DDP framework considered in Chapter 5 in Bellemare et al. (2023).
Let d be c-homogeneous, regular and convex. Proposition 7 applies and leads to
(cid:88)n
d¯(η(n),η∗) ≤ γcnd¯(η(0),η∗)+ γc(n−k)d¯(η(k),Tη(k−1)). (APE-bound)
k=1
In case Π is a non-expansion with respect to d, the approximation errors d¯(η(n),η∗)
have been studied in Chapter 5 of Bellemare et al. (2023). To discuss how the bound
(APE-bound) relates we make the following assumptions:
(i) Πisanon-expansion withrespecttod,thatisd(Πµ,Πν) ≤ d(µ,ν)forallµ,ν ∈ P(R),
(ii) T(P (R)S) ⊆ P (R)S and Π¯(P (R)S) ⊆ P (R)S,
d d d d
(iii) Π¯T has a fixed-point ηˆ∈ P (R)S.
d
Notethat(i)impliesthatΠ¯T isaγc-contraction; (ii)impliesthatΠ¯T(P (R)S) ⊆ P (R)S.
d d
Inparticular, thefixed-pointηˆofΠ¯T isuniquewithinP (R)S. Furthermore, (i)+(ii)imply
d
(iii) in case the space (P (R)S,d¯) is complete. By triangle inequality, conditions (i)-(iii)
d
lead to
d¯(η(n),η∗) ≤ γcnd¯(η(0),ηˆ)+d¯(ηˆ,η∗). (ηˆ-bound)
Note that (ηˆ-bound) makes use of assumptions (i)-(iii), while (APE-bound) does not. How-
ever, under these additional assumptions, (APE-bound) can be further bounded and is seen
to be close to (ηˆ-bound): first, for every k ∈ N it holds that
d¯(η(k),Tη(k−1)) ≤ d¯(η(k),ηˆ)+d¯(ηˆ,Tηˆ)+d¯(Tηˆ,Tη(k−1))
≤ γckd¯(η(0),ηˆ)+d¯(ηˆ,Tηˆ)+γcd¯(ηˆ,η(k−1))
≤ 2γckd¯(η(0),ηˆ)+d¯(Tηˆ,ηˆ)
and hence
d¯(Tηˆ,ηˆ)
(APE-bound) ≤ γcnd¯(η(0),η∗)+2nγcnd¯(η(0),ηˆ)+ . (11)
1−γc
The only non-vanishing terms in the upper bounds (ηˆ-bound), resp. (11), are d¯(ηˆ,η∗), resp.
d¯(Tηˆ,ηˆ)/(1−γc). They are related by
max(cid:8) d¯(Π¯η∗,η∗),d¯(Tηˆ,ηˆ)(cid:9) min(cid:8) d¯(Π¯η∗,η∗),d¯(Tηˆ,ηˆ)(cid:9)
≤ d¯(ηˆ,η∗) ≤ , (12)
1+γc 1−γc
which follows from triangle inequality and contractive properties.6 Finally,
d¯(cid:0) Π¯η∗,η∗(cid:1)
(ηˆ-bound) ≤ γcnd¯(η(0),ηˆ) + ,
1−γc
1+γc
d¯(cid:0) Π¯η∗,η∗(cid:1)
(APE-bound) ≤ γcnd¯(η(0),η∗) + 2nγcnd¯(η(0),ηˆ) + · .
1−γc 1−γc
6. Also, (12) immediately implies η∗ =ηˆ⇔Πη∗ =η∗ ⇔Tηˆ=ηˆ.
13Gerstenberg, Neininger and Spiegel
The vanishing term in the final upper bound for (APE-bound) decays to zero faster
than θn for every θ ∈ (γc,1) and the non-vanishing term is the same as in (ηˆ-bound) up to
a factor (1+γc)/(1−γc).
Insummary,weconcludethatboundingapproximationerrorsbyaccumulatedprojection
errorsisaveryflexibleapproach,thatcanalsocovervaryingprojectionsorfixed-projections
that are expansive with respect to d. In the special case of fixed non-expansive projections,
(ηˆ-bound) appears to be sharper than (APE-bound), but for practical reasons the bounds
are close.
Remark 9 TheQDP-AlgorithmwithaconstantsizefunctionM(k) ≡ mfitstheframework
of a fixed-projection, Π = Π(·,m). Note that this projection is an expansion with respect to
w , that is the approximation errors cannot be bounded by (ηˆ-bound) directly; cf. Lemma
1
5.30 and Exercise 5.20 in Bellemare et al. (2023).
5 A class of DDP algorithms
AparameterisedprojectionΠ : P(R)×Ξ → P (R)thatiswell-suitedforDDPalgorithms
fin
under(A2), closelyrelatedtoquantization schemes ofLloyd(1982), isgivenasfollows: The
(cid:83)
parameter space is Ξ = Ξ with
m∈N m
(cid:110) (cid:12) (cid:111)
Ξ = ξ = (x ,...,x ,y ,...,y ) ∈ R2m−1 (cid:12) x ≤ y ≤ x ≤ y ≤ ··· ≤ y ≤ x .
m 1 m 1 m−1 (cid:12) 1 1 2 2 m−1 m
and Π(µ,ξ) ∈ P (R) is defined, for µ ∈ P(R),ξ = (x ,...,x ,y ,...,y ) ∈ Ξ , as
fin 1 m 1 m−1 m
(cid:88)m
Π(µ,ξ) = (F (y )−F (y ))·δ , (13)
i=1
µ i µ i−1 xi
with the convention y = −∞,y = +∞ and F (−∞) = 0,F (+∞) = 1, see Figure 2. In
0 m µ µ
particular, Π(µ,ξ) depends on µ only by a finite number of evaluations F , which makes
µ
it well-suited for DDP algorithms under (A2), as the CDF of µ = T η can be evaluated in
s
this case by (5). In particular, formula (5) directly yields the following:
Theorem 10 Let m′,m ∈ N and η = [η : s¯ ∈ S] ∈ P(R)S with η = (cid:80)m′ q δ . Let
ξ = (x ,...,x ,y ,...,y ) ∈ Ξ and
ss¯
∈ S. Then Π(T (η),ξ) =
(cid:80)s¯
m o
δj=1 wis¯ tj hzs¯j
1 n 1 m−1 m s i=1 i xi
(cid:88) (cid:88)m′
o = π(a|s)·p(s¯|s,a)·q ·[F (y −γz )−F (y −γz )].
i s¯j sas¯ i s¯j sas¯ i−1 s¯j
(a,s¯)∈A×S j=1
That is, Π(T (η),ξ) can be calculated under (A2)+(A3) in time of order O(m′·m).
s
Let η(0) ∈ P (R)S be an initial approximation. In order to calculate approximations
fin
η(n),n ∈ N using Π one has to decide for a suitable parameter algorithm
A : P(R)S ×S ×N −→ Ξ = ∪ m∈N Ξ m.
In order to control the space-time complexity, it is reasonable to choose a size function
M : N → N and to consider only parameter algorithms A such that
A(·,·,k) ∈ Ξ for all k ∈ N. (14)
M(k)
Given such a parameter algorithm, the iterative procedure to calculate η(n),n ∈ N is thus
14Numerical Policy Evaluation in Distributional Dynamic Programming
Figure 2: CDFs of µ (blue) and Π(µ,ξ) (red) with ξ = (x ,...,x ,y ,...,y ) ∈ Ξ . The
1 8 1 7 8
support of Π(µ,ξ) is the compact interval [z(ξ)−w(ξ),z(ξ)+w(ξ)] = [x ,x ] and
1 8
δ(ξ) = max |x −x |.
2≤i≤8 i i−1
1. Calculate ξ(n) = [ξ(n) : s ∈ S] with ξ(n) = A(η(n−1),s,n) ∈ Ξ ,
s s M(n)
2. Calculate η(n) = [η(n) : s ∈ S] with η(n) = Π(T η(n−1),ξ(n) ) as in Theorem 10.
s s s s
For practical purposes, A has to be such that A(η(n−1),s,n) can be evaluated under
Assumptions (A2)+(A3) within a controllable space-time complexity. In Remark 12 we
discussthetopicofoptimalparameteralgorithms anddemonstratethattheseare,ingeneral,
not easily available for practical purposes. In the following subsections, we consider three
different parameter algorithms A that can be applied under Assumptions (A2)+(A3).
(i) InSection5.1weintroduceplainparameteralgorithms(PPA).Byanalysingthetrade-
offbetweenapproximationqualityandtimecomplexityindetail, wecollectadditional
evidence that constructing DDP algorithms with projection errors decaying of the
order O(θk) with θ ∈ [γc,1) seems reasonable.
(ii) In Section 5.2 we introduce an adaptive version of the plain parameter algorithms
(ADP), which is derived by bounding quantiles of convolutions. A partial analysis
leadstoausefulblack-boxalgorithm,whichissuggestedforapplicationincase(A1.Pα)
holds for large α.
(iii) In Section 5.3 we modify the algorithm of Section 5.2 further by considering spline in-
terpolations of inverse quantile functions (QSP). The emerging algorithm can be seen
as an approximation of the QDP-algorithm applicable under assumptions (A2)+(A3)
and is a second black-box algorithm, that can be used also in presence of heavy-tailed
reward distributions, see assumption (A1.Pα).
ThemathematicalanalysisofSection5.1reliesonboundingprojectionerrors,wepresent
the basic tool now. Let ξ = (x ,...,x ,y ,...,y ) ∈ Ξ. Key-characteristics of ξ are
1 m 1 m−1
x +x x −x
m 1 m 1
δ(ξ) = max |x −x |, z(ξ) = , w(ξ) = .
i i−1
2≤i≤m 2 2
15Gerstenberg, Neininger and Spiegel
So, for any µ ∈ P(R) we have that Π(µ,ξ) is finitely supported on m points that lie in the
compact interval [z(ξ)−w(ξ),z(ξ)+w(ξ)], see Figure 2. As we deal with distributions that
can have unbounded support and the analysis metrics we consider (also) depend on the
tail behaviour of distributions, we need to introduce the following terms: for µ = L(X) ∈
P(R),ξ ∈ Ξ and β ∈ (0,∞) define
(cid:90) ∞
tail (µ,ξ) = xβ−1P[|X −z(ξ)| > w(ξ)+x] dx,
w
β
0
(cid:90) ∞
tail (µ,ξ) = P[|X −z(ξ)| > w(ξ)+x]β dx.
ℓ
β
0
The following is proved in Appendix A.3:
Lemma 11 Let µ ∈ P(R),ξ ∈ Ξ. Then
(cid:110) β 1 (cid:111)
(a) w β(Π(µ,ξ),µ) ≤ 4·max δ(ξ)1∨β, tail
w
β(µ,ξ)1∨β for all β ∈ (0,∞),
(cid:110) 1 1(cid:111)
(b) ℓ β(Π(µ,ξ),µ) ≤ 4·max δ(ξ)β, tail
ℓ
(µ,ξ)β for all β ∈ [1,∞).
β
Remark 12 (Optimal Parameter Algorithms) Letdbeac-homogeneous, regular, con-
vex analysis metric and consider only parameter algorithms that satisfy (14). Choosing a
parameter algorithm A with the property
A(η,s,k) ∈ argmin d(Π(µ,ξ),µ) with (µ,m) = (T η,M(k))
ξ∈Ξm s
for all η,s,k, would result in calculating approximations with minimal projections errors
with respect to d. The task to minimise d(Π(µ,ξ),µ) over ξ ∈ Ξ for given µ ∈ P(R) can,
m
in general, only be solved by approximation methods: for example, consider d = w . For
2
ξ = (x ,...,x ,y ,...,y ) ∈ Ξ it is
1 m 1 m−1 m
(cid:88)m
(cid:90) yi
w (Π(µ,ξ),µ) = (x−x )2dµ(x).
2 i
i=1
yi−1
Minimising this expression in ξ is the topic of the groundbreaking paper Lloyd (1982), which
has led to what is now known as Lloyd’s algorithm. This had motivated various other
algorithms for related minimisation problems (involving distributions on higher dimensions,
µ ∈ P(Rk)) such as (variants of) m-meansclusteringalgorithms or algorithms to construct
optimalcentroidalVoronoitessellations. We refer to Du et al. (1999) and Kloeckner (2012),
where these types of problems are discussed also for other metrics than d = w and in higher
2
dimensions. Regarding algorithms, Lloyd’s algorithm (and its variants) may depend on µ in
a more complex way than on finitely many evaluations of the cdf F . Hence, using these in
µ
our situation for µ = T η under (A2), would need to apply further approximation methods.
s
Considering the trade-off between space-time complexity and approximation quality, we leave
it for future research to investigate if parameter algorithms of such high complexity are of
any benefit for the DDP task at hand.
Finally, note that the minimisation problem min w (ν,µ) in which the minimum runs
ν 1
over all ν of the form ν = 1 (cid:80)m δ has been solved explicitly by letting x = F−1((2i−
m i=1 xi i µ
1)/2m), see Proposition 5.15 Bellemare et al. (2023), which was used to motivate the QDP
algorithm (Example 1) in that reference.
16Numerical Policy Evaluation in Distributional Dynamic Programming
5.1 Plain parameter algorithm (PPA)
For each m ∈ N,m ≥ 2,w > 0,z ∈ R let
ξlin(m,w,z) = (x ,...,x ,y ,...,y ) ∈ Ξ with
1 m 1 m−1 m
2i−1−m
x = z+ ·w,
i
m−1
2i−m
y = z+ ·w,
i
m−1
so the interlaced points (x ,y ,x ,y ,...,y ,x ) form an evenly spaced interpolation
1 1 2 2 m−1 m
of the compact interval [z − w,z + w]. Further, with ξ = ξlin(m,w,z) it holds that
(z(ξ),w(ξ),δ(ξ)) = (z,w, 2w ). In case m = 1 let ξlin(1,w,z) = (z) ∈ Ξ . The plain
m−1 1
parameter algorithm is defined as follows:
Definition 13 (PPA) The plain parameter algorithm (PPA) has hyper-parameter (func-
tions) M : N → N, W : N → (0,∞), z ∈ R with M,W non-decreasing and is defined as
A(η,s,k) := ξlin(M(k),W(k),z).
In the following, let A be as in Definition 13 and η(0) ∈ P (R)S with η(0) = δ for all
fin s z
s ∈ S. Set M(0) := 1. The time to calculate η(n), denoted by time(n), is dominated by the
applications of Π, thus
(cid:16)(cid:88)n (cid:17)
time(n) = O M(k−1)M(k) .
k=1
Ourgoalistoanalysethetrade-offbetweentime-complexityandapproximationerrorswith
respect to metrics d = w ,β ∈ (0,∞) and d = ℓ ,β ∈ [1,∞). For that, let
β β
2W(k)
δ(k) = δ(ξlin(M(k),W(k),z)) = .
M(k)−1
In case (A1.B), that is reward distributions have uniformly bounded support [r ,r ],
min max
the condition
(cid:20) (cid:21)
r r
min max
, ⊆ [z−W(k),z+W(k)] (15)
1−γ 1−γ
implies that all components of η∗,η(n),n ∈ N are supported on [z −W(k),z +W(k)] and
β 1
Lemma 11 yields PE(w β,k) ≤ 4δ(k)1∨β resp. PE(ℓ β,k) ≤ 4δ(k)β. A natural choice for
z,W to ensure (15) for all k is z = rmax+rmin and W(k) ≡ rmax−rmin (constant).
2(1−γ) 2(1−γ)
In presence of unbounded reward distributions, an analysis of the projection errors via
Lemma 11 is still possible, but the tail bound terms need more care: in the following, O-
notation is with respect to k → ∞; the constant may depend on all quantities except k and
concrete O-constants are presented in Appendix A.3, see Theorem 30.
(cid:16) (cid:110) β (cid:111)(cid:17)
Theorem 14 (a) It is PE(w β,k) = O max δ(k)1∨β, T(k) with
 −(α−β)
W(k) 1∨β , if (A1.Pα) holds with α > β,

 (cid:16) (cid:17)
T(k) = exp −λ(1−γ) W(k) , if (A1.Eλ) holds with λ > 0,
1∨β


0, if (A1.B) and (15) hold.
17Gerstenberg, Neininger and Spiegel
(cid:16) (cid:110) 1 (cid:111)(cid:17)
(b) It is PE(ℓ β,k) = O max δ(k)β, T(k) with
 −(α−1)
W(k) β , if (A1.Pα) holds with α > 1/β,

T(k) = exp(−λ(1−γ)W(k)), if (A1.Eλ) holds with λ > 0,

0, if (A1.B) and (15) hold.
In the following, we only consider d = w with some fixed β ∈ (0,∞). Let
β
n∗(ε) = min{n ∈ N | w (η(n),η∗) ≤ ε}, ε > 0.
β
Thefollowingresultinvestigatestheasymptoticbehaviouroftime(n∗(ε))asε → 0andleads
tosuggestthatletting thesize functionM increaseexponentially (withrate 1/θ,θ ∈ (γc,1),
case (b)), is superior to letting it grow polynomial (case (a)). We write f(n) = Θ(g(n)) if
there are constants C ,C > 0 such that C g(n) ≤ f(n) ≤ C g(n) for all n ∈ N.
1 2 1 2
Theorem 15 Assume (A1.Pα) holds with α > β and let h = 1∨β α > 1.
β α−β
(a) Let r > 0. Then
 w (η(n),η∗) = O(cid:0) n−r(cid:1) as n → ∞, 
 β 
 
 M(n) = Θ(cid:16) nhr(cid:17) ,      n∗(ε) = O(cid:16) (1/ε)1/r(cid:17) as ε → 0,     
   
W(n) =
Θ(cid:16) nαβhr(cid:17)

=⇒
 time(n) =
O(cid:16) n2hr+1(cid:17)
as n → ∞,  
 
 
   time(n∗(ε)) = O(cid:16) (1/ε)2h+ r1(cid:17) as ε → 0.   
(b) Let θ ∈ (γc,1) with c = min{1,β}. Then
 
w (η(n),η∗) = O(θn) as n → ∞,
  β  
 
 M(n) = Θ(cid:16) (1/θ)hn(cid:17) ,      n∗(ε) ≤ log(ϵ) +O(1) as ε → 0,     
   log(θ) 
=⇒
W(n) =
Θ(cid:16) (1/θ)αβhn(cid:17)
  time(n) =
O(cid:16) (1/θ)2hn(cid:17)
as n → ∞, 
 
 
 
   time(n∗(ε)) = O(cid:16) (1/ε)2h(cid:17) as ε → 0.    
Proof By Theorem 14(a) we have
(cid:32) (cid:40) (cid:18) (cid:19) β (cid:41)(cid:33)
W(k) 1∨β −(α−β)
PE(w β,k) = O max , W(k) 1∨β .
M(k)
Choosing M(k),W(k) as in (a), resp. (b) leads to PE(k,w ) = O(k−r), resp. PE(k,w ) =
β β
O(cid:0) θk(cid:1) . Hence, by Lemma 8, w (η(n),η∗) = O(n−r), resp. O(θn). This immediately leads
β
to the asymptotic bounds for n∗(ε) in both cases. The time complexity bounds follow from
(cid:88)n (cid:88)n (cid:16) (cid:17)
(k−1)hrkhr = Θ(n2hr+1), (1/θh)k−1(1/θh)k = Θ (1/θh)2n ,
k=1 k=1
18Numerical Policy Evaluation in Distributional Dynamic Programming
both as n → ∞. Plugging in the obtained asymptotic bounds for n∗(ε) in the time bound
yields the claim.
Comparingtheasymptoticexpressionsfortime(n∗(ε)),wefindthatchoosingM,W asin
(b),thatisenforcingexponentialdecaywithrateθ ∈ (γc,1)intheprojectionerrors,leadsto
aslowerasymptoticgrowthoftime(n∗(ε))asε → 0whencomparedtoenforcingpolynomial
decay as in (a). This is evidence for favouring exponential decay over polynomial decay.
When considering M,W as in (b) but with θ ∈ (0,γc), the same line of reasoning applies,
but requiring time(n∗(ε)) = O((1/ε)hlog(θ)/log(γc)), which grows even faster then choosing
M,W as in (a). The case θ = γc remains open as well as the question which θ ∈ [γc,1) may
be the preferred choice. Experiments, also see Example 2, suggest the heuristic θ ≈ γ+1
2
(when bounding with a c = 1 homogeneous analysis metric).
Remark 16 Similarconclusionscanbeobtainedwhenreplacingtheanalysismetricd = w
β
with d = ℓ ,β ∈ [1,∞) and/or strengthening the moment assumption (A1.Pα) to (A1.Eλ)
β
or even (A1.B). In all of these cases, using the corresponding parts of Theorem 14, it
is possible to choose M,W to enforce either polynomial or exponential decay of projection
errors. Averbatimanalysisoftime(n∗(ε))alwaysleadstothesameconclusion: constructing
algorithms such that projection errors decay exponentially with rate θ ∈ (γc,1) seems to be
preferable over polynomial decay.
We investigated the plain parameter algorithm to collect further evidence that choosing
M(k) = ⌈(1/θ)k⌉ with θ ≈ γ+1 is a reasonable black-box approach. However, for practical
2
purposes, the plain parameter algorithms is problematic: the k-th update step projects
T η(k−1) to a distribution supported on [z −W(k),z +W(k)]. If the parameters W,z are
s
chosen poorly, it may occur that, for reasonably small k, the interval [z−W(k),z+W(k)]
carriesonlya(very)smallamountofmassofT η(k−1) whichleadstoahighprojectionerror.
s
This is (partly) resolved by a modification of the plain parameter algorithm discussed in
the next section.
5.2 Adaptive Plain Parameter Algorithm
For practical purposes, we propose a modification of the plain parameter algorithm by
replacing [z−W(k),z+W(k)] with a compact interval that contains a guaranteed amount
of mass of T η(k−1). This modification is derived and justified by the following Lemma,
s
shown in Appendix A.4:
Lemma 17 Let η = [η s¯: s¯∈ S] ∈ P(R)S,ε
u
∈ (0,1/2] and s ∈ S. Define
(cid:110) √ √ (cid:111)
x = min
F−1(cid:0)
1− 1−ε
(cid:1) +γ·F−1(cid:0)
1− 1−ε
(cid:1)
, (16)
min
(a,s¯)
sas¯ u ηs¯ u
(cid:110) √ √ (cid:111)
x = max
F−1(cid:0)
1−ε
(cid:1) +γ·F−1(cid:0)
1−ε
(cid:1)
, (17)
max
(a,s¯)
sas¯ u ηs¯ u
where both min and max run over all pairs (a,s¯) ∈ A×S with π(a|s)p(s¯|s,a) > 0. Then
(cid:110) (cid:111)
max T η(−∞,x ), T η(x ,∞) ≤ ε and T η[x ,x ] ≥ 1−2ε .
s min s max u s min max u
19Gerstenberg, Neininger and Spiegel
As the QF of a given finitely supported distribution can be evaluated, the values x
min
and x can be calculated under Assumptions (A2)+(A3) for any η ∈ P (R)S. This
max fin
leads to the following:
Definition 18 (ADP) The adaptiveplainparameteralgorithm(ADP)hashyper-parameter
functions M : N → N, E
u
: N → (0,1/2] with M non-decreasing and E
u
non-increasing and
calculates A(η,s,k) ∈ Ξ as follows:
M(k)
1. (m,ε ) = (M(k),E (k)),
u u
2. Calculate x and x as in (16) and (17),
min max
3. A(η,s,k) = ξlin(m, xmax−xmin, xmax+xmin).
2 2
For practical purposes, we suggest to use (ADP) only if assumption (A1.Pα) is satisfied
with a large α, in which case we recommend the hyper-parameter functions
1 γ +1
M(k) = ⌈(1/θ)k⌉, E (k) = with θ = . (18)
u
2M(k) 2
In case (A1.Pα) does not hold for some large α, that is in the presence of heavy-
tailed reward distributions (where we recommend to use the QSP algorithm discussed in
Section 5.3), the discussion in Appendix A.5 leads to suggest that this choice of hyper-
parameters may not yield high quality approximations, which is in line with the results of
our controlled experiment in Section 5.4. The reasoning behind recommendation (18) is
discussed in Appendix A.5.
5.3 Quantile-Spline Parameter Algorithm
Choosing a parameter algorithm A that produces evenly spaced interpolation points is not
necessary to obtain high quality approximations. Aiming to approximate QDP, we now
modify the adaptive plain parameter algorithm of Section 5.2 leading to a DDP algorithm
that is applicable under (A2) and seems suitable also in presence of heavy-tailed reward
distributions.
Recall that QDP updates η ∈ P(R)S using m ∈ N particles as η˜ = 1 (cid:80)m δ
s m i=1 xi
with x = F−1 (cid:0)2i−1(cid:1) . In case T η is continuous, we have that η˜ = Π(T η,ξ˜) with
i Tsη 2m s s s
ξ˜ = (x ,...,x ,y ,...,y ) ∈ Ξ where x is as before and y = F−1 (cid:0) i (cid:1) . As ex-
1 m 1 m−1 m i i Tsη m
plained above, calculating quantiles of T η is problematic under the assumption (A2). We
s
suggest to replace ξ˜ ∈ Ξ with an approximation of quantiles ξ ∈ Ξ that uses only a
m m
controllable number of evaluations of the cdf F . Note that the approximation we suggest
Tsη
takes place on the level of the parameters: errors in the quantile approximation ξ ≈ ξ˜may
result in a non-optimal parameter, but by calculating Π(T η,ξ) the approximated quantiles
s
are reweighed according to the true distribution T η.
s
The suggested approximation of quantiles of T η is constructed as follows: first, we
s
calculate the interval [x ,x ] ⊂ R as in (16) and (17) with E (n) = 1 . Thus, this
min max u 2m
interval contains at least a mass of 1− 1 of T η. On that interval, we perform a linear
m s
interpolation of F via m′ ∈ N evaluations on evenly spaced points in [x ,x ]. This
Tsη min max
interpolated CDF is continuous and can easily be inverted; we define ξ to contain quantiles
20Numerical Policy Evaluation in Distributional Dynamic Programming
of this interpolated CDF. Linear interpolation and inversion can be combined in one step,
which is step 4 in the following:
Definition 19 (QSP) The hyper-parameters are M : N → N, M′ : N → N with M,M′
non-decreasing. The quantile-spline parameter algorithm (QSP) calculates A(η,s,k) ∈
Ξ as follows:
M(k)
1. (m,m′) = (M(k),M′(k)),
2. Calculate x ,x as in (16) and (17) with ϵ = 1 .
min max u 2m
3. Let z ← x +(x −x )· l for l = 0,1,...,m′+1,
l min max min m′+1
4. Let L : [0,1] → R be the linear spline through the points7
(0,z ), (F (z ),z ), (F (z ),z ),..., (F (z ),z ), (1,z ),
0 Tsη 1 1 Tsη 2 2 Tsη m′ m′ m′+1
5. A(η,s,k) = (x ,...,x ,y ,...,y ) with x = L(cid:0) i−1 (cid:1) and y = L(cid:0) 2i−1 (cid:1) .
1 m 1 m−1 i m−1 i 2m−2
We suggest to choose the hyper-parameter functions M,M′ as follows:
γ +1
M(k) = ⌈(1/θ)k⌉, M′(k) = ⌈(1/4)·(1/θ)k⌉ with θ = . (19)
2
5.4 Controlled Experiment
WeconsidertwoversionsofaMDPinwhichreturndistributionsareknownanalytically: let
A = {a} have one element and S = {1,2,3}, thus π(a|s) ≡ 1. State-action-state transitions
are deterministic and circular: p(s¯|s,a) = 1 :⇔ (s,s¯) ∈ {(1,2),(2,3),(3,1)}. Besides s and
γ := 0.7 the return distribution η∗ depends only on the three reward distributions L(R ),
s 1a2
L(R ), L(R ). For all (s,s¯) with (s,s¯) ∈/ {(1,2),(2,3),(3,1)} set L(R ) = δ . For
2a3 3a1 sas¯ 0
the three relevant reward distributions, we consider two versions. Let Norm(µ,σ2) be the
normal distribution with expectation µ ∈ R and variance σ2 ∈ (0,∞) and let Cauchy(µ,s)
betheCauchydistributionwithlocationµ ∈ Randscales > 0. ItisCauchy(µ,s) ∈ P (R)
α
for α ∈ (0,1), but Cauchy(µ,s) ∈/ P (R), while Norm(µ,σ2) ∈ P (R) for all α ∈ (0,∞).
1 α
The two versions we consider are:

L(R
)  Norm(−3,1)  L(η∗)  Norm(0.761,2.380)
1a2 1
(i) L(R 2a3) =  Norm(5,2)  =⇒ L(η 2∗) = Norm(5.373,2.816),
L(R ) Norm(0,0.5) L(η∗) Norm(0.533,1.666)
3a1 3

L(R
)  Cauchy(−3,0.5)  L(η∗)  Cauchy(0.761,4.597)
1a2 1
(ii) L(R 2a3) =  Cauchy(5,0.1)  =⇒ L(η 2∗) = Cauchy(5.373,5.852).
L(R ) Cauchy(0,5) L(η∗) Cauchy(0.533,8.218)
3a1 3
We compare three DDP algorithms to approximate (estimate) η∗: The two blackbox
algorithmsofSection5withparameteralgorithmsADP(Section5.2)andQSP(Section5.3)
and hyper-parameter choices as in (18) and (19) and initial approximation η(0) = [δ : s¯∈
0
7. remove all pairs (p l,z l) for which there is l′ ∈{0,...,l−1} with p l′ =p l.
21Gerstenberg, Neininger and Spiegel
alg time(s) size n ks w ℓ type
1 2
ADP 40.17 38868 54 0.0003 0.0007 0.0003 numerical
QSP 34.32 33036 53 0.0002 0.0025 0.0005 numerical
MC 45.00 15588 30 0.0125 0.0296 0.0132 random
MC2 134.11 38868 30 0.0079 0.0168 0.0067 random
Figure 3: Resultsformdp(i),wheresizeisthenumberofstoredparticles,resp. thenumber
of stored samples in MC estimation. Calculating the next approximation of ADP
and QSP exceeds 45 seconds.
alg time(s) size n ks w ℓ type
1 2
ADP 36.96 38868 54 0.2317 ∞ 0.6892 numerical
QSP 34.61 33036 53 0.0012 ∞ 0.0399 numerical
MC 44.99 14643 30 0.0138 ∞ 0.0967 random
MC2 146.22 38868 30 0.0080 ∞ 0.1285 random
Figure 4: Results for mdp (ii). Since Cauchy(µ,s) ∈/ P (R), the w -distances are infinite.
1 1
However, Cauchy(µ,s) ∈ P (R) ⊊ P (R), thus ℓ -distances are finite.
1/2 ℓ2 2
S]. We let both algorithms run up to time t = 45 seconds (on a standard notebook)
max
and return the last completely calculated approximation η(n) before that time. Third, we
consider a Monte-Carlo estimation (MC), for which we choose n = 30 fixed, approximate
η∗ ≈ T◦n(η(0)) and estimate the components of the latter by using that, for large N, we
haveT◦n(η(0)) ≈ 1 (cid:80)N δ ,whereG ,l = 1,2,... areiidsamplesofT◦n(η(0)) =
s N l=1 G s,n−1,l s,n−1,l s
L((cid:80)n−1γtR(t)|S(0) = s). AsampleofT◦n(η(0))canbegeneratedbysimulatingatrajectory
t=0 s
of the full MDP started in s up to length n and calculating the γ-discounted sum of rewards
along the way. For each s we generate N such samples, where N is the maximal amount
that is possible within t = 45 seconds. For comparison, we repeat the Monte Carlo
max
procedure, but with N chosen such that 3N (the amount of generated samples) equals the
size of the output of the ADP algorithm. The output of this second Monte Carlo estimation
is (MC2).
Each procedure returns some approximation (estimation) η(n). We report d¯(η(n),η∗) for
d = ks,w ,ℓ inFigures3and4. Formdp(i)itisseenthatbothADPandQSPoutperform
1 2
(MC, MC2) significantly for any analysis metric. The approximation qualities of ADP and
QSP are in a comparable range. Things differ for mdp (ii): still, QSP clearly outperforms
(MC, MC2) in any metric, but now ADP fails to yield useful approximations, which is to be
expected: the heuristics in choosing the hyper-parameters of ADP, see (18), fail for heavily
tailed distributions such as the Cauchy distributions.
22Numerical Policy Evaluation in Distributional Dynamic Programming
6 Kolmogorov–Smirnov distance and density approximation
We discussed how to obtain bounds for d¯(η(n),η∗) with respect to a c-homogeneous, regular
and convex analysis metric d, for example d = w ,β ∈ (0,∞] or d = ℓ ,β ∈ [1,∞). The
β β
Kolmogorov–Smirnov distance ks = ℓ is not covered by these results. In Section 6.1, we
∞
showhowtoboundks(η(n),η∗)intermsofw (η(n),η∗),β ∈ (0,∞)providedthecomponents
β
η∗ satisfy certain continuity properties, such as possessing a bounded pdf f∗. Further, if a
s s
pdf f∗ for η∗ is known to exists, we suggest to construct an approximation of it by choosing
s s
δ > 0 and considering f(n) = f , where, for any µ ∈ P(R) and δ > 0, the probability
s,δ ηs(n),δ
density f : R → [0,∞) is defined by
µ,δ
F (x+δ)−F (x−δ)
f (x) = µ µ , x ∈ R.
µ,δ
2δ
Under suitable regularity assumptions on f∗, we further show how the supremum distance
s
between f(n) and f∗ can be bounded in terms of ks(η(n),η∗). In Section 6.2, we provide
s,δ s
sufficient criteria for the return distribution to posses probability density functions (satisfy-
ing the needed regularity properties) that can, in principle, be checked based on the given
ingredients of the mdp.
6.1 Uniform Bounds
Let M > 0,ϱ ∈ (0,1]. A function g : R → R is ϱ-H¨older (continuous) with constant M if
|g(x)−g(y)| ≤ M ·|x−y|ϱ holds for all x,y ∈ R. Recall, that a distribution ν ∈ P(R)
has pdf f : R → [0,∞] if for all x ∈ R we have F (x) = (cid:82)x f (y)dy. If f is bounded
ν ν −∞ ν ν
by M > 0, then |F (x)−F (y)| ≤ M|x−y| and thus F is ϱ = 1-H¨older continuous with
ν ν ν
constant M.
Part (a) of the following Lemma is a slight generalisation of Lemma 5.1 in Fill and
Janson (2002), part (b) follows from Lemma 2.7 in Knape and Neininger (2008):
Lemma 20 Let µ,ν ∈ P(R).
(a) If F is ϱ-H¨older with constant M, then for every β ∈ (0,∞)
ν
ϱ
ks(µ,ν) ≤ a−a·M1−a·w (µ,ν)a(1∨β) with a = . (20)
β
ϱ+β
In particular, if ν has a pdf f bounded by M, then (20) holds with ϱ = 1.
ν
(b) If ν has a pdf f that is τ-H¨older with constant C, then for every δ ∈ (0,∞)
ν
1
sup|f (x)−f (x)| ≤ ks(µ,ν)+C ·δτ.
µ,δ ν
x∈R δ
If, additionally, f is bounded by M, then ks(µ,ν) in the upper bound can be further
ν
bounded by (20) with ϱ = 1.
Proof For (a) we note that Lemma 5.1 in Fill and Janson (2002) is the case in which ν
has a density bounded by M, which they used to bound max |F (x)−F (y)| ≤ Mεϱ
|x−y|≤ϵ ν ν
23Gerstenberg, Neininger and Spiegel
with ϱ = 1. It is easy to see that their proof extends to general ρ > 0. For (b), see Lemma
2.7 in Knape and Neininger (2008).
By passing to supremum distances over states, Lemma 20 yields the following:
Corollary 21 Let η(n) = [η(n) : s ∈ S] be any approximation of η∗ = [η∗ : s ∈ S].
s s
(a) If every F is ϱ-H¨older with constant M, then for every β ∈ (0,∞)
η∗
s
ϱ
ks(η(n),η∗) ≤ a−a·M1−a·w (η(n),η∗)a(1∨β) with a = . (21)
β
ϱ+β
In particular, if every η∗ has a pdf f∗ bounded by M, then (21) holds with ϱ = 1.
s s
(b) If every η∗ has a pdf f∗ that is τ-H¨older with constant C, then for every δ ∈ (0,∞)
s s
1
maxsup|f(n) (x)−f∗(x)| ≤ ks(η(n),η∗)+C ·δτ. (22)
s∈S x∈R s,δ s δ
If, additionally, every f∗ is bounded by M, then ks(η(n),η∗) in the upper bound can
s
be further bounded by (21) with ϱ = 1.
Remark 22 The approximation of densities in supremum norm has also an application in
nonuniform random number generation in the context of von Neumann’s rejection method;
for an introduction see Section II.3 of Devroye (1986). When applying the rejection method
it may not be possible to evaluate a given probability density to decide about rejection
resp. acceptance of a sample, e.g., the density may only be given implicitly as the density
of the fixed-point of a DBO. Then, to make the rejection method applicable it is sufficient
(besides constructing an appropriate dominating density) to be able to approximate the given
density to arbitrary precision in finite time as demonstrated in Devroye et al. (2000); De-
vroye and Neininger (2002). Since the big-O constants in our Corollary 21 can be made
explicit, the bound (22) is sufficient for the purpose of the rejection method. Note that for
perfect simulation from fixed-points of the DBO also coupling from the past algorithms may,
in principle, be applicable, see Devroye and James (2011); Dadoun and Neininger (2014).
For other potential use of Corollary 21 in the context of certain statistical functionals
see Chapter 20 of van der Vaart (1998).
6.2 Sufficient criteria for existence of return densities with properties
We provide sufficient criteria for when the return distribution components possess pdfs,
resp. bounded, resp. H¨older continuous pdfs. Let s 0 ∈ S and (S(t),A(t),R(t)) t∈N 0 be the
full MDP. We write P instead of P[ · |S(0) = s ], similar E . In particular, we have
s0 0 s0
η∗ = P [G ∈ · ] with G = (cid:80) γtR(t). Further, fix an arbitrary subset G ⊆ S ×A×S and
s0 s0 t
define the N ∪{∞}-valued random hitting time
0
T = min(cid:8) t ∈ N 0 (cid:12) (cid:12) (S(t),A(t),S(t+1)) ∈ G(cid:9) ,
with min∅ := ∞. Note that E [T] < ∞ implies G ≠ ∅ and P [T ∈ N] = 1.
s0 s0
24Numerical Policy Evaluation in Distributional Dynamic Programming
Theorem 23 SupposeE [T] < ∞andthatforevery(s,a,s¯) ∈ G thedistributionr( · |s,a,s¯)
s0
has a pdf f . Then η∗ has pdf f∗ given by
(s,a,s¯) s0 s0
(cid:18) (cid:19)
(cid:104) 1 x−Z (cid:105)
f∗ (x) = E f , x ∈ R, (23)
s0 s0 γT (S,A,S¯) γT
where (S,A,S¯) = (S(T),A(T),S(T+1)) ∈ G and Z = (cid:80) γtR(t).
t∈N 0\{T}
The proof of Theorem 23 can be found in the Appendix A.6. From (23) one can deduce
sufficient criteria for when η∗ has a bounded, resp. H¨older continuous pdf. Let Ψ(z) =
s0
E [ezT],z ∈ R be the moment generating function of T. Note that we may have Ψ(z) = ∞
s0
and that Ψ(z) < ∞ for some z > 0 implies E [T] < ∞.
s0
Corollary 24 In the setting of Theorem 23, let f∗ be the pdf of η∗ given by (23).
s0 s0
(a) If every f ,(s,a,s¯) ∈ G is bounded and Ψ(cid:0) log(1/γ)(cid:1) < ∞, then f∗ is bounded.
(s,a,s¯) s0
(cid:0) (cid:1)
(b) If every f ,(s,a,s¯) ∈ G is τ-H¨older continuous and Ψ (τ+1)log(1/γ) < ∞, then
(s,a,s¯)
f∗ is τ-H¨older continuous.
s0
The proof of Corollary 24 is contained in Appendix A.6.
Example 3 ConsiderS = {0,1},A = {0},s = 0and, forall(s,a) ∈ S×A, therewarddis-
0
tributions r( · |s,a,0) = δ and let r( · |s,a,1) have pdf f. Consider G = {(0,0,1),(1,0,1)}
0
and let α = p(1|0,0) ∈ (0,1). It then holds that P [T = k] = α(1−α)k,k ∈ N , that is
s0 0
T has a geometric distribution with parameter α. In particular, E [T] < ∞ and hence, by
s0
Theorem 23, η∗ has pdf f∗ given by (23), note that f = f in this case. By geometric
s0 s0 (S,A,S¯)
series arguments, Ψ(z) < ∞ ⇔ ez(1−α) < 1 and hence, by Corollary 24,
• f bounded and (1−α) < γ =⇒ f∗ is bounded,
s0
• f τ-H¨older continuous and (1−α) < γτ+1 =⇒ f∗ is τ-H¨older continuous.
s0
The presented conditions for η∗ to have a pdf (with properties) are not necessary, as
s0
illustrated by the following example, which also shows that finding sufficient and necessary
conditions for η∗ to possess a pdf (with properties) is a hard problem in general, we refer
s0
to Section 2.5 of Diaconis and Freedman (1999) for discussions of related examples.
Example 4 Let S = A = {0}, r( · |0,0,0) = 1δ + 1δ (fair coin toss) and γ = 1.
2 0 2 1 2
Then η∗ is continuous uniform on [0,2], hence possesses a bounded pdf, although the reward
0
distribution does not have a pdf. When the reward distribution is changed to r( · |0,0,0) =
qδ +(1−q)δ with q ∈ (0,1),q ̸= 1, then η∗ does not possess a pdf, although still having
0 1 2 s0
a H¨older continuous cdf.
Acknowledgments and Disclosure of Funding
The first author was partially supported by the Deutsche Forschungsgemeinschaft (DFG,
German Research Foundation) - 502386356.
25Gerstenberg, Neininger and Spiegel
Appendix A. Appendix
A.1 Characterisation of P (R) for β ∈ [1,∞)
ℓ
β
Proposition 25 P (R) = P (R) and for every β ∈ (1,∞) it holds that
ℓ1 1
(cid:92)
P (R) ⊊ P (R) ⊊ P (R).
β1 ℓ
β 0<ε<1
β1−ε
β
Proof Letµ = L(X) ∈ P(R). ItisE[|X|α] = α(cid:82)∞ xα−1P[|X| > x]dxforeveryα ∈ (0,∞)
and ℓ (µ,δ )β = (cid:82)∞(cid:0)P[X < x]β +P[X > x]β(cid:1) dx0 for every β ∈ [1,∞). Further, for every
x > 0β it is0 P[X <0 x]β +P[X > x]β ≤ P[|X| > x]β ≤ 2β−1(cid:0)P[X < x]β +P[X > x]β(cid:1) and
thus
(cid:90) ∞ (cid:90) ∞
µ ∈ P (R) ⇔ P[|X| > x]βdx < ∞ and µ ∈ P (R) ⇔ xα−1P[|X| > x]dx < ∞.
ℓ α
β
0 0
The conditions are equivalent in case β = α = 1 and P (R) = P (R) follows immediately.
ℓ1 1
Now, let β ∈ (1,∞). For the first inclusion, let µ ∈ P (R). By Markov’s inequality, for
1
β
every x > 0 it is P[|X| > x] ≤ E[|X|β1 ]x− β1 . Using β−1 ≥ 0 and −(β−1) = 1 −1 leads to
β β
(cid:90) ∞ (cid:90) ∞
P[|X| > x]βdx = P[|X| > x]β−1·P[|X| > x] dx
0 0
≤ (cid:90) ∞(cid:16) E[|X|β1 ]x− β1(cid:17)β−1 ·P[|X| > x] dx = β·E[|X|β1 ]β,
0
which is < ∞ by assumption. For the second inclusion, let µ ∈ P (R) and ε ∈ (0, 1).
ℓ β β
Using the integral formula for E[|X|β1−ε ] and applying H¨older’s inequality with p = β
β−1
and q = β gives
(cid:18) 1 −ε(cid:19)−1 ·E[|X|β1−ε ] = (cid:90) ∞ xβ1−ε−1 ·P[|X| > x] dx
β
0
(cid:20)(cid:90) ∞ (cid:104) (cid:105) (cid:21)β−1 (cid:20)(cid:90) ∞ (cid:21)1
≤ x
β1−ε−1 β−β
1dx
β
· P[|X| > x]βdx
β
.
0 0
(cid:104) (cid:105)
Thefirstfactoris< ∞because 1 −ε−1 β < −1andthesecondbyassumption. Tosee
β β−1
thatbothinclusionsarestrict, notethat(cid:82)∞ x−1log(x)−cdx < ∞ifandonlyifc > 1. Thus,
2
if µ ∈ P(R) has tail probabilities P[|X| > x] = x− β1 log(x)−1 for x ≥ 2 it is µ ∈ P
ℓ
(R),
β
but µ ∈/ P 1(R). Similar, if P[|X| > x] = x− β1 log(x)− β1 for x ≥ 2, it is µ ∈/ P
ℓ
(R) but
β
β
µ ∈ P for every ε ∈ (0, 1).
1−ε β
β
A.2 Definition of c-homogeneous, regular and convex metric
For a ∈ R and µ = L(X) ∈ P(R) let a#µ := L(aX). Further, for µ,ν ∈ P(R) let
µ∗ν ∈ P(R) be the convolution.
26Numerical Policy Evaluation in Distributional Dynamic Programming
Definition 26 Let c ∈ (0,∞),p ∈ [1,∞). An extended metric d : P(R)×P(R) → [0,∞]
is called
• c-homogeneous if for all γ ∈ [0,1] and µ,ν ∈ P(R) it is d(γ#µ,γ#ν) = γcd(µ,ν).
• regular if for all µ,ν,ϑ ∈ P(R) it is d(µ∗ϑ,ν ∗ϑ) ≤ d(µ,ν).
• p-convexifforallm ∈ Nand(α ,...,α ) ∈ [0,1]m with(cid:80)m α = 1and(µ ,...,µ ),
1 m i=1 i 1 m
(ν ,...,ν ) ∈ P(R)m it holds that dp((cid:80)m α µ ,(cid:80)m α ν ) ≤ (cid:80)m α dp(µ ,ν ).
1 m i=1 i i i=1 i i i=1 i i i
• convex if there exists p ∈ [1,∞) such that d is p-convex.
InclassicalliteratureonprobabilitymetricssuchasRachev(1991)thepropertiesc-homogeneous
plus regular are called (c,+)-ideal.
A.3 Proof of Theorem 14
For ξ = (x ,...,x ,y ,...,y ) ∈ Ξ define the map
1 m 1 m−1 m
pr( · |ξ) : R → {x ,...,x }, pr(x|ξ) = x :⇔ x ∈ (y ,y ].
1 m i i−1 i
Further, the push-forward of a measurable map f : R → R is denoted by f# : P(R) →
P(R) defined as f#(µ) = µ◦f−1. That is, µ = L(X) ⇒ f#(µ) = L(f(X)).
Lemma 27 For every µ ∈ P(R) and ξ = (x ,...,x ,y ,...,y ) ∈ Ξ it holds that
1 m 1 m−1 m
(a) Π(µ,ξ) = pr#(µ|ξ)
(b) F =
(cid:80)m+1F
(y )1 ( · ),
Π(µ,ξ) i=1 µ i−1 [xi−1,xi)
(c) F−1 = pr(F−1( · )|ξ),
Π(µ,ξ) µ
where we set y = x = −∞,y = x = ∞,F (−∞) = 0,F (∞) = 1.
0 0 m m+1 µ µ
Proof (a)isobviousfromthedefinitionsand(b)followsfrom(a)appliedtosetsoftheform
(−∞,x],x ∈ R. (c) follows from the fact the inverse quantile function F−1 is the unique
µ
left-continuous non-decreasing function (0,1) → R such that L(F−1(U)) = µ holds for U
µ
continuousuniformon(0,1). Now,pr(F−1(U)|ξ)hasdistributionpr#(µ|ξ)andpr(F−1(·)|ξ)
µ µ
is left-continuous and non-decreasing, because both pr(·|ξ) and F−1 are.
µ
Forµ = L(X) ∈ P(R),z ∈ R,w,β ∈ (0,∞)lettail (µ,z,w)=(cid:82)∞ xβ−1P[|X−z|>w+x]dx
w β 0
andtail
(µ,z,w)=(cid:82)∞P[|X−z|>w+x]βdx.
Inparticular,tail (µ,ξ) = tail (µ,z(ξ),w(ξ)),
ℓ β 0 w β w β
similar for tail .
ℓ
β
Proof [Proof of Lemma 11] (a) It is w β(Π(µ,ξ),µ) =
E[|pr(X|ξ)−X|β]1∨1
β by Lemma 27.
Further, x ∈ [x ,x ] implies |pr(x|ξ)−x| ≤ 2δ(ξ). Writing 1 ≡ 1(X ∈ [x ,x ])+1(X <
1 m 1 m
27Gerstenberg, Neininger and Spiegel
x )+1(X > x ), using linearity of expectation and the formula E[Zβ] = β(cid:82)∞ xβ−1P[Z >
1 m 0
x]dx we have
w (cid:0) Π(µ,ξ),µ(cid:1)1∨β = E[|pr(X|ξ)−X|β]
β
≤ 2βδ(ξ)β +E[(x −X)β1(X < x )]+E[(X −x )β1(X > x )]
1 1 m m
(cid:90) ∞
= 2βδ(ξ)β +β xβ−1(P[X < x −x]+P[X > x +x])dx
1 m
0
(cid:90) ∞
= 2βδ(ξ)β +β xβ−1P[|X −z(ξ)| > w(ξ)+x]dx
0
(cid:20) (cid:21)
β β
= 2β δ(ξ)β + tail (µ,ξ) ≤ 2β+1max{δ(ξ)β, tail (µ,ξ)}.
2β w β 2β w β
Taking both sides to the power of 1/(1∨β) and using 2(β+1)/(1∨β) ≤ 4 and ( β )1/(1∨β) ≤ 1
2β
yields the claim.
(b) By Lemma 27 the cdf of Π(µ,ξ) satisfies x ∈ [x ,x ) ⇒ F (x) = F (y ) for
i−1 i Π(µ,ξ) µ i−1
every i = 1,...,m+1 where x = −∞,x = ∞,y = −∞,y = ∞. Hence
0 m+1 0 m
(cid:90) ∞
ℓ (cid:0) Π(µ,ξ),µ(cid:1)β = |F (x)−F (x)|βdx
β µ Π(µ,ξ)
−∞
m (cid:88)+1(cid:90) xi
= |F (x)−F (y )|βdx
µ µ i−1
i=1
xi−1
(cid:90) x1 (cid:90) ∞ (cid:88)m
≤ F (x)βdx+ (1−F (x))βdx+ (x −x )|F (x )−F (x )|β
µ µ i i−1 µ i µ i−1
−∞ xm
i=2
(cid:90) x1 (cid:90) ∞
≤ F (x)βdx+ (1−F (x))βdx+δ(ξ)· max |F (x )−F (x )|β−1
µ µ µ i µ i−1
−∞ xm 2≤i≤m
(cid:90) ∞
≤ 2 P[|X −z(ξ)| > w(ξ)+x]βdx+δ(ξ) ≤ 4max{δ(ξ),tail (µ,ξ)}.
ℓ
β
0
Taking both sides to the power of (1/β) yields the claim.
Let η(k),k ∈ N be calculated using (PPA) with hyper-parameter (functions) M,W,z
and let ξ(k) = ξlin(M(k),W(k),z). Assume η(0) = δ .
s z
Lemma 28 Let tail = tail ,β ∈ (0,∞) or tail = tail with β ∈ [1,∞). Then for all
w β ℓ β
s ∈ S and k ∈ N it is
(cid:16) (cid:17)
tail(T η(k−1),ξ(k)) ≤ max tail L(R ),(1−γ)z,(1−γ)W(k) .
s sas¯
(a,s¯)∈A×S
Proof Write η(k−1) = [L(G ) : s¯∈ S] with (G ) independent of (R ) and the
k−1,s¯ k−1,s¯ s¯ sas¯ sas¯
full mdp (S(t),A(t),R(t)) t∈N 0, thus L(R(0)+γG k−1,S(1)|S(0) = s) = T sη(k−1). It holds that
P[|R(0)+γG −z| > W(k)+x|S(0) = s] ≤ maxP[|R +γG −z| > W(k)+x].
k−1,S(1) sas¯ k−1,s¯
a,s¯
28Numerical Policy Evaluation in Distributional Dynamic Programming
(k−1)
Since η is the output of the previous step of the algorithm, G is concentrated on
s k−1,s¯
[z−W(k−1),z+W(k−1)] ⊆ [z−W(k),z+W(k)], note that we assume W(k−1) ≤ W(k).
Writing0 = γz−γz,triangleinequalityyields|R +γG −z| ≤ |R −(1−γ)z|+γW(k).
sas¯ k−1,s¯ sas¯
Hence for every x > 0 it is
P[|(R +γG )−z| > W(k)+x] ≤ P[|R −(1−γ)z| > (1−γ)W(k)+x].
sas¯ k−1,s¯ sas¯
Combining these bounds yields the claim.
Let B(·,·) be the Beta function and Γ(·) the Γ-function
Lemma 29 Let µ = L(X) ∈ P(R) and z ∈ R,w > 0.
(a)
B(β,α−β)E[|X −z|α]
0 < β < α =⇒ tail (µ,z,w) ≤
w β wα−β
Γ(β)E[exp(λ|X −z|)]
λ > 0,β > 0 =⇒ tail (µ,z,w) ≤
w β λβexp(λw)
P[|X −z| ≤ w] = 1 =⇒ tail (µ,z,w) = 0.
w
β
(b)
1 E[|X −z|α]β
1 ≤ < α =⇒ tail (µ,z,w) ≤
β ℓ β (αβ−1)wαβ−1
E[exp(λ|X −z|)]β
λ > 0,β > 0 =⇒ tail (µ,z,w) ≤
ℓ
β λβexp(λβw)
P[|X −z| ≤ w] = 1 =⇒ tail (µ,z,w) = 0.
ℓ
β
Proof In both (a) and (b) the third implication is obvious. For the other implications,
apply Markov’s inequality as P[|X −z| > w+x] ≤ E[h(|X −z|)]/h(w+x) with functions
h(y) = yα (first implications in (a)+(b)) and h(y) = exp(λy) (second implications in both
(a) and (b)) and then explicitly calculate the bounding integrals. For the first implication
in (a) notice that (cid:82)∞ xβ−1 dx = B(β,α−β)w−(α−β), the other integrals are more elemen-
0 (w+x)α
tary.
Still, let η(k),k ∈ N be the output of PPA with parameter functions M,W,z and initial
(0)
approximations η = δ . In the following, let α,λ ∈ (0,∞) and define
s z
P(z,α) = maxE[|R −(1−γ)z|α] and E(z,λ) = maxE[exp(λ|R −(1−γ)z|)].
sas¯ sas¯
sas¯ sas¯
That is, (A1.Pα) holds iff P(z,α) < ∞ and (A1.Eλ) holds iff E(z,λ) < ∞. Further, set
2W(k)
δ(k) = .
M(k)−1
29Gerstenberg, Neininger and Spiegel
(cid:110) β (cid:111)
Theorem 30 (a) It is PE(w β,k) ≤ 4·max δ(k)1∨β, T(k) with
1 −α−β
– T(k) ≤ [B(β,α−β)P(z,α)]1∨β ·[(1−γ)W(k)] 1∨β in case β ∈ (0,α),
– T(k) ≤
(cid:2) Γ(β)λ−βE(z,λ)(cid:3) 1∨1
β
·exp(cid:16) −λ(1−γ) W(k)(cid:17)
in case β ∈ (0,∞),
1∨β
– T(k) = 0 in case P[Rsas¯ ∈ [z−W(k),z+W(k)]] = 1 for all s,a,s¯.
1−γ
(cid:110) 1 (cid:111)
(b) It is PE(ℓ β,k) ≤ 4·max δ(k)β, T(k) with
– T(k) ≤
(cid:2) (αβ−1)−1/βP(z,α)(cid:3) ·[(1−γ)W(k)]−(α− β1)
in case β ≥ 1,β > 1/α,
– T(k) ≤
(cid:2) (λβ)−1/βE(z,λ)(cid:3)
·exp(−λ(1−γ)W(k)) in case β ≥ 1,
– T(k) = 0 in case P[Rsas¯ ∈ [z−W(k),z+W(k)]] = 1 for all s,a,s¯.
1−γ
Proof We consider only d = w , the case d = ℓ can be shown similarly. Applying
β β
Lemma 11 leads to
(cid:110) β (cid:111)
PE(w β,k) = maxw β(Π(T sη(k−1),ξ(k)),T sη(k−1)) ≤ 4·max δ(k)1∨β, T(k)
s
1
with T(k) = max stail
w
(T sη(k−1),ξ(k))1∨β. Applying Lemma 28 leads to
β
(cid:16) (cid:17)
T(k)1∨β ≤ maxtail L(R ),(1−γ)z,(1−γ)W(k) .
w sas¯
s,a,s¯ β
The claimed w -bounds follow from Lemma 29.
β
A.4 Proof of Lemma 17
Let µ = L(X) ∈ P(R) and u ∈ (0,1). Some x ∈ R is a u-quantile of µ, that is x satisfies
P[X < x] ≤ u ≤ P[X ≤ x], if and only if x ∈ [F−1(u),F+1(u)], where the function
µ µ
F+1 : (0,1) → R is defined by F+1(u) = sup{x ∈ R|lim F (y) ≤ u}.
µ µ y↑x µ
Lemma 31 Let µ,ν ∈ P(R) with convolution µ∗ν ∈ P(R) and let u ∈ (0,1).
√ √
F−1(u) ≤ F−1( u)+F−1( u)
µ∗ν µ ν
√ √
F+1(u) ≥ F+1(1− 1−u)+F+1(1− 1−u).
µ∗ν µ ν
Proof Let X,Y be independent with L(X) = µ,L(Y) = ν, thus µ∗ν = L(X +Y). Let
√ √ √ √
x = F−1( u) and y = F−1( u), thus F (x) ≥ u and F (y) ≥ u. It follows
µ ν µ ν
√ √
F (x+y) = P[X +Y ≤ x+y] ≥ P[X ≤ x,Y ≤ y] = F (x)F (y) ≥ u u = u.
µ∗ν µ ν
For all z ∈ R the equivalence F (z) ≥ u ⇔ z ≥ F−1(u) holds. Applying this to z = x+y
µ∗ν µ∗ν
gives the first claimed bound. For the second claim, note F−1 (u) = −F+1 (1−u) holds
L(−Z) L(Z)
30Numerical Policy Evaluation in Distributional Dynamic Programming
for any RV Z. Applying the first bound to (µ′,ν′) = (L(−X),L(−Y)) leads, for every
v ∈ (0,1), to
√ √
−F+1(1−v) = F−1 (v) ≤ −(F+1(1− v)+F+1(1− v)).
µ∗ν L((−X)+(−Y)) µ ν
Multiplying by (−1) and substituting v = 1−u yields the claim.
Proof [Proof of Lemma 17] Let η = [L(G ) : s¯∈ S] with (G ) independent from (R ) .
s¯ s¯ s¯ sas¯ sas¯
Then T η(−∞,x ) = (cid:80) π(a|s)p(s¯|s,a)P[R +γG < x ]. For every relevant (a,s¯),
s min a,s¯ sas¯ s¯ min
using the definition of x , the fact that F−1 ≤ F+1 and that F+1 = γF+1 in combi-
min µ µ L(γX) L(X)
nation with the previous Lemma:
√ √
P[R +γG < x ] ≤ P[R +γG < F−1(cid:0) 1− 1−ε (cid:1) +γ·F−1(cid:0) 1− 1−ε (cid:1) ]
sas¯ s¯ min sas¯ s¯ sas¯
√
u ηs¯
√
u
≤ P[R +γG < F+1(cid:0) 1− 1−ε (cid:1) +γ·F+1(cid:0) 1− 1−ε (cid:1) ]
sas¯ s¯ sas¯ u ηs¯ u
≤ P[R +γG < F+1 (ε )] ≤ ε .
sas¯ s¯ L(Rsas¯+γGs¯) u u
To show T η(x ,∞) ≤ ε proceed similar and note that for each relevant pair (a,s¯) it is
s max u
P[R +γG > x ] = 1−P[R +γG ≤ x ]
sas¯ s¯ max sas¯ s¯ max
√ √
≤ 1−P[R +γG ≤ F−1(cid:0) 1−ε (cid:1) +γ·F−1(cid:0) 1−ε (cid:1) ]
sas¯ s¯ sas¯ u ηs¯ u
≤ 1−P[R +γG ≤ F−1 (1−ε )] ≤ 1−(1−ε ) = ε .
sas¯ s¯ L(Rsas¯+γGs¯) u u u
The bound T η[x ,x ] ≥ 1−2ε follows immediately.
s min max u
A.5 Choice of the hyper-parameter functions in (18)
The reasoning behind recommendation (18) is now explained: Let η(n) be the n-th output
(n)
when using arbitrary parameter functions M,E . By construction, η is finitely supported
u s
with particles (x ,p ),i = 1,...,M(n), where the x ’s are evenly spaced with distance
i i i
|x −x | being equal to
i+1 i
(cid:16) (cid:17) x (n,s)−x (n,s)
δ(n,s) := δ A(η(n−1),s,n) = max min ,
M(n)−1
where x (n,s),x (n,s) are given by (16) and (17) with η = η(n−1) and ϵ = E (n). In
min max u u
order to obtain high quality approximations, M and E should be chosen such that both
u
δ(n,s) → 0 and E (n) → 0 as n → ∞. The asymptotic behaviour of δ(n,s) can be bounded
u
as follows:
Theorem 32 Assume (A1.Pα) with α > 0 and set E (0) := 1. Then for every s ∈ S we
u
(cid:16) (cid:17)
have δ(n) := max δ(n,s) = O 1 (cid:80)n γn−kE (k)−1/α as n → ∞.
s∈S M(n) k=0 u
Proof Let
(cid:20) (cid:21)
(cid:16) (cid:112) (cid:17) (cid:16) (cid:112) (cid:17)
x (n,s) = min F−1 1− 1−E (n) +γF−1 1− 1−E (n)
min (a,s¯) sas¯ u η s( ¯n−1) u
(cid:20) (cid:21)
(cid:16)(cid:112) (cid:17) (cid:16)(cid:112) (cid:17)
x (n,s) = max F−1 1−E (n) +γF−1 1−E (n) .
max (a,s¯) sas¯ u η s( ¯n−1) u
31Gerstenberg, Neininger and Spiegel
√ √
With H(u) = max
(cid:2) F−1(cid:0) 1−u(cid:1) −F−1(cid:0)
1−
1−u(cid:1)(cid:3)
it is
sas¯ sas¯ sas¯
x (n,s)−x (n,s)
max min
(cid:20) (cid:21)
(cid:16)(cid:112) (cid:17) (cid:16) (cid:112) (cid:17)
≤ H(E (u))+γmax F−1 1−E (n) −F−1 1− 1−E (n)
n s¯ η(n−1) u η(n−1) u
s¯ s¯
≤ H(E (u))+γmax[x (n−1,s¯)−x (n−1,s¯)],
n max min
s¯
(n−1)
wherethesecondinequalityholdssinceη issupportedon[x (n−1,s¯),x (n−1,s¯)].
s¯ min max
Hence with W(n) := max [x (n,s)−x (n,s)] it is W(n) ≤ H(E (u))+γW(n−1).
s∈S max min n
(0)
Assuming each η it supported on [a,b] and iterating this inequality inductively down to
s
n = 0 yields W(n) ≤ (cid:80)n γn−kH(E (k))+γn(b−a) and hence
k=1 u
n
W(n) 1 (cid:88) b−a
δ(n,s) ≤ maxδ(n,s¯) ≤ ≤ γn−kH(E (k))+γn .
u
s¯∈S M(n)−1 M(n)−1 M(n)−1
k=1
Suppose (A1.Pα). By Lemma 33 there exists C > 0 with F−1(v)−F−1(1−v) ≤ C
√ sas¯ sas¯ (1−v)1/α
for all v ∈ (0,1) and s,a,s¯. Using 1/(1 − 1−u) ≤ 2/u for all u ∈ (0,1), it follows
H(u) ≤
C(cid:0)2(cid:1)1/α
. Thus, setting E (0) := 1, the previous display results in
u u
1
(cid:88)n (cid:18)
2
(cid:19)1/α
b−a
δ(n,s) ≤ γn−kC +γn ,
M(n)−1 E (k) M(n)−1
u
k=1
(cid:16) (cid:17)
which is O 1 (cid:80)n γn−kE (k)−1/α as claimed.
M(n) k=0 u
Now suppose (A1.Pα) holds, let β < α and set h = 1∨β α > 1. Theorem 32 shows:
β α−β
Choosing M(n) = O((1/θ)hn) and E u(n) = O(θβhn) leads to δ(n) =
O(cid:16) θ1∨ ββn(cid:17)
, similar to
Theorem 15. For ε > 0 let n(ε) = min{n ∈ N|max{E (n),δ(n)} ≤ ε} ∈ N. Using the
u
obtained asymptotic formulas shows that n(ε) ≤ max{ 1 , β }log(1/ε) + O(1) as ε → 0.
β·h 1∨β log(1/θ)
Thus, the time to calculate the n(ε)-th approximation is of order
O(cid:0) (1/ε)2r(β,α)(cid:1)
asymp-
totically as ε → 0, where r(β,α) = max{1, α }. The function (0,α) ∋ β (cid:55)→ r(β,α) takes
β α−β
its minimum at β = α with minimal value r( α ,α) = α+1. Choosing β = α leads to
α+1 α+1 α α+1
h = (cid:0)α+1(cid:1)2 and βh = 1 = α+1. Thus, it is reasonable to choose M(k) = O(cid:16) (1/θ)(α α+1)2 ·n(cid:17)
α β α
(cid:16) (cid:17)
and E u(k) = O
θα α+1·n
with a time to calculate the n(ε)’th approximation of order
(cid:16) (cid:17)
O
(1/ε)2·α α+1
asε → 0. Forpracticalpurposes, itseemsreasonabletosuggestthismethod
only in case α+1 > 1 is small, where the lower bound 1 is attained in the limit α → ∞. In
α
the limiting case, it is M(k) =
O(cid:0) (1/θ)k(cid:1)
and E (k) = O(1/M(k)), which leads to (18).
u
It remains to show:
Lemma 33 Let µ = L(X) ∈ P (R). Then for all u ∈ (0,1) we have
α
1 1
−E[|X|α]1/α· ≤ F−1(u) ≤ E[|X|α]1/α· .
u1/α µ (1−u)1/α
32Numerical Policy Evaluation in Distributional Dynamic Programming
Proof Let D := E[|X|α] < ∞. In case D = 0 it is P[X = 0] = 1 and F−1(u) = 0.
µ
Suppose D > 0. Markov’s inequality gives 1−F (x) = P[X > x] ≤ P[|X| > x] ≤ Dx−α for
µ
all x > 0, hence F (x) ≥ 1 − Dx−α for all x > 0. Applying this to x = (cid:0)1−u(cid:1)− α1 > 0
µ D
gives F (x) ≥ u, hence x ≥ F−1(u), which is the second bound. Applying the sec-
µ µ
ond bound to µ˜ = L(−X), u = 1 − q ∈ (0,1) and using F−1 (u) = −F+1 (1 − u)
L(−X) L(X)
gives F+1(q) ≥ −E[|X|α]1/α · 1 . The lower bound follows from this by F−1(q) ≥
µ q1/α µ
limsup F+1(q′) ≥ limsup −E[|X|α]1/α· 1 = −E[|X|α]1/α· 1 .
q′↑q q′↑q (q′)1/α q1/α
A.6 Proofs of Theorem 23 and Corollary 24
Proof [Theorem 23] The stochastic process SAS¯ = (S(t),A(t),S(t+1)) t∈N
0
is a random
N
variable taking values in the infinite product space (S ×A×S) 0, which has elements of
the form sas¯ = (s t,a t,s¯ t) t∈N 0. Let Υ := P s0[SAS¯ ∈ · ] be its distribution. Recall that
T = min{t ∈ N 0|(S(t),A(t),R(t)) ∈ G} ∈ N 0 ∪{∞}. Letting H ⊆ (S ×A×S)N 0 be the
subset of sequences visiting G we have {T < ∞} = {SAS¯ ∈ H} and hence Υ(H) = 1 by
assumption. Recall that G∗ = γTR(T) + Z with Z = (cid:80) γtR(t) and (S,A,S¯) :=
t∈N 0\{T}
(S(T),A(T),S(T+1)) ∈ G. Let P sas¯[(T,S,A,S¯,R(T),Z) ∈ ·],sas¯ ∈ (S × A × S)N 0 be a
regular conditional distribution of (T,S,A,S¯,R(T),Z) given SAS¯. In particular, for every
measurable set B ⊆ R we have
(cid:90)
η∗ (B) = P [G∗ ∈ B] = P [γTR(T)+Z ∈ B]dΥ(sas¯). (24)
s0 s0 sas¯
H
For Υ-almost all sequences sas¯ ∈ H we have, under P , that the random variables
sas¯
T ∈ N,(S,A,S¯) ∈ G are deterministic, R(T) ∼ r(·|S,A,S¯) has PDF f and R(T),Z
(S,A,S¯)
are independent. Recall that, if a > 0,b ∈ R are constants and X has PDF f, then
aX + b has PDF x (cid:55)→ 1f(x−b). If further Y is a RV independent of X, then aX + Y
a a
has PDF x (cid:55)→
E(cid:2)1f(x−Y )(cid:3)
, which follows from Fubini’s theorem. Hence, for Υ-almost
a a
all sequences sas¯∈ H it holds that the distribution of G∗ = γTR(T) +Z under P has
sas¯
(cid:16) (cid:17)
density x (cid:55)→ E [ 1 f x−Z ], where T,(S,A,S¯) are non-random under P with
sas¯ γT (S,A,S¯) γT sas¯
values depending on sas¯. Hence, for every measurable B ⊆ R it is
(cid:90) (cid:90) (cid:90) (cid:20) (cid:18) (cid:19)(cid:21)
1 x−Z
P [γTR(T)+Z ∈ B] dΥ(sas¯) = E f dx dΥ(sas¯)
sas¯ sas¯ γT (S,A,S¯) γT
H H B
(cid:90) (cid:90) (cid:20) (cid:18) (cid:19)(cid:21)
1 x−Z
= E f dΥ(sas¯) dx
sas¯ γT (S,A,S¯) γT
B H
(cid:90) (cid:20) (cid:18) (cid:19)(cid:21)
1 x−Z
= E f dx,
s0 γT (S,A,S¯) γT
B
where the second equation holds by Fubini’s theorem and the third is the key property of
regular conditional distributions. The claim follows in combination with (24).
Proof [Corollary24](a)LetM > 0besuchthat|f (x)| ≤ M forall(s,a,s¯) ∈ G,x ∈ R.
(s,a,s¯)
Since (S,A,S¯) ∈ G, it is |f (x)| ≤ M for all x ∈ R and thus, for every x ∈ R, it
(S,A,S¯)
33Gerstenberg, Neininger and Spiegel
follows |f∗ (x)| ≤ M ·E [γ−T] = M ·Ψ(log(1/γ)) < ∞. For (b), let C > 0 be such that
s0 s0
|f (x)−f (y)| ≤ C|x−y|τ for all x,y ∈ R and (s,a,s¯) ∈ G. This bound is also
(s,a,s¯) (s,a,s¯)
satisfied for the random (S,A,S¯) ∈ G and thus, for all x,y ∈ R,
(cid:104) 1 (cid:12) (cid:18) x−Z(cid:19) (cid:18) y−Z(cid:19)(cid:12)(cid:105)
|f∗ (x)−f∗ (y)| ≤ E (cid:12)f −f (cid:12)
s0 s0 s0 γT(cid:12) (S,A,S¯) γT (S,A,S¯) γT (cid:12)
(cid:104) 1 (cid:12)x−Z y−Z(cid:12)τ(cid:105)
≤ E ·C ·(cid:12) − (cid:12)
s0 γT (cid:12) γT γT (cid:12)
= C ·E (cid:2) γ−(τ+1)T(cid:3) ·|x−y|τ = C ·Ψ(cid:0) (τ +1)log(γ−1)(cid:1) · |x−y|τ,
s0
with C
·Ψ(cid:0)
(τ
+1)log(γ−1)(cid:1)
< ∞ by assumption.
References
Marc G. Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on re-
inforcement learning. In Proceedings of the 34th International Conference on Machine
Learning - Volume 70, ICML’17, page 449–458. JMLR.org, 2017.
Marc G. Bellemare, Will Dabney, and Mark Rowland. Distributional Reinforcement Learn-
ing. MIT Press, 2023. http://www.distributional-rl.org.
Will Dabney, Mark Rowland, Marc G. Bellemare, and R´emi Munos. Distributional rein-
forcement learning with quantile regression. In Proceedings of the Thirty-Second AAAI
Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial
Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artifi-
cial Intelligence, AAAI’18/IAAI’18/EAAI’18. AAAI Press, 2018. ISBN 978-1-57735-800-
8.
Benjamin Dadoun and Ralph Neininger. A statistical view on exchanges in Quickselect. In
ANALCO14—Meeting on Analytic Algorithmics and Combinatorics, pages40–51.SIAM,
Philadelphia, PA, 2014. doi: 10.1137/1.9781611973204.4.
Luc Devroye. Non-Uniform Random Variate Generation. Springer New York, NY, 1986.
doi: 10.1007/978-1-4613-8643-8.
LucDevroyeandLancelotF.James. Thedoublecftpmethod. ACM Trans. Model. Comput.
Simul., 21(2), feb 2011. ISSN 1049-3301. doi: 10.1145/1899396.1899398.
Luc Devroye and Ralph Neininger. Density approximation and exact simulation of random
variablesthataresolutionsoffixed-pointequations. Adv.inAppl.Probab.,34(2):441–468,
2002. ISSN 0001-8678. doi: 10.1239/aap/1025131226.
Luc Devroye, James Allen Fill, and Ralph Neininger. Perfect simulation from the Quicksort
limit distribution. Electron. Comm. Probab., 5:95–99, 2000. ISSN 1083-589X. doi: 10.
1214/ECP.v5-1024.
Persi Diaconis and David Freedman. Iterated random functions. SIAM review, 41(1):45–76,
1999.
34Numerical Policy Evaluation in Distributional Dynamic Programming
Qiang Du, Vance Faber, and Max Gunzburger. Centroidal voronoi tessellations: Applica-
tions and algorithms. SIAM review, 41(4):637–676, 1999.
James Allen Fill and Svante Janson. Quicksort asymptotics. J. Algorithms, 44(1):4–28,
2002. ISSN 0196-6774. doi: 10.1016/S0196-6774(02)00216-X. Analysis of algorithms.
Julian Gerstenberg, Ralph Neininger, and Denis Spiegel. On solutions of the distributional
Bellmanequation.Electron.Res.Arch.,31(8):4459–4483,2023. doi: 10.3934/era.2023228.
Ben Hambly, Renyuan Xu, and Huining Yang. Recent advances in reinforcement learning
in finance. Mathematical Finance, 33(3):437–503, 2023. doi: https://doi.org/10.1111/
mafi.12382.
WolfgangH¨ormann,JosefLeydold,andGerhardDerflinger. Automatic nonuniform random
variate generation. Springer Berlin, Heidelberg, 2004. doi: 10.1007/978-3-662-05946-3.
Benoit Kloeckner. Approximation by finitely supported measures. ESAIM: Control, Opti-
misation and Calculus of Variations, 18(2):343–359, 2012.
Margarete Knape and Ralph Neininger. Approximating perpetuities. Methodol. Comput.
Appl. Probab., 10(4):507–529, 2008. ISSN 1387-5841. doi: 10.1007/s11009-007-9059-x.
Petter N Kolm and Gordon Ritter. Modern perspectives on reinforcement learning in
finance. Modern Perspectives on Reinforcement Learning in Finance (September 6, 2019).
The Journal of Machine Learning in Finance, 1(1), 2020.
Elena Krasheninnikova, Javier Garc´ıa, Roberto Maestre, and Fernando Fern´andez. Rein-
forcement learning for pricing strategy optimization in the insurance industry. Engineer-
ing applications of artificial intelligence, 80:8–19, 2019.
StuartLloyd. Leastsquaresquantizationinpcm. IEEE transactions on information theory,
28(2):129–137, 1982.
Chris Maddison, Dieterich Lawson, George Tucker, Nicolas Heess, Arnaud Doucet, An-
driy Mnih, and Yee Teh. Particle value functions. In Proceedings of the International
Conference on Learning Representations (Workshop Track), 03 2017.
Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki
Tanaka. Nonparametric return distribution approximation for reinforcement learning. In
Proceedings of the 27th International Conference on International Conference on Ma-
chine Learning, ICML’10, page 799–806, Madison, WI, USA, 2010a. Omnipress. ISBN
9781605589077.
Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki
Tanaka. Nonparametric return distribution approximation for reinforcement learning. In
Proceedings of the 27th International Conference on International Conference on Ma-
chine Learning, ICML’10, page 799–806, Madison, WI, USA, 2010b. Omnipress. ISBN
9781605589077.
35Gerstenberg, Neininger and Spiegel
Svetlozar T. Rachev. Probability metrics and the stability of stochastic models. Wiley Series
in Probability and Mathematical Statistics: Applied Probability and Statistics. John
Wiley & Sons, Ltd., Chichester, 1991. ISBN 0-471-92877-1.
Mark Rowland, Marc Bellemare, Will Dabney, Remi Munos, and Yee Whye Teh. An anal-
ysis of categorical distributional reinforcement learning. In Amos Storkey and Fernando
Perez-Cruz, editors, Proceedings of the Twenty-First International Conference on Artifi-
cial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research,
pages 29–37. PMLR, 09–11 Apr 2018.
A. W. van der Vaart. Asymptotic statistics, volume 3 of Cambridge Series in Statistical
and Probabilistic Mathematics. Cambridge University Press, Cambridge, 1998. ISBN
0-521-49603-9; 0-521-78450-6. doi: 10.1017/CBO9780511802256.
36