1
Integrated Push-and-Pull Update Model for
Goal-Oriented Effective Communication
Pouya Agheli, Graduate Student Member, IEEE, Nikolaos Pappas, Senior Member, IEEE,
Petar Popovski, Fellow, IEEE, and Marios Kountouris, Fellow, IEEE.
Abstract—Thispaperstudiesdecision-makingforgoal-oriented accomplishinga specific goal. This promotessystem scalabil-
effective communication. We consider an end-to-end status up- ity and efficient resource usage by avoiding the acquisition,
date system where a sensing agent (SA) observes a source,
processing, and transportation of information turning out to
generatesandtransmitsupdatestoanactuationagent(AA),while
be ineffective, irrelevant, or useless.
the AA takes actions to accomplish a goal at the endpoint. We
integratethepush-andpull-basedupdatecommunicationmodels Messages, e.g., in the form of status update packets, are
to obtain a push-and-pull model, which allows the transmission communicated over existing networked intelligent systems
controllerattheSAtodecidetopushanupdatetotheAAandthe mostly using a push-based communication model. Therein,
query controller at the AA to pull updates by raising queries at
packets arrived at the source are sent to the destination based
specifictime instances. To gauge effectiveness, we utilizea grade
on decisions made by the source, regardless of whether or
of effectiveness (GoE) metric incorporating updates’ freshness,
usefulness, and timeliness of actions as qualitative attributes. not the endpoint has requested for or plans to utilize these
We then derive effect-aware policies to maximize the expected updates to accomplish a goal. In contrast, in a pull-based
discounted sum of updates’ effectiveness subject to induced model, the endpoint decides to trigger and requests packet
costs. The effect-aware policy at the SA considers the potential
transmissions from the source and controls the time and the
effectiveness of communicated updates at the endpoint, while
type of generated updates [4]–[10]. Nevertheless, this model
at the AA, it accounts for the probabilistic evolution of the
source and importance of generated updates. Our results show does not consider the availability of the source to generate
the proposed push-and-pull model outperforms models solely updatesortheusefulnessofthoseupdates.Toovercomethese
based onpush-orpull-basedupdatesboth intermsof efficiency limitations, we propose an integrated push-and-pull model
and effectiveness. Additionally, using effect-aware policies at
thatinvolvesbothagents/sidesinthedecision-makingprocess,
both agents enhances effectiveness compared to periodic and/or
thereby combining push- and pull-based paradigms in a way
probabilistic effect-agnostic policies at either or both agents.
that mitigates their drawbacks. In either model, decisions at
Index Terms—Goal-oriented effective communication, status
the source or endpoint could influence the effectiveness of
update systems, push-and-pullmodel, decision-making.
communicatedupdates.Therefore,we can categorizedecision
policiesintoeffect-awareandeffect-agnostic.Underaneffect-
I. INTRODUCTION awarepolicy,thesourceadaptsitsdecisionsbytakingintoac-
The emergence of cyber-physicalsystems empowered with countthe effectsof its communicatedpacketsat the endpoint.
interactive and networked sensing and actuation/monitoring Likewise, the endpoint raises queries based on the evolution
agentshas causeda shiftin focusfromextremeto sustainable of the source and the expected importance of pulled updates.
performance.Emergingnetworksaimtoenhanceeffectiveness Underthe effect-agnosticpolicy,however,decisionsare made
in the system while substantially improving resource utiliza- regardless of their consequent effect on the performance.
tion, energy consumption, and computational efficiency. The In this work, we study a time-slotted end-to-end status
key is to strive for a minimalist design, frugal in resources, update system where a sensing agentobservesan information
which can scale effectively rather than causing network over- source and communicatesupdates/observationsin the form of
provisioning.This design philosophyhas crystallized into the packetswithanactuationagent.Theactuationagentthentakes
goal-orientedand/orsemanticcommunicationparadigm,hold- actionsbased onthe successfullyreceivedupdatesas a means
ing the potential to enhance the efficiency of diverse network to accomplish a subscribed goal at the endpoint. We develop
processes through a parsimonious usage of communication an integrated push-and-pull model, which allows both agents
and computation resources [2], [3]. Under an effectiveness to make decisions based on their local policies or objectives.
perspective,a message is generatedand conveyedby a sender In particular, a transmission controller at the sensing agent
if it has the potential to have the desirable effect or the right decides to either send or drop update packets according to
impact at the destination, e.g., executing a critical action, for their potentialusefulnessat the endpoint.On the other side, a
querycontrollerattheactuationagentalsodeterminesthetime
P.Agheli andM.Kountouris arewiththeCommunication SystemsDept., instancesaroundwhichthe actuatorshouldperformactionsin
EURECOM,France, email: agheli@eurecom.fr.N.Pappas is with the
the form of raising queries. In that sense, effective updates
Dept. ofComputer andInformation Science, Linko¨ping University, Sweden,
email:nikolaos.pappas@liu.se.P.PopovskiiswiththeDept.ofElec- are those that result in the right impact and actuation at the
tronicSystems,AalborgUniversity,Denmark,email:petarp@es.aau.dk. endpoint. Those queries, however, are not communicated to
M. Kountouris is also with the Dept. of Computer Science and Artificial
the sensing agent. Instead, the actuation agent acknowledges
Intelligence, University ofGranada, Spain, email: mariosk@ugr.es.Part
ofthisworkispresented in[1]. the sensing agent of effective updates. With prior knowledge
4202
luJ
91
]TI.sc[
1v29041.7042:viXra2
... focusesonthefreshnessandtimelinessofinformation.Inthis
Timeslot1 Timeslot2 Timeslot3 Receiving the
acknowledgement work,weconsidermultipleinformationattributesandpropose
a grade of effectiveness metric to measure the effectiveness
Processes
of updates, which goes beyond metrics, including AoI, EAoI,
Observing Deciding on Sending Waitingfor
QAoI, on-demand AoI, Age of Incorrect Information (AoII)
thesource sendinganupdate theupdate anacknowledgement
[16], and Value of Information (VoI) [17], [18]. In particular,
wefocusonthefreshnessofsuccessfullyreceivedupdatesand
Deciding on Receiving Takingaction
thetimelinessofperformedactionsastwoattributesofinterest
raisingaquery theupdate (ifavailable)
throughthelinklevel,aswellasontheusefulness/significance
Evaluating the Measuringeffectiveness (semantics)oftheupdatestofulfillthegoalatthesourcelevel.
update’s significance andsendingthe
Thispaperextendsourpriorwork[1],whichonlyconsiders
Atsensingagent acknowledgement (ifany)
Atactuation agent a pull-based model and an effectiveness metric with two
Time freshness and usefulness attributes. As such, [1] conveys a
special form of the decision problem we solve here. In this
Fig. 1. A time diagram of processes involving the sensing and actuation
agents,illustratinginteractionsandupdatecommunicationsleadingtoactions. work, we generalize the problem to a push-and-pull commu-
nication model, considering that the sensing and actuation
agents individually make decisions and converge to a point
that the effectiveness of updates depends on the actuator’s
wheretheycantransmitupdatesandraisequeries,respectively,
availability to performactions,the sensing agentcaninfer the
which maximize the effectivenessof updatesand result in the
raised queries using those acknowledgments. A time diagram
right impact at the endpoint. Importantly, we assume that the
that shows the processes at both agents is depicted in Fig. 1.
source distribution is not known to the actuation agent, and
We introduce a metric to measure the effectiveness and the
thesensingagentdoesnothaveperfectknowledgeofthegoal.
significance of updates and derive a class of optimal policies
Therefore, the agents must estimate their required parameters
for each agent that makes effect-awaredecisions to maximize
separately. This approach is substantially different from the
the long-term expected effectiveness of update packets com-
oneinourpreviousworkandotherstate-of-the-artapproaches.
municated to fulfill the goal subject to induced costs. To do
so, the agent first needs to estimate the necessary system
parameters for making the right decisions. Our analytical B. Contributions
and simulation results show that the integrated push-and-pull The main contributions can be briefly outlined as follows.
model comes with a higher energy efficiency compared to
• We develop an integrated push-and-pull update com-
the push-based model and better effectiveness performance
munication model owing to which both agents have
compared to the pull-based one. Moreover, we observe that
decision-makingroles in the acquisition and transmitting
utilizing effect-aware policies at both agents significantly
ofupdatesandtakeappropriateactionstosatisfythegoal,
improves the effectiveness performance of the system in the
followingtheparadigmofgoal-orientedcommunications.
majority of the cases with a large gap compared to those of
With this, the system becomes adaptable from an effec-
periodic and probabilistic effect-agnostic policies at either or
tiveness viewpoint compared to the conventional push-
both agents. Accordingly,we demonstrate that the solution to
and pull-based models.
findan optimaleffect-awarepolicyat eachagentconvergesto
• We use a grade of effectiveness metric to capture the
a threshold-based agent decision framework where the agent
timely impact of communicated updates at the endpoint,
can timely decide based on an individuallookupmap in hand
which relies on the freshness of successfully communi-
and threshold boundaries computed to satisfy the goal.
cated updates, the timeliness of actions performed, and
the usefulness of those updates in fulfilling the goal.
A. Related Works
Our approach maps multiple information attributes into
This paper widely broadens prior work on push-based and a unique metric that measures the impact or effect each
(query-) pull-based communications by enabling both agents statusupdatepackettravelingoverthenetworkcan offer.
to make decisions so as to maximize the effectiveness of • We obtain optimal model-based control policies for
communicated updates in the system. The pull-based com- agents that make effect-aware decisions to maximize the
munication model has been widely analyzed but not limited discounted sum of updates’ effectiveness while keeping
to [4]–[10]. In [4], a new metric called effective age of the induced costs within certain constraints. To achieve
information (EAoI), which comprises the effects of queries this, we formulate an optimization problem, derive its
andthefreshnessofupdatesin theformoftheAoI[11]–[13], dual form, and propose an iterative algorithm based on
isintroduced.QueryAoI(QAoI),whichissimilartotheEAoI, dynamic programming to solve the decision problem
isutilizedin[5]–[9].FollowingthesameconceptastheQAoI, separately from each agent’s perspective.
on-demand AoI is introduced in [10], [14]. Probe (query)- • We demonstrate that the integrated push-and-pull model
based active fault detection where actuation or monitoring offershigherenergyefficiencythanthepush-basedmodel
agents adaptively decide to probe sensing agents to detect and better effectiveness performance compared to the
probable faults at the endpoint is studied in [15]. Most prior pull-based one. We also show that applying effect-aware
work has employed a pull-based communication model and policies at both agents results in better performancethan3
update communication. We assume all transmissions and E-
Subscribed goal ACK feedback occur over packet erasure channels (PECs),
with p and p′ being the erasure probabilities in the forward
ǫ ǫ
Update communication and the acknowledgment links, respectively.
channel
Therefore, an E-ACK is not received at the SA due to either
ineffective update communication or erasure in the acknowl-
Sensingagent Actuation agent
edgment(backward)channel.Withthisinterpretation,channel
E-ACK errorsleadto gracefuldegradationofthe proposedscheme.A
Transmission Querycontroller
controller raised query does not necessarily need to be shared with the
SA.AsdiscussedinSectionIII-C,theSAcandeducetheraise
Fig.2. End-to-endstatusupdatecommunicationtosatisfyasubscribedgoal. of a query or the availability of the AA to take action from a
successful E-ACK, given prior knowledge that an update can
be effective only if it arrives within the period during which
in the scenarios where one or both agents utilize effect-
the AA is available to act.
agnostic policies. We also broaden our results via deriv-
In this model, we consider the goal to be subscribed at
ing model-free decisions using reinforcement learning.
the endpoint, with the AA fully aware of it. On the other
Eventually,we provide a lookup map presenting optimal
hand,theSAdoesnotinitiallyknowthegoalbutlearnswhich
decisions for each agent that applies the effect-aware
updates could be useful to accomplish the goal based on the
policybased on the givensolution.Thisallows the agent
received E-ACK and observations’ significance. Meanwhile,
tomakedecisionsontimebymerelylookingupthemap
the AA is not aware of the evolution of the source or the
with the obtained threshold-based policy for the goal.
likely importance of observations, attempting to approximate
Notations: R, R+, R+, and Z+ indicate the sets of real, it from arrival updates. Consequently, the agents might use
0
positive real, non-negativereal, and positive integer numbers, different bases to measure the usefulness of the updates and
respectively. E[·] denotes the expectation operator, |·| depicts may need to adjust their criteria or valuation frameworks to
theabsolutevalueoperator, {·}istheindicatorfunction,and account for possible changes in goals over time. Finally, we
O(·) shows the growth rate of a function. assume that update acquisition, potential communication,and
waiting time for receiving an E-ACK occur in one slot.
II. SYSTEMMODEL
Weconsideranend-to-endcommunicationsysteminwhich A. Communication Model
asensingagent(SA)sendsmessagesinatime-slottedmanner
Thefollowingthreestrategiescanbeemployedforeffective
to an actuation agent (AA) as a means to take effective
communication of status updates.
action at the endpoint and satisfy a subscribed goal (see
1) Push-based: Under this model, the SA pushes its
Fig. 2). Specifically, the SA observes a source and generates
updates to the AA, taken for instance based on the source
status update packets in each time slot, and a transmission
evolution, without considering whether the AA has requested
controllerdecideswhether to transmitthat observationor not,
them or is available to take any action upon receipt. This
following a specific policy. We assume that the source has
bypasses the query controller, enabling the SA to directly
finite-dimensional realizations and that observation at the n-
influence actions at the AA side.
th, ∀n ∈ N, time slot is assigned a rank of importance v
n
2) Pull-based: In this model, the query controller plays a
from a finite set V = {ν | i ∈ I}, with I = {1,2,...,|V|},
i
central role in the generation of update arrivals at the AA by
based on its significance or usefulness for satisfying the goal,
pullingthoseupdatesfromtheSA.Here,theAAcanonlytake
measuredorjudgedatthesourcelevel.1TheelementsofV are
action when queriesare raised. However,this model excludes
independentandidenticallydistributed(i.i.d.)withprobability
the SA from generating and sending updates.
p =p (ν )forthe i-thoutcome,wherep (·) denotesa given
i ν i ν
probability mass function (pmf).2 3) Push-and-pull: This model arises from integrating the
push- and pull-based models so that the transmission and
TheAAisassistedbyaquerycontrollerthatdecidestoraise
query controllers individually decide to transmit updates and
queries and pull new updates according to a certain policy.
send queries, respectively. Thereby, the AA is provided with
A received packet at the AA has a satisfactory or sufficient
a level of flexibility where it is also able to take some actions
impact at the endpoint if that update achieves a minimum ef-
beyond query instances within a limited time. As a result,
fectivenesslevelsubjecttothelatestqueryraisedandtheAA’s
the effectivenessof an update packetdependson both agents’
availability to act on it. An effective update communication
decisions. Dismissing the decision of either agent transforms
is followed by an acknowledgment of effectiveness (E-ACK)
the push-and-pull model into the push- or pull-based model.
signalsentfromtheAAtotheSAtoinformabouttheeffective
1Todeterminetheusefulnessofanupdate,wecanusethesamemetavalue
B. Agent Decision Policies
approach proposedin[19,SectionIII-A].
2A moreelaborated model could consider the importance ofa realization We propose that the agents can adhere to the following
dependentonthemostrecentlygeneratedupdateattheSA.Thisimpliesthat
decision policies, namely effect-agnosticand effect-aware,for
alessimportant update increases thelikelihood ofamoresignificant update
occurring later, whichcanbecaptured utilizing alearning algorithm. transmitting updates or raising queries to satisfy the goal.
Source
1
Endpoint4
1) Effect-agnostic: This policy uses a predetermined Action window Idle window
schedule or random process (e.g., Poisson, binomial, or
Markov [5]–[9]) to send updates (raise queries) from (by) the Push-based
SA (AA), without accounting for their impact at the desti-
nation. We define a controlled update transmission (query) Push-and-pull Θ max
rate specifying the expected constant number of updates
Pull-based
(queries) to be communicated (raised) within a period. Also,
as the effect-agnostic policy does not consider what might be Query1 Query2 Query3 Time[slot]
happening in the other agent during the time of the decision,
Fig.3. Theoutline oftheactionandidlewindows indifferent models.
there exists an aleatoric uncertainty associated with random
updates (queries).
2) Effect-aware: Theeffect-awarepolicytakesintoconsid- endpoint’s viewpoint in the n-th slot. Thus, we assume that
eration the impacts of both agents’ decisions at the endpoint. vˆ belongs to the set Vˆ = {0}∪{νˆ | νˆ > 0,j ∈ J} with
n j j
In this regard, the SA (AA) predicts the effectiveness status i.i.d.elements,whereJ ={1,2,...,|Vˆ|−1},thej-thelement
at the endpoint offered by a sent update that is potentially has probability q = p (νˆ ), and p (·) is a pmf derived in
j νˆ j νˆ
received at the AA (the usefulness of a possible update at the SectionV-A. Since the packetissentovera PEC, vˆ =0 if it
n
source). Then, based on this prediction, the agent attempts to is erased or the update ends up being useless at the endpoint.
adapt transmission (query)instants and send (pull) updates in 1) AoI: Measuring the freshness of correctly received
therightslots.Thispolicycomeswithanepistemicuncertainty updates at the AA within a query slot, the AoI is defined
because decisionsare made accordingto probabilistic estima- as ∆ =n−u(n), where ∆ =1 and u(n) is the slot index
n 0
tions, not accurate knowledge.However,such uncertaintycan of the latest successful update, which is given by
be decreased using learning or prediction techniques.
u(n)=max m | m≤n,β (1−ǫ )=1 (2)
m m
(cid:8) (cid:9)
III. EFFECTIVENESSANALYSIS METRICS withǫ
m
∈{0,1}beingthechannelerasureinthem-thslot.In
addition,β ∈{0,1}indicatesthequerycontroller’sdecision,
Toachievetherighteffectattheendpoint,anupdatepacket m
where β =1 means pulling the update; otherwise, β =0.
that is successfully received at the AA has to satisfy a set of m m
2) Actionlateness: Thelatenessofanactionperformedin
qualitative attributes, captured by the metrics as follows.
the n-th time slot in relevance to a query raised at the n′-th
slot, n′ ≤n, is given by
A. Grade of Effectiveness Metric
Θ =(1−β )(n−n′), (3)
We introduce a grade of effectiveness (GoE) metric that n n
comprises several qualitative attributes and characterizes the whichisvalidforΘ <Θ .Herein,Θ showsthewidth
n max max
amountofimpactanupdatemakesattheendpoint.Mathemat- of action windowwithin which the AA can act on each query
ically speaking, the GoE metric is modeled via a composite based on update arrivals from the SA. Outside the dedicated
function GoE n = (f ◦g)(I n) for the n-th time slot. Here, actionwindowfortheSA,theAAmightundertakeothertasks
g : Rx → Ry,x ≥ y, is a (nonlinear) function of x ∈ Z+ orcommunicatewithotheragents.Employingthepush-based,
informationattributes I n ∈Rx, and f :Ry →R is a context- pull-based, and push-and-pullupdate communication models,
aware function.3 The particular forms of functions f and g we have Θ =∞, Θ =1, and Θ >1, respectively.
max max max
could vary according to different subscribed goals and their Fig. 3 illustrates the action and idle windows for different
relevant requirements. models. It is worth mentioning that a wider action window
In this paper, without loss of generality, we consider allows for higher flexibility in cases of heavy action loads at
freshness of updates and timeliness of actions as the main the cost of longer actuation availability.
contextual attributes. The first comes in the form of age
of information (AoI) metric, which is denoted by ∆ . The
n B. Special Forms of the GoE
secondismeasuredfromtheaction’slateness,denotedbyΘ .
n
Thereby, we can formulate the GoE metric as follows The GoE metric’s formulation in (1) can simply turn into
the QAoI and the VoI metrics as special cases. In this
GoE =f (g (vˆ ,∆ ),g (Θ );g (C )) (1)
n g ∆ n n Θ n c n regard, we obtain a penalty function of the QAoI such that
GoE = g (∆ ) if we set Θ = 1, assume linear g (·),
whereC representstheoverallcostincurredin then-th time n ∆ n max Θ
n
slot. Also, g : R+ × R+ → R+, g : R+ → R+, and and overlook updates’ usefulness and cost. In addition, by
∆ 0 0 Θ 0 0
g : R+ → R+ are penalty functions, and f : R+ ×R+ × removingthe conceptsof queryand time, hence the freshness
c 0 0 g 0 0
R+ →R+ is a non-decreasingutility function.Moreover,g , and timeliness in the GoE’s definition, we arrive at a utility
0 0 ∆
function of the VoI, i.e., GoE =f (vˆ ;g (C )).
g , and g are non-increasingwith respectto (w.r.t.)∆ , Θ , n g n c n
Θ c n n
and C , respectively, while g is non-decreasing w.r.t. vˆ .
n ∆ n
Here, vˆ n is the usefulness of the received update from the C. Effectiveness Indicator
Anupdateinthen-thtimeslotisconsideredeffectiveatthe
3The GoE metric in this form can be seen as a special case of the SoI
metricintroduced in[2],[19],[20]. system level if its GoE n is higher than a target effectiveness5
grade, which is called GoE and is necessary to satisfy the B. CMDP Modeling
tgt
goal.LetusdefineE n asaneffectivenessindicatorinthen-th WecastP from(6)intoaninfinite-horizonCMDPdenoted
1
time slot. Thus, we can write by a tuple (S ,A ,P ,r ) with components that are defined
γ γ γ γ
via the agent that solves the decision problem.
E = {GoE ≥GoE ∧Θ <Θ }. (4)
n n tgt n max
1) Modeling at the SA: The CMDP at the SA is modeled
The second condition in (4) appears from (3). According to according to the following components:
(4), an update could be effective only if it arrives within the States – The state of the system S in the n-th slot from
α,n
actionwindowoftheAA.Hence,aconsequentE-ACKshared the SA’s perspective is depicted by a tuple (v ,Eˆ ) in which
n n
with the SA can imply the raise of a query or the availability v is the update’s usefulness, and Eˆ ∈ {0,1} shows the E-
n n
oftheAAtotakeaction.Giventhevaluesof∆ n andΘ n,and ACKarrivalstatusattheSAafterpassingthePEC,asdefined
byinserting(1)into(4),wereachatargetusefulnesslevelv tgt in Section II. Herein, we have Eˆ n =0 in case E n =0 or the
as the importance threshold that the update should exceed to acknowledgment signal is erased; otherwise, Eˆ = 1. In this
n
be considered effective. In this case, if Θ n <Θ max, we have regard, S α,n belongs to a finite and countable state space S α
with |S |=2·|V| elements.
v = α
tgt Actions – We consider α the decision for update commu-
n
min νˆ | νˆ ∈Vˆ,f (g (νˆ ,∆ ),g (Θ ))≥GoE ; (5) nicationinthen-thslot,whichisamemberofanactionspace
j j g ∆ j n Θ n tgt
n o A ={0,1}.Inthisspace,0standsfordiscardingtheupdate,
α
otherwise, v tgt = max{νˆ j | νˆ j ∈ Vˆ}. In (5), v tgt can be and 1 indicates transmitting the update.
computed by exhaustive search. Transition probabilities – The transition probability from
thecurrentstate S to thefuturestate S viatakingthe
α,n α,n+1
action α is written by
n
IV. MODEL-BASED AGENTDECISIONS
p (S ,α ,S )=Pr (v ,Eˆ ) | (v ,Eˆ ),α
α α,n n α,n+1 n+1 n+1 n n n
In this section, we first formulate a decision problem for =p ((cid:0)v )Pr Eˆ | v ,α ((cid:1)7)
ν n+1 n+1 n n
effect-aware policies at either or both agent(s), cast it as a
(cid:0) (cid:1)
constrainedMarkovdecisionprocess(CMDP),andthensolve since Eˆ n+1 and Eˆ n are independent, and Eˆ n is independent
it based on the problem’s dual form. of v n, ∀n. We can derive the conditionalprobabilityin (7) as
• Pr Eˆ n+1 = 0 | v n,α n = Pr(vˆ tgt > α nv n) = 1 −
P vˆt(cid:0) gt(α nv n), (cid:1)
A. Problem Formulation • Pr Eˆ n+1 =1 | v n,α n =P vˆtgt(α nv n),
wherevˆ(cid:0) isamappedtarge(cid:1)tusefulnessthattheSAconsiders,
The objective is to maximize the expected discounted sum tgt
P (vˆ ) = p (vˆ′ ) denotes its cumulative
of the updates’ effectiveness in fulfilling the subscribed goal, vˆtgt tgt vˆ t′ gt≤vˆtgt vˆtgt tgt
distribution funcPtion (CDF), and p (·) shows the pmf de-
where each agent individually derives its decision policy vˆtgt
rived in Section V-B.
subjecttotherelevantensuedcostbylookingintotheproblem
from its own perspective. Let us define π∗ and π∗ as the Rewards – Theimmediaterewardofmovingfromthe state
α β S to the state S under the action α is equal to
classesofoptimalpoliciesfortransmissionandquerycontrols, α,n α,n+1 n
r (S ,α ,S ) = Eˆ where it relies on the E-ACK
respectively.Therefore,wecanformulatethedecisionproblem α α,n n α,n+1 n+1
status in the future state.
solved at each agent as follows
Despitepossibleerasuresovertheacknowledgmentlink,the
N
1 reward defined in this model fits into the decision problem in
P : max limsup E λnE E
1 πγ N→∞ N (cid:20) nX=1 n (cid:12)
(cid:12)
0 (cid:21) t( h6 e), ew xph ee cre tedth de isc co or ure ns tep don sd ui mng oo fb Ej -e Act Civ Ke ab re rc ivo am lse .s Inm ta hx isim seiz ni sn eg
,
N
s.t. limsup 1 E λnc (γ ) ≤C (6) Eˆ n at the SA resembles E n at the AA plus noise in the form
N (cid:20)
γ n
(cid:21)
γ,max
of the E-ACK erasure.
N→∞ nX=1
2) Modeling at the AA: For modeling the problem at
whereλ∈[0,1]indicatesa discountfactor,andγ ∈{α,β} is
the AA, we have the components as follows: States – We
replaced with α and β for the update transmission and query
represent the state S in the n-th time slot using a tuple
β,n
decisionproblems,respectively,attheSAandtheAA.Herein,
(vˆ ,∆ ,Θ ), where vˆ is the usefulness of the received
n n n n
γ ∈ {0,1} denotes the decision at the relevant agent, c :
n γ update from the perspective of the endpoint, ∆ is the AoI,
{0,1} → R+ is a non-decreasing cost function, and C n
0 γ,max andΘ n denotestheactionlateness,asmodeledinSectionIII.
shows the maximum discounted cost.
Without loss of generality, we assume the values of ∆ and
n
For either update communication model introducedin Sec- Θ are truncated by the maximum values notated as ∆
n max
tion II-A, optimal decisions at the agent(s) following the and Θ , respectively, such that the conditions
max
effect-awarepolicy,i.e.,π∗ and/orπ∗,areobtainedbysolving
α β
P
1
in (6). However, for every agent that employs an effect- g ∆(vˆ n,∆ max−1)≤(1+ε ∆)g ∆(vˆ n,∆ max), (8)
agnostic policy, with regard to Section II-B, there is a pre-
for vˆ ∈Vˆ, and
defined/given set of decisions denoted by π˜ or π˜ such that n
α β
π =π˜ or π =π˜ , respectively. g (Θ −1)≤(1+ε )g (Θ ) (9)
α α β β Θ max Θ Θ max
16
are met with the relevant accuracy ε and ε . Given this, at Given Propositions1 and 2, we can show that the expected
∆ Θ
the AA, S is a member of a finite and countable space S effectiveness obtained by P in (6) is the same for all initial
β,n β 1
having |S |=∆ ·Θ ·|Vˆ| states. states [21, Proposition 4.2.3]. In this regard, E , ∀n, is
β max max n
Actions – As already mentioned, β n shows the decision of independent of E 0 for either model, thus we arrive at the
raising a query in the n-th time slot and gets values from an following decision problem:
action space A = {0,1}. Here, 0 and 1 depict refusing and
β N
1
confirming to pull an update, respectively. P : max limsup E λnE :=E¯
Transition probabilities – The transition probability from 2 πγ N→∞ N (cid:20) nX=1 n (cid:21) γ
the current state S to the future state S under the N
β,n β,n+1 1
action β n is modeled as s.t. limsup N E (cid:20) λnc γ(γ n) (cid:21)≤C γ,max (12)
N→∞ nX=1
p (S ,β ,S )=
β β,n n β,n+1
for γ = {α,β}. Applying Propositions 1 and 2 confirms that
Pr (vˆ n+1,∆ n+1,Θ n+1) | (vˆ n,∆ n,Θ n),β n . (10) thereexiststationaryoptimalpoliciesπ∗ andπ∗ forP solved
α β 2
According t(cid:0) o (10), we can write: (cid:1) at the SA and the AA, respectively, where both policies are
unichain [21, Proposition 4.2.6].
• Pr((νˆ j,min{∆ n+1,∆ max},
min{Θ +1,Θ }) | (νˆ ,∆ ,,Θ ),β )=1−β ,
n max j n n n n
• Pr((νˆ j,min{∆ n+1,∆ max},1) | (νˆ j,∆ n,Θ n),β n) = C. Dual Problem
β p ,
n ǫ To solve the decision problem P given in (12), we first
2
• Pr((νˆ j′,1,1) | (νˆ j,∆ n,Θ n),β n)=β n(1−p ǫ)q j′, define an unconstrained form for the problem via dualizing
with νˆ j,νˆ j′ ∈ Vˆ. For the rest of the transitions, we have the constraint.Then, we proposean algorithmto compute the
p (S ,β ,S )=0. As stated earlier, q =p (νˆ ) with decision policies at both agents.
β β,n n β,n+1 j νˆ j
the pmf p (·) derived in Section V-A. Theunconstrainedformoftheproblemisderivedbywriting
νˆ
Rewards – Arrivingat the state S
β,n+1
from the state S
β,n
the Lagrange function L(µ;π γ) as below
bytakingtheactionβ ,isrewardedbasedontheeffectiveness
n N
1
level provided at the future state such that L(µ;π )=max limsup E λn E −µc (γ )
γ n γ n
r (S ,β ,S )=E =
πγ N→∞ N (cid:20) nX=1 (cid:16) (cid:17)(cid:21)
β β,n n β,n+1 n+1
+µC (13)
γ,max
f (g (vˆ ,∆ ),g (Θ ))≥GoE
g ∆ n+1 n+1 Θ n+1 tgt
×(cid:8) Θ <Θ (cid:9)(11) with µ≥0 being the Lagrange multiplier. According to (13),
n+1 max
we arrive at the following dual problem to be solved:
(cid:8) (cid:9)
by the use of (1) and (4).
P : inf max L(µ;π ) (14)
3) Independence of the initial state: Before we delve 3 µ≥0 πγ γ
into the dual problem and solve it, we state and prove two
propositions to show that the expected discounted sum of
:=hγ(µ)
| {z }
effectiveness in (6) is the same for all initial states. whereh γ(µ)=L(µ;π γ∗ ,µ) isthe Lagrangedualfunctionwith
π∗ :S →A denotingastationaryµ-optimalpolicy,which
γ,µ γ γ
Proposition 1. The CMDP modeled at the SA satisfies the is obtained as
accessibility condition.
π∗ =argmax L(µ;π ) (15)
γ,µ γ
Proof. Given the transition probabilities defined in (7), every πγ
state S α,m ∈S α, m≤N, is accessible or reachable from the for µ derived in the dual problem P . As the dimension
3
stateS infinitestepswithanon-zeroprobability,following
α,n of the state space S is finite for both defined models, the
γ
the policy π . Therefore, the accessibility condition holds for
α growth condition is met [22]. Also, the immediate reward
the CMDP model at the SA [21, Definition 4.2.1].
is bounded below, having a non-negative value according to
Section IV-B. In light of the above satisfied conditions, from
Proposition2. ThemodeledCMDPattheAAmeetstheweak [22, Corollary 12.2], we can claim that P and P converge
2 3
accessibility condition. to the same expected values, thus we have
Proof. We divide the state space S β into two disjoint spaces E¯ γ = inf h γ(µ)=max u(π γ) (16)
of T and T = S −T , where T consists of all the states µ≥0 πγ
a b β a a
whose ∆ = 1, i.e., T = {S | S = (νˆ ,1,Θ ),∀νˆ ∈ underanyclassofpolicyπ .Owingtothesatisfiedconditions,
n a β,n β,n j n j γ
Vˆ,Θ = 1,2,...,Θ }. Thus, T includes the rest of the there exist non-negative optimal values for the Lagrange
n max b
states with ∆ ≥2. With regardto the transitionprobabilities multiplier µ∗ such that we can define u(π ) = L(µ∗;π ) in
n γ γ
derivedin (10), all states of T are transientunderany policy, (16) [22, Theorem 12.8].
b
while every state of an arbitrary pair of two states in T We can now proceed to derive the optimal policies at the
a
is accessible from the other state. Accordingly, the weak SA and the AA from the decision problem P by applying
3
accessibility condition in the modeled CMDP at the AA is an iterative algorithm in line with the dynamic programming
satisfied according to [21, Definition 4.2.2]. approach based on (13)–(15) [14].
1
17
Algorithm 1: Solution for deriving π∗ and µ∗ value function is derived from Bellman’s equation [24], as
γ
Input: Known parameters N ≫1, C γ,max, η, ε µ, state Vπγ,µ(s)=
k
s µcp o (0a s )c te ←fuS n 0γ c ,, ti µa o −n nd ←ca γc ( 0·t ,)io . µn I +ns i ≫p tia ac l 1e ,vaA πl γuγ ,e. µs −T lh ←←e f 0o 1 ,,r am ndof the γm ∈a Ax γsX′∈Sγp γ(s,γ,s′) hr γ,µ(s,γ,s′)+λV kπ −γ 1,µ(s′)
i
(18)
π γ,µ+ ←0. for the state s∈S γ. Consequently,the decision policy in that
1 Initialize π γ∗ ,µ(s), ∀s∈Sγ, via runningUtility(µ(0)). state is improved by
2
3
i wf hE ilh
Se
tP
e|
pµN n
+
l= :−1c µγ −(γ |n ≥) i ε≤
µ
dN
o
⊲C γ O,m ua tx erth loe on pg (o Bto is1 e1 c.
tion search)
π arγ g,µ m(s a) x∈
p γ(s,γ,s′) r γ,µ(s,γ,s′)+λV kπ −γ 1,µ(s′) . (19)
4 update µ(l) ← µ− + 2µ+. γ∈Aγ sX′∈Sγ h i
5 Improve π γ∗ ,µ ← Utility(µ(l)). In (18) and (19), we define a net reward function as
6 if E h PN n=1c γ(γ n) i≥NC γ,max then r (s,γ,s′)=r (s,γ,s′)−µc (γ), (20)
7 µ− ←µ(l), and π γ,µ− ← Utility(µ−). γ,µ γ γ
8 else µ+ ←µ(l), and π γ,µ+ ← Utility(µ+). which takes into account the cost caused by the taken action.
9 Reset l←l+1. The value iteration stops running at the k-th iteration once
the following convergence criterion is met [23]:
10 if πE γ∗ ,h µP (s)N n= ←1c ηγ π( γγ ,n µ−) i (s< )+NC (1γ −,ma ηx )πt γh ,e µn +(s), ∀s∈Sγ. sp V kπγ,µ −V kπ −γ 1,µ <ε π (21)
11 return µ∗ =µ(l) and π γ∗ (s)=π γ∗ ,µ(s), ∀s∈Sγ. where ε
π
> 0 is the(cid:0) desired converg(cid:1) ence accuracy, and sp(·)
indicates a span function R+ →R+ given as
0 0
Function Utility(µ):
Input: Known parameters N ≫1, ε π, state space sp V kπ ′γ,µ =maxV kπ ′γ,µ(s)−minV kπ ′γ,µ(s) (22)
Sγ, and action space Aγ. Initial values (cid:0) (cid:1)
s∈S s∈S
k←1, π γ,µ(s)←0, and V kπγ,µ(s)←0, by using the span seminorm [23, Section 6.6.1]. As the
∀s∈Sγ. decision policies are unichain and have aperiodic transition
Iteration k: ⊲ Inner loop (Value iteration) matrices, the criterion in (21) is satisfied after finite iterations
12 for states∈Sγ do for any value of λ∈[0,1] [23, Theorem 8.5.4].
13 compute Vπγ,µ(s) from (18). 2) Computing µ∗: We leverage the bisection search
k
14 Improve π γ,µ(s) according to (19) and (20). method to compute the optimal Lagrange multiplier over
1 15 6 if sp s(cid:0) teV pkπγ u, pµ − k←V kπ − kγ 1,µ +(cid:1) 1≥ , aε nπ da gs oin to(2 12 2.) then m frou mltip thle es inte np es r li on ot ph .e Sto au rt te inr glo wo ip thb aa nse id nito ian l t ih ne terd ve ar liv [µed −,π µγ∗ +,µ ]
suchthath (µ−)h (µ+)<0,thevalueofthemultiplieratthe
17 return π γ∗ ,µ(s)=π γ,µ(s), ∀s∈Sγ. l-th,∀l ∈Nγ , step iγ s improvedby µ(l) = µ−+µ+ . As shownin
2
Algorithm1,ateachstep,thevalueofeitherµ− orµ+ andthe
correspondingdecisionpolicyπ γ,µ− orπ γ,µ+,respectively,are
updatedaccordingtothecostconstraintin(12)untilastopping
D. Iterative Algorithm
criterion |µ+ −µ−| < ε is reached with the accuracy ε .
µ µ
Considering (13) and (14), h (µ) is a linear non-increasing
TheiterativealgorithmisgiveninAlgorithm1andconsists γ
function of µ. In this regard, the bisection method searches
oftwoinner andouter loops.Theinnerloopisforcomputing
the µ-optimal policy, i.e., π∗ , using the value iteration for the smallest Lagrange multiplier that guarantees the cost
γ,µ
constraint.Also, one canshow thath (µ) denotesa Lipschitz
method. Over the outer loop, the optimal Lagrange multiplier γ
µ∗ is derived via the bisection search method. continuous function with the Lipschitz constant as below
the1) deC co ism iop nut pin og licπ yγ∗ , iµ s: iteA rap tp ivly eli yng imth pe rov va elu de gi it ve era ntio µn fm roe mtho thd e, (cid:12)C γ,max−limsup N1 E
(cid:20)
N λnc γ(γ n) (cid:21)(cid:12).
outer loop (bisection search). Thus, π (s) ∈ A , ∀s ∈ S , (cid:12) N→∞ nX=1 (cid:12)
γ,µ γ γ (cid:12) (cid:12)
is updated such that it maximizes the expected utility (value) Therefore(cid:12),the bisectionsearch convergesto the op(cid:12)timalvalue
Vπγ,µ(s) at the k-th, ∀k ∈N, iteration, which is obtained as of µ within L∈N finite steps [25, pp. 294].
k
Vπγ,µ(s)=E r +λr +λ2r +··· | s =s After the outer loop stops running, we obtain a stationary
k k k+1 k+2 k deterministic decision policy as π∗ = π if the following
≈E(cid:2)r k+λV kπ −γ 1,µ | s k =s (cid:3)(17) condition holds: γ γ,µ
(cid:2) (cid:3)
where s denotes the state at the k-th iteration, and r is the N
k k 1
correspondingreward at that state. The approximationin (17) C : limsup
N
E
(cid:20)
λnc γ(γ n) (cid:21)=C γ,max. (23)
appears after bootstrapping the rest of the discounted sum of N→∞ nX=1
therewardsbythevalueestimateVπγ,µ.Undertheformofthe
Otherwise, the derived policy becomes randomized stationary
k−1
valueiterationfortheunichainpolicyMDPs[23],theoptimal in the shape of mixing two deterministic policies π γ,µ− =8
B. Probability of the Mapped Target Usefulness
EstimationattheSA
CMDP-baseddecisions
EstimationattheAA We assume that the mapped target usefulness, i.e., vˆ ,
tgt
E-horizon D-horizon Time is a member of the set Vˆ tgt = {ϑ j | j ∈ J tgt} with i.i.d.
elements, where J = {1,2,...,|Vˆ |}, and the probability
tgt tgt
Fig.4. Timepartitioning oftheestimation anddecision horizons. of the j-th element is equal to p (vˆ = ϑ ). Herein, as
vˆtgt tgt j
mentionedearlier,p (·) is theestimated pmfof themapped
vˆtgt
lim π and π = lim π with probability η ∈[0,1] target usefulness and obtained by
γ,µ γ,µ+ γ,µ
µ→µ− µ→µ+
[26]. Hence, we can write p (ϑ )= p ϑ | Eˆ =e Pr Eˆ =e (26)
vˆtgt j vˆtgt j
π γ∗ ←ηπ γ,µ− +(1−η)π γ,µ+, (24) e∈X {0,1} (cid:0) (cid:1) (cid:0) (cid:1)
where we find the probability of successfully receiving E-
which implies that the decision policy is randomly chosen as
ACK, i.e., e=1, or not, i.e., e=0, as follows
π γ∗ = π γ,µ− and π γ∗ = π γ,µ+ with probabilities η and 1−η,
respectively. In (24), η is computed such that the condition C 1 M
Pr Eˆ =e = Eˆ =e (27)
in (23) is maintained. M m
3) Complexity analysis: The value iteration approach in (cid:0) (cid:1) mX=1 (cid:8) (cid:9)
the inner loop is polynomial with O(|A γ||S γ|2) arithmetic where Eˆ
m
indicates the E-ACK arrival status in the m-th
operations at each iteration. Thereby, the longest running slot of the E-horizon. Furthermore, to derive the conditional
time of Algorithm 1, in terms of the number of arithmetic probabilityin (26), we first consider the successful arrivals of
operationsoverbothloops,isgivenbyO 2L|Sγ|2 log 1 E-ACK signals such that
1−λ 1−λ
(cid:16) (cid:16) (cid:17)(cid:17)
for the fixed λ, as studied in [27] and [28]. The complexity
p ν | ν ≥ϑ
of thealgorithmincreaseswith a largerstate space,additional p ϑ | Eˆ =1 = i∈I ν|Eˆ=1 i i j .
vˆtgt j P p (cid:0) ν | ν ≥(cid:1) ϑ
steps in the outer loop, and as λ→1. (cid:0) (cid:1) j∈Jtgt i∈I ν|Eˆ=1 i i j
P P (cid:0) (2(cid:1)8)
V. MONTECARLO PROBABILITY DISTRIBUTION
Then, we have
ESTIMATION
p ν | ν <ϑ
In this section, we leverage the Monte Carlo estimation p ϑ | Eˆ =0 = i∈I ν|Eˆ=0 i i j .
method to statistically compute the estimated pmfs of the vˆtgt j P p (cid:0) ν | ν <(cid:1) ϑ
(cid:0) (cid:1) j∈Jtgt i∈I ν|Eˆ=0 i i j
received updates’ usefulness from the endpoint’s perspective P P (cid:0) (2(cid:1)9)
at the AA, and the mapped target usefulness at the SA.
The pmfs p (·) and p (·) in (28) and (29) are
To this end, we consider a time interval in the format of ν|Eˆ=1 ν|Eˆ=0
associated with an observation’s importance rank given the
an estimation horizon (E-horizon), followed by a decision
successful and unsuccessful communication of the E-ACK,
horizon (D-horizon), as illustrated in Fig. 4. The E-horizon
respectively. In this regard, by applying Bayes’ theorem we
is exclusively reserved for the estimation processes and has a
can derive the following formula:
length of M time slots, which is sufficiently large to enable
an accurate estimation. The D-horizon represents the long- 1 M v =ν ∧Eˆ =e
term time horizonwith the sufficientlylargelengthof N ≫1 p ν|Eˆ=e ν i = M Pm=1 P(cid:8)r(m Eˆ =ei ) m (cid:9). (30)
slots, as defined in Section IV, during which the agents find (cid:0) (cid:1)
and apply their (model) CMDP-based decision policies.
VI. SIMULATION RESULTS
Within the E-horizon,the SA does notmake anydecisions.
Inthissection,wepresentsimulationresultsthatcorroborate
Instead, it focuses on communicating updates at the highest
our analysis and assess the performance gains in terms of
possibleratewhileadheringtocostconstraints.Oncereceiving
effectivenessachievedbyapplyingdifferentupdatemodelsand
theseupdates,theAAmeasurestheirusefulness,storesthemin
agent decision policies in end-to-end status update systems.
memory, and sends E-ACK signals for effective updates. The
SA logs whether the E-ACK has been successfully received
A. Setup and Assumptions
or not in every slot of the E-horizon. Finally, employing the
received E-ACK signals at the SA and the measured updates’ We study the performance over 5×105 time slots, which
usefulness at the AA, both agents perform their estimations. includes the E-horizon and the D-horizon with 1×105 and
4×105 slots, respectively. To model the Markovian effect-
A. Usefulness Probability of Received Updates agnostic policy, we consider a Markov chain with two states,
Picking the j-th, ∀j ∈J, outcome from the set Vˆ (defined 0and1.We assumethattheself-transitionprobabilityofstate
in Section III-A) that corresponds to the received update’s 0is0.9,whiletheoneforstate1reliesonthecontrolledupdate
usefulness from the endpoint’s perspective in the m-th slot transmission or query rate. Without loss of generality, we
of the E-horizon, i.e., vˆ , the relevant estimated probability assumethattheoutcomespacesfortheusefulnessofgenerated
m
of that outcome is given by
updates,i.e.,V,theusefulnessofreceivedupdates,i.e.,Vˆ,and
themappedtargetusefulness,i.e.,Vˆ ,areboundedwithinthe
tgt
M
1 span [0,1]. For simplicity, we divide each space into discrete
q =p (νˆ )= vˆ =νˆ . (25)
j νˆ j M m j levels based on its number of elements in ascending order,
mX=1 (cid:8) (cid:9)
1
1
19
TABLEI
PARAMETERSFORSIMULATIONRESULTS. 0.25
(E-aware,E-aware)
Name Symbol Value (Periodic,E-aware)
0.2 (Markov.,E-aware)
E-horizonlength – 1×105[slot] ( (E Pe-a riw oa dr ie c, ,P Pe er ri io od di ic c)
)
D-horizon length N 4×105[slot] (E-aware,Markov.)
0.15 (Markov.,Markov.)
Erasureprobability inupdatechannel pǫ 0.2
Erasureprobability inacknowledgment link p′ 0.1
ǫ
0.1
Lengthofgenerated updateusefulness space |V| 10
Lengthofreceived updateusefulness space |Vˆ| 11
Lengthofmappedtargetusefulness space |Vˆ tgt| 11 0.05
a 0.3
Shapeparameters forusefulness distribution
b 0.3 0
0 1 2 3 4 5
Maximumtruncated AoI ∆max 10[slot] Time slot 105
Actionwindow widthinpull-based model 1[slot]
Actionwindow widthinpush-and-pull model Θmax 5[slot] Fig. 5. The evolution of the average effectiveness accumulated over time
basedonthepush-and-pull model.
Actionwindow widthinpush-basedmodel 10[slot]
Updatetransmissioncostatn-thslot Cn,1 0.1
Queryraisingcostatn-thslot Cn,2 0.1 only the modeling process is mentioned in the legend.
Actuation availability costatn-thslot Cn,3 0.01
Maximumdiscounted costindecisionproblem Cγ,max 0.08
B. Results and Discussion
Targeteffectiveness grade GoEtgt 0.6
Fig.5illustratestheevolutionoftheaveragecumulativeef-
Discountfactor inCMDP λ 0.75
εµ 10−4 fectivenessovertimeforthepush-and-pullmodelanddifferent
Convergence accuracy inAlgorithm 1 επ 10−4 agent decision policies. As it is shown, applying the effect-
aware policy in both agents offers the highest effectiveness,
Mixingprobability inbisection method η 0.5
wherethegapbetweenitsandtheotherpolicies’performance
Controlled transmissionrate(value-agnostic) – 0.8
increases gradually as time passes. Nevertheless, using the
Controlled queryrate(value-agnostic) – 0.8
other value-agnostic policies at either or both agent(s) dimin-
ishes the effectiveness performance of the system by at least
12% or 36%, respectively. Particularly, if the SA and the AA
applytheeffect-awareandMarkovianeffect-agnosticpolicies,
whereeverylevelshowsarandomizedvalue.Wealsoconsider
sequentially, the offered effectiveness is even lower than the
that the i-th outcomeof the set V notated as ν , i∈I, occurs
i
scenario in which both agents use periodic effect-agnostic
following a beta-binomial distribution with pmf
policies.OneofthereasonsisthattheMarkovianqueryraising
|V|−1 Beta(i−1+a,|V|−i+b) misleadstheSAinestimatingthemappedtargetusefulnessof
p (ν )= (31)
ν i (cid:18) i−1 (cid:19) Beta(a,b) updates. It is also worth mentioning that throughout the E-
horizon from slot 1 to 1×105, the scenarios where the AA
whereBeta(·,·)is the betafunction,anda=0.3 andb=0.3
raises effect-aware queries offer the same performance and
are shape parameters.
better than those of the other scenarios. However, in the D-
Moreover,weplotthefiguresbasedonthefollowingformof
horizon, a performance gap appears and evolves depending
the GoE metric4, which comes from the general formulation
on the applied policy at the SA. Therefore, by making effect-
proposed in (1):
aware decisions at both agents based on estimations in the
vˆ
GoE = n −α C −β C −C (32) E-horizonandCMDP-basedpoliciesintheD-horizon,perfor-
n n n,1 n n,2 n,3
∆ nΘ n mance can be significantly improved.
forthen-thtimeslot.Herein,C isthecommunicationcost, The bar chart in Fig. 6 shows the average rate of update
n,1
C denotes the query cost, and C indicates the actuation transmissions from the SA and the consequent actions per-
n,2 n,3
availabilitycost atthe AA whichdependson the updatecom- formed at the AA based on raised queries that result in the
munication model. However,the cost function in the decision system’s effectiveness, as depicted in Fig. 5. Interestingly,the
problem’s constraint is assumed to be c (γ )= γ , ∀n ∈N. scenario where both agents apply the effect-aware policy has
γ n n
The parameters used in the simulations are summarized for lowertransmissionandactionratescomparedtotheothersce-
Table I. In the legends of the plotted figures, we depict the narios except the ones with Markovian effect-agnostic query
policies at the agentsin the formof a tuple, with the first and policies. However, the number of actions that can be done
second elements referringto the policy applied in the SA and for those scenarios with Markovian queries is limited since
the AA, respectively. For an effect-aware policy, we simply most of the updates are received out of the action windows.
usethenotation“E-aware,”whileforaneffect-agnosticpolicy, Figs. 5 and 6 show that using effect-aware policies at both
the SA and the AA not only brings the highest effectiveness
4TheanalysiscanbeeasilyextendedtoanyotherformsoftheGoEmetric. but also needs lower update transmissions by an average of
ssenevitceffe
evitalumuc
egarevA10
1 0.25
(E-aware,E-aware)
0.2 (E-aware,Periodic)
0.8 (Periodic,E-aware)
(Markov.,E-aware)
0.15 (Periodic,Periodic)
(E-aware,Markov.)
0.6 0.1 (Markov.,Markov.)
Update
Action
0.05
0.4
0
0.2
-0.05
0 -0.1
E-aware) E-aware) E-aware) Periodic) Periodic) Markov.) Markov.)
-0.15
(E-aware, (Periodic, (Markov., (E-aware, (Periodic, (E-aware, (Markov., 0 1 2
Time
slot3 4 1055
(a)
Fig.6. Theaverageupdatetransmissionandqueryratesofdifferentdecision
policies inthepush-and-pull model.
0.18
(E-aware,E-aware)
0.16 (E-aware,Periodic)
11%, saving resources compared to the scenarios that have (E-aware,Markov.)
0.14 (Periodic,E-aware)
comparable performances. We also note that although effect-
(Markov.,E-aware)
aware and periodic query decisions with effect-aware update 0.12 (Periodic,Periodic)
(Markov.,Markov.)
transmissionhavealmostthesameactionrate,theeffect-aware 0.1
case leads to more desirable effects or appropriate actions
0.08
at the endpoint. This results in around 16% higher average
0.06
cumulative effectiveness, referring to Fig. 5.
Figs. 7(a), 7(b), and 7(c) present the average cumulative 0.04
GoE provided in the system over 5×105 time slots for the 0.02
push-and-pull, push- and pull-based communication models,
0
respectively. The corresponding effectiveness at the endpoint 0 1 2 3 4 5
Time slot 105
for the primary model can be found in Fig. 5. The plots
(b)
demonstrate that when both agents decide based on effect-
awarepolicies,regardlessoftheupdatecommunicationmodel,
the highest offered GoE of the system is reached. How- 0
ever, in other scenarios, the performance of some policies
exceeds those of others, depending on the update model. -0.02
For instance, having effect-aware update transmission and
Markovian queries shows 2.52 times higher average GoE for
-0.04
the push-based model than the push-and-pull one. This is
because the push-based model has a larger action window.
-0.06 (E-aware,E-aware)
Besides, comparingFigs. 7(a)and7(b),the reasonthatthe (E-aware,Periodic)
provided GoE by the effect-aware decisions at both agents is (E-aware,Markov.)
(Periodic,E-aware)
28% lower for the push-based model compared to the push- -0.08 (Markov.,E-aware)
(Periodic,Periodic)
and-pullisthattheAAhastobeavailablelonger,whichcauses (Markov.,Markov.)
a highercost. Also,with thepull-basedmodelasinFig. 7(c), -0.1
0 1 2 3 4 5
the average GoE within a period is less than the average Time slot 105
cost since the AA is only available to act at query instants,
(c)
significantlyreducingtheaverageGoEdespitethehighupdate
Fig. 7. The evolution of the average cumulative GoE over time, following
transmission rate. In the pull-based model, however, applying
the(a)push-and-pull, (b)push-based,and(c)pull-based models.
the CMDP-based updatetransmissiondecisionsat the SA can
address this issue with 16% higher average GoE.
The trade-off between the average effectiveness and the alreadyreachtheirbestperformancebeforeΘ =10,which
max
width of the action window, i.e., Θ , is shown in Fig. 8 indicates the push-based model, we can conclude that the
max
for different decision policies. We see that the system cannot push-and-pull model with a flexible action window is more
offernotableeffectivenesswithΘ =1,i.e.,underthepull- advantageousthan the push-basedone with a very large fixed
max
basedmodel.However,byexpandingtheactionwindowwidth window.Inaddition,thescenariowherebothagentsuseeffect-
from Θ = 1 to 10[slot], the average effectiveness boosts aware policies outperforms the others for Θ > 1. It is
max max
fromitslowesttothehighestpossiblevalue.Sinceallpolicies worthmentioningthattheperformanceofthescenarioswhere
etar
egarevA
EoG
evitalumuc
egarevA
EoG
evitalumuc
egarevA
EoG
evitalumuc
egarevA11
0.25 0.25
0.2 0.2
0.15 0.15
0.1 (E-aware,E-aware) 0.1 (E-aware,E-aware)
(Periodic,E-aware) (E-aware,Periodic)
(Markov.,E-aware) (Periodic,E-aware)
(E-aware,Periodic) (Markov.,E-aware)
0.05 (Periodic,Periodic) 0.05 (Periodic,Periodic)
(E-aware,Markov.) (E-aware,Markov.)
(Markov.,Markov.) (Markov.,Markov.)
0 0
1 2 3 4 5 6 7 8 9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Action window width (Θ )[slot] Controlled update transmission rate
max
Fig.8. Theaverageeffectiveness versusthewidthoftheaction windowfor Fig.10. Theperformancecomparisonbetweenvariouspoliciesunderdifferent
thepush-and-pull model. updatetransmissionrates butfixedqueryrates forvalue-agnostic policies.
0.6
controlled rate is related to the value-agnostic update trans-
(E-aware,E-aware) mission policies at the SA and denotes the expected number
0.5 (Periodic,E-aware)
(Markov.,E-aware) of updates to be communicatedwithin the specified period of
(E-aware,Periodic)
time. Therefore, the performance of the other policies should
(Periodic,Periodic)
0.4
(E-aware,Markov.) remainfixedfordifferentcontrolledtransmissionrates.Fig.10
(Markov.,Markov.)
revealsthatincreasingthe controlledupdaterate increasesthe
0.3
offered effectiveness when the SA applies the value-agnostic
policies. However, even at the highest possible rate, subject
0.2
to the maximum discounted cost, using effect-aware policies
at both agents is necessary to ensure the highest average
0.1
effectiveness, regardless of the controlled transmission rate.
0 As an illustrative example, when the AA raises effect-aware
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Target effectiveness grade (GoE ) queries, the effectiveness drops by an average of 38% (43%)
tgt
iftheSAtransmitsupdatesbasedontheperiodic(Markovian)
Fig. 9. Theaverage effectiveness w.r.t. the target effectiveness grade in the effect-agnostic policy instead of using the effect-aware one.
push-and-pull model.
In Fig. 11, the plot shows the same trend as Fig. 10, but
thistimeitfocusesonthechangesintheaverageeffectiveness
theAAraisesMarkovianqueriesconvergestothehighestlevel versus the controlled query rate. Herein, the controlled query
forverylargewidths.Theaveragecumulativeeffectivenessof rateisdedicatedtothescenarioswheretheAAoperatesunder
these scenarios, as in Fig. 5, visibly rises for Θ ≥9. effect-agnostic policies. The results indicate that the average
max
The interplay between the average effectiveness and the effectiveness rises via the increase of the controlled query
target effectiveness grade, i.e., GoE from Section III-C, is rate for the periodic and Markovian policies. However, the
tgt
depicted in Fig. 9 for the push-and-pull model and different effectiveness increase is not significant for the latter one.
agent decision policies. Evidently, the average effectiveness Despite this, even with the highest rates, the effectiveness
under all policies decreasesgradually with the increase of the offered in the course of effect-aware queries is still higher
targeteffectivenessgradeandconvergestozeroforGoE ≥ than those of raising effect-agnostic queries. Therefore, the
tgt
0.9. Using effect-aware policies at both agents offers the highest average effectiveness is achieved when both agents
highest effectiveness for medium-to-large target grades, i.e., make effect-aware decisions, as depicted in Figs. 10 and 11.
GoE ≥ 0.52 here. However, for lower target grades, the In the context of the decision-making problem P in (12),
tgt 2
effect-aware and periodic effect-agnostic decisions at the SA alteringthemaximumdiscountedcost,i.e.,C ,canimpact
γ,max
and the AA, respectively, result in better performance. This the decisions made by each agent. To study this, we have
comes at the cost of higher transmission and action rates. plotted Fig. 12 for the push-and-pull model under different
Thus,thereisatrade-offbetweenthepaidcostandtheoffered decision policies. The figure shows that the stricter the cost
effectiveness, thus various policies can be applied depending constraint, the lower the average effectiveness, irrespective of
on the cost criterion and the target effectiveness grade. the decision policy. Also, for all cost constraints, the scenario
Fig. 10 depicts the average effectiveness obtained in the in which both agents use effect-aware policy yields the best
system through 5 × 105 time slots versus the controlled performance,whereastheotherpoliciesoutperformeachother
update transmission rate for the push-and-pull model and under different cost constraints. Due to CMDP-based deci-
various agent decision policies. Concerning Section II-B, this sions, the gap between the effectiveness performance of the
ssenevitceffe
egarevA
ssenevitceffe
egarevA
ssenevitceffe
egarevA12
0.25
0.2 (E-aware,E-aware)–MC
(E-aware,E-aware)–RL
0.15
(Periodic,E-aware)–MC
0.2 0.1 (E-aware,Periodic)–MC
(E-aware,Periodic)–RL
0.05 (Periodic,E-aware)–RL
0.15 0
0 1 2 3 4 5
Time slot 105
0.1 (E-aware,E-aware) 0.2 (E-aware,E-aware)–MC
(Periodic,E-aware) (E-aware,E-aware)–RL
(Markov.,E-aware) 0.15 (Markov.,E-aware)–MC
0.05 ( (E Pe-a riw oa dr ie c, ,P Pe er ri io od di ic c) ) 0.1 (( ME- aa rw ka or ve ., ,M E-a ar wk ao rv e.) )– –R RL L
(Markov.,Markov.) 0.05 (E-aware,Markov.)–MC
(E-aware,Markov.)
0
0 0 1 2 3 4 5
0.1 0.2 0 C.3 ontro0 l. l4
ed
qu0 e.5
ry
rat0 e.6 0.7 0.8 Time slot 105
Fig.11. Thecomparisonbetweendifferentpolicieswithvariablequeryrates Fig. 13. Theevolution ofthe average cumulative effectiveness fordifferent
butfixedupdatetransmissionrates forvalue-agnostic policies. policies undertheMCandRLapproaches inthepush-and-pull model.
0.25 0.25
(E-aware,E-aware)
(E-aware,Periodic)
(Periodic,E-aware)
0.2 (Markov.,E-aware) 0.2
(Periodic,Periodic)
(Markov.,Markov.)
(E-aware,Markov.)
0.15 0.15
(E-aware,E-aware)–MC
(E-aware,E-aware)–RL
0.1 0.1 (Periodic,E-aware)–MC
(E-aware,Periodic)–MC
(Markov.,E-aware)–MC
(E-aware,Markov.)–RL
0.05 0.05 (E-aware,Periodic)–RL
(Markov.,E-aware)–RL
(Periodic,E-aware)–RL
(E-aware,Markov.)–MC
0 0
0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 101 102 103 104 105
Maximum discounted cost (C γ,max) E-horizon length
Fig. 12. The average effectiveness attained by different policies versus the Fig.14. TheE-horizonlength’simpactontheeffectivenessfordifferentMC-
maximumdiscounted costinthepush-and-pull model. andRL-basedpolicies inthepush-and-pull model.
bestscenarioandthoseoftheothersincreasesastheconstraint anddifferentmodel-basedagentdecisionsbasedontheMonte
decreases until C = 0.1, where the SA can transmit all Carlo(MC)estimationmethodgiveninSectionV andmodel-
γ,max
updates, and the AA can raise queries without restrictions. freeRL-baseddecisions.Asshown,theMCapproachexceeds
Afterward, we compare the performance provided by the RL one under all decision policies. However, the perfor-
model-basedagentdecisionsdiscussedinSectionIVwiththat mancegapisnotsignificantinthescenariowherebothagents
of model-free decisions. The latter is based on reinforcement apply effect-aware policies.
learning (RL), where each agent separately learns to make The reason for the better effectiveness of the MC approach
decisionsthroughdirectinteractionwiththeenvironment.This could be the small state (observation) space, especially for
learningprocessismodeledunderthestate(here,observation) the modeling at the SA. Additionally, the length of the E-
spaces, action sets, and rewards according to Section IV-B, horizon as the training interval may be another factor. In
without relying on the construction of a predefined model. order to analyze the impact of the E-horizon length on the
To derive model-free decisions, we employ a deep Q-network average effectiveness of the effect-aware and effect-agnostic
(DQN) and parameterize an approximate value function for decisions based on the MC and RL approaches, we plot
everyagentwithinthe E-horizonthrougha multilayerpercep- Fig. 14 for the push-and-pullmodel. We infer that increasing
tron (MLP), assisted with the experience replay mechanism. the length of the E-horizon improves effectiveness, but the
Theneuralnetworkconsistsoftwohiddenlayers,eachwith64 MC-based policies reach their highest performance after a
neurons, and is trained using the adaptive moment estimation certain optimal length. This optimal E-horizon length varies
(ADAM) optimizer. The default values for the RL setting are dependingonthedecisionpolicy.Forexample,inthescenario
taken from [29], except for the learning rate that is 10−4, where both agents utilize the MC approach and apply effect-
andthediscountfactorassumed0.75,asalignedwithTable I. aware policies, an E-horizon length of around 316 time slots
In this regard, Fig. 13 depicts the evolution of the average is needed to ensure accurate enough estimations and achieve
cumulativeeffectivenessovertimeforthepush-and-pullmodel the highest average effectiveness. On the other hand, RL-
ssenevitceffe
egarevA
ssenevitceffe
egarevA
evitalumuc
egarevA
evitalumuc
egarevA
ssenevitceffe
ssenevitceffe
ssenevitceffe
egarevA13
differentmaximum discounted costs, the observation emerges
0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 that the more stringentthe cost constraintis, the narrower the
1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1
agentdecisionboundariesbecome.Thus,themapscouldvary
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Index of importance rank Index of importance rank bychangesintheparameters’valuesorthegoalswithdifferent
(a) (b)
targeteffectivenessgrades.ItisnoteworthythatinFig.16,the
Fig.15. ThelookupmapsfordecisionstheSAmakesbasedon(a)Cα,max= maps with Θ =1 represent the pull-based model, while the
0.06and(b)Cα,max=0.08. push-and-pulln
model convergesto the push-based model with
Θn=1 Θn=2 Θ n ≥3 and 2 in Figs. 16(a) and 16(b), respectively.
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 We compute an optimal threshold for each element of the
2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1
3 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 stateasadecisioncriterion,givenvaluesoftheotherelements.
4 0 1 1 1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1
5 0 0 1 1 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 Let us consider Ω and Ω as the decision criteria at the SA
6 0 0 1 1 1 1 1 1 1 1 6 1 1 1 1 1 1 1 1 1 1 α β
7 0 0 0 1 1 1 1 1 1 1 7 1 1 1 1 1 1 1 1 1 1 andAA,respectively.Toderivetheoptimaldecisionα∗ inthe
8 0 0 0 0 1 1 1 1 1 1 8 0 1 1 1 1 1 1 1 1 1 n
9 0 0 0 0 0 1 1 1 1 1 9 0 1 1 1 1 1 1 1 1 1 n-thslot,therearetwoalternativewaystodefinethecriterion:
10 0 0 0 0 0 0 1 1 1 1 10 0 0 1 1 1 1 1 1 1 1
11 0 0 0 0 0 0 0 1 1 1 11 0 0 1 1 1 1 1 1 1 1
1 2 3 4 5∆n6 7 8 9 10 Θn≥31 2 3 4 5∆n6 7 8 9 10 rΩ aα nki ,s ia .et .h ,r ies ∈ho Il ,d ff oo rr vthe =in νde ,x ∀o νft ∈he Vu ,p gd ia vt ee n’s thim ep Eo -r Ata Cn Kce
,
n i i 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 i.e., Eˆ n. Thus, we have
3 1 1 1 1 1 1 1 1 1 1
4 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 α∗ n = i≥Ω α(Eˆ n) | Eˆ n . (33) 6 1 1 1 1 1 1 1 1 1 1
7 1 1 1 1 1 1 1 1 1 1 (cid:8) (cid:9) 8 1 1 1 1 1 1 1 1 1 1 Ω α shows a threshold for the E-ACK given the impor-
9 1 1 1 1 1 1 1 1 1 1 tance rank of the update, such that
10 1 1 1 1 1 1 1 1 1 1
11 1 1 1 1 1 1 1 1 1 1
1 2 3 4 5∆n6 7 8 9 10 α∗
n
= Eˆ
n
≥Ω α(v n) | v
n
. (34)
(a) (cid:8) (cid:9)
For instance in Fig. 15(a), Ω (Eˆ =0)=Ω (Eˆ =1)=7,
α n α n
Θn=1 Θn≥2 ∀n.Also,Ω (v =ν )>1fori≤6,whileΩ (v =ν )=0
α n i α n i 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 for i > 6. Applying the same approach, we find the decision
3 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 criterion Ω β to obtain β n∗ from three different viewpoints.
5 1 1 1 1 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1
6 1 1 1 1 1 1 1 1 1 1 6 1 1 1 1 1 1 1 1 1 1
7 1 1 1 1 1 1 1 1 1 1 7 1 1 1 1 1 1 1 1 1 1
8 1 1 1 1 1 1 1 1 1 1 8 1 1 1 1 1 1 1 1 1 1
9 0 1 1 1 1 1 1 1 1 1 9 1 1 1 1 1 1 1 1 1 1
1 10
1
0
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 10
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
VII. CONCLUSION
1 2 3 4 ∆5 6 n7 8 9 10 1 2 3 4 ∆5 6 n7 8 9 10
(b)
We investigated decision-making for enhancing the effec-
Fig. 16. Thelookup maps the AA utilizes for (a) C β,max =0.06 and (b) tivenessofupdatescommunicatedintheend-to-endstatusup-
C β,max=0.08.
date system based on a push-and-pull communication model.
Tothisend,weconsideredthattheSAobservesaninformation
source and generates updates, and its transmission controller
based policies consistently improve across the plotted region.
decides whether to send them to the AA or not. On the other
However, the scenarios in which the AA applies effect-aware
side,theAAisresponsibleforactingbasedontheupdatesthat
policies, exhibit a notable challenge—achieving superior per-
are successfullyreceivedandthe raised queriesto accomplish
formance to MC-based policies demands an extensive E-
asubscribedgoalattheendpoint.AfterdefiningaGoEmetric,
horizon length. It is also worth mentioning that the reason
we formulatedthe decisionproblemof findingoptimaleffect-
forthemonotonicdecreaseintheperformanceofthescenario
aware policies that maximize the expected discounted sum of
with Markovianqueriesbasedon the MCapproachis thatthe
the update’s effectiveness subject to cost constraints. Using
providedeffectivenessislimited,anditgrowsataslowerpace
the dualproblem,we cast it to a CMDP solved separately for
w.r.t. the time interval we sum up the timely effectiveness.
eachagentbasedondifferentmodelcomponentsandproposed
an iterative algorithm to obtain the decision policies. Our
C. Lookup Maps for Agent Decisions
results established that the push-and-pull model, on average,
As the modeled CMDPs in Section IV-B have finite states, outperforms the push- and pull-based models in terms of
we can depictoptimalmode-baseddecisionsderivedin Algo- energyefficiencyandeffectiveness,respectively.Furthermore,
rithm1viaamulti-dimensionallookupmapforeachdecision- effect-aware policies at both agents significantly enhance the
making agent. Figs. 15 and 16 illustrate the maps for the SA effectivenessofupdateswithaconsiderabledifferenceincom-
andtheAA,respectively,underdifferentmaximumdiscounted parison to those of periodic and probabilistic effect-agnostic
costs.Thenumberofdimensionsinamapreliesonthenumber policies used at either or both agent(s). Finally, we proposed
of elements constructing every state of the relevant CMDP, a threshold-baseddecision policy complementedby a tailored
witheachdimensionassignedtooneelement.Withthelookup lookupmapforeachagentthatemployseffect-awarepolicies.
mapinhand,anagentcanmakeoptimaldecisionsineachslot Future works could explore multi-SA scenarios with varying
basedonitscurrentstate.Whencomparingthesamemapsfor sensing abilities and realizations’ time-dependentimportance.
Eˆ
ssenlufesu
fo
xednI
n
ssenlufesu
fo
xednI
ssenlufesu
fo
xednI
Eˆ
ssenlufesu
fo
xednI
ssenlufesu
fo
xednI
n
1
114
REFERENCES [29] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
[1] P. Agheli, N. Pappas, P. Popovski, and M. Kountouris, “Effective et al., “Human-level control through deep reinforcement learning,”
communication: When to pull updates?” in Proc. IEEE Int. Conf. Nature,vol.518,no.7540,pp.529–533,2015.
Commun.(ICC),2024,pp.183–188.
[2] M. Kountouris and N. Pappas, “Semantics-empowered communication
fornetworkedintelligentsystems,”IEEECommun.Mag.,vol.59,no.6,
pp.96–102, 2021.
[3] P. Popovski, O. Simeone, F. Boccardi, D. Gu¨ndu¨z, and O. Sahin,
“Semantic-effectiveness filtering and control for post-5G wireless con-
nectivity,” J.IndianInst.Sci.,vol.100,no.2,pp.435–443, 2020.
[4] B. Yin, S. Zhang, Y. Cheng, L. X. Cai, Z. Jiang, S. Zhou, and
Z. Niu, “Only those requested count: Proactive scheduling policies
for minimizing effective age-of-information,” in Proc. IEEE Int. Conf.
Comput.Commun.(INFOCOM),2019,pp.109–117.
[5] F.Li,Y.Sang,Z.Liu,B.Li,H.Wu,andB.Ji,“Waitingbutnotaging:
Optimizing information freshness under the pull model,” IEEE/ACM
Trans.Netw., vol.29,no.1,pp.465–478,2021.
[6] J.Holm,A.E.Kalør,F.Chiariotti,B.Soret,S.K.Jensen,T.B.Pedersen,
andP.Popovski,“Freshnessondemand:Optimizingageofinformation
forthequeryprocess,”inProc.IEEEInt.Conf.Commun.(ICC),2021.
[7] O.T.Yavascan, E.T.Ceran, Z.Cakir, E.Uysal, andO. Kaya, “When
topulldataforminimumagepenalty,”inProc.IEEEInt.Symp.Model.
Optim.MobileAdHocWireless Netw. (WiOpt), 2021,pp.1–8.
[8] F.Chiariotti etal.,“Query ageofinformation: Freshness inpull-based
communication,”IEEETrans.Commun.,vol.70,no.3,pp.1606–1622,
2022.
[9] M.E.Ildiz,O.T.Yavascan, E.Uysal,andO.T.Kartal, “Queryageof
information:OptimizingAoIattherighttime,”inProc.IEEEInt.Sym.
Inf.Theory(ISIT),2022,pp.144–149.
[10] M.Hatami,M.Leinonen,andM.Codreanu,“AoIminimizationinstatus
updatecontrolwithenergyharvesting sensors,”IEEETrans.Commun.,
vol.69,no.12,pp.8335–8351,2021.
[11] A. Kosta, N. Pappas, and V. Angelakis, “Age of information: A new
concept, metric,andtool,”Found.TrendsNetw.,vol.12,no.3,2017.
[12] R. D. Yates, Y. Sun, D. R. Brown, S. K. Kaul, E. Modiano, and
S. Ulukus, “Age of information: An introduction and survey,” IEEEJ.
Sel.AreasCommun.,vol.39,no.5,pp.1183–1210,2021.
[13] N. Pappas et al., Age of Information: Foundations and Applications.
Cambr.Univ.Press,2023.
[14] M. Hatami, M. Leinonen, Z. Chen, N. Pappas, and M. Codreanu,
“On-demand AoI minimization in resource-constrained cache-enabled
IoT networks with energy harvesting sensors,” IEEETrans. Commun.,
vol.70,no.11,pp.7446–7463,2022.
[15] G.J.Stamatakis etal.,“Semantics-aware activefaultdetection inIoT,”
inProc.WiOpt, 2022,pp.161–168.
[16] A.Maatouk, M.Assaad,andA.Ephremides,“Theageofincorrect in-
formation:Anenabler ofsemantics-empowered communication,” IEEE
Trans.Wireless Commun.,vol.22,no.4,pp.2621–2635, 2022.
[17] R.L.Stratonovich,“Onthevalueofinformation,”Izv.USSRAcad.Sci.
Tech.Cybern.,no.5,1965.
[18] R. A. Howard, “Information value theory,” IEEETrans. Syst. Cybern.,
vol.2,no.1,1966.
[19] P. Agheli, N. Pappas, and M. Kountouris, “Semantic filtering and
sourcecodingindistributedwirelessmonitoringsystems,”IEEETrans.
Commun.,vol.72,no.6,pp.3290–3304,2024.
[20] N.Pappas andM.Kountouris, “Goal-oriented communication forreal-
time tracking in autonomous systems,” in Proc. IEEE Int. Conf. Au-
tonomous Syst.(ICAS),2021,pp.1–5.
[21] D.Bertsekas,Dynamicprogrammingandoptimalcontrol. AthenaSci.,
2007,vol.2.
[22] E.Altman,ConstrainedMarkovdecisionprocesses. CRCpress,1999.
[23] M.L.Puterman,Markovdecisionprocesses:discretestochasticdynamic
programming. J.Wiley&Sons,2014.
[24] R.Bellman,“Onthetheoryofdynamicprogramming,”Proc.Natl.Acad.
Sci.,vol.38,no.8,pp.716–719,1952.
[25] G. Wood, Bisection global optimization methods, C. A. Floudas and
P.M.Pardalos, Eds. SpringerSci.Bus.Media,2009.
[26] F.J.Beutler andK.W.Ross,“Optimal policies forcontrolled Markov
chains with a constraint,” J. Math. Analy. Appl., vol. 112, no. 1, pp.
236–252, 1985.
[27] P.Tseng,“Solving H-horizon, stationary markovdecision problems in
time proportional to log(H),” Oper. Res.Lett., vol. 9, no. 5, pp.287–
297,1990.
[28] M. L. Littman, T. L. Dean, and L. P. Kaelbling, “On the complexity
of solving Markov decision problems,” Proc. Conf. Uncertainty Artif.
Intell. (UAI),pp.394–402,1995.