[
    {
        "title": "DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks",
        "authors": "Sarah JabbourGregory KondasElla KazerooniMichael SjodingDavid FouheyJenna Wiens",
        "links": "http://arxiv.org/abs/2407.14509v1",
        "entry_id": "http://arxiv.org/abs/2407.14509v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14509v1",
        "summary": "We propose a permutation-based explanation method for image classifiers.\nCurrent image-model explanations like activation maps are limited to\ninstance-based explanations in the pixel space, making it difficult to\nunderstand global model behavior. In contrast, permutation based explanations\nfor tabular data classifiers measure feature importance by comparing model\nperformance on data before and after permuting a feature. We propose an\nexplanation method for image-based models that permutes interpretable concepts\nacross dataset images. Given a dataset of images labeled with specific concepts\nlike captions, we permute a concept across examples in the text space and then\ngenerate images via a text-conditioned diffusion model. Feature importance is\nthen reflected by the change in model performance relative to unpermuted data.\nWhen applied to a set of concepts, the method generates a ranking of feature\nimportance. We show this approach recovers underlying model feature importance\non synthetic and real-world image classification tasks.",
        "updated": "2024-07-19 17:59:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14509v1"
    },
    {
        "title": "T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation",
        "authors": "Kaiyue SunKaiyi HuangXian LiuYue WuZihan XuZhenguo LiXihui Liu",
        "links": "http://arxiv.org/abs/2407.14505v1",
        "entry_id": "http://arxiv.org/abs/2407.14505v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14505v1",
        "summary": "Text-to-video (T2V) generation models have advanced significantly, yet their\nability to compose different objects, attributes, actions, and motions into a\nvideo remains unexplored. Previous text-to-video benchmarks also neglect this\nimportant ability for evaluation. In this work, we conduct the first systematic\nstudy on compositional text-to-video generation. We propose T2V-CompBench, the\nfirst benchmark tailored for compositional text-to-video generation.\nT2V-CompBench encompasses diverse aspects of compositionality, including\nconsistent attribute binding, dynamic attribute binding, spatial relationships,\nmotion binding, action binding, object interactions, and generative numeracy.\nWe further carefully design evaluation metrics of MLLM-based metrics,\ndetection-based metrics, and tracking-based metrics, which can better reflect\nthe compositional text-to-video generation quality of seven proposed categories\nwith 700 text prompts. The effectiveness of the proposed metrics is verified by\ncorrelation with human evaluations. We also benchmark various text-to-video\ngenerative models and conduct in-depth analysis across different models and\ndifferent compositional categories. We find that compositional text-to-video\ngeneration is highly challenging for current models, and we hope that our\nattempt will shed light on future research in this direction.",
        "updated": "2024-07-19 17:58:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14505v1"
    },
    {
        "title": "On Pre-training of Multimodal Language Models Customized for Chart Understanding",
        "authors": "Wan-Cyuan FanYen-Chun ChenMengchen LiuLu YuanLeonid Sigal",
        "links": "http://arxiv.org/abs/2407.14506v1",
        "entry_id": "http://arxiv.org/abs/2407.14506v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14506v1",
        "summary": "Recent studies customizing Multimodal Large Language Models (MLLMs) for\ndomain-specific tasks have yielded promising results, especially in the field\nof scientific chart comprehension. These studies generally utilize visual\ninstruction tuning with specialized datasets to enhance question and answer\n(QA) accuracy within the chart domain. However, they often neglect the\nfundamental discrepancy between natural image-caption pre-training data and\ndigital chart image-QA data, particularly in the models' capacity to extract\nunderlying numeric values from charts. This paper tackles this oversight by\nexploring the training processes necessary to improve MLLMs' comprehension of\ncharts. We present three key findings: (1) Incorporating raw data values in\nalignment pre-training markedly improves comprehension of chart data. (2)\nReplacing images with their textual representation randomly during end-to-end\nfine-tuning transfer the language reasoning capability to chart interpretation\nskills. (3) Requiring the model to first extract the underlying chart data and\nthen answer the question in the fine-tuning can further improve the accuracy.\nConsequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart\ncomprehension. CHOPINLLM effectively interprets various types of charts,\nincluding unannotated ones, while maintaining robust reasoning abilities.\nFurthermore, we establish a new benchmark to evaluate MLLMs' understanding of\ndifferent chart types across various comprehension levels. Experimental results\nshow that CHOPINLLM exhibits strong performance in understanding both annotated\nand unannotated charts across a wide range of types.",
        "updated": "2024-07-19 17:58:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14506v1"
    },
    {
        "title": "M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models",
        "authors": "Seunggeun ChiHyung-gun ChiHengbo MaNakul AgarwalFaizan SiddiquiKarthik RamaniKwonjoon Lee",
        "links": "http://arxiv.org/abs/2407.14502v1",
        "entry_id": "http://arxiv.org/abs/2407.14502v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14502v1",
        "summary": "We introduce the Multi-Motion Discrete Diffusion Models (M2D2M), a novel\napproach for human motion generation from textual descriptions of multiple\nactions, utilizing the strengths of discrete diffusion models. This approach\nadeptly addresses the challenge of generating multi-motion sequences, ensuring\nseamless transitions of motions and coherence across a series of actions. The\nstrength of M2D2M lies in its dynamic transition probability within the\ndiscrete diffusion model, which adapts transition probabilities based on the\nproximity between motion tokens, encouraging mixing between different modes.\nComplemented by a two-phase sampling strategy that includes independent and\njoint denoising steps, M2D2M effectively generates long-term, smooth, and\ncontextually coherent human motion sequences, utilizing a model trained for\nsingle-motion generation. Extensive experiments demonstrate that M2D2M\nsurpasses current state-of-the-art benchmarks for motion generation from text\ndescriptions, showcasing its efficacy in interpreting language semantics and\ngenerating dynamic, realistic motions.",
        "updated": "2024-07-19 17:57:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14502v1"
    },
    {
        "title": "Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery",
        "authors": "Sukrut RaoSweta MahajanMoritz BöhleBernt Schiele",
        "links": "http://arxiv.org/abs/2407.14499v1",
        "entry_id": "http://arxiv.org/abs/2407.14499v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14499v1",
        "summary": "Concept Bottleneck Models (CBMs) have recently been proposed to address the\n'black-box' problem of deep neural networks, by first mapping images to a\nhuman-understandable concept space and then linearly combining concepts for\nclassification. Such models typically require first coming up with a set of\nconcepts relevant to the task and then aligning the representations of a\nfeature extractor to map to these concepts. However, even with powerful\nfoundational feature extractors like CLIP, there are no guarantees that the\nspecified concepts are detectable. In this work, we leverage recent advances in\nmechanistic interpretability and propose a novel CBM approach -- called\nDiscover-then-Name-CBM (DN-CBM) -- that inverts the typical paradigm: instead\nof pre-selecting concepts based on the downstream classification task, we use\nsparse autoencoders to first discover concepts learnt by the model, and then\nname them and train linear probes for classification. Our concept extraction\nstrategy is efficient, since it is agnostic to the downstream task, and uses\nconcepts already known to the model. We perform a comprehensive evaluation\nacross multiple datasets and CLIP architectures and show that our method yields\nsemantically meaningful concepts, assigns appropriate names to them that make\nthem easy to interpret, and yields performant and interpretable CBMs. Code\navailable at https://github.com/neuroexplicit-saar/discover-then-name.",
        "updated": "2024-07-19 17:50:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14499v1"
    }
]