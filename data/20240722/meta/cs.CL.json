[
    {
        "title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
        "authors": "Xun LiangShichao SongZifan ZhengHanyu WangQingchen YuXunkai LiRong-Hua LiFeiyu XiongZhiyu Li",
        "links": "http://arxiv.org/abs/2407.14507v1",
        "entry_id": "http://arxiv.org/abs/2407.14507v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14507v1",
        "summary": "Large language models (LLMs) are expected to respond accurately but often\nexhibit deficient reasoning or generate hallucinatory content. To address\nthese, studies prefixed with ``Self-'' such as Self-Consistency, Self-Improve,\nand Self-Refine have been initiated. They share a commonality: involving LLMs\nevaluating and updating itself to mitigate the issues. Nonetheless, these\nefforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization without examining the motivations behind\nthese works.\n  In this paper, we summarize a theoretical framework, termed Internal\nConsistency, which offers unified explanations for phenomena such as the lack\nof reasoning and the presence of hallucinations. Internal Consistency assesses\nthe coherence among LLMs' latent layer, decoding layer, and response layer\nbased on sampling methodologies. Expanding upon the Internal Consistency\nframework, we introduce a streamlined yet effective theoretical framework\ncapable of mining Internal Consistency, named Self-Feedback. The Self-Feedback\nframework consists of two modules: Self-Evaluation and Self-Update. This\nframework has been employed in numerous studies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, ``Does Self-Feedback Really Work?'' We propose several critical\nviewpoints, including the ``Hourglass Evolution of Internal Consistency'',\n``Consistency Is (Almost) Correctness'' hypothesis, and ``The Paradox of Latent\nand Explicit Reasoning''. Furthermore, we outline promising directions for\nfuture research. We have open-sourced the experimental code, reference list,\nand statistical data, available at\n\\url{https://github.com/IAAR-Shanghai/ICSFSurvey}.",
        "updated": "2024-07-19 17:59:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14507v1"
    },
    {
        "title": "On Pre-training of Multimodal Language Models Customized for Chart Understanding",
        "authors": "Wan-Cyuan FanYen-Chun ChenMengchen LiuLu YuanLeonid Sigal",
        "links": "http://arxiv.org/abs/2407.14506v1",
        "entry_id": "http://arxiv.org/abs/2407.14506v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14506v1",
        "summary": "Recent studies customizing Multimodal Large Language Models (MLLMs) for\ndomain-specific tasks have yielded promising results, especially in the field\nof scientific chart comprehension. These studies generally utilize visual\ninstruction tuning with specialized datasets to enhance question and answer\n(QA) accuracy within the chart domain. However, they often neglect the\nfundamental discrepancy between natural image-caption pre-training data and\ndigital chart image-QA data, particularly in the models' capacity to extract\nunderlying numeric values from charts. This paper tackles this oversight by\nexploring the training processes necessary to improve MLLMs' comprehension of\ncharts. We present three key findings: (1) Incorporating raw data values in\nalignment pre-training markedly improves comprehension of chart data. (2)\nReplacing images with their textual representation randomly during end-to-end\nfine-tuning transfer the language reasoning capability to chart interpretation\nskills. (3) Requiring the model to first extract the underlying chart data and\nthen answer the question in the fine-tuning can further improve the accuracy.\nConsequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart\ncomprehension. CHOPINLLM effectively interprets various types of charts,\nincluding unannotated ones, while maintaining robust reasoning abilities.\nFurthermore, we establish a new benchmark to evaluate MLLMs' understanding of\ndifferent chart types across various comprehension levels. Experimental results\nshow that CHOPINLLM exhibits strong performance in understanding both annotated\nand unannotated charts across a wide range of types.",
        "updated": "2024-07-19 17:58:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14506v1"
    },
    {
        "title": "Evaluating the Reliability of Self-Explanations in Large Language Models",
        "authors": "Korbinian RandlJohn PavlopoulosAron HenrikssonTony Lindgren",
        "links": "http://arxiv.org/abs/2407.14487v1",
        "entry_id": "http://arxiv.org/abs/2407.14487v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14487v1",
        "summary": "This paper investigates the reliability of explanations generated by large\nlanguage models (LLMs) when prompted to explain their previous output. We\nevaluate two kinds of such self-explanations - extractive and counterfactual -\nusing three state-of-the-art LLMs (2B to 8B parameters) on two different\nclassification tasks (objective and subjective). Our findings reveal, that,\nwhile these self-explanations can correlate with human judgement, they do not\nfully and accurately follow the model's decision process, indicating a gap\nbetween perceived and actual model reasoning. We show that this gap can be\nbridged because prompting LLMs for counterfactual explanations can produce\nfaithful, informative, and easy-to-verify results. These counterfactuals offer\na promising alternative to traditional explainability methods (e.g. SHAP,\nLIME), provided that prompts are tailored to specific tasks and checked for\nvalidity.",
        "updated": "2024-07-19 17:41:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14487v1"
    },
    {
        "title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities",
        "authors": "Peng XuWei PingXianchao WuZihan LiuMohammad ShoeybiBryan Catanzaro",
        "links": "http://arxiv.org/abs/2407.14482v1",
        "entry_id": "http://arxiv.org/abs/2407.14482v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14482v1",
        "summary": "In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge\nthe gap between open-access LLMs and leading proprietary models (e.g.,\nGPT-4-Turbo) in long-context understanding and retrieval-augmented generation\n(RAG) capabilities. These two capabilities are essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt and are\ncomplementary to each other, depending on the downstream tasks and\ncomputational budgets. We present a detailed continued training recipe to\nextend the context window of Llama3-70B-base from 8K to 128K tokens, along with\na three-stage instruction tuning process to enhance the model's\ninstruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\nachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context\nunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, we\nfind that the state-of-the-art long-context retriever can alleviate the top-k\ncontext fragmentation issue in RAG, further improving RAG-based results for\nlong-context understanding tasks. We also provide extensive comparisons between\nRAG and long-context solutions using state-of-the-art long-context LLMs.",
        "updated": "2024-07-19 17:35:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14482v1"
    },
    {
        "title": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality",
        "authors": "Jayr PereiraRoberto Lotufo",
        "links": "http://arxiv.org/abs/2407.14467v1",
        "entry_id": "http://arxiv.org/abs/2407.14467v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14467v1",
        "summary": "Evaluating the quality of text generated by large language models (LLMs)\nremains a significant challenge. Traditional metrics often fail to align well\nwith human judgments, particularly in tasks requiring creativity and nuance. In\nthis paper, we propose Check-Eval, a novel evaluation framework leveraging LLMs\nto assess the quality of generated text through a checklist-based approach.\nCheck-Eval can be employed as both a reference-free and reference-dependent\nevaluation method, providing a structured and interpretable assessment of text\nquality. The framework consists of two main stages: checklist generation and\nchecklist evaluation. We validate Check-Eval on two benchmark datasets:\nPortuguese Legal Semantic Textual Similarity and SummEval. Our results\ndemonstrate that Check-Eval achieves higher correlations with human judgments\ncompared to existing metrics, such as G-Eval and GPTScore, underscoring its\npotential as a more reliable and effective evaluation framework for natural\nlanguage generation tasks. The code for our experiments is available at\nhttps://anonymous.4open.science/r/check-eval-0DB4.",
        "updated": "2024-07-19 17:14:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14467v1"
    }
]