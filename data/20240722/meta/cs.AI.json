[
    {
        "title": "DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks",
        "authors": "Sarah JabbourGregory KondasElla KazerooniMichael SjodingDavid FouheyJenna Wiens",
        "links": "http://arxiv.org/abs/2407.14509v1",
        "entry_id": "http://arxiv.org/abs/2407.14509v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14509v1",
        "summary": "We propose a permutation-based explanation method for image classifiers.\nCurrent image-model explanations like activation maps are limited to\ninstance-based explanations in the pixel space, making it difficult to\nunderstand global model behavior. In contrast, permutation based explanations\nfor tabular data classifiers measure feature importance by comparing model\nperformance on data before and after permuting a feature. We propose an\nexplanation method for image-based models that permutes interpretable concepts\nacross dataset images. Given a dataset of images labeled with specific concepts\nlike captions, we permute a concept across examples in the text space and then\ngenerate images via a text-conditioned diffusion model. Feature importance is\nthen reflected by the change in model performance relative to unpermuted data.\nWhen applied to a set of concepts, the method generates a ranking of feature\nimportance. We show this approach recovers underlying model feature importance\non synthetic and real-world image classification tasks.",
        "updated": "2024-07-19 17:59:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14509v1"
    },
    {
        "title": "On Pre-training of Multimodal Language Models Customized for Chart Understanding",
        "authors": "Wan-Cyuan FanYen-Chun ChenMengchen LiuLu YuanLeonid Sigal",
        "links": "http://arxiv.org/abs/2407.14506v1",
        "entry_id": "http://arxiv.org/abs/2407.14506v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14506v1",
        "summary": "Recent studies customizing Multimodal Large Language Models (MLLMs) for\ndomain-specific tasks have yielded promising results, especially in the field\nof scientific chart comprehension. These studies generally utilize visual\ninstruction tuning with specialized datasets to enhance question and answer\n(QA) accuracy within the chart domain. However, they often neglect the\nfundamental discrepancy between natural image-caption pre-training data and\ndigital chart image-QA data, particularly in the models' capacity to extract\nunderlying numeric values from charts. This paper tackles this oversight by\nexploring the training processes necessary to improve MLLMs' comprehension of\ncharts. We present three key findings: (1) Incorporating raw data values in\nalignment pre-training markedly improves comprehension of chart data. (2)\nReplacing images with their textual representation randomly during end-to-end\nfine-tuning transfer the language reasoning capability to chart interpretation\nskills. (3) Requiring the model to first extract the underlying chart data and\nthen answer the question in the fine-tuning can further improve the accuracy.\nConsequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart\ncomprehension. CHOPINLLM effectively interprets various types of charts,\nincluding unannotated ones, while maintaining robust reasoning abilities.\nFurthermore, we establish a new benchmark to evaluate MLLMs' understanding of\ndifferent chart types across various comprehension levels. Experimental results\nshow that CHOPINLLM exhibits strong performance in understanding both annotated\nand unannotated charts across a wide range of types.",
        "updated": "2024-07-19 17:58:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14506v1"
    },
    {
        "title": "Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery",
        "authors": "Sukrut RaoSweta MahajanMoritz BöhleBernt Schiele",
        "links": "http://arxiv.org/abs/2407.14499v1",
        "entry_id": "http://arxiv.org/abs/2407.14499v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14499v1",
        "summary": "Concept Bottleneck Models (CBMs) have recently been proposed to address the\n'black-box' problem of deep neural networks, by first mapping images to a\nhuman-understandable concept space and then linearly combining concepts for\nclassification. Such models typically require first coming up with a set of\nconcepts relevant to the task and then aligning the representations of a\nfeature extractor to map to these concepts. However, even with powerful\nfoundational feature extractors like CLIP, there are no guarantees that the\nspecified concepts are detectable. In this work, we leverage recent advances in\nmechanistic interpretability and propose a novel CBM approach -- called\nDiscover-then-Name-CBM (DN-CBM) -- that inverts the typical paradigm: instead\nof pre-selecting concepts based on the downstream classification task, we use\nsparse autoencoders to first discover concepts learnt by the model, and then\nname them and train linear probes for classification. Our concept extraction\nstrategy is efficient, since it is agnostic to the downstream task, and uses\nconcepts already known to the model. We perform a comprehensive evaluation\nacross multiple datasets and CLIP architectures and show that our method yields\nsemantically meaningful concepts, assigns appropriate names to them that make\nthem easy to interpret, and yields performant and interpretable CBMs. Code\navailable at https://github.com/neuroexplicit-saar/discover-then-name.",
        "updated": "2024-07-19 17:50:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14499v1"
    },
    {
        "title": "Explainable Post hoc Portfolio Management Financial Policy of a Deep Reinforcement Learning agent",
        "authors": "Alejandra de la Rica EscuderoEduardo C. Garrido-MerchanMaria Coronado-Vaca",
        "links": "http://arxiv.org/abs/2407.14486v1",
        "entry_id": "http://arxiv.org/abs/2407.14486v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14486v1",
        "summary": "Financial portfolio management investment policies computed quantitatively by\nmodern portfolio theory techniques like the Markowitz model rely on a set on\nassumptions that are not supported by data in high volatility markets. Hence,\nquantitative researchers are looking for alternative models to tackle this\nproblem. Concretely, portfolio management is a problem that has been\nsuccessfully addressed recently by Deep Reinforcement Learning (DRL)\napproaches. In particular, DRL algorithms train an agent by estimating the\ndistribution of the expected reward of every action performed by an agent given\nany financial state in a simulator. However, these methods rely on Deep Neural\nNetworks model to represent such a distribution, that although they are\nuniversal approximator models, they cannot explain its behaviour, given by a\nset of parameters that are not interpretable. Critically, financial investors\npolicies require predictions to be interpretable, so DRL agents are not suited\nto follow a particular policy or explain their actions. In this work, we\ndeveloped a novel Explainable Deep Reinforcement Learning (XDRL) approach for\nportfolio management, integrating the Proximal Policy Optimization (PPO) with\nthe model agnostic explainable techniques of feature importance, SHAP and LIME\nto enhance transparency in prediction time. By executing our methodology, we\ncan interpret in prediction time the actions of the agent to assess whether\nthey follow the requisites of an investment policy or to assess the risk of\nfollowing the agent suggestions. To the best of our knowledge, our proposed\napproach is the first explainable post hoc portfolio management financial\npolicy of a DRL agent. We empirically illustrate our methodology by\nsuccessfully identifying key features influencing investment decisions, which\ndemonstrate the ability to explain the agent actions in prediction time.",
        "updated": "2024-07-19 17:40:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14486v1"
    },
    {
        "title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities",
        "authors": "Peng XuWei PingXianchao WuZihan LiuMohammad ShoeybiBryan Catanzaro",
        "links": "http://arxiv.org/abs/2407.14482v1",
        "entry_id": "http://arxiv.org/abs/2407.14482v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14482v1",
        "summary": "In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge\nthe gap between open-access LLMs and leading proprietary models (e.g.,\nGPT-4-Turbo) in long-context understanding and retrieval-augmented generation\n(RAG) capabilities. These two capabilities are essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt and are\ncomplementary to each other, depending on the downstream tasks and\ncomputational budgets. We present a detailed continued training recipe to\nextend the context window of Llama3-70B-base from 8K to 128K tokens, along with\na three-stage instruction tuning process to enhance the model's\ninstruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\nachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context\nunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, we\nfind that the state-of-the-art long-context retriever can alleviate the top-k\ncontext fragmentation issue in RAG, further improving RAG-based results for\nlong-context understanding tasks. We also provide extensive comparisons between\nRAG and long-context solutions using state-of-the-art long-context LLMs.",
        "updated": "2024-07-19 17:35:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14482v1"
    }
]