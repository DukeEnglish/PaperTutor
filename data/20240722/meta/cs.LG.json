[
    {
        "title": "Nonlinear Schrödinger Network",
        "authors": "Yiming ZhouCallen MacPheeTingyi ZhouBahram Jalali",
        "links": "http://arxiv.org/abs/2407.14504v1",
        "entry_id": "http://arxiv.org/abs/2407.14504v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14504v1",
        "summary": "Deep neural networks (DNNs) have achieved exceptional performance across\nvarious fields by learning complex nonlinear mappings from large-scale\ndatasets. However, they encounter challenges such as high computational costs\nand limited interpretability. To address these issues, hybrid approaches that\nintegrate physics with AI are gaining interest. This paper introduces a novel\nphysics-based AI model called the \"Nonlinear Schr\\\"odinger Network\", which\ntreats the Nonlinear Schr\\\"odinger Equation (NLSE) as a general-purpose\ntrainable model for learning complex patterns including nonlinear mappings and\nmemory effects from data. Existing physics-informed machine learning methods\nuse neural networks to approximate the solutions of partial differential\nequations (PDEs). In contrast, our approach directly treats the PDE as a\ntrainable model to obtain general nonlinear mappings that would otherwise\nrequire neural networks. As a physics-inspired approach, it offers a more\ninterpretable and parameter-efficient alternative to traditional black-box\nneural networks, achieving comparable or better accuracy in time series\nclassification tasks while significantly reducing the number of required\nparameters. Notably, the trained Nonlinear Schr\\\"odinger Network is\ninterpretable, with all parameters having physical meanings as properties of a\nvirtual physical system that transforms the data to a more separable space.\nThis interpretability allows for insight into the underlying dynamics of the\ndata transformation process. Applications to time series forecasting have also\nbeen explored. While our current implementation utilizes the NLSE, the proposed\nmethod of using physics equations as trainable models to learn nonlinear\nmappings from data is not limited to the NLSE and may be extended to other\nmaster equations of physics.",
        "updated": "2024-07-19 17:58:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14504v1"
    },
    {
        "title": "Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification",
        "authors": "Thomas KwaDrake ThomasAdrià Garriga-Alonso",
        "links": "http://arxiv.org/abs/2407.14503v1",
        "entry_id": "http://arxiv.org/abs/2407.14503v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14503v1",
        "summary": "When applying reinforcement learning from human feedback (RLHF), the reward\nis learned from data and, therefore, always has some error. It is common to\nmitigate this by regularizing the policy with KL divergence from a base model,\nwith the hope that balancing reward with regularization will achieve desirable\noutcomes despite this reward misspecification. We show that when the reward\nfunction has light-tailed error, optimal policies under less restrictive KL\npenalties achieve arbitrarily high utility. However, if error is heavy-tailed,\nsome policies obtain arbitrarily high reward despite achieving no more utility\nthan the base model--a phenomenon we call catastrophic Goodhart. We adapt a\ndiscrete optimization method to measure the tails of reward models, finding\nthat they are consistent with light-tailed error. However, the pervasiveness of\nheavy-tailed distributions in many real-world applications indicates that\nfuture sources of RL reward could have heavy-tailed error, increasing the\nlikelihood of reward hacking even with KL regularization.",
        "updated": "2024-07-19 17:57:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14503v1"
    },
    {
        "title": "Indoor Air Quality Dataset with Activities of Daily Living in Low to Middle-income Communities",
        "authors": "Prasenjit KarmakarSwadhin PradhanSandip Chakraborty",
        "links": "http://arxiv.org/abs/2407.14501v1",
        "entry_id": "http://arxiv.org/abs/2407.14501v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14501v1",
        "summary": "In recent years, indoor air pollution has posed a significant threat to our\nsociety, claiming over 3.2 million lives annually. Developing nations, such as\nIndia, are most affected since lack of knowledge, inadequate regulation, and\noutdoor air pollution lead to severe daily exposure to pollutants. However,\nonly a limited number of studies have attempted to understand how indoor air\npollution affects developing countries like India. To address this gap, we\npresent spatiotemporal measurements of air quality from 30 indoor sites over\nsix months during summer and winter seasons. The sites are geographically\nlocated across four regions of type: rural, suburban, and urban, covering the\ntypical low to middle-income population in India. The dataset contains various\ntypes of indoor environments (e.g., studio apartments, classrooms, research\nlaboratories, food canteens, and residential households), and can provide the\nbasis for data-driven learning model research aimed at coping with unique\npollution patterns in developing countries. This unique dataset demands\nadvanced data cleaning and imputation techniques for handling missing data due\nto power failure or network outages during data collection. Furthermore,\nthrough a simple speech-to-text application, we provide real-time indoor\nactivity labels annotated by occupants. Therefore, environmentalists and ML\nenthusiasts can utilize this dataset to understand the complex patterns of the\npollutants under different indoor activities, identify recurring sources of\npollution, forecast exposure, improve floor plans and room structures of modern\nindoor designs, develop pollution-aware recommender systems, etc.",
        "updated": "2024-07-19 17:53:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14501v1"
    },
    {
        "title": "Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery",
        "authors": "Sukrut RaoSweta MahajanMoritz BöhleBernt Schiele",
        "links": "http://arxiv.org/abs/2407.14499v1",
        "entry_id": "http://arxiv.org/abs/2407.14499v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14499v1",
        "summary": "Concept Bottleneck Models (CBMs) have recently been proposed to address the\n'black-box' problem of deep neural networks, by first mapping images to a\nhuman-understandable concept space and then linearly combining concepts for\nclassification. Such models typically require first coming up with a set of\nconcepts relevant to the task and then aligning the representations of a\nfeature extractor to map to these concepts. However, even with powerful\nfoundational feature extractors like CLIP, there are no guarantees that the\nspecified concepts are detectable. In this work, we leverage recent advances in\nmechanistic interpretability and propose a novel CBM approach -- called\nDiscover-then-Name-CBM (DN-CBM) -- that inverts the typical paradigm: instead\nof pre-selecting concepts based on the downstream classification task, we use\nsparse autoencoders to first discover concepts learnt by the model, and then\nname them and train linear probes for classification. Our concept extraction\nstrategy is efficient, since it is agnostic to the downstream task, and uses\nconcepts already known to the model. We perform a comprehensive evaluation\nacross multiple datasets and CLIP architectures and show that our method yields\nsemantically meaningful concepts, assigns appropriate names to them that make\nthem easy to interpret, and yields performant and interpretable CBMs. Code\navailable at https://github.com/neuroexplicit-saar/discover-then-name.",
        "updated": "2024-07-19 17:50:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14499v1"
    },
    {
        "title": "Conformal Thresholded Intervals for Efficient Regression",
        "authors": "Rui LuoZhixin Zhou",
        "links": "http://arxiv.org/abs/2407.14495v1",
        "entry_id": "http://arxiv.org/abs/2407.14495v1",
        "pdf_url": "http://arxiv.org/pdf/2407.14495v1",
        "summary": "This paper introduces Conformal Thresholded Intervals (CTI), a novel\nconformal regression method that aims to produce the smallest possible\nprediction set with guaranteed coverage. Unlike existing methods that rely on\nnested conformal framework and full conditional distribution estimation, CTI\nestimates the conditional probability density for a new response to fall into\neach interquantile interval using off-the-shelf multi-output quantile\nregression. CTI constructs prediction sets by thresholding the estimated\nconditional interquantile intervals based on their length, which is inversely\nproportional to the estimated probability density. The threshold is determined\nusing a calibration set to ensure marginal coverage. Experimental results\ndemonstrate that CTI achieves optimal performance across various datasets.",
        "updated": "2024-07-19 17:47:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.14495v1"
    }
]