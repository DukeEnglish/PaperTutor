[
    {
        "title": "Taming Data and Transformers for Audio Generation",
        "authors": "Moayed Haji-AliWilli MenapaceAliaksandr SiarohinGuha BalakrishnanSergey TulyakovVicente Ordonez",
        "links": "http://arxiv.org/abs/2406.19388v1",
        "entry_id": "http://arxiv.org/abs/2406.19388v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19388v1",
        "summary": "Generating ambient sounds and effects is a challenging problem due to data\nscarcity and often insufficient caption quality, making it difficult to employ\nlarge-scale generative models for the task. In this work, we tackle the problem\nby introducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. We show that by leveraging metadata\navailable with the audio modality, we can substantially improve the quality of\ncaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from\nthe best available captioning model at four times faster inference speed. We\nthen use AutoCap to caption clips from existing datasets, obtaining 761,000\naudio clips with high-quality captions, forming the largest available\naudio-text dataset. Second, we propose GenAu, a scalable transformer-based\naudio generation architecture that we scale up to 1.25B parameters and train\nwith our new dataset. When compared to state-of-the-art audio generators, GenAu\nobtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. This shows that the quality of data is often as\nimportant as its quantity. Besides, since AutoCap is fully automatic, new audio\nsamples can be added to the training dataset, unlocking the training of even\nlarger generative models for audio synthesis.",
        "updated": "2024-06-27 17:58:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19388v1"
    },
    {
        "title": "The Remarkable Robustness of LLMs: Stages of Inference?",
        "authors": "Vedang LadWes GurneeMax Tegmark",
        "links": "http://arxiv.org/abs/2406.19384v1",
        "entry_id": "http://arxiv.org/abs/2406.19384v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19384v1",
        "summary": "We demonstrate and investigate the remarkable robustness of Large Language\nModels by deleting and swapping adjacent layers. We find that deleting and\nswapping interventions retain 72-95\\% of the original model's prediction\naccuracy without fine-tuning, whereas models with more layers exhibit more\nrobustness. Based on the results of the layer-wise intervention and further\nexperiments, we hypothesize the existence of four universal stages of inference\nacross eight different models: detokenization, feature engineering, prediction\nensembling, and residual sharpening. The first stage integrates local\ninformation, lifting raw token representations into higher-level contextual\nrepresentations. Next is the iterative refinement of task and entity-specific\nfeatures. Then, the second half of the model begins with a phase transition,\nwhere hidden representations align more with the vocabulary space due to\nspecialized model components. Finally, the last layer sharpens the following\ntoken distribution by eliminating obsolete features that add noise to the\nprediction.",
        "updated": "2024-06-27 17:57:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19384v1"
    },
    {
        "title": "Suri: Multi-constraint Instruction Following for Long-form Text Generation",
        "authors": "Chau Minh PhamSimeng SunMohit Iyyer",
        "links": "http://arxiv.org/abs/2406.19371v1",
        "entry_id": "http://arxiv.org/abs/2406.19371v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19371v1",
        "summary": "Existing research on instruction following largely focuses on tasks with\nsimple instructions and short responses. In this work, we explore\nmulti-constraint instruction following for generating long-form text. We create\nSuri, a dataset with 20K human-written long-form texts paired with\nLLM-generated backtranslated instructions that contain multiple complex\nconstraints. Because of prohibitive challenges associated with collecting human\npreference judgments on long-form texts, preference-tuning algorithms such as\nDPO are infeasible in our setting; thus, we propose Instructional ORPO\n(I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving\nnegative feedback from dispreferred responses, I-ORPO obtains negative feedback\nfrom synthetically corrupted instructions generated by an LLM. Using Suri, we\nperform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The\nresulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts\n(~5K tokens) than base models without significant quality deterioration. Our\nhuman evaluation shows that while both SFT and I-ORPO models satisfy most\nconstraints, Suri-I-ORPO generations are generally preferred for their coherent\nand informative incorporation of the constraints. We release our code at\nhttps://github.com/chtmp223/suri.",
        "updated": "2024-06-27 17:50:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19371v1"
    },
    {
        "title": "The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models",
        "authors": "Xiliang ZhuShayna GardinerTere RoldánDavid Rossouw",
        "links": "http://arxiv.org/abs/2406.19358v1",
        "entry_id": "http://arxiv.org/abs/2406.19358v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19358v1",
        "summary": "Sentiment analysis serves as a pivotal component in Natural Language\nProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-R\nand mT5 have contributed to the increasing interest in cross-lingual sentiment\nanalysis. The recent emergence in Large Language Models (LLM) has significantly\nadvanced general NLP tasks, however, the capability of such LLMs in\ncross-lingual sentiment analysis has not been fully studied. This work\nundertakes an empirical analysis to compare the cross-lingual transfer\ncapability of public Small Multilingual Language Models (SMLM) like XLM-R,\nagainst English-centric LLMs such as Llama-3, in the context of sentiment\nanalysis across English, Spanish, French and Chinese. Our findings reveal that\namong public models, SMLMs exhibit superior zero-shot cross-lingual performance\nrelative to LLMs. However, in few-shot cross-lingual settings, public LLMs\ndemonstrate an enhanced adaptive potential. In addition, we observe that\nproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but\nare outpaced by public models in few-shot scenarios.",
        "updated": "2024-06-27 17:38:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19358v1"
    },
    {
        "title": "DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions",
        "authors": "Nigel FernandezAlexander ScarlatosSimon WoodheadAndrew Lan",
        "links": "http://arxiv.org/abs/2406.19356v1",
        "entry_id": "http://arxiv.org/abs/2406.19356v1",
        "pdf_url": "http://arxiv.org/pdf/2406.19356v1",
        "summary": "High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is\ncrucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math\nMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a\nhuman evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.",
        "updated": "2024-06-27 17:37:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.19356v1"
    }
]