Efficient World Models with Context-Aware Tokenization
VincentMicheli*1 EloiAlonso*1 Franc¸oisFleuret1
Abstract Model-based RL (MBRL) (Sutton & Barto, 2018) is hy-
pothesized to be the key for scaling up deep RL agents
Scaling up deep Reinforcement Learning (RL)
(LeCun,2022). Indeed,worldmodels(Ha&Schmidhuber,
methodspresentsasignificantchallenge. Follow-
2018)offeradiverserangeofcapabilities:lookaheadsearch
ingdevelopmentsingenerativemodelling,model-
(Schrittwieseretal.,2020;Yeetal.,2021),learninginimag-
based RL positions itself as a strong contender.
ination(Sutton,1991;Hafneretal.,2023),representation
Recentadvancesinsequencemodellinghaveled
learning(Schwarzeretal.,2021;D’Oroetal.,2023),andun-
toeffectivetransformer-basedworldmodels,al-
certaintyestimation(Pathaketal.,2017;Sekaretal.,2020). beit at the price of heavy computations due to
Inessence,MBRLshiftsthefocusfromtheRLproblemto
the long sequences of tokens required to accu-
agenerativemodellingproblem,wherethedevelopmentof
rately simulate environments. In this work, we
propose∆-IRIS,anewagentwithaworldmodel anaccurateworldmodelsignificantlysimplifiespolicytrain-
ing. Inparticular,policieslearntintheimaginationofworld
architecturecomposedofadiscreteautoencoder
modelsarefreedfromsampleefficiencyconstraints,acom-
thatencodesstochasticdeltasbetweentimesteps
monlimitationofRLagentsthatismagnifiedincomplex
and an autoregressive transformer that predicts
environmentswithslowrollouts.
future deltas by summarizing the current state
of the world with continuous tokens. In the Recently, the IRIS agent (Micheli et al., 2023) achieved
Crafter benchmark, ∆-IRIS sets a new state of strongresultsintheAtari100kbenchmark(Bellemareetal.,
the art at multiple frame budgets, while being 2013;Kaiseretal.,2020). IRISintroducedaworldmodel
an order of magnitude faster to train than pre- composedofadiscreteautoencoderandanautoregressive
vious attention-based approaches. We release transformer,castingdynamicslearningasasequencemod-
ourcodeandmodelsathttps://github.com/ ellingproblemwherethetransformercomposesovertimea
vmicheli/delta-iris. vocabularyofimagetokensbuiltbytheautoencoder. This
approachopenedupavenuesforfuturemodel-basedmeth-
odstocapitalizeonadvancesingenerativemodelling(Ville-
1.Introduction
gasetal.,2022;Achiametal.,2023),andhasalreadybeen
adoptedbeyonditsoriginaldomain(comma.ai,2023;Hu
DeepReinforcementLearning(RL)methodshaverecently
deliveredimpressiveresults(Yeetal.,2021;Hafneretal.,
etal.,2023). However,initscurrentform,scalingIRISto
morecomplexenvironmentsiscomputationallyprohibitive.
2023; Schwarzer et al., 2023) in traditional benchmarks
Indeed,suchanendeavorrequiresalargenumberoftokens
(Bellemareetal.,2013;Tassaetal.,2018). Inlightofthe
to encode visually challenging frames. Besides, sophisti-
evermorecomplexdomainstackledbythelatestgenerations
cateddynamicsmayrequiretostorenumeroustimesteps
ofgenerativemodels(Rombachetal.,2022;Achiametal.,
inmemorytoreasonaboutthepast,ultimatelymakingthe
2023), the prospect of training agents in more ambitious
imaginationprocedureexcessivelyslow.Hence,underthese
environments (Kanervisto et al., 2022) may hold signifi-
constraints,maintainingafavorableimagined-to-collected
cant appeal. However, that leap forward poses a serious
dataratioispracticallyinfeasible.
challenge: deepRLarchitectureshavebeencomparatively
smallerandlesssample-efficientthantheir(self-)supervised In the present work, we introduce ∆-IRIS, a new agent
counterparts. Incontrast,moreintricateenvironmentsne- capableofscalingtovisuallycomplexenvironmentswith
cessitate models with greater representational power and lengthiertimehorizons. ∆-IRISencodesframesbyattend-
havehigherdatarequirements. ing to the ongoing trajectory of observations and actions,
effectivelydescribingstochasticdeltasbetweentimesteps.
*Equalcontribution 1UniversityofGeneva,Switzerland.Corre-
spondenceto: <first.last@unige.ch>. Thisenrichedconditioningschemedrasticallyreducesthe
numberoftokenstoencodeframes,offloadsthedeterminis-
Proceedings of the 41st International Conference on Machine ticaspectsofworldmodellingtotheautoencoder,andlets
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by
theautoregressivetransformerfocusonstochasticdynamics.
theauthor(s).
1
4202
nuJ
72
]GL.sc[
1v02391.6042:viXraEfficientWorldModelswithContext-AwareTokenization
xˆ1 xˆ2 xˆ1 xˆ2
Decoder
... ... ... ...
z11 z 1KI z21 z 2KI x0 a0 z11 z1K x1 a1 z21 z2K
Encoder
x1 x2 x0 a0 x1 a1 x2 a2
Figure1.DiscreteautoencoderofIRIS(Michelietal.,2023)(left)and∆-IRIS(right). IRISencodesanddecodesframesindependently,
meaningthatz
t
hastocarryalltheinformationnecessarytoreconstructx t. Ontheotherhand, ∆-IRIS’encoderanddecoderare
conditionedonpastframesandactions,thusz onlyhastocapturewhathaschangedandthatcannotbeinferredfromactions,i.e.the
t
stochasticdelta.Thisconditioningschemeenablesustodrasticallyreducethenumberoftokensrequiredtoencodeaframewithminimal
loss(K ≪K ),whichiscriticaltospeeduptheautoregressivetransformerthatpredictsfuturetokens.
I
Nonetheless, substitutingthesequenceofabsoluteimage apolicyp (a |x ,a )thatmaximizestheexpectedsum
π t ≤t <t
tokenswithasequenceof∆-tokensmakesthetaskofthe ofrewardsE [(cid:80) γtr ],withdiscountfactorγ ∈(0,1).
π t≥0 t
autoregressivemodelmorearduous. Inordertopredictthe
Learning in imagination (Sutton, 1991; Sutton & Barto,
nexttransition,itmayonlyreasonoverprevious∆-tokens,
2018) consists of 3 stages that are repeated alternatively:
and thus faces the challenge of integrating over multiple
experience collection, world model learning, and policy
timestepsasawaytoformarepresentationofthecurrent
improvement. Strikingly,theagentlearnsbehaviourspurely
state of the world. To resolve this issue, we modify the
withinitsworldmodel,andrealexperienceisonlyleveraged
sequenceoftheautoregressivemodelbyinterleavingcontin-
tolearntheenvironmentdynamics.
uousI-tokens,thatsummarizesuccessiveworldstateswith
frameembeddings,anddiscrete∆-tokens. IntheveinofIRIS(Michelietal.,2023),ourworldmodel
iscomposedofadiscreteautoencoder(VanDenOordetal.,
IntheCrafterbenchmark(Hafner,2022),∆-IRISexhibits
2017) and an autoregressive transformer (Vaswani et al.,
favorable scaling properties: the agent solves 17 out of
2017;Radfordetal.,2019),albeitwithnewconditioning
22 tasks after 10M frames of data collection, supersedes
schemes and architectures. We first expose IRIS’ world
DreamerV3 (Hafner et al., 2023) at multiple frame bud-
modelinSection2.1,thenpresent∆-IRIS’autoencoderand
gets,andtrains10timesfasterthan IRIS. Inaddition,we
autoregressivemodelinSections2.2and2.3,respectively.
include results in the sample-efficient setting with Atari
Finally,wedescribethepolicyimprovementphaseinSec-
games. Throughexperiments,weprovideevidencethat∆-
tion2.4. AppendixAgivesadetailedbreakdownofmodel
IRISlearnstodisentanglethedeterministicandstochastic
architecturesandhyperparameters.
aspects of world modelling. Moreover, we conduct ab-
lations to validate the new conditioning schemes for the
2.1.Background: IRIS
autoencoderandtransformermodels.
High-dimensionalimagesareconvertedintotokenswitha
2.Method discreteautoencoder(E I,D I)(VanDenOordetal.,2017).
TheencoderE
I
: Rh×w×3 → {1,...,N I}KI mapsanin-
WeconsideraPartiallyObservableMarkovDecisionPro- putimagex intoK tokensfromavocabularyofsizeN .
t I I
cess (POMDP) (Sutton & Barto, 2018). The transition, Thediscretizationisdonebypickingtheindexofthevector
reward, and episode termination dynamics are captured inthevocabularyembeddingtablethatisclosesttotheen-
by the conditional distributions p(x t+1 | x ≤t,a ≤t) and coderoutputy t ∈RKI×d. TheK I tokensarethendecoded
p(r t,d t | x ≤t,a ≤t), where x t ∈ X = R3×h×w is an im- backintoanimagewithD I :{1,...,N I}KI →Rh×w×3.
age observation, a t ∈ A = {1,...,A} a discrete action, ThisdiscreteautoencoderistrainedwithL 1reconstruction,
r t ∈ Rascalarreward,andd t ∈ {0,1}indicatesepisode perceptual(Esseretal.,2021)andcommitmentlosses(Van
termination. Thereinforcementlearningobjectiveistofind DenOordetal.,2017)computedoncollectedframes.
2EfficientWorldModelswithContext-AwareTokenization
dˆ
0
dˆ
1
dˆ
2
rˆ0 rˆ1 rˆ2
... ... ...
zˆ11 zˆ12 zˆ 1K−1 zˆ1K zˆ21 zˆ22 zˆ 2K−1 zˆ2K zˆ31 zˆ32 zˆ 3K−1 zˆ3K
... ... ...
x˜0 a0 z11 z12 z 1K−1 z1K x˜1 a1 z21 z22 z 2K−1 z2K x˜2 a2 z31 z32 z 3K−1 z3K
x0 x1 x2
Figure2.Unrollingdynamicsovertime.Ateachtimestep(separatedbydashedlines),theGPT-likeautoregressivetransformerGpredicts
the∆-tokensforthenextframe,aswellastherewardandapotentialepisodetermination.Itsinputsequenceconsistsofactiontokens,
∆-tokens,andI-tokens,namelycontinuousimageembeddingsthatalleviatetheneedtoattendtopast∆-tokensforworldmodelling.
Morespecifically,aninitialframex
0
isembeddedintoI-tokenx˜ 0. Fromx˜
0
anda 0,Gpredictstherewardrˆ 0,episodetermination
dˆ ∈{0,1},andinanautoregressivemannerzˆ =(zˆ1,...,zˆK),the∆-tokensforthenextframe. Notethat,duringtheimagination
0 1 1 1
procedure,thenextframe(strippedbox)iscomputedbythedecoderDbasedonpreviousframes,actions,andthe∆-tokensgeneratedby
G,i.e.x =D(x ,a ,zˆ ).
1 0 0 1
ThetransformerG modelstheenvironmentdynamicsby Onepossiblesolutiontoachievefastworldmodellingwith
I
operatingoveraninputsequenceofimageandactiontokens minimal loss is to condition the autoencoder on previous
(z1,...,zKI,a ,z1,...,zKI,a ,...,z1,...,zKI,a ). framesandactions. Intuitively,encodingaframegivenpre-
0 0 0 1 1 1 t t t
Image and action tokens are embedded with learnt viousframesconsistsindescribingwhathaschanged,the
lookup tables. At each time step, G predicts delta,betweensuccessivetimesteps.Inmanyenvironments,
I
the transition, reward, and termination distributions: thedeltabetweenframesisoftenmuchsimplertodescribe
p (zˆ |z ,a )withzˆk ∼p (zˆk |z ,a ,z<k), than the frames themselves. As a matter of fact, when
GI t+1 ≤t ≤t t+1 GI t+1 ≤t ≤t t+1
p (rˆ|z ,a ), and p (dˆ|z ,a ). The model is thetransitionfunctionisdeterministic,addingpreviousac-
GI t ≤t ≤t GI t ≤t ≤t
trainedwithacross-entropylossonsegmentssampledfrom tionstotheconditioningofthedecoderresultsinaworld
pastexperience. model,withouttheneedtoencodeanyinformationbetween
timesteps. However,mostenvironmentsofinterestfeature
Atahighlevel,theautoencoderbuildsavocabularyofimage
stochasticdynamics,andapartfromaleatoricuncertainty,
tokenstoencodeeachframe,andthetransformercaptures
architecturallimitationssuchastheagent’smemorymay
theenvironmentdynamicsbyautoregressivelycomposing
induceadditionalepistemicuncertainty. Hence, thedelta
the vocabulary over time. As a result, this world model
betweentwotimestepsusuallyconsistsofdeterministicand
iscapableofattendingtoprevioustimestepstomakeits
stochasticcomponents.
predictions,andmodelsthejointlawoffuturelatentstates.
Forinstance,anagentmovingfromonesquaretoanother
inagrid-likeenvironmentwhenpressingmovementkeys
2.2.Disentanglingdeterministicandstochastic
canbeseenasadeterministiccomponentofthetransition.
dynamics
On the other hand, the sudden apparition of an enemy in
IRIS (Michelietal.,2023)encodesframesindependently, anearbysquareisarandomevent. Interestingly,onlythe
makingnoassumptionabouttemporalredundancywithin stochasticfeaturesofatransitionshouldbeencoded,and
trajectories. One major drawback of this general formu- theautoencodercoulddirectlylearntomodelthedetermin-
lation is that, in environments with visually challenging isticdynamics,whichdonotrequiretheexpressivityand
frames, a large number of tokens is required to encode abilitytohandlemultimodalityofanautoregressivemodel.
frameslosslessly. Consequently,computationswiththedy- Therefore,whenautoencondingframesbyconditioningon
namicsmodelbecomeincreasinglyprohibitive,astheatten- previous frames and actions, a frame encoding may only
tionmechanismscalesquadraticallywithsequencelength. consistofahandfulof∆-tokens,insteadofalargenumber
Therefore,limitingcomputationundersuchatrade-offmay ofimagetokensdescribingframesindependently.
resultindegradedperformance(Michelietal.(2023)app.
E)
Section3.4providesempiricalevidencethat∆-IRIS’autoen-
coderlearnstoencodeframesinsuchfashion,andFigure1
illustratesthenewconditioningschemeoftheautoencoder.
3EfficientWorldModelswithContext-AwareTokenization
∆-tokenssampledrandomly
∆-tokenssampledbytheautoregressivetransformer
t=0 t=4 t=5 t=9 t=10 t=12
Figure3.Evidenceofdynamicsdisentanglement.Twotrajectoriesareimaginedwithdifferentwaysofgenerating∆-tokens.Inthetop
trajectory,∆-tokensaresampledrandomly.Inthebottomtrajectory,theautoregressivetransformerpredictsfuture∆-tokens.Thesame
startingframe(t = 0)andsequenceofactionsareused. Withrandom∆-tokens,thedeterministicaspectsofthedynamics(layout,
movement,items,crafting)arestillproperlymodelled,butthestochasticdynamics(mobs,healthindicators)becomeproblematic.For
instance,theagentsuccessfullycutsdownatreebetweent=4andt=5,anduseswoodplankstobuildacraftingtablebetweent=10
andt=12.Weobservethatthesedynamicsaremodelledinthesamewaywhether∆-tokensaresampledrandomlyornot.However,in
thetoptrajectory,largequantitiesofcowsappearanddisappearfromthescreenincoherently,whereasthebottomtrajectorydoesnot
displaysucherraticpatterns.Thisexperimentshowsthat∆-IRISencodesstochasticdeltasbetweentimestepswith∆-tokens,andits
decoderhandlesthedeterministicaspectsofworldmodelling.AppendixFcontainsadditionalexamples.
Moreformally,foranysetY,wedenoteS (Y)=(cid:83)n Yi 2.3.Modellingstochasticdynamics
n i=1
the set of tuples of elements from Y of maximum length
Whileitshouldbepossibletopredictfuture∆-tokens,given
n, and S(Y) = S (Y). Let Z = {1,...,N} a vocab-
∞ astartingimage,pastactionsand∆-tokens,wefoundthis
ulary of discrete tokens. Given past images and actions
taskmuchmoredifficultthansimplypredictingfutureimage
(x ,a ,...,x ,a ),theencoderE :S(X×A)×X →
0 0 t−1 t−1
ZK converts an image x into z = (z1,...,zK), a se-
tokens,givenpastimagetokensandactions,asinIRIS.
t t t t
quence of K discrete ∆-tokens. The encoder is param- To better understand why this is the case, let us consider
eterized by a Convolutional Neural Network (CNN) (Le- another example: in a grid environment, ∆-tokens may
Cun et al., 1989). Actions are embedded with a learnt describe the unpredictable movement of an enemy, ran-
lookup table and concatenated channel-wise with frames. domly jumping from one square to another at every time
We use vector quantization (Van Den Oord et al., 2017; step. Basedontheinitialenemylocationandafteronlya
Esser et al., 2021) with factorized and normalized codes fewtimesteps,itbecomesincreasinglydifficulttopredict
(Yu et al., 2021) to discretize the encoder’s continuous iftheenemyandtheagentarelocatedonthesamesquare,
outputs. The CNN decoder D : S(X ×A)×ZK → X which could trigger a battle and make the enemy disap-
reconstructsanimagexˆ tfrompastframes,actionsand∆- pear. Indeed,situatingthetwoentitiesinvolvesreasoning
tokens (x 0,a 0,...,x t−1,a t−1,z t). Action and ∆-tokens abouttheinitialobservation,andintegratingoverallofthe
areembeddedwithlearntlookuptables,andconcatenated previousactionand∆-tokens,whichmayhaveacomplex
channel-wise with feature maps obtained by forwarding dependencestructure.
framesthroughanauxiliaryCNN.
Toaddressthisproblem,wealterthesequenceofthedynam-
Thediscreteautoencoderistrainedonpreviouslycollected icsmodelbyinterleavingcontinuousI-tokens,inreference
trajectories with a weighted combination of L 1, L 2 and to MPEG’s I-frames (Richardson, 2004), and discrete ∆-
max-pixel (Anand et al., 2022) reconstruction losses, as tokens. I-tokensalleviatetheneedofintegratingoverpast
wellasacommitmentloss(VanDenOordetal.,2017). The ∆-tokens to form a representation of the current state of
codebookisupdatedwithanexponentialmovingaverage theworld,i.e. theydeploya“soft”Markovblanketforthe
(Razavietal.,2019)andweuseastraight-throughestimator predictionofthenext∆-tokens.
(Bengioetal.,2013)toenablebackpropagation.
4EfficientWorldModelswithContext-AwareTokenization
Table1.Returns,numberofparameters,andframescollectedpersecond(FPS)forthemethodsconsidered.WecomputeFPSasthetotal
numberofenvironmentframescollecteddividedbythetrainingduration.∆-IRISoutperformsDreamerV3forlargerframebudgets,and
is10xfasterthanIRIS(64tokens).
Method Return@1M Return@5M Return@10M #Parameters FPS
∆-IRIS 7.7(0.5) 15.4(0.4) 16.1(0.1) 25M 20
DreamerV3XL 9.2(0.3) 14.2(0.2) 15.1(0.3) 200M 30
IRIS(64tokens) 5.5(0.7) - - 48M 2
∆-IRISw/oI-tokens 6.6(0.2) 10.4(0.5) 12.6(0.8) 24M 22
DreamerV3M 6.2(0.5) 12.6(0.7) 13.7(0.8) 37M 40
IRIS(16tokens) 4.4(0.1) - - 50M 6
Gisparameterizedbyastackoftransformerencoderlayers
18 Δ-IRIS with causal self-attention (Vaswani et al., 2017; Radford
Δ-IRIS w/o I-tokens
IRIS (64 tokens) etal.,2019). Itistrainedwithacross-entropylossfortransi-
16 IRIS (16 tokens)
tionandterminationpredictions,andwefollowDreamerV3
DreamerV3 XL
DreamerV3 M (Hafneretal.,2023)inusingdiscreteregressionwithtwo-
14
hottargetsandsymlogscalingforrewardprediction(Imani
12 &White,2018).
10
2.4.Policyimprovement
8
Duringthepolicyimprovementphase,thepolicyπlearns
6 intheimaginationPOMDPofitsworldmodel,composedof
theautoencoder(E,D)andthedynamicsmodelG.
4
Attimestept, thepolicyobservesareconstructedimage
1M 2M 3M 4M 5M 6M 7M8M9M
Number of frames observation xˆ t and samples action a t ∼ π(a t|xˆ ≤t). The
Figure4.ReturnsatmultipleframebudgetsintheCrafterbench- worldmodelthenpredictstherewardrˆ t,theepisodeenddˆ t,
mark. ∆-IRISachieveshigherreturnsthanDreamerV3beyond and the next observation xˆ
t+1
= D(xˆ ≤t,aˆ ≤t,zˆ ≤t,zˆ t+1),
3Mframes,andsurpassesIRISforallframebudgetsconsidered. withzˆ ∼p (zˆ |xˆ ,aˆ ,zˆ ). Theimaginationpro-
t+1 G t+1 ≤t ≤t ≤t
RemovingI-tokensfromtheinputsequenceoftheautoregressive cedureisinitializedwitharealobservationx sampledfrom
0
transformersignificantlyhurtsperformance. pastexperience,andisrolledoutforHsteps.Theprocedure
stopsifanepisodeterminationispredictedbeforereaching
WeobtainI-tokensbyforwardingframesthroughanaux- theimaginationhorizon.
iliary CNN ateachtimestep. Theyarenotproducedbya
Weemploytoalargeextenttheactor-critictrainingmethod
discreteautoencoder.SinceI-tokensarenotpredictedbythe
used for IRIS (Micheli et al., 2023). A value baseline is
modelbutratherenrichitsconditioning,therearenoincen-
trainedtopredictλ-returns(Sutton&Barto,2018)withthe
tivestoincludealossydiscretizationoperatorortooptimize
samediscreteregressionobjectiveasforrewardprediction.
a reconstruction loss. Instead, they are optimized end-to-
ThepolicyoptimizestheREINFORCEwithvaluebaseline
end with the learning objectives of the dynamics model.
(Sutton & Barto, 2018) learning objective over imagined
Withthisimprovedconditioning,thedynamicsmodelper-
trajectories.Explorationisencouragedbyaddinganentropy
ceivestheongoingtrajectorywithamixtureofcontinuous
maximizationtermtothepolicy’sobjective.
anddiscreterepresentations,whilemakingitspredictions
autoregressivelyinadiscretespace.
3.Experiments
Figure 2 displays the input sequence of the dynam-
ics model and the quantities it predicts. Given a se- In our experiments, we consider the Crafter benchmark
quence of past I-tokens, action tokens, and ∆-tokens (Hafner, 2022) to illustrate ∆-IRIS’ ability to scale to a
(x˜ ,a ,z1,...,zK,...,x˜ ,a ,z1,...,zk), the dy- visually rich environment with large frame budgets. Be-
0 0 1 1 t−1 t−1 t t
namics model G outputs a categorical distribution on Z sides,wealsoincludeAtari100kgames(Bellemareetal.,
forthenext∆-tokenzˆk+1∼p (zˆk+1|x˜ ,z ,a ,z≤k). 2013;Kaiseretal.,2020)inAppendixCtoshowcasethe
t G t <t <t <t t
Italsopredictsdistributionsforrewardsp (rˆ|x˜ z ,a ) performanceandspeedofouragentinthesample-efficient
G t ≤t ≤t ≤t
andepisodeterminationsp (dˆ|x˜ ,z ,a ). setting.
G t ≤t ≤t ≤t
5
nruteREfficientWorldModelswithContext-AwareTokenization
∆-IRIS4tokens IRIS16tokens
Figure5.Bottom1%testframesautoencodedby∆-IRIS(4tokens)andIRIS(Michelietal.,2023)(16tokens). Eachtokentakesa
valuein{1,2,...,1023,1024},i.e.∆-IRISencodesframeswith4×log (1024)=40bitswhileIRISuses160bits.Originalframes,
2
reconstructions,anderrorsarerespectivelydisplayedinthetop,middle,andbottomrows.Evenintheworstinstances,∆-IRISmakes
onlyminorerrors,whereasIRISfailstoaccuratelyreconstructframes.Theseerrorsseverelyhampertheagent’sperformance,asitpurely
learnsbehavioursfromframesgeneratedbyitsautoencoder.
WeintroducetheCrafterbenchmarkandbaselinesinSec- Wekeepafixedimagined-to-collecteddataratioof64to
tion3.1.Then,wepresentourresultsinSection3.2.Finally, balancespeedandperformance. Ourexperimentsrunona
inSections3.3and3.4,weproposequalitativeexperiments NvidiaA10040GBGPU,with5seedsforallmethodsand
tovalidate∆-IRIS’worldmodelarchitecture,andbetterour ablations. Weevaluateeachrunbycomputingtheaverage
understandingofhowthemodelrepresentsinformation. returnover256testepisodesevery1Mframes. Notethat
westoptheIRISexperimentsbefore10Mframesbecause
3.1.Benchmarkandbaselines theyareprohibitivelyslow.
Crafter(Hafner,2022)isaprocedurallygeneratedenviron-
3.2.Results
ment, inspired by the video game Minecraft, with visual
inputs,adiscreteactionspaceandnon-deterministicdynam- Table1exhibitskeymetricsandFigure4displayslearning
ics. Byincorporatingmechanicsfromsurvivalgamesanda curves. After10Mframesofdatacollection,∆-IRISsolves
technologytree,thisbenchmarkevaluatesabroadrangeof onaverage17outof22tasks,settinganewstateoftheart
agentcapabilitiessuchasgeneralization,exploration,and fortheCrafterbenchmark. Beyondthe3Mframesmark,
credit assignment. During each episode, the agent’s goal ∆-IRIS consistently achieves higher returns than Dream-
is to solve as many tasks as possible, e.g. slaying mobs, erV3,althoughDreamerV3isbettersuitedforthesmallest
craftingitems,andmanaginghealthindicators. framebudgets. Akeydifferencebetweenthetwomethods
isthat∆-IRIS doesnotleveragetherepresentationsofits
Regarding baselines, we consider two model-based RL
worldmodelforpolicylearning,whichmaybeespecially
agentslearninginimagination: IRIS(Michelietal.,2023)
usefulinthescarcedataregime. Asourmainobjectiveis
andDreamerV3(Hafneretal.,2023). Werunseveralvari-
todevelopworldmodelarchitecturesthatscaletocomplex
ants: IRIS (16 tokens), encoding frames with K I = 16 environmentsandlargerframebudgets,weleavethisexplo-
tokens, IRIS (64tokens), encodingframeswithK I = 64 rationtofuturework. ∆-IRISoutperformsIRISforallframe
tokens,andconfigurationsofDreamerV3ofdifferentsizes,
budgetsconsidered,whiletraininganorderofmagnitude
namelyDreamerV3XLandDreamerV3M.Todemonstrate
faster. Finally,removingI-tokensfromthesequenceofthe
the importance of I-tokens, we also run ∆-IRIS without
dynamicsmodeldrasticallyhurtsperformance.
I-tokensinthesequenceofthetransformer,i.e. Gonlyop-
eratesoverthefirstframeaswellasactionsand∆-tokens.
6EfficientWorldModelswithContext-AwareTokenization
AutoregressivetransformerwithI-tokens
AutoregressivetransformerwithoutI-tokens
Figure6.Trajectoriesimaginedwith(top)andwithout(bottom)I-tokens. Inthetoptrajectory,weobservemorethan30secondsof
gameplaygeneratedby∆-IRIS’worldmodel. Awidevarietyofmechanicshavebeeninternalized: scrolling,choppingdowntrees,
buildingacraftingtable, miningiron, craftingpickaxes, etc. However, removingI-tokensfromthesequenceoftheautoregressive
transformermakesthetaskofpredictingfuture∆-tokensdrasticallyharderasevidencedbytheagentglitchingthroughwallsandwaterin
thebottomtrajectory. Thesemistakesultimatelyhinderthepolicyimprovementphase,sincetheagentwillreinforcebehavioursina
worldthatdoesnotproperlyreflectitsenvironment.
Webelievethatachievinghigherreturnsatthe10Mframes Figure5illustratesthebottom1%autoencodedtestframes
capposesahardexplorationproblem. Indeed,threeofthe withandwithoutconditioningtheautoencoderontheon-
missingfourtasksrequirecraftingnewtoolsinthepresence goingtrajectory(i.e. reconstructionswith∆-IRISvsIRIS).
ofanearbycraftingtableandfurnace. Discoveringthese Withasfewas4tokensperframe,∆-IRIS’autoencoderis
toolswithanaiveexplorationstrategyishighlyunlikely,and abletoencodeframeswithminimalloss. Ontheotherhand,
wehaveobservedonlyafewoccurrencesofthoseevents withoutaccesstopreviousframesandactions,andevenwith
throughouttrainingruns. 16tokens,IRIS’autoencoderproducespoorreconstructions.
Withtoofewtrainingsamples,theworldmodelisunableto Figure6displaystrajectoriesimaginedtoillustratewhether
internalizethesenewmechanicsandreflectthemduringthe crucial mechanics have been internalized by the world
imaginationprocedure. Wehypothesizethatabiaseddata model,whenincludingI-tokensinthesequenceoftheau-
samplingprocedure(Kauvaretal.,2023)couldbethekey toregressive transformer or not. We observe that, with I-
tounlockthemissingachievements. tokens,amultitudeofgamemechanicsarewellunderstood,
but in the absence of I-tokens the world model is unable
3.3.Worldmodelanalysis tosimulatekeyconcepts. AppendixBincludesadditional
quantitativeresults.
InSection3.2,wevalidatedourdesignchoicesfor∆-IRIS
with RL experiments. However, downstream RL perfor-
3.4.Evidenceofdynamicsdisentanglement
manceisanimperfectproxyforthequalityofaworldmodel
duetomanypossibleconfoundingfactors,e.g. thechoice In Section 2.2, we argued that, by design, ∆-IRIS’ en-
oftheRLalgorithm,entangledworldmodelandpolicyar- coder describes stochastic deltas between timesteps with
chitectures,orthecontinuallearningloop. Inthissection, ∆-tokens. Inthepresentsection,weproposetoexhibitthis
wedirectlyfocusontheabilitiesoftheworldmodel. phenomenon.
7EfficientWorldModelswithContext-AwareTokenization
We pick a starting frame and a sequence of actions, and discrepancybetweenthepredicteddistributionsandthedis-
predicttwodifferenttrajectorieswiththeworldmodel. In tributionsofinterestistoencouragefactorizeddistributions
one case, we sample future ∆-tokens randomly. In the (Hafner et al., 2023). On the other hand, autoregressive
other case, ∆-tokens are produced by the autoregressive architectures(Michelietal.,2023)domodelthejointdistri-
transformer.Weconsiderascenariowheretheagentcollects butionanddonotrequiretoenforceindependence,which
woodthenbuildsacraftingtableinFigure3. AppendixF mayresultinamoreexpressivemodel.
displaystwootherscenarioswheretheagentexploresits
surroundings,andwhereitmovesdownthenstandsstill. Trajectoryandvideoautoencoders
Weobservethat,evenwhensampling∆-tokensrandomly, Theideaofencodingframeswithrespecttopastframespre-
thedeterministicaspectsofthedynamicsareproperlymod- datesmoderndeeplearning,andisattheoriginofefficient
elled: gridlayout,agentmovement,woodlevelincreasing, video compression algorithms, such as MPEG (Richard-
craftingtableappearing,etc. Ontheotherhand,stochastic son, 2004). In recent years, multiple works have imple-
dynamicsbecomeproblematic: skeletonsandcowsappear- mentedvariantsofthisapproach. Ozairetal.(2021)pro-
inganddisappearing,foodandwaterindicatorsdecreasing poseanofflineversionofMuZero(Schrittwieseretal.,2020)
too early, unlikely quantities of enemies and objects, etc. equippedwithanautoregressivetransformerthatperforms
Theseobservationsconfirmthat∆-IRISencodesstochastic search over trajectory-level discrete latent variables and
deltasbetweentimestepswith∆-tokens,anditsdecoder actions. Phenaki (Villegas et al., 2022) is a text-to-video
handlesthedeterministicaspectsofworldmodelling. modelcomposedofaspatio-temporaldiscreteautoencoder
andamaskedbidirectionaltransformer. TECO(Yanetal.,
4.RelatedWork 2022)isanaction-conditionalvideopredictionmodelcom-
posedofadiscreteframeautoencoderconditionedonthe
WorldModelsandimagination previousframe,atemporalautoregressivetransformer,and
aspatialMaskGit(Changetal.,2022).Whilethesemethods
WithDyna,Sutton(1991)introducedtheideaoflearning
also encode frames by conditioning on past frames, their
behaviours in the imagination of a world model. Ha &
dynamicsmodelspurelyoperateoverdiscretetokens,and
Schmidhuber(2018)wentbeyondthetabularsettingand
donotleveragecontinuoustokenstoalleviatetheneedto
proposedanewworldmodelarchitecture,composedofa
integrateovermultipletimestepsinordertomakethenext
variationalautoencoder(Kingma&Welling,2013)anda
prediction.
recurrentnetwork(Hochreiter&Schmidhuber,1997;Gers
etal.,2000),capableofsimulatingsimplevisualenviron- Hafneretal.(2019)acknowledgethatmodellingstochastic
ments.Followingthisbreakthrough,multiplegenerationsof dynamicsmaybedifficult,asitwouldinvolveremembering
Dreameragents(Hafneretal.,2020;2021;2023)werede- informationfromprevioustimesteps. Theauthorspropose
veloped,withDreamerV2beingthefirstimagination-based tosolvethisproblembycarryinga“deterministic”stateover
agent to outperform humans in Atari games, and Dream- timeviaarecurrentnetwork,atthecoreoftheirRSSM.We
erV3beingthefirstworldmodelarchitectureapplicabletoa makeasimilarobservation,andfurthershowthatthistaskis
widerangeofdomainswithoutanyspecifictuning. Dream- stilldifficultevenwhenpastinformationdoesnothavetobe
erV2learnsintheimaginationofaworldmodelcombining carriedbyarecurrentstate,asatransformercanattendtoall
a convolutional autoencoder with a recurrent state-space previous∆-tokens. Hence,itisnotonlyamemoryproblem,
model (RSSM) (Hafner et al., 2019). The key modifica- butalsoamodellingone. Here,weaddressthisissueina
tionsthatenabledDreamerV2toimproveovertheoriginal mannerthatiscompatiblewithautoregressivetransformers,
DreameragentwerecategoricallatentsandKLbalancing namelybyinjectingcontinuousI-tokensinthesequenceof
betweenpriorandposteriorestimates. DreamerV3builds thedynamicsmodel.
uponDreamerV2withmoreuniversaldesignchoicessuch
assymlogscalingofrewardsandvalues,combiningfreebits
5.Conclusion
(Kingmaetal.,2016)withKLbalancing,returnscalingfor
staticentropyregularization,andarchitecturalnoveltiesfor We introduced ∆-IRIS, a new model-based agent relying
modelscaling. VariantsofDreamersuchasTransDreamer onanefficientworldmodelarchitecturetosimulateitsen-
(Chenetal.,2022)andSTORM(Zhangetal.,2023)have vironment and learn new behaviours. ∆-IRIS features a
alsobeenexplored,wheretransformersreplacetherecurrent discreteautoencoderthatencodesthestochasticaspectsof
networkintheRSSMfordynamicsprediction. worldmodellingwithdiscrete∆-tokens,andanautoregres-
sivetransformerleveragingcontinuousI-tokenstomodel
ApotentiallimitationofRSSM-likearchitecturesisthatthey
stochasticdynamics.
do not model the joint distribution of future latent states,
andinsteadpredictproductlaws. Onewaytomitigatethis
8EfficientWorldModelswithContext-AwareTokenization
Throughexperiments,weshowedtheabilityofouragent formforgeneralagents. JournalofArtificialIntelligence
toscale tothe challengingCrafterbenchmark, as wellas Research,47:253–279,2013.
itssampleefficiencyinAtari100k. Finally,weillustrated
how its world model internalized environment dynamics, Bengio,Y.,Le´onard,N.,andCourville,A. Estimatingor
and conducted ablations to validate our proposed design propagatinggradientsthroughstochasticneuronsforcon-
choices. ditionalcomputation. arXivpreprintarXiv:1308.3432,
2013.
Initscurrentform,∆-IRISusesthesamenumberoftokens
toencodestochasticdynamicsateachtimestep. However, Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman,
the reality of most environments is such that periods of W.T. Maskgit: Maskedgenerativeimagetransformer. In
lowuncertaintyarequicklyfollowedbymomentsofhigh ProceedingsoftheIEEE/CVFConferenceonComputer
randomness. Therefore,animprovedversionoftheworld VisionandPatternRecognition,pp.11315–11325,2022.
modelcouldpossiblypredictdynamicallyvariousnumbers
of tokens based on the current context. Besides, leverag- Chen,C.,Wu,Y.-F.,Yoon,J.,andAhn,S. Transdreamer:
ingtheinternalrepresentationsoftheworldmodelcould Reinforcementlearningwithtransformerworldmodels.
potentiallyresultinalightweightandmorerobustpolicy. arXivpreprintarXiv:2202.09481,2022.
comma.ai. commavq, 2023. URL https://github.
ImpactStatement
com/commaai/commavq.
The deployment of autonomous agents in real-world ap-
plicationsraisessafetyconcerns. Agentslearningnewbe- D’Oro,P.,Schwarzer,M.,Nikishin,E.,Bacon,P.-L.,Belle-
havioursmayharmindividualsanddamageproperty. With mare, M. G., and Courville, A. Sample-efficient rein-
worldmodels,welowertheamountoftimespentinteracting forcementlearningbybreakingthereplayratiobarrier.
withtherealworldandthusmitigaterisks. Inthiswork,we InTheEleventhInternationalConferenceonLearning
proposeaworldmodelarchitecturethatisamenabletoscal- Representations,2023.
inguptocomplexenvironments,whereaccuratesimulations
Esser,P.,Rombach,R.,andOmmer,B.Tamingtransformers
areevenmorecriticalgiventheusuallyhigherstakes.
forhigh-resolutionimagesynthesis.InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPattern
Acknowledgements
Recognition,pp.12873–12883,2021.
WewouldliketothankAdamJelley,Ba´lintMa´te´,Daniele
Gers,F.A.,Schmidhuber,J.,andCummins,F. Learningto
Paliotta, Maxim Peter, Youssef Saied, Atul Sinha, and
forget: ContinualpredictionwithLSTM. NeuralCompu-
Alessandro Sordoni for insightful discussions and com-
tation,12(10):2451–2471,2000.
ments. Vincent Micheli was supported by the Swiss Na-
tionalScienceFoundationundergrantnumberFNS-187494.
Ha,D.andSchmidhuber,J. Recurrentworldmodelsfacil-
itatepolicyevolution. Advancesinneuralinformation
References processingsystems,31,2018.
Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,
Hafner,D.Benchmarkingthespectrumofagentcapabilities.
Aleman,F.L.,Almeida,D.,Altenschmidt,J.,Altman,S.,
InInternationalConferenceonLearningRepresentations,
Anadkat,S.,etal. Gpt-4technicalreport. arXivpreprint
2022.
arXiv:2303.08774,2023.
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D.,
Agarwal,R.,Schwarzer,M.,Castro,P.S.,Courville,A.C.,
Lee,H.,andDavidson,J. Learninglatentdynamicsfor
andBellemare,M. Deepreinforcementlearningatthe
planning from pixels. In International conference on
edgeofthestatisticalprecipice. Advancesinneuralin-
machinelearning,pp.2555–2565.PMLR,2019.
formationprocessingsystems,34,2021.
Anand,A.,Walker,J.C.,Li,Y.,Ve´rtes,E.,Schrittwieser,J., Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream
Ozair,S.,Weber,T.,andHamrick,J.B. Proceduralgener- tocontrol: Learningbehaviorsbylatentimagination. In
alizationbyplanningwithself-supervisedworldmodels. InternationalConferenceonLearningRepresentations,
InInternationalConferenceonLearningRepresentations, 2020.
2022.
Hafner, D., Lillicrap, T.P., Norouzi, M., andBa, J. Mas-
Bellemare,M.G.,Naddaf,Y.,Veness,J.,andBowling,M. teringatariwithdiscreteworldmodels. InInternational
The arcade learning environment: An evaluation plat- ConferenceonLearningRepresentations,2021.
9EfficientWorldModelswithContext-AwareTokenization
Hafner,D.,Pasukonis,J.,Ba,J.,andLillicrap,T. Mastering Micheli,V.,Alonso,E.,andFleuret,F. Transformersare
diversedomainsthroughworldmodels. arXivpreprint sample-efficientworldmodels. InInternationalConfer-
arXiv:2301.04104v1,2023. enceonLearningRepresentations,2023.
Mnih,V.,Kavukcuoglu,K.,Silver,D.,Rusu,A.A.,Veness,
Hochreiter,S.andSchmidhuber,J.Longshort-termmemory.
J.,Bellemare,M.G.,Graves,A.,Riedmiller,M.,Fidje-
NeuralComputation,9(8):1735–1780,1997.
land, A. K., Ostrovski, G., et al. Human-level control
Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., throughdeepreinforcementlearning. Nature,518(7540):
Kendall, A., Shotton, J., and Corrado, G. Gaia-1: A 529–533,2015.
generativeworldmodelforautonomousdriving. arXiv
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
preprintarXiv:2309.17080,2023.
T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-
chronousmethodsfordeepreinforcementlearning. In
Imani,E.andWhite,M. Improvingregressionperformance
Internationalconferenceonmachinelearning,pp.1928–
withdistributionallosses. InInternationalconferenceon
1937.PMLR,2016.
machinelearning,pp.2157–2166.PMLR,2018.
Ozair,S.,Li,Y.,Razavi,A.,Antonoglou,I.,VanDenOord,
Kaiser,Ł.,Babaeizadeh,M.,Miłos,P.,Osin´ski,B.,Camp-
A.,andVinyals,O. Vectorquantizedmodelsforplanning.
bell,R.H.,Czechowski,K.,Erhan,D.,Finn,C.,Koza-
In International Conference on Machine Learning, pp.
kowski,P.,Levine,S.,etal. Modelbasedreinforcement
8302–8313.PMLR,2021.
learningforatari. InInternationalConferenceonLearn-
ingRepresentations,2020. Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T.
Curiosity-driven exploration by self-supervised predic-
Kanervisto, A., Milani, S., Ramanauskas, K., Topin, N., tion. InInternationalconferenceonmachinelearning,
Lin, Z., Li, J., Shi, J., Ye, D., Fu, Q., Yang, W., Hong, pp.2778–2787.PMLR,2017.
W., Huang, Z., Chen, H., Zeng, G., Lin, Y., Micheli,
Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,and
V., Alonso, E., Fleuret, F., Nikulin, A., Belousov, Y.,
Sutskever,I.Languagemodelsareunsupervisedmultitask
Svidchenko, O., and Shpilman, A. Minerl diamond
learners,2019.
2021competition: Overview,results,andlessonslearned.
InProceedingsoftheNeurIPS2021Competitionsand Razavi,A.,vandenOord,A.,andVinyals,O. Generating
DemonstrationsTrack,ProceedingsofMachineLearning diversehigh-fidelityimageswithvq-vae-2. Advancesin
Research,2022. neuralinformationprocessingsystems,32,2019.
Kapturowski,S.,Ostrovski,G.,Quan,J.,Munos,R.,and Richardson,I.E. H.264andMPEG-4videocompression:
Dabney,W. Recurrentexperiencereplayindistributed videocodingfornext-generationmultimedia. JohnWiley
reinforcementlearning. InInternationalconferenceon &Sons,2004.
learningrepresentations,2019.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer,B. High-resolutionimagesynthesiswithlatent
Kauvar, I., Doyle, C., Zhou, L., and Haber, N. Curious
diffusionmodels. InProceedingsoftheIEEE/CVFCon-
replayformodel-basedadaptation. InInternationalCon-
ferenceonComputerVisionandPatternRecognition,pp.
ferenceonMachineLearning,2023.
10684–10695,2022.
Kingma,D.P.andWelling,M. Auto-encodingvariational
Schrittwieser,J.,Antonoglou,I.,Hubert,T.,Simonyan,K.,
bayes. arXivpreprintarXiv:1312.6114,2013.
Sifre,L.,Schmitt,S.,Guez,A.,Lockhart,E.,Hassabis,
D.,Graepel,T.,Lillicrap,T.P.,andSilver,D. Mastering
Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X.,
atari, go, chess and shogi by planning with a learned
Sutskever,I.,andWelling,M. Improvedvariationalinfer-
model. Nature,588(7839):604–609,2020.
encewithinverseautoregressiveflow.Advancesinneural
informationprocessingsystems,29,2016. Schwarzer,M.,Anand,A.,Goel,R.,Hjelm,R.D.,Courville,
A.,andBachman,P. Data-efficientreinforcementlearn-
LeCun,Y.Apathtowardsautonomousmachineintelligence
ingwithself-predictiverepresentations. InInternational
version0.9.2,2022-06-27. OpenReview,62,2022.
ConferenceonLearningRepresentations,2021.
LeCun,Y.,Boser,B.,Denker,J.S.,Henderson,D.,Howard, Schwarzer, M., Ceron, J.S.O., Courville, A., Bellemare,
R. E., Hubbard, W., and Jackel, L. D. Backpropaga- M. G., Agarwal, R., and Castro, P. S. Bigger, better,
tionappliedtohandwrittenzipcoderecognition. Neural faster: Human-levelatariwithhuman-levelefficiency. In
computation,1(4):541–551,1989. InternationalConferenceonMachineLearning,2023.
10EfficientWorldModelswithContext-AwareTokenization
Sekar,R.,Rybkin,O.,Daniilidis,K.,Abbeel,P.,Hafner,D.,
andPathak,D. Planningtoexploreviaself-supervised
worldmodels. InInternationalConferenceonMachine
Learning,pp.8583–8592.PMLR,2020.
Sutton,R.S. Dyna,anintegratedarchitectureforlearning,
planning,andreacting. ACMSigartBulletin,1991.
Sutton,R.S.andBarto,A.G. ReinforcementLearning: An
Introduction. ABradfordBook,Cambridge,MA,USA,
2018.
Tassa,Y.,Doron,Y.,Muldal,A.,Erez,T.,Li,Y.,Casas,D.
d.L.,Budden,D.,Abdolmaleki,A.,Merel,J.,Lefrancq,
A., et al. Deepmind control suite. arXiv preprint
arXiv:1801.00690,2018.
VanDenOord,A.,Vinyals,O.,etal. Neuraldiscreterep-
resentation learning. Advances in neural information
processingsystems,30,2017.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-
tentionisallyouneed. Advancesinneuralinformation
processingsystems,30,2017.
Villegas,R.,Babaeizadeh,M.,Kindermans,P.-J.,Moraldo,
H.,Zhang,H.,Saffar,M.T.,Castro,S.,Kunze,J.,and
Erhan, D. Phenaki: Variable length video generation
from open domain textual description. arXiv preprint
arXiv:2210.02399,2022.
Yan, W., Hafner, D., James, S., and Abbeel, P. Tempo-
rally consistent video transformer for long-term video
prediction. arXivpreprintarXiv:2210.02396,2022.
Ye,W.,Liu,S.,Kurutach,T.,Abbeel,P.,andGao,Y. Mas-
teringatarigameswithlimiteddata. Advancesinneural
informationprocessingsystems,34,2021.
Yu,J.,Li,X.,Koh,J.Y.,Zhang,H.,Pang,R.,Qin,J.,Ku,
A.,Xu,Y.,Baldridge,J.,andWu,Y. Vector-quantized
image modeling with improved vqgan. arXiv preprint
arXiv:2110.04627,2021.
Zhang, W., Wang, G., Sun, J., Yuan, Y., and Huang, G.
Storm: Efficientstochastictransformerbasedworldmod-
elsforreinforcementlearning. InThirty-seventhConfer-
enceonNeuralInformationProcessingSystems,2023.
11EfficientWorldModelswithContext-AwareTokenization
A.Architecturesandhyperparameters
A.1.Discreteautoencoder
Table2.Encoder/Decoderhyperparameters.Welistthehyperparametersfortheencoder,thesameonesapplyforthedecoder.
Hyperparameter Value
Framedimensions(h,w) 64×64
Layers 5
Residualblocksperlayer 2
Channelsinconvolutionsperlayer [64,64,128,128,256]
Downsamplingafterlayern [1,0,1,1,0]
Pastactionsembeddingchannels 4
Decoderpastframesembedderarch. Sameasencoder
Decoderpastframesembedderoutputfeaturemapsize 8×8×8
Conditioningtimesteps 1
L lossweight 0.1
1
L lossweight 1.0
2
Max-pixellossweight 0.01
Commitmentlossweight 0.02
Table3. Embeddingtableandlatentstatehyperparameters.
Hyperparameter Value
Vocabularysize(N) 1024
Tokensperframe(K) 4
Latentfeaturemapsize 64×8×8
Pre-discretizationtokensize 64×4×4
Tokenembeddingdimension 64
Codebookmovingaveragecoefficient 0.99
Inearlyexperiments,weusedatransformerinsteadofaCNNforthearchitectureoftheautoencoder. Ithadamuchlonger
contextsizeoftwentytimesteps. Althoughthetransformer-basedautoencoderperformedbetterthanitsCNNcounterparton
staticdatasets,weobservedthattheCNNwouldlearnfasterthanthetransformerinthecontinuallearningsetup. Besides,
forthesakeofsimplicity,wedecreasedtheinitialconditioningoftheCNNautoencoderfromfourtimestepstoonetime
step,astheslightincreaseinreconstructionlossesdidnotsignificantlyhinderagentperformance. Theseobservationsare
largelyenvironment-dependent,thusthecontextsizeorthearchitectureoftheautoencodershouldmostlikelybeadapted
accordingly.
A.2.Autoregressivetransformer
Table4. Transformerhyperparameters.
Hyperparameter Value
Timesteps 21
Embeddingdimension 512
Layers 3
Attentionheads 8
Weightdecay 0.01
I-tokenframeembedderarch. Sameasencoderwithhalvedchannelsperlayer
12EfficientWorldModelswithContext-AwareTokenization
A.3.Actor-Critic
Wetietheweightsoftheactorandcritic,exceptforthelastlayer. Theactor-critictakesasinputaframe,andforwardsit
throughaconvolutionalneuralnetwork(LeCunetal.,1989)followedbyanLSTMcell(Hochreiter&Schmidhuber,1997;
Gersetal.,2000;Mnihetal.,2016). FortheCNN,weusethesamearchitectureastheencoder,exceptthatwehalvethe
numberofchannelsperlayer. ThedimensionoftheLSTMhiddenstateis512.
Before starting the imagination procedure (H = 15) from a given frame, we burn-in (Kapturowski et al., 2019) the 5
previousframestoinitializethehiddenstate. Thediscountfactorγ is0.997,theparameterforλ-returnsissetto0.95,and
thecoefficientfortheentropymaximizationtermis0.001. Targetsforvalueestimatesareproducedbyamovingaverageof
thecriticnetwork,withupdateparameter0.995(Mnihetal.,2015)
A.4.Trainingloopandsharedhyperparameters
Table5. Trainingloopandsharedhyperparameters.
Hyperparameter Value Hyperparameter Value
Epochs 1000 Autoencoderbatchsize 32
Environmentstepsfirstepoch 100000 Transformerbatchsize 32
Environmentstepsperepoch 10000 Actor-criticbatchsize 86
#Collectionepochs 990 Learningrate 1e-4
Collectionepsilon-greedy 0.01 Optimizer Adam
Trainingstepsperepoch 500 Maxgradientnorm 10.0
AsmentionedinSection2,theworldmodelandpolicyaretrainedwithtemporalsegmentssampledfrompastexperience.
Weuseacount-basedsamplingprocedureovertheentirehistoryofepisodes,i.e. thelikelihoodthatagivenepisodeis
chosentoproducethenextsampleisinverselyproportionaltothenumberoftimesitwaspreviouslyused. Weraiseinverse
countstothepowerof5tofurtherlimitthebiastowardsolderepisodes.
B.Impactofdesignchoicesonkeyworldmodellingmetrics
Metricsarecomputedonaheld-outtestsetaftertrainingvariousworldmodelsonadatasetconsistingof10Mframes
collectedbya∆-IRISagentthroughoutitstraining.
Table6.Left:Impactofremovingpastframesandactionsfromtheconditioningoftheautoencoder(∆-IRIS→IRIS).Right:Impactof
removingI-tokensfromtheconditioningoftheautoregressivetransformer.
Method L loss
2 Method Nexttokenloss(CE) Rewardloss(CE)
∆-IRIS(4tokens) 0.000185
∆-IRIS 1.57 0.108
IRIS (64tokens) 0.001715
∆-IRISw/oI-tokens 1.73 0.135
IRIS (16tokens) 0.007496
Table7. Impactofdiscardingtheauxiliarymax-pixelloss.
Method L loss Max-pixelloss
2
∆-IRIS 0.000185 0.018
∆-IRISw/omax-pixelloss 0.000178 0.031
13EfficientWorldModelswithContext-AwareTokenization
C.Atari100k
The Atari 100k benchmark (Kaiser et al., 2020) features Atari games (Bellemare et al., 2013) with diverse mechanics.
Thespecificityofthisbenchmarkisthehardconstraintonthenumberofinteractions,namelyonehundredthousandper
environment. ComparedtothestandardAtaribenchmark,thisconstraintresultsinadramaticdropinreal-timeexperience,
from900hoursto2hours.
Regarding baselines, we consider four model-based RL agents learning in imagination: SimPLe (Kaiser et al., 2020),
DreamerV3(Hafneretal.,2023),STORM(Zhangetal.,2023),andIRIS(Michelietal.,2023). Wenotethatthecurrentbest
performingmethodsforAtari100kresorttootherapproaches,suchaslookaheadsearchforEfficientZero(Yeetal.,2021),
orself-supervisedrepresentationlearningwithperiodicresetsforBBF(Schwarzeretal.,2023).
TheusualmetricofinterestistheHNS,thehuman-normalizedscore,basedontheperformanceofhumanplayerswithsimilar
experience. AnegativeHNSindicatesworsethanrandomperformancewhereasanHNSabove1signifiessuperhuman
performance. Weevaluate∆-IRISbycomputinganaverageover100episodescollectedattheendoftrainingforeachgame
(5seeds). Forthebaselines,wereportthepublishedresults.
Table8displaysreturnsacrossgamesandaggregatemetrics(Agarwaletal.,2021). ∆-IRIS achieveshigheraggregate
metricsthanIRIS,whiletrainingin26hours,a5-foldspeedup.
Table8.Returnsonthe26gamesofAtari100kafter2hoursofreal-timeexperience,andhuman-normalizedaggregatemetrics.
Game Random Human SimPLe DreamerV3 STORM IRIS ∆-IRIS(ours)
Alien 228 7128 617 959 984 420 391
Amidar 6 1720 74 139 205 143 64
Assault 222 742 527 706 801 1524 1123
Asterix 210 8503 1128 932 1028 854 2492
BankHeist 14 753 34 649 641 53 1148
BattleZone 2360 37188 4031 12250 13540 13074 11825
Boxing 0 12 8 78 80 70 70
Breakout 2 31 16 31 16 84 302
ChopperCommand 811 7388 979 420 1888 1565 1183
CrazyClimber 10781 35829 62584 97190 66776 59324 57864
DemonAttack 152 1971 208 303 165 2034 533
Freeway 0 30 17 0 34 31 31
Frostbite 65 4335 237 909 1316 259 279
Gopher 258 2413 597 3730 8240 2236 6445
Hero 1027 30826 2657 11161 11044 7037 7049
Jamesbond 29 303 101 445 509 463 309
Kangaroo 52 3035 51 4098 4208 838 2269
Krull 1598 2666 2205 7782 8413 6616 5978
KungFuMaster 259 22736 14863 21420 26182 21760 21534
MsPacman 307 6952 1480 1327 2674 999 1067
Pong -21 15 13 18 11 15 20
PrivateEye 25 69571 35 882 7781 100 103
Qbert 164 13455 1289 3405 4523 746 1444
RoadRunner 12 7845 5641 15565 17564 9615 10414
Seaquest 68 42055 683 618 525 661 827
UpNDown 533 11693 3350 9234 7985 3546 4072
#Superhuman 0 N/A 1 9 10 10 11
Mean 0.00 1.00 0.33 1.10 1.27 1.05 1.39
InterquartileMean 0.00 1.00 0.13 0.50 0.64 0.50 0.65
14EfficientWorldModelswithContext-AwareTokenization
D.Crafterscoresandindividualsuccessrates
Table9. Crafterscores,i.e.geometricmeanofsuccessrates.
Method Crafterscore@1M Crafterscore@5M Crafterscore@10M
∆-IRIS 9.30 39.67 42.47
∆-IRISw/oI-tokens 5.85 14.39 25.92
IRIS(64tokens) 6.66 - -
-IRIS -IRIS w/o I-tokens IRIS (64 tokens)
100
75
50
25
0
C Col ol lle ect c t C Do ia Cal olm lo en ctd CD or li Cln oe lk lct e I cr t o CSn oa llpl ei cn t C g oS lt l Do e ec fn t ee atW o DSo ek fd e el ae tt o Zn om b Ei ae Mt aC kEo ea Itw rP o Ml n aa Mkn Pi e at c kIrk e oa Snx t e MoS anw keo e r MPi Sd atc kok e na ex We MoS aow kdo er Pid Wc ok oa Pdx l e aS cw e or Fud r Pln aa cc ee PlPl aa cn e t S Plt ao cn ee Tabl Wae ke Up
Figure7. Individualsuccessratesaftercollecting1Mframes.
-IRIS -IRIS w/o I-tokens
100
75
50
25
0
C Col ol lle ect c t C Do ia Cal olm lo en ctd CD or li Cln oe lk lct e I cr t o CSn oa llpl ei cn t C g oS lt l Do e ec fn t ee atW o DSo ek fd e el ae tt o Zn om b Ei ae Mt aC kEo ea Itw rP o Ml n aa Mkn Pi e at c kIrk e oa Snx t e MoS anw keo e r MPi Sd atc kok e na ex We MoS aow kdo er Pid Wc ok oa Pdx l e aS cw e or Fud r Pln aa cc ee PlPl aa cn e t S Plt ao cn ee Tabl Wae ke Up
Figure8. Individualsuccessratesaftercollecting10Mframes.
E.Baselines
DreamerV3 results were obtained with commit 8fa35f8. We used the standard configuration for Crafter, and set the
run.train ratiovariablecontrollingtheimagined-to-collecteddataratioto64. NotethatanewversionofDreamerV3was
recentlyreleasedinApril2024. Thisupdateincludesadditionalandbroadlyapplicablenoveltiesforworldmodelandpolicy
learning.
IRISresultswereobtainedwithcommitac6be40. Forthetrainingloopandsharedhyperparameters,wepickedthesame
valuesasinTable5. Weincreasedthedimensionandattentionheadsofthetransformerfrom256and4to512and8,
respectively. Finally,weusedareplaybufferwithacapacityof1Mframes.
15
)%(
etaR
sseccuS
)%(
etaR
sseccuSEfficientWorldModelswithContext-AwareTokenization
F.Evidenceofdynamicsdisentanglement
∆-tokenssampledrandomly
∆-tokenssampledbytheautoregressivetransformer
t=0 t=4 t=5 t=9 t=10 t=12
∆-tokenssampledrandomly
∆-tokenssampledbytheautoregressivetransformer
t=0 t=4 t=5 t=9 t=10 t=12
Figure9. Twoadditionalexamplesofdynamicsdisentanglement,asdiscussedinSection3.4andFigure3.
16