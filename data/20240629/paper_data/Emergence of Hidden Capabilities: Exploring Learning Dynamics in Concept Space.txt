Emergence of Hidden Capabilities:
Exploring Learning Dynamics in Concept Space
CoreFranciscoPark∗1,2,MayaOkawa∗2,3,AndrewLee4
EkdeepSinghLubana†2,4,HidenoriTanaka†2,3
1DepartmentofPhysics,HarvardUniversity,Cambridge,MA,USA
2CenterforBrainScience,HarvardUniversity,Cambridge,MA,USA
3Physics&InformaticsLaboratories,NTTResearch,Inc.,Sunnyvale,CA,USA
4EECSDepartment,UniversityofMichigan,AnnArbor,MI,USA
Abstract
Moderngenerativemodelsdemonstrateimpressivecapabilities,likelystemming
fromanabilitytoidentifyandmanipulateabstractconceptsunderlyingtheirtrain-
ingdata. However,fundamentalquestionsremain: whatdeterminestheconceptsa
modellearns,theorderinwhichitlearnsthem,anditsabilitytomanipulatethose
concepts? Toaddressthesequestions,weproposeanalyzingamodel’slearning
dynamicsviaaframeworkwecalltheconceptspace,whereeachaxisrepresents
anindependentconceptunderlyingthedatageneratingprocess. Bycharacterizing
learningdynamicsinthisspace,weidentifyhowthespeedatwhichaconceptis
learned, and hence the order of concept learning, is controlled by properties of
thedatawetermconceptsignal. Further,weobservemomentsofsuddenturns
in the direction of a model’s learning dynamics in concept space. Surprisingly,
these points precisely correspond to the emergence of hidden capabilities, i.e.,
wherelatentinterventionsshowthemodelpossessesthecapabilitytomanipulate
aconcept,butthesecapabilitiescannotyetbeelicitedvianaiveinputprompting.
While our results focus on synthetically defined toy datasets, we hypothesize a
generalclaimonemergenceofhiddencapabilitiesmayhold: generativemodels
possesslatentcapabilitiesthatemergesuddenlyandconsistentlyduringtraining,
thoughamodelmightnotexhibitthesecapabilitiesundernaiveinputprompting.
1 Introduction
Moderngenerativemodels,suchastext-conditioneddiffusionmodels,showunprecedentedcapabili-
ties[1–8].Theseabilitieshaveledtouseofsuchmodelsinapplicationsasvaluableastrainingcontrol
policiesforrobotics[9–11]andmodelsforweatherforecasting[12],toasdrasticascampaigningin
democraticelections[13,14]. Similarclaimscanbemadeforgenerativemodelsofothermodalities,
e.g.,largelanguagemodels(LLMs)[15–18],speechandaudiomodels[14,19],orevensystems
designedforenablingscientificapplicationssuchasdrugdiscovery[20,21].Arguably,acquiringsuch
generalcapabilitiesrequiresformodelstointernalizethedata-generatingprocessanddisentanglethe
concepts(akalatentvariablesorfactorsofvariation)underlyingit[22,23]. Flexiblymanipulating
theseconceptscanthenenablegenerationofnovelsamplesthatareentirelyout-of-distribution(OOD)
withrespecttotheonesusedfortraining[24–28].
As shown in prior work, modern generative models do exhibit signs of disentangling concepts
underlyingthedatageneratingprocessandlearningofcorrespondingcapabilitiestomanipulatesaid
*Equalcontribution. †Equaladvising. Email: corefranciscopark@g.harvard.edu, {mayaokawa,
ekdeeplubana, hidenori_tanaka}@fas.harvard.edu
Preprint.Underreview.
4202
nuJ
72
]GL.sc[
1v07391.6042:viXra(a) (b) (c)
011
111
010
ro 110
lo
c
d
n u
o
Size
rg
kcaB Obje0 c0
t
1
color
101
000
Model or Optimization Scale 100
Figure1: ConceptLearningGeometryunderliesemergence. (a)Top: Amultimodalmodellearns
to generate the concepts in the order of “astronaut”, “horse”, and finally “riding” as it scales up
(adapted from Yu et al.[45]). Middle: “blue square apple” is generated in the order of “apple”,
“square”,and“blue”(adaptedfromLietal.[46]). Bottom: Despiteitssimplicity,ourmodeltrained
onsyntheticdatashowsconceptlearningdynamicswhereitfirstlearns“shape”andthen“color”.
(b) Concept space is an abstract coordinate space where individual axes correspond to different
conceptsandagivenpointcorrespondstoa“conceptclass”,i.e.,apredefinedcollectionofconcepts
(e.g.,large blue circlesonbottomleftcorner). Traversalalongaxesoftheconceptspaceyield
changeinaspecificpropertyofthesample(e.g.,goingfromlarge blue circletolarge red
circle along object color axis). Trajectories show a model’s dynamics in concept space for
learningtogenerateclassesshownin-distribution(bluenodes)versusoutofdistribution(pinknodes).
As we show, dynamics in concept space are highly interpretable, enabling precise comments on
whichconceptsthemodellearnsfirst,why,andwhatorderitfollows. (c)Measuringhowaccurately
amodelgeneratessamplesfromagivenconceptclass,showinganorderofconceptlearning: first
backgroundcolorislearned,thensize,andthenobjectcolor.
concepts[29–37]. Atthesametime,contemporaryworkhasarguedthatmodels’capabilitiescan
beunreliable,arbitrarilyfailingforagiveninputandsucceedingonanother[38–44]. Thus,critical
questionsremainonhowgenerativemodelsacquiretheircapabilities(seeFig.1): whatdetermines
whetherthemodelwilldisentangleaconceptandlearnthecapabilitytomanipulateit;areallconcepts
and corresponding capabilities learned at the same time; and is there a structure to the order of
conceptlearningsuchthat,giveninsufficienttime,themodellearnscapabilitiestomanipulateonlya
subsetofconceptsbutnotothers?
Thiswork. Toaddressthequestionsabove,weproposetoanalyzeamodel’slearningdynamicsat
thegranularityofconcepts. Sinceperformingsuchananalysisonrealistic,off-the-shelfdatasetscan
bechallenging,wedevelopsynthetictoydatasetsof2Dobjectswithdifferentconcepts(e.g.,shape,
size,color)thatgiveusthoroughcontroloverthedata-generatingprocessandallowforanexhaustive
characterizationofthemodel’slearningdynamics(seeFig.1). Ourcontributionsareasfollows.
• IntroducingConceptSpace. Weproposetoevaluateamodel’slearningdynamicsintheconcept
space—an abstract coordinate system whose axes correspond to specific concepts underlying
thedata-generatingprocess. Weinstantiateanotionofunderspecificationinconceptspaceand
establishitseffectsonamodel’s(in)abilitytodisentangleconcepts.
• ConceptSignalDictatesSpeedofLearning. Wefindthespeedatwhichamodeldisentangles
a concept and learns the capability to manipulate it is dictated by the sensitivity of the data-
generatingprocesstochangesinvaluesofsaidconcept—aquantitywecallconceptsignal. We
showconceptsignalsshapethegeometryofalearningtrajectoryandhencecontroltheoverall
orderofconceptlearning.
• SuddenTransitionsinConceptLearning. Weuseanalyticalcurvestoexplainthephenomenol-
ogyoflearningdynamicsinconceptspace,showingthedynamicscanbedividedintotwobroad
phases: (P1)learningofahiddencapability,wherebyevenifthemodeldoesnotproducethe
desiredoutputforagiveninput,thereexistsystematiclatentinterventionsthatleadthemodelto
generatethedesiredoutput,and(P2)learningtogeneratethedesiredoutputfromtheinputspace.
Overall, while our results focus on a toy synthetic task and text-to-image diffusion models, we
hypothesizeabroaderclaimonhiddenemergenceoflatentcapabilitiesholdstrue: generativemodels
possess latent capabilities that are learned suddenly and consistently during training, but these
capabilitiesarenotimmediatelyapparentsincepromptingthemodelviatheinputspacemaynotelicit
them,hencehidinghow“competent”themodelactuallyis[47,48]. Empirically,signsof“hidden
capabilities”havealreadybeenshowningenerativemodelsatscale[45,49–53],andourresultshelp
provideaformalframeworktopartiallygroundsuchresults.
22 RelatedWork
Conceptlearning. Thetermconceptsasusedinthisworkisbroadlyequivalenttothenotionof
factorsofvariationfrompriorworkondisentangledrepresentationlearning[23,30,54–60],andis
motivatedbyuseoftheterminasimilarsenseincognitivescience[61–66]. Thefocusofliterature
on disentanglement has been to prove identifiability results, e.g., when will a generative model
trainedonadatasetlearntoinvertthedata-generatingprocess,henceidentifyingthelatentvariables,
i.e.,concepts,thatunderlieit. Giventhesuccessofmoderngenerativemodels,weargue,despite
impossibilityresultsfrompriorwork,thesemodelsareinfactlearningtodisentanglesome,ifnot
all,concepts. Preciselywhatguideswhichconceptsaredisentangledhoweverrequiresstudyingthe
learningdynamics—thetargetofourwork.
Interpretability. A growing line of papers demonstrate highly interpretable representations of
intuitiveconceptsexistingenerativemodels,especiallyLLMs[67–72]. Similarworkinthisveinin
imagediffusionmodelshasfoundsemanticrepresentationsinvariouscomponentsofthemodel[37,
52,73–75]: e.g.,existenceoflinearrepresentationsforconceptssuchas3Ddepthorobjectversus
backgrounddistinctions[76,77]. Thesepapersfurtherbolsterourargument,moderngenerative
modelsaretrulyinvertingthedata-generatingprocessandidentifyingtheconceptsunderlyingit.
Competence vs. performance. In cognitive science, a system’s competence on a task is often
contrastedwithitsperformance[47,48,78–81]:competenceisthesystem’spossessionofacapability
(e.g., to converse in a language) and performance is system’s use of that capability in concrete
situations[78]. Forexample,abilingualpersonmaygenerallyconverseintheirprimarylanguageL,
despitepossessingknowledgeofanotherlanguageL′,unlessitiscrucialtousethelatter—clearly,
theyarecompetentinbothlanguages,butgaugingtheirperformanceonL′ requiresappropriately
“prompting”themtouseit. Onecananalogizethisdistinctionwithremarksonaneuralnetwork
possessing a capability versus us being able to elicit it on predefined benchmarks and measure
their performance [82–85]. For example, on the BigBench benchmark [86], LLMs were shown
to have perform poorly on several tasks, but follow up work [87] showed mere chain-of-thought
prompting[49,50]leadstohugeboostsonalltasks. Thisindicatestheevaluatedmodelswereinfact
“competent”,butinappropriatepromptingledtoundermininghow“performant”theyare.
3 ConceptSpace: AFrameworkforAnalyzingConceptLearningDynamics
Wefirstformallyintroduceourframeworkofconceptspaces. Motivatedbyarichbodyofliterature
ondisentangledrepresentationlearning[23,30,54–60],thisframeworkallowsustosystematically
analyzehowdifferentconceptsunderlyingthedatageneratingprocessandcorrespondingcapabilities
tomanipulatethemarelearnedduringtraining. Wehighlightthatsinceourprimaryempiricalfocus
inthispaperwillbeanabstractionoftext-to-imagegenerativemodels,partsoftheframeworkwillbe
specificallyinstantiatedwithtext-to-imagegenerationtasksinmind. Tothisend,wenoteourmodel
classofinterestisagenerativemodelF thatistrainedusingconditioninginformationhtoproduce
imagesy. Forexample,F maybeinstantiatedusingadiffusionmodelthatusesembeddingshof
textualdescriptionsofascenetoproduceimagesy. Wenextdefineaconceptspace.
Definition1. (ConceptSpace.) Consideraninvertibledata-generatingprocessG : Z → X that
samplesvectorsz ∼ P(Z)fromavectorspaceZ ⊂ Rd andmapsthemtotheobservationspace
X ∈Rn. Weassumethesamplingpriorisfactorizable,i.e.,P(z ∈Z)=Πd P(z ),andindividual
i=1 i
dimensionsofZ correspondtosemanticallymeaningfulconcepts. Then,aconceptspaceS isdefined
asthemultidimensionalspacecomposedofallpossibleconceptvectorsz,i.e.,S :={z |z ∼P(Z)}
Asanexample,consideraconceptspacedefinedusingthreeconcepts,say,z =shape,z =size,
1 2
andz =colorthatmapstoadatasetofimageswithobjectsofdifferentcombinationsofshapes,
3
sizes,andcolors(seeFig.2). Theassumptionthatconceptsareindependentlydistributedimplies
onecaninterveneonagivenconceptwithoutaffectingtheotherones. Forexample,givenasample
fromthedatasetabove,theconceptcolorinanimagecanbealteredbychangingthevalueofthe
relevantlatentvariableandmappingittotheimagespaceviathedata-generatingprocess. Now,using
amixingfunctionMthatyieldsconditioninginformationh := M(z),wecantrainaconditional
generativemodelanddefineanotionofcapabilitiesrelevanttoourworkasfollows.
Definition2. (Capability.) AconceptclassC denotesthesetofconceptvectorsz suchthatasubset
C
ofdimensionsofthesevectorsarefixedtopredefinedvalues. ClassesCandC′aresaidtodifferinthe
3kth conceptif∀z ∈ z ,thereexistsz′ ∈ z withz[k] ̸= z′[k]andz[i] = z′[i]fori ̸= k. Wesaya
C C′
modelpossessesthe“capabilitytoalterthekthconcept”ifforanyclassC whosesampleswereseen
duringtraining,themodelcanproducesamplesfromclassC′thatdiffersfromC inthekthconcept.
Intuitively,thedefinitionabovecommentsonwhetherthemodelcanflexiblymanipulateconceptsof
classesseenduringtrainingtoproducesamplesfromclassesthatwerenotseen,i.e.,classesthatare
entirelyout-of-distribution. Asanexample,consideraconceptspacewithshape,color,andsize
asconcepts. Ifshapeand colorarefixedtocircleand bluerespectively, wegetthe classof
blue circles;i.e.,∀z ∈z ,thefirstandseconddimensionrespectivelytakeonvalues
blue circles
thatcorrespondtotheshapecircleandcolorblue. Then,givenaconditionaldiffusionmodelthat
wasshownblue circlesduringtraining,wewillsaythismodelpossessesthecapabilitytoalter
theconceptcolorifitcanproducesamplesfromconceptclasseswiththesameshapeascircles,
but different colors (e.g., red or green circles). Analyzing learning dynamics in the concept
spacewillthusprovideadirectlensintothemodel’scapabilitiesastheyareacquired.
Wealsonotethatthedefinitionaboveisnotdependentontheprecisemannerviawhichthemodelis
promptedtoelicitanoutput,i.e.,itneednotbethecasethattheconditioninginformationhthatisused
fortrainingthemodelisusedtoevaluatethemodelcapability. Infact,inourexperiments,wewill
tryalternativeprotocolssuchasinterveningonthelatentrepresentationstoshowthatsubstantially
beforethemodelcanbepromptedusinghtogeneratesamplesfromanOODconceptclass,itcan
generatesamplesfromsaidclassviasuchlatentinterventions. Tothisend,wealsodefineameasure
thatassesseshowmuchlearningsignalthedataprovidestowardsdisentanglementofaconcept,and
hencelearningofacapabilitytomanipulateit.
Definition3. (ConceptSignal.) Theconceptsignalσ foraconceptz measuresthesensitivityof
i i
thedata-generatingprocesstochangeinthevalueofaconceptvariable,i.e.,σ
i
:=|∂G(z)/∂zi|.
Intuitively,ifthetrainingobjectiveisfactorizedatthegranularityofconcepts,conceptsignalindicates
howmuchthemodelwouldbenefitfromlearningaboutaconcept. Forexample,consideradiffusion
modeltrainedusingtheMSElosswithconditioningh:=ztopredictthenoiseaddedtoanimage
G(z). σ isthenmerelythecomponentofthelossrepresentinghowmuchchangeinconditioning
i
hyieldschangesinconceptz ,asitisrepresentedintheimage. Conceptsignalwillthusserveas
i
acrucialknobinourexperimentstoanalyzelearningdynamicsinconceptspaceandtheorderin
whichconceptsarelearned.
3.1 ExperimentalandEvaluationSetup
Beforeproceedingfurther, wediscussourexperimen- 01 11
talsetupthatconcretelyinstantiatestheformalization 01 11
above. Our data-generating process is motivated by
prior work on disentanglement [54–60] and involves
concept classes defined by three concepts, each with
twovalues:color={red,blue},size={large,small},
andshape={circle,triangle}. InSec.4.1andSec.4.4, Color
we use two attributes: color (with red labeled as 0 Color
00 10
andblueas1)andsize(largelabeledas0andsmall 00 10
as 1). We generate a total of 2048 images for each Figure 2: Concept spaces with differ-
class, with objects randomly positioned within each entconceptvaluesseedifferentconcept
image. We train models on classes 00 (large red signal. Consider a concept space com-
circles),01(large blue circles),and10(small prisedofconceptssizeandcolor. (Left)
red circles),shownasbluenodesinFig.2,andtest Thecolorseparationbetweentheclasses
usingclass11(small blue circles),shownaspink is stronger than the size separation, re-
nodes,toevaluateamodel’sabilitytomanipulatecon- sultinginastrongerconceptsignalinthe
cepts and generalize OOD (see App. A.1 for further colordimension. (right)Thesizesepa-
details). In Sec. 5, we will restrict to two attributes, rationbetweentheclassesisstronger,thus
shape={circle,triangle}andcolor={red,blue},and resultinginastrongersignalforsize.
studytheeffectofnoisyconditioning,i.e.,whathappens
when concepts are correlated in the conditioning information h due to some non-linear mixing
function. Forallexperiments,weuseavariationaldiffusionmodel[88]togenerate3×32×32
imagesconditionedonvectorsh(seeApp.A.2forfurthertrainingdetails).
4
eziS eziSEvaluationMetric. Toassesswhetherageneratedimagematchesthedesiredconceptclasswithout
humanintervention,wefollowliteratureondisentangledrepresentationlearning[23,30,54,55,89–
92] and train classifier probes for individual concepts using the diffusion model’s training data.
The probe architecture involves a U-Net [93] followed by an average pooling layer and n MLP
classificationheadsforthenconceptvariables. SeeApp.A.2forfurtherdetails.
4 LearningDynamicsinConceptSpace
4.1 ConceptSignalDeterminesLearningSpeed
Wefirstdemonstratetheutilityofconceptsignalasa
tooltogaugeatwhatratethemodellearnsaconcept
andthecapabilitytomanipulateit. Tothisend,we
develop controlled variants of our data-generating
process by changing the level of concept signal of
eachconceptandtraindiffusionmodelsconditioned
withthelatentconceptvectorzonthem. Weprimar-
ilyfocusonconceptscolorandsize,alteringtheir Color distance Size distance
conceptsignalbyrespectivelyadjustingtheRGBcon- 0.005 0.01 0.015 0.002 0.003 0.004
Pixel distance Pixel distance
trastbetweenredandblueandthesizedifference
betweenlargeandsmallobjects(seeApp.A.1for Figure3: Conceptsignaldetermineslearn-
details).Wedefinethespeedoflearningeachconcept ing speed. The speed of concept learning
asinverseofthenumberofgradientstepsrequiredto as an inverse of the time in gradient steps
reach80%accuracyforclass11,i.e.,theOODclass when the separation in color (left) and size
that requires learning the capability to manipulate (right) between different classes increases.
conceptsasseenduringtraining. Resultscomparing Conceptlearningisfasterwhenpixeldiffer-
different concept signals are shown in Fig. 3. For ences among concept class and hence con-
bothcolorandsize,weobservethatconceptsig- ceptsarelarger.
naldictatesthespeedatwhichindividualconcepts
arelearned. Wealsofindthatwhendifferentconceptshavevaryingstrengthsofconceptsignals,this
leadstodifferencesinthelearningspeedforeachconcept.
4.2 ConceptSignalGovernsGeneralizationDynamics
Wenextexaminethemodel’slearningdynamicsinconceptspaceundervariouslevelsofconcept
signalfortheconceptscolorandsize.Forcompleteness,weevaluateamodel’sabilitytogeneralize
bothin-distribution(ID)andOOD,butonlythelatterisdeemedlearningofacapabilitytomanipulate
aconcept. ResultsareshowninFig.4,withpanel(a)showingdynamicsforlearningtogenerate
samples from ID class 00 and panel (b) showing dynamics for OOD class 11. Results on OOD
(a) ID generalization for class 10 (b) OOD generalization
01 11 01 11
00 Color 10 00 Color 10
Figure4: Conceptsignalgovernsgeneralizationdynamics. (a)Learningdynamicsintheconcept
spaceforin-distributionconceptclass00(bottomleft). (b)Learningdynamicsforout-of-distribution
(OOD)conceptclass11(topright). Weplottheaccuracyforcoloronthex-axisandsizeonthe
y-axis. The[0,1)normalizedcolorconceptsignalleveliscolorcoded. Twotrajectoriesfor01and
10areshowntoillustrateconceptmemorization.
5
eziS eziS
Color
concept
signalgeneralizationshowaformofconceptmemorization,whichwedefineasthephenomenonwhere
themodel’sgenerationsonanOODconditioningharebiasedtowardsthetrainingclassthathelps
definethestrongestconceptsignal. Forexample,fortheunseenconditioning11,thegenerationsare
morealike01whentheconceptsignalisstrongerforsize(e.g.,bluecurveinFig.4(b))andalike
10whenthesignalisstrongerforcolor(e.g.,redcurve). Interestingly,weobservethatforsettings
withhighimbalanceinconceptsignals,e.g.,thebluecurveinFig.4(b),theendpointofconcept
memorization is very biased towards one training class, here 01, delaying its out-of-distribution
(OOD)generalization. Forthelearningtrajectoriesofallclasses,seeFig.13inApp.C.2.
Broadly,ourresultsimplythatanearlystoppedtext-to-imagemodelcanwitnessconceptmemoriza-
tionandhencesimplyassociateanunseenconditioningtothenearestconceptclasswhenaskedto
generateOODsamples(seeKangetal.[94]forasimilarresultinLLMs). However,givensufficient
time,themodelwilldisentangleconceptsunderlyingthedata-generatingprocessandlearntogenerate
entirelynovel,OODsamples. Forfutherevidenceinthisvein,wealsoconfirmourresultsacross
moregeneralscenarios,includingwiththereal-worldCelebAdataset(App.C.3)andusingthree
conceptvariables: color,background color,andsize(App.C.4).
4.3 TowardsaLandscapeTheoryofLearningDynamics
Fig.4indicatesmodelsundergophasesofun- (a)01 11 (b)01 11
derstanding of concepts at different stages of
training. Infact, anintriguingpropertyoftra-
jectories shown in Fig. 4 (b) is that there is a
suddenturninthelearningdynamicsfromcon-
ceptmemorizationtoOODgeneralization(e.g.,
see the top-left quadrant in Fig. 4 (b)). To in- 00 Color 10 00 Color 10
vestigatethisfurther,weproposeaminimaltoy
Figure5: APhenomenologicalModelofLearn-
model that captures the geometry of model’s
ingDynamicsinConceptSpace. UsingEq.1,
learningtrajectoriesshowninthatfigure.Specif-
we simulate the learning trajectory for concept
ically,weusethefollowingdynamicsequation,
d(t):=z˜+(zˆ−z˜)· 1 ,wherezˆisthe class00inpanel(a)andtheOODclass11inpanel
1+e−(t−tˆ) (b). Initially,targetvaluesaresetat(0,1)or(1,0)
target point wewantto get toand z˜is theini-
based on the concept signal strengths for color
tial,“biased”target. Forexample,considerthe
orsize,respectively. Asthemodelprogressively
casewithcolorandsizeconceptsinFig.4(b).
learns each concept, the target values gradually
Themodel’sgeneratedsamplesaremorealike
shifttowards(1,1). Thissimpletoymodelaccu-
class 01 and biased towards (z˜ ,z˜ ) = (0,1)
1 2 ratelyreproducestheobservedcurvesinFig. 3(c),
whenthesizeconceptsignalσ isstrongerthan
2 whicharisefromconceptmemorization.
σ ;andto10,(z˜ ,z˜ )=(1,0)whenthecolor
1 1 2
conceptsignalσ isstrongerthanσ . Wedefine(zˆ ,zˆ )asthetargetvaluesordirectionswewant
1 2 1 2
thelearningtoheadtowards(e.g.,(1,1)forOODgeneralization). Basedonthisframework,wecan
derivethefollowingenergyfunction.
dz (cid:40) 1 (cid:0) d(t−tˆ)−z (cid:1)2 + a(1−z )2 ifσ >σ ,
=−∇ L, L(z ,z )= 2a 1 1 2 2 1 2 (1)
dt z 1 2 1 (1−z )2+ a(cid:0) d(t−tˆ)−z (cid:1)2 otherwise.
2a 1 2 2 2
Here,aisdeterminedbythedifference|σ −σ |andtˆ andtˆ denotethetimeswhenthemodel
1 2 1 2
learnsconceptsz andz ,respectively. Fig.5illustratesthesimulatedtrajectoriesforclasses00and
1 2
11,basedonEq.1. Panels(a)and(b)correspondtoclasses00and11,respectively. Wedefinethe
actualtargetpoints(zˆ ,zˆ )as(0,0)forclass00and(1,1)forclass11.Fortheinitialtargets(z˜ ,z˜ ),
1 2 1 2
wesetbothvaluesto(0,0)forclass00. Forclass11,thetargetsaresetto(1,0)whenσ >σ and
1 2
to(0,1)whenσ <σ . Wefindourtoymodeleffectivelycapturestheactuallearningdynamicsfor
1 2
bothin-distribution(Fig. 3(b))andout-of-distribution(OOD)conceptclasses(Fig. 3(c)). Notably,
oursimulationaccuratelyreplicatesthetwotypesofcurves: clockwise(bluetrajectoryinFig. 3(b))
andcounterclockwise(redtrajectory).
Animportantconclusionthatfollowsfromtheresultsaboveisthatthenetwork’slearningdynamics
canbepreciselydecomposedintotwostages,henceyieldingthesuddenturnsseenintrajectoriesin
Fig.4. Wehypothesizethereisaphasechangeunderlyingthisdecompositionandthemodelacquires
thecapabilitytoalterconceptsatthispointofphasechange. Weinvestigatethisnext.
6
eziS eziS
a(a) (b) (c)
Figure6:Emergenceofhiddencapabilities.Weplotaccuracyasafunctionofgradientstepsforfive
differentruns,usingthreedifferentprotocolsforpromptingthemodeltogenerateoutputsforOOD
conceptclasses. (a)Thebaseline,naivepromptingprotocol;(b)linearlatentintervention,appliedin
theactivationspace;and(c)overprompting,akintoaninterventionontheinputspace.
4.4 SuddenTransitionsinConceptLearningDynamics
The concept space visualization of learning dynamics observed in Fig. 4 (b) and our toy models
analysisinFig.5indicatethatthereexistsaphaseinwhichthemodeldepartsfromconceptmem-
orization and disentangles each of the concepts, but still produces incorrect images. We claim
thatatthepointofdeparture,themodelhasinfactalreadydisentangledconceptsunderlyingthe
data-generatingprocessandacquiredtherelevantcapabilitiestomanipulatethem,henceyielding
achangeofdirectioninitslearningtrajectory. However, naiveinputpromptingisinsufficientto
elicitthesecapabilitiesandgeneratesamplesfromOODclasses,givingtheimpressionthemodel
isnotyet“competent”. Thisthenleadstothesecondphaseinthelearningdynamics,whereinan
alignmentbetweentheinputspaceandunderlyingconceptrepresentationsislearned. Wetakethe
modelcorrespondingtothesecondfromleftcurve(thegreencurve)inFig.4(b)toinvestigatethis
claimindetail. Specifically,givenintermittentcheckpointsalongthemodel’slearningtrajectory,we
usethefollowingtwoprotocolsforpromptingthemodeltoproduceimagescorrespondingtothe
class11(blue,small). SeeApp.Bforfurtherdetails.
1. ActivationSpace: LinearLatentIntervention. Givenconditioningvectorsh,duringinference
weaddorsubtractcomponentsthatcorrespondtospecificconcepts(e.g.,h ).
blue
2. InputSpace: Overprompting. Wesimplyenhancethecolorconditioningtovaluesofhigher
magnitude,e.g. (0.4,0.4,0.6)to(0.3,0.3,0.7).
Fig.6showstheaccuracyforfiveindependentrunsunder: (a)naiveinputprompting,(b)linearlatent
interventions,and(c)overprompting. InFig.6(a),weobservethatsomerunscanproducesamples
fromthetargetconceptclass(blue,small)with∼100%accuracyafteraround8,000gradientsteps,
whileotherrunsfailtodoso. However,inFig.6(b,c),wefindalternativeprotocolsforprompting
themodelcanconsistentlyelicitthedesiredoutputsmuchearlierthaninputprompting,e.g.,ataround
asearlyas6,000gradientsteps. Thisindicatesthemodeldoespossessthecapabilitytoalterconcepts
andgeneralizeOOD!Furthermore,wenotethatdifferentprotocolsenableelicitationofthecapability
atapproximatelythesamenumberofgradientsteps,irrespectiveoftheseed,andthatthisisprecisely
thepointofsuddenturninthelearningdynamicsinFig.4! Interestingly,experimentswithClassifier
FreeGuidance(CFG)[95]showthatCFGonlybecomeseffectiveafterthistransition(Fig.16).
WefurtherexplorethesecondphaseoflearninginAppendixC.6bypatchingtheembeddingmodule
usedforprocessingtheconditioninginformationfromthefinalcheckpointtoanintermediateone.
Ourresultsshowthatwhenthefinalcheckpointdoesenableuseofnaiveinputpromptingforeliciting
acapability,theembeddingmodulecanbepatchedtoanintermediatecheckpointandwecanretrieve
thedesiredoutputatapproximatelythesametimethatalternativepromptingprotocolsstarttowork
well. This suggests the second phase of learning primarily involves aligning the input space to
intermediaterepresentationsthatenableelicitingthemodelcapabilities. Overall,ourresultsabove
yieldthefollowinghypothesis.
Hypothesis1. (EmergenceofHiddenCapabilities.) Generativemodelspossesshiddencapabilities
thatarelearnedsuddenlyandconsistentlyduringtraining,butnaiveinputpromptingmaynotelicit
thesecapabilities,hencehidinghow“competent”themodelactuallyis.
7Figure 7: Underspecification and Concept Learning. (a) The state-of-the-art generative mod-
els [101] erroneously produces a red strawberry (top right corner) for the prompt “yellow
strawberry”. (b)Withoutunderspecificationinthetrainingdata,amodelF accuratelylearnsthe
conceptsofshapeandcolor,successfullygeneralizestotheunseennodeblue triangle(leftmost).
Asmasksareappliedtothewordredforthepromptred triangle,conceptsignalfortriangle
increasinglystartstocorrelatewiththeconceptred. Thiscausestheoutputimagestochangefrom
bluetopurpleasthelevelofmaskingincreases(panelslefttoright). Eventually,thecolordimension
fortrianglecollapses,biasingthemodeltowardsgeneratingsolelyred triangles(rightmost).
5 EffectofUnderspecificationonLearningDynamicsinConceptSpace
Intheresultsabove,weuseconditioninginformationthatperfectlyspecifiesconceptsunderlyingthe
data-generatingprocess,i.e.,h=z. Inpractice,however,instructionsareunderspecifiedandone
canthusexpectcorrelationsbetweenconceptsintheconditioninginformationextractedfromthose
instructions[96–100]. Forexample, imagesofastrawberryareoftencorrelatedwiththecolor
red(seeFig.7(a)). Correspondingly,unlessatext-to-imagemodelisshownexplicitdatastating
“red strawberry”orimagesofnon-redstrawberries,themodel’sabilitytodisentangletheconcept
colorfromtheconceptstrawberrymaybeimpeded(seegenerationsfor“yellow strawberry”
inFig.7). Motivatedbythis,wenextinvestigatetheeffectsofusingunderspecifiedconditioning
informationonamodel’sabilitytolearnconceptsandcapabilitiestomanipulatethem.
Experimental setup. The data generation and evaluation
processfollowstheprotocoldescribedinSec.3.1. Weselect
color (red and blue) and shape (circle and triangle) as the
concepts, drawingananalogytothe“yellow strawberry”
example. Tosimulateunderspecification,werandomlyselect
training samples that have a specific combination of shape
and color (e.g., “red triangle”). We then mask the token
representing the color (“red”) and train the model on three
concept classes {00, 01, 10}, represented by blue nodes in
Fig.7,withsomepromptsmasked.Wetestusingclass11(pink
node)withnopromptsmasked,toseeifthemodelhaslearned
disentangledconceptsandcangeneralizeOOD. Figure8: Underspecificationde-
lays out-of-distribution (OOD)
UnderspecificationDelaysandHindersOODgeneralization.
generalization. The number of
Fig.8showshowunderspecification(maskedprompts)affects
gradientstepsrequiredtoreachac-
thespeedofconceptlearning. Weseethatasthepercentage
curacy above 0.8, as the percent-
ofmaskedpromptsincreases,thespeedoflearningaconcept
age of masked prompts increases.
decreases,suggestingthatunderspecificationleadstoslower
A higher proportion of masked
learningofconcepts. Further,Fig.9(a)showsmodels’learning
prompts slows down the speed of
dynamics in concept space at varying levels of underspecifi-
conceptlearning.
cation. With0%masking,themodelaccuratelyproducesan
imageofblue triangle(seeFig.7(b)). However, asthepercentageofmaskingincreases, the
colorofgeneratedimagesshiftsfrombluetopurple(middle),andfinallyred. Thisdemonstrates
thatwhenpromptsaremasked,themodel’sunderstandingofshapetrianglebecomesintertwined
withcolorred;evenwhenblueisspecifiedintheprompt,thedynamicsarebiasedtowardsred.
Toy Model of Learning Dynamics with Underspecification. When prompts are masked (i.e.,
underspecification occurs), target values for the concept variables are shifted: e.g., in our setup,
withnomaskapplied,thetargetdirectionsfortheclass11is(1,1). Whentheword“red”in“red
triangle”isfullymasked,thetargetshiftsto(0,1). Assumingthisshiftislinearwithrespectto
8(a)Masked: 100% 80% 60% 50% 40% 20% 0% (b)
01 11 01 11
00 Color 10 00 Color 10
Figure9: Underspecificationhindersout-of-distribution(OOD)generalization. (a)Thelearning
dynamicswithvaryinglevelsofpromptmasking,from0%to100%,andthegeneratedimages. At
0%masking(toprightimage),themodelcorrectlyproducesanimageofblue trianglefromthe
prompt“blue triangle.” Asthemaskingincreases(fromrighttoleft),theimagesgraduallyshift
towardstheincorrectcolor,red.(b)Thesimulationofthelearningdynamicsunderunderspecification
inconceptspacebasedonEq.2. Ourtoymodelreplicatesatrainednetwork’slearningdynamics.
thepercentageofmaskedpromptsα,wecanderivethefollowingenergyfunction.
dz (cid:0) (cid:1)2 (cid:0) (cid:1)2
=−∇ L, L(z ,z )= (1−sα)−z + 1−z . (2)
dt z 1 2 1 2
Intheabove,parametersrepresentstheimpactofunderspecification. Fig. 9(b)showsthesimulation
ofmodelbehaviorintheconceptspaceaccordingtoEq.2(s=0.01). Asthemaskinglevelincreases,
thetargetdirectionsshiftfromz = 1inthetoprightcornertoz = 0inthetopleftcorner. Our
2 2
simulateddynamicsthusmatchwellwiththemodel’slearningdynamicsshowninFig.9(a).
Influence of Underspecification on Emer- (a) (b)
gence of Hidden Capabilities. Following
Sec. 4.4, we also explore whether it is possi-
ble to elicit the desired outputs from a model
trainedwithunderspecifieddata. Fig.10shows
the accuracy results over five runs (a) with-
out using any prompting method, and (b) us-
Figure10: UnderspecificationandHiddenCa-
ingover-prompting. Inbothscenarios,theper-
pabilities. Weusetheover-promptingprotocol
centage of masking is set at 50%. Results
fromSec.4.4toassessifthecapabilitiestoenable
clearly demonstrate that with over-prompting,
OODgeneralizationarelearnedbeforenaiveinput
themodelachieves100%accuracyafterapprox-
prompting. (a)AccuracyforOODgeneralization
imately1,000gradientsteps,whereaswithout
acrossfivedifferentseedrunsundernaiveprompt-
over-prompting,itfailsinthreeoutoffiveruns
ing; (b) Accuracy under over-prompting, with a
evenafter2,000gradientsteps. Thesefindings
fixedmaskingpercentageof50%.
confirmthatcapabilitycandeveloppriortoob-
servablebehavior,evenincasesofunderspecification.
6 Discussion
Whystudytheconceptspace? Onemightaskthequestionofwhytheconceptspacecouldbeuseful
beyondsyntheticdatasets. InFig.12,weshowtheloss,accuracy,andtrainingtrajectoryinconcept
spaceofamodelfromFig.6. Thetimeatwhichthemodelacquiresthecapabilitytomanipulate
size and color independently is not evident from either the loss or accuracy curves. However,
inconceptspace,onecandirectlyseethedivergenceofthetrajectoryfromconceptmemorization.
Benchmarking generative models is a challenging task, still often involving humans in the loop
[102,103]. Ourconceptspaceframeworksuggeststhatbenchmarkingout-of-distribution(OOD)
generalizationcanpotentiallybereducedtomonitoringthelearningtrajectoryinconceptspace.
Concept Learning vs. Grokking. We make a distinction between our delayed elicitation of
capabilitiesversusgrokking[104]. Thecommonaspectisthatthemeasuredperformanceonatest
setisdelayedfromthetrainset,asseeninFig.12. However,wedealwithout-of-distributiontest
9
epahS epahS
Percentage
of
mask
αdata,whichisdifferentfromthesetupslikemodulararithmeticorpolynomialregressioninwhich
grokkingisusuallyexplored[104–108]. Second,workonhiddenprogressmeasuresshowthatin
factamodeliscontinuouslybuildingrepresentationstowardsthemechanismtosolvethespecified
task[107,109];however,ourresultsfindthatevenattheleveloflatentrepresentations,thereisa
suddenemergencewherebythemodellearnsthecapabilitytomanipulateconceptsunderlyingthe
data-generatingprocessandgeneralizeOOD.
Is Concept Learning a Phase Transition? In Sec. 4.4, we have demonstrated that before the
capabilitytomanipulateaconceptislearned,thedesiredoutputscannotbegeneratedirrelevantofthe
protocolusedforpromptingthemodel. Moreover,weshowedthatdifferentprotocolsyielddesired
outputsatpreciselythesametime,andthistimeisalsoindependentofmodelinitialization. Wethus
hypothesizethat: Conceptlearningisawell-controlledphasetransitioninmodelcapability,butthe
abilitytoelicitthiscapabilityviaapredefinedsingleinputpromptingcanbearbitrarilydelayed,e.g.,
bytheleveloftheconceptsignal.
AcknowledgmentsandDisclosureofFunding
CFPandHTgratefullyacknowledgesthesupportofAravinthanD.T.Samuel. CFPthanksZechen
Zhang for useful discussions. ESL’s time at University of Michigan was partially supported by
theNationalScienceFoundation(IIS-2008151)andatCBS,HarvardbyNTTResearch,Inc. The
computationsinthispaperwererunontheFASRCclustersupportedbytheFASDivisionofScience
ResearchComputingGroupatHarvardUniversity.
References
[1] BahjatKawar,ShiranZada,OranLang,OmerTov,HuiwenChang,TaliDekel,InbarMosseri,
andMichalIrani. Imagic: Text-basedrealimageeditingwithdiffusionmodels. arXivpreprint
arXiv:2210.09276,2022.
[2] TimBrooks,BillPeebles,ConnorHolmes,WillDePue,YufeiGuo,LiJing,DavidSchnurr,Joe
Taylor,TroyLuhman,EricLuhman,ClarenceNg,RickyWang,andAdityaRamesh. Video
generation models as world simulators. 2024. URL https://openai.com/research/
video-generation-models-as-world-simulators.
[3] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung,
Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: A large
languagemodelforzero-shotvideogeneration. arXivpreprintarXiv:2312.14125,2023.
[4] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,
DiederikPKingma,BenPoole,MohammadNorouzi,DavidJFleet,etal. Imagenvideo: High
definitionvideogenerationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022.
[5] ChitwanSaharia,JonathanHo,WilliamChan,TimSalimans,DavidJFleet,andMohammad
Norouzi. Image super-resolution via iterative refinement. IEEE Transactions on Pattern
AnalysisandMachineIntelligence,2022.
[6] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall. Dreamfusion: Text-to-3dusing
2ddiffusion. arXivpreprintarXiv:2209.14988,2022.
[7] RuiqiGao,AleksanderHolynski,PhilippHenzler,ArthurBrussee,RicardoMartin-Brualla,
Pratul Srinivasan, Jonathan T Barron, and Ben Poole. Cat3d: Create anything in 3d with
multi-viewdiffusionmodels. arXivpreprintarXiv:2405.10314,2024.
[8] ZiyangChen,DanielGeng,andAndrewOwens. Imagesthatsound: Composingimagesand
soundsonasinglecanvas. arXivpreprintarXiv:2405.12221,2024.
[9] YilunDu,MengjiaoYang,BoDai,HanjunDai,OfirNachum,JoshuaBTenenbaum,Dale
Schuurmans,andPieterAbbeel. Learninguniversalpoliciesviatext-guidedvideogeneration.
arXive-prints,pagesarXiv–2302,2023.
10[10] YilunDu,MengjiaoYang,PeteFlorence,FeiXia,AyzaanWahid,BrianIchter,PierreSermanet,
TianheYu,PieterAbbeel,JoshuaBTenenbaum,etal.Videolanguageplanning.arXivpreprint
arXiv:2310.10625,2023.
[11] JakeBruce,MichaelDennis,AshleyEdwards,JackParker-Holder,YugeShi,EdwardHughes,
MatthewLai,AditiMavalankar,RichieSteigerwald,ChrisApps,etal. Genie: Generative
interactiveenvironments. arXivpreprintarXiv:2402.15391,2024.
[12] SamarKhanna,PatrickLiu,LinqiZhou,ChenlinMeng,RobinRombach,MarshallBurke,
DavidLobell,andStefanoErmon. Diffusionsat: Agenerativefoundationmodelforsatellite
imagery. arXivpreprintarXiv:2312.03606,2023.
[13] Yan Zhuang (New York Times). Imran Khan’s ‘Victory Speech’ From Jail Shows
A.I.’sPerilandPromise, 2024. https://www.nytimes.com/2024/02/11/world/asia/
imran-khan-artificial-intelligence-pakistan.html.
[14] Mark Sullivan (FastCompany). AI deepfakes get very real as 2024 elec-
tion season begins, 2024. https://www.fastcompany.com/91020077/
ai-deepfakes-taylor-swift-joe-biden-2024-election.
[15] SébastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,Ece
Kamar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal. Sparksofartificialgeneral
intelligence: Earlyexperimentswithgpt-4. arXivpreprintarXiv:2303.12712,2023.
[16] GeminiTeam. Gemini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensof
context. Technicalreport,GoogleDeepMind,2024. https://storage.googleapis.com/
deepmind-media/gemini/gemini_v1_5_report.pdf.
[17] OpenAI. GPT-4SystemCard. Technicalreport,OpenAI,2023. https://cdn.openai.com/
papers/gpt-4-system-card.pdf.
[18] Claudeteam. IntroducingthenextgenerationofClaude. Technicalreport,AnthropicAI,2024.
https://www.anthropic.com/news/claude-3-family.
[19] Generative Media Team (Google Deepmind). Generating audio for video, 2024. https:
//deepmind.google/discover/blog/generating-audio-for-video/.
[20] ShengchaoLiu,YanjingLi,ZhuoxinranLi,AnthonyGitter,YutaoZhu,JiaruiLu,ZhaoXu,
WeiliNie,ArvindRamanathan,ChaoweiXiao,etal. Atext-guidedproteindesignframework.
arXivpreprintarXiv:2302.04611,2023.
[21] MIT News. Speeding up drug discovery with diffusion
generative models, 2024. https://news.mit.edu/2023/
speeding-drug-discovery-with-diffusion-generative-models-diffdock-0331.
[22] YoshuaBengio,AaronCourville,andPascalVincent. Representationlearning: Areviewand
new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):
1798–1828,2013.
[23] FrancescoLocatello,StefanBauer,MarioLucic,GunnarRaetsch,SylvainGelly,Bernhard
Schölkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised
learningofdisentangledrepresentations. InProc.int.conf.onmachinelearning(ICML),2019.
[24] JivatNeetKaur,EmreKiciman,andAmitSharma. Modelingthedata-generatingprocessis
necessaryforout-of-distributiongeneralization. arXivpreprint.arXiv:2206.07837,2022.
[25] LiweiJiang,JenaDHwang,ChandraBhagavatula,RonanLeBras,JennyLiang,JesseDodge,
KeisukeSakaguchi,MaxwellForbes,JonBorchardt,SaadiaGabriel,etal. Canmachineslearn
morality? thedelphiexperiment. arXive-prints,pagesarXiv–2110,2021.
[26] BernhardSchölkopf,FrancescoLocatello,StefanBauer,NanRosemaryKe,NalKalchbrenner,
AnirudhGoyal,andYoshuaBengio. Towardscausalrepresentationlearning. arXivpreprint.
arXiv:2102.11107,2021.
11[27] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference:
foundationsandlearningalgorithms. TheMITPress,2017.
[28] JeanKaddour, AengusLynch, QiLiu, MattJKusner, andRicardoSilva. Causalmachine
learning: Asurveyandopenproblems. arXivpreprintarXiv:2206.15475,2022.
[29] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen. Hierarchical
text-conditionalimagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,2022.
[30] MayaOkawa,EkdeepSinghLubana,RobertP.Dick,andHidenoriTanaka. Compositional
abilitiesemergemultiplicatively: Exploringdiffusionmodelsonasynthetictask,2024.
[31] HuYu,HaoLuo,FanWang,andFengZhao. Uncoveringthetextembeddingintext-to-image
diffusionmodels. arXivpreprintarXiv:2404.01154,2024.
[32] Lingjing Kong, Guangyi Chen, Biwei Huang, Eric P Xing, Yuejie Chi, and Kun Zhang.
Learningdiscreteconceptsinlatenthierarchicalmodels. arXivpreprintarXiv:2406.00519,
2024.
[33] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin,
YangZhang,andShiyuChang. Uncoveringthedisentanglementcapabilityintext-to-image
diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,pages1900–1910,2023.
[34] Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, and David Bau.
Concept sliders: Lora adaptors for precise control in diffusion models. arXiv preprint
arXiv:2311.12092,2023.
[35] NanLiu,YilunDu,ShuangLi,JoshuaBTenenbaum,andAntonioTorralba. Unsupervised
compositionalconceptsdiscoverywithtext-to-imagegenerativemodels. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pages2085–2095,2023.
[36] Michel Besserve, Arash Mehrjou, Rémy Sun, and Bernhard Schölkopf. Counterfactuals
uncoverthemodularstructureofdeepgenerativemodels. arXivpreprint.arXiv:1812.03253,
2018.
[37] ZihaoWang,LinGui,JeffreyNegrea,andVictorVeitch. Conceptalgebrafor(score-based)
text-controlledgenerativemodels. AdvancesinNeuralInformationProcessingSystems,36,
2024.
[38] Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip HS Torr, Adel
Bibi, Samuel Albanie, and Matthias Bethge. No" zero-shot" without exponential data:
Pretrainingconceptfrequencydeterminesmultimodalmodelperformance. arXivpreprint
arXiv:2404.04125,2024.
[39] Colin Conwell and Tomer Ullman. Testing relational understanding in text-guided image
generation. arXivpreprintarXiv:2208.00005,2022.
[40] ColinConwellandTomerUllman. Acomprehensivebenchmarkofhuman-likerelational
reasoningfortext-to-imagefoundationmodels. InICLR2023WorkshoponMathematicaland
EmpiricalUnderstandingofFoundationModels,2023.
[41] EvelinaLeivada,ElliotMurphy,andGaryMarcus. Dall-e2failstoreliablycapturecommon
syntacticprocesses. arXivpreprintarXiv:2210.12889,2022.
[42] TejasGokhale,HamidPalangi,BesmiraNushi,VibhavVineet,EricHorvitz,EceKamar,Chitta
Baral, and Yezhou Yang. Benchmarking spatial relationships in text-to-image generation.
arXivpreprintarXiv:2212.10015,2022.
[43] GautamSingh,FeiDeng,andSungjinAhn. Illiteratedall-elearnstocompose. arXivpreprint
arXiv:2110.11405,2021.
[44] RoyiRassin,ShauliRavfogel,andYoavGoldberg. Dalle-2isseeingdouble: flawsinword-to-
conceptmappingintext2imagemodels. arXivpreprintarXiv:2210.10606,2022.
12[45] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay
Vasudevan,AlexanderKu,YinfeiYang,BurcuKaragolAyan,etal. Scalingautoregressive
modelsforcontent-richtext-to-imagegeneration. arXivpreprintarXiv:2206.10789,2022.
[46] Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, R Manmatha, Ashwin
Swaminathan,ZhuowenTu,StefanoErmon,andStefanoSoatto.Onthescalabilityofdiffusion-
basedtext-to-imagegeneration. arXivpreprintarXiv:2404.02883,2024.
[47] RaphaëlMillièreandCameronBuckner. Aphilosophicalintroductiontolanguagemodels–
parti: Continuitywithclassicdebates,2024.
[48] RaphaëlMillièreandCameronBuckner. Aphilosophicalintroductiontolanguagemodels-part
ii: Thewayforward. arXivpreprintarXiv:2405.03207,2024.
[49] TakeshiKojima,Shixiang(Shane)Gu,MachelReid,YutakaMatsuo,andYusukeIwasawa.
Largelanguagemodelsarezero-shotreasoners. InAdvancesinNeuralInformationProcessing
Systems,volume35,2022.
[50] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,EdChi,QuocLe,andDenny
Zhou. Chainofthoughtpromptingelicitsreasoninginlargelanguagemodels. arXivpreprint
arXiv:2201.11903,2022.
[51] EvanHubinger,CarsonDenison,JesseMu,MikeLambert,MegTong,MonteMacDiarmid,
Tamera Lanham, Daniel M Ziegler, Tim Maxwell, Newton Cheng, et al. Sleeper agents:
Trainingdeceptivellmsthatpersistthroughsafetytraining. arXivpreprintarXiv:2401.05566,
2024.
[52] MingiKwon,JaeseokJeong,andYoungjungUh. Diffusionmodelsalreadyhaveasemantic
latentspace. InTheEleventhInternationalConferenceonLearningRepresentations,2023.
URLhttps://openreview.net/forum?id=pd1P2eUBVfq.
[53] SébastienBubeck. Sparksofartificialgeneralintelligence: EarlyexperimentswithGPT-4
(Talk),2023. URLhttps://www.youtube.com/watch?v=qbIk7-JPB2c.
[54] HyunjikKimandAndriyMnih. Disentanglingbyfactorising. InInternationalConferenceon
MachineLearning,pages2649–2658.PMLR,2018.
[55] SjoerdVanSteenkiste,FrancescoLocatello,JürgenSchmidhuber,andOlivierBachem. Are
disentangledrepresentationshelpfulforabstractvisualreasoning? Adv.inNeuralInformation
ProcessingSystems(NeurIPS),2019.
[56] AapoHyvarinenandHiroshiMorioka. NonlinearICAoftemporallydependentstationary
sources. InProc.Int.Conf.onArtificialIntelligenceandStatistics(AISTATS),2017.
[57] AapoHyvarinenandHiroshiMorioka. Unsupervisedfeatureextractionbytime-contrastive
learningandnonlinearica. Adv.inNeuralInformationProcessingSystems(NeurIPS),2016.
[58] AapoHyvarinen,HiroakiSasaki,andRichardTurner. NonlinearICAusingauxiliaryvariables
and generalized contrastive learning. In The 22nd Int. Conf. on Artificial Intelligence and
Statistics(AISTATS),2019.
[59] JuliusVonKügelgen,YashSharma,LuigiGresele,WielandBrendel,BernhardSchölkopf,
MichelBesserve,andFrancescoLocatello. Self-supervisedlearningwithdataaugmentations
provablyisolatescontentfromstyle.Adv.inNeuralInformationProcessingSystems(NeurIPS),
2021.
[60] Luigi Gresele, Julius Von Kügelgen, Vincent Stimper, Bernhard Schölkopf, and Michel
Besserve. Independent mechanism analysis, a new concept? Adv. in Neural Information
ProcessingSystems(NeurIPS),2021.
[61] SusanCarey. Theoriginofconcepts. JournalofCognitionandDevelopment,1(1):37–41,
2000.
[62] SusanCarey. Knowledgeacquisition: Enrichmentorconceptualchange. Theepigenesisof
mind: Essaysonbiologyandcognition,pages257–291,1991.
13[63] SusanCarey. Theoriginandevolutionofeverydayconcepts. Cognitivemodelsofscience,15:
89–128,1992.
[64] Susan Carey and Elizabeth Spelke. Domain-specific knowledge and conceptual change.
Mappingthemind: Domainspecificityincognitionandculture,169:200,1994.
[65] SusanCarey. Bootstrapping&theoriginofconcepts. Daedalus,133(1):59–68,2004.
[66] SusanCarey. Whereournumberconceptscomefrom. TheJournalofphilosophy,106(4):220,
2009.
[67] SamuelMarksandMaxTegmark. Thegeometryoftruth: Emergentlinearstructureinlarge
languagemodelrepresentationsoftrue/falsedatasets. arXivpreprintarXiv:2310.06824,2023.
[68] SamuelMarks,CanRager,EricJMichaud,YonatanBelinkov,DavidBau,andAaronMueller.
Sparsefeaturecircuits:Discoveringandeditinginterpretablecausalgraphsinlanguagemodels.
arXivpreprintarXiv:2403.19647,2024.
[69] WesGurnee,NeelNanda,MatthewPauly,KatherineHarvey,DmitriiTroitskii,andDimitris
Bertsimas. Findingneuronsinahaystack: Casestudieswithsparseprobing. arXivpreprint
arXiv:2305.01610,2023.
[70] WesGurneeandMaxTegmark. Languagemodelsrepresentspaceandtime. arXivpreprint
arXiv:2310.02207,2023.
[71] Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, and
NeelNanda. Refusalinlanguagemodelsismediatedbyasingledirection. arXivpreprint
arXiv:2406.11717,2024.
[72] DannyHalawi,Jean-StanislasDenain,andJacobSteinhardt. Overthinkingthetruth: Under-
standinghowlanguagemodelsprocessfalsedemonstrations. arXivpreprintarXiv:2307.09476,
2023.
[73] RenéHaas,InbarHuberman-Spiegelglas,RotemMulayoff,andTomerMichaeli. Discovering
interpretable directions in the semantic latent space of diffusion models. arXiv preprint
arXiv:2303.11073,2023.
[74] Yong-HyunPark,MingiKwon,JunghyoJo,andYoungjungUh. Unsuperviseddiscoveryof
semanticlatentdirectionsindiffusionmodels. arXivpreprintarXiv:2302.12469,2023.
[75] SamyadeepBasu,KeivanRezaei,RyanRossi,CherryZhao,VladMorariu,VarunManjunatha,
andSoheilFeizi. Onmechanisticknowledgelocalizationintext-to-imagegenerativemodels.
arXivpreprintarXiv:2405.01008,2024.
[76] Yida Chen, Fernanda Viégas, and Martin Wattenberg. Beyond surface statistics: Scene
representationsinalatentdiffusionmodel. arXivpreprintarXiv:2306.05720,2023.
[77] MohamedElBanani,AmitRaj,Kevis-KokitsiManinis,AbhishekKar,YuanzhenLi,Michael
Rubinstein,DeqingSun,LeonidasGuibas,JustinJohnson,andVarunJampani. Probingthe
3dawarenessofvisualfoundationmodels. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages21795–21806,2024.
[78] NoamChomsky. AspectsoftheTheoryofSyntax. Number11.MITpress,2014.
[79] JohnCollins. Linguisticcompetencewithoutknowledgeoflanguage. PhilosophyCompass,2
(6):880–895,2007.
[80] JillGDeVilliersandPeterADeVilliers. Competenceandperformanceinchildlanguage:
Arechildrenreallycompetenttojudge? JournalofChildLanguage,1(1):11–22,1974.
[81] GillianBrown,KirstenMalmkjær,andJohnWilliams. Performanceandcompetenceinsecond
languageacquisition. Cambridgeuniversitypress,1996.
[82] METR. Guidelines for Capabilities Elicitation, 2024. https://metr.github.io/
autonomy-evals-guide/elicitation-protocol/.
14[83] RyanGreenblatt,FabienRoger,DmitriiKrasheninnikov,andDavidKrueger. Stress-testing
capabilityelicitationwithpassword-lockedmodels. arXivpreprintarXiv:2405.19550,2024.
[84] TomDavidson,Jean-StanislasDenain,PabloVillalobos,andGuillemBas. Aicapabilities
canbesignificantlyimprovedwithoutexpensiveretraining. arXivpreprintarXiv:2312.07413,
2023.
[85] StephenCasper,CarsonEzell,CharlotteSiegmann,NoamKolt,TaylorLynnCurtis,Benjamin
Bucknall, Andreas Haupt, Kevin Wei, Jérémy Scheurer, Marius Hobbhahn, et al. Black-
boxaccessisinsufficientforrigorousaiaudits. InThe2024ACMConferenceonFairness,
Accountability,andTransparency,pages2254–2272,2024.
[86] AarohiSrivastava,AbhinavRastogi,AbhishekRao,AbuAwalMdShoeb,AbubakarAbid,
Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.
Beyondtheimitationgame: Quantifyingandextrapolatingthecapabilitiesoflanguagemodels.
arXivpreprintarXiv:2206.04615,2022.
[87] MiracSuzgun,NathanScales,NathanaelSchärli,SebastianGehrmann,YiTay,HyungWon
Chung,AakankshaChowdhery,QuocVLe,EdHChi,DennyZhou,etal. Challengingbig-
benchtasksandwhetherchain-of-thoughtcansolvethem. arXivpreprintarXiv:2210.09261,
2022.
[88] DiederikP.Kingma,TimSalimans,BenPoole,andJonathanHo. Variationaldiffusionmodels,
2023.
[89] IrinaHiggins,LoicMatthey,ArkaPal,ChristopherBurgess,XavierGlorot,MatthewBotvinick,
ShakirMohamed,andAlexanderLerchner. beta-vae: Learningbasicvisualconceptswitha
constrainedvariationalframework. InProc.Int.Conf.onLearningRepresentations(ICLR),
2017.
[90] CianEastwoodandChristopherKIWilliams. Aframeworkforthequantitativeevaluationof
disentangledrepresentations. InInternationalConferenceonLearningRepresentations,2018.
[91] RickyTQChen, XuechenLi, RogerBGrosse, andDavidKDuvenaud. Isolatingsources
ofdisentanglementinvariationalautoencoders. Advancesinneuralinformationprocessing
systems,31,2018.
[92] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of
disentangledlatentconceptsfromunlabeledobservations. arXivpreprintarXiv:1711.00848,
2017.
[93] OlafRonneberger, PhilippFischer, andThomasBrox. U-net: Convolutionalnetworksfor
biomedicalimagesegmentation,2015.
[94] KatieKang,EricWallace,ClaireTomlin,AviralKumar,andSergeyLevine. Unfamiliarfine-
tuningexamplescontrolhowlanguagemodelshallucinate. arXivpreprintarXiv:2403.05612,
2024.
[95] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598,2022.
[96] BenHutchinson,JasonBaldridge,andVinodkumarPrabhakaran. Underspecificationinscene
description-to-depictiontasks,2022.
[97] AlexanderD’Amour,KatherineHeller,DanMoldovan,BenAdlam,BabakAlipanahi,Alex
Beutel,ChristinaChen,JonathanDeaton,JacobEisenstein,MatthewDHoffman,etal. Un-
derspecificationpresentschallengesforcredibilityinmodernmachinelearning. Journalof
MachineLearningResearch,23(226):1–61,2022.
[98] SandroPezzelle. Dealingwithsemanticunderspecificationinmultimodalnlp. InProceedings
ofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: Long
Papers).
15[99] YingtianTang,YutaroYamada,YoyoZhang,andIlkerYildirim. Whenarelemonspurple? the
conceptassociationbiasofvision-languagemodels. InProceedingsofthe2023Conference
onEmpiricalMethodsinNaturalLanguageProcessing,2023.
[100] RoyiRassin,EranHirsch,DanielGlickman,ShauliRavfogel,YoavGoldberg,andGalChechik.
Linguisticbindingindiffusionmodels: Enhancingattributecorrespondencethroughattention
map alignment. In Thirty-seventh Conference on Neural Information Processing Systems,
2023. URLhttps://openreview.net/forum?id=AOKU4nRw1W.
[101] JunsongChen,JinchengYu,ChongjianGe,LeweiYao,EnzeXie,YueWu,ZhongdaoWang,
JamesKwok,PingLuo,HuchuanLu,andZhenguoLi. Pixart-α: Fasttrainingofdiffusion
transformerforphotorealistictext-to-imagesynthesis,2023.
[102] Sharon Zhou, Mitchell L. Gordon, Ranjay Krishna, Austin Narcomey, Li Fei-Fei, and
Michael S. Bernstein. Hype: A benchmark for human eye perceptual evaluation of gen-
erativemodels,2019.
[103] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyDenton,Seyed
KamyarSeyedGhasemipour,BurcuKaragolAyan,S.SaraMahdavi,RaphaGontijoLopes,
TimSalimans,JonathanHo,DavidJFleet,andMohammadNorouzi. Photorealistictext-to-
imagediffusionmodelswithdeeplanguageunderstanding,2022.
[104] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
Grokking: Generalizationbeyondoverfittingonsmallalgorithmicdatasets. arXivpreprint
arXiv:2201.02177,2022.
[105] ZimingLiu,OuailKitouni,NiklasSNolte,EricMichaud,MaxTegmark,andMikeWilliams.
Towardsunderstandinggrokking: Aneffectivetheoryofrepresentationlearning. Advancesin
NeuralInformationProcessingSystems,35:34651–34663,2022.
[106] ZimingLiu,EricJMichaud,andMaxTegmark. Omnigrok: Grokkingbeyondalgorithmic
data. InTheEleventhInternationalConferenceonLearningRepresentations,2022.
[107] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress
measuresforgrokkingviamechanisticinterpretability,2023.
[108] TanishqKumar,BlakeBordelon,SamuelJGershman,andCengizPehlevan. Grokkingasthe
transitionfromlazytorichtrainingdynamics. arXivpreprintarXiv:2310.06110,2023.
[109] BoazBarak,BenjaminEdelman,SurbhiGoel,ShamKakade,EranMalach,andCyrilZhang.
Hiddenprogressindeeplearning: Sgdlearnsparitiesnearthecomputationallimit. Advances
inNeuralInformationProcessingSystems,35:21750–21764,2022.
[110] CoreFranciscoPark,VictoriaOno,NayantaraMudur,YueyingNi,andCarolinaCuesta-Lazaro.
Probabilisticreconstructionofdarkmatterfieldsfrombiasedtracersusingdiffusionmodels,
2023.
[111] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage
recognition,2015.
[112] JimmyLeiBa,JamieRyanKiros,andGeoffreyE.Hinton. Layernormalization,2016.
[113] DanHendrycksandKevinGimpel. Gaussianerrorlinearunits(gelus),2023.
[114] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,
LukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed,2023.
[115] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization,2019.
[116] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,
TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,AlbanDesmaison,Andreas
Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performancedeeplearninglibrary,2019.
16[117] DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,
2015.
[118] ZiweiLiu,PingLuo,XiaogangWang,andXiaoouTang. Deeplearningfaceattributesinthe
wild. In Proceedingsof International Conference on ComputerVision(ICCV), December
2015.
17A ExperimentalDetails
A.1 Syntheticdata
We design a data generation process (DGP) compatible with the concept space framework intro-
ducedinSec.3. OurfullDGPhassixconcepts: shape={circle,triangle},x coordinate∈ R,y
coordinate∈R,color={red,blue},size={big,small},andbackground color={bright,dark}.
Wegenerallyexploreonlyasubsetoftheseatagiventimeinourexperiments.
In Sec. 4, we fix shape=circle and x coordinate, y coordinate, background color
are masked out. Thus the conditioning vector h only specifies color={red,blue} and
size={big,small}.Wesamplebothoftheseconceptvariablesfromamixtureoftwouniformdistri-
butions,onecomponentforeachclass(big,small)and(red,blue). Eachvalueineachdimension
issampledfromU(mi−s,mi+s),wheremiistheclassdependentmean.Forexample,color=red
could be sampled from U(0.7,0.9)×U(0.1,0.3)×U(0.1,0.3), where m0 = (0.8,0.2,0.2) For
brevity,wenamethefourresultingclasses“00”,“01”,“10”,and“11”,wheretheclass“11”iskept
astheunseentargettoevaluateout-of-distribution(OOD)generalization. Foreachtrainingrun,the
DGPwasinitializedwithasetrandomseedand2048imagesweregeneratedineachclass.
InSec.5,weusedtwoconceptvariables,shapeandsize. Thetrainingdatasetincludedtheclasses
“00”,“01”,and“10”. Forevaluation,weusedtheclass“11”. Foreachclass,wegeneratedatotalof
1,000images,eachfeaturingobjectsofvaryingpositionsandsizestoensurevariabilityinourdataset.
In App. C.4, we add the concept variable background color∈ R3 to have a three dimensional
conceptspaceof(color,size,background color). Inthiscaseourtrainingdataiscomposedof
theclasses(“000”,“001”,“010”,“100”)andthetestclassesare(“011”,“101”,“110”,“111”)
Our DGP is resolution agnostic, but we work with
squareimagesof32x32forfastexperiments. 01 11
01 11
Adjusting concept signal To adjust concept sig-
nals, we vary the class dependent mean of the two
components of the mixture distributions. For ex-
ample a strong color concept signal is achieved
by drawing color=red from the mean mred =
(0.9,0.1,0.1) and color=blue from the mean Color
mblue = (0.1,0.1,0.9) whereas a weak color con- Color
00 10
ceptsignalcanbedrawnusingmred =(0.6,0.4,0.4) 00 10
andmblue = (0.4,0.4,0.6). Wescalethestandard Figure 11: Different distributions in con-
deviationofeachcomponentbythesameratiothe ceptvaluesresultindifferentconceptsig-
differencebetweentheclassmeanshasbeenscaled. nal. (Left) The color separation between
Thislatterisimportanttoavoidthemodel’snatural theclassesisstrongerthanthesizesepara-
interpolationorextrapolationabilitiesfromconfound- tionsresultinginastrongerconceptsignalin
ingwiththegeneralizationdynamics. thecolordimension. (right)Thesizesep-
arationbetweentheclassesisstronger,thus
Fig.11illustratestwodatasetswithvaryingconcept
resultinginastrongerconceptsignalinsize
signals. Fig.11(a)illustratesacasewherethesize
concept signal is stronger than the color concept
signal,whileFig.11(b)illustratestheinversecasewherethecolorconceptsignalisstronger.
A.2 ModelDetails
ModelArchitectureWeusedanetworkarchitecturesimilartotheU-Net[93]usedinVariational
DiffusionModels[88]. Weusedtheimplementationpubliclyavailablein[110]. Thearchitecture
has 2 ResNet [111] blocks before each downsampling layer and a has self-attention layer in the
bottlenecklayerofthenetwork. TheU-Nethas64,128,256channelsateachresolutionlevels,anhas
aLayerNorm[112]andaGELU[113]activation. Weusedasinusoidaltimestepembedding[114]
of64dimensionsfollowedbya2-layerMLPwithhiddendimension256andaconditioningvector
embeddingusinga2-layerMLPwithhiddendimensions256independentineveryresidualblock.
18
eziS eziSOptimizerWeusetheAdamW[115]optimizerwithlearningrate0.001andweightdecay0.01to
optimizetheparametersofournetwork. Wetrainournetworksfor20Kgradientsteps. Weusethe
defaultvaluesforthedecayrates: β =0.9, β =0.999.
1 2
ClassifierFreeGuidanceAttrainingtimewedroptheconditioningtoa−1.0fillednullvectorϕ
withprobability0.2. Thischoicewasmadeasopposedtotheconventionalchoiceofdroppingitto
thenullvectorsincethenullvectorwasneartheinterpolationlimitofourmodelinthecolorsubspace
oftheconditioning. Thew parameterisusedtoestimatethenoiseinthediffusionprocessby:
cfg
ϵˆ =f (x |ϕ)+w ∗(f (x |h)−f (x |ϕ)),whereθdenotesparametersofthenetworkf.
t−1 θ t cfg θ t θ t
DiffusionProcessHyperparametersWeusedthecontinuoustimevariationaldiffusionmodel[88]
framework to (i) keep the sampling step parameter T an inference time hyperparameter and (ii)
toallowthemodeltotheadjustitsoptimalnoiseschedulefortheseimages, especiallysinceour
syntheticimagesareexpectedtobedifferentinSNRfromnaturalimages. Inparticular,weinitialize
ournetworkwithγ =−5.0andγ =10.0(see[88]forthedefinitionofγ)andusealearnedlinear
i f
scheduleofγ(t). Weuseareconstructionlosscorrespondingtoanegativeloglikelihoodfroma
standardnormalwithσ =10−3centeredatthedataforthefirststepofthediffusionprocess.
EvaluationprobeDetails. WeusedaU-Net[93]backbonewith64outputchannelsfollowedbya
maxpoolinglayerandn1-layerMLPclassifierforeachofthenclassestoestimateeachconceptof
animageindependently. Wesamplefromthesamedatadistributionbutwithmaximaldatadiversity,
i.e.,withs valuesinApp.A.1maximizedwithintherangeallowingperfectclassification(nooverlap
i
ofzbetweenclasses). Wesample4096imagesperclassfromtheDGPandtraintheclassifierfor10K
gradientstepswithAdamW[115]andachievea100%accuracyontheheldouttestset. Atevaluation
phase,weaveragetheclassifiersoftmaxoutputover5datageneration/modelinitializationseeds
and32inferencesamplestoconstructtheconceptspacerepresentationofthegenerations.
ComputationalDetails. WeimplementourmodelsinPyTorch[116]. Astandardmodelrun(e.g.,in
Sec.4.2)took∼20minutesonasingleNVIDIAA10040GBGPU.TheCelebArunstook∼24
hoursonthesameGPU.
A.3 TrainingProcedure
ThediffusionmodelwasimplementedusingPyTorchandtrainedonfourNvidiaA100GPUs. We
conductedahyperparametersearchusingavalidationset,testingbatchsizesfrom32to256,number
ofchannelsperlayerfrom64to512,learningratesbetween10−4 and10−3,thenumberofsteps
inthediffusionprocessfrom100to400. WeemployedtheAdamoptimizer[117]withβ = 0.9,
1
β =0.99,andweightdecayof10−5.
2
B DetailsonAlternativeProtocolsforElicitingModelCapabilities
InputSpace: Overprompting. Ourmodelistrainedonadistributionofconditioningcentered
aroundaclassdependentmean: pi(z ) = U(mi −s ,mi +s )foreachclassi. Wepromptthe
j j j j j
model with conditioning vectors extrapolated in the direction m⃗ −m⃗ . For instance, assuming
1 0
theredconditioningwas(0.6,0.4,0.4)andtheblueconditioningwas(0.4,0.4,0.6),we“prompt”
themodelwith(0.2,0.2,0.8). Inpractice,weuse5conditionings(0.4,0.4,0.6),(0.35,0.35,0.65),
(0.25,0.25,0.75),(0.15,0.15,0.85),(0.05,0.05,0.95),andreportthemaximumjointaccuracy.
ActivationSpace:LinearLatentIntervention. Wedemonstratetheabilitytocomposecapabilities
bymanipulatingtheconditionalvectors⃗h. Namely,wecreateaconditionvector⃗h foraspecific
i
conceptibyspecifyingaconceptofinterest(e.g.,⃗h =M(z ). Duringtheforwardpass,given
blue blue
⃗h,wecancomputethecomponentofeachconceptin⃗hbyprojectingontoaspecificconcept-condition
vector(⃗h ).Wecanthenenhanceorreducethecomponentofeachconceptbyscalingeachofthese
blue
projectedcomponents. Inpractice,weperformthefollowingoperation:⃗h′ =⃗h+α⃗h −β⃗h ,
blue large
whereα,β arehyperparameters. Wesweepover[0.1,1,2,4]forαand[0.1,0.25,0.5,1]forβ.
⃗h is constructed by first deriving a blue direction in condition embedding space (⃗h ) by
blue 5,5,95
embedding a concept vector (⃗z ) where its RGB components are set to (0.05,0.05,0.95):
5,5,95
19⃗h = M(⃗z ). We then project⃗h onto this direction to derive⃗h . ⃗h is generated
5,5,95 5,5,95 blue large
similarlyusing⃗z . Themodelthengeneratesanimageconditionedon⃗h′.
size=0.7
C AdditionalResults
C.1 LossandAccuracyversusConceptSpace
Weplotloss,accuracy,andtrainingtrajectoryinconceptspaceofthemodelsfromFig.6. Thetime
atwhichthemodelacquiresacapabilitytomanipulatetheconceptcolorisnotevidentfromthe
lossoraccuracycurves;however,inconceptspace,itisevidentthattheconceptislearnedtogether
withthedeparturefromconceptmemorization. Benchmarkinggenerativemodelsisachallenging
task,stillofteninvolvinghumansintheloop[102,103]. Ourconceptspaceframeworksuggeststhat
benchmarkingout-of-distribution(OOD)generalizationcanpotentiallybereducedtomonitoringthe
learningtrajectoryinconceptspace.
Figure12: Lossvs. Accuracyvs. ConceptSpace. Lossandaccuracydonotalwaystellthefull
storyofwhat,andevenmoreimportantlywhen,themodelislearningacapability. Thestarsymbol
inpanel(c)showspointofsuddenturnandconceptlearning,i.e.,themomentwhenwelldefined
promptingprotocolscanelicitthedesiredoutputfromthemodel,indicatingthemodelhaslearned
thecapabilitytoalteraconcept.
C.2 Additionaltrajectoriesoflearningdynamics(Fig.13)
HereweshowallconceptspacetrajectoriesfortheexperimentsmentionedinFig.4(a,b),forall
classesandcolorconceptsignallevels. Wefindasymmetricbehaviorforthe01classandthe10
classwhenadjustingthecolorconceptsignallevel. Thedynamicsofthegenerationsinthetraining
setmatchesourintuitions. Atlowcolorconceptsignal,weobservethatthedynamicsfitthesizefor
both00and10,anddivergetowardstheircorrectcolors.
ID generalization for class 00 ID generalization for class 01 ID generalization for class 10 OOD generalization
01 11 01 11 01 11 01 11
00 Color 10 00 Color 10 00 Color 10 00 Color 10
Figure13: ConceptSpaceDynamicsforallclasses(00,01,10,11). Theexperimentisidenticalas
inFig.4. The[0,1)normalizedcolorconceptsignaliscolorcodedineverytrajectory. Twotraining
datatrajectoriesareshowningrayinthelastpaneltoshowconceptmemorization.
C.3 Experimentswithrealdata: CelebA
Toassessourfindingsonamorerealisticdataset,weranexperimentsontheCelebA[118]dataset.
WeselectedtheNot MaleandSmilingfeaturesasthetwoconceptdimensionstoexplore. This
choicewasmotivatedbytherelativelybalancednumberofsamplesinallfourclasses(00=(Male,
Not Smiling),01=(Male,Smiling),10=(Not Male,Not Smiling),11=(Not Male,Smiling)),
fromwhichwerandomlysampled30Kimagesineachclass. Toconstructtheconceptspace,we
trainedafullyconvolutionalnetworkwithaaveragepoolinglayerfollowedbyaclassificationMLP
20
eziS eziS eziS eziS
Color
concept
signalFigure14: ConceptSpaceDynamicsinCelebA.Wetrainontheclasses00=(Male,NOTSmiling),
01=(Male,Smiling), 10=(NOTMale,NOTSmiling) (plotted in blue) and test on the class
10=(NOTMale,Smiling) (plotted in pink). We find similar observations as in Fig. 4.2, concept
memorizationandtheresultingbias.
headtoclassifythetwoattributeswithindependentcrossentropylosses. (SeeA.2). Wetrainedthe
classifierwiththeAdamW[115]optimizerwithlearningrate10−3andweightdecay10−5for10K
gradientsteps. Theclassifierachievedafinalaccuracyofrespectively95%and97%ontheheldout
validationset,whichwas10%oftheentiredataset.
Wetrainedthesamediffusionmodel(SeeApp.A.2)fromoursyntheticexperimentson64x64resized
imagesfromtheclasses(00,01,10)andassessedtheout-of-distribution(OOD)generalizationto11.
Weusedacolorjitterof0.1forbrightness,contrastandsaturationandrandomlyflippedtheimages
horizontally. Astheclassattributesarecategorical,theywereonehotencodedandconcatenatedto
aninputconditioningvectorof4dimensions. Thediffusionmodelwastrainedfor106gradientsteps
withabatchsizeof64withthesameoptimizerasinthemainexperiments.
TheconceptspacedynamicsofgenerationsfromtheindistributionconditioningandOODcondition-
ingareshowninFig.14. Wefindsimilarobservationsfromtheconceptspacetrajectoriesasinour
syntheticexperiments. Initially,theimagescorrespondingtotheclass11followstheconceptspace
trajectoryof10,optimizingNot Male,whichweintuitivelyexpecttohaveastrongerconceptsignal,
althoughitisintractabletocomputesincetheDGPisunknown. Similartooursyntheticexperiments,
weseeatransitionintheconceptspacetrajectoryofthecompositionalclasscorrespondingtothe
modeldisentanglingtheconcepts. Afterthistransition,conceptlearningbeginsbutweseebiases,
whereweseeatrade-offofmovingintherightdirectiontowardsoneconceptdegradestheotherone
asseeninFig.4(b).ThevisualgenerationsinFig.14confirmthatourfindingsarenotmerelyaresult
ofanill-calibratedclassifiermodel. However,inthiscase,wedonotobservefullout-of-distribution
(OOD)generalizationat1Mgradientsteps. Weexpectthistaskofgenerating(NOT Male,Smiling)
isinherentlyharderthanoursyntheticsetup. Wenotethatthegoalofthisexperimentisnottoshow
goodcompositionalgeneralizationbutitismoreonconfirmingthatourqualitativefindingsgeneralize
torealdatawithouttouchingthemodelortrainingmethod.
C.4 Experimentswith3Dconceptspace(Fig.15)
TofurtherverifyourfindingsinSec.4,weexploreasetupwheretheconceptconditioningspecifies
threeconcepts: color,sizeandbackground color. Fig.15illustratestwoexamplescenarios
inwhichtheconceptsignalincolorandbackground colorarevaried,respectivelyresultingin
casesofsuccessandfailureofOODgeneralization. Thelengthofedgesofthecuboidsrepresent
theirconceptsignalmagnitude. InFig.15(a),weseethattheobjectcolorhasastrongconceptsignal
21andthisisreflectedintheconceptspacetrajectoriesasthisdirectionbeingsplitfirst. Inthiscase
wesee,similartothebluegeneralizationcurveinFig.4(b)andFig.13(rightmostpanel),aslow
generalization process for the compositional class 111. Similarly to our findings in Sec. 4.2, we
observethatthe011classinitiallyundergoesconceptmemorizationforclass010,whichsharesthe
twostrongerconceptsignalscolorandbackground color,andshowsatransitionwhereitleaves
this phase. In Fig. 15 (b), we see a case where out-of-distribution (OOD) generalization did not
succeedwithinthegiven80Kgradientsteps. Inthiscase,weseetwoclasses,011and111,which
arenotcorrectlylearned,andthussuspectthatthemodeldidnotlearntogeneralizethebackground
color. However,theconceptspacetrajectoriessuggestthatthecapabilityispresent.
(a)
011
111
010
110
001
101
000
100
(b)
011 111
010
110
001
101
000 100
Figure 15: Concept Learning Geometry underlies emergence Success and Failure modes of
out-of-distribution(OOD)generalization.(a)Asuccesscasewherecolorcarriesthestrongestconcept
signal. Weseeaspectsofconceptmemorizationfromtheclass011. (b)Afailurecasewherethe
backgroundcolordidn’tgeneralize. Inthiscase,themodeldoesnotproducetherightbackground
colorwhiletheconceptspacesuggestthatthiscapabilityispresent.
C.5 Experimentswithclassifierfreeguidance
An implication of our conclusions in Sec. 4.4 is that before the model has passed the transition
point where the model has the capability to compose concepts, the model should not be able to
generatesmallbluecirclesnomatterhowwell“prompted”. Here,insteadofprompting,weexplore
ClassifierFreeGuidance(CFG)[95]toseeifourfindingsapplytoaconditionaldiffusionmodel
trainedwithCFG.InFig.16,weseethateventhemodelswithCFGshowthistransitionfromconcept
memorizationtoout-of-distribution(OOD)generalization. Inascenariowherethereisn’tasharp
acquisitionofthecapabilitytocompositionallygeneralize,wewouldexpectthesharptransitionto
disappearwithCFGscale.
ID generalization for class 00 ID generalization for class 01 ID generalization for class 10 OOD generalization
01 11 01 11 01 11 01 11
00 Color 10 00 Color 10 00 Color 10 00 Color 10
Figure 16: Learning Dynamics in Concept space for Classifier-free Guidance We show the
learning dynamics in the concept space for all the classes (00, 01, 10, 11). The generalization
task(rightmost)showsasharptransitionfromconceptmemorizationtoout-of-distribution(OOD)
generalizationindependentlyoftheClassifier-FreeGuidance(CFG)scale.
22
eziS eziS eziS eziS wcfgC.6 Patchingtheembeddingmodule
AssumingamodeldoesnothavethecapabilitytogenerateimagesfromanOODconceptclass,we
simplyswaptheembeddingmoduleforprocessingtheconditioningvectorhwiththatofthelast
checkpoint. Onepossibleinterpretationforthismethodisthattheembeddingmoduledisentangles
theconcepts, i.e., generatesarepresentationforeachconcept, whiletheU-Net[93]thenutilizes
suchrepresentations. ThiswouldimplythattheU-Net[93]alreadylearnshowtoutilizeconcept
representations early during training, while further gradient steps lead to more robust concept
representations.
Wetestourapproachon5randominitializationseeds. ResultsareshowninFig.17andwefind
that for some seeds, we are able to elicit the target behavior at around the same time in which
overpromptingandlinearinterventionsalsoelicitthetargetbehavior;forotherseeds,thisisnotthe
case. Interestingly,wefindthatourmethodworkswellforseedswith“stable”learningdynamics,in
thatoncethemodelreaches∼100%accuracy,itconvergesandstaysatsuchaccuracy,whilesome
seedshave“unstable”learningdynamicsinthattheiraccuraciesoscillate.
(a) (b)
Figure17: Embeddingpatching. Wepatchtheembeddingmodule(anMLP)usedfortransforming
theconditioninginformationintoanembeddingthatthemodelprocessesfromthelastcheckpoint
tointermediatecheckpoints. Panel(a)showsthebaselineaccuracyforout-of-distribution(OOD)
generalizationacrossfivedifferentseedruns,whilepanel(b)showsaccuracyachievedwhenthe
patchedembeddingmoduleisused.
23