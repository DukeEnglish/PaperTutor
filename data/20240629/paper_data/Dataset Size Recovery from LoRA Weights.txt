Dataset Size Recovery from LoRA Weights
MohammadSalama JonathanKahana EliahuHorwitz YedidHoshen
SchoolofComputerScienceandEngineering
TheHebrewUniversityofJerusalem,Israel
https://vision.huji.ac.il/dsire/
{ mohammad.salama3, jonathan.kahana, eliahu.horwitz, yedid.hoshen}@mail.huji.ac.il
Abstract
Modelinversionandmembershipinferenceattacksaimtoreconstructandverify
thedatawhichamodelwastrainedon. However,theyarenotguaranteedtofind
alltrainingsamplesastheydonotknowthesizeofthetrainingset. Inthispaper,
weintroduceanewtask: datasetsizerecovery,thataimstodeterminethenumber
of samples used to train a model, directly from its weights. We then propose
DSiRe,amethodforrecoveringthenumberofimagesusedtofine-tuneamodel,
in the common case where fine-tuning uses LoRA. We discover that both the
normandthespectrumoftheLoRAmatricesarecloselylinkedtothefine-tuning
datasetsize;weleveragethisfindingtoproposeasimpleyeteffectiveprediction
algorithm. ToevaluatedatasetsizerecoveryofLoRAweights,wedevelopand
releaseanewbenchmark,LoRA-WiSE,consistingofover25,000weightsnapshots
frommorethan2,000diverseLoRAfine-tunedmodels. Ourbestclassifiercan
predictthenumberoffine-tuningimageswithameanabsoluteerrorof0.36images,
establishingthefeasibilityofthisattack.
1 Introduction
Dataisthetopfactorforthesuccessofmachinelearningmodels. Modelinversion[13,52,14]and
membershipinferenceattacks[5,42,25]aimtoreconstructandverifythetrainingdataofamodel,
using its weights[14, 11, 36]. While these methods may discover some of the training data, they
arenotguaranteedtorecoveralltrainingsamples. Onefundamentallimitthatpreventsthemfrom
discoveringtheentiretyofthetrainingdataisthattheydonothaveahaltingcondition,astheydo
notknowthesizeofthetrainingset[14]. E.g.,inmembershipinference,theattackersequentially
testsasetofimagesformembershipinthetrainingsetbutdoesnotknowwhentohaltthealgorithm,
effectivelymakingitsruntimeinfinite.
Discoveringthesizeofatrainingdatasetgiventhemodelweightsisimportant,evenwithoutexplicit
reconstructionoftheimagesthemselves. AstockphotographyprovidersuchasGettyorShuterstock,
mayallowuserstousetheirdataforfine-tuningpersonalizedgenerativemodelsandchargethem
forthenumberofimagesactuallyusedfortraining. Quantifyingthedatasetsizewouldthereforebe
essentialforbilling. Understandingthenumberofimagesusedtotrainorfine-tunemodelsisalsoof
greatinteresttoresearchers,whowishtounderstandthecostsofreplicatingamodel’sperformance.
Wethereforeproposeanewtask: DatasetSizeRecovery,whichaimstorecoverthenumberoftraining
imagesbasedonthemodel’sweights. Wetackletheimportantspecialcaseofrecoveringthenumber
ofimagesusedtofine-tuneamodel,wherefine-tuningusesLow-RankAdaption(LoRA)[21]. These
LoRApersonalizedtext-to-imagefoundationmodels[39]areamongthemostcommonlytrained
models,asevidentbythesuccessofmarketplacesforpublicsharingofthemsuchasCivitAIand
HuggingFace.
Preprint. Underreview.
4202
nuJ
72
]VC.sc[
1v59391.6042:viXraDSiRe
Finetuning
P r e M-T or da ein led LoRA
𝑑WL
eo𝑟
iR gA hts
SVD 𝜎 𝜎𝜎
𝜎 . .
.1 2
3
𝑟
KNN
𝑠𝑠𝑠
. .
.
𝐿21
M Va ojo tr eity 𝑛∗
𝑛 samples 𝐿
𝑀𝐴𝐸
𝒏
2
1
Invisible to Attacker
Figure1: DSiRe: Weintroducethetaskofdatasetsizerecovery,whichaimstorecoverthedataset
sizeusedtoLoRAfine-tuneamodelbasedonitsweights. DSiReextractsthesingularvaluesofeach
LoRAmatrixandtreatsthemasfeatures. Thesefeaturesarethenusedtotrainasetoflayer-specific
nearest-neighborclassifierswhichpredictthedatasetsize.
WefirstanalyzetherelationshipbetweenLoRAfine-tuningweightsandtheircorrespondingdataset
size. OurobservationssuggestthattheFrobeniusnormofLoRAmatricesishighlypredictiveoftheir
fine-tuningdatasetsize. Asasinglescalarperweightmatrixprovideslimitedexpressivity,singular
valuesofeachweightmatrixserveasmoreexpressivefeaturevectors. Withthisinmind,wepresent
DSiRe(DatasetSizeRecovery). Themethodrecoversthefine-tuningdatasetsizefromtheLoRA
weightspectrum. DSiReclassifiesthedatasetsizeusingatrainedclassifierontopoftheweight
spectrumofthelayer. Inpractice,wefoundthataverysimpleclassifiersuffices;inourexperiments,
DSiReusesthenearestneighborclassifier. ForanoverviewofthemethodseeFig1.
Toenabletheevaluationofthisworkandencouragefutureresearch,wereleaseanew,large-scale,
anddiversedataset: LoRA-WiSE.Thedatasetcomprisesover25,000weightscheckpointsdrawnfrom
morethan2,000independentLoRAmodels,spanningdifferentdatasetsizes,backbones,ranks,and
personalizationsets. OnLoRA-WiSE,DSiRerecoversthedatasetsizesfromLoRAweightswitha
MeanAbsoluteError(MAE)of0.36,demonstratingthatourmethodishighlyeffectiveinrealistic
settings.
Tosummarize,ourmaincontributionsare:
1. Introducingthetaskofdatasetsizerecovery.
2. PresentingDSiRe,amethodforrecoveringdatasetsizeforLoRAfine-tuning.
3. ReleasingLoRA-WiSE,acomprehensivedatasetsizerecoveryevaluationsuitebasedon
Stable-Diffusion-fine-tunedLoRAs.
2 Relatedwork
2.1 ModelFine-Tuning
Modelfine-tuning[58,56,3]adaptsamodelforadownstreamtaskandisconsideredacornerstone
inmachinelearning. Theemergenceoflargefoundationmodels[37,45,4,38]hasmadestandard
fine-tuningcostlyandunattainablewithoutsubstantialresources. Parameter-EfficientFine-Tuning
(PEFT)methodswerethenproposed[21,10,20,29,28,32,16,31,26,59,47,24],offeringvarious
waystofine-tunemodelswithfeweroptimizedparameters. Amongthesemethods,LoRA[21]stands
out, proposing to train additive low-rank weight matrices while keeping the pre-trained weights
frozen. LoRAwasfoundtobeveryeffectiveacrossseveralmodalities[48,53,2]. Recently,Horwitz
etal.[19]identifiedasecurityissueinLoRAfine-tuning,demonstratingthatmultipleLoRAscan
2be used to recover the original pre-trained weights. In this paper, we uncover a new use case of
LoRAfine-tuning,specificallyfocusingontherecoveryofthedatasetsizefromtext-to-imagemodels
fine-tunedviaLoRA.
2.2 MembershipInference&ModelInversionAttacks
Two privacy vulnerabilities found in machine learning models are Membership Inference Attack
(MIA)[41,5,23,42,25]andModelInversion[13,52,17,55,14]. Firstpresentedby[44],MIAs
aimtoverifywhetheracertainimagewasinthetrainingdatasetofagivenmodel. Typically,MIAs
assumesthattrainingsamplesareover-fittedproposingvariousmembershipcriteria;eitherbylooking
forlowerlossvalues[40,54]orsomeothermetrics[49,5]. Ingenerativemodels,MIAshavebeen
extensivelyresearchedaswell[18,15,6],includingrecentattacksagainstdiffusionmodels[35,22].
Modelinversionisasimilarattack,inadata-freesetting. Introducedby[13],modelinversionmethods
wishtogeneratetrainingsamplesfromscratch,insteadofaskingwhetheraknownspecificimagewas
inthetrainingset. Modelinversionisalsousedforsettingswheredataisunavailable,e.g.,data-free
quantization[7,51,30]anddata-freedistillation[33,60,57,12,43].
Haimetal.[14]emphasizedtheimportanceofrecoveringthetrainingsetsizeformodelinversion
applications. Whenthissizeisunknown,itpreventsmodelinversionattacksfromreconstructing
the entire dataset a model was trained on, as it is unclear how many samples are sufficient. Our
workspecificallyaddressesthisissuebyuncoveringanewvulnerabilityinfine-tunedmodels,which
enablesustoinferthesizeofthedatasetusedforfine-tuning.
3 Motivation
3.1 Background: LoRAfine-tuning.
Fine-tuninglargefoundationmodelscanbeacomputationallyexpensiveprocess,asitmodifieseach
weightmatrixofthepre-trainedmodelW ∈Rd×k,byanadditivefine-tuningmatrix∆W,asfollows:
W′ =W +∆W (1)
Recently,Huetal.[21]introducedLow-RankAdaptation(LoRA)forefficientfine-tuning. InLoRA,
thefine-tuningmatrix∆W islow-rank,i.e.,wechooseitsrankrs.t. r ≪min(d,k). Anefficient
i
andSGDamenablewaytoimplementlow-rankmatricesistoparameterizethemastheproductoftwo
rectangularmatricessothatB ∈Rd×r,A ∈Rr×k. Thefine-tuningmatrixisthereforegivenby:
i i
∆W =B A (2)
i i i
3.2 AnalysingtheLoRASpectrum.
Ourhypothesisisthatthedifferencebetweenpre-fine-tuningandpost-fine-tuningweights,denoted
as∆W ,encodesinformationaboutthesizeofthefine-tuningdataset. Here,wefocusonthecase
i
wherefine-tuningusesLoRA,i.e.,where∆W islow-rank,whichisemergingasthemostpopular
i
fine-tuningparadigmforfoundationmodels.
Webeginbyconsideringaverysimplestatisticofeachfine-tuningmatrix,itsFrobeniusnormthatwe
denotes .
F
(cid:88)
s = |∆W |2 (3)
F ij
ij
Thenormofaweightmatrixcorrelateswiththefunctioncomplexitythenetworkcanexpress. For
example,weightdecay,thateffectivelyconstrainsthenormisacommonwayforregularizingmodels.
Wesuggestasimpleexperimenttoanalyzethecorrelationbetweens andthefine-tuningdataset
F
size. Wefine-tuneStableDiffusion(SD)1.5on50micro-datasetsofsizes1to6images,keeping
all hyper-parameters, apart from the dataset size, fixed. Fig. 2a shows the range of values of the
Frobeniusnormstatistics foreachdatasetsize. Itisclearthats isnegativelycorrelatedtothe
F F
datasetsize. Wemotivatethiscorrelationbyover-fitting,i.e.,modelsover-fitfastertosmallerdataset
sizes,leadingtolargersizesofs .
F
Toobtainadeeperunderstanding,wemonitoranotherstatisticofthefine-tuningmatrix,itssingular
valuesspectrum. AsweconsiderthecaseofLoRAfine-tuning,thespectrumof∆W hasatmostr
3(a) (b)
Figure2: NormandSpectrumofFine-TuningWeightsvs. DatasetSize. Analysisof210Stable
Diffusion1.5modelsfine-tunedondatasetsofsizesfrom1−6. (a)Frobeniusnormrangeperdataset
size(b)Singularvaluesperdatasetsize. Thereisaclearnegativecorrelationbetweenweight/spectrum
magnitudesandthesizeofthefine-tuningdataset.
(a) (b)
Figure3: SpectrumRangesof2DifferentLayers. Singularvaluesdistributionoftwolayerson
oppositesidesofStableDiffusion1.5UNet,fine-tunedondatasetsofsizes1−6. (a)Firstdownblock
(b)Lastupperblock. Thelastupperblockshowsgreaterseparationofsingularvaluescomparedto
thefirstdownblock,highlightingthatnotalllayersarebornequallyfordatasetsizerecovery.
non-zerovalues(whereristherank). Wedenotethemthspectralvalueasσ ,wherem∈[1,...,r].
m
Therangeofvaluesofwereplottedσ acrossdifferentvaluesofmanddifferentdatasetsizesin
m
Fig.2b. Wenotethereisabetterseparationbetweendifferentdatasetsizesforthelargestsingular
values. This suggests that the spectrum is more discriminative than the scalar Frobenius norm.
Overall,boths andthespectrumindicatelargervaluesforsmalldatasetsizes.
F
Finally,weanalyzedhowdiscriminativedifferentlayersareforpredictingfine-tuningdatasetsize.
Weplotthespectraoftwodifferentlayers-inthefirstdownblockandthelastupblockinFig.3. We
canseethatthelayerismorediscriminativethantheformerone. Indeed,theuplayersarefoundtobe
morediscriminativethanthedownones. Withoutaclearexplanation,itcanhypothesizethatthe
UNetdecoderismorepronetoover-fittingthantheencoder. Itisnoteworthythatourexperiments
revealedthatnosinglelayerisdiscriminativeforallmodels,suggestingthatweightingtheresults
fromalllayersisbetter.
4 Method
4.1 TaskDefinition: DatasetSizeRecovery
WeintroducethetaskofDatasetSizeRecoveryforfine-tuneddataset,anewattackvectoragainst
fine-tuned models. Formally, given the fine-tuning weights of all layers of a model denoted as
4∆W = [∆W ,∆W ...∆W ], our task is to recover the number of images n that the model was
1 2 L
fine-tunedon. Moreformally,wewishtofindafunctionf,suchthat:
n=f(∆W) (4)
TheeffectivenessofthisattackwasmeasuredbytheMAEbetweenf(∆W)andnacrossasetof
models.
4.2 DSiRe
WeproposeDSiRe(DatasetSizeRecovery),asupervisedmethodforrecoveringdatasetsizefrom
LoRAfine-tunedweights. Ourapproachfirstconstructsatrainingdatasetbyfine-tuningmultiple
LoRAmodelsonconceptpersonalizationsetsacrossarangeofdatasetsizes(seeSec. 5). Itthen
trainsapredictorfunctionf thatactsonasetoffine-tuningweightsofeachmodelandoutputsthe
predicteddatasetsizen. Attesttime,itgeneralizestounseenmodelstrainedwithdifferentconcepts.
ThemethodcanbeseeninFig.1
Trainingsetsynthesis. Wefirstsynthesizeatrainingsetbyfine-tuningourmodeloneachofN
train
datasets,eachcontainingasetoftrainingimages. Thedatasetsspanarangeofsizes;inthispaper,we
testedtheranges1−6,1−50,and1−1000. TheresultisasetofN models∆W ,eachwitha
train m
correspondinglabelofthedatasetsizen .
m
Predictortraining. GiventhesetofN labeledmodels,wewishtotrainapredictorthatmaps
train
thefine-tuningweightsW todatasetsizen . Motivatedbytheresultsofouranalysis(seeSec.3),
m m
wedescribetheweightsofeachfine-tuninglayerusingitsspectrumΣconsistingofrsingularvalues.
LetusdefinethefeaturesofeachmodelasthesetofspectraofallitsLlayersf =[Σ ,Σ ..Σ ]. We
1 2 L
testedmanydifferentpredictorsandablatedtheminTab.5. Overall,thesimpleNearestNeighbor
(NN)ensembleperformedthebest. Duringinference,givenanewmodel,foreachlayer,weretrieve
theNNlayerthathasthemostsimilarspectrumtothelayerofthetargetmodel. Eachlayervotesfor
thedatasetsizeofitsNNlayer. Theoverallpredictionisthedatasetsizethatmostlayersvotedfor.
5 LoRAWiSEBenchmark
WepresenttheLoRAWeightSizeEvaluation(LoRA-WiSE)benchmark,acomprehensivebenchmark
specificallydesignedtoevaluateLoRAdatasetsizerecoverymethods,forgenerativemodels. More
specifically,itfeaturestheweightsof2350StableDiffusion[38]models,whichwereLoRAfine-tuned
byastandard,popularprotocol[39,1]. Ourbenchmarkincludesversions1.5and2ofStableDiffusion,
having2050and300trainedmodelsforeachversionrespectively.
Wefine-tunethemodelsusingthreedifferentrangesofdatasetsize: (i)Lowdatarange: 1−6images.
(ii)Mediumdatarange: 1−50images. (iii)Highdatarange: 1−1000. Foreachrange,weusea
discretesetoffine-tuningdatasetsizes. Inthelowandmediumranges,wealsoprovideotherversions
ofthesebenchmarkswithdifferentLoRAranksandbackbones. SeeTab.1fortheprecisebenchmark
details.
Forourlowdatarangeset,wechooseConcept101[27],apreviouslycollectedsetofmicro-datasets
(3−15images)designedforpersonalizationresearch. Forourmediumandhighdatarangesweuse
differentclassesofImageNet[9]asthedatasource. Thisselectionofdatasetsaimstoensurethatthe
fine-tunedmodelsaredrawnfromadiversesetofconcepts,spanningvariouscategories.
Eachmicro-datasetisusedtofine-tunethemodelsforeachdatasetsize. Theimagesarerandomly
selectedfromthemicro-dataset. EachStableDiffusionmodelconsistsof132adaptedlayers(pairsof
A ,B ),includingvariouslayertypes,suchasself-attention,cross-attention,andMLPs. WesaveA ,
i i i
B separately,i.e.,eachmodelprovidesatotalof264uniqueweightmatrices. Wethenspliteach
i
rangeofthisnewbenchmark(low,medium,andhighranges)intotrainandtestsetsbasedonthe
micro-datasets. formoredetailsseeappendix12.1
5Table1: LoRAWiSEBenchmarkOverview. Thedatasetcomprisesover25,000weightscheckpoints
drawnfrommorethan2000independentLoRAmodels,spanningdifferentdatasetsizes,backbones,
ranks,andpersonalizationsets.
DataRange DatasetSizes Source Backbone LoRARank #ofModels
8 300
Low 1,2,3,4,5,6 Concept101 SD1.5 16 300
32 300
16 300
SD1.5 32 300
Medium 1,10,20,30,40,50 ImageNet
64 300
SD2 32 300
High 1,50,100,500,1000 ImageNet SD1.5 32 250
6 Experiments
6.1 ExperimentalSetup
WeevaluateDSiReontheLoRA-WiSEbenchmark. Unlessmentionedotherwise, weuseStable
Diffusion1.5asthepre-trainedmodel, whichwefine-tuneusingaLoRAofrank32(denotedas
above∆W). Asforthetrainingset,weuse15differentpersonalizationsets,totraineach∆W model,
resultinginatrainingsetof90weightsamplesforlowandmediumdatasetsizes, and75weight
samplesforourhigher(1−1000)range. WeevaluateDSiReusing35additionalpersonalizationsets.
Wenote,Werepeatthisexperiment10times,includingsubsetsampling. Thereportedperformance
metricsaretheaverageandstandarddeviationovertheexperiments.
Baseline. WecompareDSiRetoabaseline,denotedasFrobenius-NN,whichpredictsthedataset
sizeusinganearestneighborclassifierontopoftheFrobeniusnormsofthelayers’LoRAweights.
SimilartoDSiRe,theFrobenius-NNisfittedseparatelytoeachlayer,andthenamajorityvoterule
isappliedtoselectthepredictionfromalllayer-wisepredictions. TheanalysisinSec3provides
motivationforthisbaseline.
Evaluationmetrics. AsdescribedinSec. 4.1,ourmainevaluationmetricisMeanAbsoluteError
(MAE).Forcompleteness,wechoosetoreporttwocomplementarymetricsaswell: (i)Accuracy. (ii)
MeanAbsolutePercentageError(MAPE).SinceDSiRepredictsdatasetsizes,simpleaccuracydoes
notadequatelymeasureitseffectiveness,e.g.,predicting4whenthetruevalueis5isnotasbadas
predicting1. WethereforeprovideMAPEscoresaswell,whichcomputethepercentilefromthe
groundtruththatisequaltotheabsoluteerror.
6.2 Results
Webeginbyevaluatingon1−6fine-tuningimages. Whenusingboththesingularvaluesasfeatures
forDSiReandtheFrobeniusnormforFrobenius-NN,wefindthattheyyieldrelativelysuccessful
resultsinrecoveringthedatasetsize.
These low data range results are presented in Table 2. As DSiRe outperforms Frobenius-NN by
asmallmargin(> 3%),weconcludethattheresultsalignwithouranalysis(seeSec.3.2),which
demonstratethatbothsingularvaluesandFrobeniusnormareindeedpredictiveofthedatasetsize.
Mid-rangefine-tuningdatasetsizes(10−50images)arecommoninartisticLoRAfine-tuning. We
thereforepresenttheresultsofourmethodusing1−50fine-tuningimagesinTable2,showingthat
DSiReperformswellwithanMAEof1.48. Inthisdatarange,theFrobenius-NNbaselineachieves
comparableresultstoDSiReacrossallmetrics,demonstratinggoodperformance. Whiletheabsolute
MAEvalueislargerthaninthelowdatarangecase,itisrelativelysmallcomparedtotherangeof
datasizes. Theaccuracyandmeanabsolutepercentageerror(MAPE)scoresofbothmethodsfurther
supportthisobservation. Fig. 4showsanotherfavorablepropertyofourapproach: itsmistakesare
usuallynearhits,i.e.,largeerrorsbetweengroundtruthandpredictedlabelsarerare.
6Table2: PerformanceComparisonofDatasetSizeRecoveryMethodsacrossDifferentRanges.
PerformanceofFrobenius-NNandDSiReacrossdifferentdataranges(1−6,1−50,1−1000)using
StableDiffusion1.5. Thesedecentresultsalignswithouranalysis(seeSec3),showedthatbothSVD
andFrobeniusnormareeffectivefeaturesfordatasetsizerecovery. However,DSiReoutperfomrsthe
Frobenius-NNonallevaluationmetrics.
DataRange Method MAE↓ MAPE(%)↓ Acc(%)↑
Frobenius-NN 0.43±0.04 15.14±2.12 65.29±2.42
1-6
DSiRe 0.36±0.04 11.36±1.55 69.30±3.83
Frobenius-NN 1.56±0.19 4.16±0.75 85.33±1.81
1-50
DSiRe 1.48±0.21 3.97±0.73 86.10±1.99
Frobenius-NN 68.62±5.53 9.25±1.21 86.51±1.12
1-1000
DSiRe 41.77±6.61 5.96±1.46 91.90±1.28
Table3: RobustnessofDatasetSizeRecoveryMethodsonStableDiffusion2.0. DSiRerecovers
datasetsizemoreeffectivelythanFrobenius-NNforthemediumdatarange(1−50)usingStable
Diffusion2.0. Thissupportsthebenefitfromamoreexpressiverepresentationgivenbythesingular
values,independenttothespecificbackbonemodelused.
Method MAE↓ MAPE(%)↓ Acc(%)↑
Frobenius-NN 2.95±0.28 11.99±3.93 73.90±2.21
DSiRe 2.51±0.22 7.46±0.95 77.43±1.70
At larger data quantities, dataset size recovery could aid in better understanding data collection
quantitiesneededforfine-tuning. Therefore,weconductedanadditionalexperimentusingmodels
trainedwithhigherdataranges,having1,50,100,500and1000imagesamplespermodel(notethat
herewehave5datasetsizeclasses). Results,presentedinTab. 2,showsDSiReisabletodetectthe
datasetsizewithmorethan90%accuracy,andaMAPEscoreofonly6%. Additionally,inFig. 6we
showtheconfusionmatrixgeneratedbyDSiRe,whereweseethatmostoftheerrorshappenbetween
adjacentclasses.
OtherBackbone TheLoRAfine-tuningtechniqueiscommonlyusedbypopulartext-to-image
models. Adesirableaspectofourparadigmisbeingrobusttomodelarchitecture. Inthispart,We
testtherobustnessofDSiRetothebackbonemodelbyevaluatingitonStableDiffusion2.0. Wenote
thatthesemodelsdonotsharepre-trainingweights,asStableDiffusion2.0wasnotfine-tunedfroma
previousversion. Tab. 3showsthatDSiREperformswellonStableDiffusion2.0,reachingaround
77%accuracy. Thisprovidesevidenceforthecorrelationbetweenthesingularvaluesanddatasetsize
isnotspecifictoonebackbonealone.
7 Ablationstudies
7.1 NumberofMicro-Datasets
Whileourattackisdatadrivenandrequiresaccesstothepre-trainedmodel,wefindthatonlyafew
examplesareneededforDSiRetoperformwell. E.g.,inourmediumdatasizerange,ourmodelcan
reach86.4%accuracyusingonly5micro-datasetsfortraining. Thefullresults,presentedinFig.5,
showcasesthatwhilemoresamples(fine-tunedmodels)improvestheaccuracyofourpredictor,even
asinglemicro-datasetissufficienttoachievearound80%accuracy. Thisshowsthatourmethodis
robusttothenumberofmicro-datasetsused,eventoverysmallnumbers.
7.2 RobustnesstoLoRAHyper-Parameters
While DSiRe performs well in our settings, it is important to show that it remains robust to
hyperparameters. Inthissection,wewillablateourmethodusingtheLoRA-WiSEbenchmarkby
testingDSiRewithmodelsfine-tunedusingotherrecipes. Wechecktwocommonvariationsofthe
7Figure4: DSiReConfusionMatrixforMedium
Figure5: DSiResMicro-DatasetSizevs. Accu-
DataRangeinasingleexperiment. Illustrating
racy,reportedonmediumdatasizerange(1−50).
DSiResaccuracyintherangeof1−50samples,
Evenasinglemicro-datasetissufficientforDSiRe
shows that most of the errors are near misses,
to reach 80% accuracy. This demonstrates its
highlighting DSiRe’s precision in dataset size
effectivenesswithlimitedtrainingdata.
recovery.
Table4: DSiRePerformancewithDifferentLoRARanks. Desireconsistentlyachieveshighaccuracy
acrossbothlowandmediumranges,indicatingitsrobustnessregardlessofLoRArankvariations.
DataRange LoRARank MAE↓ MAPE(%)↓ Acc(%)↑
8 0.43±0.04 14.8±2.3 66±3.08
1−6 16 0.42±0.03 12.4±1.11 67.7±2.3
32 0.36±0.04 11.36±1.55 69.30±3.83
16 1.67±0.17 4.32±0.46 84.04±1.85
1−50 32 1.48±0.21 3.97±0.73 86.10±1.99
64 1.41±0.39 3.90±1.30 86.58±3.45
LoRAmodels: (1)LoRArank(2)seeding. Foradditionalablationstudiesonthetrainingsteps,batch
size,andusedclassifierseeappendix. 11.2.
LoRARank. StartingwiththeLoRarankablation,wetrainDSiRefordifferentvaryingLoRAranks.
Tab. 4showstheresultsformediumandlowdataranges. OurmethodisrobusttotheLoRArank,
achievingsimilarresultsinall3testedranksforbothranges.
Seeding. Whileinthestandardrecipe,allmodelsuseseed = 0,wealsotestedthecasewhereall
seedswereselectedrandomly. Tab. 7showsthatthevariationinseedsonlyreducesaccuracyby
around4%,andthatMAEdecreasesbylessthan0.5. Thisisnotasmallchange,giventhatthegap
betweenpossibledatasetsizevaluesis10.
7.3 ChoiceofClassifier
Wetestedvariouspredictors,includingparametricandnon-parametricclassifiers,asshowninTable5.
ExcepttheNN-fullmodelbaseline,ourpipelineremainsunchanged,allclassifiersarefittedseparately
toeachlayerandamajorityvoteruleselectsthelabelfromalllayer-wisepredictions. Incontrast,
theNN-fullmodelusesakNNclassifierwhichisfittedtoallthelayerssimultaneously. Theresults
showthatthechoiceofclassifieraffectstheperformancesignificantly. Furthermore,theseresults
confirmourhypothesisfromSec. 3: whileeachlayerispredictiveofthedatasetsize,theaccuracyof
individuallayersisnotsufficient. Asthelayer-wiseNNpredictorcanpoolinformationacrosslayers,
ithelpsDSiReperformbetterthantheothermethods.
8 Discussion
PerformanceatLowDataRanges. Whileourapproachshowspromisingresults,thereisroomfor
improvementinlowerdataregimes,whereDSiRereacheslessthan80%accuracy. Improvingthese
resultswillprovidetighterupperboundsformembershipinferenceandmodelinversionattacks.
8Table 5: Performance of various predictors, medium dataset size range (1−50). DSiRe with
majorityvotingperformsbestbycombiningpredictionsfrommultiplelayers,opposedtothenearest
neighbor-fullmodelbaseline,whichusesthespectraofalllayerstogetherasfeaturesforasingle
prediction.
Model MAE↓ MAPE(%)↓ Acc(%)↑
NN-fullmodel 7.33±0.75 33.37±7.71 48.61±3.53
RidgeRegression 8.05±0.08 38.95±6.24 37.09±0.63
GDA 2.89±0.48 7.87±1.30 74.14±3.65
DSiRe-averagevote 3.57±0.36 19.05±3.37 64.62±3.58
DSiRe-majorityvote 1.48±0.21 3.97±0.73 86.10±1.99
Table6: DefenseagainstDSiRe,onmediumdatarange(1−50). Weshowhowthesimplestdata
augmentation(horizontalflipping),canharmDSiResperformance,reducingitsaccuracybymore
than26%.
Ablation MAE↓ MAPE(%)↓ Acc(%)↑
Defense 4.62±0.83 17.62±4.82 59.71±5.88
DSiRe 1.48±0.21 3.97±0.73 86.10±1.99
DataDrivenSolution. Ourmethodisdatadrivenasitrequirestrainingmultiplemodelsfromeach
datasetsize. However,ouranalysisshowsthereiscorrelationbetweentheFrobeniusnormanddataset
size(seeFig. 2a). Thisinsightcouldbeasteppingstoneindevelopingadata-freesolution.
Pre-training dataset size recovery. Another interesting application of dataset size recovery is
forpre-trainingcases. Lowerboundingtherequirednumberoftrainingsetsamplesforfoundation
modelswillhaveasubstantialimpactontheresearchcommunity. Answeringthisquestionwould
requirescalingupourmethodtomuchlargerdatasetsizesandweightmatrixdimensions.
DefenseagainstDSiRe. Dataaugmentationaimstoartificiallyincreasethesizeofthedatasetby
generatingmoreimagetransformations,suchasflippingandrotationonexistingtrainingimages.
Motivatedbythisobservation,weemploydataaugmentationduringthefine-tuningprocesstoinflate
theactualnumberofuniqueimagesusedforfine-tuning,addingmorevariationtoeveryimageinthe
dataset. Toavoidhurtingmodelconvergenceduringfine-tuningorthequalityofthegeneratedimages,
weappliedonlytheflipaugmentationtechniquetoeachimageinthedataset. Thisprocessreduced
thecorrelationbetweenthesingularvaluesandthetruenumberofimages,asillustratedinFig. 6.
9 Socialimpact
Webelieveournewlyproposedtaskcanpositivelyimpactboththeresearchanddigitalartscommunities.
Whendetectingsmallandmidrangedatasetsizes,ourmotivationistoestablishanupperboundfor
membershipinferenceattacks,promotingprivacyawaredeploymentofLoRAmodels. Forlarger
datasetsizes,itwillincreasetheinformationontheresourcesrequirestosuccessfullytrainmodels.
Thisisoftenforresearchersthatneedtocollectexpensivedatasetsfornewfine-tuningtaskse.g.,[50]
and[8].
10 Conclusion
Weintroducedthenewtaskofdatasetsizerecoveryandproposedamethod,DSiRe,forlearninga
predictorforthistaskformodelsthatuseLoRAfine-tuning. Ourmethodshowedpromisingresults
onanewlarge-scaledatasetthatwereleased. Webelieveourworkcanprovideanupperboundfor
modelinversionandmembershipinferenceattacks. Itcanalsoprovidebothresearchersandstock
photographyownerswithaquantitativeanalyticestimatingthedatacostforfine-tuningmodels.
9References
[1] Dreambooth training readme. https://github.com/huggingface/diffusers/blob/
main/examples/dreambooth/README.md. Accessed: 01/02/24.
[2] OmriAvrahami,KfirAberman,OhadFried,DanielCohen-Or,andDaniLischinski. Break-a-
scene: Extractingmultipleconceptsfromasingleimage. InSIGGRAPHAsia2023Conference
Papers.AssociationforComputingMachinery,2023.
[3] OmriAvrahami,ThomasHayes,OranGafni,SonalGupta,YanivTaigman,DeviParikh,Dani
Lischinski, Ohad Fried, andXi Yin. Spatext: Spatio-textualrepresentation for controllable
imagegeneration. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),2023.
[4] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
[5] NicholasCarlini,SteveChien,MiladNasr,ShuangSong,AndreasTerzis,andFlorianTramer.
Membershipinferenceattacksfromfirstprinciples. In2022IEEESymposiumonSecurityand
Privacy(SP),pages1897–1914.IEEE,2022.
[6] DingfanChen,NingYu,YangZhang,andMarioFritz. Gan-leaks: Ataxonomyofmembership
inferenceattacksagainstgenerativemodels.InProceedingsofthe2020ACMSIGSACconference
oncomputerandcommunicationssecurity,pages343–362,2020.
[7] KanghyunChoi,DeokkiHong,NoseongPark,YoungsokKim,andJinhoLee.Qimera: Data-free
quantization with synthetic boundary supporting samples. Advances in Neural Information
ProcessingSystems,34:14835–14847,2021.
[8] XiaoliangDai,JiHou,Chih-YaoMa,SamTsai,JialiangWang,RuiWang,PeizhaoZhang,Simon
Vandenhende,XiaofangWang,AbhimanyuDubey,etal. Emu: Enhancingimagegeneration
modelsusingphotogenicneedlesinahaystack. arXivpreprintarXiv:2309.15807,2023.
[9] JiaDeng, WeiDong, RichardSocher, Li-JiaLi, KaiLi, andLiFei-Fei. Imagenet: Alarge-
scalehierarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpattern
recognition,pages248–255.Ieee,2009.
[10] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient
finetuningofquantizedllms. arXivpreprintarXiv:2305.14314,2023.
[11] JinhaoDuan, FeiKong, ShiqiWang, XiaoshuangShi, andKaidiXu. Arediffusionmodels
vulnerabletomembershipinferenceattacks? InInternationalConferenceonMachineLearning,
pages8717–8730.PMLR,2023.
[12] GongfanFang,KanyaMo,XinchaoWang,JieSong,ShitaoBei,HaofeiZhang,andMingliSong.
Upto100xfasterdata-freeknowledgedistillation. InProceedingsoftheAAAIConferenceon
ArtificialIntelligence,volume36,pages6597–6604,2022.
[13] MattFredrikson,SomeshJha,andThomasRistenpart. Modelinversionattacksthatexploit
confidenceinformationandbasiccountermeasures. InProceedingsofthe22ndACMSIGSAC
conferenceoncomputerandcommunicationssecurity,pages1322–1333,2015.
[14] NivHaim,GalVardi,GiladYehudai,OhadShamir,andMichalIrani. Reconstructingtraining
datafromtrainedneuralnetworks. AdvancesinNeuralInformationProcessingSystems,35:
22911–22924,2022.
[15] JamieHayes,LucaMelis,GeorgeDanezis,andEmilianoDeCristofaro. Logan: Membership
inferenceattacksagainstgenerativemodels. arXivpreprintarXiv:1705.07663,2017.
[16] JunxianHe,ChuntingZhou,XuezheMa,TaylorBerg-Kirkpatrick,andGrahamNeubig.Towards
a unified view of parameter-efficient transfer learning. ArXiv, abs/2110.04366, 2021. URL
https://api.semanticscholar.org/CorpusID:238583580.
10[17] ZechengHe,TianweiZhang,andRubyBLee. Modelinversionattacksagainstcollaborative
inference. InProceedingsofthe35thAnnualComputerSecurityApplicationsConference,pages
148–162,2019.
[18] BenjaminHilprecht, MartinHärterich, andDanielBernau. Montecarloandreconstruction
membershipinferenceattacksagainstgenerativemodels. ProceedingsonPrivacyEnhancing
Technologies,2019.
[19] EliahuHorwitz,JonathanKahana,andYedidHoshen. Recoveringthepre-fine-tuningweights
ofgenerativemodels. arXivpreprintarXiv:2402.10208,2024.
[20] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentinDeLaroussilhe,
AndreaGesmundo,MonaAttariyan,andSylvainGelly. Parameter-efficienttransferlearningfor
nlp. InInternationalConferenceonMachineLearning,pages2790–2799.PMLR,2019.
[21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
preprintarXiv:2106.09685,2021.
[22] Hailong Hu and Jun Pang. Membership inference of diffusion models. arXiv preprint
arXiv:2301.09956,2023.
[23] Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip S Yu, and Xuyun Zhang.
Membershipinferenceattacksonmachinelearning: Asurvey.ACMComputingSurveys(CSUR),
54(11s):1–37,2022.
[24] NamHyeon-Woo,MoonYe-Bin,andTae-HyunOh. Fedpara: Low-rankhadamardproductfor
communication-efficientfederatedlearning. arXivpreprintarXiv:2108.06098,2021.
[25] MatthewJagielski,MiladNasr,KatherineLee,ChristopherAChoquette-Choo,NicholasCarlini,
andFlorianTramer. Studentsparrottheirteachers: Membershipinferenceonmodeldistillation.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[26] MenglinJia,LumingTang,Bor-ChunChen,ClaireCardie,SergeBelongie,BharathHariharan,
andSer-NamLim. Visualprompttuning. InEuropeanConferenceonComputerVision,pages
709–727.Springer,2022.
[27] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-
conceptcustomizationoftext-to-imagediffusion. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages1931–1941,2023.
[28] BrianLester,RamiAl-Rfou,andNoahConstant. Thepowerofscaleforparameter-efficient
prompttuning. arXivpreprintarXiv:2104.08691,2021.
[29] XiangLisaLiandPercyLiang. Prefix-tuning: Optimizingcontinuouspromptsforgeneration.
arXivpreprintarXiv:2101.00190,2021.
[30] ZhikaiLi,MengjuanChen,JunruiXiao,andQingyiGu. Psaq-vitv2: Towardaccurateand
generaldata-freequantizationforvisiontransformers. IEEETransactionsonNeuralNetworks
andLearningSystems,2023.
[31] HaokunLiu,DerekTam,MohammedMuqeeth,JayMohta,TenghaoHuang,MohitBansal,and
ColinARaffel. Few-shotparameter-efficientfine-tuningisbetterandcheaperthanin-context
learning. AdvancesinNeuralInformationProcessingSystems,35:1950–1965,2022.
[32] XiaoLiu,YananZheng,ZhengxiaoDu,MingDing,YujieQian,ZhilinYang,andJieTang. Gpt
understands,too. AIOpen,2023.
[33] RaphaelGontĳoLopes,StefanoFenu,andThadStarner. Data-freeknowledgedistillationfor
deepneuralnetworks. arXivpreprintarXiv:1710.07535,2017.
[34] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and
Benjamin Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https:
//github.com/huggingface/peft,2022.
11[35] TomoyaMatsumoto,TakayukiMiura,andNaotoYanai. Membershipinferenceattacksagainst
diffusionmodels. In2023IEEESecurityandPrivacyWorkshops(SPW),pages77–83.IEEE,
2023.
[36] Ngoc-BaoNguyen,KeshigeyanChandrasegaran,MiladAbdollahzadeh,andNgai-ManCheung.
Re-thinking model inversion attacks against deep neural networks. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages16384–16393,2023.
[37] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,
pages8748–8763.PMLR,2021.
[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
High-resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages10684–10695,2022.
[39] NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman.
Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
22500–22510,2023.
[40] AlexandreSablayrolles, MatthĳsDouze, CordeliaSchmid, YannOllivier, andHervéJégou.
White-boxvsblack-box: Bayesoptimalstrategiesformembershipinference. InInternational
ConferenceonMachineLearning,pages5558–5567.PMLR,2019.
[41] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael
Backes. Ml-leaks: Modelanddataindependentmembershipinferenceattacksanddefenseson
machinelearningmodels. arXivpreprintarXiv:1806.01246,2018.
[42] AvitalShafran,ShmuelPeleg,andYedidHoshen. Membershipinferenceattacksareeasieron
difficultproblems. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages14820–14829,2021.
[43] RenrongShao,WeiZhang,JianhuaYin,andJunWang. Data-freeknowledgedistillationfor
fine-grainedvisualcategorization. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pages1515–1525,2023.
[44] RezaShokri,MarcoStronati,CongzhengSong,andVitalyShmatikov. Membershipinference
attacksagainstmachinelearningmodels. In2017IEEEsymposiumonsecurityandprivacy
(SP),pages3–18.IEEE,2017.
[45] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[46] PatrickvonPlaten,SurajPatil,AntonLozhkov,PedroCuenca,NathanLambert,KashifRasul,
MishigDavaadorj,DhruvNair,SayakPaul,WilliamBerman,YiyiXu,StevenLiu,andThomas
Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/
diffusers,2022.
[47] Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon
Kim. Multitaskprompttuningenablesparameter-efficienttransferlearning. arXivpreprint
arXiv:2303.02861,2023.
[48] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu.
Prolificdreamer: High-fidelityanddiversetext-to-3dgenerationwithvariationalscoredistilla-
tion. ArXiv,abs/2305.16213,2023. URLhttps://api.semanticscholar.org/CorpusID:
258887357.
[49] LaurenWatson,ChuanGuo,GrahamCormode,andAlexSablayrolles. Ontheimportanceof
difficultycalibrationinmembershipinferenceattacks. arXivpreprintarXiv:2111.08440,2021.
12[50] DanielWinter,MatanCohen,ShlomiFruchter,YaelPritch,AlexRav-Acha,andYedidHoshen.
Objectdrop: Bootstrappingcounterfactualsforphotorealisticobjectremovalandinsertion,2024.
[51] ShoukaiXu,HaokunLi,BohanZhuang,JingLiu,JiezhangCao,ChuangrunLiang,andMingkui
Tan. Generativelow-bitwidthdatafreequantization. InComputerVision–ECCV2020: 16th
EuropeanConference, Glasgow, UK,August23–28, 2020, Proceedings, PartXII16, pages
1–17.Springer,2020.
[52] Ziqi Yang, Jiyi Zhang, Ee-Chien Chang, and Zhenkai Liang. Neural network inversion in
adversarial setting via background knowledge alignment. In Proceedings of the 2019 ACM
SIGSACConferenceonComputerandCommunicationsSecurity,pages225–240,2019.
[53] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,Anwen
Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language
modelswithmultimodality. arXivpreprintarXiv:2304.14178,2023.
[54] SamuelYeom,IreneGiacomelli,MattFredrikson,andSomeshJha. Privacyriskinmachine
learning: Analyzing the connection to overfitting. In 2018 IEEE 31st computer security
foundationssymposium(CSF),pages268–282.IEEE,2018.
[55] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem,
NirajKJha,andJanKautz. Dreamingtodistill: Data-freeknowledgetransferviadeepinversion.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages8715–8724,2020.
[56] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander
Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
18123–18133,2022.
[57] LinZhang,LiShen,LiangDing,DachengTao,andLing-YuDuan.Fine-tuningglobalmodelvia
data-freeknowledgedistillationfornon-iidfederatedlearning. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages10174–10183,2022.
[58] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-image
diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer
Vision(ICCV),pages3836–3847,October2023.
[59] QingruZhang,MinshuoChen,AlexanderBukharin,PengchengHe,YuCheng,WeizhuChen,
andTuoZhao. Adaptivebudgetallocationforparameter-efficientfine-tuning. arXivpreprint
arXiv:2303.10512,2023.
[60] ZhuangdiZhu,JunyuanHong,andJiayuZhou.Data-freeknowledgedistillationforheterogeneous
federated learning. In International conference on machine learning, pages 12878–12889.
PMLR,2021.
1311 Appendix
11.1 AdditionalAblationStudies
Weprovidemoreablationstudiesofourmethod. Specifically,wetestthetrainingsteps,batchsize,
usedclassifiertypeandusedLoRAmatrices.
11.2 RobustnesstoLoRAHyper-Parameters
BatchSize. Weablatethebatchsize,resultsatshowninTab. 7. Despitethechangeinbatchsize,
DSiRedemonstratesrobustperformance,achievingaMAEscoreof1.94comparedtotheoriginal
1.48. Additionally,theaccuracyonlydecreasesbylessthan5%,indicatingthatourmethodmaintains
comparableeffectivenessevenwithdifferentbatchsizes.
Table7: DSiReperformanceusingdifferentLoRAhyper-parameters. Mediumdatarange
Ablation MAE↓ MAPE(%)↓ Acc(%)↑
Batchsize 1.94±0.26 9.35±1.34 81.50±2.55
Baseline 1.48±0.21 3.97±0.73 86.10±1.99
TrainingSteps. TotrainDSiRe,wefirstfine-tuneasetofLoRAmodels. Thesemodelsfollow
acertainrecipe,withaspecificamountoftrainingsteps. Toevaluaterobustness,wetestedDSiRe
onmodelsfine-tunedatdifferentsteps,with1200stepsasourbaseline. AsshowninTab8,DSiRe
consistintlyachievescomparableresultsacrossdifferentfintuningsteps. e.g. theMAEscoreranges
from2.43at300stepsto1.40at1400steps,withaccuracyvariationswithin10%.
Table8: DSiReperformanceondifferentcheckpointsofStableDiffusion1.5rank16range1−50
#Steps MAE↓ MAPE(%)↓ Acc(%)↑
300 2.43±0.20 6.82±0.78 77.90±1.49
400 2.39±0.20 6.72±0.76 78.38±1.49
500 2.05±0.15 5.55±0.59 81.33±1.60
600 1.86±0.10 4.59±0.34 82.76±0.86
700 1.89±0.21 5.13±0.77 82.00±2.01
800 1.71±0.29 4.59±0.89 83.67±2.68
900 1.60±0.22 4.21±0.69 85.14±2.04
1000 1.62±0.21 4.69±0.70 85.10±1.77
1100 1.58±0.19 4.50±0.90 84.48±1.32
1200 1.48±0.21 3.97±0.73 86.10±1.99
1300 1.46±0.15 3.84±0.51 86.29±1.55
1400 1.40±0.20 3.73±0.76 86.76±2.08
11.3 ChoiceofLoRAMatrices
SeeinginSec. 3thatnotalllayersaresimilarinbehavior,wetesttoseeifdifferentLoRAmatrices
alsocapturedifferentinformation. InTab. 9,wefindthatindeeddifferentLoRAmatricescapture
differentinformation,andleadtosubstantiallyotherperformances. Unsurprisingly,wealsofindthat
usingalltheLoRAmatricescombinedyieldsthebestresult.
Table9: DSiReperformanceondifferentlayersofLoRAoftheUNetinStableDiffusion1.5,range
1−50:
#LayerType MAE ↓ MAPE(%)↓ Acc(%)
A 1.9±0.29 5.63±1.26 82.52±2.20
B 1.57±0.19 4.07±0.65 84.90±2.09
BA 1.61±0.16 4.22±0.47 85.00±1.45
fullmodel 1.48±0.21 3.97±0.73 86.10±1.99
1412 HigherDataRegimesAnalysis
To better understand the results on higher data regimes we provide here the confusion matrix of
DSiReusing1−1000trainingsamples. Wecanseethatmostoftheerrorsareinlargerdataclasses.
Figure6: DSiReConfusionmatrixinHighdataregime. IllustratingDSiRe’saccuracyintherange
datasize(1−1000)forasingleexperiment,showingthatmostpredictionsarecorrectornearmisses,
highlightingtheDSiRe’sprecisionindatasetsizerecovery.
12.1 Implementationsdetails
12.1.1 LoRA-WiSE.
wenowelaboratetheimplementationsdetailsoftheLoRA-WiSEbenchdataset.
Datasets in all ranges 1 − 6, 1 − 50, 1 − 1000. As the Pre-Ft models we use
runwayml/stable-diffusion-v1-5 and stabilityai/stable-diffusion-2 [38]. We fine-
tunethemodelsusingthePEFTlibrary[34]. Weusethescripttrain_dreambooth_lora.py[39]
withthediffuserslibrary[46]. weusethestandardrecipetofine-tunethemodelsinallranges[1]see
tab11. weusebatchsize8forrange1-1000forcomputationalresourcesand1000trainingstep. in
theablationswedon’tchangeanyhyper-parameterexcepttheablateone.
Eachmodeltookapproximately30-50minutestofine-tune. WeusedGPUswith16-21GBofRAM,
suchastheNVIDIARTXA5000. TheDSiReprocess,however,doesnotrequireGPUsandcanrun
onCPUs.
ExperimentSettings. InadditiontotheexperimentsettingsdescribedinSection6,weusedthe
followingconfigurationsforourmodels:
-Formodelsintheranges1-6and1-50,weusedthecheckpointatiteration1200. -Formodelsinthe
range1-1000,weusedthecheckpointatiteration1000.
Weusedafixedseedof42tosplitthetrainandtestdataforeveryexperiment.
Layerweighmatrices InlinewithouranalysisseeSec.3,givennexampleweightsforeachA ,B
i i
wewishtobuildaseparateclassifierforeachone. Knowingtherelationbetweenthesingularvalues
andthedatasetsize,wedecomposeeachmatrixusingthesingularvaluedecomposition(SVD),and
usetheorderedsetofsingularvaluesasfeaturesforourclassifiers. Formally,wenotethesingular
15Table10: ranges1-6and1-50 Table 11: range 1-1000 Hyper-
parameters
Name Value
Name Value
lora_rank(r) r
lr 1e−4 lora_rank(r) 32
batch_size 1 lr 1e−4
gradient_accumulation_steps 1 batch_size 8
learning_rate_scheduler Constant gradient_accumulation_steps 1
training_steps 1400 learning_rate_scheduler Constant
warmup_ratio 0 training_steps 1000
imagenet[9] warmup_ratio 0
dataset
concept101[27] dataset imagenet
seeds 0 seeds 0
values of A as Σ and the singular values of B as Σ . We include the singular values of
ij Aij ij Bij
B ·A denotedasΣ . Additionally,ourobservationsindicatethattheproductB ·A also
ij ij Bij·Aij i i
providesusefulinformationfordatasizerecovery. Thus,foreachLoRAmatrix,weobtainadataset
withnsamples,whereeachsampleisavectorofsingularvaluesΣ ,pairedwithacorresponding
ij
labely . OurmethodthentrainsthreeseparatekNN-classifierswithK =1foreachlayerover(i)A
j i
(ii)B and(iii)B A . Atinferencetime,thepredictionsfromallclassifiersaremergedbymajority
i i i
voting.
16