IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and
Toxicity Types for Indonesian Language
LuckySusanto*,1,MusaIzzanardiWijanarko*,1,PrasetiaAnugrahPratama2
TraciHong3,IkaIdris1,AlhamFikriAji1,4,DerryWijaya1
*EqualContribution
1MonashUniversity,2IndependentResearcher,3BostonUniversity,4MBZUAI
Abstract
Hate speech poses a significant threat to so-
cialharmony. Overthepasttwoyears,Indone-
sia has seen a ten-fold increase in the online
hatespeechratio,underscoringtheurgentneed
foreffectivedetectionmechanisms. However,
progressishinderedbythelimitedavailability
oflabeleddataforIndonesiantexts. Thecondi-
tionisevenworseformarginalizedminorities,
such as Shia, LGBTQ, and other ethnic mi-
Figure1: Theperceptionofhatespeechisinfluenced
noritiesbecausehatespeechisunderreported
bytheidentityofapersonoragroup. Atextmaybe
andlessunderstoodbydetectiontools. Further-
consideredhatespeechbyonegroupofpeople,while
more, the lack of accommodation for subjec-
anothergroupmaynotviewitassuch. Moredivisive
tivityincurrentdatasetscompoundsthisissue.
exampleavailableinappendixA.
Toaddressthis,weintroduceIndoToxic2024,a
comprehensiveIndonesianhatespeechandtox-
hatespeechdetectionsystem. However,manysig-
icityclassificationdataset. Comprising43,692
nificantchallengestothedevelopmentofthissys-
entriesannotatedby19diverseindividuals,the
tem exist. One of these challenges is the lack of
dataset focuses on texts targeting vulnerable
comprehensiveandup-to-datedata. Theexisting
groups in Indonesia, specifically during the
hottestpoliticaleventinthecountry: thepres- Indonesian datasets (Alfina et al., 2017; Ibrohim
idential election. We establish baselines for andBudi,2018)areconsiderablydatedandconsist
seven binary classification tasks, achieving a onlyofaround3,000labeledIndonesiantexts. Fur-
macro-F1scoreof0.78withaBERTmodel(In- thermore,thesedatasetslackcrucialinformation,
doBERTweet)fine-tunedforhatespeechclas-
suchasthedemographicinformationofannotators,
sification. Furthermore,wedemonstratehow
whichiskeyintrainingsystemssincesubjectivity
incorporatingdemographicinformationcanen-
isinherentinhatespeechannotations(Fleisigetal.,
hancethezero-shotperformanceofthelarge
2024)(Figure1). Subjectivityintheannotationof
languagemodel,gpt-3.5-turbo. However,we
also caution that an overemphasis on demo- textoftenoccurswithlatentcontent(e.g.,sarcasm)
graphicinformationcannegativelyimpactthe which contains "under the surface" information
fine-tunedmodelperformanceduetodatafrag- thatrequireshumanannotators’mentalschemeto
mentation.
deciphertheirmeaning. Incontrast,manifestcon-
tent(e.g.,howmanywordsareinthesentence)is
1 Introduction
"surfaceinformation"thatrequiresminimalinter-
IntherapidlyevolvingdigitallandscapeofIndone- pretation (Lombard et al., 2002). Arguably, the
sia,adisturbingten-foldincreaseinhatespeech morepertinentinformationisfoundinlatentcon-
ratio has been observed in just two years (CSIS, tent. However, human annotator’s judgments are
2022; AJI, 2024). Left alone, this surge threat- affectedbynotonlytheirbackground,geography,
enssocialharmony(Williamsetal.,2019),andis andpersonalexperiences(Armstrongetal.,1997)
especiallyharmfultominoritygroups(Alexandra butalsobytheirperceivedstatusinrelationtoother
andSatria,2023),becauseitcouldleadtosocietal humancodersworkingontheresearch(Campbell
polarization (Unlu and Kotonen, 2024). One po- etal.,2013).
tentialsolutioncomesintheformofanautomated As a first step to tackle this issue, we release
4202
nuJ
72
]LC.sc[
1v94391.6042:viXraIndoToxic20241,ahatespeechdatasetannotated 2.2 DetectingHateSpeech
acrossvariousdemographics,whereeachentryis
Deeplearningtechniqueshavebeenpromisingin
accompaniedbyten-dimensionaldemographicin-
automatic hate speech detection (cjadams et al.,
formation. Unlikemosthatespeechdatasetsthat
2017;Dasetal.,2021). However,thesetechniques
aresingle-labeled,IndoToxic2024’slabelsarepre-
still fall short in real-world scenarios due to the
served per annotator. This opens up the poten-
evolvingnatureofhatespeechandthecomplexity
tialforstudyingsubjectivityinannotations. This
ofthetask. Recently,Fleisigetal.(2024)demon-
datasetconsistsof43,692entriesannotatedby19
strated that incorporating demographic informa-
diverseannotators,aimedatclassifyinghatespeech
tionofannotatorscanimprovemodelperformance.
and the types of toxic behaviors. It is a human-
Fleisig et al. (2024) approaches the problem as a
annotatedcollection,assembledbygatheringposts
regressioninstead ofaclassification, wheretexts
from various social media platforms using key-
arelabeledbasedontheseverityoftheirtoxicity.
wordsrelatedtoIndonesianvulnerablegroups. Our
contributionsarethree-fold:
2.3 AvailableHateSpeechDatasets
• Creation of IndoToxic2024 Dataset, en-
ablingthecreationofbetterhatespeechdetec- Initially, Indonesian hate speech datasets (Alfina
tion systems (IndoBERTweet fine-tuned for et al., 2017), consisted of texts with a binary la-
hatespeechclassification),specificallyinthe belindicatingisahatespeechornot. Asthefield
Indonesianlanguage. evolved,datasetsbegantoincorporatedifferentlev-
• Exploring the Role of Demographic In- elsoftoxicity. Forinstance,thedatasetsbyIbrohim
formation in Hate Speech Classification, and Budi (2018) and Mathew et al. (2022) intro-
demonstratingthatgpt-3.5-turbo’shatespeech duced varying degrees of toxicity, with the latter
classificationperformancecanbesignificantly focusingonexplainablehatespeechclassification.
improved when provided with the demo- Following this trend, datasets started to include
graphic information of the annotator. This typesofhatespeech,asseeninthechallengerunby
insight suggests that demographic informa- cjadamsetal.(2017). Morerecently,demographic
tion can be a valuable addition to improve informationhasbeenincorporatedintohatespeech
modelperformance. datasets. Toourknowledge,theearliestdatasetto
• Analyzing the Impact of Excessive Demo- containmorethanjusttextandlabelswascreated
graphicInformationonFine-TunedModel by Kumar et al. (2021) and was recently utilized
Performance, providing an extensive anal- byFleisigetal.(2024). Altough,it’simportantto
ysis that shows an addition of demographic note that most modern datasets focus on the En-
informationcanleadtofragmentationofthe glishlanguage. AsidefromtheprivateCSIS(2022)
dataset (fewer training data in each demo- dataset, we have not found new Indonesian hate
graphic,thusworseperformance). speechdatasetsinthepastfiveyears.
2 Background&RelatedWork
2.4 DatasetsWithDemographicInformation
2.1 WhatisHateSpeech?
Datasets incorporating demographic information
Hate speech definitions evolved over time. Ini- are typically utilized for tasks associated with an
tiallydefinedaslanguageintendedtodemeanoth- individual’s behavior or circumstances. More of-
ers (Delgado, 1982), it has expanded to include ten than not, this demographic data is employed
public speech or writing inciting hatred towards inpredictingaspectssuchasinsurancepremiums
demographicgroups(Greenawalt,1989;Nations, (Patil et al., 2024) or an individual’s purchasing
2023). In this work, we adopt the definition by power(Olodoetal.,2022). However,considering
Indonesia’sNationalHumanRightsCommission, the subjectivity inherent in hate speech, we only
whichincludesanycommunicationmotivatedby findasinglehatespeechdatasetthatincludesde-
hatredagainstpeoplebasedontheiridentities,in- mographicinformation(Kumaretal.,2021). Most
tendingtoinciteviolence,death,andsocialunrest hatespeechdatasetsonlygoasfarasprovidingan-
(ParamadinaandMafindo,2023). notatorIDs,asinthecaseofMathewetal.(2022),
without any demographic information of the text
1Thecodeisavailableathttps://github.com/izzako/
IndoToxic2024/tree/main annotator.3 DatasetCreation Demographic Group Count
Disability WithDisability 3
3.1 DataCollection
NoDisability 16
Weobtainourtextdatafromsomeofthepopular Ethnicity Chinese 3
Indigeneous 15
socialmediaplatformsinIndonesia(Kemp,2023)
Other 1
includingFacebook,Instagram,andTwitter(orX). Religion Islam 9
Additionally, we also retrieve articles from Cek- ChristianorCatholics 4
HinduismorBuddhism 3
Fakta2,amovementthatfocusesonclarifyingmis-
AhmadiyyaorShia 2
informationthatspreadsontheinternet. TraditionalBeliefs 1
Gender Male 6
Different tools are utilized to gather data from
Female 13
platforms. We use Brandwatch (2021) to obtain
LGBTQ+ Yes 1
tweets, replies, and quotes from X, Crowdtangle No 18
Age 18-24 9
(Team,2024)toobtainpostsfromInstagramand
25-34 5
Facebook. Additionally,weretrievedarticlesfrom 35-44 3
Cek Fakta3, a fact-checking collaboration across 45-54 2
Education Master’sDegree 3
online media and fact-checking organizations in
Bachelor’sDegree 7
thecountry. WecollectdatafromSeptember2023 Associate’sDegree 2
to January 2024, following the 2024 Indonesian HighSchoolDegree 7
JobStatus Employed 9
presidentialelectiontimeline(detikcom,2022),as
CollegeStudent 8
hate speech was found to intensify in Indonesia HousewifeorUnemployed 2
PresidentialVote Candidateno.1 4
during a similar election in 2019 (Iswatiningsih
Candidateno.2 7
etal.,2019). Candidateno.3 5
We obtain data using keywords that were pre- UnknownorAbstain 3
viously used to express hate toward vulnerable
Table1: Thedemographicbackgroundofthe19annota-
groupsinIndonesiantexts,compiledbasedonvari-
torsincoarser-granularity. Theethnicitydemographic
oussourcessuchasliteratureresearch,discussions
informationthatwehavearemorefine-grainedwhere
withexperts,andafocusgroupdiscussion(FGD)
IndigenousgroupherereferstoseveralethnicIndone-
withrepresentativesfromvulnerablecommunities4.
siangroups: Java,Minang,Sunda,Bali,Dayak,Bugis,
WeprovidethekeywordsinAppendixB.Thedata etc. with1-2annotatorsperethnicity.
wasthensampledforannotationbycoders(i.e. an-
plyingthecodebookonthedatawasdeterminedby
notatorsfromdiversedemographicbackgrounds).
calculatinginter-coderreliability(ICR).HighICR
3.2 RecruitmentandValidationMetrics scoremeansthattheannotatorsconsistentlycatego-
rizedthetextsimilarly. AlthoughCohen’sKappais
18peoplefromvariousdemographicbackgrounds
apopularinter-coderreliabilitytestusedinseveral
and 1 from our research team were recruited to
hatespeechworks(AldreabiandBlackburn,2024;
annotatethedata. FromtheFGD,eachofthevul-
Ayele et al., 2023; Vo et al., 2024), Gwet’s AC1
nerable groups proposed their representatives to
has been suggested as a more stable metric that
annotate the data, ensuring representations from
isrobustagainstclassimbalance(Ohyama,2021;
each group. Table 1 gives us a coarse-grained
Wongpakaranetal.,2013). Thisisespeciallyrele-
overviewofthisdiversity. Subsequently,contracts
vantforsocialmediaplatformswithahighvolume
weredrafted,andeachannotatorwascompensated
ofdatawherethemajorityisnon-hatespeech.
with1.5millionIDRforevery1,000textstheyan-
notated. Forcomparison,anaveragemonthlywage
3.3 AnnotationInstrument
in Indonesia across sectors is 3.5 million IDR in
2024(BPS-Statistics,2024). Our goal is to capture text that contains toxicity,
Inter-coderReliabilityMetrics Coderswere be it explicit (manifest content such as inclusion
trainedonacodebookandtheiragreementonap- ofoffensivewords)orimplicit(latentcontentsuch
as sarcasm) (Krippendorff, 2018). This nuanced
2https://cekfakta.com/
3https://mafindo.or.id and contextual hate speech has not yet been con-
4identifiedminoritygroupsinIndonesiathathavebeen fronted in a consistent and unified manner in the
thetargetofhatespeechinpreviouselectionsincludingdis-
NLPcommunity(ElSheriefetal.,2021).
ability,LGBTQ+,Chinese,Ahmadiyya,Shia,Catholics,and
Christiangroups. Basedontheliteraturereviewofhatespeechanddiscussions with vulnerable groups in Indonesia, We can further explore how different demo-
wecreateacodebook(AppendixC)asaguidefor graphic groups annotate the texts. When we ag-
annotatorstoidentifytextashatespeechwhencer- gregatethedataset,weseethatthedistributionof
taincriteriaaremet(Sellars,2016,p.25-30). The toxicitylabelsdiffersbetweengenders. Malesla-
codebookhelpsannotatorsrecognizetexttypically beled19.3%oftheirdataastoxic,whereasfemales
seenashatespeechandtextthatseemsnormalbut labeled13%. Weexplorethistopicinmoredepth
isindeedharmfultoaspecificvulnerablegroup. inthenextsection.
3.4 AnnotationProcess 4 DatasetAnalysis: Existenceof
Subjectivity
Theannotationprocessisdividedintotwostages:
thetrainingphaseandthemainannotationphase.
4.1 SubjectivityinHateSpeechAnnotations
Theannotatorsweretrainedonthecodebookdur-
TheresearchbyFleisigetal.(2024)indicatesthat
ing the training phase. They were instructed to
combining demographic data with potential hate
identifywhetherthetextcontainshatespeech(or
speech improves toxicity prediction models. Ku-
toxicity). If yes, they were asked to identify the
maretal.(2021)furtheremphasizesthesubjective
types of the hate speech (i.e., whether it is an in-
nature of toxic text, as shown by annotator dis-
sult, threat, profanity, identity attack, or sexually
agreements. Ourstudyexpandsontheseinsights
explicittext).
byexploringthetopicfromthreenewperspectives.
Duringthefirsttrainingsession,theannotators
weregiven100randomly-sampledtexttocode(i.e., ThroughDistributionAssumptions Inthean-
annotate),butICRwasnotmet. Hence,asecond notationprocess,textsarerandomlyassignedwith-
(with another 100 randomly-sampled text) and a outconsideringtheannotator’sgender,education,
third (with another 249 randomly-sampled text) disability,ordomicile. Weusechi-squaretesting
training session were held to further clarify the toverifythenullhypothesis: “Iftextsarerandomly
codebookandresolveanyconfusion. Asatisfactory assignedandthere’snosubjectivity,eachannotator
ICRscorewasmetafterthethirdtrainingsession: shouldreceiveasimilarproportionofhatespeech
aGwet’sAC1scoreof0.61forthetoxicitylabel. texts”. However,Figure2showsthatthenullhy-
Following Quantitative Content Analysis (QCA) pothesis is only valid for the “domicile” group.
incommunicationresearch,whichisacommonly Therefore,werejectthenullhypothesis,confirm-
usedmethodtoderivereplicableandmeaningful ingsubjectivityinmostdemographicgroups. It’s
inferences from texts (Krippendorff, 2018), once important to note that some texts were assigned
theICRwasmet,theannotatorscontinuedtocode basedonthereligionorethnicityoftheannotator,
more texts independently in the main annotation sothesegroupsareexcludedfromthisanalysis.
phase. Ourfinaldataiscomprisedof43,692texts
ThroughICRScore WecalculatetheICRscore
thatwereannotatedfromthethreesessionsoftrain-
ingandthemainannotationphase5. within each demographic group (within-group
ICRscore)andbetweentwodemographicgroups
3.5 DatasetProperties
(between-group ICR score) (refer to Appendix F
Out of 43,692 texts in IndoToxic2024, 6,894 are for more details). It’s generally assumed that the
labeledastoxic. FromTable2,wecanobservethat between-groupICRscoreswouldbelowerthanthe
almosthalfofthetoxictextsareinsults. within-groupICRscoresduetosubjectivitywithin
thegroup. However,ourresultsmostlycontradict
ToxicityTypes #Texts
this assumption. For example, while the within-
Insults 3140
Threat/IncitementtoViolence 2837 groupICRscoreforfemalesis0.61andformales
ProfanityorObscenity 1271 is0.54,thebetween-groupICRscoreis0.58,which
IdentityAttack 1061
isnotlowerthanbothwithin-groupscores. Further
SexuallyExplicit 224
analysisrevealstheroleofintersectionalidentity.
Table2:Numberoftoxictextslabeledwithtypes.These Forinstance,ahighICRscorecontributoramong
categoriesarenotmutuallyexclusive,sinceatoxiccom- thefemale-malepairsbelongstoothersimilarde-
mentcouldexhibitmorethanonetypeofhatespeech.
mographic groups (both are disabled annotators
5Duetothelongannotationprocessandthecomplexity
andidentifyasChristians). Thissuggeststhatde-
andhumantollofthetask,someannotatorscompleteonly
partsoftheirassignmentsduringthemainannotationphase. mographicsareintersectingfactors,notmutuallyFigure2: Subjectivityaffectstheannotationprocess. Expectedratioofhatespeechvs. non-hatespeechlabeled
dataagainsttheratioofeachdemographicgroup’sannotationsusingchi-squaretesting. Bluecolorsindicatealower
p-value(moresignificantdifferencetotheexpectedratio),andredcolorsindicateahigherp-value(lesssignificant
differencetotheexpectedratio).
Teston 4.2 TextswithDifferingAnnotations
Trainon
Non-Islam Islam
Accompanying previous results, we rank texts
Non-Islam 0.605 0.597
based on their divisiveness. A text is highly di-
Islam 0.628 0.66
visive when groups of annotators in the same de-
Table3: Subjectivityaffectsperformanceofmodels. mographiccategorylargelydisagreeontheiranno-
F1-scoresofIndoBERTweet,trainedonIslamandNon- tations. For example, the text in Figure 1 where
Islamannotators’labels,using5-foldcross-validation Christian annotators unanimously agree that the
when the training and testing data are from the same
textishatespeechwhilethenon-Christiansunani-
group’sannotations(i.e.,thediagonals). Itcanbeseen
mouslydisagree. Moreexamplescanbefoundin
thattheperformanceoftrainingononegroup’sannota-
appendixA.Thetextillustrateshowtheinterpre-
tionsandtestingonanotherisworsethantrainingand
tation of hate speech can vary depending on the
testingonthesamegroup’sannotations.
annotator’sdemography,inthiscasetheirreligion.
exclusiveaspects. However,itiscrucialtoconsiderthewholepicture.
Thereareinstanceswhereatextappearstobeon
ThroughModelingResults Fine-tuningamodel thetopicofaspecificdemographiccategory,such
usingasubjectiveorbiaseddatasetcanresultina asfemale,yetannotatorsaresplitnotalonggender
modelthatinheritsthatsubjectivity(Senguptaand butalongtheir(last)educationlevel.
Srivastava, 2022). To ensure the availability and
4.3 HowTopicAffectsSubjectivity
quantity of the training dataset, we use a coarser
granularityforsomeofourdemographiccategories.
Forinstance,weuseagegroupsinsteadofspecific
agenumbersfortheagedemography,andacoarser
groupingofIslamandNon-Islamforthereligious
demography. We then fine-tune IndoBERTweet
model(Kotoetal.,2021)bylimitingthetraining
datatotextsthatareannotatedbythesamegroup
(e.g., Islam), and test on texts that are annotated
by the other group in the demographic category Figure3: Topicscanactasastabilizeroracatalyst.
(e.g.,Non-Islam). Asaresult,amodelfine-tuned Sometopicscanbeeasierforacertaindemographyto
onNon-Islamannotators’labelstendstoperform annotate. Forexample,textsrelatingtoShiaareeasier
worsewhentestedonIslamannotators’labelscom- forIslamicgroupofpeopletoannotatewhileharderfor
non-Islampeopletoannotate.
pared to when tested on their own labels (using
5-foldcross-validation)andviceversa(seeTable Wetrainedahatespeechclassifierontextsanno-
3). This trend holds true for other demographic tatedbyeachdemographicgroupandthenchecked
categoriessuchasdisability,ethnicity,andreligion. howwelleachmodeldidondifferenttopics. Top-
The result indicates that for some demographic ics of a text are target demographics of the text,
groups,theremayexistotherdemographicgroups identifiedbasedonkeywordsmentionedinthetext.
thatannotatethetextsdifferentlyduetodifferences Thekeywords-to-topicsmappingisinappendixD.
intheiridentities. An example of how models performed acrosstopics is shown in Figure 3. One model was with a focus on Indonesia and SouthEast Asian
trainedonadatasetannotatedbypeoplewithnon- (SEA)languagesrespectively.
Muslim religion ("Non-Islam"), one was trained Withoutanydemographicortopicinformation,
onadatasetannotatedbypeoplewithIslamSunni IndoBERTweet,fine-tunedonlyonIndoToxic2024,
religion("Islam-Sunni"),andthethirdonetrained has the best performance for hate speech classi-
on a dataset annotated by people with "Shia" or fication with a macro-F1 of 0.718 (5-fold cross-
"Ahmadiyya"astheirreligion("Islam-Other"). For validation). We also report the zero-shot perfor-
topicsrelatingtoShiaorAhmadiyya,bothofthe mance of the other models in Table 5a. The per-
"Islam"modelsperformsimilarlywhilethemodel formance of IndoBERTweet, after incorporating
trained on "Non-Islam" dataset perform relative topicand/ordemographicinformation,isdetailed
poorly. However,fortopicsrelatingtoLGBTQ+, inTable5b.
orChinese,there’sajumpinperformanceforthe To incorporate the topic (t) and demographic
"Non-Islam"model. Thissuggeststhatthere’sless information(d)alongwiththetexts(w),theinput
disagreement among people of the same group isformattedasfollows:
when they understand or relate to the topics, and
higherdisagreementwhentheyareunfamiliarwith d 1...d n t 1...t n[SEP]w 1...w n
thetopics.
Forinstance,acompleteinputgiventoIndoBER-
5 BenchmarkResults,Experiments,and Tweetmightbe: “Readerinformation: Chineseeth-
Analysis nicity, Christian, Male, Millennial. Topic: Chris-
tian. [SEP][TEXT]”.
5.1 BaselineModelPerformanceperTask
To use gpt-3.5-turbo and SeaLLM-7B-v2.5, a
We benchmark 6 classification tasks to identify: custompromptisused,withitssystempromptset
whether a text contains hate speech, and the five to: “You may only respond with either a 0 or a
typesofhatespeech. Wefine-tuneIndoBERTweet 1”. Wealsoenforcethestatementintheuserinput,
(Koto et al., 2021) on our dataset. To enhance whichfollowsthispattern: “IstheIndonesiantext
theperformanceofourbaselines,wemergedour belowahatespeech[DemographicInformation]?
datasetwiththatofCSIS(2022)andgeneratedsyn- If yes, respond with 1, otherwise respond with 0.
thetichatespeechdatausinggpt-3.5-turbo(Brown [TEXT]”.
etal.,2020)through10-shotgeneration. Thissyn- Eachtextinourdatasetisaccompaniedbydemo-
theticdatawasonlyusedfortraining. Thestratified graphic information about the annotator. This in-
10-foldperformanceanddatabreakdownforeach formationincludesdisabilitystatus,domicile,eth-
taskarereportedinTable4. Thepromptgivento nicity,gender,generation(agegroup),partofthe
gpt-3.5-turbotogeneratesynthetictextsisavailable LGBTQ+ communities, last education, religion,
inappendixE. workstatus,andpoliticalleaning. Ourexperiments
For most tasks, our baseline achieved a macro are divided into three parts: one with no demo-
F1-scoreabove0.7. However,forthe"incitement graphicinformationprovided,onewithalldemo-
toviolence"classificationtask,weonlyachieveda graphicinformationprovided,andonewithonlya
macroF1-scoreof0.53. It’simportanttonotethat singlepieceofdemographicinformationprovided,
thedatareportedinTable4wereusedtoachieve totaling 12 experiments. These experiments are
thebestbaselineperformancereportedhere. conducted on three models: IndoBERTweet, In-
doBERTweet with topic information given, and
5.2 IncorporatingDemographicandTopic
gpt-3.5-turbo. The performance of these models
Information
canbefoundinFigure4.
Fleisigetal.(2024)showsthatincorporatingmeta- Our findings indicate that for IndoBERTweet
datasuchasdemographicinformationandsurvey models,providingdemographicinformationdoes
resultscanincreaseahatespeechclassifier’sperfor- not yield any significant improvement. In fact, it
mance. Forthefollowingsetofexperiments,were- may even negatively impact the model’s perfor-
portontheperformanceofthreemodels: IndoBER- mance. However,inallinstances,theperformance
Tweet (Koto et al., 2021), gpt-3.5-turbo (Brown ofgpt-3.5-turboimprovedwhendemographicin-
etal.,2020),andSeaLLM-7B-v2.5(Nguyenetal., formationwasprovided. Wehypothesizethatthe
2023). IndoBERTweetandSeaLLMarepre-trained additionalinformationaddsalevelofcomplexityMetrics DataStatistic
ClassificationTask Accuracy F1 Precision Recall Our-0 Our-1 CSIS-0 CSIS-1 Synthetic-1
HateSpeech 0.89 0.78 0.80 0.76 4014 1338 21125 7112 1325
IdentityAttack 0.75 0.80 0.74 0.88 461 648 - - 228
IncitementtoViolence 0.77 0.53 0.55 0.52 345 115 945 315 890
Insult 0.79 0.85 0.81 0.88 705 423 149 1142 785
ProfanityorObscenity 0.81 0.70 0.70 0.72 824 373 - - 370
SexualExplicit 0.91 0.80 0.88 0.77 123 41 - - 82
Table4: PerformanceofIndoBERTweetacrossvariousbinaryclassificationtasks,utilizingacombinationofour
annotateddata,datafromCSIS,andsyntheticallygenerateddataviaGPT-3.5-turbo. Theterm-xrepresentsthe
quantityofdataassociatedwiththatlabelforaspecifictask. Oursamplingstrategyensuresthatthequantityof
0-labeled(e.g.,non-hatespeech)dataisatmostthreetimesthatof1-labeled(e.g.,hatespeech)data.
Information F1
Baseline 0.718
Model F1
+Demographic 0.672
IndoBERTweet 0.718 +Topic 0.755
GPT-3.5-turbo 0.627 +Topic&Demo 0.709
SeaLLM-7B-v2.5 0.517
(b) IndoBERTweet perfor-
(a)Baselineperformance mancewithaugmentedinput.
Table5: IndoBERTweet,whengivenonlytopicinfor-
mation, performs best based on the macro-F1 metric.
ModelsaretrainedusingonlyIndoToxic2024dataset.
Figure5: Theeffect(∆F1)ofgivingdomicileinforma-
tiontogpt-3.5-turboonitshatespeechtextclassification
comparedtonotgivingitanyinformation.
whenprovidedwiththeannotator’sdomicile,with-
Figure 4: Comparison of macro-F1 Score from In-
out any fine-tuning. For instance, it achieved a
doBERTweet (dark green/left bars), IndoBERTweet
withtopic(lightgreen/middlebars),andgpt-3.5-turbo 0.47 macro-F1 improvement when the annotator
(orange/right bars) given varying degrees of demo- residedinMedanandthetexttargetedRohingya
graphicinformation. Baselinemeansnodemographic refugees. We attribute this improvement to the
informationwasgiven.
training data, which might include articles about
Rohingya refugees often first arriving in Medan
duringthefine-tuningoftheIndoBERTweetmodel
(Suryono, 2024). Another example is Makassar
thatexceedswhatthedatasetcansupport.
andtexttargetingtheDisabled,wherethemodel
achievesa0.2macro-F1improvementwhengiven
5.3 AblationofImpactperTopic
demographic information. This could be due to
Leveraging its Pre-training, gpt-3.5-turbo Un- themodel’sunderstandingoftheuniquerelation-
derstandsCulturalandValueDifferences. As shipofDisabledpeopleinMakassarcomparedto
indicated by its performance in Figure 5, gpt- otherlocations(Post,2023). Withoutdemographic
3.5-turbo, thanks to its pre-training data, inher- information,weassumethatthemodeldefaultsto
entlyunderstandstheculturalandvaluedifferences an inherent bias. For instance, when domicile in-
among people based on their identities. The fig- formationisnotprovided,gpt-3.5-turboseemsto
ureshowsthatgpt-3.5-turbosignificantlyimproves assume the reader’s location is either the Indone-siancapitalJakartaoranotherbigcity,Bogor,as
suggestedbythenon-significantperformancegap
visibleinFigure5. Visualizationsforothercases
areavailableintheappendixH.
Figure7: Theeffect(∆F1)ofgivingtopicinformation
toIndoBERTweetonitshatespeechtextclassification
comparedtonotgivingitanyinformation.
6 Discussion
Figure6: Theeffect(∆F1)ofgivingreligioninforma-
Thesurgeinonlinehatespeech(CSIS,2022;AJI,
tiontoIndoBERTweetonitshatespeechtextclassifica-
2024)posessocietalrisks(Williamsetal.,2019).
tioncomparedtonotgivingitanyinformation.
Thechallengeliesindefininghatespeechandthe
However, demographic information hurts In- debateovercensoringsuchcontent. Somefearcen-
doBERTweet’sclassificationperformance. We sorship could have a domino effect (Franco and
attribute this to the diversity and subjectivity of Warburton,2013),whileothersargueforimmedi-
hatespeechtexts. Thoughwehopedtoincreasethe ateactionduetoexistingsocietaldamage(Laakso-
model’sperformancebyprovidingdemographicin- nenetal.,2020). Despitethecomplexityofdefin-
formation,thisinsteadfragmentedthetrainingdata, inghatespeech,wecan’tremainidle. Wepropose
withfewertrainingdataforeachdemographic. In thatresearchersfocusoncreatinghatespeechde-
other words, we add another dimension(s) to the tectionsystemsfocusingonprotectingvulnerable
input without increasing the sample size. When minoritygroups,oftentargetedbyhatespeechand
the data is too few, this fragmentation hurts the hatecrimes,acrucialstepfortheirprotection.
modelinstead(Figure6). Thoughthemodellearns
Todevelopahatespeechdetectionsystemthat
thedifferencebetweeneachreligion,thereisvery
filters texts targeting vulnerable minority groups,
few data for some of them, such as Ahmadiyya
datasetswithdemographicinformation,likethose
andBuddhistwhicharethetwoleastrepresented
inKumaretal.(2021)andours,areneeded. How-
religiondemographicsinourdatasetconsistingof
ever,suchdatasetsaresurprisinglyscarce. Given
only346and1,368annotationsrespectively. With-
the potential benefits to these groups, we call on
outthisdemographicinformation,themodelonly
researcherstopromptlycreatesimilardatasets.
has the text as its input, which may explain the
Furthermore,demographicdatasetsmayhavea
baseline’sgenerallygoodperformance.
larger role than expected. Our analysis indicates
Topic information increases IndoBERTweet’s thatlargelanguagemodelslikegpt-3.5-turbomay
performance. Focusingontherowentriesfrom containbiases. Thesebiasescanbereducedwith
Figure7,weobservethatthemodel’sperformance metadata, which can form a “Persona” for effec-
increasesinmostcases. Thoughwehypothesized tivemodelinteraction. Ourtestsshowthatdemo-
thatthemodelwouldinnatelylearnthetopicofthe graphicinformationmakesgpt-3.5-turboabetter
text by itself, it failed to do so in practice, poten- hate speech detector. There could be other unex-
tiallyduetoalackofdata. Therefore,addingtopic ploredscenarioswheredemographicinformation
informationimprovesthemodel’sperformance. isbeneficialfordifferenttasks.Limitations EthicsStatement
Our work has the potential to pave the way for Thecreationofthisdatasetexposesannotatorsto
futureresearchincreatingbetterhatespeechand potentially harmful hate speech texts. To avoid
toxic text detection. However, our work is not excessivementalstrain,weintentionallyextended
withoutflaws. theannotationdurationtotwoandahalfmonths.
Individualsarepreemptivelywarnedandaskedfor
Baseline Performance Relies on a Private consentduringtheinitialrecruitmentprocess. Fur-
Dataset. Thebaselineperformancefortheseven thermore, annotatorsarepermittedtoquitthean-
binaryclassificationtaskswasestablishedbyinte- notationoftextsiftheyfeelunabletoproceed. We
grating our IndoToxic2024 dataset with a private recognize the potential misuse of such datasets,
datasetfrom(CSIS,2022), accessedthroughcol- which could include training models to generate
laboration. morehatespeech. Yet,it’sworthnotingthatwith-
outthesedatasets,itisalarminglystraightforward
ComparisonofFine-tunedandZero-shotMod- to train a model to produce toxic content, as the
els. Wecomparedtheperformanceofafine-tuned sourceoftheirtrainingdata,theinternet,stillcon-
IndoBERTweet model against zero-shot gpt-3.5- sistsofhatespeechandtoxictexts. Thishasbeen
turbo and SeaLLM-7B-v2.5 models. Due to the demonstratedbynumerousresearcherswhohave
extensivenumberofexperimentsconductedwith attemptedtoreducetoxicoutputoridentifyvulner-
gpt-3.5-turbo, we were unable to fine-tune it for abilitiesinlargelanguagemodels(refertoGehman
a direct comparison. Additionally, we discontin- et al. (2020); Wen et al. (2023)). On the other
uedtheuseofSeaLLM-7B-v2.5duetoitssubpar hand,theareaofdevelopingmodelstodetecthate
performancerelativetotheothermodels. speechtargetedatspecificdemographicgroupsis
still green, with a notable lack of available data,
Inconsistent Annotator Count Across Annota- especiallyinIndonesia. Weighingtheseconsidera-
tionPhases. Thevaryingnumberofannotators tions,wefirmlybelievethatthepotentialbenefits
acrossdifferentannotationphasesmayleadtoin- of this type of dataset significantly outweigh the
consistencies in the data distribution. This could possiblemisuse.
potentiallyresultinanincompleterepresentation
oftheoveralldemographictraits,asnotalldemo- Acknowledgements
graphicdimensionsmaybeuniformlycaptured.
We thank the Boston University Computing Fa-
cility for lending us their computing unit. This
Controlled Topic Distribution in Main Anno-
work is partly supported by the Alliance of Inde-
tation Phase 1. Annotators did not received a
pendentJournalists,theMonashIncubatorGrant,
completely random texts when they annotate the
andMonashDataFutureInstituteResearchGrants.
1000-textbatchinmainannotationphase1. Instead,
Anyopinions,findings,conclusions,orrecommen-
theyreceived50%textthatmentionstheirgroup,
dationsexpressedherearethoseoftheauthorsand
and 50% random text that did not mentions their
donotnecessarilyreflecttheviewofthesponsor.
groups. ThisappliestoalldatasetexcepttheShia&
Ahmadiyyadataset,whoreceivedonly39.3%text
oftheirgroupduetothescarcityofexistingdata.
References
Use of a Naive Keyword-based Approach for AJI.2024. 2024indonesiangeneralelectionhatespeech
Topic Extraction. Our approach to extracting monitoringdashboard. https://aji.or.id/. Ac-
cessedJune14th,2024.
featured topics relies on predefined keywords,
whichmayoverlooknuancedoremergentthemes.
EsraaAldreabiandJeremyBlackburn.2024. Enhancing
Thislimitationcouldrestrictthescopeofouranal- automatedhatespeechdetection: Addressingislamo-
ysis and prevent the identification of more subtle phobiaandfreedomofspeechinonlinediscussions.
orcomplextopicspresentinthetext. Amoredy- InProceedingsofthe2023IEEE/ACMInternational
ConferenceonAdvancesinSocialNetworksAnaly-
namicandflexibletopicextractionapproachcould
sisandMining,ASONAM’23,page644–651,New
potentially enhance the richness and accuracy of
York,NY,USA.AssociationforComputingMachin-
topicidentificationwithinIndoToxic2024. ery.LinaA.AlexandraandAlifSatria.2023. Identifying detikcom.2022. Jadwalpemilu2024lengkap,termasuk
HateSpeechTrendsandPreventioninIndonesia: a jadwalpilpresjika2putaran. AccessedNovember
Cross-Case Comparison. Global responsibility to 8th,2023.
protect,15(2-3):135–176.
MaiElSherief,CalebZiems,DavidMuchlinski,Vaish-
IkaAlfina,RioMulia,MohamadIvanFanany,andYudo naviAnupindi,JordynSeybolt,MunmunDeChoud-
Ekanata.2017. Hatespeechdetectionintheindone- hury,andDiyiYang.2021. Latenthatred: Abench-
sianlanguage: Adatasetandpreliminarystudy. In markforunderstandingimplicithatespeech. InPro-
2017 International Conference on Advanced Com- ceedingsofthe2021ConferenceonEmpiricalMeth-
puterScienceandInformationSystems(ICACSIS), odsinNaturalLanguageProcessing,pages345–363,
pages233–238. OnlineandPuntaCana,DominicanRepublic.Asso-
ciationforComputationalLinguistics.
David Armstrong, Ann Gosling, John Weinman, and
EveFleisig,RedietAbebe,andDanKlein.2024. When
TheresaMarteau.1997. Theplaceofinter-raterre-
themajorityiswrong: Modelingannotatordisagree-
liabilityinqualitativeresearch: Anempiricalstudy.
mentforsubjectivetasks.
Sociology,31(3):597–606.
Joshua Franco and Nigel Warburton. 2013. Should
AbinewAliAyele,SeidMuhieYimam,TadesseDestaw
therebelimitsonhatespeech? Indexoncensorship,
Belay,TesfaAsfaw,andChrisBiemann.2023. Ex-
42(2):150–152.
ploringAmharichatespeechdatacollectionandclas-
sification approaches. In Proceedings of the 14th
Samuel Gehman, Suchin Gururangan, Maarten Sap,
InternationalConferenceonRecentAdvancesinNat-
Yejin Choi, and Noah A. Smith. 2020. Realtoxic-
uralLanguageProcessing,pages49–59,Varna,Bul-
ityprompts: Evaluatingneuraltoxicdegenerationin
garia.INCOMALtd.,Shoumen,Bulgaria.
languagemodels.
Indonesia BPS-Statistics. 2024. Average of Net K. Greenawalt. 1989. Conflicts of Law and Morality.
Wage/Salary-StatisticalData—bps.go.id. OxfordUniversityPress,NewYork.
Brandwatch. 2021. Brandwatch consumer intel- Muhammad Okky Ibrohim and Indra Budi. 2018. A
ligence. https://www.brandwatch.com/suite/ datasetandpreliminariesstudyforabusivelanguage
consumer-intelligence/. detectioninindonesiansocialmedia. ProcediaCom-
puterScience,135:222–229. The3rdInternational
TomB.Brown,BenjaminMann,NickRyder,Melanie ConferenceonComputerScienceandComputational
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Intelligence (ICCSCI 2018) : Empowering Smart
Neelakantan,PranavShyam,GirishSastry,Amanda TechnologyinDigitalEraforaBetterLife.
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, DaroeIswatiningsih,EggyFajarAndalas,andNinaIn-
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, ayati.2019. Hatespeechbysupportersofindonesian
ClemensWinter,ChristopherHesse,MarkChen,Eric presidential candidates on social media. In 6th In-
Sigler,MateuszLitwin,ScottGray,BenjaminChess, ternationalConferenceonCommunityDevelopment
Jack Clark, Christopher Berner, Sam McCandlish, (ICCD2019),pages130–133.AtlantisPress.
Alec Radford, Ilya Sutskever, and Dario Amodei.
SimonKemp.2023. Digital2023: Indonesia—datare-
2020. Languagemodelsarefew-shotlearners.
portal–globaldigitalinsights. AccessedJune14th,
2024.
John L Campbell, Charles Quincy, Jordan Osserman,
and Ove K Pedersen. 2013. Coding in-depth
FajriKoto,JeyHanLau,andTimothyBaldwin.2021.
semistructuredinterviews: Problemsofunitization
Indobertweet: Apretrainedlanguagemodelforin-
andintercoderreliabilityandagreement. Sociologi-
donesian twitter with effective domain-specific vo-
calmethods&research,42(3):294–320.
cabularyinitialization.
cjadams,JeffreySorensen,JuliaElliott,LucasDixon,
KlausKrippendorff.2018. Contentanalysis: Anintro-
MarkMcDonald,nithum,andWillCukierski.2017.
ductiontoitsmethodology. Sagepublications.
Toxiccommentclassificationchallenge.
DeepakKumar,PatrickGageKelley,SunnyConsolvo,
CSIS.2022. Hatespeechdashboard. JoshuaMason,ElieBursztein,ZakirDurumeric,Kurt
Thomas,andMichaelBailey.2021. Designingtoxic
ArijitDas,SomashreeNandy,RupamSaha,SrijanDas, contentclassificationforadiversityofperspectives.
and Diganta Saha. 2021. Analysis and detection arXiv(CornellUniversity),pages299–318.
ofmultilingualhatespeechusingtransformerbased
deeplearning. JadavpurUniversity. Salla-Maaria Laaksonen, Jesse Haapoja, Teemu Kin-
nunen,MattiNelimarkka,andReetaPöyhtäri.2020.
R.Delgado.1982. Wordsthatwound: Atortactionfor Thedataficationofhate:Expectationsandchallenges
racialinsults,epithets,andname-calling. Harvard inautomatedhatespeechmonitoring. Frontiersin
CivilRights-CivilLibertiesLawReview,17:133–181. BigData,3.Matthew Lombard, Jennifer Snyder-Duch, and JiaxinWen,PeiKe,HaoSun,ZhexinZhang,Chengfei
Cheryl Campanella Bracken. 2002. Content anal- Li,JinfengBai,andMinlieHuang.2023. Unveiling
ysisinmasscommunication: Assessmentandreport- theimplicittoxicityinlargelanguagemodels. InPro-
ingofintercoderreliability. Humancommunication ceedingsofthe2023ConferenceonEmpiricalMeth-
research,28(4):587–604. ods in Natural Language Processing, pages 1322–
1338,Singapore.AssociationforComputationalLin-
Binny Mathew, Punyajoy Saha, Seid Muhie Yimam,
guistics.
ChrisBiemann,PawanGoyal,andAnimeshMukher-
jee.2022. Hatexplain: Abenchmarkdatasetforex- Matthew L Williams, Pete Burnap, Amir Javed, Han
plainablehatespeechdetection. Liu, and Sefa Ozalp. 2019. Hate in the Machine:
Anti-BlackandAnti-MuslimSocialMediaPostsas
United Nations. 2023. Hate speech and real harm |
Predictors of Offline Racially and Religiously Ag-
UnitedNations.
gravatedCrime. TheBritishJournalofCriminology,
60(1):93–117.
Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani
Aljunied, Qingyu Tan, Liying Cheng, Guanzheng
NahathaiWongpakaran,TinakonWongpakaran,Danny
Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang
Wedding,andKilemLGwet.2013. Acomparison
Zhang, and Lidong Bing. 2023. Seallms – large
of cohen’s kappa and gwet’s ac1 when calculating
languagemodelsforsoutheastasia.
inter-raterreliabilitycoefficients: astudyconducted
TetsujiOhyama.2021. Statisticalinferenceofgwet’s with personality disorder samples. BMC Medical
ac1 coefficient for multiple raters and binary out- ResearchMethodology,13(1).
comes. CommunicationsinStatistics-Theoryand
Methods,50(15):3564–3572.
HameedatOlodo,Bukola,MoriamAremu,andMustafa
Raji.2022. Demographicvariables: Apredictorof
consumerbuyingbehaviourinformalandinformal
retailoutlets. 8:63–76.
T. P. Paramadina and Mafindo. 2023. Buku Panduan
MelawanHasutanKebenciandanHoaxEdisiPerlu-
asan. PUSADParamadina,Jakarta.
ProfPatil,KulkarniSanika,andKhurpeSanjana.2024.
Medicalinsurancepremiumpredictionwithmachine
learning. International Journal of Innovations in
EngineeringResearchandTechnology,11:5–11.
JakartaPost.2023. ASEANproduces‘MakassarRec-
ommendations’forpersonswithdisabilities-Front
Row-TheJakartaPost.
Andrew Sellars. 2016. Defining hate speech. Social
ScienceResearchNetwork.
KinshukSenguptaandPraveenRanjanSrivastava.2022.
Causaleffectofracialbiasindataandmachinelearn-
ingalgorithmsonuserpersuasiveness&discrimina-
torydecisionmaking: Anempiricalstudy.
MitraSalimaSuryono.2024. Rohingyarefugeesrisk
dangeroussearoutetoIndonesiainsearchofsafety
andfreedom.
CrowdTangle Team. 2024. Crowdtangle. Face-
book, Menlo Park, Califormnia, United States.
1816403,1824912.
AliUnluandTommiKotonen.2024. Onlinepolariza-
tionandidentitypolitics: AnanalysisofFacebook
discourseonMuslimandLGBTQ+communitiesin
Finland. Scandinavianpoliticalstudies.
CuongNhatVo, KhanhBaoHuynh, SonT.Luu, and
Trong-HopDo.2024. Exploitinghatredbytargets
forhatespeechdetectiononvietnamesesocialmedia
texts.Appendix
A ExampleofDivisiveTexts
AlistofdivisivetextisavailableinTable6.
DivisiveText Comment
Around800to1000ChristiansstillliveinGaza,whichisoneofthe 3of3ChristiansorCatholics,and0of10non-
oldestChristiancommunitiesworldwide.Theyarealsoevictedand Christians or Catholics annotators annotate this
killed. EntirefaithgroupsdiedoutinthelandwhereChristianity textashatespeech.
originated.
Beawomanwhoiscrazyaboutwork. Becausethereareenough 1of4undergraduatesand3of3highschoolgrad-
womencrazyaboutboys.Eventhoughpeoplekeepseeingushang- uatesannotatorsannotatethistextashatespeech.
ingout. However,2of5femalesand2of3malesannotators
annotatethistextashatespeech.
"GANJA(Weed)FORALLCEBONG"withalltheir45spiritputting 2of3Ganjarvoters(Group3),5of5Prabowo
upbillboards,theyforgotonecharacterandturneditintosomething voters(Group2),and0of3Aniesvoters(Group
completelydifferent... hahahaah, thisiswhathappenswhenyou 1)annotatethistextashatespeech.
wanttoscampeople.
Everybody knows that most PDIP supporters are Christians and 3of4malesand0of7femalesannotatorsannotate
thatanyonegainingthesupportofPDIPwillalsogetthesupport thetextashatespeech.
ofChristians.However,thischancesbecauseofGanjarPranowo’s
stanceonbanningIsrael’ssoccerteamfromcomingtoIndonesiato
compete,whichmakesChristiansunhappy.
IusedtopraiseJokowibutnotanymore,why?Becauseheusedhis 7of8non-Chineseand0of3Chineseannotators
childforevilpeople. IfGibranwasmadetobecomeacandidate annotatethistextashatespeech.
forpresident,itwouldhavebeenokay.But,Gibranwasmadeasa
candidateforvicepresident,usedonlytogainsupportersforGolkar
politicalgroup.
GooooGaMa(Wordplayonthepresidentandvicepresidentcandi- 6 of 7 Gen Z and 0 of 4 Gen X and Millenials
dateofGroup3)!Legallydefectiveproductswillcontinuetocreate annotatethistextashatespeech.
defectiveproducts.
Table6: Examplesofdivisivetextsandthedemographicgroupinwhichtheyaredivisive.
B KeywordsUsedforScraping
cina,china,tionghoa,chinese,cokin,cindo,chindo,shia,syiah,syia,ahmadiyya,ahmadiyah,ahmadiya,
ahmadiyyah,transgender,queer,bisexual,bisex,gay,lesbian,lesbong,gangguanjiwa,gangguanmental,
lgbt, eljibiti, lgbtq+, lghdtv+, katolik, khatolik, kristen, kris10, kr1st3n, buta, tuli, bisu, budek, conge,
idiot,autis,oranggila,orgil,gila,gendut,cacat,odgj,zionis,israel,jewish,jew,yahudi,joo,anti-christ,
antikristus,antichrist,netanyahu,setanyahu,bangsapengecut,israhell,rohingya,pengungsi,imigran,
sakitjiwa,tunanetra,tunarungu,sinting.
C AnnotationGuidelines
C.1 Definition
Toxic comments is a post, text, or comment that is harsh, impolite, or nonsensical, causing you to
becomesilentandunresponsive,orthatisfilledwithhatredandaggression,provokingfeelingsofdisgust,
anger,sadness,orhumiliation,makingyouwanttoleavethediscussionorgiveupsharingyouropinion.
ProfanityorObscenity Themessage/sentenceonsocialmediapostscontainsoffensive,indecent,
or inappropriate in a way that goes against accepted social norms. It often involves explicit or vulgar
language, graphic content, or inappropriate references. Essentially, it’s a message that is likely to be
consideredoffensiveorobjectionablebymostpeople.
Threat/IncitementtoViolence Themessage/sentenceonsocialmediapostsconveysanintentto
causeharm,danger,orsignificantdistresstoanindividualoragroup. Itoftenincludesexplicitorimplicit
threatsofviolence,physicalharm,intimidation,oranyactionthatcreatesasenseoffearorapprehension.
Insults Themessage/sentenceonsocialmediapostscontainsoffensive,disrespectful,orscornful
languagewiththeintentionofbelittling,offending,orhurtingthefeelings.
IdentityAttack Themessage/sentenceonsocialmediapostsdeliberatelytargetsandundermines
aperson’ssenseofself,identity,orpersonalcharacteristics. Thiscanincludederogatorycomments,orharmfulstatementsaimedataspectssuchasone’srace,gender,sexualorientation,religion,appearance,
orotherdefiningattributes.
Sexually Explicit The message / sentence on social media posts contains explicit and detailed
descriptionsordiscussionsofsexualactivities,bodyparts,orotherrelatedcontent.
C.2 ManualAnnotation
Q1: Doesthistextappeartoberandomspamorlackcontext?
• Yes
• No
Q2: DoesthistextrelatedtoIndonesian2024GeneralElection?
• Yes
• No
Q3: Doesthistextcontaintoxicity(hatespeech)?
Note: Irrelevanttoxicityorhatespeechincludeshatespeechthatismeantasajokeamongfriendsoris
notconsideredhatespeechbytherecipient. Thus,itwillbecodedas"No".
• Yes
• No
Q4: Whatisthetypeoftoxicity?
Note: Codeuptotwoormoretypes. Considerthefollowingsentencesasanexample: “PDIPProvokasi
Massa pendukungnya geruduk kediaman Anies”. This headline should be coded as both threat and
incitementtoviolence.
Q4-1: Doesthemessagecontainsprofanity/obscenity?
• Yes
• No
Q4-2: Doesthemessagecontainthreat/incitementtoviolence?
• Yes
• No
Q4-3: Doesthemessagecontaininsults?
• Yes
• No
Q4-4: Doesthemessagecontainanidentityattack?
• Yes
• No
Q4-5: Doesthemessagecontainsexuallyexplicit?
• Yes
• No
D MappingofKeywords-to-Topics
• Shia: shia,syia,syiah
• Ahmadiyya: ahmadiya,ahmadiyah,ahmadiyya,ahmadiyyah
• Christian: antichrist,antikristus,anti-christ,kris10,kristen,kr1st3n,katolik,khatolik
• LGBTQ+: bisex,bisexual,eljibiti,gay,lesbian,lesbong,lgbt,lgbtq+,lghdtv+,queer,transgender
• Chinese: china,chindo,chinese,cina,cindo,cokin,tionghoaJewish: israhell,israel,jew,jewish,
joo,netanyahu,netanhayu,setanyahu,yahudi,zionis,bangsapengecut
• Rohingya: rohingya,imigran,pengungsi
• Disability: odgj,idiot,autis,bisu,budek,buta,cacat,gangguanjiwa,gangguanmental,gila,ogdj,
sinting,oranggila,orgil,sakitjiwa,tuli,tunanetra,tunarungu,conge,gendutE PromptforSyntheticTextGenerationusinggpt-3.5-turbo
To generate synthetic data from gpt-3.5-turbo to help increase model performance for various binary
classificationtask,weutilizethepromptvisualizedinFigure8
Figure8: Thetemplateweusetopromptgpt-3.5-turbothroughten-shotprompting.
F Within&BetweenGroupICRScoreCalculation
TocomputethetoxicityICRscoreforademographicgroup,wecalculatetheweightedaverageofGwet’s
AC1scoreforeverypairwisecombinationofannotatorswithinthesamegroup,usingthevolumeoftext
ineachpairastheweight. Thisapproachmaximizestheutilizationofavailabledata. Asimilarcalculation
methodisimplementedtofindtheICRscorebetweentwogroups,withthemodificationthateachpair
consistsofmembersfromdifferentgroups. Werefertothesemetricsas“within-groupICRscore”and
“between-groupICRscore”respectively.
WecanrigorouslydefineanequationtocomputeICRscorewithinagroupas
(cid:80)
dim(v ij)·Gwet(ϕ i(v ij),ϕ j(v ij))
i,j∈g
γ(g) =
(cid:80)
dim(v )
ij
i,j∈g
Whereg areanarbitrarygroupsinademographic;v aresetoftextthatbothmutuallybyannotators
ij
i,j; andϕ ,ϕ areannotationresultfromi,j. TocalculateICRscorebetweentwogroups,weslightly
i j
modifiedtheequationaboveinto
(cid:80)
dim(v ij)·Gwet(ϕ i(v ij),ϕ j(v ij))
Γ(g ,g ) =
i∈g1,j∈g2
1 2 (cid:80)
dim(v )
ij
i∈g1,j∈g2
Where g ,g are arbitrary two groups in a demographic; v are set of text that both mutually by
1 2 ij
annotatorsi,j;andϕ ,ϕ areannotationresultfromi,j.
i jG PerformanceofFine-tunedModelsonMultipleTopics
Thefiguresbelowshowtheperformanceofourfine-tunedmodels,trainedperdemographicgroup,on
varioustexttopics. Asthedemographicgroupchanges,theperformanceofeachmodelalsodiffersper
topic.H EffectofDemographicandTopicInformationtoModelPerformance
H.1 gpt-3.5-turboH.2 IndoBERTweetwithDemographicInformationH.3 IndoBERTweetwithTopicH.4 IndoBERTweetwithDemographicandTopic