Stochastic Gradient Piecewise Deterministic Monte Carlo
Samplers
Paul Fearnhead Sebastiano Grazzi
School of Mathematical Sciences, Department of Statistics,
Lancaster University University of Warwick
p.fearnhead@lancaster.ac.uk sebastiano.grazzi@warwick.ac.uk
Chris Nemeth Gareth Roberts
School of Mathematical Sciences, Department of Statistics,
Lancaster University University of Warwick
c.nemeth@lancaster.ac.uk gareth.o.roberts@warwick.ac.uk
Abstract
Recent work has suggested using Monte Carlo methods based on piecewise deter-
ministic Markov processes (PDMPs) to sample from target distributions of interest.
PDMPs are non-reversible continuous-time processes endowed with momentum, and
hencecanmixbetterthanstandardreversibleMCMCsamplers. Furthermore,theycan
incorporateexactsub-samplingschemeswhichonlyrequireaccesstoasingle(randomly
selected) data point at each iteration, yet without introducing bias to the algorithm’s
stationary distribution. However, the range of models for which PDMPs can be used,
particularly with sub-sampling, is limited. We propose approximate simulation of
PDMPs with sub-sampling for scalable sampling from posterior distributions. The
approximation takes the form of an Euler approximation to the true PDMP dynamics,
and involves using an estimate of the gradient of the log-posterior based on a data sub-
sample. We thus call this class of algorithms stochastic-gradient PDMPs. Importantly,
the trajectories of stochastic-gradient PDMPs are continuous and can leverage recent
ideas for sampling from measures with continuous and atomic components. We show
these methods are easy to implement, present results on their approximation error and
demonstrate numerically that this class of algorithms has similar efficiency to, but is
more robust than, stochastic gradient Langevin dynamics.
Keywords: Bouncy Particle Sampler; Control-variates; MCMC; Stochastic gradient
Langevin dynamics; Sub-sampling; Zig-Zag Sampler
1 Introduction
Whilst Markov chain Monte Carlo (MCMC) has been the workhorse of Bayesian statistics
for the past thirty years, it is known to scale poorly for large datasets, since each iteration of
MCMC requires the evaluation of the log-posterior density which typically scales linearly
with data size. As a result, approximate MCMC methods that use only a subsample of data
at each iteration have become popular. The first such method, and arguably the most widely
used, is the stochastic gradient Langevin dynamics (SGLD) algorithm of Welling and Teh
(2011). The idea of SGLD is to approximately simulate a Langevin diffusion that has the
posterior as its stationary distribution. The method involves two approximations: firstly it
simulates an Euler discretisation of the Langevin diffusion; and secondly it approximates
the gradient of the log-posterior, i.e. the drift of the diffusion, based on a subsample of the
data. The SGLD algorithm has been applied to applications such as topic models (Baker
et al., 2018), Bayesian neural networks (Gawlikowski et al., 2023) and probabilistic matrix
1
4202
nuJ
72
]LM.tats[
1v15091.6042:viXrafactorisation (S¸im¸sekli et al., 2017); and has also been extended to more general dynamics -
e.g. Chen et al. (2014); Ma et al. (2015). See Nemeth and Fearnhead (2021) for a review.
RecentlytherehasbeeninterestindevelopingefficientcontinuoustimeMCMCalgorithms
known as Piecewise Deterministic Markov processes (PDMP) (Vanetti et al., 2017; Davis,
1984). Such methods include the Bouncy Particle Sampler (Bouchard-Coˆt´e et al., 2018) and
the Zig-Zag algorithm (Bierkens et al., 2019) amongst others (Fearnhead et al., 2018). Due
to their non-reversibility, these methods often mix better than traditional reversible MCMC
algorithms (Diaconis et al., 2000; Bierkens, 2016). Furthermore, Bierkens et al. (2019) show
how these methods can be implemented whilst only using a small subsample of data at each
iteration, and yet still target the true posterior distribution. PDMPs (without subsampling)
have been also developed and successfully applied in computational physics for example
for the simulation of hard sphere models (Peters and de With, 2012; Bernard et al., 2009;
Krauth, 2021).
However PDMP samplers can be challenging to implement, particularly when using
subsamples, as they require bounding the gradient of the log-density. There has been some
work on automating the simulation of PDMPs using numerical methods (Corbella et al.,
2022; Pagani et al., 2024), but these methods largely require exact calculation of the gradient
of the log-posterior at each iteration and are incompatible with subsampling ideas.
In this paper, we investigate the approximate simulation of PDMPs with subsampling as
a means of achieving scalable MCMC. The idea is to discretise time into intervals of length
ε, for each interval we sample a single data point, and then simulate the (approximate)
dynamics of the PDMP using only the information from this data point. Loosely speaking,
this can be viewed as simulating an Euler approximation to the dynamics of the PDMP
with subsampling. This simple idea was suggested, but not investigated, by Bertazzi et al.
(2022) and Bertazzi et al. (2023). We call these methods stochastic-gradient PDMPs, or
SG-PDMPs. We show that they give results that are competitive with SGLD, but have
the advantage of being more robust to large discretisation sizes. They simulate continuous
trajectories, and we implement a version that leverages this property to easily sample from a
trans-dimensional posterior distribution for models that incorporate variable selection.
The idea of approximately simulating a PDMP algorithm with subsampling has been
previously suggested by Pakman et al. (2017). Their algorithm, called the stochastic bouncy
particle sampler (SBPS) uses Poisson thinning to simulate events of the Bouncy Particle
Sampler with subsampling. The idea is that, if we can upper bound the actual event rate,
and can simulate events with this upper bound rate, then we can thin (or remove) events
with an appropriate probability to obtain events simulated with the required rate. For most
target distributions we cannot calculate an upper bound for the rate, so Pakman et al. (2017)
suggests estimating this upper bound based on information on the rate from sub-samples of
data at proposed event times. In practice one of the main differences between our proposal
and SBPS is that, for each discrete time-step we only need to sample a single data point.
By comparison the stochastic bouncy particle sampler uses sub-sample batch sizes n that
are of the order of, say 10% of, the full-data size N (a smaller sub-sample batch size would
compromise its performance). Empirical results suggest our ability to use a sub-sample of
one can lead to a substantial increase in efficiency. For example Figure 1 shows the trace of
the first coordinate and the autocorrelation function of two versions of the SBPS algorithm
and of our SG-BPS algorithm, for a Bayesian logistic regression problem (see Appendix B
for details). In Appendix B, we present a more detailed comparison of our SG-PDMPS
and the SBPS, for a logistic regression model and a linear regression model. In all cases we
found SBPS mixes more slowly for a fixed computational cost. Also SBPS sometimes does
not converge if started in the tail of the posterior. Our ability to use, at every iteration, a
mini-batch size of 1 data point is a key advantage of our method and, in most contexts, we
2Figure 1: Top panels: traces of SBPS, SBPS with pre-conditioning (PSBPS) (left) and our
Euler approximation of the bouncy particle sampler, SG-BPS (right) with step-size h=10−4.
Bottom panel: auto-correlation function of the first coordinate for the three algorithms. All
algorithms were implemented with the same CPU cost, and output thinned to give 10,000
samples.
expect worse performance (relative to CPU cost) for larger mini-batches (see Appendix D
for numerical simulations varying the batch-sizes of SG-PDMPs).
The paper is structured as follows. Section 2 reviews PDMPs with subsampling, while in
Section 3 we describe in detail SG-PDMPs and present the main theoretical results which
shows the order of error for this class of algorithms. Section 4 highlights the main benefit of
using SG-PDMPs on an illustrative linear regression example. In Section 5, our algorithms
are applied to a logistic regression model, with artificial data, and for a Bayesian neural
network model with a variety of real datasets. Finally, Section 6 outlines promising research
directions stemming from this work.
2 PDMP samplers
Throughout we will consider the problem of sampling from a target density on Rd of the
form π(x)∝exp(−U(x)), and assume that
N
(cid:88)
U(x)= U (x) (1)
j
j=1
for some large value N and differentiable functions U :Rd →R, j =1,2,...,N. This is a
j
common problem in, for example, Bayesian inference where π(x) is the posterior density and
the factors, U (x) for j =1,...,N, corresponds to either the log prior or the log-likelihood
j
of conditionally independent observations.
Like other MCMC algorithms, a PDMP sampler simulates a Markov process that is
designed to have π(x) as its stationary distribution. However, whilst standard MCMC
algorithmssimulatediscrete-time,andgenerallyreversible,Markovprocess,aPDMPsampler
simulates a continuous-time, non-reversible process.
The dynamics of the PDMP are defined in the product space of position and velocity
E =Rd×V where V ⊂Rd. Evolution of the process is Markovian on E and given (in our
cases) by piecewise constant velocity dynamics interspersed by a sequence of random event
times at which the velocity component of the PDMP changes.
We will denote the state of the PDMP by z =(x,v)∈E, with x denoting its position
and v, its velocity. Let ϕ (z) denote the change in state due to the deterministic dynamics
s
3over a time-period of length s. That is z =ϕ (z ), where
t+s s t
ϕ (z )=ϕ ((x ,v ))=(x +sv ,v ).
s t s t t t t t
The dynamics of the PDMP are then specified through the rate at which events occur
and how the velocity changes at each event. As a PDMP is Markovian, the instantaneous
rate, λ(z) of an event only depends on the current state. The change of velocity at an event
will be defined through a Markov kernel that only acts on the velocity.
There are many choices of event rate and Markov kernel that will have π(x) as the
x-marginal of the stationary distribution of the resulting PDMP. We are interested in PDMP
samplers that use sub-sampling ideas, for which the rate and transition kernel can each be
written as a sum of terms, each of which depends on just a single factor U (x). We call these
j
S-PDMPs.
2.1 PDMP samplers with subsampling
The idea of the S-PDMP is that we define the dynamics in terms of dynamics associated
with each factor U (x). For each factor, j =1,...,N, introduce a reflection Fj(·), such that
j x
if v′ =Fj(v) then v =Fj(v′). The rate of events in the S-PDMP is then of the form
x x
N
1 (cid:88)
λ(z)= λj(z)
N
j=1
where λj: E →R+ satisfies
λj(x,Fj(v))−λj(x,v)=N(v·∇U (x))
x j
and the Markov kernel for the transition at an event allows for N possible transitions. If the
state immediately before the event is z =(x,v), then the state after the event is z′ =(x,v′)
with
λj(z)
Pr(v′ =Fj(v))= , for j =1,...,N.
x Nλ(z)
It can be shown that for these choices, the S-PDMP targets π(x) (Bierkens et al., 2019;
Fearnhead et al., 2018; Chevallier et al., 2024).
TheN∇U (x)termthatappearsintherateλj(z)canbeviewedasanunbiasedestimator
j
of ∇U(x). Just as control variates can be used to reduce the variance of such an estimator,
we can transform each factor U (x) to Uˆ (x) so that N∇Uˆ (x) is a better approximation of
j j j
∇U(x). This is done by defining Uˆ (x)=U (x)−x(U (xˆ)−(cid:80)N U (xˆ)), for some centering
j j j j=1 j
value xˆ. This gives
N
∇Uˆ (x)=N{∇U (x)−∇U (xˆ)}+(cid:88) ∇U (xˆ). (2)
j j j j
j=1
We will use such a set of transformed factors in the following, with xˆ assumed to be an
estimate of the mode of π, obtained through an initial run of stochastic gradient descent.
However, the following arguments will apply to other choices. As we will show, and as
is consistent with SGLD Baker et al. (2018), choices of factors that give lower variance
estimators of ∇U(x) will lead to better algorithms.
42.1.1 Bouncy Particle Sampler with subsampling
The original BPS (Bouchard-Coˆt´e et al., 2018) takes values in Rd×Rd and targets a density
proportional to π(x)p(v), where p(v) is a density for the velocity that is independent of
position and is symmetric. There are two common choices for p(v), one is a uniform density
over a d-dimensional hypersphere, and the other is a standard d-dimensional Gaussian
distribution. More generally, we could consider any distribution defined in terms of an
arbitrary distribution for the speed ||v||, together with an independent uniform distribution
for the direction of v, i.e. v/||v||. The dynamics of the BPS are unaffected by this choice,
other than the initialisation, which should involve drawing v from p(v), and at the refresh
0
events (see below).
When using subsampling, the reflection event associated with factor j is
v·∇Uˆ (x)
Fj(v)=v− j ∇Uˆ (x),
x ∥∇Uˆ (x)∥2 j
j
and the rates are
λj(z)=max(v·∇Uˆ (x),0). (3)
j
Furthermore, to ensure the process is irreducible, with constant rate λ >0, the velocity
ref
component is refreshed with an independent draw v ∼p(v).
2.1.2 Zig-Zag sampler with subsampling
The Zig-Zag sampler with subsampling (Bierkens et al., 2019) has velocity v ∈{−1,1}d. It
involves d possible events, each of which flips one component of the velocity.
As there are d possible events, an additional superscript, i, is introduced for each type of
flip. For i=1,...,d we define
R (v):=(v ,v ,...,v ,−v ,v ,...,v ),
i 1 2 i−1 i i+1 d
i.e. only the ith component is flipped. This transition will be the same for all factors, j, that
is Fi,j(v)=R (v). The rate associated with this transition is
x i
λi,j(z)=max(v∂ Uˆ (x),0), (4)
xi j
where, by analogy to eq. (2),
N
∂ Uˆ (x)=N{∂ U (x)−∂ U (xˆ)}+(cid:88) ∂ U (xˆ).
xi j xi j xi j xi j
j=1
2.2 Simulating S-PDMPs
S-PDMPs are continuous-time Markov processes that have π(x) as the x-marginal of their
stationary distribution. For reasons of practicality, we require a method for simulating these
processes. The challenge in simulating an S-PDMP lies in simulating the event-times, as the
other dynamics are simple.
Givencurrentstatez =(x,v),therateofthenexteventjustdependsonthetimeuntilthe
next event, which we define as λ (s):=λ((x+vs,v)), which uses the fact that, if z =z and
z t
therehasbeennofurtherevent,thenattimet+sthestateisz =(x+vs,v). Thusthetime
t+s
until the next event will be the time of the first event in an in-homogeneous Poisson process
(IPP) of rate λ (s). We will denote the time until the next event as τ ∼IPP(s→λ (s)). If
z z
5this rate is constant, τ is distributed as an exponential random variable with rate λ(z), i.e.
τ ∼Exp(λ(z)).
Thereexistsarangeoftechniquesforsimulatingfromanin-homogeneousPoissonprocess.
The most common general methods are based on the idea of thinning (Lewis and Shedler,
1979): we upper bound the rate λ(ϕ (z)) by a piecewise linear function of s; we simulate
s
possible events with this upper bound rate; and we accept these as actual events with
probability equal to the true rate divided by the bound. The challenge with efficiently
sampling an S-PDMP then comes from finding good upper bounds for the rates.
Bierkens et al. (2019) show that if we obtain an upper bound, λ+(s), that bounds the
z
rate corresponding to each factor of the target distribution, which in other words bounds
λj(ϕ (z)) for j =1...,N, then we can simulate the S-PDMP exactly whilst accessing only a
s
single factor at each iteration. This works by (i) simulating the next possible event, at a rate
λ+(s); (ii) sample a factor uniformly at random; (iii) if the possible event is at time τ, and
z
we have sampled the jth factor, we accept the event with probability λj(ϕ (z))/λ+(τ), and
τ z
change the state according to the kernel Q (ϕ (z),·). Importantly, given the upper bound,
j τ
the only step that depends on the target is step (iii) and this involves only a single factor.
In settings where S-PDMPs can be applied, they are shown to have an overall complexity
of O(1), relative to the sample size N. This is theoretically motivated and numerically
illustrated for logistic regression models in Bierkens et al. (2019) and Bierkens et al. (2020).
However, the set of models where we can find appropriate upper bounds is currently limited
and motivates our interest in approximate stochastic gradient PDMP samplers.
3 Stochastic gradient PDMP samplers
ByanalogywithstochasticgradientLangevinalgorithms,whereoneapproximatelysimulates
aLangevindiffusionwithπ(x)asitsstationarydistribution, weintroducestochasticgradient
PDMPalgorithmsthatapproximatelysimulatesanS-PDMPalgorithm,andcalledSG-PDMP
algorithms. The idea of simulating approximations to PDMPs is introduced in Bertazzi et al.
(2022, 2023).
Following Bertazzi et al. (2022), our approximation is based on discretising time into
intervals of size ε, and for each interval choosing a factor at random and then simulating the
dynamics of the underlying S-PDMP based on the rate and dynamics for that factor. As a
starting point, and to further simplify the simulation of any event in the time interval, we fix
the event rate based on the state at the start of the time interval, and simulate at most one
event. As we show below, using these two simplifications does not impact the order of the
approximation.
The resulting general algorithm is given in Algorithm 1. We assume our true PDMP has
K types of events, and introduce a rate and transition for each type and each factor. For
example, with the Zig-Zag sampler, we have d event types, one for each possible component
of the velocity to flip. See eq. (4) for the rates associated with each pair of event type and
factor, with event i having the same transition regardless of the factor, and with v′ =R (v)
i
for the new velocity. For the Bouncy Particle Sampler, we have two types of events. The
first is a reflection, with rate for factor j given by (3), and transition given by v′ =Fj(v).
x
The second type of event is a refresh event. This is the same for all factors and has constant
rate λ with transition v drawn from the stationary distribution for the velocity.
ref
The algorithm then loops over time intervals of length ε, simulates a factor J, and then
an event time for each type of event. These occur with a constant rate defined as the rate for
each event i, and factor J, for the current state of the process. We then calculate the time,
τ, and event type i∗ which occurs first. If that event occurs within the interval, we simulate
6the exact continuous-time dynamics of the PDMP over the time interval with an event of
type i∗ for factor J at time τ. If the first event does not occur within the time interval, we
just simulate the PDMP dynamics over the time interval with no events.
Algorithm 1 Stochastic gradient PDMP sampler
Require: (x,v)∈E, step size ε>0, time horizon T >0.
t=0,
Rates λi,j(z) for event of type i associated with potential U , for i = 1,...,K and
j
j =1,...,N.
Transition kernel Qi,j(·;z) associated with event of type i and potential U .
j
while t<T do
J ∼Unif(1,2,...,N)
τ ∼Exp(λi,J(z)), for i=1,...,K
i
τ =min τ , i∗ =argminτ
i=1,...,K i i
if τ <ε then
x←x+vτ
v ∼Qi∗,J(·;(x,v)) {Simulate new velocity}
x←z+v(ε−τ)
else
x←x+vε
end if
t←t+ε
Save (x,t)
end while
Using results in Bertazzi et al. (2022) we can show that the resulting SG-PDMP sampler
can give an O(ε) approximation to the distribution of the true PDMP over any fixed time
interval t:
Proposition 3.1. LetP (z,·)andP (z,·)bethetransitionkernelsforthestochasticgradient
t t
PDMPprocessofAlgorithm1andforthetrueunderlyingPDMPprocess,respectively. Assume
the PDMP processes have bounded velocities, so ∥v∥ < C for some C . Assume that for
0 0
any state z =(x,v) and any i=1,...,K and j =1,...,N the function λi,j(x+vt,v) has a
continuous derivative with respect to t. Then there exists constants C(z,T), that depend on
the initial state, z and time interval T, and ε >0 such that for all ε<ε
0 0
||P (z,·)−P (z,·)|| ≤C(z,T)ε.
T T TV
See Appendix A for the proof.
While this result shows that the SG-PDMP algorithm is an O(ε) approximation of the
true S-PDMP, a more informal analysis gives insight into the approximation error. Consider
the probability of simulating an event in the next interval of length ε given a current state
z =(x,v). Let λj(z)=(cid:80)K λi,j(z), then the probability for the true S-PDMP of no event
i=1
inanintervaloflengthεisexp{−(cid:80)N (1/N)(cid:82)ε λj(x+vt,v)dt},whereasfortheSG-PDMP
j=1 0
7it is (1/N)(cid:80)N exp{−λj(z)ε}. We can decompose the difference into two terms
j=1
   
 (cid:88)N (cid:90) ε λj(x+vt,v)   (cid:88)N λj(z)ε
exp − dt −exp −
N N
 j=1 0   j=1 
 
+
exp −(cid:88)N λj(z)ε
−
1 (cid:88)N exp(cid:8) −λj(z)ε(cid:9)
.
N N
 
j=1 j=1
The first difference is due to the use of constant event rates over the interval. Assuming the
rates are Lipschitz continuous, the change in event rate over an interval of length ε is O(ε),
so the difference in the integral of the rates is O(ε2). We could reduce this error by using a
better approximation to the event rate over the interval (Bertazzi et al., 2023). The second
difference is due to the stochastic gradient approximation, that is we consider just one factor
for each time interval. There are two points to make. The first is that by Jensen’s inequality,
this difference is always negative. This means that the stochastic gradient approximation
reduces the probability of an event. As the PDMPs only introduce events when moving into
areas of lower probability density, the impact of this is that it samples from a heavier-tailed
approximation to the target. Second, we can use a Taylor expansion to get the highest order,
in ε, term of the approximation. This is the O(ε2) term, as the first order terms cancel,
whose coefficient is
  2
1 (cid:88)N (cid:88)N λj(z)
− N λj(z)2− N   ,
j=1 j=1
which is (minus) the variance of the λj(z)s. This means that the approximation error
dependsonthevariabilityoftheestimatorofthegradientofU(x)acrossthedifferentfactors.
Furthermore, this error is O(ε2) which means that using a better approximation for how the
rates vary over the time interval would not improve the rate in ε of the approximation we
introduce. Proposition 3.1 provides an order of approximation in terms of the transition
kernels of our algorithms. For results on the approximation of the invariant measure, we
refer to Section 6 in Huggins and Zou (2017): under appropriate mixing conditions, the order
of the error (in terms of step-size) of the approximation in the transition density will imply a
similar order of error in the invariant measure.
One disadvantage of Algorithm 1 for coarse discretisations, i.e. when ε is large, is that it
does not allow more than one event in the interval. Thus a simple improvement is, if there
is an event at time τ < ε within the interval, to then iterate the simulation of events for
the time interval [τ,ε]. See Appendix C for details of the resulting algorithm for stochastic
gradient versions of the Zig-Zag sampler and the Bouncy Particle Sampler. These are the
algorithms we evaluate in the empirical results in Sections 4 and 5.
3.1 Extension with sticky components for variable selection
The methods developed here can be naturally extended to the PDMPs with boundary
conditions recently developed in Chevallier et al. (2023); Bierkens et al. (2023a); Chevallier
et al. (2024); Bierkens et al. (2023b) for sampling from target densities which are only
piecewise smooth and for reference measures which are mixture of continuous and atomic
components. The only difference these methods have is with the dynamics between events.
For example, if there is a boundary then the PDMP may reflect off the boundary if it hits it.
For sampling from Bayesian posteriors for models with variable selection, where the
target distribution is for the co-efficients of the model, it is common to have a prior that has
8positive probability on co-efficients being zero. The sticky PDMP of Bierkens et al. (2023a)
can deal with this by setting the velocity of components of x to zero if that component of x
is zero, and then re-introducing the non-zero velocity at a certain rate. This is also easily
incorporated into the SG-PDMP algorithm by appropriately changing the dynamics of the
process. See Appendix E for further details and the corresponding SG-PDMP algorithms.
Figure 2: Left top panel: error in standard deviation estimation of each coordinate (5)
E(h,d) as a function of the stepsize h (x-axis in log-scale). Left bottom panel: E(h,d) as a
function of d. Right panels: trace plots of the first coordinate.
4 Illustrative example: Linear regression model
WeillustratethebenefitsofSG-PDMPsamplersonasimplelinearregressionmodely =Ax+ε,
foraresponsevariabley ∈RN,covariatesA∈RN×d,parametersx∈Rd andε∼N(0,NcI )
N
(the variance of the noise is re-scaled by a factor to aid comparison across different values
of N, as the posteriors will have similar variance regardless of N). For this problem, we
compare sample averages computed with the trajectories of each sampler against the true
expectation of selected functionals of the true posterior.
WesetA =1toaccountfortheinterceptandsimulatethedataasA ∼N(0,Σ)with
:,1 i,j
Σ = Σ ∼ Unif(0.4,0.8)|i−j|,i = 1,2,...,d; j = 2,3,...,d and set x = 0. We simulate
i,j j,i 1
iid
x ∼ N(0,1),i=2,3,...,d with prior N(0,100×I ) for x.
i d
We consider two experimental settings. First, we simulate N = 106 covariates with
d=5 features and run SGLD, SG-ZZ and SG-BPS for different step-sizes and T =5×106
iterations, initialising each sampler at the ordinary least squares estimate. Figure 2 (left top
panel) shows
h(cid:55)→E(d,h) =
1(cid:88)d (cid:32) σˆ i(h)−σ i(cid:33)2
(5)
d σ
i
i=1
foreachsampler,whereσˆ(h) isthesamplestandarddeviationoftheithcomponentestimated
i
with the trace. The x-axis is displayed on a log-scale. We also consider the setting where we
initialise the algorithms in the tails of the distribution and plot in Figure 2 (left panels) the
first few iteration of the trace of the first coordinate (whose true value was set to 0) and
including the SG-SZZ sampler.
9Second, we set N =105 and simulate multiple datasets with varying d=10,...,102. For
each dataset, we run SGLD, SG-ZZ and SG-BPS. We fix the step size equal to h=5×10−7
and the number of iterations T =2×106, such that all samplers have a comparable error
for the dataset with the smallest number of dimensions. Figure 2 (bottom left panel) shows
d(cid:55)→E(d,h) for each sampler. In all simulations, each sampler uses a stochastic gradient with
control variates and 1 data point at each iteration (2). The number of gradient evaluations
and the running time is comparable among the different algorithms.
This simple tractable example provides useful insight into the advantages of SG-PDMP
over SGLD. Firstly we see greater stability of SG-PDMPS for large discretisation steps. This
is in contrast with SGLD which diverges to infinity if the discretisation step is too large.
(In these simulations, the maximum step size that can be taken for SGLD before diverging
is 0.004.) Secondly, the error in SG-PDMPs grows much slower compared to SGLD as the
dimensionality of the parameter increases: in these simulations, SGLD was unstable for
d>50. This offers opportunities for SG-PDMPs especially for high dimensional problems.
Finally, the deterministic dynamics of SG-PDMPs allows these algorithms to deal easily
with the variable dimensional posterior in the case where we use a prior on each component
on x that includes a point-mass at zero. This is achieved using the stochastic gradient
version of the sticky Zig-Zag sampler of Section 3.1. The trace of the sampler is shown in
the bottom-right panel, and we see it successfully enforces sparsity of the first parameter,
whose true value is equal to 0.
5 Numerical experiments
In this section, we assess the performance of SG-PDMPs and SGLD on intractable posterior
distributions. We consider logistic regression and a Bayesian neural network model.
We analyse the performance of each sampler by computing various metrics to assess bias
and numerical accuracy. In the logistic regression example, we simulate a large dataset so
that the posterior distribution is well-approximated by a Gaussian distribution formed from
a Laplace approximation. Then, for each sampler, we compute the sum of squared errors
as in eq. (5) between the estimated standard deviations and the standard deviations of the
Laplace approximation.
The samplers considered here are approximate and asymptotically biased, which means
that standard MCMC diagnostics such as Effective Sample Size (ESS) are not appropriate
(Nemeth and Fearnhead, 2021). A popular performance metric for stochastic gradient
Monte Carlo algorithms is the Kernel Stein Discrepancy (Gorham and Mackey, 2017); see
AppendixF.1. Thismetriccandetectbothpoormixingandbiasfromthetargetdistribution.
For all the problems considered, predictive performance of each sampler was assessed
by splitting the dataset D into a training set T and its complementary test set Tc so that
T ⊔Tc =D. We set |T|=0.9|D|. Each datum D =(X ,y ) consists of a pair of covariates
i i i
together with a dependent variable. Then, for each point x of the output of each algorithm,
its predictive accuracy is given by
1 (cid:88)
ℓ(X ,y ,x) (6)
|Tc| i i
(Xi,yi)∈Tc
where ℓ is a non-negative loss function between y and the predicted outcome given the
i
covariates X and parameter x. Specific loss functions will be specified in context.
i
EachalgorithmwasimplementedM times,eachtimewithadifferentrandompermutation
of training and test datasets. Average loss was computed across these M realisations. In all
10Figure 3: Error between standard deviation estimation of each coordinate and the one
relative to the Laplace approximation as in (5) for the logistic regression as a function of the
stepsize h (x-axis in log-scale). Dashed lines corresponds to SGLD algorithms, solid lines to
SG-PDMPs.
examples, control-variate stochastic gradient estimates were employed (Equation (2)) and
samplers were initialised at the same control variate point, with no burn-in period. The
control variate was computed using the stochastic optimization algorithm ADAM (Kingma
and Ba, 2014), with 106 iterations and a subsample size equal to 1% of the dataset and all
the other parameters suggested therein.
5.1 Logistic regression
A Bayesian logistic regression problem was considered with standard Gaussian prior with
variance 10 for each component. We set the dimension of the parameter p = 10 and we
simulate N =105 covariates X ∼N(0,Σ) where Σ =Σ ∼Unif(−ρ,ρ)|i−j|, ρ=0.4 for
i,j i,j j,i
i=1,2,...,N and j =1,2,...,p. y was simulated with true parameters: x⋆ ∼N(0,1).
i
SGLDwasimplementedwithminibatchesofsizes1,10and100datapointsandcompared
against SG-PDMP samplers SG-ZZ, SG-BPS and SG-SZZ. All algorithms were run for
T = 106 iterations. The running time, and number of gradient evaluations, for SGLD
with minibatches of sizes 10 and 100 were significantly higher compared to the SG-PDMP
algorithms. Each simulation was implemented for several values of h = 10−6,...,10−3.
Figure 3 shows h → E(h,d) - eq. (5) between the estimated standard deviation and the
standard deviation of the Laplace approximation. In Appendix F, we give the Kernel Stein
discrepancy and the loss function (6). For all experiments, SG-PDMPs performs similarly to
SGLD for small step-sizes and out-perform SGLD with 1, 10, 100 mini-batch sizes for larger
step-sizes (the maximum step-size for SGLD before the algorithms diverge is 10−4).
A second experiment considering an over-parameterised regime was also considered. In
thissettingwetookp=102 parametersandN =102 covariates, withtrueparameterbeing0
with probability 0.5. SGLD, SG-ZZ, SG-BPS, SG-SZZ were all implemented for T =107 and
h=10−4. SG-SZZ utilised a spike-and-slab prior with spike weight equal to w =0.5. Table 1
shows the mean squared error between true parameters and sample mean and median.
5.2 Bayesian neural networks
Bayesian approaches to neural networks (BNNs) provides a powerful calculus to quantify
uncertainty and reduce the risk of overfitting by incorporating techniques such as dropout
(Gal and Ghahramani, 2016) and imposing sparsity-inducing priors (Polson and Roˇckov´a,
11SGLD SG-ZZ SG-BPS SG-SZZ
mean 3.08155 3.15546 2.93989 1.42291
median 3.02602 3.12318 2.90635 1.18081
Table 1: Mean squared error relative to the sample mean and sample median for over-
parameterised logistic regression.
Figure 4: Trace of the loss function for each sampler for different step-sizes h relative to the
first permutation of the training and test set of the dataset ‘boston’.
2018).
Three datasets were considered from the UCI machine learning repository1, varying in
dimension and data size labelled as boston, concrete, kin8mn. A two-layer Bayesian neural
network was utilised:
y =a (W (a (W x+b )+b )+N(0,1)
2 2 1 1 1 2
where a (x) = x and a (x) = max(0,x), for unknown parameters W ∈ R50×p,W ∈
1 2 1 2
R1×50,b ∈R50 and b ∈R for p covariates in each dataset. An independent prior N(0,10)
1 2
was chosen for each component of x = (W ,W ,b ,b ). With this setting, the number of
1 2 1 2
dimensions of the parameter x ranged from 501 to 751.
SG-ZZ, SG-SZZ, SG-BPS and SGLD were implemented for N =106 iterations on M =3
random permutations of the training and test datasets, varying h = 10−6,...,10−3. The
loss function was chosen to be the mean squared error. Figure 4 shows the loss function
trace plot for each sampler in the first permutation of training and test set, for the dataset
‘boston’ which has N =507 data points and p=13 covariates. For this experiment, SGLD
was unstable for h=10−3. In Table 5.2 we display the average loss of the trace which was
computed by averaging the M =3 random permutations of training and test datasets. The
results for the other three datasets considered here are qualitatively similar and can be found
in Appendix G.
6 Discussion
We have presented SG-PDMPs as a competitive and robust alternative to the popular SGLD
algorithm for approximate Bayesian inference. In particular, we highlighted that SG-PDMPs
are generally more stable than SGLD for large step sizes, and can take advantage of the
1https://archive.ics.uci.edu/
12h
Sampler 10−3 10−4 10−5 10−6
SGLD NaN 0.4940 0.4586 0.4526
SG-ZZ 0.4720 0.4572 0.4518 0.4555
SG-BPS 0.4692 0.4533 0.4543 0.4574
SG-SZZ 0.4704 0.4586 0.4564 0.4643
Table2: Averagelossforeachsamplerfordifferentvaluesofstepsizehrelativetothedataset
‘boston’. In bold the best performance and italic the worst performance given a step size h.
continuous-time sample paths through e.g. the sticky dynamics for regression with model
choice. The better robustness that SG-PDMPs have is due to the fixed velocity dynamics,
and is reminiscent of that of relativistic Hamiltonian dynamics (Lu et al., 2017), and the
stochastic gradient Barker dynamics of Mauri and Zanella (2024).
There are several natural extensions to this paper which could be explored as future
work. Itisknownthatsuitablepre-conditioningcanimprovemixingofPDMPs. Anadaptive
preconditioning mass matrix for PDMPs has been studied in Bertazzi and Bierkens (2022)
and Pakman et al. (2017). As noted in Livingstone and Zanella (2022), the convergence of
adaptive algorithms can be drastically improved if algorithms are robust to the choice of step
size, as SG-PDMPs appear to be. Second, SGLD is the Monte Carlo analogue of the popular
stochastic optimisation algorithm Stochastic Gradient Decent (SGD). Similarly, Piecewise
Deterministic Markov processes which converge to global minima appeared for example in
Monmarch´e (2014). It is therefore natural to develop a stochastic optimization method
based on SG-PDMP dynamics. Finally, it would be interesting to develop higher-order
approximation schemes. This will be non-trivial, as, in Section 3, we showed that higher
order approximations of the Poisson rates, as developed in Bertazzi et al. (2023), cannot
be directly adopted in this context since the error given by taking the stochastic gradient
dominates the overall error of the algorithm.
References
Baker, J., Fearnhead, P., Fox, E. and Nemeth, C. (2018). Large-scale stochastic sampling
from the probability simplex. Advances in Neural Information Processing Systems 31.
Bernard, E. P., Krauth, W. and Wilson, D. B. (2009). Event-chain Monte Carlo algorithms
for hard-sphere systems. Phys. Rev. E 80, 056704.
Bertazzi, A. and Bierkens, J. (2022). Adaptive schemes for piecewise deterministic Monte
Carlo algorithms. Bernoulli 28(4), 2404–2430.
Bertazzi, A., Bierkens, J. and Dobson, P. (2022). Approximations of piecewise determin-
istic Markov processes and their convergence properties. Stochastic Processes and their
Applications 154, 91–153.
Bertazzi, A., Dobson, P. and Monmarch´e, P. (2023). Piecewise deterministic sampling with
splitting schemes. ArXiv:2301.02537.
Bierkens, J. (2016). Non-reversible Metropolis-Hastings. Statistics and Computing 26(6),
1213–1228.
13Bierkens, J., Fearnhead, P. and Roberts, G. (2019). The Zig-Zag process and super-efficient
sampling for Bayesian analysis of big data. Ann. Statist. 47(3), 1288–1320.
Bierkens, J., Grazzi, S., Kamatani, K. and Roberts, G. (2020). The Boomerang Sampler. In:
International Conference on Machine Learning, PMLR, 908–918.
Bierkens, J., Grazzi, S., Meulen, F. v. d. and Schauer, M. (2023a). Sticky PDMP samplers
for sparse and local inference problems. Statistics and Computing 33(1), 8.
Bierkens, J., Grazzi, S., Roberts, G. and Schauer, M. (2023b). Methods and applications of
PDMP samplers with boundary conditions. ArXiv:2303.08023.
Bouchard-Cˆot´e, A., Vollmer, S. J. and Doucet, A. (2018). The bouncy particle sampler: A
nonreversible rejection-free Markov chain Monte Carlo method. Journal of the American
Statistical Association 113(522), 855–867.
Chen, T., Fox, E. and Guestrin, C. (2014). Stochastic gradient Hamiltonian Monte Carlo. In:
International Conference on Machine Learning, PMLR, 1683–1691.
Chevallier, A., Fearnhead, P. and Sutton, M. (2023). Reversible jump PDMP samplers for
variable selection. Journal of the American Statistical Association 118, 2915–2927.
Chevallier, A., Power, S., Wang, A. Q. and Fearnhead, P. (2024). PDMP Monte Carlo
methods for piecewise-smooth densities. Advances in Applied Probability, to appear .
Corbella, A., Spencer, S. E. and Roberts, G. O. (2022). Automatic zig-zag sampling in
practice. Statistics and Computing 32(6), 1–16.
Davis,M.H.(1984).Piecewise-deterministicmarkovprocesses: Ageneralclassofnon-diffusion
stochasticmodels.Journalofthe Royal Statistical Society: SeriesB(Methodological) 46(3),
353–376.
Diaconis, P., Holmes, S. and Neal, R. M. (2000). Analysis of a nonreversible Markov chain
sampler. Annals of Applied Probability 10, 726–752.
Fearnhead, P., Bierkens, J., Pollock, M. and Roberts, G. O. (2018). Piecewise Deterministic
Markov Processes for Continuous-Time Monte Carlo. Statistical Science 33(3), 386–412.
Gal, Y. and Ghahramani, Z. (2016). Dropout as a Bayesian approximation: Representing
model uncertainty in deep learning. In: International Conference on Machine Learning,
PMLR, 1050–1059.
Gawlikowski, J., Tassi, C.R.N., Ali, M., Lee, J., Humt, M., Feng, J., Kruspe, A., Triebel, R.,
Jung, P., Roscher, R., Shahzad, M., Yang, W., Bamler, R.andZhu, X.X.(2023).Asurvey
of uncertainty in deep neural networks. Artificial Intelligence Review 56, S1513–S1589.
Gorham, J. and Mackey, L. (2017). Measuring sample quality with kernels. In: International
Conference on Machine Learning, PMLR, 1292–1301.
Huggins, J. and Zou, J. (2017). Quantifying the accuracy of approximate diffusions and
markov chains. In: Artificial Intelligence and Statistics, PMLR, 382–391.
Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization.
ArXiv:1412.6980.
14Krauth, W. (2021). Event-chain monte carlo: Foundations, applications, and prospects.
Frontiers in Physics 9.
Lewis, P. W. and Shedler, G. S. (1979). Simulation of nonhomogeneous Poisson processes by
thinning. Naval Research Logistics Quarterly 26(3), 403–413.
Livingstone, S. and Zanella, G. (2022). The Barker proposal: combining robustness and
efficiency in gradient-based MCMC. Journal of the Royal Statistical Society Series B:
Statistical Methodology 84(2), 496–523.
Lu, X., Perrone, V., Hasenclever, L., Teh, Y. W. and Vollmer, S. (2017). Relativistic Monte
Carlo . In: Proceedings of the 20th International Conference on Artificial Intelligence
and Statistics (eds. A. Singh and J. Zhu), volume 54 of Proceedings of Machine Learning
Research, PMLR, 1236–1245.
Ma, Y.-A., Chen, T. and Fox, E. (2015). A complete recipe for stochastic gradient MCMC.
Advances in Neural Information Processing Systems 28.
Mauri, L. and Zanella, G. (2024). Robust approximate sampling via stochastic gradient
barker dynamics. In: International Conference on Artificial Intelligence and Statistics,
PMLR, 2107–2115.
Monmarch´e, P. (2014). Piecewise deterministic simulated annealing. ArXiv:1410.1656.
Nemeth,C.andFearnhead,P.(2021).StochasticgradientMarkovchainMonteCarlo.Journal
of the American Statistical Association 116(533), 433–450.
Pagani, F., Chevallier, A., Power, S., House, T. and Cotter, S. (2024). NuZZ: numerical
Zig-Zag sampling for general models. Statistics and Computing 34, 61.
Pakman, A., Gilboa, D., Carlson, D. and Paninski, L. (2017). Stochastic bouncy particle
sampler. In: International Conference on Machine Learning, PMLR, 2741–2750.
Peters, E. A. J. F. and de With, G. (2012). Rejection-free monte carlo sampling for general
potentials. Physical Review E 85(2).
Polson, N. G. and Roˇckov´a, V. (2018). Posterior concentration for sparse deep learning.
Advances in Neural Information Processing Systems 31.
S¸im¸sekli, U., Durmus, A., Badeau, R., Richard, G., Moulines, E. and Cemgil, A. T. (2017).
Parallelized stochastic gradient Markov chain Monte Carlo algorithms for non-negative
matrix factorization. In: 2017 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), IEEE, 2242–2246.
Vanetti, P., Bouchard-Cˆot´e, A., Deligiannidis, G. and Doucet, A. (2017). Piecewise-
Deterministic Markov Chain Monte Carlo. ArXiv:1707.05296.
Welling, M. and Teh, Y. W. (2011). Bayesian learning via stochastic gradient Langevin
dynamics. In: Proceedings of the 28th International Conference on Machine Learning
(ICML-11), 681–688.
15A Proof of Proposition 3.1
The proof follows by application of Theorem 4.17 of Bertazzi et al. (2022). Within Bertazzi
et al. (2022), the authors discuss approximations of the Zig-Zag sampler with sub-sampling –
see their Example 5.7. However they comment that such an approximation is different from
the algorithms they consider and just sketch how their proofs could be extended to such
an algorithm. Here we consider a different approach, by showing that we can re-formulate
our SG-PDMP algorithm as an specific case of their Algorithm 3, and then directly apply a
result for that algorithm.
Let the current state be z. The distribution of the time to the next, τ, that is simulated
in Algorithm 1 satisfies
N (cid:40) K (cid:41)
(cid:88) 1 (cid:88)
Pr(τ >t)= exp −t λi,j(z) .
N
j=1 i=1
This follows by averaging Pr(τ >t|J =j) over the possible values of j. Given an event at
time τ, type of event i∗ and factor J has probability mass function
(cid:110) (cid:111)
λi,j(z)exp −t(cid:80)K λk,j(z)
k=1
Pr(i∗ =i,J =j|τ)= . (7)
(cid:16) (cid:17) (cid:110) (cid:111)
(cid:80)N (cid:80)K λk,l(z) exp −t(cid:80)K λk,l(z)
l=1 k=1 k=1
This follows as, by Bayes theorem, this is proportional to the density of choosing factor j,
having an event at time τ and the event being of type i – which is the term in the numerator.
The denominator is then just the normalising constant of the probability mass function.
In Algorithm 3 of Bertazzi et al. (2022) they simulate the time τ such that
(cid:26) (cid:90) t (cid:27)
Pr(τ >t)=exp − λ¯(z,s)ds , (8)
0
for some time-inhomogeneous rate λ¯(z,s), that is the sum of m event specific rates λ¯(z,s)=
(cid:80)m λ¯ (z,s). The the type of event, k say, is simulated with probability λ¯ (z,τ)/λ¯(z,τ).
k=1 k k
To relate the two algorithms, we first let m=KN. We then slightly adapt the notation
of Bertazzi et al. (2022) and subscript the event rates by the pair (i,j), rather than a
single index. Define the event specific time-inhomogeneous rates by, for i = 1,...,K and
j =1,...,N,
(cid:110) (cid:111)
exp −s(cid:80)K λk,j(z)
λ¯ (z,s)=λ (z) k=1 . (9)
i,j i,j (cid:110) (cid:111)
(cid:80)N exp −s(cid:80)K λk,l(z)
l=1 k=1
Then
(cid:110) (cid:111)
N K exp −s(cid:80)K λk,j(z)
λ¯(z,s)=(cid:88)(cid:88)
λi,j(z)
k=1
,
(cid:110) (cid:111)
(cid:80)N exp −s(cid:80)K λk,l(z)
j=1i=1 l=1 k=1
and by noting that
(cid:40) N (cid:40) K (cid:41)(cid:41)
d log (cid:88) 1 exp −s(cid:88) λk,l(z) =−λ¯(z,s),
ds N
l=1 k=1
(cid:90) t λ¯(z,s)ds=(cid:34) −log(cid:40) 1 (cid:88)N exp(cid:40) −s(cid:88)K λk,l(z)(cid:41)(cid:41)(cid:35)t =−log(1)+log(cid:40) 1 (cid:88)N exp(cid:40) −t(cid:88)K λk,l(z)(cid:41)(cid:41)
.
N N
0 l=1 k=1 0 l=1 k=1
16Substituting into eq. (8), we see that the distribution of τ in Algorithm 3.1 of Bertazzi et al.
(2022) is the same of the distribution of τ simulated by Algorithm 1. Conditional on τ, the
probability of choosing event i,j in Algorithm of Bertazzi et al. (2022) is
(cid:110) (cid:111)
λ¯ i,j(z,τ)
=
λi,j(z)exp −s(cid:80)K k=1λk,j(z)
,
λ¯(z,τ) (cid:80)N (cid:16) (cid:80)K λk,l(z)(cid:17) exp(cid:110) −s(cid:80)K λk,l(z)(cid:111)
l=1 k=1 k=1
which is the same as the probability for Algorithm 1 – see (7).
Thus Algorithm 1 is equivalent to Algorithm 3 of Bertazzi et al. (2022), with the specific
choiceofrates, λ¯ (z,s)givenin(9). Proposition3.1followsimmediatelyfromTheorem4.17
i,j
of Bertazzi et al. (2022) if we can show their Assumption 4.14 holds. As they comment (see
their Note 4.16), this will hold if the state of the true and approximate PDMP has bounded
norm for a finite time horizon, and if their Assumption 4.6 holds. The former is true as we
are considering PDMPs with bounded velocity.
Their Assumption 4.6 requires (A) an M¯(z) such that for 0≤s≤ε≤ε , and all i,j
0
|λ¯ (z,s)−λ ((x+sv,v))|≤εM¯(z),
i,j i,j
where z =(x,v); and (B) that for any future time time nε<T for positive integer n, that
E [M¯(Z¯ )]≤M(nε,z)<∞,
z nε
where Z¯ is that state of the approximate PDMP at time t and expectation is with respect
t
to this state assuming Z¯ =z.
0
Part (A) follows (i) as the two rates are identical at s=0, i.e. λ¯ (z,0)=λi,j((x,v)) and
i,j
(ii) both rates have bounded derivative with respect to s (this is simple to show for λ¯ (z,s)
i,j
from its definition, and is by assumption for λ ((x+sv,v))). The constant M¯(z) can be
i,j
defined by to be the sum of the maximum of each of these derivatives for 0≤s≤ε . Part
0
(B) will then follow immediately as the region of possible values of Z¯ within a finite time
nε
interval is compact.
B Comparison with Stochastic Bouncy Particle Sampler
Inthissection,wecompareSG-PDMPswiththeStochasticBouncyParticleSampler(SBPS),
and its adaptive preconditioned version (pSBPS) as presented in Pakman et al. (2017). The
SBPS and pSBPS algorithms attempt to tackle a similar underlying problem as our paper:
how to approximately implement a PDMP sampler with sub-sampling. The challenge lies in
how to efficiently sample events of the PDMP with sub-sampling. As we shall explain below,
our approach is simpler and computationally more efficient than SBPS and pSBP.
There are a number of substantive and practical differences between this approach and
our algorithms. Our approach discretises time, and simulates events with a constant rate
within each time-interval. This constant rate is simple to calculate, and involves accessing
a single data point and calculating the current event rate associated with the process. By
comparison, SBPS involves fitting a linear model to the rate based on the observed rates for
mini-batches of data for rejected events since the last event. The SBPS approach needs to
account for the randomness across the choice of mini-batch, and deal with the challenge of
fitting a (probably incorrect) linear model to few (initially just one) data points. Together
this means that SBPS uses much larger mini-batch sizes: the authors considered mini-batch
sizesofapproximatelyn=0.1N comparedton=1forourEulerscheme(alowermini-batch
17can affect negatively the performance of SBPS as the thinning rate grows linearly with N/n
– see Equation 10 in Pakman et al. (2017)). The uncertainty in fitting the linear model
means one has to take conservative upper bounds, which can also lead to a resulting loss of
efficiency.
We consider two Baysian problems: a Bayesian linear regression and a Bayesian logistic
regression. In both examples, we set an improper uniform prior. In both cases, we set the
dimensiontobed=10andwesimulateN =105 covariatesX ∼N(0,I )). ywassimulated
i,j d
from the model with the true parameters x⋆ ∼ N(−5, 100) (for the linear regression, we
√ i
re-scaled the noise with N). For both problems, we perform analogous experiments and
we set the sub-sample batch size for the SBPS to be equal to 10%N as in Pakman et al.
(2017) and all the other parameters suggested therein. Note that the need for SBPS to scale
minibatch size with N means that the computational advantages of our approach will grow
(linearly in N) for larger data sets.
For both problems, we compare SBPS and pSBPS with SG-PDMPs in two numerical
experiments. To make the comparison fair, for all implementations we have thinned the
continuous-time output so that, for the same CPU cost, each algorithm outputs the same
number of samples. In the first experiment we initialise all algorithms far away from the
bulk of the posterior measure, by setting x =N(0,1002).
0
With this initialisation, we observe that, in the logistic regression problem, SBPS (re-
spectively pSBPS) fails to approximate the Bouncy Particle Sampler with sub-sampling: it
estimates an upper bound which fails to bound the real rate more than 50% (respectively
22%). This undesirable behaviour translates in an algorithm which does not converge to the
posterior density. In contrast, SG-PDMPs converge to the bulk of the posterior measure, for
different values of step-size h. In the linear regression example, SBPS and pSBPS converge
to the bulk of the distribution, but the rate of convergence is significantly slower compared
to SG-PDMPs.
Figure 6 and Figure 9 display the trace of the first coordinate of SBPS, SG-BPS, SG-ZZ
withstep-sizeequaltoh=10−3 andh=10−4 forthelogisticregressionandlinearregression
examples.
In the second experiment, we initialise all algorithms close to the posterior mode. In this
setting, SBPS behaves better: for example, for the logistic regression, SBPS (respectively
pSBPS) fails to upper bound the real rate 3% (4%) of the time. However, when comparing
SBPS to SG-PDMPs, we notice that SBPS is substantially worse at mixing for both of the
problems considered. We show in Figure 5 and in Figure 8 the traces of the first coordinate
fo SBPS, SG-BPS and SG-ZZ (with h=10−3, 10−4) and the auto-correlation functions for
the logistic and linear regression problem. Furthermore, for each algorithm we compute
ε(t ,t )=
1(cid:88)d (cid:32) σˆ i(t1,t2)−σ i(cid:33)2
1 2 d σ
i
i=1
where σˆ(t1,t2), t ≤ t is the empirical standard deviation of the ith coordinate computed
i 1 2
with theoutput withburninequal to t andtotal numberof samplepoints equalto t and σ
1 2 i
is the empirical standard deviation computed with a long-run (108 iterations) of the SG-BPS
with small step-size h=10−5. Figure 7 and Figure 9 show for each algorithm ε(⌊t⌋,t) for
2
different values of t respectively for the logistic and linear regression problem.
18Figure 5: From the top-left to the bottom-right. Traces and auto-correlation function of
the first coordinate of SBPS, SG-BPS, SG-ZZ (with step-sizes 10−3,10−4) relative to the
Bayesian logistic regression model. All algorithms where initialised near the mode of
the posterior (second experiment).
Figure 6: Traces of the first coordinate
Figure 7: ε(t÷2,t) for t = 1,2,...,6000
of SBPS, SG-BPS, SG-ZZ (with step-sizes
relative to the Bayesian logistic regres-
10−3,10−4) relative to the Bayesian logis-
sionmodel. Allalgorithmswhereinitialised
ticregressionmodel. Allalgorithmswhere
near the mode of the posterior (second ex-
initialised far away from the bulk of the pos-
periment).
terior density (first experiment).
C Stochastic Gradient ZZ and BPS
In Algorithms 2 and 3 we describe the specific implementation of stochastic gradient versions
of the Zig-Zag sampler and the Bouncy Particle Sampler.
D Comparison of SG-PDMPs with varying mini-batch
size
PDMP samplers with subsampling and SG-PDMPs, as presented in Section 2-3, can be
extended by using, at every iteration, a batch-size of n<N random data points instead of a
single one. We view the ability to use a mini-batch size of 1 to be a key advantage of our
method and, in most contexts, we would expect worse performance (relative to CPU cost)
for larger mini-batches. In this section we support our claim with numerical experiments. A
19Figure 8: From the top-left to the bottom-right. Traces and auto-correlation function of
the first coordinate of SBPS, SG-BPS, SG-ZZ (with step-sizes 10−3,10−4) relative to the
Bayesian linear regression model. All algorithms where initialised near the mode of the
posterior (second experiment).
Figure 9: Traces of the first coordinate
Figure 10: ε(t÷2,t) for t = 1,2,...,6000
of SBPS, SG-BPS, SG-ZZ (with step-sizes
relative to the Bayesian linear regression
10−3,10−4) relative to the Bayesian linear
model. Allalgorithmswhereinitialisednear
regression model. All algorithms where
the mode of the posterior (second experi-
initialised far away from the bulk of the pos-
ment).
terior density (first experiment).
summary of our experiments is shown in Figure 11 where we run the SG-Zig-Zag algorithm
with different mini-batch sizes for a linear regression problem with 106 observations. As can
be seen, for a fixed CPU cost using a mini-batch size of 1 is the best – this is most strikingly
shown in the right-hand plot.
E Sticky Zig-Zag sampler
In what follows, we present the stochastic gradient sticky Zig-Zag (SG-SZZ) sampler which
approximatesthestickyZig-Zag(Bierkenset al.(2023a)). Inthiscase,thetargetdistribution
is assumed to be of the form
d
(cid:89) 1
π(dx)∝exp(−U(x)) (dx + δ (dx ) (10)
i κ 0 i
i
i=1
20Algorithm 2 Stochastic gradient Zig-Zag (SG-ZZ) sampler
Require: (x,v)∈E, step size ε>0, time horizon T >0.
t=0, δ =ε
t
while t<T do
J ∼Unif(1,2,...,N)
τ ∼Exp(λi,J(z)), for i=1,...,d
i
τ =min τ , i∗ =argminτ
i=1,...,d i i
if τ <δ then
t
δ ←δ −τ, x←x+vτ, t←t+τ
t t
v ←R (v) {Flip i∗ velocity component}
i∗
else
x←x+vδ , t←t+δ
t t
δ =ε
t
Save (x,t)
end if
end while
Algorithm 3 Stochastic gradient Bouncy Particle Sampler (SG-BPS)
Require: (x,v)∈E, step size ε>0, time horizon T >0.
t=0, δ =ε
t
while t<T do
J ∼Unif(1,2,...,d)
τ ∼Exp(λJ(z))
if τ <δ then
t
v(new) =FJ(v) {Velocity reflection}
x
δ ←δ −τ, x←x+vτ, t←t+τ
t t
v =v(new)
else
x←x+vδ , t←t+δ
t t
δ =ε
t
Save (x,t)
end if
end while
where U(x) =
(cid:80)N
U (x). With simple algebra, it is not difficult to see that the taregt
j=1 j
measure given by a smooth log-likelihood
ℓ(x)=(cid:80)N
ℓ (x) and spike and slab prior
i=1 i
π (dx)=⊗d (1−w )π (x )dx +w δ (dx ) (11)
0 i=1 i i i i i 0 i
is of the form of equation (10), with
d
U (x)=C−ℓ (x)− 1 (cid:88) log(π (x )), κ = 1−w iπ (0),
i i N j j i w i
j=1
for some constant C indepent from x. In algorithm 4, we present the SG-SZZ as a minor
modification of the SG-ZZ, enabling to approximate measures which are of the form of (10).
21Figure 11: Left panel: error in the standard deviation estimation of each coordinate E(h,d)
(y-axis) against the step-size divided by the mini-batch size h (x-axis, on a log-scale). We
n
run all algorithms for the same time horizon so that h is a proxy for the inverse of the
n
running time of each algorithm. Middle panel: traces of one coordinate relative of SG-ZZ
with step-size h = 10−3 and mini-batch size equal to 1 (blue), 10 (red), 102 (green). All
algorithms were initialised out of stationarity. The star symbols indicate the location of each
Markov chain after 25×103 gradient evaluations. Right panel: traces normalised by the
mini-batch size.
Algorithm 4 Stochastic gradient sticky Zig-Zag (SG-SZZ) samplers
Require: (x,v,s)∈E, step size ε>0, time horizon T >0, sticky parameter κ∈(0,∞)d.
t=0, δ =ε
t
vc =v {Copy of velocity}
A={1,2,...,d}, Ac =∅
while t<T do
J ∼Unif(1,2,...,d)
τ =min {τ ∼Exp(λi,J(z))}
i∈A i
τ⋆ =min {t⋆: x +v t =0}
i∈A i i i i
τ◦ =min {t◦ ∼Exp(k )}
i∈Ac i i
if τ <min(δ ,τ⋆,τ◦) then
t
x=x+vτ, t=t+τ, δ ←δ −τ
t t
v ←R (v) where j :=argmin {τ }
j i∈A i
else if τ⋆ <min(δ ,τ◦) then
t
x=x+vτ⋆, t=t+τ⋆, δ ←δ −τ⋆
t t
Ac ←j⋆ where j⋆ :=argmin {τ⋆}
i∈A i
v =0 {Stick }
i
else if τ◦ <δ then
t
x=x+vτ◦, t=t+τ◦, δ ←δ −τ◦
t t
A←j◦ where j◦ :=argmin {τ◦}
i∈Ac i
v =vc {Unstick}
i i
else
x=x+vδ , t=t+δ
t t
δ =ε
t
Save (t,x)
end if
end while
22F Logistic regression
F.1 Stein discrepancy kernel
The Stein discrepancy kernel evaluated on K sample points with dimenision d is defined as
(cid:118)
(cid:88)d (cid:117) (cid:117)(cid:88)K κ k(x(i),x(j))
(cid:116)
K2
k=1 i,j=1
where
κ (y,z)=∂ U(x)∂ U(y)κ(x,y)+∂ U(x)∂ κ(x,y)+∂ U(y)∂ κ(x,y)+∂ ∂ κ(x,y).
k xk yk xk yk yk xk xk yk
FollowingNemethandFearnhead(2021)Section4, wechooseκ(x,y)=(c2+∥x−y∥2)β, β ∈
(−1,0), c > 0. Gorham and Mackey (2017) shows that this metric is able to detect both
poor mixing and bias from the target distribution.
F.2 Further simulations
We compute the Stein discrepancy kernel relative to the traces obtained with SGLD with 1,
10, 100 minibatch sample points and SG-ZZ and SG-BPS. We run each algorithm varying
h and we plot in Figure 12, top panel the results. Figure 12, bottom panel, displays the
loss function of the samplers above and additionaly SG-SZZ with spike-and-slab prior as
in Equation (11) with w = 0.5, i = 1,2,...,d. The loss function has been averaged over
i
M =10 random permutation of train and test datasets, as described in Section 4.1 of the
main manuscript. For this model, we set the loss function of Equation (6) equal to
(cid:18) (cid:19) (cid:18) (cid:19)
1 1
ℓ(X ,y ,x)=y log +(1−y )log 1− .
i i i 1+exp(−⟨x,X ⟩) i 1+exp(−⟨x,X ⟩)
i i
Figure 12: Stein discrepancy kernel (left panel) and average loss function on M =10 random
permutation of train and test sets (right panel) for the logistic regression as a function of the
stepsize h (x-axis in log-scale). Dashed lines corresponds to SGLD algorithms, solid lines to
SG-PDMPs. For SGLD, the Stein Discrepancy kernel was finite only for h≤10−5.
23G Bayesian neural networks
In this section, we show the trace of the loss function (Figure 13) and report the average
loss on the test sets relative to the traces obtained by running SGLD, SG-ZZ, SG-BPS,
SG-SZZ on M =3 different permuation of train and test sets for the other 2 datasets from
UCI machine learning repository2 labeled as ‘concrete’ and ‘kin8nm’ (Table 3) which have
respectively N =1031,N =45731 and p=8,p=9 covariates.
Figure 13: Trace of the loss function for each sampler for different step-sizes h relative to
the first permutation of the training and test set of the dataset ‘concrete’ (left panel) and
‘kin8nm’ (right panel). SGLD was unstable for h=10−3 for the dataset ‘concrete’ and for
h=10−3,10−4 for the dataset ‘kin8nm’.
Concrete h
sampler 10−3 10−4 10−5 10−6
SGLD NaN 0.5219 0.4625 0.4513
SG-ZZ 0.5103 0.4803 0.4526 0.4739
SG-BPS 0.4681 0.4606 0.4759 0.4885
SG-SZZ 0.5167 0.5017 0.4932 0.4877
Kin8mn h
sampler 10−3 10−4 10−5 10−6
SGLD NaN NaN 0.4480 0.4143
SG-ZZ 0.4536 0.4465 0.4224 0.4250
SG-BPS 0.4257 0.4272 0.4230 0.4460
SG-SZZ 0.4520 0.4375 0.4233 0.4506
Table3: Averagelossforeachsamplerfordifferentvaluesofstepsizehrelativetothedataset
‘concrete’ (left table) and ‘kin8mn’ (right table). In bold the best performance and italic the
worst performance given a step size h.
2https://archive.ics.uci.edu/
24