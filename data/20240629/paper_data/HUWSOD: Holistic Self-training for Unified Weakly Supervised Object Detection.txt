HUWSOD: Holistic Self-training for Unified Weakly
Supervised Object Detection
Liujuan Cao1, Jianghang Lin1, Zebo Hong1, Yunhang Shen2*, Shaohui Lin3,
Chao Chen1, Rongrong Ji1
1Media Analytics and Computing Lab, School of Informatics, Xiamen University, 361102,
China.
2Youtu Lab, Tencent, 200233, China.
3School of Computer Science and Technology, East China Normal University, 200062,
China.
*Corresponding author(s). E-mail(s): shenyunhang01@gmail.com;
Contributing authors: caoliujuan@xmu.edu.cn; hunterjlin007@stu.xmu.edu.cn;
debology@stu.xmu.edu.cn; shaohuilin007@gmail.com; chenchao.tencent@gmail.com;
rrji@xmu.edu.cn;
Abstract
As an emerging problem in computer vision, weakly supervised object detection (WSOD) aims to
use only image-level annotations to train object detectors. Most WSOD methods rely on traditional
objectproposalstogeneratecandidateregionsandareconfrontedwithunstabletraining,whicheasily
gets stuck in a poor local optimum. In this paper, we propose a unified and high-capacity WSOD
networkwithholisticself-trainingframework,termedHUWSOD,whichisself-contained,andrequires
no external modules or additional supervision. To this end, the proposed framework innovates in
two perspectives, i.e., unified network structure and self-training scheme, both of which are rarely
touched in WSOD before. First, we design a self-supervised proposal generator and an autoencoder
proposalgeneratorwithmulti-rateresamplingpyramidtohypothesizeobjectlocations,whichreplace
thetraditionalobjectproposalsandenablethestrictlyend-to-endtrainingandinferenceforWSOD.
Second,weintroduceholisticself-trainingschemethatconsistsofstep-wiseentropyminimizationand
consistency-constraintregularization,whichrefinebothdetectionscoresandcoordinatesprogressively,
as well as enforcing consistency to match predictions produced from stochastic augmentations of
the same image. Extensive experiments on PASCAL VOC and MS COCO show that the proposed
HUWSODperformscompetitivelytothestate-of-the-artWSODmethods,whilegettingridofoffline
proposals and additional data. Moreover, the upper-bound performance of HUWSOD with class-
agnosticground-truthboundingboxesapproachesFasterR-CNN,whichdemonstratesthatHUWSOD
is capable to achieve fully-supervised accuracy. Our work also brings one important finding that
randominitializedboxes,althoughdrasticallydifferentfromtheofflinewell-designedobjectproposals,
arealsoeffectiveobjectcandidatesforWSODtraining.Thecodeisavailableat:https://github.com/
shenyunhang/HUWSOD.
Keywords:Objectdetection,Weaklysupervisedlearning,Objectproposalgeneration
1
4202
nuJ
72
]VC.sc[
1v49391.6042:viXra1 Introduction
efficient. Recent endeavors in Tang et al (2018);
SinghandLee(2019);Chengetal(2020)proposed
Last decade has witnessed rapid advances in
toimproveobjectproposalsespeciallyforWSOD.
object detection Girshick (2015); Cai and Vas-
However, those methods are still not in an end-
concelos (2019); He et al (2020); Girshick (2015),
to-end fashion, which simply ensemble traditional
which are accommodated by open-source object
object proposals Tang et al (2018) and motion
datasets such as PASCAL VOC Everingham
segmentation Singh and Lee (2019) with addi-
et al (2010) and MS COCO Lin et al (2014).
tional video dataset Cheng et al (2020), respec-
Despite this great success, the expensive object
tively.Besides,WSODissensitivetoinitialization,
annotations have long plagued the scalable and
thus how to produce high-quality proposals in
real-world application of existing fully super-
the early training stage to alleviate the instabil-
vised object detection (FSOD). To reduce heav-
ity remains unexplored. On the other hand, the
ily instance-level annotations, i.e., bounding-box
existing WSOD methods use multi-scale image
and dense-pixel labels, one promising poten-
pyramids to remedy the scale-variation prob-
tial is weakly supervised learning that trains
lem during training and inference, which however
object detectors from only image-level supervi-
neglect how to design the internal network archi-
sions that indicating the presence or absence
tecture to strengthen single-image proposals. Sec-
of objects, termed weakly supervised object
ond, most instance refinement methods are based
detection (WSOD). Most cutting-edge WSOD
on entropy minimization principle with pseudo
paradigms follow a two-phase training procedure,
labels Diba et al (2017); Tang et al (2017), i.e.,
which consists of object mining and instance
usinghigh-confidentpredictionaspseudo-ground-
refinement phases, as illustrated in Fig. 1a. The
truthboundingboxes,whichhowevermaynotwell
object mining phase is commonly formulated as
cover objects and are unstable Gao et al (2019),
an image classification task via multiple-instance
thus over-fitting to incorrect pseudo-labels. One
learning(MIL)Amores(2013),whichtreatsinput
way to reduce mislocalizations is bounding-box
images as bags and candidate regions of the
regression Gao et al (2018); Zeng et al (2019);
images as instances Cinbis et al (2017); Wang
Fang et al (2020); Ren et al (2020). However,
et al (2015), i.e., a positive bag contains at
those methods either require super-pixel evidence
least one positive instance for the target category
and additional supervision to fine-tune bounding
and all instances in negative bags are negative
boxes or fail to achieve the trade-off between pre-
samples. Typically, those candidate regions are
cision and recall in different branches of instance
estimated offline from images via traditional pro-
refinement, which also easily gets stuck in a poor
posal methods, such as selective search (SS) Van
local minimum. Meanwhile they only consider a
De Sande et al (2011) and edge boxes (EB) Zit-
one-offunidirectionalconnectionbetweeninstance
nick and DollÂ´ar (2014). Then, instance-level MIL
refinement branches, i.e., taking predictions from
classifiers Amores (2013) are trained to distin-
the preceding branch to supervise the succeeding
guish candidate regions among objects and back-
one, which neglects the mutual benefits between
ground Bilen and Vedaldi (2016); Kantorov et al
MIL classifiers and instance refinement.
(2016); Wei et al (2018). After obtaining poten-
In this paper, we propose a holistic self-
tial object instances, instance refinement phase is
training based, unified weakly-supervised object
kickedintobuildmultipleparallelbranchestofur-
detection framework, termed HUWSOD, which
ther minimize classification entropy, i.e., encour-
innovates in two essential prospects, i.e., unified
aging the model to produce higher-confidence
network structure and holistic training scheme,
results,andadjustobjectlocalizationbyusingthe
bothofwhicharerarelytouchedinWSODbefore.
precedingpredictionsaspseudotargetsTangetal
The proposed HUWSOD framework is illustrated
(2017); Yang et al (2019a).
in Fig. 1b. To design a unified network struc-
Although the above paradigm has achieved
ture, we focus on learnable object proposals and
promising results, therestill exist two major chal-
make use of multi-scale information in WSOD
lenges in WSOD. First, object proposal modules
setting. First, we propose two object proposal
work in an external manner, which is indepen-
generators,namelyself-supervisedobjectproposal
dent of the object detection learning and less
2(a) Common WSOD framework. (b) The proposed HUWSOD framework.
Fig. 1: An illustration of common WSOD (a) and the proposed HUWSOD framework (b). Our key
innovations compared to existing methods include end-to-end object proposal generators and holistic
self-training scheme.
generator (SSOPG) and autoencoder object pro- to reduce entropy and bootstraps the quality of
posal generator (AEOPG), both of which are predicted bounding boxes. And CCR enforces
end-to-end training to hypothesize object loca- consistency to match predicted boxes produced
tions without additional supervision. Specifically, from stochastic augmentations, i.e., views, of the
SSOPGleveragesfinalresultspredictedbyHUW- sameimage.Wealsoexploretheexponentialmov-
SOD as objectness classification and regression ing average scheme to propagates partial learned
targetsinaself-supervisedfashion,whileAEOPG knowledge among object mining and instance
is designed to capture salient objects by learn- refinement to improve the performance of all
ing low-rank decomposition of feature maps via branches.
an unsupervised autoencoder. Second, to further We conduct extensive experiments on PAS-
increase proposal diversities and enrich object CAL VOC Everingham et al (2010) and MS
features, we construct a multi-rate resampling COCOLinetal(2014)toelaboratelyconfirmthe
pyramid (MRRP) to aggregate multi-scale con- effectivenessofourmethod.Inparticular,thepro-
textual information with various dilation rates in posedHUWSODachievescompetitiveresultswith
backbone networks, which is the first network- the state-of-the-art WSOD methods while not
embedded feature hierarchy to handle scale varia- requiring external modules or additional supervi-
tioninWSOD.DifferentfromcommonFSODthat sion. Our ablation study also includes basic fully-
attaches new parameters to build feature pyra- supervised learning experimental setting that
mids Lin et al (2017), MRRP does not need to are often ignored or not reported when new
learn new parameters and thus avoids over-fitting WSOD methods are proposed. The results shows
by sharing the same parameters of pre-trained that the upper-bound performance of HUWSOD
backbones. We further introduce a holistic self- with class-agnostic ground-truth bounding boxes
training scheme including step-wise entropy mini- approachesFasterR-CNNRenetal(2017),which
mization (SEM) as well as consistency-constraint demonstratesthatHUWSODiscapabletoachieve
regularization (CCR), which seamlessly reduces fully-supervised accuracy. We also quantitatively
entropy as the dominant WSOD paradigms while hint that using traditional proposal modules in
integrating consistency constraint. Our motiva- WSOD is a historical workaround so far when
tionofCCRliesinthesmoothnessassumptionvan the community mainly focused on object min-
Engelen and Hoos (2020): Two object instances ing and instance refinement phases. In addition,
that lie close together in the input space should traditional proposals have been largely thought
have the same label and minor variations in the of as a âfreeâ resource, attributed to a series
input space should only cause minor variations of open-source codes and benchmarks. However,
in the output space. This assumption is com- along with ever-increasing data and computa-
monly used in semi-supervised learning, such as tion resources, e.g., large-scale web images and
pseudo-ensemble agreement regularization Bach- high-capacity models, learning end-to-end object
manetal(2014),buthaslittleexploreforWSOD. proposal generators becomes more feasible, espe-
In this paper, we extend the smoothness assump- ciallywhentrainingdatacontainsinformationlike
tion transitively to the learning of weakly-labeled image-levellabels,thatcannotbewelldigestedby
data. In details, SEM progressively selects high- traditionalobjectproposalapproaches.HUWSOD
confidence object proposals as positive samples is able to learn effective object candidates from
3random initialized boxes with only image-level labels, which alternates between localizing object
labels, which enables strictly end-to-end training instances and training an appearance model.
and inference at scale. The contributions of this Given a target class, positive bags are assumed
work are concluded as follows: to contain at least one instance of the object,
â¢ We propose a unified and high-capacity whilenegativebagsonlyhavebackgroundsamples
weakly supervised object detection (WSOD) or other objects. The goal is to train instance-
framework, which is self-contained and levelclassifiersthatdistinguishobjectpresenceor
requires no external modules in online infer- absence. Early efforts, such as multi-fold train-
ence nor additional supervision in offline ingCinbisetal(2017),objectivefunctionsmooth-
training, to develop a general WSOD model. ing Song et al (2014), and MIL constraint relax-
â¢ We design two object proposal generators ation Wang et al (2015), focused on directly solv-
with self-supervised and autoencoder-style ing the classical MIL problem. Bilen et al. Bilen
unsupervised learning, respectively, which and Vedaldi (2016) selected proposals by parallel
is cooperated with a multi-rate resampling detection and classification networks in the deep
pyramid to utilize contextual information. learning era. Contextual information Kantorov
â¢ We introduce a holistic self-training scheme etal(2016),gradientmapShenetal(2019b),and
to seamlessly reduces classification entropy semantic segmentation Wei et al (2018) are lever-
while integrating consistency constraint, aged to learn outstanding object proposals. Since
which make full use of weakly-labeled data. the objective function is not convex, the opti-
Compared to our conference version in Shen mization is easily trapped in a local optimum. In
et al (2020b), we first extend the original self- thispaper,weexploreexponentialmovingaverage
supervised object proposal generator as well as scheme that enables MIL classifiers to aggregate
a complementary object proposal generator using information from the subsequent instance refine-
an unsupervised autoencoder. We further exploit ment phrase, which alleviates local minimum and
consistency-based self-training with exponential avoids over-fitting. The instance refinement phase
moving average, which is widely overlooked in is typically posed as a self-training problem by
the existing literatures. Equipped with autoen- implicitly constructing pseudo labels from high-
coder object proposal generator and consistency- confidence predictions on weakly-labeled data.
constraint regularization, HUWSOD significantly Thus, learning instance refinement can be viewed
outperforms the original method Shen et al as a form of entropy minimization, which reduces
(2020b). Detailed analysis is further given with the density of data points at the decision bound-
an in-depth comparison to the most recent aries and encourages the network to make low-
related works. Finally, we provide more exper- entropy predictions. Tang et al. Tang et al (2017)
iment results to shown the effectiveness of our and Jie et al. Jie et al (2017) took confident can-
method. The rest of our paper is organized as didate proposals from the object mining phase
follows. In Sec. 2, related work is introduced. In to learn instance-level instance for acquiring tight
Sec. 3, the details of our method are described. positive objects. Different strategies Tang et al
Elaborateexperimentsandanalysesareconducted (2020); Kosugi et al (2019); Zhang et al (2018a);
inSec.4.Finally,conclusionsandfuturedirections Yang et al (2019a); Zhang et al (2018b); Chen
are discussed in Sec. 5. et al (2020); Ren et al (2020) are also proposed
to estimate pseudo targets and assign labels to
2 Related Work proposals. Some methods simultaneously learn
above two-phase modules with randomness min-
2.1 Weakly Supervised Object imization Li et al (2016b); Wan et al (2019b),
proposal adjacent relationship Wan et al (2019a);
Detection
Zhang et al (2021), utilizing uncertainty Arun
Most cutting-edge WSOD procedures consist of et al (2019); Gao et al (2019), generative adver-
two phases: object mining and instance refine- sarial learning Shen et al (2018b) and knowledge
ment. The object mining phase is formulated distillationZengetal(2019).Collaborationmech-
using multiple-instance learning (MIL) to implic- anisms are also introduced to take advantage
itlymodellatentobjectlocationswithimage-level
4of the complementary interpretations of differ- et al (2020) combined selective search Van De
ent weakly supervised tasks Shen et al (2019a, Sande et al (2011) and gradient-weighted class
2021a,b) and models Yan et al (2019). Our work activation map to generate proposals. Tang et
proposes to combine consistency-based constraint al. Tang et al (2018) used a lightweight WSOD
with entropy minimization as a holistic self- network Tang et al (2017) to refine the coarse
training framework for WSOD, which makes full proposals generated by edge boxes Zitnick and
use of weakly-labeled data available to enhance DollÂ´ar(2014)onedge-likeresponsemaps.Singhet
robustness. al. Singh and Lee (2019) learned object propos-
With the output of the above two-phase als from motion information in weakly-labeled
paradigm, a separated detector can also be videos.Dongetal.Dongetal(2021)leveragedthe
trained to further improve performance. Many boundingboxregressionknowledgefromauxiliary
efforts Zhang et al (2018b); Ge et al (2018) mine dataset with instance-level annotations. However,
pseudo-ground-truth bounding boxes from pre- these methods are not in an end-to-end manner
trained WSOD models for FSOD. Recent, some and still need traditional object proposals Tang
works Huang et al (2022); Sui et al (2022) also et al (2018), motion segmentation Singh and Lee
manage to select high-quality predictions to learn (2019), and additional video dataset Cheng et al
semi-supervised object detectors. The proposed (2020). Instead in this work, the proposed self-
HUWSOD is a generic WSOD framework and is supervised generator and autoencoder proposal
compatible with those orthogonal improvements generator are end-to-end trainable, requiring no
by taking our well-trained HUWSOD models as external modules nor additional information.
the WSOD part in their framework.
Another line of research exploits knowledge 2.3 Handling scale variation
transfer for cross-domain adaptation Cao et al
Representing features at multiple scales is cru-
(2021); Zhong et al (2020); Hou et al (2021);
cial for object detection models to handle scale
Xu et al (2022), i.e., adapting the fully super-
variation in challenging conditions. Most WSOD
viseddetectorstoanoveltargetdomainwithonly
methods adopt image pyramids to detect objects
image-level annotations. Methods in Fang et al
acrossscalesduringtrainingandinferencetorem-
(2020); Xu et al (2019); Yang et al (2019b); Shen
edy scale variation. However, the image pyramid
et al (2020a); Chen et al (2021); Vo et al (2022)
method increases the inference time and does
trained object detectors from different supervi-
not consider building internal network structures.
sions. Chen et al. Chen et al (2021) learned from
SNIP Singh et al (2018) selectively trained the
small fully-labeled images and large point-labeled
objects of appropriate sizes in each image scale.
images. In contrast to the above work, the pro-
However, SNIP is not adaptable to WSOD due
posed HUWSOD does not use any additional
to the lack of instance-level annotations. Another
annotation or external proposal models.
stream of utilizing multi-scale information is to
considerbothlowandhigh-levelinformation,e.g.,
2.2 Object proposal generation
U-NetRonnebergeretal(2015)andFPNLinetal
Object proposal methods aim to generate candi- (2017) However, it requires attaching new layers
date regions for object detection and image seg- to build feature pyramids and may converge to
mentationmodels.Traditionalmethodsareeither an undesirable local minimum in WSOD. In this
based on grouping super-pixels, e.g., selective paper, we introduce a simple yet efficient multi-
search Van De Sande et al (2011) and multiscale rateresamplingpyramidarchitecturetoaggregate
combinatorial grouping ArbelÂ´aez et al (2014), or multi-scale contextual information, in which each
based on sliding windows, e.g., edge boxes Zit- level shares the same parameters and predicts
nick and DollÂ´ar (2014). In most existing WSOD objects independently before aggregation. To the
works, object proposal methods were typically best of our knowledge, this is the first attempt
integrated as external modules independent of tolearnscale-invariantnetwork-embeddedfeature
training the detection networks. Few literatures hierarchy in WSOD.
exploittrainableobjectproposalgenerationunder
weakly supervised settings. Cheng et al. Cheng
52.4 Consistency Regularization In subsection 3.4, step-wise entropy minimiza-
tion(SEM)andconsistency-constraintregulariza-
Consistency regularization minimizes variation in
tion(CCR)areintroducedtorefinebothdetection
the outputs of models when they are subject to
scores and coordinates progressively and stably.
noise on their inputs or internal state Bachman
et al (2014). Consistency regularization has been
3.1 The Preliminary
successfullyappliedtosemi-supervisedimageclas-
sification Sohn et al (2020a); Xie et al (2020); We first introduce the basic WSOD method
Zhao et al (2022), object detection Liu et al which the proposed HUWSOD framework is built
(2021);Sohnetal(2020b);Chenetal(2022);Guo upon.Givenaninputimage,mostmethodsfirstly
et al (2022) and semantic segmentation Liu et al use traditional proposal algorithms, e.g., selec-
(2022); Fan et al (2022) to encourage task pre- tive search Van De Sande et al (2011) and edge
dictionconsistency,whereunlabeledexamplesare boxes Zitnick and DollÂ´ar (2014), to extract candi-
usedasregularizerstohelpsmoothdatamanifold. dateboundingboxes.Thenthecorrespondingpro-
Inspired by STAC Sohn et al (2020b), we com- posalfeaturesarecomputedonthelastimagefea-
bine both consistency regularization and entropy turemapsF frombackbones,e.g.,VGGSimonyan
minimization in a holistic self-training framework and Zisserman (2015) and DRN Shen et al
for WSOD, which serves as a new way of lever- (2020c),viaRoIPoolRenetal(2017)layer.Lastly,
aging the weakly labeled data to find a smooth those proposal features are fed into the WSOD
manifold on which the dataset lies. Different from head,whichconsistsofobjectminingandinstance
the student-teacher based siamese architecture in refinement.
STACSohnetal(2020b),weexploreconsistency- ObjectminingphaseBilenandVedaldi(2016);
constraintregularizationamongdifferentbranches Kantorov et al (2016) forks the proposal fea-
within a single WSOD model, i.e., each branch tures into classification and detection streams,
is the student of the preceding branch and the producing two score matrices Xcls,Xdet â
teacher of the subsequent branch, simultaneously. ânproÃncat by two fully-connected layers, respec-
Our work is also related to CASD Huang et al tively. npro and ncat denote the number of
(2020), which proposed self-distillation to learn objectproposalsandcategories,respectively.Both
consistent attention on low-to-high feature layers. score matrices are normalized by the softmax
Different from attention-based feature learning in Ï(Â·) over categories and proposals, respectively:
C teA ntS rD eguH lu aa rin zg atie ot na ol n( d2 e0 t2 e0 ct) i, onwe prc eo dn icd tu ioc nt s,c won hs ii cs h- Ï(Xcls) p,c = eX pcl ,s c/(cid:80)n i=ca 1t eX pcl ,s i and Ï(Xdet) p,c =
explicitly aggregates supervision within WSOD
eX pd ,e ct /(cid:80)n i=pr 1o eX id ,e ct . In this way, Ï(Xcls)
p,c
esti-
mates the probability of the pth proposal belong-
networks, such as the different pseudo labels pro-
ing to the cth category, and Ï(Xdet) indicates
duced under various image transformations and p,c
thecontributionofthepthproposaltoimagebeing
detection branches.
classified to the cth category. Thus, we interpret
Ï(Xcls)asatermthatpredictscategories,whereas
3 The Proposed Method
Ï(Xdet) selects regions. Then the element-wise
product of the output of these two streams is
In this section, the proposed HUWSOD frame-
again a score matrix: Xs = Ï(Xcls) â Ï(Xdet).
work is introduced. In subsections 3.1 and 3.2,
This equation can be considered as re-weighting
we first introduce the preliminary of the basic
classification results with localization scores. To
WSOD and the proposed HUWSOD framework.
acquire image-level classification scores, we apply
In subsection 3.3, self-supervised proposal gener-
a sum pooling: y =(cid:80)npro Xs . Note that y is a
ator (SSOPG) and autoencoder object proposal c p=1 p,c c
weighted sum of the element-wise product of soft-
generator (AEOPG) are introduced to hypoth-
maxnormalizedscoresoverallregionsandfallsin
esize candidate object locations. A multi-rate
the range of (0,1). We build binary cross-entropy
resampling pyramid (MRRP) is also constructed
to aggregate multi-scale contextual information.
6Fig. 2: The overall framework of the proposed HUWSOD. We replace the traditional object proposals
with a self-supervised object proposal generator (SSOPG) and an autoencoder object proposal gener-
ator (AEOPG), which hypothesize object locations in an end-to-end manner. We further construct a
multi-rate resampling pyramid (MRRP) to connect backbone and WSOD head, which strengthen the
progress of object proposals. The optimization algorithm is based on a holistic self-training scheme that
consists of step-wise entropy minimization (SEM) and consistency-constraint regularization (CCR).
objective function L as follows: head,wherencat+1denotesncat objectcategories
OM
and 1 background.
ncat During training, we utilize the detection
(cid:88)(cid:110) (cid:111)
L OM = t clogy c+(1ât c)log(1ây c) , (1) results from the (r â 1)th branch to generate
c=1 classification labels tr âânpro and regression tar-
gets Gr â ânproÃ4 for object proposals in the
where t â {0,1}ncat is the image-level labels, and rth branch. Noted that we make use of the score
t i is the ground-truth labels of whether an object matrixXs fromtheobjectminingphasetosuper-
of the cth category is presented in input image. vise the first branch, i.e., r = 1. In detail, for
The object mining phase forms the WSOD the cth category that t = 1, we select all boxes
c
problem as multiple-instance learning and implic- whose class confidences from the previous predic-
itly classifies the region proposals with only tions are greater than the pre-defined threshold,
image-level labels. In this way, the model can i.e., 0.5, as pseudo ground truths. Especially, if
correctly classify an image even only âseeâ a no box is selected, we seek the box with the high-
part of the object, and as a result, the proposal est score. With the above pseudo-ground-truth
scores may not correctly indicate probabilities boxes, we assign positive/negative labels for each
that proposals properly cover the entire objects proposal and form the training targets tr and
of target categories. To further reduce the mis- regressiontargetsGr followingFasterR-CNNRen
localization in instance refinement, we tweak a et al (2017). Thus, the corresponding objective
similar idea from instance refinement Tang et al function is:
(2017) and bounding-box regression Yang et al
(2019a); Zeng et al (2019) to reduce classification nirf npro
(cid:88)(cid:88)
entropy and adjust object location. To this end, L IR = y tr pL CE(S pr,tr p)+
, (2)
instance refinement contains multiple branches, r=1p=1
and each branch has proposal classification and 1[tr >0]y L (Br ,Gr)
bounding-box regression networks, which enables p tr p SL1 p,tr p p
the refinement of both bounding-box scores and wherenirf isthebranchnumberininstancerefine-
coordinates. In details, it produces new classifi-
ment L is the softmax cross-entropy loss, and
cation scores Sr â ânproÃ(ncat+1) and bounding CE
L is the smooth L1 loss Girshick (2015). The
boxes Br â ânproÃncatÃ4 for the rth refinement inS dL i1 cator bracket indicator function 1[tr > 0]
p
7evaluates to 1 if tr > 0 and 0 otherwise. Noted WSOD network with strictly end-to-end training
p
that tr = 0 denotes background category. From andinference,andiscapabletoachievepromising
p
another perspective of semi-supervised learning, results even under fully/semi-supervised settings.
the instance refinement exploits the noisy predic- The overall loss function is:
tions from the teacher, i.e., object mining phase,
and learn student detectors. Thus, it works by L=L +L +L +L , (3)
SSPOG AEOPG OM IR
selecting pseudo ground-truths from current pre-
dictionsandlearninganewdetectionbranchwith where L and L are the loss functions
SSOPG AEOPG
both classification and regression iteratively. Pro- of the proposed SSOPG and AEOPG, which will
posal scores inferred from weak supervision are be detailed in the following subsection. SEM self-
propagatedtotheirspatiallyoverlappedproposals training provides a stepwise procedure to mini-
to calibrate the classifiers. In testing, the average mizethepredictionentropyofinstancerefinement
output of all branches is used. for pseudo-learning, which aims to consider the
trade-off between precision and recall in differ-
3.2 The HUWSOD Framework ent refinement branches. And CCR self-training
imposes consistency-constraint regularization to
Based on the above basic framework, we remove
thepredictionsproducedfromstochasticaugmen-
theexternalmoduleoftraditionalobjectproposal
tations of input images. Thus, our scheme is a
and build learnable object proposal generators in
holistic self-training framework for WSOD which
a fully end-to-end manner. Then we construct
incorporates ideas from the dominant paradigms
a network-embedded feature hierarchy between
of weakly supervised learning Zhou (2018), i.e.,
the backbone and WSOD head to handle large
entropy minimization Miyato et al (2018); Berth-
scalevariations.Finally,weproposeaholisticself-
elot et al (2019), consistency regularization Xie
training scheme to replace vanilla pseudo-labeling
et al (2020); Huang et al (2020) and exponen-
training.
tialmovingaverageTarvainenandValpola(2017);
The overall framework of the proposed HUW-
Laine and Aila (2017). During inference, the pro-
SOD is shown in Fig. 2. First, given an input
posed AEOPG, SEM, and CCR self-training are
image, conventional and strong augmentations Ï
safely removed, and only MRRP and SSOPG
are applied to it separately, and scale-invariant
remain with the basic WSOD network.
full-image feature maps are extracted from the
backbone with a multi-rate resampling pyra-
3.3 Unified Weakly Supervised
mid (MRRP). MRRP aggregates multi-scale con-
Detection Network
textual information at the top of the back-
bone, which leverages a set of different recep- 3.3.1 Self-supervised object proposal
tive fields to remedy scale-variation problem.
generator (SSOPG)
Second, self-supervised object proposal genera-
tor (SSOPG) and autoencoder object proposal We propose an anchor-based self-supervised
generator (AEOPG) simultaneously predict a set object proposal generator, which takes full-image
of high-quality and high-confidence object pro- feature maps as input and outputs a set of rect-
posals, which is followed by the RoIPool layer to angular object proposals, each with an objectness
generate proposal features. SSOPG learns object score. Generating anchors with the sliding win-
candidate regions from the final predictions of dow manner in feature maps has been widely
HUWSOD, and AEOPG captures the salient adopted by anchor-based various detectors Ren
objectsinimagebymodelinglow-rankapproxima- etal(2017);CaiandVasconcelos(2019).Anchors
tion that retains important values and correlative are regression references and classification candi-
relationships in full-image feature maps. Third, dates to predict object proposals. SSOPG uses a
the object mining phase outputs initial detection smallfullyconvolutionalnetworktomapeachslid-
scores, and the multi-branch instance refinement ing window anchor to a low-dimensional feature,
is involved to refine both the scores and coor- as in Ren et al (2017). To this end, SSOPG has
dinates of proposals to bootstrap the quality of a 3Ã3 convolutional layer with 256 channels fol-
predicted results. Thus, our model is a unified lowed by two siblings 1Ã1 convolutional layers
8forobjectnessclassificationandregression,respec-
tively. Formally, we denote nanc as the number of
anchorsineachlocation,andhandwastheheight
andwidthoffeaturemaps,respectively.Thus,the
regression layer has 4nanc outputs encoding the
coordinatesofnancboxes,andtheobjectnesslayer
outputs nanc scores that estimate the probability
Fig. 3: Autoencoder architecture in AEOPG.
of the object for each proposal. Given a feature
maps with spatial size (hÃw), SSOPG outputs
objectproposalsBpro âânanchwÃ4 andobjectness
where L is the binary sigmoid cross-entropy
scoresSpro âânanchwÃ1.Weapplynon-maximum BCE
loss.
suppression(NMS)onBpro andkeepthetop-npro
object proposals.
3.3.2 Autoencoder object proposal
We leverage self-supervised learning to train
generator (AEOPG)
SSOPG with supervision created by HUWSOD
without additional human effort or external mod- Without ground-truth-bounding-boxes informa-
ules. Our intuition is that as WSOD can discover tion,SSOPGmaynotgeneratehigh-qualityobject
category-specific objects, it also has the poten- proposals in the early training stage and reduce
tial to learn objectness instances. To this end, we the learning efficiency of the subsequent object
attach a new objectness detection branch to the mining and instance refinement, which in turn
proposal features with two sibling fully-connected provides low-quality supervision for SSOPG. We
layers to predict objectness classification Sobn â observe that the output feature maps from self-
ânpro and regression Bobn â ânproÃ4, respec- supervised pre-trained backbones have high acti-
tively. Given the output from instance refinement vations around the spatial locations of salient
phase, i.e., scores Sr â ânproÃncat and bounding objects in images. And we also assume that fea-
boxes Br â ânproÃncatÃ4, we generate object- ture vectors at different spatial locations should
ness pseudo-ground-truth boxes as {Br |p = have high correlations if they belong to the same
p,c
argmax Sr ,c={i|t =1}},whereBr andSr category. Thus, reducing the dimension of feature
p p,c i p,c p,c
denote the pth predicted bounding box and score maps from the backbone by low-rank approx-
for the cth category. For each object proposal Bp, imation can find a representation of the data
we assign classification labels tobn and regression that retains the important values and correlative
p
targetsGobnwithanintersection-over-union(IoU) relationship.
p
ratio of Î»obn. To this end, we propose an autoencoder archi-
Finally, we further select the top-nobn pre- tectureforobjectproposals,whichtakesfullimage
feature maps as input and outputs high-quality
dicted boxes from objectness detection branch as
proposals, as shown in Fig. 3. Like all autoen-
pseudoground-truthstosuperviseproposalgener-
ator, where nobn = |{t |t = 1}|. We label object coders, AEOPG has an encoder E that project
i i
proposals Bp by an IoU ratio of Î»pro with Gpro to image feature maps F â âhwÃo from the back-
compute the classification labels tpro and regres- bone to a q-channel compressed representation,
siontargetsGpro foreachobjectprop posalBp.The i.e., E(F) â âhwÃq, and a decoder D that recon-
p
structs the original signal from the compressed
overall loss function of SSOPG is:
representation. We set oâ«q, and h and w as the
nobn heightandwidthoffeaturemaps,respectively.To
(cid:88)
L = L (Sobn,tobn)+L (Bobn,Gobn) prevent the autoencoder from learning the iden-
SSOPG CE p p SL1 p p
tity function, we leverage matrix decomposition
p
,
npro as low-rank regularization on latent representa-
+(cid:88) L (Spro,tpro)+L (Bpro,Gpro) tion to improve its ability to capture important
BCE p p SL1 p p
salientobjectsinimages.Theoveralllossfunction
p
(4) L AEOPG is formulated as:
L =L +L , (5)
AEOPG Encoder Decoder
9where L is the low-rank regularization loss
Encoder
andL isthereconstructionloss.Concretely,
Decoder
we project image feature maps F to the first q
principal components of low rank matrix Fq â
âhwÃq vialinearmatrixdecompositionalgorithm,
e.g., SVD, which is then binarized at a threshold
Fig. 4: Illustration of MRRP on the 4th and 5th
of0.Thus,thelossfunctionL isformulated
Encoder
stages of backbone. We set nstg =2, npls =3 and
as:
hw Î±vdl ={1,2,3}.
(cid:88)
L = L (E(F) ,Fq). (6)
Encoder BCE i i
i
The encoder E can be viewed as a form of stage, so this module does not affect the inference
non-linear dimensional reduction to find a low- speed.
dimensionalembeddingoffull-imagefeaturemaps
that preserves semantic correspondences across 3.3.3 Multi-rate resampling
them, which is guided by linear low-rank decom- pyramid (MRRP)
position. Thus, the larger the absolute value in
Aggregating multi-scale information is critical for
E(F) is, the higher the positive correlation will
detectors to exploit context and improve perfor-
be. Specifically, in the weakly-supervised learn-
mance in large scale variations. Existing WSOD
ing scenario, the positive correlation indicates the
methods leverage multi-scale image pyramids to
salient characteristic in images. Therefore, the
remedy scale-variation problem. However, it does
valuezeroisusedasanaturalthresholdfordivid-
ing E(F) and Fq into binary segmentation maps. not design specific network architecture to handle
thelargevariationofscales.Especially,weobserve
To capture salient objects, we binarize the low-
that directly employing an encode-decoder struc-
rankregularizedfeaturemapsE(F)atathreshold
ture, e.g., U-Net Ronneberger et al (2015) and
of0andcomputeallconnectedregions.Toachieve
FPN Lin et al (2017), does not achieve competi-
this,twopixelsareconnectedwhentheyareneigh-
tiveresultsinWSOD.Asitrequiresattachingnew
bors and have the same value. Then the tightest
layerstobuildfeaturepyramidsandmayconverge
bounding boxes of connected regions are consid-
to an undesirable local minimum.
eredasobjectproposalsforsalientobjects.Differ-
Inspired by spatial pyramid pooling He et al
ent from SSOPG which relies on WSOD models
(2015)infullysupervisedlearning,weconstructa
itself in a self-supervision manner, AEOPG cap-
multi-rate resampling pyramid (MRRP) to aggre-
tures the salient objects in low-rank feature maps
gate multi-scale contextual information, in which
decomposed by an unsupervised encoder-decoder
each level shares the same parameters. Our intu-
network. Additionally, to constrain the quality of
ition is that integrating information from other
the compressed feature maps, we input the com-
receptivefieldshelpswidenthescales,thusallevi-
pressedfeaturemapsintoadecodertoreconstruct
atingambiguitiesandreducinginformationuncer-
theoriginalfeaturemaps.Thus,L losscom-
Decoder
taintyinthelocalarea.Thus,weusealargerange
putes the squared difference between the input
of receptive fields to describe objects at different
and reconstructed feature maps.:
scales. Note that current backbone models com-
hw monly set receptive fields at the same size with a
(cid:88)
L = â¥F âD(E(F)) â¥ . (7) regularsamplinggridonafeaturemap.Therefore,
Decoder i i 2
we generalize trident block Li et al (2019) to iter-
i
atively replicate npls parallel streams for the last
We concatenate all object proposals from nstg stages of backbones, which share the same
SSOPG and AEOPG to extract proposal features structures and parameters, but with various dila-
during training, which is the input of the sub- tionratesÎ±vdr ={Î±vdr,...,Î±vdr}.Takingthe4th
i npls
sequent object mining and instance refinement and5thstagesofbackboneasanexampleinFig.4,
phases. Note that we only use AEOPG to gener- we first replicate the original 4th stage npls times
ate high-quality object proposals in the training with various dilation rates, and for each output
10feature map we repeat the replicating operation at separated IoU level, where nirf is the number
inthe5th stage.Finally,MRRPoutputs(nstg)npls of refinement branches. Thus, the loss function of
feature maps, and nstg is the number of MRRP Eq. 2 is replaced to:
stages.
We apply SSOPG and AEOPG on each fea- (cid:88)nirf n (cid:88)pro
L = y L (Sr,tr(Î»r))+
ture map separately to generate object proposals. SEM tr p CE p p , (8)
Thetopnpro proposalsfromSSOPGandallpro- r=1p=1
infer
posals from AEOPG are mapped to their feature [tr p(Î»r)>0]y tr pL SL1(B pr ,tr p,Gr p(Î»r))
maps to extract proposal features in the RoIPool
layer. Although it does not directly use the entire wheretr(Î»r)andGr(Î»r)aretheclassificationand
p p
resampling pyramid, scale-invariant features are regressiontargetsfortheithobjectproposalinthe
stilldistilledintothebackbonebyoptimizationof rth branchunderIoUthresholdofÎ»r,respectively.
the shared parameters. We restrict Î to be in ascending order, i.e.,
{Î»1 â¤ Â·Â·Â· â¤ Î»nirf}, which offers a good trade-
3.4 Holistic Self-Training with offbetweenprecisionandrecallamongrefinement
Weak Supervision branches.Astheformerbranchesestablishahigh-
recall set of positive samples, while the successive
3.4.1 Step-wise entropy
branches receive high-precision positive samples.
minimization (SEM)
Step-wise fashion guarantees a sequence of effec-
tive refinement branches of increasing quality. As
Most WSOD methods only apply instance refine-
positivesamplesdecreasequicklywithÎ,wesam-
mentTangetal(2017)torescoreobjectproposals,
ple each branch to keep a fixed proportion of
which may result in low-quality bounding boxes
positive and negative samples.
predicted. To explain, the bounding box predic-
tions heavily rely on the quality of candidate
3.4.2 Consistency-constraint
boxes generated by object proposal algorithms,
which limits further performance improvement. regularization (CCR)
Although recent methods Gao et al (2018); Zeng
Inspired by the effectiveness of consistency-based
et al (2019); Fang et al (2020); Ren et al (2020)
learning Bachman et al (2014), we propose
integratebounding-boxregressioninWSOD,they
a consistency-constraint regularization scheme
either require external modules or additional
for WSOD. Consistency-constraint regularization
supervision and fail to consider the trade-off
aims to enforce consistent predictions produced
between precision and recall in different refine-
from stochastic augmentations, i.e., views, of the
ment branches.
same input. Concretely, we construct the same
To reduce mislocalizations, we propose a step-
pseudo ground truths for different augmentations
wise entropy minimization strategy, which pro-
of the same images, thus imposing consistency-
gressivelyselectshigh-confidenceobjectproposals
constraint regularization to the learning process.
aspositivesamplestorefinebothdetectionscores
In detail, each branch in instance refinement is
and coordinates. Our intuition is that the for-
supervisedbythepseudogroundtruthsgenerated
mer branches in instance refinement have large
from the original images and the corresponding
ambiguity of selecting positive and negative sam-
branch. Thus, CCR objective function of the rth
ples, as their pseudo ground truths are noisy
instance refinement branch is:
and do not cover objects well. Thus, we step-
wisely learn instance refinement from low-quality nirf npro
(cid:88)(cid:88)(cid:88)
to high-quality positive samples. To this end, we L = y (I)L (Sr(Ï(I)),tr(I,Î»r))
first add bounding-box regression to each branch CCR tr p CE p p ,
ÏâT r=1p=1
of vanilla classifier refinement Tang et al (2017), +[tr(I,Î»r)>0]y (I)L (Br (Ï(I)),Gr(I,Î»r))
which can fine-tune both scores and coordinates p tr p SL1 p,tr p p
(9)
in all refinement branches. We use a series of IoU
where tr(I,Î»r) â ânpro and Gr(I,Î»r) â ânproÃ4
thresholds Î = {Î»1,...,Î»nirf} to label positive
are the classification and regression targets of the
and negative proposals and optimize each branch
11originalimageI intherth headunderIoUthresh- only image-level labels for training. MS COCO
old of Î»r, respectively. Specifically, the trans- 2014 consists of 80 object categories, which is
formation operation Ï from a candidate set of among the most challenging datasets for instance
augmentationsT isappliedonimageI togenerate segmentation and object detection. Our experi-
stochastic augmented images Ï(I). Sr(Ï(I)) and mentsuse83ktrain setwithimage-levellabelsfor
p
Br (Ï(I)) denote the classification and regres- training, and 41k val set for testing.
p,tr
p
sion predictions on augmented image Ï(I). Thus,
final objective function L in Eq. 3 combines 4.2 Evaluation Protocol
IR
Eqs. 8 and 9:
Two evaluation protocols are used for PASCAL
VOC: mean Average Precision (mAP) and Cor-
L =L +L . (10)
IR SEM CCR rect Localization (CorLoc). The mAP follows
PASCALVOCprotocoltoreportthemeanAPat
Different from consistency-based semi-
50% intersection-over-union (IoU) of the detected
supervised object detection Sohn et al (2020b)
boxeswiththeground-truthones.CorLocquanti-
that relies on student-teacher-based siamese
fiesthelocalizationperformancebythepercentage
architecture, the proposed method imposes
ofimagesthatcontainatleastoneobjectinstance
consistency-constraint regularization within a
with at least 50% overlap to the ground-truth
single network. In other words, each branch is the
boxes.Thelocalizationperformance,i.e.,CorLoc,
preceding of the previous branch and the teacher
is defined as predicting boxes when categories are
of the succeeding branch.
known, which is evaluated on the train+val set.
For each branch, besides providing supervi-
And detection performance, i.e., mAP, is defined
sionsignalstothesucceedingone,wealsoexplore
aspredictingcategoriesandboxessimultaneously,
propagating information to the preceding branch.
which is evaluated on the testing set. For MS
Therefore, we update branch weight Wr from the
COCO, we report COCO metrics, including AP
succeeding branch weight Wr+1 with exponential
at different IoU thresholds.
moving average (EMA). At ith iteration, we have
Wr(i) = Î²Wr(iâ1)+(1âÎ²)Wr+1(i). We per-
4.3 Implementation Details
form this information propagation from the last
branch of instance refinement to the detection We use VGG16 Simonyan and Zisserman (2015)
branch of object mining. Thereby, more accurate and WS-ResNet Shen et al (2020c) backbones,
pseudo targets lead to a positive feedback loop which is initialized with the weights pre-trained
betweeneachconsecutivebranch,resultinginbet- on ImageNet Jia et al (2009). We use synchro-
teraccuracy.Consequently,EMAweightstransfer nized SGD training on 8 Tesla V100 with a total
and share knowledge among different branches batch size of 8. We use a learning rate of 0.001, a
and lead to better generalization Tarvainen and momentumof0.9,adropoutrateof0.5,alearning
Valpola (2017). ratedecayweightof0.1,andastepsizeof140,000
iterations.Thetotalnumberoftrainingiterations
4 Quantitative Evaluations is 200,000. We adopt 400,000 training schedules
for MS COCO. In the first quarter of iterations,
4.1 Datasets we only train the AEOPG and set the weights of
other losses zeros. In the multi-scale setting, we
We evaluate the proposed HUWSOD frame-
use scales ranging from 480 to 1,216 with stride
work on PASCAL VOC 2007, PASCAL VOC
32. We randomly adjust the exposure and satura-
2012 Everingham et al (2010) and MS COCO
tion of the images by up to a factor of 1.5 in the
2014 Lin et al (2014) datasets, which are the
HSV space. A random crop with 0.9 of the image
widely-usedbenchmark.PASCALVOC2007con-
sizes is applied.
sists of 5,011 trainval images, and 4,092 test
Our proposed modules use the following
images over 20 categories. PASCAL VOC 2012
parameter settings in all experiments unless spec-
consistsof11,540trainval images,and10,991test ified otherwise. We set npro to 2,048 in SSOPG
images over 20 categories. Following the standard
and the latent representation channels q to 1 in
settings of WSOD, we use the trainval set with AEOPG. We fix the number of anchors nanc to
12Table 1: Ablation study of SSOPG, SEM and MRRP on PASCAL VOC 2007 in terms of CorLoc (%)
and mAP (%).
SSOPG SEM MRRP
npro npro nirf Î npls Î±vdr CorLoc mAP
train infer
â â â â â â 55.1 36.3
a â â â â â â 55.7 36.7
b 1024 1024 â â â â 50.8 32.1
c 2048 2048 â â â â 50.9 32.3
d 4096 4096 â â â â 50.8 32.1
e 2048 2048 3 {0.3,0.4,0.5} â â 60.5 41.8
f 2048 2048 4 {0.35,0.4,0.45,0.5} â â 60.8 42.1
g 2048 2048 5 {0.3,0.35,0.4,0.45,0.5} â â 60.5 42.0
h 2048 2048 4 {0.35,0.4,0.45,0.5} 2 {1,2} 62.3 43.2
j 2048 2048 4 {0.35,0.4,0.45,0.5} 3 {1,2,4} 63.0 44.0
k 2048 2048 4 {0.35,0.4,0.45,0.5} 4 {1,2,4,8} 62.7 43.6
9 with 3 scales and 3 aspect ratios following Ren in the row (a) of Tab. 1 achieves superior perfor-
et al (2017). We set labeling threshold Î»obn and mance, which is due to larger mini-batch size and
Î»p to 0.5 and 0.7, respectively. For SEM, we set epochs. In the rows (b-d) of Tab. 1, we vary the
the number of refinement branches nirf to 4, and number of proposals in training and testing via
Î to {0.35,0.4,0.45,0.5}. We apply MRRP on hyper-parameters npro and npro . We find that
train infer
the last stage of backbone with npls = 3 and SSOPG is generally robust for a wide range of
Î±vdr ={1,2,4}.ForCCR,wealsorandomlyapply values between [1,024,4,096], which shows com-
strongtransformsfromasetofcandidateaugmen- petitiveperformancecomparedtothebaseline(a).
tations, i.e., color jittering, gray scale, gaussian Weobservethatdirectlyreplacingtraditionalpro-
blur,andCutOut,whichfollowsthesettinginLiu posals with SSOPG decreases about 5% CorLoc
et al (2021). We use default Î² =0.9996 in EMA. and4%mAP.However,thecompromisingperfor-
mance enables end-to-end training of the entire
4.4 Ablation Study model. It also reveals that the existing WSOD
models have the inherent characteristic of object-
Without loss of generality, our ablation study is
ness localization, which has been ignored in the
conducted on PASCAL VOC 2007 and ResNet18.
previous methods.
When tuning each group of hyper-parameters,
others are kept as default.
4.4.2 Step-wise entropy minimization
4.4.1 Self-supervised object proposal The proposed SEM is designed to progressively
generator select high-confidence object proposals as posi-
tive samples to refine both detection scores and
The proposed SSOPG is designed to leverage the coordinates. Specifically, SEM has two key hyper-
prediction of the WSOD model to learn object parameters,i.e.,nf andÎ.nf denotesthenumber
locations without relying on external proposal of branches and Î controls the smoothness of
algorithms,suchasselectivesearchVanDeSande refinement.Wevarybothhyper-parametersinthe
et al (2011), edge boxes Zitnick and DollÂ´ar (2014) rows (e-g) of Tab. 1 to discuss their impacts. We
and MCG ArbelÂ´aez et al (2014). Thus we first firstsetnf to3followingthesettingintheprevious
build a strong baseline ContextLocNet Kantorov work Tang et al (2017), and set IoU thresh-
etal(2016),whichiswidelyusedinrecentWSOD old Î to {0.30,0.40,0.50} in ascending order.
methods. Our implementation of ContextLocNet Row (e) shows that SEM significantly achieves
absolute gains of 9.5% CorLoc and 9.5% mAP
13Table 2: Ablation study of AEOPG on PASCAL VOC 2007 in terms of CorLoc (%) and mAP (%).
AEOPG
CorLoc mAP
L L E q D
Encoder Decoder
â â â â â 63.0 44.0
a â â (C,32,2) 1 - 67.2 48.4
b â (C,32,2) 1 (32,C,2) 65.8 46.5
c â â (C,32,2) 1 (32,C,2) 68.0 50.2
d â â (C,32,2) 2 (32,C,2) 51.1 34.1
e â â (C,32,2) 3 (32,C,2) 48.2 32.7
f â â (C,32,2) 4 (32,C,2) 44.7 29.7
g â â (C,16,2) 1 (16,C,2) 67.7 49.8
h â â (C,32,4) 1 (32,C,4) 68.1 50.1
i â â (C,8,4) 1 (8,C,4) 67.7 49.7
over without SEM, i.e., vs. row (c). Compared to notlearnbymulti-scaleimagepyramids.Rows(i-
the vanilla instance refinement Tang et al (2017) k)varyhyper-parametersnpls andÎ±vdr withmore
that brings 6.4% mAP improvement, the pro- scales.TheresultsshowthattheproposedMRRP
posed SEM achieves larger performance-boosting, further improves both the localization and detec-
demonstratingtheeffectivenessofstep-wiselearn- tion performance robustly. When dilation rates
ing. The benefits of SEM are mainly from: First, are too large, the performance tends to decrease
the proposed step-wise learning paradigm pro- slightly. To explain, large receptive fields lead
gressivelyselectshigh-confidenceobjectproposals to inferior classification capacity by introducing
as positive samples for refining. Second, each redundant context for small objects.
refinement branch has a bounding-box regressor
to refine bounding-box coordinates step-wisely. 4.4.4 Autoencoder object proposal
Rows (e-f) show that the models quickly saturate generator
at nf = 4, which is consistent with the results
The proposed AEOPG aims to utilize full image
in Tang et al (2017).
feature maps to capture salient objects, which is
also end-to-end learnable. We ablate the impact
4.4.3 Multi-rate resampling pyramid
of two loss functions, i.e., L and L ,
Encoder Decoder
MRRP aims to aggregate multi-scale information the structures of encoder E and decoder D as
fordetectorstoexploitcontextandachievebetter well as the channels q of the latent representa-
performance in large scale variations. We ablate tion E(F). We employ the model in the row (j)
twokeyhyper-parametersnpls andÎ±vdr inTab.1. of Tab. 1 as the baseline,whichcombinesSSOPG,
We also set nstg = 1 to only use the last stage SEM,andMRRPwiththebesthyper-parameters.
ofbackbonetoprovidemulti-scaleinformation,as The network structure of encoder E and decoder
larger strides lead to a larger difference in recep- D is denoted as triplet tuples (C ,C ,C ),
in stride out
tivefieldsasneededLietal(2019).Row(h)shows where C is the channel number of backbone
in
that the proposed MRRP further improves both outputs, which is set to 512, 512, and 2,048 for
the localization and detection performance with VGG16, ResNet18, and ResNet50, respectively;
gainsof1.6%CorLocand1.2%mAP,respectively. C denotes the channel reduction/increase
stride
Note that the improvement of MRRP is stacked ratio between the successive convolutional layers
on the multi-scale training and testing, which inencoder/decoder;AndC isthechannelnum-
out
already provide 3 â¼ 4% absolute gains in CorLoc ber of the last feature maps that project to the
and mAP Tang et al (2020). This demonstrates latent representation. Rows (a-c) of Tab. 2 show
thatMRRPhasleveragedusefulinformationfrom
network-embedded feature hierarchy, which does
14Table 3: Ablation study of CCR on PASCAL VOC 2007 in terms of CorLoc (%) and mAP (%).
CCR
CorLoc mAP
|T| color jitter gray scale blur cutout Î²
â â â â â â 68.0 50.2
a 2 â â â â â 70.6 52.2
b 2 â â â â â 70.6 52.2
c 2 â â â â â 70.8 52.7
d 2 â â â â â 71.1 52.9
e 3 â â â â â 71.2 52.6
f 4 â â â â â 71.2 53.0
g 5 â â â â â 71.0 52.4
h 2 â â â â 0.996 70.6 51.8
i 2 â â â â 0.9996 71.5 53.4
j 2 â â â â 0.99996 71.2 53.3
k 2 â â â â 0.999996 71.2 52.6
that both L and L improve detec-
Encoder Decoder
tion and localization. And a complete autoen-
coder brings the largest performance gains, which
demonstrates the effectiveness of AEOPG. In the
rows (d-f), we vary the channels of latent repre-
sentationsandobservethatsingle-channelfeature
maps achieve the best performance with 68.0%
CorLoc and 50.2% mAP, respectively. To explain,
when the number of channels is large than 1, the
information of objects may be scattered into dif-
ferentchannels.Therefore,encodingthehigh-level
semantic information into a single feature map
aggregatessalientcuesfromdifferentchannels.To
further ablate AEOPG with different structures, Fig.5:AblationstudyoflossweightsonPASCAL
rows (g-i) show that AEOPG is insensitive to the VOC 2007 test in terms of mAP (%).
structure of the autoencoder.
brings strong consistency constraints. Rows (e-g)
4.4.5 Consistency-regularized
report that increasing T has slight performance
self-training
improvement, as T mainly affects the training
TheproposedCCRaimstoenforceconsistentpre- batchsize.Rows(h-k)showthatthedefaultEMA
dictions produced from stochastic augmentations decay rate Î² improves the final performance by
of the same image. We analyze the influence of 0.4% CorLoc and 0.5% mAP, respectively. The
augmentationnumber T,typesofaugmentations, goodÎ²valuesspanroughlyanorderofmagnitude,
and EMA decay rate Î². We employ the model in the performance degrades quickly outside these
the row (c) of Tab. 2 as the baseline, which uses ranges.
the best hyper-parameters of other modules in
termsofmAP.Intherows(a-d)ofTab.3,different 4.4.6 Influence of loss weight
data augmentation policies have certain improve-
We also ablate each objective function with dif-
ments. And combing all data augmentation poli-
ferent weights, which range in [0,1]. When tuning
ciesachievesthemostgains,asthelargediversity
the weight of each loss, we keep others as default,
15Table 4: Comparison with the state-of-the-art methods on PASCAL VOC 2007 in terms of AP (%) on
test.
Method Backbone Av.
WSODwithexternalobjectproposalmodulesoradditionaldata
WSDDN BilenandVedaldi(2016) VGG16 39.4 50.1 31.5 16.3 12.6 64.5 42.8 42.6 10.1 35.7 24.9 38.2 34.4 55.6 9.4 14.7 30.2 40.7 54.7 46.9 34.8
ContextLocNetKantorovetal(2016) VGG-F 57.1 52.0 31.5 7.6 11.5 55.0 53.1 34.1 1.7 33.1 49.2 42.0 47.3 56.6 15.3 12.8 24.8 48.9 44.4 47.8 36.3
WCCN Dibaetal(2017) VGG16 49.5 60.6 38.6 29.2 16.2 70.8 56.9 42.5 10.9 44.1 29.9 42.2 47.9 64.1 13.8 23.5 45.9 54.1 60.8 54.5 42.8
Jieetal. Jieetal(2017) VGG16 52.2 47.1 35.0 26.7 15.4 61.3 66.0 54.3 3.0 53.6 24.7 43.6 48.4 65.8 6.6 18.8 51.9 43.6 53.6 62.4 41.7
TST Shietal(2017) AlexNet - - - - - - - - - - - - - - - - - - - - 33.8
TSC2 Weietal(2018) VGG16 59.3 57.5 43.7 27.3 13.5 63.9 61.7 59.9 24.1 46.9 36.7 45.6 39.9 62.6 10.3 23.6 41.7 52.4 58.7 56.6 44.3
OICR Tangetal(2017) VGG16 58.0 62.4 31.1 19.4 13.0 65.1 62.2 28.4 24.8 44.7 30.6 25.3 37.8 65.5 15.7 24.1 41.7 46.9 64.3 62.6 41.2
MELM Wanetal(2019b) VGG16 55.6 66.9 34.2 29.1 16.4 68.8 68.1 43.0 25.0 65.6 45.3 53.2 49.6 68.6 2.0 25.4 52.5 56.8 62.1 57.1 47.3
ZLDN Zhangetal(2018a) VGG16 55.4 68.5 50.1 16.8 20.8 62.7 66.8 56.5 2.1 57.8 47.5 40.1 69.7 68.2 21.6 27.2 53.4 56.1 52.5 58.2 47.6
WSRPN Tangetal(2018) VGG16 57.9 70.5 37.8 5.7 21.0 66.1 69.2 59.4 3.4 57.1 57.3 35.2 64.2 68.6 32.8 28.6 50.8 49.5 41.1 30.0 45.3
PCL Tangetal(2020) VGG16 54.4 69.0 39.3 19.2 15.7 62.9 64.4 30.0 25.1 52.5 44.4 19.6 39.3 67.7 17.8 22.9 46.6 57.5 58.6 63.0 43.5
Kosugietal. Kosugietal(2019) VGG16 61.5 64.8 43.7 26.4 17.1 67.4 62.4 67.8 25.4 51.0 33.7 47.6 51.2 65.2 19.3 24.4 44.6 54.1 65.6 59.5 47.6
C-MIL Wanetal(2019a) VGG16 62.5 58.4 49.5 32.1 19.8 70.5 66.1 63.4 20.0 60.5 52.9 53.5 57.4 68.9 8.4 24.6 51.8 58.7 66.7 63.5 50.5
PredNet Arunetal(2019) VGG16 66.7 69.5 52.8 31.4 24.7 74.5 74.1 67.3 14.6 53.0 46.1 52.9 69.9 70.8 18.5 28.4 54.6 60.7 67.1 60.4 52.9
OICRW-RPNSinghandLee(2019) VGG16 - - - - - - - - - - - - - - - - - - - - 46.9
WSOD2 Zengetal(2019) VGG16 65.1 64.8 57.2 39.2 24.3 69.8 66.2 61.0 29.8 64.6 42.5 60.1 71.2 70.7 21.9 28.1 58.6 59.7 52.2 64.8 53.6
GAM+REG Yangetal(2019a) VGG16 55.2 66.5 40.1 31.1 16.9 69.8 64.3 67.8 27.8 52.9 47.0 33.0 60.8 64.4 13.8 26.0 44.0 55.7 68.9 65.5 48.6
C-MIDN Yanetal(2019) VGG16 53.3 71.5 49.8 26.1 20.3 70.3 69.9 68.3 28.7 65.3 45.1 64.6 58.0 71.2 20.0 27.5 54.9 54.9 69.4 63.5 52.6
Renetal. Renetal(2020) VGG16 68.8 77.7 57.0 27.7 28.9 69.1 74.5 67.0 32.1 73.2 48.1 45.2 54.4 73.7 35.0 29.3 64.1 53.8 65.3 65.2 54.9
PG-PS Chengetal(2020) VGG16 63.0 64.4 50.1 27.5 17.1 70.6 66.0 71.1 25.8 55.9 43.2 62.7 65.9 64.1 10.2 22.5 48.1 53.8 72.2 67.4 51.1
Zhangetal. Zhangetal(2021) VGG16 62.2 61.1 51.1 33.8 18.0 66.7 66.5 65.0 18.5 59.4 44.8 60.9 65.6 66.9 24.7 26.0 51.0 53.2 66.0 62.2 51.2
SLV Chenetal(2020) VGG16 65.6 71.4 49.0 37.1 24.6 69.6 70.3 70.6 30.8 63.1 36.0 61.4 65.3 68.4 12.4 29.9 52.4 60.0 67.6 64.5 53.5
OICR Tangetal(2017) WSR18 61.3 54.5 52.4 30.1 34.9 68.9 65.0 75.0 22.5 57.4 19.7 66.6 64.8 64.9 16.8 22.3 53.2 54.9 69.9 64.8 51.0
PCL Tangetal(2020) WSR18 54.4 69.5 48.7 29.7 33.2 70.7 69.7 57.2 11.5 62.4 37.2 39.3 66.3 67.5 23.7 30.9 60.1 52.0 65.3 55.3 50.2
C-MIL Wanetal(2019a) WSR18 57.0 54.9 43.6 39.9 32.2 70.9 69.8 75.2 14.2 59.9 28.5 66.3 67.5 65.3 37.6 21.8 56.7 49.8 71.1 68.9 52.6
GAM+REG Yangetal(2019a) WSR101 67.3 72.1 55.8 31.8 31.3 71.6 70.0 76.7 19.4 58.7 21.1 68.5 74.6 69.9 19.1 18.8 48.4 55.1 71.9 53.2 52.8
VGG16 57.2 75.6 53.8 31.1 38.6 76.7 74.0 69.7 3.4 73.6 47.9 72.1 75.8 74.1 8.5 27.4 59.2 50.3 71.6 70.2 55.5
HUWSODâ WSR18 69.0 67.3 62.0 42.3 44.8 77.8 75.8 76.3 18.5 75.5 41.3 78.3 75.9 75.1 22.7 26.7 56.1 60.8 74.7 68.4 59.5
WSR50 67.3 61.1 63.5 46.4 38.1 74.6 76.2 80.8 7.0 71.7 37.5 77.0 76.8 72.7 8.9 22.5 53.4 67.9 75.7 63.8 57.2
WSODwithoutexternalobjectproposalmodulesoradditionaldata
BeamSearch Bencyetal(2016) VGG16 - - - - - - - - - - - - - - - - - - - - 25.7
OM+MIL Lietal(2016a) AlexNet 37.2 35.7 25.8 13.8 12.7 36.2 42.4 22.3 14.3 24.2 9.4 13.1 27.9 38.9 3.7 18.7 20.1 16.3 36.1 18.4 23.4
OPG Shenetal(2018a) VGG16 48.9 49.9 25.6 14.0 6.1 47.1 22.5 52.7 3.4 19.6 33.2 33.3 55.3 30.2 9.9 9.1 14.8 25.8 50.8 24.7 28.8
SPAM-CAM Gudietal(2017) VGG16 - - - - - - - - - - - - - - - - - - - - 27.5
VGG16 63.23 77.08 52.49 38.74 21.33 74.90 72.20 26.62 6.91 72.25 38.41 41.68 79.10 78.63 14.65 29.70 52.55 48.22 67.26 58.7 50.7
HUWSOD WSR18 69.7 77.8 55.2 37.2 36.1 73.7 72.6 44.3 8.7 69.8 48.1 61.6 76.1 73.1 24.7 27.1 54.6 57.9 68.8 30.6 53.4
WSR50 70.2 67.7 62.0 45.7 33.9 73.4 74.0 85.2 3.3 68.1 50.4 79.6 77.3 72.0 12.5 10.1 58.1 62.4 70.2 15.5 54.6
FSOD
FastRCNN Girshick(2015) VGG16 74.5 78.3 69.2 53.2 36.6 77.3 78.2 82.0 40.7 72.7 67.9 79.6 79.2 73.0 69.0 30.1 65.4 70.2 75.8 65.8 66.9
FasterRCNN Renetal(2017) VGG16 70.0 80.6 70.1 57.3 49.9 78.2 80.4 82.0 52.2 75.3 67.2 80.3 79.8 75.0 76.3 39.1 68.3 67.3 81.1 67.6 69.9
i.e., 1. We plot the weight-mAP trade-off curve in the bottom part of Tab. 4. HUWSOD signif-
in Fig. 5. As expected, gradually reducing the icantly outperforms the state-of-the-art methods
loss weights usually causes performance drops. that do not use external modules or additional
The plot also shows that the overall performance data. This indicates the efficiency of HUWSOD.
is robust to the loss weight of AEOPG, which Comparedtoothermethodswithexternalobject
mainly provides high-quality proposals for the proposals or data, HUWSOD also achieves com-
salient object. However, we observe that SSOPG petitive results. It demonstrates that HUWSOD
is more sensitive to loss weight than AEOPG, as significantlyreducestheperformancegapbetween
proposal learning is a vital component of WSOD. WSOD and FSOD methods. Note that HUW-
Surprisingly, OM loss has significant impact on SODiscompatiblewithexternalobjectproposals.
the final performance among all losses. Although Thus, we further add offline object proposals to
all loss weights are equally set to 1 as default, we HUWSOD only during training, which is denoted
empiricallyfindthatgridsearchoverthosehyper- as HUWSODâ. Thetop partof Tab. 4 shows that
parameter spaces can find better-performing con- HUWSODâ significantly outperforms all previous
figurations for HUWSOD. methods. In Tab. 5, we compare the localiza-
tion performance on train+val images in terms of
4.5 Comparison with the CorLoc. HUWSOD reports 71.5% CorLoc, which
State-of-the-Art Methods consistently outperforms other methods that do
not use external modules or additional data.
Tofullycomparewithotherbackbones,wecatego- HUWSODâ further achieves 75.0% CorLoc and
rize the detection results into two groups accord- suppressothermethodswithexternalobjectpro-
ing to whether they use external object proposal posals and data. We also report the performance
modules or additional data. We first compare the on Pascal VOC 2012 and MS COCO in Tab. 6.
results on Pascal VOC 2007 in terms of mAP HUWSOD shows superior results compared to
16Table 5: Comparison with the state-of-the-art methods on PASCAL VOC 2007 in terms of CorLoc (%)
on train+val.
Method Backbone Av.
WSODwithexternalobjectproposalmodulesoradditionaldata
WSDDN BilenandVedaldi(2016) VGG16 65.1 58.8 58.5 33.1 39.8 68.3 60.2 59.6 34.8 64.5 30.5 43.0 56.8 82.4 25.5 41.6 61.5 55.9 65.9 63.7 53.5
ContextLocNet Kantorovetal(2016) VGG-F 83.3 68.6 54.7 23.4 18.3 73.6 74.1 54.1 8.6 65.1 47.1 59.5 67.0 83.5 35.3 39.9 67.0 49.7 63.5 65.2 55.1
WCCN Dibaetal(2017) VGG16 83.9 72.8 64.5 44.1 40.1 65.7 82.5 58.9 33.7 72.5 25.6 53.7 67.4 77.4 26.8 49.1 68.1 27.9 64.5 55.7 56.7
Jieetal. Jieetal(2017) VGG16 72.7 55.3 53.0 27.8 35.2 68.6 81.9 60.7 11.6 71.6 29.7 54.3 64.3 88.2 22.2 53.7 72.2 52.6 68.9 75.5 56.1
TST Shietal(2017) AlexNet â â â â â â â â â â â â â â â â â â â â 59.5
TS2C Weietal(2018) VGG16 84.2 74.1 61.3 52.1 32.1 76.7 82.9 66.6 42.3 70.6 39.5 57.0 61.2 88.4 9.3 54.6 72.2 60.0 65.0 70.3 61.0
OICR Tangetal(2017) VGG16 81.7 80.4 48.7 49.5 32.8 81.7 85.4 40.1 40.6 79.5 35.7 33.7 60.5 88.8 21.8 57.9 76.3 59.9 75.3 81.4 60.6
MELM Wanetal(2019b) VGG16 â â â â â â â â â â â â â â â â â â â â 61.4
ZLDN Zhangetal(2018a) VGG16 74.0 77.8 65.2 37.0 46.7 75.8 83.7 58.8 17.5 73.1 49.0 51.3 76.7 87.4 30.6 47.8 75.0 62.5 64.8 68.8 61.2
WSRPN Tangetal(2018) VGG16 77.5 81.2 55.3 19.7 44.3 80.2 86.6 69.5 10.1 87.7 68.4 52.1 84.4 91.6 57.4 63.4 77.3 58.1 57.0 53.8 63.8
Kosugietal. Kosugietal(2019) VGG16 85.5 79.6 68.1 55.1 33.6 83.5 83.1 78.5 42.7 79.8 37.8 61.5 74.4 88.6 32.6 55.7 77.9 63.7 78.4 74.1 66.7
C-MIL Wanetal(2019a) VGG16 â â â â â â â â â â â â â â â â â â â â 65.0
PredNet Arunetal(2019) VGG16 88.6 86.3 71.8 53.4 51.2 87.6 89.0 65.3 33.2 86.6 58.8 65.9 87.7 93.3 30.9 58.9 83.4 67.8 78.7 80.2 70.9
OICRW-RPN SinghandLee(2019) VGG16 - - - - - - - - - - - - - - - - - - - - 66.5
WSOD2 Zengetal(2019) VGG16 87.1 80.0 74.8 60.1 36.6 79.2 83.8 70.6 43.5 88.4 46.0 74.7 87.4 90.8 44.2 52.4 81.4 61.8 67.7 79.9 69.5
OICR+GAM+REGYangetal(2019a) VGG16 81.7 81.2 58.9 54.3 37.8 83.2 86.2 77.0 42.1 83.6 51.3 44.9 78.2 90.8 20.5 56.8 74.2 66.1 81.0 86.0 66.8
C-MIDN Yanetal(2019) VGG16 - - - - - - - - - - - - - - - - - - - - 68.7
Renetal. Renetal(2020) VGG16 87.5 82.4 76.0 58.0 44.7 82.2 87.5 71.2 49.1 81.5 51.7 53.3 71.4 92.8 38.2 52.8 79.4 61.0 78.3 76.0 68.8
PG-PS Chengetal(2020) VGG16 85.4 80.4 69.1 58.0 35.9 82.7 86.7 82.6 45.5 84.9 44.1 80.2 84.0 89.2 12.3 55.7 79.4 63.4 82.1 82.1 69.2
Zhangetal. Zhangetal(2021) VGG16 86.3 72.9 71.2 59.0 36.3 80.2 84.4 75.6 30.8 83.6 53.2 75.1 82.7 87.1 37.7 54.6 74.2 59.1 79.8 78.9 68.1
SLV Chenetal(2020) VGG16 84.6 84.3 73.3 58.5 49.2 80.2 87.0 79.4 46.8 83.6 41.8 79.3 88.8 90.4 19.5 59.7 79.4 67.7 82.9 83.2 71.0
OICR Tangetal(2017) WSR18 82.1 60.3 81.1 49.3 67.6 81.4 87.2 84.0 33.4 76.8 21.6 78.8 87.0 87.5 30.8 52.6 81.2 66.6 81.8 82.8 68.7
PCL Tangetal(2020) WSR18 76.7 81.9 74.4 48.1 53.9 84.5 87.7 86.5 25.4 68.1 36.0 67.4 84.8 86.6 52.5 51.1 81.2 54.9 78.7 62.5 67.1
C-MIL Wanetal(2019a) WSR18 80.3 64.6 68.3 53.0 56.8 84.5 89.1 86.5 28.1 72.4 28.8 77.3 84.1 79.1 56.8 51.8 85.4 62.1 81.1 80.4 68.5
GAM+REG Yangetal(2019a) WSR101 88.8 86.6 66.6 57.0 48.5 78.6 91.1 91.3 34.3 88.8 29.1 78.9 90.5 89.6 34.1 41.0 77.0 74.5 87.3 66.4 70.1
VGG16 73.0 88.2 67.3 56.0 41.0 85.4 92.6 79.9 21.7 86.1 60.2 81.2 89.9 89.6 19.5 42.0 79.2 79.7 83.6 75.8 69.6
HUWSODâ WSR18 88.4 81.9 84.4 56.8 62.6 89.7 91.0 88.3 42.9 85.5 48.5 85.7 91.4 92.5 37.0 48.1 79.2 83.8 86.6 76.6 75.0
WSR50 82.5 72.4 75.3 73.0 54.3 85.4 91.1 89.1 29.0 90.3 40.8 90.8 89.9 87.2 20.2 40.2 77.1 78.8 91.0 71.9 71.5
WSODwithoutexternalobjectproposalmodulesoradditionaldata
Shietal. Shietal(2015) â 67.3 54.4 34.3 17.8 1.3 46.6 60.7 68.9 2.5 32.4 16.2 58.9 51.5 64.6 18.2 3.1 20.9 34.7 63.4 5.9 36.2
OM+MIL Lietal(2016a) AlexNet 64.3 54.3 42.7 22.7 34.4 58.1 74.3 36.2 24.3 50.4 11.0 29.2 50.5 66.1 11.3 42.9 39.6 18.3 54.0 39.8 41.2
OPG Shenetal(2018a) VGG16 57.1 43.2 53.9 23.8 12.3 47.9 48.8 69.1 16.6 47.5 39.0 61.3 54.7 60.8 32.1 22.0 49.0 44.1 59.4 27.7 43.5
VGG16 82.5 89.8 74.7 67.0 45.7 84.3 90.5 39.7 27.2 90.3 40.8 63.3 89.9 92.8 28.7 58.9 79.2 69.5 82.8 62.5 68.0
HUWSOD WSR18 80.2 90.6 74.7 69.0 53.3 88.8 92.6 59.8 32.6 91.7 45.6 77.1 91.9 88.8 40.6 58.0 81.4 75.4 85.8 51.6 71.5
WSR50 86.6 82.8 85.0 66.7 67.6 84.5 91.8 92.0 16.5 79.7 59.8 90.2 95.7 93.3 26.2 15.8 81.3 74.8 82.7 39.8 70.6
other models and achieves new state-of-the-art classannotationfromtheground-truthbounding-
performance. The benefits are mainly from effec- box labels during training, termed Cls-agnostic
tively learnable object proposal generation, well- GT-bboxKnown.Thus,WSODstillneedstocap-
designed self-training scheme. For AP and AP , ture coarse object location, while predicted boxes
50
our results are only comparable to the state-of- have ground-truth regression supervision to fine-
the-artmethods,whichismainlybecausethatMS tune themselves. As shown in the bottom part of
COCO has more complex scenes and a larger cat- Tab. 7, the performance of HUWSOD approaches
egory set. Thus, design principles in traditional its upper bound of GT-bbox Known, which also
proposalmethodsmayfurtherimproveend-to-end demonstrates that HUWSOD has the ability to
proposal generators for WSOD. achieve fully-supervised accuracy. We find that
the accurate bounding-box localization is one of
4.6 Upper-bound Performance the main obstacles to reducing the performance
gap between HUWSOD and its fully supervised
We further analyze the upper-bound performance
counterpart.
of WSOD methods with ground-truth bounding
boxes (GT-bbox Known) setting in Tab. 7. The
4.7 Object Proposals in WSOD
upper bound of recent state-of-the-art methods,
e.g., works in Yang et al (2019a); Ren et al We analyze the recall of object proposals for
(2020), are Fast R-CNN Girshick (2015), as they different methods at different IoU ratios with
all heavily rely on external object proposal algo- ground-truthboxes.Fig.6showsthattheSSOPG
rithms. Different from them, HUWSOD learns to suppresses selective search Van De Sande et al
generate object proposals and has upper-bound (2011) (SS), edge boxes Zitnick and DollÂ´ar
performancewithFasterR-CNNRenetal(2017). (2014) (EB) and multiscale combinatorial group-
We further introduce a regression-disentangled ing ArbelÂ´aez et al (2014) (MCG) at IoU 0.5
learning setting to decouple proposal classifica- in few proposal regimes, i.e., 4 â¼ 64 candi-
tionandregressiontasks.Indetail,weremovethe dates per image. At regimes of 256 â¼ 1,024,
17Table 6: Comparison with the state-of-the-art methods on PASCAL VOC 2012 and MS COCO.
VOC 2012 MS COCO
Method Bakcbone
mAP CorLoc AP AP AP
50 75
WSOD with external object proposal modules or additional data
WSDDN Bilen and Vedaldi (2016) VGG16 â â 9.5 19.2 8.2
ContextLocNetKantorov et al (2016) VGG-F 35.3 54.8 11.1 22.1 10.7
WCCN Diba et al (2017) VGG16 37.9 â â â â
Jie et al. Jie et al (2017) VGG16 38.3 58.8 â â â
TS2C Wei et al (2018) VGG16 40.0 64.4 â â â
OICR Tang et al (2017) VGG16 37.9 62.1 â â â
MELM Wan et al (2019b) VGG16 42.4 â â â â
ZLDN Zhang et al (2018a) VGG16 42.9 61.5 â â â
WSRPN Tang et al (2018) VGG16 40.8 64.9 â â â
PCL Tang et al (2020) VGG16 â â 8.5 19.4 â
Kosugi et al. Kosugi et al (2019) VGG16 43.4 66.7 â â â
C-MIL Wan et al (2019a) VGG16 46.7 67.4 â â â
Pred Net Arun et al (2019) VGG16 48.4 69.5 â â â
OICR W-RPN Singh and Lee (2019) VGG16 43.2 67.5 â â â
WSOD2 Zeng et al (2019) VGG16 47.2 71.9 10.8 22.7 â
C-MIDN Yan et al (2019) VGG16 50.2 71.2 9.6 21.4 â
PG-PS Cheng et al (2020) VGG16 48.3 68.7 â 20.7 â
Zhang et al. Zhang et al (2021) VGG16 46.3 68.7 11.1 23.6 â
SLV Chen et al (2020) VGG16 49.2 69.2 â â â
VGG16 53.0 71.2 10.4 24.9 7.2
HUWSODâ
WSR18 57.1 78.6 8.0 20.8 5.0
WSOD without external object proposal modules or additional data
Shi et al. Shi et al (2015) â 36.2 â â â â
Beam Search Bency et al (2016) VGG16 26.5 â â â â
OM+MIL Li et al (2016a) AlexNet 29.1 â â â â
VGG16 48.1 68.1 6.3 17.9 3.1
HUWSOD
WSR18 50.3 72.3 5.8 16.8 3.1
FSOD
Fast RCNN Girshick (2015) VGG16 65.7 â 18.9 38.6 â
Faster RCNN Ren et al (2017) VGG16 67.0 â 21.2 41.5 â
SSOPGpracticallyachievessimilarproposalqual- and AEOPG (SSOPG+AEOPG) achieves con-
ity as SS, but has lower quality than EB and sistent improvements across different numbers of
MCG.OnepossiblereasonisthatSSOPGisover- proposal regimes. It demonstrates that SSOPG
fitting to noisy pseudo labels, which is biased and AEOPG learn complementary knowledge to
with true distribution in self-supervised frame- hypothesize object locations.
work. We also observe that the recall of AEOPG We further investigate the per-category pro-
is saturated at 32 proposals per image. The posal recall at IoU 0.5 with 1,024 candidates per
average proposal number generated by AEOPG image in Fig. 7. The ensemble of SSOPG and
is 16.0 for each image, which reaches 31.4% AEOPG improves the recall of most categories,
recall at IoU 0.5. A simple ensemble of SSOPG whichachievesmorethan95%recallforcategories
18Table 7: Upper bound of various WSOD methods on PASCAL VOC and MS COCO.
VOC 2007 VOC 2012 MS COCO
Method Bakcbone
mAP CorLoc mAP CorLoc AP AP AP
50 75
FSOD
Fast RCNN Girshick (2015) VGG16 66.9 â 65.7 â 18.9 38.6 â
Faster RCNNRen et al (2017) VGG16 69.9 â 67.0 â 21.2 41.5 â
GT-bbox Known
Yang et al. Yang et al (2019a) VGG16 66.9 â 65.7 â 18.9 38.6 â
Ren et al. Ren et al (2020) VGG16 66.9 â 65.7 â 18.9 38.6 â
HUWSOD VGG16 69.9 â 67.0 â 21.2 41.5 â
Cls-agnostic GT-bbox Known
Yang et al. Yang et al (2019a) VGG16 54.2 67.6 53.9 68.1 13.7 27.1 12.5
Ren et al. Ren et al (2020) VGG16 64.9 73.8 62.1 74.9 15.1 29.9 13.7
VGG16 67.7 93.3 65.3 91.1 15.3 32.4 12.8
HUWSOD
WSR18 69.7 92.5 66.1 92.3 13.7 27.9 12.5
Table 8: Adopting HUWSOD to multi-stage WSOD methods on PASCAL VOC and MS COCO.
VOC 2007 VOC 2012 MS COCO
Method Bakcbone
mAP CorLoc mAP CorLoc AP AP AP
50 75
VGG16 55.5 69.6 53.5 70.8 10.3 22.6 6.9
HUWSOD
WSR18 59.5 75.0 55.9 75.5 9.6 19.4 6.1
VGG16 60.3 â 57.7 â 15.5 30.5 14.3
SoS Sui et al (2022)
ResNet50 64.4 â 61.9 â 16.6 32.8 15.2
HUWSOD + SoS ResNet50 64.5 80.1 62.4 80.4 16.2 31.3 14.5
BiB Vo et al (2022) VGG16 65.1 â â â 17.2 34.1 â
HUWSOD + BiB VGG16 65.7 82.7 63.1 83.2 17.9 36.2 16.4
bicycle,bus,cat,dog,horse,sofa andtrain inPAS- FSOD,andsemi-superviseddetection,whichpays
CAL VOC. The proposed generators obtain the more attention to relatively high-quality pseudo
lowest recall for category bottle due to its small labels and carries out a dynamic label updat-
size.Formostcategories,âSSOPG+AEOPGâhas ing for noisy labels. BiB introduced an active
competitive performance compared to those well- learning strategy to fine-tune a base pre-trained
designed traditional proposal methods. WSODmodelwithafewfully-annotatedsamples,
which are automatically selected from the train-
4.8 Compatibility with other ingsetaccordingtothefailuremodesofdetectors.
WSOD methods Results are shown in Tab. 8. When integrating
with SoS, HUWSOD achieves 64.5% mAP and
HUWSOD is compatible with various orthogo- 16.2%APonVOC2007andCOCO,respectively.
nal WSOD methods. We adapt our method as Underthe10%and10shotssettinginactivelearn-
the WSOD part in multi-stage approaches, e.g., ing, HUWSOD with BiB finally reaches 65.7%
SoS Sui et al (2022) and BiB Vo et al (2022). SoS mAP and 17.9% AP on VOC 2007 and COCO,
proposed a three-stage framework, i.e., WSOD,
19Table 9: Comparison with the state-of-the-art methods on Cityscapes in terms of AP on val set.
Method Backbone person rider car truck bus train motorcycle bicycle AP50
WSDDNBilenandVedaldi(2016) WSR18 0.0 2.7 0.1 20.2 36.1 8.4 2.9 0.0 8.8
OICR Tangetal(2017) WSR18 0.2 0.0 20.7 12.3 36.1 9.1 3.1 0.9 10.3
HUWSOD WSR18 0.9 4.0 31.2 27.1 44.8 13.4 6.3 1.1 16.1
2 proposals 4 proposals 8 proposals 16 proposals 32 proposals
64 proposals 128 proposals 256 proposals 512 proposals 1,024 proposals
Fig. 6:Recallvs.IoUoverlapratioofdifferentobjectproposalmethodsonthePASCALVOC2007test.
respectively. These results demonstrate the flexi- to our AEOPG, which accurately hypothesize the
bility and effectiveness of our HUWSOD. location of small objects, unlike traditional offline
proposal generation methods that often fail to do
4.9 WSOD in Autonomous Driving so. Additionally, our MRRP provides multi-scale
information, enabling better detection of small
To explore the value of WSOD in more complex
objects. It is worth noting that person and rider
application scenarios, we compare our HUWSOD
are semantically overlapping and very similar in
with common WSOD methods such as WSDDN
appearance. Even with full supervision, it is diffi-
and OICR on the Cityscapes Cordts et al (2016).
cult to distinguish between the two. However, our
The Cityscapes dataset contains 2,975 training
HUWSOD can effectively distinguish between the
images and 500 val images of size 1024 Ã 2048
two under weak supervision.
takenfromacardrivinginGermancities,labeled
with 8 semantic instance categories, i.e. person,
4.10 Quantitative Analysis
rider, car, truck, bus, train, motorcycle and bicy-
cle. These results shown in Tab. 9 demonstrate We first visualize object proposals from AEOPG,
thatourHUWSODcomprehensivelysurpassesthe encoded representation E(F), and low-rank fea-
common WSOD method in autonomous driving ture maps Fq in Fig. 8. The first three rows
and achieves a detection performance of 16.1% show that AEOPG generates high-quality can-
AP 0. In the large category, we have significantly didates with very few proposals. AEOPG also
5
improved the OICR by 10.5%, 14.8%, 8.7%, and distinguishes multiple instances of the same cate-
4.4% in the car, truck, bus, and train categories, gory, as shown in the next two rows. In the sixth
respectively. This improvement is attributed to row,AEOPGisabletodetectsmallobjectsinout-
our SSOPG, which provides better recall for doorscenes.TheseventhrowdepictsthatAEOPG
large objects compared to traditional offline pro- captures salient objects in complex backgrounds.
posalgenerationmethods,andSEM,whichbetter Wealsoshowsomeincorrectcasesinthelastrow.
refinestheboundingboxcoordinates.Inthesmall Our failure modes mainly come from two parts:
category, we have improved performance in the (1) object overlap, and (2) cluttered scenes.
person, rider, and motorcycle categories by 0.7%, Next, we show qualitative comparisons among
4.0%, 3.2%, and 0.2%, respectively. This is due WSDDN Bilen and Vedaldi (2016), PCL Tang
20Fig. 7: Per-category recall of different object proposal methods on the PASCAL VOC 2007 test.
AEOPG E(F) Fq AEOPG E(F) Fq AEOPG E(F) Fq
Fig. 8: Visualization of object proposals, encoded representations E(F) and low-rank feature maps Fq
in AEOPG.
et al (2020), UWSOD Shen et al (2020b), our We finally visualize some failure detection results
HUWSOD and HUWSODâ on PASCAL VOC ofourmodelsinthelasttworowsofFig.9,which
2007 with ResNet18 backbone in Fig. 9. The are mainly due to confusion with similar objects.
first column represents the original images with
ground-truth bounding boxes. In the first two 5 Conclusion
rows, we observe that the proposed HUWSOD
provides more accurate detections in complex In this paper, we propose HUWSOD, a novel
scenes. The next two rows show that HUWSOD framework with holistic self-training and unified
is able to distinguish multiple instances of the WSOD network, to develop a high-capacity gen-
samecategory.Ourmethodisrobusttotheobject eral detection model with only image-level labels,
size,especiallyforsmallobjectscomparedtoother whichisself-containedanddoesnotrequireexter-
methods,asshowninthefifthandsixthrows.We nal modules or additional supervision. HUWSOD
alsofindthatHUWSODâimprovessomefalsepos- innovates in two essential prospects, i.e., end-
itivesoverHUWSODintheseventhrowofFig.9. to-end object proposal generation and holistic
21Ground Truths WSDDN PCL UWSOD HUWSOD HUWSODâ
Fig. 9: Visualization of detection results from WSDDN, PCL, UWSOD, the proposed HUWSOD and
HUWSODâ.
self-training, both of which are rarely touched in methods have been a critical auxiliary module for
WSODbefore.Moreover,theproposedHUWSOD WSOD community to progress. It enables to see
isupper-boundedinitsFSODcounterparttightly, significant improvements before end-to-end pro-
which demonstrates HUWSOD is capable to posal generators are available, i.e., SSOPG and
achievefully-supervisedaccuracy.Comparedwith AEOPG.Thus,exploitingwell-designedprinciples
thepreviousmethodsthatusingexternalproposal in traditional proposal methods is a promising
modules, HUWSOD has limited performance on research direction for WSOD proposal learning.
challengingMSCOCObenchmark,asSSOPGand As data augmentation policies plays a key role
AEOPGmaynothaveenoughsupervisiontocap- inCR,furtherinvestigationofaugmentationvari-
ture large appearance and scale variability. In a antsisalsopromising,whichweleavetoourfuture
generic large-scale scenario, traditional proposal work.
22References
Chen L, Yang T, Zhang X, et al (2021) Points
As Queries: Weakly Semi-Supervised Object
Amores J (2013) Multiple Instance Classification:
DetectionbyPoints.In:IEEE/CVFConference
Review, Taxonomy and Comparative Study.
on Computer Vision and Pattern Recognition
Artificial Intelligence (AI)
(CVPR)
ArbelÂ´aez P, Pont-Tuset J, Barron J, et al
Chen Z, Fu Z, Jiang R, et al (2020) SLV: Spa-
(2014) Multiscale Combinatorial Grouping. In:
tial Likelihood Voting for Weakly Supervised
IEEE/CVF Conference on Computer Vision
Object Detection. In: IEEE/CVF Conference
and Pattern Recognition (CVPR)
on Computer Vision and Pattern Recognition
(CVPR)
Arun A, Jawahar CV, Kumar MP (2019) Dis-
similarity Coefficient Based Weakly Supervised
Cheng G, Yang J, Gao D, et al (2020)
Object Detection. In: IEEE/CVF Conference
High-Quality Proposals for Weakly Supervised
on Computer Vision and Pattern Recognition
Object Detection. IEEE Transactions on Image
(CVPR)
Processing (TIP)
BachmanP,AlsharifO,PrecupD(2014)Learning
Cinbis RG, Verbeek J, Schmid C (2017) Weakly
withPseudo-Ensembles.In:ConferenceonNeu-
SupervisedObjectLocalizationwithMulti-Fold
ral Information Processing Systems (NeurIPS)
Multiple Instance Learning. IEEE Transactions
on Pattern Analysis and Machine Intelligence
Bency AJ, Kwon H, Lee H, et al (2016) Weakly
(TPAMI)
Supervised Localization Using Deep Feature
Maps. In: European Conference on Computer
Cordts M, Omran M, Ramos S, et al (2016)
Vision (ECCV)
The cityscapes dataset for semantic urban
scene understanding. In: IEEE/CVF Confer-
Berthelot D, Carlini N, Goodfellow I, et al
enceonComputerVisionandPatternRecogni-
(2019)MixMatch:AHolisticApproachtoSemi-
tion (CVPR), pp 3213â3223
Supervised Learning. In: Conference on Neural
Information Processing Systems (NeurIPS)
Diba A, Sharma V, Pazandeh A, et al (2017)
Weakly Supervised Cascaded Convolutional
Bilen H, Vedaldi A (2016) Weakly Supervised
Networks. In: IEEE/CVF Conference on Com-
Deep Detection Networks. In: IEEE/CVF Con-
puter Vision and Pattern Recognition (CVPR)
ferenceonComputerVisionandPatternRecog-
nition (CVPR)
Dong B, Huang Z, Guo Y, et al (2021) Boost-
ing Weakly Supervised Object Detection Via
Cai Z, Vasconcelos N (2019) Cascade R-CNN:
Learning Bounding Box Adjusters. In: IEEE
High Quality Object Detection and Instance
International Conference on Computer Vision
Segmentation. IEEE Transactions on Pattern
(ICCV)
Analysis and Machine Intelligence (TPAMI)
van Engelen JE, Hoos HH (2020) a Survey on
Cao T, Du L, Zhang X, et al (2021) CaT: Weakly
Semi-Supervised Learning. Machine Learning
Supervised Object Detection with Category
Transfer. In: IEEE International Conference on
Everingham M, Van Gool L, Williams CK, et al
Computer Vision (ICCV)
(2010) the Pascal Visual Object Classes (VOC)
Challenge. International Journal of Computer
Chen B, Li P, Chen X, et al (2022) Dense Learn-
Vision (IJCV)
ing Based Semi-Supervised Object Detection.
In:IEEE/CVFConferenceonComputerVision
Fan J, Gao B, Jin H, et al (2022) UCC:
and Pattern Recognition (CVPR)
Uncertainty Guided Cross-Head Co-Training
forSemi-SupervisedSemanticSegmentation.In:
IEEE/CVF Conference on Computer Vision
23and Pattern Recognition (CVPR) Detection. In: IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR)
FangL,XuH,LiuZ,etal(2020)EHSOD:CAM-
Guided End-To-End Hybrid-Supervised Object Huang Z, Zou Y, Bhagavatula V, et al (2020)
Detection with Cascade Refinement. In: AAAI Comprehensive Attention Self-Distillation for
Conference on Artificial Intelligence (AAAI) Weakly-Supervised Object Detection. In: Con-
ference on Neural Information Processing Sys-
Gao M, Li A, Yu R, et al (2018) C-WSL: tems (NeurIPS)
Count-GuidedWeaklySupervisedLocalization.
In: European Conference on Computer Vision Huang Z, Bao Y, Dong B, et al (2022)
(ECCV) W2N:Switching from Weak Supervision to
Noisy Supervision for Object Detection. In:
Gao Y, Liu B, Guo N, et al (2019) Utilizing European Conference on Computer Vision
the Instability in Weakly Supervised Object (ECCV)
Detection. In: IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) Jia D, Wei D, Socher R, et al (2009) ImageNet:
Workshops ALarge-ScaleHierarchicalImageDatabase.In:
IEEE/CVF Conference on Computer Vision
Ge W, Yang S, Yu Y (2018) Multi-Evidence and Pattern Recognition (CVPR)
Filtering and Fusion for Multi-Label Classifica-
tion, Object Detection and Semantic Segmen- JieZ,WeiY,JinX,etal(2017)DeepSelf-Taught
tation Based on Weakly Supervised Learning. Learning for Weakly Supervised Object Local-
In:IEEE/CVFConferenceonComputerVision ization. In: IEEE/CVF Conference on Com-
and Pattern Recognition (CVPR) puter Vision and Pattern Recognition (CVPR)
GirshickR(2015)FastR-CNN.In:IEEEInterna- Kantorov V, Oquab M, Cho M, et al (2016)
tional Conference on Computer Vision (ICCV) ContextLocNet: Context-Aware Deep Network
Models for Weakly Supervised Localization.
Gudi A, van Rosmalen N, Loog M, et al (2017) In: European Conference on Computer Vision
Object Extent Pooling for Weakly Super- (ECCV)
vised Single-Shot Localization. In: The British
Machine Vision Conference (BMVC) Kosugi S, Yamasaki T, Aizawa K (2019) Object-
AwareInstanceLabelingforWeaklySupervised
Guo Q, Mu Y, Chen J, et al (2022) Scale- Object Detection. In: IEEE International Con-
Equivalent Distillation for Semi-Supervised ference on Computer Vision (ICCV)
Object Detection. In: IEEE/CVF Conference
on Computer Vision and Pattern Recognition Laine S, Aila T (2017) Temporal Ensembling
(CVPR) for Semi-Supervised Learning. In: The Interna-
tional Conference on Learning Representations
He K, Zhang X, Ren S, et al (2015) Spatial (ICLR)
Pyramid Pooling in Deep Convolutional Net-
works for Visual Recognition. IEEE Transac- Li D, Huang JB, Li Y, et al (2016a) Weakly
tions on Pattern Analysis and Machine Intelli- Supervised Object Localization with Progres-
gence (TPAMI) sive Domain Adaptation. In: IEEE/CVF Con-
ferenceonComputerVisionandPatternRecog-
He K, Gkioxari G, DollÂ´ar P, et al (2020) Mask R- nition (CVPR)
CNN. IEEE Transactions on Pattern Analysis
and Machine Intelligence (TPAMI) Li Y, Liu L, Shen C, et al (2016b) Image Co-
Localization by Mimicking a Good Detectorâs
Hou L, Zhang Y, Fu K, et al (2021) Infor- Confidence Score Distribution. In: European
mative and Consistent Correspondence Mining Conference on Computer Vision (ECCV)
for Cross-Domain Weakly Supervised Object
24Li Y, Chen Y, Wang N, et al (2019) Scale-Aware Shen Y, Ji R, Wang C, et al (2018a) Weakly
Trident Networks for Object Detection. In: Supervised Object Detection Via Object-
IEEE International Conference on Computer Specific Pixel Gradient. IEEE Transactions
Vision (ICCV) on Neural Networks and Learning Systems
(TNNLS)
Lin TY, Maire M, Belongie S, et al (2014)
MicrosoftCOCO:CommonObjectsinContext. Shen Y, Ji R, Zhang S, et al (2018b) Genera-
In: European Conference on Computer Vision tiveAdversarialLearningTowardsFastWeakly
(ECCV) Supervised Detection. In: IEEE/CVF Confer-
enceonComputer VisionandPatternRecogni-
Lin TY, DollÂ´ar P, Girshick R, et al (2017) Fea- tion (CVPR)
ture Pyramid Networks for Object Detection.
In:IEEE/CVFConferenceonComputerVision Shen Y, Ji R, Wang Y, et al (2019a) Cyclic Guid-
and Pattern Recognition (CVPR) ance for Weakly Supervised Joint Detection
and Segmentation. In: IEEE/CVF Conference
Liu Y, Tian Y, Chen Y, et al (2022) Perturbed on Computer Vision and Pattern Recognition
and Strict Mean Teachers for Semi-Supervised (CVPR)
SemanticSegmentation.In:IEEE/CVFConfer-
enceon ComputerVisionandPattern Recogni- Shen Y, Ji R, Yang K, et al (2019b) Category-
tion (CVPR) Aware Spatial Constraint for Weakly Super-
vised Detection. IEEE Transactions on Image
Liu YC, Ma CY, He Z, et al (2021) Unbiased Processing (TIP)
Teacher for Semi-Supervised Object Detection.
In: The International Conference on Learning Shen Y, Ji R, Chen Z, et al (2020a) Noise-Aware
Representations (ICLR) Fully Webly Supervised Object Detection. In:
IEEE/CVF Conference on Computer Vision
Miyato T, Maeda Si, Koyama M, et al (2018) and Pattern Recognition (CVPR)
Virtual Adversarial Training: A Regularization
Method for Supervised and Semi-Supervised Shen Y, Ji R, Chen Z, et al (2020b) UWSOD:
Learning. IEEE Transactions on Pattern Anal- Toward Fully-Supervised-Level Capacity
ysis and Machine Intelligence (TPAMI) Weakly Supervised Object Detection. In:
Conference on Neural Information Processing
Ren S, He K, Girshick R, et al (2017) Faster Systems (NeurIPS)
R-CNN: Towards Real-Time Object Detection
withRegionProposalNetworks.IEEETransac- Shen Y, Ji R, Wang Y, et al (2020c) Enabling
tions on Pattern Analysis and Machine Intelli- DeepResidualNetworksforWeaklySupervised
gence (TPAMI) Object Detection. In: European Conference on
Computer Vision (ECCV)
Ren Z, Yu Z, Yang X, et al (2020) Instance-
Aware,Context-Focused,andMemory-Efficient ShenY,CaoL,ChenZ,etal(2021a)TowardJoint
Weakly Supervised Object Detection. In: Thing-And-StuffMiningforWeaklySupervised
IEEE/CVF Conference on Computer Vision PanopticSegmentation.In:IEEE/CVFConfer-
and Pattern Recognition (CVPR) enceonComputerVisionandPatternRecogni-
tion (CVPR)
Ronneberger O, Fischer P, Brox T (2015) U-
Net: Convolutional Networks for Biomedical Shen Y, Cao L, Chen Z, et al (2021b) Par-
Image Segmentation. In: International Confer- allel Detection-And-Segmentation Learning for
ence on Medical Image Computing and Com- Weakly Supervised Instance Segmentation. In:
puter Assisted Intervention (MICCAI) IEEE International Conference on Computer
Vision (ICCV)
25Shi M, Caesar H, Ferrari V (2017) Weakly Super- Conference on Computer Vision and Pattern
vised Object Localization Using Things and Recognition (CVPR)
Stuff Transfer. In: IEEE International Confer-
ence on Computer Vision (ICCV) Tang P, Wang X, Wang A, et al (2018)
Weakly Supervised Region Proposal Network
Shi Z, Hospedales TM, Xiang T (2015) Bayesian andObjectDetection.In:EuropeanConference
Joint Modelling for Object Localisation in on Computer Vision (ECCV)
Weakly Labelled Images. IEEE Transactions
on Pattern Analysis and Machine Intelligence Tang P, Wang X, Bai S, et al (2020) PCL:
(TPAMI) Proposal Cluster Learning for Weakly Super-
vised Object Detection. IEEE Transactions
Simonyan K, Zisserman A (2015) Very Deep on Pattern Analysis and Machine Intelligence
Convolutional Networks for Large-Scale Image (TPAMI)
Recognition. In: The International Conference
on Learning Representations (ICLR) TarvainenA,ValpolaH(2017)MeanTeachersAre
Better Role Models: Weight-Averaged Consis-
Singh B, Najibi M, Davis LS (2018) Sniper: tency Targets Improve Semi-Supervised Deep
Efficient Multi-Scale Training. In: Conference Learning Results. In: Conference on Neural
on Neural Information Processing Systems Information Processing Systems (NeurIPS)
(NeurIPS)
Van De Sande KE, Uijlings JR, Gevers T, et al
Singh KK, Lee YJ (2019) You Reap What You (2011) Segmentation As Selective Search for
Sow: Using Videos to Generate High Preci- Object Recognition. International Journal of
sion Object Proposals for Weakly-Supervised Computer Vision (IJCV)
Object Detection. In: IEEE/CVF Conference
on Computer Vision and Pattern Recognition Vo HV, SimÂ´eoni O, Gidaris S, et al (2022) Active
(CVPR) Learning Strategies for Weakly-Supervised
Object Detection. In: European Conference on
Sohn K, Berthelot D, Li CL, et al (2020a) Fix- Computer Vision (ECCV)
Match: Simplifying Semi-Supervised Learning
with Consistency and Confidence. In: Confer- WanF,LiuC,KeW,etal(2019a)C-MIL:Contin-
enceonNeuralInformationProcessingSystems uation Multiple Instance Learning for Weakly
(NeurIPS) Supervised Object Detection. In: IEEE/CVF
Conference on Computer Vision and Pattern
Sohn K, Zhang Z, Li CL, et al (2020b) Recognition (CVPR)
a Simple Semi-Supervised Learning Frame-
work for Object Detection. arXiv preprint Wan F, Wei P, Han Z, et al (2019b) Min-Entropy
arXiv:200504757 Latent Model for Weakly Supervised Object
Detection. In: IEEE/CVF Conference on Com-
Song HO, Girshick R, Jegelka S, et al (2014) puter Vision and Pattern Recognition (CVPR)
on Learning to Localize Objects with Mini-
mal Supervision. In: International Conference Wang X, Zhu Z, Yao C, et al (2015) Relaxed
on Machine Learning (ICML) Multiple-Instance SVM with Application to
Object Discovery. In: IEEE International Con-
Sui L, Zhang CL, Wu J (2022) Salvage of Super- ference on Computer Vision (ICCV)
vision in Weakly Supervised Object Detection.
In:IEEE/CVFConferenceonComputerVision WeiY,ShenZ,ChengB,etal(2018)TS2C:Tight
and Pattern Recognition (CVPR) Box Mining with Surrounding Segmentation
Context for Weakly Supervised Object Detec-
Tang P, Wang X, Bai X, et al (2017) Multi- tion. In: European Conference on Computer
ple Instance Detection Network with Online Vision (ECCV)
Instance Classifier Refinement. In: IEEE/CVF
26Xie Q, Dai Z, Hovy E, et al (2020) Unsupervised Zhang Y, Li Y, Ghanem B (2018b) W2F : A
Data Augmentation for Consistency Training. Weakly-Supervised to Fully-Supervised Frame-
In: Conference on Neural Information Process- work for Object Detection. In: IEEE/CVF
ing Systems (NeurIPS) Conference on Computer Vision and Pattern
Recognition (CVPR)
Xu M, Bai Y, Ghanem B (2019) Missing Labels
inObjectDetection.In:IEEE/CVFConference Zhao Z, Zhou L, Duan Y, et al (2022) DC-
on Computer Vision and Pattern Recognition SSL:AddressingMismatchedClassDistribution
(CVPR) Workshops in Semi-Supervised Learning. In: IEEE/CVF
Conference on Computer Vision and Pattern
Xu Y, Sun Y, Yang Z, et al (2022) H2Fa R-CNN: Recognition (CVPR)
Holistic and Hierarchical Feature Alignment
for Cross-Domain Weakly Supervised Object Zhong Y, Wang J, Peng J, et al (2020) Boost-
Detection. In: IEEE/CVF Conference on Com- ing Weakly Supervised Object Detection with
puter Vision and Pattern Recognition (CVPR) Progressive Knowledge Transfer. In: European
Conference on Computer Vision (ECCV)
Yan G, Liu B, Guo N, et al (2019) C-MIDN:
Coupled Multiple Instance Detection Network Zhou ZH (2018) a Brief Introduction to Weakly
withSegmentationGuidanceforWeaklySuper- Supervised Learning. National Science Review
vised Object Detection. In: IEEE International
Conference on Computer Vision (ICCV) Zitnick CL, DollÂ´ar P (2014) Edge Boxes: Locat-
ingObjectProposalsfromEdges.In:European
Yang K, Li D, Dou Y (2019a) Towards Precise Conference on Computer Vision (ECCV)
End-To-End Weakly Supervised Object Detec-
tion Network. In: IEEE International Confer-
ence on Computer Vision (ICCV)
Yang Z, Mahajan D, Ghadiyaram D, et al
(2019b) Activity Driven Weakly Supervised
Object Detection. In: IEEE/CVF Conference
on Computer Vision and Pattern Recognition
(CVPR)
Zeng Z, Liu B, Fu J, et al (2019) WSOD2:
Learning Bottom-up and Top-down Object-
ness Distillation for Weakly-Supervised Object
Detection. In: IEEE International Conference
on Computer Vision (ICCV)
Zhang D, Zeng W, Yao J, et al (2021) Weakly
Supervised Object Detection Using Proposal-
andSemantic-LevelRelationships.IEEETrans-
actions on Pattern Analysis and Machine Intel-
ligence (TPAMI)
Zhang X, Feng J, Xiong H, et al (2018a) Zigzag
Learning for Weakly Supervised Object Detec-
tion. In: IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR)
27