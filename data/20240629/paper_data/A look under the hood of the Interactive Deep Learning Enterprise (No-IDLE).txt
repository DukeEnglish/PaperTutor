AlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE)
DANIELSONNTAG,MICHAELBARZ,andTHIAGOGOUVÊA,GermanResearchCenterforArtificial
Intelligence(DFKI),Germany
ThisDFKItechnicalreportpresentstheanatomyoftheNo-IDLEprototypesystem(fundedbytheGermanFederalMinistryof
EducationandResearch)thatprovidesnotonlybasicandfundamentalresearchininteractivemachinelearning,butalsorevealsdeeper
insightsintousers’behaviours,needs,andgoals.Machinelearninganddeeplearningshouldbecomeaccessibletomillionsofend
users.No-IDLE’sgoalsandscienfificchallengescentrearoundthedesiretoincreasethereachofinteractivedeeplearningsolutions
fornon-expertsinmachinelearning.Oneofthekeyinnovationsdescribedinthistechnicalreportisamethodologyforinteractive
machinelearningcombinedwithmultimodalinteractionwhichwillbecomecentralwhenwestartinteractingwithsemi-intelligent
machinesintheupcomingareaofneuralnetworksandlargelanguagemodels.
1 INTRODUCTION
Inrecentyears,machineshavesurpassedhumansintheperformanceofspecificandnarrowtaskssuchassomeaspects
ofimagerecognitionordecisionmakingalongclinicalpathwaysinthemedicaldomain(weakAI).Althoughitis
veryunlikelythatmachineswillexhibitbroadly-applicableintelligencecomparabletoorexceedingthatofhumans
inthenext30years(strongAI),itistobeexpectedthatmachineswillreachandexceedhumanperformanceon
moreandmoreappliedtasks.TodevelopthepositiveaspectsofAI,manageitsrisksandchallenges,andensurethat
everyonehastheopportunitytohelpinbuildinganAI-enhancedsocietyandtoparticipateinitsbenefits,inthisproject,
humanintelligenceandmachinelearning(ML)takethecentrestage:InteractiveMachineLearning(IML)isthedesign
andimplementationofalgorithmsandintelligentuserinterfaceframeworksthatfacilitateMLwiththehelpofhuman
interaction.
Ourfocusistoimprovetheinteractionbetweenhumansandmachines,byleveragingstate-of-the-arthuman-
computerinteraction(HCI)approaches,aswellassolutionsthatinvolvestate-of-the-artMLtechniques.Inthisproject,
wefocusonInteractiveDeepLearning(IDL):deeplearning(DL)approachesforIML.Wewantcomputerstolearnfrom
humansbyinteractingwiththeminnaturallanguageforexampleandbyobservingthem.OurgoalinNo-IDLE1is
toimprovetheinteractionbetweenhumansandmachinestoupdateDLmodels,byleveragingbothstate-of-the-art
human-computer-interactionandDLapproaches.Basicandfundamentalresearchinthiscorridorprojectshouldalso
revealdeeperinsightsintousers’behaviours,needs,andgoals.MachinelearningandDLshouldbecomeaccessible
tomillionsofendusers,andbefunctionallymoreadvancedthancurrentrecommendersystemsinonlineshopsthat
providesuggestionsforitemsthataremostpertinenttoaparticularuser.Explicit(ontological)knowledgerepresentation
andreasoningcapabilitiesarehowevernotpartofthisfocusedproject,butafollow-upprojectwouldhighlybenefitfrom
them.Inaddition,weemphasisetheroleofmultimodalinteractionandmixed-initiativeinteraction.Whilefocusingon
IDLinthiscorridorproject,weposethedevelopmentofamethodologyforIDLasachallengeproblem.Amethodology
forIDLwillbecomecentralwhenwestartinteractingmorewithsemi-intelligentmachines.Asalayerusedtorepresent
theinteractions,opinionsandfeedback,itiscriticalthatIMLiswellunderstoodanddefined.Also,therehasbeen
recentandrelativelyrapidsuccessofAIandMLsolutionsthatarisefromneuralnetworkarchitectures.Butneural
networkslacktheinterpretabilityandtransparencyneededtounderstandtheunderlyingdecisionprocessandlearned
1https://www.dfki.de/en/web/research/projects-and-publications/project/no-idle
Authors’ContactInformation:DanielSonntag,daniel.sonntag@dfki.de;MichaelBarz,michael.barz@dfki.de;ThiagoGouvêa,thiago.gouvea@dfki.de,
GermanResearchCenterforArtificialIntelligence(DFKI),Oldenburg&Saarbrücken,Germany.
1
4202
nuJ
72
]GL.sc[
1v45091.6042:viXra2 DanielSonntag,MichaelBarz,andThiagoGouvêa
representations.Makingsenseofwhyaparticularmodelmisclassifiestestdatainstancesorbehavespoorlyattimes
isachallengingtaskformodeldevelopersandisanimportantproblemtoaddress[Hohmanetal.2018].Arelated
argumentationisthatdespitetheirhugesuccesses,largelyinproblemswhichcanbecastasclassificationproblems,the
effectivenessofneuralnetworksisstilllimitedbytheirun-debuggability,andtheirinabilityto“explain”theirdecisions
inahumanunderstandableandreconstructableway[Goebeletal.2018].
InNo-IDLE,weexploretherelationshipbetweenDL,HCI,andexplainableAI(XAI).Forexample,byapproaching
theproblemfromtheHCIperspective,recentworkhasshownthebenefitsofvisualisingcomplexdatainvirtual
reality(VR),e.g.,indatavisualisation[Donaleketal.2014],andbigdataanalytics[Moranetal.2015].InoneHCI
subtaskinNo-IDLEforexample,weextendaninteractiveimageclusteringmethodinVR[PrangeandSonntag2021],
wheretheusercanexploreandthenfine-tunetheunderlyingDLmodelthroughintuitivehandgestures.WhileHCI
constitutesakeyapproach,wewillattacktheIMLproblemfrommultipleangles.Informedbyemergingdirections
inbothresearchandcommercialisationofIMLsystems[Oviattetal.2019;Zachariasetal.2018],wewilldeployour
expertiseinmultimodal-multisensorinterfaces(MMI)andnaturallanguageprocessing(NLP),whilealsotappingonthe
broaderinterdisciplinarycommunity,todeliveronthemissiontoimproveinteractionbetweenhumansandmachines.
PastapplicationprojectsofDFKI’sIMLgroupincludedeepactivelearningsuchasdescribedin[Shuietal.2020],
explanatoryinteractiveimagecaptioning[Biswasetal.2020],IDLsystemsformelanomadetection[Sonntagetal.
2020]andwildlifemonitoring[Gouvêaetal.2023],toolkitsforbuildingmultimodalsystemsandapplications[Barz
etal.2021a;Oviattetal.2019],andinteractionswithMLsystemsasdomain-specificexplanations[Hartmannetal.
2021].InNo-IDLE,webringtheseapproaches,technologiesandourexperiencetogethertoapplythemtoaspecialuse
case,namelyinteractivephotobookcreation,totestandevaluatethebasicandfundamentalresearchinthiscorridor
project.TheproposedprojectbuildsuponthisbroadexperienceandresearchresultsoftheIMLgroupintheareas
ofhuman-computerinteraction(HCI),machinelearning(ML),multimodalhuman-computerinteraction(MMI),and
naturallanguageprocessing(NLP).
Inanutshell,inNo-IDLEweexploreIDLfromfourdifferentperspectives(HCI,ML,NLP,MMI).No-IDLEisabasic
researchprojecttoadvanceourunderstandingofIML.Weexpectpracticalcontributionstobemadewhilebringingthe
fourworkinggroupsofIMLclosertogethertoworkonaspecificapplicationaroundIMLforphotobookcreationand
theexploitationofthefindingsinongoingDFKIconsortialandindustrialprojects.
2 USECASE:INTERACTIVEPHOTOBOOKCREATION
TheresearchquestionsraisedinNo-IDLEwillbeinvestigatedinthecontextofaspecificusecase:theinteractive
creationofaphotobook.Considerthefollowingscenario:
FamilySmith(afamilyoffour)takesmanyphotosfromallkindsofeventsandoccasionsandregularlylikestocreate
personalphotobooksandcalendarsforthemselvesandasgiftsforfamilymembersandfriends.Selectingtheright
photos,arrangingthemandwritingcaptionsisfunbutverytimeconsuming,andwhiletheyappreciateitasameans
oftheirpersonalexpressionandcreativity,theywouldliketospeeduptheprocess,especiallywithrespecttothemore
tediouspartslikeselectingamongsimilarphotosorfindingabasicarrangement.Atthesametime,theywouldliketo
maintaincontrolandapersonalconnectiontotheresults.Eachfamilymemberhastheirownpersonaltaste:someare
moreinclinedtofunnysituationsandphotosofpeople,otherpreferscenicviewsandinterestinglightingandtheir
personalstyleofarrangement,someliketoputthephotossimplysidebyside,othersliketomakeuseofinteresting
frames,clipartandcreativearrangements.Inaddition,thegoalandtargetaudienceinfluencetheirchoices.Forinstance,
theyliketocreatediarytypephotobooksoftheirtravelsfortheirownarchivebutliketotellimagestoriesofthesameAlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE) 3
On thefirstday, we Image
tookthebusfromthe Retrieval
Airport toVancouver
Image …
Captioning
Person
Recognition
Fig.1. Weplantocombineseveralmodulesbasedondeeplearningmodelstocreatephotobookpagesfromnaturallanguageinput.
Thesemodulesinclude,forinstance,imageretrieval,imagecaptioning,andpersonrecognition.
Image
Retrieval
This isSarah, not Mary
+
… Image
Captioning
Person
Recognition
Use correctivefeedbacktoimproveunderlyingDL models
Fig.2. Theusercanprovidemultimodalfeedbacktothephotobooktooltoalterthecreatedcontent.Forinstance,weplantojointly
interprettheuser’sgazesignalandspokenutterancestoimprovepersonrecognition.Anexampleisshowninfigure3.
tripforshowingthemorgiftingthemtoothers.Whentheycreatebooksorcalendarsforspecialholidaysorbirthday
gifts,theytypicallyselectphotosthatsomehowmatchtheoccasionbutthatalsocontainthegiftedpersonifpossible.
Thankfully,theyfindoutabouttheAIsoftwarethatintegratestechniquesdevelopedwithinNo-IDLE.Usingthese,a
photobookcanbecreatedbyprovidingasetofimagesandbysequentiallydescribingtheoccasioninnaturallanguage,
beitaholidaytriporaweddingparty.Theycanalsodescribethestyleandpurposeofthephotobooktoguidethe
creationprocess.Tomakeanexample,imaginethattheyplantocreateaphotobookabouttheirlastfamilytripto
Canada.Theystartoffbytellingthesystem:“ThiswillbeaphotobookforauntMaryaboutourlasttriptoCanada.We
wouldliketoaddsomedramatictouchtoit”.Inreturn,thephotobookcreationtoolsuggestsasuitablecaptionand
basicstyleforthephotobook.Ifnotsuitable,theycaneditthecaptionoradaptthestyle,e.g.,byselectinganother
frametypeforcaptionsoranotherfontfamily.Theywouldcontinuebydescribinghowtheyperceivedtheirvacation
tothephotobooktooljustliketheywoulddescribeittoanotherhuman:“Onthefirstday,wetookthebusfromthe
airporttoVancouver”(seefigure1).Asaresponse,thesystemcreatesasinglepagewithsuitablephotos,i.e.,from
gettingonthebusattheairport,aphotooftheskylineofVancouverfrominsidethebusandonewithauntMarywho
waswaitingforthematthebusstop.SincethisisthefirsttimefamilySmithisusingthistool,theautomaticcaption
generationmoduleisuncertainwhetheritsoutputissuitableand,hence,activelyasksforfeedback.4 DanielSonntag,MichaelBarz,andThiagoGouvêa
This isSarah, not Mary
Fig.3. Exampleofamultimodaluserinputtoourphotobookapplication(basedonanexistingdemosetup).Theuserprovides
correctivefeedbackinnaturallanguagebysaying"ThisisSarah,notMary".Thesystemuseshisgazetoresolvethefacethatwas
referredtoandusesthenewinformationtoupdatetheunderlyingdeeplearningmodelsasdepictedinfigure2.
Beinghappywiththispartialresult,thefamilycontinuestodescribetheeventssaying“Theincidentwiththebears
wasextremelyfunnyandthewoodsweresoimpressive”.Thenewlygeneratedpagesofthephotoincludepicturesof
thebearandthewoodsfromtheirhikingtrip,butnonewithauntMary,sotheycomplainaboutthis.“Pleaseadda
picturewithMaryhere”.AsthesystemdoesnotknowyethowMarylooks,itshowsextractedfacesfromtheprovided
photosandaskstoselectapictureofMary.Mrs.Smithlooksatapictureandsays“that’smysisterMary”.Thesystem
usesthegazesignaltoidentifythefacethatwasreferredtoandlearnstorecogniseMary.Eventually,familySmith
reportshowtheirvacationended:“itwasalsosomethinghowauntMaryhadtotakeustotheairportonshortnotice
becauseourcarbrokedownandwealmostthoughtwewouldn’tmakeitandhowtheywelcomedusbackattheairport
afterwelanded.”OneoftheimagesshowsSarahinfrontofauntMary’scar,butthecaptionstates“ThisisauntMary
aftercarryingustotheairport”.Mr.Smithcorrectsthesystembysaying“thisisSarah,notMary”(seefigures2,3,and
4).Thesystemautomaticallycorrectsthecaptionandcorrectsthelabelforthedetectedface.Fromnowon,thesystem
willbebetteratdifferentiatingbetweenSarahandhersisterMary.Alternatively,Mr.Smithcouldeditthecaptionto
“ThisisSarahinfrontofhercaraftercarryingustotheairportlastminute.”andthefeedbackcontainedinthispost-edit
wouldbeusedtoupdatetheimagecaptioningmodel.
Whiletheinitialdraftofthephotobookisalreadyquitegood,theSmithswanttoaddsomepersonaltouchandthey
alsospotsomeerrorsbrowsingthroughthesuggestions.ThesystemsupportsanimmersivemodeusingVRorjusta
normaldesktop/tablet-basedpresentation.Whiletheycoulduseeithermodeandintuitivehandortouchgesturesin
combinationwithgaze-tracking/spokendialoguetorearrangeandediteachcaptionandphotobypointingortouching
aphotoandselectingfrombetteralternativespresentedbythesystem,byratingaphotoasnotsuitable,orbyprovidingAlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE) 5
Fig.4. Visualisationofthevirtualrealityscenario.Imagesandthephotobookarepresentedinanimmersivevirtualenvironment.
Throughmultimodalinteraction(pointing,eye-/gaze-tracking,naturalspeech)theuserengageswiththesystemandprovides
correctivefeedbackbysaying"ThisisSarah,notMary".Thesystemusesimplicitandexplicitpointingorgazetoresolvethefacethat
wasreferredtoandusesthenewinformationtoupdatetheunderlyingdeeplearningmodelsasdepictedinfigure2.Inadditionto
themultimodalsetupdepictedinfigure3,VRtrackingprovidesdetailedspatialtrackinginformationthatwillbeincludedinthedata
analysis.
feedbacktoacaption,thesystemalsoprovidessomehigher-leveltools:forstory-basedbookstheoveralltimeand
dramaticflowofthestoryandtheincludedeventsarevisualisedalongatimeline(whichworksespeciallywellin
VRthankstoalmostunlimitedvirtualspace).Toavoidclutter,eacheventisrepresentedbysomeiconicphotosanda
summarisingcaption,generatedbythesystem.TheSmithscannowputmoreorlessemphasisoncertainevents,addor
removewholeevents,or“zoom”inandidentifykeycharactersandphotos.Fordiary-typeorlocation-centeredbooks,
thephotosareclusteredaccordinglyandvisualisedoverafloatingmapandagaintheSmithscannoweditandprovide
feedbackusingrichmulti-modalinput.Thesystemwillcontinuetolearnfromtheuserinputandactivelyaskforhelp
inuncertaincases.Therichinput/outputmodalities(especiallyintheVRcase)willbenefituserandsystemonseveral
levels.Theywillmakeactivelearningbythesystemmoreeffectivebecausemultimodalitycanbeusedtodisambiguate
andtocompensatefornoiseinsinglemodalities.Theywillalsoimprovetheuserexperiencebecausetheyallowfora
moreintuitiveandeffectiveinteractionandvisualisationandastheyprovidemoredataabouttheusertothesystem,
thesystemcanlearnmoreeffectively(usingnotonlyexplicitbutimplicitinputs)abouttheuserpreferenceandcan
adapttheinformationload.6 DanielSonntag,MichaelBarz,andThiagoGouvêa
Overthepastdecade,researchershavestudiedsimilarscenarios[Sandhausetal.2008]andproposedpartialsolutions
forcertainsub-task.Forinstance,differentmethodsrangingfromsemanticmodelling[SandhausandBoll2011]and
metadataanalysis[Bolletal.2006,2007]todeeplearningsolutions[Withöftetal.2022]havebeeninvestigatedfor
retrievingandfilteringphotosaccordingtogeneralcriteriaorpersonalpreferences[Maszuhnetal.2021].Someof
theseworkshavealsolookedatdatafromsocialmediaactivitytolearnaboutuserpreferencesorevents[Rabbathetal.
2011a,b].Otherworkshavelookedatthepresentationlayer,forinstance,athowtocreateaestheticlayouts[Sandhaus
etal.2011]orhowtodesignnovelaugmentedrealityinteractiontechniquestoallowuserstoeasilyannotatetheir
photos[HenzeandBoll2011].However,integratedsolutionsforacompletesystemarestillmissing,whichhighlights
boththerelevancebutalsothechallengeofthepresentedscenario.Whilethegoalofthisprojectisnottodevelopa
market-readyphotobookapplicationsoftware,wearecertainthatwewillbeabletoimplementtheusecaseasanAI
testbedtoextendthecurrentstate-of-the-artininteractivedeeplearning.Weproposeauniqueandintegratedapproach
thatdrawsonourexpertisefrommachinelearning,NLP,multimodalinteractionandHCIresearch.
3 GOALSANDSCIENTIFICCHALLENGESOFNO-IDLE
WiththeconvergenceofartificialintelligenceandmachineLearning,IDLiswheretheHCIcommunitymeetstheDL
community[Amershietal.2014;DudleyandKristensson2018;Holzinger2016;Sonntag2010;TesoandHinz2020;
Zachariasetal.2018].No-IDLE’sgoalsandscientificchallengescentrearoundthedesiretoincreasethereachofDL
solutions(andMLsolutionsingeneral):DLfornon-expertsinMLandimprovingDLmodelswhennotenoughdatais
available(e.g.,duetohighlyindividualisedtaskslikephotobookcreation)ordataqualityisnotsufficient.Inaddition,
tofullyautomatetasksinpracticalapplicationssuchasourusecaseofinteractivephotobookcreationcanbeextremely
difficultandevenundesirable.Asaconsequence,ourgoalsaretofindacomputationalanddesignmethodologyto
gracefullycombineautomatedserviceswithdirectuserinputormanipulation.Weinvestigateourscientificgoalsinthe
contextofourphotobookapplication.However,thetechnologiesdevelopedshallbebeneficialforotherdomainsas
wellsuchashealthcareorsmartmanufacturing.Theycanbesummarisedasfollows:
(1) DefineanddeclaretheroleofhumansinIDL(HCI):(1)realisingtheimportanceofstudyingusers;(2)reducing
theneedforsupervisionbyMLpractitioners;(3)exploreinteractivityinatightcouplingbetweenthesystem
andtheuser;(4)handlehumanambiguityandconfusionandinstiltrustandconfidencethroughfeedbackand
explanations;(5)exploregamificationandseriousgamesinthecontextofIDLandIMLingeneral.
(2) Provideawayforusersto(1)understandwhythesystemhadmadeaparticularprediction,and(2)adjustthe
(DL)learner’sreasoningifitspredictionwaswrong.Tothisend,thesystemshouldprovideanexplanationfor
itspredictions,andincorporatecorrectivefeedbackgivenbytheuser.Howcanthisbedoneinpracticalterms?
Forprovidingusefulexplanationsofmodelpredictions,wewillinvestigatethefeasibilityofsolvingtaskswith
interpretable(DL)modelsratherthanblackboxmodels[RudinandRadin2019].
(3) Activeandpassiveuserinputneedstobeinterpretedcarefullytoestablishanefficientandeffectiveinteraction
betweenhumansandanAIsystem.Thechallengeincludestointerpretsignalsfrommultipleinputmodalities
(e.g.,gazeandspokeninstructions).Itmayberequiredtointerprettheinputsignalsaccordingtoauserorcontext
model(e.g.,reflectingauser’spreferencesortheinteractioncontext).InNo-IDLE,wedevelopmultimodal
interactiontechniquesforincrementalphotobookcreationwiththegoaltoimprovemodeltrainingthrough
richmultimodaluserfeedbackandtoimprovetheuserexperiencethroughrobustandintuitiveinterfaces.At
thesametime,weshouldavoidthelimitationsofhumancognitiveabilities.AlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE) 7
(4) Implementmixedinitiativeinteraction,anopportunitytoexploreinterfacesthatcanleverageknowledgeand
capabilitiesofdomainexpertsmoreefficientlyandeffectively.TheMLsystemandthedomainexpertshould
engageinatwo-waydialoguetofacilitatemoreaccuratelearningfromlessdatacomparedtotheclassical
approachofpassivelyobservinglabelleddata.Inthecontextofourphotobookusecase,weaimatusing,
e.g.,activelearningandprinciplesfromhuman-in-the-loopexpertsystems.Thegreatergoalistoperform
applicationtasksmoresatisfactorily:human-machineteamsshallsurpasstheefficiency/effectivenessofhumans
ormachinesinthistaskalone[vanZoelenetal.2023].
3.1 NaturalLanguageProcessing(NLP)
Ourapproachforsupportingphotobookcreationreliesonseveralcomponentsbasedondeeplearningmodelsforimage
andmultimediadata,inparticularfaceandbodyshaperecognition,text-to-imageretrieval,imagecaptioning,visual
storytelling,andVisualQuestionAnswering(VQA)models.Thedifferentcomponentsaretriggeredbasedonauser’s
commands(e.g.,"Onthefirstday,wetookthebusfromtheairporttoVancouver"triggersthetext-to-imageretrieval
componentandtheimagecaptioningcomponent).Weplantomodelthisbyeitherexplicitlymappingtriggering
keywordstocomponents,orbyapplyingmoresophisticatedsemanticparsers.Theoptimalwayofprocessinguser
inputwillbedeterminedinthecourseoftheprojectbasedoninsightsfromuserstudies.InthispartoftheNo-IDLE
project,weinvestigatethreecoreresearchproblemsassociatedwiththeapplicationandinteractionwithDLmodels
inthecontextofourusecase:(1)howtoadaptstate-of-the-artmultimediaDLmodelstoprocessuser-specifictexts
andimages,which,incontrasttothegenericdatathemodelsareusuallyappliedto,requirestoaccountforspecific
informationrelatedtotheuserandtheeventstheywanttopresentintheirphotobook;(2)howtoimprovetheDL
componentsbasedonuserfeedbackcollectedintherefinementphasebasedontheIMLparadigm;(3)howtousemodel
explanationstoachieveoptimalinteractionbetweenuserandmodelandbestsupportthephotobookcreationprocess.
Usershaveapersonalrelationshipwithobjectsandconceptsdisplayedintheimagesoftheirphotobook,and
providingsupportinthephotobookcreationprocessrequiresmodellingimagecontentfromauser’sperspective.For
example,weneedtotakeintoaccountthatauserwillrefertonamedentitiesinanimagebypropernameratherthana
commonnoun(Maryinsteadofawoman).InNo-IDLE,weinvestigatehowtoadaptmultimediaandmultimodalDL
modelstoaccountforsuchuser-specificinformation.Forcross-modal(text-to-image)retrieval,weplantoimplement
state-of-the-artDLretrievalmodels[Alikhanietal.2022;Jiaetal.2021;Zhangetal.2020],whichretrieveitemsbased
onembeddingsimilaritiesinasharedrepresentationspace,incombinationwithrule-basedfiltersthattakeintoaccount
outputfromapersonrecognitionmodelaswellasavailableimagemetadata,suchastimestampsandgeolocation.For
example,givenauserqueryShowmethepicturesofPeterandMaryplayingfootballwhenwevisitedVancouver,the
componentretrievesimagesgiventhequeryTwopeopleplayingfootballandreturnsthesubsetofimagesforwhich
thepersonrecognitionmodelindicatesPeterandMarybeingpresent,andthegeolocationindicatesanimagetaken
inVancouver.IncontrasttoimagecaptionsthatcanbefoundingeneralpurposedatasetssuchasMSCOCO[Lin
etal.2014]orFlickr30k[Plummeretal.2015],thecaptionsgeneratedbyourcaptioningcomponentshouldbe(1)
entity-aware(e.g.,insteadofgenericdescriptionsofobjectsorconcepts,thecaptionscontainpropernamesfornamed
entities),(2)stylised,and(3)controllable(seetable1forexamples).Existingmodelsforentity-awarecaptioningusually
firstgenerateatemplatecaptionwithplace-holdersfornamedentities,whichisthenfilledwithinformationretrieved
fromassociatedtextorknowledgebases[Bitenetal.2019;Luetal.2018].Ramnathetal.[Ramnathetal.2014]propose
anapproachforpersonalisedtemplate-fillingwithinformationsuchasgeolocation,timestamp,detectedlandmarks,
recognisedfaces,whichweplantoextendtoincorporatefiner-grainedlocationinformationspecifiedbytheuser.To8 DanielSonntag,MichaelBarz,andThiagoGouvêa
generatestylisedcaptions,wewillexplorecaptiongenerationreflectingsentiment[Mathewsetal.2016],specificstyles
[Ganetal.2017;Guoetal.2019],andtakingintoaccountauser’sactivevocabulary[ChunseongParketal.2017].In
therefinementphase,whenadditionalcaptionsaregeneratedfornewlyretrievedimages,theusershouldbeableto
exertfine-grainedcontrolovertheconceptstobeincludedinthecaption,e.g.,byactivelymodifyinganabstractscene
graphrepresentationbasedonwhichthecaptionisgenerated[Chenetal.2020].Incontrasttogeneratingcaptions
forimagesinisolation,thevisualstorytellingcomponentgeneratesasequenceofcaptionsthatformacoherentstory
foraretrievedsequenceofimages[Huangetal.2016;Jungetal.2020;Wangetal.2020].Similartothecaptioning
component,thevisualstorycomponentneedstobeentity-awareandcontrollable.Tothisend,wewillinvestigateto
whatextentapproachesforadaptingthecaptioningmodelcanbetransferredtothevisualstorytellingtask.Finally,
intherefinementphase,aVQAcomponentcandirectlyanswertheuser’squestionsaboutimagecontent,suchas
Whatwasthenameofthemountaininthebackground?,orDidPeterjoinusforthetriptoLakeBaikal?.Here,wewill
focusonimplementingmodelsforansweringquestionsthatcannotbeansweredfrominformationintheimagealone,
butrequireadditionalknowledgeaboutnamedentitiesandspecificevents,thatcouldforexamplebeprovidedbya
knowledgegraph[Shahetal.2019].
Inordertoimprovetheabovedescribedcomponentsbasedonfeedbackcollectedinthephotobookrefinement
phase,weimplementanIMLframeworkthatallowsustoiterativelyupdatethemodelsbasedonnewinformation
viaincrementalandfocusedupdates[Amershietal.2014].TrainingandimprovingthemodelsinanIMLframework
iscrucialtoourusecase,aswecannotassumelargeamountsoflabelledpersonaliseddatatobeavailableatonce,
andthereforeneedtolearnfromuser-specificdataincrementally.InNo-IDLE,weexplorehowIMLcanbeappliedto
improvethemultimodalDLcomponentsforphotobookcreation,consideringthreescenarios:(1)debuggingtrained
models,e.g.,identifyingandcorrectingspuriouspatternslearnedbythemodel[LertvittayakumjornandToni2021].
Here,weassumeanexplanation-basedinteractivelooptobeparticularlyhelpful;(2)adaptingpre-trainedmodelsto
user-specificdatawithsmallamountsofannotations[Yaoetal.2021](3)personalisingmodels[Kuleszaetal.2015],e.g.,
forgeneratingcaptionsfollowingstylisticpreferencesofusers.Wefocusonimprovingmodelsbasedonexplanatory
feedbackprovidedbytheuser,i.e.,insteadofprovidingonlylabel-levelfeedback(e.g.,acorrectanswertoaVQAmodel),
theuseradditionallyprovidesinformationthatstateswhytheprovidedansweristhecorrectone.Interactingonthe
basisofexplanationshasthepotentialtobenefitboththeuserandthemodel:ontheuserside,providingricherfeedback
beyondthelabellevelisinlinewiththeirpreferredwayofinteraction[Amershietal.2014;Ghaietal.2021].Fromthe
modellingperspective,learningfromexplanatoryfeedbackinsteadoflabel-levelfeedbackcanimprovedataefficiency
[Hancocketal.2018;Yeetal.2020]andgeneralisation[Yaoetal.2021].Wefocusonthetwomostcommonlyconsidered
typesofhumanexplanations,whicharehighlightexplanations,i.e.,subsetsofinputelementsdeemedrelevantfor
assigningaspecificlabel;andfree-textexplanations,i.e.,naturallanguagestatementsprovidinginformationabout
whyspecificlabelshouldbeassigned[WiegreffeandMarasovic2021].Severalwaysforimprovingmodels(exceptfor
[Selvarajuetal.2019]theseweredevelopedformodelsthatprocesseithertextorimagedata)basedonsuchhuman
explanationshavebeenproposed[Hartmannetal.2021;HaseandBansal2021]:usingnaturallanguageexplanations
asadditionalinputs[Co-Reyesetal.2019;Rajanietal.2019;Rupprechtetal.2018],usingexplanationgenerationas
auxiliarytask[Camburuetal.2018;Haseetal.2020;Narangetal.2020;Wiegreffeetal.2021],directlyconstraining
intermediaterepresentations[Riegeretal.2020;Rossetal.2017;Selvarajuetal.2019;Shaoetal.2021],orexploiting
explanationstogenerateadditionaltraininginstances[Awasthietal.2020;Hancocketal.2018;Yaoetal.2021;Ye
etal.2020].WewillinvestigatehowtocombineandextendthesemethodstoupdatemultimodalDLmodelsbased
onmultimodalfeedback.Mostoftheseapproacheshaveonlybeentestedinofflinesetups,wherethemodelcanbeAlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE) 9
trainedontheentireexplanatoryfeedbackatonce.Asafirststep,wewillinvestigatewhichmethodsareapplicablein
aninteractivesetupwheremodelsareupdatedincrementally.AsallDLcomponentsprocessthesameuser-specific
data,weassumethatitmightbeusefultoshareuser-specificinformationamongthecomponentsbyexploitinguser
feedbacktoupdatemultiplecomponentsatonce.Tothisend,wewillexperimentwithamulti-taskarchitecturewith
hardparametersharing,whichtrains𝑛 modelsfor𝑛 taskswithasubsetofparametersbeingsharedamongthem
[Caruana1993;Collobertetal.2011],e.g.,sharingthemulti-modalencoderwhilemaintainingtask-specificclassifier
layers(ordecodersforlanguagegenerationtasks).Byupdatingtheencoderbasedonfeedbackcollectedforonetask,
theinformationwillbeavailabletomodelsfortheothertasksaswell.Forevaluatingourmethodsforinteractivedeep
learning,wewillfollowpreviousworkinre-splittingexistingtask-specificdatasets(e.g.,MicrosoftCOCO[Linetal.
2014]andFlickr30k[Plummeretal.2015]forimagecaptioningandtext-to-imageretrieval,VQAv2[Goyaletal.2017]
andKB-VQA[Wangetal.2017]forvisualquestionanswering,VIST[Huangetal.2016]forvisualstorytelling)into
newdatasplitsthatallowtoevaluatespecificmodelbehaviour,e.g.,ifamodelrelieslessonlanguagebias[Agrawal
etal.2018],orifamodelhasbettercontinuallearningabilities[DelChiaroetal.2020;Grecoetal.2019].
ThecentralcomponentofanIMLsystemisatightinteractiveloopbetweenuserandMLmodel,inwhichthemodel
presentsitscurrentstateofknowledgetotheuser,andtheuserprovidesfeedbacktothemodelaccordingly[Amershi
etal.2011;DudleyandKristensson2018;Wangetal.2021].Theformerpartoftheloopcouldbesupportedbyshowing
anexplanationforwhythemodelmadeaspecificpredictionortookaspecificaction.Theabilitytoprovideexplanations
forpredictions,i.e.,informationaboutthereasonsforwhyaspecificpredictionwasmade,isconsideredessential
forlarge-scaleadoptionofAIsystemsbyend-users[BarredoArrietaetal.2020;Gunning2017].InNo-IDLE,wewill
investigatehowtousemodelexplanationstoachieveoptimalinteractionbetweenuserandmodel.ForDLblack-box
models,thisrequireschoosinganadequatemechanismtoconstructexplicitrepresentationsofexplanationsthatcan
beprovidedtotheuser[Kimetal.2021].Whileforimageprocessingmodels,saliencymethodscanprovideuseful
visualisationsofimportantinputregions,suchmethodsarelessintuitivefortextinputs.Here,thecompositionalnature
oflanguagecallsformoreexpressiveattributionmethodsthatcanmodelinteractionsbetweeninputtokens[Bastings
andFilippova2020].Wefocusonthegenerationofsuitableexplanationsforgenerativeorpredictivemulti-modaltasks,
e.g.,bygeneratingnaturallanguageexplanationswhileatthesametimemarkingimageregionsthatwererelevantfor
aprediction[Parketal.2018a].Forpresentingtheexplanationtothetargetend-user,weinvestigatethepersonalisation
ofexplanations[Ghaietal.2021;Mohsenietal.2021;Rasetal.2018;SokolandFlach2020;Tomsettetal.2018]toelicit
highqualityfeedbackandincreaseusersatisfaction.Howtoevaluatemodelexplanationsisanactiveresearchtopic
[DeYoungetal.2020;Doshi-VelezandKim2017;JacoviandGoldberg2020;Pruthietal.2022]andwewillfocuson
usingpreviouslyproposedmetricsforcomparingmodel-generatedexplanationswithhuman-generatedexplanations
onpubliclyavailablemulti-modaldatasets,inparticularVQA-Xande-ViL[Kayseretal.2021;Parketal.2018b].
Themaindeliverablesofthispartoftheprojectare:
(1) Implementationofmulti-modalDLcomponentsforphotobookcreationsupportthatareentity-awareand
controllable.Forimagecaptioningandvisualstorytelling,thecomponentsshouldbeabletogeneratetextina
specificstyle.
(2) ImplementationofanIMLframeworkwhichallowstoupdatetheDLcomponentsbasedonexplanatoryuser
feedbackcollectedinthephotobookrefinementphase.Inadditiontolearningfromexplanatoryfeedback,
themodelshouldretainitsknowledgewhilelearningnewthings,whichcallsfortheapplicationofcontinual
learningmethods[Biesialskaetal.2020;d’Autumeetal.2019;Lietal.2020]withinthefeedbacklooptoprevent10 DanielSonntag,MichaelBarz,andThiagoGouvêa
catastrophicforgetting[Kirkpatricketal.2017].Incompletenessanduncertaintyofhumanexplanations[Tan
2021]shouldbeaccountedforwhenimplementingafeedbackmechanismintothemodelasasoftwarepackage.
Tothisend,wewillbuildoninsightsfromthecoreMLpartoftheprojectthatinvestigatestheuseofBayesian
modellingforfeedbackintegrationasdescribedinsection3.3.
(3) ImplementationofXAImethodsformultimodalmodelswhichprovideexplanationsforblackboxDLmodel
decisionsandtakeintoaccountuser-specificinformation,e.g.,backgroundknowledgeandthemotivationfor
consumingtheexplanation.
3.2 Multimodal-MultisensorInteraction(MMI)
InNo-IDLE,weaimatdevelopinginteractivetrainingmechanismsthatenablecontinuousimprovementsofDLmodels.
Acentralaspectofthisinteractiveloopishumanfeedback.Weinvestigatetheeffectofintegratingmultimodaluser
inputontheeffectiveness,efficiency,andusabilityofinteractivemodeltraining.Wetargetmodelsofourphotobook
applicationwhichinclude,forinstance,modelsforrecognizingspecificpersonsandobjects(seesection3.3)andnatural
languagegenerationmodels(seesection3.1).
Onegoalistoimplementagaze-drivendialoguethatcansupporttheinitialcreationanditerativerefinementofa
photobook.ThemultimodalfeedbackfromtheusershallenabletheunderlyingDLmodelstolearnnewconcepts,
todifferentiatebetweeninstancesofaconcept,andtoimprovethedetection/recognitionofknowclasses.Weplan
toimplementsimplestate-baseddialoguestorealiseinteractivemodeltrainingwithhumangazeasadditionalinput
modality(e.g.,basedontheopensourcedialogueplatformRasa2).Thegoalisnottodevelopbeyondstate-of-the-
artmultimodaldialoguesystems,buttoinvestigatetheeffectofintegratinggaze(orpointinggestures)insimple
speech-basedinstructionsontheusabilityandeffectivenessofinteractivemachinelearningsystems.Forinstance,
afacerecognitionmodelcouldwronglydetectSarahasMaryasdescribedinsection2.Whentheuserdetectsthat
thepersonidentificationsystemfailed,hecouldprovideacorrectivefeedbackinnaturallanguage:"ThisisSarah,
notMary".Thesystemshouldanalysetheuser’sgazetoidentifytowhichfacehereferredinhisutterance.Figure3
illustratesthisinteractionbasedonanexistingdemosetupwiththreewall-sizedscreens.Thiscorrectivefeedbackshall
beusedtoimprovetheunderlyingdeeplearningmodels(seefigure2).While,inNo-IDLE,weputafocusongaze-based
input,pointinggestureswillbeconsideredforthiskindofreferenceresolutionaswell,especiallyinthecontextof
AR/VRinteractionsettingsorwheninteractingwithawall-sizedscreen.Also,multimodalinteractioncanbenefitfrom
system-initiatedinteraction.Thisisparticularlyinterestingincombinationwithactivelearningtechniquesthatshall
bedevelopedbytheMLgroup(seesection3.3).Wewanttoexploretheeffectiveness(doesthesystemactuallylearn
torecognisenewpersonsandobjects),efficiency(whattimeisrequiredforthemodeluntilitcanrecogniseanew
class),andusability(isthesystemusableforlayusers)ofdifferentapproachesincollaborationwiththeHCIgroup(see
section3.4).Anothergoalistoproducecaptionsthataremorefocusedonwhattheuserwantstodescribe.Weplanto
integrateaggregated[Corniaetal.2018;SuganoandBulling2016]orsequential[Mengetal.2021;Pont-Tusetetal.
2020;Takmazetal.2020]humanattentiontracesestimatedfromthemultimodalinputsignal(gazeandpointing)into
thegenerationprocess.Wehypothesisethatincorporatingmultimodalinteractionsignalscanimprovetherobustness
ofandtheuserexperienceduringtheinteractionwithaninteractivemachinelearningsystem.Eventually,thisshould
improvethequalityofhumanfeedbackand,hence,theefficiencyofmodelupdatesduringtraining.Also,weexpect
2https://rasa.com/open-source/AlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE) 11
thatmultimodalinteractioncanleadtoabetterunderstandingofhowamodelworks,toabetterunderstandingofthe
model’sstrengthsandweaknesses,andeventuallytomoretrustinthemodel’sdecisions.
Humangazeiswellknownforcarryingnon-verbalcuesthatcanbeusedintelligentuserinterfaces:theeyemovement
behaviourdependsonthetaskinwhichauseriscurrentlyengaged[DeAngelusandPelz2009],whichprovidesan
implicitinsightintotheirintentionsandallowsanexternalobserverorintelligentuserinterfacetomakepredictions
abouttheongoingactivity[FlanaganandJohansson2003;GredebäckandFalck-Ytter2015;Rothkopfetal.2016;Rotman
etal.2006].Forinstance,knowingwhichobjectsinascenearefixatedisavaluablecontextinformationforspoken
feedbackinpersonalisedphotobookcreation.Inparticular,whendeicticreferencesmustberesolved[Matuszek2018;
Mehlmannetal.2014].Also,thereisastronglinkbetweengazebehaviourandspokenlanguage:speakersfixate
elements“lessthanasecondbeforenamingthem”[GriffinandBock2000]andthecoordinationofhand-movements
dependsonhumanvision,e.g.,when“directingthehandorobjectinthehandtoanewlocation”[Landetal.1999].
Humangazecanalsobeusedtoanalyseormodelthebehaviourofauser(usermodelling),e.g.,tolearnabouta
user’songoingactivity[Bullingetal.2013;SteilandBulling2015],theirpreferences[Barzetal.2022;Lalléetal.2021],
intentions[Barzetal.2020b;HuangandMutlu2016],orstate[BullingandZander2014;Huangetal.2019].Observing
eyemovementbehaviourduringinteractionwithaninteractivemachinelearningsystemcouldrevealsituationsin
whichtheuserdisagreeswiththemodeloutput.Ifthesesituationscoincidewiththemodelbeinguncertainaboutthe
output,thismaybeagoodpointintimetotriggerafeedbackrequesttotheuser(system-initiative).
InNo-IDLE,wefocusonhumangazeandpointinggesturesasadditionalinteractionmodalities.Weinvestigate
theimpactofusingmultimodalinteractionsignalsonrecognisingobjectsorpersonsascontext-informationandto
personalisethenaturallanguagegenerationprocessinthecontextofthephotobookcreationandrefinementprocess.
Thechallengeisthatrelevantpersonsandobjects,theirappearance,orsimilarpropertiescansignificantlyvarybetween
usersandtheoccasionforcreatingsuchabook[BarzandSonntag2021].However,pre-trainedmodelscannotaccount
forsuchdynamiccircumstancesandadaptivemodelsoragentsarerequiredthatincrementallyandcontinuouslylearn
fromhumancollaboratorsorinterlocutors.ThemaindeliverableisasoftwareextensionofanexistingDFKIsystem,
themultisensor-pipeline(MSP)3.Theresultingmodulesshallbeintegratedandevaluatedinthephotobookcreation
processbasedontheexperimentalprocedureasdepictedinsection3.5:
(1) Implementation of a module that enables to learn about unseen classes (objects) when the context shifts
(class-incrementallearning)andtoimprovetherecognitionofknownclassesviamultimodaluserinteraction
basedon,e.g.,transferlearning[Kädingetal.2017]andactivelearning(seesection3.3).Similarly,weplanthe
implementationofamoduletodifferentiatebetweenmultipleinstancesofthesameclasstroughmultimodal
human-machineinteraction.Wefocusonthedifferentiationbetweenmultiplepersonsaccordingtoourphoto
bookusecase(e.g.,tofilterforimagesshowingaparticularperson).Thispartwillbenefitfromnovelactive
learningapproachesasdescribedinsection3.3.Wewillalsoinvestigateinhowfarthismodulecanbeused
totrackmetainformationlikeownership(seetheCOPDAproject4).Real-timetrackingofmultipleinstances
couldbeachievedbyacombinationof(multi-)objecttracking[Lietal.2019]andmodelsthatestimateobject
propertiessuchascolour,size,andshape[Thomasonetal.2016].Suchmodelsareofparticularinterestwhen
groundedinnaturallanguage,whichwouldfacilitateexpressiveexplanationsforclassificationresults(related
tosection3.1andtheXAINESproject5).
3https://github.com/DFKI-Interactive-Machine-Learning/multisensor-pipeline
4https://www.dfki.de/en/web/research/projects-and-publications/projects-overview/project/copda
5https://www.dfki.de/en/web/research/projects-and-publications/projects-overview/project/xaines12 DanielSonntag,MichaelBarz,andThiagoGouvêa
(2) Implementation of a module of a new image clustering and object tracking method that can help "quick
start"multimodalinteractivemodeltrainingupondomainshifts,becauseasingle(user-provided)labelcanbe
propagatedtomultiplesamples,e.g.,toanimageclusterortosamplesfromobjecttracking(semi-supervised
learning).Theideaistoclusterfixatedimagecontentsinthephotobookapplicationand,oncealabelisprovided
viaspeech,topropagatethislabeltothewholecluster.Similarly,fewshotlearning(FSL)fromtheMLtask(see
section3.3)shouldhelptoovercomethiscold-startproblem.Few-shotimageclassification[Wertheimeretal.
2021]orfew-shotobjectdetection[Fanetal.2019]enablesimageclassificationorobjectdetection,respectively,
usingaroundfiveexampleimages.
(3) Implementationofmultimodalinteractiontechniquesbasedoneyetrackingforthephotobookapplication.
This includes approaches to provide feedback on model outputs multimodally (e.g., correcting labels for
misclassifiedpersons,triggeringpost-editingofgeneratedcaptions,andguidingthecaptiongenerationprocess),
butalsogeneralmultimodalinteractionwithphotobookrepresentationsindesktoporVRsettings(forinstance
rearrangingimages,selectingbetterphotos,orsimilarselectionandmanipulationactions).
3.3 MachineLearning(ML)
AnimportantfactorwhichcontributestotherecentsuccessofDL(apartfromsuperiorcomputingpowerandtraining
algorithms)istheavailabilityoflabelleddata.Infact,neuralnetworksareknowntobedata-hungry(e.g.,popular
benchmarkdatasetsrangefromtensofthousandsoflabelledsamplesasinCIFAR-10tomillionsasinImageNet
dataset).However,datalabellingisacostly,humanlabourintensiveactivity.Incertaindomainssuchashealthcareand
biomedicinewhereconsiderableexpertisemayberequired,datalabellingbecomesalimitingstepintherealisation
ofthevalueofML.Thisisalsothecaseforthecreationofpersonalisedphotobooks.Forinstance,whenthesystem
shouldlearntodifferentiatebetweenfacesandbodyshapesofasetofpersonsinordertoselectimagescontaining
themornotwhilethepersonsmaydifferperuserandphotobook.Thus,itisimperativetobuildMLalgorithmswhich
arecapableoflearningfromsignificantlyfewerlabelledsamplestosavehumantime.
Asetofmethodsknownasactivelearning[Monarch2021;Settles2010]tacklethisproblembyallowingthesystem
toidentifyasubsetofmaximallyinformativesamplesfromagivenpoolofunlabelleddatatobequeriedforadditional
labelling/feedback.InthecontextofIMLinthisproposal,activelearningplaysakeyroleinhowalearningsystem
requests,receives,andlearnsfromuserinput.IncombinationwiththeHCItasks(section3.4),thisformsajointtaskfor
mixed-initiativeinteraction:MLsystemandhumandomainexpertengageinatwo-waydialogue,facilitatinglearning
fromlessdatacomparedtotheclassicalapproachofpassiveconsumptionoflabelleddata.Onedirectiontoexplore
arenewinputtechniquesthatallowuserstoprovidemoreinformativefeedback[Ratneretal.2016],comparedto
traditionallowdimensionallabels.
Popularmethodsinactivelearningmightbeuncertainty-based[Joshietal.2009;Konyushkovaetal.2019;Tongand
Koller2001],density-ordiversity-basedapproaches[GissinandShalev-Shwartz2019;Souratietal.2018],ensemble
methods[Beluchetal.2018;Freundetal.1997;McCallumzyandNigamy1998],andexpectederrorreduction[Roy
andMcCallum2001].Acommonproblemofpureuncertainty-basedmethodsisthattheselectionstrategydepends
ontheperformanceofanexistingmodel.Thiscouldbeproblematicintheearlyphaseoftrainingsinceoutcomes
arelikelytobeunreliable,leadingthealgorithmtoquerypoorexamplesandthusleadtoinefficiencies.Similarly,in
puredensity-basedapproachesdatalabellingcouldberedundantifthepresentmodelproducesalreadyhighconfident
predictions.Recently,methodshavebeenproposedwhichtrytomitigatethisproblembycombiningandbalancing
uncertaintyanddiversityofthenewsamplesw.r.t.thedatadistribution[Ashetal.2020;Huangetal.2010;OzdemirAlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE) 13
etal.2018;Smailagicetal.2018;Yangetal.2017].Bayesianapproacheshavealsobeenproposed[Galetal.2017;Kapoor
etal.2007;Kirschetal.2019],buttheydonotscalewelltodeepnetworkswithlargedatasets.Otherrecentworks
includeFisherinformation[Ashetal.2021;Souratietal.2018]andlearningtoselectfromdata[Konyushkovaetal.
2017].
WetailoractivelearningtechnologiestobeappliedinNo-IDLEinthecontextofourphotobookscenario.Thegoalis
totrainamodelthatisabletodifferentiatebetweenindividualpersonscontainedinasetofphotoswithlittlelabelling
effortbytheuser.Thebasisforthisfeaturearecomputervisionmodelsthatenablearobustdetectionandlocationof
facesandbodyshapes.Foranysetofimages,thesemodelscanprovideapoolofunlabelledfaceandbodyimages.This
ishelpfultofilterforimagesshowinghumansversus,e.g.,landscapephotos.However,forpersonalisedphotobooks,
wewantthesystemtobeabletodifferentiatebetweenindividualpersonstofilterforphotoswithspecificpersons.For
instance,auserrequestcouldbe“pleaseaddanimageofMaryinfrontofourrentalcar”.Thepersonsinvolvedmay
varyastheyarehighlydependentontheuserandtheoccasionforcreatingthephotobook.Wewill(1)implement
andevaluatenewsamplingtechniques/activelearningapproachesthatenablemodeltrainingwithsmallamountsof
labelleddataand(2)investigatewhensystem-initiativefeedbackrequestsshouldbeshownandhowtheyshouldbe
designedinordertomaintainagooduserexperience.Agoodopportunitytotriggerafeedbackrequestcouldberight
afterausertakestheinitiativetoprovideanewname(i.e.,alabel)foraperson/faceorcorrectsalabel.Forinstance,if
ausertellsthesystem"thisisMary",thesystemcouldqueryforthemostinformativeunlabelledinstancesthatmay
alsoshowMarylike"Ah,thisisMary.Iguess,I’veseenheronotherpicturestoo.IsthisMaryagain[systemshows
anotherfaceimage]?".
Inthisproposal,weaimtoaddressthefollowingMLproblems:
(1) Ontheexperimentalside,wefirstinvestigatetheperformanceofexistinguncertaintyfunctionsforvarious
neuralnetworkarchitecturesonimageclassification/segmentationtasks(see,e.g.,figure5).
(2) Ontheexperimentalside,thispointisrelatedtostudyingifweshouldonlyusetheDLblackboxmodelsinthe
IMLprocesswhenweperhapsdonotneedto.Thepointbroughtforwardin[RudinandRadin2019]isthat
onemightconsiderthat(inIML)maybeinterpretabledeep-learningmodelscanbeconstructed,ortransparent
modelsbeusedinconjunctionwithDLmodelsaccordingtotheuserfeedback.Inmachinelearning,theseblack
boxmodelsarecreateddirectlyfromdatabyanalgorithm,meaningthathumans,eventhosewhodesignthem,
cannotunderstandhowvariablesarebeingcombinedtomakepredictions.Alsoseesurrogatemodelsforthis
purpose,co-creatingatransparentmodelfromthepredictions.Aglobalsurrogatemodelisaninterpretable
modelthatistrainedtoapproximatethepredictionsofablackboxmodel.Wecandrawconclusionsaboutthe
DLblackboxmodelbyinterpretingthesurrogatemodel[BurkartandHuber2021].
(3) OnthepracticalsideincludingFew-Shot-Learning:themotivationforthisMLtaskcomesfromMMI(section
3.2),wherewewantthesystemtolearnnewobjectsduringaninteractivetrainingsessionwiththeuser,given
thattheuserhasprovidedfeedback/labelsforafewexamples.Intheliterature,thisproblemcouldbetackled
usingtechniquesfromFSL[Tianetal.2020].Themainchallengeishowtolearnagoodlatentembeddingsof
theinputsandthelabels,andtoalignthemtogetherinsuchawaythatcertainattributesfrombothinputsand
labelscanbetransferredtounseenobjects.
Theresearchoutcomesandmaindeliverableswillincludethedesignofnewuncertaintyfunctions,whichwillbe
usedinIML-relatedtaskssuchasNLP(section3.1)andMMI(section3.2).Additionally,togetherwithHCI(section3.4)
wewillpromoteanactiveroleforthehuman-in-the-loop:besidesprovidinglabels,wewanttoexploredifferent14 DanielSonntag,MichaelBarz,andThiagoGouvêa
waysofproviding/correctingexplanations,aligningimportantfeatureslearnedbythemachinewithhumanintuition,
interpretinglearnedmodels,andfindingacommongroundwithgeneralHCItasks,includingamoregenericapproach
forgeneratingexplanationsandinsightsintotheeffectivenessoffew-shotlearning.
3.4 Human-ComputerInteraction(HCI)
WeexploretheroleofhumansinIDL.Fromourownpreviouswork[Herrlichetal.2017]andfromtheliterature[Oviatt
2006;Picard2000;RyanandDeci2000],therelevanceofmotivation,emotionandfactorslikecognitiveloadonhow
interfacesandsystemsareusedand,consequently,howthesefactorsshouldbetakenintoaccountduringinterface
design is quite clear. IDL presents both a potential solution and an additional challenge in this regard [Amershi
etal.2014].Furthermore,wewanttotransferinsightsfromourpreviousworksinthemedicaldomainandvirtual
reality.Wehavestudiedexpertuserssuchasmedicaldoctors6andexploredVRforIDL,e.g.,forimageclassification
inVR[PrangeandSonntag2021],andasageneralprototypingandevaluationenvironmentforhuman-centered
interactiondesign[KlonigandHerrlich2020;OmarJubranetal.2021;Quecketal.2022;Reinschluesseletal.2017;Vera
Eymannetal.2021].
Referringtotheexample“photobook”applicationscenariodescribedabove,weplantoexplorethecombinationof
VRandIDLasamulti-modal,immersiveinteractionenvironment.Thisenvironmentsupportsrichdatainputsignals,
forexample,gazeandeyetracking,trackingofspatialmovementsandfeaturessuchaspointingusingacontroller
orfreehandgesturesandrecording3Dtrajectoriesovertimeaswellasaudioandspeechinput.Italsointegrates
multi-modaloutputsignalsintheformof3Dgraphics,spatialaudioandsimpleformsoftactilefeedback.Lastbut
notleast,itprovidesunlimitedvirtualspace.Aswesketchedintheapplicationscenario,wewanttoinvestigatehow
toleveragethepotentialofVRforIDLbuttheVRenvironmentalsoprovidesanidealtestbedforgeneratingand
comparingdataandmodelstobeusedintherealworldbecauseitismucheasiertocontrolanddeploy.Whileexisting
worksinthisareahaveinvestigatedspecificcomponentsandtasksoftheexampleusagescenario,e.g.,theselectionof
aestheticallypleasingphotos[Withöftetal.2022],takingaspecificlookatthehumanfactorswithrespecttotherich
inputandoutputmodalitieswithinvirtualrealityisanovelideaandhasnotbeenexploredinthecontextofIDLtothe
bestofourknowledge.
FromanHCIperspective,thegoalscanbesummarisedasexploringnewwaysforlearningsystemstointeractwith
theirusers,namely:(1)howuser-drivenlearningcyclescaninvolvemorerapid,focused,andincrementalmodelupdates;
(2)howtoreducetheneedforsupervisionbyMLpractitioners;(3)Asaresultoftheserapidinteractioncyclescommon
inIML,evenuserswithlittleornomachine-learningexpertiseshouldbeabletosteermachine-learningbehaviours
throughlow-costtrialanderrororfocusedexperimentationwithinputsandoutputs.Howcanthisbesupported
fromtheHCIperspective?(4)Transparencycanhelpprovidebetterlabels(contextualfeatures,MLpredictions,etc.)
towardsexplainableIML.TheexperimentalsetupshouldincludeexplainableIML,wheretheuserfeedbackisderived
afterthesystemexplainsitsresults,toavoid“rightanswersforthewrongreasons”,see,e.g.,[Andersetal.2022].
(5)Understandinghowpeopleactuallyinteract—andwanttointeract—withmachine-learningsystemsiscriticalto
designingsystemsthatpeoplecanuseeffectively[Simardetal.2017].
Morespecifically,weplantostudybasicpropertieslikementalandphysicalload,attentionsplitproblems,confusion,
andemotionalaffect.Theseprovidethefoundationtoinvestigatemorecomplexeffectsregardinguserintentionand
strategies,trust,andconfidenceinusingthesystem.Furthermore,weexpectalargeimpactofexplainabilitytechniques
6https://medicalcps.dfki.de/wp-content/uploads/2017/08/KDI_V2_Pro_v04_2.mp4AlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE) 15
onthesefactors.Weplantoexperimentwithdifferentgraphicalandtextualorspokenexplanations.Bystudyingthese
factorsfromtheuser’sperspectiveweintendtooptimisetheeffectivenessofactivelearningtechniques.
WeplantoruncomparativestudieswithinVR,forexample,exploringdifferentinteractiondesigns,information
presentationandDLtechniques.Theideaistomeasurehumanfactorsaslistedabove,e.g.,cognitiveload,butalso
otherfactorsoftheuserexperience,suchasemotionalaffectandmotivationalmeasuressuchasuserengagementand
studytheirimpactonactivelearningefficiencyandeffectiveness.
Consideringthepotentialeffectofusermotivation,experimentingwithformsofgamification[Deterdingetal.
2011a,b]andseriousgameswithintheframeworkoftheexamplescenarioseemsrelevant.Oneapproachinthatregard
willbetoturntherespectivetask,e.g.,findingphotoswithcertaincontents,describingapicture,sortingorclustering
pictures,insertingamissingorbestfittingpictureintovisualphotobookstory,intochallengesbyintroducinga
timelimit(softorhard),rewards(short,mid,longterm)andpotentiallyformsofsocialrelatedness(synchronousor
asynchronousformsofmulti-player).Gamificationcouldalsobeusedtoprovideameasurementofthequalityofthe
DLmodelbyusingittoacquireuserratingsoftheoveralloutput.
Asasidenote,tofacilitateuserparticipationinourexperiments,weplantosetupanopenlabspaceinthecentreof
thecityofOldenburg(intheCOREOldenburg)toincreaseparticipationandrecruitvolunteerswithdiversedemographic
backgrounds.
Themaindeliverablesinthisareaare:
(1) Implementationofdifferentinteractionmodalitieswithinvirtualreality,e.g.,freehandgesturesvs.controller
basedselectionormanipulationvs.NLPandpossiblecombinations.
(2) Studiesabouttheinfluenceofconsciousandunconsciousgestures,e.g.,certainmovementsorposturethatrelate
toconfusionordecisioninsecurity;gazeoreyetracking(herethereisaverystronglinktomultimodality).
(3) ImplementationofdifferentfeedbackformsandmodalitiestoencodeinformationabouttheDLresultsand
decisionprocess,from“simple”visualfeatures(colour,location,etc.)toaudioortactilechannels.
(4) Conceptsandstudiesoftheeffectofmoreplayfulapproaches(seriousgamesandgamification)withrespectto
usermotivationanduserfeedbackqualityandquantityforIDL.
3.5 EvaluationPlan
Inthissubsectionweprovidedetailsaboutourgeneralevaluationprocessandstudyplan.Ofcourse,duetothenovelty
oftheresearch,theplanwillhavetobeadjustedthroughouttheprojectasitdependsontheprogressandresultsofthe
technicalpartsandworkpackages.Wewanttoemphasisethattheguidingoverallfocusofallevaluationactivityis
toinvestigateandimprovetheIDLprocessasdiscussedinthespecificsubsections,e.g.,howcantheobserveduser
behaviouranduserexperiencebeutilisedasameansforimprovingefficiencyandeffectivenessofIDL.Thisalsois
reflectedinthewaythatVRisusedwithinthisproject,i.e.,aspowerfultoolforstudyinguserbehaviourandcollecting
datausingphotobookcreationasanexampleapplicationasopposedtoinvestigatingtheuseofVRforphotobook
creation,whichisexplicitlynotafocuspointofthisproject.
Firstly,weplantoconductanumberofsmallerstudiesthatlookatveryspecificaspectsandthatlaythefoundation
foralargerstudytowardstheendoftheproject.Atthebeginning,wewillfocusonfundamentalsandisolatedelements
andshifttoinvestigatingmorecomplexcombinationsofsystemfeaturesandtasksovertime.Thiswillalsobereflected
inthemethodsweapply.Atthebeginningwewillemploymethodsofamoreexploratoryandformativetype,for16 DanielSonntag,MichaelBarz,andThiagoGouvêa
instance,casestudiesusingmethodssuchasinterviews,cognitivewalk-troughs,think-aloud,observationandformsof
moderateddiscussion.Ofcourse,thisdoesnotexcludealsocollectingquantitativedataalreadyinthisphaseifpossible.
Themainstudyapproachofamoresummativecharacterwillbeusinganexperimentalsetupcomparingtwo
conditions (control + intervention) or (if applicable) a factorial design with up to three or four conditions using
appropriatetoolsandcollectingquantitativemeasureslikecompletiontimes,labellingaccuracyinadditionto(preferably
validated)questionnairesforsubjectivefeedbackespeciallyformeasuringuserexperienceandusability,e.g.,SUS[Brooke
1986],PANAS-X[WatsonandClark1994]andotherSDT-based[RyanandDeci2000]toolsrelatedtomotivationand
alsophysicalandmentalload(e.g.,NASA-TLX[Hart2006;HartandStaveland1988]).
Thefinaldecisionfortheexperimentaldesignwithrespecttoindependentordependentgroups(within-subjects
vs.betweensubjectsdesign)hingesonfactorsliketheexpectedlearningeffectvs.fatigueeffectsandissubjecttothe
specificexperimentaldesignforeachstudybasedontestingandpre-studiestoquantifytheseconfoundingeffects.
Inaddition,theVRsetupinparticularbutalsotheeye-trackingscenarioprovideuniqueopportunitiestocollect
objectivedata,mostimportantly,eye-trackingandmovementdata,e.g.,trajectoriesofthecontrollers.Wewillalsolook
intoadditionalpsycho-physiologicalmeasures,suchasheartratethatarerelativelyeasytomeasurewithoff-the-shelf
wearables.
WewillbasethenumberofparticipantsoncomparablestudiesandstandardsinHCI,typicallyintherangeof20-80
participantsperexperiment.Thegeneralexperimentalprocedureincludesthefollowingsteps:
(1) Introductionandwelcomeofparticipantsandcollectingtheirinformedconsent.
(2) Atrainingoraccommodationphase,whichisespeciallyimportantintheVRcase.
(3) Acalibrationphaseorprocedure,whichcanalsoincludecollectingbaselevelsofcertainmeasures.
(4) Themainpart,i.e.,participantsperformspecifiedtasksunderdifferentconditions,e.g.,differentformsofvisual
feedback,inputgesturesoractivelearningprompts.Somedataarecollectedcontinuouslythroughlogging
otherdata(e.g.,subjectivefeedback)arecollectedaftereachcondition(inaccordancetotherespectivemeasure
orquestionnaire).
(5) Collectionofpost-experimentalandindependentdata(e.g.,demographics).
(6) De-briefingand“Goodbye”.
Throughouttheprocedureparticipantswillbeabletotakebreaksasneeded(especiallyintheVRscenario)andwe
willadheretoscientificstandardsincludinggettingapprovaloftheDFKIethicscommittee.Thestatisticalanalysisof
individualmeasureswillbecarriedoutusinglinearmodelssuchasANOVAforcomparingmeansornon-parametric
testslikeFriedman[Cairns2019].Inaddition,formsoftimeseriesanalysisandclusteringwillbelookedintofor
analysingandcorrelatingspatialmeasuressuchasbody,hand,orcontrollermovements.Wewillalsoconsiderpost-hoc
experimentsbasedonrecordeduserinputstotestadditionalIMLapproaches.Thiscanbedonebysimulatingthe
interactionsignalsofourstudyparticipantsifthemodeloutputshavenoimmediateimpactontheinteractionflow.
4 EXISTINGHARDWAREANDSOFTWAREFRAMEWORKSATDFKIIML
Byharnessingthepoweroffoundationmodels[Alietal.2019],i.e.,anyMLmodelwhichistrainedonalarge-scale
datasetandcanbeadaptedtoawiderangeofdownstreamtasks,theresearchcommunityisoptimisticabouttheir
socialapplicability[Bommasanietal.2021],especiallyinthehealthcaredisciplinewithintegratedhumaninteraction.
Especially,patientcareviadiseasetreatmentusuallyrequiresexpertknowledgethatislimitedandexpensive.Foundation
modelstrainedontheabundanceofdataacrossmanymodalities(e.g.,images,text,molecules)presentclearopportunitiesAlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE) 17
totransferknowledgelearnedfromrelateddomainstoaspecificdomainandfurtherimproveefficiencyintheadaptation
stepbyreducingthecostofexperttime.Asaresult,afastprototypeapplicationcanbeemployedwithoutcollecting
significantamountsofdataandtraininglargemodelsfromscratch.Intheoppositedirection,end-userswhowilldirectly
useorbeinfluencedbytheseapplicationscanprovidefeedbacktopowerthesefoundationmodelstowardcreating
tailoredmodelsforthedesiredgoalofIDL,basedonDFKIIML’sexistingsoftwareframeworks:[Nguyenetal.2020;
NunnariandSonntag2021;Sonntagetal.2020;Zachariasetal.2018].
TheplannedmultimodalmultisensorinterfacesinNo-IDLEwillbebasedonthemultisensor-pipeline(MSP)7,our
lightweight,flexible,andextensibleframeworkforprototypingMMIbasedonreal-timesensorinput[Barzetal.2021a].
TheMSPecosystemwillbenefitfromthedevelopmentsinNo-IDLE,becausenovelmoduleswillbereleasedasopen
sourcetotheresearchcommunity.No-IDLEwilltakeadvantagefromrecentandupcomingdevelopmentsintheBMBF
ProjectGeAR8(endsinSeptember2022):wearedevelopingmethodsthatreducethehumaneffortintheprocessof
annotatingmobileeyetrackingdataasdescribedin[BarzandSonntag2021].InGeAR,wetargetsemi-automatic
annotationforanalyticalapplications(post-hoc)ratherthanreal-timeinteractivemodeltraining,whichisintegrated
intotheapplicationitself.
5 EXISTINGAPPLICATIONDOMAINSANDDEMOSCENARIOSATDFKIIML
WebuildtheMMIandHCIcomponentsofthisprojectuponfourpastapplicationdomainsanddemoscenarios,which
wedetailintherespectivefigurecaptions:
• InteractiveDoctorFeedback(usecasefromBMBFOphthalmo-AI9)project(seefigure5)
• InteractiveImageClassificationinVR(seefigure6)
• ExplanatoryIML(usecasefromXAINESproject,seefigure7):InXAINES,wedevelopmodelsthatprovide
explanationsforpredictionsinanexplanation-feedbackloop,whichcanservetoimprovethemodelbasedon
humanfeedback,andtopersonalizeexplanations.Thesemodelswillserveasastartingpointfordeveloping
interactiveDLmodelsfortheNo-IDLEphotobookusecase.
• ThemultimodalinteractionsystemsinNo-IDLEwillbebuildbasedonourexperienceandoutcomesfrom
recentresearchprojects(SciBot,GeAR).Thisincludesmethodsforreal-timeinterpretationofmultimodal
sensorstreamssuchasmobileeyetrackingdata[Barzetal.2022,2021b;BarzandSonntag2021;Barzetal.
2020b;Bhattietal.2021;Kappetal.2021](foranexample,seefigure8),butalsopen-basedinputsignals[Barz
etal.2020a].Inaddition,wewilluseandfurtherdevelopourframeworkforbuildingmultimodal,real-time
interactiveinterfaces,themultisensor-pipeline[Barzetal.2021a].
6 CONCLUSION
WepresentedtheanatomyoftheNo-IDLEprototypesystem(fundedbytheGermanFederalMinistryofEducation
andResearch)anddescribedbasicandfundamentalresearchininteractivemachinelearningwhileaddressingusers’
behaviours,needs,andgoals.Wedecribedgoalsandscienfificchallengesthatcentrearoundthedesiretoincrease
thereachofinteractivedeeplearningsolutionsfornon-expertsinmachinelearning,followedbyamethodologyfor
interactivemachinelearningcombinedwithmultimodalinteractionwhichwillbecomecentralwhenwestartinteracting
withsemi-intelligentmachinesintheupcomingareaofneuralnetworksandlargelanguagemodels.Futurework
7https://github.com/DFKI-Interactive-Machine-Learning/multisensor-pipeline
8https://www.dfki.de/en/web/research/projects-and-publications/projects-overview/project/gear
9https://www.dfki.de/en/web/research/projects-and-publications/projects-overview/project/ophthalmo-ai18 DanielSonntag,MichaelBarz,andThiagoGouvêa
Fig.5. HighleveloverviewofourproposedmethodintheIDLworkflowoftheOphthalmo-AIproject(BMBF).Givenaretinalimage,
ourDLmodelswillgenerate3typesofpredictions(DRgrade,lesionregion,visualexplanation)simultaneously.Ophthalmologists
canobservethepredictionsandprovidefeedbackformodelfine-tuning.
AlexNet fine-tuning
interactive re-positioning
image classification
ℝ4096 ℝ1000 virtual reality
fc7 final layer ℝ4096 ℝ50 ℝ3
PCA t-SNE
Fig.6. Architectureofourapproachin[PrangeandSonntag2021]basedonPCAandt-SNEdimensionalityreduction.Basedona
pre-trainedAlexNetwecalculate3Dcoordinatesforeachimage.InVR,informationrelatedtoaparticularimageisdisplayedifthe
userlooksatit.
includes"No-IDLEmeetsChatGPT".Theoverallobjectiveofthisfollow-upprojectwillbetoleveragetheopportunities
arisingfromlargelanguagemodelsandtechnologiesfortheNo-IDLEproject.No-IDLEaimstoenhancetheinteraction
betweenhumansandmachinesforthepurposeofupdatingdeeplearningmodels,integratingcutting-edgehuman-
computerinteractiontechniquesandadvanceddeeplearningapproaches.ConsideringtherecentadvancesinLLMs
andtheirmultimodalcapabilities,theoverallobjectiveof"No-IDLEmeetsChatGPT"shouldbewellmotivated.
ACKNOWLEDGMENTS
ThisworkisfundedbytheGermanFederalMinistryofEducationandResearchundergrantnumber01IW23002.AlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE) 19
Natural Language Inference (NLI) Commonsense Reasoning Visual Question Answering (VQA)
P:A 2-3 year old blond child is kneeling Question: What would not be true
on a couch. about a basketball if it had a hole in it Image: H →:T Che o c nh til rd a h da is c b tr io ow nn hair. b A →u :t pi At u d nni c sd t w un ro eet d r l , o B s Be : i t fs u g ll e on fe ar ia rl , s Ch :a p roe u? nd Question: What is the person doing?
→ Skiing
The child would not have brown Air cannot stay in any object … because they are on skis and in
hair if he/she was blond. that has a hole in it. a skiing outfit.
Fig.7. Examplesofexistingdatasetswithhumanexplanationsfornaturallanguageinference[Camburuetal.2018],commonsense
reasoning[Rajanietal.2019],andvisualquestionanswering[Parketal.2018a].Explanationsareeitherfree-form(bottomline)or
subsetsoftheinputdata(highlightsinblue).Thesedatasetscanbeusedforbothlearningtogeneratenaturallanguageexplanations
aswellassimulatingexplanatoryfeedbackfedtothemodelinthesenseofexplanatoryIML,see[TesoandKersting2019],wherein
eachhuman-in-the-loopstep,thelearnerexplainsitspredictiontotheuser,andtheusercanprovideexplanatoryfeedbackbackto
themodelinordertoimproveit.WhereasexplanatoryIMLmainlyfocusesoncorrectingrightforthewrongreasonbehaviour,wewe
willalsoexplorehowtouseexplanatoryfeedbacktoadaptmodelstouser-specificinputdata.
Fig.8. OurprototypebasedonMicrosoft’sHoloLens2classifiesandaugmentsfixatedobjectsinreal-time[Barzetal.2021b].It
displaysclassificationlabelsandthedurationofrecentattentioneventstotheuserasahologram.Thedemovideocanbeviewedhere:
https://www.youtube.com/watch?v=bdNClVz9ylE.InNo-IDLE,weplantoenableinteractivemodeladaptationbasedonfoundation
models:Forinstance,theusercouldcreateaspecificinstanceof"reflexcamera"andnameit"Nikoncamera"viaspeech(asshownin
theimage).ThisisrelatedtheCOPDAprojectwhichaimstoestablishandmaintainobjectrelationslikeownership.Otherexamples
includethatusersmaycorrectwrongclassificationsorteachnewclassestotheMLsysteminamixed-initiativedialogue.
Visual explanation:
REFERENCES
AishwaryaAgrawal,DhruvBatra,DeviParikh,andAniruddhaKembhavi.2018. Don’tjustassume;lookandanswer:Overcomingpriorsforvisual
questionanswering.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.4971–4980.
ShahbazAli,HailongSun,andYongwangZhao.2019.ModelLearning:ASurveyonFoundation,ToolsandApplications.InarXiv:1901.01910.
MaliheAlikhani,FangdaHan,HareeshRavi,MubbasirKapadia,VladimirPavlovic,andMatthewStone.2022.Cross-ModalCoherenceforText-to-Image
Retrieval.ProceedingsoftheAAAIConferenceonArtificialIntelligence36,10(Jun.2022),10427–10435. https://doi.org/10.1609/aaai.v36i10.21285
SaleemaAmershi,MayaCakmak,WilliamBradleyKnox,andToddKulesza.2014. Powertothepeople:Theroleofhumansininteractivemachine
learning.AiMagazine35,4(2014),105–120.
ksaT
ataD
.lpxE20 DanielSonntag,MichaelBarz,andThiagoGouvêa
Generic: Twoboysareplayingfrisbeeonthebeach.
Personalised: PeterandTomareplayingfrisbeeatthePylacampsite.
Stylised: AheatedgameoffrisbeeonthePylacourt.
Controllable: Davidhasenoughandreturnstothecabin.
Table1. ExampleimagecaptionsforanimagetakenfromMSCOCO,showingthedifferencebetweengenericimagecaptionsand
entity-aware,stylised,andcontrollableimagecaptionsrequireforphotobookcreationsupport.
SaleemaAmershi,JamesFogarty,AshishKapoor,andDesneyTan.2011.Effectiveend-userinteractionwithmachinelearning.InProceedingsoftheAAAI
ConferenceonArtificialIntelligence,Vol.25.
ChristopherJ.Anders,LeanderWeber,DavidNeumann,WojciechSamek,Klaus-RobertMüller,andSebastianLapuschkin.2022.Findingandremoving
CleverHans:Usingexplanationmethodstodebugandimprovedeepmodels.Inf.Fusion77(2022),261–295. https://doi.org/10.1016/j.inffus.2021.07.015
JordanT.Ash,SurbhiGoel,AkshayKrishnamurthy,andShamKakade.2021.GoneFishing:NeuralActiveLearningwithFisherEmbeddings.InNeurIPS.
JordanT.Ash,ChichengZhang,AkshayKrishnamurthy,JohnLangford,andAlekhAgarwal.2020.DeepBatchActiveLearningbyDiverse,Uncertain
GradientLowerBounds.InICLR.
AbhijeetAwasthi,SabyasachiGhosh,RasnaGoyal,andSunitaSarawagi.2020.LearningfromRulesGeneralizingLabeledExemplars.InInternational
ConferenceonLearningRepresentations. https://openreview.net/forum?id=SkeuexBtDr
AlejandroBarredoArrieta,NataliaDíaz-Rodríguez,JavierDelSer,AdrienBennetot,SihamTabik,AlbertoBarbado,SalvadorGarcia,SergioGil-Lopez,
DanielMolina,RichardBenjamins,RajaChatila,andFranciscoHerrera.2020. ExplainableArtificialIntelligence(XAI):Concepts,taxonomies,
opportunitiesandchallengestowardresponsibleAI.InformationFusion58(2020),82–115. https://doi.org/10.1016/j.inffus.2019.12.012
MichaelBarz,KristinAltmeyer,SarahMalone,LuisaLauer,andDanielSonntag.2020a.DigitalPenFeaturesPredictTaskDifficultyandUserPerformance
ofCognitiveTests.InProceedingsofthe28thACMConferenceonUserModeling,AdaptationandPersonalization,UMAP2020,Genoa,Italy,July12-18,
2020,TsviKuflik,IlariaTorre,RobinBurke,andCristinaGena(Eds.).ACM,23–32. https://doi.org/10.1145/3340631.3394839
MichaelBarz,OmairShahzadBhatti,BengtLüers,AlexanderPrange,andDanielSonntag.2021a.Multisensor-Pipeline:ALightweight,Flexible,and
ExtensibleFrameworkforBuildingMultimodal-MultisensorInterfaces.InCompanionPublicationofthe2021InternationalConferenceonMultimodal
Interaction(ICMI’21Companion).AssociationforComputingMachinery,NewYork,NY,USA,13–18. https://doi.org/10.1145/3461615.3485432
event-place:Montreal,QC,Canada.
MichaelBarz,OmairShahzadBhatti,andDanielSonntag.2022.ImplicitEstimationofParagraphRelevanceFromEyeMovements.FrontiersinComputer
Science3(2022),136. https://doi.org/10.3389/fcomp.2021.808507
MichaelBarz,SebastianKapp,JochenKuhn,andDanielSonntag.2021b.AutomaticRecognitionandAugmentationofAttendedObjectsinReal-time
usingEyeTrackingandaHead-mountedDisplay.InACMSymposiumonEyeTrackingResearchandApplications(ETRA’21Adjunct).Associationfor
ComputingMachinery,NewYork,NY,USA,1–4. https://doi.org/10.1145/3450341.3458766
MichaelBarzandDanielSonntag.2021.AutomaticVisualAttentionDetectionforMobileEyeTrackingUsingPre-TrainedComputerVisionModelsand
HumanGaze.Sensors21,12(Jan.2021),4143. https://doi.org/10.3390/s21124143Number:12Publisher:MultidisciplinaryDigitalPublishingInstitute.
MichaelBarz,SvenStauden,andDanielSonntag.2020b.VisualSearchTargetInferenceinNaturalInteractionSettingswithMachineLearning.InACM
SymposiumonEyeTrackingResearchandApplications(ETRA’20FullPapers),AndreasBulling,AnkeHuckauf,EaktaJain,RalphRadach,andDaniel
Weiskopf(Eds.).AssociationforComputingMachinery,1–8. https://doi.org/10.1145/3379155.3391314
JasmijnBastingsandKatjaFilippova.2020.Theelephantintheinterpretabilityroom:Whyuseattentionasexplanationwhenwehavesaliencymethods?.
InProceedingsoftheThirdBlackboxNLPWorkshoponAnalyzingandInterpretingNeuralNetworksforNLP.AssociationforComputationalLinguistics,
Online,149–155. https://doi.org/10.18653/v1/2020.blackboxnlp-1.14
WilliamH.Beluch,TimGenewein,AndreasNurnberger,andJanM.Kohler.2018.Thepowerofensemblesforactivelearninginimageclassification.In
CVPR.
OmairShahzadBhatti,MichaelBarz,andDanielSonntag.2021.EyeLogin-Calibration-freeAuthenticationMethodforPublicDisplaysUsingEyeGaze.
InACMSymposiumonEyeTrackingResearchandApplications(ETRA’21ShortPapers).AssociationforComputingMachinery,NewYork,NY,USA,
1–7. https://doi.org/10.1145/3448018.3458001
MagdalenaBiesialska,KatarzynaBiesialska,andMartaRCosta-jussà.2020.ContinualLifelongLearninginNaturalLanguageProcessing:ASurvey.In
Proceedingsofthe28thInternationalConferenceonComputationalLinguistics.6523–6541.
RajarshiBiswas,MichaelBarz,andDanielSonntag.2020.TowardsExplanatoryInteractiveImageCaptioningUsingTop-DownandBottom-UpFeatures,
BeamSearchandRe-ranking.KI-KünstlicheIntelligenz34,4(2020),571–584.AlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE) 21
AliFurkanBiten,LluisGomez,MarçalRusinol,andDimosthenisKaratzas.2019.Goodnews,everyone!contextdrivenentity-awarecaptioningfornews
images.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.12466–12475.
SusanneBoll,PhilippSandhaus,AnsgarScherp,andSabineThieme.2006.MetaXa—Context-andContent-DrivenMetadataEnhancementforPersonal
PhotoBooks.InAdvancesinMultimediaModeling(LectureNotesinComputerScience),Tat-JenCham,JianfeiCai,ChitraDorai,DeepuRajan,Tat-Seng
Chua,andLiang-TienChia(Eds.).Springer,Berlin,Heidelberg,332–343. https://doi.org/10.1007/978-3-540-69423-6_33
SusanneBoll,PhilippSandhaus,AnsgarScherp,andUtzWestermann.2007.Semantics,content,andstructureofmanyforthecreationofpersonalphoto
albums.InProceedingsofthe15thACMinternationalconferenceonMultimedia(MM’07).AssociationforComputingMachinery,NewYork,NY,USA,
641–650. https://doi.org/10.1145/1291233.1291385
RishiBommasani,DrewA.Hudson,EhsanAdeli,RussAltman,SimranArora,SydneyvonArx,MichaelS.Bernstein,JeannetteBohg,AntoineBosselut,
EmmaBrunskill,ErikBrynjolfsson,ShyamalBuch,DallasCard,RodrigoCastellon,NiladriS.Chatterji,AnnieS.Chen,KathleenCreel,JaredQuincy
Davis,DorottyaDemszky,ChrisDonahue,MoussaDoumbouya,EsinDurmus,StefanoErmon,JohnEtchemendy,KawinEthayarajh,LiFei-Fei,Chelsea
Finn,TrevorGale,LaurenGillespie,KaranGoel,NoahD.Goodman,ShelbyGrossman,NeelGuha,TatsunoriHashimoto,PeterHenderson,John
Hewitt,DanielE.Ho,JennyHong,KyleHsu,JingHuang,ThomasIcard,SaahilJain,DanJurafsky,PratyushaKalluri,SiddharthKaramcheti,Geoff
Keeling,FereshteKhani,OmarKhattab,PangWeiKoh,MarkS.Krass,RanjayKrishna,RohithKuditipudi,andetal.2021.OntheOpportunitiesand
RisksofFoundationModels.CoRRabs/2108.07258(2021).arXiv:2108.07258 https://arxiv.org/abs/2108.07258
JohnBrooke.1986.Systemusabilityscale(SUS):aquick-and-dirtymethodofsystemevaluationuserinformation.Reading,UK:Digitalequipmentcoltd
43(1986),1–7.
AndreasBulling,ChristianWeichel,andHansGellersen.2013.EyeContext:RecognitionofHigh-levelContextualCuesfromHumanVisualBehaviour.In
ProceedingsoftheSIGCHIConferenceonHumanFactorsinComputingSystems.AssociationforComputingMachinery,NewYork,NY,USA,305–308.
https://doi.org/10.1145/2470654.2470697
AndreasBullingandThorstenO.Zander.2014.Cognition-AwareComputing.IEEEPervasiveComputing13,3(July2014),80–83. https://doi.org/10.1109/
MPRV.2014.42
NadiaBurkartandMarcoF.Huber.2021. ASurveyontheExplainabilityofSupervisedMachineLearning. J.Artif.Intell.Res.70(2021),245–317.
https://doi.org/10.1613/jair.1.12228
PaulCairns.2019.Doingbetterstatisticsinhuman-computerinteraction.CambridgeUniversityPress.
Oana-MariaCamburu,TimRocktäschel,ThomasLukasiewicz,andPhilBlunsom.2018.e-SNLI:NaturalLanguageInferencewithNaturalLanguage
Explanations.AdvancesinNeuralInformationProcessingSystems31(2018),9539–9549.
RCaruana.1993. Multitasklearning:Aknowledge-basedsourceofinductivebias1.InProceedingsoftheTenthInternationalConferenceonMachine
Learning.Citeseer,41–48.
ShizheChen,QinJin,PengWang,andQiWu.2020.Sayasyouwish:Fine-grainedcontrolofimagecaptiongenerationwithabstractscenegraphs.In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.9962–9971.
CescChunseongPark,ByeongchangKim,andGunheeKim.2017.Attendtoyou:Personalizedimagecaptioningwithcontextsequencememorynetworks.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.895–903.
JohnDCo-Reyes,AbhishekGupta,SuvanshSanjeev,NickAltieri,JohnDeNero,PieterAbbeel,andSergeyLevine.2019.Meta-LearningLanguage-Guided
PolicyLearning.InInternationalConferenceonLearningRepresentations. https://openreview.net/forum?id=HkgSEnA5KQ
RonanCollobert,JasonWeston,LéonBottou,MichaelKarlen,KorayKavukcuoglu,andPavelKuksa.2011.Naturallanguageprocessing(almost)from
scratch.Journalofmachinelearningresearch12,ARTICLE(2011),2493–2537.
MarcellaCornia,LorenzoBaraldi,GiuseppeSerra,andRitaCucchiara.2018.Payingmoreattentiontosaliency:Imagecaptioningwithsaliencyand
contextattention.ACMTransactionsonMultimediaComputing,Communications,andApplications(TOMM)14,2(2018),1–21.
CypriendeMassond’Autume,SebastianRuder,LingpengKong,andDaniYogatama.2019.Episodicmemoryinlifelonglanguagelearning.InProceedings
ofthe33rdInternationalConferenceonNeuralInformationProcessingSystems.13132–13141.
MarianneDeAngelusandJeffB.Pelz.2009. Top-downcontrolofeyemovements:Yarbusrevisited. VisualCognition17,6-7(Aug.2009),790–811.
https://doi.org/10.1080/13506280902793843Publisher:Routledge.
RiccardoDelChiaro,BartłomiejTwardowski,AndrewBagdanov,andJoostVandeWeijer.2020.Ratt:Recurrentattentiontotransienttasksforcontinual
imagecaptioning.AdvancesinNeuralInformationProcessingSystems33(2020),16736–16748.
SebastianDeterding,DanDixon,RillaKhaled,andLennartNacke.2011a. Fromgamedesignelementstogamefulness:defining"gamification".In
Proceedingsofthe15thinternationalacademicMindTrekconference:Envisioningfuturemediaenvironments.9–15.
SebastianDeterding,MiguelSicart,LennartNacke,KentonO’Hara,andDanDixon.2011b.Gamification.usinggame-designelementsinnon-gaming
contexts.InCHI’11extendedabstractsonhumanfactorsincomputingsystems.2425–2428.
JayDeYoung,SarthakJain,NazneenFatemaRajani,EricLehman,CaimingXiong,RichardSocher,andByronCWallace.2020.ERASER:ABenchmarkto
EvaluateRationalizedNLPModels.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics.4443–4458.
C.Donalek,S.G.Djorgovski,A.Cioc,A.Wang,J.Zhang,E.Lawler,S.Yeh,A.Mahabal,M.Graham,A.Drake,S.Davidoff,J.S.Norris,andG.Longo.2014.
Immersiveandcollaborativedatavisualizationusingvirtualrealityplatforms.In2014IEEEInternationalConferenceonBigData(BigData).609–614.
FinaleDoshi-VelezandBeenKim.2017.Towardsarigorousscienceofinterpretablemachinelearning.arXivpreprintarXiv:1702.08608(2017).
JohnJ.DudleyandPerOlaKristensson.2018.AReviewofUserInterfaceDesignforInteractiveMachineLearning.ACMTransactionsonInteractive
IntelligentSystems8,2(June2018). https://doi.org/10.1145/3185517Place:NewYork,NY,USAPublisher:AssociationforComputingMachinery.22 DanielSonntag,MichaelBarz,andThiagoGouvêa
QiFan,WeiZhuo,andYu-WingTai.2019.Few-ShotObjectDetectionwithAttention-RPNandMulti-RelationDetector.CoRRabs/1908.01998(2019).
http://arxiv.org/abs/1908.01998arXiv:1908.01998.
J.RandallFlanaganandRolandS.Johansson.2003.Actionplansusedinactionobservation.Nature424,6950(Aug.2003),769–771. https://doi.org/10.
1038/nature01861
YoavFreund,HSebastianSeung,EliShamir,,andNaftaliTishby.1997.Selectivesamplingusingthequerybycommitteealgorithm.MachineLearning28
(1997),2–3.
YarinGal,RiashatIslam,andZoubinGhahramani.2017.Deepbayesianactivelearningwithimagedata.InICML.
ChuangGan,ZheGan,XiaodongHe,JianfengGao,andLiDeng.2017.Stylenet:Generatingattractivevisualcaptionswithstyles.InProceedingsofthe
IEEEConferenceonComputerVisionandPatternRecognition.3137–3146.
BhavyaGhai,QVeraLiao,YunfengZhang,RachelBellamy,andKlausMueller.2021.Explainableactivelearning(xal)towardaiexplanationsasinterfaces
formachineteachers.ProceedingsoftheACMonHuman-ComputerInteraction4,CSCW3(2021),1–28.
DanielGissinandShaiShalev-Shwartz.2019.DiscriminativeActiveLearning.CoRRabs/1907.06347(2019).arXiv:1907.06347http://arxiv.org/abs/1907.06347
RandyGoebel,AjayChander,KatharinaHolzinger,FreddyLecue,ZeynepAkata,SimoneStumpf,PeterKieseberg,andAndreasHolzinger.2018.Explainable
AI:TheNew42?.InMachineLearningandKnowledgeExtraction,AndreasHolzinger,PeterKieseberg,AMinTjoa,andEdgarWeippl(Eds.).Springer
InternationalPublishing,Cham,295–303.
ThiagoS.Gouvêa,HannesKath,IliraTroshani,BengtLüers,PatriciaP.Serafini,IvanB.Campos,AndréS.Afonso,SergioM.F.M.Leandro,Lourens
Swanepoel,NicholasTheron,AnthonyM.Swemmer,andDanielSonntag.2023.InteractiveMachineLearningSolutionsforAcousticMonitoringof
AnimalWildlifeinBiosphereReserves.InProceedingsoftheThirty-SecondInternationalJointConferenceonArtificialIntelligence,IJCAI2023,19th-25th
August2023,Macao,SAR,China.ijcai.org,6405–6413. https://doi.org/10.24963/IJCAI.2023/711
YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh.2017.Makingthevinvqamatter:Elevatingtheroleofimageunderstanding
invisualquestionanswering.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.6904–6913.
ClaudioGreco,BarbaraPlank,RaquelFernández,andRaffaellaBernardi.2019.PsycholinguisticsMeetsContinualLearning:MeasuringCatastrophic
ForgettinginVisualQuestionAnswering.InProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics.3601–3605.
GustafGredebäckandTerjeFalck-Ytter.2015.EyeMovementsDuringActionObservation.PerspectivesonPsychologicalScience10,5(2015),591–598.
https://doi.org/10.1177/1745691615589103_eprint:https://doi.org/10.1177/1745691615589103.
ZenziM.GriffinandKathrynBock.2000.WhattheEyesSayAboutSpeaking.PsychologicalScience11,4(2000),274–279. https://doi.org/10.1111/1467-
9280.00255_eprint:https://doi.org/10.1111/1467-9280.00255.
DavidGunning.2017.Explainableartificialintelligence(xai).Defenseadvancedresearchprojectsagency(DARPA),ndWeb2,2(2017),1.
LongtengGuo,JingLiu,PengYao,JiangweiLi,andHanqingLu.2019.Mscap:Multi-styleimagecaptioningwithunpairedstylizedtext.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.4204–4213.
BradenHancock,ParomaVarma,StephanieWang,MartinBringmann,PercyLiang,andChristopherRé.2018.TrainingClassifierswithNaturalLanguage
Explanations.InProceedingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers).1884–1895.
SandraG.Hart.2006.NASA-taskloadindex(NASA-TLX);20yearslater.InProceedingsofthehumanfactorsandergonomicssocietyannualmeeting,
Vol.50.SagepublicationsSageCA:LosAngeles,CA,904–908. Issue:9.
SandraG.HartandLowellE.Staveland.1988.DevelopmentofNASA-TLX(TaskLoadIndex):Resultsofempiricalandtheoreticalresearch.InAdvances
inpsychology.Vol.52.Elsevier,139–183.
MareikeHartmann,IvanaKruijff-Korbayová,andDanielSonntag.2021.InteractionwithExplanationsintheXAINESProject.
PeterHaseandMohitBansal.2021.Whencanmodelslearnfromexplanations?aformalframeworkforunderstandingtherolesofexplanationdata.
arXivpreprintarXiv:2102.02201(2021).
PeterHase,ShiyueZhang,HarryXie,andMohitBansal.2020.Leakage-AdjustedSimulatability:CanModelsGenerateNon-TrivialExplanationsofTheir
BehaviorinNaturalLanguage?.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:Findings.4351–4367.
NielsHenzeandSusanneBoll.2011.Who’sThatGirl?HandheldAugmentedRealityforPrintedPhotoBooks.InHuman-ComputerInteraction–INTERACT
2011(LectureNotesinComputerScience),PedroCampos,NicholasGraham,JoaquimJorge,NunoNunes,PhilippePalanque,andMarcoWinckler(Eds.).
Springer,Berlin,Heidelberg,134–151. https://doi.org/10.1007/978-3-642-23765-2_10
MarcHerrlich,ParnianTavakol,DavidBlack,DirkWenig,ChristianRieder,RainerMalaka,andRonKikinis.2017.Instrument-mounteddisplaysfor
reducingcognitiveloadduringsurgicalnavigation.InternationalJournalofComputerAssistedRadiologyandSurgery12,9(Sept.2017),1599–1605.
https://doi.org/10.1007/s11548-017-1540-6
FredHohman,MinsukKahng,RobertPienta,andDuenHorngChau.2018.VisualAnalyticsinDeepLearning:AnInterrogativeSurveyfortheNext
Frontiers.IEEETransactionsonVisualizationandComputerGraphics(2018).
AndreasHolzinger.2016.Interactivemachinelearningforhealthinformatics:whendoweneedthehuman-in-the-loop?BrainInformatics3,2(2016),
119–131.
Chien-MingHuangandBilgeMutlu.2016.AnticipatoryRobotControlforEfficientHuman-RobotCollaboration.InTheEleventhACM/IEEEInternational
ConferenceonHumanRobotInteraction(HRI’16).IEEEPress,83–90. event-place:Christchurch,NewZealand.
MichaelXuelinHuang,JiajiaLi,GraceNgai,HongVaLeong,andAndreasBulling.2019.Moment-to-MomentDetectionofInternalThoughtfromEye
VergenceBehaviour.(Jan.2019). http://arxiv.org/abs/1901.06572arXiv:1901.06572.
Sheng-JunHuang,RongJin,andZhi-HuaZhou.2010.ActiveLearningbyQueryingInformativeandRepresentativeExamples.InNIPS.AlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE) 23
Ting-HaoHuang,FrancisFerraro,NasrinMostafazadeh,IshanMisra,AishwaryaAgrawal,JacobDevlin,RossGirshick,XiaodongHe,PushmeetKohli,
DhruvBatra,etal.2016.Visualstorytelling.InProceedingsofthe2016ConferenceoftheNorthAmericanChapteroftheAssociationforComputational
Linguistics:HumanLanguageTechnologies.1233–1239.
AlonJacoviandYoavGoldberg.2020.TowardsFaithfullyInterpretableNLPSystems:HowShouldWeDefineandEvaluateFaithfulness?.InProceedingsof
the58thAnnualMeetingoftheAssociationforComputationalLinguistics.4198–4205.
ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocLe,Yun-HsuanSung,ZhenLi,andTomDuerig.2021.ScalingUpVisual
andVision-LanguageRepresentationLearningWithNoisyTextSupervision.InInternationalConferenceonMachineLearning.PMLR,4904–4916.
AjayJJoshi,FatihPorikli,andNikolaosPapanikolopoulos.2009.Multi-classactivelearningforimageclassification.InCVPR.
YunjaeJung,DahunKim,SanghyunWoo,KyungsuKim,SungjinKim,andInSoKweon.2020.Hide-and-Tell:LearningtoBridgePhotoStreamsforVisual
Storytelling.ProceedingsoftheAAAIConferenceonArtificialIntelligence34,07(Apr.2020),11213–11220. https://ojs.aaai.org/index.php/AAAI/article/
view/6780
AshishKapoor,KristenGrauman,RaquelUrtasun,,andTrevorDarrell.2007.Activelearningwithgaussianprocessesforobjectcategorization..InICCV.
SebastianKapp,MichaelBarz,SergeyMukhametov,DanielSonntag,andJochenKuhn.2021.ARETT:AugmentedRealityEyeTrackingToolkitforHead
MountedDisplays.Sensors21,6(March2021),2234. https://doi.org/10.3390/s21062234Number:6Publisher:MultidisciplinaryDigitalPublishing
Institute.
MaximeKayser,Oana-MariaCamburu,LeonardSalewski,CorneliusEmde,VirginieDo,ZeynepAkata,andThomasLukasiewicz.2021.e-vil:Adataset
andbenchmarkfornaturallanguageexplanationsinvision-languagetasks.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.
1244–1254.
Mi-YoungKim,ShahinAtakishiyev,HousamKhalifaBashierBabiker,NawshadFarruque,RandyGoebel,OsmarRZaïane,Mohammad-HosseinMotallebi,
JulianoRabelo,TalatSyed,HengshuaiYao,etal.2021.Amulti-componentframeworkfortheanalysisanddesignofexplainableartificialintelligence.
MachineLearningandKnowledgeExtraction3,4(2021),900–921.
JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,JoelVeness,GuillaumeDesjardins,AndreiARusu,KieranMilan,JohnQuan,TiagoRamalho,
AgnieszkaGrabska-Barwinska,etal.2017.Overcomingcatastrophicforgettinginneuralnetworks.Proceedingsofthenationalacademyofsciences114,
13(2017),3521–3526.
AndreasKirsch,JoostvanAmersfoort,andYarinGal.2019.BatchBALD:EfficientandDiverseBatchAcquisitionforDeepBayesianActiveLearning.In
NIPS.
JohannesKlonigandMarcHerrlich.2020.Integrating3Dand2DViewsofMedicalImageDatainVirtualRealityforEfficientNavigation.In2020IEEE
InternationalConferenceonHealthcareInformatics(ICHI).1–7. https://doi.org/10.1109/ICHI48887.2020.9374344ISSN:2575-2634.
KseniaKonyushkova,SznitmanRaphael,andPascalFua.2017.LearningActiveLearningfromData.InNeurIPS.
KseniaKonyushkova,RaphaelSznitman,andPascalFua.2019.GeometryinActiveLearningforBinaryandMulti-classImageSegmentation.Computer
VisionandImageUnderstanding(2019),1077–3142.
ToddKulesza,MargaretBurnett,Weng-KeenWong,andSimoneStumpf.2015.Principlesofexplanatorydebuggingtopersonalizeinteractivemachine
learning.InProceedingsofthe20thinternationalconferenceonintelligentuserinterfaces.126–137.
ChristophKäding,ErikRodner,AlexanderFreytag,andJoachimDenzler.2017.Fine-TuningDeepNeuralNetworksinContinuousLearningScenarios.In
ComputerVision–ACCV2016Workshops,Chu-SongChen,JiwenLu,andKai-KuangMa(Eds.).SpringerInternationalPublishing,Cham,588–605.
SébastienLallé,DereckToker,andCristinaConati.2021. Gaze-DrivenAdaptiveInterventionsforMagazine-StyleNarrativeVisualizations. IEEE
TransactionsonVisualizationandComputerGraphics27,6(2021),2941–2952. https://doi.org/10.1109/TVCG.2019.2958540
MichaelLand,NeilMennie,andJenniferRusted.1999.TheRolesofVisionandEyeMovementsintheControlofActivitiesofDailyLiving.Perception28,
11(1999),1311–1328. https://doi.org/10.1068/p2935_eprint:https://doi.org/10.1068/p2935.
PiyawatLertvittayakumjornandFrancescaToni.2021.Explanation-basedhumandebuggingofnlpmodels:Asurvey.TransactionsoftheAssociationfor
ComputationalLinguistics9(2021),1508–1528.
BoLi,WeiWu,QiangWang,FangyiZhang,JunliangXing,andJunjieYan.2019.SiamRPN++:EvolutionofSiameseVisualTrackingWithVeryDeep
Networks.In2019IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).4277–4286. https://doi.org/10.1109/CVPR.2019.00441
YuanpengLi,LiangZhao,KennethChurch,andMohamedElhoseiny.2020.CompositionalLanguageContinualLearning.InInternationalConferenceon
LearningRepresentations. https://openreview.net/forum?id=rklnDgHtDS
Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,andCLawrenceZitnick.2014.Microsoftcoco:
Commonobjectsincontext.InEuropeanconferenceoncomputervision.Springer,740–755.
DiLu,SpencerWhitehead,LifuHuang,HengJi,andShih-FuChang.2018. Entity-awareImageCaptionGeneration.InProceedingsofthe2018
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.AssociationforComputationalLinguistics,Brussels,Belgium,4013–4023. https:
//doi.org/10.18653/v1/D18-1435
MatthiasMaszuhn,LarbiAbdenebaoui,andSusanneBoll.2021.AUser-CenteredApproachforRecognizingConvenienceImagesinPersonalPhoto
Collections.In2021InternationalConferenceonContent-BasedMultimediaIndexing(CBMI).1–4. https://doi.org/10.1109/CBMI50038.2021.9461908
ISSN:1949-3991.
AlexanderMathews,LexingXie,andXumingHe.2016.SentiCap:GeneratingImageDescriptionswithSentiments.ProceedingsoftheAAAIConferenceon
ArtificialIntelligence30,1(Mar.2016). https://doi.org/10.1609/aaai.v30i1.1047524 DanielSonntag,MichaelBarz,andThiagoGouvêa
CynthiaMatuszek.2018.GroundedLanguageLearning:WhereRoboticsandNLPMeet.InProceedingsoftheTwenty-SeventhInternationalJointConference
onArtificialIntelligence,IJCAI-18.InternationalJointConferencesonArtificialIntelligenceOrganization,5687–5691. https://doi.org/10.24963/ijcai.
2018/810
AndrewKachitesMcCallumzyandKamalNigamy.1998.Employingemandpool-basedactivelearningfortextclassification.InICML.
GregorMehlmann,MarkusHäring,KathrinJanowski,TobiasBaur,PatrickGebhard,andElisabethAndré.2014.ExploringaModelofGazeforGrounding
inMultimodalHRI.InProceedingsofthe16thInternationalConferenceonMultimodalInteraction(ICMI’14).AssociationforComputingMachinery,
NewYork,NY,USA,247–254. https://doi.org/10.1145/2663204.2663275event-place:Istanbul,Turkey.
ZihangMeng,LichengYu,NingZhang,TamaraLBerg,BabakDamavandi,VikasSingh,andAmyBearman.2021.Connectingwhattosaywithwhereto
lookbymodelinghumanattentiontraces.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.12679–12688.
SinaMohseni,NiloofarZarei,andEricDRagan.2021.AmultidisciplinarysurveyandframeworkfordesignandevaluationofexplainableAIsystems.
ACMTransactionsonInteractiveIntelligentSystems(TiiS)11,3-4(2021),1–45.
RobertMunroMonarch.2021.Human-in-the-LoopMachineLearning:Activelearningandannotationforhuman-centeredAI.SimonandSchuster.
A.Moran,V.Gadepally,M.Hubbell,andJ.Kepner.2015.ImprovingBigDatavisualanalyticswithinteractivevirtualreality.In2015IEEEHighPerformance
ExtremeComputingConference(HPEC).1–6.
SharanNarang,ColinRaffel,KatherineLee,AdamRoberts,NoahFiedel,andKarishmaMalkan.2020.WT5?!trainingtext-to-textmodelstoexplaintheir
predictions.arXivpreprintarXiv:2004.14546(2020).
DuyMinhHoNguyen,AbrahamObinwanneEzema,FabrizioNunnari,andDanielSonntag.2020.AVisuallyExplainableLearningSystemforSkin
LesionDetectionUsingMultiscaleInputwithAttentionU-Net.InKI2020:AdvancesinArtificialIntelligence-43rdGermanConferenceonAI,Bamberg,
Germany,September21-25,2020,Proceedings(LectureNotesinComputerScience,Vol.12325),UteSchmid,FranziskaKlügl,andDiedrichWolter(Eds.).
Springer,313–319. https://doi.org/10.1007/978-3-030-58285-2_28
FabrizioNunnariandDanielSonntag.2021.ASoftwareToolboxforDeployingDeepLearningDecisionSupportSystemswithXAICapabilities.InEICS
’21:ACMSIGCHISymposiumonEngineeringInteractiveComputingSystems,VirtualEvent,TheNetherlands,8-11June2021,PanosMarkopoulos,JunHu,
andPhilippeA.Palanque(Eds.).ACM,44–49. https://doi.org/10.1145/3459926.3464753
OmarJubran,VeraEymann,NicoleBurkard,JanSpilski,MarcHerrlich,DanielaCzernochowski,andThomasLachmann.2021.ExpandingonBehavioral
DataCollectioninanAdaptedN-BackTaskforVirtualReality.InContributionstothe63rdTagungExperimentellarbeitenderPsychologen.PabstScience
Publishers,Lengerich,Germany,Ulm,Germany.
SharonOviatt.2006.Human-centereddesignmeetscognitiveloadtheory:designinginterfacesthathelppeoplethink.InProceedingsofthe14thACM
internationalconferenceonMultimedia(MM’06).AssociationforComputingMachinery,NewYork,NY,USA,871–880. https://doi.org/10.1145/
1180639.1180831
SharonOviatt,BjörnSchuller,PhilipCohen,DanielSonntag,GerasimosPotamianos,andAntonioKrüger.2019.TheHandbookofMultimodal-Multisensor
Interfaces,Volume3:LanguageProcessing,Software,Commercialization,andEmergingDirections.Morgan&Claypool.
FiratOzdemir,ZixuanPeng,ChristineTanner,PhilippFuernstahl,andOrcunGoksel.2018.ActiveLearningforSegmentationbyOptimizingContent
InformationforMaximalEntropy.InMICCAIWorkshop:DeepLearninginMedicalImageAnalysis(DLMIA).
DongHukPark,LisaAnneHendricks,ZeynepAkata,AnnaRohrbach,BerntSchiele,TrevorDarrell,andMarcusRohrbach.2018a.Multimodalexplanations:
Justifyingdecisionsandpointingtotheevidence.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.8779–8788.
DongHukPark,LisaAnneHendricks,ZeynepAkata,AnnaRohrbach,BerntSchiele,TrevorDarrell,andMarcusRohrbach.2018b.MultimodalExplanations:
JustifyingDecisionsandPointingtotheEvidence.2018IEEE/CVFConferenceonComputerVisionandPatternRecognition(2018),8779–8788.
RosalindW.Picard.2000.Affectivecomputing.MITpress.
BryanAPlummer,LiweiWang,ChrisMCervantes,JuanCCaicedo,JuliaHockenmaier,andSvetlanaLazebnik.2015. Flickr30kentities:Collecting
region-to-phrasecorrespondencesforricherimage-to-sentencemodels.InProceedingsoftheIEEEinternationalconferenceoncomputervision.2641–2649.
JordiPont-Tuset,JasperUijlings,SoravitChangpinyo,RaduSoricut,andVittorioFerrari.2020.Connectingvisionandlanguagewithlocalizednarratives.
InEuropeanconferenceoncomputervision.Springer,647–664.
AlexanderPrangeandDanielSonntag.2021.ADemonstratorforInteractiveImageClusteringandFine-TuningNeuralNetworksinVirtualReality.InKI
2021:AdvancesinArtificialIntelligence-44thGermanConferenceonAI,VirtualEvent,September27-October1,2021,Proceedings(LectureNotesin
ComputerScience,Vol.12873),StefanEdelkamp,RalfMöller,andElmarRueckert(Eds.).Springer,194–203. https://doi.org/10.1007/978-3-030-87626-5_14
DanishPruthi,RachitBansal,BhuwanDhingra,LivioBaldiniSoares,MichaelCollins,ZacharyC.Lipton,GrahamNeubig,andWilliamW.Cohen.2022.
EvaluatingExplanations:HowMuchDoExplanationsfromtheTeacherAidStudents?TransactionsoftheAssociationforComputationalLinguistics10(04
2022),359–375. https://doi.org/10.1162/tacl_a_00465arXiv:https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00465/2006971/tacl_a_00465.pdf
DirkQueck,IannisAlbert,NicoleBurkard,PhilippZimmer,GeorgVolkmar,BastianDänekas,RainerMalaka,andMarcHerrlich.2022. SpiderClip:
TowardsanOpenSourceSystemforWearableDeviceSimulationinVirtualReality.InExtendedAbstractsofthe2022CHIConferenceonHumanFactors
inComputingSystems(CHIEA’22).AssociationforComputingMachinery,NewYork,NY,USA,1–7. https://doi.org/10.1145/3491101.3519758
MohamadRabbath,PhilippSandhaus,andSusanneBoll.2011a.Automaticcreationofphotobooksfromstoriesinsocialmedia.ACMTransactionson
MultimediaComputing,Communications,andApplications7S,1(Nov.2011),27:1–27:18. https://doi.org/10.1145/2037676.2037684
MohamadRabbath,PhilippSandhaus,andSusanneBoll.2011b. Multimediaretrievalinsocialnetworksforphotobookcreation.InProceedings
ofthe1stACMInternationalConferenceonMultimediaRetrieval(ICMR’11).AssociationforComputingMachinery,NewYork,NY,USA,1–2.
https://doi.org/10.1145/1991996.1992068AlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE) 25
NazneenFatemaRajani,BryanMcCann,CaimingXiong,andRichardSocher.2019.ExplainYourself!LeveragingLanguageModelsforCommonsense
Reasoning.InProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics.4932–4942.
KrishnanRamnath,SimonBaker,LucyVanderwende,MotazEl-Saban,SudiptaNSinha,AnithaKannan,NoranHassan,MichelGalley,YiYang,Deva
Ramanan,etal.2014.Autocaption:Automaticcaptiongenerationforpersonalphotos.InIEEEWinterConferenceonApplicationsofComputerVision.
IEEE,1050–1057.
GabriëlleRas,MarcelvanGerven,andPimHaselager.2018.Explanationmethodsindeeplearning:Users,values,concernsandchallenges.InExplainable
andinterpretablemodelsincomputervisionandmachinelearning.Springer,19–36.
AlexanderJRatner,ChristopherMDeSa,SenWu,DanielSelsam,andChristopherRé.2016.Dataprogramming:Creatinglargetrainingsets,quickly.
Advancesinneuralinformationprocessingsystems29(2016),3567–3575.
AnkeVerenaReinschluessel,JoernTeuber,MarcHerrlich,JeffreyBissel,MelanievanEikeren,JohannesGanser,FeliciaKoeller,FenjaKollasch,Thomas
Mildner,LucaRaimondo,LarsReisig,MarcRuedel,DannyThieme,TobiasVahl,GabrielZachmann,andRainerMalaka.2017.VirtualRealityfor
User-CenteredDesignandEvaluationofTouch-freeInteractionTechniquesforNavigatingMedicalImagesintheOperatingRoom.InProceedings
ofthe2017CHIConferenceExtendedAbstractsonHumanFactorsinComputingSystems(CHIEA’17).ACM,NewYork,NY,USA,2001–2009. https:
//doi.org/10.1145/3027063.3053173event-place:Denver,Colorado,USA.
LauraRieger,ChandanSingh,WilliamMurdoch,andBinYu.2020.Interpretationsareuseful:penalizingexplanationstoalignneuralnetworkswithprior
knowledge.InInternationalConferenceonMachineLearning.PMLR,8116–8126.
AndrewSlavinRoss,MichaelCHughes,andFinaleDoshi-Velez.2017.RightfortheRightReasons:TrainingDifferentiableModelsbyConstrainingtheir
Explanations.InIJCAI.
ConstantinA.Rothkopf,DanaH.Ballard,andMaryM.Hayhoe.2016.Taskandcontextdeterminewhereyoulook.JournalofVision7,14(July2016),
16–16. https://doi.org/10.1167/7.14.16
GerbenRotman,NikolausF.Troje,RolandS.Johansson,andJ.RandallFlanagan.2006.EyeMovementsWhenObservingPredictableandUnpredictable
Actions.JournalofNeurophysiology96,3(2006),1358–1369. https://doi.org/10.1152/jn.00227.2006_eprint:https://doi.org/10.1152/jn.00227.2006.
N.RoyandA.McCallum.2001.Towardoptimalactivelearningthroughsamplingestimationoferrorreduction.InICML.
CynthiaRudinandJoannaRadin.2019. WhyAreWeUsingBlackBoxModelsinAIWhenWeDon’tNeedTo?ALessonFromAnExplainableAI
Competition.HarvardDataScienceReview1,2(2019). https://doi.org/10.1162/99608f92.5a8a3a3d
ChristianRupprecht,IroLaina,NassirNavab,GregoryDHager,andFedericoTombari.2018.Guideme:Interactingwithdeepnetworks.InProceedingsof
theIEEEConferenceonComputerVisionandPatternRecognition.8551–8561.
RichardM.RyanandEdwardL.Deci.2000.Self-determinationtheoryandthefacilitationofintrinsicmotivation,socialdevelopment,andwell-being.
Americanpsychologist55,1(2000),68.
PhilippSandhausandSusanneBoll.2011.Semanticanalysisandretrievalinpersonalandsocialphotocollections.MultimediaToolsandApplications51,1
(Jan.2011),5–33. https://doi.org/10.1007/s11042-010-0673-1
PhilippSandhaus,MohammadRabbath,andSusanneBoll.2011. EmployingAestheticPrinciplesforAutomaticPhotoBookLayout.InAdvancesin
MultimediaModeling(LectureNotesinComputerScience),Kuo-TienLee,Wen-HsiangTsai,Hong-YuanMarkLiao,TsuhanChen,Jun-WeiHsieh,and
Chien-ChengTseng(Eds.).Springer,Berlin,Heidelberg,84–95. https://doi.org/10.1007/978-3-642-17832-0_9
PhilippSandhaus,SabineThieme,andSusanneBoll.2008. Processesofphotobookproduction. MultimediaSystems14,6(Dec.2008),351–357.
https://doi.org/10.1007/s00530-008-0136-y
RamprasaathRSelvaraju,StefanLee,YilinShen,HongxiaJin,ShaliniGhosh,LarryHeck,DhruvBatra,andDeviParikh.2019.Takingahint:Leveraging
explanationstomakevisionandlanguagemodelsmoregrounded.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.
2591–2600.
BurrSettles.2010.Activelearningliteraturesurvey.UniversityofWisconsin,Madison52,55-66(2010),11.
SanketShah,AnandMishra,NaganandYadati,andParthaPratimTalukdar.2019.KVQA:Knowledge-AwareVisualQuestionAnswering.Proceedingsof
theAAAIConferenceonArtificialIntelligence33,01(Jul.2019),8876–8884. https://doi.org/10.1609/aaai.v33i01.33018876
XiaotingShao,ArsenySkryagin,WolfgangStammer,PatrickSchramowski,andKristianKersting.2021.RightforBetterReasons:TrainingDifferentiable
ModelsbyConstrainingtheirInfluenceFunction.InProceedingsofThirty-FifthAAAIConferenceonArtificialIntelligence(AAAI).
ChangjianShui,FanZhou,ChristianGagné,andBoyuWang.2020. Deepactivelearning:Unifiedandprincipledmethodforqueryandtraining.In
InternationalConferenceonArtificialIntelligenceandStatistics.PMLR,1308–1318.
PatriceY.Simard,SaleemaAmershi,DavidMaxwellChickering,AliciaEdelmanPelton,SoroushGhorashi,ChristopherMeek,GonzaloA.Ramos,JinaSuh,
JohanVerwey,MoWang,andJohnWernsing.2017.MachineTeaching:ANewParadigmforBuildingMachineLearningSystems.CoRRabs/1707.06742
(2017).arXiv:1707.06742 http://arxiv.org/abs/1707.06742
AsimSmailagic,HaeYoungNoh,PedroCosta,DeveshWalawalkar,KartikKhandelwal,MostafaMirshekari,JonathonFagert,AdriánGaldrán,andSusu
Xu.2018.MedAL:DeepActiveLearningSamplingMethodforMedicalImageAnalysis.InICMLA.
KacperSokolandPeterFlach.2020.Explainabilityfactsheets:aframeworkforsystematicassessmentofexplainableapproaches.InProceedingsofthe
2020ConferenceonFairness,Accountability,andTransparency.56–67.
DanielSonntag.2010.Ontologiesandadaptivityindialogueforquestionanswering.Vol.4.IOSPress.
DanielSonntag,FabrizioNunnari,andHans-JürgenProfitlich.2020.TheSkincareproject,aninteractivedeeplearningsystemfordifferentialdiagnosisof
malignantskinlesions.TechnicalReport.arXivpreprintarXiv:2005.09448(2020).26 DanielSonntag,MichaelBarz,andThiagoGouvêa
JamshidSourati,AliGholipour,JenniferG.Dy,SilaKurugol,andSimonK.Warfield.2018.ActiveDeepLearningwithFisherInformationforPatch-wise
SemanticSegmentation.DeepLearninginMedicalImageAnalysisandMultimodalLearningforClinicalDecisionSupport(2018),83–91.
JulianSteilandAndreasBulling.2015.DiscoveryofEverydayHumanActivitiesfromLong-TermVisualBehaviourUsingTopicModels.InProceedingsof
the2015ACMInternationalJointConferenceonPervasiveandUbiquitousComputing(UbiComp’15).AssociationforComputingMachinery,NewYork,
NY,USA,75–85. https://doi.org/10.1145/2750858.2807520event-place:Osaka,Japan.
YusukeSuganoandAndreasBulling.2016.Seeingwithhumans:Gaze-assistedneuralimagecaptioning.arXivpreprintarXiv:1608.05203(2016).
EceTakmaz,SandroPezzelle,LisaBeinborn,andRaquelFernández.2020.GeneratingImageDescriptionsviaSequentialCross-ModalAlignmentGuided
byHumanGaze.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).4664–4677.
ChenhaoTan.2021.OntheDiversityandLimitsofHumanExplanations.arXivpreprintarXiv:2106.11988(2021).
StefanoTesoandOliverHinz.2020. ChallengesinInteractiveMachineLearning:TowardCombiningLearning,Teaching,andUnderstanding. ,
127–130pages.
StefanoTesoandKristianKersting.2019.Explanatoryinteractivemachinelearning.InProceedingsofthe2019AAAI/ACMConferenceonAI,Ethics,and
Society.239–245.
JesseThomason,JivkoSinapov,MaxwellSvetlik,PeterStone,andRaymondJ.Mooney.2016.LearningMulti-ModalGroundedLinguisticSemanticsby
Playing"ISpy".InProceedingsoftheTwenty-FifthInternationalJointConferenceonArtificialIntelligence(IJCAI’16).AAAIPress,3477–3483. event-place:
NewYork,NewYork,USA.
YonglongTian,YueWang,DilipKrishnan,JoshuaB.Tenenbaum,andPhillipIsola.2020.RethinkingFew-ShotImageClassification:aGoodEmbeddingIs
AllYouNeed?.InECCV.
RichardTomsett,DaveBraines,DanHarborne,AlunPreece,andSupriyoChakraborty.2018.Interpretabletowhom?Arole-basedmodelforanalyzing
interpretablemachinelearningsystems.arXivpreprintarXiv:1806.07552(2018).
SimonTongandDaphneKoller.2001.Supportvectormachineactivelearningwithapplicationstotextclassification.JMLR2(2001),45–66.
EmmaM.vanZoelen,TinaMioch,ManiTajaddini,ChristianFleiner,StefaniTsaneva,PietroCamin,ThiagoS.Gouvêa,KimBaraka,MaaikeH.T.
deBoer,andMarkA.Neerincx.2023. DevelopingTeamDesignPatternsforHybridIntelligenceSystems.InHHAI2023:AugmentingHuman
Intellect-ProceedingsoftheSecondInternationalConferenceonHybridHuman-ArtificialIntelligence,June26-30,2023,Munich,Germany(Frontiersin
ArtificialIntelligenceandApplications,Vol.368),PaulLukowicz,SvenMayer,JaninKoch,JohnShawe-Taylor,andIlariaTiddi(Eds.).IOSPress,3–16.
https://doi.org/10.3233/FAIA230071
VeraEymann,OmarJubran,NicoleBurkard,JanSpilski,MarcHerrlich,ThomasLachmann,andDanielaCzernochowski.2021.QuantifyingCognitive
LoadbyCombiningEyeTrackingandEEGinaVirtualRealityEnvironment.InContributionstothe63rdTagungExperimentellarbeitenderPsychologen.
PabstSciencePublishers,Lengerich,Germany,Ulm,Germany.
PengWang,QiWu,ChunhuaShen,AnthonyDick,andAntonVanDenHenge.2017.Explicitknowledge-basedreasoningforvisualquestionanswering.
InProceedingsofthe26thInternationalJointConferenceonArtificialIntelligence.1290–1296.
RuizeWang,ZhongyuWei,PijiLi,QiZhang,andXuanjingHuang.2020.Storytellingfromanimagestreamusingscenegraphs.InProceedingsofthe
AAAIConferenceonArtificialIntelligence,Vol.34.9185–9192.
ZijieJWang,DongjinChoi,ShenyuXu,andDiyiYang.2021.PuttingHumansintheNaturalLanguageProcessingLoop:ASurvey.InProceedingsofthe
FirstWorkshoponBridgingHuman–ComputerInteractionandNaturalLanguageProcessing.47–52.
DavidWatsonandLeeAnnaClark.1994. ThePANAS-X:Manualforthepositiveandnegativeaffectschedule-expandedform. (1994). Publisher:
UniversityofIowa.
DavisWertheimer,LumingTang,andBharathHariharan.2021.Few-ShotClassificationWithFeatureMapReconstructionNetworks.InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).8012–8021.
SarahWiegreffeandAnaMarasovic.2021.TeachMetoExplain:AReviewofDatasetsforExplainableNaturalLanguageProcessing.InThirty-fifth
ConferenceonNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack(Round1). https://openreview.net/forum?id=ogNcxJn32BZ
SarahWiegreffe,AnaMarasović,andNoahA.Smith.2021.MeasuringAssociationBetweenLabelsandFree-TextRationales.InProceedingsofthe2021
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.AssociationforComputationalLinguistics,OnlineandPuntaCana,Dominican
Republic,10266–10284. https://doi.org/10.18653/v1/2021.emnlp-main.804
AniWithöft,LarbiAbdenebaoui,andSusanneBoll.2022. ILMICA-InteractiveLearningModelofImageCollageAssessment:ATransferLearning
ApproachforAestheticPrinciples.InMultiMediaModeling(LectureNotesinComputerScience),BjörnÞórJónsson,CathalGurrin,Minh-TrietTran,
Duc-TienDang-Nguyen,AnitaMin-ChunHu,BinhHuynhThiThanh,andBenoitHuet(Eds.).SpringerInternationalPublishing,Cham,84–96.
https://doi.org/10.1007/978-3-030-98355-0_8
LinYang,YizheZhang,JianxuChen,SiyuanZhang,andDannyZ.Chen.2017.SuggestiveAnnotation:ADeepActiveLearningFrameworkforBiomedical
ImageSegmentation.InMICCAI.
HuihanYao,YingChen,QinyuanYe,XisenJin,andXiangRen.2021.RefiningLanguageModelswithCompositionalExplanations.AdvancesinNeural
InformationProcessingSystems34(2021).
QinyuanYe,XiaoHuang,ElizabethBoschee,andXiangRen.2020.TeachingMachineComprehensionwithCompositionalExplanations.InProceedingsof
the2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:Findings.1599–1615.
JanZacharias,MichaelBarz,andDanielSonntag.2018.Asurveyondeeplearningtoolkitsandlibrariesforintelligentuserinterfaces.arXivpreprint
arXiv:1803.04818(2018).AlookunderthehoodoftheInteractiveDeepLearningEnterprise(No-IDLE) 27
QiZhang,ZhenLei,ZhaoxiangZhang,andStanZ.Li.2020.Context-AwareAttentionNetworkforImage-TextRetrieval.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR).