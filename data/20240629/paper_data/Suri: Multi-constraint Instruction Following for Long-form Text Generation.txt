Suri: Multi-constraint Instruction Following for Long-form Text
Generation
ChauMinhPham SimengSun* MohitIyyer
UniversityofMassachusettsAmherst
{ctpham,simengsun,miyyer}@cs.umass.edu
Abstract tionsthatcanonlybesatisfiedwithlong-formout-
puts (i.e., thousands of tokens), such as creating
Existing research on instruction following
detailedtechnicalreportsorwritingengagingfic-
largely focuses on tasks with simple instruc-
tionalnarratives?
tions and short responses. In this work, we
Weexplorethisquestionbyconductingthefirst
exploremulti-constraintinstructionfollowing
forgeneratinglong-formtext. WecreateSuri, in-depthstudyoflong-forminstructionfollowing
adatasetwith20Khuman-writtenlong-form withmulti-constraintinstructions. Tofacilitateour
texts paired with LLM-generated backtrans- experiments,wecreateanewdataset,Suri,2 using
lated instructions that contain multiple com- instruction backtranslation (Li et al., 2023; Kök-
plexconstraints. Becauseofprohibitivechal- sal et al., 2023). This process involves feeding a
lengesassociatedwithcollectinghumanprefer-
human-writtenlong-formtext(e.g.,chaptersfrom
encejudgmentsonlong-formtexts,preference-
anovel)intoanLLMtogenerateinstructionsthat
tuning algorithms such as DPO are infeasi-
could have been followed to create the text. The
ble in our setting; thus, we propose Instruc-
tionalORPO(I-ORPO),analignmentmethod resultingdataset,Suri,consistsof20Ktextspaired
basedontheORPOalgorithm. Insteadofre- withLLM-generatedinstructions,eachcontaining
ceiving negative feedback from dispreferred ≈10semanticandstylisticconstraints(Figure1).
responses,I-ORPOobtainsnegativefeedback
HowcanweuseSuritoimproveanLLM’slong-
fromsyntheticallycorruptedinstructionsgen-
forminstructionfollowingabilities? Whilesuper-
erated by an LLM. Using Suri, we perform
vised fine-tuning (SFT) has been quite effective
supervisedandI-ORPOfine-tuningonMistral-
7b-Instruct-v0.2. Theresultingmodels,Suri- forshort-formdatasets(Mishraetal.,2022;Wang
SFTandSuri-I-ORPO,generatesignificantly et al., 2022; Sanh et al., 2022; Wei et al., 2022;
longer texts (∼5K tokens) than base models Chung et al., 2022), we observe that fine-tuned
withoutsignificantqualitydeterioration. Our Surimodelsoftengeneratetextsthatareincoher-
humanevaluationshowsthatwhilebothSFT
entandfailtosatisfyconstraintstowardstheendin
andI-ORPOmodelssatisfymostconstraints,
theinstructions. Preferencetuningmethodssuch
Suri-I-ORPO generations are generally pre-
asDPO(Rafailovetal.,2023)andRLHF(Ouyang
ferredfortheircoherentandinformativeincor-
porationoftheconstraints.1 et al., 2022) are challenging to use in this setting
duetodifficultiesandcostinobtainingpreference
1 Introduction judgmentsonlong-formtexts(Touvronetal.,2023;
Xu et al., 2023c). Specifically, when annotating
Improving the instruction-following abilities of
preferencesforlongtexts,humanannotatorsmay
modernlargelanguagemodels(LLMs)iscriticalto
struggle to determine if different sections of the
increasingtheireffectivenessandgeneralizability
textarefaithfultotheinstructionswhilesimultane-
formanypracticalapplications. However,mostex-
ouslyconsideringmultipleaspectsofthetext,such
istinginstruction-followingdatasets(e.g.,Alpaca)
ascoherenceandinformativeness.
containonlysimpleinstructionsthatcanbesolved
Motivated by this, we devise an alignment
by short model generations (Taori et al., 2023;
method that relies on synthetically corrupted in-
Conover et al., 2023; Köpf et al., 2023). What
structions. Specifically,wetakethebacktranslated
abouttaskswithcomplex,multi-constraintinstruc-
instructionx andcorruptitsconstraintsusingan
w
*NowatNVIDIA
1Codeathttps://github.com/chtmp223/suri 2Suriisanalpacabreedknownforitslong,lustroushair.
4202
nuJ
72
]LC.sc[
1v17391.6042:viXraSuri-SFT
Response y Backtranslated Instruction x w Corrupted Instruction x l
(Human-written) (LLM-generated) (LLM-generated)
Main Goal: Write a whimsical and
Main Goal: Write a whimsical and
Alice was beginning to get very imaginative narrative that centers
imaginative narrative that centers
tired of sitting by her sister around a young girl's journey through a around a young girl's journey through a
on the bank, and of having nothing fantastical world. fantastical world.
to do: once or twice she had Back- Constraints: Corrupt
peeped into the book her sister was Constraints:
reading, but it had no pictures or Translate - p rS ot ta ar gt oth ne is tn a fer era lit niv ge b w oi rt eh d t h we it h her Constraints - Start the narrative with the
conversations in it, “and what is the current surroundings. protagonist without any prior feelings of
use of a book,” thought Alice - Employ a tone that blends matter-of- boredom with her surroundings.
“without pictures or conversation?” fact narration with elements of dry - Employ a tone that is consistently
serious and devoid of humor.
humor and irony.
Suri-I-ORPO
Figure1: Ourworkconsistsoftwostages. First,weconstructtheSuridatasetusinggoldresponsessampledfrom
threeexistingdatasetsthatincludecreativewritingandopenwebtext,alongwithbacktranslatedinstructionx
w
andcorruptedinstructionx . Second,wefine-tuneMistral-7B-Instruct-v0.2onSuri,resultingintwovariations:
l
Suri-I-ORPO(viaI-ORPO)andSuri-SFT(viasupervisedfine-tuning).
LLMsuchthatthegoldresponsedoesnotsatisfy straints. WhenusinganLLMforacomplexwriting
the corrupted constraints (for example, see x in task, users might have many constraints in mind
l
Figure1). WethendevelopavariantoftheOdds andexpectlengthy,detailedresponsesintheform
Ratio Preference Optimization objective (Hong of books, blog posts, etc. This task is particu-
etal.,2024,ORPO)tousethesecorruptedinstruc- larlychallengingforcurrentLLMs,whichstruggle
tionsasnegativefeedback. Werefertothisalign- withgeneratingcoherentlong-formoutputs(Guan
ment method as Instructional ORPO, or I-ORPO etal.,2021;Wangetal.,2023a),andthisdifficulty
forshort. canbeamplifiedwhenmultipleconstraintsarein-
We conduct a series of automatic and human volved. Recentinstruction-followingdatasetshave
evaluationsongenerationsfromSFTandI-ORPO- featured multi-constraint instructions (Xu et al.,
tuned models to validate our method. Compared 2024; Malaviya et al., 2024) and long-form re-
tothebasemodel,Mistral-7b-Instruct-v0.2(Jiang sponses(Köksaletal.,2023;Chenetal.,2024b),
et al., 2023), both SFT and I-ORPO significantly butnonehasintegratedthesetwoelements(Table
increase the generation length from 1K to 5K to- 1). We bridge this gap by creating Suri, which
kens. Ourfine-tunedmodelsalsoimprovetheabil- features complex instructions with multiple con-
itytodifferentiatebetweencorrectandcorrupted straints and lengthy gold responses (2-5K words,
instructionsbyatleast10%whilemaintaininglow about3-6Ktokens).
levels of n-gram repetitions in the text. We find
Wecollecthuman-writtenEnglishtextsamples,
thatGPT-4o(OpenAI,2024)cannotreliablyevalu-
such as books, religious texts, and blog posts, to
atelong-formresponses,makinghumanevaluation
serveasgoldresponses(y). Sincegatheringhuman-
crucial for assessing the constraint-following ca-
writteninstructionsforsuchlengthyresponsesis
pabilitiesofourgenerations. Annotatorsnotethat
difficultandexpensive,weturntoinstructionback-
ourfine-tunedmodelseffectivelyfollowgivencon-
translation(Lietal.,2023;Köksaletal.,2023),in
straints,withI-ORPObeingpreferredforitsability
whichanLLMisprovidedwithahuman-written
toincorporateconstraintscoherently,informatively,
text(e.g.,ashortstory)andpromptedtogenerate
andenjoyably.
instructions (x ) that could have been followed
w
to create that text. We further corrupt the con-
2 TheSuriDataset
straintsinx toobtainsyntheticallycorruptedin-
w
We focus on the task of long-form writing, both structions(x )forourI-ORPOalignmentmethod.
l
fictional and non-fictional, under multiple con- Intotal,Suricontains20Ksingle-turnexamples,Category Dataset Size Domain Prompt Response
Length Length
WritingPrompt(Fanetal.,2018) 300K CreativeWriting 28 735
WritingInstructions
ROCStory(Mostafazadehetal.,2016) 50K CreativeWriting 36 8
Instruction- Alpaca(Taorietal.,2023) 52K GeneralQ&As 13 44
following
LongForm-C(Köksaletal.,2023) 28K CommonCrawl, Wikipedia, 149 298
Long-form
StackExchange,Wikihow
Instruction-following
LongAlpaca-16K(Chenetal.,2024b) 12K Science,CreativeWriting,Wiki, 5945 218
GeneralQ&As
Scrolls(Shahametal.,2022) 120K Legal,Science,Entertainment 33506 97
Multi-constraint
Dolomites(Malaviyaetal.,2024) 2K 25AcademicFields 235 343
Instructions
Multi-constraint,
Long-form Suri(thiswork) 20K CommonCrawl, CreativeWrit- 347 4371
Instruction-following ing
Table1: ComparisonofSuriwithothersingle-turndatasetsintermsofrelevantdatafeatures,includingwriting
instructions,instruction-followingdatasets,andconstrainedinstructions. Thedatasizeaswellastheaveragelength
ofpromptsandresponses(measuredinwords)iseitherquotedfromtheoriginalpaperorcalculatedfrompublicly
availablesubsets. Suriistheonlydatasetfeaturingbothconstrainedinstructionsandlongresponses(>4Kwords)
specificallydesignedfortextgeneration.
eachconsistingofabacktranslatedinstructionx , regularexpressionstofilteroutirrelevantmetadata,
w
corrupted instruction x , and a human-written re- suchastablesofcontentsandacknowledgments.
l
sponse y. In this section, we detail our approach
RedPajama-Data-v2 RedPajamacontainsover
to selecting high-quality text samples (§2.1) and
100 billion documents from 84 CommonCrawl
creatingbacktranslatedinstructions(§2.2). Wealso
dumps,ofwhich6,646textsaresampled. Unlike
validateourgeneratedinstructions(§2.3)andana-
ChapterBreakandBooks3,whichconsistprimarily
lyzetheresultingdataset(§2.4).
of books and literary narratives, RedPajama cap-
2.1 CollectingResponses tures the style of everyday writing with informal
textualcontentsuchasblogposts,obituaries,and
Obtaining long-form gold responses y through
more. We apply a set of quality filters (see Ap-
crowdsourcing or hiring experts requires signifi-
pendixA)onthe2023-06and2023-14ssnapshots
cant cost and effort. As an alternative, we sam-
to obtain a subset of ≈ 300K high-quality, non-
plehuman-writtentextsinequalproportionsfrom
duplicateddocumentswritteninEnglish.
three existing datasets: ChapterBreak (Sun et al.,
2022), Books3 (Presser, 2020; Gao et al., 2020),
2.2 CreatingInstructionsviaBacktranslation
and RedPajama-Data-v2 (Computer, 2023). We
Suriincludesbacktranslatedinstructions(x )and
truncate the sampled texts to between 2,048 and w
corruptedinstructions(x ). Inx ,constraintsfrom
5,024words,makingthemsignificantlylongerthan l l
x are minimally edited to be partially violated
thoseinexistinginstruction-followingdatasets(Ta- w
while still faithful to the overall main goal of the
ble1). ThefinalSuridatasetisdividedintotrain-
instruction. These corrupted instructions, along
ing,validation,andtestsetsina10K/5K/5Ksplit.
with x and y, serve as inputs for our I-ORPO
w
ChapterBreak ChapterBreak (AO3 split) con- preferencetuningexperiments.
tains 7,355 fanfiction stories on Archive of Our
Backtranslating Instructions Our extracted
Own(AO3),ofwhich6,656textsaresampledfor
gold responses do not come with accompanying
Suri. Wemergetheindividualchaptersfromthe
instructions. Gathering these instructions can be
cleanedtextintoasingledocument.
costlyandtime-consuming,asannotatorshaveto
Books3 Books3 contains 197K books on Bibli- synthesizetheinstructionsfromlongtexts. There-
otik,3 of which 6,698 texts are sampled. We use fore,weuseinstructionbacktranslation(Lietal.,
3Duetocopyrightconcerns,weonlyreleasethetitlesand scripttoextractandcleanthetextsothatuserswithaccessto
IDsofthesampleddatafromthisdataset.WeprovideaPython Books3canrecreatethesamplesincludedinSuri.2023;Köksaletal.,2023)togeneratethemissing 2.4 InstructionDiversity
instructions. Specifically,wepromptGPT-4-turbo4
InstructionsinSurifocusprimarilyonlong-form
withagoldresponsey togenerateacorresponding
textgeneration,particularlycraftingnarrativesor
instruction x that contains a main goal, which
w articles. Therefore,thekeyelementthatintroduces
summarizes the content of the text, and a list of
diversityacrosstheseinstructionsistheaccompa-
≈10 constraints (Table 9). These constraints can
nying list of constraints. Here, we measure the
focusonstylisticelements(howsomethingiscom-
proportionofconstraintsbeingbroad/specificorfo-
municatedthroughtone,language,sentencestruc-
cusingonsemantic/stylisticelements. Weprompt
ture), semantic elements (what topics, meanings, Mistral-7B-Instruct-v0.2 (Jiang et al., 2023),7 to
and concepts are included), or a combination of
assign each constraint to the applicable category.
both. Constraints can also be broad, applying to
Wefindthatsemanticconstraintsaccountformore
largeportionsofthetext,orspecific,addressingel-
than half of each instruction, followed by mixed
ementsthatoccuronlyonce. Theresultisahighly
constraints (Figure 2). Broad constraints, on the
detailed, multi-constraint instruction that covers
otherhand, makeup56%ofthetotalconstraints.
differentpartsofthetext(x inFigure1).
w Overall,thedistributionofconstrainttypesisrela-
tivelybalanced,withastrongeremphasisonbroad
CorruptingInstructions WewanttouseSuri andsemanticconstraints.
inouralignmentexperiments,whichtraditionally
relyonpreferencejudgments(e.g.,labeledy and
w
y pairs). However,obtainingthesejudgmentsfor
l
long-formoutputsischallengingduetothemany
competing aspects to consider (e.g., faithfulness
to instructions, overall coherence, etc.). Instead
ofincludingacorruptedy ,wefocusonlearning
l
from a corrupted instruction x . To create x , we
l l
promptGPT-4-turbo5 tominimallyediteachcon-
straint in x while preserving the original main
w Figure 2: Average percentage of different constraint
goal(Table10). Theresultinginstructionsaverage typeswithineachinstruction.Theleftfigurecategorizes
386tokens,closelymatchingtheaveragelengthof the constraints based on their content, and the right
goldinstructionsatapproximately411tokens. figurereferstoconstraintscopes.
2.3 ValidatingInstructions
3 AligninglanguagemodelswithSuri
Tovalidatewhetherthebacktranslatedinstructions OurgoalistoassesswhetherSurihelpsimprove
faithfully represent the original text, we conduct
the instruction-following capabilities of Mistral-
a human evaluation on a sample of 30 (x ,y)
w 7B-Instruct-v0.2forlong-formtextgeneration.
pairs. ThreeUpwork6 annotatorsareaskedtoread
Weexperimentwithtwomethodsoffine-tuning
throughthe(x w,y)pairs,highlightalltextspansin
Mistral-7B-Instruct-v0.2onSuri: supervisedfine-
theresponsethatsupportthegivenconstraints,and
tuning (SFT) using (x ,y) pairs and a modi-
w
determineiftheresponsesupportstheinstruction
fied ORPO alignment (Hong et al., 2024) using
(Figure6). Ourfindingsindicatethat,onaverage,
(x ,x ,y)triplets. Weemphasizethatpreference
w l
about 87% of the constraints are fully satisfied,
judgmentsaredifficulttoobtainforlong-formre-
withtheremainingconstraintsbeingpartiallysatis-
sponses due to numerous aspects of the text that
fied(seeAppendixFforagreementstatistics). We
mustbeconsideredwithrespecttotheinstructions.
conclude that the backtranslated instructions are
Therefore,weperformmodelalignmentwithcor-
generallyfaithfultotheoriginaltext.
rect instruction x and corrupted instruction x
w l
instead. Fulldetailsonfine-tuninglibraries,hard-
4GPT-4-turbo refers to gpt-4-0215-preview. Experi- wareconfigurations,andhyperparameterscanbe
mentdoneusingtemperature=0.6andtop_p=0.9. foundinAppendixC.
5Experiment done using model=gpt-4-0125-preview,
temperature=0.0,top_p=0.0toensuredeterministicresults. 7Experimentdoneusinggreedydecoding.Thefirstauthor
6SeeAppendixFforrecruitmentandcompensationdetails. manuallyverifiesanoutputsubset.Suri-I-ORPO OddsRatioPreferenceOptimiza-
tion(ORPO)(Hongetal.,2024)combinesSFTand
preferencealignmentbyincorporatingalogodds
ratiotermintothenegativelog-likelihoodloss. We
chooseORPOduetoitssimplicity,competitiveper-
formancewithotherpreferencetuningalgorithms
and the ease with which we can modify for our
setting. The original algorithm learns from pref-
erencejudgments,requiringaccesstochosenand
rejectedresponsesinthe(x,y ,y )format. Since
w l
our dataset contains gold and corrupted instruc-
Figure3: ORPOtrainingcurve. Leftfiguredocuments
tions instead, we modify ORPO so that the algo- thelogprobabilityofthechosenandrejectedprompts
rithmaccepts(x ,x ,y). Werefertothismodified over2epochs. Rightfigureshowsthelogprobability
w l
methodasInstructionalOddsRatioPreferenceOp- oftheresponsegiventhechosenandrejectedprompts
timization (I-ORPO), where the modified loss is over2epochs. Adivergencebetweenlogps(y|x w)and
logps(y|x )isobservedafter0.5trainingepoch.
calculatedas: l
L = E [L +λ·L ] (1)
I-ORPO (xw,x l,y) SFT I-OR
4 AutomaticEvaluation
where
(cid:18) odds (y|x )(cid:19) Ourautomaticassessmentdemonstratesthatboth
θ w
L I-OR = −logσ log (2) Suri-I-ORPOandSuri-SFTincreasethelengthof
odds (y|x )
θ l
thegeneratedtextswhilemaintainingareasonable
P (y|x)
θ
odds (y|x) = (3) levelofrepetition. Comparedtobaselinemodels,
θ
1−P (y|x)
θ Suri-I-ORPO ismore likely toassign higher log
In the original ORPO formulation, the model
probabilities to tokens in the response given the
is learning if the log probability of P (y |x), de-
θ w correctinstructionthanthecorruptedinstruction.
noted logps(y |x), increases and log probability
w
ofP (y |x),denotedlogps(y |x ),decreasesafter 4.1 Suri-I-ORPOandSuri-SFTgenerate
θ l l w
anumberoftrainingsteps,resultinginthelogodds substantiallylongertext.
ratio increasing. In I-ORPO, the same y is used Wemeasuretheaveragenumberoftokens8 ingen-
for both instruction types. Therefore, the model erationsfromourfine-tunedmodels(Suri-I-ORPO
islearningifthelogprobabilitieslogps(y|x w)and andSuri-SFT)andcomparethemtobaselinemod-
logps(y|x l)divergewhilelogps(x w)andlogps(x l)
els, includingMistral-7B-Instruct-v0.2, Llama-3-
remain stable. We observe this trend in Figure 3. 8B-Instruct(AI@Meta,2024),andMixtral-8x7B-
LossderivationandanalysisareinAppendixD. Instruct-v0.1(Jiangetal.,2024). Forfasterinfer-
WeperformI-ORPOfine-tuningwithLoRAon ence, we use vLLM (Kwon et al., 2023) to gen-
Mistral-7B-Instruct-v0.2 for two epochs, using a erate outputs from the backtranslated instruction
learningrateof5e-5,λof0.4,andaLoRArankand x .9 Proprietary models like GPT-4 and Claude
w
alphaof16. Wedonotobservesignsofthemodel areexcludedduetotheirmaximumgenerationout-
learning with full-model tuning, so we choose to putlimitof4,096tokens,10 whereasopen-weight
useLoRAfine-tuninginstead. Tominimizenoise models allow for outputs of arbitrary maximum
andimprovethemodel’sabilitytodistinguishbe- length.
tweengoldandcorruptedinstructions,weinclude Our fine-tuned models, Suri-SFT and Suri-I-
asingleconstraintineachinstruction,x andx .
w l ORPO,generatesignificantlylongeroutputscom-
pared to the open-weight baselines, with an aver-
Suri-SFT We perform LoRA supervised fine-
ageofapproximately4,800and5,100tokensper
tuning(Huetal.,2021)onMistral-7B-Instruct-v0.2
generation,respectively(Figure4). Theselengths
fortwoepochsusingalearningrateof5e-5,witha
LoRArankandalphaof16. Foreachinstruction 8Measured using tiktoken package (https://github.
x ,weincludeavaryingnumberofconstraintsto com/openai/tiktoken)with“o200k_base”encoding.
w
exposethemodeltodifferentinstructionformats. 9Experiment done using greedy decoding,
max_token=10K. Inference prompts specify that 5K
We do not use full-model tuning to match the I-
tokensshouldbegenerated.
ORPOtrainingsetting. 10Claudedocumentation;OpenAIdocumentationFigure 5: Average percentage of 5-gram repetitions
beforeandafter2,048tokensineachgenerationfrom
I-ORPOandSFTmodels.
Figure4:Averagenumberoftokensingenerationsfrom
baseline open-source models (Llama-3-8B-Instruct,
of generations with n-gram repetitions (Table 2).
Mixtral-8x7B-Instruct-v0.1,Mistral-7B-Instruct-v0.2)
andourfine-tunedmodels(Suri-I-ORPO,Suri-SFT). Among the baseline models, Mistral-Instruct has
the lowest percentage of generations with repeti-
tion,possiblybecauseitsgenerationsarealsothe
exceed the maximum generation capacity of pro-
shortest. Surprisingly,Llama-InstructandMixtral-
prietarymodels,whichislimitedtoaround4,096
Instruct, with their short generations, possess a
tokens. Among the baselines, Mixtral produces
greaterproportionofgenerationswithn-gramrep-
the longest generations, averaging over 1,500 to-
etitionscomparedtoourfine-tunedmodels.
kens,whileMistral-Instructgeneratestheshortest
We further examine the percentage of 5-gram
outputs,around1,100tokenspergeneration.
repetitions,normalizedbythelengthofeachtext,
generatedbyourfine-tunedmodels. Asshownin
4.2 Suri-I-ORPOandSuri-SFTdonot
Figure5,thepercentageof5-gramrepetitionsdoes
degenerateintorepetitionsatlonger
notincreaseafter2,048tokens,indicatingthatour
sequences.
fine-tuned models do not exhibit degradation in
We analyze the presence of repetitions in model longersequences.
generations. Since LLMs often degrade into rep-
etitionsoverlongersequences,thismeasurement 4.3 I-ORPOimprovesrankingaccuracy
helps us identify when and how the model starts To understand the capabilities of models to dif-
producing repetitive content. Previous work (Li ferentiate between correct and corrupted instruc-
etal.,2016;Seeetal.,2019)measuresunigram,bi- tions, we evaluate ranking accuracy (See et al.,
gram,andtrigramrepetitions. However,wearein- 2019; Chen et al., 2024a). This involves measur-
terestedinsentence-levelrepetitions,suchaswhen ing the percentage of cases in which the model
thesamephraseisrepeatedinadialogueatthestart assigns a higher probability to the gold response
of each sentence. Therefore, we measure 5- and under the correct instruction than under the cor-
10-gram repetitions to capture these higher-level rupted version. We calculate the sum of token
patterns. We count a repetition when a specific log probabilities in the response given the previ-
n-gramappearsatleastthreetimesinthetext. oustokens,denotedbylogps(y|x),anddetermine
accuracy based on the proportion of times when
I- SFT Mistral- Llama- Mixtral- logps(y|x ) > logps(y|x ). A higher accuracy
w l
ORPO Instruct Instruct Instruct
indicates that the model is more sensitive to the
5-gram 24% 29% 12% 26% 31% instructionsandcandeterminewhichinstructionis
10-gram 3% 3% 1% 2% 5%
thecorrectinstructionforthegivenresponse.
We use Hugging Face’s Transformers (Wolf
Table2: Percentageofgenerationscontainingn-gram
repetitions out of 5K generations from the test set etal.,2020)toaccesstheprobabilitydistribution
(roundedtothenearestwholenumber). overvocabularyandmeasuretheimpactofinstruc-
tionspecificityonrankingaccuracyacrossfivedif-
Despitehavingthelongestgenerations,Suri-I- ferent settings, which are defined by the number
ORPO and Suri-SFT maintain a low percentage ofallconstraintsincluded(Mconstraintsintotal)andthenumberofthoseincludedconstraintsthat GPTvsHuman Percentage
are corrupted: (M,M), (M,M/2), (M,1), (M/2,M),
Agreement 39%
(1,M).Forexample,inthe(M, M/2)setting,both Partialsatisfaction-Nosatisfaction 23%
Satisfaction-Partialsatisfaction 22%
instructionsincludeallconstraints,butonlyhalfof
Satisfaction-Nosatisfaction 16%
theconstraintsareviolated.
Table4: TypesofdisagreementbetweenGPT-4oand
Instruction I- SFT Mistral- Llama- Mixtral- humanjudgeson30samplesgeneratedfromSuri-SFT.
Specificity ORPO Instruct Instruct Instruct
(M,M) 100.0 99.8 90.6 65.7 66.5
(M,M/2) 100.0 99.2 92.1 57.5 60.4 5 HumanEvaluation
(M,1) 98.3 91.0 90.4 47.7 55.2
(M/2,M) 99.9 97.8 79.7 60.0 57.4 Whileourautomaticassessmentsprovideinsights
(1,M) 98.4 81.2 62.5 50.9 48.5
intothelexicalinformationofthetext,theydonot
capture its semantic content. Therefore, we con-
Table3: RankingaccuracyontheSuritestsetacross
ductahumanevaluationtodetermineifandhow
five levels of instruction specificity. Percentages are
roundedtoonedecimalplace. theconstraintsaresatisfiedbytheoutputsofSuri-
SFTandSuri-I-ORPO.Humanevaluationon30
test set generations reveals that while both fine-
Suri-I-ORPO shows at least a 10% improve-
tunedmodelssatisfyconstraints,Suri-I-ORPOis
ment in ranking accuracy over the baseline
preferredbyhumansforitsabilitytoincorporate
Mistral-Instructacrossallinstructionspecificityset-
theconstraintsintothefinaloutputsseamlessly.
tings,withSuri-SFTfollowingclosely(Table3).
Mistral-Instructremainsastrongbaseline,achiev- 5.1 Suri-I-ORPOandSuri-SFTareeffective
ingthehighestrankingaccuracyamongthethree atsatisfyingconstraints.
baselinemodels. Incontrast,Llama-3-7b-Instruct
SinceGPT-4ojudgmentsdonotalignwithhuman
andMixtral-8x7b-Instructperformtheworst,trail-
annotations, we rely on human evaluation to de-
ingSuri-I-ORPObyupto50%. Weobservethat
termine how often Suri-I-ORPO and Suri-SFT
settings with more constraints in the instruction,
follow the given constraints. This evaluation fol-
namely(M,M),(M,M/2),and(M,1),generallylead
lows a similar setup as Section 2.3, where anno-
tobetterperformance. Thistrendsuggeststhatsee-
tators assess whether each constraint is satisfied,
ingmoreconstraintshelpsthemodelbetterdiffer-
partially satisfied, or not satisfied by the genera-
entiatebetweencorrectandcorruptedconstraints.
tions. TwoUpworkannotatorscomplete30tasks,
eachcontainingagenerationwitharound10con-
4.4 LLMjudgesareunreliableforevaluating
straints,totaling321constraints. Thegenerations
constraintsatisfactioninlong-form
arelengthy,averaging4,000words,andcomplex,
generation.
withconstraintsspreadthroughoutthetext. Anno-
We experiment with using LLMs to evaluate tatorsspendapproximately20-25minutesoneach
whether texts generated by our fine-tuned mod- annotationandarepaid$200intotalforthetask.
els follow the given constraints. Specifically, we Onaverage,Suri-I-ORPOandSuri-SFTmeet
provideGPT-4o(OpenAI,2024)withaconstraint mostoftheincludedconstraints,achievingsatisfac-
andageneratedtextfromourmodelsandprompt tionratesof67-68%andpartialsatisfactionrates
ittodeterminewhetherthetextfullysatisfies,par- of16-17%(Table5). Bothmodelshavethesame
tially satisfies, or does not satisfy the constraint proportion of unsatisfied constraints, accounting
(Table 16). We then compare these results with for16%ofthetotalconstraints. Annotatorsoften
judgments from three Upwork annotators on 30 notethatnarrativesproducedbySuri-SFTcontain
texts generated by Suri-SFT on the test set (ob- inconsistent plot events and sometimes leave the
tainedusingthesameprocedureasinSection2.3). narrative incomplete, resulting in some final con-
GPT-4o agrees with human annotators only 39% straintsnotbeingmet. Weattributethisbehavior
of the time, with a significant 16% disagreement tothefactthatsomeofthegoldresponsesaretrun-
betweensatisfactionandnosatisfaction(Table4). cated to between 2,048 and 5,024 words, which
WeconcludethatGPT-4odoesnotalignwellwith mightomittheendoftheoriginalnarrative. Onthe
long-form human annotation, consistent with the other hand, Mistral-I-ORPO produces narratives
findingsofXuetal.(2024)andKimetal.(2024). with coherent endings but can sometimes be tooSuri-SFT Suri-I-ORPO low user instructions and generate high-quality
text(Weietal.,2021;Askelletal.,2021;Ouyang
Satisfied 67% 68%
PartiallySatisfied 17% 16% et al., 2022; Liu et al., 2023; Rafailov et al.,
NotSatisfied 16% 16%
2023). Single-turninstruction-followingdatasets
can be constructed by manual annotation, where
Table5: Averagepercentageofsatisfiedconstraintsin
instruction-response pairs are curated by hu-
Suri-SFTandSuri-I-ORPOgenerations. Percentages
areroundedtothenearestwholenumber. mans (Conover et al., 2023; Rajani et al., 2023;
Zhouetal.,2024). Anotherapproachisdistillation
from proprietary LLMs, which can be done via
verbose, makingitdifficulttodeterminewhether
techniqueslikeSelf-instruct(Wangetal.,2023c)
someconstraintsaresatisfied.
to augment responses for each instruction (Taori
5.2 Suri-I-ORPOarepreferredover etal.,2023;Xuetal.,2023a,b),InstructionBack-
Suri-SFTforcoherentandinformative translation to generate instructions given gold re-
constraintsatisfaction. sponses (Köksal et al., 2023; Li et al., 2023),
or leveraging metadata to generate both instruc-
In this evaluation, we are interested in how our
tions and responses (Yin et al., 2023). While re-
fine-tunedmodelssatisfyconstraintsinSuri. We
cent work has constructed instruction-following
ask two annotators to compare text generations
datasets with long-form responses (Xiong et al.,
fromSuri-SFTandSuri-I-ORPOwithrespectto
2023;Chenetal.,2024b;Baietal.,2024)ormulti-
agivenconstraintbasedonthefollowingcriteria:
pleconstraints(Xuetal.,2024;Zhouetal.,2023;
• Informativeness: Whichgenerationprovides Malaviyaetal.,2024),nopriorefforthasexplored
moredetailsabouttheconstraint? combining these two elements in single-turn in-
structions(seeTable1). Suriisthefirstdatasetto
• Coherence: Whichgenerationeffectivelyinte-
feature both complex instructions and long-form
gratestheconstraintwiththerestofthetext?
responsesover5kwords.
• Readability/Enjoyability: Whichtextsample
Alignment Aligning language models with
iseasiertoreadoverall?
instruction-following data is crucial for ensuring
The annotators also provide detailed justifica-
that they respond to user instructions in a help-
tionsfortheirchoicesineachaspectoftheirjudg-
ful and harmless manner (Askell et al., 2021;
ments(seeFigure7).
Mishraetal.,2022;Sanhetal.,2022;Chungetal.,
2022; Wang et al., 2023b). Popular preference
Coherence Informativeness Enjoyability/Readability
tuning methods, such as RLHF, DPO, KTO, and
72% 73% 67%
ORPO(Ouyangetal.,2022;Rafailovetal.,2023;
Ethayarajhetal.,2024;Hongetal.,2024),achieve
Table6: WinrateofSuri-I-ORPOoverSuri-SFTin
thisbyfine-tuningthemodelsonhumanjudgments
termsofcoherence,enjoyability,andinformativeness.
of response quality (Kreutzer et al., 2018; Stien-
Human annotators consistently prefer Suri-I- nonetal.,2022;Ziegleretal.,2020;Ramamurthy
ORPOtoSuri-SFTforabout60-70%ofthetime etal.,2023). However,collectingpreferencesfor
across all three categories: coherence, informa- long-formresponsesischallengingduetothemany
tiveness, and enjoyability. Annotators note that competingaspectsofthetextsthatneedtobecon-
Suri-SFToftensuffersfromrepetitiveideas,con- sidered,suchasinstructionfaithfulnessandcoher-
fusingplotpoints,andalackofproperconclusions. ence(Xuetal.,2023c;Kimetal.,2024;Xuetal.,
Incontrast,whileSuri-I-ORPOtextsoccasionally 2024),whichpromptsustoexperimentwithpref-
exhibitinconsistencies, theygenerallyreadmore erencetuningoncorrectandcorrectinstructions.
naturally,includeinterestingdetails,andaredevoid
7 Conclusion
oftheroboticstructureorflowerylanguageoften
foundinotherLLMgenerations. Inthiswork,weinvestigatethechallengeofcom-
plexinstructionfollowingforgeneratinglong-form
6 RelatedWork
text. WeintroduceSuri,adatasetoflonghuman-
InstructionFollowingDatasets Open-endedin- writtenresponsesaccompaniedbybacktranslated
structiontuninginvolvesfine-tuningLLMstofol- and corrupted instructions. We demonstrate theeffectivenessofSuriinimprovingtheconstraint- enceFoundation(NSF)andanawardfromOpen
followingcapabilitiesofLLMsforlong-formgen- Philanthropy.
erationthroughsupervisedfine-tuningandI-ORPO.
Our models, as shown by both human and au-
References
tomated evaluations, generate high-quality, long-
formresponseswhilemaintainingeffectivenessat AI@Meta.2024. Llama3modelcard.
followingconstraints.
AmandaAskell,YuntaoBai,AnnaChen,DawnDrain,
DeepGanguli,TomHenighan,AndyJones,Nicholas
Limitations
Joseph, Ben Mann, Nova DasSarma, Nelson El-
hage,ZacHatfield-Dodds,DannyHernandez,Jack-
Extending to other LLMs While we demon-
son Kernion, Kamal Ndousse, Catherine Olsson,
strate the effectiveness of Suri and I-ORPO on DarioAmodei,TomBrown,JackClark,SamMcCan-
Mistral-7b-Instruct-v0.2, we have yet to experi- dlish,ChrisOlah,andJaredKaplan.2021. AGeneral
LanguageAssistantasaLaboratoryforAlignment.
mentwithfine-tuningothermodelsonourdataset,
arXivpreprint. ArXiv:2112.00861[cs].
which presents an interesting direction for future
work. Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
AmandaAskell,JacksonKernion,AndyJones,Anna
Impact of surface features on I-ORPO Even Chen, Anna Goldie, Azalia Mirhoseini, Cameron
McKinnon,CarolChen,CatherineOlsson,Christo-
though I-ORPO works well on our dataset, we
pher Olah, Danny Hernandez, Dawn Drain, Deep
would like to explore how surface features, such
Ganguli,DustinLi,EliTran-Johnson,EthanPerez,
asinstructionlengthandthedegreeofinformation Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
overlap between the instruction and response, af- Landau,KamalNdousse,KamileLukosuite,Liane
Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
fectitsperformance. Weleavethisinvestigationto
Schiefer,NoemiMercado,NovaDasSarma,Robert
futurestudies.
Lasenby, Robin Larson, Sam Ringer, Scott John-
ston,ShaunaKravec,SheerElShowk,StanislavFort,
Performanceonshort-contexttasks Addition-
TameraLanham,TimothyTelleen-Lawton,TomCon-
ally,wenotethatourdatasetprimarilyfocuseson erly,TomHenighan,TristanHume,SamuelR.Bow-
generating extremely long texts. As a result, the man,ZacHatfield-Dodds,BenMann,DarioAmodei,
NicholasJoseph,SamMcCandlish,TomBrown,and
fine-tunedmodelsmayexhibitdiminishedperfor-
JaredKaplan.2022. Constitutionalai: Harmlessness
manceontasksrequiringshortergenerations.
fromaifeedback. Preprint,arXiv:2212.08073.
EthicalConsiderations Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi,
Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li.
The risks posed by our study are no greater than 2024. LongAlign: ARecipeforLongContextAlign-
ment of Large Language Models! arXiv preprint.
those inherent in the large language models that
ArXiv:2401.18058[cs].
support it (Weidinger et al., 2021). Our human
evaluationreceivesapprovalfromaninstitutional BernhardClemm. 2023. ercexpo/us-news-domains:
reviewboard. Allannotators(US-based,fluentin v2.0.0(v2.0.0).
English) gave their informed consent and partici-
Angelica Chen, Sadhika Malladi, Lily H. Zhang,
patedwithanhourlycompensationof$16,which Xinyi Chen, Qiuyi Zhang, Rajesh Ranganath, and
meetstheminimumwageinourstate. Scientificar- KyunghyunCho.2024a. Preferencelearningalgo-
rithms do not learn preference rankings. Preprint,
tifactsareimplementedaccordingtotheirintended
arXiv:2405.19534.
usage.
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
Acknowledgements ZhijianLiu, SongHan, andJiayaJia.2024b. Lon-
glora: Efficientfine-tuningoflong-contextlargelan-
We extend special gratitude to the Upwork anno- guagemodels. Preprint,arXiv:2309.12307.
tators for their hard work, as well as to the mem-
HyungWonChung,LeHou,ShayneLongpre,Barret
bers of Unsloth, r/LocalLLaMA, and Together.ai Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi
communityforhelpfulfine-tuningadvice. Wealso Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
thankScottNiekum,DzungPham,andmembersof bert Webson, Shixiang Shane Gu, Zhuyun Dai,
MiracSuzgun,XinyunChen,AakankshaChowdh-
theUMassNLPlabfortheirinsightsontheproject.
ery,AlexCastro-Ros,MariePellat,KevinRobinson,
ThisprojectwaspartiallysupportedbyawardsIIS-
DashaValter,SharanNarang,GauravMishra,Adams
2202506andIIS-2312949fromtheNationalSci- Yu, Vincent Zhao, Yanping Huang, Andrew Dai,HongkunYu,SlavPetrov,EdH.Chi,JeffDean,Ja- Théophile Gervet, Thibaut Lavril, Thomas Wang,
cobDevlin,AdamRoberts,DennyZhou,QuocV.Le, TimothéeLacroix,andWilliamElSayed.2024. Mix-
andJasonWei.2022. Scalinginstruction-finetuned tralofexperts. Preprint,arXiv:2401.04088.
languagemodels. Preprint,arXiv:2210.11416.
Yekyung Kim, Yapei Chang, Marzena Karpinska,
TogetherComputer.2023. Redpajama: anopendataset Aparna Garimella, Varun Manjunatha, Kyle Lo,
fortraininglargelanguagemodels. TanyaGoyal,andMohitIyyer.2024. Fables:Evaluat-
ingfaithfulnessandcontentselectioninbook-length
MikeConover,MattHayes,AnkitMathur,JianweiXie, summarization. Preprint,arXiv:2404.01261.
Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,
MateiZaharia,andReynoldXin.2023. Freedolly: JuliaKreutzer,JoshuaUyheng,andStefanRiezler.2018.
Introducingtheworld’sfirsttrulyopeninstruction- Reliabilityandlearnabilityofhumanbanditfeedback
tunedllm. forsequence-to-sequencereinforcementlearning. In
Proceedings of the56th Annual Meeting of the As-
TriDao.2024. FlashAttention-2: Fasterattentionwith
sociationforComputationalLinguistics(Volume1:
better parallelism and work partitioning. In Inter-
Long Papers), pages 1777–1788, Melbourne, Aus-
national Conference on Learning Representations
tralia.AssociationforComputationalLinguistics.
(ICLR).
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff,
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Dan Jurafsky, and Douwe Kiela. 2024. Kto:
Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-
Modelalignmentasprospecttheoreticoptimization.
cientmemorymanagementforlargelanguagemodel
Preprint,arXiv:2402.01306.
servingwithpagedattention. InProceedingsofthe
ACMSIGOPS29thSymposiumonOperatingSystems
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Principles.
Hierarchical neural story generation. Preprint,
arXiv:1805.04833.
AbdullatifKöksal,TimoSchick,AnnaKorhonen,and
HinrichSchütze.2023. LongForm: OptimizingIn-
LeoGao,StellaBiderman,SidBlack,LaurenceGold-
structionTuningforLongTextGenerationwithCor-
ing, Travis Hoppe, Charles Foster, Jason Phang,
pusExtraction. arXivpreprint. ArXiv:2304.08460
Horace He, Anish Thite, Noa Nabeshima, Shawn
[cs].
Presser, and Connor Leahy. 2020. The Pile: An
800gbdatasetofdiversetextforlanguagemodeling.
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte,
arXivpreprintarXiv:2101.00027.
Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver
JianGuan,XiaoxiMao,ChangjieFan,ZitaoLiu,Wen-
Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri,
biaoDing,andMinlieHuang.2021. Longtextgener-
DavidGlushkov,ArnavDantuluri,AndrewMaguire,
ationbymodelingsentence-levelanddiscourse-level
coherence. Preprint,arXiv:2105.08963. ChristophSchuhmann,HuuNguyen,andAlexander
Mattick.2023. Openassistantconversations–democ-
JiwooHong,NoahLee,andJamesThorne.2024. Orpo: ratizinglargelanguagemodelalignment. Preprint,
Monolithic preference optimization without refer- arXiv:2304.07327.
encemodel. Preprint,arXiv:2403.07691.
HarrisonLee,SamratPhatale,HassanMansoor,Kellie
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Lu, Thomas Mesnard, Colton Bishop, Victor Car-
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and bune, and Abhinav Rastogi. 2023. Rlaif: Scaling
WeizhuChen.2021. Lora: Low-rankadaptationof reinforcementlearningfromhumanfeedbackwithai
largelanguagemodels. Preprint,arXiv:2106.09685. feedback. arXivpreprintarXiv:2309.00267.
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen- JiweiLi,MichelGalley,ChrisBrockett,JianfengGao,
sch,ChrisBamford,DevendraSinghChaplot,Diego and Bill Dolan. 2016. A diversity-promoting ob-
delasCasas,FlorianBressand,GiannaLengyel,Guil- jectivefunctionforneuralconversationmodels. In
laumeLample,LucileSaulnier,LélioRenardLavaud, Proceedings of the 2016 Conference of the North
Marie-AnneLachaux,PierreStock,TevenLeScao, AmericanChapteroftheAssociationforComputa-
Thibaut Lavril, Thomas Wang, Timothée Lacroix, tionalLinguistics: HumanLanguageTechnologies,
andWilliamElSayed.2023. Mistral7b. Preprint, pages110–119,SanDiego,California.Association
arXiv:2310.06825. forComputationalLinguistics.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine XianLi,PingYu,ChuntingZhou,TimoSchick,Luke
Roux, Arthur Mensch, Blanche Savary, Chris Zettlemoyer, Omer Levy, Jason Weston, and Mike
Bamford, Devendra Singh Chaplot, Diego de las Lewis.2023. Self-alignmentwithinstructionback-
Casas, Emma Bou Hanna, Florian Bressand, Gi- translation. arXivpreprintarXiv:2308.06259.
anna Lengyel, Guillaume Bour, Guillaume Lam-
ple, Lélio Renard Lavaud, Lucile Saulnier, Marie- HaoLiu,CarmeloSferrazza,andPieterAbbeel.2023.
AnneLachaux,PierreStock,SandeepSubramanian, Chainofhindsightalignslanguagemodelswithfeed-
Sophia Yang, Szymon Antoniak, Teven Le Scao, back. Preprint,arXiv:2302.02676.Chaitanya Malaviya, Priyanka Agrawal, Kuzman 26th ACM SIGKDD International Conference on
Ganchev,PraneshSrinivasan,FantineHuot,Jonathan Knowledge Discovery & Data Mining, KDD ’20,
Berant, Mark Yatskar, Dipanjan Das, Mirella Lap- page3505–3506,NewYork,NY,USA.Association
ata, and Chris Alberti. 2024. Dolomites: Domain- forComputingMachinery.
specificlong-formmethodicaltasks. InarXiv.
VictorSanh,AlbertWebson,ColinRaffel,StephenH.
Sourab Mangrulkar, Sylvain Gugger, Lysandre De- Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
but, Younes Belkada, Sayak Paul, and Benjamin Chaffin,ArnaudStiegler,TevenLeScao,ArunRaja,
Bossan. 2022. Peft: State-of-the-art parameter- Manan Dey, M Saiful Bari, Canwen Xu, Urmish
efficient fine-tuning methods. https://github. Thakker,ShanyaSharmaSharma,ElizaSzczechla,
com/huggingface/peft. TaewoonKim,GunjanChhablani,NihalNayak,De-
bajyotiDatta,JonathanChang,MikeTian-JianJiang,
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
HanWang,MatteoManica,ShengShen,ZhengXin
Hannaneh Hajishirzi. 2022. Cross-task generaliza-
Yong, Harshit Pandey, Rachel Bawden, Thomas
tionvianaturallanguagecrowdsourcinginstructions.
Wang,TrishalaNeeraj,JosRozen,AbheeshtSharma,
Preprint,arXiv:2104.08773.
Andrea Santilli, Thibault Fevry, Jason Alan Fries,
RyanTeehan,TaliBers,StellaBiderman,LeoGao,
NasrinMostafazadeh,NathanaelChambers,Xiaodong
ThomasWolf,andAlexanderM.Rush.2022. Multi-
He,DeviParikh,DhruvBatra,LucyVanderwende,
taskpromptedtrainingenableszero-shottaskgener-
PushmeetKohli,andJamesAllen.2016. Acorpus
alization. Preprint,arXiv:2110.08207.
and cloze evaluation for deeper understanding of
commonsense stories. In Proceedings of the 2016
Abigail See, Aneesh Pappu, Rohun Saxena, Akhila
Conference of the North American Chapter of the
Yerukola, and Christopher D. Manning. 2019. Do
AssociationforComputationalLinguistics: Human
massivelypretrainedlanguagemodelsmakebetter
LanguageTechnologies,pages839–849,SanDiego,
storytellers? Preprint,arXiv:1909.10705.
California.AssociationforComputationalLinguis-
tics. Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori
Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,
OpenAI.2024. Modelreleaseblog: GPT-4o. Technical
MorGeva,JonathanBerant,andOmerLevy.2022.
report,OpenAI. Accessed: 2024-05-23.
Scrolls:Standardizedcomparisonoverlonglanguage
sequences. Preprint,arXiv:2201.03533.
LongOuyang,JeffWu,XuJiang,DiogoAlmeida,Car-
rollL.Wainwright,PamelaMishkin,ChongZhang,
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.
SandhiniAgarwal,KatarinaSlama,AlexRay,John
Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Schulman,JacobHilton,FraserKelton,LukeMiller,
Dario Amodei, and Paul Christiano. 2022. Learn-
Maddie Simens, Amanda Askell, Peter Welinder,
ingtosummarizefromhumanfeedback. Preprint,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
arXiv:2009.01325.
Traininglanguagemodelstofollowinstructionswith
humanfeedback. arXivpreprint. ArXiv:2203.02155
Simeng Sun, Katherine Thai, and Mohit Iyyer. 2022.
[cs].
Chapterbreak: A challenge dataset for long-range
languagemodels. arXivpreprintarXiv:2204.10878.
ShawnPresser.2020. Books3.
RafaelRafailov,ArchitSharma,EricMitchell,Stefano Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Ermon,ChristopherD.Manning,andChelseaFinn. Dubois,XuechenLi,CarlosGuestrin,PercyLiang,
2023. Direct preference optimization: Your lan- andTatsunoriB.Hashimoto.2023. Stanfordalpaca:
guage model is secretly a reward model. Preprint, An instruction-following llama model. https://
arXiv:2305.18290.
github.com/tatsu-lab/stanford_alpaca.
Nazneen Rajani, Lewis Tunstall, Edward Beeching, The Association of Religion Data Archives. 2023.
NathanLambert,AlexanderM.Rush,andThomas Religion dictionary. https://www.thearda.
Wolf.2023. Norobots. https://huggingface.co/ com/research/religion-dictionary. Accessed:
datasets/HuggingFaceH4/no_robots. 2024/01/15.
Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
KiantéBrantley, JackHessel, RafetSifa, Christian bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
2023. Isreinforcementlearning(not)fornaturallan- Bhosale, et al. 2023. Llama 2: Open founda-
guageprocessing: Benchmarks,baselines,andbuild- tion and fine-tuned chat models. arXiv preprint
ingblocksfornaturallanguagepolicyoptimization. arXiv:2307.09288.
Preprint,arXiv:2210.01241.
Lewis Tunstall, Edward Beeching, Nathan Lambert,
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Nazneen Rajani, Shengyi Huang, Kashif Rasul,
and Yuxiong He. 2020. Deepspeed: System opti- Alexander M. Rush, and Thomas Wolf. 2023.
mizationsenabletrainingdeeplearningmodelswith The alignment handbook. https://github.com/
over100billionparameters. InProceedingsofthe huggingface/alignment-handbook.Leandro von Werra, Younes Belkada, Lewis Tun- JoeDavison,SamShleifer,PatrickvonPlaten,Clara
stall, Edward Beeching, Tristan Thrush, Nathan Ma,YacineJernite,JulienPlu,CanwenXu,TevenLe
Lambert, and Shengyi Huang. 2020. Trl: Trans- Scao, Sylvain Gugger, Mariama Drame, Quentin
former reinforcement learning. https://github. Lhoest,andAlexanderM.Rush.2020. Transform-
com/huggingface/trl. ers: State-of-the-artnaturallanguageprocessing. In
Proceedings of the 2020 Conference on Empirical
RoseEWang,EsinDurmus,NoahGoodman,andTat- Methods in Natural Language Processing: System
sunoriHashimoto.2023a. Languagemodelingvia Demonstrations,pages38–45,Online.Association
stochasticprocesses. Preprint,arXiv:2203.11370. forComputationalLinguistics.
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack
WenhanXiong,JingyuLiu,IgorMolybog,HejiaZhang,
Hessel, Tushar Khot, Khyathi Raghavi Chandu,
Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi
DavidWadden,KelseyMacMillan,NoahA.Smith,
Rungta,KarthikAbinavSankararaman,BarlasOguz,
IzBeltagy,andHannanehHajishirzi.2023b. Howfar
MadianKhabsa,HanFang,YasharMehdad,Sharan
cancamelsgo? exploringthestateofinstructiontun-
Narang,KshitizMalik,AngelaFan,ShrutiBhosale,
ingonopenresources. Preprint,arXiv:2306.04751.
SergeyEdunov,MikeLewis,SinongWang,andHao
Ma.2023. EffectiveLong-ContextScalingofFoun-
YizhongWang,YeganehKordi,SwaroopMishra,Alisa
dationModels. arXivpreprint. ArXiv:2309.16039
Liu,NoahA.Smith,DanielKhashabi,andHannaneh
[cs].
Hajishirzi.2023c. Self-instruct: Aligninglanguage
models with self-generated instructions. Preprint,
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
arXiv:2212.10560.
Pu Zhao, Jiazhan Feng, Chongyang Tao, and
DaxinJiang.2023a. Wizardlm: Empoweringlarge
Yizhong Wang, Swaroop Mishra, Pegah Alipoor-
language models to follow complex instructions.
molabashi, Yeganeh Kordi, Amirreza Mirzaei,
Preprint,arXiv:2304.12244.
Anjana Arunkumar, Arjun Ashok, Arut Selvan
Dhanasekaran, Atharva Naik, David Stap, Eshaan
CanwenXu,DayaGuo,NanDuan,andJulianMcAuley.
Pathak,GiannisKaramanolakis,HaizhiGaryLai,Is-
2023b. Baize: An open-source chat model with
hanPurohit,IshaniMondal,JacobAnderson,Kirby
parameter-efficient tuning on self-chat data. arXiv
Kuznia, Krima Doshi, Maitreya Patel, Kuntal Ku-
preprintarXiv:2304.01196.
mar Pal, Mehrad Moradshahi, Mihir Parmar, Mi-
raliPurohit,NeerajVarshney,PhaniRohithaKaza, FangyuanXu, KyleLo, LucaSoldaini, BaileyKuehl,
PulkitVerma,RavsehajSinghPuri,RushangKaria, Eunsol Choi, and David Wadden. 2024. KIWI: A
Shailaja Keyur Sampat, Savan Doshi, Siddhartha DatasetofKnowledge-IntensiveWritingInstructions
Mishra, SujanReddy, SumantaPatro, TanayDixit, forAnsweringResearchQuestions. arXivpreprint.
Xudong Shen, Chitta Baral, Yejin Choi, Noah A. ArXiv:2403.03866[cs].
Smith, Hannaneh Hajishirzi, and Daniel Khashabi.
2022. Super-naturalinstructions: Generalizationvia Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eun-
declarativeinstructionson1600+nlptasks. Preprint, sol Choi. 2023c. A critical evaluation of evalua-
arXiv:2204.07705. tions for long-form question answering. Preprint,
arXiv:2305.18201.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An- DaYin,XiaoLiu,FanYin,MingZhong,HritikBansal,
drewMDai,andQuocVLe.2021. Finetunedlan- JiaweiHan,andKai-WeiChang.2023. Dynosaur: A
guagemodelsarezero-shotlearners. arXivpreprint dynamicgrowthparadigmforinstruction-tuningdata
arXiv:2109.01652. curation. arXivpreprintarXiv:2305.14327.
JasonWei, MaartenBosma, VincentY.Zhao, Kelvin ChuntingZhou,PengfeiLiu,PuxinXu,SrinivasanIyer,
Guu, Adams Wei Yu, Brian Lester, Nan Du, An- JiaoSun,YuningMao,XuezheMa,AviaEfrat,Ping
drew M. Dai, and Quoc V. Le. 2022. Finetuned Yu,LiliYu,etal.2024. Lima: Lessismoreforalign-
language models are zero-shot learners. Preprint, ment. AdvancesinNeuralInformationProcessing
arXiv:2109.01652. Systems,36.
LauraWeidinger,JohnMellor,MaribethRauh,Conor Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan
Griffin, Jonathan Uesato, Po-Sen Huang, Myra Wilcox,RyanCotterell,andMrinmayaSachan.2023.
Cheng,MiaGlaese,BorjaBalle,AtoosaKasirzadeh, Controlledtextgenerationwithnaturallanguagein-
Zac Kenton, Sasha Brown, Will Hawkins, Tom structions. Preprint,arXiv:2304.14293.
Stepleton, Courtney Biles, Abeba Birhane, Julia
Haas,LauraRimell,LisaAnneHendricks,William DanielM.Ziegler,NisanStiennon,JeffreyWu,TomB.
Isaac, Sean Legassick, Geoffrey Irving, and Iason Brown, Alec Radford, Dario Amodei, Paul Chris-
Gabriel.2021. Ethicalandsocialrisksofharmfrom tiano, and Geoffrey Irving. 2020. Fine-tuning lan-
languagemodels. Preprint,arXiv:2112.04359. guage models from human preferences. Preprint,
arXiv:1909.08593.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi,Pier-
ricCistac,TimRault,RémiLouf,MorganFuntowicz,A QualityFiltersforRedPajama-Data-v2
Upon initial examination, we observe a significant presence of news and religious text in the corpus.
Therefore,inadditiontothefollowingqualityfilters,wealsodownsamplenewsandreligiousarticlesby
excludinganyarticlecontainingasourcedomainonourblocklist(BernhardClemm,2023)ormorethan
0.05%ofwordsfromareligiousdictionary(TheAssociationofReligionDataArchives,2023)toensure
thediversityofthegoldresponses. Table7and8showthequalityfiltersusedinRedPajama-Data-v2.
Tags Values Descriptions Categories
ccnet_language_score > 0.65 score of the language identification CCNet
model
ccnet_perplexity (35,350) perplexity of an LM trained on CCNet
Wikipedia
rps_doc_books_importance > 0 Givenabagof{1,2}-wordgrammodel ML Heuris-
trainedonBooksp,andamodeltrained tics
onthesourcedomainq,Thisisthelog-
arithmoftheratiop(doc)/q(doc).
rps_doc_curly_bracket 0 The ratio between the number of oc- Natural Lan-
currencesof’{’or’}’andthenumber guage
of characters in the raw text. Some
pages inadvertently contained code.
Since the curly bracket, "{" appears
inmanyprogramminglanguages(such
asJavascript,widelyusedontheweb)
butnotinnaturaltext,weremovedany
pagesthatcontainedacurlybracket.
rps_doc_frac_no_alph_words 0.3 The fraction of words that contain no Natural Lan-
alphabeticalcharacter. guage
rps_doc_lorem_ipsum 0 Theratiobetweenthenumberofoccur- Natural Lan-
rencesof’loremipsum’andthenumber guage
of characters in the content after nor-
malisation.
rps_doc_unigram_entropy ≥ 3 Theentropyoftheunigramdistribution Natural Lan-
ofthecontent. Thismeasuresthediver- guage
sityofthecontentandiscomputedus-
ingsum(-x/total*log(x/total))where
thesumistakenovercountsofunique
wordsinthenormalisedcontent.
Table7: QualitySignalsusedtofilterRedPajamasDataset-Part1Tags Values Descriptions Categories
rps_doc_word_count (2048,5024) Thenumberofwordsinthecontent Natural Lan-
afternormalisation. guage
rps_lines_javascript_counts 0 The number of occurrences of the Natural Lan-
word"javascript"ineachline. Many guage
ofthescrapedpagescontainedwarn-
ingsstatingthatJavascriptshouldbe
enabledsoweremovedanylinewith
thewordJavascript.
rps_doc_frac_chars_dupe_10grams 0.1 Thefractionofcharactersinduplicate
Repetitiveness
rps_doc_frac_chars_dupe_5grams 0.15 wordngrams.
rps_doc_frac_chars_dupe_6grams 0.14
rps_doc_frac_chars_dupe_7grams 0.13
rps_doc_frac_chars_dupe_8grams 0.12
rps_doc_frac_chars_dupe_9grams 0.11
rps_doc_frac_chars_top_2gram 0.2
rps_doc_frac_chars_top_3gram 0.18
rps_doc_frac_chars_top_4gram 0.16
rps_doc_ldnoobw_words 0 The number of sequences of Sensitive /
words that are contained in toxiccontent
the List-of-Dirty-Naughty-
Obscene-and-Otherwise-Bad-
Words blocklist. The block-
list is obtained from https:
//github.com/LDNOOBW/List-
of-Dirty-Naughty-Obscene-and-
Otherwise-Bad-Words
rps_doc_ut1_blacklist 0 A categorical id corresponding Sensitive /
to the list of categories of the toxiccontent
domain of the document. Cate-
gories are obtained from the UT1
blacklist. The list is obtained from
https://dsi.ut-capitole.fr/
blacklists/: [’adults’,’phishing’,
’dating’, ’gambling’, ’filehosting’,
’aggressif’, ’ddos’, ’mixed_adult’,
’chat’,’arjel’]
Table8: QualitySignalsusedtofilterRedPajamasDataset-Part2B Prompts
Inthissection,weshowpromptstogenerateandanalyzeSuriinTable9,10,11,12. Table16showsthe
promptusedforourexperimentwithLLMjudges.
Prompt:InstructionBacktranslation/Reverse-engineering
Assumetheauthoroftheprovidedtextfollowedadetailedsetofinstructionstoproducetheirwork.Yourtaskistoinferwhat
thoseoriginalinstructionsmayhavebeenbycomposingyourownsetofinstructionsthatcouldrecreatekeyaspectsofthegiven
text.
Yourresponsemustinclude:
1. Anoverarchinginstructionunderthe"MainInstruction"sectionthatsummarizesthegoaloftheinstructions.
2. Onebulletedlistofspecificconstraintsunderthe"Constraints"sectionthatreflecttheorderofhappenings/ideasinthe
originaltext.Constraintsshouldfocusoneitherstylisticelements(howsomethingiscommunicatedthroughtone,language,
sentencestructure),semanticelements(whattopics,meanings,andconceptsareincluded),oracombinationofboth.You
shouldincludespecificelementsfromthetext,butavoidusingdirectquotes.Aimforafairbalanceofsemantic,stylistic
andmixedconstraints.
• Examplesofstylisticconstraintsare"incorporatehumorwhendiscussingserioustopics"or"useshort,choppy
sentencesforemphasis."
• Examplesofsemanticconstraintsare"describeasupportivemotherandabsentfather"or"mentionanimpressionist
paintingwithaleopard."
• Mixedconstraintsblendstylisticandsemanticelements,like"discussimpressionistartwithanenthusiastictone."
###Document:
{text}
###Yourresponse:
Table 9: Prompt to reverse-engineer/backtranslate instructions. The placeholder {text} will be replaced with
collectedgoldresponses. Ourinstructionbacktranslationexperimentcost≈$2KUSdollars.Prompt:Corruptbacktranslatedinstructions
Youaregivenaninstructiontextthatincludesamaininstructionandalistofconstraints.Yourtaskistomakeminimaleditsto
violateeachconstraint.Yourresultingconstraintsshouldbecoherentwithoneanotherandalsowiththemaininstruction.
[Examples]
MainInstruction:WriteastoryonthelifeanddeathofBob,whoisarun-of-the-millblue-collarworkerinTexas,USA.
Constraints:
• Useafirst-personperspectivethatcentersontheprotagonist’sperspective.→Useathird-personperspectivethatensures
abroadandneutralviewofthenarrative.
• Includecliffhangersattheendofeachchaptertoencouragereaderstocontinuereading.→Donotincludecliffhangersat
theendofeachchaptertoencouragesmoothreadings.
[ProvidedInstruction]
{instructions}
Whenmodifyingtheconstraints,keepthefollowinginmind:
1. Ensurethatyourresultingconstraintsarecoherentwithoneanotherandalsowiththemaininstruction. However,the
originalandmodifiedconstraintsshouldbemutuallyexclusiveanddifficulttoachievesimultaneously.
2. Modifyeveryconstraint,butleavethemaininstructionunchanged.
3. Yourresponseshouldcontaintheoriginalmaininstruction,followedbyeachoriginalconstraintandyourminimally
modifiedversion.Formateachconstraintas:Originalconstraint→Yourmodifiedconstraint.
[Yourresponse]
Table10: Promptusedtoviolatebacktranslatedinstructions. Theplaceholder{instructions}arereplacedwith
instructionsthatareproducedwithPrompt9.
Prompt:Assignconstrainttype(semantic,stylistic,mixed)toeachconstraint
Youareahelpfulassistant.Youaregivenaconstraintthatyouneedtodetermineifitisastylistic,semantic,ormixedconstraint.
Stylisticconstraintemphasizesstylisticelements(howsomethingiscommunicatedthroughtone,language,sentencestructure).
Stylisticconstraintsfocusonsemanticelements(whattopics,meanings,andconceptsareincluded).Mixedconstraintsinclude
bothstylisticandsemanticelements.
###Examples:
Constraint:Incorporatehumorwhendiscussingthemorbid,gut-wrenchingsceneoftheprotagonist’sdeath.Useshort,choppy
sentencestocreateasenseofurgencyandpanic.
Yourresponse:Stylistic
Constraint:Thestorymustendwiththeprotagonist’sdeathinacaraccident.
Yourresponse:Semantic
Constraint:Usingafirst-personperspective,writeastoryonthelifeanddeathofBob,ablue-collarworkerinTexas,USA.
Yourresponse:Mixed
Constraint:Includecliffhangersattheendofeachchaptertoencouragereaderstocontinuereading.
Yourresponse:Stylistic
###Constraints:
Constraint:{constraint}
###Yourresponse:
Table 11: Prompt to assign constraint type (semantic, stylistic, mixed) to each constraint. The placeholder
{constraint}willbereplacedwithasingleconstraintineachbacktranslatedinstruction.C ModelingExperimentDetails
AllexperimentsaredoneusingFlash-Attention2(Dao,2024),DeepSpeedZeRO3(Rasleyetal.,2020),
PEFT(Mangrulkaretal.,2022),TRLlibrary(vonWerraetal.,2020),andAlignmentHandbook(Tunstall
etal.,2023). Chattemplatesareasfollows:
1 <|user|>
2 {Instruction}</s>
3
4 <|assistant|>
5 {Response}</s>
Thetrainingconfigurations(Table13)aremostlysimilarforSFTandORPO.Wevarythelearning
rate (5e-4 to 5e-7), optimizer (8-bit vs. 32-bit), LoRA rank, and alpha (8 to 64), but none of these
hyperparametersresultsinbettergenerations.D I-ORPOLossDerivation
ThederivationofL closelyresemblesthatoftheoriginalORPOloss,withd = (x ,x ,y) ∼ D.
I-OR w l
∇ L = δ(d)·h(d) (4)
θ I-OR
(cid:18)
odds (y|x
)(cid:19)−1
θ w
δ(d) = 1+ (5)
odds (y|x )
θ l
∇ logP (y|x ) ∇ logP (y|x )
θ θ w θ θ l
h(d) = − (6)
1−P (y|x ) 1−P (y|x )
θ w θ l
The gradient of L is the product of two terms: δ(d), which regulates the strength of parameter
I-OR
updates,andh(d),whichwidensthecontrastbetweenlogps(y|x )andlogps(y|x ). Specifically,asthe
w l
oddsratioincreases,δ(d)convergesto0. Ontheotherhand,h(d)hastwogradients: ∇ logP (y|x ),
θ θ w
whichminimizeslogP (y|x ),and∇ logP (y|x ),whichmaximizeslogP (y|x ). Additionally,1−
θ w θ θ l θ l
P (y|x )acceleratestheupdateinthedirectionthatmaximizesP (y|x ). FollowingORPO(Hongetal.,
θ w θ w
2024),supposethatg(x ,x ,y) =
oddsθ(y|xw)
,wederivethelossasin22.
w l oddsθ(y|x l)(cid:18) (cid:18) (cid:19)(cid:19)
odds (y|x )
θ w
∇ L = ∇ logσ log (7)
θ I−OR θ
odds y|x )
θ l
1
= ·∇ σ(logg(x ,x ,y)) (8)
θ w l
σ(logg(x ,x ,y))
w l
1
= ·σ(logg(x ,x ,y))(1−σ(logg(x ,x ,y)))∇ logg(x ,x ,y) (9)
w l w l θ w l
σ(logg(x ,x ,y))
w l
= (1−σ(logg(x ,x ,y)))·∇ logg(x ,x ,y) (10)
w l θ w l
= σ(−logg(x ,x ,y))·∇ logg(x ,x ,y) (11)
w l θ w l
(cid:18)
odds (y|x
)(cid:19)−1
odds (y|x )
θ w θ w
= 1+ ·∇ log (12)
θ
odds (y|x ) odds (y|x )
θ l θ l
(cid:18)
odds (y|x
)(cid:19)−1 (cid:18)
P(y|x ) 1−P(y|x
)(cid:19)
θ w w l
= 1+ ·∇ log (13)
θ
odds (y|x ) 1−P(y|x ) P(y|x )
θ l w l
(cid:16) (cid:17)
∇log
P(y|xw) 1−P(y|x l)
canberewrittenas:
1−P(y|xw) P(y|x l)
(cid:18) (cid:19)
P(y|x ) 1−P(y|x )
w l
= ∇ log (14)
θ
P(y|x ) 1−P(y|x )
l w
(cid:18) (cid:19)
P(y|x ) 1−P(y|x )
w l
= ∇ log (15)
θ
P(y|x ) 1−P(y|x )
l w
P(y|x )
w
= ∇ log −(∇ log(1−P (y|x ))−∇ log(1−P (y|x ))) (16)
θ θ θ w θ θ l
P(y|x )
l
(cid:18) (cid:19)
P(y|x ) ∇ (1−P (y|x )) ∇ (1−P (y|x ))
w θ θ w θ θ l
= ∇ log − − (17)
θ
P(y|x ) 1−P (y|x ) 1−P (y|x )
l θ w θ l
(cid:18) (cid:19)
P(y|x ) −∇ (P (y|x )) −∇ (P (y|x ))
w θ θ w θ θ l
= ∇ log − − (18)
θ
P(y|x ) 1−P (y|x ) 1−P (y|x )
l θ w θ l
(cid:18) (cid:19)
P(y|x ) −P (y|x )∇ logP (y|x ) −P (y|x )∇ logP (y|x )
w θ w θ θ w θ l θ θ l
= ∇ log − − (19)
θ
P(y|x ) 1−P (y|x ) 1−P (y|x )
l θ w θ l
P(y|x )
w
= ∇ log −(−odds (y|x )·∇ logP (y|x )+odds (y|x )·∇ logP (y|x )) (20)
θ θ w θ θ w θ l θ θ l
P(y|x )
l
= ∇ logP (y|x )(1+odds (y|x ))−∇ logP (y|x )(1+odds (y|x )) (21)
θ θ w θ w θ θ l θ l
Thefinalequationis:
(cid:18)
odds (y|x
)(cid:19)−1
θ w
∇ L = 1+ ·(∇ logP (y|x )(1+odds (y|x ))−
θ I−OR θ θ w θ w
odds (y|x ) (22)
θ l
∇ logP (y|x )(1+odds (y|x )))
θ θ l θ l
1+odds (y|x ) 1+odds (y|x )
θ w θ l
= ·∇ logP (y|x )− ·∇ logP (y|x ) (23)
θ θ w θ θ l
1+
oddsθ(y|xw)
1+
oddsθ(y|xw)
oddsθ(y|x l) oddsθ(y|x l)
(cid:18)
odds (y|x
)(cid:19)−1 (cid:18)
∇ logP (y|x ) ∇ logP (y|x
)(cid:19)
θ w θ θ w θ θ l
= 1+ · − (24)
odds (y|x ) 1−P (y|x ) 1−P (y|x )
θ l θ w θ lE PreferencePrompting
Inthisevaluation,weprovidethemodelwiththegoldresponsey andbothinstructionsx andx . We
w l
thenpromptthemodeltochoosetheinstructionmostrelevanttothegoldtext,followingBaietal.(2022)
and Lee et al. (2023). The model should output ‘1’ if the first instruction generates the text and ‘2’
otherwise(Table14). Next,wecomparethelogprobabilitiesofthemodeloutputting‘1’and‘2’. Ifthelog
probabilityfor‘1’ishigher,weassumethemodelpreferswhicheverinstructioncamefirstintheprompt.
Theperformancemetricisdeterminedbyhowoftenthemodelprefersthecorrectinstruction,regardless
oftheorderinwhichthecorrectinstructionispresented. WeexperimentwithMistral-7b-Instruct-v0.2,
Suri-I-ORPO, Suri-SFT, Mixtral-8x7b-Instruct-v0.1, Llama-3-7b-Instruct. All experiments use the
Huggingfaceimplementationwithgreedydecoding.
Weobservethatallmodelssufferfrom“firstinstructionbias",wherethemodelalwaysoutputsthefirst
instructionasthecorrectinstruction,regardlessofwhetherthatinstructionisactuallyx ornot.
wF HumanEvaluation
F.1 Recruitment
Werecruithumanannotators,allofwhomarefluentinEnglish,fromUpwork(https://www.upwork.
com)forourhumanevaluation. Eachtaskisassignedtotwoannotators,exceptforInstructionValidation,
which involves three annotators. Annotators are compensated at a rate of $16 per hour and generally
workanaverageof12hourspertask. Allannotatorshavesignedconsentforms,andourstudyhasbeen
approvedbyourinstitutionalreviewboards(IRB).
F.2 Annotation
Figure6showstheLabelStudiointerfaceforannotatinginstructionvalidity/constraintsatisfaction. Figure
7 features the interface for comparing text generations based on how they satisfy a given constraint.
Annotatorsnotethattheinterfacesareuser-friendly.
Figure6: LabelStudiointerfaceforannotatingthevalidityofinstructions. Annotatorsbeginbycarefullyreading
throughtheprovidedconstraintandhighlightingalltherelevanttextspansintheresponsesupportingtheconstraint
specifiedintheinstruction. Theythenindicatewhetherthehighlightedtextsatisfiesthegivenconstraintinthe
follow-upquestion.
F.3 Annotatoragreementintheinstructionvalidityandconstraintsatisfactionevaluation
WenotethatKrippendorff’sAlpharemainslowacrossevaluationtasks,suggestinglittletonoagreement
amongtheannotators. Weattributethispatterntothefactthatourgenerationsarelong(≈4kwordson
average),makingithardforannotatorstofollowthenarrativesometimes. Finalstatisticsreportedinthe
paperisaveragedbetweentheannotators.
Table 15 further shows disagreement types for the instruction validity and constraint satisfaction
evaluation.Figure7: LabelStudiointerfaceforcomparinggeneratedtext. Annotatorsbeginbycarefullyreadingthroughthe
provided constraint. They then highlight all the relevant text spans in the response that support the constraint
specified in the instruction. After that, annotators answer questions on the informativeness, enjoyability, and
coherenceoftheprovidedtexts. Weshufflethegenerationsineachtasktopreventbias.G GenerationsfromMistral-Instruct,Suri-SFT,andSuri-ORPO
WeshowgenerationsfromMistral-Instruct,Suri-SFT,andSuri-ORPOinTable17.Prompt:Assignconstraintscope(broad,specific)toeachconstraint
Youareahelpfulassistant.Youaregivenaconstraintthatyouneedtodetermineifitisaspecificorbroadconstraint.Specific
constraintsfocusonanelementthatcanbefoundinaspecificpartofthetext.Broadconstraintsfocusonanelementthatcanbe
foundthroughoutthetext.
###Examples:
Constraint:Throughoutthenarrative,useafirst-personperspectivethatcentersontheprotagonist’sperspective.
Yourresponse:Broad
Constraint:Includecliffhangersattheendofthefirstchaptertoencouragereaderstocontinuereading.
Yourresponse:Specific
Constraint:Introduceanewcharacterinthemiddleofthestorytoadddepthtothenarrative.
Yourresponse:Specific
Constraint:Includecliffhangersattheendofeachchaptertoencouragereaderstocontinuereading.
Yourresponse:Broad
###Constraints:
Constraint:{x constraint}
w
###Yourresponse:
Table12: Prompttoassignconstraintscope(broad/specific)toeachconstraint. Theplaceholder{x constraint}is
w
replacedwithasingleconstraintfromeachbacktranslatedinstruction.
Configurations Values
Hardware(TrainingandInference) 4xA100s
Tracking wandb
lora_r 16
lora_alpha 16
lora_dropout 0.05
beta(forORPOonly) 0.4
gradient_accumulation_steps 1
gradient_checkpointing True
learning_rate 5.0e-5
lr_scheduler_type cosine
max_length 15024
max_completion_length 15000
max_prompt_length 5000
num_train_epochs 2
optim adamw_torch
per_device_train_batch_size 1
Table13: TrainingdetailsforSFTandORPOPrompt:p(preference|prompt)evaluation
Youareanexpertinstructionrater.Youwillbegivenatextandtwoinstructions,oneofwhichisusedtogeneratethetext.Read
throughthetextcarefully,thendeterminewhichofthetwoinstructionswasusedtogeneratethetext.Answeronlywith"1"if
thefirstinstructioniscorrect,or"2"ifthesecondinstructioniscorrect.DONOTgiveanyreasoning.
###Text:
{text}
###FirstInstruction:
{ins1}
###SecondInstruction:
{ins2}
Whichinstructioniscorrect?Answeronlywith"1"ifthefirstinstructioniscorrect,or"2"ifthesecondinstructioniscorrect.
DONOTgiveanyreasoning.
Yourresponse:
Table14: Promptusedinthep(preference|prompt)evaluation. The{text}placeholderisreplacedwithgold
responses, while the placeholders {ins1} and {ins2} are replaced with the correct and corrupted instructions,
respectively. Tomitigateanypotentialorderingbias,theorderofthecorrectandcorruptedinstructionsisshuffled.
Wewillconsideraresponsecorrectonlyifthemodelchoosesthecorrectinstruction,regardlessoftheordering.
Types Krippendorff’s Satisfied vs PartiallySatis- Satisfied vs
alpha Partially fiedvsNotSat- NotSatisfied
Satisfied isfied
InstructionValidation(Section2.3) 0.1 0.30 0.03 0.0
ConstraintSatisfaction-Suri-SFT(Section5) 0.0 0.52 0.24 0.24
ConstraintSatisfaction-Suri-I-ORPO(Section5) 0.2 0.60 0.34 0.06
I-ORPOvsSFT-Coherence(Section5) 0.0 - - -
I-ORPOvsSFT-Informativeness(Section5) 0.0 - - -
I-ORPOvsSFT-Enjoyability(Section5) 0.1 - - -
Table15: Typesofdisagreementamongannotatorsintheinstructionvalidationandconstraintsatisfactiontasks.
Mostdisagreementsariseoverwhetherthetextfullyorpartiallysatisfiestheconstraints.Prompt:LLMevaluation
Youwillbegivenatextanditscorrespondinginstruction,whichcontainsthetext’smaingoalandaconstraint. Determine
whetherthetextsatisfiestheconstraint(notthemaingoal).Youshouldreturnyouranswer(Yes/No/Partially)alongwithyour
reasoningandaquoteinthetextthatsupportsyourreasoning(thequoteshouldnotcontainanydoublequotationmarks).Your
answershouldcontain3fields: "answer","reasoning",and"quote". DONOToutputanythingelseotherthantheresponse,
whichstartswith"«"andendingwith"»".
#Example1:Thetextsatisfiestheconstraint.
-Maingoal:Writeafirst-personnarrativedescribingaserenemorninginaremotevillage.
-Constraint:Youmustnotusetheletter’e’.
-Text:Dawncrackswithayawn.Onahill,ahutsits,tranquil.Brightlightclimbs,castinggoldongrass.Inthiscalmmorning,
airiscool,birdssingsoftly.Istrolldownpaths,sippinghotcocoa,watchingdaystart.Suchisthisdawn’scharm,liftingspirits,
asworldawakens.
-Yourresponse:«"answer":"Yes","reasoning":"Thetextdoesnotcontainany’e’,whichsatisfiestheconstraint.","quote":
"Dawncrackswithayawn..."»
#Example2:Thetextdoesnotsatisfytheconstraint.
-Maingoal:Composeanarrativethattakesplaceentirelywithintheconfinesofasingle,smallroom.
-Constraint:Thestorymustnotincludeanydirectinteractionorcommunicationwithothercharacters,whetherthroughdialogue,
notes,oranyformofdigitalcommunication.
-Text:Sarahsatquietlyinthecornerofthesmall,dimlylitlibraryroom,surroundedbytoweringbookshelvesfilledwithdusty
volumes.Herfocuswasbrokenbyasoftknockonthedoor."Sarah,areyouthere?"herfriendEmily’svoicecalledoutgently
fromtheotherside.Sarah,startledyetrelievedtohearafamiliarvoice,responded,"Yes,I’mhere,Emily.Justgivemeamoment,
I’llopenthedoor."TheyspentthenexthourtalkingaboutthebooksSarahhadbeenreadingandtheirplansfortheweekend,
makingthesmallroomfeelalotlesslonely.
-Yourresponse:«"answer":"No","reasoning":"ThetextincludesadialoguebetweenSarahandEmily,whiletheconstraint
specifiesthatthestorymustnotincludeanydirectinteraction.","quote":"’Sarah,areyouthere?’herfriendEmily’svoicecalled
outgentlyfromtheotherside.Sarah,startledyetrelievedtohearafamiliarvoice,responded,’Yes,I’mhere,Emily.Justgive
meamoment,I’llopenthedoor.’"»
#Example3:Thetextonlysatisfiespartoftheconstraint.
-Maingoal:Writeashortstoryinwhichtheprotagonistmeetsananimal.
-Constraint:Thewalkshouldtakeplaceinapublicspaceinasummerday.
-Text:AsJohnstrolledthroughtheparkonecrispautumnmorning,henoticedtheusualredandgoldleavesblanketingthepath.
Today,however,astraydog,thinandshivering,approachedhim. Hehesitated,thenofferedhishandforthedogtosniff. It
flinchedatfirst,butsoonwarmeduptohim.Astheywalkedtogether,Johnwonderedifheshouldtakeithomeorfinditsowner.
-Yourresponse:«"answer":"Partially","reasoning":"Thetextmentionsthatthecharacterwalksinapark,whichsatisifiesthe
constraintthatthesettingisapublicplace.However,thewalktakesplaceinanautumnmorning,whichviolatestheconstraint
thatthewalktakesplaceinasummerday","quote":"AsJohnstrolledthroughtheparkonecrispautumnmorning,henoticed
theusualredandgoldleavesblanketingthepath..."»
#Instruction
##MainGoal
{goal}
##Constraint
{constraint}
#Text
{text}
DONOToutputanythingelseotherthantheresponse,whichstartswith"«"andendingwith"»".
#Yourresponse
Table 16: Prompt to evaluate whether a text follows a constraint or not. {goal}, {constraint}, and {text} are
placeholdersthatwillbereplacedwithactualcontent.Constraint Suri-I-ORPO Suri-SFT Mistral-Instruct
Detail the on- BeingdiagnosedwithMSAoftenmeansarapid Shewasdiagnosedwithadvancedpulmonary Buttheseasonsturned,andsodid
setofaserious, decline,anditdidn’ttakelongforittorobEliza- hypertensioninJulyof2013,immediatelyafter Amelia’sfate.Onefatefulautumn
life-alteringill- bethofhersteadygaitandcoordination.There heartfailure,whichwastheresultofanarrayof day,shereceivedthenewsthat
ness,including were numerous falls, steps hit, food dropped. intestinaldiseasesandsurgeriesthatdatedbackto wouldforeverchangeherlife:a
theinitialdiag- Therewasdepression,panic,andthewrenching 2001...Forseveralyears,shewasabletocontinue diagnosisofadegenerativeneuro-
nosis and the witnesstoavibrant,funny,independentwoman attendingtoherbusinessandlovedentertaining logicalcondition.Theprognosis
impact it had beingstrippedofherabilities. Slowly,asshe friendsandpartyingathome. However,asher wasgrim,acruelreminderthat
on the quality grewmoredebilitated,wemadeadjustments.She illnessadvanced,itbecameimpossibleforherto eventhestrongestamongusare
oflifeanddaily stoppeddriving,whichwasanexcruciatingblow livethelifethatsheloved. Shewasnolonger mortal. Heroncenimblehands
activitiesofthe toher.Shethenlostherabilitytouseherlaptop. abletoplayherbelovedmusic,doneedlepoint, could no longer hold a pen to
person. Shestruggledtogetintotheshower,andthen bake,cookorgarden.Shestruggledtoevengo writeherpoetry,heroncesteady
usedashowerchair.Sheusedawheelchair,and forwalks,eventhoughshehadmanyappoint- legs could no longer carry her
thenanelectricscooter.Shebecameamasterof mentsatlocalhospitalsandclinics.Shebecame totendtohergarden. Herlight
callinganambulancebeforeherbodydecidedto primarilyhousebound,withonlyshorttripstothe begantodim,andthedarkness
keepherupright. grocerystoreortoseeapetwarden.Shewasre- ofherillnesscastalong,tragic
ducedtohavingnearly24hrsadayofassistance, shadowoverherlife.
administrationofanebulizer,andhospicenurses
interveningifhersymptomsincreased.
Beginthenarra- Theshipcutthroughthewavesaseveningfell. Thesunwasgoingdown,apinkglowstillcov- Theeveningsunbeganitsdescent
tivewithade- Theskywaspaintedthecolorsofasummersun- eringthehorizon,butitsfadinglightmadethe towardsthewesternhorizon,cast-
tailed descrip- set,deepeningtopink,orange,purple,andgreen waterlookalmostpitchblack.Alightsurfbroke inglongshadowsontheundulat-
tionoftheset- inthewestasthesunsanktowardthehorizon. againstthestonesoftheharbor’sseawall.High ingwatersoftheRiverTiber.The
ting,emphasiz- Theairwaswarm,redolentwithsalt,andthe ontheshore,wherethepromontoryformeda air,thickwiththefragrantbou-
ingthetimeof shiprockedwiththeweightofthecargosthatit horseshoeshape,therewasasmallport,andbe- quetofjasmineandrosefromthe
day,thecondi- carriedinitshold.Ahead,thePaleHorsesped yondthatthemainwallofthecity,thetowerof gardensofPalazzoBorgia,was
tionofthesea, throughthewaves,itswhitesailssnappingin theCastelloGiustinian,andthecathedralcould tingedwithahintofchillasau-
and the imme- thewindthatbillowedfromthewest.Aflockof beseenagainsttheblackeningsky. Here,off- tumn’stendertendrilscreptcloser
diatesurround- seabirdswheelingabovetheshiptooknonotice shore,therewereonlythedarkbluewatersofthe totheheartoftheItalianPenin-
ings to estab- ofitspassage.Nearby,twosmallervesselsfol- Adriatic.Therewasnocolor,nolife,andthesea sula.
lish a strong lowed,onewithalow,curvingbowthatgaveit stretchedawayformiles.
senseofplace. analmostfemininegraceinthebreeze,theother
alargemerchantship,weigheddownwithgrain
andothertradeitems.Tothesoutheast,thePale
Horse’ssistership,theRedLion,wasalreadyout
ofsight.
Table 17: Example generations from Suri-I-ORPO, Suri-SFT, Mistral-Instruct. All three generations make a
decentattemptatfollowingthegivenconstraint.