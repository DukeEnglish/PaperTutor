Statistical Test for Data Analysis Pipeline
by Selective Inference
Tomohiro Shiraishi1∗, Tatsuya Matsukawa1∗,
Shuichi Nishino1, Ichiro Takeuchi12†
June 28, 2024
Abstract
Adataanalysispipelineisastructuredsequenceofprocessingstepsthat
transformsrawdataintomeaningfulinsightsbyeffectivelyintegratingvar-
iousanalysisalgorithms. Inthispaper,weproposeanovelstatisticaltest
designed to assess the statistical significance of data analysis pipelines.
Our approach allows for the systematic development of valid statistical
tests applicable to any data analysis pipeline configuration composed of
asetofdataanalysiscomponents. Wehavedevelopedthisframeworkby
adapting selective inference, which has gained recent attention as a new
statistical inference technique for data-driven hypotheses. The proposed
statistical test is theoretically designed to control the type I error at the
desired significance level in finite samples. As examples, we consider a
classofpipelinescomposedofthreemissingvalueimputationalgorithms,
threeoutlierdetectionalgorithms,andthreefeatureselectionalgorithms.
We confirm the validity of our statistical test through experiments with
bothsyntheticandrealdataforthisclassofdataanalysispipelines. Addi-
tionally, we present an implementation framework that facilitates testing
across any configuration of data analysis pipelines in this class without
extra implementation costs.
1NagoyaUniversity
∗Equalcontribution
2RIKEN
†Correspondingauthor. e-mail: ichiro.takeuchi@mae.nagoya-u.ac.jp
4202
nuJ
72
]LM.tats[
1v20981.6042:viXra1 Introduction
Inpracticaldataanalysistasks,integratingvarioustypesofalgorithmsisessen-
tial to address diverse challenges. For example, in genetic research that seeks
to identify genes linked to a specific disease, we often start with preprocessing
tasks such as filling in missing values and detecting outliers. Subsequently, we
screen for potentially related genes using simple descriptive statistics, followed
by more complex machine learning-based feature selection algorithms. Such a
systematic sequence of steps designed to analyze data to derive useful insights
isreferredtoasadata analysis pipeline. Adataanalysispipelineplaysacrucial
role in ensuring reproducibility and reliability in data-driven decision-making.
Describingtheentiredataanalysisprocessasapipelinefacilitatespreciserepli-
cationoftheanalysis,enhancingtheconsistencyoftheresults. Thisisespecially
effective in data-driven scientific research, a well-designed pipeline fortifies the
scientificprocess,allowingresearcherstobuildoneachother’sworkconfidently,
advancing knowledge with verified and consistent findings. In this study, as an
example, we examine a simple class of data analysis pipelines that integrates
various missing-value imputations (MVI) algorithms outlier detection (OD) al-
gorithms, and feature selection (FS) algorithms. Figure 1 displays examples
of two such pipelines. The pipeline on the left starts with a mean value im-
putation algorithm, followed by L regression based OD algorithm, proceeds
1
with marginal screening to refine feature candidates, and concludes by using
two FS algorithms—stepwise feature selection and Lasso—selecting their union
as the final features. The pipeline on the right initiates with regression impu-
tation, continues with marginal screening to narrow down feature candidates,
usesCook’sdistanceforOD,andappliesbothstepwiseFSandLasso,ultimately
choosing the intersection of their results as the final features.
When a data-driven approach is used for critical issues in scientific studies,
it is necessary to quantify the reliability of the final results obtained through
the pipeline. The goal of this study is to construct a framework for quantify-
ing the statistical reliability of data analysis pipelines within statistical testing
2framework and quantify the statistical significance in the form of p-values. P-
values provideameasurablecriteriontodeterminewhetherobservedeffectsare
likely due to chance or are statistically significant, making objective decisions
regarding the validity of a hypothesis. Furthermore, p-values are one of the
most commonly used metrics in scientific research to quantify the significance.
However,conductingstatisticaltestsontheentiredataanalysispipelineischal-
lengingduetothecomplexinterrelationsamongmultiplealgorithms. Giventhe
numerous possible combinations of algorithms and their integration, designing
individualstatisticaltestsforeachpipelineisnotpractical. Instead,auniversal
approach to calculating p-values for any pipeline configuration is required. To
tacklethesechallenges,weintroducetheconceptofselectiveinference(SI)[Tay-
lor and Tibshirani, 2015, Fithian et al., 2015, Lee and Taylor, 2014], a novel
statistical inference approach that has garnered considerable attention in the
past decade. The key concept of SI is to characterize the selection process of
hypotheses from the data and calculate the corresponding p-value using the
sampling distribution conditional on this hypothesis selection process. In this
study,weproposeanapproachbasedonSIthatenablestoprovidevalidp-values
for any pipeline configuration within a specific class. Furthermore, we develop
an implementation framework that facilitates SI for any pipeline configuration
withintheaboveclassofpipelineswithoutadditionalimplementationcost. The
code is available as supplementary material.
Figure 1: Two examples of pipelines within the class considered in this study.
Related Works. Most research on data analysis pipelines is concentrated in
the field of software engineering, rather than machine learning [Sugimura and
Hartl, 2018, Hapke and Nelson, 2020, Drori et al., 2021]. These studies fo-
3cus on the design, implementation, testing, and maintenance of data analysis
pipeline systems to ensure their efficiency, scalability and robustness. Research
in AutoML has prompted the creation of frameworks capable of autonomously
selecting appropriate algorithms and hyperparameters, effectively automating
the construction of data analysis pipelines [Yang et al., 2020, Yakovlev et al.,
2020, Karmaker et al., 2021]. However, to the best of our knowledge, there is
noexistingresearchthatsystematicallydiscussesthereliabilityofdataanalysis
pipelines and its automatic generation from a statistical perspective. Resam-
plingtechniquessuchascross-validation(CV)arecommonlyusedforevaluating
theentiredataanalysisprocess. However,practicaldataanalysisoftenincludes
unsupervised learning tasks such as MVIs and ODs, where resampling cannot
be directly applied. Furthermore, dividing the data reduces the sample size,
leading to decreased accuracy in hypothesis selection and statistical power.
SI has gained attention as a method of statistical inference for feature selec-
tion in linear models [Taylor and Tibshirani, 2015, Fithian et al., 2015]. SI for
variousfeatureselectionalgorithmssuchasmarginalscreening[LeeandTaylor,
2014], stepwise FS [Tibshirani et al., 2016], and Lasso [Lee et al., 2016] have
been explored, and then extended to more complex FS methods, e.g., [Yang
et al., 2016, Suzumura et al., 2017, Hyun et al., 2018, Ru¨gamer and Greven,
2020, Das et al., 2021]. SI proves valuable not only for FS in linear models but
also for statistical inference across various data-driven hypotheses, including
unsupervised learning tasks such as OD [Chen and Bien, 2020, Tsukurimichi
etal.,2021],segmentation[Tanizakietal.,2020,Duyetal.,2022,LeDuyetal.,
2024],clustering[Leeetal.,2015,Gaoetal.,2022],change-pointdetection[Duy
etal.,2020,Jewelletal.,2022]. Moreover, SIisbeingappliednotonlytolinear
models but also to more complex models such as kernel models [Yamada et al.,
2018], tree structure models [Neufeld et al., 2022], and CNNs [Duy et al., 2022,
Miwa et al., 2023] and Transformers [Shiraishi et al., 2024]. The basic form of
SIappliestohypothesisselectioneventscharacterizedasasetoflinearinequal-
ities within the data space, with several extensions have been made to apply it
to a broader class of selection events [Liu et al., 2018, Tian and Taylor, 2018,
4Duy and Takeuchi, 2022, Panigrahi et al., 2024]. To our knowledge, existing SI
researchhaspredominantlyconcentratedonindividualalgorithms,withlimited
exploration into evaluating the statistical significance of outcomes arising from
the combination of multiple algorithms.
Contributions. The summary of our contributions is as follows. First, we
develop a statistical test with finite sample validity for data analysis pipelines
consisting of a prespecified class of components. Second, we develop a com-
putational and implementation framework that allows for constructing statis-
tical tests across any configuration of pipelines without extra implementation
costs. Third, instead of naively applying existing SI, we construct a more pow-
erful version by formulating it as a line search problem and verify its perfor-
mance through experiments with synthetic and real datasets. Our implemen-
tation is available at https://github.com/shirara1016/statistical_test_
for_data_analysis_pipeline.
52 Preliminaries
Given a set of algorithm components, a pipeline is defined by selecting some
components from the set and connecting the selected components in an appro-
priate way. A pipeline can be represented as a directed acyclic graph (DAG)
with components as nodes, and the connections as edges. In this study, as an
example class of pipelines, we consider a set of algorithms consisting of three
MVIalgorithms, threeADalgorithms, threeFSalgorithms, aswellasIntersec-
tion andUnion operations(specificthreealgorithmseachforMVI,OD,andFS
are described later in this section). Figure 1 shows two examples of pipelines
within this class.
Problem Setting. Inthisstudy,weconsidertheproblemoffeatureselection
for linear models from a dataset containing missing values and/or outliers us-
ing the aforementioned class of data analysis pipelines. Let us consider linear
regression problem with n instances and d features. We denote the observed
dataset as (X,y) , where X ∈Rn×d is the fixed design matrix, while y ∈Rn′ is
the response vector which contains outlying values but excludes missing values
(i.e., n′ ≤ n). We assume that the observed response vector y is a random
realization of the following random response vector
Y =µ(X)+ε,ε∼N(0,Σ)
whereµ(X)∈Rn′ istheunknowntruevaluefunction,whileε∈Rn′ isnormally
distributed noise with covariance matrix Σ which is known or estimable from
an independent dataset1. Although we do not pose any functional form on the
true value function µ(X) for theoretical justification, we consider a case where
the true values µ(X) are reasonably approximated by a linear model as long as
they are non-outliers. This is a common setting in the field of SI, referred to as
the saturated model setting. Furthermore, we denote the response vector with
imputedmissingvaluesasy(+) ∈Rn. Usingtheabovenotations,adataanalysis
1Wediscusstherobustnessoftheproposedmethodwhenthecovariancematrixisunknown
andthenoisedeviatesfromtheGaussiandistributioninAppendixE.
6pipeline comprising of MVI, OD, and FS algorithm components is represented
as a function
P :Rn×d×Rn′ ∋(X,y)(cid:55)→(y(+),O,M)∈Rn×2[n]×2[d], (1)
where y(+) ∈Rn is the response vector with missing values imputed, O ⊂[n] is
the set of outliers, and M⊂[d] is the set of selected features.
Statistical Test for Pipelines. Given the output of a pipeline in (1), the
statisticalsignificanceofthefinallyselectedfeaturescanbequantifiedbasedon
the coefficients of the linear model fitted only with the selected features from a
dataset with missing values imputed and outliers removed. To formalize this,
we denote the design matrix after removing outliers and composed only of the
selectedfeaturesasX ∈Rn−|O|×|M|, anddenotetheresponsevectorwith
−O,M
outliers removed and missing values imputed as y(+). Using these notations,
−O
theleastsquaressolutionofthelinearmodelafterimputationofmissingvalues,
removal of outliers, and feature selection is expressed as
βˆ=(cid:0) X⊤ X (cid:1)− X⊤ y(+).
−O,M −O,M −O,M −O
Similarly, we consider the population least-square solution for the unobservable
true value vector µ(X) in (2), which is defined as
β∗ =(cid:0) X⊤ X (cid:1)− X⊤ µ(+)(X ),
−O,M −O,M −O,M −O −O,M
where µ(+)(X )∈Rn−|O| indicates a vector of dimension n−|O| that are
−O −O,M
obtained by providing X to the unknown true function µ with the miss-
−O,M
ing values imputed with the same MVI algorithm. To quantify the statistical
significance of the selected features, we consider the following null hypothesis
H and the alternative hypothesis H :
0 1
H :β∗ =0 v.s. H :β∗ ̸=0,j ∈M, (2)
0 j 1 j
where, with a slight abuse of notation, β∗ and βˆ respectively indicates the
j j
element of β∗ and βˆcorresponding to the selected feature j ∈M.
7Missing-Value Imputation (MVI) Algorithm Components. Inthispa-
per, as three examples of MVI algorithms, we consider mean value imputa-
tion, nearest-neighbor imputation, and regression imputation algorithms (see
Appendix A.1). A MVI algorithm component is represented as
f :{X,y,O,M}(cid:55)→{X,y+,O,M},
MVI
where, among the four variables, only y is updated to y(+), but note that
this notation is used to uniformly handle all components in the pipeline. It is
important to note that these three MVI algorithms are linear algorithm in the
sense that, using a matrix D ∈Rn×n′ that depends on X, the imputed values
X
are written as y(+) =D y.
X
OutlierDetection(OD)AlgorithmComponents. Inthispaper,asthree
examples of OD algorithms, we consider Cook’s distance-based OD, DFFITS
OD, and L regression based OD algorithms (see Appendix A.2). A OD algo-
1
rithm component is represented as
f :{X,y(+),O,M}(cid:55)→{X,y(+),O′,M},
OD
where,O′ istheupdatedsetofoutliers. Notethat,ifoutlierremovalandfeature
selectionhavenotyetbeenperformed,thesetsOandMareinitializedasO =∅
and M=[d].
Feature Selection (FS) Algorithm Components. In this paper, as three
examples of FS algorithms, we consider marginal screening, stepwise feature
selection,andLasso algorithms(seeAppendixA.3). AFSalgorithmcomponent
is represented as
f :{X,y(+),O,M}(cid:55)→{X,y(+),O,M′},
FS
where, M′ is the updated set of features.
Union and Intersection Components. WhenusingmultipleOD/FSalgo-
rithms, it is necessary to include components in the pipeline that perform the
8union/intersection of the detected outliers/selected features. Such union/inter-
section components for OD/FS are respectively written as
fO :{X,y(+),{O } ,M}(cid:55)→{X,y(+),Σ O ,M},
Σ e e∈[E] e∈[E] e
fM :{X,y(+),O,{M } }(cid:55)→{X,y(+),O,Σ M },
Σ e e∈[E] e∈[e] e
whereE isthenumberofOD/FSalgorithmsandanoperatorΣindicateseither
union or intersection operation of multiple sets.
Automatic Pipeline Construction. In this study, we consider two cases
for pipeline configuration: an option specified by the user and an option deter-
mined based on the data. In the first option, the user can select some of the
aforementioned data analysis components and specify their own configuration.
On the other hand, the second option allows for the selection of the optimal
configurationfromamongmultiplepre-definedpipelineconfigurationsbasedon
cross-validation. Animportantpointinthesecondoptionisthatourstatistical
test is designed by properly considering the fact that the optimal pipeline con-
figuration has been selected based on the data. For more details on the second
option, see §6 and Appendix F.
Selective Inference (SI). For the statistical test in (2), it is reasonable to
use βˆ ,j ∈ M as the test statistic. An important point when addressing this
j
statistical test within the SI approach is that the test statistic is represented as
a linear function of the observed response vector as βˆ = η⊤y,j ∈ M, where
j j
η ∈ Rn′,j ∈ M is a vector that depends on y only through the detected
j
outlier set O and the selected feature set M 2. In SI, this property is utilized
to perform statistical inference based on the sampling distribution of the test
statistic conditional on O and M. More specifically, since y follows a normal
distribution,itcanbederivedthatthesamplingdistributionoftheteststatistic
βˆ = η⊤y,j ∈ M conditional on O, M, and the sufficient statistic of the
j j
nuisance parameters follows a truncated normal distribution. By computing p-
2NotethattheMVIalgorithmsconsideredinthispaperdependonlyonX,notony.
9valuesbasedonthisconditionalsamplingdistributionrepresentedasatruncated
normal distribution, it is ensured that the type I error can be controlled even
in finite samples. For more details on SI, please refer to the explanations in the
following sections or literatures such as [Taylor and Tibshirani, 2015, Fithian
et al., 2015, Lee and Taylor, 2014].
103 Selective Inference for Data Analysis Pipeline
Toperformstatisticaltestforpipelines,itisnecessarytoconsiderhowthedata
influencedthefinalresultthroughthecalculationsofeachalgorithmcomponent
of the pipeline and in operations where they are combined with a specified
configuration. WeaddressthischallengebyutilizingtheSIframework. IntheSI
framework,statisticalinferenceisperformedbasedonthesamplingdistribution
conditional on the process by which the data selects the final result, thereby
incorporating the influence of how data is processed in the pipeline.
Selective Inference. In SI, p-values are computed based on the null distri-
bution conditional on an event that a certain hypothesis is selected. The goal
of SI is to compute a p-value such that
P (p≤α|O =O,M =M)=α, ∀α∈(0,1), (3)
H0 Y Y
where O and M respectively indicate the random set of detected outliers
Y Y
and selected features given the random response vector Y, thereby making the
p-value is a random variable. Here, the condition part M =M and O =O
Y Y
in(3)indicatesthatweonlyconsiderresponsevectorsY fromwhichacertainset
ofoutliersO andacertainsetofMareobtained. IftheconditionaltypeIerror
rate can be controlled as in (3) for all possible hypotheses (M,O)∈2[p]×2[n],
then, by the law of total probability, the marginal type I error rate can also be
controlled for all α∈(0,1) because
(cid:88) (cid:88)
P (p≤α)= P (M,O)P (p≤α|M =M,O =O)=α.
H0 H0 H0 Y Y
M∈2[p]O∈2[n]
Therefore, in order to perform valid statistical test, we can employ p-values
conditional on the hypothesis selection event. To compute a p-value that satis-
fies (3), we need to derive the sampling distribution of the test-statistic
T(Y)|{M =M ,O =O }. (4)
Y y Y y
Selective p-value. Toperformstatisticalhypothesistestbasedonthecondi-
tional sampling distribution in (4), we introduce an additional condition on the
11sufficient statistic of the nuisance parameter Q , defined as
Y
(cid:18) ηη⊤(cid:19)
Q = I − Y. (5)
Y n′ ∥η∥2
ThisadditionalconditioningonQ(Y)isastandardpracticeintheSIliterature
required for computational tractability3. Based on the additional conditioning
on Q(Y), the following theorem tells that the conditional p-value that satis-
fies (3) can be derived by using a truncated normal distribution.
Theorem 1. Consider a constant design matrix X, a random response vec-
tor Y ∼ N(µ,σ2I ) and an observed response vector y. Let (M ,O ) and
n′ Y Y
(M ,O ) be the pairs of selected features and detected outliers obtained, by ap-
y y
plying a pipeline process P in the form of (1) to (X,Y) and (X,y), respectively.
Let η ∈Rn′ be a vector depending on (M ,O ), and consider a test-statistic in
y y
the form of T(Y)=η⊤Y. Furthermore, define the nuisance parameter Q as
Y
in (5).
Then, the conditional distribution
T(Y)|{M =M ,O =O ,Q =Q }
Y y Y y Y y
isatruncatednormaldistribution TN(η⊤µ,σ2∥η∥2,Z)withthemeanη⊤µ, the
variance σ2∥η∥2, and the truncation intervals Z. The truncation intervals Z is
defined as
Z ={z ∈R|M =M ,O =O }, a=Q , b=η/∥η∥2. (6)
a+bz y a+bz y y
TheproofofTheorem1isdeferredtoAppendixB.1. Byusingthesampling
distribution of the test statistic T(Y) conditional on M = M , O = O ,
Y y Y y
and Q =Q in Theorem 1, we can define the selective p-value as
Y y
p =P (|T(Y)|≥|T(y)||M =M ,O =O ,Q =Q ). (7)
selective H0 Y y Y y Y y
3ThenuisancecomponentQY correspondstothecomponentz intheseminalpaper[Lee
etal.,2016](seeSec. 5,Eq. (5.2),andTheorem5.2)andisusedinalmostalltheSI-related
worksthatwecited.
12Theorem 2. The selective p-value defined in (7) satisfies the property in (3),
i.e.,
P (p ≤α|M =M ,O =O )=α, ∀α∈(0,1).
H0 selective Y y Y y
Then,theselectivep-valuealsosatisfiesthefollowingpropertyofavalidp-value:
P (p ≤α)=α, ∀α∈(0,1).
H0 selective
The proof of Theorem 2 is deferred to Appendix B.2. This theorem guaran-
teesthattheselectivep-valueisuniformlydistributedunderthenullhypothesis
H , and thus can be used to conduct the valid statistical inference in (2). Once
0
the truncation intervals Z is identified, the selective p-value in (7) can be easily
computedbyTheorem1. Thus,theremainingtaskisreducedtoidentifyingthe
truncation intervals Z.
134 Computations: Line Search Interpretation
From the discussion in §3, it is suffice to identify the one-dimensional subset Z
in (6) to conduct the inference. In this section, we propose a novel line search
method to efficiently identify the Z.
4.1 Overview of the Line Search
ThedifficultyinidentifyingtheZ arisesfromthefactthatthemultipleFS/OD
algorithms are applied in an arbitrary complex order. To surmount this diffi-
culty,weproposeanefficientsearchmethodthatleveragesparametric-programming
and the fact that our pipeline can be conceptualized as a directed acyclic graph
(DAG) whose nodes represent the operations. In a standard analysis pipeline,
M and O are computed and updated along the DAG. However, in our frame-
work,intervalsforwhichMandOareconstantarealsocomputedandupdated,
allowing the computation of the truncation intervals Z.
In the following, we first discuss how, given a certain computational proce-
dure (combining update rules as discussed in later), the Z can be identified by
parametric-programming. Then, we summarize the overall procedure to com-
pute the selective p-value from the Z. Finally, we describe the update rules for
each node based on the existing methods of SI for each FS and OD algorithm.
Note that DAGs can topologically sortable, so that update rules can be applied
in sequence. The overview of the proposed line search method is illustrated in
Figure 2.
4.2 Parametric-Programming
To identify the truncation intervals Z, we assume that we have a procedure to
compute the interval [L ,U ] for any z ∈R which satisfies
z z
∀r ∈[L ,U ], M =M ,O =O .
z z a+br a+bz a+br a+bz
14Figure 2: Schematic illustration of the proposed line search method to identify
the truncation intervals Z. The upper part shows the DAG representation
of the pipeline and its topological sorting (i). The lower left part shows the
operations performed by update rules in sequence (ii). The lower right part
shows the identification of the truncation intervals Z by taking the union of
some intervals based on parametric-programming (iii).
15Then, the truncation intervals Z can be obtained by the union of the intervals
[L ,U ] as
z z
(cid:91)
Z = [L ,U ]. (8)
z z
z∈R|Ma+bz=My,Oa+bz=Oy
Theprocedurein(8)iscommonlyreferredtoasparametric-programming(e.g.,
lasso regularization path). We discuss the details of the procedure to compute
the interval [L ,U ] by defining the update rules for each node in the next
z z
subsection.
4.3 Update Rules
In this subsection, we discuss the computation procedure to obtain the interval
[L ,U ] for any z ∈R just mentioned in §4.2. To compute the interval [L ,U ],
z z z z
we consider extending the input of each node in a DAG and denote it as a
pair of (X,a,b,z,M,O,l,u), where X is the design matrix, a, b and z are the
currently linear expression of the response vector a+bz, M and O are the
currently selected features and detected outliers, and l and u are the currently
interval. The input of the first node is initialized to (X,a,b,z,[d],∅,−∞,∞),
where d is the number of features. We details the update rules for this pair at
each node of a DAG in Appendix C.
The overall procedure for computing the interval [L ,U ] by applying the
z z
update rules in the order of the topological sorting of the DAG is summarized
in Algorithm 1, where the operation pa receives the index of the target node
and returns the indexes of its parent nodes, and pa(1) is set to 0. Algorithm 1
satisfies the specifications described in §4.2, i.e., the following theorem holds.
Theorem 3. Consider a pipeline P, a design matrix X, and vectors a and b
representingthelinearexpressionoftheresponsevectorasfixed. Foranyz ∈R,
let [L ,U ], M and O be the output of Algorithm 1 with P, X, a, b
z z a+bz a+bz
and z as input.
Then, for any r ∈ [L ,U ], the output of Algorithm 1 does not change by
z z
16changing the input z to r:
update interval(P,X,a,b,r)=([L ,U ],M ,O ).
z z a+bz a+bz
The proof Theorem 3 is deferred to Appendix B.3.
Procedure 1 Apply Update Rules in Order of Topological Sorting of DAG
(Update Interval)
Require: P, X, a, b and z
1: Converts the pipeline P to a topologically sorted graph (V,E)
2: InitializetheinputofthefirstnodeB 0as(X,a,b,z,[p],∅,−∞,∞)(see§4.3)
3: for each index of node i∈{1,...,|V|} do
4: Apply the update rule of the node v i to its input B pa(i) to obtain the
output B (see §4.3)
i
5: end for
6: Let the last four components of B |V| be M a+bz, O a+bz, L z and U z, respec-
tively
Ensure: [L ,U ], M and O
z z a+bz a+bz
175 Implementations: Auto-Conditioning
All of the update rules defined in §4.3 are node-specific operations and do not
depend on the type of node corresponding to the input/output. Then, we can
modularize the update rules and apply them sequentially as in Algorithm 1,
which implementation we call auto-conditioning. The auto-conditioning allows
one to simply define an arbitrary pipeline and perform hypothesis testing on
it without additional implementation costs. In this section, we show some ex-
amples of defining pipelines and performing hypothesis testing using the auto-
conditioning. The implementation we developed can be interactively executed
using the provided Jupyter Notebook (ipynb) file, which is available as supple-
mentary material.
As an example, Listing 1 shows a code example that defines the pipeline
shown in Figure 2 based on our package pipelineprocessor and performs
hypothesis testing on it. A similarly simple UI allows for easy implementation
of other pipeline structures as well as automatic pipeline construction based on
the cross-validation. For more examples, please refer to the Appendix G and
the supplementary material.
18Listing 1: Code example that defines the pipeline shown in Figure 2. We
can implement an instance of the pipeline with inference methods simply
byspecifyingeachoperationinturn. Toperformhypothesistesting,wecan
call the inference method of the pipeline instance with the input dataset
(X,y) and the deviation of the noise σ.
1 import numpy as np
2 import pipelineprocessor as plp
3
4 def option1():
5 X, y = plp.make_dataset()
6 y = plp.mean_value_imputation(X, y)
7
8 O = plp.soft_ipod(X, y, 0.02)
9 X, y = plp.remove_outliers(X, y, O)
10
11 M = plp.marginal_screening(X, y, 5)
12 X = plp.extract_features(X, M)
13
14 M1 = plp.stepwise_feature_selection(X, y, 3)
15 M2 = plp.lasso(X, y, 0.08)
16 M = plp.union(M1, M2)
17 return plp.make_pipeline(output=M)
18
19 pl = option1()
20 X, y = np.random.normal(size=(100, 10)), np.random.normal(size=100)
21 M, p_list = pl.inference(X, y, sigma=1.0)
196 Numerical Experiments
Methods for Comparison. In our experiments, we consider the three types
of pipelines: op1, op2, and all cv. The op1 and op2 are defined in Figure 2.
The all cv is a pipeline selected based on cross-validation from 16 different
parameters sets each in the op1 and op2 pipelines (i.e., from 32 pipelines in
total). For each three types of pipelines, we compare the proposed method
(proposed) with oc (simple extension of SI literature to our setting) and naive
test (naive), in terms of type I error rate and power. See Appendix D.1 for
more details.
Experimental Setup. In all experiments, we set the significance level α =
0.05. For the experiments to see the type I error rate, we change the number
of samples n ∈ {100,200,300,400} and set the number of features d to 20.
See Appendix D.2 for the case when changing the number of features. In each
setting,wegenerated10,000nulldatasets(X,y),whereX ∼N(0,1),∀(i,j)∈
ij
[n]×[d], y ∼ N(0,I ) and each y ,∀i ∈ [n] was set to NaN with probability
n i
0.03. To investigate the power, we set n = 200 and d = 20 and generated
10,000datasets(X,y),whereX ∼N(0,1),∀(i,j)∈[n]×[d],y =Xβ+ϵwith
ij
ϵ ∼ N(0,I ), β ∈ Rd is a vector whose first three elements are ∆’s and the
n
others are 0’s and each y , ∀i ∈ [n] was set to NaN with probability 0.03. We
i
set ∆ ∈ {0.2,0.4,0.6,0.8}. See Appendix D.3 for the computer resources used
in the experiments.
Results. TheresultsoftypeIerrorrateareshowninFigure3. Theproposed
and oc successfully controlled the type I error rate under the significance level
in all settings for all types of pipelines, whereas the naive could not. Because
thenaivefailedtocontrolthetypeIerrorrate,wenolongerconsideritspower.
TheresultsofpowerareshowninFigure4andweconfirmedthattheproposed
has much higher power than the oc in all settings for all types of pipelines.
20Real Data Experiments. We compared the proposed method and the oc
method for all cv pipeline on eight real datasets: Energy Efficiency (heating
load;Data1andcoolingload;Data2),GasTurbineCOandNOxEmissionData
Set (Data3), Wine Quality (red wine; Data4 and white wine; Data5), Abalone
(Data6), Concrete Compressive Strength Data Set (Data7), and Real Estate
Valuation (Data8) in the UCI Machine Learning Repository. From each of the
original dataset, we randomly generated 1,000 sub-sampled datasets with size
n ∈ {100,150,200}. We applied the proposed and oc to check the powers of
the two methods. The results are shown in Table 1. In all datasets for all
n ∈ {100,150,200}, the proposed method has much higher power than the oc
method. Moreover, the power of our proposed method increases as n increases.
1.0 1.0 1.0
proposed proposed proposed
oc oc oc
0.8 naive 0.8 naive 0.8 naive
significance level significance level significance level
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
100 200 300 400 100 200 300 400 100 200 300 400
number of samples number of samples number of samples
(a) op1 (b) op2 (c) all cv
Figure3: TypeIErrorRatewhenchangingthenumberofsamplesn. Onlyour
proposed method and the oc method are able to control the type I error rate in
all settings for all types of pipelines.
21
etaR
rorrE
I epyT
etaR
rorrE
I epyT
etaR
rorrE
I epyT1.0 1.0 1.0
proposed proposed proposed
oc oc oc
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8
signal signal signal
(a) op1 (b) op2 (c) all cv
Figure 4: Power when changing the true coefficient. Our proposed method has
much higher power than the oc method in all settings for all types of pipelines.
Table1: Powerontherealdatasetswhenchangingthenumberofsamples. Each
cellindicatestwopowers: proposedv.s. oc,whichareseparatedbyaslashand
thehigheroneisboldfaced. Ourproposedmethodhasmuchhigherpowerthan
the oc method in all datasets for all n∈{100,150,200}.
Data1 Data2 Data3 Data4 Data5 Data6 Data7 Data8
n=100 .59/.11 .54/.10 .74/.11 .33/.05 .33/.06 .34/.06 .54/.07 .50/.05
n=150 .70/.13 .60/.12 .79/.12 .43/.07 .38/.06 .43/.06 .69/.06 .63/.06
n=200 .75/.16 .69/.14 .82/.15 .46/.05 .45/.07 .55/.07 .74/.09 .77/.06
22
rewoP rewoP rewoP7 Conclusions, Limitations and Future Works
In this study, we introduced a novel framework for testing the statistical sig-
nificance of data analysis pipelines based on the concept of SI. We believe that
this study will contribute to improving the reliability of data-driven scientific
research in various application fields. Currently, there are limitations on the
applicable data analysis components, leaving several challenges in applying the
proposed method to more complex data analysis pipelines. Furthermore, it is
stillchallengingtoselecttheoptimalpipelinefromallpossiblecandidates,which
requires exponentially increasing computational costs in our current implemen-
tation scheme.
23Acknowledgement
This work was partially supported by MEXT KAKENHI (20H00601), JST
CREST (JPMJCR21D3, JPMJCR22N2), JST Moonshot R&D (JPMJMS2033-
05), JST AIP Acceleration Research (JPMJCR21U2), NEDO (JPNP18002,
JPNP20006) and RIKEN Center for Advanced Intelligence Project.
24A Pipeline Components
A.1 Missing-Value Imputation (MVI) Algorithm Compo-
nents
A MVI algorithm component is represented as
f :{X,y,O,M}(cid:55)→{X,y(+),O,M},
MVI
where y ∈Rn′ is the response vector which excludes missing values and y(+) ∈
Rn isthevectorwithimputedmissingvalues. MVIalgorithmsinthispaperare
linear algorithminthesensethat, usingamatrixD ∈Rn×n′ thatdependson
X
X are written as y(+) =D y.
X
Mean Value Imputation. This method replaces missing values with the
meanvalueofobserveddataandallowsforquickandeasyimputationofmissing
values. An example of D for y = (y ,y ,y )⊤ (i.e., y is missing value and
X 1 3 4 2
n=4) is:
 
1 0 0
 
1/3 1/3 1/3
 
D
X
= .
 0 1 0 
 
 
0 0 1
Nearest-Neighbor Imputation. This method replaces missing values with
the most similar instance in the dataset. In this method, similarity between in-
stancesismeasuredbysomedistancebetweentheirfeaturevectors. Asdistance
measuresℓ(·,·),forexample,Euclidean,Manhattan,orChebyshevdistancecan
be used. An example of D for y = (y ,y ,y )⊤ (i.e., y is missing value and
X 1 3 4 2
n=4) is:
 
1 0 0
 
 e⊤ 
D X =  j  , j = argminℓ(x 2,x i),
 0 1 0  i∈{1,3,4}
 
0 0 1
25wheree isthevectorconstructedbyremovingtheindicesofthemissingvalues
j
(i.e., {2}) from the j-th unit vector in R4.
Regression Imputation. This method replaces missing values with esti-
mated values based on a regression model. We use the observed instances to
estimate the regression coefficients, and then use the estimated coefficients to
predict the missing values from its feature vector. We denote the indices of the
missingvaluesasNaN,andtheindicesoftheobservedvaluesas−NaN. There-
gressioncoefficientscanbeestimatedasβˆ=(XT X )−1XT y and
−NaN,: −NaN,: −NaN,:
theneachimputedmissingvaluey(+),i∈NaNcanbeexpressedasy(+) =x⊤βˆ.
i i i
An example of D for y =(y ,y ,y )⊤ (i.e., y is missing value and n=4) is:
X 1 3 4 2
 
1 0 0
 
X (X⊤ X )−1X⊤ 
D X =  {2},: {1,3,4},: {1,3,4},: {1,3,4},: 
0 1 0 
 
 
0 0 1
A.2 Outlier Detection (OD) Algorithm Components
A OD algorithm component is represented as
f :{X,y(+),O,M}(cid:55)→{X,y(+),O′,M},
OD
where O′ is the updated set of outliers.
Cook’s Distance-based OD. This method identifies instances as outliers
whenCook’s distance, ameasureoftheinfluenceofaparticularinstanceonthe
entire regression model, exceeds a predefined threshold value. Cook’s distance
of the i-th instance is defined as
(cid:80)n (cid:0)
yˆ −yˆ
(cid:1)2
D = j=1 j j(i) ,
i d MSE
where yˆ and yˆ are the predicted value of j-th instance from the regres-
j j(i)
sion model with and without i-th instance, respectively, and MSE is the mean
squarederrorofthefullmodel. ThisD representsthestandardizedvalueofthe
i
26change in predictions for all other instances due to the removal of i-th instance,
and the larger D is, the more it affects the model. By utilizing the leverage
i
value, it can also be represented as
εˆ2 h
D = i ii ,
i d MSE(1−h )2
ii
where εˆ is the i-th residual, h is the i-th leverage value (i.e., the diagonal
i ii
component of the matrix X(X⊤X)−1X⊤). We identify the i-th instance as an
outlier if D >λ where λ is a predefined threshold value.
i
DFFITS OD This method has the same concept as Cook’s distance-based
OD but uses DFFITS instead of Cook’s distance as the measure of influence.
DFFITS of the i-th instance is defined as
yˆ −yˆ
i i(i)
DFFITS = ,
i (cid:112)
MSE h
(i) ii
whereyˆ andyˆ arethepredictedvalueofthei-thinstancefromtheregression
i i(i)
model with and without the i-th instance, respectively, MSE is the mean
(i)
squared error of the regression model without the i-th instance, and h is the
ii
i-th leverage value. Thus, DFFITS is a value that standardizes the difference
between the predicted value when excluding and including a specific instance,
and the larger DFFITS is, the more it affects the model. By utilizing the
i
external Studentized residual r , it can also be represented as
i,ext
(cid:114)
h
DFFITS = ii r .
i 1−h i,ext
ii
We identify the i-th instance as an outlier if DFFITS2 >λd/(n−d) where λ is
i
a predefined threshold value and usually set to 4.
L Regression based OD This method identifies instances as outliers by
1
using L1 regularization for the mean-shift model. In this method, we assume
that the unknown true value function µ(X) follows the following mean-shift
model:
µ(X)=Xβ+u,
27where u ∈ Rn is an outlier term and u ̸= 0 if the i-th instance is an outlier,
i
otherwise u = 0. We estimate (βˆ ,uˆ ) by solving the following optimization
i λ λ
problem:
1
(βˆ ,uˆ )= argmin ∥y(+)−Xβ−u∥2+λ∥u∥ ,
λ λ 2n 2 1
β∈Rd,u∈Rn
where λ is a predefined regularization parameter. We identify the i-th instance
as an outlier if uˆ ̸=0.
λ,i
A.3 Feature Selection (FS) Algorithm Components
A FS algorithm component is represented as
f :{X,y(+),O,M}(cid:55)→{X,y(+),O,M′},
FS
where, M′ is the updated set of features.
Marginal Screening This method selects the k features that are most cor-
related with the response variable, where k is a predefined number. The corre-
lation is computed as the absolute value of the inner product |x⊤y(+)| between
j
the normalized feature vector x and the response vector y(+).
j
StepwiseFeatureSelection Thismethodselectsfeaturesbyiteratingthrough
the steps of adding or deleting the features that best improve the goodness of
fitoftheregressionmodel. Inthispaper, wedealwithforwardstepwisefeature
selection, which only adds features. The residual sum of squares (RSS) of the
least squares regression model constructed using the features selected up to the
previous step is used as the goodness of fit of the model. First, a null model (a
model consisting of an intercept term) is used as an initial state, and in each
step,RSSiscalculatedfromtheleastsquaresregressionmodelconstructedwith
the features selected in the previous step and the residual of y(+). After that,
select the feature that minimize RSS and update the model. The algorithm
terminates if the RSS is not improved by adding any feature, or if the number
of selected features reaches a predefined upper limit.
28Lasso ThismethodselectsfeaturesbyusingalinearregressionmodelwithL1
regularization. Weestimatetheregressioncoefficientβˆbysolvingthefollowing
optimization problem:
1
βˆ=argmin ∥y(+)−Xβ∥2+λ∥β∥ ,
2n 2 1
β∈Rd
where λ is a predefined regularization parameter. We select the features for
which βˆ ̸=0.
i
29B Proofs
B.1 Proof of Theorem 1
According to the conditioning on Q =Q , we have
Y y
(cid:18) η⊤η(cid:19)
Q =Q ⇔ I − Y =Q ⇔Y =a+bz,
Y y n′ ∥η∥2 y
where z =T(Y)∈R. Then, we have
{Y
∈Rn′
|M =M ,O =O ,Q =Q }
Y y Y y Y y
={Y ∈Rn′ |M =M ,O =O ,Y =a+bz,z ∈R}
Y y Y y
={a+bz ∈Rn′ |M =M ,O =O ,z ∈R}
a+bz y a+bz y
={a+bz
∈Rn′
|z ∈Z}.
Therefore, we obtain
T(Y)|{M =M ,O =O ,Q =Q }∼TN(η⊤µ,σ2∥η∥2,Z).
Y y Y y Y y
B.2 Proof of Theorem 2
By probability integral transformation, under the null hypothesis, we have
p |{M =M ,O =O ,Q =Q }∼Unif(0,1),
selective Y y Y y Y y
which leads to
P (p ≤α|M =M ,O =O ,Q =Q )=α, ∀α∈(0,1).
H0 selective Y y Y y Y y
For any α∈(0,1), by marginalizing over all the values of the nuisance parame-
ters, we obtain
P (p ≤α|M =M ,O =O )
H0 selective Y y Y y
(cid:90)
P (p ≤α|M =M ,O =O ,Q =Q )
H0 selective Y y Y y Y y
= Rn′
P (Q =Q |M =M ,O =O )dQ
H0 Y y Y y Y y y
(cid:90)
=α P (Q =Q |M =M ,O =O )dQ =α.
H0 Y y Y y Y y y
Rn′
30Therefore, we also obtain
P (p ≤α)
H0 selective
(cid:88) (cid:88)
= P (M ,O )P (p ≤α|M =M ,O =O )
H0 y y H0 selective Y y Y y
My∈2[p]Oy∈2[n]
(cid:88) (cid:88)
=α P (M ,O )=α.
H0 y y
My∈2[p]Oy∈2[n]
B.3 Proof of Theorem 3
It is sufficient to consider only z as input to Algorithm 1. In addition, as a
notation, we define G as the mapping that returns the last four components of
i
B for i∈{0,1,...,|V|}, i.e.,
i
G : R∋z (cid:55)→(Mi ,Oi ,li,ui)∈2[p]×2[n]×R2, i∈{0,1,...,|V|}
i a+bz a+bz z z
According to the above notation, all we have to show is that G (z) = G (r)
|V| |V|
foranyz ∈Randanyr ∈[l|V|,u|V|]. Weshowthisbymathematicalinduction.
z z
In the case i=0, it is obvious from the definition of B in Algorithm 1 that
0
G (z)=G (r)=([p],∅,−∞,∞) for any z ∈R and any r ∈[l0,u0]=[−∞,∞].
0 0 z z
Next,weassumethatforanyfixedi∈{0,...,|V|−1},G (z)=G (r)forany
j j
j ∈ {0,...,i}, any z ∈ R and any r ∈ [lj,uj]. Under this assumption, noting
z z
thatpa(i+1)⊂{0,...,i}fromapropertyoftopologicalsort, itisobviousthat
G (z) = G (r) for any z ∈ R and any r ∈ [li+1,ui+1] from the update rule
i+1 i+1 z z
of v described in §4.3.
i+1
31C Details of the Update Rules
UpdateRulufortheNodeofMVI. ThenodeofMVIimputesthemissing
valuesintheresponsevectora+bz. AllMVIalgorithmsconsideredinthisstudy
areexpressed aslineartransformationsdetermined onthebasisof X. Thus, let
D be the linear transformation matrix, the update rule should be as follows:
X
(X,a,b,z,M,O,l,u)(cid:55)→(X,D a,D b,z,M,O,l,u).
X X
Update Rule for the Node of FS. The node of FS selects the features
M′(z) from the dataset (X ,a +b z), which means that feature se-
−O,M −O −O
lectionisperformedonthedatasetextractedfrom(X,a+bz)basedonMand
O. For all FS algorithms considered in this study, the computation procedure
to obtain the interval [l ,u ]∋z, which satisfies
z z
∀r ∈[l ,u ], M′(r)=M′(z),
z z
have been proposed in previous studies [Lee and Taylor, 2014, Tibshirani et al.,
2016, Lee et al., 2016]. Utilizing this, the update rule should be as follows:
(X,a,b,z,M,O,l,u)(cid:55)→(X,a,b,z,M∩M′(z),O,max(l,l ),min(u,u )).
z z
Update Rule for the Node of OD. The node of OD detects the outliers
O′(z)fromthedataset(X ,a +b z),whichmeansthatoutlierdetection
−O,M M M
isperformedonthedatasetextractedfrom(X,a+bz)basedonMandO. For
allODalgorithmsconsideredinthisstudy,thecomputationproceduretoobtain
the interval [l ,u ]∋z, which satisfies
z z
∀r ∈[l ,u ], O′(r)=O′(z),
z z
have been proposed in previous studies [Chen and Bien, 2020]. Utilizing this,
the update rule should be as follows:
(X,a,b,z,M,O,l,u)(cid:55)→(X,a,b,z,M,O∩O′(z),max(l,l ),min(u,u )).
z z
32UpdateRulefortheNodeofUnion/IntersectionofFeatures/Outliers.
The node computes the union or intersection of selected features or detected
outliers. WithE beingthenumberofinputedges, foreachselectedfeatureand
detected outlier, the update rules should be as follows:
(cid:88)
{(X,a,b,z,M ,O,l ,u )} (cid:55)→(X,a,b,z, M ,O,maxl , min u ),
e e e e∈[E] e e e
e∈[E] e∈[E]
e∈[E]
(cid:88)
{(X,a,b,z,M,O ,l ,u )} (cid:55)→(X,a,b,z,M, O ,maxl , min u ),
e e e e∈[E] e e e
e∈[E] e∈[E]
e∈[E]
(cid:80)
where represents the union or intersection depending on the type of the
node.
33D Details of the Experiments
D.1 Methods for Comparison
We compared our proposed method with the following methods:
• oc: Our proposed method conditioning on the only one intervals [L ,U ]
z z
to which the observed test statistic T(y) belongs. This method is compu-
tationally efficient, however, its power is low due to over-conditioning.
• naive: This method uses a classical z-test without conditioning, i.e., we
compute the naive p-value as p =P (|T(Y)|≥|T(y)|).
naive H0
D.2 Additional Type I Error Rate Results
WealsoconductedexperimentstoinvestigatethetypeIerrorratewhenchang-
ingthenumberoffeaturesd∈{10,20,30,40}andsettingthenumberofsamples
n to 200. The null datasets were generated in the same way as in the main ex-
periments (§6), and the results are shown in Figure 5.
1.0 1.0 1.0
proposed proposed proposed
oc oc oc
0.8 naive 0.8 naive 0.8 naive
significance level significance level significance level
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
10 20 30 40 10 20 30 40 10 20 30 40
number of features number of features number of features
(a) op1 (b) op2 (c) all cv
Figure5: TypeIErrorRatewhenchangingthenumberoffeaturesd. Onlyour
proposed method and the oc method are able to control the type I error rate in
all settings for all types of pipelines.
D.3 Computer Resources
Allnumericalexperimentswereconductedonacomputerwitha96-core3.60GHz
CPU and 512GB of memory.
34
etaR
rorrE
I epyT
etaR
rorrE
I epyT
etaR
rorrE
I epyTE Robustness of Type I Error Rate Control
In this experiment, we confirmed the robustness of the proposed method for
all cv pipeline in terms of type I error rate control by applying our method to
thetwocases: thecasewhere the variance isestimated fromthe samedata and
the case where the noise is non-Gaussian.
E.1 Estimated Variance
In the case where the variance is estimated from the same data, we considered
the same two options as in type I error rate experiments in §6: number of
samples and number of features. For each setting, we generated 10,000 null
datasets (X,y), where X ∼ N(0,1), ∀(i,j) ∈ [n]×[d] and y ∼ N(0,I ) and
ij n
estimated the variance σˆ2 as
1
σˆ2 = ∥y−X(X⊤X)−1X⊤y∥2.
n−d 2
We considered the three significance levels α=0.05,0.01,0.10. The results are
shown in Figure 6 and our proposed method can properly control the type I
error rate.
0.20 0.20
alpha=0.05 alpha=0.05
alpha=0.01 alpha=0.01
alpha=0.10 alpha=0.10
0.15 0.15
0.10 0.10
0.05 0.05
0.01 0.01
0.00 0.00
100 200 300 400 10 20 30 40
number of samples number of features
(a) Number of Samples (b) Number of Features
Figure 6: Robustness of Type I Error Rate Control. Our proposed method can
robustly control the type I error rate even when the variance is estimated from
the same data.
35
etaR
rorrE
I epyT
etaR
rorrE
I epyTE.2 Non-Gaussian Noise
In the case where the noise is non-Gaussian, we set n = 200 and d = 20. As
non-Gaussian noise, we considered the following five distribution families:
• skewnorm: Skew normal distribution family.
• exponnorm: Exponentially modified normal distribution family.
• gennormsteep: Generalized normal distribution family (limit the shape
parameter β to be steeper than the normal distribution, i.e., β <2).
• gennormflat: Generalized normal distribution family (limit the shape
parameter β to be flatter than the normal distribution, i.e., β >2).
• t: Student’s t distribution family.
NotethatallofthesedistributionfamiliesincludetheGaussiandistributionand
are standardized in the experiment.
To conduct the experiment, we first obtained a distribution such that the
1-Wasserstein distance from N(0,1) is l in each distribution family, for l ∈
{0.01,0.02,0.03,0.04}. We then generated 10,000 null datasets (X,y), where
X ∼ N(0,1), ∀(i,j) ∈ [n]×[d] and y , ∀i ∈ [n] follows the obtained distri-
ij i
bution. We considered the two significance levels α = 0.05,0.01. The results
are shown in Figure 7 and our proposed method can properly control the type
I error rate.
360.20 0.05
skewnorm skewnorm
exponnorm exponnorm
gennormsteep gennormsteep
gennormflat 0.04 gennormflat
0.15 t t
0.03
0.10
0.02
0.05
0.01
0.00 0.00
0.01 0.02 0.03 0.04 0.01 0.02 0.03 0.04
Wasserstein Distance Wasserstein Distance
(a) Significance Level 0.05 (b) Significance Level 0.01
Figure 7: Robustness of Type I Error Rate Control. Our proposed method can
robustly control the type I error rate even when the noise follows non-Gaussian
distributions.
37
etaR
rorrE
I epyT
etaR
rorrE
I epyTF Automatic Pipeline Construction based on Cross-
Validation
In this section, we discuss cross-validation for pipelines. We consider selecting
thepipelineP fromagivensetofcandidates{P ,...,P }whereSisthenumber
1 S
of candidates. Note that this formulation is general enough to handle many
cross-validation targets in a unified form. For examples, (i) the case where only
changing the regularization strength of lasso node, (ii) the case where changing
the method of missing value imputation, and (iii) the case where changing the
all structure of the pipeline (i.e., type and order of nodes).
Thereafter,wediscusshowstatisticalinferencechangeswhencross-validation
is performed and how cross-validation can be formulated. Then, based on
above discussion, Algorithm 1 is extended to be applicable to the case of cross-
validation.
F.1 Statistical Inference after Cross-Validation
Changes in Statistical Inference As a formulation of statistical inference
aftercross-validation,thediscussionin§2and§3canbedoneinexactlythesame
way,withtheonlydifferencebeingtheprocedureforcomputingMandO(in§2
and §3, M and O are simply the outputs of a given mapping P representing a
target pipeline). This implies that the procedure for computing the truncation
intervals Z in §4 can not be directly applied to the case of cross-validation.
Formulation of Cross-Validation Procedure We consider the case where
K-fold cross-validation is performed. Let (X,y) be the observed data set and
{(T ,V )} betheK typesofpartitionoftrainingandvalidationsets,which
k k k∈[K]
satisfies T ,V ∈ 2[n], T ∩V = ∅, and T ∪V = [n] for any k ∈ [K]. Then,
k k k k k k
the cross-validation error E (X,y) for the pipeline P is defined as
s s
E (X,y)= (cid:88) 1 ∥(Ds y) −X βˆ (y)∥2,
s |V | X Vk Vk,Ms,k s,k 2
k
k∈[K]
38whereDs isthelineartransformationmatrixinthemissingvalueimputationof
X
(cid:16) (cid:17)−1
thepipelineP ,βˆ (y)= X⊤ X X⊤ (Ds y) ,
s s,k Tk\Os,k,Ms,k Tk\Os,k,Ms,k Tk\Os,k,Ms,k X Tk\Os,k
and (M ,O ) is the output of the pipeline P with input (X ,(Ds y) ).
s,k s,k s Tk X Tk
In K-fold cross-validation, the pipeline P is selected to minimize the cross-
s∗
validation error E (X,y), i.e., s∗ =argmin E (X,y).
s s∈[S] s
F.2 Auto-Conditioning for Cross-Validation
To conduct the statistical inference after cross-validation, it is suffice to have
the procedure to compute the interval [L ,U ] for any z ∈R which satisfy
z z
∀r ∈[L ,U ],
z z
argminE (X,a+br)=argminE (X,a+bz)(:=s∗),
s s
s∈[S] s∈[S]
P (X,a+br)=P (X,a+bz).
s∗ s∗
If we have this procedure, for any r ∈ [L ,U ], the selected features and the
z z
detected outliers after selecting the pipeline by cross-validation from the data
set(X,a+br)areinvariant. Therefore,thep canbecomputedinexactly
selective
thesamewayasin§4.2. Hereafter,weprovidetheaboveprocedurebyextending
Algorithm 1.
Forimplementationoftheaboveprocedure,wecomputetwointervals[Lcv,Ucv]
z z
and [Lsel,Usel] for any z ∈R which satisfy
z z
∀r ∈[Lcv,Ucv], argminE (X,a+br)=argminE (X,a+bz)(:=s∗),
z z s s
s∈[S] s∈[S]
∀r ∈[Lsel,Usel], P (X,a+br)=P (X,a+bz),
z z s∗ s∗
respectively, and let L =max(Lcv,Lsel) and U =min(Ucv,Usel).
z z z z z z
To compute the interval [Lcv,Ucv], we use Algorithm 1 repeatedly. For any
z z
(s,k)∈[S]×[K] and any z ∈R, we compute the interval [L(s,k),U(s,k)] which
z z
satisfy
∀r ∈[L(s,k),U(s,k)], P (X ,(Ds a+Ds br) )=P (X ,(Ds a+Ds bz) ),
z z s Tk X X Tk s Tk X X Tk
39by using Algorithm 1 with input (P ,X ,(Ds a+Ds bz) ,z). Thus, if we
s Tk X X Tk
consider the k-th term of the sum in E (X,a+br) as a function of r, then it
s
becomesquadraticinr ontheinterval[L(s,k),U(s,k)]. Therefore,ontheinterval
z z
∩ ∩ [L(s,k),U(s,k)], the cross-validation errors {E (X,a+br)} are
s∈[S] k∈[K] z z s s∈[S]
allquadraticinr. Onthisinterval∩ ∩ [L(s,k),U(s,k)],thesimultaneous
s∈[S] k∈[K] z z
inequalities for r
E (X,a+br)≤E (X,a+br),∀s∈[S],
s∗ s
with s∗ = argmin E (X,a+bz) become simultaneous quadratic inequali-
s∈[S] s
ties, which can be solved analytically to finally obtain the interval [Lcv,Ucv].
z z
To compute the interval [Lsel,Usel], we simply use Algorithm 1 with input
z z
(P ,X,a,b,z).
s∗
40G Examples of Implementations
We show an example of how the pipeline is implemented in our experiments.
Listing 2 shows a code example that defines the pipeline referred to as op2 in
the experiments. In this example, the pipeline can be defined by exactly the
same UI as in Listing 1. Listing 3 shows the implementation of the automatic
pipelineconstructionschemereferredtoasall cvintheexperiments. Notethat
wecanspecifythecandidatesoftheparametersforeachoperationandperform
cross-validation to determine the optimal pipeline by using fit method.
Listing 2: Code example that defines the pipeline referred to as op2 in the
experiments. Wecanimplementaninstanceofthepipelinewithinference
methodssimplybyspecifyingeachoperationinturn. Toperformhypothesis
testing,wecancalltheinferencemethodofthepipelineinstancewiththe
input dataset (X,y) and the deviation of the noise σ.
1 import numpy as np
2 import pipelineprocessor as plp
3
4 def option2():
5 X, y = plp.make_dataset()
6 y = plp.definite_regression_imputation(X, y)
7
8 M = plp.marginal_screening(X, y, 5)
9 X = plp.extract_features(X, M)
10
11 O = plp.cook_distance(X, y, 3.0)
12 X, y = plp.remove_outliers(X, y, O)
13
14 M1 = plp.stepwise_feature_selection(X, y, 3)
15 M2 = plp.lasso(X, y, 0.08)
16 M = plp.intersection(M1, M2)
17 return plp.make_pipeline(output=M)
18
19 pl = option2()
20 X, y = np.random.normal(size=(100, 10)), np.random.normal(size=100)
21 M, p_list = pl.inference(X, y, sigma=1.0)
41Listing 3: Code example that defines the automatic pipeline construction
scheme referred to as all cv in the experiments. We can implement an
instanceofthetunablepipelinewithfitmethodssimplybyspecifyingeach
operation and its candidates of parameters in turn. To perform hypothesis
testingaftercross-validation,wecancallthefitandinferencemethodof
the pipeline instance sequentially.
1 import numpy as np
2 import pipelineprocessor as plp
3
4 def option1_cv():
5 X, y = plp.make_dataset()
6 y = plp.mean_value_imputation(X, y)
7
8 O = plp.soft_ipod(X, y, 0.02, {0.02, 0.018})
9 X, y = plp.remove_outliers(X, y, O)
10
11 M = plp.marginal_screening(X, y, 5, {3, 5})
12 X = plp.extract_features(X, M)
13
14 M1 = plp.stepwise_feature_selection(X, y, 3, {2, 3})
15 M2 = plp.lasso(X, y, 0.08, {0.08, 0.12})
16 M = plp.union(M1, M2)
17 return plp.make_pipeline(output=M)
18
19 def option2_cv():
20 X, y = plp.make_dataset()
21 y = plp.definite_regression_imputation(X, y)
22
23 M = plp.marginal_screening(X, y, 5, {3, 5})
24 X = plp.extract_features(X, M)
25
26 O = plp.cook_distance(X, y, 3.0, {2.0, 3.0})
27 X, y = plp.remove_outliers(X, y, O)
28
29 M1 = plp.stepwise_feature_selection(X, y, 3, {2, 3})
30 M2 = plp.lasso(X, y, 0.08, {0.08, 0.12})
31 M = plp.intersection(M1, M2)
32 return plp.make_pipeline(output=M)
33
34 pls = plp.make_pipelines(option1_cv(), option2_cv())
35 X, y = np.random.normal(size=(100, 10)), np.random.normal(size=100)
36
37 pls.tune(X, y, n_iters=[16, 16], cv=5, random_state=0)
38 M, p_list = pls.inference(X, y, sigma=1.0)
42References
Shuxiao Chen and Jacob Bien. Valid inference corrected for outlier removal.
Journal of Computational and Graphical Statistics, 29(2):323–334, 2020.
Diptesh Das, Vo Nguyen Le Duy, Hiroyuki Hanada, Koji Tsuda, and Ichiro
Takeuchi. Fast and more powerful selective inference for sparse high-order
interaction model. arXiv preprint arXiv:2106.04929, 2021.
Iddo Drori, Yamuna Krishnamurthy, Remi Rampin, Raoni de Paula Lourenco,
Jorge Piazentin Ono, Kyunghyun Cho, Claudio Silva, and Juliana
Freire. Alphad3m: Machine learning pipeline synthesis. arXiv preprint
arXiv:2111.02508, 2021.
Vo Nguyen Le Duy and Ichiro Takeuchi. More powerful conditional selective
inference for generalized lasso by parametric programming. The Journal of
Machine Learning Research, 23(1):13544–13580, 2022.
Vo Nguyen Le Duy, Hiroki Toda, Ryota Sugiyama, and Ichiro Takeuchi. Com-
puting valid p-value for optimal changepoint by selective inference using dy-
namicprogramming. InAdvances in Neural Information Processing Systems,
2020.
VoNguyenLeDuy,ShogoIwazaki,andIchiroTakeuchi. Quantifyingstatistical
significance of neural network-based image segmentation by selective infer-
ence. Advances in Neural Information Processing Systems, 35:31627–31639,
2022.
William Fithian, Jonathan Taylor, Robert Tibshirani, and Ryan Tibshirani.
Selective sequential model selection. arXiv preprint arXiv:1512.02565, 2015.
LucyLGao,JacobBien,andDanielaWitten.Selectiveinferenceforhierarchical
clustering. Journal of the American Statistical Association,pages1–11,2022.
Hannes Hapke and Catherine Nelson. Building machine learning pipelines.
O’Reilly Media, 2020.
43Sangwon Hyun, Max G’sell, and Ryan J Tibshirani. Exact post-selection in-
ference for the generalized lasso path. Electronic Journal of Statistics, 12(1):
1053–1097, 2018.
SeanJewell,PaulFearnhead,andDanielaWitten. Testingforachangeinmean
afterchangepointdetection. Journal of the Royal Statistical Society Series B:
Statistical Methodology, 84(4):1082–1104, 2022.
ShubhraKantiKarmaker,MdMahadiHassan,MicahJSmith,LeiXu,Chengx-
iang Zhai, and Kalyan Veeramachaneni. Automl to date and beyond: Chal-
lenges and opportunities. ACM Computing Surveys (CSUR), 54(8):1–36,
2021.
VoNguyenLeDuy,Hsuan-TienLin,andIchiroTakeuchi. Cad-da: Controllable
anomaly detection after domain adaptation by statistical inference. In Inter-
nationalConferenceonArtificialIntelligenceandStatistics,pages1828–1836.
PMLR, 2024.
Jason D Lee and Jonathan E Taylor. Exact post model selection inference for
marginal screening. Advances in neural information processing systems, 27,
2014.
Jason D Lee, Yuekai Sun, and Jonathan E Taylor. Evaluating the statistical
significance of biclusters. Advances in neural information processing systems,
28, 2015.
Jason D Lee, Dennis L Sun, Yuekai Sun, and Jonathan E Taylor. Exact post-
selectioninference, withapplicationtothelasso. The Annals of Statistics, 44
(3):907–927, 2016.
KeliLiu,JelenaMarkovic,andRobertTibshirani. Morepowerfulpost-selection
inference, with application to the lasso. arXiv preprint arXiv:1801.09037,
2018.
44Daiki Miwa, Duy Vo Nguyen Le, and Ichiro Takeuchi. Valid p-value for deep
learning-driven salient region. In Proceedings of the 11th International Con-
ference on Learning Representation, 2023.
Anna C Neufeld, Lucy L Gao, and Daniela M Witten. Tree-values: selective
inferenceforregressiontrees. JournalofMachineLearningResearch,23(305):
1–43, 2022.
Snigdha Panigrahi, Kevin Fry, and Jonathan Taylor. Exact selective inference
with randomization. Biometrika, page asae019, 2024.
David Ru¨gamer and Sonja Greven. Inference for l 2-boosting. Statistics and
computing, 30(2):279–289, 2020.
TomohiroShiraishi,DaikiMiwa,TeruyukiKatsuoka,VoNguyenLeDuy,Koichi
Taji, and Ichiro Takeuchi. Statistical test for attention maps in vision trans-
formers. International Conference on Machine Learning, 2024.
Peter Sugimura and Florian Hartl. Building a reproducible machine learning
pipeline. arXiv preprint arXiv:1810.04570, 2018.
Shinya Suzumura, Kazuya Nakagawa, Yuta Umezu, Koji Tsuda, and Ichiro
Takeuchi. Selectiveinferenceforsparsehigh-orderinteractionmodels. InPro-
ceedings of the 34th International Conference on Machine Learning-Volume
70, pages 3338–3347. JMLR. org, 2017.
Kosuke Tanizaki, Noriaki Hashimoto, Yu Inatsu, Hidekata Hontani, and Ichiro
Takeuchi. Computing valid p-values for image segmentation by selective in-
ference. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 9553–9562, 2020.
Jonathan Taylor and Robert J Tibshirani. Statistical learning and selective
inference. Proceedings of the National Academy of Sciences, 112(25):7629–
7634, 2015.
45Xiaoying Tian and Jonathan Taylor. Selective inference with a randomized
response. The Annals of Statistics, 46(2):679–710, 2018.
RyanJTibshirani, JonathanTaylor, RichardLockhart,andRobertTibshirani.
Exact post-selection inference for sequential regression procedures. Journal
of the American Statistical Association, 111(514):600–620, 2016.
Toshiaki Tsukurimichi, Yu Inatsu, Vo Nguyen Le Duy, and Ichiro Takeuchi.
Conditional selective inference for robust regression and outlier detection us-
ingpiecewise-linearhomotopycontinuation.arXivpreprintarXiv:2104.10840,
2021.
Anatoly Yakovlev, Hesam Fathi Moghadam, Ali Moharrer, Jingxiao Cai, Nikan
Chavoshi, Venkatanathan Varadarajan, Sandeep R Agrawal, Sam Idicula,
TomasKarnagel,SanjayJinturkar,etal. Oracleautoml: afastandpredictive
automl pipeline. Proceedings of the VLDB Endowment, 13(12):3166–3180,
2020.
Makoto Yamada, Yuta Umezu, Kenji Fukumizu, and Ichiro Takeuchi. Post
selection inference with kernels. In International conference on artificial in-
telligence and statistics, pages 152–160. PMLR, 2018.
ChengrunYang,JicongFan,ZiyangWu,andMadeleineUdell. Automlpipeline
selection: Efficientlynavigatingthecombinatorialspace. Inproceedingsofthe
26th ACM SIGKDD international conference on knowledge discovery & data
mining, pages 1446–1456, 2020.
Fan Yang, Rina Foygel Barber, Prateek Jain, and John Lafferty. Selective in-
ference for group-sparse linear models. In Advances in Neural Information
Processing Systems, pages 2469–2477, 2016.
46