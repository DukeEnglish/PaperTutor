[
    {
        "title": "Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos",
        "authors": "Tarun KalluriBodhisattwa Prasad MajumderManmohan Chandraker",
        "links": "http://arxiv.org/abs/2403.05535v1",
        "entry_id": "http://arxiv.org/abs/2403.05535v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05535v1",
        "summary": "We introduce LaGTran, a novel framework that utilizes readily available or\neasily acquired text descriptions to guide robust transfer of discriminative\nknowledge from labeled source to unlabeled target data with domain shifts.\nWhile unsupervised adaptation methods have been established to address this\nproblem, they show limitations in handling challenging domain shifts due to\ntheir exclusive operation within the pixel-space. Motivated by our observation\nthat semantically richer text modality has more favorable transfer properties,\nwe devise a transfer mechanism to use a source-trained text-classifier to\ngenerate predictions on the target text descriptions, and utilize these\npredictions as supervision for the corresponding images. Our approach driven by\nlanguage guidance is surprisingly easy and simple, yet significantly\noutperforms all prior approaches on challenging datasets like GeoNet and\nDomainNet, validating its extreme effectiveness. To further extend the scope of\nour study beyond images, we introduce a new benchmark to study ego-exo transfer\nin videos and find that our language-aided LaGTran yields significant gains in\nthis highly challenging and non-trivial transfer setting. Code, models, and\nproposed datasets are publicly available at\nhttps://tarun005.github.io/lagtran/.",
        "updated": "2024-03-08 18:58:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05535v1"
    },
    {
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
        "authors": "Machel ReidNikolay SavinovDenis TeplyashinDmitry LepikhinTimothy LillicrapJean-baptiste AlayracRadu SoricutAngeliki LazaridouOrhan FiratJulian SchrittwieserIoannis AntonoglouRohan AnilSebastian BorgeaudAndrew DaiKatie MillicanEthan DyerMia GlaeseThibault SottiauxBenjamin LeeFabio ViolaMalcolm ReynoldsYuanzhong XuJames MolloyJilin ChenMichael IsardPaul BarhamTom HenniganRoss McIlroyMelvin JohnsonJohan SchalkwykEli CollinsEliza RutherfordErica MoreiraKareem AyoubMegha GoelClemens MeyerGregory ThorntonZhen YangHenryk MichalewskiZaheer AbbasNathan SchucherAnkesh AnandRichard IvesJames KeelingKarel LencSalem HaykalSiamak ShakeriPranav ShyamAakanksha ChowdheryRoman RingStephen SpencerEren SezenerLuke VilnisOscar ChangNobuyuki MoriokaGeorge TuckerCe ZhengOliver WoodmanNithya AttaluriTomas KociskyEvgenii EltyshevXi ChenTimothy ChungVittorio SeloSiddhartha BrahmaPetko GeorgievAmbrose SloneZhenkai ZhuJames LottesSiyuan QiaoBen CaineSebastian RiedelAlex TomalaMartin ChadwickJuliette LovePeter ChoySid MittalNeil HoulsbyYunhao TangMatthew LammLibin BaiQiao ZhangLuheng HeYong ChengPeter HumphreysYujia LiSergey BrinAlbin CassirerYingjie MiaoLukas ZilkaTaylor TobinKelvin XuLev ProleevDaniel SohnAlberto MagniLisa Anne HendricksIsabel GaoSantiago OntañónOskar BunyanNathan ByrdAbhanshu SharmaBiao ZhangMario PintoRishika SinhaHarsh MehtaDawei JiaSergi CaellesAlbert WebsonAlex MorrisBecca RoelofsYifan DingRobin StrudelXuehan XiongMarvin RitterMostafa DehghaniRahma ChaabouniAbhijit KarmarkarGuangda LaiFabian MentzerBibo XuYaGuang LiYujing ZhangTom Le PaineAlex GoldinBehnam NeyshaburKate BaumliAnselm LevskayaMichael LaskinWenhao JiaJack W. RaeKefan XiaoAntoine HeSkye GiordanoLakshman YagatiJean-Baptiste LespiauPaul NatsevSanjay GanapathyFangyu LiuDanilo MartinsNanxin ChenYunhan XuMegan BarnesRhys MayArpi VezerJunhyuk OhKen FrankoSophie BridgersRuizhe ZhaoBoxi WuBasil MustafaSean SechristEmilio ParisottoThanumalayan Sankaranarayana PillaiChris LarkinChenjie GuChristina SorokinMaxim KrikunAlexey GuseynovJessica LandonRomina DattaAlexander PritzelPhoebe ThackerFan YangKevin HuiAnja HauthChih-Kuan YehDavid BarkerJustin Mao-JonesSophia AustinHannah SheahanParker SchuhJames SvenssonRohan JainVinay RamaseshAnton BriukhovDa-Woon ChungTamara von GlehnChristina ButterfieldPriya JhakraMatthew WiethoffJustin FryeJordan GrimstadBeer ChangpinyoCharline Le LanAnna BortsovaYonghui WuPaul VoigtlaenderTara SainathCharlotte SmithWill HawkinsKris CaoJames BesleySrivatsan SrinivasanMark OmernickColin GaffneyGabriela SuritaRyan BurnellBogdan DamocJunwhan AhnAndrew BrockMantas PajarskasAnastasia PetrushkinaSeb NouryLorenzo BlancoKevin SwerskyArun AhujaThi AvrahamiVedant MisraRaoul de LiedekerkeMariko IinumaAlex PolozovSarah YorkGeorge van den DriesschePaul MichelJustin ChiuRory BlevinsZach GleicherAdrià RecasensAlban RrustemiElena GribovskayaAurko RoyWiktor GworekSéb ArnoldLisa LeeJames Lee-ThorpMarcello MaggioniEnrique PiquerasKartikeya BadolaSharad VikramLucas GonzalezAnirudh BaddepudiEvan SenterJacob DevlinJames QinMichael AzzamMaja TrebaczMartin PolacekKashyap KrishnakumarShuo-yiin ChangMatthew TungIvo PenchevRishabh JoshiKate OlszewskaCarrie MuirMateo WirthAle Jakse HartmanJosh NewlanSheleem KashemVijay BolinaElahe DabirJoost van AmersfoortZafarali AhmedJames Cobon-KerrAishwarya KamathArnar Mar HrafnkelssonLe HouIan MackinnonAlexandre FrechetteEric NolandXiance SiEmanuel TaropaDong LiPhil CroneAnmol GulatiSébastien CeveyJonas AdlerAda MaDavid SilverSimon TokumineRichard PowellStephan LeeMichael ChangSamer HassanDiana MincuAntoine YangNir LevineJenny BrennanMingqiu WangSarah HodkinsonJeffrey ZhaoJosh LipschultzAedan PopeMichael B. ChangCheng LiLaurent El ShafeyMichela PaganiniSholto DouglasBernd BohnetFabio PardoSeth OdoomMihaela RoscaCicero Nogueira dos SantosKedar SoparkarArthur GuezTom HudsonSteven HansenChulayuth AsawaroengchaiRavi AddankiTianhe YuWojciech StokowiecMina KhanJustin GilmerJaehoon LeeCarrie Grimes BostockKeran RongJonathan CatonPedram PejmanFilip PaveticGeoff BrownVivek SharmaMario LučićRajkumar SamuelJosip DjolongaAmol MandhaneLars Lowe SjösundElena BuchatskayaElspeth WhiteNatalie ClayJiepu JiangHyeontaek LimRoss HemsleyJane LabanowskiNicola De CaoDavid SteinerSayed Hadi HashemiJacob AustinAnita GergelyTim BlythJoe StantonKaushik ShivakumarAditya SiddhantAnders AndreassenCarlos ArayaNikhil SethiRakesh ShivannaSteven HandAnkur BapnaAli KhodaeiAntoine MiechGarrett TanzerAndy SwingShantanu ThakoorZhufeng PanZachary NadoStephanie WinklerDian YuMohammad SalehLoren MaggioreIain BarrMinh GiangThais KagoharaIvo DanihelkaAmit MaratheVladimir FeinbergMohamed ElhawatyNimesh GhelaniDan HorganHelen MillerLexi WalkerRichard TanburnMukarram TariqDisha ShrivastavaFei XiaChung-Cheng ChiuZoe AshwoodKhuslen BaatarsukhSina SamangooeiFred AlcoberAxel StjerngrenPaul KomarekKaterina TsihlasAnudhyan BoralRamona ComanescuJeremy ChenRuibo LiuDawn BloxwichCharlie ChenYanhua SunFangxiaoyu FengMatthew MaugerXerxes DotiwallaVincent HellendoornMichael SharmanIvy ZhengKrishna HaridasanGabe Barth-MaronCraig SwansonDominika RogozińskaAlek AndreevPaul Kishan RubensteinRuoxin SangDan HurtGamaleldin ElsayedRenshen WangDave LaceyAnastasija IlićYao ZhaoLora AroyoChimezie IwuanyanwuVitaly NikolaevBalaji LakshminarayananSadegh JazayeriRaphaël Lopez KaufmanMani VaradarajanChetan TekurDoug FritzMisha KhalmanDavid ReitterKingshuk DasguptaShourya SarcarTina OrnduffJavier SnaiderFantine HuotJohnson JiaRupert KempNejc TrdinAnitha VijayakumarLucy KimChristof AngermuellerLi LaoTianqi LiuHaibin ZhangDavid EngelSomer GreeneAnaïs WhiteJessica AustinLilly TaylorShereen AshrafDangyi LiuMaria GeorgakiIrene CaiYana KulizhskayaSonam GoenkaBrennan SaetaKiran VodrahalliChristian FrankDario de CesareBrona RobenekHarry RichardsonMahmoud AlnahlawiChristopher YewPriya PonnapalliMarco TagliasacchiAlex KorchemniyYelin KimDinghua LiBill RosgenZoe AshwoodKyle LevinJeremy WiesnerPraseem BanzalPraveen SrinivasanHongkun YuÇağlar ÜnlüDavid ReidZora TungDaniel FinchelsteinRavin KumarAndre ElisseeffJin HuangMing ZhangRui ZhuRicardo AguilarMai GiménezJiawei XiaOlivier DousseWilli GierkeSoheil Hassas YeganehDamion YatesKomal JalanLu LiEri Latorre-ChimotoDuc Dung NguyenKen DurdenPraveen KallakuriYaxin LiuMatthew JohnsonTomy TsaiAlice TalbertJasmine LiuAlexander NeitzChen ElkindMarco SelviMimi JasarevicLivio Baldini SoaresAlbert CuiPidong WangAlek Wenjiao WangXinyu YeKrystal KallarackalLucia LoherHoi LamJosef BroderDan Holtmann-RiceNina MartinBramandia RamadhanaDaniel ToyamaMrinal ShuklaSujoy BasuAbhi MohanNick FernandoNoah FiedelKim PatersonHui LiAnkush GargJane ParkDongHyun ChoiDiane WuSankalp SinghZhishuai ZhangAmir GlobersonLily YuJohn CarpenterFélix de Chaumont QuitryCarey RadebaughChu-Cheng LinAlex TudorPrakash ShroffDrew GarmonDayou DuNeera VatsHan LuShariq IqbalAlex YakubovichNilesh TripuraneniJames ManyikaHaroon QureshiNan HuaChristel NganiMaria Abi RaadHannah ForbesAnna BulanovaJeff StanwayMukund SundararajanVictor UngureanuColton BishopYunjie LiBalaji VenkatramanBo LiChloe ThorntonSalvatore ScellatoNishesh GuptaYicheng WangIan TenneyXihui WuAshish ShenoyGabriel CarvajalDiana Gage WrightBen BariachZhuyun XiaoPeter HawkinsSid DalmiaClement FarabetPedro ValenzuelaQuan YuanChris WeltyAnanth AgarwalMia ChenWooyeol KimBrice HulseNandita DukkipatiAdam PaszkeAndrew BoltElnaz DavoodiKiam ChooJennifer BeattieJennifer PrendkiHarsha VashishtRebeca Santamaria-FernandezLuis C. CoboJarek WilkiewiczDavid MadrasAli ElqurshGrant UyKevin RamirezMatt HarveyTyler LiechtyHeiga ZenJeff SeibertClara Huiyi HuMohamed ElhawatyAndrey KhorlinMaigo LeAsaf AharoniMegan LiLily WangSandeep KumarAlejandro LinceNorman CasagrandeJay HooverDalia El BadawyDavid SoergelDenis VnukovMatt MiecnikowskiJiri SimsaAnna KoopPraveen KumarThibault SellamDaniel VlasicSamira DarukiNir ShabatJohn ZhangGuolong SuJiageng ZhangJeremiah LiuYi SunEvan PalmerAlireza GhaffarkhahXi XiongVictor CotrutaMichael FinkLucas DixonAshwin SreevatsaAdrian GoedeckemeyerAlek DimitrievMohsen JafariRemi CrockerNicholas FitzGeraldAviral KumarSanjay GhemawatIvan PhilipsFrederick LiuYannie LiangRachel SterneckAlena RepinaMarcus WuLaura KnightMarin GeorgievHyo LeeHarry AskhamAbhishek ChakladarAnnie LouisCarl CrousHardie CateDessie PetrovaMichael QuinnDenese Owusu-AfriyieAchintya SinghalNan WeiSolomon KimDamien VincentMilad NasrChristopher A. Choquette-ChooReiko TojoShawn LuDiego de Las CasasYuchung ChengTolga BolukbasiKatherine LeeSaaber FatehiRajagopal AnanthanarayananMiteyan PatelCharbel KaedJing LiJakub SygnowskiShreyas Rammohan BelleZhe ChenJaclyn KonzelmannSiim PõderRoopal GargVinod KoverkathuAdam BrownChris DyerRosanne LiuAzade NovaJun XuSlav PetrovDemis HassabisKoray KavukcuogluJeffrey DeanOriol Vinyals",
        "links": "http://arxiv.org/abs/2403.05530v1",
        "entry_id": "http://arxiv.org/abs/2403.05530v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05530v1",
        "summary": "In this report, we present the latest model of the Gemini family, Gemini 1.5\nPro, a highly compute-efficient multimodal mixture-of-experts model capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio.\nGemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks\nacross modalities, improves the state-of-the-art in long-document QA,\nlong-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's\nstate-of-the-art performance across a broad set of benchmarks. Studying the\nlimits of Gemini 1.5 Pro's long-context ability, we find continued improvement\nin next-token prediction and near-perfect retrieval (>99%) up to at least 10M\ntokens, a generational leap over existing models such as Claude 2.1 (200k) and\nGPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large\nlanguage models at the frontier; when given a grammar manual for Kalamang, a\nlanguage with fewer than 200 speakers worldwide, the model learns to translate\nEnglish to Kalamang at a similar level to a person who learned from the same\ncontent.",
        "updated": "2024-03-08 18:54:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05530v1"
    },
    {
        "title": "GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM",
        "authors": "Hao KangQingru ZhangSouvik KunduGeonhwa JeongZaoxing LiuTushar KrishnaTuo Zhao",
        "links": "http://arxiv.org/abs/2403.05527v1",
        "entry_id": "http://arxiv.org/abs/2403.05527v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05527v1",
        "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
        "updated": "2024-03-08 18:48:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05527v1"
    },
    {
        "title": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
        "authors": "Haoyu LuWen LiuBo ZhangBingxuan WangKai DongBo LiuJingxiang SunTongzheng RenZhuoshu LiYaofeng SunChengqi DengHanwei XuZhenda XieChong Ruan",
        "links": "http://arxiv.org/abs/2403.05525v1",
        "entry_id": "http://arxiv.org/abs/2403.05525v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05525v1",
        "summary": "We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed\nfor real-world vision and language understanding applications. Our approach is\nstructured around three key dimensions:\n  We strive to ensure our data is diverse, scalable, and extensively covers\nreal-world scenarios including web screenshots, PDFs, OCR, charts, and\nknowledge-based content, aiming for a comprehensive representation of practical\ncontexts. Further, we create a use case taxonomy from real user scenarios and\nconstruct an instruction tuning dataset accordingly. The fine-tuning with this\ndataset substantially improves the model's user experience in practical\napplications. Considering efficiency and the demands of most real-world\nscenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently\nprocesses high-resolution images (1024 x 1024), while maintaining a relatively\nlow computational overhead. This design choice ensures the model's ability to\ncapture critical semantic and detailed information across various visual tasks.\nWe posit that a proficient Vision-Language Model should, foremost, possess\nstrong language abilities. To ensure the preservation of LLM capabilities\nduring pretraining, we investigate an effective VL pretraining strategy by\nintegrating LLM training from the beginning and carefully managing the\ncompetitive dynamics observed between vision and language modalities.\n  The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user\nexperiences as a vision-language chatbot in real-world applications, achieving\nstate-of-the-art or competitive performance across a wide range of\nvisual-language benchmarks at the same model size while maintaining robust\nperformance on language-centric benchmarks. We have made both 1.3B and 7B\nmodels publicly accessible to foster innovations based on this foundation\nmodel.",
        "updated": "2024-03-08 18:46:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05525v1"
    },
    {
        "title": "Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought",
        "authors": "James ChuaEdward ReesHunar BatraSamuel R. BowmanJulian MichaelEthan PerezMiles Turpin",
        "links": "http://arxiv.org/abs/2403.05518v1",
        "entry_id": "http://arxiv.org/abs/2403.05518v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05518v1",
        "summary": "While chain-of-thought prompting (CoT) has the potential to improve the\nexplainability of language model reasoning, it can systematically misrepresent\nthe factors influencing models' behavior--for example, rationalizing answers in\nline with a user's opinion without mentioning this bias. To mitigate this\nbiased reasoning problem, we introduce bias-augmented consistency training\n(BCT), an unsupervised fine-tuning scheme that trains models to give consistent\nreasoning across prompts with and without biasing features. We construct a\nsuite testing nine forms of biased reasoning on seven question-answering tasks,\nand find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of\nbiased reasoning by 86% on held-out tasks. Moreover, this model generalizes to\nother forms of bias, reducing biased reasoning on held-out biases by an average\nof 37%. As BCT generalizes to held-out biases and does not require gold labels,\nthis method may hold promise for reducing biased reasoning from as-of-yet\nunknown biases and on tasks where supervision for ground truth reasoning is\nunavailable.",
        "updated": "2024-03-08 18:41:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05518v1"
    }
]