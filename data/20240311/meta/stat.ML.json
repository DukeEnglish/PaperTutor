[
    {
        "title": "The Computational Complexity of Learning Gaussian Single-Index Models",
        "authors": "Alex DamianLoucas Pillaud-VivienJason D. LeeJoan Bruna",
        "links": "http://arxiv.org/abs/2403.05529v1",
        "entry_id": "http://arxiv.org/abs/2403.05529v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05529v1",
        "summary": "Single-Index Models are high-dimensional regression problems with planted\nstructure, whereby labels depend on an unknown one-dimensional projection of\nthe input via a generic, non-linear, and potentially non-deterministic\ntransformation. As such, they encompass a broad class of statistical inference\ntasks, and provide a rich template to study statistical and computational\ntrade-offs in the high-dimensional regime.\n  While the information-theoretic sample complexity to recover the hidden\ndirection is linear in the dimension $d$, we show that computationally\nefficient algorithms, both within the Statistical Query (SQ) and the Low-Degree\nPolynomial (LDP) framework, necessarily require $\\Omega(d^{k^\\star/2})$\nsamples, where $k^\\star$ is a \"generative\" exponent associated with the model\nthat we explicitly characterize. Moreover, we show that this sample complexity\nis also sufficient, by establishing matching upper bounds using a partial-trace\nalgorithm. Therefore, our results provide evidence of a sharp\ncomputational-to-statistical gap (under both the SQ and LDP class) whenever\n$k^\\star>2$. To complete the study, we provide examples of smooth and Lipschitz\ndeterministic target functions with arbitrarily large generative exponents\n$k^\\star$.",
        "updated": "2024-03-08 18:50:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05529v1"
    },
    {
        "title": "Poly-View Contrastive Learning",
        "authors": "Amitis ShidaniDevon HjelmJason RamapuramRuss WebbEeshan Gunesh DhekaneDan Busbridge",
        "links": "http://arxiv.org/abs/2403.05490v1",
        "entry_id": "http://arxiv.org/abs/2403.05490v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05490v1",
        "summary": "Contrastive learning typically matches pairs of related views among a number\nof unrelated negative views. Views can be generated (e.g. by augmentations) or\nbe observed. We investigate matching when there are more than two related views\nwhich we call poly-view tasks, and derive new representation learning\nobjectives using information maximization and sufficient statistics. We show\nthat with unlimited computation, one should maximize the number of related\nviews, and with a fixed compute budget, it is beneficial to decrease the number\nof unique samples whilst increasing the number of views of those samples. In\nparticular, poly-view contrastive models trained for 128 epochs with batch size\n256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k,\nchallenging the belief that contrastive models require large batch sizes and\nmany training epochs.",
        "updated": "2024-03-08 17:55:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05490v1"
    },
    {
        "title": "An Improved Algorithm for Learning Drifting Discrete Distributions",
        "authors": "Alessio Mazzetto",
        "links": "http://arxiv.org/abs/2403.05446v1",
        "entry_id": "http://arxiv.org/abs/2403.05446v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05446v1",
        "summary": "We present a new adaptive algorithm for learning discrete distributions under\ndistribution drift. In this setting, we observe a sequence of independent\nsamples from a discrete distribution that is changing over time, and the goal\nis to estimate the current distribution. Since we have access to only a single\nsample for each time step, a good estimation requires a careful choice of the\nnumber of past samples to use. To use more samples, we must resort to samples\nfurther in the past, and we incur a drift error due to the bias introduced by\nthe change in distribution. On the other hand, if we use a small number of past\nsamples, we incur a large statistical error as the estimation has a high\nvariance. We present a novel adaptive algorithm that can solve this trade-off\nwithout any prior knowledge of the drift. Unlike previous adaptive results, our\nalgorithm characterizes the statistical error using data-dependent bounds. This\ntechnicality enables us to overcome the limitations of the previous work that\nrequire a fixed finite support whose size is known in advance and that cannot\nchange over time. Additionally, we can obtain tighter bounds depending on the\ncomplexity of the drifting distribution, and also consider distributions with\ninfinite support.",
        "updated": "2024-03-08 16:54:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05446v1"
    },
    {
        "title": "An Adaptive Dimension Reduction Estimation Method for High-dimensional Bayesian Optimization",
        "authors": "Shouri HuJiawei LiZhibo Cai",
        "links": "http://arxiv.org/abs/2403.05425v1",
        "entry_id": "http://arxiv.org/abs/2403.05425v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05425v1",
        "summary": "Bayesian optimization (BO) has shown impressive results in a variety of\napplications within low-to-moderate dimensional Euclidean spaces. However,\nextending BO to high-dimensional settings remains a significant challenge. We\naddress this challenge by proposing a two-step optimization framework.\nInitially, we identify the effective dimension reduction (EDR) subspace for the\nobjective function using the minimum average variance estimation (MAVE) method.\nSubsequently, we construct a Gaussian process model within this EDR subspace\nand optimize it using the expected improvement criterion. Our algorithm offers\nthe flexibility to operate these steps either concurrently or in sequence. In\nthe sequential approach, we meticulously balance the exploration-exploitation\ntrade-off by distributing the sampling budget between subspace estimation and\nfunction optimization, and the convergence rate of our algorithm in\nhigh-dimensional contexts has been established. Numerical experiments validate\nthe efficacy of our method in challenging scenarios.",
        "updated": "2024-03-08 16:21:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05425v1"
    },
    {
        "title": "Variational Inference of Parameters in Opinion Dynamics Models",
        "authors": "Jacopo LentiFabrizio SilvestriGianmarco De Francisci Morales",
        "links": "http://arxiv.org/abs/2403.05358v1",
        "entry_id": "http://arxiv.org/abs/2403.05358v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05358v1",
        "summary": "Despite the frequent use of agent-based models (ABMs) for studying social\nphenomena, parameter estimation remains a challenge, often relying on costly\nsimulation-based heuristics. This work uses variational inference to estimate\nthe parameters of an opinion dynamics ABM, by transforming the estimation\nproblem into an optimization task that can be solved directly.\n  Our proposal relies on probabilistic generative ABMs (PGABMs): we start by\nsynthesizing a probabilistic generative model from the ABM rules. Then, we\ntransform the inference process into an optimization problem suitable for\nautomatic differentiation. In particular, we use the Gumbel-Softmax\nreparameterization for categorical agent attributes and stochastic variational\ninference for parameter estimation. Furthermore, we explore the trade-offs of\nusing variational distributions with different complexity: normal distributions\nand normalizing flows.\n  We validate our method on a bounded confidence model with agent roles\n(leaders and followers). Our approach estimates both macroscopic (bounded\nconfidence intervals and backfire thresholds) and microscopic ($200$\ncategorical, agent-level roles) more accurately than simulation-based and MCMC\nmethods. Consequently, our technique enables experts to tune and validate their\nABMs against real-world observations, thus providing insights into human\nbehavior in social systems via data-driven analysis.",
        "updated": "2024-03-08 14:45:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05358v1"
    }
]