[
    {
        "title": "Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos",
        "authors": "Tarun KalluriBodhisattwa Prasad MajumderManmohan Chandraker",
        "links": "http://arxiv.org/abs/2403.05535v1",
        "entry_id": "http://arxiv.org/abs/2403.05535v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05535v1",
        "summary": "We introduce LaGTran, a novel framework that utilizes readily available or\neasily acquired text descriptions to guide robust transfer of discriminative\nknowledge from labeled source to unlabeled target data with domain shifts.\nWhile unsupervised adaptation methods have been established to address this\nproblem, they show limitations in handling challenging domain shifts due to\ntheir exclusive operation within the pixel-space. Motivated by our observation\nthat semantically richer text modality has more favorable transfer properties,\nwe devise a transfer mechanism to use a source-trained text-classifier to\ngenerate predictions on the target text descriptions, and utilize these\npredictions as supervision for the corresponding images. Our approach driven by\nlanguage guidance is surprisingly easy and simple, yet significantly\noutperforms all prior approaches on challenging datasets like GeoNet and\nDomainNet, validating its extreme effectiveness. To further extend the scope of\nour study beyond images, we introduce a new benchmark to study ego-exo transfer\nin videos and find that our language-aided LaGTran yields significant gains in\nthis highly challenging and non-trivial transfer setting. Code, models, and\nproposed datasets are publicly available at\nhttps://tarun005.github.io/lagtran/.",
        "updated": "2024-03-08 18:58:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05535v1"
    },
    {
        "title": "Bayesian Preference Elicitation with Language Models",
        "authors": "Kunal HandaYarin GalEllie PavlickNoah GoodmanJacob AndreasAlex TamkinBelinda Z. Li",
        "links": "http://arxiv.org/abs/2403.05534v1",
        "entry_id": "http://arxiv.org/abs/2403.05534v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05534v1",
        "summary": "Aligning AI systems to users' interests requires understanding and\nincorporating humans' complex values and preferences. Recently, language models\n(LMs) have been used to gather information about the preferences of human\nusers. This preference data can be used to fine-tune or guide other LMs and/or\nAI systems. However, LMs have been shown to struggle with crucial aspects of\npreference learning: quantifying uncertainty, modeling human mental states, and\nasking informative questions. These challenges have been addressed in other\nareas of machine learning, such as Bayesian Optimal Experimental Design (BOED),\nwhich focus on designing informative queries within a well-defined feature\nspace. But these methods, in turn, are difficult to scale and apply to\nreal-world problems where simply identifying the relevant features can be\ndifficult. We introduce OPEN (Optimal Preference Elicitation with Natural\nlanguage) a framework that uses BOED to guide the choice of informative\nquestions and an LM to extract features and translate abstract BOED queries\ninto natural language questions. By combining the flexibility of LMs with the\nrigor of BOED, OPEN can optimize the informativity of queries while remaining\nadaptable to real-world domains. In user studies, we find that OPEN outperforms\nexisting LM- and BOED-based methods for preference elicitation.",
        "updated": "2024-03-08 18:57:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05534v1"
    },
    {
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
        "authors": "Machel ReidNikolay SavinovDenis TeplyashinDmitry LepikhinTimothy LillicrapJean-baptiste AlayracRadu SoricutAngeliki LazaridouOrhan FiratJulian SchrittwieserIoannis AntonoglouRohan AnilSebastian BorgeaudAndrew DaiKatie MillicanEthan DyerMia GlaeseThibault SottiauxBenjamin LeeFabio ViolaMalcolm ReynoldsYuanzhong XuJames MolloyJilin ChenMichael IsardPaul BarhamTom HenniganRoss McIlroyMelvin JohnsonJohan SchalkwykEli CollinsEliza RutherfordErica MoreiraKareem AyoubMegha GoelClemens MeyerGregory ThorntonZhen YangHenryk MichalewskiZaheer AbbasNathan SchucherAnkesh AnandRichard IvesJames KeelingKarel LencSalem HaykalSiamak ShakeriPranav ShyamAakanksha ChowdheryRoman RingStephen SpencerEren SezenerLuke VilnisOscar ChangNobuyuki MoriokaGeorge TuckerCe ZhengOliver WoodmanNithya AttaluriTomas KociskyEvgenii EltyshevXi ChenTimothy ChungVittorio SeloSiddhartha BrahmaPetko GeorgievAmbrose SloneZhenkai ZhuJames LottesSiyuan QiaoBen CaineSebastian RiedelAlex TomalaMartin ChadwickJuliette LovePeter ChoySid MittalNeil HoulsbyYunhao TangMatthew LammLibin BaiQiao ZhangLuheng HeYong ChengPeter HumphreysYujia LiSergey BrinAlbin CassirerYingjie MiaoLukas ZilkaTaylor TobinKelvin XuLev ProleevDaniel SohnAlberto MagniLisa Anne HendricksIsabel GaoSantiago OntañónOskar BunyanNathan ByrdAbhanshu SharmaBiao ZhangMario PintoRishika SinhaHarsh MehtaDawei JiaSergi CaellesAlbert WebsonAlex MorrisBecca RoelofsYifan DingRobin StrudelXuehan XiongMarvin RitterMostafa DehghaniRahma ChaabouniAbhijit KarmarkarGuangda LaiFabian MentzerBibo XuYaGuang LiYujing ZhangTom Le PaineAlex GoldinBehnam NeyshaburKate BaumliAnselm LevskayaMichael LaskinWenhao JiaJack W. RaeKefan XiaoAntoine HeSkye GiordanoLakshman YagatiJean-Baptiste LespiauPaul NatsevSanjay GanapathyFangyu LiuDanilo MartinsNanxin ChenYunhan XuMegan BarnesRhys MayArpi VezerJunhyuk OhKen FrankoSophie BridgersRuizhe ZhaoBoxi WuBasil MustafaSean SechristEmilio ParisottoThanumalayan Sankaranarayana PillaiChris LarkinChenjie GuChristina SorokinMaxim KrikunAlexey GuseynovJessica LandonRomina DattaAlexander PritzelPhoebe ThackerFan YangKevin HuiAnja HauthChih-Kuan YehDavid BarkerJustin Mao-JonesSophia AustinHannah SheahanParker SchuhJames SvenssonRohan JainVinay RamaseshAnton BriukhovDa-Woon ChungTamara von GlehnChristina ButterfieldPriya JhakraMatthew WiethoffJustin FryeJordan GrimstadBeer ChangpinyoCharline Le LanAnna BortsovaYonghui WuPaul VoigtlaenderTara SainathCharlotte SmithWill HawkinsKris CaoJames BesleySrivatsan SrinivasanMark OmernickColin GaffneyGabriela SuritaRyan BurnellBogdan DamocJunwhan AhnAndrew BrockMantas PajarskasAnastasia PetrushkinaSeb NouryLorenzo BlancoKevin SwerskyArun AhujaThi AvrahamiVedant MisraRaoul de LiedekerkeMariko IinumaAlex PolozovSarah YorkGeorge van den DriesschePaul MichelJustin ChiuRory BlevinsZach GleicherAdrià RecasensAlban RrustemiElena GribovskayaAurko RoyWiktor GworekSéb ArnoldLisa LeeJames Lee-ThorpMarcello MaggioniEnrique PiquerasKartikeya BadolaSharad VikramLucas GonzalezAnirudh BaddepudiEvan SenterJacob DevlinJames QinMichael AzzamMaja TrebaczMartin PolacekKashyap KrishnakumarShuo-yiin ChangMatthew TungIvo PenchevRishabh JoshiKate OlszewskaCarrie MuirMateo WirthAle Jakse HartmanJosh NewlanSheleem KashemVijay BolinaElahe DabirJoost van AmersfoortZafarali AhmedJames Cobon-KerrAishwarya KamathArnar Mar HrafnkelssonLe HouIan MackinnonAlexandre FrechetteEric NolandXiance SiEmanuel TaropaDong LiPhil CroneAnmol GulatiSébastien CeveyJonas AdlerAda MaDavid SilverSimon TokumineRichard PowellStephan LeeMichael ChangSamer HassanDiana MincuAntoine YangNir LevineJenny BrennanMingqiu WangSarah HodkinsonJeffrey ZhaoJosh LipschultzAedan PopeMichael B. ChangCheng LiLaurent El ShafeyMichela PaganiniSholto DouglasBernd BohnetFabio PardoSeth OdoomMihaela RoscaCicero Nogueira dos SantosKedar SoparkarArthur GuezTom HudsonSteven HansenChulayuth AsawaroengchaiRavi AddankiTianhe YuWojciech StokowiecMina KhanJustin GilmerJaehoon LeeCarrie Grimes BostockKeran RongJonathan CatonPedram PejmanFilip PaveticGeoff BrownVivek SharmaMario LučićRajkumar SamuelJosip DjolongaAmol MandhaneLars Lowe SjösundElena BuchatskayaElspeth WhiteNatalie ClayJiepu JiangHyeontaek LimRoss HemsleyJane LabanowskiNicola De CaoDavid SteinerSayed Hadi HashemiJacob AustinAnita GergelyTim BlythJoe StantonKaushik ShivakumarAditya SiddhantAnders AndreassenCarlos ArayaNikhil SethiRakesh ShivannaSteven HandAnkur BapnaAli KhodaeiAntoine MiechGarrett TanzerAndy SwingShantanu ThakoorZhufeng PanZachary NadoStephanie WinklerDian YuMohammad SalehLoren MaggioreIain BarrMinh GiangThais KagoharaIvo DanihelkaAmit MaratheVladimir FeinbergMohamed ElhawatyNimesh GhelaniDan HorganHelen MillerLexi WalkerRichard TanburnMukarram TariqDisha ShrivastavaFei XiaChung-Cheng ChiuZoe AshwoodKhuslen BaatarsukhSina SamangooeiFred AlcoberAxel StjerngrenPaul KomarekKaterina TsihlasAnudhyan BoralRamona ComanescuJeremy ChenRuibo LiuDawn BloxwichCharlie ChenYanhua SunFangxiaoyu FengMatthew MaugerXerxes DotiwallaVincent HellendoornMichael SharmanIvy ZhengKrishna HaridasanGabe Barth-MaronCraig SwansonDominika RogozińskaAlek AndreevPaul Kishan RubensteinRuoxin SangDan HurtGamaleldin ElsayedRenshen WangDave LaceyAnastasija IlićYao ZhaoLora AroyoChimezie IwuanyanwuVitaly NikolaevBalaji LakshminarayananSadegh JazayeriRaphaël Lopez KaufmanMani VaradarajanChetan TekurDoug FritzMisha KhalmanDavid ReitterKingshuk DasguptaShourya SarcarTina OrnduffJavier SnaiderFantine HuotJohnson JiaRupert KempNejc TrdinAnitha VijayakumarLucy KimChristof AngermuellerLi LaoTianqi LiuHaibin ZhangDavid EngelSomer GreeneAnaïs WhiteJessica AustinLilly TaylorShereen AshrafDangyi LiuMaria GeorgakiIrene CaiYana KulizhskayaSonam GoenkaBrennan SaetaKiran VodrahalliChristian FrankDario de CesareBrona RobenekHarry RichardsonMahmoud AlnahlawiChristopher YewPriya PonnapalliMarco TagliasacchiAlex KorchemniyYelin KimDinghua LiBill RosgenZoe AshwoodKyle LevinJeremy WiesnerPraseem BanzalPraveen SrinivasanHongkun YuÇağlar ÜnlüDavid ReidZora TungDaniel FinchelsteinRavin KumarAndre ElisseeffJin HuangMing ZhangRui ZhuRicardo AguilarMai GiménezJiawei XiaOlivier DousseWilli GierkeSoheil Hassas YeganehDamion YatesKomal JalanLu LiEri Latorre-ChimotoDuc Dung NguyenKen DurdenPraveen KallakuriYaxin LiuMatthew JohnsonTomy TsaiAlice TalbertJasmine LiuAlexander NeitzChen ElkindMarco SelviMimi JasarevicLivio Baldini SoaresAlbert CuiPidong WangAlek Wenjiao WangXinyu YeKrystal KallarackalLucia LoherHoi LamJosef BroderDan Holtmann-RiceNina MartinBramandia RamadhanaDaniel ToyamaMrinal ShuklaSujoy BasuAbhi MohanNick FernandoNoah FiedelKim PatersonHui LiAnkush GargJane ParkDongHyun ChoiDiane WuSankalp SinghZhishuai ZhangAmir GlobersonLily YuJohn CarpenterFélix de Chaumont QuitryCarey RadebaughChu-Cheng LinAlex TudorPrakash ShroffDrew GarmonDayou DuNeera VatsHan LuShariq IqbalAlex YakubovichNilesh TripuraneniJames ManyikaHaroon QureshiNan HuaChristel NganiMaria Abi RaadHannah ForbesAnna BulanovaJeff StanwayMukund SundararajanVictor UngureanuColton BishopYunjie LiBalaji VenkatramanBo LiChloe ThorntonSalvatore ScellatoNishesh GuptaYicheng WangIan TenneyXihui WuAshish ShenoyGabriel CarvajalDiana Gage WrightBen BariachZhuyun XiaoPeter HawkinsSid DalmiaClement FarabetPedro ValenzuelaQuan YuanChris WeltyAnanth AgarwalMia ChenWooyeol KimBrice HulseNandita DukkipatiAdam PaszkeAndrew BoltElnaz DavoodiKiam ChooJennifer BeattieJennifer PrendkiHarsha VashishtRebeca Santamaria-FernandezLuis C. CoboJarek WilkiewiczDavid MadrasAli ElqurshGrant UyKevin RamirezMatt HarveyTyler LiechtyHeiga ZenJeff SeibertClara Huiyi HuMohamed ElhawatyAndrey KhorlinMaigo LeAsaf AharoniMegan LiLily WangSandeep KumarAlejandro LinceNorman CasagrandeJay HooverDalia El BadawyDavid SoergelDenis VnukovMatt MiecnikowskiJiri SimsaAnna KoopPraveen KumarThibault SellamDaniel VlasicSamira DarukiNir ShabatJohn ZhangGuolong SuJiageng ZhangJeremiah LiuYi SunEvan PalmerAlireza GhaffarkhahXi XiongVictor CotrutaMichael FinkLucas DixonAshwin SreevatsaAdrian GoedeckemeyerAlek DimitrievMohsen JafariRemi CrockerNicholas FitzGeraldAviral KumarSanjay GhemawatIvan PhilipsFrederick LiuYannie LiangRachel SterneckAlena RepinaMarcus WuLaura KnightMarin GeorgievHyo LeeHarry AskhamAbhishek ChakladarAnnie LouisCarl CrousHardie CateDessie PetrovaMichael QuinnDenese Owusu-AfriyieAchintya SinghalNan WeiSolomon KimDamien VincentMilad NasrChristopher A. Choquette-ChooReiko TojoShawn LuDiego de Las CasasYuchung ChengTolga BolukbasiKatherine LeeSaaber FatehiRajagopal AnanthanarayananMiteyan PatelCharbel KaedJing LiJakub SygnowskiShreyas Rammohan BelleZhe ChenJaclyn KonzelmannSiim PõderRoopal GargVinod KoverkathuAdam BrownChris DyerRosanne LiuAzade NovaJun XuSlav PetrovDemis HassabisKoray KavukcuogluJeffrey DeanOriol Vinyals",
        "links": "http://arxiv.org/abs/2403.05530v1",
        "entry_id": "http://arxiv.org/abs/2403.05530v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05530v1",
        "summary": "In this report, we present the latest model of the Gemini family, Gemini 1.5\nPro, a highly compute-efficient multimodal mixture-of-experts model capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio.\nGemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks\nacross modalities, improves the state-of-the-art in long-document QA,\nlong-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's\nstate-of-the-art performance across a broad set of benchmarks. Studying the\nlimits of Gemini 1.5 Pro's long-context ability, we find continued improvement\nin next-token prediction and near-perfect retrieval (>99%) up to at least 10M\ntokens, a generational leap over existing models such as Claude 2.1 (200k) and\nGPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large\nlanguage models at the frontier; when given a grammar manual for Kalamang, a\nlanguage with fewer than 200 speakers worldwide, the model learns to translate\nEnglish to Kalamang at a similar level to a person who learned from the same\ncontent.",
        "updated": "2024-03-08 18:54:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05530v1"
    },
    {
        "title": "GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM",
        "authors": "Hao KangQingru ZhangSouvik KunduGeonhwa JeongZaoxing LiuTushar KrishnaTuo Zhao",
        "links": "http://arxiv.org/abs/2403.05527v1",
        "entry_id": "http://arxiv.org/abs/2403.05527v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05527v1",
        "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
        "updated": "2024-03-08 18:48:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05527v1"
    },
    {
        "title": "Authorship Attribution in Bangla Literature (AABL) via Transfer Learning using ULMFiT",
        "authors": "Aisha KhatunAnisur RahmanMd Saiful IslamHemayet Ahmed ChowdhuryAyesha Tasnim",
        "links": "http://dx.doi.org/10.1145/3530691",
        "entry_id": "http://arxiv.org/abs/2403.05519v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05519v1",
        "summary": "Authorship Attribution is the task of creating an appropriate\ncharacterization of text that captures the authors' writing style to identify\nthe original author of a given piece of text. With increased anonymity on the\ninternet, this task has become increasingly crucial in various security and\nplagiarism detection fields. Despite significant advancements in other\nlanguages such as English, Spanish, and Chinese, Bangla lacks comprehensive\nresearch in this field due to its complex linguistic feature and sentence\nstructure. Moreover, existing systems are not scalable when the number of\nauthor increases, and the performance drops for small number of samples per\nauthor. In this paper, we propose the use of Average-Stochastic Gradient\nDescent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an\neffective transfer learning approach that addresses the problem of complex\nlinguistic features extraction and scalability for authorship attribution in\nBangla Literature (AABL). We analyze the effect of different tokenization, such\nas word, sub-word, and character level tokenization, and demonstrate the\neffectiveness of these tokenizations in the proposed model. Moreover, we\nintroduce the publicly available Bangla Authorship Attribution Dataset of 16\nauthors (BAAD16) containing 17,966 sample texts and 13.4+ million words to\nsolve the standard dataset scarcity problem and release six variations of\npre-trained language models for use in any Bangla NLP downstream task. For\nevaluation, we used our developed BAAD16 dataset as well as other publicly\navailable datasets. Empirically, our proposed model outperformed\nstate-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset.\nFurthermore, we showed that the proposed system scales much better even with an\nincreasing number of authors, and performance remains steady despite few\ntraining samples.",
        "updated": "2024-03-08 18:42:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05519v1"
    }
]