[
    {
        "title": "Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets",
        "authors": "Lorenzo BrigatoStavroula Mougiakakou",
        "links": "http://arxiv.org/abs/2403.05532v1",
        "entry_id": "http://arxiv.org/abs/2403.05532v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05532v1",
        "summary": "We introduce Tune without Validation (Twin), a pipeline for tuning learning\nrate and weight decay without validation sets. We leverage a recent theoretical\nframework concerning learning phases in hypothesis space to devise a heuristic\nthat predicts what hyper-parameter (HP) combinations yield better\ngeneralization. Twin performs a grid search of trials according to an\nearly-/non-early-stopping scheduler and then segments the region that provides\nthe best results in terms of training loss. Among these trials, the weight norm\nstrongly correlates with predicting generalization. To assess the effectiveness\nof Twin, we run extensive experiments on 20 image classification datasets and\ntrain several families of deep networks, including convolutional, transformer,\nand feed-forward models. We demonstrate proper HP selection when training from\nscratch and fine-tuning, emphasizing small-sample scenarios.",
        "updated": "2024-03-08 18:57:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05532v1"
    },
    {
        "title": "The Computational Complexity of Learning Gaussian Single-Index Models",
        "authors": "Alex DamianLoucas Pillaud-VivienJason D. LeeJoan Bruna",
        "links": "http://arxiv.org/abs/2403.05529v1",
        "entry_id": "http://arxiv.org/abs/2403.05529v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05529v1",
        "summary": "Single-Index Models are high-dimensional regression problems with planted\nstructure, whereby labels depend on an unknown one-dimensional projection of\nthe input via a generic, non-linear, and potentially non-deterministic\ntransformation. As such, they encompass a broad class of statistical inference\ntasks, and provide a rich template to study statistical and computational\ntrade-offs in the high-dimensional regime.\n  While the information-theoretic sample complexity to recover the hidden\ndirection is linear in the dimension $d$, we show that computationally\nefficient algorithms, both within the Statistical Query (SQ) and the Low-Degree\nPolynomial (LDP) framework, necessarily require $\\Omega(d^{k^\\star/2})$\nsamples, where $k^\\star$ is a \"generative\" exponent associated with the model\nthat we explicitly characterize. Moreover, we show that this sample complexity\nis also sufficient, by establishing matching upper bounds using a partial-trace\nalgorithm. Therefore, our results provide evidence of a sharp\ncomputational-to-statistical gap (under both the SQ and LDP class) whenever\n$k^\\star>2$. To complete the study, we provide examples of smooth and Lipschitz\ndeterministic target functions with arbitrarily large generative exponents\n$k^\\star$.",
        "updated": "2024-03-08 18:50:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05529v1"
    },
    {
        "title": "GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM",
        "authors": "Hao KangQingru ZhangSouvik KunduGeonhwa JeongZaoxing LiuTushar KrishnaTuo Zhao",
        "links": "http://arxiv.org/abs/2403.05527v1",
        "entry_id": "http://arxiv.org/abs/2403.05527v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05527v1",
        "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
        "updated": "2024-03-08 18:48:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05527v1"
    },
    {
        "title": "Poly-View Contrastive Learning",
        "authors": "Amitis ShidaniDevon HjelmJason RamapuramRuss WebbEeshan Gunesh DhekaneDan Busbridge",
        "links": "http://arxiv.org/abs/2403.05490v1",
        "entry_id": "http://arxiv.org/abs/2403.05490v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05490v1",
        "summary": "Contrastive learning typically matches pairs of related views among a number\nof unrelated negative views. Views can be generated (e.g. by augmentations) or\nbe observed. We investigate matching when there are more than two related views\nwhich we call poly-view tasks, and derive new representation learning\nobjectives using information maximization and sufficient statistics. We show\nthat with unlimited computation, one should maximize the number of related\nviews, and with a fixed compute budget, it is beneficial to decrease the number\nof unique samples whilst increasing the number of views of those samples. In\nparticular, poly-view contrastive models trained for 128 epochs with batch size\n256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k,\nchallenging the belief that contrastive models require large batch sizes and\nmany training epochs.",
        "updated": "2024-03-08 17:55:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05490v1"
    },
    {
        "title": "Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference",
        "authors": "Akshat RamachandranZishen WanGeonhwa JeongJohn GustafsonTushar Krishna",
        "links": "http://arxiv.org/abs/2403.05465v1",
        "entry_id": "http://arxiv.org/abs/2403.05465v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05465v1",
        "summary": "Traditional Deep Neural Network (DNN) quantization methods using integer,\nfixed-point, or floating-point data types struggle to capture diverse DNN\nparameter distributions at low precision, and often require large silicon\noverhead and intensive quantization-aware training. In this study, we introduce\nLogarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by\nposits that dynamically adapts to DNN weight/activation distributions by\nparameterizing LP bit fields. We also develop a novel genetic-algorithm based\nframework, LP Quantization (LPQ), to find optimal layer-wise LP parameters\nwhile reducing representational divergence between quantized and full-precision\nmodels through a novel global-local contrastive objective. Additionally, we\ndesign a unified mixed-precision LP accelerator (LPA) architecture comprising\nof processing elements (PEs) incorporating LP in the computational datapath.\nOur algorithm-hardware co-design demonstrates on average <1% drop in top-1\naccuracy across various CNN and ViT models. It also achieves ~ 2x improvements\nin performance per unit area and 2.2x gains in energy efficiency compared to\nstate-of-the-art quantization accelerators using different data types.",
        "updated": "2024-03-08 17:28:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05465v1"
    }
]