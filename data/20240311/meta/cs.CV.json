[
    {
        "title": "Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos",
        "authors": "Tarun KalluriBodhisattwa Prasad MajumderManmohan Chandraker",
        "links": "http://arxiv.org/abs/2403.05535v1",
        "entry_id": "http://arxiv.org/abs/2403.05535v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05535v1",
        "summary": "We introduce LaGTran, a novel framework that utilizes readily available or\neasily acquired text descriptions to guide robust transfer of discriminative\nknowledge from labeled source to unlabeled target data with domain shifts.\nWhile unsupervised adaptation methods have been established to address this\nproblem, they show limitations in handling challenging domain shifts due to\ntheir exclusive operation within the pixel-space. Motivated by our observation\nthat semantically richer text modality has more favorable transfer properties,\nwe devise a transfer mechanism to use a source-trained text-classifier to\ngenerate predictions on the target text descriptions, and utilize these\npredictions as supervision for the corresponding images. Our approach driven by\nlanguage guidance is surprisingly easy and simple, yet significantly\noutperforms all prior approaches on challenging datasets like GeoNet and\nDomainNet, validating its extreme effectiveness. To further extend the scope of\nour study beyond images, we introduce a new benchmark to study ego-exo transfer\nin videos and find that our language-aided LaGTran yields significant gains in\nthis highly challenging and non-trivial transfer setting. Code, models, and\nproposed datasets are publicly available at\nhttps://tarun005.github.io/lagtran/.",
        "updated": "2024-03-08 18:58:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05535v1"
    },
    {
        "title": "Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets",
        "authors": "Lorenzo BrigatoStavroula Mougiakakou",
        "links": "http://arxiv.org/abs/2403.05532v1",
        "entry_id": "http://arxiv.org/abs/2403.05532v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05532v1",
        "summary": "We introduce Tune without Validation (Twin), a pipeline for tuning learning\nrate and weight decay without validation sets. We leverage a recent theoretical\nframework concerning learning phases in hypothesis space to devise a heuristic\nthat predicts what hyper-parameter (HP) combinations yield better\ngeneralization. Twin performs a grid search of trials according to an\nearly-/non-early-stopping scheduler and then segments the region that provides\nthe best results in terms of training loss. Among these trials, the weight norm\nstrongly correlates with predicting generalization. To assess the effectiveness\nof Twin, we run extensive experiments on 20 image classification datasets and\ntrain several families of deep networks, including convolutional, transformer,\nand feed-forward models. We demonstrate proper HP selection when training from\nscratch and fine-tuning, emphasizing small-sample scenarios.",
        "updated": "2024-03-08 18:57:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05532v1"
    },
    {
        "title": "Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapola",
        "authors": "Yijiang LiSucheng RenWeipeng DengYuzhi XuYing GaoEdith NgaiHaohan Wang",
        "links": "http://arxiv.org/abs/2403.05523v1",
        "entry_id": "http://arxiv.org/abs/2403.05523v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05523v1",
        "summary": "Out-of-distribution (OOD) generalization is a favorable yet challenging\nproperty for deep neural networks. The core challenges lie in the limited\navailability of source domains that help models learn an invariant\nrepresentation from the spurious features. Various domain augmentation have\nbeen proposed but largely rely on interpolating existing domains and frequently\nface difficulties in creating truly \"novel\" domains. Humans, on the other hand,\ncan easily extrapolate novel domains, thus, an intriguing question arises: How\ncan neural networks extrapolate like humans and achieve OOD generalization?\n  We introduce a novel approach to domain extrapolation that leverages\nreasoning ability and the extensive knowledge encapsulated within large\nlanguage models (LLMs) to synthesize entirely new domains. Starting with the\nclass of interest, we query the LLMs to extract relevant knowledge for these\nnovel domains. We then bridge the gap between the text-centric knowledge\nderived from LLMs and the pixel input space of the model using text-to-image\ngeneration techniques. By augmenting the training set of domain generalization\ndatasets with high-fidelity, photo-realistic images of these new domains, we\nachieve significant improvements over all existing methods, as demonstrated in\nboth single and multi-domain generalization across various benchmarks.\n  With the ability to extrapolate any domains for any class, our method has the\npotential to learn a generalized model for any task without any data. To\nillustrate, we put forth a much more difficult setting termed, data-free domain\ngeneralization, that aims to learn a generalized model in the absence of any\ncollected data. Our empirical findings support the above argument and our\nmethods exhibit commendable performance in this setting, even surpassing the\nsupervised setting by approximately 1-2\\% on datasets such as VLCS.",
        "updated": "2024-03-08 18:44:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05523v1"
    },
    {
        "title": "Probabilistic Image-Driven Traffic Modeling via Remote Sensing",
        "authors": "Scott WorkmanArmin Hadzic",
        "links": "http://arxiv.org/abs/2403.05521v1",
        "entry_id": "http://arxiv.org/abs/2403.05521v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05521v1",
        "summary": "This work addresses the task of modeling spatiotemporal traffic patterns\ndirectly from overhead imagery, which we refer to as image-driven traffic\nmodeling. We extend this line of work and introduce a multi-modal, multi-task\ntransformer-based segmentation architecture that can be used to create dense\ncity-scale traffic models. Our approach includes a geo-temporal positional\nencoding module for integrating geo-temporal context and a probabilistic\nobjective function for estimating traffic speeds that naturally models temporal\nvariations. We evaluate our method extensively using the Dynamic Traffic Speeds\n(DTS) benchmark dataset and significantly improve the state-of-the-art.\nFinally, we introduce the DTS++ dataset to support mobility-related location\nadaptation experiments.",
        "updated": "2024-03-08 18:43:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05521v1"
    },
    {
        "title": "Poly-View Contrastive Learning",
        "authors": "Amitis ShidaniDevon HjelmJason RamapuramRuss WebbEeshan Gunesh DhekaneDan Busbridge",
        "links": "http://arxiv.org/abs/2403.05490v1",
        "entry_id": "http://arxiv.org/abs/2403.05490v1",
        "pdf_url": "http://arxiv.org/pdf/2403.05490v1",
        "summary": "Contrastive learning typically matches pairs of related views among a number\nof unrelated negative views. Views can be generated (e.g. by augmentations) or\nbe observed. We investigate matching when there are more than two related views\nwhich we call poly-view tasks, and derive new representation learning\nobjectives using information maximization and sufficient statistics. We show\nthat with unlimited computation, one should maximize the number of related\nviews, and with a fixed compute budget, it is beneficial to decrease the number\nof unique samples whilst increasing the number of views of those samples. In\nparticular, poly-view contrastive models trained for 128 epochs with batch size\n256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k,\nchallenging the belief that contrastive models require large batch sizes and\nmany training epochs.",
        "updated": "2024-03-08 17:55:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.05490v1"
    }
]