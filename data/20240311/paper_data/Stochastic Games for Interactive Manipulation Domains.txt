To appear at the 2024 IEEE International Conference on Robotics and Automation (ICRA), May 2024.
Stochastic Games for Interactive Manipulation Domains
Karan Muvvala∗1, Andrew M. Wells∗2, Morteza Lahijanian1, Lydia E. Kavraki2, and Moshe Y. Vardi2
Abstract—As robots become more prevalent, the complexity
ofrobot-robot,robot-human,androbot-environmentinteractions
increases. In these interactions, a robot needs to consider not
only the effects of its own actions, but also the effects of
other agents’ actions and the possible interactions between
agents. Previous works have considered reactive synthesis,
where the human/environment is modeled as a deterministic,
adversarial agent; as well as probabilistic synthesis, where the
human/environment is modeled via a Markov chain. While
theyprovidestrongtheoreticalframeworks,therearestillmany
aspectsofhuman-robotinteractionthatcannotbefullyexpressed Fig. 1: Tic-tac-toe game between a robot and a human. The
and many assumptions that must be made in each model. In robot is unaware of the level of expertise of the human and
this work, we propose stochastic games as a general model for
suffers from the “trembling hand” problem. In this case, the
human-robot interaction, which subsumes the expressivity of
robot needs to reason about the probabilities of reaching a
all previous representations. In addition, it allows us to make
fewer modeling assumptions and leads to more natural and given state as well as the strategic responses of both agents
powerful models of interaction. We introduce the semantics of from that state.
this abstraction and show how existing tools can be utilized to
synthesize strategies to achieve complex tasks with guarantees.
Further, we discuss the current computational limitations and robot becomes overly pessimistic, resulting in conservative
improve the scalability by two orders of magnitude by a new strategies that are “unfriendly” and “competetive” with the
way of constructing models for PRISM-games.
human. Italso drastically limits the scenarios,forwhichtask-
completion guarantees can be provided. This is the reason
I. INTRODUCTION
previous methods place unwieldy limitations on human [3],
Traditionally, robots have accomplished complex tasks
e.g., human takes at most a fixed number of actions.
through planning i.e., computing a “path” from the initial
Probabilistic synthesis has been proposed as an alternative
state to a goal state. As robots become more prevalent, the
to reactive synthesis to address this issue. Those methods
complexityofrobot-robot,robot-human,orrobot-environment
view the human/environment as a probabilistic agent [4], [5].
interactions increases. In these interactions, a robot needs
This can describe many types of human behaviors, not just
to consider not only the effects of its own actions, but
adversarial behavior; however, it is unrealistic as it assumes
also the effects of other agents’ actions as well as the
that the human behavior is Markovian. Generally, humans
possible interactions between agents. These complexities
havetheirownobjectivesandtakeactionsaccordingly.Hence,
mean planning is often insufficient. Insteadthe robots should
they should be treated as strategic agents. For instance, in
compute a strategy, anticipating the possible effects of each
a tic-tac-toe game, as depicted in Fig. 1, both human and
agent’s actions and reasoning in advance how it should
robot aim to win. However, it is not clear if the human is
respond to different contingencies.
a novice player (makes imperfect moves), a master player
Inordertoensuresafetyandgeneralcorrectness,synthesis,
(makes perfect moves), or somewhere in between. Hence,
either reactive or probabilistic, has emerged as a promising
both probabilistic and strategic aspects are present, which a
approach to creating correct-by-construction robot strategies
purely probabilistic model cannot capture [5], [6].
[1]–[3]. In reactive synthesis, the worst-case behavior of the
To address these limitations, we present robot strategy
human/environmentisconsidered. Thisisoverlyconservative
synthesisusingstochasticgames.Intuitively,stochasticgames
and lacks the power to describe many scenarios where one
can be considered as a generalization of Markov Decision
possibility is known to be more likely than another. Essen-
Processes (MDPs), where instead of one agent making de-
tially,byassumingthehuman/environmentisadversarial,the
cisions, multiple agents make decisions. Stochastic games
subsume the expressive power of reactive and probabilistic
This work was supported in part by NASA 80NSSC21K1031, NASA
80NSSC17K0162,NSF1830549,andNSFRI2008720.Authorswouldalso synthesis; givingusthemostgeneralmodelforrobotstrategy
like to thank the authors of PRISM,especially Dr. Dave Parker for their synthesis. For instance, they allow modeling of the scenario
excellenttoolandfortheirhelpinmodifyingittoimportstochasticgames.
∗Equalcontribution in Fig. 1,even fora robot with imperfect actuation,e.g.,may
1AerospaceEng.SciencesDept.attheUniversityofColoradoBoulder. accidentally drop a piece in an unintended location due to a
firstname.lastname@colorado.edu bad grasp. The key benefit of stochastic games compared
2Computer Science Dept. at Rice University. Andrew Wells was
to prior works is not that the models are more accurate
a student at Rice University at the time this work was conducted.
andrewmw94@gmail.com,{kavraki,vardi}@cs.rice.edu (thoughthiscanbethecase),butrathermodelinghuman-robot
1
4202
raM
7
]OR.sc[
1v01940.3042:viXramanipulation as a stochastic game makes fewer assumptions
4
and is thus more robust. Because of their expressive power, 4 0
2 3
however, stochastic games bring new challenges in terms of 0
2 3
scalability.Thispaperonlybeginstoaddressthesechallenges.
In this work, we bridge the gap between robotic ma-
Fig. 2: Manipulation domain: (left) the locations of interest,
nipulation domain and the expressive power of stochastic
wheretheElselocation(L )containsallobjectsnototherwise
games. We mainly focus on the abstraction construction of 1
shown. (right) Initial state with red and yellow blocks at L
the continuous manipulation domain in the presence of a 2
and L and the blue block at L .
human and robot action uncertainty as a discrete two-player 3 1
stochastic game. We present conditions and semantics under
which this abstraction can be viewed as a turn-based game, II. PROBLEMSETUP
improving computation tractability. Further,we show thatthe In this work, we focus on a robotic manipulator with
strongassumptionthatthehumantakesapre-definednumber “tremblinghands”operatinginthepresenceofahuman.Given
of actions (as in [7]) can be relaxed in our abstraction. We ahigh-leveltaskfortherobotandknowledgeonthebehaviors
also provide an implementation that enables scalability by of general humans,ouraim is to synthesize a strategy forthe
bypassingthebuilt-inmodelconstructionofstochasticgames robot to maximize the probability of completing the task.
in the existing tool, namely PRISM-games [8]. Finally, we
illustrate the power of our approach on several case studies A. Probabilistic Abstraction of Manipulation Domain
and show scalability in a set of benchmarks. We model the manipulation domain as an MDP by
Thecontributionsofthisworkarefourfold:(i)weformalize abstracting configuration space C =C ×C , where C and
r o r
how to model the human-robot manipulation domain as turn- C are the robot and movable objects configuration spaces,
o
based, two-player stochastic game and use existing tools to respectively. Intuitively,a state ofthis MDP captures relevant
synthesize optimal strategies for the robot; (ii) we relax the features of C. That is, the state is a tuple of objects and
assumptions on human interventions while still treating the their locations. Further, using Planning Domain Definition
humanasastrategicagent; (iii)weimprovethescalabilityof Language (PDDL) [24], we ground and define robot actions
the existing toolandprovide an open-source toolforefficient along with preconditions and effects of these actions from
synthesis for robotic manipulation scenarios (available on every state [17]. Since our actions have stochastic outcomes,
Github [9]); (iv) we illustrate the efficacy of our proposed wedefineaprobabilitydistributionassociatedwiththeeffects
approach on several case studies and benchmarks. of robot actions. Formally,
Related Work. Synthesis is the problem of automati-
Definition 1 (Probabilistic Manipulation Domain Abstrac-
cally generating a correct-by-construction plan or strategy
tion). A probabilistic manipulation domain is an MDP tuple
from a high-level description (specification) of a task. The
M=(S,A,P,s ,AP,L) where,
specificationsareusuallyexpressedinLinearTemporalLogic 0
(LTL)[10],andforroboticsystems,LTLinterpretedoverfinite • S is a finite set of states,
traces(LTLf)[11],[12]ispopularduetoitsabilitytodescribe • s 0 ∈S is the initial state,
tasksthatneedtobecompletedinfinitetime. Whenanagent • A is a finite set of robot actions,
interacts with the world, we are interested in synthesizing a • P : S×A×S → [0,1] is the probability distribution
strategythatreactstotheenvironment.Reactivesynthesishas over the effects of the robot’s action a ∈ A and
beenexaminedasastand-aloneproblemaswellasinrobotics (cid:80) P(s,a,s′)=1 for all state-action pairs,
s′∈S
[1], [2]. Most works on reactive synthesis for robotics focus • AP isthesetoftask-relatedpropositionsthatcaneither
onmobilerobots[1],[13]–[15],whichhasarelativelysimple be true or false, and
state space comparedto manipulation. Reactive synthesis has • L : S → 2AP is the labeling function that maps each
also been examined for manipulation [7], [16], [17] domains. state to a set of AP that are true in s∈S.
We build on these later works in this paper. An execution of MDP M is a path ω = ω −a →0 ω −a →1
0 1
Probabilistic synthesis has been examined for general ... −a −n−−→1 ω , where ω ∈ S, a ∈ A, ω = s , and
domains [18]–[22],including roboticmanipulation [5]. In the n i i 0 0
P(ω ,a ,ω ) > 0 for all 0 ≤ i ≤ n − 1. The set of
contextoflearning,stochasticgameswithunknowntransitions i i i+1
finite paths is denoted by S∗. The observation trace of
have been studied forabstracted robotic systems [23]. In syn-
ω is ρ = L(ω )...L(ω ) the sequence of sets of atomic
thesis, we assume the transitions between states are known a 0 n
propositions observed along the way. We define the task of
priori.Existingworksonstochasticsynthesisformanipulation
the robot according to these observations, below. A robot
domainuseMDPs,whichonlyallowonestrategicagent.Thus,
strategy π :S∗ →A is a function that chooses an a∈A for
they assume the human behaves in a mechanical fashion
the robot given the path ω ∈S∗ executed so far.
and synthesize an optimal robot policy. Using stochastic
games allows us to reason about a strategic human agent. Example 1 (MDP). Consider the continuous manipulation
We focus on modeling human-robot manipulation scenarios domain in Fig. 2. The corresponding MDP is depicted in
with stochastic games where tasks are defined using formal Fig. 3. The robot is tasked with building an arch with blue
language, which has not been studied. block (not shown) on top. The initial state is defined as
20.9 to the robot. Also, we assume general knowledge on the
0.1 0.1 likelihood of the human moving a specific block to some
4 4 4 location. Such general likelihoods can be inferred from
0 0 0
2 3 0.9 2 3 0.9 2 3
past experiences (data) on various humans. Our goal is to
0.1
synthesizeastrategyfortherobottomaximizethelikelihood
0.9
0.9 of achieving its task.
4
0.1 2 3 0 Problem 1. Given a robotic manipulator with its MDP
abstraction and LTLf task formula ϕ in the presence of
a human with a latent objective and general likelihood of
Fig. 3: Example abstraction of manipulation domain from
(taking) actions,
Fig. 2 with stochasticity for robot actions.
1) Abstraction: generate a finite abstraction of the inter-
action between the robot and human through object
s := {O ,O ,O } where O corresponds to object i
0 02 13 21 ij manipulation such that it captures the strategic and
placed at location j. Here O ,O ,O are the red, yellow,
0 1 2 stochastic aspects of both agents,
and blue blocks, respectively. From the initial state, under
2) Synthesis: synthesize a strategy for the robot that
the robot-grasp blue block action, there is a 10% chance of
maximizes its probability of accomplishing ϕ.
failure and a 90% chance of success. The alternate action is
to grasp the yellow block and finally place blue on top. There are several challenges in Problem 1. We want to
model a strategic human with imperfect decision making
B. Manipulation Tasks as LTLf formulas
capabilities.Also,wewanttoallowtherobottobeastrategic
Asrobotictasksmustbeaccomplishedinfinitetime,Linear agentthatcouldfailsometimesinexecutingitsaction.Hence,
TemporalLogicoverfinitetraces(LTLf)[11]isanappropriate thefocusofourapproachistheconstructionofanabstraction
choice for the specification language. That is because LTLf thatcapturesallthenecessaryaspectsoftheproblemforbest
isveryexpressive(samesyntaxas LTL)butitsinterpretations decisionmaking.Additionally,notethatboththemanipulation
are over finite behaviors. domain and reactive synthesis are notorious for their state-
explosion problem [5], [17]. Problem 1 combines the two;
Definition 2 (LTLf Syntax). Given a set of atomic proposi-
hence, computational tractability is an aspect that we want
tions AP, an LTLf formula is defined recursively as
to ensure in our approach.
ϕ:=⊤|p|¬ϕ|ϕ ∧ϕ |Xϕ|ϕUϕ
1 2 III. STOCHASTICGAMEABSTRACTION
where p ∈ AP is an atomic proposition, ⊤ (“true”), ¬ In this section, we discuss how strategic and stochastic
(“negation”)and∧(“conjuction”)aretheBooleanoperators, elements of the human and robot are combined to form
and X (“next”) and U (“until”) are the temporal operators. a two-player stochastic game. Specifically, we deal with a
fully observable two-player game. Naturally, this game is
The common temporal operators “eventually” (F) and “glob-
concurrent in the continuous domain, but concurrent games
ally” (G) are defined as: F ϕ=⊤Uϕ and Gϕ=¬F ¬ϕ.
are known to sufferfrom computational tractability [26]. Our
The semantics of an LTLf formula are defined over finite
traces in (2AP)∗ [25]. We say a path ω ∈S∗ accomplishes goal is to define semantics that allow a turn-based modeling
for the purpose of strategy synthesis such that the execution
ϕ, denoted by ω |=ϕ, if its observation trace satisfies ϕ.
of the strategy is seemingly concurrent at the runtime. We
Example 2 (Example LTLf specification). The LTLf formula first formally define a two-player, turn-based stochastic game
for constructing an arch from Fig. 2 with any two blocks as (simply,stochasticgame) [27],andthen showourabstraction
support and blue block on top can be written as, to this game.
(cid:0) (cid:1)
ϕ =F p ∧p ∧p ∧ Definition 3 (StochasticGame). Astochasticgameisatuple
arch block,support block,support blue,top
1 2
G(cid:0) ¬(p
block,support
∧p
block,support
)→¬p blue,top(cid:1) G=(S,s 0,A s,A e,T,C,AP,L),whereS,s 0,AP andLare
1 2 as in Def. 1, and
where support ∈{L ,L }∀i∈{1,2} and top:=L .
i 2 3 4 • A s and A e are the finite set of robot & human actions,
C. Problem Statement • T : S ×(A s ∪A e)×S → [0,1] is the probabilistic
transition relation, and
In this work, we are interested in synthesizing strategies
• C : S (cid:55)→ {s,e} designates which player controls the
for the robot operating in the presence of human. In our
choice of action at each state.
setting, human behavior can be abstracted as human moving
objectsandtheactionscanbeformalizedasobject(s)moving Here, players s (system) and e (environment) are the
from one location to another. We aim to develop a general robot and human, respectively. An execution of the game
framework; hence, we do not assume knowledge about the G is a sequence of visited states as players take turns in
particular human, with whom the robot is interacting. making moves. The choice of action for Player i∈{s,e} is
Further, we assume that humans are strategic agents who determined by the strategy π :S∗ →A that picks actions
i i
choose actions according to some objective, which is latent according to the execution of the game so far.
3For the strategic players, we follow the models of prior
... ...
work [3]. The robot player’s actions follow a standard pick- 4 4
0 0
place domain, which typically can be encoded in PDDL as 2 3 2 3
describedabove. Thehumanplayerhasthesameabilitiesbut
is assumed to move relatively quickly compared to the robot. 4
0
Additionally, unlike previous approaches, we do allow the 2 3
human to hold onto an object. Thus, we model the robot’s
gripperandthehumangripperandmakeafairnessassumption
that the human will eventually return the object. 4 0 4 0 4 0
2 3 2 3 2 3
Game States. As in [3], we model the continuous world
bygroupinglocationsinto“regionsofinterest”.Theseinclude
Fig. 4: Stochastic game variant of MDP in Fig. 3. The circle
a “end-effector” region representing the robot’s gripper and
and rectangle states belong to the robot and human player.
an “Else” region representing all locations not particularly
Forthise.g.weallowhumantomoveobjectsfromtherobot’s
specified. To allow the robot to react at any point, the model
gripper.Thetoprowshowsmultiplehumanmovements,while
should be constructed such that every valid arrangement of
the state on the right corresponds to no human intervention.
objects in the real world has game states for both human and
robot turns. These states are equivalent to the robot MDP
stochastic games allows us to reason over strategic players
states in Def. 1.
while also considering stochasticity in their execution.
Game Actions. In prior works [7], [17], [28], the human
In addition to assigning control of game states to each
is typically assumed to move faster than the robot, leading
player, we need some way to turn the continuous, real-time
to multiple human moves per robot move. We follow this
interaction into a turn-based game. We do this as in [7], by
assumption and examine several models of turn allocation.
assuming certain robot actions are “atomic” while giving
We have several modeling choices and present results for
the human actions priority over all non-atomic actions. Here,
all of them. The set of robot actions A = A is the same
s
as in previous papers, the atomic robot actions are grasp
set of actions in Def. 1. The human actions A are every
e
and place (not including the transit or transfer preceding
possible move of the objects to the locations of interest,
the opening / closing of the gripper). Once we have chosen
“Else”, and human’s gripper. Then, the transition relation
a way to model actions, and under our assumptions about
T(s,a,s′)=P(s,a,s′) if a∈A ; otherwise, it is obtained
s
discrete states and atomic action executions, we can create a
from the likelihood of the human actions as discussed below.
turn-based stochastic game.
In reactive synthesis [7], a limit k is placed on the total
number of human interventions to ensure the specification is Example 3. A partial two-player stochastic game for our
realizable. This limit is unintuitive and somewhat unrealistic. manipulationdomainisshowninFig.4. Theactionstakenin
For example, suppose the model assumes the human inter- circular states are controlled by the system and those taken
venesatmost30times(k =30).Then,duringexecution,once in rectangular states are controlled by the environment. The
the robot observes the 30th action, it will act as though the same action could stochastically lead to different possible
human will no longer interfere. Unfortunately,unless there is states. For example, the robot’s action from initial state is
some external reason for this limit,the robot should arguably to grasp a block from the initial state and stochastically
assume the human is more likely to interfere because it has determine whether to grasp the yellow or the blue block.
observed this happen 30 times already.
We generalize this as a ratio of human and robot actions. Remark 1. Winning the game translates to finishing task
Forexample,wecouldallow1humanactionforevery2robot ϕ, and winning strategies are strategies that guarantee task
actions (denoted by 2:1). We implement this using counters completion. Termination of the game is typically defined as
thatreseteverytimethegamechangescontrolfromoneplayer reaching an accepting or violating finite prefix of a trace.
toanother.Soaplayercannot“skip”turnsnowinordertotake This could be insufficient in certain cases, e.g., a robot asked
more consecutive turns later. While this could be encoded as to tidy a room will stop once the room is cleaned even if
a two-player game, the ability to express the stochasticity in the human is approaching some object to displace it. Our
robot’ssuccessrate(ofexecutingactions)or/andthehuman’s game modeling allows the human and robot to “negotiate”
tendency to intervene at particular locations can not modeled terminationsothattherobotonlyconsidersthetaskcomplete
using a purely game theoretic approach. when the human agrees.
Ourothermodelusesaprobabilityofhandingcontrolfrom
one player to another. This implies a probabilistic limit but
IV. STRATEGYSYNTHESIS
avoids setting a hard limit on action for either player. This is ForagivenLTLf specification,synthesisreducestosolving
achieved by including an action in A that evolves to a state a stochastic game for a reachability objective, i.e., reach a
e
after which the human does not intervene. This is a natural target state [8]. That game is the composition of G with the
weakening of the hard limit on human intervention. Note automaton thatis constructedfrom ϕ [11]. Solving stochastic
that this cannot be modeled using MDP where the effects gameswithreachabilityobjectivesliesinthecomplexityclass
of human actions are inherently random in nature. Thus, NP ∩ coNP [29]. PRISM-games makes use of the model
4(a) |O|=3, varying |L| (b) |O|=3, varying |L| (c) |O|=3, varying |L|
(d) |L|=8, varying |O| (e) |L|=8, varying |O| (f) |L|=8, varying |O|
Fig. 5: Benchmark results for different scenarios using our approach. (a) and (d) illustrate the model construction time using
the original PRISM-games and our implementation for the probabilistic human termination scenario. (b) and (e) illustrate
computationtimesforthe1:1actionratioscenario,and(c)and(f)correspondtotheprobabilistichumanterminationscenario.
checking algorithms described in [8], that relies on Value available on GitHub [9]. The results are shown in Fig. 5.
Iterationtocomputethevaluesforallstatesofthegame[30]. Scalability. We test a simple pick-and-place manipulation
The algorithm can be decoupled into two stages. domain, varying the number of objects and locations. Three
Precomputation Stage: During this stage, we identify locations are reserved for the robot and human gripper, and
states of the game for which the probability of satisfaction the terminal location for each player. Only one object can be
is 0 or 1, and the maximal end components of the game. manipulated by the robot and human, while multiple objects
Informally, an end component is a set of states for which, can be placed at other locations. The task is to place objects
there exists a robot strategy such that it is possible to remain in their desired locations.
forever in that set once entered. Efficiency and accuracy can PRISM’s default configuration reads in modeling files
be improved by using this step. Next, numerical computation written in the PRISM modeling language. As PRISM is
is performed on the remaining states in the game. (primarily) a symbolic engine, a structured, hierarchical
Numerical computation stage: The probability of reach- model is preferred as it exploits regularity in the abstraction.
ing a target state is 1 if the state belongs to the target Our model is naturally flat, and hence PRISM modeling
end component, else we iteratively update state values until language suffers from scalability. Therefore, we implement
we reach a fixed point. At every iteration, we perform functionality to import the stochastic games models through
max a((cid:80) s′T(s,a,s′) · p(s′)) if s belongs to robot player the direct specification of their transition matrix, state, label,
elseweperformmin a((cid:80) s′T(s,a,s′)·p(s′)).HereT isfrom and player vectors.
Def. 3,ands,a,s′ are the currentstate,action,andsuccessor We use a Python script to automate the construction of the
state,respectively. p(s′)denotesthevalueassociatedwiththe model files for direct specification of transition matrix and
successorstateinthepreviousiteration.WhilePRISM-games statevectorsoutsidePRISMandthenimporttheminPRISM-
isamaturetoolbox,theimplementationforsolvingstochastic games. We benchmark this method of model construction as
games is less mature than tooling for MDPs, and we found showninFig.5aandFig.5d. WeseethatwhilethePRISM’s
the bottleneck to be model construction rather than Value originalimplementation(inred)failstoscalebeyond3objects
Iteration. Below, we discuss how we mitigate this bottleneck. and8locations,ourapproach(inblue)notonlyscalesbeyond
this bottleneck but is also 2 orders of magnitude faster.
V. IMPLEMENTATIONANDRESULTS
Wealsopresentbenchmarksbasedonthemodelingchoices
Here, we present benchmarks based on experiments from described in Sec. III. Fig. 5b and Fig. 5e correspond to
[5]. We run our tests using PRISM-games [8] and discuss model construction and synthesis time for 1:1 scenario
our modifications to remove a performance bottleneck when where we allow one human action for every robot action.
importing models. All the experiments are run on an Intel In this scenario, the human could potentially undo every
i5 -13th Gen 3.5 GHz CPU with 32 GB RAM. The tool is action the robot does and hence the robot cannot guarantee
5Fig.6:Thegamebeginswith(A)and(B).Instate(C)therobotchoosesamovethatmaximizestheprobabilityofhumanfailure
underthe“tremblinghand”model.In(D)thehumanwilllikelyplacetheobjectinthebottomcenter,buthastwoopenneighbor
cells. Under the robot strategy,the human will have three more chances to fail (video: https://youtu.be/UUBW7QEw6Ng).
TABLEI:Abstraction&Synthesiscomp.timesfor3objects.
only under the assumption that the robot does not fail to
complete an action, i.e., reactive synthesis cannot capture
Case |L| States Transitions ModelConst.(s) Synthesis(s)
Study stochasticity in the robot’s ability to correctly place a marker
7 8,480 42,240 0.161 0.515 at its desired location as per the strategy. While an MDP can
9 43,848 298,080 0.496 2.679
1:1 capture the stochastic outcomes,itcannotmodelthe strategic
11 148,608 1,288,704 1.671 11.522
13 393,800 4,166,400 5.216 21.126 nature of the human. On the other hand, using the stochastic
7 9200 63440 0.148 1.002 games model,we can account for the strategic and stochastic
Prob 9 45,864 442,008 0.551 5.885 nature of both players. Fig. 6 illustrates a run of the game.1
11 152,928 1,905,120 1.862 23.822
13 401,720 6,156,920 5.692 75.021 In our experiment, we have stochasticity in placing the
marker for both the robot and the human. We model this
TABLEII:Abstraction&Synthesiscomp.timesfor5objects. probability of marker placements (say a normal distribution
with standard deviation of 1-cell width) as the uncertainty
|L| States Transitions ModelConst.(s) Synthesis(s) in the player’s action. We restrict both the robot and human
4 148 387 0.052 0.037 to not be able to place the marker outside the cells or in an
5 4,896 17,184 0.248 0.688 already occupied cell. Stochastic games allows us to reason
6 43,416 190,107 1.101 7.366
overpossiblehumanandrobotfailure.Dependingonthetask,
7 217,600 1,144,320 4.406 38.768
8 787,500 4,846,875 5.944 60.933 the robot can move so as to either maximize its chances of
9 2,304,288 16,280,352 90.314 677.584 winning the game or the chances of human failure. In both
casestudies,therobotstartsthegame. Wespecifythesetasks
as P [F(“RobotWin”)] and P [F(“HumanWin”)].
max=? min=?
task completion. We see that the computation time grows The emergent behavior for the robot under specification 1 is
exponentially as the state space increases for fixed |O| and to initially place its markerin the middle. This maximizes its
varying |L| and vice versa. chancesofwinningwhilereducingthenumberofunoccupied
We also benchmark scenarios where there is 5% chance cells, which reduces the probability of failure in future robot
of human termination at every state. In contrast to [3], [7], actions. As the game progresses, we observe that the robot
where a parameter k was used to constrain the number of places its marker near crowded cells with fewer empty cells
human interventions, this approach allows greater flexibility around it. For the second specification, we observe that the
and a more intuitive model while still allowing the robot to optimalaction,initially,forthe robotis to place its markerin
win the game. The computation times are shown in Fig. 5c the middle. Next, the robot places markers to maximize its
and Fig. 5f. Similar to the previous scenario, computation winning probability while ensuring as many empty locations
times grow exponentially as the state space increases. For as possible for the next optimal human action.
bothscenarios,themodelconstruction time,whileincreasing,
VI. CONCLUSION
is relatively small compared to the synthesis time.
We present a framework for robot manipulation based on
For all of the experiments using our modified implementa-
stochastic games. Stochastic games subsume the expressivity
tion, PRISM-games required at most 8 GB of RAM. Table I
of reactive and probabilistic synthesis proposed in previous
reports the size of the game and time for model construction
works. We illustrate the efficacy of our approach through
andstrategysynthesis. TableIIillustrateshowtheabstraction
various scenarios and discuss the emergent behavior. Future
grows for the 1:1 scenario. The Python script runs out of
work should examine symbolic approach to scale to more
memory for 5 objects and 10 locations for both scenarios.
objects and locations. Additional work can examine model-
Physical Experiment: Tic-Tac-Toe with “Trembing
ing of uncertain observations, reasoning over other agents
Hand”. Recall the game of tic-tac-toe in Fig. 1, where
strategies, concurrent games or games with varying rewards.
human and robot players alternate turns placing markers in
empty cells. Tic-tac-toe can be solved using min-max, but 1Videoofmoreruns:https://youtu.be/UUBW7QEw6Ng
6REFERENCES [24] P. Haslum, N. Lipovetzky, D. Magazzeni, C. Muise, R. Brachman,
F.Rossi,andP.Stone,Anintroductiontotheplanningdomaindefinition
[1] H.Kress-Gazit,G.Fainekos,andG.J.Pappas,“Where’swaldo?sensor- language. Springer,2019,vol.13.
basedtemporallogicmotionplanning,”inInt.Conf.onRoboticsand
[25] G.DeGiacomoandM.Y.Vardi,“SynthesisforLTLandLDLonfinite
Automation. Rome,Italy:IEEE,2007,pp.3116–3121. traces,”inIntl.JointConf.onArtificialIntelligence(IJCAI),vol.15,
[2] H.Kress-Gazit,M.Lahijanian,andV.Raman,“Synthesisforrobots:
2015,pp.1558–1564.
Guaranteesandfeedbackforrobotbehavior,”AnnualReviewofControl,
[26] M.Kwiatkowska,G.Norman,D.Parker,andG.Santos,“Automatic
Robotics,andAutonomousSystems,vol.1,no.1,pp.211–236,2018.
verification of concurrent stochastic systems,” Formal Methods in
[3] K.He,A.M.Wells,L.E.Kavraki,andM.Y.Vardi,“Efficientsymbolic SystemDesign,vol.58,no.1-2,pp.188–250,2021.
reactive synthesis for finite-horizon tasks,” in 2019 Intl. Conf. on [27] A.Condon,“Thecomplexityofstochasticgames,”Informationand
RoboticsandAutomation(ICRA). IEEE,2019,pp.8993–8999. Computation,vol.96,no.2,pp.203–224,1992.
[4] S.Junges,N.Jansen,J.-P.Katoen,andU.Topcu,“Probabilisticmodel
[28] K. Muvvala and M. Lahijanian, “Efficient symbolic approaches for
checking for complex cognitive tasks–a case study in human-robot quantitative reactive synthesis with finite tasks,” in 2023 IEEE/RSJ
interaction,”arXivpreprintarXiv:1610.09409,2016.
International Conference on Intelligent Robots and Systems (IROS),
[5] A. M. Wells,Z. Kingston,M. Lahijanian,L. E. Kavraki,and M. Y.
2023,pp.8666–8672.
Vardi,“Finitehorizonsynthesisforprobabilisticmanipulationdomains,” [29] A.Condon,“Onalgorithmsforsimplestochasticgames,”inAdvances
inIntl.Conf.onRoboticsandAutomation. IEEE,2021. inComputationalComplexityTheory,volume13ofDIMACSSeriesin
[6] A.Abate,J.Gutierrez,L.Hammond,P.Harrenstein,M.Kwiatkowska, DiscreteMathematicsandTheoreticalComputerScience. American
M.Najib,G.Perelli,T.Steeples,andM.Wooldridge,“Rationalveri-
MathematicalSociety,1993,pp.51–73.
fication:game-theoreticverificationofmulti-agentsystems,”Applied
[30] K.ChatterjeeandT.A.Henzinger,ValueIteration. Berlin,Heidelberg:
Intelligence,vol.51,pp.6569–6584,2021.
SpringerBerlinHeidelberg,2008,pp.107–138.
[7] K. He, M. Lahijanian, L. E. Kavraki, and M. Y. Vardi, “Reactive
synthesisforfinitetasksunderresourceconstraints,”inInt.Conf.on
IntelligentRobotsandSystems(IROS). Vancouver,BC,Canada:IEEE,
2017,pp.5326–5332.
[8] M.Kwiatkowska,G.Norman,D.Parker,andG.Santos,“PRISM-games
3.0:Stochasticgameverificationwithconcurrency,equilibriaandtime,”
inProc.32ndInternationalConferenceonComputerAidedVerification
(CAV’20),ser.LNCS,vol.12225. Springer,2020,pp.475–487.
[9] A. M. Wells, “Stochastic games for robotics.” [Online]. Available:
https://github.com/andrewmw94/stochasticgamesforroboticscode
[10] A. Pnueli, “The temporal logic of programs,” in Foundations of
ComputerScience,1977.,18thAnnualSymposiumon. IEEE,1977,
pp.46–57.
[11] G.DeGiacomoandM.Y. Vardi,“Lineartemporallogicandlinear
dynamic logic on finite traces.” in Intl. Joint Conf. on Artificial
Intelligence(IJCAI),vol.13,2013,pp.854–860.
[12] S. Zhu, L. M. Tabajara, J. Li, G. Pu, and M. Y. Vardi, “Symbolic
LTLf synthesis,” in Proc. of the 26th Intl. Joint Conf. on Artificial
Intelligence. AAAIPress,2017,pp.1362–1369.
[13] T.Wongpiromsarn,U.Topcu,andR.M.Murray,“Recedinghorizon
temporal logic planning,” IEEE Transactions on Automatic Control,
vol.57,no.11,pp.2817–2830,2012.
[14] C.I.VasileandC.Belta,“Reactivesampling-basedtemporallogicpath
planning,”inIntl.Conf.onRoboticsandAutomation(ICRA). IEEE,
2014,pp.4310–4315.
[15] E.M.Wolff,U.Topcu,andR.M.Murray,“Efficientreactivecontroller
synthesis for a fragment of linear temporal logic,” in Intl. Conf. on
RoboticsandAutomation(ICRA). IEEE,2013,pp.5033–5040.
[16] K. He,M. Lahijanian,L. E. Kavraki,andM. Y. Vardi,“Automated
abstractionofmanipulationdomainsforcost-basedreactivesynthesis,”
IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. 285–292,
2018.
[17] K.Muvvala,P.Amorese,andM.Lahijanian,“Let’scollaborate:Regret-
based reactive synthesis for robotic manipulation,” in Int. Conf. on
RoboticsandAutomation. IEEE,2022,pp.4340–4346.
[18] M.Kwiatkowska,G.Norman,andD.Parker,“PRISM4.0:Verification
of probabilistic real-time systems,” in Proc. 23rd Intl. Conf. on
ComputerAidedVerification(CAV’11),ser.LNCS,vol.6806. Springer,
2011,pp.585–591.
[19] C.BaierandJ.-P.Katoen,PrinciplesofModelChecking. TheMIT
Press,2008.
[20] F.Miao,Q.Zhu,M.Pajic,andG.J.Pappas,“Ahybridstochasticgame
forsecurecontrolofcyber-physicalsystems,”Automatica,vol.93,pp.
55–63,2018.
[21] L.Feng,C.Wiltsche,L.Humphrey,andU.Topcu,“Controllersyn-
thesisforautonomoussystemsinteractingwithhumanoperators,”in
ProceedingsoftheACM/IEEESixthInternationalConferenceonCyber-
PhysicalSystems,ser.ICCPS’15. NewYork,NY,USA:Association
forComputingMachinery,2015,p.70–79.
[22] A.M.Wells,M.Lahijanian,L.E.Kavraki,andM.Y.Vardi,“LTLf
synthesisonprobabilisticsystems(onlineversion).”
[23] A.K.Bozkurt,Y.Wang,M.M.Zavlanos,andM.Pajic,“Model-free
reinforcementlearningforstochasticgameswithlineartemporallogic
objectives,”in2021IEEEInternationalConferenceonRoboticsand
Automation(ICRA),2021,pp.10649–10655.
7