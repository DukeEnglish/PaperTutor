Probabilistic Image-Driven Traffic Modeling via Remote Sensing
ScottWorkman ArminHadzic
DZYNETechnologies DZYNETechnologies
Abstract
This work addresses the task of modeling spatiotempo-
ral traffic patterns directly from overhead imagery, which
we refer to as image-driven traffic modeling. We extend
this line of work and introduce a multi-modal, multi-task
transformer-based segmentation architecture that can be
usedtocreatedensecity-scaletrafficmodels.Ourapproach
includesageo-temporalpositionalencodingmoduleforin-
tegratinggeo-temporalcontextandaprobabilisticobjective
functionforestimatingtrafficspeedsthatnaturallymodels
temporal variations. We evaluate our method extensively
usingtheDynamicTrafficSpeeds(DTS)benchmarkdataset
Figure1.Weproposeamethodforimage-driventrafficmodeling,
and significantly improve the state-of-the-art. Finally, we
whichcanbeusedtocreatedensecity-scaletrafficmodels. (left)
introduce the DTS++ dataset to support mobility-related Historicalground-truthtrafficdataisoftensparseasnotallroads
locationadaptationexperiments. are traversed at all times. For example in Brooklyn on Monday
at8am,manyroadsaremissingempiricalspeeddata. (right)Our
methodcancreateadensemodeloftrafficflowatthesametime.
1.Introduction
The relationship between humans, the physical environ-
to directly model spatiotemporal mobility patterns, which
ment, and motion guides a number of real-world applica-
werefertoasimage-driventrafficmodeling. Weintroduce
tions. As Chen et al. [7] note, “transportation researchers
a multi-modal, multi-task transformer-based segmentation
have long sought to develop models to predict how peo-
architecture that operates on overhead imagery and can be
pletravelintimeandspaceandseektounderstandthefac-
usedtocreatedense,city-scalemodelsoftrafficflow(Fig-
tors that affect travel-related choices.” For example, mo-
ure1,right). Ourapproachintegratesgeo-temporalcontext
bility has been shown to have a strong influence on traffic
(i.e., geographic location and time metadata) to enable lo-
safety[43],publichealth[33],housingprices[5],andmore.
cation and time-dependent traffic speed predictions, along
Recently, with the growth of autonomous driving efforts,
withtwoauxiliarytasks(roadsegmentationandorientation
there has been an increased focus on using vision-based
estimation). Theseauxiliarytasksprovidesynergyfortraf-
methods to characterize the physical environment [16, 23]
ficspeedestimationthroughmulti-tasklearning,aswellas
andhowitistraversed[10,45].
allowingourapproachtogeneralizetolocationswherethe
A primary challenge that remains is how to scale
roadnetworkisunknownorhaschanged.
mobility-related analysis to the size of a city. For ex-
ample, traffic speed data is predominately collected using Ourapproachhasseveralkeycomponents. First,wein-
fixed-pointsensorsdeployedatstaticlocationsonroads[3]. tegratecontextintoourmethodbyintroducinganovelgeo-
Thoughanincreasingamountoftrafficspeeddataisavail- temporal positional encoding (GTPE) module. GTPE op-
ablefromalternativesources,suchasautomaticvehiclelo- eratesongeo-temporalcontextthroughthreedistinctpath-
cation systems, much of this data is still proprietary [32]. ways corresponding to location, time, and space-time fea-
Eveninthebestcasescenario,noteveryroadistraversedat tures, ultimately producing a positional encoding that cap-
everypossibletime,resultinginpartial,incompletemodels tures mobility-context. Second, we propose a probabilis-
oftrafficflow(Figure1,left). tic formulation for estimating traffic speeds that naturally
Thisworkaddressesthetaskofusingoverheadimagery modelstemporalvariationinempiricalspeeddata. Specif-
1
4202
raM
8
]VC.sc[
1v12550.3042:viXraically,wecaptureuncertaintybyestimatingper-pixelprior Kumar et al. [26] propose a method for estimating traffic
distributionsovertrafficspeeds,insteadofregressingtraffic flow using vehicle-mounted cameras and showcase it in a
speedsdirectly. Tosupportthis, weintroduceanobjective popular driving simulator. Zhang et al. [51] introduce a
function that explicitly accounts for the number of traffic graph convolutional framework for traffic forecasting that
observations at a given time, during model training. This capturesspatiotemporaldependencies. Manyothervision-
allowsourmethodaccesstoanimplicitformofconfidence based methods have been proposed for estimating vehicle
intheunderlyingtrafficspeedaveragesforagivenroadseg- speeds from ground-level imagery; refer to [15] for an in-
ment. depthoverview.
Extensive experiments on a recent benchmark dataset, Though much progress has been made, a primary chal-
including ablation studies, demonstrate how our approach lengethatremainsishowtobuilddescriptionsoftheenvi-
achieves superior performance versus baselines. In addi- ronmentataglobalscale. Traditionalapproachesformod-
tion, we extend this dataset to include a new, diverse, city eling traffic flow assume prior knowledge of the road net-
and show how it can be used to support mobility-related work and its connectivity [17], as well as the existence of
location adaptation experiments. We also illustrate sev- large quantities of historical traffic data for each road seg-
eral scenarios where our approach could be applied to ur- ment [27]. As such, these methods are unable to function
banplanningapplications. Overall,thecontributionsofthis in locations where the road network is unknown or where
workcanbesummarizedasfollows: roadsegmentshaveinsufficienthistoricaldata. Tocircum-
• a multi-modal, multi-task architecture for image- vent this, overhead imagery has become a useful resource
driventrafficmodeling, for many tasks due to its dense coverage, high resolution,
• a novel approach for integrating geo-temporal con- andeverincreasingrevisitrates[2,30].
textintheformofageo-temporalpositionalencoding
Fortrafficmodeling,overheadimageryhasbeenusedto
module,
automaticallygeneratemapsofroadnetworks[34,52],de-
• aprobabilisticformulationforestimatingtrafficspeeds
tect lane boundaries [20], understand roadway safety [36],
that incorporates knowledge of the number of traffic
approximatetrafficnoise[12],estimateemissions[35],and
observationsasaformofconfidence,
manyotherformsofimage-drivenmapping[38,39,42,46–
• an extensive evaluation of our methods on the Dy-
48]. Related to our work, Song et al. [41] and Hadzic et
namic Traffic Speeds (DTS) benchmark, achieving
al.[18]showhowoverheadimagerycanbeusedtounder-
state-of-the-artresults,
stand motion in the form of free-flow traffic speeds (i.e.,
• andanewdataset,DTS++,tosupportmobility-related
average speed without adverse conditions). Workman et
locationadaptationexperiments.
al.[45]introducethetaskofdynamictrafficmodelinganda
newbenchmarkdatasetfortrafficspeedestimation. Weex-
2.RelatedWork
tendthislineofworkinseveralways,includingintroducing
aprobabilisticformulationforestimatingtrafficspeeds.
Trafficmodelingisimportantformanyapplicationsinclud-
ingurbanplanning[19]andautonomousdriving[6]. These
applications ultimately require: 1) descriptions of the un- 3. An Architecture for Image-Driven Traffic
derlying environment and 2) knowledge of how environ- Modeling
ments are traversed. For the former, descriptions of the
staticenvironmentmightincludepropertiessuchasthelo- We address the problem of image-driven traffic modeling
cation of roads, number of lanes, traffic directions, traffic using overhead imagery. Our network architecture, de-
signs, etc. For the latter, information relating human ac- picted in Figure 2, has three inputs: an overhead image,
tivity to the underlying infrastructure is necessary, such as S(l), a geolocation, l, and a time, t. It has three out-
time-varyingtrafficspeeds,modelsoftrafficcongestionand puts, corresponding to our primary task of traffic speed
safety, and other characterizations of driver and pedestrian estimation and two auxiliary tasks: road segmentation
behavior. Decades of research has focused on understand- and orientation estimation. We start with an overview of
ingrelatedtopicsinurbanmobility. ourmulti-tasktransformer-basedsegmentationarchitecture
Numerous learning-based methods have been proposed (Section3.1). Next,wedescribeourapproachforintegrat-
for relating properties of the environment to how it is tra- ing geo-temporal context via a novel geo-temporal posi-
versed. For example, recent work in autonomous driving tional encoding module (Section 3.2). Then, we describe
hasexploredlanedetectionfromground-levelimages[14] our proposed probabilistic formulation for estimating traf-
and how to estimate road layout and vehicle occupancy ficspeeds(Section3.3).Finally,wedetailthelossfunctions
in top views given a front-view monocular image [50]. fortheauxiliarytasks(Section3.4). Pleaserefertothesup-
Related to motion, Chen et al. [8] use traffic cameras to plemental material for an in-depth description of architec-
generate mobility statistics from pedestrians and vehicles. turedesignchoices.
2Encoder Decoder
Ground Truth
Loss
Loss
Location
Geo-Temporal
Time Positional
Encoding
Loss
Figure2.Anoverviewofourarchitectureforimage-driventrafficmodeling.
3.1.ArchitectureOverview produce the segmentation output, a final linear layer oper-
atesonthefusedfeatureandgeneratesatask-specificnum-
Our segmentation architecture has two primary compo-
ber of outputs. Then, we rescale the output to match the
nents:1)amulti-stagevisualencoderforextractingfeatures
spatialresolutionoftheinputimage.
fromaninputoverheadimage,and2)task-specificdecoders
whichusethesefeaturestogenerateasegmentationoutput. 3.2.IncorporatingGeo-TemporalContext
Multi-stage Visual Encoder Drawing inspiration from
Weinfuseourmodelwithanunderstandingoflocationand
CoAtNet[11],whichdemonstratesthatcombiningconvolu-
timemetadatabyintroducingageo-temporalpositionalen-
tionalandattentionlayerscanachievebettergeneralization
codingmodule,shownvisuallyinFigure3.
andcapacity,weusearobustmulti-stagepipelineforvisual
feature encoding that integrates both convolutional stages
and transformer stages. First, an input image is passed 3.2.1 Geo-TemporalPositionalEncoding
through an initial convolutional stem with three convolu-
Westructurethegeo-temporalpositionalencoding(GTPE)
tionallayers(eachwithBatchNormandReLU),downsam-
module across three pathways corresponding to location,
plingthespatialresolutionbytwo. Thisisfollowedbytwo
time, and space-time (referred to as loc+time). The loca-
convolutional stages, each of which uses multiple inverted
tion and time pathways reflect that location and time have
residual blocks in the style of EfficientNet [21]. Follow-
utilityindependently,whilethespace-timepathwayallows
ingthetwoconvolutionalstagesaretwotransformerstages
themtointeract. Ultimately,theoutputofeachpathwayis
inspired by SwinV2 [29]. Each stage consists of an over-
addedtogethertoformageo-temporalpositionalencoding.
lapping patch embedding [44] followed by a sequence of
Next,wedetailourchoiceofcontextparameterizationsand
multi-headself-attention(MHSA)blockstoallowglobalin-
encodingnetwork.
formation to be propagated throughout the visual features.
Each self-attention block uses a learned relative positional Location Overheadimageryisuniqueinthatimagesare
encoding[24,28]. typicallygeoreferenced,enablingcomputationoftheworld
coordinateofeachpixel.Weleveragethistogeneratedense
Task-Specific MLP Decoder To support semantic seg-
locationmapsineasting(x-axis)andnorthing(y-axis)web
mentation, we extend the multi-stage visual encoder with
mercatorworldcoordinates,whicharethennormalizedtoa
atask-specificdecoderforeachpredictiontask. Takingin-
[−1,1]rangeusingtheboundsoftheinputregion.
spirationfromSegFormer[49],weuseadecoderconsisting
onlyoflinearlayers. Featuresfromeachencoderstageare Time We represent temporal context using day of week,
fused via the following process. First, a stage-specific lin- d,andhourofday,h,eachscaledto[−1,1].Thesevariables
earlayerisusedtounifythenumberofchannelsacrossfea- are parameterized using a cyclic representation, similar to
tures. Theresultingfeaturesarethenupsampledtoafourth Aodhaetal.[31]:
oftheresolutionoftheinputimageusingbilinearinterpola-
tionandarethenconcatenated. Thisisfollowedbyanaddi- f(d,h)=[sin(πd),cos(πd),sin(πh),cos(πh)]. (1)
tionallinearlayerthatfusesinformationacrossfeatures. To
3
Stem
Convolutional
Stage
1
Convolutional
Stage
2
Convolutional
Stage
3
Transformer
Stage
4
Transformer
(Speed)
(Road)
(Orientation)
MLP
MLP
MLPLocation Time 3.3.ProbabilisticTrafficSpeedEstimation
We propose a probabilistic approach for traffic speed es-
Location Temporal timation that naturally models variations in likely traffic
Embedding Embedding speeds, alongwithanobjectivefunctionthatexplicitlyac-
counts for the number of traffic observations (i.e., a form
of confidence in the underlying traffic speed averages for
a given road segment). For this we use the Student’s t-
distribution,asymmetricdistributionsimilartothenormal
distributionthatisusedwhenestimatingthemeanofanor-
mally distributed population in scenarios where: 1) there
areasmallnumberofobservationsand2)thestandardde-
SIREN SIREN SIREN viation of the population is unknown. In other words, a
(location) (loc+time) (time) Student’s t-distribution relates the distribution of the sam-
plemeantothetruemean.
For our purposes, we use the generalized form of the
Student’s t-distribution denoted as t (µ,σ), where µ is a
ν
location (shift) parameter, σ > 0 is a scale parameter, and
ν > 0isashapeparameter(oftenreferredtoasdegreesof
freedom). Theprobabilitydensityfunctionforthisformof
theStudent’st-distributionisgivenby:
Figure 3. An overview of our proposed geo-temporal positional Γ(ν+1) (cid:32) 1 (cid:20) x−µ(cid:21)2(cid:33)−ν+ 21
encodingmodule. p(x|ν,µ,σ)= √2 1+ ,
Γ(ν) νπσ ν σ
2
(2)
ContextualEncoding WeselectSIREN[40]asthefoun- whereΓisthegammafunction,
dationofourencodingnetworkdueitsabilitytoextractfine (cid:90) ∞
Γ(α)= xα−1e−xdx. (3)
detailfromnaturaldataanditsrepresentationpowerforspa-
0
tialandtemporalderivatives.ASIRENblockiscomposedof
Instead of regressing traffic speeds directly, we capture
asinglelinearlayerfollowedbyaweighted(W)sinusoidal
uncertaintybyestimatingper-pixelpriordistributionsover
activationfunction(sin(Wa)). Foreachofourcontextual
trafficspeeds. Givenanoverheadimageandgeo-temporal
features,a,wepasstheparameterizedinputsthroughanen-
context as input, the output of the traffic speed decoder is
codingnetworkmadeupofthree SIREN blocks, eachwith
per-pixelestimatesoftheshift-scaleparametersoftheprior
a hidden dimension of 64 (128 for loc+time), followed by
distribution (i.e., µ and σ2). A softplus activation is used
a final linear layer which produces a 64-dimensional em-
toensuretheseoutputsarepositive. Duringmodeltraining,
bedding. Whenfusingtimewithlocation, wereplicatethe
we combine the estimated shift-scale parameters with the
spatialdimensionstomatch.
truecountoftrafficobservationsforeachroadsegment,as
theshapeparameterν,toformaStudent’stdistribution.
Tooptimizeourapproach,weminimizethenegativelog
3.2.2 FusingwithVisualFeatures likelihood of the resulting distributions, treating ground-
truth traffic speeds on a given road segment as samples
Similar to a traditional positional encoding, we merge the from the corresponding Student’s t-distribution. Specifi-
GTPE output with the visual features in the transformer cally, given an observed traffic speed, y, we compute the
stages of our multi-modal fusion architecture. At each likelihoodunderthedistribution(2).Ourobjectivefunction
stage, we linearly reproject the GTPE output to match the thenbecomes:
stage-specificembeddingsize. Thisisfollowedbybilinear
L =−logG(S(l),l,t;Θ)(y), (4)
interpolation to scale the spatial dimensions to match the speed
spatial resolution at that stage. Finally, we treat the out- where G represents our proposed approach which takes as
putasapositionalencodingandaddittothecorresponding input an overhead image S, geolocation l, and time t, and
visualtokenstoinfusethenetworkwithgeo-temporalcon- outputspriordistributionsovertrafficspeeds,andΘarethe
text. While our primary task is mobility analysis, GTPE weights of the network, which we optimize. At inference,
is applicable to other problem domains (see supplemental weusetheestimatedshiftparameterofouroutputdistribu-
material). tions,µ,asourpredictionfortrafficspeed.
4Image Roads Orientation Mon. 8am Mon. 8pm Sat. 8am Sat. 8pm
Figure4.ExampleimagesfromtheDynamicTrafficSpeeds(DTS)datasetandcorrespondinglabels.Therightfourlabelsdepictavailable
historicaltrafficspeedsatdifferenttimes,wheregreen(red)correspondstofaster(slower)speeds.
Region Aggregation We assume ground-truth traffic 4.Experiments
speeds are provided as averages over road segments (i.e.,
Weevaluateourapproachforthetaskoftrafficspeedesti-
inaggregateform). However,ournetworkgeneratesdense,
mationthroughavarietyofexperiments.
full-resolution predictions. We use a variant of the re-
gion aggregation layer [22] to facilitate comparison to the
4.1.DynamicTrafficSpeedsDataset
ground-truth values. This process averages predictions
acrossagivenroadsegmenttoproduceasingleaggregated Wetrainandevaluateourmethodusingtherecentlyintro-
estimate. Inourcase,weaggregatetheestimatedshiftand duced Dynamic Traffic Speeds (DTS) dataset [45], a fine-
scale parameters for each road segment before forming a grained road understanding benchmark. DTS relates over-
per-road-segmentStudent’st-distribution. head imagery and road metadata with a year of historical
trafficspeedsforNewYorkCity,NY.Thetrafficspeeddata
3.4.AuxiliaryTasks originatesfromUberMovementSpeeds[1],acollectionof
aggregatedspeeddataattheroadsegmentlevel,withhourly
For the auxiliary tasks of road segmentation and orienta-
frequency, fromUberridesharetrips. Thedatasetcontains
tion estimation, we create a task-specific decoder as de-
11,902non-overlappingoverheadimages(1024×1024)at
scribed in Section 3.1. For road segmentation, we follow
approximately 0.3 meters per-pixel, with associated road
recent work and formulate this as a binary segmentation
segment information, and corresponding historical traffic
task. As our objective function, we use a combined loss
data. Figure 4 visualizes example data from DTS. We use
thatincorporatesbinarycrossentropyandtheDiceloss[52]
theoriginaldatasplits,consistingof85%training,5%val-
(L +(1−L )). For orientation estimation, we treat
bce dice idation, and 10% testing. Similar to [45], we dynamically
this as a classification task over K angular bins, and use
generatespeedmasksduringtraining.
standard cross entropy as the objective function. The cu-
mulativeobjectivefunctionforprimaryandauxiliarytasks Evaluation Metrics We report three metrics for traffic
becomes: speed (km/h) estimation: root-mean-square error (RMSE),
meanabsoluteerror(MAE),andthecoefficientofdetermi-
nation (R2), a proportion which describes how well varia-
L=L +L +L . (5)
speed road orientation tionsintheobservedvaluescanbeexplainedbythemodel.
When computing evaluation metrics, we apply region ag-
3.5.ImplementationDetails gregation to average predictions along each road segment
toenablecomparisontothegroundtruth.
OurmethodsareimplementedusingPyTorch[37]andPy-
TorchLightning[13]andoptimizedusingAdam[25](λ=
4.2.TrafficSpeedEstimation
1e−4). We simultaneously optimize the entire network for
all tasks (loss terms weighted equally) and train for 50 As each test image is associated with time-varying histor-
epochs. Model selection is performed using a validation ical traffic data, we consider two strategies for selecting a
set.Foreachtransformerstage,weuseeightattentionheads time for evaluation. For our first experiment, we sample
in all instances of MHSA. The expansion rate for each in- a time for each test image from the set of observed traffic
vertedresidualblockis4. Theexpansion/shrinkrateforthe data across all contained road segments, and use this time
squeeze-and-excitation layers is 0.25. For road segmenta- to generate ground-truth speed masks. We refer to this as
tion, webufferroadgeometriesassumingatwometerhalf a micro strategy, as metrics are computed globally across
width. For orientation estimation, we use K = 16 angu- time. Table 1 shows the results of this study versus a re-
larbins. Wetrainonfullsizeimagesandtrafficspeedsare centbaseline. Acrossallmetrics, ourmethodsignificantly
representedinkilometersperhour. outperformsthepriorstate-of-the-art.
5Figure5. Qualitativeresultsfortrafficspeedestimation(Monday,8am). (top)Ground-truthtrafficspeedsareprovidedasaggregatesfor
individualroadsegments.Forvisualization,wereplicatetheground-truthspeedacrosstheentireroadsegment.(middle)Predictionsfrom
ourapproach,afterapplyingregionaggregation. (bottom)Ourresultswithoutregionaggregationcapturenuancesoftrafficflow,suchas
slowingdownaroundcurves.Green(red)correspondstofast(slow)trafficspeeds.
Forthesecondexperiment,weemployamacrostrategy Table1.Microevaluationoftrafficspeedestimation.
thatconsidersaspecificsetoftimes(Monday&Saturday,
with hours 12am, 4am, 8am, 12pm, 5pm, and 8pm). For Method Loss RMSE↓ MAE↓ R2 ↑
each time, we select all images in the test set that have a [45] Pseudo-Huber 10.66 8.10 0.46
containedroadsegmentwithobservedtrafficspeeddataat Ours Pseudo-Huber 10.08 7.38 0.52
that time. Metrics are then averaged across time such that Ours Student’st 8.94 6.70 0.62
alltimesareweightedequally. Table2showstheresultsof
thisstudyforasubsetoftimes,withtheoverallaverageper-
Table2.Macroevaluationoftrafficspeedestimation.
formanceshowninthebottomrow. Asbefore,ourmethod
significantlyoutperformspriorworkindependentoftheday
Workmanetal.[45] Ours
oftheweekorhourofday. RMSE MAE R2 RMSE MAE R2
Figure5showsqualitativeresultsfromourmethod. The
Mon(4am) 13.24 9.77 0.63 11.07 7.93 0.74
top row shows ground-truth traffic speeds, which are pro-
Mon(12pm) 10.41 7.87 0.52 8.85 6.61 0.65
videdasaggregatesacrossroadsegments. Themiddlerow Sat(5pm) 10.36 7.97 0.43 8.76 6.65 0.59
shows the results of our method, after applying region ag- Sat(8pm) 10.27 7.79 0.46 8.55 6.39 0.63
gregation to individual road segments. The bottom row
Overall 11.13 8.35 0.49 9.09 6.74 0.65
showsthedenseoutputofourapproach,withoutregionag-
gregation (per-pixel traffic speeds). As observed, our ap-
proachisabletocapturenuancesoftrafficflow. Forexam-
compare our probabilistic approach (Student’s t) versus a
ple,thatroundaboutstypicallyhavefastertrafficinstraight-
variantofourmethodwhichdirectlyregressestrafficspeeds
awaysbutslowertrafficonon-rampsandexits.
usingthePseudo-Huberloss. Asobserved,ourprobabilis-
ticapproachsignificantlyimprovesperformancerelativeto
4.3.AblationStudy
this baseline. The choice of baseline objective function is
Weconductanextensiveablationstudytovalidatecompo- consistent with that used in the prior state-of-the-art [45],
nentsofourapproach. First,weconsiderthechoiceofob- highlightingthatourprobabilisticformulationdirectlyleads
jectivefunction. Table1showstheresultsofthisstudy. We toanincreaseinperformance.
6
hturTdnuorG
).ggA(noitciderP
)esneD(noitciderPNext,weevaluatetheimpactofgeo-temporalcontexton Table3.Ablationstudyevaluatingtheimpactoflocationandtime
our traffic speed predictions. Table 3 shows the results of context.
this experiment. All variants of our method that include
geo-temporal context outperform the image-only variant. Method Context RMSE MAE R2
Additionally, results show that including time context is image 11.35 8.69 0.39
superior to including location context for predicting time- image,loc 10.95 8.44 0.43
[45]
varyingtrafficspeeds. Like-for-likecomparisonswithprior image,time 10.68 8.12 0.46
image,loc,time 10.66 8.10 0.46
work show the superior performance of our approach in
eachsetting. image 10.34 7.74 0.49
We also perform an ablation study comparing strate- image,loc 9.77 7.32 0.55
Ours
gies for integrating geo-temporal context. The results are image,time 9.18 6.81 0.60
image,loc,time 8.94 6.70 0.62
shown in Table 4. For this experiment, we vary the loca-
tion representation, context encoding, and context fusion
method. Theresultsshowthatusingaper-pixelrepresenta-
Table4.Ablationstudycomparingdifferentstrategiesforintegrat-
tion(Dense)forlocationissuperiortothecentercoordinate
inggeo-temporalcontext.
(Center) as in prior work. Additionally, encoding location
andtimecontextusingGTPEleadstobetterperformanceas
ContextEnc. ContextFusion
compared to an MLP or CNN encoder. For fusing context Loc.Rep. RMSE MAE R2
loc time loc time
featureswithvisualfeatures,treatinglocationandtimeasa
Center MLP MLP Concat Concat 10.66 8.10 0.46
positionalencoding(PE),asinGTPE,performsbetterthan Center MLP MLP Token Token 9.20 6.92 0.60
channel-wise concatenating (Concat) these features in the Dense CNN MLP PE Token 9.09 6.76 0.61
Dense GTPE GTPE PE PE 8.94 6.70 0.62
decoder, or adding context features as an additional token
(Token)ineachtransformerstage. Insummary,GTPEout-
performsotherstrategiesforintegratinggeo-temporalcon-
Table5.Ablationstudyontheimpactofmulti-tasklearning.
text,whicharerepresentativeofthepriorstate-of-the-art.
Finally, we compare our approach, which includes the
Road Orientation RMSE MAE R2
auxiliarytasksofroadsegmentationandorientationestima-
✗ ✗ 9.25 6.91 0.60
tion,tovariantsofourapproachthatconsidersubsetsofthe
✓ ✗ 9.11 6.75 0.61
tasks (Table 5). Results show that including the auxiliary
✗ ✓ 9.01 6.74 0.62
tasks has a positive impact on performance. This matches
✓ ✓ 8.94 6.70 0.62
recentworkinmulti-tasklearningthatshowssharinginfor-
mationacrosstaskscanleadtoperformanceimprovements
whenthetasksaresynergistic[9].
Table6. AnexampleoflocationadaptationusingDTS++. When
adaptingourapproachfromNewYorkCity(NYC)toCincinnati
4.4.DTS++: ADatasetforLocationAdaptation
(Cincy),fine-tuningthegeo-temporalpositionalencoding(GTPE)
Locationadaptation,ananalogtodomainadaptation,aims moduledramaticallyimprovesperformance.
toaddresstheproblemofadaptingamodeltrainedonone
region to another region. To support mobility-related lo- Train Test GTPE RMSE MAE R2
cation adaptation experiments, we introduce the Dynamic NYC Cincy original 24.11 18.89 0.03
TrafficSpeeds++(DTS++)dataset,anextensionofDTSto NYC Cincy fine-tuned 12.41 9.24 0.70
include a new city, Cincinnati, OH. DTS++ is constructed
inasimilarmannertoDTSandincludesoneyearofhistor-
icaltrafficdatacollectedfromUberMovementSpeeds[1].
show that fine-tuning our geo-temporal positional encod-
This mobility data is paired with 11,137 overhead images
ing module (GTPE) on Cincy dramatically improves per-
(1024×1024)atapproximately0.3metersper-pixel.
formance. This experiment simultaneously highlights the
Using DTS++, we conducted an initial experiment to
impactofGTPEontrafficspeedestimationandshowsthat
evaluate how our approach, trained on New York City
fine-tuningonlytheassociated∼65kparameterscanbean
(NYC),performswhenadaptedtoCincinnati(Cincy). We
efficientwayofadaptingmodelstonewlocations.
showtheresultsofthisexperimentinTable6. Asexpected,
performancedeterioratesasCincyhasvastlydifferentspa- Insummary,whilelocationadaptationisnottheprimary
tiotemporal mobility patterns than NYC. For example, the focus of this work, it is a nascent and important research
averagespeedacrossallroadsegmentsinDTS++forNYC direction and our hope is that DTS++ helps enable future
is 18.93 km/h while Cincy is 31.51 km/h. In addition, we studiesrelatedtomobility-relatedlocationadaptation.
75.Applications
40
CreatingDenseCity-ScaleTrafficModels Thoughem- 35
pirical traffic data has become increasingly available, not 30
allroadsaretraversedatalltimes. Thispresentschallenges
25
fordownstreamapplicationswhichrelyondatafromtrans-
port modeling, as they are limited to regions/times where 20
Sun Mon Tue Wed Thu Fri Sat
data is available, or must collect their own data. Our ap-
proach presents an alternative as it can be used to create 35.0
32.5
densecity-scaletrafficmodels. Figure1(left)showsavail-
30.0
ablehistoricaltrafficdatafromtheDynamicTrafficSpeeds 27.5
dataset(Brooklyn,Monday8am).Asobserved,manyroads 25.0
22.5
are missing empirical speed data (white) as they were not
20.0
traversedatthegiventime. Figure1(right)showshowour
Sun Mon Tue Wed Thu Fri Sat
approach can be used to generate a dense model of traffic
flowduetoitsabilitytogeneralizeacrossspaceandtime. 90
Modeling Traffic Flow Our approach can be used to 80
model spatiotemporal traffic patterns for individual road 70
segments. To demonstrate this, we analyze how predic-
60
tionsfromourapproachonunseenroadsegmentscompare
to historical traffic data. Figure 6 shows the results of this Sun Mon Tue Wed Thu Fri Sat
experiment. Thex-axisrepresentstime,witheachdayrep- 40
resenting a 24 hour interval. Our approach (red) is able to 35
capturetemporaltrendsinhistoricaltrafficdata(green). 30
25
Generalizing to Novel Locations Our approach can be
20
thoughtofasestimatingalocalmotionmodelfromanover-
15
head image which describes how the environment is tra-
Sun Mon Tue Wed Thu Fri Sat
versed. Figure7visualizeshowourmethodcanbeapplied
to novel locations to generate a local motion model. This Figure6. Visualizingtrafficspeedpredictionsforindividualroad
allowsourapproachtobeappliedtonovellocationswhere segmentsvstime.(left)Aroadsegmentshowninred.(right)Our
transport data is incomplete or unavailable, a common oc- approach’spredictions(red)versushistoricaltrafficdata(green).
curence. Forexample, roadnetworksarenotalwaysaccu-
rateforagivenlocation(e.g.,notyetmappedorchanged).
Further, traffic speed data is sparse and not always avail-
able for all roads. The generalizability of our approach is
madepossiblebytheinclusionoftheauxiliarytasksofroad
segmentationandorientationestimation,inadditiontotheir
performancebenefitfrommulti-tasklearning.
6.Conclusion
Ourgoalwastounderstandcity-scalemobilitypatternsus-
ing overhead imagery, a task we refer to as image-driven
Figure7. Theoutputofourapproachonunseenlocationsvisual-
traffic modeling. We proposed a multi-modal, multi-task
izedasalocalmotionmodel.Wesamplefromlocationspredicted
transformer-based segmentation architecture and showed
asroadandvisualizecorrespondingorientationestimatesasvec-
how it can be used to create dense city-scale traffic mod-
tors, coloredbythepredictedspeed. Green(red)correspondsto
els. Our method has several key components, including a fast(slow)trafficspeeds.
probabilistic formulation for naturally modeling variations
in traffic speeds, and a geo-temporal positional encoding
moduleforintegratinggeo-temporalcontext. Extensiveex- ducedtheDTS++datasettosupportmotion-relatedlocation
perimentsusingtheDynamicTrafficSpeeds(DTS)dataset adaptation studies across diverse cities. Our hope is that
demonstrate how our approach significantly improves the these results continue to demonstrate the real-world utility
state-of-art in traffic speed estimation. Finally, we intro- ofimage-driventrafficmodeling.
8
)h/mk(
deepS
ciffarT
)h/mk(
deepS
ciffarT
)h/mk(
deepS
ciffarT
)h/mk(
deepS
ciffarTReferences [15] DavidFerna´ndezLlorca,AntonioHerna´ndezMart´ınez,and
Iva´nGarc´ıaDaza. Vision-basedvehiclespeedestimation:A
[1] UberMovement: Speedscalculationmethodology. Techni-
survey. IETIntelligentTransportSystems,15(8):987–1005,
calreport,UberTechnologies,Inc.,2019. 5,7,12
2021. 2
[2] Adrian Albert, Jasleen Kaur, and Marta C Gonzalez. Us- [16] AndreasGeiger, PhilipLenz, andRaquelUrtasun. Arewe
ingconvolutionalnetworksandsatelliteimagerytoidentify ready for autonomous driving? the kitti vision benchmark
patterns in urban environments at a large scale. In ACM suite. InIEEEConferenceonComputerVisionandPattern
SIGKDDInternationalConferenceonKnowledgeDiscovery Recognition,2012. 1
andDataMining,2017. 2
[17] Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and
[3] Peter J Bickel, Chao Chen, Jaimyoung Kwon, John Rice, HuaiyuWan. Attentionbasedspatial-temporalgraphconvo-
ErikVanZwet,andPravinVaraiya. Measuringtraffic. Sta- lutionalnetworksfortrafficflowforecasting. InAAAICon-
tisticalScience,pages581–597,2007. 1 ferenceonArtificialIntelligence,2019. 2
[4] Geoff Boeing. OSMnx: New methods for acquiring, con- [18] Armin Hadzic, Hunter Blanton, Weilian Song, Mei Chen,
structing, analyzing, and visualizing complex street net- Scott Workman, and Nathan Jacobs. Rasternet: Model-
works. Computers, Environment and Urban Systems, 65: ing free-flow speed using lidar and overhead imagery. In
126–139,2017. 13 IEEE/CVF Conference on Computer Vision and Pattern
[5] Sandip Chakrabarti, Triparnee Kushari, and Taraknath RecognitionWorkshops,2020. 2
Mazumder. Does transportation network centrality deter- [19] BenHamilton-BaillieandPhilJones. Improvingtrafficbe-
minehousingprice? JournalofTransportGeography,103: haviourandsafetythroughurbandesign. InProceedingsof
103397,2022. 1 theInstitutionofCivilEngineers-CivilEngineering,pages
[6] Qianwen Chao, Huikun Bi, Weizi Li, Tianlu Mao, Zhaoqi 39–47,2005. 2
Wang,MingCLin,andZhigangDeng. Asurveyonvisual [20] Namdar Homayounfar, Wei-Chiu Ma, Justin Liang, Xinyu
traffic simulation: Models, evaluations, and applications in Wu, JackFan, andRaquelUrtasun. Dagmapper: Learning
autonomousdriving. 39(1):287–308,2020. 2 tomapbydiscoveringlanetopology. InIEEEInternational
ConferenceonComputerVision,2019. 2
[7] Cynthia Chen, Jingtao Ma, Yusak Susilo, Yu Liu, and
Menglin Wang. The promises of big data and small data [21] JieHu,LiShen,andGangSun. Squeeze-and-excitationnet-
for travel behavior (aka human mobility) analysis. Trans- works.InIEEEConferenceonComputerVisionandPattern
portation researchpart C: emerging technologies, 68:285– Recognition,2018. 3
299,2016. 1 [22] Nathan Jacobs, Adam Kraft, Muhammad Usman Rafique,
and Ranti Dev Sharma. A weakly supervised approach
[8] Li Chen, Ian Grimstead, Daniel Bell, Joni Karanka, Laura
forestimatingspatialdensityfunctionsfromhigh-resolution
Dimond,PhilipJames,LukeSmith,andAlistairEdwardes.
satelliteimagery. InACMSIGSPATIALInternationalCon-
Estimatingvehicleandpedestrianactivityfromtownandcity
ference on Advances in Geographic Information Systems,
trafficcameras. Sensors,21(13):4564,2021. 2
2018. 5
[9] Michael Crawshaw. Multi-task learning with deep neural
[23] Joel Janai, Fatma Gu¨ney, Aseem Behl, Andreas Geiger,
networks:Asurvey.arXivpreprintarXiv:2009.09796,2020.
etal. Computervisionforautonomousvehicles: Problems,
7
datasets and state of the art. Foundations and Trends® in
[10] Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou,
ComputerGraphicsandVision,12(1–3):1–308,2020. 1
Tsung-HanLin,ThiNguyen,Tzu-KuoHuang,JeffSchnei-
[24] GuolinKe, DiHe, andTie-YanLiu. Rethinkingpositional
der,andNemanjaDjuric. Multimodaltrajectorypredictions
encodinginlanguagepre-training. InInternationalConfer-
forautonomousdrivingusingdeepconvolutionalnetworks.
enceonLearningRepresentations,2020. 3
In International Conference on Robotics and Automation,
[25] Diederik Kingma and Jimmy Ba. Adam: A method for
2019. 1
stochastic optimization. In International Conference on
[11] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan.
LearningRepresentations,2014. 5
Coatnet: Marrying convolution and attention for all data
[26] AshutoshKumar, TakehiroKashiyama, HiroyaMaeda, Hi-
sizes. In Advances in Neural Information Processing Sys-
roshiOmata,andYoshihideSekimoto.Citywidereconstruc-
tems,2021. 3
tionoftrafficflowusingthevehicle-mountedmovingcamera
[12] Leonardo Eicher, Michael Mommert, and Damian Borth. inthecarladrivingsimulator.InIEEEInternationalConfer-
Traffic noise estimation from satellite imagery with deep enceonIntelligentTransportationSystems,2022. 2
learning. InInternationalGeoscienceandRemoteSensing [27] Mingqian Li, Panrong Tong, Mo Li, Zhongming Jin, Jian-
Symposium,2022. 2 qiangHuang,andXian-ShengHua. Trafficflowprediction
[13] WA Falcon. Pytorch lightning. GitHub. Note: with vehicle trajectories. In AAAI Conference on Artificial
https://github.com/PyTorchLightning/pytorch-lightning, Intelligence,2021. 2
3,2019. 5 [28] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng
[14] ZhengyangFeng,ShaohuaGuo,XinTan,KeXu,MinWang, Zhang, Stephen Lin, and Baining Guo. Swin transformer:
and Lizhuang Ma. Rethinking efficient lane detection via Hierarchical vision transformer using shifted windows. In
curve modeling. In IEEE Conference on Computer Vision IEEEConferenceonComputerVisionandPatternRecogni-
andPatternRecognition,2022. 2 tion,2021. 3
9[29] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, tunitiesinmachinelearningandremotesensing. IEEEGeo-
YixuanWei,JiaNing,YueCao,ZhengZhang,LiDong,etal. scienceandRemoteSensingMagazine,9(1):184–199,2020.
Swintransformerv2:Scalingupcapacityandresolution. In 2
IEEEConferenceonComputerVisionandPatternRecogni- [43] Chao Wang, Mohammed Quddus, and Stephen Ison. A
tion,2022. 3 spatio-temporalanalysisoftheimpactofcongestionontraf-
[30] LeiMa,YuLiu,XueliangZhang,YuanxinYe,GaofeiYin, fic safety on major roads in the uk. Transportmetrica A:
and Brian Alan Johnson. Deep learning in remote sensing TransportScience,9(2):124–148,2013. 1
applications: Ameta-analysisandreview. ISPRSjournalof [44] WenhaiWang,EnzeXie,XiangLi,Deng-PingFan,Kaitao
photogrammetryandremotesensing,152:166–177,2019. 2 Song,DingLiang,TongLu,PingLuo,andLingShao. Pvt
[31] OisinMacAodha,ElijahCole,andPietroPerona.Presence- v2: Improved baselines with pyramid vision transformer.
only geographical priors for fine-grained image classifica- ComputationalVisualMedia,2022. 3
tion.InIEEEInternationalConferenceonComputerVision, [45] ScottWorkmanandNathanJacobs. Dynamictrafficmodel-
2019. 3,11 ing from overhead imagery. In IEEE Conference on Com-
[32] VishalMahajan,NicoKuehnel,AikateriniIntzevidou,Guido puterVisionandPatternRecognition,2020. 1,2,5,6,7
Cantelmo,RolfMoeckel,andConstantinosAntoniou. Data [46] ScottWorkman,RichardSouvenir,andNathanJacobs. Un-
to the people: a review of public and proprietary data for derstandingandmappingnaturalbeauty. InIEEEInterna-
transportmodels. Transportreviews,42(4):415–440,2022. tionalConferenceonComputerVision,2017. 2
1 [47] Scott Workman, Menghua Zhai, David J Crandall, and
[33] Wesley E Marshall, Daniel P Piatkowski, and Norman W NathanJacobs. Aunifiedmodelfornearandremotesens-
Garrick. Community design, street networks, and public ing. InIEEEInternationalConferenceonComputerVision,
health. JournalofTransport&Health,1(4):326–340,2014. 2017.
1 [48] Scott Workman, M. Usman Rafique, Hunter Blanton, and
[34] Gelle´rt Ma´ttyus, Wenjie Luo, and Raquel Urtasun. Deep- NathanJacobs. Revisitingnear/remotesensingwithgeospa-
roadmapper:Extractingroadtopologyfromaerialimages.In tialattention. InIEEEConferenceonComputerVisionand
IEEEInternationalConferenceonComputerVision,2017.2 PatternRecognition,2022. 2
[35] Ryan Mukherjee, Derek Rollend, Gordon Christie, Armin [49] EnzeXie,WenhaiWang,ZhidingYu,AnimaAnandkumar,
Hadzic,SallyMatson,AnshuSaksena,andMarisaHughes. JoseMAlvarez,andPingLuo. Segformer: Simpleandeffi-
Towardsindirecttop-downroadtransportemissionsestima- cientdesignforsemanticsegmentationwithtransformers.In
tion.InIEEE/CVFConferenceonComputerVisionandPat- AdvancesinNeuralInformationProcessingSystems,2021.
ternRecognitionWorkshops,2021. 2 3
[36] Alameen Najjar, Shun’ichi Kaneko, and Yoshikazu [50] Weixiang Yang, Qi Li, Wenxi Liu, Yuanlong Yu, Yuexin
Miyanaga. Combining satellite imagery and open data Ma, Shengfeng He, and Jia Pan. Projecting your view at-
to map road safety. In AAAI Conference on Artificial tentively:Monocularroadscenelayoutestimationviacross-
Intelligence,2017. 2 viewtransformation. InIEEEConferenceonComputerVi-
[37] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, sionandPatternRecognition,2021. 2
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming [51] QiZhang,JianlongChang,GaofengMeng,ShimingXiang,
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An andChunhongPan. Spatio-temporalgraphstructurelearn-
imperativestyle,high-performancedeeplearninglibrary. In ingfortrafficforecasting. InAAAIConferenceonArtificial
AdvancesinNeuralInformationProcessingSystems,2019. Intelligence,2020. 2
5 [52] Lichen Zhou, Chuang Zhang, and Ming Wu. D-linknet:
[38] TawfiqSalem,MenghuaZhai,ScottWorkman,andNathan Linknetwithpretrainedencoderanddilatedconvolutionfor
Jacobs. A multimodal approach to mapping soundscapes. high resolution satellite imagery road extraction. In IEEE
In IEEE/CVF Conference on Computer Vision and Pattern Conference on Computer Vision and Pattern Recognition
RecognitionWorkshops,2018. 2 Workshops,2018. 2,5
[39] TawfiqSalem,ScottWorkman,andNathanJacobs.Learning
adynamicmapofvisualappearance.InIEEEConferenceon
ComputerVisionandPatternRecognition,2020. 2
[40] VincentSitzmann,JulienMartel,AlexanderBergman,David
Lindell, andGordonWetzstein. Implicitneuralrepresenta-
tionswithperiodicactivationfunctions.InAdvancesinNeu-
ralInformationProcessingSystems,2020. 4
[41] Weilian Song, Tawfiq Salem, Hunter Blanton, and Nathan
Jacobs. Remote estimation of free-flow speeds. In IEEE
International Geoscience and Remote Sensing Symposium,
2019. 2
[42] JohnEVargas-Munoz,ShivangiSrivastava,DevisTuia,and
AlexandreXFalcao.Openstreetmap:Challengesandoppor-
10Supplemental Material :
Probabilistic Image-Driven Traffic Modeling via Remote Sensing
This document contains additional details and experi- TableS1.Ablationstudyofourgeo-temporalpositionalencoding
mentsrelatedtoourmethods. oniNaturalist2018.
1.ExtendedEvaluation: iNaturalist2018 Method Context Top-1 Top-3 Top-5
loc 72.41 87.19 90.60
We investigate the performance of our geo-temporal posi- [31](paper) loc,time 72.68 87.26 90.79
tional encoding (GTPE) on a different task, fine-grained
loc 71.62 86.62 90.27
visual categorization, using the iNaturalist 2018 dataset.
[31](local) loc,time 71.53 86.75 90.28
Thisdatasetcontainsover450,000imagesofbirds(approx.
8000 unique species) collected from the iNaturalist social loc 72.64 87.21 90.80
Ours loc,time 72.66 87.28 90.70
network. The task is to perform large-scale, fine-grained
loc,time,loc+time 72.73 87.31 90.75
speciesclassificationinthescenariowheretherearemany
visuallysimilarspeciesandhighclassimbalance.
Forthisexperiment,wetakeadvantageoftheframework
GTPEforseveralspecies.
introduced by Aodha et al. [31]. The authors proposed to
learn a spatiotemporal prior, P(y|x), conditioned on lo-
2.VisualizingGeo-TemporalContext
cation and time context, x, that estimates the probability
thatagivenobjectcategory,y,occursatthatlocation. The
Our method integrates location and time through dis-
spatiotemporalprioriscombinedwiththeoutputofanim-
tinct pathways in GTPE which can be thought of as 64-
age classifier, P(y|I). The results show that including the
dimensional embeddings. To visualize the learned feature
spatiotemporal prior helps disambiguate fine-grained cate-
representations for time, we construct a false color image
gories. We take advantage of this framework, but use our
using principal component analysis (PCA), shown in Fig-
proposed geo-temporal positional encoding (GTPE) mod-
ure S1. For every time combination (day of week, time of
ule to learn the spatiotemporal prior, instead of their ap-
day),weextractitsfeaturerepresentationfromthetimeem-
proachwhichusesaMLPconsistingofseveralresiduallay-
bedding, and stack them. This results in a matrix of size
ersandafinaloutputembeddinglayer(eachof256dimen-
168×64. WeapplyPCAtoreducethefeaturedimension-
sions).
alityandreshapeto7×24(daysbyhours). Theresulting
We start from public code made available by the au-
imagerepresentstime-dependenttrafficpatternslearnedby
thors. Theonlychangeswemaketothebasetrainingcon-
ourapproach.
figuration are to set the learning rate to 10−4 and remove
As observed, red correlates with higher traffic activity
thelearningratedecaypolicy(forfairnessacrossmethods).
andbluedepictslower. Forexample,thereislessactivityin
WhenintegratingGTPE,weusethesameparameterization
theevenings(midnight-7am),withanincreaseinactivityat
for location/time as the authors and set the hidden/output
8amacrossalldaysaspeopletendtobecomeactiveatthat
dims to 256 to follow precedent. Note that our goal here
time. Thevisualizationalsoshowsthatthetimeembedding
isnottoachievestate-of-the-artperformanceonthisbench-
captures additional trends, such as: 1) peak traffic around
mark,butrathertounderstandtheimpactofourGTPEmod-
5pm-6pmacrossalldays,2)Sundayhavingreducedtraffic
ule against a seminal approach for encoding location/time
throughouttheentireday,and3)increasedactivityearlyon
context.
Table S1 shows the result of this experiment in terms
of Top-1, Top-3 and Top-5 accuracy. For all methods, we
keep training parameters consistent, only changing which
context (location, loc, or time, time) is made available.
For GTPE, loc, time, and loc+time, correspond to the en-
ablingordisablingofindividualpathwaysinsidethemod-
ule. GTPEoutperformsthebaseline,includingbothavari-
anttrainedlocallywiththesametrainingconfiguration,and
theoriginalresultspresentedinthepaper. FigureS2shows
an example of the spatiotemporal distributions learned by FigureS1.VisualizingthetimeembeddinglearnedbyGTPE.
11January May September
Monarch
Butterfly
WesternHoney
Bee
FigureS2.Visualizingthespatiotemporalprior,learnedusingtheGTPEmodule,forseveralspeciesintheiNaturalist2018dataset.Darker
colorscorrespondtohigherlikelihoodofthespeciesappearingatthatlocation.
SaturdayandSundaymornings. Overall,thisvisualization
showsthatourproposedGTPEmoduleislearningtorelate
geo-temporalcontexttomobilitypatterns.
3.DynamicTrafficSpeeds++(DTS++)
WeintroducedDTS++,anewdatasetforimage-driventraf-
ficmodelingthatincludesanewcity,Cincinnati,OH.Fig-
ureS3givesavisualoverviewofthecoverageofthedataset
(traffic speeds averaged across time). It includes 11,137
overhead images (1024×1024) at approximately 0.3 me-
tersper-pixel,pairedwithoneyearofhistoricaltrafficdata
(2018)collectedfromUberMovementSpeeds[1]. InFig-
ureS4,wevisualizetheaverageroadsegmentspeedversus
time for individual days of the week. Additionally, Fig-
ureS5showstheaverageroadsegmentspeedfordifferent FigureS3.ExampletrafficspeeddatafromDTS++forCincinnati
typesofroadsusingOpenStreetMap’sroadtypeclassifica- (visualizedasaveragespeeds).
tion. ComparedtoNewYorkCity(fromDTS),Cincinnati
hasverydifferentspatiotemporalmobilitypatterns. Forex-
ample, the maximum observed traffic speed in Cincinnati
foramotorwayisontheorderof100km/h,whereinNew
YorkCityitis80km/h.
4.CapturingUncertaintyinTrafficSpeeds
Ourmethodimplicitlymodelsuncertaintyintrafficspeeds
at a location/time. This is due to our probabilistic formu-
lation, where instead of regressing traffic speeds directly,
weestimatepriordistributionsovertrafficspeeds. Theout-
putofourapproacharethelocationandscaleparametersof
a(per-pixel) Student’st-distribution, whichare aggregated
across an individual road segment and combined with the
observed count of traffic observations to form a per-road-
FigureS4. Visualizingtheaverageroadsegmentspeedovertime
segment Student’s t-distribution. During model training,
inCincinnatifordifferentdays.
we treat the ground-truth traffic speed for a given segment
as a sample from the estimated distribution, and minimize
negative log-likelihood. Figure S6 visualizes how our ap-
12Monday, 9
0.10
0.08
0.06
0.04
0.02
0.00
22.5 25.0 27.5 30.0 32.5 35.0 37.5 40.0
Traffic Speed (km/h)
Thursday, 13
Figure S5. Visualizing the average road segment speed against 0.06
time in Cincinnati, where each road is categorized by its Open- 0.05
StreetMaproadtypeclassification. 0.04
0.03
0.02
0.01
proach captures the underlying uncertainty in traffic speed
0.00
foragivenroadsegment. 27.5 30.0 32.5 35.0 37.5 40.0 42.5 45.0
Traffic Speed (km/h)
Wednesday, 17
5.AuxiliaryTasks
0.08
Figure S7 shows example outputs for road segmentation 0.06
and orientation estimation. Given an overhead image,
0.04
our method is able to identify roads and directions of
travel.Combiningtheseoutputswithourestimatesoftraffic 0.02
speedsenablesourapproachtoestimatelocalmotionmod- 0.00
20.0 22.5 25.0 27.5 30.0 32.5 35.0 37.5
els that describe traffic patterns. Finally, Table S2 shows Traffic Speed (km/h)
quantitativeresultsfortheauxiliarytasks. Thoughourob-
Sunday, 11
jectivewasnotnecessarilytoproducethebestroaddetector 0.07
ororientationestimator,theseresultsshowthatourmethod 0.06
0.05 iscapableofcapturinganotionoftraversability.
0.04
0.03
6.Application: EstimatingTravelTime 0.02
0.01
0.00
Ourapproachcanbeusedtogeneratedensecity-scaletraf- 27.5 30.0 32.5 35.0 37.5 40.0 42.5 45.0
Traffic Speed (km/h)
fic models (in space and time). One potential application
of this is generating travel time maps. We demonstrate
Figure S6. Our method outputs prior distributions over traffic
this using OSMnx [4], a library for visualizing street net- speedsthatimplicitlycaptureuncertainty.(right)Outputfromour
worksfromOpenStreetMap. Wecorrelatetheoutputfrom approachvisualizedasaprobabilitydensityfunctioncorresponds
our approach with the graph topology for New York City tothe(left)roadsegmentdepictedintheimage. Thegreenxrep-
fromOSMnx,usingroadsegmentlengthsandourestimated resentstheground-truthtrafficspeedatthegiventime.
speedstoassigntraveltimestoeachedge. FigureS8shows
onesuchresult,visualizedasanisochronemaphighlighting
regionswithsimilartraveltime.
7.DetailedArchitecture
TableS2.Evaluationofauxiliarytasks.
We provide detailed architecture descriptions for the com-
ponents of our network. Table S3 shows the feature en-
Speed Road Orientation
coder. TableS4showsourMLPdecoderusedforgenerat-
(RMSE↓) (F1Score↑) (Accuracy↑)
ing the segmentation output. Finally, Table S5 shows the
8.94 77.51% 73.78%
geo-temporalpositionalencodingmodule.
13
ytisneD
ytisneD
ytisneD
ytisneDImage Road(Label) Road(Pred.) Orientation(Label) Orientation(Pred.)
FigureS7.Qualitativeresultsfortheauxiliarytasksofroadsegmentationandorientationestimation.Orientation(−πtoπ)isrepresented
byK =16binsusingtheHSVcolormap.
FigureS8.Theoutputofourapproachcanbeusedforvariousapplications,suchasgeneratingisochronemapsreflectingtraveltime.This
mapreflectstraveltimesonaMondayat8am.Startingfromthecenter,eachbandreflectsareasreachableinfiveminuteincrements(dark
redshortest,darkbluefurthest).
14TableS3.Encoderarchitecture.
Layer(type:depth-idx) InputShape KernelShape OutputShape Param#
Encoder [1,3,1024,1024] – [1,64,256,256] –
—Sequential:1-1 [1,3,1024,1024] – [1,64,512,512] –
——Conv2d:2-1 [1,3,1024,1024] [3,3] [1,48,512,512] 1,296
——BatchNorm2d:2-2 [1,48,512,512] – [1,48,512,512] 96
——ReLU:2-3 [1,48,512,512] – [1,48,512,512] –
——Conv2d:2-4 [1,48,512,512] [3,3] [1,64,512,512] 27,648
——BatchNorm2d:2-5 [1,64,512,512] – [1,64,512,512] 128
——ReLU:2-6 [1,64,512,512] – [1,64,512,512] –
——Conv2d:2-7 [1,64,512,512] [3,3] [1,64,512,512] 36,864
—BatchNorm2d:1-2 [1,64,512,512] – [1,64,512,512] 128
—ReLU:1-3 [1,64,512,512] – [1,64,512,512] –
—MaxPool2d:1-4 [1,64,512,512] 3 [1,64,256,256] –
—ModuleList:1-5 – – – –
——MBConvBlock:2-8 [1,64,256,256] – [1,64,256,256] 44,688
——MBConvBlock:2-9 [1,64,256,256] – [1,64,256,256] 44,688
—ModuleList:1-6 – – – –
——MBConvBlock:2-10 [1,64,256,256] – [1,128,128,128] 61,200
——MBConvBlock:2-11 [1,128,128,128] – [1,128,128,128] 171,296
——MBConvBlock:2-12 [1,128,128,128] – [1,128,128,128] 171,296
—ContextEncoder:1-7 [1,2,1024,1024] – [1,64,1024,1024] –
——LocationEncoder:2-13 [1,2,1024,1024] – [1,64,1024,1024] 12,736
——TimeEncoder:2-14 [1,4] – [1,64] 12,800
——LocationTimeEncoder:2-15 [1,2,1024,1024] – [1,64,1024,1024] 42,304
—Conv2d:1-8 [1,64,1024,1024] [3,3] [1,256,1024,1024] 147,712
—ModuleList:1-9 – – – –
——MHSABlock:2-16 [1,128,128,128] – [1,4096,256] 1,213,704
——MHSABlock:2-17 [1,4096,256] – [1,4096,256] 918,024
——MHSABlock:2-18 [1,4096,256] – [1,4096,256] 918,024
——MHSABlock:2-19 [1,4096,256] – [1,4096,256] 918,024
——MHSABlock:2-20 [1,4096,256] – [1,4096,256] 918,024
—Conv2d:1-10 [1,64,1024,1024] [3,3] [1,512,1024,1024] 295,424
—ModuleList:1-11 – – – –
——MHSABlock:2-21 [1,256,64,64] – [1,1024,512] 4,363,784
——MHSABlock:2-22 [1,1024,512] – [1,1024,512] 3,182,600
TableS4.Decoderarchitecture.
Layer(type:depth-idx) InputShape KernelShape OutputShape Param#
Decoder [1,64,256,256] – [1,1,1024,1024] –
—MLP:1-1 [1,512,32,32] – [1,1024,512] –
——Linear: 2-1 [1,1024,512] – [1,1024,512] 262,656
—MLP:1-2 [1,256,64,64] – [1,4096,512] –
——Linear: 2-2 [1,4096,256] – [1,4096,512] 131,584
—MLP:1-3 [1,128,128,128] – [1,16384,512] –
——Linear: 2-3 [1,16384,128] – [1,16384,512] 66,048
—MLP:1-4 [1,64,256,256] – [1,65536,512] –
——Linear: 2-4 [1,65536,64] – [1,65536,512] 33,280
—Sequential: 1-5 [1,2048,256,256] – [1,512,256,256] –
——Conv2d: 2-5 [1,2048,256,256] [1,1] [1,512,256,256] 1,048,576
——BatchNorm2d: 2-6 [1,512,256,256] – [1,512,256,256] 1,024
——ReLU:2-7 [1,512,256,256] – [1,512,256,256] –
—Dropout2d: 1-6 [1,512,256,256] – [1,512,256,256] –
—Conv2d: 1-7 [1,512,256,256] [1,1] [1,1,256,256] 513
15TableS5.Geo-temporalpositionalencoding(GTPE)module.
Layer(type:depth-idx) InputShape OutputShape Param#
GTPE [1,2,1024,1024] [1,64,1024,1024] –
—LocationEncoder: 1-1 [1,2,1024,1024] [1,64,1024,1024] –
——LocParam: 2-1 [1,2,1024,1024] [1,3,1024,1024] –
——SirenNet: 2-2 [1048576,3] [1048576,64] –
———ModuleList: 3-1 – – –
————Siren: 4-1 [1048576,3] [1048576,64] 256
—————Linear: 5-1 [1048576,3] [1048576,64] 256
—————Sine: 5-2 [1048576,64] [1048576,64] –
————Siren: 4-2 [1048576,64] [1048576,64] 4,160
—————Linear: 5-3 [1048576,64] [1048576,64] 4,160
—————Sine: 5-4 [1048576,64] [1048576,64] –
————Siren: 4-3 [1048576,64] [1048576,64] 4,160
—————Linear: 5-5 [1048576,64] [1048576,64] 4,160
—————Sine: 5-6 [1048576,64] [1048576,64] –
———Siren: 3-2 [1048576,64] [1048576,64] 4,160
————Linear: 4-4 [1048576,64] [1048576,64] 4,160
————Identity: 4-5 [1048576,64] [1048576,64] –
—TimeEncoder: 1-2 [1,4] [1,64] –
——TimeParam: 2-3 [1,4] [1,4] –
——SirenNet: 2-4 [1,4] [1,64] –
———ModuleList: 3-3 – – –
————Siren: 4-6 [1,4] [1,64] 320
—————Linear: 5-7 [1,4] [1,64] 320
—————Sine: 5-8 [1,64] [1,64] –
————Siren: 4-7 [1,64] [1,64] 4,160
—————Linear: 5-9 [1,64] [1,64] 4,160
—————Sine: 5-10 [1,64] [1,64] –
————Siren: 4-8 [1,64] [1,64] 4,160
—————Linear: 5-11 [1,64] [1,64] 4,160
—————Sine: 5-12 [1,64] [1,64] –
———Siren: 3-4 [1,64] [1,64] 4,160
————Linear: 4-9 [1,64] [1,64] 4,160
————Identity: 4-10 [1,64] [1,64] –
—LocationTimeEncoder: 1-3 [1,2,1024,1024] [1,64,1024,1024] –
——LocParam: 2-5 [1,2,1024,1024] [1,3,1024,1024] –
——TimeParam: 2-6 [1,4] [1,4] –
——SirenNet: 2-7 [1048576,7] [1048576,64] –
———ModuleList: 3-5 – – –
————Siren: 4-11 [1048576,7] [1048576,128] 1,024
—————Linear: 5-13 [1048576,7] [1048576,128] 1,024
—————Sine: 5-14 [1048576,128] [1048576,128] –
————Siren: 4-12 [1048576,128] [1048576,128] 16,512
—————Linear: 5-15 [1048576,128] [1048576,128] 16,512
—————Sine: 5-16 [1048576,128] [1048576,128] –
————Siren: 4-13 [1048576,128] [1048576,128] 16,512
—————Linear: 5-17 [1048576,128] [1048576,128] 16,512
—————Sine: 5-18 [1048576,128] [1048576,128] –
———Siren: 3-6 [1048576,128] [1048576,64] 8,256
————Linear: 4-14 [1048576,128] [1048576,64] 8,256
————Identity: 4-15 [1048576,64] [1048576,64] –
16