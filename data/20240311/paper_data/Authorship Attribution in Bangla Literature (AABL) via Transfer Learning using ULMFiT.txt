Authorship Attribution in Bangla Literature (AABL) via
Transfer Learning using ULMFiT
AISHAKHATUN,ANISURRAHMAN,MDSAIFULISLAM,HEMAYETAHMEDCHOWD-
HURY,andAYESHATASNIM,DepartmentofComputerScienceandEngineering,ShahjalalUniversity
ofScienceandTechnology
AuthorshipAttributionisthetaskofcreatinganappropriatecharacterizationoftextthatcapturestheauthors’
writingstyletoidentifytheoriginalauthorofagivenpieceoftext.Withincreasedanonymityontheinternet,
thistaskhasbecomeincreasinglycrucialinvarioussecurityandplagiarismdetectionfields.Despitesignificant
advancementsinotherlanguagessuchasEnglish,Spanish,andChinese,Banglalackscomprehensiveresearch
inthisfieldduetoitscomplexlinguisticfeatureandsentencestructure.Moreover,existingsystemsarenot
scalablewhenthenumberofauthorincreases,andtheperformancedropsforsmallnumberofsamplesper
author.Inthispaper,weproposetheuseofAverage-StochasticGradientDescentWeight-DroppedLong
Short-TermMemory(AWD-LSTM)architectureandaneffectivetransferlearningapproachthataddresses
theproblemofcomplexlinguisticfeaturesextractionandscalabilityforauthorshipattributioninBangla
Literature(AABL).Weanalyzetheeffectofdifferenttokenization,suchasword,sub-word,andcharacterlevel
tokenization,anddemonstratetheeffectivenessofthesetokenizationsintheproposedmodel.Moreover,we
introducethepubliclyavailableBanglaAuthorshipAttributionDatasetof16authors(BAAD16)containing
17,966sampletextsand13.4+millionwordstosolvethestandarddatasetscarcityproblemandreleasesix
variationsofpre-trainedlanguagemodelsforuseinanyBanglaNLPdownstreamtask.Forevaluation,we
usedourdevelopedBAAD16datasetaswellasotherpubliclyavailabledatasets.Empirically,ourproposed
modeloutperformedstate-of-the-artmodelsandachieved99.8%accuracyintheBAAD16dataset.Furthermore,
weshowedthattheproposedsystemscalesmuchbetterevenwithanincreasingnumberofauthors,and
performanceremainssteadydespitefewtrainingsamples.
CCSConcepts:•Computingmethodologies→Informationextraction;Supervisedlearningbyclassi-
fication;Neuralnetworks.
AdditionalKeyWordsandPhrases:Authorshipattribution,TransferLearning,Languagemodel,AWD-LSTM,
Bangla
ACMReferenceFormat:
AishaKhatun,AnisurRahman,MdSaifulIslam,HemayetAhmedChowdhury,andAyeshaTasnim.2022.
AuthorshipAttributioninBanglaLiterature(AABL)viaTransferLearningusingULMFiT.J.ACM 37,4,
Article111(August2022),29pages.https://doi.org/10.1145/1122445.1122456
1 INTRODUCTION
Authorshipattribution(AA)isadistincttypeofclassificationtaskthatdealswithidentifyingthe
authorofananonymouspieceofwritingwithinasetofprobableauthors.Identifyingtheoriginal
Authors’address:AishaKhatun,aysha.kamal7@gmail.com;AnisurRahman,emailforanis@gmail.com;MdSaifulIslam,
saiful-cse@sust.edu;HemayetAhmedChowdhury,hemayetchoudhury@gmail.com;AyeshaTasnim,tasnim-cse@sust.edu,
DepartmentofComputerScienceandEngineering,ShahjalalUniversityofScienceandTechnology,Kumargaon,Sylhet,
Bangladesh,3114.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfee
111
providedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeand
thefullcitationonthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACMmustbehonored.
Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires
priorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
©2022AssociationforComputingMachinery.
0004-5411/2022/8-ART111$15.00
https://doi.org/10.1145/1122445.1122456
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.
4202
raM
8
]LC.sc[
1v91550.3042:viXra111:2 Khatun,etal.
authordependsoncapturingtheelusivecharacteristicsofanauthor’swritingstylefromtheir
digitizedtextsbyanalyzingstylometricandlinguisticfeatures.Despiteextensiveuseofstylometry
toidentifytheliterarystylesofauthors,AAischallengingbecauseof(i)thecomplexstructure
of language (ii) similarity of the topic of discussion, (iii) implicit writing styles of authors, (iv)
knownformsofwriting,suchasnovelsorstories,oftenhavesimilarstructures.Extractinguseful
informationaboutanauthor’sstyleisdifficult,especiallytobuildanend-to-endsystemtodetect
theauthorofatext.
Anonymityiswidespreadinrecenttimes,primarilyduetothewidespreaduseoftheinternet;
theuseandmisuseofanonymityhavebecomeanessentialfactortoconsider.Theplethoraof
unattributeddigitalfootprintsmakesauthorshipattributionindispensableinvariousfields,andits
applicationsareconstantlygrowing.Applicationofauthorshipattributioncoverssectorssuchas
forensicinvestigation,plagiarismdetection,computersecurity,criminallaw,cyber-crime,literature,
trading,etc.Intelligenceagenciescanuseittolinkinterceptedmessagestoknownenemies;original
authorsofharassingmessagescanbeidentified.Ineducation,studentsubmissionscanbeverified,
andactualauthorscanbeidentifiedusingauthorshipattribution.
EarlyapproachesofAAfollowedstatisticalmethodsthatsufferedfromtextlengthlimitation
problemsandfailedtocapturethetext’ssemanticmeaning.Moststylometricstudiesusesome
languageitems,suchaslexicalandsyntacticcomponents.Amongthem,charactern-gramhasbeen
usedwidelyandshowntoprovideinsightsintothewritingstyleoftheauthors[58].Inrecent
studies,machinelearningapproaches[65]areusedtoextractcomplexandessentialfeaturesofthe
texttoidentifytheoriginalauthor’swritingstyle.Someresearchextractedsemanticinformation
usingpre-trainedword,andcharacterembeddings[34].Withincreasingcomputingpower,various
deeplearningmodelshavealsobeensuccessfullyappliedtothetask,achievingimpressiveresults
[30,52].
AAisnotlimitedtoanyspecificlanguageandpertainstoanywrittentext.BanglaisanIndic
language,andthe7thmostspokenlanguageglobally,sharedamongthepeopleofBangladesh,
theIndianstatesofWestBengal,Tripura,Assam,andtheglobalBangladiasporacommunities
[62].DespitesuccessinEnglishandotherlanguages[30,36,52,59],Banglalackssignificantwork
inthisareaduetoitshighinflection"withmorethan160differentinflectedformsforverbs,36
differentformsfornouns,and24differentformsforpronouns"[8].Moreover,Banglaexhibits
diglossia,ShadhuandCholitoforms:thesearetwodifferentwritingstyleswithslightlydifferent
vocabulariesandsyntax.Althoughtheformerisuncommoninmodernwritings,itconstitutesa
vastmajorityofclassicalliteratureinBangla,andbothformssharealargenumberofcommon
roots.AllthesecontributetothecomplexwrittenformofBanglaanditsextendedvocabulary.
OnlyafewresearchworkwasdonetoconductAuthorshipAttributioninBanglaLiterature
(AABL),butmostofthemlackinsomeimportantaspect.Mostworksapplytraditionalapproaches
toextractfeatureslikewordlength,sentencelength,wordusefrequency,etc.,alongwithn-grams
[2,17,25,28].Besides,embedding-basedmodelshavealsobeenusedtoencodewords[12]and
characters[34]toextractauthorialinformationwithoutanymanualfeatureselection.Although
afewpromisingstudieshavebeendoneusingdeeplearning,existingsystemssufferduetothe
substantialdependenceonthedataset.Thelimitationsofthepresentsystemsaredescribedbelow:
(1) MostAABLworksusemanualfeatureengineering,whichiscorpusdependentandlabour-
intensive.High-performingend-to-endsystemsarenotwidelyavailableforAABL.
(2) Lack of resources and scarcity of open benchmark datasets limit existing systems’ rapid
developmentandexpansion.Mostpreviousworksuseavailablesmalldatasetsofamaximum
of10authorsforthispurpose[12,17,34].
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.AABL 111:3
(3) Current systems are not scalable and perform poorly for increasing number of authors,
especiallywherenumberofsampletextsperauthorislimited,whichisthecasegenerally
forreal-lifescenarios.
(4) Despitethedeepneuralnetwork-basedmodel’sgoodperformance,theysufferduetothe
useofsmalldatasetsandshortlengthoftexts.Theytypicallyrequiremanysamplesforeach
author,whichiscostly,time-consuming,andnotalwaysavailable.
Alinguisticallydriventransferlearningmethodwasproposedin[27],whichusespre-trained
languagemodelstrainedonunsupervisedlargetextcorporatoimproveperformanceinvarious
downstreamtasksthroughrepresentationlearning.Withthehelpoftransferlearning,itispossible
tothesolveexistingproblemsandbuildahighlyscalable,robustsystemforauthorshipattribution.
Hence,anelaboratestudyontheapplicationoftransferlearningforauthordetectioninBangla
usinglanguagemodelingispresentedinthispaper.Thelanguagemodelactsasare-usablelanguage
learnertrainedonalargecorpusinanunsupervisedmanner.Thenthesemanticandsyntactic
informationlearnedbythelanguagemodelisleveragedtoperformauthorshipattributioninthe
targetdatasets.
1 Language model training on general corpus 2 Fine-tuning with authorship attribution dataset 3 Authorship Attribution
Probable author
Next token prediction Next token prediction Classifier
AWD-LSTM AWD-LSTM AWD-LSTM
Embedding Layer Embedding Layer Embedding Layer
W1 W2 W3 W4 Wn Wn+1 W1 W2 W3 W4 Wn Wn+1 W1 W2 W3 W4 Wn
Tokenization (general data) Tokenization (Task specific data)
Fig.1. Schematicdiagramoftheproposedsystem.Thefirsttwostepsarelanguagemodellingtasks,andthe
laststepisauthorshipattribution.TheembeddinglayerandtheAWD-LSTMbaseremainthesameinallsteps,
onlythefinaldenselayerchanges.Thetrainedweightsofthefixedpartsarepassedonfrompre-trainingto
fine-tuningtotheclassificationstep,updatedineachstep.
Inthispaper,wepresentaneffectivetransferlearningapproachthatusestask-specificoptimiza-
tionwithlanguagemodelspre-trainedontwodifferentdatasetsduringthetrainingphaseand
fine-tunedontargetdatasets.Figure1showsabriefoverviewofourmethodologywhereweuse
AWD-LSTM(AverageStochasticGradientDescentWeight-DroppedLSTM)[44]architectureas
thelanguagemodel.ThelanguagemodelisfirsttrainedwithageneralBanglacorpusandthen
fine-tunedwithauthorshipattributiontextinanunsupervisedmanner.Later,aclassifierisaddedto
thepre-trainedlanguagemodels,andsupervisedauthorshipattributioniscarriedout.Additionally,
theeffectsofvarioustokenizationonproposedmodelsareanalyzedintermsofperformance;the
robustnessistestedagainsttheexistingmodelsbyincreasingthenumberofauthorsanddecreasing
samplesperauthor.Extensiveexperimentsweredonewithallthevariations,andthebestmodel
forAABLwasidentified.Theresultsdemonstrateaclearsuperiorityofthetransfer-learning-based
approachagainsttheothertraditionalmodels.
Ourcontributionsarethree-fold:
(1) Inthisarticle,weintroducethelargestandmostvarieddatasetforAABLwithlongtext
samplesof16authorsinanimbalancedmanner,imitatingreal-worldscenariosmoreclosely.
(2) Wepresentanintuitivelysimplebutcomputationallyeffectivetransferlearningapproachin
whichthemodelispre-trainedonalargecorpus,fine-tunedwiththetargetdataset,andlater
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.111:4 Khatun,etal.
theclassifieristrainedwithlabelleddataforAABL.Tothebestofourknowledge,noworkhas
beendonetoleveragethepoweroftransferlearningforAABL,reducingmanuallabourand
enhancingmodelre-usabilitymanifold.Experimentalresultsshowthattheproposedmodel
considerablyoutperformstheexistingmodelsandachievesstate-of-the-artperformance,
efficientlysolvingthelimitationsofthepreviousworks.
(3) Thevariouslanguagemodelstrainedinthisworkcanbeusedaspre-trainedmodelsfor
manydownstreamtasksinBanglalanguage.Allofthesepre-trainedmodels,alongwiththe
codeanddatasetofthispaperhavebeenreleasedforpublicuse1.
Thepaperisorganizedasfollows.Section2givesanoverviewofthepreviousrelatedwork.
Section3describesallthedatasetsusedinthisworkwithbriefstatisticalreports.Section4contains
the methodology of our work. In Section 5 we describe the proposed model along with some
traditionalmodelsusedforcomparison.Section6showstheresultsandtheiranalysis.Section7
containsinformationaboutthereleasedpre-trainedmodels.Finally,conclusionsanddirectionsfor
furtherresearcharesummarizedinSection8.
2 RELATEDWORK
2.1 OnAuthorshipAttribution
Authorshipattributiontaskisanessentialresearchtopicandhasbeenprevalentforquitealong
time. Researchin authorship attribution relies on detecting authors’style through stylometric
analysis, assuming that the authors subconsciously use homological idiolect in their writings.
Differentkindsoffeatureextractionbasedmethodsareimplementedtodetectthesepatterns.The
featuresareclassifiedaslexical,character,syntactic,andsemantic.Functionwords,punctuation,
ratioofcertainwords,averagewordlength,averagesentencelength,vocabularyrichness,bagof
n-grams,etc.,areextractedtodeterminetheauthorialstyles[17,20,25,53].Functionwordsare
partsofspeechthatdonothavelexicalmeaningbutexpressgrammaticalrelationshipsamong
otherwordsandcanbeusedtoidentifytheauthorsfromtheirsub-consciouspatternoftheuse
ofthesewordsandotherwritingcharacteristics.NaiveBayesandSupportVectorMachineswere
usedwithfeaturessuchasratioofpunctuation,apostrophesperword,uppercaseratio,etc.[7].
Wordfrequency,part-of-speechbigrammodel,andpreterminaltagbigrammodelwerealsoused
[23].Topicmodelingapproacheswereemployedtodetectauthorsofunknowntexts[49,55]
Mostoftheearlierresearchersdealtwithsmallcollectionswhereeachauthormayhavebeen
heavilyinclinedtowardsparticulartopicswhichmadetheauthorshipattributiontaskborderon
topicclassification[53].Charactern-gramscanpickupauthornuancescombininglexicaland
syntacticinformation,andsoareanessentialsetoffeatures[53,58].Acombinationoflexicaland
syntacticfeaturesprovidesvaluableinformationandhasshownenhancementinperformance[36].
Suchcombinationscanbecrucialtoimproveperformanceincross-topicauthorshipattribution
andsingle-domainattribution[59].
Proceedingwiththeadvancementofdeeplearning,alargebodyofworkisavailableonauthor-
shipattributionandstylometryusingvariousdeeplearningmodels.Forexample,multi-headed
recurrentneuralnetworkcharacterlanguagemodelwasusedthatoutperformedtheothermethods
inPAN2015[6].Someworksusedsyntacticrecurrentneuralnetworks,whichlearndocument
representationsfromparts-of-speechtagsandthenuseattentionmechanismstodetecttheau-
thorialwritingstyle[30].Convolutionalneuralnetworkshavealsobeenemployedforthistask.
Impressiveperformancewasachievedbyusingcharacter-levelandmulti-channelCNNforlarge-
scaleauthorshipattribution[52].Charactern-gramshelpidentifytheauthorsoftweets,andCNN
architecturescapturethecharacter-levelinteractions,representingpatternsinhigherlevels,thus
1https://github.com/tanny411/Authorship-Attribution-using-Transfer-Learning
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.AABL 111:5
detectingdistinctstylesofauthors[56].Acombinationofpre-trainedwordvectorswithonehot
encodedPOStagwasalsoused[24].Othersinvestigatesyntacticinformationinauthorshiptask
bybuildingseparatelanguagemodelsforeachauthorusingpart-of-speechtagsbesideswordand
character-levelinformation[21].
2.2 OnBangla
DespitethesignificantprogressinEnglishandotherwesternlanguagesforauthorshipattribution,
notmuchhasbeendoneintheBanglalanguage,especiallywithtransferlearningmethods. A
notablebodyofliteratureexistsontheuseofhand-drawnfeaturestoextractauthorialstylesfrom
texts.FeaturessuchasPOStagcount,wordfrequency,wordandsentencelength,type-tokenratio,
unique words percentage, common words, spelling variation, etc., are often used. Some of the
suchextractedfeatureswereusedwithadatasetof4[17]and3[9]currentBangladeshiauthors.
The use of N-gram features is the most common along with other extracted features and has
beenusedwithprobabilisticclassificationmethods[18],votingclassifierwithcosinesimilarity
measurementsfor6authors[25],NaiveBayesfor3authors[4,49],Randomforestsfor10authors
[29],andFeed-forwardneuralnetworksfor3,5and23authors[2,28,48].SVMwasemployedona
datasetof6authors[46].Islametal.usedunigram,bigram,trigram,part-of-speechlikepronoun
andconjunctionasfeatures,removedstopwords,andusedInformationGain(IG)toselectthe
mostimportantfeatures.TheclassificationwascomparedbetweenNaiveBayes,Decisiontree,
andRandomforestclassifiers[29].Ontheotherhand,Phanietal.usedtermfrequency(tf),term
frequency-inversedocumentfrequency(tf-idf),andbinarypresencemethodsonstopwords,word
andcharacterunigrams,bigrams,andtrigramstoperformauthorattributionwithNaiveBayes,
SVMandDecisionTrees[49].
Traditionalmethodsofusingn-gramscountsconsidereachfeatureindependentofmeaning
andcontext.Instead,allwordscanbecombinedunderacontinuousvectorspacebyrepresenting
eachwordortokenwithahighdimensionalvector.Thesevectorsarecalledwordembedding
andcanbetterrepresentsemanticandsyntacticsimilarityamongwordssuchthatsimilarwords
arecloserinthevectorspacethanwordsthataresemanticallydifferent.Theeffectsofvarious
kindsofwordembeddingsonauthorshipattributionwereanalyzedona6authordatasetwith
multipleneuralnetworkarchitecturesshowingthatusingfastTextskip-gramwordembeddings
withaConvolutionalNeuralNetwork(CNN)performsbesttoextractauthorialinformationfrom
texts[10,12,13].Besides,acharacter-levelCNNmodelwasusedfor6to14authordetectionwhere
characterembeddingswerepre-trainedonalargercorpus[34].
2.3 OnTransferLearning
Transferlearningistheprocessofreusingamodeltrainedonaninitialtaskasastartingpointfora
differenttask.Deeplearningapproacheshavebeenusingthismethodtoboostmodelperformance
andsaveanenormousamountoftimeonvariouscomputervisiontasksandnaturallanguage
processing(NLP).Transferlearninghasbeenprevalentincomputervisionforalongtime,and
pre-trainedmodelsonmassivedatasetsarereadilyavailable.Naturallanguageprocessingstarted
approachingtransferlearningbyusingpre-trainedwordembeddingswhichaimatonlythefirst
layerofthemodel. Someapproachescombinemultiplederivedembeddingsatdifferentlayers
[47].Theideaofusingtransferlearningfromlanguagemodelshasbeenapproached[16]butnot
widelyadoptedduetoitsneedforlarge-scaledatasets.Toaddresstheseissues,amethodcalled
ULMFiTwasproposedthatsuccessfullyenablesrobusttransferlearninginanyNLPtask[27].Later
transformer-basedtransferlearningapproacheswereabletoreducetrainingtimesignificantly
[19,40,51].Thesearchitecturesuseattentionmechanism,whichworksontheentiresequence
simultaneouslyinsteadofworkingtokenbytokenlikeinLSTM.Manyvariationsofthesemodels
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.111:6 Khatun,etal.
havebeenusedinnumerousNLPtasks.Pre-trainedmodelsusingmultiplelanguagesarealsomade
availableforvarioustransformer-basedarchitectures[14,19,39,64].
TransferlearninghasbeenadoptedinBanglalanguagetasksinbothcomputervisionandNLP.
VariouscomputervisiontasksexistsuchashandwrittenBanglawordrecognition[50],Banglasign
languagerecognition[45],BengaliEthnicityandGenderClassification[32]etc.InNLP,transfer
learning started with word, and character embeddings [12, 34]. BengFastText was created as
thelargestBanglawordembeddingmodelusing250millionarticles[33].BERT[19]wasused
forsentimentanalysisincode-mixedEnglish-Banglasocialmediadata[31].Thecontextualized
embeddingsfromBERTwereusedforBanglanamedentityrecognitiontask[5].Transferlearning
frompre-trainedmodelswasalsousedfordetectingfakenewsintheBangla[26].Nosuchwork
onauthorshipattributionintheBanglalanguagehasbeendonetodate.
3 CORPORA
ThescarcityofstandardbenchmarkdatasetsintheBanglalanguagemakesthetaskofauthorship
attributionquitetricky.MostresearchersinBanglahaveuseddatasetscontaining3-6authorson
average.Forthispurpose,wecreateadatasetof16authorsandnameitBAAD16.Itisthelargest
datasetforauthorshipattributioninBangla,containingtextsfrom16authorsand750wordsper
samplewith17966samplesintotal(MoredescriptioninSection3.1),unlikemostdatasetswhich
haveupto10authorsatmost[17,28,29,48].Oneexceptionisadatasetwith23authors[2],itisnot
publiclyavailableforuse,andthewordcountforeachdocumentinthisdatasetisonly50wordson
average,makingitashort-textdetectiondataset.Wealsouseadatasetof6authorscalledBAAD6,
whichhas1000wordsperdocument.Bothofthesedatasetsarelong-text-containingdatasets,which
becomesmorechallengingfortraditionalmachinelearningtechniquestohandle.Acomparison
ofexistingdatasets,alongwiththeirrespectiveperformance,willbeshowninSection6Table
9.Besides,alargercorpusofBanglatextisrequiredtoperformpre-traininginanunsupervised
manner.Forthisstep,weuseaNewscorpusandaWikipediacorpus.Intotal,fourdatasetswere
usedindifferentstepsoftheexperiments.Descriptionsofallthedatasetsusedinthisworkare
givenbelow.
3.1 BAAD16:BanglaAuthorshipAttributionDatasetof16authors
ThisdatasethasbeencreatedbyscrapingtextfromanonlineBanglae-libraryusingacustomweb
crawlerandcontainsliteraryworksofdifferentBanglafamouswriters.Itcontainsnovels,stories,
series,andotherworksof16authors.Eachsampledocumentiscreatedwith750words.Theaimis
tocreatealong-textdataset,butthelengthisoftencappedbythememoryusageoflonginput
stringsandthelengthofcontentusablebymodelstolessthan1000words.Thereforewechosea
lengthof750tohelpprovideasmuchcontextaspossibleforanygivenpieceoftext.InTable1
detailsareshownaboutthedataset,andFigure2ashowsthedistributionoftextsamplesforeach
ofthe16authors.Thisdatasetisimbalanced,asapparentfromFigure2andresemblesreal-world
scenariosmoreclosely,wherenotalltheauthorswillhavealargenumberofsampletexts.Thisis
thelargestdatasetinBanglaliteratureforauthorshipattributionintermsofthenumberofwords
availableperauthorandthetotalnumberofwords(13.4+million).Figure2bshowsthedistribution
ofthetotalnumberofwordsandtheuniquewordcountforeachauthor.Wefurthercreatemultiple
balancedsubsetsofthisdatasetforvariousnumbersofauthorsinordertomeasuremodelstability
withincreasingauthornumbers.Inthiscase,thedatasetsaretruncatedtotheminimumnumberof
samplesavailableperauthor.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.AABL 111:7
1e6
3.5
totalWords
4000 3.0 uniqueWords
2.5
3000
2.0
2000 1.5
1.0
1000
0.5
0 0.0
Candidate Author Candidate Author
(a)Sampledistribution (b)Wordsdistribution
Fig.2. BAAD16dataset:Distributionperauthor.Authorsareindicatedbytheirindexnumber.Seeindicesin
Table1
Table1. BAAD16datasetstatistics
AuthorId AuthorName Samples WordCount UniqueWord
0 zahir_rayhan 185 138k 20k
1 nazrul 223 167k 33k
2 manik_bandhopaddhay 469 351k 44k
3 nihar_ronjon_gupta 476 357k 43k
4 bongkim 562 421k 62k
5 tarashonkor 775 581k 84k
6 shottojit_roy 849 636k 67k
7 shordindu 888 666k 84k
8 toslima_nasrin 931 698k 76k
9 shirshendu 1048 786k 69k
10 zafar_iqbal 1100 825k 53k
11 robindronath 1259 944k 89k
12 shorotchandra 1312 984k 78k
13 shomresh 1408 1056k 69k
14 shunil_gongopaddhay 1963 1472k 109k
15 humayun_ahmed 4518 3388k 161k
Total 17966 13474500 590660
Average 1122.875 842156.25 71822.25
3.2 BAAD6:BanglaAuthorshipAttributionDatasetof6authors
BAAD6isa6AuthordatasetcollectedandanalyzedbyHemayetetal.[12].Thisdataset’stotal
numberofwordsanduniquewordsare2,304,338and230,075respectively.Thedatawasobtained
fromdifferentonlinepostsandblogs.Thisdatasetisbalancedamongthe6Authorswith350sample
textsperauthor.ThetotalwordcountandnumberofuniquewordsperauthorareshowninTable
2.Thisisarelativelysmalldatasetbutisnoisygiventhesourcesitwascollectedfromandits
cleaningprocedure.Nonetheless,itmayhelpevaluatetheproposedsystemasitresemblestexts
oftenavailableontheInternet.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.
selpmas
fo
rebmuN
0 1 2 3 4 5 6 7 8 9 01 11 21 31 41 51
sdrow
fo
rebmuN
0 1 2 3 4 5 6 7 8 9 01 11 21 31 41 51111:8 Khatun,etal.
Table2. BAAD6datasetstatistics
Author Samples Wordcount Uniqueword
fe 350 357k 53k
ij 350 391k 72k
mk 350 377k 47k
rn 350 231k 50k
hm 350 555k 72k
rg 350 391k 58k
Total 2100 2304338 230075
Average 350 384056.33 59006.67
3.3 NewsCorpus
ThiscorpuscomprisesofvariousBanglanewspaperarticlesin12differentcategories.Itissignifi-
cantlylargecomparedtothedatasetsusedinpreviousworksinBangla.Thereare28.5+million
wordtokensinthiscorpus,andthenumberofuniquewordsis836,509formingaround3%ofthe
totalvocabulary.Table3showsthedetailsofthedataset,alongwithsomenecessarystatistical
measures.
Table3. Newsdatasetstatistics
Category Samples Wordcount Uniqueword
opinion 8098 4185k 243k
international 5155 1089k 86k
economics 3449 909k 58k
art 2665 1312k 154k
science 2906 697k 76k
politics 20050 6167k 196k
crime 8655 2016k 128k
education 12212 3963k 225k
sports 11903 3087k 174k
accident 6328 1086k 77k
environment 4313 1347k 103k
entertainment 10121 2669k 204k
Average 7988 2377803 144342
Total 95855 28533646 836509
3.4 WikipediaCorpus
BesidestheNewscorpus,wealsotestourmethodbypre-trainingonaWikipediacorpus.Asubset
oftheBanglaWikipediatextwascollectedfromtheBanglawiki-dump2.Thefilesarethenmerged,
andeacharticleisselectedasasampletext.AllHTMLtagsareremoved,andthepage’stitleis
strippedfromthebeginningofthetext.Thisdatasetcontains70,377sampleswiththetotalnumber
ofwordsapproximatingat18+million.Theentiredatasethasmorethan1.2millionuniquewords,
whichis7%ofthetotalwordcount.Comparedtothenewsdataset,thismakestheWikipedia
datasetmorevariedregardingthetypesofwordsused.
2collectedon10thJune2019
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.AABL 111:9
4 METHODOLOGY
4.1 Overview
Transfer learning generally implies three basic steps: (i) Pre-training, (ii) Fine-tuning, and (iii)
Downstreamtasks.Inthiscase,thedownstreamtaskisauthorshipattribution.Wepre-trainthe
modelsfromscratchduetotheunavailabilityofpre-trainedBanglalanguagemodels.First,we
tokenizeourtextsinoneofthreeways-word,subword,orcharacter.Thenwetrainthelanguage
modelonalargeBanglatextcorpus.Toperformthedownstreamtask,wefine-tunethelanguage
modelusingtextfromthetokenizedauthorshipattributiondatasetinanunsupervisedmannerand
thenfinallyperformclassificationbyaddingaclassifiertothetrainedlanguagemodel.Theentire
procedure,startingfromtokenizationtoauthoridentification,waspreviouslyshownschematically
inFigure1.
Theideaofusingsuchasetupismuchlikethetraditionalapproachofusingasingleword-
embeddinglayer,onlyhere,thelanguagemodelisamulti-layereddeepneuralnetworkonitsown,
whichiscapableofholdingmuchmoreinformationinitsweightsthanasinglelayerofembedding
matrixcould[27].ThetraditionalapproachesofauthorshipattributioninBanglaLanguage,even
indeeplearningmethods,implementaclassificationmodelwithwordrepresentationsorother
extractedfeatures,takingminimalregardforthelanguageonitsown.However,thegeneralidea
ofthisresearchistocapturetheessenceoftheBanglalanguagefirst,thenproceedtothemore
granulartaskofclassificationwiththehelpoflanguagemodels.Afterthetrainingphase,wetest
withsomeexamplesandfindthatthemodelscreatedinourexperimentscancompleteentireBangla
paragraphssomewhatmeaningfullywithminimalgrammaticalorsemanticflaws.So,itissafeto
assumethattheweightsofthelanguagemodelsholdenoughrelevantinformationtounderstand
thepatternsatwhichBanglawordsappearoneafteranotherinsteadofasingleword-embedding
layer,whichonlyrepresentstherelationshipamongwords.
4.2 Tokenization
Traditionally tokenization in Bangla has been carried out at the word level, separating words
byspaces,includingpunctuationorspecialcharactersasseparatetokens.Nevertheless,Bangla
languagehassomedistinctcharacteristicswhichmakeamereseparationbywordslessmeaningful.
Lemmatizationandstemmingprovideawayforremovinginflection,butthisprocessremains
arduous,andonlybasicrule-basedsystemsareavailabletoperformthistaskinBangla.Besides,
theremovedpartsofthewordsarenotalwayswhollymeaninglessandmayprovidemeaningin
termsofgender,person,case,number,oranimacy.Informationprovidedbydeclensionisalsoa
necessarypartofthelanguagetoconsider.Therefore,besidesword-levelbasictokenization,we
alsoperformsub-wordandcharacterleveltokenizationandcomparethevariousmethods’effects
inthefinalresult.Allthesemethodsaredescribedelaboratelyinthefollowingsubsections,andthe
tokenizationofasampletextisshowninFigure3.
4.2.1 Wordlevel. Wordleveltokenizationisperformedmainlybyconsideringwordsseparatedby
spacesasseparatetokens.Besides,wordsthatoccurlessthanthreetimesarediscarded,considering
them as rare words, names, or misspellings. We select the most frequent 60,000 words as the
vocabularytoproceedtothenetwork.Unknownwordsarereplacedwiththe<unk>token.Some
specializedtokensarealsoadded,suchasthebeginningofastring<bos>,endofastring<end>,
padding<pad>,characterrepetition,andwordrepetitionrepresentingtokens,etc.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.111:10 Khatun,etal.
জনি য় ' বাংলা ! ' রচনা
Word
level
tokenization
Sub-word level tokenization
জনি য় 'বাংলা!' রচনা জন ি  য় ▁ ' বাং লা ! ' ▁ রচনা
(Popular 'Bangla!' prose) Character
level
tokenization
জ ন প ◌্ র ি◌ য় ' ব ◌া ◌ং ল
◌া ! ' র চ ন ◌া
Fig.3. Exampleofthreekindsoftokenizationofasampletext.Punctuationaddedfordemonstrationpurposes.
4.2.2 Sub-wordlevel. Sub-wordtokenizationmeansdividingawordintopartsandusingeachpart
asatoken.Despitehavingconsiderableinflection,thereisnospecializedsub-wordtokenizerfor
Banglalanguage.Totacklethisproblem,sub-wordtokenizationwasperformedusingSentencePiece
tokenizer[38].SentencePieceisalanguageindependenttokenizerthattokenizesrawsentences
inanunsupervisedmanner.UsingSentencePiece,theunigramsegmentationalgorithm[37]was
employedtocreatethesub-wordvocabulary.Fortraining,wechose30,000mostfrequenttokens
[15].Tokensappearinglessthanthreetimeswerediscardedandreplacedwith<unk>token.Other
tokensinclude<s>asthestartand</s>astheendofasentence.
4.2.3 Characterlevel. Incharacterleveltokenization,asentenceissplitintocharacters,andeach
characterisconsideredatoken.Inthiscase,thetotalnumberoftokensismuchsmaller,188inour
case,bycombiningBangla,Englishalpha-numerals,andspecialcharacters.Thelanguagemodel
generatesonecharacteratatime,concatenatedtoformwords,sentences,andevenparagraphs.
Besidesthevocabulary,wealsousespecialtokens,justaswasusedinword-leveltokenization.
Theuseofcharacter-leveltokenizationnotonlyreducesthevocabularysizedrasticallybutalso
removesthebottleneckforout-of-vocabularywords,misspellings,etc.Characterscanbeusedto
buildcorrelationamonggroupsofcharactersdespitetheinflectionanddeclensionthatoccurand
thusaddadditionalmeaningtotheword.Relatedwordscanbekeptastheyare,withoutdiscarding
anypartandlosinganyinformation,andstillberecognizedasbeingrelated.
4.3 ProposedArchitecture
4.3.1 LanguageModel. Thepatternsinthewritingsofauthorsrecur,andthisnatureofthetask
suggeststheuseofarecurrentneuralnetwork(RNN).Asimplifieddiagramofthearchitecture
isshowninFigure4a.Thearchitectureconsistsofanencoderandadecoderpart.Theencoder
networkstartswithanEmbeddinglayerwithanembeddingsizeof400,followedby3regularLSTM
layers,eachwith1150hiddennodes.Ithasafewshort-cutconnectionsandnumerousdrop-out
hyper-parametersbutdoesnotuseanyattentionmechanism.Thedecoderisformedbyadense
andasoftmaxlinearlayerthatprovidestheprobabilisticestimationsforthenextwordoverthe
vocabulary. Input passes through the encoder, and the outputs from the LSTMs are passed on
tothedecoder,wherethepredictionisgenerated.WeusedAdamoptimizer[35]andflattened
cross-entropylossfunctiontotrainourmodels.
Inthiscase,weemployedaparticularvariantoftheLSTMcalledtheAWD-LSTM[43].Itstands
for ASGD(Average Stochastic Gradient Descent) Weight-Dropped LSTM. This model provides
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.AABL 111:11
Predicted Author
Softmax
Predict Next Word
A1 A2 A3 A16
Softmax
fully connected layer 50 x 16
Batch Norm 1D
W1 W2 W3 Wv
fully connected layer 1200 X 50
fully connected layer 400 x V Batch Norm 1D
O1 O2 O3 O4 On O1 O2 O3 O4 On
LSTM LSTM LSTM LSTM LSTM 1150 x 400 LSTM LSTM LSTM LSTM LSTM 1150 x 400
LSTM LSTM LSTM LSTM LSTM 1150 x 1150 LSTM LSTM LSTM LSTM LSTM 1150 x 1150
LSTM LSTM LSTM LSTM LSTM 400 x 1150 LSTM LSTM LSTM LSTM LSTM 400 x 1150
Embedding V x 400 Embedding V x 400
W1 W2 W3 W4 Wn W1 W2 W3 W4 Wn
(a)LanguageModelArchitecture (b)ClassificationModelArchitecture
Fig.4. Simplifieddiagramofthearchitectures.Thebase(embeddingandLSTMlayers)ofthemodelremain
thesametotransferlearnedweights,buttheclassifierpartsarechangedwhenthetasksarechanged.
specialregularizationtechniquessuchasdrop-connect[61]andotheroptimizationsthatmakeita
suitablechoiceforgeneralizingcontextandlanguagemodelling.Intraditionalmodels,overfitting
isasignificantissue.Drop-connecthandlesthisissuebyrandomlyselectingtheactivationsubset
onthehidden-to-hiddenweightmatrices.ThispreservestheLSTMsabilitytorememberlong-term
dependencyyetnotoverfit.Figure5providesagraphicalexampleofthedrop-connectnetwork.
Fig.5. DropConnectNetwork[1]
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.111:12 Khatun,etal.
Amongsomeothertechniquesisthegenerationofadropoutmaskonthefirstcall,namelythe
variationaldropout[22].EmbeddingDropout[22]andreductionofembeddingsizehelpedthe
architectureinachievinghighperformanceinlanguagemodelling[44].Thismodelistherefore
chosenforthesuccessfulapplicationoftransferlearning[27].
4.3.2 Classifier. Theclassifierisbuiltontopofthelanguagemodelbyonlychangingthedecoder
partofthemodel.Twolinearlayersareaddedwithbatchnormalization,anddropout[27].Activation
functionReLUisusedforintermediatelayersandsoftmaxfortheoutputlayer,whichoutputsthe
probabilitydistributionamongthegivenauthors.Theauthorwiththehighestprobabilityvalueis
thenselectedasthepredictedauthor.AdiagramoftheclassifierarchitectureisshowninFigure4b.
4.4 Trainingtechniques
Inrecentresearch,trainingneuralnetworksefficientlyhasbecomeacumulativetaskofproper
architecture selection, suitable techniques application, and fine-tuning. This section briefs the
techniquesusedinthispaper.
4.4.1 1cyclepolicy. Thelearningrateisoneofthemostcrucialhyper-parameterthatdrastically
affectsexperimentsandhasbeenmanuallytunedforquitesometime.Auniqueapproachwas
publishedinthisregard[57].Inthismethod,asingletrialisrunoverthedataset,startingwitha
lowlearningrateandisincreasedexponentiallybatch-wise.Thelossobservedforeachvalueof
thelearningrateisrecorded.Thepointofdecreasinglossandahighlearningrateisselectedas
theoptimalvalueofthelearningrate.Examplesoflearningratefinderdiagramsobtainedinour
differentexperimentalstepsareshowninFigure6.
11.0
10.8 5.85 1.8
10.6
10.4 5.80 1.6
10.2 5.75
10.0 1.4
9.8 5.70
9.6 1.2
5.65
9.4
1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00
Learning Rate Learning Rate Learning Rate
(a)Cyclicallearningrateplot (b)Cyclicallearningrateplot (c)Cyclicallearningrateplot
achieved achieved achieved
duringthepre-trainingphase. duringthefine-tuningphase. duringtheclassificationphase.
Fig.6. Thecyclicallearningrateforvariousstagesoftraining.Theentiredatasetissweptonce,andlosses
forvariouslearningrates(inlogscale)areplottedastheblueline.Theredpointsshowthesuggestedlearning
rate,wherethelosshasanegativeslopeandcontinuesassuchforawhile.
4.4.2 Discriminativelearningrate. Thechosenlearningrateisoftenfixedthroughoutthetraining
processandamongallthelayers.Thevariouslayersinadeeplearningmodelcontaindifferentlevels
ofinformation,andsometimesitmaybenecessarytoallowforsomeinformationtobepreserved
orchangedslowlyorfastcomparedtotheothers.Atechniqueknownasdiscriminativelearning
rate[27]solvestheproblem.Thismethodensuresthatthelaterlayersofaneuralnetworktrain
fasterthanthebaselayersbyapplyingarangeoflearningratesacrossdifferentlayers.Building
deeplearningmodelsontopoftheembeddinglayershasshownpromisingresultsintherecent
past,whichimpliesthatthelaterlayersofthemodelneedtobemodified.Thiscanbeaccomplished
usingahigherlearningrateforthelastlayers,whereasforthebaselayers,weusealowlearning
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.
ssoL ssoL ssoLAABL 111:13
ratetoslowlychangethepre-trainedweightsandretainthemostlearnedinformation.Figure7
illustratesagenericdeeplearningarchitecturetoshowdiscriminativelearningrates.
Fig.7. ApplicationofDiscriminativeLearningRates[42].Theinitiallayers(red)havealowerlearningrateto
retainthepre-trainedweights,whilelaterlayers(green)haveahigherlearningratetotunefastertothetask
athand.
4.4.3 Gradualunfreezing. Whenatask-specificmodelistrainedontopofapre-trainedmodelin
transferlearning,allthelayers’weightsarealteredtogether.Thismethodcarriesnoisebacktothe
baselayersfromthenewlyattached(randomlyinitialized)layers.Topreventsuchabruptalteration
and,therefore,catastrophicforgetting,weemploygradualunfreezing[27].Thismethodfirstfreezes
theinitiallayersandtrainsonlythelastlayers;then,thepre-trainedlayersareunfrozenoneby
oneandtrainedfurthertotunetotheproblem-specificdomain.Thus,thelearnedinformationof
thepre-trainedmodelisusedefficiently.
4.4.4 Slantedtriangularlearningrates. Theoriginalconceptofstochasticgradientdescent[41]
tries to employ the idea that the neural networks should be getting closer to the loss’s global
minimumvalue.Astheglobalminimumforlossgetsclosertotheminimum,thelearningrate
shoulddecreasequiteobviously.Ifitdoesn’t,thesystemmayfallinaninfiniteloopasitjumps
fromonesideoftheglobalminimatoanotherbecauseitkeepssubtractingthemultipleofthe
largelearningrateselected.Slantedtriangularlearningrate(STLR)linearlyincreasesthelearning
ratetoconvergequicklyandthenlinearlydecaysitaccordingtoanupdatedscheduletotunethe
parameters[27].
4.4.5 StochasticGradientDescentwithRestarts. Theneuralnetworktrainingphasewilllikely
encounterlocalminimainthelosscurveratherthantheglobalminimum.Themodelmayget
stuckatthelocalminimainsteadofreachingfortheglobalminimainsuchcases.Atechniquethat
approachesthisprobleminvolvesasuddenincreaseinlearningrateshopingthatitwill’jump’over
thelocalminimaifthereareany.Theprocessiscalledstochasticgradientdescentwithrestarts
(SGDR)andwasintroducedin[41].SGDRsuddenlyincreasesthelearningrateanddecreasesit
againbycosineannealing.Figure8showshowlearningratesarerestartedaftereveryepochto
avoidtheproblem.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.111:14 Khatun,etal.
Fig.8. ResettingLearningRateaftereachEpochandreducinggraduallybycosineannealing[42]
4.5 Training
Theprimaryapproachtoperformingtransferlearningwiththehelpoflanguagemodelsclosely
followsULMFiT[27].Theentireprocedureisdividedintothreebroadsteps,asshownpreviously
inFigure1.Eachstephasbeenexplainedinthefollowingsubsections.
4.5.1 Language model Pre-training. Two reasonably large Bangla datasets have been used to
performthistrainingstep.TheaimisforthemodeltolearntheBanglalanguagethroughlanguage
modeling.Forthis,weusedtheWikipediadatasetandNewsdatasetseparately;thesecontaina
varietyofBanglatextsthatcangeneralizetomosttextthatmayappearindownstreamtasks.For
allthemodels,batchsizeof32,back-propagation-through-time(bptt)of70,andweightdecayof
0.1areusedonalllayers.Weusedadrop-outmultiplierof0.5totheratiosselectedin[27]forall
thelayerstoavoidover-fitting.Anappropriatelearningrateforeachmodelisselectedthrough
acycliclearningratefinder[57].ThisisdepictedinFigure6.Sometimeslearningratesneeded
tobealteredastrainingprogressed,asdeterminedbythelossofthevalidationset.Forexample,
whenlossstartstoincrease,themodelistrainedfromapreviouscheckpointwithalowerlearning
rate.Thisisshownascomma-separatedlearningratesinTable4.Eachepochemployedstochastic
gradientdescentwithrestarts(SGDR)mentionedinSection2.Allmodelsaretraineduntilthey
starttoshowsignsofover-fitting,whichis30epochsforthewordandsub-wordlevelsand15
epochsforthecharacter-levelmodels.Importanthyper-parametersarementionedinTable4forall
themodels.
Table4. Pre-trainingLanguagemodelhyper-parameters
Dataset Tokenization Epoch Batchsize Learningrate
Word 30 32 1e-3
News Sub-word 30 32 1e-2,1e-3
Character 15 32 1e-2,1e-6,1e-2
Word 30 32 1e-2,1e-3
Wiki Sub-word 30 32 1e-3
Character 15 32 1e-2
Aftertraining,themodelsbegintolearnBangla.Whenprovidedwithoneormoreinitialwords,
themodelscancompleteentireparagraphs.Illustrationsofmodels’predictionareshowninFigure
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.AABL 111:15
Word level:
িতন মাস দীঘ  সময় ধের ধের যাওয়া এক ধরেণর ভয়ংকর  খলা । এ ট এক ট song  খলা
যােক সাধারণত সাধারণভােব বলা হয় হাফ - অফ  খলা । এই  খলা উৎসেবর সময় রােতর
 বলা রােতর  বলায় িদেয়    হেব । তেব িকছ  িকছ   খলায়  দখা যায় , তেব  বিশরভাগ
সময়ই তা হেত পাের ।
A dangerous sport going on on for three long months. It is a
song game which is in general generally called half-off sport.
This game will start in night night at the time of festival.
But in some sports it is seen, but in most of the time it can
happen
Sub-word level:
িতন মাসব াপী▁রা  য়▁স ান▁পদক▁ দােনর▁ব ব া▁করা▁হয়।▁বতম  ােন▁িচিকৎসা
▁িব ােন▁সবেচেয়▁ বিশ▁অবদােনর▁জন ▁এ▁স াননা▁ দান▁করা▁হে ।
▁১▁ ম,▁২০১৭▁তািরেখ▁সব   থম▁এই▁পুর ার▁ দান▁করা▁হয়।▁xxbos▁ইরসাল
-উল-হক▁(মতৃ   :▁১৬▁মাচ ▁১৮৯৩)▁িছেলন▁একজন▁মুসিলম▁প  ত,
▁ লখক,▁দাশিন  ক▁ও▁রাজনীিতিবদ।
Arrangements were made to confer state honors awards for three
months. At present, this award is being given for the greatest
contribution in medical science. On May 1, 2018 this award was
given for the first time. Irsal-ul-Haq (died 16 March 1893) was
a Muslim scholar, writer, philosopher and politician.
Character level:
িতন মাস পের আি কা মহােদেশর িবে র অন তম  সরা িতন ট িবষয়  যখােন
আলবানীয় স াট  জ জােলেমর উ র অিভবাসী ও সা াজ বাদী ত িকর  নামানুসাের
পিরিচত িছল এবন উে খেযাগ   রামান িবেরাধী  লােকরা এ টেক  ভািবত
Three months later the top three things in the world for the
African continent where the Albanian emperor emigrant of the
north Jerusalem were known by the name of imperialist Turks.
And noteworthy Anti-Roman people influenced it
Fig.9. Textgenerationfromthedifferenttrainedlanguagemodels.
9fromtheWikipediadatasettrainedmodelfordifferentlevelsoftokenization.Allthreemodels
weregiventhestartingwords’Threemonths’inBangla,andtherestweregenerated.Notethat
theoutputsweretokensthatwereboundtogether,removingextraspacestoformwordswhere
required(exceptinword-leveltokenization,whereoutputisalreadyinwords).Englishtranslations
forthegeneratedtextareprovidedundereachparagraph.Wereproducedallgrammaticaland
semanticerrorsproducedbythemodelinEnglishaswell.Althoughthetext,onthewhole,does
notmakemuchsense,themodelhaslearnedtoformreasonablephrasesandwords.Thislearning
isleveragedinthefine-tuningandclassificationstages.
4.5.2 LanguagemodelFine-tuning. Aftertrainingthelanguagemodelongeneraltext,itisfine-
tunedtothetask-specifictypeoftext.Eachmodelfromtheprevioussection(pre-trainedonnews
andWikipediadataset)isthenfine-tunedontwoauthorshipattributiondatasetsasmentionedin
Section3.Thisstepallowsthemodeltogetaccustomedtotheauthorialwritingstyle.Thetarget
datasetsaresplitintotrainingandtestingsetsinan80%-20%ratio,andonlythetrainingsetis
usedinthisphase.Theoriginallanguagemodelisfirstloadedandfrozentotunethemodelexcept
thelastlayersthatholdthemosttask-specificinformation.Usingcycliclearningratefinder[57],
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.111:16 Khatun,etal.
thelearningrateforeachmodelisdetermined.Themodelsweretrainedfortwoepochsandthen
trainedfortwomoreepochsbyunfreezinganotherlayergroup,repeatingitonemoretimebefore
unfreezing the entire model. The models are trained as long as the loss keeps decreasing, and
trainingishaltedonsignsofover-fitting.Learningratesarealteredasrequiredbyobservingthe
lossofthevalidationdata,depictedbycomma-separatedvaluesinTable5.Therestofthemodel
settingsarekeptthesameasbefore.Batchsize,alongwithotherhyper-parametersissummarized
inTable5.
Table5. Fine-TuningLanguagemodelhyper-parameters
Pre-training Fine-tuning
Tokenization Epoch Batchsize Learningrate
Dataset Dataset
BAAD16 19 32 1e-3
Word
BAAD6 10 32 1e-2
BAAD16 22 32 1e-3,1e-4
News Sub-word
BAAD6 10 32 2e-2
BAAD16 14 32 1e-2
Character
BAAD6 10 128 1e-3
BAAD16 26 32 1e-2,1e-3,1e-4
Word
BAAD6 10 32 1e-2
BAAD16 24 32 1e-2,1e-3
Wikipedia Sub-word
BAAD6 10 32 2e-2
BAAD16 20 32 1e-2
Character
BAAD6 10 128 1e-1,1e-2,1e-3
4.5.3 Classification. Thefinaldownstreamtaskisclassification.Themodelismodified,asmen-
tionedinSection4.3.2.Theencoderweightsfromthefine-tunedlanguagemodelareloadedinto
theclassifier.Thedecoderweightsarerandomlyinitialized.Thedropoutmultiplierissetto0.5,
andthechosenlearningrateis1e-2usingacyclicallearningratefinder.Momentumvaluesof0.8
and0.7areusedforoptimization[27].Theentiremodelisfrozenexceptthedecoder,whichis
availablefortraining.Aftertrainingwiththetrainingsetoftheauthorshipattributiondatasetfor
twoepochs,alayergroupisunfrozenandtrainedusingslicedlearningratesfortwoepochs.Slicing
mainlydistributeslearningratesamongthelayerssothattheinitiallayersareupdatedslowlyto
maintainthepre-trainedweights.Incontrast,thelaterlayers,whicharethemosttask-specific
layers,areupdatedswiftlytolearnthetaskathand.Ageneralruleofslicinglearningratehas
𝑙𝑟
beenfollowedwherethesliceconsistsofinitiallearningrate(lr)and [27].Theunfreezingstep
2.64
iscarriedoutonemoretimeandtrainedfortwoepochsbeforeunfreezingtheentiremodeland
trainingforsixepochs.Thetrainingwasstoppedwhenitstartedtooverfit.Weusedabatchsizeof
32forallthemodelsatthisstage.
5 EXPERIMENTS
Previousstate-of-the-artmodelswerere-createdtocomparetheproposedmodelsagainstbaselines,
andresultsweredrawnintermsofaccuracyandF1score.Neitherofthepreviousmodelsuses
transferlearningfromentiremodels.Theyarebasedonusingpre-trainedcharacterandword
embeddingstoboostperformance.Basedonexperimentsfrom[34],and[12],weselectthebest
performingmodelsforwordembeddings(usingfastTextskip-gram)andcharacterembeddingand
evaluatethemonBAAD16andBAAD6datasets.Besides,wealsoexperimentwithatransformer-
basedmultilingualmodelcalledmBERTandasimilarmodeltrainedonlyonBanglatextcalled
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.AABL 111:17
Bangla-BERT.Allthemodelsusedforcomparisonarediscussedinbriefbelow.Table6showsthe
varioushyper-parametersusedforthemodelsdescribedbelow.
5.1 CNN-LSTMWordlevelclassifier
The CNN-LSTM model [34] comprises of a mix of convolutional and LSTM layers suitable for
trainingthecorpusonword-leveltokenization.Thearchitectureconsistsofanembeddinglayer
thatisinitializedwithpre-trainedword-embeddingsusingskip-gramoffastText.Vocabularysize
of60,000andembeddingvectorsizeof300areused.Twoconvolutionalandmaxpoollayerpairs
followtheembeddinglayer.TheoutputsfromtheconvolutionallayersarefedintoanLSTMlayer
of100neurons,whoseoutputsareinturnfedintoafullyconnectedlayerof512neurons.Dropout
isusedinthefullyconnectedlayertopreventthemodelfromoverfitting.Afinalsoftmaxlayer
outputsprobabilitiesfortheclassification.Allparametersforthelayersarekeptthesameasthe
originallyproposedmodel.Themodelistrainedfor15epochswithalearningrateof0.001,decay
of1e-4,andbatchsizeof128.WeusedAdamoptimizerandcategoricalcross-entropyastheloss
function.
5.2 CNNCharacterlevelclassifier
Thismodelaimstousecharactersignalsasembeddingandpassesthemontoseveralconvolutional
layerstoextractinformationandlaterperformclassification.TheoriginalCNNmodel[34]hasbeen
recreatedandtrainedfortheentiredataset.Thefirstlayerconsistsoftheembeddinglayerwhose
weightsaresetasthepre-trainedweights[34]obtainedfromtrainingthenewsdatasetmentioned
inSection3.Thevocabularyandembeddingsizeis181.Thevocabularyisthesameastheoneused
withtheproposedcharacter-levelAWD-LSTMmodelexceptforthespecialtokens.Theembedding
layerisconnectedto4sequentiallyconnectedconvolutionallayers.Eachconvolutionallayeris
followedbyamaxpoollayerofkernelsize3.Afullyconnectedlayerisstackedontopwiththe
numberofnodessetas512.Thenumberoffiltersandkernelsizesfortheconvolutionallayersis
keptthesameasintheoriginalmodel.Thelastlayeristheoutputlayerwithsoftmaxactivation.
Withabatchsizeof128andamaximumtextsamplelengthof3000,themodelhasbeentrainedon
thedatasetfor15epochs.Adropoutrateof0.5isusedinthefullyconnectedlayerasregularization.
Therestofthehyper-parametersarekeptconsistentwiththeCNN-LSTMmodel.
5.3 MultilingualBERT
Inrecenttimestransferlearningwithtransformer-basedmodelshasshownpromiseinmanyNLP
tasks.Thesemodelsarelargeandarepre-trainedonhugeamountsoftextinanunsupervised
manner.OneofthemostwidelyusedmodelsisBERT(BidirectionalEncoderRepresentationsfrom
Transformers)[19].MultilingualBERT(mBERT)hasBanglaaspartofitstrainingcorpus,among
100+otherlanguages.BERT’sarchitectureisamulti-layerbidirectionalTransformerencoderbased
ontheoriginalTransformer[60].BERT’suniquenessisthatitusesbi-directionalcontextinstead
of only left-right context as in autoregressive language modeling tasks. BERT uses WordPiece
embeddings[63]whereawordmaybebebrokenintomultiplepieces,andeachpieceisprefixed
withthesymbols##toindicatetheirorigins.Thefirsttokenofallsequencesisaspecialtoken
[CLS].Thefinalhiddenstateofthistokenisusedasanaggregatesequencerepresentation.BERT
hasthreekindsofembeddings:wordembedding,positionalembedding,andsentenceembedding.
Andthefinalembeddingofthemodelisthesumofthesethreeembeddings.Weusedthesmall
versionofBERTwhichhasL=12attentionblocks,A=12self-attentionheadsandthehiddensize
isH=768[19].
For classification, we only concern ourselves with the output of the [CLS] token present at
the beginning of every sentence. We add a single-layer classifier on top of the BERT encoder.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.111:18 Khatun,etal.
Specifically,thefinalhiddenvector𝐶 ∈IRHcorrespondingtotheinputtoken[CLS]isinputtothe
classifierlayer,andthenumberofoutputsisthenumberofclasslabels;inourcase,thenumberof
authorsinthedataset.Theentiremodelisthenfine-tunedend-to-endalongwiththenewlayer.For
ourtaskofauthorshipattributioninBangla,weusethetrained,multilingualBERTmodelcalled
bert-base-multilingual-cased.Weusebatchsizeof6andtotalsequencelengthof512asperthe
originalsetting.UnliketheoriginalBERT,weuseSlantedTriangularlearningrates,Discriminative
Learningrates,andgradualunfreezing,similartoourproposedapproach.Weselectthelearning
ratesatvariousstagesusingalearningratefinder.Thuswechooseaninitiallearningrateof0.0004
fortheBAAD16datasetandtraintheclassifierfor40epochs.FortheBAAD6datasetlearningrate
of0.0008wasselectedandwastrainedfor35epochs.Thenumberofepochswasselectedbasedon
thevalidationseterrors.Whenthevalidationseterrorsstarttoincreaseoroscillate,thetrainingis
stopped.
5.4 Bangla-BERT
BERT based models have been used extensively in various tasks recently. To this end, a BERT
variationwastrainedpurelyinBanglabySarkar[54].IncontrasttomBERT,thismodelincorporates
moreBanglatrainingdatathanmBERT.ItwastrainedontheBanglacommoncrawlcorpusfrom
OSCAR3aswellastheBanglaWikipediadumpdataset.Thisusesthesamebert-basearchitecture
asinmBERTandconsistsof110Mparameters.Weusethesagorsarker/bangla-bert-basecheckpoint
fromtheHuggingFaceTransformerslibraryforthispurpose.Forthismodelalso,weuseabatch
sizeof6andamaximumsequencelengthof512aspertheoriginalmodel’ssettings.Totrainfor
theBAAD16dataset,wechooseaninitiallearningrateof1.32𝐸−6usingthelearningratefinder.
Themodelwasfine-tunedforatotalof40epochsusinggradualunfreezing,andthelearningrate
wasadjustedusingthelearningratefinderateachunfreezingstep.Thetrainingwasstoppedwhen
thevalidationsetperformancestarteddegrading,andthelossstartedincreasing.ForBAAD6,the
initiallearningratewas1.74𝐸 −3,anditwastrainedforatotalof30epochsfollowingsimilar
unfreezingandlearningrateadjustmentmethods.
Table6. Hyper-parametersofothermodelsusedforcomparison.
Pre-training Fine-tuning Batch Learning
Model Tokenization Epoch
dataset dataset size rate
News BAAD6 15 128 1e-3
CNN-LSTM Word
(fastText) BAAD16 15 128 1e-3
News BAAD6 15 128 1e-3
CNN Character
(Embedding
BAAD16 15 128 1e-3
layer)
WordPiece BAAD6 35 6 8e-4
Wikidump mBERT
(Sub-word) BAAD16 40 6 4e-4
Wikidump, WordPiece BAAD6 30 6 1.74e-3
Bangla-BERT
Commoncrawl (Sub-word) BAAD16 40 6 1.32e-6
3https://oscar-corpus.com/
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.AABL 111:19
6 RESULTSANDDISCUSSIONS
6.1 TrainandTestsplits
Aftertraining,themodelsweretestedwiththeheld-outtestset.BAAD6hasaseparatetestset
with12.5%data[12].Thesametestsetwasusedforthepurposesofconsistency.BAAD16,onthe
otherhand,holdsout20%ofthedatasetfromeachauthorasatestset.Thisdatasetwasstructured
inlinewiththeoneusedin[34]tobeabletocompareanddrawfromitsresultsdirectly.Since
thenumberofsamplesisnotinthemillions,testsizessmallerthan20%arenotrepresentative,
andchoosingtestsizesgreaterthan20%reducestrainingdatasignificantlysincethetrainingsetis
usedforbothtuningthelanguagemodelaswellastolearnclassification.
6.2 Metrics
AllthemodelsweremeasuredintermsofaccuracyandF1score.AlthoughBAAD6isbalanced,
BAAD16isnot(refertoSection3),forwhichalookattheF1-scoreisalsonecessary.Accuracy
measuresthepercentageofsamplescorrectlyidentified,whereastheF1scoreistheharmonicsum
oftheprecisionandrecall.F1-scorethusgivesustheabilitytolookintopiecescorrectlyidentified
fromeachclassinacomparableform.Table7showsthesummaryoftheresultsobtainedfrom
variousexperimentedmodelsagainstallthevariationsoftheproposedmodel.
Table7. ResultsofClassification
Pre-traning FiT
Dataset Model Tokenization Accuracy% F1Score
Dataset Perplexity
News 74.67 99.58 0.9855
Word
Wiki 60.91 99.67 0.9967
News 62.45 99.80 0.9980
ULMFiT Sub-word
Wiki 57.85 99.72 0.9972
News 3.42 98.55 0.9855
BAAD16 Character
Wiki 3.36 98.58 0.9858
Char-CNN Character News - 86.28 0.7981
CNN-LSTM Word News - 93.82 0.8934
mBERT Sub-word Wiki - 94.79 0.9283
Wiki,
Bangla-BERT Sub-word - 96.80 0.9533
Common-crawl
News 203.11 94.67 0.9469
Word
Wiki 149.58 94.33 0.9437
News 233.04 95.33 0.9536
ULMFiT Sub-word
Wiki 260.90 94.67 0.9476
News 4.47 83.67 0.8360
BAAD6 Character
Wiki 3.71 90.00 0.9007
Char-CNN Character News - 73.33 0.7147
CNN-LSTM Word News - 66.33 0.6320
mBERT Sub-word Wiki - 93.33 0.9301
Wiki,
Bangla-BERT Sub-word - 90.33 0.9012
Commoncrawl
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.111:20 Khatun,etal.
6.3 Results
FromTable7,weseethattheproposedmethodsoftransferlearningusinglanguagemodelingwith
AWD-LSTMarchitectureoutperformtheprevioushigh-performingmodelsinBanglaliterature
inthetargetdatasets.Specifically,thesub-wordtokenizedmodelshaveconsistentlyperformed
well.ThemBERTmodelperformswellconsideringthatitispre-trainedonover100languages,of
whichonlyatinyportionconsistsofBanglatexts.Butcounter-intuitively,apureBangla-BERT
modeldoesnotseemtooutperformthemBERTmodel.Onehypothesiscouldbethesizeofthe
datasetweusetotrainthemodel.Themodelmaybeover-parameterizedforthecurrentAuthorship
Attributiondatasets,andtrainingmoreandmoreepochsleadstoamemorizingissue,ultimately
causingthevalidationandtestsetstoperformpoorly.Nevertheless,weonlytrainuptothepoint
wherethevalidationlossstopsdecreasing,thusgettingthebestpossiblemodelcheckpointfrom
theavailabledatasets.
6.4 K-foldvalidation
Totesttherobustnessofourproposedsystems,weperformK-foldcross-validationonBAAD6and
BAAD16datasetswiththebestperformingmodel,i.e,theNewsdatasetpre-trainedsub-wordlevel
ULMFiTmodel.Weperforma5-foldcross-validationandaveragetheaccuraciesandF1scores
across the folds. Table 8 shows the results along with the margin of error at a 95% confidence
interval.Theauthorship-attributionfine-tunedmodelforeachdatasetwasused,i.e,westartfrom
theclassificationleveloftraining.Theyweretrainedwithanearlystoppingcallbackof2epochs.
Forbothdatasetsandallfolds,thenewlyinitializedlayerwastrainedfor2epochs,followedby
2epochswithalayergroupunfrozen,followedbyanother2epochswithanotherlayergroup
unfrozen.Thenthecompletelyunfrozenmodelwastraineduntilthelossstoppeddecreasing.For
BAAD6,itwasatotalof10epochs;forBAAD16,themodeltrainedforabout10-13epochsinvarious
folds.BesidesK-foldvalidation,moreminorvariationsofBAAD16werealsotestedseparately.The
resultsofthisanalysiscanbefoundinSection6.9.
Table8. Resultsof5-foldcross-validationforthesubwordULMFiTmodelsonBAAD6andBAAD16datasets.
Themarginoferrorat95%confidenceintervalwasreportedalongsidethemeanaccuracyandF1-scores
Pre-training
Tokenization Model Dataset Accuracy% F1Score
Datset
95.0952 0.9511
BAAD6
News Sub-word ULMFiT ±0.339(±0.36%) ±0.00347(±0.37%)
99.2597 0.9924
BAAD16
±0.436(±0.44%) ±0.00466(±0.47%)
6.5 Comparativeanalysis
Inaddition,wecompareourproposedmodeltomostexistingresearchinBanglaonauthorship
attributioninTable9,demonstratingthatourtechniquehasoutstandingperformancewhenconsid-
eringthesizeofthecorpus,lengthofsamplesutilized,andthenumberofauthorscategorized.Most
worksuseself-createddatasetswithveryfewauthors,possiblyborderingontopicclassification.
Some authors tend to write on specific topics, so classification becomes very easy and unlike
real-lifescenarios,thereforeachievinghighaccuracy.ThepubliclyavailabledatasetBAAD6isa
slightlynoisydatasetandwasusedtoevaluatetheproposedsystemandsomeoftheprevious
approaches.Ourapproachshowsimprovedaccuracyinthisdatasetat95.33%fromtheprevious
92.9%usingfastTextwordembedding[12].
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.AABL 111:21
Table9. ComparisonoftheproposedsystemwithexistingworksinBanglaauthorshipattribution(ascending
orderofthenumberofauthorsinthedataset).The*markindicatesthattheworkwasreplicatedinthis
paperandappliedtotherespectivedataset.
Average Average
No.of Accuracy
Paper/Dataset word/ samples/ Method
author (%)
author author
Probabilisticclassification
91.67
Dasetal.[18] 3 325k 12 unigram+vocabularyrichness
Probabilisticclassification
100
bigram+vocabularyrichness
Chakraborty
3 4921k 150 ExtractedfeatureswithSVM 83.3
etal.[9]
Anisuzzaman
3 35k - N-gramswithNaiveBayes 95
etal.[4]
TfidfofN-gramsofbothword&
98.93
Phanietal.[49] 3 104k 1000 character,stopwordswithSVM
Topicmodelingasfeatures
100
withNaiveBayes
N-gramsandextractedfeatures
Islametal.[28] 5 - 394 89
withMLP
N-gramsandextractedfeatures
Hossainetal.[25] 6 185k 100+ 90.67
withvotingclassifier
WordembeddingwithMLP 85.46
Chowdhury
WordembeddingwithLSTM 89.6
etal.[12](BAAD6)
WordembeddingwithCNN 92.9
Wordembeddingwith
61
Chowdhury 6 384k 350 NaiveBayes
etal.[11](BAAD6) WordembeddingwithSVM 84.4
Wordembeddingwith
80.8
heirarchicalclassifier
Khatun
CharacterEmbeddingwithCNN 73.3
etal.[34](BAAD6)*
Khatun WordEmbeddingwith
66.3
etal.[34](BAAD6)* CNN-LSTM
ProposedModel Transferlearningusing
95.33
(BAAD6) LanguageModel
N-gramsandPOStags
Islametal.[29] 10 - 312 96
withRandomforests
Khatun
CharacterEmbeddingwithCNN 86.28
etal.[34](BAAD16)*
16 842k 1122
Khatun WordEmbeddingwith
93.82
etal.[34](BAAD16)* CNN-LSTM
ProposedModel Transferlearningusing
99.8
(BAAD16) LanguageModel
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.111:22 Khatun,etal.
Furthermore, our curated dataset, BAAD16, was also used to compare the performance of
someofthepreviouslypublishedmodelsbyreplicatingthosearchitectures(detailsinSection5).
Performanceimprovesfromappx.94%(ofmBERT,CNN-LSTM,etc)to99.8%,whichisasignificant
improvement.
6.6 Erroranalysis
Asmallnumberoffalseclassificationswerebrieflyanalyzedtorevealthatthemodelconfuses
similarwritingsofdifferentauthors.Itisnotuncommonforwriterstofollowanotherfamous
writeroranyoftheirpeers.Besides,somewritersofthesameperiodalsotendtofollowsimilar
writingpatterns.Despitethese,mostcaseswerein-factrandomerrorswhosecorrelationswere
notfoundinthisbriefanalysis,atleastnotmanually.Anin-depthanalysisoftheerrorscanhelp
revealmoreaboutwhatthemodelslearnintermsofunderstandingthestructureandsemanticsof
alanguagewhichremainsascopeforfuturestudy.
Theeffectsofvariousotherfactorsontheobtainedresults,suchasarchitecture,tokenization,
numberofauthors,sampledistribution,anddatasetusedforpre-training,areanalysedtotestthe
robustnessandscalabilityoftheproposedsystem.Thesearebrieflydiscussedbelow.
6.7 ModelEffect
ToassesstheeffectsofULMFiTmodelforthetaskofauthorshipattribution,wetrainsomeprevious
state-of-the-artmodels,namelyChar-CNNandword-levelCNN-LSTM.FromTable7,itisevident
thattheULMFiTmodeloutperformstheothermodelsbyasignificantamountonbothBAAD6and
BAAD16datasets.Thisclearlyshowsthatthetransferlearningapproachiseffectivelyapplicableto
thetaskofauthorshipattribution.ThemodellearnsBanglafromlanguagemodeltrainingandtunes
toauthorialwritingstylesonfine-tuning.Thus,withtheadditionalstepsofteachingthemodela
language,itcanbetterdetectthetext’soriginalauthor.Besidesthis,theAWD-LSTMarchitecture,
alongwiththetrainingtechniquesemployed,offersastrongbaseforthelanguagemodelaswell
astheclassifiermakingthetransferlearningthereofhighlyeffective[27].
6.8 TokenizationEffect
TokenizationisanintegralpartofanyNLPtask.InBangla,themostcommonwaysofarhasbeen
totokenizebywords.Inthispaper,threetypesoftokenizationsareattempted,andtheireffects
areanalyzedonthetaskofauthorclassification.AsdescribedinSection4.2,wehaveperformed
word,sub-word,andcharacterleveltokenization.Ingeneral,thecharacterlevelmodelsperform
worsethanbothwordandsub-wordmodels.Thisreflectsthatthecharacter-levelmodelfaces
difficultychoosingthecorrectauthorbecauseofthelongstreamoftokensithastogothrough
beforereachinganyconclusion.LSTMlayerspassonecharacteratatime,makinganysentencea
longsetoftokens.Therefore,theLSTMlayersmayhavetroublegatheringenoughdataabouteach
author’sdifferenceintextorstyle.
Thewordandsub-wordlevelmodelsperformnearlyequallywell,butthesub-wordmodels
begintooutruntheword-levelmodelsonfurtherepochs.Thisinformationcanbedrawnfrom
Figure10,whereweseetheprogressforeachtokenizationtype’smodelonbothBAAD16and
BAAD6inazoomed-inscaleforeasyunderstanding.Amorezoomed-ingraphinFigure11shows
therisingaccuracyofthesub-wordlevelmodeloneachepoch,indicatingaspecificcomparisonof
wordandsub-wordlevelmodels.Thesub-wordtokenizationbreaksdownthetextintomultiple
partsbutnotwholly.Asaresult,linguisticinformationaboutthewordsiskeptintact,anditalso
providesinformationabouttherelationshipbetweenstructuralcomponentsofwords.Forexample,
itseparatesinflectedprefixesorsuffixesfromwords,sothemodelretainsrootwordinformation
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.AABL 111:23
aswellasgetsadditionalinformationfromtheseparatedparts.Whereaswhentokenizedbywords,
therelationshipbetweenrelatedwordswithslightstructurevariationsisnoteasilyrecognized.
100.0 100.0
97.8 91.1
95.5 82.1
93.3 73.2
word-level word-level
subword-level subword-level
91.1 character-level 64.3 character-level
0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9
Epochs Epochs
(a)OnBAAD16 (b)OnBAAD6
Fig.10. Theaccuracyversusepochplotshowsthemodels’performancecomparisonwithdifferenttokeniza-
tions.Herethecharactertokenizedmodelcannotreachthelevelofsubwordandwordtokenizedmodelsin
boththetesteddatasets(BAAD6andBAAD16).Noticetheverticalaxisdoesnotstartfromzeroforclarity.
100.0
100.0
97.3
99.8
94.6
99.7
91.9
word-level
99.5
subword-level
89.2
word-level 0 1 2 3 4 5 6 7 8 9
99.3 subword-level Epochs
0 1 2 3 4 5 6 7 8 9
(b)OnBAAD6
Epochs
(a)OnBAAD16
Fig.11. Theaccuracyversusepochplotshowsthemodels’performancecomparisonwithwordandsubword
tokenizations.Itisanextendedversionofthepreviousfigure,showingthetwomodelsadvancingexceptionally
closelytogether.Forclarity,theverticalaxisdoesnotbeginatzero.
6.9 EffectofnumberofAuthors
Inthissection,theproposedmodels’resultsarecomparedwithvaryingnumbersofauthorsto
determinethemodels’effectivenessagainstincreasingclassestrainedwithfewersamples.BAAD16
contains16authorsinanimbalancedmanner.Itischunkedintofivepartscontaining6,8,10,12,
and14authors,respectively,randomlytakingsubsetsfromtheoriginaldataset.Thesamplesper
classinthederiveddatasetsaretruncatedtotheminimumnumberofsamplesamongtheclasses.
Wehaveonlycomparedtheaccuracybecauseallthemodelsarenowbeingtrainedandtested
onbalanceddatasets.Accuracyismeasuredon20%heldoutdatasetforeachcase,aftertraining
on80%ofthedata.Eachofthesixpre-trainedmodelsof3types(word,sub-word,andcharacter
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.
ycaruccA
ycaruccA
ycaruccA
ycaruccA111:24 Khatun,etal.
level)isfine-tunedonthesefivesub-datasets.Asweincreasethenumberofauthors,thesample
perauthoralsodecreases,makingitdifficultforanydeeplearningmodeltolearneachclassof
text.ThesummaryispresentedinTable10.Agraphdepictingtheaccuracytrendwithincreasing
numberofauthorsforallthemodelsisshowninFigure12.
Table10. AccuracyofauthorshipattributionforincreasingnumberofAuthors.Allsubsetsofthedatasetare
balanced,andthesamplesperauthoraredisplayedunderneaththeauthornumberineachcolumn.
#ofauthorsonthesubset
#ofsamplesperauthor
Pre-training 6authors 8authors 10authors 12authors 14authors
Tokenization
Dataset 1100 931 849 562 469
News 99.69 99.53 99.58 99.41 98.63
Word
Wiki 99.62 99.56 99.70 99.48 99.47
News 99.47 99.40 98.71 99.56 99.39
Sub-word
Wiki 99.62 99.67 99.41 99.63 99.54
News 98.79 98.60 98.00 98.38 97.26
Character
Wiki 98.79 98.66 98.18 97.78 97.11
99.5
99.0
98.5
word_news
98.0 word_wiki
subword_news
subword_wiki
97.5
char_news
char_wiki
97.0
6 8 10 12 14
# of authors
Fig.12. Performanceofdifferentmodelswithincreasingnumberofauthors.Themodelsvarybytokenization
andthedatasetitwaspre-trainedon,depictedinthelegendintheform’tokenization_pretraining-dataset’.
Theverticalaxisshowstheclassificationaccuracy,andthehorizontalaxisindicatesthenumberofauthors
takenfromtheBAAD16dataset.
Outofthefivesubsetscreated,thesub-wordmodelpre-trainedontheWikipediacorpusperforms
bestinthemajorityofcases(3inthisoccasion).Moreimportantly,asthesamplenumberdecreases,
allothermodelsmoreorlessstartperformingworsethanthesub-wordtokenizedmodeltrained
ontheWikipediacorpus.Figure12showsthedeclineinthecharactermodeltrainedwithlesser
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.
ycaruccAAABL 111:25
samples.Moreover,thewordandsub-wordlevelwikimodelsconsistentlyperformbetter,and
thesub-wordmodeltendstogivehigheraccuracy.Fromtheseresults,wecanconcludethatthe
Wikipediapre-trainedsub-wordmodelshowsmorestabilitythanothers.Thereasonbehindthis
couldbetwo-fold.FirstlytheWikipediadatasethasmorevariedtextthanthenewsdataset.This
helpsthemodelgeneralizebetter.Secondly,thesub-wordlevelmodelprovidesenoughinformation
aboutwordandsentencestructureyetdoesnotbreakitdowntoomuch(intocharacters)tolose
sightoftheoverallpictureofthesampledtext.
6.10 Effectofpre-trainingdatasets
AlthoughtrainedmodelswiththeWikipediadatasetshowbetterdownstreamgeneralization,the
scenariochangesinreal-worldapplications.Dataisoftenimbalanced,withsomeauthorshaving
veryfewtrainingsamplescomparedtoothers.Theclassifier’slossforeachepochisplottedto
comparetheeffectsofnewsandWikipediapre-trainedsubwordmodelsforeachtargetdatasetin
Figure13.
We see that the trained model with the news dataset works significantly better in terms of
classifier’slossforbothBAAD6andBAAD16datasets.Thelosstendstodecreaseconsistentlywith
eachepochinbothsituations,althoughthenewsmodel’slossissubstantiallysmallerthanthewiki
model’s.Itshowsthatthenewsdataset’scategoriesandsizeenabledtheclassifiertodistinguish
betteramongtheauthors.
Loss of classification for different pretrained models Loss of classification for different pretrained models
0.40 news-ulm 0.030 news-ulm
wiki-ulm wiki-ulm
0.35
0.025
0.30
0.020
0.25
0.015
0.20
0.15 0.010
0.10 0.005
0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9
Epoch Epoch
(a)OnBAAD16 (b)OnBAAD6
Fig.13. Lossversusepochplotshowingtheeffectofthedatasetusedinpre-trainingonclassification.The
newspre-trainedmodelachieveslowerlossforthetaskofauthorshipattributioninourcase.Boththemodels
aresub-wordtokenizedsincesub-wordtokenizationachievesbetterperformancethanothers.
7 RELEASEDPRE-TRAINEDLANGUAGEMODELS
A set of language models have been pre-trained on two different datasets, namely news and
Wikipedia, as part of our workflow. Language models’ performance is measured in terms of
perplexity.Thismeasurecapturesthedegreeofuncertaintyinpredictingthenextwordofthe
sentence.Itiscalculatedastheexponentiationoftheobtainedloss.Lowperplexityisasignof
a well-trained model. Table 11 lists the perplexities of the pre-trained models. We see that the
character-levelmodelsperformsignificantlybetterforthetaskoftextgeneration.Anillustrationof
theproducedtextcanbeseeninFigure9.Thenextbestperformanceisobtainedbythesub-word
levelmodelbeingslightlylowerthantheword-levelmodels.Thereasoncouldbethatthetokens’
representationinsmallerforms(e.g.characters)mayprovideadditionalinformationaboutthe
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.
ssoL ssoL111:26 Khatun,etal.
wordsandrelationshipsamongthewordsthanthelargerchunkscanprovide.Nevertheless,such
information is not reliable for downstream tasks, as evident from our experiments where the
character-levelmodelsunderperformedcomparedtotheircounterparts.
Allthepre-trainedmodelshavebeenmadeavailableforuseandcanbeappliedtoanyother
BanglaNLPtaskofchoice.Theheavytaskofpre-traininghasalreadybeendone.Theweightshave
tobeloaded,andthefinaldenselayerhastobechangedtoperformthedownstreamtasktousethese
models.Itistobenotedthatthedownstreamtaskhastobedoneinstepsandpreferablyfollowing
thediscriminativelearningrateproceduresothatthepre-trainedweightscanbeefficientlyutilized.
Table11. PerplexityofthetrainedLanguageModels.
Dataset Tokenization Perplexity
Word 47.52
News Sub-word 43.61
Character 3.20
Word 83.62
Wiki Sub-word 74.20
Character 3.08
8 CONCLUSIONANDFUTUREWORK
Transfer learning has been proven beneficial for domains with inadequate data labelling as it
isfirstpre-trainedonaresource-richsourcedomainbeforebeingfine-tunedonadownstream
task.Therefore,inthiswork,weanalyzetransferlearningapplicationsonauthorshipattribution
usinglanguagemodelinginthreephases:Pre-training,Fine-tuning,andAuthorprediction.Inthis
article,wepresentanAWD-LSTMandtransferlearning-basedscalableframeworkthataddresses
theproblemofscarcelanguageresources,scalability,andmanualfeatureengineeringforAABL.
Towardsthatgoal,experimentswereconductedusingtheword,sub-word,andcharacterlevel
tokenization,demonstratingtheefficacyofourapproachwithdifferenttokenization.Furthermore,
comparativeanalysisshowsthatourapproachoutperformsallexistingtraditionalmethods.Totest
ourmodelwithalargerdatasetandlongertext,webuildadatasetcontainingtheliteraryworks
of16authorswithmorethan13.4millionwordsintotal.Ourcontributiontothisworkincludes
theproposalofarobustandscalablesystemforAABLusingtransferlearning,acomprehensive
analysisoftheeffectivenessofdifferenttokenization,astandarddataset,andthereleaseofthe
pre-trainedmodelsforfurtheruse.Theproposedmethodprovidesstabilitywithfewerdataand
alargepoolofauthors.Weconcludethatthesub-wordtokenizedmodelsconsistentlyperform
better regardless of the number of samples from our experiments. We want to investigate the
applicationoftransferlearningondeepCNN-basedarchitecturesandcomparethemwithLSTM
architecturewithanattentionmodel[3]forfurtherworkduetoitspatterndetectionabilityand
fastertrainingtime.Wewouldalsoliketomakeuseofvarioustransformer-basedmodelsforthis
task.Additionally,cross-lingualauthorshipattributionsandimprovementofexistingmodelson
suchtasksalsointerestus.
REFERENCES
[1] 2022. RegularizationofNeuralNetworksUsingDropConnect. https://cds.nyu.edu/projects/regularization-neural-
networks-using-dropconnect/
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.AABL 111:27
[2] SumnoonIbnAhmad,LamiaAlam,andMohammedMoshiulHoque.2020. AnEmpiricalFrameworktoIdentify
AuthorshipfromBengaliLiteraryWorks.InInternationalConferenceonCyberSecurityandComputerScience.Springer,
465–476.
[3] IbrahimAlAzhar,SohelAhmed,MdSaifulIslam,andAishaKhatun.2021.IdentifyingAuthorinBengaliLiteratureby
Bi-LSTMwithAttentionMechanism.In202124thInternationalConferenceonComputerandInformationTechnology
(ICCIT).IEEE,1–6.
[4] DMAnisuzzamanandAbdusSalam.2018.AuthorshipattributionforBengalilanguageusingthefusionofN-gram
andNaïvebayesalgorithms. InternationalJournalofInformationTechnologyandComputerScience(IJITCS)10,10
(2018),11–21.
[5] ImranulAshrafi,MuntasirMohammad,AraniShawkatMauree,GalibMdAzrafNijhum,RedwanulKarim,Nabeel
Mohammed,andSifatMomen.2020. Banner:ACost-SensitiveContextualizedModelforBanglaNamedEntity
Recognition.IEEEAccess8(2020),58206–58226.
[6] DBagnall.2016.Authorshipclusteringusingmulti-headedrecurrentneuralnetworks—notebookforPANatCLEF
2016.InCLEF2016EvaluationLabsandWorkshop–WorkingNotesPapers.5–8.
[7] KurtBarryandKatherineLuna.2012.Stylometryforonlineforums.
[8] SamitBhattacharya,MonojitChoudhury,SudeshnaSarkar,andAnupamBasu.2005.Inflectionalmorphologysynthesis
forbengalinoun,pronounandverbsystems.InProc.oftheNationalConferenceonComputerProcessingofBangla
(NCCPB05).Citeseer,34–43.
[9] TanmoyChakraborty.2012. Authorshipidentificationinbengaliliterature:acomparativeanalysis. arXivpreprint
arXiv:1208.6268(2012).
[10] HemayetAhmedChowdhury,MdAzizulHaqueImon,SyedMdHasnayeen,andMdSaifulIslam.2019.Authorship
AttributioninBengaliLiteratureusingConvolutionalNeuralNetworkswithfastText’swordembeddingmodel.In
20191stInternationalConferenceonAdvancesinScience,EngineeringandRoboticsTechnology(ICASERT).IEEE,1–5.
[11] HemayetAhmedChowdhury,MdAzizulHaqueImon,andMdSaifulIslam.2018.AuthorshipAttributioninBengali
LiteratureUsingfastText’sHierarchicalClassifier.In20184thInternationalConferenceonElectricalEngineeringand
Information&CommunicationTechnology(iCEEiCT).IEEE,102–106.
[12] HemayetAhmedChowdhury,MdAzizulHaqueImon,andMdSaifulIslam.2018.AComparativeAnalysisofWord
EmbeddingRepresentationsinAuthorshipAttributionofBengaliLiterature.(2018).
[13] HemayetAhmedChowdhury,MdAzizulHaqueImon,AnisurRahman,AishaKhatun,andMdSaifulIslam.2019.A
continuousspaceneurallanguagemodelforbengalilanguage.In201922ndInternationalConferenceonComputerand
InformationTechnology(ICCIT).IEEE,1–6.
[14] AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzmán,
EdouardGrave,MyleOtt,LukeZettlemoyer,andVeselinStoyanov.2019.Unsupervisedcross-lingualrepresentation
learningatscale.arXivpreprintarXiv:1911.02116(2019).
[15] PiotrCzapla,SylvainGugger,JeremyHoward,andMarcinKardas.2019.UniversalLanguageModelFine-Tuningfor
PolishHateSpeechDetection.ProceedingsofthePolEval2019Workshop(2019),149.
[16] AndrewMDaiandQuocVLe.2015.Semi-supervisedsequencelearning.InAdvancesinneuralinformationprocessing
systems.3079–3087.
[17] PraptiDas,RishmitaTasmim,andSabirIsmail.2015.Anexperimentalstudyofstylometryinbanglaliterature.In2015
2ndInternationalConferenceonElectricalInformationandCommunicationTechnologies(EICT).IEEE,575–580.
[18] SuprabhatDasandPabitraMitra.2011.Authoridentificationinbengaliliteraryworks.InInternationalConferenceon
PatternRecognitionandMachineIntelligence.Springer,220–226.
[19] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert:Pre-trainingofdeepbidirectional
transformersforlanguageunderstanding.arXivpreprintarXiv:1810.04805(2018).
[20] AbeerHElBakly,NagyRamadanDarwish,andHeshamAHefny.2020.UsingOntologyforRevealingAuthorship
AttributionofArabicText.InternationalJournalofEngineeringandAdvancedTechnology(IJEAT)9,4(2020),143–151.
[21] OlgaFourkioti,SymeonSymeonidis,andAviArampatzis.2019.Languagemodelsandfusionforauthorshipattribution.
InformationProcessing&Management56,6(2019),102061.
[22] YarinGalandZoubinGhahramani.2016.Atheoreticallygroundedapplicationofdropoutinrecurrentneuralnetworks.
Advancesinneuralinformationprocessingsystems(2016).
[23] ErikGoldmanandAbelAllison.[n.d.].UsingGrammaticalMarkovModelsforStylometricAnalysis.([n.d.]).
[24] JulianHitschler,EsthervandenBerg,andInesRehbein.2017. Authorshipattributionwithconvolutionalneural
networksandPOS-Eliding.InProceedingsoftheWorkshoponStylisticVariation.53–58.
[25] MTahmidHossain,MdMoshiurRahman,SabirIsmail,andMdSaifulIslam.2017.AstylometricanalysisonBengali
literatureforauthorshipattribution.(2017).
[26] MdZobaerHossain,MdAshrafulRahman,MdSaifulIslam,andSudiptaKar.2020. Banfakenews:Adatasetfor
detectingfakenewsinbangla.arXivpreprintarXiv:2004.08789(2020).
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.111:28 Khatun,etal.
[27] JeremyHowardandSebastianRuder.2018.UniversalLanguageModelFine-tuningforTextClassification.InProceedings
ofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers).328–339.
[28] MdAshikulIslam,MdMinhazulKabir,MdSaifulIslam,andAyeshaTasnim.2018.AuthorshipAttributiononBengali
LiteratureusingStylometricFeaturesandNeuralNetwork.In20184thInternationalConferenceonElectricalEngineering
andInformation&CommunicationTechnology(iCEEiCT).IEEE,360–363.
[29] NazmulIslam,MohammedMoshiulHoque,andMohammadRajibHossain.2017.Automaticauthorshipdetectionfrom
Bengalitextusingstylometricapproach.In201720thInternationalConferenceofComputerandInformationTechnology
(ICCIT).IEEE,1–6.
[30] FereshtehJafariakinabad,SansiriTarnpradab,andKienAHua.2019. SyntacticRecurrentNeuralNetworkfor
AuthorshipAttribution.arXivpreprintarXiv:1902.09723(2019).
[31] AnupamJamatia,SteveDurairajSwamy,BjörnGambäck,AmitavaDas,andSwapanDebbarma.2020.DeepLearning
BasedSentimentAnalysisinaCode-MixedEnglish-HindiandEnglish-BengaliSocialMediaCorpus.International
JournalonArtificialIntelligenceTools(2020).
[32] MdJewel,MdIsmailHossain,andTamannaHaiderTonni.2019.BengaliEthnicityRecognitionandGenderClassification
usingCNN&TransferLearning.In20198thInternationalConferenceSystemModelingandAdvancementinResearch
Trends(SMART).IEEE,390–396.
[33] MdKarim,BharathiRajaChakravarthi,MihaelArcan,JohnPMcCrae,MichaelCochez,etal.2020. Classification
BenchmarksforUnder-resourcedBengaliLanguagebasedonMultichannelConvolutional-LSTMNetwork. arXiv
preprintarXiv:2004.07807(2020).
[34] AishaKhatun,AnisurRahman,MdSaifulIslam,etal.2019. AuthorshipAttributioninBanglaliteratureusing
Character-levelCNN.In201922ndInternationalConferenceonComputerandInformationTechnology(ICCIT).IEEE,
1–5.
[35] DiederikPKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980
(2014).
[36] TimKreutzandWalterDaelemans.2018. Exploringclassifiercombinationsforlanguagevarietyidentification.In
ProceedingsoftheFifthWorkshoponNLPforSimilarLanguages,VarietiesandDialects,SantaFe,NewMexico,USA,
August20,2018.191–198.
[37] TakuKudo.2018. Subwordregularization:Improvingneuralnetworktranslationmodelswithmultiplesubword
candidates.arXivpreprintarXiv:1804.10959(2018).
[38] TakuKudoandJohnRichardson.2018.Sentencepiece:Asimpleandlanguageindependentsubwordtokenizerand
detokenizerforneuraltextprocessing.arXivpreprintarXiv:1808.06226(2018).
[39] GuillaumeLampleandAlexisConneau.2019.Cross-linguallanguagemodelpretraining.arXivpreprintarXiv:1901.07291
(2019).
[40] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,
andVeselinStoyanov.2019.Roberta:Arobustlyoptimizedbertpretrainingapproach.arXivpreprintarXiv:1907.11692
(2019).
[41] IlyaLoshchilovandFrankHutter.2016.SGDR:StochasticGradientDescentwithRestarts.CoRR(2016).
[42] S.Lynn-Evans.2019.TenTechniquesLearnedFromfast.ai. https://blog.floydhub.com/ten-techniques-from-fast-ai/
[43] StephenMerity,NitishShirishKeskar,andRichardSocher.2017.RegularizingandOptimizingLSTMLanguageModels.
CoRR(2017).
[44] StephenMerity,NitishShirishKeskar,andRichardSocher.2018.RegularizingandOptimizingLSTMLanguageModels.
CoRR(2018).
[45] ZinniaKhanNishatandMdShopon.2019. UnsupervisedPretrainingandTransferLearning-BasedBanglaSign
LanguageRecognition.InInternationalJointConferenceonComputationalIntelligence.Springer,529–540.
[46] UrmeePal,AyeshaSiddikaNipu,andSabirIsmail.2017. Amachinelearningapproachforstylometricanalysisof
Banglaliterature.(2017).
[47] MatthewPeters,MarkNeumann,MohitIyyer,MattGardner,ChristopherClark,KentonLee,andLukeZettlemoyer.
2018.DeepContextualizedWordRepresentations.InProceedingsofthe2018ConferenceoftheNorthAmericanChapter
oftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers).2227–2237.
[48] ShantaPhani,ShibamouliLahiri,andArindamBiswas.2016.Amachinelearningapproachforauthorshipattribution
forBengaliblogs.(2016).
[49] ShantaPhani,ShibamouliLahiri,andArindamBiswas.2017.Asupervisedlearningapproachforauthorshipattribution
ofBengaliliterarytexts.ACMTransactionsonAsianandLow-ResourceLanguageInformationProcessing(TALLIP)16,4
(2017),1–15.
[50] RahulPramanikandSoumenBag.2020.Segmentation-basedrecognitionsystemforhandwrittenBanglaandDevanagari
wordsusingconventionalclassificationandtransferlearning.IETImageProcessing14,5(2020),959–972.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.AABL 111:29
[51] AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever.2018.Improvinglanguageunderstandingby
generativepre-training.
[52] SebastianRuder,ParsaGhaffari,andJohnGBreslin.2016.Character-levelandmulti-channelconvolutionalneural
networksforlarge-scaleauthorshipattribution.arXivpreprintarXiv:1609.06686(2016).
[53] YunitaSari,MarkStevenson,andAndreasVlachos.2018. Topicorstyle?exploringthemostusefulfeaturesfor
authorshipattribution.InProceedingsofthe27thInternationalConferenceonComputationalLinguistics.343–353.
[54] SagorSarker.2020. BanglaBERT:BengaliMaskLanguageModelforBengaliLanguageUnderstading. https:
//github.com/sagorbrur/bangla-bert
[55] YanirSeroussi,IngridZukerman,andFabianBohnert.2014.Authorshipattributionwithtopicmodels.Computational
Linguistics40,2(2014),269–310.
[56] PrashaShrestha,SebastianSierra,FabioGonzalez,ManuelMontes,PaoloRosso,andThamarSolorio.2017.Convolu-
tionalneuralnetworksforauthorshipattributionofshorttexts.InProceedingsofthe15thConferenceoftheEuropean
ChapteroftheAssociationforComputationalLinguistics:Volume2,ShortPapers.669–674.
[57] LeslieN.Smith.2015.NoMorePeskyLearningRateGuessingGames.CoRR(2015).
[58] EfstathiosStamatatos.2009.Asurveyofmodernauthorshipattributionmethods.JournaloftheAmericanSocietyfor
informationScienceandTechnology60,3(2009),538–556.
[59] KalaivaniSundararajanandDamonWoodard.2018.Whatrepresents“style”inauthorshipattribution?.InProceedings
ofthe27thInternationalConferenceonComputationalLinguistics.2814–2822.
[60] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
Polosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationprocessingsystems.5998–6008.
[61] LiWan,MatthewZeiler,SixinZhang,YannLeCun,andRobFergus.2013.Regularizationofneuralnetworksusing
dropconnect.InInternationalconferenceonmachinelearning.1058–1066.
[62] Wikipedia.2021.BengaliLanguage. https://en.wikipedia.org/wiki/Bengali_language
[63] YonghuiWu,MikeSchuster,ZhifengChen,QuocVLe,MohammadNorouzi,WolfgangMacherey,MaximKrikun,Yuan
Cao,QinGao,KlausMacherey,etal.2016.Google’sneuralmachinetranslationsystem:Bridgingthegapbetween
humanandmachinetranslation.arXivpreprintarXiv:1609.08144(2016).
[64] ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,RussRSalakhutdinov,andQuocVLe.2019.Xlnet:Generalized
autoregressivepretrainingforlanguageunderstanding.InAdvancesinneuralinformationprocessingsystems.5753–
5763.
[65] ChunxiaZhang,XindongWu,ZhendongNiu,andWeiDing.2014.Authorshipidentificationfromunstructuredtexts.
Knowledge-BasedSystems66(2014),99–111.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2022.