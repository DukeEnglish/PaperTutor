Beyond Finite Data: Towards Data-free
Out-of-distribution Generalization via Extrapolation
YijiangLi1,SuchengRen1,WeipengDeng2,YuzhiXu3,YingGao4,EdithNgai2,and
HaohanWang5
1 JohnsHopkinsUniversity
2 TheUniversityofHongKong
3 SouthChinaUniversityofTechnology
4 UniversityofIllinoisUrbana-Champaign
yli556@jh.edu
Abstract. Out-of-distribution(OOD)generalizationisafavorableyetchalleng-
ing property for deep neural networks. The core challenges lie in the limited
availabilityofsourcedomainsthathelpmodelslearnaninvariantrepresentation
from the spurious features. Various domain augmentation have been proposed
butlargelyrelyoninterpolatingexistingdomainsandfrequentlyfacedifficulties
increatingtruly“novel”domains.Humans,ontheotherhand,iscapableofex-
trapolatingnoveldomains,thus,anintriguingquestionarises:Howcanneural
networks extrapolate truly “novel” domains and achieve OOD generaliza-
tion?
Weintroduceanovelapproachtodomainextrapolationthatleveragesreasoning
abilityandtheextensiveknowledgeencapsulatedwithinlargelanguagemodels
(LLMs)tosynthesizeentirelynewdomains.Startingwiththeclassofinterest,
wequerytheLLMstoextractrelevantknowledgeforthesenoveldomains.We
thenbridgethegapbetweenthetext-centricknowledgederivedfromLLMsand
thepixelinputspaceofthemodelusingtext-to-imagegenerationtechniques.By
augmentingthetrainingsetofdomaingeneralizationdatasetswithhigh-fidelity,
photo-realistic images of these new domains, we achieve significant improve-
mentsoverallexistingmethods,asdemonstratedinbothsingleandmulti-domain
generalizationacrossvariousbenchmarks.
Withtheabilitytoextrapolateanydomainsforanyclass,ourmethodhasthepo-
tentialtolearnageneralizedmodelforanytaskwithoutanydata.Toillustrate,
we put forth a much more difficult setting termed, data-free domain general-
ization, thataims to learn ageneralized model in theabsence of anycollected
data. Our empirical findings support the above argument and our methods ex-
hibit commendable performance in this setting, approximating the supervised
withsyntheticdataonlyandevensurpassingthesupervisedsettingbyapprox-
imately1-2%ondatasetssuchasVLCS.
Keywords: Out-of-distribution Generalization · Domain Extrapolation · Large
LanguageModel
Preprint.Underreview.
Wewillreleaseourcodeuponacceptance.
4202
raM
8
]VC.sc[
1v32550.3042:viXra2 YLi,SRenWDengetal.
1 Introduction
Deepneuralnetworkshavedemonstratedremarkableachievementsinvariousfieldsand
applications[13,17,18,21,31],yettheireffectivenessheavilydependsontheassump-
tionthatthetrainingandtestingenvironmentsaresubjecttoindependentandidentically
distributions[7,8].Out-of-distribution(OOD)generalizationaimstolearnmodelfrom
some training distribution that can generalize well to unseen testing domains, usually
withdistributionorlabelshifts[32].Atypicalscenarioisdomaingeneralization(DG)
where multiple source domains are available and these available source domains aid
the training of generalizable models that learn invariant features and discard spurious
ones.However,asignificantchallengearises.Theavailabilityofthesesourcedomains
often becomes a limiting factor, hindering the success of current OOD approaches in
morechallengingscenarios[37,50,52,56],whichcanbeattributedtothedifficultyand
high expenses to collect, not just, data but data in diverse domains with annotations,
which is sometimes impossible in critical areas such as healthcare or extreme condi-
tions(e.g.deepseaorspace).Motivatedbythesechallenges,domainaugmentationis
straightforward and multiple methods have been proposed to generate novel domains
andimagesthroughmixup[57],mixingofstatistics[61],uncertaintymodeling[29,62]
and convex combination [1]. However, these methods generally interpolate the exist-
ingtrainingdomainstogeneratenoveldomainsthatstillfallwithintheconvexhallof
availabledomains[1].Consequently,theconstrainednumberofsourcedomainsham-
perstheexpressivenessofthesemethods,continuingtoactasaperformancebottleneck.
Ontheotherhand,Humansharnesstheinnateabilityofthehumanbraintocreatenovel
domains as illustrated in [39,45] where a pre-defined set of novel domains and styles
areutilized.However,thisalsorequireshumanlaborwhichfailstoscaletolargersizes.
Naturally,anintriguingquestionarises:Howcanneuralnetworksextrapolatetruly
“novel”domainsandachieveOODgeneralization?
Largelanguagemodels(LLMs)[9]havebeenshowntoencapsulateavastwealthof
knowledgeandsimulatehumancognitiveprocesses.Thus,apertinentquestionemerges:
CanoneharnessthepowerofLLMstoproducenoveldomainsandrelevantknowledge,
therebyreplacingthehumanintheabovetrainingprocess?Stemmingfromthisprimary
query,weinvestigatehowwecanextractknowledgeofaspecifictaskandproducenovel
domainsfromLLMs.Asubsequentresearchquestionis:Howcanweleveragethistext-
centricknowledgefromLLMstoinstructanimagesystemthatprocessespixelinput?
State-of-the-art text-to-image generation models such as Imagen [44], Stable Diffu-
sion [40] and GLIDE [34] exhibit promosing capability to synthesize photo-realistic
imagespositioningthemastheoptimalconduitbetweentextualandvisualrealms.Fi-
nally,weseektoanswertowhatextentthesynthesizedimagesbasedonknowledgecan
serveasOut-of-distributionlearnersthatcangeneralizetounseentestingdomains.Fol-
lowingtheseproblems,wearethefirststudytodesignanewparadigmthatleverages
theknowledgeofLLMstoextrapolatenoveldomainsfortrainingbettergeneralizable
andsample-efficientmodels.Withtheabilitytoextrapolateanydomainsforanyclass,
our method has the potential to learn a generalized model for any task without any
existingdata.
Inaddition,wepresentdata-freedomaingeneralization.Data-freegeneralization
endeavorstoenableamodelacrossunseentestingdomainsbasedsolelyontaskspec-Data-freeGeneralization 3
ifications (for example, distinguishing between dog and cat classes) without the need
forgatheringorutilizinganypre-existingdatasets.Intheeraoflargefoundationmod-
els, data-free domain generalization is formulated as OOD problem with inaccessible
meta distribution and domain distribution (detailed in Section 2.1) – essentially, de-
voidofanyreal-worlddata.Thisscenariopresentsasignificantlymorecomplexchal-
lenge than that encountered in multi-domain or single-domain generalization efforts.
Moreover, it holds pragmatic significance in democratizing machine learning, by urg-
ing the community to develop methodologies that are viable under stringent resource
constraints. Such an initiative paves the way for wider access to, and application of,
machine learning technologies. For instance, our proposed method provides a viable
solutionbyleveragingLLMasaknowledgebasetoextrapolatedomainsandscenarios
domainsandscenarioswherespecificclassesmayberepresented.Thensyntheticdata
isgeneratedviaatext-to-imagegenerationmodel.Thesesyntheticdataonlyareusedto
trainamodelthatcangeneralizewelltothegiventaskandfulfillitsrequirements.Our
methodnotonlyaddressesthechallengeofdatascarcityinDGproblemsbutalsoun-
derscoresthepotentialofsyntheticdatainovercomingtraditionalbarrierstomachine
learningimplementation.
Extensive experiments on single, multi-domain and data-free evaluations demon-
stratetheeffectivenessofourproposedmethod.Inbothsingleandmulti-domainconfig-
urations,wedemonstratethatsyntheticdataintheextrapolatednoveldomainsmarkedly
outperformsbaselineresultsacrossvariousdatasets.Onthemorechallengingdata-free
setting,ourproposedmethodexhibitsnear-supervisedperformanceinthissetting,even
surpassingthesupervisedbaselinebyapproximately1-2%onVLCS.Datasynthesized
via the knowledge from LLMs excels compared to the synthetic data directly gener-
ated from text-to-image generation models. This demonstrates the ability of LLMs to
effectivelyextrapolatelikehumansandintegratethispriorknowledgeintothemodel.
Wealsounderscorethescalabilityofourapproachbyhighlightingthatasthenum-
berofdomainsescalates,theperformancecorrespondinglyimproves.Intriguingly,this
trend diverges from the outcomes observed when augmenting with synthetic data di-
rectlyproducedbytext-to-imagemodelsreportedin[5,22].Thisfurtherdemonstrates
thepivotalroleoftheknowledgederivedfromLLMsinmitigatingoverfittingtosyn-
theticdata.
The remainder of this paper is organized as follows: In Section 2, we will first
motivate our method from the perspective of the theoretical error bound for out-of-
distribution(OOD)generalization.Thenwewilldetailourmethoddesignandspecifi-
cations.Section3introducesthedata-freegeneralizationanditspotentialusageinthe
eraoflargefoundationmodels.Section4describesourexperimentdesign,resultsand
theimplicationsofourfindings.Section5introducesrelatedwork.Section6concludes
ourpaperandpotentiallimitationofourwork.
2 Method
Wemotivateourmethodfromtheperspectiveofthetheoreticalerrorboundforout-of-
distribution(OOD)generalization.Wewillfirstprovidethenotationforthetheoretical
framework.ThenwemotivateourresearchproblemfromtheOODgeneralizationerror4 YLi,SRenWDengetal.
bound,i.e.limitednumberofsourcedomainsleadingtoalargererrorbound.Thenwe
propose a proxy method that approximates the meta-distribution with a proxy distri-
bution.Wegiveanewerrorboundonthismethod.Lastly,weproposeonerealization
ofourmethodbyusingLLMstoapproximatethemeta-distributionandtext-to-image
generationmodelstobridgethetext-centricknowledgewiththeinputpixelspace.
2.1 TheoreticalBound
Notation. Let X denote the observation space and Y = {1,−1} the output space.
Denote P as the joint probability of the joint space of X ×Y and assume a meta
XY
distribution µ and n domains P(1),··· ,P(i) ,P(n) are i.i.d realizations from µ. A
XY XY XY
decision function is a function f ∈ F : X → Y predicts yˆ = f(x ). We denote
i i
l : Y × Y → R a loss function and define the generalization error of a decision
+
functionas
Lµ(f)=E E [l(f(x),y)] (1)
PXY∼µ (x,y)∼PXY
SincewehavenoaccesstoµandalltherealizationsP(1),··· ,P(i) ,P(n) butsampled
XY XY XY
imagesfromtheserealizations,wecanderiveanempiricalerror:
n m
Lˆµ(f)=(cid:88)(cid:88) l(f(x(j)),y(j)) (2)
i i
i=1j=1
It’seasytoseethatwhenn → ∞,m → ∞,Lˆµ(f)convergestoLµ(f),whichgives
theintuitivesensethatincreasingmandngivesusbetter-approximatedsolutions.This
motivatesustoincreasenandm,i.e.increasingthenumberofdomainsandtrainingim-
agesperdomain,whichisdifficultduetotheinaccessibleµandP(1),··· ,P(i) ,P(n).
XY XY XY
Prior arts have proposed various methods to generate novel domains but the majority
fallsintheinterpolationofexistingdomains,failingtoeffectivelyincreasen.Howcan
toapproachthisproblem?Wecanapproximateµbynewdistributionµ′sufficiently
closetoµthatcanbesampled.
Definition1. Wedefinethedistancebetweenthetwodistributionsas
D(µ,µ′)= sup|Lµ′ (f)−Lµ(f)|
f∈F
Withthefollowingassumption,
Assumption1 WeassumethedistanceD(µ,µ′)≤ϵ.
wecanderiveaboundthroughtheapproximatedµ′.
Theorem1. Withconfidenceatleast1−2δandforallf ∈F,wehave
(cid:114) (cid:114)
ln(2/δ) ln(2/δ)
Lµ(f)≤Lˆµ′
(f)+2R (F)+2R (F)+3 +3 +ϵ
mn n 2mn nData-freeGeneralization 5
ProofinAppendix??.Byreplacingµwithµ′,wenowhavecontroloverLˆµ′(f),mand
naswecansampleasmanydomainsandimagesfromµ′ aspossible.Thisisobtained
atthecostofϵ,whichweassumetobesmall.
Remark1. Wealsonotethatasnandmincrease,theupperboundofthegeneralization
errordecreases,whichgivesusbettergeneralizationerrors.
Withsufficientlylargenandm,thedecreasepartofthegeneralizationerrorwillcancel
outthecostofϵ,leadingtoalowergeneralizationerror.
Prompt Step 1: Generate Novel Domains
Airport, Steampunk, minimalism
[Role] … …
Task
[Task description]
Description: classify image to [Steps] Step 2: Get Diffusion Prompts
different classes [Output Format]
Classes: dog, cat, car, chair Dog -street: A dog sitting on the
and person streets with houses behind it
.…
Test
&
Deploy
Caption 1 Caption 2 … Caption n
Frozen
train … Text-to-Image
Generalized Model
Diffusion Model
Fig.1:Overallpipelineofourparadigm:Extrapolationofnoveldomainsviatheknowledgeof
LLMs,anovellearningparadigmwhereknowledgefromLLMsassiststhetrainingofgeneraliz-
ablemodelsviatext-to-imagemodelsinacompletelydata-freefashion.
2.2 DomainExtrapolationwithLLMs
Giventheaforementionedtheoreticalbound,ourobjectiveistoapproximateµwithµ′.
Humans,asevidencedin[39,45],usuallycanefficientlyextrapolatenoveldomains(by
imagination),whichisagoodapproximationofµ.Nonetheless,humaninterventionis
expensiveandnotscalabletoarbitrarydatasets.Conversely,LLMsnotonlyembodya
vastexpanseofknowledge[36]andexhibitcomparablereasoningcapabilities[38],but
theyalsopresentthebenefitofbeingamenabletoextensivesampling.Tothisend,we
proposetoqueryLLMs,inplaceofhuman,toextrapolatenoveldomains.
After sampling from meta distribution µ′, we need to further sample from the do-
main distribution to generate images in this particular novel domain. As discussed in
Section1,thisleadstoagapbetweenthetext-basedknowledgeoutputbytheLLMsand
the input pixel space of vision systems. Text-to-image generation models (e.g. stable
diffusion[41])exhibitthegreatcapabilitytooutputphoto-realisticimagesthroughin-
puttingtextspositioningthemastheoptimalbridgebetweentextualandvisualrealms.
The synthetic images of extrapolated novel domains are used to augment the original
dataset or train the models solely in a data-free fashion. An overall illustration of our
paradigmcanbeseeninFigure1.6 YLi,SRenWDengetal.
ExtractingKnowledgefromLLMs.TheobjectiveistoapproximateµviaLLMs
ascloseaspossible.Thisintroducesaconstraintwherebythegeneratednoveldomains
must reside within the high-density regions of distribution µ. To ensure adherence to
this criterion, we purposefully instruct the LLMs to conceive the most plausible and
reasonable domains where a particular class would realistically exist. To better guide
LLMstounderstandtheinstructionandgeneratetherequiredresponseaccordingly,we
craftsystempromptsthatincluderoledescription([Role])andtaskdescription([Task
Description]), as illustrated by the example in Figure 2. Numerous strategies exist to
solicitknowledgeandnoveldomainsfromLLMs.
– Dataset-wise query. The most direct approach entails querying the LLMs with
comprehensivedatasetinformation(i.e.alloftheclassnames)andinstructingthe
modeltoproducennoveldomains.However,asthemarginaldistributionforeach
classmightexhibitminimaloverlap(worsewhenthenumberofclassesgrows),it
becomesconsiderablyintricatetosamplenoveldomainsthatarebothplausibleand
likelyforallclasses.
– Class-wisequery.Thus,weproposetoquerytheLLMsfornoveldomainsofspe-
cificclasses.Foreachclassinthetask,wequerytheLLMsforknowledgeand n
novel domain information specific to that class. We repeat the process one class
afteranotheruntilalloftheclassesareiterated.Weprovideaexamplepromptin
Figure2.
Step1:ConstructNovel Domains from LLM
SystemPrompt
Home: wooden dining table,
PromptStructure [Role] high-back chairs
{domainknowledgeexpert}
Office: spacious cabin, leather
[Task description] executive chair
{askthemodeltogive
[ YR oo ul e a] re an expert on the given class and domains} Park: greenery, wooden bench
object. You have knowledge of its attribute, [Steps] KnowledgeProvider chair
s dh ifa fup se is o, n ap mp oe da era l}nce.{orexpertinusing {guideLLMstepbystep} C ma ef te a: ls cid he aw irsalk, cafe tables,
[Output Format] ……
[Task description] {Outputformatinstructions}
{askthemodeltogivedomains,environment,
stylesorgeneratediffusionprompts} Step2:ConstructPromptsfor Diffusion
[Steps]
{“CoT”promptingtoguideLLMstepbystep SystemPrompt Dog-Airport:In the airport's pet
forbetterqualityanddiversity.} [Role] relief area, a fluffy white poodle
[Output Format] {diffusionexpert} e on wj no eys r, a s ug ra rm oue n o df e f de t bc yh tw rai vth e li et rs s
{Outputformatinstructions} [Task description] and their luggage.
{askthemodeltowrite
promptsfordiffusionmodel} Cat–Classroom:A curious tabby
cat perches on a classroom desk,
[Steps] DiffusionPrompter its inquisitive eyes fixed on a
{guideLLMstepbystep} textbook as if trying to decipher
the mysteries of math.
[Output Format] ……
{Outputformatinstructions}
Fig.2:Knowledgeextractionpipeline.WefirstemployvariousSOTApromptingmethods:e.g.
"ChainofThought[53]"(CoT)prompting,rolepromptingtoextractdomainsfromLLM(Step
1)andautomaticallygeneratepromptforaText-to-Imagemodel.(Step2)
Bridging text and pixel with text-to-image generation models. After obtaining
a number of the most plausible and reasonable domains of a specific class, we trans-
formthetext-centricknowledgefromLLMstopixelspacebytext-to-imagegenerationData-freeGeneralization 7
models. This process is exactly the realization of sampling X from P(i) where P(i)
X X
istheithdomaingeneratedbyµ′ (i.e.theLLM).Numerousstrategiesexisttoprompt
text-to-imagegenerationmodelsconditionedonclassanddomaininformation.
– Template prompt. The most immediate strategy involves employing templates as
prompts(e.g.,"animageof[CLASS_NAME]inthedomainof[DOMAIN_NAME]").
However,thelimitationliesinitslackofdiversity:utilizingtheidenticalpromptto
producemultipleimagesresultsinimagesbearingresemblancetooneanother.
– LLMgeneratedprompt.Thus,weproposetoquerytheLLMsforpromptscondi-
tionedontheclassnameanddomaininformationacquiredinthepreviousstep.As
illustrated in Figure 2, we craft system prompts that specifically tailor the LLM
to generate prompts for text-to-image generation models and generate multiple
promptsforeachofthenoveldomainsofeachclass.
3 Data-freeDomainGeneralization
We present Data-free Generalization, a new formation of generalization in the era of
largefoundationmodels.Givenataskwithdetaileddescriptionandrequirements(e.g.
the classes to be classified and the definition of each class), Data-free Generalization
endeavors to learn a model that can generalize to this specific task and fulfill the re-
quirementwithoutcollectinganydataorutilizinganyexistingdatasets.Formally,this
problemisformulatedasfollows.Taskdescriptionandrequirementsspecifythedeci-
sionfunctionf ∈ F : X → Y andthemetadistributionµ.Theproblemthenturnsto
minimizingEquation1,asdetailedinSection2.1.Thedifferenceisthatnowthemeta
distributionµcannotbesampledandthuswehavenoaccesstoanytrainingdomains
P(1),··· ,P(i) ,P(n) orimagesthataresampledfromthesedomains.However,inthe
XY XY XY
eraoflargefoundationmodels,themetadistributionµcanbeapproximatedbyLLMs
whilethedomaindistributioncanbeapproximatedbyimagegenerationmodels.Con-
sequently,wecanprovideaguaranteeonthelearningwithTheorem1.Weprovideone
suchmethodinSection2.
Data-freegeneralizationcannotonlyserveamoredifficultsettingtopushthelim-
its of current OOD methods but also holds pragmatic significance in democratizing
machine learning. It does so by mitigating or potentially eliminating the necessity for
data collection and annotation within the machine learning pipeline, which facilitates
a broader access to and application of machine learning technologies, particularly for
entitiesfacingresourceconstraints.
Envision a modest-sized enterprise incapable of investing in the training of large
foundationalmodels,norpossessingthenecessarytimeandfundingtocollectandla-
bel an extensive dataset for particular tasks. This situation aligns with the concept of
Data-free Generalization, characterized by the availability of only task specifications
intheabsenceofaccessibledata.Ourmethodologyoffersanidealresolutionforsuch
organizations.Initially,theycanleverageLLMs’APIsforalimitednumberofqueries
to derive extrapolated domains and scenarios. Following this, they may engage text-
to-image models for data synthesis. This synthetic data can then be utilized to either
develop new models or enhance existing ones, thereby circumventing the limitations
posedbyresourceconstraints.8 YLi,SRenWDengetal.
4 Experiments
Theobjectiveofourexperimentsisto(i)demonstratethatknowledgefromLLMssuc-
cessfully extrapolates novel domains and leads to performance benefits grounded by
theoreticalbounds.(ii)Investigatethemostefficientandeffectiveapproachforextract-
ing knowledge and sampling from text-to-image models. (iii) Analyze to what extent
thesyntheticimagesgeneratedconditiononLLMs’knowledgecanserveasgoodout-
of-distributionlearnersthatleadtogeneralizationonunseentestingdomains.
4.1 ExperimentSetup
Datasets.Weevaluategeneralizationtodomainshiftusingfourmulti-domaindatasets
inDomainBed[20],namelyPACS,VLCS,OfficeHomeandDomainNet.Wefollowthe
train-validate-testsplitofeachdatasetasin[20]andusethetraining-domainvalidation
settoperformthehyperparametersearch.
Evaluation.Tocomprehensivelyevaluateourmethod,Weexperimentonboththe
leave-one-out evaluation protocol and single-domain generalization protocol. In addi-
tion,weproposethedata-freedomaingeneralizationtoevaluatewhetheritispossible
to train a generalizable model in a data-free fashion with only task information, the
knowledge from LLMs and text-to-image models that bridge the text space to pixel
space.
Baseline. We set two baselines for our experiments, namely empirical risk min-
imization (ERM) and ERM with exponential moving average (ERM + EMA). ERM
withexponentialmovingaverageisdemonstratedtobemorestableandeffectivethan
ERM[4].Itisthusadoptedtoperformablationstudyandanalysissinceitsstableper-
formanceandmorecorrelatedtothevalidationaccuracy[4].
Implementation.AllexperimentsuseResNet50pretrainedonImageNet1kasthe
image encoder unless otherwise stated. We remove the dropout and follow the rest of
the implementation as in [20] since dropout is reported to have a negative impact on
some of the DG methods [24], e.g. RSC [25]. We adopt GPT-4 to extract novel do-
main knowledge and leverage Stable Diffusion 2 [42] as the text-to-image generation
model.WeuseoneA100GPUtogeneratesyntheticimages.Allexperimentsoftraining
ResNet50andCLIPViT-B16modelcanberunon1RTX3090GPU.
4.2 MainResults
WeperformtwoexistingevaluationsonthefourdatasetsinDomainBedbenchmarks.
Additionally, we propose a more challenging evaluation to further investigate to the
synthetic images generated condition on LLMs’ knowledge can serve as good repre-
sentationlearners.
Leave-one-out evaluation. Leave-one-out Evaluation leaves one domain as the
testing domain and uses the rest as training domains. For our method, all of the syn-
theticimagesaretreatedasanadditionaldomaintothesourcedomains.AsperTable1,
augmentingwiththenoveldomainsyntheticimagesleadstoaconsistentimprovement
(aslargeas5.2%)overtheERMandERM+EMAbaselines.Onaverage,weachievea
2.9%and2.4%improvementoverERMandERM+EMAbaselinesrespectively.OurData-freeGeneralization 9
Table 1: Leave-one-out Evaluation on DomainBed Benchmark. CLIP adopts ViT-B16 as the
backbone.†denotesreproducedresults.MixStyleresultistakenfrom[11]
Algorithm VLCS PACS OfficeHome DomainNet Avg
ERM[48] 77.5±0.4 85.5±0.2 66.5±0.3 40.9±0.1 67.6
IRM[3] 78.5±0.5 83.5±0.8 64.3±2.2 33.9±2.8 65.1
GroupDRO[43] 76.7±0.6 84.4±0.8 66.0±0.7 33.3±0.2 65.1
MLDG[27] 77.2±0.4 84.9±1.0 66.8±0.6 41.2±0.1 67.5
CORAL[46] 78.8±0.6 86.2±0.3 68.7±0.3 41.5±0.1 68.8
Mixup[51,55,57] 78.1±0.3 86.8±0.3 68.0±0.2 39.6±0.1 68.1
MMD[28] 77.9±0.1 87.2±0.1 66.2±0.3 23.5±9.4 63.7
RSC[25] 77.8±0.6 86.2±0.5 66.5±0.6 38.9±0.6 67.4
VREx[26] 78.1±0.2 87.2±0.6 65.7±0.3 30.1±3.7 65.3
SWAD[11] 79.1±0.4 88.1±0.4 70.6±0.3 46.5±0.2 66.9
MIRO[12] 79.0±0.2 85.4±0.4 70.5±0.4 44.3±0.2 65.9
MixStyle[61] 77.9 85.2 60.4 34.0 64.4
ERM†[48] 77.2±1.0 84.4±0.8 64.8±0.4 43.6±0.1 67.5
+ours 78.5±0.4 88.0±0.3 70.0±0.1 45.2±0.1 70.4
∆ +1.3 +3.6 +5.2 +1.6 +2.9
ERM+EMA 78.8±0.6 87.8±0.3 70.5±0.1 46.0±0.1 70.8
+ours 80.2±0.3 90.3±0.4 74.6±0.2 47.5±0.3 73.2
∆ +1.4 +2.5 +4.1 +1.5 +2.4
CLIPZero-shot 80.1 96.2 83.0 58.5 79.5
CLIPFinetune 82.4±0.1 95.3±0.2 84.8±0.1 59.9±0.1 80.6
+ours 82.7±0.3 96.5±0.3 86.5±0.2 61.3±0.0 81.8
∆ +0.3 +1.2 +1.7 +1.4 +1.2
methodalsoachievedasignificantimprovement(1.2%onaverage)overtheCLIPfine-
tunedbaseline.Thisimprovementisremarkable,giventhealreadyhighperformanceof
theCLIPmodel.
Single Domain Generalization. Single-domain generalization Evaluation lever-
ages a single domain for training and subsequently assesses the outcomes on the re-
mainingdomains.Thisscenariopresentsagreaterchallengewhenjuxtaposedwiththe
Leave-one-out setting due to the model’s exclusive exposure to just one domain dur-
ing its training phase. Such a setting accentuates the issue of restricted availability of
sourcedomains.Consideringourmethodologydoesnotimposeassumptionsoneither
thesourcedomainsorthemodel,butinsteadextrapolatesnoveldomainsviaLLMsto
augment the training set, it is optimally more suited for this specific context. Empiri-
cal evidence underscores its exceptional efficacy and with merely one source domain
of real images, our results closely mirror, and at times even surpass, those obtained
in a multi-domain configuration, as per Table 2. Specifically, we achieve the highest
of 78.0%, 87.6%, 69.4% on the three datasets, outperforming the ERM with multiple
source domains by margins of 0.8%, 3.2% and 4.6% respectively. Compared to base-
lines,ourmethodachievesaremarkableimprovementofover10%acrossalldatasets
and baselines. This evidences that our methodology substantially mitigates the chal-
lengesassociatedwithrestrictedsourcedomains,renderingitparticularlyoptimaland
effective in scenarios where source domains are unavailable, such as single-domain
generalization.10 YLi,SRenWDengetal.
Table 2: Single-domain Evaluation on DomainBed Benchmark. CLIP adopts ViT-B16 as the
backbone.
Algorithm VLCS PACS OfficeHome Avg
ASA[19] - 67.0 - -
Pro-RandConv[14] - 67.0 - -
CPerb[59] - 73.3 - -
RSC[25] 59.2±0.7 60.9±1.7 46.9±1.7 55.7
ERM(Multi-domain) 77.2±1.0 84.4±0.8 64.8±0.4 75.5
ERM[48] 59.2±0.8 64.6±0.6 51.5±0.3 58.4
+ours 76.3±0.2 83.9±0.9 64.7±0.2 75.0
∆ +17.1 +19.3 +13.2 +16.5
ERM+EMA(Multi-domain) 78.8±0.6 87.8±0.3 70.5±0.1 79.0
ERM+EMA 64.2±0.7 67.9±1.1 58.2±0.1 62.7
+ours 78.0±0.1 87.6±0.6 69.4±0.3 78.3
∆ +13.1 +21.7 +12.0 +15.6
Table3:ComparisonwithtwobaselinesandcurrentSOTAaugmentation-basedDGmethods.
AllmodelsareequippedwithEMAforfaircomparison.
Algorithm VLCS PACS Avg
MixStyle[61] 78.7±0.1 87.7±0.1 83.2
DSU[29] 77.7±0.0 87.6±0.2 82.7
AutoAug[15] 78.6±0.3 88.6±0.1 83.6
RandAug[16] 79.1±0.0 87.5±0.3 83.3
ERM+EMA 78.8±0.6 87.8±0.3 83.3
+largerbatch-size 78.1±0.1 87.4±0.1 82.7
+class-template 79.3±0.1 88.0±0.3 83.7
+class-prompt 79.3±0.0 88.5±0.2 83.9
+ours 80.2±0.3 90.3±0.4 85.3
Comparison with augmentation-based DG methods. We compared with SOTA
augmentation methods in Table 3 including MixStyle [61], DSU [29], AutoAug [15]
andRandAug[16],whereourmethoddemonstratesanimprovementofmorethan2%
onaverage.
4.3 Data-freeGeneralization.
Data-free Generalization Evaluation serves as a more difficult setting to evaluate our
proposed methods. Data-free Generalization aims to generalize to unseen testing do-
mains with only knowledge of the task, i.e. the classes and definition of each class
are available and no available data of any kind. To simulate Data-free Generalization
with existing benchmarks, we use all the domains in existing DG datasets as testing
domains. To evaluate our method, we directly train models on the synthetic images
generatedconditionedonnoveldomainknowledge.Thenthemodelistestedonallthe
available real images of the domains for evaluation. Results are illustrated in Table 4
whereweachievethehighestperformanceof79.9%,86.9%,67.4%withonlylessthan
1% gap between its multi-domain counterparts and largely surpasses single-domainData-freeGeneralization 11
counterparts.Notably,data-freeERM+EMApresentsanaccuracyof79.9%onVLCS
outperforming the multi-domain supervised counterparts by more than 1%. With the
knowledgeinjectedandnoveldomainextrapolated,thisempiricalresultillustratesthe
promiseofachievinggeneralizationinacompletelydata-freefashionfreeoflaborious
datacollectionandannotation.
Table4:Data-freegeneralizationonDomainBedBenchmark.
Algorithm VLCS PACS OfficeHome DomainNet Avg
ERM
Multi-domain 77.2±1.0 84.4±0.8 64.8±0.4 43.6±0.1 67.5
Single-domain 59.2±0.8 64.6±0.6 51.5±0.3 - -
Data-free(ours) 73.9±0.3 82.5±0.9 62.1±0.1 25.9±0.2 61.1
ERM+EMA
Multi-domain 78.8±0.6 87.8±0.3 70.5±0.1 46.0±0.1 70.8
Single-domain 64.2±0.7 67.9±1.1 58.2±0.1 - -
Data-free(ours) 79.9±0.6 86.9±0.1 67.4±0.2 30.3±0.1 66.1
4.4 AblationStudyandAnalysis
To fully understand the performance of our method, we perform an ablation study by
first providing three baselines building upon ERM + EMA with minor modifications.
First, we provide larger batchsize baseline, which is used to ablate the influence of
largerbatchsizesincurredbytheadditionalaugmentationdata.Then,weprovideclass
templatebaseline,whichpromptsthetext-to-imagesgenerationmodeltogeneratesyn-
theticimageswiththetemplate"Animageof[CLASS]".Thenwewillprovideathird
baseline,termedclasspromptthatwillpromptLLMstogiveadiffusion-styleprompt
(without explicitly instructing it to extrapolate novel domains) and use the generated
promptstoquerytext-to-imagemodelsforsyntheticdata.ComparisonisshowninTa-
ble3.Wecanseethatalargerbatchsizeinfacthasanegativeeffectwhilebothtemplate
and prompt baseline underperform our method. This ablates the influence brought by
text-to-imagemodelsandfurtherunderscorestheimportanceofLLMs’knowledgere-
gardingthenoveldomain.12 YLi,SRenWDengetal.
variancemeasureon PACS
w/ CLIP filter
94 w/o CLIP filter
LLMsextrapolation 89.87±0.4
text-to-imagegeneration 89.72±0.2
92
modeltraining 90.3±0.4
90 90.3 90
Table5:Varianceanalysisoverthethreemod-
88
ules,i.e.LLMs,text-to-imagegenerationandfi-
86.9
nalmodeltraining,withtheobjectivetomeasure
86 85.5
howstableourmethodperforms.
data-free leave-one-out
90 (a)
90
85
85.7 dataset-wise query
80 85 83.5 class-wise query
llm generated prompt
75 80 79.5 79.9
78.2 70 syth etic (class tem p late)
syth etic (class p rom p t) 75 74.3
65 ou rs
60 70
67.4
55 65.7
16 32 48 64 80 96 112 128 65 64.3
# d o m a in s
60
PACS VLCS OfficeHome
Fig.3: Scaling the training dataset by adding
(b)
more novel domains. Each novel domain con-
sistsof64images.Tofacilitatefaircomparison,Fig.4: (a) Effectiveness of CLIP filtering.
wescaletheclasstemplatemethodbythesame(b) Comparison between different knowl-
amountofimages. edgeextractionmethods.
Comparisonbetweendifferentknowledgeextraction.Weprovidethreeapproaches
toextractknowledgeregardingthenoveldomainsofparticularclasses.Comparisoncan
beseenin(b)ofFigure4,whereweshowthat,overall,class-wisecombinedwithLLM-
generatedpromptleadstobetterperformancethanclass-wisequeryonlyanddata-wise
query.Thisisbecauseclass-wisequeryprovidesmoreplausibleandreasonablenovel
domains given some class and LLM-generated prompt further extracts knowledge re-
gardingthisnoveldomainandincreasesdiversityingeneration.
Scalingtolargersyntheticdataset.Ithasbeenwidelyreportedthatdatagenerated
bygenerationmodelsnegativelyimpactsthemodel,especiallywhenthenumberofsyn-
theticimagesgrowsatscale[5,22].Tothisend,weinvestigatewhethertheperformance
increases scales with more synthetic data from more extrapolated novel domains. We
performscalingbypromptingLLMstoextrapolatemorenoveldomainsandgenerate64
imageperdomain.WecanseeinFigure3thatwithmoredomains(largerninSection
2.1),performancekeepsincreasing,whichisconsistentwithourtheoreticalframework.
Wealsomakeacomparisonwithclass-templateandclass-promptbaselinesandscale
the two baselines by increasing the synthetic images to the corresponding size. How-
ever,thesetwomethodsbothsufferfromperformancesaturationanddegradationwhen
syntheticdataincreases,whichisconsistentwithpreviousstudies[5,22].Thisdemon-
stratedthatourmethodcanscalebettertolargersizesofsyntheticdataandunderscore
theimportanceofnewknowledgeinjectedbyLLMsthatbenefitsgeneralization.
VarianceAnalysis.Weaimtomeasurehowstableourmethodisbydecomposing
thevarianceintothreeparts,i.e.LLMsextrapolation,text-to-imagegenerationandfinal
modeltraining.Werepeateachexperimentthreetimesandreporttheaverageandstan-
y
c ar
u c
c
A
ycaruccAData-freeGeneralization 13
dard deviation in Table 5. For instance, to conduct variance anlysis on text-to-image
generation, we use the same set of novel domains generated by LLMs, can generate
syntheticdatasetswiththesametext-to-imagemodelthreetimes.Asperthetable,we
canseethatallthreepartscontributetoarelativelysmallvariance,suggestingthatour
methodisstable.
AdditionalCLIPfiltering.Text-to-imagegenerationmodelsareessentiallynoisy
andmightgenerateimagesofdistortionorwithoutthemainclassofinterest.Weexper-
imentwithCLIPfilteringbeforethetrainingprocess.Asshownin(a)ofFigure4,we
canobserveanincreasewithadditionalfilteringtechniquesby1%.Tofurtherillustrate
theeffectivenessoffiltering,wevisualizesomefilteredfailurecasesinAppendix??.
DifferentLLMs.TomakesurethatourmethoddoesnotreplyonspecificLLMs,
i.e.ChatGPT-4,weconductexperimentswithLLMsfromdifferentfamilies,e.gLlama
andMixtralintable.
LLM A C P S Avg
GPT-4 94.4±0.2 85.0±0.5 98.5±0.1 83.3±1.7 90.3
Llama-13B 92.6±0.5 83.2±0.5 98.2±0.1 80.9±0.7 88.7
Llama-70B 93.0±0.4 83.6±0.4 98.5±0.2 81.9±0.4 89.3
Mixtral-8x7B 92.4±0.0 84.6±0.3 98.8±0.0 81.1±0.6 89.2
Table6:PerformancewithdifferentLLMs.WeexperimentwithGPT4,Llama-13B,Llama-70B
andMixtral-8x7Bmodels.
Visualization.Weprovidevisualizationtovalidatethatourmethoddoextrapolate
noveldomainsandgeneratethedesiredclass.Wedemonstrategeneratedimagesfrom
three different novel domains of the PACS dataset in the last four columns of Figure
5andcomparethemwiththerealimagesinthePACSdataset(firsttwocolumns).We
canseethatthegeneratednoveldomainsarebynomeansaninterpolationofthereal
domains and are different from the existing training domains by a large margin. This
illustratesthatourmethodtakesonestepfurthertoward"truly"extrapolationofnovel
domainswithouthumanlabor.WeprovidemorevisualizationintheAppendix.
5 RelatedWork
DomainGeneralization. Variousapproacheshavebeenproposedtosolvethisprob-
lem,suchasdomainalignment[28,30],meta-learning[6,27],ensemblelearning[4,10]
and augmentation-based [1,29,56,60–62]. Augmentation-based methods are closely
related to this work, both with the intention of generating more source domains to
approximate the expected generalization error. However, these methods resort to in-
terpolation of existing domains and fail to extrapolate the "truly" novel domains. For
instance,MixStyle[61]mixesthestatisticsoftwosamplesbylinearinterpolation.More
recently,withtheadventofvision-languagemodelssuchasCLIP[39]andStableDif-
fusion [42], researchers propose to utilize Stable Diffusion to identify and cure short-
cuts [54] or CLIP to generate novel domain augmentation [49]. However, they all re-
quiresomeformofhumanlabortopre-defineasetofdomainsorstyles,whichmakes
themlaboriousandnotscalable.Ourworkaimstosolvethisproblemandachievegen-
uinedomainextrapolation.14 YLi,SRenWDengetal.
artpainting cartoon cityscapes expressionist steampunk renaissance
dog
horse
person
house
Fig.5:ExamplesofsyntheticimagesconditionedonnoveldomainknowledgefromLLM.The
firsttwocolumns(i.e.artpaintingandcartoon)areselectedfromPACSdatasetswhiletherest
fourcolumnsareimagesgeneratedbasedonthenoveldomains(i.e.cityscapes,etc)providedby
LLMs.
Languagescaffoldedvisionaimstodevelopbetterandmorerobustvisionsystems
with the help of language. Our method also falls within this category. Clipood [45]
proposes to fine-tune a CLIP model to adapt the downstream DG tasks by a text sim-
ilarityawareloss.[33]utilizeanRNNasanexplanationnetworkenforcingthemodel
toself-explain,therebyincreasingtherobustness.[58]utilizelanguagemodelstopro-
duceacomprehensivesetofbottleneckfeaturesandleverageCLIPtoclassify.Withthe
helpfromLLMs,[58]haspushedtheperformanceofthebottlenecknetworktoSOTA.
Despite many works proposed, this research, to the best of our knowledge, is the first
endeavortoinvestigatethepotentialofaLargeLanguageModel(LLM)infacilitating
thetrainingofarobustandgeneralizablevisionmodel.
LargeLanguageModels.RecentadvancesinNLP,asevidencedby [9,35])high-
light the impressive capabilities of Large Language Models like ChatGPT, GPT4 [9],
andLlama2[47].Thesemodelsgleandiverseknowledgefromvasttrainingdatasourced
from the Internet, positioning LLMs as next-generation knowledge bases for various
tasks.Motivatedbystudiesshowcasingthevastknowledge[2,36]andtheexceptional
reasoning ability [23,38,53] within LLMs, we aim to harness this knowledge for the
trainingofrobustvisionmodels.
6 Conclusion
ThelimitedavailabilityofdomainshasbeenaprevailingprobleminDomainGeneral-
ization.Inthiswork,weproposethefirstdata-freelearningparadigmthatleveragesthe
knowledgeandreasoningofLLMstoextrapolatenoveldomains.Bybridgingthetext-
centric knowledge and pixel input space by sampling from text-to-image generation
models,weareabletotraingeneralizablemodelswithtaskinformationonly.Thesyn-
theticimagescanbeusedtoaugmenttheexistingdatasetortrainamodelinadata-freeData-freeGeneralization 15
fashion. Extensive experiments have demonstrated that our method achieves signifi-
cant improvements over baselines and the state-of-the-art by a significant margin. We
also demonstrate a promising learning paradigm where LLMs’ knowledge combined
with text-to-image generation models are sufficient to train a generalizable model to
any task. However, it’s important to acknowledge the inherent biases present in foun-
dationalmodelslikeLLMsandtext-to-imagegenerators,whichourvisionmodelsmay
inherit.Thisincludesabiastowardscommonobjectrepresentations,exacerbatingthe
long-taildistributionchallengebyprivilegingcommonentitiesingenerationprocesses.
Furthermore, our method faces limitations in domain specificity, with current text-to-
image models excelling in generating natural images but underperforming in special-
izedfieldssuchasmedicalimaging,highlightingacriticalareaforfutureimprovement
andadaptation.
References
1. Albuquerque,I.,Monteiro,J.,Falk,T.H.,Mitliagkas,I.:Adversarialtarget-invariantrepre-
sentationlearningfordomaingeneralization.arXivpreprintarXiv:1911.008048(2019) 2,
13
2. Alivanistos,D.,Santamaría,S.B.,Cochez,M.,Kalo,J.C.,vanKrieken,E.,Thanapalasingam,
T.:Promptingasprobing:Usinglanguagemodelsforknowledgebaseconstruction.arXiv
preprintarXiv:2208.11057(2022) 14
3. Arjovsky, M., Bottou, L., Gulrajani, I., Lopez-Paz, D.: Invariant risk minimization. arXiv
preprintarXiv:1907.02893(2019) 9
4. Arpit, D., Wang, H., Zhou, Y., Xiong, C.: Ensemble of averages: Improving model selec-
tion and boosting performance in domain generalization. Advances in Neural Information
ProcessingSystems35,8265–8277(2022) 8,13
5. Azizi,S.,Kornblith,S.,Saharia,C.,Norouzi,M.,Fleet,D.J.:Syntheticdatafromdiffusion
modelsimprovesimagenetclassification.arXivpreprintarXiv:2304.08466(2023) 3,12
6. Balaji, Y., Sankaranarayanan, S., Chellappa, R.: Metareg: Towards domain generalization
usingmeta-regularization.Advancesinneuralinformationprocessingsystems31(2018) 13
7. Ben-David,S.,Blitzer,J.,Crammer,K.,Kulesza,A.,Pereira,F.,Vaughan,J.W.:Atheoryof
learningfromdifferentdomains.Machinelearning79,151–175(2010) 2
8. Blanchard,G.,Lee,G.,Scott,C.:Generalizingfromseveralrelatedclassificationtaskstoa
newunlabeledsample.Advancesinneuralinformationprocessingsystems24(2011) 2
9. Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,Dhariwal,P.,Neelakantan,A.,
Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shotlearners.Advancesin
neuralinformationprocessingsystems33,1877–1901(2020) 2,14
10. Cha,J.,Cho,H.,Lee,K.,Park,S.,Lee,Y.,Park,S.:Domaingeneralizationneedsstochastic
weight averaging for robustness on domain shifts. arXiv preprint arXiv:2102.08604 3, 3
(2021) 13
11. Cha, J., Chun, S., Lee, K., Cho, H.C., Park, S., Lee, Y., Park, S.: Swad: Domain gener-
alizationbyseekingflatminima.AdvancesinNeuralInformationProcessingSystems34,
22405–22418(2021) 9
12. Cha,J.,Lee,K.,Park,S.,Chun,S.:Domaingeneralizationbymutual-informationregular-
izationwithpre-trainedmodels.In:EuropeanConferenceonComputerVision.pp.440–457.
Springer(2022) 9
13. Chen,J.,Lu,Y.,Yu,Q.,Luo,X.,Adeli,E.,Wang,Y.,Lu,L.,Yuille,A.L.,Zhou,Y.:Tran-
sunet:Transformersmakestrongencodersformedicalimagesegmentation.arXivpreprint
arXiv:2102.04306(2021) 216 YLi,SRenWDengetal.
14. Choi,S.,Das,D.,Choi,S.,Yang,S.,Park,H.,Yun,S.:Progressiverandomconvolutionsfor
single domain generalization. In: Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition.pp.10312–10322(2023) 10
15. Cubuk,E.D.,Zoph,B.,Mane,D.,Vasudevan,V.,Le,Q.V.:Autoaugment:Learningaugmen-
tationpoliciesfromdata.arXivpreprintarXiv:1805.09501(2018) 10
16. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated data aug-
mentation with a reduced search space. In: Proceedings of the IEEE/CVF conference on
computervisionandpatternrecognitionworkshops.pp.702–703(2020) 10
17. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional
transformersforlanguageunderstanding.arXivpreprintarXiv:1810.04805(2018) 2
18. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,T.,De-
hghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is
worth16x16words:Transformersforimagerecognitionatscale.ICLR(2021) 2
19. Fan,X.,Wang,Q.,Ke,J.,Yang,F.,Gong,B.,Zhou,M.:Adversariallyadaptivenormalization
forsingledomaingeneralization.In:ProceedingsoftheIEEE/CVFconferenceonComputer
VisionandPatternRecognition.pp.8208–8217(2021) 10
20. Gulrajani, I., Lopez-Paz, D.: In search of lost domain generalization. arXiv preprint
arXiv:2007.01434(2020) 8
21. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. arXiv
preprintarXiv:1512.03385(2015) 2
22. He,R.,Sun,S.,Yu,X.,Xue,C.,Zhang,W.,Torr,P.,Bai,S.,Qi,X.:Issyntheticdatafrom
generativemodelsreadyforimagerecognition?arXivpreprintarXiv:2210.07574(2022) 3,
12
23. Huang,J.,Chang,K.C.C.:Towardsreasoninginlargelanguagemodels:Asurvey.In:Find-
ingsoftheAssociationforComputationalLinguistics:ACL2023.pp.1049–1065.Associa-
tionforComputationalLinguistics,Toronto,Canada(Jul2023).https://doi.org/10.
18653/v1/2023.findings-acl.67, https://aclanthology.org/2023.
findings-acl.67 14
24. Huang,Z.,Wang,H.,Huang,D.,Lee,Y.J.,Xing,E.P.:Thetwodimensionsofworst-case
trainingandtheirintegratedeffectforout-of-domaingeneralization.In:Proceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.9631–9641(2022)
8
25. Huang,Z.,Wang,H.,Xing,E.P.,Huang,D.:Self-challengingimprovescross-domaingen-
eralization. In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
August23–28,2020,Proceedings,PartII16.pp.124–140.Springer(2020) 8,9,10
26. Krueger, D., Caballero, E., Jacobsen, J.H., Zhang, A., Binas, J., Zhang, D., Le Priol, R.,
Courville,A.:Out-of-distributiongeneralizationviariskextrapolation(rex).In:International
ConferenceonMachineLearning.pp.5815–5826.PMLR(2021) 9
27. Li, D., Yang, Y., Song, Y.Z., Hospedales, T.: Learning to generalize: Meta-learning for
domain generalization. In: Proceedings of the AAAI conference on artificial intelligence.
vol.32(2018) 9,13
28. Li,H.,Pan,S.J.,Wang,S.,Kot,A.C.:Domaingeneralizationwithadversarialfeaturelearn-
ing.In:ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.pp.
5400–5409(2018) 9,13
29. Li, X., Dai, Y., Ge, Y., Liu, J., Shan, Y., Duan, L.Y.: Uncertainty modeling for out-of-
distributiongeneralization.arXivpreprintarXiv:2202.03958(2022) 2,10,13
30. Li,Y.,Tian,X.,Gong,M.,Liu,Y.,Liu,T.,Zhang,K.,Tao,D.:Deepdomaingeneralization
viaconditionalinvariantadversarialnetworks.In:ProceedingsoftheEuropeanconference
oncomputervision(ECCV).pp.624–639(2018) 13
31. Li,Y.,Cai,W.,Gao,Y.,Li,C.,Hu,X.:Morethanencoder:Introducingtransformerdecoder
toupsample.arXivpreprintarXiv:2106.10637(2021) 2Data-freeGeneralization 17
32. Liu, J., Shen, Z., He, Y., Zhang, X., Xu, R., Yu, H., Cui, P.: Towards out-of-distribution
generalization:Asurvey.arXivpreprintarXiv:2108.13624(2021) 2
33. Min,S.,Park,N.,Kim,S.,Park,S.,Kim,J.:Groundingvisualrepresentationswithtextsfor
domaingeneralization.In:EuropeanConferenceonComputerVision.pp.37–53.Springer
(2022) 14
34. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I.,
Chen,M.:Glide:Towardsphotorealisticimagegenerationandeditingwithtext-guideddif-
fusionmodels.arXivpreprintarXiv:2112.10741(2021) 2
35. Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,Mishkin,P.,Zhang,C.,Agarwal,
S.,Slama,K.,Ray,A.,etal.:Traininglanguagemodelstofollowinstructionswithhuman
feedback.AdvancesinNeuralInformationProcessingSystems35,27730–27744(2022) 14
36. Petroni,F.,Rocktäschel,T.,Riedel,S.,Lewis,P.,Bakhtin,A.,Wu,Y.,Miller,A.:Language
modelsasknowledgebases?In:Proceedingsofthe2019ConferenceonEmpiricalMethods
inNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLan-
guageProcessing(EMNLP-IJCNLP).pp.2463–2473.AssociationforComputationalLin-
guistics, Hong Kong, China (Nov 2019). https://doi.org/10.18653/v1/D19-
1250,https://aclanthology.org/D19-1250 5,14
37. Qiao,F.,Zhao,L.,Peng,X.:Learningtolearnsingledomaingeneralization.In:Proceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.12556–
12565(2020) 2
38. Qiao,S.,Ou,Y.,Zhang,N.,Chen,X.,Yao,Y.,Deng,S.,Tan,C.,Huang,F.,Chen,H.:Rea-
soningwithlanguagemodelprompting:Asurvey.In:Proceedingsofthe61stAnnualMeet-
ingoftheAssociationforComputationalLinguistics(Volume1:LongPapers).pp.5368–
5393. Association for Computational Linguistics, Toronto, Canada (Jul 2023). https:
//doi.org/10.18653/v1/2023.acl-long.294,https://aclanthology.
org/2023.acl-long.294 5,14
39. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,Askell,
A.,Mishkin,P.,Clark,J.,etal.:Learningtransferablevisualmodelsfromnaturallanguage
supervision.In:Internationalconferenceonmachinelearning.pp.8748–8763.PMLR(2021)
2,5,13
40. Rombach,R.,Blattmann,A.,Lorenz,D.,Esser,P.,Ommer,B.:High-resolutionimagesyn-
thesiswithlatentdiffusionmodels.In:ProceedingsoftheIEEE/CVFconferenceoncom-
putervisionandpatternrecognition.pp.10684–10695(2022) 2
41. Rombach,R.,Blattmann,A.,Lorenz,D.,Esser,P.,Ommer,B.:High-resolutionimagesyn-
thesiswithlatentdiffusionmodels.In:ProceedingsoftheIEEE/CVFConferenceonCom-
puterVisionandPatternRecognition(CVPR).pp.10684–10695(June2022) 5
42. Rombach,R.,Blattmann,A.,Lorenz,D.,Esser,P.,Ommer,B.:High-resolutionimagesyn-
thesiswithlatentdiffusionmodels(2021) 8,13
43. Sagawa,S.,Koh,P.W.,Hashimoto,T.B.,Liang,P.:Distributionallyrobustneuralnetworks
for group shifts: On the importance of regularization for worst-case generalization. arXiv
preprintarXiv:1911.08731(2019) 9
44. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,K.,Gon-
tijoLopes,R.,KaragolAyan,B.,Salimans,T.,etal.:Photorealistictext-to-imagediffusion
modelswithdeeplanguageunderstanding.AdvancesinNeuralInformationProcessingSys-
tems35,36479–36494(2022) 2
45. Shu,Y.,Guo,X.,Wu,J.,Wang,X.,Wang,J.,Long,M.:Clipood:Generalizingcliptoout-
of-distributions.arXivpreprintarXiv:2302.00864(2023) 2,5,14
46. Sun,B.,Saenko,K.:Deepcoral:Correlationalignmentfordeepdomainadaptation.In:Com-
puterVision–ECCV2016Workshops:Amsterdam,TheNetherlands,October8-10and15-
16,2016,Proceedings,PartIII14.pp.443–450.Springer(2016) 918 YLi,SRenWDengetal.
47. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N.,
Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat
models.arXivpreprintarXiv:2307.09288(2023) 14
48. Vapnik,V.:Statisticallearningtheoryjwileynewyork(1998) 9,10
49. Vidit,V.,Engilberge,M.,Salzmann,M.:Clipthegap:Asingledomaingeneralizationap-
proachforobjectdetection.In:ProceedingsoftheIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition.pp.3219–3229(2023) 13
50. Wang,J.,Lan,C.,Liu,C.,Ouyang,Y.,Qin,T.,Lu,W.,Chen,Y.,Zeng,W.,Yu,P.:General-
izingtounseendomains:Asurveyondomaingeneralization.IEEETransactionsonKnowl-
edgeandDataEngineering(2022) 2
51. Wang, Y., Li, H., Kot, A.C.: Heterogeneous domain generalization via domain mixup. In:
ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Pro-
cessing(ICASSP).pp.3622–3626.IEEE(2020) 9
52. Wang,Z.,Luo,Y.,Qiu,R.,Huang,Z.,Baktashmotlagh,M.:Learningtodiversifyforsin-
gle domain generalization. In: Proceedings of the IEEE/CVF International Conference on
ComputerVision.pp.834–843(2021) 2
53. Wei,J.,Wang,X.,Schuurmans,D.,Bosma,M.,ichter,b.,Xia,F.,Chi,E.,Le,Q.V.,Zhou,
D.:Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.In:Koyejo,S.,
Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.) Advances in Neural In-
formation Processing Systems. vol. 35, pp. 24824–24837. Curran Associates, Inc. (2022),
https://proceedings.neurips.cc/paper_files/paper/2022/file/
9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf 6,14
54. Wu,S.,Yuksekgonul,M.,Zhang,L.,Zou,J.:Discoverandcure:Concept-awaremitigation
ofspuriouscorrelation.arXivpreprintarXiv:2305.00650(2023) 13
55. Xu,M.,Zhang,J.,Ni,B.,Li,T.,Wang,C.,Tian,Q.,Zhang,W.:Adversarialdomainadapta-
tionwithdomainmixup.In:ProceedingsoftheAAAIconferenceonartificialintelligence.
vol.34,pp.6502–6509(2020) 9
56. Xu,Z.,Liu,D.,Yang,J.,Raffel,C.,Niethammer,M.:Robustandgeneralizablevisualrep-
resentation learning via random convolutions. arXiv preprint arXiv:2007.13003 (2020) 2,
13
57. Yan,S.,Song,H.,Li,N.,Zou,L.,Ren,L.:Improveunsuperviseddomainadaptationwith
mixuptraining.arXivpreprintarXiv:2001.00677(2020) 2,9
58. Yang,Y.,Panagopoulou,A.,Zhou,S.,Jin,D.,Callison-Burch,C.,Yatskar,M.:Languagein
abottle:Languagemodelguidedconceptbottlenecksforinterpretableimageclassification.
In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
pp.19187–19197(2023) 14
59. Zhao,D.,Qi,L.,Shi,X.,Shi,Y.,Geng,X.:Anovelcross-perturbationforsingledomain
generalization.arXivpreprintarXiv:2308.00918(2023) 10
60. Zhou,K.,Yang,Y.,Hospedales,T.,Xiang,T.:Learningtogeneratenoveldomainsfordo-
main generalization. In: Computer Vision–ECCV 2020: 16th European Conference, Glas-
gow, UK, August 23–28, 2020, Proceedings, Part XVI 16. pp. 561–578. Springer (2020)
13
61. Zhou,K.,Yang,Y.,Qiao,Y.,Xiang,T.:Domaingeneralizationwithmixstyle.arXivpreprint
arXiv:2104.02008(2021) 2,9,10,13
62. Zhou, T., Konukoglu, E.: Fedfa: Federated feature augmentation. arXiv preprint
arXiv:2301.12995(2023) 2,13