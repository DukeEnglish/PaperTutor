WatChat: Explaining perplexing programs
by debugging mental models
Kartik Chandra (MIT), Tzu-Mao Li (UCSD), Rachit Nigam (Cornell/MIT),
Josh Tenenbaum (MIT), Jonathan Ragan-Kelley (MIT)
Spring 2024
Abstract
Often,agoodexplanationforaprogram’sunexpectedbehaviorisabugintheprogrammer’s
code. But sometimes, an even better explanation is a bug in the programmer’s mental model
of the language they are using. Instead of merely debugging our current code (“giving the
programmer a fish”), what if our tools could directly debug our mental models (“teaching the
programmer to fish”)?
Inthispaper,weapplyideasfromcomputationalcognitivesciencetodoexactlythat. Given
aperplexingprogram,weuseprogramsynthesistechniquestoautomaticallyinferpotentialmis-
conceptions that might cause the user to be surprised by the program’s behavior. By analyzing
these misconceptions, we provide succinct, useful explanations of the program’s behavior. Our
methods can even be inverted to synthesize pedagogical example programs for diagnosing and
correcting misconceptions in students.
1 Introduction
In a well-known 2012 tech talk called “Wat” [2], Gary Bernhardt asks the audience, “Does anyone
know, in JavaScript, what array plus array ( ) is? Is it empty array? Type error?” When
[] + []
he types this expression into a REPL, the audience bursts out laughing to see that it returns ,
""
the empty string. As the audience’s reaction shows, such behavior is surprising even to expert
programmers. It is natural to ask, then: Why does [] + [] return "" in JavaScript?
A typical explanation of this behavior, e.g. on a website like StackOverflow [47], gives two key
pieces of information: First, when given non-numbers, JavaScript’s operator converts its operands
+
to strings for concatenation. Second, Arrays cast to strings by joining their elements with commas
(without outer square brackets), so the empty array casts to the empty string.
Why do these two statements together form a satisfying explanation of ? If that seems
[] + []
like an odd question to ask, note that there are many other possible responses that could theoreti-
cally have passed for “explanations.” For example, “because the ECMAScript specification says so”
would technically be a correct explanation, as would a tedious step-by-step trace of how the given
program is parsed, JIT-compiled, and executed on hardware. But of course, intuitively we find such
explanations to be dissatisfying, uninsightful, unhelpful, and in some cases even condescending.
It is this intuition that we are after in this paper. What makes a good explanation? And
how do humans recognize and construct good explanations for one another? These are important
questions that lie squarely at the intersection of PL and HCI: we would like our tools to be able
to explain themselves to us in a helpful, human-like way. But even beyond PL and HCI, the study
of explanation has important consequences for disciplines ranging from education [29] to artificial
intelligence [32].
1
4202
raM
8
]LP.sc[
1v43350.3042:viXraA erroneous mental model (Σ̃) B
❓
🧑💻 ⚙ [] 💥
🧑💻
❌ 🧑🏫
expected output (r)̃ ⚙
✅
when r ̃≠ r, user
[]+[] 💥 asks a "why…?"
sT eh ee
s
e px
,
p Σ,l a ai nn de r
r , She simulates the user to infer (Σ̃, r)̃
question: but not the user's that best explain why the user asked
The user writes Why does []+[] internal Σ̃ and r.̃ the "why?" question…
a program (p) true language semantics (Σ) return ""? …and then offers the user a
"patch" from Σ̃ to Σ, i.e. an
⚙ “” explanation:
✅
observed output (r) Because + doesn't concatenate
arrays. Instead, it…
(A) People ask "why…?" questions when their expectations are violated (B) People answer "why…?" questions by inferring and
due to misconceptions in their mental model of the language semantics. patching "bugs" in the asker's mental model.
Figure 1: System overview. Panel “A” shows WatChat’s model of how programmers come to ask
“why?” questions. Panel “B” shows how an explainer reasons over the model in Panel “A” to find a
good explanation.
Inthispaper, wetakeinspirationfromempiricalcognitivescienceworkonexplanationtodesign
an algorithm that gives helpful, human-like explanations of unexpected program behavior. Our
guiding premise is that explanation is a cooperative social interaction: by asking a “Why?”
question, the asker signals that they do not understand an observed phenomenon, and requests a
correctionoradditiontotheirmentalmodel. Theexplainerinturninferspossiblegapsintheasker’s
mental model that would cause them to ask “why?,” and makes a relevant “because” statement that
helpfullyfillsthosegapsin. Crucially,thisprocessiscontext-sensitive: weflexiblytailorexplanations
to our audiences (e.g. giving simpler explanations to children than to adults) and to the current
situation (e.g. giving simpler explanations to someone in a rush vs. someone at leisure). Putting
these ideas into practice, we design a system that answers “Why?” questions about JS
type coercion.
Oursystem, WatChat, explainsthesurprisingbehaviorsfromthe“Wat” talk, aswellasavariety
of other cases, in a helpful, human-like, and conversational manner. If you ask WatChat “Why does
return ?” then WatChat does what Bernhardt does in his talk: it asks you if you
[] + [] ""
expected the empty array as output. If you say yes, then WatChat explains that “The operator
+
does not concatenate arrays; instead, it casts them to strings and concatenates those.” If you then
go on to ask, “But then why isn’t the output ?” WatChat responds, “When converted to
"[][]"
strings, arrays don’t have s around them. Hence, gives and thus gives .”
[] [] "" [] + [] ""
How does WatChat intelligently engage in such explanation interactions? The key idea behind
WatChat is to identify possible misconceptions the user has based on the program being asked
about. This is a technical challenge: we must not only formalize the vast space of erroneous
mental models a user might have, but also efficiently search through it. Our solution is
to model users’ mental models as counterfactual (erroneous) semantics for JS (Figure 1A). Given a
program, we apply standard program synthesis techniques “in reverse” to synthesize counterfactual
semantics under which that program gives a different result from standard JS semantics (i.e. the
program’s behavior is “unexpected” to a user with that mental model). Explanation then takes
the form of patching bugs in the counterfactual semantics, i.e. debugging the user’s
mental model (Figure 1B). We show that this procedure can be implemented efficiently, searching
through a space of 232 possible mental models within milliseconds, and that it consistently produces
2helpful, human-like explanations. Finally, we show that we can even run our system in reverse
to produce high-quality pedagogical examples that can diagnose specific misconceptions with high
precision.
Our implementation of WatChat is available at .
https://github.com/kach/watchat
2 Background
Philosophers and cognitive scientists have long debated what makes a good explanation, and a full
account is beyond the scope of this paper. We instead direct readers to Miller [32], who surveys
the field for explainable AI researchers and distills from the literature four desiderata for good
explanations:
1. They are contrastive, i.e. they explain why Y (the “fact”) is true instead of the expected X
(the “foil,” which may or may not be explicit in the “why?” question asked) [28, 41].
2. Theyareselective,i.e.theydonotexhaustivelyrecountthefullcausalchainbehindanevent,
but rather a sparse subset chosen pragmatically by the explainer [14, 13, 37].
3. They are causal, not statistical, i.e. it is not enough to say “because Y is more common than
X” [21] and it is not enough to give the likeliest causal factor [19, 31].
4. They are social and interactive, occurring as part of a conversation between two agents in a
given context [18, 22].
Prior work in PL, HCI, and CS education has addressed some of these desiderata independently.
Notably, the Whyline [24, 26, 25, 23] (which inspired WatChat’s name) was a seminal system that
gave causal, contrastive explanations in the form of traces showing all causally-relevant events that
answer“why?” or“whynot?” questionsaboutJavaprograms. Similarly,Amalgam[34]showslogical
derivationstoanswer“why” and“whynot?” questionsaboutmodelsfoundbyAlloy. However, these
two methods are not selective (they always give the full causal graph) and they are not social (they
do not interactively reason over models of their interlocutors). Other methods flag uncommon code
patterns or common bugs [12, 43]: that is, they give a statistical account (“this is a common bug”)
rather than a causal account of why a program might give unexpected results (“this bug leads to
this behavior because...”). Thus, they cannot properly explain bugs caused by conceptual errors
[43].
In this paper we seek to build a system that satisfies all of Miller’s desiderata at once. Our work
builds on recent ideas in cognitive science [6] that seeks to characterize the nature of cooperative
explanation. Instead of debugging the given program, we debug the user’s mental model [9, 20] of
the language they are using. The abstract idea of representing mental models with programs (and
misconceptions as bugs) is not new — for example, in his book “Mind Bugs,” VanLehn [46] applies
this analogy to understand young math students’ procedural misconceptions about subtraction.
More recently, Lu and Krishnamurthi [30] apply this idea to model misconceptions about scope and
mutation as “bugs” in interpreters for the language. Our work goes one step further by inferring
and correcting (i.e. synthesizing and debugging) these mental models automatically. Our work is
closelyrelatedtoworkapplyingmentalstateinferencetoautomatictutoring[38,39]andexplainable
robotics [4]. Similarly, error messages in some languages (notably Elm [8, 36]) heuristically offer
manually-written “hints” to help programmers when programs fail to compile — our method can
be seen as a way to automate the generation of such hints, in order to help developers at all skill
levels [1].
33 Method
WatChat takes as input a JavaScript program p whose behavior is to be explained. Under standard
ECMAScript semantics Σ [16], p produces some result p = r. However, because the user is
Σ
asking for an explanation of p’s behavior, we assume th(cid:74)at(cid:75)they instead expected some alternate
result r ̸= r. This alternate expectation must have been caused by the user having one or more
(cid:101)
misconceptionsaboutthelanguagesemanticsintheirmentalmodelofthelanguage. Agood,helpful
explanation of p’s behavior should infer these misconceptions and correct them.
To operationalize this idea, we model the user’s misconceptions as counterfactual, erroneous
semantics Σ(cid:101) for the language, under which p actually does produce r (cid:101). Our goal is thus to infer Σ(cid:101)
given p. (Note that we do not necessarily know r in advance — depending on the “why?” question
(cid:101)
asked, a user may or may not have revealed their expectation. As we will see below, our system
handles ambiguity about r by asking clarification questions when needed.)
(cid:101)
We perform this inference task with an unconventional application of program synthesis: we
“s myn isth ine ts ei rz pe ra et“ em r”is fi rn ot merp Lr uet ae nr” df Kor riΣ(cid:101) sht nh aa mt usa rt ti hs ifi [e 3s 0t ])h .e Tco hn isst ar pa pin lit ct ah tia ot n(cid:74)p
of(cid:75)Σ
sy̸= nt(cid:74)hp
e(cid:75)sΣ(cid:101)
is( iw se unb co or nro vw ent th ioe nt ae lr im
n
thesensethatwhilesynthesistraditionallysearchesforaprogramwithrespecttoafixedinterpreter,
here we do the opposite: search for an interpreter with respect to a fixed program.
Of course, this is a severely underdetermined problem. Given only p, there are infinitely many
pairs ⟨Σ(cid:101),r (cid:101)⟩ that satisfy our criteria (e.g. for each r (cid:101)̸= r, consider the degenerate semantics Σ(cid:101)r where
(cid:101)
all programs vacuously output r). Thus, we must reason probabilistically: we define a probability
(cid:101)
distribution over misinterpreters based on our prior belief about how likely various misconceptions
are. Then, we infer Σ(cid:101) and r (cid:101)on a maximum a posteriori (MAP) basis. If there are multiple equally-
good explanations, we ask the user for clarification about their expectations (e.g. by asking “Did
you expect r or r ?”). The strong inductive bias given by the prior distribution, combined with the
(cid:101)1 (cid:101)2
ability to ask clarification questions, allows us to find reasonable solutions to this underconstrained
problem.
Notice that this method for explanation fulfills all of Miller’s desiderata: it is contrastive (it
explainswhypdidnot producer), itisselective(itonlycommunicatestherelevantmisconceptions,
(cid:101)
not the full evaluation trace), it is causal (the misconceptions are causally relevant to producing
r), and it is social (it makes inferences about the asker’s mental state, and engages in interactive
clarification when necessary).
3.1 Implementation details
We implemented our system using the Rosette solver-aided DSL [45, 44] and used the Z3 solver
[11], along with standard techniques to encode symbolic programs and interpreters [3, 5] in order
to facilitate efficient SMT-powered program synthesis.
Taking inspiration from prior work [15, 35], we implemented an interpreter for a subset of the
ECMAScript 2024 specification that covers the “essence” of type coercion in JavaScript.1 As we
read the specification, we marked parts that we found unintuitive or surprising. Then, based on
our notes, we introduced N = 32 potential bugs (representing potential misconceptions) in our
interpreter. These bugs are listed in Table 1.
1ThissubsetincludestheprimitivetypesUndefined,Null,Boolean,String,Number,andObject(withsupport
for Array exotic objects), but not Symbol or BigInt. We support expressions involving the operators typeof, !,
ternary ?:, ===, ==, unary +, binary +, unary -, binary -, <, >=, &&, ||, and ?? on these types. Finally, we support
indexing arrays and strings with subscripts [], and we support the .sort() method on Arrays.
4We designed the interpreter so that each misconception can be toggled on and off indepen-
dently with a boolean flag. Each of the 2N possible subsets of bugs M represents one “misinter-
preter,” i.e. one possible erroneous mental model. We define Σ to be the semantics given by the
M
(mis)interpretercontainingthebugsinsetM. Ofcourse, Σ correspondstostandardECMAScript
{}
semantics because there are no bugs present. (We will continue to refer to this with the unadorned
Σ.)
As an example, below is our implementation of the operator, as described in §13.5.3 of
typeof
theECMAScriptspecification. Thisdefinitionincludestwopotentialmisconceptions:
typeof-null
, which says that is (it is actually ), and
-is-null typeof(null) "null" "object" typeof-array
, which says that returns for Arrays (it actually returns ).
-is-array typeof "array" "object"
1
(define (sem-typeof M val)
2
(cond
3
[(and (mis-typeof-null-is-null? M) (js-null? val))
4
(js-string "null")]
5
[(js-null? val) (js-string "object")]
6
[(js-undefined? val) (js-string "undefined")]
7
[(js-boolean? val) (js-string "boolean")]
8
[(js-number? val) (js-string "number")]
9
[(js-string? val) (js-string "string")]
10
[(and (mis-typeof-array-is-array? M) (js-array? val))
11
(js-string "array")]
12
[(js-object? val) (js-string "object")]))
To briefly review our notation using this example: if M = {typeof-null-is-null}, and the
program p = typeof(null)+typeof([]), then a programmer who has the misconceptions in M
would expect p to produce p
ΣM
= "nullobject". Of course, under standard ECMAScript
semantics it actually produce(cid:74)s (cid:75) p
Σ
= "objectobject".
Finally, we defined a probab(cid:74)il(cid:75)ity distribution over these misinterpreters. We assume each mis-
conception m is independently present with probability q , so the probability of misinterpreter M
i i
is P(M) = (cid:81) q . For now, we set all q equal. This naturally places a higher prior belief on
mi∈M i i
misinterpreters with fewer bugs.
Of course, this is a very coarse estimate of the distribution of erroneous mental models a user
might have. In the future, we would like to better capture this distribution. Instead of brain-
storming misconceptions, which is subject to the “expert blind spot” [33], we plan to systematically
collect misconceptions based on behavioral studies on real programmers, as previous work has done
in other domains [10, 30]. Additionally, we plan to measure the frequencies of these misconceptions
empirically, and to model their covariance structure instead of treating misconceptions as indepen-
dent. Finally, we plan to use many additional available sources of information to inform the prior
over the user’s mental state. For example, we could take into account previous interactions with the
system (either actual or inferred [7]), whether the user is a beginner or an expert, and what other
programming languages the user might be familiar with [42].
3.2 Applications
3.2.1 Explanation as misinterpreter synthesis
We can now pose our main synthesis task: given program p, we seek the set M that maximizes
P(M) subject to p ̸= p . We pose this as an optimization query to Z3, which easily finds
ΣM Σ
(cid:74) (cid:75) (cid:74) (cid:75)
5A B
Figure2: WatChat’suserinterface. TheuserentersanexpressioninaREPL,whichoffersa“WAT?”
button alongside the output. When clicked, WatChat clarifies what the user expected, and offers an
explanation. Panel “B” shows a case where multiple possible explanations were found. (The dialogs
are closed by default and only open when clicked.)
a solution set M within a few milliseconds. The last step is to convert a solution set M into a
textual explanation. To do this, we instrument the interpreter and trace the execution of p. Then,
we selectively report execution steps that would be affected by misconceptions in M. Finally, we
print a (hand-written) clarification for each misconception that was encountered.
As an example, consider the expression . When the user types this
[9, 9, 8, 7].sort()[1]
into WatChat, they see that it returns . Additionally, a button labeled “WAT?” appears on the
8
side. If the user did not expect , they can click the button to reveal an explanation. When the
8
buttonisclicked, weposeaquerytoZ3andreceivethesolutionsetM = {0-indexed}. Oursystem
asks, “Did you expect ?” and if the user agrees, then it explains that “JavaScript is 0-indexed, not
7
1-indexed. Hence, gives .” This is shown in panel “A” of Figure 2.
[9, 9, 8, 7].sort()[1] 8
Thatwasarelativelystraightforwardexplanation. Nowconsideraslightmodificationtothisex-
pression, ,whichinsteadreturns . Iftheuserclicks“WAT?” WatChat
[10, 9, 8, 7].sort()[0] 10
again asks, “Did you expect ?” However, this time, the explanation is different: WatChat explains,
7
“ casts elements (including numbers) to string and compares them using
Array.prototype.sort()
the semantics of the operator. If neither operand is a number, then comparison operators like
< <
attempt to compare string representations of the operands lexicographically. Hence,
[10, 9, 8,
gives . Hence, gives .”
7].sort() [10, 7, 8, 9] [10, 9, 8, 7].sort()[0] 10
Notice that these programs are structurally identical: a complete trace of the interpreter
would show the same pattern of behavior on both programs, differingonlyintheparticular
numerical values being sorted and indexed. WatChat, however, gives very different explanations
of the behavior of the two programs. The first explanation addresses 0-indexing, whereas the
second explanation addresses the peculiarities of sorting. Unlike the first explanation, the second
explanation shows the intermediate sorted list (because the asker likely expects the wrong sorted
list). However, it does not address 0-indexing (because there is no evidence that the asker has a
misconception about indexing). These examples show how our system is selective (only surfacing
relevant information) and social (designed to reason about the asker’s mental state).
WatChat is robust to situations where multiple possible explanations exist. To check if there
are alternate explanations, we add an additional distinctness constraint to the solver and query
again. Given an existing solution M , we add the constraint that p ̸= p ∨M ̸⊆ M . In
1
(cid:74)
(cid:75)ΣM1
(cid:74)
(cid:75)ΣM 1
other words, the new solution must either produce a fresh expected output, or give a nontrivially
different new misconception set for the same expected output (i.e. not a superset). When there are
6multiple competing explanations, we can ask the user a clarification question (“Did you expect ...?”)
to disambiguate between them. This conversational structure is a hallmark of human explanation
interactions.
As an example, consider the program , which outputs . If a user
[10, 9, 8, 7].sort()[1] 7
findsthissurprising,thentheycouldhaveeitherofthetwomisconceptionsreferencedabove.2 Thus,
forthisprogram,WatChatasksaclarificationquestiontodeterminethemisconceptiontheuserhas:
“Did you expect or ?” Depending on the user’s response, WatChat explains about 0-indexing
10 8
or lexicographic comparison, respectively. This is shown in Panel “B” of Figure 2.
HerearesomemoreexamplesofthekindsofexplanationsthatWatChatgives(chosenforvariety
in number of possible expectations and misconceptions):
Why does return ?
NaN == +null false
• casts to number as (not , as you might expect). Hence, gives . As a
null 0 NaN (+null) 0
special case, is never equal to anything (even itself). Hence,
NaN NaN (NaN == (+null))
gives .
false
(Notice how WatChat infers that the user mistakenly thinks that +null produces NaN, and
explicitly corrects that misconception.)
Why does return ?
[false, null] + [false] "false,false"
• I expected "[false,null][false]" → null is printed as empty string when Arrays are
cast to string. When converted to string, Arrays don’t have s around them. Hence,
[]
gives . Hence, gives
[false, null] "false," ([false, null] + [false]) "false,
.
false"
• I expected ... (other options elided for brevity)
Why does return ? (Example from the “Wat” talk [2].)
{} + [] "[object Object]"
• I expected "{}[]" → Objects cast to the string "[object Object]". Hence, {} gives
. When converted to string, arrays don’t have s around them.
"[object Object]" []
Hence, gives . Hence, gives .
[] "" ([] + {}) "[object Object]"
• I expected ... (other options elided for brevity)
Why does return ?
["null", typeof(null)]["1"] "object"
• I expected undefined →
JavaScript casts all indices to string. When indexing arrays and strings, it checks if the
indices represent numbers. Hence, gives .
["null", typeof(null)]["1"] "object"
• I expected "null" →
hastype (not , asyoumightexpect). Hence, gives
null "object" "null" typeof(null)
. Hence, gives . JavaScript
"object" ["null", typeof(null)] ["null", "object"]
2Notice that in this case, if the user had both misconceptions, then they would have expected 7 (albeit for the
wrong reasons) and not pressed the “WAT?” button in the first place.
7is 0-indexed, not 1-indexed. Hence, gives .
["null", typeof(null)]["1"] "object"
Notice how WatChat is able to disentangle two completely different sets of misconceptions that
could cause someone to ask “why?” about this program’s behavior.
3.2.2 Synthesizing diagnostic programs / concept inventories
Educatorsoftenusediagnosticquestionsonassessmentstopinpointstudents’misconceptions. Such
questions are carefully designed so that an incorrect response from a student reveals that they have
a particular misconception. Collections of such questions are called concept inventories [17].
Brainstorming new diagnostic questions for a concept inventory (e.g. when writing a new exam
each semester) can be a challenging, time-consuming process [27]. Furthermore, there is always the
risk that a diagnostic question is imprecise because multiple misconceptions could lead to the same
mistake. Thus, high-quality concept inventories are often published as research contributions unto
themselves [10].
The infrastructure behind WatChat can be used to synthesize fresh diagnostic programs that
are guaranteed to pinpoint a given misconception. Given a specific misconception m, we search
for a program p such that ∀M, p = p ⇐⇒ m ∈ M . In other words, if asked what p
Σ
{m}
ΣM
(cid:74) (cid:75) (cid:74) (cid:75)
outputs, the student should incorrectly respond r = p if and only if they have misconception
(cid:101) Σ {m}
m. (cid:74) (cid:75)
As an example, consider the misconception m = empty-array-is-falsey (in JavaScript,
emptyarrayisactuallytruthy). Anaïvediagnosticprogramtodetectmmightbep =([] || true
). Under standard ECMAScript semantics Σ, this program returns
[]
due to the short-circuiting
behavior of the || operator. On the other hand, if a user has misconception m and thinks [] is
falsey, then under Σ they would expect true. Hence, p appears to be a reasonable diagnostic:
{m}
based on their response, we can tell if they have misconception m.
However, this is not quite right. Consider a student who has a different misconception m′ =
, under which always casts its output to a boolean value. This student would
||-casts-bool ||
also say p outputs true, but for a different reason unrelated to the target misconception m. Hence,
p cannot distinguish between misconceptions m and m′, making it a poor diagnostic.
Our system might instead return a program like p = ([] ? [] : "abc"), and furthermore
offer a guarantee that the response cannot be caused by any other misconceptions (or com-
"abc"
binations of misconceptions) known to the system. A list of sample diagnostic programs for each of
the misconceptions currently known to WatChat—that is, an automatically-generated concept
inventory for JavaScript type coercion—is given in Tables 2 and 3.
Note that this framework allows for many natural variations: using standard program synthesis
techniques, we can search for multiple unique diagnostic programs p , minimal diagnostic programs,
i
single diagnostic programs that isolate multiple misconceptions (e.g. “power questions” [40]), and
so on with various small modifications to the solver query.
3.2.3 Verifying diagnostic programs
Similarly, we can use our system to check if a candidate diagnostic program really does precisely
expose a given misconception. We ask the solver for an M that violates the earlier condition, i.e.
an M such that p = p ⇍ ⇒ m ∈ M . If such an M is found by the solver, it can expose
Σ
{m}
ΣM
(cid:74) (cid:75) (cid:74) (cid:75)
one of two issues with p:
1. A set of misconceptions containing m produces a different expected output from r. In this
(cid:101)
8casewegetafalsenegative: astudentmighthavemisconceptionmbutnotrespondr because
(cid:101)
some other misconception “masks” m.
2. A set of misconceptions not containing m nevertheless does produce r. In this case we get a
(cid:101)
false positive: a student who does not have misconception m might still respond r because of
(cid:101)
some other misconception.
Continuing our example from above: if we tell our system that m is empty-array-is-falsy and
p =([] || true), thenthesolverpromptlyreturnsM = {||-casts-bool}asacounterexample.
Asexplainedabove,astudentwiththismisconceptionwouldindeedexpectptoproduce true. This
shows that the program p could produce false positives, and is thus a poor diagnostic for m.
4 Limitations, discussion, and future work
There are many forms of unexpected behavior our system currently cannot handle. We of course
cannot explain unexpected behavior that falls outside the subset of ECMAScript we currently
support. Additionally, because we currently only model misconceptions in semantics, we cannot yet
explain unexpected behavior caused by misconceptions about syntax (e.g. operator precedence,
automatic semicolon insertion).
Our method is currently only applicable to behavior that is unexpected because of miscon-
ceptions about the language. But not all unexpected behavior is due to such misconceptions.
For example, a good explanation for why fails with an error
document.getElementByID("xyz")
(“TypeError: undefined is not a function”) is that the user has a misconception about the method
name, whichisactuallycalled (lowercase“d”). Butagoodexplanation
document.getElementById
forwhy failsisthattheusermadeatypingerrorandmisspelled
document.getElemntById("xyz")
the word “Element.” This explanation is not based on a misconception about the language, but
rather a false belief about the actual program being executed. Thus, even though these
programs nominally both fail due to misspellings, their failures are best explained at very different
levels.
To handle such cases in WatChat, we would ultimately like to jointly infer not only the user’s
misconceptions and expected output, but also the intended program they meant to write. This
requires a well-tuned prior distribution over programs that are natural to write. We believe
modern large language models maybe be well-suited for this task.
More broadly, more work is needed to understand how best to use language models to produce
helpful explanations. In early informal explorations, we found that as of fall 2023, OpenAI’s GPT-4
producesreasonableexplanationsforsimpleJavaScriptexpressions. However,formoresophisticated
cases the model typically gives comprehensive step-by-step accounts of the expression’s evaluation:
the explanations are not contrastive, selective, or social. Furthermore, we observed it occasionally
produce false or irrelevant explanations, and we found that it produces convincing but incorrect
“explanations” of false statements like “Why does output in JavaScript?” (that
""+[null] "null"
programactuallyoutputs ). Moreworkisneededtosystematicallyevaluatelargelanguagemodels
""
on explanation tasks, and to design neurosymbolic systems that combine the flexibility of
large language models with the guarantees of formal methods.
Finally, we are interested in extending this method to explain other forms of surprising program
behavior. We are particularly interested in explaining performance regressions by debugging
users’ mental models about their hardware’s performance characteristics. For example, if a user
swaps the order of two nested loops in a matrix multiplication routine and observes a surprising
slowdown, we would like to automatically explain to them what happened (e.g. by explaining that
9cache hits are important for performance, and that the new program has worse locality). Other
interesting domains include page layout (e.g. surprising results when positioning figures in CSS or
LATEX) and version control (e.g. surprising results when trying to perform operations with git).
5 Conclusion
Inthispaper,wedescribedanewsystem,WatChat,thatexplainsunexpectedbehaviorinJavaScript
programs. Building on empirical cognitive science work on explanations, we design a system that
chooses the best explanation to give by inferring misconceptions that would cause the user to
ask “why?” (or exclaim “WAT!”) in response to the given program’s output. We implement this
system by applying standard program synthesis techniques “in reverse,” to infer “misinterpreters”
that represent flawed mental models of JavaScript semantics. Finally, we show how our framework
can also be used to automatically generate and validate pedagogical diagnostic programs.
Taking a step back: in this paper, we imagine a future where our programming environments
act not only as tools, but as active collaborators. WatChat begins to demonstrate how studying
the cognitive science behind cooperative social interactions (e.g. explanations) can help us design
thoughtful, human-like interlocutors from first principles.
6 Acknowledgements
We thank Sorawee Porncharoenwase for help using Rosette, Shriram Krishnamurthi and Kuang-
Chen Lu for a thought-provoking discussion about “misinterpreters,” our PLATEAU mentor Will
Crichton for careful feedback on this paper, and the attendees of the PLATEAU 2024 workshop for
a series of lovely conversations about these early-stage ideas.
References
[1] Titus Barik et al. “Do developers read compiler error messages?” In: 2017 IEEE/ACM 39th
International Conference on Software Engineering (ICSE). IEEE. 2017, pp. 575–585.
[2] Gary Bernhardt. CodeMash. 2012. url:
https://www.destroyallsoftware.com/talks/
.
wat
[3] RastislavBodiketal.“Domain-specificsymboliccompilation”.In:2nd Summit on Advances in
Programming Languages (SNAPL 2017). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.
2017.
[4] Tathagata Chakraborti et al. “Plan explanations as model reconciliation: moving beyond ex-
planationassoliloquy”.In:Proceedings of the 26th International Joint Conference on Artificial
Intelligence. 2017, pp. 156–163.
[5] Kartik Chandra and Rastislav Bodik. “Bonsai: synthesis-based reasoning for type systems”.
In: Proceedings of the ACM on Programming Languages 2.POPL (2017), pp. 1–34.
[6] Kartik Chandra et al. “Cooperative Explanation as Rational Communication”. In: In submis-
sion. 2024.
[7] Kartik Chandra et al. “Inferring the future by imagining the past”. In: Advances in Neural
Information Processing Systems 36 (2024).
[8] Compiler Errors for Humans. 2015. url: https://elm-lang.org/news/compiler-
.
errors-for-humans
10[9] Kenneth James Williams Craik. The nature of explanation. Vol. 445. CUP Archive, 1967.
[10] Will Crichton, Gavin Gray, and Shriram Krishnamurthi. “A Grounded Conceptual Model for
Ownership Types in Rust”. In: Proceedings of the ACM on Programming Languages 7.OOP-
SLA2 (2023), pp. 1224–1252.
[11] Leonardo De Moura and Nikolaj Bjørner. “Z3: An efficient SMT solver”. In: International
conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer.
2008, pp. 337–340.
[12] Dawson Engler et al. “Bugs as deviant behavior: A general approach to inferring errors in
systems code”. In: ACM SIGOPS Operating Systems Review 35.5 (2001), pp. 57–72.
[13] Tobias Gerstenberg and Thomas Icard. “Expectations affect physical causation judgments.”
In: Journal of Experimental Psychology: General 149.3 (2020), p. 599.
[14] Herbert Paul Grice. “Logic and conversation”. In: Speech acts. Brill, 1975, pp. 41–58.
[15] Arjun Guha, Claudiu Saftoiu, and Shriram Krishnamurthi. “The essence of JavaScript”. In:
ECOOP 2010–Object-Oriented Programming: 24th European Conference, Maribor, Slovenia,
June 21-25, 2010. Proceedings 24. Springer. 2010, pp. 126–150.
[16] Shu-yu Guo, Michael Ficarra, and Kevin Gibbons, eds. ECMAScript(R) 2024 Language Spec-
ification. 2023. url: https://tc39.es/ecma262/.
[17] David Hestenes, Malcolm Wells, and Gregg Swackhamer. “Force concept inventory”. In: The
physics teacher 30.3 (1992), pp. 141–158.
[18] Denis J Hilton. “Conversational processes and causal explanation.” In: Psychological Bulletin
107.1 (1990), p. 65.
[19] Denis J Hilton. “Mental models and causal explanation: Judgements of probable cause and
explanatory relevance”. In: Thinking & Reasoning 2.4 (1996), pp. 273–308.
[20] PhilipNJohnson-Laird.“Mentalmodelsincognitivescience”.In:Cognitive science 4.1(1980),
pp. 71–115.
[21] John R Josephson and Susan G Josephson. Abductive inference: Computation, philosophy,
technology. Cambridge University Press, 1996.
[22] Lara Kirfel, Thomas Icard, and Tobias Gerstenberg. “Inference from explanation.” In: Journal
of Experimental Psychology: General 151.7 (2022), p. 1481.
[23] Amy J Ko and Brad A Myers. “Debugging reinvented: asking and answering why and why
not questions about program behavior”. In: Proceedings of the 30th international conference
on Software engineering. 2008, pp. 301–310.
[24] Amy J Ko and Brad A Myers. “Designing the whyline: a debugging interface for asking
questions about program behavior”. In: Proceedings of the SIGCHI conference on Human
factors in computing systems. 2004, pp. 151–158.
[25] Amy J Ko and Brad A Myers. “Extracting and answering why and why not questions about
Java program output”. In: ACM Transactions on Software Engineering and Methodology
(TOSEM) 20.2 (2010), pp. 1–36.
[26] Amy J Ko and Brad A Myers. “Finding causes of program output with the Java Whyline”.
In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2009,
pp. 1569–1578.
11[27] Rebecca S Lindell, Elizabeth Peak, and Thomas M Foster. “Are they all created equal? A
comparison of different concept inventory development methodologies”. In: AIP conference
proceedings. Vol. 883. 1. American Institute of Physics. 2007, pp. 14–17.
[28] Peter Lipton. “Contrastive explanation”. In: Royal Institute of Philosophy Supplements 27
(1990), pp. 247–266.
[29] TaniaLombrozo.“Thestructureandfunctionofexplanations”.In:Trends in cognitive sciences
10.10 (2006), pp. 464–470.
[30] Kuang-Chen Lu and Shriram Krishnamurthi. “Identifying and Correcting Programming Lan-
guage Behavior Misconceptions”. In: OOPSLA. 2024.
[31] John McClure. “Goal-based explanations of actions and outcomes”. In: European review of
social psychology 12.1 (2002), pp. 201–235.
[32] Tim Miller. “Explanation in artificial intelligence: Insights from the social sciences”. In: Arti-
ficial intelligence 267 (2019), pp. 1–38.
[33] Mitchell J Nathan, Kenneth R Koedinger, Martha W Alibali, et al. “Expert blind spot: When
content knowledge eclipses pedagogical content knowledge”. In: Proceedings of the third inter-
national conference on cognitive science. Vol. 644648. 2001, pp. 644–648.
[34] Tim Nelson et al. “The power of “why” and “why not”: Enriching scenario exploration with
provenance”. In: Proceedings of the 2017 11th Joint Meeting on Foundations of Software En-
gineering. 2017, pp. 106–116.
[35] Jihyeok Park et al. “JISET: JavaScript IR-based semantics extraction toolchain”. In: Proceed-
ings of the 35th IEEE/ACM International Conference on Automated Software Engineering.
2020, pp. 647–658.
[36] Falco Peijnenburg. “Type Directives in Elm”. MA thesis. 2016.
[37] Gabriel Poesia Reis e Silva and Noah Goodman. “Left to the Reader: Abstracting Solutions
in Mathematical Reasoning”. In: 44.44 (2022). url:
https://escholarship.org/uc/item/
.
0j8753pd
[38] Anna N Rafferty, Michelle M LaMar, and Thomas L Griffiths. “Inferring learners’ knowledge
from their actions”. In: Cognitive Science 39.3 (2015), pp. 584–618.
[39] AnnaNRaffertyetal.“Fasterteachingviapomdpplanning”.In:Cognitivescience 40.6(2016),
pp. 1290–1332.
[40] Stuart Reges. “The mystery of" b:=(b= false)"”. In: ACM SIGCSE Bulletin 40.1 (2008),
pp. 21–25.
[41] Maria Riveiro and Serge Thill. ““That’s (not) the output I expected!” On the role of end user
expectations in creating explanations of AI systems”. In: Artificial Intelligence 298 (2021),
p. 103507.
[42] Nischal Shrestha, Titus Barik, and Chris Parnin. “It’s like python but: Towards supporting
transferofprogramminglanguageknowledge”.In:2018IEEESymposiumonVisualLanguages
and Human-Centric Computing (VL/HCC). IEEE. 2018, pp. 177–185.
[43] Rishabh Singh, Sumit Gulwani, and Armando Solar-Lezama. “Automated feedback genera-
tion for introductory programming assignments”. In: Proceedings of the 34th ACM SIGPLAN
conference on Programming language design and implementation. 2013, pp. 15–26.
[44] Emina Torlak and Rastislav Bodik. “A lightweight symbolic virtual machine for solver-aided
host languages”. In: ACM SIGPLAN Notices 49.6 (2014), pp. 530–541.
12[45] Emina Torlak and Rastislav Bodik. “Growing solver-aided languages with Rosette”. In: Pro-
ceedings of the 2013 ACM international symposium on New ideas, new paradigms, and reflec-
tions on programming & software. 2013, pp. 135–152.
[46] Kurt VanLehn. Mind bugs: The origins of procedural misconceptions. MIT press, 1990.
[47] Ventero.Answerto“WhatistheexplanationforthesebizarreJavaScriptbehavioursmentioned
in the ‘Wat’ talk for CodeMash 2012?” url: https://stackoverflow.com/a/9033306.
13# Correction, i.e. a “patch” from Σ(cid:101) to Σ
1 has type (not , as you might expect).
null "object" "null"
2 Arrays have type (not , as you might expect).
"object" "array"
3 As a special case, is never equal to anything (even itself).
NaN NaN
4 Empty objects ( and ) are truthy.
{} []
5 undefined is printed as empty string when arrays are cast to string.
6 is printed as empty string when arrays are cast to string.
null
7 prints as the string (not , as you might expect).
NaN "NaN" ""
8 casts to the string , not the empty string.
null "null"
9 undefined casts to the string (not , as you might expect).
"undefined" ""
10 is (not 0) when cast to number.
{} NaN
11 JavaScript is 0-indexed, not 1-indexed.
12 undefined casts to number as (not 0, as you might expect).
NaN
13 casts to number as 0 (not , as you might expect).
null NaN
14 Array.prototype.sort() casts elements (including numbers) to string and com-
pares them lexicographically.
15 ?? does not treat as null-ish.
false
16 ?? does not treat as null-ish.
NaN
17 The operator does not concatenate arrays; instead, it casts them to strings.
+
18 Short-circuiting boolean operators like && and || return the determining
operand (rather than a boolean value).
19 The operator, unlike the operator, attempts a series of type coercions
== ===
that can cause unexpected results.
20 Whengivenoperandsthatareneithernumbersnorstrings,the operatortries
+
to cast them to numbers (if possible) or else strings.
21 Objects cast to the string .
"[object Object]"
22 When converted to string, arrays don’t have the square brackets around them.
23 The empty string by definition casts to 0 (not , as you might expect).
NaN
24 The operator only attempts to add if both sides are numbers. Otherwise, it
+
casts its operands to string and concatenates them.
25 When subscripted, primitive booleans and numbers are implicitly converted to
Boolean and Number objects.
26 When one side of an is a boolean, JS does not attempt to convert the other
==
side to a boolean as well. Instead, the boolean is converted to a number (0 or
1) and the comparison is tried again.
27 The operator is defined as the negation of , rather than the disjunction of
>= <
> and .
==
28 If neither operand is a number, then comparison operators like attempt to
<
compare string representations of the operands lexicographically.
29 The characters and sort after capital letters but before lowercase
"[" "]"
letters.
30 The comma character ( ) sorts before all letters, numbers, and delimiters.
","
31 and compare objects and arrays by reference, not by value.
== ===
32 JavaScript casts all indices to string. When indexing arrays and strings, it
checks if the indices represent numbers.
Table 1: A list of misconceptions currently known to our system.
14# Time Synthesized diagnostic program True output / distractor(s)
1 2 min [typeof(null), {}[[]]] ["object", undefined]
["null", undefined]
2 2 min (typeof([])?? (false ? false : false)) "object"
"array"
3 8 min (NaN === ({} - true)) false
true
4 2 min ((+undefined)? {} : ({} ? null : true)) null
true
5 4 min ([undefined, undefined] + "") ","
"undefined,undefined"
"[,]"
"[undefined,undefined]"
6 5 min ("10"+ [null, []]) "10,"
"10null,"
"10[,[]]"
"10[null,[]]"
7 10 min (NaN + "10") "NaN10"
"10"
NaN
8 3 min ("10"+ null) "10null"
"10"
9 3 min ((false ? undefined : undefined)+ "") "undefined"
""
10 2 min [NaN, (+{})] [NaN, NaN]
[NaN, 0]
11 6 min [false, true][1] true
false
12 3 min [({} ?? {}), (+undefined)] [{}, NaN]
[{}, 0]
13 2 min [false, (false - null)] [false, 0]
[false, NaN]
14 29 min [10, 2].sort() [2, 10]
[10, 2]
15 2 min ((false ?? true)== false) true
false
16 3 min (({} - undefined)?? (!true)) NaN
false
Table 2: A concept inventory (diagnostic “quiz”) automatically synthesized by our system. For each
misconception known to WatChat, we synthesize a multiple-choice question where the first choice
is correct, and the second choice would be chosen if and only if the student has the respective
misconception. In some cases, our system can generate other distractors as well. (Misconceptions
numbered as in Table 1. Continued below in Table 3.)
15# Time Synthesized diagnostic program True output / distractor(s)
17 2 min [(!true), ([] + [])] [false, ""]
[false, []]
(error)
[false, "[][]"]
18 2 min ((+true)|| NaN) 1
true
19 3 min [(undefined == null), (+true)] [true, 1]
[false, 1]
20 1 min ((!true)? ({} ? {} : false): (undefined + null)) NaN
(error)
0
21 6 min ("0"+ {}) "0[object Object]"
"0{}"
22 7 min ("">= []) true
false
23 2 min (""- "") 0
NaN
24 3 min ([undefined, {}] + ([] - true)) ",[object Object]-1"
NaN
",[object Object]NaN"
"[,[object Object]]NaN"
",[object Object]"
",{}-1"
"undefined,[object Object]-1"
25 2 min (false[[]] === {}) false
(error)
26 5 min ((undefined == false)|| (undefined == {})) false
true
27 3 min (",">= (+false)) true
false
28 4 min ("2">= "10") true
false
29 33 min ("["< typeof(true)) true
false
30 19 min (",">= [{}, []]) false
true
31 2 min (""|| ([] == [])) false
true
32 3 min [[], {}]["1"] {}
undefined
[]
Table 3: A concept inventory (diagnostic “quiz”) automatically synthesized by our system. For each
misconception known to WatChat, we synthesize a multiple-choice question where the first choice
is correct, and the second choice would be chosen if and only if the student has the respective
misconception. In some cases, our system can generate other distractors as well. (Misconceptions
numbered as in Table 1. Continued from above in Table 2.)
16