PublishedasaconferencepaperatICLR2024
POLY-VIEW CONTRASTIVE LEARNING
AmitisShidani∗ DevonHjelm,JasonRamapuram,RussWebb,
DepartmentofStatistics EeshanGuneshDhekane,andDanBusbridge
UniversityofOxford,UK Apple
shidani@stats.ox.ac.uk dbusbridge@apple.com
ABSTRACT
Contrastivelearningtypicallymatchespairsofrelatedviewsamonganumberof
unrelatednegativeviews. Viewscanbegenerated(e.g. byaugmentations)orbe
observed. We investigate matching when there are more than two related views
whichwecallpoly-viewtasks,andderivenewrepresentationlearningobjectives
using information maximization and sufficient statistics. We show that with un-
limitedcomputation,oneshouldmaximizethenumberofrelatedviews,andwith
afixedcomputebudget,itisbeneficialtodecreasethenumberofuniquesamples
whilstincreasingthenumberofviewsofthosesamples. Inparticular,poly-view
contrastive models trained for 128 epochs with batch size 256 outperform Sim-
CLRtrainedfor1024epochsatbatchsize4096onImageNet1k,challengingthe
beliefthatcontrastivemodelsrequirelargebatchsizesandmanytrainingepochs.
1 INTRODUCTION
Self-SupervisedLearning(SSL)trainsmodelstosolvetasksdesignedtakeadvantageofthestructure
and relationships within unlabeled data (Bengio et al., 2013; Balestriero et al., 2023; Logeswaran
& Lee, 2018; Baevski et al., 2020; Grill et al., 2020). Contrastive learning is one form of SSL
thatlearnsrepresentationsbymaximizingthesimilaritybetweenconditionallysampledviewsofa
singledatainstance(positives)andminimizingthesimilaritybetweenindependentlysampledviews
ofotherdatainstances(negatives)(Qi&Su,2017;vandenOordetal.,2018;Bachmanetal.,2019;
Hénaffetal.,2019;Heetal.,2019;Tianetal.,2020a;b;Chenetal.,2020a).
OneprinciplebehindcontrastivelearningisMutualInformation(MI)maximization(vandenOord
etal.,2018;Hjelmetal.,2019). Manyworkshaveelucidatedtherelationshipbetweencontrastive
learningandinformationtheory(Pooleetal.,2019;Tschannenetal.,2020;Leeetal.,2023;Gálvez
etal.,2023).However,MImaximizationisonlypartofthestory(Tschannenetal.,2020);successful
contrastivealgorithmsrelyonnegativesampling(Wang&Isola,2020;Robinsonetal.,2021;Song
et al., 2016; Sohn, 2016) and data augmentation (Bachman et al., 2019; Tian et al., 2020b; Chen
etal.,2020a;Fortetal.,2021;Balestrieroetal.,2022b;a)toachievestrongperformance.
Whileitispossibletodesigntasksthatdrawanynumberofviews,contrastiveworkstypicallysolve
pairwisetasks,i.e. theymaximizethesimilarityofexactlytwoviews,orpositivepairs(Balestriero
etal.,2023;Tianetal.,2020a). Theeffectofmoreviews,orincreasedviewmultiplicity(Bachman
et al., 2019), was investigated in SSL (van den Oord et al., 2018; Hjelm et al., 2019; Tian et al.,
2020a;Caronetal.,2020). However,theseworksoptimizealinearcombinationofpairwisetasks;
increasingviewmultiplicitymainlyimprovesthegradientsignaltonoiseratioofanequivalentlower
viewmultiplicitytask,aswasobservedinsupervisedlearning(Hofferetal.,2019;Fortetal.,2021).
In this work, we investigate increasing view multiplicity in contrastive learning and the design of
SSLtasksthatusemanyviews.Wecallthesetaskspoly-viewtodistinguishthemfrommulti-view,as
multiusuallymeansexactlytwo(Tianetal.,2020a;Balestrieroetal.,2023).Inadditiontoimproved
signaltonoise(Hofferetal.,2019;Fortetal.,2021),poly-viewtasksallowamodeltoaccessmany
relatedviewsatonce,increasingthetotalinformationabouttheproblem.Weshowtheoreticallyand
empiricallythatthishasapositiveimpactonlearning. Wemakethefollowingcontributions:
1. Wegeneralizetheinformation-theoreticfoundationofexistingcontrastivetaskstopoly-view
(Section2.3),resultinginanewfamilyofrepresentationlearningalgorithms.
∗WorkdoneduringaninternshipatApple.ForadetailedbreakdownofauthorcontributionsseeAppendixI.
1
4202
raM
8
]GL.sc[
1v09450.3042:viXraPublishedasaconferencepaperatICLR2024
2. Weusetheframeworkofsufficientstatisticstoprovideanadditionalperspectiveoncontrastive
representation learning in the presence of multiple views, and show that in the case of two
views, this reduces to the well-known SimCLR loss, providing a new interpretation of con-
trastivelearning(Section2.4)andanothernewfamilyofrepresentationlearningobjectives.
3. Finally,wedemonstratepoly-viewcontrastivelearningisusefulforimagerepresentationlearn-
ing. WeshowthathigherviewmultiplicityenablesanewcomputeParetofrontforcontrastive
learning,whereitisbeneficialtoreducethebatchsizeandincreasemultiplicity(Section3.2).
Thisfrontshowsthatpoly-viewcontrastivemodelstrainedfor128epochswithbatchsize256
outperformsSimCLRtrainedfor1024epochsatbatchsize4096onImageNet1k.
2 VIEW MULTIPLICITY IN CONTRASTIVE LEARNING
Weseektounderstandtheroleofviewmultiplicityincontrastivelearning(Definition2.1).
Definition2.1(ViewMultiplicity). TheviewmultiplicityM isthenumberofviewspersample. In
batchedsampling,drawingKsamplesresultsinV =M Kviewsperbatch.(Hofferetal.,2019).
×
Multiple data views may occur naturally as in CLIP (Radford et al., 2021) or, as is our primary
interest,besamplesfromanaugmentationpolicyasiscommoninSSL.Ourgoalistodeveloptasks
c
Multi-view Poly-view
Id
M = 2
M=2
SuffiM cient≥Sta2
tistics
ρ
SimCLR/InfoNCE
I(x;y) ≥ LInfoNCE I(x;S Yec )ti ≥on L2. S4 uffStats x h⋆ cˆ
M
M=2 = 2 η1 η2 ηM
M 2 Lower M 2
Multi-CropInf≥oNCEℓ(x,y) bounds GeneralizedM≥ISection2.3
...
I(x;y) ≥M1 (cid:80)M α=1ℓα(x,y) I(x;Y) ≥ LGenNWJ x1 x2 xM
Views
(a)Viewmultiplicityincontrastivelearning. (b)Viewmultiplicitygenerativeprocess.
Figure 1: (a) The role of multiplicity in contrastive learning. I(x;y) present the MI between two random
variablesxandy,whileI(x;Y)istheMIbetweenxandthesetofRandomVariable(RV)sY.L denotes
Method
thecontrastivelower-boundachievedbyeachmethod,ignoringtheconstants. Inthemulti-cropbox,ℓ α(x,y)
is the contrastive lower-bound produced by the α-th crop/view. (b) The multiple view sample generation
with generative factor c, where the main sample is generated through the generative process ρ, and views
aregeneratedthroughdifferentview-generationprocessesη αforα∈[M],e.g. augmentations. Thegoalisto
findthemaph⋆suchthatthereconstructedgenerativefactorcˆrecoversc,hencetheidentitymap.
thatcanusemultiplicityM. Westartbypresentingthegenerativeprocessunderlyingmultiplicity
(Section2.1).Wethenconsideroptimizingmanypairwisetasks(Section2.2),knownasMulti-Crop,
and show that Multi-Crop reduces the variance of the corresponding paired objective but cannot
improveboundsonquantitieslikeMI.Next,werevisittheinformationtheoreticoriginofInfoNCE,
andderivenewobjectivesthatsolvetasksacrossallviewsanddonotdecomposeintopairwisetasks
(Section2.3). Finally,astheframeworkofsufficientstatisticsisnaturalathighmultiplicity,weuse
ittoderivenewobjectiveswhichsolvetasksacrossallviews(Section2.4). Alloftheseobjectives
arerelated,asisshowninFigure1a. Beforeproceeding,weintroduceournotation.
Notation We denote vector and set of random variables (RVs) as x and X, with corresponding
densities p and p , and realizations x and X. Vector realizations x live in spaces denoted by
x X
. Theconditionaldistributionofygivenarealizationxisdenotedp . Theexpectationofa
y|x=x
sX calarfunctionf : RisE[f(x)]=E [f(x)]. Fora c b,X = x ,x ,...,x
X (cid:55)→
x∼px
≤ ≤
a:b
{
a a+1 b
}
representsasetofRVs,andX(̸=c)
=X x . ThedensityofX isthejointofitsconstituent
a:b a:b \{ c } a:b
RVs. MIbetweenxandy isdenoted (x;y)andisdefinedoverRVsetsas (X;Y). Wedenote
I I
the Shannon and differential entropy of x as H(x), and the Kullback-Leibler Divergence (KLD)
betweendensitiespandq by (p q). Finally,wewritetheintegerset 1,...,K as[K],and
KL
D ∥ { }
useLatinandGreekalphabettoindexsamplesandviewsrespectively.
2PublishedasaconferencepaperatICLR2024
2.1 GENERATIVEPROCESSANDINFOMAXFORVIEWMULTIPLICITY
WepresentthecausalgraphunderlyingM viewX = x ; α [M] generationinFigure1b.
1:M α
{ ∈ }
TheInfoMaxprinciple(Linsker,1988)proposestoreconstructanunknowncbyoptimizingh⋆ =
argmax (x,h(x)). To avoid trivial solutions, two-view contrastive methods (van den Oord
h∈HI
et al., 2018; Hjelm et al., 2019; Hénaff et al., 2019; Tian et al., 2020a) perform InfoMax through
aproxytaskthatinsteadmaximizesalowerboundontheMIbetweentwoviews (h(x );h(x )).
1 2
I
These methods rely on information about c being in the information shared between each pair of
views. A natural extension to two-view contrastive learning is to consider many views, where the
total amount of information about c is potentially larger. In Sections 2.2 to 2.4, we investigate
differentapproachestosolvingthisgeneralizedInfoMax,beginningwithMulti-Crop(Section2.2)
beforeconsideringmoregeneralMIapproaches(Section2.3)andsufficientstatistics(Section2.4).
2.2 LINEARCOMBINATIONSOFPAIR-WISETASKS
Thefirstapproachcombinesobjectivesonpairsx ,x fromthesetofM viewsX
α β 1:M
M M
1 (cid:88)(cid:88)
(X )= (x ,x ). (1)
LMulti-Crop 1:M M(M 1) LPair α β
− α=1β̸=α
The objective Equation 1 is the all-pairs formulation of Tian et al. (2020a), and corresponds to
Multi-Crop(Caronetal.,2020;2021)inthepresenceofM globalviews1. Forconvenience,wewill
refertotheobjectiveEquation1asMulti-Crop. Multi-CrophasbeenusednumeroustimesinSSL,
herewewillshowhowitachievesimprovedmodelperformancethroughitsconnectiontoInfoMax.
Proposition2.1. ForKindependentsamplesandmultiplicityM denotedX ,theMulti-Crop
1:K,1:M
ofany inEquation1hasthesameMIlowerboundasthecorresponding :
Pair Pair
L L
(x ;x ) log(K) E[ (X )]=log(K) E[ (X )], (2)
1 2 Multi-Crop 1:K,1:M Pair 1:K,1:2
I ≥ − L − L
wheretheexpectationisoverK independentsamples(seeAppendixC.1fortheproof).
Proposition 2.1 shows that increasing view multiplicity in Multi-Crop does not improve the MI
lower-boundcomparedtovanillaInfoNCEwithtwoviews. However,Multi-Cropdoesimprovethe
varianceoftheMIestimate(Proposition2.2).
Proposition2.2. ForK independentsamplesandmultiplicityM,M 3,denotedX ,the
1:K,1:M
≥
Multi-Cropofany inEquation1hasalowersamplevariancethanthecorresponding :
Pair Pair
L L
2(2M 1)
Var[ (X )] − Var[L (x ,x )]<Var[L (x ,x )], (3)
LMulti-Crop 1:M ≤ 3M(M 1) Pair 1 2 Pair 1 2
−
wherethevarianceisoverK independentsamples(seeAppendixC.2fortheproof).
Propositions2.1and2.2showthatbetterMulti-Cropperformancefollowsfromimprovedgradient
signal to noise ratio as in the supervised case (Fort et al., 2021) and supports the observations of
Balestrieroetal.(2022b). SeeAppendixDforfurtherdiscussionaboutMulti-Crop.
2.3 GENERALIZEDINFORMATIONMAXIMIZATIONASCONTRASTIVELEARNING
Inthissubsection,wedevelopourfirstobjectivesthatuseM viewsatonceanddonotdecompose
intoobjectivesoverpairsofviewsasinSection2.2.
2.3.1 GENERALIZEDMUTUALINFORMATIONBETWEENM VIEWS
AsInfoNCEoptimizesalowerboundonoftheMIbetweentwoviews(vandenOordetal.,2018;
Pooleetal.,2019),considertheOne-vs-RestMI(Definition2.2).
1TheoriginalMulti-Cropalsotakesamixtureofsmallerviewsandcomparesthemtolargerviews,resulting
inamorecomplicatedaugmentationpolicy. Asourworkisfocusedonstudyingtheeffectofmultiplicity,we
donotinvestigatetheextrabenefitsobtainablebyalsochangingtheaugmentationpolicy. Forinvestigations
intoaugmentations,seeTianetal.(2020b).
3PublishedasaconferencepaperatICLR2024
Definition 2.2 (One-vs-Rest MI). The One-vs-Rest MI for any α [M] given a set of M 2
∈ ≥
RandomVariables(RVs)X = x ; α [M] is
1:M α
{ ∈ }
(cid:16) (cid:17) (cid:16) (cid:17)
x ;X̸=α = p p p . (4)
I α 1:M DKL X1:M ∥ xα X̸= 1:α M
One-vs-RestMI(Definition2.2)alignswithgeneralizedInfoMax(Section2.1);thelargersetX̸=α
1:M
cancontainmoreinformationaboutthegenerativefactorc. Notethatduetothedataprocessingin-
equality (x ;X̸=α ) (x ;c),estimatingOne-vs-RestMIgivesusalower-boundonInfoMax.
I α 1:M ≤I α
Estimating One-vs-Rest MI Contrastive learning estimates a lower-bound to the MI using a
sample-based estimator, for example InfoNCE (van den Oord et al., 2018; Poole et al., 2019) and
(Hjelmetal.,2019;Nguyenetal.,2008). Theorem2.1generalizesthe lower-boundfor
NWJ NWJ
I I
theOne-vs-RestMI(seeAppendixC.3fortheproof).
Theorem2.1(Generalized ). ForanyM 2,α [M],asetofM randomvariablesX ,
NWJ 1:M
andforanypositivefunctionI
F(M) :
M−≥
1
R+∈
X ×X (cid:55)→
(cid:104) (cid:105) (cid:104) (cid:105)
I(x α;X̸= 1:α M) ≥E
pX1:M
F(M)(x α,X 1̸= :Mα) −E
pxαp X̸= 1:α
M
eF(M)(xα,X 1̸= :Mα) +1= IGenNWJ. (5)
Wecanusethe lowerbound(Theorem2.1)foranyfunctionF(M) : M−1 R+.
GenNWJ
I X ×X (cid:55)→
InordertoefficientlymaximizetheMI,wewanttheboundinEquation5tobeastightaspossible,
whichwecanmeasureusingtheMIGap(Definition2.3).
Definition2.3(MIGap). ForanyM 2,α [M],asetofM randomvariablesX ,andmap
1:M
≥ ∈
g(M) : M−1 R+oftheform
α
X ×X (cid:55)→
p p
g α(M)(x α,X 1̸= :Mα)= x pα X 1̸= :Mα eF(M)(xα,X 1̸= :Mα) , (6)
X1:M
(cid:16) (cid:17)
theMIGap X ;g(M) is
MI 1:M α
G
(cid:16) (cid:17) (cid:16) (cid:17) (cid:104) (cid:16) (cid:17) (cid:105)
X ;g(M) = x ;X̸=α =E g(M) log g(M) 1 , (7)
GMI 1:M α I α 1:M −IGenNWJ pX1:M α − α −
wherewehavewritteng(M) insteadofg(M)(x ,X̸=α)whentheargumentsareclear.
α α α 1:M
Themapg(M) inEquation6aggregatesoverM viewsandiscalledtheaggregationfunction.
α
2.3.2 PROPERTIESOFTHEAGGREGATIONFUNCTION
Thechoiceofg(M) isimportantasitdeterminestheMIGap(Definition2.3)atanymultiplicityM.
α
Aswewishtoemployg(M)
toobtainalowerboundonOne-vs-RestMI,itshouldbe
α
1. Interchangeable: (x ;X̸=α )= (X̸=α ;x ) = g(M)(x ,X̸=α)=g(M)(X̸=α,x ),
I α 1:M I 1:M α ⇒ α α 1:M α 1:M α
2. Reorderable: (x ;X̸=α )= [x ;Π(X̸=α )] = g(M)(x ,X̸=α)=g(M)[x ,Π(X̸=α)],
I α 1:M I α 1:M ⇒ α α 1:M α α 1:M
whereΠ( x ,...,x )= x ,...,x isapermutationoperator,and
{
1 N
} {
Π1 ΠN}
3. Expandable: g(M) canaccommodatedifferentsizedrest-setsX̸=α ,i.e.canexpandtoanyM.
α 1:M
We seek non-trivial lower bounds for the One-vs-Rest MI (Equation 5), and to minimize the MI
Gap(Equation7). TheDataProcessingInequality(DPI)gives (x ;X̸=α ) (x ;x )forall
x X̸=α . So, (x ;X̸=α ) (M 1)−1(cid:80) (x ;x )2,I proα vides1:M aba≥ selI inefα orthβ elower-
β ∈ 1:M I α 1:M ≥ − βI α β
boundforOne-vs-RestMI,leadingustointroducethefollowingrequirement:
4. Valid: The aggregation function
g(M)
should give a gap that is at most the gap given by the
α
meanofpairwisecomparisonswithg(2)
α
(cid:16) (cid:17) 1 (cid:88) (cid:16) (cid:17)
X ;g(M) x ,x ;g(2) . (8)
GMI 1:M α ≤ M 1 GMI { α β } α
− β̸=α
2WenotethattheobjectiveintroducedbyTianetal.(2020a)forthemulti-viewsettingisindeedtheaverage
lower-boundwepresenthere.
4PublishedasaconferencepaperatICLR2024
2.3.3 POLY-VIEWINFOMAXCONTRASTIVEOBJECTIVES
Wenowpresentthefirstpoly-viewobjectives,correspondingtochoicesofF(M)anditsaggregation
functiong(M) withthepropertiesoutlinedinSection2.3.2.ForanyfunctionF(2),defineF(M),and
α
theiraggregationfunctionscorrespondinglybyEquation6asfollowing:
 
Arithmeticaverage: F(M)(cid:16) x α,X 1̸= :Mα(cid:17) =log M1
1
(cid:88) eF(2)(xα,xβ) , (9)
− β̸=α
(cid:16) (cid:17) 1 (cid:88)
Geometricaverage: F(M) x ,X̸=α = F(2)(x ,x ). (10)
α 1:M M 1 α β
− β̸=α
BothfunctionssatisfythepropertiesinSection2.3.2(seeAppendixC.4forproof).
Toestablishaconnectiontocontrastivelosses,weintroducenotationforsamplingthecausalgraph
inFigure1b. Fromthejointdistributionp ,wedrawK independentsamplesdenotedby:
X1:M
(cid:110) (cid:111)K
X K = (x ,...,x ) K = x M =X i.e.X =x . (11)
{ i,1:M }i=1 { i,1 i,M }i=1 { i,α }α=1 i=1 1:K,1:M i,α i,α
EvaluatingthefunctionsinEquations9and10inTheorem2.1revealsthelowerboundonOne-vs-
RestMIandthePoly-viewContrastiveLosses(Theorem2.2,seeAppendixC.5fortheproof).
Theorem2.2(ArithmeticandGeometricPVClowerboundOne-vs-RestMI). ForanyK,M 2,
B =KM,α [M],anyscalarfunctionf : R,andmaph: ,wehave ≥
∈ C×C (cid:55)→ X (cid:55)→C
 
(cid:16) (cid:17) 1 (cid:88)K 1 (cid:88)
I
x α;X̸= 1:α
M
≥c(B,M)+E 
K
log
M 1
ℓ i,α,β ≡c(B,M) −LArithmeticPVC, (12)
i=1 − β̸=α
 
(cid:16) (cid:17) 1 (cid:88)K 1 (cid:88)
I
x α;X̸= 1:α
M
≥c(B,M)+E 
K M 1
log ℓ i,α,β ≡c(B,M) −LGeometricPVC, (13)
i=1 − β̸=α
wherec(B,M)=log(B M+1),theexpectationisoverK independentsamplesX ,and
1:K,1:M
−
ef(x(cid:101)i,α,x(cid:101)i,β)
ℓ (X )= , x =h(x ). (14)
i,α,β 1:K,1:M ef(x(cid:101)i,α,x(cid:101)i,β)+(cid:80) (cid:80)M ef(x(cid:101)j,γ,x(cid:101)i,β) (cid:101)i,α i,α
j̸=i γ=1
Wehavewrittenℓ insteadofℓ (X )wherethemeaningisclear.
i,α,β i,α,β 1:K,1:M
Maximizinglower-boundmeansmaximizingmaph,leadingtoh⋆ inFigure1b. InAppendixC.5,
weshowF(2)(X˜ ,x )=c(B,M)+log ℓ ,whereX˜ = X (cid:83) x .
i,α i,β i,α,β i,α { j,β }j̸=i,β { i,α }
TightnessofMIGap Validproperty(Equation8)ensuresthatthelower-boundforafixedM has
asmallerMIGapthantheaverageMIGapofthoseviews. Withoutlossofgenerality,takingα=1,
avalidsolutionguaranteesthattheMIGapforM >2issmallerthantheMIGapforM =2. The
(cid:16) (cid:17) (cid:16) (cid:17)
DPIimpliesthatforN M andfixedα, x ;X̸=α x ;X̸=α . Onewouldexpectthe
≥ I α 1:M ≤ I α 1:N
lower-boundtobealsoincreasing, whichindeedisthecase. Infact, wecanprovemore; consider
thattheMIGapismonotonicallynon-increasingwithrespecttoM3,i.e.theMIGapwouldeither
becometighterorstaythesameasM grows.WeshowthattheaggregationfunctionsbyEquations9
and10havethisproperty(Theorem2.3,seeAppendixC.6fortheproof).
Theorem2.3. Forfixedα, theMIGapofArithmeticandGeometricPVCaremonotonicallynon-
increasingwithM:
(X ;g(M2)) (X ;g(M1)) M M . (15)
GMI 1:M2 α ≤GMI 1:M1 α ∀ 1 ≤ 2
Recoveringexistingmethods ArithmeticandGeometricPVCoptimizeOne-vs-RestMI.M =2
gives the two-view MI that SimCLR maximizes and the corresponding loss (see Appendix E.2).
Additionally,forachoiceofF(2),werecoverSigLIP(Zhaietal.,2023b),providinganinformation-
theoreticperspectiveforthatclassofmethods(seeAppendixE.3).
3Notethatthisguaranteesthatthelower-boundisincreasingwithrespecttoM.
5PublishedasaconferencepaperatICLR2024
2.4 FINDINGGENERALIZEDSUFFICIENTSTATISTICSASCONTRASTIVELEARNING
NowwedevelopoursecondobjectivesthatuseMviewsatonce.Usingaprobabilisticperspectiveof
thecausalgraph(Figure1b),weshowhowtorecoverthegenerativefactorswithsufficientstatistics
(Section 2.4.1). We then explain how sufficient statistics connects to InfoMax, and derive further
poly-viewcontrastivelosses(Section2.4.2). Finally,wewillseethattheapproachesofMIlower-
boundmaximizationofSection2.3,andsufficientstatisticsareconnected.
2.4.1 REPRESENTATIONSAREPOLY-VIEWSUFFICIENTSTATISTICS
Todevelopanintuitionfortheutilityofsufficientstatisticsforrepresentationlearning,webeginin
thesimplifiedsettingofaninvertiblegenerativeprocess, h = ρ−1, andalosslessviewgeneration
procedureη : (c;η (x)) = (c;x). Ifthefunctionspace islargeenough,then h such
α α
I I H ∃ ∈ H
thatcˆ=h(x)=c. UsingtheDPIforinvertiblefunctions,wehave
max (x;h(x))= (x;c)=max (h(x);c). (16)
h∈HI I h∈HI
Ifweleth⋆ =argmax (x;h(x)),thenh⋆(x)isasufficientstatisticofxwithrespecttoc(see
h∈HI
e.g. Cover&Thomas(2006)),andtheinformationmaximizationhereisrelatedtoInfoMax.
Ifweknewtheconditionaldistributionp ,findingthesufficientstatisticsT(x)ofxwithrespect
x|c
tocgivesT =h⋆. Ingeneral,wedonotknowp ,andgenerativeprocessesaretypicallylossy.
x|c
Therefore, to make progress and find h⋆ = argmax (x;h(x)) with sufficient statistics, we
h∈HI
needtoestimatep . Forthispurpose, weuseviewmultiplicity; weknowfromDPIthatalarger
x|c
set of views X may contain more information about c, i.e. (X ;c) (X ;c) for
1:M
I
1:M2
≥ I
1:M1
M M . OurassumptionsforfindingthesufficientstatisticsT (x)ofxwithrespecttoyare
2 1 y
≥
1. Thepoly-viewconditionalp isabetterestimateforp forlargerM,
xα|X̸= 1:α
M
xα|c
2. Allviewshavethesamegenerativefactor: T (x )=T (x ),
c α c β
The representations are given by a neural network and are therefore finite-dimensional. It means
that the generative factor is assumed to be finite-dimensional. Fisher-Darmois-Koopman-Pitman
theorem(Daum,1986)provesthattheconditionaldistributionsp andp areexponential
xα|X̸= 1:α
M
xα|c
families,i.e. forsomefunctionsr ,r ,T andreorderablefunction(Section2.3.2)Q:
1 2
(cid:16) (cid:17)
p =r (x )r (X̸=α )exp T (x ) Q(X̸=α ) , (17)
xα|X̸= 1:α M 1 α 2 1:M X̸= 1:α M α · 1:M
p =r⋆(x )r⋆(c)exp(T (x ) Q⋆(c)). (18)
xα|c 1 α 2 c α ·
The first assumption says that for any M, it is enough to find the sufficient statistics of x with
α
respecttoX̸=α asanestimateforT (x ). Sincetheestimationofthetrueconditionaldistribution
1:M c α
becomesmoreaccurateasM grows,
limsup T (x ) T (x ) 0, limsup Q⋆(c) Q(X̸=α ) 0. (19)
M→∞ ∥ c α − X̸= 1:α M α ∥→ M→∞ ∥ − 1:M ∥→
WeseethatsufficientstatisticsgivesusanewperspectiveonInfoMaxforrepresentationlearning:
representationsforxaresufficientstatisticsofxwithrespecttothegenerativefactorc,whichcan
beapproximatedbysufficientstatisticsofoneviewx withrespecttotheotherviewsX̸=α .
α 1:M
2.4.2 POLY-VIEWSUFFICIENTCONTRASTIVEOBJECTIVES
AsinSection2.3.3,webeginbyoutliningournotationforsamplesfromtheempiricaldistribution.
LetusassumethatwehavethefollowingdatasetofK independentM-tuples:
(cid:91)
= (x ,X̸=α ) (x ,X̸=α ) K . (20)
D { i,α i,1:M } { j,α j,1:M }j̸=i
FollowingSection2.4.1,thegoalistodistinguishbetweenconditionalsp andp
xi,α|X̸= i,1α
:M
xi,α|X̸= j,γ
1:M
foranyj =iandγ,i.e.classifyx correctly i [K],givingthefollowingprocedureforfinding
i,α
thesuffici̸ entstatisticsT⋆andQ⋆. ∀ ∈
p
T⋆,Q⋆ =argmax
xi,α|X̸= i,1α
:M =argmaxℓ˜ , (21)
T,Q p xi,α|X i̸= ,1α
:M
+(cid:80)K j̸=i(cid:80)M
γ=1p xi,α|X̸= j,γ
1:M
T,Q
i,α
6PublishedasaconferencepaperatICLR2024
leadingtothethesufficientstatisticscontrastiveloss(Equation22),
(cid:34) (cid:35)
= E
1 (cid:88)K 1 (cid:88)M
logℓ˜ , ℓ˜ =
eT iT ,αQi,α˜
, (22)
LSuffStats − K M i,α i,α eT iT ,αQi,α˜ +(cid:80)K (cid:80)M eT iT ,αQj,γ˜
i=1 α=1 j=1 γ=1
wherexTdenotesvectortransposition,T T(x ),andQ Q(X̸=α ).
i,α ≡ i,α i,α˜ ≡ i,1:M
DesigningQ AsQparameterizestheconditional(Equation17),itisreorderable. ChoicesforQ
includeDeepSets(Zaheeretal.,2017)andTransformers(Vaswanietal.,2017).RequiringM =2to
recoverSimCLR(Chenetal.,2020a)impliesQ(x)=T(x),soforsimplicity,werestrictourselves
topoolingoperatorsoverT. Finally,wewanttherepresentationspacetohavenospecialdirection,
whichtranslatestoorthogonalinvarianceoftheproductofT andQ
[OT(x )]T Q( OT(x ):β =α )=T(x )TQ( T(x ):β =α ), (23)
α β α β
{ ̸ } { ̸ }
i.e. QisequivariantQ( OT(x ):β =α )=OQ( T(x ):β =α )whichissatisfiedby
β β
{ ̸ } { ̸ }
M
1 (cid:88)
Q(X̸=α )=Q( T(x ):β =α )= T(x ) T(X̸=α ) T . (24)
1:M { β ̸ } M 1 β ≡ 1:M ≡ α˜
− β̸=α
WiththechoiceQ=T ,whenM =2, (Equation22)recoversSimCLR(seeAppendixE.2
α˜ SuffStats
L
for the detailed connection), and therefore lower bounds two-view MI. For general M,
SuffStats
L
lowerboundsOne-vs-RestMI(Theorem2.4).
Theorem2.4(SufficientStatisticslowerboundOne-vs-RestMI). ForanyK,M 2,B = KM,
≥
α [M],andthechoiceofQinEquation24,wehave(seeAppendixC.7fortheproof)
∈
(cid:34) (cid:35)
(cid:16) (cid:17) 1 (cid:88)K
x ;X̸=α c(B,M)+E logℓ˜ , (25)
I α 1:M ≥ K i,α
i=1
wherec(B,M)=log(B M +1),theexpectationisoverK independentsamplesX .
1:K,1:M
−
Theorem2.4completestheconnectionbetweenSufficientStatisticsandInfoMax(Section2.1). We
note that contrary to Average and Geometric PVC (Equations 9 and 10), the Sufficient Statistics
objectiveforM >2(Equation25)cannotbewrittenusingF(2)asafunctionbasis.
3 EXPERIMENTS
3.1 SYNTHETIC1DGAUSSIAN
Our first interests are to check our intuition and to validate how well each objective bounds the
One-vs-RestMIasdescribedinTheorems2.2and2.4. Webeginwitha1DGaussiansetting,which
forthegenerativegraph(Figure1b)correspondstoIndependentandIdenticallyDistributed(i.i.d.)
samplesc N(0,σ2)fori [K],ρisidentitymap,andviewsx N(c ,σ2)foreachα [M]
i ∼ 0 ∈ i,α ∼ i ∈
andi. OnecancomputeOne-vs-RestMIinclosedform(seeAppendixE.6fortheproof):
1 (cid:20)(cid:18) σ2(cid:19)(cid:18) σ2 (cid:19)(cid:21)
(x ;X̸=α )= log 1+ 0 1 0 , (26)
I i,α i,1:M 2 σ2 − σ2+Mσ2
0
which, as anticipated (Section 2.1), is an increasing function of M. Using the closed form for
Gaussiandifferentialentropy,wesee:
limsup (x ;X̸=α )=H(x ) H(x c)= (x ;c), (27)
I α 1:M α − α | I α
M→∞
i.e. One-vs-RestMIbecomesabetterproxyforInfoMaxasM increases. Finally,wecanevaluate
theconditionaldistributionforlargeM andsee(seeAppendixE.6fortheproof):
lim p =p , (28)
M→∞
xi,α|X̸= i,1α
:M
xi,α|ci
validatingourfirstassumptionforSufficientStatistics(Section2.4.1).
7PublishedasaconferencepaperatICLR2024
To empirically validate our claims we train a Multi-Layer Perceptron (MLP) with the architecture
(1->32, GeLU, 32->32)usingtheobjectivespresentedinSections2.2,2.3.3and2.4onthe
syntheticGaussiansetup. WeuseAdamW(Loshchilov&Hutter,2019)withlearningrate5 10−4
andweightdecay5 10−3,generateK =10241Dsamplesineachbatch,M viewsofeachs× ample,
×
andtraineachmethodfor200epochs.
WecompareOne-vs-Restlowerboundsofthesedifferentobjectivestothetruevalue(Equation26).
In Figure 2, we see that increasing multiplicity M decreases the MI Gap for Geometric, Arith-
metic and Sufficient, with Geometric having the lowest gap, whereas for Multi-Crop, the MI Gap
increases, validating Theorem 2.3 and Proposition 2.1. The Multi-Crop loss expectation is also
M-invariant,whereasitsvariancereduces,aswasproveninSection2.2.
0.3 1.0
7.5 One-vs-Rest
1.5 0.2 Geometric
0.9 7.0 Sufficient
1.4 0.1 Arithmetic
Multi-Crop
6.5
1.3 0.0 0.8
2 4 810 2 4 810 2 4 810 2 4 810
ViewMultiplicityM ViewMultiplicityM ViewMultiplicityM ViewMultiplicityM
Figure2: ComparingMIboundswithtrueMIintheGaussiansetting. Eachmethodistrainedfor200with
multiplicitiesM ∈ {2,4,8,10}. Lefttoright: 1)TrueOne-vs-RestMI(Equation26);2)MIGapsdecrease
asM growsforallmethodsexceptMulti-Cropduetothelog(K)factor;3)RelativeMI=TrueMI/Lower
BoundMI;and4)lossesforeachobjective. Bandsindicatethemeanandstandarddeviationacross16runs.
Pointsindicatefinalmodelperformanceofcorrespondinghyperparameters.
3.2 REAL-WORLDIMAGEREPRESENTATIONLEARNING
WeinvestigateimagerepresentationlearningonImageNet1k(Russakovskyetal.,2014)following
SimCLR(Chenetal.,2020a). FullexperimentaldetailsareinAppendixF.1, andpseudo-codefor
losscalculationsareinAppendixF.3.2. WeconsidertwosettingsasinFortetal.(2021):
1. GrowingBatch,wherewedrawviewsV =K M withmultiplicityM whilstpreservingthe
×
numberofuniquesamplesK inabatch.
2. Fixed Batch, where we hold the total number of views V = K M fixed by reducing the
×
numberofuniquesamplesK asweincreasethemultiplicityM.
We investigate these scenarios at multiplicity M = 8 for different training epochs in Figure 3a.
Weobservethat, givenanumberoftrainingepochsor modelupdates, oneshouldmaximizeview
multiplicityinbothFixedandGrowingBatchsettings,validatingtheclaimsofSections2.3and2.4.
To understand any practical benefits, we introduce Relative Compute4(Equation 29), which is the
totalamountofcomputeusedfortheruncomparedtoaSimCLRrunat128epochs,
M Epochs
RelativeCompute(M,Epochs)= . (29)
2 × 128
IntheGrowingBatchcase,thereareonlyminorgainswithrespecttothebatchsize4096SimCLR
baselinewhenmeasuringrelativecompute.IntheFixedBatchcase,weobserveanewParetofrontin
RelativeCompute. Betterperformancecanbeobtainedbyreducingthenumberofuniquesamples
while increasing view multiplicity when using Geometric PVC or Sufficient Statistics. Notably, a
batch size 256 Geometric PVC trained for 128 epochs outperforms a batch size 4096 SimCLR
trained for 1024 epochs. We also note that better performance is not achievable with Multi-Crop,
whichiscompute-equivalenttoSimCLR.
To further understand the role of multiplicity, we hold Epochs = 128 and vary multiplicity M in
Figure3b. Increasingmultiplicityisneverharmful,withGeometricPVC performingthestrongest
overall. WenotethatMulti-CropoutperformsSufficientStatisticsintheGrowingBatchsetting.
4Note that there is no dependence on the number of unique samples per batch K, as increasing K both
increasesthecomputerequiredperupdatestepanddecreasesthenumberofstepsperepoch.
8
IMtseR-sv-enO
paGIM
IMevitaleR
ssoLPublishedasaconferencepaperatICLR2024
GrowingBatch GrowingBatch
70
70
68
65
66
64 128 256 512 1024 0.0 0.1 0.2 0.3 1/2 1 2 4 8 16 3322
FixedBatch FixedBatch
72
70 70
SimCLR SimCLR
Geometric 68 Geometric
65 Sufficient Sufficient
Multi-Crop Multi-Crop
66
64 128 256 512 1024 0.0 0.5 1.0 1/2 1 2 4 8 16 3322 5 10 15
Epochs Updates 106 RelativeCompute ViewMultiplicityM
×
(a)TrainingatmultiplicityM =8varyingtrainingepochs. (b)Varyingmultiplicity.
Figure3:ContrastiveResNet50trainedonImageNet1kfordifferentepochsorwithdifferentviewmultiplicities.
Blue,red,orangeandblackdashedlinesrepresentGeometric,Multi-Crop,SufficientStatistics,andSimCLR
respectively.Bandsindicatethemeanandstandarddeviationacrossthreeruns.Pointsindicatefinalmodelper-
formanceofcorrespondinghyperparameters.WeuseK =4096forGrowingBatchandK =(2/M)×4096
forFixedBatch. (a)EachmethodistrainedwithamultiplicityM = 8excepttheM = 2SimCLRbaseline.
Wecomparemodelsintermsofperformanceagainsttrainingepochs(left),totalupdates(middle)whichisaf-
fectedbybatchsizeK,andrelativecompute(right)whichisdefinedinEquation29.SeeAppendixF.3.1fora
FLOPscomparison.b)Eachmethodistrainedfor128epochsforeachmultiplicityM ∈{2,3,4,6,8,12,16}.
4 RELATED WORK
WepresentworkrelatedtoviewmultiplicityhereandadditionalrelatedworkinAppendixG.
Viewmultiplicity Hofferetal.(2019)showedthatmultiplicityimprovesbothgeneralizationand
convergenceofneuralnetworks,helpingtheperformancescaling. Balestrieroetal.(2022b)showed
that more augmentations in two-view contrastive learning helps the estimation of the MI lower-
boundtohavesmallervarianceandbetterconvergence. Similarly,Tianetal.(2020a)studiedmulti-
plepositiveviewsincontrastivelearning,however,theirworkenhancesthelossvariancebyaverag-
ingovermultipletwo-viewlosses. WhilesimilartotheextensionwepresentinSection2.3.3,Tian
etal.(2020a)donotconsiderthemultiplicityeffectinnegatives,andthelog(K)factor,resultingto
justamoreaccuratelower-bound. Song&Ermon(2020),however,increasesthelog(K)factorby
includingpositivestosolveamulti-labelclassificationproblem. Inthesupervisedsetting,Fortetal.
(2021)studiedtheeffectofaugmentationmultiplicityinbothgrowingandfixedbatchsize,showing
thatthesignaltonoiseratioincreasesinbothcases,resultingtoabetterperformanceoverall.
5 CONCLUSION
In self-supervised learning, the multi in multi-view representation learning typically refers to two
viewsperuniquesample.Giventheinfluenceofpositives,andthenumberofnegativesincontrastive
learning,weinvestigatedtheroleofthenumberofpositives.
WeshowedthatMulti-Crop,apopularself-supervisedapproach,whichoptimizesacombinationof
pair-wisetasks,reducesthevarianceofestimators,butcannotchangeexpectationsor,equivalently,
bounds.TogobeyondMulti-Crop,weusedinformationtheoryandsufficientstatisticstoderivenew
familiesofrepresentationlearningmethodswhichwecallpoly-viewcontrastive.
We studied the properties of these poly-view contrastive methods algorithms, and find that it is
beneficialtodecreasethenumberofuniquesampleswhilstincreasingthenumberofviewsofthose
samples. In particular, poly-view contrastive models trained for 128 epochs with batch size 256
outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k, challenging the
beliefthatcontrastivemodelsrequirelargebatchsizesandmanytrainingepochs.
9
)%(1poTtseT
)%(1poTtseT
)%(1poTtseT
)%(1poTtseTPublishedasaconferencepaperatICLR2024
6 ACKNOWLEDGEMENTS
We thank Arno Blaas, Adam Golin´ski, Xavier Suau, Tatiana Likhomanenko, Skyler Seto, Barry
Theobald, Floris Weers, and Luca Zappella for their helpful feedback and critical discussions
throughout the process of writing this paper; Okan Akalin, Hassan Babaie, Brian Gamp, Denise
Hui, Mubarak Seyed Ibrahim, Li Li, Cindy Liu, Rajat Phull, Evan Samanas, Guillaume Seguin,
andthewiderAppleinfrastructureteamforassistancewithdevelopingscalable,faulttolerantcode.
Namesareinalphabeticalorderbylastnamewithingroup.
REFERENCES
Philip Bachman, R. Devon Hjelm, and William Buchwalter. Learning representations by max-
imizing mutual information across views. In Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances
in Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
pp. 15509–15519, 2019. URL https://proceedings.neurips.cc/paper/2019/
hash/ddf354219aac374f1d40b7e760ee5bb7-Abstract.html.
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec
2.0: A framework for self-supervised learning of speech representations. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html.
Randall Balestriero and Yann LeCun. Contrastive and non-contrastive self-supervised
learning recover global and local spectral embedding methods. In NeurIPS,
2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
aa56c74513a5e35768a11f4e82dd7ffb-Abstract-Conference.html.
Randall Balestriero, Léon Bottou, and Yann LeCun. The effects of regu-
larization and data augmentation are class dependent. In NeurIPS, 2022a.
URL http://papers.nips.cc/paper_files/paper/2022/hash/
f73c04538a5e1cad40ba5586b4b517d3-Abstract-Conference.html.
RandallBalestriero,IshanMisra,andYannLeCun. Adata-augmentationisworthAthousandsam-
ples: Exactquantificationfromanalyticalaugmentedsamplemoments. CoRR,abs/2202.08325,
2022b. URLhttps://arxiv.org/abs/2202.08325.
Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein,
Florian Bordes, Adrien Bardes, Grégoire Mialon, Yuandong Tian, Avi Schwarzschild, An-
drewGordonWilson,JonasGeiping,QuentinGarrido,PierreFernandez,AmirBar,HamedPir-
siavash, Yann LeCun, and Micah Goldblum. A cookbook of self-supervised learning. CoRR,
abs/2304.12210, 2023. doi: 10.48550/arXiv.2304.12210. URL https://doi.org/10.
48550/arXiv.2304.12210.
AdrienBardes,JeanPonce,andYannLeCun.Vicreg:Variance-invariance-covarianceregularization
for self-supervised learning. CoRR, abs/2105.04906, 2021. URL https://arxiv.org/
abs/2105.04906.
Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. Representation learning: A review and
new perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798–1828, 2013. doi: 10.
1109/TPAMI.2013.50. URLhttps://doi.org/10.1109/TPAMI.2013.50.
Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Big vision. https://github.com/
google-research/big_vision,2022.
Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 - mining discriminative
components with random forests. In David J. Fleet, Tomás Pajdla, Bernt Schiele, and Tinne
10PublishedasaconferencepaperatICLR2024
Tuytelaars (eds.), Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzer-
land, September 6-12, 2014, Proceedings, Part VI, volume 8694 of Lecture Notes in Com-
puter Science, pp. 446–461. Springer, 2014. doi: 10.1007/978-3-319-10599-4\_29. URL
https://doi.org/10.1007/978-3-319-10599-4_29.
Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for un-
supervised learning of visual features. CoRR, abs/1807.05520, 2018. URL http://arxiv.
org/abs/1807.05520.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand
Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020,virtual,2020. URLhttps://proceedings.neurips.cc/paper/2020/hash/
70feb62b69f16e0238f741fab228fec2-Abstract.html.
MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,PiotrBojanowski,andAr-
mandJoulin.Emergingpropertiesinself-supervisedvisiontransformers.CoRR,abs/2104.14294,
2021. URLhttps://arxiv.org/abs/2104.14294.
TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyE.Hinton.Asimpleframeworkfor
contrastivelearningofvisualrepresentations.InProceedingsofthe37thInternationalConference
onMachineLearning,ICML2020,13-18July2020,VirtualEvent,volume119ofProceedingsof
MachineLearningResearch, pp.1597–1607.PMLR,2020a. URLhttp://proceedings.
mlr.press/v119/chen20j.html.
Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, vir-
tual, June 19-25, 2021, pp. 15750–15758. Computer Vision Foundation / IEEE,
2021. doi: 10.1109/CVPR46437.2021.01549. URL https://openaccess.
thecvf.com/content/CVPR2021/html/Chen_Exploring_Simple_Siamese_
Representation_Learning_CVPR_2021_paper.html.
Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momen-
tumcontrastivelearning. CoRR,abs/2003.04297,2020b. URLhttps://arxiv.org/abs/
2003.04297.
Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vi-
sion transformers. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV
2021, Montreal, QC, Canada, October 10-17, 2021, pp. 9620–9629. IEEE, 2021. doi: 10.
1109/ICCV48922.2021.00950. URL https://doi.org/10.1109/ICCV48922.2021.
00950.
Yanzhi Chen, Dinghuai Zhang, Michael Gutmann, Aaron Courville, and Zhanxing Zhu. Neural
approximatesufficientstatisticsforimplicitmodels. arXivpreprintarXiv:2010.10079,2020c.
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-
scribingtexturesinthewild. In2014IEEEConferenceonComputerVisionandPatternRecog-
nition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pp. 3606–3613. IEEE Computer
Society,2014. doi: 10.1109/CVPR.2014.461. URLhttps://doi.org/10.1109/CVPR.
2014.461.
ThomasM.CoverandJoyA.Thomas. ElementsofInformationTheory2ndEdition(WileySeriesin
TelecommunicationsandSignalProcessing). Wiley-Interscience,July2006. ISBN0471241954.
Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. Randaugment:
Practical automated data augmentation with a reduced search space. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
d85b63ef0ccb114d0a3bb7b7d808028f-Abstract.html.
11PublishedasaconferencepaperatICLR2024
Frederick E. Daum. The fisher-darmois-koopman-pitman theorem for random processes. In 1986
25thIEEEConferenceonDecisionandControl,pp.1043–1044,1986. doi: 10.1109/CDC.1986.
267536.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszko-
reit,andNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionat
scale. In9thInternationalConferenceonLearningRepresentations, ICLR2021, VirtualEvent,
Austria,May3-7,2021.OpenReview.net,2021.URLhttps://openreview.net/forum?
id=YicbFdNTTy.
LiFei-Fei,RobertFergus,andPietroPerona. Learninggenerativevisualmodelsfromfewtraining
examples:Anincrementalbayesianapproachtestedon101objectcategories.Comput.Vis.Image
Underst.,106(1):59–70,2007. doi: 10.1016/J.CVIU.2005.09.012. URLhttps://doi.org/
10.1016/j.cviu.2005.09.012.
Stanislav Fort, Andrew Brock, Razvan Pascanu, Soham De, and Samuel L. Smith. Drawing mul-
tiple augmentation samples per image during training efficiently decreases test error. CoRR,
abs/2105.13343,2021. URLhttps://arxiv.org/abs/2105.13343.
BorjaRodríguezGálvez,ArnoBlaas,PauRodríguez,AdamGolinski,XavierSuau,JasonRamapu-
ram, Dan Busbridge, and Luca Zappella. The role of entropy and reconstruction in multi-view
self-supervisedlearning. InAndreasKrause,EmmaBrunskill,KyunghyunCho,BarbaraEngel-
hardt,SivanSabato,andJonathanScarlett(eds.),InternationalConferenceonMachineLearning,
ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine
Learning Research, pp. 29143–29160. PMLR, 2023. URL https://proceedings.mlr.
press/v202/rodri-guez-galvez23a.html.
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena
Buchatskaya,CarlDoersch,BernardoÁvilaPires,ZhaohanGuo,MohammadGheshlaghiAzar,
BilalPiot,KorayKavukcuoglu,RémiMunos,andMichalValko. Bootstrapyourownlatent-A
newapproachtoself-supervisedlearning. InHugoLarochelle,Marc’AurelioRanzato,RaiaHad-
sell,Maria-FlorinaBalcan,andHsuan-TienLin(eds.),AdvancesinNeuralInformationProcess-
ing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December6-12, 2020, virtual, 2020. URLhttps://proceedings.neurips.cc/
paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html.
Paul R. Halmos and Leonard J. Savage. Application of the radon-nikodym theorem to the theory
of sufficient statistics. Annals of Mathematical Statistics, 20:225–241, 1949. URL https:
//api.semanticscholar.org/CorpusID:119959959.
KaimingHe, XiangyuZhang, ShaoqingRen, andJianSun. Delvingdeepintorectifiers: Surpass-
ing human-level performance on imagenet classification. In 2015 IEEE International Confer-
ence on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 1026–1034.
IEEEComputerSociety,2015. doi: 10.1109/ICCV.2015.123. URLhttps://doi.org/10.
1109/ICCV.2015.123.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2016,LasVegas,NV,USA,June27-30,2016,pp.770–778.IEEEComputerSociety,2016. doi:
10.1109/CVPR.2016.90. URLhttps://doi.org/10.1109/CVPR.2016.90.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast
for unsupervised visual representation learning. CoRR, abs/1911.05722, 2019. URL http:
//arxiv.org/abs/1911.05722.
OlivierJ.Hénaff, AravindSrinivas, JeffreyDeFauw, AliRazavi, CarlDoersch, S.M.AliEslami,
and Aäron van den Oord. Data-efficient image recognition with contrastive predictive coding.
CoRR,abs/1905.09272,2019. URLhttp://arxiv.org/abs/1905.09272.
R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman,
Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information
12PublishedasaconferencepaperatICLR2024
estimation and maximization. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https:
//openreview.net/forum?id=Bklr3j0cKX.
EladHoffer,TalBen-Nun,ItayHubara,NivGiladi,TorstenHoefler,andDanielSoudry. Augment
your batch: better training with larger batches. CoRR, abs/1901.09335, 2019. URL http:
//arxiv.org/abs/1901.09335.
Jin Young Kim, Soonwoo Kwon, Hyojun Go, Yunsung Lee, and Seungtaek Choi. Scorecl:
Augmentation-adaptivecontrastivelearningviascore-matchingfunction.CoRR,abs/2306.04175,
2023. doi: 10.48550/arXiv.2306.04175. URL https://doi.org/10.48550/arXiv.
2306.04175.
Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei. Collecting a large-scale dataset of
fine-grained cars. 2013. URL https://api.semanticscholar.org/CorpusID:
16632981.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search). 2014. URLhttp://www.cs.toronto.edu/~kriz/cifar.html.
Kyungeun Lee, Jaeill Kim, Suhyun Kang, and Wonjong Rhee. Towards a rigorous analysis of
mutualinformationincontrastivelearning. CoRR,abs/2308.15704,2023. doi: 10.48550/arXiv.
2308.15704. URLhttps://doi.org/10.48550/arXiv.2308.15704.
Ralph Linsker. An application of the principle of maximum information preservation to lin-
ear systems. In David S. Touretzky (ed.), Advances in Neural Information Processing Sys-
tems 1, [NIPS Conference, Denver, Colorado, USA, 1988], pp. 186–194. Morgan Kauf-
mann, 1988. URL https://papers.nips.cc/paper_files/paper/1988/hash/
ec8956637a99787bd197eacd77acce5e-Abstract.html.
LajanugenLogeswaranandHonglakLee. Anefficientframeworkforlearningsentencerepresenta-
tions. In6thInternationalConferenceonLearningRepresentations,ICLR2018,Vancouver,BC,
Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL
https://openreview.net/forum?id=rJvJXZb0W.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net,2019. URLhttps://openreview.net/forum?id=Bkg6RiCqY7.
SubhransuMaji,EsaRahtu,JuhoKannala,MatthewB.Blaschko,andAndreaVedaldi.Fine-grained
visual classification of aircraft. CoRR, abs/1306.5151, 2013. URL http://arxiv.org/
abs/1306.5151.
XuanLongNguyen,MartinJ.Wainwright,andMichaelI.Jordan.Estimatingdivergencefunctionals
andthelikelihoodratiobyconvexriskminimization. CoRR,abs/0809.0853,2008. URLhttp:
//arxiv.org/abs/0809.0853.
OmkarM.Parkhi,AndreaVedaldi,AndrewZisserman,andC.V.Jawahar. Catsanddogs. In2012
IEEEConferenceonComputerVisionandPatternRecognition,Providence,RI,USA,June16-21,
2012,pp.3498–3505.IEEEComputerSociety,2012. doi: 10.1109/CVPR.2012.6248092. URL
https://doi.org/10.1109/CVPR.2012.6248092.
Ben Poole, Sherjil Ozair, Aäron van den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Pro-
ceedingsofthe36thInternationalConferenceonMachineLearning,ICML2019,9-15June2019,
Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp.
5171–5180. PMLR, 2019. URL http://proceedings.mlr.press/v97/poole19a.
html.
Ce Qi and Fei Su. Contrastive-center loss for deep neural networks. In 2017 IEEE International
ConferenceonImageProcessing,ICIP2017,Beijing,China,September17-20,2017,pp.2851–
2855.IEEE,2017. doi: 10.1109/ICIP.2017.8296803. URLhttps://doi.org/10.1109/
ICIP.2017.8296803.
13PublishedasaconferencepaperatICLR2024
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision. In Ma-
rina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Ma-
chineLearning,ICML2021,18-24July2021,VirtualEvent,volume139ofProceedingsofMa-
chineLearningResearch,pp.8748–8763.PMLR,2021. URLhttp://proceedings.mlr.
press/v139/radford21a.html.
Joshua David Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learn-
ing with hard negative samples. In 9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https:
//openreview.net/forum?id=CR1XOQ0UTh-.
Alex Rogozhnikov. Einops: Clear and reliable tensor manipulations with einstein-like notation.
In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event,
April25-29,2022.OpenReview.net,2022. URLhttps://openreview.net/forum?id=
oapKSVM2bcj.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, and Li Fei-Fei. Imagenet large
scalevisualrecognitionchallenge. CoRR,abs/1409.0575,2014. URLhttp://arxiv.org/
abs/1409.0575.
Ravid Shwartz-Ziv, Randall Balestriero, Kenji Kawaguchi, Tim G. J. Rudner, and Yann LeCun.
An information-theoretic perspective on variance-invariance-covariance regularization. CoRR,
abs/2303.00633, 2023. doi: 10.48550/arXiv.2303.00633. URL https://doi.org/10.
48550/arXiv.2303.00633.
Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In
Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Gar-
nett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on
Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp.
1849–1857, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/
6b180037abbebea991d8b1232f8a8ca9-Abstract.html.
Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted
structuredfeatureembedding. In2016IEEEConferenceonComputerVisionandPatternRecog-
nition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 4004–4012. IEEE Computer
Society,2016. doi: 10.1109/CVPR.2016.434. URLhttps://doi.org/10.1109/CVPR.
2016.434.
Jiaming Song and Stefano Ermon. Multi-label contrastive predictive coding. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
5cd5058bca53951ffa7801bcdf421651-Abstract.html.
YonglongTian,DilipKrishnan,andPhillipIsola. Contrastivemultiviewcoding. InAndreaVedaldi,
Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision - ECCV 2020 -
16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI, volume
12356 of Lecture Notes in Computer Science, pp. 776–794. Springer, 2020a. doi: 10.1007/
978-3-030-58621-8\_45. URL https://doi.org/10.1007/978-3-030-58621-8_
45.
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip
Isola. What makes for good views for contrastive learning? In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
Advances in Neural Information Processing Systems 33: Annual Conference on Neu-
ral Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, vir-
tual, 2020b. URL https://proceedings.neurips.cc/paper/2020/hash/
4c2e5eaae9152079b9e95845750bb9ab-Abstract.html.
14PublishedasaconferencepaperatICLR2024
Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On
mutual information maximization for representation learning. In 8th International Conference
on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
view.net,2020. URLhttps://openreview.net/forum?id=rkxoh24FPH.
AäronvandenOord,YazheLi,andOriolVinyals. Representationlearningwithcontrastivepredic-
tivecoding. CoRR,abs/1807.03748,2018. URLhttp://arxiv.org/abs/1807.03748.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.
5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
Julius von Kügelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Schölkopf,
Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmen-
tations provably isolates content from style. In Marc’Aurelio Ranzato, Alina Beygelz-
imer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances
in Neural Information Processing Systems 34: Annual Conference on Neural Informa-
tion Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 16451–
16467, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/
8929c70f8d710e412d38da624b21c3c8-Abstract.html.
Haoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu. Rethinking minimal sufficient represen-
tationincontrastivelearning. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pp.16041–16050,2022.
TongzhouWangandPhillipIsola. Understandingcontrastiverepresentationlearningthroughalign-
ment and uniformity on the hypersphere. In Proceedings of the 37th International Conference
onMachineLearning, ICML2020, 13-18July2020, VirtualEvent, volume119ofProceedings
ofMachineLearningResearch,pp.9929–9939.PMLR,2020. URLhttp://proceedings.
mlr.press/v119/wang20k.html.
XiaoWangandGuo-JunQi. Contrastivelearningwithstrongeraugmentations. IEEEtransactions
onpatternanalysisandmachineintelligence,45(5):5549–5560,2022.
JianxiongXiao,JamesHays,KristaA.Ehinger,AudeOliva,andAntonioTorralba. SUNdatabase:
Large-scale scene recognition from abbey to zoo. In The Twenty-Third IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2010, San Francisco, CA, USA, 13-18 June
2010,pp.3485–3492.IEEEComputerSociety,2010. doi: 10.1109/CVPR.2010.5539970. URL
https://doi.org/10.1109/CVPR.2010.5539970.
YangYou,IgorGitman,andBorisGinsburg. ScalingSGDbatchsizeto32kforimagenettraining.
CoRR,abs/1708.03888,2017. URLhttp://arxiv.org/abs/1708.03888.
ManzilZaheer,SatwikKottur,SiamakRavanbakhsh,BarnabásPóczos,RuslanSalakhutdinov,and
Alexander J. Smola. Deep sets. CoRR, abs/1703.06114, 2017. URL http://arxiv.org/
abs/1703.06114.
Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe
Zhang,JiataoGu,andJoshuaM.Susskind. Stabilizingtransformertrainingbypreventingatten-
tionentropycollapse.InAndreasKrause,EmmaBrunskill,KyunghyunCho,BarbaraEngelhardt,
SivanSabato,andJonathanScarlett(eds.),InternationalConferenceonMachineLearning,ICML
2023,23-29July2023,Honolulu,Hawaii,USA,volume202ofProceedingsofMachineLearning
Research, pp. 40770–40803. PMLR, 2023a. URL https://proceedings.mlr.press/
v202/zhai23a.html.
XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer. Sigmoidlossforlanguage
image pre-training. CoRR, abs/2303.15343, 2023b. doi: 10.48550/arXiv.2303.15343. URL
https://doi.org/10.48550/arXiv.2303.15343.
15PublishedasaconferencepaperatICLR2024
Appendices
A Broaderimpact 17
B Limitations 17
C ProofsofTheorems 18
C.1 MIlower-boundwithMulti-Crop . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C.2 LowervarianceofMulti-CropMIbound . . . . . . . . . . . . . . . . . . . . . . . 18
C.3 Generalized . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
NWJ
I
C.4 ValidityProperty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.5 ArithmeticandGeometricPVC. . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.6 BehaviorofMIGap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.7 ConnectionbetweenSufficientStatisticsandMIbounds . . . . . . . . . . . . . . 25
D NotesonMulti-Crop 26
D.1 DistributionfactorizationchoiceofMulti-Crop . . . . . . . . . . . . . . . . . . . 26
D.2 AggregationFunctionforMulti-Crop . . . . . . . . . . . . . . . . . . . . . . . . 27
E Additionaltheoreticalresultsanddiscussions 28
E.1 Generalizingone-vs-restMItosets . . . . . . . . . . . . . . . . . . . . . . . . . . 28
E.2 RecoveringSimCLR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
E.3 SigLIPconnectiontoMIbound . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
E.4 SufficientStatisticsextension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
E.5 OptimalmultiplicityintheFixedBatchsetting . . . . . . . . . . . . . . . . . . . 30
E.6 Synthetic1DGaussian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
F Real-worldimagerepresentationlearning 34
F.1 Experimentaldetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
F.2 Fine-tuningresultsontransfertasks . . . . . . . . . . . . . . . . . . . . . . . . . 35
F.3 Theroleofaugmentationstrengthathighmultiplicity . . . . . . . . . . . . . . . . 36
F.3.1 Comparisonoftotalfloatingoperations . . . . . . . . . . . . . . . . . . . 37
F.3.2 Implementationoflossfunctions . . . . . . . . . . . . . . . . . . . . . . . 38
G Expandedrelatedwork 40
H Extensionstodistillation 41
I Contributions 41
16PublishedasaconferencepaperatICLR2024
A BROADER IMPACT
Thisworkshowsdifferentwaysthatviewmultiplicitycanbeincorporatedintothedesignofrepre-
sentationlearningtasks. Thereareanumberofbenefits:
1. TheimprovedcomputeParetofrontshowninSection3.2,providesawayforpractitionersto
achievethedesiredlevelofmodelperformanceatreducedcomputationalcost.
2. Increasingviewmultiplicityhasahigherpotentialoffullycapturingtheaspectsofasample,
as is reinforced by the limiting behavior of the synthetic setting (Section 3.1). This has the
potentialtolearnmoreaccuraterepresentationsforunderrepresentedsamples.
Wealsonotethepotentialundesirableconsequencesofourproposedmethods:
1. Wefoundthatforafixednumberofupdates,thebestresultsareachievedbymaximizingthe
multiplicityM. Ifauserisnotcomputelimited,theymaychooseahighvalueofM,leading
togreaterenergyconsumption.
2. In the case one wants to maximize views that naturally occur in data as in CLIP (Radford
etal.,2021),theintentionalcollectionofadditionalviewsmaybeencouraged. Thispresents
anumberofchallenges: 1)thecollectionofextensivedataaboutasinglesubjectincreasesthe
effort needed to collect data responsibly; 2) the collection of more than one type of data can
beresourceintensive;and3)notalldatacollectionprocessesareequal,andalargernumberof
collectedviewsincreasesthechancethatatleastoneoftheviewsisnotagoodrepresentation
ofthesubject,whichmaynegativelyinfluencemodeltraining.
Theenvironmentalimpactofeachofthesetwopointsmaybesignificant.
B LIMITATIONS
Theworkpresentedattemptstopresentafairanalysisofthedifferentmethodsdiscussed. Despite
this, we acknowledge that the work has the following limitations, which are mainly related to the
real-worldanalysisonImageNet1k(Section3.2):
1. Our ImageNet1k analysis is restricted to variations of SimCLR contrastive learning method.
However, there are other variations of contrastive learning, for example van den Oord et al.
(2018); Chen et al. (2020b; 2021); Caron et al. (2020). There are also other types of Self-
SupervisedLearning(SSL)methodsthattrainmodelstosolvetasksinvolvingmultipleviews
of data, for example Grill et al. (2020); Caron et al. (2021). While we expect our results to
transfertothesemethods,wecannotsaythisconclusively.
2. OurImageNet1kanalysisisalsorestrictedtotheperformanceoftheResNet50architecture.
ItispossibletotrainSimCLRwithaVisionTransformer(ViT)backbone(Chenetal.,2021;
Zhaietal.,2023a),andanticipatetheeffectofincreasingviewmultiplicitytobestrongerinthis
case,asViTshasalessstrongprioronimagestructure,andaugmentationplaysalargerrolein
thetraining(Dosovitskiyetal.,2021). However,wecannotmakeanyconclusivestatements.
3. The largest number of views we consider is 16. It would be interesting to see the model
behaviorinfore.g. twouniquesamplesperbatch, and2048viewspersample, orincreasing
thenumberofviewsbeyond16foralargersetting. However, thesesettingsarenotpractical
for us to investigate, limiting the concrete statements we make for real world applications to
viewsM 16.
≤
4. AlthoughwepresentedsomesensitivityanalysisregardingaugmentationpolicychoiceinAp-
pendixF.3,alloftheaugmentationsweconsiderforImageNet1karevariationsontheSimCLR
augmentationpolicy.
5. Ourmethodislessapplicableinthecaseofnaturallyoccurring(multi-modal)data,ashereM
islimitedbythedataavailableandcannotbearbitrarilyincreased.
6. OurempiricalanalysisislimitedtosyntheticdataandthecomputervisiondatasetImageNet1k.
Whilewedon’tanticipatesignificantlydifferentconclusionsforotherdomains,weareunable
tomakeanyconclusiveempiricalstatements.
17PublishedasaconferencepaperatICLR2024
7. TherearealternativestoOne-vs-RestMutualInformation(MI)whenconsideringM variables.
We introduce an alternative partitioning in Appendix E.1, but do not investigate as it is less
simpletoworkwith.
8. In all of our experiments, hyperparameters are fixed to be those of the reference SimCLR
model.Inprincipleitispossiblethatadifferentconclusioncouldbedrawnifahyperparameter
searchwasdonepermultiplicityconfiguration,andthenthebestperforminghyperparameters
foreachpointwerecomparedtoeachother.
C PROOFS OF THEOREMS
C.1 MILOWER-BOUNDWITHMULTI-CROP
Proposition2.1. ForKindependentsamplesandmultiplicityM denotedX ,theMulti-Crop
1:K,1:M
ofany inEquation1hasthesameMIlowerboundasthecorresponding
Pair Pair
L L
(x ;x ) log(K) E[ (X )]=log(K) E[ (X )], (30)
1 2 Multi-Crop 1:K,1:M Pair 1:K,1:2
I ≥ − L − L
wheretheexpectationisoverK independentsamples.
Proof. Note that for the pair objective , we have the following lower-bound for the pair MI
pair
L
using (Hjelmetal.,2019;Nguyenetal.,2008)sampleestimator:
NWJ
I
(x ;x ) log(K)
E(cid:2)
(X
)(cid:3)
. (31)
α β pair 1:K,{α,β}
I ≥ − L
Iftheviewsareuniformlyandindependentlygenerated, i.e.η Uniform(Γ), whereΓistheset
α
∼
ofview-generatingprocesses,then
(x ;x )= (x ;x ) α=β,γ =ν [M]. (32)
α β γ ν
I I ∀ ̸ ̸ ∈
FollowingEquations31and32,wehave
M
1 (cid:88)(cid:88)
(x ;x )= (x ;x ) (33)
1 2 α β
I M(M 1) I
− α=1β̸=α
 
M
1 (cid:88)(cid:88)
≥log(K) −E 
M(M 1)
Lpair(X 1:K,{α,β}) (34)
− α=1β̸=α
=log(K) E[ (X )]. (35)
Multi-Crop 1:K,1:M
− L
Moreover,wecanrewritetheMulti-Cropobjectiveasfollowsinexpectation:
 
M
1 (cid:88)(cid:88)
E[ LMulti-Crop(X 1:K,1:M)]=E 
M(M 1)
Lpair(X 1:K,{α,β}) (36)
− α=1β̸=α
=E[E [ (X )]], (37)
Γ pair 1:K,1:2
L
where the second equality is due to the fact that all the views are uniformly and independently
sampledfromthesetΓ. Now,gettingexpectationoveralltherandomnessleadusto
E[ (X )]=E[E [ (X )]]=E[ (X )]. (38)
Multi-Crop 1:K,1:M Γ pair 1:K,1:2 pair 1:K,1:2
L L L
Thiscompletestheproof.
C.2 LOWERVARIANCEOFMULTI-CROPMIBOUND
Proposition2.2. ForK independentsamplesandmultiplicityM,M 3,denotedX ,the
1:K,1:M
≥
Multi-Cropofany inEquation1hasalowersamplevariancethanthecorresponding
Pair Pair
L L
2(2M 1)
Var[ (X )] − Var[L (x ,x )]<Var[L (x ,x )], (39)
LMulti-Crop 1:M ≤ 3M(M 1) Pair 1 2 Pair 1 2
−
wherethevarianceisoverK independentsamples.
18PublishedasaconferencepaperatICLR2024
Proof. WestartwithcomputingthevarianceofbothsideofEquation1. Notethatforanytwopairs
of(x ,x )and(x ,x )suchthat α,β γ,ν = ,wehave
α β γ ν
{ }∩{ } ∅
Cov[ (x ,x ), (x ,x )]=0, (40)
pair α β pair γ ν
L L
whereCovdenotesthecovarianceoperator.Thisisduetothefactthatviewgenerationprocessesare
conditionallyindependent(conditiononx). Thus, foranyrealizationofx, theconditionalcovari-
ancewouldbezero,whichleadstotheexpectationoftheconditionalcovariance,andconsequently
Equation40bezero. WecanalsorewriteEquation1asfollows:
M M
1 (cid:88)(cid:88)
(X )= (x ,x ) (41)
LMulti-Crop 1:M M(M 1) LPair α β
− α=1β̸=α
M M
2 (cid:88) (cid:88)
= (x ,x ). (42)
M(M 1) LPair α β
− α=1β>α
Havingthepairwiselosstobesymmetric,wecannowcomputethevarianceofbothsidesasfollows:
(cid:34)
M M
4 (cid:88) (cid:88)
Var[ (X )]= Var[L (x ,x )]+
LMulti-Crop 1:M M2(M 1)2 Pair α β
− α=1β>α
(43)
(cid:35)
(cid:88)(cid:88)(cid:88)
2 Cov[ (x ,x ), (x ,x )] .
Pair α γ Pair γ β
L L
α γ β
Onewaytocountthenumberofelementsinthecovariancetermistonotethatwecansampleα,β,
andγfrom[M]butonlyoneoftheorderedsequenceofthesethreeisacceptableduetotheordering
conditioninEquation42,whichresultsin M(M−1)(M−2) choices,whereM 3.
6 ≥
Anothermainpointhereisthatduetotheidenticallydistributedview-generativeprocesses,
Var[L (x ,x )]=Var[L (x ,x )] α=β,γ =ν [M]. (44)
Pair α β Pair γ ν
∀ ̸ ̸ ∈
Thus,usingthevariance-covarianceinequality,wecanwritethat
(cid:12) (cid:12)
(cid:12)Cov[ (x ,x ), (x ,x )](cid:12) Var[L (x ,x )]=Var[L (x ,x )]. (45)
(cid:12) Pair α γ Pair γ β (cid:12) Pair α γ Pair γ β
L L ≤
SubstitutingEquations44and45inEquation43,wehavethefollowing:
(cid:34)
4 M(M 1)
Var[ (X )] − Var[L (x ,x )]+
LMulti-Crop 1:M ≤ M2(M 1)2 2 Pair 1 2
−
(46)
(cid:35)
M(M 1)(M 2)
2 − − Var[L (x ,x )] .
6 Pair 1 2
Simplifyingtherighthandside,weget
2(2M 1)
Var[ (X )] − Var[L (x ,x )]<Var[L (x ,x )], (47)
LMulti-Crop 1:M ≤ 3M(M 1) Pair 1 2 Pair 1 2
−
foranyM 3. IfM = 2,theclaimistrivialasbothsidesareequal. Thus,theproofiscomplete
≥
andMulti-Cropobjectivehasstrictlylowervariancecomparedtothepairobjectiveinthepresence
ofviewmultiplicity.
C.3 GENERALIZED
NWJ
I
Theorem2.1. ForanyM 2,α [M],asetofM randomvariablesX ,andforanypositive
1:M
functionF(M) :
M−≥
1
R+∈
X ×X (cid:55)→
(cid:104) (cid:105) (cid:104) (cid:105)
I(x α;X̸= 1:α M) ≥E
pX1:M
F(M)(x α,X 1̸= :Mα) −E
pxαp X̸= 1:α
M
eF(M)(xα,X 1̸= :Mα) +1= IGenNWJ. (48)
19PublishedasaconferencepaperatICLR2024
Proof. WestartbythedefinitionofMI:
(cid:34) (cid:35)
(cid:16) (cid:17) p(x ,X̸=α)
x ;X̸=α =E log α 1:M (49)
I α 1:M pX1:M p(x )p(X̸=α)
α 1:M
(cid:34) (cid:35)
=E log
p(x α,X 1̸= :Mα )eF(M)(xα,X 1̸= :Mα)
(50)
pX1:M p(x α)p(X 1̸= :Mα)eF(M)(xα,X 1̸= :Mα)
(cid:34) (cid:35)
=E
(cid:104)
F(M)(x
,X̸=α)(cid:105)
E
logp(x α)p(X 1̸= :Mα)eF(M)(xα,X 1̸= :Mα)
(51)
pX1:M α 1:M − pX1:M p(x ,X̸=α)
α 1:M
Now, we note that the argument of the second term of right hand side in Equation 51 is always
positive. Foranyz 0,wehavethatlog(z) z 1. Thus,wehave:
≥ ≤ −
(cid:34) (cid:35)
(cid:16)
x ;X̸=α
(cid:17)
=E
(cid:104)
F(M)(x
,X̸=α)(cid:105)
E
logp(x α)p(X 1̸= :Mα)eF(M)(xα,X 1̸= :Mα)
(52)
I α 1:M pX1:M α 1:M − pX1:M p(x ,X̸=α)
α 1:M
(cid:34) (cid:35)
≥E pX1:M(cid:104) F(M)(x α,X 1̸= :Mα)(cid:105) −E
pX1:M
p p(x (xα)p ,X(X ̸=1̸= α:Mα )) eF(M)(xα,X 1̸= :Mα) +1. (53)
α 1:M
Now,wecanusethechangeofmeasureforthesecondtermontherighthandsideandtheproofis
complete:
(cid:34) (cid:35)
(cid:16)
x ;X̸=α
(cid:17)
E
(cid:104)
F(M)(x
,X̸=α)(cid:105)
E
p(x α)p(X 1̸= :Mα)eF(M)(xα,X 1̸= :Mα)
+1 (54)
I α 1:M ≥ pX1:M α 1:M − pX1:M p(x ,X̸=α )
α 1:M
(cid:104) (cid:16) (cid:17)(cid:105) (cid:104) (cid:105)
=E
pX1:M
F(M) x α,X 1̸= :Mα −E
pxαp X̸= 1:α
M
eF(M)(xα,X 1̸= :Mα) +1 (55)
= . (56)
GenNWJ
I
C.4 VALIDITYPROPERTY
Theorem C.1. Both aggregation functions introduced by Equation 9 and Equation 10 satisfy the
Validityproperty,i.e.Equation8.
Proof. Let us define z = exp(F(2)(x ,x )) for a given x and β = α. Thus, we can rewrite
β α β α
̸
Equations9and10asfollows:
 
(cid:16) (cid:17) 1 (cid:88)
Arithmetic: F(M) x α,X 1̸= :Mα =log
M 1
z β, (57)
− β̸=α
(cid:16) (cid:17) 1 (cid:88)
Geometric: F(M) x ,X̸=α = log(z ). (58)
α 1:M M 1 β
− β̸=α
Followingthedefinitionoftheaggregationfunction,anddenotingc =
pxαp X̸= 1:α
M,wecanrewrite
α pX1:M
theaggregationfunctionsasfollowing:
  
1 (cid:88) 1 (cid:88)
Arithmetic: g α(M) =c αexplog
M 1
z β=
M 1
c αz β, (59)
− β̸=α − β̸=α
    1
M−1
1 (cid:88) (cid:89)
Geometric: g α(M) =c αexp
M 1
log(z β)= c αz β . (60)
− β̸=α β̸=α
20PublishedasaconferencepaperatICLR2024
Now,toprovetheValidityforthesetwoaggregationfunctions,itisenoughtoshowthefollowing:
 
1 (cid:88) 1 (cid:88)
Arithmetic: GMI
M 1
c αz β
≤ M 1
GMI(c αz β), (61)
− β̸=α − β̸=α
  1 
M−1
(cid:89) 1 (cid:88)
Geometric: GMI  c αz β  ≤ M 1 GMI(c αz β). (62)
β̸=α − β̸=α
WestartbyprovingEquation61. FollowingthedefinitionofMIGapinEquation7,wenotethatthe
MIGapisaconvexfunctionsinceg log(g ) 1isconvex. Now,usingtheJensen’sinequality,
α α
− −
wehave:
(E [c z]) E [ (c z)], (63)
MI z α z MI α
G ≤ G
which is another expression of Equation 61 and completes the proof for Arithmetic mean. For
theGeometricmean, byexpandingonthedefinitionofMIGapinEquation62, andremovingthe
constant1frombothsides,wegetthefollowinginequality:
  1   1 
M−1 M−1
(cid:89) (cid:89) 1 (cid:88)
E  c αz β −log c αz β  ≤ M 1 E[c αz β −log(c αz β)] (64)
β̸=α β̸=α − β̸=α
 
1 (cid:88)
=E  (c αz β log(c αz β)). (65)
M 1 −
− β̸=α
So,provingEquation62isequivalenttoprovethefollowing:
  1   1 
M−1 M−1
(cid:89) (cid:89) 1 (cid:88)
E  c αz β −log c αz β
− M 1
(c αz β −log(c αz β)) ≤0. (66)
β̸=α β̸=α − β̸=α
We show for any realization of z , the inequality is true, then the same applies to the expectation
β
(cid:16) (cid:17) 1
andtheproofiscomplete. Notethatlog (cid:81) c z M−1 = 1 (cid:80) log(c z ),moreover,
β̸=α α β M−1 β̸=α α β
usingarithmetic-geometricinequalityforanynon-negativevaluesofz andc ,wehave:
β α
  1
M−1
(cid:89) 1 (cid:88)
 c αz β c αz β, (67)
≤ M 1
β̸=α − β̸=α
whichprovesEquation66,andcompletestheproof.
C.5 ARITHMETICANDGEOMETRICPVC
Theorem2.2. ForanyK,M 2,B = KM,α [M],anyscalarfunctionf : R,and
≥ ∈ C×C (cid:55)→
maph: ,wehave
X (cid:55)→C
 
(cid:16) (cid:17) 1 (cid:88)K 1 (cid:88)
ArithmeticPVC:
I
x α;X̸= 1:α
M
≥c(B,M)+E 
K
log
M 1
ℓ i,α,β, (68)
i=1 − β̸=α
 
(cid:16) (cid:17) 1 (cid:88)K 1 (cid:88)
GeometricPVC:
I
x α;X̸= 1:α
M
≥c(B,M)+E 
K M 1
log ℓ i,α,β, (69)
i=1 − β̸=α
wherec(B,M)=log(B M +1),theexpectationisoverK independentsamplesX ,and
1:K,1:M
−
ef(x(cid:101)i,α,x(cid:101)i,β)
ℓ (X )= , x =h(x ). (70)
i,α,β 1:K,1:M ef(x(cid:101)i,α,x(cid:101)i,β)+(cid:80) (cid:80)M ef(x(cid:101)j,γ,x(cid:101)i,β) (cid:101)i,α i,α
j̸=i γ=1
Wehavewrittenℓ insteadofℓ (X )wherethemeaningisclear.
i,α,β i,α,β 1:K,1:M
21PublishedasaconferencepaperatICLR2024
Proof. LetussampleKindependentsetsofX ,whereidenotesthesamplenumberfori [K].
i,1:M
∈
Byindependenthere,wemean i=j, β,γ;X X . Now,letusdefineX˜ asfollowing:
i,β j,γ i,α
∀ ̸ ∀ ⊥⊥
(cid:91)
X˜ = X x . (71)
i,α { j,β }j̸=i,β { i,α }
Sincethesamplesarei.i.dandtheviewsofdifferentsamplesarealsoindependent,thenX˜ hasno
i,α
moreinformationthanX aboutX̸=α . Thus,
i,α i,1:M
(x ;X̸=α )= (X ;X̸=α )= (X˜ ;X̸=α ). (72)
I i,α i,1:M I i,α i,1:M I i,α i,1:M
Moreover,sincethesamplesareidenticallydistributed,wehave:
K K
1 (cid:88) 1 (cid:88)
(x ;X̸=α )= (x ;X̸=α )= (X˜ ;X̸=α ) (73)
I α 1:M K I i,α i,1:M K I i,α i,1:M
i=1 i=1
(cid:16) (cid:17)
Now,followingtheproofofTheorem2.1,weneedtodefineF(M) X˜ ,X̸=α . Followingthe
i,α i,1:M
(cid:16) (cid:17)
ArithmeticandGeometricmeaninEquations9and10, weonlyneedtodefineF(2) X˜ ,x
i,α i,β
forβ =αasthebasis. Definingℓ (X )asfollows:
i,α,β 1:K,1:M
̸
ef(x(cid:101)i,α,x(cid:101)i,β)
ℓ (X )= , x =h(x ), (74)
i,α,β 1:K,1:M ef(x(cid:101)i,α,x(cid:101)i,β)+(cid:80) (cid:80)M ef(x(cid:101)j,γ,x(cid:101)i,β) (cid:101)i,α i,α
j̸=i γ=1
wecannowdefineF(2)forbothArithmeticandGeometricas:
(cid:16) (cid:17)
F(2) X˜ ,x =log((B M +1)ℓ (X ))=c(B,M)+log ℓ (75)
i,α i,β i,α,β 1:K,1:M i,α,β
−
(cid:16) (cid:17)
Now,substitutingF(M) X˜ ,X̸=α (denotedbyF(M)forsimplicity)inTheorem2.1,wehave
i,α i,1:M
thefollowing:
Arithmeticmean:
K
1 (cid:88)
(x ;X̸=α )= (X˜ ;X̸=α ) (76)
I α 1:M K I i,α i,1:M
i=1
(cid:34) (cid:35) (cid:34) (cid:35)
K K
E 1 (cid:88) F(M) E 1 (cid:88) eF(M) +1 (77)
≥ pX1:K,1:M K
i=1
− Πj̸=ip X˜jpxi,αp X̸= i,1α
:M
K
i=1
 
K
1 (cid:88) B M +1 (cid:88)
=E pX1:K,1:M 
K
log M−
1
ℓ i,α,β
i=1 − β̸=α
 
K
1 (cid:88)B M +1 (cid:88)
−E Πj̸=ip X˜jpxi,αp X̸= i,1α
:M
 K
i=1
M− −1 β̸=αℓ i,α,β+1. (78)
NotingthattheexpectationinEquation78istakingovervariablesindependently,andnotingthatthe
samplesareidenticallydistributed,anddifferentviewsaregeneratedindependently,wecanreplace
x byafixedi,e.g.withoutlossofgenerality,i=1.Now,wecaneasilyseethatthistermbecomes
i,β
equaltoone. Thus,
 
K
1 (cid:88) B M +1 (cid:88)
I(x α;X̸= 1:α M) ≥E pX1:K,1:M 
K
log M−
1
ℓ i,α,β (79)
i=1 − β̸=α
 
K
1 (cid:88) 1 (cid:88)
=c(B,M)+E  log ℓ i,α,β, (80)
K M 1
i=1 − β̸=α
whichistheclaimofthetheorem,andtheproofiscompleteforArithmeticmean.
22PublishedasaconferencepaperatICLR2024
Geometricmean:
K
1 (cid:88)
(x ;X̸=α )= (X˜ ;X̸=α ) (81)
I α 1:M K I i,α i,1:M
i=1
(cid:34) (cid:35) (cid:34) (cid:35)
K K
E 1 (cid:88) F(M) E 1 (cid:88) eF(M) +1 (82)
≥ pX1:K,1:M K
i=1
− Πj̸=ip X˜jpxi,αp X̸= i,1α
:M
K
i=1
 
K
1 (cid:88) 1 (cid:88)
=E
pX1:K,1:M
c(B,M)+
K M 1
log ℓ i,α,β+1
i=1 − β̸=α
 
K
1 (cid:88) 1 (cid:88)
−E Πj̸=ip X˜jpxi,αp X̸= i,1α
:M
 K i=1exp(c(B,M)+ M −1 β̸=αlog ℓ i,α,β). (83)
Sinceexp(z)isaconvexfunction,wecanusetheJensen’sinequalityforEquation83:
 
K
1 (cid:88) 1 (cid:88)
E Πj̸=ip X˜jpxi,αp X̸= i,1α
:M
 K i=1exp(c(B,M)+ M −1 β̸=αlog ℓ i,α,β) (84)
 
K
1 (cid:88) 1 (cid:88)
≤E Πj̸=ip X˜jpxi,αp X̸= i,1α
:M
 K
i=1
M −1 β̸=α(B −M +1)ℓ i,α,β (85)
=1.
WherethelastequalityisresultedwiththesamereasoningbehindEquation78. Thus,wehave:
 
K
1 (cid:88) 1 (cid:88)
I(x α;X̸= 1:α M) ≥E
pX1:K,1:M
c(B,M)+
K M 1
log ℓ i,α,β+1 −1 (86)
i=1 − β̸=α
 
K
1 (cid:88) 1 (cid:88)
=c(B,M)+E  log ℓ i,α,β, (87)
K M 1
i=1 − β̸=α
andtheproofiscomplete.
C.6 BEHAVIOROFMIGAP
ToinvestigatethebehaviorofMIGapandtoprovidetheproofofTheorem2.3,wefirstprovidethe
followinglemma,whichisresultedonlybythedefinitionofexpectationinprobabilitytheory:
LemmaC.2. LetI 1,...,k with I =m,m k,beauniformlydistributedsubsetofdistinct
⊂{ } | | ≤
indicesfrom 1,...,k . Then,thefollowingholdsforanysequenceofnumbersa ,...,a .
1 k
{ }
(cid:20) (cid:21)
a +...+a a +...+a
E i1 im = 1 k (88)
I={i1,...,im}
m k
Now,forTheorem2.3,wehavethefollowing:
Theorem2.3. Forfixedα, theMIGapofArithmeticandGeometricPVCaremonotonicallynon-
increasingwithM:
(X ;g(M2)) (X ;g(M1)) M M . (89)
GMI 1:M2 α ≤GMI 1:M1 α ∀ 1 ≤ 2
23PublishedasaconferencepaperatICLR2024
Proof. Letususethenewformofaggregationfunctions’definitionwithz inEquations59and60.
β
ForM M ,andforArithmeticmean,i.e.g(M) = 1 (cid:80) c z ,wehave:
1 ≤ 2 α M−1 β̸=α α β
 
1 (cid:88)
GMI(X 1:M2;g α(M2))=E pX1:M 
M
2
1
c αz β
− β̸=α
  
1 (cid:88)
−E pX1:M2 log
M
2
1
c αz β −1 (90)
− β̸=α
  
1
M (cid:88)1−1
=E pX1:M2 E I={γ1,...,γM1−1}
M
1
1
c αz γj
− j=1
   
1
M (cid:88)1−1
−E pX1:M2 logE I={γ1,...,γM1−1}
M
1
1
c αz γj −1 (91)
− j=1
 
1 (cid:88)
≤E pX1:M1 
M
1
1
c αz β
− β̸=α
   
1
M (cid:88)1−1
−E pX1:M1 E I={γ1,...,γM1−1}log
M
1
1
c αz γj −1 (92)
− j=1
 
1 (cid:88)
=E pX1:M1 
M
1
1
c αz β
− β̸=α
  
1 (cid:88)
−E pX1:M1 log
M
1
1
c αz β −1 (93)
− β̸=α
= (X ;g(M1)), (94)
GMI 1:M1 α
where the first equality is due to the Lemma C.2, and the inequality is resulted from Jensen’s in-
equality. Therefore,forArithmeticmean,theMIGapisdecreasingwithrespecttoM.
FortheGeometricmean,andfollowingthedefinitionofMIGap,wehave:
  1 
M2−1
(cid:89)
GMI(X 1:M2;g α(M2))=E pX1:M2   c αz β  
β̸=α
 
1 (cid:88)
−E pX1:M2 
M
2
1
log(c αz β) −1 (95)
− β̸=α
  1 
M2−1
(cid:89)
=E pX1:M2   c αz β  
β̸=α
 
1 (cid:88)
−E pX1:M1 
M
1
1
log(c αz β) −1, (96)
− β̸=α
24PublishedasaconferencepaperatICLR2024
where the equality is followed by Lemma C.2, similarly to the corresponding proof for the Arith-
meticmean. Now,mainlyfocusingonthefirsttermoftheMIGap,wehave:
   1 
M (cid:89)2−1 M2−1
GMI(X 1:M2;g α(M2))=E pX1:M2  exp log c αz β  
β̸=α
 
1 (cid:88)
−E pX1:M1 
M
1
1
log(c αz β) −1 (97)
− β̸=α
 
1
M (cid:88)2−1
=E pX1:M2 exp
M
2
1
log c αz β
− β̸=α
 
1 (cid:88)
−E pX1:M1 
M
1
1
log(c αz β) −1 (98)
− β̸=α
   
1
M (cid:88)1−1
=E pX1:M2 expE I={γ1,...,γM1−1}
M
1
1
logc αz γj
− j=1
 
1 (cid:88)
−E pX1:M1 
M
1
1
log(c αz β) −1 (99)
− β̸=α
   
1
M (cid:88)1−1
≤E pX1:M2 E I={γ1,...,γM1−1}exp
M
1
1
logc αz γj
− j=1
 
1 (cid:88)
−E pX1:M1 
M
1
1
log(c αz β) −1 (100)
− β̸=α
  1 
M1−1
(cid:89)
=E pX1:M1   c αz β  
β̸=α
 
1 (cid:88)
−E pX1:M1 
M
1
1
log(c αz β) −1 (101)
− β̸=α
= (X ;g(M1)). (102)
GMI 1:M1 α
Here,Equation100isresultedusingLemmaC.2byreplacinga =logc z ,andtheinequalityis
β α β
duetotheJensen’sinequality. Thus,theproofiscomplete.
C.7 CONNECTIONBETWEENSUFFICIENTSTATISTICSANDMIBOUNDS
Theorem2.4. ForanyK,M 2,B = KM,α [M],andthechoiceofQinEquation24,we
≥ ∈
have(seeAppendixC.7fortheproof)
(cid:34) (cid:35)
(cid:16) (cid:17) 1 (cid:88)K
x ;X̸=α c(B,M)+E logℓ˜ , (103)
I α 1:M ≥ K i,α
i=1
wherec(B,M)=log(B M +1),theexpectationisoverK independentsamplesX .
1:K,1:M
−
Proof. Theproofconsistsoftwoparts:
1. ShowthatthereisF(M)correspondingtothechoiceofQinEquation24.
25PublishedasaconferencepaperatICLR2024
(cid:16) (cid:17)
2. Achievingthelower-boundusingthegivenF(M)for x ,X̸=α .
I α 1:M
Weprovebothpointstogetherbystudyingthelower-boundforone-vs-restMIgiventheaforemen-
tioned F(M). The proof is very similar to the proof of Theorem 2.2. We use the definition of X˜
i
as Equation 71. We also note that since the samples are i.i.d, and the view generation is indepen-
dent,wecanalsouseEquations72and73. Consequently,weonlyneedtodefinethesample-based
(cid:16) (cid:17)
F(M) X˜ ,X̸=α . Note that here, in contrast with Arithmetic and Geometric, we do not have
i i,1:M
(cid:16) (cid:17)
F(2)asourbasisforF(M). WedefinetheF(M) X˜ ,X̸=α asfollows:
i i,1:M
K
1 (cid:88)
F(M)(X ;α)=c(B,M)+ logℓ˜ , (104)
1:K,1:M i,α,β
K
i=1
which is the sample-based generalization of F(M)(cid:16) x ,X̸=α (cid:17) = T(x ) (cid:80)M β̸=αT(xβ) . We also
α 1:M α · M−1
note that the introduced F(M) and its corresponding aggregation function, follows all the main
properties, i.e. interchangeable arguments, poly-view order invariance, and expandability. Thus,
the first point is correct. Now, we continue with the lower-bound. Substituting Equation 104 in
Theorem2.1,wegetthefollowing:
K
1 (cid:88)
(x ;X̸=α )= (X˜ ;X̸=α ) (105)
I α 1:M K I i i,1:M
i=1
(cid:34) (cid:35) (cid:34) (cid:35)
K K
E 1 (cid:88) F(M) E 1 (cid:88) eF(M) +1 (106)
≥ pX1:K,1:M K
i=1
− Πj̸=ip X˜jpxi,αp X̸= i,1α
:M
K
i=1
 
K
1 (cid:88) 1 (cid:88)
=E pX1:K,1:M c(B,M)+
K M 1
log ℓ˜ i,α,β
i=1 − β̸=α
(cid:34) (cid:35)
K
1 (cid:88)
E (B M +1)ℓ˜ +1 (107)
− Πj̸=ip X˜jpxi,αp X̸= i,1α
:M
K
i=1
− i,α,β
 
K
1 (cid:88) 1 (cid:88)
=c(B,M)+E pX1:K,1:M 
K M 1
log ℓ˜ i,α,β, (108)
i=1 − β̸=α
wherethelastinequalityisresultedwiththesamereasoningashavingidenticallydistributedpairs
of (X˜ ,X̸=α ) due to the sample generation process, and the fact that expectation is taken over
i i,1:M
randomvariablesindependently. Notethathere,maximizingthelower-boundcorrespondstomaxi-
mizingℓ˜ ,whichprovidesthesameoptimizationproblemasEquation21withQinEquation24.
i,α,β
Thus,theproofiscompleteandsufficientstatisticsisalsoanMIlower-bound.
D NOTES ON MULTI-CROP
D.1 DISTRIBUTIONFACTORIZATIONCHOICEOFMULTI-CROP
As explained in more detail in the main text, Tian et al. (2020b) and Caron et al. (2020) studied
the idea of view multiplicity. While their technical approach is different, they both took a similar
approachofmultiplicity;gettingaverageofpairwise(two-view)objectivesasinEquation1. Here,
weshowthatthischoiceofcombiningobjectivesinherentlyappliesaspecificchoiceoffactorization
to the estimation of true conditional distribution of p(xc) in Figure 1b using multiple views, i.e.
|
appliesaninductivebiasinthechoiceofdistributionfactorization.
Following the InfoMax objective, we try to estimate (x;c) using the pairwise proxy (x ;x ).
α β
I I
Thus,theideaofMulti-Cropcanbewrittenasfollowinginequalities:
M M M
1 (cid:88) 1 (cid:88)(cid:88)
(x;c) (x;x ) (x ;x ). (109)
α α β
I ≥ M I ≥ M(M 1) I
α=1 − α=1β̸=α
26PublishedasaconferencepaperatICLR2024
Assumingthatthesetwolower-boundtermsareestimationsforthelefthandsideappliesdistribu-
tionalassumption. Toseethis,westartwithexpandingontheMIdefinitionineachterm:
(cid:20) (cid:21)
p(xc)
(x;c)=E log | (110)
I p(x)
M M (cid:20) (cid:21)
1 (cid:88)
(x;x )=
1 (cid:88)
E
logp(x |x α)
(111)
α
M I M p(x)
α=1 α=1
M M M M (cid:20) (cid:21)
1 (cid:88)(cid:88) (x ;x )= 1 (cid:88)(cid:88) E logp(x α |x β) . (112)
α β
M(M 1) I M(M 1) p(x )
α
− α=1β̸=α − α=1β̸=α
Now,assumingthattheview-generativeprocessesdonotchangethemarginaldistributions,i.e.for
anyα,p(x)=p(x ),andconsideringEquation109,wehave:
α
(cid:20) (cid:21)  (cid:32) M (cid:33) M1    M M  M(M1 −1)
E logp p(x (x|c )) ≥E log (cid:89) p( px (| xx )α)  ≥E log(cid:89) (cid:89) p(x pα (x|x )β)   . (113)
α=1 α=1β̸=α
Therefore, thedistributionalassumptionorthechoiceoffactorizationisestimatingp(xc)bythe
|
followingdistributions:
(cid:32) (cid:33)1
M M
(cid:89)
p(xc)= p(xx ) , (114)
(cid:98) α
| |
α=1
  1
M M M(M−1)
(cid:89) (cid:89)
p(xc)= (cid:98)  p(x α x β) , (115)
| |
α=1β̸=α
which translates to estimating the distribution using its geometric mean. Note that the symbol =
(cid:98)
readsas“estimates”,anditisnotequality.
D.2 AGGREGATIONFUNCTIONFORMULTI-CROP
Following the result of Proposition 2.1, Multi-Crop is an average of pairwise objectives, which
means that if we know the aggregation function and F(2) for the pairwise objective, then we can
writetheaggregationfunctionformulti-cropasfollowing:
1 (cid:88)
(x ;X̸=α ) (x ;x ), (116)
I α 1:M ≥ M 1 I α β
− β̸=α
(x ;x ) E[F(x ,x )]
E(cid:20)
p(x α)p(x
β)eF(xα,xβ)(cid:21)
+1, (117)
I α β ≥ α β − p(x ,x )
α β
wherethesecondlineisfromTheorem2.1bysettingM =2. Now,bygettingaverageoverβfrom
bothsides,wecansee:
 
(cid:80)
β̸= MαI(x 1α;x β) ≥E β[E[F(x α,x β)]] −E  M1
1
(cid:88) p(x α) pp (( xx β ,) xeF( )xα,xβ) +1. (118)
α β
− − β̸=α
Following the proof of Theorem 2.1 and the definition of aggregation function in Equation 6, we
achievethefollowingaggregationfunctionforMulti-Crop:
g(M)(cid:16) X̸=α (cid:17) = 1 (cid:88)M g(2)(cid:0) X (cid:1) (119)
α 1:M M 1 α {α,β}
− β̸=α
27PublishedasaconferencepaperatICLR2024
E ADDITIONAL THEORETICAL RESULTS AND DISCUSSIONS
E.1 GENERALIZINGONE-VS-RESTMITOSETS
Ageneralizationoftheone-vs-restMIistoconsidertheMIbetweentwosets. Letusassumethat
the set of [M] is partitioned into two sets A and B, i.e. XA XB = X 1:M, and A B = ,
∪ ∩ ∅
where denotestheemptyset. DefiningthedensityofXA andXB asthejointdistributionoftheir
∅
correspondingrandomvariables,wecandefinethegeneralizedversionofone-vs-restMI:
DefinitionE.1(Two-SetMI). ForanytwopartitionsetofAandBover[M],definethetwo-setMI
asfollowing:
I(XA,XB)= DKL(p
X1:M
∥p XAp XB). (120)
Wecannowalsogeneralizethe tothetwo-setcase. Themainchangehereisthedefinitionof
NWJ
I
F(M)asitneedstobedefinedovertwosetsasinputs. Wehavethefollowing:
TheoremE.1. ForanyM 2,andpartitionsetsAandBover[M],suchthatA = andB = ,
andforanypositivefunction≥ F(M) : |A| |B| R+,wehave: ̸ ∅ ̸ ∅
(cid:104) X ×X (cid:55)→(cid:105) (cid:104) (cid:105)
I(XA;XB) ≥E
pX1:M
F(M)(XA,XB) −E
pXApXB
eF(M)(XA,XB) +1. (121)
Proof. The proof follows the exact proof of Theorem 2.1 by replacing x
α
and X̸= 1:α
M
by XA and
XB,respectively.
Thus,aslongasonecandefinesuchafunctionF(M),theotherresultsofthispaperfollows.
E.2 RECOVERINGSIMCLR
GeometricandArithmeticPVC Here,weshowthatincaseofM =2andforspecificchoicesof
functionF(2),wecanrecovertheexistinglossobjectiveforSimCLR,i.e.InfoNCE.SettingM =2,
wemakethefollowingobservations:
• InthecaseofM =2,thearithmeticandgeometricaggregationfunctionsresultinthesame
lower-bound.
• RecoveringSimCLRChenetal.(2020a): SubstitutingM = 2inEquations12and13,
we recover the following contrastive loss, which is equivalent to InfoNCE, i.e. SimCLR
objective:
(cid:34) (cid:35)
1 (cid:88)K ef(xi,1,xi,2)
M=2 = E log = . (122)
LPVC − K ef(xi,1,xi,2)+(cid:80) (cid:80)2 ef(xj,γ,xi,2) LInfoNCE
i=1 j̸=i γ=1
Lettingf(x,y)= x·y leadustotheexactSimCLRloss.
∥x∥∥y∥
SufficientStatistics Wecanalsoshowthatinthesufficientstatisticsloss(Equation22),thecase
ofM = 2andthechoiceofQ = T (Equation24)recoverstheSimCLRloss. Toprovethis,note
α˜
thefollowingobservations:
• WhenM =2,X̸=α =x ,i.e.ifα=1,X̸=α =x andifα=2,X̸=α =x .
1:M 3−α 1:M 2 1:M 1
• ByEquation24,Q(X̸=α )= 1 (cid:80)M T(x )=T(x )whenM =2. Therefore,in
1:M M−1 β̸=α β 3−α
Equation22,wehavethefollowing:
(cid:34) (cid:35)
= E
1 (cid:88)K 1 (cid:88)2
log
eT iT ,αTi,3−α
LSuffStats − K 2 eT iT ,αTi,3−α +(cid:80)K (cid:80)2 eT iT ,αTj,γ
i=1 α=1 j=1 γ=1
(cid:34) (cid:32)
= E
1 (cid:88)K 1
log
eT iT ,1Ti,2
(123)
− K 2 eT iT ,1Ti,2 +(cid:80)K (cid:80)2 eT iT ,1Tj,γ
i=1 j=1 γ=1
(cid:33)(cid:35)
eT iT ,2Ti,1
+log ,
eT iT ,2Ti,1 +(cid:80)K (cid:80)2 eT iT ,2Tj,γ
j=1 γ=1
28PublishedasaconferencepaperatICLR2024
whichisthesymmetricInfoNCE.ChoosingT(x)= x recoverstheSimCLRobjective.
∥x∥
E.3 SIGLIPCONNECTIONTOMIBOUND
WeshowthattheobjectiveintroducedinZhaietal.(2023b)isanMIbound. Asofourbestunder-
standing,thisisnotpresentintheexistingliterature.
SigLIPisanMIbound: AsshownintheproofofTheorem2.2,toachievethelower-boundswe
defineF(M)tohaveaSoftmax-basedform(seeAppendixC.5formoredetails).However,wecould
chooseotherformsoffunctions. IfwereplaceSoftmaxwithaSigmoid-basedform,wecanrecover
theSigLIPloss,i.e.:
1
F(2)(x ,x )=log z =1if(i=j)else 1. (124)
i,1 j,2 1+ezi,j(−txi,1·xj,2+b) i,j −
FollowingthesameprocedureastheproofofTheorem2.2,anddefiningtheF(2)overpositivesand
negatives as F(2)(X ) = 1 (cid:80)K (cid:80)K log 1 . This shows that SigLIP is
1:K,1:2 K i=1 j=1 1+ezi,j(−txi,1·xj,2+b)
also a MI bound. Zhai et al. (2023b) has a discussion on the importance of having the bias term
(b) in the practical setting to alleviate the imbalance effect of negatives in the initial optimization
steps. However,itwouldbeofafutureinteresttoseewhetherthegeneralizationofSigLIPbyeither
arithmetic or geometric aggregation function to poly-view setting would help to remove the bias
term.
E.4 SUFFICIENTSTATISTICSEXTENSION
Sofar,wehaveassumedthatthereisgenerativefactorcaffectingthesamples. However,inamore
generalcase, wehavemultiplefactorsaffectingthesamplegeneration. Letusconsiderthecausal
graphpresentedinFigure4. Here,weassumethatthemainfactorsimportantforthedown-stream
taskaredenotedbyc, calledcontent, whilethenon-relatedfactorsareshownbys , calledstyles.
α
Thestylescanbedifferentamongviewswhilethetask-relatedfactorciscommonamongthemall.
c
Figure4: Content-Style causal graph A
more general poly-view sample generation
withtask-relatedgenerativefactorc, called
content. For each α ∈ [M], s α shows the
... view-dependent and task non-related fac-
s s s
1 2 M tors, calledstyles. Theviewsareshownas
Styles beforebyx α.Inthemostgeneralcase,con-
tentandstylesarenotindependent,whilein
somesettingstheymightbeindependent.In
...
theindependentscenario,thearrowsfromc
x 1 x 2 x M tos αcanbeignored.
Views
ThegoalofthissectionistogeneralizetheapproachofsufficientstatisticsintroducedinSection2.4
tothecaseofcontent-stylecausalgraph. Westartwithassumingthatcontentandstyleareindepen-
dentandthenmovetothegeneralcaseofdependentfactors.
Independent content and style In this scenario, the arrows in Figure 4 from c to s will be
α
ignoredasthereisnodependencybetweenthesetwofactors. Weshowthatforanyα [M], the
∈
sufficientstatisticsofx withrespectto c,s hastightrelationswithsufficientstatisticsofx to
α α α
{ }
cands separately.
α
TheoremE.2. InthecausalgenerativegraphofFigure4,ifc s ,thenwehave:
α
⊥⊥
T (x)=(T (x), T (x)) (125)
{c,sα} c sα
Proof. Having independent factors means p(x c,s ) = p(x c)p(x s ). Now, using the
α α α α α
| | |
Neyman-Fisher factorization (Halmos & Savage, 1949) for sufficient statistics, alongside assum-
ing that we have exponential distribution families, similar to Section 2.4, we have the following
29PublishedasaconferencepaperatICLR2024
factorization:
(cid:0) (cid:1)
p(x c,s )=R(x)C(c,s )exp T (x) Q (c,s ) (126)
α
|
α α {c,sα}
·
{c,sα} α
p(x c)=r (x)c (c)exp(T (x) Q (c)) (127)
α 1 1 c c
| ·
p(x s )=r (x)c (s )exp(T (x) Q (s )). (128)
α
|
α 2 2 α sα
·
sα α
Substitutingthesefactorizationsinthedefinitionofindependentgenerativefactorsresultsin:
p(x c,s )=p(x c)p(x s ) (129)
α α α α α
| | |
=(r (x)c (c)exp(T (x) Q (c)))(r (x)c (s )exp(T (x) Q (s ))) (130)
1 1 c
·
c 2 2 α sα
·
sα α
=R(x)C(c,s )exp((T (x),T (x)) (Q (c),Q (s ))), (131)
α c sα
·
c sα α
whichcompletestheproofbyshowing:
T (x)=(T (x), T (x)) (132)
{c,sα} c sα
Q (c,s )=(Q (c), Q (s )) (133)
{c,sα} α c sα α
NotethatTheoremE.2alsoshowsthatifthespaceofgenerativefactorcinFigure1bisadisentan-
gledspaceoftwoormorespaces, i.e. = , thenthesufficientstatisticsofxwithrespect
1 2
C C ⊗C
tocisequaltoaconcatenationofsufficientstatisticsofxwithrespecttoc andc ,i.e.sufficient
1 2
statisticskeepsthedisentanglement.
Dependent content and style When the factors are dependent, the sufficient statistics becomes
anentangledmeasureofcands . However, ifweassumex = f(c,s )foranyα [M]anda
α α α
∈
specificfunctionf : ,wehavethefollowingtheorem:
C×S (cid:55)→X
TheoremE.3. InthecausalgenerativegraphofFigure4,assumeforanyα [M],x =f(c,s )
α α
∈
foranunknowninvertiblefunctionf,suchthat
c=f−1(x ) s =f−1(x ) , (134)
α nc α α ns
wheren andn showtheelementsoff−1(x )thatiscorrespondedtocands respectively. Then,
c s α α
T (x )=T (x ) α,β [M]. (135)
c α c β
∀ ∈
Proof. Assumethats ands aresampledi.i.dfromp . Then,wehave:
α β s|c
p(x=x c)=p(f(c,s)=x c) (136)
α α
| |
=δ p(s=s =f−1(x ) c) (137)
c α α ns|
=δ p(s=s c) (138)
c β
|
=δ p(s=s =f−1(x ) c) (139)
c β β ns|
=p(f(c,s)=x c) (140)
β
|
=p(x=x c) (141)
β
|
Thus, p(x c) = p(x c), which using the Neyman-Fisher factorization and exponential family
α β
| |
distribution,resultsinT (x )=T (x ). Thiscompletestheproof.
c α c β
NotethattheresultofTheoremE.3recoverstheresultofvonKügelgenetal.(2021)usingtheidea
ofsufficientstatistics.
E.5 OPTIMALMULTIPLICITYINTHEFIXEDBATCHSETTING
InthepreviousresultsofArithmeticandGeometricPVC(Theorem2.2),weassumedthatM canbe
anynumber,andaccordinglythetotalnumberofviewsB = K M forafixedK increasesifM
×
increases. However,itisinterestingtoinvestigatetheFixedBatchscenariooutlinedinSection3.2
which corresponds to holding B fixed by reducing K when M is increased. What is the optimal
multiplicityM∗ forthebottomrowresultsofFigure3? Wefirstnotethatduetothecomplexityof
30PublishedasaconferencepaperatICLR2024
MIlower-boundsinEquations12and13,itisnottrivialtoanswerthisquestionasitwilldependon
thebehavioroffunctionf andthemaph.
Here, we attempt to provide a simplified version of Geometric PVC by adding some assumptions
on the behavior of f and h. Although this result is for a simplified setting, we believe it provides
aninterestinginsightthatforafixedbatchsizeB,dependingonthefunctionsf andh,thereisan
optimalnumberofmultiplicityM⋆whichmaximizesthelower-bound.Whileeveninthissimplified
version,itiscomputationallychallengingtocomputeM⋆exactly,itispossibletoproveitsexistence.
In the case that B is fixed, we can rewrite the Geometric PVC in Equation 13 as following by
replacingK = B:
M
 
(cid:16) (cid:17) 1 (cid:88)K 1 (cid:88)
I
x α;X̸= 1:α
M
≥c(B,M)+E 
K M 1
log ℓ i,α,β (142)
i=1 − β̸=α
 
B
M (cid:88)M 1 (cid:88)
=c(B,M)+E  log ℓ i,α,β (143)
B M 1
i=1 − β̸=α
= (144)
Geometric
I
To prove that there is an optimal M, we need to show that there is M⋆ such that ∂I Geometric = 0 at
∂M
pointM = M⋆. Sinceℓ dependsonfunctionsf andh(seeEquation14),andthesefunctions
i,α,β
areinpracticetrained,weassumethatforalongenoughtrainingoftheircorrespondingneuralnet-
works,ℓ convergestoitsoptimumvaluedenotedbyℓ⋆ . Moreover,weassumethatnegative
i,α,β i,α,β
samplesareuniformlydistributedonthehypersphere(followingtheWang&Isola(2020)benefits
of uniformity criteria). Also, we assume that the convergence of ℓ⋆ to its desired value of 15,
i,α,β
happens when M in a linear way. Note that the first part of this assumption is not limiting
→ ∞
since we know as M grows, the lower bound becomes tighter. Due to the fact that one point is
M ,therewillbemanymappingsthatfollowthedesiredbehaviorofconvergingtooneasM
grow→ s.∞ Here,weintroducetwochoicesforthemappingℓ⋆ withthecorrectlimitingbehaviorin
i,α,β
M toinvestigatetheimportanceofmappingchoiceontheoptimumvalueofM:
M 2
1. ℓ⋆ (M)=p⋆+ − (1 p⋆)=ℓ⋆(M), (145)
i,α,β M −
1 p⋆
2. ℓ⋆ (M)=1 − =ℓ⋆(M), (146)
i,α,β − M 1
−
where in both equations, p⋆ = ℓ⋆ (M = 2) and lim ℓ⋆ (M) = 1. Note that the uni-
i,α,β M→∞ i,α,β
formityassumption helpstohave thesamevalue ofp⋆ forany α and β. Now, we canrewritethe
asfollowing:
Geometric
I
 
B
M (cid:88)M 1 (cid:88)
IGeometric =c(B,M)+E  B M 1 log ℓ⋆(M) (147)
i=1 − β̸=α
=c(B,M)+log ℓ⋆(M). (148)
Now,wecancompute ∂I Geometric =0. Wehave,
∂M
∂ ∂c(B,M) 1 ∂ℓ⋆(M)
IGeometric = + (149)
∂M ∂M ℓ⋆(M) ∂M
1 1 ∂ℓ⋆(M)
= + (150)
−B M +1 ℓ⋆(M) ∂M
−
=0. (151)
5Thedesiredvalueofℓ i,α,β isoneasitisalikelihood.
31PublishedasaconferencepaperatICLR2024
Therefore, finding the solution of
∂ℓ⋆(M)
=
ℓ⋆(M)
would provide us with M⋆. For each of
∂M B−M+1
choicesofℓ⋆(M)inEquations145and146,wehave:
2 M⋆ 2(1 p⋆)
1. (1 p⋆) − − =0 (152)
M⋆ − − B M⋆+1
−
1 p⋆ (M⋆ 1) (1 p⋆)
2. − − − − =0. (153)
M⋆ 1 − B M⋆+1
− −
Solvingtheseequationsresultsinthefollowingoptimumvalueforeach:
(cid:112)
1. M⋆ = 2(B+1)(1 p⋆) (154)
−
(cid:112)
2. M⋆ =1+ B(1 p⋆). (155)
−
Itisinterestingtoseethatp⋆ playsaroleintheoptimumvalueofM,whichshowstheimportance
ofthedesignofscalarfunctionf andthemaph. ThedifferencesbetweenthevaluesofM⋆ inthe
twoscenariosalsoemphasizetheimportanceofthebehaviorofthecontrastivelossasM increases.
E.6 SYNTHETIC1DGAUSSIAN
FollowingthesyntheticsettinginSection3.1,here,weshowthefollowingresults:
• Providetheprooffortheclosedformofone-vs-restMI.
• EvaluatetheconvergenceoftheconditionaldistributionforlargeM tothetruedistribution.
Closed form of one-vs-rest MI We start with finding the closed form for joint distributions,
p(X ) and p(x )p(X̸=α ). Since all the samples and their views are Gaussian, the joint dis-
1:M α 1:M
tribution will also be Gaussian. Thus, it is enough to find the covariance matrix of each density
function;notethatthemeanissettozeroforsimplicity.
(cid:90) (cid:90)
E[x ]= p(c=c)E[x c=c] dc= cp(c=c)dc=E[c]=0, (156)
α α
|
(cid:90) (cid:90)
Var[x ]=E(cid:2) x2(cid:3) = p(c=c)E(cid:2) x2 c=c(cid:3) dc=σ2+ c2p(c=c)dc=σ2+σ2, (157)
α α α| 0
(cid:90)
Cov(x ,x )=E[x x ]= p(c=c)E[x x c=c] dc (158)
α β α β α β
|
(cid:90) (cid:90)
= p(c=c)E[x c=c]E[x c=c] dc= c2p(c=c)dc=σ2. (159)
α | β | 0
Thus,ifwepresentthecovariancematricesofp(X 1:M)andp(x α)p(X̸= 1:α M)byΣ
M
andΣ(cid:101)M respec-
tively,wehavethefollowing:
σ2+σ2 σ2 ... σ2 
0 0 0
 σ 02 σ2+σ 02 ... σ 02  (cid:20) σ2+σ2 0 (cid:21)
Σ M =  . . . . . . ... . . .  , Σ(cid:101)M = 0 0 Σ M−1 . (160)
σ2 σ2 ... σ2+σ2
0 0 0
Consequently,wecanwritethedensityfunctionsasfollows:
(cid:18) (cid:19)
p(X 1:M)=(2π)−M 2 det(Σ M)− 21 exp 1 2xTΣ− M1x , (161)
(cid:16) (cid:17)−1 (cid:18) 1 (cid:19)
p(x α)p(X̸= 1:α M)=(2π)−M 2 det Σ(cid:101)M 2 exp 2xTΣ˜− M1x , (162)
where x = (x ,...,x ). Now, we can compute the closed form for one-vs-rest MI using the
1 M
followinglemma:
LemmaE.4. AssumexandyaretwomultivariateGaussianrandomvariablesofsizenwithlaws
and ,covariancematricesΣ andΣ ,andmeanvectorsµ ,µ ,respectively. Then,
x y x y x y
N N
(cid:18) (cid:18) (cid:19)(cid:19)
( )=1 tr(cid:0) Σ−1Σ (cid:1) +(µ µ )TΣ−1(µ µ ) n+log det(Σ y) . (163)
DKL Nx ∥Ny 2 y x y − x Y y − x − det(Σ )
x
32PublishedasaconferencepaperatICLR2024
Therefore,theone-vs-restMIwillbeasfollows:
(cid:16) (cid:17)
(x ;X̸=α )= p(X ) p(x )p(X̸=α ) (164)
I α 1:M DKL 1:M ∥ α 1:M
  (cid:16) (cid:17)
1 (cid:16) (cid:17) det Σ(cid:101)M
= 2tr Σ(cid:101)− M1Σ
M
−M +log
det(Σ )
. (165)
M
DefineA=σ2I,B =σ21andC =A+B. OnecaneasilyseethatAandB commute;therefore,
0
theyaresimultaneouslydiagonizable. Thus,thereexistsmatrixP suchthatthefollowingholds:
A=PD P−1, B =PD P−1, C =P(D +D )P−1, (166)
A B A B
whereD andD showthediagonalizedmatrices. Wealsoknowthatdet(C) =
(cid:81)M
λ (D +
A B i=1 i A
D ),whereλ denotesthei-theigenvalueofmatrixD +D .Duetothestructureofthematrices
B i A B
AandB,wecanshowD =σ2I,andD isasfollows:
A B
M 0 ... 0
0 0 ... 0
 
D B =σ 02  . . . . . . ... . . . . (167)
0 0 ... 0
Asaresultwehavethefollowing:
det(Σ
)=(cid:0) σ2(cid:1)M−1(cid:0) σ2+Mσ2(cid:1)
(168)
M 0
det(cid:16) Σ(cid:101)M(cid:17) =(cid:0) σ2+σ 02(cid:1) det(Σ M−1)=(cid:0) σ2+σ 02(cid:1)(cid:0) σ2(cid:1)M−2(cid:0) σ2+(M −1)σ 02(cid:1) (169)
Also,Σ(cid:101)− M1Σ
M
hasablockmatrixmultiplicationform:
(cid:18)(cid:20)
σ2+σ2 0
(cid:21)(cid:19)−1(cid:20)
σ2+σ2 v
(cid:21)
Σ(cid:101)− M1Σ
M
=
0
0
Σ vT
0
Σ
(170)
M−1 M−1
(cid:20) (σ2+σ2)−1 0 (cid:21)(cid:20) σ2+σ2 v (cid:21)
= 0 0 Σ−1 vT 0 Σ (171)
M−1 M−1
(cid:20)
1
(σ2+σ2)−1v(cid:21)
= Σ−1 vT I0 (172)
M−1
(cid:16) (cid:17)
Wherev = (σ 02,...,σ 02)isa 1 ×(M −1)matrix. Therefore, tr Σ(cid:101)− M1Σ
M
= M. Thus, forthe
one-vs-restMI,wehave:
1 (cid:18)(cid:18) σ2(cid:19)(cid:18) σ2 (cid:19)(cid:19)
(x ;X̸=α )= log 1+ 0 1 0 (173)
I α 1:M 2 σ2 − σ2+Mσ2
0
Convergence of conditional distribution Using the covariance matrices in Equation 160, and
expanding the distributions p(X ) and p(X̸=α ), we can write the conditional distribution
i,1:M i,1:M
p asfollows,whichhelpsustoevaluateEquation19:
xi,α|X̸= i,1α
:M
1 (cid:18) σ2 (cid:19) (cid:34) (cid:80)M (x x¯ )2 Mx¯2
p = 1 0 exp α=1 i,α − i i
xi,α|X̸= i,1α :M √2πσ2 − σ2+Mσ 02 − 2σ2 − 2(σ2+Mσ 02)
(174)
+ (cid:80)M β̸=α(x i,β −x¯̸= i α)2 + (M −1)(x¯̸= i α)2 (cid:35) ,
2σ2 2(σ2+(M 1)σ2)
− 0
wherex¯ andx¯̸=αaretheaverageofX andX̸=α ,respectively. AsM increases, Mx¯2 i
i i i,1:M i,1:M 2(σ2+Mσ2)
0
(M−1)(x¯̸=α)2 c2
and i convergeto i ,whichresultsinthesetermscancellingeachother. Therefore,
2(σ2+(M−1)σ2) 2σ2
0 0
wehave:
(cid:34)
1 (x x¯ )2
i,α i
lim p = lim exp −
M→∞ xi,α|X̸= i,1α :M M→∞√2πσ2 − 2σ2
. (175)
(cid:80) (x x¯̸=α)2 (x x¯ )2(cid:35)
+ β̸=α i,β − i − i,β − i
2σ2
33PublishedasaconferencepaperatICLR2024
Table1:HyperparametersforallImageNet1kexperimentsinSection3.2
ResNet50
Weightinitialization kaiming_uniform(Heetal.,2015)
Backbonenormalization BatchNorm
Headnormalization BatchNorm
SynchronizedBatchNormoverreplicas Yes
Learningrateschedule SingleCycleCosine
Learningratewarmup(epochs) 10
Learningratebasevalue 0.2× 4096 =3.2
256
Learningrateminimumvalue 0
Optimizer LARS(Youetal.,2017)
Optimizerscalingrule None
Optimizermomentum 0.9
Gradientclipping None
Weightdecay 1×10−4
Weightdecayscalingrule None
Weightdecayskipbias Yes
Numericalprecision bf16
Augmentationstack SimCLR(Chenetal.,2020a)
Usingthecentrallimittheorem,x¯ convergestoc ,andthesecondtermconvergestozero,yielding
i i
lim p =p . (176)
M→∞
xi,α|X̸= i,1α
:M
xi,α|ci
F REAL-WORLD IMAGE REPRESENTATION LEARNING
F.1 EXPERIMENTALDETAILS
Hyperparameters WepresentthebasefortrainingSimCLR(Chenetal.,2020a)andothermulti-
viewmethodswithaResNet50(Heetal.,2016)inTable1.
Augmentations We use SimCLR augmentations throughout (Chen et al., 2020a), with
color_jitter_strength = 1.0 and an image size override of 224 224. For complete-
×
ness, we provide our training augmentation here, our testing augmentation is the standard resize,
center crop and normalize, and general multiplicity M corresponds to sampling M independent
transformations.
[
transforms.RandomResizedCrop(
image_size_override, scale=crop_scale, interpolation=Image.BICUBIC
),
transforms.RandomHorizontalFlip(p=0.5),
transforms.RandomApply(
[
transforms.ColorJitter(
brightness=0.8 * color_jitter_strength,
contrast=0.8 * color_jitter_strength,
saturation=0.8 * color_jitter_strength,
hue=0.2 * color_jitter_strength,
)
],
p=0.8,
),
transforms.RandomGrayscale(p=0.2),
transforms.RandomApply([M.GaussianBlur([0.1, 2.0])], p=0.5),
transforms.ToTensor(),
IMAGENET_NORMALIZE,
]
Data All experiments in Section 3.2 are performed on ImageNet1k (Russakovsky et al., 2014).
Thisdatasetiscommonlyusedincomputervisionandcontains1.28Mtraining,50Kvalidationand
100Ktestimagesofvaryingresolutions,eachwithalabelfromoneof1000objectclasses.
34PublishedasaconferencepaperatICLR2024
Table2:Hyperparametersforfinetuningexperiments
ResNet50
Headweightinitialization 0(Beyeretal.,2022)
Headbiasinitialization lnn−1 (Beyeretal.,2022)
classes
SynchronizedBatchNormoverreplicas No
Learningrateschedule SingleCycleCosine
Learningratewarmup(epochs) 5
Learningratebasevalue {0.0001,0.001,0.001}
Learningrateminimumvalue 0
Batchsize {384,1024}
Trainingepochs {300,1000,2000,4000}
Optimizer SGD
Optimizerscalingrule SGD
Optimizerbasebatchsize 256
Optimizermomentum 0.9
Gradientclipping None
Weightdecay 0.0
Numericalprecision fp32
Augmentationstack RandAug(Cubuketal.,2020)
RepeatedAug. 2
RandAug 9/0.5
Mixupprob. 0.8
Cutmixprob. 1.0
RandomErasingprob. 0.25
F.2 FINE-TUNINGRESULTSONTRANSFERTASKS
Toinvestigatewhetherpoly-viewmethodsimprovetransferlearningperformance,weevaluatedthe
ImageNet1kpre-trainedmodelsofSection3.2byfine-tuningallmodelweightsforanewtask.
Datasets Following SimCLR (Chen et al., 2020a) we investigated transfer learning performance
ontheFood-101dataset(Bossardetal.,2014),CIFAR-10andCIFAR-100(Krizhevskyetal.,2014),
SUN397(Xiaoetal.,2010),StanfordCars(Krauseetal.,2013),Aircraft(Majietal.,2013),theDe-
scribableTexturesDataset(DTD)(Cimpoietal.,2014),Oxford-IIITPets(Parkhietal.,2012),and
Caltech-101(Fei-Feietal.,2007). Wereporttop-1accuracyforalldatasets,andusethepredefined
train,validationandtestsplitsintroducedbythedatasetcreators.
Hyperparameters Fine-tuning hyperparameters are summarized in Table 2. Hyperparameters
areoptimizedfortheSimCLRmodelonlyandarethenre-usedforthepoly-viewmethods,following
the experimental protocol outlined in Section 3.2. All fine-tuning is performed using SGD using
momentum. Fine-tuning on smaller datasets is done for a larger number of (e.g. 4k epochs for
Aircraft)andwithalowerlearningrate(e.g. 10−4 forCaltech-101). Fine-tuningheadweightsare
initializedtozero, withheadbiasinitializedtolnn−1 , wheren isthenumberofclassesin
classes classes
thecorrespondingdownstreamdataset(Beyeretal.,2022).
Results In Table 3 we report the test top-1 accuracy after fine-tuning. We do this for the Geo-
metricPVCmodelswithbothbatchstrategiesintroducedinSection3.2: GrowingBatchandFixed
Batch, as well as the SimCLR model, for small (256 epochs) and large (1024 epochs) amounts of
pretraining.Inallcases,Poly-viewmethodsmatchoroutperformtheSimCLRbaselineforthesame
set of hyperparameters. We also observe that the Geometric PVC method trained for 256 epochs
outperformstheSimCLRmethodtrainedfor1024epochsontransfertoFood,SUN297,Cars,Pets,
andCaltech-101,reinforcingthecomputationalefficiencyfindingsofSection3.2.
35PublishedasaconferencepaperatICLR2024
Model Food CIFAR10 CIFAR100 SUN397 Cars Aircraft DTD Pets Caltech-101
Pre-trainingEpochs:256
GeometricPVC(growing) 89.56 98.33 87.38 65.91 93.47 81.13 72.41 91.31 88.27
GeometricPVC(shrinking) 90.08 98.28 87.63 66.12 93.61 81.61 73.59 91.05 88.77
SimCLR 88.58 97.76 86.55 65.10 92.96 78.54 71.44 90.51 86.49
Pre-trainingEpochs:1024
GeometricPVC(growing) 90.00 98.31 87.66 66.59 93.58 80.58 73.24 90.78 89.80
GeometricPVC(shrinking) 90.27 98.41 87.73 65.68 93.78 82.42 73.30 90.88 88.57
SimCLR 89.42 98.17 86.70 65.24 93.48 80.72 72.60 90.36 89.66
Table3:Comparisonoftransferlearningperformanceofpoly-viewGeometricPVCagainstabaselineSimCLR
for the same hyperparameter set across nine natural image datasets. Geometric (growing) and Geometric
(shrinking) correspond to Geometric PVC (M = 8) using Growing Batch and Shrinking Batch strategies
respectively (see Section 3.2). Top: ImageNet1k models pre-trained for 256 epochs; bottom: ImageNet1k
modelspre-trainedfor1024epochs.Resultsnotsignificantlyworsethanthebest(bootstrapconfidenceinterval
of90%)areshowninbold.
F.3 THEROLEOFAUGMENTATIONSTRENGTHATHIGHMULTIPLICITY
WepresenttheroleofaugmentationstrengthathighmultiplicityinFigure5,investigatingtheeffect
of different color jittering (Figure 5a) and cropping (Figure 5b). We do not observe significantly
differentqualitativebehaviorbetweentheSimCLRbaselineandthepoly-viewmethods.
GrowingBatch
GrowingBatch(Kconstant)
70.0
69
67.5
68 65.0
67 62.5
60.0
66
57.5
65 55.0
64 52.5
FixedBatch(M Kconstant)
FixedBatch 70.0 ×
69
67.5
65.0
68
62.5 SimCLR
67 S Gi em oC mL etR ric 60.0 G Ae rio thm met er ti ic c
66 Arithmetic 57.5
55.0
65
52.5
64
0.25 0.50 0.75 1.00 1.25 1.50 1.75
(0.08,0. (2 0.0) 08,0. (4 0.0) 08,0. (6 0.5) 08,0. (8 0.0) 08,1. (0 0.0) 15,1. (0 0.0) 25,1. (0 0.0) 40,1. (0 0.0) 65,1.00)
ColorJitterStrength CropScale
(a)Varyingcolorstrength. (b)Varyingcroppingstrategy.
Figure5:ResNet50trainedfor128epochswithdifferentobjectivesfordifferentstrengthsofcoloraugmenta-
tion(a)andcroppingstrategy(b).GeometricandArithmeticmethodspresentedusemultiplicityM =4.
36
)%(1poTtseT
)%(1poTtseT
)%(1poTtseT
)%(1poTtseTPublishedasaconferencepaperatICLR2024
F.3.1 COMPARISONOFTOTALFLOATINGOPERATIONS
In Section 3.2 and Figure 3a, Relative Compute (Equation 29), which measures the total number
of encoded views during training, was used to quantify the practical benefits of using Poly-View
methods.
To quantify the overall training budget, in Figure 6 we report the total number of FLOPs (FLoat-
ing OPerations) performed during training. This is the total number of FLOPs in the forward and
backwardpassesforeverytrainingstepofthemodelasmeasuredbythePyTorchprofiler6.
The conclusion of Section 3.2 are unchanged when considering total FLOPs instead of Relative
Compute. This happens because for sufficiently large models like the ResNet50 we use here, the
FLOPscomputationisdominatedbythemodelencoderandnotthelosscomputation. Thisresults
inRelativeComputebeingaproxyfortotalFLOPs.
GrowingBatch
70
65
64 128 256 512 1024 0.0 0.1 0.2 0.3 1018 1019 1020
FixedBatch
70
SimCLR
Geometric
65 Sufficient
Multi-Crop
64 128 256 512 1024 0.0 0.5 1.0 1018 1019 1020
Epochs Updates 106 TotalFLOPs
×
Figure6:TrainingatmultiplicityM =8varyingtrainingepochs.
Figure7:ContrastiveResNet50trainedonImageNet1kfordifferentepochsorwithdifferentviewmultiplicities.
Blue,red,orangeandblackdashedlinesrepresentGeometric,Multi-Crop,SufficientStatistics,andSimCLR
respectively. Bands indicate the mean and standard deviation across three runs. Points indicate final model
performanceofcorrespondinghyperparameters. WeuseK = 4096forGrowingBatchandK = (2/M)×
4096forFixedBatch.EachmethodistrainedwithamultiplicityM =8excepttheM =2SimCLRbaseline.
We compare models in terms of performance against training epochs (left), total updates (middle) which is
affectedbybatchsizeK,andtotalFLOPs.
6https://pytorch.org/docs/stable/profiler.html
37
)%(1poTtseT
)%(1poTtseTPublishedasaconferencepaperatICLR2024
F.3.2 IMPLEMENTATIONOFLOSSFUNCTIONS
Thepseudocodesforpoly-viewcontrastivelossandsufficientstatisticscontrastivelossarepresented
in Algorithms 1 and 2 respectively. Both algorithms have the same structure, with the definition
of score matrix as the primary difference. The rearrange and repeat functions are those of
einops(Rogozhnikov,2022).
Algorithm1Poly-ViewContrastiveLosspseudocode.
# net - encoder + projector network
# aug - augmentation policy
# X[k, h, w, c] - minibatch of images
# tau - temperature
def get_mask(beta: int) -> Tensor:
"""The self-supervised target is j=i, beta=alpha. Produce a mask that
removes the contribution of j=i, beta!=alpha, i.e. return a [k,m,k]
tensor of zeros with ones on:
- The self-sample index
- The betas not equal to alpha
"""
# mask the sample
mask_sample = rearrange(diagonal(k), "ka kb -> ka 1 kb")
# mask the the beta-th view
mask_beta = rearrange(ones(m), "m -> 1 m 1")
mask_beta[:, beta] = 0
return mask_beta * mask_sample # [k, m, k]
# generate m views for each sample
X_a = cat([X_1, X_2, ..., X_m], dim=1) = aug(X) # [k, m, h, w, c]
# extract normalized features for each view
Z = l2_normalize(net(X_a), dim=-1) # [k, m, d]
# build score matrix
scores = einsum("jmd,knd->jmnk", Z, Z) / tau # [k, m, m, k]
# track the losses for each alpha
losses_alpha = list()
# iterate over alpha and beta
for alpha in range(m):
losses_alpha_beta = list()
for beta in range(m):
# skip on-diagonal terms
if alpha != beta:
logits = scores[:, alpha] # [k, m, k]
labels = arange(k) + beta * k # [k]
mask = get_mask(beta) # [k, m, k]
logits = flatten(logits - mask * LARGE_NUM) # [k, m * k]
loss_alpha_beta = cross_entropy(logits, labels) # [k]
losses_alpha_beta.append(loss_alpha_beta)
losses_alpha = stack(loss_alpha, dim=-1) # [k, m-1]
# aggregate over the betas for each alpha
if method == "arithmetic":
loss_alpha = logsumexp(losses_alpha, dim=-1) - log(k) # [k]
elif method == "geometric"
loss_alpha = mean(losses_alpha, dim=-1) # [k]
losses_alpha.append(loss_alpha)
# build final loss matrix
losses = stack(losses_alpha, dim=-1) # [k,m]
# take expectations
sample_losses = mean(losses, dim=-1) # [k]
loss = mean(sample_losses) # scalar
38PublishedasaconferencepaperatICLR2024
Algorithm2SufficientStatisticsContrastiveLosspseudocode.
# net - encoder + projector network
# aug - augmentation policy
# X[k, h, w, c] - minibatch of images
# tau - temperature
def get_mask(beta: int) -> Tensor:
"""The self-supervised target is j=i, beta=alpha. Produce a mask that
removes the contribution of j=i, beta!=alpha, i.e. return a [k,m,k]
tensor of zeros with ones on:
- The self-sample index
- The betas not equal to alpha
"""
# mask the sample
mask_sample = rearrange(diagonal(k), "ka kb -> ka 1 kb")
# mask the the beta-th view
mask_beta = rearrange(ones(m), "m -> 1 m 1")
mask_beta[:, beta] = 0
return mask_beta * mask_sample # [k, m, k]
# generate m views for each sample
X_a = cat([X_1, X_2, ..., X_m], dim=1) = aug(X) # [k, m, h, w, c]
# extract normalized features for each view
Z = l2_normalize(net(X_a), dim=-1) # [k, m, d]
# build the average of the rest-set
# step 1: repeat the features M times
Z_tilde = repeat(Z, "k m1 d -> k m1 m2 d", m2=m) # [k, m, m, d]
# step 2: replace the effect of alpha-th view by zero
# and correct the bias coefficient of mean
diagonal_one = rearrange(eye(m), "m1 m2 -> 1 m1 m2 1")
diagonal_zero = ones([k, m, m, d]) - diagonal_one # [k, m, m, d]
Z_tilde = m / (m - 1) * Z_tilde * diagonal_zero # [k, m, m, d]
# step 3: getting the average of rest-set and nomalize
Z_tilde = mean(Z_tilde, dim=1) # [k, m, d]
Z_tilde = l2_normalize(Z_tilde, dim=-1) # [k, m, d]
# build score matrix
scores = einsum("jmd,knd->jmnk", Z, Z_tilde) / tau # [k, m, m, k]
# track the losses for each alpha
losses_alpha = list()
# iterate over alpha and beta
for alpha in range(m):
losses_alpha_beta = list()
for beta in range(m):
# skip non-diagonal terms
if alpha == beta:
logits = scores[:, alpha] # [k, m, k]
labels = arange(k) + beta * k # [k]
mask = get_mask(beta) # [k, m, k]
logits = flatten(logits - mask * LARGE_NUM) # [k, m * k]
loss_alpha_beta = cross_entropy(logits, labels) # [k]
losses_alpha_beta.append(loss_alpha_beta)
losses_alpha = stack(loss_alpha, dim=-1) # [k, m-1]
# aggregate over the betas for each alpha
loss_alpha = mean(losses_alpha, dim=-1) # [k]
losses_alpha.append(loss_alpha)
# build final loss matrix
losses = stack(losses_alpha, dim=-1) # [k,m]
# take expectations
sample_losses = mean(losses, dim=-1) # [k]
loss = mean(sample_losses) # scalar
39PublishedasaconferencepaperatICLR2024
G EXPANDED RELATED WORK
SSLmethodsandcontrastivelearning ContrastivelearningappearsinmanySSLmethods.Sim-
CLR(Chen&He,2021)leveragestheInfoNCEobjectivetotraintheencoderstofindgoodrepre-
sentations. MoCo (He et al., 2019; Chen et al., 2020b; 2021) uses a momentum encoder to create
a moving average of the model’s parameters, enabling it to learn powerful image representations
withouttheneedforlabeleddata. CLIP(Radfordetal.,2021)isanovelmulti-modalapproachthat
leverages contrastive learning to bridge the gap between text and images. VICReg (Bardes et al.,
2021) is another SSL method that uses contrastive learning but also address the collapse problem
inwhichtheencodersproducenon-informativevectorsusingregularizationterms. Therearesome
works(Shwartz-Zivetal.,2023;Balestriero&LeCun,2022)providingtheoreticalunderstandingof
VICReg’sperformanceandcomparingittotheothermethodslikeSimCLR.Tianetal.(2020a)is
theclosestworkweknowofthathasinvestigatedasimpleformofmultiplicityviewincontrastive
learning. Theirapproachistogettheaverageofpairwisecontrastivelearning, whichtranslatesto
thelower-boundofValiditypropertythatwehave,forwhichwehaveshowntheoreticallyinEqua-
tion8thatouraggregationfunctionoutperformsthislower-bound. Wealsonotethattheauthorsdid
notprovideanytheoreticalexplanationregardingmultiplicity.
Information-theoretic perspective in SSL One of the main approaches to understand SSL and
contrastivelearningistostudythedependenciesbetweenpairsofvariablesorviews.MIprovidesan
insightfulmetricforquantifyingdependenciesresultingtothepointthatestimatingandoptimizing
theMIbecomesimportant. vandenOordetal.(2018)introducesInfoNCElossforthefirsttime. It
combinespredictingfutureobservationswithaprobabilisticcontrastiveloss,hencethenameCon-
trastivePredictiveCoding. Theintuitionbehindthisworkisthatdifferentpartsofthesignalshare
sameinformation. Pooleetal.(2019)providesaframeworktoestimateMIbyshowingconnections
between different MI lower-bounds, and investigating the bias and variance of their sample-based
estimators. Tschannen et al. (2020) leverages this framework and builds connection between MI
maximization in representation learning and metric learning by also pinpointing that under which
dependency conditions MI approaches perform well. Lee et al. (2023) provides more insights on
MI maximization in contrastive learning like the effect of same-class-sampling for augmentations
byupper-boundingtheMI.Gálvezetal.(2023)showsthatnotonlycontrastiveSSLmethods, but
alsoclusteringmethods(Caronetal.,2020;2018),and(partially)distillationmethods(Grilletal.,
2020;Caronetal.,2021)implicitlymaximizeMI.
The role of augmentation in SSL Augmentation is a critical part of SSL methods in computer
vision to keep the task-relevant information (Tian et al., 2020b). Trivial augmentations result in
non-informative representations, preventing the model to find the main features to distinguish be-
tweenpositivesandnegatives,whilehardaugmentationsmakeitdifficultforthemodeltoclassify
the positives from negatives. Balestriero et al. (2022b) tackles this problem by quantifying how
many augmentations are required for a good sample-based estimation of MI to have low variance
and better convergence. Kim et al. (2023) addresses this challenge in contrastive learning with a
different approach and by adding weights that implies the goodness of the augmentation. On the
importance of augmentation, von Kügelgen et al. (2021) shows that augmentation helps to disen-
tanglethecontentfromthestyleintheimages. Fromanotherperspective,someworksexplorethe
effect of different augmentation strategy like multi-crop in contrastive learning and SSL methods
(Caronetal.,2020;2021). Fortetal.(2021)hasasimilarsettingtooursandshowsthatincreasing
thenumberofaugmentations,i.e.increasingthesignaltonoiseratio,helpsthesupervisedlearning
classifier in both growing batch and fixed batch scenarios. Wang & Qi (2022) studied the effect
ofstrongaugmentationsincontrastivelearning,proposinganewframeworktotransferknowledge
fromweakaugmentationstostrongeronesinordertoaddressthelossofinformationduetoharsh
transformations;improvingtheperformance.
SufficientStatistics Sufficientstatisticsprovideasummaryofthedata,fromwhichonecanmake
inferencesontheparametersofthemodelwithoutreferencingsamples. Sufficientstatisticscanbe
readily connected to the Infomax principle and have been used to re-formulate contrastive learn-
ing Chen et al. (2020c). One key observation is that two-view contrastive learning may not yield
representationsthataresufficientw.r.t.theinformationtheycontaintosolvedownstreamtasks(Tian
etal.,2020a;Wangetal.,2022). Poly-viewcontrastivelearningtaskshaveanincreasedamountof
40PublishedasaconferencepaperatICLR2024
available information shared between views, which we believe improves the resulting represen-
tations’ sufficiency w.r.t. any possible downstream task that would have been possible from the
unknowngenerativefactors.
H EXTENSIONS TO DISTILLATION
Ourprimarycontributionsusetheframeworksofinformationtheoryandsufficientstatisticstoin-
vestigate what is possible in the presence of a view multiplicity M > 2 and derive the different
Poly-Viewobjectivesfromfirstprinciples.
It is possible to incorporate multiplicity M > 2 into a distillation setup like BYOL (Grill et al.,
2020). Forexample,DINOv1(Caronetal.,2021),whichsharesmanyalgorithmicpartsofBYOL,
benefits a lot from using the pair-wise Multi-Crop task that we described in Section 2 and Ap-
pendixD(althoughinDINOv1,thereismorethanoneaugmentationpolicy).
One option for extending distillation methods like BYOL and DINOv1 from Multi-Crop to poly-
view tasks in a One-vs-Rest sense is to have the EMA teacher produce M 1 logits, which are
−
aggregated into a single logit (similar to the sufficient statistics choice for Q in Equation 24) for
producing the target pseudo-label distribution. The gradient-based student could then be updated
based on its predictions from the held-out view, and this procedure aggregated over all the view
hold-outs.
The core difference between the distillation procedure above and the poly-view contrastive meth-
ods is that the large-view limit of poly-view contrastive methods is provably a proxy for InfoMax
(Section 2 and Equation 26). There may be a way to obtain theoretical guarantees for the large-
view distillation methods (using for example tools from Gálvez et al. (2023)), and could prove an
interestingfuturedirectionforinvestigation.
I CONTRIBUTIONS
All authors contributed to writing this paper, designing the experiments, and discussing results at
everystageoftheproject. Allcontributionsareinalphabeticalorderbylastname.
Writing and framing Majority of writing done by Dan Busbridge, Devon Hjlem and Amitis
Shidani. ResearchframingdonebyDanBusbridgeandDevonHjlem.
Theoretical results Proofs of MI lower-bound with Multi-Crop (Appendix C.1), lower variance
ofMulti-CropMIbound(AppendixC.2),Generalized (AppendixC.3),ValidityofArithmetic
NWJ
I
and Geometric PVC (Appendix C.4), the connection between sufficient statistics and MI bounds
(AppendixC.7),thegeneralizationofone-vs-restMItoarbitrarysetpartitions(AppendixE.1),and
the linking of poly-view methods to SimCLR (Appendix E.2) and SigLIP (Appendix E.3) done
by Amitis Shidani. Derivation of optimal multiplicity (Appendix E.5) done by Amitis Shidani,
Dan Busbridge and Devon Hjelm. Derivation of Arithmetic and Geometric PVC loss functions
(Appendix C.5) done by Dan Busbridge and Amitis Shidani. Proof of Behavior of MI Gap (Ap-
pendixC.6)donebyEeshanGuneshDhekaneandAmitisShidani.
Sufficientstatistics Extensiontosufficientstatisticsframework(AppendixE.4)proposedbyDe-
von Hjlem. Derivation of sufficient statistics loss function (Equation 22) done by Amitis Shidani
andRussWebb. DerivationofQ(Equation24)donebyDanBusbridge.
Synthetic1DGaussian SyntheticsettingproposedbyDanBusbridgeandAmitisShidanibased
on discussions with Devon Hjelm. One-vs-rest MI (Equations 26 and 173) derived by Dan Bus-
bridge and Amitis Shidani. Proofs of convergence to InfoMax (Equation 27) and to the sufficient
statisticconditionaldistribution(Equations28and176)derivedbyAmitisShidani.Codetoproduce
empiricalresults(Figure2)andrelatedanalysisbyAmitisShidani.
Real world representation learning Experimental protocol for training duration experiments
(Section 3.2 and Appendix F.3.1) designed by Dan Busbridge. Experiments conducted by Dan
41PublishedasaconferencepaperatICLR2024
Busbridge, Eeshan Gunesh Dhekane, Jason Ramapuram, Amitis Shidani, and Russ Webb. Fine-
tuningtransferexperiments(AppendixF.2)donebyJasonRamapuram. Investigationintotherole
ofaugmentationstrength(AppendixF.3)donebyAmitisShidani.
Implementation details ImageNet1k investigations carried out in PyTorch distributed frame-
worksdevelopedbyDanBusbridge,EeshanGuneshDhekaneandJasonRamapuram. Designand
implementation of fast poly-view contrastive losses (Algorithm 1) by Dan Busbridge. Design and
implementation of fast sufficient statistics loss (Algorithm 2) by Amitis Shidani. Baseline imple-
mentationofSimCLR,byJasonRamapuram.
42