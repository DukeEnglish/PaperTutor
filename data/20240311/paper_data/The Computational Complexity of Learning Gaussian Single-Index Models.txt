THE COMPUTATIONAL COMPLEXITY OF LEARNING GAUSSIAN SINGLE-INDEX
MODELS
ALEXDAMIAN†,LOUCASPILLAUD-VIVIEN∗,JASOND.LEE#ANDJOANBRUNA⋆
ABSTRACT. Single-Index Models are high-dimensional regression problems with planted structure,
whereby labels depend on an unknown one-dimensional projection of the input via a generic, non-
linear, and potentially non-deterministic transformation. As such, they encompass a broad class of
statisticalinferencetasks,andprovidearichtemplatetostudystatisticalandcomputationaltrade-offs
inthehigh-dimensionalregime.
While the information-theoretic sample complexity to recover the hidden direction is linear in the
dimensiond,weshowthatcomputationallyefficientalgorithms,bothwithintheStatisticalQuery(SQ)
and the Low-Degree Polynomial (LDP) framework, necessarily require Ω(dk⋆/2) samples, where k⋆
is a “generative” exponent associated with the model that we explicitly characterize. Moreover, we
show that this sample complexity is also sufficient, by establishing matching upper bounds using a
partial-tracealgorithm. Therefore, ourresultsprovideevidenceofasharpcomputational-to-statistical
gap(underboththeSQandLDPclass)wheneverk⋆ >2. Tocompletethestudy,weprovideexamples
ofsmoothandLipschitzdeterministictargetfunctionswitharbitrarilylargegenerativeexponentsk⋆.
CONTENTS
1. Introduction 2
2. TheGenerativeExponent 7
3. ComputationalLowerBounds 10
4. PartialTraceEstimator 13
5. ExistenceofSmoothDistributionsforanygenerativeexponentk⋆ 16
6. Information-TheoreticSample-Complexity 17
7. Conclusions 18
References 19
AppendixA. AdditionalNotation 22
AppendixB. SQFrameworkforSearchProblems 22
AppendixC. HermitePolynomialsandHermiteTensors 23
AppendixD. ProofsofSection2 24
AppendixE. ProofsofSection3 27
AppendixF. ProofsofSection4 34
AppendixG. ProofsofSection5 48
AppendixH. ProofsofSection6 53
AppendixI. ConcentrationLemmas 55
† PACM,PRINCETONUNIVERSITY
∗ ECOLEDEPONTSPARISTECH,CERMICS
# ELECTRICALENGINEERINGDEPARTMENT,PRINCETONUNIVERSITY
⋆ COURANT INSTITUTE OF MATHEMATICAL SCIENCES AND CENTER FOR DATA SCIENCE, NEW YORK
UNIVERSITY
1
4202
raM
8
]GL.sc[
1v92550.3042:viXra2 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
1. INTRODUCTION
1.1. ProblemSetup
The focus of this paper is to study a family of high-dimensional inference tasks characterized by
planted low-dimensional structure. In the context of supervised learning, where a learner observes
a dataset {(x i,y i) }n
i=1
with input features x
∈
Rd and labels y
∈
R, the natural starting point is to
considerdatawithhiddenone-dimensionalstructure,andwherefeaturesaredrawnfromthestandard
Gaussianmeasureγ
d
inRd:
Definition 1.1 (Gaussian Single-Index Model). We say that a joint distribution P (Rd R)
∈ P ×
follows a Gaussian single index model if there exists a probability measure P (R2) and
∈ G ⊂ P
w⋆ Sd−1 such that P = [R w⋆ Id] #[γ d−1 P], where R w⋆ d is any orthogonal matrix whose
∈ ⊗ ⊗ ∈ O
lastcolumnisw⋆,i.e. oftheformR = [R w⋆]and
w⋆ ⊥
(cid:8) (cid:9)
(1) = ν
(z,y)
(R R); ν
z
= γ 1;Eν[Y2] < ; Dχ2(ν γ
1
ν y) > 0
G ∈ P × ∞ || ⊗
is the class of non-separable bivariate probability measures, whose first marginal is the standard one-
dimensionalGaussian,andwhosesecond-ordermomentw.r.t. itssecondargumentisfinite.
In words, a single index model is a joint distribution in Rd R that admits a product structure
×
dP(x,y) = dγ d−1(R ⊥⊤x)dP(w⋆
·
x,y) into a non-informative component R ⊥⊤x of dimension d
−
1,
andaninformativecomponent,determinedpreciselybyPandthedirectionw⋆. TheGaussiansetting
further specifies the marginal distribution of the features. We will denote the planted model by Pw⋆,P
(orsimplyPw⋆ whenthecontextisclear). Wehaveusedtheconventionsthat,foranyrandomvariable
X, P stands for the law of X under P, e.g. P = γ , and P the marginal of Y. Similarly, we will
x z 1 y
makeuseofnotationsP ,P ,thatstandrespectivelyfortheconditionalprobabilitylawsofZ given
z|y y|z
Y andY givenZ.
Notethatinthislanguage,Gaussiansingle-indexmodelsarecloselyrelatedtoNon-GaussianCom-
ponent Analysis (NGCA) [DKS17]. In NGCA, a d-dimensional distribution admits a product struc-
ture in terms of a univariate non-Gaussian marginal and a d 1 Gaussian distribution. In our case,
−
theplantednon-Gaussiancomponentistwo-dimensional,butthestatisticianisgivenonedirectionof
thenon-Gaussiansubspace(thelabelY).
Throughout the paper, we use the letter Z to denote the one-dimensional (Gaussian) random vari-
able w⋆ X. When there exists σ : R R such that P y|z( ,z) = δ σ(z) , we say that (X,Y) follows
· → ·
a deterministic single-index model as Y = σ(Z) = σ(w⋆ X), where σ is said to be link function.
·
However,Definition1.1allowsforadditionalrandomnessinthelabel,aslongasitsdistributiononly
dependsonxthroughz = w⋆ x. Examplesofnon-deterministicsingle-indexmodelsincludeadditive
·
noise,whereY = σ(Z)+ξ andξ isanindependentrandomvariable,e.g. ξ N(0,1);multiplicative
∼
noise, where Y = ξσ(Z), Mixture of distributions, where Y µ if Z 0 and Y µ if Z < 0, or
1 2
∼ ≥ ∼
Massart-typenoise,whereY = ξσ(Z)andP(ξ = 1) = 1 η(Z)andP(ξ = 1) = η(Z);seeFigure
− −
1.
In the remainder of this paper, and unless stated otherwise, we assume that P is known, so that
the inference task reduces to estimating the hidden direction w⋆ drawn from the uniform prior over
Sd−1, after obserivng n iid samples from Pw⋆,P. By instantiating specific choices for P, one recov-
ers several well-known statistical inference problems, such as linear recovery, phase-recovery, one-
bit compressed sensing, generalized linear models or Non-Gaussian Component Analysis [DKS17],
as well as close variants of Tensor PCA [MR14]. An important common theme across these dif-
ferent statistical models over recent years has been to understand computational-to-statistical gaps,THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 3
AdditiveGaussianNoise MultiplicativeGaussianNoise MixtureofDistributions
3 3 3
E[Y Z] E[Y Z] E[Y Z]
| | |
2 2 2
1 1 1
Y0 Y0 Y0
1 1 1
− − −
2 2 2
− − −
3 3 3
− − −
3 2 1 0 1 2 3 3 2 1 0 1 2 3 3 2 1 0 1 2 3
− − − Z − − − Z − − − Z
FIGURE 1. VisualizationofthejointdensityPof(Z,Y)fortheadditivenoisemodel,
multiplicative noise model, and mixture of distributions model. The heatmap shows
the density of P and the plots to the left and below the heatmap show the densities of
themarginalsP andP respectively.
y z
namely comparing the required amount of samples needed to estimate w⋆, using either computation-
ally efficient methods or brute-force search. Computational efficiency may be measured either by
restricting estimation algorithms to belong to certain computational models, or by establishing re-
ductions to problems believed to be computationally hard. In this paper, we focus our attention on
Statistical-Query algorithms [Kea98], which capture a broad class of learning algorithms including
robustgradient-descentmethods,aswellastheLow-DegreePolynomialmethod[Hop18,KWB19],a
flexibleframeworktoassesstheaverage-casecomplexityofstatisticalinferencetasks.
Theprimaryfocusofthisworkistoderivetheoptimalsamplecomplexityforrecoveringw⋆ given
i.i.d. samples (x ,y ) n from a single-index model (Definition 1.1). Perhaps unsuprinsingly, one
{ i i }i=1
can recover w⋆ up to error ϵ by brute-force search when n = Θ(d/ϵ2) (Theorem 6.1), in accordance
with related statistical inference tasks. However, when restricted to SQ and LDP algorithms, our
main results will establish a tight sample complexity of order n = Θ(dk⋆/2), with matching upper
andlowerbounds,wherek⋆ = k⋆(P)isanexponentassociatedwithPthatweexplicitlycharacterize
(Definition2.4). Wethusobtainevidenceofacomputational-statisticalgapofpolynomialscalefora
broadclassofinferenceproblems.
1.2. BackgroundandRelatedWork
Single-indexmodels(alsocalledgeneralizedlinearmodels)havealonghistoryinthestatisticslitera-
ture[MN83,Ich93,HJS01,HMSW04,DJS08,DH18]. Whenthelinkfunctionσ ismonotonic,there
are perceptron-like algorithms (e.g. Isotron/GLM-tron [KS09, KKSK11]) that recover the ground
truthsignalwithn = Θ(d)sampleswheredisthedimensionofthedata.
Perhaps the simplest example of a generalized linear model with a non-monotonic link function
is phase retrieval in which σ(u) = u . In contrast to the monotonic case, phase retrieval exhibits a
| |
conjectured statistical-computational gap in the noisy setting [MM18, BKM+19, MLKZ20] In the
presenceoflabelnoise,thereareconstantsc < c suchthatrecoveryisinformationtheoretically
IT ALG
possible with n/d c but is conjectured to be computationally hard unless n/d c . Note,
IT ALG
≥ ≥
however,thatthisisnottrueinthenoise-freesettingastherearecomputationallyefficientalgorithms
basedonlatticereductionthatachievetheinformation-theoreticthreshold[AHSS17,SZB21].4 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
For both monotonic and quadratic link functions, the optimal sample complexity scales linearly in
the input dimension d. In addition, this rate can even be recovered by directly optimizing the maxi-
mum likelihood objective using simple first order methods such as online stochastic gradient descent
(SGD). However, the problem is significantly harder when the link function is more complicated.
[BAGJ21] demonstrated that for general single index models, online SGD on the maximum likeli-
hood objective succeeds if and only if n = Θ˜ (dmax(1,l⋆−1)) where l⋆ is the information exponent of
the single index model, which is defined to be the degree of the first nonzero Hermite coefficient of
σ (Definition 2.2). However, the significance of the information exponent extends far past optimiz-
ing the maximum likelihood objective. It has appeared as the fundamental object in determining the
samplecomplexityinmanyfollowupworks[BBSS22,DLS22,DNGL23,DKL+23,ABAM23].
[DLS22] formalized this by proving a correlational statistical query (CSQ) lower bound which
shows that either n ≳ dmax(1,l⋆/2) samples or exponentially many queries are necessary for learning a
singleindexmodelwithinformationexponentl⋆. Inaddition,thislowerboundistightasitispossible
to learn a single index model with
Θ˜ (dmax(1,l⋆/2))
samples in polynomial time by training wide three
layerneuralnetworks[CBL+20]orbysmoothingthelandscapeofthemaximumlikelihoodobjective
[DNGL23].
The information exponent arose as the fundamental object governing sample complexities given
correlational queries of the form E[Yh(X)]. This is rich enough to capture gradient methods with
meansquarederrorlossastheyonlyinteractwiththedatathroughcorrelationalqueries:
(2) L(θ) = E[(f θ(X) Y)2] = θL(θ) = E[(f θ(X) Y) θf θ(X)].
− ⇒ ∇ − ∇
(cid:124) (cid:123)(cid:122) (cid:125)
correlationalquery
However, methods outside of the correlational statistical query (CSQ) framework can break the n ≳
dmax(1,l⋆/2) sample complexity barrier [CM20, MM18, BKM+19, MLKZ20, DTA+24]. The general
technique for these methods is to first apply a pre-processing function to the label Y to lower the
T
information exponent to 2 before running a CSQ algorithm. This is possible because the information
exponent defined by [BAGJ21] is not composition invariant, i.e. it is possible that the information
exponent of (X, (Y)) is strictly smaller than the information exponent of (X,Y). In fact, [MM18,
T
BKM+19, MLKZ20] give a necessary and sufficient condition on P that enables such to lower the
T
informationexponentto2. UsingthenotationdescribedinDefinition1.1,theconditiononPwrites:
(cid:104) (cid:105)
(3) E
E(cid:2)
Z2 1
Y(cid:3)2
= 0,
− | ̸
where expectations are taken with respect to (Z,Y) P . Such ‘pre-processing’ methods that go
∼
beyond the CSQ lower bound are in fact instances of SQ-algorithms, which interact with data via
generalqueriesoftheformE[ϕ(X,Y)]forabroadclassoftestfunctionsϕ. Inparticular,theseworks
implythattheSQcomplexityforlearningsingle-indexmodelssatisfying(3)isn = Θ(d). Thenatural
follow-up question –and the main focus of this work– is to quantify the statistical-computational gap
for arbitrary P, going beyond the criterion given by Eq.(3) and identifying necessary and sufficient
conditionsforefficientrecovery.
1.3. SummaryofMainResults
Our first main result establishes sample complexity lower bounds required by any SQ-algorithm to
solvethesingle-indexproblemdeterminedbyP:
Theorem1.2(SQlowerbound,informalversionofTheorem3.2). GivenP andni.i.d. samples
∈ G
from Pw⋆,P, there exists an explicit exponent k⋆ = k⋆(P) < such that no polynomial time SQ-
∞
algorithmcansucceedinrecoveringw⋆ unlessn ≳ dk⋆/2.THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 5
Additionally,thisSQcomplexitycoincideswiththeratewherethelow-degreemethodsucceeds:
Theorem 1.3 (Low-degree method detection lower bound, informal version of Theorem 3.5). Under
thelow-degreeconjecture(Conjecture3.4),ifk⋆ = k⋆(P)istheexponentinTheorem1.2,thengivenn
i.i.d. samplesfromeitherPw⋆,PoranulldistributionP0,nopolynomialtimealgorithmcandistinguish
Pw⋆,P fromP0 unlessn ≳ dk⋆/2.
Wedenotetheassociatedexponentk⋆(P)asthegenerativeexponent ofthemodel,incontrastwith
theinformationexponent,andanalyzeitsmainpropertiesinSection2. Inthemeantime,theattentive
readermightalreadyanticipatethatindeedonehask⋆(P) l⋆(P).
≤
Our second main result shows that these computational lower bounds are tight, by exhibiting a
polynomial-timealgorithm,basedonthepartial-traceestimator,thatsucceedsassoonasn ≳ dk⋆/2:
Theorem1.4(informalversionofTheorem4.1andCorollary4.4). GivenP andni.i.d. samples
from Pw⋆,P, there exists an efficient algorithm that succeeds in estimating w⋆∈ wG hen n ≳ dmax(1,k⋆/2),
evenwhenPismisspecified.
Combinedwithaninformation-theoreticsamplecomplexityupperboundn = O(d)(Theorem6.1)
—whichfollowsfromrelativelystandardarguments,Theorem1.4thusestablishesasharpcomputational-
to-statistical gap (under both the SQ and the LDP frameworks) as soon as k⋆(P) > 2. Our last main
contributionshowsthatforanyk,thereexistssmoothdistributionssuchthatk⋆(P) = k:
Theorem 1.5 (informal version of Theorem 5.1 and Theorem 5.2). For any k N and τ 0, there
∈ ≥
exists σ C∞(R) such that Z γ, Y = σ(Z)+W, with W N(0,τ2) defines a joint distribution
∈ b ∼ ∼
(Y,Z) Psatisfyingk⋆(P) = k.
∼
Notations.Given a probability measure µ defined over Rm, we denote by L2(Rm,µ) the space of µ-
measurable,square-integrablefunctions. Forf,g L2(Rm,µ),wewrite f,g
µ
= EX∼µ[f(X)g(X)]
∈ ⟨ ⟩
and f 2 = f,f . We use γ = γ to denote the standard Gaussian measure N(0,1) and for d 1
∥ ∥µ ⟨ ⟩µ 1 ≥
weuseγ
d
todenotethestandardisotropicGaussianmeasureinRd,N(0,I d).
HermitePolynomials.WedefinethenormalizedHermitepolynomials h
k k≥0
L2(R,γ 1)by
{ } ∈
( 1)k 1 ∂kγ (z)
1
(4) h k(z) := −
√k! γ (z) ∂zk
, z
∈
R, k
∈
N.
1
ThesepolynomialssatisfytheorthogonalityrelationsEZ∼γ1[h j(Z)h k(Z)] = 1 j=k.
Acknowledgements:WethankGuyBresler,YatinDandi,IliasDiakonikolas,FlorentKrzakala,Theodor
Misiakievicz,TselilSchramm,DennyWu,IliasZadikandLenkaZdeborovaforusefulfeedbackdur-
ing the completion of this work, which was partially developed during 2023’s Summer School “Sta-
tistical Physics and ML back together again" in Cargese. AD acknowledges support from a NSF
Graduate Research Fellowship. AD and JDL acknowledge support of the ARO under MURI Award
W911NF-11-1-0304, the Sloan Research Fellowship, NSF CCF 2002272, NSF IIS 2107304, NSF
CIF 2212262, ONR Young Investigator Award, and NSF CAREER Award 2144994. JB was par-
tiallysupportedbytheAlfredP.SloanFoundationandawardsNSFRI-1816753,NSFCAREERCIF
1845360,NSFCHS-1901091andNSFDMS-MoDL2134216.6 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Tiltedsincurve+noise OptimalTransformationζ1 JointDistributionof(Z,ζ1(Y))
3
E[Y |Z]
0.04
E[Y |Z]
2 0.2
1
0.02
0.1
Y 0 ζ1(Y) 0.00
ζ1(Y) 0.0
−1 −0.02
−2 −0.1
0.04
−
3
− 0.2
−
3 2 1 0 1 2 3 3 2 1 0 1 2 3 3 2 1 0 1 2 3
− − − Z − − − Y − − − Z
Y=Zξ whereξ ∼N(0,1) OptimalTransformationζ2 JointDistributionof(Z,ζ2(Y))
3 1.50
E[YZ] E[YZ]
| 1.25 |
2 1.5
1.00
1 0.75
Y 0 1.0 ζ2(Y) 0.50
−1
ζ2(Y)
0.5
00 .. 02 05
0.25
2 −
−
0.0 −0.50
3
−
0.5
−
3 2 1 0 1 2 3 3 2 1 0 1 2 3 3 2 1 0 1 2 3
− − − Z − − − Y − − − Z
Constructionfrom[MM18] OptimalTransformationζ4 JointDistributionof(Z,ζ4(Y))
3
E[Y |Z] 0.25
0.4
E[Y |Z]
2 0.20
1 0.15
0.2
Y 0 0.10 ζ4(Y) 0.0
−1
ζ4(Y)
0.05 −0.2
2 0.00
− 0.4
0.05 −
3 −
−
0.10
−
−3 −2 −1 Z0 1 2 3 −2.0 −1.5 −1.0 −0.5 0 Y.0 0.5 1.0 1.5 2.0 −3 −2 −1 Z0 1 2 3
FIGURE 2. We plot three examples of a joint distribution P of (Z,Y), the witness
function ξ (y), and the joint distribution of (Z,ξ (Y)). In the first example, Y =
k⋆ k
c x+sin(c Z) for constants c ,c such that β = 0. The transformation ξ zeros out
1 2 1 2 1 1
thebulkandamplifiesthecapsofthecurveinordertolowertheinformationexponent
from l⋆(P) = 2 to l⋆((Id ζ ) P) = 1. As a result, k⋆(P) = 1. In the second
1 #
⊗
example, the model Y = Zξ has multiplicative Gaussian noise and E[Y Z] = 0 so
| (cid:112)
this model has l⋆(P) = . The transformation ζ interpolates between Y for
2
∞ | |
Y 0 and Y for Y farther from 0. The transformed distribution (right column)
| | ≈ | | | |
nowhasl⋆((Id ξ ) P) = 2sok⋆(P) = 2. Thethirdexampleisthedistributionused
k #
⊗
in [MM18] as an example where k⋆ > 2. In this case, we verify that k⋆ = l⋆ = 4 so
thetightsamplecomplexityforthesingleindexmodelcorrespondingtothischoiceof
Pisn ≳ d2.THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 7
2. THE GENERATIVE EXPONENT
Let us start by defining the information exponent of [BAGJ21, DH18] in our framework. We begin
with Parseval’s identity for Hermite polynomials. We define σ such that σ(Z) := EP[Y Z] is the
|
conditionalexpectationofY givenZ,thusσ L2(R,γ)andwehave
∈
Fact2.1(SpectralVarianceDecomposition). Thevarianceofσ(Z)verifiestheexpansion
(cid:88)
(5) Var P[σ(Z)] = β l2 where β
l
:= EP[Yh l(Z)].
l≥1
Therefore if σ is not constant, there exists l 1 such that β = 0. We define the information
l
≥ ̸
exponentl⋆ asthefirstsuchl:
Definition2.2(InformationExponentrevisited). Theinformationexponent ofP isdefinedby:
∈ G
(6) l⋆(P) := min l 1;β
l
= 0 where β
l
:= EP[Yh l(Z)].
{ ≥ ̸ }
Note that Definition 2.2 only depends on P through the conditional expectation σ, and is therefore
agnostic to any form of label noise. Below, we define another index, the generative exponent. As for
the information exponent, its definition follows from an expansion, but for this exponent, we do it
throughtheexpansionoftheχ2-mutualinformationof(Z,Y) P,i.e. theχ2-divergencebetweenP
∼
andtheproductofitsmarginalsI χ2[P] := Dχ2[P P
z
P y]:
|| ⊗
Lemma2.3(MutualInformationDecomposition). Wehavethefollowingexpansion
(cid:88)
(7) I χ2[P] = λ2
k
where λ
k
:= ∥ζ
k ∥Py
and ζ
k
:= E[h k(Z) |Y].
k≥1
The proof of this Lemma is postponed to Section D of the Appendix. We note that because
EP[Y2] < , the conditional expectation ζ
k
:= E[h k(Z) Y] is well defined. In addition, ζ
k
∞ | ∈
L2(R,P y) because for each k, ∥ζ
k
∥2
Py ≤
EYEZ|Y[h k(Z)2] = EZ[h k(Z)2] = 1. Therefore when P is
not a product measure, as is required by Definition 1.1, we have that I [P] > 0 so there exists k 1
χ2
≥
suchthatλ = 0. Wedefinethegenerativeexponentk⋆ asthefirstsuchk:
k
̸
Definition2.4(GenerativeExponent). ThegenerativeexponentofP isdefinedby:
∈ G
(8) k⋆(P) := min {k
≥
1;λ
k
̸= 0
}
where λ
k
:= ∥ζ
k ∥Py
and ζ
k
:= EP[h k(Z) |Y].
Observethatβ andζ arerelatedbyβ = y,ζ . Wecanthusreinterprettheexponentl⋆(P)as
k k k
⟨
k ⟩Py
the smallest k such that ζ
k
has non-zero correlation with linear functions in L2(R,P y), capturing the
correlational structure of CSQ queries. This also implies that the generative exponent is at most the
Informationexponent,i.e. k⋆(P) l⋆(P)becauseλ = 0 β = 0.
k k
≤ ⇒
The function ζ (y) measures the k-th Hermite moment of the conditional ‘generative’ process
k
Z Y = y; as such, the fact that λ > 0 ‘witnesses’ χ2-mutual information carried by order-k mo-
k
|
ments, and reciprocally λ = 0 indicates the absence of order-k exploitable information, even after
k
conditioning on the observed labels. This is illustrated in Figure 2 and formalized by our SQ and
low-degreelowerbounds;seenextsection.
Remark2.5. ObservethatitispossibletobuilddistributionsPsuchthatk⋆(P) < ,butl⋆(P) = ,
∞ ∞
i.e. such that E[Y Z] is constant. Consider for example P = φ #γ 2, with φ(z,ξ) = (z,zξ). We have
|
E[Y Z] = 0, hence l⋆(P) = . However, k⋆(P) = 2 (see Figure 2). This is reflected by the fact that
| ∞
squaringthelabelsreducestheproblemtonoisyphaseretrievalasE[Y2 Z = z] = z2.
|8 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Finally, we show that the generative exponent can be expressed as the smallest information expo-
nentoverallpossibletransformationsofthelabelbysquared-integrablefunctions:
Proposition2.6(AVariationalRepresentation). Thegenerativeexponentk⋆(P)canbewrittenas:
(9) k⋆(P) = inf l⋆((Id ) P).
#
T∈L2(Py) ⊗T
This provides a ‘user-friendly’ characterization of the generative exponent: for any polynomial Q
of degree k < k⋆(P) and any measurable test function L2(R,P y), Proposition 2.6 tells that we
T ∈
must have EP[ (Y)Q(Z)] = 0. In addition, because l⋆ only depends on the conditional expectation
T
E[ (Y) Z], this variational representation extends to non-deterministic channels, i.e. if Y Y′ is a
T | →
MarkovchainwithE[(Y′)2] < thenk⋆(Z,Y′) k⋆(Z,Y). Inparticular,nopost-processingofY,
∞ ≥
eitherrandomordeterministic,canreducethegenerativeexponent.
We conclude this section with some representatitve examples for deterministic models, illustrated
inFigures3and2.
Example2.7(ExplicitExamplesofgenerativeexponent). Wegivethefollowingexplicitexamples:
(i) Forσ apolynomial,wehavek⋆(σ) 2andk⋆(σ) = 2iffσ iseven. Inparticular,k⋆(h ) = 1
j
≤
ifj isoddandk⋆(h ) = 2ifj iseven.
j
(ii) Forσ(z) = z2e−z2 ,wehavek⋆(σ) = 4.
(iii) From[MM18,Remark3],ify 1,1 isbooleanwithP(Y = 1 Z = z) = EW∼γ[tanh(c 1z)2
∈ {− } | −
tanh(c z)2]forcarefullychosenc ,c thenk⋆(σ) = 4.
2 1 2
Finally,animmediateconsequenceof Proposition2.6isthefollowing:
Corollary2.8(invariancetobijections). Ifφ : R Risabijectionsuchthatφ,φ−1 L2(P y),then
→ ∈
k⋆(σ) = k⋆(φ σ).
◦THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 9
Y=h3(Z)+additiveGaussiannoise OptimalTransformationζ1 JointDistributionof(Z,ζ1(Y))
3 3
E[YZ] E[YZ]
| |
2 2 2
1 1
1
Y 0 ζ1(Y) 0
ζ1(Y) 0
1 1
− −
−2 −1 −2
3 3
− −
2
−
3 2 1 0 1 2 3 3 2 1 0 1 2 3 3 2 1 0 1 2 3
− − − Z − − − Y − − − Z
Y=h4(Z)+additiveGaussiannoise OptimalTransformationζ2 JointDistributionof(Z,ζ2(Y))
3 3
E[YZ] E[YZ]
| |
2 4 2
1 1
3
Y 0 ζ2(Y) 0
ζ2(Y)2
1 1
− −
−2 1 −2
3 3
− 0 −
3 2 1 0 1 2 3 3 2 1 0 1 2 3 3 2 1 0 1 2 3
− − − Z − − − Y − − − Z
Y=h5(Z)+additiveGaussiannoise OptimalTransformationζ1 JointDistributionof(Z,ζ1(Y))
3 3
E[Y |Z]
2
E[Y |Z]
2 2
1 1 1
Y 0 ζ1(Y) 0
ζ1(Y) 0
1 1
− −
2 2
− 1 −
−
3 3
− −
2
−
3 2 1 0 1 2 3 3 2 1 0 1 2 3 3 2 1 0 1 2 3
− − − Z − − − Y − − − Z
Y=h6(Z)+additiveGaussiannoise OptimalTransformationζ2 JointDistributionof(Z,ζ2(Y))
3 8 3
E[YZ] E[YZ]
| |
2 2
6
1 1
Y 0 ζ2(Y) 0
ζ2(Y)4
1 1
− −
−2 2 −2
3 3
− −
0
3 2 1 0 1 2 3 3 2 1 0 1 2 3 3 2 1 0 1 2 3
− − − Z − − − Y − − − Z
FIGURE 3. For every k, Y = h k(Z) has generative exponent 1 if k is odd and 2 if
k is even. In particular, the difficulty of learning the single index model defined by
P = (Id h ) γ doesnotgrowwithk.
k # 1
⊗10 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
3. COMPUTATIONAL LOWER BOUNDS
3.1. FromCSQtoSQlowerbounds
Recall that the CSQ complexity of Gaussian single-index models is established by considering the
correlationEX [E[Y X] E[Y′ X]],where(X,Y) Pw and(X,Y′) Pw′ aretwohypothesisinthe
| · | ∼ ∼
class. This correlation admits a closed-form representation via the Ornstein-Ulhenbeck semigroup in
L2(R,γ 1)andtheHermitedecompositionofσ(z) = EP[Y |Z = z] = (cid:80) kβ kh k(z):
(cid:88)
(10) EX [E[Y |X] ·E[Y′ |X]] = mkβ k2 , wherem = w ·w′ .
k≥l⋆
For randomly chosen w,w′ Unif(Sd−1), the correlation m = w w′ is of order d−1/2 so the kth
∼ ·
term in this expansion is of order d−k/2. Therefore, the first nonzero term of this expansion, which is
oforderd−l⋆/2,dominatesthesearchproblem.
ThisisusedintheproofoftheCSQlowerboundthat
CSQalgorithmsrequiren ≳ dl⋆/2 samplestolearnw⋆ [DLS22,ABAM22].
However, unlike the CSQ complexity, which is determined by average pairwise correlations, the
SQ-complexityisdeterminedbytheχ2 symmetrizeddivergence:
(cid:20) (cid:21)
(11) χ2 0(Pw,Pw′) := E P0 d dP Pw
0
d dP Pw 0′ −1 where P0 = γ d ⊗P y .
Remarkably,onecanexhibitananalogof(10),wherethecoefficientsβ arereplacedbyλ :
k k
Lemma3.1(χ2 Representation). Letw,w′ Sd−1,anddenotem = w w′,thenwehave
∈ ·
(cid:88)
(12) χ2 0(Pw,Pw′) = mkλ2
k
.
k≥k⋆
TheproofofthisLemmaispostponedtoSectionEoftheAppendix. Asabove,misoforderd−1/2
so this sum is dominated by the first nonzero term in the sequence, which is given precisely by the
generativeexponentk⋆ = k⋆(P).
3.2. SQFrameworkforSingle-IndexModels
Appendix B reviews the basic framework to establish SQ query complexity for search problems,
which has been used for a variety of statistical inference tasks, e.g. [DKS17, DH21], and instantiates
it for the particular setting of the single-index model; see Section B.2. We consider the statistical
query oracle VSTAT(n) which responds to query h : Rd R [0,1] with hˆ which satisfies:
ˆ (cid:112) × →
h p p(1 p)/n+1/n,wherep = EX,Y[h(X,Y)].
| − | ≤ −
The primary challenge in proving an SQ lower bound is computing the SQ-dimension; see Defini-
tion B.5. This is enabled in our setting thanks to the key Lemma 3.1. This result leads to a control of
therelativeχ2-divergenceoftheformχ2 0(Pw,Pw′)
≤
λ2 k⋆mk⋆ + 1m −k m⋆ ,withm = w ·w′ (LemmaE.2),
whichyieldsthefollowingSQlowerbound(provedinAppendixE.2):
Theorem 3.2 (Statistical Query Lower Bound). Assume that (X,Y) follow a Gaussian single-index
model (Definition 1.1) with generative exponent k⋆. Let q = dr for any r d1/4 and assume that
≤
λ2 rd−1/2. Then there exists a constant c depending only on k⋆ such that to return wˆ with
k⋆
≥
k⋆
wˆ w⋆ ω˜(d−1/2),anyalgorithmusingq queriestoVSTAT(n)requires:
| · | ≥
(cid:18)
(cid:19)k⋆
c k⋆ d 2
(13) n .
≥ λ2 r2
k⋆THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 11
Therefore under the standard heuristic that VSTAT(n) measures the algorithmic complexity of an
algorithm using n samples, any algorithm that uses poly(d) queries must use n ≳ dk⋆/2 samples. A
remarkablefeatureofeq.(13)istheabsenceofpolylogfactors;thisisachievedthankstoanimproved
constructionofthediscretehypothesisset (DefinitionB.5)usingsphericalcodes;seeLemmaE.3.
D
D
Remark 3.3 (Choice of Null Model). In the proof of Theorem 3.2, we compare Pw with the null
model P0 := γ d P y. As a result, Theorem 3.2 primarily measures the detection threshold. Indeed,
⊗
[DH21] observed a detection/estimation gap for the related problem of tensor PCA and proved that
SQ algorithms require n ≳ dk 2⋆ +1 for estimation. We leave open the possibility that such a gap exists
undertheSQframeworkforoursettingaswell. However,becauseAlgorithm1succeedsinestimating
w⋆ withn dk⋆/2 (seeTheorem4.1)thiswouldbeanartifactoftheSQframework.
≍
3.3. TheLowDegreePolynomialMethod
In this section we prove a lower bound for the class of low-degree polynomials, which has been used
to argue for the existence of statistical-computational gaps in a wide variety of problems [KWB19,
BEAH+22, Wei23]. This has been formalized through various versions of the low degree conjecture
[Hop18,Hypothesis2.1.5,Conjecture2.2.4],forwhichwestateaninformalversionfordistinguishing
between two distributions, P (signal) and P0 (null). We define the likelihood ratio
R
:= dd PP
0
and the
low-degree likelihood ratio
≤D
by the orthogonal projection in L2(P0) of onto polynomials of
R R
degree at most D. The low-degree conjecture states that low-degree polynomials (i.e. D log(n))
≈
actasaproxyforpolynomialtimealgorithms:
Conjecture 3.4 (Low-Degree Conjecture, informal). 1 Let D = ω(log(d)). Then if =
≤D
∥R ∥P0
1+o d(1),nopolynomialtimealgorithmcandistinguishPandP0. If
≤D
= O d(1),theycando
∥R ∥P0
sowithonlyconstantprobability.
By definition, we have that ∥R∥2
L2(P0)
= 1+Dχ2[P ||P0] = 1+I χ2[P], which hints at the fact that
thedecompositionlemma3.1anditsassociatedgenerativeexponentwillalsobedrivingthebehavior
of the Low-Degree estimator. This is indeed the case: our main result in this section computes the
lowdegreelikelihoodratio andshowsthatitremainsnear1unlessn ≳ dk⋆/2:
≤D
∥R ∥P0
Theorem 3.5 (Low-Degree Method Lower Bound). Let P be a single index model, let P be the dis-
tribution of Pw when w is chosen randomly from Unif(Sd−1). Let X Rn×d and Y Rn denote a
∈ ∈
sequence of n i.i.d. inputs and targets drawn from P. Let P0 := γ d P y denote the null distribution.
⊗
Let (x,y) denote the likelihood ratio dP (x,y) and let (x,y) denote the orthogonal projection
R dP0 R≤D
inL2(P0)of R(x,y)ontopolynomialsofdegreeatmostD inx. Thenif λD
2 k ≪
√dandδ := dkn ⋆/2,
⌊D⌋
(cid:88)k⋆ (λ2 δ)j
2 = (1+o (1)) 1 (k⋆j 1)!! k⋆ .
≤D d 2|k⋆j
∥R ∥P0 − j!
j=0
TheproofisinAppendixE.3. Thisprovidesanimmediatecorollarywhichprovesimpossibilityre-
sultsforweakandstrongrecoveryforallpolynomialtimealgorithmsunderthelow-degreeconjecture
(Conjecture3.4):
Corollary3.6. LetP,P0, beasinTheorem3.5andassumek⋆ > 1. Then,
R
1Theconjectureisstatedforinferenceproblemswithsomelevelofrobustnesstonoise;thisexcludesknownfailures
ofLDmethodstocapturecomputationalhardnessinproblemswithalgebraicstructure;see[ZSWB22,DK22].12 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
WeakDetection: IfD = log(d)2 andn dγ forγ < k⋆ ,then = 1+o (1).
• ≤ k⋆ 2 ∥R≤D ∥P0 d
StrongDetection: ForanyD √d,ifn d 2 ,then = O (1).
• ≪ ≤ Dk 2⋆ −1 ∥R≤D ∥P0 d
This shows that under Conjecture 3.4, n ≳ dk 2⋆ samples are necessary for polynomials of degree
D = log(d)2 todistinguishbetweenPandP0. Notethatbecauserecoveryisstrictlyharderthandetec-
tion,thisalsoimpliesthatrecoveringw⋆ fromPalsorequiresn ≳ dk 2⋆ samples,whichismatchedby
our upper bound (Theorem 4.1). We also remark that the
dk⋆/2
threshold in Corollary 3.6 matches
Dk⋆/2−1
theoptimalknowncomputational-statisticaltrade-offfortensorPCA[BGL17,WAM19].
3.4. Discussion
Relationship between SQ and LD lower bounds.Our statistical query lower bound (Theorem 3.2)
and our low-degree polynomial lower bound (Theorem 3.5 and Corollary 3.6) have similar forms.
The statistical query lower bound says that given dr statistical queries or a polynomial of degree D
(whichcanbecomputedindD time)weneed:
dk⋆/2 dk⋆/2
n ≳ or n ≳
rk⋆ Dk⋆/2−1
samplesrespectively. Acommonthemeintheselowerboundsisthepossibilityoftradingoffstatistics
with computation. In particular, for k⋆ > 2, given n = δdk⋆/2 samples, w⋆ is always learnable given
sufficientlymanyqueries(psufficientlylarge)orasufficientlyhighdegreepolynomial(Dsufficiently
large). These bounds differ slightly in their dependence on r,D: the SQ bound depends on rk⋆
while the low-degree bound depends on
Dk⋆/2−1.
We believe the low-degree result is tight, as it
exactly matches existing statistical-computational tradeoffs that are known for tensor PCA [BGL17,
WAM19]. This gap is due to the fact that our low-degree lower bound relies on average correlations
over a prior, while the SQ lower bound relies on worst case correlations over a discrete set of points,
whichweconstructedusingsphericalcodes(seeLemmaE.3).
Indeed, the proof of the low-degree lower bound (Theorem 3.5) suggests an algorithm for achiev-
ing this continuous statistical-computational tradeoff by computing higher order tensors than in Al-
gorithm1. Specifically,ifpisasufficientlylargeeveninteger,thenweconsider
(cid:32) (cid:33)
1 (cid:88) (cid:79)
T :=
(cid:0)n(cid:1)
Sym y ih k(x i)
∈
Rk⋆p.
p S⊂[n],|S|=p i∈S
We can now apply partial trace to T to reduce it to a matrix M and then return the top eigenvector of
M. Note that by construction, E[T] = (w⋆)⊗k⋆p so E[M] = w⋆w⋆T. We leave the analysis of M and
thequestionofwhetherM obeysGaussianuniversality(asinSection4)tofuturework.
Finally, we mention the work [BBH+21], which provides generic ‘translations’ between SQ and
low-degree lower bounds under mild conditions, at the expense of a loss in the parameters (number
ofqueriestodegreeofpolynomial). Itwouldbeinterestingtoquantifythislossinoursetting.
NGCAaka‘GaussianPancakes’.TheSQlowerboundsthatwepresenthereforsingleindexmodels
are similar in essence to the SQ lower bounds for Gaussian hypothesis testing from [DKS17]. In
short, given a non-gaussian univariate distribution µ (R), this problem considers distributions
∈ P
Q (Rd) of the form Q = Qµ,w⋆ = R #(γ d−1 µ), where R d as in Definition 1.1. The
∈ P ⊗ ∈ O
relevantquantitythatgovernstheSQ-complexityofrecoveringtheplanteddirectionw⋆ (whereagain
w⋆ = Re d)isthesmallestdegreek suchthatEµ[h k(z)] = 0. Assuch,thereisadirectreductionfrom
̸
thesingleindexmodelsettingtoNGCA:THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 13
Proposition3.7(Single-IndextoNGCAReduction). AnSQalgorithmthatsolvesNGCAwithplanted
distribution µ of exponent k and direction w∗ Sd−1 yields an efficient SQ algorithm to solve the
∈
single-indexmodelwithgenerativeexponentk anddirectionw∗.
Asaresult,ourSQ-hardnessresultsimplythehardnessresultsin[DKS17]. Anintriguingquestion
is whether the reduction could go the other way, namely whether the SQ-hardness of Gaussian pan-
cakes implies the SQ-hardness of learning certain single-index models. While we do not answer this
questioninthepresentpaper,weshowinAppendixE.5areductionfromNGCAtoaslightvariantof
thesingle-indexmodel.
Tensor PCA.Both our upper bound (Theorem 4.1) and lower bound (Theorem 3.2) were heavily
inspired by related work in the tensor PCA literature. Specifically, the partial trace estimator in
Algorithm 1 is related to the partial trace estimator for tensor PCA [HSSS16, DH24] which is the
simplest method that achieves the conjectured optimal rate of n = Θ(dk/2) for tensor PCA. The
derivation of our lower bounds were also inspired by similar results for tensor PCA, including the
SQ lower bound for tensor PCA proved by [DH21], and the Low-Degree method lower bound of
[KWB19,Theorem3.3].
CLWE and periodic structures.[SZB21] proved a lower bound for learning single-index models.
They showed that under a cryptographic hardness assumption, there is a noisy single-index problem
suchthatregardlessofthesamplesn,nopolynomialtimealgorithmcanrecoverthegroundtruthw⋆.
Our results imply a similar lower bound for the special case of SQ algorithms without the crypto-
graphic hardness assumption (Lemma E.4). We also note that in the noise-free setting, the situation
is different, and in fact there are (non-SQ) polynomial-time algorithms, such as LLL, that break the
SQ-lower bound for certain single-index models and related NGCA [SZB21, ZSWB22, DK22] by
exploitingperiodicstructuresinthelinkfunction.
4. PARTIAL TRACE ESTIMATOR
We have shown so far that Theorems 3.2 and 3.5 extend the ‘necessary’ criterion presented in Eq.
(3) from [MM18, BKM+19, MLKZ20] to arbitrary exponents k⋆ > 2, as indeed a simple calculation
showsthatE[E[Z2 1 Y]2] = 2λ2. Concerningthe‘sufficient’directionofEq.(3),thealgorithmsthat
− | 2
match the previous lower bound for k⋆ 2 suggest a meta-strategy for the general case k⋆ > 2: first
≤
apply a transformation of the labels to reduce to correlational queries, and then apply an optimal
T
CSQ algorithm. Moreover, Theorem 3.5 further illuminates the matter, since it identifies the optimal
degree-D test.
Thedefinitionofthegenerativeexponentrevealsthatthedesiredpre-processingisprecisely (y) =
T
ζ (y). We can then combine this pre-processing with an order-k⋆ optimal CSQ algorithm, based on
k⋆
the partial-trace algorithm [HSSS16, FD23, DNGL23], described in Algorithm 1. As shown in Ap-
pendixF,thisalgorithmachievesthepromisedoptimalsamplecomplexity:
Theorem 4.1 (Algorithm 1 Statistical Guarantee). Let {(x i,y i) }n
i=1
be i.i.d. samples from Pw⋆,P. For
any ϵ > 0 and k 1, there exists a constant C k depending only on k and a denoiser : R R
such that if λ˜ = ≥ λ /logk(3/λ ) and n C [dk/2/λ˜2 +d/(λ˜2ϵ2)], Algorithm 1 appliedT to sam→ ples
(x , (y )) k n retk urns a vectok r wˆ with≥ (wˆ k w⋆)2 k 1 ϵ2 wik th probability greater than 1 2e−dc
{ i T i }i=1 · ≥ − −
foraconstantc = c(k)dependingonlyonk.
The partial trace algorithm has been analysed in the context of Tensor PCA [HSSS16, FD23], and
provides efficient recovery of the planted direction up to the computational threshold [DH21]. At the14 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
technical level, the primary challenge we face is that the errors in Algorithm 1 are heavy tailed and
non-Gaussian. Explicitly, while it is possible to reduce learning a single index model to a form of
tensor PCA, the resulting noise matrix has highly correlated entries with heavy tails. For example,
while the individual entries of the resulting noise tensor are of order n−1/2, as for tensor PCA, the
operatornormofthistensorisoforderdk/2 ratherthand1/2 fortensorPCA.
Characterizingthepreciseweakrecoverythreshold.Aremarkableconsequenceofouranalysisis
that, when k⋆ > 2 is even, after applying the partial trace operation to reduce this tensor to a matrix,
this matrix obeys Gaussian universality [BvH23], and in particular, the spectrum of the partial trace
matrix M converges to a semicircle law. Explicitly, this means that the spectrum of M is close to
n n
thespectrumoftheGaussianmatrixGwithE[G] = E[M n]andE[G G] = E[M
n
M n]. Therefore
⊗ ⊗
tounderstandthespectralpropertiesofM ,itsufficestocomputethethecovarianceofM ,whichwe
n n
do in Lemma F.10. From Lemma F.10, we see that when k⋆ 4, the Gaussian equivalent matrix G
≥
canbeapproximatedbyanotherGaussianmatrixG⋆ (inthespaceprobabilityspace)whichsatisfies:
(cid:115) (cid:115)
(14) G⋆ =d β w⋆w⋆T +
E[
T
(Y)2]dk⋆/2
W, G G⋆ ≲
dk⋆ 2−1
,
k⋆ nk⋆(k⋆ 1)!! · ∥ − ∥2 n
−
whereβ
k⋆
= E[ (Y)h k⋆(Z)]isthesignalstrengthafterapplyingtheappropriatelabeltransformation,
T
W is a standard Wigner matrix, and the bound on G G⋆ holds with high probability. We can
∥ − ∥2
thereforeleverageexistingresultsforspikedWignermatricestogetexactconstantsforweakrecovery
when considering this partial trace estimator. With high probability, the spectrum of G⋆ EG⋆ is
−
containedintheinterval[ 2R(1+o (1)),2R(1+o (1))]where
d d
−
E[ (Y)2] dk⋆/2
(15) R2 := T .
k⋆(k⋆ 1)!! · n
−
Inaddition,thespectrumofGexhibitsaBBPtransition[BBAP05],i.e. thereisanoutliereigenvalue
at (β + R2/β )(1 + o (1)). It is also known that for β R, (v (G⋆) w⋆)2 1 (R/β )2
k⋆ k⋆ d k⋆ 1 k⋆
≥ · → −
where v (G⋆) is the eigenvector of G⋆ corresponding to the largest eigenvalue (in absolute value).
1
However, as we only prove universality of the spectrum of M (and not of its eigenvectors), this
n
does not directly imply that v (M ) w⋆ has the same behavior. Nevertheless, we empirically verify
1 n
·
this property for M in Figure 4. The combination of Gaussian universality with the BBP transition
n
impliesthereisaphasetransitionforweakrecoveryofw⋆ atthecutoff:
n E[Y2]
(16) δ := = =: δ⋆,
dk⋆/2 β2 k⋆(k⋆ 1)!!
k⋆
−
However, unlike for phase retrieval, in which the spectral estimator gives a tight threshold for esti-
mation [MM18], this constant can be improved by increasing the order of the tensor before applying
partial trace (see Section 3.4). This is consistent with the known statistical-computational tradeoffs
for tensor PCA ([BGL17, WAM19]). We note also that the Gaussian universality does not hold for
k⋆ = 2inwhichR andσ2 arebothorderd/n,correspondingtothesettingstudiedin[MM18].
Remark 4.2 (Memory and Runtime of Algorithm 1). It is possible to implement Algorithm 1 with
O(d) memory (not counting the memory to store the dataset), and runtime O˜ (dk 2+1). This requires
usingLemmaF.3andLemmaF.8toavoidexplicitlycomputingtheHermitetensorh (x).
k
Extension to unknown P.The only instance where we used the knowledge of P in the previous
algorithm is in Lemma F.2, where a suitable thresholding is applied to the labels. This guarantees
T
thatη = E[ (Y)h k(Z)] = 0,leadingtotherecoveryofw⋆ inthehigh-dimensionalregime. However,
T ̸ ˜
it is sufficient to consider a label transformation that has non-negligible correlation with ζ . For
k
TTHECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 15
5 EmpiricalEigenvalueDistribution TopEigenvalues EigenvectorAlignment
Rank1Spike(BBP) LargestEmpiricalEigenvalue
SemicircleLaw GaussianUniversalityPrediction 0.5
4 EmpiricalDistribution 0.50 S Ge ac uo sn sid anLa Urg ne ivs et rE sm alip ti yri Pc ra el dE icig tie on nvalue
δ=δ? 0.4
3 0.45 0.3
2 0.40 0.2
0.35 0.1
1
EmpiricalEigenvectorAlignment
0.0 GaussianUniversalityPrediction
0.30 δ=δ?
0 −0.2 −0.1 Ei0 g.0 envalue0.1 0.2 0.3 0.4 δ:=0. n5 /d2k 0.6 0.7 0.3 0.4 δ:=0. n5 /d2k 0.6 0.7
FIGURE 4. We empirically verify our observation in Lemma F.9 that M
n
satisfies
Gaussian universality for k⋆ > 2. We take k⋆ = 4 and compute M for d = 4096
n
and varying δ = n. The target function is Y = ζ (Z2e−Z2) (see Figure 2). Here
d2 4
δ⋆ = E[Y2] is the predicted BBP threshold [BBAP05] for this problem. The dots
β2k(k−1)!!
k
represent the medians over 10 random seeds of each quantity (eigenvalues and eigen-
vector correlation) and the error bars represent the standard deviation over these 10
trials.
Algorithm1:TensorPowerIterationwithPartialTraceWarmStart
Input: dataset = (x ,y ) n ,momentk
D { i i }i=1
Drawn/2freshsamples from
0
D D
ifk iseventhen
Constructthepartialtracematrix: M
n ←
|D1
0|
(cid:80) (x,y)∈D0yh k(x)[I⊗k− 22 ];
v v (M ),theeigenvectorcorrespondingtothetopeigenvalueofM (inabsolutevalue)
1 n n
←
elseifk isodd then
Constructthepartialtracevector: v
n ←
|D1
0|
(cid:80) (x,y)∈D1yh k(x)[I⊗k− 21 ];
Normalizev v / v ;
n n
← ∥ ∥
ifk 3then
≥
Runlog(d)stepsoftensorpoweriteration:
fori = 1,...,log(d)do
Drawn/2i+2 freshsamples from
i
v 1 (cid:80) yh (x)[vD ⊗(k−1)];D
← Di (x,y)∈Di k
v v/ v ;
← ∥ ∥
end
Runonefinalstepoftensorpoweriteration:
Drawn/4freshsamples from
fin
v 1 (cid:80) yh (D x)[v⊗(k−D 1)];
← D fin (x,y)∈D fin k
v v/ v ;
← ∥ ∥
Output: v
instance, this can be guaranteed in a setting where P is only known to belong to a certain non-
parametric class of distributions, as we now illustrate. Given P , let ϕ denote the orthogonal
k k
polynomialbasisofL2(R,P y). Wenowdecomposeζ
k⋆
inthisb∈ asG
is: ζ
k⋆{
=
(cid:80)}
lυ lϕ l.
ytisneD seulavnegiE 2)?w
·)nM(1v(16 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Assumption4.3(SourceCondition). Form 1,defineε := λ−2(cid:80) υ2 1,thenthereexistsN
≥ m k⋆ l≥m l ≤
suchthatε < 1form N.
m
≥
This assumption is mild as it only prevents extreme cases for which the first harmonics (υ )
l l≤N
have infinitesimally small mass. Note that we choose a polynomial basis only for convenience2. We
˜
cannowbuild asatruncatedrandompolynomialspannedbythefirstM terms;LemmaF.14shows
T ˜
that such a random choice already provides a label transformation with non-negligible correlation
˜ T
η˜= EP[ (Y)h k⋆(Z)],withhighprobabilityoverthedrawoftherandompolynomial.
T
Plugging this lemma directly in the proof of Theorem 4.1 and via a union-bound, we obtain an
analogousguaranteefortherecoveryofw⋆,wherethepriceofnotknowingPonlymanifestsitselfin
theworseningoftheconstant,fromη toη˜. Wecannowgoonestepfurther,andignorek⋆ altogether.
This may be achieved using the technique from [DH18, Algorithm 2], that tries all possible k
∈
1,K andoutputsthedirectionyieldingthebestgoodness-of-fitonaheld-outdatasetofsizeL. The
{ }
overallprocedure,describedinAlgorithm2inAppendixF.8,enjoysthefollowingguarantees:
Corollary 4.4 (Partial Trace for unknown P and k⋆). Let {(x i,y i) }n
i=1
be i.i.d. samples from Pw⋆,P
with k⋆(P) = k⋆. Then, under Assumption 4.3, if n Ω(λ2 dk⋆/2 + d/ϵ2), L ≳ δ−4log(1/δ˜ ), the
≥
k⋆
procedure described in Algorithm 2 with K = k⋆ returns wˆ satisfying (wˆ w⋆)2 1 ϵ2 with
probabilitygreaterthan1 e−dκ δ 2δ˜ k⋆. · ≥ −
− − −
5. EXISTENCE OF SMOOTH DISTRIBUTIONS FOR ANY GENERATIVE
k⋆
EXPONENT
Now that we have identified the precise sample and query complexity of the single index model in
terms of the exponent k⋆(P), we turn to the question of characterizing the class P;k⋆(P) = k for
{ }
any k. We focus our attention on the additive gaussian noisy setting, namely (Z,Y) P satisfy
∼
Y = σ(Z)+τξ, where σ : R R is a link function, and ξ γ 1 is independent of Z. When τ = 0
→ ∼
werecoverthedeterministiccase.
Inthiscontext,theInformationexponentprovidesatransparentdescriptionintermsoftheHermite
decomposition of σ; in particular for each k one can easily construct analytic functions such that
l⋆(P) = l⋆(σ) = k (e.g. the degree-k Hermite polynomial). Such structure is absent in the generative
exponent setting: observe that in virtue of Lemma 2.6, the exponent only depends on the set of level
sets u R;σ(u) = t t,whichdoesnoteasilylenditselftoharmonicanalysis.
{{ ∈ }}
A simple example of link function with k⋆ > 2 is σ(x) = x2e−x2 . For this σ, k⋆ = 4 which
implies the single index model determined by σ is unlearnable in polynomial time without n ≳ d2
samples.3 However, this construction does not easily generalize to higher k⋆. Nonetheless, we are
abletoestablishtheexistenceofsmoothlinkfunctionswithprescribedgenerativeexponent:
Theorem5.1(SmoothSingle-Indexmodelswithprescribedk⋆). Foreachk,thereexistsσ C∞(R)
∈ b
suchthatthedeterministicsingleindexmodelP = (Id σ) γ satisfiesk⋆(P) = k.
# 1
⊗
The main idea of the proof, presented in Appendix G.1, is to build the link function via the coarea
formula, by evolving a one-parameter family of level sets S := σ−1(t) . Each level set S =
t t t
{ }
2We note that from observed data (x ,y ) , one could in particular estimate the marginal P using a (scalar)
i i i≤n y
{ }
non-parametric kernel density estimator, and therefore estimate the first terms of the orthogonal polynomial basis. For
simplicity,andw.l.o.g.,wewillassumethatsuchbasiselementsareavailable.
3Examplesofσwithk⋆ >2werediscoveredinpriorworksaswell,e.g. [MM18,Remark3]. SeeFigure2forafigure
ofthisconstructionalongwiththecorrespondingζ .
4THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 17
{z 1,t,...,z
nt,t
}needstobesuchthatE[h k(z) |z
∈
σ−1(t)] = 0,whichbecomesapolynomialequation
in the points z . We first determine a suitable form for S , based on Hermite-Gauss quadrature, and
j,t 0
then obtain S , t [0,T) as the solution of an ODE that enforces that this condition is preserved
t
∈
through ‘time’ using a Vandermonde-type kernel. See Figure 5 for examples of link functions σ with
generativeexponentsk⋆ = 3,...,8.
k?=3 k?=4 k?=5
4 2 0 2 4 4 2 0 2 4 4 2 0 2 4
− − − − − −
(a) k⋆ =3 (b) k⋆ =4 (c) k⋆ =5
k?=6 k?=7 k?=8
4 2 0 2 4 4 2 0 2 4 4 2 0 2 4
− − − − − −
(d) k⋆ =6 (e) k⋆ =7 (f) k⋆ =8
FIGURE 5. Explicitconstructionsofσ withdifferentprescribedgenerativeexponents.
TheseweregeneratedbynumericallyintegratingtheODEineq.(181).
Finally,weestablishthataddingGaussiannoisetothelabelspreservesthegenerativeexponent:
Theorem 5.2 (Additive Gaussian noise preserves generative exponent). For τ 0 and σ : R R,
we denote Φ τ,σ(u,v) = (u,σ(u)+τv) R2. Then the additive noisy model P˜ ≥ = (Φ τ,σ) #γ
2
sa→ tisfies
∈
k⋆(P˜ ) = k⋆(P),whereP = (Id σ) γ .
# 1
⊗
The proof ofTheorem 5.2reveals thatnon-Gaussian noisedistributionsµalsoyield thesame con-
clusion, provided the characteristic function φ(ξ) = EZ∼µ[e−iξZ] satisfies φ(ξ) = 0 for all ξ. Under
̸
these conditions, like the information exponent, the generative exponent is ‘oblivious’ to additive
˜
noise(albeitwithpotentiallysmallersignalstrengthλ ).
k⋆
6. INFORMATION-THEORETIC SAMPLE-COMPLEXITY
We conclude the analysis of the single-index model by obtaining an upper bound for the sample
complexity, irrespective of the estimation procedure. This question has been addressed in several
planted problems of similar structure [MM18, DH21, MR14], providing the groundwork needed to
establishthefollowingresult:18 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Theorem 6.1 (Information-Theoretic Upper Bound). For all k 1, there exists a procedure which
≥ (cid:16) (cid:17)
returnswˆ with(w w⋆)2 1 ϵ2 withprobabilityatleast1 2e−d ifn = Θ˜ d .
· ≥ − − λ2 kϵ2
In essence, the (inefficient) estimator optimizes a certain correlation over the sphere, and uses a
‘corrector’ step from [DH21, Theorem 5] to yield a tight dependence on ϵ. Combined with the SQ
andLDP-lowerbounds,theseresultsthusestablishasharpcomputation-to-statisticalgap(underboth
theSQandLDPframeworks)forthesingle-indexproblemassoonask⋆(P) > 2.
7. CONCLUSIONS
In this work, we have explored the question of efficiently estimating the planted structure from data
drawn from a single-index model. We identified the generative exponent k⋆(P) as the fundamental
quantitydrivingthesamplecomplexityrequiredforrecoveringthehiddendirection,andprovedtight
matchingupperandlowerbounds,usingthepartialtracealgorithmandtheSQandLDPframeworks
respectively. Taken together, our results give a unified perspective on a variety of planted high-
dimensional problems, and provide evidence of a tight computational-to-statistical gap as soon as
k⋆(P) > 2.
That said, there are nuances that future work should aimto address: On one hand, our partial trace
algorithmincludespoweriterations,whicharenotbona-fideSQ,makingthefine-grainedcomparison
between our upper and SQ-lower bounds somewhat murky [DH21]. On the other hand, our Low-
Degree lower bound concerns the detection problem (that is, testing Pw vs P0 for some w), while the
naturalsettingforusistherecoveryproblem. Althoughthismismatchdoesnotimpactthetightnessof
theratedk⋆/2,inotherhigh-dimensionalinferenceproblemsitrevealsmorefine-grainedinformation,
e.g. [PWBM18],suchassharprecoverythresholds.
One natural extension of our work is to the multi-index setting, in which the labels depend on a
projectionoftheinputontoasubspaceofdimensionr 1. Amulti-indexmodelcanbedescribedby
≥
anorthogonalmatrixW⋆ Rr×dandajointdistributionP (Rr R)over(Z,Y)wherez = W⋆x.
∈ ∈ P ×
The goal in the multi-index setting is to recover the subspace defined by W⋆. This already gives rise
torichstructurenotpresentinthesingle-indexsetting. Forexample,thenaturalgeneralizationofthe
information exponent [AGJ21] is the leap complexity [ABAM23, DTA+24, BBPV23]. However, it
remainsunclearwhatthenaturalgeneralizationofthegenerativeexponentis,evenfork⋆ = 2.THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 19
REFERENCES
[ABAM22] EmmanuelAbbe,EnricBoix-Adsera,andTheodorMisiakiewicz. Themerged-staircase
property: anecessaryandnearlysufficientconditionforsgdlearningofsparsefunctions
ontwo-layerneuralnetworks. arXivpreprintarXiv:2202.08658,2022. 10
[ABAM23] EmmanuelAbbe,EnricBoix-Adsera,andTheodorMisiakiewicz. Sgdlearningonneu-
ralnetworks: leapcomplexityandsaddle-to-saddledynamics,2023. 4,18
[AGJ21] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient
descent on non-convex losses from high-dimensional inference. J. Mach. Learn. Res.,
22:106–1,2021. 18
[AHSS17] AlexandrAndoni,DanielHsu,KevinShi,andXiaoruiSun.Correspondenceretrieval.In
Satyen Kale and Ohad Shamir, editors, Proceedings of the 2017 Conference on Learn-
ing Theory, volume 65 of Proceedings of Machine Learning Research, pages 105–126.
PMLR,07–10Jul2017. 3
[BAGJ21] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient
descent on non-convex losses from high-dimensional inference. Journal of Machine
LearningResearch(JMLR),22:106–1,2021. 4,7
[BBAP05] Jinho Baik, Gérard Ben Arous, and Sandrine Péché. Phase transition of the largest
eigenvalue for nonnull complex sample covariance matrices. Annals of Probability,
pages1643–1697,2005. 14,15
[BBH+21] M Brennan, G Bresler, S Hopkins, J Li, and T Schramm. Statistical query algorithms
and low-degree tests are almost equivalent. In Conference on Learning Theory, 2021.
12
[BBPV23] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning gaussian multi-
indexmodelswithgradientflow. arXivpreprintarXiv:2310.19793,2023. 18
[BBSS22] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index
modelswithshallowneuralnetworks. arXivpreprintarXiv:2210.15651,2022. 4,46
[BEAH+22] Afonso S Bandeira, Ahmed El Alaoui, Samuel Hopkins, Tselil Schramm, Alexander S
Wein, and Ilias Zadik. The franz-parisi criterion and computational trade-offs in high
dimensionalstatistics. AdvancesinNeuralInformationProcessingSystems,35:33831–
33844,2022. 11
[BGL17] Vijay Bhattiprolu, Venkatesan Guruswami, and Euiwoong Lee. Sum-of-squares certifi-
catesformaximaofrandomtensorsonthesphere,2017. 12,14
[BKM+19] Jean Barbier, Florent Krzakala, Nicolas Macris, Léo Miolane, and Lenka Zdeborová.
Optimal errors and phase transitions in high-dimensional generalized linear models.
Proceedings of the National Academy of Sciences, 116(12):5451–5460, March 2019.
3,4,13
[BvH23] TatianaBrailovskayaandRamonvanHandel. Universalityandsharpmatrixconcentra-
tioninequalities,2023. 14,56
[CBL+20] Minshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and
Richard Socher. Towards understanding hierarchical learning: Benefits of neural repre-
sentations. AdvancesinNeuralInformationProcessingSystems,2020. 4
[CM20] Sitan Chen and Raghu Meka. Learning polynomials in few relevant dimensions. In
ConferenceonLearningTheory,pages1161–1227.PMLR,2020. 4
[DH18] Rishabh Dudeja and Daniel Hsu. Learning single-index models in gaussian space. In
Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the
31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning20 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Research,pages1887–1930.PMLR,06–09Jul2018. 3,7,16,47
[DH21] Rishabh Dudeja and Daniel Hsu. Statistical query lower bounds for tensor pca, 2021.
10,11,13,17,18,55
[DH24] Rishabh Dudeja and Daniel Hsu. Statistical-computational trade-offs in tensor pca and
relatedproblemsviacommunicationcomplexity,2024. 13
[DJS08] Arnak S Dalalyan, Anatoly Juditsky, and Vladimir Spokoiny. A new algorithm for es-
timatingtheeffectivedimension-reductionsubspace. TheJournalofMachineLearning
Research,9:1647–1678,2008. 3
[DK22] IliasDiakonikolasandDanielKane. Non-gaussiancomponentanalysisvialatticebasis
reduction. InConferenceonLearningTheory,pages4535–4547.PMLR,2022. 11,13
[DKL+23] YatinDandi,FlorentKrzakala,BrunoLoureiro,LucaPesce,andLudovicStephan. How
two-layerneuralnetworkslearn,one(giant)stepatatime,2023. 4
[DKS17] IliasDiakonikolas,DanielMKane,andAlistairStewart. Statisticalquerylowerbounds
for robust estimation of high-dimensional gaussians and gaussian mixtures. In 2017
IEEE58thAnnualSymposiumonFoundationsofComputerScience(FOCS),pages73–
84.IEEE,2017. 2,10,12,13
[DLS22] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn
representationswithgradientdescent. InConferenceonLearningTheory,2022. 4,10
[DNGL23] Alex Damian, Eshaan Nichani, Rong Ge, and Jason D Lee. Smoothing the landscape
boosts the signal for sgd: Optimal sample complexity for learning single index models.
arXivpreprintarXiv:2305.10633,2023. 4,13,32,55,56
[DR07] P.J. Davis and P. Rabinowitz. Methods of Numerical Integration. Dover Books on
MathematicsSeries.DoverPublications,2007. 51
[DTA+24] Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborov’a, and
Florent Krzakala. The benefits of reusing batches for gradient descent in two-layer
networks: Breaking the curse of information and leap exponents. arXiv preprint
arXiv:2402.03220,2024. 4,18
[FD23] Michael Jacob Feldman and David Donoho. Sharp recovery thresholds of tensor pca
spectral algorithms. In Thirty-seventh Conference on Neural Information Processing
Systems,2023. 13
[FGR+17] Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh S. Vempala, and Ying Xiao.
Statistical algorithms and a lower bound for detecting planted cliques. J. ACM, 64(2),
2017. 22,23
[HJS01] Marian Hristache, Anatoli Juditsky, and Vladimir Spokoiny. Direct estimation of the
indexcoefficientinasingle-indexmodel.TheAnnalsofStatistics,29(3):595–623,2001.
3
[HMSW04] Wolfgang Härdle, Marlene Müller, Stefan Sperlich, and Axel Werwatz. Nonparametric
andsemiparametricmodels,volume1. Springer,2004. 3
[Hop18] Samuel Hopkins. Statistical inference and the sum of squares method. PhD thesis,
CornellUniversity,2018. 3,11
[HSSS16] Samuel B Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer. Fast spectral al-
gorithmsfromsum-of-squaresproofs: tensordecompositionandplantedsparsevectors.
In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing,
pages178–191,2016. 13
[Ich93] Hidehiko Ichimura. Semiparametric least squares (sls) and weighted sls estimation of
single-indexmodels. Journalofeconometrics,58(1-2):71–120,1993. 3THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 21
[Kea98] MichaelKearns. Efficientnoise-tolerantlearningfromstatisticalqueries. Journalofthe
ACM(JACM),45(6):983–1006,1998. 3
[KKSK11] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of
generalizedlinearandsingleindexmodelswithisotonicregression. AdvancesinNeural
InformationProcessingSystems,24,2011. 3
[KS09] AdamTaumanKalaiandRaviSastry. Theisotronalgorithm: High-dimensionalisotonic
regression. InCOLT,2009. 3
[KWB19] Dmitriy Kunisky, Alexander S Wein, and Afonso S Bandeira. Notes on computational
hardnessofhypothesistesting: Predictionsusingthelow-degreelikelihoodratio. arXiv
preprintarXiv:1907.11636,2019. 3,11,13
[MLKZ20] Antoine Maillard, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborová. Phase
retrievalinhighdimensions: Statisticalandcomputationalphasetransitions,2020. 3,4,
13
[MM18] Marco Mondelli and Andrea Montanari. Fundamental limits of weak recovery with
applicationstophaseretrieval,2018. 3,4,6,8,13,14,16,17,26,44
[MN83] P. McCullagh and J.A. Nelder. Generalized Linear Models. Monographs on Statistics
andAppliedProbability.SpringerUS,1983. 3
[MR14] AndreaMontanariandEmileRichard. Astatisticalmodelfortensorpca,2014. 2,17
[NNW12] Jelani Nelson, Huy Nguyen, and David P. Woodruff. On deterministic sketching and
streamingforsparserecoveryandnormestimation,2012. 28
[PWBM18] Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality
and sub-optimality of pca i: Spiked random matrix models. The Annals of Statistics,
46(5):2416–2451,2018. 18
[SZB21] Min Jae Song, Ilias Zadik, and Joan Bruna. On the cryptographic hardness of learning
single periodic neurons. Advances in Neural Processing Systems (NeurIPS), 2021. 3,
13
[WAM19] Alexander S. Wein, Ahmed El Alaoui, and Cristopher Moore. The kikuchi hierarchy
andtensorpca,2019. 12,14
[Wei23] Alexander S Wein. Average-case complexity of tensor decomposition for low-degree
polynomials. In Proceedings of the 55th Annual ACM Symposium on Theory of Com-
puting,pages1685–1698,2023. 11
[ZSWB22] Ilias Zadik, Min Jae Song, Alexander S Wein, and Joan Bruna. Lattice-based methods
surpass sum-of-squares in clustering. In Conference on Learning Theory, pages 1247–
1248.PMLR,2022. 11,1322 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
APPENDIX A. ADDITIONAL NOTATION
A.1. TensorNotation
ThroughoutthissectionletT (Rd)⊗k beak-tensor.
∈
Definition A.1 (Tensor Action). For a j tensor A (Rd)⊗j with j k, we define the action T[A] of
∈ ≤
T onAby
d
(cid:88)
(17) (T[A])
i1,...,i
k−j
:= T
i1,...,i
kAi k−j+1,...,i k
∈
(Rd)⊗(k−j).
i ,...,i =1
k−j+1 k
We will also use T,A to denote T[A] = A[T] when A,T are both k tensors. Note that this
⟨ ⟩
correspondstothestandarddotproductafterflatteningA,T.
Definition A.2 (Permutation/Transposition). Given a k-tensor T and a permutation π S , we use
k
∈
π(T)todenotetheresultofpermutingtheaxesofT bythepermutationπ,i.e.
(18) π(T) := T .
i1,...,i
k
i π(1),...,i
π(k)
DefinitionA.3(Symmetrization). WedefineSym (Rd)⊗2k by
k
∈
1 (cid:88)
(19) (Sym ) = δ δ
k i1,...,i k,j1,...,j k k! i π(1),j1··· i π(k),j k
π∈S
k
whereS isthesymmetricgroupon1,...,k. NotethatSym actsonk tensorsT by
k k
1 (cid:88)
(20) (Sym [T]) = π(T).
k i1,...,i k k!
π∈S
k
i.e. Sym [T]isthesymmetrizedversionofT.
k
We will also overload notation and use Sym to denote the symmetrization operator, i.e. if T is a
k-tensor,Sym(T) := Sym [T].
k
LemmaA.4. ForanytensorT,
(21) Sym(T) T .
∥ ∥F ≤ ∥ ∥F
Proof.
(cid:13) (cid:13)
(cid:13) 1 (cid:88) (cid:13) 1 (cid:88)
(22) Sym(T) = (cid:13) π(T)(cid:13) π(T) = T
∥ ∥F (cid:13)k! (cid:13) ≤ k! ∥ ∥F ∥ ∥F
(cid:13) (cid:13)
π∈S π∈S
k F k
becausepermutingtheindicesofT doesnotchangetheFrobeniusnorm. ■
APPENDIX B. SQ FRAMEWORK FOR SEARCH PROBLEMS
The relevant framework is developed in [FGR+17]. We provide a quick recap of the relevant defini-
tions.THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 23
B.1. MainIngredients
Definition B.1 (Search Problem over Distributions). Let X be a domain, be a set of distributions
D
over X, be a set of solutions, and : 2F be a map to the set of valid solutions. The
F Z D →
distributional search problem is to find a valid solution f (D) given oracle access to samples
∈ Z
from an unknown D . We will also use to denote the set of distributions for which f is a
f
∈ D Z D
validsolution.
Definition B.2 (STAT oracle). Let D be the unknown distribution. Given a tolerance τ and a
∈ D
queryh : X [ 1,1],theSTAT(τ)oraclereturnsavaluewithinτ ofEx∼D[h(x)].
→ −
Definition B.3 (Relative Pairwise Correlation). Given two distributions D ,D and a reference dis-
1 2
tributionD,
(cid:90)
D (x)D (x)
1 2
(23) χ (D ,D ) := dx 1.
D 1 2
D(x) −
DefinitionB.4((γ,β)-correlation). Wesaythatasetofmdistributions = D ,...,D is(γ,β)
1 m
D { }
correlated relative to a distribution D over X if χ (D ,D ) γ for i = j and χ (D ,D ) β
0
|
D i j
| ≤ ̸ |
D0 i i
| ≤
foralli [m].
∈
Definition B.5 (SQ Dimension). Given a search problem and parameters γ,β, we define the sta-
Z
tistical query dimension ( ,γ,β) to be the largest integer m such that there exists a distribution
SD Z
D over X and a finite set of distributions with m such that for any f ,
0 D D
D ⊂ D |D | ≥ ∈ F
:= is(γ,β)-correlatedrelativetoD .
f D f 0
D D \Z
Thefollowinglemmaisfrom[FGR+17,Corollary3.12]:
Lemma B.6 (General SQ Lower Bound). For any γ′ > 0, any SQ algorithm requires at least
(cid:16) (cid:17)
( ,γ,β) γ′ queriestoSTAT(√γ +γ′)orVSTAT 1 tosolve .
SD Z · β−γ 3(γ+γ′) Z
WewillusethefollowingcorollarywhichisequivalenttoLemmaB.6:
3−γ
Corollary B.7. For any γ,β,τ 0, any algorithm requires at least ( ,γ,β) n queries to
≥ SD Z · β−γ
VSTAT(n)tosolve .
Z
B.2. InstantationfortheSingle-IndexProblem
Wenowinstantiatethisframeworkforthesingle-indexmodelfromDefinition1.1:
Domain: X = Rd R(representsthe(X,Y)pair).
• ×
Distributions: = Pw : w Sd−1 .
• D { ∈ }
SolutionSet: = Sd−1.
• ValidSolutionF s: (Pw⋆) = w : w w⋆ Θ˜ (d−1/2) .
• Inverse:
w
= PZ
w⋆
: w⋆ { Sd∈ −1F and | w· w⋆| ≥ Θ˜ (d−1/2)} ,
• Z { ∈ | · | ≥ }
ReferenceDistribution: D = γ P .
d y
• ⊗
APPENDIX C. HERMITE POLYNOMIALS AND HERMITE TENSORS
WeprovideabriefreviewofthepropertiesofHermitepolynomialsandHermitetensors.24 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
DefinitionC.1. Letu Rd. WedefinethekthnormalizedHermitetensorh
k
(Rd)⊗k by
∈ ∈
( 1)k kγ (u)
d
(24) h (u) := − ∇
k
√k! γ (u)
d
whereγ (u) :=
e−∥u∥2/2
isthePDFofastandardGaussianinddimensions.
d (2π)d/2
Notethatwhend = 1,thisdefinitionreducestothestandardunivariateHermitepolynomials h ,
k
{ }
whichareorthonormalwithrespecttoγ :
1
(25) Eu∼γ1[h j(u)h k(u)] = δ
jk
.
Furthermore, if u,v are correlated Gaussians with correlation α, this inner product scales with αk.
Explicitly,
(cid:18) (cid:20) (cid:21)(cid:19)
1 α
(26) E u,v∼γ(α)[h j(u)h k(v)] = δ jkαk where γ 2(α) = N 0,
α 1
.
2
Theorthogonalitypropertyalsohasatensoranalogue:
(27) Eu∼γ d[h j(u) ⊗h k(u)] = δ jkSym k.
Equivalently,foranyj tensorAandk tensorB:
(28) Eu∼γ [ h j(x),A h k(x),B ] = δ jk Sym(A),Sym(B) .
d ⟨ ⟩⟨ ⟩ ⟨ ⟩
TheHermitetensorsinRd arerelatedtotheunivariateHermitepolynomialsbytheidentity:
(29) h k(u v) = h k(u),v⊗k forallu Rd,v Sd−1.
· ⟨ ⟩ ∈ ∈
APPENDIX D. PROOFS OF SECTION 2
Fact2.1(SpectralVarianceDecomposition). Thevarianceofσ(Z)verifiestheexpansion
(cid:88)
(5) Var P[σ(Z)] = β l2 where β
l
:= EP[Yh l(Z)].
l≥1
Proof. We have Var P[σ(Z)] = E[σ(Z)2] E[σ(Z)]2 and by decomposition of σ into h
k
k≥0, the
− { }
orthogonalbasisofL2(R,γ 1),wehave
(cid:88) (cid:88) (cid:88)
(30) E[σ(Z)2] = E[σ(Z)h l(Z)]2 = E[E[Y Z]h l(Z)]2 = E[Yh l(Z)]2 ,
|
l≥0 l≥0 l≥0
where the last equality stems from the property of conditional expectation. Finally, substracting
E[σ(Z)]2 = β2 endstheproofoftheequality. ■
0
WebeginbycomputingtheHermiteexpansionof dP whereP = P P isthenulldistribution:
dP0 0 z ⊗ y
LemmaD.1. WehavethefollowingexpansioninL2(P ):
0
dP (cid:88)
(31) (z,y) = ζ (y)h (z).
k k
dP
0
k≥0THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 25
Proof. WecandirectlycomputethekthHermitecoefficientofthelikelihoodratioasafunctionofY:
(cid:20) (cid:21)
dP
(32) EP0
dP
(Z,Y)h k(Z) |Y = EP[h k(Z) |Y] = ζ k.
0
ThereforetheHermiteexpansionof dP is,
dP0
(33)
dP
(z,y)
L2 =(P0) (cid:88)
ζ (y)h (z).
k k
dP
0
k≥0
■
WecannowrestateandproveLemma2.3.
Lemma2.3(MutualInformationDecomposition). Wehavethefollowingexpansion
(cid:88)
(7) I χ2[P] = λ2
k
where λ
k
:= ∥ζ
k ∥Py
and ζ
k
:= E[h k(Z) |Y].
k≥1
ProofofLemma2.3. Recallthatthemutualinformationisgivenby
(cid:34) (cid:35)
(cid:18)
dP
(cid:19)2
(34) I χ2[P] = EP0
dP
−1.
0
ThereforebyLemmaD.1,thisisequalto
(cid:88) (cid:88) (cid:88)
(35) I χ2[P] = EPy[ζ k(Y)2] −1 = λ2
k
−1 = λ2 k.
k≥0 k≥0 k≥1
■
Proposition2.6(AVariationalRepresentation). Thegenerativeexponentk⋆(P)canbewrittenas:
(9) k⋆(P) = inf l⋆((Id ) P).
#
T∈L2(Py) ⊗T
Proof. Let k⋆ = k⋆(P). For any k < k⋆ and L2(R,P y), By properties of the conditional
T ∈
expectation:
(36) E[ (Y)h k(Z)] = E[E[ (Y)h k(Z) Y]] = E[ (Y)E[h k(Z) Y]] = E[ (Y)ζ k(Y)] = 0.
T T | T | T
Thereforeforall L2(R,P y),wehavek⋆(P) l⋆((Id ) #P) andhencetakingtheinfimum
T ∈ ≤ ⊗T
oversuch ,ityieldsthat
T
(37) k⋆ inf l⋆((Id ) P).
#
≤ T∈L2(Py) ⊗T
Next, let define for all y R, ∗(y) := ζ k⋆(y) = E[h k⋆(Z) Y = y]. Note that L2(P y) and by
∈ T | T ∈
thesamecalculationaspreviously,wehave
(cid:2) (cid:3)
(38) E[ T ∗(Y)h k⋆(Z)] = E[ T ∗(Y)ζ k⋆(Y)] = E ζ k2 ⋆(Y) = ∥ζ k⋆ ∥2 Py > 0,
whichconcludestheproofofthetheorem.
■
Example2.7(ExplicitExamplesofgenerativeexponent). Wegivethefollowingexplicitexamples:
(i) Forσ apolynomial,wehavek⋆(σ) 2andk⋆(σ) = 2iffσ iseven. Inparticular,k⋆(h ) = 1
j
≤
ifj isoddandk⋆(h ) = 2ifj iseven.
j
(ii) Forσ(z) = z2e−z2 ,wehavek⋆(σ) = 4.26 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
(iii) From[MM18,Remark3],ify 1,1 isbooleanwithP(Y = 1 Z = z) = EW∼γ[tanh(c 1z)2
∈ {− } | −
tanh(c z)2]forcarefullychosenc ,c thenk⋆(σ) = 4.
2 1 2
Proof. Whenσ isapolynomial,weneedtoshowthatthereexistsg P suchthat
y
∈
(39) E[g(σ(z))h l(z)] = 0
̸
for l = 1 or l = 2. Since both σ and h are monotonic after their largest root, picking g(t) =
l
1 forsufficientlylargeR andδ > 0yields
t∈[R−δ,R+δ]
(40) E[g(σ(z))h l(z)] = E[h l(z)1 z∈A],
where either A = I is a single interval (if σ has odd degree) or A = I I two intervals if σ has
− +
∪
even degree. We have E[h 1(z)1 z∈A] = 0 whenever A = I or I
−
= I +, and E[h 2(z)1 z∈A] = 0
̸ ̸ − ̸
otherwise. Toconclude,observethatwecanfindR andδ suchthatI = I iffσ isnoteven.
− +
̸ −
Lemma D.2. Let σ(Z) := Z2exp( Z2). Then the single index model defined by P := (Id σ) γ
# 1
− ⊗
satisfiesk⋆(P) = 4.
ProofofLemmaD.2. As σ is even it suffices to prove that λ = 0 and λ > 0. By Lemma G.2, to
2 4
provethatλ = 0itsufficestocheckthat
4
(cid:88)
(41) sign(σ′(z))zγ(z)
z∈σ−1(y)
is constant in y P -almost everywhere. Therefore it suffices to check this for 0 < y < 1, as
y
max σ(z) = 1. On this interval, σ−1(y) = z (y), z (y),z (y),z (y) with σ′(z (y)) > 0
z 2 1 1 2 1
{− − }
andσ′(z (y)) < 0. Thereforefory (0,1),
2
∈
(cid:88)
sign(σ′(z))zγ(z)
z∈σ−1(y)
= 2[z (y)γ(z (y)) z (y)γ(z (y))]
1 1 2 2
−
= 2[√y √y]
−
(42) = 0.
Next,weneedtoverifythatζ = 0. Infact,weclaimthatβ = 0,i.e. l⋆(P) = 4. Wehavethat:
4 4
̸ ̸
(cid:90) ∞ e−Z2/2
E[σ(Z)h 4(Z)] = Z2e−Z2 He 4(Z) dZ
· · √2π
−∞
(cid:90) ∞ e−3Z2/2
= (Z6 6Z4 +3Z2) dZ
− · √2π
−∞
4√3
=
− 27
(43) = 0
̸
■
byroutineGaussianintegration.
■THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 27
APPENDIX E. PROOFS OF SECTION 3
The main goal of this section is to prove Theorem 3.2. We begin this section by proving some
necessary intermediate results, and then conclude the section proving the aforementioned theorem.
Finally we conclude providing results on reduction from the NGCA to the single index model and
back.
E.1. Proofofthepreliminaryresults
Webeginby generalizingLemmaD.1tothe fulldistributionPw over(X,Y)wherethenull distribu-
tionisnowP0 := γ
d
P y.
⊗
LemmaE.1. Foranyw Sd−1 wehavethefollowingexpansioninL2(P0):
∈
(44)
dPw
(X,Y)
L2 =(P0) (cid:88)
ζ (Y)h (X w)
k k
dP0 ·
k≥0
Proof. NotethatbyDefinition1.1,dPw(X,Y) = γ d−1(X⊥)P(X w,Y)anddP0(X,Y) = γ d−1(X⊥)P 0(X
· ·
w,Y). ThereforebyLemmaD.1,
dPw dP (cid:88)
(45) (X,Y) = (X w,Y) = ζ (Y)h (X w).
k k
dP0 dP
0
· ·
k≥0
■
Wecannowprovetheexpansionofχ2 0(Pw,Pw′)presentedinLemma3.1.
Lemma3.1(χ2 Representation). Letw,w′ Sd−1,anddenotem = w w′,thenwehave
∈ ·
(cid:88)
(12) χ2 0(Pw,Pw′) = mkλ2
k
.
k≥k⋆
ProofofLemma3.1. Wehave:
(cid:20) (cid:21)
(46) χ2 0(Pw,Pw′) := E
P0
d dP Pw
0
·
d dP Pw 0′ −1.
ThereforebyLemmaE.1andtheorthogonalitypropertyofHermitepolynomials,thisisequalto
(cid:88) (cid:88)
χ2 P0(Pw,Pw′) = E[ζ k(Y)2](w ·w′)k −1 = λ2 kmk.
k≥0 k≥1
■
E.2. ProofofTheorem3.2
We now move towards proving Theorem 3.2. For this we introduce two intermediate results in
LemmaE.2andE.3.
LemmaE.2. Letm = w w′. Wehave
·
mk⋆+1
(47) χ2 0(Pw,Pw′)
≤
λ2 k⋆mk⋆ +
1
m.
−28 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Proof. Fromthepreviouslemma,wehavethat
(cid:88)
χ2 0(Pw,Pw′) = λ2 kmk
k≥k⋆
(cid:88)
= λ2 m + λ mk
k⋆ k⋆ k
k>k⋆
(cid:88)
λ2 m + mk
≤
k⋆ k⋆
k>k⋆
mk⋆+1
(48) = λ2 m + .
k⋆ k⋆ 1 m
−
■
Thefollowinglemmashowsthattherearealargenumberofnearlyorthogonalvectors:
Lemma E.3. There exists an absolute constant C such that for any m dd1/4 , there exist m vectors
(cid:113) ≤
w ,...,w withmax w w ϵforϵ = Clog d(m)2 .
1 m i̸=j | i · j | ≤ d
Proof. From[NNW12],wehavethatthereexistsaconstantcsuchthatsuchpointsexistwhenever
(cid:18)
logm
(cid:19)2
(49) d cϵ−2 .
≤ loglogm+log(1/ϵ)
Wewilltake
(cid:114)
Clog (m)2
(50) ϵ = d .
d
forasufficientlylargeconstantC. Thenwehavethat
dlog2d d1/4log1/2d
(51) logm 1/ϵ log2m logm
≤ ⇐⇒ ≤ C2log2m ⇐⇒ ≤ C1/2
whichistruebytheassumptiononm. Inaddition,notethatlog(1/ϵ) ≲ log(d). Therefore,
(cid:18)
logm
(cid:19)2
d
(cid:18) logm(cid:19)2
d
(52) ϵ−2 ≳ =
loglogm+log(1/ϵ) Clog (m)2 · logd C
d
andtakingC sufficientlylargecompletestheproof. ■
Nowweareinplacetoprovethemainresultofthesection,Theorem3.2.
ProofofTheorem3.2. Letm < dd1/4 beapositiveintegertobechosenlater. FromLemmaE.3,there
(cid:113)
existmvectorsw ,...,w suchthat w w ϵforalli = j whereϵ = Clog d(m)2 foranabsolute
1 m | i · j | ≤ ̸ d
constantC. Let
D
= {Pwi : i
∈
[m] }. ForanyPwi,Pwj withi ̸= j,
(w w )k⋆+1
(53) χ2(D ,D ) λ2 (w w )k⋆ + 1 · 2 λ2 ϵk⋆ +2ϵk⋆+1.
0 w1 w2 ≤ k⋆ 1 · 2 1 (w w ) ≤ k⋆
2 2
− ·
Therefore for λ2 > 2ϵ, we can bound this by 2λ2 ϵk⋆ . Therefore ( ,2λ2 ϵk⋆,1) m. Then by
k⋆ k⋆
SD Z
k⋆
≥
CorollaryB.7,
3 2λ2 ϵk⋆ 1 (cid:18) 3 (cid:19)
(54) q m n − k⋆ m 2λ2 ϵk⋆
≥ · 1 2λ2 ϵk⋆ ≥ 2 · n − k⋆
−
k⋆THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 29
whichimplies
3 2q
(55) 2λ2 ϵk⋆ + .
n ≤ k⋆ m
Nowsettingm = 2qngivesthat
1 1
(cid:18)
d
(cid:19)k⋆/2
(56) n ≳ .
≥ λ2 ϵk⋆ λ2 · log2(2qn)
k⋆ k⋆ d
Now let c be a sufficiently small constant which depends only on k⋆ and assume for the sake of
k⋆
contradictionthat
c
(cid:18)
d
(cid:19)k⋆/2
k⋆
(57) n .
≤ λ2 · log2(q)
k⋆ d
Thenwemusthavethat
k⋆ +1
(58) log (2n) +log (2c ) k⋆.
d ≤ 2 d k⋆ ≤
Thereforeforthealgorithmtosucceedwemusthave
1
(cid:18)
d
(cid:19)k⋆/2
1
(cid:18)
d
(cid:19)k⋆/2
1
(cid:18)
d
(cid:19)k⋆/2
n
(59) n ≳ ≳ = .
λ2 log2(2qn) ≥ λ2 (log (q)+k⋆)2 k⋆ λ2 log (q) c
k⋆ d k⋆ d k⋆ d k⋆
Thereforeforsufficientlysmallc wehavederivedacontradictionsowemusthavethat
k⋆
c
(cid:18)
d
(cid:19)k⋆/2
k⋆
(60) n
≥ λ2 · log2(q)
k⋆ d
■
whichcompletestheproof.
Lemma E.4 (Lower Bound for Highly Periodic Neuron). Let σ(Z) = cos(2πγZ). Then for w,w′
withm := w w′ < 1 ,
| · | 8πγ
mγ
(61) χ2 0(Pw,Pw′) ≲ exp(cid:0) −2π2γ2(cid:1) +
1
m.
−
Inaddition,whenγ = cd1/4 andm = d−1/4 forasufficientlysmallconstantc,anyalgorithmrequires
m ≳ dd1/4 queriestoVSTAT(n)torecoverw⋆ whenP = γ(z)δ (y)unlessn ≳ dd1/4 .
σ(z)
Proof. WewillbeginbycomputingP . Notethatthelevelsetsaregivenby:
z|y
(cid:91) arccos(y)+2πj
(62) z : σ(z) = y = Z+(y),Z−(y) where Z±(y) := ± .
{ } { j j } j 2πγ
j∈Z
Thereforebythecoareaformula,
(cid:80) γ(Z+(y))δ (z)+γ(Z−(y))δ (z)
(63) P =
j∈Z j Z j+(y) j Z j+(y)
.
z|y (cid:80) γ(Z+(y))+γ(Z−(y))
j∈Z j j
WewillnowusethePoissonsummationformula. Foranyf,
(cid:88) (cid:88)
(64) f(Z+(y)) = γ cos(jarccos(y))fˆ (γj).
j
j∈Z j∈Z30 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Plugginginf = γ gives
(cid:88) (cid:88) (cid:0) (cid:1)
(65) γ(Z+(y)) = γ cos(jarccos(y))exp 2π2j2γ2
j −
j∈Z j∈Z
(cid:88) (cid:0) (cid:1)
(66) = γ +γ cos(jarccos(y))exp 2π2j2γ2 .
−
j̸=0
Thissecondtermcanbeboundedby
(cid:88) (cid:0) (cid:1) (cid:88) (cid:0) (cid:1) 2 (cid:0) (cid:1)
(67) 2 exp 2π2j2γ2 2 exp 2π2jγ2 = 4exp 2π2γ2 .
− ≤ − exp(2π2γ2) 1 ≤ −
j>1 j>1 −
WecanconductthesameanalysiswithZ− sothenormalizationfactoris2γ(1+O(e−cγ2)). Wenow
j
applythesametechniquetocompute ζ . Wehavethatfork γ,
k
{ } ≤
(cid:12) (cid:12)
(cid:12)(cid:88) (cid:12)
(cid:12) γ(Z+(y))h (Z+(y))(cid:12)
(cid:12) j k j (cid:12)
(cid:12) (cid:12)
j∈Z
(cid:12) (cid:12)
(cid:12)(cid:88)
(cid:0) (cid:1)
(cid:12)
= γ(cid:12) cos(jarccos(y))(2πjγ)kexp 2π2j2γ2 exp( jiπ/2)(cid:12)
(cid:12) (cid:12)
− −
(cid:12) (cid:12)
j∈Z
(cid:12) (cid:12)
(cid:12) (cid:88) (cid:0) (cid:1) (cid:12)
= (cid:12)2γ cos(jarccos(y))(2πjγ)kexp 2π2j2γ2 exp( jiπ/2)(cid:12)
(cid:12) (cid:12)
− −
(cid:12) (cid:12)
j>1
(cid:88) (cid:0) (cid:1)
2γ (2πjγ)kexp 2π2j2γ2
≤ −
j>1
(cid:88) (cid:0) (cid:1)
2γ(2πγ)k jkexp 2π2jγ2
≤ −
j>1
exp(2kπ2γ2)+k!exp(2(k 1)π2γ2)
2γ(2πγ)k −
≤ (exp(2π2γ2) 1)k+1
−
(68) ≲ γ(4πγ)kexp(cid:0) 2π2γ2(cid:1) .
−
Thereforeform < 1 ,
8πγ
(69) χ2 0(Pw,Pw′) ≲ exp(cid:0) −2π2γ2(cid:1)(cid:88) (4πγm j)k + 1mγ
m
k≤γ −
mγ
(cid:0) (cid:1)
(70) 2exp 2π2γ2 + .
≤ − 1 m
−
■
TheSQlowerboundnowdirectlyfollowsfromCorollaryB.7.
E.3. LowDegreePolynomialLowerBound
UsingLemmaE.1,wecandirectlyproveTheorem3.5:
Theorem 3.5 (Low-Degree Method Lower Bound). Let P be a single index model, let P be the dis-
tribution of Pw when w is chosen randomly from Unif(Sd−1). Let X Rn×d and Y Rn denote a
∈ ∈
sequence of n i.i.d. inputs and targets drawn from P. Let P0 := γ d P y denote the null distribution.
⊗THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 31
Let (x,y) denote the likelihood ratio dP (x,y) and let (x,y) denote the orthogonal projection
R dP0 R≤D
inL2(P0)of R(x,y)ontopolynomialsofdegreeatmostD inx. Thenif λD
2 k ≪
√dandδ := dkn ⋆/2,
⌊D⌋
(cid:88)k⋆ (λ2 δ)j
2 = (1+o (1)) 1 (k⋆j 1)!! k⋆ .
≤D d 2|k⋆j
∥R ∥P0 − j!
j=0
ProofofTheorem3.5. Let := dPw denote the likelihood ratio conditioned on w. We begin by
Rw dP0
computingthefulllikelihoodratio:
(cid:34) (cid:35)
R((x 1,y 1),...,(x n,y n)) = Ew (cid:81)[(cid:81) n i=n i 1= P1 [P xw i][ Px [i y, iy ]i]] = Ew (cid:89) i=n 1Rw(x i,y i) .
ThenbyLemmaE.1,wecanexpandthisas
(cid:34) (cid:32) (cid:33)(cid:35)
n
(cid:89) (cid:88)
= Ew ζ k(y i)h k(x i w) .
R ·
i=1 k≥0
We will isolate the low degree part with respect to x ,...,x , which we denote by . To
1 n ≤D
{ } R
computethis,weneedtoswitchtheproductandthesummation:
(cid:34) (cid:32) (cid:33)(cid:35)
∞ n
(cid:88) (cid:88) (cid:89)
R
= Ew ζ ki(y i)h ki(x i ·w) .
p=0 k1+...+kn=p i=1
We note that each term on the right hand side is a polynomial in x ,...,x of degree p which is
1 n
orthogonaltoallpolynomialsofdegreelessthanp. Therefore isgivenby:
≤D
R
(cid:34) (cid:32) (cid:33)(cid:35)
D n
(cid:88) (cid:88) (cid:89)
R≤D = Ew ζ ki(y i)h ki(x i ·w) .
p=0 k1+...+kn=p i=1
WecannowusetheorthogonalitypropertyofHermitepolynomialstocomputethenormswithrespect
tothenulldistributionP0. Ififw,w′ areindependentdrawsfromtheprioronw then:
(cid:34) (cid:32) (cid:33)(cid:35)
D n
(cid:88) (cid:88) (cid:89)
∥R≤D ∥2 L2(P0) = Ew,w′ λ2 ki(w ·w′)ki
p=0 k1+...+kn=p i=1
(cid:32) (cid:33)
D n
(cid:88) (cid:88) (cid:89)
= E[(w w′)p] λ2 .
· ki
p=0 k1+...+kn=p i=1
Let z be a random variable with distribution w w′ where w,w′ are drawn independently from the
·
prior on w, and let be the projection operator onto polynomials of degree at most D in z. Then
≤D
P
wecanrewritetheaboveexpressionas:
(cid:34) (cid:34)(cid:32) (cid:33)n(cid:35)(cid:35)
(cid:88)
∥R≤D ∥2 L2(P0) = Ez P≤D λ2 kzk .
k≥0
Bylinearityofexpectationandoftheprojectionoperator ,wecanexpandthisusingthebinomial
≤D
P
theorem:
  
(cid:18) (cid:19)
(cid:32) (cid:33)j
(cid:88) n (cid:88)
∥R≤D ∥2 L2(P0) = j E P≤D λ2 kzk .
j≥0 k≥k⋆32 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Wecannowlowerbound 2 byisolatingthetermswherek = k⋆:
∥R≤D ∥L2(P0)
⌊D/k⋆⌋(cid:18) (cid:19)
∥R≤D
∥2
L2(P0) ≥
(cid:88) n
j
E(cid:2) P≤D(cid:2) λ2 kj ⋆zk⋆j(cid:3)(cid:3)
j=0
⌊D (cid:88)/k⋆⌋(cid:18) n(cid:19) (k⋆j 1)!!
= λ2j1 − =: ⋆.
j k 2|k⋆j(cid:81)k⋆j/2−1(d+2i) R
j=0 i=0
Wecansimilarlyupperboundthisexpressionbyusingthatλ2 1. Pluggingthisinfork > k⋆ gives:
k ≤
⌊D (cid:88)/k⋆⌋(cid:18) n(cid:19) (cid:34) (cid:34) (cid:18) zk⋆+1(cid:19)j(cid:35)(cid:35)
∥R≤D ∥2 L2(P0) ≤ j E P≤D λ2 k⋆zk⋆ + 1 z
j=0 −
⌊D (cid:88)/k⋆⌋(cid:18) n(cid:19) (cid:34) (cid:34) (cid:88)j (cid:18) j(cid:19) (cid:18) zk⋆+1(cid:19)i(cid:35)(cid:35)
= R⋆ +
j
E P≤D
i
(λ2 k⋆zk⋆)j−i
1 z
j=0 i=1 −
⌊D (cid:88)/k⋆⌋(cid:18) n(cid:19) (cid:34) (cid:88)j (cid:18) j(cid:19) (cid:34) (cid:18) zk⋆+1(cid:19)i(cid:35)(cid:35)
= R⋆ +
j
E
i
(λ2 k⋆zk⋆)j−i P≤D−k⋆(j−i)
1 z
j=0 i=1 −
⌊D (cid:88)/k⋆⌋(cid:18) n(cid:19) (cid:34) (cid:88)j (cid:18) j(cid:19) (cid:34) (cid:18) zk⋆+1(cid:19)i(cid:35)(cid:35)
≤
R⋆ +
j
E
i
(λ2 k⋆zk⋆)j−i
1 z
j=0 i=1 −
⌊D (cid:88)/k⋆⌋(cid:18) n(cid:19) (cid:88)j (cid:18) j(cid:19) (cid:20) zk⋆j+i (cid:21)
= ⋆ + λ2(j−i) E
R j i k⋆ (1 z)i
j=0 i=1 −
⌊D/k⋆⌋(cid:18) (cid:19) j (cid:18) (cid:19)
⋆ +C (cid:88) n (cid:88) j λ2(j−i) E(cid:2) zk⋆j+i(cid:3)
≤ R j i k⋆
j=0 i=1
wherethelastlinefollowsfrom[DNGL23,Lemma26]. Wecannowrelatethek⋆j+iandthek⋆j-th
moments:
2 ⋆
+C⌊D (cid:88)/k⋆⌋(cid:18) n(cid:19) (cid:88)j (cid:18) j(cid:19)
λ2(j−i) z
jk⋆(cid:18) j(k⋆ +1)(cid:19)i/2
∥R≤D ∥L2(P0) ≤ R j i k⋆ ∥ ∥jk⋆ d
j=0 i=1
⌊D/k⋆⌋(cid:18) (cid:19)  (cid:32) (cid:115) (cid:33)j 
= ⋆ +C (cid:88) n λ2j z jk⋆  1+ j(k⋆ +1) 1
R j k⋆ ∥ ∥jk⋆ λ4 d −
j=0 k⋆
(cid:115) ⌊D/k⋆⌋(cid:18) (cid:19)
⋆ + CD (cid:88) n λ2j z jk⋆
≤ R λ4 d j k⋆ ∥ ∥jk⋆
k⋆ j=0
⋆(1+o (1)),
d
≤ R
■
whichcompletestheproof.
Thisimpliesthefollowingcorollary:
Corollary3.6. LetP,P0, beasinTheorem3.5andassumek⋆ > 1. Then,
RTHECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 33
WeakDetection: IfD = log(d)2 andn dγ forγ < k⋆ ,then = 1+o (1).
• ≤ k⋆ 2 ∥R≤D ∥P0 d
StrongDetection: ForanyD √d,ifn d 2 ,then = O (1).
• ≪ ≤ Dk 2⋆ −1 ∥R≤D ∥P0 d
Proof. TheweakrecoverythresholdfollowsdirectlyfromTheorem3.5bysettingδ = dγ−k 2⋆ . Forthe
strongrecoverythreshold,wehavebyTheorem3.5,
⌊D⌋
(cid:88)k⋆ (λ2 δ)j
2 ≲ 1 (k⋆j 1)!! k⋆
≤D 2|k⋆j
∥R ∥P0 − j!
j=0
⌊ (cid:88)kD ⋆⌋(cid:18) e(cid:19)j(cid:18) k⋆j(cid:19)k⋆j/2
≲ (λ2 δ)j
j e k⋆
j=0
⌊ (cid:88)kD ⋆⌋(cid:32) δk⋆k⋆/2jk⋆/2−1(cid:33)j
=
ek⋆/2−1
j=0
(cid:88)∞ (cid:18) δk⋆Dk⋆/2−1(cid:19)j
≤ ek⋆/2−1
j=0
∞
(cid:88)
(2/e)j
≤
j=0
1
=
1 2/e
−
■
whichcompletestheproof.
E.4. ReductionfromSingle-IndexModeltoNGCA
WerestateandproveProposition3.7.
Proposition3.7(Single-IndextoNGCAReduction). AnSQalgorithmthatsolvesNGCAwithplanted
distribution µ of exponent k and direction w∗ Sd−1 yields an efficient SQ algorithm to solve the
∈
single-indexmodelwithgenerativeexponentk anddirectionw∗.
Proof. Let us fix a single index problem, Pw∗,P with generative exponent k = k⋆(P), and direction
w∗ Sd−1. By definition of the generative exponent, we have that ζ > 0. Define A =
∈ ∥
k ∥Py +
y;ζ (y) > 0 ,A = y;ζ (y) < 0 andα := max P (A ),P (A ) . Thenα > 0asotherwiseζ
k − k y + y − k
{ } { } { }
wouldbe0P -a.e. Inthefollowing,weconsiderwithoutlossofgeneralitythatα = P (A ).
y y +
(cid:82)
Let us define µ(dz) = α−1 A+P z|y(dz,y)P y(dy)
∈
P(R), where we recall that P
z|y
is the con-
ditional distribution of Z given Y. Let us assume that we have access to (x ,y ) , i.i.d. sam-
i i i≤n
{ }
ples distributed according to Pw⋆,P. Now, let us perform rejection sampling, that is, for all i n,
≤
we keep x if and only if y A . This builds a data set of n˜(n) samples x˜ that are
i i + i i≤n˜(n)
∈ { }
i.i.d. from a NGCA-model with distribution µ and direction w⋆. From α > 0, we have that
(cid:82)
Eµ[h k(Z)] = α−1 ζ k(y)P y(dy) > 0. Moreover, the number of samples obtained after rejection
sampling will concentrate (with binomial tails) at n˜ = αn, with α independent of dimension. Hence
any SQ-algorithm that solves the built NGCA model, i.e. that is able to find the direction w∗, will
■
solvethelatersingle-indexoneefficiently.34 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
E.5. ReductionfromNGCAtoSingle-IndexModelVariant
Given X′ Qµ,w⋆, we consider X γ
d
drawn independently from X′, and Y = h k(X X′).
∼ ∼ ·
Assume wlog that w⋆ is the first canonical vector, and let us write x′ = (x′,x¯′) and x = (x ,x¯). The
1 1
conditionaldistributionofY givenX satisfies
(cid:90) (cid:90) (cid:18)(cid:90) (cid:19)
p(y |x) = p(y |x,x′)Q(dx′) = δ
y−h k(x1x′
1+x¯·x¯′)γ d−1(dx¯′) µ(dx′ 1)
R Rd−1
(cid:90) (cid:18)(cid:90) (cid:19)
= δ γ (dz) µ(dx′)
y−h k(x1x′ 1+∥x¯∥z) 1 1
R R
(cid:34) (cid:35)
(cid:90)
γ (z)
1
= Ex′ 1∼µ x¯ h′(x x′ + x¯ z) d H0(z)
{z;h k(x1x′ 1+∥x¯∥z)=y} ∥ ∥| k 1 1 ∥ ∥ |
˜
(71) = ψ (y,x , x ) ,
k 1
∥ ∥
where we used the coarea formula and the rotational symmetry of the Gaussian measure. In other
words, the conditional distribution p(y x) now depends on two scalar summary statistics: the projec-
|
tionalongonehiddendirectionand thenormofx. Thelimitationofthisconstruction,however,isthe
factthatthesignalstrengthx isoforderO(1),whilethefluctuationsoftheuninformativenorm x
1
∥ ∥
arealsooforderΘ(1),leadingtoapresumablyharderestimationtaskthanthatofDefinition1.1.
APPENDIX F. PROOFS OF SECTION 4
Webeginbydefiningtheun-normalizedHermitepolynomialsandHermitetensors:
DefinitionF.1.
(72) He (x) := √k!h (x) and He (x) := √k!h (x).
k k k k
Unlikeh ,He naturallytensorizes,i.e.
k k
d
(cid:89)
(73) He (x) = He (x ).
k i1,...,i k |{j :ij=i}| i
i=1
Forexample,
(74) He (x) = He (x )He (x ).
3 1,1,2 2 1 1 2
F.1. Truncatingζ (Y)
k
Throughoutthissection,let
λ2
(75) λ˜2 := k .
k log(3/λ )k/2
k
Lemma F.2. Let P . Then for any k, there exists a bounded function : R [ 1,1] such that
EP[
T
(Y)h k(Z)] ≳ λ˜∈ 2 kG andEP[
T
(Y)2] ≲ λ˜2 k. T → −THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 35
Proof. Let k⋆ = k⋆(P) and recall ζ k⋆(y) := EP[h k⋆(Z) Y = y] where (Z,Y) P. We will fix a
| ∼
truncation radius R and define
T
: R
→
R by
T
(y) := R1ζ k⋆(y)1 |ζ k⋆(y)|≤R. Note that
|T
(y)
| ≤
1 by
definition. Then,
R ·EP[
T
(Y)h k⋆(Z)] = EP[ζ k⋆(Y)2] −EP[ζ k⋆(Y)21
|ζ
k⋆(Y)|≥R]
(76) = λ2
k⋆
−EP[ζ k⋆(Y)21
|ζ
k⋆(Y)|≥R].
Thefirsttermisnonzerobecausebythedefinitionofk⋆. Thereforeitsufficestoprovethatthesecond
termvanishesasR . UsingLemmaI.4andMarkov’sinequality,wecanboundthissecondterm
→ ∞
by:
(cid:12) (cid:12) (cid:112)
(cid:12)EP[ζ k⋆(Y)21
|ζ
k⋆(Y)|≥R](cid:12)
≤
EP[ζ k⋆(Y)2h k⋆(Z)2]P[ |ζ k⋆(Y)
| ≥
R]
≲
(cid:113)
E[ζ k(Y)2]log(3/E[ζ k(Y)2])k⋆
E[ζ k(Y)2]
· R
log(3/λ
)k⋆/2λ2
(77) = k⋆ k⋆.
R
ThereforetakingR = Clog(3/λ
)k⋆/2
forasufficientlylargeconstantC gives:
k⋆
λ2
(78) EP[
T
(Y)h k⋆(Z)] ≳ log(3/λk⋆ )k⋆/2.
k⋆
Inaddition, alsosatisfies:
T
1 λ2
(79) EP[
T
(Y)2]
≤
REP[ζ k⋆(Y)2] ≲ log(3/λk⋆ )k⋆/2.
k⋆
■
F.2. TensorPowerIteration
The following lemma shows that Tensor Power Iteration can be computed with O(d) memory as it
doesnotrequirestoringthetensorh (x).
k
LemmaF.3(EfficientTensorPowerIteration). Foranyv Sd−1,
∈
(80) He (x)[v⊗(k−1)] = xHe (x v) (k 1)vHe (x v).
k k−1 k−2
· − − ·
Proof. BecauseHe (x)tensorizes,wehavethatforw v,
k
⊥
(81) He (x)[v⊗(k−1)] v = He (x v) and He (x)[v⊗(k−1)] w = (x w)He (x v).
k k k k−1
· · · · ·
Therefore,
He (x)[v⊗(k−1)] = xHe (x v)+v[He (x v) (x v)He (x v)]
k k−1 k k−1
· · − · ·
(82) = xHe (x v) (k 1)vHe (x v)
k−1 k−2
· − − ·
bythetwotermrecurrenceforHe . ■
k
Lemma F.4 (Tensor Power Iteration Convergence). Let c,C = c(k),C(k) be constants depending
only on k. Let v Sd−1 with α := v w⋆. Let = (x ,y ) be a dataset of size n with y 1.
i i i∈[n] i
∈ · D { } | | ≤
Let
1 (cid:88) vˆ
(83) vˆ = yh (x)[v⊗(k−1)], and v′ = .
k
n vˆ
(x,y)∈D ∥ ∥36 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
˜
Wewilldenotetheeffectivesignalstrengthbyβ :
k
˜
E[Yh k(Z)]
(84) β :=
k (cid:113)
E[Y2]logk(3/E[Y2])
andwewillassumewithoutlossofgeneralitythatβ˜ > 0. Thenforα [d−1/4,1/2]wehavethatif
k
∈
Cd
(85) n ,
≥ β˜2α2k−4
k
thenwithprobabilityatleast1 2e−dc ,v′ w 2(v w). Inaddition,foranyϵ > 0if
− · ≥ ·
Cd
(86) n ,
≥ ϵβ˜2α2k−2
k
thenwithprobabilityatleast1 2d−c,v′ w⋆ 1 ϵ.
− · ≥ −
Proof. Let λ˜2 := E[Y2]logk(3/E[Y2]). Note that E[vˆ] = w⋆β kαk−1. In addition for any w, we have
byLemmaI.4that
1
λ˜2
(87) E[(vˆ w)2] (E[vˆ] w)2 E[Y2h k(x)[v⊗(k−1),w]2] ≲ .
· − · ≤ n n
ThereforebyLemmaI.3,foranyw Sd−1,withprobabilityatleast1 δ,
∈ −
(cid:115) (cid:115)
λ˜2log(1/δ) log(n/δ)k/2 λ˜2log(1/δ)
(88) (vˆ E[vˆ]) w ≲ + ≲ ,
| − · | n n n
becausen d. Next,foranyxletx⊥ := P⊥ xanddecomposevˆ⊥ as:
≥ w,v
1 1 (cid:88)
(89) vˆ⊥ = yx⊥h (v x).
k−1
√k · n ·
(x,y)∈D
Thenx⊥ isindependentofy,h (v x)sothisisequalindistributionto
k−1
·
 
1 1 (cid:88)
(90) N 0,
k · n2
y2h k−1(v ·x)2I d.
(x,y)∈D
Thereforebythestandardχ2 tailbound,withprobabilityatleast1 2e−d,
−
d 1 (cid:88)
(91) v⊥ 2 ≲ y2h (v x)2.
k−1
∥ ∥ n · n ·
(x,y)∈D
AgainbyLemmaI.3wehavethatwithprobabilityatleast1 δ,
−
 (cid:115) 
d λ˜ log(1/δ) log(n/δ)k
(92) v⊥ 2 ≲ λ˜2 + + .
∥ ∥ n · n n
Therefore for n d, we have with probability 1 2e−dc that v⊥ 2 ≲ λ˜2d. In combination with the
≥ − ∥ ∥ n
aboveboundon (vˆ E[vˆ]) w ,thisgivesthatwithprobabilityatleast1 2e−dc ,
| − · | −
λ˜2d
vˆ E[vˆ] 2 ≲ .
∥ − ∥ nTHECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 37
In the first case when α [d−1/4,1/4] and n ≳ d , this gives that with probability at least
∈ β˜2α2k−4
k
1 2e−dc ,
−
vˆ w⋆
v′ w⋆ = ·
· vˆ
∥ ∥
β kαk−1 +(vˆ E[vˆ]) w⋆
− ·
≥ β kαk−1 + vˆ E[vˆ]
∥ − ∥
3β αk−1
4 k
≥ β kαk−1 + vˆ E[vˆ]
∥ − ∥
3β αk−1
4 k
≥ β αk−1 +β αk−2/8
k k
6α
=
8α+1
(93) 2α
≥
becauseα 1/4. Inthesecondcasewhenn ≳ d ,wehavewithprobabilityatleast1 2e−dc ,
≤ ϵβ˜2α2k−2 −
k
(94) v′ w⋆ =
vˆ ·w⋆ β kαk−1 +(vˆ −E[vˆ]) ·w⋆ β kαk−1(1 −ϵ/2)
1 ϵ
· vˆ ≥ β kαk−1 + vˆ E[vˆ] ≥ β kαk−1(1+ϵ/2) ≥ −
∥ ∥ ∥ − ∥
■
whichcompletestheproof.
F.3. PartialTrace
Webeginbydefiningtheun-normalizedorthogonalpolynomialsforχ2 randomvariables:
DefinitionF.5.
Letp(d)(r)bedefinedby
k
k (cid:18) (cid:19) k−1
(cid:88) k (cid:89)
(95) p(d)(r) = rj( 1)k−j (d+2i).
k j −
j=0 i=j
Thesesatisfythefollowingorthogonalityrelation:
LemmaF.6. Letr χ2(d). Then,
∼
k−1
(cid:89)
(96) E[p( jd)(r)p( kd)(r)] = δ jkk!2k (d+2i).
i=038 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Proof. Foranyj k,
≤
k (cid:18) (cid:19) k−1
(cid:88) k (cid:89)
E[rjp(d)(r)] = E[rl+j]( 1)k−l (d+2i)
k l −
l=0 i=l
k (cid:18) (cid:19) l+j−1 k−1
(cid:88) k (cid:89) (cid:89)
= ( 1)k−l (d+2i) (d+2i)
l −
l=0 i=0 i=l
k−1 k (cid:18) (cid:19) l+j−1
(cid:89) (cid:88) k (cid:89)
= (d+2i) ( 1)k−l (d+2i)
l −
i=0 l=0 i=l
k (cid:89)−1 (cid:88)k (cid:18) k(cid:19) (cid:18)d
+l+j
1(cid:19)
= (d+2i) ( 1)k−l 2 − 2jj!
l − j
i=0 l=0
k−1
(cid:89)
(97) = 1 2kk! (d+2i)
j=k
i=0
where the last line followed from the fact that the kth finite difference of the polynomial g(l) =
(cid:0)d 2+l+j−1(cid:1) is0unlessj = k,inwhichcaseitis1. ■
j
ThesearerelatedtotheHermitepolynomialsbythefollowinglemma:
LemmaF.7. Foranyx Rd,
∈
(98) p(d)( x 2) = He (x)[I⊗k]
k ∥ ∥ 2k
Proof. Note that both sides are monic polynomials in x 2 . In addition, the right hand side is an
∥ ∥
orthogonalfamilyofpolynomialsin x 2 becauseforj = k,
∥ ∥ ̸
(cid:2) (cid:3)
(99) E h 2j(x)[I⊗j]h 2k(x)[I⊗k] = 0
becauseE[h
2j
h 2k] = 0. Becausethesetofmonicorthogonalpolynomialsisunique,wemusthave
⊗
thatp(d)( x 2) = (2k!)1/2h (x)[I⊗k]forallx. ■
k ∥ ∥ 2k
LemmaF.8(EfficientComputationofPartialTrace). Ifk = 2j +1,
(100) He (x)[I⊗j] = p(d+2)( x 2)x
k j ∥ ∥
andifk = 2j +2. Then,
(101) He (x)[I⊗j] = p(d+4)( x 2)xxT p(d+2)( x 2)I .
k j ∥ ∥ − j ∥ ∥ d
Proof. WestartwithLemmaF.7forj +1:
(102) He (x)[I⊗(j+1)] = p(d) ( x 2).
2j+2 j+1 ∥ ∥
Differentiatingbothsideswithrespecttoxinthev directiongives:
(103) (2j +2)Sym(v He (x))[I⊗j+1] = (2j +2)(x v)p(d+2)( x 2).
⊗ 2j+1 · j ∥ ∥
Note that the left hand side is simply equal to He (x)[I⊗j] v, so and rearranging gives the result
2j+1
·
fork odd. Next,wewilldifferentiateagaininthev direction. Thenwehavethat:
(104) (2j +1)Sym(v v He (x))[I⊗j+1]
2j
⊗ ⊗
(105) = (2j)(x v)2p(d+4)( x 2)+p(d+2)( x 2).
· j−1 ∥ ∥ j ∥ ∥THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 39
Ofthe(2j+1)!!pairingsonthelefthandside,(2j 1)!!pairv withitselfandthenpairallindicesof
−
He (x) while 2j(2j 1)!! pair 2j 2 indices of He (x) and the remaining two with v. Therefore
2j 2j
− −
thelefthandsideisequalto:
(106) p(d)( x 2)+(2j)vTHe (x)[I⊗(j−1)]v.
j ∥ ∥ 2j
Thereforewemusthave
p(d+2)( x 2) pd( x 2)
(107) vTHe (x)[I⊗(j−1)]v = (x v)2p(d+4)( x 2)+ j ∥ ∥ − j ∥ ∥ .
2j · j−1 ∥ ∥ 2j
Becausep(d+2) p(d) = (2j)p(d+2) ,thisreducesto
j − j − j−1
(108) vTHe (x)[I⊗(j−1)]v = (x v)2p(d+4)( x 2) p(d+2).
2j · j−1 ∥ ∥ − j−1
Asthisistrueforanyv Sd−1,thiscompletestheproof. ■
∈
F.4. TheEvenCase
Firstwewillstartwiththeevencase. Wewillshowthatv := v (M )hasgoodalignmentwithw⋆.
1 n
LemmaF.9. Letk 4andletM bethepartialtracematrixinAlgorithm1andassumethat y 1
n i
≥ | | ≤
foralli. Definetheeffectivesignalstrength
˜
E[Yh k(Z)]
(109) β := .
k (cid:113)
E[Y2]logk(3/E[Y2])
Thenfork 4,withprobabilityatleast1 2e−dc ,
≥ −
(cid:115)
β2dk/2
(110) ∥M
n
−E[M n]
∥2
≲ βk
˜2n
.
k
Inaddition,for
dk/2
(111) n ≳ ,
ϵβ˜2
k
wehavethatv = v (M )satisfies(v w⋆)2 1 ϵ.
1 n
· ≥ −
Proof. Let η = β k2 = E[Y2]logk(3/E[Y2]]). We will use Lemma I.5. First we compute σ2 :=
β˜2
k
E[(M
n
M)2] 2.
∥ − ∥
σ2 := E[(M
n
M)2]
2
∥ − ∥
1
= E[(M
1
M)2]
2
n∥ − ∥
1
≲
n
·∥E[M 12]
∥2
(cid:13) (cid:20) (cid:21)(cid:13)
(112) =
1 (cid:13)
(cid:13)EX,Y Y2h
k⋆(X)(cid:104) I⊗k⋆ 2−2(cid:105)2 (cid:13)
(cid:13) .
n ·(cid:13) (cid:13)
240 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
NowbyLemmaI.4,foranyv Sd−1,
∈
(cid:20) (cid:21)
EX,Y Y2(cid:13) (cid:13) (cid:13)h k⋆(X)(cid:104) I⊗k⋆ 2−2 ,v(cid:105)(cid:13) (cid:13) (cid:13)2
(cid:20) (cid:21)
≲ η E (cid:13) (cid:13) (cid:13)h k⋆(X)(cid:104) I⊗k⋆ 2−2 ,v(cid:105)(cid:13) (cid:13) (cid:13)2
·
ηd(cid:13) (cid:13)Sym(cid:16) I⊗k⋆−2 v(cid:17)(cid:13) (cid:13)2
(cid:13) 2 (cid:13)
≤ ⊗
(113) ηdk/2.
≤
Therefore,
ηdk/2
(114) σ2 ≲ .
n
Weboundσ similarly:
∗
σ ∗2 := sup E[(vT(M
n
−M)w)2]
v,w∈Sd−1
1
= E[(uT(M
1
M)v)2]
n −
1
≲ E[(uTM 1v)2]
n
= 1 EX,Y[Yh k⋆(X)[I⊗k⋆ 2−2 ,u,v]2]
n
1 EX[h k⋆(X)[I⊗k⋆ 2−2 ,u,v]2]
≤ n
=
1 (cid:13) (cid:13)Sym(cid:16) I⊗k⋆−2
u
v(cid:17)(cid:13) (cid:13)2
n ·(cid:13) 2 ⊗ ⊗ (cid:13) F
1
I k⋆−2
≤ n ·∥ ∥F
dk⋆
−1
2
(115) = .
n
Next,byLemmaF.12andLemmaI.6wehavethat
(116) R ≲
dk⋆ 4+2 +log(n)k 2⋆ dk 4⋆
n
andforanyδ′ 0,withprobabilityatleast1 δ′ wehavethat
≥ −
(117) max
∥M
i ∥2 ≲
dk⋆ 4+2 +log(n/δ)k 2⋆ dk 4⋆
.
i∈[n] n n
Thereforewithprobabilityatleast1
nexp(cid:0) d1/k⋆(cid:1)
,wehave
− −
M
dk⋆+2
(118) max ∥ i ∥2 ≲ 4 .
i∈[n] n n
Nowwecanset
(cid:115)
dk⋆+2 dk⋆ dk⋆+1
4 4 2
(119) R = C = C
n · n1/2 · n3/4THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 41
forasufficientlylargeconstantC andapplyLemmaI.5togetthatwithprobabilityatleast1 de−t
− −
ne−d1/k
,
(cid:32) dk⋆−2 d3k⋆+1 dk⋆+2 (cid:33)
(120) ∥M
n
−EM
n ∥2 ≤
2σ +O n14
/2
t1/2 + n7/12
12
t2/3 + n4 t .
Nowfork⋆ 4wecansett = dc forc < 1/8togetthatwithprobabilityatleast1 de−dc ne−d1/k ,
foranyn ≥ dk 2⋆ wehave − −
≥
(cid:114)
ηdk/2
(121) M
n
EM
n 2
≲ σ = .
∥ − ∥ n
Theconclusionforv = v (M )nowdirectlyfollowsfromtheDavis-Kahaninequality. ■
1 n
LemmaF.10. Assumethat(Z,Y) Pandk⋆(P) > 2. Thenforanyk 4,ifwedefineΣby
∼ ≥
d(k−2)/2
(122) Σ :=
k(k 1)!!
·(cid:2) E[Y2He 4(Z)]w⋆⊗4 +4E[Y2He 2(Z)]T +2E[Y2]Sym 2(cid:3)
−
whereT = 6Sym(w⋆⊗2 I) w⋆⊗2 I I w⋆⊗2,then
⊗ − ⊗ − ⊗
(123) E[M
1
M 1] Σ
op
≲ dk− 24 .
∥ ⊗ − ∥
Proof. We will temporarily switch to the un-normalized Hermite polynomials He . If k = 2j + 2,
k
thisisequalto
k!E[Y2h k(X)[I⊗k− 22 ] h k(X)[I⊗k− 22 ]]
⊗
E[Y2He k(X)[I⊗k− 22 ] He k(X)[I⊗k− 22 ]]
⊗
(cid:104) (cid:104) (cid:105) (cid:104) (cid:105)(cid:105)
= E Y2 p(d+4)( x )xxT p(d+2)( x 2)I p(d+4)( x )xxT p(d+2)( x 2)I
j ∥ ∥ − j ∥ ∥ ⊗ j ∥ ∥ − j ∥ ∥
(cid:104) (cid:105)
= E Y2p(d+4)( x )2x⊗4
j ∥ ∥
(cid:104) (cid:105)
E Y2p(d+4)( x )p(d+2)( x )x⊗2 I
− j ∥ ∥ j ∥ ∥ ⊗
(cid:104) (cid:105)
E Y2p(d+4)( x )p(d+2)( x )I x⊗2
− j ∥ ∥ j ∥ ∥ ⊗
(cid:104) (cid:105)
(124) +E Y2p(d+2)( x )2I I .
j ∥ ∥ ⊗
We will now compute this term by term. We will use O(dj−1) to refer to error terms whose tensor
operatornormsareboundedbyO(dj−1). Forthefirstterm,wehavethat
(cid:104) (cid:105)
E Y2p(d+4)( x )2x⊗4
j ∥ ∥
= 2jj!dj E[Y2x⊗4]+O(dj−1)
= 2jj!dj[E[Y2He 4(Z)]w⋆⊗4
+6E[Y2He 2(Z)]Sym(w⋆⊗2 I)
⊗
(125) +3E[Y2]Sym(I I)]+O(dj−1).
⊗42 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Forthesecondandthirdterms,
(cid:104) (cid:105)
E Y2p(d+4)( x )p(d+2)( x )x⊗2 I
j ∥ ∥ j ∥ ∥ ⊗
= 2jj!dj E[Y2x⊗2 I]+O(dj−1)
⊗
(126) = 2jj!dj[E[Y2He 2(Z)]w⋆⊗2 I +E[Y2]I I]+O(dj−1).
⊗ ⊗
Finallyforthelasttermwehave:
(cid:104) (cid:105)
E Y2p(d+2)( x )2I I
j ∥ ∥ ⊗
(127) = 2jj!dj E[Y2]I I +O(dj−1).
⊗
Renormalizingbyk!toreducebacktothenormalizedHermitepolynomialsh gives:
k
E[Y2h k(X)[I⊗k− 22 ] h k(X)[I⊗k− 22 ]]
⊗
dj (cid:104)
= E[Y2He 4(Z)]w⋆⊗4
k(k 1)!! ·
−
+E[Y2He 2(Z)][6Sym(w⋆⊗2 I) w⋆⊗2 I I w⋆⊗2]
⊗ − ⊗ − ⊗
(cid:105)
+E[Y2][3Sym(I I) I I] +O(dj−1)
⊗ − ⊗
dj
(128) =
k(k 1)!!
·(cid:2) E[Y2He 4(Z)]w⋆⊗4 +4E[Y2He 2(Z)]T +2E[Y2]Sym 2(cid:3) +O(dj−1)
−
whereT = 6Sym(w⋆⊗2 I) w⋆⊗2 I I w⋆⊗2. ■
⊗ − ⊗ − ⊗
F.5. TheOddCase
Next,wewillstudytheoddcase. Thisismuchsimplerthantheevencaseasitdoesn’trequirematrix
concentration. However,itisnotpossibletodirectlyreachϵerrorwiththisstep. Wethereforeanalyze
thesamplecomplexityforreachingv w⋆ d−1/4:
· ≥
LemmaF.11. Letk beoddandletv bethevectorfromstage1ofAlgorithm1. Assumethat y 1.
n i
˜ | | ≤
Denotetheeffectivesignalstrengthβ by
k
˜
E[Yh k(Z)]
(129) β := .
k (cid:113)
E[Y2]logk(3/E[Y2])
ThenforasufficientlylargeconstantC = C(k),if
Cdk/2
(130) n =
β˜2
k
k+1
withprobabilityatleast1 2e−dc , vn w⋆ d−1/4. Furthermore,ifn Cd 2 ,wehavethatwith
− ∥vn∥ · ≥ ≥ ϵβ˜ k2
probabilityatleast1 2e−dc ,v w⋆ 1 ϵ.
− · ≥ −THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 43
Proof. AsinLemmaF.9,letη := λ2logk(3/λ ). Wewillbeginbycomputingthevariance. Notethat
k k
foranyv Sd−1,byLemmaI.4wehave:
∈
E[Y2h k(X)[I⊗k− 21 ,v]2]
≲ ηE[h k(X)[I⊗k− 21 ,v]2]
k−1
(131) ηd 2 .
≤
ThereforebyLemmaI.3,withprobabilityatleast1 δ wehave
−
(cid:115)
ηlog(1/δ)dk−1 log(n/δ)k/2+1dk−1
(132) (v
n
E[v n]) w ≲ 2 + 2 .
− · n n
Forthenorm,recallthatifk = 2j +1,
(133) √k!h k(x)[I⊗k− 21 ] = xp( jd)( ∥x ∥2).
Let x¯ := xi and let x¯⊥ = x¯ w⋆(x¯ w⋆). Then x¯ is independent of x and x w⋆. Therefore
i ∥xi∥ i i − i · i ∥ i ∥ i ·
viewed as a function of x¯⊥ , v⊥ is a sub-Gaussian vector with constant σ2 = 1 (cid:80)n y2p(d)( x )2.
{ i } n n i=1 i j ∥ ∥
Therefore with probability at least 1 2e−d, v⊥ ≲ σ√d so it suffices to bound σ. We have by
− ∥ n∥
LemmaI.3,
(134) σ2 ≲ ηdj +O˜ (dj/√n).
Therefore,withprobabilityatleast1 2e−dc ,σ2 ≲ λ2logk(3/λ )dj and
− k k
(cid:115)
(135) (cid:13) (cid:13)v⊥(cid:13) (cid:13) ≲
ηdk+ 21
.
n n
Combiningthiswiththeboundof(v
n
E[v n]) w⋆ thisgiveswithprobabilityatleast1 e−dc ,
− · −
(cid:115)
ηdk+1
(136) v
n
E[v n] ≲ 2 .
∥ − ∥ n
Combiningeverythinggivesthatwhenn =
Cdk/2/β˜2,
k
(cid:115) (cid:115)
(137)
v
n w⋆
β
k
+(v
n
−E[v n]) ·w⋆
≳
β k2n
=
β˜ k2n
= Cd−1/4.
v
n
· ≥ β
k
+ v
n
E[v n] ηdk+ 21 dk+ 21
∥ ∥ ∥ − ∥
Inaddition,whenn = Cdk+ 21/(ϵβ˜ k2),
(138)
v
n w⋆
β
k
+(v
n
−E[v n]) ·w⋆ β k(1 −ϵ/2)
1 ϵ.
v
n
· ≥ β
k
+ v
n
E[v n] ≥ β k(1+ϵ/2) ≥ −
∥ ∥ ∥ − ∥
■
F.6. ProofofTheorem4.1
WearenowreadytoproveTheorem4.1:44 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
ProofofTheorem4.1. By Lemma F.2 there exists a truncation of ζ k, : R [ 1,1] such that the
˜ T → −
effectivesignalstrengthλ satisfies:
k
(139) λ˜2 =
EP[
T
(Y)h k⋆(Z)]2
≳
λ2
k⋆ .
k EP[ (Y)2]logk(3/EP[ (Y)2]) log(3/λ k)2k⋆
T T
We will first show that the output of the first stage satisfies v w⋆ = Θ(1). For k = 1, this follows
·
directlyfromLemmaF.11. Fork = 2,theresultfollowsfrom[MM18,Theorem2]. Fork 2withk
≥
even,theresultfollowsfromLemmaF.9. Finally,whenk 3withkoddwehavethatafterthepartial
≥
trace warm start, by Lemma F.11 v w⋆ d−1/4. Then until v w⋆ = 1/4, each step of tensor power
iteration will double v w⋆ with n
≳· ≥
d by Lemma
F.·
4. This will converge to v w⋆ = 1/4
· (v·w⋆)2k−4β˜2 ·
(cid:0) (cid:1) k
in log d1/4 log(d) steps. Finally, by Lemma F.4 one more step of tensor power iteration with
≤
n d gives (v w⋆)2 1 ϵ. As every step happens with probability at least 1 2e−dc , a union
≥ ϵβ˜2 · ≥ − −
k
boundgivesthatthefinalsuccessprobabilityisalso1 2e−dc forconstantcdependingonlyonk. ■
−
F.7. Concentration
(cid:104) (cid:105)
LemmaF.12. LetX γ d,letk beanevennumber,anddefineM := h k(x) I⊗k− 22 . Then,
∼
(140) E[ ∥M 2 ∥2 2]1/2
≤
dk+ 42 and E[ |∥M ∥2 −E ∥M ∥2 |p]1/p ≲ pk/2dk 4.
Proof. Forthefirstinequalitywehave:
E[ ∥M
2
∥2 2]
≤
E[ ∥M
2
∥2 F]
(cid:88)
= E[h k(X)[I⊗k− 22 ,e i,e j]2]
i,j
(cid:88)(cid:13) (cid:16) (cid:17)(cid:13)2
= (cid:13)Sym I⊗k−2 e e (cid:13)
(cid:13) 2 i j (cid:13)
⊗ ⊗ F
i,j
(141) d2 dk− 22 = dk+ 22 .
≤ ·
Forthemomentbound,firstnotethat
(142) E[Tr(M)2] = E[h k(X)[I⊗k/2]2]
≤
∥I ∥k
F
= dk/2.
Next, note that by symmetry, there exist polynomials p,q of degree at most k⋆ 1 such that M =
2 −
p( x 2)xxT +q( x )2I. WecanexpandTr(M)as:
∥ ∥ ∥ ∥
(143) Tr(M) = p( x 2) x 2 +q( x 2)d.
∥ ∥ ∥ ∥ ∥ ∥
Therefore both p( x 2) x 2 and q( x 2)d must have variance bounded by dk/2. By Gaussian hyper-
∥ ∥ ∥ ∥ ∥ ∥
contractivity,theyalsohavepnormsboundedby(p 1)k/2dk/4. Then,
−
(cid:0) (cid:1)
M = max q( x 2) , p( x 2) x 2 +q( x 2)
2
∥ ∥ | ∥ ∥ | | ∥ ∥ ∥ ∥ ∥ ∥ |
(144) p( x 2) x 2 + q( x 2)
≤ | ∥ ∥ |∥ ∥ | ∥ ∥ |
solettingC denotethemeanoftherighthandside,ifwesubtractC frombothsideswegetthat
(145) E[ M
2
C p]1/p ≲ pk/2dk/4.
|∥ ∥ − |THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 45
Finally,
E[ M 2 E M 2 p]1/p E[ M 2 C p]1/p + E M 2 C
|∥ ∥ − ∥ ∥ | ≤ |∥ ∥ − | | ∥ ∥ − |
2E[ M
2
C p]1/p
≤ |∥ ∥ − |
(146) ≲ pk/2dk/4
■
wherethesecondtolastlinefollowsfromJensen’sinequality.
(cid:104) (cid:105)
LemmaF.13. LetX γ d,letk beanevennumber,letv Sd−1 anddefineM := h k(x) I⊗k− 23,v .
∼ ∈
Then,
(147) E[ ∥M 2 ∥2 2]1/2
≤
dk+ 41 and E[ |∥M ∥2 −E ∥M ∥2 |p]1/p ≲ pk/2dk− 41 .
Proof. Asabovewehave
E[ ∥M
2
∥2 2]
≤
E[ ∥M
2
∥2 F]
(cid:88)
= E[h k(X)[I⊗k− 23 ,v,e i,e j]2]
i,j
(cid:88)(cid:13) (cid:16) (cid:17)(cid:13)2
= (cid:13)Sym I⊗k−3 v e e (cid:13)
(cid:13) 2 i j (cid:13)
⊗ ⊗ ⊗ F
i,j
(148) d2 dk− 23 = dk+ 21 .
≤ ·
Inaddition,Asabovewehave
(149) E[Tr(M)2] = E[h k[I⊗k− 21 ,v]2]
≤
∥I ∥k F−1 = dk− 21 .
■
TheremainderoftheproofisidenticaltotheproofofLemmaF.12.
F.8. ProofsforUnknownPLearning
Lemma F.14 (Unknown P label transformation). Assume M N and Assumption 4.3. Let θ
Unif( M−1) and consider Ψ = (cid:80)M θ ϕ . Let R > 0 and de≥ fine ˜ (y) := 1Ψ(y)1 . The∼ n
S l=1 l l T√ R |Ψ(y)|≤R
withprobabilitygreaterthan1 δ overthedrawofθ,forR =
3k⋆
√4 M wehave
− λ k⋆δ 1−εM
(cid:12) (cid:12)
(150) η˜:= (cid:12) (cid:12)EP[ T˜ (Y)h k⋆(Z)](cid:12)
(cid:12)
≳ δ2λ2
k⋆
> 0,
where≳ hidesconstantsink⋆ andappearinginAssumption4.3.
Proof. Let A Mζ
k⋆
= (cid:80)
l≤M
υ lϕ
l
the L2(R,P y)-orthogonal projection of ζ
k⋆
onto the space spanned
bydegree-M polynomials. Wehave
˜
REP[
T
(Y)h k⋆(Z)] = ⟨Ψ,ζ
k⋆ ⟩Py
−EP[ζ k⋆(Y)Ψ(Y)1 |Ψ(Y)|≥R]
= ⟨Ψ,A Mζ
k⋆ ⟩Py
−EP[ζ k⋆(Y)Ψ(Y)1 |Ψ(Y)|≥R]
(cid:28) (cid:29)
A ζ
M k⋆
(151) = A Mζ
k
Ψ, EP[ζ k⋆(Y)Ψ(Y)1 |Ψ(Y)|≥R].
∥ ∥ A ζ −
∥ M k ∥ Py46 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Now,followingtheproofofLemmaF.2weboundthesecondtermintheRHS:
(cid:113)
(cid:12) (cid:12)
(152) (cid:12)EP[ζ k⋆(Y)Ψ(Y)1 |Ψ(Y)|≥R](cid:12)
≤
∥Ψ(Y) ∥2 PyEP[ζ k⋆(Y)21 |Ψ(Y)|≥R]
(cid:113)
= EP[ζ k⋆(Y)21 |Ψ(Y)|≥R]
(cid:112)
EP[h k⋆(Z)4]P[ Ψ(Y) R]
≤ | | ≥
(cid:112)
3k⋆ E[Ψ(Y)2] 3k⋆
= ,
≤ R R
wherewehaveusedthefactthat Ψ = 1.
∥ ∥
Now, thanks to Assumption 4.3, and M N, there exists κ > 0 such that A ζ 2 = λ2 (1
≥ ∥
M k⋆
∥
k⋆
−
ε ) κλ2 . Wethusobtain
M
≥
k⋆
(cid:12) (cid:12)
(cid:12) (cid:12) 1 (cid:12)(cid:28) A ζ (cid:29) (cid:12) 3k⋆
(cid:12) (cid:12)EP[ T˜ (Y)h k⋆(Z)](cid:12)
(cid:12) ≥
Rλ k⋆√κ(cid:12)
(cid:12)
Ψ, AM ζk⋆ (cid:12)
(cid:12) − R2
(cid:12)
∥
M k
∥
Py(cid:12)
λ √κ 3k⋆
k⋆
(153) = θ υ .
R | · |− R2
Finally,weconcludewithabasicanti-concentrationpropertyoftheuniformmeasureon :
M−1
S
LemmaF.15(Anti-ConcentrationoftheUniformmeasure,[BBSS22,LemmaA.7]). Letθ Unif( )
M−1
∼ S
andθ
0 M−1
arbitrary. Foranyϵ > 0,wehaveP[ θ θ
0
ϵ] 4ϵ√M.
∈ S | · | ≤ ≤
Putting everything together, and picking ϵ = δ/(4√M) in Lemma F.15, we obtain that with prob-
√
abilitygreaterthan1 δ,forR
3k⋆
4 √M,
− ≥ λ k⋆δ κ
(cid:12) (cid:12) λ δ√κ 3k⋆
(cid:12) (cid:12)EP[ T˜ (Y)h k⋆(Z)](cid:12)
(cid:12) ≥
4√k⋆
MR − R2
λ δ√κ
k⋆
(154) .
≥ 8√MR
■
Algorithm2:PartialTraceAlgorithm,unknownPandk⋆
Input: Dataset = (x ,y ) n ,largestdegreeM,largestexponentk⋆,signalstrengthλ ,
D { i i }i=1 k⋆
basis(ϕ )
l l≤M
SetR = C/λ2 andR˜ = C˜ .
k⋆
Split intotrain ′ andvalidationD˜ suchthat D˜ = L.
D D (cid:80)| |
Drawrandomθ Unif( )andformΨ = θ ϕ .
∈ SM−1 l≤M l l
fork k⋆ do
≤
RunAlgorithm1on ′ with = R−1Ψ1 toobtainwˆ .
|Ψ|≤R k
ComputeF = 1
(cid:80)LD
Ψ(y
)T
h (x wˆ )1 .
k L l=1 l k l · k |Ψ(y l)|≤R˜
end
ˆ
Definek = argmax F .
k k
| |
Output: wˆ
kˆTHECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 47
Corollary 4.4 (Partial Trace for unknown P and k⋆). Let {(x i,y i) }n
i=1
be i.i.d. samples from Pw⋆,P
with k⋆(P) = k⋆. Then, under Assumption 4.3, if n Ω(λ2 dk⋆/2 + d/ϵ2), L ≳ δ−4log(1/δ˜ ), the
≥
k⋆
procedure described in Algorithm 2 with K = k⋆ returns wˆ satisfying (wˆ w⋆)2 1 ϵ2 with
probabilitygreaterthan1 e−dκ δ 2δ˜ k⋆. · ≥ −
− − −
Proof. Theproofisadaptedfrom[DH18,Theorem14].
Theorem4.1togetherwithLemmaF.14ensuresthat,providedn
dk⋆/2
,withprobabilitygreater
(cid:112)
≫ δ2λ2 k⋆
than1 δ e−dκ , wˆ w⋆ ≲ dk⋆/2/n,aswellas Ψ,ζ Cλ δ.
k⋆ k⋆ k⋆
− − ∥ − ∥ |⟨ ⟩| ≥
Wenowstudytheaccuracyofourgoodness-of-fitstatistic:
Lemma F.16 (Concentration of Goodness-of-Fit Statistic). For any k 1,k⋆ , R˜ > 0 and δ > 0,
∈ { }
wehave
(155)
PD′(cid:40) (cid:12)
(cid:12)F
k
−(wˆ
k
·w⋆)k ⟨Ψ,ζ
k
⟩Py(cid:12)
(cid:12)
≤
3 R˜k
+C
KR˜(cid:112) l √og L(1/δ)
+
R L˜ log(1/δ)log(L/δ)k/2(cid:41)
≥
1 −2δ .
For L logδ−1log(Lδ−1)k , a union bound over 1,k⋆ thus yields, with probability greater than
1
2δ˜ k⋆≫
,
{ }
−
(cid:114)
(cid:16) (cid:17)
˜ ˜
R log 1/δ
(156) (cid:12) (cid:12)F (wˆ w⋆)k Ψ,ζ (cid:12) (cid:12) inf
3k⋆
+C˜
k − k · ⟨ k ⟩Py ≤ R˜ R˜ K √L
(cid:32) (cid:33)1/4
logδ˜−1
= O
L
:= ∆(δ˜ ,L) , k 1,k⋆ .
∀ ∈ { }
Let us now relate the performance of our estimator wˆ in terms of the ‘good’ estimator wˆ . Fol-
kˆ k⋆
lowing[DH18,Theorem14],wehave
(157) (wˆ
w⋆)kˆ
Ψ,ζ F ∆
|
kˆ
· |·|⟨
kˆ
⟩| ≥
kˆ
−
F ∆
k⋆
≥ −
(wˆ w⋆)k⋆ Ψ,ζ 2∆
k⋆ k⋆
≥ | · |·|⟨ ⟩|−
> 0
whenever∆(δ˜ ,L) < Cλ δ 1 Ψ,ζ . Butthisimpliesthat Ψ,ζ ,whichmeansthatkˆ = k⋆.
k⋆ ≤ 2|⟨ k⋆ ⟩| |⟨ kˆ ⟩| ■
ProofofLemmaF.16. WehaveE[F k2]
≤
R˜2 E[h k(X ·wˆ k)2]
≤
R˜2,andbyGaussianhypercontractivity,
(158) E[F kl]
≤
R˜l E[h k(Z)l]
≤
R˜l(l −1)kl/2 .
WecanthenapplyLemmalemmaI.3,toF
k
E[F k]todeducethatforanyδ > 0,
−
(cid:34) R˜(cid:112) log(1/δ) R˜ (cid:35) R˜2
(159) P |F k −E[F k] | ≳ k √L + L log(1/δ)log(L/δ)k/2 ≤ 2δ . Lt2 .48 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Nextweboundtheeffectofthetruncation:
(cid:12) (cid:104) (cid:105)(cid:12)
(160) |E[F k] −E[Ψ(Y)h k(X ·wˆ k)] | = (cid:12) (cid:12)E P Ψ(Y)h k⋆(X ·wˆ k) ·1 |Ψ(Y)|>R˜ (cid:12) (cid:12)
(cid:113)
≤
∥Ψ(Y) ∥2 PyE[h k(X ·w˜ k)21 |Ψ(Y)|≥R˜]
(cid:113)
= E[h k(X w˜ k)21 |Ψ(Y)|≥R]
·
(cid:113)
E[h k(Z)4]P[ Ψ(Y) R˜ ]
≤ | | ≥
(cid:112)
3k E[Ψ(Y)2] 3k
= ,
≤ R˜ R˜
andfinallyletuscompute
(cid:112)
E[Ψ(Y)h k(X wˆ k)] = EY[Ψ(Y)EZ|YEW∼γh k((wˆ
k
w⋆)Z + 1 (wˆ
k
w⋆)2W)]
· · − ·
= (wˆ
k
w⋆)k EY[Ψ(Y)EZ|Yh k(Z)]
·
(161) = (wˆ w⋆)k Ψ,ζ ,
k
· ⟨
k ⟩Py
whereweusedthefactthatHermitepolynomialsareeigenfunctionsoftheOrnstein-Ulhenbecksemi-
■
group.
APPENDIX G. PROOFS OF SECTION 5
G.1. ProofofTheorem5.1
Throughout this section, we will occasionally use the un-normalized Hermite polynomials (Defini-
tionF.1)whichwillsimplifythenotation.
Lemma G.1. There exists a function f : [0,1] [0,1] such that f(0) = 0, f(1) = 1, f is strictly
→
monotonicandforallk N,f(k)(0) = f(k)(1) = 0.
∈
Proof. Let
1 (cid:90) x (cid:26) 1 (cid:27) (cid:90) 1 (cid:26) 1 (cid:27)
(162) f(x) = exp ds where Z = exp ds
Z −s(1 s) −s(1 s)
0 0
− −
Thenitisclearthatf(0) = 0,f(1) = 1,andifa < b,
1 (cid:90) b (cid:26) 1 (cid:27)
(163) f(b) f(a) = exp ds > 0
− Z −s(1 s)
a
−
sof ismonotonic. Finally,wehavethat
1(cid:26) dk−1 (cid:26) 1 (cid:27)(cid:27)
(164) f(k)(0) = exp
Z dxk−1 −x(1 x)
−
(cid:26) (cid:26) (cid:27)(cid:27)
p (x) 1
k
= lim exp
x→0 q k(x) −x(1 x)
−
= 0.
wherep (x),q (x)arepolynomialsinx. Theproofforf(k)(1) = 0isidentical. ■
k kTHECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 49
In the deterministic setting P = (Id σ) #γ where σ C1(R), the condition that ζ
k
= 0 in
⊗ ∈
L2(R,P y) reduces to E[g(Y)h k(Z)] = 0 for any g
∈
L2 Py. From Sard’s theorem, the set of critical
values of σ, ie, the set S
σ
:= y R; z σ−1(y)s.t.σ′(z) = 0 has Lebesgue measure zero. In
{ ∈ ∃ ∈ }
particular, we have that a.e. σ−1(y) is a discrete set with σ′(z) = 0 for any z σ−1(y). Therefore,
̸ ∈
applyingthecoareaformulaleadsto
0 = E[g(Y)h k(Z)]
(cid:90)
= g(σ(z))h (z)γ(z)dz
k
(cid:90) σ′(z)
= g(σ(z))h (z)γ(z)| |dz
k σ′(z)
| |
(cid:90) (cid:18)(cid:90) (cid:19)
h (z)γ(z)
k
(165) = g(y) d (z) dy ,
σ′(z) H0
σ−1(y)
| |
andtherefore
(cid:90)
h (z)γ(z)
k
0 = d (z)
σ′(z) H0
σ−1(y)
| |
(cid:88) h (σ−1(y) )γ(σ−1(y) )
k i i
(166) = , fory / S .
σ′(σ−1(y) ) ∈ σ
i
i | |
Wefirstverifyanequivalentintegralcondition:
Lemma G.2 (Integral Form). The condition (cid:82) h k(z)γ(z)dz = 0 for P -a.e. y is equivalent to the
σ−1(y) |σ′(z)| y
followingquantitybeingconstantiny:
(cid:90)
(167) γ(z)h (z)sign(σ′(z))d (z) = C .
k−1 0
H
σ−1(y)
Proof. Followsdirectlyfromintegratingwithrespecttoy,andusingthat[h γ]′ = h γ. ■
k k−1
−
DefinitionG.3. Givenu = (u 1,...,u n) Rn,wedefineQ(u)by:
∈
n ⌊n/2⌋
(cid:89) (cid:88)
(168) Q(u) := Ez∼γ (u i +z) = (2i 1)!!e n−2i(u),
−
i=1 i=0
wheree (u)isthekthelementarysymmetricpolynomialonu ,...,u .
k 1 n
DefinitionG.4. Givenndistinctpointsu 1,...,u
n
wedefinev(u) Rn,u = (u 1,...,u n)by
∈
Q(u ,...,uˆ ,...,u )
1 i n
(169) v(u) = ,
i (cid:81)
(u u )
i̸=j j − i
where (u ,...,uˆ ,...,u ) is the (n 1)-dimensional vector in which the i-th coordinate has been
1 i n
−
removed.
LemmaG.5. Foranydistinctpointsu ,...,u andfor0 k nwehave
1 n
≤ ≤

1 k = 0
(cid:88)n  
(170) He (u )v(u) = 0 0 < k < n .
k i i

i=1 ( 1)n+1Q(u) k = n
−50 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Proof. We will first prove the result for k < n. Let A Rn×n be defined by A
ij
:= He i−1(x j). Then
∈
we can decompose A = CV where C Rn×n contains the coefficients of the Hermite polynomials,
∈
i.e. C is the coefficient of xj in He (x), and V Rn×n is a Vandermonde matrix defined by
ij i
∈
V = xi−1. Note that C is invertible (since it is triangular) and as the x are distinct, V is invertible
ij j i
as well so A is invertible. Then the unique solution to Av⋆ = e is given by v⋆ = V−1C−1e . By the
1 1
formula for converting Hermite polynomials to monomials, we have that C−1 = C so (C−1e ) =
1 j
| |
(j 2)!!1 . ThereforefromtheformulafortheinverseofaVandermondematrix,
2|j−1
−
(cid:88)
v⋆ = (V−1) (j 2)!!
i ij −
2|j−1
(cid:88) e (x ,...,xˆ ,...,x )
n−j 1 i n
= (j 2)!!
(cid:81)
(x x ) −
2|j−1 k̸=i k − i
(cid:80)
e (x ,...,xˆ ,...,x )(j 1)!!
= 2|j n−j−1 1 i n −
(cid:81)
(x x )
j̸=i j − i
Q(x ,...,xˆ ,...,x )
1 i n
=
(cid:81)
(x x )
j̸=i j − i
(171) = v(x) .
i
Therefore v⋆ = v(x) so Av(x) = e . Next, pick some x which is distinct from x ,...,x . By the
1 n+1 1 n
abovecomputationweknowthat
n+1
(cid:88)
(172) He (x )v(x ,...,x ) = 0
n i 1 n+1 i
i=1
whichimpliesthat
n
(cid:88)
He (x )v(x ,...,x ) = He (x )v(x ,...,x )
n i 1 n+1 i n n+1 1 n+1 i
−
i=1
Q(x ,...,x )
1 n
(173) = He (x ) .
−
n n+1 (cid:81)n
(x x )
i=1 i − n+1
Wecannowtakex onbothsides. Forthelefthandsidewehavefori n:
n+1
→ ∞ ≤
Q(x ,...,xˆ ,...,x ,x )
1 i n n+1
lim v(x ,...,x ) = lim
1 n+1 i (cid:81)
xn+1→∞ xn+1→∞ (x n+1 −x i) j̸=i,j≤n(x j −x i)
= lim
Ez∼N(0,1)[(cid:81) j̸=i(xj+z)]
(cid:81)
xn+1→∞ j̸=i(x j −x i)
(174) = v(x ,...,x ) .
1 n i
Ontherighthandsidewehave:
He (x )
(175) lim n n+1 = ( 1)n.
xn+1→∞
(cid:81)n
i=1(x i −x n+1) −
Puttingittogethergivesthat
n
(cid:88)
(176) He (x )v(x ,...,x ) = ( 1)n+1Q(x ,...,x )
n i 1 n i 1 n
−
i=1THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 51
■
whichcompletestheproof.
Wewillusethefollowingwell-knowresultfortheGauss-Hermitequadrature([DR07]):
LemmaG.6(Gauss-HermiteQuadrature). Letr ,...,r betherootsofHe . Then
1 n n
1
(177) v(r ,...,r ) = > 0.
1 n i nHe (r )2
n−1 i
Lemma G.7. For any k⋆ 0 and any ϵ > 0 there exist points x ,...,x such that if r ,...,r are
1 k⋆ 1 k⋆
≥
therootsofHe (x),wehave
k⋆
(178) x r ϵ i and Q(x ,...,x ) = 0.
i i 1 n
| − | ≤ ∀ ̸
Proof. NotethatQisapolynomialinnvariablesofdegreenwhichcanhaveonlyfinitelymanyroots.
Thereforeitisnotpossibleforallpointsx n [r ϵ,r +ϵ]toberootsofQ. ■
∈×i=1 i − i
WearenowreadytoproveTheorem5.1:
Theorem5.1(SmoothSingle-Indexmodelswithprescribedk⋆). Foreachk,thereexistsσ C∞(R)
∈ b
suchthatthedeterministicsingleindexmodelP = (Id σ) γ satisfiesk⋆(P) = k.
# 1
⊗
Proof. Let r ,...,r be the roots of He . From Lemma G.6, we know that v(r ,...,r ) > 0.
1 k⋆ k⋆ 1 k⋆
Thereforebycontinuitythereexistsδ suchthat
(179) x r δ i = v(x ,...,x ) > 0.
i i 1 n
| − | ≤ ∀ ⇒
ThenbyLemmaG.7thereexistx (0),...,x (0)with x (0) r δ/2suchthatQ(x (0),...,x (0)) =
1 n i i 1 n
| − | ≤ ̸
0. Again by continuity there exists ϵ such that for all x ,...,x with x x (0) ϵ for all i, we
1 n i i
| − | ≤
haveboth
(180) v(x ,...,x ) > 0 and sign(Q(x ,...,x )) = sign(Q(x(0))).
1 n 1 n
Nowletγ(x) := e− √x2/2 bethePDFofastandardGaussianandletxevolveaccordingtotheODE:
2π
(181) x′(t) = γ(x )−1v(x ,...,x ) for i = 1,...,n.
i i 1 n i
We will run this ODE for t [ τ,τ] for τ sufficiently small so that x(t) x(0) < ϵ for all
1
∈ − ∥ − ∥
t [ τ,τ]. Wewillalsodefinethequantity:
∈ −
n
(cid:88)
(182) Z (t) := He (x (t))γ(x (t)).
k k−1 i i
i=1
Thenusingthat d He (x)γ(x) = He (x)γ(x),wehavethatforany1 k k⋆,
dx k−1 − k ≤ ≤
n
(cid:88)
Z′(t) = He (x )γ(x )x′(t)
k − k i i i
i=1
n
(cid:88)
= He (x )v(x(t))
k i i
−
i=1
(cid:40)
0 k < k⋆
(183) = .
( 1)k⋆+1Q(x(t)) k = k⋆
−
ThereforewehavethatZ (t) = Z (0)forallk < k⋆ and Z (t) Z ( t) > 0.
k k k⋆ k⋆
| − − |52 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Note that by construction, x′ i(t) > 0 for all t
∈
[ −τ,τ]. Therefore x i : [ −τ,τ] : R is injective and
forϵsufficientlysmall,theimages x ([ τ,τ]) don’tintersect. Wecannowdefineσˆ by
i i
{ − }
(cid:40)(cid:12) (cid:12)
(cid:12)x−1(x)(cid:12) x x ([ τ,τ])
(184) σˆ(x) := i ∈ i −
τ otherwise.
Note that σˆ is smooth except at σˆ−1(0) σˆ−1(τ). Now let f : [0,1] [0,1] be the mollifier con-
∪ →
structedinLemmaG.1. Thenifσˆ hasgenerativeexponentk⋆,σ(x) := f(σˆ(x)/τ)alsohasgenerative
exponentk⋆ andissmooth. Thereforeitsufficestoshowthatσˆ hasgenerativeexponentk⋆.
WewillcomputeE[He k(x) y]fory (0,τ]. First,letusconsidery = τ. UsingE[He k(x)] = 0we
| ∈
canwrite:
E[He k(x) y = τ] = E[He k(x) y < τ]
| − |
(cid:88)n (cid:90) xi(τ)
= He (x)γ(x)dx
k
−
i=1
xi(−τ)
n
(cid:88)
= He (x (τ))γ(x (τ)) He (x ( τ))γ(x ( τ))
k−1 i i k−1 i i
− − −
i=1
(185) = Z (τ) Z ( τ).
k k
− −
By the above calculation this is 0 for k < k⋆ because Z (τ) = Z ( τ) = Z(0) and it is nonzero for
k k
−
k = k⋆. Next, let us consider y (0,τ). Then σˆ−1(y) is a set of discrete points at which σ is smooth
∈
soy hasacontinuousdensityandwecanapplytheco-areaformula(166):
(cid:88) He (x)γ(x)
k
E[He k(x) |y]
∝ σˆ′(x)
x∈σˆ−1(y) | |
n
(cid:88) He (x (y))γ(x (y)) He (x ( y))γ(x ( y))
k i i k i i
(186) = + − − .
σˆ′(x (y)) σˆ′(x ( y))
i i
i=1 | | | − |
Fromthedefinitionofσˆ wehavethatifx x ([ τ,τ]):
i
∈ −
sign(x−1(x))
(187) σˆ′(x) = i .
x′(x−1(x))
i i
Thereforewecansimplifytheaboveexpressionas:
n
(cid:88)
(188) E[He k(x) |y]
∝
He k(x i(y))γ(x i(y)) |x′ i(y) |+He k(x i( −y))γ(x i( −y)) |x′ i( −y) |.
i=1
Nowbecausex′(t) > 0forallt [ τ,τ],wecanremovetheabsolutevaluestoget:
i ∈ −
n
(cid:88)
E[He k(x) |y]
∝
He k(x i(y))γ(x i(y))x′ i(y)+He k(x i( −y))γ(x i( −y))x′ i( −y)
i=1
(189) = [Z′(y) Z′( y)]
− k − k −
which we computed above. In particular, this is 0 for k < k⋆ and nonzero for k = k⋆ for any y > 0.
Therefore,
Ey(cid:2)
Ex[He k(x)
y]2(cid:3)
= 0 for k < k⋆ and is nonzero for k = k⋆ which completes the
| ■
proof.THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 53
G.2. ProofofTheorem5.2
Theorem 5.2 (Additive Gaussian noise preserves generative exponent). For τ 0 and σ : R R,
we denote Φ τ,σ(u,v) = (u,σ(u)+τv) R2. Then the additive noisy model P˜ ≥ = (Φ τ,σ) #γ
2
sa→ tisfies
∈
k⋆(P˜ ) = k⋆(P),whereP = (Id σ) γ .
# 1
⊗
Proof. Since k⋆(P) = k⋆, by Lemma F.2 there exists g : R [ 1,1] such that E[g(Y)h k⋆(Z)] = 0.
→ − ̸
We consider g R(y) := g(y)1 |y|≤R. For R sufficiently large, we claim that E[g R(Y)h k⋆(Z)] = 0. We
̸
havethat
(cid:12) (cid:12)
E[g R(Y)h k⋆(Z)] E[g(Y)h k⋆(Z)] = (cid:12)E[g(Y)h k⋆(Z)1 |y|≥R](cid:12)
| − |
(cid:112)
E[g(Y)2h k⋆(Z)2]P[ Y R]
≤ | | ≥
(cid:112)
(190) E[Y2]/R2
≤
which vanishes as R . Therefore for sufficiently large R we have E[g R(Y)h k⋆(Z)] = 0. Now
→ ∞ (cid:82) ̸
g
R
L1(R) L2(R). LetusconsideritsFourierrepresentation (y) = gˆ R(ξ)eiξydξ. Then
∈ ∩ T
(cid:90)
(191) E[g R(Y)h k⋆(Z)] = gˆ R(ξ)E[eiξYh k⋆(Z)]dξ ,
which shows that there must exist ξ such that ϕ ξ(y,z) = eiξyh k⋆(z) satisfies EP[ϕ ξ(Y,Z)] = 0.
̸
Now, let G (y,z) = τγ(τy)δ , where γ is the standard Gaussian density. By definition we have
τ z
P˜ = P G
τ
:= τP. Recallthat
τ
isself-adjointinL2(R). Thus
∗ G G
(cid:90)
˜
E P˜[ϕ ξ(Y,Z)] = ϕ ξ(y,z)dP(y,z)
(cid:90)
= ϕ (y,z)d( P)(y,z)
ζ τ
G
(cid:90)
= ∗(ϕ)(y,z)dP(y,z)
Gτ
(cid:90)
(cid:0) (cid:1)
= exp ξ2τ2 ϕ(y,z)dP(y,z)
−
(cid:0) (cid:1)
(192) = exp ξ2τ2 EP[ϕ ξ(Y,Z)] = 0.
− ̸
This shows that k⋆(P˜ ) k⋆(P). But note that by Proposition 2.6, we also have k⋆(P˜ ) k⋆(P).
Thereforek⋆(P˜
) =
k⋆(P≤
).
≥
■
APPENDIX H. PROOFS OF SECTION 6
Theorem 6.1 (Information-Theoretic Upper Bound). For all k 1, there exists a procedure which
≥ (cid:16) (cid:17)
returnswˆ with(w w⋆)2 1 ϵ2 withprobabilityatleast1 2e−d ifn = Θ˜ d .
· ≥ − − λ2 kϵ2
Proof. Wewillshowthat
(cid:16) (cid:17)k/2 (cid:16) (cid:17)
dlog 3 log 3 (cid:18) (cid:19)
(193) n C λ k λ kϵ = Θ˜ d ,
≥ k λ2ϵ2 λ2ϵ2
k k
whereC isanconstantdependingonlyonk,issufficientforrecoverywhp.
k54 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
Throughouttheproofwewilluse≲ tohideconstantsthatdependonlyonk. Let beanδ netof
δ
N
Sd−1 with (3)d. Wedefineg(z,y) := ζ (y)h (z)andλ = ζ . Fixatruncationradius
|Nδ | ≤ δ k k k ∥ k ∥L2(Py)
R anddefineL (w)by
n
n
1 (cid:88)
(194) L (w) := g(w x ,y )1
n
n ·
i i |g(w·xi,yi)|≤R
i=1
Wewillconsidertheestimator:
(cid:12) (cid:12)
(195) wˆ argminmax(cid:12)L (w) λ2(w wˆ)k (cid:12).
∈ wˆ∈Sd−1 w∈N δ n − k ·
WewillbeginbyconcentratingL . FirstnotethatbyLemmaI.4,
n
(196) E[g(w ·X,Y)2] = E[ξ k(Y)2h k(w ·X)2] ≲ λ2 klog(cid:0) 3/λ2 k(cid:1)k/2 .
ThereforebyBernstein’sinequalitywehavethatforanyw Sd−1,withprobabilityatleast1 2e−ι,
∈ −
(cid:118)
(cid:117) (cid:16) (cid:17)k
(cid:117)λ2log 3 2ι
(197) L n(w) E[L n(w)] ≲
(cid:116) k λ2
k +
Rι
.
| − | n n
Thereforebyaunionboundwehavethatwithprobabilityatleast1 2e−d,
−
(cid:118)
(cid:117) (cid:16) (cid:17)k
(cid:117)λ2log 3 2dlog(3/δ)
(198) sup L n(wˆ) E[L n(wˆ)] ≲
(cid:116) k λ2
k
+Rdlog(3/δ)
.
| − | n n
w∈N
δ
NextweboundtheeffectoftruncationonE[L n(w)]:
E[L n(w)] E[g(w X,Y)]
| − · |
(cid:12) (cid:12)
= (cid:12)E[g(w X,Y)1 |g(w·X,Y)|>R](cid:12)
·
(cid:112)
E[g(w X,Y)2]P[ g(w X,Y) > R]
≤ · | · |
(cid:112)
(199) 3k P[ g(w X,Y) > R].
≤ | · |
To control this probability, we use the moment method. By Jensen’s inequality and Gaussian hyper-
contractivitywehaveforallp 1,
≥
(cid:112)
(200) E[ g(w X,Y) p] E[ξ k(Y)2p]E[h k(Z)2p] E[h k(Z)2p] (2p)kp.
| · | ≤ ≤ ≤
ThereforeforR (2e)k,wecantakep = R1/k/(2e)toget
≥
(2p)kp (cid:18) k (cid:19)
(201) P[ g(w X,Y) > R] = exp R1/k .
| · | ≤ Rp −2e
Pluggingthisingives
(cid:18) (cid:19)
k
(202) E[L n(w)] E[g(w X,Y)] 3kexp R1/k .
| − · | ≤ −4e
Finally,becausethek-thHermitecoefficientofν isλ2,wehavethat
k
(203) E[g(w X,Y)] = λ2(w w⋆)k.
· k ·THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 55
Combiningeverythinggivesthatwithprobabilityatleast1 2e−d:
−
(cid:12) (cid:12)
max(cid:12)λ2(w wˆ)k λ2(w w⋆)k
(cid:12)
w∈N k · − k ·
δ
(cid:12) (cid:12) (cid:12) (cid:12)
max(cid:12)L (w) λ2(w wˆ)k (cid:12)+ max(cid:12)L (w) λ2(w w⋆)k (cid:12)
≤ w∈N n − k · w∈N n − k ·
δ δ
(cid:12) (cid:12)
2max(cid:12)L (w) λ2(w w⋆)k (cid:12)
≤ w∈N n − k ·
δ
= 2max L n(w) E[g(w X,Y)]
w∈N | − · |
δ
(cid:18) (cid:19)
k
≲ sup L n(wˆ) E[L n(wˆ)] +3kexp R1/k
| − | −4e
w∈N
δ
(cid:118)
(cid:117) (cid:16) (cid:17)k
(cid:117)λ2log 3 2dlog(3/δ) (cid:18) (cid:19)
(204) ≲
(cid:116) k λ2
k
+Rdlog(3/δ)
+3kexp
k
R1/k .
n n −4e
Thereforeby[DH21,Lemma25],wehavethat
λ2min( wˆ w⋆ , wˆ +w⋆ )
k ∥ − ∥ ∥ ∥
(cid:118)
(cid:117) (cid:16) (cid:17)k
(cid:117)λ2log 3 2dlog(3/δ) (cid:18) (cid:19)
(205) ≲ δ
+(cid:116) k λ2
k
+Rdlog(3/δ)
+exp
k
R1/k .
n n −4e
NowtakeR = (4elog(3/δ))k. Then,
(cid:118)
(cid:117) (cid:16) (cid:17)k
(206) λ2min( wˆ w⋆ , wˆ +w⋆ ) ≲ δ
+(cid:117) (cid:116)λ2 klog λ3
2
k
2dlog(3/δ)
+
dlog(3/δ)k+1
k ∥ − ∥ ∥ ∥ n n
andtakingδ = O(ϵλ2)andusingthat3/δ log(3/δ)k completestheproof. ■
k ≥
APPENDIX I. CONCENTRATION LEMMAS
LemmaI.1(GaussianHypercontractivity). Letf beapolynomialofdegreek. Thenforp 2,
≥
(207) EX∼γ[ f(X) p]2/p (p 1)k Ex∼γ[f(X)2].
| | ≤ −
Suchmomentsimplythefollowingtailbound(e.g. see[DNGL23,Lemma24]):
LemmaI.2. Letδ 0andletX beameanzerorandomvariablesatisfying
≥
2log(1/δ)
(208) E[ X p]1/p Bpk/2 for p =
| | ≤ k
forsomek. Thenwithprobabilityatleast1 δ, X B(ep)k/2.
− | | ≤
We will combine this with the following tail bound which can be easily proved with a routine
truncationargument:
Lemma I.3. Let X ,...,X be independent mean zero random variables such that for all p 2,
1 n
∥X
i ∥p ≤
Bpk/2 for some k and let σ = (cid:80)n i=1E[ ∥X
i
∥2]. Let Y = (cid:80)n i=1X i. Then with probabili≥ ty at
least1 2δ,
−
(cid:112)
(209) Y ≲ σ log(1/δ)+Blog(1/δ)log(n/δ)k/2.
k
∥ ∥56 THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS
˜
Proof. LetRbeatruncationradiustobechosenlateranddefineX = X 1 . Wehavethatwith
i i ∥Xi∥≤R
probability at least 1 δ, X C Blogk/2(1/δ). Therefore by a union bound we have that with
1 k
probabilityatleast1 − δ,m∥ ax∥ X≤ C Blogk/2(n/δ) =: R. LetY˜ = (cid:80) X˜ . Then,
− i ∥ i ∥ ≤ k i i
(cid:13) (cid:13) (cid:88)n (cid:13) (cid:13)
(cid:13) (cid:13)E[Y˜ ] E[Y](cid:13)
(cid:13)
(cid:13) (cid:13)E[X i] E[X˜ i](cid:13)
(cid:13)
− ≤ −
i=1
n
(cid:88)(cid:13) (cid:13)
= (cid:13)E[X i1 ∥X∥≥R](cid:13)
i=1
n (cid:113)
(cid:88)
E[ X
i
2]P[ X R]
≤ ∥ ∥ ∥ ∥ ≥
i=1
n
(cid:88)
σ P[ X
i
R]
≤ ∥ ∥ ≥
i=1
(210) σδ.
≤
Finally,becauseE[X˜2] E[X2],wehavebyBernstein’sinequalitythatwithprobabilityatleast1 δ,
i ≤ i −
(cid:13) (cid:13) (cid:13)Y˜ E[Y˜ ](cid:13) (cid:13)
(cid:13)
≲ σ(cid:112) log(1/δ)+Rlog(1/δ)
−
(cid:112)
(211) ≲ σ log(1/δ)+Blogk/2(n/δ)log(1/δ).
Combiningeverythinggiveswithprobabilityatleast1 2δ,
−
(cid:112)
Y ≲ σδ +σ log(1/δ)+Blogk/2(n/δ)log(1/δ)
∥ ∥
(cid:112)
(212) ≲ σ log(1/δ)+Blogk/2(n/δ)log(1/δ).
■
Wewilloftenusethefollowinglemmafrom[DNGL23,Lemma23]whenwehavetightboundson
X andallmomentsofY butonlyaverylooseboundon X :
∥ ∥1 ∥ ∥2
LemmaI.4. LetX,Y berandomvariableswith Y Bpk/2 for
∥ ∥p ≤
(cid:18) (cid:18) (cid:19)(cid:19)
1 X
p = min 2, log ∥ ∥2 .
k · X
∥ ∥1
Then,
(213) E[XY] X B (ep)k/2.
≤ ∥ ∥1 · ·
Formatrixconcentrationwewillusethefollowingcorollaryof[BvH23,Theorem2.7]:
Lemma I.5. Let Y = (cid:80)n Z where Z n are self-adjoint, mean zero, and independent random
i=1 i { i }i=1
matrices. Define:
(cid:20) (cid:21)1/2
(214) σ := ∥E[Y2] ∥1 2/2, σ ∗ := v,ws ∈u Sp d−1E[(vTYw)2]1/2, R := E m i∈a [nx
]
∥Z i ∥2 2 .
1/2
ThenforanyR R σ1/2+R√2andanyt 0,ifδ = P[max
i∈[n]
Z
i 2
R],thenwithprobability
≥ ≥ ∥ ∥ ≥
atleast1 δ de−t,
− −
(215) Y EY
2
2σ ≲ σ ∗t1/2 +R1/3σ2/3t2/3 +Rt.
∥ − ∥ −THECOMPUTATIONALCOMPLEXITYOFLEARNINGGAUSSIANSINGLE-INDEXMODELS 57
TocomputeR andthetailprobabilityδ,wewillusethefollowinglemma:
Lemma I.6. Let Z n be a sequence of independent random variables with polynomial tails, i.e.
{ i }i=1
there exists B,k such that E[ |Z
i
|p]1/p
≤
Bpk/2. Define R = maxn i=1Z i. Then for any p
≤
log(n)/k,
E[ R p]1/p ≲ Blogk/2(n)andforanyδ 0,withprobabilityatleast1 δ,R ≲ Blogk/2(n/δ).
| | ≥ −
Proof.
(cid:20) (cid:21)1/p
(cid:34)
n
(cid:35)1/p
(cid:88)
(216) E[ R p]1/p = E max Z i p 2 E Z i p 2 n1/pBpk/2.
| | i∈[n] | | ≤ | | ≤
i=1
Nowplugginginp = log(n)/k gives:
(217) E[Rp]1/p =
E(cid:20)(cid:18)
max Z
i
2(cid:19)p(cid:21)1/p B(cid:18) e2log(n)(cid:19)k
2 ≲ Blogk 2(n).
i∈[n] | | ≤ k
InadditionbyMarkov’sinequalitywehavethatwhenp = t2/k ,
e
(cid:18) n1/ppk/2(cid:19)p (cid:18)
k
(cid:19)
(218) P[R tB] = nexp t2/k .
≥ ≤ t −2e
■