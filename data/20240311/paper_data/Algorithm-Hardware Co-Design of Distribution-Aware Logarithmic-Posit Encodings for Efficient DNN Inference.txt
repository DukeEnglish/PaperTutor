Algorithm-Hardware Co-Design of Distribution-Aware
Logarithmic-Posit Encodings for Efficient DNN Inference∗
AkshatRamachandran1,ZishenWan1,GeonhwaJeong1,JohnGustafson2,TusharKrishna1
1GeorgiaInstituteofTechnology,Atlanta,GA,2ArizonaStateUniversity,Tempe,AZ
ABSTRACT
TraditionalDeepNeuralNetwork(DNN)quantizationmethods
usinginteger,fixed-point,orfloating-pointdatatypesstruggleto
capturediverseDNNparameterdistributionsatlowprecision,and
oftenrequirelargesiliconoverheadandintensivequantization-
aware training. In this study, we introduce Logarithmic Posits
(LP),anadaptive,hardware-friendlydatatypeinspiredbyposits
thatdynamicallyadaptstoDNNweight/activationdistributions (a) (b)
byparameterizingLPbitfields.Wealsodevelopanovelgenetic- Figure1:(a)WeightdistributionsofResNet50andViT(De:
algorithmbasedframework,LPQuantization(LPQ),tofindoptimal Decoder,En:Encoder)layers,(b)LP’srelative-accuracyplot,
layer-wiseLPparameterswhilereducingrepresentationaldiver- showingdistribution-awarepropertiescomparedtoAF[20].
gencebetweenquantizedandfull-precisionmodelsthroughanovel notefficientenoughforadoptioninresource-constraineddevices.
global-localcontrastiveobjective.Additionally,wedesignaunified Inspiredbytheefficiencyofintegerscombinedwiththebenefits
mixed-precisionLPaccelerator(LPA)architecturecomprisingof offloatsin[7],weproposeLogarithmicPosits(LP),acomposite
processingelements(PEs)incorporatingLPinthecomputational datatypethatblendstheadaptabilityofpositswiththehardwareef-
datapath.Ouralgorithm-hardwareco-designdemonstratesonaver- ficiencyofLNS.LPexploitsthetaperedaccuracyofposits(regime),
age<1%dropintop-1accuracyacrossvariousCNNandViTmodels. exponentsizeandscalefactor(exponentbias)totailortherepresen-
Italsoachieves∼2×improvementsinperformanceperunitarea tationrange,shapeandpositiontotheDNNparameterdistribution
and2.2×gainsinenergyefficiencycomparedtostate-of-the-art whilecapitalizingonthecomputationalefficiencyofLNS(Fig.1(b)).
quantizationacceleratorsusingdifferentdatatypes. ToutilizeLPforDNNquantization,weintroduceanautomated
1 INTRODUCTION
LPQuantization(LPQ)Frameworkbasedongeneticalgorithms.
LPQoperatesinaPostTrainingQuantization(PTQ)settingwith
Inresponsetotheescalatingcomputationalandstoragedemandsof
accesstoasmallunlabelledcalibrationdataset(128images).Build-
DNNs,compressingmodelsbeforedeployingthemonedgedevices
ingonpreviousworks[2,6,14],weincorporateanovelglobal-local
andcloudservershasbecomeimperative[14,20].Quantization
contrastiveobjectivetocombatover-fittingtothecalibrationdata
hasemergedasoneofthemostpromisingsolutionstoaddressthe
and prevent premature convergence by minimizing divergence
challengesofdeployingDNNsonresource-constraineddevices.
betweenintermediaterepresentations(intermediatelayeroutput
Alongtheselines,numeroustechniquesfocusonuniformquanti-
activations)ofthequantizedandfullprecision(FP)model.
zation,representingvaluesasintegers[19]orfixed-pointnumbers.
ToefficientlyexecutecomputationswithLP,wefurtherpropose
However,asFig.1(a)illustrates,thereissubstantialdistributional
amixed-precisionLPAccelerator(LPA)thatintegratesmixed-
varianceandordersofmagnitudedifferencesinDNNparameters
precisionLPprocessingelements(PEs)intoasystolicarrayarchitec-
betweenlayersandacrossmodels,leadingtosignificantquantiza-
ture.Ourco-designtargetsmixed-precisionquantizationofDNNs.
tionerrorswhenappliedtomodernDNNs[7].
ExtensiveexperimentsonCNNsandViTsdemonstrate<1%dropin
Seeking wider dynamic range and distribution-adaptive data
top-1accuracyacrossmodelfamiliesandsurpassstate-of-the-art
formats,interesthasgrowninnon-uniformquantizationmethods
mixed-precisionaccelerators[7,19]with 2×performanceperunit
involvingfloating-point[14,20],posits[10,16],andlogarithmic
areaimprovementand2.2×energyreduction.
numbersystems(LNS)[1].Adaptivefloating-pointtechniquessuch
as[20]adjusttheexponentrangeempiricallybasedonthedynamic
2 BACKGROUNDANDRELATEDWORK
rangeofparameters.However,theyfailtoadapttothetapered
distributionofDNNparametersanduseflataccuracy.Floating-
StandardPositRepresentation.Thestandardpositformat[8]
pointencodingsalsocomewithincreasedhardwarecomplexity,
withsize𝑛-bitsincludesthesign,regime(𝑟)ofsizers,exponent(𝑒)
wastedbitpatterns,andconvolutedexceptionhandling,hindering
ofsizees,andfraction(𝑓)fields.Unlikefloats,positsencodethe
regimefieldusingarun-length𝑚of0sor1sterminatedbya1or0,
adoptioninedgedevices[15].
respectively,orbythefinalbit.Theregimevalue𝑘isdetermined
Recently, posit-based representations have demonstrated ad-
vantagesoverstandardfloatsforDNNinference,offeringtapered
as𝑘 =−𝑚ifthefirstbitoftheregimeis0,or𝑘 =𝑚−1otherwise.
Theregimecreatestaperedaccuracyinthepositrepresentation
accuracy(duetorun-lengthencodedregime),providingalarger
(Fig.1(b))unlikefloats,whichhasaflataccuracy.Thisproperty,
dynamicrange,higheraccuracy,andsimplerexceptionhandling
whichisbeneficiallyusedinpriorworkssuchas[10,15],ispartic-
[8,15].Posithardware,thoughcheaperthanfloathardware,isstill
ularlyuseful.InPositNN[11],rsismanuallyconfiguredforDNNs,
∗202461stIEEE/ACMDesignAutomationConference(DAC) butthismaynotguaranteeperformanceacrossdifferentmodels.
4202
raM
8
]RA.sc[
1v56450.3042:viXraI hn anc co en atr da as pt, taw bie lip tyro wp io thse oup ta hra am nde ct re ar fi tz ein dg tua nll inp go ,s wit hb ili et- mfie al id ns tat io nie nn g-
L0
Ste Lp
1
1: Candidate In Lit Niz -a 1lization DPNrNeFc u Misll oiodnel
Mbeatxwimeeizne u innlnikeer cplraosdsuecst
( sf sf sf, )
a simha ilr ad rw aa dr ae p-f tr ai be in lid tyly inre sp tr ae ns de an rt dat Lio Nn S( wSe oc ut li don res q3 ua irn ed a4 c). oA mc bh inie av ti in ong ( L0 sf L1 sf LN-1 sf, ) Population BInaptcuht Intermediate RepresC Ceo onn nc ta ca att ie to en n na aste t e( I I R IR R) bMeitnwimeeizne s iinmnielarr pcrloadsusects
ofa Qr ubi atr na tr iy zab ta is oe ns[ O1] b, jl ee ca td ii vn eg .t Po rc eo vm iop ul sic sa tt ue dd iea srit lh evm ee rt ai gc eci crc ou ni vtr ey n.
- poc pa uS n le adl tie d ioc a nt t et wo s ip tf h r2 o bm est
Ste Cp
h
r2
o
e:
o g s
eR
e
ne eBg
rl
ae
o
tn
c
ioke
n
r foa rtion
CP roe srf s co o hr vm ile d rM tou t pa rti oo dn u & ce
Qu MDa onNdtNi ez led
Step 4: EvaluatioS ne ale nct ded PBl ooc pk ulation Update
tionallossfunctionslikeKL-Divergence,mean-squared-error(MSE),
andcosinesimilarityasaglobalquantizationobjective(finaloutput) Step 3: Diversity Prompting Selection Eva cclu aha li ilt bde rr aea tnl il o g o ne n dn t ae h tr e aated Obtai cn a ndid f ao tr e each poA o pd f u d ( la tion , a sa (nd . .b ..)e ts ot )
todeterminethebestparameters[2,7,14].InourPTQframework,
traditionallossfunctionstendtooverfittothecalibrationdataand G rae nn de ora mte p m aru elt nip tsle Mutate with chilG dre en ne (rate d ,i verse ....)
lackgeneralizationtothetestset.Furthermore,relyingsolelyonthe
finaloutputforthequantizationsearchprocesscanleadtoprema- Figure2:OverviewofLPQFrameworkillustratingthefour
tureconvergenceorsub-optimalsolutionsasthesearchprogresses, majorstepsandevaluationoffitnessfunction.
ignoringtherepresentationalcollapseofintermediatelayeroutputs
RegimeSize(rs).Thisparameterenablesustocontrolthede-
comparedtotheFPmodel.Contrastivelossfunctions,common
greeoftaperingofthenumbersystem(shapeofdistribution)as
inself-supervisedsettings,havebeenprovenbypriorwork[5]to
highlightedinFig.1(b).Standardpositshaveafixedtaperingforall
combatoverfittingbyregularizingagainstnegativesamplesinthe
precisions.𝑟𝑠isconstrainedtoatmost𝑛−1bitstoallow1bitfor
testset.Weleverageaglobal-localcontrastiveloss,estimatingthe
thesign.Thefractionfield(𝑓)occupiestheremainingbits,ifany.
representationaldivergenceofintermediaterepresentations(inter-
Becausethereisanimpliedvaluebeforetheradixpoint(similar
mediatelayeroutputactivations)inadditiontothefinaloutputto
tothehiddenbitinfloats),theabsenceoffractionbitsdoesnot
identifythebestprecision,preventingtherepresentationalcollapse
representzero.
ofthequantizedmodel.(Section.4.1)
ScaleFactorsf.Thescalefactorisacontinuous-valuedparame-
QuantizationAccelerators.ThereisarecentsurgeinDNN
terthatbiasesthescalingoftherepresentationupwardsordown-
inferenceacceleratorsembracingmixed-precisiontechniquesand
wards. By adding a scale factor bias we can shift the region of
noveldatatypes.BitFusionfeaturesfusiblelow-precisioninteger
maximumaccuracyofposits(taperedregion)tothedesiredregion.
PEswithinasystolic-arrayarchitecture[19].AdaptivFloatintro-
Instandardpositsthereisnoscalefactorbiasandthetaperedregion
ducesadaptivefloating-pointquantizationandhybridfloatPEsto
isalwayscenteredatmagnitude0.
mitigateintegerquantizationerrors,albeitwithsubstantialarea
Finally,inspiredbythearithmeticefficiencyandlowhardware
overheads[20].Improvingonpriorworks[19,20],theANT[7]
cost of LNS [1], we augment the parameterized posit represen-
designemploysa4-bitINTPEwithdecoderstosupportfloatcom-
tationabovetoincludeitsadvantages.Weexpressthestandard
putationsonthesameINTPE.Recenteffortsalsoexplorebenefits
positfraction(1.𝑓)andexponentfieldsinthelogarithmicdomain
ofpositsoverfloatforDNNinference,likethefixed-resolutionposit
as a unified fixed-point exponent of the power of two as 2𝑒+𝑓′,
MACunitandthemixed-precisionpositPEin[10].Despiteposits’
where𝑓′ =𝑙𝑜𝑔2(1.𝑓).𝑓′correspondstothefractionalpartand𝑒
superiorityoverfloats,thehighresourceutilizationinpriorworks
totheintegerpartofthefixed-pointformatwhichwetermUnified
hindersadoptioninresource-constraineddevices.Inspiredby[7]
LogarithmicFractionandExponent(ulfx).
andposit’sadvantages,inthisworkweproposeaLogarithmicPosit
Mathematically,LPcanberepresentedas,
PEdesign,thatexploitsboththehigheraccuracyandadaptability
𝑥⟨𝑛,es,rs,sf⟩=sign(𝑝)×22es×𝑘+𝑒−sf×2ulfx (1)
ofposits(Fig.1(b))andthecomputationalefficiencyofLNS.
4 LPQ:LPQUANTIZATIONFRAMEWORK
WepresentanoverviewofourLPQframeworkinFig.2,whichis
3 LP:LOGARITHMICPOSITS composedoffourstages.
TheproposedLogarithmicPositdatatype(LP)closelyfollowsthe Step1:CandidateInitialization.Aquantizationsolutioncom-
generalschemeofthestandardpositformat[8]whileleveraging prisesanencodedvectorΔoflength4𝑁 andeachsetof4values
thecomputational/hardwareefficiencyofLNS[1].Weparameter- representthe4LPparametersofalayer𝑙:Δ[𝑙] =⟨𝑛 𝑙,𝑒𝑠 𝑙,𝑟𝑠 𝑙,𝑠𝑓 𝑙⟩.
izeseveraladditionalbitfieldsofastandardposit,whichprovides Weconstrainthesearchspaceasfollows,𝑛within [2,8],esand
fine-grainedcontroloverthedynamicrange,position,andshape rs within [0,𝑛−3] and [2,𝑛−1].Followingpreviouswork[6],
ofthedistributionofnumberencodingstobetteremulatethehet- whichhighlightsquantizationsensitivitytosmallscaleperturba-
erogeneousweight/activationdistributionsofDNNencodings.The tions,weextendthistosf.Thesf searchspaceforeachlayer𝑙 is
parametersweincorporateare: auniformballofradius10−3 centeredaroundthemeanweight
Numberofbits (𝑛). Wedynamicallyadjustthenumberof distributionofthatlayer.Prospectivescalefactorsaresampledas
bits to allow mixed-precision quantization of a DNN to enable sf𝑙 = mean(𝑙)+𝜂(−10−3,+10+3),where𝜂 isarandomsampling
ustochoosetheoptimalprecisionforalayerandachievehigher function.LPQinitiatesthepopulationbyrandomlysamplingKcan-
compressionrate. didateΔvectorsconsistingofdifferentquantizationstrategiesper
ExponentSize(es).ModifyingesallowsLPtoadapttodiverse layer.ThefitnessfunctionL𝐹 isevaluatedforeachcandidate(used
dynamicranges.Eachincrementinesdoublesthedynamicrange. toidentifybestcandidatesinlaterstages,explainedinSec.4.1).
esislimitedto𝑛−3bitstoallow1-bitsignandatleast2-bitsregime. WecreateKtuples(Δ 𝑘,L𝑘 𝐹)toformtheinitialpopulation.Fitnessvaluesofinitialcandidatesarepre-computedandstoredtoavoid tensorofintermediaterepresentationsfromeachlayeroftheFPand
repeatedevaluations. quantizedmodelbedenotedasHFP ={HF 0P,HF 1P,...,HF 𝑁P −1}and
didS at te ep in2: tR hee- pg oe pn uer laa tt ioio nn is(C raro ns ks eo dve br asa en dd oM nu tht eat fii to nn e) s. sE fa uc nh ctc ia on n- H ou𝑞 tp= ut{ tH en𝑞 0 s, oH r𝑞 1 a, f. te. r., pH as𝑞 𝑁 si− n1 g} t, hw roh ue gre hH lay𝑙 er re 𝑙p .r Hes oe wn ets vet rh ,e usin inte gr Hm Fe Pd ,i Hat 𝑞e
andthetoptwocandidatesserveasparentsforgeneratingthenext
directlyisimpracticalduetohighdimensionality.Weaddressthis
candidategeneration(child).Whenevolvingcandidates,perturbing
byapplyingrow-wisepoolingusingKurtosis-3[3]insteadofmean
toomanylayerparametersbasedonparentscanleadtoahigh-
pooling.Kurtosis-3bettercharacterizesdistributiontailednessof
dimensionalsearchspace;tomitigatethis,weemployablock-wise
DNNparameters.Thecontrastiveobjectiveforrepresentational
regenerationapproach,evolvingonlyasubset/blockofsizeBof
divergenceestimationisthusformulatedinEquation.6,closely
childparametersbasedonchosenparents,settingallotherstothe
followingthecontrastivelossdefinitionin[5].
bestparent’sparameters.Thechild’sparameterregenerationforthe
specificblockinvolvesadjustmentsaccordingtoparentcandidates LCO=log(1+𝑒−⟨H𝑝𝑞,H𝑝FP +⟩/𝜏∑︁ 𝑒⟨H𝑝𝑞,H𝑝FP −⟩/𝜏 ) (6)
(𝑝1,𝑝2)andisformulatedas:
𝑝−
𝑛 =rand(min(𝑝1.𝑛,𝑝2.𝑛)−1,max(𝑝1.𝑛,𝑝2.𝑛)+1) (2)
child where𝜏controlsconcentrationlevel[5]followingthetypicaldefini-
𝑒𝑠 child=rand(min(p1.es,𝑝2.𝑒𝑠)−1,max(𝑝1.𝑒𝑠,𝑝2.𝑒𝑠)+1) (3)
tionusedincontrastivelossliterature;𝑝,𝑝+,and𝑝−arequantized
𝑟𝑠 =rand(0,ceil(mean(𝑝1.𝑟𝑠,𝑝2.𝑟𝑠))+1) (4) modelpredictiononaparticularimage,thecorrespondingFPmodel
child
𝑠𝑓 =mean(𝑝1.𝑠𝑓,𝑝2.𝑠𝑓)+𝜂(−10−3,103 ) (5) prediction(positiveexample),andFPmodelpredictionsforallother
child calibrationdataimages(negativeexamples).
Weprefermean()forparametersthatinfluencetheshapeofthe
Wefurtherpenalizehigherbit-widthcandidatesusingaloss
distributionofthenumberencodingsandmin()/max()forparam-
etersthataffectthedynamicrange. thatquantizesthecompressionfactorinspiredby[14]as,LCR =
Step3:DiversityPromotingSelection.Insteadofdirectlyadding (cid:205) 𝑙∈𝑁 #PARAM(H 𝑙FP) ×𝑛 𝑙. The complete fitness function LF is
definedasacombinationofthetwocomponentsdefinedabove,
theregeneratedchildbackintothepopulationforuseinthenext
iterationofthesearchprocess,weproposetointroducediversity
balancedbyacoefficient𝜆,LF=LCO·L C𝜆 R.Inourexperiments,
intothepopulationandpreventprematureconvergence.Tothis
weempiricallysettheparameter𝜆=0.4toachievethebestcompro-
misebetweenrepresentationalaccuracyandcompressionfactor.
end,wecreateadditionalrandomparents(empiricallychosento
befiveinthiswork)andusetheregeneratedchildintheprevious
stageastheotherparenttogeneratefivediversechildren. 5 LPA:LP-BASEDDNNACCELERATOR
Step4:EvaluationandPopulationUpdate. Weevaluateall Inthissection,weintroducethemodificationstoasystolicarray
generatedchildreninStep2and3andacquirethefitnessfunction.
architecturetosupportLPandthedesignofanLPPEtonatively
ThechildgeneratedinStep2andthecorrespondingfitnessfunc-
handlemultipleprecisionsandparametersets.
tionvalueisaddedtothepopulation.Wethenrankthediversity
promotingchildreninStep3andselectthebestchildtobeadded
tothepopulationforthenextiteration. 5.1 ArchitectureOverview
Inourblock-wisegeneticalgorithmsearchstrategy,weemploy Fig.3,showsLPAwitharchitecturaloptimizationsforLPsupport
P passesoverthewholeDNN,i.e.overalltheblocksofsizeB, onasystolicarray.Ourdesignoptimizescomputationthroughput
andeachblockisiteratedoverC cyclesineachpass.Therefore, inaweight-stationarydataflow,enablingthemappingofmultiple
thepopulationisupdatedP×Ctimes,i.e.,Steps2,3,and4are low-precisionweightssharinganeastboundinputactivationtoa
executedP×Ctimes. singlePE.
QuantizationforActivation.Afterdeterminingthequanti- WeightandActivationOrganization. Forhardwareefficiency
zationparametersforallDNNweights,weidentifytheLPquanti- andbitpackingduringDNNinferenceonLPA,weconstrainthe
zationvaluesforeachinputactivationinthecorrespondinglayer. LPQsearchspaceof𝑛tointegerpowersof2.Forweights,𝑛ranges
Activationquantizationsensitivitycloselyalignswiththatofthe from 2 to 8, and corresponding activations are 4- or 8-bit. The
weightparametersproducingthem.TheLPparametersofoutputac- weightsandinputactivationsarestoredas8-bitLPintheircorre-
tivationoflayer𝑙are𝑛𝑙
𝑎𝑐𝑡
=min(8,𝑛𝑙 𝑤×2),𝑒𝑠 𝑎𝑙
𝑐𝑡
=min(5,𝑒𝑠𝑙 𝑤×2), spondingbuffers.Intheinputbuffer,the4-bitactivationsarealso
and sf𝑎𝑙 𝑐𝑡 = sf𝑎𝑙 𝑐− 𝑡1 +sf𝑤𝑙. We find that retaining the regime, i.e. storedasan8-bitvaluebyzero-extendingtheLSB.Theinterpreta-
𝑟𝑠𝑙 act=rs𝑤𝑙,achievesbestperformance. t pi ro en cio sf ioth ne ow fte hig eh mtb au pff pe er d’s wb eit igp ha tt ste ar nn dd ie sp ie dn ed ns tio fin edth be yq tu ha ent Miz Oat Dio En
4.1 FitnessFunction (providedbythecontroller).EachPEsupportsthreemodesbasedon
Weintroduceafitnessfunction,L𝐹,toevaluatequantizationstrate- thequantizationprecision:MODE-A(four2-bitweights),MODE-B
gies.Itassessestwokeymetrics:intermediaterepresentationdiver- (two4-bitweights),andMODE-C(one8-bitweight).
genceandcompressionratiorelativetotheFPmodel.Therepresen- LPDecoder. WeinsertLPdecodersbetweenon-chipbuffersand
tationdivergencemetricaimstoalignthedistributionofquantized thePEarray,strategicallyplacingthemonlyalongtheboundary
model’sintermediaterepresentationscloselywiththeFPmodel, tominimizeareaoverhead.Quantizedtensorsarestoredinlow
whilethecompressionratiometricincentivizeslowerbit-widths. precisionbothon-chipandoff-chip,andLPdecodersareemployed
Weformulateacombinedglobal-localcontrastiveobjectivetoad- toconvertthemtoaunifiedformat.Foran8-bitLPweightinthe
dresslimitationsintraditionallossfunctions.Lettheconcatenated WeightBuffer(WB),wedecomposeitintosign(4bits),regimeop7:6op7 op5:4op7op7op5 op3:2op7op3op3 op1:0op7op3op1
Weight Buffer(WB)
rw +: 1 W : Meig Ah Ct, fa r: o mAc ct uiv ra reti no tn P, Er: , MMA OC D eEf sr :o mm p =r e mv. 1 P mE 0,
XOR XOR
1010 m m0
1 XOR
1010 m m0
1 XOR
1010 m m0
1
External
Memory
Input Buffer
(IB)
Unified LP
Decoder
P
PPU
E
EEnifie PP Pd EE
E
LP PP PD EE Eecod PP Pe EE ErM es,O sfD ControllerE
8-bit LP
Input
2's
Complement [MSB-R m 1e ]gim
Zero
<< LZCe
Count
Out
ulfx
constructor
+/m
- <<
u (
R
(1 1l ef 6 6x g-
-
ib b( mi iu t tew s s) ))
(rw) M M MO O OD D DE E E-
-( -ba
i C B
At)
7
op_+ Ln Pg 7 2:6 -m bi1
t
(Am 30 )o Lp P7
4-bi5 t
(B1o )p_ L+ n Pg 5 2:4 -bm it1 (Am 20 )Loo Ppp 57
8-bi3 t
(C0o )p L_+ Png 23: -2 bm it1 (Am 1)0 Lo Pp 3
4-bi1 t
(B0o )p L_+ Png 21 -:0 bm
it
1 (Am 0)0ooo ppp 137
0
Post P Uro nc itessing U On uif ti pe ud t L BP u fE fen rc (o Ode Br )MODE m E Sx it gra nct m `1` sfSign (sw) c_AL 3ZD-4 v_A3 +c_A2LZ vD _- A3 3 v_A2 c_ALZ 1D-2 v_A1 +c_A0LZD v- _1 A1 v_A0
s r ua a a441 rw[15:12] +4 m rw[11:8] 0+4 16-bs m iw
t
4 rr mw[7:4] 0+4 m r 0w[3:0] +4 mrw 16 uw[15:12]ua[3:0] +400ua[3:2] 000ua[3:3] m uw[11:8] 0ua[3:0] +4 B100ua[1:0]
6
i
et-000u ma[2:2]
Ubi
nmu
t
p
w 1
u
au cw[7:4] m06
lk
nua[3:0] + f4 m00ua[3:2] m000ua[1:1] muw[3:0] 0
s
X
sw
mOua[3:0] +
s
R4 a00ua[1:0] 4000u( a[0:0]4 m-bits) M U L S tage (F oa ni )g
P
tau honr ese d
t
c(b 4 oL) P: ne rc v A t: a:
o
vc rao dr ol cu id cn i
e
lt nhv l_
s
eA gi s3 rt ie tZ
n
oc e gt qr0 u o
U
urc ae_ n1 DB n1 io e
tt
if t ze
(
em Pc tt PO i hoxR v
U
e_ reB1
)
pd ( .L a- rp Z
T
tr D
ih
ae e) lc -i ( sPvv s b__ uBA Pi )1 1 mo . Un o0 ic 2 us_0 C ’ t0 s c+ 1 po1 ucc n_ o tB s0 fimv
g
f_ rB p uv1 oO _ l rv CR mO _ e0 eBR 0 m
d
thbe en
as
Pt ee Edr
-
m Log - Linear Converter arraytoeither4-or8-bitLP,calculateactivationscalefactors,and
sr4 er sr sm Two's Comlf pm l= em (1 e+ nf t) A D D performnon-linearoperations(ReLU/Softmax)similarto[8,9].
rr16 rr < lfm[7:6]lfr[7:6]lfm[5:4]lfA r[l 5i :g 4]n
l
fF mr [a 3c :2t ]io lfrn [s 3:2]lfmlf [1r
:0]lfr[1:0]
S tage
5.2 PEArchitecture
+2 m +2 m +2 m +2 Theweight-stationaryPEinthearrayisdouble-bufferedwiththe
lfr16 0
8-bit
lfr+0
1
0
abilitytostoredecodedweightsforthenextcomputation,allowing
Two's Complement amortizationofdecodingandfillingthesystolic-arrayPEsforeach
rr+1er+1sr+1 lfr+1 computation.EachPEreceivesdecodedactivationsfromtheleft
Figure3:LPAArchitecturedepictingdetailedLPPEandUni- andpartialsumsfromthetopwhicharepropagatedtotheright
fiedLPDecoderunits. andbottomrespectively,everycycle.
Multi-precision.Incontrasttoearliermixed-precisionsystolic
arrayarchitectureslike[7,19],whichemploylow-precisionPro-
(16bits),andulfx(16bits),asillustratedinFig.3.Thedecoding cessingElements(PEs)andcombinemultiplePEstosupporthigher
processbeginswithaunified2’scomplementer,highlightedinFig. precisions.Wesuggestastrategyofmappingmultipleweights(de-
4(a)thathandlesmultipleprecisionssimultaneouslycontrolledby pendsonMODE-A/-B/-C)thatsharethesameinputactivationtoa
multiplexers.Theregimeisthendecodedbycountingthenumberof singlePE.Byusinglowerprecisionweightsinalayer,itbecomes
leadingonesorzerosafterthesignbit.Toavoidtheimplementation possibletoassignmoreweightstothesamePE.Thisfacilitates
of both a leading zero and a leading one counter, the binary is theparallelevaluationofmultiplepartialsumsalongthesamePE
invertedaccordingtotheregime’sfirstbit.TheLZC(Fig.4(b)), column,therebyamortizingtheoverheadofmemoryandcontrol,
similartotheunified2’scomplementer,providesthezerocountof andincreasingperformanceperunitarea.
multipleprecisioninputs.Basedonthezerocount,theregimevalue MultiplicationStage. InLP,themultiplicationofweightsand
isshiftedoutfromeachLPusingfoursmallleftshifters.Depending activationsisreplacedbyadditionofulfxandregimes.Asshown
ontheMODE,theshiftedvaluefromoneshiftermaybesenttothe inFig.3,thisisdoneinparallelusing2-setsoffour4-bitadders.
nextshifter. Theactivation’sregimeandulfxtobeaddedarechosenbasedon
Aftershiftingouttheregime,theremainingbitsformtheulfx, theMODE.InMODE-A,eachweighthastobeaddedwitheach
consistingoftheexponentandlog-domainfraction.Theulfxis activation,thereforethecompleteulfxandregimearepassedtothe
interpretedasafixed-pointnumberwithequalbitallocationsto correspondingadderswithnocarrypropagationbetweenadders.
theexponent(integerpart)andlog-domainfraction(fractionpart). Similarly,inMODE-B/-C,theactivation’sulfxandregimebitsare
For example, in MODE-B, the ulfx is interpreted as two fixed- splitintomultiplelowerbit-widthcomponents,zero-extendedand
pointnumbers,eachwith4-bitintegerpartand4-bitfractionpart. passedtoeachoftheweightsindividually.TheMUL-stageresults
Thezerocount(in2’scomplement)isalsousedtocalculatethe ina16-bitregime,16-bitulfxandisguaranteedtonotoverflow.
regimevalue,adjustedforscalefactor,andstoredasa16-bitregime The16-bitulfxissplitinto8-bitexponentand8-bitfraction(lnf)
value.Thehigherprecisiontorepresentulfxandregimeinthe forthenextstage.
unifiedformatischosentopreventoverflowduringcalculations. AccumulationStage. Thisstagereceivesthesplitexponent,lnf
Theactivationdecoderfollowsasimilarprocess,excepttheoutputs andthe16-bitregimefromtheMUL-stagealongwiththeexponent,
area1-bitsign,4-bitregime,and4-bitulfx.Thesesizesfacilitate lf(linear-domainfraction),regimeandsignofthepartialsumfrom
easierroutingtotheaddersintheProcessingElement(PE). thepreviousPEinthesamecolumn.Whilemultiplicationinthelog-
LPEncoder. TheunifiedLPencoder,mirroringdecoderprimi- domainischeap,additionisinefficient.Therefore,weconvertlnfto
tivecomponents,performstheinverseoperation,packingLPcom- lf(fractioninthelineardomaini.e.,1.𝑓).Insteadofimplementing
ponentsintoan8-bitzero-extendedformat.Theencoderisalso anexpensiveLUTbasedconverter,inspiredby[1],weusean8-bit
responsibleforconvertingthelineardomain(lf)fractionsofthe Log-Linearconverterusingasetofgates.Thelogicfunctionfor
partialsumsintothelog-domain(lnf)(explainedlater). theconverterisidentifiedbyusingaKarnaugh-mapsolveronthe
10 10 10 10 10 10 10 10Table1:QuantizationaccuracycomparisonagainstcompetingmethodsonResNet18,ResNet50andMobileNetV2.
ResNet18 ResNet50 MobileNetV2
Method W/A ModelSize(MB) Top-1Accuracy W/A ModelSize(MB) Top-1Accuracy W/A ModelSize(MB) Top-1Accuracy
Baseline 32/32 44.60 71.08 32/32 97.80 77.72 32/32 13.40 72.49
EMQ[4] MP/4 5.50 70.12 MP/5 17.86 76.70 MP/8 1.50 70.75
HAWQ-V3[21] 4/4 5.81 68.45 MP/MP 18.70 75.39 MP/MP 1.68 70.84
AFP[14] - - - MP4.8/MP 13.20 76.09 MP4.8/MP 1.94 70.91
ANT[7] MP/MP 5.87 70.30 MP/MP 14.54 76.70 MP/MP 1.84 70.74
BREC-Q[12] MP/8 5.10 68.88 MP/8 13.15 76.45 MP/8 1.30 68.99
LPQ(Ours) MP4.2/MP5.5 4.10 70.30 MP5.3/MP5.9 14.0 76.98 MP4.1/MP4.98 1.30 71.20
Table 2: Quantization accuracy comparison against SoTA Table3:ComparisonofLPAwithbaselinesunder28nmpro-
methodsonVisionTransformers(ViT-B,DeiT-SandSwin-T).
cesswiththesameon-chipbuffer(512kB(4.2𝑚𝑚2
)).
ViT-B DeiT-S Swin-T ComputeAreaThroughputComputeDensityTotalArea
Method W/A Top-1Accuracy W/A Top-1Accuracy W/A Top-1Accuracy Architecture Component(Area) (𝜇𝑚2) (GOPS) (TOPS/𝑚𝑚2) (𝑚𝑚2)
Baseline 32/32 84.53 32/32 79.80 32/32 81.20 Decoder(5.2𝜇𝑚2)
Evol-Q[6] 4/8 79.50 4/8 77.06 4/8 80.43 LPA Encoder(9.4𝜇𝑚2) 12078.72 203.4 16.84 4.212
FQ-ViT[13] 4/8 78.73 4/8 76.93 4/8 80.73 2/4/8-bitPE(187.43𝜇𝑚2)
LPQ(Ours)MP4.7/MP6.3 80.14 MP3.9/MP5.5 78.01 MP4.5/MP6.2 80.98 ANT 4/8-D bie tc Io nd te Pr E(4 (.9 79𝜇 .5𝑚 72 𝜇) 𝑚2) 5102.28 44.95 8.81 4.205
truthtableconstructedforallpossiblelog-linearconversionsand BitFusion 2/4/8-bitPE 5093.75 44.01 8.64 4.205
allpossiblebit-patterninterpretations.Thelinear-logconverterin AdaptivFloat 8-bitPE 23357.14 63.99 2.74 4.223
theencoderisalsoimplementedinasimilarfashionbutwithan
primitivesandotherconventionalrepresentations.LPQisutilized
inversetruthtable.Afterlfisobtained,itistwo’scomplemented
forquantizationofalldatatypes,withmodifiedsearchparameters
throughaunifiedtwo’scomplementer,andasimplefloating-point
suitedtoeachdatatypeforafaircomparison.Figure5(b)illustrates
fractionalignmentandscale-factorshifterlogicisemployed.The
per-layerquantizationerror,measuredwithRootMeanSquared
alignedfractionsareaddedthroughfour2-bitadderstoobtainthe
Error (RMSE), for various data types on ViT-B. LP consistently
accumulatedlfin2’scomplementformalongwiththejointregime,
exhibitsthelowestaverageRMSE,outperformingallothernumber
8-bitexponent,and4-bitsign.Thefractionisretainedinthelinear
formats.AdaptivFloatfarespoorlycomparedtoLP,primarilydue
domainandnotjuxtaposedwithexponenttopreventredundant
toitslimitedabilitytoadaptonlythedynamicrange,lackingthe
conversions to linear domain since the partial sum output of a
distributionaladjustmentofferedbyLP.
PEisalwaysprogressivelyaccumulated.Thisiswhytheencoder
ComparisonwithState-of-The-Art(SoTA).Ourmixedpre-
employsalinear-logconverter.
cisionquantizationframework,LPQ,iscomparedagainstvarious
competingworks,bothmixed-precisionanduniform.Resultsare
6 EVALUATION tabulatedinTable1forCNNsandTable2forViTs.LPQconsistently
Inthissection,weevaluatethethreecontributionsofthepaper(LP, outperformsothertechniques,demonstrating<1%averagedropin
LPQandLPA)ontheaspectsofquantizationaccuracy,performance, accuracy.Notably,LPQachievesloweraveragebit-widthsforboth
area,throughput,andenergyefficiency. weightsandactivations,resultinginanaveragecompressionof
BenchmarksandDatasets.Ourexperimentsareconducted 7.5×.Theseoutcomescanbeattributedtotwokeyfactors:1)LP’s
on the ImageNet (ILSVRC2012) dataset, evaluating top-1 accu- dynamicadaptationtotheDNNparameterdistribution,allowing
racyacrossvariousCNNs(ResNet18,ResNet50,MobileNetV2)and forlowerbit-widthtolerance,and2)theproposedfitnessfunction
Transformer-basedmodels(ViT-B,DeiT-S,andSwin-T)forcom- components,whichpreventrepresentationcollapse(contrastive
putervisiontasks.TheFPpre-trainedmodelsfrompytorchcvserve objective)whileencouraginglowerbit-width(costfunction).
asthebaselineforourexperiments. ConvergenceBehavior.Tovalidatetheeffectivenessofthepro-
WeimplementLPQinPyTorchandemployacalibrationdataset posedglobal-localcontrastiveloss,wecompareitagainstcommon
comprising128randomlysampledimagesfromtheImageNettrain- globallossfunctions—meansquarederror(MSE),KL-divergence,
ingset.Thealgorithm’ssearchparametersareempiricallydeter- andglobalcontrastiveloss[6].InFig.5(a)weobservethatwith
mined:PopulationSize(K)=20,NumberofPasses(P)=10,Number increasingLPQiterations,MSEandKL-Divergencecurvesplateau,
ofCycles(C)=4,andBlockSize(B)issetto4forCNNsandone indicatingoverfittingtothecalibrationdataset.Conversely,the
attentionblockforTransformer-basedmodels. globalcontrastivelossinitiallymatchestheperformanceofthe
LPA, consisting of LP PEs, decoders, and encoders, is imple- global-localcontrastiveobjective.However,asthenumberofitera-
mentedinVerilogRTLandsynthesizedusingSynopsysDesign tionsincreases,theaccuracy-gapwidensbecause,theglobalcon-
CompilerwithaTSMC28nmprocess.LPAiscomparedagainst trastiveobjectivefailstoaccountfortherepresentationalcollapseof
threestate-of-the-artbaselines(referSection.2),ANT[7],BitFusion intermediaterepresentationsasmorelayersundergoquantization.
[19],andAdaptivFloat[20].Forend-to-endperformanceevaluation
ofLPAandallbaselines,wedevelopacycle-accuratesimulatortool 6.2 EffectivenessofLPA
basedonDnnWeaver[18].DeepScale[17]isemployedtoscaleall Area.WecomparetheacceleratorareabreakdownofLPAwiththe
designstothe28nmprocessforafaircomparison. baselinesinTable3.Allacceleratorshaveidenticalconfigurations,
featuringan8×8weightstationarysystolicarraywithsameon-
6.1 EffectivenessofLPQ chipbufferconfiguration.Thereporteddecoderandencoderarea
NumberFormatComparison.LPQemploysanoveldatatype, representsasingleblockforeachrow/columnofthesystolicarray.
LP,consistingoftwoprimitivedatatypes—LNSandposits.We TheAdaptivFloatarchitecture,notsupportingmixed-precisionand
assesstheimpactofLPonquantizationaccuracycomparedtoits limitedto8-bit[20],exhibitssignificantlylargerareautilization(a) (b)
Figure5:(a)LPQperformancewithvariouslossfunctions,(b) Figure6:NormalizedLatencyandEnergycomparisonofLPA.
RMSEdistributionofquantizationerrorofdifferentformats. 7 CONCLUSION
Thispaperpresentsanalgorithm-hardwareco-designfeaturinga
Table4:Impactonperformance,accuracyandenergyeffi-
novelcompositedatatype,LP,whichcombinespositsandLNS.
ciencywithdifferentPEtypesinLPA.
LP dynamically adapts to diverse DNN parameter distributions
PE-type Density(TOPS/𝑚𝑚2) Top-1Accuracy Efficiency(GOPS/W)
LPA-2/4/8 16.84 76.98 212.17 anddynamicrangesbyconfiguringbitfields.LPQ,anautomated
LPA-8 6.98 77.70 124.26 quantizationframeworkemployinggeneticalgorithmsoptimizes
LPA-2 23.79 0.0 438.96
Posit-2/4/8 3.15 73.65 70.36 LPparametersthroughaglobal-localcontrastiveobjective.Wealso
AdaptivFloat-8 2.74 76.13 71.12 propose LPA that integrates a unified LP PE in a systolic array
architecture.Ourco-designachievesonaverage<1%accuracydrop
duetoitsfloating-pointformat.LPAPEsnativelysupport2/4/8- andsignificantlyimprovesPPAandenergyefficiencycomparedto
bitmixedprecision.Whereas,ANTandBitFusionsupport4-bit SoTAquantizationacceleratorsandframeworks.
and2-bitPEsrespectively,achievingmixed-precisionsupportby
groupingneighboringPEs.DespiteANTandBitFusionexhibiting 8 ACKNOWLEDGEMENTS
lowerareawhencomparedwithLPAforthesamenumberofPEs, ThisworkwassupportedinpartbyCoCoSys,oneofsevencenters
LPAresultsinproportionatelyhigherperformanceperunitarea inJUMP2.0,aSemiconductorResearchCorporation(SRC)program
(TOPS/𝑚𝑚2)formixed-precisionDNNinference.
sponsoredbyDARPA.
PerformancePerUnitArea(TOPS/𝑚𝑚2 ).UsingResNet50
astheworkload,wedetermineper-layerquantizationparameters REFERENCES
forLPAandBitFusionusingLPQ.ForANTandAdaptivFloat,we [1] S.A.Alametal.2021.Low-precisionlogarithmicnumbersystems:beyondbase-2.
adheretotheframeworksintheiroriginalpapers.Weensureall
ACMTACO18,4(2021),1–25.
[2] Y.Caietal.2020.ZeroqAnovelzeroshotquantizationframework.InCVPR.
baselines use quantization parameters that showcase their best [3] LawrenceTDeCarlo.1997.Onthemeaninganduseofkurtosis.Psychological
possibleaccuracyforafaircomparison.InTable3(column5),we methods2,3(1997),292.
[4] P.Dongetal.2023.EMQ:EvolvingTraining-freeProxiesforAutomatedMixed
presenttheperformanceperunitareaofeachdesignduringquan-
PrecisionQuantization.InCVPR.17076–17086.
tizedResNet50inference.LPAachievesnearlya2×improvementin [5] P.Fradkinetal.2022. RobustnesstoAdversarialGradients:AGlimpseInto
performanceperunitareacomparedtoANTandBitFusionforthe theLossLandscapeofContrastivePre-training.InWorkshoponPre-training:
Perspectives,Pitfalls,andPathsForwardatICML2022.
samearchitectureconfiguration.Becausethesearchitecturestend [6] N.Frumkinetal.2023.JumpingthroughLocalMinima:QuantizationintheLoss
tobehaveas8-by-4or8-by-2systolicarraysathigherprecisions LandscapeofVisionTransformers.InCVPR.16978–16988.
[7] C.Guoetal.2022.Ant:Exploitingadaptivenumericaldatatypeforlow-bitdeep
(becauseofPEfusion),LPA’sadvantagebecomespronouncedby
neuralnetworkquantization.InMICRO.IEEE,1414–1433.
stillmaintainingan8-by-8behavior.TomatchLPA’sperformance, [8] J.GustafsonandI.Yonemoto.2017.Beatingfloatingpointatitsowngame:Posit
ANT/BitFusionwouldneedwidersystolicarrays,8×16or8×24 arithmetic.Supercomputingfrontiersandinnovations4,2(2017),71–86.
[9] B.Kelleretal.2023.A95.6-TOPS/WDeepLearningInferenceAcceleratorWith
respectively,offsettingtheirareaadvantage.
Per-VectorScaled4-bitQuantizationin5nm.IJSSC58,4(2023),1129–1141.
PerformanceandEnergyComparisonwithBaselines.We [10] H.Langroudietal.2019.Cheetah:Mixedlow-precisionhardware&software
compareLPAwiththebaselinesonViT-BandResNet50,andreport co-designframeworkforDNNsontheedge.arXiv:1908.02386(2019).
[11] H.F.Langroudietal.2019.Positnnframework:Taperedprecisiondeeplearning
thenormalizedexecutiontimeandenergyinFig.6.LPAexhibits inferencefortheedge.In2019(SCC).IEEE,53–59.
thelowestlatencyacrossmodels,withamodestincreaseinenergy [12] Y.Lietal.2021.Brecq:Pushingthelimitofpost-trainingquantizationbyblock
consumptionoverANTattributedtooverheadsduetonativemixed-
reconstruction.arXiv:2102.05426(2021).
[13] Y.Linetal.2021.Fq-vit:Post-trainingquantizationforfullyquantizedvision
precisionsupportandconversionlogic. transformer.arXiv:2111.13824(2021).
NumberFormatandMixed-PrecisionComparison.Exam- [14] F.Liuetal.2021.Improvingneuralnetworkefficiencyviapost-trainingquanti-
iningtheimpactonperformance,accuracy,andenergyefficiency
zationwithadaptivefloating-point.InCVPR.5281–5290.
[15] R.Murilloetal.2020.DeepPeNSieve:Adeeplearningframeworkbasedonthe
withdifferentPEssupportingsingle-/mixed-precisionforResNet50 positnumbersystem.DSP102(2020),102762.
inTable4,weobservethattheidealscenarioforthebestperfor- [16] A.Ramachandranetal.2022.PositIV:AConfigurablePositProcessorArchitec-
tureforImageandVideoProcessing.In202225thEuromicroDSD.
manceperunitareaandenergyefficiencyoccurswhenalllayersare [17] S.Sarangietal.2021. DeepScaleTool:Atoolfortheaccurateestimationof
quantizedto2-bit(LPA-2),albeitwithpooraccuracy.Conversely, technologyscalinginthedeep-submicronera.InISCAS.IEEE,1–5.
thebestquantizationperformanceisachievedwhenalllayersare
[18] H.Sharmaetal.2016.Fromhigh-leveldeepneuralmodelstoFPGAs.InMICRO.
[19] H.Sharmaetal.2018.Bitfusion:Bit-leveldynamicallycomposablearchitecture
quantizedto8-bit(LPA-8),butwithlowerperformanceperunit foracceleratingdeepneuralnetwork.InISCA.IEEE,764–775.
areaandenergyefficiency.Despiteincorporatingmixed-precision [20] T.Tambeetal.2020.Algorithm-hardwareco-designofadaptivefloating-point
support,LPA-2/4/8achievesaccuracytendingtotheidealscenario
encodingsforresilientdeeplearninginference.InDAC.IEEE,1–6.
[21] Z.Yaoetal.2021.Hawq-v3:Dyadicneuralnetworkquantization.InICML,PMLR.
forbothmetrics,demonstratingabalancedtrade-off.