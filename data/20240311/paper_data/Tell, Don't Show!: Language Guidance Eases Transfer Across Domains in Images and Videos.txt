Tell, Don’t Show!: Language Guidance Eases
Transfer Across Domains in Images and Videos
TarunKalluri1 BodhisattwaPrasadMajumder2 ManmohanChandraker1
1UCSanDiego 2AllenInstituteforAI
https://tarun005.github.io/lagtran/
Abstract
Source Domain Image Classifier Target Domain
We introduce LaGTran, a novel framework that
-17.1%
utilizes readily available or easily acquired text
Large
descriptionstoguiderobusttransferofdiscrim- Domain Gap
inative knowledge from labeled source to unla-
Smaller
beled target data with domain gaps. While un- A photo C ofa p at i ro on o: m with a Domain Gap -9.5% A pictureC oa fp ati o ron o: m with a
supervisedadaptationmethodshavebeenestab- fireplac Te a a gn sd : carpet televisio Tn a a gn s:d a table
#CozyNights, #SofaStyle, #이지리빙,#2008,
lished to address this problem, they show lim- #ModernLiving, #HomeStyle #⼤韓⺠國
Text Classifier
itations in handling challenging domain shifts Label: Living Room Label: ?
duetotheirexclusiveoperationwithinthepixel-
Figure1:AsummaryofourinsightsforLaGTran:Inadomaintransfer
space. Motivatedbyourobservationthatseman- settingwithlabeledsourceandunlabeledtargetdomaindata,weobserve
tically richer text modality has more favorable significantlymoredropincurredwhiletransferringanimage-classifier
transfer properties, we devise a transfer mech- trainedonsourceimagestotarget(17.1%),comparedtoatext-classifier
trainedoncorrespondingtextdescriptionsofsourceimages(9.5%).We
anism to use a source-trained text-classifier to
usethisinsighttobuildasimpleframeworkcalledLaGTranthatleverages
generate predictions on the target text descrip-
thesetextdescriptionseasilyavailableinbothdomainstoimprovetransfer
tions, and utilize these predictions as supervi- inimagesandvideos.
sionforthecorrespondingimages. Ourapproach
drivenbylanguageguidanceissurprisinglyeasy tobridgethedomaingap(Ganin&Lempitsky,2015;Long
andsimple,yetsignificantlyoutperformsallprior etal.,2018;Saitoetal.,2018;Xuetal.,2019;Sharmaetal.,
approachesonchallengingdatasetslikeGeoNet 2021;Weietal.,2021;Chenetal.,2022;Zhuetal.,2023).
andDomainNet,validatingitsextremeeffective- Despitetheirnotedsuccess,theirlimitationsinaddressing
ness. To further extend the scope of our study challengingtransferbeyondregulardomainshifts(Saenko
beyondimages,weintroduceanewbenchmark etal.,2010;Venkateswaraetal.,2017;Pengetal.,2017)
tostudyego-exotransferinvideosandfindthat isrecentlyhighlighted(Prabhuetal.,2022;Kallurietal.,
ourlanguage-aidedapproachLaGTranyieldssig- 2023). We posit that a part of this limitation potentially
nificantgainsinthishighlychallengingandnon- stems from their dependence on pixel-level data alone to
trivial transfer setting. Code, models, and pro- bridgedomaingaps,asaccuratelycharacterizingshiftsand
posed datasets are publicly available at https: devisingbridgingstrategiessolelybasedonimagesbecomes
//tarun005.github.io/lagtran/. challengingbeyondstandarddomainshiftscenarios.
Incontrast,weproposeanalternativeapproachtoeasetrans-
1.Introduction feracrosssuchchallengingshiftsbyinsteadleveragingubiq-
uitouslyavailablelanguageguidanceduringtraining. Our
Despitegreatstridesintheperformanceinseveralapplica- framework,calledLaGTranforLanguageGuidedTransfer
tionsofcomputervisionrecentyears,achievingrobustness AcrossDomains,issurprisinglysimpletoimplement,yet
todistributionshiftsattest-timestillremainsachallenge. showsextremeeffectivenessandcompetenceinhandling
Inparticular,afundamentalneedtoimprovegeneralization transfer across challenging domain shifts in images and
to domains without manual supervision arises due to the videos compared to any image-based adaptation method.
costandscarcityofacquiringlabeledimages. Adominant Ourkeyinsightliesinobservingthattextguidance,which
paradigmtoaddressthislimitationhasbeenunsupervised is readily available in the form of metadata for internet-
domainadaptation(UDA),whichuseslabelsfromarelated sourceddatasetsoreasilygeneratedwithemergingimage
sourcedomainalongwithdistributionalignmenttechniques captioningmodels,requiresnohumanannotationwhileof-
4202
raM
8
]VC.sc[
1v53550.3042:viXraLaGTran:LanguageGuidedTransferAcrossDomains
feringamoresuitableavenueintransferringdiscriminative • AnovelframeworkLaGTranhighlightingthefeasibility
knowledgeevenacrosschallengingdomainshifts. ofincorporatingvariousformsofreadilyavailabletext
supervisioninenhancingtransferacrossdomainshifts
WefurtherillustratethispropertyinFig.1,whereweexam-
(Sec.3.2).
inethetransferabilityofimageandtextclassifierstrained
• AnewdatasetEgo2Exotostudytheproblemofcross-
usingimageortextsupervisionrespectivelybetweenUSA
viewtransferinvideoswithfine-grainedlabelscovering
andAsiadomainsfromtheGeoNetdataset(Kallurietal.,
adiversepoolofactionsandfree-formtextdescriptions
2023). Weobservesignificantlylessdrop(9.5%)whenap-
providinglanguageguidance(Sec.4.5).
plyingatextclassifiertrainedonthesourcetexttotarget
• Demonstration of the competence of LaGTran across
text,comparedto17.1%dropincurredwhentransferringan
avarietyofdomainshifts, withnon-trivialgainsover
imageclassifiertoclassifytargetimages1. Astextoperates
UDA methods on challenging datasets like GeoNet
inasignificantlylower-dimensionalspace,languagemodal-
(+10%),DomainNet(+3%)andtheproposedEgo2Exo
itynaturallytendstohavelesserdomaingapsasopposed
(+10%)datasets(Sec.4).
toimagesorvideos. Furthermore,textdescriptionsoften
containvaluableattributesandidentifiersthatenhancethe
abilitytoaccuratelyrecognizeimagesinastandardclassifi- 2.RelatedWork
cationsetting,suggestingmorefavorabledomainrobustness
Domainrobustnessincomputervision. Ourworkstud-
anddiscriminativepropertiesoflanguagedescriptionscom-
ies the setting where it is necessary to improve accuracy
paredtoimages.
on an unlabeled target domain by leveraging labels from
Inthecurrentwork,weincorporatetheseobservationsto adifferentsourcedomain. Asuiteofmethodshavebeen
improve transfer in a scenario where the source domain proposed to study this problem using unsupervised adap-
hastextdescriptionsaccessiblealongwiththelabels,but tation(Ben-Davidetal.,2006;2010;Ganin&Lempitsky,
thetargetdomainonlyhastextdescriptionscorresponding 2015;Longetal.,2018),wherepriorworksproposevarious
totheimages. Accordingly, wefirsttrainatextclassifier domainalignmentstrategiesincludingMMD-basedalign-
usingthesourcedomainlanguagedescriptionsandlabels ment (Tan et al., 2020; Long et al., 2017; Sun & Saenko,
and transfer this classifier to assign pseudo-labels to the 2016;Kangetal.,2019),adversarialalignment(Bousmalis
target text descriptions, which, from Fig. 1, would yield et al., 2016; Tzeng et al., 2017; Saito et al., 2017; Chen
morerobustpseudo-labelscomparedtothecommonimage- et al., 2019a; Tzeng et al., 2015; Wei et al., 2021), class-
basedtransfer(Liuetal.,2021a;Sunetal.,2022;Kumar specificadaptation(Peietal.,2018;Saitoetal.,2018;Luo
etal.,2020). We,therefore,directlyusethesepseudo-labels etal.,2019;Xieetal.,2018;Kumaretal.,2018;Guetal.,
as supervisionfor theunlabeled target imagesto trainan 2020),clustering(Dengetal.,2019;Parketal.,2020;Li
image classifier jointly with source labels. This simple etal.,2021a;Kalluri&Chandraker,2022)instance-specific
technique,freeofanycomplicatedadaptationmechanisms, adaptation(Sharmaetal.,2021;Kallurietal.,2022;Wang
showsremarkablystrongperformancesurpassingcompeti- et al., 2022), self-training (French et al., 2017; Liu et al.,
tivebaselinesandpriorUDAmethods. 2021a; Sun et al., 2022; Prabhu et al., 2022) and more
recentlytransformers(Xuetal.,2021)orpatch-basedmech-
TofurtherdemonstratethebroadusefulnessofLaGTranbe-
anisms (Zhu et al., 2023). Similar ideas have also been
yondimages, weintroduceandstudyanovelbenchmark
explored in video domain adaptation (Choi et al., 2020),
fortransferlearninginvideoscalledEgo2Exo, whichfo-
withextensionstoincorporatetemporalalignment (Chen
cusesonthepreviouslyunder-exploredchallengeoftrans-
etal.,2019a;Weietal.,2023;Sahooetal.,2021;Dasgupta
ferring action recognition between ego (first-person) and
etal.,2022). However,alltheseudamethodspredominantly
exo(third-person)perspectivesinvideos(Lietal.,2021b;
operate in the pixel-space, and often fall short in bridg-
Ohkawaetal.,2023;Quattrocchietal.,2023;Xue&Grau-
ingmorecomplicatedformsofdomainshiftinchallenging
man,2023)fromatransferlearningstandpoint. Wecurate
transfersettings(Kallurietal.,2023;Prabhuetal.,2022).
Ego2ExobenchmarkusingcookingvideosfromtheEgo-
Alternatively,ourworkintroducesanewparadigmtostudy
Exo4Ddatasetutilizingkeystepannotationstoassignaction
robustness,wherewebuildasimpleframeworkusingrich
labelsandatomicactiondescriptionsfortextualguidance.
textualdescriptionstoovercomelargedomaingapsinimage
Our language-aided transfer shows remarkable utility in
andvideorecognitiontasks.
thischallengingsetting,significantlyoutperformingprior
Video-UDAmethods(Chenetal.,2019b;Weietal.,2023).
Languagesupervisionincomputervision. Therecent
Insummary,ourcontributionsarethree-fold.
proliferation of internet-sourced datasets highlights the
1Theseobservationsholdforotherdomainswithdifferenttext ready availability of natural language supervision with-
sourcesaswell,asshowninthesupplementary. out the need for any labeling or annotation efforts in im-
ages(Thomeeetal.,2016;Changpinyoetal.,2021;Schuh-LaGTran:LanguageGuidedTransferAcrossDomains
mannetal.,2022;Mahajanetal.,2018;Desaietal.,2021) canyon park airport
and videos (Miech et al., 2019; Bain et al., 2021; Grau-
manetal.,2022;2023). Thisavailabilityoflanguagesu-
pervision has been effectively utilized to learn scalable Source r w to i hc t ek h y b c am l co o ku u gn d rt s oa ui i nn n ds A c p o er a oo s pl t ll e ee r r o w ni t ih t G s qr t uo a eu n up d e io nf g p ie no p al e BERT Vision
Classifier
weakly supervised models (Mahajan et al., 2018; Singh
Transfer
etal.,2022),robustvision-languagerepresentations(Rad- A p b h oc o xl t io o ns ge o - f ru ip a n g A b c u rw i oh l si d st i e n o g n w ti ot ph a A o cn f o aa a se tr r ei o ra ll l ev ri ew BERT
fordetal.,2021;Jiaetal.,2021;Phametal.,2023;Desai
Target
&Johnson,2021;Sariyildizetal.,2020;Linetal.,2022; (no labels)
Zhaoetal.,2023;Goyaletal.,2022),text-conditionedgen-
erativemodels(Rombachetal.,2022;Rameshetal.,2021;
Figure2: AnoverviewoftrainingusingLaGTran: Weoperateina
Saharia et al., 2022) and improving sampling techniques settingwherethelabeledsourcedomainandunlabeledtargetdomain
forself-supervisedlearning(ElBananietal.,2023). Even datapossesscheaplyavailableoreasilygeneratedlanguagedescriptions
intheabsenceofassociatedlanguagesupervision,recent foreachimage.LaGTranproceedsbyfirsttrainingaBERT-classifierB
usingsourcecaptionsandlabels(Eq.(1)),andusingthetrainedmodel
innovationsshowedthepotentialofgeneratingcorrelated
togeneratepseudo-labelsyˆtforthetargetcaptionsandcorresponding
descriptionsforimagesusingimage-to-textorimagecap-
images(Eq.(2)). Wethenusethisgeneratedsupervisionalongwith
tioning models (Li et al., 2023; Liu et al., 2023; Achiam sourcedomaindatainjointlytrainingaVisionclassifierGforimage
et al., 2023). Despite this ubiquity and proven effective- orvideoclassification(Eq.(3)).
nessoflanguagesupervisionforvisiontasks,littleattention
has been directed at leveraging their utility in improving getdomainD t :{X ti}N i=t 1,whereX s∼P s,X t∼P t,N sand
transferlearningacrossdomains. Inthiswork,weuselan- N tarethenumberofsamplesinsourceandtargetdomains,
guageguidancetodevelopastraightforwardmechanismto and the covariate shift assumption means marginal distri-
improveimageandvideoclassificationondomainswithout butionsP s ̸=P t(Ben-Davidetal.,2006;2010). However,
manualsupervision. differentfrompriorworks,weadditionallyassumeaccess
tonaturallanguagedescriptions,denotedbyc ,correspond-
i
ingtoeachimageorvideoinputinbothsourceandtarget
Domainrobustnessusinglanguagesupervision. Recent domainsduringtraining. Consequently,wedenotethela-
emergenceoflarge-scalepre-trainedvision-languagefoun- beled source domain with D : {Xi,yi,ci}Ns and the
s s s s i=1
dationalmodelssuchasCLIP(Radfordetal.,2021)enabled unlabeledtargetdomainwithD :{Xi,ci}Nt . Thesetext
t t t i=1
strongzero-shotgeneralizationacrossdiversedomainsand descriptionsarereadilyavailablethroughassociatedmeta-
tasks (Devillers et al., 2021). However, the zeroshot in- datainweb-collectedimages(Mahajanetal.,2018),orcan
ference using frozen pre-trained models still fall short of beeffortlesslygeneratedwithstate-of-the-artimage-to-text
supervised fine-tuning (Radford et al., 2021; Pham et al., models (Li et al., 2023). In Sec. 4, we show robust per-
2023;Andreassenetal.,2021),whichin-turnsuffersfrom formanceusingtextdescriptionsderivedfromavarietyof
poorgeneralizationtodistributionsoutsidethefine-tuning sources,including: imagemetadata(e.g.,alt-text,hashtags)
data (Kumar et al., 2022; Wortsman et al., 2022). Prior forweb-sourcedimages,state-of-the-artimagecaptioners
worksexploredrobustfine-tuningofzero-shotmodels,but formanuallycurateddatasets,aswellasactiondescriptions
donotleveragetargetdomaindata(Udandaraoetal.,2023) ornarrationsinvideos. Notethatoursettingrequireslan-
orlanguagesupervision(Wortsmanetal.,2022;Goyaletal., guagedescriptionsc onlyduringtrainingandnotduring
i
2023)duringfine-tuning. Whilerecentworksincorporate inferenceordeployment,andthereforeincursnospeedor
languageguidancefordomaingeneralization(Dunlapetal., memoryoverheadattest-timewhencomparedwithprior
2023;Wangetal.,2024;Liu&Wang,2023;Huangetal., approaches.
2023;Minetal.,2022),theymostlyrelyondomainorclass
descriptorsanddonotleveragesemanticallyricherfreeform 3.2.LaGTranforCross-DomainTransfer
textsupervisionfromtargetimagesduringtransfer. Incon-
trasttotheseefforts,weshowthatincorporatinglanguage Overview. The training pipeline used in LaGTran for
aidedtransferyieldsaremarkablyeffectiveframeworkfor cross-domaintransferissummarizedinFig.2, wherewe
improvingdomainrobustness. firsttrainaBERTsentenceclassifierusingthe(text,label)
pairsfromthesourcedomaindataset,andutilizethistrained
classifierto inferpredictions onallthe descriptionsfrom
3.MethodDetails
thetargetdomain. Wethenusethesepredictionsaspseudo-
3.1.ProblemDescriptionandBackground labelsforthetargetimages,andtrainajointvisionclassifier
alongwiththelabeledsourcedomainimages.
Weconsiderthesettingofunsupervisedcross-domaintrans-
fer, with access to labeled data from a source domain Trainingthetextclassifier. Weusethesupervisedtext-
D : {Xi,yi}Ns along with unlabeled data from a tar- label pairs from the source domain (cs,ys) and train a
s s s i=1 i iLaGTran:LanguageGuidedTransferAcrossDomains
whereCisthesetofcategoriesintheclassificationtask. Us-
ingthesepredictions,weconstructapseudo-labeledtarget
dataset, givenbyD(cid:99)t = {xt i,yˆ it}N i=t 1. Finally, wecombine
thispseudo-labeledtargetimagesalongwithmanuallyla-
(a) (b) (c) (d)
beled source domain images to train an image classifier
Figure3:tSNEvisualizationofcross-domainfeaturesonGeoNet.We backboneG.
showimproveddomain-alignmentwithbetterclass-separationinsource
and target when extracting features from a text-classifier (Fig. 3c-3d)
comparedtofeaturesfromimage-classifier(Fig.3a-3b)highlightingbetter argmin E L (G(x ;θ),y )+
transferabilitythroughtextmodality.(Sourceinorangeandtargetinblue). CE i i
θ (xi,yi)∼Ds
E L (G(x ;θ),y ) (3)
CE i i
BERT (Devlin et al., 2019) sentence classifier B to pre- (xi,yˆi)∼D(cid:98)t
dictthecategorylabelfromaninputtextdescription,using
thetrainingobjective
Notethattheinferenceisperformedexclusivelyusingthe
ϕ∗ =argminE (ci,yi)∼DsL CE(B(c i;ϕ),y i), (1) trainedimage-basedclassifierG(;θ∗)onimageinputs,and
ϕ
neitherthetextinputsnorthesentence-classifierBisneeded
whereϕdenotestheparametersoftheBERTclassifierand orusedattest-time.
L isthesupervisedcross-entropyloss. Weadoptapre-
CE
trainedDistill-BERT(Sanhetal.,2019)modelfromHug- 3.3.ExtendingLaGTrantoHandleOutliers
gingFaceasthesentenceclassifierB(;ϕ),andfine-tuneit
Owingtothesimplicityinthedesign,LaGTrancaneasily
onthesourcedomaindata. Weobservedsub-optimalperfor-
beextendedtothecasewherethetargetdomainpotentially
manceusingotherpre-trainedbackbonessuchasT5(Raffel
containsoutliersamplesfromoutsidethecategoryset,also
etal.,2020)orGPT-2(Radfordetal.,2019)(Tab.6).Across
called open-world or universal adaptation (UniDA) (You
alldatasetsandexperimentsettingsusedinthispaper,we
et al., 2019; Saito et al., 2020). While classical transfer
feedtherawtextdescriptionsdirectlyintothesentenceclas-
necessitatescompletematchingbetweensourceandtarget
sifiernetworkwithoutanypreprocessingormanualcuration.
category spaces, open-world transfer relaxes this require-
We observed remarkable robustness of the trained classi-
ment,allowingthepossibilityofencounteringimagesfrom
fierinhandlingseveralchallengesposedbyunfilteredtext,
previouslyunseenandoutliercategoriesduringtest-timein
including their variable lengths across images, language
thetargetdomain(Youetal.,2019;Saitoetal.,2020). The
barriersprevalentingeographicallydiversedata,unrelated
taskisthentoaccuratelyclassifyatest-imageintooneofC
tagsanddescriptionscommonlyfoundinweb-sourcedim- s
categoriessharedbetweensourceandtargetdomainswhile
agesorpotentiallyimperfectcaptionsfromstate-of-the-art
simultaenouslydetectingoutlierimagesfromtargetprivate
captioningmodels.
classes. To suit LaGTran for UniDA, we modify Eq. (2)
To further illustrate our motivation to use text classifier toadditionallylabelpredictionsmadebythetext-classifier
forlabeltransfer,weshowthetSNEvisualizationsofthe networkB withanoutlier classusingmaximumsoftmax
featureembeddingsderivedfromasource-trainedsentence probabilitythresholding(Hendrycks&Gimpel,2016)after
classifier, andcomparethemtothefeaturesderivedfrom training.
asource-trainedimageclassifierinFig.3. Evidently, the
featurescomputedusingthetextclassifier(Fig.3cand3d)
(cid:40)
arewell-separated(moreintra-classseparation)andwell- argmax B(ct;ϕ∗) if max B(ct;ϕ∗)>τ
aligned(lessinter-domainseparation)comparedtoimage yˆ it =
|C
|+1Cs i otherwiC ss
e,
i
classifier(Fig.3aand3b)furthervalidatingourhypothesis s
(4)
thatthetextdescriptionsofsame-classimagesfromboth
whereτ isathresholdusedtodetectoutliersamplesduring
withinandacrossdomainslieclosetoeachother.
inference. Wethenproceedtotrainadownstreamclassifier
Cross-modalsupervisiontransfer. Wedistillthepower- on|C| +1classesusingdatafromsupervisedsourceand
s
fuldiscriminativeknowledgelearnedfromtextintoimages psuedo-labeledtargetdatafromsharedclassesaswellas
throughcross-modal(texttoimage)supervisiontransferin theoutlierclass. Duringinference,weassignatest-image
thetargetdomain.Specifically,wefirstfreezetheweightsof tooneoftheC classesorthespecialoutlierclassbasedon
s
thesource-trainedBERTclassifierBandcomputepseudo- theprediction. Weheuristicallychooseτ =0.75anddonot
labelsonallthetargetimagesusingtheircorrespondingtext ablateonthis. WeshowinSec.4.4thatthissimpleexten-
descriptions. Foranimagextwithcaptionct, sionyieldshighestaccuracyonthechallengingGeoUniDA
i i
dataset(Kallurietal.,2023),highlightingtheversatilityof
yˆt =argmaxB(ct;ϕ∗), (2)
i i LaGTrantohandlediversestylesofdomaintransfer.
CLaGTran:LanguageGuidedTransferAcrossDomains
4.Experiments GeoImnet GeoPlaces
Average
U→A A→U U→A A→U
WefirststudytheeffectivenessofLaGTranonstandardim- UnsupervisedAdaptation
agedatasets(Sec.4.1)andextensionstoopenworldtransfer SourceOnly 52.46 51.91 44.90 36.85 46.53
CDAN(Longetal.,2018) 54.48 53.87 42.88 36.21 46.86
(Sec.4.4). Wethenshowourresultsonanewdatasetfor MemSAC(Kallurietal.,2022) 53.02 54.37 42.05 38.33 46.94
transfer between ego-exo views in videos (Sec. 4.5) fol- ToAlign(Weietal.,2021) 55.67 55.92 42.32 38.40 48.08
MDD(Zhangetal.,2019) 51.57 50.73 42.54 39.23 46.02
lowedbyextensiveablationsandinsightsintoourframe-
DALN(Chenetal.,2022) 55.36 55.77 41.06 40.41 48.15
work(Sec.4.6). PMTrans(Zhuetal.,2023) 56.76 57.60 46.18 40.33 50.22
ZeroshotClassification
CLIP†(Radfordetal.,2021) 49.84 53.83 43.41 54.34 50.36
4.1.LaGTranforImageClassification
TextMatch 49.68 54.82 53.06 50.11 51.92
nGramMatch 49.53 51.02 51.70 49.87 50.93
LaGTran 63.82 63.46 58.33 56.87 60.62
Datasets. WeadoptGeoNet(Kallurietal.,2023)andDo-
mainNet(Pengetal.,2019)datasetswhichtogethercover
Table1: LaGTranoutperformsallpriormethodsby>10%onaverage
a range of domain shifts across varying difficulty levels.
withthechallengingGeoImnetbenchmarkwith600classesandGeoPlaces
GeoNet is the largest dataset for domain adaptation with with205classesdesignedforgeographicaltransfer(Sec.4.2).Allmethods
morethan750kimages,proposedtostudyapracticalreal- useaViT-Bbackbone.†denotesdomainaware-prompting.Bestvaluesin
worldproblemofgeographicdisparitiesinimagesfortwo bold,secondbestunderlined.U:USA,A:Asia.
tasks-GeoImnetforimageclassificationfrom600classes
andGeoPlacesforscenerecognitionfrom205classes. Do- scorewiththetextembeddings: yˆ i =argmax |C|(et i·LT),
mainNetisachallengingdatasetproposedforadaptation and (ii) nGramMatch, where we additionally compute
with 400,000 images from 345 classes. Following prior thesetofalln-grams{w}foreachtextdescriptionc i for
work(Weietal.,2021;Kallurietal.,2022),weshowour n={1,2,3,4}andfindtheembeddingsforeachofthese
results on all 12 transfer settings from the 4 most stud- ngramsseparately: W∈R|w|×d. Thepseudo-labelisthen
ied domains real, clipart, sketch and painting. We use a assignedtothelabelwiththehighestsimilarityscorewith
ViT-base(Dosovitskiyetal.,2020)backboneastheimage thebestmatchingngram: yˆ i =argmax |C|max w(W·LT).
encoderontheGeoNet,andfollowpriorwork(Zhuetal., Once the psuedo-labels are generated, we proceed with
2023)anduseSwin-basebackbone(Liuetal.,2021b)for trainingajointmodelusingEq.(3)asbefore.
experimentsonDomainNet. Completetrainingdetailsare
Inadditiontothese,wealsocomparethezero-shotclassi-
includedinthesupplementary(Supp.C).
ficationobtained usingCLIP (Radfordet al.,2021)with
ViT-base backbone. We adopt domain-aware prompting
Sourceoftextsupervision. ForGeoNet,weusetextsu-
following prior work (Dunlap et al., 2023; Liu & Wang,
pervisionfromthemetadatapubliclyreleasedalongwith
2023),whereweincorporatethedomaininformationinto
thedataset,andconcatenatethetags,alt-textandfree-form
theprompt-text(eg: A sketch of a <class>insteadofA
captionsprovidedforeachimagetocreatethetextdescrip-
photo of a <class>toclassifysketchimages).
tions. FortheDomainNetdataset,sincenoassociatedtext
descriptionsareprovided,weuseaBLIP-2(Lietal.,2023)
4.2.LaGTranOutperformsPriorWorksonGeoNet
modeltogenerateshortcaptionsforeachimagefromall
thedomains. Notethatourmethodonlyrequirestextduring WepresentresultsforGeoPlacesandGeoImnetbenchmarks
training,andinferenceisdonesolelybasedonimages. inTab.1. Asnotedin(Kallurietal.,2023),previousUDA
methodsoftenfallshortofbridginggeographicdisparities,
Baselines. Apossibleargumentfortheeffectivenessof highlightingthechallengeofgeographicaltransferwithim-
textsupervisionmightbethedirectpresenceoflabelinfor- agedataalone. Notably,LaGTranachieves60.62%average
mationinthetextdescription,eliminatingtheneedforany Top-1 accuracy on all transfer tasks, beating all previous
manualsupervisionatall. Tostudythisingreaterdetail,we UDAmethodsandstrongbaselinesbysignificantmargins,
devisetwostrongbaselinestoderivepsuedo-labelsdirectly providing solid validation to our transfer approach using
usingthetextdescriptionsinthetargetwithoutusingany languageguidance. Specifically,LaGTranoutperformsthe
sourcedomaindataasfollows. Wefirstuseapre-trained source-onlybaselineby∼14%andbestadaptationapproach
Sentence-BERT(Reimers&Gurevych,2019)encoder,and PMTrans(Zhuetal.,2023)by∼10%ontheaverageaccu-
computethelabelembeddingsofallthecategorynamesas racy,highlightingthenaturalbenefitconferredbytraining
L ∈ R|C|×d, where d is the embedding dimension of the whileleveragingtextsupervisioninsourceandtargetdo-
sentence encoder, followed by zero-shot inference using: mains. LaGTran even surpasses zeroshot accuracy using
(i)TextMatch,wherewecomputetheembeddingofeach domain-aware prompting on CLIP (Radford et al., 2021)
text description et ∈ R1×d from the target domain, and by∼10%,whilebeingtrainedonorderofmagnitudefewer
i
assignpsuedo-labeltothelabelwiththehighestsimilarity datacomparedtoCLIP’shundredsofmillionsofimage-textLaGTran:LanguageGuidedTransferAcrossDomains
Source Real→ Clipart→ Sketch→ Painting→
Target C S P R S P R C P R C S Avg.
UnsupervisedAdaptation
SourceOnly 63.02 49.47 60.48 70.52 56.09 52.53 70.42 65.91 54.47 73.34 60.09 48.25 60.38
MCD(2018) 39.40 25.20 41.20 44.60 31.20 25.50 34.50 37.30 27.20 48.10 31.10 22.80 34.01
MDD(Zhangetal.,2019) 52.80 41.20 47.80 52.50 42.10 40.70 54.20 54.30 43.10 51.20 43.70 41.70 47.11
CGDM(Duetal.,2021) 49.40 38.20 47.20 53.50 36.90 35.30 55.60 50.10 43.70 59.40 37.70 33.50 45.04
SCDA(Lietal.,2021a) 54.00 42.50 51.90 55.00 44.10 39.30 53.20 55.60 44.70 56.20 44.10 42.00 48.55
SSRT-B(Sunetal.,2022) 69.90 58.90 66.00 75.80 59.80 60.20 73.20 70.60 62.20 71.40 61.70 55.20 65.41
MemSAC(Kallurietal.,2022) 63.49 42.14 60.32 72.33 54.92 46.14 73.46 68.04 52.75 74.42 57.79 43.57 59.11
CDTrans(Xuetal.,2021) 66.20 52.90 61.50 72.60 58.10 57.20 72.50 69.00 59.00 72.10 62.90 53.90 63.16
PMTrans(Zhuetal.,2023) 74.10 61.10 70.00 79.30 63.70 62.70 77.50 73.80 62.60 79.80 69.70 61.20 69.63
Zero-shotClassification
CLIP†(Radfordetal.,2021) 72.39 60.90 66.81 81.37 60.90 66.81 81.37 72.39 66.81 81.37 72.39 60.90 70.38
TextMatch 71.36 64.30 65.32 81.25 65.65 64.85 81.09 72.65 63.94 81.08 70.84 64.17 70.14
nGramMatch 68.92 59.82 63.15 76.35 61.72 62.87 76.35 69.28 62.51 76.04 68.52 60.52 67.17
LaGTran 77.30 68.25 67.35 81.31 67.03 66.81 80.78 75.62 68.08 79.23 73.80 63.44 72.41
Table2:LaGTransetsnewstate-of-the-artonDomainNet-345dataset,outperformingpriormethodsandbaselinesinmosttasks.AllmodelsuseSwin-B
backbone,andUDAnumbersaretakenfrom(Zhuetal.,2023).†denotesdomainaware-prompting.Bestvaluesinbold,secondbestunderlined.
R:Real,C:Clipart,S:Sketch,P:Painting.
Method ClosedSetAcc. OpenSetAcc. H-score 4.4.LaGTranImprovesTransferwithOutliers
SourceOnlyw/MSP 38.00 73.90 50.20
Weshowourresultsonopen-worldtransfersettingusingthe
UniDA(Youetal.,2019) 27.64 43.93 33.93
GeoUniDAdataset(Kallurietal.,2023),whichexamines
DANCE(Saitoetal.,2020) 38.54 78.73 51.75
OVANet(Saito&Saenko,2021) 36.54 66.89 47.26 unsupervised transfer across geographies in the presence
LaGTran 52.98 72.35 61.16 ofgeographicallyuniqueclassesinbothsourceandtarget
alongwithcommonclasses. Specifically,GeoUniDAcon-
Table3: Resultsonopen-worldtransferonGeoUniDAshowsstrong tains 62 shared classes between source and target, along
performanceofLaGTranevenwithtargetoutlierclasses,achievingthe
with 138 private categories in each domain. We follow
highestH-score.Baselinenumberstakesfrom(Kallurietal.,2023).
OVANet(Saito&Saenko,2021)toadopttheH-scoreeval-
uationmetric,whichweighsequalimportancetoclosed-set
pairs. Remarkably,wealsooutperformthestrongestbase-
andopen-setaccuraciesbymeasuringtheharmonicmeanof
lineTextMatchby∼8%,underliningthefactthatincases
both. Inadditiontostandardworksthataddressoutlierde-
whenthetextdescriptionsmightnotalwayshaveembed-
tectionthroughuniversaladaptation(Youetal.,2019;Saito
dedlabelinformationdirectly,usinglabelsfromasource
&Saenko,2021;Saitoetal.,2020),wealsotrainabaseline
domainstillhassignificantadvantage.
modelusingonlythesourcedomaindataperformingtest-
timeoutlierdetectionusingMSPthresholding(Hendrycks
4.3.LaGTranachievesnewSOTAonDomainNet &Gimpel,2016). AsshowninTab.3,LaGTranachieves
aH-scoreof61.16%,significantlysurpassingthebaseline
WesummarizetheresultsonDomainNetinTab.2,where
source-onlyaccuracyaswellasallprioruniversaladaptation
LaGTranyieldslargegainsoverseveralpriorUDAmethods
approachesby>10%,indicatingthatlanguageguidancenat-
andallthecompetitivebaselines,settingnewstate-of-the-art
urallyprovidesastrongsignaltodetecttargetsampleswhile
onthischallengingdataset. Notably,manypriormethods
handlingoutliersinopen-settargetdomaindata.
return lesser numbers than directly training on a source
model(Saitoetal.,2018;Zhangetal.,2019;Duetal.,2021;
4.5.LaGTranforVideoDomainAdaptation
Lietal.,2021a),indicatingtheirpoorscalabilitytonatural
domainshiftsinlarge-scaledata. Whilemorerecentinno-
vationsinUDAsuchasself-training(Sunetal.,2022)and
patch-basedmixing(Zhuetal.,2023),aswellaszeroshot Ego2Exo dataset. Despite rapid advances in meth-
inferenceusingCLIPofferimprovedperformance,LaGTran ods(Chenetal.,2019b;Munro&Damen,2020;Choietal.,
stilloutperformsthesemethodsonmosttasks. Finally,our 2020;Weietal.,2023)andbenchmarks(Munro&Damen,
superior accuracy compared to both baselines TextMatch 2020; Plizzari et al., 2023) for video domain adaptation,
andnGramMatch,thatemploytarget-onlypseudo-labeling, little insight is available into their ability to address chal-
underscoresthesignificanceofhavingaccesstosupervised lengingsettingssuchastransferbetweenego(first-person)
text data and labels from a source domain for enhanced andexo(third-person)perspectivesinvideos. Whileprior
targetaccuracy. effortsstudyingego-exotransferrequirepairedvideosfromLaGTran:LanguageGuidedTransferAcrossDomains
2000 2000 Ego→Exo Exo→Ego Avg.
1200 1200 UnsupervisedAdaptation
SourceOnly 17.06 29.36 23.21
400 400 CDAN(Longetal.,2018) 14.08 17.03 15.56
0 10 20 30 40 50 12345678910111213141516171819202122 UAR(Choietal.,2020) 14.65 18.75 16.70
Segment Duration (sec) Action Label Ids
TA3N(Chenetal.,2019b) 17.25 19.54 18.40
(a)SegmentDuration (b)LabelDistribution
TransVAE(Weietal.,2023) 11.31 16.01 13.66
Figure4: DatasetStatisticsforEgo2Exo: Fig.4aShowsthedistribu- Zero-shotVideoRecognition
tionofsegmentdurationsofactionvideosfromEgo2Exowhichrange EgoVLP(Linetal.,2022) 2.25 14.78 8.51
LaVILA(Zhaoetal.,2023) 8.32 19.99 14.15
from0.4sec-1min.Fig.4bshowsthelong-tailofcategorydistributionin
Ego2Exoindicatingthechallengeinrobustclassificationandtransfer. TextMatch 13.98 15.92 14.95
nGramMatch 14.96 21.51 18.24
LaGTran 22.75 43.53 33.14
bothviews(Quattrocchietal.,2023;Sigurdssonetal.,2018) TargetSup. 28.91 49.88 39.40
or do not leverage unlabeled data in the target (Li et al.,
2021b;Ohkawaetal.,2023;Xue&Grauman,2023),lim- Table4: ResultsonEgo2ExobenchmarkLaGTranachievesthehighest
itedworkslookedintotheaspectofunsuperviseddomain accuracycomparedtopriorvideoUDAmethodsaswellaszeroshotvideo-
textpre-trainedmodels. Bestvaluesinbold,secondbestunderlined. All
transferfromegotoexoviewsduetothelackofasuitable
methodsusepre-extractedomnivore-basefeatures,EgoVLPandLaVILAuse
benchmark.
Timesformer-basebackbone.
Therefore,weintroduceanewbenchmarkcalledEgo2Exo
tostudytransferbetweentheegoandexoviewsinvideos.
We curate our dataset using the recently proposed Ego- priorUDAapproaches(Chenetal.,2019a;Weietal.,2023)
Exo4D(Graumanetal.,2023), utilizingtheirkeystepan- aswellasVideo-CLIPbasedmethodswithdomain-aware
notationsforactionlabels,andatomicdescriptionsastext prompting(Linetal.,2022;Zhaoetal.,2023).
supervision. Wemanuallyremapthelabelstoacoarserhier-
archytoeasethedifficulttaskofpredictingveryfine-grained
actionclassesfromshortsegments(eg:add coffee beans
vs. add coffee grounds). Thecompletedetailsaboutour LaGTranefficientlyhandlescross-viewtransfer. Firstly,
datasetcreationprocessareincludedinthesupplementary, wehighlighttheimportanceofstudyingrobustnessacross
Supp.B. ego and exo views in Tab. 4 by examining the ego-test
accuracyofamodeltraineddirectlyonegovideos,which
OurproposedEgo2Exoconsistsof10,102videosegments
achieves49.88%,comparedtoamodeltransferredfromexo-
labeledwithactionsfromoneofthe22keysteps,andwe
videos, which only achieves 29.36%. Similarly, a model
splitthesevideosegmentsintotwoequalgroupsclasswise,
trainedonegovideosachieveonly17.06%forrecognition
andcollectego-videosfromonegroupandexo-videosfrom
in exo view, compared to a potential 28.91% achievable
the other to create our adaptation benchmark. We finally
bytrainingdirectlyonexovideos,indicatingasignificant
obtain5247ego-videosand4855exo-videoscapturingmu-
domainshift.Currentstate-of-the-artvideoadaptationmeth-
tuallyexclusiveactionsandscenes. Theatomicactionde-
ods (Wei et al., 2023) yield limited gains to bridge these
scriptions from all the timestamps within each segment,
gaps,highlightingtheneedfornovelapproachestoaddress
wheneveravailable,formthetextsupervisionforthatseg-
thischallenge. Moreover,zeroshotvideoclassificationac-
ment. Thesameprocedureappliedtothevalidationvideos
curacyusingEgoVLP(Linetal.,2022)andLaVILA(Zhao
yields 3147 segments with both ego and exo views. The
et al., 2023) also show limited gains. Notably, LaGTran
distributionofthedurationofsegmentsinthebenchmark,
which efficiently leverages action descriptions available
alongwiththelabeldistributionforegoandexodomainsis
alongsidethevideos,achievesanaccuracyof33.14%onav-
presentedinFig.4. Thevideos,labelsalongwiththetext
eragesignificantlyoutperformingthesource-onlybaseline
descriptionsarepubliclyavailableonourprojectpage.
by10%andprioradaptationmethodsby>15%. LaGTran
also outperforms pseudo-labeling using nGramMatch or
Training details. We use the pre-computed Omnivore- TextMatch, as the text descriptions, independently devel-
base(Girdharetal.,2022)featuresprovidedalongwiththe opedfromkeysteplabels,oftenlackutilityfordeciphering
EgoExo4Ddatasetfortrainingandevaluation,andfollow theactioncategorylabelsontheirown. Wehowevernote
thesamestrategyfortrainingalltheotherbaselinesaswell thesubstantialscopeforfurtherimprovementinfuture,both
asprioradaptationmethodsforfaircomparison. Weusethe intermsofthelowwithin-domainaccuracyaswellasthe
top-1accuracyonthevalidationsetforevaluation. More remaininggaptosupervisedtargetaccuracy.
details on the training procedure are provided in the sup-
plementary,Supp.C.WecompareLaGTranforvideowith
4.6.AnalysisandAblationsLaGTran:LanguageGuidedTransferAcrossDomains
73 Model params(M) GeoImnet GeoPlaces DomainNet
60
T5-small(Raffeletal.,2020) 60.87 73.93 63.61 68.57
55 O PMur Ts rans 68 O PMur Ts rans C GL PI TP -2-t (( RR aa dd ff oo rr dd ee tt aa ll .. ,, 22 00 12 91 )) 6 13 2.1 46 7 79 7. .8 87 8 6 66 6. .4 65 5 7 61 9. .1 65 0
50 Source 63 Source DistilBERT(Devlinetal.,2019) 67.1 83.53 69.31 71.43
Table6:Comparisonoftext-classifierbackbonesusingtext-classification
45 10 20 30 50 75 100 58 10 20 30 50 75 100 accuracyonGeoNetandDomainNetdatasets.BERTbackboneoutperforms
Text Supervision (%) Text Supervision (%) othertext-pretrainedbackbonesandvision-languagepre-trainedCLIP-t.
(a)AccuracyonGeoNet (b)AccuracyonDomainNet
Figure5: Impactoftheamountoftextsupervisiononthetargetac-
curacy.LaGTranoutperformsstrongUDAmethodswhilerequiringtext
supervisionfromonly20%ofsamplesinGeoNetand50%inDomainNet, Caption:Veterans Caption:Hiroshima Caption:Okinawa
withpotentialforfurtherenhancementwithincreasedtextdata. Memorial Park Peace Memorial Park 1 Peace Memorial Park Label: castle Label: airframe
Label:cemetry Label:cemetry Label:cemetry
DomainNet(Real→) Ego2Exo
Cl Sk Pa Avg. Eg→Ex Ex→Eg Avg.
OnlyTarget 76.03 67.16 65.01 69.35 21.98 42.64 32.31
Caption:Cindrella Caption:Carriage in Caption:Double
JointTrain 77.30 68.25 67.35 70.97 22.75 43.53 33.14 Carriage Tokyo Deck coach A 2008 Label: kabab_shop Label: chime
Label:passenger_car Label:passenger_car Label:passenger_car
Table5: ImportanceofJointTrainingUsingsourcedomaindataim-
provesperformanceontargetimageswhileallowingdeployingjointmod-
elsacrossmultipledomains.
Caption:Arlington- Caption:The Gardens Caption:Orchard
Westov Ler a bS eh lo :p pp lain zg a Center S Lho ap bp eli :ng p lM aza all Road L aS bh eo lp :p pi ln ag z aMall Label: bus_stop Label: bazaar
How much text supervision is needed for LaGTran?
Since natural language supervision is fundamental to
LaGTran, we analyze the impact of the amount of super-
Caption:St Philip Caption:Pomegranates in Caption:Grewia
vision available on the eventual target accuracy. We re- Episcopal, San Antonio Dushanbe Green Market umbelliferaBedd. Label: chime Label: assembly_hall
Label:church Label:farmers_market Label:umbel
train LaGTran by assuming text supervision from only Source Image Neare Ts et xN te Fi eg ah tb uo rers s using Neare Imst a N ge ei g Feh ab to ur rs e u ssing
µ% of images in both source and target domains, where
µ={10,20,30,50,75,100}%,andsimplydiscardthetar- Figure6:Visualizationofnearestneighborsoftheleftmostsource
image,usingtext-trainedandimage-trainedfeatures,alongwithground
getimagesthatdonothavecorrespondingtextualsupervi-
truthlabelsforeachimagefromGeoNet. Weobservebetter“same-
sion. AsshowninFig.5,LaGTranoutperformsimage-only
class”retrievalsusingtext-captionsduetoreduceddomaingap,as
method PMTrans (Zhu et al., 2023) even with just 20% opposedtoimages.
textsupervisioninGeoNet(Fig.5a)and50%inDomain-
Net(Fig.5b),indicatingitshighdataefficiency. Notably, Wereferreaderstotherespectivepapersfordetailsontheir
thegraphremainsunsaturated,suggestingthepotentialfor architecturesandpre-trainingdatasets. FromTab.6, Dis-
furtherimprovementthroughthecollectionofmorecheaply tilBERTyieldsbesttext-classificationaccuracyonallour
availabletextsupervisioninthetargetdomain. threebenchmarks,outperformingtext-onlymodelslikeT5
andGPT2. Despitelarge-scalevision-languagepre-training,
Importance of source domain images. While the ma- CLIP-Tdidnotyieldsubstantialbenefits.
jority of our accuracy gains stem from the text guidance,
thesourcedomainimagesprovidingnoise-freesupervision Nearestneighborsusingimageandtextfeatures. We
are also important in learning strong models on the tar- show the top-2 nearest neighbor retrievals using text-
getdomain. FromTab.5, jointtrainingusingsourceand features computed from source-trained text-classifier as
pseudo-labeled target yields improvements of 1.57% for opposed to image-features in Fig. 6. We observe more
DomainNetand0.8%onEgo2Exobenchmarkscompared robustretrievalsbasedontext-featurescorrespondingtothe
totarget-onlytraining. Moreimportantly,trainingjointly captionsoftheimages,ratherthantheimagesdirectlysig-
onsourceandtargetallowsdeployingasinglejointmodel nifyingthereduceddomaingapinthetextspace. Wealso
acrossbothdomainsasopposedtodomainspecificmodels, noteafailurecaseinthelastrowofFig.6,whereneither
greatlyoptimizinginferencecosts. thetextfeaturesnortheimagefeaturescouldretrievethe
imagefromthecorrectclasschurch.
Effectoftextclassifierbackbone. Wecomparedifferent 5.ConclusionandFutureDirections
choicesoftextclassifierssuchasDistilBERT(Sanhetal.,
2019),T5-Small(Raffeletal.,2020),GPT2(Radfordetal., We introduce a novel framework called LaGTran to use
2019)aswellastextbranchofCLIP(Radfordetal.,2021) readilyavailabletextsupervisionandenhancetargetperfor-
(CLIP-T)usingtext-classificationaccuracyonourdatasets. manceinunsuperviseddomaintransferscenarios. Wefirst
ycaruccA
.gvA
ycaruccA
.gvALaGTran:LanguageGuidedTransferAcrossDomains
startwiththeobservationthattraditionaldomainalignment inneuralinformationprocessingsystems, 19:137–144,
approachesyieldlimitedbenefitsbeyondwell-understood 2006. 2,3
domainshifts,followedbyinsightsthatlanguageprovidesa
Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A.,
semanticallyrichermediumoftransferwithreduceddomain
Pereira, F., and Vaughan, J. W. A theory of learning
gaps. Thisleadstoalanguage-guidedtransfermechanism
fromdifferentdomains. Machinelearning,79:151–175,
where we train a text classifier on language descriptions
2010. 2,3
from a source domain and then use its predictions on de-
scriptionsfromadifferenttargetdomainassupervisionfor Bousmalis,K.,Trigeorgis,G.,Silberman,N.,Krishnan,D.,
thecorrespondingimages. Despitebeingconceptuallysim- andErhan,D. Domainseparationnetworks. InAdvances
ple and straightforward, we show the remarkable ability inneuralinformationprocessingsystems,pp.343–351,
ofourmethodtooutperformcompetitivepriorapproaches 2016. 2
on challenging benchmarks like GeoNet and DomainNet
Changpinyo,S.,Sharma,P.,Ding,N.,andSoricut,R. Con-
for images and proposed Ego2Exo for videos. Through
ceptual12m: Pushingweb-scaleimage-textpre-training
anemphasisoncost-effectiveoreasilyproducibletextsu-
to recognize long-tail visual concepts. In Proceedings
pervision,weopennewpossibilitiesforadvancingdomain
of the IEEE/CVF Conference on Computer Vision and
transferinscenarioswithlimitedmanualsupervision.While
PatternRecognition,pp.3558–3568,2021. 2
LaGTranalreadyyieldsnon-trivialboostsinperformance,
weidentifypotentialareasoffurtherimprovementthrough Chen,C.,Xie,W.,Huang,W.,Rong,Y.,Ding,X.,Huang,
additionallybridgingthedomaingapsintextspaceusing Y.,Xu,T.,andHuang,J. Progressivefeaturealignment
languageadaptationtechniques(Hungetal.,2023)orde- forunsuperviseddomainadaptation. InProceedingsof
visingmechanismstoincorporatebothimageandlanguage the IEEE Conference on Computer Vision and Pattern
guidancethatoffercomplimentarybenefitsduringtransfer. Recognition,pp.627–636,2019a. 2,7
Chen,L.,Chen,H.,Wei,Z.,Jin,X.,Tan,X.,Jin,Y.,and
6.BroaderImpactStatement
Chen, E. Reusing the task-specific classifier as a dis-
criminator: Discriminator-freeadversarialdomainadap-
Ourpaperpresentsanapproachthatcanimproveaccuracy
tation. InProceedingsoftheIEEE/CVFConferenceon
ondomainsfacinglabelscarcity. Advancingthisresearch
Computer Vision and Pattern Recognition (CVPR), pp.
areawouldenhancewideradoptionofcurrentAItechnolo-
7181–7190,June2022. 1,5
gies, and unlocks new capabilities in democratizing the
progressinAI.Giventhatourproposedmethodologyonly Chen, M.-H., Kira, Z., AlRegib, G., Yoo, J., Chen, R.,
operatesinthestandardrealmofimageclassificationand and Zheng, J. Temporal attentive alignment for large-
ourshowcasedresultsonlyusealreadypubliclyavailable scale video domain adaptation. In Proceedings of the
datasets, we do not foresee any negative societal conse- IEEE/CVFInternationalConferenceonComputerVision,
quencesspecificallyarisingduetoourmethod. pp.6321–6330,2019b. 2,6,7,15
Choi,J.,Sharma,G.,Chandraker,M.,andHuang,J.-B. Un-
References
supervised and semi-supervised domain adaptation for
Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I., action recognition from drones. In Proceedings of the
Aleman,F.L.,Almeida,D.,Altenschmidt,J.,Altman,S., IEEE/CVFWinterConferenceonApplicationsofCom-
Anadkat,S.,etal. Gpt-4technicalreport. arXivpreprint puterVision,pp.1717–1726,2020. 2,6,7
arXiv:2303.08774,2023. 3
Dasgupta, A., Jawahar, C., and Alahari, K. Overcoming
label noise for source-free unsupervised video domain
Andreassen,A.,Bahri,Y.,Neyshabur,B.,andRoelofs,R. adaptation. InProceedingsoftheThirteenthIndianCon-
Theevolutionofout-of-distributionrobustnessthrough- ference on Computer Vision, Graphics and Image Pro-
outfine-tuning. arXivpreprintarXiv:2106.15831,2021. cessing,pp.1–9,2022. 2
3
Deng, Z., Luo, Y., and Zhu, J. Cluster alignment with
a teacher for unsupervised domain adaptation. In Pro-
Bain,M.,Nagrani,A.,Varol,G.,andZisserman,A. Frozen
ceedingsoftheIEEE/CVFinternationalconferenceon
intime: Ajointvideoandimageencoderforend-to-end
computervision,pp.9944–9953,2019. 2
retrieval. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pp.1728–1738,2021. 3 Desai,K.andJohnson,J. Virtex: Learningvisualrepresen-
tationsfromtextualannotations. InProceedingsofthe
Ben-David,S.,Blitzer,J.,Crammer,K.,andPereira,F.Anal- IEEE/CVF conference on computer vision and pattern
ysisofrepresentationsfordomainadaptation. Advances recognition,pp.11162–11173,2021. 3LaGTran:LanguageGuidedTransferAcrossDomains
Desai,K.,Kaul,G.,Aysola,Z.,andJohnson,J. Redcaps: Goyal,S.,Kumar,A.,Garg,S.,Kolter,Z.,andRaghunathan,
Web-curatedimage-textdatacreatedbythepeople,for A. Finetune like you pretrain: Improved finetuning of
thepeople. arXivpreprintarXiv:2111.11431,2021. 3 zero-shotvisionmodels.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,
Devillers, B., Choksi, B., Bielawski, R., and VanRullen,
pp.19338–19347,2023. 3
R. Doeslanguagehelpgeneralizationinvisionmodels?
arXivpreprintarXiv:2104.08313,2021. 3 Grauman,K.,Westbury,A.,Byrne,E.,Chavis,Z.,Furnari,
A.,Girdhar,R.,Hamburger,J.,Jiang,H.,Liu,M.,Liu,
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K. Bert:
X., et al. Ego4d: Around the world in 3,000 hours of
Pre-training of deep bidirectional transformers for lan-
egocentricvideo. InProceedingsoftheIEEE/CVFCon-
guage understanding. North American Chapter of the
ferenceonComputerVisionandPatternRecognition,pp.
Association for Computational Linguistics, 2019. doi:
18995–19012,2022. 3
10.18653/v1/N19-1423. 4,8
Grauman,K.,Westbury,A.,Torresani,L.,Kitani,K.,Malik,
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
J., Afouras, T., Ashutosh, K., Baiyya, V., Bansal, S.,
D.,Zhai,X.,Unterthiner,T.,Dehghani,M.,Minderer,M.,
Boote,B.,etal.Ego-exo4d:Understandingskilledhuman
Heigold,G.,Gelly,S.,etal. Animageisworth16x16
activityfromfirst-andthird-personperspectives. arXiv
words:Transformersforimagerecognitionatscale.arXiv
preprintarXiv:2311.18259,2023. 3,7,14
preprintarXiv:2010.11929,2020. 5,15
Gu,X.,Sun,J.,andXu,Z. Sphericalspacedomainadapta-
Du,Z.,Li,J.,Su,H.,Zhu,L.,andLu,K. Cross-domaingra- tionwithrobustpseudo-labelloss. InProceedingsofthe
dientdiscrepancyminimizationforunsuperviseddomain IEEE/CVFConferenceonComputerVisionandPattern
adaptation. InProceedingsoftheIEEE/CVFconference Recognition,pp.9101–9110,2020. 2
oncomputervisionandpatternrecognition, pp.3937–
3946,2021. 6 Hendrycks, D. and Gimpel, K. A baseline for detecting
misclassifiedandout-of-distributionexamplesinneural
Dunlap, L., Mohri, C., Guillory, D., Zhang, H., Darrell, networks. arXivpreprintarXiv:1610.02136,2016. 4,6
T.,Gonzalez,J.E.,Raghunathan,A.,andRohrbach,A.
Using language to extend to unseen domains. In The Huang, Z., Zhou, A., Ling, Z., Cai, M., Wang, H., and
EleventhInternationalConferenceonLearningRepresen- Lee, Y. J. A sentence speaks a thousand images: Do-
tations,2023. URLhttps://openreview.net/forum? maingeneralizationthroughdistillingclipwithlanguage
id=eR2dG8yjnQ. 3,5 guidance. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pp.11685–11695,2023.
El Banani, M., Desai, K., and Johnson, J. Learning vi- 3
sual representations via language-guided sampling. In
ProceedingsoftheIEEE/CVFConferenceonComputer Hung, C.-C., Lange, L., and Strotgen, J. Tada: Efficient
VisionandPatternRecognition,pp.19208–19220,2023. task-agnosticdomainadaptationfortransformers.Annual
3
MeetingoftheAssociationforComputationalLinguistics,
2023. doi: 10.48550/arXiv.2305.12717. 9
French, G., Mackiewicz, M., and Fisher, M. Self-
Jia,C.,Yang,Y.,Xia,Y.,Chen,Y.-T.,Parekh,Z.,Pham,H.,
ensemblingforvisualdomainadaptation. arXivpreprint
Le, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up
arXiv:1706.05208,2017. 2
visualandvision-languagerepresentationlearningwith
Ganin,Y.andLempitsky,V. Unsuperviseddomainadapta- noisy text supervision. In International conference on
tionbybackpropagation. InInternationalconferenceon machinelearning,pp.4904–4916.PMLR,2021. 3
machinelearning,pp.1180–1189.PMLR,2015. 1,2
Kalluri,T.andChandraker,M. Cluster-to-adapt: Fewshot
Girdhar,R.,Singh,M.,Ravi,N.,vanderMaaten,L.,Joulin, domainadaptationforsemanticsegmentationacrossdis-
A., andMisra, I. Omnivore: Asinglemodel formany jointlabels. InProceedingsoftheIEEE/CVFConference
visualmodalities. InProceedingsoftheIEEE/CVFCon- onComputerVisionandPatternRecognition,pp.4121–
ferenceonComputerVisionandPatternRecognition,pp. 4131,2022. 2
16102–16112,2022. 7,15
Kalluri,T.,Sharma,A.,andChandraker,M.Memsac:Mem-
Goyal,P.,Duval,Q.,Seessel,I.,Caron,M.,Misra,I.,Sagun, oryaugmentedsampleconsistencyforlargescaledomain
L., Joulin, A., and Bojanowski, P. Vision models are adaptation. InComputerVision–ECCV2022: 17thEuro-
morerobustandfairwhenpretrainedonuncuratedimages peanConference,TelAviv,Israel,October23–27,2022,
withoutsupervision. arXivpreprintarXiv:2202.08360, Proceedings,PartXXX,pp.550–568.Springer,2022. 2,
2022. 3 5,6LaGTran:LanguageGuidedTransferAcrossDomains
Kalluri, T., Xu, W., andChandraker, M. Geonet: Bench- Liu, H., Li, C., Wu, Q., andLee, Y.J. Visualinstruction
marking unsupervised adaptation across geographies. tuning. NEURIPS,2023. 3
In Proceedings of the IEEE/CVF Conference on Com-
puterVisionandPatternRecognition(CVPR),pp.15368– Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,Lin,
15379,June2023. 1,2,4,5,6 S.,andGuo,B. Swintransformer: Hierarchicalvision
transformerusingshiftedwindows. InProceedingsofthe
Kang,G.,Jiang,L.,Yang,Y.,andHauptmann,A.G. Con- IEEE/CVFinternationalconferenceoncomputervision,
trastive adaptation network for unsupervised domain pp.10012–10022,2021b. 5,15
adaptation. In Proceedings of the IEEE Conference
onComputerVisionandPatternRecognition,pp.4893– Long,M.,Zhu,H.,Wang,J.,andJordan,M.I.Deeptransfer
4902,2019. 2 learningwithjointadaptationnetworks. InInternational
conferenceonmachinelearning,pp.2208–2217.PMLR,
Kumar,A.,Sattigeri,P.,Wadhawan,K.,Karlinsky,L.,Feris, 2017. 2
R.,Freeman,B.,andWornell,G. Co-regularizedalign-
mentforunsuperviseddomainadaptation.InAdvancesin Long,M.,Cao,Z.,Wang,J.,andJordan,M.I. Conditional
NeuralInformationProcessingSystems,pp.9345–9356, adversarial domain adaptation. In Advances in Neural
2018. 2 InformationProcessingSystems,pp.1640–1650,2018.
1,2,5,7
Kumar,A.,Ma,T.,andLiang,P.Understandingself-training
forgradualdomainadaptation. InInternationalConfer- Luo,Y.,Zheng,L.,Guan,T.,Yu,J.,andYang,Y. Takinga
enceonMachineLearning,pp.5468–5479.PMLR,2020. closerlookatdomainshift:Category-leveladversariesfor
2 semanticsconsistentdomainadaptation. InProceedings
oftheIEEEConferenceonComputerVisionandPattern
Kumar, A., Raghunathan, A., Jones, R., Ma, T., and
Recognition,pp.2507–2516,2019. 2
Liang, P. Fine-tuning can distort pretrained features
and underperform out-of-distribution. arXiv preprint Mahajan,D.,Girshick,R.,Ramanathan,V.,He,K.,Paluri,
arXiv:2202.10054,2022. 3 M.,Li,Y.,Bharambe,A.,andVanDerMaaten,L. Ex-
ploringthelimitsofweaklysupervisedpretraining. In
Li, J., Li, D., Savarese, S., and Hoi, S. C. H. Blip-2:
Proceedings of the European conference on computer
Bootstrappinglanguage-imagepre-trainingwithfrozen
vision(ECCV),pp.181–196,2018. 3
image encoders and large language models. Interna-
tional Conference on Machine Learning, 2023. doi:
Miech,A.,Zhukov,D.,Alayrac,J.-B.,Tapaswi,M.,Laptev,
10.48550/arXiv.2301.12597. 3,5
I.,andSivic,J. Howto100m: Learningatext-videoem-
beddingbywatchinghundredmillionnarratedvideoclips.
Li, S., Xie, M., Lv, F., Liu, C.H., Liang, J., Qin, C., and
InProceedingsoftheIEEE/CVFinternationalconference
Li,W. Semanticconcentrationfordomainadaptation. In
oncomputervision,pp.2630–2640,2019. 3
ProceedingsoftheIEEE/CVFinternationalconference
oncomputervision,pp.9102–9111,2021a. 2,6
Min,S.,Park,N.,Kim,S.,Park,S.,andKim,J. Grounding
visualrepresentationswithtextsfordomaingeneraliza-
Li,Y.,Nagarajan,T.,Xiong,B.,andGrauman,K. Ego-exo:
tion. InEuropeanConferenceonComputerVision,pp.
Transferringvisualrepresentationsfromthird-personto
37–53.Springer,2022. 3
first-person videos. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,
Munro,J.andDamen,D. Multi-modaldomainadaptation
pp.6943–6953,2021b. 2,7
forfine-grainedactionrecognition. InProceedingsofthe
Lin,K.Q.,Wang,J.,Soldan,M.,Wray,M.,Yan,R.,XU, IEEE/CVF conference on computer vision and pattern
E.Z.,Gao,D.,Tu,R.-C.,Zhao,W.,Kong,W.,etal. Ego- recognition,pp.122–132,2020. 6
centricvideo-languagepretraining. AdvancesinNeural
Ohkawa,T.,Yagi,T.,Nishimura,T.,Furuta,R.,Hashimoto,
InformationProcessingSystems,35:7575–7586,2022. 3,
A.,Ushiku,Y.,andSato,Y. Exo2egodvc: Densevideo
7,15
captioningofegocentricproceduralactivitiesusingweb
Liu,G.andWang,Y. Tdg: Text-guideddomaingeneraliza- instructionalvideos. arXivpreprintarXiv:2311.16444,
tion. arXivpreprintarXiv:2308.09931,2023. 3,5 2023. 2,7
Liu, H., Wang, J., and Long, M. Cycle self-training for Park, C., Lee, J., Yoo, J., Hur, M., and Yoon, S. Joint
domainadaptation. AdvancesinNeuralInformationPro- contrastivelearningforunsuperviseddomainadaptation.
cessingSystems,34:22968–22981,2021a. 2 arXivpreprintarXiv:2006.10297,2020. 2LaGTran:LanguageGuidedTransferAcrossDomains
Pei,Z.,Cao,Z.,Long,M.,andWang,J. Multi-adversarial Reimers,N.andGurevych,I. Sentence-bert: Sentenceem-
domain adaptation. arXiv preprint arXiv:1809.02176, beddingsusingsiamesebert-networks. InProceedings
2018. 2 ofthe2019ConferenceonEmpiricalMethodsinNatu-
ralLanguageProcessing.AssociationforComputational
Peng,X.,Usman,B.,Kaushik,N.,Hoffman,J.,Wang,D., Linguistics, 11 2019. URL http://arxiv.org/abs/
and Saenko, K. Visda: The visual domain adaptation 1908.10084. 5
challenge. arXivpreprintarXiv:1710.06924,2017. 1
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Peng,X.,Bai,Q.,Xia,X.,Huang,Z.,Saenko,K.,andWang,
Ommer,B. High-resolutionimagesynthesiswithlatent
B. Momentmatchingformulti-sourcedomainadaptation. diffusionmodels. InProceedingsoftheIEEE/CVFcon-
InProceedingsoftheIEEEInternationalConferenceon
ferenceoncomputervisionandpatternrecognition,pp.
ComputerVision,pp.1406–1415,2019. 5
10684–10695,2022. 3
Pham, H., Dai, Z., Ghiasi, G., Kawaguchi, K., Liu, H.,
Saenko,K.,Kulis,B.,Fritz,M.,andDarrell,T. Adapting
Yu, A. W., Yu, J., Chen, Y.-T., Luong, M.-T., Wu, Y.,
visual category models to new domains. In Computer
etal. Combinedscalingforzero-shottransferlearning.
Vision–ECCV2010: 11thEuropeanConferenceonCom-
Neurocomputing,555:126658,2023. 3
puterVision,Heraklion,Crete,Greece,September5-11,
2010, Proceedings, Part IV 11, pp. 213–226. Springer,
Plizzari,C.,Perrett,T.,Caputo,B.,andDamen,D. What
2010. 1
can a cook in italy teach a mechanic in india? action
recognitiongeneralisationoverscenariosandlocations.
Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,
arXivpreprintarXiv: 2306.08713,2023. 6
E.L.,Ghasemipour,K.,GontijoLopes,R.,KaragolAyan,
B.,Salimans,T.,etal. Photorealistictext-to-imagedif-
Prabhu, V., Selvaraju, R. R., Hoffman, J., and Naik, N.
fusion models with deep language understanding. Ad-
Candomainadaptationmakeobjectrecognitionworkfor
vances in Neural Information Processing Systems, 35:
everyone? InProceedingsoftheIEEE/CVFConference
36479–36494,2022. 3
onComputerVisionandPatternRecognition,pp.3981–
3988,2022. 1,2
Sahoo, A., Shah, R., Panda, R., Saenko, K., and Das, A.
Contrast and mix: Temporal contrastive video domain
Quattrocchi,C.,Furnari,A.,DiMauro,D.,Giuffrida,M.V.,
adaptationwithbackgroundmixing. AdvancesinNeural
and Farinella, G. M. Synchronization is all you need:
InformationProcessingSystems,34,2021. 2
Exocentric-to-egocentrictransferfortemporalactionseg-
mentationwithunlabeledsynchronizedvideopairs.arXiv
Saito, K. and Saenko, K. Ovanet: One-vs-all network
preprintarXiv:2312.02638,2023. 2,7
for universal domain adaptation. IEEE International
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Conference on Computer Vision, 2021. doi: 10.1109/
Sutskever,I.,etal. Languagemodelsareunsupervised ICCV48922.2021.00887. 6
multitasklearners. OpenAIblog,1(8):9,2019. 4,8,15
Saito, K., Ushiku, Y., Harada, T., and Saenko, K.
Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G., Adversarial dropout regularization. arXiv preprint
Agarwal,S.,Sastry,G.,Askell,A.,Mishkin,P.,Clark,J., arXiv:1711.01575,2017. 2
etal. Learningtransferablevisualmodelsfromnatural
language supervision. In International conference on Saito,K.,Watanabe,K.,Ushiku,Y.,andHarada,T. Max-
machinelearning,pp.8748–8763.PMLR,2021. 3,5,6, imum classifier discrepancy for unsupervised domain
8,15 adaptation. In Proceedings of the IEEE Conference
onComputerVisionandPatternRecognition,pp.3723–
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., 3732,2018. 1,2,6
Matena,M.,Zhou,Y.,Li,W.,andLiu,P.J. Exploring
thelimitsoftransferlearningwithaunifiedtext-to-text Saito,K.,Kim,D.,Sclaroff,S.,andSaenko,K. Universal
transformer. TheJournalofMachineLearningResearch, domainadaptationthroughselfsupervision. Advancesin
21(1):5485–5551,2020. 4,8,15 neuralinformationprocessingsystems,33:16282–16292,
2020. 4,6
Ramesh,A.,Pavlov,M.,Goh,G.,Gray,S.,Voss,C.,Rad-
ford, A., Chen, M., and Sutskever, I. Zero-shot text- Sanh,V.,Debut,L.,Chaumond,J.,andWolf,T. Distilbert,
to-image generation. In International Conference on a distilled version of bert: smaller, faster, cheaper and
MachineLearning,pp.8821–8831.PMLR,2021. 3 lighter. NEURIPS,2019. 4,8,15LaGTran:LanguageGuidedTransferAcrossDomains
Sariyildiz,M.B.,Perez,J.,andLarlus,D. Learningvisual Tzeng,E.,Hoffman,J.,Saenko,K.,andDarrell,T. Adver-
representationswithcaptionannotations. InComputer sarialdiscriminativedomainadaptation. InProceedings
Vision–ECCV2020:16thEuropeanConference,Glasgow, oftheIEEEconferenceoncomputervisionandpattern
UK,August23–28,2020,Proceedings,PartVIII16,pp. recognition,pp.7167–7176,2017. 2
153–170.Springer,2020. 3
Udandarao,V.,Gupta,A.,andAlbanie,S. Sus-x: Training-
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., freename-onlytransferofvision-languagemodels. In
Wightman,R.,Cherti,M.,Coombes,T.,Katta,A.,Mullis, ProceedingsoftheIEEE/CVFInternationalConference
C.,Wortsman,M.,etal. Laion-5b: Anopenlarge-scale onComputerVision,pp.2725–2736,2023. 3
dataset for training next generation image-text models.
AdvancesinNeuralInformationProcessingSystems,35: Venkateswara, H., Eusebio, J., Chakraborty, S., and Pan-
25278–25294,2022. 2 chanathan, S. Deep hashingnetworkfor unsupervised
domainadaptation. InProceedingsoftheIEEEConfer-
Sharma,A.,Kalluri,T.,andChandraker,M. Instancelevel
ence on Computer Vision and Pattern Recognition, pp.
affinity-based transfer for unsupervised domain adap-
5018–5027,2017. 1
tation. In Proceedings of the IEEE/CVF Conference
onComputerVisionandPatternRecognition,pp.5361– Wang,R.,Wu,Z.,Weng,Z.,Chen,J.,Qi,G.-J.,andJiang,
5371,2021. 1,2 Y.-G.Cross-domaincontrastivelearningforunsupervised
domainadaptation. IEEETransactionsonMultimedia,
Sigurdsson,G.A.,Gupta,A.,Schmid,C.,Farhadi,A.,and
2022. 2
Alahari,K. Actorandobserver: Jointmodelingoffirst
and third-person videos. In proceedings of the IEEE Wang, Z., Zhang, L., Wang, L., and Zhu, M. Landa:
conferenceoncomputervisionandpatternrecognition, Language-guidedmulti-sourcedomainadaptation. arXiv
pp.7396–7404,2018. 7 preprintarXiv:2401.14148,2024. 3
Singh,M.,Gustafson,L.,Adcock,A.,deFreitasReis,V.,
Wei,G.,Lan,C.,Zeng,W.,Zhang,Z.,andChen,Z.Toalign:
Gedik, B., Kosaraju, R. P., Mahajan, D., Girshick, R.,
Task-orientedalignmentforunsuperviseddomainadapta-
Dolla´r, P., and Van Der Maaten, L. Revisiting weakly
tion. InNeurIPS,2021. 1,2,5
supervisedpre-trainingofvisualperceptionmodels. In
ProceedingsoftheIEEE/CVFConferenceonComputer Wei, P., Kong, L., Qu, X., Ren, Y., Xu, Z., Jiang, J., and
VisionandPatternRecognition,pp.804–814,2022. 3 Yin,X. Unsupervisedvideodomainadaptationforaction
recognition: Adisentanglementperspective. InAdvances
Sun,B.andSaenko,K. Deepcoral: Correlationalignment
inNeuralInformationProcessingSystems,2023. 2,6,7,
fordeepdomainadaptation. InComputerVision–ECCV
15
2016Workshops: Amsterdam,TheNetherlands,October
8-10and15-16,2016,Proceedings,PartIII14,pp.443– Wortsman,M.,Ilharco,G.,Kim,J.W.,Li,M.,Kornblith,
450.Springer,2016. 2 S.,Roelofs,R.,Lopes,R.G.,Hajishirzi,H.,Farhadi,A.,
Namkoong, H., et al. Robust fine-tuning of zero-shot
Sun,T.,Lu,C.,Zhang,T.,andLing,H.Safeself-refinement
models. In Proceedings of the IEEE/CVF Conference
fortransformer-baseddomainadaptation. InProceedings
onComputerVisionandPatternRecognition,pp.7959–
of the IEEE/CVF conference on computer vision and
7971,2022. 3
patternrecognition,pp.7191–7200,2022. 2,6
Xie,S.,Zheng,Z.,Chen,L.,andChen,C. Learningseman-
Tan,S.,Peng,X.,andSaenko,K. Class-imbalanceddomain
ticrepresentationsforunsuperviseddomainadaptation.
adaptation: anempiricalodyssey. InComputerVision–
In International Conference on Machine Learning, pp.
ECCV 2020 Workshops: Glasgow, UK, August 23–28,
5423–5432,2018. 2
2020, Proceedings, Part I 16, pp. 585–602. Springer,
2020. 2
Xu, R., Li, G., Yang, J., and Lin, L. Larger norm more
Thomee,B.,Shamma,D.A.,Friedland,G.,Elizalde,B.,Ni, transferable: Anadaptivefeaturenormapproachforun-
K.,Poland,D.,Borth,D.,andLi,L.-J. Yfcc100m: The supervised domain adaptation. In Proceedings of the
newdatainmultimediaresearch. Communicationsofthe IEEEInternationalConferenceonComputerVision,pp.
ACM,59(2):64–73,2016. 2 1426–1435,2019. 1
Tzeng, E., Hoffman, J., Darrell, T., and Saenko, K. Si- Xu, T., Chen, W., Wang, P., Wang, F., Li, H., and Jin,
multaneousdeeptransferacrossdomainsandtasks. In R. Cdtrans: Cross-domaintransformerforunsupervised
ProceedingsoftheIEEEinternationalconferenceoncom- domain adaptation. arXiv preprint arXiv:2109.06165,
putervision,pp.4068–4076,2015. 2 2021. 2,6LaGTran:LanguageGuidedTransferAcrossDomains
Xue, Z.S.andGrauman, K. Learningfine-grainedview-
invariantrepresentationsfromunpairedego-exovideos
viatemporalalignment. AdvancesinNeuralInformation
ProcessingSystems,36,2023. 2,7
You, K., Long, M., Cao, Z., Wang, J., and Jordan, M. I. DomainNet-Image GeoNet-Image Ego2Exo-Video
Universal domain adaptation. In Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition,pp.2720–2729,2019. 4,6
Zhang,Y.,Liu,T.,Long,M.,andJordan,M.I. Bridgingthe-
oryandalgorithmfordomainadaptation. arXivpreprint DomainNet-Text GeoNet-Text Ego2Exo-Text
arXiv:1904.05801,2019. 5,6
Figure7:DomainRobustnessTextvsImage:Cross-domainaccuracyof
imageorvideoclassifiertransferacrossdomainscomparedtotextmodality.
Zhao,Y.,Misra,I.,Kra¨henbu¨hl,P.,andGirdhar,R.Learning
Asopposedtosignificantlylargedomaindropswhentransferringimage-
video representations from large language models. In
basedmodelsacrossdomains,text-classifierbasedmodelsaresurprisingly
ProceedingsoftheIEEE/CVFConferenceonComputer robustonallthestudiedbenchmarks,leadingtominimaldomaindrops
VisionandPatternRecognition,pp.6586–6597,2023. 3, andhighaccuracy.
7
A.IllustratingCross-domainRobustness
Zhu,J.,Bai,H.,andWang,L. Patch-mixtransformerfor
Weillustratethecross-domainrobustnesspropertiesofim-
unsuperviseddomainadaptation: Agameperspective. In
agevstextclassifiersinFig.7. Weshowtheremarkably
ProceedingsoftheIEEE/CVFConferenceonComputer
powerfultarget-domainmodelsobtainedbytransferringthe
VisionandPatternRecognition,pp.3561–3571,2023. 1,
text classifier, as opposed to image-based models which
2,5,6,8,15
sufferhighdomaingap. Thisbehaviorisconsistentacross
all the datasets studied, and forms the backbone of our
motivations in leveraging textual guidance in performing
unsupervisedtransferacrossdomains.
B.ConstructionofEgo2Exobenchmark
We curate Ego2Exo dataset from the larger Ego-Exo4D
dataset(Graumanetal.,2023). Specifically,thekeystepan-
notationsprovidedalongwithEgo-Exo4Dofferfine-grained
actionlabelsforseveralshortvideoclips,calledsegments,
thataremanuallytrimmedfromlongproceduralvideosto
focus on the keysteps to recognize. We restrict focus to
videosfromcookingactivity,astheyincludethelargestset
ofsegmentsandlabelscapturingadiversepoolofactions.
Moreover,toeasethedifficulttaskofpredictingveryfine-
grainedactionclassesfromshortsegments(eg:add coffee
beansvs. add coffee grounds),weusetheprovidedla-
belhierarchyandmanuallyremaptheoriginal96annotated
actionslabelsinto22labelsbymergingsimilarclassesinto
alarger,commonclass. Thefinalcategorylistisasfollows:
1. Cleanup
2. GetIngredients
3. Prepareingredients
4. Getkitchenware&utensils
5. Constructundressedsalad
6. Makemilktea
7. Serve
8. Cook
9. PrepareaskilletLaGTran:LanguageGuidedTransferAcrossDomains
10. Brewcoffee(instantcoffee) computedfeaturesastheclassifierhead. Acrossalltransfer
11. Preparemilk(boiled) settings,wetrainthesebackbonesfor90,000iterationsusing
12. Brewcoffee(frenchpress) theobjectivefunctionspecifiedinEq.(3),employingSGD
13. Brewcoffee(manualpour-over) withalearningrateof3e-4andbatchsizeof64fromeach
14. Makesalad domain,alongwithacosinedecayschedule.
15. Preparemilk(frothed)
16. Preparesaladdressing
TextClassifier Weuseapre-trainedDistill-BERT(Sanh
17. Boilnoodlesinboilingwater
etal.,2019)modelfromHuggingFaceasthesentenceclas-
18. Cooknoodlesinaskillet
sificationmodelB(;ϕ),andfine-tuneitforfiveepochsover
19. Mixnoodleswithsauceinabowl
the source domain data using AdamW optimizer with a
20. Preparedressing
learning rate of 5e-5 and cosine decay over the training
21. Makechaitea
schedule.Weobservedsub-optimalperformanceusingother
22. Checkpaperrecipe
pre-trainedbackbonessuchasT5(Raffeletal.,2020),GPT-
2(Radfordetal.,2019)ortextencoderinCLIP(Radford
To provide text supervision to our algorithm, we use the
etal.,2021)(Sec.4.6).
atomicactiondescriptionsprovidedinEgo-Exo4Ddataset.
Thesedescriptionsprovideanarrativeoftheeventsinthe
video,presentedinfree-formtextfromtheperspectiveof
Video Classifier We use the pre-computed Omnivore-
a third-party observer. Unlike keystep labels, which are
base (Girdhar et al., 2022) features provided along with
definedbetweenspecificstartandendtimeswithinavideo,
the EgoExo4D dataset for training and evaluation. Since
these text descriptions are associated with distinct times-
differentkeystepsmayberepresentedbylargelydifferent
tamps, or a single point in time within the video. To cre-
timespans(Fig.4a),wecollectallfeaturesthatfallwithin
atecorrespondencemappingbetweenthekeystepsegments
thestartandendtimesofasegment,andpoolthesefeatures
and text descriptions, we adopt the method followed in
togethertoforma1536-dimensionalfeaturerepresentation
EgoVLP(Linetal.,2022): togenerateatextdescriptionfor
of that segment. We then train a 2-layer MLP classifier
asegment,wecompilealltextdescriptionsthatfallwithin
on top of these features, using the labeled source feature
the timestamps defined by the start and end times of that
aswellaspsuedo-labeledtargetfeaturesfollowingEq.(3).
segment. Ifmultipletimestampsexist,weconcatenatethe
Notethatthistrainingstrategyisequivalenttotrainingan
correspondingtexts;ifnotimestampsareavailable,wein-
MLPclassifierontopoffrozenOmnivorebackbone. For
cludenoassociatedtextwiththesegment. Furthermore,we
faircomparison,wefollowthesamestrategyfortrainingall
concatenatetheannotationsprovidedbymultipleannotators
theotherbaselinesaswellasprioradaptationmethods. For
increatingthetextdescription.
methodsthatrequireatemporalsequenceoffeatures(Wei
OurproposedEgo2Exoconsistsof10,102videosegments etal.,2023;Chenetal.,2019b),wesample8equallyspaced
labeled with actions from one of the 22 keysteps, out of featuresfromthecompletesetofsegmentfeatures,anduse
which4200(41.9%)segmentshavevalidtextdescriptions thisfeaturesequenceasinput. Wefollowsimilarstrategy
available. We split these video segments into two equal forevaluation,andusefeaturespre-extractedfromthevali-
groupsclasswise, andcollectego-videosfromonegroup dationvideosfortesting. Weusethetop-1accuracyonthe
and exo-videos from the other to create our adaptation validationsetforevaluation.
benchmark. Thesameprocedureappliedtothevalidation
videosyields3147validationsegmentswithbothegoand
exo views. The json file containing our complete set of
videos (referenced from Ego-Exo4D dataset), along with
annotationsandtextdescriptionsisinludedalongwiththe
supplementaryfile.
C.TrainingDetails
ImageClassifier WeuseaViT-base(Dosovitskiyetal.,
2020)backboneastheimageencoderontheGeoNetdataset,
and follow prior work (Zhu et al., 2023) and use Swin-
basebackbone (Liuetal., 2021b)forexperimentsonthe
DomainNet data. Both the backbones are pre-trained on
ImageNet-1k, and we add a 2-layer MLP on top of the