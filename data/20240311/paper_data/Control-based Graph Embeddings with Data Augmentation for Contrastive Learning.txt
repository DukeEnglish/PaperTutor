Control-based Graph Embeddings with Data Augmentation
for Contrastive Learning
Obaid Ullah Ahmad, Anwar Said, Mudassir Shabbir, Waseem Abbas, Xenofon Koutsoukos
Abstract—In this paper, we study the problem of unsuper- graph representation learning, aiming to maximize agree-
vised graph representation learning by harnessing the control ment between similar subgraphs and produce informative
properties of dynamical networks defined on graphs. Our ap-
embeddings that capture the graph structure [5]. While
proach introduces a novel framework for contrastive learning,
existing GCRL approaches primarily focus on node-level
a widely prevalent technique for unsupervised representation
learning. A crucial step in contrastive learning is the creation embeddings [6], our proposed architecture has the potential
of‘augmented’graphsfromtheinputgraphs.Thoughdifferent to generate graph-level embeddings suitable for SSL.
from the original graphs, these augmented graphs retain the Inthiswork,weintroduceanovelapproachthatleverages
original graph’s structural characteristics. Here, we propose
controlpropertiestodesigngraph-levelembeddingsforself-
a unique method for generating these augmented graphs by
supervised learning. Recent research has uncovered deep
leveragingthecontrolpropertiesofnetworks.Thecoreconcept
revolves around perturbing the original graph to create a connections between network controllability and various
new one while preserving the controllability properties specific graph-theoretic constructs, including matching, graph dis-
to networks and graphs. Compared to the existing methods, tances,andzeroforcingsets[7]–[9].Additionally,significant
we demonstrate that this innovative approach enhances the
progress has been made in characterizing the controlla-
effectiveness of contrastive learning frameworks, leading to
bility of different families of network graphs, including
superior results regarding the accuracy of the classification
tasks. The key innovation lies in our ability to decode the paths, cycles, random graphs, circulant graphs, and product
network structure using these control properties, opening new graphs [9]. These investigations shed light on the interplay
avenues for unsupervised graph representation learning. between network structures and their controllability proper-
ties,enhancingourunderstandingofnetworkdynamics.Our
I. INTRODUCTION
aim is to explore and harness the interconnections between
Networks serve as fundamental data structures for rep- networkstructuresandtheircontrollabilitypropertiestoform
resenting relationships, connectivity, and interactions across a foundation for comprehensive graph representations.
various domains, such as social networks, biology, trans- Furthermore,weintroducesystematicgraphaugmentation
portation, brain connectivity, and recommendation sys- for creating positive and negative pairs in CL, in contrast to
tems [1]. Network representation learning plays a pivotal previous random edge perturbation methods [5]. Our sys-
role in acquiring meaningful network representations, which tematic approach focuses on preserving the graph’s control
find applications in tasks like node classification, link pre- properties, leading to improved performance in downstream
diction, and community detection [2]. Traditional network machine-learning tasks.
representationlearningheavilyreliesonsupervisedlearning, Our main contributions can be summarized as follows:
necessitating substantial labeled data for effective train-
• We introduce a novel graph embedding—representing
ing [2]. However, obtaining labeled network data is often
graphs as vectors— called CTRL, which is based on
challenging, expensive, and limited in availability.
the control properties of networks defined on graphs,
Contrarily, contrastive learning (CL) has emerged as a including meaningful metrics of controllability such as
prominent self-supervised learning (SSL) technique in un- the spectrum of the Gramian matrix.
supervised network representation learning [3]. CL methods • WepresenttheControl-basedGraphContrastiveLearn-
operate by comparing augmented positive and negative sam- ingarchitectureforunsupervisedrepresentationlearning
ples with the original graph. The positive samples exhibit of networks, applicable to various downstream graph-
similarity, while the negative samples manifest dissimilarity. level tasks.
This framework empowers CL methods to acquire represen- • We devise innovative augmentation techniques that
tationsthatcapturetheinherentnetworkstructure,evenwhen mainly preserve the controllability of the network.
labeled data is absent [4]. Graph Contrastive Representation • We conduct extensive numerical evaluations on real-
Learning has recently gained attention in the context of world graph datasets, showcasing the effectiveness of
our method in graph classification compared to several
Obaid Ullah Ahmad is with the Electrical Engineering Department
state-of-the-art (SOTA) benchmark methods.
at the University of Texas at Dallas, Richardson, TX. Email: Obaidul-
lah.Ahmad@utdallas.edu.
Anwar Said, Mudassir Shabbir and Xenofon Koutsoukos are with the II. PRELIMINARIESANDPROBLEMSTATEMENT
ComputerScienceDepartmentattheVanderbiltUniversity,Nashville,TN.
Emails:anwar.said,mudassir.shabbir,xenofon.koutsoukos@vanderbilt.edu. A. Notations
Waseem Abbas is with the Systems Engineering Department
A network composed of interconnected entities is rep-
at the University of Texas at Dallas, Richardson, TX. Email:
waseem.abbas@utdallas.edu resented as a graph, denoted as G = (V,E), where the
4202
raM
7
]GL.sc[
1v32940.3042:viXraset of vertices V = V(G) = v ,v ,...,v represents the of the powerful techniques of SSL is contrastive learn-
1 2 N
entities, and the collection of edges E = E(G) ⊆ V ×V ing which has achieved remarkable success across various
represents pairs of entities with established relationships. In domains, including computer vision [4]. In the realm of
this context, the terms ’vertex,’ ’node,’ and ’agent’ are used graph representation learning, researchers have introduced
interchangeably. The neighborhood of a vertex v is defined Contrastive Graph Representation Learning (CGRL) [10].
i
as N = v ∈V :(v ,v )∈E. The distance between ver- Thisapproachoperatesontheprincipleofgeneratingdiverse
i j j i
tices v and v , denoted as d(v ,v ), represents the shortest augmented perspectives of the same data samples through
i j i j
path length between them. The transpose of a matrix X is pretexttasks.Weproposetointroduceadynamicalsystemon
denotedasXT.AnN-dimensionalvectorwithallitsentries graphs, examining the control characteristics of this system,
set to zero is represented as 0 , and a vector with all its and subsequently crafting an embedding that utilizes these
N
entries set to 1 is denoted as 1 . Although our primary control attributes, as elaborated in Section III.
N
focusisonundirectedgraphsforsimplicityinexplanation,it a) Graph Contrastive Representation Learning: Graph
isimportanttonotethatallmethodsandfindingsareequally Contrastive Representation Learning (GCRL) offers distinct
applicable to directed graphs. advantages over traditional unsupervised graph representa-
tion methods. GCRL encourages the model to bring similar
B. Problem Description
nodes or subgraphs closer together in the embedding space
In this subsection, we introduce the problem of generat-
while pushing dissimilar nodes or subgraphs farther apart
ing graph-level representations in an unsupervised manner.
[5], enhancing performance in various downstream tasks
We begin by providing a formal definition of the problem
[11]. It excels in data efficiency, leveraging limited labeled
and then present a contrastive learning-based solution. For
data efficiently with unlabeled data sources [5]. GCRL is
graph-level embeddings, we develop a control-based graph
scalable, handling large-scale graph datasets effectively [5],
embedding method in Section III
[12]. It facilitates easy transfer of learned representations to
A graph embedding, denoted as ϕ(G) : G → Rd, maps
diverse downstream tasks, including node classification, link
graphs from the family G to a Euclidean space of dimension
prediction, and graph classification [5]. Lastly, GCRL often
d. The main goal of graph embedding is to meet specific
producesinterpretableembeddings,aidingintheunderstand-
design criteria. Due to the limited availability of labeled
ing and analysis of the learned representations [11].
data for large real-world benchmark datasets, we need a
b) Control-basedGraphContrastiveLearning(CGCL):
mechanism to learn graph representations without heavy
We introduce Control-based Graph Contrastive Learning
reliance on labels. One crucial objective is to ensure that ϕ
(CGCL), which computes control-based graph-level features
retainsinformationaboutstructuralsimilaritiesbetweenpairs
denoted as CTRL(.). These features are utilized to bring
of graphs, both at local and global scales. This means that
an augmented version G′ of a graph G closer together in
if two graphs share structural similarities, their embeddings
a latent space z(.) using the normalized temperature-scaled
should yield vectors that are close to the target vector space,
cross-entropy loss (NT-Xent) [3]. Additionally, we propose
as measured by Euclidean distance. It’s important to note
various augmentation techniques designed to preserve the
that the notion of similarity between graphs depends on the
CTRL properties of the graph to a certain extent. This is
specificapplicationandthetypeofgraphsbeingconsidered,
illustrated in Figure 1.
such as chemical compounds or social networks.
For a given graph G, we apply a control-based augmen-
Another crucial design objective for graph embeddings
tation T(G) to obtain G′, creating a positive pair. We then
is scalability. An optimal graph embedding should not only
computecontrol-basedfeaturesCTRL(.)forbothGandG′
map graphs of different sizes to a fixed-dimensional space
andpassthemthroughalearnableencoderf(.)totransform
but should also transcend graph size to capture underlying
them into a new latent space. The goal is to optimize the
structural characteristics. For example, an effective graph
similarity of each positive pair in this latent space. This
embedding would position the mapping of a ten-node
concept is illustrated in Figure 1, where the embeddings z
circulant graph closer to that of a twenty-node circulant G
and z are represented as yellow cuboids.
graph compared to the mapping of a fifteen-node wheel G′
This optimization is achieved using the NT-Xent loss
graph. In this paper, we tackle the challenge of generating
proposedbyOordetal.[3].Thelossfunctionencouragesthe
graphembeddingswhileadheringtothesedesignobjectives.
similarity between the embeddings of the original graph and
Problem 1: Given a graph G, generate unsupervised graph
its transformed counterpart (positive pair) while minimizing
representations ϕ(G) that capture essential structural in-
thesimilaritywiththetransformedembeddingsz(.)ofother
formation and node relationships for subsequent machine
graphs in the dataset (negative pairs). This self-supervised
learning tasks.
learning approach enables the model to capture meaningful
These learned representations ϕ(G) are intended to be se- representationseffectively.TheNT-Xentlossisemployedas:
mantically meaningful and effective for various downstream (cid:34) (cid:35)
tasks, including node classification, link prediction, graph L=E −log exp(sim(z G,z G′)/τ) ,
classification, and community detection. 1 (cid:80) exp(sim(z ,z )/τ)
|G| g∈G,g̸=G G g′
C. Proposed Approach wheresim(z ,z )representsthecosinesimilaritybetween
G G′
Atypicalapproachtolearningunsupervisedrepresentation the embeddings of the graph G and its augmentation G′, G
of raw data is called self-supervised learning (SSL). One is the set containing all the graphs in the dataset, g′ is theInputgraph
(G) (zG)
LeaderVertex
F So hl alo rew der EV nce ort de ex r T(G) CTRL(G) f(CTRL(.)) AM ga rex eim mi ez ne t
CTRLEmbeddings
LearnedEmbeddings
Augmentedgraph
(G(cid:48))
(zG(cid:48))
CTRL(G(cid:48))
Fig. 1: Block diagram of the proposed CGCL approach
augmented version of graph g, and E is the expectation. establish an ordered arrangement for the nodes, with V
f
The temperature hyperparameter τ is used to control the consistingofnodesv ,v ,...,v ,andV containingnodes
1 2 Nf ℓ
sharpness of the distribution. v ,...,v . The subgraph composed of follower nodes
Nf+1 N
In summary, CGCL leverages contrastive learning princi- isreferredtoasthefollowergraph(G ),representedmathe-
f
ples to generate expressive graph representations by consid- matically using the Laplacian matrix L, which is partitioned
ering control-based graph-level features and optimizing the as:
similarity of positive pairs, demonstrating its potential for (cid:20) A B (cid:21)
self-supervised graph representation learning. L= BT C ,
III. NETWORKCONTROLLABILITYANDGRAPH where A∈RNf×Nf, B ∈RNf×Nℓ, and C ∈RNℓ×Nℓ.
EMBEDDINGS
Inthisconfiguration,weintroduceanexternalinputsignal
In this section, we introduce a novel graph embedding u appliedtoleaderagentv ∈V .Thefollowernodesupdate
l l ℓ
approach rooted in the control properties of networks. We their states according to the dynamics given by:
begin by presenting a network as a controllable dynamic
x˙ (t)=−Ax (t)−Bu(t),
system. We then provide a formal definition of network f f
controllability and subsequently explore several metrics em- where x f(t) ∈ RNf represents the state vector of follower
ployed to quantify it. These metrics serve as the foundation nodes at time t, and u(t) = [u Nf+1(t) ··· u N(t)]T ∈ RNℓ
forconstructingcontrol-basedgraphembeddings,denotedas is the control signal at time t. The matrices −A and −B in
CTRL(G), for a given graph G. these dynamics are derived from the network structure and
leader agent selection.
A. Networks as Dynamical Systems
From a control perspective, we are interested in assessing
Inthecontextofnetworkdynamics,eachagent,denotedas the feasibility of steering the system described by these
v i, represents a dynamic unit with a state x i(t)∈R at time dynamics from an initial state to a final state within a finite
t. These agents share their states with neighboring agents in time interval t . If control is achievable, we aim to quantify
1
N i and update their states based on specific dynamics, like the control energy E(u) required, as defined below:
consensus dynamics. The collective system state at time t is
represented as the vector x(t)=[x 1(t)x 2(t) ... x N(t)]T.
E(u)=(cid:90) t1
∥u(τ)∥2dτ.
To control the states of this dynamical system, we intro-
τ=0
duce external control signals applied to a subset of agents Wealsoinvestigatethedimensionofthesubspacecontaining
knownasleaders.Theseleaderagentshavestatesthatcanbe
reachable states and the impact of changing leader agents.
directly manipulated, expressed as x˙ l = u l(t), where u l(t) These inquiries provide insights into the underlying graph
represents the input signal. Conversely, non-leader agents, structure and guide the derivation of network controllability
referred to as followers, update their states by aggregating metrics for graph embeddings.
information from their local neighborhoods. The dynamics
of the followers, denoted as x˙ (t) in this leader-follower B. Network Controllability Metrics
f
system, can be expressed as: The process of controlling a network entails the respon-
sibility of directing it from an initial state to a desired final
x˙ (t)=−M(G)x (t)+H(G)u(t),
f f state by applying control inputs to specific leader nodes
where M(G) represents system matrices related to the within the network. A state x∗ ∈RNf is considered reach-
f
followers’ subgraph, and H(G) denotes the topological in- able when there exists an input that can propel the network
teractions between leader and follower agents. We use the fromtheorigin0 tothestatex∗ withinafinitetimeframe.
Nf f
Laplacian matrix for a matrix representation of a network. The set comprising all such reachable states defines what
For a network represented as a graph G = (V,E), we we refer to as the controllable subspace. Importantly, in
partition the node set V into two groups: followers (V ) continuous linear time-invariant systems, such as the one
f
and leaders (V ), where |V | = N and |V | = N . We described by equation (III-A), if a state x ∗ is reachable
ℓ f f ℓ ℓ ffromtheorigin,itisalsoreachablefromanyarbitraryinitial topologicalconfigurationandtheplacementofleaderswithin
state within any given duration of time. it. The impact of leader selection on network controllability
The dimension of this controllable subspace γ(G,V ), is is visualized in Figure 2. In this scenario, we examine a
ℓ
a pivotal concept in control theory. It can be determined network consisting of 10 agents, with one designated as the
by examining the rank of the Controllability matrix: C = leader,resultinginN =9.InFigure2(b),thedimensionof
f
(cid:2) −B (−A)(−B) ··· (−A)Nf−1(−B) (cid:3) . The rank of the controllable subspace is 9, indicating complete control-
this matrix hinges on the properties of matrices A and B, lability of the follower network. The edges connecting the
which,inturn,arecontingentonthenetwork’sstructureand leader and follower nodes, which define the structure of the
the selection of leader nodes. B matrix in (III-A), are highlighted in red. Transitioning to
The Controllability Gramian serves as a significant math- Figure 2(c), we opt for a different leader while preserving
ematical construct that offers vital insights into the control complete controllability; however, this results in a modified
characteristicsofanetwork[13]–[15].UtilizingtheControl- trace of W.
lability Gramian, we can quantitatively assess the ease of
transitioning from one state to another, taking into account
the necessary control energy as defined in equation (III-A).
For the system delineated in equation (III-A), the infinite
horizon controllability Gramian is defined as follows: (a) input G (b) rank = 9, tr = 1.5
(cid:90) ∞
W = e−Aτ(−B)(−B)Te−ATτdτ ∈ RNf×Nf.
0
If the system is stable, signifying that all eigenvalues of
−A have negative real parts, W asymptotically converges (c) rank = 9, tr = 3.5 (d) rank = 8, tr = 7.1
and can be computed through the Lyapunov equation:
Fig. 2: Controllability metrics vary with leader selection
(−A)W +W(−A)T +(−B)(−B)T =0,
For a solution to exist for (III-B), it is necessary for −A To gather valuable insights into the graph structure, we
to be a stable matrix. This condition holds for connected employaneffectiveprobingstrategy.Byvaryingthenumber
graphs. and positions of leader nodes, we can observe the resulting
Lemma 1: If we partition the Laplacian matrix L of an controllability behavior, as quantified by metrics such as
undirected connected graph as shown in (III-A), the matrix tr(W),µ j(W),rank(W),andld(W).Inourempiricaleval-
A is positive definite [9]. uation in Section V, we primarily focus on the rank, trace,
In summary, when partitioning the Laplacian matrix L of andbothminimumandnon-zeroeigenvaluesoftheGramian
an undirected connected graph, as demonstrated in equation matrix, considering multiple leader set configurations.
(III-A), the matrix A is revealed to be positive definite,
ensuring the system’s stability. This stability enables the IV. CONTROL-BASEDGRAPHAUGMENTATIONS
computationoftheControllabilityGramianW,whichserves
Contrastive Graph Representation Learning (CGRL) is
as a valuable measure of controllability in terms of energy-
a self-supervised technique that relies on augmented data
related quantification. It also facilitates the derivation of
to create positive and negative pairs. As discussed in the
various controllability statistics [13]–[15]. Some of these
previous section, we have developed graph-level control
statistics are further discussed below.
embeddingsthatactasinputstotheencoder,minimizingthe
i Trace of W: The trace of the controllability Gramian
NT-Xent loss [3]. In this section, we will introduce several
inversely correlates with the average control energy
innovative methods for data augmentation.
required to reach random target states. It also indicates
The primary purpose of data augmentation is to generate
theoverallcontrollabilityacrossalldirectionswithinthe
new data that is logically consistent while preserving the
state space.
semanticlabels.AsdepictedinFigure1,CGRLincorporates
ii Minimum eigenvalue of W: This metric represents
a contrastive module into the conventional Graph Machine
the worst-case scenario and demonstrates an inverse
Learning (GML) architecture, introducing a contrastive loss
relationshipwiththecontrolenergyrequiredtonavigate
for fine-tuning the models. This process involves contrasting
the network in the least controllable direction.
the original graph with an augmentation graph using posi-
iii Rank of W: The rank of W corresponds to the dimen-
tive–positive and positive–negative pairs. The core idea be-
sion of the controllable subspace.
hindCGRListhattheoriginalgraphshould,tosomeextent,
iv Determinant of W: The metric ld(W) =
(cid:16) (cid:17) be equivalent to the augmentation graph. Therefore, each
(cid:81)
log µ (W) , where µ (W) denotes a non-zero
j j j graphinthedatasetshouldhavepreciselyonecorresponding
eigenvalue of W, provides a volumetric assessment of positive pair (an equivalent graph) in the augmented dataset,
the controllable subspace that can be accessed with while the remaining augmented graphs serve as negative
one unit or less of control energy. pairs. During training, CGRL strives to optimize the sim-
Examples: We illustrate, through examples, that net- ilarities of positive–positive pairs to approach unity and the
work controllability is influenced by both the network’s similarities of positive–negative pairs to approach zero.Since the graph-level embeddings used in our work are
based on control properties, the primary goal is to devise an
augmentation technique that primarily preserves the control
properties of the original graph in the augmented version.
(a) input G (b) Edge deletion
Problem IV.1: Given a graph G = (V,E) and a leader
set V , perform an augmentation T(G) to obtain G′ by
ℓ
perturbing k edges while ensuring that the controllability
properties are preserved.
(c) Edge addition (d) Edge substitution
Inthiscontext,edgeperturbationcanrefertoactionssuch
as edge deletion, edge addition, or edge substitution. Fig.3:Control-basedgraphaugmentationswhereδ =γ =4
While performing augmentation, our primary focus is on for original and augmented graphs.
preserving one of the key features of our CTRL embedding,
which is the rank of controllability denoted as γ(G,V ).
ℓ
However, preserving the exact rank of controllability is a A. Egde Deletion
highly complex task. Consequently, the literature has ex-
We propose using the concept of discerning essential
plored lower bounds on network controllability [7], [16],
edges, referred to as controllability backbone edges that we
[17]. In our edge perturbation techniques, we employ a
introduced in [18], which do not decrease δ(G,V ). We
ℓ
rigorous lower bound based on topological node distances
begin by introducing the concept of the distance-based con-
and introduce algorithms to augment the original network
trollability backbone and subsequently present an algorithm
while preserving this lower bound [7].
for computing an augmented graph with k-perturbed edges
Assuming the presence of m leaders denoted as V ℓ = while utilizing the controllability backbone.
{ℓ ,ℓ ,··· ,ℓ } within a leader-follower network G =
1 2 m
(V,E),wedefinethedistance-to-leader(DL)vectorforeach Definition (ControllabilityBackbone)[18]Foragivengraph
vertex v
i
∈V as follows: G = (V,E) and a set of leaders V ℓ, the controllability
backbone is represented as a subgraph B = (V,E ). In
D i =(cid:2) d(ℓ 1,v i) d(ℓ 2,v i) ··· d(ℓ m,v i) (cid:3)T ∈Zm. this subgraph B, the condition δ(G,V ℓ) ≤ δ(Gˆ,V ℓ).B holds
In this vector, the jth component, denoted as [D ] , repre- for any subgraph Gˆ = (V,Eˆ) where the edge set satisfies
i j E ⊆Eˆ ⊆E.
sents the distance between leader ℓ and vertex v . We then B
j i
proceed to define a sequence of distance-to-leader vectors, In other words, for any subgraph Gˆ = (V,Eˆ) of G that
referred to as a pseudo-monotonically increasing sequence includes the backbone edges E , it ensures that at least the
B
(PMI), as described in [7]. samelevelofcontrollabilityastheoriginalgraphGismain-
tained. Essentially, retaining the backbone edges guarantees
Definition (Pseudo-monotonically Increasing Sequence
that controllability remains unchanged or improves within
(PMI)) A sequence D = [D 1 D 2 ··· D k] of distance-to- any subgraph Gˆ.
leadervectorsisaPMIif,foranyvectorD inthesequence,
i Now, we present Algorithm 1 to perform graph augmen-
there exists a coordinate π(i)∈{1,2,··· ,m} such that
tation by deleting edges from a graph. First, we compute
[D ] <[D ] , ∀j >i. the important edges (of the controllability backbone) that
i π(i) j π(i)
need to be preserved to maintain the minimum bound on
In essence, the PMI property (IV) ensures that for each
controllability. The controllability backbone can be found
vector D in the PMI sequence, there is an index/coordinate
i by using Algorithm 2 of [18]. Then, we calculate a set of
π(i) such that the values of all subsequent vectors at the potential edges in the given graph G that does not include
coordinate π(i) are strictly greater than [D i] π(i). anyedgesfromthebackbonegraphB.Finally,werandomly
The length of the PMI sequence provides a precise select k edges from the set of potential edges and remove
lower bound on the dimension of the controllable subspace themfromtheoriginalgraphGtoobtainanaugmentedgraph
γ(G,V ℓ). This is presented in the subsequent result. G′. If k is larger than the set of potential edges, we delete
Theorem 4.1: [7] If we denote the length of the longest all the edges of the potential edge set.
PMI sequence of DL vectors in a network G = (V,E) Proposition 4.2: Given a graph G = (V,E) and a
with V ℓ leaders as δ(G,V ℓ) or simply δ(G), then we can leader set V ℓ, Algorithm 1 returns an augmented graph
establishtheinequality:δ(G,V ℓ)≤γ(G,V ℓ)Here,γ(G,V ℓ) G′ = (V,E′), where E′ ⊆ E, while ensuring δ(G,V ℓ) ≤
represents the dimension of the controllable subspace. δ(G′,V ).
ℓ
We propose the following three sophisticatedly designed Proof: Let G = (V,E) be a graph with a set of
edge perturbation methods that maintain the lower bound leaders V . The backbone graph B = (V,E ) is defined
ℓ B
δ(G,V )ontherankofcontrollability.Theyareillustratedin as the smallest set of essential edges required to maintain
ℓ
Figure 3. The red vertices represent leaders, the gray dashed the necessary distances within the graph for establishing
edge can be removed, and the blue dashed edge can be the lower bound on controllability, as formally described in
added while ensuring that the bound δ(G,V ) = N = 4 Theorem 4.2 of [18]. The theorem establishes that for any
ℓ f
is maintained for all augmented graphs. edge e present in G but absent in B, the removal of edge eAlgorithm 1 Edge Deletion Algorithm 2 Edge Substitution
Input: G=(V,E), V , k Input: G=(V,E), V , k
ℓ ℓ
Output: G′ =(V,E′), |E|−|E′|=k Output: G′ =(V,E′)
1: Compute the distance-based controllability backbone 1: G¯ =(V,E¯)← Edge Deletion(G,V ℓ,k)
B =(V,E B) for G=(V,E) and V ℓ. 2: Compute the maximal edge set E max for G = (V,E)
2: pot edges ← E \ E B % Set of potential and V ℓ.
edges. 3: pot edges ← E max − E % Set of potential
3: E pot ← randomly selected k edges from pot edges edges.
4: E′ ←E\E pot 4: E pot ← randomly selected k edges from pot edges
5: return G′ =(V,E′) 5: E′ ←E¯∪E′.
6: return G′ =(V,E′)
does not diminish the lower bound on γ(G,V ). Therefore,
ℓ
any edge that is not part of the backbone graph B can be G¯ = (V,E¯), then by Proposition 4.2, δ(G¯,V ) ≤ δ(G,V )
ℓ ℓ
eliminated from G without reducing the lower bound on the i.e. the longest PMI sequence of G is a subsequence of the
rank of the controllability matrix. Furthermore, this property longest PMI sequence of G¯ as the distances between leaders
impliesthattheremovalofanysubsetofthesenon-backbone and nodes in V are exactly the same in G and G¯.
D
edges from G continues to maintain the bound on the rank Now, from Algorithm 1 of [19], we find a maximal set
of the controllability matrix. of edges E for the original graph G = (V,E). This
max
edge set contains all the edges that can be added to G while
B. Egde Addition
preserving the distances between leaders and nodes in V .
D
Building on the concept of recognizing removable edges Hence, any edge added to G from E will maintain the
max
while preserving the distance-based bound δ(G,V ), as pro- lower bound δ(G,V ) for γ(G,V ). Hence, by deleting k
ℓ ℓ ℓ
posed in our previous work [19], we use an augmentation edges that are mutually exclusive from the controllability
methodthatdeterminestheedgesthatcanbeaddedtoagiven backboneandaddingkedgesthatkeepthedistancesbetween
graph G=(V,E) while still maintaining the distance-based leadersandnodesinV thesame,wecansubstitutek edges
D
bound δ(G,V ). We employ the same approach outlined in in G for given V such that δ(G,V )≤δ(G′,V ).
ℓ ℓ ℓ ℓ
[19]toidentifyedgesthatcanbeaddedtoGwithoutdimin- These edge perturbation methods are employed to create
ishing the distance-based bound δ(G,V ). After determining positive pairs. Subsequently, we use the CTRL embeddings
ℓ
such potential edges that can be safely incorporated into to generate graph representations for these pairs and apply
G without reducing the controllability rank γ(G,V ), we the NT Xent loss for unsupervised learning of representa-
ℓ
randomly select k such potential edges and introduce them tions for each graph within the dataset. In the following
into G, resulting in the augmented graph G′ =(V,E′). section,weconductanempiricalassessmentusingreal-world
Proposition 4.3: [19]Ifδ(G,V )servesasalowerbound graph datasets. Our proposed approach is numerically eval-
ℓ
for the dimension of the controllable subspace γ(G,V ) of uated through the task of graph classification and compared
ℓ
a graph G = (V,E) with leaders V ⊂ V, then it also with state-of-the-art methods.
ℓ
servesasalowerboundforγ(G′,V )ofanaugmentedgraph
ℓ
G′ = (V,E′), where E ⊆ E′ and E′ contains the edges
V. NUMERICALEVALUATION
that preserve the distances of DL vectors in the longest PMI A. Benchmark Datasets
sequence of G. Datasets: We conducted experiments on 7 standard graph
classification benchmark datasets, which include MUTAG,
C. Egde Substitution
PTC MR, PROTEINS, and DD, representing bioinformat-
Next, we propose a novel approach that combines the ics datasets, as well as IMDB-BINARY, IMDB-MULTI,
edge deletion and edge addition methods while preserving and COLLAB, representing social network datasets [20].
the size of the edge set |E| of the given graph G = (V,E) The bioinformatics datasets provide descriptions of small
and thebound γ(G,V ℓ). Algorithm2 outlinesthis approach. molecules and chemical compounds. Among the social net-
First, we remove k edges from G to create G¯ = (V,E¯) work datasets, IMDB-BINARY and IMDB-MULTI describe
using Algorithm 1. Then, we introduce k distinct edges actors’ ego-networks, while COLLAB is a scientific collab-
into G¯ using the method described in IV-B, resulting in oration dataset where graphs consist of researchers as nodes
G′ =(V,E′), where |E|=|E′|. and their collaborations as edges. Basic dataset statistics are
The maximal edge set E max can be computed by from provided in Table I.
Algorithm 1 of our previous work [19].
B. The Role of Data Augmentation in Graph CL
Proposition 4.4: Given a graph G=(V,E) and a leader
setV ,Algorithm2yieldsanaugmentedgraphG′ =(V,E′), We evaluate the effectiveness of our proposed framework
ℓ
where |E′|=|E|, ensuring that δ(G,V )≤δ(G′,V ), forgraphclassificationusingtheTUDatasetbenchmark[20].
ℓ ℓ
Proof: Let D be the longest PMI sequence of length We utilize the CL method to unsupervisedly learn represen-
δ(G,V ) in G = (V,E) and V ⊆ V are the nodes whose tations z(.) from CTRL embeddings, followed by the eval-
ℓ D
DL vectors are in D. If we remove k edges from G to form uation of these representations for graph-level classification.TABLE I: Statistics of the datasets. Number of graphs,
of contrastive learning to unearth profound insights between
average number of nodes and edges, range of number of
data samples, leading to significant improvements in graph
vertices, and the number of classes. representation learning. In this comparative study, we’ve
chosen two state-of-the-art contenders, InfoGraph [11] and
Dataset #Graphs avg.|V| avg.|E| Range(|V|) #Classes
MUTAG 188 17.93 19.79 10-28 2 GraphCL [5], which both employ graph neural networks as
PTC 344 14.29 14.69 2-109 2 their foundational architecture.
PROTEINS 1113 39.06 72.82 4-620 2 The results of graph classification are presented in Table
DD 1178 284.32 715.66 30-743 2
III. When compared to the top-performing unsupervised
COLLAB 5000 284.32 715.66 32-492 3
IMDB-B 1000 19.77 96.53 12-136 2 methods, our proposed approach exhibits significant im-
IMDB-M 1500 13.00 65.94 7-89 3 provementsacrossseveraldatasets,includingMUTAG,PTC,
PROTEINS, COLLAB, and IMDB-B, resulting in gains of
This evaluation involves training and testing a linear SVM
6.76%, 6.17%, 0.83%, 13.0%, and 1.6%, respectively. No-
classifier using the acquired representations. We employ
tably, our method consistently outperforms all unsupervised
a 10% label rate and 10-fold cross-validation, conducting
competitors across all datasets except IMDB-M.
experiments over 5 repetitions and reporting the evaluation
In contrast to self-supervised counterparts, our proposed
accuracy as a mean value along with the standard deviation.
method surpasses the current SOTA on MUTAG, PRO-
As a baseline reference, we directly employ the CTRL
TEINS,andCOLLAB,andachievesthesecond-bestaccura-
embeddings for training the SVM classifier. The results for
cies on DD and IMDB-B datasets. Specifically, in MUTAG,
fourdistinctbioinformaticsdatasetsaresummarizedinTable
PROTEINS, and COLLAB, our approach outperforms the
II.EdgedeletionyieldsthebestresultonMUTAGandPTC,
existing standards by margins of 0.90%, 4.64%, and 3.74%,
whileonPROTEINSandDD,edgeadditionandsubstitution
respectively. However, on PROTEINS and IMDB-M, our
providethebestresults,respectively.Wevarythenumberof
method falls short by 0.31% and 1.17%, respectively, com-
edges perturbed from 1 to 3. Our augmentation techniques,
pared to the best self-supervised approaches.
combinedwithcontrastivelearning,consistentlyyieldhigher
In summary, as demonstrated by Table III, our proposed
classification accuracies across all datasets compared to the
method outperforms its competitors on two out of seven
baseline approach.
graph classification datasets by a considerable margin and
achieves top-two accuracy rankings for five out of seven
TABLEII:GraphClassificationaccuracy(%)with10%label
datasets.Fortheremainingtwodatasets,ourmethodachieves
rate. The baseline involves a linear SVM trained directly on
accuracy levels within 2% of the SOTA methods.
CTRL embeddings.
We also perform an evaluation of our proposed CGCL
Method MUTAG PTC PROTEINS DD approach using the edge augmentation method proposed by
Baseline 75.86±11.0 52.85±9.5 58.72±11.9 59.10±13.9 You [5]. We follow an i.i.d. uniform distribution to add/drop
CGCL 79.54±11.0 56.10±8.3 69.97±4.5 63.22±11.1 edges instead of the systematic edge perturbation methods
mentioned in Section IV. We call this approach Random-
CGCL. Table IV presents the results for graph classification
C. Comparison with the State-of-the-art Methods
accuracies for all seven datasets under consideration. It can
The effectiveness of CGCL is assessed in the context of be seen that the accuracy of Random-CGCL is comparable
unsupervisedrepresentationlearning,followingtheapproach to both GraphCL and CGCL. However, CGCL outperforms
outlined in [11], [21]. We closely adhere to the established Random-CGCL on all the datasets. These results suggest
approach within Graph CL for graph classification [5], [11]. that a sophisticated augmentation technique, as employed in
We compare our results with the following kernel-based, CGCL, is essential for effectively leveraging control-based
unsupervised, and self-supervised methods. embeddings in graph contrastive learning.
a) Graph Kernels Methods: Graph kernel-based tech-
niques represent traditional approaches to graph classifica- VI. CONCLUSION
tion.Theyengagedirectlywithgraphdatabycraftingkernel In this work, we introduced Control-based Graph Con-
functionsthatpreservethegraph’sstructuralinformation.For trastive Learning (CGCL), a novel framework for unsu-
the purpose of this comparative analysis, we have chosen pervised graph representation learning that leverages graph
four well-established graph kernel-based methods: Graphlet controllability properties. We employed advanced edge aug-
kernel (GK) [22], Weisfeiler-Lehman sub-tree kernel (WL) mentation methods to create augmented data for contrastive
[23], and deep graph kernels (DGK) [24]. learning while preserving the controllability rank of graphs.
b) Unsupervised Methods: Unsupervised techniques Our extensive experiments on standard graph classification
for graph representation learning leverage sub-graph and benchmarksshowcasedCGCL’seffectiveness,outperforming
nodesimilarityscorestoguidethelearningprocess,allwith- SOTAunsupervisedandself-supervisedmethodsonmultiple
outrelyingonlabelinformation.Inthiscomparativeanalysis, datasets. We also compared CGCL with a random edge
we have chosen three prominent unsupervised benchmarks augmentation approach, underscoring the significance of our
methods: node2vec [25], sub2vec [26], and graph2vec [21]. controllability-driven augmentation strategy. The success of
c) Self-Supervised Methods: Unlike traditional unsu- CGCLsuggeststhatincorporatingdomain-specificstructural
pervisedgraphrepresentationtechniques,theself-supervised knowledge, like controllability, can significantly enhance
approach to graph representation harnesses the capabilities graph representation learning, opening avenues for furtherTABLE III: Comparing classification accuracy on top of graph representations learned from graph kernels, SOTA
representation learning method. The top two results are highlighted by First, Second. The numerical values presented for
comparison are obtained from the respective papers, following the identical experimental configurations.
Methods MUTAG PTC PROTEINS DD COLLAB IMDB-B IMDB-M
Kernel Approaches
GK 81.70±2.1 57.30±1.4 - - 72.80±0.3 65.90±1.0 43.90±0.4
WL 80.63±3.1 56.91±2.8 72.92±0.6 - 78.90±1.9 72.30±3.4 47.00±0.5
DGK 87.44±2.7 60.10±2.6 73.30±0.8 - - 66.96±0.6 44.60±0.5
Unsupervised Approaches
node2vec 72.63±10.2 58.85±8.0 57.48±3.6 - 55.70±0.2 50.20±0.9 36.0±0.7
sub2vec 61.05±15.8 59.99±6.4 53.03±5.6 - 62.10±1.4 55.26±1.5 36.7±0.8
graph2vec 83.15±9.2 60.17±6.9 73.30±2.1 - 59.90±0.0 71.10±0.5 50.40±0.9
Self-Supervised Approaches
InfoGraph 89.01±1.1 61.70±1.4 74.44±0.3 72.85±1.8 70.65±1.1 73.03±0.9 49.70±0.5
GraphCL 86.80±1.3 61.30±2.1 74.39±0.5 78.62±0.4 71.36±1.2 71.14±0.4 48.58±0.7
CGCL 89.91±6.4 66.34±7.9 74.13±2.8 75.33±3.3 75.10±1.8 72.70±4.4 48.53±3.1
TABLE IV: Graph Classification accuracies using different Augmentation methods. The top accuracies are highlighted.
Methods MUTAG PTC PROTEINS DD COLLAB IMDB-B IMDB-M
GraphCL 86.80±1.3 61.30±2.1 74.39±0.5 78.62±0.4 71.36±1.2 71.14±0.4 48.58±0.7
Random-CGCL 87.81±7.4 65.73±6.6 73.23±1.8 75.15±2.9 75.04±1.8 71.40±4.0 47.40±2.7
CGCL 89.91±6.4 66.34±7.9 74.13±2.8 75.33±3.2 75.10±1.8 72.70±4.4 48.53±3.1
research. Overall, CGCL presents a promising approach [14] T.H.Summers,F.L.Cortesi,andJ.Lygeros,“Onsubmodularityand
for applications requiring informative graph representations. controllability in complex dynamical networks,” IEEE Transactions
onControlofNetworkSystems,vol.3,no.1,pp.91–101,2015.
In the future, we aim to refine our graph augmentation
[15] E.Wu-Yan,R.F.Betzel,E.Tang,S.Gu,F.Pasqualetti,andD.S.Bas-
techniques to preserve all relevant control features. sett,“Benchmarkingmeasuresofnetworkcontrollabilityoncanonical
REFERENCES graphmodels,”JournalofNonlinearScience,pp.1–39,2018.
[16] A.ChapmanandM.Mesbahi,“Onstrongstructuralcontrollabilityof
[1] M.Newman,Networks. Oxforduniversitypress,2018. networked systems: A constrained matching approach,” in American
[2] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation ControlConference,2013,pp.6126–6131.
learningonlargegraphs,”Advancesinneuralinformationprocessing [17] N. Monshizadeh, K. Camlibel, and H. Trentelman, “Strong targeted
systems,vol.30,2017. controllabilityofdynamicalnetworks,”in201554thIEEEConference
[3] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with onDecisionandControl(CDC),2015,pp.4782–4787.
contrastivepredictivecoding,”arXivpreprintarXiv:1807.03748,2018. [18] O.U.Ahmad,W.Abbas,andM.Shabbir,“Controllabilitybackbone
[4] T.Chen,S.Kornblith,M.Norouzi,andG.Hinton,“Asimpleframe- innetworks,”toappearinCDC,2023,arXiv:2309.02649.
work for contrastive learning of visual representations,” in Interna- [19] W. Abbas, M. Shabbir, H. Jaleel, and X. Koutsoukos, “Improv-
tionalconferenceonmachinelearning. PMLR,2020,pp.1597–1607. ing network robustness through edge augmentation while preserving
[5] Y.You,T.Chen,Y.Sui,T.Chen,Z.Wang,andY.Shen,“Graphcon- strongstructuralcontrollability,”in2020AmericanControlConference
trastivelearningwithaugmentations,”Advancesinneuralinformation (ACC). IEEE,2020,pp.2544–2549.
processingsystems,vol.33,pp.5812–5823,2020. [20] C. Morris, N. M. Kriege, F. Bause, K. Kersting, P. Mutzel, and
[6] L. Xia, C. Huang, Y. Xu, J. Zhao, D. Yin, and J. Huang, “Hyper- M. Neumann, “Tudataset: A collection of benchmark datasets for
graph contrastive collaborative filtering,” in Proceedings of the 45th learningwithgraphs,”inICML2020WorkshoponGraphRepresen-
InternationalACMSIGIRconferenceonresearchanddevelopmentin tationLearningandBeyond,2020.
informationretrieval,2022,pp.70–79. [21] A. Narayanan, M. Chandramohan, R. Venkatesan, L. Chen, Y. Liu,
[7] A. Yazıcıog˘lu, W. Abbas, and M. Egerstedt, “Graph distances and and S. Jaiswal, “graph2vec: Learning distributed representations of
controllabilityofnetworks,”IEEETransactionsonAutomaticControl, graphs,”arXivpreprintarXiv:1707.05005,2017.
vol.61,no.12,pp.4125–4130,2016. [22] N.Shervashidze,S.Vishwanathan,T.Petri,K.Mehlhorn,andK.Borg-
[8] N. Monshizadeh, S. Zhang, and M. K. Camlibel, “Zero forcing sets wardt, “Efficient graphlet kernels for large graph comparison,” in
and controllability of dynamical systems defined on graphs,” IEEE Artificialintelligenceandstatistics. PMLR,2009,pp.488–495.
TransactionsonAutomaticControl,vol.59,pp.2562–2567,2014. [23] N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn,
[9] M.MesbahiandM.Egerstedt,Graphtheoreticmethodsinmultiagent andK.M.Borgwardt,“Weisfeiler-lehmangraphkernels.”Journalof
networks. PrincetonUniversityPress,2010,vol.33. MachineLearningResearch,vol.12,no.9,2011.
[10] H.Duan,C.Xie,B.Li,andP.Tang,“Self-supervisedcontrastivegraph [24] P.YanardagandS.Vishwanathan,“Deepgraphkernels,”inProceed-
representationwithnodeandgraphaugmentation,”NeuralNetworks, ingsofthe21thACMSIGKDDinternationalconferenceonknowledge
2023. discoveryanddatamining,2015,pp.1365–1374.
[11] F.-Y. Sun, J. Hoffmann, V. Verma, and J. Tang, “Infograph: Unsu- [25] A. Grover and J. Leskovec, “node2vec: Scalable feature learning
pervised and semi-supervised graph-level representation learning via for networks,” in Proceedings of the ACM SIGKDD International
mutualinformationmaximization,”arXivpreprintarXiv:1908.01000, ConferenceonKnowledgeDiscoveryandDataMining,2016.
2019. [26] B. Adhikari, Y. Zhang, N. Ramakrishnan, and B. A. Prakash,
[12] K.HassaniandA.H.Khasahmadi,“Contrastivemulti-viewrepresen- “Sub2vec:Featurelearningforsubgraphs,”inAdvancesinKnowledge
tation learning on graphs,” in International conference on machine DiscoveryandDataMining. Springer,2018,pp.170–182.
learning. PMLR,2020,pp.4116–4126.
[13] F. Pasqualetti, S. Zampieri, and F. Bullo, “Controllability metrics,
limitationsandalgorithmsforcomplexnetworks,”IEEETransactions
onControlofNetworkSystems,vol.1,no.1,pp.40–52,2014.