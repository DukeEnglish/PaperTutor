An Adaptive Dimension Reduction Estimation
Method for High-dimensional Bayesian
Optimization
Shouri Hu∗, Jiawei Li∗, and Zhibo Cai†
March 11, 2024
Abstract: Bayesian optimization (BO) has shown impressive
results in a variety of applications within low-to-moderate di-
mensional Euclidean spaces. However, extending BO to high-
dimensional settings remains a significant challenge. We address
this challenge by proposing a two-step optimization framework.
Initially, we identify the effective dimension reduction (EDR)
subspace for the objective function using the minimum average
variance estimation (MAVE) method. Subsequently, we con-
struct a Gaussian process model within this EDR subspace and
optimize it using the expected improvement criterion. Our al-
gorithm offers the flexibility to operate these steps either con-
currently or in sequence. In the sequential approach, we meticu-
louslybalancetheexploration-exploitationtrade-offbydistribut-
ing the sampling budget between subspace estimation and func-
tion optimization, and the convergence rate of our algorithm
in high-dimensional contexts has been established. Numerical
experiments validate the efficacy of our method in challenging
scenarios.
Keywordsandphrases: Expectedimprovement,high-dimensional
BO, MAVE, simple regret.
∗School of Mathematical Sciences, University of Electronic Science and Technology of
China
†Centerfor Applied Statistics and School of Statistics, Renmin University of China
1
4202
raM
8
]LM.tats[
1v52450.3042:viXra1 Introduction
Bayesian Optimization (BO) is a robust sequential design technique
used for the global optimization of functions that are expensive to eval-
uate. It constructs a probabilistic model for the objective function and
employs an acquisition function to steer the search towards the optimum.
Theacquisition functionis designedtostrikeabalance between exploration,
whichinvolvesgatheringdatafromvariousregionstominimizemodeluncer-
tainty, and exploitation, which concentrates on sampling the most promis-
ing region. BO has been extensively applied in numerous fields, including
hyper-parameter tuning, engineering system optimization and material sci-
ence design (Fukazawa et al., 2019; Letham et al., 2019; Sim et al., 2021;
Torun et al., 2018).
Despite the effectiveness of BO in low-dimensional Euclidean space, its
application to high-dimensional situations remains to beachallenging prob-
lem. As the dimensionality increases, both statistical and computational
difficulties arise. Statistically, constructing an accurate probabilistic model
becomes exponentially more difficult in higher dimensions due to the well-
known ”curse of dimensionality”. Computationally, the cost of optimizing
the acquisition function escalates significantly, hence the task of locating its
global optimum becomes more complex. These issues are particularly crit-
ical given that many real-world problems are inherently high-dimensional
(Bergstra et al., 2013; Binois and Wycoff, 2022).
Several approaches have been proposed to address the high-dimensional
challenge in Bayesian optimization. One prevalent approach assumes that
objective function can be predicted by a low-dimensional function, and it
projects high-dimensional inputs into a lower-dimensional subspace using
randomprojectionmatrices,asdiscussedinWang et al.(2016),Nayebi et al.
(2019) and Letham et al. (2020). However, random projections often fail to
accurately identify the true subspace where the objective function resides,
limiting their effectiveness. Another common approach assumes the that
the objective function has an additive structure, meaning the value of the
objective function is the sum of several lower-dimensional functions. Meth-
ods employing this assumption include those in Kandasamy et al. (2015)
and Li et al. (2016). This approach could be effective if the decomposition
of objective function is known in advance, but this is seldom the case in
practice, which can limit the practical application of this approach.
In this paper, we leverage the assumption that the true objective func-
tion resides in a low-dimensional effective subspace, and this subspace can
be estimated via linear dimension reduction. Linear dimension reduction
2is a technique employed in machine learning and statistics to reduce the
dimensionality of data while preserving as much information as possible.
This is often necessary when dealing with high-dimensional data, as it can
help to mitigate issues such as over-fitting and the curse of dimensionality.
Common methods of linear dimension reduction include the principal com-
ponentanalysis, sliced inverse regression (Li ,1991;Lin et al.,2019), among
others. These techniques have been applied in the field of high-dimensional
BO and have demonstrated competitive performances, see Djolonga et al.
(2013), Zhang et al. (2019), and Raponi et al. (2020).
In our proposed algorithm, we employ the concept of minimum average
variance estimation (MAVE) asproposedinXia et al.(2002)toperformlin-
ear dimension reduction. MAVE aims to identify projection directions that
minimize the conditional variance of the linear projection model, thereby
simplifying the high-dimensional space. Once the directions are identified,
high-dimensional input vectors are projected into the low-dimensional sub-
space, and Bayesian optimization is subsequently performed. We refer to
this method as the MAVE-BO algorithm. In high-dimensional Bayesian op-
timization, estimating projection directions is of crucial importance. It has
beshownthatMAVEcanachieve afasterconsistencyrate, andatheoretical
guarantee has been established for the MAVE-BO algorithm.
Although the optimization process is more tractable and computation-
ally efficient in the low dimensional space, the objective function evaluation
remains in the original high-dimensional space. Given that the input vector
typically has a box-constraint, when a new point in the subspace is trans-
formed back into high-dimensional space using a projection matrix, it may
exceed this constraint. To address this issue, we propose an alternating
projection algorithm. The idea of alternating projection is simple, analyti-
callytractableandeasytoimplement. Numericalstudieshavedemonstrated
competitive performance of the MAVE-BO algorithm.
The layout of the paper is as follows. In Section 2 we outlines the set-up
forthelineardimensionreductionframework andtheBayesian optimization
problem. InSection3weprovidesareviewofthehigh-dimensionalBayesian
optimization literature. In Section 4 we describe our proposed MAVE-BO
algorithm, and its theoretical properties are presented in Section 5. In Sec-
tion 6 we discuss the potential extensions to enhance the generality and
computational efficiency of the MAVE-BO algorithm. In Section 7 we con-
duct numerical studies.
32 Preliminary background
Let AT denote the transpose of a vector or matrix A and let x :=
2
k k
x2+ +x2 denote the Euclidean norm of vector x= (x ,...,x )T. Let
1 ··· d 1 d
qA = trace(ATA) denote the Frobenius norm of a matrix A and let
F
k k
A denotethis spectralnorm, whichis thelargest singularvalue of matrix
k k2 p
A. Let φ() and Φ() denote the density and cumulative distribution func-
· ·
tions of standard normal distribution respectively. Let a+ denote max(0,a).
Let a b if lim (a /b ) = 1, a = O(b ) if limsup a /b <
n ∼ n n→∞ n n n n n→∞| n n | ∞
and a = Ω(b ) if liminf a /b > 0.
n n n→∞ n n
| |
Considerthesequentialoptimization ofanunknownfunctionf : R,
X →
where RD is a compact and convex domain. At each iteration n, the
X ⊂
algorithm selects one point x to evaluate, and it receives back a value
n
y = f(x ). After N total iterations, we are interested in minimizing the
n n
simple regret
r = f(x∗) max f(x ), (2.1)
N n
−1≤n≤N
where x∗ = argmaxx∈X f(x) denots the global maximum location.
2.1 General linear dimension reduction framework
A general linear dimension reduction framework assumes that there ex-
ists an unknown smooth link function g such that x ,
∀ ∈X
f(x)= g(BTx), (2.2)
where B = (β ,β ,...,β ) is a D d orthogonal matrix (BTB = I ) with
1 2 de
×
e de
d < D, and g : Rde R is a s function for s > 1. When (2.2)
e
Z ⊂ → C
holds, the projection of the D-dimensional vector x onto the d -dimensional
e
subspace BTx captures all the information that is provided by x on f. The
orthonormality condition on B seems to be very restrictive at a first glance.
However, for general matrix B of rank d , if consider its singular value
e
decomposition BT = UΣVT, then we can write
f(x)= g(BTx)= g˜(B˜Tx), (2.3)
where g˜(z) = g(UΣz), B˜T = VT and B˜TB˜ = I . Hence we can always
de
assume that BTB = I . Under some mild conditions, the space spanned
de
by the column vectors of B is uniquely defined, and it is called the effective
dimension reduction (EDR) space (Cook, 1994; Li , 1991; Xia et al., 2002).
4Our main interest is to estimate thecolumn vectors in B = (β ,β ,...,β ),
1 2 de
which are referred to as EDR direction, and the corresponding dimension of
the EDR space d .
e
As highlighted in Fornasier et al. (2012) and Letham et al. (2019), given
thattheoptimizationprocessoccurswithintheeffectivedimensionreduction
(EDR) space, a potential challenge arises in identifying the corresponding
input vectors in the original high-dimensional space . When a vector in
X
the EDR space is transformed back into , it may exceed the imposed box
X
constraints. This transformation step is inevitable as the objective function
evaluation remains in . Therefore to avoid complications, we will first
X
focus on objective functions that are defined in the Euclidean ball, that is
we consider = RD(1+ǫ¯)and = Rde(1+ǫ¯). Sincethenormof avector
X B Z B
is not changed by multiplication by an orthogonal matrix B, this ensures
that Bz for any z . The extension of to general box constraint
∈ X ∈ Z X
will be discussed in Section 6.
2.2 The Bayesian optimization and the Gaussian process
model
Bayesian optimization (BO) is a sequential design framework for the
global optimization of black-box functions whose input vector resides in
low- to moderate-dimensional Euclidean space. It builds a probabilistic
model for the objective function which quantifies the uncertainty in that
model using a Bayesian machine learning technique called Gaussian process
regression, which is defined as follows. A random function g is said to follow
a Gaussian process (GP) with mean function µ : R and covariance
Z →
kernel: k : R, denoted by g (µ,k), if and only if the following
Z×Z → ∼ GP
condition holds. For every finite set of points Z = (z ,...,z )T, the vector
1 n
Y = (g(z ),...,g(z ))T follows multivariate normal distribution. That is
1 n
Y (µ,Σ)
n
∼ N
withmeanvectorµ = (µ(z ),...,µ(z ))T andcovariancematrix(Σ ) =
1 n ij 1≤i,j≤n
k(z ,z ). See Williams and Rasmussen (2006) and Kanagawa et al. (2018)
i j
for a more complete overview. One of the most exciting and well-known
results of the GP is that the posterior distribution of g, conditioning on
the sampled data, is still a Gaussian process. Let Z = (z ,...,z )T and
1 n
Y = (g(z ),...,g(z ))T denote the sampled points and corresponding func-
1 n
tion values up to iteration n. We have
g (Z,Y) (µ ,k ), (2.4)
n n
| ∼ GP
5with µ : R and k : R given as
n n
X → X ×X →
µ n(z) = µ(z)+kzZΣ−1(Y µ), (2.5)
−
k n(z,z′) = k(z,z′) kzZΣ−1kZz′, (2.6)
−
where kzZ = k ZT
z
= (k(z 1,z),...,k(z n,z)). From (2.4), it can be deduced
that thepredictive distributionatany pointfollows aGaussian distribution.
That is for any z ,
∈Z
g(z)(X,Y) N(µ (z),σ2(z)), with σ2(z) = k (z,z). (2.7)
| ∼ n n n n
The variance component in (2.7) typically represents the spatial character-
istics of the data. Sets of locations with high variance are not near any
existing sampled points. Conversely, when the posterior variance is close to
zero, it means that this location is very near to an existing sampled point.
The mean function and the covariance kernel serve as a prior for the
GP model, and they reflect the initial belief about g. The mean function is
usually set as constant, indicating that there is no prior knowledge of the
global maxima location. The choice of covariance kernel is more varied. We
introduceheretwocommonlyusedkernelfunctions,thesquared-exponential
(SE) and the Mat´ern. Let θ := (θ ,...,θ )T with θ > 0 for all 1 i d.
1 d i
≤ ≤
Define the squared exponential (SE) kernel
d
k SE(z,z′ )=τ2exp
−
1 2X(zi θ− izi′ )2 (cid:1), (2.8)
i=1
(cid:0)
and the Mat´ern kernel
k Mat´ern(z,z′ )= 2ν−τ 1Γ2
(ν)
√2νXd (zi θ− izi′ )2 (cid:1)νB ν(cid:0)√2νXd (zi θ− izi′ )2 (cid:1), (2.9)
i=1 i=1
(cid:0)
whereτ2 > 0isthesignalvarianceofthefunctionf,ν > 0isthesmoothness
parameter,andB isthemodifiedBesselfunction. θ,τ2andν arethehyper-
ν
parametersofGPmodel,andtheirestimationcanbecarriedoutbystandard
methods in the literature, such as the maximum likelihood method (MLE)
(Santner et al., 2018) and the maximum a posterior (MAP) (Ng and Yin,
2012).
Once the GP model has been built, BO employs the acquisition function
toguidehowthedomainshouldbeexploredduringoptimization. Theacqui-
sitionfunctioncombinestheposteriormeanandvarianceof(2.4)intoacrite-
rion that will directthe search. Commonly used acquisition functions in BO
include the expected improvement which quantifies the expected gain over
the current best observation (Hu et al., 2022; Jones, Schonlau, and Welch,
61998; Nguyen et al., 2017), the upper confidence bound which calculates the
upperconfidenceboundofeachpointinthedomain(Chowdhury and Gopalan,
2017; Srinivas et al., 2010), the knowledge gradient which measures the in-
crement to posterior mean function (Frazier, Powell, and Dayanik, 2009;
Wu and Frazier,2016),andtheentropy search whichselectsthepointthatis
mostinformativeaboutthelocationoftheglobaloptimum(Hennig and Schuler,
2012;Hern´andez-Lobato, Hoffman and Ghahramani,2014;Wang and Jegelka,
2017). Theacquisition function balances the exploration-exploitation trade-
off by consideringboth the uncertainty of the GP modelpredictions and the
potential improvement of the objective function. At each iteration, a point
with the largest acquisition function value will be sampled. Thereafter, the
GPmodelandtheacquisition functionwillbeupdatedaccordingly basedon
the new observation. This continues until the global maximum is founded.
Despite BOhas beensuccessfully appliedinmany applications andcanhan-
dlefunctionswithmanylocaloptima,itsperformanceisextremelybadwhen
the input dimension is high. Therefore, the extension of current methods
in BO to higher-dimensional spaces holds great interest for both researchers
and practitioners.
3 Literature review on high-dimensional BO algo-
rithms
Therearemainlytwostrategiesforaddressinghigh-dimensionalBayesian
optimization (BO). Thefirststrategy employs agenerallinear dimensionre-
ductionframework,identicaltotheonepresentedinequation(2.2). Thesec-
ondstrategy,ontheotherhand,adoptsanadditivemodelinlow-dimensional
spaces for the objective function.
In the context of the linear dimension reduction framework method, the
primary challenge lies in obtaining the projection matrix B. To tackle the
challenge, Wang et al. (2016) initially employed the idea of random projec-
tion, leading to the development of the Random Embedding Bayesian Opti-
mization (REMBO) algorithm. In REMBO, the random matrix B is drawn
with each entry following an i.i.d standard normal distribution. The idea
of random projection was subsequently developed by Nayebi et al. (2019),
where each row of B contains only one non-zero entry, which is equally
likely to be 1 or 1. Letham et al. (2020) proposed sampling each row of B
−
from a unit hypersphere, and introduced a complex kernel called the Maha-
lanobis kernel to construct the GP model more accurately within the true
subspace. In addition to random projection, statistical dimension reduction
7methods can also beutilized to estimate B. Djolonga et al. (2013) proposed
the subspace identification Bayesian optimization (SI-BO) algorithm. This
algorithm estimates the gradient matrix [ f(x ),..., f(x )] via low-rank
1 n
∇ ∇
approximation techniques, and then derives B through the singular value
decomposition. Other notable contributions include the SIR-BO algorithm
by Zhang et al. (2019), which performs estimation via the sliced inverse re-
gression, and the PCA-BO algorithm by Raponi et al. (2020), which utilizes
principal component analysis to recover the dimension reduction matrix.
Within the framework of the additive model, Kandasamy et al. (2015)
assumedthattheobjectivefunctionf hasanadditiveform: f(x)= M f(i)(x(i)).
i=1
Here, each x(i) di are lower-dimensional components and mutually
∈ R P
disjoint. By decomposing x into several low-dimensional vectors, Gaus-
sian process models can be conveniently constructed on each subspace.
Li et al. (2016) further extended this concept to a projected-additive struc-
ture: f(x) = M f(i) (W(i))Tx . To estimate W = (W(1),...,W(M)),
i=1
they employed the algorithm proposed by Gilboa et al. (2013) to provide an
initial estimateP Wˆ . Sub(cid:0) sequently,(cid:1) they proposed the restricted projection
matrix W = (1 α)Wˆ +αIwith0 < α < 1todealwiththebox constraints.
−
Beyond these two primary strategies, other dimensionality reduction
methods include the Dropout algorithm (Li et al. (2017)), which randomly
selectsandoptimizesasubsetofdimensionsateachiteration. Eriksson et al.
(2019) introduced TurBO algorithm, which enhances BO by conducting si-
multaneous localoptimization runswithinsmalltrustregions. Each ofthese
regionshasindependentprobabilisticmodels,andtheyaremanageddynam-
ically and effectively using a multi-armed bandit method.
4 Methodology
Recall the linear dimension reduction framework:
f(x)= g(BTx), (4.1)
where x RD and B is a D d orthogonal projection matrix. Due to the
e
∈ ×
curseofdimensionality, estimatingB accuratelybecomescrucialinthehigh-
dimensionalBOproblem. Supposethereisanestimator Bˆ ofsizeD dwith
×
BˆTBˆ = I , and define the corresponding smoothing link function (which is
d
the GP model in our setting) gˆ(= gˆ ) : (1+ǫ¯) R as
Bˆ BRd
→
gˆ(z) := f(Bˆz) = g(BTBˆz).
8Therefore, we effectively work with an approximation fˆto f given by
fˆ(x) := gˆ(BˆTx) = g(BTBˆBˆTx).
Provided d d and the space spanned by the column vectors of Bˆ is well
e
aligned with≥ the EDR space, fˆ will be a good approximation of the true
objective function f.
4.1 MAVE-BO
The high-dimensional BO is mainly comprised of two tasks: estimating
the EDR space and searching the global optimum of the objective function
viatheGaussianprocessmodel. OurproposedMAVE-BOalgorithmhasthe
flexibility to either execute them concurrently or sequentially, which is an
advantage over other algorithms for high-dimensional BO in the literature.
We shall first describe how the EDR direction can be adaptively esti-
mated. With a slight abuse of notation, let BT = (β ,...,β ) denote a
1 d
D d orthogonal matrix. Consider the regression-type model:
×
y = g (BTx)+ǫ, (4.2)
B
with g (z) = g (z ,...,z ) and E(ǫ x) = 0 almost surely. Based on the
B B 1 d
|
linear dimension reduction framework, the EDR direction is the solution of
minE [y E(y BTx)]2 = minE σ2(BTx) , (4.3)
B { − | } B { B }
whereσ2(BTx) = E [y E(y BTx)]2 BTx . Therefore,findingtheEDRdi-
B { − | | }
rection is essentially minimizing the average (conditional) variance (MAVE)
with respect to B, subjection to the condition that BTB = I .
d
Conditioning on BTx, we have E(y BTx) = g (BTx). Its local linear
B
|
expansion at x is given by
0
E(y BTx) a+bTBT(x x ) (4.4)
0
| ≈ −
with a = g (BTx ) and b = g (BTx ), the gradient of g computed at
B 0 B 0 B
∇
z = BTx . Given the sampled points (x ,y ),...,(x ,y )) , σ2(BTx )
0 { 1 1 n n } B 0
can be approximated by (using the idea of local linear regression)
n n
[y E(y BTx)]2w [y a+bTBT(x x ) ]2w , (4.5)
i i0 i i 0 i0
− | ≈ −{ − }
i=1 i=1
X X
9where w 0 are some weights with n w = 1. The weights are
i0 ≥ i=0 i0
calculated as
P
n
w = K BT (x x ) K BT (x x )
i0 h i 0 h l 0
− −
l=1
(cid:8) (cid:9)(cid:14)X (cid:8) (cid:9)
where K (z) = 3h−d(1
kzk2
) is the Epanechnikov function with a band-
h 4 − h2
width h.
Since a and b are unknown quantities, they can be estimated so that the
value of (4.5) is minimized, and hence the estimator of σ2 at BTx is
B 0
n
σˆ2 BTx = min y a+bTBT (x x ) 2 w . (4.6)
B 0 a,b i − i − 0 i0 !
i=1
(cid:0) (cid:1) X(cid:2) (cid:8) (cid:9)(cid:3)
Under some mild conditions, it can be shown that σˆ2 BTx σ2 BTx
B 0 − B 0
converge to zero in probability. On the basis of expressions (4.3) and (4.6),
(cid:0) (cid:1) (cid:0) (cid:1)
we can estimate the EDR directions by solving the minimization problem
n
min σˆ2 BTx (4.7)
B:BTB=I B j 
Xj=1
(cid:0)
(cid:1)
n n
= min  y  a +bTBT (x x ) 2 w ,
B:BTB=I  i − j j i − j ij 
aj,bj,j=1,...,n Xj=1 Xi=1 (cid:2) (cid:8) (cid:9)(cid:3)
 
where bT = (b ,...,b ). The MAVE method can be seen as a combination
j j1 jd
of non-parametric functionestimation anddirection estimation, which isex-
ecuted simultaneously withrespecttothedirections andthenon-parametric
link function. As we shall see, we benefit from this simultaneous minimiza-
tion.
The implementation of the minimization problem in (4.7) is non-trivial
because the weights depend on B. We consider an iterative method that
alternates between estimating the projection matrix B and computing the
weights w . Suppose we have an initial estimator Bˆ, the estimators of
ij
weights can be computed as
n
wˆ = K BˆT (x x ) K BˆT (x x ) .
ij h i j h l j
− −
n o (cid:14)Xl=1 n o
The weights estimator is then used to derive the new estimator of the pro-
jection matrix. That is, we substitute wˆ back into the (4.7), with w
ij ij
10replaced by wˆ , and re-estimate B. The above iteration is repeated until
ij
the estimator Bˆ converges. The complete implementation of the MAVE is
given in A.
Algorithm 1 The sequential MAVE-BO (sMAVE-BO) algorithm
Require: N,N , (µ,k).
0
GP
1: Sample N 0 points uniformly at random from RD and receive
X ⊂
(x ,y ) .
{
i i }1≤i≤N0
2: Estimate the projection matrix Bˆ according to (4.7).
3: Perform the dimension reduction via z = BˆTx and denote GN0 =
(z ,y ),(z ,y )...,(z ,y ) .
{
1 1 2 2 N0 N0
}
4: for n = N 0,...,N 1 do
−
5: Update the d-dimensional GP posterior model (µ n,k n) using n.
GP G
6: Let z n+1 = argmaxz∈Zα n(z), where α is the desired acquisition
function criterion.
7: Evaluate x n+1 := Bˆz n+1 and observe y n+1 = f(x n+1).
8: Augment the history of data n+1 = n (z n+1,y n+1) .
G G ∪{ }
9: end for
Oncewehave an estimate of theprojection matrix Bˆ, wecan projectthe
sample onto thelow-dimensional domain andreceive (y ,z ),...,(y ,z ) ,
1 1 n n
{ }
where z = BˆTx . The data are used to fit the Gaussian process model on
i i
= (1+ǫ¯). Bayesianoptimizationcanthenbeperformedwithasuitable
Rd
Z B
acquisition function. In this paper, we consider the expected improvement
(EI) acquisition function:
αEI(z) = E [(g(z) y∗)+] (4.8)
n n − n
= (µ (z) y∗)Φ µn(z)−y n∗ +σ (z)φ µn(z)−y n∗ ,
n − n σn(z) n σn(z)
(cid:0) (cid:1) (cid:0) (cid:1)
where y∗ = max y . EI quantifies the expected gain of sampling the
n 1≤m≤n m
point z over the current best function value, and a point that maximizes
(4.8) will be sampled in the next iteration, that is
z = maxαEI(z).
n+1 z∈Z n
Once z is determined, the sampled point at the next iteration is given
n+1
by x = Bˆz and hence y = f(x ).
n+1 n+1 n+1 n+1
We propose here two forms of the MAVE-BO algorithms: the sequential
MAVE-BO (sMAVE-BO) and the concurrent MAVE-BO (cMAVE-BO). In
thesMAVE-BO,thebudgetisdividedintotwo portions. Thefirstportionis
11Algorithm 2 The concurrent MAVE-BO (cMAVE-BO) algorithm
Require: N,N , (µ,k).
0
GP
1: SampleN 0 points uniformlyat random from
X
⊂RD and receive IN0 =
(x ,y ) .
{
i i }1≤i≤N0
2: for n = N 0,...,N 1 do
−
3: Estimate the projection matrix Bˆ
(n)
using (4.7) based on n.
I
4: Perform the dimension reduction via z = Bˆ (T n)x and denote Gn =
(z ,y ),(z ,y )...,(z ,y ) .
1 1 2 2 n n
{ }
5: Update the d-dimensional GP posterior model (µ n,k n) using n.
GP G
6: Let z n+1 = argmaxz∈Zα n(z), where α is the desired acquisition
function criterion.
7: Evaluate x n+1 := Bˆ (n)z n+1 and observe y n+1 = f(x n+1).
8: Augment the history of data n+1 = n (x n+1,y n+1) .
I I ∪{ }
9: end for
usedtoestimatetheEDRdirectionsandoutputsaprojectionmatrixestima-
torBˆ. Thesampledpointisthenprojectedintothelower-dimensionalspace,
andthealgorithmusesthesecondportiontoperformBayesianoptimization.
Theoretical results can be derived, see Section 4 for more details. Whereas
in the cMAVE-BO, the estimated EDR directions are ever-changing. The
newly sampled points by BO have beenadded to thedata for theestimation
of the projection matrix.
5 Theoretical Analysis
To analyze the theoretical properties of the sMAVE-BO algorithm, we
need to choose a smoothness class for the unknown smooth-link function g.
Sinceeach covariance kernelk specifiedintheGPmodelisassociated witha
spaceoffunctions , called theReproducingkernelHilbertSpace(RKHS),
k
H
it is natural to perform theoretical analysis on the class of functions which
belong to . In Section 5.1, we present a brief introduction to the RKHS.
k
H
A complete overview of RKHS can befoundin Berlinet and Thomas-Agnan
(2011). In Section 5.2, we establish a simple regret upper bound for the
sMAVE-BO algorithm. In Section 5.3, we prove our main theorem.
5.1 Reproducing kernel Hilbert Space
Let beanon-emptysetandk(, ) : Rbeasymmetricpositive
Z · · Z×Z →
definite kernel. A Hilbert space ( ) of functions equipped with an inner-
k
H Z
12product , is called a reproducing kernel Hilbert space (RKHS) with
h·
·iHk(Z)
reproducing kernel k if the following conditions are satisfied:
1. z ,k(,z) ( );
k
∀ ∈ Z · ∈ H Z
2. z and g ( ), g(z) = g,k(,z) .
∀ ∈ Z ∀ ∈
Hk
Z h ·
iHk(Z)
Every RKHS defines a reproducing kernel k that is both symmetric and
positive definite. The other direction also holds as shown by the Moore-
Aronszajn theorem, which states that given a positive definite kernel k, we
can construct a unique RKHS with k as its reproducing kernel function.
Hence RKHS and positive definite kernel is one-to-one: for every kernel k,
there exists a unique associated RKHS, and vice versa.
5.2 Simple regret bound
We make the following assumptions.
(A1)Theobjectivefunctionf : = (1+ǫ¯) Rsatisfiesf(x)= g(BTx)
RD
X B →
with unknown projection matrix B D×de and d < D.
e
∈ R
(A2) The smooth link function g belongs to the RKHS associated with
k
H
the covariance kernel k(, ). Denote K(z z′):= k(z,z′), then K is contin-
· · −
uous and integrable.
(A3) The Fourier transform of K
K(ξ):=
e−2πihz,ξiK(z)dz
F Rd
Z
is isotropic and radially non-increasing.
(A4) The smooth link function g is of class C2 on = (1 + ǫ¯) and
Rde
Z B
sup Dβg C for some C > 0. Moreover, g has a fullrank Hessian
|β|≤2k k∞ ≤ 2 2
at 0.
Let Bˆ(:= Bˆ ) RD×d denote the estimated EDR directions based on
n
∈
the sequence x ,f(x ) , where x ’s are sampled uniformly at ran-
i i i=1,...,n i
{ }
dom from = (1 + ǫ¯). Let ∆ = BT(I BˆBˆT) , which is the
X (cid:0) BRD (cid:1) n k D − kF
distance between the space spanned by the column vectors of B and the
space spanned by the column vectors of Bˆ.
Theorem 1. Assume (A1)–(A6). Suppose N is large enough such that
0
∆ < 1 is satisfied and N N 3, then for d= d , the the simple regret
N0
−
0
≥
e
13of sMAVE-BO as described in Algorithm 1 achieves
sup r 2 f fˆ + gˆ(z∗) max gˆ(z )
N ∞ n
f:f(x)=g(BTz)
≤ k − k −N0+1≤n≤N
(cid:16) (cid:17)
kgkH k(Z)≤R
(1+ǫ¯)2C √d∆ + h(R′) [4R′N−1+(1+ǫ¯)(R′+1) C 21/ddN−1/d ],
≤ 2 N0 h(−R′) 1 2 1
p
where R′ = (1 ∆2 )−1/4R, h(x) = xΦ(x)+φ(x) and N = N N 1.
− N0 1 − 0 −
Thediscrepancybetween Bˆ andB hastwo effects onthesimpleregretof
thesMAVE-BOalgorithm. Thefirsteffect isthattheconsideredfˆdisagrees
with the true objective function f, and consequently additional regret is
quantified by f fˆ . The second effect is due to the implementation of
∞
k − k
Bayesian optimization on estimated smooth link function gˆ rather than g.
Bˆ
Fortunately, gˆ still remains in the RKHS associated with kernel k, but its
RKHS norm gˆ increases, and consequently may the regret.
k
kHk(Z)
The next corollary provides an asymptotic result for sMAVE-BO, based
on the lemma from Xia et al. (2002):
Lemma 1. Assume (A1)-(A6). Suppose x are sampled uniformly
i i=1,...,n
{ }
at random from = (1 + ǫ¯). Let Bˆ = (Bˆ(n)) RD×d which are
RD
X B ∈
the estimated EDR directions based on (4.7). Suppose the bandwidth h
∼
n−1/(D+4) as n , provided d d ,
e
→ ∞ ≥
3
−
∆ = O n D+4 logn .
n P
(cid:0) (cid:1)
Corollary 1. Suppose both N and N as N . The simple
0 1
→ ∞ → ∞ → ∞
regret of sMAVE-BO satisfies
3 1
r = O (N− D+4 logN +N− d). (5.1)
N P 0 0 1
If D 3d 4, then the first term of (5.1) dominates the second term.
≥ −
This means that if the effective dimension is relatively small, then most of
the regret is incurred due to the discrepancy between fˆand f.
5.3 Proof of the main theorem
We prefacetheproofof Theorem1with theLemmas 2–4. Theselemmas
are proved in the appendix B.
14Lemma 2. Suppose x are sampled uniformly at random from =
i i=1,...,n
{ } X
RD(1 + ǫ¯) and y
i
= f(x i). Let ∆
n
= BT(I
D
BˆBˆT) F, which is the
B k − k
distance between the space spanned by the column vectors of B and the space
spanned by the column vectors of Bˆ with d d . We have that the function
e
fˆ(x) = gˆ(BˆTx) defined by mean of gˆ(z) :=≥ f(Bˆz) satisfies
f fˆ (1+ǫ¯)C √d∆ . (5.2)
∞ 2 n
k − k ≤
Lemma 3. Let g ( ) with RKHS norm g . Define the function
∈
Hk
Z k
kHk(Z)
gˆ: R as gˆ(z) := g(BTBˆz). If d = d and ∆ := BT(I BˆBˆT) <
e n D F
Z → k − k
1, then
gˆ 2 1 g 2 . (5.3)
k kHk(Z) ≤ √1−∆2k kHk(Z)
n
For any function g : R with RKHS norm not larger than R, define
Z →
the loss suffered in ( ) after n steps by EI acqusition function
k
H Z
L (g, ( ),R) := sup g(z∗) max g(z ) . (5.4)
n k i
H Z
kgkH k(Z)≤R
−1≤i≤n
(cid:0) (cid:1)
Lemma 4. Let π be a prior with length-scale θ = (θ ,...,θ ) and θ > 0.
1 d i
For any R > 0 and n 3,
≥
L (g, ( ),R) h(R) 4R(n 1)−1+(1+ǫ¯)(R+1) C 21/dd(n 1)−1/d ,
n Hk Z ≤ h(−R) − 2 −
(cid:2) p (cid:3)
where h(x) = xΦ(x)+φ(x).
Proof of Theorem 1. Weareinterestedinboundingthesimpleregret
r = f(x∗) max f(x ). (5.5)
N n
−1≤n≤N
Let z∗ := argmaxz∈Z gˆ(z) be the global maximum of gˆ. Since kf −fˆ
k∞
=
supx∈X |f(x) −fˆ(x) |,
f(x∗) fˆ(x∗)+ f fˆ gˆ(z∗)+ f fˆ . (5.6)
∞ ∞
≤ k − k ≤ k − k
Let xˆ∗ = argmax fˆ(x ). Since x = Bz for n > N , we have
N0+1≤n≤N n n n 0
max f(x ) f(xˆ∗) fˆ(xˆ∗) f fˆ = max gˆ(z ) f fˆ .
n ∞ n ∞
1≤n≤N ≥ ≥ −k − k N0+1≤n≤N −k − k
(5.7)
15Substitute (5.6) and (5.7) back into (5.5) gives us
r 2 f fˆ + gˆ(z∗) max gˆ(z ) (5.8)
N ∞ n
≤ k − k −N0+1≤n≤N
(cid:16) (cid:17)
2 f fˆ +L (gˆ, ( ),R′),
∞ n k
≤ k − k H Z
where the last inequality follows from Lemma 3. Substitute Lemmas 2 and
4 into (5.8) gives us Theorem 1.
⊓⊔
6 Extension
So far we only consider the case when the domain is a D-dimensional
X
Euclideanball. SincetheprojectionmatrixBˆ isorthogonal,thisimpliesthat
the estimated EDR space is also an Euclidean ball with d-dimension. In
Z
this situation, for every z , we can easily find a corresponding vector
∈ Z
x which satisfies f(x) = g(z) by letting x = Bˆz. However, in a
∈ X
wide range of real-world applications, the Euclidean ball situation is not
satisfied and this leads to complications. To tackle this issue, we proposed
an alternating projection method for general convex sets. Specifically, we
demonstrate below how this method can be applied to the box constraint
domain, which is the most common situation in BO.
Given a set M Rm, m N, the distance of a vector a to M is defined
⊆ ∈
as
d(a;M) := inf b a : b M .
{k − k ∈ }
For closed convex sets, animportantconsequence istheprojection property:
Lemma 5. Let M be a nonempty, closed convex subset of Rm. For each
a Rm, there exists a unique b M such that
∈ ∈
b a = d(a;M),
2
k − k
where b is called the projection of a onto M, and is denoted by P (a).
M
Moreover, b = P (a) if and only if
M
a b,w b 0, w M. (6.1)
h − − i ≤ ∀ ∈
The alternating projection algorithm is structured as follows. Suppose
M and N are closed convex sets, and let P and P denote projection on
M N
M and N, respectively. The algorithm starts with any u(0) M, and then
∈
alternately projects onto M and N:
v(k) = P (u(k)), u(k+1) =P (v(k)), k = 0,1,2, .
N M
···
16This process generates a sequence of points u(k) M and v(k) N. As
∈ ∈
shown in Cheney and Goldstein (1959), if M N = , both sequences u(k)
∩ 6 ∅
and v(k) converge to a point u∗ M N.
∈ ∩
Inthecontext of high-dimensionalBO,thedomainoftheobjective func-
tion usually takes the form of = [ 1,1]D. For any z , we aim to find
X − ∈ Z
the corresponding vector xz which satisfies xz , where
∈ X ∩Y
= x RD : BˆTx= z .
Y { ∈ }
Since both and are closed convex sets, the alternating projection algo-
X Y
rithm can be applied to find xz. The details of how the projection should
proceed are demonstrated in Algorithm 3.
Algorithm 3 The Alternation Projection algorithm
Require: z, Bˆ.
1: Let = [ 1,1]D and = x RD : BˆTx = z , where is z the
X − Y { ∈ }
suggested sample point in the lower-dimensional GP model.
2: Construct an initial estimate u(0) = Bˆz .
∈ Y
3: for i = 0,1,2 do
···
4: If u(i) , stop the algorithm and output xz = u(i). Otherwise,
∈ X
consider v(i) = (v(i) ,...,v(i) )T with
1 D
(i)
1, if u < 1,
− j −
v(i) = u(i) , if 1 u(i) 1, (6.2)
j  j − ≤ j ≤
 (i)
 1, if u > 1.
j

5: If v(i) , stop the algorithm and output xz = v(i). Otherwise,
∈ Y
consider
u(i+1) = v(i) Bˆ(BˆTv(i) z).
− −
6: end for
Lemma 6. The alternating projection described in Algorithm 3 satisfies
v(i) = P (u(i)) and u(i+1) = P (v(i)) for i 0.
X Y
≥
177 Simulation Studies
A Implementation of MAVE
B Proof of the supporting lemmas in Section 5.2
Proof of Lemma 2. Since fˆ(x) = gˆ(BˆTx) = g(BTBˆBˆTx), uniformly
over x :
∈ X
f(x) fˆ(x) = g(BTx) g(BTBˆBˆTx) (B.1)
| − | | − |
C √d (BT BTBˆBˆT)x
2 2
≤ k − k
C √d BT(I BˆBˆT) x
2 D F 2
≤ k − k k k
(1+ǫ¯)C √d BT(I BˆBˆT)
2 D F
≤ k − k
= (1+ǫ¯)C √d∆ ,
2 n
where the first inequality of (B.1) follows because g is continuously differ-
entiable and D1g C .
∞ 2
k k ≤ ⊓⊔
Proof of Lemma 3. Let ψ (A) and λ (A) denote the i-th largest
i i
singular value and eigenvalue of the matrix A respectively and let M =
BT BT(BˆBˆT). Since
−
(0 )ψ2(M) = λ (MMT) = λ (I BTBˆBˆTB)
≤ i i i −
= 1 λ (BTBˆBˆTB)= 1 ψ2(BˆTB),
− i − i
we have 0 ψ (BˆTB) 1 for i= 1,...,d.
i
≤ ≤
Consider first the situation when the function domain is on Rd, then
by Lemma 1 of Bull (2011), (Rd) contains all real continuous functions
k
H
g L2(Rd) with finite norm:
∈
g ξ 2
g 2 := |F | dξ. (B.2)
k
kHk(Rd)
K(ξ)
Z F (cid:0) (cid:1)
18Since gˆ(z) = g(BTBˆz), if = Rd, by (B.2),
Z
gˆ(ξ)2
gˆ 2 = |F | dξ (B.3)
k kHk(Rd) K(ξ)
Z F
g (BˆTB)−1ξ 2
= det(BTBˆ)−2 |F | dξ
| | K(ξ)
Z (cid:0) F (cid:1)
ξ=Bˆ =TBζ
det(BTBˆ)−1
|Fg ζ |2
dζ
| | K(BˆTBζ)
Z F (cid:0) (cid:1)
g ζ 2
det(BTBˆ)−1 |F | dζ
≤ | | K(ζ)
Z F (cid:0) (cid:1)
= det(BTBˆ)−1 g 2 .
| | k
kHk(Rd)
wheretheinequalityfollows fromthefactthat K isradiallynon-increasing
F
and
BˆTBζ BˆTB ζ = ψ (BˆTB) ζ ζ .
2 2 2 1 2 2
k k ≤ k k k k k k ≤ k k
If Rd,let ( )bethespaceoffunctionsg = g forsomeg (Rd)
k 0 Z k
Z ⊂ H Z | ∈ H
with norm
g = inf g ,
k
0 kHk(Z)
g|Z=g0k
kHk(Rd)
then by Theorem 1.5 of Aronszajn (1950), g exists and is the unique min-
0
imum norm extension of g. Hence gˆ (z) := g (BTBˆz) agrees with gˆ on
0 0
Z
and (B.3) holds on ( ).
k
H Z
Finally we need to show that det(BTBˆ) 1 ∆2. Check that
| | ≥ − n
p
(∆ :=) BT(I BˆBˆT) = Trace BT(I BˆBˆT)(I BˆBˆT)B
n D F D D
k − k − −
q
= d Tr(cid:0) ace(BTBˆBˆTB) (cid:1)
−
q
= d BTBˆ 2
−k kF
q
= d Σd ψ2(BTBˆ),
− i=1 i
q
we have Σd ψ2(BTBˆ) = d ∆2. Moreover, since 0 < ∆ < 1 and 0
i=1 i − n n ≤
ψ (BTBˆ) 1 for all i, we have
i
≤
d
det(BTBˆ) = ψ (BTBˆ) 1 ∆2. (B.4)
| | i ≥ − n
i=1
Y p
Therefore Lemma 3 follows.
⊓⊔
We state two additional lemmas which are used to prove Lemma 4.
19Lemma 7. For any m N, and sequences z , the inequality
i
∈ { }
1
σ i(z i+1)> (1+ǫ¯) C 2dm− d
p
holds for at most m distinct i.
Proof. For any j i, since µ (z) = E[g(z) ], we have
i i
≤ |G
σ2(z) = E[(g(z) µ (z))2 ] (B.5)
i − i |Gi
= E[(g(z) g(z ))2 (g(z ) µ (z))2 ]
j j i i
− − − |G
E[(g(z) g(z ))2 ]
j i
≤ − |G
= 2(1 K(z z )),
j
− −
wherethelastinequalityfollowsfromthereproducingpropertyoftheRKHS.
Since K is symmetric, K(0) = 0. By (A1) and the multivariate Taylor’s
∇
Theorem, for any z Rd,
∈
K(z) K(0) = K(z) K(0) [ K(0)]Tz C2d z 2. (B.6)
| − | | − − ∇ | ≤ 2 k k
Since K(0) = 1, substitute (B.6) back into (B.5) gives us
σ2(z) C d z z 2. (B.7)
i ≤ 2 k − i k
We next show that most design points z are close to a previous z .
i+1 j
Since = B (1+ǫ¯), itcanbecovered by mballsof radius(1+ǫ¯)√dm−1/d.
Rd
Z
If z lies in a ball containing some earlier point z ,j i, then we may
i+1 j
≤
conclude
2
σ i2(z i+1)
≤
(1+ǫ¯)2C 2d2m− d. (B.8)
Hence as there are m balls, at most m points z can satisfy
i+1
2
σ i2(z i+1)> (1+ǫ¯)2C 2d2m− d.
⊓⊔
(B.9)
Lemma8. Letg ( )with g R. DenoteI (z) = max 0,g(z)
∈
Hk
Z k
kHk(Z)
≤
n
{ −
max g(z ) . Foranyz andanysequences z ,g(z ) , we have
1≤i≤n i i i 1≤i≤n
} ∈ Z { }
max I (z) Rσ (z), h(−R) I (z) αEI(z) I (z)+(R+1)σ (z).
n − n h(R) n ≤ n ≤ n n
n o
where h(z) := zΦ(z)+φ(z).
20Proof. Let ξ = max g(z ), z = µn(z)−ξn and q = g(z)−ξn. By
n 1≤i≤n i n σn(z) n σn(z)
Lemma 6 of Bull (2011), we have
µ (z) g(z)
n
z q = − g R.
| n − n | σ (z) ≤ k kHk(Z) ≤
n
(cid:12) (cid:12)
(cid:12) (cid:12)
Since αEI(z) = σ (z)h(µn(z)(cid:12)−ξn) = σ (z)h(cid:12) (z ), to show the upper bound,
n n σn(z) n n
αEI(z) σ (z)h(q +R)
n ≤ n n
σ (z)h(max 0,q +R)
n n
≤ { }
σ (z)(max 0,q +R+1)
n n
≤ { }
I (z)+(R+1)σ (z),
n n
≤
where the third inequality follows from h(z) z+1 for z 0.
≤ ≥
To show the lower bound, note that
αEI(z) σ (z)z (B.10)
n ≥ n n
(q R)σ (z)
n n
≥ −
= g(z) ξ Rσ (z)
n n
− −
I (z) Rσ (z),
n n
≥ −
where the first inequality follows from h(z) z for all z. Also, supposethat
≥
g(z) ξ 0, we have
n
− ≥
αEI(z) σ (z)h(q R) σ (z)h( R). (B.11)
n ≥ n n − ≥ n −
Combining (B.10) and (B.11) gives us
h( R) h( R)
αEI(z) − I (z) = − I (z), (B.12)
n ≥ h( R)+R n h(R) n
−
where the last equality follows from the fact that h(z) = z + h( z). If
−
g(z) ξ < 0, (B.12) still holds as αEI(z) > 0. Hence Lemma 8 follows.
− n n ⊔⊓
Proof of Lemma 4. Let y∗ = max g(z ). From Lemma7, for any
i 1≤j≤i j
1
sequence z i and m N, the inequality σ i(z i+1)> (1+¯ǫ)√C 2dm− d holds
{ } ∈
at most m times. Furthermore, y∗ y∗ 0, and for g R,
i+1− i ≥ k k ≤
y∗ y∗ maxg(z) y∗ 2R. (B.13)
i+1− i ≤ z − 1 ≤
n
X
21Hence y∗ y∗ > 2Rm−1 holds for at most m times. Since g(z ) y∗
i+1− i i+1 − i ≤
y∗ y∗, we have also g(z ) y∗ > 2Rm−1 at most m times. Therefore,
i+1− i i+1 − i
there is a time i , m i 2m+1, for which both
m m
≤ ≤
1
σ im(z im+1)
≤
(1+ǫ¯) C 2dm− d and g(z im+1) −y i∗
m ≤
2Rm−1
holds. Since y∗ is monotop nically increasing, then for m large and i
i m ≥
2m+1, uniformly over g ( ),
k
∈ H Z
g(z∗) y∗ g(z∗) y∗ (B.14)
− i ≤ − im
h(R)
αEI(z )
≤ h( R) im ∗
−
h(R)
αEI(z )
≤ h( R) im im+1
−
h(R)
I (z )+(R+1)σ (z )
≤ h( R)
im im+1 im im+1
−
h(R) (cid:2) (cid:3) 1
2Rm−1+(1+¯ǫ)(R+1) C 2dm− d .
≤ h( R)
−
(cid:2) p (cid:3)
Thesecondandtheforth inequality of (B.14)follows from Lemma8andthe
third inequality follows from the fact that z is the maximizer of αEI.
im+1 im
Hence Lemma 4 follows by setting m = (n 1)/2.
− ⊓⊔
C Proof of the supporting lemmas in Section 6
Proof of Lemma 5. By definition of d(a;M), there exists b M
k
∈
such that
d(a;M) b a < d(a;M)+ 1.
≤ k k − k2 k
It follows that b is a bounded sequence. Therefore it has a sub-sequence
k
{ }
b which converges to a point b. Since M is closed, b M. Moreover,
{
kl}
∈
b a = d(a;M) by considering the limit of
2
k − k
d(a;M) b a < d(a;M)+ 1.
≤ k kl − k2 kl
To show theuniqueness, assumethe contrary thatthere exists b = b M
1 2
6 ∈
satisfying
b a = b a = d(a;M).
1 2 2 2
k − k k − k
Check that
2 b a 2 = b a 2+ b a 2 = 2 b 1+b 2 a 2 + 1 b b 2.
k 1 − k2 k 1 − k2 k 2 − k2 2 − 2 2k 1 − 2 k2
(cid:13) (cid:13)
22 (cid:13) (cid:13)Since M is convex,
b 1+b
2 M. This gives
2 ∈
b 1+b 2 a 2 = b a 2 1 b b 2 < b a 2 = d(a;M)2.
2 − 2 k 1 − k2− 4k 1 − 2 k2 k 1 − k2
(cid:13) (cid:13)
We g(cid:13)et a contra(cid:13)diction. Hence b
1
= b 2.
Suppose b = P (a). Let w M,λ (0,1). Since M is convex, λw+
M
∈ ∈
(1 λ)b M. Then
− ∈
a b 2 = d(a;M)2 a b λ(w b) 2
k − k2 ≤ k − − − k2
= a b 2 2λ a b,w b +λ2 w b 2.
k − k2− h − − i k − k2
That is 2 a b,w b λ w b 2. Let λ 0, we have a b,w b 0.
h − − i ≤ k − k2 ↓ h − − i≤
Conversely, suppose a b,w b 0, w M. Then
h − − i ≤ ∀ ∈
a w 2 = a b 2+2 a b,b w + b w 2
k − k2 k − k2 h − − i k − k2
a b 2 2 a b,w b a b 2.
≥ k − k2− h − − i ≥ k − k2
Hence a b a w for all w M and b = P (a).
2 2 M
k − k ≤ k − k ∈ ⊔⊓
Proof of Lemma 6. To prove v(i) = P (u(i)) and u(i+1) = P (v(i)), by
X Y
Lemma 5, it suffices to show that
u(i) v(i),w v(i) 0 for all w and (C.1)
h − − i ≤ ∈ X
v(i) u(i+1),w u(i+1) 0 for all w . (C.2)
h − − i≤ ∈ Y
Let w = (w ,...,w ) , we have
1 D
∈ X
D
u(i) v(i),w v(i) = (u(i) v(i) )(w v(i) ). (C.3)
h − − i j − j j − j
j=1
X
By (6.2), for all j,
(i) (i)
(u +1)(w +1), if u < 1,
j j j −
(u(i) v(i) )(w v(i) )= 0, if 1 u(i) 1, (C.4)
j − j j − j  − ≤ j ≤
 (i) (i)
(u 1)(w 1), if u > 1,
j − j − j

which is less than or equalto zero in all three cases. Substitute (C.4) back
into (C.3) gives us (C.1).
23Let w . Since u(i+1) = v(i) Bˆ(BˆTv(i) z), we have
∈Y − −
v(i) u(i+1),w u(i+1)
h − − i
= Bˆ(BˆTv(i) z),w v(i) +Bˆ(BˆTv(i) z)
h − − − i
= (BˆTv(i) z)TBˆT(w v(i) +Bˆ(BˆTv(i) z))
− − −
= (BˆTv(i) z)T(BˆTw BˆTv(i) +BˆTv(i) z)
− − −
= (BˆTv(i) z)T(BˆTw z) = 0,
− −
where the second last line follows from BˆTBˆ = I and the last line follows
d
from BˆTw z = 0. Hence (C.2) follows.
− ⊓⊔
References
Aronszajn, N. (1950). Theory of reproducing kernels. Transactions of the
American Mathematical Society 68, 337–404.
Bergstra, J., Yamins, D., and Cox, D. (2013). Making a science of model
search: Hyperparameter optimization in hundreds of dimensions for vi-
sion architectures. Proceedings of the 30th International Conference on
Machine Learning 28, 115–123.
Berlinet, A. and Thomas-Agnan, C. (2011). Reproducing kernel Hilbert
spaces in probability and statistics. Springer, Boston, MA.
Binois, M. and Wycoff, N. (2022). A survey on high-dimensional Gaussian
process modeling with application to Bayesian optimization. ACM Trans-
actions on Evolutionary Learning and Optimization 2, 1–26.
Bull, A. D. (2011). Convergence rates of efficient global optimization algo-
rithms. Journal of Machine Learning Research 12, 2879–2904.
Brochu, E., Cora, V. M., and De Freitas, N. (2010). A tutorial on
Bayesian optimization of expensive cost functions, with application
to active user modeling and hierarchical reinforcement learning. arXiv
preprint:1012.2599.
Cheney, W. and Goldstein, A. A. (1959). Proximity maps for convex sets.
Proceedings of the American Mathematical Society 10, 448–450.
Chowdhury, S. R. and Gopalan, A. (2017). On kernelized multi-armed ban-
dits. In International Conference on Machine Learning, 844–853.
24Cook, R. D. (1994). On the interpretation of regression plots. Journal of the
American Statistical Association 89, 177–189.
Djolonga, J., Krause, A., and Cevher, V. (2013). High-dimensional gaussian
processbandits.InAdvances in neural information processing systems26,
1–9.
Eriksson, D., Pearce, M., Gardner, J., Turner, R. D., and Poloczek,
M.(2019). ScalableGlobalOptimizationviaLocalBayesian Optimization.
In Proceedings of the 33rd International Conference on Neural Informa-
tion Processing Systems 493, 5496—5507.
Fornasier, M., Schnass, K., and Vybiral, J. (2012). Learning functions of
few arbitrary linear parameters in high dimensions. Foundations of Com-
putational Mathematics 12, 229–262.
Frazier, P., Powell, W., and Dayanik, S. (2009). The knowledge-gradient
policy for correlated normalbeliefs. INFORMS journal on Computing 21,
599–613.
Frazier, P. I. and Wang, J. (2016). Bayesian optimization for materials de-
sign. In Information science for materials discovery and design, 45–75.
Springer, Cham.
Fukazawa, T., Harashima, Y., Hou, Z., and Miyake, T. (2019). Bayesian
optimization of chemical composition: A comprehensive framework and
its application to RFe -type magnet compounds. Physical Review Mate-
12
rials 3, 053807.
Gilboa, E., Saatc¸i, Y., and Cunningham, J. (2013). Scaling Multidimen-
sional Gaussian Processes using Projected Additive Approximations. In
Proceedings of the 30thInternational Conference onMachineLearning28,
454–461.
Ginsbourger, D., Le Riche, R., and Carraro, L. (2007). A multi-points cri-
terion for deterministic parallel global optimization based on kriging. In
International Conference on Nonconvex Programming, Rouen, France.
Hennig,P.andSchuler,C.J.(2012). Entropysearchforinformation-efficient
global optimization. Journal of Machine Learning Research 13, 1809–
1837.
25Hern´andez-Lobato, J. M., Hoffman, M. W., and Ghahramani, Z. (2014).
Predictive entropy search for efficient global optimization of black-box
functions. In Proceedings of the 27th International Conference on Neural
Information Processing Systems 1, 918—926.
Hoang, T. N., Hoang, Q. M., Ouyang, R., and Low, K. H. (2018). Decen-
tralized high-dimensional Bayesian optimization with factor graphs. In
Proceedings of the AAAI Conference on Artificial Intelligence 32, 3231-
3238.
Hu, S.,Wang, H., Dai, Z., Low, B.K.H., andNg, S.H.(2022). Adjustedex-
pected improvement for cumulative regret minimization innoisy Bayesian
optimization. arXiv preprint:2205.04901.
Huang, D., Allen, T. T., Notz, W. I., and Zeng N. (2006). Global optimiza-
tion of stochastic black-box systems via sequential kriging meta-models.
Journal of Global Optimization 34, 441-–466.
Jones, D. R., Schonlau, M., and Welch, W. J. (1998). Efficient global opti-
mization of expensive black-box Functions. Journal of Global Optimiza-
tion 13, 455–492.
Kandasamy, K., Schneider, J. and Poczos, B. (2015). High Dimensional
Bayesian Optimisation and Bandits via Additive Models. In Proceedings
of the 32nd International Conference on Machine Learning PMLR 37,
295–304.
Kanagawa, M.,Hennig,P.,Sejdinovic,D.,andSriperumbudur,B.K.(2018).
Gaussian processes and kernel methods: A review on connections and
equivalences. arXiv preprint:1807.02582.
Kushner, H. J. (1964). A new method of locating the maximum point of
an arbitrary multipeak curve in the presence of noise. Journal of Basic
Engineering 86, 97-–106.
Letham, B., Calandra, R., Rai, A., and Bakshy, E. (2020). Re-examining
Linear Embeddings for High-dimensional Bayesian Optimization. In Ad-
vances in Neural Information Processing Systems 33, 1546–1558.
Letham, B., Karrer, B., Ottoni, G., and Bakshy, E. (2019). Constrained
Bayesian optimization with noisy experiments. Bayesian Analysis 14,
495–519.
26Li, K. C. (1991). Sliced inverse regression for dimension reduction. Journal
of the American Statistical Association 86, 316–327.
Li, C., Kandasamy, K., Poczos, B. and Schneider, J. (2016). High Dimen-
sionalBayesianOptimizationviaRestrictedProjectionPursuitModels.In
Proceedings of the 19th International Conference on Artificial Intelligence
and Statistics 51,884–892.
Li, C., Gupta, S., Rana, S., Nguyen, V., Venkatesh, S., and Shilton, A.
(2017). High dimensional Bayesian optimization using dropout. In Pro-
ceedings of the 26th International Joint Conference on Artificial Intelli-
gence, 2096–2102.
Lin, Q., Zhao, Z., and Liu, J. S. (2019). SparseSliced Inverse Regression via
Lasso. Journal of the American Statistical Association 114, 1726–1739.
Mˇockus, J. (1975). On Bayesian methods for seeking the extremum. In Op-
timization techniques IFIP technical conference, 400–404. Springer.
Mˇockus, J. (1989). Bayesian Approach to Global Optimization: Theory and
Applications. Springer Dordrecht, Netherlands.
Mˇockus, J., Tiesis, V., and Zˇilinskas A.(1978). The application of Bayesian
methods for seeking the extremum. Towards Global Optimization 2, 117–
129.
Nayebi, A., Munteanu, A., and Poloczek, M. (2019). A framework for
Bayesian optimization in Embedded Subspaces. In International Confer-
ence on Machine Learning, 4752–4761. PMLR.
Negoescu, D. M., Frazier, P. I., and Powell, W. B. (2011). The knowledge-
gradient algorithm for sequencing experiments in drug discovery. IN-
FORMS Journal on Computing 23, 346–363.
Nguyen, V., Gupta, S., Rana, S., Li, C., and Venkatesh, S. (2017). Re-
gret for expected improvement over the best-observed value and stopping
condition. In Proceedings of Machine Learning Research 77, 279–294.
Ng, S. H. and Yin, J. (2012). Bayesian kriging analysis and design for
stochastic simulations. ACM Transactions on Modeling and Computer
Simulation (TOMACS) 22, 1–26.
Packwood,D.(2017).BayesianOptimizationforMaterialsScience.Springer
Singapore.
27Raponi, E., Wang, H., Bujny, M., Boria, S., and Doerr, C. (2020). High Di-
mensionalBayesian Optimization AssistedbyPrincipalComponentAnal-
ysis. In PPSN 2020: Parallel Problem Solving from Nature – PPSN XVI,
169–183.
Ryzhov, I. O. (2016). On the convergence rates of expected improvement
methods. Operations Research 64, 1515–1528.
Sano, S., Kadowaki, T., Tsuda, K., and Kimura, S. (2020). Application of
Bayesian optimization for pharmaceutical product development. Journal
of Pharmaceutical Innovation 15, 333–343.
Santner, T. J., Williams, B. J., and Notz, W. I. (2018). The design and
analysis of computer experiments (Second Edition). Springer Nature.
Scarlett, J., Bogunovic, I., and Cevher, V. (2017). Lower bounds on regret
fornoisyGaussianprocessbanditoptimization.InConferenceonLearning
Theory, 1723–1742.
Sim,R.H.L.,Zhang,Y.,Low,B.K.H.,andJaillet, P.(2021).Collaborative
Bayesian optimization with fair regret. In International Conference on
Machine Learning, 9691–9701.
Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian opti-
mization of machine learning algorithms. Advances in Neural Information
Processing Systems 25.
Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M. (2010). Gaussian
processoptimization inthebanditsetting: noregretandexperimentalde-
sign.In Proceedings of the 27th International Conference on International
Conference on Machine Learning, 1015–1022.
Stuart, A. and Teckentrup, A. (2018). Posterior consistency for Gaussian
process approximations of Bayesian posterior distributions. Mathematics
of Computation 87, 721–753.
Torun, H. M., Swaminathan, M., Davis, A. K., and Bellaredj, M. L. F.
(2018). A global Bayesian optimization algorithm and its application to
integrated system design. IEEE Transactions on Very Large Scale Inte-
gration (VLSI) Systems 26, 792–802.
Vakili, S., Khezeli, K., and Picheny, V. (2021). On information gain and
regret bounds in Gaussian process bandits. In International Conference
on Artificial Intelligence and Statistics, 82–90.
28Valko, M., Korda, N., Munos, R., Flaounas, I., and Cristianini, N.(2013).
Finite-time analysisofkernelisedcontextual bandits.InProceedings of the
Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, 654–
663.
Vazquez, E. and Bect, J. (2010). Convergence properties of the expected
improvementalgorithmwithfixedmeanandcovariancefunctions.Journal
of Statistical Planning and inference 140, 3088–3095.
Wang, J., Clark, S. C., Liu, E., and Frazier, P. I. (2020). Parallel Bayesian
globaloptimization ofexpensivefunctions.Operations Research68, 1850–
1865.
Wang, Z. and De Freitas, N. (2014). Theoretical analysis of Bayesian
optimisation with unknown Gaussian process hyper-parameters. arXiv
preprint:1406.7758.
Wang, Z., Hutter, F., Zoghi, M., Matheson, D., and De Feitas, N. (2016).
Bayesian Optimization in a Billion Dimensions via Random Embeddings.
Journal of Artificial Intelligence Research 55, 361–387.
Wang, Z. and Jegelka, S. (2017). Max-value entropy search for efficient
Bayesian Optimization. In Proceedings of the 34th International Confer-
ence on Machine Learning 70, 3627–3635.
Wang, Z., Kim, B., and Kaelbling, L. P. (2018). Regret bounds for meta
Bayesian optimization withanunknownGaussianprocessprior.Advances
in Neural Information Processing Systems 31, 10477–10488.
Wendland, H. (2004). Scattered Data Approximation. Cambridge University
Press, Cambridge.
Williams, C. K. and Rasmussen, C. E. (2006). Gaussian Processes for Ma-
chine Learning. Cambridge, MA: MIT press.
Wu, J. and Frazier, P. (2016). The parallel knowledge gradient method for
batch Bayesian optimization. In Advances in Neural Information Process-
ing Systems 29, 3126–3134.
Xia, Y. (2007). A constructive approach to the estimation of dimension
reduction directions. The Annals of Statistics 35, 2654–2690
Xia, Y., Tong, H., Li, W. K., and Zhu, L. X. (2002). An adaptive estima-
tion of dimension reduction space. Journal of the Royal Statistical Society
Series B: Statistical Methodology 64, 363–410.
29Zhang,M.,Li,H.,andSu,S.(2019)HighdimensionalBayesianoptimization
via supervised dimension reduction. In International Joint Conference on
Artificial Intelligence, 4292–4298.
Zˇilinskas, A. (1975) Single-step Bayesian search method for an extremum
of functions of a single variable. Cybernetics and Systems Analysis 11,
160–166.
30