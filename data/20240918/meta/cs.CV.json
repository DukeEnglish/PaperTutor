[
    {
        "title": "Do Pre-trained Vision-Language Models Encode Object States?",
        "authors": "Kaleb NewmanShijie WangYuan ZangDavid HeffrenChen Sun",
        "links": "http://arxiv.org/abs/2409.10488v1",
        "entry_id": "http://arxiv.org/abs/2409.10488v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10488v1",
        "summary": "For a vision-language model (VLM) to understand the physical world, such as\ncause and effect, a first step is to capture the temporal dynamics of the\nvisual world, for example how the physical states of objects evolve over time\n(e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs\npre-trained on web-scale data learn to encode object states, which can be\nextracted with zero-shot text prompts. We curate an object state recognition\ndataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models\ntrained with contrastive and generative objectives. We observe that while these\nstate-of-the-art vision-language models can reliably perform object\nrecognition, they consistently fail to accurately distinguish the objects'\nphysical states. Through extensive experiments, we identify three areas for\nimprovements for VLMs to better encode object states, namely the quality of\nobject localization, the architecture to bind concepts to objects, and the\nobjective to learn discriminative visual and language encoders on object\nstates. Data and code are released.",
        "updated": "2024-09-16 17:22:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10488v1"
    },
    {
        "title": "Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance",
        "authors": "Simone Maurizio La CavaSara ConcasRuben TolosanaRoberto CasulaGiulia OrrùMartin DrahanskyJulian FierrezGian Luca Marcialis",
        "links": "http://arxiv.org/abs/2409.10481v1",
        "entry_id": "http://arxiv.org/abs/2409.10481v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10481v1",
        "summary": "3D face reconstruction (3DFR) algorithms are based on specific assumptions\ntailored to distinct application scenarios. These assumptions limit their use\nwhen acquisition conditions, such as the subject's distance from the camera or\nthe camera's characteristics, are different than expected, as typically happens\nin video surveillance. Additionally, 3DFR algorithms follow various strategies\nto address the reconstruction of a 3D shape from 2D data, such as statistical\nmodel fitting, photometric stereo, or deep learning. In the present study, we\nexplore the application of three 3DFR algorithms representative of the SOTA,\nemploying each one as the template set generator for a face verification\nsystem. The scores provided by each system are combined by score-level fusion.\nWe show that the complementarity induced by different 3DFR algorithms improves\nperformance when tests are conducted at never-seen-before distances from the\ncamera and camera characteristics (cross-distance and cross-camera settings),\nthus encouraging further investigations on multiple 3DFR-based approaches.",
        "updated": "2024-09-16 17:17:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10481v1"
    },
    {
        "title": "SimInversion: A Simple Framework for Inversion-Based Text-to-Image Editing",
        "authors": "Qi QianHaiyang XuMing YanJuhua Hu",
        "links": "http://arxiv.org/abs/2409.10476v1",
        "entry_id": "http://arxiv.org/abs/2409.10476v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10476v1",
        "summary": "Diffusion models demonstrate impressive image generation performance with\ntext guidance. Inspired by the learning process of diffusion, existing images\ncan be edited according to text by DDIM inversion. However, the vanilla DDIM\ninversion is not optimized for classifier-free guidance and the accumulated\nerror will result in the undesired performance. While many algorithms are\ndeveloped to improve the framework of DDIM inversion for editing, in this work,\nwe investigate the approximation error in DDIM inversion and propose to\ndisentangle the guidance scale for the source and target branches to reduce the\nerror while keeping the original framework. Moreover, a better guidance scale\n(i.e., 0.5) than default settings can be derived theoretically. Experiments on\nPIE-Bench show that our proposal can improve the performance of DDIM inversion\ndramatically without sacrificing efficiency.",
        "updated": "2024-09-16 17:10:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10476v1"
    },
    {
        "title": "MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion",
        "authors": "Lehong WuLilang LinJiahang ZhangYiyang MaJiaying Liu",
        "links": "http://arxiv.org/abs/2409.10473v1",
        "entry_id": "http://arxiv.org/abs/2409.10473v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10473v1",
        "summary": "Self-supervised learning has proved effective for skeleton-based human action\nunderstanding. However, previous works either rely on contrastive learning that\nsuffers false negative problems or are based on reconstruction that learns too\nmuch unessential low-level clues, leading to limited representations for\ndownstream tasks. Recently, great advances have been made in generative\nlearning, which is naturally a challenging yet meaningful pretext task to model\nthe general underlying data distributions. However, the representation learning\ncapacity of generative models is under-explored, especially for the skeletons\nwith spacial sparsity and temporal redundancy. To this end, we propose Masked\nConditional Diffusion (MacDiff) as a unified framework for human skeleton\nmodeling. For the first time, we leverage diffusion models as effective\nskeleton representation learners. Specifically, we train a diffusion decoder\nconditioned on the representations extracted by a semantic encoder. Random\nmasking is applied to encoder inputs to introduce a information bottleneck and\nremove redundancy of skeletons. Furthermore, we theoretically demonstrate that\nour generative objective involves the contrastive learning objective which\naligns the masked and noisy views. Meanwhile, it also enforces the\nrepresentation to complement for the noisy view, leading to better\ngeneralization performance. MacDiff achieves state-of-the-art performance on\nrepresentation learning benchmarks while maintaining the competence for\ngenerative tasks. Moreover, we leverage the diffusion model for data\naugmentation, significantly enhancing the fine-tuning performance in scenarios\nwith scarce labeled data. Our project is available at\nhttps://lehongwu.github.io/ECCV24MacDiff/.",
        "updated": "2024-09-16 17:06:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10473v1"
    },
    {
        "title": "Deep-Wide Learning Assistance for Insect Pest Classification",
        "authors": "Toan NguyenHuy NguyenHuy UngHieu UngBinh Nguyen",
        "links": "http://arxiv.org/abs/2409.10445v1",
        "entry_id": "http://arxiv.org/abs/2409.10445v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10445v1",
        "summary": "Accurate insect pest recognition plays a critical role in agriculture. It is\na challenging problem due to the intricate characteristics of insects. In this\npaper, we present DeWi, novel learning assistance for insect pest\nclassification. With a one-stage and alternating training strategy, DeWi\nsimultaneously improves several Convolutional Neural Networks in two\nperspectives: discrimination (by optimizing a triplet margin loss in a\nsupervised training manner) and generalization (via data augmentation). From\nthat, DeWi can learn discriminative and in-depth features of insect pests\n(deep) yet still generalize well to a large number of insect categories (wide).\nExperimental results show that DeWi achieves the highest performances on two\ninsect pest classification benchmarks (76.44\\% accuracy on the IP102 dataset\nand 99.79\\% accuracy on the D0 dataset, respectively). In addition, extensive\nevaluations and ablation studies are conducted to thoroughly investigate our\nDeWi and demonstrate its superiority. Our source code is available at\nhttps://github.com/toannguyen1904/DeWi.",
        "updated": "2024-09-16 16:29:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10445v1"
    }
]