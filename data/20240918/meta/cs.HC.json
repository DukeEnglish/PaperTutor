[
    {
        "title": "Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation",
        "authors": "Minsuk ChangSoohyun LeeAeri ChoHyeon JeonSeokhyeon ParkCindy Xiong BearfieldJinwook Seo",
        "links": "http://arxiv.org/abs/2409.10459v1",
        "entry_id": "http://arxiv.org/abs/2409.10459v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10459v1",
        "summary": "We introduce a novel crowdsourcing method for identifying important areas in\ngraphical images through punch-hole labeling. Traditional methods, such as gaze\ntrackers and mouse-based annotations, which generate continuous data, can be\nimpractical in crowdsourcing scenarios. They require many participants, and the\noutcome data can be noisy. In contrast, our method first segments the graphical\nimage with a grid and drops a portion of the patches (punch holes). Then, we\niteratively ask the labeler to validate each annotation with holes, narrowing\ndown the annotation only having the most important area. This approach aims to\nreduce annotation noise in crowdsourcing by standardizing the annotations while\nenhancing labeling efficiency and reliability. Preliminary findings from\nfundamental charts demonstrate that punch-hole labeling can effectively\npinpoint critical regions. This also highlights its potential for broader\napplication in visualization research, particularly in studying large-scale\nusers' graphical perception. Our future work aims to enhance the algorithm to\nachieve faster labeling speed and prove its utility through large-scale\nexperiments.",
        "updated": "2024-09-16 16:49:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10459v1"
    },
    {
        "title": "Charting EDA: Characterizing Interactive Visualization Use in Computational Notebooks with a Mixed-Methods Formalism",
        "authors": "Dylan WoottonAmy Rae FoxEvan PeckArvind Satyanarayan",
        "links": "http://arxiv.org/abs/2409.10450v1",
        "entry_id": "http://arxiv.org/abs/2409.10450v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10450v1",
        "summary": "Interactive visualizations are powerful tools for Exploratory Data Analysis\n(EDA), but how do they affect the observations analysts make about their data?\nWe conducted a qualitative experiment with 13 professional data scientists\nanalyzing two datasets with Jupyter notebooks, collecting a rich dataset of\ninteraction traces and think-aloud utterances. By qualitatively coding\nparticipant utterances, we introduce a formalism that describes EDA as a\nsequence of analysis states, where each state is comprised of either a\nrepresentation an analyst constructs (e.g., the output of a data frame, an\ninteractive visualization, etc.) or an observation the analyst makes (e.g.,\nabout missing data, the relationship between variables, etc.). By applying our\nformalism to our dataset, we identify that interactive visualizations, on\naverage, lead to earlier and more complex insights about relationships between\ndataset attributes compared to static visualizations. Moreover, by calculating\nmetrics such as revisit count and representational diversity, we uncover that\nsome representations serve more as \"planning aids\" during EDA rather than tools\nstrictly for hypothesis-answering. We show how these measures help identify\nother patterns of analysis behavior, such as the \"80-20 rule\", where a small\nsubset of representations drove the majority of observations. Based on these\nfindings, we offer design guidelines for interactive exploratory analysis\ntooling and reflect on future directions for studying the role that\nvisualizations play in EDA.",
        "updated": "2024-09-16 16:37:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10450v1"
    },
    {
        "title": "KoroT-3E: A Personalized Musical Mnemonics Tool for Enhancing Memory Retention of Complex Computer Science Concepts",
        "authors": "Xiangzhe YuanJiajun WangSiying HuAndrew CheungZhicong Lu",
        "links": "http://arxiv.org/abs/2409.10446v1",
        "entry_id": "http://arxiv.org/abs/2409.10446v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10446v1",
        "summary": "As the demand for computer science (CS) skills grows, mastering foundational\nconcepts is crucial yet challenging for novice learners. To address this\nchallenge, we present KoroT-3E, an AI-based system that creates personalized\nmusical mnemonics to enhance both memory retention and understanding of\nconcepts in CS. KoroT-3E enables users to transform complex concepts into\nmemorable lyrics and compose melodies that suit their musical preferences. We\nconducted semi-structured interviews (n=12) to investigate why novice learners\nfind it challenging to memorize and understand CS concepts. The findings,\ncombined with constructivist learning theory, established our initial design,\nwhich was then refined following consultations with CS education experts. An\nempirical experiment(n=36) showed that those using KoroT-3E (n=18)\nsignificantly outperformed the control group (n=18), with improved memory\nefficiency, increased motivation, and a positive learning experience. These\nfindings demonstrate the effectiveness of integrating multimodal generative AI\ninto CS education to create personalized and interactive learning experiences.",
        "updated": "2024-09-16 16:35:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10446v1"
    },
    {
        "title": "Learnings from a Large-Scale Deployment of an LLM-Powered Expert-in-the-Loop Healthcare Chatbot",
        "authors": "Bhuvan SachdevaPragnya RamjeeGeeta FulariKaushik MuraliMohit Jain",
        "links": "http://arxiv.org/abs/2409.10354v2",
        "entry_id": "http://arxiv.org/abs/2409.10354v2",
        "pdf_url": "http://arxiv.org/pdf/2409.10354v2",
        "summary": "Large Language Models (LLMs) are widely used in healthcare, but limitations\nlike hallucinations, incomplete information, and bias hinder their reliability.\nTo address these, researchers released the Build Your Own expert Bot (BYOeB)\nplatform, enabling developers to create LLM-powered chatbots with integrated\nexpert verification. CataractBot, its first implementation, provides\nexpert-verified responses to cataract surgery questions. A pilot evaluation\nshowed its potential; however the study had a small sample size and was\nprimarily qualitative. In this work, we conducted a large-scale 24-week\ndeployment of CataractBot involving 318 patients and attendants who sent 1,992\nmessages, with 91.71% of responses verified by seven experts. Analysis of\ninteraction logs revealed that medical questions significantly outnumbered\nlogistical ones, hallucinations were negligible, and experts rated 84.52% of\nmedical answers as accurate. As the knowledge base expanded with expert\ncorrections, system performance improved by 19.02%, reducing expert workload.\nThese insights guide the design of future LLM-powered chatbots.",
        "updated": "2024-09-17 03:22:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10354v2"
    },
    {
        "title": "Co-Designing Dynamic Mixed Reality Drill Positioning Widgets: A Collaborative Approach with Dentists in a Realistic Setup",
        "authors": "Mine DastanMichele FiorentinoElias D. WalterChristian DiegritzAntonio E. UvaUlrich EckNassir Navab",
        "links": "http://dx.doi.org/10.1109/TVCG.2024.3456149",
        "entry_id": "http://arxiv.org/abs/2409.10258v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10258v1",
        "summary": "Mixed Reality (MR) is proven in the literature to support precise spatial\ndental drill positioning by superimposing 3D widgets. Despite this, the related\nknowledge about widget's visual design and interactive user feedback is still\nlimited. Therefore, this study is contributed to by co-designed MR drill tool\npositioning widgets with two expert dentists and three MR experts. The results\nof co-design are two static widgets (SWs): a simple entry point, a target axis,\nand two dynamic widgets (DWs), variants of dynamic error visualization with and\nwithout a target axis (DWTA and DWEP). We evaluated the co-designed widgets in\na virtual reality simulation supported by a realistic setup with a tracked\nphantom patient, a virtual magnifying loupe, and a dentist's foot pedal. The\nuser study involved 35 dentists with various backgrounds and years of\nexperience. The findings demonstrated significant results; DWs outperform SWs\nin positional and rotational precision, especially with younger generations and\nsubjects with gaming experiences. The user preference remains for DWs (19)\ninstead of SWs (16). However, findings indicated that the precision positively\ncorrelates with the time trade-off. The post-experience questionnaire\n(NASA-TLX) showed that DWs increase mental and physical demand, effort, and\nfrustration more than SWs. Comparisons between DWEP and DWTA show that the DW's\ncomplexity level influences time, physical and mental demands. The DWs are\nextensible to diverse medical and industrial scenarios that demand precision.",
        "updated": "2024-09-16 13:10:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10258v1"
    }
]