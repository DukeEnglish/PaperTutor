[
    {
        "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
        "authors": "Di LiuMeng ChenBaotong LuHuiqiang JiangZhenhua HanQianxi ZhangQi ChenChengruidong ZhangBailu DingKai ZhangChen ChenFan YangYuqing YangLili Qiu",
        "links": "http://arxiv.org/abs/2409.10516v1",
        "entry_id": "http://arxiv.org/abs/2409.10516v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10516v1",
        "summary": "Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).",
        "updated": "2024-09-16 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10516v1"
    },
    {
        "title": "An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems",
        "authors": "Hitesh TulsianiDavid M. ChanShalini GhoshGarima LalwaniPrabhat PandeyAnkish BansalSri GarimellaAriya RastrowBjörn Hoffmeister",
        "links": "http://arxiv.org/abs/2409.10515v1",
        "entry_id": "http://arxiv.org/abs/2409.10515v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10515v1",
        "summary": "Dialog systems, such as voice assistants, are expected to engage with users\nin complex, evolving conversations. Unfortunately, traditional automatic speech\nrecognition (ASR) systems deployed in such applications are usually trained to\nrecognize each turn independently and lack the ability to adapt to the\nconversational context or incorporate user feedback. In this work, we introduce\na general framework for ASR in dialog systems that can go beyond learning from\nsingle-turn utterances and learn over time how to adapt to both explicit\nsupervision and implicit user feedback present in multi-turn conversations. We\naccomplish that by leveraging advances in student-teacher learning and\ncontext-aware dialog processing, and designing contrastive self-supervision\napproaches with Ohm, a new online hard-negative mining approach. We show that\nleveraging our new framework compared to traditional training leads to relative\nWER reductions of close to 10% in real-world dialog systems, and up to 26% on\npublic synthetic data.",
        "updated": "2024-09-16 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10515v1"
    },
    {
        "title": "DILA: Dictionary Label Attention for Mechanistic Interpretability in High-dimensional Multi-label Medical Coding Prediction",
        "authors": "John WuDavid WuJimeng Sun",
        "links": "http://arxiv.org/abs/2409.10504v1",
        "entry_id": "http://arxiv.org/abs/2409.10504v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10504v1",
        "summary": "Predicting high-dimensional or extreme multilabels, such as in medical\ncoding, requires both accuracy and interpretability. Existing works often rely\non local interpretability methods, failing to provide comprehensive\nexplanations of the overall mechanism behind each label prediction within a\nmultilabel set. We propose a mechanistic interpretability module called\nDIctionary Label Attention (\\method) that disentangles uninterpretable dense\nembeddings into a sparse embedding space, where each nonzero element (a\ndictionary feature) represents a globally learned medical concept. Through\nhuman evaluations, we show that our sparse embeddings are more human\nunderstandable than its dense counterparts by at least 50 percent. Our\nautomated dictionary feature identification pipeline, leveraging large language\nmodels (LLMs), uncovers thousands of learned medical concepts by examining and\nsummarizing the highest activating tokens for each dictionary feature. We\nrepresent the relationships between dictionary features and medical codes\nthrough a sparse interpretable matrix, enhancing the mechanistic and global\nunderstanding of the model's predictions while maintaining competitive\nperformance and scalability without extensive human annotation.",
        "updated": "2024-09-16 17:45:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10504v1"
    },
    {
        "title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles",
        "authors": "Kulin ShahNishanth DikkalaXin WangRina Panigrahy",
        "links": "http://arxiv.org/abs/2409.10502v1",
        "entry_id": "http://arxiv.org/abs/2409.10502v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10502v1",
        "summary": "Causal language modeling using the Transformer architecture has yielded\nremarkable capabilities in Large Language Models (LLMs) over the last few\nyears. However, the extent to which fundamental search and reasoning\ncapabilities emerged within LLMs remains a topic of ongoing debate. In this\nwork, we study if causal language modeling can learn a complex task such as\nsolving Sudoku puzzles. To solve a Sudoku, the model is first required to\nsearch over all empty cells of the puzzle to decide on a cell to fill and then\napply an appropriate strategy to fill the decided cell. Sometimes, the\napplication of a strategy only results in thinning down the possible values in\na cell rather than concluding the exact value of the cell. In such cases,\nmultiple strategies are applied one after the other to fill a single cell. We\nobserve that Transformer models trained on this synthetic task can indeed learn\nto solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly)\nwhen trained on a logical sequence of steps taken by a solver. We find that\ntraining Transformers with the logical sequence of steps is necessary and\nwithout such training, they fail to learn Sudoku. We also extend our analysis\nto Zebra puzzles (known as Einstein puzzles) and show that the model solves\n$92.04 \\%$ of the puzzles fully correctly. In addition, we study the internal\nrepresentations of the trained Transformer and find that through linear\nprobing, we can decode information about the set of possible values in any\ngiven cell from them, pointing to the presence of a strong reasoning engine\nimplicit in the Transformer weights.",
        "updated": "2024-09-16 17:42:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10502v1"
    },
    {
        "title": "Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation",
        "authors": "Noah BuchananSusan GauchQuan Mai",
        "links": "http://arxiv.org/abs/2409.10494v1",
        "entry_id": "http://arxiv.org/abs/2409.10494v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10494v1",
        "summary": "This paper presents a diffusion-based recommender system that incorporates\nclassifier-free guidance. Most current recommender systems provide\nrecommendations using conventional methods such as collaborative or\ncontent-based filtering. Diffusion is a new approach to generative AI that\nimproves on previous generative AI approaches such as Variational Autoencoders\n(VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in\na recommender system that mirrors the sequence users take when browsing and\nrating items. Although a few current recommender systems incorporate diffusion,\nthey do not incorporate classifier-free guidance, a new innovation in diffusion\nmodels as a whole. In this paper, we present a diffusion recommender system\nthat augments the underlying recommender system model for improved performance\nand also incorporates classifier-free guidance. Our findings show improvements\nover state-of-the-art recommender systems for most metrics for several\nrecommendation tasks on a variety of datasets. In particular, our approach\ndemonstrates the potential to provide better recommendations when data is\nsparse.",
        "updated": "2024-09-16 17:27:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10494v1"
    }
]