[
    {
        "title": "An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems",
        "authors": "Hitesh TulsianiDavid M. ChanShalini GhoshGarima LalwaniPrabhat PandeyAnkish BansalSri GarimellaAriya RastrowBjörn Hoffmeister",
        "links": "http://arxiv.org/abs/2409.10515v1",
        "entry_id": "http://arxiv.org/abs/2409.10515v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10515v1",
        "summary": "Dialog systems, such as voice assistants, are expected to engage with users\nin complex, evolving conversations. Unfortunately, traditional automatic speech\nrecognition (ASR) systems deployed in such applications are usually trained to\nrecognize each turn independently and lack the ability to adapt to the\nconversational context or incorporate user feedback. In this work, we introduce\na general framework for ASR in dialog systems that can go beyond learning from\nsingle-turn utterances and learn over time how to adapt to both explicit\nsupervision and implicit user feedback present in multi-turn conversations. We\naccomplish that by leveraging advances in student-teacher learning and\ncontext-aware dialog processing, and designing contrastive self-supervision\napproaches with Ohm, a new online hard-negative mining approach. We show that\nleveraging our new framework compared to traditional training leads to relative\nWER reductions of close to 10% in real-world dialog systems, and up to 26% on\npublic synthetic data.",
        "updated": "2024-09-16 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10515v1"
    },
    {
        "title": "MusicLIME: Explainable Multimodal Music Understanding",
        "authors": "Theodoros SotirouVassilis LyberatosOrfeas Menis MastromichalakisGiorgos Stamou",
        "links": "http://arxiv.org/abs/2409.10496v1",
        "entry_id": "http://arxiv.org/abs/2409.10496v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10496v1",
        "summary": "Multimodal models are critical for music understanding tasks, as they capture\nthe complex interplay between audio and lyrics. However, as these models become\nmore prevalent, the need for explainability grows-understanding how these\nsystems make decisions is vital for ensuring fairness, reducing bias, and\nfostering trust. In this paper, we introduce MusicLIME, a model-agnostic\nfeature importance explanation method designed for multimodal music models.\nUnlike traditional unimodal methods, which analyze each modality separately\nwithout considering the interaction between them, often leading to incomplete\nor misleading explanations, MusicLIME reveals how audio and lyrical features\ninteract and contribute to predictions, providing a holistic view of the\nmodel's decision-making. Additionally, we enhance local explanations by\naggregating them into global explanations, giving users a broader perspective\nof model behavior. Through this work, we contribute to improving the\ninterpretability of multimodal music models, empowering users to make informed\nchoices, and fostering more equitable, fair, and transparent music\nunderstanding systems.",
        "updated": "2024-09-16 17:28:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10496v1"
    },
    {
        "title": "Flash STU: Fast Spectral Transform Units",
        "authors": "Y. Isabel LiuWindsor NguyenYagiz DevreEvan DogariuAnirudha MajumdarElad Hazan",
        "links": "http://arxiv.org/abs/2409.10489v2",
        "entry_id": "http://arxiv.org/abs/2409.10489v2",
        "pdf_url": "http://arxiv.org/pdf/2409.10489v2",
        "summary": "This paper describes an efficient, open source PyTorch implementation of the\nSpectral Transform Unit. We investigate sequence prediction tasks over several\nmodalities including language, robotics, and simulated dynamical systems. We\nfind that for the same parameter count, the STU and its variants outperform the\nTransformer as well as other leading state space models across various\nmodalities.",
        "updated": "2024-09-17 12:01:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10489v2"
    },
    {
        "title": "Do Pre-trained Vision-Language Models Encode Object States?",
        "authors": "Kaleb NewmanShijie WangYuan ZangDavid HeffrenChen Sun",
        "links": "http://arxiv.org/abs/2409.10488v1",
        "entry_id": "http://arxiv.org/abs/2409.10488v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10488v1",
        "summary": "For a vision-language model (VLM) to understand the physical world, such as\ncause and effect, a first step is to capture the temporal dynamics of the\nvisual world, for example how the physical states of objects evolve over time\n(e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs\npre-trained on web-scale data learn to encode object states, which can be\nextracted with zero-shot text prompts. We curate an object state recognition\ndataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models\ntrained with contrastive and generative objectives. We observe that while these\nstate-of-the-art vision-language models can reliably perform object\nrecognition, they consistently fail to accurately distinguish the objects'\nphysical states. Through extensive experiments, we identify three areas for\nimprovements for VLMs to better encode object states, namely the quality of\nobject localization, the architecture to bind concepts to objects, and the\nobjective to learn discriminative visual and language encoders on object\nstates. Data and code are released.",
        "updated": "2024-09-16 17:22:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10488v1"
    },
    {
        "title": "Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance",
        "authors": "Simone Maurizio La CavaSara ConcasRuben TolosanaRoberto CasulaGiulia OrrùMartin DrahanskyJulian FierrezGian Luca Marcialis",
        "links": "http://arxiv.org/abs/2409.10481v1",
        "entry_id": "http://arxiv.org/abs/2409.10481v1",
        "pdf_url": "http://arxiv.org/pdf/2409.10481v1",
        "summary": "3D face reconstruction (3DFR) algorithms are based on specific assumptions\ntailored to distinct application scenarios. These assumptions limit their use\nwhen acquisition conditions, such as the subject's distance from the camera or\nthe camera's characteristics, are different than expected, as typically happens\nin video surveillance. Additionally, 3DFR algorithms follow various strategies\nto address the reconstruction of a 3D shape from 2D data, such as statistical\nmodel fitting, photometric stereo, or deep learning. In the present study, we\nexplore the application of three 3DFR algorithms representative of the SOTA,\nemploying each one as the template set generator for a face verification\nsystem. The scores provided by each system are combined by score-level fusion.\nWe show that the complementarity induced by different 3DFR algorithms improves\nperformance when tests are conducted at never-seen-before distances from the\ncamera and camera characteristics (cross-distance and cross-camera settings),\nthus encouraging further investigations on multiple 3DFR-based approaches.",
        "updated": "2024-09-16 17:17:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.10481v1"
    }
]