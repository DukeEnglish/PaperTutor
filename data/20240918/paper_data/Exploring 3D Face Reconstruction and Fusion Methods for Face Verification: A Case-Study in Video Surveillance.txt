Exploring 3D Face Reconstruction and Fusion
Methods for Face Verification: A Case-Study in
Video Surveillance
Simone Maurizio La Cava1 , Sara Concas1 , Ruben Tolosana2 ,
Roberto Casula1 , Giulia Orrù1 , Martin Drahansky3 ,
Julian Fierrez2 , and Gian Luca Marcialis1
1 University of Cagliari, Cagliari, Italy
{simonem.lac,sara.concas90c,roberto.casula,giulia.orru,marcialis}@unica.it
2 Autonomous University of Madrid, Madrid, Spain
{ruben.tolosana,julian.fierrez}@uam.es
3 Masaryk University, Brno, Czech Republic
drahansky@sci.muni.cz
Abstract. 3Dfacereconstruction(3DFR)algorithmsarebasedonspe-
cific assumptions tailored to distinct application scenarios. These as-
sumptions limit their use when acquisition conditions, such as the sub-
ject’s distance from the camera or the camera’s characteristics, are dif-
ferent than expected, as typically happens in video surveillance. Addi-
tionally, 3DFR algorithms follow various strategies to address the re-
construction of a 3D shape from 2D data, such as statistical model
fitting, photometric stereo, or deep learning. In the present study, we
explore the application of three 3DFR algorithms representative of the
SOTA, employing each one as the template set generator for a face ver-
ification system. The scores provided by each system are combined by
score-level fusion. We show that the complementarity induced by differ-
ent 3DFR algorithms improves performance when tests are conducted
at never-seen-before distances from the camera and camera characteris-
tics(cross-distanceandcross-camerasettings),thusencouragingfurther
investigations on multiple 3DFR-based approaches.
Keywords: 3D face reconstruction · Authentication · Surveillance
1 Introduction
In the last few years, much attention has been paid to the generation of face
synthetic images [10,29,37], in particular, 3D data [35,40]. The acquisition of
3D data has been proven to be robust to adverse factors of uncontrolled en-
vironments, such as unfavorable illumination conditions and non-frontal poses
of the face [1,8,39]. Moreover, high accuracy and efficiency can be achieved
when comparing faces due to the complementary information of shape and tex-
ture [2,19]. However, in comparison to standard 2D images, the acquisition of
such 3D data requires a much more complex enrolment process and expensive
4202
peS
61
]VC.sc[
1v18401.9042:viXra2 S. M. La Cava et al.
hardware [24,30]. This is the main reason why current face recognition tech-
nology is still mostly based on the acquisition of 2D face images, considering
popular and cost-effective 2D acquisition devices.
Approaches based on 3D face reconstruction (3DFR) from 2D images and
videos can be a good solution to overcome the limits of 2D data, combining
the ease of acquiring 2D data with the robustness of 3D facial models [25]. An
example of this can be seen in Figure 1. 3DFR algorithms have proven to be
particularly suitable in video surveillance scenarios, where a face acquired in
unconstrained and different acquisition settings, even with arbitrary pose and
distance, is compared to the identity using a reference image, e.g., mugshot or
identity document [24].
Fig.1: Example of 3D face reconstruction from a single 2D image using EOS [22].
However, it is important to highlight that each 3DFR algorithm is usually
designedforaspecificscenario,suchasrecognitionundercertainenvironmental
andtechnologicalconditions,orevenforapplicationsnotstrictlyrelatedtofacial
recognition [24,30]. Therefore, they are not required to generalize well in other
contexts. Accordingly, the designer "maximizes" specific characteristics, such as
fineorgraineddetailsorevensinglefacialcomponents,usefulforthespecificap-
plication scenario. With this in mind, in the present study, we intend to answer
the following research question: is it possible to improve face recognition perfor-
mance in video surveillance scenarios through the fusion of the complementary
information provided by different 3DFR algorithms?
In order to answer this research question, we propose to jointly use differ-
ent 3DFR algorithms for individually aiding the training of multiple classifiers,
finally adopting a score-level fusion rule to moderate their matching score [7]
before making the final decision in a verification task, i.e., determine whether
the subject identity associated with the reference data is the same represented
intheprobedatabasedontheresultinga posteriori probability.Tosummarize,
the main contributions of the present study are the following: (i) the analy-
sis and exploitation of the complementary information provided by different
3DFR algorithms (EOS, 3DDFA v2, and NextFace), not strictly proposed for
recognitionpurposes,toenhanceavideosurveillancescenariousingtwodistinct
deep-learning systems withsignificantlydifferent architectural complexities; (ii)
the exploration of the effectiveness, advantages, and disadvantages of various
score-level fusion methods to take advantage of the complementary information3DFR and Fusion Methods for Face Verification in Video Surveillance 3
provided by different 3DFR algorithms; and (iii) the assessment of the robust-
ness of the proposed approach in challenging conditions, i.e., when dealing with
data acquired in a different setting, concerning acquisition distance and surveil-
lance camera, from the one considered for the system design.
Therestofthepaperisorganizedasfollows.Section2discussesstate-of-the-
art3Dfacereconstructioninvideosurveillanceandfusionmethodsinfacerecog-
nition. Section 3 describes our proposed approach based on 3DFR algorithms
and fusion methods. Section 4 reports the experimental protocol considered to
investigate the effectiveness in a surveillance scenario. The experimental results
are reported in Section 5. Finally, conclusions are drawn in Section 6.
2 Related Works
2.1 Exploiting 3DFR for Face Recognition
3DFRhasbeenmainlyproposedtoincreasetherobustnessoffacerecognitionto
facesacquiredwithvariousviewangles,asthisisthetypicalacquisitionobserved
in unconstrained scenarios, with non-frontal and looking-down probe faces due
to the non-cooperation of the subjects [20,25]. However, the performance im-
provement between 2D and 3DFR strongly depends on the approaches chosen
for its application to the face recognition task. These are usually divided into
two main categories, namely, model- and view-based approaches [25].
The first one synthesizes frontal faces from the 2D images containing non-
frontal views. The normalized (or "frontalized") faces are then compared to the
frontal ones to determine the subjects’ identity [17]. This category is prone to
produce textural artifacts in the synthesized frontal images [4,18,45]; thus, it is
usually considered in the so-called face identification task rather than for highly
accurate authentication [17].
The second one adapts the 2D images containing frontal faces to non-frontal
ones; in other words, lateral views derive from the 3D template (or model) [17].
Additionally, the 3D facial model can be projected to various poses in the 2D
domain to enhance the representation capability of each subject, considering
them as synthesized templates [27,44]. In general, view-based approaches are
more expensive than model-based ones in terms of computational and storage
costs. Thus, they are usually used in the verification task when high reliability
is required [17].
2.2 3DFR in Video Surveillance Scenarios
Video surveillance scenarios are characterized by faces captured at an exten-
sive range of lighting, pose, and scale due to environmental conditions and the
subject’scooperationlevel.Inotherwords,wemustrelyonuncalibratedimages.
The 3D reconstruction from uncalibrated images is an inherently ill-posed
problem:thefacialgeometry,theposeofthehead,anditstexturemustberecov-
ered from a single picture, leading to an undetermined problem, while different4 S. M. La Cava et al.
3D faces could generate the same 2D image. Therefore, the research community
has proposed that prior knowledge should be included to estimate the actual
3D geometry [30]. This information can be added through three main ways:
photometric stereo, statistical model fitting, and deep learning.
Through the first approach, a 3D template is combined with photometric
stereo methods to estimate the facial surface model. Despite the capability of
acquiringfinedetails,thereconstructionistypicallyperformedthroughmultiple
images, thus further constraining the problem [19,30].
Statistical model fitting consists of adapting a 3D facial model built from a
set of 3D facial scans to the input images. In this context, the most commonly
used statistical model is the 3DMM (3D Morphable Model), which consists of
a shape model and, optionally, an albedo model, separately constructed using
PrincipalComponentAnalysis(PCA)[3].Despiteprovidinggenerallyconvincing
results and lower computational complexity, this approach focuses primarily on
global characteristics rather than fine details [30].
Similarly, the deep learning approach maps 2D to 3D information through a
trained deep neural network. This approach provides more detailed results than
thepreviousoneatthecostofrequiringmore3Dscanstotrainthenetwork[30].
Each of the approaches presented above performs 3DFR from uncalibrated
images, as those typically acquired in video surveillance scenarios, following as-
sumptions to make an intrinsically ill-posed problem into a manageable one
[13,30].Theseassumptionsandrelatedprosandconspointoutanintrinsiccom-
plementarity. Moreover, some studies highlight that it is possible to reconstruct
highlydetailed3Dfacesevenwithasingleimagebycombiningthepriorknowl-
edge of the global facial shape encoded in the 3DMM and refining it through a
photometricoradeeplearningapproach[24].Thisfurthersupportsthehypoth-
esis of the complementarity of the 3DFR methods. Therefore, combining face
recognition systems based on multiple 3DFR algorithms could increase the gen-
eralization capabilities of the systems. To achieve this, we propose fusion tech-
niques to exploit the complementary information provided by different 3DFR
algorithms.
2.3 Ensemble and Fusion Methods for Face Recognition
Ensemble methods and multi-modal or uni-modal fusion approaches are con-
stantly being exploited in many fields of pattern recognition to improve the
ability to generalize and deal with intra-class variations and inter-class simi-
larity [42]. In biometrics, fusion can be performed at various levels, including
sensor, feature, score, and decision levels [31–33,36].
Among others, score-level fusion allows the exploitation of several sources of
informationwithoutincreasingthesystem’scomplexity(asinthecaseofsensor-
level and feature-level fusion) and without relying only on the binary outcome,
as in the case of decision-level fusion. A comprehensive set of previous studies
supports this since the earlier attempts at face recognition [9,28,34,36].3DFR and Fusion Methods for Face Verification in Video Surveillance 5
3 Proposed Method
Figure2providesagraphicalrepresentationoftheproposedmethodtoimprove
the performance of face verification in video surveillance scenarios, i.e., deter-
mine whether the identity in a surveillance image (i.e., probe) matches that
represented in the reference data (i.e., mugshot or template). In the following,
we summarize the main modules.
Fig.2: Proposed method. The synthetic view generation produces a 2D image from
the 3D template (i.e., with various view angles obtained from gallery enlargement
duringthesystem’straining,onlyinfrontalviewduringinference).TheSiameseNeural
Networks(samearchitecture)providecomplementaryinformationastheyareenhanced
through different 3DFR algorithms (EOS, 3DDFA v2, or NextFace). The example
images are from the SCface database [14].
3DFRmethods:Wehavechosenthreetypesofstate-of-the-art3DFRalgo-
rithms, each representative of one or a combination of the previously described
approachesforreconstructingthe3Dtemplatesfromhigh-qualityreferencedata
(i.e., frontal mugshot images):
– EOS [22]: based on 3DMM and proposed for reconstructing 3D faces from
videos and images in time-critical applications (Figure 3, b)4.
– 3DDFA v2 [15]: based on a lightweight network for regressing the 3DMM
parameters and proposed for 3D dense face alignment (Figure 3, c)5.
– NextFace [11]: based on the combination of a statistical 3DMM and a pho-
tometric approach for making 3D reconstruction robust to light conditions
(Figure 3, d)6.
3DFR-basedenhancementstrategy:Wefocusonview-basedapproaches
for enhancing face verification through 3DFR. In particular, we train multiple
4 Original implementation from https://github.com/patrikhuber/eos
5 Original implementation from https://github.com/cleardusk/3DDFA_V2
6 Original implementation from https://github.com/abdallahdib/NextFace6 S. M. La Cava et al.
Fig.3:Examplesofpersonalized3DtemplatesgeneratedfromamugshotintheSCface
database [14] (a), through EOS [22] (b), 3DDFA v2 [15] (c), and NextFace [11] (d).
Algorithm 1 Gallery enlargement from a single personalized 3D template.
Require: 3Dtemplate
Ensure: 2Dviewsofthe3Dtemplate
N ←30 {Maximumabsoluteazimuth}
M ←30 {Maximumabsoluteelevation}
offset←10 {Angleoffset}
el←−M {Initialelevation}
whileel≤M do
az←−N {Initialazimuth}
whileaz≤N do
Project3Dtemplateincurrentazandel
az←az+offset
endwhile
el←el+offset
endwhile
facerecognitionsystemsbyconsideringagalleryenlargementstrategythrougha
single3DFRmethodforeachneuralnetworktomakethesystemrobusttopose
variations (Algorithm 1). Specifically, we use it on each 3D template generated
from training mugshots to project the face in multiple view angles (Figure 4).
Then, we train each neural network with all view representations to aid the
task of learning how to extract useful information for face verification from
non-frontal poses. Concerning the evaluation of each face recognition system at
the inference stage, we only compare the frontal face of each subject with the
corresponding set of probe images. The computational cost introduced by the
galleryenlargementstrategyismainlyoffline,thusrepresentingaminorissuein
many of the application scenarios to which this contribution is intended [24].
Face recognition methods: We consider two state-of-the-art deep learn-
ing networks, XceptionNet [6] and VGG19 [38], with different computational
complexitiestosimulatevariousapplicationscenarios.Inparticular,weselected
a Siamese architecture [41], which has demonstrated accurate results in face
recognition using low-resolution images [26].
In order to determine the a posteriori probability P(match|X,Y) between
the representation obtained from the reconstructed 3D reference model X and
the probe image Y match, we calculate the similarity between such a pair of
images using the Euclidean distance value d between their feature embeddings
(i.e., the output of the Siamese Network) as follows:
1
P(match|X,Y)= (1)
d+13DFR and Fusion Methods for Face Verification in Video Surveillance 7
Fig.4: Example of gallery enlargement from personalized 3D template obtained from
a mugshot in the SCface database [14] using the EOS [22] 3DFR algorithm.
In particular, we estimate the a posteriori probability through the previous
formula to limit the range to ]0,1].
Fusion methods: To the best of our knowledge, the score-level fusion be-
tween face recognition systems enhanced through various 3DFR algorithms is
still missing in the literature. Accordingly, we explore different non-parametric
fusionmethods[36]byapplyingasetofrulestothea posteriori probabilityval-
ues, namely the scores predicted by the single Siamese Neural Networks. Hence,
the final a posteriori probability obtained from the comparison between a refer-
ence and a probe represents a combination of such scores.
Here,weintroduceacommonnotationtoeasetheunderstandingofsuchfu-
sionrules.LetusconsiderthefusionofN classifierscores,whereP (match|X,Y)
i
represents the score of the i-th classifier when comparing X and Y. According
tothisnotation,itispossibletocomputethefusionscoresthroughthefollowing
formulasthatrepresent,inorder,thesimpleaveragebetweenthescoresobtained
from the single recognition systems, their maximum, and their minimum:
N
1 (cid:88)
avg_score= P (match|X,Y) (2)
N i
i=1
max_score=max (P (match|X,Y)) (3)
i i
min_score=min (P (match|X,Y)) (4)
i i
4 Experimental Framework
The description of the experimental framework is divided into three parts. Sec-
tion 4.1 describes the database used in our experimental framework. Section 4.2
explains the experimental protocol considered for the configurations regarding
the face recognition systems. It is important to highlight that, as indicated in
Section 3, no additional data was used to re-train the 3DFR modules’ parame-
ters, nor the one described in Section 4.1. We consider the original versions of
the3DFRalgorithmsavailableinthecorrespondingGitHubrepositories.Finally,
Section 4.3 discusses the analyzed performance metrics.8 S. M. La Cava et al.
4.1 Database
State-of-the-artfacerecognitionsystems,eventheonesinthe2Ddomain,achieve
nearly perfect performance on traditional benchmark databases [12,29]. How-
ever, the most significant advantage of 3DFR algorithms in recognition tasks is
in more challenging scenarios, such as surveillance [25].
Hence,weusetheSCfacedatabase[14],containingbothhigh-qualitymugshot-
likeimagesandlower-qualityRGBandgrayscalesurveillanceimagesof130sub-
jects.Thesurveillanceimageswereacquiredthroughfivedifferentcameramodels
at three varying distances from the subject, ranging from 1 to 4.2 meters. Each
subject was captured once for every possible camera-distance combination. The
observedheadposesaretypicallyfoundinsurveillancefootage,withthecamera
slightly above the subject’s head [5]. Therefore, SCface is considered to analyze
the effect of different quality and resolution cameras on face recognition perfor-
mance and the robustness to different distances [24], thus making this database
suitable for evaluating interoperability across settings using data from specific
camerasoratcertaindistances.Werefertotheseevaluationsas"cross-settings"
experiments, while experiments involving training and testing data from the
same camera and distance are referred to as "intra-settings".
Concretely, for both experimental protocols, we use RGB samples related to
25 identities (i.e., about 20% of the subjects) as the test set, while the samples
associated with the remaining 105 subjects are further divided, using 90% as
training samples and 10% as validation ones. In any experiment concerning the
single camera-distance settings for training and test sets, we perform such divi-
sions randomly to limit the possible bias on performance and introduce a face
detection stage following the setting used in the same database in [43].
4.2 Face Recognition Systems: Setup
All Siamese networks are pre-trained on the LFW database [21]. Then, a fine-
tuning through the training set of the SCFace is done, using a validation set
for early stopping, according to partitions described in Section 4.1. As shown in
Figure 5, the inputs to the networks are the probe image and the representa-
tions obtained from a 3D template of the face related to the claimed identity.
For comparability reasons, all the models have been trained on the images re-
sampled at a resolution of 128×128, on up to 256 epochs with the patience of
5 epochs, batches of 128 triplets, and the Adam optimizer with a learning rate
equal to 0.001. Note that this configuration does not necessarily represent the
best parameters and preprocessing stages; rather, it is a general configuration
that might be further improved in future work.
4.3 Performance Evaluation
After training the individual face recognition systems using information from
the different 3DFR algorithms, we investigate the potential of combining them
through a correlation analysis between the sets of scores obtained using the test3DFR and Fusion Methods for Face Verification in Video Surveillance 9
Fig.5: ExampleoftheproposedfaceverificationsystemusingaSiamesearchitecture,
introducing as input the probe image and the facial representation in a non-frontal
view obtained from a mugshot image through the EOS method [22]. The images are
relatedtoasubjectintheSCfacedatabase[14].EMBreferstothefeatureembeddings
obtainedfromthebackbone(i.e.,VGG19orXceptionNet).Onlyfrontalviewsareused
in the final inference stage.
data in intra-setting experiments. In particular, we evaluate the linear correla-
tion between pairs of face recognition systems composed of the same network
architecturebutenhancedbytwodifferent 3DFRalgorithms.Therefore,wedis-
cussthecomplementaryinformationprovidedbythedifferent3DFRalgorithms
in relation to the obtained Pearson Correlation Coefficient (PCC).
Then, we pursue two sets of experiments to evaluate the effectiveness of the
fusion methods (i.e., the fusion of face recognition systems trained with infor-
mation provided by different 3DFR algorithms) in the RGB domain, namely,
intra-setting and cross-setting. The first one consists of training and testing
the single systems on data acquired by a specific camera and at a fixed distance
fromthesubjects,thereforeevaluatinghowmodelsbehaveundercontrolledcon-
ditions. The cross-setting protocol involves training and testing the systems on
images acquired with different cameras and distances.
In particular, we evaluate the reliability of the examined fusion methods
through metrics commonly used in face recognition. From the matching scores,
we compute the percentage of False Match Rate (FMR), i.e., the rate of non-
matching pairs of samples classified as match, and the percentage of False Non-
Match Rate (FNMR), i.e., the rate of matching pairs of samples that have
been incorrectly classified, at various threshold values. This allows us to obtain
the Receiver Operating Characteristic (ROC) curve as a function of FMR and
1−FNMRand,therefore,theAreaUndertheCurve(AUC).Similarly,fromthe
distributions of the scores related to matching and non-matching identities, we
also assess the effect size through Cohen’s d to further highlight the discrimina-
toryabilityoftheevaluatedsystems.Infact,eveniftwoverificationsystemsre-
portsimilarAUCvalues,Cohen’sdcanalsoprovideinformationaboutthemag-
nitudeofthedifferencesbetweenthescoredistributions.Inadditiontotheseper-
formancemetrics,wealsoincludeforcompletenesstheEqualErrorRate(EER),
commonly used for assessing the performance of a biometric system as the per-
centage of mistakes when the proportions of errors on matching identities and
non-matching ones are balanced. Finally, we include the %FNMR@FMR = 1%10 S. M. La Cava et al.
Fig.6: Pearson correlation coefficient (PCC) between the set of scores obtained in
theintra-settingexperimentsfrompairsoffacerecognitionsystemsbasedonthesame
backbone but enhanced by different 3DFR algorithms.
(%FNMR at FMR equal to 1%) and %FMR@FNMR = 1% (%FMR at FNMR
equal to 1%) to assess the performance under stringent accuracy constraints,
such as high-security and high-usable applications, respectively.
5 Results
This section reports the results obtained through the previously described ex-
perimentalsetup.Section5.1providesananalysisofthecorrelationbetweenthe
single classification models enhanced through different 3DFR methods. Section
5.2describestheoutcomeoftheintra-settinganalysis.Finally,Section5.3shows
the performance obtained through the cross-setting analysis.
5.1 Correlation Analysis
Figure 6 shows that recognition systems based on the VGG19 backbone are
mainly poorly correlated, with the highest correlation obtained from the sys-
tems enhanced through 3DDFA v2 and EOS (0.27). Despite the higher values
observed, a similar trend is visible in systems based on XceptionNet. This was
expectedduetothehighestqualityofthe3Dtemplatesgeneratedby3DDFAv2
and EOS algorithms, as can be seen in Figure 3. Hence, the general weak corre-
lation highlighted by this analysis enforces the hypothesis of potential comple-
mentary information provided by different 3DFR algorithms. However, despite
providing clues on the linear correlation between the scores of pairs of systems,
thePCCneedstobepairedwithananalysisoftheeffectivenessoftheindividual
modelstoobserveifalowcorrelationcouldberelatedtoarelevantdifferencein
the overall performance or mostly to differences in single types of errors, which
could therefore be exploited through a combination between them.
5.2 Intra-Setting Analysis
Table1reportstheaverageperformancevaluesintheintra-settingscenario(i.e.,
fixedcamera-distancesettings)fortheindividual3DFRalgorithms(3DDFAv2,
EOS, and NextFace) and their fusion (Avg, Min, and Max). The best values for3DFR and Fusion Methods for Face Verification in Video Surveillance 11
Table 1: Average results in the intra-settings context. For each metric and backbone
(VGG19 and XceptionNet), we highlight the best result achieved in the individual
system (3DDFA v2, EOS, or NextFace) and the best fusion approach (Avg, Min, or
Max).
VGG19[38] XceptionNet[6]
AUCEERCohen’s %FMRat %FNMRatAUCEERCohen’s %FMRat %FNMRat
Method
[%] [%] d FNMR=1% FMR=1% [%] [%] d FNMR=1% FMR=1%
Baseline 77.27 15.52 0.91 85.37 87.42 71.89 22.77 0.79 81.37 94.57
3DDFAv2[15]81.5112.25 1.09 75.72 88.75 74.9916.33 0.77 86.83 91.85
EOS[22] 82.0612.50 1.21 80.46 90.66 78.1816.33 0.92 86.43 90.68
NextFace[11] 72.58 17.67 0.78 86.45 93.99 71.02 25.83 0.62 90.09 95.58
Fusion(Avg) 86.9410.78 1.53 56.25 80.41 79.9215.67 1.04 71.57 90.19
Fusion(Min) 84.37 13.36 1.02 84.49 81.78 76.52 18.17 0.61 95.05 91.27
Fusion(Max) 74.07 11.39 1.08 68.12 93.03 77.95 17.75 0.99 71.26 94.99
each metric and backbone (VGG19 and XceptionNet) are highlighted in bold
within the single 3DFR-enhanced models and the fusion approach.
We can observe that, in terms of AUC and Cohen’s d, the best-performing
single models are those enhanced through the EOS algorithm. For example,
for the VGG19 backbone, the performance achieved when including informa-
tion from the EOS algorithm is 82.06% AUC, an absolute improvement of
4.79% AUC in comparison with the Baseline system (77.27% AUC), proving
to be 3DFR algorithms a good solution to increase the performance in video
surveillancescenarios.AsimilartrendisobservedforXceptionNet.However,for
some specific operational points, for example, EER, %FMR@FNMR=1%, and
%FNMR@FMR = 1%, and the VGG19 backbone, the information provided by
the 3DDFA v2 algorithm performs better than the EOS algorithm, enforcing
the hypothesis that the fusion of face verification systems trained with different
3DFR algorithms may be beneficial.
Analyzing the proposed fusion (Avg, Min, and Max) of different 3DFR algo-
rithms, we can observe that, in general, the Avg score-level fusion achieves the
bestresults,revealingimprovedperformancebothataglobal(i.e.,AUCandCo-
hen’sd)andlocallevel(i.e.,EER,%FMR@FNMR=1%,and%FNMR@FMR=
1%). For example, for the VGG19 backbone, the Avg score-level fusion achieves
anAUCof86.94%,anabsoluteimprovementof9.67%and4.88%AUCcompared
totheBaselinesystemandthebestindividual3DFRalgorithm,respectively.In
particular, this fusion rule shows improvements in the separation between the
score distributions related to genuine and impostor subjects, also solving the is-
suesrelatedtotheworstperformanceatchallengingthresholdsafterintroducing
the 3DFR algorithm in the pipeline. Furthermore, it is worth noting that this
fusion rule is also able to deal with the variability in performance observed by
the 3DFR-enhanced recognition systems between the different camera-distance
settings. This can be seen in Figure 7. The more stable results across the exper-
iments represent an important feature for the aimed application field [24], and
this is especially achieved with the proposed Avg score-level fusion.
Furthermore, as can be seen in Table 1 and Figure 7, the performance of the
deepneuralnetworkstrainedwithmugshotsorenhancedwithsingle3DFRalgo-
rithmsismorerobustwhenemployingtheVGG19architectureasthebackbone.12 S. M. La Cava et al.
Fig.7: AUC values obtained for the intra-setting scenario for the different camera-
distance configurations using VGG19 [38] (a) and XceptionNet [6] (b) as backbones.
This outcome was expected, considering the significantly higher performance of
this network in face recognition tasks [16,23]. Moreover, the fusion methods re-
lated to such architecture also reveal the greatest improvement with respect to
thesinglesystems.Regardingthedifferentfusionmethodsstudied,theminimum
and the maximum rules are generally less performing, as they inherently tend
to produce more false matches and false non-matches, respectively.
Finally, for completeness, we analyze in Figure 8 the robustness of the pro-
posed systems with an increasing distance of the camera during the acquisi-
tion (i.e., 4.2 meters, 2.6 meters, and 1 meter). As expected, both the systems
trained through single 3DFR algorithms and their fusion through the Avg rule
suffer with an increasing acquisition distance. Similar to our previous analysis,
wecanobservethatthesingle3DFRalgorithmsshowdifferencesinenhancement
capability, revealing that there is not a single best choice for all settings and,
therefore, further highlighting a certain degree of fusion between 3D templates,
as can be seen with the Avg score-level fusion.
5.3 Cross-Setting Analysis
Table 2 shows the results achieved when considering different acquisition dis-
tances (cross-distance setting) and surveillance cameras (cross-camera setting)
between training and test sets. As expected, both single 3DFR-enhanced and
fused systems suffer from a significant decay in performance compared to the
intra-setting scenario, remarking how challenging the task is. For example, for
theVGG19backboneandAvgscore-levelfusion,theAUCdecreasesfrom86.94%
in the intra-setting scenario to 74.55% in the cross-setting scenario. Still, the
Avg fusion rule is confirmed to be able to improve the overall performance, out-
performing the single systems and the Baseline one from both a global and a
localperspective.Anotablefindingisthat,unliketheintra-settingscenario,the3DFR and Fusion Methods for Face Verification in Video Surveillance 13
Fig.8: Summary of AUC values obtained from all the intra-setting camera-distance
configurations (i.e., 4.2 meters, 2.6 meters, and 1 meter) through the system based
on the VGG19 [38] (a) and XceptionNet [6] (b), trained with mugshots, single 3DFR
algorithms, and fusion of them through the Average rule (Avg).
Table 2: Average results in cross-settings context. For each metric and backbone
(VGG19 and XceptionNet), we highlight the best result achieved in the individual
system(3DDFAv2,EOS,NextFace)andthebestfusionapproach(Avg,Min,orMax).
VGG19[38] XceptionNet[6]
AUCEERCohen’s %FMRat %FNMRatAUCEERCohen’s %FMRat %FNMRat
Method
[%] [%] d FNMR=1% FMR=1% [%] [%] d FNMR=1% FMR=1%
Baseline 67.30 36.27 0.55 99.24 98.15 68.97 36.21 0.63 91.81 96.53
3DDFAv2[15]68.84 20.69 0.64 95.23 94.57 70.95 19.70 0.69 95.22 92.53
EOS[22] 70.1319.70 0.63 93.16 93.87 71.2119.09 0.61 96.57 91.58
NextFace[11] 64.91 22.29 0.47 93.71 96.90 68.47 23.46 0.57 95.99 94.97
Fusion(Avg) 74.5519.18 0.86 84.70 90.74 75.7716.44 0.86 83.41 89.84
Fusion(Min) 70.49 20.14 0.58 96.38 91.50 70.70 17.24 0.57 96.67 91.18
Fusion(Max) 68.04 20.57 0.66 88.18 95.66 74.8214.96 0.86 84.13 92.89
XceptionNetbackboneenhancedwiththeconsidered3DFRalgorithmsachieves
bettergeneralizationcapabilityintermsofAUCandEERthantheVGG19one.
Tosummarize,despitetheexpectedperformancedegradationobservedinthe
challengingcross-settingexperiments,theproposedfusionoffaceverificationsys-
tems trained with complementary information from different 3DFR algorithms
demonstrates significant improvement compared to the Baseline and individual
3DFR-enhanced systems. Therefore, the proposed fusion strategy represents a
good solution for video surveillance contexts. The effects of the differences be-
tween the technical characteristics of the surveillance cameras and the impact
of the different acquisition distances should be further investigated in future
research to highlight their impact on system reliability and, therefore, the suit-
ability in specific application scenarios.
6 Conclusions
In this paper, we investigate the complementary information provided by three
state-of-the-art 3D face reconstruction (3DFR) algorithms and the effectiveness
ofscore-levelfusioninleveragingthisinformationtoimprovefacerecognitionin14 S. M. La Cava et al.
a video surveillance scenario. We first assess this complementarity by analyzing
the linear correlation between the scores produced by different face verification
systems,eachutilizingthesameSiameseNeuralNetwork,usingonebetweenthe
two examined backbones, but enhanced by a different 3DFR algorithm. Then,
we also employ three non-parametric fusion methods to combine the individual
systems based on the same backbone.
Therefore,weevaluatetherobustnessoftheproposedapproachusingacom-
mon experimental setup when dealing with data acquired in intra- and cross-
setting,involvingvariationsinacquisitiondistanceandsurveillancecamera.This
allows us to explore different application contexts: the intra-setting experiments
simulate a context in which the designer knows the data characteristics and
has access to representative data for training the recognition system; the cross-
setting experiments simulate a more unfavorable application context in which
data characteristics are unknown or representative data cannot be obtained.
The results obtained from our experiments confirm the initial hypothesis
thatdeep-learningfacerecognitionsystemscanextractdistinctinformationfrom
different state-of-the-art 3DFR algorithms, and the score-level fusion methods
effectively leverage this information to enhance inference robustness. This im-
provement can be observed even on probe images obtained from surveillance
camerasandatdistancesdifferentfromtheonesrelatedtotrainingdata.There-
fore, this approach could represent a valid tool for improving facial recognition
in challenging surveillance scenarios like the one considered in this work.
Future studies should explore other fusion approaches, such as parametric
rules and those based on machine-learning models, which may better exploit
the observed complementarity. Similarly, more 3DFR algorithms, enhancement
approaches, and deep neural networks could be included in the study, and the
impact of the proposed approach on different pre-processing settings should be
investigated. Finally, the effects of the differences in terms of acquisition dis-
tance and surveillance camera between training and test data should be further
investigated.
This preliminary study provides valuable insights into the effectiveness of
fusionmethodsinexploitingthecomplementarityamong3Dfacereconstruction
algorithmswithinvideosurveillancecontexts.Webelievethisinitialexploration
holds promise and could pave the way for developing robust multi-modal face
recognition systems capable of recognizing never-seen-before subjects, thereby
aiding forensic practitioners and security personnel in identifying criminals and
finding missing persons.
Acknowledgment
FundingfromCátedraENIAUAM-VERIDASenIAResponsable(NextGenera-
tionEU PRTR TSI-100927-2023-2) and project BBforTAI (PID2021-127641OB-
I00MICINN/FEDER).TheworkhasbeenconductedwithinthesAIferLaband
the ELLIS Unit Madrid.3DFR and Fusion Methods for Face Verification in Video Surveillance 15
References
1. Abate, A.F., Nappi, M., Riccio, D., Sabatino, G.: 2D and 3D face recognition: A
survey.PatternRecognitionLetters28(14),1885–1906(2007),image:Information
and Control
2. Amor, B.B., Ouji, K., Ardabilian, M., Chen, L.: 3D face recognition by icp-based
shape matching. LIRIS Lab, Lyon Research Center for Images and Intelligent In-
formation Systems, UMR 5205 (2005)
3. Blanz, V., Vetter, T.: A Morphable Model For The Synthesis Of 3D Faces. Asso-
ciation for Computing Machinery, New York, NY, USA, 1 edn. (2023)
4. Cao,J.,Hu,Y.,Zhang,H.,He,R.,Sun,Z.:Towardshighfidelityfacefrontalization
in the wild. International Journal of Computer Vision 128(5), 1485–1504 (2020)
5. Castro, H.F., Cardoso, J.S., Andrade, M.T.: A systematic survey of ml datasets
for prime cv research areas—media and metadata. Data 6(2), 12 (2021)
6. Chollet, F.: Xception: Deep learning with depthwise separable convolutions. In:
ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.
pp. 1251–1258 (2017)
7. Concas, S., Gao, J., Cuccu, C., Orrù, G., Feng, X., Marcialis, G.L., Puglisi, G.,
Roli,F.:Experimentalresultsonmulti-modaldeepfakedetection.In:International
Conference on Image Analysis and Processing. pp. 164–175. Springer (2022)
8. Creusot, C., Pears, N., Austin, J.: A machine-learning approach to keypoint de-
tectionandlandmarkingon3Dmeshes.InternationalJournalofComputerVision
102(1), 146–179 (2013)
9. Dass, S.C., Nandakumar, K., Jain, A.K.: A principled approach to score level
fusion in multimodal biometric systems. In: International Conference on Audio-
andVideo-basedBiometricPersonAuthentication.pp.1049–1058.Springer(2005)
10. DeAndres-Tame,I.,Tolosana,R.,Melzi,P.,Vera-Rodriguez,R.,Kim,M.,Rathgeb,
C.,Liu,X.,Morales,A.,Fierrez,J.,Ortega-Garcia,J.,etal.:SecondEditionFRC-
Syn challenge at CVPR 2024: face recognition challenge in the era of synthetic
data.In:Proc.IEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion (2024)
11. Dib,A.,Bharaj,G.,Ahn,J.,Thébault,C.,Gosselin,P.,Romeo,M.,Chevallier,L.:
Practicalfacereconstructionviadifferentiableraytracing.In:ComputerGraphics
Forum. vol. 40, pp. 153–164. Wiley Online Library (2021)
12. Fuad,M.T.H.,Fime,A.A.,Sikder,D.,Iftee,M.A.R.,Rabbi,J.,Al-Rakhami,M.S.,
Gumaei, A., Sen, O., Fuad, M., Islam, M.N.: Recent advances in deep learning
techniques for face recognition. IEEE Access 9, 99112–99142 (2021)
13. Geng,Z.,Cao,C.,Tulyakov,S.:Towardsphoto-realisticfacialexpressionmanipu-
lation. International Journal of Computer Vision 128, 2744–2761 (2020)
14. Grgic, M., Delac, K., Grgic, S.: Scface–surveillance cameras face database. Multi-
media Tools and Applications 51(3), 863–879 (2011)
15. Guo, J., Zhu, X., Yang, Y., Yang, F., Lei, Z., Li, S.Z.: Towards fast, accurate and
stable 3D dense face alignment. In: Proceedings of the European Conference on
Computer Vision (ECCV) (2020)
16. Gwyn,T.,Roy,K.,Atay,M.:Facerecognitionusingpopulardeepnetarchitectures:
A brief comparative study. Future Internet 13(7), 164 (2021)
17. Han, H., Jain, A.K.: 3D face texture modeling from uncalibrated frontal and pro-
file images. In: 2012 IEEE Fifth International Conference on Biometrics: Theory,
Applications and Systems (BTAS). pp. 223–230. IEEE (2012)16 S. M. La Cava et al.
18. Hassner, T., Harel, S., Paz, E., Enbar, R.: Effective face frontalization in uncon-
strainedimages.In:ProceedingsoftheIEEEConferenceonComputerVisionand
Pttern Recognition. pp. 4295–4304 (2015)
19. Heike,C.L.,Upson,K.,Stuhaug,E.,Weinberg,S.M.:3Ddigitalstereophotogram-
metry: a practical guide to facial image acquisition. Head & Face Medicine 6(1),
1–11 (2010)
20. Hu, X., Peng, S., Wang, L., Yang, Z., Li, Z.: Surveillance video face recognition
withsinglesampleperpersonbasedon3Dmodelingandblurring.Neurocomputing
235, 46–58 (2017)
21. Huang, G.B., Mattar, M., Berg, T., Learned-Miller, E.: Labeled faces in the wild:
Adatabaseforstudyingfacerecognitioninunconstrainedenvironments.In:Work-
shoponFacesin’Real-Life’Images:Detection,Alignment,andRecognition(2008)
22. Huber,P.,Hu,G.,Tena,R.,Mortazavian,P.,Koppen,P.,Christmas,W.J.,Ratsch,
M.,Kittler,J.:Amultiresolution3Dmorphablefacemodelandfittingframework.
In: Proceedings of the 11th International Joint Conference on Computer Vision,
Imaging and Computer Graphics Theory and Applications (2016)
23. Kumar,M.,Mann,R.:Maskedfacerecognitionusingdeeplearningmodel.In:2021
3rdInternationalConferenceonAdvancesinComputing,CommunicationControl
and Networking (ICAC3N). pp. 428–432. IEEE (2021)
24. La Cava, S.M., Orrù, G., Drahansky, M., Marcialis, G.L., Roli, F.: 3D face recon-
struction: the road to forensics. ACM Computing Surveys (CSUR) 56(3), 1–38
(2023)
25. La Cava, S.M., Orrù, G., Goldmann, T., Drahansky, M., Marcialis, G.L.: 3D face
reconstruction for forensic recognition-a survey. In: 2022 26th International Con-
ference on Pattern Recognition (ICPR). pp. 930–937. IEEE (2022)
26. Lai,S.C.,Lam,K.M.:Deepsiamesenetworkforlow-resolutionfacerecognition.In:
2021 Asia-Pacific Signal and Information Processing Association Annual Summit
and Conference (APSIPA ASC). pp. 1444–1449. IEEE (2021)
27. Liang,J.,Liu,F.,Tu,H.,Zhao,Q.,Jain,A.K.:Onmugshot-basedarbitraryview
face recognition. In: 2018 24th International Conference on Pattern Recognition
(ICPR). pp. 3126–3131. IEEE (2018)
28. Marcialis, G.L., Roli, F.: Fusion of appearance-based face recognition algorithms.
Pattern Analysis and Applications 7, 151–163 (2004)
29. Melzi, P., Tolosana, R., Vera-Rodriguez, R., Kim, M., Rathgeb, C., Liu, X.,
DeAndres-Tame, I., Morales, A., Fierrez, J., Ortega-Garcia, J., et al.: FRCSyn-
onGoing: benchmarking and comprehensive evaluation of real and synthetic data
to improve face recognition systems. Information Fusion 107, 102322 (2024)
30. Morales, A., Piella, G., Sukno, F.M.: Survey on 3D face reconstruction from un-
calibrated images. Computer Science Review 40, 100400 (2021)
31. Noore, A., Singh, R., Vasta, M.: Fusion, Sensor-Level, pp. 616–621. Springer US,
Boston, MA (2009)
32. Osadciw, L., Veeramachaneni, K.: Fusion, Decision-Level, pp. 593–597. Springer
US, Boston, MA (2009)
33. Peng,J.,AbdEl-Latif,A.A.,Li,Q.,Niu,X.:Multimodalbiometricauthentication
based on score level fusion of finger biometrics. Optik 125(23), 6891–6897 (2014)
34. Punyani, P., Kumar, A.: Evaluation of fusion at different levels for face recogni-
tion. In: 2017 Int. Conference on Computing, Communication and Automation
(ICCCA). pp. 1052–1055 (May 2017)
35. Richardson, E., Sela, M., Kimmel, R.: 3D face reconstruction by learning from
synthetic data. In: Proc. Fourth International Conference on 3D Vision (2016)3DFR and Fusion Methods for Face Verification in Video Surveillance 17
36. Ross,A.,Nandakumar,K.:Fusion,Score-Level,pp.611–616.SpringerUS,Boston,
MA (2009)
37. Shahreza, H.O., Ecabert, C., George, A., Unnervik, A., Marcel, S., Di Domenico,
N., Borghi, G., Maltoni, D., Boutros, F., Vogel, J., et al.: SDFR: synthetic data
for face recognition competition. In: Proc. IEEE International Conference on Au-
tomatic Face and Gesture Recognition (2024)
38. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)
39. Soltanpour, S., Boufama, B., Wu, Q.J.: A survey of local feature methods for 3D
face recognition. Pattern Recognition 72, 391–406 (2017)
40. Sun,K.,Wu,S.,Huang,Z.,Zhang,N.,Wang,Q.,Li,H.:Controllable3Dfacesyn-
thesis with conditional generative occupancy fields. In: Proc. Advances in Neural
Information Processing Systems (2022)
41. Tolosana, R., Vera-Rodriguez, R., Fierrez, J., Ortega-Garcia, J.: Exploring recur-
rentneuralnetworksforon-linehandwrittensignaturebiometrics.IEEEAccess6,
5128–5138 (2018)
42. Vishi, K., Mavroeidis, V.: An Evaluation of Score Level Fusion Approaches for
Fingerprint and Finger-vein Biometrics (2018)
43. Yang, F., Yang, W., Gao, R., Liao, Q.: Discriminative multidimensional scaling
forlow-resolutionfacerecognition.IEEESignalProcessingLetters25(3),388–392
(2017)
44. Zhang,X.,Gao,Y.,Leung,M.K.:Recognizingrotatedfacesfromfrontalandside
views:Anapproachtowardeffectiveuseofmugshotdatabases.IEEETransactions
on Information Forensics and Security 3(4), 684–697 (2008)
45. Zhou,H.,Liu,J.,Liu,Z.,Liu,Y.,Wang,X.:Rotate-and-render:Unsupervisedpho-
torealisticfacerotationfromsingle-viewimages.In:ProceedingsoftheIEEE/CVF
conference on computer vision and pattern recognition. pp. 5911–5920 (2020)