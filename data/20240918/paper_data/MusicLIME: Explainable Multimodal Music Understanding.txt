MUSICLIME:EXPLAINABLEMULTIMODALMUSICUNDERSTANDING
TheodorosSotirou,VassilisLyberatos,OrfeasMenisMastromichalakis,GiorgosStamou
NationalTechnicalUniversityofAthens,Greece
ABSTRACT andobservinghowpredictionschange,offeringavaluabletoolfor
examiningmodelbehaviorattheinstancelevel. Recentadvancesin
Multimodalmodelsarecriticalformusicunderstandingtasks,as
theycapturethecomplexinterplaybetweenaudioandlyrics. How- the area include AUDIOLIME, a variant of LIME adapted specif-
ically for the audio domain, which applies the same principle to
ever,asthesemodelsbecomemoreprevalent,theneedforexplain-
audio-specificfeatures[8].Inthemusicdomain,XAImethodshave
abilitygrows—understandinghowthesesystemsmakedecisionsis
beenappliedtointerpretmodelsthroughattentionmechanisms[9],
vitalforensuringfairness,reducingbias,andfosteringtrust. Inthis
perceptualmusicalfeatures[10],genreandspectralprototypes[11, paper,weintroduceMUSICLIME,amodel-agnosticfeatureimpor-
12],andconcept-basedexplanations[13].
tance explanation method designed for multimodal music models.
WhileexistingXAImethodshaveadvancedexplainabilityinthe
Unliketraditionalunimodalmethods,whichanalyzeeachmodality
musicdomain,thereisanotablegapinapproachestailoredtomul-
separatelywithoutconsideringtheinteractionbetweenthem, often
timodalmodels, particularlyinmusic, whichcombinesbothaudio
leadingtoincompleteormisleadingexplanations,MUSICLIMEre-
andlyricaldata. Multimodalexplainabilityoffersasignificantad-
vealshowaudioandlyricalfeaturesinteractandcontributetopre-
vantageoverunimodalmethodsbyprovidingamorecomprehensive dictions,providingaholisticviewofthemodel’sdecision-making.
understandingofhowdifferentmodalitiesinteractwithinamodel’s
Additionally, we enhance local explanations by aggregating them
decision-makingprocess. Unlikeunimodalapproaches,whichana-
intoglobalexplanations,givingusersabroaderperspectiveofmodel
lyzeeachmodalityinisolationandcanleadtoincompleteormis-
behavior. Throughthiswork,wecontributetoimprovingtheinter-
leadinginsights,multimodalexplanationsenableaholisticoverview
pretabilityofmultimodalmusicmodels,empoweringuserstomake
ofthemodel’sbehaviorbyrevealingthecontributionsandinterac-
informedchoices,andfosteringmoreequitable,fair,andtransparent
tionsbetweenfeaturesfromdifferentmodalities. Thisallowsusers
musicunderstandingsystems.
to better understand the intricate dynamics between, for example,
Index Terms— Explainable Artificial Intelligence, Music Un-
lyricalcontentandaudiofeaturesinmusic.Severalstudieshaveex-
derstanding,Multimodality
plored XAI methodologies for multimodal settings [14], including
thedevelopmentofamultimodalLIMEapproachforimageandtext
1. INTRODUCTION sentiment classification [15]. However, these methodologies have
yettobefullyappliedtotheMIRdomain,leavingagapinexplain-
As artificial intelligence (AI) continues to evolve, researchers are abilityformultimodalmusicmodels.
increasingly focusing on multimodal approaches to harness the Inthispaper,weintroduceMusicLIME,amodel-agnosticfea-
strengthsofdeeplearning(DL)acrossdiversetypesofdata. These tureimportanceexplanationmethodspecificallydesignedformul-
multimodalmodelsintegratevariousdatasources,suchastext,au- timodalmusicunderstandingsystems. Aspartofourmethodology,
dio, and images, to enhance accuracy and make better use of the wecuratedtwodatasetstailoringthemformultimodalmusicemo-
available data [1]. In the Music Information Retrieval (MIR) do- tionandgenrerecognitionanddevelopedatransformer-basedmul-
main,multimodalapproachesarebecomingincreasinglyprominent timodalmodeltotacklethesechallenges.MusicLIMEaddressesthe
astheycombineaudioandlyricaldatatoachievemoreprecisemu- challenge of explaining the interactions between audio and lyrical
sicanalysis[2]. Thisincludestaskssuchasmoodclassification[3], features,providingamorecomprehensiveviewofhowthesemodal-
emotion recognition [4], and music auto-tagging [5]. However, itiescontributetopredictions. Additionally, weprovideglobalex-
the complexity of multimodal models amplifies transparency chal- planationsbyaggregatinglocalexplanations,offeringabroaderun-
lenges. The interaction between modalities makes understanding derstandingofthemodel’soverallbehavior. Allcode,implementa-
their decisions harder, adding to the transparency issues already tiondetails,andinstructionsforreproducingtheresultsareavailable
present in unimodal systems. This lack of interpretability can ob- inourGitHubrepository1.
scure the decision-making process, impacting the reliability and
fairnessofthemodels.
2. METHODOLOGY
ExplainableAI(XAI)hasemergedasacrucialareaofresearch
focused on enhancing the interpretability and transparency of ma-
2.1. ModelArchitecture
chinelearningmodels.XAImethodsareessentialforunderstanding
howmodelsmakedecisions,therebyimprovingusertrustandfacil- Weexperimentedwithtwomodalities:text(lyrics)andaudio,utiliz-
itatingresponsibleAIdeployment[6].Amongthesemethods,Local inglanguagemodelfortextandaaudiomodelrespectively. These
Interpretable Model-agnostic Explanations (LIME) stands out as a two transformer-based models were combined into a single multi-
seminalandwidelyacceptedapproachintheXAIfield[7]. Itpro-
videslocalexplanationsbysystematicallyperturbinginputfeatures 1https://github.com/IamTheo2000/MusicLIME
4202
peS
61
]DS.sc[
1v69401.9042:viXraFig.1:OverviewofMUSICLIME
modalmodelbyconcatenatingtheirembeddingsintoaunifiedvec- CLIME, an extension of the LIME FRAMEWORK specifically tai-
tor,whichisthenfedintoaclassificationhead.Theaimwastoestab- loredformultimodalmusicunderstandingmodels. Anoverviewof
lishabaselinemodelthatwillbeusedtoevaluatetheeffectiveness MUSICLIMEisshowninFigure1.MUSICLIMEprocessesthetwo
ofourMUSICLIMEmethodinprovidinginsightsintothemodel’s modalitiesseparatelybeforeintegratingthemFortheaudiomodal-
behavioracrossmusicgenreandemotionclassificationtasks. ity,ourapproachbuildsonAUDIOLIMEbysplittingtheinputinto
Afterathoroughinvestigationofmodelarchitectures,wechoose temporal segments and further decomposing each segment into its
toexperimentwithROBERTA[16]asourlanguagemodelandAu- constituentsources.However,weenhancethesourceseparationpro-
dioSpectrogramTransformer(AST)[17]asouraudiomodel.These cessbyutilizingtheopen-unmixmodel(UMX)[18],whichprovides
modelswerechosenfortheireaseofimplementationandtheirbal- improvedseparationqualitycomparedtotheoriginalAUDIOLIME
ancedsizeandperformance.Itisimportanttonotethatourmethod- method measured in the MUSDB18 [19] dataset. Each audio in-
ologyismodel-agnosticandcanbeeasilyappliedtolargermodels. stance is divided into 10 segments and split into the components:
We utilized ROBERTA-LARGE 2, by first preprocessing and tok- vocals, drums, bass, and other instruments. For the text modality,
enizing the input lyrics. We generated mel-spectrograms from the wefollowanapproachsimilartotraditional LIME fortext,where
audios,usingtheprovidedFEATURE EXTRACTOR,specificforthe the input is split into individual words. After pre-processing, the
AST3.Ourmultimodalframeworkwascreatedbyconcatenatingthe featuresfrombothmodalitiesareencodedandconcatenatedintoa
pooledoutputfromtheASTwiththeCLStokenfromROBERTA. unifiedfeaturevector,indicatingthepresenceorabsenceoffeatures.
Thiscombinedfeaturevectorwasthenfedintoaclassificationhead, FollowingLIME’smethodology,weperturbthisvectorrepresenta-
comprisinganormalizationlayerandafullyconnectedlayer,togen- tion by selectively including or excluding features, allowing us to
eratethefinalpredictions.Fine-tuningonourdatasetwasperformed observe changes in the model’s predictions. Using these results,
bothseparatelyforeachmodalityandjointlyforthemultimodalset- wetrainaninterpretablelinermodeltoapproximatethemultimodal
ting. model’sbehaviorlocally. Thisapproachenablesustocomputefea-
ture importance scores for both audio and text simultaneously, fa-
cilitating a direct comparison of their contributions to the model’s
2.2. UnimodalandMultimodalExplainability
decision-makingprocess.
In this study, we selected LIME as the foundation for our ex-
plainability approach due to its simplicity, widespread adoption, 2.3. GlobalAggregationsofLocalExplanations
andproveneffectivenessinprovidingintuitivemodelexplanations.
To gain a comprehensive understanding of the model’s behavior
LIMEhasbeensuccessfullyadaptedtovariousdomainsandmodal-
beyond individual instances, generating class-wide explanations,
ities, including images, audio, and text. In the music domain, the
we implemented Global Aggregations of Local Explanations as
twoprimarymodalitiesofinterestaretextandaudio.Fortext-based
describedin[20]. Inourworkweapplytwomethods: (1)Global
models,LIMEassignsimportancescorestoindividualwords,indi-
AverageImportance,and(2)GlobalHomogeneity-WeightedImpor-
catingtheircontributiontothefinalprediction.Intheaudiodomain,
tance.
whilespectrogramscanbetreatedasimagestohighlightimportant
TheGlobalAverageClassImportanceiscalculatedas:
partsusingLIME,suchexplanationsareoftendifficulttounderstand
(cid:80)
o izr ei dn vte er rp sr ie ot n. tA hatm so er ge ms eu nit ta sb al ue da iopp ir no toac mh ei as nA inU gD fuI lO tL imIM eiE n, tea rvs ap le sc aia nl d-
I cA jVG = (cid:80)
i∈Sc|W ij|
1
(1)
isolates components like vocals or instruments, resulting in more
i∈Sc:Wij̸=0
comprehensibleandintuitiveexplanations. whereS c isthesetofallinstancesclassifiedasclassc,andW ij is
While the aforementioned approaches provide useful explana- theweightoffeaturejforinstanceiprovidedbyLIME.
tionsforunimodalmodels,themultimodalnatureofmusicrequires Thesecondmethodinvolvescalculatinganor √malizationvector
an adaptation that can capture the intricate interplay between its foreachfeaturejacrossallclassesLasp = (cid:80) √i∈Sc|Wij| .
different modalities. To addressthis limitation, we created MUSI- cj (cid:80) c∈L (cid:80) i∈Sc|Wij|
ThenormalizedLIMEimportancep representsthedistributionof
j
2https://huggingface.co/docs/transformers/en/model_doc/roberta featurej’simportanceacrossclasses. TheShannonentropyofthis
(cid:80)
3https://huggingface.co/docs/transformers/en/model_doc/audio- distribution is calculated as H j = − c∈Lp cjlog(p cj), measur-
spectrogram-transformer ingthehomogeneityoffeatureimportanceacrossmultipleclasses.Finally,theHomogeneity-WeightedImportanceis: fromthetitles, andretrievingthecorrespondinglyricswhenavail-
IH
=(cid:18)
1− H j−H min
(cid:19)(cid:115)
(cid:88) |W | (2)
a evb ale lu. aT tehi ts hepr ro oc be ud su tnre esy si oel fd oe ud ra apse pt roo af c3 h0 a8 cra ou sd sio d- il fy fer ric es ntp da air ts asu es te sd
.
to
cj H −H ij
max min
i∈Sc
whereH andH aretheminimumandmaximumentropyval- 3.2. ExperimentalSetup
min max
ues across all features. Intuitively this method penalizes features
OurconfigurationsutilizedNVIDIA’sV100andP100GPUs, with
thatinfluencemultipleclasses,whereashigherweightsareassigned
16GBofRAMeach. AllmodelswereimplementedusingthePy-
tofeaturesthatareimportantforspecificclasses.
Torchframework,withadditionalutilitylibrariesprovidedbyHug-
Implementing(1)and(2)wenotethatforthemultimodalmod-
gingFace.Apreprocessingstepwasnecessaryforourdata.Forthe
els, homogeneity-weightedimportancedoesnotaccuratelycapture
textual data, this involved standard data-cleaning procedures, such
the influence of multimodal features. This is due to the different
asconvertingcharacterstolowercaseandremovingpunctuation.Af-
natureofthefeatures. Whilewordsaredistinct,audiofeaturesen-
tercleaning,thetextwastokenizedintosequencesofupto256to-
capsulatedifferentsounds.Forexample,avocalfeaturecancontain
kens. Fortheaudiodata, weextractedmel-spectrogramswith128
variousstylesrangingfromsoothingsingingtoscreamsandshouts.
mel bands, utilizing a window and FFT size of 512, with a sam-
Asaresult,thesameaudiofeaturescanimpactmanyclassesfordif-
plingrateof44100Hz. ThisprocedureresultedinaninputMFCC
ferent reasons. Since Homogeneity-weighted importance punishes K ∈RS×P,whereSisthenumberofsegmentsandP isthenumber
featuresthatimpactmultipleclasses,lowerweightsareassignedto
ofMFCCs.
audiofeaturescomparedtothetextones,whichisinaccurate.There-
Togeneratetheglobalaggregates,wecombinedtheweightspro-
fore,globalaverageclassimportanceismoresuitedformultimodal
ducedbymultipleinstances,eachgeneratedwithadifferentnumber
analysis.
ofperturbations. Specifically,fortheM4A dataset,weaggregated
theresultsfrom640instancesforthelyricalmodel,240instancesfor
3. EXPERIMENTS theaudiomodel,and128instancesforthemultimodalmodel. For
theAudioSetdataset,wecombinedtheresultsofalltheinstancesfor
3.1. Datasets
thelanguagemodeland232fortheaudioandmultimodalmodels.
Although the Music Information Retrieval (MIR) community has Thenumberofperturbationsperinstanceforthelanguage,audioand
created various multimodal datasets [21], many of which can be multimodalmodelswere2500,2000and5000respectively.Finally,
found on ISMIR’s resource page4, finding a dataset that includes to visualize the aggregate weights of the words for each class and
both audio and lyrics remains challenging due to copyright re- facilitatecomparisons, weemployedGloVeembeddingscombined
strictions. Forthisstudy, wecuratedtwodatasets: Music4All[22] withT-SNEfordimensionalityreduction.
(M4A),amultimodaldatasetwithbothaudioandlyrics,andaman-
ually constructed multimodal subset of AUDIOSET[23], where we 4. RESULTS&DISCUSSION
combinedaudiofromAUDIOSETwithlyricssourcedfromexternal
databases. Table 1 summarizes the performance of our models on the M4A
M4A provides 30-second audio clips and lyrics for each in- dataset.Overall,themultimodalmodelconsistentlyoutperformsthe
stance, along with metadata including genre labels and valence- unimodal models, demonstrating the value of combining text and
energyvalues. Usingthesemetadata,wecategorizedthesongsinto audiofeaturesinmusicclassification. Thelanguagemodelshowed
ninedistinctgenres(heavymusic,punk,rock,alternativerock,folk, limitedaccuracyinpredictingemotionsbutexcelledatidentifying
pop, rhythm music, hip hop and electronic) based on Musicmap 5 specificgenres,suchashiphopandheavymusic,likelyduetorecur-
andninedistinctemotioncategoriesderivedfromRussel’scircum- ringthematicelementsinthelyrics,asfurtherelucidatedbyourex-
plexmodel[24]. Toensurelabelaccuracy,wecross-referencedthe planations(seeFigures2and3).Conversely,theaudiomodel,gener-
genre labels with Spotify’s artist genre classifications, refining the allyoutperformedthelyricalmodelacrosstasks,especiallyinemo-
dataset to include around 60,000 songs, with 50,000 reserved for tionclassification,indicatingthatmood-relatedinformationismore
training.Wemaintainedadatasplitwherenoartistfromthetraining effectivelycapturedintheaudiodomain.Additionally,genrepredic-
andvalidationsetsappearedinthetestset,ensuringthatthemodel tionprovedmoreaccuratethanemotionprediction,whichmaybeat-
wasevaluatedontrulyunseendataforgeneralizability. tributedtotheinherentlysubjectivenatureofhumanemotions[25]
Tofurthervalidateourmethodologyandensurethatourresults on one side, but also to the distinctive features of various genres,
are not dependent on a single dataset, we created a small multi- whetherinlyrics(e.g.,hiphop)oraudio(e.g.,vocalsanddrumsin
modal dataset based on a subset of music-related recordings from
AUDIOSET [23]. AUDIOSET containsdescriptivelabels(e.g.,fire-
works,harmonica)ofYouTubevideos’audiosegments.Wefocused Model ValidationAcc. TestAcc.
onmusicsamplesandmatchedthesongtitleswithlyricsfromtwo LyricalEmotion(RoBERTa) 34.03% 32.33%
openlyavailablesources67. Thisprocessinvolvedfetchingvideoti- AudioEmotion(AST) 48.33% 48.29%
tlesforallentries,filteringoutnon-songinstances(suchascompila- MultimodalEmotion 49.05% 48.53%
tions,remixes,orseriesepisodes),extractingartistandsongnames LyricalGenre(RoBERTa) 46.90% 45.14%
AudioGenre(AST) 55.63% 53.75%
4https://ismir.net/resources/datasets/
MultimodalGenre 60.33% 57.34%
5https://musicmap.info/
6https://docs.genius.com/
7https://lyrics.lyricfind.com/ Table1:Modelperformancesummarypunkmusic). Theseobservationsarefurthervalidatedbyourmulti-
modalexplanationspresentedinthefollowingparagraphs. Overall,
theresultsemphasizethecomplementarystrengthsofeachmodal-
ity and highlight the importance of using multimodal explanations
tobetterunderstandmodelbehavior.
Figure 2 demonstrates the effectiveness of our approach and
highlights its advantages over unimodal explanations. The Figure
presentsglobalmultimodalexplanationsforhiphop,punk,andpop
For hip hop, the explanations reveal that lyrical features predomi-
nantlydrivethemodel’sdecision.Incontrast,forpunkmusic,audio
featuresappeartoplayamoresignificantrole. Forpopmusic,nei-
theraudionorlyricalfeaturesdominate,suggestingamorebalanced
influence from both modalities. These insights, which cannot be
fully derived from unimodal explanations due to the lack of direct
comparison between feature importances, align with the nature of
eachgenre. hiphop’sstronglyricalfocus,punk’sdistinctivemusi-
calcharacteristics,andpop’smorediversethematiccontentarere-
flectedintheexplanations. Thesefindingsarefurthersupportedby
thegloballyricalexplanationsshowninFigure3.This2Dvisualiza-
tionofthetop10lyricalfeaturesforhiphop,heavymusic,andpop
revealsthatgenreswherelyricalfeaturesdominatealsoexhibitdis-
tinctthematictopics. Forinstance,hiphopfeaturespredominantly
revolvearoundstreetculture, slurs, slang, andartisticexpressions, Fig.3:Top10lyricalfeaturesfortheheavymusic,hiphop,andpop
whileheavymusic’slyricalcontentcentersondarkthemesandfan- genresforbothdatasetsclustered
tasyelements. Conversely,popmusic’slyricalcontentlacksadom-
inantthematicfocus,leadingthemultimodalmodeltorelyonboth
audioandlyricsforaccurategenreclassification. as the dominant modality, as evidenced by the marginal perfor-
Ourfindings,furtherdetailedonourGitHubrepository8,align manceimprovementwhenincorporatingmultimodalinputsandthe
withtheestablishedcharacteristicsofvariousmusicgenresandasso- predominanceofaudio-basedfeaturesintheexplanationsforemo-
ciatedemotions,supportingtherobustnessofourmethodology[26]. tionpredictions. Conversely, ingenrerecognition, bothmodalities
Themultimodalexplanationsweidentifiedalignwiththeanticipated contribute significantly, enhancing overall model performance and
genre-specificandemotion-specificfeatures. Forinstance,folkmu- yieldingexplanationsthatattributenearlyequalimportancetoeach
sic frequently incorporates regional instruments and lyrics that re- modality.
flectrurallife. Incontrast,electronicmusicischaracterizedbythe
prominence of drums and synthesizers. Similarly, the presence of 5. CONCLUSIONS&FUTUREWORK
guitarsisadefiningfeatureinrockmusic. Regardingemotiontags,
the tense emotion appears to be strongly associated with vocal el- Inthisstudy,weexploreddeep-learningmultimodalmodelsandin-
ements, likely due to its connection with the hip hop genre. Ad- troduced MUSICLIME,anovelmodel-agnosticmultimodalexpla-
ditionally, positive emotions such as happy and exciting are often nationmethodologytailoredformusicunderstanding. Ourfindings
correlatedwiththeuseofdrums,possiblyduetotheirpowerfuland highlightthatmultimodalapproachesoutperformunimodalonesby
dynamicsound. leveraging the complementary information embedded in different
Itisnoteworthythatthemultimodalexplanationsproducedby modalities and their interactions. We also developed a global ag-
MUSICLIMEareconsistentwiththeobservationsandassumptions gregationmethodologythatenhancestheinterpretationoftherela-
that a user makes based on the performance metrics outlined in tionshipsbetweengenresoremotionsandtheirassociatedaudioand
Table1.Inthecontextofmusicemotionrecognition,audioemerges lyricalfeatures,providingacomprehensiveviewofthemostrepre-
sentative characteristics of each class. We assessed the robustness
8https://github.com/IamTheo2000/MusicLIME
ofMUSICLIMEthroughitsapplicationtotwodistinctdatasetsand
tasks,demonstratingitseffectivenessinvariouscontexts.
Future research will focus on improving MUSICLIME by re-
fining various modules of the pipeline, including data preprocess-
ing,encodingtechniques,aswellassampleselectionandperturba-
tionstrategieswithinthecore LIME algorithm. Onelimitationof
our current approach is that the lyrical modality is analyzed at the
word level, which may overlook broader contextual meaning. To
addressthis, weplantoexplorewaystomake MUSICLIME more
context-aware,enablingittocapturemoregeneralideasbeyondin-
dividualwords. Additionally,wewillinvestigatealternativeexpla-
Fig.2: Top10featuresfromtheglobalaggregatesforthehiphop, nationmethods,suchascounterfactualexplanations,andassesstheir
punk,andpopgenresfromtheMusic4All dataset applicabilityinamultimodalframeworkformusicunderstanding.6. REFERENCES [14] NikolaosRodis,ChristosSardianos,GeorgiosTh.Papadopou-
los,PanagiotisRadoglou-Grammatikis,PanagiotisSarigianni-
[1] Xue Han, Yi-Tong Wang, Jun-Lan Feng, Chao Deng, Zhan- dis, and Iraklis Varlamis, “Multimodal explainable artificial
Heng Chen, Yu-An Huang, Hui Su, Lun Hu, and Peng-Wei intelligence: A comprehensive review of methodological ad-
Hu, “A survey of transformer-based multimodal pre-trained vancesandfutureresearchdirections,”2023.
modals,” Neurocomputing,vol.515,pp.89–106,2023.
[15] Israa Khalaf Salman Al-Tameemi, Mohammad-Reza Feizi-
[2] Federico Simonetta, Stavros Ntalampiras, and Federico Derakhshi,SaeidPashazadeh,andMohammadAsadpour,“In-
Avanzini, “Multimodalmusicinformationprocessingandre- terpretable multimodal sentiment classification using deep
trieval: Surveyandfuturechallenges,” in2019international multi-view attentive network of image and text data,” IEEE
workshop on multilayer music representation and processing Access,vol.11,pp.91060–91081,2023.
(MMRP).IEEE,2019,pp.10–18. [16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar
Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-
[3] Rémi Delbouys, Romain Hennequin, Francesco Piccoli, Ji-
moyer,andVeselinStoyanov, “Roberta:Arobustlyoptimized
menaRoyo-Letelier,andManuelMoussallam, “Musicmood
BERT pretraining approach,” CoRR, vol. abs/1907.11692,
detection based on audio and lyrics with deep neural net,”
2019.
CoRR,vol.abs/1809.07276,2018.
[17] YuanGong,Yu-AnChung,andJamesR.Glass, “AST:audio
[4] Tibor Krols, Yana Nikolova, and Ninell Oldenburg, “Multi-
spectrogramtransformer,” CoRR,vol.abs/2104.01778,2021.
modality in music: Predicting emotion in music from high-
[18] F.-R.Stöter,S.Uhlich,A.Liutkus,andY.Mitsufuji, “Open-
levelaudiofeaturesandlyrics,”2023.
unmix-areferenceimplementationformusicsourcesepara-
[5] QingqingHuang,ArenJansen,JoonseokLee,RaviGanti,Ju- tion,” JournalofOpenSourceSoftware,2019.
dithYueLi,andDanielP.W.Ellis,“Mulan:Ajointembedding
[19] Zafar Rafii, Antoine Liutkus, Fabian-Robert Stöter,
ofmusicaudioandnaturallanguage,”2022.
Stylianos Ioannis Mimilakis, and Rachel Bittner, “The
[6] CeciliaPanigutti,AndreaBeretta,DanieleFadda,FoscaGian- MUSDB18corpusformusicseparation,”Dec.2017.
notti, Dino Pedreschi, Alan Perotti, and Salvatore Rinzivillo,
[20] Ilse van der Linden, Hinda Haned, and Evangelos Kanoulas,
“Co-designofhuman-centered,explainableaiforclinicalde-
“Globalaggregationsoflocalexplanationsforblackboxmod-
cisionsupport,” ACMTransactionsonInteractiveIntelligent
els,” CoRR,vol.abs/1907.03039,2019.
Systems,vol.13,no.4,pp.1–35,2023.
[21] Anna-Maria Christodoulou, Olivier Lartillot, and Alexan-
[7] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin, der Refsum Jensenius, “Multimodal music datasets? chal-
“"whyshouldItrustyou?": Explainingthepredictionsofany lengesandfuturegoalsinmusicprocessing,” .
classifier,” inProceedingsofthe22ndACMSIGKDDInterna-
[22] IgorAndréPegoraroSantana,FabioPinhelli,JulianoDonini,
tionalConferenceonKnowledgeDiscoveryandDataMining,
LeonardoCatharin,RafaelBiazusMangolin,YandreMaldon-
SanFrancisco,CA,USA,August13-17,2016,2016,pp.1135–
adoeGomesdaCosta, ValériaDelisandraFeltrim, andMar-
1144.
cosAurélio Domingues, “Music4all: Anewmusic database
[8] Verena Haunschmid, Ethan Manilow, and Gerhard Widmer, anditsapplications,” in27thInternationalConferenceonSys-
“audiolime: Listenableexplanationsusingsourceseparation,” tems, Signals and Image Processing (IWSSIP 2020), Niterói,
CoRR,vol.abs/2008.00582,2020. Brazil,2020,pp.1–6.
[23] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren
[9] Minz Won, Sanghyuk Chun, and Xavier Serra, “Toward in-
Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal,
terpretable music tagging with self-attention,” CoRR, vol.
andMarvinRitter,“Audioset:Anontologyandhuman-labeled
abs/1906.04972,2019.
datasetforaudioevents,” inProc.IEEEICASSP2017, New
[10] Vassilis Lyberatos, Spyridon Kantarelis, Edmund Dervakos, Orleans,LA,2017.
and Giorgos Stamou, “Perceptual musical features for inter-
[24] Yeong-SeokSeoandJun-HoHuh, “Automaticemotion-based
pretableaudiotagging,”2024.
musicclassificationforsupportingintelligentiotapplications,”
[11] Pablo Alonso-Jiménez, Leonardo Pepino, Roser Batlle-Roca, Electronics,vol.8,pp.164,022019.
PabloZinemanas,DmitryBogdanov,XavierSerra,andMartín
[25] JSGómez-Cañón,ECano,PHerrera,andEGómez, “Joyful
Rocamora, “Leveraging pre-trained autoencoders for inter-
foryouandtenderforus:theinfluenceofindividualcharacter-
pretable prototype learning of music audio,” arXiv preprint
isticsandlanguageonemotionlabelingandclassification,” in
arXiv:2402.09318,2024.
Proceedingsofthe21stInternationalSocietyforMusicInfor-
[12] RomainLoiseau, BaptisteBouvier, YannTeytaut, ElliotVin- mationRetrievalConference,JCumming,JHaLee,BMcFee,
cent, Mathieu Aubry, andLoïcLandrieu, “Amodelyou can MSchedl,JDevaney,CMcKay,EZager,andTdeReuse,Eds.,
hear: Audioidentificationwithplayableprototypes,” ISMIR, Montréal,Canada,October11-162020,ISMIR,pp.853–860,
2022. ISMIR.
[26] Chijioke Worlu, “Predicting listener’s mood based on music
[13] Francesco Foscarin, Katharina Hoedt, Verena Praher, Arthur
genre: an adapted reproduced model of russell and thayer,”
Flexer,andGerhardWidmer, “Concept-basedtechniquesfor"
JournalofTechnologyManagementandBusiness, vol.4, no.
musicologist-friendly"explanationsinadeepmusicclassifier,”
1,2017.
arXivpreprintarXiv:2208.12485,2022.