MacDiff: Unified Skeleton Modeling with
Masked Conditional Diffusion
Lehong Wu1,2, Lilang Lin1, Jiahang Zhang1, Yiyang Ma1, and Jiaying Liu1⋆
1 Wangxuan Institute of Computer Technology, Peking University
2 School of Electronics Engineering and Computer Science, Peking University
aladonwlh@stu.pku.edu.cn
{linlilang, zjh2020, myy12769, liujiaying}@pku.edu.cn
Abstract. Self-supervised learning has proved effective for skeleton-
basedhumanactionunderstanding.However,previousworkseitherrely
on contrastive learning that suffers false negative problems or are based
onreconstruction thatlearnstoomuchunessentiallow-levelclues,lead-
ing to limited representations for downstream tasks. Recently, great ad-
vanceshavebeenmadeingenerativelearning,whichisnaturallyachal-
lengingyetmeaningfulpretexttasktomodelthegeneralunderlyingdata
distributions. However, the representation learning capacity of genera-
tive models is under-explored, especially for the skeletons with spacial
sparsityandtemporalredundancy.Tothisend,weproposeMaskedCon-
ditional Diffusion (MacDiff) as a unified framework for human skeleton
modeling. For the first time, we leverage diffusion models as effective
skeletonrepresentationlearners.Specifically,wetrainadiffusiondecoder
conditionedontherepresentationsextractedbyasemanticencoder.Ran-
dommaskingisappliedtoencoderinputstointroduceainformationbot-
tleneck and remove redundancy of skeletons. Furthermore, we theoreti-
cally demonstrate that our generative objective involves the contrastive
learningobjectivewhichalignsthemaskedandnoisyviews.Meanwhile,
it also enforces the representation to complement for the noisy view,
leading to better generalization performance. MacDiff achieves state-of-
the-artperformanceonrepresentationlearningbenchmarkswhilemain-
taining the competence for generative tasks. Moreover, we leverage the
diffusion model for data augmentation, significantly enhancing the fine-
tuningperformanceinscenarioswithscarcelabeleddata.Ourprojectis
available at https://lehongwu.github.io/ECCV24MacDiff/.
Keywords: Self-supervised learning · Unified skeleton modeling · Dif-
fusion model
1 Introduction
Human action understanding has been a crucial problem in computer vision.
Skeletonsuse3Dcoordinatestorepresenthumanjoints,providingalightweight,
⋆ Corresponding author.
4202
peS
61
]VC.sc[
1v37401.9042:viXra2 L. Wu et al.
compact, and privacy-preserving data modality of human representation. Ow-
ing to these advantages, skeletons are widely used for human action analysis
in real-world applications such as human-robotics interaction [24], autonomous
driving [2] and video surveillance [13]. Since well-annotated data are expensive
to obtain, self-supervised methods have been proposed to extract meaningful
representations from unlabeled skeletons.
Prevalent high-performing self-supervised methods for skeletons mainly con-
taintwoparadigms,i.e.,contrastivelearning-basedmethodsandreconstruction-
basedmethods.Contrastivelearning(CL)trainsthemodeltocapturesharedin-
formationbetweentwoaugmentedviewsofthesamesample.Numerousworks[8,
27,40,48,49] focus on designing skeleton-specific augmentations to benefit CL
performance. Reconstruction-based methods [17,32,50,55] carefully design re-
constructiontargetsthatpromptthemodeltocapturethespatial-temporalcor-
relationsofskeletons.However,existingself-supervisedmethodsallfocusonspe-
cificsemanticsrequiredbythepre-definedtasks.Forexample,contrastivelearn-
ing is proved to only learn the discriminative information for positive/negative
pairs [53], which limits its generalization ability and makes it sensitive to the
augmentation design. Reconstruction-based methods overly focus on full signal
reconstruction, causing the representations to contain too much low-level infor-
mation irrelevant for high-level action understanding.
Generative models, aiming to approximate the real-world data distribution,
naturally form a more general self-supervised learning target than carefully de-
signed tasks. Thus, introducing generative models in self-supervised learning
would force the model extract better representations with richer semantics.
Autoencoder-based generative models like VAEs [22,37] rely on the informa-
tion bottleneck to obtain a compact meaningful latent representation, result-
ing in a trade-off between discriminability and generation authenticity. More
recently, diffusion models [11,18,41,42] have shown remarkable generative ca-
pacities. However, diffusion models which directly predict the noise contained
in the noisy states do not explicitly learn a meaningful latent representation
tailored for discriminative tasks. Moreover, due to the spatial sparsity and tem-
poral redundancy of skeletons, diffusion models on skeletons maintain too much
irrelevantlow-levelappearanceinformation,whichfailstoconstructatightbot-
tleneckandobtainmeaningfulrepresentation.Therefore,effortsshouldbemade
to mine the potential of such powerful diffusion models to obtain more powerful
representations.
Totacklethesechallenges,weproposeMaskedConditionalDiffusion(MacD-
iff) which, for the first time, tames diffusion for both skeleton representation
learning and skeleton generation. Specifically, we train a semantic encoder to
guide a diffusion decoder. The encoder-decoder design serves to disentangle the
high-levelrepresentationlearningwithlow-levelgenerativetraining.Meanwhile,
this architecture mitigates the conflict between discrimination and generation
authenticity, allowing for a more flexible design of the encoder. Considering the
spatial-temporalcorrelationofskeletons,atighterinformationbottleneckshould
be imposed in addition to restricting the dimensions of representations. To thisUnified Skeleton Modeling with Masked Conditional Diffusion 3
end, we propose to apply random masking on patchified skeletons with a high
masking ratio to constrain the representation’s dimension.
We conduct theoretical analyses on MacDiff from a mutual information per-
spective to prove the effectiveness of our framework. We demonstrate that the
training of MacDiff is equivalent to a combination of the contrastive learning
and reconstruction objective. The contrastive learning objective, conducted on
the masked and noisy views of skeletons, enforces the representation to cap-
ture shared information between different views, while the very small portion
of reconstruction objective enriches the representation by complementing the
missinginformationinthenoisyview,includingmoredownstreamtask-relevant
information in the representation.
We provide thorough experiments and results on NTU RGB+D [29,43] and
PKUMMD [28] datasets to demonstrate the effectiveness and versatility of our
method. MacDiff achieves remarkable results on self-supervised learning bench-
marks. In addition, we utilize the pre-trained diffusion for generating label-
preserving training data in scenarios with limited training data, which brings
significantperformancegaininsemi-supervisedprotocols.Ourcontributionscan
be summarized as follows:
• We propose Masked Conditional Diffusion (MacDiff), a unified framework
forhumanskeletonmodeling.Asemanticencoderisemployed,learninghigh-
level compact representations, to assist the conditioned generative learning
of the diffusion decoder. By virtue of this, our model learns powerful repre-
sentations for both discriminative and generative downstream tasks.
• We theoretically demonstrate that the generative objective of MacDiff in-
volves both diffusion learning and contrastive learning that aligns the rep-
resentation of the masked view with the noisy view. Moreover, MacDiff is
capable of preserving more semantics in the learned representation, leading
to better downstream performance than contrastive-only paradigms.
• MacDiff achieves state-of-the-art performance on three large-scale bench-
marks. Remarkably, we leverage diffusion-based data augmentation for en-
coderfine-tuning,significantlyimprovingtheactionrecognitionperformance
with scarce labeled data.
2 Related Work
Self-Supervised Learning for Skeletons. Self-supervised learning aims to
extractmeaningfulrepresentationsfromunlabeleddatatofacilitatedownstream
tasks. Prevalent methods for skeleton representation learning can be divided
intocontrastivemethodsandreconstructionmethods.Contrastivelearning(CL)
extracts meaningful representations by discriminating positive/negative sam-
ple pairs from different augmented views [4,6]. To leverage CL for skeletons,
numerous works [27,40,48,49] focus on developing skeleton-specific data aug-
mentations. Other works extract shared information between different skele-
ton modalities [25,33]. Most of these methods use RNNs or GCNs as back-
bone. Among reconstruction methods, LongT GAN [62] and P&C [23] design4 L. Wu et al.
reconstruction tasks on autoencoders to learn compressed representations. GL-
Transformer[21]designspredictiontaskswithTransformerbackbonetocapture
spatial-temporal dynamics of skeletons. Some works combine reconstruction or
prediction tasks with contrastive learning, including [7,26,58]. More recently,
MaskedAutoencoders(MAEs)areintroducedtoskeletonsbySkeletonMAE[55]
and MAMP [32], which achieve remarkable performance by modeling spatial-
temporal correlations of skeletons with Transformers.
DiffusionModels. Diffusionmodelsareatypeofgenerativemodelthatgradu-
allymapsanoisepriortoatargetdistribution.Theyhavedemonstratedsuperior
capacity in image generation [11,18,41,42] and a variety of applications [31,34].
Therefore,diffusionmodelshavebeenwidelyusedfortext-guidedhumanmotion
generation[5,16,47,59],motionprediction[3]andanomalydetection[13].These
efforts adapt image diffusion models to skeletons with modifications including
specialized loss functions for skeletons and leveraging Transformer architecture.
Amongafewworksthatpioneerinleveragingdiffusionrepresentations,most
works directly use intermediate representations of pre-trained diffusion models
or fine-tune the model. While proved effective for dense prediction tasks like
segmentation[36]anddensematching[35],theserepresentationsarenotcompact
enough compared with other self-supervised methods. Another paradigm [61] is
to distill information from fixed diffusion models. More recently, efforts that
jointly train a separate encoder to guide diffusion yield promising results [20,
39]. In this paper, we propose the first framework that tailors diffusion models
for skeleton representation learning, to the best of our knowledge. Besides, we
provide theoretical analysis for the effectiveness of the proposed framework.
UnifiedSkeletonModeling. RecentadvancementsinNaturalLanguagePro-
cessing have demonstrated the potential for building models that unify multi-
ple tasks. For a general-purpose human representation learning, several works
[7,26,58] leverage multiple tasks as self-supervised training. MotionBERT [63]
learns unified 2D skeleton representation by training on 2D-to-3D lifting task.
UniHCP[10]providesaunifiedmodelforseveralhuman-centrictasks,e.g.,pose
estimation,ReIDandpedestriandetection.UPS[14]formsskeletonsandaction
labels as language tokens. Skeleton-in-Context [54] leverages in-context learning
to unify multiple estimation and prediction tasks. However, most unified meth-
ods are restrained to the specific tasks they are trained on. GFPose [9] utilizes
score-based generative model to learn a unified human pose prior but only fo-
cusonsingle-frameskeletons,limitingitsapplications.Inthispaper,weexplore
the capability of a versatile generative model, diffusion model, to unify skeleton
representation learning with generation.
3 Method
3.1 Diffusion Models Preliminary
Diffusion models are a family of generative models that learn the target dis-
tributionbyperformingdatadenoising.DenoisingDiffusionProbabilisticModelUnified Skeleton Modeling with Masked Conditional Diffusion 5
Representation Extraction spatial DiffusionTrainingat Timestep
Masking Semantic
Embed Operation Encoder Pool Block Loss
temporal
+ + …
Sampling
Embed Project
Diffusion Decoder
Fig.1:Theoverviewoftheproposedmethod.Wetrainadiffusiondecoderconditioned
ontherepresentationsextractedbyasemanticencoder.Intheabovestream,weembed
theinputskeletonsintotokensandemployrandommasking.Theglobalrepresentation
isobtainedbypoolingthelocalrepresentationsextractedbythesemanticencoder.In
thebelowstream,wetrainaconditionaldiffusionmodel.Wesamplethenoisyskeleton
x following the diffusion process q(x |x ). The diffusion decoder predicts the noise ϵ
t t 0
fromx guidedbythelearnedrepresentationz.Thepre-trainedencodercanbeutilized
t
independently in downstream discriminative tasks.
(DDPM)[18]employsaforward(diffusion)processthatsequentiallycorruptsthe
data distribution q(x ) to the standard Gaussian distribution N(0,I) with the
0
conditional distribution q(x |x ). The noise level of the timesteps t is defined
t t−1
by a fixed increasing variance schedule {β }T . This process can be denoted as:
t t=0
(cid:112)
q(x |x )=N(x ; 1−β x ,β I). (1)
t t−1 t t t−1 t
With a given t, the forward process allows sampling x directly from x with
√ t 0
q(x |x )=N(x ; α x ,(1−α )I), where α =1−β and α =(cid:81)t α .
t 0 t t 0 t t t t i=0 i
The reverse process is defined as another Markov Chain parameterized by θ,
whichmapsthestandardGaussiandistributiontothedistributionofcleandata
by gradually denoising. Each step is a Gaussian distribution with a predicted
mean µ (x ,t) and covariance matrix σ2I:
θ t t
p (x |x )=N(µ (x ,t),σ2I). (2)
θ t−1 t θ t t
Thefinaltrainingobjectiveisderivedfromoptimizingthevariationalbound
on E[−logp (x )] [18], where γ are positive coefficients depending on α :
θ 0 1:T 1:T
√ √
L(θ)=E (cid:2) γ ∥ϵ−ϵ ( α x + 1−α ϵ,t)∥2(cid:3) . (3)
x0,t,ϵ∼N(0,I) t θ t 0 t
Other implementations beyond ϵ-prediction include x - and x -prediction,
t−1 0
which are all equivalent to predicting µ (x ,t) in mathematical formulation.
θ t
For sampling, we adopt Denoising Diffusion Implicit Model (DDIM) [45],
which sharesthesametrainingobjectivewith DDPMbutdefinesanon-Markov
diffusion process. DDIM allows for deterministic sampling with better quality.
3.2 Masked Conditional Diffusion
Inthissection,wedescribetheproposedmethod,MaskedConditionalDiffusion
(MacDiff),asaunifiedframeworkforhumanskeletonmodeling.Fig.1illustrates
the overall pipeline of our method.
CF
NLadA ASM NLadA PLM6 L. Wu et al.
Concretely,wejointlyoptimizeasemanticencoderE andadenoisingdecoder
D (also denoted as ϵ ) by training a conditional diffusion model ϵ (x ,t,z).
θ θ t
First, the encoder extracts a latent representation from the masked view of
skeleton data x, formulated as z = E(M(x)) where M(·) denotes the masking
operation. Then, the decoder predicts the noise contained in x conditioned on
t
z. With the aforementioned pre-training, the encoder learns to extract compact
representation applicable for discriminative downstream tasks.
Patchify and Embedding. Given the input skeleton x ∈ RT0×V×3, we first
divide it into non-overlapping patches with equal length along the temporal di-
mensionx′ ∈RTp×V×(l×3).T 0isthenumberofframes,V isthenumberofjoints,
l is the patch length, and T =T /l. The patches are then flattened and embed-
p 0
ded to C dimensions with a trainable linear projection: E = Embedding(x′) ∈
RTp×V×C. Now we have T ·V tokens of dimension C as the encoder’s input.
Random Masking. Considering the redundancy in the skeleton, especially in
the temporal dimension, naive reconstruction training can result in the unde-
sirable shortcuts and degrade the model performance. Meanwhile, based on the
informationbottleneckprinciple[15,30],themodellearnsacompactrepresenta-
tion if information compression is introduced to remove redundancy. Therefore,
weemployrandommaskingfortheencoderinputwithamaskingratior,retain-
ing a total of K = ⌈(1−r)·T ·V⌉ tokens. In practice, we adopt an extremely
p
high masking ratio r =90%, enforcing a tight bottleneck on the representation.
In addition, the masking operation significantly speeds up training by reducing
the encoder’s computational graph.
Model Architecture. Our encoder and decoder both follow a vanilla Trans-
former[51]architecture.First,trainablespatialandtemporalpositionalembed-
dings Es ∈R1×V×C,Et ∈RTp×1×C are added to the tokens E with broad-
pos pos
casting. Note that this is implemented before the masking operation for the
encoder. The Transformer network consists of alternating layers of multi-head
self-attention (MSA) and multi-layer perceptron (MLP) with residual connec-
tion. Layer Norm (LN) is applied before each layer and after the last layer.
Theencoderoutputz ∈RK×C isthelocalrepresentationscorresponding
local
to unmasked patches. The global representation z is obtained by pooling
global
all tokens. For the decoder output, we linearly project it to the final prediction
of the same shape as x .
0
Conditioning. To incorporate the condition z into the denoising decoder, we
replace Layer Norm with Adaptive Layer Norm (AdaLN), following [38]:
AdaLN(h,z,t)=z ·(t ·LN(h)+t )+z , (4)
s s b b
wherehisthehiddenrepresentation,(t ,t )and(z ,z )areobtainedfromlinear
s b s b
projection of the timestep embedding t and condition z, respectively. Through
AdaLNlayers,theconditionz guidesthedenoisingprocessbyscalingandshift-
ingnormalizedhiddenrepresentation.Tofurtherdisentanglethecontributionsof
theencoderanddecodertothefinalprediction,wedropoutz withaprobability
of 0.1, meanwhile enabling unconditional generation.
In practice, we observe an over-smoothing problem of the encoder if we sim-
ply utilize z (with broadcast) as z. Over-smoothing is a common problem
globalUnified Skeleton Modeling with Masked Conditional Diffusion 7
for GCNs and Transformers [12,19] that degrades performance and can be re-
flected by high similarity between tokens. We aim to increase token uniformity
by preserving more local information in tokens in addition to global informa-
tion. To this end, we unshuffle the unmasked tokens z and fill the masked
local
positions with z to form z. Since a high masking ratio is implemented,
global
the model prioritizes optimizing the global representation while also attempts
to benefit from local information by directly optimizing local representations.
DiffusionTraining. Consideringthediscrepancybetweenthestatisticsoforig-
inalskeletondataandthestandardGaussiandistribution,wenormalizethedata
using the mean µ and standard variation σ calculated from the training set:
x −µ
x = orig , µ,σ ∈R1×1×3. (5)
0 σ
Fordetailedsettingsofdiffusion,weadoptsomecommonpracticesincluding
ϵ-prediction,totaltimestepsT =1000andlossweightsγ =1[18].Ourtraining
t
objective is a simplified and conditional version of Eq. (3):
√ √
L=E (cid:2) ∥ϵ−D( α x + 1−α ϵ, t, E(M(x )))∥2(cid:3) . (6)
x0,t,ϵ t 0 t 0
Aminoritychoiceinourworkregardingdiffusionistheinverse-cosinesched-
ule[20].Theinverse-cosineschedulepullsthenoiselevelofalltimestepstowards
amediumlevelcomparedwithcommonly-usedcosine[1]orlinear[18]schedules.
We verify this choice with experiments (see Sec. 4.5).
3.3 Information Analysis on MacDiff
Inthissection,weconductinformation-theoreticanalysesonourproposedframe-
work MacDiff. We formulate the generative objective of MacDiff as an improve-
ment of contrastive learning (CL) objectives, leading to a better guarantee of
downstream performance. For mathematical formulation, we use random vari-
ables X,X ,X and Z to denote the original view, noisy view, masked view,
t m
and latent representation of the skeleton data.
The Training Objective of MacDiff. A generative model (e.g., VAEs and
MAEs)thatpredictssometargetX fromlatentcodeV maximizestheirmutual
information (MI), i.e. I(X;V). In the case of MacDiff, V takes the form of
(Z,X ). MacDiff can thereby be described as:
t
max I(X;(Z,X )), Z =E(X ). (7)
t m
E,D
This training objective is further formulated as follows:
I(X;(Z,X ))=I(X;Z)+I(X;X |Z). (8)
t t
ThefirsttermI(X;Z)trainstheencodertocontainmoreinformationaboutX in
representationZ,whilethesecondtermtrainsthedecodertopredictX fromX
t
conditioned on Z. By employing the encoder-decoder design and dropouting on8 L. Wu et al.
therepresentation,wefurtherdisentangletheencoder’shigh-levelrepresentation
learning from the decoder’s low-level prediction objective.
MacDiff as an Improvement of Contrastive Learning. We can further
decompose I(X;Z) into two terms with the intermediate variable X :
t
I(X;Z)=I(X ;Z)+I(X;Z|X ). (9)
t t
Wepointoutthatthefirsttermisconsistentwiththecontrastivelearningobjec-
tive. CL assumes that the information needed for downstream task Y is shared
between two views X ,X [44,52,53]. Therefore, CL aims to optimize the MI
1 2
between their representations Z ,Z .
1 2
I(Z ;Z )≤I(Z ;X )≤I(X ;X )=I(X;Y). (10)
1 2 1 2 1 2
In our case, X ,X correspond to the masked and noisy views X ,X . MacDiff
1 2 m t
doesnotdirectlyextractrepresentationfromX .ItinsteadoptimizesI(Z ,X )=
t 1 2
I(Z,X ) as a tighter lower bound of I(X ;X ), which avoids feature collapse.
t 1 2
Moreover, the second term aims to contain more information about X in
Z that is complementary to X . CL is proved to suffer from the discriminative
t
information overfitting problem [30] that the model is biased to extract only
the discriminative information of two views. However, optimizing I(X;Z|X )
t
requires the representation to contain more task-relevant information that is
not shared between views. Therefore, the generative objective of MacDiff can
be viewed as an improvement of CL methods and provides a better theoretical
guarantee of downstream performance as discussed next.
Relation to Downstream Performance. The Bayes error rate P is the
e
lowest error that can be achieved by any classifier trained on the given data
representations, defined as P =1−E [max p(y|z)], where Z denotes
e z∼p(Z) y∈Y
the representation and Y denotes the labels. Then, we can prove that (refer to
the supplementary material):
Theorem 1. (Bayes Error Rate of Representations) For arbitrary data repre-
sentation distribution Z, and V denotes a certain view of the data, its Bayes
error rate can be estimated as:
P ≤1−e−(H(Y)−I(Z;Y)) (11)
e
≤1−e−(H(Y)−I(Z;Y;V)−I(Z;Y|V)). (12)
WesetV =X inthistheorem.Thus,ourgoalistoincreasethetermsI(Z;Y;X )
t t
and I(Z;Y|X ). These two terms are bounded by I(Z;X ) and I(Z;X|X ) re-
t t t
spectively. As shown in Eq. (9), the MacDiff objective directly increases both
terms. Note that the contrastive learning objective merely optimizes the first
term I(Z;X ), which explains the improved downstream performance of our
t
method compared to contrastive-only methods.
3.4 Diffusion-Based Data Augmentation
For generative self-supervised methods (e.g., MAEs), only the encoders are uti-
lizedfordownstreamtasks,whiletherestofthemodelsarecompletelydiscarded.Unified Skeleton Modeling with Masked Conditional Diffusion 9
In MacDiff, we propose that the denoising decoder can be used for data aug-
mentation when fine-tuning the encoder. We focus on scenarios where labeled
data is scarce, which is quite common in reality, especially for skeletons.
To synthesize training data that follows the real-data distribution q(x,y),
generativemethodshavetobelabel-preserving.Existingmethodsachievethisby
leveragingpre-trainedtext-guideddiffusion[56,60].However,oursemi-supervised
setting does not provide text guidance, which forms a more general challenge of
synthesizing label-preserving samples given labeled ones.
To this end, we propose diffusion-based data augmentation based on the
assumption that samples generated with the same representation guidance are
label-consistent.Specifically,wetaketwosteps:(1)pre-calculatetherepresenta-
tions of labeled samples, and (2) generate new samples with the decoder condi-
tioned on these representations. Nevertheless, sampling from Gaussian noise is
time-consuming and thus can only be performed before training. We find that
one-step denoising from some medium timestep t yields similar effects with
s
smallercomputationalcost,whichallowsforgeneratingdiverseaugmenteddata
at different epochs.
4 Experiments
4.1 Experimental Setup
Datasets. Forevaluation,ourexperimentsareconductedonthefollowingthree
datasets: NTU RGB+D 60 dataset (NTU 60) [43], NTU RGB+D 120 dataset
(NTU 120) [29] and PKU Multi-Modality Dataset (PKUMMD) [28]. All three
datasets use 25 joints to represent the human body.
NTU 60 is a large-scale dataset for human action recognition with 60 cate-
gories and 56,578 videos. We follow the widely-used evaluation protocols, cross-
subject (xsub) and cross-view (xview). The former uses action sequences from
half of the 40 subjects for training, and the rest for testing. The latter uses
sequencesfromcamera2,3fortrainingandsequencesfromcamera1fortesting.
NTU 120 is an extension of NTU 60, with 120 categories and 114,480 videos
from 106 subjects. Evaluation protocols on NTU 120 are cross-subject (xsub)
and cross-setup (xset). Specifically, xset divides sequences into 32 setups based
on the camera distance and background, half of which are used for training and
the rest for testing.
PKUMMDcoversamulti-modality3Dunderstandingofhumanactions,with
52 categories and almost 20,000 instances. PKUMMD is divided into part I and
II,andpartIIismorechallengingduetothenoisecausedbyviewvariation.We
split training and testing sets according to the cross-subject protocol.
Implementation Details. The input sequence of 300 frames is cropped and
interpolated to 120 frames, and the patch length l = 4. Apart from random
crop, we use random rotation and small Gaussian noise (σ = 0.005) as data
augmentation. Note that the small noise is only added to encoder inputs. For
Transformer architecture, the embedding dimension is 256, the MLP hidden10 L. Wu et al.
Table1:ComparisonoflinearevaluationresultsonNTU60,NTU120,andPKUMMD
datasets. 3s- represents the ensemble results of joint(J), bone(B) and motion(M)
streams. Bold and underlined indicate the best and second best results, respectively.
The same notation applies throughout.
NTU60 NTU120
Method Stream PKUI
xsub xview xsub xset
3s-CrosSCLR[25] J+M+B 77.8 83.4 67.9 66.7 84.9
3s-AimCLR[49] J+M+B 78.9 83.8 68.2 68.8 87.4
3s-SkeleMixCLR[49] J+M+B 82.7 87.1 70.5 70.7 91.1
3s-ActCLR[27] J+M+B 84.3 88.8 74.3 75.7 -
3s-CPM[57] J+M+B 83.2 87.0 73.0 74.0 90.7
LongTGAN[62] J 39.1 48.1 - - 67.7
MS2L[26] J 52.6 - - - 64.9
AS-CAL[40] J 58.5 64.8 48.6 49.2 -
ISC[48] J 76.3 85.2 67.1 67.9 80.9
GL-Transformer[21] J 76.3 83.8 66.0 68.7 -
CMD[33] J 79.4 86.9 70.3 71.5 -
PCM3 [58] J 83.9 90.4 76.5 77.5 -
SkeletonMAE[55] J 74.8 77.7 72.5 73.5 82.8
MAMP[32] J 84.9 89.1 78.6 79.1 92.2
MacDiff (Ours) J 86.4 91.0 79.4 80.2 92.8
dimensionis1024,andthenumberofheadsinMSAis8.Bydefault,theencoder
anddecoderhave8layersand5layers,respectively.Wetrainourmodelonfour
NVIDIA TITAN Xp GPUs with a total batch size of 128 for 500 epochs. The
AdamWoptimizerisadoptedwiththelearningratedecreasingfrom1e-3to1e-5.
4.2 Self-Supervised Learning Evaluation
Linear Evaluation. Inthelinearevaluationprotocol,alinearclassifierispost-
attached to the encoder to classify the learned representations. We fix the en-
coderandtraintheclassifierfor100epochswiththeSGDoptimizerandalearn-
ingrateof0.1.WecompareMacDiffwithlatestmethods,withactionrecognition
accuracy reported as a measurement.
As shown in Tab. 1, our method surpasses high-performing reconstruction-
based methods, e.g., SkeletonMAE [55] and MAMP [55]. With only the joint
stream, our method also outperforms multi-stream contrastive learning meth-
ods, e.g. 3s-AimCLR [49], 3s-CMD [33] and 3s-ActCLR [27]. The result demon-
stratesthatMacDiffcapturesthespatial-temporalcorrelationofskeletonsbetter
than existing methods, and also confirms our theoretical analysis that MacDiff
provides a better framework than contrastive-only paradigms.
Supervised Fine-tuning Evaluation. Inthefine-tuningevaluationprotocol,
weattachanMLPheadtothepre-trainedencoderandtrainthewholemodelfor
another100epochswiththeAdamWoptimizerandthelearningratedecreasing
from 3e-4 to 1e-5.
As shown in Tab. 2, our method yields comparable results to MAMP and
outperforms other existing methods. We point out that our performance gap
in the fine-tuning protocol with MAMP is trivial since we share the same en-
coderarchitectureandthelearnedrepresentationisdisruptedduringfine-tuning.Unified Skeleton Modeling with Masked Conditional Diffusion 11
Table 2: Comparison of supervised fine- Table 3: Comparison of transfer learn-
tuningevaluationresultsonNTU60xsub ing results on PKUMMD II dataset. The
andxviewdatasets.Thebottomfourrows source datasets are NTU 60, NTU 120
are unified models, among which UPS is and PKUMMD I. All datasets use the
a supervised method. xsub split.
NTU60 ToPKUII
Method Backbone Method
xsub xview NTU60 NTU120 PKUI
CrosSCLR[25] 3s-ST-GCN 86.2 92.5 LongTGAN[62] 44.8 - 43.6
AimCLR[49] 3s-ST-GCN 86.9 92.8 MS2L[26] 45.8 - 44.1
ActCLR[27] 3s-ST-GCN 88.2 93.9 ISC[48] 51.1 52.3 45.1
MCC[46] 2s-AGCN 89.7 96.3 CMD[33] 56.0 57.0 -
SkeletonMAE[55] Transformer 88.5 94.7 SkeletonMAE[55] 58.4 61.0 62.5
MAMP[55] Transformer 93.1 97.5 MAMP[32] 70.6 73.2 70.1
MacDiff (Ours) 72.2 73.4 71.4
Hi-TRS[7] 3s-Transformer 90.0 95.7
MotionBERT[63] DSTformer 93.0 97.2
UPS[14] Transformer 92.6 97.0
MacDiff (Ours) Transformer 92.7 97.3
Table 4: Comparison of semi-supervised fine-tuning results on NTU 60 datasets. By
defaultwesetthestartingtimestept =500andaugment-to-realratioλ=2.0,0.5,0.25
s
for p = 1%,2%,10%. The results are averaged over 5 runs. For MacDiff, results with
and without diffusion-based augmentation are both reported.
NTU60xsub NTU60xview
Method
1% 2% 10% 1% 2% 10%
ISC[48] 35.7 - 65.1 38.1 - 72.5
3s-AimCLR[49] 54.8 - 78.2 54.3 - 81.6
3s-CMD[33] 55.6 - 79.0 55.5 - 82.4
CPM[57] 56.7 - 73.0 57.5 - 77.1
PCM3 [58] 53.8 - 77.1 53.1 - 82.8
SkeletonMAE[55] 54.4 - 80.6 54.6 - 83.5
MAMP[32] 66.0 80.3 88.0 68.7 83.5 91.5
MacDiff w/o aug 65.6 80.7 88.2 77.3 84.1 92.5
MacDiff 72.0 82.1 89.2 79.2 85.6 93.1
Meanwhile, MacDiff outperforms other unified models such as Hi-TRS [7], Mo-
tionBERT [63] and UPS [14].
Transfer Learning Evaluation. In the transfer learning evaluation protocol,
the backbones are pre-trained on a source dataset and evaluated on a target
dataset following linear evaluation protocol to keep the backbone intact.
AsshowninTab.2,ourmethodachievessignificanttransferlearningperfor-
manceonthechallengingPKUMMDII,demonstratingthegeneralizationability
and robustness of our method.
4.3 Semi-Supervised Fine-tuning with Diffusion-based Data
Augmentation
We next evaluate the effectiveness of the diffusion-based data augmentation
in scenarios with limited labeled data. We report results following the semi-
supervised protocol, which is consistent with the fine-tuning protocol except12 L. Wu et al.
that only a proportion p of the training set is used. We evaluate our methods
when p=1%,2%,10%.
We explore different augment-to-real ratios λ, defined as the ratio of aug-
mented data to real data, for different proportions of training data. We em-
pirically find that the optimal augment-to-real ratio λ declines as p increases.
Intuitively,whenlabeleddataisseverelyscarce,augmenteddatathatfallsinthe
neighborhood of labeled data representations helps the classifier to learn more
robustboundaries.However,asthecardinalityofthetrainingsetincreases,since
the dataset inherently contains ambiguous class boundaries (e.g., "phone call"
and "play with phone/tablet" in NTU 60), generating new data around them
further confuse the classifier. See more experiments in Sec. 4.5.
As shown in Tab. 4, with the aid of augmented training samples, MacD-
iff outperforms state-of-the-art MAMP by 6.0%, 1.8%, 1.2% in NTU 60 xsub,
and 10.5%, 2.1%, 1.6% in NTU 60 xview within the three settings, respectively.
Compared with encoder-only MacDiff, the augmentation brings significant per-
formance gain of 6.4%, 1.4%, 1.0% in NTU 60 xsub, and 1.9%, 1.5% , 0.6% in
NTU 60 xview.
4.4 Generative Evaluation
In this section, we implement MacDiff for motion reconstruction and motion
generation tasks. We compare our method with reconstruction-based method
SkeletonMAE[55]anddiffusion-basedmethodsDDIM[45]andMDM[47].Note
that MAMP [32] cannot be applied for either task because it predicts the nor-
malized motion. For fair comparison, all methods are implemented with our
Transformer decoder architecture. In addition, we also implement the original
MDM (denoted as MDM-orig) with temporal-only attention, 8 layers, and 512
hiddendimensions.Pleasefindimplementationdetailsinthesupplementaryma-
terial. All experiments are conducted on the testing set of NTU 60 xsub.
Motion Reconstruction. Real-world skeleton data suffer from occlusions, re-
sultinginincompletesequences.Weevaluatemotionreconstructionintwotypes
of occlusions: (1) random consecutive frames, and (2) a random body part from
{trunk, left arm, right arm, left leg, right leg}, following the division of previ-
ous works. We follow a diffusion-based inpainting paradigm [31] for DDIM and
MacDiff. The MacDiff decoder is fine-tuned for another 100 epochs with the
encoder fixed and only global representations.
Wereport MeanPerJointPosition Error (MPJPE)as ourmetric.As shown
in Tab. 5, MacDiff is capable of recovering incomplete skeletons as a unified
framework and surpasses reconstruction-based SkeletonMAE and DDIM.
Motion Generation. We utilize the MacDiff decoder for unconditional mo-
tiongeneration.WereportfourmetricsFID,KID,diversity,andprecision/recall.
Please refer to the supplementary material for detailed implementation of these
metrics. Note that reconstruction-based methods are not capable of uncondi-
tionalgeneration.AsshowninTab.6,MacDiffachievescomparableresultswith
DDIM and MDM.Unified Skeleton Modeling with Masked Conditional Diffusion 13
Table 5: Comparison of motion reconstruction results on NTU 60 xsub. MPJPE is
reported on four occlusion settings: 10 frames, 20 frames, 40 frames and 1 body part.
10frames 20frames 40frames 1bodypart
SkeletonMAE[55] 0.191 0.221 0.255 0.319
DDIM[45] 0.041 0.087 0.205 0.251
MacDiff (Ours) 0.033 0.089 0.147 0.241
Table 6: Comparison of motion generation results on NTU 60 xsub. MacDiff is eval-
uated both with and without fine-tuning.
Precision↑
Method Prediction FID↓ KID↓ Diversity↑
Recall↑
Realdata - 0.05 0.0001 2.09 0.886,0.908
MDM-orig[47] x0 3.49 0.0328 1.51 0.047,0.030
MDM[47] x0 1.06 0.0059 1.43 0.828,0.070
DDIM[45] ϵ 1.32 0.0082 1.89 0.576,0.537
MacDiff (Ours) ϵ 1.52 0.0085 1.96 0.414,0.289
+ fine-tune ϵ 1.30 0.0070 1.97 0.460,0.505
Qualitative Results. We provide visualization of motion reconstruction, un-
conditional motion generation and one-step denoising results (for data augmen-
tation) in the supplementary material.
4.5 Ablation Study
Masking Strategy and Ratio. In Tab. 7, we compare the results of differ-
entmaskingstrategies,includingrandommasking,temporal-onlymasking,tube
masking[50],spatial-temporalmaskingandmotion-awarerandommasking[32].
For tube masking, the tube length is set to 5. For spatial-temporal masking, we
keep 8 out of 25 joints and 10 out of 30 temporal patches. For motion-aware
masking, we follow the implementation of MAMP. The results show that the
simple random masking works best as a spacetime-agnostic masking. We also
compare different masking ratios and find a high masking ratio of 90% works
best, which coincides with the findings in the video field.
NoiseSchedule. Weconstructaseriesofnoiseschedulesaslinearcombinations
of the inverse-cosine [20] and cosine schedule controlled by τ (see definition in
the supplementary material). τ = 1 and τ = −1 represents the inverse-cosine
and cosine schedule, respectively. We compare these schedules with the widely
used linear schedule. As shown in Tab. 8, cosine-based schedule performs better
than the linear schedule, and the performance peaks at τ = 1, indicating that
medium noise levels are preferred for representation learning.
Diffusion-basedDataAugmentation. Fortheablationstudyofourdiffusion-
based data augmentation, We compare the effects of different starting timestep
t and augment-to-real ratio λ. As shown in Tab. 9, the performance gain is
s
highest when t = 500. Intuitively, an overly large t may introduce too much
s s
noise since we implement one-step denoising, while an overly small t fails to
s14 L. Wu et al.
Table 7: Ablation study on the masking Table 8: Ablation study on the noise
strategyandthemaskingratio.Wereport schedule. We report results on NTU 60
results on NTU 60 xsub under the linear xsubunderthelinearevaluationprotocol.
evaluation protocol.
NTU60
NoiseSchedule
NTU60 NTU60 xsub
Strategy Ratio
xsub xsub τ =1.5 85.7
Temporal 84.1 0 79.3 τ =1.0(Inverse) 86.4
Tube 83.1 50% 82.7 τ =0.5 85.8
Spatial-temporal 85.3 80% 83.8 τ =−0.5 85.2
Random 86.4 90% 86.4 τ =−1.0(Cosine) 83.8
Motion-aware 85.5 95% 83.6 Linear 83.4
Table9:Ablationstudyonthestartingtimestept Table 10: Ablation study on
s
and augment-to-real ratio λ of diffusion-based aug- the depth of the decoder. We
mentation.WereportresultsonNTU60xsubunder reportresultsonNTU60xsub
thesemi-supervised1%and10%protocol.t =0or under the linear evaluation
s
λ=0 means results without augmentation. protocol.
ts semi1% λ semi1% semi10%
Depth
NTU60
xsub
0 65.6 0 65.6 88.2
100 66.4 0.25 67.8 89.1 2 84.8
300 71.4 0.5 68.2 89.2 3 86.4
500 72.0 1.0 70.0 88.5 4 86.0
900 68.7 2.0 72.0 - 5 85.9
provide sufficient strength of augmentation. For larger proportion of training
data, the optimal λ is smaller, which is consistent with our analysis in Sec. 4.3.
Decoder Design. Tab. 10 reports the effects of different decoder depths on
representation learning. The best result is achieved with a depth of 3, but gen-
erally our method is robust to the decoder depth. Therefore, we adopt a depth
of 5 by default considering the generative capability.
5 Conclusion
WepresentMacDiff,anovelgenerativeframeworktoenhanceskeletonrepresen-
tationlearning forhumanaction understanding.By trainingadiffusion decoder
guidedbytherepresentationfromtheencoder,theencoderisenforcedtocontain
richsemanticsintherepresentation.WeformulatetheobjectiveofMacDiffasan
improvement of the contrastive learning objective, theoretically demonstrating
the effectiveness of the proposed framework.
Acknowledgements
ThisworkwassupportedinpartbytheNationalNaturalScienceFoundationof
ChinaunderGrantNo.62172020,andinpartbytheKeyLaboratoryofScience,
TechnologyandStandardinPressIndustry(KeyLaboratoryofIntelligentPress
Media Technology).Unified Skeleton Modeling with Masked Conditional Diffusion 15
References
1. Alex Nichol, P.D.: Improved denoising diffusion probabilistic models. arXiv
preprint arXiv:2102.09672 (2021)
2. Camara,F.,Bellotto,N.,Cosar,S.,Nathanael,D.,Althoff,M.,Wu,J.,Ruenz,J.,
Dietrich,A.,Fox,C.W.:PedestrianmodelsforautonomousdrivingpartI:low-level
models,fromsensingtotracking.IEEETransactionsonIntelligentTransportation
Systems 22(10), 6131–6151 (2020)
3. Chen, L.H., Zhang, J., Li, Y., Pang, Y., Xia, X., Liu, T.: HumanMAC: Masked
motioncompletionforhumanmotionprediction.In:Int.Conf.Comput.Vis.(2023)
4. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-
trastive learning of visual representations. In: Int. Conf. Mach. Learn. (2020)
5. Chen,X.,Jiang,B.,Liu,W.,Huang,Z.,Fu,B.,Chen,T.,Yu,J.,Yu,G.:Executing
yourcommandsviamotiondiffusioninlatentspace.In:IEEEConf.Comput.Vis.
Pattern Recog. (2023)
6. Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum con-
trastive learning. arXiv preprint arXiv:2003.04297 (2020)
7. Chen,Y.,Zhao,L.,Yuan,J.,Tian,Y.,Xia,Z.,Geng,S.,Han,L.,Metaxas,D.N.:
Hierarchicallyself-supervisedtransformerforhumanskeletonrepresentationlearn-
ing. In: Eur. Conf. Comput. Vis. (2022)
8. Chen,Z.,Liu,H.,Guo,T.,Chen,Z.,Song,P.,Tang,H.:Contrastivelearningfrom
spatio-temporalmixedskeletonsequencesforself-supervisedskeleton-basedaction
recognition. arXiv preprint arXiv:2207.03065 (2022)
9. Ci,H.,Wu,M.,Zhu,W.,Ma,X.,Dong,H.,Zhong,F.,Wang,Y.:GFPose:Learning
3D human pose prior with gradient fields. In: IEEE Conf. Comput. Vis. Pattern
Recog. (2023)
10. Ci, Y., Wang, Y., Chen, M., Tang, S., Bai, L., Zhu, F., Zhao, R., Yu, F., Qi, D.,
Ouyang, W.: UniHCP: A unified model for human-centric perceptions. In: IEEE
Conf. Comput. Vis. Pattern Recog. (2023)
11. Dhariwal,P.,Nichol,A.:DiffusionmodelsbeatGANsonimagesynthesis.In:Adv.
Neural Inform. Process. Syst. (2021)
12. Dong,Y.,Cordonnier,J.B.,Loukas,A.:Attentionisnotallyouneed:Pureatten-
tion loses rank doubly exponentially with depth. arXiv preprint arXiv:2103.03404
(2021)
13. Flaborea, A., Collorone, L., di Melendugno Stefano D’Arrigo, G.M.D., Prenkaj,
B.,Galasso,F.:Multimodalmotionconditioneddiffusionmodelforskeleton-based
video anomaly detection. In: Int. Conf. Comput. Vis. (2023)
14. Foo, L.G., Li, T., Rahmani, H., Ke, Q., Liu, J.: Unified pose sequence modeling.
In: IEEE Conf. Comput. Vis. Pattern Recog. (2023)
15. Goldfeld, Z., Polyanskiy, Y.: The information bottleneck problem and its applica-
tions in machine learning. IEEE JSAIT 1(1), 19–38 (2020)
16. Gong,J.,Foo,L.G.,Fan,Z.,Ke,Q.,Rahmani,H.,Liu,J.:DiffPose:Towardmore
reliable 3D pose estimation. In: IEEE Conf. Comput. Vis. Pattern Recog. (2023)
17. He,K.,Chen,X.,Xie,S.,Li,Y.,Dollár,P.,Girshick,R.:Maskedautoencodersare
scalable vision learners. In: IEEE Conf. Comput. Vis. Pattern Recog. (2022)
18. Ho,J.,Jain,A.,bbeel,P.:Denoisingdiffusionprobabilisticmodels.arXivpreprint
arXiv:2006.11239 (2020)
19. Huang, W., Rong, Y., Xu, T., Sun, F., Huang, J.: Tackling over-smoothing for
general graph convolutional networks. arXiv preprint arXiv:2008.09864 (2020)16 L. Wu et al.
20. Hudson,D.A.,Zoran,D.,Malinowski,M.,Lampinen,A.K.,Jaegle,A.,McClelland,
J.L., Matthey, L., Hill, F., Lerchner, A.: SODA: Bottleneck diffusion models for
representation learning. arXiv preprint arXiv:2311.17901 (2023)
21. Kim, B., Chang, H.J., Kim, J., , Choi, J.Y.: Global-local motion transformer for
unsupervised skeleton-based action learning. In: Eur. Conf. Comput. Vis. (2022)
22. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 (2013)
23. KunSu,XiulongLiu,E.S.:PREDICT&CLUSTER:Unsupervisedskeletonbased
action recognition. arXiv preprint arXiv:1911.12409 (2019)
24. Lee,J.,Ahn,B.:Real-timehumanactionrecognitionwithalow-costRGBcamera
and mobile robot platform. Sensors 20(10), 2886 (2020)
25. Li, L., Wang, M., Ni, B., Wang, H., Yang, J., Zhang, W.: 3D human action rep-
resentation learning via cross-view consistency pursuit. In: IEEE Conf. Comput.
Vis. Pattern Recog. (2021)
26. Lin, L., Song, S., Yang, W., Liu, J.: MS2L: Multi-task self-supervised learning for
skeleton based action recognition. In: ACM Int. Conf. Multimedia (2020)
27. Lin, L., Zhang, J., Liu, J.: Actionlet-dependent contrastive learning for unsuper-
vised skeleton-based action recognition. In: IEEE Conf. Comput. Vis. Pattern
Recog. (2023)
28. Liu, J., Song, S., Liu, C., Li, Y., , Hu, Y.: A benchmark dataset and comparison
study for multi-modal human action analytics. In: ACM TOMM (2020)
29. Liu,J.,Shahroudy,A.,Perez,M.,Wang,G.,Duan,L.Y.,Kot,A.C.:NTURGB+D
120: A large-scale benchmark for 3D human activity understanding. IEEE Trans.
Pattern Anal. Mach. Intell. (2019)
30. Liu,Z.,Li,B.,Han,C.,Guo,T.,Nie,X.:Maskedreconstructioncontrastivelearn-
ingwithinformationbottleneckprinciple.arXivpreprintarXiv:2211.09013(2022)
31. Lugmayr, A., Danelljan, M., Romero, A., Yu, F., Gool, R.T.L.V.: RePaint: In-
painting using denoising diffusion probabilistic models. In: IEEE Conf. Comput.
Vis. Pattern Recog. (2022)
32. Mao, Y., Deng, J., Zhou, W., Fang, Y., Ouyang, W., Li, H.: Masked motion pre-
dictors are strong 3D action representation learners. In: Int. Conf. Comput. Vis.
(2023)
33. Mao, Y., Zhou, W., Lu, Z., Deng, J., Li, H.: CMD: Self-supervised 3D action rep-
resentation learning with cross-modal mutual distillation. In: Eur. Conf. Comput.
Vis. (2022)
34. Meng,C.,He,Y.,Song,Y.,Song,J.,Wu,J.,Zhu,J.Y.,Ermon,S.:SDEdit:Guided
image synthesis and editing with stochastic differential equations. arXiv preprint
arXiv:2108.01073 (2021)
35. Nam,J.,Lee,G.,Kim,S.,Kim,H.,Cho,H.,Kim,S.,Kim,S.:Diffusionmodelfor
dense matching. In: Int. Conf. Learn. Represent. (2024)
36. Namekata, K., Sabour, A., Fidler, S., Kim, S.W.: EmerDiff: Emerging pixel-level
semantic knowledge in diffusion models. In: Int. Conf. Learn. Represent. (2024)
37. van den Oord, A., Vinyals, O., koray kavukcuoglu: Neural discrete representation
learning. In: Adv. Neural Inform. Process. Syst. (2017)
38. Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: Int. Conf.
Comput. Vis. (2023)
39. Preechakul, K., Chatthee, N., Wizadwongsa, S., Suwajanakorn, S.: Diffusion au-
toencoders: Toward a meaningful and decodable representation. In: IEEE Conf.
Comput. Vis. Pattern Recog. (2022)Unified Skeleton Modeling with Masked Conditional Diffusion 17
40. Rao,H.,Xu,S.,Hu,X.,Cheng,J.,Hu,B.:Augmentedskeletonbasedcontrastive
action learning with momentum lstm for unsupervised action recognition. In: In-
formation Sciences (2021)
41. Rombach,R.,Blattmann,A.,Lorenz,D.,Esser,P.,Ommer,B.:High-resolutionim-
agesynthesiswithlatentdiffusionmodels.arXivpreprintarXiv:2112.10752(2021)
42. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour,
S.K.S., Ayan, B.K., Mahdavi, S.S., Lopes, R.G., Salimans, T., Ho, J., Fleet, D.J.,
Norouzi,M.:Photorealistictext-to-imagediffusionmodelswithdeeplanguageun-
derstanding. arXiv preprint arXiv:2205.11487 (2022)
43. Shahroudy,A.,Liu,J.,Ng,T.T.,,Wang,G.:NTURGB+D:Alargescaledataset
for3Dhumanactivityanalysis.In:IEEEConf.Comput.Vis.PatternRecog.(2016)
44. Shwartz-Ziv, R., LeCun, Y.: To compress or not to compress - self-supervised
learningandinformationtheory:Areview.arXivpreprintarXiv:2304.09355(2023)
45. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.arXivpreprint
arXiv:2010.02502 (2020)
46. Su,Y.,Lin,G.,Wu,Q.:Self-supervised3Dskeletonactionrepresentationlearning
with motion consistency and continuity. In: Int. Conf. Comput. Vis. (2021)
47. Tevet,G.,Raab,S.,Gordon,B.,Shafir,Y.,Cohen-or,D.,Bermano,A.H.:Human
motion diffusion model. In: Int. Conf. Learn. Represent. (2023)
48. Thoker, F.M., Doughty, H., , Snoek, C.G.: Skeleton-contrastive 3D action repre-
sentation learning. In: ACM Int. Conf. Multimedia (2021)
49. Tianyu, G., Hong, L., Zhan, C., Mengyuan, L., Tao, W., Runwei, D.: Contrastive
learning from extremely augmented skeleton sequences for self-supervised action
recognition. In: AAAI (2022)
50. Tong,Z.,Song,Y.,Wang,J.,Wang,L.:VideoMAE:Maskedautoencodersaredata-
efficient learners for self-supervised video pre-training. In: Adv. Neural Inform.
Process. Syst. (2022)
51. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Łukasz
Kaiser, Polosukhin, I.: Attention is all you need. In: Adv. Neural Inform. Process.
Syst. (2017)
52. Wang, H., Guo, X., Deng, Z.H., Lu, Y.: What makes for good representations for
contrastive learning. In: Adv. Neural Inform. Process. Syst. (2020)
53. Wang,H.,Guo,X.,Deng,Z.,Lu,Y.:Rethinkingminimalsufficientrepresentation
in contrastive learning. In: IEEE Conf. Comput. Vis. Pattern Recog. (2022)
54. Wang, X., Fang, Z., Li, X., Li, X., Chen, C., Liu, M.: Skeleton-in-context:
Unified skeleton sequence modeling with in-context learning. arXiv preprint
arXiv:2312.03703 (2023)
55. Wu, W., Hua, Y., Zheng, C., Wu, S., Chen, C., Lu, A.: SkeletonMAE: Spatial-
temporal masked autoencoders for self-supervised skeleton action recognition.
arXiv preprint arXiv:2209.02399 (2022)
56. Yuan,J.,Zhang,J.,Sun,S.,Torr,P.,Zhao,B.:Real-Fake:Effectivetrainingdata
synthesis through distribution matching. In: Int. Conf. Learn. Represent. (2024)
57. Zhang, H., Hou, Y., Zhang, W., Li, W.: Contrastive positive mining for unsuper-
vised 3D action representation learning. In: Eur. Conf. Comput. Vis. (2022)
58. Zhang, J., Lin, L., Liu, J.: Prompted contrast with masked motion modeling: To-
wards versatile 3D action representation learning. In: ACMMM (2023)
59. Zhang, M., Cai, Z., Pan, L., Hong, F., Guo, X., Yang, L., Liu, Z.: MotionDiffuse:
Text-driven human motion generation with diffusion model. IEEE Trans. Pattern
Anal. Mach. Intell. pp. 1–15 (2024)
60. Zhang,Y.,Zhou,D.,Hooi,B.,Wang,K.,Feng,J.:Expandingsmall-scaledatasets
with guided imagination. In: Adv. Neural Inform. Process. Syst. (2023)18 L. Wu et al.
61. Zhang,Z.,Zhao,Z.,Lin,Z.:Unsupervisedrepresentationlearningfrompre-trained
diffusion probabilistic models. In: Adv. Neural Inform. Process. Syst. (2022)
62. Zheng, N., Wen, J., Liu, R., Long, L., Dai, J., , Gong, Z.: Unsupervised represen-
tationlearningwithlong-termdynamicsforskeletonbasedactionrecognition.In:
AAAI (2018)
63. Zhu, W., Ma, X., Liu, Z., Liu, L., Wu, W., Wang, Y.: MotionBERT: A unified
perspective on learning human motion representations. In: Int. Conf. Comput.
Vis. (2023)