Reducing Leximin Fairness to Utilitarian Optimization
Eden Hartman1, Yonatan Aumann1, Avinatan Hassidim1,2, Erel Segal-Halevi3
1Bar-Ilan University, Israel
2Google, Israel
3Ariel University, Israel
eden.r.hartman@gmail.com, aumann@cs.biu.ac.il, avinatan@cs.biu.ac.il, erelsgl@gmail.com
Abstract ever, is NP-hard (Bansal and Sviridenko 2006), even in
this relatively simple case.
Two prominent objectives in social choice are util- Inthispaper,weshowthatknowinghowtoefficiently
itarian - maximizing the sum of agents’ utilities,
maximize the utilitarian welfare is sufficient in order to
andleximin -maximizingthesmallestagent’sutil-
find a fair leximin solution.
ity, then the second-smallest, etc. Utilitarianism is
typically computationally easier to attain but is
generally viewed as less fair. This paper presents Contributions. The core contribution of this paper
ageneralreductionschemethat,givenautilitarian is a general protocol that, when provided with a proce-
solver, produces a distribution over outcomes that dure for optimizing the utilitarian welfare (for a given
is leximin in expectation. Importantly, the scheme problem), outputs a solution that optimizes the ex-
is robust in the sense that, given an approximate pected leximin welfare (for the same problem). By ex-
utilitarian solver, it produces an outcome that is pected leximin we mean a distribution over determin-
approximately-leximin (in expectation) - with the
istic solutions, for which the expectations of the play-
same approximation factor. We apply our scheme
ers’ utilities is leximin optimal. Crucially, our proto-
toseveralsocialchoiceproblems:stochasticalloca-
col extends to approximations, in the following sense:
tions of indivisible goods, giveaway lotteries, and
fair lotteries for participatory budgeting. given an approximate solver for the utilitarian welfare,
the protocol outputs a solutions that approximates the
leximin optimal, and the same approximation factor is
1 Introduction preserved. In all, with our protocol at hand, optimiz-
ing expected leximin welfare is no more difficult than
In social choice, the goal is to find the best choice for
optimizing utilitarian welfare.
society, but ’best’ can be defined in many ways. Two
frequent,andoftencontrastingdefinitionsaretheutili- We demonstrate the significance of this reduction by
tarian best, which focuses on the total welfare (i.e., the applying it to three social choice problems. First, we
sumofutilities);andtheegalitarian best,whichfocuses consider the classic problem of allocations of indivisi-
on maximizing the least utility. The leximin best gen- ble goods, where one seeks to fairly distribute a a set of
eralizes the egalitarian one. It first aims to maximize indivisible goods among a set of heterogeneous agents.
the least utility; then, among all options that maxi- Maximizing the utilitarian welfare in this case is well-
mizetheleastutility,itchoosestheonethatmaximizes studied.Usingourreduction,thepreviouslymentioned
the second-smallest utility, among these — the third- greedy algorithm for agents with additive utilities, al-
smallest utility, and so forth. Leximin is often the so- lows us to achieve a leximin optimal lottery over the
lution of choice in social choice applications, and fre- goodsinpolynomialtime.Forsubmodularutilities,ap-
quentlyused(Bei,Lu,andSuksompong2022;Freeman proximating leximin to a factor better than (1− 1) is
e
et al. 2019; Flanigan et al. 2024; Cheng et al. 2023). NP-hard. However, by applying our reduction, existing
approximation algorithms forutilitarian welfarecanbe
Calculating the Optimal Choice. Calculating a leveraged to prove that a 0.5-approximation can be ob-
choice that maximizes utilitarian welfare is often eas- taineddeterministically,whilethebest-possible(1−1)-
e
ierthanfindingonethatmaximizesegalitarianwelfare, approximation can be obtained with high probability.
while finding one that is leximin optimal is typically Second, we consider the problem of giveaway lotteries
even more complex. For example, when allocating indi- (ArbivandAumann2022),wherethereisaneventwith
visiblegoodsamongagentswithadditiveutilities,find- limited capacity and groups wish to attend, but only-if
ingachoice(inthiscase,anallocation)thatmaximizes theycanallbeadmittedtogether.Maximizingtheutil-
theutilitarianwelfarecanbedonebygreedilyassigning itarianwelfareinthissettingcanbeseenasaknapsack
each item to the agent who values it most. Finding an problem, for which there is a well-known FPTAS (fully
allocation that maximizes the egalitarian welfare, how- polynomial-time approximation scheme). Using our re-
4202
peS
61
]TG.sc[
1v59301.9042:viXraduction, we obtain an FPTAS for leximin. Lastly, we 2 Preliminaries
consider the problem of fair lotteries for participatory We denote the set {1,...,n} by [n] for n∈N.
budgeting,whereoneseekstofindafairlotteryoverthe
possiblebudgetallocations.Whenagentshaveadditive MathematicalProgramming. Throughoutthepa-
utilities, maximizing the utilitarian welfare can also be per, we frequently use mathematical programming to
interpreted as a knapsack problem (albeit in a slightly define optimization problems. A program is character-
different way), which allows us to provide an FPTAS ized by the following three elements. (1) Constraints:
for leximin. used to define the set of feasible solutions, which forms
a subset of Rm for some m ∈ N. (2) Type: the pro-
Organization. Section 3 introduces the model and gram can be either a maximization or minimization
required definitions. Then, Sections 4-11 describe the program. (3) Objective function: assigns an objective
reduction. The applications are shown in Section 12. valuetoeachfeasiblesolution.Thegoalistofindafea-
Lastly, Section 13 concludes with some future work di- sible solution with an optimal objective value (either
rections. Most proofs are deferred to the appendix. maximal or minimal, depending on the problem type).
1.1 Related Work Notations. For a given program P, the feasible re-
gion, denoted by F(P), represents the set of vectors
Recently, there has been a wealth of research focused
that satisfy all the constraints of P. We say that a vec-
on finding a fair leximin lottery for specific problems.
tor v ∈ F(P) is a feasible solution for P (interchange-
Examples include algorithms proposed for Representa-
ably: a solution for P or feasible for P), and denote its
tiveCohortSelection(Henzingeretal.2022),Giveaway
objectivevalueaccordingtotheobjectivefunctionofP
Lotteries(ArbivandAumann2022),AllocatingUnused
by obj(P,v).
Classrooms(Kurokawa,Procaccia,andShah2018),and
Selecting Citizens’ Assemblies (Flanigan et al. 2021).
3 Model and Definitions
Thispaper,incontrast,providesageneralprotocolthat
can be applied to a wide range of problems. ThesettingpostulatesasetofnagentsN ={1,...,n},
Alongside these, many papers describe general algo- and a set of deterministic options - S (these represent
rithms for exact leximin optimization (Ogryczak 1997; the possible deterministic allocations - in the fair di-
Ogryczak, Pi´oro, and Tomaszewski 2004; Ogryczak vision setting, or the possible budget allocations - in
and S´liwin´ski 2006). These algorithms usually rely a budgeting application). For simplicity from now on,
on a solver for single-objective problem, which, in we refer to S as selections and number them S =
our context, is NP-hard. Recently, Hartman et al. {s 1,...,s |S|}. We seek solutions that are distributions
(2023) adapted one of these algorithms to work with over selections. Formally, an S-distribution is a proba-
approximately-optimal solver. However, designing such
bilitydistributionoverthesetS,andX isthesetofall
such distributions:
a solver remains quite challenging. Our work general-
izes these approaches by proving that this algorithm
|S|
still functions with even a weaker type of solver, which X ={x=(x ,...,x )∈R|S| | X x =1}
we show can be implemented for many problems. 1 |S| ≥0 j
j=1
Another significant area of research focuses on lex-
imin approximations. Since leximin fairness involves Importantly, we allow the number of selections in S to
multiple objectives simultaneously, it is not straight-
beexponentialinn.However,ouralgorithmswillreturn
forward to define what leximin approximation is. Sev- sparse solutions, that is, the number of selections with
eral definitions have been proposed. The definition we
a positive probability will be polynomial in n.
employ is related to that of (Kleinberg, Rabani, and Definition 3.1 (A Poly-sparse Vector). A vector, v∈
E´vaTardos2001;Abernethy,Schapire,andSyed2024). Rm for some m∈N, is a poly-sparse if no more than
≥0
Otherdefinitionscanbefoundin(Hartmanetal.2023; polynomial number (in n) of its entries are non-zero.
Henzinger et al. 2022). An extensive comparison of the
different definitions, including examples, is provided in Dummy-Selection. We assume that there exists a
Appendix H. dummy selection, s d ∈S, that gives all the agents util-
ity 0. This allows us to also consider sub-probabilities.
The work closest to ours is (Kawase and Sumita
Theuseofdummy-selectionmakessenseinoursetting,
2020),whichlaidthefoundationforthisresearch.Their
as we assume all utilities are non-negative.
paperstudiestheproblemofstochasticallocationofin-
divisible goods (see Section 12.1 for more details), and Utilities. The utility of agent i∈N from a given se-
proposes a reduction from egalitarian welfare to utili- lectionisdescribedbythefunctionu : S →R ,which
i ≥0
tarianwelfareforthisspecificproblem.Weextendtheir is provided as a value oracle.1 For x ∈ X, agent i’s
workintwoways.First,weextendtheirapproachfrom utility is the expectations E (x)=P|S| x ·u (s ).We
the allocation problem to any problem where lotteries i j=1 j i j
make sense. Second, we show that a black-box for the
1This means that the algorithm does not have a direct
utilitarian welfare is even more powerful, as it can also
accesstotheutilityfunction.Rather,givens∈S,thevalue
beusedforleximin,ratherthantheegalitarianwelfare. u (s) can be obtained in O(1) for any i=1,...,n.
i
2denote E(x) = (E (x),...,E (x)) (referred to as the
1 n
expected vector of x).
3.1 Leximin Fairness
We aim for a leximin-optimal S-distribution: one that
maximizes the smallest expected-utility, then, subject
tothat,maximizesthesecond-smallestexpected-utility,
and so on. It is formally defined below.
The Leximin Order. For v∈Rn, let v↑ be the cor-
responding vector sorted in non-decreasing order, and
v↑ thei-thsmallestelement(countingrepetitions).For
i
example, if v = (1,4,7,1) then v↑ = (1,1,4,7), and
v↑ = 4. For v,u ∈ Rn, we say that v is (weakly) Figure 1: High level description of the reduction algorithm.
le3 ximin-preferred over u, denoted v ⪰ u, if one of the An arrow from element A to B denotes that A calls B.
following holds. Either v↑ =u↑ (in which case they are White components are implemented in this paper; gray
components represent existing algorithms; black
leximin-equivalent, denoted v ≡ u). Or there exists an
component is the black-box for the utilitarian welfare.
integer 1 ≤ k ≤ n such that v↑ = u↑ for i < k, and
i i
v↑ > u↑ (in which case v is strongly leximin-preferred
k k
over u, denoted v ≻ u). Note that the E(x)’s are n- (I)α-ApproximateBlack-BoxforUtilitarianWelfare
tuples, so the leximin order applies to them.
Observation 3.1. Let v,u ∈ Rn. Exactly one of the Input: n non-negative constants c 1,...,c n.
following holds: either v⪰u or u≻v.
Output: a selection, suo ∈S, for which:
Leximin Optimal. Wesaythatx∗ ∈X isaleximin-
optimal S-distribution if E(x∗)⪰E(x) for all x∈X. Xn Xn
∀s∈S: c ·u (suo)≥α c ·u (s).
i i i i
Approximation. Throughout the paper, we denote i=1 i=1
by α∈(0,1] a multiplicative approximation ratio.
Whenα=1,wesaythatwehaveanexact black-box.
Definition 3.2 (Leximin-Approximation). We say
that an S-distribution, xA ∈ X, is an α-leximin- Many existing solvers for the utilitarian welfare are
approximation if E(xA)⪰α·E(x) for all x∈X. inherently robust to scaling the utility functions by a
constant, as they handle a class of utilities that are
Observation 3.2. An S-distribution is a 1-leximin-
closedunderthisoperation.Forinstance,inthedivision
approximation if-and-only-if it is leximin-optimal.
ofgoods,variousclassesofutilities,suchasadditiveand
submodular, are closed under constant scaling.
3.2 Utilitarian Optimization
Autilitarian-optimalS-distributionisanS-distribution 4 Reduction: Overview
xuo ∈X that maximizes the sum of expected utilities: InSections5-11weproveourmainresult:analgorithm
for finding a leximin-approximation using an approx-
n n
∀x∈X: X E (xuo)≥X E (x) imate black-box for utilitarian welfare, while impor-
i i tantly, preserving the same approximation factor α.
i=1 i=1
Thereductionisdonestepbystep.Section5reduces
In fact, for utilitarian welfare, stochasticity is unneces- theproblemofleximin-approximationtoanotherprob-
sary, as there always exists a deterministic solution (a lem; then, each Section k ∈ {6,7,8,9,10} reduces the
single selection) that maximizes the utilitarian welfare. problem from Section k−1 to another problem, where
in Section 10 the reduced problem is approximate util-
n
Lemma 3.3. Let suo ∈argmaxX u (s). Then itarian optimization. Finally Section 11 ties the knots
i to prove the entire reduction, and extends the result to
s∈S
i=1
randomized solvers. An schematic description of reduc-
n n tion structure is provided in Figure 1.
X X
∀x∈X: u (suo)≥ E (x).
i i
i=1 i=1 5 Main Loop
UtilitarianSolver. Theproposedreductionrequires The main algorithm used in the reduction is described
autilitarianwelfaresolverthatisrobusttoscalingeach in Algorithm 1. It is adapted from the Ordered Out-
utility function by a different constant. Formally: comes algorithm of (Ogryczak and S´liwin´ski 2006) for
3finding an exact leximin-optimal solution. It uses a Algorithm 1: Main Loop
given solver for the maximization program below. Input:AsolverforP1.
TheProgramP1. Theprogramisparameterizedby 1: for t=1 to n do
an integer t ∈ N and t−1 constants (z ,...,z ); its 2: Let xt be the solution returned by applying the
only variable is x (a vector of size |S| r1 epresent t− i1 ng an givensolverwithparameterstand(z 1,...,z t−1).
S-distribution): 3: Let z
t
:=obj(P1,xt).
4: end for
t t−1 5: return xn.
X X
max E↑(x)− z (P1)
i i
i=1 i=1
solution xt ∈ F(P1) with optimal objective value —
|S|
s.t. (P1.1)
X
x =1
obj(P1,xt)≥obj(P1,x) for any solution x∈F(P1).
j However, in some situations, no efficient exact solver
j=1 for P1 is known. An example is stochastic allocation of
(P1.2) x j ≥0 j =1,...,|S| indivisible goods, described in Section 12.1.3
Xℓ Xℓ Solving P1 Approximately. Ourinitialattemptto
(P1.3) E↑(x)≥ z ∀ℓ∈[t−1]
i i dealwiththisissuewastofollowtheapproachofHart-
i=1 i=1 man et al. (2023). They consider an approximately-
Constraints (P1.1–2) simply ensure that x ∈ X. Con- optimal solver for P1, which returns a solution xt ∈
straint (P1.3) says that for any ℓ < t, the sum of F(P1) with approximately-optimal objective value —
the smallest ℓ expected-utilities is at least the sum of obj(P1,xt)≥α·obj(P1,x)foranysolutionx∈F(P1).
the ℓ constants z ,...,z . The objective of a solution2 However, designing such a solver is very challenging;
1 ℓ
x ∈ F(P1) is the difference between the sum of its all our efforts to design such a solver for several NP-
smallest t expected-utilities and the sum of the t−1 hard problems were unsuccessful.
constants z 1,...,z t−1. Solving P1 Shallowly. Our first contribution is to
Algorithm 1. The algorithm has n iterations. In show that Algorithm 1 can work with an even weaker
each iteration t, it uses the given solver for P1 with kind of solver for P1, that we call a shallow solver.
the following parameters: the iteration counter t, and Recallthats
d
isadummy-selectionthatgivesutility
t(z io1 n, s. ... T, hz et− s1 o) lvt eh ra rt etw ure nr se aco som lup tu iote nd foi rn thp ir se Pvi 1o ,u ds enit oe tr ea d- 0 Wt eo da el fil na ege tn ht es, foa ln lod wx ind gis si ut bs sp er to ob fa Xbil :ity according to x.
ib sy ux set. dI if nt t< hen fot lh loen wii nt gs o itb ej re ac tt ii ov ne sv aa slu ae n, d ae dn do itt ie od nab ly pz at -, X α− ={x∈X |x d ≥(1−α)}.
rameter. Finally, the solution xn generated at the last In words: an x ∈ X α− uses at most α fraction of the
distribution for selections that give positive utility to
application of the solver is returned by the main loop.
Noticethattheprogramevolvesineachiteration.For some agents. A shallow-solver is defined as follows:
any t ∈ [n−1], the program at iteration t+1 differs
(II) α-Shallow-Solver for P1
from the program at iteration t in two ways: first, the
objective function changes; and second, an additional
Input: An integer t∈N and rationals z ,...,z .
constraint is introduced as part of Constraint (P1.3), 1 t−1
Pt E↑(x) ≥ Pt z (for ℓ = t). This constraint is
equi= iv1 alei ntto:Pt i= E1 ↑i (x)−Pt−1z ≥z ,whichessen- Output: A solution xt ∈ F(P1) such that
i=1 i i=1 i t obj(P1,xt)≥obj(P1,x) for any x∈F(P1)∩X−.
tially ensures that any solution for following programs α
achieves an objective value of at least z according to
t
the objective function of the program at iteration t. In In words: the solver returns a solution xt ∈ F(P1)
other words, this constraint guarantees that as we con- whoseobjectivevalueisguaranteedtobeoptimalcom-
tinually improving the situation. paring only to solutions that are also in X−. This is in
α
This implies, in particular, that xt remains feasible contrast to an exact solver, where the objective value
for the (t+1)-th program. of the returned solution is optimal comparing to all so-
lutions.4 Clearly, when α = 1, we get an exact solver,
Observation5.1. Lett∈[n−1].Thesolutionobtained
as in this case X− =X.
in the t-th iteration of Algorithm 1, xt, is also feasible α
Notice that xt is not required to be in X−, so its ob-
for the (t+1)-th program. α
jective value might be strictly-higher than the optimal
Solving P1 Exactly. By Ogryczak and S´liwin´ski objective value of the set X α−.
(2006)(Theorem1),thereturnedxn isleximin-optimal 3Instochasticallocation,eventhecasewheret=1isal-
when the solver for P1 is exact - that is, it returns a ready NP-hard, as its optimal solution maximizes the egal-
itarian welfare (Kawase and Sumita 2020).
2See Preliminaries for more details. 4SeeTable1inAppendixBforcomparisonofthesolvers.
4Lemma 5.2. Given an α-shallow-solver for P1, Algo- i ∈ N (i.e., with the original utilities u ), to obtain a
i
rithm 1 returns an α-leximin-approximation. valueU′.Recallthattheobjectivefunctionisthesumof
the smallest t utilities minus a positive constant. Thus,
Proof sketch. Consider the solution xn returned by Al- itisclearthatanupperboundonthesumofallutilities
gorithm 1. As xn is feasible for the program solved in can be used also as an upper bound on the maximum
the last iteration, and as constraints only being added objective value.
along the way, we can conclude that xn is feasible for
Duringthebinarysearch,wequerytheapproximate-
all of the n programs solved during the algorithm run. feasibility-oracle about the current value of z . If the
Next, we suppose by contradiction that xn is not oracle asserts Feasible, we increase the lowert bound;
an α-leximin-approximation. By definition, this means
otherwise, we decrease the upper bound. We stop the
that there exists an x ∈ X such that αE(x) ≻ E(xn). binary search once we reach the desired level of accu-
That is, there exists an integer k ∈ [n] such that: racy.5 Finally, we return the solution x corresponding
αE↑(x)=E↑(xn) for i≤k and αE↑(x)>E↑(xn). tothehighestz forwhichtheoraclereturnedFeasible.
i i k k t
We then construct x′ from x as follows: x′ j := α·x j By the definition of the oracle, this x is feasible for P1.
for any j, and x′ = 1−αP x . It is easy to verify In addition, the value z is at least as high as the opti-
d j̸=d j t
tha Tt hx en′ ,∈ wX eα− coa nn sd idt eh rat thE ei( px r′ o) g= raα mE si( ox lv) e.
d in iteration
m
z
a tl ho ab tje wc et riv ee ca oc nr so idss erX edα−, byas thal el ath lge ov ria tl hu mes ,h wig erh eer deth tea rn
-
t
t = k. As xn satisfies all its constraints, and as these mined as Infeasible-Under-X α−. This gives us a shallow
constraints impose a lower bound on the sum of the solver for P1.
least ℓ < k expected-utilities, we can conclude that x′
satisfies all these constraints too. 7 Approx. Feasibility Oracle for P1
Finally, we prove that the objective-value of x′ for
this program is strictly-higher than the one obtained To design an approximate feasibility oracle for P1, we
by the shallow-solver (i.e., z ) — in contradiction to modify P1 as follows. First, we convert the optimiza-
the solver guarantees. k tion program P1, to a feasibility program (without an
objective), by adding a constraint saying that the ob-
6 A Shallow-Solver for P1 jective function of P1 is at least the given constant z t:
Pt E↑(x) ≥ Pt z . We then make two changes to
Our next task is to design a shallow solver for P1. We i=1 i i=1 i
this feasibility program: (1) remove Constraint (P1.1)
useanapproximate-feasibility-oracle,definedasfollows:
thatensuresthatthesumofvaluesis1,and(2)addthe
(III) α-Approximate-Feasibility-Oracle for P1 objective function: minP| jS =| 1x j. We call the resulting
program P2:
Input: Anintegert∈N andrationalsz ,...,z ,
1 t−1
and another rational z . |S|
t X
min x s.t. (P2)
j
Output: One of the following claims regarding z : j=1
t
(P2.1) x ≥0 j =1,...,|S|
∃x∈F(P1) s.t. obj(P1,x)≥z . j
Feasible Inthiscase,theoraclereturnsst uchx. ℓ ℓ
X X
(P2.2) E↑(x)≥ z ∀ℓ∈[t]
Infeasible i i
∄x∈F(P1)∩X− s.t.obj(P1,x)≥z .
Under-X− α t i=1 i=1
α
Notethat(P2.2)containstheconstraints(P1.3),aswell
Notethattheseclaimsarenotmutuallyexclusive,as as the new constraint added when converting to a fea-
z can satisfy both conditions simultaneously. In this sibility program.
t
case, the oracle may return any one of these claims. As before, the only variable is x. However, in this
Lemma 6.1. Given an α-approximate-feasibility- program, a vector x can be feasible without being in
oracle for P1 (III), an α-approximate black-box for
X, as its elements are not required to sum to 1.
We shall now prove that an approximate feasibility
the utilitarian welfare (I), and an arbitrary vector in
F(P1). An efficient α-shallow-solver for P1 (II) can oracle for P1 can be designed given a solver for P2,
be design.
whichreturnsapoly-sparse6 approximately-optimalso-
lution. As P2 is a minimization program and as α ∈
Proof sketch. The solver is described in Algorithm 2 in (0,1], it is defined as follows:
AppendixC.Weperformbinarysearchoverthepoten-
tial objective-values z for the program P1.
t 5Forsimplicity,weassumethebinarysearcherrorisneg-
As a lower bound for the search, we simply use the
ligible,asitcanbereducedbelowϵintimeO(log1),forany
objective value of the given feasible solution. ϵ
ϵ>0; the full proof in Appendix C, omits this assumption.
Foranupperbound,weusethegivenα-approximate 6A poly-sparse vector (def. 3.1) is one whose number of
black-box for the utilitarian welfare with c = 1 for all non-zero values can be bounded by a polynomial in n.
i
5ofconstraintsintheappendix;andshowthatitimplies
(IV) 1-Approx.-Optimal-Sparse-SolverforP2
α that the required solver for P2 can be easily derived
from the same type of solver for P3.
Input: An integer t∈N and rationals z ,...,z .
1 t
Output: A poly-sparse solution xA ∈ F(P2) such (V) α1-Approx.-Optimal-Sparse-Solver for P3
that obj(P2,xA)≤ 1obj(P2,x) for any x∈F(P2).
α Input: An integer t∈N and rationals z ,...,z .
1 t
Lemma 7.1. Given an α1-approximately-optimal- Output:Apoly-sparse(cid:0) xA,yA,mA(cid:1) ∈F(P3)such
sparse-solver for P2 (IV), an α-approximate-
feasibility-oracle for P1 (III) can be obtained. that obj(P3,(cid:0) xA,yA,mA(cid:1)) ≤ 1obj(P3,(x,y,m))
α
for any x∈F(P3).
Proof sketch. WeapplythesolverandgetavectorxA ∈
F(P2).Wethencheckwhetherobj(P2,xA)=P|S| xA
j=1 j
isatmost1.Thiscanbedoneinpolynomialtime,since Lemma 8.1. Given an 1-approximately-optimal-
α
xA is a poly-sparse vector. sparse-solver for P3 (V), an 1-approximately-
α
If so, we assert that z is Feasible. Indeed, we can optimal-sparse-solver for P2 (IV) can be obtained.
t
construct a poly-sparse vector x∈F(P1) that is equal
toxAexceptthatx :=(1−P|S| xA).Itcanbeverified
d j=1 j
that the new vector x is indeed in F(P1), and that Proof sketch. The equivalence between the two sets of
obj(P1,x)≥z t.
constraints says that (x,y,m) ∈ F(P3) if-and-only-if
Otherwise, P|S| xA >1. In this case, we assert that x∈F(P1).Thus,givenapoly-sparse 1-approximately-
j=1 j α
z t is Infeasible-under-X α−. Indeed, assume for contra- optimal solution (cid:0) xA,yA,mA(cid:1) ∈ F(P3); then, xA
diction that there exists x ∈ F(P1) ∩ X− such that (which is clearly a poly-sparse vector), is a 1-
α
obj(P1,x) ≥ z t. Construct a new vector x′ that is approximately-optimal solution for P2. α
equal to x except that x′ := 0. Since x ∈ F(P1) and
d
E (x) = E (x′) for all i ∈ N, we can conclude that
i i
x′ ∈ F(P2). In addition, as x ∈ X−, we can also con- 9 Approximate Optimal Solver for P3
α
cludethatobj(P2,x′)=P|S| x′ isatmostα.Butthis
j=1 j
means that the optimal objective is at most α. As a re- P3isalinearprogram,buthasmorethan|S|variables.
sult, any 1-approximately-optimal for P2 should have Although |S| (the number of selections) may be expo-
a sum of aα t most 1 — a contradiction. nential in n, P2 can be approximated in polynomial
timeusinganapproximateseparationoracletoitsdual
program, similarly to Karmarkar and Karp (1982), as
8 Approximate Optimal Solver for P2
described bellow. The dual of the linear program P3 is:
The use of E↑() operator makes both P1 and P2 non-
linear.However,OgryczakandS´liwin´ski(2006)showed
t ℓ
thatP1canbe“linearized“byreplacingtheconstraints X X
max q z s.t. (D3)
using E↑() with a polynomial number of linear con- ℓ i
straints.WetakeasimilarapproachforP2toconstruct ℓ=1 i=1
the following linear program P3: Xn Xt
(D3.1) u (s ) v ≤1 ∀j =1,...,|S|
i j ℓ,i
|S| i=1 ℓ=1
X
min x s.t. (P3) n
j X
(D3.2) ℓq − v ≤0 ∀ℓ∈[t]
j=1 ℓ ℓ,i
(P3.1) x ≥0 j =1,...,|S| i=1
j (D3.3) −q +v ≤0 ∀ℓ∈[t], ∀i∈[n]
ℓ ℓ,i
n ℓ
(P3.2) ℓy −X m ≥X z ∀ℓ∈[t] (D3.4) q ℓ ≥0 ∀ℓ∈[t]
ℓ ℓ,i i
(D3.5) v ≥0 ∀ℓ∈[t], ∀i∈[n]
i=1 i=1 ℓ,i
|S|
(P3.3) m ≥y −X x ·u (s ) ∀ℓ∈[t], ∀i∈[n] Similarly to P3, it is parameterized by an integer t,
ℓ,i ℓ j i j and rational numbers (z ,...,z ). It has a polynomial
j=1 1 t
number of variables: q and v for any ℓ∈[t] and j ∈
(P3.4) m ℓ,i ≥0 ∀ℓ∈[t], ∀i∈[n] [n];andapotentiallyeℓ xponenℓ t, ij alnumberofconstraints
Constraints(P3.2–4)introducet(n+1)≤n(n+1)aux- due to (D3.1); see Appendix E for derivation.
iliary variables: y and m for all ℓ ∈ [t] and i ∈ [n]. We prove that the required solver for P3 can be de-
ℓ ℓ,i
Weformallyprovetheequivalencebetweenthetwosets signed given the following procedure for its dual D3:
6By Lemma 7.1, this solver allows us to construct an
(VI) 1-Approx.-Separation-Oracle for D3
α α-approximate feasibility-oracle for P1.
By Lemma 6.1, a binary search with (1) this proce-
Input: Anintegert∈N,rationalsz ,...,z ,anda
potentialassignmentoftheprogramv1 ariablest
(q,v).
dure, (2) the given α-approximate black-box for utili-
tarian welfare, and (3) an arbitrary solution for P1 (as
follows);allowsustodesignanα-shallow-solverforP1.
Output: One of the following regarding (q,v): As an arbitrary solution, for t = 1, we take x0 defined
At least on of the constraints is vio- as the S-distribution with x0 =1 and x0 =0 for j ̸=d.
d j
Infeasible latedby(q,v).Inthiscase,theoracle However,for t≥2,werelyonthefactthattheshallow
returns such a constraint. solverisusedwithintheiterativeAlgorithm1,andtake
the solution returned in the previous iteration, xt−1,
Each one of the constraints is approx- which, by Observation 5.1, is feasible for the program
1-Approx. imately maintained — the left-hand at iteration t as well.
α side of the inequality is at least 1 ByLemma5.2,whenthisshallowsolverisusedinside
Feasible
α
times the its right-hand side. Algorithm1,theoutputisanα-leximinapproximation.
In Appendix F, we present a variant of the ellipsoid
Together with Observation 3.2, this implies
method that, given a 1-approximate-separation oracle
forthe(min.)dualprogα ram,allowsustoα-approximate Corollary 11.2. Given an exact black-box for the util-
the (max.) primal program (Lemma F.1). This allows itarian welfare, a leximin-optimal S-distribution can be
us to conclude the following: obtained in polynomial time.
Corollary 9.1. Given a 1-approximate-separation- Randomized Solvers. A randomized black-box re-
oracle for D3 (VI),
aα
1-approximately-optimal-
turns a selection that α-approximates the utilitarian
α welfare with probability p, otherwise returns an arbi-
sparse-solver for P3 (V) can be derived.
trary selection. Theorem 11.1 can be extended as fol-
lows:
10 Approx. Separation Oracle for D3
Theorem 11.3. Given a randomized α-approximate
Now, we design such an oracle using the given approx-
black-box for the utilitarian welfare with success proba-
imate black-box for the utilitarian welfare.
bility p ∈ (0,1]. An α-leximin-approximation can be
Lemma 10.1. Given an α-approximate black-box obtained with probability p in time polynomial in n and
for the utilitarian welfare (I), a 1-approximate- the running time of the black-box.
α
separation-oracle for D3 (VI) can be constructed.
12 Applications
Proof sketch. Given an assignment of the variables of This section provides three applications of our general
the program D3, the constraints (D3.2–5) can be ver-
reduction framework. Each application employs a dif-
ified directly, as their number is polynomial in n. The
ferent black-box for the associated utilitarian welfare.
constraintsin(D3.1)aretreatedcollectively.Weusethe
approximate black-box with c := Pt v for i ∈ N, 12.1 Stochastic Indivisible Allocation
and get a selection s k ∈ S.i If P icℓ= iu1 i(sℓ, ki ) > 1, we In the problem of fair stochastic allocations of indi-
declare that constraint k in (D3.1) is violated. Other- visible goods, described by Kawase and Sumita (2020),
wise, we assert that the assignment is approximately-
there is a set of m indivisible goods, G, that needs to
feasible (as this implies that all constraints in (D3.1)
be distributed fairly among the n agents.
are approximately-satisfied).
TheSetS. Aselectionisanallocationofthegoodsto
the agents, which can be described by a function map-
11 The Main Result
ping each good to the agent who gets it. Accordingly,
Putting it all together, we obtain:
S ={s|s: G→N}, and |S|=nm.
Theorem 11.1. Given an α-approximate black-box Agents care only about their own share, so we can
for the utilitarian welfare (I). An α-leximin- abusenotationandleteachu takeabundleBofgoods.
i
approximation (Def. 3.2) can be computed in time poly- The utilities are assumed to be normalized (u (∅)=0)
i
nomial in n and the running time of the black-box. and monotone (u (B ) ≤ u (B ) if B ⊆ B ). Under
i 1 i 2 1 2
these assumptions, different black-boxes for the utili-
Proof. By Lemma 10.1, given an α-approximate black- tarian welfare exist.
boxforutilitarianwelfare,a 1-approximate-separation-
oracle for D3 can be construα cted. The Utilitarian Welfare. For any n constants
By Corollary 9.1, using this oracle, we can construct c 1,...,c n, the goal is to maximize the following:
a 1-approximately-optimal-sparse-solver for P3. n
α By Lemma 8.1, we can use this solver to design a maxX u (s)·c
i i
1-approximately-optimal-sparse-solver for P2. s∈S
i=1
α
7Many algorithms for approximating utilitarian welfare This is just a knapsack problem with n item (one for
are already designed for classes of utilities, which are each group), where the weights are the group sizes w
i
closed under multiplication by a constant. This means (as we only look at the legal packing s ∈ S), and the
that, given such an algorithm for the original utilities values are the constants c .
i
(u ) , we can use it as-is for the utilities (c ·u ) .
i i∈N i i i∈N Result. It is well known that there is an FPTAS for
Results. Whentheutilitiesareadditive,maximizing the Knapsack problem. By Theorem 11.1:
the utilitarian welfare can be done in polynomial time
Corollary 12.4. There exists an FPTAS for leximin
by greedily giving each item to the agent who values
for the problem of giveaway lotteries.
it the most. This means, according to Corollary 11.2,
that: In addition, there is a pseudopolynomial-time algo-
rithm for knapsack, which implies:
Corollary 12.1. For additive utilities, a leximin-
optimal S-distribution can be obtained in polynomial Corollary 12.5. There exists an pseudopolynomial-
time. time algorithm for leximin for the problem of giveaway
lotteries.
When the utilities are submodular, approximat-
ing leximin to a factor better than (1 − 1) is NP- 12.3 Participatory Budgeting Lottery
e
hard (Kawase and Sumita 2020).7 However, as there is
Theproblemoffairlotteriesforparticipatorybudgeting,
a deterministic 1-approximation algorithm for the util-
2 was described by Aziz et al. (2024). The n agents are
itarian welfare (Fisher, Nemhauser, and Wolsey 1978),
voters,whoshareacommonbudgetB ∈R andmust
by Theorem 11.1: ≥0
decidewhichprojectsfromasetP tofund.Eachvoter,
Corollary12.2. Forsubmodularutilities,a 1-leximin- i∈N,hasanadditiveutilityoverthesetofprojects,u ;
2 i
approximation can be found in polynomial time. while the projects have costs described by cost: P →
R . Aziz et al. (2024) study fairness properties based
There is also a randomized (1 − 1)-approximation >0
algorithm for the utilitarian welfare e with high proba- on fair share and justified representation.
bility (Vondrak 2008), and thus by Theorem 11.3: The set S. The selections are the subsets of projects
Corollary 12.3. For submodular utilities, a (1− 1)- thatfitinthegivenbudget:S ={s⊂P |cost(s)≤B}.
leximin-approximation can be obtained with high proe b- The size of S in this problem is only bounded by 2|P|.
ability in polynomial time.
The Utilitarian Welfare. For any n constants
(c ) , the goal is to maximize the following:
12.2 Giveaway Lotteries i i∈N
In giveaway lotteries, described by Arbiv and Aumann n n
X XX
(2022), there is an event with a limited capacity, and max u (s)·c =max u (p)·c (Additivity)
i i i i
groups who wish to attend only-if they can all be ad- s∈S s∈S
i=1 i=1p∈s
mittedtogether.Here,eachgroupofpeopleisanagent. n !
We denote the size of group i by w ∈ N and the =maxX X u (p)·c
i ≥0 i i
event capacity by W ∈ N (w ≤ W for i ∈ N and s∈S
≥0 i p∈s i=1
P w >W).8
i∈N i Thiscanalsobeseenasaknapsackproblemwhere:the
The Set S. A selection is simply a subset of the items are the projects, the weights are the costs, and
groups that can attend the event together: S = {s ⊆ the value of item p∈P is Pn u (p)·c .
N |P w ≤W}. Here, |S| is only bounded by 2n. i=1 i i
Thei∈ us tiliti y of group i ∈ N from a selection s is 1 if Result. As before, the existence of a FPTAS for the
Knapsack problem together with Theorem 11.1, give:
they being chosen according to s (i.e., if i ∈ s) and 0
otherwise. Corollary 12.6. There is an FPTAS for leximin for
participatory budgeting lotteries.
The Utilitarian Welfare. For any n constants
c ,...,c , the goal is to maximize the following: While by the existence pseudopolynomial-time algo-
1 n
rithm:
n
X X
max u (s)·c =max c Corollary 12.7. There is an pseudopolynomial-time
i i i
s∈S s∈S algorithm for leximin for participatory budgeting lotter-
i=1 i∈s
ies.
7Kawase and Sumita (2020) prove that approximating
theegalitarianwelfaretoafactorbetterthan(1−1)isNP- 13 Conclusion and Future Work
e
hard.However,anα-leximin-approximationisfirstandfore-
In this work, we establish a strong connection between
most an α-approximation to the egalitarian welfare, thus,
leximin fairness and utilitarian optimization, demon-
the same hardness result applies to leximin as well.
stratedbyareduction.Itisrobusttoerrorsinthesense
8ArbivandAumann(2022)provideanalgorithmtocom-
that, given a black-box that approximates the utilitar-
putealeximin-optimalsolution.However,theiralgorithmis
polynomial only for a unary representation of the capacity. ian value, a leximin-approximation with respect to the
8same approximation factor can be obtained in polyno- Aumann,Y.;Dombb,Y.;andHassidim,A.2013. Com-
mialtime.Thisapproachraisesmanyfuturedirections. puting socially-efficient cake divisions. In Interna-
The first question is whether one can obtain a re- tional conference on Autonomous Agents and Multi-
duction also for deterministic solutions, where the goal Agent Systems, AAMAS, 343–350. IFAAMAS.
is to find a leximin-optimal (or approximate) selection, Aziz, H.; Lu, X.; Suzuki, M.; Vollen, J.; and Walsh,
ratherthanadistribution.Additionally,thispapersug- T. 2024. Fair Lotteries for Participatory Budgeting.
gestsaweakerdefinitionofleximin-approximationthan In Proceedings of the AAAI Conference on Artificial
theoneproposedby(Hartmanetal.2023);isitpossible Intelligence, volume 38, 9469–9476.
to obtain similar results with the stronger definition?
Bansal, N.; and Sviridenko, M. 2006. The Santa Claus
Regarding applications, we believe that this method
problem. In Proceedings of the Thirty-Eighth Annual
can also be applied to other problems, e.g., select-
ACM Symposium on Theory of Computing, STOC ’06,
ing a representative committee (Henzinger et al. 2022)
31–40.NewYork,NY,USA:AssociationforComputing
and allocating unused classrooms (Kurokawa, Procac-
Machinery. ISBN 1595931341.
cia, and Shah 2018), cake-cutting (Aumann, Dombb,
Bei, X.; Lu, X.; and Suksompong, W. 2022. Truthful
and Hassidim 2013; Elkind, Segal-Halevi, and Suksom-
pong 2021), nucleolus (Elkind et al. 2009), and more. Cake Sharing. Proceedings of the AAAI Conference on
Also, there are questions regarding the applications Artificial Intelligence, 36(5): 4809–4817.
mentioned in this paper. For stochastic allocation of Cheng, H.; Kong, S.; Deng, Y.; Liu, C.; Wu, X.; An,
goods, can the same method be applied to chores? In B.; and Wang, C. 2023. Exploring Leximin Principle
giveaway lotteries, (Arbiv and Aumann 2022) prove for Fair Core-Selecting Combinatorial Auctions: Pay-
thataleximin-optimalsolutionisnotonlyfairbutalso ment Rule Design and Implementation. In Proceedings
truthful; can our leximin-approximation also be truth- of the Thirty-Second International Joint Conference on
ful? Lastly, regarding budget allocation, (Aziz et al. Artificial Intelligence, IJCAI-23, 2581–2588. ijcai.org.
2024) focus on fairness that is both ex-ante and ex- Elkind, E.; Goldberg, L. A.; Goldberg, P. W.; and
post, while our fairness is only ex-ante; is it possible to Wooldridge, M. J. 2009. On the computational com-
take a best-of-both-worlds perspective with respect to plexity of weighted voting games. Ann. Math. Artif.
leximin as well? Intell., 56(2): 109–131.
Elkind, E.; Segal-Halevi, E.; and Suksompong, W.
Acknowledgements 2021. Mind the Gap: Cake Cutting With Separation.
This research is partly supported by the Israel Science In Thirty-Fifth AAAI Conference on Artificial Intelli-
Foundation grants 712/20 and 2697/22. We sincerely gence, AAAI, 5330–5338. AAAI Press.
appreciate Arkadi Nemirovski, Yasushi Kawase, Nikhil Fisher, M. L.; Nemhauser, G. L.; and Wolsey, L. A.
Bansal, Shaddin Dughmi, and Edith Elkind for their 1978. An analysis of approximations for maximizing
valuable insights, helpful answers, and clarifications. submodular set functions—II, 73–87. Berlin, Heidel-
We are also grateful to the following members of the berg: Springer Berlin Heidelberg. ISBN 978-3-642-
stack exchange network for their very helpful answers 00790-3.
toourtechnicalquestions:NealYoung,9 1Rock,10 Mark
Flanigan,B.;G¨olz,P.;Gupta,A.;Hennig,B.;andPro-
L. Stone.11 Rob Pratt,12 mtanneau, 13 and mhdadk 14
caccia,A.D.2021.Fairalgorithmsforselectingcitizens’
assemblies. Nature, 596(7873): 548–552.
References
Flanigan,B.; Liang, J.;Procaccia,A. D.;and Wang,S.
Abernethy,J.D.;Schapire,R.;andSyed,U.2024. Lex- 2024. Manipulation-Robust Selection of Citizens’ As-
icographic optimization: Algorithms and stability. In semblies. In Wooldridge, M. J.; Dy, J. G.; and Natara-
International Conference on Artificial Intelligence and jan, S., eds., Proceedings of the AAAI Conference on
Statistics, 2503–2511. PMLR. Artificial Intelligence, 9696–9703. AAAI Press.
Arbiv, T.; and Aumann, Y. 2022. Fair and Truthful Freeman, R.; Sikdar, S.; Vaish, R.; and Xia, L. 2019.
Giveaway Lotteries. Proceedings of the AAAI Confer- Equitable Allocations of Indivisible Goods. In Kraus,
ence on Artificial Intelligence, 36(5): 4785–4792. S., ed., Proceedings of the Twenty-Eighth International
Joint Conference on Artificial Intelligence, IJCAI-
9https://cstheory.stackexchange.com/questions/51206 2019, 280–286. ijcai.org.
and https://cstheory.stackexchange.com/questions/51003 Gr¨otschel, M.; Lov´asz, L.; and Schrijver, A. 1981. The
and https://cstheory.stackexchange.com/questions/52353 ellipsoid method and its consequences in combinatorial
10https://math.stackexchange.com/questions/4466551
optimization. Combinatorica, 1(2): 169–197.
11https://or.stackexchange.com/questions/8633 and
https://or.stackexchange.com/questions/11007 and Gr¨otschel, M.; Lov´asz, L.; and Schrijver, A. 1993. Ge-
https://or.stackexchange.com/questions/11311 ometric Algorithms and Combinatorial Optimization,
12https://or.stackexchange.com/questions/8980 volume2ofAlgorithmsandCombinatorics. Berlin,Hei-
13https://or.stackexchange.com/questions/11910 delberg: Springer Berlin Heidelberg. ISBN 978-3-642-
14https://or.stackexchange.com/questions/11308 78240-4.
9Hartman, E.; Hassidim, A.; Aumann, Y.; and Segal-
Halevi, E. 2023. Leximin Approximation: From Single-
ObjectivetoMulti-Objective. InECAI2023,996–1003.
IOS Press.
Henzinger,M.;Peale,C.;Reingold,O.;andShen,J.H.
2022. Leximax Approximations and Representative
Cohort Selection. In Celis, L. E., ed., 3rd Sympo-
siumonFoundationsofResponsibleComputing(FORC
2022), volume 218 of Leibniz International Proceed-
ings in Informatics (LIPIcs), 2:1–2:22. Dagstuhl, Ger-
many: Schloss Dagstuhl – Leibniz-Zentrum fu¨r Infor-
matik. ISBN 978-3-95977-226-6.
Karmarkar, N.; and Karp, R. M. 1982. An effi-
cient approximation scheme for the one-dimensional
bin-packing problem. In 23rd Annual Symposium on
FoundationsofComputerScience(sfcs1982),312–320.
Chicago, IL, USA: IEEE.
Kawase, Y.; and Sumita, H. 2020. On the Max-Min
Fair Stochastic Allocation of Indivisible Goods. Pro-
ceedings of the AAAI Conference on Artificial Intelli-
gence, 34(02): 2070–2078.
Kleinberg, J.; Rabani, Y.; and E´va Tardos. 2001. Fair-
ness in Routing and Load Balancing. Journal of Com-
puter and System Sciences, 63(1): 2–20.
Kurokawa, D.; Procaccia, A. D.; and Shah, N. 2018.
Leximin Allocations in the Real World. ACM Trans.
Economics and Comput., 6(3-4): 11:1–11:24.
Ogryczak, W. 1997. On the lexicographic minimax ap-
proach to location problems. European Journal of Op-
erational Research, 100(3): 566–585.
Ogryczak, W.; Pi´oro, M.; and Tomaszewski, A. 2004.
Telecommunications network design and max-min op-
timizationproblem. Journaloftelecommunicationsand
information technology, 4: 43–53.
Ogryczak, W.; and S´liwin´ski, T. 2006. On Direct
Methods for Lexicographic Min-Max Optimization. In
Gavrilova, M.; Gervasi, O.; Kumar, V.; Tan, C. J. K.;
Taniar, D.; Lagan´a, A.; Mun, Y.; and Choo, H., eds.,
Computational Science and Its Applications - ICCSA
2006, 802–811. Berlin, Heidelberg: Springer Berlin Hei-
delberg. ISBN 978-3-540-34076-8.
Vondrak, J. 2008. Optimal approximation for the sub-
modular welfare problem in the value oracle model. In
Proceedings of the fortieth annual ACM symposium on
Theory of computing,67–74.VictoriaBritishColumbia
Canada: ACM. ISBN 978-1-60558-047-0.
10A Stochasticity Is Unnecessary for the Exact Solver. An exact solver returns a solution,
Utilitarian Welfare (Proof of xt ∈F(P1), whose objective value is maximum among
the objective values of all the solutions; formally,
Lemma 3.3)
obj(P1,xt)≥obj(P1,x) for any solution x∈F(P1).
Recall that the Lemma 3.3 says that:
Approximately-Optimal Solver. An
n
Lemma. Let suo ∈argmaxX u (s). Then approximately-optimal solver returns a solution,
s∈S i xt ∈ F(P1), whose objective value is at least α times
i=1
the maximum among the objective values of all the
n n
∀x∈X: X u (suo)≥X E (x). solutions; equivalently, obj(P1,xt) ≥ α·obj(P1,x) for
i i any solution x∈F(P1).
i=1 i=1
Proof. Letxuo beanS-distributionthatmaximizesthe ShallowSolver. Incontrast,ashallow solverreturns
sum of expected-utilities: a solution, xt ∈ F(P1), whose objective value is at
leastthemaximumamongtheobjectivevaluesofaspe-
n
xuo ∈argmaxX E (s). cific subset of solutions — specifically, the solutions in
i
x∈X i=1 X α−; formally, z t ≥ r for any solution obj(P1,xt) ≥
It follows that:
α·obj(P1,x) for any solution x∈F(P1)∩X α−.
The name ’shallow’ aims to reflect that this solver
n |S| n |S| only considers a subset of the entire solution space. It
X X X X
u i(suo)= xu jo u i(suo) (As xu jo =1) is important to note that the objective value of the so-
i=1 j=1 i=1 j=1 lution returned by a shallow solver might be strictly-
|S| n higher thanthemaximumwithinX−,thisisbecausex
≥X xuoX u (s ) (By def. of suo) is not restricted to be in X− (it migα ht be in X\X−).
j i j α α
j=1 i=1 Appendix B.5 provides an extensive analysis of the
n |S| n hierarchy between the solvers.
XX X
= xuo·u (s )= E (xuo).
j i j i
B.2 Preparations for the Proof
i=1j=1 i=1
Inthissection,wepresentanotherdefinitionofthelex-
imin approximation and prove that it is equivalent to
the definition provided in the main paper. We will use
B Using a Shallow Solver (Including this new definition in the proof of Lemma 5.2. Specif-
Proof of Lemma 5.2) ically, we prove that an S-distribution xA ∈ X is an
ThisappendixprovidesananalysisofAlgorithm1when α-leximin-approximation if and only if E(xA) ⪰ E(x)
usingashallow solverfortheprogramP1.Weuseabit for any x∈X α−.
morecompactrepresentationoftheprogramwherethe Recall that s d is the dummy-selection that gives all
two constraints (P1.1–2) are merged: agents utility 0, and that x d is the probability the S-
distributionxassignstothedummy-selection;andalso
max
Xt
E
i↑(x)−Xt−1
z i (P1)
Wtha et sX
taα
r− ti bs ya ds eu fib nse int gof twX ow ophe er re atx io∈ nsX
.
α− ifx
d
≥(1−α).
i=1 i=1
Definition B.1 (α-Upgrade). For x ∈ X−, an α-
s.t. (P1.1–2) x∈X α
upgrade of x is the output vector of the following func-
ℓ ℓ tion:
X X
(P1.3) E↑(x)≥ z ∀ℓ∈[t−1]
i i up(x)=(up(x,1),...,up(x,|S|))
i=1 i=1
Recall that this program is parameterized by an inte- ( 1 ·x ∀j ̸=d
ger t ∈ N and t−1 constants (z 1,...,z t−1); its only s.t. up(x,j)= 1α
−
1j P
x otherwise
variable is x (a vector of size |S| representing an S- α j̸=d j
distribution).
Lemma B.1. Let x∈X−, and let xup :=up(x) be its
α
α-upgrade. Then, xup ∈X and E(xup)= 1E(x).
B.1 The Three Different Solver Types α
Comparing the Types. In Section 5, three types of Proof. As x ∈ X−, we know that x ≥ 0 for all j and
solvers for P1 were mentioned - exact, approximately- that P x ≤αα . j
optimal, and shallow. Table 1 provides a comparison j̸=d j
For j ̸=d, it is clearthat xup ≥0. It is also true that
between the three types. We denote the solution re- j
xup ≥0:
turnedbythesolverbyxt,andrecallthatF(P1)isthe d
set of feasible solutions for P1 (i.e., those who satisfy 1 1
X
all its constraints); and that obj(P1,x) describes the xu dp =1− α x j ≥1− α ·α=0
objective value of such a solution x∈F(P1). j̸=d
11Exact Solver α-Approx.Optimal Solver α-Shallow Solver
(Ogryczak and S´liwin´ski 2006) (Hartman et al. 2023) (This paper)
Returns xt ∈F(P1) (i.e., a feasible solution for P1) such that
obj(P1,xt)≥obj(P1,x) obj(P1,xt)≥α·obj(P1,x) obj(P1,xt)≥obj(P1,x)
for all x∈F(P1). for all x∈F(P1). for all x∈F(P1)∩X−.
α
Table 1: Comparing the three solvers for (P1), where α∈(0,1] is the approximation-factor.
We can also conclude that P xup =1: We can also conclude that P xdown =1:
j j j j
|S|
|S| X X
X
xup
=X
xup+xup
xd jown = xd jown+xd down
j j d
j=1 j̸=d
j=1 j̸=d
 
 
=X α1 x
j
+ 1− α1 X x j=1 =X α·x j + 1−αX x j=1
j̸=d j̸=d
j̸=d j̸=d
Thus, xdown ∈X−.
Thus, xup ∈X.
In addition,
Eα
(xdown) = αE(x), as the following
In addition, regarding the expected utilities, holds for any i∈N:
E(xup)= 1E(x) as the following holds for any i∈N:
α |S| |S|
X X
E (xdown)= xdown·u (s )= α·x ·u (s )
|S| |S| 1 i j i j j i j
X X
E (xup)= xup·u (s )= x ·u (s ) j=1 j=1
i j i j α j i j
j=1 j=1 |S|
X
=α x ·u (s )=αE (x)
1 |S| 1 j i j i
X
= x ·u (s )= E (x) j=1
α j i j α i
j=1
Essentially, α-upgrade multiplies the probability of
all non-dummy selections by 1/α, and α-downgrade
multiplies them by α. The operations are converse:
Definition B.2 (α-Downgrade). For x ∈ X, an α-
downgrade of x is the output vector of the following Observation B.3. For all x ∈ X α−, the α-downgrade
of the α-upgrade of x equals x. Similarly, for all x ∈
function:
X, The α-upgrade of the α-downgrade of x equals x.
down(x)=(down(x,1),...,down(x,|S|))
∀x∈X−: down(up(x))=x
α
(
α·x ∀j ̸=d ∀x∈X: up(down(x))=x
s.t. down(x,j)= j
1−αP
x otherwise Upgradesanddowngradespreservetheleximinorder:
j̸=d j
Observation B.4. For two vectors x,x′ ∈ X−,
Lemma B.2. Let x∈X, and let xdown :=down(x) be E(x)⪰E(x′) if-and-only-if the same relation holds fα or
its α-downgrade. Then, xdown ∈X− and E(xdown)= their α-upgrades E(up(x))⪰E(up(x′)).
α
αE(x). Similarly, for two vectors x,x′ ∈ X, E(x) ⪰ E(x′)
if-and-only-if the same relation holds for their α-
downgrades E(down(x))⪰E(down(x′)).
Proof. As x ∈ X, we know that x ≥ 0 for all j and
j
that P x ≤1. Relation Between X and X−. Recall that an S-
j̸=d j α
For j ̸= d, it is clear that xdown ≥ 0. Also, xdown ≥ distribution, x∗ ∈ X, is leximin optimal15 if E(x∗) ⪰
j d E(x) for all x∈X.
(1−α), since:
Lemma B.5. Let x∗ be a leximin optimal S-
xdown =1−αX x distribution. Then, E(down(x∗)) ⪰ E(x) for all x ∈
d j X−.
j̸=d α
≥1−α (as X x ≤1) 15Notice that there might be different solutions that are
j leximin-optimal,butalloftheirexpectedvectorareleximin-
j̸=d equivalent.
12Proof. Let x ∈ X−. By Lemma B.1, up(x) ∈ X. As the objective value of xn for the program P1 that was
α
x∗ is leximin optimal, we get that E(x∗) ⪰ E(up(x)). solved in k-th iteration at least z :
t
By Observation B.4, E(down(x∗)) ⪰ E(down(up(x));
k k−1
andbyObservationB.3,E(down(up(x))=E(x).Thus, X E↑(xn)−X z ≥z (1)
E(down(x∗))⪰E(x). i i k
i=1 i=1
We shall now see that x′ is also a solution to this
problem. Constraints (P1.1–2) are satisfied since x′ ∈
Now, recall that an S-distribution, xA ∈ X, is an X− ⊆ X. For Constraint (P1.3), we notice that the
α-leximin-approximation if E(xA) ⪰ α · E(x) for all (kα −1)leastexpectedvaluesofx′ equalstothoseofxn,
x∈X. which means that, for any ℓ<k:
Lemma B.6. An S-distribution xA is an α-leximin- ℓ ℓ ℓ
X X X
approximation if and only if E(xA)⪰E(x) for all x∈ E i↑(x′)= E i↑(xn)≥ z i
X α−. i=1 i=1 i=1
Therefore,x′ isalsoasolutionforthisprogram,andits
Proof. Let x∗ be a leximin optimal S-distribution. objective value for it is:
LetxAbeanα-leximin-approximation.Bydefinition,
k k−1
X X
E(xA) ⪰ α·E(x) for all x ∈ X. Since x∗ ∈ X, we get E↑(x′)− z
i i
that E(xA) ⪰ α·E(x∗). Now, consider down(x∗). By
i=1 i=1
Lemma B.2, E(down(x∗))=αE(x∗). This implies that We shall now see that this means that the objective
E(xA)⪰E(down(x∗)).TogetherwithLemmaB.5,and value of x′ is strictly-higher than the objective value
by transitivity, this means that E(xA) ⪰ E(x) for all of the solution returned by the solver in this iteration,
x∈X α−. namely z k.
On the other hand, let xA be an S-distribution such
k k−1 k−1 k−1
that E(xA) ⪰ E(x) for all x ∈ X α−. Let x′ ∈ X. X E↑(x′)−X
z
=X E↑(x′)+E↑(x′)−X
z
By Lemma B.2, down(x′) ∈ X− and E(down(x′)) = i i i k i
αE(x′). As down(x′) ∈ X−, wα e get that E(xA) ⪰ i=1 i=1 i=1 i=1
E(down(x′)); and as
E(dowα
n(x′)) = αE(x′). this im-
By definition of x′ for i<k:
plies that E(xA)⪰αE(x′). k−1 k−1
X X
= E↑(xn)+E↑(x′)− z
i k i
B.3 Proof of Lemma 5.2 i=1 i=1
We can now use the new definition to prove a slightly
By definition of x′ for k:
different version of our Lemma 5.2. k−1 k−1
X X
> E↑(xn)+E↑(xn)− z
Lemma B.7. Given an α-shallow-solver for P1, Algo- i k i
rithm1returnsanS-distributionxn suchthatE(xn)⪰ i=1 i=1
By Equation (1):
E(x) for all x∈X−.
α
≥z
Clearly, together with Lemma B.6, this proves our t
Lemma 5.2 — as required. But this contradicts the guarantees of our shallow
solver — by definition, the objective value of the solu-
tion returned by the solver, namely z , is at least as
Proof. As a first observation, we note that xn returned high as the objective of any solution ink F(P1)∩X−.
by the algorithm is a solution for P1 that was solved α
in the last iteration. However, as constraints are only
added along the way, it implies that:
B.4 A more general theorem
TheproofofLemma5.2doesnotusethespecificstruc-
Observation B.8. xn is a solution for the program ture of X, X−, or the functions E . Therefore, we have
P1 that was solved in each one of the iterations t = in fact provedα a more general theoi rem.
1,...,n.
LetX beanysubsetofRm forsomem∈N,letY be
any subset of X, and let E for i∈N be any functions
Now, suppose by contradiction that there exists a from X to R . i
x′ ∈X α− such that E(x′)≻E(xn). By definition, there Define a Y≥ -s0 hallow-solver for P1 as a solver that re-
exists an integer 1≤k ≤n such that E↑(x′)=E↑(xn) turns a solution xt ∈ F(P1) such that obj(P1,xt) ≥
i i
for i≤k, and E↑(x′)>E↑(xn). obj(P1,x) for all solutions x∈F(P1)∩Y. Then:
k k
As xn is a solution for the program P1 that was Lemma B.9. Given a Y-shallow-solver for P1, Algo-
solved in the last iteration (t = n), we can conclude rithm 1 returns an xn ∈X such that E(xn)⪰E(x) for
that Pk E↑(xn) ≥ Pk z (by constraint (P1.3) if all x∈Y.
i=1 i i=1 i
k <nandbyitsobjectiveotherwise).Thisimpliesthat The proof is identical to that of Lemma 5.2.
13B.5 Hierarchy of the Solvers for P1 down(x) ∈ F(P1). Constraints (P1.1–2) are satisfied
Consider the two non-exact types: approximately-
as down(x)∈X− ⊆X. Constraint (P1.3) is empty for
α
optimal, and shallow. First, both are relaxations of t = 1 and is therefore vacuously satisfied by down(x).
the exact solver. One can easily verify it by taking However, as t = 1, we get that obj(P1,down(x)) >
α=1fortheapproximately-optimalsolverandbytak-
obj(P1,xt):
ing X− =X for the shallow solver.
In
α
addition, we shall now prove that every α-
obj(P1,down(x))=E 1↑(down(x))
approximately-optimal solver is an α-shallow-solver. =αE↑(x)=α·obj(P1,x)>obj(P1,xt)
1
Claim B.10. Anysolutionxt thatsatisfiestherequire-
ments of the approximately-optimal solver also satisfies
This in contradiction to xt being the returned solution
by the shallow solver.
the requirements of the shallow solver.
Proof. Let xt be a solution that satisfies the require- Lastly, we prove that for t = 1 a solution returned
ments of the approximately-optimal solver. By defini-
by the shallow solver might not satisfies the require-
tion, xt ∈ F(P1) and obj(P1,xt) ≥ α·obj(P1,x) for mentsoftheapproximately-optimalsolver.Thisimplies
all x ∈ F(P1). We need to prove that obj(P1,xt) ≥ thattherequirementsforouralgorithmareweakerthan
obj(P1,x) for all x∈F(P1)∩X−.
those of Hartman et al. (2023).
α
Suppose by contradiction that there exists x ∈
F(P1)∩X− such that obj(P1,x) > obj(P1,xt). Con- Claim B.12. There exists an instance for which the
α
siderup(x).ByLemmaB.1,up(x)∈X andE(up(x))= solution returned by the shallow solver does not satisfy
1E(x). We shall now see that up(x) ∈ F(P1). Con- the requirements of the approximately-optimal solver.
α
straints(P1.1–2)aresatisfiedasup(x)∈X.Constraint
(P1.3)issatisfiedasitdoesbyxandasalltheexpected Proof. Let α = 0.9, N = {1,2}, S = {s ,s ,s } and
1 2 d
values of up(x) are at least as those of x; specifically, u ,u as follows:
1 2
for any ℓ<t:
u (s )=10 u (s )=0 u (s )=0
1 1 1 2 1 d
Xℓ E↑(up(x))= 1 Xℓ E↑(x)≥Xℓ E↑(x)≥Xℓ z u 2(s 1)=10 u 1(s 2)=1000 u 1(s d)=0
i α i i i
i=1 i=1 i=1 i=1 At the first iteration, t = 1, the optimal solution
However, obj(P1,up(x))> 1obj(P1,xt): that maximizes the minimum expected value is (1,0,0)
α with objective value 10. However, the optimal solution
t t−1 among X− is (0.9,0,0.1) with objective value 9. Thus,
obj(P1,up(x))=X E↑(up(x))−X
z any
solutα
ion with a minimum expected value 9 satis-
i i
fies the requirements of the shallow solver (and also
i=1 i=1
1 t−1 1 t−1 ! the requirements of the approximately-optimal solver
X X
= E↑(x)− z ≥ E↑(x)− z by claim B.11).
α i i α i i Suppose that the solver returned this solution,
i=1 i=1
1 1 (0.9,0,0.1), and so z := 9. Then, at the second it-
= obj(P1,x)> obj(P1,xt)
eration t = 2,
Constra1
int (P1.3) says that the smallest
α α
expected value is at least 9. As s is the only selection
Which means that α·obj(P1,up(x)) > obj(P1,xt) — that gives agent 1 a positive utili1 ty of 10, any solution
in contradiction to xt being approximately-optimal. that satisfies this constraint must give this selection a
probability of at least 0.9. Thus, the only solution in
X− that satisfies this constraint is (0.9,0,0.1) with ob-
Next, we prove that the opposite is true for t = 1, α
jective value 9:
but only for t=1.
Claim B.11. For t = 1, any solution xt that satisfies obj(P1,(0.9,0,0.1))
the requirements of the shallow solver also satisfies the
=E↑((0.9,0,0.1))+E↑((0.9,0,0.1))−z
requirements of the approximately-optimal solver. 1 2 1
=9+9−9=9
Proof. Let xt be a solution that satisfies the require-
ments of the shallow solver. By definition, xt ∈ F(P1) Therefore, any solution with objective value 9 satis-
and obj(P1,xt) ≥ obj(P1,x) for all x ∈ F(P1)∩X−. fies the requirements of the shallow solver. Specifically,
We need to prove that obj(P1,xt) ≥ α·obj(P1,x) fα or (0.9,0,0.1) does.
all x∈F(P1). However, as we have the solution (0.9,0.1,0) with
Suppose by contradiction that there exists x ∈ objective value 109, the solution (0.9,0,0.1) does not
F(P1) such that α · obj(P1,x) > obj(P1,xt). Con- satisfy the requirements of the approximately-optimal
sider down(x). By Lemma B.2, down(x) ∈ X− solver — as 0.9×109>9.
α
and E(down(x)) = αE(x). We shall now see that
14Algorithm 2: α-Shallow Solver for (P1) objective function is the sum of the smallest t utilities
minus a positive constant. Thus, it is clear that an up-
Input: An integer t∈N and rationals z ,...,z .
1 t−1 per bound on the sum of all utilities can be used also
* If t≥2, then also xt−1.
as an upper bound on the maximum objective value.
Oracles: an α-approximate-feasibility-oracle for P1
(III), an α-approximate black-box for the utilitarian Toperformthesearch,weuseuthatholdstheupper
welfare (I), and an arbitrary vector in F(P1). bound and l that holds the lower bound; one of which
Parameter: An error factor ϵ>0. is updated at each iteration: In addition, we use retSol
that is initialized to the solution given as input. It is
updated only in some of the iterations as follows.
1: Let retSol := the given arbitrary vector in F(P1).
Ateachiteration,weexaminethemidpointvaluebe-
2: Let l:=obj(P1,retSol)
3: Let U′ be the utilitarian welfare obtained by using tweentheupperandlowerbounds,z t = u 2+l,andquery
theoracleaboutthisvalue.Ifthevalueisdeterminedto
the approximate black-box with c =1 for i∈N.
i be Feasible, we update retSol to the solution returned
4: Let u:= 1U′.
α by the oracle, and the lower bound l:=z to search for
t
larger values. Otherwise, we update the upper bound
5: while u−l>ϵ do
6: Let z :=(l+u)/2.
u:=z
t
to search for smaller values. We stop the search
7: Let (at ns, x′) be the answer of the approximate- when l and u are sufficiently close — for now let us as-
feasibility-oracle for the value z . sume that we stop it when l = u; we revisit this issue
t extensively in Appendix C.1.
8: if ans = Feasible then
9: update l:=z . To prove that the solver acts as described above, we
10: update retSolt := x′. needtoshowthat (a)thereturnedx(retSol)isfeasible
11: else ▷ ans = Infeasible-Under-X− for P1; and (b) obj(P1,retSol)≥obj(P1,x) for all x∈
12: update u:=z t. α F(P1)∩X α−.
(a) Suppose first that the solver returns the initial
13: end if
value of retSol - by definition, it is feasible for P1. If
14: end while
the solver returns a modified value of retSol, then this
15: return retSol value must have been returned by the approximate-
feasibility-oracle. By definition of the oracle, the re-
turned solution is feasible.
C Designing a Shallow Solver for P1
(b) We first note that obj(P1,retSol) is always at
least the lower bound l. Next, suppose by contradic-
(Proof of Lemma 6.1)
tion that there exists x ∈ F(P1) ∩ X− such that
Recall Lemma 6.1: obj(P1,x) > obj(P1,retSol). Therefore, oα bj(P1,x) >
l. As we stop the search when l = u, this implies
Lemma. Given an α-approximate-feasibility-oracle for
obj(P1,x)>u. But u is a value for which the approx-
P1 (III), an α-approximate black-box for the utilitarian
imate feasibility oracle has asserted Infeasible-Under-
welfare (I), and an arbitrary vector in F(P1). An
X−. By monotonicity, as obj(P1,x) > u, this im-
efficient α-shallow-solver for P1 (II) can be design. α
plies thatobj(P1,x) is Infeasible-Under-X− too. But
α
x∈F(P1)∩X− and has an objective value obj(P1,x)
Proof. The solver is described in Algorithm 2. It per- α
— a contradiction.
forms a binary search over the potential objective-
values z for the program P1.
t
We start by proving that performing a binary search
makessenseaswehavemonotonicity.First,ifsomeob- C.1 The Binary Search Error
jective value z is Feasible (i.e., there exits a solution The shallow solver described in Algorithm 2 actually
t
x ∈ F(P1) with objective value at least z t), then any has an additional additive error ϵ > 0 that arises from
value z− ≤z is also Feasible. To see that, assume that the binary search ending condition. So far, we have as-
t t
z isFeasible,andletx∈F(P1)beasolutionwithob- sumed that this error is negligible, as it can be made
t
jectivevalueatleastz .Clearly,thesamexalsohasan smaller than ϵ in O(log1), for any ϵ>0. Here we pro-
objective value at least t z−. Similarly, if z is Infeasible, videamoreaccurateanaϵ lysis,thatdoesnotneglectthis
t t
then any value z+ ≥z is also Infeasible. error.
t t
Lines 1–4 set bounds for the binary search. Formally, let ϵ > 0 be the error obtained from the
As a lower bound, we use the objective value of the binarysearch;thatis,westopthesearchwhenu−l≤ϵ.
solution given as input. We claim that, with this modification, the solver de-
Foranupperbound,weusethegivenα-approximate scribed by Algorithm 2 returns a solution retSol such
black-box for the utilitarian welfare with c = 1 for all that obj(P1,retSol) ≥ obj(P1,x)−ϵ for all solutions
i
i ∈ N (i.e., with the original utilities u ), to obtain a x ∈ F(P1)∩X−. That is, the solver returns a solu-
i α
value U′. By definition of α-approximation, 1U′ is an tion whose objective is at least the maximum objective
α
upper bound on the sum of utilities. Recall that the among the subset X− minus ϵ. Clearly, this reduces to
α
15the definition of α-shallow solver when ϵ is negligible. Let xA be an (α,ϵ)-leximin-approximation. By defi-
We call it an (α,ϵ)-shallow-solver. nition,E(xA)⪰α·E(x)−ϵ·1 forallx∈X.Sincex∗ ∈
n
X,wegetthatE(xA)⪰α·E(x∗)−ϵ·1 .Now,consider
n
(II’) (α,ϵ)-Shallow-Solver for P1 down(x∗). By Lemma B.2, E(down(x∗)) = αE(x∗).
This implies that E(xA) ⪰ E(down(x∗))−ϵ·1 . By
n
Input: An integer t∈N and rationals z ,...,z . Lemma B.5, E(down(x∗)) ⪰ E(x) for all x ∈ X−.
1 t−1 α
Since subtracting the same constant from each com-
ponent preserves the leximin order, it follows that
Output: A solution xt ∈ F(P1) such that
obj(P1,xt)≥obj(P1,x)−ϵforanyx∈F(P1)∩X−.
E(down(x∗))−ϵ·1
n
⪰E(x)−ϵ·1
n
forallx∈X α−.By
α transitivity, this means that E(xA)⪰E(x)−ϵ·1 for
n
all x∈X−.
We also claim that using this type of solver in Algo- On theα other hand, let xA be an S-distribution such
rithm 1 affects the guarantees on its output as follows. that E(xA)⪰E(x)−ϵ·1 for all x∈X−. Let x′ ∈X.
pL oet ne1 nn tb ise 1a .v Tec ht eo nr ,o Af s lgiz oe rin th, mwh 1er re ete ua rc nh so ane soo lf ui tt is oncom xn- B αEy (L xe ′)m .m Aa
s
B d. o2 w, nd (o xw ′)n( ∈x′n ) X∈ −,X wα− ea gn ed
t
E tα h(d ao tw En (( xx A′) )) =
⪰
such that E(xn)⪰α·E(x)−ϵ·1
n
for all x∈X. E(down(x′))−ϵ·1 n; and aα s E(down(x′)) = αE(x′).
Comparingthethepreviousguarantees,herewehave this implies that E(xA)⪰αE(x′)−ϵ·1 .
n
an additional subtraction of ϵ from each component. It
is again clear, that this reduces to the definition of α- Weshallnowprovethefollowinglemmathatextends
leximin-approximation when ϵ is negligible. We call it Lemma B.7:
an (α,ϵ)-leximin-approximation. Lemma C.3. Given an (α,ϵ)-shallow-solver for P1,
Definition C.1 (An (α,ϵ)-leximin-approximation). Algorithm 1 returns an S-distribution xn such that
An S-distribution, xA ∈ X, is an (α,ϵ)-leximin- E(xn)⪰E(x)−ϵ·1 n for all x∈X α−.
approximationifE(xA)⪰α·E(x)−ϵ·1 forallx∈X. Together with Lemma C.2, this proves that Algo-
n
rithm 1 returns an (α,ϵ)-leximin-approximation.
Weshallnowprovideare-analysisofAlgorithm2and
Algorithm 1, which considers the binary search error. Proof. supposebycontradictionthatthereexistsax′ ∈
X−suchthatE(x′)−ϵ·1 ≻E(xn).Bydefinition,there
Re-Analysis of Algorithm 2. We prove a moreac- α n
curate version of Lemma 6.1: exists an integer 1 ≤ k ≤ n such that E↑(x′)−ϵ =
i
E↑(xn) for i≤k, and E↑(x′)−ϵ>E↑(xn).
Lemma C.1. Given an α-approximate-feasibility- i k k
As xn is a solution for the program P1 that was
oracle for P1 (III), an α-approximate black-box for
solved in the last iteration (t = n), we can conclude
the utilitarian welfare (I), and an arbitrary vector in
F(P1). An efficient (α,ϵ)-shallow-solver for P1 (II) that Pk i=1E i↑(xn) ≥ Pk i=1z i (by constraint (P1.3) if
can be design. k <nandbyitsobjectiveotherwise).Thisimpliesthat
the objective value of xn for the program P1 that was
Proof. The proof is similar to the original proof except solved in k-th iteration at least z t:
(b),whichherebecomesobj(P1,retSol)≥obj(P1,x)−
k k−1
ϵ for all x∈F(P1)∩X α−. X E↑(xn)−X z ≥z (2)
Suppose by contradiction that there exists x ∈ i i k
F(P1)∩X− suchthatobj(P1,x)−ϵ>obj(P1,retSol). i=1 i=1
α
Then, as obj(P1,retSol) ≥ l, we get that obj(P1,x) > We shall now see that x′ is also a solution to this
l+ϵ. As we stop the search when u−l ≤ ϵ, this im- problem. Constraints (P1.1–2) are satisfied since x′ ∈
plies obj(P1,x) > u. But u is a value for which the X− ⊆ X. For Constraint (P1.3), we notice that the
α
approximate feasibility oracle has asserted Infeasible- (k−1)leastexpectedvaluesofx′ arehigherthanthose
Under-X−. By monotonicity, as obj(P1,x) > u, this of xn, which means that, for any ℓ<k:
α
impliesthatobj(P1,x)isInfeasible-Under-X− too.But
x∈F(P1)∩X− and has an objective valueα obj(P1,x) Xℓ Xℓ (cid:16) (cid:17) Xℓ Xℓ
α E↑(x′)= E↑(xn)+ϵ ≥ E↑(xn)≥ z
— a contradiction. i i i i
i=1 i=1 i=1 i=1
Re-Analysis of Algorithm 1. We start by prov- Therefore,x′ isalsoasolutionforthisprogram,andits
ing the following lemma that extends Lemma B.6, and objective value for it is:
providesanotherequivalentdefinitionfor(α,ϵ)-leximin-
approximation: k k−1
X X
E↑(x′)− z
Lemma C.2. An S-distribution xA is an (α,ϵ)- i i
leximin-approximation if and only if E(xA) ⪰ E(x)− i=1 i=1
ϵ·1 for all x∈X−. We shall now see that this means that the objective
n α
value of x′ is higher by more than ϵ than the objec-
Proof. Let x∗ be a leximin optimal S-distribution. tive value of the solution returned by the solver in this
16iteration, namely z . Now, consider the sum P m , we can change the
k i∈N k,i
order of the elements as follows:
k k−1 k−1 k−1
X E↑(x′)−X
z
=X E↑(x′)+E↑(x′)−X
z
X
m =
X
max(0,v↑−v )
i i i k i k,i k i
i=1 i=1 i=1 i=1 i∈N i∈N
By definition of x′ for i<k: = X max(0,v↑−v↑)
k i
k X−1 (cid:16) (cid:17) k X−1 i∈N
= E i↑(xn)+ϵ +E k↑(x′)− z i Asv↑isthek-thleastvalueofv,wegetthatv↑−v↑ ≥
k k i
i=1 i=1 0 for any i<k, that v↑−v↑ =0; and that v↑−v↑ ≤0
k X−1 k X−1 for any i>k. It followk s thak t: k i
≥ E↑(xn)+E↑(x′)− z
i k i k−1
i=1 i=1 X m =X (v↑−v↑)
By definition of x′ for k: k,i k i
i∈N i=1
k−1 k−1
X X k−1
> E i↑(xn)+E k↑(xn)+ϵ− z i =(k−1)v k↑−X v i↑
i=1 i=1
i=1
k k−1 WecannowprovethatEquation(5)issatisfiesaswell:
X X
= E i↑(xn)− z i+ϵ X
ky − m
i=1 i=1 k k,i
By Equation (2): i∈N
≥z +ϵ k X−1
t =kv↑−(k−1)v↑+ v↑
But this contradicts the guarantees of our shallow k k i
i=1
solver — by definition, the objective value of the solu-
k
tion returned by the solver, namely z k, is at least as =X v↑ ≥c (As Equation (3) holds)
high as the objective of any solution in F(P1)∩X− i
minus ϵ. α — i=1
On the other hand, assume that there exist y ∈R and
k
D Equivalence Between (P2) and (P3) m ∈RN that satisfy Equations (4–6).
k
(Including Proof of Lemma 8.1) By Equations (5–6), we get that m ≥max(0,y −
k,i k
The proof uses the following lemma, which considers v i)fori∈N.Usingthesametechniqueofchangingthe
order of the elements we can conclude that:
general vectors:
X X X
Lemma D.1. Let c∈R
≥0
be a non-negative constant, m k,i ≥ max(0,y k−v i)= max(0,y k−v i↑)
v∈RN any vector, and k ∈N. Then, i∈N i∈N i∈N
Now, consider the left hand side of Equation (4), it
k
X follows that:
v↑ ≥c (3)
i ky −X m ≤ky −X max(0,y −v↑)
i=1 k k,i k k i
if and only if there exist y ∈R and m ∈RN s.t. i∈N i∈N
k k
ky
k−Xn
m
k,i
≥c (4) ≤ky
k−Xk
max(0,y k−v i↑)
i=1 i=1
m
k,i
≥y k−v
i
∀i∈N (5) =Xk (cid:16)
y −max(0,y
−v↑)(cid:17)
m ≥0 ∀i∈N (6) k k i
k,i
i=1
WenotethatthisproofsimplifiestheproofofLemma However, each element in the sum can be simplified to
7 in (Hartman et al. 2023).
min(y ,v↑): if max(0,y −v↑) = 0 (which means that
k i k i
Proof. Assume that Equation (3) holds, that is: y ≤ v↑) then this element gives y − 0 = y , and
k i k k
Pk v↑ ≥c. Let otherwise (if max(0,y − v↑) > 0 which means that
i=1 i k i
y k :=v k↑ wy k e> cav ni↑) coit ncg li uv de esy thk a− t:(y k−v i↑)=v i↑.Whichmeansthat
m k,i :=max(0,v k↑−v i) ∀i∈N X Xk (cid:16) (cid:17)
It is easy to see that Equation (6) is satisfied. For ky − m ≤ y −max(0,y −v↑)
k k,i k k i
Equation (5), observe that:
i∈N i=1
m
k,i
=max(0,v k↑−v i) =Xk
min(y
,v↑)≤Xk
v↑
≥v↑−v =y −v k i i
k i k i i=1 i=1
17However, by Equation (4), ky −P m ≥c, so we E Primal-Dual Derivation
k i∈N k,i
can conclude that Pk v↑ ≥c — as required. This is the primal LP - the program P3, in standard
i=1 i form, with the corresponding dual variable shown to
Using this general lemma, we now prove an equiv-
the left of each constraint.
alence between Constraint (P2.2) and the set of Con-
straints (P3.2–4):
|S|
Lemma D.2. x satisfies Constraint (P2.2) if-and- X
min x s.t. (P3)
only-if there exist y and m for 1 ≤ ℓ ≤ t and j
ℓ ℓ,i
1≤i≤n such that (x,y,m) satisfies (P3.2–4). j=1
n ℓ
Proof. Let x ∈ R|S| that satisfies Constraint (P2.2) — q (1) ℓy −X m ≥X z ∀ℓ∈[t−1]
ℓ ℓ ℓ,i i
that is,
i=1 i=1
ℓ ℓ |S|
X E↑(x)≥X z ∀ℓ∈[t] (7) v (3) m −y +X x ·u (s )≥0 ∀ℓ∈[t], ∀i∈[n]
i i ℓ,i ℓ,i ℓ j i j
i=1 i=1 j=1
Let 1 ≤ ℓ ≤ t. Combining Lemma D.1 with Equa- (4) m ≥0 ∀ℓ∈[t], ∀i∈[n]
ℓ,i
wtio en ge( t7) thw ish ie tre poc ss: i= bleP ifℓ i= a1 nz di, onv ly:=
if
E th( ex r) ea en xd istk y:= ∈Rℓ, (5) x j ≥0 j =1,...,|S|
and m ∈RN s.t. ℓ This is the dual LP - the program D3, in standard
ℓ ≥0 form, with the corresponding primal variable shown to
n ℓ the left of each constraint.
X X
ℓy − m ≥ z
ℓ ℓ,i i
t ℓ
i=1 i=1 max X q X z s.t. (D3)
m ≥y −E (x) ∀i∈N ℓ i
ℓ,i ℓ i
ℓ=1 i=1
m ≥0 ∀i∈N
k,i n t
X X
Putting it all together, we get that x satisfies Con- x j(1) u i(s j) v ℓ,i ≤1 ∀j =1,...,|S|
straint (P2.2) if-and-only-if there exist y ℓ and m ℓ,i for i=1 ℓ=1
1 ≤ ℓ ≤ t and 1 ≤ i ≤ n such that (x,y,m) satisfies n
X
Constraints (P3.2–4) — as required. y (2) ℓq − v ≤0 ∀ℓ∈[t]
ℓ ℓ ℓ,i
ItissimpletoverifythatthisimpliesourLemma8.1 i=1
m (3) −q +v ≤0 ∀ℓ∈[t], ∀i∈[n]
Lemma. Let (cid:0) xA,yA,mA(cid:1) be a poly-sparse 1- ℓ,i ℓ ℓ,i
α (4) q ≥0 ∀ℓ∈[t]
approximately-optimal solution for P3. Then, xA is ℓ
a poly-sparse 1-approximately-optimal solution for P2. (5) v ℓ,i ≥0 ∀ℓ∈[t], ∀i∈[n]
α
Proof. To prove that xA is a poly-sparse 1- F Ellipsoid Method Variant for
α
approximately-optimal solution for P2, we need to Approximation
prove that (a) xA is a poly-sparse solution for P2, and Thisappendixpresentsavariantoftheellipsoidmethod
(b) its objective value is 1-approximately-optimal.
designed to approximate linear programs (LPs) that
α
(a) First, xA is a poly-sparse vector since cannot be solved directly due to a large number of
(cid:0) xA,yA,mA(cid:1) is. Second, since (cid:0) xA,yA,mA(cid:1) is a so- variables. The method relies on an approximate sep-
lution for P3, xA satisfies Constraint (P3.1) which arationoracleforthedualprogram.Theappendixuses
is similar to Constraint (P2.1). In addition, it means standardnotationforlinearprograms(bothprimaland
that (cid:0) xA,yA,mA(cid:1) satisfies Constraints (P3.2–4). By dual); it is self-contained, and the notations used here
are independent of the notation used in the main pa-
Lemma D.2, this means that xA satisfies Constraint
per.Themethodintegratestechniquesfrom(Gr¨otschel,
(P2.2). Thus, xA is a solution for P2.
Lov´asz,andSchrijver1993,1981;KarmarkarandKarp
(b) Suppose by contradiction that xA is not 1-
α 1982).
approximately-optimal. As P2 is a minimization pro-
gram, this means that there exists a solution x for P2 Lemma F.1. Given a 1-approximate-separation-
α
such that obj(P2,x)> 1 obj(P2,xA). By Lemma D.2, oracle for the (max.) dual program, a poly-sparse 1-
thismeansthatthereexα istyandmsuchthat(x,y,m)
approximately-optimal solution for the (min.)
primα
al
is a solution for P3. However, as both P2 and P3 has
program can be obtained in polynomial time.
the same objective, we get that:
Thegoalistosolvethefollowinglinearprogram(the
obj(P3,(cid:0) xA,yA,mA(cid:1))> 1 obj(P3,(x,y,m)) primal):
α
In contradiction to the fact that (cid:0) xA,yA,mA(cid:1) is 1- min cT ·x (P)
approximately-optimal for P3. α s.t. A·x≥b, x≥0;
18We assume that (P) has a small number of constraints, bTy′ ≥bTy∗ (10)
butmayhaveahugenumberofvariables,sowecannot
solve (P) directly. We consider its dual:
Inequality (9) holds since, by definition, y′ is
max bT ·y approximately-feasible.
(D)
s.t. AT ·y ≤c, y ≥0. To prove (10), suppose by contradiction that bTy∗ >
bTy′. Since y∗ is feasible for (D), it is in the initial el-
Assumethatbothproblemshaveoptimalsolutionsand lipsoid. It remains in the ellipsoid throughout the algo-
denote the optimal solutions of (P) and (D) by x∗ and rithm:itisremovedneitherbyafeasibilitycut(sinceit
y∗ respectively. By the strong duality theorem: is feasible), nor by an optimality cut (since its value is
at least as large as all values used for optimality cuts).
cT ·x∗ =bT ·y∗ (8)
Therefore,itremainsinthefinalellipsoid,anditischo-
While (D) has a small number of variables, it has sen as the highest-valued feasible point rather than y′
a huge number of constraints, so we cannot solve it — a contradiction.
directlyeither. Inthis Appendix, we showthat (P) can
Now, we construct a reduced version of (D), where
be approximated using the following tool:
there are only at most K constraints — only the con-
Definition F.1. An approximate separation oracle straints used to make feasibility cuts. Denote the re-
(ASO) for the dual LP is an efficient function parame- duced constraints by AT ·y ≤ c , where AT is a
terized by a constant β ≥0. Given a vector y it returns matrixcontainingasubsr eed tofatmore sd tK rowsofre od fAT,
one of the following two answers: andc isavectorcontainingthecorrespondingsubset
red
1. ”y is infeasible”. In this case, is returns a violated of the elements of c. The reduced-dual LP is:
constraint, that is, a row aT ∈ AT such that aTy >
i i
c i. max bTy
2. ”y is approximately feasible”. That means that (RD)
ATy ≤(1+β)·c s.t. AT red·y ≤c red, y ≥0
Given the ASO, we apply the ellipsoid method as
follows(thisisjustasketchtoillustratethewayweuse Notice that it has the same number of variables as
the MASO; it omits some technical details): the program (D). Further, if we had run this ellipsoid
method variant on (RD) (instead of (D)), then the re-
• Let E 0 be a large ellipsoid, that contains the entire sultwouldhavebeenexactlythesame—y′.Therefore,
feasible region, that is, all y ≥0 for which ATy ≤c. (10) holds for the (RD) too:
• For k =0,1,...,K (where K is a fixed constant, as
will be explained later):
bTy′ ≥bTy∗ (11)
– Let y be the centroid of ellipsoid E . red
k k
– Run the MASO on y .
k where y∗ is the optimal value of (RD).
– If the MASO returns ”y is infeasible” and a vi- red
k
olated constraint aT, then make a feasibility cut As AT contains a subset of at most K rows of AT,
— keep in E k+1 oi nly those y ∈ E k for which the matr red ix A red contains a subset of columns of A.
aTy ≤c . Therefore, the dual of (RD) has only at most K vari-
i i ables,whicharethosewhocorrespondtotheremaining
– IftheMASOreturns”yisapproximatelyfeasible”,
thenmakeanoptimality cut —keepinE only
columns of A:
k+1
those y ∈E for which bTy ≥bTy .
k k
• Fromthesety 0,y 1,...,y K,choosethepointwiththe min cT red·x red (RP)
highest bT ·y among all the approximately-feasible s.t. A ·x ≥b, x ≥0
k red red red
points.
Since both cuts are through the center of the ellip- Since (RP) has a polynomial number of variables and
soid, the ellipsoid dilates by a factor of at least 1 constraints, it can be solved exactly by any LP solver
r
at each iteration, where r > 1 is some constant (see (not necessarily the ellipsoid method). Denote the op-
(Gr¨otschel, Lov´asz, and Schrijver 1981) for computa- timal solution by x∗ .
red
tionofr).Therefore,bychoosingK :=log 2r·L,where
Let x′ be a vector which describes an assignment to
L is the number of bits in the binary representation the variables of (P), in which all variables that exist
of the input, the last ellipsoid E K is so small that all in (RP) have the same value as in x∗ , and all other
pointsinitcanbeconsideredequal(uptotheaccuracy red
variables are set to 0.It follows that A·x′ =A ·x∗ ,
of the binary representation). red red
therefore, since x∗ is feasible to (RP), x′ is feasible
The solution y′ returned by the above algorithm sat- red
to (P). Similarly, cT · x′ = cT · x∗ . We shall now
isfies the following two conditions: red red
see that this implies that the objective obtained by x′
ATy′ ≤(1+β)·c (9) approximates the objective obtained by x∗:
19objective value. On the other hand, if the oracle is cor-
rectinallofitsoperations,theellipsoidmethodvariant
cT ·x′ =cT ·x∗ would indeed produce an approximately optimal solu-
red red
By strong duality for the reduced LPs: tion. That is, with probability pE the ellipsoid method
variant returns an approximately optimal solution (as
=bT ·y∗
red was for the deterministic oracle).
By Equation (11):
≤bT ·y′
G Using a Randomized Black-box
By the definition of (P):
(Proof of Theorem 11.3)
≤(A·x∗)Ty′
This appendix extends our main result to the use of
By properties of transpose
a randomized black-box for utilitarian welfare, defined
and associativity of multiplication: as follows: the black-box returns a selection that α-
approximatestheoptimalutilitarianwelfarewithprob-
=(x∗)T(AT ·y′)
ability p, and an arbitrary selection otherwise. Section
By Equation (9):
G.1 provides a summary of known claims we use inside
≤(x∗)T((1+β)·c) the proof.
Recall that Theorem 11.3 says that:
By properties of transpose:
Theorem. Given a randomized α-approximate black-
=(1+β)·(cTx∗)
box for the utilitarian welfare with success probability
So,x′ (x∗ withallmissingvariablessetto0)isanap- p ∈ (0,1]. An α-leximin-approximation can be ob-
red
proximatesolutiontotheprimalLP(P)—asrequired. tained with probability p in time polynomial in n and
the running time of the black-box.
F.1 Using a Randomized Approximate
Proof. We first prove that the use of the random-
Separation Oracles ized black-box does not effect feasibility — that is,
Here, we allow the oracle to also be half-randomized, the output returned by Algorithm 1 is always an S-
thatis,whenitsaysthatasolutionisinfeasible,itisal- distribution. Then, we prove the required guarantees
wayscorrectandreturnsaviolatedconstraint;however, regarding its optimally.
when it says that a solution is approximately feasible, Recallthattheblackboxisusedinonlytwoplaces–
itisonlycorrectwithsomeprobabilityp∈[0,1].LetE as part of the binary search to obtain an upper bound
beanupperboundonthenumberofiterationoftheel- and as part of the separation oracle for D3. Inside the
lipsoidmethodonthegiveninputwhenoperatingwith binary search, if the obtained value does not approx-
adeterministicoracle;weprovethatahalf-randomized imate the optimal utilitarian welfare, it might cause
oracle can be utilized as follows: us to overlook larger possible objective values. While
this could affect optimality, it will not impact feasibil-
Lemma F.2. Given a half-randomized 1-
α ity. As for the separation oracle, this change makes the
approximate-separation-oracle for the (max.) dual
oracle half-randomized as described in Appendix F.1
program, with success probability p ∈ (0,1], a poly-
since we might determine that Constraint (D3.1) is
sparse 1-approximately-optimal solution for the (min.)
α approximately-feasible even though it is not. However,
primal program can be obtained in polynomial time
by Lemma F.2, the solution returned by the ellipsoid
with probability pE.
methodwillstillbefeasiblefortheprimal,whichmakes
the solution we eventually return through Algorithm 1
Proof. Since the ellipsoid method variant is iterative,
also feasible (i.e., in X).
and since the oracle calls are independent, there is a
We can now move on to the optimally guarantees.
probabilitypE thattheoracleanswerscorrectlyineach
Let k be an upper bound on the total number of calls
iteration,andso,theoverallprocessperformsasbefore.
for the black-box.
We first explain why, using a half-randomized oracle,
this ellipsoid method variant always returns a feasible k =n·( 1 + logU · E )
solutiontotheprimal(eveniftheoraclewasincorrect). |{z} |{z} |{z}
Since the oracle is always correct when it determines forupperbound binarysearch ellipsoid
that a solution is infeasible and as the construction of Notice that k is polynomial in the problem size.
(RD)isentirelydeterminedbytheviolatedconstraints, Under the assumption that success events of differ-
we can use the same arguments to conclude that x′ ent activations are independent, it is clear that if the
would still be a feasible solution for P. solver succeeds in all the executions, the returned so-
However, since the oracle might be mistaken when it lution will be a leximin approximation (as everything
determinesthatasolutionisapproximately-feasiblefor behavesasitwouldwithadeterministicsolver).There-
the dual, ellipsoid method variant might return a solu- fore, with probability pk, we return a leximin approxi-
tionthatnotnecessarilyhaveanapproximatelyoptimal mation. However, pk <p since k >1 and p∈(0,1).
20To increase the probability of success, we can use a Toillustratethedifference,weuseaslightlymodified
new black-box that operates as follows. It operates the version of an example given by (Abernethy, Schapire,
given black-box multiple times and then returns the and Syed 2024) (Appendix B)16 to compare their defi-
best outcome. The probability of success of this new nition to the one by Hartman et al. (2023).
black-box is the probability that at least one of these
Example H.1. We have 3 agents and only 4 possible
operationsissuccessful,whichcanalsobecalculatedas
outcomes in X, where:
1 minus the probability that all operation fail. Specifi-
cally,byoperatingthegivingblack-boxq ≥1times,the E↑(x1)=(10,10,100)
success probability is (1−(1−p)q). We then get that
E↑(x2)=(9,9,90)
the the success probability of the overall success prob-
ability of the algorithm becomes (1−(1−p)q)k, which E↑(x3)=(9,50,50)
is at least p for q :=⌈log(1/k) +1⌉ : E↑(x4)=(8,1000,1000)
log(1−p)
(1−(1−p)q)k For simplicity, we consider α = 0.9. Notice that the
optimal solution is x1.
≥1−k·(1−p)q (By Claim G.1)
Our Definition. Recall that our definition says
=1−k·(1−p)⌈ ll oo gg (( 11 −/k p) )+1⌉ that an S-distribution x′ is α-leximin-approximation if
E(x′)⪰αE(x′) for any x∈X.
≥1−k·(1−p) ll oo gg (( 11 −/k p) )+1 (By Claim G.2)
InExampleH.1,ourdefinitionsaysthatthethreeso-
=1−k·(1−p) ll oo gg (( 11 −/k p) )(1−p) l xuti io sn ns ox t1 s, ix n2 c, ea 0n .d 9·x E3 (a xr 1e )0 ≻.9- Eap (xp 4ro ).ximations.However,
1 4
=1−k· ·(1−p)=p (By Claim G.3) Hartman et al. (2023). Recently, Hartman et al.
k
(2023) proposed a general definition for leximin-
It is important to note that q is O(logk): approximation,17 whichinourcontextcanbedescribed
as follows. First, an S-distribution x is said to be α-
log(1/k) log(1/k)
q =⌈ +1⌉≤ +2 preferred over another S-distribution x′, denoted by
log(1−p) log(1−p) x ≻ x′, if there exists an 1 ≤ k ≤ n such that
α
logk E↑(x) ≥ E↑(x′) for i < k and E↑(x) > 1E↑(x′).
= +2 i i k α k
log 1 Then, an S-distribution x′ is said to be α-leximin-
(1−p) approximation if there is no x such that x≻ x′.
α
As k is polynomial in the problem size, Algorithm 1 In Example H.1, their definition says that the two
remainspolynomialintheproblemsizeevenwhenusing solutions x and x are 0.9-approximations. However,
1 3
this new black-box. both x2 and x4 are not - x2 is not since x3 ≻ x2 (for
0.9
k =2), while x4 is not since x1 ≻ x4 (for k =1).
0.9
This definition is stronger than ours:
G.1 Required Background
ClaimH.1. Anyα-leximin-approximationaccordingto
Claim G.1. For any ϵ∈(0,1) and k ∈Z +: the definition of Hartman et al. (2023) is also a α-
leximin-approximation according to our definition, but
(1−ϵ)k ≥1−k·ϵ
the opposite is not true.
Claim G.2. For any a∈(0,1), and c>b>0: Proof. Let x′ ∈ X. We first prove that if x′ is not
anα-leximin-approximationaccordingtoourdefinition
ac ≤ab
then x′ is not an α-leximin-approximation according
Claim G.3. For any a,b>0: to their definition. Clearly, this implies that if x′ is
an approximation according to their definition, it is
bl lo og ga
b
=a also an approximation according to our definition. If
x′ is not an approximation according to our defini-
H Definitions for Leximin tion, this means that there exists an x ∈ X such that
αE(x) ≻ E(x′). Which means that there exists an in-
Approximation: Comparison
teger k ∈ [n] such that αE↑(x) = E↑(x′) for i < k
In this paper, we propose a new definition for leximin- i i
approximation that applies only problems considered and αE k↑(x) > E k↑(x′). For any i < k, as α ∈ (0,1], we
here (for which using a lottery is meaningful). There
16The definition considered in (Abernethy, Schapire, and
are other definitions for leximin approximation. This
Syed2024)isforadditiveapproximation,weadaptitallfor
appendixprovidesanextensivecomparisonbetweenthe
multiplicative approximations.
differenttypesofdefinitionsforleximin-approximation.
17The definition in (Hartman et al. 2023) capture both
However, to make comparison easier, we describe the
-additive and multiplicative errors, combined; here we only
definitions only for our model. consider multiplicative errors.
21get that αE↑(x)=E↑(x′) implies E↑(x)≥E↑(x′). For undertheelement-wisedefinition.Accordingtoourdef-
i i i i
k, it is easy to see that E (x) > 1E (x′). Therefore, inition, x3 is also considered a leximin-approximation,
x ≻ x′. But this means tk hat x′ isα nok t an approxima- andthisisnocoincidence—anysolutionpreferredover
α
tion according to their definition. anotherthatisanapproximationisitselfconsideredan
To see that the opposite is not true, consider Ex- approximation.
ample H.1 and observe that x2 is an approximation
according to our definition but it is not according to
their.
However,thispaperprovesthatourdefinitioncanbe
obtained for many problems, whereas their approxima-
tion appears to be very challenging to obtain.
Element-Wise. Another common definition is an
element-wise approximation, e.g., in (Abernethy,
Schapire, and Syed 2024) and (Kleinberg, Rabani, and
E´vaTardos2001)(inwhichitiscalledacoordinate-wise
approximation). In our context, it can be described as
follows.AnS-distributionx′ issaidtobeα-leximindef-
inition if E↑(x) ≥ αE↑(x∗) for all i ∈ N, where x∗ is
i i
the leximin-optimal solution.
InExampleH.1,thisdefinitionsaysthatthetwosolu-
tions x and x are 0.9-approximations. However, both
1 2
x3 and x4 are not - x3 is not since x3 = 50 < 90 =
3
0.9·100 = 0.9·x2, while x4 is not since x∗ = 8 < 9 =
2 1
0.9·10=0.9·x∗ (recall that x∗ =x1 in this example).
1
This definition is also stronger than ours in the same
sense (as described for (Hartman et al. 2023)):
Claim H.2. Any α-leximin-approximation according
to the element-wise definition is also a α-leximin-
approximation according to our definition, but the op-
posite is not true.
Proof. Let x∗ ∈X be a leximin-optimal S-distribution
and let x′ ∈X be an α-leximin-approximation accord-
ing to the element-wise definition. This means that
E↑(x′) ≥ αE↑(x∗) for any i ∈ [n]. We first prove
i i
that this also means that E(x′) ⪰ αE(x∗). If E(x′) ≡
αE(x∗) then the claim is clearly holds. Otherwise, let
k be the smallest integer such that E↑(x′) > αE↑(x∗)
k k
(noticethattheremustbeonesinceE↑(x′)≥αE↑(x∗)
i i
for any i ∈ [n] and E(x′) ̸≡ αE(x∗)). We get that
E↑(x′)=E↑(x∗) for any i<k and E↑(x′)>αE↑(x∗).
i i k k
Therefore, E(x′) ⪰ αE(x∗). Now, let x ∈ X. By
the optimality of x∗, E(x∗) ⪰ E(x). By transitivity,
E(x′)⪰αE(x) — as required.
To see that the opposite is not true, consider Ex-
ample H.1 and observe that x3 is an approximation
according to our definition but it is not according to
their.
Wenotethathere,ourdefinitioncapturesthefollow-
ing problem. The solution x2 is considered an element-
wise0.9-approximationwhilex3 doesnot.However,the
expectedvectorofx3 isstrongly-leximin-preferredover
the expected vector of x2 (for k =2). Thus, in the lex-
imin sense, it seems reasonable that x3 would be con-
sidered at-least-a-good as x2, yet this is not reflected
22