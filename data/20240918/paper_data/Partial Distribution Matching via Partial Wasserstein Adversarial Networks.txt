1
Partial Distribution Matching via
Partial Wasserstein Adversarial Networks
Zi-Ming Wang, Nan Xue, Ling Lei, Rebecka Jo¨rnsten, Gui-Song Xia
Abstract—Thispaperstudiestheproblemofdistributionmatching(DM),whichisafundamentalmachinelearningproblemseekingto
robustlyaligntwoprobabilitydistributions.Ourapproachisestablishedonarelaxedformulation,calledpartialdistributionmatching
(PDM),whichseekstomatchafractionofthedistributionsinsteadofmatchingthemcompletely.Wetheoreticallyderivethe
Kantorovich-RubinsteindualityforthepartialWasserstain-1(PW)discrepancy,anddevelopapartialWassersteinadversarialnetwork
(PWAN)thatefficientlyapproximatesthePWdiscrepancybasedonthisdualform.Partialmatchingcanthenbeachievedbyoptimizing
thenetworkusinggradientdescent.Twopracticaltasks,pointsetregistrationandpartialdomainadaptationareinvestigated,wherethe
goalsaretopartiallymatchdistributionsin3Dspaceandhigh-dimensionalfeaturespacerespectively.Theexperimentresultsconfirmthat
theproposedPWANeffectivelyproduceshighlyrobustmatchingresults,performingbetteroronparwiththestate-of-the-artmethods.
IndexTerms—partialdistributionmatching,partialWassersteinadversarialnetwork,pointsetregistration,partialdomainadaptation
✦
1 INTRODUCTION
DISTRIBUTIONmatching(DM)isafundamentalmachine
learning task with many applications. As illustrated
in Fig. 1(a), given two probability distributions, the goal
of DM is to match one distribution to the other. For ex-
ample, in generative modelling, to describe the observed
(a) Giventwonormalizeddistributions,DMaimstomatchthe
data, a parametric model distribution is matched to the
sourcedistribution(red)tothereferencedistribution(blue).
data distribution. Recently advanced DM algorithms [1],
[2], [3] are well known for being able to handle highly
complicateddistributions.However,thesemethodsstillhave
difficulty handling distributions that are contaminated by
outliers,becausetheirattempttocompletelymatchthe“dirty”
distributionswillinevitablyleadtobiasedresults.
(b) Giventwounnormalizeddistributions,PDMaimstomatch
A natural way to address this issue is to consider the
acertainfractionofthesourcedistribution(red)tothereference
partialdistributionmatching(PDM)taskasarelaxationof distribution(blue).
theDMtask:giventwounnormalizeddistributionsofmass,
i.e., the total mass of each distribution is not necessarily Fig.1.ComparisonbetweenDMandPDM.DMaimstocompletelymatch
1, the goal of PDM is to match a certain fraction of mass twodistributions,butPDMonlyaimstomatchacertainfractionofthem.
ofthetwodistributions.AnexampleofPDMispresented
in Fig. 1(b). In contrast to DM, PDM allows to omit the
fractionofdistributionthatdoesnotmatch,thusitnaturally stein adversarial network (PWAN), based on deep neural
toleratesoutliersbetter.Inotherwords,itismoresuitable networksandtheoptimaltransport(OT)theory[7].Ourkey
forapplicationswithdirtydata. idea is to partially match the distributions by minimizing
SeveralPDMmethodshavebeeninvestigated,butthey their partial Wasserstein-1 (PW) discrepancy [8] which is
tendtobecomputationallyexpensiveforlarge-scalePDM approximated by a neural network. Specifically, we first
problems,becausetheygenerallyrelyonthecorrespondence derive the Kantorovich-Rubinstein (KR) dual form [9] for
betweendistributions[4],[5],[6].Toaddressthisissue,we the PW discrepancy, and show that its gradient can be
proposeanefficientPDMalgorithm,calledpartialWasser- explicitlycomputedviaitspotential.Theseresultsthenallow
us to compute the PW discrepancy by approximating the
• Zi-MingWangandRebeckaJo¨rnstenarewiththeDepartmentofMath- potentialusingadeepneuralnetwork,andminimizeitby
ematicalSciences,UniversityofChalmers,Gothenburg,41296,Sweden gradientdescent.ComparedtotheexistingPDMmethods,
(email:zimingwa@chalmers.se,jornsten@chalmers.se). theproposedPWANiscorrespondence-free,thusitismore
• Nan Xue is with Ant Group, Hangzhou, Zhejiang, 310013, China
efficient for large-scale PDM tasks. In addition, PWAN is
(email:xuenan@ieee.org).
• Ling Lei is with the School of Mathematics and Statistics, Wuhan a direct generalization of the well-known DM algorithm
University,Wuhan,430072,China(email:leiling@whu.edu.cn). WassersteinGAN(WGAN)[1]forPDMtasks.
• Gui-SongXiaiswiththeSchoolofComputerScience,NationalEngineering
To show the effectiveness of the proposed PWAN, we
Research Center for Multimedia Software (NERCMS), and Institute
forMathematicsandArtificialIntelligence,WuhanUniversity,Wuhan, applyittotwopracticaltasks:pointsetregistration[10]and
430072,China(email:guisong.xia@whu.edu.cn). partialdomainadaptation[11],[12],wherePWANisusedto
Correspondingauthor:Gui-SongXia matchdistributionsin3Dspaceandhighdimensionalfeature
4202
peS
61
]GL.sc[
1v99401.9042:viXra2
space respectively. Specifically, point set registration [10] - To demonstrate the ability of PWAN in matching dis-
seekstoaligntwopointsetsrepresentingincompleteshapes tributions in high dimensional feature space, we apply
in3Dspace.Weregardpointsetsasdiscretedistributions, PWAN to partial domain adaptation tasks. We provide
then the registration problem can be naturally formulated aconcretealgorithmforpartialdomainadaptation,and
as a PDM problem. On the other hand, the goal of partial experimentallydemonstrateitseffectiveness.
domain adaptation is to train a classifier for the test data
under the assumption that the test and training data are
sampledfromdifferentdomains,andthetestlabelsetisa 2 RELATED WORKS
subsetofthetraininglabelset.Weformulatetheadaptation
problem as a PDM problem, where we seek to align the PWANisdevelopedbasedonOTtheory[7],[8],[14],which
testfeaturedistributiontoafractionofthetrainingfeature isapowerfultoolforcomparingdistributions.Varioustypes
distribution,sothattheclassifierlearnedforthetrainingset of numerical OT solvers have been proposed [5], [6], [15],
canbereadilyusedforthetestset. [16],[17],[18].Awell-knowntypeofsolveristheentropy-
regularizedsolver[4],[19],[20],whichiterativelyestimates
ThesetwoPDMtaskshavetwocommonchallenges:1)
thetransportplan,i.e.,thecorrespondence,betweendistribu-
they involve large-scale distributions, such as point sets
consistingof105 points,orlarge-scaleimagedatasets,and tions.Thistypeofsolverisflexibleasitcanhandleawide
rangeofcostfunctions[7],butitisgenerallycomputationally
2)thedistributionsmaybedominatedbyoutliers,e.g.,the
outlier ratio can be up to 90% in partial adaptation tasks. expensiveforlarge-scaledistributions.Someefficientmini-
batch-basedapproximations[21],[22]havebeenproposedto
Thanks to its efficient formulation and partial matching
alleviatethisissue,butthesemethodsareknowntosuffer
principle,PWANcannaturallyaddressthesetwodifficulties.
frommini-batcherrors,thusextramodificationsareusually
The experiment results confirm that PWAN can produce
needed[23].
highlyrobustmatchingresults,anditperformsbetterthan
PWAN belongs to the Wasserstein-1-type OT solver,
orisatleastcomparablewiththestate-of-the-artmethods.
which is dedicated to the special OT problem where the
Insummary,thecontributionsofthisworkare:
costfunctionisametric.ThistypeofsolverexploitstheKR
- Theoretically,wederivetheKantorovich-Rubinstein(KR) dualityoftheWasserstein-1-typemetrics,thusitisgenerally
dualityofthepartialWasserstein-1(PW)discrepancy.We highly efficient. A representative method in this class is
furtherstudyitsdifferentiabilityandpresentaqualitative WGAN[1],whichapproximatestheWasserstein-1distance
descriptionofthesolutiontotheKRdualform. usingneuralnetworks.PWANisadirectgeneralizationof
- Based on the theoretical results, we propose a novel WGAN,becauseitapproximatesthePWdivergence,which
algorithm,calledPartialWassersteinAdversarialNetwork isageneralizationoftheWasserstein-1distance,underthe
(PWAN),forpartialdistributionmatching(PDM).PWAN same principle as WGAN. In addition, [18], [24] studied
approximatesdistance-typeormass-typePWdivergences the KR duality of various Wasserstein-1-type divergences,
byneuralnetworks,thusitcanbeefficientlytrainedusing includingthedistance-typePWdivergenceconsideredinthis
gradient descent. Notably, PWAN is a generalization of work.Ourworkcompletestheseworks[18],[24]inthesense
thewell-knownWassersteinGAN. thatweadditionallystudythemass-typePWdivergence,and
- We apply PWAN to point set registration, where we weprovideacontinuousapproximationofPWdiscrepancies
partially align discrete distributions representing point usingneuralnetworks,whichissuitableforabroadrange
setsin3Dspace.WeevaluatePWANonbothnon-rigid ofapplications.
andrigidpointsetregistrationtasks,andshowthatPWAN PointsetregistrationisaclassicPDMtaskthatseeksto
exhibitsstrongrobustnessagainstoutliers,andcanregister matchdiscretedistributions,i.e.,pointsets.Thistaskhasbeen
point sets accurately even when they are dominated by extensivelystudiedfordecades,andvariousmethodshave
noisepointsorareonlypartiallyoverlapped. been proposed [10], [25], [26], [27], [28], [29], [30]. PWAN
- We apply PWAN to partial domain adaptation, where is related to the probabilistic registration methods, which
wealignthetestfeaturedistributiontoafractionofthe solve the registration task as a DM problem. Specifically,
trainingfeaturedistribution.WeevaluatePWANonfour these methods first smooth the point sets as Gaussian
benchmarkdatasets,andshowthatitcaneffectivelyavoid mixturemodels(GMMs),andthenalignthedistributionsvia
biasedresultsevenwithhighoutlierratio. minimizingtherobustdiscrepanciesbetweenthem.Coherent
pointdrift(CPD)[10]anditsvariants[30]arewell-known
The rest of this work is organized as follows: Sec. 2
examples in this class, which minimize Kullback-Leibler
reviews some related works. Sec. 3 recalls the definitions
(KL) divergence between the distributions. Other robust
ofPWdivergenceswhicharethemajortoolsusedinPWAN.
Sec. 4 derives the formulation of PWAN, and provides discrepancies,includingL 2 distance[31],[32],kerneldensity
estimator [33] and scaled Geman-McClure estimator [34]
a concrete training algorithm. Some connections between
havealsobeenstudied.
PWANandWGANarealsodiscussed.Sec.5appliesPWAN
TheproposedPWANhastwomajordifferencesfromthe
topointsetregistration,andSec.6appliesPWANtopartial
existingprobabilisticregistrationmethods.First,itdirectly
domainadaptation.Sec.7finallydrawssomeconclusions.
processes the point sets as discrete distributions instead
Thisworkextendsourearlierwork[13]inthefollowing
of smoothing them to GMMs, thus it is more concise and
ways:
natural.Second,itsolvesaPDMprobleminsteadofaDM
- We complete the point set registration experiments by problem, as it only requires matching a fraction of points,
applyingPWANtorigidregistrationtasks. thusitismorerobusttooutliers.3
Recently,someworkshavebeendevotedtoanewPDM NotethatL D,h canbeinterpretedasfindinganoptimalplan
task,i.e.,partialdomainadaptation[11],[12],[35],wherethe π withadistancethresholdh.Importantly,L D,h isknownto
goalistopartiallymatchtwofeaturedistributions.Thistask haveanequivalentform[18],[24]
wasfirstformallyaddressedby[12],whereare-weighting (cid:90) (cid:90)
procedurewasusedtodiscardafractionofthedistributions L D,h(α,β)= sup fdα− fdβ−hm β. (4)
asoutliers,andaDMmodelwasthenusedtoaligntherest f∈Lip(Ω) Ω Ω
−h≤f≤0
ofthedistributions.OtherDMmethods,suchasmaximum
mean discrepancy network [36], and more advanced re- We call (2) and (4) KR forms, and the solution to KR forms
weightingprocedures[11],[37],[38]havebeenstudied.To potentials.
eliminate the need for re-weighting procedures, the mini-
batchOTsolver[21],[23]hasbeeninvestigated.
4 THE PROPOSED METHOD
TheproposedPWANisconceptuallyclosetotheexisting
mini-batch-OT-based methods [21], [23] because they are WepresentthedetailsoftheproposedPWANinthissection.
based on the OT theory. However, in contrast to these We first formulate the PDM problem in Sec. 4.1. Then we
methods,PWANdoesnotsufferfrommini-batcherrors[23], present an efficient approach for the problem in Sec. 4.2
because it performs PDM for the whole data distributions and4.3.WefinallydiscusstheconnectionsbetweenPWAN
insteadofthemini-batchedsamples.Inaddition,theGAN- and the well-known WGAN [1] in Sec. 4.4 to provide a
basedadaptationmethods[39],[40]canbeseenasaspecial deeperunderstandingofPWAN.
caseoftheproposedPWAN,wherethedatadoesnotcontain
outliers. 4.1 ProblemFormulation
Let α ∈ M +(Ω), β ∈ M +(Ω′) be the reference and the
3 PRELIMINARIES ON PW DIVERGENCES
sourcedistributionofmass,whereΩ′andΩaretwocompact
subsets of Euclidean spaces. Let T θ : Ω′ → Ω denote a
Thissectionintroducesthemajortoolsusedinthiswork,i.e., differentiabletransformationparametrizedbyθ,andβ
θ
=
twotypesofPWdivergences:thepartial-massWasserstein-1 (T θ) #β ∈M +(Ω)denotethepush-forwardofβ byT θ.Our
discrepancy[8],[14]andthebounded-distanceWasserstein-1 goalistopartiallymatchβ
θ
toαbysolving
discrepancy [24], [41]. We refer readers to [7] for a more
completeintroductiontotheOTtheory. minL(α,β θ)+C(θ), (5)
θ
Let α,β ∈ M +(Ω) be the reference and the source
distributionofmass,whereΩisacompactmetricspace,and where L represents L M,m or L D,h, which measures the
M +(Ω) is the set of non-negative measures defined on Ω. dissimilarity between β θ and α, and C is a differentiable
Denotem β =β(Ω)andm α =α(Ω)thetotalmassofβ and regularizerthatreducestheambiguityofsolutions.
αrespectively.Thepartial-massWasserstein-1discrepancy
L M,m(α,β)isdefinedas 4.2 PartialMatchingviaPWDivergences
(cid:90) Toseetheeffectivenessofframework(5),wepresentatoy
L M,m(α,β)= inf d(x,y)dπ(x,y), (1)
example of matching discrete distributions in Fig. 2. Let
π∈Γm(α,β) Ω×Ω Y ={y j}r
j=1
andX ={x i}q
i=1
bethesourceandreference
whereΓ m(α,β)isthesetofπ ∈M +(Ω×Ω)satisfying 2-D point sets, β = (cid:80) yj∈Y δ yj and α = (cid:80) xi∈Xδ xi be the
corresponding discrete distributions, where δ is the Dirac
π(A×Ω)≤α(A), π(Ω×A)≤β(A) and π(Ω×Ω)≥m
function.WeseektofindatransformationT θ thatpartially
matchesY toX bysolving(5).
forallmeasurablesetA⊆ΩanddisthemetricinΩ.
In the complete OT case, i.e., m = m β = m α, L M,m is By expressing L M,m (1) and L D,h (3) in their primal
forms,wecanwrite(5)as
theWasserstein-1metricW 1,whichisalsocalledtheearth
mover’sdistance.AccordingtotheKantorovich-Rubinstein min(cid:88) π ijd(x i,T θ(y j))+C(θ)+const, (6)
(KR)duality[42],W 1 canbeequivalentlyexpressedas θ i,j
(cid:90) (cid:90)
where const is a constant that is not related to θ, and
W 1(α,β)= f∈s Lu ipp
(Ω)
Ωfdα− Ωfdβ. (2)
π ∈ Rq×r is the solution to (1) or (3) obtained via linear
programming.Werepresentthenon-zeroelementsinπ by
where Lip(Ω) represents the set of Lipschitz-1 functions linesegmentsinFig.2.Ascanbeseen,thevalueof(6)only
defined on Ω. (2) is exploited in WGAN [1] to efficiently dependsonthedistancesbetweenthecorrespondingpoint
computeW 1,wheref isapproximatedbyaneuralnetwork. pairs that are within the mass or distance threshold, thus
Several methods have been proposed to generalize (2) minimizing(6)willonlyalignthesepointpairssubjecting
to unbalanced OT [4], [16]. A unified framework of these totheregularizerC,whileomittingallotherpoints,i.e.,the
approaches can be found in [18]. Amongst these general- hollowpoints.Inotherwords,L M,m andL D,h providetwo
izations,weconsideredthebounded-distanceWasserstein-1 ways to control the degree of alignment of distributions
discrepancyL D,h,whichcanberegardedas(1)withasoft basedonthemassordistancecriteria.
massconstraint: However, although the solution to (6) indeed partially
(cid:90) matches the distributions, it is intractable for large-scale
L D,h(α,β)= inf d(x,y)dπ(x,y)−hm(π). (3)
discrete distributions or continuous distributions, due to
π∈Γ0(α,β) Ω×Ω4
Using the approximated KR forms of L M,m (10) and
L M,m (11),wecanfinallyrewrite(5)as
min(cid:0) −f w∗ ,h(T θ(y j))+C(θ)+const(cid:1) , (12)
θ
where f w∗ ,h is the solution to L M,m (10) or L D,h (11). To
seetheconnectionbetweenthisKR-basedformulation(12)
and the primal formulation (6), we present an example in
Fig.3.Incontrastto(6),whichmatchesafractionofβ θ tothe
(a) LM,6 (b) LD,0.9 correspondingpointsinα(1-strow),(12)iscorrespondence-
free, and it seeks to move β θ along the direction of ∇f w∗ ,h.
Fig.2.Thecomputedcorrespondenceπbetweenα(blue)andβ θ(red).
Notethatthepointswithzerogradients(hollowpointsinthe
3-rdrow)areomittedby(12),andthesamegroupofpoints
(hollowpointsinthe1-strow)arealsoomittedby(6),which
thehighcomputationalcostofsolvingforπ usingalinear
indicatestheconsistencyofthesetwoformulations.Further
programineachiteration.
numericalresultsinAppx.B.1confirmthatthesetwoforms
Toaddressthischallenge,weproposetoavoidcomputing
are indeed consistent. More discussions of the property of
π by optimizing L D,h or L M,m in their KR forms as an
thepotentialarepresentedinSec.4.4andAppx.A.2.
alternativetotheirprimalforms.Tothisend,wefirstpresent
the KR forms for L D,h and L M,m, and then show that
theyaredifferentiable.AsforL D,h,theKRformisknown 4.3 Algorithm
in (4), and we further show that it is valid to compute its WenowdevelopaconcretePDMalgorithmbasedonapprox-
gradient. We also show that a similar statement for L M,m imatedKRformsdiscussedinSec.4.2.Weusethefollowing
holds.Specifically,wehavethefollowingtwotheorems: threetechniquestofacilitateefficientcomputations.First,we
Theorem 1. L D,h(α,β) can be equivalently expressed as (4). n tee ne td ialto fue nn cs tu ior ne ,t ih .ea .,t bt oh ue nn de eu dra al ndne Ltw ipo scr hk itf zw -1,h .Wis ea env sa uli rd etp ho e-
T m dih fi fle edr re ea nsis s tu ia am bs lpo etlu aio lt mnio , on t sh tf e en∗ veL: rD yΩ w,h→ h( eα re,R .β Fθt uo ) rp i ts hro ecb ro ml ne tm oi rn eu( ,4 o w) u. esI hf w aT . vrθ . et.sa θt ,is afi ne ds ia s wbo hu en rede fd wn ie sss ab my ud lte i-fi lan yin erg pf ew rc,h e( px t) io= n(c Mlip L( P− )| wf w it( hx l) e|, a− rnh a, b0 l) e,
parameterw,andclip(·)istheclipfunction,i.e.,wetakethe
∇ θL D,h(α,β θ)=−(cid:90) ∇ θ(f∗◦T θ)dβ (7) n ite tg oa tt hiv ee inab tes ro vl au lte [−v hal ,u 0e ].o Tf oth ee nso uu rt ep tu ht eo Lf if pw sc( hx i) t, za nn ed ss,th we en ac dli dp
Ω
agradientpenalty[43]GP(f)=max xˆ∈Ω{||∇ xˆf(xˆ)||2,1}to
whenbothsidesarewell-defined.
thetraininglossdefinedbelow.
Theorem2. L M,m(α,β)canbeequivalentlyexpressedas Second,tohandlecontinuousorlarge-scaledistributions,
(cid:90) welearnf w,h inamini-batchmanner.Specifically,ineach
L M,m(α,β)= sup fd(α−β)+h(m−m β). (8) iteration, we sample q˜ and r˜ i.i.d. samples {x i}q i˜ =1 and
f∈L −i hp( ≤Ω f), ≤h 0∈R + Ω { rey si p}r i e˜ = c1 tivfr eo lym
,
at nh de n apo prm roa xl ii mze ad ted αist ari nb dut βio bn ys tm h1 α eiα
r
ea mnd pim r1 iβ caβ
l
There is a solution f∗ : Ω → R to problem (8). If T θ satisfies a distributions:α˜ = m q˜α (cid:80)q i˜ =1δ xi andβ˜= m r˜β (cid:80)r i˜ =1δ yi.
mildassumption,thenL M,m(α,β θ)iscontinuousw.r.t.θ,andis Finally,insteadofsolvingfortheoptimalf w∗ ,h,weonly
differentiablealmosteverywhere.Furthermore,wehave update f w,h for a few steps in each iteration to reduce
(cid:90) the computation cost. In summary, by combining all three
∇ θL M,m(α,β θ)=− ∇ θ(f∗◦T θ)dβ (9) techniques,weoptimize
Ω
minmaxL(α,β θ;w˜)+C(θ) (13)
whenbothsidesarewell-defined. θ w˜
by alternatively updating w˜ and θ using gradient descent,
Theorem1and2immediatelyallowustoapproximate
where
L D,horL M,musingneuralnetworks.Specifically,letf w,hbe
1a ,n weu hr ea rl en wetw ao nr dk hsat ∈isfy Ri +ng a− reh p≤ arf aw m,h et≤ ers0a on fd th∥ e∇ nf ew t, wh∥ or≤
k
L= m q˜α (cid:88)q˜ f w,h(x i)− m r˜β (cid:88)r˜ f w,h(T θ(y j))
f w,h,and∥∇f w,h∥isthegradientoff w.r.t.theinput.We i=1 j=1
cancomputeL M,m(α,β θ)andL D,h(α,β θ)accordingto(4) +h(m−m β)−GP(f w,h) (14)
and(8):
andw˜ ={w,h}forL M,m;
(cid:90) (cid:90)
L M,m(α,β θ)=m wa ,hx Ωf w,hdα− Ω(f w,h◦T θ)dβ
L=
m
q˜α
(cid:88)q˜
f w,h(x i)−
m
r˜β
(cid:88)r˜
f w,h(T θ(y j))
+h(m−m β), (10) i=1 j=1
−GP(f w,h) (15)
and
(cid:90) (cid:90) andw˜ ={w}forL D,h.Notethat
uL siD ng,h g(α ra, dβ iθ e) n= tdm ew sa cx entΩ af nw d,h bd aα ck− -pΩ ro( pf aw g,h a◦ tiT oθ n) .dβ−hm β (11) ∇ θL=−m r˜β (cid:88)r˜ ∇ θf w,h(T θ(y i)), (16)
j=15
π
f∗
||∇f∗||
(a) LM,25orLD,0.64 (b) LM,50orLD,1.09 (c) LM,78orLD,5 (d) W1,LM,98orLD,100
Fig.3. ComparisonoftheprimalformandourapproximatedKRformondiscretedistributionsα(blue)andβ θ(red).Thesolutionstothesetwo
formsarepresentedinthe1-stand2-ndrowrespectively,andthegradientsofthepotentialsarepresentedinthe3-rdrow.
andoptimizingthemin-maxproblem(13)isgenerallyknown Algorithm1PDMusingPWAN
asadversarialtraining[40],[44]. Input: referencedistributionα,sourcedistributionβ,trans-
WecallourmethodpartialWassersteinadversarialnet- form T θ, network f w,h, network update frequency u,
work(PWAN),andcallf w,h potentialnetwork.Thedetailed training type (“mass” or “distance”), mass threshold m
algorithm is presented in Alg. 1. For clearness, we refer or distance threshold h, training step T, regularizer C θ,
to PWAN based on L M,m and L D,h as mass-type PWAN batchsizeq˜andr˜
(m-PWAN)ordistance-typePWAN(d-PWAN)respectively. Output: learnedθ
We note that these two types of PWAN are not equivalent iftrainingtype=“mass”then
despitetheircloserelation.Specifically,foreach(α,β θ,m), w˜ ←(w,h); L←(14)
there exists an h∗ such that the solution to L D,h∗(α,β θ) elseiftrainingtype=“distance”then
recoverstothatofL M,m(α,β θ),butoptimizingL M,m(α,β θ) w˜ ←(w); L←(15)
isgenerallynotequivalenttooptimizingL D,h(α,β θ)with endif
any fixed h, because the corresponding h∗ depends on β θ, fort=1,...,T do
whichvariesduringtheoptimizationprocess. OO bb tt aa ii nn aa mm ii nn ii -- bb aa tt cc hh oo ff rq ˜˜ ss aa mm pp ll ee ss {{ yx ii }}
r
i˜q i˜ == 11 ff rr oo mm mm 11 βα βα ..
4.4 ConnectionswithWGAN for=1,...,udo
PWAN includes the well-known WGAN as a special case,
Updatew˜ byascendingthegradient∂L/∂w˜.
becausetheobjectivefunctionofWGAN,i.e.,W 1 isaspecial endfor
case of that of PWAN, i.e., L M,m and L D,h, and they both Computethegradient∂L/∂θaccordingto(16).
approximatetheKRformsusingneuralnetworks.Inother
Computethegradient∂C(θ)/∂θ.
words, when m α = m β = m for m-PWAN, or m α = m β Updateθbydescendingthegradient∂L/∂θ+∂C/∂θ.
andh>diam(Ω)ford-PWAN,theybecomeWGAN. endfor
However,despiteitscloserelationswithWGAN,PWAN
hasauniquepropertythatWGANdoesnothave:PWANcan
automaticallyomitafractionofdatainthetrainingprocess. Finally,wenotethatPWANhasanintuitiveadversarial
This property can be theoretically explained by observing explanationasananalogueofWGAN:Letαandβ θ bethe
thelearnedpotentialfunctionf∗.AsforPWAN,||∇f∗||=1 distribution of real and fake data respectively. During the
or ||∇f∗|| = 0 on the input data (Corollary 2 and 1 in trainingprocess,f w,h istryingtodiscriminateαandβ θ by
theappendix),i.e.,thedatapointswith0gradientswillbe assigning each data point a realness score in range[−h,0].
omittedduringtraining,becausetheydonotcontributeto Thepointswiththehighestscore0orthelowestscore−h
the update of θ in (16). While for WGAN, ||∇f∗|| = 1 on areregardedasthe“realest”or“fakest”pointsrespectively.
theinputdata(Corollary1in[43]),i.e.,nodatapointwillbe Meanwhile,T θ istryingtocheatf w,h bygraduallymoving
omitted. thefakedatapointstoobtainhigherscores.However,itdoes6
nottendtomovethe“fakest”points,astheirscorescannot
bechangedeasily.ThetrainingprocessendswhenT θ cannot
makefurtherimprovements.
(a) Experimentalsetting.
5 APPLICATIONS I: POINT SET REGISTRATION
This section applies Alg. 1 to point set registration tasks.
WepresentthedetailsofthealgorithmsinSec.5.1,andthe
numericalresultsinSec.5.2.
5.1 AligningPointSetUsingPWAN
Recall the definition for point set registration in Sec. 4.2:
(cid:80) (cid:80)
G {yi jv }e r jn =1β an= dXy =j∈ {Y xδ i}yj q i=a 1n ad reα the= sourx ci e∈X anδ dxi r, efw erh ener ce epY oin= t (b) L2 (c) KL
setsin3Dspace.Weseektofindanoptimaltransformation
T thatmatchesY toX bysolvingproblem(5).
First of all, we define the non-rigid transformation T θ
parametrizedbyθ =(A,t,V)as
T θ(y j)=y jA+t+V j, (17)
wherey
j
∈R1×3 representsthecoordinateofpointy
j
∈Y,
A∈R3×3isthelineartransformationmatrix,t∈R1×3isthe
translationvector,V ∈Rr×3 istheoffsetvectorofallpoints (d) LM,10 (e) LD,2
inY,andV
j
representsthej-throwofV. Fig.4.Comparisonofdifferentdiscrepanciesonatoypointset.
Then we define the coherence regularizer C(θ) similar
to[10],i.e.,weenforcetheoffsetvectorv j tovarysmoothly. where x N(j) is the closest point to T θ(y j), and s j is the
Formally,letG∈Rr×r beakernelmatrix,e.g.,theGaussian thresholdforpointT θ(y j)definedas
kernelG ρ(i,j)=e−||yi−yj||2/ρ,andσ ∈R +.Wedefine (cid:40)
1 if ||x −T (y )||≤h
C(θ)=λTr(VT(σI+G ρ)−1V), (18) s j = 0 else N(j) θ j (21)
whereλ∈R + isthestrengthofconstraint,I istheidentity forL D,h,and
matrix,andTr(·)isthetraceofamatrix.
(cid:40)
Since the matrix inversion (σI + G ρ)−1 is computa- s j = 1 if j ∈Topk(||{x N(i)−T θ(y i)||}r i=1,m) (22)
tionally expensive, we approximate it via the Nystro¨m 0 else
method[45],andobtainthegradient
forL M,m,whereTopk(·,m)presentsthesmallestmelements
∂C(θ) ≈2λσ−1(V−σ−1Q(Λ−1+σ−1QTQ)−1QTV), (19) in·.WeobtainT θ byoptimizing(20)untilconvergenceusing
∂V gradient descent, which generally only takes a few steps.
where G ρ ≈ QΛQT, Q ∈ Rr×k, Λ ∈ Rk×k is a diagonal Note that this approach is reasonable because when the
pointsetsaresufficientlywellaligned,theclosestpointpairs
matrix,andk ≪r.Notethecomputationalcostof(19)isonly
arelikelytobethecorrespondingpointpairs,thus(20)can
O(r)ifweregardkasaconstant.Thedetailedderivationis
presented in Appx. B.2. Note that ∂C(θ) = ∂C(θ) = 0 since beregardedasanapproximationof(6).
∂A ∂t
C(θ)isnotrelatedtoAandt.
5.2 ExperimentsonPointSetRegistration
Finally,sinceitcanbeshownthatT θ satisfiestheassump-
tion required by Theorem 1 and Theorem 2 (Proposition 9 WeexperimentallyevaluatetheproposedPWANonpointset
intheappendix),itisvalidtouseAlg.1.Wedonotadopt registrationtasks.Wefirstpresentatoyexampletohighlight
themini-batchsetting,i.e.,wesampleallpointsinthesets the robustness of the PW discrepancies in Sec. 5.2.1. Then
in each iteration: q˜ = q and r˜ = r, and set m α = q and wecomparePWANagainstthestate-of-the-artmethodson
m β = r. For rigid registration tasks, i.e., when T θ (17) is synthesizeddatainSec.5.2.2,anddiscussitsscalabilityin
requiredtobearigidtransformation,wesimplyfixV =0 Sec. 5.2.3. We further evaluate PWAN on real datasets in
(C(θ)=0),anddefineAtobearotationmatrixparametrized Sec.5.2.4.Finally,wepresenttheresultsofrigidregistration
byaquaternion. inSec.5.2.5.
Due to the approximation error of neural networks,
5.2.1 ComparisonofSeveralDiscrepancies
the optimization process often has difficulty converging
to the exact local minimum. To address this issue, when To provide an intuition of the PW divergence L M,m and
the objective function no longer decreases, we replace the L D,h used in this work, we compare them against two
objectivefunctionby representativerobustdiscrepancies,i.e.,KLdivergence[30]
r andL 2 distance[32],thatarecommonlyusedinregistration
min(cid:88) s j||x N(j)−T θ(y j)||+C(θ) (20) tasksonatoy1-dimensionalexample.Morediscussioncan
θ befoundinAppx.B.4.
j=17
We construct the toy point sets X and Y shown in
Fig.4(a),wherewefirstsample10equi-spaceddatapoints
in interval [0,3], then we define Y by translating the data
pointsbyt,anddefineX byaddingN outliersinanarrow
interval[7.8,8.2]tothedatapoints.ForN ={1,10,103},we
recordfourdiscrepancies:KL,L 2,L M,10 andL D,2 between
X and Y as a function of t, and present the results from
Fig.5.Thesynthesizeddatasetsusedinourexperiments.
Fig.4(b)toFig.4(e).
By construction, the optimal (perfect) alignment is
the standard deviations of the results of PWAN are much
achievedatt=0.However,duetotheexistenceofoutliers,
lower than that of TPS-RPM. Fig. 6(b) presents the results
there are two alignment modes, i.e., t = 0 and t = 6.5 in
of the second experiment. Two types of PWANs perform
all settings, where t = 6.5 represents the biased result. As
comparably, and they outperform the baseline methods
forKLandL 2,thelocalminimumt=0graduallyvanishes
by a large margin in terms of both median and standard
and they gradually bias toward t = 6.5 as N increases.
deviationswhentheoverlapratioislow,whileallmethods
In contrast, L M,10 and L D,2 do not suffer from this issue,
perform comparably when the data is fully overlapped.
becausethelocalminimumt=0remainsdeep,i.e.,itisthe
These results suggest the PWAN are more robust against
globalminimum,regardlessofthevalueofN.
outliersthanbaselinemethods,andcaneffectivelyhandle
TheresultsuggeststhatcomparedtoKLandL 2,L
M,m
partialoverlaps.MoreresultsareprovidedinAppx.B.5.
andL D,h exhibitstrongerrobustnessagainstoutliers,asthe
Toprovideanintuitionofthelearnedpotentialnetwork
optimalalignmentcanalwaysbeachievedbyanoptimiza-
tionprocess,suchasgradientdescent,regardlessoftheratio
of PWAN, we visualize the gradient norm ||∇f w,h|| on a
pairofpartiallyoverlappedarmadillopointsetsattheend
ofoutliers.
oftheregistrationprocessinFig.7.Ascanbeseen,||∇f w,h||
5.2.2 EvaluationonSynthesizedData iscloseto0innon-overlappingregions,suchasthenoseand
theleftarm,suggestingthatthepointsintheseregionsare
This section evaluates the performance of PWAN on syn-
successfullyomittedbyPWAN.
thesized data. We use three synthesized datasets shown
in Fig. 5: bunny, armadillo and monkey. The bunny and
5.2.3 EvaluationoftheEfficiency
armadillodatasetsarefromtheStanfordRepository[46],and
themonkeydatasetisfromtheinternet.Theseshapesconsist ToevaluatetheefficiencyofPWAN,wefirstneedtoinvesti-
of8,171,106,289and7,958pointsrespectively.Wecreate gatetheinfluenceoftheparameteru,i.e.,thenetworkupdate
deformed sets following [47], and evaluate all registration frequency,whichisdesignedtocontrolthetradeoffbetween
resultsviameansquareerror(MSE). efficiencyandeffectiveness.Tothisend,weregisterapairof
WecomparetheperformanceofPWANwithfourstate- bunnydatasetsconsistingof2000pointswithvaryinguand
of-the-art methods: CPD [10], GMM-REG [32], BCPD [30] reporttheresultsontheleftpanelofFig.8.Ascanbeseen,
andTPS-RPM[27].Weevaluateallmethodsinthefollowing the median and standard deviation of MSE decrease as u
twosettings: increases,andthecomputationtimeincreasesproportionally
- Extra noise points: We first sample 500 random points withu,whichverifiestheeffectivenessofu.
from the original and the deformed sets as the source We then benchmark the computation time of different
and reference sets respectively. Then we add uniformly methodsonacomputerwithtwoNvidiaGTXTITANGPUs
distributednoisetothereferenceset,andnormalizeboth andanInteli7CPU.Wefixu = 20forPWAN.Wesample
sets to mean 0 and variance 1. We vary the number q =rpointsfromthebunnyshape,whereq variesfrom103
of outliers from 100 to 600 at an interval of 100, i.e., to7×105.PWANisrunontheGPUwhiletheothermethods
the outlier/non-outlier ratio varies from 0.2 to 1.2 at an arerunontheCPU.Wealsoimplementamulti-GPUversion
intervalof0.2. ofPWANwherethepotentialnetworkisupdatedinparallel.
- Partialoverlap:Wefirstsample1000randompointsfrom We run each method 10 times and report the mean of the
the original and the deformed sets as the source and computationtimeontherightpanelofFig.8.Ascanbeseen,
referencesetsrespectively.Wethenintersecteachsetwith BCPDisthefastestmethodwhenq issmall,andPWANis
arandomplane,andretainthepointsononesideofthe comparablewithBCPDwhenq isnear106.Inaddition,the
plane and discard all points on the other side. We vary 2-GPUversionPWANisfasterthanthesingleGPUversion,
theretainingratiosfrom0.7to1.0atanintervalof0.05 anditisfasterthanBCPDwhenq islargerthan5×105.
forboththesourceandthereferencesets,i.e.,theminimal
overlapratio(2s−1)/svariesfrom0.57to1. 5.2.4 EvaluationonRealData
We evaluate m-PWAN with m = 500 in the first setting, TodemonstratethecapabilityofPWANinhandlingdatasets
whileweevaluatem-PWANwithm=(2s−1)∗1000and withnon-artificialdeformations,weevaluateitonthespace-
d-PWANwithh=0.05inthesecondsetting.Moredetailed timefacesdataset[48]andhumanshapedataset[49].
experimentsettingsarepresentedinAppx.B.3.Werunall Thehumanfacedataset[48]consistsofatimeseriesof
methods 100 times and report the median and standard pointsetssampledfromarealhumanface.Eachfaceconsists
deviationofMSEinFig.6. of23,728points.Weusethefacesattimeiandi+20asthe
Fig.6(a)presentstheresultsofthefirstexperiment.The source and the reference set, where i = 1,...,20. All point
median error of PWAN is generally comparable with TPS- setsinthisdatasetarecompletelyoverlapped.Anexampleof
RPM,andismuchlowerthantheothermethods.Inaddition, ourregistrationresultispresentedinFig.9.Theregistration8
(a) Evaluationofrobustnessagainstnoisepoints.
(b) Evaluationofrobustnessagainstpartialoverlap.
Fig.6.Registrationaccuracyofthebunny(left),monkey(middle)andarmadillo(right)datasets.Theerrorbarsrepresentthemediansandstandard
deviationsofMSE.
Fig.9.Anexampleofourregistrationresultsonthehumanfacedataset.
(a) Initialstep (b) Finalstep (c) ||∇f w,h|| Thealignedpointsets(right)isobtainedbymatchingthe1-stframe(left)
tothe21-stframe(middle).
Fig.7.Visualizationofthelearnedpotentialnetworkonapairofpartially
overlapped armadillo point sets. ||∇f w,h|| is lower (darker) in non- handlearticulateddeformationsas[50],itcanstillproduce
overlapping regions, suggesting that the points in those regions are
good full-body registration (1-st row) and natural partial
successfullyomitted.
alignment(2-ndrow).Inaddition,itisworthnoticingthatthe
partialalignmentproducesanovelshapethatdoesnotexist
intheoriginaldataset,whichsuggeststhepotentialofPWAN
inshapeediting.Nevertheless,weobservesomedegreeof
misalignmentnearcomplicatedarticulatedstructures,such
asfingers.Thisissuemightbeaddressedbyincorporating
mesh-levelconstraints[51]inthefuture.
5.2.5 ResultsofRigidRegistration
Fig. 8. Scalability of PWAN. Left: u controls the tradeoff between PWAN can be naturally applied to rigid registration as a
efficiencyandeffectiveness.Right:ThespeedofPWANiscomparable specialcaseofnon-rigidregistration.WeevaluatePWANon
withthatofBCPDwhenqisnear106.
rigidregistrationusingtwodatasets:ASL[52]andUWA[53].
We construct three registration tasks: outdoor, indoor and
results are quantitatively compared with CPD and BCPD object,wheretheoutdoortaskconsistsoftwoscenesfrom
inTab.6intheappendix,wherePWANoutperformsboth ASL:mountainandwood-summer,theindoortaskconsists
baselinemethods. oftwoscenesfromASL:stairandapartment,andtheobject
We further evaluate PWAN on the challenging human taskconsistsoftwoscenesfromUWA:parasaurolophusand
body dataset [49]. We apply PWAN to both complete and T-rex.Foreachofthe6scenes,weregisterthei-thviewto
incompletehumanshapes.Anexampleofourregistration the(i+1)-thview,wherei=1,...,10.Wereporttherotation
resultsisshowninFig.10,andmoredetailscanbefoundin error
180arccos(1(Tr(RR˜T)−1)),whereRandR˜
arethe
π 2
Appx.B.6.Ascanbeseen,althoughPWANisnotdesignedto estimatedandtruerotationmatricesrespectively.SinceUWA9
sets. For example, R I is the reference image distribution,
and L R consists of all possible labels of R. Given the
labelled reference dataset D R = {(x i,l i)}q i=1 ∼ (R)q and
theunlabeledsourcedatasetD
S
={(y i)}r
i=1
∼(S I)r,where
L S isassumedtobeanunknownsubsetofL R,thegoalof
partialdomainadaptationistoobtainanaccurateclassifierη
forD S.Notethatfollowing[11],[40],weassumeη =D ν◦T θ,
whereT θ isafeatureextractorwithparameterθ,D ν isalabel
predictorwithparameterν.
Thekeystepinsolvingadaptationproblemsistomatch
S I tothefractionofR I belongingtoclassL S inthefeature
space,sothatthepredictortrainedonD R canalsobeused
onD S.WeformulatethisstepasaPDMproblem,wherethe
fractionofdatainR I belongingtoclassL R\L S treatedare
outliers. Formally, let β θ = (T θ) #S I and α = (T θ(cid:98)) #R I be
the feature distributions of S I and R I respectively, where
Fig.10. Examplesofourregistrationresultsonthehumanbodydataset. θ(cid:98)denotes a frozen copy of θ which does not allow back-
We present the result of registering complete shapes (1-st row) and
incompleteshapes(2-ndrow),wherethesourcepointset(left)isaligned propagation. We seek to match β θ to α by minimizing
tothereferenceset(middle). L M,1(α,β θ),whereweassumem α ≥m β =1.Bycombining
the PDM formulation with the classifier η, our complete
dataset does not provide ground true poses, we run the formulationcanbeexpressedas
global search algorithm Go-ICP [54] on UWA dataset and (cid:88)
min µ iCE(η(x i),l i)+λ 1L M,1(α,β θ)+λ 2C(η), (23)
useitsresultasthegroundtruth. θ,ν
Wepre-processallpointsetsbynormalizingthemtobe
(xi,li)∈R
inside the cubic [−1,1]3, and down-sampling them using where C(η) is the entropy regularizer [57], CE is the cross-
a voxel grid filter with grid size 0.08. We compare d- entropy loss, µ i = |D1 S|(cid:80) y∈DS1(η(y) = l i) is the re-
PWANandm-PWANagainstBCPD[30],CPD[10],ICPwith weighting coefficient of class l i [58], where the indicator
trimming[55](m-ICP)andICPwithdistancethreshold[25],
function1(·)equals1if(·)istrue,and0otherwise.
[56](d-ICP).Weseth=0.05ford-PWAN,m=0.8min(q,r) To apply PWAN to (23), we first verify that the feed-
form-PWAN,andfixu=25. forward network T θ is a valid transformation for PWAN,
Aquantitativecomparisonoftheregistrationresultispre- i.e., it satisfies the assumption required by Theorem 2
sentedinTab.1,andmoredetailsarepresentedinAppx.B.7. (Proposition10intheappendix).Thenweneedtodetermine
As can be seen, m-PWAN and d-PWAN outperform all theparameterm α,whichspecifiesthealignmentratio1/m α
baseline methods in all but the object class, where BCPD ofthereferencedata.Iftheoutlierratior ofαisknownas
performsslightlybetter,andm-PWANachievesbetterresults a prior, we can simply set m α ≈ 1/(1−r) to ensure that
thand-PWANintermsofstandarddeviation. the correct ratio of data is aligned. However, r is usually
unknown in practice, so we first heuristically set m α as
Quantitativeres su talt ns do af rr dig did evr ie aT gA tii osB ntL raE ot fio1 rn o. taW tioe nre ep rro or rt st .hemedianand m
t
e˜
h
nα
e coe
u=
s rti am
g|L
ea
sR
te
a| d/ li| gL L˜ nS
S
in| ., gTw
h
L˜h
i
Sse cr ce
lh ao
sL
i
s˜
c
eS
e so
o=
f
fdm{ al
α
ta∈
is
beiL
tn
wtR
u
e|
i
ew
t
nil
v
De> Rbe| acL na1 R
u
d| s} Dei Sis
t
.
indoor outdoor object Then,tofurtheravoidbiasingtowardoutliers,wegradually
BCPD 0.35(10.37) 8.08(8.54) 0.10(0.26) increasem α duringtraining.Formally,weset
CPD 0.79(28.46) 5.89(3.95) 0.17(61.99)
m-ICP 0.54(22.46) 13.36(12.37) 1.96(6.43) m α =m˜ α×sv (24)
d-ICP 6.62(18.85) 14.86(8.20) 21.17(6.65)
m-PWAN(Ours) 0.26(14.22) 2.00(3.62) 0.10(0.35) at training step v, where s > 1 is the annealing factor. An
d-PWAN(Ours) 0.26(22.44) 2.01(3.63) 0.11(5.60) explanationoftheeffectofm α ispresentedinAppx.B.8.
NowwecanuseAlg.1.Weadoptthemini-batchsetting
andsetthebatchsizeq˜= r˜.Forclearness,weprovidethe
explicitformulationofourtraininggoal(23):
6 APPLICATION II: PARTIAL DOMAIN ADAPTATION
q˜ q˜
ATh ftis erse pc rt eio sn ena tp inp glie ts hA el dg e. t1 at io lsp oa frt ti ha eld mom eta hi on dad inap St ea cti .o 6n .1t ,as wk es. m θ,i νnm wax(cid:88) µ iCE(η(x i),l i)+λ 1(cid:16)m q˜α (cid:88) f w(T θˆ(x i))
i=1 i=1
showtheresultsofournumericalexperimentsinSec.6.2. q˜
1(cid:88) (cid:17)
−
q˜
f w(T θ(y i))−GP(f w) +λ 2C(η), (25)
6.1 PartialDomainAdaptationviaPWAN i=1
Thepartialdomainadaptationproblemisdefinedasfollows. where{(x i,l i)}q i˜
=1
isarandombatchofD R,{(y i)}q i˜
=1
isa
Consider an image classification problem defined on the randombatchofD S,m α issetaccordingto(24),andf w ≤
image space I and the label space L. Let R and S be 0 is the potential network with parameter w. The overall
the reference and source distributions over I ×L, (·)| I be network architecture is presented in Fig. 11. By abusing
their marginal distributions on I, and L (·) be their label notation,wealsocall(25)PWAN.10
Fig.11.TheproposedPWANforpartialdomainadaptation.
(a) λ1=λ2=0 (b) λ1=1,mα=1;λ2=0
6.2 ExperimentsonPartialDomainAdaptation
WeexperimentallyevaluatePWANonpartialdomainadap-
tation tasks. After describing the experimental settings in
Sec. 6.2.1, we first present an ablation study in Sec. 6.2.2
to explain the effectiveness of our formulation. Then we
comparetheperformanceofPWANwiththestate-of-the-art
methodsinSec.6.2.3.Wefinallydiscussparameterselection
inSec.6.2.4.
6.2.1 ExperimentalSettings
WeconstructPWANasfollows:thepotentialnetf w isafully (c) λ1=1,mα=10;λ2=0 (d) λ1=1,mα=10;λ2=0.1
connected network with 2 hidden layers (feature → 256 →
256→1);Following[40],thefeatureextractorT θ isdefined Fig.12.t-SNEvisualizationofthelearnedsourceandreferencefeatures,
asaResNet-50[59]networkpre-trainedonImageNet[60], whereblueandgreypointsrepresentnon-outlierandoutlierreference
wan id tht 1he bol ta tb lee nl ep cr ked lai yct eo rr (fD eaν tui rs
e
→a fu 25ll 6y →con lan be elc )t .e Wd en uet pw do ar tk
e
f P oe fWa tt hAu er Ne res ( fλ er 1 re es ̸= np ce e0c ,t fi emv ae α tl uy, r> ea sn 1 wd ) hcr ile a ed n avap olo i igi dn n ints t ghr e be iap s sr oe iu ns r ge cn e tot wfet ah a re t durs oeo usu tlr it ec o re sa .fe fra at cu tr ie os n.
{θ,ν} using Adam optimizer [61] and update {w} using
RMSpropoptimizer[62],andmanuallysetthelearningrate
example, AC-OfficeHome represents the task of adapting
to1e−4(1+10i)−0.75 following[40],whereiisthetraining
domainAtodomainConOfficeHome.Weomitthedataset
step. We fix λ 1 = 0.05 and λ 2 = 0.1 for all experiments. nameifitisclearfromthetext.
MoredetailedsettingsarepresentedinAppx.B.9.
We consider the following four datasets in our experi-
6.2.2 AblationStudy
ments:
Toexplaintheeffectivenessofformulation(25),weconduct
- OfficeHome [63]: A dataset consisting of four domains:
anablationexperimentonAR-OfficeHome.WetrainPWAN
Artistic (A), Clipart (C), Product (P) and real-world (R).
Following[11],weconsiderall12adaptationtasksbetween with different values of {λ 1,m α,λ 2}, and visualize the
these 4 domains. We keep all 65 classes of the reference features extracted by T θ in Fig. 12 using t-SNE [68]. For
domainandkeepthefirst25classes(inalphabeticorder)
clearness,wefixµ
i
=1ands=1inthisexperiment.
ofthesourcedomain. Whenaclassifieristrainedonthereferencedatawithout
- VisDa17 [64]: A large-scale dataset for synthesis-to-real
theaidofPWAN(λ
1
=λ
2
=0),thereferenceandthesource
adaptation,wherethereferencedomainconsistsofabun- featuredistributionsarewell-separated,whichsuggeststhat
dantrenderedimages,andthesourcedomainconsistsof the trained classifier cannot be used for the source data.
realimagesbelongingtothesameclasses.Wekeepall12 PWAN(λ 1 >0)canalleviatethisissuebyaligningthesetwo
classesofthereferencedomainandkeepthefirst6classes distributions.However,completealignment(m α =1)leads
(inalphabeticorder)ofthesourcedomain. to degraded performance due to serious negative transfer,
- ImageNet-Caltech: A challenging large-scale dataset for
i.e.,mostofthesourcefeaturesarebiasedtowardtheoutlier
general-to-specify adaptation. The reference domain is features.Incontrast,byaligningalowerratioofthereference
ImageNet [60] (1000 classes), and the source domain features,PWANwithlargerm α (m α =10)canlargelyavoid
consists of data from Caltech [65] belonging to the 84 the negative transfer effect, i.e., most of the outliers are
classessharedbyCaltechandImageNet.
omitted.Inaddition,theentropyregularizer(λ
2
>0)further
- DomainNet [66]: A recently proposed challenging large- improves the alignment as it pushes the source features
scale dataset. Following [67], we adopt four domains: to their nearest class centers. The results suggest that our
Clipart (C), Painting (P), Real (R) and Sketch (S). We formulation(25)canindeedalignthefeaturedistributions
considerall12adaptationtasksonthisdataset.Wekeep whileavoidingnegativetransfer,whichmakesthetrained
all126classesofthereferencedomainandkeepthefirst classifiersuitableforthesourcedata.
40classes(inalphabeticorder)ofthesourcedomain.
6.2.3 ResultsofPartialDomainAdaptation
For clearness, for OfficeHome and DomainNet, we name
each task RS-D, where D represents the dataset, R and S We compare the performance of PWAN with the state-of-
representthereferenceandsourcedomainrespectively.For the-art algorithms: DPDAN [69], PADA [12], BAUS [58],11
TABLE2
Results(accuracy%)ofpartialdomainadaptationonOfficeHome.WeadditionallyreportPWANwithextraclassificationtechniques:labelsmoothing
(PWAN+L),complementobjectiveregularizer(PWAN+C),andthecombinationofthesetwotechniques(PWAN+A).
AC AP AR CA CP CR PA PC PR RA RC RP Avg
RefOnly 42 67 79.2 56.8 55.9 65.4 59.3 35.5 75.5 68.7 43.4 74.5 60.2
DANN 46.6 45.8 57.5 37.3 32.6 40.5 40.2 39.4 55.4 54.5 44.8 57.6 46.0
PADA 48.9 66.9 81.6 59.1 55.3 65.7 65 41.6 81.1 76 47.6 82 64.2
SAN++ 61.25 81.57 88.57 72.82 76.41 81.94 74.47 57.73 87.24 79.71 63.76 86.05 75.96
BAUS 60.6 83.1 88.3 71.7 72.7 83.4 75.4 61.5 86.5 79.2 62.8 86.0 75.9
DPDAN 59.4 — 79.0 — — — — — 81.7 76.7 58.6 82.1 —
mPOT 64.6 80.6 87.1 76.4 77.6 83.5 77.0 63.7 87.6 81.4 68.5 87.3 77.9
SHOT 62.9(1.9) 87.0(0.5) 92.5(0.1) 75.8(0.3) 77.0(1.2) 86.2(0.6) 77.7(0.8) 62.8(0.6) 90.4(0.5) 81.8(0.1) 65.5(0.4) 86.2(0.1) 78.8(0.1)
ADV 62.3(0.4) 82.3(2.9) 91.5(0.5) 77.3(0.9) 76.6(2.4) 84.9(2.6) 79.8(0.8) 63.4(0.7) 90.1(0.5) 81.6(1.1) 65.0(0.6) 86.6(0.5) 78.5(0.4)
PWAN(Ours) 63.5(3.3) 83.2(2.7) 89.3(0.2) 75.8(1.5) 75.5(2.5) 83.3(0.1) 77.0(2.4) 61.1(1.5) 86.9(0.7) 79.9(0.6) 65.0(3.8) 86.2(0.2) 77.2(1.6)
PWAN+L(Ours) 63.9(0.1) 84.5(3.3) 90.3(0.3) 75.4(0.9) 75.4(2.5) 85.2(1.2) 78.0(1.6) 63.3(1.4) 87.5(0.8) 79.7(0.7) 66.3(2.7) 86.5(0.8) 78.0(1.4)
PWAN+C(Ours) 63.7(1.5) 83.6(2.6) 89.2(0.4) 76.3(2.0) 77.1(3.0) 83.6(0.1) 77.6(1.9) 62.2(0.5) 86.0(0.4) 79.6(0.7) 66.2(2.4) 86.7(0.3) 77.6(1.3)
PWAN+A(Ours) 65.2(0.6) 84.5(3.0) 89.9(0.2) 76.7(0.6) 76.8(1.9) 84.3(1.8) 78.7(3.2) 64.2(0.9) 87.3(0.9) 79.9(1.1) 68.0(2.3) 86.9(0.9) 78.5(1.0)
TABLE3
Results(accuracy%)ofpartialdomainadaptationonImageNet-Caltech
andVisDa17
ImageNet-Clatech VisDa17
SourceOnly 70.6 60.0
DANN 71.4 57.1
PADA 75 66.8
SAN++ 83.34 63.0
BAUS 84 69.8
DPDAN — 65.2
SHOT 83.8(0.2) —
(a) Learnedfeatures (b) ∥∇fw∥ ADV 85.4(0.2) 80.1(7.9)
PWAN(Ours) 86.0(0.5) 84.8(6.1)
To show the effectiveness of PWAN, we present more
training details of PWAN on AC-OfficeHome in Fig. 13.
As shown in Fig. 13(a) and Fig. 13(b), the learned source
featuresarematchedtoafractionofthereferencefeatures,
(c) Classificationresult (d) Trainingprocess
whiletheotherreferencefeaturesaresuccessfullyomitted,
i.e.,thegradientnormislowonthesefeatures.Classification
Fig.13.MoredetailedresultsofPWANonAC-OfficeHome.(a)Visu-
alization of the learned features. The red and blue points represent resultsinFig.13(c)showthatmostofthesourcedata(about
thesourceandreferencefeaturesrespectively.(b)PWANautomatically 92%)arecorrectlyclassifiedintonon-outlierclasses,which
omitsafractionofreferencefeatures(thedarkpoints).(c)Mostofthe
suggeststhatthenegativetransfereffectislargelyavoided.
sourcedata(92%)iscorrectlyclassifiedintonon-outlierclasses,i.e.,the
negativetransfereffectislargelyavoided.(d)Duringthetrainingprocess, In addition, the training process is shown in Fig. 13(d),
thetestaccuracyincreasesastheratioofbiasedsourcedatadecreases. whereweobservethatclassificationaccuracy(onthesource
domain) gradually increases as the biased ratio decreases
during the training process. The results show that our
SHOT[70],ADV[38],SAN++[11]andmPOT[23].Wealso
trainingprocesscaneffectivelynarrowthedomaingapwhile
reporttheperformanceofDANN[40],whichcanbeseenas
avoidingthenegativetransfereffect.
aspecialcaseofPWANforclosed-setdomainadaptation.We
evaluatetheperformancesofallalgorithmsbytheaccuracy The results on large-scale datasets VisDa17, ImageNet-
onthesourcedomain.WerunthecodeofPWAN,ADVand Caltech and DomainNet are reported in Tab. 3 and Tab. 4.
SHOTwith3differentrandomseedsandreportthemean OnVisDa17,PWANoutperformsallotherbaselinemethods
andstandarddeviation.Theresultsofotheralgorithmsare by a large margin. As shown in Fig. 26 in the appendix,
adoptedfromtheiroriginalpapers. PWAN can discriminate all data almost perfectly except
TheresultsonOfficeHomearereportedinTab.2.Fora forthe“knife”and“skateboard”classeswhicharevisually
faircomparison,weadditionallyreporttheresultsofPWAN similar.OnImageNet-Caltech,PWANalsooutperformsall
withtwoadvancedtechniquesdevelopedfordiscriminative other baseline methods, which suggests that PWAN can
neural networks: complement objective regularizer [71] successfully handle datasets dominated by outliers (more
(PWAN+C) and label smoothing [72] (PWAN+L), and the than90%ofclassesinImageNetareoutlierclasses)without
combinationofthesetwotechniques(PWAN+A),astheyare complicated sampling techniques [38] or self-training pro-
alsousedinADV,SHOTandBAUS.WeobservethatPWAN cedures[11].OnDomainNet,PWANperformscomparably
withthesetechniques(PWAN+A)performscomparablywith withADV,andoutperformsallothermethods.Moreanalysis
ADVandSHOT,andtheyoutperformallothermethods. canbefoundinAppx.B.10.12
TABLE4
Results(accuracy%)ofpartialdomainadaptationonDomainNet
CP CR CS PC PR PS RC RP RS SC SP SR Avg
RefOnly 41.2 60.0 42.1 54.5 70.8 48.3 63.1 58.6 50.2 45.4 39.3 49.7 51.9
DANN 27.8 36.6 29.9 31.7 41.9 36.5 47.6 46.8 40.8 25.8 29.5 32.7 35.6
PADA 22.4 32.8 29.9 25.7 56.4 30.4 65.2 63.3 54.1 17.4 23.8 26.9 37.4
BAUS 42.8 54.7 53.7 64.0 76.3 64.6 79.9 74.3 74.0 50.3 42.6 49.6 60.6
ADV 54.1(3.7) 72.0(0.4) 55.4(1.5) 68.8(0.5) 78.9(0.2) 75.4(0.6) 77.4(0.6) 72.3(0.5) 70.4(1.0) 58.4(0.6) 53.4(1.6) 65.3(0.6) 66.8(0.2)
PWAN(Ours) 54.4(0.9) 74.1(1.0) 58.7(2.5) 65.3(0.8) 81.4(0.5) 73.0(0.6) 78.0(0.5) 73.4(0.8) 70.8(0.7) 51.6(2.5) 55.1(0.6) 66.2(3.3) 66.8(0.5)
Fig.14.Effectofparametermαandu.Left:TheperformanceandtrainingtimeofPWANasfunctionsofu.Middle:TheperformanceofPWANon
ImageNet-Caltechwithdifferentvaluesofm˜α.Right:ComparisonofperformanceonAC-OfficeHomewithdifferentratiosofoutlierclass.
6.2.4 EffectofParameters m˜ α onAC-OfficeHomewithvaryingoutlierratio,wherewe
keepthefirstC S classes(inalphabeticorder)inthesource
In this section, we investigate the effects of the two major
hyper-parameters involved in PWAN, i.e., m α and u, so d clo am ssa vin ara ien sd frv oa mry 9C 2%S f tr oom 0%5 .t To h6 e0 r, ei s. ue. l, tsth ae rera st hio owof no iu ntl ti he er
thattheycanbeintuitivelychoseninpracticalapplications
right panel of Fig. 14, where we observe that the selected
withouthyper-parametersearching[73].
m˜ α canindeedhandledifferentoutlierratios,anditenables
Parameteruisdesignedtocontrolthetradeoffbetween
PWANtooutperformthebaselinemethods.
theeffectivenessandefficiencyofPWAN.Toseethepractical
effectsofu,werunPWANonAC-OfficeHomewithvarying
u, and report the test accuracy and the corresponding 7 CONCLUSION
training time in the left panel in Fig. 14. We observe that
In this work, we propose PWAN for PDM tasks. To this
increasing u leads to more accurate results and longer
end, we first derive the KR form and the gradient for the
training time, and the accuracy saturates when u is larger
PWdivergence.Basedontheseresults,weapproximatethe
than10.Moreover,asuincreasesfrom1to20,thetraining
PW divergence by a neural network and optimize it via
time only increases by 10%. This result suggests that the
gradientdescent.WeapplyPWANtopointsetsregistration
costoftrainingthepotentialnetworkissmallcomparedto
andpartialdomainadaptationtasks,andshowthatPWAN
trainingtheclassifier,thusrelativelylargeucanbechosen
achieves favorable results on both tasks due to its strong
to achieve better performance without causing a heavy
robustnessagainstoutliers.
computationalburden.
There are several issues that need further study. First,
Parameterm α (24)isdesignedtocontrolthealignment whenappliedtopointsetregistration,thecomputationtime
ratio,i.e.,largerm α indicatesthatalowerratioofreference of PWAN is still relatively high. A promising approach to
featurewillbealigned.Toinvestigatetheeffectsofm α,we accelerate PWAN is to use a forward generator network
runPWANonImageNet-Caltechwithvaryingm˜ α.Weup- as in [74], or to take an optimize-by-learning strategy [75].
datePWANforT =48000steps,andwefixs=exp(10/T),
Second,itisinterestingtoexplorePWANinotherpractical
i.e., m α = 10m˜ α at step T. The results are reported in the tasks, such as outlier detection [76], and other types of
middlepanelofFig.14,whereweobservethatoverlylarge domainadaptationtasks[35],[77],[78].
or small m˜ α leads to poor performance, but the variation
of performance is not large (about 2%) when m˜ α is in the
rangeof[100,102].Thisresultsuggeststhattheperformance ACKNOWLEDGMENTS
ofPWANisstableifareasonablevalueofm˜ α isselected. This work was funded by the National Natural Science
To further reduce the difficulty of selecting m˜ α, we Foundation of China (NSFC) grants under contracts No.
˜
suggestedchoosingm˜ α = |L R|/|L S|inSec.6.1.Toseethe 62325111 and U22B2011, and Knut and Alice Wallenberg
effectiveness of this selection, we compute the suggested Foundation. The computation was partially enabled by
m˜ α forImageNet-Caltechandmarkitinthemiddlepanel resourcesprovidedbytheNationalAcademicInfrastructure
ofFig.14,whereweseethatalthoughthesuggestedvalue for Supercomputing in Sweden (NAISS) at C3SE partially
isnotoptimal,itstillleadstoreasonableperformance.We funded by the Swedish Research Council through grant
furtherreporttheperformanceofPWANusingthesuggested agreementNo.2022-06725.13
REFERENCES [26] Z.Zhang,“Iterativepointmatchingforregistrationoffree-form
curvesandsurfaces,”InternationalJournalofComputerVision,vol.13,
[1] M.Arjovsky,S.Chintala,andL.Bottou,“Wassersteingenerative no.2,pp.119–152,1994.
adversarialnetworks,”inProceedingsofthe34thInternationalCon-
[27] H.ChuiandA.Rangarajan,“Anewalgorithmfornon-rigidpoint
ferenceonMachineLearning,ser.ProceedingsofMachineLearning
matching,”inProceedingsIEEEConferenceonComputerVisionand
Research,D.PrecupandY.W.Teh,Eds.,vol.70. PMLR,06–11 PatternRecognition.CVPR2000(Cat.No.PR00662),vol.2,2000,pp.
Aug2017,pp.214–223.
2044–2051.
[2] Y. Song and S. Ermon, “Generative modeling by estimating
[28] B.Maiseli,Y.Gu,andH.Gao,“Recentdevelopmentsandtrendsin
gradientsofthedatadistribution,”Advancesinneuralinformation
pointsetregistrationmethods,”JournalofVisualCommunicationand
processingsystems,vol.32,2019.
ImageRepresentation,vol.46,no.46,pp.95–106,2017.
[3] J.Ho,A.Jain,andP.Abbeel,“Denoisingdiffusionprobabilistic
[29] J. Vongkulbhisal, B. I. Ugalde, F. D. la Torre, and J. P. Costeira,
models,”Advancesinneuralinformationprocessingsystems,vol.33,
“Inversecompositiondiscriminativeoptimizationforpointcloud
pp.6840–6851,2020.
registration,”in2018IEEE/CVFConferenceonComputerVisionand
[4] L.Chizat,G.Peyre´,B.Schmitzer,andF.-X.Vialard,“Scalingalgo-
PatternRecognition,2018,pp.2993–3001.
rithmsforunbalancedoptimaltransportproblems,”Mathematicsof
[30] O.Hirose,“Abayesianformulationofcoherentpointdrift,”IEEE
Computation,vol.87,no.314,pp.2563–2609,2018.
Transactions on Pattern Analysis and Machine Intelligence, vol. 43,
[5] L.Chapel,M.Z.Alaya,andG.Gasso,“Partialoptimaltranportwith
no.7,pp.2269–2286,2021.
applicationsonpositive-unlabeledlearning,”AdvancesinNeural
InformationProcessingSystems,vol.33,pp.2903–2913,2020. [31] J.Ma,J.Zhao,J.Tian,Z.Tu,andA.L.Yuille,“Robustestimation
ofnonrigidtransformationforpointsetregistration,”in2013IEEE
[6] D.Mukherjee,A.Guha,J.M.Solomon,Y.Sun,andM.Yurochkin,
Conference on Computer Vision and Pattern Recognition, 2013, pp.
“Outlier-robustoptimaltransport,”inInternationalConferenceon
2147–2154.
MachineLearning. PMLR,2021,pp.7850–7860.
[7] C.Villani,Optimaltransport:oldandnew. Springer,2009,vol.338. [32] B. Jian and B. C. Vemuri, “Robust point set registration using
[8] A. Figalli, “The optimal partial transport problem,” Archive for gaussianmixturemodels,”IEEETransactionsonPatternAnalysisand
RationalMechanicsandAnalysis,vol.195,no.2,pp.533–560,2010. MachineIntelligence,vol.33,no.8,pp.1633–1645,2011.
[9] C.Villani,Topicsinoptimaltransportation. AmericanMathematical [33] Y.TsinandT.Kanade,“Acorrelation-basedapproachtorobust
Soc.,2003,no.58. pointsetregistration,”inEuropeanConferenceonComputerVision,
[10] A.MyronenkoandX.Song,“Pointsetregistration:Coherentpoint 2004,pp.558–569.
drift,”IEEETransactionsonPatternAnalysisandMachineIntelligence, [34] Q.-Y.Zhou,J.Park,andV.Koltun,“Fastglobalregistration,”in
vol.32,no.12,pp.2262–2275,2010. EuropeanConferenceonComputerVision,2016,pp.766–782.
[11] Z. Cao, K. You, Z. Zhang, J. Wang, and M. Long, “From big to [35] F.KuhnkeandJ.Ostermann,“Deepheadposeestimationusing
small:adaptivelearningtopartial-setdomains,”IEEETransactions synthetic images and partial adversarial domain adaption for
onPatternAnalysisandMachineIntelligence,vol.45,no.2,pp.1766– continuouslabelspaces,”in2019IEEE/CVFInternationalConference
1780,2022. onComputerVision(ICCV),2019,pp.10164–10173.
[12] Z.Cao,L.Ma,M.Long,andJ.Wang,“Partialadversarialdomain [36] S.Li,C.H.Liu,Q.Lin,Q.Wen,L.Su,G.Huang,andZ.Ding,
adaptation,”inProceedingsoftheEuropeanconferenceoncomputer “Deepresidualcorrectionnetworkforpartialdomainadaptation,”
vision(ECCV),2018,pp.135–150. IEEEtransactionsonpatternanalysisandmachineintelligence,vol.43,
[13] Z.-M. Wang, N. Xue, L. Lei, and G.-S. Xia, “Partial wasserstein no.7,pp.2329–2344,2020.
adversarialnetworkfornon-rigidpointsetregistration,”inInterna- [37] J.Zhang,Z.Ding,W.Li,andP.Ogunbona,“Importanceweighted
tionalConferenceonLearningRepresentations(ICLR),2022. adversarialnetsforpartialdomainadaptation,”inProceedingsof
[14] L.A.CaffarelliandR.J.McCann,“Freeboundariesinoptimaltrans- theIEEEconferenceoncomputervisionandpatternrecognition,2018,
portandmonge-ampereobstacleproblems,”AnnalsofMathematics, pp.8156–8164.
vol.171,no.2,pp.673–730,2010. [38] X.Gu,X.Yu,J.Sun,Z.Xuetal.,“Adversarialreweightingforpartial
[15] A.Makkuva,A.Taghvaei,S.Oh,andJ.Lee,“Optimaltransport domain adaptation,” Advances in Neural Information Processing
mapping via input convex neural networks,” in International Systems,vol.34,pp.14860–14872,2021.
ConferenceonMachineLearning. PMLR,2020,pp.6672–6681. [39] J.Shen,Y.Qu,W.Zhang,andY.Yu,“Wassersteindistanceguided
[16] B.Schmitzer,“Stabilizedsparsescalingalgorithmsforentropyreg- representationlearningfordomainadaptation,”inProceedingsof
ularizedtransportproblems,”SIAMJournalonScientificComputing, theAAAIConferenceonArtificialIntelligence,vol.32,no.1,2018.
vol.41,no.3,pp.A1443–A1481,2019. [40] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle,
[17] N.Bonneel,J.Rabin,G.Peyre´,andH.Pfister,“Slicedandradon F.Laviolette,M.Marchand,andV.Lempitsky,“Domain-adversarial
wasserstein barycenters of measures,” Journal of Mathematical trainingofneuralnetworks,”Thejournalofmachinelearningresearch,
ImagingandVision,vol.51,no.1,pp.22–45,2015. vol.17,no.1,pp.2096–2030,2016.
[18] B.SchmitzerandB.Wirth,“Aframeworkforwasserstein-1-type
[41] V.I.Bogachev,Measuretheory. SpringerScience&BusinessMedia,
metrics,”JournalofConvexAnalysis,vol.26,no.2,pp.353–396,2019.
2007,vol.1.
[19] J.-D. Benamou, G. Carlier, M. Cuturi, L. Nenna, and G. Peyre´,
[42] L. V. Kantorovich, “On the translocation of masses,” Journal of
“Iterativebregmanprojectionsforregularizedtransportationprob-
MathematicalSciences,vol.133,no.4,pp.1381–1382,2006.
lems,” SIAM Journal on Scientific Computing, vol. 37, no. 2, pp.
[43] I.Gulrajani,F.Ahmed,M.Arjovsky,V.Dumoulin,andA.Courville,
A1111–A1138,2015.
“Improvedtrainingofwassersteingans,”inNIPS’17Proceedings
[20] M.Cuturi,“Sinkhorndistances:Lightspeedcomputationofoptimal
ofthe31stInternationalConferenceonNeuralInformationProcessing
transport,”inAdvancesinneuralinformationprocessingsystems,2013,
Systems,vol.30,2017,pp.5769–5779.
pp.2292–2300.
[44] I.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-Farley,
[21] K.Fatras,T.Se´journe´,R.Flamary,andN.Courty,“Unbalanced
S.Ozair,A.Courville,andY.Bengio,“Generativeadversarialnets,”
minibatchoptimaltransport;applicationstodomainadaptation,”
Advancesinneuralinformationprocessingsystems,vol.27,2014.
inInternationalConferenceonMachineLearning. PMLR,2021,pp.
3186–3197. [45] C. K. I. Williams and M. Seeger, “Using the nystro¨m method
[22] K. Fatras, Y. Zine, R. Flamary, R. Gribonval, and N. Courty, tospeedupkernelmachines,”inAdvancesinNeuralInformation
“Learningwithminibatchwasserstein:asymptoticandgradient ProcessingSystems13,vol.13,2000,pp.682–688.
properties,” in AISTATS 2020-23nd International Conference on [46] Standford,“Thestanford3dscanningrepository,”[Online].Avail-
ArtificialIntelligenceandStatistics,vol.108,2020,pp.1–20. able:http://graphics.stanford.edu/data/3Dscanrep/.
[23] K.Nguyen,D.Nguyen,T.Pham,N.Hoetal.,“Improvingmini- [47] O.Hirose,“Accelerationofnon-rigidpointsetregistrationwith
batchoptimaltransportviapartialtransportation,”inInternational downsamplingandgaussianprocessregression,”IEEETransactions
ConferenceonMachineLearning. PMLR,2022,pp.16656–16690. onPatternAnalysisandMachineIntelligence,vol.43,no.8,pp.2858–
[24] J. Lellmann, D. A. Lorenz, C.-B. Scho¨nlieb, and T. Valkonen, 2865,2021.
“Imagingwithkantorovich–rubinsteindiscrepancy,”SiamJournal [48] L. Zhang, N. Snavely, B. Curless, and S. M. Seitz, “Spacetime
onImagingSciences,vol.7,no.4,pp.2833–2859,2014. faces:High-resolutioncaptureformodelingandanimation,”in
[25] P.BeslandH.McKay,“Amethodforregistrationof3-dshapes,” Data-Driven3DFacialAnimation. Springer,2008,pp.248–276.
IEEETransactionsonPatternAnalysisandMachineIntelligence,vol.14, [49] DataSet,“Matchinghumanswithdifferentconnectivity.”[Online].
no.2,pp.239–256,1992. Available:http://profs.scienze.univr.it/∼marin/shrec19/14
[50] S. Ge, G. Fan, and M. Ding, “Non-rigid point set registration Proceedings of the IEEE conference on computer vision and pattern
withglobal-localtopologypreservation,”inProceedingsoftheIEEE recognition,2016,pp.2818–2826.
Conference on Computer Vision and Pattern Recognition Workshops, [73] K.You,X.Wang,M.Long,andM.Jordan,“Towardsaccuratemodel
2014,pp.245–251. selectionindeepunsuperviseddomainadaptation,”inInternational
[51] A. Fan, J. Ma, X. Tian, X. Mei, and W. Liu, “Coherent point ConferenceonMachineLearning. PMLR,2019,pp.7124–7133.
driftrevisitedfornon-rigidshapematchingandregistration,”in [74] V.Sarode,X.Li,H.Goforth,Y.Aoki,R.A.Srivatsan,S.Lucey,and
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern H.Choset,“Pcrnet:Pointcloudregistrationnetworkusingpointnet
Recognition,2022,pp.1424–1434. encoding,”2019.
[52] F. Pomerleau, M. Liu, F. Colas, and R. Siegwart, “Challenging [75] P.L.Donti,D.Rolnick,andJ.Z.Kolter,“Dc3:Alearningmethodfor
datasetsforpointcloudregistrationalgorithms,”TheInternational optimizationwithhardconstraints,”arXivpreprintarXiv:2104.12225,
JournalofRoboticsResearch,vol.31,no.14,pp.1705–1711,2012. 2021.
[53] A.S.Mian,M.Bennamoun,andR.Owens,“Three-dimensional [76] X.Xia,X.Pan,N.Li,X.He,L.Ma,X.Zhang,andN.Ding,“Gan-
model-based object recognition and segmentation in cluttered basedanomalydetection:areview,”Neurocomputing,2022.
scenes,”IEEEtransactionsonpatternanalysisandmachineintelligence, [77] C. Li and G. H. Lee, “From synthetic to real: Unsupervised
vol.28,no.10,pp.1584–1601,2006. domain adaptation for animal pose estimation,” in Proceedings
oftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
[54] J.Yang,H.Li,D.Campbell,andY.Jia,“Go-icp:Agloballyoptimal
solutionto3dicppoint-setregistration,”IEEEtransactionsonpattern 2021,pp.1482–1491.
analysisandmachineintelligence,vol.38,no.11,pp.2241–2254,2015. [78] P.PanaredaBustoandJ.Gall,“Opensetdomainadaptation,”in
ProceedingsoftheIEEEinternationalconferenceoncomputervision,
[55] D. Chetverikov, D. Stepanov, and P. Krsek, “Robust euclidean
2017,pp.754–763.
alignment of 3d point sets: the trimmed iterative closest point
[79] R.HartleyandA.Zisserman,Multipleviewgeometryincomputer
algorithm,”Imageandvisioncomputing,vol.23,no.3,pp.299–309,
vision. Cambridgeuniversitypress,2003.
2005.
[80] J.Xu,D.J.Hsu,andA.Maleki,“Globalanalysisofexpectation
[56] Q.-Y.Zhou,J.Park,andV.Koltun,“Open3D:Amodernlibraryfor
maximizationformixturesoftwogaussians,”AdvancesinNeural
3Ddataprocessing,”arXiv:1801.09847,2018.
InformationProcessingSystems,vol.29,2016.
[57] Y.GrandvaletandY.Bengio,“Semi-supervisedlearningbyentropy
[81] B. B. Damodaran, B. Kellenberger, R. Flamary, D. Tuia, and
minimization,” Advances in neural information processing systems,
N.Courty,“Deepjdot:Deepjointdistributionoptimaltransportfor
vol.17,2004.
unsuperviseddomainadaptation,”inProceedingsoftheEuropean
[58] J. Liang, Y. Wang, D. Hu, R. He, and J. Feng, “A balanced and conferenceoncomputervision(ECCV),2018,pp.447–463.
uncertainty-aware approach for partial domain adaptation,” in
Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
UK,August23–28,2020,Proceedings,PartXI. Springer,2020,pp.
123–140.
[59] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningfor
imagerecognition,”inProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition,2016,pp.770–778.
[60] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z.Huang,A.Karpathy,A.Khosla,M.Bernsteinetal.,“Imagenet
large scale visual recognition challenge,” International journal of
computervision,vol.115,pp.211–252,2015.
[61] D.P.KingmaandJ.Ba,“Adam:Amethodforstochasticoptimiza-
tion,”arXivpreprintarXiv:1412.6980,2014.
[62] T.TielemanandG.Hinton,“Lecture6.5—RmsProp:Dividethegra-
dientbyarunningaverageofitsrecentmagnitude,”COURSERA:
NeuralNetworksforMachineLearning,2012.
[63] H.Venkateswara,J.Eusebio,S.Chakraborty,andS.Panchanathan,
“Deephashingnetworkforunsuperviseddomainadaptation,”in
Proceedings of the IEEE conference on computer vision and pattern
recognition,2017,pp.5018–5027.
[64] X. Peng, B. Usman, N. Kaushik, D. Wang, J. Hoffman, and
K.Saenko,“Visda:Asynthetic-to-realbenchmarkforvisualdomain
adaptation,”inProceedingsoftheIEEEConferenceonComputerVision
andPatternRecognitionWorkshops,2018,pp.2021–2026.
[65] G.Griffin,A.Holub,andP.Perona,“Caltech-256objectcategory
dataset,”2007.
[66] X.Peng,Q.Bai,X.Xia,Z.Huang,K.Saenko,andB.Wang,“Moment
matchingformulti-sourcedomainadaptation,”inProceedingsof
theIEEE/CVFinternationalconferenceoncomputervision,2019,pp.
1406–1415.
[67] K. Saito, D. Kim, S. Sclaroff, T. Darrell, and K. Saenko, “Semi-
superviseddomainadaptationviaminimaxentropy,”inProceedings
oftheIEEE/CVFinternationalconferenceoncomputervision,2019,pp.
8050–8058.
[68] L.VanderMaatenandG.Hinton,“Visualizingdatausingt-sne.”
Journalofmachinelearningresearch,vol.9,no.11,2008.
[69] J.Hu,H.Tuo,C.Wang,L.Qiao,H.Zhong,J.Yan,Z.Jing,and
H.Leung,“Discriminativepartialdomainadversarialnetwork,”in
EuropeanConferenceonComputerVision,2020,pp.632–648.
[70] J. Liang, D. Hu, and J. Feng, “Do we really need to access the
sourcedata?sourcehypothesistransferforunsuperviseddomain
adaptation,”inInternationalConferenceonMachineLearning. PMLR,
2020,pp.6028–6039.
[71] H.-Y.Chen,P.-H.Wang,C.-H.Liu,S.-C.Chang,J.-Y.Pan,Y.-T.Chen,
W.Wei,andD.-C.Juan,“Complementobjectivetraining,”arXiv
preprintarXiv:1903.01182,2019.
[72] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,
“Rethinking the inception architecture for computer vision,” in15
APPENDIX A
THEORETICAL RESULTS
WefirstderivetheKRdualityofL M,m inSec.A.1,andthencharacterizeitspotentialinSec.A.2.Wefinallydiscussthe
differentiabilityinSec.A.3.
A.1 KRDualityofL
M,m
Inthissubsection,wederivetheKRformulationofL M,m.Weusethefollowingnotations:
- (Ω,d):ametricdassociatedwithacompactmetricspaceΩ.Forexample,ΩisaclosedcubicinR3inpointsetregistration
tasks,andΩisaclosedcubicinRp inpartialdomainadaptationtasks,wherepisthedimensionoffeatures.
- C(Ω):thesetofcontinuousboundedfunctionsdefinedonΩequippedwiththesupremenorm.
- Lip(Ω)⊆C(Ω):thesetof1-LipschitzfunctiondefinedonΩ.
- M(X):thespaceofRadonmeasuresonspaceX.
- π1:Themarginalofπ onitsfirstvariable.Similarly,π2 representsthemarginalofπ onitssecondvariable.
# #
- GivenafunctionF:X →R∪+∞,theFenchelconjugateofFisdenotedasF∗ andisgivenby:
F∗(x∗)= sup <x,x∗ >−F(x), ∀x∗ ∈X∗ (26)
x∈X
whereX∗ isthedualspaceofX and<·>isthedualpairing.
RecallthedefinitionofL
M,m
andL D,h:forα,β ∈M +(Ω),
(cid:90)
L M,m(α,β)= inf c(x,y)dπ(x,y), (27)
π∈Γm(α,β) Ω×Ω
wherec:Ω×Ω→R+ isacontinuouscostfunction,andΓ m(α,β)isthesetofnon-negativemeasureπ definedonΩ×Ω
satisfying
π(A×Ω)≤α(A), π(Ω×A)≤β(A) and π(Ω×Ω)≥m
forallmeasurablesetA⊆Ω.Foreaseofnotations,weabbreviatem
α
=α(Ω),m
β
=β(Ω)andm(π)=π(Ω×Ω).Wealso
define
(cid:90)
L D,h(α,β)= inf c(x,y)dπ(x,y)−hm(π). (28)
π∈Γ0(α,β) Ω×Ω
whereh>0istheLagrangemultiplier.
WefirstderivetheFenchel-RockafellardualofL M,m(α,β).
Proposition1(DualformofL M,m). (27)canbeequivalentlyexpressedas
(cid:90) (cid:90)
L M,m(α,β)= sup fdα+ gdβ+mh. (29)
(f,g,h)∈R Ω Ω
wherethefeasiblesetRis
(cid:110) (cid:111)
R= (f,g,h)∈C(Ω)×C(Ω)×R +|f ≤0, g ≤0, c(x,y)−h−f(x)−g(y)≥0,∀x,y ∈Ω (30)
Inaddition,theinfimumin(27)isattained.
Proof. We prove this proposition via Fenchel-Rockafellar duality. We first define space E: C(Ω)×C(Ω)×R, space F:
C(Ω×Ω),andalinearoperatorA:E →F as
A(f,g,h):(x,y)→f(x)+g(y)+h; ∀f,g ∈C(Ω), ∀h∈R, ∀x,y ∈Ω. (31)
ThenweintroduceaconvexfunctionH:F →R∪+∞as
(cid:40)
0 if u≥−c
H(u)= (32)
+∞ else
andL:E →R∪+∞as
(cid:40)(cid:82) (cid:82)
fdα+ gdβ+hm if f ≥0, g ≥0, h≤0
L(f,g,h)= (33)
+∞ else
Wecancheckwhenf ≡g ≡1andh=−1,HiscontinuousatA(f,g,h).ThusbyFenchel-Rockafellarduality,wehave
inf H(A(f,g,h))+L(f,g,h)= sup −H∗(−π)−L∗(A∗π) (34)
(f,g,h)∈E π∈M(Ω×Ω)16
WefirstcomputeH∗(−π)andL∗(A∗π)ontheright-handsideof(34).Forarbitraryπ ∈M(Ω×Ω),wehave
H∗(−π)
(cid:110)(cid:90) (cid:111)
= sup (−u)dπ−H(u)
u∈F
(cid:110)(cid:90) (cid:111)
= sup (−u)dπ|u(x,y)≥−c(x,y), ∀(x,y)∈Ω×Ω
u∈F
(cid:110)(cid:90) (cid:111)
= sup udπ|u(x,y)≤c(x,y), ∀(x,y)∈Ω×Ω
u∈F
(cid:82)
Itiseasytoseethatifπ isanon-negativemeasure,thenthissupremumis cdπ,otherwiseitis+∞.Thus
(cid:40)(cid:82)
c(x,y)dπ(x,y) if π ∈M (Ω×Ω)
H∗(−π)= + (35)
+∞ else
Similarly,wehave
L∗(A∗π)
(cid:110) (cid:111)
= sup <(f,g,h),A∗π >−L(f,g,h)
(f,g,h)∈E
(cid:110) (cid:90) (cid:90) (cid:111)
= sup <A(f,g,h),π >−( fdα+ gdβ+hm)|f,g ≥0, h≤0
(f,g,h)∈E
(cid:110)(cid:90) (cid:90) (cid:111)
= sup fd(π1 −α)+ gd(π2 −β)+h(π(Ω×Ω)−m)|f,g ≥0, h≤0
# #
(f,g,h)∈E
If(α−π1)and(β−π2)arenon-negativemeasures,andπ(Ω×Ω)−m≥0,thissupremumis0,otherwiseitis+∞.Thus
# #
(cid:40)
0 if(α−π1)∈M (Ω), (β−π2)∈M (Ω), π(Ω×Ω)≥m
L∗(A∗π)= # + # + (36)
+∞ else
Inaddition,theleft-handsideof(34)reads
inf H(A(f,g,h))+L(f,g,h)
(f,g,h)∈E
(cid:110)(cid:90) (cid:90) (cid:111)
= inf fdα+ gdβ+hm|f,g ≥0, h≤0, f(x)+g(y)+h≥−c(x,y),∀x,y ∈Ω
(f,g,h)∈E
(cid:110)(cid:90) (cid:90) (cid:111)
=− sup fdα+ gdβ+hm|f,g ≤0, h≥0, f(x)+g(y)+h≤c(x,y),∀x,y ∈Ω
(f,g,h)∈E
Finally,byinsertingthesetermsinto(34),wehave
(cid:90) (cid:90) (cid:90)
sup fdα+ gdβ+hm= inf c(x,y)dπ(x,y),
(f,g,h)∈E π∈M+(Ω×Ω)
f,g≤0,h≥0 (α−π #1)∈M+(Ω),(β−π #2)∈M+(Ω)
f(x)+g(y)+h≤c(x,y)∀x,y∈Ω π(Ω×Ω)≥m
whichproves(29).
In addition, we can also check right-hand side of (34) is finite, since we can always construct independent coupling
π (cid:101) = α(Ωm )β(Ω)α⊗β, such that π (cid:101) ∈ M +(Ω×Ω), π (cid:101)#1 = αm (Ω)α ≤ α, π (cid:101)#2 = βm (Ω)β ≤ β and π (cid:101)(Ω×Ω) = m. Thus the
Fenchel-Rockafellardualitysuggeststheinfimumisattained.
Similarly,wecanderivetheFenchel-RockafellardualformofL D,h.
Proposition2(DualformofL D,h). (28)canbeequivalentlyexpressedas
(cid:90) (cid:90)
L D,h(α,β)= sup fdα+ gdβ. (37)
(f,g)∈R(h) Ω Ω
wherethefeasiblesetis
(cid:110) (cid:111)
R(h)= (f,g)∈C(Ω)×C(Ω)|f ≤0, g ≤0, c(x,y)−h−f(x)−g(y)≥0,∀x,y ∈Ω (38)
Inaddition,theinfimumin(28)isattained.17
BycomparingProposition2withProposition1,wecanseethatL D,h andL M,m arerelatedby
(cid:90) (cid:90)
L = sup fdα+ gdβ+mh
M,m
(f,g,h)∈R Ω Ω
= sup L D,h+mh. (39)
h∈R
+
Therefore,whenthecostfunctioncisthedistanced,weobtaintheKRformofL M,m byinserting(4)into(39).
Proposition3(KRformofL M,m). Whenc(x,y)=d(x,y),(27)canbereformulatedas
(cid:90) (cid:90)
L M,m(α,β)= sup fdα− fdβ+h(m−m β) (40)
f∈Lip(Ω),h∈R+ Ω Ω
−h≤f≤0
orequivalentlyas
(cid:90) (cid:90)
L M,m(α,β)= sup fdα− fdβ−inf(f)(m−m β). (41)
f∈Lip(Ω),f≤0 Ω Ω
Proof. Forequation(41),notethatgivenanf ∈C(Ω),theoptimalhissimply−inf(f)<+∞(f iscontinuousandΩis
compact).Sowecanreplacehby−inf(f)in(40)toobtain(41).
Forsimplicity,wedefinesomefunctionalsassociatedwithL M,m andL D,h.
Definition1(L M,m andL M,m). Define
(cid:40)(cid:82)
fd(α−β)+h(m−m ) h∈R , f ∈Lip(Ω), −h≤f ≤0
Lα,β (f,h)= Ω β +
M,m −∞ else,
and
(cid:40)(cid:82)
fd(α−β)−inf(f)(m−m ) f ∈Lip(Ω), f ≤0
Lα,β (f)= Ω β
M,m −∞ else.
Definition2(L D,h).
(cid:40)(cid:82) (cid:82)
fdα− fdβ−hm f ∈Lip(Ω), −h≤f ≤0,
Lα,β(f)= Ω Ω β
D,h −∞ else.
WealsodefinethefunctionalassociatedwithWasserstein-1metricW 1:
Definition3(W 1).
(cid:40)(cid:82) (cid:82)
fdα− fdβ f ∈Lip(Ω), m =m
Wα,β(f)= Ω Ω α β (42)
−∞ else,
Withthesedefinitions,wecanwrite
L (α,β)= sup Lα,β(f),
D,h D,h
f∈C(Ω)
L (α,β)= sup Lα,β (f,h) and L (α,β)= sup Lα,β (f),
M,m M,m M,m M,m
f∈C(Ω),h∈R f∈C(Ω)
and
W (α,β)= sup Wα,β(f).
1
f∈C(Ω)
Forsimplicity,weomitthenotationofαandβ wheretheyareclearfromthetext.
A.2 PropertiesofKRPotentials
BeforewecandiscussthepropertyoftheKRpotential,i.e.,themaximizerofL M,m andL D,h,wefirstneedtoshowthatthe
potentialsexist.AsforL D,h,theexistenceofpotentialsisalreadyknownin[18]:
Proposition4([18]). Forα,β ∈M +(Ω)andh>0,thereexistsanf∗ ∈C(Ω)suchthatL D,h(α,β)=Lα D, ,β h(f∗).
AsforL M,m,wecanalsoprovetheexistenceofthemaximizersimilarly:
Proposition5. Forα,β ∈M +(Ω)andm>0,thereexistf∗ ∈C(Ω)andh∈RsuchthatL M,m(α,β)=Lα M,β ,m(f∗,h).
Toprovethisproposition,weneedthefollowinglemma.
Lemma1(Continuity). Lα,β iscontinuousonC(Ω)×R.
M,m18
Proof. Let (f n,h n) → (f,h) in C(Ω)×R. Assume Lα M,β ,m(f n,h n) > −∞ when n is sufficiently large. We first check
Lα M,β ,m(f,h)>−∞asfollows.Forarbitraryϵ>0,thereexistsN >0suchthatforn>N,h
n
<h+ϵ,thusf
n
>−h
n
>
−h−ϵ.Bytakingn → ∞,weseeforarbitraryϵ > 0,f > −h−ϵ,whichsuggestsf ≥ −h.Inaddition,itiseasytosee
f ≤0andh≥0.ItisalsoeasytoseeLip(f)≤1duetotheclosenessofLip(Ω).Thusaccordingtodefinition42,weclaim
Lα M,β ,m(f,h)>−∞.Furthermore,since−h−ϵ<f
n
<0,andf
n
→f,bydominatedconvergencetheorem,wehave
(cid:90) (cid:90) (cid:90)
lim f ndα= lim f ndα= fdα, (43)
n→∞ Ω Ωn→∞ Ω
(cid:90) (cid:90) (cid:90)
and lim f ndβ = lim f ndβ = fdβ. (44)
n→∞ Ω Ωn→∞ Ω
Note we also have h n(m−m β) → h(m−m β). We conclude the proof by combining these three terms and obtaining
Lα M,β ,m(f n,h n)→Lα M,β ,m(f,h).
ProofofProposition5. Ifwecanfindamaximizingsequence(f n,h n)thatconvergesto(f,h)∈C(Ω)×R,thenLemma1
suggests that L M,m(α,β) = sup f,hLα M,β ,m(f,h) = lim n→∞Lα M,β ,m(f n,h n) = Lα M,β ,m(f,h), which proves this proposition.
Therefore,weonlyneedtoshowthatitisalwayspossibletoconstructsuchamaximizingsequence.
Let(f n,h n)beamaximizingsequence.Weabbreviatemax(f n)=max x∈Ω(f n(x))andmin(f n)=min x∈Ω(f n(x)).We
first assume f n does not have any bounded subsequence, then there exists N > 0, such that for all n > N, min(f n) <
−diam(Ω) (otherwise we can simply collect a subsequence of f n bounded by diam(Ω)). We can therefore construct
f(cid:102)n = f n −(min(f n)+diam(Ω)) and h(cid:102)n = h n +(min(f n)+diam(Ω)). Note that max(f(cid:102)n) ≤ min(f(cid:102)n)+diam(Ω) =
−diam(Ω)+diam(Ω)=0,f(cid:102)n+h(cid:102)n =f n+h
n
≥0,andf(cid:102)n ∈Lip(Ω),soLα M,β ,m(f(cid:102)n,h(cid:102)n)>−∞,and
(cid:90)
Lα M,β ,m(f(cid:102)n,h(cid:102)n)= f(cid:102)nd(α−β)+h(cid:102)n(m−m β)
Ω
(cid:90)
= f d(α−β)+h (m−m )+(min(f )−diam(Ω))(m−m )
n n β n α
Ω
≥Lα,β (f ,h ),
M,m n n
whichsuggeststhat(f(cid:102)n,h(cid:102)n)isabettermaximizingsequencethan(f n,h n).Notethatf(cid:102)n isuniformlyboundedbydiam(Ω)
because 0 ≥ f(cid:102)n ≥ min(f(cid:102)n) = −diam(Ω). As a result, we can always assume h(cid:102)n is also bounded by diam(Ω). Because
otherwisewecanconstructh
n
=−min(f(cid:102)n)≤diam(Ω),anditiseasytoshow(f(cid:102)n,h n)isabettermaximizingsequence
than(f(cid:102)n,h(cid:102)n).Insummary,wecanalwaysfindamaximizingsequence(f n,h n),suchthatbothf n andh n areboundedby
diam(Ω).
Finally,sincef nisuniformlyboundedandequicontinuous,f nconvergesuniformly(uptoasubsequence)toacontinuous
functionf.Inaddition,h n hasaconvergentsubsequencesinceitisbounded.Therefore,wecanalwaysfindamaximizing
sequence(f n,h n)thatconvergestosome(f,h)∈C(Ω)×R,whichfinishestheproof.
RemarkTheproofofProposition5isananalogueofProposition2.11in[18].ThedifferenceisthatinProposition5,
besidesf,weadditionallyneedtohandleanothervariablehactingasthelowerboundoff.
Nowweproceedtothemainresultinthissubsection,whichstatesthatthepotentialis0or−hontheomittedmass,
thusithas0gradientonthismass.Thisqualitativedescriptionrevealsaninterestingconnectionbetweenouralgorithmand
WGAN[1].
First,wenotethat,thepotentialofW 1 canbecharacterizedasfollows.
Lemma 2 (Potential of W 1 [43]). Let f∗ be a maximizer of W 1α,β . If f∗ is differentiable, and there exists a primal solution π
satisfyingπ(x=y)=0,thenf∗ hasgradientnorm1(α+β)-almostsurely.
ThenwecharacterizetheflatnessofthepotentialofL D,h inthefollowingtwolemmas.
Lemma3. Letπ bethesolutiontotheprimalformofL D,h(α,β).Assumeα′,β′ ∈M(Ω)satisfy
π1 ≤α′ ≤α and π2 ≤β′ ≤β. (45)
# #
(1)π isalsothesolutiontotheprimalformofL D,h(α′,β′)andW 1(π #1,π #2),thus
L D,h(α,β)=L D,h(α′,β′)=W 1(π #1,π #2)−hm(π) (46)
(2)Letf∗ beamaximizerofLα,β .Thenf∗ isalsoamaximizerofLα′,β′ andWπ #1,π #2 .
D,h D,h 1
Proof. (1)First,itiseasytoverifythatπisindeedanadmissiblesolutiontotheprimalformofL D,h(α′,β′).Then,noticethat
alltheadmissiblesolutionstotheprimalformofL D,h(α′,β′)arealsoadmissiblesolutionstotheprimalformofL D,h(α,β).
Finally,wecanconcludebycontradiction:IfthereexistsabettersolutionthanπforL D,h(α′,β′),thenitisalsobetterthanπ
forL D,h(α,β),whichcontradictstheoptimalityofπ.Similarly,wecanprovethatπ isalsothesolutiontoW 1(π #1,π #2).19
(2)Notewehave
(cid:90) (cid:90) (cid:90) (cid:90)
Lα D′ ,, hβ′ (f∗)= f∗dα′+ (−h−f∗)dβ′ ≥ f∗dα+ (−h−f∗)dβ =Lα D, ,β h(f∗)=L D,h(α,β), (47)
Ω Ω Ω Ω
where the inequality holds because f∗ ≤ 0, −h−f∗ ≤ 0, α′ ≤ α and β′ ≤ β. According to the first part of this proof,
wehaveL D,h(α′,β′)=L D,h(α,β),thusL Dα′ ,, hβ′ (f∗)≤L D,h(α′,β′)=L D,h(α,β).Bycombiningthesetwoequalities,we
concludethatLα D′ ,, hβ′ (f∗)=L D,h(α′,β′),i.e.,f∗ isamaximizerofLα D′ ,, hβ′ .
Inaddition,wehave
(cid:90) (cid:90)
Wπ #1,π #2 (f∗)−hm(π)= f∗dπ1 − f∗dπ2 −hm(π2)=Lπ #1,π #2 (f∗).
1 # # # D,h
Ω Ω
Duetothefirstpartof(2),f∗ isamaximizerofL Dπ #1 ,h,π #2 ,thusLπ D#1 ,h,π #2 (f∗)=L D,h(π #1,π #2).Inaddition,L D,h(π #1,π #2)=
W 1(π #1,π #2)−hm(π)duetothefirstpartofthislemma.ThereforewehaveW 1π #1,π #2 (f∗)−hm(π)=W 1(π #1,π #2)−hm(π),
whichimpliesthatf∗ isamaximizerofWπ #1,π #2 .
1
Lemma4(Flatnessonomittedmass). Letf∗ beamaximizerofLα,β ,andπ bethesolutiontotheprimalformofLα,β .Assume
D,h D,h
thatα S,β
S
∈M +(Ω)satisfy
π1 ≤α−α ≤α and π2 ≤β−β ≤β.
# S # S
Thenf∗ =0α S-almostsurely,andf∗ =−hβ S-almostsurely.
Proof. AccordingtoLemma3,wehaveLα−αS,β =Lα,β ,andf∗ isamaximizerofLα−αS,β ,i.e.,wehave
D,h D,h D,h
(cid:90) (cid:90) (cid:90) (cid:90)
Lα,β = f∗dα− f∗β−hm = f∗d(α−α )− f∗β−hm =Lα−αS,β.
D,h β S β D,h
Ω Ω Ω Ω
By cleaning this equation, we obtain (cid:82) Ωf∗dα S = 0. Since f∗ ≤ 0 and α S is a non-negative measure, we conclude that
f∗ =0α S-almostsurely.Thestatementforβ S canbeprovedsimilarly.
Finally,wecanpresentaqualitativedescriptionofthepotentialofL D,h bydecomposingthemassintotransportedand
omittedmass:
Proposition 6. Letf∗ beamaximizerofL D,h(α,β),andassumethatthereexistsaprimalsolutionπ satisfyingπ(x = y) = 0.
Thenthereexistnon-negativemeasuresµ,ν
α
≤αandν
β
≤β satisfyingµ+ν α+ν
β
=α+β,suchthat1)f∗ hasgradientnorm1
µ-almostsurely,2)f∗ =0ν α-almostsurely,and3)f∗ =−hν β-almostsurely.
Proof. Let ν α = α−π #1, where π is the solution to the primal form of Lα D, ,β h. It is easy to verify π #1 ≤ α−ν α ≤ α, thus
Lemma4suggeststhatf∗ =0ν α-almostsurely.Similarly,letν β =β−π #2,thenf∗ =−hν β-almostsurely.Accordingto
Lemma3,f∗isamaximizerofWπ #1,π #2 ,thenLemma2immediatelysuggeststhatf∗hasgradientnorm1(π2 +π1)-almost
1 # #
surely.Wefinishtheproofbylettingµ=π2 +π1.
# #
Since0and−haretheupperandlowerboundsoff∗ respectively,bydefiningν =ν β +ν α,weimmediatelyhavethe
followingcorollary:
Corollary1. Letf∗ beamaximizerofL D,h(α,β),andassumethatthereexistsaprimalsolutionπsatisfyingπ(x=y)=0.Iff∗ is
differentiable,thenthereexistnon-negativemeasuresµ,ν satisfyingµ+ν =α+β,suchthat1)||∇f∗||=1µ-almostsurely,2)
∇f∗ =0ν-almostsurely.
Finally,wenotethatasimilarstatementholdsforL M,m,becausethepotentialofL M,m canberecoveredfromL D,h:
Lemma5(RelationsbetweenL M andL D). Letf∗ beamaximizerofLα M,β ,m.Forthefixedh∗ =−inf(f∗),f∗ isalsoamaximizer
ofLα,β
.
D,h∗
Corollary2. Letf∗ beamaximizerofL M,m(α,β).andassumethatthereexistsaprimalsolutionπ satisfyingπ(x=y)=0.Iff∗
isdifferentiable,thenthereexistnon-negativemeasuresµ,ν satisfyingµ+ν =α+β,suchthat1)||∇f∗||=1µ-almostsurely,2)
∇f∗ =0ν-almostsurely.20
A.3 Differentiability
WenowconsiderthedifferentiabilityoftheKRforms.Wehavethefollowingtwopropositions.
Assumption 1. For every θ˜ , there exist a neighborhood U of θ˜ and a constant ∆(θ˜), such that for θ,θ′ ∈ U and y 1,y 2 ∈ Ω′,
||T θ(y 1)−T θ′(y 2)||≤∆(θ˜)(||θ−θ′||+||y 2−y 1||).
Proposition7(DifferentiabilityofL M,m). IfT θ satisfiesassumption1,thenL M,m(α,β θ)iscontinuousw.r.t.θ,andisdifferentiable
almosteverywhere.Furthermore,wehave
(cid:90)
∇ θL M,m(α,β θ)=− ∇ θf∗(T θ(x))dβ, (48)
Ω
whenbothsidesarewelldefined.
Proof. For every θ˜∈ Rp, select a neighborhood U of θ˜ and a constant ∆(θ˜) according to assumption 1, such that for all
θ,θ′ ∈U andy,y′ ∈Ω′,||T θ(y)−T θ′(y′)||≤∆(θ˜)(||θ−θ′||+||y−y′||).Bylettingy =y′,wehave
||T θ(y)−T θ′(y)||≤∆(θ˜)(||θ−θ′||). (49)
Nowweconsiderthetransportationbetweenα,β
θ
andβ θ′.LetπbethesolutiontotheprimalformofL M,m(α,β θ),and
π′ bethesolutiontotheprimalformofL M,m(α,β θ′).Letβ 1 =π #2 ≤β θ,α 2 =π #′1 ≤αandβ 2 =π #′2 ≤β θ′.SinceW 1 isa
metric,bytriangleinequality,wehave
W 1(α 2,β 1)≤W 1(α 2,β 2)+W 1(β 2,β 1) (50)
AccordingtoLemma3,wehaveW 1(α 2,β 2)=L M,m(α,β θ′).BytheoptimalityofL M,m(α,β θ),wehaveL M,m(α,β θ)≤
W 1(α 2,β 1).Inaddition,wehave
(cid:90)
W (β ,β )≤W (β ,β )≤ d(T (y)−T (y))dβ(y)≤m ∆(θ˜)(||θ−θ′||),
1 2 1 1 θ θ′ θ θ′ β
Ω
wherethefirstinequalityholdsbecauseβ 2 ≤β θ′ andβ 1 ≤β θ,andthethirdinequalityholdsbecauseof(49).Byinserting
theseinequalitiesandequationsbackinto(50),weobtain
L (α,β )≤L (α,β )+m ∆||θ−θ′||,
M,m θ M,m θ′ β
whichprovesL D,h(α,β θ)islocallyLipschitzw.r.t.θ,thereforeRadamacher’stheoremstatesthatitisdifferentiablealmost
everywhere.
Finally,sincethemaximizeroftheKRformofL M,m(α,β θ)existsaccordingtoProposition5,followingtheenvelope
theoremandtheargumentsin[1],wecanconclude
(cid:90) (cid:90)
∇ L (α,β )=−∇ f∗(T (x))dβ =− ∇ f∗(T (x))dβ,
θ M,m θ θ θ θ θ
Ω Ω
whenbothsidesoftheequationarewell-defined.
Proposition8(DifferentiabilityofL D,h). IfT θ satisfiesassumption1,thenL D,h(α,β θ)iscontinuousw.r.t.θ,andisdifferentiable
almosteverywhere.Furthermore,wehave
(cid:90)
∇ θL D,h(α,β θ)=− ∇ θf∗(T θ(x))dβ, (51)
Ω
whenbothsidesarewelldefined.
Proof.
SimilartotheproofofProposition7,foreveryθ˜∈Rp,weselectaneighborhoodU ofθ˜ andaconstant∆(θ˜),such
thatforallθ,θ′ ∈U andy ∈Ω′ inequality(49)holds.
DefineKRh(α,β)=L D,h(α,β)+ h 2(m α+m β).ItisknownthatKRh isametric[24],[41],thusbytriangleinequalityof
theKRmetric,
(cid:16) h (cid:17) (cid:16) h (cid:17)
|L D,h(α,β θ)−L D,h(α,β θ′)|=| KRh(α,β θ)− 2(m α+m βθ) − KRh(α,β θ′)− 2(m α+m
β
θ′) |
=|KRh(α,β θ)−KRh(α,β θ′)|≤KRh(β θ′β θ) (52)
forarbitraryθ,θ′ ∈Rd.Notethatthesecondequalityholdsbecausem β θ′ =m βθ =m β.Inaddition,wehave
(cid:90) (cid:90) (cid:90)
KRh(β θ′,β θ)= sup fd(β
θ′
−β θ)≤ sup fd(β
θ′
−β θ)=W 1(β θ′,β θ)≤ d(T θ(y)−T θ′(y))dβ(y)
f∈Lip(Ω) Ω f∈Lip(Ω) Ω Ω
−h≤f≤h
2 2
≤m ∆(θ˜)(||θ−θ′||),
β21
wherethelastinequalityholdsbecauseof(49).Therefore,wehave
L D,h(α,β θ)−L D,h(α,β θ′)≤KRh(β θ′,β θ)≤m β∆||θ−θ′||,
which proves L D,h(α,β θ) is locally Lipschitz w.r.t. θ, and Radamacher’s theorem states that it is differentiable almost
everywhere.TherestoftheproofissimilartothatofProposition7.
Forclearness,wesummarizetheresultsandprovethetheoremsinourwork.
ProofofTheorem1. The KR formulation of L D,h and the existence of its solution is proved in [18]. The gradient of
L D,h(α,β θ)isderivedinProposition8.
ProofofTheorem2. TheKRformulationofL M,m isgiveninProposition3,andtheexistenceoftheoptimizerisprovedin
Proposition5.ThegradientofL M,m(α,β θ)isderivedinProposition7.
WefinallyverifythattheparametrizedtransformationT θ (17)satisfiesassumption1.
Proposition9(Definition17satisfiesassumption1). Lety ∈Ω′ ⊆R1×3 beapointin3Dspace.DefineT θ(y)=yA+t+v(y)
and θ = (A,t,v), where A ∈ R3×3 is an affinity matrix, t ∈ R1×3 is a translation vector, v(y) ∈ R1×3 is the offset vectors of
y. Assume that T θ is coherent, i.e., for all y ∈ Ω′, there exists a constant J > 0, such that ||∇ yv|| ≤ J where ∇ is the Jacobian
matrix.Then,foreveryθ˜ ,thereexistaneighborhoodU θ˜(δ)ofθ˜ andaconstant∆(θ˜),suchthatforθ,θ′ ∈ U θ˜(δ)andy 1,y
2
∈ Ω′,
||T θ(y 1),T θ′(y 2)||≤∆(θ˜)(||θ−θ′||+||y 2−y 1||).
Proof. Wehave
||T (y )−T (y )||=||T (y )−T (y )+T (y )−T (y )||
θ 1 θ′ 2 θ 1 θ 2 θ 2 θ′ 2
≤||T (y )−T (y )||+||T (y )−T (y )||
θ 1 θ 2 θ 2 θ′ 2
=||(y −y )A+(v(y )−v(y ))||+||y (A−A′)+(t−t′)+(v(y )−v′(y ))||
2 1 2 1 2 2 2
≤||y −y ||||θ||+||∇ v(y )||||y −y ||+||y ||||θ−θ′||+||θ−θ′||+||θ−θ′||
2 1 y t 2 1 2
≤||y −y ||(||θ˜||+δ)+J||y −y ||+diam(Ω′)||θ−θ′||+2||θ−θ′||
2 1 2 1
=(||θ˜||+δ)||y −y ||+(diam(Ω′)+2)||θ−θ′||
2 1
≤max(cid:0) (||θ˜||+δ),(diam(Ω′)+2)(cid:1) (||y −y ||+||θ−θ′||),
2 1
wherey t
isgivenbythemeanvaluetheorem.Wefinishtheproofbyletting∆(θ˜)=max(cid:0) (||θ˜||+δ),(diam(Ω′)+2)(cid:1)
.
Thestatementalsoholdsforaforwardneuralnetwork:
Proposition10(Aforwardneuralnetworksatisfiesassumption1). Lety ⊆Ω′ ⊆Rp beapdimensionalinput,andaforward
neuralnetworkT
θ
consistingoflinearandactivationlayers.Foreveryθ˜ ,thereexistaneighborhoodU θ˜(δ)ofθ˜ andaconstant∆(θ˜),
suchthatforθ,θ′ ∈U θ˜(δ)andy 1,y
2
∈Ω′,||T θ(y 1),T θ′(y 2)||≤∆(θ˜)(||θ−θ′||+||y 2−y 1||).
Proof. WeonlydiscussthecasewhenT θ isdifferentiable.Wehave
||T (y )−T (y )||≤||T (y )−T (y )||+||T (y )−T (y )||≤||y −y ||||∇ T (x)||+||∇ T (y )||||θ−θ′||,
θ 1 θ′ 2 θ 1 θ 2 θ 2 ′θ 2 2 1 x θ θ θ′′ 2
where x and θ′′ is given by the mean value theorem. In addition, by chain rule, we have ||∇ xT θ(x)|| ≤ ∆ 1(θ), and
||∇ θT θ(y 2)||≤||y 2||∆ 2(θ),where∆
1
and∆
2
arefunctionsofθ,andtheexplicitformscanbefoundin[1].Bycombining
theseinequalities,wehave
||T (y )−T (y )||≤||y −y ||∆ (θ)+diam(Ω′)∆ (θ)||θ−θ′||≤max(cid:0) ∆ (θ),diam(Ω′)∆ (θ)(cid:1) (||y −y ||+||θ−θ′||),
θ 1 θ′ 2 2 1 1 2 1 2 2 1
whichfinishesourproof.22
APPENDIX B - ExperimentsinSec.5.2.2:ForPWAN,weset(ρ,λ,σ,T)=
MORE EXPERIMENT DETAILS (2,0.01,0.1,2000); For TPS-RPM, we set T finalfac =
500, frac = 1, and T init = 1.5; For GMM-REG,
B.1 MoreDetailsinSec.4.2
we set sigma = 0.5,0.2,0.02, Lambda = .1,.02,.01,
Todemonstratetheaccuracyoftheproposedneuralapprox- max function evals = 50,50,100 and level = 3. For
imation, we quantitatively compare the approximated KR BCPDandCPD,weset(β,λ,w)=(2.0,2.0,0.1).
forms and the primal forms in each setting in Fig. 3. We - Experimentsonthehumanfacedataset:Weusem-PWAN
computetheprimalformsusinglinearprograms.Theresults with(ρ,λ,σ,T)=(0.5,5×10−4,1.0,3000);ForBCPDand
aresummarizedinTab.5.Ascanbeseen,ourapproximated CPD,weset(β,λ,w)=(3.0,20.0,0.1).
KR forms are close to the true values with an average - Experimentsonthehumanbodydataset:Weusem-PWAN
relativeerrorlessthan0.2%,whichissufficientlyaccurate with (ρ,λ,σ,T) = (1.0,1 × 10−4,1.0,3000); For BCPD
formachinelearningapplications. and CPD, we set (β,λ,w) = (0.3,2.0,0.1). We also tried
w = 0.6 but the results are similar (not shown in our
TABLE5 experiments).
QuantitativecomparisonbetweentheapproximatedKRformsand - Rigid registration: We set (β,λ,w) = (2.0,1e9,0.1) for
primalformsonthefishshapeinFig.3
BCPD.WeusetherigidversionoftheCPDsoftware.We
setthesameparameterforICPasPWAN,i.e.,wesetthe
LM,25 LM,50 LM,78 LD,0.648 LD,1.09 LD,5 W1
distancethresholdd=0.05ford-ICP,andsetthetrimming
Primal 0.1354 0.4191 0.8955 -0.0722 -0.2795 -4.1044 1.0835
KR(Ours) 0.1352 0.4202 0.8994 -0.0724 -0.2791 -4.1004 1.0893 ratem=0.8min(q,r)form-ICP.
TheparametersforCPDandBCPDaresuggestedin[30].
Wenotethatinourexperiments,wedetermineparameter
mform-PWANbyestimatingtheoverlapratioandsetting
B.2 MoreDetailsinSec.5.1 masthenumberofoverlappedpoints.Afuturedirectionis
Toestimatethegradientofthecoherenceenergyefficiently, toautomaticallydeterminetheoverlapratioasin[79].For
we first decompose G as G ≈ QΛQT via the Nystro¨m d-PWAN, we always assume the point sets are uniformly
method, where k ≪ r, Q ∈ Rr×k, and Λ ∈ Rk×k is a distributed (otherwise we can downsample the point sets
diagonalmatrix.ThenweapplytheWoodburyidentityto usingvoxelfiltering),andsethclosetothenearestdistance
(σI+QΛQT)−1 andobtain betweenpointsinapointset.
(σI+QΛQT)−1 =σ−1I−σ−2Q(Λ−1+σ−1QTQ)−1QT.
B.4 MoreDetailsinSec.5.2.1
As a result, the gradient of the coherence energy can be KLdivergenceandL
2
distancebetweenpointsetsX andY
approximatedas areformallydefinedas
∂C θ =2λ(σI+G)−1V L (X,Y)= (cid:88) 1 ϕ(0|x −x ,2σ)+ 1 ϕ(0|y −y ,2σ)
∂V 2 q2 i j r2 i j
≈(2λ)(σ−1V −σ−2Q(Λ−1+σ−1QTQ)−1QTV). xi,xj∈X
yi,yj∈Y
2
− ϕ(0|x −y ,2σ),
B.3 DetailedExperimentalSettingsinSec.5.2 qr i j
Thenetworkusedinourexperimentisa5-layerpoint-wise 1 (cid:88) (cid:16) 1 (cid:88) 1 (cid:17)
KL(X,Y)=− log ω +(1−ω) ϕ(y |x ,σ) ,
multi-layerperceptronwithaskipconnection.Thedetailed q q r j i
structureisshowninFig.15.
yj∈Y xi∈X
whereϕ(·|u,σ)istheGaussiandistributionwithmeanuand
varianceσ.Forsimplicity,wesetσ =1andω =0.2forKL
andL 2 inSec.5.2.1.
WenotethatcomparedtoKLdivergenceandL 2distance,
Wasserstein type divergence is “smoother”, i.e., it can be
optimizedmoreeasily.Toseethis,wepresentatoyexample
comparing W 1, KL divergence and L 2 distance in Fig. 16.
Fig.15.Thestructureofthenetworkusedinourexperiments.Theinput We set ω = 0 for KL divergence since the dataset is clean,
is a matrix of shape (n,3) representing the coordinates of all points andwesetσ =1.WefixthereferencesetX ={−2,2},and
intheset,andtheoutputisamatrixofshape(n,1)representingthe
movethesourcesetY ={x,y}inthe2Dspace.Wepresent
potentialofthecorrespondingpoints.mlp(x)representsamulti-layer
perceptron(mlp)withthesizex.Forexample,mlp(m,n)represents thediscrepanciesbetweenX andY asafunctionof(x,y),
an mlp consisting of two layers, and the size of each layer is m and andcomputetheirrespectivegradients.
n.WeuseReLuactivationfunctioninallexcepttheoutputlayer.The
Ascanbeseen,alldiscrepancieshavetwoglobalminima
activationfunctionl(x;h) = max{−|x|,−h}isaddedtotheoutputto
cliptheoutputtotheinterval[−h,0].
(−2,2)and(2,−2)correspondingtothecorrectalignment.
However,KLdivergencehasasuspiciousstationarypoint
Wetrainthenetworkf w,husingtheAdamoptimizer[61], (0,0), which can trap both expectation-maximization-type
and we train the transformation T θ using the RMSprop andgradient-descent-typealgorithms[80].Inaddition,there
optimizer [62]. The learning rates of both optimizers are existsomeregionswherethegradientnormofL 2 distanceis
setto10−4.Theparametersaresetasfollows: small,whichindicatesoptimizingL 2 maybeslowinthese23
(a) Experimentalsetting.
(b) L2 (c) KL (d) W1
Fig.16.Comparisonofdifferentdiscrepanciesonapairoftoypointsets.Thetopandbottomrowin(b),(c)and(d)representthediscrepanciesand
theirgradientnormsrespectively.
TABLE6
regions. In contrast, the gradient norm of W 1 is constant, Quantitativeresultofregisteringthespace-timefacesdataset.
thustheoptimizationprocesscaneasilyconvergetoglobal
minima. BCPD CPD PWAN
MSE(×10−3) 1.7(1.0) 0.49(0.2) 0.32(0.08)
B.5 MoreDetailsinSec.5.2.2
Thisdatasetconsistsof44shapes,andwemanuallyselect3
WepresentqualitativeresultsoftheexperimentsinFig.17
pairsofshapesforourexperiments.Togenerateapointset
andFig.18.WedonotshowtheresultsofTPS-RPMonthe forashape,wefirstsample50000randompointsfromthe
secondexperimentasitgenerallyfailstoconverge.Ascanbe
3Dmesh,andthenapplyvoxelgridfilteringtodown-sample
seen,PWANsuccessfullyregistersthepointsetsinallcases, thepointsettolessthan10000points.Thedescriptionfor
whileallbaselinemethodsbiastowardthenoisepointsor
thegeneratedpointsetsispresentedinTab.7
tothenon-overlappedregionwhentheoutlierratioishigh,
exceptforTPS-RPMwhichshowsstrongrobustnessagainst
TABLE7
noisepointscomparablywithPWANinthefirstexample. Pointsetsusedforregistration.no.mrepresentsthem-thshapeinthe
We further evaluate PWAN on large-scale armadillo dataset[49].
datasets.WecomparePWANwithBCPDandCPD,because
they are the only baseline methods that are scalable in (no.1,no.42) (no.18,no.19) (no.30,no.31)
Size (5575,5793) (6090,6175) (6895,6792)
this experiment. We present some registration results in
samepose differentpose differentpose
Fig.20.Ascanbeseen,ourmethodcanhandlebothcases Description differentperson sameperson differentperson
successfully,whilebothCPDandBCPDbiastowardoutliers.
ThetrainingdetailsofPWANinthisexampleareshown
We conduct two experiments to evaluate PWAN on
inFig.21.Duetoitsadversarialnature,thelossofPWAN
registeringcompleteandincompletepointsetsrespectively.
doesnotdecreasemonotonically,instead,italwaysincreases
In the first experiment, we register 3 pairs of point sets
during the first few steps, and then starts to decrease.
usingPWAN,wherethehumanshapescomefromdifferent
In addition, the maximal gradient norm of the network
peopleor/andwithdifferentposes.Inthesecondexperiment,
(Lipschitz constant) is indeed controlled near 1, and the
we register incomplete point sets which are generated by
MSEdecreasesduringthetrainingprocess.Anexampleof
cropping a fraction of the no.30 and no.31 point sets. For
theregistrationprocessispresentedinFig.19(a).
both experiments, we compare PWAN with CPD [10] and
BCPD [30], and we only present qualitative registration
B.6 MoreDetailsinSec.5.2.4
results, because we do not know the true correspondence
The quantitative comparison on human face datasets is betweenpointsets.
presentedinTab.6. TheresultsofthefirstexperimentareshowninFig.22.As
ThehumanbodydatasetistakenfromaSHREC’19track canbeseen,PWANcanhandlearticulateddeformationsand
called“matchinghumanswithdifferentconnectivity”[49]. producegoodfull-bodyregistrationresults.Incontrast,CPD24
(a) Initialsets (b) BCPD (c) CPD (d) GMM-REG (e) TPS-RPM (f) PWAN
Fig.17.Anexampleofregisteringnoisypointsets.Theoutlier/non-outlierratiosare0.2(1strow),1.2(2ndrow)and2.0(3rdrow).
(a) Initialsets (b) BCPD (c) CPD (d) GMM-REG (e) d-PWAN (f) m-PWAN
Fig.18.Anexampleofregisteringpartiallyoverlappedpointsets.Theoverlapratiosare0.57(1strow),0.75(2ndrow)and1(3rdrow).
and BCPD have difficulties aligning point sets with large example,inthe3-rdrow,theybothwronglymatchtheleft
articulateddeformations,assignificantregistrationerrorsare arm to the body, which causes highly unnatural artifacts.
observednearthelimbs. In contrast, the proposed PWAN can handle the partial
matchingproblemwell,sinceitsuccessfullymaintainsthe
TheresultsofthesecondexperimentareshowninFig.23.
shapeofnon-overlappingregions,whichcontributestothe
Ascanbeseen,bothCPDandBCPDfailinthisexperiment,
naturalregistrationresults.
as the non-overlapping points are seriously biased. For25
(a) Atrajectoryofnon-rigidregistrationofthearmadillodataset.
(b) Atrajectoryofrigidregistrationofthemountaindataset.
Fig.19.Registrationtrajectory.Weshowtheprocessofnon-rigid(a)andrigidregistration(b)fromlefttoright.
Initialsets BCPD CPD PWAN
(a) Anexampleofregisteringnoisypointsets.Thesourceandreferencesetcontain8×104and1.76×105pointsrespectively.
Initialsets BCPD CPD d-PWAN m-PWAN
(b) Anexampleofregisteringpartiallyoverlappedpointsets.Thesourceandreferencesetbothcontain7×104points.
Fig.20.Examplesofregisteringlarge-scalepointsets.
seen that both types of PWAN can accurately align all
point sets, while all baseline algorithms failed to handle
theparasaurolophusshapes.
B.8 ExplanationoftheEffectofm
α
Toexplaintheeffectofm α inourformulation,wepresenta
toyexampleinFig.25,whereαandβ θ areuniformdistribu-
tionssupportedonpoints.Weconsiderthecorrespondence
Fig. 21. Training details of PWAN on the pair of point sets shown in
Fig.20(a)
givenbyL M,1(α,β θ),wherewefixm
β
=1andincreasem
α
from1toinfinity.Fewerdatapointsinαarealignedasm
α
increases,andthealignmentgraduallybecomesthenearest
B.7 MoreDetailsinSec.5.2.5 neighborhoodalignmentwhenm α isclosetoinfinity.
The detailed quantitative results of rigid registration are
B.9 DetailedExperimentalSettingsinSec.6.2
presented in Tab. 8, and we additionally present some
examples of the registration results in Fig. 24. It can be TheparametersofPWANaresetasfollows:26
(a) Sourceset (b) Referenceset (c) PWAN(Ours) (d) BCPD[30] (e) CPD[10]
Fig.22. Theresultsofregisteringcompletepointsetsno.1tono.42(1-strow),no.18tono.19(2-ndrow),andno.30tono.31(3-rdrow).
(a) Sourceset (b) Referenceset (c) PWAN(Ours) (d) BCPD[30] (e) CPD[10]
Fig.23. Registeringincompletepointsetsno.30tono.31.Wepresenttheresultsofcomplete-to-incomplete(1-strow),incomplete-to-complete(2-nd
row)andincomplete-to-incomplete(3-rdrow)registration.27
TABLE8
Quantitativeresultsofrigidregistration.Wereportthemedianandstandarddeviationofrotationerrors.
apartment mountain stair wood-summer parasaurolophus T-rex
BCPD 0.71(13.4) 8.07(7.4) 0.24(3.8) 7.32(9.4) 0.16(0.2) 0.09(0.2)
CPD 0.50(1.7) 6.39(3.6) 0.9(39.2) 1.99(1.6) 0.17(60.8) 0.08(63.2)
m-ICP 1.86(28.9) 11.23(13.9) 0.32(2.3) 11.29(7.9) 1.94(8.1) 1.85(2.5)
d-ICP 11.14(23.2) 13.44(6.1) 3.92(6.4) 23.68(8.2) 21.29(8.1) 21.05(4.2)
m-PWAN(Ours) 0.32(19.5) 4.67(4.1) 0.23(0.2) 1.17(0.8) 0.11(0.4) 0.10(0.1)
d-PWAN(Ours) 0.36(29.9) 4.7(4.1) 0.23(0.2) 1.20(0.8) 0.12(2.3) 0.07(7.4)
(a) Initialsets (b) d-PWAN (c) m-PWAN (d) BCPD (e) CPD (f) d-ICP (g) m-ICP
Fig.24. Examplesofrigidregistrationresultsonapairofapartment(indoor),mountain(outdoor)andparasaurolophus(object)pointsets.
We evaluate all methods by their test accuracy at the end
of the training instead of the highest accuracy during the
training [38]. In our experiments, the bottleneck size of
ADV[38]isreducedto512from2048forImageNet-Caltech
topreventtheout-of-memoryerror.
(a) mα=1 (b) mα=3 (c) mα=20 B.10 MoreDetailsinSec.6.2.3
WeprovidemoredetailsoftheresultofPWANonVisDa17
Fig.25.Thealignmentofα(blue)andβ θ(red)specifiedbyLM,1(α,β θ)
withm
β
=1andvaryingmα.Thesizeofeachpointisproportionalto in Fig. 26. We observe that PWAN generally performs
itsmass.Completealignment(mα=1)graduallybecomesthenearest well on all classes, except that it sometimes recognizes
neighborhoodalignment(mα=∞)asmαincreases.
the “knife” class as the “skateboard” classes as shown in
Fig.26(a)andFig.26(b).Thiserrorissomehowreasonable
becausethesetwoclassesarevisuallysimilarasshownini.e.,
- Office-Home:Thebatchsizeis65andweuseastratified
Fig.26(c)andFig.26(d).Wearguethatthisambiguitycan
sampler as in [81], i.e., a mini-batch contains 1 random
be easily addressed in real applications, as we can choose
samplefromeachreferenceclass.Weset(T,u,s)=(5×
thehigh-performancemodelbytestingthemodelonasmall
103,5,103/10000).
annotatedvalidationset.Inaddition,comparedtotheresults
- VisDa17: We use batch size 60 and a stratified sampler.
in Fig.8 in [11], PWAN achieves much better results, as it
(T,u,s)=(104,20,105/10000).
caneasilydiscriminateclasses“bicycle”/“motorcycle”and
- ImageNet-Caltech:Weusebatchsize100.(T,u,s)=(4.8×
“bus”/“car”/“train”.
104,1,101/48000).
The training process on ImageNet-Caltech dataset is
- DomainNet: We use batch size 100. (T,u,s) =
presentedinFig.27,wherebothcross-entropyandPWdiver-
(104,1,106/100000).Following[38],forthisdataset,wedo
genceconvergeduringthetrainingprocess.Inaddition,we
notuseReluactivationfunctionatthebottlenecklayer.
provideavisualizationofthelearnedfeaturesonImageNet-
Intheabovesettings,weselectthelargestpossiblebatchsize CaltechandDomainNetinFig.28,wherePWANsuccessfully
tofitintheGPUmemoryforeachtask,andsufficientlylong alignsthesourcefeaturestothereferencefeaturesevenwhen
training steps are chosen to make sure PWAN converges. thereferencedataisdominatedbyoutliers(68%ofclasses28
(c) Randomsamplesoftheknifeclassinthe
(a) TheresultofPWANwithrandomseed0.(Accuracy=94%)
sourcedomain.
(d) Randomsamplesoftheskateboardclass
inthereferencedomain.
(b) TheresultofPWANwithrandomseed1.(Accuracy=80%)
Fig.26. TheresultsofPWANonVisDa17.PWANsometimeshasdifficultydiscriminatingthe“skateboard”andthe“knife”classduetotheirvisual
similarity.Inpractice,thisissuecanbeeasilysolvedbymodelselectionusingasmallannotatedvalidationset.
Fig.27.ThetrainingprocessofPWANonImageNet-Caltech.PWloss
andcross-entropylossgraduallydecreaseuntilconvergence.Meanwhile,
testaccuracyincreasesduringthetrainingprocess.
(a) CP-DomainNet (b) ImageNet-Caltech
Fig. 28. t-SNE visualization of the learned features of PWAN. Blue
andgraypointsrepresentfeaturesinnon-outlierandoutlierclassesin
thereferencedomainrespectively.Redpointsrepresentfeaturesinthe
sourcedomain.
ofDomainNetand92%ofclassesofImageNet-Caltechare
outliers).