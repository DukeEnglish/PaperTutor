Flash STU: Fast Spectral Transform Units
Y.IsabelLiu* WindsorNguyen* YagizDevre
PrincetonUniversity PrincetonUniversity PrincetonUniversity
EvanDogariu AnirudhaMajumdar EladHazan
PrincetonUniversity/NYU† PrincetonUniversity PrincetonUniversity‡
September18,2024
Abstract
Thispaperdescribesanefficient,opensourcePyTorchimplementation1oftheSpectralTransformUnit[1](STU).
Weinvestigatesequencepredictiontasksoverseveralmodalitiesincludinglanguage,robotics,andsimulateddynamical
systems.Wefindthatforthesameparametercount,theSTUanditsvariantsoutperformtheTransformeraswellas
otherleadingstatespacemodelsacrossvariousmodalities.
1 Introduction
The Spectral Transform Unit (STU) was recently proposed in [1] based on the spectral filtering technique of [16].
This neural network architectural unit is motivated by state space models for linear dynamical systems. The key
innovationofspectralstatespacemodelsliesintheiruseoffixedconvolutionalfilterswhichdonotrequirelearning.
Thisstructureofferssignificantrobustnessintheoryastheperformanceofthemodelisnotinfluencedbythespectrum
oftheunderlyingdynamicsnorthedimensionalityoftheproblem,makingitsuitablefortasksthatrequirelong-term
memory.
In this paper we describe an open source PyTorch implementation of the STU and experiments as well as abla-
tionstudiestounderstanditsproperties. Westudyseveralsequencepredictionproblemsacrossvariousmodalities,
including synthetic time series generated from linear dynamical systems, robotics control sequences, and natural
languagesequences.
1.1 DescriptionoftheSpectralTransformUnit
IntheSTUarchitecture,theschematicofwhichisgiveninFigure1,theoutputisgeneratedasatransformationofthe
inputsequencethatinvolves(optional)liftingoftheinputdimensionbyalearnedtransformationtoahigherdimension,
convolutionwithasetoffixedfilters(i.e. spectralfiltering),projectionwithasetoflearnedparameters,and(optional)
learnednonlinearities. Wecanthuswrite
(cid:32) k (cid:33)
(cid:88)
yˆ =σ M ·⟨Φ ,u ⟩ ,
t i i t:t−L
i=1
whereM arefixedprojections,σisanonlinearity,andΦ arekfixedfiltersthatcanbecomputeda-priori,andfor
i 1:k
simplicitywedon’texplicitlywritetheliftinginthemathematicalexpression. ThefiltersΦ aretheeigenvectors
1:k
*Equalcontribution.Orderdeterminedalphabeticallybylastname.
†ed2719@nyu.edu
‡isabel.liu, minhtringuyen, yagiz.devre, ani.majumdar, ehazan,@princeton.edu
1https://github.com/windsornguyen/flash-stu/
1
4202
peS
71
]GL.sc[
2v98401.9042:viXraLifting
dmodel dmodel * k
Input
STU
dmodel
MLP + non-
dout
din filtering linearity Out
Projection
Multiply by k fixed filters
STU Block
Figure1:ThebasicarchitectureoftheSpectralTransformerUnit.
correspondingtothelargesteigenvaluesofthepositivesemidefinitematrixgivenbytheouterproductsofallimpulse
responsevectorsofagivenlengthL.2
(cid:90) 1
Z = µ µ⊤dα, µ =(1,α,...,αL).
α α α
α=0
Thesefilterscanbecomputera-prioriandstoredbeforetheactualsequenceisobserved.
An important speedup due to [1] is called the tensordot approximation, or STU-T, whereby the tensors M are
i
approximatedasatensoroftwomatrices. Inmoredetail,ifd isthedimensionoftheinputtokenembeddings,andd
in out
oftheoutputdimensions,thenthetensorsM havedimensionalityd ×k×d .
i in out
ThetensordotapproximationdecomposeseachM iasaproductofM i1×M i2withdimensionsM i1 ∈Rdmodel×k,M i2 ∈
Rdmodel×dout,wherekisthenumberoffilters. Thisallowsustoperformtheprojectionbeforetheconvolutionoperation,
andreducethecomputationalcomplexitybyapproximatelyafactorofk. However,theapproximationreducesthe
expressivityofthemodel,andthetheoreticresultsonthelearnabilityofmarginallystablelineardynamicalsystemsdo
nottriviallyapply. Inpractice,itsperformanceiscompetitive,andthecomputationaladvantageissignificant.
2 Experiments with synthetic data
WebeginourinvestigationofSTUphenomenawithsomesimpleyetrepresentativesynthetictasksthathavebecome
commonplace in the sequence modeling literature. In particular, we aim to understand the behavior of the STU
in environments with long memory and nonlinearities, especially as we introduce feed-forward layers and deeper
architectures. WecompareagainstS4[14],astandardSSMarchitecture,aswellasthevanillatransformerlayer[35].
2.1 Lineardynamicalsystems
Inalineardynamicalsystem(LDS)theoutputs{y }ofasequencearegeneratedaccordingtothedynamicsequations
t
x =Ax +Bu , y =Cx +Du ,
t+1 t t t t t
2Moreprecisely,therecouldbemoremultiplierstothematrixoftheform(1−α)or(1−α2),dependingontheprecisevariant,see[16,1,15].
2where{u }isan(observed)inputsequenceand{x }arehiddenstatesofthesystem. ThematricesA,B,C,D are
t t
called"systemmatrices"andareunknowntothesequencepredictor. Typicalapproachestolearninginthissetting
scaleincomplexitywithhiddenstatedimensiond andeffectivesystemmemory1/δ,whereρ(A)=1−δisthe
hidden
spectralradiusofA. Foranin-depthtreatmentoflineardynamicalsystemsaswellasmethodstolearnandpredict
them,seethetext[15].
The task of sequence prediction in this case is to predict yˆ given all the previous inputs and outputs u ,y
t+1 1:t 1:t
andtominimizethelossvs. theactualoutputy . WeevaluatethemeansquarederroroftheSTU’spredictionsvs.
t+1
othermethodswherethesequencewasgeneratedbyalineardynamicalsystem.
Figure2:Meansquarederror∥yˆ −y ∥2ofthediffer-
t+1 t+1
entlayersonasinglesequencefromanLDS.
Experimentdetailsandconclusions. Weapplyrandominputstolineardynamicalsystemswithrandomsystem
matriceswithd =d =5andhiddenstatedimensiond =256. ThetransitionmatrixAissymmetrized
input output hidden
andnormalizedsothatρ(A)=0.99togivethesystemaneffectivememoryof≈100,whichiswhatwesetthecontext
lengthto. Suchasettinghasthepropertiesof(1)longmemoryand(2)alargehiddenstate,whichwebelievetobe
majorsourcesofcomplexityinreal-worldapplications.
We compare against a standard attention layer, with 8 heads and the ALiBi [25] positional embedding, and a di-
agonalS4layer[14]withhiddendimensionequaltod . Alllayershaveawidthof32andaretrainedfor5,000
hidden
stepswiththeRMSPropoptimizer. ResultsareplottedinFigure2witherrorbarsover16trials.
We see that both the vanilla STU and its approximate version STU-T are able to reliably achieve small loss in
thissettingwithquickandrobustconvergence,whiletheperformancesoftheothermethodsvaryacrossrandomseeds.
NotethatthewidthoftheSTUlayerdoesnotneedtobeaslargeasd toperfectlycapturethedynamics,whichis
hidden
consistentwiththeory. Furthermore,theSTU-TapproximationisabletoroughlymatchvanillaSTU’sperformance
evenonthesemulti-inputmulti-output(MIMO)systems. ThisexperimentconfirmsthattheSTUlayerisapowerful
androbustSSMprimitive.
2.2 Optimizationbehavior
WesawonthelineardynamicalsystemthattheSTUlayersseemtohaveacomparativelyeasiertimeoptimizing. This
isexpectedsinceasingleSTUlayerunderMSElossisaconvexparameterization,whereasthelossesofanS4and
anattentionmodelarenonconvex3. Following[20],wechoosetworandomdirectionsinthehigh-dimensionalspace,
movealongthesedirectionsbyvaryingamountsx_stepandy_step,computethelossofthemodelwithperturbed
parametersforeachcoordinatepairinthesedirections,andplotthelossvaluesasheightsona2Dgrid. Bydoingthis,
3ForS4,Mamba[13],andanyothermodelthatparameterizesanLDSdirectly,thisnonconvexityevengrowswithsequencelength.
3weareabletogetasenseforthelocalgeometryofareduced-dimensionlosslandscape.
Figures 3-5 visualize these loss landscapes for the STU, S4, and attention layers, respectively, after 10 steps of
trainingontheLDS.Figures15-17inAppendixBinsteadvisualizethelocalgeometrythroughsharpnessinterms
ofHessianeigenvalueratios. Flatterminimaarepreferablesinceithasbeenproposedthatreducingsharpnesshelps
withgeneralization[10]. OneofthemainstrengthsoftheSTUisitscleanandsaddle-freeoptimizationlandscape;this
benefitbecomesmoreimportantinlargerandmorecomplexmodels.
Figure 3: Local loss landscape of the Figure4:LocallosslandscapeoftheS4 Figure 5: Local loss landscape of the
STUlayer. layer. attentionlayer.
2.3 Othersynthetictasks
WealsoinvestigatetheperformanceoftheSTUlayerinsynthetictasksthataremorewell-knowninthedeeplearning
literature. Webrieflyintroducetheinductionheads[13]andassociativerecall[11]tasks, ourexperimentalsetups,
andthecorrespondingresultshere. Forcompleteness,weincludeinAppendixBtheresultsonothertasksincluding
selectivecopy[3]andanovelmodepredictiontaskthatmaybeindependentlyuseful.
In the induction heads task, the model is required to memorize one token (sampled uniformly from a vocabulary)
immediatelyafteraspecialflagtoken;therestofthecontextconsistsofthesamespecialblanktoken,whichthe
modelshouldlearntoignore. Theassociativerecalltaskisslightlydifferent,asitfirstgivesthemodelan(unordered)
sequenceofkey-valuepairsandthenasksforthevalueassociatedwithaquerykeyafter–themodelmustmemorizethe
entirecontext,notjustasingletoken. Bothofthesetaskshavenonlineardynamicsandrequiresingle-tokenprecisionto
fullysolve;furthermore,sinceweapplydeepermodels,theoptimizationsarenonconvexforallthemodelsconsidered.
Experimentdetailsandconclusions. Wetrainedtwo-layermodels(withMLPlayersinbetween)usingthecross-
entropyloss,theAdamoptimizerwithdefaultPyTorchparameters,andbatchesofsize32. Wesetthecontextlengthto
32andallthemodelwidthsto32. Inaddition,theattentionmodelhas8headsandtheS4modelhashiddendimension
64. For induction heads, the vocabulary size is 4. We plot the accuracies during training for induction heads and
selectivecopyinFigures6and7,respectively,averagedover8trialswitherrorbars.
Aswecansee,onbothtaskstheSTUisabletosolvethetaskquicklyandconsistently. Bycontrast,ontheinduction
headstaskweseethatSTU-TandtheattentionandS4baselinesappeartolearninstages,withtheaccuracyplateauing
occasionally. Furthermore,thereissignificantvariationacrossseedsforwhichtimesteptheattentionmodelbegins
tosolvethetask. Bothoftheseobservationspointtothedifficultyoftheunderlyingoptimizationproblem: whileall
modelsconsideredareexpressiveenoughtosolveinductionheads,theSTUappearstohaveaneasiertimefindingsuch
asolution.
Ontheassociativerecalltask,weseeasimilarstory. TheSSMmodelsallconvergeveryquickly,whiletheattention-
basedmodelrequiresmoreoptimizationstepsandlearnsmoregradually. Theseresultsareparticularlysurprising
consideringthatthetasksinvolvedfeeldifficulttoapproximatebylineardynamics,soonemighthaveexpectedthe
nonlinear,non-dynamicalstructureofsoftmaxattentiontodominatehere.
4Figure6: Predictionaccuracyforthetokenimmediately Figure7:Predictionaccuracyforthevaluecorresponding
followingthespecialflagtokenduringtraining tothegivenqueryduringtraining.
Overall, the optimization behavior of STU on these synthetic tasks give us confidence that it is a simple, easily
optimizable,yetexpressivelayerforuseinlargermodels,whichwillbethemainfocusfortherestofthepaper.
3 Experiments with robotics sequence prediction
Wenowconsidernext-statepredictionusingtheMuJoCophysicsengine[34],amorechallengingsequenceprediction
task. Thegoalistolearnthedynamicsofacertainphysicalsimulationagent,forexampletheAnt-v1system,
x =f(x ,u ), (1)
t+1 t t
wherex ∈R29andu ∈R8correspondtothestateandactionattimet,respectively.
t t
More precisely, x corresponds to the positions, orientations, joint angles, and velocities of the various limbs of
t
the Ant-v1 controller, whereas u represents the torques applied to the joints of the Ant-v1 controller and are
t
generated from a pretrained proximal policy optimization [29]. Unlike the synthetic linear dynamical system in
Section2,thedynamicsf forthisparticularMuJoCopredictiontaskarenonlinearandhybrid,i.e. non-smooth.
Intheexperimentsbelow,wedefinedthelossfunctiontobethesquaredEuclideandistancebetweenthepredictedstate
vectorxˆ andthetruestatevectorx
t+1 t+1
n
L(θ)=
1
∥xˆ (θ)−x ∥2 =
1(cid:88)
(xˆ (θ)−x )2,
2 t+1 t+1 2 2 t+1,i t+1,i
i=1
wherexˆ (θ)isthepredictedstatevectorparameterizedbyθ,andx isthetruestatevector.
t+1 t+1
Modelarchitectures. TheMixture-of-Experts(MoE)architecturehasseennewfoundresurgenceinmodernneural
networksthankstoitsabilitytoincreasemodelcapacitywithoutaproportionalincreaseincomputation[31]. Weran
smallablationstudiesandfoundthatasparselymixture-of-experts(MoE)overgatedMLPsafterthemodel’smain
sublayerperformedthebest. ThegatedMLPsplitstheinputsintotwoparts: amaincomponentanda"gate"that
modulatesthemaincomponentusingtheSiLU[28]activationfunction. ThegatingMoEnetworkthendynamically
selectswhichtopexpertstouseforeachinputbasedonalearneddistribution,whereeachexpertisagatedMLP,applies
thetop_kselectedexpert(s)totheinput,andcombinestheiroutputsusingaweightedsummation,wheretheweights
comefromthegatingmechanism. Figure20showsthemodelarchitecturesindetail. Table1givesthespecificationfor
5eachtestedarchitectureinthesetwosetsofexperiments.
We attempted to maintain the same parameter count 0.5M for each model given the various architectural differ-
enceswhicharepresentedinSectionAoftheappendix. ForSTU,Transformer,andMamba-2models,weensured
that they have the same widths and depths and, following [36], only adjusted the MLP expansion factor as the
primarymethodtoequalizetheparametercount. AsfortheSTU-Tmodel,itstensordotapproximationallowedfor
extraparameterspacethatcanbeallocatedtowardsotherareasinthemodel. Forexample,usingexactlythesame
configurationsasSTU,STU-Tonlyhas0.05Mparameters;toscaleitupforfairmodelcomparisons,herewechooseto
expandthe"width"ofSTU-T,whichisthemodeldimensiond inFigure1.
model
Experimentcontrollers. WetestedourmodelsonthreeMuJoCocontrollers: Ant-v1,HalfCheetah-v1,and
Walker2D-v1. The full results are in Section B of the appendix; we only provide results for Ant-v1 here for
illustration(Figure8andTable2). Wealsonotethattheresultspresentedherebyarepartial,afterhyperparameter
tuning. ThefullresultsoftheseablationstudiescanbefoundinSectionD.1.
Additionalexperiments. Tofurthervalidatetheresults,weconductnext-steppredictionandautoregressivenext-step
predictionusingthetrainedmodels(Figure9and10). Next-steppredictionusesground-truthstatesandactionsfrom
afixedinputwindowtoforecastthenextstate,mirroringthemodel’strainingevaluation. Autoregressivenext-step
prediction,however,incorporatesthemodel’sownpastpredictionsasinputs,illustratinghowerrorsaccumulateover
time.
Table1:ModelSpecificationsfortheAnt-v1Task.
Model Parameters Width/Depth MLPScale Filters MoE Time/Step
STU 0.55M 64/4 1 164 False 11.4ms
STU-T 0.47M 128/4 8 16 False 11.4ms
Transformer 0.51M 64/4 6 - True 21.3ms
Mamba-2 0.51M 64/4 4 - True 40.5ms
Table2:Ant-v1comparativevalidationlossresults.
Model ValidationLoss
STU 0.0181
STU-T 0.0092
Transformer 0.0237
Mamba-2 0.0139
Figure8:Ant-v1comparativetrainingresults.
4AblationstudiesontheperformanceofSTUusingdifferentnumberoffilters(K)showthatataroundK=15lossstopsdecayingexponentially
andplateaus[1].
6Figure10:Ant-v1comparativeautoregressivenext-steppre-
Figure 9: Ant-v1 comparative next-step prediction results
dictionresults(withlossesaveragedover500predictionsfor
(withlossesaveragedover500predictionsforeachmodel).
eachmodel).
3.1 Takeawaysfromroboticsexperiments
Ascanbeseenintheexperimentresults,onaparameter-adjustedbasis,theSTU-Toutperformsallothermodels.
Attentionvs. StateSpaceModels. IthasbeenobservedthatTransformersstrugglewithtasksrequiringtherecall
oflong-rangedependencies[32],whereasstatespacemodels(SSMs)excelinthisaspect. Transformers’quadratic
complexityinsequencelength[35]contrastswithSSMs’linearcomplexityandabilitytotheoreticallycaptureinfinitely
longdependencies[14]. Mamba[13],arecentSSMvariant,introducesselectivestatespacesthatdynamicallyadaptto
inputsequences,achievingstate-of-the-artperformanceacrossvarioustasks. Inourstudy,Mamba-2indeedsignificantly
outperformsTransformerinMuJoCoroboticssequencepredictiontasks. Moreexcitingly,ourSTU-Tperformseven
betterthanMamba-2(andfaster),demonstratingitspotentialinpushingtheboundariesofperformanceandefficiency.
Tensordotapproximationeffect. Asnotedintheintroduction,thetensordotapproximationofSTU-Tmodelsisa
significantoptimizationinbothcomputationalandmemoryefficiency. WefoundthatSTU-Tmodelsaremuchsmaller
yethaveperformanceonpar,andatmosttimesexceeding,thatoffullSTUmodels. Moreover,theparametersavings
duetothetensordotapproximationcanbereallocatedtowardsotherareasofthemodel. ThismakesSTU-Tmodelsour
bestperformingvariantgivenequalizedparametercounts,beatingstate-of-the-artMamba-2modelinallthreeMuJoCo
tasks.
Mixture-of-STUs. WefoundthatformulatingourMLPlayerintheSwiGLUconfiguration[30]improvesperformance
inSTUmodels;however,formulatingtheMLPlayerasasparsely-gatedmixture-of-SwiGLUshurtsperformance. We
envisionthereisaripefieldofresearchinarchitecturaldesignfortheSTUwaitingtobeexplored,e.g. adaptiveonline
methodsandvariantsofMoE[15],whichweleavetofuturework.
Widthvs. Depth. TheSTUmodelbenefitsmorefromincreasingmodeldepth,whereastheTransformermodelis
moresensitivetoincreasesinmodelwidth. Whenmodelwidthisheldconstant,weobservethattheTransformer’s
performancegainsstagnateearlywithrespecttodepth—particularlyafterjusttwolayers. Wesuspectthisisrelated
tothe’inductionheads’phenomenonobservedinattention-basedarchitectures[23]. Inductionheadsarespecialized
attentionmechanismsthatenablethemodeltorecognizeandreplicatepatternsintheinputdata,facilitatingin-context
learning. TheseheadstypicallyemergewithinthefirsttwolayersofaTransformer,providingthebulkofitslearning
capabilities. Asaresult,additionallayerscontributediminishingreturns,sincethecorefunctionalitiesarealready
establishedearlyon. DespitetheapparentabsenceofinductionheadcircuitsintheSTU,itstillperformswellonthe
MuJoCotask,outperformingtheTransformer.
7Noise. Weintroducedzero-meanGaussiannoisewithstandarddeviationdefinedbythenoiseparameterapplied
withprobabilitypateachtimestep. FromTable7,8,and9,wecanseethattheSTUismoreresistanttosuchnoise
comparedtotheTransformer,suggestingthattheSTUcouldbelesspronetooverfitting.
4 Experiments with language modeling
Inthissection,weexploretheSTU’sabilityforsequencepredictioninthecontextoflanguagemodeling. Wemodel
ourarchitectureinthestyleofGPT-2[26],andweopensourceasimple,fullydistributedlargelanguagemodel(LLM)
pretrainingpipeline[22]forthecommunitytoenjoyandbuildupon.
4.1 Experimentsetup
Data. Wepretrainonroughly10Bhigh-qualitytokensfromFineWeb-Edu[24],alarge-scale,opensourcedatasetfor
languagemodeling. Wetokenizedthedatasetinto95trainingshardsand1validationshard,eachcontainingabout
100Mtokens,usingtheo200k_basetokenizerfromtheOpenAI tiktoken5 library.
Generaldesignchoices. Foreachmodel,weusedRMSNorm[38]topre-normalizetheinputsbeforeeachattention
layer,FlashSTUlayer,andMLPlayer. Wefollowedthestandardpre-normresidualconnectionpatternaroundeachof
thesublayers,i.e. theoutputofeachsublayerisx + Sublayer(RMSNorm(x)). Tofurtherstabilizetraining,we
cappedlogits[4,37,33]ineachattentionlayerat50.0. FollowingNanoGPT,weroundedupthevocabularysizeof
eachmodeltothenearestmultipleof64inordertousemoreefficientCUDAkernelpathways. Wetiedembeddings,
andwedidnotusedropout.
Transformer architecture. We followed the GPT-2-styled Transformer from NanoGPT [19]. We added small
optimizationssuchasFlashAttention-2[7]andtheALiBi[25]modificationtotheattentionscores. Weusedposition
interpolation[6]tohelpthemodel"lengthgeneralize"beyonditscontextwindowsizefromtraining. Ithasbeenshown
thatpositioninterpolationworksevenwithALiBi[2],soweusedaninterpolationfactorof0.25,extendingthemodel’s
effectivecontextwindowsizeatinferencetimebyafactorof4. WeusedhighlyoptimizedTriton6kernels[18]forthe
implementationofourMLP(SwiGLU),RMSNorm,andlossfunction.
FlashSTUarchitecture. WeaugmentedtheSTU-TmodelwithFlashFFT[12]forefficientconvolutions. Wefound
thatthetensordotapproximationwasnecessarytoscaleupSTU-basedmodels,asitwasdifficulttoscalebeyond1B
parameterswithoutexperiencingfrequentout-of-memory(OOM)errors. Followingtheworkfromthestatespace
literature [8, 36] relating to the role of attention layers, we opted to use a simple hybrid model architecture with
alternatinglayersconsistingofSTU-Tandlocalattention,bothfollowedbyanMLPlayer. Weusedthesameoptimized
TritonkernelsfromtheTransformer,andwereplacedglobalattentionwithslidingwindowattention[5].
5https://github.com/openai/tiktoken/
6https://openai.com/index/triton/
8FlashSTUModelArchitecture
Input
RMSNorm
STU-T/SWA†
+
×N
RMSNorm
MLP
+
Output
Figure11:FlashSTUModelArchitecture,alternatingbetweenSTU-Tand(slidingwindow)attention†.
Training. WeusedfusedAdamW[21]withdefaultPyTorchhyperparameters,andwesetthemaximumlearning
rateandminimumlearningrateto3.0×10−4and3.0×10−5,respectively,per[17,27]. Wefollowedalineardecay
withwarmuplearningrateschedule[9]andallocated10%ofthetotalnumberoftrainingstepstowardswarmingup
thelearningratetothemaximumlearningratebeforelinearlydecayingbackdowntotheminimumlearningforthe
remainderofthetrainingrun. Forbettermemoryefficiency,weaccumulatedgradientsandperformedgradientupdates
onlyafterevery8trainingsteps. Eachmodelwastrainedfor16hoursacross8×H10080GBHBM3GPUsusingthe
FullyShardedDataParallel(FSDP)frameworkfromPyTorch[39].
4.2 Results
Table3summarizesthekeyperformancemetricsforbothmodels. WefoundthattheFlashSTUmodeloutperformed
theTransformermodelintermsofvalidationlossandperplexity. SampleautoregressivetextgenerationfromtheFlash
STUmodelcanbefoundinAppendixE.Forcompleteness,weincludedtheperformancesofthetwovariantsofSTU-T
aswellinFigure27.
9Table3:PerformanceComparisonofFlashSTUandTransformerModels.
Model ValidationLoss Perplexity MeanTime/Step
FlashSTU 3.09 22.08
≈3.0s
Transformer 3.18 24.11
Figure12:ComparisonofFlashSTUandTransformermodelsonvalidationset.
Qualitatively,wefoundthattheTransformerwasmoresensitivetohyperparametertuning,e.g. learningrate. Addition-
ally,theTransformerwasmorepronetospikesinitslosscomparedtoFlashSTUdespiteourbestattemptstostabilize
training.
We also note that although our Flash STU implementation is fairly optimized, it is not yet at the level at which
thecommunityhasoptimizedtheTransformermodelinrecentyears. AlthoughFlashSTUhastheedgeintermsof
asymptotictimecomplexity,theTransformerisheavilyoptimizedformatrixmultiplicationunitsonmodernaccelerator
hardware,whichgenerallytranslatestosuperiorwall-clockperformanceinpractice. However,wenotablyfoundthat
theaveragetimeperstepwasapproximatelyaroundthesameforbothmodels. Thus,webelievethattheFlashSTU
modeldemonstratesgreatpromiseforfutureimprovementsgiventhatitoutperformedtheTransformerwhilehavingan
asymptoticallybettertimecomplexity.
4.3 Finetuningfilters
Thecorebuildingblockofthespectralstatespacemodelisthespectralfilter,describedinmoredetailinAppendix A.
SlightlydeviatingfromtheoriginallyproposedSpectralStateSpacemodelarchitecture,werananablationtrainingrun
whereweinitializedtheK filtersfromtheHankelmatrixperusualfollowingthetheoryofspectralfilteringbutmade
thefiltersateachlayertobelearnableparameterswithalearningrateequivalenttothatofthemainmodelarchitecture.
Interestingly enough, this slightly improved the performance of Flash STU, but we do not yet make any strong
claimsaboutthisobservationandleavethistofutureworkunderamorerigorousexperimentationframework.
10Table4:PerformanceComparisonofFlashSTUandTransformerModels.
Model ValidationLoss Perplexity MeanTime/Step
FlashSTU 3.00 20.11
≈3.0s
Transformer 3.18 24.11
Figure13:FinetuningtheFlashSTUspectralfiltersimprovesitsperformancevs.Transformerover10Btokens.
5 Conclusions and future directions
Thispaperpresentedanexperimentaloverviewofthenewtheoretically-foundeddeepneuralarchitectureofspectral
transformers[1]onthreedifferentmodalities: syntheticdata,roboticscontrolsequences,andlanguagemodeling. We
findthattheSTUisaviablealternativerelativetootherpopularsequencemodels,notablytheTransformerandvarious
otherstatespacemodels. Ourexperimentswerecoupledwithhighlyoptimizedopensourcesoftwareavailableat[22].
The theoretical guarantees of the STU, notably its ability to handle large context efficiently, is yet to be investi-
gatedthoroughly,whichweleaveforfuturework.
6 Acknowledgements
WethankNamanAgarwal,XinyiChen,andDanielSuoforhelpfulconversations. WindsorNguyen,IsabelLiu,Yagiz
DevreandEvanDogariuwerefundedinpartbyEladHazan’sgrantsfromtheOfficeofNavalResearchandtheNational
ScienceFoundation. WewouldliketogivespecialthankstoPrincetonResearchComputing7andPrincetonLanguage
andIntelligence8(PLI)forsupplyinguswiththecomputerequiredformakingourexperimentspossible.
7https://researchcomputing.princeton.edu/
8https://pli.princeton.edu/
11References
[1] N.Agarwal,D.Suo,X.Chen,andE.Hazan. SpectralStateSpaceModels. arXivpreprintarXiv:2312.06837,
2023.
[2] F.Al-Khateeb,N.Dey,D.Soboleva,andJ.Hestness. PositionInterpolationImprovesALiBiExtrapolation,2023.
[3] M.Arjovsky,A.Shah,andY.Bengio. UnitaryEvolutionRecurrentNeuralNetworks. InProceedingsofThe33rd
InternationalConferenceonMachineLearning,pages1120–1128,2016.
[4] I.Bello,H.Pham,Q.V.Le,M.Norouzi,andS.Bengio. NeuralCombinatorialOptimizationwithReinforcement
Learning,2017.
[5] I.Beltagy,M.E.Peters,andA.Cohan. Longformer: TheLong-DocumentTransformer,2020.
[6] S.Chen,S.Wong,L.Chen,andY.Tian. ExtendingContextWindowofLargeLanguageModelsviaPositional
Interpolation,2023.
[7] T.Dao,D.Y.Fu,S.Ermon,A.Rudra,andC.Ré. FlashAttention: FastandMemory-EfficientExactAttention
withIO-Awareness,2022.
[8] T.DaoandA.Gu. TransformersareSSMs: GeneralizedModelsandEfficientAlgorithmsThroughStructured
StateSpaceDuality,2024.
[9] A.Defazio,A.Cutkosky,H.Mehta,andK.Mishchenko. When,WhyandHowMuch? AdaptiveLearningRate
SchedulingbyRefinement,2023.
[10] P.Foret,A.Kleiner,H.Mobahi,andB.Neyshabur. Sharpness-AwareMinimizationforEfficientlyImproving
Generalization,2020.
[11] D.Y.Fu,T.Dao,K.K.Saab,A.W.Thomas,A.Rudra,andC.Ré. HungryHungryHippos: TowardsLanguage
ModelingwithStateSpaceModels,2023.
[12] D.Y.Fu,H.Kumbong,E.Nguyen,andC.Ré. FlashFFTConv: EfficientConvolutionsforLongSequenceswith
TensorCores,2023.
[13] A.GuandT.Dao. Mamba: Linear-TimeSequenceModelingwithSelectiveStateSpaces,2024.
[14] A.Gu,K.Goel,andC.Ré. EfficientlyModelingLongSequenceswithStructuredStateSpaces,2022.
[15] E.HazanandK.Singh. IntroductiontoOnlineNonstochasticControl. arXivpreprintarXiv:2211.09619,2022.
[16] E.Hazan, K.Singh, andC.Zhang. LearningLinearDynamicalSystemsviaSpectralFiltering. Advancesin
NeuralInformationProcessingSystems,30,2017.
[17] J.Hoffmann,S.Borgeaud,A.Mensch,E.Buchatskaya,T.Cai,E.Rutherford,D.deLasCasas,L.A.Hendricks,
J.Welbl,A.Clark,T.Hennigan,E.Noland,K.Millican,G.vandenDriessche,B.Damoc,A.Guy,S.Osindero,
K.Simonyan,E.Elsen,J.W.Rae,O.Vinyals,andL.Sifre. TrainingCompute-OptimalLargeLanguageModels,
2022.
[18] B.Hsu. Liger-Kernel. https://github.com/linkedin/Liger-Kernel,2024.
[19] A.Karpathy. build-nanogpt. https://github.com/karpathy/build-nanogpt,2024.
[20] H.Li,Z.Xu,G.Taylor,C.Studer,andT.Goldstein. VisualizingtheLossLandscapeofNeuralNets,2017.
[21] I.LoshchilovandF.Hutter. DecoupledWeightDecayRegularization,2019.
12[22] W. Nguyen and contributors. Flash STU Repository. https://github.com/windsornguyen/
flash-stu,2024.
[23] C.Olsson,N.Elhage,N.Nanda,N.Joseph,N.DasSarma,T.Henighan,B.Mann,A.Askell,Y.Bai,A.Chen,
T.Conerly,D.Drain,D.Ganguli,Z.Hatfield-Dodds,D.Hernandez,S.Johnston,A.Jones,J.Kernion,L.Lovitt,
K.Ndousse,D.Amodei,T.Brown,J.Clark,J.Kaplan,S.McCandlish,andC.Olah. In-contextLearningand
InductionHeads,2022.
[24] G.Penedo,H.Kydlícˇek,L.B.allal,A.Lozhkov,M.Mitchell,C.Raffel,L.V.Werra,andT.Wolf. TheFineWeb
Datasets: DecantingtheWebfortheFinestTextDataatScale,2024.
[25] O.Press,N.A.Smith,andM.Lewis. TrainShort,TestLong: AttentionwithLinearBiasesEnablesInputLength
Extrapolation,2022.
[26] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskever,etal. Languagemodelsareunsupervisedmultitask
learners. OpenAIBlog,1(8):9,2019.
[27] J.W.Rae,S.Borgeaud,T.Cai,K.Millican,J.Hoffmann,F.Song,J.Aslanides,S.Henderson,R.Ring,S.Young,
E.Rutherford,T.Hennigan,J.Menick,A.Cassirer,R.Powell,G.vandenDriessche,L.A.Hendricks,M.Rauh,
P.-S.Huang,A.Glaese,J.Welbl,S.Dathathri,S.Huang,J.Uesato,J.Mellor,I.Higgins,A.Creswell,N.McAleese,
A.Wu,E.Elsen,S.Jayakumar,E.Buchatskaya,D.Budden,E.Sutherland,K.Simonyan,M.Paganini,L.Sifre,
L.Martens,X.L.Li,A.Kuncoro,A.Nematzadeh,E.Gribovskaya,D.Donato,A.Lazaridou,A.Mensch,J.-B.
Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama,
C.deMassond’Autume,Y.Li,T.Terzi,V.Mikulik,I.Babuschkin,A.Clark,D.deLasCasas,A.Guy,C.Jones,
J.Bradbury,M.Johnson,B.Hechtman,L.Weidinger,I.Gabriel,W.Isaac,E.Lockhart,S.Osindero,L.Rimell,
C.Dyer,O.Vinyals,K.Ayoub,J.Stanway,L.Bennett,D.Hassabis,K.Kavukcuoglu,andG.Irving. Scaling
LanguageModels: Methods,Analysis&InsightsfromTrainingGopher,2022.
[28] P.Ramachandran,B.Zoph,andQ.V.Le. SearchingforActivationFunctions,2017.
[29] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov. ProximalPolicyOptimizationAlgorithms,2017.
[30] N.Shazeer. GLUVariantsImproveTransformer,2020.
[31] N.Shazeer,A.Mirhoseini,K.Maziarz,A.Davis,Q.Le,G.Hinton,andJ.Dean. OutrageouslyLargeNeural
Networks: TheSparsely-GatedMixture-of-ExpertsLayer,2017.
[32] Y.Tay,M.Dehghani,S.Abnar,Y.Shen,D.Bahri,P.Pham,J.Rao,L.Yang,S.Ruder,andD.Metzler. Long
RangeArena: ABenchmarkforEfficientTransformers,2020.
[33] G.Team,M.Riviere,S.Pathak,P.G.Sessa,C.Hardin,S.Bhupatiraju,L.Hussenot,T.Mesnard,B.Shahriari,
A.Ramé,J.Ferret,P.Liu,P.Tafti,A.Friesen,M.Casbon,S.Ramos,R.Kumar,C.L.Lan,S.Jerome,A.Tsitsulin,
N.Vieillard,P.Stanczyk,S.Girgin,N.Momchev,M.Hoffman,S.Thakoor,J.-B.Grill,B.Neyshabur,O.Bachem,
A.Walton,A.Severyn,A.Parrish,A.Ahmad,A.Hutchison,A.Abdagic,A.Carl,A.Shen,A.Brock,A.Coenen,
A.Laforge, A.Paterson, B.Bastian, B.Piot, B.Wu, B.Royal, C.Chen, C.Kumar, C.Perry, C.Welty, C.A.
Choquette-Choo, D. Sinopalnikov, D. Weinberger, D. Vijaykumar, D. Rogozin´ska, D. Herbison, E. Bandy,
E.Wang,E.Noland,E.Moreira,E.Senter,E.Eltyshev,F.Visin,G.Rasskin,G.Wei,G.Cameron,G.Martins,
H.Hashemi,H.Klimczak-Plucin´ska,H.Batra,H.Dhand,I.Nardini,J.Mein,J.Zhou,J.Svensson,J.Stanway,
J.Chan,J.P.Zhou,J.Carrasqueira,J.Iljazi,J.Becker,J.Fernandez,J.vanAmersfoort,J.Gordon,J.Lipschultz,
J.Newlan,J.yeongJi,K.Mohamed,K.Badola,K.Black,K.Millican,K.McDonell,K.Nguyen,K.Sodhia,
K.Greene,L.L.Sjoesund,L.Usui,L.Sifre,L.Heuermann,L.Lago,L.McNealus,L.B.Soares,L.Kilpatrick,
L.Dixon,L.Martins,M.Reid,M.Singh,M.Iverson,M.Görner,M.Velloso,M.Wirth,M.Davidow,M.Miller,
M. Rahtz, M. Watson, M. Risdal, M. Kazemi, M. Moynihan, M. Zhang, M. Kahng, M. Park, M. Rahman,
M. Khatwani, N. Dao, N. Bardoliwalla, N. Devanathan, N. Dumai, N. Chauhan, O. Wahltinez, P. Botarda,
P.Barnes,P.Barham,P.Michel,P.Jin,P.Georgiev,P.Culliton,P.Kuppala,R.Comanescu,R.Merhej,R.Jana,
13R.A.Rokni, R.Agarwal, R.Mullins, S.Saadat, S.M.Carthy, S.Perrin, S.M.R.Arnold, S.Krause, S.Dai,
S.Garg,S.Sheth,S.Ronstrom,S.Chan,T.Jordan,T.Yu,T.Eccles,T.Hennigan,T.Kocisky,T.Doshi,V.Jain,
V.Yadav,V.Meshram,V.Dharmadhikari,W.Barkley,W.Wei,W.Ye,W.Han,W.Kwon,X.Xu,Z.Shen,Z.Gong,
Z. Wei, V. Cotruta, P. Kirk, A. Rao, M. Giang, L. Peran, T. Warkentin, E. Collins, J. Barral, Z. Ghahramani,
R. Hadsell, D. Sculley, J. Banks, A. Dragan, S. Petrov, O. Vinyals, J. Dean, D. Hassabis, K. Kavukcuoglu,
C.Farabet,E.Buchatskaya,S.Borgeaud,N.Fiedel,A.Joulin,K.Kenealy,R.Dadashi,andA.Andreev. Gemma
2: ImprovingOpenLanguageModelsataPracticalSize,2024.
[34] E. Todorov, T. Erez, and Y. Tassa. MuJoCo: A physics engine for model-based control. In 2012 IEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems,pages5026–5033.IEEE,2012.
[35] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,L.Kaiser,andI.Polosukhin. Attention
IsAllYouNeed,2017.
[36] R.Waleffe,W.Byeon,D.Riach,B.Norick,V.Korthikanti,T.Dao,A.Gu,A.Hatamizadeh,S.Singh,D.Narayanan,
G.Kulshreshtha,V.Singh,J.Casper,J.Kautz,M.Shoeybi,andB.Catanzaro.AnEmpiricalStudyofMamba-based
LanguageModels,2024.
[37] xAI. Grok1openrelease. https://github.com/xai-org/grok-1,2024.
[38] B.ZhangandR.Sennrich. RootMeanSquareLayerNormalization,2019.
[39] Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer,
A.Desmaison,C.Balioglu,P.Damania,B.Nguyen,G.Chauhan,Y.Hao,A.Mathews,andS.Li. PyTorchFSDP:
ExperiencesonScalingFullyShardedDataParallel,2023.
14A Spectral filters
Following[1],weconstructspectralfiltersbyextractingthetopK eigenvectorsfromtheHankelmatrix:
2
Z(i,j)= , (2)
(i+j)3−(i+j)
or,alternatively,from
Z
(i,j)=(cid:0) (−1)i+j−2+1(cid:1)
·
8
(3)
L (i+j+3)(i+j−1)(i+j+1)
ThesetopK eigenvectorsarethenscaledbytheircorrespondingeigenvalues,eachraisedtothe 1-thpower. These
4
scaledeigenvectorsserveasourspectralfiltersandareusedtoconvolveinputsintothespectralbasis.
Figure14:Visualizationofthefirst20spectralfiltersusedinourSTU-basedmodels.
B Additional synthetic experiments
Extraexperimentaldetails. ThesyntheticexperimentswereallrunonasingleA100GPU,witheachtrialforeach
modeltakingaroundaminutetocomplete. TheSTUimplementationfollowedFigure1exactly,withthetensordot
approximationreplacingtheSTUlayerintheSTU-Timplementation. ThesoftmaxattentionbaselineisFlashAttention
[7], and for S4 we used the official implementation [14]. Wherever we introduce nonlinearities in the synthetic
experimentsitisalwaysReLU;weleaveacloseinvestigationofmultiplicativegatingnonlinearitieslikeGLUtofuture
work.
Localsharpnessoftheloss. InFigures15-17,thesharpnessvaluesofthelossHessianareplottedafter10stepsof
trainingontheLDStaskfromthemainpaper. Arationear1.0meansthecurvatureissimilarinalldirections,whilea
15rationear0.0meansthecurvatureisstrongerinonedirection. Weuseabsolutevaluebecauseweareinterestedinthe
magnitudeofthecurvature. Thecontourlinesoverlaidontheheatmapshowthelosslandscape,allowingustovisualize
howlocalconvexityrelatestotheoveralllosssurface. Blueareasrepresentmoresphericalcurvature,andredareas
representmoreelongatedcurvature. Movementalongthexandyaxescorrespondstolocalmovementinparameter
space.
ForSTU,weseeverysmoothbehaviorandanobviousdirectionofprogresstowardwell-conditionedareas,whereas
thelosslandscapeismorecomplicatedforS4andattention.
Figure 15: Heatmap of the ratio Figure 16: Heatmap of the ratio Figure 17: Heatmap of the ratio
|λ min/λ max| of (an estimate of a |λ min/λ max| of (an estimate of a |λ min/λ max| of (an estimate of a
dimensionality-reducedversionof)the dimensionality-reducedversionof)the dimensionality-reducedversionof)the
lossHessianforanSTUlayer. lossHessianforanS4layer. lossHessianforanattentionlayer.
Other tasks. For a comprehensive study of how the STU layer performs on different types of small sequence
predictionproblems,wealsoexperimentontheselectivecopytask[3]andpredictionofthemodeofthetokensinthe
context9. Theselectivecopytaskrequiresthemodeltorecall(inorder)asequenceofafixednumberoftokens(which
aresampleduniformlyfromavocabularyanddistributedrandomlythroughoutthecontextofotherwiseblanktokens).
Bycontrast,themodepredictiontasksimplyrequiresthemodeltooutputthemodeofasequenceoftokenssampled
randomlyfromavocabulary.
Speakingbroadly,anLDSrequiresastructuredaggregationofthecontext,whileinductionheadsrequiresonly
localunderstandingofthecontext. Modepredictionandselectivecopycombinetheseprinciples,testingamodel’s
abilitytosynthesizesingle-token-precisioninformationacrossthewholesequence. Trainingresultsarepresentedin
Figures18and19,respectively.
9Asfarasthewecantell,ataskinvolvingpredictionofthemodeofdiscretedatahasnotbeenusedtostudythebehaviorsofsequencemodelsin
anyexistingliterature.Wethinkitisanelegantwaytoprobeamodel’sabilitytononlinearlysynthesizeverysimplesingle-tokeninformation,asitis
clearhowthedifficultyoftheproblemscaleswithsequencelengthandvocabularysize.Itmayalsolenditselftostrongtheoreticalanalyseson
accountofitssimplicity,andthereisclearstructurewithwhichtostudygeneralizationtonewtokensorseqencelengths.
16Figure18:Predictionaccuracyforthemodeofthecontext Figure19:Predictionaccuracyforrepeatingthenon-blank
duringtraining. tokensinthecontextduringtraining.
Experimentdetailsandconclusions. Asbefore,wetraintwo-layermodelswithMLPlayerswithnonlinearitiesin
between. ThemodelsaretrainedforthenumberofstepsshownwiththeAdamoptimizerwithdefaultparameters. Both
tasksareconstructedwithasequencelengthof32,andformodepredictionweuseasmallvocabularyof5tokens.
Onmodeprediction,weseethesamesurprisingresultsasoninductionheadsandassociativerecall: SSMmodels
performlearnthequickest,withSTUleadingtheway. However,theS4modeldominatesonselectivecopy,while
STU-Tappearsabletomatchtheattentionbaseline. Allmodelseventuallyconvergetoperfectaccuracyandarealways
abletoexpressasolutionforthistask,butthistimeitisS4thatappearstohavetheeasiestoptimization.
ToseeempiricalresultsbetweenSTUandS4ontheLongRangeArenabenchmark,seeTable1of[1].
C Additional model architectures
Figure20showsthemodelarchitecturesusedfortheroboticsexperimentsinsection3. Weransmallablationstudies
andfoundthatglobalskipconnectionsoftentimesimprovedperformanceinourMuJoCoroboticsexperimentsetting.
Innon-hybridSTUarchitectures,wefoundthatperformancedecreaseswhenusinganMoEarchitecture,asdiscussed
in3.1,soweusedasimplegatedMLPinstead.
17Input
Positional
Input + Input
Embedding
RMSNorm RMSNorm RMSNorm
STU Attention Mamba-2
+ + +
×N ×N ×N
RMSNorm RMSNorm RMSNorm
MLP MoE* MoE*
+ + +
Output Output Output
*GatedExperts
(a)STU-basedarchitecture. (b)Attention-basedarchitecture. (c)Mamba-basedarchitecture.
Figure20:ComparisonofSTU-based,Attention-based,andMamba-basedmodelarchitectures.
D Additional experiments with robotics data
In this section we give the remaining experimental results over robotics data for the other two MuJoCo tasks:
HalfCheetah-v1andWalker2D-v1. ComparedtoAnt-v1(showninSection2inthemainpaper),thesetwotasksonly
involvemotionina2Dplane. Asaresult,thestaterepresentationsforHalfCheetah-v1andWalker2D-v1areinherently
lesscomplex,potentiallyaffectingtherelativeperformanceofdifferentmodelsonthesetasksintheory.
In this setting, the comparison of the models’ performances still remains consistent, with Transformer showing
theleastcompetitiveresults,Mamba-2demonstratingsignificantimprovementsoverTransformer,andSTU-Toutper-
formingbothTransformerandMamba-2. HalfCheetah-v1task’strainingresultsareshowninFigure21andTable
5,(auto-regressive)next-steppredictionsresultsinFigure22andFigure23. Walker2D-v1task’strainingresultsare
showninFigure24andTable6,(auto-regressive)next-steppredictionsresultsinFigure25andFigure26.
18Table5: HalfCheetah-v1comparativevalidationlossre-
sults.
Model ValidationLoss
STU 0.0119
STU-T 0.0076
Transformer 0.0157
Mamba-2 0.0081
Figure21:HalfCheetah-v1comparativetrainingresults.
Figure22:HalfCheetah-v1comparativenext-steppredic- Figure 23: HalfCheetah-v1 comparative auto-regressive
tionresults(withlossesaveragedover500predictionsforeach next-steppredictionresults(withlossesaveragedover500pre-
model). dictionsforeachmodel).
Table6:Walker2D-v1comparativevalidationlossresults.
Model ValidationLoss
STU 0.0112
STU-T 0.0062
Transformer 0.0134
Mamba-2 0.0066
Figure24:Walker2D-v1comparativetrainingresults.
19Figure26: Walker2D-v1comparativeauto-regressivenext-
Figure25:Walker2D-v1comparativenext-steppredictionre-
steppredictionresults(withlossesaveragedover500predic-
sults(withlossesaveragedover500predictionsforeachmodel).
tionsforeachmodel).
D.1 Hyperparametersforroboticsexperiments
Weconductedcomprehensiveablationstudiestoinvestigatetheimpactofvarioushyperparametersontheperformance
ofSTU,STU-T,andTransformermodelsinthecontextofroboticstasks. Thesestudiesexploretheeffectsofmodel
width,depth,andinputnoiseonthefinalMeanSquaredError(MSE)loss. Tables7,8,and9presenttheresultsofthese
experiments.
Table7:AblationstudiesforSTUmodels.
Model ParameterCount Width Layers Noise/Frequency MSELoss
STU 0.18M 32 4 0.0/0.0 0.0217
STU 0.63M 64 4 0.0/0.0 0.0139
STU 1.33M 96 4 0.0/0.0 0.0120
STU 2.30M 128 4 0.0/0.0 0.0108
STU 0.05M 32 1 0.0/0.0 0.0449
STU 0.09M 32 2 0.0/0.0 0.0306
STU 0.18M 32 4 0.0/0.0 0.0217
STU 0.27M 32 6 0.0/0.0 0.0203
STU 0.18M 32 4 0.0/0.0 0.0217
STU 0.18M 32 4 0.1/0.1 0.0357
STU 0.18M 32 4 0.5/0.1 0.0561
20Table8:AblationstudiesforSTU-Tmodels. Table9:AblationstudiesforTransformermodels.
STU-T 0.06M 32 4 0.0/0.0 0.0239 Transformer 0.07M 32 4 0.0/0.0 0.0472
STU-T 0.12M 64 4 0.0/0.0 0.0146 Transformer 0.17M 64 4 0.0/0.0 0.0294
STU-T 0.20M 96 4 0.0/0.0 0.0116 Transformer 0.30M 96 4 0.0/0.0 0.0214
STU-T 0.28M 128 4 0.0/0.0 0.0105 Transformer 0.47M 128 4 0.0/0.0 0.0204
STU-T 0.02M 32 1 0.0/0.0 0.0464 Transformer 0.02M 32 1 0.0/0.0 0.0545
STU-T 0.03M 32 2 0.0/0.0 0.0328 Transformer 0.04M 32 2 0.0/0.0 0.0464
STU-T 0.06M 32 4 0.0/0.0 0.0239 Transformer 0.07M 32 4 0.0/0.0 0.0472
STU-T 0.09M 32 6 0.0/0.0 0.0218 Transformer 0.10M 32 6 0.0/0.0 0.0462
STU-T 0.06M 32 4 0.0/0.0 0.0239 Transformer 0.07M 32 4 0.0/0.0 0.0472
STU-T 0.06M 32 4 0.1/0.1 0.0429 Transformer 0.07M 32 4 0.1/0.1 0.0637
STU-T 0.06M 32 4 0.5/0.1 0.0688 Transformer 0.07M 32 4 0.5/0.1 0.0961
NotethatthenoiselevelreferstothestandarddeviationofGaussiannoiseaddedtotheinputdata,whilethefrequency
represents the probability of applying this noise to each data point. For example, 0.1/0.1 means that, on average,
Gaussiannoisewithastandarddeviationof0.1isappliedto10%ofthedatapoints.
21E LLM experiments
Table10:ModelandTrainingConfigurationDetailsforLLMExperiments.
ModelArchitecture
Description FlashSTU Transformer
ParameterCount Totalnumberofparameters 2,672M 2,667M
EmbeddingDimension Dimensionalityofembeddingspace 1,536 1,536
NumberofHeads Attentionheads(notmulti-queriedormulti-grouped) 8 8
NumberofLayers Transformerlayers 26 25
ALiBiAttention Attentionscoresmodificationusinglinearbiases Yes(interpolationfactor:0.25) Yes(interpolationfactor:0.25)
SlidingWindowSize Slidingwindowattentioncontextlookbacksize 1,024 8,192
SequenceLength(Training) Inputsequencelengthduringtraining 8,192 8,192
SequenceLength(Inference) Inputsequencelengthduringinferenceviapositioninterpolation 32,768 32,768
VocabularySize Sizeofthemodel’svocabulary 200,064 200,064
MLPExpansionFactor ExpansionfactorinMLPlayers 12 12
Bias Useofbiastermsinlinearlayers No No
Dropout Dropoutrate 0.0 0.0
NumberofFilters Numberoffilters(FlashSTUonly) 24 –
UseHankelL AlternativeHankelmatrix(FlashSTUonly) No –
LearnableFilters Learnablefilters(FlashSTUonly) Yes –
TrainingandOptimization
Epochs Numberoftrainingepochs 1 1
GlobalBatchSize Numberoftokensprocessedperstep 524,288 524,288
MicroBatchSize BatchsizeperGPU 1 1
GradientAccumulationSteps Numberofstepsbeforeperformingagradientupdate 8 8
WarmupSteps Numberofwarmupsteps 1,907 1,907
EvaluationPeriod Evaluationfrequency(steps) 25 25
MaxGradNorm Maximumgradientnormforclipping 1.0 1.0
OptimizerConfiguration
Optimizer Optimizertype AdamW AdamW
LearningRateSchedule LRschedulingstrategy Lineardecaywithwarmup Lineardecaywithwarmup
MaxLearningRate Maximumlearningrate 3.0×10−4 3.0×10−4
MinLearningRate Minimumlearningrate 3.0×10−5 3.0×10−5
Betas Optimizerbetas (0.9,0.999) (0.9,0.999)
Epsilon Optimizerepsilon 1.0×10−8 1.0×10−8
WeightDecay Weightdecayfactor 1.0×10−2 1.0×10−2
AMSGrad UseAMSGradvariant No No
Fused Usefusedoptimizer Yes Yes
OptimizationTechniques
ActivationCheckpointing Enableactivationcheckpointing Yes Yes
UseFlashFFT EnableFlashFFT(FlashSTUonly) Yes –
UseTensordotApprox. EnableTensordotapproximation Yes –
UseAttention Enableattentionmechanism Yes Yes
Softcap Softcapthreshold 50.0 50.0
TorchCompile EnableTorchcompileoptimization No No
DistributedTrainingConfiguration
FSDP FullyShardedDataParallel Yes Yes
DDP DistributedDataParallel No No
MixedPrecision Usemixedprecisiontraining Yes Yes
TorchDtype DatatypeforPyTorchtensors bfloat16 bfloat16
CPUOffload OffloadcomputationstoCPU No No
ShardingStrategy Strategyformodelsharding FullShard FullShard
FSDPModules ModulestoapplyFSDP (STU,Attention,MLP) (Attention,MLP)
StateDictType Typeofstatedictionary Full Full
AutoWrapPolicy Policyforautomaticwrapping Partial Partial
BackwardPrefetch Backwardprefetchstrategy BackwardPre BackwardPre
ForwardPrefetch Forwardprefetch No No
SyncModuleStates Synchronizemodulestatesacrossreplicas Yes Yes
UseOriginalParams Useoriginalparametersduringtraining Yes Yes
22Figure27:ComparisonofSTU-T,FlashSTU,andTransformeronvalidationset.
TofurtherillustratethecapabilitiesoftheFlashSTUmodel,wepresentasampleoutputgeneratedusingatemperature
of1.0andtop_ksamplingwithk =50:
Starter prompt: "In a shocking finding, scientist discovered a herd of unicorns living in a remote,
previouslyunexploredvalley,intheAndesMountains. Evenmoresurprisingtotheresearcherswasthe
factthattheunicornsspokeperfectEnglish."
Generated output: "In a shocking finding, scientist discovered a herd of unicorns living in a remote,
previouslyunexploredvalley,intheAndesMountains. Evenmoresurprisingtotheresearcherswasthe
factthattheunicornsspokeperfectEnglish. Researchersnowbelievethattheseanimalscouldnowbea
kindofhominidspecies.
This discovery was recently made by scientists from the University of London. Previously, scientists
hadthoughtthatthisspeciesstoodonly5feet(1.2meters)for15minutes. Butthisnewdiscoverywasthe
firsttimeaunicorn’seyeswereeverrevealed.
It was a perfect choice to do a lot of research on the species. In the end, to this day, we still can’t
saywhytheunicornslivedinthisremoteworld-theywerejustanotherbigblackmonkey.Butthediscovery
provedthattheseunicornsusedwordsandwereabletocommunicatewithotherbeingsusingthesame
words.
The study, which involved researchers from both Washington State University and Cornell University,
confirmedthattheunicornfamilyhasexistedformorethan10,000years. Itwasalsothefirstthespecies
wasstudiedandstudiedinitsentirety.
’While there are also other rare wild animals like unicorns and hippopotami, most of these findings
werenotreported,butconfirmedbynumerousstudies. Inordertofindtheirwayoutofanendangered
speciesandfindthem,manyendangeredspeciesrequireconservationprogrammes,’saidleadresearcher,
DrThomasW.Huxley.
23One of the key findings of the study in this week’s issue is that the unicorns were able to walk for-
wardandbackwardacrosstheentirelandscape. Theirfrontlimbswereabletoreachallthewayacrossthe
samelandmassandinthesamedirectionastheirfrontlegswereableto. Theseuniquebacklimbshelped
theanimalstosurviveandthrive.
Other research has suggested that these unicorns have also been observed in the Andes Mountains.
ThisiswheretheresearchersbelievetheymayhavebeenabletofindtheirwaytotheAndes. Theareathey
foundistheAmazonValley.
The scientists believe that they came across an area that is part of the Amazon basin. In order to
findoutthetruenatureofthisarea,theresearchersusedsatelliteimageryfromNASAfortheirresearch.
The scientists also believe that the unicorns might now also be found in other parts of the world, in-
cludingNorthandSouthAmerica. Thesediscoveriesmighthelpscientistsstudyanunknownarea,suchas
theAmazoniaandtheAndes."
24