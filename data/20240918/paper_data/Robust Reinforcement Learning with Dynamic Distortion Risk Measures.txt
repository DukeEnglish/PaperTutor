ROBUST REINFORCEMENT LEARNING
WITH DYNAMIC DISTORTION RISK MEASURES
AnthonyCoache∗ SebastianJaimungal†
DepartmentofMathematics DepartmentofStatisticalSciences
ImperialCollegeLondon UniversityofToronto
& &
DepartmentofStatisticalSciences Oxford-ManInstituteofQuantitativeFinance
UniversityofToronto sebastian.jaimungal@utoronto.ca
a.coache@imperial.ac.uk http://sebastian.statistics.utoronto.ca/
https://anthonycoache.ca/
September17,2024
ABSTRACT
In a reinforcement learning (RL) setting, the agent’s optimal strategy heavily depends on her risk
preferences and the underlying model dynamics of the training environment. These two aspects
influencetheagent’sabilitytomakewell-informedandtime-consistentdecisionswhenfacingtest-
ing environments. In this work, we devise a framework to solve robust risk-aware RL problems
where we simultaneously account for environmental uncertainty and risk with a class of dynamic
robustdistortionriskmeasures.RobustnessisintroducedbyconsideringallmodelswithinaWasser-
stein ball around a reference model. We estimate such dynamic robust risk measures using neural
networksbymakinguseofstrictlyconsistentscoringfunctions,derivepolicygradientformulaeus-
ingthequantilerepresentationofdistortionriskmeasures,andconstructanactor-criticalgorithmto
solvethisclassofrobustrisk-awareRLproblems.Wedemonstratetheperformanceofouralgorithm
onaportfolioallocationexample.
Keywords Reinforcementlearning·Dynamicriskmeasures·Robustoptimization·Wassersteindistance
1 Introduction
Reinforcement learning (RL) is a model-agnostic framework for learning-based control. In brief, an agent observes
feedback from interactions with an environment, updates its current behavior according to its experience, and aims
to discover the best possible actions based on a certain criterion. RL provides an appealing alternative to model-
basedmethods;indeed,withRLvariousenvironmentalassumptionscomeatalowcomputationalcost,whilesolving
analyticallyfortheoptimalpoliciesmaybedifficultandareoftenintractableforcomplexmodels.Advancementswith
∗ACacknowledgessupportfromtheFondsderechercheduQuébec–NatureettechnologiesandOntarioGraduateScholarship
programs.
†SJ acknowledges support from the Natural Sciences and Engineering Research Council of Canada (NSERC) for partially
fundingthisworkthroughgrantsRGPIN-2018-05705andRGPIN-2024-04317.
4202
peS
61
]GL.sc[
1v69001.9042:viXraRobustRLwithDynamicDistortionRisk
neural network structures have paved the way to deep learning, which has shown a lot of success recently (see e.g.
Mnihetal.,2015;Silveretal.,2018;BrownandSandholm,2019;Berneretal.,2019).
DuringthetrainingphaseofRL,theagentattemptstodiscoverthebestpossiblestrategybyinteractingwithavirtual
representationoftheenvironment,usuallyasimulationengineorhistoricaldatawhenthestateprocessisexogenous,
i.e. actionsdonotaffectthedistributionofthestates. Therationalebehindthisapproachisthatdespitenotinteracting
with the intended environment, training experience should reflect events similar to those likely to occur during the
testing phase. Uncertainty in the real-world environment, however, may result in algorithms optimized on training
modelstoperformpoorlyduringtesting. Therefore,itiscrucialtoconsiderrobustifyingtheagent’sactions,thatisto
accountforinherentenvironmentaluncertaintyinsequentialdecisionmakingproblems.
Thereexistseveraldistancestoconstructuncertaintysetsandrobustifyoptimizationproblems,suchastheKantorovich
distance (see e.g. Pflug and Wozabal, 2007), Kullback-Leibler divergence (see e.g. Glasserman and Xu, 2014), dis-
tances originating from mass transportation (see e.g. Blanchet and Murthy, 2019), or the supremum over all risk
induced by Bayesian mixture probability measures (Cuchiero et al., 2022). In the literature, some researchers have
developed methodologies to account for model uncertainty in RL problems. Among others, Smirnova et al. (2019)
propose a distributionally robust risk-neutral RL algorithm, where the uncertainty set consists of all policies having
a Kullback-Leibler divergence within a given epsilon of a reference action probability distribution, Abdullah et al.
(2019)developarobustrisk-neutralRLmethod,wheretherobustnessisinducedbyconsideringalltransitionproba-
bilitiesinanWassersteinballfromareferencedynamicsmodel,andClavieretal.(2022)givearobustdistributional
RLalgorithmconstrainedwithaϕ-divergenceonthetransitionprobabilities.
Accounting for uncertainty in the testing environment is important, but ideally, risk must also simultaneously be
accounted for. Indeed, agents often want to follow a strategy that goes beyond “on-average” optimal performance,
especiallyinmathematicalfinanceapplicationswithlow-probabilitybuthigh-costoutcomes. Risk-awareRL,orrisk-
sensitive RL, aims to mitigate risk by replacing the expectation in the optimization problem with risk measures. It
alsoprovidesmoreflexibilitythantraditionalrisk-neutralapproaches,becausetheagentmaychoosethemeasureof
riskaccordingtoherowngoalsandriskpreferences–moreover,theagentmayuseriskmeasurestotradeoffriskand
reward.
Therearenumerousproposalsforoptimizingstaticriskmeasuresinsequentialdecisionmakingproblems. Onemain
issue with these works is that their proposed algorithms find optimal precommitment strategies, i.e., they result in
time-inconsistent strategies. In the more recent literature, many authors have attempted to overcome this issue with
risk measures adapted to a dynamic setting. Among others, Bäuerle and Glauner (2021) propose iterated coherent
riskmeasures,wheretheybothderiverisk-awaredynamicprogramming(DP)equationsandprovidepolicyiteration
algorithms, Ahmadi et al. (2021) investigate bounded policy iteration algorithms for partially observable Markov
decisionprocesses,KoseandRuszczynski(2021)provetheconvergenceoftemporaldifferencealgorithmsoptimising
dynamic Markov coherent risk measures, and Cheng and Jaimungal (2022) derive a DP principle for Kusuoka-type
conditionalriskmappings. Theseworks,however,requirecomputingthevaluefunctionforeverypossiblestateofthe
environment,limitingtheirapplicabilitytoproblemswithasmallnumberofstate-actionpairs.
ThearticlesclosestinspirittooursarethoseinJaimungaletal.(2022),Coacheetal.(2023),andBieleckietal.(2023).
First, Jaimungal et al. (2022) develop a deep RL approach to solve a wide class of robust risk-aware RL problems,
whereanagentminimizesthestaticworst-caserankdependentexpectedutilitymeasureofriskofallrandomvariables
withinacertainuncertaintyset. ItgeneralizestheapproachfromPesentiandJaimungal(2023),inwhichtheauthors
aim to find an optimal strategy, whose terminal wealth is distributionally close to a benchmark’s according to the
2-Wasserstein distance, minimizing a static distortion risk measure of the terminal P&L in a portfolio allocation
application. WuandJaimungal(2023)applytheapproachtorobustifypathdependentoptionhedging. Then,Coache
et al. (2023) design a deep RL algorithm to solve time-consistent RL problems where the agent optimizes dynamic
spectral risk measures. It builds upon the work from Coache and Jaimungal (2024) by exploiting the conditional
2RobustRLwithDynamicDistortionRisk
elicitabilitypropertyofspectralriskmeasurestoimprovetheirestimation,andMarzbanetal.(2023)whichfocuson
dynamic expectile risk measures. These ideas are also used in Jaimungal et al. (2023) for risk budgeting allocation
with dynamic distortion risk measures. Finally, Bielecki et al. (2023) derive dynamic programming equations for
risk-aversecontrolproblemswithmodeluncertaintyfromaBayesianperspectiveandpartiallyobservedcosts. This
approachsimultaneouslyaccountsforriskandmodeluncertainty,butrequiresfinitestateandactionspaces.
To the best of our knowledge, this paper bridges the gaps between those works, as it simultaneously accounts for
risk with dynamic risk measures and robustifies the actions against the uncertainty of the environment using the
Wassersteindistancewithintheone-stepconditionalriskmeasures. Ourcontributionsmaybesummarizedasfollows:
(i)weconsiderrobustrisk-awareRLproblemswithaclassofdynamicrobustdistortionriskmeasures;(ii)weanalyze
the worst-case distribution function of those dynamic robust distortion risk measures with uncertainty sets induced
by the conditional Wasserstein distance via their quantile representation; (iii) we derive a formula for computing
the gradient with respect to the policy parameters using the Envelope theorem for saddle-point problems; (iv) we
deviseadeepactor-criticstylealgorithmforsolvingthoseRLproblems,whichoptimizesadeterministicpolicyand
estimates elicitable functionals using strictly consistent scoring functions; and (v) we prove that, for a fixed policy,
there exists a neural network approximating the corresponding Q-function to any arbitrary accuracy. Moresco et al.
(2024) proves (under certain technical assumptions) that the form of robustification that we utilize indeed leads to
time-consistentoptimalstrategies,aswell,theyprovideanevenmoregeneralequivalencebetweentime-consistency
androbustdynamicriskmeasures.
The remainder of this paper is structured as follows. Section 2 summarizes the fundamental concepts to formally
define dynamic robust risk measures and their properties. We then introduce the class of RL problems and explore
their worst-case distributions for various dynamic robust distortion risk measures in Section 3. Section 4 presents
our developed actor-critic algorithm to solve those risk-aware RL problems, and Section 5 provides some universal
approximationtheoremresultsforourapproach. Finally,weillustratetheperformanceofourRLmethodologyona
portfolioallocationapplicationinSection6andexplainourwork’slimitationsandfutureextensionsinSection7.
2 RiskAssessment
In this section, we provide a brief overview of dynamic risk measures. There exist several classes of static risk
measures (see e.g. Föllmer and Schied, 2016, and the references therein), and various extensions to dynamic risk
measuresintheliterature(seee.g.AcciaioandPenner,2011;Bieleckietal.,2016). Here,webrieflysummarizethe
workofRuszczyn´ski(2010),whichderivesarecursiveequationfordynamicriskmeasuresusinggeneralprinciples,
andpresentaclassofone-stepconditionalrobustdistortionriskmeasureswiththeirproperties.
2.1 DynamicRisk
Whileonemayemploystaticriskmeasuresinsequentialdecisionmakingproblems,doingsooftenleadstoanoptimal
precommitment strategy, as static risk measures are not dynamically time-consistent risk measures – we discuss the
precisedefinitionbelow.Inessence,anoptimalstrategyplannedforafuturestateoftheenvironmentwhenoptimizing
a static risk measure may not be optimal anymore once the agent reaches this state. Therefore, we must adapt risk
assessmenttoadynamicframeworktoproperlymonitortheflowofinformation. Fortheremainingofthissection,we
followtheworkofRuszczyn´ski(2010).
Let T := {0,...,T} denote a sequence of periods, and define Z := L∞(Ω,F ,P) as the space of bounded F -
t t t
measurablerandomvariables. Weconsiderafiltration{∅,Ω}=:F ⊆F ⊆...⊆F ⊆F onafilteredprobability
0 1 T
space(Ω,F,{F } ,P). WealsodefineZ :=Z ×···×Z . Inwhatfollows,weassumethatZ ∈Z isa
t t∈T t1,t2 t1 t2 t+1
F -measurablerandomcostwithsupportonK∈R¯,conditionalcumulativedistributionfunction(CDF)
t+1
F (z):=P(Z ≤z |F )∈[0,1],
Z|Ft t
3RobustRLwithDynamicDistortionRisk
andconditionalquantilefunction
n o
F˘ (u):=inf z ∈K:F (z)≥u .
Z|Ft Z|Ft
Furthermore,fortheremainingofthepaper,allinequalitiesbetweensequencesofrandomvariablesaretobeunder-
stoodcomponent-wiseandinthealmostsuresense.
Definition2.1. Adynamicriskmeasureisasequenceofconditionalriskmeasures{ρ } ,whereρ isamap
t,T t∈T t1,t2
ρ :Z →Z foranyt ,t ∈T suchthatt <t .
t1,t2 t1,t2 t1 1 2 1 2
Themappingsρ (Z)maybeinterpretedasF -measurablechargesonewouldbewillingtoincurattimetinstead
t,T t
ofthesequenceofcostsZ. Wenextenumeratesomepropertiesofvariousdynamicriskmeasures–notallproperties
arerequired,butsome,suchasnormalization,monotonicity,andcashadditivity,arecrucialinvarioussettings.
Definition 2.2. Let Z,W ∈ Z , and β > 0, where β ∈ Z . A dynamic risk measure {ρ } is said to be the
t,T t t,T t∈T
followingifthestatementholdsforanyt∈T:
1. normalizedifρ (0,...,0)=0;
t,T
2. monotoneifZ ≤W impliesρ (Z)≤ρ (W);
t,T t,T
3. cashadditiveifρ (Z ,Z ,...,Z )=Z +ρ (0,Z ,...,Z );
t,T t t+1 T t t,T t+1 T
4. positivehomogeneousifρ (βZ)=βρ (Z);
t,T t,T
5. subadditiveifρ (Z+W)≤ρ (Z)+ρ (W);
t,T t,T t,T
6. comonotonicadditiveifρ (Z+W)=ρ (Z)+ρ (W)forallcomonotonicpairs(Z,W).
t,T t,T t,T
As in the static setting, a dynamic risk measure is called coherent if it satisfies the properties of monotonicity, cash
additivity,positivehomogeneityandsubadditivity.
Apivotal propertyofdynamicrisk measuresistheir time-consistency, toensurethat riskassessmentsoffuture out-
comesdonotresultincontradictionsovertime(seee.g.Cheriditoetal.,2006).
Definition2.3. Adynamicriskmeasure{ρ } issaidtobestronglytime-consistentiffforanysequenceZ,W ∈
t,T t∈T
Z andanyt ,t ∈T suchthat0≤t <t ≤T,
t1,T 1 2 1 2
ρ (Z )≤ρ (W ) and Z =W
t2,T t2,T t2,T t2,T t1,t2−1 t1,t2−1
impliesthatρ (Z )≤ρ (W ).
t1,T t1,T t1,T t1,T
Definition2.3maybeinterpretedasfollows: ifZ willbeatleastasgoodasW tomorrow(intermsofthedynamic
riskρ )andtheyareidenticaltoday(betweent andt ),then,allotherthingsbeingequal,Z shouldnotbeworse
t2,T 1 2
thanW today(intermsofρ ). Akeyresulttoderivearecursiverelationshipforstronglytime-consistentdynamic
t1,T
riskmeasuresisthefollowingcharacterisation(seeTheorem1ofRuszczyn´ski(2010)).
Proposition2.4. Let{ρ } beadynamicriskmeasuresatisfyingthenormalization,monotonicityandcashaddi-
t,T t∈T
tivityproperties. Then{ρ } istime-consistentiffforany0≤t ≤t ≤T andZ ∈Z ,wehave
t,T t∈T 1 2 0,T
(cid:16) (cid:17)
ρ (Z )=ρ Z ,...,Z ,ρ (Z ) .
t1,T t1,T t1,t2 t1 t2−1 t2,T t2,T
A time-consistent dynamic risk measure at any time t can be seen to include the sequence of costs up to a certain
1
timet ,andtheF -measurablechargefortheremainingfuturecosts. AsaconsequenceofProposition2.4,forany
2 t2
t∈T,wehavetherecursiverelationship
(cid:18) (cid:16) (cid:17) (cid:19)!
ρ (Z )=Z +ρ Z +ρ Z +···+ρ Z +ρ (Z ) ··· , (2.1)
t,T t,T t t t+1 t+1 t+2 T−2 T−1 T−1 T
4RobustRLwithDynamicDistortionRisk
wheretheone-stepconditionalriskmeasuresρ : Z → Z satisfyρ (Z) = ρ (0,Z)foranyZ ∈ Z . The
t t+1 t t t,t+1 t+1
one-step conditional risk measures may have stronger properties, e.g., be convex or coherent. Eq. (2.1) provides a
tractableexpressiontoworkwithforderivingdynamicprogrammingprinciplesinRLproblems.
Thefollowingclassofone-stepconditionalriskmeasures,definedthroughChoquetintegralsandintroducedbyYaari
(1987)inastaticsetting,subsumesmanyriskmeasurescommonlyusedintheliterature.
Definition 2.5. Let ν : F × Ω → [0,1] be a regular conditional distribution of Z given F , i.e. ν(·,ω) is a
t+1 t
probabilitymeasureforanyω ∈ Ωandν(z,·)isF -measurableforanyz ∈ F . Aone-stepconditionaldistortion
t t+1
riskmeasureρg :Z →Z isdefinedas
t t+1 t
Z 0 (cid:16) (cid:17) Z ∞ (cid:16) (cid:17)
ρg(Z)(ω):=− 1−g 1−F (z)(ω) dz+ g 1−F (z)(ω) dz, fora.e. ω ∈Ω,
t Z|Ft Z|Ft
−∞ 0
where g : [0,1] → [0,1], called distortion function, is F -measurable, nondecreasing and such that g(0) = 0 and
t
g(1)=1. Alternatively,ifgisabsolutelycontinuous,itadmitstherepresentation
ρg t(Z)(ω)=E(cid:20) Z γ(cid:16) F Z|Ft(Z)(cid:17)(cid:12) (cid:12) (cid:12) (cid:12)F t(cid:21) (ω)=Z 1 γ(u)F˘ Z|Ft(u)(ω)du, fora.e. ω ∈Ω,
0
whereγ : [0,1] → R satisfiesγ(u) = ∂ g(x)| (andhenceR γ(u)du = 1)and∂ denotesthederivative
+ − x=1−u [0,1] −
fromtheleft(seeTheorem6inDhaeneetal.,2012).
Remark2.6. Iftheσ-algebraF istrivial,i.e.F ={∅,Ω},thenρg(Z)inDefinition2.5becomesthestaticdistortion
t t t
riskmeasureofarandomcostZ.Wesuppressthedependenceonωforone-stepconditionalriskmeasuresinthesequel
forreadability. ◁
BythepropertiesofChoquetintegrals,suchone-stepconditionaldistortionriskmeasuresarecashadditive,monotone,
positivelyhomogeneousandcomonotonicadditive.Suchriskmeasuresallowforrisk-averse,risk-seeking,orpartially
risk-averseandrisk-seekingattitudesbyjudiciouslychoosingthedistortionfunctiong (orγ). Furthermore,thereare
several relationships between (static) distortion risk measures and commonly used risk measures in the literature.
Indeed, for positive random variables, a distortion risk measure is coherent if and only if the distortion function g
is nondecreasing and concave (Wirch and Hardy, 2001). Similar relationships between distortion and spectral risk
measuresmaybefoundinGzylandMayoral(2008).
In practice, there is often uncertainty on distribution of the random costs Z, and therefore we propose to robustify
riskmeasuresbyusingtheworst-caseriskofarandomvariableW thatlieswithinanϵ-WassersteinballofZ. Such
robustification has desirable properties and is an important line of research in financial risk management. Indeed,
recently,Bernardetal.(2023)havederivedexplicitboundsforstaticdistortionriskmeasureswhentherandomvari-
able’s distribution has known first two moments and lies within a 2-Wasserstein ball from a reference distribution,
and Moresco et al. (2024) have investigated the inclusion of uncertainty sets within dynamic risk measures from a
verygeneralperspective. Oneoftheirresultsshowsthat,undersuitablemildassumptions,consideringuncertaintyon
theentirestochasticprocessisequivalenttoconsideringone-stepuncertaintysets. Inourwork,weusepreciselythis
one-stepuncertaintyballformulation.
Definition2.7. TheconditionalWassersteindistanceoforder2betweentworandomvariablesZ,W ∈Z isgiven
t+1
by
Z
d2(Z,W):= inf (z−w)2χ(dz,dw),
t
χ∈Π(FZ|Ft,FW|Ft) R2
whereΠ(F ,F )isthesetofallbivariateprobabilitymeasureswithmarginalsF andF . Fordistri-
Z|Ft W|Ft Z|Ft W|Ft
butionsontherealline,itmaybesimplifiedto
Z 1(cid:16) (cid:17)2
d2(Z,W)= F˘ (u)−F˘ (u) du.
t Z|Ft W|Ft
0
5RobustRLwithDynamicDistortionRisk
AstheconditionalWassersteindistanceinDefinition2.7dependsonlyonquantilefunctions,wemaydenoteitinthe
sequelasd (F˘ ,F˘ ) = ∥F˘ −F˘ ∥,where⟨f,g⟩ = R f(u)g(u)duistheL2-innerproductbetween
t Z|Ft W|Ft Z|Ft pW|Ft [0,1]
tworealfunctionsf,gon[0,1]and∥f∥= ⟨f,f⟩itsL2-norm.
Definition2.8. Arobustone-stepconditionalmeasureofriskρ :Z →Z undertheuncertaintysetφϵ :Z →
t t+1 t t+1
2Zt+1 withtoleranceϵ≥0isdefinedas
ϱϵ(Z):=esssup ρ (Zϕ).
t t
Zϕ∈φϵ
Z
Inthispaper,weconsiderthefollowingtwouncertaintysetsinducedbytheconditionalWassersteindistance:
n o
ϑϵ = Zϕ ∈Z : ∥F˘ −F˘ ∥≤ϵ , and (2.2a)
Z t+1 Z|Ft Zϕ|Ft
 ∥F˘ −F˘ ∥≤ϵ, 
 Z|Ft Zϕ|Ft 
ςϵ = Zϕ ∈Z : ⟨F˘ ,1⟩=⟨F˘ ,1⟩, . (2.2b)
Z

t+1 ∥F˘Z|Ft
∥2=∥F˘
Zϕ|F
∥t
2

Z|Ft Zϕ|Ft
Eq.(2.2a)containsallF -measurablerandomvariablethataredistributionallyclosetoZwrttheconditionalWasser-
t+1
stein distance, while Eq. (2.2b) additionally imposes they have the same first two moments. These uncertainty sets
expandasthetoleranceϵincreases,andonerecoverstheoriginalriskmeasureρ whenϵ=0.Here,ϵinDefinition2.8
t
isdirectlydrivenbytheagent’sriskpreferences. RulesofthumbforthistoleranceϵareexploredinSection6.
In what follows, we work with dynamic risk measures where each one-step conditional risk measure is a robust
distortionriskmeasure,asdescribedinDefinitions2.5and2.8,withatoleranceϵ ∈F andanabsolutelycontinuous
t t
piecewiselineardistortionfunctiong :
t
ϱϵ tt,gt(Z):=e Zs ϕs ∈s φu ϵp
t
E(cid:20) Zϕ γ t(cid:16) F Zϕ|Ft(Zϕ)(cid:17)(cid:12) (cid:12) (cid:12) (cid:12)F t(cid:21) . (2.3)
Z
Thisclassofriskmeasurestakesintoaccounttheuncertaintyoftherandomvariablewiththerobustification,allows
risk-averse and risk-seeking behaviors with the distortion function, and are conditionally elicitable, because they
may be written as a linear combination of CVaRs. We first explore uncertainty sets φϵ satisfying Eq. (2.2a) in
Z
Subsection3.1,andthenshowinSubsection3.2whyincludingmomentconstraintsasinEq.(2.2b)becomesessential
fordecisionmakingproblems.
2.2 Elicitability
One desirable property for risk measures, which plays a crucial role in the algorithmic part of this work, is their
elicitability. Indeed,anelicitablemeasureofriskadmitstheexistenceofacertainlossfunctionthatcanbeusedasa
penalizerwhenupdatingitspointestimate. Here,wefollowtheworkfromGneiting(2011).
Definition2.9. LetA ⊆ R¯k,fork ≥ 1. Amappingρ : Z → Aisk-elicitableiffthereexistsastrictlyconsistent
t t+1
scoringfunctionS :A×K→Rforρ,thatisforanyF anda∈A,wehave
Z|Ft
h i h i
E S(ρ (Z),Z) ≤E S(a,Z) ,
Z∼FZ|Ft t Z∼FZ|Ft
withequalitywhena=ρ (Z).
t
In view of Definition 2.9, a risk measure is elicitable if and only if there exists a scoring function S such that its
estimateistheuniqueminimizeroftheexpectedscore,i.e.
h i
ρ (Z)=argmin E S(a,Z) .
t
a∈A
Z∼FZ|Ft
6RobustRLwithDynamicDistortionRisk
To fix ideas, let us describe some strictly consistent scoring functions for well-known risk measures. The mean is
1-elicitableandastrictlyconsistentscoringfunctionmustbeoftheform
S(a,z)=h(z)−h(a)+h′(a)(a−z), (2.4)
where h : K → R is strictly convex with subgradient h′. We remark that using h(z) = z2 in Eq. (2.4) leads to the
squarederror. Thevalue-at-risk(VaR )atlevelα ∈(0,1),andthusanyα-quantile,isalso1-elicitableandastrictly
α
consistentscoringfunctionisnecessarilywrittenas
(cid:16) (cid:17)
S(a,z)= 1 −α (h(a)−h(z)), (2.5)
{a≤z}
whereh : K → Risnondecreasing. Onemayprovethattheconditionalvalue-at-risk(CVaR )atlevelα ∈ (0,1)is
α
not1-elicitable,butrather2-elicitablealongsidethevalue-at-risk. Onecharacterizationofastrictlyconsistentscoring
functionforthepair(VaR ,CVaR )is
α α
(cid:18) a +C(cid:19) a a (cid:0)1 −α(cid:1) +z1
S(a ,a ,z)=log 2 − 2 + 1 {z≤a1} {z>a1} , (2.6)
1 2 z+C a +C (a +C)(1−α)
2 2
whereC >0anda ≤a ,i.e. theCVaR mustbegreaterthanVaR .
1 2 α α
Remark2.10. AswecanobservefromEqs.(2.4)to(2.6),thereexistinfinitelymanycharacterizationsofstrictlycon-
sistentscoringfunctionsfork-elicitablemappings.Inthispaper,wedonotinvestigatehowdifferentcharacterizations
mayaffectoptimizationperformancesandpotentiallyimprovetheconvergencespeedofourRLalgorithm. ◁
We end this section with a result characterizing scoring functions for vectors of elicitable risk measures. Indeed, as
notedbyFrongilloandKash(2015),wemayconstructastrictlyconsistentscoringfunctionforavectorofelicitable
componentsusingthescoringfunctionsofeachcomponent.
Proposition 2.11. Let I = {1,...,I} with I ∈ N. Suppose {ρ(i)} are k(i)-elicitable mappings, where ρ(i) :
i∈I
Z → A(i) with its corresponding strictly consistent scoring function, denoted by S(i) : A(i) × K → R. Then
ρ:=(ρ(1),···,ρ(I))isk-elicitablewithastrictlyconsistentscorefunctionoftheform
X
S(a,Z)= S(i)(a ,Z),
i
i∈I
wherek =P k(i)anda:=(a ,···,a )∈Q A(i).
i∈I 1 I i∈I
Proof. ThisresultfollowsfromthedecompositionoftheargminoperatorduetoS beingaseparablefunction. Lete
i
beavectorwhoseentriesareallzerosexceptonesattheentriesbetween1+Pi−1k(i) andPi k(i) inclusively.
j=1 j=1
Wehave
(cid:20) (cid:21)
argmin E S(a,Z)
Q
Z∼FZ
a∈ A(i)
i∈I
(cid:20) (cid:21)
X
[decompositionofargmin] = ar Qgmin E Z∼FZ S(i)(a i,Z)
i∈I aiei∈ i∈IA(i)
X
[elicitability] = ρ(i)(Z)e i
i∈I
=ρ(Z).
7RobustRLwithDynamicDistortionRisk
3 ProblemSetup
In this section, we introduce and rigorously define the class of RL problems we aim to solve. We describe each
problem as an agent who tries to learn an optimal behavior, or agent’s policy, that attains the minimum in a given
objectivefunctionbyinteractingwithacertainenvironmentinamodel-agnosticmanner.
Let S and A be arbitrary state and action spaces respectively, and let C ⊂ R be a cost space. We represent the
environment as a Markov decision process with the tuple (S,A,c,P), where c(s,a,s′) ∈ C is a cost function and
P characterizes the transition probabilities P(s = s′ | s = s,a = a). The transition probability is assumed
t+1 t t
stationary,althoughtimemaybeacomponentofthestate. Inwhatfollows,wetakethestatespaceastheaugmented
one where the first dimension corresponds to the time space. An episode consists of a sequence of T +1 periods,
denoted by T := {0,...,T}, where T ∈ N is known and finite. We often assume that the periods refer to fixed
intervals,buttheframeworkmaybeextendedtoperiodsofarbitrarylength. Ateachperiod,theagentbeginsinastate
s ∈ S andtakesanactiona ∈ Aaccordingtoadeterministicpolicyπ : S → A. Theagentthenmovestothenext
t t
states ∈S,andreceivesacostc =c(s ,a ,s )<∞. Weviewthecostfunctionasadeterministicmappingof
t+1 t t t t+1
thestatesandactions,butwecaneasilygeneralizetoincludeothersourcesofrandomness.
We consider strongly time-consistent dynamic risk measures {ϱ } where we assume the one-step conditional
t,T t∈T
risk measures are robust (under the conditional 2-Wasserstein distance with tolerance ϵ ) distortion risk measures
st
(withanabsolutelycontinuouspiecewiselineardistortionfunctiong ),asdefinedinSubsection2.1. Weaimtosolve
st
(T +1)-periodrobustrisk-awareRLproblemsoftheform
(cid:16) (cid:17) (cid:18) (cid:17) (cid:19)!
min ϱϵ,g {cπ} =min ϱϵs0,gs0 cπ+ϱϵs1,gs1 cπ+···+ϱϵsT,gsT(cπ) ··· , (P)
0,T t t∈T 0 0 1 1 T T
π π
wherecπ =c(s ,π(s ),s )isaboundedF -measurablerandomcostmodulatedbythepolicyπ.State-dependent
t t t t+1 t+1
distortionfunctionsg andtolerancesϵ maybeusedtoillustrateanagentthat,forinstance,slowlyreduceshermodel
st st
uncertaintyasshegetsclosertotheterminalperiodordrasticallychangesherriskpreferencesinlessfavorablestates.
In this paper, we prove the results under the assumption of state-dependent distortion functions and tolerances, but
focusonsimplerstate-independentdynamicriskmeasuresintheexperimentalsection.
Remark 3.1. Here, we do not subtract the (dynamic) robust risk of zero to obtain a normalized time-consistent
dynamic risk measure (see Theorem 4 of Moresco et al., 2024), as the risk of zero does not depend on the policy
parametersand,hence,doesnotplayaroleintheoptimizationprocedure. ◁
GivenEq.(P),wedefinethevaluefunctionforanagentastherunningrisk-to-go
V t(s;π):=ϱϵ tst,gst cπ t +ϱϵ t+st 1+1,gst+1(cid:18) cπ t+1+···+ϱϵ TsT,gsT(cid:16) cπ T(cid:17)(cid:19)(cid:12) (cid:12) (cid:12) (cid:12)s t =s! ,
foralls ∈ S andt ∈ T. Itgivesthe(time-consistent)dynamicriskofthesequenceoffuturecostsfortheagentfol-
lowingthepolicyπatacertaintimetwhenbeinginaspecificstates.UsingDefinition2.8,thedynamicprogramming
equationsforaspecificpolicyπare
(cid:16) (cid:12) (cid:17) D E
V (s;π)=ϱϵs,gs cπ+V (sπ ;π)(cid:12)s =s = esssup γ ,F˘ ,
t t t t+1 t+1 (cid:12) t
Z tϕ∈φϵ Zs tπ
s Z tϕ|st=s
whereZπ = cπ +V (sπ ;π)andV = 0. Weapplythedynamicprogrammingprinciple(DPP)torecovera
t t t+1 t+1 T+1
Bellman-likeequationforthevaluefunction:
D E
V (s;π∗)=min esssup γ ,F˘ , (3.1)
t
a∈A Zϕ∈φϵs
s Z tϕ|st=s
t Zta,π∗
8RobustRLwithDynamicDistortionRisk
whereZa,π∗ =c(s ,a,s )+V (s ;π∗).Thepreviousequationindicatestheoptimalpolicyfortheagentatany
t t t+1 t+1 t+1
pointinstatewitharecursiveequationinvolvingthefuturecosts,composedofthecostatcurrenttimet,therunning
risk-to-goatthenexttimet+1,andanadversarywhodistortsbothtogettheworstperformance.
Alternatively,wedefinethequalityfunctionorQ-function,whicheffectivelyrepresentstherunningrisk-to-goforan
agentstartinginanystate-actiontupleandthereafterfollowingthepolicyπ,as
Q t(s,a;π):=ϱϵ tst,gst c(s t,a t,s t+1)+ϱϵ t+st 1+1,gst+1(cid:18) cπ t+1+···+ϱϵ TsT,gsT(cid:16) cπ T(cid:17)(cid:19)(cid:12) (cid:12) (cid:12) (cid:12)s t =s, a t =a! ,
foralls∈S,t∈T anda ∈A. UsingtheDPPforthevaluefunctioninEq.(3.1),wehave
t
V (s;π∗)=min Q (s,a;π∗),
t t
a∈A
whichleadstoaBellman-likeequationfortheQ-functionoftheform
D E
Q (s,a;π∗)= esssup γ ,F˘ . (3.2)
t
Zϕ∈φϵs
s Z tϕ|st=s,at=a
t Zta,π∗
ThegoalistominimizethevaluefunctionV (s;π)=Q (s,π(s);π)overpoliciesπ,whichalsorequiresmaximizing
t t
theworst-caseriskoverϕintheuncertaintysetandestimatingthevaluefunctionitself. Weproposetouseparametric
approximatorsforthedifferentcomponentsweoptimize,andthusweaimtoestimatetheQ-function
D E
Q (s,a;θ)= esssup γ ,F˘ , (3.3)
t
Zϕ∈φϵs
s Z tϕ|st=s,at=a
t Ztθ
whereθarethepolicyparameters.Forcompactnotation,wedenotecosts-to-gobyZθ :=c +Q (s ,πθ(s );θ)
t t t+1 t+1 t+1
andtheirconditionalCDFbyF (z|s,a) := F (z). Inthenextsections,wedeterminethedistributionof
Z tθ Z tθ|st=s,at=a
theworst-casecost-to-goZϕconditionallyonastate-actionpair(s,a)fordifferentuncertaintysets. Thisallowsusto
t
designrandomvariablesthatmaximizetheone-stepconditionalriskmeasurewithintheQ-functionwhileremaining
distributionallyclosetotheoriginalcost-to-goaccordingtotheappropriateuncertaintyset.
3.1 Wassersteinuncertaintyset
Inthissection,weinvestigatedynamicrobustriskmeasures,wheretheone-stepconditionalriskmeasuresaredistor-
tion risk measures with uncertainty sets of the form in Eq. (2.2a). More precisely, we restrict the random variable’s
distributiontoliewithinaWassersteinballwithintheoriginaldistribution. Thenextresultfollows(Theorem3.9of
PesentiandJaimungal,2023)byreformulatingtheoptimizationproblemwithitsquantilerepresentationtoworkwith
aconvexoptimizationproblem. TheproofofProposition3.2isdeferredtoAppendixA.1.
Proposition3.2. LetF↑betheisotonicprojectionofafunctionF,morepreciselyitsprojectionontothesetofquantile
functions F↑ := argmin ∥G−F∥2, where F˘ := {F ∈ L2([0,1]) : F isnondecreasingandleft-continuous}.
G∈F˘
Further,considertheQ-functioninEq.(3.3),wheretheone-stepconditionalriskmeasureisarobustdistortionrisk
measure, with state-dependent distortion function and tolerance, and its uncertainty set is of the form in Eq. (2.2a).
ThequantilefunctionoftheoptimalrandomvariableintheoptimizationproblemQ (s,a;θ)isgivenby
t
(cid:18)
γ
(·)(cid:19)↑
F˘∗(·|s,a)= F˘ (·|s,a)+ s ,
ϕ Z tθ 2λ∗
whereλ∗ >0issuchthat
(cid:13) (cid:13)2
(cid:13)F˘∗(·|s,a)−F˘ (·|s,a)(cid:13) =ϵ2.
(cid:13) ϕ Zθ (cid:13) s
t
9RobustRLwithDynamicDistortionRisk
Equippedwiththepreviousresult,findingtheoptimalquantilefunctionmaybecomputationallyintensive,becauseit
requires repeatedly solving the optimization problem in Proposition 3.2 to find the optimal λ∗ for any pair (s,a) ∈
S ×A. One solution to alleviate this issue consists of performing parallel computations to accelerate the process.
Anotherapproachistoworkwithasmallerclassofdynamicriskmeasures. Indeed,Proposition3.2maybesimplified
ifweconsiderone-stepconditionaldistortionriskmeasuresthatarecoherent. Wespecifythisobservationintheresult
below.
Corollary3.3. ConsidertheQ-functioninEq.(3.3),wheretheone-stepconditionalriskmeasureisarobustdistortion
riskmeasure,withstate-dependentdistortionfunctionandtolerance,anditsuncertaintysetisoftheforminEq.(2.2a).
In addition, assume that the distortion function g of each one-step conditional risk measure is nondecreasing and
s
concave,i.e. γ isnondecreasing. Thequantilefunctionoftheoptimalrandomvariableintheoptimizationproblem
s
Q (s,a;θ)isthengivenby
t ϵ γ
F˘∗(·|s,a)=F˘ (·|s,a)+ s s.
ϕ Z tθ ∥γ s∥
Proof. TheresultfollowsfromProposition3.2.Indeed,recallthatthequantilefunctionoftheoptimalrandomvariable
is
(cid:18)
γ
(·)(cid:19)↑
F˘∗(·|s)= F˘ (·|s,a)+ s .
ϕ Z tθ 2λ∗
Since both F˘ and γ are nondecreasing, the isotonic projection equals itself. Finally, one recovers the analytical
Zθ s
t
formofλ∗,because
(cid:13) (cid:13) ∥γ ∥
ϵ =(cid:13)F˘∗(·|s,a)−F˘ (·|s,a)(cid:13)= s .
s (cid:13) ϕ Z tθ (cid:13) 2λ∗
Usingnondecreasingdistortionfunctions,Corollary3.3impliesthat
D E
Q (s,a;θ)= esssup γ ,F˘ (·|s,a)
t s Zϕ
Zϕ∈φϵs t
t Ztθ
D E
= γ ,F˘∗(·|s,a)
s ϕ
D E
= γ ,F˘ (·|s,a) +ϵ ∥γ ∥
s Zθ s s
t
D E
= γ ,F˘ (·|s,a) +ϵ ∥γ ∥
s ct+Qt+1(st+1,πθ(st+1);θ) s s
D E
= γ ,F˘ (·|s,a) . (3.4)
s (ct+ϵs∥γs∥)+Qt+1(st+1,πθ(st+1);θ)
Asconditionaldistortionriskmeasuresarecash-additive,theF -measurableshiftϵ ∥γ ∥inEq.(3.4)maybeincluded
t s s
aspartofthecostfunctionc ,regardlessofthetolerancebeingconstantorstate-dependent. Therefore,fordynamic
t
distortionriskmeasureswithnondecreasingdistortionfunctionsanduncertaintysetsoftheforminEq.(2.2a),robust-
nessisequivalenttomodulatingthecostfunction.Infact,thisobservationmaybeextendedtootherdynamicmonetary
riskmeasureswithuncertaintysetsinducedonlybysemi-normsonthespaceofrandomvariables. Furthermore,with
astate-independenttoleranceϵanddistortionfunctionγ,therobustoptimalpolicyremainsidenticaltothenon-robust
optimalpolicy,becausetheshiftϵ∥γ∥ doesnotdependwhatsoeveronthepolicyparametersθ.
2
Remark 3.4. For this class of problems, the structure of the resulting actor-critic algorithm resembles other deep
deterministicpolicygradientalgorithmsfoundintheliterature(seee.g.Lillicrapetal.,2015;Marzbanetal.,2023).
Weleaveforfutureworksaninvestigationofthealgorithmperformances,andinsteadinspecttheeffectsofmodifying
theformoftheconditionaluncertaintyset. ◁
10RobustRLwithDynamicDistortionRisk
3.2 Wassersteinuncertaintysetwithmomentconstraints
As explained in the previous section, an uncertainty set Eq. (2.2a) with a constant tolerance and distortion function
leadstoidenticaloptimalpoliciesinboththerobustandnon-robustcases. Toovercomethissituation,wecaneither
include a state-dependent tolerance, used to illustrate an agent that changes its risk preferences in less favorable
states,ormodifytheuncertaintyset. Inthissection,weexplorethelatteroptionbyconsideringdynamicrobustrisk
measureswheretheone-stepconditionalriskmeasuresaredistortionriskmeasureswithuncertaintysetsoftheform
inEq.(2.2b). Moreprecisely,werestricttherandomvariable’sdistributiontoliewithinaWassersteinballfromthe
originaldistributionandhavethesamefirstandsecondmoments.1
Next,wecastinadynamicsetting(Theorem3.1of Bernardetal.,2023),whichprovidestheoptimalquantilefunction
ofarandomvariablewithWassersteinandmomentconstraints. TheproofisprovidedinAppendixA.2.
Theorem3.5. ConsidertheQ-functioninEq.(3.3),wheretheone-stepconditionalriskmeasureisarobustdistortion
riskmeasure,withstate-dependentdistortionfunctionandtolerance,anditsuncertaintysetisoftheforminEq.(2.2b).
In addition, assume that the distortion function g of each one-step conditional risk measure is nondecreasing and
s
concave,i.e. γ isnondecreasing. Thequantilefunctionoftheoptimalrandomvariableintheoptimizationproblem
s
Q (s,a;θ)isthengivenby
t
(cid:16) (cid:17)
λ∗ F˘ (u|s,a)−µ +γ (u)−1
Zθ s
F˘∗(u|s,a)=µ+ t ,
ϕ b
λ∗
where
r
(cid:16)D E (cid:17)
(λ∗σ)2+σ2+2λ∗ F˘ (·|s,a),γ −µ
γ Zθ s
t
b = ,
λ∗
σ
(cid:16)D E (cid:17) √
−2 F˘ (·|s,a),γ −µ + ∆
Zθ s
λ∗ = t ,
2σ2
4K2 (cid:18)(cid:16)D E (cid:17)2 (cid:19)
∆= F˘ (·|s,a),γ −µ −σ2σ2 ,
K2−σ4 Z tθ s γ
ϵ2 D E (cid:13) (cid:13)2
K =σ2− s, µ= F˘ (·|s,a),1 , σ2 =(cid:13)F˘ (·|s,a)(cid:13) −µ2, σ2 =∥γ ∥2−1.
2 Z tθ (cid:13) Z tθ (cid:13) γ s
Iftheuncertaintytoleranceϵ issuchthat
s
 D E 
F˘ (·|s,a),γ −µ
Zθ s
ϵ2
s
>2σ2 1− t
σσ
, (3.5)
γ
thentheoptimalsolutionremainsvalidwithλ∗ =0.
We note that both λ∗ and b of the optimal quantile function in Theorem 3.5 depend non-trivially on the quantile
λ∗
function F˘ . This uncertainty set differs from the previous case, because even with a constant tolerance ϵ and
Zθ
t
distortionfunctionγ,therobustoptimalpolicymaybedifferentthanthenon-robustoptimalpolicyduetotheintricate
dependenceonthepolicyparametersθ.
3.3 DeterministicGradient
Next,wederivetheanalyticalexpressionofthegradientofthevaluefunction.TheproofisprovidedinAppendixA.3.
1Constrainingonlythefirstmomentleadstostrategiesthatarealsoidenticaltothenon-robustcaseasinEq.(2.2a).Constraining
onlythesecondmoment,however,doesleadtodistinctpoliciesandthealgorithmwederivecaneasilybemodifiedtothiscase.
11RobustRLwithDynamicDistortionRisk
Proposition3.6. ThegradientofthevaluefunctionV (s;θ)withanuncertaintysetoftheforminEq.(2.2b)is
t
∇ V (s;θ)=∇ Q (s,πθ(s);θ)
θ t θ t
(cid:12)
= ∇ Q (s,a;θ)(cid:12)
a t (cid:12)
a=πθ(s)
− b λ∗
b
λ− ∗λ∗ E t,s" (cid:16) (b λ∗ −λ∗)(Z tθ−µ)+1(cid:17) ∇∇ xaF FZ Ztθ θ( (x x| |s s, ,a a) )(cid:12) (cid:12) (cid:12)
(cid:12)
(x,a)=(Zθ,πθ(s))#! ∇ θπθ(s),
t t
whereλ∗,b aregiveninTheorem3.5.
λ∗
Whentheuncertaintytoleranceϵ tendstozero,thegradientofthevaluefunction,andthustheQ-function,reducesto
s
thewell-knowndeterministicpolicygradientupdaterulefromSilveretal.(2014). Weprovethattheusualdetermin-
isticgradientformulaisalimitingcase,asϵ approacheszero,ofProposition3.6.
s
Corollary3.7. ThegradientofthevaluefunctionV (s;θ)withanuncertaintysetoftheforminEq.(2.2b)satisfies
t
(cid:12)
lim∇ V (s;θ)= lim∇ Q (s,πθ(s);θ)=∇ Q (s,a;θ)(cid:12) ∇ πθ(s).
θ t θ t a t (cid:12) θ
ϵs↓0 ϵs↓0 a=πθ(s)
Proof. From Theorem 3.5, we observe that as the uncertainty tolerance decreases, the optimal quantile function F˘∗
ϕ
convergestothequantilefunctionofthecosts-to-goF˘ . Moreprecisely,K →σ2and∆,λ∗,b →∞asϵ ↓0. In
Zθ λ∗ s
t
addition,wehavethat
λ∗ λ∗σ
lim = lim =1.
r
b (cid:16)D E (cid:17)
ϵs↓0 λ∗ ϵs↓0 (λ∗σ)2+σ2+2λ∗ F˘ (·|s,a),γ −µ
γ Zθ s
t
Thisleadsto(b −λ∗)/b →0asϵ ↓0,whichconcludestheproof.
λ∗ λ∗ s
4 Algorithm
In this section, we highlight the main steps of our learning algorithm and provide details on its implementation.
The full algorithm is provided in Algorithm 1 with detailed steps for the critic (Subsections 4.1 and 4.2) and actor
(Subsection4.3). Aswell,ourPythoncodeispubliclyavailableintheGithubrepositoryRL-DynamicRobustRisk.
RecallthatweaimtooptimizeV (s;π) = Q (s,π(s);π)overpoliciesπ. Todoso,weproposeanactor-criticstyle
t t
(Konda and Tsitsiklis, 2000) algorithm, known for their ability to find optimal policies using low variance gradient
estimatesandtheirgoodconvergenceproperties,withanadditionalcomponenttoallowrobustification.Ouralgorithm
aimstolearnfourfunctionsinanalternatingmanner: (i)thecriticestimatestheCDFofthecosts-to-goconditionally
on the current state-action pair, as well as both the conditional first moment of costs-to-go and the Q-function with
a deep composite model using the elicitability of the dynamic risk, while (ii) the actor updates the current policy
via a policy gradient method. In addition, our proposed approach, similar to the deep deterministic policy gradient
algorithm (Lillicrap et al., 2015), is off-policy, in the sense that the agent learns the Q-function and the conditional
distributionofthecosts-to-goindependentlyoftheagent’sactions. Off-policyRLalgorithmscanleadtobetterdata
efficiency,byreusingobservationswithareplaybuffer,andthusfasterconvergencespeed.
Weuseartificialneuralnetworks(ANNs),knowntobeuniversalapproximators,asfunctionapproximationsforboth
the critic and actor components of our algorithms. ANNs do an astounding job at modeling complicated functions
via compositions of simple functions through several layers and avoiding the curse of dimensionality issue when
representing nonlinear functions in high dimensions. We consider the following fully-connected multi-layered feed
forward ANN structures, or multilayer perceptron (MLP), as illustrated in Fig. 1. We characterize the policy by
12RobustRLwithDynamicDistortionRisk
Algorithm1:Actor-CriticforDynamicRobustDistortionRiskMeasures
Input:MainnetworksforFϑ,Qψ,µχ,πθ,numberofepochsKϑ,Kψ,Kχ,Kθ,
mini-batchsizesBϑ,Bψ,Bχ,Bθ,initiallearningratesηϑ,ηψ,ηχ,ηθ
distortionfunctionsg,tolerancesϵ,explorationparametersp ,N,softtargetparameterτ
ex
1 Instantiateandinitializetheenvironment,optimizersandschedulers;
2
InitializetargetnetworksforF,Q,µ,πwith(ϑ˜,ψ˜,χ˜,θ˜)←(ϑ,ψ,χ,θ);
3 foreachiterationi=1,2,...do
4
Simulatefulltrajectoriesinducedbythepolicyandexplorationnoiseπθ+N;
5 Generateapartition{y i} icoveringthespanofcosts-to-go;
6 repeat
7 foreachepochk=1,...,Kϑdo ▷ Critic
8
ZerooutthegradientsofFϑ;
9
Sampleamini-batchofBϑfulltrajectories;
10
ComputethelossLϑinEq.(L1);
11
UpdateϑbyperforminganAdamoptimizationstep,andtunethelearningrateηϑ;
12 ifconvergenceisachievedthen
13 break
14
foreachepochk=1,...,Kχdo
15
Zerooutthegradientsofµχ;
16
Sampleamini-batchofBχfulltrajectories;
17
ComputethelossLχinEq.(L2);
18
UpdateχbyperforminganAdamoptimizationstep,andtunethelearningrateηχ;
19 ifconvergenceisachievedthen
20 break
21
foreachepochk=1,...,Kψdo
22
ZerooutthegradientsofQψ;
23
Sampleamini-batchofBψfulltrajectories;
24
ComputethelossLψinEq.(L3);
25
UpdateψbyperforminganAdamoptimizationstep,andtunethelearningrateηψ;
26 ifconvergenceisachievedthen
27 break
28 untilconvergenceofbothstepsisachieved;
29
Simulatefulltrajectoriesinducedbythepolicyπθ;
30 foreachepochk=1,...,Kθdo ▷ Actor
31
Zerooutthegradientsofπθ;
32
Sampleamini-batchofBθfulltrajectories;
33
ComputethelossLθinEq.(L4);
34
UpdateθbyperforminganAdamoptimizationstep,andtunethelearningrateηθ;
35 Decaytheexplorationprobabilityp ex;
36
Updatetargetnetworksusing(ϑ˜,ψ˜,χ˜,θ˜)←τ ·(ϑ,ψ,χ,θ)+(1−τ)·(ϑ˜,ψ˜,χ˜,θ˜);
Output:ApproximationoftheoptimalpolicyπθanditscorrespondingQ-functionQψ
13RobustRLwithDynamicDistortionRisk
an ANN, denoted by πθ : S → A, which takes a state s as input and outputs a deterministic action. The Q-
t
function is characterized by Qψ : S ×A → R, which outputs the running risk-to-go at any pair (s,a). To make
ouralgorithmrobust,weneedadditionalANNstoapproximaterelevantstatisticsofthecosts-to-go. Wecharacterize
theconditionalexpectationofcosts-to-gobyµχ : S ×A → R. WealsocharacterizetheCDFofthecosts-to-goby
Fϑ :S×A×R→[0,1],whereFϑ(s,a,z)givestheconditionalCDFevaluatedatzofthecosts-to-gogiventhestate-
actionpair(s,a). Aswell, forlayersthataredescendantofz, weconstraintheweightsoftheANNtononnegative
values and use monotonic activation function to ensure a nondecreasing mapping (see e.g. Sill, 1997). The exact
structure of those neural networks in terms of number of nodes and layers is ultimately application-dependent and,
consequently,describedinSection6.
MLPθ π(s)
s
MLPψ Q(s,a)
MLPχ µ(s,a)
a
MLPϑ MLP↗ϑ F(z,s,a)
z
Figure 1: Neural network structures for the various components, i.e. the policy πθ, the Q-function Q (s,a;θ), the
t
conditionalfirstmomentE[Zθ|s ,a ],andtheconditionalCDFofcosts-to-gogivenstate-actionpairsF . Here,
t t t Z tθ|st,at
MLP↗denotestheconstrainedMLPtoensuremonotonicity.
Remark4.1. Onemaycalculatetheconditionalfirstmomentofthecosts-to-gousingtheconditionalCDFandignore
the ANN µχ. In practice, however, we observe that having separate networks for both the CDF and first moment
producesmorestableresults. ◁
Remark4.2. Inourcurrentalgorithm,wefavorsimplerneuralnetworkstructuresasaproofofconcept. Wedonot
address here how different ANN structures, such as recurrent neural networks, convolutional neural networks, con-
catenatinginputsatdifferentlayersorusingpre-trainedfoundationmodels,maybettercapturethehiddenunderlying
patternsformorecomplexapplicationsand,asanendresult,improvethelearningspeed. ◁
The following sections describe in more detail the derivation of the updates rules, as well as some implementation
technicalities. We assume that we have access to mini-batches of B transitions induced by the exploratory policy
πθ+N,i.e.thecurrentpolicywithsomewhitenoiseN,whetherbygeneratingtransitionsfromthesimulationengine
orbysamplingtransitionsfromareplaybuffer,whichwedenoteby
(cid:16) (cid:17)
s ;a ;c ;s , b=1,...,B.
t,b t,b t,b t+1,b
4.1 CDFofCosts-to-go
The objective consists of estimating with the ANN Fϑ the CDF of the costs-to-go Zθ conditionally on the current
t
state-actionpair(s,a),formallyF (z|s,a). Ifoneknowstheconditionaldistributionofthecosts-to-go,theninview
Zθ
t
ofProposition3.2andTheorem3.5,oneunderstandshowtoperturbthecosts-to-goinordertomaximizethedistortion
risk. Inthisprocedure, wesupposethatallothernetworks, i.e., Q-functionQψ, firstmomentµχ andpolicyπθ, are
fixed.
WeproposetousescoringrulesinordertoobtainanestimationofthisconditionalCDF.Astrictlyproperscoringrule
for probabilistic forecasts is the equivalent of a strictly consistent scoring function for point forecasts. We refer the
14RobustRLwithDynamicDistortionRisk
readerto(GneitingandRaftery,2007)forathoroughdiscussionandnumerousexamplesofproperscoringruleson
generalsamplespaces. Thecontinuousrankedprobabilityscoreisknowntobeastrictlyproperscoringruleforthe
classofCDFswithfinitefirstmoments. ForanyrandomvariableZ,itsCDFF is
Z
h i
F =argminE S(F,Z) ,
Z
F∈F
Z∼FZ
withthestrictlyconsistentscoringfunction
Z (cid:16) (cid:17)2
S(F,z)= F(y)−1 dy.
{z≤y}
R
Therefore,wemayupdateourestimationFϑfortheconditionalCDFF (z|s,a)byusingthefactthat
Zθ
t
" (cid:12) #
Z (cid:16) (cid:17)2 (cid:12)
F (·|s,a)=argmin E F(y|s ,a )−1 dy (cid:12)s =s,a =a . (4.1)
Zθ sθ ∼P t t {Zθ≤y} (cid:12) t t
t F∈F t+1 R t (cid:12)
For each epoch of the training procedure, we first compute realizations of the costs-to-go Zθ := c +
t,b t,b
Qψ(s ,πθ(s )). We then evaluate the integral within the continuous ranked probability score of Eq. (4.1)
t+1,b t+1,b
over a partition of N points covering the cost space C, denoted by {y } . The choice of the partition is ul-
i i=1,...,N
timately application-dependent, and must be carefully chosen by the user. The loss we aim to minimize is given
by
B N
1 XXX(cid:16) (cid:17)2
Lϑ = Fϑ(s ,a ,y )−1 ∆y . (L1)
BT t,b t,b i {Z tθ ,b≤yi} i
b=1t∈T i=1
WerepeatthesestepsformanyepochstoupdatetheparametersϑandtraintheANNFϑuntilconvergence.
4.2 FirstMomentofCosts-to-goandQ-function
For the critic, we want to estimate the first moment µ(s,a) and Q-function Q (s,a;θ) while assuming the ANN
t
structuresfortheCDFFϑandpolicyπθ arefixed.
Withoutlossofgenerality,weassumethattheone-stepconditionalriskmeasuresare1-elicitablewithacorresponding
strictlyconsistentscoringfunctionS. Recallthat,inoursetting,thesequenceofcostsinducedbytheagent’spolicy
isexplainedbythesequenceofstatesandactions. Therefore,thefirstmomentandQ-functionmustbeapproximated
using a function, as opposed to a point forecast, because it is an elicitable functional of the conditional CDF given
(s ,a ). Wewishtofindmappingsµ,Q:S×A→R¯ thatminimizethefollowingexpectedscores:
t t
µ:Sm ×Ain →RE sθ t+1∼P(cid:20)(cid:16) µ(s t,a t)−Z tθ(cid:17)2 (cid:12) (cid:12) (cid:12) (cid:12)s t =s,a t =a(cid:21) and
(4.2)
Q:Sm ×i An →RE sθ t+1∼P(cid:20) S(cid:16) Q(s t,a t); Z tϕ(cid:17)(cid:12) (cid:12) (cid:12) (cid:12)s t =s,a t =a(cid:21) .
For the critic, we use this deep composite regression approach, where we restrict the space of mappings to ANNs
parametrizedbysomeparametersχ,ψ,respectively. Tokeepamodel-agnosticalgorithm,wereplacetheexpectation
by the empirical mean over a batch of B observed transitions. Finally, we generate realizations of the worst-case
costs-to-goZϕusingtheinversetransformsamplingonitsoptimalquantilefunction.
t
Whenestimatingthefirstmoment,weoptimizethelossfunction
B
1 XX(cid:16) (cid:17)2
Lχ = µχ(s ,a )−Zθ . (L2)
BT t,b t,b t,b
t∈T b=1
15RobustRLwithDynamicDistortionRisk
Letθ˜,ψ˜,χ˜,ϑ˜denotefrozenparametrizationsoftheANNapproximationsthatareslowlyupdated. Thistechniquecan
be thought of as using target networks to avoid numerical instabilities (see e.g. Van Hasselt et al. (2016)). For the
Q-function,iftheuncertaintysetisoftheforminEq.(2.2b)withanondecreasingdistortionfunctionγ ,wemayuse
s
theoptimalquantilefunctionF˘∗asinTheorem3.5. Itleadstothelossfunction
ϕ
Lψ =
1 XXB
S Qψ(s ,a ); µ +
λ∗ t,b(Ze tθ ,b−µ et,b)+γ st,b(Uet,b)−1!
, (L3)
BT t,b t,b et,b b
λ∗
t∈T b=1 t,b
whereµ et,b :=µχ˜(s t,b,a t,b),Ze tθ ,b :=c t,b+Qψ˜ (s t+1,b,πθ˜ (s t+1,b)),Uet,b :=Fϑ˜ (s t,b,a t,b,Ze tθ ,b),andλ∗ t,b,b λ∗ t,b foreach
transitionaregiveninTheorem3.5.UsingtargetnetworksensuresthattherandomvariablesUet,bremainconditionally
uniformonF whiletheQ-functionchanges.
t
Remark4.3. Alternatively,iftheuncertaintysetisoftheforminEq.(2.2a),weobtainanestimateoftheappropriate
optimalquantile functionby(i) evaluatingtheCDF Fϑ(s ,a ,·) onthe partitioncoveringthe costspace {y } , (ii)
t t i i
invertingittogetanestimateofthequantilefunctionF˘ϑ(s ,a ,·),and(iii)performinganisotonicregressionwitha
t t
givenλ∗untiltheWassersteinconstraintinProposition3.2issatisfied. Altogetherwewishtominimizetheloss
Lψ =
1 XXB S(cid:18)
Qψ(s ,a );
(cid:16)
F˘ϑ(s ,a ,x)+
γ st,b(x)(cid:17)↑(cid:12)
(cid:12)
(cid:19)
.
BT t∈T b=1 t,b t,b t,b t,b 2λ∗ t,b (cid:12) x=Uet,b
Note that this loss function involves constantly solving optimization problems to obtain λ∗ and calculating isotonic
t
projections,whichiscomputationallyexpensive. ◁
This approach is straightforward to implement with a 1-elicitable functional, as one substitutes S with the strictly
consistent scoring function for the corresponding one-step conditional risk measure. For k-elicitable mappings, one
must modify the structure of Qψ according to the number of elicitable mappings such that the one-step conditional
riskmeasurebecomeselicitable. IfweconsiderthedynamicCVaR ,theQ-functionconsistsoftwoANNsreturning
α
the approximations of the dynamic VaR and the excess between the dynamic CVaR and dynamic VaR , while
α α α
the scoring function is of the form given in Eq. (2.6). For general α-β risk measures with a distortion function
characterizedby
1(cid:16) (cid:17) 1(cid:16) (cid:17)
g(x)= p(x−(1−α)) +(1−p)min{x,1−β} and γ(u)= p1 +(1−p)1 ,
η + η {u<α} {u≥β}
wherep∈[0,1],0<α≤β <1andη =pα+(1−p)(1−β),wewishtoestimatethevector
(cid:16) (cid:17)
LTE (Z),VaR (Z),VaR (Z),CVaR (Z) ,
α α β β
whichis4-elicitableinviewofProposition2.11. Morespecifically,weproposethefollowingdecompositionforthe
Q-function: (i) VaR ; (ii) excess between VaR and LTE ; (iii) excess between VaR and VaR ; and (iv) excess
α α α β α
betweenCVaR andVaR . Thisdecompositionsatisfiestheinequalities
β β
LTE (Z)≤VaR (Z)≤VaR (Z)≤CVaR (Z),
α α β β
andwerecovertheQ-functionwith
pα (1−p)(1−β)
LTE (Z)+ CVaR (Z).
η α η β
WereferthereadertoCoacheetal.(2023)forasimilarprocedurewithdynamicspectralriskmeasures.
16RobustRLwithDynamicDistortionRisk
4.3 Policy
Toupdatethepolicy,wewanttoupdatethepolicyparametersinthedirectionofthegradientofthevaluefunction. In
thisprocedure,wesupposethattheQ-functionQψ,firstmomentµχandCDFFϑarefixed.
Usingamini-batchofBfulltrajectories,wewanttoestimatethegradientinProposition3.6anduseitinapolicygra-
dientapproach. Theexistenceofthisgradientrequiresthepolicyπθ tosatisfysomeregularityassumptionsregarding
itscontinuity,whicharefairlystandardinRLwithneuralnetworksasfunctionapproximators. Altogetherweaimto
minimizetheloss
1 XXB " (cid:12)
Lθ = ∇ πθ(s ) ∇ Qψ(s ,a;θ)(cid:12)
BT
t∈T b=1
θ t,b a t,b (cid:12) a=πθ(st,b)
(L4)
− b λ∗ t, bb λ−
∗
t,bλ∗ t,b(cid:16) (b λ∗
t,b
−λ∗ t,b)(Z tθ ,b−µ t,b)+1(cid:17) ∇∇ xaF Fϑ ϑ( (s st t, ,b b, ,a a, ,x x) )(cid:12) (cid:12) (cid:12)
(cid:12) (x,a)=(Z tθ
,b,πθ(st,b))# .
Weignoreanygradient∇ F ,becausewefixFϑ whileperformingapolicygradientstepduringtheactor,i.e. the
Zθ Zθ
t t
ANN(s)donotexplicitlydependonθ. Thatisacommonapproachintheliterature(seee.g.Degrisetal.,2012). We
repeatthesestepsforacertainnumberofepochs,whichupdatesthepolicyparametersinthedirectionofthegradient
of the value function. The number of epochs for the actor must remain relatively small, because the estimations
Qψ,µχ,Fϑquicklybecomeobsoleteasthepolicychanges.
5 UniversalApproximationTheorem
In this section, we show that, all things being held equal, there exist sufficiently large ANNs of the form given in
Section 4 accurately approximating the relevant mappings, in the same spirit as universal approximation theorems.
Beforewestatethemainresults,webeginbylistingsomecruciallemmas.
Lemma5.1. (e.g.,Theorem3.1ofPinkus,1999) Letd ∈ NandK ⊂ Rd beacompactsubset. Foranyε > 0and
continuousfunctionf(x),x∈K,thereexistsanANN,denotedbyfˆ,suchthatsup ∥f(x)−fˆ(x)∥<εifandonly
x∈K
iftheactivationfunctionisnotapolynomial.
Lemma 5.2. (Theorem 2 of Mikulincer and Reichman, 2022) Let d ∈ N and K ⊂ Rd be a compact subset. For
any ε > 0 and continuous monotone function f(x), x ∈ K, there exists a 4-layer ANN with indicator functions as
activationfunctions,denotedbyfˆ,suchthatsup ∥f(x)−fˆ(x)∥<ε.
x∈K
Lemma5.3. (Lemma6.4ofCoacheandJaimungal,2024) Letd ∈ NandK ⊂ Rd beacompactsubset. Forany
ε > 0 and ensemble of a finite number of ANNs {fˆ(x)} , x ∈ K, there exists an ANN, denoted by gˆ, such that
t t∈T
sup ∥fˆ(x)−gˆ(x)∥<ε,∀t∈T.
x∈K t t
Lemma 5.4. Let ρ be a monetary one-step conditional risk measure – that is cash additive, where ρ (m+Z) =
t t
m+ρ (Z)foranym ∈ Z ,andmonotone,whereZ ≤ W impliesρ (Z) ≤ ρ (W). Considertherobustversionϱϵs
t t t t t
underthe2-WassersteindistancewithanuncertaintysetofeithertheformEq.(2.2a)orEq.(2.2b). Then,ϱϵs remains
t
monetary.
Proof. Wehavethatϱϵs iscashadditivesince,foranym∈Z ,
t t
ϱϵs(m+Z)= esssup ρ (Zϕ)=esssupρ (m+Zϕ)=m+ϱϵs(Z).
t t t t
Zϕ∈φϵs Zϕ∈φϵs
m+Z Z
Next, we show that ϱϵs remains monotone by contradiction using the fact that the 2-Wasserstein distance defines a
t
metric on the space of probability measures. Denoting Z∗ = argmax Zϕ∈φϵs ρ t(Zϕ), let Z ≤ W and assume that
Z
Z∗ > W∗. We cannot have ⟨F˘ ,F˘ ⟩ ≤ ϵ , because we would obtain Z∗ ≤ W∗. On the other hand, if
Z∗|Ft W|Ft s
17RobustRLwithDynamicDistortionRisk
⟨F˘ ,F˘ ⟩>ϵ ,weget,usingthetriangleinequality,that
Z∗|Ft W|Ft s
ϵ <⟨F˘ ,F˘ ⟩≤⟨F˘ ,F˘ ⟩+⟨F˘ ,F˘ ⟩≤ϵ +⟨F˘ ,F˘ ⟩,
s Z∗|Ft W|Ft Z∗|Ft Z|Ft Z|Ft W|Ft s Z|Ft W|Ft
whichleadstoacontradiction,since⟨F˘ ,F˘ ⟩≥0. Therefore,wemusthaveZ∗ ≤W∗and
Z|Ft W|Ft
ϱϵs(Z)=ϱϵs(Z∗)≤ϱϵs(W∗)=ϱϵs(W),
t t t t
wheretheinequalityfollowsfromthecashadditivityofρ .
t
Withthoselemmasinhand,weproceedwiththeuniversalapproximationtheoremsfortheQ-function,firstmoment
andCDFofthecosts-to-go.
Theorem5.5. Letπθ,Fϑ,Qψ befixed. Then,foranyε>0,thereexistsanANN,denotedbyµχ,suchthat∀t∈T,
(cid:13) h (cid:12) i (cid:13)
sup (cid:13)E c +Qψ(s ,πθ(s ))(cid:12)(s ,a )=(s,a) −µχ(s,a)(cid:13)<ε.
(cid:13) t t+1 t+1 (cid:12) t t (cid:13)
(s,a)∈S×A
Proof. Wegiveasketchoftheproof,astheresultfollowsclosely(Theorem6.2ofCoacheandJaimungal,2024). We
aimtoestimatethedynamicexpectationofthecosts-to-go,whereeachone-stepconditionalriskmeasureismonetary.
Mappings satisfying cash additivity and monotonicity are Lipschitz continuous and, hence, absolutely continuous.
UsingLemma5.1,foreacht∈T,thereexistsanANN,denotedbyµ ,approximatingtoanarbitraryaccuracyε >0
t t
thefirstmomentofthecosts-to-goE[c +Qψ(s ,πθ(s ))|s ,a ]. UsingLemma5.3,thereexistsasingleANN
t t+1 t+1 t t
approximatingtoanarbitraryaccuracythiscollectionofANNs{µ } .
t t∈T
Theorem5.6. Letπθ,Fϑ,µχ befixedandconsidertheQ-functioninEq.(3.3). Then,foranyε > 0,thereexistsan
ANN,denotedbyQψ,suchthat,∀t∈T,
(cid:13) (cid:13)
sup (cid:13)Q (s,a;θ)−Qψ(s,a)(cid:13)<ε.
(cid:13) t (cid:13)
(s,a)∈S×A
Proof. Again,wegiveasketchoftheproof,astheresultfollowsclosely(Theorem6.1andCorollary6.2ofCoache
etal.,2023). AssumethattheQ-functionisk-elicitablewithagivendecomposition. UsingLemma5.4,theone-step
conditional risk measures, which are robust distortion risk, satisfy the monetary properties and, thus, are absolutely
continuous. Furthermore, all k components may be expressed as linear combinations of VaRs and CVaRs, so they
areabsolutelycontinuousaswell.
We use a proof by induction to show that all each component may be approximated arbitrarily accurately at every
periodt ∈ T aslongaswehaveanadequateapproximationatthesubsequentperiods. UsingLemma5.1, thebase
case at t = T is true. We then assume that the statement is true for t + 1, that is there exist ANNs, denoted by
Q , approximating to an arbitrary accuracy ε > 0 the κ-th component for κ = 1,...,k. We prove that the k
κ,t κ,t
componentsmaybeapproximatedtoanyarbitraryaccuracyattusingthecashadditivityproperty,triangleinequality
andLemma5.1,whichcompletestheproofbyinduction.
UsingLemma5.3,thereexistkANNsapproximatingtoanarbitraryaccuracythecollectionofANNs{Q } for
κ,t t∈T
κ=1,...,k. Finally,wecanconstructasingleANNapproximatingtoanarbitraryaccuracytheQ-functionusingthe
triangleinequality,sinceitisalinearcombinationofthosekANNs.
Conjecture5.7. Letπθ,µχ,Qψ befixed. Then,thereexistsa(n+4)-layerANNwiththearchitecturegiveninFig.1,
denotedbyFϑ,suchthat∀t∈T,itadequatelyapproximatestheconditionalCDFofcosts-to-goF (z|s,a).
Zt|st,at
The issue here lies in the partial monotonicity of the conditional CDF, which makes proving uniform convergence
resultsquitechallenging. Nonetheless,webelievethecombinationofunconstrainedandconstrainedlayerspreserves
some suitable universal approximation guarantees. Indeed, we suspect that Lemma 5.2 may be extended to ANNs
18RobustRLwithDynamicDistortionRisk
with sigmoid or tanh activation functions, because they can approximate indicator functions (by fixing the bias and
choosing a large enough weight). Using Lemma 5.1, the first n unconstrained layers should not be problematic for
providing an arbitrary accurate approximation. We then believe that the last 4 constrained layers approximate the
partiallymonotonefunctionofinterest,potentiallywithanaccuracydependingonthenonmonotonicinputs,butthis
remainstobeproven. Inpractice,aswearemoreinvestedinensuringthemonotonicitypropertytoavoidnumerical
instabilities than the approximation guarantees, we recover an adequate estimation of the costs-to-go distribution as
part of our actor-critic algorithm, which points that this conjecture holds given a sufficiently large ANN in terms of
widthanddepth.
6 ExperimentalResults
ThiscollectionofexperimentsisperformedonaportfolioallocationproblemsimilartoSection7.2ofCoacheetal.
(2023). Suppose an agent can allocate her wealth between different risky assets during T = 12 periods over a six
month horizon. The agent intents on minimizing a dynamic risk measure of her profit and loss (P&L) with robust
distortionone-stepconditionalriskmeasures. Dependingonherownriskpreferences,theagenthasthepossibilityto
tuneherobjectiveby(i)selectingadistortionfunctionthatleadstorisk-neutral,risk-averseorrisk-seekingpolicies,
and(ii)choosinglargerϵ’storobustifyheractionsagainsttheuncertaintyofthetruedynamicsofthemarket.
The choice of the tolerance ϵ may be influenced by some additional exogenous information, such as expert opinion
or known bounds on the moments of the cost distribution that the agent is willing to accept (see e.g. Pesenti and
Jaimungal, 2023; Bernard et al., 2023). Otherwise, the agent may decide on an appropriate ϵ using the following
data-driven approach. Given a training environment, suppose that the agent designs a testing environment she aims
to robustify against. The tolerance ϵ should then be just large enough such that the optimal time-consistent robust
policy(i)staysclosetotheoptimalpolicyofthetrainingenvironment,and(ii)performsrelativelywellforthetesting
environment. Fromtheagent’sperspective,thisspecificchoiceofrobustdistortionone-stepconditionalriskmeasure
givesanotionofrobustnesssheiswillingtotolerateforagivenpairoftraining-testingenvironments,whichshould
robustifyaswellother(unknown)testingenvironments.
Foreveryriskyasset,denotedbyi ∈ I := {1,...,I},weconsiderpricedynamicsdrivenbyaco-integrationmodel
tomimicrealisticpricepaths. Moreprecisely,weestimateavectorerrorcorrectionmodel(VECM)usingdailydata
fromI = 8differentstockslistedontheNASDAQexchangebetweenSeptember31, 2020andDecember31, 2021
inclusively. Theresultingestimatedmodel,withtwocointegrationfactorsandnolagdifferences(bothselectedusing
theBICcriterion),isusedasasimulationenginetogeneratepricepaths(S(i)) . Wereferthereaderto(AppendixC
t t
ofCoacheetal.,2023)forexplanationsonVECMsandtheparameterestimatesforthisdataset. InTable1,wereport
somestatisticsofinterestforthedifferentstocksandhighlighttwodistinctgroups: riskierassetswithgreaterreturns
onaverage(i.e. AAL,CCL,LYFT,OXY)andlessvolatileassetswithsmallerreturns(i.e. AMZN,FB,IBM,INTC).
Stock InitialpriceS Mean Standarddeviation
0
AAL 31.76 0.0137 0.1696
AMZN 1780.75 0.0025 0.0707
CCL 50.72 0.0217 0.2145
FB 166.69 0.0031 0.0784
IBM 141.10 0.0026 0.0732
INTC 53.7 0.0044 0.0947
LYFT 78.29 0.0130 0.1662
OXY 66.20 0.0193 0.2023
Table 1: Initial price S , mean and standard deviation of the relative price change St+1−St for each asset estimated
0 St
over100,000simulatedsamplepathsfromtheresultingVECM.
19RobustRLwithDynamicDistortionRisk
At each period t ∈ T, the agent observes the information available and decides on the proportions of her wealth,
denotedby(π(i)) ,i∈I,toinvestinthedifferentfinancialinstruments. WeimposetheseactionstobeaI-simplexin
t t
ordertoavoidshortselling,i.e. π(i) ≥0,∀i∈I andP π(i) =1. Inpractice,weapplyasoftmaxtransformation
t i∈I t
asanactivationfunctionontheoutputlayerofthepolicyneuralnetwork. TheobservedcostscorrespondstotheP&L
fluctuationbetweentwosubsequentperiodsc =y +y ,wheretheagent’swealth(y ) isdeterminedby
t t t+1 t t
dy =y
XI
π(i)dS
t(i)!
, y =1. (6.1)
t t t S(i) 0
i=1 t
Fromanalgorithmicperspective, wealternatebetweenallcomponentsofthecritic, whereeachstepisexecutedfor
5 epochs, before executing the actor for 1 epoch. We generate a partition {y } of 501 evenly spaced points on an
i i
intervalthatcoversthecosts-to-goZeθ ateachiteration,andusemini-batchsizesofBϑ = Bψ = Bψ = Bθ = 128.
t,b
TheANNshavethefollowingstructure:
πθ :fivelayersof32hiddennodeseachwithSiLUactivationfunctionsandasoftmaxoutputactivationfunction;
Qψ : sixlayersof32hiddennodeseachwithSiLUactivationfunctions,atanhoutputactivationfunctionforthe
dynamicVaRandasoftplusoutputactivationfunctionfortheexcessbetweendynamicCVaRandVaR;
µχ : sixlayersof32hiddennodeseachwithSiLUactivationfunctions;
Fϑ : fourlayersof32hiddennodeseachwithSiLUactivationfunctions, followedbyfourlayersof32hidden
nodeseachwithtanhactivationfunctions,positiveweights,andasigmoidoutputactivationfunction.
Learningratesforthecriticareinitiallysetto5×10−4, whilethelearningratefortheactorissetto3×10−6 and
decaysby0.999995ateveryepoch. Thetargetnetworksareupdatedateveryiterationwithasoftupdateparameterof
τ = 0.008. Wetrainallmodelsfor500,000iterations(approximately24hours)ontheGrahamservers,managedby
theDigitalResearchAllianceofCanada.
We suggest the following approach to include exploratory noise in the current policy. For each action a = π(s) ∈
[0,1]I, we generate independently a Bernoulli random variable ξ ∈ {0,1} with an exploration probability p =
ex
0.5 such that ξ ∼ Ber(p ). For each action such that ξ = 1, we additionally generate a Dirichlet noise N ∼
ex
Dirichlet(0.05),andthencomputetherandomizedactiona′ =0.75a+0.25N.
Tounderstandwhatthelearnedoptimalpolicydictates,Fig.2showstheaverageinvestmentproportionsineachasset
for every period when optimizing a dynamic robust CVaR and varying the the uncertainty tolerance ϵ. Without
0.1
robustification, when ϵ = 0, the agent prioritizes a mix of assets, some with greater returns and others with lower
volatilities. ThelearnedoptimalpolicyevolvesasweincreaseϵandtheP&Ldistributionmovestotheleft. Forlarger
tolerances, the learned policy becomes closer to an equal weight portfolio over all available assets. We then repeat
the exercise with the dynamic robust CVaR , as shown in Fig. 3, and obtain the same behavior when increasing
0.2
ϵ. Additionally, withahigherrisk-awareness, weobservethatthelearnedoptimalpolicyprefersinvestmentsinless
volatile assets on average. In general, larger uncertainty tolerances lead to more conservative policies that do not
performoptimallyintermsofP&L,andweretrievethenon-robustoptimalpoliciesasϵdecreasestozero.
7 Conclusion
Inthiswork,wepresentarobustRLframeworkfortime-consistentrisk-awareagents. Ourapproachutilizesdynamic
riskmeasuresconstructedwithaclassofrobustdistortionriskmeasuresofallrandomvariableswithinaWasserstein
uncertainty set to robustify the agent’s actions. We estimate the dynamic risk using the elicitability of distortion
risk measures and derive a deterministic policy gradient procedure by reformulating the optimization problem via a
quantilerepresentation. Furthermore,weshowthatourproposeddeeplearningalgorithmperformswellonaportfolio
allocationexample.
20RobustRLwithDynamicDistortionRisk
=0 =106 =102 =101
11..00
00..88 AAL
AMZN
00..66 CCL
FB
IBM
00..44 INTC
LYFT
00..22 OXY
00..00
00..00 0.2 0.4 0.6 00.2.8 0.0 0.2 0.4 0.04.6 0.8 0.0 0.2 0.60.4 0.6 0.8 0.0 0.80.2 0.4 0.6 0.8 1.0
t
(a)Learntpolicies
=0
1.2 =10 6
=10 2
1.0 =10 1
0.8
0.6
0.4
0.2
0.0
1 0 1 2 3 4
Terminal P&L
(b)DistributionsofterminalP&L
Figure2: P&LdistributionswhenfollowinglearntoptimalpolicieswrtdynamicrobustCVaR .
0.1
=0 =106 =102 =101
11..00
00..88 AAL
AMZN
00..66 CCL
FB
IBM
00..44 INTC
LYFT
00..22 OXY
00..00
00..00 0.2 0.4 0.6 00.2.8 0.0 0.2 0.4 0.04.6 0.8 0.0 0.2 0.60.4 0.6 0.8 0.0 0.80.2 0.4 0.6 0.8 1.0
t
(a)Learntpolicies
2.00 =0
=10 6
1.75 =10 2
1.50 =10 1
1.25
1.00
0.75
0.50
0.25
0.00
1 0 1 2 3 4
Terminal P&L
(b)DistributionsofterminalP&L
Figure3: P&LdistributionswhenfollowinglearntoptimalpolicieswrtdynamicrobustCVaR .
0.2
21
)i(
)i(
t
t
ytisneD
ytisneDRobustRLwithDynamicDistortionRisk
OnelimitationoftheuniversalapproximationtheoremsprovidedinSection5isthatwhileweprovetheexistenceof
arbitrarilyaccurateANNs,wedonotshowhowtoattainthem. Itremainsanopenchallengetoprovidemethodologies
withconvergenceguaranteestothetruedynamicrisk,and,moregenerally,todevelopdeepactor-criticalgorithmswith
convergenceguaranteestotheoptimalpolicy. Otherinterestingavenuestopursueinfutureworkconsistofderiving
RLalgorithmsfordifferentclassesoftime-consistentdynamicrobustriskmeasures,suchasthosewithanuncertainty
setconstructedusingtheKullback-Leiblerdivergence,andformallycomparingrobustRLapproachesavailableinthe
literature.
References
MohammedAminAbdullah,HangRen,HaithamBouAmmar,VladimirMilenkovic,RuiLuo,MingtianZhang,and
JunWang. Wassersteinrobustreinforcementlearning. arXivpreprintarXiv:1907.13196,2019.
BeatriceAcciaioandIrinaPenner. Dynamicriskmeasures. InAdvancedMathematicalMethodsforFinance,pages
1–34.Springer,2011.
MohamadrezaAhmadi, UgoRosolia, MichelDIngham, RichardMMurray, andAaronDAmes. Constrainedrisk-
averseMarkovdecisionprocesses. InThe35thAAAIConferenceonArtificialIntelligence(AAAI-21),2021.
NicoleBäuerleandAlexanderGlauner. Markovdecisionprocesseswithrecursiveriskmeasures. EuropeanJournal
ofOperationalResearch,2021.
CaroleBernard, SilvanaMPesenti, andSteven Vanduffel. Robust distortionriskmeasures. MathematicalFinance,
2023.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław De˛biak, Christy Dennison, David
Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning.
arXivpreprintarXiv:1912.06680,2019.
TomaszRBielecki,IgorCialenco,SamuelDrapeau,andMartinKarliczek. Dynamicassessmentindices. Stochastics,
88(1):1–44,2016.
Tomasz R Bielecki, Igor Cialenco, and Andrzej Ruszczyn´ski. Risk filtering and risk-averse control of Markovian
systemssubjecttomodeluncertainty. MathematicalMethodsofOperationsResearch,98(2):231–268,2023.
Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport. Mathematics of
OperationsResearch,44(2):565–600,2019.
NoamBrownandTuomasSandholm. SuperhumanAIformultiplayerpoker. Science,365(6456):885–890,2019.
Ziteng Cheng and Sebastian Jaimungal. Markov decision processes with Kusuoka-type conditional risk mappings.
arXivpreprintarXiv:2203.09612,2022.
PatrickCheridito,FreddyDelbaen,andMichaelKupper. Dynamicmonetaryriskmeasuresforboundeddiscrete-time
processes. ElectronicJournalofProbability,11:57–106,2006.
PierreClavier,StéphanieAllassonière,andErwanLePennec. Robustreinforcementlearningwithdistributionalrisk-
averseformulation. arXivpreprintarXiv:2206.06841,2022.
AnthonyCoacheandSebastianJaimungal.Reinforcementlearningwithdynamicconvexriskmeasures.Mathematical
Finance,34(2):557–587,2024.
AnthonyCoache, SebastianJaimungal, andAlvaroCartea. Conditionallyelicitabledynamicriskmeasuresfordeep
reinforcementlearning. SIAMJournalonFinancialMathematics,14(4):1249–1289,2023.
22RobustRLwithDynamicDistortionRisk
Christa Cuchiero, Guido Gazzani, and Irene Klein. Risk measures under model uncertainty: a bayesian viewpoint.
arXivpreprintarXiv:2204.07115,2022.
ThomasDegris,MarthaWhite,andRichardSSutton. Off-policyactor-critic. arXivpreprintarXiv:1205.4839,2012.
JanDhaene,AlexanderKukush,DaniëlLinders,andQiheTang. Remarksonquantilesanddistortionriskmeasures.
EuropeanActuarialJournal,2(2):319–328,2012.
HansFöllmerandAlexanderSchied. Stochasticfinance. InStochasticFinance.deGruyter,2016.
Rafael Frongillo and Ian A Kash. Vector-valued property elicitation. In Conference on Learning Theory, pages
710–727.PMLR,2015.
PaulGlassermanandXingboXu. Robustriskmeasurementandmodelrisk. QuantitativeFinance,14(1):29–58,2014.
TilmannGneiting. Makingandevaluatingpointforecasts. JournaloftheAmericanStatisticalAssociation,106(494):
746–762,2011.
Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the
AmericanstatisticalAssociation,102(477):359–378,2007.
HenrykGzylandSilviaMayoral. Onarelationshipbetweendistortedandspectralriskmeasures. 2008.
SebastianJaimungal,SilvanaMPesenti,YeShengWang,andHariomTatsat. Robustrisk-awarereinforcementlearn-
ing. SIAMJournalonFinancialMathematics,13(1):213–226,2022.
Sebastian Jaimungal, Silvana M Pesenti, Yuri F Saporito, and Rodrigo S Targino. Risk budgeting allocation for
dynamicriskmeasures. arXivpreprintarXiv:2305.11319,2023.
VijayRKondaandJohnNTsitsiklis. Actor-criticalgorithms. InAdvancesinNeuralInformationProcessingSystems,
pages1008–1014.Citeseer,2000.
Umit Kose and Andrzej Ruszczynski. Risk-averse learning by temporal difference methods with Markov risk mea-
sures. JournalofMachineLearningResearch,22,2021.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and
DaanWierstra. Continuouscontrolwithdeepreinforcementlearning. arXivpreprintarXiv:1509.02971,2015.
SaeedMarzban,ErickDelage,andJonathanYu-MengLi.Deepreinforcementlearningforoptionpricingandhedging
underdynamicexpectileriskmeasures. QuantitativeFinance,23(10):1411–1430,2023.
DanMikulincerandDanielReichman.Sizeanddepthofmonotoneneuralnetworks:interpolationandapproximation.
AdvancesinNeuralInformationProcessingSystems,35:5522–5534,2022.
PaulMilgromandIlyaSegal. Envelopetheoremsforarbitrarychoicesets. Econometrica,70(2):583–601,2002.
VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiARusu,JoelVeness,MarcGBellemare,AlexGraves,
MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,etal. Human-levelcontrolthroughdeepreinforcement
learning. Nature,518(7540):529–533,2015.
MarlonRMoresco,MélinaMailhot,andSilvanaMPesenti. Uncertaintypropagationanddynamicrobustriskmea-
sures. MathematicsofOperationsResearch,2024.
Silvana M Pesenti and Sebastian Jaimungal. Portfolio optimization within a Wasserstein ball. SIAM Journal on
FinancialMathematics,14(4):1175–1214,2023.
23RobustRLwithDynamicDistortionRisk
GeorgPflugandDavidWozabal. Ambiguityinportfolioselection. QuantitativeFinance,7(4):435–442,2007.
AllanPinkus. Approximationtheoryofthemlpmodelinneuralnetworks. ActaNumerica,8:143–195,1999.
Andrzej Ruszczyn´ski. Risk-averse dynamic programming for Markov decision processes. Mathematical Program-
ming,125(2):235–261,2010.
JosephSill. Monotonicnetworks. AdvancesinNeuralInformationProcessingSystems,10,1997.
DavidSilver,GuyLever,NicolasHeess,ThomasDegris,DaanWierstra,andMartinRiedmiller. Deterministicpolicy
gradientalgorithms. InInternationalConferenceonMachineLearning,pages387–395.PMLR,2014.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot,
LaurentSifre, DharshanKumaran, ThoreGraepel, etal. Ageneralreinforcementlearningalgorithmthatmasters
chess,shogi,andGothroughself-play. Science,362(6419):1140–1144,2018.
ElenaSmirnova,ElvisDohmatob,andJérémieMary. Distributionallyrobustreinforcementlearning. arXivpreprint
arXiv:1902.08708,2019.
HadoVanHasselt,ArthurGuez,andDavidSilver. DeepreinforcementlearningwithdoubleQ-learning. InProceed-
ingsofthe30thAAAIConferenceonArtificialIntelligence,2016.
Julia L Wirch andMary R Hardy. Distortion risk measures: Coherence and stochastic dominance. In International
CongressonInsurance: MathematicsandEconomics,pages15–17,2001.
DavidWuandSebastianJaimungal. Robustrisk-awareoptionhedging. AppliedMathematicalFinance,30(3):153–
174,2023.
MenahemEYaari. Thedualtheoryofchoiceunderrisk. Econometrica: JournaloftheEconometricSociety,pages
95–115,1987.
A Proofs
A.1 ProofofProposition3.2
Proof. Since we are working with distortion risk measures and the Wasserstein distance, both components of the
optimizationprobleminEq.(3.3)canbeexpressedintermsofquantilefunctionsinsteadofrandomvariables. Indeed,
wehave
(cid:26) (cid:13) (cid:13) (cid:27)
φϵs = F˘ ∈F˘ : (cid:13)F˘(·|s,a)−F˘ (·|s,a)(cid:13)≤ϵ
F˘
Ztθ(·|s,a)
(cid:13) Z tθ (cid:13) s
andtheone-stepconditionaldistortionriskmeasure
D E
γ ,F˘ (·|s,a) .
s Zϕ
t
Therefore,wehavetheequivalencerelationship
(cid:16) (cid:12) (cid:17) D E
esssup ρgs Zϕ (cid:12)s =s,a =a ≡ esssup γ ,F˘ (·|s,a) , (A.1)
t t (cid:12) t t s ϕ
Z tϕ∈φϵ Zs
tθ
F˘ ϕ∈φϵ F˘s
Ztθ(·|s,a)
wheretheequivalenceistobeunderstoodas:(i)thequantilefunctionofanyoptimalrandomvariablefortheleft-hand
side of Eq. (A.1) is optimal for the right-hand side; and (ii) any random variable with an optimal quantile function
fortheright-handsideofEq.(A.1)isoptimalfortheleft-handside. Here,weremarkthat,asopposedtotheoriginal
24RobustRLwithDynamicDistortionRisk
formulation of the problem, the optimization problem on the right-hand side is convex over the space of quantile
functions. WeusetheLagrangemultipliermethodtofindtheoptimalsolution. Wehave
L(F˘ ,λ;θ)
ϕ
D E (cid:18)(cid:13) (cid:13)2 (cid:19)
= γ ,F˘ (·|s,a) −λ (cid:13)F˘ (·|s,a)−F˘ (·|s,a)(cid:13) −ϵ2
s ϕ (cid:13) ϕ Zθ (cid:13) s
t
(cid:13) (cid:16) γ (cid:17)(cid:13)2
[squarecompletion] =−λ(cid:13) (cid:13)F˘ ϕ(·|s,a)− F˘ Z tθ(·|s,a)+ 2λs (cid:13) (cid:13)
(cid:13) (cid:13)2 (A.2)
(cid:18) (cid:13) (cid:13)2(cid:19) (cid:13) (cid:13)2λF˘ Zθ(·|s,a)+γ s(cid:13) (cid:13)
+λ ϵ2−(cid:13)F˘ (·|s,a)(cid:13) + t .
s (cid:13) Z tθ (cid:13) 4λ
UsingSlater’sconditionandtheconvexityofthequantilerepresentationproblem,strongdualityholds:
Q (s,a;θ)= maxminL(F˘ ,λ;θ)=minmaxL(F˘ ,λ;θ).
t ϕ ϕ
F˘ ϕ∈F˘λ>0 λ>0F˘ ϕ∈F˘
SinceonlythefirstintegralinEq.(A.2)actuallydependsonF˘ ,theinneroptimizationproblemisattainedforagiven
ϕ
λbytheisotonicprojection
(cid:18)
γ
(·)(cid:19)↑
F˘∗(·|s,a)= F˘ (·|s,a)+ s .
ϕ Z tθ 2λ
Finally,fortheouterproblem,theWassersteinconstraintisbinding,andthustheoptimalλ∗isthepositivevaluesuch
that
(cid:13) (cid:13)
(cid:13)F˘∗(·|s,a)−F˘ (·|s,a)(cid:13)=ϵ ,
(cid:13) ϕ Zθ (cid:13) s
t
whichgivesthedesiredresult.
A.2 ProofofTheorem3.5
Proof. SimilarlytoProposition3.2,weusetheLagrangemultipliermethodtofindtheoptimalsolution. Inthisproof,
weremovethedependenceonthestate-actionpairforreadability. Bysquarecompletion,wehave
L(F˘ ,λ,ζ,η;θ)
ϕ
D E (cid:18)(cid:13) (cid:13)2 (cid:19) (cid:18)(cid:13) (cid:13)2 (cid:13) (cid:13)2(cid:19) (cid:18)D E (cid:19)
= γ ,F˘ −λ (cid:13)F˘ −F˘ (cid:13) −ϵ2 −ζ (cid:13)F˘ (cid:13) −(cid:13)F˘ (cid:13) −η F˘ ,1 −µ
s ϕ (cid:13) ϕ Zθ(cid:13) s (cid:13) ϕ(cid:13) (cid:13) Zθ(cid:13) ϕ
t t
(cid:13) (cid:13)2
=−(λ+ζ)(cid:13) (cid:13) (cid:13)F˘ − 2λF˘ Z tθ +γ s−η(cid:13) (cid:13) (cid:13)2 +(ζ−λ)(cid:13) (cid:13)F˘ (cid:13) (cid:13)2 +λϵ2+ηµ+ (cid:13) (cid:13)2λF˘ Z tθ +γ s−η(cid:13) (cid:13) .
(cid:13)
(cid:13)
ϕ 2(λ+ζ) (cid:13)
(cid:13)
(cid:13) Z tθ(cid:13) s 4(λ+ζ)
UsingSlater’sconditionandtheconvexityofthequantilerepresentationproblem,strongdualityholds. Theoptimal
quantilefunctionthushasthefollowingform:
2λF˘ +γ −η λF˘ +γ −a
F˘∗ = Z tθ s = Z tθ s λ . (A.3)
ϕ 2(λ+ζ) b
λ
FromtheLagrangianconstraintonthefirstmoment,weget
D E
F˘∗,1 =µ =⇒ a =(λµ+1)−b µ.
ϕ λ λ
25RobustRLwithDynamicDistortionRisk
FromtheLagrangianconstraintonthesecondmoment,weobtain
(cid:13) (cid:13)2 (cid:13) (cid:13)2 (cid:13) (cid:13)2
(cid:13)F˘∗(cid:13) =(cid:13)F˘ (cid:13) =⇒ b2σ2 =(cid:13)λF˘ +γ (cid:13) −(λµ+1)2
(cid:13) ϕ(cid:13) (cid:13) Zθ(cid:13) λ (cid:13) Zθ s(cid:13)
t t
r
(cid:16) (cid:17)
(λσ)2+σ2+2λ ⟨F˘ ,γ ⟩−µ
γ Zθ s
t
=⇒ b = .
λ σ
Next,letK =σ2− ϵ2 s. FromtheLagrangianconstraintontheWassersteindistance,weget
2
(cid:13) (cid:13)2
(cid:13)F˘∗−F˘ (cid:13) =ϵ2
(cid:13) ϕ Zθ(cid:13)
t
(cid:13) (cid:13)2 (cid:13) (cid:13)2 D E
=⇒ (cid:13)F˘∗(cid:13) +(cid:13)F˘ (cid:13) −2 F˘∗,F˘ =ϵ2
(cid:13) ϕ(cid:13) (cid:13) Zθ(cid:13) ϕ Zθ
t t
(cid:13) (cid:13)2
λ(cid:13)F˘ (cid:13) +⟨F˘ ,γ ⟩−a µ
(cid:13) Zθ(cid:13) Zθ s λ
=⇒ K = t t −µ2
b
λ
=⇒ b K =λσ2+⟨F˘ ,γ ⟩−µ
λ Zθ s
t
(cid:16) (cid:16) (cid:17)(cid:17) (cid:16) (cid:17)2
=⇒ K2 (λσ)2+σ2+2λ ⟨F˘ ,γ ⟩−µ =σ2 λσ2+⟨F˘ ,γ ⟩−µ
γ Zθ s Zθ s
t t
(cid:16) (cid:16) (cid:17)(cid:17)
=⇒ K2 λ2σ2+σ2+2λ ⟨F˘ ,γ ⟩−µ
γ Zθ s
t
(cid:18) (cid:16) (cid:17)2 (cid:16) (cid:17)(cid:19)
=σ2 λ2σ4+ ⟨F˘ ,γ ⟩−µ +2λσ2 ⟨F˘ ,γ ⟩−µ
Zθ s Zθ s
t t
(cid:16) (cid:17) (cid:16) (cid:17)(cid:16) (cid:17) (cid:16) (cid:16) (cid:17)2(cid:17)
=⇒ λ2 K2σ2−σ6 +2λ ⟨F˘ ,γ ⟩−µ K2−σ4 + K2σ2−σ2 ⟨F˘ ,γ ⟩−µ =0
Zθ s γ Zθ s
t t
(cid:16) (cid:17)2
K2σ2−σ2 ⟨F˘ ,γ ⟩−µ
=⇒ λ2σ2+2λ(cid:16) ⟨F˘ ,γ ⟩−µ(cid:17) + γ Z tθ s =0. (A.4)
Z tθ s K2−σ4
ThediscriminantofEq.(A.4),aquadraticequationinλ,is
 (cid:16) (cid:17)2 
σ2 ⟨F˘ ,γ ⟩−µ −K2σ2
∆=4(cid:16) ⟨F˘ ,γ ⟩−µ(cid:17)2 +σ2 Z tθ s γ ,
 Z tθ s K2−σ4 
mustbenonnegative. Indeed,usingtheCauchy-Schwarzinequality,wehaverespectively
(cid:16)D E (cid:17)2 (cid:16) (cid:17)2
K2−σ4 = F˘∗,F˘ −µ2 −σ4 ≤ ∥F˘∗∥∥F˘ ∥−µ2 −σ4 =0,
ϕ Zθ ϕ Zθ
t t
and
(cid:16) (cid:17)2 (cid:16) (cid:17)2 (cid:18)(cid:16) (cid:17)2 (cid:19)
σ2 ⟨F˘ ,γ ⟩−µ −K2σ2 ≤σ2 ⟨F˘ ,γ ⟩−µ −σ4σ2 =σ2 ⟨F˘ ,γ ⟩−µ −σ2σ2 ≤0.
Zθ s γ Zθ s γ Zθ s γ
t t t
Therefore,thequadraticequationinEq.(A.4)hastworoots,whichonlyoneispositive,moreprecisely
(cid:16) (cid:17) √
−2 ⟨F˘ ,γ ⟩−µ + ∆
Zθ s
λ∗ = t .
2σ2
Iftheuncertaintytoleranceϵ islargeenoughandsatisfiesEq.(3.5),thenµ+γs(u)−1 solvestheoptimizationproblem
s b0
Q (s,a;θ). Indeed,itisadmissibleandforallλ>0,usingtheCauchy-Schwarzinequality,wehavethat
t
(cid:28) (cid:29)
γ (u)−1
γ ,µ+ s =µ+σσ
s b γ
0
26RobustRLwithDynamicDistortionRisk
D (cid:16) (cid:17) E (cid:16) (cid:17)
γ , λF˘ +γ −(λµ+1) * λF˘ +γ −(λµ+1)+
s Zθ s Zθ s
≥µ+σ t = γ ,µ+ t .
r (cid:13) (cid:13)λF˘ +γ (cid:13) (cid:13)2 −(λµ+1)2 s b λ
(cid:13) Zθ s(cid:13)
t
Thisconcludestheproof.
A.3 ProofofProposition3.6
Proof. UsingthequantilerepresentationinEq.(A.1)andstrongdualityfromTheorem3.5,wehave
∇ V (s;θ)=∇ min maxL(F˘ ,λ,ζ,η;θ).
θ t θ ϕ
λ,ζ,η>0F˘ ϕ∈F˘
We apply the Envelope theorem for saddle-point problems (Milgrom and Segal, 2002), which differs from standard
results by considering arbitrary choice sets instead of convex ones. All conditions of the theorem hold, because the
optimization problem is convex over the space of quantile functions: (i) L(F˘ ,λ,ζ,η;θ) is absolutely continuous
ϕ
in (F˘ ,λ,ζ,η), because of the convexity and the fact that a distortion risk measure is monetary, and thus Lipschitz
ϕ
andabsolutelycontinuous; (ii)∇ L(F˘ ,λ,ζ,η;θ)iscontinuousandboundedateach(F˘ ,λ,ζ,η), sinceLisLips-
θ ϕ ϕ
chitz; (iii)thereexistsatleastonesaddle-point, asshowninProposition3.2; and(iv){L(F˘ ,λ,ζ,η;θ)} is
ϕ (F˘ ϕ,λ,ζ,η)
equidifferentiableinθ,i.e. itsderivativewrtθconvergesuniformly. Thisleadsto
(cid:12)
∇ V (s;θ)=∇ L(F˘ ,λ,ζ,η;θ)(cid:12)
θ t θ ϕ (cid:12)
(F˘ ϕ=F˘ ϕ∗,λ=λ∗,ζ=ζ∗,η=η∗)
D E (cid:16)(cid:13) (cid:13)2 (cid:17)
=∇ γ ,F˘ −λ (cid:13)F˘ −F˘ (cid:13) −ϵ2
θ s ϕ (cid:13) ϕ Zθ(cid:13) s
t
!(cid:12) (A.5)
(cid:16)(cid:13) (cid:13)2 (cid:13) (cid:13)2(cid:17) (cid:16)D E (cid:17) (cid:12)
−ζ (cid:13)F˘ (cid:13) −(cid:13)F˘ (cid:13) −η F˘ ,1 −µ (cid:12) .
(cid:13) ϕ(cid:13) (cid:13) Zθ(cid:13) ϕ (cid:12)
t (cid:12)
(F˘∗,λ∗,ζ∗,η∗)
ϕ
ForthefirsttermofEq.(A.5),wehave
D E(cid:12)
∇ γ ,F˘ (cid:12)
θ s ϕ (cid:12)
(F˘∗,λ∗,ζ∗,η∗)
ϕ
Z 1 (cid:12)
=∇ γ (u)F˘ (u|s ,πθ(s ))du(cid:12)
θ s ϕ t t (cid:12)
0
(F˘ ϕ∗,λ∗,ζ∗,η∗)
Z 1 (cid:12) (cid:12)
=∇ γ (u)F˘∗(u|s ,a)du(cid:12) ∇ πθ(s)=∇ Q (s,a;θ)(cid:12) ∇ πθ(s). (A.6)
a s ϕ t (cid:12) θ a t (cid:12) θ
0 a=πθ(st) a=πθ(s)
ForthesecondtermofEq.(A.5),weget
−∇ θλ(cid:18)(cid:13) (cid:13) (cid:13)F˘ ϕ−F˘
Z
tθ(cid:13) (cid:13) (cid:13)2 −ϵ2 s(cid:19)(cid:12) (cid:12) (cid:12)
(cid:12)
(F˘∗,λ∗,ζ∗,η∗)
ϕ
=−∇ θλ(cid:18)Z 01(cid:16) F˘ ϕ(u|s t,πθ(s t))−F˘
Z
tθ(u|s t,πθ(s t))(cid:17)2 du−ϵ2 s(cid:19)(cid:12) (cid:12) (cid:12)
(cid:12)
(F˘∗,λ∗,ζ∗,η∗)
ϕ
!
Z 1
=−2λ∗ F˘∗(u|s ,πθ(s ))−F˘ (u|s ,πθ(s ))
ϕ t t Zθ t t
0 t
(cid:16) (cid:17) (cid:12)
×∇ F˘ (u|s ,πθ(s ))−F˘ (u|s ,πθ(s )) du(cid:12)
θ ϕ t t Z tθ t t (cid:12) (F˘∗,λ∗,ζ∗,η∗)
ϕ
=−2λ∗(cid:0) ∇ πθ(s)(cid:1)Z 1(cid:16) F˘∗(u|s ,πθ(s ))−F˘ (u|s ,πθ(s ))(cid:17)
θ ϕ t t Zθ t t
0 t
27RobustRLwithDynamicDistortionRisk
(cid:16) (cid:17)(cid:12)
×∇ F˘∗(u|s ,a)−F˘ (u|s ,a) (cid:12) du
a ϕ t Zθ t (cid:12)
t a=πθ(st)
Z 1 2λ∗F˘ (u|s ,πθ(s ))+γ (u)−η∗ !
=−2λ∗(cid:0) ∇ πθ(s)(cid:1) Z tθ t t s −F˘ (u|s ,πθ(s ))
θ
0
2(λ∗+ζ∗) Z tθ t t
(cid:18) 2λ∗ (cid:19) (cid:12)
× −1 ∇ F˘ (u|s ,a)(cid:12) du
2(λ∗+ζ∗) a Z tθ t (cid:12) a=πθ(st)
= 4λ∗ζ∗ (cid:0) ∇ πθ(s)(cid:1)Z 1(cid:16) γ (u)−η∗−2ζ∗F˘ (u|s ,πθ(s ))(cid:17) ∇ F˘ (u|s ,a)(cid:12) (cid:12) du. (A.7)
4(λ∗+ζ∗)2 θ 0 s Z tθ t t a Z tθ t (cid:12) a=πθ(st)
ForthethirdtermofEq.(A.5),wehave
(cid:18)(cid:13) (cid:13)2 (cid:13) (cid:13)2(cid:19)(cid:12)
−∇ ζ (cid:13)F˘ (cid:13) −(cid:13)F˘ (cid:13) (cid:12)
θ (cid:13) ϕ(cid:13) (cid:13) Z tθ(cid:13) (cid:12) (F˘∗,λ∗,ζ∗,η∗)
ϕ
Z 1(cid:16) (cid:17)2 (cid:16) (cid:17)2 (cid:12)
=−∇ ζ F˘ (u|s ,πθ(s )) − F˘ (u|s ,πθ(s )) du(cid:12)
θ
0
ϕ t t Z tθ t t (cid:12) (F˘ ϕ∗,λ∗,ζ∗,η∗)
=−2ζ∗(cid:0) ∇ πθ(s)(cid:1)Z 1 ∇ (cid:16) F˘∗(u|s ,πθ(s ))F˘∗(u|s ,a)−F˘ (u|s ,πθ(s ))F˘ (u|s ,a)(cid:17)(cid:12) (cid:12) du
θ a ϕ t t ϕ t Zθ t t Zθ t (cid:12)
0 t t a=πθ(st)
(cid:16) (cid:17)
=−2ζ∗(cid:0) ∇
πθ(s)(cid:1)Z 1 2λ∗F˘
Z
tθ(u|s t,πθ(s t))+γ s(u)−η∗ 2λ∗∇ aF˘
Z
tθ(u|s t,a)(cid:12)
(cid:12) du
θ 0 4(λ∗+ζ∗)2 (cid:12) a=πθ(st)
Z 1 (cid:12)
+2ζ∗(cid:0) ∇ πθ(s)(cid:1) F˘ (u|s ,πθ(s ))∇ F˘ (u|s ,a)(cid:12) du
θ Zθ t t a Zθ t (cid:12)
0 t t a=πθ(st)
= −4λ∗ζ∗ (cid:0) ∇ πθ(s)(cid:1)Z 1(cid:16) 2λ∗F˘ (u|s ,πθ(s ))+γ (u)−η∗(cid:17) ∇ F˘ (u|s ,a)(cid:12) (cid:12) du
4(λ∗+ζ∗)2 θ 0 Z tθ t t s a Z tθ t (cid:12) a=πθ(st) (A.8)
Z 1 (cid:12)
+2ζ∗(cid:0) ∇ πθ(s)(cid:1) F˘ (u|s ,πθ(s ))∇ F˘ (u|s ,a)(cid:12) du.
θ Zθ t t a Zθ t (cid:12)
0 t t a=πθ(st)
ForthefourthtermofEq.(A.5),wehave
−∇ θη(cid:18)D F˘ ϕ,1E −µ(cid:19)(cid:12) (cid:12) (cid:12)
(cid:12)
(F˘∗,λ∗,ζ∗,η∗)
ϕ
Z 1 (cid:12)
=−∇ η F˘ (u|s ,πθ(s ))−F˘ (u|s ,πθ(s ))du(cid:12)
θ
0
ϕ t t Z tθ t t (cid:12) (F˘ ϕ∗,λ∗,ζ∗,η∗)
=−η∗(cid:0) ∇ πθ(s)(cid:1)Z 1 ∇ (cid:16) F˘∗(u|s ,a)−F˘ (u|s ,a)(cid:17)(cid:12) (cid:12) du
θ a ϕ t Zθ t (cid:12)
0 t a=πθ(st)
=−η∗(cid:0) ∇
πθ(s)(cid:1)Z 1(cid:18) 2λ∗ −1(cid:19)
∇ F˘ (u|s
,a)(cid:12)
(cid:12) du
θ 0 2(λ∗+ζ∗) a Z tθ t (cid:12) a=πθ(st)
=
2η∗ζ∗
(cid:0) ∇
πθ(s)(cid:1)Z 1
∇ F˘ (u|s
,a)(cid:12)
(cid:12) du. (A.9)
2(λ∗+ζ∗) θ 0 a Z tθ t (cid:12) a=πθ(st)
CombiningalltermsinEqs.(A.6)to(A.9)together,weget
∇ V (s;θ)
θ t
(cid:12)
=∇ Q (s,a;θ)(cid:12) ∇ πθ(s)
a t (cid:12) θ
a=πθ(s)
+(cid:18) 2ζ∗− 4λ∗ζ∗ (cid:19) (cid:0) ∇ πθ(s)(cid:1)Z 1(cid:16) F˘ (u|s ,πθ(s ))(cid:17) ∇ F˘ (u|s ,a)(cid:12) (cid:12) du
2(λ∗+ζ∗) θ 0 Z tθ t t a Z tθ t (cid:12) a=πθ(st)
+
2η∗ζ∗
(cid:0) ∇
πθ(s)(cid:1)Z 1
∇ F˘ (u|s
,a)(cid:12)
(cid:12) du
2(λ∗+ζ∗) θ 0 a Z tθ t (cid:12) a=πθ(st)
28RobustRLwithDynamicDistortionRisk
(cid:12)
=∇ πθ(s) ∇ Q (s,a;θ)(cid:12)
θ a t (cid:12)
a=πθ(s)
!
2ζ∗ Z 1(cid:16) (cid:17) (cid:12)
+ 2ζ∗F˘ (u|s ,πθ(s ))+η∗ ∇ F˘ (u|s ,a)(cid:12) du .
2(λ∗+ζ∗) 0 Z tθ t t a Z tθ t (cid:12) a=πθ(st)
UsingthenotationinTheorem3.5withλ,b ,a ,especiallyEq.(A.3),andinterpretingtheintegralasanexpectation
λ λ
overauniformrandomvariable,wegetthegradientofthevaluefunctionand,hence,theQ-function:
∇ V (s;θ)
θ t
(cid:12)
=∇ πθ(s) ∇ Q (s,a;θ)(cid:12)
θ a t (cid:12)
a=πθ(s)
!
b −λ∗ Z 1(cid:16) (cid:17) (cid:12)
+ λ∗ (b −λ∗)(F˘ (u|s ,πθ(s ))−µ)+1 ∇ F˘ (u|s ,a)(cid:12) du
b λ∗ 0 λ∗ Z tθ t t a Z tθ t (cid:12) a=πθ(st)
(cid:12)
=∇ πθ(s) ∇ Q (s,a;θ)(cid:12)
θ a t (cid:12)
a=πθ(s)
b −λ∗ (cid:20)(cid:16) (cid:17) (cid:12) (cid:21)!
− λ∗ E (b −λ∗)(Zθ−µ)+1 ∇ F˘ (x|s,a)(cid:12)
b λ∗ t,s λ∗ t a Z tθ (cid:12) (x,a)=(Z tθ,πθ(s))
(cid:12)
=∇ πθ(s) ∇ Q (s,a;θ)(cid:12)
θ a t (cid:12)
a=πθ(s)
− b λ∗
b
λ− ∗λ∗ E t,s" (cid:16) (b λ∗ −λ∗)(Z tθ−µ)+1(cid:17) ∇ fa ZF θZ (tθ x( |x s| ,s a, )a)(cid:12) (cid:12) (cid:12)
(cid:12)
(x,a)=(Zθ,πθ(s))#!
t t
(cid:12)
=∇ πθ(s) ∇ Q (s,a;θ)(cid:12)
θ a t (cid:12)
a=πθ(s)
− b λ∗
b
λ− ∗λ∗ E t,s" (cid:16) (b λ∗ −λ∗)(Z tθ−µ)+1(cid:17) ∇∇ xaF FZ Ztθ θ( (x x| |s s, ,a a) )(cid:12) (cid:12) (cid:12)
(cid:12)
(x,a)=(Zθ,πθ(s))#! .
t t
Here,wewritethegradientofaquantilefunctionbyinsteadconsideringthegradientoftheCDF.Indeed,foranyCDF
F ,usingthefactthatF (F˘ (u))=uandthechainruletoexpandthegradientintermsofthepartialderivatives,we
θ θ θ
have
∇ F (F˘ (u))=∇ u
θ θ θ θ
(cid:12) (cid:12)
⇐⇒ ∇ F (x)(cid:12) ∇ F˘ (u)+∇ F (x)(cid:12) =0
x θ (cid:12) θ θ θ θ (cid:12)
x=F˘ θ(u) x=F˘ θ(u)
(cid:12)
∇ F (x)(cid:12)
θ θ (cid:12)
⇐⇒ ∇ F˘ (u)=− x=F˘ θ(u) .
θ θ f (F˘ (u))
θ θ
29