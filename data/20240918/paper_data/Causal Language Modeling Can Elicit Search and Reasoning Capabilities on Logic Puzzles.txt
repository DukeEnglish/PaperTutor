Causal Language Modeling Can Elicit Search and Reasoning
Capabilities on Logic Puzzles
KulinShah* NishanthDikkala
UTAustin GoogleResearch
kulinshah@utexas.edu nishanthd@google.com
XinWang RinaPanigrahy
GoogleResearch GoogleResearch
wanxin@google.com rinap@google.com
September17,2024
Abstract
CausallanguagemodelingusingtheTransformerarchitecturehasyieldedremarkablecapabilities
inLargeLanguageModels(LLMs)overthelastfewyears. However,theextenttowhichfundamental
searchandreasoningcapabilitiesemergedwithinLLMsremainsatopicofongoingdebate. Inthiswork,
westudyifcausallanguagemodelingcanlearnacomplextasksuchassolvingSudokupuzzles. Tosolve
aSudoku,themodelisfirstrequiredtosearchoverallemptycellsofthepuzzletodecideonacelltofill
andthenapplyanappropriatestrategytofillthedecidedcell. Sometimes,theapplicationofastrategy
onlyresultsinthinningdownthepossiblevaluesinacellratherthanconcludingtheexactvalueofthe
cell. Insuchcases,multiplestrategiesareappliedoneaftertheothertofillasinglecell. Weobserve
thatTransformermodelstrainedonthissynthetictaskcanindeedlearntosolveSudokus(ourmodel
solves 94.21% of the puzzles fully correctly) when trained on a logical sequence of steps taken by a
solver. WefindthattrainingTransformerswiththelogicalsequenceofstepsisnecessaryandwithout
suchtraining,theyfailtolearnSudoku. WealsoextendouranalysistoZebrapuzzles(knownasEinstein
puzzles)andshowthatthemodelsolves92.04%ofthepuzzlesfullycorrectly. Inaddition,westudythe
internalrepresentationsofthetrainedTransformerandfindthatthroughlinearprobing,wecandecode
informationaboutthesetofpossiblevaluesinanygivencellfromthem,pointingtothepresenceofa
strongreasoningengineimplicitintheTransformerweights1.
1 Introduction
Language models using the Transformer architecture [VSP+17] have displayed remarkable abilities on a
varietyofMachineLearningtasksoverthelastfewyears[BMR+20,RWC+19]. Trainedwithsimplythe
taskofpredictingthenexttokenonhugeamountsoftext,thesemodelsdisplayhighlyperformantanddeep
languageunderstandingskills. Inordertomakeprogressonachievingahuman-likeartificialintelligence,
one of the most important ability is the ability to perform human-like reasoning and planning. Although
LLMshavedisplayedaseeminglyremarkableabilitytoexcelatreasoningandplanningtasksaswell,itisa
*WorkdoneduringaninternshipatGoogleResearch.
1Thecodewillbeavailableathttps://github.com/kulinshah98/llm-reasoning-logic-puzzles
1
4202
peS
61
]GL.sc[
1v20501.9042:viXraongoingdebateastowhetherthisabilitycomesfromatrueunderstandingandreasoningoftheunderlying
problemorsomeotherprocesswhichsimulatesreasoningbutcanbehighlybrittle. Forinstance,although
LLMsshowremarkableperformanceonbenchmarksrequiringnon-trivialreasoningandplanningskillssuch
asMATH[HBK+21],HumanEval[CTJ+21]andothers,thereisresearchshowingthattheseabilitiescanbe
extremelybrittleorworse,themodelissimplyperforming‘approximateretrieval’[VOSK22,DLS+24].
Inthiswork,weaimtounderstandhowcomplexareasoningtaskcanTransformerstrainedwithnext-token
predictionsolvebyfocusingonasetofsynthetictasks: logicpuzzles. Inthisworkwefocusouranalysison
twotypesoflogicpuzzles: SudokupuzzlesandZebrapuzzles.
• Sudoku. IntheclassicvariantofSudoku,wearegivena9×9gridwhereeachcellistobeoccupiedby
anumberintherange{1,2,...,9}. Theconstraintsarethatthenumbersalongeachrowandcolumn
shouldbeunique. Inaddition,thenumberswithineach3×3mini-gridshouldalsobeunique. Givena
setofinitiallyfilledpositions,thegoalistofigureoutthevaluesthatcanoccurintheunfilledcells. In
standardSudokupuzzles,therewillalwaysonlyexistauniquesolutiontothepuzzle.
• ZebraPuzzles. Thereareamoreverbalstyleofapuzzle(Figure1)whereweneedtofillinvalues
inagridagainbutthistimethetypeofpossibleconstraintsismuchricher. Thesearealsoknownas
Einsteinriddles.
Focusingonsynthetictaskslikethisgivesaprecisehandleonwhatdatathemodelhasseen,andallowsusto
alsocontrolthedifficultyofreasoningrequiredforthetask,seee.g. [LHB+23,AZL23,LSL+23,LAG+22].
Thereare3peoplenexttoeachotherinarow. Everyonehasadifferentname: Ali,Rose,Randy. Everyone
livesinadifferentcoloredhouse: gold,silver,indigo. Everyonelikesadifferentdrink: orangejuice,beer,
coffee. Matchthepeopletothecorrectvalueforeachoftheircharacteristicsusingtheclues.
1. Thepersonwholikesorangejuiceisimmediatelytotheleftofthepersonwholikescoffee.
2. Thepersonwholikesbeerissomewheretotheleftofthepersonwholivesintheindigohouse.
3. Thepersonatthe1stpositionisRose.
4. Randyisnotthepersonwholikesorangejuice.
5. Randyisthepersonwholivesinthegoldhouse.
Figure1: AnexampleZebrapuzzlewith3entities,eachhaving3attributes.
PriorworkshavestudiedhowcausallanguagemodelingwithTransformersperformsonsynthetictaskssuch
as learning how to make valid moves in Othello, learning context-free grammars, learning deterministic
finiteautomataandlearningspecificalgorithmictasks[LAG+22,LHB+23,NLW23,AZL23,YXLAZ24a,
YXLAZ24b]. Comparedtothese,Sudokupuzzlespresentamorechallengingtask. TheSudokuenvironment
isahighlychallengingConstraintSatisfactionProblem(CSP)anddeterminingthevalueinevenasingle
cellcanrequirehighlycomplexreasoninginvolvingmultiplesteps. Ingeneraltheextensionofthepuzzle
ton×ngridsisknowntobeNP-complete[YS03]. SameisthecaseforZebrapuzzles. However,wewill
consideraclassoflogicpuzzleswhichcanbesolvedinpolynomialtime. Thisclassstillremainsnon-trivial
tolearn. Foranideaofhowchallengingthesecanbe,weperformedasmallexperimentonhowwellsome
frontierLLM’softodaycansolveSudokupuzzles. Wepromptedthemina4-shotmannerwith4Sudoku
puzzles (we serialize a puzzle by converting it into a sequence of (row, column, value) triplets) and their
2corresponding solutions given before asking for the solution for a 5th test Sudoku puzzle. We evaluated
3000examplesonGemini-1.5ProandGPT-4o. Weobservedthatneithermodelsareabletosolveanyofthe
puzzlesfullycorrectly. TheresultsaresummarizedinTable1.
Model Percentofpuzzlessolvedfully Percentofcellsansweredcorrectly
GPT-4o 0% 9.5%
Gemini-1.5Pro 0% 10.2%
Table1: Resultsof4-shotwithCoTpromptingonSudokusolvingbytwoofthefrontierLLMmodels. They
solved0%ofthepuzzlescompletelyrightandtheiraccuracyonapercellbasiswasaround9-10%(closeto
randomguessing).
1.1 Oursetup
WetreateachSudoku/Zebrapuzzleasasequencetosequenceproblem. InaSudoku,giventhesequenceof
filledcellpositionsandtheirvalues,themodelneedstooutputthesequenceofunfilledcellpositionsand
theircorrespondingvalues. SimilarlyinaZebrapuzzle,wearegivenallthecluesandthepossiblevaluesfor
thecharacteristicsinasequentialmannerandweneedtopredictthevaluesinthegrid. Tofocusattention
on a model’s reasoning abilities, we abstract out symbolic versions of the Zebra puzzles. This means we
refertoeachpersonasanentityindexedbyanumberandtheirfavoritecolororcarbecomesanattribute
number. Forbothtypesofpuzzles,clearly,theorderinwhichthemodeloutputsthesequenceoffilledcells
doesn’tmatteraslongasthevaluesarecorrect. However,wewillseethattheorderinwhichthesolutions
arepresentedtothemodelduringtrainingmakesasignificantdifferenceinthefinalperformanceofthemodel.
We consider a dataset of Sudoku puzzles of varying difficulty levels from [Rad20]. In addition, we use a
Sudokusolverwhichemploysasetof7strategiesthathumanscommonlyuseforsolvingSudokus. Given
thesesetof7strategiesthesolveriterativelyscansthroughallunfilledcellsandchecksifprogresscanbe
madeusingoneormoreofthestrategies. Ifitfindsacellwhereprogresscanbemadeiffillsinitsvalueand
repeatstheprocessofsearchingforthenextcelltofill. Althoughsomeofthe7strategiesaresimpleand
direct,someofthemarehighlynon-trivialandnon-local. Fromthedataset,wefilteroutthosepuzzleswhich
cannotbesolvedbyoursolverandendupwith1.9Mexamples. Thisensuresthatallourpuzzlesaresolvable
inpolynomialtime2.
WecancharacterizethesizeofaZebrapuzzlebyatupleoftwonumbers: thenumberofentitiesandthe
numberofattributes. Eachclueinapuzzleisoneof7differenttypes. Wegeneratearound320,000Zebra
puzzlesofsizesvaryingbetween(3,3)to(6,6)inthefollowingmanner. Wefirstdesignahuman-likesolver
forthesepuzzleswhichtriestosolvethepuzzlesinaniterativemannerwithoutbacktracking. Thissolver
runs in time cubic in the number of clues of the puzzle. When generating a puzzle of a certain size, we
iterativelykeepaddingcluestoacluesetuntiloursolverisabletosolvethepuzzle. Thisway,wecanensure
that,similartoourSudokupuzzles,alloursampledZebrapuzzlesarealsosolvableefficiently. Notethat,
eveninthesymbolicformat,thereareanexponentialnumberofpuzzlespossibleimplyingthatourtrainset
andtestsetwon’toverlapwithaveryhighprobability.
2Eachofthe7strategiesthesolverusescanbegeneralizedtoageneraln×ngridandtheycanbeappliedinpoly(n)time
31.2 OurResults
Inthissection,weprovideanoverviewofourresults. WemainlyfocusontheSudokupuzzlestoexplainour
resultsandincludeabriefdiscussionontheZebrapuzzles. Themoredetailedresultsareprovidedinthe
respectivesections.
OurfirstexperimentstudieswhetheraTransformeriscapableofsolvingtheSudokupuzzleinafixedcell
order(fromtop-lefttobottom-right). Thiswouldamounttothemodelknowingwhatvaluestofillineach
unfilledcellinasingleforwardpass. Weobservethatalthoughthemodellearnstopredictthevaluesfor
somecellsinapuzzle(averagecellaccuracy58.64%acrossallunfilledcells),ingeneral,thisleadstopoor
accuracyofsolvingthecompletepuzzle(7.2%).
Observe that solving a Sudoku puzzle can be thought of as finding easy-to-decode cells and then finding
correctvalueatsuchcells. WecombinethisobservationwithinsightsfromChain-of-Thoughtpromptingand
useoursolvertoprovidetheordertofillcellsforagivenpuzzle. Inthissetting,weusethecellpositions
providedbythesolverduringthedecoding(i.e.,positionhintsofeasytodecodecells)andcalculatehow
manycellvaluesthemodelgetsright. Inotherwords,givenaprefixofapartiallysolvedpuzzle,wequery
thesolvertofindoutthe"easiest"cellpositiontosolvenextandthen,conditioningonthisposition,querythe
modelforitsvalue. Theaveragecellaccuracyonlygoesupmarginallybyabout3%.
Toexploitthefullvalueoftheordergivenbyoursolver,wetrainthemodelfromscratchusingthesolver
order. Thisallowsthemodeltolearnwhatisagoodstrategicorderinwhichtofillthecells. Importantlythis
orderisadaptivebasedonthepuzzle. Totrainitinthismanner,wefirstfeedeachpuzzletooursolverand
collectthesequenceofcellsitfillsinorder. Weusethesesequencesasourtrainingdatawhichactsasour
Chain-of-Thoughtdataforthemodel. Weobservethatthisleadstoamuchstrongermodelwhichisableto
solvefullSudokupuzzlestoanaccuracyof87.18%(seeSection3.4).
Giventhisnewmodel,weagaintrygivingpositionhintsduringdecodingasaboveandweseetheaverage
cellaccuracyshootupto99.02%. Thisindicatesthefollowing. TheiterativeprocessofsolvingSudokuscan
bebrokendownintotwosteps: (1)searchingandfindingacellpositionwherewecanapplyasubsetofthe
strategies,(2)givenacellposition,computingthevaluethatneedstobefilledinthatposition. Step(1)isthe
hardertaskforamodeltolearn. WeprovideexamplesofSudokupuzzleswherethemodelmakesamistake
instep(1)wherestep(2)isquitetrivial(seeSection4.3formoredetails). Tomakethemodelmoreproficient
atsolvingthepuzzlewithoutthepositionhints,weperformabeamsearchofwidth3or5andnoticethatthis
sufficestogetstrongerfullpuzzlesolvingaccuraciesof91.36%and94.21%respectively(seeSection3.5).
Inanenvironmentwherethemodelneedstosearchoverasetofcandidatestotakeasthenextstep,recent
workby[BN24]demonstratedthatnext-tokenpredictionmightbeaflawedobjective. Anotherrecentwork
[LSM+24] posit including the entire search trace as part of the training Chain-of-Thought data to help a
Transformerlearntasksinvolvingsearchandplanningdynamics. Incontrasttotheseworks,weobservethat
Transformerstrainedwiththenext-tokenpredictionobjectiveandwithoutaccesstotheentiresearchtrace
canlearncomplexreasoningtasks.
Finally, we further ask if we can see a similarity between the model’s way of solving the puzzles to a
humans/solverswayofsolvingthepuzzle. Westudythisviaprobingwhichhasbeenatechniquetounder-
standthelatent conceptualcontent, seee.g. [LHB+23,AZL23,PCV23,NLW23,JRR+24]. Inparticular,
workssuchas[PCV23,NLW23,JRR+24]arguethatoftensimplefunctionsofthemodel’sactivationsor
weightscanextractusefullatentinformation. Westudythefollowingviaprobing. Generally,humansand
algorithmicsolversforSudokukeeptrackofapossiblesetofvaluesforeachcellatagivenstateoftheboard
tomakeprogressonsolvingtheSudokupuzzle. Wealsoseethatthemodelalsoimplicitlykeepstrackof
4acandidatesetandthiscandidatesetmatcheswiththesolver’scandidateset(seeSection4.4formoredetails).
WeperformasimilarsetofexperimentsasaboveonZebrapuzzlesandobservequalitativelysimilartrends
givingevidencethatourconclusionsarenotlimitedtothedomainofSudoku(SeeSection5formoredetails).
Insummary,ourcontributionsare
1. WeshowthatcausallanguagemodelingwiththeTransformerarchitectureiscapableoflearningto
performsearchandreasoninginhighlynon-trivialdomainslikeSudokuandZebrapuzzles.
2. Wepresentevidencethattherightformoftrainingdatawhichdecomposestheproblemintosmaller
constituentsiscrucial. Howeverthisdataisnotrequiredtobetoodescriptive. Inparticular,itneednot
containsearchtracessimilartothoseprovidedin[LSM+24].
3. Weperformaprobinganalysistoshowthathuman-likeabstractreasoningconceptssuchascandidate
setofvaluesemergeimplicitlywithinthemodel’sactivations.
1.3 Relatedwork
There are many works which study the ability of language models to perform reasoning tasks which in-
volve search and planning with mixed evidence as to whether they are actually learning to reason and
plan. [BMR+20] was a seminal work which showed that large language models (LLMs) are few-shot
learnersand[KGR+22]arguedthattheycanbezero-shotreasoners. [LAD+22]showsthatbyfine-tuning
ontheappropriatedataLLMscanexhibitahighperformanceonthenon-trivialMATH[HBK+21]dataset.
In addition, the reports of frontier models like GPT-4 [AAA+23] and Gemini [TAB+23] also contain
support for the idea that LLMs can perform reasoning and planning. Building on these lines of work,
[HXX+22,SBM+23,ABB+22,SWW+23]employLLMsinplanningtasksintheroboticsdomain.
Anumberoffollow-upworksstudyhowwecanimprovethereasoningandplanningcapabilitiesofLLMs
using various techniques such as prompt engineering, tool use, using LLMs in combination with an ex-
ternal deduction engine. Some of the prominent works in this bracket are Chain-of-Thought prompting
[WWS+22b],least-to-mostprompting[ZSH+22],self-consistency[WWS+22a],tree-of-thoughtprompting
[Lon23,YYZ+24,XKZ+24],programofthoughts[CMWC22],planningforcodegeneration[ZCS+23]. Of
these,[Lon23]evaluatetheefficactofTree-of-ThoughtreasoningusingLLMslikeGPT-4onsolvingSudoku
puzzlesandonlyachieveresultson5x5Sudokupuzzles,Moreover,thetree-of-thoughtpromptingtechnique
isknowntoquiteexpensivetorun. [ZLH24]useLLMstoprovideacommonsenseworldmodelandapolicy
whichcanbefedtoaMonte-Carlotreesearchalgorithm. TheReActframework[YZY+22]usesLLMsto
generatereasoningtracesandtask-specificactionsinaninterleavedmanner.
Incontrasttotheabove,[VOSK22]showthatLLMswhenactingaloneorwhencombinedwithtechniques
suchasChain-of-ThoughtorTree-of-thoughtcannotsolvesomestandardplanningandreasoningbenchmarks
when the questions are rephrased with a new terminology. This is even when we use techniques such as
Chain-of-Thought, fine-tuning etc. [XZC+24] show that even the biggest LLMs perform very poorly at
real-worldtravelplanningtaskswithamultitudeofsoftandhardconstraints. [DLS+24]showthatLLMsare
limitedandbrittleintheirabilitytoperformcompositionaltaskssuchasmulti-digitmultiplication, logic
grid puzzles and dynamic programming. [MHF+23] argue that LLMs have weak cognitive maps which
arecrucialforplanning. [BN24]showthatratherthanthearchitecture,thetrainingobjectiveofnext-token
predictionmightbecripplingtheplanningandreasoningabilityofalanguagemodel.
TherearemanyworkswhichusethehelpofsynthetictaskstogaininsightsintohowTransformerlanguage
modelswork. Wepresentanon-exhaustivelisthere. [LHB+23]usethesynthetictaskoflearningtopredict
5valid nextmovesinan Othellogameto studywhetheraninternal modeloftheOthello boardemergesin
the model or not. [AZL23] use synthetic tasks such as learning context-free grammars to understand the
mechanics of how large-scale learning in LLMs works. [GTLV22] use linear regression from in-context
examples to study the in-context learning phenomenon. [NCL+23] use the task of modular addition to
understandthespecificalgorithmashallowTransformerimplementstosolvetheproblem. [AG23]usethe
taskofsortingalistofnumberstostudylengthgeneralization.
Comparison to Traditional Solvers and other ML approaches. Traditional constraint satisfaction li-
brariesuseverypowerfulcombinatorialsearchalgorithmstosolvelogicpuzzlesandaremuchmorepowerful
thananydeepmodelwelearnhere. Inaddition,manypriorworksstudymachinelearning-basedapproaches
forsolvinggeneralcombinatorialproblems[BPL+16,MSIB21,CFK+23]. Inaddition,thereareseveralap-
proachesthattendtohandcraftthearchitectureorlosstothepuzzleusinghumanunderstandingofthepuzzle
structure[MKPZ11,PPW18,Zhu]. Eventhoughourgoalistounderstandthecapabilitiesandlimitationsof
causallanguagemodelingandnottocompetewithsuchsolvers,wediscusssomeoftheseworksmorein
detail.
[MKPZ11] try to setup a Hopfield network to solve Sudoku puzzles. [PPW18] handcrafts the recurrent
networktomatchthepuzzlestructure(andobeytheconstraints)andperformsmultipleroundsofmessage
passingbetweencellsofthesudokupuzzletoarriveatasolution. Weevaluateourtrainedmodel(trained
using causal language modeling) on the test dataset proposed in this work and we observe a comparable
performance without handcrafting the network or loss function. [Zhu] achieves a 65% accuracy of RNN
basedsolverson3x3Sudokupuzzles. [NB21]studyhowwellGPT-2modelstrainedonnaturallanguage
performonpuzzletaskssuchasRubik’scubeandSudoku. [YIL23]studysolvingSudokususingarecurrent
form of Transformers by baking the knowledge of Sudoku’s constraints into the model architecture and
trainingpipeline. Forchess,workslike[RDM+24]useachessenginesuchasStockfishtoprovidesupervised
labelsfordifferentboardstatesandtrainaTransformernetworktopredictthevaluefunctionofaboardstate.
Thiscanthenbeusedtoplayexpertlevelchess.
2 Preliminaries and setup
Inthissection,weprovideabriefoverviewofthelogicpuzzlesweconsiderandtheinput/outputdataformat
thatisfedtothemodel.
Sudokupuzzleandsolver. Thegoalofthesudokupuzzleistofilloutthewholeboardwithnumbers1to9
withouthavingduplicatesineachrow,column,andbox(SeeappendixEformoredetails). Unlessspecified
otherwise,wewilluse(r,c)todenotethepositionofacellontheboardandv(r,c)todenotethevalueat
position(r,c)wherer ∈ {1,2,...,9}denotestherownumberofthecellandc ∈ {1,2,...,9}denotesthe
columnnumberofthecell. Additionally,weuseb(r,c)todenotetheblocknumber(amongoneofthenine
3×3blocks)ofthecellatposition(r,c). TosolveaSudokupuzzle,asudoku-solver(andhumansuptoan
extent)keepstrackofthecandidatesetforeachoftheemptycells. SeemoredetailsaboutCandidatesetin
AppendixE.
Asmentionedearlier,thegeneralizedversionofSudokuwithboardsizen×nisNP-complete[YS03]. This
implies that for some Sudoku puzzles, progress likely can not be made using any strategy that executes
in polynomial time. We avoid such puzzles by restricting our focus to those Sudoku puzzles that can be
6Figure2: Examplesofcomplexstrategiesthatinvolvesreasoningaboutmultiplecells. Left: XY-Wing,where
apivotcell(gray)hastwocandidatevalues(XandY),thewingcells(green)shareacolumn,roworbox
withthepivotandshareonecandidatevalue(XorY)withpivotandanothercommoncandidatevalue(Z),
theninanycellthatsharesacolumn,roworboxwithbothwingcells(yellow),wecaneliminateZfromthe
candidateset;Right: UniqueRectangle,wherefourcellsformarectangle,amongwhichthreecells(gray)
sharetheexactsame2candidatevalues,andthefourthcell(green)shareatleastoneofthe2values,then
bothvaluescanbeeliminatedfromthecandidatesetforthefourthcell.
solvedusingasetof7well-knownandcommonlyusedstrategieswhichareexecutableefficiently3. Further
details about each of the strategies is provided in Appendix A. An important point to note is that not all
the strategies fill a value in a cell. In fact, only 2 out of 7 strategies that we use, fill a value in a cell and
the other strategies are used to eliminate possible values of a cell and narrow down the candidate set at a
particularcell. Additionally,somestrategies(e.g.,XYwing,Uniquerectangle)involvereasoningonmultiple
cellsindifferentrows/columns/blocksandthesestrategiesdon’tfillavalueatanycellandtherefore,these
strategiesneedtobeappliedincombinationwithotherstrategiestodeduceavalueatacell. Additionally,we
onlyprovidethesolutionlistofcellvaluestothepuzzleduringtraining,thereforethemodelisnotgetting
anydirectsignalaboutthestrategiesthateliminatepossiblevaluesofacellandisonlygettingasignalin
combinationsofthestrategiesthatdeduceavalue.
Zebrapuzzleandsolver. TheZebrapuzzleischaracterizedbythenumberofentitiesmandthenumber
of attributes n for each entity. e.g., in Figure 1 each person is an entity, and name, house color and drink
areattributesassociatedwitheachentity. Therelationshipsbetweenentitiesandattributevaluesaregiven
ascluesinthepuzzleandthetaskistofigureoutvaluesforallattributesandalltheentities. SeeFigure1.
Observethateachclueputssomeconstraintsonthevalueofattributesandentities. e.g.,“Thepersonwho
likes beer is somewhere to the left of the person who lives in the indigo house.” clue says that the house
(entity)whichhasdrinkattribute=beerissomewherelefttothehousewhosecolorattribute=indigo. As
weallowmoreandmorecomplexrelationshipsinclues,thepuzzlesbecomemoreandmorecomplex. In
addition,increasingsizemakesthepuzzlesmorecomplexaswellasmoreandmoreinterconnectedcluesare
requiredtouniquelypindownasolution. Largerpuzzlesalsohaveahigherchanceofdeeperandtrickier
reasoningchainsbeingutilized. SimilartoSudokus,ageneralizedversionofm×nZebrapuzzlesisalso
NP-hard. UnlikeSudokupuzzles,wheretheconstraintsareonlytheuniquenessconstraintswithineachrow,
columnandbox,Zebrapuzzlescanhaveamuchmorediversesetofconstraintswhichsignificantlyincreases
thenumberof‘strategies’thatcanbeusedtomakeprogress. Moreover,Zebrapuzzlesareastepcloserto
naturallanguagethanSudokupuzzles.
3ThelistofthestrategiesweconsiderisLoneSingle,HiddenSingle,NakedPair,NakedTriplet,LockedCandidate,XYWing,
UniqueRectangle,
7In the Zebra puzzles, we have 7 different types of clues. Details about each of the clue types is provided
in Appendix A. We generate our own dataset of Zebra puzzles as follows. We first create a Zebra puzzle
solver. The solver for the Zebra puzzle takes in a clue set and iteratively tries to make progress by using
k-sizedsubsetsofthecluesatatime(fork rangingfrom1to3). Ifitisabletomakeadeduction,thesolver
marksthatentryintheanswertableanditeratesoverthecluesubsetsagain. Giventhissolver,anewpuzzle
is generated by starting with an empty clue set and iteratively adding randomly generated clues until the
solverisabletosuccessfullysolvethepuzzle. Whileaddingnewcluesweensurewedonotaddduplicates.
Nonetheless,somecluesmightstillberenderedredundantduetothepresenceof2ormoreotherclues. To
keepthecluesetlean,oncewehaveapuzzlethatthesolverisabletosolve,wefilteroutthecluesunusedby
thesolver. Wegeneratepuzzlesofsizesm×nform,nrangingin[3,4,5,6].
Dataset, model architecture and training. Our training dataset for the Sudoku experiment contains
1.8M puzzles and the test dataset contains 0.1M puzzles. Each puzzle also comes with a difficulty rating
calculatedasfollows. Torateapuzzle,abacktrackingbasedsolver(differentfromtheoneweusetogenerate
our solver-order data) is employed. Thissolvertries to iterativelymake progress on apuzzle using some
elimination techniques. When it gets stuck, it makes guesses and tries to solve the puzzle. The difficulty
ratingisthemaximumdepthoftheguessstackthesolverhadtousetosolvethepuzzle. Therefore,evena
puzzlerated0.5canrequirecomplexstrategiesbeyondsimplescanningtosolvethemwithoutguessing. We
trainasequence-to-sequencemodelthattakesinasinputarepresentationofaSudokupuzzleasasequence
andneedstooutputthesolutionofthepuzzleasasequence. Duringthetraining,weprovideinformation
aboutasinglecellusingthreetokens(r,c,v(r,c)): thefirsttwotokens(r,c)containinformationaboutthe
position of the cell (row and column number) and the third token contains the number in that cell. Each
trainingsequenceisdividedintotwoparts. Thefirstpartcontainstheinformationaboutcellswhosevalues
aregiveninthepuzzlequestionandthesecondpartcontainsinformationaboutunfilledcellsinthesolution.
Notethattherecanmultiplevalidordersforthesolution. Also,notethatthelengthofthefirstpartdepends
onthenumberofcellsfilledinthepuzzle. Wetrainthemodelusingthenext-tokenpredictionlossbutwe
don’tapplythelosscorrespondingtothepredictionofthefilledcellsgiveninthequestion.
OurtrainingdatasetfortheZebraexperimentcontains0.3Mpuzzlesandthetestdatasetcontains15kpuzzles.
The input to the model during the training is divided into two parts. The first part (given in the puzzle)
containsthecluesandthesecondpart(solution)containsvaluesforallattributesandallentities. Eachclue
containstwoparts: 1)therelationshiptypebetweenattributesandentitiesand2)thespecificattributesand
entityvaluesthatareinthisrelationshipwhereasthesolutionpartofthepuzzleconsistsofmultipletriplets
ofentity,attribute,andthesolutionforthatentityandattribute. Similartothesudokupuzzle,therecanbe
differentordersinwhichthesolutiontripletsforeachentityandattributecanbeprovidedduringthetraining.
WeuseaTransformer-basedGPT-2[RWC+19]architecturewith8layersforbothpuzzles. Eachlayerhas8
attentionheadswithamodeldimensionof576andanMLPofhiddendimension3456(6×modeldimension)
follows in each layer. The total number of parameters of our model is 42M. We use causal masking in
theattentionlayerstoavoidlookingintothefuture. Moredetailsaboutthedataset, ourarchitecturesand
hyperparameterscanbefoundinAppendixB.
Evaluationmetrics. Toevaluatetheperformanceofourmodel,weusethefollowingtwometricsprimarily:
1) Cell accuracy: denotes the percentage of the unfilled cells whose values are correctly predicted by
themodel. 2)Completepuzzleaccuracy: denotesthepercentagesofthecorrectlysolvedpuzzlesinthe
evaluationdataset. Apuzzlewithevenasinglemistakeiscountedasincorrect.
83 Experiments on Sudoku puzzles
WestudytheperformanceofaTransformermodelwhenthemodelistrainedwiththenext-tokenprediction
objective. We set up the model architecture and training of the model as discussed in Section 2 however,
thequestionremainshowtoordercellsintheinputsequencesoftheSudokupuzzleduringthetrainingof
the model. Note that given the state of a Sudoku puzzle, some cells might be easier to solve than others
sotheorderofthecellsofinputsequencesprovidedduringtrainingcouldbeimportant. Wefirsttryusing
a predefined fixed order or a random order of the cells during the training and inference in Section 3.1.
However,thisleadstopoorperformance. Thereafter,weturnourfocusonusingasolvertocreateabetter
order which we call solver-decomposed reasoning order (Section 3.2). Inspired by Chain-of-Thoughts
literature[WWS+22b],Section3.3usessolver-decomposedreasoningorderonlyduringtheinferenceonthe
abovetrainedmodelstoprovidepositionhints. Yet,conditioningonthesepositionhintsduringdecoding
onlyprovidesarelativelysmallimprovementintheperformanceshowingthatevenifwetellthemodelto
findthevalueinaparticularcell,ithasnotlearntfullyhowtodoso.
Therefore,inSection3.4,weexploretrainingthemodelusingcellsprovidedinsolver-decomposedreasoning
order. Thisprovidesahugeboosttotheperformanceallowingthemodeltosolveover85%ofthepuzzlesin
thetestsetaccurately. However,itstilldoesnotachievenear-perfectcellaccuracy. Therefore,Section3.5
usesbeamsearchdecodingtoimprovetheperformance.
3.1 Trainingusingfixedorrandomorderofthecells
Anaturalchoiceforthecellorderintheinputsequencewouldbetouseafixedorderofthecellsorarandom
order of the cells in the puzzle for the input sequence. Note that the order of the puzzle is only provided
duringthetrainingandwedonotpenalizethemodelforwrongorderduringevaluationaslongasitsolves
thegivensudokupuzzlecorrectly.
Fixedorderofthecells. Inthisorderingofthecells,wearrangethecellsinapredefinedfixedorderof
top-lefttobottom-rightoftheboardofthepuzzle. Tobemoreprecise,foranytwocells(r,c)and(r′,c′)
wherer andr′ denotetherownumbersandcandc′ denotethecolumnnumber,wewillorderthefirstcell
(r,c)before(r′,c′)ifr < r′ orr = r′ andc < c′. Weorderbothpartsofthepuzzle(inputsequence)-given
cellsinthepuzzleandtheremainingsolutionofthepuzzleusingtheabove-mentionedordering.
Randomorderofthecells. Anotherwaytoarrangethecellsthatweconsideristorandomlyordercellsin
givencellsofthepuzzleandsolutionoftheinputs. Foranygivenprefix(stateofthepuzzle),werandomly
pickacellfromthesetofemptycellsandappendthatcellandcorrespondingvaluetotheprefix.
Results. WeprovidetheexperimentalresultsforthefixedorderinFigure3. Weseethatthemodeltrained
withfixedorderachieves58.64%cellaccuracyandonly7.2%fullpuzzleaccuracywhereasthemodeltrained
withrandomorderonlyachievesaround52%cellaccuracyandonly1%completepuzzleaccuracy.
Intheaboveorderingofthecells,givenastateofthepuzzle,themodeldecidesonarandomcellorfixed
cell to output value but at that state, only a few cells might be easier to solve than others and the model
trainedusingrandomorfixedorderofcellsdonotnecessarilydecodetheeasiercellsatthatstate. Therefore,
inspiredbyChain-of-thoughtliterature[WWS+22b],weaskthefollowingquestion: ifweprovidethemodel
informationduringinferencewhichcellsareeasiertofillthendoestheperformanceimprove? Beforewe
answertheabovequestion,wedefinethesolver-decomposedreasoningorderwhichwillbeusefulinfinding
cellsthatareeasiertofill.
9Figure3: Comparisonofcellaccuracyandfullpuzzleaccuracyforfixedordertraining,randomordertraining
andsolver-decomposedreasoningordertraining.
3.2 Solver-decomposedreasoningorder
AnaturalwayhumanssolveSudokuisbyiterativelytryingtofindcellsthatlookeasiertofill. Thesearch
processinvolvestryingtoseeifanyofagivensetofstrategiescanbeappliedtofillinthevalueorotherwise
makeprogressonacell. Inspiredbythisanalogy,weconstructanorderoffillingcellsusingasolver. The
solveruses7strategiesasmentionedinSection2. Atanygivenstateofthepuzzle, ittriestoapplytoan
easierstrategyfirstandifitcannotmakeprogresswithaneasierstrategythenitgoestoaharderstrategy.
Toapplyastrategy,thesolvergoesthroughallthecellsandtriestoapplythestrategyforeachcelltomake
progresstowardsolvingthepuzzle. Progressdoesn’tnecessarilymeanfillingavalueinacellbutsimply
eliminatingpossiblevaluesfromthecandidateset(setofpossiblevalues)ofacellalsocountsasprogress.
Wecalltheordergivenbythesolverassolver-decomposedreasoningorderordecomposedreasoningorder
forbrevitywhenitisclearfromthecontext. Notethatthedecomposedreasoningorderarrangesthecells
basedonhoweasytheyaretofillin,asthesolverinitiallyemployssimplerstrategiestomakeprogress.
3.3 Hintedcellaccuracy
Inrecentyears,chain-of-thought(CoT)prompting[WWS+22b]hasemergedasoneoftheeffectivetech-
niquestoextractcomplexreasoningabilitiesfromamodel. ThemainideaofCoTistoleadthemodeltothe
correctoutputbyprovidingintermediatestepstohelpthemodel. InspiredbytheCoTprompting,weask
ifprovidingthemodeladditionalinformationabouteasier-to-decodecellsduringinferenceimprovesthe
performance?
Specifically,weusedecomposedreasoningordertoprovidepositionhintsduringinference. Recallthatto
inferavalueatposition(r,c)atastateofthepuzzles,weprovide(s,r,c)asinputtothetransformermodel
andtherandom-orderbaselinemodelistrainedtopredictvaluev asnexttokengivenpositionsinprevious
two-tokens(r,c),andbecausepositionsarechosenrandomlyduringthetraining,themodelisforcedtouse
thepositionsinprevioustwo-tokens(r,c)whilepredictingthevalue. (Notethatthisisnotthecaseforthe
modeltrainedwithfixedorderandtherefore,wedon’tconsidertheminthisexperiment).
Now,toprovideadditionalinformationtothemodelabouttheeasiercellstopredict,weusethedecomposed
reasoningorder. Specifically,foranygivenstates,weprovidethestatestothesolvertoobtaintheposition
oftheeasiestcell. Supposethesolverpicks(r′,c′),thenweprovide(s,r′,c′)tothetrainedmodeltopredict
a value at (r′,c′). We reiterate this process for every non-empty cell of the puzzle. We measure the cell
accuracyinthissettingandwecallthisaccuracyashintedcellaccuracytodenotetheprovidedhintsabout
easy-to-decodepositionsfromthesolver.
10Results. We see that the model trained using random order achieves 54.57% hinted cell accuracy. This
meansthatprovidinghintsabouteasy-to-decodepositionsimprovestheaccuracybyaround3%overwithout
anyhints. Atfirstglance,itseemslikethemodelisstrugglingsignificantlytoimplementthecorrectstrategy
even when we provide information about the positions of easy-to-fill cells as hints during the inference.
However,thismightnotbethecorrectconclusionbecauseofthefollowingreasons: duringthetraining,the
modelistrainedtopredictthevaluefromrandomcells,andthemodelneedstolearnandapplyveryhard
strategies as well to improve its training loss. In the process of learning hard strategies, the model might
failtolearneasierstrategiesaswellbecauseofvariousreasons(e.g.,limiteddatacorrespondingtoeasier
strategies,limitedmodelsize,etc.).
When we use decomposed reasoning order during the inference, it helps to improve the performance for
the model trained using random-order of the cells but because the model needs to perform a hard search
andreasoningtaskwhiledecodingavalueatasinglecell,itnotonlyseemstohurtthemodelinsearching
easy-to-decodecellsbutalsoaffectsitsreasoningcapabilitiestodecodeavalueatgivencellsevenafterwe
explicitlyprovidepositionsofeasytofillcells. Thismotivatesustousedecomposedreasoningorderduring
thetraining.
3.4 UsingsolverforCoTtraining
Solving the sudoku puzzle can be decomposed into two sub-tasks: 1) Search across the board to find the
cells that are easy to fill and 2) After finding the easy-to-fill cell, apply the correct strategy on the cell to
obtainacorrectvalueinit. Asmentionedearlier,themodeltrainedforfixed-orderandrandom-orderdoes
nothaveexplicitincentivestoperformasearchforeasy-to-fillcells. Therefore,themotivationforproviding
thesolver-decomposedreasoningorderduringthetrainingistoprovideanorderofcellssuchthattraining
a model using the order helps the model to decompose the complex task of solving sudoku into smaller
sub-tasks.
Toprovidedecomposedreasoningorderofcellsduringthetraining,wearrangethecellsaccordingtohow
easytofilltheyare. NotethatwecanobtainthisusingThen,weusethesesequencesduringthetrainingwith
thenext-token-predictionlossforallthetokens. Therefore,givenaboardstates,thelosscorrespondingto
positiontokensincentivizesthemodeltolearntofindeasy-to-decodecells,andthelosscorrespondingto
valuetokensincentivizesthemodeltolearnthestrategy.
Result. WeprovidetheresultforthedecomposedreasoningordertraininginFigure3. Weseethatusing
thedecomposedreasoningorderachievesthecellaccuracy94.23%andcompletepuzzleaccuracy87.18%
accuracy. Trainingthemodelonthedecomposedreasoningorderimprovescellaccuracybyaround36%over
thefixed-ordertrainingandbyaround43%overtherandom-ordertraining. Themostnoticeableimprovement
comesincompletepuzzleaccuracywheredecomposedreasoningordertrainingachieves87.18%accuracy
whereasthefixed-ordertrainingachievesaround8%accuracyandtherandom-ordertrainingachievesaround
1%.
Hintedaccuracyfortrainingusingsolver-decomposedreasoningorder. Eventhoughsolver-decomposed
reasoning order training significantly improves performance over fixed-order training and random-order
training,itdoesnotachievenear-perfectaccuracy. Therefore,tounderstandwhetherthemodelisstruggling
to perform a search for easy-to-decode training or to employ a strategy given a position, we perform the
experiment of providing hints about easy-to-decode positions (presented in Section 3.3). Recall that to
measurethehintedcellaccuracyforamodel,weprovideinformationabouteasy-to-decodecellstothemodel
duringinferenceandmeasurecellaccuracyinthatsetting. Weseethatthemodelwithsolver-decomposed
reasoningordertrainingachieves99.02%hintedcellaccuracy. Thismeansthatthemodelcanemploythe
11Cellaccuracy Completepuzzleaccuracy
Beamwidthk =1 94.23% 87.18%
Beamwidthk =3 96.07% 91.36%
Beamwidthk =5 98.03% 94.21%
Table2: Performance(cellaccuracyandcompletepuzzleaccuracy)changeasweincreasebeam-widthin
beam-search.
correctstrategyassumingithasaccesstoinformationabouteasy-to-decodecellsandthattheperformance
gapincellaccuracy(94.23%tonear-perfectaccuracy)ismainlyduetosearchingforeasy-to-decodecells.
Next, we try to bridge the gap between 94.23% cell accuracy and 99.02% hinted cell accuracy achieved
bythemodeltrainedusingdecomposedreasoningorder. Toimprovetheaccuracy,wefirstunderstandthe
numberofcellsthatarefilledforapuzzlewhenthemodelismakingthefirstmistakeforthepuzzle. Note
thatwhenthemodelmakesamistakebyfillinginanincorrectvalueonthepuzzle,thentheprobabilityofthe
modelmakingamistakeontheremainingemptycellsincreases. Wealsoseethishappeninourexperiments
(SeeAppendixC).
Ahypothesisaboutlowercellaccuracythanhintedcellaccuracyisthatgivenacertainstateofthepuzzle,the
modelmightbeconfusedbetweenseveralcellsaboutwhichcellsareeasiertodecode. However,ifthemodel
isallowedtoexploremultiplepotentialcellsofthepuzzle,itmightfigureoutthetruesolutionasthemodel
will make a prediction confidently for the true solution of the puzzle compared to other wrong solutions.
Becauseofthisreason,wetrybeam-searchdecodingforthemodeltrainedusingsolver-decomposedreasoning
order.
3.5 Beam-searchdecoding
Beam-searchdecoding(usedinmanypopularNLPsystems,e.g. [WSC+16])inlanguagemodelingallows
themodeltoexploremultiplepartialdecodingofthesequencesduringtheinferenceofalanguagemodel
andoutputthemostprobableexploredsequence. Thebeamwidthk ofthebeamsearchdecodingdenotes
how many partial sequences (hypotheses) are kept at each step. At each step of the decoding, it expands
allpartialsolutionsofthepuzzlebydecodingonemoretoken. Then,amongthisexpandedpartialsolution
set,themodelselectsthetopk mostprobablepartialsolutions. Thisprocessisrepeatedforthedecodingof
everytoken. Notethat beamsearchonlymaintains k possibleoutput sequencesthroughoutthe decoding
process. Comparedtostandarddecoding,beamsearchincursacomputationaloverheadofafactorofk2. In
theSudokupuzzle,Itisimportanttonotethatthebeamsearchcannottryoutallpossibleoutputsforthe
Sudokupuzzle. Afterall,thetotalnumberofoutputscanbearbitrarilylargebecausemanyoftheemptycells
willhaveonaverage2to5possiblevaluesandthetotalnumberofemptycellsinthepuzzleisatleast50.
Results. We present our results for beam-search decoding in Table 2. The beam search with k = 1 is
equivalenttogreedydecodingasitonlykeepsonepartialsequence. Weseethatbeamsearchwithk = 3
improvesthecellaccuracybyaround2%andcompletepuzzleaccuracybyaround4%. Weseeasimilar
improvementwhenweincreasebeamwidthfromk = 3tok = 5. Notethatthecellaccuracywithbeam
widthk = 5isabletobridgethegapfromthehintedcellaccuracyuptoalargeextentbutdoesnotneed
hintsabouteasy-to-decodepositions.
124 Analysis
In the above section, we showed that solver-decomposed reasoning order during the training can greatly
helpimprovethemodel’sperformance. Inthissection,weanalyzethetrainedmodelonseveralfronts. We
comparethemodel’sperformancetoaneuralnetwork-basedmethoddesignedtosolvetheSudokupuzzle
[PPW18]insection4.1. Section4.2containsthebreakdownofthecompletepuzzleaccuracyacrossvarious
difficulties. Section4.3containsadiscussionaboutfailurecasesofthemodelinsearchingeasy-to-decode
cells. InSection4.4,weshowthatthecandidatesetinformationemergesinthemodeltoexplainhowthe
learnedmodelissolvingthepuzzles.
4.1 ComparisonwithneuralnetworkbasedSudokusolver
Thegoalofourworkistounderstandthecapabilitiesandlimitationsofcausallanguagemodeling(andnotto
proposeanewapproachtosolvetheSudokupuzzle). Tounderstandifthereexistsaperformancegapbetween
amodeltrainedusingcausallanguagemodellingandamodelspecificallydesignedtosolveSudokupuzzles,
wecomparetheperformanceofourmodelwithaneuralnetworkbasedSudokusolverproposedin[PPW18].
Palmetal. [PPW18]handcraftstherecurrentnetworktomatchtheSudokupuzzlestructure(andobeythe
constraints)andperformmultipleroundsofmessagepassingbetweencellsoftheSudokupuzzletoarriveat
asolution. Weevaluateourtrainedmodelonthetestdatasetproposedin[PPW18]of18000Sudokupuzzles
(SeeTable3fortheresult). Weobservethatourtrainedmodel(trainedusingsolver-decomposedreasoning
order and causal language modeling) combined with the beam search obtains a comparable performance
withouthandcraftingthenetworkorlossfunction.
Evalaccuracy Evalcompletepuzzleaccuracy
Beamsearchwidth=1 98.15% 94.76%
Beamsearchwidth=3 98.28% 95.12%
Beamsearchwidth=5 98.37% 95.43%
RecurrentRelationalNetwork NA 96.6%
Table3: EvaluatingourmodelontheevaluationdatasetofSudokugiveninRecurrentRelationalNetwork
(RRN)byPalmetal. [PPW18]. OurtrainedmodelperformscomparablytotheRRNmodelbutdoesnot
requirehandcraftingthenetworkandtrainingprocedurefortrainingontheSudokupuzzles.
4.2 Performanceanalysisofthemodelusingthedifficultyofthepuzzles
WeprovidethebreakdownofthecompletepuzzleaccuracyacrossvariousdifficultiesinFigure4tobetter
understand the performance of the trained model. We use the difficulty measure provided in the Kaggle
dataset [Rad20]. To obtain the difficulty of a puzzle, it considers a solver (different from the one we use
togenerateoursolver-orderdata)whichtriestoiterativelymakeprogressonapuzzleusingsomesimple
strategies. Whenthesolvergetsstuck,itmakesguessesandtriestosolvethepuzzle. Thedifficultyratingis
theaveragenumberofguessesthesolverhadtomaketosolvethepuzzle. Wewishtopointoutthatevena
puzzlerated1.0canrequirecomplexstrategiesbeyondsimplescanningtosolvethemwithoutguessingand
therefore,thisisanimperfectmeasureofthedifficulty.
InFigure4,weobservethatthemodelachievesalmostperfectcompletepuzzleaccuracyforlowerdifficulty
accuracyandasthedifficultyofthepuzzleincreases,thecompletepuzzleaccuracygoesdown. Wewantto
notethatevenwhenthedifficultyratingisbetween3to3.5,themodelcansolvearound50%ofthepuzzles
completely. Additionally, the advantage of beam search increases for the higher difficulty puzzles as the
modelcanexploremultiplesolutionswhenithasconfusionandoutputasolutionattheend.
13Figure 4: Complete puzzle accuracy for different difficulty Sudoku puzzles. The difficulty rating is
computedastheaveragenumberofguessestherating-solverhadtomaketosolvethepuzzletherefore,the
difficultyratingisanimperfectmeasureofthedifficulty.
4.3 Failureinsearchforeasy-to-decodecells
Asdiscussedinsection3.4,themodeltrainedusingsolver-decomposedreasoningordersolves94.23%cells
ofthesudokupuzzlescorrectly. Tounderstandthefailuremodesofthemodel,wemeasurethehintedcell
accuracybyprovidinginformationabouteasy-to-decodecellstothemodelduringinference. Weseethatthe
modelachieves99.02%accuracy. Thisshowsthatthemodelcanfindthecorrectvalueatthecellwhenitis
providedtheinformationabouteasy-to-decodecellsandtheperformancegapincellaccuracyismainlydue
totheinabilitytosearchforeasy-to-decodecells.
WealsoprovidesomeexamplesofthepuzzlesituationsinFigure5andFigure8(inAppendix)whenthe
model makes a mistake by trying to fill a cell but there is another cell for which it is easier to fill. This
supports our finding byhinted cell accuracy that the performance gap of ourtrained model to the perfect
accuracyisduetotheinabilitytosearchforeasy-to-decodecells.
Additionally,acellcanbeeasy-to-decodebecauseofeitherrow,columnorblockconstraintoftheSudoku
puzzle. Wefoundthatourtrainedmodelmissesmorecellswhichareeasy-to-decodebecauseoftheblock
constraint. Thismightbeduetotheinputformatbeingnotexplicitforblockandexplicitforrowandcolumn
ofacell(recallthatacellisintheformatof(r,c,v(r,c))asinputtothemodel). Anaturalextensioninthis
casecouldbetoprovideablocknumberalsoasaninputtothemodel. Weleaveitforfuturework.
4.4 Emergenceofcandidatesetinformationinthemodel
Wesawintheprevioussectionthatamodeltrainedwithpuzzlesgiveninsolver-decomposedreasoningorder
performsverywell. Therefore,wefocusonhowthemodelislearningsuchataskthatrequiresplanningand
reasoning. Asmentionedearlier,thesudokusolver(andtoanextenthumans)keepstrackofpossiblevalues
thatacellcantakeforthegivenpuzzle. Therefore,weaskthefollowingquestion: doesthemodelalsokeep
trackofpossiblevaluesofacell? Canweextractthemfromthemodel?
We answer both of these questions (perhaps surprisingly) positively by showing that for a given puzzle,
the candidate set of the solver can be extracted from the logits of the model. The candidate set of an
emptycellkeepstrackofpossiblevaluesthatthecellcantakegivenastateofthepuzzle. Notethatgiven
14Figure 5: A failure case of the model in searching for easy-to-decode cells. The left figure shows the
sudokupuzzlestatewhenthemodelmakesthefirstmistakeandtherightfigureshowsthepuzzle’ssolution.
Numbersgivenintheblueareprovidedinthepuzzle. Thepuzzlemakesamistakebychoosingtofillthe
red-coloredcellwhereasthegreenbackgroundcellcanbeeasilyfilled.
some state of the puzzle, the candidate set at an empty cell (r,c) can be different from {1,2,...,9} −
{setoffilledvaluesinrowr,columncandboxb(r,c)}assomeofthestrategyremovesavaluewhichdoes
notoccurinthesamerow,columnorbox.
Calculatingcandidatesetequivalence. Forallpuzzlesinthevalidationdataset,weobtainthecandidateset
ofallemptycellsfromthesolverwhenthenumberoffilledcellsisinthesetS = {35,40,45,50,55,60,65,70,75}.
Forastatesofapuzzle,wedenotethecandidatesetofthesolveratanemptycell(r,c)asf∗(s,r,c). Weuse
|f∗(s,r,c)|todenotethenumberofpossiblevaluesinthecandidatesetf∗(s,r,c). Toextractthecandidate
setfromthemodelatthestatesofthepuzzleandatanemptycell(r,c),wefeed(s,r,c)astheprefixto
the model andvalues corresponding totop-k output logits wherek = |f∗(s,r,c)| becomes the candidate
set of the model. We denote the model’s candidate set as m(s,r,c). Importantly, note that we DO NOT
only evaluate the top-k candidates on the cell the model chooses to predict. Although during its natural
courseofdecodingthemodelmightwishtodecodecelllocationA,weforceit(byconditioning)todecode
at every other location and evaluate the top-k candidates. This ensures that we are looking at what the
modelthinksisthesetofpossiblecandidatesofcelllocation(r ,c )evenwhenithasdecidedtodecodecell
1 1
(r ,c ) ̸= (r ,c )next. Wenotethatthisstyleofprobingdiffersfromthemorecommonwaytoperforma
2 2 1 1
probinganalysiswhichinvolveslearningalinear/non-linearprobewhichtakesintheembeddingandoutputs
alabelindicatingaconcept. However,weuseprobinginmoregeneralsensetorefertounderstandsomeof
theinnerworkingsofthemodel.
Theaccuracyforthecandidatesetequivalencebetweenthesolverandthemodelatastatesofapuzzleand
at an empty (r,c) is measured by |f∗(s,r,c)∩m(s,r,c)|/|f∗(s,r,c)|. The reported accuracy at position
n ∈ S inTable4istheaverageoverallemptycellswhenthenumberoffilledcellsisnforthepuzzleswhich
arecorrectlysolvedbythemodel. Intuitively,thecandidate-setequivalenceaccuracymeasurestheaverage
overlapbetweensolver’sandmodel’scandidatesetforthecorrectlysolvedpuzzles.
15Numberoffilledcells 35 40 45 50 55 60 65 70 75
Accuracy(%) 93.19 93.23 94.14 94.81 94.70 96.60 97.71 98.54 99.37
Table4: Candidatesetequivalenceaccuracywhenthenumberoffilledcellsisdifferentinthegivenpuzzle.
Thecandidate-setequivalenceaccuracymeasurestheaverageoverlapbetweenthesolver’sandthemodel’s
candidatesetforthecorrectlysolvedpuzzles.
Results. TheresultsofcandidatesetequivalenceaccuracyaregiveninTable4. Weseethatforallpositions
theaverageoverlapbetweenthesolver’sandthemodel’scandidatesetisabove93%. Thisoverlapimproves
to around 96.5 % when the prefix has information about 60 cells and to around 98.5 % when the prefix
containsinformationabout70cells. Notethattoextractthecandidatesetofthemodel,wearejustreading
thelogitsandnoteventrainingalinearfunction. Additionally,thecandidatesetequivalenceresultisnotonly
forcellsthatareeasytodecodebutforallemptycells. Moreover,duringthetrainingofthemodel,nodirect
informationaboutthecandidatesetisprovidedandthemodelisonlytrainedtopredictthecorrectvaluefora
cellandthereforeisnotdirectlyincentivizedtopredictthecorrectcandidatesetforalltheemptycellswith
suchahighaccuracy.
5 Experiments on Zebra puzzle
ToextendourresultsbeyondSudoku,wealsoconductourexperimentsontheZebrapuzzle(alsoknownas
Einstein’sPuzzle). LikeSudokupuzzles,weconsiderprovidingthesolutionineitherafixed,random,or
solver-decomposedreasoningorderduringthetraining.
Orderofthesolution TheinputprovidedforaZebrapuzzleduringtrainingconsistsoftwoparts: theclues
andthesolution. Thesolutionpartcontainsvaluesassignedtoeachentityandattribute. Basedonthegiven
clues,certainvaluesforspecificentitiesandattributesareeasiertodeterminethanothers(e.g.,inFigure1,
thethirdclueimmediatelyrevealsthatthefirsthousehasapersonwiththenameattribute=Rose). Thus,as
seenintheSudokupuzzle,theorderinwhichthesolutionisprovidedisimportant.
SimilartoSudokupuzzles,weconsiderprovidingthesolutionineitherafixed,random,orsolver-decomposed
reasoningorderduringthetraining. Inallcases,thecluespartoftheinputremainsunchanged. Infixed-order
training,thesolutionisgiveninapredeterminedsequence(weusetheorderstartingfromthefirsthouse’s
firstattributetothelasthouse’sfirstattribute,followedbythenextattribute). Inrandom-ordertraining,the
solutionisshuffled. Insolver-decomposedreasoningorder,thesolutionisarrangedbasedonhowthesolver
approachesthepuzzle,progressivelyusingsmallersubsetsofcluestomakeprogressandtherefore,dividing
thereasoningtosolvethepuzzleintomultiplestages.
Results Weplotthecellaccuracyandcompletepuzzleaccuracyonanevaluationsetof1kpuzzlesduring
thetraininginFigure6. Weseethatthetrainingusingsolver-decomposedreasoningorderachieves95.63
%cellaccuracyand91.17%completepuzzleaccuracywhereastherandomordertrainingachievesalmost
zerocompletepuzzleaccuracyandthefixedordertrainingachieves79.36%completepuzzleaccuracy. We
believethisisduetoalargernumberofsmall-sizedZebrapuzzlesintheevaluationset(e.g.,puzzleswith3
or4attributesandentities)whichareeasiertosolvethanlarger-sizedZebrapuzzles. Wealsoevaluatethe
model’sperformancebyusingbeamsearchdecodingandreporttheresultsinTable5. Weseethatusingbeam
searchdecodingwithwidth=3improvestheperformanceby0.7%andincreasingittowidth=5improvesthe
performancebyanadditional0.2%.
16Evaluationaccuracy Evalcompletepuzzleaccuracy
Beamsearchwidth=1 95.63% 91.17%
Beamsearchwidth=3 96.03% 91.83%
Beamsearchwidth=5 96.26% 92.04%
Table5: Zebrapuzzleresults. ThetrainingdatasetcontainsZebrapuzzleswiththeno. ofentitiesandthe
no. ofattributesin{3,4,5,6}set. Foreachcombinationoftheno. ofentriesandattributes,wegenerate20k
puzzlestherefore,thecompletedatasetcontains320kpuzzlesofvaryingsizes. Fromthecompletedataset,
werandomlychoose15kpuzzlesforevaluationandtherestofthepuzzlesfortrainingthemodel. Evaluation
accuracy: the percentage of correctly predicted attributes on the evaluation set and eval complete puzzle
accuracy: thepercentageofcorrectlyandcompletelysolvedpuzzles.
Figure6: Comparisonofcellaccuracyandfullpuzzleaccuracyforfixedordertraining,randomordertraining,
andsolver-decomposedordertraining
6 Conclusion
We have shown that even on complex logical reasoning tasks such as Sudoku and Zebra puzzles, simple
next-tokenpredictionprovidedwithahigh-leveldecompositionofthereasoningstepsduringtrainingisable
tolearntosolvethetask. Thissuggeststhat,giventherightlevelofdetailandbreakdownofreasoningsteps
inthetrainingdata,apre-trainedmodelmightalreadypresentasastrongreasoningengine(withouttheneed
forpost-trainingtechniquessuchasfine-tuning,promptengineering,self-consistency,tree-of-thoughtsetc).
Thesetechniquesmighthelpsignificantlyboostthebaselineperformanceofamodelorpotentiallymake
upfordeficienciesinthepre-trainingdatahowever. Tomovetowardsmoregeneralreasoningsystems,an
interestingchallengetoovercomewouldbetosimulatethedecomposedreasoningdatainanefficientmanner.
Thesetaskscapturemanydifferenttypesofconstraintsatisfactionproblemsandwebelievetheframework
andresultsshouldgeneralizetoothersettingsaswell.
Finally,weconcludewithsomelimitationsofourstudy. Firstly,wenotethatwestudiedasyntheticsetting
onatoytaskandreal-worldreasoningandplanningtaskscanbemuchmoreabstractandchallenging. More
specifically,Sudokuisataskwhichdoesn’trequirethesamedegreeoflong-termplanningassomeharder
benchmarks. Thatis,anycellwecanmakeprogressonisprogressunlikeconstraintproblemswhereone
might need to backtrack. Moreover, we focused on a reasoning setting where creative thinking was not
required. That is, the model did not need to invent new strategies to solve any test time puzzle. It is an
interesting future direction to study to what extent causal language modeling can yield novel reasoning
strategies. Moreover,therecanbemanydifferenttypesofreasoningtaskswhicharenotlogicpuzzles(for
instanceprobabilisticpuzzlesorrule-lesspuzzles,seee.g. [GLFS24])andourexperimentsdonotexplore
17those.
Acknowledgments
Authors would like to thank Erik Vee for guiding them to use the hinted cell accuracy to understand the
failuremodesofthetrainedmodel.
References
[AAA+23] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
technicalreport. arXivpreprintarXiv:2303.08774,2023.
[ABB+22] MichaelAhn,AnthonyBrohan,NoahBrown,YevgenChebotar,OmarCortes,ByronDavid,
ChelseaFinn,ChuyuanFu,KeerthanaGopalakrishnan,KarolHausman,etal. Doasican,not
asisay: Groundinglanguageinroboticaffordances. arXivpreprintarXiv:2204.01691,2022.
[AG23] PranjalAwasthiandAnupamGupta. Improvinglength-generalizationintransformersviatask
hinting. arXivpreprintarXiv:2310.00726,2023.
[AZL23] ZeyuanAllen-ZhuandYuanzhiLi. Physicsoflanguagemodels: Part1,context-freegrammar.
arXivpreprintarXiv:2305.13673,2023.
[BMR+20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgar-
wal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,
DanielM.Ziegler,JeffreyWu,ClemensWinter,ChristopherHesse,MarkChen,EricSigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish,AlecRadford,IlyaSutskever,andDarioAmodei. Languagemodelsarefew-shot
learners. InNeurIPS,2020.
[BN24] Gregor Bachmann and Vaishnavh Nagarajan. The pitfalls of next-token prediction. arXiv
preprintarXiv:2403.06963,2024.
[BPL+16] Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural
combinatorialoptimizationwithreinforcementlearning. arXivpreprintarXiv:1611.09940,
2016.
[CFK+23] Constantine Caramanis, Dimitris Fotakis, Alkis Kalavasis, Vasilis Kontonis, and Christos
Tzamos. Optimizingsolution-samplersforcombinatorialproblems: Thelandscapeofpolicy-
gradientmethod. InA.Oh,T.Naumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,
editors,AdvancesinNeuralInformationProcessingSystems,volume36,pages14035–14069.
CurranAssociates,Inc.,2023.
[CMWC22] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts
prompting: Disentanglingcomputationfromreasoningfornumericalreasoningtasks. arXiv
preprintarXiv:2211.12588,2022.
[CTJ+21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,
JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman,etal. Evaluating
largelanguagemodelstrainedoncode. arXivpreprintarXiv:2107.03374,2021.
18[DLS+24] NouhaDziri,XimingLu,MelanieSclar,XiangLorraineLi,LiweiJiang,BillYuchenLin,
SeanWelleck,PeterWest,ChandraBhagavatula,RonanLeBras,etal. Faithandfate: Limits
oftransformersoncompositionality. AdvancesinNeuralInformationProcessingSystems,36,
2024.
[GLFS24] Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, and Giorgos Sta-
mou. Puzzle solving using reasoning of large language models: A survey. arXiv preprint
arXiv:2402.11291,2024.
[GTLV22] ShivamGarg,DimitrisTsipras,PercySLiang,andGregoryValiant. Whatcantransformers
learnin-context? acasestudyofsimplefunctionclasses. AdvancesinNeuralInformation
ProcessingSystems,35:30583–30598,2022.
[HBK+21] DanHendrycks,CollinBurns,SauravKadavath,AkulArora,StevenBasart,EricTang,Dawn
Song,andJacobSteinhardt. Measuringmathematicalproblemsolvingwiththemathdataset.
arXivpreprintarXiv:2103.03874,2021.
[HXX+22] WenlongHuang,FeiXia,TedXiao,HarrisChan,JackyLiang,PeteFlorence,AndyZeng,
Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied
reasoningthroughplanningwithlanguagemodels. arXivpreprintarXiv:2207.05608,2022.
[JRR+24] Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, and Victor Veitch.
On the origins of linear representations in large language models. arXiv preprint
arXiv:2403.03867,2024.
[KGR+22] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
Largelanguagemodelsarezero-shotreasoners. Advancesinneuralinformationprocessing
systems,35:22199–22213,2022.
[LAD+22] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,
VinayRamasesh,AmbroseSlone,CemAnil,ImanolSchlag,TheoGutman-Solo,etal.Solving
quantitative reasoning problems with language models. Advances in Neural Information
ProcessingSystems,35:3843–3857,2022.
[LAG+22] BingbinLiu,JordanTAsh,SurbhiGoel,AkshayKrishnamurthy,andCyrilZhang. Trans-
formerslearnshortcutstoautomata. arXivpreprintarXiv:2210.10749,2022.
[LH16] IlyaLoshchilovandFrankHutter. Sgdr: Stochasticgradientdescentwithwarmrestarts. arXiv
preprintarXiv:1608.03983,2016.
[LHB+23] Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda B. Viégas, Hanspeter Pfister, and
MartinWattenberg. Emergentworldrepresentations: Exploringasequencemodeltrainedon
asynthetictask. InICLR,2023.
[Lon23] JieyiLong. Largelanguagemodelguidedtree-of-thought. arXivpreprintarXiv:2305.08291,
2023.
[LSL+23] NayoungLee,KartikSreenivasan,JasonDLee,KangwookLee,andDimitrisPapailiopoulos.
Teachingarithmetictosmalltransformers. arXivpreprintarXiv:2307.03381,2023.
[LSM+24] Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian.
Beyond a*: Better planning with transformers via search dynamics bootstrapping. arXiv
preprintarXiv:2402.14083,2024.
19[MHF+23] IdaMomennejad,HoseinHasanbeig,FelipeVieiraFrujeri,HiteshiSharma,NebojsaJojic,
HamidPalangi,RobertNess,andJonathanLarson. Evaluatingcognitivemapsandplanning
inlargelanguagemodelswithcogeval. InThirty-seventhConferenceonNeuralInformation
ProcessingSystems,2023.
[MKPZ11] ValeriMladenov,PKarampelas,CPavlatos,andEZirintsis. Solvingsudokupuzzlesbyusing
hopfieldneuralnetworks. Proc.ofICACM,11:174–179,2011.
[MP23] AlekseiMaslakovandBasilPapadimas. Sudokusolverwithstep-by-stepguidance. https:
//github.com/unmade/dokusan,2023.
[MSIB21] Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement
learning for combinatorial optimization: A survey. Computers & Operations Research,
134:105400,2021.
[NB21] DavidNoeverandRyersonBurdick. Puzzlesolvingwithoutsearchorhumanknowledge: An
unnaturallanguageapproach. arXivpreprintarXiv:2109.02797,2021.
[NCL+23] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress
measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217,
2023.
[NLW23] NeelNanda,AndrewLee,andMartinWattenberg. Emergentlinearrepresentationsinworld
modelsofself-supervisedsequencemodels. arXivpreprintarXiv:2309.00941,2023.
[PCV23] KihoPark,YoJoongChoe,andVictorVeitch. Thelinearrepresentationhypothesisandthe
geometryoflargelanguagemodels. arXivpreprintarXiv:2311.03658,2023.
[PPW18] RasmusPalm,UlrichPaquet,andOleWinther. Recurrentrelationalnetworks. Advancesin
neuralinformationprocessingsystems,31,2018.
[Rad20] DavidG.Radcliffe. 3millionsudokupuzzleswithratings,2020.
[RDM+24] AnianRuoss,GrégoireDelétang,SourabhMedapati,JordiGrau-Moya,LiKevinWenliang,
ElliotCatt,JohnReid,andTimGenewein. Grandmaster-levelchesswithoutsearch. arXiv
preprintarXiv:2402.04494,2024.
[RWC+19] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal.
Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
[SBM+23] IshikaSingh,ValtsBlukis,ArsalanMousavian,AnkitGoyal,DanfeiXu,JonathanTremblay,
DieterFox,JesseThomason,andAnimeshGarg. Progprompt: Generatingsituatedrobottask
plansusinglargelanguagemodels. In2023IEEEInternationalConferenceonRoboticsand
Automation(ICRA),pages11523–11530.IEEE,2023.
[SWW+23] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and
YuSu. Llm-planner: Few-shotgroundedplanningforembodiedagentswithlargelanguage
models. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pages2998–3009,2023.
[TAB+23] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,Jiahui
Yu,RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyof
highlycapablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
20[VOSK22] KarthikValmeekam,AlbertoOlmo,SarathSreedharan,andSubbaraoKambhampati. Large
language models still can’t plan (a benchmark for llms on planning and reasoning about
change). arXivpreprintarXiv:2206.10498,2022.
[VSP+17] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,
LukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InNIPS,2017.
[WSC+16] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural
machinetranslationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXiv
preprintarXiv:1609.08144,2016.
[WWS+22a] XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,SharanNarang,Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in
languagemodels. arXivpreprintarXiv:2203.11171,2022.
[WWS+22b] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,
DennyZhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.
Advancesinneuralinformationprocessingsystems,35:24824–24837,2022.
[XKZ+24] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and
MichaelXie. Self-evaluationguidedbeamsearchforreasoning. AdvancesinNeuralInforma-
tionProcessingSystems,36,2024.
[XZC+24] JianXie,KaiZhang,JiangjieChen,TinghuiZhu,RenzeLou,YuandongTian,YanghuaXiao,
andYuSu. Travelplanner: Abenchmarkforreal-worldplanningwithlanguageagents. arXiv
preprintarXiv:2402.01622,2024.
[YIL23] ZhunYang,AdamIshay,andJoohyungLee. Learningtosolveconstraintsatisfactionprob-
lems with recurrent transformer. In The Eleventh International Conference on Learning
Representations,2023.
[YS03] Takayuki Yato and Takahiro Seta. Complexity and completeness of finding another so-
lution and its application to puzzles. IEICE transactions on fundamentals of electronics,
communicationsandcomputersciences,86(5):1052–1060,2003.
[YXLAZ24a] TianYe,ZichengXu,YuanzhiLi,andZeyuanAllen-Zhu. Physicsoflanguagemodels: Part
2.1,grade-schoolmathandthehiddenreasoningprocess. arXivpreprintarXiv:2407.20311,
2024.
[YXLAZ24b] TianYe,ZichengXu,YuanzhiLi,andZeyuanAllen-Zhu. Physicsoflanguagemodels: Part
2.2,howtolearnfrommistakesongrade-schoolmathproblems,2024.
[YYZ+24] ShunyuYao, DianYu, JeffreyZhao, IzhakShafran, TomGriffiths, YuanCao, andKarthik
Narasimhan. Tree of thoughts: Deliberate problem solving with large language models.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[YZY+22] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and
Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint
arXiv:2210.03629,2022.
21[ZCS+23] Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B Tenenbaum, and
Chuang Gan. Planning with large language models for code generation. arXiv preprint
arXiv:2303.05510,2023.
[Zhu] RichardZhu. Solvingsudokupuzzleswithrecurrentneuralnetworks.
[ZLH24] ZiruiZhao,WeeSunLee,andDavidHsu.Largelanguagemodelsascommonsenseknowledge
forlarge-scaletaskplanning. AdvancesinNeuralInformationProcessingSystems,36,2024.
[ZSH+22] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale
Schuurmans,ClaireCui,OlivierBousquet,QuocLe,etal. Least-to-mostpromptingenables
complexreasoninginlargelanguagemodels. arXivpreprintarXiv:2205.10625,2022.
22A Details about our list of strategies
Asmentionedearlier,weconsiderpuzzleswith7strategiesforbothSudokuandZebrapuzzles.
A.1 StrategiesforSudokupuzzles
WefirstlistallthestrategiesusedinSudokupuzzleswithanexplanation.
1. Lonesingle: Thisstrategyisappliedtoacellwhereonlyonecandidatenumberispossiblebasedon
therules.
2. Hiddensingle: Thisstrategyisappliedthesituationwhereanumbercanonlybeplacedinonespecfic
cellwithinarow,columnorbox.
3. Nakedpair: Thisstrategyisappliedwhentwocellsinarow,columnorboxcontaintheexactsame
twoadmissiblenumbers. Thisstrategyisusedtoeliminatethenumberofpossiblevalues.
4. Nakedtriplet": Thisstrategyisappliedwhenthreecellsinarow,columnorboxcontaintheexact
samethreeadmissiblenumbers. Similartothe"nakedpair"strategy,thisstrategyisusedtoeliminate
thenumberofpossiblevalues.
5. Lockedcandidate: Thisstrategyisappliedwhenallthepossiblepositionsforaspecificnumberwithin
aboxareonthesameroworcolumn.
6. XYwing: Thisisacomplicatedstrategythatinvolvesthreecellsandmultipledeductionsteps. First,
identifyavacantcell(calledapivot)thathastwoadmissiblenumbers(denotedbyX andY);second,
identifytwoothercells(calledwingcells)suchthateachofthemsharesacolumn,roworboxwith
thepivot,andonecellhastwoadmissiblenumbersX andZ,andtheothercellhastwoadmissible
numbersY andZ. third,foreveryothercellthatshareacolumn,roworboxwithbothwingcells,Z
canbeeliminatedfromtheiradmissiblenumbers.
7. Uniquerectangle: Thisisanothercomplicatedstrategythatinvolvesfourcells. First,identifyfour
cellsthatformsarectanglesuchthatthreeofthesecellshaveonlytwoadmissiblenumbersandthe
numbers are the same, and the fourth cell share at least of the numbers as an admissible number;
second,bothnumberscanbeeliminatedfromtheadmissiblenumbersforthefourthcell.
A.2 RelationtypesforZebrapuzzles
WenowlistalltherelationshiptypesfortheZebrapuzzles. Theexamplesofthefollowingrelationtypesare
givenfortheoriginalZebrapuzzles.
1. Isequalto: Thisrelationtypeprovidesthevalueforanattribute. Anexampleofthistypeofcluecan
betheNorwegianlivesinthefirsthouse.
2. Isnotequalto: Thisrelationtypeprovidestheinformationthatanattributecannothaveaparticular
value. AnexampleofthistypeofcluecanbetheEnglishmandoesnotliveinthefirsthouse.
3. Immediateleft: Thisrelationtypeprovidestherelationorderforthevalueseitherbetweenattribute
valuesorentitiesinthesolution. Anexampleofthistypeofcluecanbethepersonwiththedogis
immediatelyleftofthepersonwhodrinksthecoffee.
23Figure7: Leftfigure: Plotsthenumberofmistakesmadeafterhowmanynumberofcellswerefilled. Right
figures: Plotsthenumberoffirstmistakesthataremadeagainstnumberofcellsthatwerefilledwhenitmade
thefirstmistake.
4. Neighbour of: This relation type provides the information that an entity with a particular attribute
valueistheneighborofanotherentity. Thisrelationtypegeneralizesthe"immediateleft"relationship
to include the immediate neighbors of the left and right sides. An example this type of clue is the
personwithadogisnexttothepersonwhodrinksmilk.
5. Endsin: Thisrelationtypeprovidestheinformationthatanentitywiththeparticularattributevalueis
oneitherendoftheorder. Forexample,thepersonwiththeZebraisoneitherendoftheorder.
6. Leftof: Thisrelationtypeprovidestherelativeorderoftwoentitieswithsomeparticularvalues. Note
thatleft-ofrelationdoesnotmeantheimmediateleftofanentity. Forexample,thepersonwhodrinks
teaisleftoftheJapaneseperson.
7. Inbetween: Thisrelationtypeprovidestherelativeorderofthreeentitieswithsomeparticularattribute
values. Forexample,theEnglishmanlivesin-betweenthepersonwiththeHorseandthepersonwho
drinkscoffee.
B Details about hyperparameters of training
Dataset. WeconsidertheSudokudatasetfrom[Rad20]andthenweadaptaSudokusolverfrom[MP23]
to filter out the puzzles that can not be solved by the 7 strategies listed above. After filtering, the dataset
contains1.9Mpuzzles. Werandomlychoose0.1Mpuzzlesfromthesepuzzlesandusethemasavalidation
datasetfortheevaluationofthemodelandtheremaining1.8Mpuzzlesarepartofourtrainingdataset.
WeusetheAdamWoptimizerforourexperiments. Foralltheexperiments,learningrateissetto1×10−4
andmodelsaretrainedfor4millionstepswithabatchsizeof64. Weusethecosinelearningrateschedule
[LH16]withthefirst4000tokensasthewarmupphaseandanendlearningratefactorof0.2.
C Mistake position frequency experiment.
WepresenttheresultsaboutthemistakefrequencyandfirstmistakefrequencyinFigure7. Inthissection,we
showthatforapuzzle,themodelmakesmorefirstmistakesforthatpuzzleatthestartofthepuzzlewhen
therearemoreemptycellsbecauseforamodel,itishardertopredictthecorrectvalueforthatcellbutthe
distributionofallmistakesismoretowardsthelatermistakes. Thisshowsthatwhenamodelmakesamistake
onasequence,itislikelythatitwillkeepmakingamistakebecauseoftheinvalidprefix.
24D Additional examples of failure mode of the model
Figure8: Additionalexamplesofthefailureofthemodelinsearchingforeasy-to-decodecells. Bothleft
figuresshowthesudokupuzzlestatewhenthemodelmakesthefirstmistakeandbothrightfigureshowsthe
correspondingpuzzle’ssolution. Numbersgivenintheblueareprovidedinthepuzzle. Thepuzzlemakesa
mistakebychoosingtofillthered-coloredcellwhereasthegreenbackgroundcellcanbeeasilyfilled.
E An example of candidate set for the puzzle
E.1 Sudokupuzzle
√ √
AgeneralizedversionofSudokuhasaboardofsizen×nwithnboxesofsize n× n,andthegoalof
√ √
thegameistofillapartiallyfilledboardsothateachrow,columnand n× nboxescontainsafullsetof
numbersfrom1ton. Implicitly,thismeansthatthegoalistofilloutthecompleteboardsuchthatnoneof
25therows,columns,andboxescontainduplicates. Inourexperiments,weconsiderSudokuofboardsize9×9
whichisfurtherdividedintonine3×3boxes.
E.2 Candidatesetexample
Thecandidatesetforaposition(r,c)keepstrackofallpossiblevaluesthatthecellcantake(SeeSection4.4
formoredetails)andthenusesthecandidatesetstoeitherdeduceavalueataparticularcellornarrowdown
thecandidateset(setofpossiblevalues)atanemptycell.
Figure9: Anexampleofthecandidatesetforapuzzle
26