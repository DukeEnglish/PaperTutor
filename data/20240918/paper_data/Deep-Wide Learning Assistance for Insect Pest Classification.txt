Deep-Wide Learning Assistance
for Insect Pest Classification
Toan T. Nguyen1,2, Huy T. Nguyen1,5, Huy Q. Ung4,
Hieu T. Ung1, Binh T. Nguyen1,2,3*
1AISIA Research Lab, Ho Chi Minh City, Vietnam.
2University of Science, Ho Chi Minh City, Vietnam.
3Vietnam National University, Ho Chi Minh City, Vietnam.
4KDDI Research, Inc., Fujimino City, Japan.
5Universit´e de Bourgogne, Dijon, France.
*Corresponding author(s). E-mail(s): ngtbinh@hcmus.edu.vn;
Contributing authors: nguyentientoan190401@gmail.com;
bihuyz@gmail.com; xhu-ung@kddi.com; ungtrunghieu99@gmail.com;
Abstract
Accurate insect pest recognition plays a critical role in agriculture. It is a chal-
lenging problem due to the intricate characteristics of insects. In this paper, we
presentDeWi,novellearningassistanceforinsectpestclassification.Withaone-
stage and alternating training strategy, DeWi simultaneously improves several
Convolutional Neural Networks in two perspectives: discrimination (by optimiz-
ingatripletmarginlossinasupervisedtrainingmanner)andgeneralization(via
dataaugmentation).Fromthat,DeWicanlearndiscriminativeandin-depthfea-
tures of insect pests (deep) yet still generalize well to a large number of insect
categories(wide).ExperimentalresultsshowthatDeWiachievesthehighestper-
formances on two insect pest classification benchmarks (76.44% accuracy on the
IP102datasetand99.79%accuracyontheD0dataset,respectively).Inaddition,
extensive evaluations and ablation studies are conducted to thoroughly investi-
gate our DeWi and demonstrate its superiority. Our source code is available at
this GitHub repository.
Keywords:InsectPestClassification,SmartAgriculture,ConvolutionalNeural
Networks,ContrastiveLearning,ImageAugmentation
1
4202
peS
61
]VC.sc[
1v54401.9042:viXra1 Introduction
Agricultureplaysavitalroleintheeconomyofmanycountriesandthefoodsupplyfor
the whole world’s population [1, 2]. Many factors, such as climate change, deforesta-
tion,soildegradation,andinsectpests,affectagriculturalcrops.Foralongtime,many
studies have been conducted to improve various aspects of agriculture [3]. Recently,
with the great achievements of machine learning, many applications have been devel-
oped and practically utilized to improve the farming efficiency and the quality of
agriculturalproduce[4–7].Inparticular,thereareseveralstudiesofmachine-learning-
based methods for weather forecasting [8, 9], disease forecasting [10, 11], or watering
schedule recommendation [12, 13]. Following the popularity of high-quality image
capturing devices and the rapid development of computer vision algorithms, vision-
basedsolutionshavebeenappliedinmanyaspectsofagricultureincludingmonitoring
crops[14,15],controllingdiseasesofcrops[16,17],automaticcropharvesting[18,19],
and automated management systems for crops [20, 21].
Insect pests are well-known as one of the most significant hazards to agricultural
produce [22]. Due to their rapid reproduction, evolution, and spread, insect pests can
cause hefty and irreversible damage to crops unless early warnings and actions are
taken. However, manually identifying insect pests on a large farm by human experts
is extremely time-consuming and expensive. An automated vision-based insect pest
recognition system is thus promising to perform this task more efficiently [23]. In this
work, by training on a large amount of pest image data, our method can accurately
classify many pest species given images from different sources (e.g., farmland cam-
era,mobilephone,newspaper,internet)withvaryingimagequality.Furthermore,our
model can be potentially embedded in smart devices or camera systems on fields to
help farmers identify insect pests effectively. Preventing insect pests in agriculture,
however, is a complex and multi-step process. In the scope of this work, we focus on
insect pest classification, which is one of the most important and challenging tasks. A
high-performancerecognitionmodelisofparamountimportanceforsubsequentmeth-
odstocounterinsectpests.Thesemethodsincludebiologicalcontrol[24,25],chemical
control [26, 27], and insect food consumption [28–30].
Thereareseveraldifficultiesinrecognizinginsectpests.Themostnotableproblem
is high intra-class and low inter-class variances. For example, an insect pest species
can have different forms at different growing stages (e.g., egg, pupa, worm), while
different species could be remarkably similar in visual appearance [31, 32]. Therefore,
recognizing insect pests can be considered as a fine-grained classification problem
that requires specific techniques to handle [33]. The second challenge is the long-
taileddistributionininsectpestdata,wheresomespeciesarefarmoreprevalentthan
others. Other challenges include the small scales of the insect pests and the missing
information caused by other objects (e.g., leaves, soil) obscuring.
Up to now, many methods [9, 34–44, 44–46], have been proposed for the insect
pestclassificationtask,eachfocusingondealingwithspecificdifficulties.Forinstance,
Ung et al. [40] designed an ensemble framework consisting of a feature pyramid net-
workthathandlestheproblemofhighlyvariedscalepestsandamulti-branchnetwork
that aims at discriminative part localization. For the challenge of long-tailed distri-
bution, Feng et al. [41] employed a decoupled learning [47] strategy. To address the
2imbalancedataproblem,authorsin[48]recentlyproposedamethodbasedonmultiple
residual convolutional blocks. The limitations of these methods are primarily in the
use of ensemble methods that integrate many different models or design complex and
burdensome neural networks with many modules. A high-performance and efficient
method is the decisive factor for the practicality of insect pest detection systems [23].
With that inspiration, in this research, we propose novel and effective Deep-Wide
(DeWi, in short) learning assistance that applies a novel one-stage learning strategy.
This strategy enables the model to learn in-depth features by employing the triplet
margin loss [49] and generalize effectively to a wide variety of pest categories with
the help of a data augmentation technique - Mixup [50]. Additionally, we leverage
the discrimination capability of the triplet loss in a novel and effective manner. Our
DeWi assistance is simple in design, easy to implement, and compatible with various
convolutionalNeuralNetwork(CNN)backbonearchitectures.Themaincontributions
of our work are summarized as follows:
1. We design a novel architecture that can apply to multiple CNN backbones. With
this architecture, we facilitate the deep-feature learning ability by employing the
triplet margin loss in a novel and effective style.
2. Weproposeanalternatingtrainingstrategytosimultaneouslypromoteourmodel’s
discrimination and generalization capabilities.
3. We conduct comprehensive experiments and achieve state-of-the-art performances
on insect pest classification benchmarks. We also conduct ablation studies to
investigate other aspects of our method.
The rest of the paper is organized as follows. First, we briefly present notable
relatedworksininsectpestclassification,contrastivelearning,anddataaugmentation
in Section 2. Next, the details of our proposed DeWi are illustrated in Section 3. We
thenpresentexperimentstodemonstratetheeffectivenessofourmethodandcompare
it with previous techniques in Section 4. Ablation studies are also presented in this
section. Finally, our paper ends with our conclusion and discussion for future works
in Section 5.
2 Related Works
This paper draws on the extensive literature of previous works on insect pest classifi-
cation, contrastive learning, and image data augmentation. In this section, we briefly
review the literature and summarize related studies of each domain.
2.1 Insect Pest Classification
Insect pest classification has been well-concerned for many years [51]. Various tradi-
tional methods have been proposed to tackle this task. For instance, Wen et al. [34]
proposed a local feature-based approach for orchard insects employing local region
feature detectors for feature detection, the scale-invariant feature transform [52] for
local feature description, the bag of words for visual representation, and six clas-
sifiers for classification. Xie et al. [35] proposed an insect recognition system using
3multi-tasksparserepresentationwithsparse-codinghistogramandmulti-kernellearn-
ing techniques. The authors in [9] constructed an automated identification system
by designing seven features following basic geometrical features. The classification
results are then obtained using artificial neural networks or support vector machine.
Althoughthesetraditionalmethodsacquiredpromisingresults,theycomewithseveral
limitations. In particular, they primarily depend on hand-crafted features, which are
time-consumingtocollectandrequireprofessionals’expertise.Traditionalapproaches,
therefore, are easily failed when dealing with domain shifts in the morphological fea-
tures of insects. Besides, the generalizability of such methods is also not satisfied as
the number of insect classes they could handle is severely limited. As a result, perfor-
mance of traditional methods is considerably low on several insect pest classification
benchmarks [32, 53], hindering their applicability in real-world scenarios.
In recent years, Convolutioxnal Neural Networks have been successfully used in
multiple domains [54]. CNN-based recognition methods [48, 55–61] have shown great
success in different computer vision tasks, e.g., classification [62], detection [63], seg-
mentation [64], and generation [65]. Modern approaches that leverage CNN models
achieveremarkableresultsininsectpestclassificationandoutperformmosttraditional
techniques. Yang et al. [37] proposed a convolutional rebalancing network to tackle
the problem of long-tailed distribution of insect pest data. Their network includes
a convolutional rebalancing module, an image augmentation module, and a feature
fusion module. Ung et al. [40] proposed a CNN-based ensemble model including a
multi-branch network [66], a residual attention network [67], and a feature pyramid
network [68]. Also using the ensemble method, Nanni et al. [38] combined six CNNs
and optimized the whole framework using two new optimizers improved from the
Adam algorithm [69]. Unlike previous works, our proposed DeWi does not combine
many networks or modules. Furthermore, DeWi can be applied or fine-tuned for dif-
ferent CNN backbones. These advantages make DeWi more applicable and adaptable
in practical settings.
2.2 Contrastive Learning
Contrastive learning has been widely explored in recent years for a variety of fields,
suchascomputervision[70],sensordataanalysis[71],naturallanguageprocessing[72],
robot learning [73], and graph representation learning [74]. Contrastive learning con-
sists of strategies that try to learn an embedding space in which the embeddings of
similarsamplesstayclosetoeachotherwhiletheembeddingsofdissimilaronesarefar
apart.Asimilaritymetricisusuallyemployedtodeterminehowclosetwoembeddings
are.Acontrastivelossisthecriticalcomponentofmostcontrastivelearningmethods.
In the particular field of computer vision tasks, it is computed on the embeddings of
theimagesextractedfromanencodernetwork.Themodelthenlearnsmeaningfulrep-
resentations of the input images by optimizing that loss. The obtained model weights
are later transferred to multiple downstream tasks.
Contrastive learning usually utilizes contrastive pairs or triplets to achieve gener-
alizable and transferable representations. For example, the authors in [75] proposed
a framework that encodes predictions over future observations by extracting compact
4latent representations and optimizing a loss function based on noisy-contrastive esti-
mation [76] called InfoNCE. A dynamic memory bank was presented by He et al. [77]
to store the embeddings of negative sample data. To make it more challenging to dis-
tinguishnegativesamplesfrompositivequeries,Huetal.[78]usedadversarialtraining
that maximizes the adversarial contrastive loss of miss-assigning each positive sample
to negative ones. Other studies [79, 80] suggested using prototypical representations
todiscriminate thedata tolocatemore appropriatenegative samples.Chen et al. [81]
demonstrated the importance of large batch size and the number of training steps
in contrastive self-supervised learning. Zbontar et al. [82] proposed a loss function
that naturally avoids trivial solutions [83] by calculating the cross-correlation matrix
between the outputs of two identical networks fed with distorted versions of a sam-
ple, and making it close to the identity matrix. Inspired by the success of contrastive
learninginmanyproblems,inthiswork,weapplyacontrastivelearninglossfunction-
thetripletmarginloss[49]inanovelone-stagemanner.Experimentalresultsindicate
that this approach significantly improves our model’s ability in learning meaningful
representations while optimizing for the primary classification task.
2.3 Image Augmentation
Insect pests can take on various appearances. Applying image augmentation methods
isthereforeapromisingapproachtoimprovethegeneralizabilityofrecognitionmodels.
Two traditional directions of image augmentation are geometric transformation and
color processing [84]. With geometric transformation, an image can be rotated to
change the perspective of its objects. Besides, depending on the characteristics of
the training and testing sets, flipping can be performed on images horizontally or
vertically. Color image processing for augmentation assumes that the distributions of
the training and testing datasets differ in colors, such as contrast or lighting. Such
methods alter the color properties of the input image by changing its pixel intensity.
These traditional augmentation methods preserve the input image’s general structure
and the corresponding label, thus limiting the model’s generalization ability [85].
Over the years, many advanced image augmentation methods have been proposed
toimprovetheperformanceofdeepneuralnetworksonimagerydata.Intensitytrans-
formation is a noteworthy sub-domain. One of the soonest and most straightforward
strategiesinthiskindisrandomnoiseinjection,suchasGaussiannoise[86].Onesub-
area of intensity-based data augmentation is image mixing and image deleting. To
encourage the network to focus on the overall structure of the object, this particular
sort of augmentation either fuses two images or deletes parts of an image to obscure
or confuse specific image properties. CutOut [87], CutMix [88], and MixUp [50] are
notable techniques that achieved significant improvements with various CNN archi-
tectures. With these advanced methods, objects of different classes can be mixed in
a single image, and the one-hot label is no longer preserved. Among those methods,
Mixup has shown to be the most impactful in the contrastive learning field. There
have been many recent additional studies of Mixup, such as Manifold MixUp [89] or
MixCo[90].Inthisstudy,weleveragethismethodforourDeWiassistancetoenhance
the generalization abilities of CNN models.
53 Methodology
In this section, we present our novel DeWi learning assistance to assist residual-
network-based classification models (ResNet-based models, in short) during their
training process. To improve discrimination and generalization capabilities, DeWi
consists of Deep steps and Wide steps, which are alternately applied during train-
ing, as shown in Figure 1. The architecture of our model consists of three main
modules:amulti-levelfeatureextractor,twolossfunctions,(i.e.,thetraditionalcross-
entropy loss [91] and the triplet margin loss [49]), and the Mixup data augmentation
module [50]. Our proposed model is trained by the supervised learning strategy
iteration-by-iteration [92]. In an epoch, an iteration is defined as a process of forward
propagationfollowedbybackwardpropagationforthemini-batchofimages.TheDeep
step is executed at the (2e+1)-th (e ∈ N) iterations (odd iterations), whereas the
Wide step is executed at the (2e)-th iterations (even iterations). Two steps cannot be
applied in the same iteration because the triplet loss in the Deep step requires labels
of training samples as one-hot vectors, while those in the Wide step are mixed labels
due to applying the Mixup data augmentation.
Fig. 1 The overview of our proposed method. During training, the Deep step and Wide step are
alternatelyappliedinanepoch.WeusethesamenetworkarchitectureinDeepandWidesteps.The
tripletmarginlossiscomputedintheDeepstep,whiletheMixupdataaugmentationisemployedin
theWidestep.Ininference,theinputimageisfedthroughtheentirenetwork,fromthemulti-level
featureextractorF(·)tothelinearlayerandthesoftmaxlayer,togetthefinalprediction.
WedesignDeWifortheResNet-basedmodels(ResNet[58],ResNext[59]andWide
ResNet[60]),butthemethodcanbeadaptedforusewithothernetworks.Wedescribe
our architecture and learning strategy in detail below.
63.1 Deep step
We first define the important notations. Given a mini-batch of B input samples, we
denote the t-th training sample as (cid:0) x(t),y(t)(cid:1) , t = 1,2,..,B, where x(t) ∈ RH×W×C
is the C-channel input image of size H ×W and y(t) ∈ RK is the one-hot ground
truth label, K is the number of insect pest classes in the dataset. We also denote
yˆ(t) ∈RK astheresultsfromtheclassifier.Inspiredbycontrastivelearning,wepropose
the Deep training step to assist the ResNet-based models in extracting fine-grained
and meaningful features. In the Deep step, we apply an additional contrastive loss
function,i.e.thetripletmarginloss,toforcethesemodelstodiscriminateimagesthat
are similar in appearance but belong to different classes.
Inordertoapplythetripletmarginloss[49],wemodifytheResNet-basednetworks
via the following steps. First, we remove the final linear layer from these networks.
Wethenformamulti-levelfeatureextractorF(·)byattachingtwoprojectorstothese
ResNet-based models. Concretely, a high-level projector is connected to an average
pooling layer right after the last core block (it is important to know that the ResNet-
based models have a general architecture of four consecutive core blocks - blue blocks
in Figure 1). Besides, a low-level projector is attached to the end of the third core
blocktoencouragethemodeltolearnfine-grainedfeaturesatalowerlevel,asshownin
Figure1.Toconnectthelow-levelprojectortothethirdcoreblock,theoutputofthis
block is first plugged into a 2D convolution layer of 2048-dimensional (dim) output.
Its output is then fed to an adaptive pooling layer, whose output is flattened as a
2048-dim feature vector. This vector is then used as input to the low-level projector.
Both the low-level and high-level projectors have the same architecture, as shown
in Figure 2; however, their parameters are not shared. Each projector has three linear
layers of 4096-dim output. The first two layers are followed by a batch normalization
layer [93] and ReLU activation function [94]. For each input image x(t), we obtain
two 4096-dim embedding vectors from each projector. We then concatenate these two
vectors to get a multi-level semantic representation vector z(t) of 8192-dim.
Fig. 2 Thedetailedarchitectureofourprojectornetwork.Boththehigh-levelandlow-levelprojec-
torsproduce4096-dimvectors.
Our main objective in this design is to optimize an embedding space where the
learned feature vectors of images of the same class are close together, and features
of images of different classes are located far apart. To achieve that, we optimize our
7model using the triplet margin loss [49] as follows:
 
L T({z(t),y(t)}B t=1)=(cid:80)B t=1  m+max p=1,...B D(cid:0) z(t),z(p)(cid:1) − min n=1,...BD(cid:0) z(t),z(n)(cid:1)   , (1)
y(p)=y(t) y(n)̸=y(t)
+
where the margin m is a hyper-parameter, and D(·) is a distance function; we use
the Euclidean distance in this study. As presented in Equation 1, we apply the batch
hardminingstrategyintroducedin[49]forourtripletmarginloss.Giventhesemantic
representation of one sample, we find the hardest positive sample (of the same class
but with the largest distance) and the hardest negative sample (of a different class
but with the smallest distance) and use those to form a single triplet contributing to
the loss function.
Subsequently, we normalize the embedding z(t) by a drop-out layer and feed it to
a linear layer, followed by a softmax layer, to get the final outputs yˆ(t). During the
Deep step, we optimize the overall loss function formulated as follows:
L({z(t),yˆ(t),y(t)}B )=β ×L ({yˆ(t),y(t)}B )+β ×L ({z(t),y(t)}B ), (2)
t=1 1 CE t=1 2 T t=1
whereL isthecross-entropyloss[91]computedonthefinaloutputsandtheground-
CE
truth labels, and β and β are hyper-parameters.
1 2
3.2 Wide step
In the Wide step, our assistance applies the Mixup data augmentation [50] for images
in a batch. A new input image and the corresponding label are sampled from two
randomly chosen original samples
(cid:0) x(i),y(i)(cid:1)
and
(cid:0) x(j),y(j)(cid:1)
as follows:
x˜ =λx(i)+(1−λ)x(j), (3)
y˜ =λy(i)+(1−λ)y(j), (4)
where λ ∼ Beta(α,α), for α ∈ (0,∞). Examples of Mixup augmentation are shown
in Figure 3. It is important to note that we do not increase the number of images
loadedtomemory.Thenewimagesaresampledfromtheoriginalones,buttheoverall
number of samples fed to the network stays the same. From that, we do not burden
the learning process, yet we still utilize the advantages of Mixup augmentation. We
use the same architecture in the Wide step as in the Deep step. Yet in this step, we
optimizethecross-entropylossL alonewithnewgroundtruthy˜ obtainedsincethe
CE
triplet margin loss requires the initial one-hot labels, which are not preserved when
applying Mixup augmentation. The pseudo-code for DeWi in a particular epoch is
shown as Algorithm 1.
In inference, the input image is fed through the whole network (the multi-level
feature extractor, then the linear layer, and finally, the softmax layer) to predict the
insect pest class. The inference stage of DeWi is shown in Figure 1.
8Fig. 3 Examples of images from IP102 dataset augmented by Mixup. Top: mix of two images of
differentclasses.Bottom: mixoftwoimagesofthesameclass.
4 Experiments
In this section, we conduct several experiments to validate the effectiveness of DeWi.
First, DeWi is compared with other state-of-the-art methods on the two widely used
insect pest classification benchmarks, i.e., IP102 [32] and D0 [31]. Afterward, we
analyzetheperformancesofbaselineresidualmodelswith/withoutapplyingourDeWi.
4.1 Datasets
Thereareseveraldatasetsthathavebeenintroducedforthetaskofinsectpestrecog-
nition [9, 31, 32, 35, 95, 96]. Among them, the D0 [31] and IP102 [32] datasets are
ones with the largest numbers of insect classes. Therefore, we use them to evaluate
our proposed method in this study. D0 contains 4,508 images of 40 insect pest species
capturedinthenaturalenvironment.Figure4showsexamplesofthisdataset.IP102is
currently the largest public dataset for the insect pest classification task, with 75,222
images of 102 pest species. Examples of IP102 are shown in Figure 5. This dataset is
more challenging than D0 due to multiple factors. Firstly, the challenge of high intra-
class (shown in column (a) of Figure 5) and low inter-class (shown in column (b) of
Figure 5) variances of its is particularly significant. Secondly, there are a considerable
number of images captured of the damaged crops caused by pests, as shown in col-
umn (c). In addition, there are images of small-scale pests on noisy backgrounds, as
shown in column (d). Lastly, the distribution of instances of different classes in IP102
is significantly imbalanced. This dataset also covers a large number of pest species
and a massive number of images, providing a practical simulation of the biodiversity
of pests in real-world scenarios.
4.2 Evaluation metrics
Previousworks[40,41]primarilyevaluatedtheinsectrecognitionmodelsonthreemet-
rics,consistingoftheaccuracyscore(Acc),themacro-averageF1-score(mF1),andthe
geometric mean score (GM). Similarly, we also use these metrics in our experiments.
We detail their calculations in detail below.
9Algorithm 1 Pseudo-code for DeWi in the training phase.
# INPUT:
# + batch_data: a batch of input images.
# + batch_labels: a batch of corresponding labels.
# OUTPUT: classification model trained.
# NOTATIONS:
# + mnb_data: a mini-batch of input images in batch_data.
# + mnb_labels: a mini-batch of corresponding labels in batch_labels.
# + F: the multi-level feature extractor.
# + Linear: linear layer.
# + Softmax: softmax function.
# + l_triplet: triplet loss function with the batch hard miner.
# + l_ce: cross-entropy loss function.
# + mixup: mixup function.
# + beta1, beta2: weights of cross-entropy and triplet loss values.
Initialize deep_turn to True
Repeat for (batch_data, batch_labels):
If deep_turn is True:
# Deep step
For each (mnb_data, mnb_labels) in (batch_data, batch_labels):
features = F(mnb_data)
features = Linear(features)
results = Softmax(features)
triplet_loss = l_triplet(features, mnb_labels)
ce_loss = l_ce(results, mnb_labels)
total_loss = beta1 * ce_loss + beta2 * triplet_loss
Update model using total_loss
Else:
# Wide step
For each (mnb_data and mnb_labels) in (batch_data, batch_labels):
mixed_data, mixed_labels = mixup(mnb_data, mnb_labels)
features = F(mixed_data)
features = Linear(features)
results = Softmax(features)
ce_loss = l_ce(results, mixed_labels)
total_loss = ce_loss
Update model using total_loss
If stopping training conditions are satisfied:
Stop repeat.
Toggle deep_turn
End Repeat
We compute the recall (Rec) for each class and then take the average over all
classes to obtain the mean recall (mRec) as follows:
TP
Rec = k , (5)
k TP +FN
k k
(cid:80)K
Rec
mRec= k=1 k, (6)
K
where k denotes the k-th class and K is the number of classes. TP and FN stand for
thetruepositiveandthefalsenegativeoutcomes,respectively.Similarly,theprecision
10Fig. 4 ExamplesofsixinsectpestsinD0dataset.
(Pre) for one class and the mean precision (mPre) is defined by:
TP
Pre = k , (7)
k TP +FP
k k
(cid:80)K
Pre
mPre= k=1 k. (8)
K
FP stands for the false positive result. mF1 is then computed as the harmonic mean
of mRec and mPre as follows:
mRec×mPre
mF1=2× . (9)
mRec+mPre
The accuracy is computed by the number of true predictions over all samples:
(cid:80)K
TP
Acc= k=1 k, (10)
N
where N is the number of image samples.
Finally, we calculate the geometric mean score from the recall outcomes of every
class as follows:
K
(cid:89) (cid:112)
GM= K Rec . (11)
k
k=1
4.3 Experimental settings
ThefirstdatasetusedinthisworkistheIP102dataset.Theauthorsin[32]partitioned
the IP102 into three subsets: a training set of 45,095 images, a validation set of 7,508
images, and a testing set of 22,619 images. For fair comparisons, we used the same
settingofIP102inourexperiments.AllIP102imagesarefirstresizedto400×400.In
11Fig. 5 Samples of IP102 dataset. Column (a) presents three different morphologies corresponding
to three developmental stages of the same worm species. Column (b) shows examples of three dif-
ferentbutterflyspecies,buttheirappearancesareparticularlyhardtodistinguish.Column(c)gives
examples of damaged crop fields without the appearance of insect pests. Column (d) shows images
ofsmall-scaleinsectsonnoisybackgrounds.
the training phase, the images are applied multiple transformations after being ran-
domlycroppedto384×384.Forvalidationandtesting,theimagesarecenter-cropped
to 384×384. For the D0 dataset, we apply the ratio 7:1:2 for training, validation,
and testing sets, which is the same as in [40]. We do not crop the validation and test
images of this dataset.
We initialize residual backbones using the weights pre-trained on the Imagenet
dataset [97]. We set α = 1 for Mixup augmentation. For the hyper-parameter m of
the triplet margin loss, we try multiple values of 0.1, 0.2, and 0.3, among which 0.2
gives the best performance. Weights of the loss function in the Deep step are set as
β = β = 1.0. During training, we optimize the model using Stochastic Gradient
1 2
Descent(SGD)[98]withamomentumof0.9andanL2weightdecayof1e−4.Weuse
multi-step decay for scheduling the learning rate after every 15 epochs with an initial
learning rate of 3e−3, a decay rate of 0.9, and the minimum learning rate of 3e−7.
The mini-batch size and the number of epochs are set to 32 and 100, respectively.
4.4 Compare with other state-of-the-art methods
Wecompareourproposedmethodwithotherstate-of-the-artinsectpestclassification
methods on IP102 and D0 datasets. For brevity, we refer to the networks with the
supportofDeWiasDeWinetworksorDeWimodels,orjustDeWi.WeuseResNet-152
for the backbone of DeWi. The comparison results are shown in Table 1. On IP102,
DeWi outperforms other methods by large margins on all three metrics. Specifically,
12DeWi outperforms the second-best method by 1.83, 1.64, and 1.75 percentage points
on accuracy, mF1, and GM, respectively. On the D0 dataset, DeWi also achieves the
highest results. Concretely, DeWi reaches 99.79% accuracy while the second place
method [40] obtains 99.78%.
Table 1 ComparisonbetweenourproposedDeWiandthe
previousworks.
Dataset Method Acc mF1 GM
IP102 Ayanet al.[99] 67.13 65.76 -
Yanget al.[37] 70.42 - -
Nanniet al.[38] 73.62 - -
Unget al.[40] 74.13 67.65 62.52
Fenget al.[41] 74.61 67.82 63.32
DeWi(ours) 76.44 69.46 65.07
D0 Thenmozhiet al.[100] 95.97 - -
Yuet al.[39] 98.16 98.34 -
Ayanet al.[36] 98.81 98.88 -
Unget al.[40] 99.78 99.68 99.65
DeWi(ours) 99.79 99.70 99.65
4.5 Compare with baseline residual networks
We investigate the improvements of DeWi-assisted models compared to baseline
residual networks. We also validate the effectiveness of the multi-level extractor by
comparing DeWi models with their counterparts that use a single-level projector. In
the design of such models, we only use the high-level projector and discard the low-
level one. We change the output dimension of the last linear layer of the high-level
projector to 8192 and compute the triplet margin loss for its output vector. The rest
of the network stays the same.
We report the results on the accuracy score in Figure 6. From now on, the IP102
dataset will be used for our evaluation if not stated otherwise. The results show that
DeWi significantly improves the performances of all the baseline networks, from 0.78
percentage points on Wide ResNet-50-2 to 1.4 percentage points on Wide ResNet-
101-2. Additionally, the reported results demonstrate the effectiveness of the design
of the multi-level feature extractor when models applying this design (DeWi models)
outperform their single-projector counterparts. On our best version using ResNet-152
astheunderlyingbackbone,DeWioutperformsby0.46percentagepointsinaccuracy.
Theseresultsdemonstratethatthemulti-levelextractorhelpsmodelslearnimportant
featuresatbothlowandhighlevels.Inaddition,thetwoprojectorssupporteachother
and contribute to the overall performance.
4.5.1 Activation map
We present additional visualizations to compare our DeWi with baseline residual net-
works. We select several cases where the baseline ResNet-152 misclassifies while our
13Fig.6 Accuracyscorecomparisonbetweenoriginalresidualnetworks,DeWinetworks,andnetworks
usingasingleprojector(SPmodels).Weusedifferentresidualmodelsforourevaluation,i.e.,ResNet,
ResNext,andWide-ResNet.
DeWi works correctly. We employ the Gradient-Weighted Class Activation Mapping
(Grad-CAM) in [101] and provide class activation maps using the flowing gradients
through the last core block. Visual results in Figure 7 demonstrate that the DeWi
model is better at focusing on meaningful features than the baseline ResNet-152.
4.5.2 Feature space
We further analyze the learning ability of our DeWi by utilizing the t-SNE algo-
rithm [102] to visualize the feature space. In particular, we select test images of five
random classes from the IP102 dataset and present the feature visualization of the
baseline ResNet-152 and our DeWi on a 2D plane. For ResNet-152, we select the flat-
tenedoutputvectorofthelastcoreblock.ForourDeWi,wechoosetheconcatenating
feature vector of the multi-level extractor. The t-SNE is run for 1,000 iterations, with
resultisindicatedinFigure8.ThevisualizationshowsthatDeWiisbetteratgrouping
features of the same classes and discriminating features of different classes, fortifying
its effectiveness.
4.6 Efficiency evaluation
Previous works usually employ ensemble methods that integrate many different mod-
els [36, 38, 40] or design heavy neural networks with many components that require
many times of forwarding through the backbone [41]. In contrast, DeWi is simple and
exhibits a compact design. For that reason, our method is expected to be more effi-
cient. To validate this point, we perform an evaluation to compare the running time
14Fig.7 VisualizationresultsoftheResNet-152whenapplyingandnotapplyingDeWi.Eachcolumn
representstheresultsofaspecificinsect.Top row: inputimages.Middle row: resultsofthebaseline
model.Bottom row: resultsoftheDeWimodel.
of our method with the method proposed by Ung et al. [40], which is the only one
whose source code is publicly available. Specifically, we benchmark the average infer-
encetime(IT)ofanimageforeachmodelonanNVIDIAGeForceRTX3090TiGPU.
The results shown in Table 2 indicate that most of DeWi models are faster than [40]
(with the only exception of Wide ResNet-101-2) while, as shown in Section 4.5, all
DeWi models consistently outperform [40] by a large margin.
Ungetal. DeWi DeWi DeWi DeWiResNext DeWiResNext DeWiResNext DeWiWide DeWiWide
[40] ResNet-50 ResNet-101 ResNet-152 -5032x4D -10132x8D -10164x4D ResNet-50-2 ResNet-101-2
IT(ms) 76.23 60.93 61.52 68.95 61.77 70.54 70.19 69.42 78.55
Table 2 Inferencetimecomparison.
4.7 Ablation Studies
In this section, we conduct multiple ablation studies to investigate other aspects of
DeWi. Firstly, we analyze the importance of each of DeWi’s components. We then
demonstratetherobustnessofDeWitothechangeinbatchsize.Next,webenchmark
the performance of DeWi when replacing the triplet margin loss with other state-
of-the-art loss functions in contrastive learning. Lastly, we compare DeWi with the
self-supervisedsettinganddiscusstheoutcomes.WeuseResNet-152astheunderlying
architecture for our ablation studies.
15(a) ResNet152 (b) DeWi
Fig. 8 The t-SNE visualization results of the baseline ResNet-152 model and our DeWi model for
fiverandomlyselectedclasses.
The importance of each component.Weconductthefirststudyontheimpor-
tance of each DeWi’s component and report the results in Table 3. Note that for the
networks that use triplet margin loss but without the multi-level feature extractor,
the triplet margin loss is computed on the output of the last block of the residual
backbone (a 2048-dim vector). For the model that applies Mixup augmentation but
discards the triplet margin loss, we use Mixup for all mini-batch. The results indi-
catethattripletmarginlossimprovestheperformancesofnetworksnotably.Applying
only the Mixup, on the other hand, does not improve the performance of the baseline
modelandthemodelusingtripletlosswithoutthemulti-levelextractor.Infact,their
performancesareevenworsethantheoriginalResNet-152.Mixup,however,improves
the model’s performance using the multi-level extractor and the triplet margin loss
by 0.8 percentage points on the accuracy metric. The combination of all the compo-
nents (DeWi model) achieves the best performance, illustrating the effectiveness of
the overall framework.
Table 3 TheimportanceofeachDeWi’scomponent.ResNet-152ischosenas
theunderlyingnetwork.MP denotesthemodelsemployingthemulti-level
featureextractor.
Method Acc mF1 GM
ResNet-152 75.15 68.06 64.89
ResNet-152+Mixup 75.07 66.68 61.19
ResNet-152+Mixup+tripletloss 75.11 67.55 63.58
ResNet-152+MP 74.86 67.77 64.70
ResNet-152+MP+tripletloss 75.64 68.12 64.81
ResNet-152+Mixup+MP+tripletloss(DeWi) 76.44 69.46 65.07
16Change in the margin hyper-parameter of triplet loss.Themarginmisan
importanthyper-parameterforthetripletmarginloss[49].Forthatreason,wereport
theperformanceofDeWiwithdifferentmarginvaluesof0.1,0.2,and0.3.Theresults
in Figure 9 show that the model with the margin of 0.2 achieves the best results on
three metrics. We realize that when the margin is overly low, the triplet margin loss
works ineffectively because of the low contrasting level. In contrast, when the margin
is too high, it is challenging to optimize our loss. The appropriate value 0.2 of m
balances the learning of our model and leads the highest result.
Fig. 9 PerformancesofDeWiwithdifferenttripletlossmargins.
Different contrastive learning loss functions. We validate the effectiveness
of the triplet margin loss function with the batch hard miner in our method by alter-
nately replacing it with other two state-of-the-art contrastive learning loss functions,
i.e., normalized temperature-scaled cross-entropy loss (NTXent loss) [81] and Circle
loss [103]. For the NTXent loss L , we set the temperature hyperparameter to
NTXent
0.07. The overall loss is computed by L = L +0.1×L . For the Circle loss
CE NTXent
L , we set the relaxation factor to 0.4 and the scale factor to 80. The overall loss
Circle
is then computed by L = L +0.01×L . The results are shown in Table 4.
CE Circle
Highestscoresonallthreemetricsbelongtothetripletmarginloss.However,allthree
lossfunctionsstablyobtaingoodresults,demonstratingtheflexibilityofourproposed
framework.
Table 4 PerformancesofDeWiwithdifferentcontrastivelearninglossfunctions.
Batchsize Acc mF1 GM
NTXentloss[81] 75.75 68.90 64.55
Circleloss[103] 76.18 69.13 64.88
Tripletmarginloss+batchhardminer[49](ours) 76.44 69.46 65.07
17The robustness to batch size.Thebatchsizehyper-parameterisanimportant
factorforeverycontrastivelearningmethod[70].Therefore,wealsostudyitseffecton
the performance of our proposed DeWi. In particular, we evaluate the performances
ofDeWi-assistedmodelswiththebatchsizeof32,24,16,8.Forthemodelswithbatch
sizesof24,16,and8,weset1e−3asthestartinglearningratetospeedupthelearning
process. Figure 10 shows that all DeWi models trained by those different batch sizes
obtain better results than other works (Table 1).
Fig. 10 PerformancesofDeWiwithdifferentbatchsizes.
The effect of hyperparameter α in Mixup. In the Mixup process, we choose
α = 1.0 so that the Beta(α,α) distribution is equivalent to a uniform distribution.
Our reason for this choice is that the uniform distribution is suitable for establishing
differentmixedcombinationsofinputdatawithvariousratios,consequentlyimproving
our model’s generalization. To prove this point, we perform an additional ablation
study on different values of α. Particularly, we report the additional results on the
IP102 dataset of our DeWi model with ResNet-152 backbone when α = 0.5 and
α = 2.0, which lead to non-uniform distributions. Table 5 shows that α = 1.0 yields
the best performance, validating our choice.
Table 5 Ablationstudyonα.
α Acc mF1 GM
0.5 76.13 69.25 64.71
1.0 76.44 69.46 65.07
2.0 76.08 69.12 64.30
Comparison with the pretext-based setting. We compare DeWi with a two-
stage training setting where we train the model on a pre-text task in a self-supervised
18manner and then fine-tune the pre-trained network for the downstream classification
task.Intheself-supervisedtrainingstage,weremovethelastlinearlayerofourDeWi
and learn a pre-text task by optimizing the triplet margin loss on the concatenating
output of the multi-level extractor. We train the model for 100 epochs with an initial
learning rate of 3e−4. After pre-training, we re-insert the linear layer and train the
downstreamclassificationtaskfor50moreepochs.Itisimportanttonotethatduring
the training on the main task, we freeze the pre-trained part and only update weights
of the inserted linear layer.
Table 6 shows the experimental results. First, it is shown that DeWi significantly
outperformstheself-supervisedpre-trainingmethod.Concretely,DeWiislargelybet-
ter than its counterpart by 13.36, 15.74, and 20.26 percentage points on accuracy,
mF1,andGM,respectively.Wearguethatthereasonforthedeficientperformanceof
the self-supervised setting is that the batch size 32 is insufficiently small to learn the
pre-text task. Like other self-supervised approaches, this setting is likely to require
the batch size to be sufficiently large to achieve acceptable results. DeWi’s one-stage
training fashion, on the other hand, makes it work effectively even with small batch
sizes.ItalsofosterstheefficiencyandapplicabilityofDeWiinreal-worldapplications.
Table 6 ComparisonbetweenDeWiandthe
pretext-basedsetting.
Method Acc mF1 GM
Self-supervisedsetting 63.08 53.72 44.81
DeWi(ours) 76.44 69.46 65.07
5 Conclusion
We have presented DeWi, novel learning assistance that can be applied to a wide
range of CNN architectures. DeWi simultaneously facilitates the capabilities of learn-
ingdiscriminativefeaturesandgeneralizingtoawiderangeofinsectpestclasses.Our
extensive experimental results showed that DeWi significantly improves the recogni-
tioncapabilitiesofseveralCNNmodels,outperformingotherstate-of-the-artmethods
oninsectpestrecognition.Wehavealsopresentedcomprehensiveablationstudiesand
experiments to validate the effectiveness and robustness of our method.
For future works, we plan to apply our DeWi for other CNN networks as well as
othermodernarchitectureslikeVisionTransformer[104]toimprovetheperformance.
Besides, we aim to apply many other different contrastive learning loss functions and
further propose our novel loss for the particular task of insect pest recognition. In
addition,wewillalsoinvestigatetheperformanceofDeWiwhenadoptingotherimage
data augmentation methods, such as CutMix [88].
19References
[1] Johnston, B.F., Mellor, J.W.: The role of agriculture in economic development.
The American Economic Review 51(4), 566–593 (1961)
[2] Bruinsma,J.:Worldagriculture:towards2015/2030:anFAOperspective(2017)
[3] Liakos,K.G.,Busato,P.,Moshou,D.,Pearson,S.,Bochtis,D.:Machinelearning
in agriculture: A review. Sensors 18(8), 2674 (2018)
[4] King,A.:Technology:Thefutureofagriculture.Nature544(7651),21–23(2017)
[5] Kamilaris, A., Prenafeta-Boldu´, F.X.: Deep learning in agriculture: A survey.
Computers and electronics in agriculture 147, 70–90 (2018)
[6] Zhu, N., Liu, X., Liu, Z., Hu, K., Wang, Y., Tan, J., Huang, M., Zhu, Q., Ji,
X., Jiang, Y., et al.: Deep learning for smart agriculture: Concepts, tools, appli-
cations, and opportunities. International Journal of Agricultural and Biological
Engineering 11(4), 32–44 (2018)
[7] Zhang, Q., Liu, Y., Gong, C., Chen, Y., Yu, H.: Applications of deep learning
for dense scenes analysis in agriculture: A review. Sensors 20(5), 1520 (2020)
[8] Salman, A.G., Kanigoro, B., Heryadi, Y.: Weather forecasting using deep
learning techniques. In: 2015 International Conference on Advanced Computer
Science and Information Systems (ICACSIS), pp. 281–285 (2015). https://doi.
org/10.1109/ICACSIS.2015.7415154
[9] Wang, J., Lin, C., Ji, L., Liang, A.: A new automatic identification system of
insect images at the order level. Knowledge-Based Systems 33, 102–110 (2012)
[10] Domingues,T.,Brand˜ao,T.,Ferreira,J.C.:MachineLearningforDetectionand
Prediction of Crop Diseases and Pests: A Comprehensive Survey. Agriculture
12(9), 1350 (2022)
[11] Fenu, G., Malloci, F.M.: An application of machine learning technique in fore-
casting crop disease. In: Proceedings of the 3rd International Conference on Big
Data Research, pp. 76–82 (2019)
[12] Sit,M.,Demiray,B.Z.,Xiang,Z.,Ewing,G.J.,Sermet,Y.,Demir,I.:Acompre-
hensive review of deep learning applications in hydrology and water resources.
Water Science and Technology 82(12), 2635–2670 (2020)
[13] Abioye,E.A.,Hensel,O.,Esau,T.J.,Elijah,O.,Abidin,M.S.Z.,Ayobami,A.S.,
Yerima, O., Nasirahmadi, A.: Precision irrigation management using machine
learning and digital farming solutions. AgriEngineering 4(1), 70–103 (2022)
[14] Zhu,Y.,Cao,Z.,Lu,H.,Li,Y.,Xiao,Y.:In-fieldautomaticobservationofwheat
20headingstageusingcomputervision.BiosystemsEngineering143,28–41(2016)
[15] Sadeghi-Tehran, P., Sabermanesh, K., Virlet, N., Hawkesford, M.J.: Automated
methodtodeterminetwocriticalgrowthstagesofwheat:headingandflowering.
Frontiers in Plant Science 8, 252 (2017)
[16] Wang,G.,Sun,Y.,Wang,J.:Automaticimage-basedplantdiseaseseverityesti-
mation using deep learning. Computational intelligence and neuroscience 2017
(2017)
[17] Sabzi, S., Abbaspour-Gilandeh, Y., Garc´ıa-Mateos, G.: A fast and accurate
expert system for weed identification in potato crops using metaheuristic
algorithms. Computers in Industry 98, 80–89 (2018)
[18] Wei, J., Zhijie, Q., Bo, X., Dean, Z.: A nighttime image enhancement
method based on retinex and guided filter for object recognition of apple
harvesting robot. International Journal of Advanced Robotic Systems 15(1),
1729881417753871 (2018)
[19] Davidson, J.R., Silwal, A., Hohimer, C.J., Karkee, M., Mo, C., Zhang, Q.:
Proof-of-concept of a robotic apple harvester. In: 2016 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pp. 634–639 (2016)
[20] Halstead,M.,McCool,C.,Denman,S.,Perez,T.,Fookes,C.:Fruitquantityand
ripenessestimationusingaroboticvisionsystem.IEEEroboticsandautomation
LETTERS 3(4), 2995–3002 (2018)
[21] Gonz´alez-Esquiva, J., Oates, M.J., Garc´ıa-Mateos, G., Moros-Valle, B., Molina-
Mart´ınez, J.M., Ruiz-Canales, A.: Development of a visual monitoring system
for water balance estimation of horticultural crops using low cost cameras.
Computers and Electronics in Agriculture 141, 15–26 (2017)
[22] Dent, D., Binks, R.H.: Insect pest management (2020)
[23] Rustia, D.J.A., Chao, J.-J., Chiu, L.-Y., Wu, Y.-F., Chung, J.-Y., Hsu, J.-C.,
Lin,T.-T.:Automaticgreenhouseinsectpestdetectionandrecognitionbasedon
a cascaded deep learning classification method. Journal of Applied Entomology
145(3), 206–222 (2021)
[24] Kenis, M., Hurley, B.P., Hajek, A.E., Cock, M.J.: Classical biological control of
insectpestsoftrees:factsandfigures.BiologicalInvasions19,3401–3417(2017)
[25] Cock, M.J., Murphy, S.T., Kairo, M.T., Thompson, E., Murphy, R.J., Francis,
A.W.: Trends in the classical biological control of insect pests by insects: an
update of the BIOCAT database. BioControl 61, 349–363 (2016)
[26] Pickett, J., Wadhams, L., Woodcock, C.: Developing sustainable pest control
21from chemical ecology. Agriculture, ecosystems & environment 64(2), 149–156
(1997)
[27] Roubos, C.R., Rodriguez-Saona, C., Isaacs, R.: Mitigating the effects of insec-
ticides on arthropod biological control at field and landscape scales. Biological
control 75, 28–38 (2014)
[28] Ku¨¸cu¨kkurt,I.,Ince,S.,Kele¸s,H.,Akkol,E.K.,Avcı,G.,Ye¸silada,E.,Bacak,E.:
Beneficial effects of Aesculus hippocastanum L. seed extract on the body’s own
antioxidant defense system on subacute administration. Journal of ethnophar-
macology 129(1), 18–22 (2010)
[29] Ince, S., Kozan, E., Kucukkurt, I., Bacak, E.: The effect of levamisole and lev-
amisole+vitaminconoxidativedamageinratsnaturallyinfectedwithsyphacia
muris. Experimental parasitology 124(4), 448–452 (2010)
[30] Avci, G., et al.: Effects of thymoquinone on plasma leptin, insulin, thyroid hor-
mones and lipid profile in rats fed a fatty diet. Kafkas U¨niversitesi Veteriner
Faku¨ltesi Dergisi 19(6) (2013)
[31] Xie, C., Wang, R., Zhang, J., Chen, P., Dong, W., Li, R., Chen, T., Chen,
H.: Multi-level learning features for automatic classification of field crop pests.
Computers and Electronics in Agriculture 152, 233–241 (2018)
[32] Wu, X., Zhan, C., Lai, Y.-K., Cheng, M.-M., Yang, J.: Ip102: A large-scale
benchmarkdatasetforinsectpestrecognition.In:ProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.8787–8796(2019)
[33] Zhao, B., Feng, J., Wu, X., Yan, S.: A survey on deep learning-based fine-
grained object classification and semantic segmentation. International Journal
of Automation and Computing 14(2), 119–135 (2017)
[34] Wen, C., Guyer, D.E., Li, W.: Local feature-based identification and classifica-
tion for orchard insects. Biosystems engineering 104(3), 299–307 (2009)
[35] Xie, C., Zhang, J., Li, R., Li, J., Hong, P., Xia, J., Chen, P.: Automatic
classification for field crop insects via multiple-task sparse representation and
multiple-kernellearning.ComputersandElectronicsinAgriculture119,123–132
(2015)
[36] Ayan,E.,Erbay,H.,Var¸cın,F.:Croppestclassificationwithageneticalgorithm-
basedweightedensembleofdeepconvolutionalneuralnetworks.Computersand
Electronics in Agriculture 179, 105809 (2020)
[37] Yang,G.,Chen,G.,Li,C.,Fu,J.,Guo,Y.,Liang,H.:Convolutionalrebalancing
network for the classification of large imbalanced rice pest and disease datasets
in the field. Frontiers in Plant Science, 1150 (2021)
22[38] Nanni, L., Manf`e, A., Maguolo, G., Lumini, A., Brahnam, S.: High perform-
ing ensemble of convolutional neural networks for insect pest image detection.
Ecological Informatics 67, 101515 (2022)
[39] Yu, J., Shen, Y., Liu, N., Pan, Q.: Frequency-Enhanced Channel-Spatial Atten-
tion Module for Grain Pests Classification. Agriculture 12(12), 2046 (2022)
[40] Ung,H.T.,Ung,H.Q.,Nguyen,B.T.:Anefficientinsectpestclassificationusing
multiple convolutional neural network based models. International Conference
on Intelligent Software Methodologies, Tools, and Techniques (2022)
[41] Feng,F.,Dong,H.,Zhang,Y.,Zhang,Y.,Li,B.:MS-ALN:MultiscaleAttention
Learning Network for Pest Recognition. IEEE Access 10, 40888–40898 (2022)
[42] Xia, W., Han, D., Li, D., Wu, Z., Han, B., Wang, J.: An ensemble learning
integration of multiple cnn with improved vision transformer models for pest
classification. Annals of Applied Biology 182(2), 144–158 (2023)
[43] An, J., Du, Y., Hong, P., Zhang, L., Weng, X.: Insect recognition based on
complementary features from multiple views. Scientific Reports 13(1), 2966
(2023)
[44] Ali, F., Qayyum, H., Iqbal, M.J.: Faster-PestNet: A Lightweight deep learning
framework for crop pest detection and classification. IEEE Access (2023)
[45] Guo, Q., Wang, C., Xiao, D., Huang, Q.: A lightweight open-world pest image
classifier using ResNet8-based matching network and NT-Xent loss function.
Expert Systems with Applications 237, 121395 (2024)
[46] Nguyen, H.-Q., Truong, T.-D., Nguyen, X.B., Dowling, A., Li, X., Luu, K.:
Insect-foundation: A foundation model and large-scale 1m dataset for visual
insect understanding. In: Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pp. 21945–21955 (2024)
[47] Kang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A., Feng, J., Kalantidis,
Y.: Decoupling Representation and Classifier for Long-Tailed Recognition. In:
International Conference on Learning Representations (2019)
[48] Zhao, S., Sun, X., Gai, L.: Data enhancement and multi-feature learning model
for pest classification. Journal of Intelligent & Fuzzy Systems (Preprint), 1–13
(2023)
[49] Hermans, A., Beyer, L., Leibe, B.: In defense of the triplet loss for person re-
identification. Computing and Research Repository (2017)
[50] Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond Empirical
Risk Minimization. In: International Conference on Learning Representations
23(2018)
[51] Li, W., Zheng, T., Yang, Z., Li, M., Sun, C., Yang, X.: Classification and detec-
tionofinsectsfromfieldimagesusingdeeplearningforsmartpestmanagement:
A systematic review. Ecological Informatics 66, 101460 (2021)
[52] Lowe, D.G.: Distinctive image features from scale-invariant keypoints. Interna-
tional journal of computer vision 60(2), 91–110 (2004)
[53] Kasinathan, T., Singaraju, D., Uyyala, S.R.: Insect classification and detection
infieldcropsusingmodernmachinelearningtechniques.InformationProcessing
in Agriculture 8(3), 446–457 (2021)
[54] Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T., Wang,
X., Wang, G., Cai, J., et al.: Recent advances in convolutional neural networks.
Pattern recognition 77, 354–377 (2018)
[55] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-
scale image recognition. International Conference on Learning Representations,
(2015)
[56] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep
convolutional neural networks. Communications of the ACM 60(6), 84–90
(2017)
[57] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks
for mobile vision applications. arXiv preprint arXiv:1704.04861 (2017)
[58] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recogni-
tion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 770–778 (2016)
[59] Xie, S., Girshick, R., Doll´ar, P., Tu, Z., He, K.: Aggregated residual transfor-
mations for deep neural networks. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1492–1500 (2017)
[60] Zagoruyko, S., Komodakis, N.: Wide Residual Networks. In: British Machine
Vision Conference 2016 (2016). British Machine Vision Association
[61] Li,C.,Zhen,T.,Li,Z.:Imageclassificationofpestswithresidualneuralnetwork
based on transfer learning. Applied Sciences 12(9), 4356 (2022)
[62] Wang,W.,Yang,Y.,Wang,X.,Wang,W.,Li,J.:Developmentofconvolutional
neural network and its application in image classification: a survey. Optical
Engineering 58(4), 040901–040901 (2019)
24[63] Dhillon, A., Verma, G.K.: Convolutional neural network: a review of mod-
els, methodologies and applications to object detection. Progress in Artificial
Intelligence 9(2), 85–112 (2020)
[64] Minaee,S.,Boykov,Y.Y.,Porikli,F.,Plaza,A.J.,Kehtarnavaz,N.,Terzopoulos,
D.: Image segmentation using deep learning: A survey. IEEE transactions on
pattern analysis and machine intelligence (2021)
[65] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances
in neural information processing systems 33, 6840–6851 (2020)
[66] Zhang, F., Li, M., Zhai, G., Liu, Y.: Multi-branch and multi-scale attention
learning for fine-grained visual categorization. In: MultiMedia Modeling: 27th
International Conference, MMM 2021, Prague, Czech Republic, June 22–24,
2021, Proceedings, Part I 27, pp. 136–147 (2021). Springer
[67] Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X., Tang, X.:
Residualattentionnetworkforimageclassification.In:ProceedingsoftheIEEE
ConferenceonComputerVisionandPatternRecognition,pp.3156–3164(2017)
[68] Lin, T.-Y., Doll´ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature
pyramid networks for object detection. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 2117–2125 (2017)
[69] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In:
International Conference for Learning Representations (2015)
[70] Jaiswal, A., Babu, A.R., Zadeh, M.Z., Banerjee, D., Makedon, F.: A survey on
contrastive self-supervised learning. Technologies 9(1), 2 (2020)
[71] Zhou, Y., Xie, C., Sun, S., Zhang, X., Wang, Y.: A Self-Supervised Human
Activity Recognition Approach via Body Sensor Networks in Smart City. IEEE
Sensors Journal (2023)
[72] Le-Khac, P.H., Healy, G., Smeaton, A.F.: Contrastive representation learning:
A framework and review. IEEE Access 8, 193907–193934 (2020)
[73] Nguyen,T.,Vu,M.N.,Vuong,A.,Nguyen,D.,Vo,T.,Le,N.,Nguyen,A.:Open-
vocabulary affordance detection in 3d point clouds. IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS) (2023)
[74] Xie, Y., Xu, Z., Zhang, J., Wang, Z., Ji, S.: Self-supervised learning of graph
neural networks: A unified review. IEEE transactions on pattern analysis and
machine intelligence (2022)
[75] Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive
predictive coding. arXiv preprint arXiv:1807.03748 (2018)
25[76] Gutmann, M., Hyv¨arinen, A.: Noise-contrastive estimation: A new estimation
principle for unnormalized statistical models. In: Proceedings of the Thirteenth
International Conference on Artificial Intelligence and Statistics, pp. 297–304
(2010). JMLR Workshop and Conference Proceedings
[77] He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsu-
pervised visual representation learning. In: Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.9729–9738(2020)
[78] Hu, Q., Wang, X., Hu, W., Qi, G.-J.: Adco: Adversarial contrast for efficient
learning of unsupervised representations from self-trained negative adversaries.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 1074–1083 (2021)
[79] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., Joulin, A.: Unsuper-
visedlearningofvisualfeaturesbycontrastingclusterassignments.Advancesin
Neural Information Processing Systems 33, 9912–9924 (2020)
[80] Li, J., Zhou, P., Xiong, C., Hoi, S.: Prototypical Contrastive Learning of
UnsupervisedRepresentations.In:InternationalConferenceonLearningRepre-
sentations (2021)
[81] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for
contrastive learning of visual representations. In: International Conference on
Machine Learning, pp. 1597–1607 (2020). PMLR
[82] Zbontar, J., Jing, L., Misra, I., LeCun, Y., Deny, S.: Barlow twins: Self-
supervised learning via redundancy reduction. In: International Conference on
Machine Learning, pp. 12310–12320 (2021). PMLR
[83] Jing, L., Vincent, P., LeCun, Y., Tian, Y.: Understanding Dimensional Col-
lapse in Contrastive Self-supervised Learning. In: International Conference on
Learning Representations (2022)
[84] Shorten,C.,Khoshgoftaar,T.M.:Asurveyonimagedataaugmentationfordeep
learning. Journal of big data 6(1), 1–48 (2019)
[85] Xu, M., Yoon, S., Fuentes, A., Park, D.S.: A comprehensive survey of image
augmentationtechniquesfordeeplearning.PatternRecognition,109347(2023)
[86] Vapnik, V.: Principles of risk minimization for learning theory. Advances in
neural information processing systems 4 (1991)
[87] DeVries, T., Taylor, G.W.: Improved regularization of convolutional neural
networks with cutout. Computing Research Repository (2017)
[88] Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization
26strategy to train strong classifiers with localizable features. In: Proceedings of
the IEEE/CVF International Conference on Computer Vision, pp. 6023–6032
(2019)
[89] Verma, V., Lamb, A., Beckham, C., Najafi, A., Mitliagkas, I., Lopez-Paz, D.,
Bengio, Y.: Manifold mixup: Better representations by interpolating hidden
states.In:InternationalConferenceonMachineLearning,pp.6438–6447(2019).
PMLR
[90] Kim, S., Lee, G., Bae, S., Yun, S.-Y.: Mixco: Mix-up contrastive learning
for visual representation. In: NeurIPS Workshop on Self-Supervised Learning:
Theory and Practice0 (2020)
[91] Good, I.J.: Rational decisions. Journal of the Royal Statistical Society: Series B
(Methodological) 14(1), 107–114 (1952)
[92] Murphy, K.P.: Machine Learning: a Probabilistic Perspective. MIT press,
Cambridge, Mass. [u.a.] (2012)
[93] Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In: International Conference on Machine
Learning, pp. 448–456 (2015). pmlr
[94] Fukushima, K.: Cognitron: A self-organizing multilayered neural network.
Biological cybernetics 20(3-4), 121–136 (1975)
[95] Samanta, R., Ghosh, I.: Tea insect pests classification based on artificial neural
networks.InternationalJournalofComputerEngineeringScience(IJCES)2(6),
1–13 (2012)
[96] Venugoban, K., Ramanan, A.: Image classification of paddy field insect pests
using gradient-based features. International Journal of Machine Learning and
Computing 4(1), 1 (2014)
[97] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L.: Imagenet: A large-
scalehierarchicalimagedatabase.In:2009IEEEConferenceonComputerVision
and Pattern Recognition, pp. 248–255 (2009). Ieee
[98] Ruder, S.: An overview of gradient descent optimization algorithms. arXiv
preprint arXiv:1609.04747 (2016)
[99] Ren, F., Liu, W., Wu, G.: Feature reuse residual networks for insect pest
recognition. IEEE access 7, 122758–122768 (2019)
[100] Thenmozhi, K., Reddy, U.S.: Crop pest classification based on deep convo-
lutional neural network and transfer learning. Computers and Electronics in
Agriculture 164, 104906 (2019)
27[101] Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.:
Grad-cam: Visual explanations from deep networks via gradient-based localiza-
tion.In:ProceedingsoftheIEEEInternationalConferenceonComputerVision,
pp. 618–626 (2017)
[102] Maaten, L., Hinton, G.: Visualizing data using t-SNE. Journal of machine
learning research 9(11) (2008)
[103] Sun, Y., Cheng, C., Zhang, Y., Zhang, C., Zheng, L., Wang, Z., Wei, Y.: Circle
loss: A unified perspective of pair similarity optimization. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
6398–6407 (2020)
[104] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.:
An image is worth 16x16 words: Transformers for image recognition at scale.
International Conference on Learning Representations (2021)
28