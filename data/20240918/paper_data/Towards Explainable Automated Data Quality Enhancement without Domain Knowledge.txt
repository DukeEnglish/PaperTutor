TOWARDS EXPLAINABLE AUTOMATED DATA QUALITY
ENHANCEMENT WITHOUT DOMAIN KNOWLEDGE
DjibrilSarr
LAGA,UMR7539CNRS
UniversitéSorbonneParisNord
99Av. JeanBaptisteClément,93430Villetaneuse
FBHAssociés
11Ruedu4septembre,75002Paris
sarr@math.univ-paris13.fr
djibril.sarr@fbh-associes.com
September17,2024
ABSTRACT
In the era of big data, ensuring the quality of datasets has become increasingly crucial across
various domains. We propose a comprehensive framework designed to automatically assess and
rectifydataqualityissuesinanygivendataset,regardlessofitsspecificcontent,focusingonboth
textualandnumericaldata. Ourprimaryobjectiveistoaddressthreefundamentaltypesofdefects:
absence,redundancy,andincoherence. Attheheartofourapproachliesarigorousdemandforboth
explainabilityandinterpretability,ensuringthattherationalebehindtheidentificationandcorrection
ofdataanomaliesistransparentandunderstandable. Toachievethis,weadoptahybridapproach
thatintegratesstatisticalmethodswithmachinelearningalgorithms. Indeed,byleveragingstatistical
techniquesalongsidemachinelearning, westrikeabalancebetweenaccuracyandexplainability,
enabling users to trust and comprehend the assessment process. Acknowledging the challenges
associatedwithautomatingthedataqualityassessmentprocess,particularlyintermsoftimeefficiency
andaccuracy,weadoptapragmaticstrategy,employingresource-intensivealgorithmsonlywhen
necessary,whilefavoringsimpler,moreefficientsolutionswheneverpossible. Throughapractical
analysis conducted on a publicly provided dataset, we illustrate the challenges that arise when
tryingtoenhancedataqualitywhilekeepingexplainability. Wedemonstratetheeffectivenessofour
approachindetectingandrectifyingmissingvalues,duplicatesandtypographicalerrorsaswellas
thechallengesremainingtobeaddressedtoachievesimilaraccuracyonstatisticaloutliersandlogic
errorsundertheconstraintssetinourwork.
Keywords Dataquality,explainability,interpretability,statisticaloutliers,typographicalerrors,logicalerrors
1 Introduction
Dataassumesanescalatingsignificanceincontemporarybusinessoperations. Acrossdiversesectorssuchascommerce,
entertainment,medicine,andtheoilindustry,dataemergesasapotentcatalystforaugmentingbusinessefficacy(Marr,
2016). Anabundantreservoirofqualitativedata,coupledwithArtificialIntelligence(AI),statisticalmethodologies,
andprobabilisticapproaches,facilitatesthediscernmentofintricatebehavioralpatternsorassociationsthatwouldbe
arduousforhumanstodiscern. However,theefficacyofthesemethodologiesdependsontheavailabilityofhigh-quality
data,oftennecessitatingvoluminousdatasets. Notably,thepreparatoryphase,whichentailsrenderingdatacompatible
withanalyticalalgorithms,haslongbeenestimatedtoconsumeanimportantpartoftheprocessofexploitingthedata.
Twodecadesprior,Pyle(1999)alreadyestimatedtheproportionoftimespenttobeover50%oftheoverallprocessing
time. Presently,notwithstandingtheadvancementsinanalyticaltechniques,theincreasingvolumeandintricacyof
data sources have exacerbated the challenges associated with data preparation (García et al., 2016). This critical
4202
peS
61
]BD.sc[
1v93101.9042:viXraTowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
aspect,knownasdatapreprocessing,encompassesasuiteoftechniquesexecutedpriortotheextractionofactionable
insightsfromthedata,constitutingapivotalfacetofdataexploitation. Duetotheinherentanomaliesindata,initiating
dataminingendeavorsdirectlyisimpracticalwithoutaddressinginconsistenciesandredundancies. Furthermore,the
exponentialsurgeindatagenerationratesacrossmultiplesectorsnecessitatestheadoptionofincreasinglysophisticated
analyticalmechanisms(Hanetal.,2022;ZakiandMeira,2014). Inlightofconstraintspertainingtotimeandhuman
resources,thereexistsapressingimperativetoautomateandstreamlinethispreparatoryphase. Centraltothisprocess
istheidentificationandrectificationoferroneousdatavalues. Theyconstituteacrucialcomponentofthedatacleansing
process(referto,forinstance,Section3.2ofHanetal.(2012)).
Forthereasonsoutlinedabove,itisimperativeforpractitionerstopossessefficienttoolsformeasuringandenhancing
dataquality. Thisstudyfocusesonoptimalstrategiesforprovidingautomatedtoolsforthisphase. Wealsoassertthat
withoutexplainabilityandinterpretability,theeffectivenessofdataqualityassessmentalgorithmsiscompromised,
asusersmaylackconfidenceinthereliabilityoftheresults. Forinstance, ifanentryisdeemedinvalidduetothe
detection of a statistical outlier on a specific line of a data table, but the problematic field(s) cannot be precisely
identifiedbythealgorithm,itbecomeschallengingtofullyrelyonthealgorithmformakinginformeddecisions. We
proposeaframeworkcomprisingfivemajorstepsthatcanautomaticallyidentifydefectsinadatasetwithoutprior
knowledge of its contents. This is accomplished while ensuring the ability to explain and justify each conclusion
reachedbyourframework. Thisfeatureenablestheframework,whichconsistsofaseriesofalgorithms,toprovide
nativecorrectionsforallerrorsidentifiedinagivendataset. Returningtotheearlierexample,theidealframework
should not only identify the presence of a statistical outlier but also pinpoint the problematic field and propose a
correction. Ourframeworkisapplicabletoanytablecontainingnumericalvalues,text,orboth. Itaddressesthreetypes
ofdefectsthatcanaffectdata: absence,redundancy,andinconsistency. Thefirsttwostepsofthefive-stepsalgorithm
weintroducedealwiththefirsttwotypesoferrorsandthelastthreestepsallhandleinconsistenciesasweconsider
statisticaloutliers,typographicalerrorsandlogicerrors. Whilethefirsttwodefectsmaybeself-explanatory,indicating
respectivelymissingandduplicatedvalues,thethirdislessstraightforward. Inourframework,inconsistenciescan
impactanytypeofobservation,whethernumericalortextual,occurringwhenaninputinagivenfieldappearsabnormal
relativetoothersinthesamefield. WhileArtificialIntelligence(AI)-basedtechniquesmayofferpotentiallysuperior
performance,theyoftenlackthetransparencynecessaryforthoroughunderstandingandvalidationofresults. Therefore,
thealgorithmswithinourcomprehensiveframeworkattempttocombineAI-basedtechniquesandstatisticalresultsto
identifyandcorrectdefectsingivendatasets. Thischaracteristic,specificallyitsoutcomeofbeingabletoexplainall
results,isthemaindistinctionbetweenourworkandexistingliterature.
Theliteraturecontainsseveralalgorithmsforanalyzingthevalidityofadataset. Asinthiswork,manyofthemleverage
Machine-Learning(ML),Deep-Learning(DL)andstatisticalmethods. Consideringspecificallythehandlingofmissing
values,theydonotconstituteperseatoughchallengeastheycanbeidentifiedinprettystraightforwardways. However,
intheprocessofhandlingthem,twochallengesarise. Thefirstoneisanalyzingtherightfulnessofthemissingdata.
Indeed, a missing value does not necessarily designate a defect in the dataset as it can, in various situations, be a
legitimateandinformation-providingentry. AsdevelopedinSainani(2015),themoststraightforwardandcommonly
employedapproachtomanagingmissingdatainvolvesexcludingincompleteobservationsentirelyfromtheanalysis.
That solution, while very easily applicable, is drastic. As already stated, it might delete the valuable information
providedbythemissingvalue,butitobviouslyalsodiscriminatesalltheothervalidentries. Theauthoralsopresents
theverycommonsolution,whichistoimputethemissingvaluewitharepresentativevalueofthefield. Itcanbethe
averagevalue(forinstance,ifitisheightsofpeople),itcanbealocalaverage,forinstancetheweightofthemalesina
specificstudy,andsoonandsoforth. OtherresearchershaveinsteadusedMLandDLtohandlemissingvalues. For
instance,SavarimuthuandKaresiddaiah(2021)imputationmethodontimeseriesusesaniterativeimputationalgorithm
thatclustersunivariatetimeseriesdata,takingintoaccountthedata’strend,seasonality,andcyclicalpatterns. Then
withinthecluster,theauthorsemployasimilarity-basednearestneighborimputationmethodwithineachclusterto
fillinmissingvalues. Morerecently,Kachueeetal.(2020)usedageneratornetworktogenerateimputationsthata
discriminatornetworkistaskedtodistinguish;thismethodallowsalsotoproperlyestimatethedistributionofthetargets.
WhilemethodsusingAIareshownbytheirauthorstobeveryefficient,theyhardlyprovidethelevelofexplainability
thatweaimfor. Forthesereasons,insection4.2.2,wewillonlyuseaclassicimputationmethodforhandlingmissing
numericalvalues,ensuringexplainabilityandinterpretability. However,forcharacterentries,whenaddressinglogic
errorsinSubsection4.2.5,wewillemployMLalgorithmsforimputationwhilemaintainingtransparency. Ourstrategy
willguaranteethatlegitimatemissingvalueswillnotbediscriminated,thankstothesubsequentapplicationofthelogic
errordetectionalgorithm. Ourapproachalsoguaranteesthateverychoicewillbeexplainableandinterpretable.
Researchhasalsoexploredthecombinationofneuralnetworksandstatisticsspecificallyforoutlierdetection. For
example,Smalletal. Daietal.(2018)trainedaneuralnetworktoreplicateaspecificfieldwithintheirdatasettoidentify
outliers. Theycomparedtheneuralnetwork’soutputwiththeactualvaluesandemployedstatisticalindicatorssimilar
tothosediscussedinSubection4.2.3. Theauthorscalculatedthedifferencebetweentheoutputandactualvaluesofthe
2TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
fieldunderconsideration,constructingatoleranceintervalbasedonthedistributionoferrorsbetweenthetwovalues. If
theerrorfelloutsidethetoleranceinterval,theactualvaluewasdeemedtohavebeenincorrectlyattributed. Therefore,
theirapproachnecessitatesbuildingandtraininganeuralnetworkforidentifyingstatisticaloutliers,whichcanbecostly,
particularlyforhigh-dimensionaldatasets. Thereliabilityofthismethodreliesontheneuralnetwork’sperformance,as
theentireapproachdependsonitsabilitytoaccuratelyreproduceactualvalues. Anothercommonlyexploredapproach
intheliteratureistheuseofDBSCAN,Density-BasedSpatialClusteringofApplicationswithNoise(Çeliketal.,2011;
Pandyaetal.,2020). However,thechallengewiththismethodliesinselectingtheparametersϵ,whichdefinesthe
maximumdistancebetweenpointsinthesameneighborhood,andminPts,theminimumnumberofpointsneeded
toformadenseregion. Thisparameterselectionissuehasbeenextensivelydiscussedintheliterature(Karamiand
Johansson,2014;ThangandKim,2011). Moreover,consideringourgoalofachievingmaximuminterpretabilityand
explainability,itisessentialtonotethatDBSCANisdesignedforclusteringanddensity-basedoutlierdetectionin
multi-dimensionaldatasets,makingitchallengingtomaintainexplainabilitywhileidentifyingoutliers,particularlyas
definedinSection2.
Regardingtypographicalerrors,theyhavelongbeenafocusofresearchers,particularlyintextanalysis. Earlyscholars,
suchas thosereferenced inMorrisand Cherry(1975), were amongthefirst toaddress thisissue. Asdiscussed in
Subection4.2.4,theirapproach,similartoours,drewinsightsdirectlyfromthedataset,enablingthealgorithmtoadapt
dynamicallytothetextunderscrutiny. Forexample,theyutilizedstatisticalanalysisofwordtrigramswithintheir
documents. Incontrast,ourmethod,asexplainedlater,utilizeswordfrequenciesextractedfromthetableweaimto
rectify. Ourtypographicalerrordetectionandcorrectionalgorithmincorporatesmachinelearningtechniques. This
integrationofAIandtextanalysisiswell-documented; previousresearchershaveemployedmachinelearningand
deeplearningmethodstorectifytypographicalerrors,oftenleveragingcontextualcuesordomain-specificknowledge
(e.g.,Huangetal.(2013)). Inourstudy,weoptforunsupervisedmachinelearningapproaches,particularlyclustering
algorithms,toavoiddependenceondomain-specificknowledgeandtoexpeditetheidentificationoftypographical
errors. Furthermore,muchoftheexistingliteratureinthisfieldreliesonexternaldatasets. Forinstance,onestepinthe
algorithmproposedbyBassilandAlwani(2012)involvescross-referencingeachwordwithGoogleWeb1T5-Gram
extensivewords’unigramsdataset. Thealgorithmweintroducedoesn’trequireexternalinformation. Itcanhoweverbe
usedwhendesiredtoincreasethespeedoftheprocess. Theapplicationwewillprovidewillsolelyuseinformationthat
isprovidedwithinthedatasets.
Analyzingandcorrectinglogicerrorsisnotasfrequentlyaddressedintheliteratureastheothertypesoferrorswehave
previouslydiscussed,particularlywiththedefinitionprovidedinSubsection4.2.3. However,aswewillseeindetails,
identifyinglogicerrorscanbelikenedtorecognizinganomalousbehavioracrossanentireobservation(takinginto
accountallfieldsofthedatatable)incomparisontoothers. Thiscanincludetaskssuchasdetectingcreditcardfraudor
identifyingcancercells. Oneofthemodelfamiliescommonlyemployedforthispurposeisrule-basedalgorithms,as
demonstratedbyKarthikeyanandVembandasamy(2015a)indiagnosingTypeIIdiabetes. Similarly,thisisthemodel
familywewillemployforlogicerrordetection. Nonetheless,numerousalternativesolutionsalsoexist. Forinstance,in
Xuetal.(2018),variationalauto-encodersareutilizedtodetectseasonalanomaliesinkeymetrics(suchasthenumber
ofviews,onlineusers,andorders)ofwebapplications. Alternatively,SupportVectorMachines(SVMs)havebeen
explored,asdemonstratedinRezapour(2019)forcreditfrauddetection. Acommoncharacteristicamongallthese
approaches,includingours,istheirrelianceonutilizingallavailablefieldsforaspecificobservation. Thisrequirement
wasnotnecessarilypresentinthehandlingofotherdefectsdiscussedinthisdocument.
Afterconductinganextensivereviewofthecurrentstateoftheart,itappearsthatthispaperaimstoaddressanotable
gapinthedatacleansingliterature: theneedforanexplainable,interpretable,andautomaticsolution. Whilenumerous
efficientsolutionsareavailable,theydonothavethedualconstraintsemphasizedinourwork.Byapplyingourapproach
toapubliclyavailableagriculturalequipmentsalesdataset,weillustrateitseffectiveness,indetectingandrectifying
missingvalues,duplicatesandtypographicalerrorswhilebeingabletokeepamaximalexplainabilityandwithout
usingdomainknowledge. Similarly,wediscussthechallengesremainingtobeaddressedtoachievesimilaraccuracy
onstatisticaloutliersandlogicerrorsunderthesameconstraints.
In Section 2 of this document, we present fundamental definitions that underpin the selection of algorithms and
strategies. Thesedefinitionsprimarilyaddresstheconceptsofdatavalidityandtheexplainabilityofanalgorithm,and
consequently,itsresults. Followingthis,inSection3,weintroducethedatasetonwhichwewillapplytheframework.
Thiswillallowustoprovideconcreteillustrationswhenpresentingthealgorithms. Then,Section4willbethecoreof
thisworkasweintroducetheframework. Initially,weofferabroadoverviewoftheprocess,followedbyadetailed
exposition of each of its five steps. Subsequently, in Section 5, we present the results obtained after applying the
frameworktothedataset.
3TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
2 Definitions
Webeginbypresentingkeydefinitionsthatwillguidetheselectionofalgorithmsandshapethefocusoftheframework
thatwewillintroduce. Specifically,wewilldefinewhatwewillconsiderinthisworktobevaliddataandwewill
provideacontextualiseddefinitionofexplainabilityandinterpretability. Itisworthemphasizingthatthesedefinitions
mightnotachieveunanimousagreementintheliterature,astheydonotalwayscaptureclearanduniversallyobjective
concepts.
Inwhatfollows,adatasetDisdefinedasafinitecollectionofnrecords{r ,r ,...,r },whereeachrecordr isa
1 2 n i
tupleconsistingofmattributes. Formally,wecanrepresentthedatasetas:
D ={r ,r ,...,r },
1 2 n
wherer =(a ,a ,...,a )fori=1,2,...,n. Eachattributea isanelementofapredefinedattributedomainA .
i i1 i2 im ij j
Thisformalizationallowsustoconsideradatasetasastructuredcollectionofmulti-dimensionaldatapoints,wherethe
dimensionalityisdeterminedbythenumberofattributesm. Notethat,anyelementa canbeanumericalvalueas
ij
wellasatextvalue.
2.1 Validdata
In the literature, data quality is frequently characterized by the presence or absence of defects. According to this
perspective,dataisconsideredtobeofhighqualityifitisdevoidofsuchdefectsoriftheirimpactisminimal. This
conceptualizationcanbeseenforinstanceintheworkofSchelteretal.(2018)andCorralesetal.(2018). Wewillalso
adheretothisapproachinthispaper. Itisalsoimportanttonotethatthroughoutthisdocument,thedataisnotdefined
explicitlywhenunambiguous. Itencompassesbothindividualobservationswhichcorrespondstoaspecificcolumnofa
rowwithinadataarrayandobjectsofdimensionsgreaterthanonedenotedasp>1. Theyrefertoaroworsubsets
thereofwithinadataarray. Also,obviouslytheobservationscanbetextaswellasnumericalvalues. Letusnowdefine
thedefaultsthatwillformthebasisoftheframework.
1. Absence:
Thisreferstotheabsenceofdataoritsconstituentelements,whichcanariseduringthedatacollectionprocess
duetotechnicalfailure,error,orinsufficientinformation(AydilekandArslan,2013). Insuchinstances,the
dataisdeemedinvalid.
2. Redundancy:
Data(wherep ≥ 1)thatisreplicatedacrossmultipleentriesisregardedasinvalidandisoftendenotedas
duplicatesintheliterature.
3. Inconsistency:
Wedefineinconsistencyasthelackofagreementbetweenadataitemandotherobserveddata. Consequently,
itencompassesseveraltypesofdefects:
(a) Statisticaloutliers:Thesearedatapointsthataresignificantlyandunjustifiablydistantfromotherobser-
vations. Anabnormalvaluemayresultfrominconsistencyoraberrantbehaviorduringthemeasurement
process(BarnettandLewis,1994;JohnsonandWichern,2014).
(b) Typographical errors: These errors encompass all text-related issues, including typing errors (e.g.,
"France"vs. "Frange")andformattingdiscrepancies(e.g.,"Citroën"vs. "CitroÃ?n").
(c) Logicalerrors:Theseerrors,morechallengingtodefine,denoteincompatibilitiesbetweendifferent
variables(columns)withinadataset.
Datawill,therefore,beconsideredvalidonlywhenitexhibitsnoneofthesedefects. Theforthcomingframeworkwill
becraftedtodetectandaddressthesevarieddefaults. Itwillstrivetorectifysuchanomalieswherefeasible,allthe
whileprioritizingthepreservationofexplainabilityandinterpretabilityofboththealgorithmsusedandtheirresults.
2.2 ExplainabilityandInterpretability
ThelackofexplainabilityandInterpretabilityandtheblackboxcharacteritentailsisoneofthemainargumentsagainst
amorewidespreadandsystematicuseofMLorDLtechniques(Escalanteetal.,2018). Therefore,oneofourfocal
pointistoupholdtheseprinciplesofexplainabilityandinterpretabilitytothefullestextentwecanrealizewithinthis
framework.
Tofacilitateourcomprehensionoftheconceptofinterpretability,letusbeginwithamoreliteraryperspective.According
totheLarousse,theverbinterpretreferstotheactof"seekingtomakeatext,anauthorintelligible,explainingthem,
4TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
commentingonthem.". Inthecontextofstatisticalalgorithmsandmachinelearning,asemphasizedbyDoshi-Velez
andKim(2018),thisintelligibilityisdirectedtowardshumans. Analgorithmisconsideredinterpretableiftheresultsit
generatescanbearticulatedintermsunderstandabletohumans. Meanwhile,theconceptofexplainabilityismore
commonlyencounteredandappearsinherentlymoreintuitive. However,theliteraturedoesnotalwaysconvergeona
formaldefinitionofthisconceptwithinanalgorithmiccontext. Beginningagainwithaliteraryperspective,Larousse
defines explainability as "making someone understand (a question, an enigma), clarifying them by providing the
necessaryelements."(Keil,2006)emphasizesthateverystageoftheclarificationprocesscontributestoexplainability.
Toprovideacomprehensiveview,(Rasetal.,2018)definesexplainability(particularlyinadeeplearningcontext)as
thecombinationofinterpretabilityandtransparency. Transparency,inthiscontext,referstothedegreetowhichan
explanationrendersaspecificresultacceptable. Itisimportanttonotethatacceptanceoftheresultdoesnotnecessarily
implythatthepractitionerperceivesitasrightorwrong,butrather,thatthepathleadingtothatspecificresultisclear.
Drawingfromboththeliteraryperspectiveandinsightsfromtheliterature,itbecomesevidentthattheconceptsof
interpretability and explainability play pivotal roles in making sense of algorithmic outputs, especially within the
contextofstatisticalalgorithmsandML.Wecannow,inourcontextprovidethefollowingdefinitions.
1. Aresultthatcanbeexplainedandinterpretedwillreferto:
(a) Aresultforwhichwecanunderstandthepath,regardlessofitscorrectness. Thatis,wemustbecapable
ofelucidatingtheinputs,outputs,rules,etc.,throughwhichadefect(asdefinedintheprevioussection)
hasbeenidentified.
(b) Aresultwhoseboundariescanberecognizedandcomprehendedisimperative. Thatis,onemustbe
abletounderstand,explain,anddeliberateonthescenarioswherearesult,whosepathhasalreadybeen
understood,ismeaningfulornot.
Inadditiontothesetwoelementsthatrespectivelyaddressexplainabilityandinterpretability,weintroduce
athirdequallysignificantconstraint.
(c) Aresultthatwecanclearlyidentifyandlocate. Thatis,withinadataqualitycontext,ifaparticularrow
isflaggedasproblematic,wemustbeabletoidentifywhichcolumnsaredeemedincorrect.
2. Analgorithmthatcanbeexplainedandinterpretedis:
(a) Analgorithmwhoseresultscanbeexplainedandinterpreted.
Forthesakeofconciseness,wewillrefertothedualityofexplainabilityandinterpretabilitysimplyasexplainability.
3 Datasetfortheapplicationoftheframework
Inthefollowingsections,wewillintroduceeachofthealgorithmscomprisingtheframeworkoutlinedinSection4. To
provideclarityonhowthesealgorithmsoperate,weproposestartingbyintroducingthedataset. Thisapproachwill
enableustoreferencethedatasetwhennecessarytoillustrateanalgorithm’sfunction.
ThedatasetusedinthisworkisbuildupontheBlueBookforBulldozersdatasetpubliclyavailable1. Thedatasetis
madeavailablebyFrenchagencyAMIES(AgencyforMathematicsinInteractionwithIndustryandSociety2)fortheir
2021competition3.
The database contains 100,000 observations described by 53 parameters briefly defined in the table 3. This table
categorizes various attributes of auction machinery, grouping them primarily under unique identifiers, descriptive
information,configurationdetails,andlocation-specificdata. KeyidentifierssuchasSalesID,MachineID,ModelID,
datasource, and auctioneerID are crucial for tracking and analyzing sales transactions. Descriptive attributes like
YearMade, MachineHoursCurrentMeter, andUsageBandprovideinsightsintothemachine’sage, usagelevel, and
operationalstatus.Thetablealsoextensivelydetailsmachineconfigurations,highlightingfeaturessuchasDrive_System,
Enclosure,Forks,andnumerousothersthatspecifythemachine’sphysicalandoperationalsetup,whichiscritical
for potential buyers to assess the machinery’s capabilities and condition. Furthermore, geographical and market
segmentation is addressed through variables like State and ProductGroupDesc, which help in understanding the
categorizationofthemachinery. Thisstructureddataarrangementaidsinthecomprehensiveanalysisandvaluationof
themachineryatthepointofsale. Table2belowsummarizesallthetypesoferrorsthatwewillbeidentifyingand
correctioninthispaper. Obviously,noneoftheinformationprovidedbyTables2or3willbeusedinthealgorithms.
1https://www.kaggle.com/c/bluebook-for-bulldozers/data
2AgencepourlesMathématiquesenInteractionavecl’EntrepriseetlaSociété
3The work presented in this article won the first prize of the data quality 2021 competition (https://challenge-
maths.sciencesconf.org/resource/page/id/1).
5TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
Variables Description
SalesID Uniqueidentifierofaparticularauctionmachinesale.
MachineID Identifierofaparticularmachine.
ModelID Modelidentifier.
datasource Uniquemachineidentifier.
auctioneerID Identifierofaparticularauctioneer.
YearMade Machinemanufacturingyear.
MachineHoursCurrentMeter Currentmachineusageinhoursattimeofsale.
UsageBand Value (low, medium, high) calculated by comparing the hours of this particular
salesmachinewiththeaverageusageofthebasemodel.
Saledate Saledate.
Saleprice CostinUSD.
fiModelDesc Descriptionofauniquemodelofmachine.
fiSecondaryDesc fiModelDescdisaggregation
fiModelSeries fiModelDescdisaggregation
fiModelDescriptor DisaggregationoffiModelDesc
ProductSize Sizeclassgroupingforaproductfamily.
ProductClassDesc Descriptionofthe2ndlevelhierarchicalgrouping.
State U.S.stateinwhichthesaletookplace.
ProductGroup Identifierforfirst-levelhierarchicalgrouping.
ProductGroupDesc Descriptionofhigh-levelhierarchicalgrouping.
Drive_System Machinesetup1.
Enclosure Machineconfiguration-whetherthemachinehasanenclosedcabornot?
Forks Machineconfiguration-liftingaccessory.
Pad_Type Machineconfiguration-typeoftreadonatrackedmachine.
Ride_Control Machineconfiguration-optionalfeatureonloaders.
Stick Machineconfiguration-controltype.
Transmission Machineconfiguration-describesthetransmissiontype.
Turbocharged Machineconfiguration-naturallyaspiratedorturbochargedengine.
Blade_Extension Machineconfiguration-standardbladeextension.
Blade_Width Machineconfiguration-bladewidth
Enclosure_Type Machineconfiguration-doesthemachinehaveanenclosedcabornot?
Engine_Horsepower Machineconfiguration-motorpower.
Hydraulics Machineconfiguration-typeofhydraulics.
Pushblock Machineconfiguration-option.
Ripper Machineconfiguration-toolattachedtothemachinetoworktheground.
Scarifier Machineconfiguration-toolattachedtothemachineforsoilconditioning
Tip_control Machineconfiguration-typeofbladecontrol.
Tire_Size Machineconfiguration-primarytiresize.
Coupler Machineconfiguration-typeofmachineinterface.
Coupler_System Machineconfiguration-machineinterfacetype.
Grouser_Tracks Machineconfiguration-describesgroundcontactinterface.
Hydraulics_Flow Machineconfiguration-describesthegroundcontactinterface.
Track_Type Machineconfiguration-typeoftreadonatrackedmachine.
Undercarriage_Pad_Width Machineconfiguration-widthoftracktreads.
Stick_Length Machineconfiguration-lengthofmachinediggingtool.
Thumb Machineconfiguration-accessoryusedforinput.
Pattern_Changer Machineconfiguration-operatorcontrolconfigurationcanbeadaptedtotheuser.
Grouser_Type Machineconfiguration-typeoftreadonatrackedmachine.
Backhoe_Mounting Machineconfiguration-optionalinterfaceusedtoaddanexcavatoraccessory.
Blade_Type Machineconfiguration-describesthebladetype.
Travel_Controls Machineconfiguration-describestheoperatorcontrolconfiguration.
Differential_Type Machineconfiguration-differentialtype.
Steering_Controls Machineconfiguration-describestheoperatorcontrolconfiguration.
Table1: Fieldsinthedataset
6TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
Thedatasetisalteredwithdifferenttypesoferrors.
Correspondence Typesoferror Numberofcases Subtotal
Redundancy Duplicates 20 20
Absence MissingValue 161 161
Inconsistency(Outliers) AberrantValue 200 200
EntryError 100
Inconsistency(Typography) Uppercase 50 200
Lowercase 50
WrongCategory 25
IncoherentMachine 100
Inconsistency(Logic) IncoherentDriveSystem 200 450
IncoherentProductGroupDescription 100
YearMade>saledate 25
Total 1031
Table2: Summaryoferrortypesandcorrespondence
Nowthatthedatasetisproperlyintroduced,wecanproceedwithpresentingthealgorithmswithinthedataquality
measurementandenhancementframework.
4 Automaticdataqualityenhancementframework
The framework illustrated in Figure 1 consists of two phases: one preceding the commencement of actual quality
enhancement(Pre-QualityEnhancement,PQEphase),andtheotherconstitutingtheactualdataqualityenhancement
process(QualityEnhancementPhase,QEphase). Initially,thePQEPinvolvesidentifyingaprimarykeyinthedataset
(PQE1)anddeterminingthecolumnsthatwillreceiveeachoftherequiredtreatments(PQE2). Thesetwostepsare
fundamentalinenablinganautomatedprocessthatdoesnotrelyondomainknowledge. Followingthispreparation,the
QEPtargetsthethreekeyareasthatwerementionedearlier:
• Redundancies handling encompasses the identification and elimination of duplicate entries (QE1). As
depictedintheflowchart,thisstepnecessitatestheutilizationoftheprimarykeyidentifiedinthePQEphase.
• AbsencesHandlingconcentratesontheimputationofabnormalmissingvaluestopreservedataintegrity
(QE12). This step is divided into two actions: firstly, the identification of abnormal missing values, and
secondly,theirimputation. Indeed,asexplainedinSubsection4.2.2,theprocessofidentifyingmissingvalues
andtheirimputationisnotsimultaneous. Similartotheprecedingstep,handlingabsenceswillalsoleverage
thekeyidentifiedduringstepA1ofthePQEphase.
• Inconsistencies Handling is divided into three sub-steps: the identification and imputation of statistical
outliers (QE31), the detection and correction of typographical errors (QE32), and the identification and
correctionoflogicerrors(QE33). Noneofthesesub-stepsnecessitatessplittingtheactions,asforthesetasks,
theprocessesofidentifyingtheproblemandresolvingitoccursimultaneously. Thesestepsdonotrequirethe
identificationofaprimarykey. TheyonlybenefitfromthePQEphasebytargetingtherightcolumnstodeal
with.
Thesestepscollectivelyenhancedataqualitybysystematicallyremovingerrorsandinconsistencies,therebypreparing
thedatasetforaccurateandreliableanalysis.
ItisnoteworthythatinFigure1,thereisanarrowlinkinglogicerrorstotheimputationofmissingvalues. Thisis
because,aswewillseeinSections4.2.2and4.2.5,somespecifictypesofmissingvalueswillbehandledaslogicerrors.
7TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
Figure1: Invaliddatadetectionframework
4.1 Pre-qualityenhancementphase
4.1.1 Identificationoftheprimarykeyinthedataset
GivenadatasetDconsistingofnrecords{r ,r ,...,r },whereeachrecordr isatuple(a ,a ,...,a )ofm
1 2 n i i1 i2 im
attributes,aprimarykeyisasubsetofthesetofattributes(columns){A } thatuniquelyidentifieseachrecord
j j∈ 1,m
inthedataset. LetP ={p ,p ,...,p }⊆{A ,A ,...,A }bethesetofk(cid:74) fie(cid:75)ldscorrespondingtotheprimarykey
1 2 k 1 2 m
attributesandP ⊆{1,2,...,m}therespectiveindicesofeachelementofP. TheprimarykeyP mustsatisfythe
ind
followingtwoproperties:
1. Uniqueness
Foranytwodistincttuples(rows)r andr inthetablewherei ̸= j,theirprojectionsontheprimarykey
i j
attributesmustbedifferent,itthus,mustholdthat
rP ̸=rP ∀r ,r ∈Dwithi̸=j,
i j i j
Here,rP denotestheprojectionoftuplerontothesubsetofattributesP whichis
rP =(a ) .
i ij j∈Pind
ThisensuresthatnotworowscanhavethesamevalueforP,providingauniqueidentifierforeachrecord.
2. Minimality
The set P must be minimal with respect to the uniqueness property. This means that no proper subset of
P satisfiestheuniquenesscondition. IfanyattributeisremovedfromP,theremainingattributesnolonger
uniquelyidentifyeveryrowinthetable. ItmustthereforeholdthatifQ⊂P andQ̸=∅,thenthereexistat
leasttworecordsr andr inDsuchthat:
i j
rQ =rQ,
i j
whererQdenotestheprojectionofrecordr ontothesubsetQ.
i i
8TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
Primary keys in a database play a crucial role in ensuring data integrity and efficient data management. They are
fundamentalbecausetheyuniquelyidentifyeachrecordwithinatable,whichisessentialformaintainingtheuniqueness
and accuracy of the data stored. This unique identification facilitates efficient data retrieval, management, and
manipulationprocesses,ensuringthateachrecordcanbeaccessed,updated,orrelatedtootherdatainthedatabase
withoutambiguity(Köhleretal.,2015;Leetal.,2012;Zhouetal.,2014). Theywillbepivotaltofindingduplicatesand
willshortenprocessingtimeswhenfindingmissingvalues.
Theapproachweintroducecombinestwostrategiesofkeysidentification. Patternrecognitionanduniquenessanalysis.
Theprocedureissummarizedasfollow:
1. Theoretically,thesetofattributesthatconstituteaprimarykeyshouldneverhavemissingvalues,particularly
ifthedataisstoredinadatabasesoftware. However,sincethisframeworkisdesignedtoaccommodateany
dataset,suchanassumptioncannotbemade. Nevertheless,wecananticipateprimarykeycandidatestoexhibit
onlyafewmissingvalues. Therefore,weidentifyallfieldswithlessthan5%percentmissingvalues.
2. Wethenidentifyeachcolumnnamethatcouldpotentiallydesignateaprimarykey,astheytypicallycontain
termssuchasID,CODE,orKEY.Fromthere,twopossibilitiesarise:
(a) Patternrecognition
Webeginbyattemptingaquick-winsolution. Ifthisprocesshasresultedinonlyonefieldremaining,then
wehaveagoodcandidateforbeingtheprimarykey. Weconductasetofsanitycheckstoensurethatthe
choiceisjustified. Weverifythatwedonothavemorethan5%ofduplicates(seebelow). Additionally,
weensurethatthefieldisnotanumericalvaluedfield. Whiletheoretically,anumericalvaluedfieldcould
serveasaprimarykey,itisratherunlikelyduetoformatchanges,uniquenessissues,etc. Ifallthesanity
checksaresuccessfullypassed,wehaveidentifiedouruniqueprimarykey.
(b) Uniquenessanalysis
If not, we proceed with candidates obtained from step 1 and conduct a uniqueness analysis for all
combinationsofvariables,rangingfromthesmallesttothelargestset. Similartothefactthatwecannot
assumethefieldscomprisingtheprimarykeyswillbedevoidofmissingvalues,wecannotassumethey
willlackduplicates. However,wecananticipatethatonlyafewduplicateswilloccur. Again,welimit
themto5%. Therefore,assoonasacombinationofcandidateattributes,withitsnumberofduplicates
fallingbelowthethreshold,isidentified,itisconsideredasaprimarykeyfortheentiredataset. Asfields
labeledwiththeexpressionsID,CODE,orKEY arestilllikelycandidatestoformsetsofprimarykeys,
weinitiatetheprocesswiththem.
Wecannowanalyzethecomplexityofthisalgorithm. Itisclearthatifweareinacasewherethepatternrecognition
wasnotconclusive(step2.(a)),thecomplexityisdominatedbytheuniquenessanalysisstep. Also,itisclearthatthe
advantagesobtainedfromtheidentificationoftheprimarykey,intermsofstructuringthedatasetandacceleratingother
processes,mightbeoutweighedbytheresourcesdeployedtoobtainsaidprimarykeyifthatprocessisnotcontrolled.
Forthatreason,itismorecomputationallyreasonabletolimittheanalysisofcombinationsanalyzedforuniqueness. For
instance,whenconsideringonlypairs,theworst-casecomplexityisO(m2nlogn),wherenisthenumberofrecords
andmisthenumberofattributes. Meanwhile,ifthepatternrecognitionissuccessful,thetimecomplexityisO(mn).
Inbothcases,thespacecomplexityisO(mn),primarilyduetothestoragerequirementsforintermediateresultsduring
uniquenesschecks. Inpractice,however,aswewillseeinsection5.1.1,theworst-casescenarioisnotreachedaswe
startwiththefieldsidentifiedinthepatternrecognitionstep. Indeed,ifthosefieldscanconstitutetheprimarykey,letq
betheirnumber,thenthecomplexitywouldbeO(q2nlogn).
• Timecomplexityanalysis
– Theinitialstepofidentifyingfieldswithlessthan5%missingvaluesinvolvesiteratingthrougheachfield
(column)andcheckingformissingvaluesacrossallrows. Thisprocessresultsinatimecomplexityof
O(nm),wherenrepresentsthenumberofrowsandmrepresentsthenumberofcolumns. Thisstepis
essentialtofilteroutcolumnswithexcessivemissingdata,ensuringthatpotentialprimarykeycandidates
areviable.
– Next,theprocessofidentifyingcolumnsthatcouldpotentiallydesignateaprimarykeybycheckingfor
specifictermssuchas"ID,""CODE,"or"KEY"intheirnameshasatimecomplexityofO(m). Thisis
becauseeachcolumnnameisexaminedindividually. Thisstepnarrowsdownthelistofpotentialprimary
keysbasedonnamingconventions,servingasaquickheuristiccheck. Thepatternrecognitionstepoffers
aquick-winsolutionwhere,ifonlyonefieldremains,sanitychecksareconductedtoverifyitssuitability
asaprimarykey. However,ifthequick-winsolutionisnotapplicable,thealgorithmproceedstothemore
intensiveuniquenessanalysis.
9TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
– Theuniquenessanalysisstepisthemostcomputationallyexpensivepartofthealgorithm. Itinvolves
examiningcombinationsofcandidateattributestoensuretheiruniqueness. Ifweconsidercombinations
ofsizes,thenumberofsuchcombinationsisgivenby(cid:0)m(cid:1) ,whichisO(ms). Theuniquenesscheckfor
s
eachcombinationcanbeperformedbysortingrecords,whichhasaO(nlogn)timecomplexity. Thus,
theoveralltimecomplexityisO(ms)×O(nlogn)=O(msnlogn). Inthecaseofpairsandfocusing
oncandidatefields,thetimecomplexityisO(q2nlogn).
• Spacecomplexityanalysis
– Thespacecomplexityforidentifyingfieldswithlessthan5%missingvaluesisO(m), asitrequires
storingalistofcolumnsthatmeetthecriteria. Thisstepensuresthatonlyrelevantcolumnsareconsidered
forprimarykeycandidacywithoutconsumingexcessivespace.
– Foridentifyingcolumnsbasedonspecifictermsintheirnames,thespacecomplexityremainsO(m). The
listofpotentialprimarykeycolumnsisstored,whichisproportionaltothenumberofcolumnsinthe
dataset.
– TheuniquenessanalysisstephasaspacecomplexityofO(mn). Thisisbecausethealgorithmneedsto
storecombinationsofcolumnsandintermediateresultsforeachuniquenesscheck. Thespacerequired
forthesecombinationsandresultscangrowsignificantlywithanincreasingnumberofcolumns,making
thisstepthemostspace-intensivepartofthealgorithm.
4.1.2 Mappingprocessestospecificdatafields
Asourworkaimstoenhanceandmeasuredataqualitywithoutrelyingondomainknowledgeorknowledgeofthe
currenttable’scontent,itiscrucialtobeabletopreciselytargeteachanalysisandcorrectiontomaintainreasonable
computerresourcedemands. AsimilarapproachistakenbySchelteretal.(2018). Whiletheauthorsdonotspecifically
aim to make their work domain-knowledge-free, their software can propose a set of constraints to verify for each
columnintheabsenceofuser-providedconstraints. Thisresemblesourproblem,butaccordingtotheauthors,their
constraintsuggestionprocessisdesignedtoinvolvehumanintervention. Whiletheautomatedprocessweintroduce
maybetime-consuming,itultimatelysavestimeandcomputationalresourceswhenconsideringtheentireprocess. The
definedrulesforeachtypeoferrorconsideredinthisworkareasfollows:
• Redundancies
Onlythesubsetofattributescomprisingtheprimarykeywillbeanalyzedforduplicates. Thisisoneofthe
justificationsforwhyitisimportanttobeginbyidentifyingit.
• Absences
All fields will be analyzed for missing values, as they can affect any field. However, as we will see in
Subsection4.2.2,notallfields’missingvalueswillbeimputed.
• Statisticaloutliers
Onlyfieldswithnumericalvalueswillbeconsidered. Wealsoassertthatwithoutsufficientinformationonthe
field,accuratelyapprehendingitsstatisticalpropertiesischallenging. Hence,wewillnotattempttodetect
statisticaloutliersunlessmorethanhalfofthevaluesareavailable.
• Typographicalerrors
ThealgorithmintroducedinSubsection4.2.4functionsforbothrealwordsandnon-words. However,itis
notintendedforapplicationtoentriescontainingbothnumericalandtext. Forinstance,thefieldfiBaseModel
includesvalueslikeZX160. Withoutdomainknowledge,itisatbestverydifficulttoascertainwhetheranother
entry, such as ZX161, is a typographical error or a legitimate model. For this treatment, we also exclude
fieldswithlessthanhalfofthevaluesavailable. Indeed,weargueagainthatwithmorethanhalfofthedata
missing,anyanalysisorinsightsderivedfromthisfieldarealreadysignificantlycompromised.Thepresenceof
typographicalerrorsinaminorityofthedataislesslikelytohaveasubstantialimpactontheoverallanalysis
comparedtothelargeproportionofmissingvalues.
• Logicerrors
These fields should contain strings, have less than 75% of their values missing, and be represented by at
leastfivedifferentobservations. Allowingupto75%missingvaluesinthesecolumnsconsiderstheunique
natureoflogicerrors—errorsthatmightnotnecessarilybeabouttheabsenceofdatabutaboutdatabeing
presentwhenitshouldn’tbe,ormissingwhenitshouldbepresent. Therefore,whenanalyzinglogicerrors,it
isnotconservativeenoughtodisqualifyfieldssolelybecausetheydonothaveenoughdata. Moreover,the
requirementforatleastfivedifferentobservationsensuresadiversityofdataentriesthatcanhelpidentify
inconsistenciesanderrorsthatasmallernumberofobservationsmightmiss. Bysettingtheseparameters,we
aimtoenhancethereliabilityandvalidityofthecorrections,focusingonfieldswherethescopeforlogicerrors
issignificantandwherecorrectionscansubstantiallyimprovedataquality.
10TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
4.2 Qualityenhancementphase
4.2.1 Redundancieshandling
Methodology
GivenourdatasetDconsistingofnrecords{r ,r ,...,r },whereeachrecordr isatuple(a ,a ,...,a )of
1 2 n i i1 i2 im
mattributes,weaimtoidentifyduplicatesbasedontheprimarykeyattributes. WerecallthatinSubsection4.1.1we
definedP ⊆ {A ,A ,...,A },thesetofattributesdefiningtheprimarykey. TheprimarykeyP mustsatisfythe
1 2 k
propertiesofuniquenessandminimality,ensuringthateachrecordinDisuniquelyidentifiablebyP. Tworecordsr
i
andr areconsideredduplicatesiftheirprojectionsontheprimarykeyattributesareidentical,i.e.,rP =rP. Therefore,
j i j
thesetofduplicatesDUP isdefinedas:
DUP ={(r ,r )∈D×D |i̸=j andrP =rP}.
i j i j
Togeneralizethisconcepttosetsofmorethantworecords,wedefineasetS ⊆ DasduplicatesifallrecordsinS
haveidenticalprojectionsontheprimarykeyattributeswhichis: ∀r ,r ∈S, rP =rP. Wehencehave,themore
i j i j
generalizedexpression:
DUP ={S ⊆D |∀r ,r ∈S, rP =rP and|S|>1}.
i j i j
ToidentifyallduplicatesetsinD,wefollowthesesteps:
1. Projection
Westartbycomputingtheprojectionofeachrecordontotheprimarykeyattributes:
{rP,rP,...,rP}.
1 2 n
2. Grouping
Then, we group records by their primary key projections. Let G be the collection of L groups, where
∀l∈ 1,L eachgroupG ,containsrecordswiththesameprimarykeyprojection:
l
(cid:74) (cid:75)
G={G ,G ,...,G },
1 2 L
whereG ={r ∈D |rP =v }forsomeuniqueprimarykeyprojectionv .
l i i l l
3. Identifyingduplicatesets
WenowcanidentifyallgroupsG inGthatcontainmorethanonerecord. Thesegroupsrepresentsetsof
l
duplicaterecords:
DUP ={G ||G |>1}.
l l
4. Droppingduplicates
Finally,forallsubsetsG ,wedropallrecordsbutone,wetypicallykeepthefirstinstance.
l
Complexityanalysis
Thisgeneralizedapproachensuresthatanysetofrecordswithidenticalprimarykeyattributevaluesisidentifiedas
duplicates4. Let’sanalyzeitstimeandspacecomplexity. Itcanbeanalyzedintermsofthenumberofrecordsninthe
datasetDandthenumberofattributesk.
• Timecomplexity
– First,weconsidertheprojectionstep,wherewecomputetheprojectionofeachrecordontotheprimary
keyattributes. Thisinvolvesiteratingoverallnrecordsandprojectingeachrecordontothekprimary
key attributes. If k is relatively small compared to n, the projection step has a time complexity of
O(n),otherwiseifkislarge,thetimerequiredforeachprojectionincreases. Specifically,thetotaltime
complexityoftheprojectionphaseisO(nk).
– Next, in the grouping step, we group records by their primary key projections. This step involves
insertingnprojectedrecordsintoasuitabledatastructureforgrouping(typicallyadictionaryorahash
table). Insertingeachprojectedtypicallyhasanaverage-casetimecomplexityofO(1). Therefore,the
groupingstephasanaverage-casetimecomplexityofO(n).
– Then,intheduplicateidentificationstep,weidentifyallgroupsthatcontainmorethanonerecord. This
stepinvolvesiteratingoverthegroupsformedinthepreviousstep. Intheworstcase,therecouldbeup
tongroups(ifallrecordshaveuniqueprimarykeyprojections). Checkingthesizeofeachgroupand
collectinggroupswithmorethanonerecordhasatimecomplexityofO(n).
4Inpractice,inourimplementation,weusedthedrop_duplicatedmethodfromPython’spandaslibrary(McKinney,2010)
11TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
– Finally,foreachgroupG thatcontainsmorethanonerecord,wekeeponlythefirstrecordanddropthe
l
rest. IteratingoverallgroupstakesO(L)time. Again,intheworstcase,wehavengroups. Foreach
groupG ,droppingtherecordsbutthefirstonetakesO(|G |)where|G |isthenumberofrecordsin
l l l
thegroupl. Thesumofallrecordsinthedifferentgroupsbeingn. Thetimecomplexityofthisstepis
thereforeO(n).
Therefore,theoveralltimecomplexityisO(nk+n+n+n)=O(nk)whenkislarge. Thiscomplexityis
reducedtoO(n)whenn>>k.
• Spacecomplexity
Thespacecomplexityofthealgorithmisalsoimportanttoconsider. Theprojectionsoftherecordsrequire
O(n)k space for a large k and O(n) otherwise. The hash table used for grouping requires O(n) space.
CollectingtheduplicatesetsrequiresO(n)spaceintheworstcase(ifallrecordsareduplicates). Droppingthe
duplicatesdoesnotrequireanyadditionalstorage. Therefore,theoverallspacecomplexityisO(n)k.
ThisanalysisindicatesthatifthenumberofattributeskdefiningtheprimarykeyPPisreduced,bothtimeandspace
complexitybecomeO(n). However,foralargevalueofk,thetimeandspacecomplexitiesofthealgorithmbecome
O(nk),whichcouldimpactperformanceforlargedatasets. Thetypicalscenariooccurswhennopreviousworkhas
beendonetoidentifyaprimarykey,resultinginkbeingexactlyequaltothenumberoffieldsinthedatabase(53inour
case). Thisunderscorestheimportanceofthepre-qualityenhancementphase,asitnotonlyallowsforbetterstructuring
ofthedatasetbutalsoreducesthecomputationalcostofthealgorithmsintermsofbothtimeandspacecomplexities.
4.2.2 Missingvalueshandling
As we have already stated, identifying missing values is not a tedious task in itself. The real challenge lies in
distinguishingjustifiablymissingvaluesfromothersandcorrectingonlythesevalues. Withoutdomainknowledge,this
taskbecomesevenmoredifficult. Inthissection,wedecidetoimplementarathersimpletwo-stepsolution.
• Weidentifyfieldsthathavemissingvalueswhichmightbeunjustified. Thesefieldsfallintotwocategories:
first,theattributesoftheprimarykey;andsecond,theattributesthatdon’thavemanymissingvalues. We
consistentlymaintainthesame95%thresholdusedforprimarykeycandidates(seeSubsection4.1.1). Any
fieldwithmorethan95%missingvalueswillbeconsideredtohavejustifiedmissingvalues,andwewillnot
attempttoimputethesemissingvalues. Inanindustrializedimplementation,suchfieldswouldbeflaggedby
warnings.
• Ifthefieldbeinganalyzedispartoftheprimarykey,themissingvalueisautomaticallydeemedabnormal.
Sincetheprojectionofanyrecordontheprimarykeycannotbeduplicated,wecannotusethedistribution
ofvaluesinthesamefieldforthiscase(eveniftheprimarykeymightbecomprisedofmultiplefields,this
approachisnotconservativeenough). Therefore,weautomaticallygenerateaplaceholdercharacterthatwill
actastherecordfortheremainderoftheprocess.Thenifthefieldbeinganalyzedisnotpartoftheprimarykey,
insteadofattemptingtodeterminewhetheramissingvalueisjustifiedornot,weassumethatanyobservation,
includingmissingvalues,couldbevalidandtherearetwopossibilities:
– Ifthefieldisacharacter-valuedfield, noimputationismade, andweassumethatthismissingvalue
isjustified. Intheremainderoftheprocess,specificallyinSubsection4.2.5,thiswillbecheckedasa
logicalerror. Forinstance,iftheEnclosurefieldinonerowisempty,thealgorithmintroducedlaterwill
determineifitislikelyforthemodeldescription(fieldfiModelDesc)intheconsideredrowtohavean
emptyEnclosure-withoutusingdomainknowledge-.
– Ifthefieldcontainsnumericalvalues,weapplyasimpleandclassicalmethodbyreplacingthemissing
valuesusinglinearinterpolationwiththetwoclosestpresentvalues(onewithalowerindexandonewitha
higherindex).Inthiswork,wherewedonothavedetailedinformationaboutthedataset,itisnotadvisable
toreplacemissingvalueswithastatistic(typicallytheaverageorthemedian)oftheobservationsofthe
specificfield. Indeed,thedatasetcould,forinstance,containtimeserieswithseasonality. Inthatcase,the
averagemightnotbesuited.
4.2.3 Processingstatisticaloutliers
Let us now focus on identifying and correcting statistical outliers. We recall that we had defined a dataset D as a
finitecollectionofnrecords{r } witheachr havingkattributes,i.e.,r =(a ,...,a ). Weareinterested
i i∈ 1,n i i i1 ik
inanalyzingtheoutliersforagiven(cid:74)attr(cid:75)ibute. Thatistosay,wewanttodeterminewhether,foragivenattributej of
recordr ,theobservationa ishomogeneouswiththerestoftheobservations{a } . Ofcourse,depending
i ij lj l∈ 1,n \{i}
onthemeaninggiventohomogeneous,theidentificationandhandlingofstatisticalout(cid:74)lier(cid:75)smayvary.
12TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
AswehavediscussedinSection1,severaltechniqueshavebeendevelopedtoidentifyoutliers. Amongthem,many
useclassicalresultsoninterquantileintervals. Thisisnotablythecasewith(Corralesetal.,2018), whichdecides
thatvalidstatesa arethosethatverifya ∈[Q −1.5(Q −Q ), Q +1.5(Q −Q )]whereQ andQ define
ij ij 1 3 1 3 3 1 1 3
respectivelythefirstandthirdquartilesofthedistributionofattributej’sobservations{a } . Anothercommon
lj l∈ 1,n
method(seeforinstanceHowell(1998))involvesinvalidatingallvaluesthatlieoutsideaninterv(cid:74)alb(cid:75)asedonthestandard
deviation. Inotherwords,a isinvalidatedassoonasa ∈/ [−ασ+µ, ασ+µ], α∈R∗. µdesignatesthemeanof
ij ij +
thedistributionandσitsstandarddeviation. Themostcommonpracticeseemstobetochooseα=3(Miller(1991)for
example).
Thereareseverallimitstothesemethods:
• Firstly,theyassumethatthedistributionofobservationsisnormal(Leysetal.,2013).
• Secondly,themeanandstandarddeviationarestronglyinfluencedbytheoutlierswewanttodetect(Leys
etal.,2013),leadingtodistortedmeasuresofcentraltendencyanddispersion.
• Finally,asCousineauandChartier(2010)pointsout,thismethodhasverylittlechanceofdetectingoutliersin
smallsamples.
Therefore,applyingthesemethodscanleadtoincorrectidentificationofoutliers. Inskeweddistributions,onemight
eithermisstrueoutliersorfalselyidentifyregulardatapointsasoutliers. Formultimodaldistributions,thesemethods
mightfailtorecognizeoutlierswithineachmode. Overall,thereisanon-negligiblechanceoffalselyidentifyingnormal
datapointswhentheassumptionsrequiredbythesemethodsarenotmet. Despitethesedisadvantages,therearegood
reasonstocontinueusingthesemethods. Theyareverysimpletouseandinterpret,andwhentheamountofdataused
islarge,thesamplecanapproachaGaussiandistribution.
IsolationForest
Asothershavedoneintheliterature(seesection1),wewanttoleveragemachinelearning’sabilitytoovercomethe
abovelimitations. Awidelyutilizedandhighlyeffectivealgorithmforanomalydetectionthattakesanovelapproach
toidentifyingoutliersisIsolationForest(IF)(Liuetal.,2008). Unliketraditionalmethodsthatfirstestablishwhat
constitutesnormaldataandthenidentifydeviationsfromit,IsolationForestfocusesdirectlyonisolatinganomalies.
Thecoreideabehindthisalgorithmisthatanomaliesareeasiertoisolatethannormalpointsbecausetheyarefewand
different. Byrecursivelypartitioningthedataset,thealgorithmcreatesaseriesofdecisiontreesdesignednottogroup
similardatapointsbuttoisolateindividualpoints. Anomalies,duetotheirrarityanddistinctiveness,requirefewer
partitionstobeisolatedcomparedtonormaldatapoints. Thisprocessmakesanomaliesstandoutmoreclearlyand
bedetectedmoreefficiently. TheprocessandefficiencyofIsolationForestinisolatingoutliersisvisuallydepictedin
Figures2,3and4,wherethepartitionshighlighttheisolationofanomaliesfromtherestofthedataset. Thismethodis
particularlyadvantageousinlargedatasetsandreal-timeanomalydetectionscenariosduetoitslineartimecomplexity
andscalability. Asthismethodseemslesscommonthanotherusedinthiswork,wegiveamoredetailedexplanationof
thealgorithm’sfunctioningwhichcanbedescribedbythefollowingsteps:
1. Subsampling:
• Thealgorithmrandomlyselectsasubsetofthedatapointsfromthedataset. Thisstephelpsmakethe
algorithmscalableandreducescomputationalcomplexity.
2. TreeConstruction:
• RecursivePartitioning: Foreachselectedsubset,thealgorithmrecursivelypartitionsthedatapointsby
randomlyselectingafeatureandthenrandomlyselectingasplitvalueforthatfeature.
• NodeCreation: Eachpartitioncreatesanodeinthetree. Theprocesscontinuesuntileachdatapointis
isolatedinitsownleafnode,themaximumtreeheightisreached,orthenodecontainsasingledatapoint.
3. ForestBuilding:
• Thetreeconstructionprocessisrepeatedmultipletimestobuildacollectionofisolationtrees,forming
theIsolationForest.
4. PathLengthCalculation:
• Foreachdatapointinthedataset,thealgorithmcomputestheaveragepathlengthfromtherootnode
totheterminatingnode(leaf)acrossallthetreesintheforest. Thepathlengthisthenumberofedges
traversedfromtheroottotheleaf.
5. AnomalyScoreComputation:
13TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
• Theanomalyscoreforeachdatapointiscalculatedbasedontheaveragepathlength. Thescoreisdefined
suchthatshorteraveragepathlengthscorrespondtooutliers(sinceoutlierstendtobeisolatedquickly),
whilelongeraveragepathlengthscorrespondtonormaldatapoints.
6. AnomalyThresholding:
• Thealgorithmdeterminesathresholdfortheanomalyscore. Datapointswithanomalyscoresabovethis
thresholdareclassifiedasoutliers,whilethosebelowthethresholdareconsiderednormal.
For sake of illustration, let us consider a dummy data set, with two features (attributes) Feature 1 and Feature 2
representedbyfigure2
Figure2: Dummydataset,theredandbluepointsarethetwopointswewillfocusourillustrationon.
Nowletusconsidertwoisolationtreesfocusingrespectivelyonetheblueandtheredpoint.
Figure4: Isolationtreeontheredpoint
Figure3: Isolationtreeonthebluepoint
Figures3and4illustratetheisolationprocessfortwoobservations: onethatseemstobevalid(thebluecross)andone
thatseemstobeanoutlier(theredcross). Wecaneasilyunderstandhowtheisolationforestalgorithmworks. Theblue
crossisisolatedaftermanystepswhileonlythreestepsareenoughtoisolatetheredcross5. Thisindicatesthatthe
redcrossisanoutlier. Thisprocessisrepeatedoveralargenumberofrandomtrees(leadingtotheforest). Then,the
numberofsteps(branches)beforethesplittingareaveraged(step4above),andacomparativeanalysisisperformed
withthethreshold(steps5and6above). Figure5showstheaveragenumberofstepsfordifferentsizesofforests.
5Notethatthesegraphicsareforthesakeofillustration;thedatasetandtreeparameterizationarechosentoperfectlyandonly
matchtheisolationoftheredandbluecrosses.Inpractice,theisolationanalysisisperformedforallobservationssimultaneously
14TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
Figure5: Numberofstepsforavaliddataandastatisticaloutlier
Thisalgorithmhasprovenitsefficiency,particularlyonlargedatasets. However,thisispreciselywherethelimitation
liesinourcontext.Asthismethodismoreinterestinginhighdimensions,itnaturallylosesitsexplainability,particularly
inpoint1.cofthedefinition(erroridentification). Infact,ifthedatasetcontainsonlyoneortwocolumns,thealgorithm
losesitsefficiency(ascanbeintuitedfromFigures3and4above). Ontheotherhand,ifitusesallthevariablesor
asubsetofthem,itbecomesimpossibletoknowwhichcolumnreallyposesaproblem. Wethereforechoosenotto
usethispowerfultechniqueinhighdimensions. Instead,weproposetocombinestatisticalmethodsusingstandard
deviationandIFinonedimension.
WeworkwithZ theZ-scoreofobservationa ,Z = aij−µj. Whereµ andσ designatetheunbiasedestimatorsof
ij σj j j
respectivelytheexpectationandthestandarddeviationofthedistributionofattributej’sobservations{a } . Let
lj l∈ 1,n
φ bethefunctiondefiningwhethertheinputX isanoutlier,returning1inthatcaseand0otherwise. L(cid:74)etu(cid:75)salso
outlier
assumethatourIFisafunctionthatreturns1whengivenanoutlierand0otherwise.
φ :R→{0, 1}
outlier
Z (cid:55)→φ (Z).
outlier
(cid:26)
f (Z, β , β ) if|µ |<α and|µ |<α
φ (Z)= 1 1 2 3 s 4 k (1)
outlier f (Z, β , β , γ) otherwise,
2 1 2
where:
(cid:26)
1 ifZ ∈/]−β , β [
f (Z, β , β )= 1 2 (2)
1 1 2 0 otherwise
and
f (Z, β , β , γ)=f (Z, γβ , γβ )×IF(Z), (3)
2 1 2 1 1 2
where
(cid:26)
1 iftheIsolationForestalgorithmdetectsanoutlier
IF(Z)= (4)
0 otherwise.
Wedefinetheterms6:
• µ andµ respectivelydefinethe3rdand4thstandardizedmoment,skewnessandkurtosis.
3 4
• α andα aretwostrictlypositivemomentacceptancethresholds. Wetake6and30respectively.
s k
• β ,β aretwostrictlypositiveacceptancethresholdsforthestandarddeviation-basedtoleranceinterval. We
1 2
take3forbothcoefficients.
• Finally,γ isaparametertowidenthetoleranceintervalonthevalueofZ,wechoose2.
Theideabehindthisalgorithmistosuggestthatifwehaveenoughdatawithadistributionthatisnottoodistorted,with
reasonablekurtosisandskewness,theGaussianapproximationisnotasignificantassumption. Inthiscase,wecan
applythestandarddeviationruledirectly. If,ontheopposite,thisisnotthecase,weapplythesamerulebutwitha
6Thelargenumberofparametersmayseemlikeanobstacletoeasyautomation,butinreality,theseparametersarequitesimple
tochooseandcorrespondtofairlyintuitiveandflexiblechoicesbasedonthedesiredtolerance.
15TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
toleranceintervalγ timeslarger(e.g.,twiceaslarge). Sincewearelosingreliability,weseekasecondopinionusing
theIFalgorithmandconsiderasoutliersonlythoseentrieswherebothtechniquesagreeontheinvalidityofthedata.
Themainadvantageofthismethodisthatwebenefitfromthestandarddeviation’sabilitytodiscriminatevaluesthatare
toohighortoolow,whiletheIFnotonlylooksatextremevaluesbutalsoidentifiesoutliersasvaluesthatareisolated
fromtheremainingdataset. Indeed,thisalgorithmwillnotidentifyanextremevalueasanoutlierifithasotherextreme
valuesinitsneighborhood.
Figures 6 and 7 below illustrate the Kernel Density Estimation (KDE) of two attributes: Saleprice and YearMade.
Theyhelpeasilyunderstandingthealgorithm’sprocess. FortheattributeYearMade,thedataiscenteredaroundthe
year 2000, with some noise around the mode of the distribution. We also observe that the support of the density
reachesnegativevaluesaswellasveryhighvalues,indicatingthepresenceofinvalidvalues. Inthiscase,duetothe
relativelycorrectappearanceofthedensity,wedonotneedthecomputationalpowerrequiredfortheIFalgorithm. We
canbeoptimisticinourabilitytosuccessfullyidentifyoutliersusingonlythestandarddeviationrule,thatistosay,
φ (Z)=f (Z, ...). Ontheotherhand,lookingattheattributeSaleprice’sdensity,weobservethattherightmost
outlier 1
partofthedensityhashighvalues. Usingonlythestandarddeviationrule,thesevalueswouldprobablybewrongfully
discarded. Therefore, thealgorithmwidensthetoleranceofthestandarddeviationruletoensurethatwefocuson
extremevalues,andthentheIFidentifiesisolatedextremevalues,ensuringthatapriceisnotdiscardedforbeinghigh
butforbeinginahighregionwherenootherproducthasacloseprice. Thatistosay,φ (Z)=f (Z, ...). Ifwe
outlier 2
haddomainknowledge,thesolutionwewouldapplywouldbeveryclosetotheoneouralgorithmapplies. Indeed,we
wouldhaveknownthatthepricesoftheproductswillgenerallybereasonablyclose,butforsomespecificproducts,the
pricescouldbeveryhigh(hencethebehaviorofthedistribution).
Figure6: KDEofattributeSaleprice Figure7: KDEofattributeYearMade
Onceoutliershavebeenrigorouslyidentifiedasinvalid,theyaretreatedlikeanyothernumericalmissingvalue. They
arethereforeimputedusinglinearinterpolation,asdescribedinSubsection4.2.2.
Onthechoiceofα andα
s k
Thechoiceofthetoleranceparametersfortheskewnessandthekurtosis,respectivelyα andα ,isimportant. One
s k
mustkeepinmindthattheframeworkshouldworkwithoutdomainknowledgeandonanygivendataset. Therefore,the
valueschosenmustprovideapractical,flexible,androbustframeworkforhandlingreal-worlddatadistributions,which
oftendeviatefromnormality. Wechosetoselectourparametersbasedonaknownskewedandheavy-taileddistribution.
Weusedachi-squareddistributionwith1degreeoffreedom. Theskewnessthresholdof6isapproximatelydoublethe
skewnessofthatdistribution,indicatingasubstantialbutreasonableallowanceforasymmetry. Thekurtosisthreshold
of30,beingdoublethekurtosisofthereferencedistribution,suggeststoleranceforheavy-taileddistributions. These
thresholdsenabletheprimaryoutlierdetectionmechanism(basedonstandarddeviation)tobebroadlyapplicable,
ensuringminordeviationsindistributionshapedonotoverlyinfluenceoutlierdetection. Thisapproachandthechosen
valuesbalancesensitivityandspecificity,reducingfalsepositives. Thehighthresholdsensurerobustness,makingthe
16TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
algorithmadaptabletovariousscenarioswithoutfrequentfalsealarms,whilestillbeingstringentincasesofextreme
skewnessorkurtosis. Theseparameterscanbetuneddependingonone’stolerancetofalsepositives.
Complexityanalysis
The complexity of this algorithm is dominated by the IF component. A thorough analysis on its time and space
complexity is conducted by (Liu et al., 2008). Their work demonstrates a time complexity of the IF algorithm in
O(tψlog(cid:32)+ntlog(cid:32)),andaboundedmemoryrequirementthatgrowslinearlywithn. Intheseexpressions,tdesignates
thenumberoftrees,ϕisthesub-samplingsize,andnisthenumberofrecords.
4.2.4 ProcessingTypographicalErrors
Thecorrectionoftypographicalerrors(TPOs)isaveryimportantissueinthepreparationofdataprocessing. Spell
checkingstandsasthecrucialprocessofidentifyingandsuggestingcorrectionsforwordsthatareinaccuratelyspelled.
Essentially,classicspellcheckersserveascomputationaltoolsleveragingadictionaryofwordstoexecutethistask.
The efficacy of such a checker relies on the extent of its dictionary. A larger repository of words typically results
inanincreasedabilitytodetecterrors. However,duetotheirrelianceonstandarddictionaries,theyareinadequate
incapturingmistakessuchaspropernouns,specializedtermspertinenttoparticulardomains,acronyms,andother
specializedterminologies. Thesetypesofwordsarecallednon-wordsintheliterature(Leeetal.,2020). Theyrepresent
anotablecontrasttoconventionalwords,commonlyreferredtoasrealwords. ThisiswellexplainedinBassiland
Alwani(2012). AswehavealreadymentionedinSection1,ononehand,muchoftheliteraturerequirestheuseof
externaldictionaries,andontheotherhand,manyalgorithmsintheliteratureuseML/DLtechniquestohandledomain
knowledge,particularlytocorrectnon-words. WeintroduceinwhatfollowsaprocedureleveragingML,efficientfor
bothrealandnon-wordswithoutanyuseofdomainknowledge. Letusstartbyintroducingkeyconceptsthatarepivotal
forouralgorithm.
Damerau-LevenshteinDistance
Consider two distinct character strings A and B and let us consider the task of finding the minimum number of
operationsrequiredtotransformBintoA.
Whenonlysubstitutionsareallowed,theminimumnumberofoperationsisgivenbytheHammingdistance(Robinson,
2003),orwhenonlytranspositionsareallowed,theJarodistance(Jaro,1989). TheLevenshteindistance(Levenshtein,
1966)isthelengthoftheshortestsequencewhensubstitutions,insertions,anddeletionsareallowed. Thisdistanceis
usedinthelongestcommonsubsequenceproblem(Maier,1978). Inthispaper,wewillusetheDamerau-Levenshtein
distance(Levenshtein,1966;Damerau,1964),whichisoptimalwhenfouroperationsarepossible(substitution,insertion,
deletion,andtranspositionoftwoconsecutivecharacters). TheDamerau-Levenshteindistance(DLD)fortheletters
numberiandj ofwordsAandBisrecursivelyobtainedthroughd(.,.)below(Boytsov,2011):

0 ifi=j =0
d
A,B(i−1,j)+1 ifi>0
d (i,j)=min d (i,j−1)+1 ifj >0 (5)
A,B A,B
dd
A,B
(( ii −− 21 ,, jj −− 21 )) ++ 11
(Ai̸=Bj)
i if fi i, ,j
j
> >0
1andA =B andA =B .
A,B i j−1 i−1 j
Forexample,sinceitonlytakesonetranspositiontogofromFrancetoFracne:
DLD(France, Fracne)=1.
Inpractice,wewillnotusetheDLDitself,butratherascorethatincreasestheclosertwowordsaretoeachother.
Indeed,adistanceof2betweentwo4-letterswords,forexample,andadistanceof2betweentwo15-letterswords
shouldnotbeinterpretedinthesameway. TheDamerau-LevenshteinScore(DLS)isgivenasafunctionoftheDLD
betweenAandB,DLD(A,B)andthemaximumpossibledistancebetweentwowordsofthelengthsofAandB,
notedDMAX :
A,B
DMAX −DLD(A,B)
DLS(A,B)= A,B . (6)
DMAX
A,B
Consideringthepreviousexample,notethatthemaximalDamerau-Levenshteindistancebetweentwowordsoflength
6is6. Therefore,thescorebetweenFranceandFracneisgivenby:
6−1
DLS(France, Fracne)= ≈0.8334.
6
17TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
Whenconsideringnowtwowordsoflength3,forexamplePauandPou,withstillaDLDof1,thescoreislowerthan
forthepreviousexample:
3−1
DLS(Pau, Pou)= ≈0.667.
3
Detectingtypographicalerrors
Todetectalltypographicalerrorsinadatabasefield,theidealsolutionwouldbetocalculatetheDLSofeachentrywith
alltheothers. ThiswouldgiveusamatrixofDLSs,andwewouldthengrouptogethertheentrieswithlowDLSs,as
thiswouldindicatethattheywerespelledverysimilarlyandwerethereforethesamewordspelledindifferentways. To
findthetruevalueandthuscorrecttheTPOs,itwouldbelogicaltotaketheentrythathasbeenusedthemostoften.
Unfortunately,thisidealsolutionisnotfeasibleduetoitscomputationalcost. Indeed,evenwhenonlyconsideringthe
stepinvolvedinbuildingtheDLSmatrix,itwouldrequireO(n2)Damerau-Levenshteinfunctioncalls,whichisnot
feasibleforatablewithhundredsofthousandsorevenmillionsofentries. Nevertheless,wehavedecidedtokeepthe
essenceofthismethodbutattempttoreducethesizeoftheproblem. Toachievethis,wewillsortthelistofobservations
alphabeticallybeforecalculatingtheconsecutiveDLSs. Theideaisthatbysortingtheminalphabeticalorder,wewill
increaseourchancesofgroupingsimilarentriestogetherandformingpartoftheaforementionedgroups.
Toidentifythegroups,weexaminethecurveofconsecutiveDLSs. AssoonasajumpisobservedontheDLScurve,
wehaveapriorireachedadifferentword. Thethresholdbeyondwhichweadmittohavingchangedwordsissetat
0.7. Figure4.2.4illustratesthecurveofDLSsofwordsalphabeticallyorderedaswellasthethreshold. Thewords
correspondtotheentriesofthefieldState.
Figure8: Damerau-LevenshteinscorejumpsforvariableState
HavingidentifiedallthegroupsbasedontheDLSjumps,wecannowrepresenteachofthesegroupsbytheirdominant
element(theonewiththemostoccurrencesinthedataset). Itisthennecessarytoretrieveanyelementsofagroupwe
mayhavemissed. Forexample,FranceandGranceareverycloseinDLSterms,butwhensortedalphabetically,they
willnotbeinthesamegroup. ThisisthestepwhereweuseML,specificallyunsupervisedlearningviaclustering
techniques. TheclusteringalgorithmseekstogrouptogetherallwordswithsimilarDLSvectors(arowoftheDLS
matrix).
The algorithm we decided to use is hierarchical agglomerative clustering (Müllner, 2011). In a different context
fromthatofthisdocument,wheretheuseofaprioriknowledgeofthedatabasecouldbeexploited,theusercould
communicatethecorrectvalue(whichcouldcorrespond,forinstance,tothenumberofstateswheresaleshavebeen
concluded). However,inourcontext,weareobligedtodeterminethenumberofclusters. Todothis,weusethegap
statisticmethod(Tibshiranietal.,2001),whichproceedsbycomparingtheintra-clusterdispersionwiththatwhich
wouldhavebeenobtainedinanemptyset.
18TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
Atthispoint,allsimilarelementshavebeengroupedtogether. However,anadditionalstepisnecessary: correcting
wrongfullygroupedelements. Whilethissituationisunlikely,itcanoccurwhendealingwithlongandsimilarwords.
Forexample,NorthDakotaandSouthDakotasharetenoutoftwelvecharacters,resultinginaDLSof0.8334,which
exceedsthe0.7threshold.
Theintroducedsolutionisquitestraightforward. Withineachgroup,weidentifyelementsthatdifferfromthedominant
element(theonewiththemostoccurrences)butstillhavehighfrequencies. Theseentriesarecandidatesforbeing
incorrectlyattributedtoagroup. Dependingonthenatureofthefieldbeinganalyzed,oneoftwosolutionswillbe
applied. Ifthewordsbeingtestedaredictionaryentries,asimpletestistodetermineiftheelementsdifferingfromthe
dominantelementwithhighoccurrencesarevaliddictionaryentries. Iftheyare,theyareidentifiedasvalidentries,and
aflag(typicallyawarninginprogramminglanguages)israisedtoprompttheusertocheckthesesspecificcorrections
afterthewholeprocess. Ontheotherhand,ifthewordsarenotdictionaryentriesbutnon-words,nocorrectionismade
duetoinsufficientinformationtoconfidentlydiscardthepreviouslyrigorouslyobtainedresult. Again,aflagisraised
fortheusertoreview.
Remarks
• Itisimportanttonotethatthisprocess,especiallycheckingifthewordsarerealornon-words,isnotexpensive
sinceweonlyneedtoconsidertheuniquedominantentries,whosenumberissignificantlylowerthanthe
numberofobservations.
• Theaimofthisworkistoconstructafullyautomatedprocesstomeasureandenhancethequalityofadataset.
Flagsareraisedsolelytoinformtheuserofcorrectionsthatwereorwerenotconductedincasesofpotential
ambiguity. Noactionisrequiredfromtheuser,ensuringthattheprocessremainsfullyautomated.
• Itisimportanttounderlinethesignificanceofthechoiceofthe0.7thresholdtodetectDLSjumps. Thisis
ahyper-parameterofthealgorithm. Thehigherthethreshold,thefewerthenumberofmistakenlygrouped
words,butasatrade-off,thenumberofelementstoincludeintheclusteringanalysisincreases. Ontheother
hand,thelowerthethreshold,themoremistakenlygroupedwords,andtherefore,morecorrectionsarerequired
afterclustering. Tostriketherightbalancebetweenaccuracyandcomputationalperformance,itisnecessary
tocarefullychoosethethreshold. Ahigherorlowerthresholdcouldyieldbetterorworseresultsdepending
onthedataset. Practitionersshouldnotethatitisbettertouseahigherthreshold(typicallycloseto0.7)and
recoverwordsthatwerenotinitiallygroupedthroughclusteringratherthangroupingwordsincorrectlyfrom
thestart.
TheTPOdetectionalgorithmdescribedcanbesummarizedasfollows:
1. ThelistofwordsrequiringTPOdetectionissortedalphabetically,andtheconsecutiveDLSsarecalculated.
2. UsingDLSjumps,wedistinguishtheinitialsimilaritygroups. Eachgroupcontainsalltheelementsfromthe
currentpeak(included)tothenext(excluded). Eachgroupisthenrepresentedbyitsdominantelement(the
elementwiththehighestfrequency).
3. Dominantelementsthatseemtorepresentthesamewordareregrouped. Thisisachievedbycalculatingthe
matrixofDLSsonalldominantelements,thenclusteringthismatrixtoregroupsimilarwords. Dominant
elementsthatareinthesameclustersareconsideredtorepresentthesameword. Theirinitialgroupsare
merged. Thenewdominantelementistheoneamongthemwiththehighestoccurrence.
4. A finalverification is made to ensure thatno words are wrongfully flaggedas a TPO. Ineach cluster, we
identifynon-dominantelementswithhighfrequencies(forinstance,membersofaclusterthathavemorethan
halfthefrequencyofthedominantelement),thesehavepotentiallybeenwrongfullyidentifiedastypographical
errors.
(a) Ifthefieldbeinganalyzedcontainsrealwordsthatarelistedinthedictionary,wecheckwhetherthe
potentiallywrongfullyidentifiedwordsarevaliddictionaryentries,ifso,theyareconsideredasvalid
wordsandnotTPOs.
(b) Otherwise,wedonotmakeanymodificationstothepreviouslyconstructedgroups.
5. Attheendoftheprocess,wehaveasmanygroupsasvalidwords(realwordsornon-words,dependingonthe
fieldcontent). Eachgroupisrepresentedbyoneanduniquedominantelement,andeachelementinthegroup
writtendifferentlywillbecorrectedtothedominantelement.
Complexityanalysis
Letusanalyzethetimeandspacecomplexityofthisalgorithm. Werecallthatnisthenumberofrecordsinthedataset,
19TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
thusitisthenumberofwordsinagivenfield. Letusdefinegasthenumberofdominantelements,g <<n. Letl
avg
betheaveragelengthofthewordsbeingcorrected. l isanegligiblevaluecomparedton.
avg
• Timecomplexity
– SortingthelistofwordsalphabeticallytakesO(nlogn)time.
– The DLS calculation between two words has an average complexity of O(l2 ). Calculating DLS
avg
betweenconsecutivewordsinthesortedlisthasanaveragecomplexityofO(nl2 ). Wecanreasonably
avg
statethatl2 <<n. WhichleadstoatimecomplexityofO(n)forthisstep.
avg
– Distinguishing initial similarity groups and determining dominant elements involves scanning
throughthelistofwordsoncetoformgroupsanddeterminethedominantelementwithineachgroup.
ThisisO(n)sinceitinvolvesalinearpassthroughthelist.
– ToregroupdominantelementsbyclusteringDLSmatrix,westartbyconstructingtheDLSmatrix
forgdominantelementwhichinvolvesg(g−1)/2DLScalculations,eachtakingO(l2 ),resultingin
avg
O(g2l2 ). Herewecannotmaketheassumptionl2 <<g2. Clusteringthematrixusinghierarchical
avg avg
clusteringhasacomplexityofO(g3).
– ThelaststepofthealgorithmconsistsinverifyingflaggedTPOs. Itrequiresidentifyingnon-dominant
elementswithhighfrequenciesbyscanningthroughthewholelist,resultinginacomplexityofO(n).
Then, checking against a dictionary (assuming the dictionary check is O(1)) involves scanning each
flaggedword,takingO(n)intheworstcase.
– Constructingthefinalgroupsandcorrectingwords: Thisstepinvolvesscanningthroughthelistand
makingnecessarycorrectionsandthereforehasatimecomplexityofO(n).
Letusnowaggregateallcomplexities. Basedontheanalysisconductedforeachoftheprevioussteps,the
overallcomplexityisO(cid:0) nlogn+n+g2l2 +g3(cid:1) ,whichisequivalenttoO(cid:0) nlogn+g2l2 +g3(cid:1) . This
avg avg
complexityanalysisagaindemonstrateshowbeneficialitis,fromacomputationalpointofview,toreduce
the clustering process dimension by sorting the elements and only taking dominant elements. When g is
verysmall,theoverallcomplexitybecomesO(nlogn)whereasthealgorithmwithoutthereductionthrough
g3wouldhaveanoverallcomplexityofO(n3). Thislowcomplexity,justifiestheverygoodcomputational
performanceobtainedaswewillshowinSection5.
• SpacecomplexityThespacecomplexity,ismainlydominatedbytwostorageneeds. First,weneedtostore
allthewordsbeinganalyzedandtheirconsecutiveDLSs. ThishasaspacecomplexityofO(n). Thenweneed
tostorethedominatelementsmatrixwhichhasaspacecomplexityofO(g2). Thereforetheoverallspace
complexityisO(n+g2). Therefore,wheng <2<n,thespacecomplexityisinO(n).
The space complexity is mainly dominated by two storage needs. First, we need to store all the words
beinganalyzedandtheirconsecutiveDLSs. ThishasaspacecomplexityofO(n). Thenweneedtostore
thedominantelementsDLSmatrix,whichhasaspacecomplexityofO(g2). Therefore,theoverallspace
complexityisO(n+g2). Consequently,wheng <2<n,thespacecomplexityisO(n).
4.2.5 Processinglogicerrors
Logicerrorsinadatasetarethemostdelicatetoidentifyandrectify. Unliketypographicalerrorsorcertainoutliers,
theseerrorsdonotreadilyrevealthemselves,eventhroughobservationofthedataset. Logicerrorsarisewhenthe
datadoesnotadheretotheexpectedlogicalrelationshipsbetweenvariables,makingthemchallengingtodetect. To
addressthisissue,weproposeanalgorithmthat,despiteitssimplicity,effectivelyidentifiestheseelusiveerrors. Our
methodinvolvestheuseofdataminingtechniquestouncoverrelationshipsbetweenvariousvariablesinthedataset. By
identifyingandestablishingthestrongestrelationshipsastrue,wecanidentifyobservationsthatfailtoconformtothese
logicalconnections. Suchnon-conformingobservationsarethenflaggedasinconsistentwiththelogicalstructureof
thedataset. Thisapproachisusedintheworkof(A.M.Rajeswarietal.,2014)and(KarthikeyanandVembandasamy,
2015b),wheretheauthorsusedsimilartechniquesnottospecificallydetectlogicalerrorsbuttoidentifyoutliers7. This
methodensuresthattheintegrityofthedatasetismaintainedbyvalidatingthatthedataadherestotheunderlying
logicalrelationshipsexpectedwithinthedataset. AswehadalreadymentionedinSection4.2.2,unjustifiedmissingtext
valueswillalsobeflaggedinthisprocess.
Todetectrelationshipsbetweenvariables,wedecidedtousetheassociationruleminingalgorithm,Apriori(Agrawal
etal.,1994). Thealgorithmisoftenusedinmarketbasketanalysisandpurchaserecommendationsystems. Likethe
IsolationForestusedforoutliers,thisalgorithmseemslessknown,sowedescribeitsfunctioninghere:
7Inthesereferences,outliersdonotdesignatestatisticaloutlierslikeinthswork.Instead,theyindicateabnormalbehaviorofa
wholerecord.
20TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
Apriorialgorithm
LetusstartbyrecallingthemainconstituentsoftheApriorialgorithm:
• Itemset: Anitemsetisacollectionofoneormoreitems. Ak-itemsetisanitemsetcontainingkitems.
• Support: Thesupportisthefrequencyofaparticularitemset. IfanitemsetI appearsinsoutofnrecords,
thenthesupportofI iscalculatedass/n. Theminimumsupportthresholdisauser-definedthresholdthatan
itemset’ssupportmustmeetorexceedtobeconsideredfrequent. Itemsetswithsupportbelowthisthreshold
arediscardedfromfurtherconsideration.
• AssociationRule: AnassociationruleisanimplicationoftheformI →I ,whereI andI areitemsets.
1 2 1 2
TherulesuggeststhatrecordscontainingI arelikelytoalsocontainI .
1 2
• Confidence: Confidence is a measure of the reliability of an association rule. For a rule I → I , the
1 2
confidence is given by confidence(I → I ) = support(I ∪I )/support(I ). The minimum confidence
1 2 1 2 1
thresholdisauser-definedthresholdthatanassociationrule’sconfidencemustmeetorexceedtobeconsidered
strong.
ThestepsoftheApriorialgorithmwithconstituentsareasfollows:
1. Generate candidate itemsets: The algorithm starts with all individual items in the database, generating
candidate1-itemsets. Theseareevaluatedtodeterminetheirsupport.
2. Filtercandidates: Foreachcandidateitemset,thealgorithmcalculatesitssupport. Onlythoseitemsetswhose
supportmeetsorexceedstheminimumsupportthresholdareconsideredfrequentitemsets. Letusdesignate
byF ,thesetofk-itemsetskeptafterthefilter.
k
3. Generatefrequentitemsets: Usingthefrequentk-itemsetsF ,thealgorithmgeneratescandidate(k+1)-
k
itemsetsbyjoiningF withitself. Itthenfiltersthecandidatesthatdonotmeetthesupportthresholdtoform
k
thefrequent(k+1)-itemsetsF .
k+1
4. Repeat: Thisprocessisrepeatediteratively,generatingcandidateandfrequentitemsetsofincreasingsizeuntil
nonewfrequentitemsetsarefound.
5. Generateassociationrules: Fromthecollectionofallfrequentitemsets,thealgorithmgeneratespossible
associationrules. Foreachfrequentitemset,itgeneratesallpossiblerulesandcalculatestheirconfidence.
Onlytherulesthatmeettheminimumconfidencethresholdareconsideredstrongassociationrules.
Naturally,thealgorithminitsinitialversiontakesalongtimetorunasithasahightimecomplexity. Forinstance,for
eachk,thealgorithmgenerates(cid:0)n(cid:1)
candidates. Forthatreason,wedecidedtomakeachangeinthefunctioningofthe
k
algorithm. Intheoriginalversionofthealgorithm,theusercanadjusttheminimumsupport,aswellastheconfidence
levelofarule. Thehigherthesetwovaluesare,thefewerthenumberofrulesthatwillbesearched. Weaddanother
parameterthatcanbeadjusted,themaximumlengthofk-itemsets. Thisgivesustheoptionoflimitingthenumberof
elementsthatcanactuallydefinearule. Thisaffectsthenumberofrepetitionsconductedinstep4aswellasthesizes
ofthedatabeinggeneratedandanalyzedinsteps1to3ofthedescriptionabove. Thischangesavesaconsiderable
amountoftime. Beyondthecomputationalpragmatismofthischange,thismodificationisalsosupportedbythefact
thataboveacertainsizeofk-itemsets,therulebecomesunreadable,andwelosetheexplainabilitythatisatthecoreof
ourconcernsinthisarticle.
Inourresearch,weemployedtheApriorialgorithmwiththefollowingparametrization: theminimumsupportthreshold
issetat0.33%andtheconfidencelevelof99%. Thecombinationofalowminimumsupportthresholdof0.33%anda
highconfidenceintervalof99%impliesthattheApriorialgorithmwillfocusonidentifyingrarebuthighlyreliable
associationswithinthedataset. Thisconfigurationisparticularlyusefulinscenarioswherediscoveringinfrequentyet
stronglyindicativepatternsiscrucial,suchaserrorsinadataset. Bysettingalowsupportthreshold,thealgorithm
ensuresthateventhequietuncommonitemsetsareconsidered,whilethehighconfidencerequirementensuresthatonly
themostreliableandsignificantassociationsareretained. Thisapproachallowsforacomprehensiveanalysis,albeitat
thecostofincreasedcomputationalresourcesduetothelargernumberofcandidateitemsets. Themaximumlength
ofk-itemsetsislimitedat3,reducingtheimpactofthelowminimumsupportthreshold. Thisprocessyieldsasetof
rules,whicharesubsequentlyfilteredtoretainonlythosewithconfidencelevelsbelow100%. Inthisway,weidentify
thosethathavehighconfidencelevelsbutwhichinrarecasesfail: theyareourpotentiallogicerrors. Foreachofthese
identifiedrules,allthecomposingobservations(whichareasubsetofagivenrecord)aremarkedasinvalidduetologic
errors. Finally,correctionscanbemadebasedontheexpectedrulesthatwereviolateddespitethehighconfidencein
thoserules.
21TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
Complexityanalysis
Toourknowledge,theliteraturedoesnotcontainresearchthatspecificallyaddressestheissueofthecomplexityofthe
Apriorialgorithm. Here,weattemptabriefcomplexityanalysis.
• TimeComplexity
ThetimecomplexityoftheApriorialgorithmisinfluencedbythenumberofrecordsinthedatabase(n),the
averagetransactionlength(a),andthenumberoffrequentk-itemsetsgenerated,C .
k
– Foreachlevelk,thealgorithmgeneratescandidateitemsetsoflengthk+1fromfrequentitemsets
oflengthk. Intheworstcase,eachpairoffrequentk-itemsetsisconsidered(hasasupportabovethe
threshold). Thejoinoperationiscombinatorial. Thecomplexityofgeneratingcandidateitemsetsforeach
levelcanthereforebeapproximatedasO(C2)intheworstcase.
k
– Foreachcandidatek-itemset,thealgorithmneedstoscantheentiredatabasetocountitssupport.
Scanningthedatabasentimes,takesO(naC )foreachlevelk.
k
Therefore,theoveralltimecomplexitycanbeexpressedas: O(cid:0)(cid:80)a (cid:0) C2+naC (cid:1)(cid:1) . Notethatinourcase,
k=1 k k
aisboundedtothemaximallengthofk-itemsetsthatwesetat3.
• SpaceComplexity
The space complexity of the Apriori algorithm is mainly influenced by the number of candidate itemsets
storedateachlevel. Ateachlevelk,thealgorithmstorescandidatek-itemsetsandtheirsupportcounts. The
averagespacerequiredforthisisO(aC )foreachlevel. Therefore,theoverallspacecomplexitycanhencebe
k
approximatedas:
O((cid:80)a
aC ).
k=1 k
5 Results
Werecallthatforthequalityenhancementphase,wehaveatourdisposal(again,withoutusingthem)theexactpositions
oftheinvalidentries(seeTable2). Thisallowsustocomputetheexactaccuracieswehaveachieved. However,forthe
pre-qualityenhancementphase,specificallyfortheprimarykey,thisisnotthecase.
5.1 Pre-QualityenhancementResults
5.1.1 Identificationofprimarykeys
WeappliedthestepsdescribedinSubsection4.1.1.
• Step1oftheprocedureconsistedinidentifying,columnswithlowratesofmissingvalues. Thisnarrowed
downtheselectiontoonlythetwelveoutoffifty-threefieldsthatarelistedintable3.
• Then,instep2,identifyingfieldswithspecificexpressionsintheirnames(ID,CODE,orKEY)wouldleaveus
withthreefields. Therefore,weneedtoproceedtostep2.b,whichinvolvesfindingcombinationsthatallowto
identifyuniquevalues.
• Instep2.b,itisquicklydeterminedthatforthesubsetK =(SalesID,ModelID),onlyabout0.1%ofvalue
rowsareduplicated. Consequently,thissubsetisidentifiedastheprimarykeyandwillbeutilizedforthe
subsequentstepsoftheframework.
22TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
Field VariableType Proportionofmissingvalues(%)
auctioneerID INT64 0.000
Enclosure STR 0.025
fiBaseModel STR 0.000
fiModelDesc STR 0.000
fiProductClassDesc STR 0.000
ModelID INT64 0.000
ProductGroupDesc STR 0.000
SalesID FLOAT64 0.047
saledate STR 0.000
state STR 0.000
YearMade INT64 0.000
datasource INT64 0.000
Table3: Candidatesfortheprimarykey. Thelistedfieldshavelessthan5%ofmissingvalues,thefieldsinredare
potentialcandidatesduetotheirnamesandarethefirsttestedintheuniquenessanalysis,thefieldsshadedinblueare
theonesfinallychosenastheprimarykey.
Thewholeprocessisexecutedunder1second. Thisefficientexecutioncanbeattributedtothefactthatweinitiated
step2.bwithattributesidentifiedduringthepatternrecognitionstep. Alsonotethatinadataset,primarykeysarenot
necessarilyauniquesubset.
5.1.2 Processesmappedtospecificfields
AfterapplyingthefiltersintroducedinSubsection4.1.2,eachofthealgorithmsoftheframeworkwillbeappliedtothe
followingfields.
• Redundancies
Theredundancyanalysiswillbeconductedontheattributesthatcomprisetheprimarykey,namelySalesID
andModelID.
• Absences
Allfieldswillbescreenedformissingvalues.
• Statisticaloutliers
Theanalysisofoutlierswillonlybeappliedtothreefields: SalePrice,YearMade,andMachineHoursCurrent-
Meter.
• Typographicalerrors
Afterapplyingthefilter,onlythreeoutoffifty-threefieldswillbeanalyzedfortypographicalerrors: state,
Enclosure,andProductGroupDesc.
• Logicerrors
Finally,evenafterthefilteringprocess,notasmanyfieldsareexcludedfromtheanalysiscomparedtothe
otherstepsoftheframework. Fourteenfieldsremain: Enclosure,Ripper,fiBaseModel,Drive_System,state,
Transmission,ProductGroupDesc,fiProductClassDesc,Pad_Type,fiSecondaryDesc,saledate,ProductSize,
Hydraulics,andfiModelDesc.
5.2 Qualityenhancementresults
Overall,theframeworkweintroduceproducesreasonablygoodresults. Thedifficultyofourworkliesinfindingthe
rightbalancebetweenperformanceandexplainability. Anyinvaliddataidentifiedisfullyexplainableandisnatively
corrected. Theaccuracyforstatisticaloutliersandlogicerrors,couldbeimprovedwithreasonableeaseifwewereless
exigentregardingtheexplainabilityoftheresultsandthealgorithms.
Intermsofcomputerperformance8,withtheexceptionofthelogicerrorsdetectionalgorithm,allthealgorithmsrunin
amatterofseconds. Handlingduplicatestakes1second,missingvaluesaredealtwithinlessthan1second,outliersare
identifiedandimputedin3seconds,andtypographicalerrorsarehandledin33seconds. Thelogicerroridentification
took45minutes.
8Theconfigurationofthecomputerusedisrathermoderate:Intel®Core™i5-7200Uwith2.5GHzbasefrequency.
23TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
RedundanciesandAbsences
100%ofduplicatesand100%ofmissingvaluesweresuccessfullyidentified. Thistaskdoesnotrepresentachallenge
perse. However,itisinterestingtonotethatthesimpleruleofthumbusedtodesignateunjustifiedmissingnumerical
values(95%threshold)isefficient.
StatisticaloutliersandLogicErrors
Only 51% of statistical outliers were identified. As we mentioned in Section 1, detecting statistical outliers while
maintainingmaximalexplainabilitypresentsseveralchallenges. ThesolutionweintroducedinSubsection4.2.3allows
forfullyexplainableidentification. Foranyflaggedvariable,itispossibletoexplainexactlywhythevariablewas
considered an outlier. The variable is either too extreme in a non-extreme distribution, or it is isolated from other
extremevaluesinadistributionthathasvalidextremevalues. Theissuewiththisapproachisthatitdoesnotconsider
thesurroundingvariables. Forinstance,whenanalyzingstatisticaloutliersinadatasetwithfieldsrepresentingprices
andproducts,itisverychallengingtodosowithoutconsideringthespecificproducts. Therearemanyalgorithmsthat
couldeffectivelyconsiderallthesurroundinginformation(typicallyanIsolationForestinhighdimensions),butthey
wouldnotbeabletoflagthespecificfieldwithanissue(thereforefailingpoint1.(c)ofourdefinitionofexplainability
andinterpretability)orwouldsomehownotbeabletoguaranteeexplainabilityandinterpretability. Asolutioncouldbe
tohandleoutliersaslogicerrors. Thissolutionwasconsideredbutwasdiscardedduetotheheavycomputationalcostit
wouldimply. Indeed,itwouldmechanicallyincreasethenumberoffieldstoconsiderforlogicerrors,anditwould
requiretransformingnumericalvaluesintocategoricalones(forinstance,pricesfrom1to100,then100to500,andso
oninsteadofusingnumericalvalues).
Only35%oflogicerrorsareidentified. Thisresultisattributabletotheotherconstraintsetinthiswork,whichisnot
usingdomainknowledge. LetusrecallthelogicerrorsinthedatasetintroducedinTable2. WehadWrongcategory,
Incoherentmachine/Drivesystem/Productgroupdescription,andYearMade>saledate. Apartfromthelastcase,all
thesetypesoferrorscouldbedirectlyidentifiedusingdomainknowledge. Specifically,inourcase,byusingthespecific
anduniquemappingthatdescribestherelationshipfromonemachine(MachineID)toitsclass(ProductClassDesc)with
allspecificationsin-between(Forks,Transmissions,...). Asolutiontokeeptheframeworkdomainknowledge-free
whilebeingmoreaccuratewouldbetoaddathirdstepinthepre-qualityenhancementphasethatwouldtrytoestablish
dependencyrelationshipsbetweeneachfieldofthedatasetandthenfocusanintensiveApriorianalysisbetweenthese
variableswithoutlimitingthemaximallengthofk-itemsets. Todeterminethedependencyrelationships,solutions
thatcouldbeexploredinclude,butarenotlimitedto,randomforestfeatureimportance(Breiman,2001)andANOVA
analysis(Stetal.,1989). Additionally,itwouldbehighlybeneficialtotransformnumericalvariablesintomeaningful
categoricalvariablestoincludetheminthelogicerrordetections. Asstatedabove,thiswouldalsobeadvantageousfor
thedetectionofstatisticaloutliers. Deepeningtheresearchinthementioneddirectionswouldallowformoreaccurate
detectionandcorrectionoflogicerrorswhileensuringexplainabilityandinterpretability. However,thiswouldcomeat
aheftycomputationalprice. Itwouldbeadvisabletoconducttheseevolutionsalongwithparallelizedimplementation
ofthealgorithms.
Typographicalerrors
100%oftypographicalerrorsareidentified.Thisisaverygoodresultaswehavesuccessfullyidentifiedalltypographical
errors, includingentryerrorsaswellascaseerrors. WehaveanalgorithmthatcombinesclusteringandDamerau-
Levenshteindistanceinaninterestingyetnotcomplicatedway,achievingremarkableaccuracyindetectingtypographical
errors. Thealgorithmismadefeasiblebythesimpletrickofsortingthewordstobecorrectedalphabetically,which
considerablyreducesprocessing(byachievingaverygoodtimecomplexity,closetoO(nlogn),asshowninSubsection
4.2.4). Forexample,forthevariablestate,westartwith102uniqueobservationsamongthe100,000initialwords. We
endupwith63groupsresultingfromtheDLSjumps,whichcorrespondstoareductionofaround60%inthematrix
sizeofDLSs. Anotherverygoodresultobtainedisthatnofalsepositivesareproducedbythealgorithm.
6 Conclusion
Wehaveintroducedaframeworkthatmeasuresthequalityofadatasetwithoutanyuseofdomainknowledge,while
maintainingtheexplainabilityandinterpretabilityofourresults. Ourframeworkisindeedabletoidentifyandcorrect
errorsinadatasetwithoutusinganyinformationbeyondtheprovideddataset,whilebeingabletofullyjustifywhyeach
flaggedobservationisconsideredinvalidandproposecorrections. Theframeworkhandlesmissingvalues,duplicates,
outliers,typographicalerrors,andlogicerrors. Afterthoroughlyreviewingtheliteratureandanalyzingourframework
indetail,wecanseethatourworkdistinguishesitselffromtheliteraturebyadheringtotwoconstraints(guaranteeing
explainabilityandinterpretabilityandbeingdomainknowledge-free),bythesimplicityofthealgorithmsused,andby
thewaythealgorithmscombinemachinelearning,statistics,andsomecommon-sensetricks.
24TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
Theanalysisoftheresultswehaveconductedallowsustodrawtwomainconclusions. First,theconceptofanalyzing
thequalityofadatasetwithoutanyinformationaboutthedatawilloftencomeattheexpenseofcomputingperformance.
Forinstance,theneedtostarttheframeworkwithapre-qualityenhancementphasetosearchforinformationthatwould
havebeenobtainedinaclassicalsituationisanadditionalcomputationalcost. Anotherillustrationcanbefoundinthe
logicerrordetectionalgorithm,wheredomainknowledgewouldexpeditetheassociationruleminingalgorithmasthe
relatedtargetswouldbemanuallyselected,thusallowingfortargetedminingofstrongrelationships. Thiswouldalso
considerablyincreasetheaccuracyofthealgorithm. Secondly,theaimofbeingentirelyexplainableandinterpretable
willoftenreducetheaccuracyofthealgorithmsused. Indeed,thisaimsystematicallyinvolvessteppingawayfrom
powerfultools,typicallysomedeeplearning(DL)andmachinelearning(ML)algorithms. Agoodillustrationisour
choiceofusingtheisolationforestalgorithmonlyinonedimension.
Evenwiththesestrictconstraints,wehavealsoshownthatitisentirelypossibletoobtainverygoodresults. Indeed,
apartfromoutliersandlogicerrors,allothertypesoferrorshavebeenfullyidentifiedandcorrected. Typically,the
algorithm to identify and correct typographical errors performs very well with a very good complexity. Another
conclusionthatcanbedrawnfromthisexploratoryworkisthat,inasetupconstrainedasours,itisnecessarytohave
well-definedpre-analysisstepsthatwilltargettheanalysisandreducetheprocessingtime.
Toextendtheworkwehaveconducted,manyinterestinganalysescouldbeundertaken. First,wecouldconsider,as
alreadymentioned,analyzinginmoredepththerelationshipsbetweentheattributes. Thiswouldnotonlyhelpreduce
computationalcostsbutalsoprovideabetterunderstandingofthedatabaseweareworkingon,allowingforanalysis
beyondthisframework. Additionally, itcouldbebeneficialtoconsiderconfidencelevelsonthecorrectionsmade.
Forinstance,whenhandlingoutliers,wecouldrunthealgorithmmultipletimeswithdifferentparameters,especially
α , α ,andγ. Basedonthedifferentresults,wecouldexplorethepossibilityofcomputingaconfidencelevelonthe
s k
identificationsmade.
25TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
References
Agrawal,R.,Srikant,R.,etal.(1994). Fastalgorithmsforminingassociationrules. InProc.20thint.conf.verylarge
databases,VLDB,volume1215,pages487–499.Santiago.
A.M.Rajeswari,M.Sridevi,andC.Deisy(2014). Outliersdetectiononeducationaldatausingfuzzyassociationrule
mining.
Aydilek,I.andArslan,A.(2013). Ahybridmethodforimputationofmissingvaluesusingoptimizedfuzzyc-means
withsupportvectorregressionandageneticalgorithm. Inf.Sci.,233:25–35.
Barnett,V.andLewis,T.(1994). OutliersinStatisticalData,volume3. Wiley,NewYork,NY,USA.
Bassil,Y.andAlwani,M.(2012). Context-sensitivespellingcorrectionusinggoogleweb1t5-graminformation. arXiv
preprintarXiv:1204.5852.
Boytsov,L.(2011). Indexingmethodsforapproximatedictionarysearching. JournalofExperimentalAlgorithmics.
Breiman,L.(2001). Randomforests. Machinelearning,45:5–32.
Çelik,M.,Dadas¸er-Çelik,F.,andDokuz,A.S¸.(2011). Anomalydetectionintemperaturedatausingdbscanalgorithm.
In2011internationalsymposiumoninnovationsinintelligentsystemsandapplications,pages91–95.IEEE.
Corrales, D. C., Ledezma, A., and Corrales, J. C. (2018). From theory to practice: A data quality framework for
classificationtasks. Symmetry,10(7):248.
Cousineau,D.andChartier,S.(2010). Outliersdetectionandtreatment: Areview. InternationalJournalofPsychologi-
calResearch,3(1):58–67.
Dai,W.,Yoshigoe,K.,andParsley,W.(2018). Improvingdataqualitythroughdeeplearningandstatisticalmodels.
InInformationTechnology-NewGenerations: 14thInternationalConferenceonInformationTechnology, pages
515–522.Springer.
Damerau,F.(1964). Atechniqueforcomputerdetectionandcorrectionofspellingerrors. CommunACM,7(3):171–176.
Doshi-Velez,F.andKim,B.(2018). Considerationsforevaluationandgeneralizationininterpretablemachinelearning.
Explainableandinterpretablemodelsincomputervisionandmachinelearning,pages3–17.
Escalante,H.J.,Escalera,S.,Guyon,I.,Baró,X.,Güçlütürk,Y.,Güçlü,U.,andvanGerven,M.(2018). Explainable
andInterpretableModelsinComputerVisionandMachineLearning. Springer.
García,S.,Ramírez-Gallego,S.,Luengo,J.,Benítez,J.M.,andHerrera,F.(2016). Bigdatapreprocessing: methods
andprospects. Bigdataanalytics,1:1–22.
Han,J.,Kamber,M.,andPei,J.(2012). Dataminingconceptsandtechniquesthirdedition. UniversityofIllinoisat
Urbana-ChampaignMichelineKamberJianPeiSimonFraserUniversity.
Han,J.,Pei,J.,andTong,H.(2022). Datamining: conceptsandtechniques. Morgankaufmann.
Howell,D.(1998). Statisticalmethodsinhumansciences. Wadsworth,NewYork.
Huang,Y.,Murphey,Y.L.,andGe,Y.(2013). Automotivediagnosistypocorrectionusingdomainknowledgeand
machinelearning. In2013IEEEsymposiumoncomputationalintelligenceanddatamining(CIDM),pages267–274.
IEEE.
Jaro,M.(1989). Advancesinrecord-linkagemethodologyasappliedtomatchingthe1985censusoftampa,florida. J
AmStatAssoc,84(406):414–420.
Johnson,R.andWichern,D.(2014). AppliedMultivariateStatisticalAnalysis,volume4. Hall,UpperSaddleRiver,NJ,
USA.
Kachuee, M., Karkkainen, K., Goldstein, O., Darabi, S., and Sarrafzadeh, M. (2020). Generative imputation and
stochasticprediction. IEEETransactionsonPatternAnalysisandMachineIntelligence,44(3):1278–1288.
Karami,A.andJohansson,R.(2014). Choosingdbscanparametersautomaticallyusingdifferentialevolution. Interna-
tionalJournalofComputerApplications,91(7):1–11.
Karthikeyan,T.andVembandasamy,K.(2015a). Anovelalgorithmtodiagnosistypeiidiabetesmellitusbasedon
associationruleminingusingmpso-lssvmwithoutlierdetectionmethod. IndianJournalofScienceandTechnology,
8(S8):310–320.
Karthikeyan,T.andVembandasamy,K.(2015b). Anovelalgorithmtodiagnosistypeiidiabetesmellitusbasedon
associationruleminingusingmpso-lssvmwithoutlierdetectionmethod. IndianJournalofScienceandTechnology,
8(S8).
Keil,F.C.(2006). Explanationandunderstanding. Annu.Rev.Psychol.,57:227–254.
26TowardsExplainableAutomatedDataQualityEnhancementwithoutDomainKnowledge
Köhler,H.,Link,S.,andZhou,X.(2015). Possibleandcertainsqlkey. PVLDB,8(12):1978–1989.
Le,V.B.T.,Link,S.,andMemari,M.(2012). Schema-anddata-drivendiscoveryofsqlkeys. JournalofComputing
ScienceandEngineering,6(3):193–204.
Lee,J.-H.,Kim,M.,andKwon,H.-C.(2020). Deeplearning-basedcontext-sensitivespellingtypingerrorcorrection.
IEEEAccess,8:152565–152578.
Levenshtein, V. (1966). Binary codes capable of correcting deletions, insertions, and reversals. Sov Phys Dokl,
10(8):707–710.
Leys,C.,Ley,C.,Klein,O.,Bernard,P.,andLicata,L.(2013). Detectingoutliers: Donotusestandarddeviationaround
themean,useabsolutedeviationaroundthemedian. JournalofExperimentalSocialPsychology,49(4):764–766.
Liu,F.T.,Ting,K.M.,andZhou,Z.-H.(2008). Isolationforest. In2008,eighthieeeinternationalconferenceondata
mining.
Maier,D.(1978). Thecomplexityofsomeproblemsonsubsequencesandsupersequences. JACM,25(2):322–336.
Marr,B.(2016). Bigdatainpractice: How45successfulcompaniesusedbigdataanalyticstodeliverextraordinary
results. JohnWiley&Sons.
McKinney,W.(2010). Datastructuresforstatisticalcomputinginpython. InProceedingsofthe9thPythoninScience
Conference,volume445,pages51–56.Austin,TX.
Miller,J.(1991). Reactiontimeanalysiswithoutlierexclusion: Biasvarieswithsamplesize. TheQuarterlyJournalof
ExperimentalPsychology,43(4):907–912.
Morris,R.andCherry,L.L.(1975). Computerdetectionoftypographicalerrors. IEEETransactionsonProfessional
Communication,(1):54–56.
Müllner,D.(2011). Modernhierarchical,agglomerativeclusteringalgorithms. arXivpreprintarXiv:1109.2378.
Pandya,U.,Mistry,V.,Rathwa,A.,Kachroo,H.,andJivani,A.(2020). 2dbscanwithlocaloutlierdetection. InAmbient
CommunicationsandComputerSystems: RACCCS2019,pages255–263.Springer.
Pyle,D.(1999). Datapreparationfordatamining. MorganKaufmann.
Ras,G.,vanGerven,M.,andHaselager,P.(2018). Explanationmethodsindeeplearning: Users,values,concernsand
challenges.
Rezapour,M.(2019). Anomalydetectionusingunsupervisedmethods: creditcardfraudcasestudy. International
JournalofAdvancedComputerScienceandApplications,10(11).
Robinson,D.(2003). AnIntroductiontoAbstractAlgebra. WalterdeGruyter,Berlin.
Sainani,K.L.(2015). Dealingwithmissingdata. Pm&r,7(9):990–994.
Savarimuthu,N.andKaresiddaiah,S.(2021). Anunsupervisedneuralnetworkapproachforimputationofmissing
valuesinunivariatetimeseriesdata. ConcurrencyandComputation: Practiceandexperience,33(9):e6156.
Schelter,S.,Lange,D.,Schmidt,P.,Celikel,M.,Biessmann,F.,andGrafberger,A.(2018). Automatinglarge-scaledata
qualityverification. ProceedingsoftheVLDBEndowment,11(12):1781–1794.
St, L., Wold, S., et al. (1989). Analysis of variance (anova). Chemometrics and intelligent laboratory systems,
6(4):259–272.
Thang,T.M.andKim,J.(2011). Theanomalydetectionbyusingdbscanclusteringwithmultipleparameters. In2011
InternationalConferenceonInformationScienceandApplications,pages1–5.IEEE.
Tibshirani,R.,Walther,G.,andHastie,T.(2001). Estimatingthenumberofclustersinadatasetviathegapstatistic.
JournaloftheRoyalStatisticalSociety: SeriesB(StatisticalMethodology),63(2):411–423.
Xu, H., Chen, W., Zhao,N., Li, Z., Bu, J., Li, Z., Liu, Y., Zhao, Y., Pei, D., Feng,Y., etal.(2018). Unsupervised
anomalydetectionviavariationalauto-encoderforseasonalkpisinwebapplications. InProceedingsofthe2018
worldwidewebconference,pages187–196.
Zaki, M. J. and Meira, W. (2014). Data mining and analysis: fundamental concepts and algorithms. Cambridge
UniversityPress.
Zhou,P.,Li,M.,Huang,J.,andFang,H.(2014). Researchondatabaseschemacomparisonofrelationaldatabasesand
key-valuestores. AdvancedMaterialsResearch,1049-1050:1860–1863.
27