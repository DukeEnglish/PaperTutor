Kolmogorov-Arnold Networks in Low-Data Regimes: A
Comparative Study with Multilayer Perceptrons
Farhad Pourkamali-Anaraki
Department of Mathematical and Statistical Sciences, University of Colorado Denver,
CO, USA
Abstract
Multilayer Perceptrons (MLPs) have long been a cornerstone in deep learning, known
for their capacity to model complex relationships. Recently, Kolmogorov-Arnold Networks
(KANs)haveemergedasacompellingalternative,utilizinghighlyflexiblelearnableactivation
functions directly on network edges, a departure from the neuron-centric approach of MLPs.
However, KANs significantly increase the number of learnable parameters, raising concerns
about their effectiveness in data-scarce environments. This paper presents a comprehensive
comparative study of MLPs and KANs from both algorithmic and experimental perspectives,
with a focus on low-data regimes. We introduce an effective technique for designing MLPs
with unique, parameterized activation functions for each neuron, enabling a more balanced
comparison with KANs. Using empirical evaluations on simulated data and two real-world
data sets from medicine and engineering, we explore the trade-offs between model complexity
and accuracy, with particular attention to the role of network depth. Our findings show that
MLPswithindividualizedactivationfunctionsachievesignificantlyhigherpredictiveaccuracy
with only a modest increase in parameters, especially when the sample size is limited to
around one hundred. For example, in a three-class classification problem within additive
manufacturing, MLPs achieve a median accuracy of 0.91, significantly outperforming KANs,
whichonlyreachamedianaccuracyof0.53withdefaulthyperparameters. Theseresultsoffer
valuable insights into the impact of activation function selection in neural networks.
1 Introduction
MultilayerPerceptrons(MLPs)areessentialtomoderndeeplearningduetotheirabilitytomodel
intricate nonlinear relationships [1]. MLPs consist of interconnected layers of neurons, forming
a network where information flows from input to output. The connections between neurons
in adjacent layers are represented by edges, each associated with a weight that determines the
strength of the signal passed between them. Within each hidden layer, every neuron performs
two key tasks. The first one is to calculate a weighted sum of its inputs, aggregating the signals
received from the neurons in the previous layer. The second task involves applying a nonlinear
activation function to this weighted sum. Without the nonlinear transformations provided by
activation functions, neural networks would be limited to simple linear relationships, severely
hindering their ability to extract complex patterns [2].
Traditionally, MLPs relied on fixed activation functions such as the Rectified Linear Unit
(ReLU) [3] or the hyperbolic tangent (tanh). Training in this paradigm focuses mainly on adjust-
ing edge weights to reduce error, keeping the activation functions constant. Recently, there has
been a growing interest in parameterized activation functions, which offer greater control during
the learning process [4, 5]. These functions contain adjustable or learnable parameters, allowing
the network to fine-tune its nonlinearities as it trains. This enhanced flexibility can improve
1
4202
peS
61
]GL.sc[
1v36401.9042:viXraperformance by tailoring the model’s internal transformations to the unique characteristics of the
data.
In the pursuit of adaptive activation functions, significant research has focused on parame-
terizing functions that build upon ReLU, allowing for more nuanced handling of negative inputs.
For instance, Leaky ReLU introduces a slight linear slope for negative values [6], while the Expo-
nential Linear Unit (ELU) employs an exponential function in this region [7]. Additionally, the
Sigmoid Linear Unit (SiLU), also known as the swish function [8, 9, 10], has garnered attention
for its smooth, non-step-like behavior and its ability to approximate both linear and ReLU char-
acteristics depending on its parameter. Several empirical studies have demonstrated the superior
performance of learnable activation functions in large-scale problems involving tens of thousands
of samples or more [11].
Recently, Kolmogorov-Arnold Networks (KANs) have been introduced as a novel neural net-
work architecture [12]. Unlike traditional MLPs, KANs place learnable activation functions on
the connections or edges between neurons, rather than within the neurons themselves. This de-
sign is rooted in the mathematical principle that multivariate functions can be decomposed into
simplerunivariateonesusingsums[13]. Figure1illustratesthestructuralcontrastbetweenMLPs
and KANs. In MLPs, each neuron performs both summation of weighted inputs and applies a
nonlinear activation function, while in KANs, nonlinearity is introduced through the edges of the
network themselves.
Figure 1: Illustrating the structural differences between Multilayer Perceptrons (MLPs) and
Kolmogorov-Arnold Networks (KANs), highlighting that nonlinearities are introduced via the
edges or connections between neurons in KANs.
The recent KAN research [12] introduces the concept of a KAN layer, effectively stacking
the neurons depicted in Figure 1(b), and provides an implementation within a widely-used deep
learning framework. This allows for the creation of arbitrarily deep networks through automatic
differentiation. In practice, KANs often parameterize their edge activation functions using a
combination of a SiLU function and a spline function. Training a KAN then primarily focuses on
learning the optimal coefficients for these local B-spline basis functions.
While KANs are designed to utilize more expressive activation functions than standard MLPs
with ReLU-like learnable activations, this expressivity comes at the cost of increased parameter
2count. This raises concerns about their performance in data-sparse scientific and engineering
domains, where small sample sizes can hinder effective training. In many fields, such as medicine
and engineering, data collection is often limited to a few tens or hundreds of samples due to high
costs and time-consuming procedures [14, 15].
Hence, the primary goal of this paper is to provide a comprehensive comparison between
MLPs and KANs in low-data scenarios with a few hundred samples. Our key contributions are
listed in the following.
• Enabling Fair Comparison: Most deep learning libraries implement MLPs with the same
activation function applied to all neurons within a layer. Even when using learnable activa-
tion functions, neurons in the same hidden layer typically share the same set of parameters.
To ensure a fair comparison with KANs, we present a straightforward yet effective tech-
nique for designing and implementing MLPs where each neuron in a hidden layer has its
own distinct, parameterized activation function. This approach can be applied to networks
of any depth or width. Furthermore, we implement a parameterized version of the SiLU
activation function to ensure that the activation functions used in MLPs are comparable to
those used in KANs.
• Mathematical Connections: We present a mathematical analysis that elucidates the rela-
tionshipbetweenMLPsandKANs, showingthatKANscanessentiallybeconsideredMLPs
with activation functions possessing greater flexibility. This analysis underscores the im-
portance of explicitly comparing the number of learnable parameters for both MLPs and
KANs in our empirical study.
• Empirical Evaluation: We conduct experiments on a simulated data set and two real-world
classification problems (cancer detection and 3D printer type prediction) to investigate
the trade-offs between model complexity (parameter count) and accuracy in data-limited
settings. As a key feature of recently introduced KANs is their ability to stack multiple
KANlayers, wespecificallyexaminetheimpactofnetworkdepthontestingaccuracyacross
variousdatasplits. Additionally,ourexperimentsrevealthatthepiecewisepolynomialorder
of splines significantly influences the performance of KANs.
The remainder of this paper is organized as follows. In Section 2, we provide a concise math-
ematical introduction to MLPs, detail our modification to design MLPs with individual learnable
activation functions for each neuron in hidden layers, and explain the underlying transformation
in KANs, along with connections to MLPs with trainable activation functions. In Section 3, we
report experiments on simulated data sets to understand the impact of data size on the predictive
accuracy of MLPs and KANs. In Section 4, we present numerical experiments using real-world
data representing complex problems in medicine and engineering. Finally, we conclude this paper
with remarks in Section 5.
2 Foundations
Awidelyadoptedneuralnetworkarchitecturefortasksinvolvingstructuredortabulardataisthe
Multilayer Perceptron (MLP). These networks are composed of sequentially arranged layers with
dense interconnections, providing an effective mechanism for learning multivariate functions. To
be formal, imagine a network with L hidden layers between the input and output layers. The
l-th layer consists of N neurons. Also, assume that this network takes an input vector x ∈ RD,
l
where D is the number of input features. Moreover, the weight matrix and the bias vector can
3be written as W(l) ∈ RN l×N l−1 and b(l) ∈ RN l, for each layer indexed by l = 1,...,L+1. The
predicted output f(x) is then defined from the input x according to the following equations:
x(0) = x,
x(l) = g(l)(cid:0) W(l)x(l−1)+b(l)(cid:1) , l = 1,...,L,
f(x) =
g(L+1)(cid:0) W(L+1)x(L)+b(L+1)(cid:1)
. (1)
Hence, f(x) takes on a composite or nested form. The number of units in the output layer N
L+1
andthecorrespondingactivationfunctiong(L+1) dependontheproblemathand. Forexample, in
binaryclassificationproblems, asingleneuronistypicallyplaced, i.e., N = 1, andtheSigmoid
L+1
activation function is used to find the probability that the data point x belongs to one of the two
classes [16]. However, when treating classification problems of more than two classes, the last
layer contains one neuron for each class. In this case, we employ the Softmax function to find the
categorical distribution for all classes. That is, if we have C classes, then the Softmax function
accepts a set of C real-valued numbers z ,...,z and converts them into a valid probability
1 C
distribution by returning ezc/(cid:80) c′ez c′, for c = 1,...,C [17].
In contrast, we have more flexibility when choosing activation functions for the L hidden
layers, as they produce latent representations. Note that in the equation above, the activation
function is applied element-wise. Consequently, the standard implementation of dense layers in
popular deep learning libraries like TensorFlow/Keras applies the same activation function to all
neurons within a given layer. In other words, every neuron in a layer employs the same nonlinear
function to transmit information to the subsequent layer.
One of the most popular activation functions is the Rectified Linear Unit (ReLU), defined as
ReLU(z) = max{0,z}. There are other variants built upon ReLU, such as the Sigmoid Linear
Unit (SiLU) [8], also known as the swish function (see [18, 19] for a comprehensive list). A key
advantage of SiLU(z;β) = z/(1+e−βz) is its smooth, non-step-like behavior. Its behavior can
transition between linear (when β = 0) and ReLU-like (when β is sufficiently large). In many
cases, such choices, like the value of β, are predefined hyperparameters and not optimized during
training.
2.1 MLPs with Adaptive Activation Functions
One approach to enhancing the performance and adaptability of MLPs is to treat the parameters
within activation functions as trainable during the learning process. This involves incorporating
these parameters into the computation graph, allowing the calculation of gradients of the loss
function with respect to them. As a result, these activation function parameters, such as the
optimal value of β for SiLU, can be learned alongside the network weights.
While several empirical studies [11, 20, 21] have demonstrated the advantages of adaptive ac-
tivation functions over fixed ones in MLPs, particularly in large-scale data settings like computer
vision, it is common to assume that all neurons within a layer share the same activation function
parameters. In other words, in Equation (1), g(l) is identical for all N neurons in the l-th layer.
l
However, for a fair comparison with KANs, where each connection has its own unique activa-
tion function, it is crucial to allow each activation function in MLPs to operate independently,
increasing their expressivity.
To achieve this goal, we utilize the built-in concatenation layer available in deep learning
libraries,enablingustoseamlesslymergetheoutputsofneuronswithdistinctactivationfunctions.
Let W(l) denote the i-th row of the weight matrix W(l), and let b(l) represent the i-th element of
i i
b(l). Then, each neuron in the l-th layer computes its output as follows:
a(l)
=
g(l)(cid:0) W(l) x(l−1)+b(l)(cid:1)
, i = 1,...,N , (2)
i i i i l
4(l)
where g is the activation function specific to the i-th neuron, with its own learnable parameter
i
β . The outputs from all N neurons are then concatenated to form the final output of the l-th
i l
hidden layer:
x(l) = concatenate(cid:0) [a(l) ,...,a(l) ](cid:1) . (3)
1 N
l
This approach offers significant flexibility, allowing us to design and implement MLPs of any
depth and width. Each neuron has its own unique activation function, and the entire process
leverages standard automatic differentiation within existing deep learning libraries.
2.2 Kolmogorov-Arnold Networks (KANs)
KANs depart from the traditional MLP architecture by placing learnable functions directly on
the network’s edges (connections between neurons), rather than within the neurons themselves.
Consequently, the primary role of each neuron is to simply sum its incoming signals, without
applyinganyadditionalnonlinearities. Thisdesignaimstointegratethenonlineartransformation
directly into the weighting mechanism of the edges, rather than separating linear weighting and
nonlinear activation as in conventional MLPs.
To formalize this, let us describe the transformation performed by the l-th layer of a KAN,
drawing parallels to Equation (1). Recall that the incoming signal x(l−1) has N dimensions
l−1
(l)
and the output of the l-th layer has N elements. Let g (·) denote a univariate function that
l j,i
operates on the i-th dimension of x(l−1) to be used for calculating the j-th element of the output.
Then, we have the following transformation performed by the l-th KAN layer:
 
(l) (l)
g (·) ... g (·)
1,1 1,N
l−1
 (l) (l) 
g (·) ... g (·)
x(l) =  2,1 2,N l−1 x(l−1). (4)
 . . . . 
 . . 
 
(l) (l)
g (·) ... g (·)
N,1 N,N
l l l−1
Consequently, the transformation performed by each layer remains expressible as a matrix-vector
multiplication. However, a crucial aspect of designing KANs lies in the selection of activation
functions. For simplicity, let us omit all superscripts and subscripts in the following discussion.
It has been proposed that each activation function in Equation (4) can be represented as the
weighted sum of SiLU and a spline function:
g(x) = w SiLU(x)+w spline(x), (5)
b s
where SiLU(x) = x/(1 + e−x) represents the Sigmoid Linear Unit function with β = 1 and
(cid:80)
spline(x) = c B (x) is a linear combination of B-splines [22, 23]. Thus, the training process
i i i
involves learning the optimal values of c , w , and w . This approach enables high expressivity
i b s
by utilizing rich activation functions that go beyond the popular ReLU function typically used
in MLPs. Moreover, both the degree of each spline (spline order) and the number of splines used
for each function are hyperparameters of the KAN architecture.
2.3 Bridging MLPs and KANs
While KANs clearly utilize activation functions with greater flexibility than standard MLPs
employing functions like SiLU, it is worth questioning whether the KAN architecture is truly
novel. Comparing Equations (1) and (4) reveals that layer transformations in both architectures
can be expressed as matrix-vector multiplications. However, a key distinction lies in the order of
5operations. MLPs first compute a weighted sum of inputs from the previous layer, followed by a
nonlinear activation function. In contrast, KANs apply nonlinear transformations to the inputs
first, then perform a weighted sum.
To explain this, we rewrite Equation (5):
 
SiLU(x)
g(x) = (cid:2) w w c w c
...(cid:3)

B 1(x) 
. (6)
b s 1 s 2  B 2(x) 
 
.
.
.
Therefore,theactivationfunctionusedonKANedgescanbeviewedasanonlineartransformation
followed by a weighted sum. This is similar to MLPs, except for the order of these operations.
Fromapracticalstandpoint,thecriticalquestioniswhetherthisdifferenceinordering,alongwith
the increased number of trainable parameters in KANs, leads to higher accuracy levels compared
to MLPs with trainable activation functions, particularly in low-data regimes. The remainder of
this paper will focus on investigating this question.
3 Performance Evaluation: Synthetic Data
In this section, we commence our comparative study between MLPs and KANs using a two-
dimensional simulated data set. This data set encompasses two classes, each consisting of two
clusters. Our objective is to evaluate their performance on a separable yet relatively complex
data set. Figure 2 illustrates the two data sets central to our analysis: data set A (1,000 samples)
and data set B (100 samples). This enables us to directly understand the impact of data size on
the performance of KANs, which possess higher degrees of freedom due to the choice of activation
functions; see Equation (5). For consistency, the default values of grid size 3 and spline order 3
are employed for KANs in this paper, with the exception of Section 4.3 where we investigate the
impact of hyperparameter adjustments.
Figure 2: Visualization of the two simulated data sets with varying sample sizes (1,000 vs. 100
data points).
Furthermore, toensureafaircomparison, weutilizetheparameterizedSiLUactivationforthe
MLP architecture with the learnable parameter β. As previously discussed in Section 2.1, each
neuron in this MLP architecture possesses its own set of trainable parameters, achieved through
6the concatenation process. For model fitting, we consistently use 20 epochs and a fixed learning
rate of 0.05 across all experiments in this paper.
OurcomparativeanalysisfocusesonvaryingnetworkdepthLwhilemaintainingafixedwidth
(number of neurons per hidden layer) to ensure a compact network for evaluating performance in
low-data regimes and align with the KAN paper’s focus on the KAN layer’s stacking potential for
deeper models. All experiments use a width of 2 and consider depths of 1, 2, and 3, employing
a 70-30 train-test split and measuring accuracy on the testing set, repeated 25 times to account
for the impact of data splits and weight initializations.
For our visualizations, we select violin plots, which provide a richer understanding of the data
distribution compared to traditional box plots. The width of each violin at a particular value
reflects the concentration of data points around that value, while the vertical axis represents the
probability density. Additionally, a tick mark within each violin pinpoints the median of the
associated evaluation metric.
As shown in Figure 3(a), both models perform well on data set A due to its separability,
resultinginclassificationaccuraciesnear1. However,itisnoteworthythatbothmodelsexperience
a slight decrease in accuracy as depth increases. Interestingly, this decrease is slightly more
pronounced in KANs. For instance, KANs with a depth of 3 reach a minimum accuracy of 0.973,
the lowest observed accuracy level. Furthermore, the median accuracy for KANs with depth 3 is
0.990, compared to approximately 0.993 for all MLP depths.
Figure 3: Classification accuracy comparison: MLPs vs. KANs on simulated data sets A and B.
Furthermore, Figure 3(b) reveals a larger performance gap between MLPs and KANs on the
7smaller data set (data set B). While MLPs maintain accuracy levels comparable to the previous
data set (e.g., median values near 1), the performance of KANs noticeably degrades in this small-
scale scenario. For instance, KANs with depth 3 have a median accuracy of 0.983 and a minimum
accuracy of 0.933, a significant drop compared to the minimum of 0.973 observed for data set A.
When comparing the distribution of results across all depths, MLPs show a clear advantage over
KANs.
To gain deeper insights into the behavior of MLPs and KANs in relation to depth, we present
the total number of learnable parameters in Figure 4. This reveals striking differences between
the two models due to their degrees of freedom. KANs, using more complex spline functions
compared to the parameterized SiLU in MLPs, have an order of magnitude more parameters
to learn. For example, at depth 3, MLPs have 27 learnable parameters while KANs have 192.
This suggests that even with distinct activation functions per neuron, the parameterization used
by MLPs provides sufficient flexibility to effectively classify this data set with its two clusters
per class. Hence, this analysis highlights that MLPs have a distinct advantage over KANs when
dealing with limited labeled data.
Figure 4: Parameter count comparison: a substantial disparity in the total number of learnable
parameters is observed between KANs and MLPs on simulated data.
4 Real-World Case Studies
4.1 Cancer Detection
In this case study, we use the breast cancer wisconsin data set from scikit-learn, which is a well-
known data set in machine learning and medical research. It consists of diagnostic information
aboutbreastcancer,includinginputfeaturesderivedfromdigitizedimagesoffineneedleaspirates
(FNAs) of breast masses. The data set contains 569 samples and each sample is described by 30
real-valued input features, thus D = 30. There are 212 malignant and 357 benign samples in the
data set, providing a somewhat balanced view of both malignant and benign cases, with the goal
of training machine learning models to classify the diagnosis based on the input features.
As detailed in the previous section, we maintain a constant network width of 2 while varying
the depth L (1, 2, and 3). We also ensure consistency by using the same number of epochs and
8learning rate values. To assess classification accuracies, we conduct 25 independent experiments,
eachwithitsowntrain-testsplitandweightinitialization. Ourgoalistocomparetheperformance
of MLPs and KANs across these 25 repetitions using violin plots.
Figure 5 demonstrates that MLPs significantly outperform KANs in this cancer detection
problem. The maximum and minimum accuracy levels for MLPs across all three depths are
approximately 0.99 and 0.94, respectively. Therefore, our implementation of MLPs with individ-
ually trainable parameters in their SiLU activation functions aligns with existing neural network
implementations on this popular data set, which have achieved high accuracies approaching 0.99
[24].
In contrast, the maximum and minimum accuracy values for KANs are around 0.98 and 0.88,
respectively. This wider range of accuracy values, approximately twice that of MLPs, suggests
lower overall accuracy and potentially less reliability in their predictions. Similar results are ob-
served when comparing median accuracy levels. For instance, at depth 3, the median accuracy of
MLPsis0.976, whileKANshaveasubstantiallylowermedianvalueof0.947. Importantly, MLPs’
performance remains fairly consistent across the three depths, suggesting that hyperparameter
tuning may be more straightforward compared to KANs.
Figure 5: Evaluating the effect of depth (number of hidden layers) on classification accuracy: a
comparative study of MLPs and KANs using the cancer detection data set with 569 samples.
To further analyze the impact of activation function choice on model complexity, we present
the total number of learnable parameters for MLPs and KANs in Figure 6, particularly consid-
ering the 30-dimensional input. The figure reveals a stark contrast: KANs generally possess an
orderofmagnitudemoreparametersthanMLPs. Despitethisincreasedcomplexity,KANsunder-
perform in this real-world cancer detection task. This suggests that MLPs, even with individual
parameterized activation functions, achieve sufficient complexity for accurate classification using
fewer parameters. In this case, the simpler MLP architecture appears to be more effective at
learning from the 30 input features, highlighting the potential advantages of parameter efficiency
in low-data regimes.
4.2 3D Printer Type Prediction
In fused filament fabrication (FFF), the mechanical properties of printed parts are influenced
not only by printing parameters but also by the specific 3D printer used. Variations in hardware
andfirmwareacrossprintermodelsleadtodifferencesinmaterialdeposition,movementprecision,
andtemperaturecontrol, impacting factorslikeinterlayeradhesionandultimately thefinalpart’s
9Figure 6: On the cancer data set, KANs have a substantially higher number of learnable param-
eters compared to MLPs.
strength and surface quality [25].
In this section, we evaluate classification models based on MLPs and KANs with varying
depths for identifying the 3D printer used to manufacture a given part. Using a data set [26]
comprising tensile properties of parts printed on three different printers (MakerBot Replicator
2X, Ultimaker 3, and Zortrax M200) with varying printing parameters, our models aim to predict
the printer type based on 7 input features: tensile strength, elastic modulus, elongation at break,
extrusion temperature, layer height, print bed temperature, and print speed. This approach
seekstocapturethesubtlerelationshipsbetweenprintingprocess,partproperties,andthespecific
printerused,whichcanbeviewedasathree-classclassificationproblemwithD = 7inputfeatures
and 104 samples. Thus, this section aims to evaluate the performance of MLPs and KANs in
scenarios with severely limited data, a common challenge in many experimental settings.
Figure 7 presents classification accuracy results on this data set across 25 repetitions, each
utilizing a 70-30 train-test split ratio, to capture the effects of weight initialization and other
stochastic factors in training neural network models. Overall, MLPs demonstrate strong perfor-
mance across all three depth values. For instance, with a depth of 3, the maximum, median, and
minimum accuracies are 1, 0.906, and 0.844, respectively. These results highlight the effective-
ness of MLPs, especially considering this is a three-class classification problem where a random
classifier would yield accuracies closer to 0.333, significantly lower than our observed results.
On the other hand, we observe a more pronounced accuracy drop in KANs compared to
the previous cancer detection data set. While KANs can achieve accuracies at or near 1 in
some repetitions, there are instances where accuracy falls below 0.333, the baseline for a random
classifier. Fortunately, the median values for KANs at depths 1, 2, and 3 are 0.531, 0.406,
and 0.406, respectively, all surpassing the baseline. Nevertheless, MLPs consistently outperform
KANs in this scenario.
Similar to previous cases, we also report the total number of learnable parameters on the 3D
printer type prediction data set in Figure 8. Again, we observe that MLPs have approximately
an order of magnitude fewer parameters to learn, which is significant given the small sample size.
Specifically, the total number of learnable parameters for MLPs at depths 1, 2, and 3 are 27, 35,
and 43, respectively. These values are below the sample size, which may explain the consistent
10Figure 7: Evaluating the effect of depth (number of hidden layers) on classification accuracy: a
comparative study of MLPs and KANs using the 3D printer type prediction data set with 104
samples.
good performance of MLPs on the test set across different repetitions.
Figure 8: On the 3D printer type prediction data set, MLPs possess significantly fewer learnable
parameters, a number that remains lower than the total number of samples available in this data
set.
4.3 Dependence of KANs on the Polynomial Order of Activations
In this section, we delve deeper into the performance of KANs on these two real-world data sets
to understand how the complexity of the activation functions influences their behavior. Recall
that each activation function in KANs is parameterized as a B-spline, and a crucial hyperpa-
rameter is the polynomial order of these splines. An order of 1 results in an activation function
similar to ReLU, i.e., a piecewise linear function, while higher orders provide increasing degrees
of nonlinearity.
The default spline order in the KAN implementation is 3, offering a reasonable balance of
11nonlinearity. To gain further insight into the impact of this choice in low-data regimes, we
consider networks of depth 2 and width 2, but we vary the spline order from 1 to 5.
Figure 9(a) reveals an interesting trend: in most cases, the accuracy of KANs on the cancer
detectiondatasetdecreasesasthesplineorderincreases. Thehighestmedianaccuracyisachieved
at order 2 (0.959), while the lowest is at order 5 (0.929). However, MLPs with parameterized
swishactivationsperneuronstilloutperformthebestKANconfigurationinthisexperiment. The
median accuracy for MLPs with depth 2 reaches 0.965, while maintaining an order of magnitude
fewer learnable parameters. This demonstrates that MLPs with individualized parameterized
activations can achieve higher accuracy with significantly fewer parameters, a crucial advantage
in low-data scenarios.
Figure 9: Investigating the impact of spline order on both the accuracy of KANs and the number
oflearnableparameters,utilizingtworeal-worlddatasets: (a)cancerdetectionand(b)3Dprinter
type prediction.
Furthermore, we investigate the impact of spline order on the 3D printer type prediction data
set, whichhasasubstantiallysmallersamplesizeandinvolvesatrinary(three-class)classification
problem instead of binary. In this case, Figure 9(b) demonstrates that even a spline order of 1
can lead to a noticeable number of instances where accuracy falls below 0.8, the minimum value
observed for MLPs in the previous section. While all spline orders except for 4 can achieve high
accuracies close to 1, a serious concern is the potential for substantial performance degradation
due to data splits and other stochastic factors in each repetition. Moreover, even with a spline
order of 1, KANs have a much higher number of learnable parameters compared to MLPs. This
suggests that more compact MLPs can achieve superior accuracy levels in this scenario.
125 Conclusions and Future Directions
While Kolmogorov-Arnold Networks (KANs) offer an intriguing alternative to Multilayer Percep-
trons (MLPs) by replacing linear weights with highly expressive activation functions, our findings
highlight their notable performance degradation in low-data regimes compared to MLPs. As our
algorithmic comparison revealed, the primary strength of KANs lies in their choice of activa-
tion functions because they implicitly incorporate a linear weighting mechanism similar to MLPs.
Thisunderscoresthecriticalroleofactivationfunctioncomplexityinneuralnetworkperformance.
While complex activation functions with greater flexibility might seem like the obvious choice,
our findings suggest that simpler alternatives such as the SiLU can provide adequate capacity for
some practical applications, particularly in scenarios with limited data availability.
Additionally, ourresearchhasrevealedthatindividuallyparameterizedneuronswithinhidden
layers can derive advantages from utilizing independent, individualized activation functions. This
approach does not compromise accuracy and opens new avenues for enhancing the predictive
power of smaller networks on small-scale data sets. Implementing such MLPs with fully adaptive
activation functions is straightforward in popular deep learning libraries like TensorFlow/Keras
and PyTorch using concatenation layers. It is imperative to include these MLPs in future com-
parative studies, as existing benchmarks focusing on fixed-shape activation functions, e.g., [27],
may not provide a fair comparison. This paves the way for exploring novel activation functions
that offer controlled nonlinear transformations for analyzing complex data.
Several recent works have explored activation functions beyond splines in KANs, which can
also be applied to MLPs with individualized activations. For example, wavelet functions can
capturebothhigh-frequencyandlow-frequencycomponentsofinputdata[28]. Anotherpromising
direction isdevelopingadaptive algorithms for selecting activation functions per neuronin MLPs,
considering a predefined space including splines, wavelets, Chebyshev polynomials, and others
[29, 30]. Such an approach could factor in sample size and data complexity measures, proving
especially beneficial in low-data scenarios.
Finally, future comparative studies should also investigate the sensitivity of MLPs and KANs
to their hyperparameters. Our experiments demonstrated that MLPs are relatively insensitive
to network depth, whereas KAN performance can significantly degrade with increasing depth or
spline order. A thorough comparative analysis necessitates a comprehensive examination of other
hyperparameters like learning rate, number of epochs, and grid size, to name a few.
References
[1] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein, “On the expressive
powerofdeepneuralnetworks,”inInternational Conference on Machine Learning,pp.2847–
2854, 2017.
[2] S. Hayou, A. Doucet, and J. Rousseau, “On the impact of the activation function on deep
neuralnetworkstraining,” inInternational Conference on Machine Learning, pp.2672–2680,
2019.
[3] R. Vershynin, “Memory capacity of neural networks with threshold and rectified linear unit
activations,” SIAM Journal on Mathematics of Data Science, vol. 2, no. 4, pp. 1004–1033,
2020.
[4] F. Agostinelli, M. Hoffman, P. Sadowski, and P. Baldi, “Learning activation functions to
improve deep neural networks,” arXiv preprint arXiv:1412.6830, 2014.
13[5] K. Lee, J. Yang, H. Lee, and J. Hwang, “Stochastic adaptive activation function,” Advances
in Neural Information Processing Systems, pp. 13787–13799, 2022.
[6] A. Maniatopoulos and N. Mitianoudis, “Learnable leaky ReLU (LeLeLU): An alternative
accuracy-optimized activation function,” Information, vol. 12, no. 12, p. 513, 2021.
[7] L. Trottier, P. Giguere, and B. Chaib-Draa, “Parametric exponential linear unit for deep
convolutional neural networks,” in International Conference on Machine Learning and Ap-
plications, pp. 207–214, 2017.
[8] P. Ramachandran, B. Zoph, and Q. Le, “Searching for activation functions,” arXiv preprint
arXiv:1710.05941, 2017.
[9] M. Tanaka, “Weighted sigmoid gate unit for an activation function of deep neural network,”
Pattern Recognition Letters, vol. 135, pp. 354–359, 2020.
[10] M. Kaytan, I. Aydilek, and C. Yero˘glu, “Gish: a novel activation function for image classi-
fication,” Neural Computing and Applications, vol. 35, no. 34, pp. 24259–24281, 2023.
[11] A. Apicella, F. Donnarumma, F. Isgr`o, and R. Prevete, “A survey on modern trainable
activation functions,” Neural Networks, vol. 138, pp. 14–32, 2021.
[12] Z. Liu, Y. Wang, S. Vaidya, F. Ruehle, J. Halverson, M. Soljaˇci´c, T. Hou, and M. Tegmark,
“KAN: Kolmogorov-Arnold Networks,” arXiv preprint arXiv:2404.19756, 2024.
[13] J.Schmidt-Hieber,“TheKolmogorov–Arnoldrepresentationtheoremrevisited,”Neural Net-
works, vol. 137, pp. 119–126, 2021.
[14] G. Qi and J. Luo, “Small data challenges in big data era: A survey of recent progress on
unsupervised and semi-supervised methods,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 44, no. 4, pp. 2168–2187, 2020.
[15] F. Pourkamali-Anaraki, T. Nasrin, R. Jensen, A. Peterson, and C. Hansen, “Adaptive acti-
vation functions for predictive modeling with sparse experimental data,” Neural Computing
and Applications, 2024.
[16] F. Pourkamali-Anaraki, T. Nasrin, R. Jensen, A. Peterson, and C. Hansen, “Evaluation of
classification models in limited data scenarios with application to additive manufacturing,”
Engineering Applications of Artificial Intelligence, vol. 126, p. 106983, 2023.
[17] J. Ren, C. Yu, X. Ma, H. Zhao, and S. Yi, “Balanced meta-softmax for long-tailed visual
recognition,” Advances in Neural Information Processing Systems, vol. 33, pp. 4175–4186,
2020.
[18] S. Dubey, S. Singh, and B. Chaudhuri, “Activation functions in deep learning: A compre-
hensive survey and benchmark,” Neurocomputing, 2022.
[19] A. Jagtap and G. Karniadakis, “How important are activation functions in regression and
classification? asurvey,performancecomparison,andfuturedirections,”Journal of Machine
Learning for Modeling and Computing, vol. 4, no. 1, 2023.
[20] K.Biswas,S.Kumar,S.Banerjee,andA.Pandey,“Erfactandpserf: Non-monotonicsmooth
trainableactivationfunctions,”inAAAIConferenceonArtificialIntelligence,pp.6097–6105,
2022.
14[21] S. Kili¸carslan and M. Celik, “Parametric RSigELU: a new trainable activation function for
deep learning,” Neural Computing and Applications, vol. 36, no. 13, pp. 7595–7607, 2024.
[22] M. Unser, “A representer theorem for deep neural networks,” Journal of Machine Learning
Research, vol. 20, no. 110, pp. 1–30, 2019.
[23] P. Bohra, J. Campos, H. Gupta, S. Aziznejad, and M. Unser, “Learning activation functions
in deep (spline) neural networks,” IEEE Open Journal of Signal Processing, vol. 1, pp. 295–
309, 2020.
[24] M. Alshayeji, H. Ellethy, S. Abed, and R. Gupta, “Computer-aided detection of breast
cancer on the wisconsin dataset: An artificial neural networks approach,” Biomedical Signal
Processing and Control, vol. 71, p. 103141, 2022.
[25] T. Nasrin, F. Pourkamali-Anaraki, and A. Peterson, “Application of machine learning in
polymer additive manufacturing: A review,” Journal of Polymer Science, vol. 62, no. 12,
pp. 2639–2669, 2024.
[26] D. Braconnier, R. Jensen, and A. Peterson, “Processing parameter correlations in material
extrusion additive manufacturing,” Additive Manufacturing, vol. 31, p. 100924, 2020.
[27] E. Poeta, F. Giobergia, E. Pastor, T. Cerquitelli, and E. Baralis, “A benchmarking study of
kolmogorov-arnold networks on tabular data,” arXiv preprint arXiv:2406.14529, 2024.
[28] Z. Bozorgasl and H. Chen, “Wav-kan: Wavelet kolmogorov-arnold networks,” arXiv preprint
arXiv:2405.12832, 2024.
[29] Z. Yang, J. Zhang, X. Luo, Z. Lu, and L. Shen, “Activation space selectable kolmogorov-
arnold networks,” arXiv preprint arXiv:2408.08338, 2024.
[30] K. Shukla, J. Toscano, Z. Wang, Z. Zou, and G. Karniadakis, “A comprehensive and FAIR
comparison between MLP and KAN representations for differential equations and operator
networks,” arXiv preprint arXiv:2406.02917, 2024.
15