AnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
HiteshTulsiani*1 DavidM.Chan*12 ShaliniGhosh*1
GarimaLalwani1 PrabhatPandey1 AnkishBansal1 SriGarimella1 AriyaRastrow1 Bjo¨rnHoffmeister1
Abstract
Supervised ASR
Transcription Loss
Dialogsystems,suchasvoiceassistants,areex-
pectedtoengagewithusersincomplex,evolving Target Utterance
ASR Predicted Transcription
conversations. Unfortunately, traditional auto- Model
Text
maticspeechrecognition(ASR)systemsdeployed
Context Data
insuchapplicationsareusuallytrainedtorecog- (e.g. past utterances)
Traditional Context Modeling
nizeeachturnindependentlyandlacktheabilityto
Contrastive Learning Supervised
adapttotheconversationalcontextorincorporate (CLCf )r o +m H aC ro dn Nte ex gt ative TransA cS riR ption
Mining (Ohm) Loss
userfeedback.Inthiswork,weintroduceageneral Past Utterances
framework for ASR in dialog systems that can
go beyond learning from single-turn utterances ASTe Ra Mch oe dr el Predicted transcription
Target Utterance
andlearnovertimehowtoadapttobothexplicit
Model
supervisionandimplicituserfeedbackpresentin Distillation
Future Utterances (with predicted transcription)
multi-turnconversations.Weaccomplishthatby
leveragingadvancesinstudent-teacherlearning
andcontext-awaredialogprocessing,anddesign- ASS Rtu Mde on dt el Predicted transcription
Target Utterance
ingcontrastiveself-supervisionapproacheswith Proposed Approach
Ohm,anewonlinehard-negativeminingapproach. Figure1:TraditionaldialogsystemslearntoperformASRusing
Weshowthatleveragingournewframeworkcom- only supervised feedback with large-scale unsupervised/semi-
supervisedpre-trainingonsingleisolatedutterances. Thiswork
paredtotraditionaltrainingleadstorelativeWER
introducesanovelgeneralframeworkleveragingstudent-teacher
reductionsofcloseto10%inreal-worlddialog
distillation,contrastivelearning,andonlinehard-negativemining,
systems,andupto26%onpublicsyntheticdata. allowingASRsystemstolearnfromcontextualcluesandimplicit
feedbackpresentinfullconversationaltranscripts.Ourtwostage
system naturally allows us to distill contextual signals from a
1.Introduction context-awareteachermodeltoacontextunawarestudentmodel.
Automaticspeechrecognition(ASR)fordialogsystemshas
(Schwarzetal.,2023;Kim&Metze,2018;Changetal.,2021;
traditionallybeenafocusedfield,wheretheprimarygoalis
Chenetal.,2019;Sathyendraetal.,2022;Weietal.,2021).
toproduceatexttranscriptforanutterancegiventheacoustic
Such a struggle with long-tailed distributions has led
signalcorrespondingtothatutterance(Radfordetal.,2023;
to several promising directions of research aimed at
Baevskietal.,2020;Hsuetal.,2021;Mitraetal.,2023).
specializinglarge-scalegeneralmodelstohandlerarewords.
Whilesuchsystemshavebeenlargelysuccessful,particu-
Theseapproachesgenerallycenteraroundfine-tuningwhere
larlyinthedomainofdialogsystemsandvoiceassistants
modelsaretunedonrarewordsastheyarediscovered(Li
(leadingtoworderrorratesbelow2%ontheLibrispeech
etal.,2022;Kuetal.,2024;Gaoetal.,2022;Changetal.,
benchmark(Radfordetal.,2022)),inreal-worldapplications
2022;Yangetal.,2023b;Hungetal.,2023),“ASRmodel
suchsingle-utterancesystemshavebeenshowntostruggle
personalization”wheremodelparametersarelocallyadapted
withalong-taileddistributionofrarewords,propernouns,
withuser-specificcontext(Gouravetal.,2021;Biadsyetal.,
etc.,leadingtodecreasedusersatisfactionwithsuchsystems
2022; Shor et al., 2019), or “Contextual biasing” where
*Equal contribution 1Amazon AGI 2UC Berkeley (work modelinputsincludeadditionaluser-specificcontextaspart
done while at Amazon). Correspondence to: Shalini Ghosh oftheinputtothemodel(Jayanthietal.,2023;Kim&Metze,
<ghoshsha@amazon.com>.
2018;Tangetal.,2024;Sathyendraetal.,2022;Changetal.,
2021;Chenetal.,2019;Weietal.,2021;Dingliwaletal.,
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by 2023). Whiletheseapproacheshaveshownpromisingre-
theauthor(s). sults,theyoftenrequireadditionalcomputeduringtrainingor
1
4202
peS
61
]SA.ssee[
1v51501.9042:viXraAnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
additionalstorageandretrievalformodelparameteradapters, TerminologyWerefertooursystemas”self-learning”to
leadingtosignificantcompromisesintermsofcriticallatency conveythesystem’sabilitytoiterativelyimproveitsperfor-
factors. Thesemethodsalsooftenmustrelyonadditional mancebylearningfromdialoguecontextsanduserfeedback.
supervisedtrainingdataduringthetrainingstage,leadingto ThisgoesbeyondtraditionalSSL(self-supervisedlearning)
increasedreal-worldsystemcosts(suchasdatalabeling)that techniquesbyintegratingan”interactive”(thoughoffline)
areoftennotjustifiablebymarginalperformanceincreases. componentwherethesystemlearnsfromitsenvironment
ratherthansolelyrelyingonpre-existingunlabeleddata.
In this work, we aim to address the two key challenges –
run-timeperformanceandincreaseddatacosts–byintro-
ducingatwo-stageframeworkforcontext-awareautomatic 2.Background&RelatedWork
speechrecognition.Toreducerun-timeperformancecosts,
Methods for modeling context for automatic speech
weexploretheuseofmodeldistillationinastudent-teacher
recognition (ASR) systems can be generally categorized
framework—weleveragecontextsignalsduringtraining
intotwomaincategories: supervisedmethods,whichrely
oftheteachermodel,butdonotusecontextsignalsduring
onadditionaldataandlabelstoinfercontextwhichisuseful
run-time inference of the student model for efficiency.
forspeechrecognition,andunsupervisedmethods,which
To reduce data costs, we leverage recent advances in
learn context cues directly from the utterance, and any
self-supervisedlearningfromintrinsiccontextualsignals,
associatedprior/futureutterances.Inthissection,wediscuss
augmentedwithanovelalgorithmforonlinehard-negative
ourproposedapproachincontextwithpriorapproachesfor
mining—thisenablestheteachermodeltolearncontext
context-awareASR.
signalsinaself-supervisedfashion,eliminatingtheneedfor
additionalsuperviseddataduringtraining.
2.1.SupervisedContextModeling
Evaluatingthereal-worldperformanceofsuchasystemis
Asdiscussedinsection1,supervisedcontextmodelingforau-
challenging,asthereislittlepubliclyavailabledialogdata.
tomaticspeechrecognitionlargelyfallsintothreecategories:
To evaluate our approach, we run experiments on a large
datasetofover200Khoursofreal-worldde-identifieddata • Fine-tuning:wheremodelsarefine-tunedonspecific
fromapopularconversationalassistantsystemandshowthat datasetstoincreaseglobalcontextawareness.
leveragingcontextcanhelptosignificantlyimproveteacher
modelperformance. • Model Personalization: where model parameters
are updated on a per-user basis using a small set of
Ourkeycontributionsareasfollows:
user-specificsamples.
• Weintroduceamulti-stageteachermodelforautomatic • Contextual biasing: where models take additional
speechrecognitionincontextualdialogsystemswhich contextasinputduringthetrainingandinferencestages.
iscapableofleveragingbothexplicitcontextsignals
(throughaudioandtextcontext)andimplicitfeedback Each of these approaches has benefits and drawbacks.
signals(throughcontrastivelearningcombinedwitha Perhapsthemostcommonapproachforcontextmodeling
novelonlinehard-negativeminingalgorithm)presentin is fine-tuning, which includes context by training on
sequentialtask-orienteddialogues(Chanetal.,2024). specializeddatasets(Lietal.,2022;Kuetal.,2024;Gao
etal.,2022;Changetal.,2022;Yangetal.,2023b;Hung
etal.,2023). Suchanapproachcanbequiteeffective, as
• We leverage our teacher model in a distillation
itturnsalong-taildistributionproblemintoanin-domain
framework,anddemonstratethatcontextsignalscan
problem.However,itrequiresthecollectionofexplicitdata
bedistilledintoastudentmodelrequiringnoadditional
forthetargetproblem,andthescopeofthecontextthata
run-timecomputecomparedtoconventionalsystems.
modelcanlearnislimitedtothecollecteddata.Further,this
datacollectionprocessisoftenexpensive–thusfine-tuning
• Wedemonstrateclosetoa10%relativeWERimprove-
isoftenemployedlargelyasanaugmentationtoanexisting
mentinreal-worlddialogsystemsapplicationsforthe
pre-trainedmodeltofixspecificerrors,ratherthanasagood
teachermodel,andupto24.4%WERRonthepublic
methodforimprovingcontextawarenessingeneral.
OD3dataset. Further,wedemonstrateclosetoa4%
relativeWERimprovementwhenourteachermodelis Whilefine-tuningadjuststhemodelgloballytoincorporate
distilledtothestudentmodel;providingstrongevidence context(suchasrarewords),recentlysomeapproacheshave
thatlearningfromcontextattrainingtimecanbeeffec- beenexploredthatfocusonadjustingthemodelparameters
tiveattesttime(evenwhensuchcontextisunavailable). locallytoaccountforcontext.Gouravetal.(2021)showthat
We additionally show our approach does extremely smallpersonalizedmodelscanbeeffectiveatincorporating
wellinlower-resourcedomains,demonstratinguptoa information from user contexts, and Biadsy et al. (2022)
22.8%WERRonthetaildistributionofreal-worlddata. show that small model adapters consisting of only a few
2AnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
thousand parameters can be locally fine-tuned for each Duarte-Torresetal.(2024)showthattakinginrelatedtext
usertoimproveASRrecognitionperformance. Shoretal. contextfrompastutterancescanimproveASRperformance.
(2019)showthatsuchmodelscanbetrainedusingaslittle Theseapproaches,whileinteresting,focusprimarilyontext
asfiveminutesofpersonalizedspeech. Theseapproaches embeddingsofpriorcontext,anddonotshowthatsuchASR
representgoodwaysoffine-tuningmodelstofocusonusers’ performance can persist in a context-free scenario (as is
individualneeds,however,trainingmodeladaptersforeach oftenthecaseinon-devicelearning)ordiscusstheinclusion
usercanbeexpensiveandcomeswithstoragerequirements, offuturecontext(availableattrain,butnotinferencetime).
inferenceperformancequestions,anddataprivacyconcerns
Theapproachofunsupervisedlearningfromdialogcontexts
(asspeechneedstobeprocessedinthecloud, oftenwith
iscloselyinspiredbyChanetal.(2023)whointroducea
batchesofotheruserdata).
familyofmethods(CLC)forlearningfrombothpastand
Insteadofadjustingthemodelparameters,contextualbiasing futuredialogcontexts,usingcontrastivelearningbetweenthe
movestheinclusionofcontexttotheinputdomain.Several latentrepresentationsofpast/futuredialoguesandthelatent
types of context are effective including user information representationofthetargetutterance.Themotivationbehind
(suchascontactnames)(Tangetal.,2024;Sathyendraetal., thisworkisthataudiothatsharessimilardialogcontexts
2022),priorutterances(Changetal.,2021),visualclues(Hsu shouldhavemoresimilarlatentrepresentations,andthus,is
etal.,2021;Chanetal.,2022),textcatalogs(Dingliwaletal., morelikelytocontainrelevantacousticinformation.While
2023;Chanetal.,2023).OutsideofASR,contextualbiasing ourcurrentapproachborrowsthePF-CLCobjectivefrom
has long been shown to be effective in NLP applications Chanetal.(2023)asanadditionalpre-trainingobjectiveon
(Novotneyetal.,2022;Shenoyetal.,2021;Zhaoetal.,2019; topofourfullysupervisedandself-supervisedfine-tuning
Liu&Lane,2017;Jaech&Ostendorf,2018;Kim&Metze, process, we also found that alone, PF-CLC led to only
2018;Linetal.,2015;Williamsetal.,2018;Munkhdalai minorimprovementsinoverallperformanceduetothesmall
et al., 2022; Sun et al., 2023). While contextual biasing per-gpubatchsizesusedduringtraining(inourcase,each
represents an important component of context modeling, GPUhasamaximumbatchsizeof16).Thus,toimprovethe
itisoftenlimitedbytherequirementtocollectsupervised performanceofPF-CLCinourreal-worldtrainingscenario,
data during the training phase (i.e. contexts need to be weintroduceanovelschemeforonlinehard-negativemining,
explicitlycollectedandstored),aswellastherequirement allowingforimprovedefficiencywhenapplyingtheCLC
to have contexts during the inference phase, which can lossesduringfine-tuning. Further,Chanetal.(2023)does
leadtosignificantlydegradedperformanceincontext-free notstudyindetailhowtoembedcontextsduringtraining,
scenarios. Further, contextual biasing often suffers from theimpactoftrainingonbothpastandfuturecontexts,or
increased model complexity during inference, leading to ifthiscontexttrainingpersistsunderdistillation.
slowerresponsetimesanddecreasedusersatisfaction.
2.3.ModelDistillation
2.2.UnsupervisedLearningFromDialogueContexts
Modeldistillation(Buciluaˇetal.,2006;Hintonetal.,2015)
Insteadoflearningcontextexplicitly,unsupervisedlearning haslongbeenaneffectivetoolwhenusedtoimprovethe
ofcontextcluesisalargelyunder-exploredareainautomatic performanceofmodelsduringinferencetime.Notonlyare
speechrecognition.Recentworkhasstartedtoexplorehow student models often more efficient than teacher models,
we can learn contextual information from audio context butsurprisingly, suchmodelsareoftenmoreeffectiveon
alone. Hori et al. (2021) and Hori et al. (2020) take in downstreamtestdata(Radosavovicetal.,2018;Zhangetal.,
several utterances at once, and use this joint context to 2019;Phametal.,2022).ThesetrendshaveheldinASRas
perform automatic speech recognition on the final target well,whereHuangetal.(2018);Kimetal.(2019a);Mun’im
utterance(demonstratingupto15%improvementsinWER). etal.(2019)allshowthatlargeteacherASRmodelscanbe
Unfortunately, thesemethodsrequirepreviousutterances distilledtoresource-efficientbutperformantstudentmodels.
to be available at test time and suffer when no previous
Beyondmodelcompression,however,modeldistillationhas
contextisavailable. Usingonlythetargetutterance,Chan
morerecentlyalsobeenusedeffectivelytobridgestreaming
&Ghosh(2022)showthatotherunrelatedutteranceswithin
modelsandnon-streamingmodels,asbothYuetal.(2021)
abatchcanbeusedtofilternoisefromautomatedspeech
andKurata&Saon(2020)haveshownthatusingdistillation
recognitionmodels, however, theydonotshowthatsuch
betweenmodelarchitecturescanleadtoovercomingfunda-
methodshelpbeyondglobalandlocalnoiseremoval.
mentalarchitecturelimitationsatinferencetime.Recently,
Insteadofusingaudio,Kimetal.(2019b)applyBERTto Futamietal.(2022)showedthatASRcanbeimprovedbydis-
the partial ASR transcript generated so far and use those tillinginlanguagemodelssuchasBERT,however,theydidso
BERT embeddings to inform the generation of the next usingavector-basedrepresentation,unlikeourproposedap-
token (effectively fusing the language model with the proachthatleveragesmodeldistillationandself-supervision
speech model). Similarly, both Chang et al. (2023) and (Phametal.,2022). Closesttoourframework,Masumura
3AnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
ASR Ouput Joint Loss byassistantencodedintextform(indicatedasAP andAF).
Traditionally, a transducer-based system, at each time
Joint step,outputsaprobabilitydistributionoveritsvocabulary
Model
(word-pieces) conditioned on the acoustic observations
Acoustic Model X = x ,x ,...,x and previously observed word-piece
1 2 T
Contrastive Loss / Ohm tokens y , y , ..., y , which could be expressed as
1 2 u−1
P(y |X,y ,y ,...,y ).Tomodeltheexplicitlong-termcon-
u 1 2 u−1
. Text
.. Prediction textintheteachermodel,weextendtheaboveequationby
Network furtherconditioningonthesetofcontextsignals,Z={YˆP,
M Au ttlt ei n-H tie oa nd YˆF,AP,AF,XP,XF},togetP(y |X,y ,y ,...,y ,Z).
u 1 2 u−1
FC Layers ... FC Layers Language Model Audiocontextmodeling LikeHorietal.(2021)weex-
Speech Encoder ploretwomethodsforaddingaudiocontextfromdialogues:
Past Past Future Future
EA nu cod dio e r EnT ce ox dt er EA nu cod dio e r EnT ce ox dt er Feature Concatenation: In feature concatenation, we
Input Layer
concatenatefeaturesofpastandfutureutterancesalongwith
Past Past Future Future the seed utterance and pass it through the audio encoder.
Utterance ASR Output Utterance ASR Output
Encoderoutputsarethensegmentedtoextractembeddings
Figure2:Anoverviewofourapproach.Duringtrainingourteacher corresponding to seed utterance and are combined with
modelingestscontextfrompast/futureaudioandtextalongwith predictionnetworkoutputtocomputetransducerloss.The
the current utterance, and learns both implicitly and explicitly
presenceofaself-attentionnetworkintheaudioencoder
usingCLC(Chanetal.,2024)forimplicitcontextlearningand
allowsustolearnthedependencyoncontextstreams.
supervisedjointlossforexplicitlearningfromsuperviseddata.
↑showdataflowinforward-pass,and↓↓showlosspropagation AudioEmbeddings:Foraudioembeddings,pastandfuture
fromeachofthecomponents.
contextisencodedviaaseparateencoder(called“context
encoder”).WeuseeitheraHuBERTpre-trainedconformer
etal.(2021)usedistillationtobridgemodelsthathavelong
encoderortheaudioencoderofthetransducernetworkas
audio input contexts (such as several input utterances) to
thecontextencoder. Theseaudioembeddingsarepassed
singleutterancemodels,howevertheirapproachislimitedto
throughamulti-headedself-attentivepoolinglayer(Chang
usingtextmodelsincontext,andtheydonotexploreusing
et al., 2023) and concatenated in time dimension with
audiocontextorlearningthecontextualhintsfromdialog.
keys and values of self-attention module (MHSA) in the
audioencoder(representedinFigure2. Thustheinputsto
3.Self-LearningforDialogueASR
MHSA(query-q,key-k,value-v)canberepresentedas
q=X;k=[XP,X,XF];v=[XP,X,XF]. Thisallowsusto
An overview of our approach is given in Figure 1, and
attendtothecontextualsignalonaper-querybasis. Note
consistsoftwokeycomponents: acontext-awareteacher
here that the output and input of the MHSA module still
model,leveragingbothexplicitcontextsignalsandimplicit
havethesamenumberoftimeframes.Thisensuresthatno
userfeedbackinthedialogue,andasingle-utterancestudent
othercomponentinthemodelneedstobemodified.Another
modeldistilledfromthecontext-awareteacher.
distinctadvantageofre-purposingMHSAinthismanner
asopposedtointroducingaseparatecross-attentionlayer
3.1.TeacherModel
(to attend to context) is that it allows us to easily extend
Followingbestpractices,ourteachermodeliscomposedof
conventionalsingleutterancemodelstobecontextaware.
aConformer-basedtransducernetwork(Graves,2012)-a
nonstreamingmodelwhichcanattendtoallframesinanutter- Text context modeling In addition to audio context,
ance.Inaddition,ourteachermodelalsoleveragesbothpast followingDuarte-Torresetal.(2024),Kimetal.(2019a),and
andfuturecontextsasexplainedinthefollowingsections. Changetal.(2023),weexploretwovariantsforencoding
textcontextfrompriorutterancesinadialog:
3.1.1.LEARNINGFROMEXPLICITCONTEXT
BERT Embeddings: In the BERT embedding case, we
Inthiswork, weleverageseveralexplicitcontextsources leverageapre-trainedtextembeddingmodel(Devlinetal.,
drawnfromthedialoguesthemselves.Thefirstistheaudio 2019) with 5M parameters based on BERT to generate a
context,formedbythesequenceofuserinputqueriesina summaryvectorforthepast/futuretextrepresentations.
givendialogue(precedingandsucceedingaudiocontextis
LearnedEmbeddings:Inthelearnedembeddingcase,text
representedasXP andXF respectivelyinFigure2). The
contextistokenizedusingasentencepiecemodel(Kudo&
secondistextcontext,theASRone-besthypothesis(indi-
Richardson,2018)andeachtokenisrepresentedasaone-hot
catedasYˆP andYˆF)correspondingtothesequenceofuser
vectorovervocabularysize.Thisisthenconvertedtoanem-
inputqueriesinadialoguealongwiththeresponsegenerated
4AnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
beddingandcombinedintheself-attentionlayeroftheaudio While several technical methods have been developed
encoder(similartotheaudioembeddingsdescribedabove). for contrastive learning with small batch sizes including
BASIC (Pham et al., 2023), which leverage tools from
3.1.2.LEARNINGFROMIMPLICITCONTEXT gradientcheckpointandmodelparallelismtoimprovethe
Inamulti-turninteractionwithavoiceassistant, theuser “effective”batchsize,suchmethodshavesignificantcompute
mayrepeatorrephrasetheirquery(tocorrectthesystem) bottlenecks, and still rely on some form of all-reduce to
following an unexpected response by the assistant. To computetheglobalcontrastiveloss. Theseall-reducesare
empiricallyestablishthatsuchuserreformulations(implicit technicallycomplex,andonmanyGPUclusterscanlead
interactions) are correlated with ASR, we conducted a tosignificantcommunicationoverheadwhenmachinesmust
simpleexperiment. Wepreparedtwodatasets: (i)Natural communicatewithnon-localdevices.
sampling: dataisuniformlysampledtoformourtestset; Instead, of such a complex all-reduce based approach,
and(ii)Reformulationsampling:wesampleutterancesthat we target the root of the problem by aiming to build
causetheusertorepeatorrephrasetheirquery. Wethen moreeffectivelocalbatches(i.e. batchesthatwillinduce
evaluate both our existing teacher and student models on high contrastive loss by leveraging hard negative mining,
thesedatasets.Inthisexperiment,weobservedthatboththe introducedbyRobinsonetal.(2021). Traditionally,such
teacherandstudentmodelshavesignificantlyhigherword methods for hard-negative mining rely on a pre-labeling
errorrates(11%and15%respectively)onthereformulation step,wherebatchesarepre-constructedinanoffline-scan,
sampling dataset compared to uniform sampling. This andthenconsumedduringtraining. Unfortunately,sucha
observation,combinedwiththefactthatapproximately15% pre-labelingapproachdoesnotscalewellasthesizeofthe
of analyzed interactions had user reformulations, shows trainingdataincreases.Toremedythis,weintroduceOhm,a
thatuser-providedimplicitfeedbackcancorrectASRerrors. simpleonlinehard-negativeminingprocedurethatcanrunin
Please note that such feedback is not directly solicited linewithtraditionalstreamingdatapipelines.Anoverview
throughthedialoguebutinferredfromusercorrectionsand oftheOhmapproachisgiveninAlgorithm1.
follow-upqueries,hencewerefertoitas”implicit”.
Ohm consists of several key stages each augmenting a
Thus,whileitispossibletolearntoleveragecontextsignals datapipeline. Inthefirststage,samplesarecollectedinto
fromtheexplicitcontext,itisalsoimportanttolearnfrom aninitialbufferB usingastatefulmap. Whenthesizeof
implicit signals in the data. Recently, Chan et al. (2024) the initial buffer exceeds the update window size, then a
showedthatimplicitcontextpresentinthedialoguescan parametricclusteringmethodC isfitonthesamplesfrom
ϕ
be used to further augment the training process through B.Notethatthisprocesshappensper-device,andonlywith
contrastivelearning. Drawingontheirwork,inthiswork, thesamplesyieldedtoeachdevice,leadingtonon-blocking,
we leverage the past-future CLC objective from Chan andnon-communicativebehavior. FutureversionofOhm
etal.(2024)(whichwerefertoasPF-CLC),toincorporate could , however, communicate the C model, leading to
ϕ
implicitcontextinadditiontotheexplicitcontextdiscussed all-reducelikebehaviorwithreducedoverhead. OnceC
ϕ
in the previous section. In this PF-CLC approach, the hadbeentrained,C isusedinastreamingfashiontoassign
ϕ
positive pairs contain past/future/current utterances from labelstoeachsample. Finally,reservoirsampling(Vitter,
the same utterance, while negative pairs are formed by 1985)isusedtosamplebatchesofsamplesfromeachcluster
past/future pairs from other utterances in the batches, groupastheybecomeavailable.Thisleadstobatcheswhich
furtherencouragingthemodeltoorganizethelatentspace aregenerallyclosersemantically, eveninthepresenceof
semanticallyinadditiontophoneticallyduringpre-training. apoorclusteringalgorithmC . Sincetherepresentations
ϕ
thatweareusingchange,weperiodicallyupdateC (every
ϕ
3.1.3.ONLINEHARDNEGATIVEMINING(OHM)
10,000steps)usingthebufferB.Ohmthussolvesapractical
Whentrainingourself-learningbasedsystem,wefounda problem:trainingonGPUswithlessVRAMprecludesthe
significantcorrelationbetweenthelocalGPUbatchsizeand use of larger (more effective) batch sizes, and leveraging
theperformanceofthepre-training(Jainetal.,2024). We Ohmclawsbacksomeofthatperformanceloss.
hypothesizethatthiscorrelationiscausedbythePF-CLC
learningintroducedin(Chanetal.,2024),aswithsmaller
3.1.4.REFORMULATIONUP-SAMPLING
localbatchsizes,therearefewer“hardnegatives”ineach Inadditiontocontrastivelearningwefurtherover-sample
batch, leading to reduced efficiency when training with interactions containing reformulations during training of
CLC-basedlosses. Unfortunately, scalingthelocalGPU boththetransducerandre-scorer. Thisapproachcanhelp
batchsizecanbepracticallydifficult,withoutintroducing to emphasize loss from reformulation samples, without
complex optimizers and training procedures for model introducingadditionaloverhead.Inourexperiments,weem-
sharding. Thispresentsachallengingissue: howcanwe piricallyfindanover-samplingratioof1:5(1reformulation
traincontrastivemodelsunderrestrictedlocalbatchsizes? toevery5standardsamples)tobeeffective(SeeTable1).
5AnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
Algorithm1Ohm:OnlineHardNegativeMining 4.ExperimentalDesign
Require: stream Inthissection,wediscussthedetailsusedwhenevaluating
Ensure: Reorderedstream ourapproachonbothreal-worldconversationaldata, and
opensemi-syntheticdata.
Initializestatewithzeroindexandanemptyfeaturearray
▷Step1: Updateclusterswithstreamingscan(stateful 4.1.Datasets
map)
All our transducer models are first pre-trained on 200k+
stream←stream.scan(initial state,update clusters)
hoursofde-identifiedASRdata(PRETRAIN)usingtrans-
▷Step2:Generateclusterlabelsforeachsample ducerlosswithoutincorporatinganycontextualinformation.
stream←stream.map(generate labels) We then fine-tune and evaluate our approach on one of
thetwosourcesofdatabelow: aclosed-sourcereal-world
▷Step3:Groupsamplesbyclusterlabel
dataset from a conversational assistant, and the recently
stream←reservoir sample(dataset)
introducedOD3dataset(Chanetal.,2024).
returnbatch processeddataset
4.1.1.CLOSEDSOURCEDATA
procedureupdate clusters(buffer,sample)
Thisdatasetconsistsofde-identifieddialoguesconstructed
Addsampletobuffer,andtrimtoupdate window size
fromreal-worldinteractionswithavoiceassistant. These
ifbuffer.size()≥update window sizethen
dialogues are each constructed around a seed utterance,
Partiallyfitclusterstobuffer
which is human transcribed. Given the seed utterance,
endif
themethodpullsinallrelatedconversationsoccurring90
returnbuffer
secondsbeforeandafterit. Thisstepisiterativelyapplied,
proceduregenerate labels(sample) amassingadditionalconversationalexchangesastheyappear.
Useclusterstoassignsampleaclusterlabel If this approach yields over five utterances, the interval
return{...sample,cluster label} forgatheringconversationsiscutdownto45seconds,and
the process is repeated. This shortening of the interval
procedurereservoir sample(stream)
persistsuntilthenumberofutterancesfallsbelowfiveorthe
reservoir←emptylistforeachcluster
intervalnarrowstoa15-secondminimum.Theserestrictions
whilestream.size()>0do
are enforced to ensure that utterances in a dialog are a
sample←stream.take()
semanticallycoherentinteractionaroundthesamerequest.
ifanyreservoir[cluster].size()>batch sizethen
yieldfromreservoir[cluster] Miningdialogueswithreformulations Totrainandeval-
endif uateoursystem,weadditionallydetectasubsetofdialogues
endwhile consistingofreformulations.Todetectreformulations,we
use a text similarity-based approach. To be precise, we
use cosine similarity and edit distance between the ASR
Table1: Worderrorratereduction(WERR)ontheALLdataset
whenusingdifferentreformulationup-samplingrates. hypothesis of the seed and context utterances (generated
duringtheuser’sinteractionwiththeassistant).Thedialog
Rate None 1:10 1:5 1:4 1:3 1:2 1:1
issaidtocontainareformulationifany{seed,context}pair
WERR - 0.23 0.46 0.42 0.39 0.35 0.24
hasasimilaritygreaterthanthethreshold.
3.2.Student-TeacherDistillation We train our context encoder teacher models on 10M
de-identified dialogues. Additionally, we upsample
Ouroverallsystemcomprisesbothastudentandateacher
dialoguescontaininguserreformulations,by20%,during
ASR system. Our student model is a Conformer-based
trainingofthetransduceri.e. oneineveryfivedialogues
transducer network (Gulati et al., 2020) - a conventional
hasreformulation.Forevaluation,weonlyselectdialogues
streaming model i.e. it operates on single-utterance and
containingreformulationsandensurethatallutterancesin
attendstoonlypastframesintheutterance.Duringrun-time,
thedialogarehuman-transcribed.Byfocusingondialogues
governedbylatencyconstraints,weusethestudentmodel
wheretheuserreformulatedhisquery,weensurethatthe
to recognize user queries. These interactions (including
selecteddialoghassignificantASRerrors(asdiscussedin
reformulationsandrepeats)arecaptured–detailsonhow
sectionsubsubsection3.1.2)andwherecontextualsignals
todeterminereformulationsaredescribedinsubsection4.1.
areexpectedtobemeaningfullyrelatedtouserqueries.We
Suchinteractionsarethendecodedusingacontext-aware
createtwodatasetsforevaluation(1)ALL: Alltranscribed
teacher(discussedintheprevioussection)togetarecogni-
utterancesacrossallvalidationdialogues(60Kutterances)
tionhypothesis,whichactsasalabelforsemi-supervised
and(2)REF:AsubsetofALLcontainingonlyutterancesthat
trainingofstudentmodel(Parthasarathi&Strom,2019).
6AnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
leadtouserreformulationsofthequery(8.5Kutterances). Table2: WERimprovementsontheALLandREFdatasetsforthe
teachermodel.LE:LearnedEmbeddings,AE:AudioEmbeddings,
Todistilltheknowledgeoftheteachermodeltothestudent FC:FeatureConcatenation,WERR:WordErrorRateReduction.
model, we use closed-source dialogues containing refor-
mulations.Dialoguesusedfordistillationaredistinctfrom Model Audio Text CLC+ WERR(↑)
Context Context Ohm ALL REF
thoseusedfortrainingteachermodelsandwedon’trequire
Baseline(200M) - - - - -
seedutterancestobehumantranscribed.Suchdialoguesare FC - - 3.73 7.04
fedtoacontext-awareteachermodeltoobtaintranscription AE/HuBERT - - 1.94 4.04
AE/Transducer - - 2.77 4.97
for seed utterances. This single utterance audio-text pair AE/Transducer LE - 3.32 6.70
Teacher(200M)
(approximately25,000hours)isthenusedfortrainingthe - BERT - 0.69 2.66
- LE - 2.63 5.66
studentmodel. Werefertothisassemi-supervisedsingle FC LE - 4.70 8.08
utterancereformulationdataset(SSRD). FC LE (cid:33) 6.91 9.58
- - - 0.55 2.89
Teacher(1B)
4.1.2.OD3 FC LE - 5.53 9.24
FollowingChanetal.(2024),wefurtherevaluateourmodels usemagnitude-basedgradientclippingwithavalueof10.
on the open directed dialogue dataset (OD3). The OD3
Wethenfine-tuneourteachermodelsfor150ksteps,using
datasetisasemi-syntheticdataset,wherehuman-generated
an Adam optimizer with gradient clipping, featuring a
task-orienteddialoguesfromseveralpopulardatasetsare
learningratedecayschedulethatstartsat1e−8,holdsat1e−5,
augmentedwithLLM-generatedconversationalerrorsand
anddecaysto1e−6,withtheclippingnormsetto0.3,and
computer-generatedTTSaudio.OD3contains620Kturns
aschedulepolicythatadjuststhelearningrateat20K,80K,
ofaudio(approximately1,172hours).
and 600K training steps. In addition, we apply dynamic
L2regularizationtotheMulti-headself-attentionlayersof
4.2.ModelDetails
theconformerusingaPiecewiseConstantDecayscheduler
Forourexperiments,weusetransducerarchitecture,with thatincreasestheregularizationfactorattrainingsteps15K
Conformer(Gulatietal.,2020)astheaudioencoder. We and30K(calculatedas2×numberofconformerblocks×
experiment with two different teacher architectures: (i) warmupsteps), withtheregularizationvaluessetto1e−6,
200Mparameters:17conformerblocksandattentionsizeof 5e−6,and1e−5attheseintervals.
1024(ii)1Bparameters:18conformerblocksandattention
Formodelstrainedwithcontrastivelearning,weuseasetof
sizeof1536. Eachconformerblockiscomposedoffour
32learnedclustersforhard-negativemining,withabuffer
modules-multi-headattentionandconvolutionalmodules
sizeof4096fortheonlinereservoirsampling.Weleverage
aresandwichedbetweentwofeed-forwardmodules. The
theBIRCH(Zhangetal.,1997)clusteringalgorithm,overthe
convolutionalmodulehasakernelsizeof30frames.Before
embeddingsofXp.Inthefuture,weintendtoexploreaddi-
conformerblocks,weuseapre-processingblockconsisting
tionalclusteringalgorithmsandleveragebetterdistancefunc-
oftwoconvolutionlayers,whichtakesinfeaturesata30ms
tionsfortheOhmminingapproach.Forthehyper-parameters
framerate,andhasakernelsizeof5andstrideof3.
ofthePF-CLCloss,wefollowtheparametersinChanetal.
Our student model has 1B parameters and consists of 18 (2024). Ourstudentmodelispre-trainedusingPRETRAIN
conformerblockswithanattentionsizeof1536.However, datasetfor400Kiterationswith64A100GPUsandaper-
for student models we restrict the attention module to gpubatchsizerangingfrom128to1.Formodeldistillation,
only attend to past frames - this ensures user-perceived weusebothSSRDandPRETRAINdata,sampledatdiffering
latencyisminimal. Forthepredictionnetwork, weusea ratios,andstandardASRtransducerloss.
two-layerLSTMnetworkwith1024hiddendimensionsand
avocabularyof4,000tokens. 5.ResultsandDiscussion
4.3.Trainingdetails Asdiscussedinsection4,weevaluateoursystemonboth
closed-sourcedataandtheOD3dataset.Ingeneral,weuse
Ourteachermodelispre-trainedusingthePRETRAINdataset
bothstandardworderrorrate(WER,↓)andrelativeword
for500Kiterations,usingaper-gpubatchsizerangingfrom
errorrateimprovement(WERR,↑)toevaluateoursystem.
32to1,dependingonthelengthofthesequence(sequences
arebatchedtomaximallyuseGPUmemory)acrosseither
5.1.TeacherPerformance
64P100GPUs(for200Mmodel)or64A100GPUs(for1B
model).Wepre-trainusinganAdamoptimizer-welinearly OuroverallresultsfortheteachermodelontheALLandREF
increase the learning rate for 5000 steps and thereafter aregiveninTable2.Wecanseethatoursystem,combining
decreaseitproportionallytotheinversesquarerootofthe the feature-concatenation audio context (subsection 3.1),
step(asperscheduledescribedin(Vaswanietal.,2017)),and learnedtextcontext(subsection3.1),andCLC/Ohmlosses
(subsubsection3.1.2), outperformsthebaselinemodelby
7AnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
Table3: ResultsonOD3(overallandrepeat/rephraseinducing) Table 5: Ablation experiments on the teacher model (200M).
usingthe200Mmodel.WER(↓):WordErrorRate,BERT-S(↑): WERR:WordErrorRateReduction.
BertScore.B:Basline,CX:Context,C:CLCLoss,O:Ohm.
ContextType WERR(↑)
Model Overall Repeat/Rephrase Past Future Audio Text ALL REF
WER(WERR) BERT-S WER(WERR) BERT-S
- - - - - -
B 11.90(-) 0.9711 19.09(-) 0.9402 (cid:33) - (cid:33) - 1.52 3.35
B/CX 11.13(6.47%) 0.9762 16.17(15.29%) 0.9690 (cid:33) - - (cid:33) 1.24 1.5
B/CX/C 8.99(24.4%) 0.9812 13.81(27.65%) 0.9737 (cid:33) - (cid:33) (cid:33) 2.90 5.08
B/CX/C/O 8.73(26.2%) 0.9817 13.21(30.8%) 0.9771 - (cid:33) (cid:33) (cid:33) 3.60 7.39
(cid:33) (cid:33) (cid:33) (cid:33) 4.70 8.08
Table 4: Zero-shot results on OD3 for several open-source
models-Whisper(Radfordetal.,2023),Conformer(Gulatietal., Table6: WERimprovementsontheALLandREFdatasetsforthe
2020),Wav2Vec2(Baevskietal.,2020),StreamingConformer teachermodelwithCLC/Ohmandcontext-awareteachermodel
(Tsunooetal.,2021), CLC(Chanetal.,2024). Modelsinthis asbaseline. Note: BaselineisdifferentfromTable1toensure
tablearenotdirectlycomparable(trainedondifferingdata,setups, comparabletrainingsetup.WERR:WordErrorRateReduction.
hyperparameters,optimizersetc.),butserveasabenchmarkfor
Context CLC Ohm WERR(↑)
performanceonOD3underseveralvaryingsetups.WER(↓):Word Model
ALL REF
ErrorRate,BERT-S(↑):BertScore
(cid:33) - - - -
Model Overall Repeat/Rephrase Teacher(200M) (cid:33) (cid:33) - 6.09 8.68
WER BERT-S WER BERT-S (cid:33) (cid:33) (cid:33) 6.88 10.60
WhisperS(200M) 11.24 0.9775 14.17 0.9727
WhisperL(1.3B) 8.51 0.9852 12.37 0.9792 CombiningContextTypes: Wegetthebestperformance
Conformer(100M,Librispeech) 19.26 0.9612 22.19 0.9571 whenbothaudioandtextcontextsarecombined(compared
Wav2Vec2(433M,Librispeech) 19.41 0.9582 22.03 0.9544
StreamingConformer(45M) 14.38 0.9701 16.70 0.9665 toaddingtwomodalitiesindividually). Interestingly, the
CLC(200M) 8.99 0.9812 13.81 0.9737 200Mmodelwithcontextissignificantlybetterthanthe1B
Ourmodel(200M) 8.73 0.9817 13.21 0.9771
modelwithoutcontextualsignals,highlightingtheefficacyof
ourproposedapproachinmodelingimplicitcontextsignals.
upto9.58%ontheREFandupto6.91%ontheALLdataset.
OnOD3,wecanseethataddingbothcontexttypesleads
Thesetrendsholdacrossmodelsizes,asourcontext-enabled
to a 6.47% performance improvement, tracking similarly
model has similar improvement in both the 200M and
1B cases, implying such improvements are model-size
totheperformanceimprovementsseenintheALLdataset.
independent. Note that for ASR models, 1B parameters Causalvs. Non-CausalContext: InTable5,weablate
isgenerallyconsideredquitelarge, giventhechallenging thetypesofcontextthatweshowtothemodel.Weobserve
latencyandrun-timerequirementsforASRapplications. thatinjectingnon-causal(“future”)contextduringtraining
provides relative WERR of 7.39% as opposed to 5.08%
Impact of audio context: Both the approaches of
incorporating audio context (feature concatenation and on the REF dataset (as well as improvements on the ALL
dataset),indicatingthatfuturecontextissignificantlymore
audio embeddings) improve over a baseline system that
importantwhencorrectinguserreformulations.Thisislikely
doesn’tusecontextualsignals.Improvementsduetofeature
duetothefactthatuserreformulationisa“futuresignal”i.e.
concatenation are larger (> 7% on the reformulation test
itfollowstheutterancethatcausedtheerror.
set),whichisnotunexpected;byconcatenatingfeatureswe
allowthemodeltoattendtoallcontextframesasnecessary, Implicit Context Learning: We can see that learning
asopposedtoattendingtosummaryvectors(N=8,inour fromtheimplicitcontextinthemodelisimportantforunder-
experiments)comingfrommulti-headedattentivepooling. standingandcorrectingdialogerrors.AsshowninTable6,
Amongthetwoapproachesofextractingaudioembeddings, AddingCLCandOhmtothebaselinemodelleadstosignifi-
wefindusingatransducerencoderasacontextencoderis cantimprovementintheoverallperformance,particularlyon
marginallybetter,likelyastheembeddingsare“on-policy” theREFdataset(somuchthatitenablesa200Mparameter
forthetrainedmodel,asopposedtocomingfromanexternal modeltooutperforma1Bparametermodelwithoutsuch
embeddingmodel. losses). On the OD3 dataset (Table 3), the performance
isevenmoredistinct,withlearningfromimplicitcontext
Impactoftextcontext: Ingeneral,wefindthatencoding
leadingtouptoa26.6%relativeimprovementoverabaseline
textvialearnedembeddingsasopposedtosummaryvector
non-contextmodel.Inaddition,zeroshotcomparisonwith
byBERTencoderismorebeneficial. Thisisalignedwith
otheropensourcebenchmarkmodelsisshowninTable4.
theobservationmadeaboveforaudiocontextembeddings
-again, likelybecausethelatentspaceis“on-policy”and
5.2.Distillingknowledgetostudentmodel
trained specifically on in-domain data. We also see that
textualcontextunder-performsaudiocontext,likelydueto Table7showstheperformanceofourmodelwhendistilled
incorrecttextcontentfromtheteachermodel. to a context-free student model. We can see that in all
8AnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
Table7: Tableshowingtheimpactofdistillationfromateacher (forutterancesthatdon’trequirecontextualdisambiguation).
modeltrainedwithimplicit/explicitcontext.WERR:WordError
RateReduction.DE:DistillationEfficiency.
6.Conclusion
%Params WERR(↑)/DE(↑)
Model SSRDweight Adapted ALL REF ThisworkintroducesaframeworkthatimprovesASRin
Student(1B) - - - - dialogsystemsthroughadual-modelapproachtocontextual
learning: a context-aware teacher model that improves
100 20 1.38/19.97% 3.06/31.94%
+Distillation 20 20 1.76/25.47% 1.2/12.52% learningthroughexplicitandimplicitcontextsignals,anda
50 100 1.51/21.85% 2.95/30.79%
distilledstudentmodelthatmaintainsefficiencyduringinfer-
encewithoutcontextreliance.WeachievesignificantWER
Table 8: WERR when normalized by the domain (instead of reductions,upto9.58%onreal-worlddatasetsand26.6%on
by-utterance)ontheALLdataset. WERR(↑): WordErrorRate
theOD3dataset,withthestudentmodelmaintainingupto
Reduction.SERR(↑):SentenceErrorRateImprovement.
33%ofthereductionwithoutcontextacrossthedistillation
Model Context CLC Ohm WERR SERR process. Theenhancementsobserved,particularlyforrare
(cid:33) - - - - wordsanddiverseuserqueries,indicateapathtowardmore
Teacher(200M) (cid:33) (cid:33) - 3.75 2.40 robustandsatisfyingconversationalexperiences,notably,the
(cid:33) (cid:33) (cid:33) 6.64 7.79
pronouncedgainsfortailqueriessuggeststhatourapproach
can significantly improve performance on less common
cases, the student model distilled from a context-trained
tasks.Futuredirectionsforthisworkinvolveexploringthe
modelachievessuperiorperformance.Wealsoevaluatethe
dynamicadjustmentoftherelativeimportanceofcontext
distillationefficiency(DE)ofthemodels–howmuchof
versusthetargetutterancebasedontheirpredictedutility,
theWERgainsoftheteachermodelwereretainedduring
errorcorrection(Yangetal.,2023a)andsafety(Mehrabi
distillation.Itisinterestingtonotethatwhenleveragingthe
et al., 2023). This could potentially unlock even greater
SSRDdataset,only20%oftheparametersinthemodelare
improvements in ASR performance, paving the way for
necessaryduringthedistillationprocesstoachievethesame
moreintelligentandadaptableconversationalAIsystems.
WERR,comparedtowhenlessreformulationdataisused
(seesubsection4.3),indicatingthattheusingthepre-trained
ImpactStatement
teachermodelwithcontextnotonlyismoreaccurate,but
canbemoreefficientaswell.
Thispaperpresentsworkwhosegoalistoadvancethefieldof
MachineLearning.Therearemanypotentialsocietalconse-
5.3.Tail-DistributionPerformance
quencesofourwork.Automaticspeechrecognitiontechnol-
WhileoverallWERisanimportantmeasure,manytimes,a ogyenhancesaccessibility,education,healthcare,legalpro-
strongindicatorofusersatisfactionisperformanceonawide cesses,customerservice,workplaceproductivity,language
rangeofqueriesondifferenttopics(Suchashomeautoma- preservation,globalconnectivity,mediaaccessibility,and
tion,calling/messagingandshopping).InTable8,wepresent safetyacrossvarioussocietalsectors.Whilesuchimpactis
WERRandSERR(SentenceErrorRateImprovement)when largelypositive,itisimportanttorecognizetheimpactofself-
the WER is computed on each topic independently, and learningsystemsforautomaticspeechrecognitionongreater
thenaveragedinsteadofbeingaveragedoverallutterances discussionsinprivacyandsecurity,whicharewelldiscussed
(independentofdomain,i.e.Table8makestheassumption inrelatedwork(Chennupatietal.,2022;Aloufietal.,2021).
thatalldomainsareequallylikely). Fromthis,wecansee
thatwhileournon-contextmodelsperformwellonthemost References
commonutterances,thecontrastivemodelsleadtosignif-
Aloufi, R., Haddadi, H., and Boyle, D. Configurable
icant improvements in less-common domains in our ALL
privacy-preserving automatic speech recognition. In
dataset,includingqueriescategorizedintoshopping(82.86%
Hermansky, H., Cernocky´, H., Burget, L., Lamel, L.,
WERR), calling/messaging tasks (73.7% WERR), and
Scharenborg, O., and Motl´ıcek, P. (eds.), Interspeech
musicrequesttasks(36.8%WERR),allofwhichoftenneed
2021, 22nd Annual Conference of the International
contextualdisambiguation.Ontheotherhand,whilestillin
SpeechCommunicationAssociation,Brno,Czechia,30
thelongtailofthedataset,ourapproachperformsworsethan
August-3September2021,pp.861–865.ISCA,2021.doi:
thebaselineonhomeautomationtasks(-22.68%WERR),
10.21437/Interspeech.2021-1783. URLhttps://doi.
one of the less diverse tasks that requires less contextual
org/10.21437/Interspeech.2021-1783.
disambiguation. Insuchcases,ourmodelmayberelying
moreonthecontext,thanthetargetutterance:leadingtode-
creasedperformance.Itremainsinterestingforfuturework Baevski,A.,Zhou,Y.,Mohamed,A.,andAuli,M. wav2vec
toexplorehowwecandynamicallytradeoffbetweencontext 2.0:Aframeworkforself-supervisedlearningofspeech
clues(forchallengingutterances),andnon-contextlearning representations. InLarochelle,H.,Ranzato,M.,Hadsell,
9AnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
R.,Balcan,M.,andLin,H.(eds.),AdvancesinNeural 15-19September2019,pp.3490–3494.ISCA,2019. doi:
InformationProcessingSystems33:AnnualConference 10.21437/Interspeech.2019-1434. URLhttps://doi.
onNeuralInformationProcessingSystems2020,NeurIPS org/10.21437/Interspeech.2019-1434.
2020,December6-12,2020,virtual,2020.
Chennupati,G.,Rao,M.,Chadha,G.,Eakin,A.,Raju,A.,
Biadsy,F.,Chen,Y.,Zhang,X.,Rybakov,O.,Rosenberg, Tiwari,G.,Sahu,A.K.,Rastrow,A.,Droppo,J.,Oberlin,
A., andMoreno, P.J. Ascalablemodelspecialization A., Nandanoor, B., Venkataramanan, P., Wu, Z., and
framework for training and inference using submodels Sitpure,P. Ilasr:privacy-preservingincrementallearning
and its application to speech model personaliza- forautomaticspeechrecognitionatproductionscale. In
tion. ArXiv preprint, abs/2203.12559, 2022. URL KDD2022,2022.
https://arxiv.org/abs/2203.12559.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
Buciluaˇ,C.,Caruana,R.,andNiculescu-Mizil,A. Model BERT: Pre-training of deep bidirectional transformers
compression. InProceedingsofthe12thACMSIGKDD forlanguageunderstanding. InProceedingsofthe2019
international conference on Knowledge discovery and ConferenceoftheNorthAmericanChapteroftheAsso-
datamining,pp.535–541,2006. ciationforComputationalLinguistics:HumanLanguage
Technologies,Volume1(LongandShortPapers),pp.4171–
Chan, D. M. and Ghosh, S. Content-context factorized
4186, Minneapolis, Minnesota, 2019. Association for
representations for automated speech recognition. In
ComputationalLinguistics. doi:10.18653/v1/N19-1423.
Interspeech,2022.
URLhttps://aclanthology.org/N19-1423.
Chan,D.M.,Ghosh,S.,Chakrabarty,D.,andHoffmeister,
Dingliwal,S.,Sunkara,M.,Ronanki,S.,Farris,J.,Kirchhoff,
B. Multi-modal pre-training for automated speech
K., and Bodapati, S. Personalization of ctc speech
recognition. InICASSP,2022.
recognition models. In 2022 IEEE Spoken Language
Chan,D.M.,Ghosh,S.,Rastrow,A.,andHoffmeister,B. TechnologyWorkshop(SLT),pp.302–309.IEEE,2023.
Domainadaptationwithexternaloff-policyacousticcata-
Duarte-Torres, S., Sen, A., Rana, A., Drude, L., Gomez-
logsforscalablecontextualend-to-endautomatedspeech
Alanis, A., Schwarz, A., Ra¨del, L., and Leutnant, V.
recognition. InICASSP2023-2023,pp.1–5.IEEE,2023.
Promptformer: Prompted conformer transducer for
Chan, D. M., Ghosh, S., Tulsiani, H., Rastrow, A., asr. ArXiv preprint, abs/2401.07360, 2024. URL
and Hoffmeister, B. Task oriented dialogue as a https://arxiv.org/abs/2401.07360.
catalyst for self-supervised automatic speech recog-
Futami, H., Inaguma, H., Mimura, M., Sakai, S., and
nition. In ICASSP 2024 - 2024 IEEE International
Kawahara, T. Distilling the knowledge of bert for
Conference on Acoustics, Speech and Signal Pro-
ctc-based asr. ArXiv preprint, abs/2209.02030, 2022.
cessing (ICASSP), pp. 11806–11810, 2024. doi:
URLhttps://arxiv.org/abs/2209.02030.
10.1109/ICASSP48485.2024.10447164.
Chang,F.-J.,Liu,J.,Radfar,M.,Mouchtaris,A.,Omologo, Gao, H., Ni, J., Qian, K., Zhang, Y., Chang, S., and
M., Rastrow, A., and Kunzmann, S. Context-aware Hasegawa-Johnson,M. Wavprompt: Towardsfew-shot
transformertransducerforspeechrecognition. In2021 spoken language understanding with frozen language
ASRU,pp.503–510.IEEE,2021. models. ArXiv preprint, abs/2203.15863, 2022. URL
https://arxiv.org/abs/2203.15863.
Chang, K.-W., Tseng, W.-C., Li, S.-W., and Lee, H.-y.
Speechprompt: An exploration of prompt tuning on Gourav,A.,Liu,L.,Gandhe,A.,Gu,Y.,Lan,G.,Huang,
generativespokenlanguagemodelforspeechprocessing X., Kalmane, S., Tiwari, G., Filimonov, D., Rastrow,
tasks. ArXiv preprint, abs/2203.16773, 2022. URL A., et al. Personalization strategies for end-to-end
https://arxiv.org/abs/2203.16773. speechrecognitionsystems. InICASSP2021-2021IEEE
InternationalConferenceonAcoustics,SpeechandSignal
Chang, S.-Y. et al. Context-aware end-to-end asr using Processing(ICASSP),pp.7348–7352.IEEE,2021.
self-attentiveembeddingandtensorfusion. InICASSP
2023-2023,pp.1–5.IEEE,2023. Graves, A. Sequence transduction with recurrent neural
networks. arXivpreprintarXiv:1211.3711,2012.
Chen,Z.,Jain,M.,Wang,Y.,Seltzer,M.L.,andFuegen,C.
Jointgraphemeandphonemeembeddingsforcontextual Gulati,A.,Qin,J.,Chiu,C.,Parmar,N.,Zhang,Y.,Yu,J.,
end-to-end ASR. In Kubin, G. and Kacic, Z. (eds.), Han,W.,Wang,S.,Zhang,Z.,Wu,Y.,andPang,R. Con-
Interspeech2019,20thAnnualConferenceoftheInterna- former:Convolution-augmentedtransformerforspeech
tionalSpeechCommunicationAssociation,Graz,Austria, recognition. InMeng,H.,Xu,B.,andZheng,T.F.(eds.),
10AnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
Interspeech2020,21stAnnualConferenceoftheInterna- Jain,Y.,Chan,D.M.,Dheram,P.,Khare,A.,Shonibare,O.,
tionalSpeechCommunicationAssociation,VirtualEvent, Ravichandran,V.,andGhosh,S. Multi-stagemulti-modal
Shanghai,China,25-29October2020,pp.5036–5040. pre-training for automatic speech recognition. In
ISCA,2020. doi:10.21437/Interspeech.2020-3015. URL LREC-COLING,2024.
https://doi.org/10.21437/Interspeech.
Jayanthi,S.M.,Kulshreshtha,D.,Dingliwal,S.,Ronanki,
2020-3015.
S., and Bodapati, S. Retrieve and copy: Scaling asr
Hinton, G., Vinyals, O., and Dean, J. Distilling the personalizationtolargecatalogs. InEMNLP2023,2023.
knowledgeinaneuralnetwork. stat,1050:9,2015.
Kim, H., Na, H., Lee, H., Lee, J., Kang, T. G., Lee, M.,
Hori,T.,Moritz,N.,Hori,C.,andRoux,J.L. Transformer- and Choi, Y. S. Knowledge distillation using output
based long-context end-to-end speech recognition. In errors for self-attention end-to-end models. In IEEE
Meng,H.,Xu,B.,andZheng,T.F.(eds.),Interspeech2020, International Conference on Acoustics, Speech and
21stAnnualConferenceoftheInternationalSpeechCom- Signal Processing, ICASSP 2019, Brighton, United
municationAssociation,VirtualEvent,Shanghai,China, Kingdom,May12-17,2019,pp.6181–6185.IEEE,2019a.
25-29October2020,pp.5011–5015.ISCA,2020. doi: doi: 10.1109/ICASSP.2019.8682775. URL https:
10.21437/Interspeech.2020-2928. URLhttps://doi. //doi.org/10.1109/ICASSP.2019.8682775.
org/10.21437/Interspeech.2020-2928.
Kim, S. and Metze, F. Dialog-context aware end-to-end
speechrecognition. InSLT,2018.
Hori,T.,Moritz,N.,Hori,C.,andRoux,J.L. Advanced
long-contextend-to-endspeechrecognitionusingcontext-
Kim,S.,Dalmia,S.,andMetze,F. Gatedembeddingsin
expandedtransformers. InHermansky,H.,Cernocky´,H.,
end-to-endspeechrecognitionforconversational-context
Burget,L.,Lamel,L.,Scharenborg,O.,andMotl´ıcek,P.
fusion. In Proceedings of the 57th Annual Meeting
(eds.),Interspeech2021,22ndAnnualConferenceofthe
of the Association for Computational Linguistics, pp.
InternationalSpeechCommunicationAssociation,Brno,
1131–1141, Florence, Italy, 2019b. Association for
Czechia,30August-3September2021,pp.2097–2101.
ComputationalLinguistics. doi:10.18653/v1/P19-1107.
ISCA,2021. doi:10.21437/Interspeech.2021-1643. URL
URLhttps://aclanthology.org/P19-1107.
https://doi.org/10.21437/Interspeech.
2021-1643. Ku, P.-J., Chen, I.-F., Yang, H., Raju, A., Dheram, P.,
Ghahremani,P.,King,B.,Liu,J.,Ren,R.,andNidadavolu,
Hsu, W.-N., Tsai, Y.-H. H., Bolte, B., Salakhutdinov, R.,
P. Hot-fixingwakewordrecognitionforend-to-endasr
andMohamed,A. Hubert:Howmuchcanabadteacher
vianeuralmodelreprogramming. InICASSP2024,2024.
benefitasrpre-training? InICASSP,2021.
Kudo, T. and Richardson, J. SentencePiece: A simple
Huang,M.,You,Y.,Chen,Z.,Qian,Y.,andYu,K. Knowl- and language independent subword tokenizer and
edgedistillationforsequencemodel. InYegnanarayana, detokenizerforneuraltextprocessing. InProceedings
B.(ed.), Interspeech2018, 19thAnnualConferenceof of the 2018 Conference on Empirical Methods in
the International Speech Communication Association, NaturalLanguageProcessing: SystemDemonstrations,
Hyderabad,India,2-6September2018,pp.3703–3707. pp. 66–71, Brussels, Belgium, 2018. Association for
ISCA,2018. doi:10.21437/Interspeech.2018-1589. URL ComputationalLinguistics. doi:10.18653/v1/D18-2012.
https://doi.org/10.21437/Interspeech. URLhttps://aclanthology.org/D18-2012.
2018-1589.
Kurata,G.andSaon,G. Knowledgedistillationfromoffline
Hung, Y.-N., Yang, C.-H. H., Chen, P.-Y., and Lerch, A. to streaming RNN transducer for end-to-end speech
Low-resourcemusicgenreclassificationwithcross-modal recognition. InMeng,H.,Xu,B.,andZheng,T.F.(eds.),
neural model reprogramming. In ICASSP 2023-2023 Interspeech2020,21stAnnualConferenceoftheInterna-
IEEEInternationalConferenceonAcoustics,Speechand tionalSpeechCommunicationAssociation,VirtualEvent,
SignalProcessing(ICASSP),pp.1–5.IEEE,2023. Shanghai,China,25-29October2020,pp.2117–2121.
ISCA,2020. doi:10.21437/Interspeech.2020-2442. URL
Jaech,A.andOstendorf,M. Personalizedlanguagemodel
https://doi.org/10.21437/Interspeech.
forqueryauto-completion. InProceedingsofthe56th
2020-2442.
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pp. 700–705, Li, B., Pang, R., Zhang, Y., Sainath, T.N., Strohman, T.,
Melbourne, Australia, 2018.AssociationforComputa- Haghani,P.,Zhu,Y.,Farris,B.,Gaur,N.,andPrasad,M.
tional Linguistics. doi: 10.18653/v1/P18-2111. URL Massivelymultilingualasr:Alifelonglearningsolution.
https://aclanthology.org/P18-2111. InICASSP2022-2022IEEEInternationalConferenceon
11AnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
Acoustics,SpeechandSignalProcessing(ICASSP),pp. Parthasarathi,S.H.K.andStrom,N. Lessonsfrombuilding
6397–6401.IEEE,2022. acoustic models with a million hours of speech. In
IEEE International Conference on Acoustics, Speech
Lin, R., Liu, S., Yang, M., Li, M., Zhou, M., and Li, S.
andSignalProcessing,ICASSP2019,Brighton,United
Hierarchical recurrent neural network for document
Kingdom,May12-17,2019,pp.6670–6674.IEEE,2019.
modeling. In Proceedings of the 2015 Conference on
doi: 10.1109/ICASSP.2019.8683690. URL https:
Empirical Methods in Natural Language Processing,
//doi.org/10.1109/ICASSP.2019.8683690.
pp. 899–907, Lisbon, Portugal, 2015. Association for
ComputationalLinguistics. doi:10.18653/v1/D15-1106. Pham, H. et al. Combined scaling for zero-shot transfer
URLhttps://aclanthology.org/D15-1106. learning. Neurocomputing,pp.126658,2023.
Liu, B. and Lane, I. Dialog context language model- Pham, M., Cho, M., Joshi, A., andHegde, C. Revisiting
ing with recurrent neural networks. In 2017 IEEE self-distillation. ArXivpreprint,abs/2206.08491,2022.
International Conference on Acoustics, Speech and URLhttps://arxiv.org/abs/2206.08491.
Signal Processing, ICASSP 2017, New Orleans, LA,
USA, March 5-9, 2017, pp. 5715–5719. IEEE, 2017. Radford,A.,Kim,J.W.,Xu,T.,Brockman,G.,McLeavey,C.,
doi: 10.1109/ICASSP.2017.7953251. URL https: andSutskever,I.Robustspeechrecognitionvialarge-scale
//doi.org/10.1109/ICASSP.2017.7953251. weaksupervision. ArXivpreprint,abs/2212.04356,2022.
URLhttps://arxiv.org/abs/2212.04356.
Masumura, R., Makishima, N., Ihori, M., Takashima, A.,
Tanaka, T., and Orihashi, S. Hierarchical transformer- Radford,A.,Kim,J.W.,Xu,T.,Brockman,G.,McLeavey,
based large-context end-to-end asr with large-context C., and Sutskever, I. Robust speech recognition via
knowledge distillation. In ICASSP 2021-2021 IEEE large-scaleweaksupervision. InICML,2023.
InternationalConferenceonAcoustics,SpeechandSignal
Radosavovic, I., Dolla´r, P., Girshick, R.B., Gkioxari, G.,
Processing(ICASSP),pp.5879–5883.IEEE,2021.
andHe,K. Datadistillation: Towardsomni-supervised
Mehrabi,N.,Goyal,P.,Dupuy,C.,Hu,Q.,Ghosh,S.,Zemel, learning. In2018IEEEConferenceonComputerVision
R., Chang, K.-W., Galstyan, A., and Gupta, R. Flirt: and Pattern Recognition, CVPR 2018, Salt Lake City,
Feedbackloopin-contextredteaming. InarXiv, 2023. UT,USA,June18-22,2018,pp.4119–4128.IEEECom-
URLhttps://arxiv.org/abs/2308.04265. puter Society, 2018. doi: 10.1109/CVPR.2018.00433.
URL http://openaccess.thecvf.com/
Mitra, S. et al. Unified modeling of multi-domain content_cvpr_2018/html/Radosavovic_
multi-deviceASRsystems. InTSD,2023. Data_Distillation_Towards_CVPR_2018_
paper.html.
Mun’im, R. M., Inoue, N., and Shinoda, K. Sequence-
level knowledge distillation for model compression of Robinson, J. D., Chuang, C., Sra, S., and Jegelka, S.
attention-basedsequence-to-sequencespeechrecognition. Contrastive learning with hard negative samples. In
InIEEEInternationalConferenceonAcoustics,Speech 9th International Conference on Learning Repre-
andSignalProcessing,ICASSP2019,Brighton,United sentations, ICLR 2021, Virtual Event, Austria, May
Kingdom,May12-17,2019,pp.6151–6155.IEEE,2019. 3-7, 2021. OpenReview.net, 2021. URL https:
doi: 10.1109/ICASSP.2019.8683171. URL https: //openreview.net/forum?id=CR1XOQ0UTh-.
//doi.org/10.1109/ICASSP.2019.8683171.
Sathyendra, K. M., Muniyappa, T., Chang, F.-J., Liu, J.,
Munkhdalai,T.,Sim,K.C.,Chandorkar,A.,Gao,F.,Chua,
Su,J.,Strimel,G.P.,Mouchtaris,A.,andKunzmann,S.
M., Strohman, T., and Beaufays, F. Fast contextual
Contextualadaptersforpersonalizedspeechrecognition
adaptationwithneuralassociativememoryforon-device in neural transducers. In ICASSP 2022-2022, pp.
personalized speech recognition. In ICASSP, pp.
8537–8541.IEEE,2022.
6632–6636.IEEE,2022.
Schwarz, A., He, D., Van Segbroeck, M., Heth-
Novotney, S., Mukherjee, S., Ahmed, Z., andStolcke, A.
nawi, M., and Rastrow, A. Personalized predic-
CUE vectors: Modular training of language models
tive asr for latency reduction in voice assistants.
conditionedondiversecontextualsignals. InFindings
ArXiv preprint, abs/2305.13794, 2023. URL
oftheAssociationforComputationalLinguistics: ACL
https://arxiv.org/abs/2305.13794.
2022,pp.3368–3379,Dublin,Ireland,2022.Association
forComputationalLinguistics. doi: 10.18653/v1/2022. Shenoy, A., Bodapati, S., and Kirchhoff, K. Con-
findings-acl.265. URL https://aclanthology. textual biasing of language models for speech
org/2022.findings-acl.265. recognition in goal-oriented conversational agents.
12AnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
ArXiv preprint, abs/2103.10325, 2021. URL Yang,C.-H.H.,Gu,Y.,Liu,Y.-C.,Ghosh,S.,Bulyko,I.,
https://arxiv.org/abs/2103.10325. and Stolcke, A. Generative speech recognition error
correctionwithlargelanguagemodelsandtask-activating
Shor,J.,Emanuel,D.,Lang,O.,Tuval,O.,Brenner,M.P., prompting. In2023IEEEAutomaticSpeechRecognition
Cattiau, J., Vieira, F., McNally, M., Charbonneau, T., andUnderstandingWorkshop(ASRU),December2023a.
Nollstadt,M.,Hassidim,A.,andMatias,Y. Personalizing
Yang,C.-H.H.,Li,B.,Zhang,Y.,Chen,N.,Prabhavalkar,
ASR for dysarthric and accented speech with limited
data. In Kubin, G. and Kacic, Z. (eds.), Interspeech R.,Sainath,T.N.,andStrohman,T. Fromenglishtomore
2019, 20th Annual Conference of the International languages:Parameter-efficientmodelreprogrammingfor
Speech Communication Association, Graz, Austria, cross-lingualspeechrecognition. InICASSP2023-2023
IEEEInternationalConferenceonAcoustics,Speechand
15-19September2019,pp.784–788.ISCA,2019. doi:
10.21437/Interspeech.2019-1427. URLhttps://doi. SignalProcessing(ICASSP),pp.1–5.IEEE,2023b.
org/10.21437/Interspeech.2019-1427.
Yu, J., Han, W., Gulati, A., Chiu, C., Li, B., Sainath,
T. N., Wu, Y., and Pang, R. Dual-mode ASR: unify
Sun,C.,Ahmed,Z.,Ma,Y.,Liu,Z.,Pang,Y.,andKalinli,O.
and improve streaming ASR with full-context mod-
Contextualbiasingofnamed-entitieswithlargelanguage
eling. In 9th International Conference on Learning
models. ArXiv preprint, abs/2309.00723, 2023. URL
Representations, ICLR 2021, Virtual Event, Austria,
https://arxiv.org/abs/2309.00723.
May 3-7, 2021. OpenReview.net, 2021. URL https:
//openreview.net/forum?id=Pz_dcqfcKW8.
Tang, J., Kim, K., Shon, S., Wu, F., Sridhar, P., and
Watanabe, S. Improving asr contextual biasing with Zhang, L., Song, J., Gao, A., Chen, J., Bao, C., and Ma,
guidedattention. ArXivpreprint,abs/2401.08835,2024. K. Be your own teacher: Improve the performance
URLhttps://arxiv.org/abs/2401.08835.
of convolutional neural networks via self distillation.
In 2019 IEEE/CVF International Conference on
Tsunoo, E., Kashiwagi, Y., and Watanabe, S. Streaming
Computer Vision, ICCV 2019, Seoul, Korea (South),
transformerasrwithblockwisesynchronousbeamsearch.
October27-November2,2019,pp.3712–3721.IEEE,
In2021IEEESpokenLanguageTechnologyWorkshop
2019. doi: 10.1109/ICCV.2019.00381. URLhttps:
(SLT),pp.22–29.IEEE,2021.
//doi.org/10.1109/ICCV.2019.00381.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones, Zhang, T., Ramakrishnan, R., and Livny, M. Birch: A
L.,Gomez,A.N.,Kaiser,L.,andPolosukhin,I. Attention newdataclusteringalgorithmanditsapplications. Data
isallyouneed. InGuyon,I.,vonLuxburg,U.,Bengio, miningandknowledgediscovery,1:141–182,1997.
S.,Wallach,H.M.,Fergus,R.,Vishwanathan,S.V.N.,
Zhao,D.,Sainath,T.N.,Rybach,D.,Rondon,P.,Bhatia,D.,
andGarnett,R.(eds.),AdvancesinNeuralInformation
Li,B.,andPang,R.Shallow-fusionend-to-endcontextual
Processing Systems 30: Annual Conference on Neural
biasing. InKubin,G.andKacic,Z.(eds.),Interspeech
Information Processing Systems 2017, December 4-9,
2019, 20th Annual Conference of the International
2017,LongBeach,CA,USA,pp.5998–6008,2017.
SpeechCommunicationAssociation,Graz,Austria,15-19
Vitter, J. S. Random sampling with a reservoir. ACM September 2019, pp. 1418–1422. ISCA, 2019. doi:
TransactionsonMathematicalSoftware(TOMS),11(1): 10.21437/Interspeech.2019-1209. URLhttps://doi.
37–57,1985.
org/10.21437/Interspeech.2019-1209.
Wei,K.etal. Attentivecontextualcarryoverformulti-turn
end-to-end spoken language understanding. In 2021
ASRU,pp.837–844.IEEE,2021.
Williams, I., Kannan, A., Aleksic, P. S., Rybach, D.,
and Sainath, T. N. Contextual speech recognition
in end-to-end neural network systems using beam
search. In Yegnanarayana, B. (ed.), Interspeech 2018,
19th Annual Conference of the International Speech
Communication Association, Hyderabad, India, 2-6
September 2018, pp. 2227–2231. ISCA, 2018. doi:
10.21437/Interspeech.2018-2416. URLhttps://doi.
org/10.21437/Interspeech.2018-2416.
13AnEfficientSelf-LearningFrameworkForInteractiveSpokenDialogSystems
A.Insertions/Deletions/SubstitutionsinCLC/Ohm
WecanfurtherbreakdowntheperformanceinTable6intermsofinsertions,deletionsandsubstitutions,whichisgiven
inTableA.1. WecanseethataddingCLClosssignificantlyimprovestherateofdeletioncomparedtobaselinemodels.
Unfortunately,thiscomesatthecostofimprovementinsubstitutionandinsertion. CLC,insteadofdoingthebestjobof
disambiguatinggeneratedtokens,focusesonrecallasopposedtoprecision. Ohmimprovesthedisambiguation,asatthe
costofdeletions:moretokensaredropped,butthetokensthatarepreservedaremoreaccurate.
TableA.1: WERRwhennormalizedbythedomain(insteadofby-utterance)ontheALLdataset.WERR(↑):WordErrorRateReduction.
SERR(↑):SentenceErrorRateImprovement.INSR:RelativeInsertionrate.SUBR:RelativeSubstitutionRate.DELR:RelativeDeletion
Rate
Model Context CLC Ohm WERR SERR SUBR INSR DELR
(cid:33) - - - - - - -
Teacher(200M) (cid:33) (cid:33) - 3.75 2.40 1.56 0.82 13.65
(cid:33) (cid:33) (cid:33) 6.64 7.79 3.19 1.70 6.18
14