Incorporating Classifier-Free Guidance in Diffusion
Model-Based Recommendation
Noah Buchanan, Susan Gauch, Quan Mai
Department of Electrical Engineering and Computer Science
University of Arkansas
Fayetteville, USA
{njb004, sgauch, quanmai}@uark.edu
Abstract—Thispaperpresentsadiffusion-basedrecommender forabroadscopeoftasksforsometime.Despitetheirsuccess,
system that incorporates classifier-free guidance. Most current GANsandVAEsremainonlypartiallycompetitivewithrecent
recommender systems provide recommendations using conven-
state-of-the-art techniques - diffusion.
tional methods such as collaborative or content-based filtering.
Diffusion-based techniques outperform other recent tech-
Diffusion is a new approach to generative AI that improves on
previous generative AI approaches such as Variational Autoen- niques in the visual fidelity of images and their understanding
coders(VAEs)andGenerativeAdversarialNetworks(GANs).We of textual prompts. Additionally, they excel in the range of
incorporate diffusion in a recommender system that mirrors the generated images and their ability to create variations of the
sequence users take when browsing and rating items. Although
same image while still accurately following the guidelines
a few current recommender systems incorporate diffusion, they
provided by the text prompt. Diffusion models (DMs) are
do not incorporate classifier-free guidance, a new innovation in
diffusionmodelsasawhole.Inthispaper,wepresentadiffusion based on the idea of diffusion in the natural sciences and
recommendersystemthataugmentstheunderlyingrecommender are heavily inspired by non-equilibrium thermodynamics. The
system model for improved performance and also incorporates process of diffusion in natural science is the movement of
classifier-free guidance. Our findings show improvements over
solutes and molecules from a high concentration to a lower
state-of-the-artrecommendersystemsformostmetricsforseveral
concentration. As the name suggests, DMs attempt to mimic
recommendation tasks on a variety of datasets. In particular,
our approach demonstrates the potential to provide better this process using Gaussian noise instead as our measure of
recommendations when data is sparse. concentration, a fully noised image being the parallel of a
Index Terms—recommender system, generative AI, diffusion, perfectly diffused liquid. The underlying concept is that, in
classifier-free guidance
actual diffusion, regardless of where or how dye is introduced
intoacontainerofwater,thedyewilleventuallydiffuseevenly
I. INTRODUCTION
throughout the container, producing a consistent result each
Theintroductionofgenerativemodelshasmarkedaturning time. DMs replicate this process, but instead of producing
point in the capabilities of AI and it gives us a glimpse a uniform distribution, they recreate a distribution of the
into the distances we can go using such tools. Autoencoders, original data from random Gaussian noise. Leveraging this
a foundational generational technology, learn a compressed principle, diffusion-based techniques have achieved unparal-
format of information to pass into neural models. However, leled results in text-to-image synthesis and general image
thelatentspacecreatedbytheautoencoder’scompressionmay generation. While text-to-image is likely the most popular
contain parts that do not correspond to any data point in the use case, they are capable of other tasks, such as text-to-text
original input, limiting their generative capabilities. Because (sequence-to-sequence) or text-to-video. One such use case is
the latent space is not regularized, autoencoder models only recommendation.
havegenerativecapabilitiesthatcorrespondtogeneratingdata Thecurrentchallengewithgenerativerecommendermodels
wehaveseenbefore.Duringinference,whenthetrainingdata lies in the limited capablities of current approaches that
isnotavailable,wehavenowayofknowingwhetherornotthe primarily rely on GANs and VAEs. GANs are plagued by
output generated is garbage. To address this, Variational Au- instability issues [29], whereas VAEs face a trade-off be-
toencoders (VAEs) [28] and Generative Adversarial Networks tweentractabilityandthequalityoftheirrepresentations[30].
(GANs)[31]weredeveloped,bothofwhichhavebeenshown In contrast, Diffusion Models (DMs) do not encounter the
tobecapableofawiderangeofofgenerationcapabilities.[31] aforementioned issues and therefore represent a promising
[28] [30]. Text-to-image synthesis is one such capability and, solutionforgenerativerecommendersystems(RSs).However,
not coincidentally, a very popular area of research. The idea the current implementation of diffusion RSs is limited by
ofapipelinecapableoftransformingourhumanlanguageinto their failure to incorporate recent innovations in DMs, i.e.,
anotherformofmediathathasnoneoftheeaseofcreationthat classifier-free guidance. Essentially, classifier-free guidance,
language does is an extremely appealing concept; as a result, as implied by its name, enhances the generative capability of
there has been an explosion of research in this area in recent DMs without the need for an external classifier. Our contribu-
years.GANsandVAEswerethemaingenerativemodelsused tions are threefold. First, we develop an implementation that
4202
peS
61
]RI.sc[
1v49401.9042:viXraseamlessly integrates classifier-free guidance into diffusion- to now and became one of, if not the most, popular datasets
based RSs. Second, we provide an in-depth analysis of the for recommendation studies.
architecture of the employed models. Third, we offer insights AI-based methods have been very popular, with the explo-
and solutions to the challenges posed by few-shot and zero- sion of AI research in the last 20 years being an obvious
shot scenarios, which frequently hamper the performance of indicator of its soon-to-be application to the recommendation
RSs. domain.Theapplicationinthisfielddidseesuccessasearlyas
SectionIIprovidesasummaryofrelatedwork,situatingour 2006and2009,inspiredbytheNetflixPrize,matrixfactoriza-
contributions within the existing literature. In Section III, we tion models that would learn a user embedding matrix and an
detail the process of integrating Classifier-Free Guidance into item embedding matrix; using these embedding matrices, we
the implementation of a diffusion RS, originally described in can find similarities using the measure of closeness we talked
our inspiration paper, and explain the inner workings of our aboutearlier,inthiscasetypicallyitwouldbethedotproduct
enhanced RS. Section V presents the results of our modifi- [22], [23]. Other techniques were created during this period,
cations and compares them to the outcomes from unmodified and linear models for this task gained some popularity. [24]
implementations. Finally, Section VI offers conclusions and presented a logistic regression model that achieved tangible
discusses potential avenues for future research to further improvements on new user-centric evaluation methods, such
advance the capabilities of diffusion-based RSs. as errors in click-through rate estimation. [25] introduced a
VAEforcollaborativefilteringwithsuccessontheMovieLens
II. BACKGROUNDANDRELATEDWORK and Netflix Prize datasets. [26] showed that GANs are also
capable of creating a recommender model. Models for video
In this section, we discuss RSs and DMs and how these
recommendation were also created following the introduction
fieldscombine.Wealsogivesomebackgroundonatechnique
ofdeep learningfor RSs[27],showing thatdeep modelshave
used in DMs that has seen significant success in recent
asurprisingabilitytorecommendvideosandthecapabilityto
literature,knownasclassifier-freeguidance,whichisthebasis
perform both candidate generation and ranking.
for the improvement demonstrated in this paper.
B. Diffusion
A. Recommender Systems
The first paper that broaches the topic of diffusion mod-
RSsareacrucialpartofhowbusinessesoperatetoday,with els coined “Deep Unsupervised Learning using Nonequilib-
users providing information in return for recommendations rium Thermodynamics”, aimed to attack the tradeoff between
on products, whether items from a store or shows/movies. tractabilityandflexibilitythatprobabilisticmodelshistorically
The foundation for RSs was generally considered to be first are afflicted by [4]. The next major breakthrough in DMs was
introducedbyElaineRichin1979throughasystemshecalled achievedbytheworkin[5],whichmarkedthefirstsignificant
Grundy [1]. She wanted a way to recommend users’ books adoption of this model for high-quality image generation.
by asking them specific questions, in turn classifying them This study not only demonstrated that DMs are capable of
into classes of preferences depending on their answers. Jussi producing high-quality images, competitive with state-of-the-
Karlgren improved upon Elaine Rich’s system by introducing art methods on the CIFAR-10 dataset, but also highlighted
ameasureof“closeness”toattempttoimitatetheoccurrences the efficacy of class-conditional models that use a classifier
in bookcases where interesting documents often happen to to guide generation towards higher fidelity on the same task.
be found adjacent to each other regardless of a relatively The influential findings of this paper catalyzed widespread
unordered bookcase [2]. This idea of closeness was adopted adoption, leading to numerous subsequent improvements.
and used in a variety of models after its inception. The first DMs, already known for the prowess in unconditional image
information filtering system based on collaborative filtering generation, have shown superior performance in various other
through human evaluation was introduced by [3], where they areas. Especifically, they have become the de-facto state-of-
would make recommendations based on similarities between the-art in class-conditional generation, particularly with the
the interest profile of a user in question and other users; incorporation of classifier guidance [7] [6]. Additional, DMs
the concept was coined “weaving an information tapestry.” have begun to dominate the domain of super-resolution [8].
Inspired by the results of the study, researchers from MIT The advancements prompted by this major adoption extended
and the University of Minnesota (UMN) developed a new beyond the models’ use cases and capabilities to their per-
recommendation service called GroupLens, which features a formance optimization. Techniques such as latent diffusion
user-to-user collaborative filtering model [20]. Following the models, introduced in the widely acclaimed Stable Diffusion
publication of these initial findings, the GroupLens research paper [9], have been developed to enhance these models.
lab was established at UMN and became a pioneering force Latentdiffusionmodelsleveragethelatentspaceconceptfrom
in the field of recommender system studies. The GroupLens autoencoders: data is compressed into the latent space before
research lab subsequently launched the MovieLens project, the forward process, and both forward and reverse Markov
which led to the creation of the first version of their rec- transitions occur in this reduced dimensionality. The result is
ommender model [21]. As a result of this project and its then upscaled back to the original size before output. This
success, several MovieLens datasets were released from then paper gained significant attention not only within the researchcommunity but also among non-researchers, as it provided the same year, several other diffusion recommender models
open-access resources to code and model weights, enabling emerged, employing varied approaches to achieve similar
widespread use with proper guidance. Further improvements tasks. For instance, [16] introduced a conditional diffusion-
borrowed techniques from VAEs. For instance, [13] demon- based RS using a cross-attentive denoising decoder to remove
strated that Vector Quantization in the latent space could noise. Unlike previous approaches that relied on guidance
enhance the efficiency of both training and sampling speeds. via partially corrupted data, this method employed direct
AnotherinnovativeapproachbyateamatSnapchatintroduced conditioning. This inspired other papers, such as [18], which
step-distillationtodecreasethenumberofstepsrequiredinthe developed sequential recommendation methods using DMs.
denoisingprocess.ThisleadtothecreationoftheSnapFusion Additionally,[17]exploredtheapplicationofDMsforrerank-
model,whichboaststhecapabilityofimagegenerationwithin ing purposes in RSs, achieving notable results.
two seconds on mobile devices [14].
Large language models (LLMs) have represented a signif- D. Classifier-Free Guidance
icant breakthrough in AI research, demonstrating exceptional
Classifierguidance[6]modifiesthediffusionscore,specif-
performance across various applications, including chatbots,
ically the gradient of the log probability density function,
content generation, language translation, text summarization,
which is more tractable to learn than directly modeling the
question-answering systems, and personalized recommenda-
data distribution. This approach employs a classifier to guide
tions. This breakthrough has coincided with the rise in
the generation process by increasing the probability of data
popularity of prompt-based conditional DMs, making it an
thattheclassifierassignsahighlikelihoodtothecorrectlabel.
opportune moment to merge LLMs’ textual understanding
As demonstrated by [19], data that is well-classified tends to
capabilities with these advanced image generation models.
exhibit high perceptual quality, contributing to superior image
Google explored this integration and discovered that large
generation outcomes.
language models were surprisingly effective at encoding text
Despite its effectiveness in balancing precision and recall,
for image synthesis. This led to the development of a model
classifier guidance depends on the gradients of an image
namedImagen[10],whichexemplifiedthepotencyofcombin-
classifier, limiting the variability of generated images. This
ing these techniques. Similarly, OpenAI’s DALL-E 2 model
dependency raises the question of whether it is possible to
employed large language models with diffusion techniques,
achieve comparable or superior guidance without relying on a
usingaCLIPmodeltogenerateimagesbasedonitsenhanced
classifier.
language-image understanding [11]. Although the third itera-
Classifier-free guidance, introduced by [7], addresses this
tion of DALL-E continued this approach, it uncovered some
issue by eliminating the need for a dedicated classifier. In-
limitations; specifically, the imperfections in image captions
stead, it involves training an unconditional denoising diffu-
were found to introduce flaws in the generated images. This
sion model, parameterized by a score estimator, alongside
highlighted the critical role of the language component in the
a conditional denoising diffusion model, parameterized by a
effectiveness of these models [12].
conditional score estimator. These are parameterized using
C. Diffusion-Based Recommender Systems a single neural network. For the unconditional model, a
null token is used for the class identifier. The models are
The success of DMs and the subsequent expansion of
jointly trained, with the conditioning randomly set to a null
their use cases have led to increased interest in diffusion-
token based on a hyperparameter probability p-uncond. This
based RSs. While GANs and VAEs are commonly utilized
methodology not only simplifies the model architecture but
to model the generative process of user interactions, they
also enhances guidance capabilities without the dependence
share similar limitations with their traditional counterparts.
on an external classifier.
With diffusion models showing promise in overcoming these
issues within the domain of image generation, it is reasonable
III. METHODOLOGY
to hypothesize that they could address these shortcomings in
generative recommender systems as well. The study by [15] Theapproachadoptedinthispaperconsistsoftwoprinciple
provides compelling evidence in support of this hypothesis. components, detailed in green and blue in Fig.1. The first
This research introduced one of the first diffusion-based rec- componentisthediffusionprocess,encompassingtheforward
ommender models, arguing that the objectives of RSs align and reverse processes as well as the sampling procedure.
wellwiththemethodologicalframeworkofDMs.Specifically, Our methodology is inspired by [15], which serves as our
RSs aim to infer future interaction probabilities based on baseline. In contrast to the original work, which relies solely
corrupted historical interactions, a process analogous to the on partially corrupted original data to guide the denoising
noise-corruption mechanism inherent in DMs. Their approach modelduringthereverseprocess(pseudo-guidance),thisstudy
did not involve any direct conditioning, instead, the amount requires an adaptation to implement classifier-free guidance.
of noise added to the data was limited so that the data was In this section, we will elucidate the implementation of this
corrupted but not completely indecipherable. external guidance and explore the options available for this
Although diffusion-based recommendation is a relatively type of task. Furthermore, we will discuss the modifications
new research area, it is rapidly gaining traction. Within made to the model architecture both independently and as aFig.1. DiagramofOurRecommenderSystem
resultofthenewformofguidance,andelaborateonthework- In our work, we employ a linear scheduling approach,
ings of the diffusion process. By thoroughly examining these which has gained significant traction in recent research. As
components, we aim to underscore the necessity and efficacy the term suggests, a linear schedule refers to a sequential
of incorporating classifier-free guidance into diffusion-based method for incrementally adding noise to an image or, in
RSs. our specific application, to a sequence of interactions. This
The use of partially-noised data for guidance [15] is inher- process can be modeled as a Markov chain, where each step
ently incompatible with the classifier-free guidance paradigm. in the sequence is dependent on the previous one to ensure
To address this, instead of using partially noised data, we the accurate introduction of noise. Although this iterative
employedpre-noiseddatatoderiveguidance.Thisnovelform approach is characteristic of Markov chains, it could result
ofguidance,whichwillbeelaborateduponlaterinthissection, in inefficiencies during training due to its inherently slow
enablestheseamlessintegrationofclassifier-freeguidanceinto progression. However, this issue is circumvented by adopting
the training process. By implementing this revised approach, the reparameterization technique introduced in the Stable
we aim to enhance the efficacy and flexibility of diffusion- Diffusion framework by [5]. This method allows us to bypass
based RSs. theneedforiterativesamplingateachtimestep,enablingusto
generate the desired output in a single step using the formula:
A. Diffusion Process
√ √ √
The diffusion process consists of a forward process and a q(x |x )= α¯ x + 1−α¯ ϵ=N(x ; α¯ x ,(1−α¯ )I)
t 0 t 0 t t t 0 t
reverseprocess.Giventhatourmodificationsprimarilyimpact (2)
the reverse process and the denoising model, we will provide With this new formula, we can efficiently calculate the
an in-depth exploration of the reverse process in the subse- noise to be added at any arbitrary timestep without the need
quent section. In this section, we will offer a brief overview to sequentially traverse all preceding steps. This significantly
of how it integrates within the overall diffusion framework. accelerates the training process.
The notations for the forward and reverse processes differ.
B. Denoising Model/Reverse Process
The forward process is described as follows:
The primary modifications pertain to the denoising model.
q(x |x )=N(x ,(cid:112) 1−β x ,β I) (1) Inthissection,wewillthoroughlyexaminethesemodifications
t t−1 t t t|1 t
and their implementation. First, we will elucidate the purpose
where x represents the output of the forward process of the denoising model in diffusion models and discuss any
t
at step t, the output of step t − 1 (x ) is the input at differencesinitsapplicationforRSs.Wewillthenidentifythe
t−1
time step t. N denotes a normal distribution. In the reverse shortcomingsoftheoriginaldenoisingmodelinincorporating
process, denoted as p, we predict the noise to reconstruct classifier-free guidance and explain the steps taken to address
q, whereas q is a predetermined function for the forward these issues.
process. For the forward process at timestep t, modeled as The denoising model is fundamental to reversing the noise
a normal distribution, we define it by its mean and variance introducedintheforwardprocess.Inimagediffusionmodels,a
parameters, respectively. β in both mean and variance refers U-Netarchitectureiscommonlyemployedduetoitseffective-
t
to a scheduler. nessinnoiseremovaltasks.However,giventhatourdatasetisnotasextensiveastypicalimagedata,wecanoptforasimpler concatenationofthedataandguidance,wasthemosteffective.
architecture, such as a straightforward feedforward network. The embedding size from the original architecture was main-
The primary function of the denoising model is to predict tained, so the concatenated data is transformed to dimensions
the noise at each timestep, facilitating the reversal process of (batch size × embedding size). This transformed data is
until timestep 0 (fully reversed) is reached. Once the noise is then run through a hyperbolic tangent activation function and
predicted, only a portion of that predicted noise, proportional subsequently through another linear layer to project it back to
to the timestep, is removed. Building upon the equation for the dimensions corresponding to the number of items in the
thereverseprocessasoutlinedintheoriginalStableDiffusion dataset. This process provides a heatmap of outputs for each
paper [5] (Eq. 3, 4), our modifications adapt this framework item in the dataset. The model remains relatively lightweight,
to our specific needs. and the rationale behind this design choice will be elaborated
upon in Section IV-B.
T
(cid:89)
p (x :T):=p(x ) p (x |x ) (3)
θ 0 T θ t−1 t IV. EXPERIMENTALSETUP
t=1
In this section, we will discuss the specifics of the datasets
(cid:88)
p θ(x t−1|x t)=N(x t−1;µ θ(x t,t), (x t,t)) (4) usedandhowtheypertaintoourtrainingandtesting.Wewill
θ alsodiscusstheconfigurationoftheexperimentsweranwhen
where p (x : T), known as the diffusion process, is a discovering the best method for personalizing recommenda-
θ 0
chain of Gaussian transitions starting at p(x ) that iterates tions through conditioning and finding the best denoising
T
to T using p (x |x ) equation for each singular step. The model architecture. Lastly, the actual results of our chosen
θ t−1 t
varianceinEq.4istime-dependent,butisnottrainable.Rather architecture and method of guidance in comparison to the
than being set to a constant, it is equal to β I from the original implementation that we based ours upon, as well as
T
scheduledescribedintheforwardprocess.Wereferreadersto the special use cases we have identified that our modified
[5] for details on mean estimation and other specific aspects. system excels in.
The output for a given timestep t is calculated by:
A. Datasets
x = √1 (x − √ β t ϵ (x ,t))+(cid:112) β ϵ (5) We report results for three datasets: the MovieLens 1
t−1 t θ t t
αt 1−α¯
Milliondataset[32],theYelpdataset[34],andseveralAmazon
where ϵ (x ,t) is our model’s predicted noise. We simply review datasets [33]. The inspiration paper specifies two
θ t
use this to reverse the diffusion process iteratively using the training formats: clean and noisy. While our modifications
predicted noise from our denoising model. were tested in both formats, all externally created datasets
Theoriginalmodelarchitectureemployedastraightforward were limited to the clean format. No significant differences
pipeline where the input was passed through an embedding were observed between our modified implementation and the
layer and then to an output layer. While this architecture original method for either training format. Datasets statistics
was sufficient for their implementation, we introduced several are shown in Table I.
modifications to enhance performance, particularly with the For the MovieLens and Yelp datasets, noisy and clean set-
integration of classifier-free guidance. Firstly, the input to the tingsareavailable.However,duetocomputationalconstraints,
model was revised to include an unnoised version of the data we only employed the MovieLens noisy configuration; the
batch, which serves as guidance for conditioning. Previously, Yelpnoisydatasetwasexcluded.Cleantraininginvolvesusing
partially noised data acted as pseudo-guidance, but with the onlyratingsof4or5;suchratingsareconsidered“hits,”while
new approach, we require explicit external guidance. There- ratingsof3orbelowaredeemed“misses.”Hitsarerepresented
fore, the forward method now takes both the noisy batch of as1inavectoroflengthN (numberofuniqueitems),whereas
data and the corresponding unnoised batch (hereafter referred misses and unrated items are labeled as 0.
toasguidance).Additionally,aparameter,p uncond,pertinent The numeric ratings in the MovieLens 1 Million and Yelp
to classifier-free guidance, was introduced. This parameter datasets facilitate straightforward classification of hits and
represents the proportion of data where the gradients from misses.Incontrast,theAmazondatasetslacknumericratings,
theguidancewillbezero,effectivelyenablingnon-conditional leading us to consider all provided reviews as hits. Although
prediction. In this paper, p uncond = 0.2, meaning that this introduces some noise, it does not significantly affect the
20% of the guidance data will have zero gradients. The differencesbetweenourmodificationsandtheoriginalmethod.
data is then normalized and concatenated with the noised The data was split with an 80:20 ratio for training and test-
data (x). This process trains the denoising model to generate ing, or 70:20:10 for training, validation, and testing. The last
recommendations with and without guidance simultaneously, segment of each sequence was reserved for testing. Datasets,
thereby enhancing the model’s capability, as supported by primarily containing user ID, item ID, ratings (1-5), review
the principles of classifier-free guidance. Although there are time, and extraneous data, were preprocessed by discarding
various methods to condition the denoising model (e.g., at- entries with ratings of 3 or below. Subsequently, the rating
tention mechanisms), we found that the simplest method, column was removed.TABLEI resultingrecommendationsweresubparandnotcomparableto
DATASETSTATISTICS otherresults,leadingustoabandonthisapproach.Thereasons
for this performance decline remain unclear.
Dataset Users Items TotalReviews
ML-1MClean 5949 2810 403,277 Through our experiments, we identified a general rule: a
ML-1MNoisy 5949 3494 429,993 significantly larger parameter count tends to reduce effec-
YelpClean 54,574 34,395 1,014,486
tiveness, likely due to overfitting during testing. Given the
AmazonKitchen 11,566 7,722 100,464
AmazonBeauty 3,782 2,658 38,867 iterative nature of the denoising process, a large model was
AmazonOffice 1,719 901 21,466 unnecessary for the incremental removal of noise from the
AmazonToys 2,676 2,474 26,850 data. Consequently, configurations with additional layers or
AmazonVG 5,435 4,295 60,497
increased neurons per layer did not perform well.
Wealsoexperimentedwithattentionmechanisms,including
self-attentionfortheinputsequenceandcross-attentivecondi-
For datasets without numeric ratings, all reviews were tioning from the guidance. Overall, the inclusion of attention
assumed positive, with allowances for minor noise. Data did not yield the expected benefits, leading us to opt for a
was sorted by user ID and review time, ensuring sequential simpler concatenation method to incorporate guidance with
interactions per user starting with the oldest. User and item input data. This may be attributed to the earlier rule that
IDs were renumbered from zero to maintain accurate counts moreparametersdonotnecessarilyenhanceperformance,and
post-cleaning. attention mechanisms inherently add parameters.
The preprocessed data was then split into training, vali- Different embeddings for the guidance were tested with
dation, and testing sets based on the specified ratios. Post- mixed success. Embeddings proved beneficial primarily with
splitting, the data comprised sequential user interactions, fa- exceptionally large datasets, although they were slightly less
cilitating model training and comparison of recommendations effective than using raw data. However, embeddings demon-
against the most recent user ratings. strated improved efficiency. Therefore, while raw data might
The datasets are prepared for training by transforming the be preferable for smaller datasets, embeddings are recom-
pre-runtime format into a more suitable structure. The initial mended for scaling the system to handle large datasets. The
formatisinadequatefortrainingsinceitdoesnotdifferentiate Yelpdataset,ourlargest,performedwellwithoutembeddings,
users effectively, leading to a continuous flow of entries from suggesting that the raw data approach suffices for our current
one user to the next. To address this, we convert the data scope.
into a sparse format using multi-hot vectors. Unlike one-hot
vectors, which indicate a single hit among items for a specific C. Evaluation
user, multi-hot vectors represent all hits or interactions with For all datasets, we saved the highest-performing model
multiple 1’s for each user. based on the nDCG@10 metric. For evaluation, we com-
This approach necessitates that the vector length corre- puted metrics at K = [1, 5, 10, 20], reflecting the practical
sponds to the total number of items in the dataset, ensur- importance for recommender systems to generate valuable
ing accurate item representation. Although the model does recommendations within this range. We initially considered
not make recommendations based on an exact sequence, it higher values but concluded that average users are unlikely to
leverages historical data to generate recommendations and searchthrough50or100recommendations.Allconfigurations
uses future data for validation. By preparing the data in this were trained for varying durations depending on the dataset
manner, we ensure that the model can effectively differentiate size but extended well beyond the point of reaching the best-
between users and items, thereby facilitating more accurate performing results on the testing datasets.
recommendations.
V. EXPERIMENTALRESULTS
B. Experimental Configurations
Some of the results from our experiments yield intriguing
In this section, we detail the experiments conducted to insights into specific scenarios where our model excels, par-
identify the best-performing recommender system, examining ticularly when compared to other methods and the original
what was successful, what was not, and our reasoning behind implementation. The hyperparameter configuration remained
these outcomes. Most experiments involved modifications ei- the same as in the original work, except for the addition of
thertotheguidanceprovidedtothedenoisingmodelortothe the classifier-free guidance hyperparameter p uncond set at
denoising model itself. We also conducted minor experiments 0.2. The results are shown in Tables II an III, in which any
to explore the potential of certain functionalities. bold value represents a score that either surpasses or ties with
One key experiment investigated making the recommen- the other implementations.
dation model aware of the actual numeric ratings, rather Our system generally performs on par with, or better than,
than using binary signals (1’s and 0’s) to represent hits and the original method across most scenarios. When our method
misses. In this implementation, negative signals (ratings of 1- is outperformed, the margin is typically small. Conversely,
3) were introduced alongside positive signals (ratings of 4-5), when our method outperforms the original, the margin is
distinguishingbetweendislikedorunrateditems.However,the substantially larger, outperforming the original by more thanTABLEII nDCG, and mean reciprocal rank @ K on held-out user data,
TESTINGRESULTS.ORGDENOTESORIGINALRESULTS.VALUESAREIN revealing clear benefits over previous methods. Additionally,
PERCENTAGE.
ourapproachshowedpotentiallyenhancedcapabilitiesinfew-
ML-1MClean ML-1MNoisy YelpClean shot and zero-shot recommendation scenarios.
Dataset
Original Ours Original Ours Original Ours Our major contributions include: (1) the implementation
8.64 9.25 2.99 4.63 2.8 2.61
Precision and evaluation of classifier-free guidance in diffusion-based
7.92 8.61 3.37 3.6 2.22 2.08
@ 7.16 7.83 3.46 3.57 1.9 1.87 recommendation tasks; (2) an in-depth discussion on the
[1,5,10,20]
6.54 6.82 3.45 3.55 1.61 1.61 parameterization and architecture of the denoising model
0.69 0.92 0.29 0.61 0.48 0.48
Recall utilized in these tasks; and (3) insights into the applicability
3.18 3.96 1.51 2.22 1.85 1.77
@ 5.49 6.6 3.04 3.7 3.12 3.11 of diffusion models beyond the visual domain. Our findings
[1,5,10,20]
9.72 11.66 5.55 6.57 5.14 5.19 indicate that classifier-free guidance improves performance
8.64 9.25 2.99 4.63 2.8 2.61
nDCG in recommendation tasks similarly to its success in visual
8.33 9 3.49 4.18 2.67 2.53
@ 8.43 9.32 3.97 4.58 2.94 2.88 tasks. Additionally, we provide a framework that addresses
[1,5,10,20]
9.44 10.56 4.83 5.57 3.56 3.53 challenges within diffusion-based recommendation systems
8.64 9.25 2.99 4.63 2.8 2.61
MRR while highlighting the newfound capabilities in few-shot and
15.41 16.39 6.78 8.27 5.29 4.94
@ 17.06 18.27 8.34 9.73 6.04 5.77 zero-shot scenarios.
[1,5,10,20]
18.15 19.33 9.38 10.9 6.62 6.35 The potential for future work in this area is substantial.
One promising direction involves improving the system by
consideringactualnumericalratings.Whileourinitialattempt
100% in some situations. This trend holds for nearly all at this was inconclusive, a more sophisticated approach, po-
scenarios, with only occasional outliers. One such outlier is tentially involving triplet loss to account for negative reviews,
the Yelp dataset with clean data. Our model fails to beat the couldyieldbetterresults.Cross-attentiveconditioning,another
original approach on this dataset systematically, we suspect feature that did not meet our performance expectations, might
that this is due to the method of applying guidance. For the benefit from refined architectural innovations for improved
majority of our datasets, a complex embedding to encapsulate guidance.
theinformationoftheguidancematrixisnotneeded,however, The use of transformers in the diffusion process is another
the Yelp dataset may be one such scenario as it is noticeably avenue worth exploring. Recent advancements in image gen-
larger.Theexperimentationintoanefficientembeddingmodel eration have shown that replacing the U-Net backbone with a
for guidance fell out of the scope of this paper. Overall, our Vision Transformer (ViT) can enhance denoising capabilities,
approach equals or surpasses the original implementation in and similar benefits might be realized in recommendation
85 out of 128 categories across all datasets, indicating a clear systems. Additionally, the few-shot and zero-shot capabilities
improvement. of diffusion recommender systems trained with classifier-
Wealsobelievethatourrecommendationsystem,enhanced free guidance warrant further investigation. This aspect is
by classifier-free guidance, could excel in few-shot and po- particularly relevant for e-commerce, where new users with
tentially zero-shot scenarios. This hypothesis is supported by no interaction history present ongoing challenges.
the marked improvements observed in the Amazon Office Inconclusion,ourstudylaysthegroundworkforadvancing
dataset, which has a limited item count and thus offers less diffusion-based recommendation systems. By addressing the
personalization.Generally,wefoundourmodeltoworkbetter identified shortcomings and exploring the outlined directions
inscenarioswhereeitherfeweritemswereofferedforperson- for future research, we can further enhance these systems to
alization or there were fewer reviews per user. Classifier-free meet the evolving needs of recommender applications.
guidance involves training without guidance for 20% of the
REFERENCES
samples, creating a scenario analogous to zero-shot learning,
which might contribute to its effectiveness in this area and [1] E. Rich, “User modeling via stereotypes,” Cognitive Science, vol. 3,
Issue4,1979.
wouldexplainitssignificantperformanceincomparisontothe
[2] J. Karlgren, “An Algebra for Recommendations,” 1990.
original method when less guiding data is available. Although [Online]. Available: https://jussikarlgren.wordpress.com/wp-
wedidnotexplorethisaspectin-depth,ourpreliminaryresults content/uploads/1990/09/algebrawp.pdf
[3] D. Goldberg, D. Nichols, B. M. Oki, D. Terry, “Using collaborative
suggest potential benefits in few-shot and zero-shot scenarios.
filtering to weave an information tapestry,” Communications of the
ACM, Volume 35, Issue 12, pp 61-70, 1992. [Online]. Availabile:
VI. CONCLUSIONANDFUTUREWORK https://doi.org/10.1145/138859.138867
[4] J.Sohl-Dickstein,E.A.Weiss,N.Maheswaranathan,S.Ganguli,“Deep
Thispaperpresentsclassifier-freeguidanceasaformofper- UnsupervisedLearningusingNonequilibriumThermodynamics,”CoRR,
sonalizationandincorporatesdenoisingindiffusion-basedrec- 2015.[online].Availabile:https://arxiv.org/abs/1503.03585
[5] J. Ho,A. Jain,P. Abbeel.“Denoising Diffusion Probabilistic Models,”
ommender frameworks. By restructuring the guidance mecha-
CoRR,2020.[Online].Available:https://arxiv.org/abs/2006.11239
nismbyremovingpartiallynoisedpseudo-guidanceandincor- [6] P.Dhariwal,A.Nichol,“DiffusionModelsBeatGANsonImageSynthe-
poratingtrueguidancethroughconditioningonpre-noisedata, sis,”CoRR,2021.[Online].Available:https://arxiv.org/abs/2105.05233
[7] J. Ho, et al., “Cascaded Diffusion Models for High Fidelity
we demonstrated improvements in performance. The effec-
Image Generation,” CoRR, 2021. [Online]. Available:
tiveness of our method was evaluated using precision, recall, https://arxiv.org/abs/2106.15282TABLEIII
RESULTSFORAMAZONDATASET.VALUESAREINPERCENTAGE.
AmazonKitchen AmazonBeauty AmazonToys AmazonOffice AmazonVG
Dataset
Original Ours Original Ours Original Ours Original Ours Original Ours
0.35 0.52 0.93 1.06 1.12 0.75 0.29 0.58 1.47 1.29
Precision
0.21 0.24 0.87 0.90 0.45 0.30 0.35 0.52 0.90 1.09
@
0.14 0.16 0.83 0.81 0.32 0.26 0.26 0.61 0.90 0.84
[1,5,10,20]
0.10 0.10 0.62 0.67 0.20 0.23 0.17 0.44 0.70 0.71
0.13 0.39 0.38 0.42 0.42 0.47 0.04 0.10 0.82 0.72
Recall
0.42 0.56 1.64 1.71 0.97 0.64 0.74 0.65 2.32 2.63
@
0.62 0.68 3.44 3.18 1.15 1.36 1.08 1.77 4.63 4.12
[1,5,10,20]
0.89 0.79 5.19 5.28 1.37 2.20 1.34 2.57 6.80 6.99
0.35 0.52 0.93 1.06 1.12 0.75 0.29 0.58 1.47 1.29
nDCG
0.35 0.55 1.37 1.42 0.94 0.65 0.61 0.61 1.94 2.03
@
0.41 0.59 2.05 1.92 0.98 0.89 0.72 1.06 2.75 2.54
[1,5,10,20]
0.50 0.64 2.58 2.57 1.04 1.15 0.82 1.35 3.40 3.49
0.35 0.52 0.93 1.06 1.12 0.75 0.29 0.58 1.47 1.29
MRR
0.53 0.70 1.94 1.76 1.54 0.87 0.95 1.09 2.52 2.61
@
0.57 0.74 2.22 2.14 1.59 1.02 1.05 1.42 3.07 2.94
[1,5,10,20]
0.61 0.77 2.36 2.32 1.61 1.14 1.10 1.54 3.37 3.27
[8] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, “Image Super- mating the click-through rate for new ads,” ACM, pp 521-530, 2007.
Resolution via Iterative Refinement,” IEEE Transactions on Pattern [Online].Available:https://dl.acm.org/doi/10.1145/1242572.1242643
Analysis and Machine Intelligence, vol. 45, no. 4, pp. 4713-4726, 1 [25] D. Liang, R. G. Krishnan, M. D. Hoffman, T. Jebara, “Variational
April,2023.[Online].Available:https://arxiv.org/abs/2104.07636 Autoencoders for Collaborative Filtering,” 2018. [Online]. Available:
[9] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, B. Ommer, “High- https://arxiv.org/abs/1802.05814
Resolution Image Synthesis with Latent Diffusion Models,” CoRR, [26] M.Gao,etal.,“RecommenderSystemsBasedonGenerativeAdversarial
2022.[Online].Available:https://arxiv.org/abs/2112.10752 Networks: A Problem-Driven Perspective,” 2020. [Online]. Available:
[10] C.Saharia,W.Chan,S.Saxena,L.Li,J.Whang,E.Denton,S.Kamyar, https://arxiv.org/pdf/2003.02474
S.Ghasemipour,B.K.Ayan,S.S.Mahdavi,R.G.Lopes,T.Salimans, [27] P.Covington,J.Adams,E.Sargin,“DeepNeuralNetworksforYouTube
J.Ho,D.J.Fleet,M.Norouzi,“PhotorealisticText-to-ImageDiffusion Recommendations,” ACM, pp 191-198, 2016. [Online]. Available:
ModelswithDeepLanguageUnderstanding,”NeurIPS,2022.[Online]. https://dl.acm.org/doi/10.1145/2959100.2959190
Available:https://arxiv.org/abs/2205.11487 [28] D. P. Kingma, M. Welling, “Auto-Encoding Variational Bayes,” 2013.
[11] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, “Hierarchical Text- [Online].Available:https://arxiv.org/abs/1312.6114
Conditional Image Generation with CLIP Latents,” 2022. [Online]. [29] M. M. Saad, R. O’Reilly, M. H. Rehmani, “A Survey on Training
Available:https://arxiv.org/abs/2204.06125 Challenges in Generative Adversarial Networks for Biomedical Image
Analysis,”2023.[Online].Available:https://arxiv.org/abs/2201.07646
[12] J. Betker, et al., “Improving Image Generation with Better Captions,”
[30] S.Bond-Taylor,A.Leach,Y.Long,C.G.Willcocks,“DeepGenerative
2023.[Online].Available:https://arxiv.org/abs/2006.11807
Modeling:AComparativeReviewofVAEs,GANs,NormalizingFLows,
[13] S. Gu, et al., “Vector Quantized Diffusion Model for
Energy0Based and Autoregressive Models,” CoRR, 2022. [Online].
Text-to-Image Synthesis,” CoRR, 2022. [Online]. Available:
Available:https://arxiv.org/abs/2103.04922
https://arxiv.org/abs/2111.14822
[31] I. J. Goodfellow, et al. “Generative Adversarial Nets,” NeurIPS, 2014.
[14] Y. Li, et al., “SnapFusion: Text-to-Image Diffusion Model on Mo-
[Online].Available:https://arxiv.org/abs/1406.2661
bile Devices with Two Seconds,” NeurIPS, 2023. [Online]. Available:
[32] F. Maxwell Harper, Joseph A. Konstan, ”The MovieLens Datasets:
https://arxiv.org/abs/2306.00980
History and Context,” ACM Transactions on Interactive Intelli-
[15] W. Wang, et al., “Diffusion Recommender Model,” SIG IR, 2023.
gent Systems (TiiS) 5, 4, Article 19, 19 pages, 2015. [Online].
[Online].Available:https://arxiv.org/abs/2304.04971
Available:http://dx.doi.org/10.1145/2827872
[16] Y. Wang, Z. Liu, L. Yang, P. S. Yu, “Conditional Denoising Dif- [33] Yupeng Hou, et al., ”Bridging Language and Items for Re-
fusion for Sequential Recommendation,” 2023. [Online]. Available: trieval and Recommendation,” arXiv, 2023. [Online]. Available:
https://arxiv.org/abs/2304.11433 https://arxiv.org/pdf/2403.03952
[17] X.Lin,etal.,“DiscreteConditionalDiffusionforRerankinginRecom- [34] Yelp, ”Yelp Open Dataset,” 2021. [Online]. Available:
mendation,”2023.[Online].Available:https://arxiv.org/abs/2308.06982 https://www.yelp.com/dataset
[18] Z. Li, A. Sun, C. Li, “DiffuRec: A Diffusion Model for
Sequential Recommendation,” 2023. [Online]. Available:
https://arxiv.org/abs/2304.00686
[19] T.Salimans,etal.,“ImprovedTechniquesforTrainingGANs,”CoRR,
2016.[Online].Available:https://arxiv.org/abs/1606.03498
[20] P. Resnick, N. Lacovou, M. Suchak, P. Bergstrom, J. Riedl,
“GroupLens: An Open Architecture for Collaborative Filtering
of Netnews,” ACM, pp 175-186, 1994. [Online]. Available:
https://dl.acm.org/doi/10.1145/192844.192905
[21] F. M. Harper, J. A. Konstan, “The MovieLens Datasets: History and
Context,” ACM, Volume 5, Issue 4, Article No.: 19, pp 1-19, 2015.
[Online].Available:https://dl.acm.org/doi/10.1145/2827872
[22] Y.Koren,“Factorizationmeetstheneighborhood:amultifacetedcollab-
orative filtering model,” ACM, pp 426-434, 2008. [Online]. Available:
https://dl.acm.org/doi/10.1145/1401890.1401944
[23] Y. Koren, “Collaborative filtering with temporal dy-
namics,” ACM, pp 447-456, 2010. [Online]. Available:
https://dl.acm.org/doi/pdf/10.1145/1721654.1721677
[24] M. Richardson, E. Dominowska, R. Ragno, “Predicting clicks: esti-