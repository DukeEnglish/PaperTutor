Do Pre-trained Vision-Language Models Encode Object States?
KalebNewman ShijieWang YuanZang DavidHeffren ChenSun
BrownUniversity
Abstract whether it is melted, dirty, or undergoing a process such
ascuttingorpouring. Whileobjectstaterecognitionisof-
For a vision-language model (VLM) to understand the ten associated with video analysis due to its applications
physical world, such as cause and effect, a first step is to inmodelingtemporaldynamicsandactionrecognition,our
capture the temporal dynamics of the visual world, for ex- studyfocusedontemporallylocalizedkeyframesthatcon-
ample how the physical states of objects evolve over time taintheobjectstatesofinterest,fromwhichhumanannota-
(e.g. a whole apple into a sliced apple). Our paper aims torsareabletoaccuratelylabelthestateswithouttemporal
toinvestigateifVLMspre-trainedonweb-scaledatalearn context,andonwhichpre-trainedVLMsareappliedtogen-
to encode object states, which can be extracted with zero- eratepseudolabels[16].
shot text prompts. We curate an object state recognition
To explore VLMs’ capabilities in this area, we em-
dataset ChangeIt-Frames, and evaluate nine open-source
ploy them as zero-shot classifiers on a dataset we intro-
VLMs, including models trained with contrastive and gen-
duce called ChangeIt-Frames, based on [15]. This dataset
erative objectives. We observe that while these state-of-
contains images depicting various object states in natural
the-artvision-languagemodelscanreliablyperformobject
scenes.Weaugmentthisdatasetwithboundingboxannota-
recognition, they consistently fail to accurately distinguish
tionsforasubsetof1,736images,inwhichtheoriginalob-
theobjects’physicalstates.Throughextensiveexperiments,
jectstatelabelsareverifiedbyhumanannotators.Weevalu-
we identify three areas for improvements for VLMs to bet-
ate9state-of-the-artopen-sourcemodels.Weusetwotypes
ter encode object states, namely the quality of object lo-
ofVLMs: dual-towerVLMs, whichclassifyimagesbased
calization,thearchitecturetobindconceptstoobjects,and
onthesimilaritybetweenimageandtextembeddingsbased
the objective to learn discriminative visual and language
on contrastive learning, and Multimodal Large Language
encoders on object states. Data and code are released at
Models (MLLMs), which utilize a visual encoder paired
github.com/brown-palm/object-states.
with a generative language model backbone to respond to
prompts. We observe that while these models excel in ob-
jectrecognition,theyconsistentlyunderperforminreliably
1.Introduction
identifyingtheobjects’physicalstates.
Vision-Language Models (VLMs) have become founda- Recognizing object physical states in images presents
tional in various visual understanding tasks, including ob- unique challenges. Our study reveals that standard fine-
ject recognition [13], visual question answering [9], and tuningofVLMswithphysicallygroundeddata[2],orpre-
roboticsapplications[3]. Thesemodelsintegratevisualand trainingwithcurateddatasets,doesnotnecessarilyenhance
linguisticdata,enablingnuancedinterpretationandinterac- object state recognition performance on the ChangeIt-
tionwithimagesandvideo. However, acriticalyetunder- Framesdataset.Wehypothesizethateffectiveconceptbind-
exploredaspectofVLMsistheirabilitytoencodethephys- ing—linking visual features to corresponding objects—is
icalstatesofobjects—suchaswhetheranappleiswholeor crucial for VLMs to accurately discern object states. To
sliced. Understanding these states is essential for physical test this hypothesis, we construct object-centric represen-
commonsense reasoning, which underpins many practical tations using CLIP and demonstrate that these modified
applications,fromassistingwithdailytasks(e.g.,recogniz- VLMs improve at solving concept binding tasks involving
ing that hot water can be poured into an empty glass) to colorandshape,wherevanillaCLIPstruggles[8].Nonethe-
enhancinginteractioninroboticsystems. less,object-centricVLMsstillexhibitlimitationsinrecog-
We define object state as the physical and functional nizingobjectstates,whichweattributetoinadequateobject
conditionorconfigurationofanobject,asdiscerniblefrom localizationandinsufficientlydiscriminativevisualandlan-
visual data. For example, an object’s state can indicate guagerepresentations. Weadditionallyobserveanincrease
4202
peS
61
]VC.sc[
1v88401.9042:viXrain model parameters or training data size can provide im- Standard Distractor
provementsbuttheperformancesarestillfarfromsatisfac-
Model Obj. State Obj. State
tory, and that the challenge prevails in Multimodal Large
CLIP 0.918 0.614 0.925 0.408
LanguageModels,suchasLLaVAandPaliGemma.
OpenCLIP 0.906 0.604 0.925 0.373
ALIGN 0.910 0.620 0.918 0.403
2.RecognizingObjectStates
FLAVA 0.966 0.633 0.837 0.560
PhysVLM∗ - 0.338 - -
Dataset: Weconstructedourevaluationdataset,ChangeIt-
Frames,fromthevideo-basedChangeItdataset[15],which
Table1.ObjectandStateRecognitionperformanceforbothneg-
includes650videosof44objectcategoriesundergoingvar-
ativepromptsamplingstrategiesillustratedinFigure1. Random
ious state-changing actions. From these videos, we ex-
performanceis10%. ∗: Evaluatedoncroppedobjectsasrecom-
tracted25,735images,eachdepictingoneof96distinctob-
mendedbytheauthors.
jectstates. Thisimage-baseddatasetisexclusivelyusedfor
zero-shot evaluation of Vision-Language Models (VLMs).
Evaluation Setup: For each image, we select a list of de-
Toprovidedetailedannotations,wemanuallylabeledasub-
scriptivepromptsforpossibleobjectstates,suchas“whole
set of 1,736 images from ChangeIt-Frames with bounding
apple” or “fried bacon”. We refer to the correct descrip-
boxesaroundthetargetobjects. Eachimageislabeledwith
tion as a “positive prompt”, and the incorrect descriptions
asingleboundingboxcorrespondingtotheobjectinitsspe-
as “negative prompts”. We employ two strategies for se-
cific state. The annotation process was carried out using
lectingnegativeprompts:Thestandardstrategywhichse-
thedefaultannotatorpoolfromAmazonMechanicalTurk.
lectsthenegativepromptscorrespondingtodifferentstates
TheseannotationsarereleasedundertheMITlicense.
within the same object category, such as peeled apple for
There have been introduced in the past to explore the
wholeapple.Theremainingnegativepromptsarerandomly
compositionality of objects and their states, most notably
selectedfromthecandidatepooltototal10promptsperim-
MIT-States[6]andC-GQA[12]. UnlikeC-GQA,whichin-
age. Wealsoconsiderthedistractorstrategy,andtheneg-
cludesstates(e.g.,“cutedog”)thatmaynotnecessarilybe
ativepromptsconsistofdistractorsspecificallydesignedto
theresultofobservablephysicalchanges,ChangeIt-Frames
be semantically similar yet incorrect regarding the object
isexclusivelyconcernedwithirreversiblephysicalchanges
state. For example, for a positive prompt whole apple, the
inobjects. ThisalsoseparatesChangeIt-FramesfromMIT-
distractorpromptsmightincludeanapplethatiscut oran
States, which organizes state variations primarily through
applethatispeeled. Theremainingpromptsarerandomly
adjective-noun pairings that may include reversible states
selected,ensuringatotalof10prompts. Thissettingisde-
like open/closed door, or the states of global objects like
signedtochallengethemodel’sabilitytodiscernsubtledif-
cluttered/empty room. Furthermore, MIT-States uses Bing
ferencesinobjectstates.
search engine with limited human annotation, leading to
We utilize two methods for zero-shot classification
missingorinaccuratestatelabels.
based on the architecture of the Vision-Language Models
(VLMs): For the dual-tower VLMs, we compute the co-
sine similarity between the image and text embeddings.
Thelabelcorrespondingtothehighestsimilarityisselected
as the predicted output. For the Multimodal Language
Modelswepresentthemodelswithapromptformattedas:
“Which of these does this image depict: [numbered list of
prompts]? Only reply with the single number that corre-
sponds to the correct answer.” The model’s output is then
usedtodeterminethepredictedlabel. Thesemethodsallow
ustoevaluatethemodels’abilitytocorrectlyidentifyobject
statesacrossdifferentarchitectures.
Metrics: We separately calculate Object Accuracy and
State Accuracy. For object accuracy, a prediction is con-
sidered correct if the predicted label includes the object’s
Figure 1. ChangeIt-Frames dataset. The images are sourced
frominstructionalvideos[15]. WeuseAmazonMTurktomanu- name (e.g., both whole apple and cut apple are correct for
allyverifyasubsetoftheimageannotations,andtodrawbounding an image of an apple). For state accuracy, the model must
boxesfortheobjectsofinterest. Forevaluation,weaskanVLM predicttheexactgroundtruthobjectstateintheimage.
to choose the correct object state among ten candidates prompts Results and Analysis: We conduct experiments on CLIP
selectedviastandardordistractorstrategies. ViT-L/14 [13], OpenCLIP ViT-L/14 [5], ALIGN [7],FLAVA [14], and PhysVLM [4]. Notably, CLIP, Open- benchmark [8], which involves differentiating between vi-
CLIP, and ALIGN rely on image-level representation for sual concepts such as “red cube and blue sphere” versus
image-text contrastive learning; FLAVA uses patch-level “bluecubeandredsphere”,aswellasspatialrelationships
asopposedtoimage-levelrepresentations; PhysVLMfine- oftwoobjects,suchas“cubeleftofsphere”versus“sphere
tunes InstructBLIP [2] with “physically grounded” anno- leftofcube”. InTables2and3,wereportaccuracyonthe
tations collected on [18]. Our results, summarized in Ta- training, validation, and generalization splits of CLEVR-
ble1,revealthatwhileobjectrecognitionaccuracyisgen- Binding. Weobservethatobject-centricVLMsoutperform
erally high, there is a consistent drop of approximately image-levelVLMsbyhugemarginsonbothtasks.
30%instaterecognitionaccuracy.Whendistractorprompts
areused, modelperformancegenerallydropssignificantly.
Model Object-Centric Train Val Gen
FLAVAshowsgreaterrobustnesswithonlya7%drop. No-
tably,incorrectpredictionsfrequentlycorrespondtoourde- CLIP No 27.02 7.17 31.40
signeddistractors. NegCLIP No 25.39 0.29 41.33
Discussion: Our results show that simply fine-tuning with
CLIP Yes 93.96 94.12 96.53
more physically grounded data, as in PhysVLM, does not
NegCLIP Yes 96.58 71.20 81.82
help VLM better encode object states. It might even hurt
the performance, presumably due to domain mismatch be-
Table2. Resultsontwo-objectadjective-nounbindingtaskin-
tween fine-tuning data and ChangeIt-Frames. We further
troducedbyCLEVR-Binding.
noticethatFLAVAoutperformsotherVLMsunderthemore
challengingdistractorsetup. Wehypothesizethatthismay
be due to its use of patch-level representation, which can
Model Object-Centric Train Val Gen
supportassociationofobjectregionsandtextdescriptions.
Wetestthishypothesisinthenextsection. CLIP No 26.80 14.99 0.00
NegCLIP No 24.34 1.57 63.45
3.ExploringPossibleRemedies CLIP Yes 65.31 54.48 92.48
NegCLIP Yes 89.51 90.97 82.33
Inthissection,weexplorepotentialsolutionsforimproving
therecognitionofobjectstatesinVision-LanguageModels.
Table3.ResultsforrelationalreasoninginCLEVR-Binding.
WehypothesizethatVLMsfailtorecognizephysicalstates
duetothelackofanexplicitnotionofobjectsinthesemod-
Forphysicalstaterecognition,weevaluateVLMsonour
els. These models may process images as a “bag of con-
human-annotatedChangeIt-Framesdatasetsubsetusingthe
cepts”, associating them with the scene as a whole rather
full image, or ground truth objects. We present the results
thanwithindividualentities.
inTable4. AlthoughtheresultsinTable2demonstratethat
To address this, we investigate the use of object-centric
VLMscaneffectivelyassociatevisualconceptswithcorre-
representations. Wealsoevaluatetheperformanceoflarger
sponding objects using object-centric representations, this
VLMstrainedonmoreextensivedatatoseeifthelowper-
improvementdoesnotextendtobetterperformanceinrec-
formance can be rectified by scale. The testbed for these
ognizingphysicalstates. ThiscanbeshowninTable4(GT
improvementsisalsodoneonasubsetofChangeIt-Frames
Crop) where models generally do not improve when pro-
that include bounding box annotations and verified object
vided with ground truth crops. This suggests that object
statelabels,bothofwhicharedonebyhumanannotators.
crops do not compel models to do the fine-grained object
Totestwhetherfocusingonspecificobjectscanenhance
analysisrequiredtoimproveatstaterecognition.
staterecognition,weimplementobject-centricVLMs. This
approachinvolvesisolatingobjectsusingbounding-boxin-
3.2.LargerVLMs
formation, either provided by the dataset or generated by
anoff-the-shelfdetectionmodellikeGroundingDINO[11].
WealsoinvestigatewhetherlargerVLMstrainedonexten-
By cropping the images to these object regions, we aim to
sivedatacanbetterrecognizeobjectstates. Ourevaluations
create representations that explicitly associate visual con-
includeOpenCLIPViT-G-14[5]andSigLIP[17],weeval-
ceptswithdistinctentities.
uate their performance on the standard and distractor set-
tings of the annotated ChangeIt-Frames dataset. We ob-
3.1.Object-CentricVLMs
serve that while larger dual-tower models show improved
Weassesstheeffectivenessofobject-centricrepresentations performancecomparedtoCLIPandOpenCLIP,challenges
intwomaintasks:conceptbindingandphysicalstaterecog- remain under the distractor setting, where both are outper-
nition. For concept binding, we use the CLEVR-Binding formedbyFLAVA.Standard Distractor
Model Image GTCrop Image GTCrop
CLIP 0.655 0.642 0.463 0.465
OpenCLIP 0.653 0.642 0.466 0.455
ALIGN 0.715 0.702 0.502 0.548
FLAVA 0.614 0.643 0.620 0.686
ViT-G-14 0.710 0.726 0.494 0.520
SigLIP 0.791 0.789 0.572 0.571
Table 4. State Recognition performance on our annotated
ChangeIt-Framessubset. Weshowaccuracyforwholeimageand
Figure2.TheT-SNEvisualizationofCLIPtextembeddings.The
groundtruthcropsunderourStandardandDistractorsetting.The
representationsofthetextpromptsareclusteredbytheobjectcate-
lasttwomodelscorrespondtoexperimentsruninSection3.2.
goryandtherepresentationsfordifferentstatesofthesameobject
areverysimilar.
3.3.MultimodalLLMs
In our state recognition experiments, we found that model
performance typically declined in the distractor setting. We first investigate whether the text encoder can prop-
To further explore state recognition, we examined Mul- erly reflect the physical state descriptions, we utilize T-
timodal Large Language Models (MLLMs), a recent ad- SNEtovisualizetheCLIPtextembeddingofdifferenttext
vancementovertheVision-LanguageModels(VLMs)pre- promptsof“state+object”combinations. Asillustratedin
viouslyused. UnlikeVLMs, whichrelyonastandardtext Figure 2, the representations of the text prompts are clus-
encoder,MLLMsincorporateagenerativelanguagemodel tered by the object category rather than the physical state,
to process language inputs, significantly increasing the to- thisindicatesthatthetextencodersfailtolearndiscrimina-
talmodelparameters. Withthisinmind,wetestedwhether tiverepresentationsforphysicalstatesofobjects.
theadditionalparametersandenhancedlanguagecapabili-
tiesofMLLMswouldimproveaccuracyinthestandardset-
tingandaddressthemorelinguisticallycomplexchallenges
posedbythedistractorsetting.
To investigate this, we asses the performance of
PaliGemma [1] and two LLaVA-NeXT [10] models
(Mistral-7B and LLama-8B). The results in Table 5 show
that the state recognition problem in dual-tower VLMs
translatetoMLLMs.EvenwiththeuseofaLargeLanguage
Model(LLM)andextensiveVisualInstructionTuning,the
distractorsettingremainschallenging.
Model Standard Distract
PaliGemma 0.716 0.242
LLaVA-Mistral-7B 0.767 0.656
Figure3. TSNEprojectionsofCLIPvisualembeddingsfor“ba-
LLaVA-Llama-8B 0.579 0.430
con”.Thisincludesoriginalandcroppedimagesforbothstates.
Wefurthervalidatethelowerperformancebyvisualizing
Table5. StateRecognitionperformanceontheannotatedsubset
for selected MLLMs. The last two models are both versions of the distributions of encoded object-level and image-level
LLaVA-NeXT.Eachcolumncorrespondstothedifferentevalua- visual representations for the same objects with opposite
tionsettings,detailedinSection2. states. Weobservethatthet-SNEprojectionsdonotshowa
cleardistinctionbetweenthestates,norarethecroppedim-
agesembeddingsforagivenstateclosertothewholeimage
4.InspectingtheEncodedRepresentations
embedding. Wealsoobservethatcroppinghasalargeref-
Aswehaveruledoutseverallikelyremediestofixexisting fectontherepresentationthanthestateitself,suggestingthe
pre-trainedVLMsonrecognizingphysicalstatesofobjects, embeddings are not robust to transformation. An example
wenowaimtoinvestigatewhysuchmodelsfailbyinspect- ofthiscanbeseeninFigure3.
ingtheirencodedvisualandtextrepresentations. Althoughwehavedemonstratedthelackofdiscrimina-tive information in the encoded text and visual representa- vancesinNeuralInformationProcessingSystems,36,2024.
tions for object states, there are many plausible remedies 1,3
that need to be investigated. We hypothesize that a com- [3] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
bination of stronger object localization and modeling in Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
the VLMs, combined with training objectives that explic- Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-
e:Anembodiedmultimodallanguagemodel.arXivpreprint
itly encourage the recognition of object states (e.g. track-
arXiv:2303.03378,2023. 1
ingobjectsthatmayundergostatetransformationsinvideo
[4] JensenGao, BidiptaSarkar, Fei Xia, TedXiao, JiajunWu,
data), wouldbeneededtoencourageVLMstocaptureob-
BrianIchter,AnirudhaMajumdar,andDorsaSadigh. Phys-
jectphysicalstates.
icallygroundedvision-languagemodelsforroboticmanipu-
lation. arXivpreprintarXiv:2309.02561,2023. 3,6
5.Conclusion
[5] GabrielIlharco,MitchellWortsman,RossWightman,Cade
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
Despite excellent performance on zero-shot object recog-
VaishaalShankar,HongseokNamkoong,JohnMiller,Han-
nition, we demonstrate that existing pre-trained Vision-
nanehHajishirzi,AliFarhadi,andLudwigSchmidt. Open-
Language Models struggle to encode object state informa-
clip,2021. 2,3,6
tion, which we believe hinder their capabilities to under- [6] PhillipIsola, JosephJ.Lim, andEdwardH.Adelson. Dis-
standandreasonaboutthephysicalworld. Wehypothesize coveringstatesandtransformationsinimagecollections. In
the challenge may come from lack of physically grounded CVPR,2015. 2
trainingdata,orthelackofobject-centricinductivebiasfor [7] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,
VLMstobindconceptstoobjects.WecollecttheChangeIt- HieuPham,QuocV.Le,YunhsuanSung,ZhenLi,andTom
Framesbenchmarkwithobjectboundingboxandphysical Duerig. Scaling up visual and vision-language representa-
state annotations, and conduct extensive evaluations. We tionlearningwithnoisytextsupervision. InProceedingsof
MachineLearningResearch,2021. 2,6
observe that addressing the data or model architecture is-
[8] Martha Lewis, Nihal V Nayak, Peilin Yu, Qinan Yu, Jack
sues alone does not solve object state recognition, and ex-
Merullo,StephenHBach,andElliePavlick. Doesclipbind
pectfurtherprogresstobemadeonobjectlocalizationqual-
concepts? probingcompositionalityinlargeimagemodels.
ity, concept binding, and pre-training objectives. We hope
arXivpreprintarXiv:2212.10537,2022. 1,3
ourfindingswillhelpdevelopfuturegenerationVLMsthat
[9] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
canbettercaptureobjectstates. Blip-2: Bootstrapping language-image pre-training with
Limitations: Our evaluation mainly relies on a single frozen image encoders and large language models. arXiv
datasetderivedfrominstructionalvideoscollectedfromthe preprintarXiv:2301.12597,2023. 1
internet. Although the annotations are collectively manu- [10] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan
ally, they are still subject to label noise. Evaluation per- Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-
formedonmorediversevisualdomainsisdesired. provedreasoning,ocr,andworldknowledge,2024. 4
Acknowledgements: WewouldliketothankNihalNayak [11] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
forhelpwithCLEVR-Binding,andLichengYu,TianYun, Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu,Jun
andNingZhangforvaluablefeedback. Theprojectwasin Zhu, etal. Groundingdino: Marryingdinowithgrounded
partsupportedbyMetaAI. pre-training for open-set object detection. arXiv preprint
arXiv:2303.05499,2023. 3,6
References [12] MFNaeem,YXian,FTombari,andZeynepAkata. Learn-
inggraphembeddingsforcompositionalzero-shotlearning.
[1] LucasBeyer,AndreasSteiner,Andre´SusanoPinto,Alexan- In 34th IEEE Conference on Computer Vision and Pattern
derKolesnikov,XiaoWang,DanielSalz,MaximNeumann, Recognition.IEEE,2021. 2
Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele [13] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
NeilHoulsby,ManojKumar,KeranRong,JulianEisensch- Krueger, and Ilya Sutskever. Learning transferable visual
los, Rishabh Kabra, Matthias Bauer, Matko Bosˇnjak, Xi modelsfromnaturallanguagesupervision. InProceedings
Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, ofthe38thInternationalConferenceonMachineLearning,
Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, pages8748–8763.PMLR,2021. 1,2,6
OlivierHenaff,XiXiong,RaduSoricut,JeremiahHarmsen, [14] AmanpreetSingh, RonghangHu, VedanujGoswami, Guil-
and Xiaohua Zhai. PaliGemma: A versatile 3B VLM for laume Couairon, Wojciech Galuba, Marcus Rohrbach, and
transfer. arXivpreprintarXiv:2407.07726,2024. 4 DouweKiela. FLAVA:Afoundationallanguageandvision
[2] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat alignmentmodel. InCVPR,2022. 3,6
Tiong, JunqiZhao, WeishengWang, BoyangLi, PascaleN [15] Toma´sˇSoucˇek,Jean-BaptisteAlayrac,AntoineMiech,Ivan
Fung, and Steven Hoi. Instructblip: Towards general- Laptev,andJosefSivic. Lookforthechange: Learningob-
purposevision-languagemodelswithinstructiontuning.Ad- jectstatesandstate-modifyingactionsfromuntrimmedwebvideos.InProceedingsoftheIEEEConferenceonComputer Flan-T5-XXL. We use the prompt ”Question: Does this
VisionandPatternRecognition(CVPR),2022. 1,2 frame depict 1. init state text, 2. action text, 3. end state
[16] ZihuiXue,KumarAshutosh,andKristenGrauman. Learn- text or 4. none of the above” and query each video frame
ingobjectstatechangesinvideos: Anopen-worldperspec- to generate predictions and confidence scores for each.
tive. InProceedingsoftheIEEE/CVFConferenceonCom- GroundingDINO GroundingDINO is a zero-shot object
puterVisionandPatternRecognition, pages18493–18503,
detection model that can identify objects based on textual
2024. 1
input [11]. GroundingDINO combines a Transformer-
[17] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and
based DINO detector with grounded pre-training. We use
LucasBeyer. Sigmoidlossforlanguageimagepre-training,
this model to provide object-centric information to our
2023. 3
experiments.
[18] Chenchen Zhu, Fanyi Xiao, Andre´s Alvarado, Yasmine
Babaei,JiaboHu,HichemEl-Mohri,SeanCulatana,Roshan
Sumbaly, and Zhicheng Yan. Egoobjects: A large-scale
egocentricdatasetforfine-grainedobjectunderstanding. In
ProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages20110–20120,2023. 3
A.Appendix
A.1.ModelDescriptions
CLIP/OpenCLIP ViT-L/14 The first models we consider
are CLIP [13] and OpenCLIP [5] which share the same
architectureandtraining. ThemaindifferenceisthatCLIP
is trained with the private WebImageText dataset whereas
OpenCLIP is trained with the public LAION dataset. For
both CLIP and OpenCLIP model we use the ViT-L/14
architecture. For our analysis we calculate the cosine
similarity between the encoded image and encoded text -
furtherdescribedin.
ALIGN We next use the ALIGN [7] which is proposed to
leverage a noisy dataset of over one billion image alt-text
pairs. TheALIGNisadualencoderwithEfficientNetasits
visionencoderandBERTasitstextencoder.
FLAVAFLAVAisoptimizedonmultiplevision,language,
and cross- and multi-modal tasks [14]. The FLAVA
model contains an image encoder and a text encoders
well as a multimodal encoder combines image and text
representationsforhigher-leveltasks. Bothimageandtext
encoder follows a ViT-B/16 architecture that outputs a list
of state vectors. For the multimodal encoding, the two
listsofstatevectorsarefurtherforwardedtoatransformer FigureA1. VisualizationofInitialandTerminalstatesfromcate-
goriesinChangeIT-Frames
module that is based on the ViT architecture. Different
from the other models, FLAVA learns representations
from multimodal data (image-text pairs) and unimodal
A.2.DataCollectionMethodology
data (unpaired images and text). For our experiments, we
use the image and text encoders and compute text-image Fordataannotation,weprovidedclearandconciseinstruc-
similarityina768-dimensionalembeddingspace. tions to the participants involved in the image annotation
PhysVLMWeusethePhysVLMmodel[4], finetunedon task. The instructions were as follows: “Please draw one
thePhysObjectsdataset,toevaluatehowamodelgrounded bounding box around the object matching one of the pro-
in the physical world, focused on understanding physical vided attribute labels. If the object is unclear or obscured
concepts performs on this sort of task. Intuitively, under- by text, use ’Nothing to Label’. Draw only ONE bound-
standing of physical reasoning would help differentiating ing box per image. If multiple objects with the same at-
object states. Specifically, we use PG-InstructBLIP, a tribute are close together, include all in a single bounding
fine-tunedversionofInstructBLIPwiththelanguagemodel box. If objects are in different states making it difficult toidentifythem,labeltheimageas’CannotDetermine’.” Ad- textencoder:
ditionally,toensureclarityandaccuracyintheannotations,
we provided examples of both good and bad annotations n s =argmax(x s·T(a photo of n)) (1)
n
throughscreenshots. Thiswasintendedtoguidetheanno-
n =argmax(x ·T(a photo of n)) (2)
tators in making judicious decisions about the objects and o n o
their states in the images. Participants were recruited via
In the second stage, a linear head L takes in the object-
Amazon Mechanical Turk, a widely used crowdsourcing
centricrepresentationstopredicttherelationRbetweenob-
platform,andthelabelingtasksweresetupusingAmazon
jectsando. Thelinearheadistrainedonthetrainingset.
SageMaker. Werecruitedparticipantsunderthedefaultset-
ting. The payment for the task was determined according
R=L(x ∥x ) (3)
s o
to Amazon’s recommended guidelines, ensuring fair com-
pensationbasedonthecomplexityofthetaskandthetyp-
ical rates in the participants’ countries of residence. This
approach helped us maintain ethical standards while also
attractingcompetentandmotivatedannotators.
A.3.DetailsonCLEVR-Binding
The CLEVR-binding dataset is split into training, vali-
dation, and generalization set with distinct attribute-noun
pairs. For each object in each image, the answer candi-
datesareformedbytheground-truthpairandfourdistrac-
tor pairs. In details, in the example of image with a red
sphereandbluecube,theanswercandidatesforredsphere
are composed of itself and two distractors that switching
theexistingattributeandnouncompositions(redcubeand
bluesphere)andtworandomlysampledfromothernegative
pairs.Tozero-shotevaluateCLIPanditsvariants’abilityof
attribute-nounbinding,wedesignthefollowingpromptrec-
ommendedbyOpenAI:“aphotoof[adj][noun]”.
A.3.1 RelationalReasoning
Thetwo-objectrelationalreasoningtaskrequiresmodelsto
predicttheobjectsinthesceneandtheirrelationship. The
CLEVR-bindingdatasetcontains3typesofobjects: {cube,
sphere, cylinder} and 4 types of relationship: {left, right,
front,behind}with24possiblecombinationsofspatialre-
lations (Note that the relations are “symmetric”, e.g. cube
leftsphereisequivalenttosphererightcube).
In the setting of relational reasoning, an input image v
containing two objects (s,o). The goal is to predict the
relationship in the format of subject relation-object triple
(n ,R,n ) where n ,n ∈ N = {cube, sphere, cylinder}
s o s o
is the shape of object s and o and R ∈ R = {left, right,
front,behind}isthespatialrelationship.
Weproposeatwo-stagemethodforrelationalreasoning.
Inthefirststage,CLIPtakesintheobject-centricrepresen-
tations(x ,x )extractedbythefrozenCLIPvisionencoder
s o
from the masked images, and zero-shot recognize the ob-
jectsintheimage,wheren ∈ NandT isthefrozenCLIP