[
    {
        "title": "DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control",
        "authors": "Zichen Jeff CuiHengkai PanAadhithya IyerSiddhant HaldarLerrel Pinto",
        "links": "http://arxiv.org/abs/2409.12192v1",
        "entry_id": "http://arxiv.org/abs/2409.12192v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12192v1",
        "summary": "Imitation learning has proven to be a powerful tool for training complex\nvisuomotor policies. However, current methods often require hundreds to\nthousands of expert demonstrations to handle high-dimensional visual\nobservations. A key reason for this poor data efficiency is that visual\nrepresentations are predominantly either pretrained on out-of-domain data or\ntrained directly through a behavior cloning objective. In this work, we present\nDynaMo, a new in-domain, self-supervised method for learning visual\nrepresentations. Given a set of expert demonstrations, we jointly learn a\nlatent inverse dynamics model and a forward dynamics model over a sequence of\nimage embeddings, predicting the next frame in latent space, without\naugmentations, contrastive sampling, or access to ground truth actions.\nImportantly, DynaMo does not require any out-of-domain data such as Internet\ndatasets or cross-embodied datasets. On a suite of six simulated and real\nenvironments, we show that representations learned with DynaMo significantly\nimprove downstream imitation learning performance over prior self-supervised\nlearning objectives, and pretrained representations. Gains from using DynaMo\nhold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,\nand nearest neighbors. Finally, we ablate over key components of DynaMo and\nmeasure its impact on downstream policy performance. Robot videos are best\nviewed at https://dynamo-ssl.github.io",
        "updated": "2024-09-18 17:59:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12192v1"
    },
    {
        "title": "Massively Multi-Person 3D Human Motion Forecasting with Scene Context",
        "authors": "Felix B MuellerJulian TankeJuergen Gall",
        "links": "http://arxiv.org/abs/2409.12189v1",
        "entry_id": "http://arxiv.org/abs/2409.12189v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12189v1",
        "summary": "Forecasting long-term 3D human motion is challenging: the stochasticity of\nhuman behavior makes it hard to generate realistic human motion from the input\nsequence alone. Information on the scene environment and the motion of nearby\npeople can greatly aid the generation process. We propose a scene-aware social\ntransformer model (SAST) to forecast long-term (10s) human motion motion.\nUnlike previous models, our approach can model interactions between both widely\nvarying numbers of people and objects in a scene. We combine a temporal\nconvolutional encoder-decoder architecture with a Transformer-based bottleneck\nthat allows us to efficiently combine motion and scene information. We model\nthe conditional motion distribution using denoising diffusion models. We\nbenchmark our approach on the Humans in Kitchens dataset, which contains 1 to\n16 persons and 29 to 50 objects that are visible simultaneously. Our model\noutperforms other approaches in terms of realism and diversity on different\nmetrics and in a user study. Code is available at\nhttps://github.com/felixbmuller/SAST.",
        "updated": "2024-09-18 17:58:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12189v1"
    },
    {
        "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning",
        "authors": "Zayne SpragueFangcong YinJuan Diego RodriguezDongwei JiangManya WadhwaPrasann SinghalXinyu ZhaoXi YeKyle MahowaldGreg Durrett",
        "links": "http://arxiv.org/abs/2409.12183v1",
        "entry_id": "http://arxiv.org/abs/2409.12183v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12183v1",
        "summary": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications.",
        "updated": "2024-09-18 17:55:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12183v1"
    },
    {
        "title": "A Controlled Study on Long Context Extension and Generalization in LLMs",
        "authors": "Yi LuJing Nathan YanSonglin YangJustin T. ChiuSiyu RenFei YuanWenting ZhaoZhiyong WuAlexander M. Rush",
        "links": "http://arxiv.org/abs/2409.12181v2",
        "entry_id": "http://arxiv.org/abs/2409.12181v2",
        "pdf_url": "http://arxiv.org/pdf/2409.12181v2",
        "summary": "Broad textual understanding and in-context learning require language models\nthat utilize full document contexts. Due to the implementation challenges\nassociated with directly training long-context models, many methods have been\nproposed for extending models to handle long contexts. However, owing to\ndifferences in data and model classes, it has been challenging to compare these\napproaches, leading to uncertainty as to how to evaluate long-context\nperformance and whether it differs from standard evaluation. We implement a\ncontrolled protocol for extension methods with a standardized evaluation,\nutilizing consistent base models and extension data. Our study yields several\ninsights into long-context behavior. First, we reaffirm the critical role of\nperplexity as a general-purpose performance indicator even in longer-context\ntasks. Second, we find that current approximate attention methods\nsystematically underperform across long-context tasks. Finally, we confirm that\nexact fine-tuning based methods are generally effective within the range of\ntheir extension, whereas extrapolation remains challenging. All codebases,\nmodels, and checkpoints will be made available open-source, promoting\ntransparency and facilitating further research in this critical area of AI\ndevelopment.",
        "updated": "2024-09-23 14:39:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12181v2"
    },
    {
        "title": "Finetuning Language Models to Emit Linguistic Expressions of Uncertainty",
        "authors": "Arslan ChaudhrySridhar ThiagarajanDilan Gorur",
        "links": "http://arxiv.org/abs/2409.12180v1",
        "entry_id": "http://arxiv.org/abs/2409.12180v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12180v1",
        "summary": "Large language models (LLMs) are increasingly employed in information-seeking\nand decision-making tasks. Despite their broad utility, LLMs tend to generate\ninformation that conflicts with real-world facts, and their persuasive style\ncan make these inaccuracies appear confident and convincing. As a result,\nend-users struggle to consistently align the confidence expressed by LLMs with\nthe accuracy of their predictions, often leading to either blind trust in all\noutputs or a complete disregard for their reliability. In this work, we explore\nsupervised finetuning on uncertainty-augmented predictions as a method to\ndevelop models that produce linguistic expressions of uncertainty.\nSpecifically, we measure the calibration of pre-trained models and then\nfine-tune language models to generate calibrated linguistic expressions of\nuncertainty. Through experiments on various question-answering datasets, we\ndemonstrate that LLMs are well-calibrated in assessing their predictions, and\nsupervised finetuning based on the model's own confidence leads to\nwell-calibrated expressions of uncertainty, particularly for single-claim\nanswers.",
        "updated": "2024-09-18 17:52:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12180v1"
    }
]