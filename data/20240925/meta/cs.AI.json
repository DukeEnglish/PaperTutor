[
    {
        "title": "Vista3D: Unravel the 3D Darkside of a Single Image",
        "authors": "Qiuhong ShenXingyi YangMichael Bi MiXinchao Wang",
        "links": "http://arxiv.org/abs/2409.12193v1",
        "entry_id": "http://arxiv.org/abs/2409.12193v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12193v1",
        "summary": "We embark on the age-old quest: unveiling the hidden dimensions of objects\nfrom mere glimpses of their visible parts. To address this, we present Vista3D,\na framework that realizes swift and consistent 3D generation within a mere 5\nminutes. At the heart of Vista3D lies a two-phase approach: the coarse phase\nand the fine phase. In the coarse phase, we rapidly generate initial geometry\nwith Gaussian Splatting from a single image. In the fine phase, we extract a\nSigned Distance Function (SDF) directly from learned Gaussian Splatting,\noptimizing it with a differentiable isosurface representation. Furthermore, it\nelevates the quality of generation by using a disentangled representation with\ntwo independent implicit functions to capture both visible and obscured aspects\nof objects. Additionally, it harmonizes gradients from 2D diffusion prior with\n3D-aware diffusion priors by angular diffusion prior composition. Through\nextensive evaluation, we demonstrate that Vista3D effectively sustains a\nbalance between the consistency and diversity of the generated 3D objects.\nDemos and code will be available at https://github.com/florinshen/Vista3D.",
        "updated": "2024-09-18 17:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12193v1"
    },
    {
        "title": "DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control",
        "authors": "Zichen Jeff CuiHengkai PanAadhithya IyerSiddhant HaldarLerrel Pinto",
        "links": "http://arxiv.org/abs/2409.12192v1",
        "entry_id": "http://arxiv.org/abs/2409.12192v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12192v1",
        "summary": "Imitation learning has proven to be a powerful tool for training complex\nvisuomotor policies. However, current methods often require hundreds to\nthousands of expert demonstrations to handle high-dimensional visual\nobservations. A key reason for this poor data efficiency is that visual\nrepresentations are predominantly either pretrained on out-of-domain data or\ntrained directly through a behavior cloning objective. In this work, we present\nDynaMo, a new in-domain, self-supervised method for learning visual\nrepresentations. Given a set of expert demonstrations, we jointly learn a\nlatent inverse dynamics model and a forward dynamics model over a sequence of\nimage embeddings, predicting the next frame in latent space, without\naugmentations, contrastive sampling, or access to ground truth actions.\nImportantly, DynaMo does not require any out-of-domain data such as Internet\ndatasets or cross-embodied datasets. On a suite of six simulated and real\nenvironments, we show that representations learned with DynaMo significantly\nimprove downstream imitation learning performance over prior self-supervised\nlearning objectives, and pretrained representations. Gains from using DynaMo\nhold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,\nand nearest neighbors. Finally, we ablate over key components of DynaMo and\nmeasure its impact on downstream policy performance. Robot videos are best\nviewed at https://dynamo-ssl.github.io",
        "updated": "2024-09-18 17:59:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12192v1"
    },
    {
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
        "authors": "Peng WangShuai BaiSinan TanShijie WangZhihao FanJinze BaiKeqin ChenXuejing LiuJialin WangWenbin GeYang FanKai DangMengfei DuXuancheng RenRui MenDayiheng LiuChang ZhouJingren ZhouJunyang Lin",
        "links": "http://arxiv.org/abs/2409.12191v1",
        "entry_id": "http://arxiv.org/abs/2409.12191v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12191v1",
        "summary": "We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL\nmodels that redefines the conventional predetermined-resolution approach in\nvisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,\nwhich enables the model to dynamically process images of varying resolutions\ninto different numbers of visual tokens. This approach allows the model to\ngenerate more efficient and accurate visual representations, closely aligning\nwith human perceptual processes. The model also integrates Multimodal Rotary\nPosition Embedding (M-RoPE), facilitating the effective fusion of positional\ninformation across text, images, and videos. We employ a unified paradigm for\nprocessing both images and videos, enhancing the model's visual perception\ncapabilities. To explore the potential of large multimodal models, Qwen2-VL\ninvestigates the scaling laws for large vision-language models (LVLMs). By\nscaling both the model size-with versions at 2B, 8B, and 72B parameters-and the\namount of training data, the Qwen2-VL Series achieves highly competitive\nperformance. Notably, the Qwen2-VL-72B model achieves results comparable to\nleading models such as GPT-4o and Claude3.5-Sonnet across various multimodal\nbenchmarks, outperforming other generalist models. Code is available at\n\\url{https://github.com/QwenLM/Qwen2-VL}.",
        "updated": "2024-09-18 17:59:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12191v1"
    },
    {
        "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning",
        "authors": "Zayne SpragueFangcong YinJuan Diego RodriguezDongwei JiangManya WadhwaPrasann SinghalXinyu ZhaoXi YeKyle MahowaldGreg Durrett",
        "links": "http://arxiv.org/abs/2409.12183v1",
        "entry_id": "http://arxiv.org/abs/2409.12183v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12183v1",
        "summary": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications.",
        "updated": "2024-09-18 17:55:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12183v1"
    },
    {
        "title": "Computational Dynamical Systems",
        "authors": "Jordan CotlerSemon Rezchikov",
        "links": "http://arxiv.org/abs/2409.12179v1",
        "entry_id": "http://arxiv.org/abs/2409.12179v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12179v1",
        "summary": "We study the computational complexity theory of smooth, finite-dimensional\ndynamical systems. Building off of previous work, we give definitions for what\nit means for a smooth dynamical system to simulate a Turing machine. We then\nshow that 'chaotic' dynamical systems (more precisely, Axiom A systems) and\n'integrable' dynamical systems (more generally, measure-preserving systems)\ncannot robustly simulate universal Turing machines, although such machines can\nbe robustly simulated by other kinds of dynamical systems. Subsequently, we\nshow that any Turing machine that can be encoded into a structurally stable\none-dimensional dynamical system must have a decidable halting problem, and\nmoreover an explicit time complexity bound in instances where it does halt.\nMore broadly, our work elucidates what it means for one 'machine' to simulate\nanother, and emphasizes the necessity of defining low-complexity 'encoders' and\n'decoders' to translate between the dynamics of the simulation and the system\nbeing simulated. We highlight how the notion of a computational dynamical\nsystem leads to questions at the intersection of computational complexity\ntheory, dynamical systems theory, and real algebraic geometry.",
        "updated": "2024-09-18 17:51:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12179v1"
    }
]