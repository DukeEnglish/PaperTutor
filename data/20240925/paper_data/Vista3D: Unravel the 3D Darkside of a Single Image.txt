Vista3D: Unravel the 3D Darkside of
a Single Image
Qiuhong Shen1, Xingyi Yang1, Michael Bi Mi2, and Xinchao Wang1⋆
1 National University of Singapore 2 Huawei Technologies Ltd
{qiuhong.shen,xyang}@u.nus.edu xinchao@nus.edu.sg
Abstract. We embark on the age-old quest: unveiling the hidden di-
mensionsofobjectsfrommereglimpsesoftheirvisibleparts.Toaddress
this, we present Vista3D, a framework that realizes swift and consis-
tent 3D generation within a mere 5 minutes. At the heart of Vista3D
lies a two-phase approach: the coarse phase and the fine phase. In the
coarse phase, we rapidly generate initial geometry with Gaussian Splat-
ting from a single image. In the fine phase, we extract a Signed Dis-
tance Function (SDF) directly from learned Gaussian Splatting, opti-
mizing it with a differentiable isosurface representation. Furthermore, it
elevates the quality of generation by using a disentangled representa-
tionwithtwoindependentimplicitfunctionstocapturebothvisibleand
obscured aspects of objects. Additionally, it harmonizes gradients from
2D diffusion prior with 3D-aware diffusion priors by angular diffusion
prior composition. Through extensive evaluation, we demonstrate that
Vista3D effectively sustains a balance between the consistency and di-
versityofthegenerated3Dobjects.Demosandcodewillbeavailableat
https://github.com/florinshen/Vista3D.
Keywords: 3D Generation · 3D Reconstruction · Score Distillation
1 Introduction
Sincetheearliesttimes,ourancestorsgazedupontheluminousmoon,asymbol
of mystery and wonder. Its bright facade, an elegant sphere in the cosmos, has
alwaysmadeusthinkaboutwhatremainshidden:themoon’sobscureandelusive
darkside.Thiscuriosity,asancientashumanhistoryitself,representsourinnate
desire to uncover the concealed dimensions that exist beyond the visible.
This quest, once purely philosophical, has now ventured into the realm of
practicality,propelledbytheadvancementsin3Dgenerativemodel[29,34,42,45,
48].Thesetechnologiesenableabroadrangeofapplications,especiallyingaming
and virtual reality, allowing for the creation of rich, detailed environments and
objects without extensive modeling.
Nevertheless,thedevelopmentofrobustlarge-scale3Dgenerativemodelsre-
mainsaformidablechallenge,predominantlyduetothelimitedavailabilityof3D
data.Numerousattempts[1,13,27]havebeenmadetotrain3Ddiffusionmodels
on relatively small 3D datasets, condition on textual or visual prompts; Yet,
⋆ Corresponding Author.
4202
peS
81
]VC.sc[
1v39121.9042:viXra2 Shen et al.
AgreyBMWX5
AgreyVolvoXC90
Awhite skin horse with flowing mane Asleek silver horse with flowing mane
Apalecream-colorbearpillow
Apalecream-colorpandapillow
Reference Amuscle dog with capital S Amuscle dog wearing a superman cape
Fig.1:3DDarksideofSingleImage.Byemployingvarioustextprompts,Vista3D
iscapableofunveilingthediversityofunseenviewswhileretaining3Dconsistencyand
detail. Two novel views and the normal map are visualized for each text prompt.
these endeavors often fall short in creating 3D objects with structural integrity
and textural consistency.
This challenge is further compounded in the context of reconstructing 3D
objectsfromsingleimages.Inthiscontext,twoprimaryapproachesemerge.The
firstconsidersthetaskasaproblemofsparse-viewreconstruction.However,this
oftenleadstoblurred3Doutputsduetotheneglectofunseenelements,resulting
in excessively blurred 3D objects [8,52] as most views remain unseen.
On the other hand, the generative approach, which leverages large-scale 2D
diffusion models [29,42], introduces its own set of challenges. Efforts to develop
3D-aware 2D diffusion models [19,21,30,32,34,39,40,51] involve fine-tuning 2D
modelswithcameratransformationmodelingon3Ddatasets[5,6].Nevertheless,
the prevalence of synthetic objects in these datasets can lead to a compromise
in 2D diversity. This often results in the generation of oversimplified geometries
and textures.
In this paper, we present Vista3D, a framework designed for reconstructing
the unseen view (or "darkside") from a single image. Central to Vista3D is a
dual-phase strategy: a coarse phase followed by a fine phase.
In the coarse phase, we leverage 3D Gaussian splatting [14] to swiftly cre-
ate basic geometry and textures. To stabilize Gaussian Splatting optimization,
we employ a gradient-based Top-K densification strategy, focusing on Gaussian
points with the highest gradients. Additionally, we introduce two novel regular-
izationtermstargetingtheGaussianscaleandtransmittancevalues,significantly
enhancing the convergence speed.
The fine phase then transforms this initial geometry into signed distance
fields (SDF) for further optimization. Here, we employ FlexiCubes [38], an ad-
vanced differentiable isosurface technique, to refine the geometry. This refine-Vista3D 3
ment aids in learning the signed distance fields (SDFs), deformation, and in-
terpolation weights. The parameters are optimized by ensuring fidelity to the
original image and guided by a score function derived from diffusion priors.
Despite these advancements, a unified representation and supervision across
allviews,bothseenandunseen,proveinsufficientforcapturingtheuniquechar-
acteristics of different viewpoints and generating diverse, consistent 3D objects.
To address this, we enhance the representation by implementing Disentangled
Texture Representation, using two angularly disentangled networks for accurate
texture prediction. Furthermore, our Angular-based Composition method amal-
gamates different diffusion priors, adjusting their gradients within specific an-
gular bounds according to their gradient magnitudes. This strategic adjustment
assures 3D consistency while promoting diversity in the unseen views.
Vista3D excels in efficiently generating diverse and consistent 3D objects
from a single image within five minutes. Our extensive evaluations demonstrate
its ability to maintain a flexible balance between the consistency and diversity
of the generated 3D objects.
We summarize our contribution as follows:
– We present Vista3D, a framework for revealing the 3D darkside of single
images, efficiently generating diverse 3D objects using 2D priors.
– We develop a transition from Gaussian Splatting to isosurface 3D represen-
tations,refiningcoarsegeometrywithadifferentiableisosurfacemethodand
disentangled texture for textured mesh creation.
– Weproposeanangularcompositionapproachfordiffusionpriors,constrain-
ingtheirgradientmagnitudestoachievediversityonthe3Ddarksidewithout
sacrificing 3D consistency.
2 Related-works
2.1 3D Generation Conditioned on a Single Image
The objective of image-to-3D generation is to create 3D objects from a single
reference image. Initial methods [8,52] approached this challenge as a variant of
sparseview3Dreconstruction.However,thesemethodsoftenresultedinblurred
objectoutputsduetoinsufficientpriors.Recently,drawinginspirationfromtext-
to-3D initiatives that utilize Score Distillation Sampling (SDS) to elevate 2D
diffusion priors into 3D generative models, image-to-3D works [24,33,34,40,42]
have adopted a similar approach for 3D object generation based on a single im-
age. However, 2D diffusion priors alone cannot ensure 3D consistency, as they
are typically trained solely on image datasets. To address this, several stud-
ies [19–21,39] have attempted to refine 2D diffusion priors with 3D data [5,6],
enhancingtheirabilitytomodel3Dconsistency.AnotableexampleisZero-1-to-
3,whichcangeneratenovelviewsconditiononsingleimageandcameraposition.
Integrating this refined model with SDS [30,41] allows for the reconstruction of
coherent 3D objects. Moreover, another stream of works [9,17,36,46,47,50,55]
pretrained on large-scale 3D dataset [5] directly predicting the representation4 Shen et al.
of a 3D object from a single image. Diverging from previous works, our work
does not solely view this as a 3D reconstruction issue. We redefine it as a 3D
generation task aimed at uncovering the unseen 3D aspects behind a single im-
age. Through a meticulously crafted framework, our method efficiently generate
diverse and consistent 3D objects.
2.2 3D Representations for Generation
Presently, most zero-shot text-to-3D and image-to-3D models utilize an opti-
mization based pipeline, parameterizing the 3D object as a differentiable repre-
sentation,whichvariesamongdifferentmethods.Themostprevalentrepresenta-
tioningroundbreakingworkslikedreamfields[12],dreamfusion[29],andSJC[43]
is Neural Radiance Fields (NeRF) [25]. However, training a NeRF is computa-
tionally intensive and takes long time to convergence. Magic3D [16] introduced
atwo-stagerepresentation,initiallylearningacoarseNeRF,followedbyrefining
the polygon mesh using a differentiable isosurface method, DMTet [37]. Fanta-
sia3D [2]suggested directlyoptimizingDMTet [37]in separatephases for geom-
etry and texture, but this often leads to mode collapse in the geometry phase
and extends training time beyond NeRF. Gaussian Splatting [10,14,35,44,53]
hasgainedattentionforitsefficiencyinvarious3Dtasks,withseveral3Dgener-
ative models [3,4,41,49] incorporating it for effective generation. However, as a
point-basedrepresentation,itcannotyieldhigh-fidelitymeshes.Inourapproach,
weemployGaussianSplattingexclusivelytocreatecoarsegeometry.Thiscoarse
geometry is then transformed into SDF, optimized with a hybrid isosurface rep-
resentation, FlexiCubes [38], to produce high-fidelity meshes. Additionally, we
propose an angular disentangled texture representation, tailored to the specifics
of this task.
3 Methodology
In this section, we outline our framework to generate detailed 3D object from
single image with 2D diffusion priors. As depicted in Figure 2, our exploration
of the 3D darkside of a single image commences with the efficient generation
of basic geometry (Section 3.1), represented through 3D Gaussian Splatting. In
refinement stage (Section 3.2), we devise a method for transforming the rudi-
mentary 3D Gaussian geometry into signed distance fields, and thereafter, we
introduceadifferentiableisosurfacerepresentationtofurtherenhancethegeome-
tryandtextures.Toenablediverse3Ddarksideofgivensingleimage,wepresent
anovelapproachtoconstraintwodiffusionpriors(Section3.3),enablingthecre-
ation of varied yet coherent darkside textures by bounding gradient magnitude.
Withtheseapproaches,ourmethodcanefficientlygeneratediverse,high-fidelity
meshes from a single image.Vista3D 5
Rasterization
𝐿!"#+𝐿$%&’
3DPrior
Stage I SDS
Top-KDensification Regularization
GaussianSplatting
InputImage
SDF
Stage II 𝜃"
𝐿!"#+𝐿$%&’ 2DPrior 3DPrior
𝜃!
DisentangledTexture Angular-based
FlexiCubes Representation SDS Composition
Fig.2: Overview of Vista3D. We generate high-fidelity mesh from single image
input in a coarse-to-fine manner. In the coarse stage, we utilize Gaussian Splatting to
learn a coarse geometry with a 3D-aware 2D diffusion prior. We further extract sign
distance fields from Gaussian Splatting for refinement. Another 2D diffusion prior is
enabledwithanangular-basedcompositiontoexplorediversedarksidewhileretain3D
consistency in refinement stage.
3.1 Coarse geometry from Gaussian Splatting
Inthecoarsestageofourframework,wefocusonconstructingabasicobjectge-
ometryusingGaussianSplatting.Thistechnique,asdescribedin[14],represents
3D scenes as set of anisotropic 3D Gaussians. Compared to other neural inverse
rendering methods, such as NeRF [25,26], Gaussian Splatting demonstrates a
notably faster convergence speed in inverse rendering tasks.
Some works [3,41,49] has attempted to introduce Gaussian Splatting into
3D generative models. In these methods, we found that directly using Gaussian
splatting to generate detailed 3D objects requires optimizing a large number of
3D Gaussians, necessitating significant time for optimization and densification,
which is still time-consuming. However, Gaussian Splatting can quickly create
a coarse geometry from a single image using a limited number of 3D Gaussians
withinjustoneminute.Therefore,inourapproach,weutilizeGaussianSplatting
solely for the initial coarse geometry generation.
Specifically, each 3D Gaussians is parameterized by its central position x ∈
R3, scaling r ∈ R, rotation quaternion q ∈ R4, opacity α ∈ R, and spherical
harmonicsc∈R3 torepresentcolor.Togenerateacoarse3Dobject,weoptimize
a set of these Gaussian parameters Ψ = {Φ }, where Φ = {x ,r ,q ,α ,c }. To
i i i i i i i
render 3D Gaussians to 2D images, we utilized the highly-optimized tile based
rasterization implementation [14].
To generate the coarse geometry of given single image I , we adopt Zero-
ref
1-to-3 XL [5,19] as 2D diffusion priors ϵ with pretrained parameters ϕ. This
ϕ
priorenablesdenoisingofnovelviewsbasedonthegivenimageI andrelative
ref
camera pose ∆π. Accordingly, we optimize the 3D Gaussians Ψ with SDS [29]:
(cid:20) ∂Iπ(cid:21)
∇ L =E (ϵ (Iπ;t,I ,∆π)−ϵ) R (1)
Ψ SDS t,ϵ ϕ R ref ∂Ψ6 Shen et al.
where π denotes the camera pose sampled around the object with fixed camera
radius and FoV, Iπ is the rendered image from 3D Gaussian set Ψ with cam-
R
era pose π, timestep t is annealed to weight the gaussian noise ϵ added to the
rendered image. Beyond this basic approach, we introduce a Top-K Gradient-
based Densification strategy to accelerate convergence and add two regulariza-
tion terms to enhance the reconstructed geometry.
Top-K Gradient-based Densification. In the optimization process, we find
theperiodicaldensification[14]withnaivegradientthresholdishardtotunedue
tothenaturerandomnessofSDS.Soweinsteaduseamorerobustdensification
strategy.Onlygaussianspointswithtop-kgradientswillbedensifiedduringeach
interval, this simple strategy can stablize training cross various given images.
Scale & Transmittance Regularization. Additionally, We add two regular-
ization terms to encourage Gaussian Splatting to learn more detailed geometry
inthisphase.Ascaleregularizationisintroducedtoavoidtoolarge3dgaussians,
and another transmittance regularization is adopted to encourage the geometry
learning from transparent to solid. The overall loss function in this stage can be
written as:
∇ L =λ ∇ L +λ ∇ L
Ψ coarse SDS Ψ SDS rgb Ψ rgb
(cid:88)
+λ ∇ L +λ ∇ ∥s ∥
mask Ψ mask scale Ψ i
i
(cid:124) (cid:123)(cid:122) (cid:125)
ScaleRegularization (2)
1 (cid:88)
−λ ∇ min(τ, T );
tr Ψ N k
fg
k
(cid:124) (cid:123)(cid:122) (cid:125)
TransmittanceRegularization
where L and L are two MSE loss computed between the rendered refer-
rgb mask
ence view and the given image. The term T
=(cid:80)
α
(cid:81)i−1(1−α
) denotes the
k i i j=1 j
transmittance value for the k-th pixel in Iπ, where N is the total number of
R fg
foreground pixels. Additionally, τ serves as a hyperparameter that is gradually
annealed from 0.4 to 0.9, effectively regularizing transmittance over time.
3.2 Mesh refinement and texture disentanglement
In the refinement stage, our focus shifts to transforming the coarse geometry,
produced via Gaussian splatting, into signed distance fields (SDF) and refining
its parameters using a hybrid representation.
This stage is crucial for overcoming the challenges presented in the coarse
stage,notablythesurfaceartifactsfrequentlyintroducedbyGaussiansplatting.
Due to the inability of Gaussian splatting to provide direct estimates of surface
normals, we cannot employ traditional smoothing methods to alleviate these
artifacts.Tocounterthis,ourmethodincorporatesahybridmeshrepresentation,
which entails modeling the 3D object’s geometry as a differentiable isosurface
and learning the texture using two distinct, disentangled networks. This dual
approach not only smooths out the surface irregularities but also significantly
improves the fidelity and overall quality of the 3D model.Vista3D 7
Geometry representation. We utilize FlexiCubes to represent the geometry
in our approach. FlexiCubes is a differentiable isosurface representation which
allow local flexible adjustments to the extracted mesh geometry and connectiv-
ity [38]. The geometry of an object is depicted as a deformable voxel grid with
learnable weights. Deformation δ ∈ R3 and sign distance field (SDF) s ∈ R is
i i
learnt for every vertices v in the voxel grid. And interpolation weights β ∈R20
i
and splitting weights γ ∈R are learnt for each grid cell to position dual vertices
andcontrolquadrilateralssplitting.Trianglemeshescanbeextractedfromitdif-
ferentiablely through Dual Marching Cubes [28]. To bridge the gap between the
learned coarse geometry and the isosurface representation, we initially extract a
density field from Gaussian splattings using local density queries [41], followed
by the application of marching cubes [22] to extract a base mesh M . Sub-
coarse
sequently,wequerythisbasemeshatgridverticesv toobtaintheinitialSigned
i
Distance Field (SDF) s(v ). For stable optimization, the queried SDF is then
i
scaled as follows:
ξ·s(v )
s(v )= i , where S ={s } (3)
i max{|s |:s ∈S,s <0} i
j j j
where s < 0 indicates the field within the object. The scale factor ξ linearly
j
increases from 1 to 3 during the optimization process.
Disentangled Texture Representation. For texture learning, we employ
hash encoding followed by a MLP to directly learn albedo. However, distinct
from text-to-3D tasks, we recognize two primary supervision sources in this
task: the provided reference image and the SDS gradient from 2D Diffusion pri-
ors. Typically, a substantial loss weight λ is assigned for the reference image.
rgb
Thisdominantreferenceimagesupervisioncandeceleratetheconvergenceoftex-
tures in unseen views, particularly when unseen views significantly differ from
the reference view.
To address this, we separate the texture into two hash encoding, utilizing a
ratio that combines with the relative azimuth angle ∆θ = θ −θ , where θ
π ref π
representstheazimuthofthesampledcameraposeπ,andθ istheazimuthof
ref
thereferenceimage.Thehashencodingforagivenquerypointκintherasterized
triangle mesh is expressed as:
E =(1−η)H (κ)+ηH (κ) (4)
back ref
where H and H denote learnable hash encoding facing forward and back,
ref back
η =(cos(∆θ)+1)/2 is the balance factor that varies with the sampled azimuth
angle. Then the encoded feature E is fed into a MLP predict albedo values.
Withthesegeometryandtexturerepresentation,wecanrenderthe3Dobject
to images by memory-efficient rasterization coupled with lambertian shading.
Above learnable parameters Θ is refined with ∇ L :
Θ refine
∇ L =λ ∇ L
Θ refine SDS Θ SDS
+λ ∇ L +λ ∇ L (5)
SDF Θ SDF consistency Θ consistency
+λ λ ∇ L +λ ∇ L ;
rgb SDS Θ rgb mask Θ mask8 Shen et al.
wheretheL isasimpleSDFregulariztiontermtoavoidfloaters,L
SDF consistency
isasmoothlossappliedonsurfacenormals[16,24],L andL aretwoMSE
rgb mask
loss between the rendered reference view and the given image.
3.3 Darkside Diversity via Prior Composition
Inimplementingourpipeline,weencounteredakeychallengerelatedtothelack
of diversity in unseen views. This issue largely stems from the reliance on the
Zero-1-to-3 XL prior, a model trained on synthetic 3D objects from Objaverse-
XL [5]. While this prior is adept at handling 3D-aware generation based on
reference images and relative camera poses, it tends to produce oversimplified
or overly smooth results in unseen views. This limitation becomes especially
pronounced when dealing with objects captured in the real world.
Toaddressthis,weintegrateanadditionalpriorfromStable-Diffusion,known
for its ability to synthesize diverse images.
Darkside diversification with 2D diffusion.Weintroduceasecondprior,ϵ
ρ
with pretrained parameters ρ, leading to two Score Distillation Sampling (SDS)
loss terms ∇Lϕ and ∇Lρ (Equation 1) for optimization. The optimal
SDS SDS
balancebetweenthesetwopriorsremainsrelativelyunexplored.WhileMagic123
[30] uses an empirical loss weight of 1/40 for the latter term, this approach may
not fully harness the potential of the 2D prior. The key objective in introducing
this 2D prior is to introduce greater diversity in unseen view. A small weight
with ∇Lρ may largely limit its effect.
SDS
Toenhancethediversityintheunseenaspectsofthegivenimage,weemploy
agradientconstrainmethodtomergethesetwopriors.WereformulatetheSDS
loss as a score function [29], ∇ L (ϕ, x) = −E ∇ logp (z |y), where t
Θ SDS t,zt|x Θ ϕ t
is the timestep and z is noise latent.
t
Here ∇Lϕ is a 3D-aware term conditioned on y = {∆π,I }, while
SDS ref
∇Lρ is a diverse text-to-image term conditioned on text prompt y = P .
SDS T
With different condition y, the score function of these two SDS term varies. To
retain 3D consistency of unseen views, the magnitude of ∇ logp (z |y) need to
Θ ρ t
be constrained with respect to the 3D-aware term ∇ logp (z |y). And to avoid
Θ ϕ t
thetexturetobeover-smoothedbythe3D-awarediffusionmodel,themagnitude
of ∇ logp (z |y) is indeed to be constrained with the ∇ logp (z |y) term.
Θ ϕ t Θ ρ t
Angular-based Score Composition.Sincethenoiselatentsz inbothpriors
t
have different encoding spaces, direct evaluation of their magnitudes using the
predictednoisedifferenceϵ −ϵisnotfeasible.Instead,weevaluatethemagnitude
ρ
of these terms by observing their gradient on the rendered image x, specifically
∇ L . Consequently, we establish upper and lower bounds for the gradient
x SDS
magnituderatioofthesetwoSDSterms,allowingforamoreaccurateandfeasible
evaluation method:
||∇ Lρ ||
B (η,ι)≤G= x SDS 2 ≤B (η,ι) (6)
lower ||∇ Lϕ || upper
x SDS 2
When this ratio exceeds B , we adjust the magnitude of ∇ Lρ using
upper x SDS
the factor B /G. Conversely, if the ratio falls below B , we scale the
upper lowerVista3D 9
magnitudeof∇ Lϕ usingG/B .AndthisB andB areregulated
x SDS lower upper lower
by the balance factor η, influenced by the camera pose, and by iterations ι,
facilitating a balance between diversity and 3D consistency.
4 Experiments
4.1 Implementation Details
Coarse geometry learning. In this phase, the input image undergoes pre-
processing with SAM [15,23,34], where the object is extracted and recentered.
We initialize all 3D Gaussians with an opacity of 0.1 and a grey color, confined
withinasphereofradius0.5.Therenderingresolutionisprogressivelyincreased
from 64 to 512. This stage involves a total of 500 optimization steps, with the
densification and pruning of 3D Gaussians occurring every 100 iterations. The
top-Kdensificationstartsataratioof0.5andgraduallyannealsto0.1,whilethe
pruning opacity remains constant at 0.1. After the first densification, transmit-
tance regularization is activated and selectively applied to the top-80% opacity
values of 3D Gaussians to avoid affecting transparent Gaussians. Scale regular-
ization is enforced using L norm. The weights of λ and λ are maintained
1 scale tr
at0.01and1,respectively,throughouttheoptimization,whereasλ andλ
rgb mask
aregraduallyincreasedfrom0to10000and1000,respectively.Thetimestepfor
SDSislinearlyannealedfrom980to20.Forcameraposesampling,theazimuth
is sampled in the range of [−180,180] and elevation in [−45,45], with a fixed
radius of r =2. This phase of optimizing the coarse geometry takes about 30 s.
Mesh refinement. In the refinement phase, we configure the grid size of Flex-
iCubes to 803 within the space [−1,1]3. The coarse geometry obtained from the
initial stage is recentered and rescaled to initialize the Signed Distance Field
(SDF) for the vertices of this grid. Interpolation weights are set to 1, and all
deformationsstartat0.Fortexture,weusetwohashencodingswithatwo-layer
Multilayer Perceptron (MLP). The batch size is maintained at 4. The learning
ratefordeformationandinterpolationweightsis0.005,whileit’s0.001forSDF,
and0.01fortextureparameters.Therenderingresolutionisgraduallyincreased
from 64 to 512. In Equation 5, the loss weights are set as follows: λ = 1500,
rgb
λ = 5000, λ = 1, and λ = 1. We develop two versions for opti-
mask sdf SDS
mization: Vista3D-S and Vista3D-L. Vista3D-S performs 1000 steps of op-
timization solely with the 3D-aware prior, aiming to generate 3D mesh within
5 minutes. Vista3D-L undergoes 2000 steps of optimization with two diffusion
priors to create more detailed 3D objects. The entire optimization process for
Vista3D ranges from 15 to 20 minutes. In this stage, camera poses are sampled
using a 3D-aware Gaussian unsampling strategy to expedite convergence (addi-
tional details are provided in the supplementary material). All experiments are
conducted on an RTX3090 GPU.
Score distillation sampling. In SDS optimization, the practice of linearly
annealingthetimestepttoadjustthenoiselevelhasbeenestablishedaseffective
for producing higher-quality 3D objects [11]. However, in our experiments, we
observedthatlinearannealingmaynotbetheoptimalstrategy.Consequently,we10 Shen et al.
haveimplementedanintervalannealingapproach.Inthisapproach,thetimestep
t is randomly sampled from an annealing interval rather than adhering to a
fixed linear progression. This strategy has been found to effectively mitigate the
artifacts commonly observed with linear annealing.
Reference DreamGaussian Magic123 Vista3D-S(𝟐𝟎×faster)
2minutes 2hours 5minutes
Fig.3: Qualitative Comparison on image-to-3D generation. We compare our
Vista3D-S with DreamGaussian [41], and Magic123 [30]. Vista3D-S only takes 5 min-
utestoreconstructsingle3Dobject,yieldingcompetitivegeometryandmoreconsistent
textures compared to Magic123 [30] with 20× speedup.
Angular diffusion prior composition. In our model, we utilize two diffusion
models:Zero-1-to-3XL[5,19]andtheStable-Diffusionmodel[31].FortheStable-
Diffusion model, the timestep t is scaled by the factor η to ensure consistency
with the reference view. When editing with both diffusion priors, we start with
alargeinitialupperboundB =100,whichislinearlyannealedto10across
upper
optimization iterations. For front-facing views, where η > 0.75, we adjust the
upperboundusingthefactor(1−η).Thelowerboundisspecificallyimplemented
for unseen views with η < 0.5, and its range is gradually reduced from 10 to 1
during the optimization process. For enhancements using the diffusion prior,
we apply tighter constraints, with B being reduced from 2 to 0.5. The
upper
text prompts utilized for the Stable-Diffusion model are derived from the image
captions generated by GPT-4.Vista3D 11
a white standing panda, cartoon style
An anthropomorphic cat wearing a tweed outfit,
complete with a matching cap
An anthropomorphic bull in a casual grey
sweaterand blue jeans
Two cute pandas stacked on top of each other
A dark orange and green dinosaur, lifelike style
Reference One-2-3-45 Wonder3D Vista3D-L(ours)
Fig.4: Qualitative Comparison with One-2-3-45 [18] and Wonder3D [21].In
this comparison, we render two views of each 3D object as generated by One-2-3-45
and Wonder3D. For Vista3D-L, we detail the text prompts utilized for the generation
of each 3D object, showcasing three rendered views alongside a single normal map for
a comprehensive comparison.
4.2 Qualitative Comparison
InFigure3,weshowourefficientVista3D-Siscapableofgeneratingcompetitive
3Dobjectswitha20×speedupcomparedtoexistingcoarse-to-finemethods.For
Vista3D-L, as depicted in Figure 1 and Figure 4, we highlight our angular gra-
dient constraint which distinguishes our framework from previous image-to-3D
methods, as it can explore the diversity of the backside of single images with-
outsacrificing3Dconsistency.InFigure3,weprimarilycompareourVista3D-S
with two baselines, Magic123 [30] and DreamGaussian [41], for generating 3D
objects from a single reference view. Regarding the quality of generated 3D ob-
jects, our method outperforms these two methods in terms of both geometry
andtexture.RegardingVista3D-L,wecompareitwithtwoinference-onlysingle
view reconstruction models, specifically One-2-3-45 [18] and Wonder3D [21]. As
shown in Fig. 4, One-2-3-45 tends to produce blurred texture and may result
in incomplete geometry for more complex objects, while our Vista3D-L achieves
more refined textures, particularly on the backside of 3D objects, using user-
specified text prompts. And Wonder3D often resorts to simpler textures due to
itsprimarytrainingonsyntheticdatasets[5],whichoccasionallyleadstoout-of-12 Shen et al.
distributionissuesforcertainobjects.Incontrast,Vista3D-Lofferszero-shot3D
object reconstruction by controlling two diffusion priors, enabling more detailed
and consistent textural. Moreover, given that only a single reference view of the
objectisprovided,wepositthattheobjectshouldbeamenabletoeditingduring
optimization with user-specified prompts. To illustrate this, we display several
results in Figure 1 that emphasize the potential for editing.
Type CLIP-Similarity ↑Time Cost ↓
One-2-3-45 [18] Inference 0.594 45 s
Point-E [27] Inference 0.587 78 s
Shape-E [13] Inference 0.591 27 s
Zero-1-to-3 [19] Optimization 0.778 30 min
DreamGaussian [41]Optimization 0.738 2 min
Magic123 [30] Optimization 0.802 2 h
DreamCraft3D [40] Optimization 0.842 3.5 h
Vista3D-S Optimization 0.831 5 min
Vista3D-L Optimization 0.868 15 min
Table 1: QuantitativeComparisonsongenerationqualityintermsofCLIP-Similarity
for image-to-3D task. Average generation time is reported.
4.3 Quantitative Comparison
In our evaluation, we employ the CLIP-similarity metric [19,24,30] to assess
the performance of our method in 3D reconstruction using the RealFusion [24]
dataset,whichcomprises15diverseimages.Consistentwiththesettingsusedin
previousstudies,wesample8viewsevenlyacrossanazimuthrangeof[−180,180]
degreesatzeroelevationforeachobject.Thecosinesimilarityisthencalculated
using the CLIP features of these rendered views and the reference view. Table 1
highlights that Vista3D-S attains a CLIP-similarity score of 0.831, with an av-
erage generation time of just 5 minutes, thereby surpassing the performance of
the Magic123 [30]. Furthermore, when compared to another optimization-based
method, DreamGaussian [41], Vista3D-S may take longer at 5 minutes, but it
significantly improves consistency, as evidenced by the higher CLIP-Similarity
score. For Vista3D-L, we apply an enhancement-only setting. By employing an-
gulardiffusionpriorcomposition,ourmethodachievesahigherCLIP-Similarity
of 0.868. The capabilities of Vista3D-L, especially in generating objects with
moredetailedandrealistictexturesthroughpriorcomposition,aredemonstrated
in Figure 4. Additionally, we conduct quantitative experiments on the Google
Scanned Object (GSO) [7] Dataset, following the setting in SyncDreamer [20].
We evaluate each method using 30 objects and computed PSNR, SSIM, and
LPIPS [54] between the rendered views of the 3D object and 16 ground-truth
anchorviews.Theresults,asshowninTab.2,revealthatourVista3D-Lachieves
SOTA performance among these methods with a large margin. Vista3D-S also
demonstrates competitive performance, albeit with a single diffusion prior.Vista3D 13
PSNR ↑ SSIM ↑ LPIPS ↓
RealFusion [24] 15.26 0.722 0.283
Make-it-3D [42] 15.79 0.741 0.245
Zero-1-to-3 [19] 18.93 0.779 0.166
One-2-3-45 [18] 17.47 0.768 0.184
SyncDreamer [20] 20.05 0.798 0.146
DreamGaussian [41] 23.43 0.832 0.092
Magic123 [30] 24.89 0.875 0.084
Vista3D-S 25.42 0.912 0.073
Vista3D-L 26.31 0.929 0.062
Table 2: Quantitative Comparison on the GSO [7] dataset
4.4 User study
In our user study, we evaluate reference view consistency and overall 3D model
quality [41]. The evaluation encompasses four methods: DreamGaussian [41],
Magic123 [30], and our own Vista3D-S and Vista3D-L. We recruited 10 par-
ticipants for this user study. Each was asked to sort generated 3D object from
different methods in terms of view consistency and overall quality respectively.
Thus, the scores presented for each metric range from 1 to 4. The results, pre-
sented in Table 3, reveal that our Vista3D-S outperforms the previous methods
in both view consistency and overall quality. Furthermore, the adoption of the
angular prior composition in Vista3D-L leads to additional improvements in
both the consistency and quality of the generated 3D objects.
DreamGaussian[41] Magic123[30] Vista3D-S Vista3D-L
ViewConsistency↑ 1.78 2.11 2.87 3.24
OverallQuality↑ 2.02 1.83 2.81 3.33
Table3:UserstudyofVista3D.Weconductuserstudyintermsofviewconsistency
and overall quality, the score ranges from 1 to 4, the higher the better.
4.5 Ablation Study
Coarse-to-fine framework. Our framework integrates a coarse stage to learn
initial geometry then a fine stage to refine geometry and shade textures. We
validate the necessity of such a coarse-to-fine pipeline in Figure 5 (a). We first
commence with isosurface representation to learn geometry directly, finding the
geometry optimization is prone to collapse without preliminary geometry ini-
tialization. Thus, a coarse initialization becomes imperative. Beside, we present
the normal map of a rough mesh extracted from 3DGS from the coarse stage.
Itisobservedthatthecoarsestagetendstogenerateroughevennon-watertight
geometry,bothdifficulttomitigate.Thesefindingsdemonstratethatcombining
both stages is crucial for the optimal performance of Vista3D.
Disentangled Texture. For validating the effectiveness of the disentangled
texture, we compare adopting both hash encodings with single hash encoding14 Shen et al.
Ref FlexiCubes 3DGS Ours Singlehash-encoding Bothhash-encoding
a)Ablationstudyofoverallframework b)Ablationstudyofangularhash-encoding
Fig.5: Ablation study of overall framework and disentangled texture.
in Figure 5 (b). With both hash-encodings, the artifacts on the reconstructed
robot are notably reduced, especially at the backside. Further, we visualize the
disentangledtextureinsupplementaryFigure6(b).Specifically,whenvisualizing
H , H is set as 0 in Equation 4, and vice versa. From the shown visual-
ref back
ization, we can clearly find that the facing-forward hash encoding H mainly
ref
encodes the detail features consistent with the given reference view. While the
back hash encoding H mainly encodes the features in the unseen views. The
back
texturesofthefacing-forwardviewandbackviewsaredisentangledandlearned
intwoseparatehashencodings,whichcanfacilitatelearningbettertexturesnear
the reference view and in unseen views.
5 Conclusion
In this paper, we present a coarse-to-fine framework Vista3D to delve into the
3Ddarksideofasingleinputimage.Thisframeworkfacilitatesuser-drivenedit-
ing through text prompts or enhances generation quality using image captions.
The generation process begins with a coarse geometry obtained through Gaus-
sian Splatting, which is subsequently refined using an isosurface representation
complemented by disentangled textures. The design of these 3D representations
enablesthegenerationoftexturedmesheswithinamere5minutes.Additionally,
the angular composition of diffusion priors empowers our framework to reveal
the diversity of unseen views while maintaining 3D consistency. Our approach
surpasses previous methods in terms of realism and detail, striking an optimal
balancebetweengenerationtimeandthequalityofthetexturedmesh.Wehope
our contributions will inspire future advancements and foster future exploration
into the 3D darkside of single images.
Acknowledgement
This project is supported by the Ministry of Education, Singapore, under its
Academic Research Fund Tier 2 (Award Number: MOE-T2EP20122-0006), and
the National Research Foundation, Singapore, under its Medium Sized Center
for Advanced Robotics Technology Innovation.Vista3D 1
Vista3D: Unravel the 3D Darkside of
a Single Image
Supplementary Material
1 More experimental results
1.1 More ablation studies
Front Back Front Back
Front Back Front Back
Coarses Cta og ae rsestaW gd ei et nh so iu fit WcT a dto i eitp noh- snoK iu fit cT atoW ip o- ni Kth ro eu gt uWT lar irta ihn z roa es um t git uoi Tt lnt ara ran in zc ase m tioiW t nrteai gt nh uco leau rt iWzs rc a eia t gtihl o ui on n laug rt izsc aa til oin ng (b) Visua𝐻 li! z" a# t𝐻 io!" n# of the disenta𝐻 n$% g& l’ e𝐻 d$%&t’ex-
(a)Ablationstudyofthecoarsestage.Here ture. Here we showcase a generated 3D object.
weconductfoursettingsonthecoarsestage,in- Theleftsideisvisualizedfromthefacing-forward
cluding w/o Top-K densification, w/o transmit- hashencodingHref,whiletherightsideisvisu-
tanceandscalingregularizationforcomparison. alizedfromthebackhashencodingHback.
Fig.6: Ablation study of the coarse stage and disentangled texture.
Top-k densification. We compare our densification strategy against a naive
gradient threshold approach. This comparison is illustrated in the second col-
umn of Figure 6a. Using a naive gradient threshold often results in excessive
densification of 3D Gaussians, causing geometry to appear swollen. Further-
more, finding an appropriate gradient threshold is challenging, as it varies from
casetocase.Incontrast,ourmethoddeterministicallycontrolsthedensification
ratiothroughouttheoptimizationprocess.Consequently,thetotalnumberof3D
Gaussians at convergence is solely influenced by the hyperparameter of pruning
opacity,effectivelymaintainingthenumberof3DGaussianswithinareasonable
range and yielding more accurate geometry.
Regularization with 3DGS. In the third and fourth columns of Figure 6a,
we conduct ablation experiments on the two regularization terms specified in
Equation 2: transmittance regularization and scale regularization. Removing
the transmittance regularization tends to produce objects with holes, result-
ing in coarse meshes from these 3D Gaussians that are often not watertight,
complicating refinement stage optimization. On the other hand, excluding only
the scale regularization often leads to coarser details in the geometry. This may
be caused by Gaussians with larger scales oversmoothing the local geometries.
The effect of prior composition. To explore the 3D dark side of a single
image, we introduce a gradient constraint-based method in Sec. 3.3 to control
two diffusion priors inthe image-to-3D task.Here we conduct anablation study
to validate the effectiveness of this component. As shown in Fig. 7, without this2 Shen et al.
score composition, though detailed texture on the backside can still be gener-
ated, results in degraded consistency between front views and reference images.
Another setting involves a naive weighting strategy; we follow Magic123 [30]
to set a weighting factor of 1/40 on the SDS term Lρ with diffusion prior
SDS
ϵ . With this setting, the backside of the generated 3D objects appears overly
ρ
smoothed. In contrast, incorporating score composition enables our Vista3D to
robustlygeneratetexturesthatarebothdetailedandconsistentacrossthefront
and back views of 3D objects.
w/oscorecomposition withscorecomposition naiveweighting
Fig.7: Ablation Study of Score Composition. Without score composition, the
consistency between the reference view and front view is degraded. Applying naive
weighting results in over-smoothed textures on back views.
1.2 More qualitative results
Figure8showcasesthequalitativeresultsofVista3D-Lwithdiffusionpriorcom-
positioncomparedtoVista3D-Swithasinglediffusionprior.Particularlyinsce-
narios where the provided reference view is less informative, such as when only
a side or back view of an object is available, Vista3D-L demonstrates a superior
ability to generate more detailed textures compared to Vista3D-S, especially
when specific text prompts are used. For example, in the case of the astronaut,
Vista3D-S tends to produce oversmoothed textures. In contrast, when using
Vista3D-L, the textures generated are notably more vivid and detailed.
2 Camera Pose Sampling
As illustrated in Fig. 9, our approach adopts a 3D-aware camera pose sampling
strategy in the refinement stage, diverging from the standard uniform sampling
used in previous image-to-3D works [30,41,42]. This approach not only speeds
up convergence but also enhances visual quality.
Specifically,foragivenconditionalreferenceimageI ,thepre-trainedZero-
ref
1-to-3 model [19] ϵ is capable of approximating the underlying 3D object dis-
ϕ
tribution P (x). Leveraging this, we employ its estimated empirical error for
Iref
3D-aware sampling.
In this sampling stage, camera poses are sampled from a sphere surface sur-
rounding the central object, divided evenly into N sub-regions R with azimuth
iVista3D 3
Reference Vista3D-S Vista3D-L
Fig.8: Qualitative Comparison between Vista3D-S and Vista3D-L
ranging from [−180,180] degrees, as shown on the left side of Figure 9. Memory
queues of fixed length T are established for each sub-region to store empiri-
cal errors estimated during the SDS optimization, directly derived from SDS as
(ϵ −ϵ) in Equation 1.
ϕ
When performing pose sampling, an empirical Probability Density Function
(PDF) P (R ) is created from these N memory queues. Additionally, given the
3d i
supplementarysupervisionfromthereferenceimageI forforward-facingcam-
ref
era poses, we integrate Gaussian unsampling to reduce sampling frequency on
forward-facing poses and increase it for unseen views. This unsampling employs
a rejection sampling with a truncated Gaussian distribution, depicted on the
right side of Figure 9. Each sub-region is mapped onto this truncated Gaussian
PDF, with regions overlapping significantly with the reference view being more
likely to be sampled.
In this process, a camera pose is sampled by initially performing Gaussian
unsampling to determine a rejection index n ∈ [0,N − 1]. Subsequently, we
modify the empirical PDF by setting P (R ) = 0 and normalizing it. A sub-
3d n
regionindexisthensampledfromthisdiscretePDFP˜ (R ),andacamerapose
3d i
is uniformly sampled from this chosen sub-region.
In our implementation, we configure N = 5, and initially perform uniform
camera pose sampling during the first 100 iterations. For the Gaussian Un-
sampling, we utilize a truncated Gaussian distribution spanning [−1,1], with
N(0,0.5). This distribution is evenly divided into N intervals to facilitate the
sampling process.4 Shen et al.
Sampled
Pose
Default
Azimuth
Fig.9: 3D-aware Pose Sampling, Camera poses are sampled from an empirical
PDF with a truncated Gaussian unsampling.
3 Timestep Sampling in SDS
Pioneering work DreamFusion [29] randomly sample timestep t from U(20,980)
in the SDS optimization. However, Dreamtime [11] critiques this strategy, sug-
gesting that such random sampling is misaligned with the Denoising Diffusion
Probabilistic Models (DDPM) sampling process and leads to inefficient and in-
accurate optimization in SDS. Dreamtime suggests a deterministic Time Priori-
tized(TP)strategywhereeachiterationstepisassignedaunique,decrementally
decreasing timestep t.
However, we observed that this deterministic approach falls short in SDS
optimization. Artifacts generated by large timesteps are not effectively compen-
sated for by smaller timesteps, often exacerbating the problem. To rectify this,
we propose an interval-based annealing method for the timestep. Specifically,
we define a maximum timestep t and a minimum timestep t for each
max min
optimization interval, updating them every 50 optimization steps. The timestep
is then sampled from the dynamically adjusted interval U(t ,t ). This ap-
min max
proach effectively alleviates the artifacts that larger timesteps tend to cause.
4 Limitations
Despite Vista3D demonstrating prowess in exploring the 3D dark side of a sin-
gle image, we acknowledge several limitations for future exploration. Employing
a Score Distillation Sampling (SDS) based architecture, Vista3D necessitates
optimization for each 3D object it generates, positioning its efficiency a notch
below that of purely feed-forward image-to-3D methods. The amount of public
3D data is relatively limited, often resulting in the generation of simplistic 3DVista3D 5
objectsbyfeed-forwardmethodologies.Vista3Dleveragesdiffusionpriorcompo-
sition to facilitate the reconstruction of more diverse 3D objects. This strategy
holds promise for the creation of additional 3D data, potentially alleviating the
current data scarcity and enabling the development of more sophisticated pre-
trained image-to-3D models.6 Shen et al.
References
1. Cao, Z., Hong, F., Wu, T., Pan, L., Liu, Z.: Large-vocabulary 3d diffusion model
with transformer. arXiv preprint arXiv:2309.07920 (2023) 1
2. Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry and
appearance forhigh-qualitytext-to-3d content creation.In: ICCV(October 2023)
4
3. Chen, Z., Wang, F., Liu, H.: Text-to-3d using gaussian splatting. arXiv preprint
arXiv:2309.16585 (2023) 4, 5
4. Chung, J., Lee, S., Nam, H., Lee, J., Lee, K.M.: Luciddreamer: Domain-free gen-
eration of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384 (2023)
4
5. Deitke, M., Liu, R., Wallingford, M., Ngo, H., Michel, O., Kusupati, A., Fan, A.,
Laforte, C., Voleti, V., Gadre, S.Y., et al.: Objaverse-xl: A universe of 10m+ 3d
objects. arXiv preprint arXiv:2307.05663 (2023) 2, 3, 5, 8, 10, 11
6. Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E.,
Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of
annotated 3d objects. In: CVPR. pp. 13142–13153 (2023) 2, 3
7. Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K.,
McHugh, T.B., Vanhoucke, V.: Google scanned objects: A high-quality dataset
of3dscannedhouseholditems.In:2022InternationalConferenceonRoboticsand
Automation (ICRA). pp. 2553–2560. IEEE (2022) 12, 13
8. Duggal, S., Pathak, D.: Topologically-aware deformation fields for single-view 3d
reconstruction. In: CVPR. pp. 1536–1546 (2022) 2, 3
9. Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K.,
Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv
preprint arXiv:2311.04400 (2023) 3
10. Huang, B., Yu, Z., Chen, A., Geiger, A., Gao, S.: 2d gaussian splatting for geo-
metricallyaccurateradiancefields.In:ACMSIGGRAPH2024ConferencePapers.
pp. 1–11 (2024) 4
11. Huang, Y., Wang, J., Shi, Y., Qi, X., Zha, Z.J., Zhang, L.: Dreamtime: An
improved optimization strategy for text-to-3d content creation. arXiv preprint
arXiv:2306.12422 (2023) 9, 4
12. Jain,A.,Mildenhall,B.,Barron,J.T.,Abbeel,P.,Poole,B.:Zero-shottext-guided
object generation with dream fields. In: CVPR. pp. 857–866. IEEE (2022) 4
13. Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. arXiv
preprint arXiv:2305.02463 (2023) 1, 12
14. Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for
real-time radiance field rendering. ACM Transactions on Graphics 42(4) (July
2023), https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/ 2, 4, 5,
6
15. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint
arXiv:2304.02643 (2023) 9
16. Lin,C.H.,Gao,J.,Tang,L.,Takikawa,T.,Zeng,X.,Huang,X.,Kreis,K.,Fidler,
S.,Liu,M.Y.,Lin,T.Y.:Magic3d:High-resolutiontext-to-3dcontentcreation.In:
CVPR. pp. 300–309 (2023) 4, 8
17. Liu, M., Shi, R., Chen, L., Zhang, Z., Xu, C., Chen, H., Zeng, C., Gu, J., Su, H.:
One-2-3-45++:Fastsingleimageto3dobjectswithconsistentmulti-viewgenera-
tion and 3d diffusion (2023) 3Vista3D 7
18. Liu, M., Xu, C., Jin, H., Chen, L., Xu, Z., Su, H., et al.: One-2-3-45: Any single
image to 3d mesh in 45 seconds without per-shape optimization. arXiv preprint
arXiv:2306.16928 (2023) 11, 12, 13
19. Liu,R.,Wu,R.,VanHoorick,B.,Tokmakov,P.,Zakharov,S.,Vondrick,C.:Zero-
1-to-3: Zero-shot one image to 3d object. In: Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision. pp. 9298–9309 (2023) 2, 3, 5, 10, 12,
13
20. Liu,Y.,Lin,C.,Zeng,Z.,Long,X.,Liu,L.,Komura,T.,Wang,W.:Syncdreamer:
Generating multiview-consistent images from a single-view image. arXiv preprint
arXiv:2309.03453 (2023) 3, 12, 13
21. Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H.,
Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-
domain diffusion. arXiv preprint arXiv:2310.15008 (2023) 2, 3, 11
22. Lorensen, W.E., Cline, H.E.: Marching cubes: A high resolution 3d surface con-
structionalgorithm.In:Seminalgraphics:pioneeringeffortsthatshapedthefield,
pp. 347–353 (1998) 7
23. Lu, J., Yang, X., Wang, X.: Unsegment anything by simulating deformation. In:
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
(2024) 9
24. Melas-Kyriazi,L.,Laina,I.,Rupprecht,C.,Vedaldi,A.:Realfusion:360degrecon-
structionofanyobjectfromasingleimage.In:CVPR.pp.8446–8455(2023) 3,8,
12, 13
25. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.:Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis.Commu-
nications of the ACM 65(1), 99–106 (2021) 4, 5
26. Müller,T.,Evans,A.,Schied,C.,Keller,A.:Instantneuralgraphicsprimitiveswith
amultiresolutionhashencoding.ACMTransactionsonGraphics(ToG)41(4),1–
15 (2022) 5
27. Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-e: A system for
generating3dpointcloudsfromcomplexprompts.arXivpreprintarXiv:2212.08751
(2022) 1, 12
28. Nielson, G.M.: Dual marching cubes. In: IEEE visualization 2004. pp. 489–496.
IEEE (2004) 7
29. Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using
2d diffusion. In: ICLR. OpenReview.net (2023) 1, 2, 4, 5, 8
30. Qian,G.,Mai,J.,Hamdi,A.,Ren,J.,Siarohin,A.,Li,B.,Lee,H.Y.,Skorokhodov,
I.,Wonka,P.,Tulyakov,S., etal.:Magic123: Oneimageto high-quality 3dobject
generationusingboth2dand3ddiffusionpriors.arXivpreprintarXiv:2306.17843
(2023) 2, 3, 8, 10, 11, 12, 13
31. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR. pp. 10684–10695 (2022)
10
32. Sargent, K., Li, Z., Shah, T., Herrmann, C., Yu, H.X., Zhang, Y., Chan, E.R.,
Lagun,D.,Fei-Fei,L.,Sun,D.,etal.:Zeronvs:Zero-shot360-degreeviewsynthesis
from a single real image. arXiv preprint arXiv:2310.17994 (2023) 2
33. Seo, J., Jang, W., Kwak, M.S., Ko, J., Kim, H., Kim, J., Kim, J.H., Lee, J., Kim,
S.: Let 2d diffusion model know 3d-consistency for robust text-to-3d generation.
arXiv preprint arXiv:2303.07937 (2023) 3
34. Shen, Q., Yang, X., Wang, X.: Anything-3d: Towards single-view anything recon-
struction in the wild (2023) 1, 2, 3, 98 Shen et al.
35. Shen,Q.,Yang,X.,Wang,X.:Flashsplat:2dto3dgaussiansplattingsegmentation
solved optimally. arXiv preprint arXiv:2409.08270 (2024) 4
36. Shen, Q., Yi, X., Wu, Z., Zhou, P., Zhang, H., Yan, S., Wang, X.: Gamba: Marry
gaussian splatting with mamba for single view 3d reconstruction. arXiv preprint
arXiv:2403.18795 (2024) 3
37. Shen, T., Gao, J., Yin, K., Liu, M.Y., Fidler, S.: Deep marching tetrahedra: a
hybrid representation for high-resolution 3d shape synthesis. Advances in Neural
Information Processing Systems 34, 6087–6101 (2021) 4
38. Shen, T., Munkberg, J., Hasselgren, J., Yin, K., Wang, Z., Chen, W., Gojcic, Z.,
Fidler, S., Sharp, N., Gao, J.: Flexible isosurface extraction for gradient-based
mesh optimization. ACM Trans. Graph. 42(4), 37–1 (2023) 2, 4, 7
39. Shi,R.,Chen,H.,Zhang,Z.,Liu,M.,Xu,C.,Wei,X.,Chen,L.,Zeng,C.,Su,H.:
Zero123++: a single image to consistent multi-view diffusion base model. arXiv
preprint arXiv:2310.15110 (2023) 2, 3
40. Sun, J., Zhang, B., Shao, R., Wang, L., Liu, W., Xie, Z., Liu, Y.: Dreamcraft3d:
Hierarchical 3d generation with bootstrapped diffusion prior. arXiv preprint
arXiv:2310.16818 (2023) 2, 3, 12
41. Tang,J.,Ren,J.,Zhou,H.,Liu,Z.,Zeng,G.:Dreamgaussian:Generativegaussian
splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)
3, 4, 5, 7, 10, 11, 12, 13, 2
42. Tang, J., Wang, T., Zhang, B., Zhang, T., Yi, R., Ma, L., Chen, D.: Make-it-3d:
High-fidelity 3d creation from a single image with diffusion prior. In: ICCV. pp.
22819–22829 (October 2023) 1, 2, 3, 13
43. Wang, H., Du, X., Li, J., Yeh, R.A., Shakhnarovich, G.: Score jacobian chaining:
Lifting pretrained 2d diffusion models for 3d generation. In: CVPR. pp. 12619–
12629 (2023) 4
44. Wang, S., Yang, X., Shen, Q., Jiang, Z., Wang, X.: Gflow: Recovering 4d world
from monocular video. arXiv preprint arXiv:2405.18426 (2024) 4
45. Wu, Z., Zhou, P., Yi, X., Yuan, X., Zhang, H.: Consistent3d: Towards consistent
high-fidelity text-to-3d generation with deterministic sampling prior. In: Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
pp. 9892–9902 (2024) 1
46. Xu,J.,Cheng,W.,Gao,Y.,Wang,X.,Gao,S.,Shan,Y.:Instantmesh:Efficient3d
meshgenerationfromasingleimagewithsparse-viewlargereconstructionmodels.
arXiv preprint arXiv:2404.07191 (2024) 3
47. Xu, Y., Tan, H., Luan, F., Bi, S., Wang, P., Li, J., Shi, Z., Sunkavalli, K., Wet-
zstein, G., Xu, Z., et al.: Dmv3d: Denoising multi-view diffusion using 3d large
reconstruction model. arXiv preprint arXiv:2311.09217 (2023) 3
48. Yang, X., Wang, X.: Hash3d: Training-free acceleration for 3d generation. arXiv
preprint arXiv:2404.06091 (2024) 1
49. Yi, T., Fang, J., Wu, G., Xie, L., Zhang, X., Liu, W., Tian, Q., Wang, X.: Gaus-
siandreamer: Fast generation from text to 3d gaussian splatting with point cloud
priors. arXiv preprint arXiv:2310.08529 (2023) 4, 5
50. Yi,X.,Wu,Z.,Shen,Q.,Xu,Q.,Zhou,P.,Lim,J.H.,Yan,S.,Wang,X.,Zhang,H.:
Mvgamba: Unify 3d content generation as state space sequence modeling. arXiv
preprint arXiv:2406.06367 (2024) 3
51. Yi,X.,Wu,Z.,Xu,Q.,Zhou,P.,Lim,J.H.,Zhang,H.:Diffusiontime-stepcurricu-
lumforoneimageto3dgeneration.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 9948–9958 (2024) 2
52. Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelnerf: Neural radiance fields from
one or few images. In: CVPR. pp. 4578–4587 (2021) 2, 3Vista3D 9
53. Yu, Z., Sattler, T., Geiger, A.: Gaussian opacity fields: Efficient and compact sur-
face reconstruction in unbounded scenes. arXiv preprint arXiv:2404.10772 (2024)
4
54. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: CVPR (2018) 12
55. Zou, Z.X., Yu, Z., Guo, Y.C., Li, Y., Liang, D., Cao, Y.P., Zhang, S.H.: Triplane
meetsgaussiansplatting:Fastandgeneralizablesingle-view3dreconstructionwith
transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 10324–10335 (2024) 3