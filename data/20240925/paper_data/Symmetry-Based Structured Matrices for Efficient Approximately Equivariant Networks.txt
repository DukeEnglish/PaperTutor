Symmetry-Based Structured Matrices for Efficient
Approximately Equivariant Networks
AshwinSamudre∗ MirceaPetrache† BrianD.Nord‡ ShubhenduTrivedi§
Abstract
Therehasbeenmuchrecentinterestindesigningsymmetry-awareneuralnetworks
(NNs) exhibiting relaxed equivariance. Such NNs aim to interpolate between
beingexactlyequivariantandbeingfullyflexible,affordingconsistentperformance
benefits. Inaseparatelineofwork,certainstructuredparametermatrices—those
with displacement structure, characterized by low displacement rank (LDR)—
have been used to design small-footprint NNs. Displacement structure enables
fast function and gradient evaluation, but permits accurate approximations via
compression primarily to classical convolutional neural networks (CNNs). In
this work, we propose a general framework—based on a novel construction of
symmetry-based structured matrices—to build approximately equivariant NNs
with significantly reduced paramter counts. Our framework integrates the two
aforementionedlinesofworkviatheuseofso-calledGroupMatrices(GMs),a
forgottenprecursortothemodernnotionofregularrepresentationsoffinitegroups.
GMsallowthedesignofstructuredmatrices—resemblingLDRmatrices—which
generalizethelinearoperationsofaclassicalCNNfromcyclicgroupstogeneral
finitegroupsandtheirhomogeneousspaces. WeshowthatGMscanbeemployed
toextendalltheelementaryoperationsofCNNstogeneraldiscretegroups.Further,
thetheoryofstructuredmatricesbasedonGMsprovidesageneralizationofLDR
theoryfocussedonmatriceswithcyclicstructure,providingatoolforimplementing
approximateequivariancefordiscretegroups. WetestGM-basedarchitecturesona
varietyoftasksinthepresenceofrelaxedsymmetry. Wereportthatourframework
consistentlyperformscompetitivelycomparedtoapproximatelyequivariantNNs,
andotherstructuredmatrix-basedcompressionframeworks,sometimeswithaone
ortwoordersofmagnitudelowerparametercount.
1 Introduction
Over thepast few years, theincorporation ofsymmetry inneuralnetworksthrough groupequiv-
ariance [1] has started to mature as a fruitful line of research [1–14]. Such networks involve
implementingageneralizedformofconvolutionovergroups—sincelinearequivariantmapsover
generalfieldsarenecessarilyconvolutionalinnature[7,4,3],butcanalsobedesignedusingideas
frominvarianttheory[15],orbyusingnon-linearequivariantmapsviaattentionmechanisms[16–18].
Successfulapplicationshaverangedoveradozenareas,involvingmultipledatatypes[19–37].
∗SchoolofComputingScience,SimonFraserUniversity,Burnaby,Canada.ashwin_samudre@sfu.ca
†UCChile,Fac.deMatemáticas,&Inst.deIngenieríaMatematicayComputacional,Av.VicuñaMackenna
4860,Santiago,6904441,Chile.mpetrache@mat.uc.cl.
‡FermiNationalAcceleratorLaboratory,Batavia,IL60510;DepartmentofAstronomyandAstrophysics,
UniversityofChicago,Chicago,IL60637;KavliInstituteforCosmologicalPhysics,UniversityofChicago
Chicago,IL60637.nord@fnal.gov
§Independent.shubhendu@csail.mit.edu
Preprint.Underreview.
4202
peS
81
]LM.tats[
1v27711.9042:viXraDespitethesuccessesofequivariantnetworks,severalchallengesremain. Forinstance,ithasbeen
demonstratedempirically[38–41]thatastrictequivarianceconstraintcouldharmperformance—
symmetry in real world tasks is rarely perfect and data measurements could be corrupted or sys-
tematicallybiased. Indeed,itisreasonabletoexpectthattherecouldoftenbeamismatchbetween
the symmetry that the model encodes and that the data possesses. A natural modeling paradigm
suggestedby[38–41]prescribesthedesignofmoreflexiblemodelsthatcancalibratethelevelof
equivariancebasedonthedataortask. Thisobservationhasbeensupportedbyagrowingbodyof
recentempirical[42–46]andtheoreticalwork[47,48].
InthispaperwestudytheprincipleddesignofapproximatelyequivariantNNswithsignificantlyre-
ducedparametercountsandtheattendantcomputationalbenefits. Forthispurpose,weexaminealine
ofworkindependentofthatonequivariantNNs,whichisprimarlyconcernedwithdesigningcompact
deeplearningmodelsusingstructuredmatrixrepresentationsoffasttransforms(see[49]andtheref-
erencestherein). Oneclassofpopularmethodsexplicitlyinvolvesthedesignofcomputationally-and
memory-effectivestructuredmatrices,intendedtoreplacedenseweightmatricesinNNs[50–52]. Yet
anotherclassofmethods,proposedintheseminalworksof[53,54],insteadoperatewithtraditional
structuredmatrices: Vandermonde(includingpolynomialandChebyshevVandermonde),Hankel,
Toeplitz, Cauchy, Pick etc. Such matrices are a key object in engineering, especially in control
theory,filteringtheory,andsignalprocessing[55]. Thespecialpropertyofthesematricesisthattheir
compositionsdon’tinherittheirstructureexactly;however,theydosoinanapproximatesense. This
approximationismeasuredbytheclassicalnotionofthedisplacementrankproposedbyKailathand
co-workers[56–58]—suchmatricesaresaidtopossessdisplacementstructure,andalowdegree
oferroroncompositeoperationsisindicatedbyalowdisplacementrank (LDR).While[53,54]
demonstratedthatLDRmatricescouldbeusedtodesignefficientandcompactNNs,theywereonly
concernedwithclassicalMLPsandCNNsanddonotobviouslyextendtoequivariantNNs.
We start with the observation that general cyclic matrices—also known as circulant matrices, a
sub-class of LDR matrices—are used to represent classical circular convolution5. It has already
beenobservedby[54]thatthesereadilyyieldanotionofapproximateconvolution(andapproximate
equivariance)inclassicalCNNs. Itisthisproperty, whichisdependentoncyclicmatricesbeing
LDR, that [54] exploit to build low-resource CNNs. With motivation from the theory of regular
representations,weusethesomewhatforgottennotionofgroupmatrices(GMs)6,andobtainanew
familyofsymmetry-basedstructuredmatricesthatcanbeusedtomodelconvolutionforgeneral
discretegroups.WefirstdevelopageneralizationofclassicalCNNsusingGMsbybuildinganalogues
foreachoftheirelementaryoperations. Then,simplifyingolderworksongroupmatrices[60,61],we
showthatourformalismpermitsaprinciplednotionofapproximateequivarianceforgeneraldiscrete
groups. Infact,somewhatanalogousto[54],wegeneralizeLDRtheoryfromcyclicmatricestotheir
analoguesforgeneraldiscretegroups—thusgivingahandleonquantifyingerrortoexactequivariance.
ToalignourexpositionwiththatofmodernequivariantNNs,weshowhowourframeworknaturally
generalizestothehomogeneousspacesofdiscretegroups,aswellastocompactlysupporteddata
on infinite discrete groups. On a variety of tasks, we show that our proposed method is able to
consistentlymatchoroutperformbaselines,bothfromapproximatelyequivariantNNsandstructured
matrix-basedframeworks,oftenwithanorderofmagnitudeormore,reducedparametercount.
This paper brings together the above mentioned lines of work: equivariant NNs, approximately
equivariantNNs,andstructuredmatrix-basedcompressionapproaches. Theresultisaformalismthat
allowstheconstructionofsymmetry-aware,approximatelyequivariant,andparameter-efficientNNs
withcompetitiveperformance. Wesummarizethemaincontributionsofourpaperbelow:
• Wedevelopaformalismforconstructingequivariantnetworksforgeneraldiscretegroups
using the notion of group matrices (GMs). We show how all the elementary operations
in classical CNNs, such as taking strides and pooling, can be generalized to discrete
groups using group matrices. The resulting networks—GM-CNNs—involve the use of
certainsymmetry-basedstructuredmatriceswhichfacilitatetheconstructionoflight-weight
equivariant NNs. Further, the generalized pooling operation involves a form of group
coarseningandisrelevanttothedevelopmentofgroupequivariantautoencoders.
5Orconvolutionovercyclicgroups.
6Groupmatricesareaprecursortothemodernnotionofregularrepresentations,andpre-datemoderngroup
theory.Forahistory,wedirectthereaderto[59]
2• We present a simple procedure to construct GMs for larger discrete groups which are
composedofdirectproductsorsemi-directproductsofcyclicorpermutationgroups,for
whichGMsareeasytocompute.
• We show that the GM-CNN formalism naturally permits a principled implementation
of approximately equivariant group CNNs. Further, we connect GMs to classical low
displacementrank(LDR)theory, generalizingthetheoryfromthespecificcaseofLDR
matrices with cyclic structure (and thus cyclic groups) to discrete groups. We use the
developedtheorytoquantifyerrorfromexactequivariance.
• Weshowhowourproposedframeworkeasilyextendstohomogeneousspacesofdiscrete
groups,aswellastocompactlysupporteddataoninfinitediscretegroups.
• Acrossavarietyoftasks,wedemonstratethatourframeworkconsistentlyreturnscompetitive
performance,oftenwithasignificantlyreducedparametercount,ascomparedtovarious
approximatelyequivariantnetworks,andstructuredmatrix-basedframeworks. Ourcodeis
availableat: https://github.com/kiryteo/GM-CNN
2 PreliminariesandFormalSetup
2.1 MatriceswithDisplacementStructureandCompressedTransformsinDeepLearning
Theworkof[54,53]considerstraditionalfamiliesofstructuredmatrices—Hankel,Toeplitz,Vander-
monde,Cauchy—tobuildcompressedrepresentationsofMLPsandclassicalCNNs. Inthissection,
we briefly describe the main idea. A matrix M ∈ Rm×n could be called structured if it can be
representedinmuchfewerthanmnparameters. Examplesincludematriceswheretheelementshave
asimpleformulaicrelationshipwithotherelements. Thedisplacementoperatorapproach,pioneered
byKailathetal.[56,57],consistsofrepresentingM viamatricesA∈Rm×mandB ∈Rn×n,which
definealinearmapknownasthedisplacementoperator,asfollows:
Definition 2.1 (Sylvester-type Displacement Operator:). Let A and B be fixed, not necessarily
squarematriceswithcompatibledimensions. TheSylvester-typedisplacementoperator, denoted
as∇ (M),isdefinedasthelinearmap:
A,B
∇ (M):M (cid:55)→AM −MB,
A,B
where the difference AM − MB is the residual R. The rank of the matrix R is called the
displacementrank(DR).Further,therecoveryofM isimmediatefromA,B,R.
Remark. An alternative popular formulation of the displacement operator is the Stein-type dis-
placement operator, which defines ∆ (M) : M (cid:55)→ M −AMB. For most purposes, the two
A,B
formulationscanbetreatedinterchangeably[55].
TheoriginalformulationofDRwasinthecontextofsolutionsofcertainleastsquareestimation
problemsofsystemswritteninstate-spaceform[56,57]. Itwasoriginallystatedformatricesthat
wereToeplitz-like,whichweren’texactlyshift-invariant,onlyapproximatelyso. Theformulation
in[56]showedthataToeplitz-likematrixwouldbeLDR.Suchresultswerelatershownforallthe
classesofstructuredmatriceslistedabove[57,55]. Moreimportantly,manycompositeoperationson
LDRmatrices—includingtranspose/inverse,addition,composition/multiplication,takingdirectsums
toconstructalargerblockmatrix—stillresultedinLDRmatrices(seeproposition1onclosurein
[54]). Itisthispropertythatwasusedby[53,54]toconstructcompressedrepresentationsofNNs. A
particularformoftheToeplitzmatrix,thecirculantmatrix,couldbeseenasimplementingtheusual
circularconvolution(see2.2below). Thus,acirculant-likematrix7wouldimplementanapproximate
convolution(andthusapproximateequivariance),andstackingthemtogetherinaNNwouldpreserve
thepropertyofapproximateequivariance. Thispointwasnotedandprovedinsection4of[54]and
wasthebasisforbuildingapproximatelyshift-equivariantCNNsin[54]. Intheexamplebelow,we
illustratehowworkingwithcirculantmatricescorrespondstoaconvolutiononthegroupC .
n
Example2.2(CirculantMatrices). NotethatforA∈Rn×nequaltothecanonicalcyclicpermutation
P onnelementsandB =P−1,wehave∇ (M)=0iff∆ (M)=0,oriffM iscirculant.
P,P−1 P,P−1
Then,wecaninterprettheentriesofv ∈Rnasencodingafunctionf :C →Roverthecyclicgroup
n
7Acirculant-likematrixiscirculantmatrixtowhichanextremelysparsenoisematrixisadded.Acirculant
matriximplementscircularconvolution,butamatrixwithsomenoisewillimplementapprox.convolution.
3C =Z/nZ. Thus,forcirculantM withfirstrow(m ,...,m )encodingmapϕ:C →R,the
n 11 1n n
multiplicationv (cid:55)→Mvcorrespondstogroup-convolutionoperationf (cid:55)→ϕ⋆f onthegroupC .
n
Theaboveexpositionprovidesatantalizinghintthatbuildingcompressedapproximatelyequivariant
NNsinthespiritof [54,53]couldbeareasonablegoalforgeneraldiscretegroups—butwewould
needtoidentifyspecialstructuredmatricesforeachgroupandensuretheyobeyananalogousLDR
propertyasabove. Inthenextsection,westategroupconvolutionandshowitcouldbewrittenin
termsofspecialmatricescalledgroupmatrices,whichwillprovekeyforustoachieveourgoal.
2.2 GroupMatrices,GroupConvolutions,andCNNs
Inthissection,wepresentourformulationofGM-CNNs. Wefirstdefinegroupmatrices,classical
groupconvolutionandreformulategroupconvolutionintermsofGMs. Followingwhichweshow
GMsnaturallyallowtogeneralizealltheelementaryoperationsofclassicalCNNs,includingstrides
andpooling,toanydiscretegroup. WealsoprovideaproceduretoconstructGMsofcertainlarger
discretegroupsefficiently. Finally,wealsoshowthatGMsenjoyanaturalsetofclosureproperties.
Foreaseofexposition,wedescribetheframeworkinthissectionusingimagesasinputs,butitreadily
generalizestogeneraldatatypesaswillbecomeclearlater. Inparticular,weconsiderimagesofsize
n×n,modulotheperiodicitystructureofeachside,whichisidentifiedwiththeoppositesideofthe
image. SuchimagescouldbeconsideredasfunctionsovercopiesofthegroupG=C ×C ,which
n n
permitsrepresentingconvolutions(asappearinginclassicalCNNs8asagroupconvolutiononG.
Sinceweseekaframeworkthatusessymmetry-basedstructuredmatricestowardsspecificends,we
depart from the customary treatment of group convolutions that uses group representations. We
insteadusetheformalismofgroupmatrices(GMs)ofdiscretegroups;whiletheGMformalismhas
alonghistory, ithasmostlybeenforgottenandhasnotyetbeenusedtorepresentgeneralgroup
convolutions. Wefirstdefinetherequisitenotionsofgroupmatricesandgroupdiagonals:
Groupmatrices. IfGisagroupwithcardinalityN,thenamatrixM ∈RN×N isagroupmatrix
ofGifthereexistsalabelingoftherowsandcolumnsofM byelementsofGsuchthatwhenever
gh = g′h′ (withg,h,g′,h′ ∈ G)wehaveanidentificationofentrieslabeledby(g,h),(g′,h′)i.e.
thatM =M .
gh g′h′
Groupdiagonals. Forg ∈GthegroupdiagonalmatrixB associatedwithgisaparticulargroup
g
matrixwithentriesin{0,1},whoseentriesaredefinedas
(B ) =δ(h=gh′), (1)
g h,h′
where,δ(h=gh′)equals1ifh=gh′,and0otherwise(similartotheKroneckerdeltanotation). We
observethatgroupdiagonalsformabasisofthespaceofgroupmatriceswithentriesinX=R.
B areextremelysparsematriceswithexactlyonenon-zeroentryperrow.Thus,wecanstorethemas
g
arraysofsize|G|,listingonlythecolumnindicesoftheseentries,ratherthanas|G|×|G|matrices.
Groupconvolution. Forstandardgroupconvolution,ifGisafinitegroupandϕ,ψ :G→R,then
convolutionofthesefunctionsis
(cid:88) (cid:88)
ϕ⋆ψ(x):= ϕ(g)ψ(g−1x)= ϕ(g)ψ(h). (2)
g∈G (g,h)∈G×G:gh=x
In the second expression, we write the set of pairs (g,g−1x),g ∈ G implicitly as pairs (g,h) ∈
G×Gsatisfyingtheconditionhg =x. ObservethatGMspermitasimpleformulationforgroup
convolutionsforfinitegroupsbecausetheyencodethegroupoperationviaalinearoperatorh.
WenextuseGMformulationstoproducegeneralversionsofcommongroupconvolutionalnetwork
choicesandoperations: convolutions,smallkernels,strides,andpooling.
Groupconvolutionusinggroupmatrices. Wecanre-expressgroupconvolutioninthelanguageof
groupmatricesandgroupdiagonalsasfollows. Considerthefunctionsϕ,ψ :G→Rasvectorsin
→− →− →−
ϕ, ψ ∈RG,wherethevectorentriesareindexedbyG–e.g. ϕ :=ϕ(g). Inthiscase,thelinear
g
8Tobefullyprecise,infact,classicalCNNsuseconvolutionwithinfinitegroupZ×Z,whichgivessome
extratechnicaldifficultiesasweneedtokeeptrackofimagefinitesupports.ThisissolvedinclassicalCNNsby
padding.Later,in§C.2weshowhowtoextendourframeworktogeneraldiscreteinfinitegroups.
4→− −−−→
transformation ψ (cid:55)→ ϕ⋆ψ isexpressedbyasimplematrix. Moreover,itissufficienttocombine
groupdiagonalsB withcoefficientsencodedintheentriesofψ todefineconvolutionas:
g g
→− −−−→ (cid:88) →−
Conv ϕ :=ϕ⋆ψ = ϕ(g)B ψ. (3)
ψ g
g∈G
Theverificationof(3)isstraightforward: usingdefinitions(2)and(1),wehave,forallx∈G,
(cid:34) (cid:35)
(cid:104)−−−→(cid:105) (cid:88) (cid:88) (cid:88) (cid:104) →−(cid:105) (cid:88) →−
ϕ⋆ψ := ϕ(g)ψ(h)= ϕ(g)ψ(h)δ(gh=x)= ϕ(g) B ψ = ϕ(g)B ψ .
g g
x x
g,h:gh=x g,h g g x
Choosingsmallkernels. InclassicalCNNs,kernelshavesupportinak×k-neighborhoodofthe
originforsmallvaluesofk: kernelsarenonzeroatpixelsneartheorigin. Theanalogueofthischoice
→−
forgeneralfinitegroupsGistofixaworddistance9andtorestrictkernelcoefficients ϕ tobezero
onelements of G ata distancelargerthan k fromthe origin. Consequently, if N isthe number
k
ofelementsintheradius-kneighborhoodoftheidentityinG,thenthesumfrom(3)onlyincludes
N non-zerosummands,whichistypicallyN ordersofmagnitudesmallerthanthecardinality|G|.
k k
NotethatweareneverrequiredtocomputetheB matricesexceptfortheseN choicesofg. This
g k
permitsanefficientimplementationanalogoustoclassicalCNNs.
Poolingoperations:Groupdiagonalscanberestrictedtosubgroups.Poolingandstrideoperations
mapinputchannels,writtenasfunctionsψ :G→R,tooutputsoftheformψ′ :H →R,indexedby
asubgroupH ⊆G. ConvolutionrestrictsnaturallytoH,apropertythatcanbeformulatedinterms
→−
ofgroupdiagonals. ThegroupdiagonalsBH (|H|×|H|matriceswithgroupH)actingover ψ′can
h
beobtainedfromtheBG(|G|×|G|matriceswithgroupGcorrespondingtoelementsh∈H ⊆G)
h
byremovingrowsandcolumnsindexedbyelementsofG\H. Inparticular,groupdiagonalsare
mappedtogroupdiagonalsunderthisoperation,asaconsequenceofthefollowinglemma:
Lemma2.3. Ifh∈H,thentherowsofG-groupdiagonalmatrixB labeledbyelementsofH,have
h
entries1onlyincolumnswhoseindexlabelsbelongtoH.
Allproofsarerelegatedtotheappendix.
Pooling operations on finite groups. As mentioned above, we define the pooling operation via
asubgroupH ⊂ G,wherepoolingisdoneoversubsetsofsizen = |G|/|H|. Wenowexplicitly
describetheoperation. ConsiderrightcosetsHg ,...,Hg . ForeachcosetHg ,weselectacoset
1 n i
representativeattheclosestworddistancefromtheorigin: g¯ ∈argmin dist(hg ,e). Notethat
i h∈H i
forh ∈ H,thesetsP := {hg¯ : 1 ≤ i ≤ n}againformapartitionofG. Then,thepoolingcan
h i
bedefinedasacoarseningoperation,basedonafunction□∈{max,aver,...}thatfixesawayof
associatingtoafinitesetofelementsofR. Typicalchoicesare□(X) = maxX formax-pooling
andaver(X):= 1 (cid:80) xforaverage-pooling. Wecanthendefine(H,□)-poolingas
|X| x∈X
ψ :G→R (cid:55)→ Pool□ (ψ):H →R, Pool□ (ψ)(h):=□{ψ(g′): g′ ∈P }. (4)
H H h
InclassicalCNNs,forfixedpoolingparameterk,theoutputvalueatpixelposition(i,j)issettobe
themaximum(oraverage)ofpixelvaluesatdistanceat≤kfrompixel(ki,kj)oftheinput.
Example2.4. IfthegroupisG=C ×C withgeneratorsdenotedas(1,0),(0,1),andassuming
N N
thatwecanfactorizeN =mnwithm,n≥2,thenwecanfocusonthesubgroupH ⊆Gisomorphic
toC ×C ,generatedbyelements(m,0),(0,m)∈G. Inthiscase,thecosetsofH havetheform
n n
H(i,i′)={(km+i,k′m+i′): 0≤k <n}, for 0≤i,i′ <m.
Wecanthendirectlytakevalues(i,i′)asaboveastheclosest-to-identityrepresentativesg . With
α
thisnotion,the(H,aver)-poolingoperationtakesasinputasignaloftheform
ψ :{0,...,N −1}2 →R, withentriesdenoted ψ(a,a′), 0≤a,a′ <N,
andmapsittoasignalgivenby
1 (cid:88)
ψ′ :{0,...,n−1}2 →R, ψ′(k,k′):=[Poolaverψ](k,k′):= ψ(k+i,k′+i′).
H m2
0≤i,i′<m
9ForafinitegroupGw.r.tthegeneratingsetS,assigneachedgeoftheCayleygraphametricoflength1.
Thenthedistancebetweeng,h∈GequalstheshortestpathlengthintheCayleygraphfromvertexgtovertexh
5□
Remark. Inpractice,successivelayerstobeappliedtotheoutputofaPool -layershouldusegroup
H
matricesassociatedwithgroupH. Suchgroupmatricescanbeconstructedasfollows: takegroup
diagonalsB ofC ×C ,withgoftheformg =(km,k′m)only,ofwhichweremoverowsand
g N N
columnswhoseindexisnotamultipleofm.
Implementingstrideviasubgroups. Addingstrideafterconvolutionaloperationsallowstheuser
tomanipulatesignalsizesacrossthearchitecture. IftheinputismodeledbyafinitegroupG,we
considerasubgroupH,andapplytheconvolutionkernelonlyatpositionsindexedbyelementsofH:
→− (cid:88) →−
ConvH :RG →RH, ConvHψ := ψ(g)BHψ, (5)
ϕ ϕ g
g∈G
inwhichBH isthe|H|×|G|matrixobtainedbyremovingrowsofB thatarelabeledbyG\H.
g g
Obtaininggroupmatricesofcomplexgroupsfromsimplerones. Inmanyapplications,groups
are products or semi-direct products of cyclic or permutation groups. For any finitely generated
group,onecanproducethegroupmatrixusingthegroupoperationandanalgorithmforbringing
groupelementstocanonicalform. However,thiscanbetime-consuming: onewouldideallyusethe
groupmatricesofindividualcomponentsandproduceagroupmatrixofthelargergroupdirectly.
Toachievethis,weusethefollowingprocedure. LetG,H betwogroups,andconsidertheproduct
groupG×H. Then,ifg ∈G,h∈H,andBG,BH arethecorrespondinggroupdiagonals,agroup
g h
diagonalinG×H forelement(g,h)isgivenbytakingtheKroneckerproduct:
BG×H =BG⊗BH. (6)
(g,h) g h
Consider a semi-direct product G⋊ H. As usual, it requires defining a group homomorphism
ϕ
ϕ:H →Aut(G),andthengroupoperationisdefinedby(g,h)·(g′,h′):=(gϕ (g′),hh′). Inthis
h
case,g (cid:55)→ϕ (g)inducesapermutationofG,andthuswecanassociatetoita|G|×|G|permutation
h
matrixP . Thenitisdirecttocheckthat
h
BG⋊ ϕH =(P BG)⊗BH. (7)
(g,h) h g h
We have thus fully described generalizations of the classical CNN operations to general discrete
groupsviagroupmatrices. Wealsoshowedthatdirectorsemi-directproductgroupsofcyclicor
permutationgroupspermitefficientcomputationoftheirgroupmatrices.
Closureofgroupmatricesunderelementaryoperations.Asreviewedin§2.1,closurepropertiesof
LDRmatrices[54]werekeytotheiruseinNNcompression[54]. Beforeweintroduceerrorcontrol
andapproximateequivariance,wenotethefollowingsimpleclosureproperties,provedin§A.2.
Proposition2.5. IfM,M′aregroupmatricesforgroupG,thenMT,M−1,MM′arealsogroup
matricesforgroupG. IfN isagroupmatrixforgroupH,thentheKroneckerproductM ⊗N isa
groupmatrixforthedirectproductgroupG×H.
3 ImplementingApproximatelyEquivariantGM-CNNs
Anumberofresearchershaverecentlyargued,bothempirically[38–41]andtheoretically[47,48],
thatimposingahardequivarianceconstraintinNNscanbedetrimentaltoaccuracy;practitioners
canbenefitbyrelaxingequivariancerequirementsatminimalcomputationalcost. OurGM-based
convolutionalformalismcanbeconsideredasonlyusingasuperpositionofconstant-valuediagonals
ϕ(g)B asdictatedbytheformula(3).Wenowproposeageneralizationthatallowslearnableweights
g
beyondSection2.2,thuspermittingaprincipledimplementationofapproximatelyequivariantNNs.
Generalmatricesingroup-diagonalbasis. Wefirstshowhowgeneralmatricescanbewrittenin
agroup-diagonalbasis. Considerageneral|G|×|G|-matrixM,andletB ,g ∈ Gbethegroup
g
diagonals(1). ThenwecanalwaysencodetheentriesfromM via|G|-dimensionalarraysF ,g ∈G:
g
(cid:88)
M = diag(F )B , whereforh∈G (F ) :=M . (8)
g g g h h,hg−1
g∈G
6The validity of expression (8) follows directly from definition (1): the (h,h′)th entry of B is
g
δ(h=gh′),andthusisnonzero(infact=1)onlyifg =h(h′)−1. Usingthedefinition(8)ofF ,
g
 
(cid:88) (cid:88)
 diag(F g)B g = (F g) hδ(h=gh′)=(F h(h′)−1) h =M h,h(h(h′)−1)−1 =M h,h′,
g∈G g∈G
h,h′
andthusalltheentriesofthetwosidesof(8)coincide.
WecanobtainthecoordinatesF(M)associatedwithanylinearoperationψ (cid:55)→Mψ,byshuffling
theentriesofM andreducingthematrixF(M),withrowsandcolumnslabeledbyG,andwhose
rowsarethecoefficientsF ,g ∈G. WecaninterpretthecolumnsofF(M)asrelativecoefficients
g
multipliedatpositiongofthegroup,androwentriesasparameterizingtherelativeposition:
(cid:88)
ψ (g¯)= ψ (g¯g−1)[F(M)] .
out in g¯,g
g∈G
Withtheabove, wegetaformulationofamoregeneralconvolution(learnedviaM), permitting
approximateequivariance,butexpressedintermsofgroupmatrices.Wenowconnectthisformulation
tothetheoryofdisplacementstructuresandshowthatitcorrespondstoaLDRimplementation.
Displacementoperatorforgeneralgroups. Letbrepresentthevectorsuchthatitsithentryisthe
constantvalueassociatedwithrowi. SupposeifthematrixF(M)hastheformF(M) = 1⊗b,
thenitisequivalenttoM beingagroupmatrix,andtheconvolutiondescribedabovereducestoan
exactconvolutionasin(3). ThispropertyofF(M)canbetestedbytakingthefollowingdifference:
D(M)=D (M):=F(M)−PF(M), whereP(x ,...,x )=(x ,...,x ,x ) (9)
P 1 N 2 N 1
ThenM isagroupmatrix(andthusencodesagroupconvolution)ifandonlyifD(M)=0.
In fact, the same property D(M) = 0 holds even if in defining D we apply a different cyclic
permutation for each row. More explicitly, for each g ∈ G we select a cyclic permutation σ ∈
g
Perm(G),andthensetP⃗ :=(σ ) anddefineentrywise
g g∈G
(cid:2) (cid:3)
D (M) :=[F(M)] −[F(M)] . (10)
P⃗ g,g′ g,g′ g,σg(g′)
Displacementdimensionandrank. Whenweincreaseexpressivitybyallowingacontrollederror
toequivariance,anaturalmetricforthiscontrolisviathedimensionofthespaceofallowedmatrices
D(M)withinamodel. WedefinethedisplacementdimensionofasubsetM⊆R|G|×|G|as:
dim D(M):=dimR(Span({D(M): M ∈M})). (11)
Note that dim (M) does not depend on the choice of P⃗ in (10), as the dimension considered in
D
(11) can be computed by summing the dimensions of spans row by row, and that row spans for
D (M),M ∈Mdonotdependonthechoiceofpermutationsσ .
P⃗ g
Another metric for measuring the discrepancy of a matrix M from being a group matrix, is the
displacementrankofM,definedas
DR(M):=rank(D(M)). (12)
For further discussion about this notion of displacement rank and connections to the classical
generalizationofdisplacementrankasintroducedin[60,61],seeAppendixB.
Lowdisplacementrankimplementations. Inordertointroduceerrorstoequivariance, weuse
kernels,encodedasmatricesM whosedisplacementmatricesD(M)havelowrank. Asimplechoice
istoaddamatrixwithrlearnablevectorcolumnsa ,1≤i≤rtotheagroupmatrixM:
gi
d
(cid:88)
F(M)=1⊗b+ a ⊗1. (13)
gi
i=1
Thenitisdirecttoverifythatrank(D(M))=dim(Span(a ,...,a ))≤r,andthatifMisthe
g1 gr
spaceofmatricesoftheform(13)thendim (M)=|G|r.
D
7Quantifyingequivarianceerrorunderelementaryoperations. Asacounterparttotheclosure
propertiesofProp. 2.5forgroupmatrices,itisinterestingtoquantifycontrolonhowaboundonthe
errortoequivariance(orto"beingagroupmatrix")behavesunderthesameoperations.
Afirstapproachistousedisplacement-basedstructuralmetricssuchasDR(M)ordim (M)and
D
askiftheybehavethesamewayoncompositeoperationsbetweenmatricesofthesameclass. Sucha
controlforthecaseofclassicalLDRisavailablein[54,Prop. 1]. Howeverwefoundthatthenatural
displacementoperatorD,ingeneral,hasaverycomplicatedbehaviorundermatrixproducts,andwe
werenotabletoextendthenaturalboundsfordeteriorationundermultiplicationfrom[54].
Ontheotherhand,ratherthandemandingstructural/algebraiccontrol,aquantitativecontrolover
theerrortoequivariancemaybemoreusefulinpractice,forequivarianceerrorbounds. Anatural
quantificationistomeasurethedistanceofamatrixfromthesetofG-groupmatrices:
dist(M,GM):=min{∥M −M ∥: M ∈GM}, (14)
0 0
inwhich∥·∥isFrobeniusnormformatrices. Wethenhavethefollowingproperties,provedin§A.3:
Proposition 3.1. Let M,M′ be |G|×|G|-matrices andlet GM = GMG be theset of G-group
matrices. ForasecondgroupH,letN bean|H|×|H|-matrixandGMH,GHG×H bethesetof
groupmatriceswithgroupH,G×H respectively. Then
1. dist(M,GM)=dist(MT,GM).
2. dist(MN,GM)≤max{∥M∥,∥N∥}(dist(M,GM)+dist(M′,GM)).
3. dist(M ⊗N,GMG×H)≤max{∥M∥,∥N∥}(dist(M,GMG)+dist(N,GMH)).
Finally,asdiscussedinSection1,ourframeworkgeneralizestonotjustdiscretegroups,butalso
totheirhomogeneousspaces. Duetospacelimitations,wedescribetheextensiontohomogeneous
spacesinappendixC.Wealsoshowthattheframeworkextendstofinitelysupporteddataoninfinite
discretegroupsinC.2. TheerrortoperfectequivarianceforthisgeneralsetupisquantifiedinC.3.
4 ExperimentalResults
GMConvandGMPooloperations: OurarchitectureisbuiltaroundGMConvandGMPooloper-
ations, whicharedesignedtohandlegroup-basedinteractionsandsymmetry-preservingpooling,
respectively. TheGMConvoperationusesgroupmatricestodefineconvolutionalkernels,wherethe
neighborhoodparametercontrolstheextentoflocalinteractionsbasedontheworddistanceidea
describedinsection2.2toimplicitlysetthekernelsize. Forcyclicgroups,aneighborhoodradiusof1
resultsin3groupentries[x−1,x,x+1]inacyclicgroup,whilefordirectproductoftwosuchgroups,
itresultsin9learnableparameters. Generally,aGMConvlayerwithaneighborhoodsizekcomprises
(2k +1)×(2k +1)×numberofchannels learnable parameters. In error addition experiments,
weintroduceanadditionalerrormatrixwithsamestructureastheinitialgroupmatrix, doubling
parameters from p to 2p. This formulation allows learning approximate equivariance. GMPool
(Section 2.2)performspoolingbyleveragingpredefinedgroupstructures,whichareconstructedby
generatinggroupelements,formingtheirKroneckerproducts,andcalculatingsubgroupcosets. These
cosetsandtheircorrespondingtensorindicesareprecomputedtostreamlinethepoolingprocess.
Duringtheforwardpass,GMPoolefficientlyutilizestheseprecomputedindicestoselectelements
formaxormeanpooling,ensuringtheoperationremainsconsistentwiththegroupsymmetry. Before
presentingourresultsondifferentdatasets,wewouldliketonotethatfollowing[41],wealsoperform
anequivarianceerroranalysis,whichcanbefoundinappendixD.
4.1 DynamicsPrediction
We evaluate our framework on two dynamics prediction tasks: the smoke plumes task (Plumes)
andtheJetFlowtask(MJetFlow)asdescribedin[41]. ForPlumes, eachmodeltakessequences
of64×64cropsofasmokesimulationgeneratedbyPhiFlow[62]asinputtopredictthevelocity
fieldforthenexttimestep. Theevaluationisconductedundertwosettings: “Future", wherewe
evaluateonthesameportionofthesimulation,butpredictfuturetimestepswhicharenotincludedin
training. Thesecondsettingis“Domain"wheretheevaluationisdoneonthesametimestepsbutat
differentspatiallocations. Thedataarecollectedfromsimulationswithdifferentinflowpositionsand
buoyantforces. TheJetFlowtaskcomprises24subregionsofjetflowssized62×23,asdescribed
8in[41]followingasimilarevaluationprotocol. Ourmethodisnotinherentlysteerable;butwestill
compareitagainstvarioussteerablebaselinesdespiteputtingitatadisadvantage. Thebaselinesalso
includeanordinaryMLPandaCNN.OthersincludeanE2-CNN(Equiv)[63],twoapproximately
equivariantnetworkse.g. RPP[64]andLIFT[65].
Experimental setup: Our architecture consists of four GMConv layers with 128 channels each,
utilizingPReLUactivationsandresidualconnections. Wemaintainspatialresolutionthroughoutthe
networkbyavoidingpoolingoperations. Afinal1×1convolutionmapsthefeaturestothedesired
outputchannels. Theneighborhoodparametersaresetto4forcyclicgroupsand2fordihedralgroups
tooptimizelocalinteractions.WeinitializeweightsusingKaiminginitialization.Themodelistrained
usingtheAdamWoptimizerwithlearningratesbetween0.003and0.009,aweightdecayof0.0153,
andaReduceLROnPlateauscheduler(factor0.7,patience8)onameansquarederrorloss. Training
isconductedwithabatchsizeof256forupto100epochs,employingearlystopping(patience6)
basedonvalidationaccuracytopreventoverfitting. WeutilizePyTorchforourexperimentsandthe
hyperparametersarefine-tunedforeachdataset.
Run times: Using an Nvidia L40SGPU (48GB RAM), forward pass times are approximately1
secondforJetFlow(62×23samples)and1.4secondsforPlumes(64×64samples),reflectingthe
complexityofthesetasks. FLOPSaredetailedinTables1and2.
Results on JetFlow: Table 1 shows GM-CNN achieves top performance (tied with RGroup) on
thetranslationtask,usingthefewestparameters(26,325). StandardConvandLiftuse51,548and
1,994,818 parameters respectively. For the rotation task, GM-CNN performs slightly better than
ECNN while using only about 10% of the parameters (29,583 vs 304,128). It also outperforms
LiftandRSteer. Finally, forthescalingtask, GM-CNN’sperformanceiscomparabletothebest
performers(RppandRSteer)whileusingonlyafractionoftheirparameters(26,325comparedto
1,421,832forRppand6,742,530forRSteer). Theseresultshighlightthecompetitiveperformanceof
ourmethoddespitenotbeinginherentlysteerable.
ResultsonPlume: AsseeninTable2,GM-CNNconsistentlyusesthefewestparameters(19,267)
acrossalltaskswhileachievingcompetitiveresults. Forthetranslationtask,GM-CNN’sperformance
isclosetothebest-performingRGroup, despiteusing25timesfewerparameters. InRotation, it
competes well with top performing methods like Lift and RSteer while maintaining its dramatic
parameterefficiencyadvantage. Lastly,forscaling,GM-CNNiscompetitive,withRSteerachieving
onlyslightlybetterresultsatthecostof350timesmoreparameters.
Table1: RMSEonJetFlowdataset. Seetextfordetails[41]. FLOPSare×1010.
Model Translation Rotation Scaling
Conv Lift RGroupGM-CNN E2CNN Lift RSteerGM-CNN Rpp RSteerGM-CNN
Future 0.22 0.17 0.15 0.15 0.21 0.18 0.17 0.16 0.16 0.14 0.15
±0.06 ±0.02 ±0.00 ±0.01 ±0.02 ±0.02 ±0.01 ±0.01 ±0.06 ±0.01 ±0.01
Domain 0.23 0.18 0.16 0.17 0.27 0.21 0.16 0.18 0.16 0.15 0.17
±0.06 ±0.04 ±0.01 ±0.01 ±0.03 ±0.04 ±0.01 ±0.02 ±0.07 ±0.00 ±0.01
Params 515481994818 53798 26325 1071361915872 961538 29583 14218326742530 26325
FLOPS 0.007 0.023 0.007 0.006 0.185 0.015 0.922 0.008 0.141 0.024 0.006
4.2 Comparisonwithstructuredmatrixbaselines[53,54]
WeevaluateGM-CNNsonavarietyofimagedatasetsandcomparethemagainstasetofcompetitive
baselines—themethodsreportedin[54]stillremainamongstthemostcompetitive. Wecompareour
methodsontwodimensions: accuracyofthemodelsandthetotalnumberofparameters. Inaddition
todatasetsfrom[54],wefurtherconsidertheSmallNORB[66],RotatedMNIST,andRectangles
datasets [67]. SmallNORB is a condensed version of the NORB dataset [66], and is specifically
tailored for object recognition tasks focusing on shape. The Rotated MNIST dataset comprises
MNIST data samples that have been randomly rotated, offering a variation that lacks the noise
typicallyintroducedintheMNIST-bg-rot[67]dataset. TheRectanglesdataset,acompactbinary
classificationdataset,distinguishesrectanglesbasedonwhethertheirwidthorheightisgreater. For
theseimageclassificationtasks,theinputdimensionsrangefrom24×24to32×32. Whiletheseare
9Table2: RMSEonthePlumesdataset. Seetextfordetails[41]. FLOPSare×1010.
Model MLP Conv Equiv Rpp Combo Lift RGroup RSteer GM-CNN
Translation F 1.56±0.08 —– 0.94±0.02 0.92±0.01 1.02±0.02 0.87±0.03 0.71±0.01 —– 0.81±0.02
D 1.79±0.13 —– 0.68±0.05 0.93±0.01 0.98±0.01 0.70±0.00 0.62±0.02 —– 0.74±0.01
Params 8678240 —– 1821186 7154902 3683332 8235362 1921158 —– 19267
FLOPS 0.001 —– 0.745 0.002 1.492 0.145 0.756 —– 0.003
Rotation F 1.38±0.06 1.21±0.01 1.05±0.06 0.96±0.10 1.07±0.00 0.82±0.08 0.82±0.01 0.80±0.00 0.93±0.02
D 1.34±0.03 1.10±0.05 0.76±0.02 0.83±0.01 0.82±0.02 0.68±0.09 0.73±0.02 0.67±0.01 0.79±0.02
Params 8678240 1821186 1198080 5628298 431808 1801748 1883536 7232258 19267
FLOPS 0.001 0.745 2.965 3.966 0.585 2.955 3.087 1.986 0.003
Scaling F 2.40±0.02 0.83±0.01 0.75±0.03 0.81±0.09 0.78±0.04 0.85±0.01 0.76±0.04 0.70±0.01 0.79±0.02
D 1.81±0.18 0.95±0.02 0.87±0.02 0.86±0.05 0.85±0.01 0.77±0.02 0.86±0.12 0.73±0.01 0.82±0.01
Params 8678240 1821186 1744774 3966984 1059270 2833558 1275266 2427394 19267
FLOPS 0.001 0.745 0.368 0.507 0.326 0.016 0.132 0.041 0.003
smalldatasets,theycaptureawiderangeofvariation,butmoreimportantly,theypermitcomparison
withexistingstructuredmatrixbaselines.
Experimentalsetup: OurbasearchitectureconsistsoftwoGMConvlayers(120channelseach)
withresidualconnections,LayerNormwithoutlearnableparameters,andPReLUactivations. After
theGMConvlayers,anadaptivemaxpoolinglayerisapplied,followeddirectlybyasinglefully
connectedlayerthatperformsclassificationwiththecross-entropylossfunction. Weexperimentwith
GMConvlayerswithaneighborhoodof3,incorporatingerroraddition,andwithGMPool. Wealso
includeexperimentsfordifferentneighborhoodsizes. Trainingisconductedwithabatchsizeof1024
foramaximumof100epochs,withearlystoppingtopreventoverfitting. Otherhyperparametersare
consistentwiththoseusedinourdynamicspredictionexperiments. Forpooling-basedexperiments,
weemployaGM-CNNarchitecturewith3GMConvlayers(44,44,and56outputchannels).
Runtimes: Forwardpasstimesvarybyinputsize: ∼0.6sfor24x24images(smallNORB),∼0.7s
for 28x28 (MNIST variants), and ∼ 0.8s for 32x32 (CIFAR-10, NORB). These are on par for
allthecompetingmethods. NotethatourmethodcanbespedupsignificantlywithcustomGPU
implementationsforhandlingstructuredmatrices.
Results. TheresultsarepresentedinTable3. GM-CNNvariantsdemonstratestrongperformance
acrossalldatasets. Ourapproach,whichcombinespoolinganderroradditionwithaneighborhood
parametersetto1,achievesthebestresultsontheMNIST-bg-rot,MNIST-noise,andRectangles
datasetswithminimalparameters. OnCIFAR-10,allGMvariantsoutperformothermethods. For
NORBandSmallNORB,GM-CNNwithaneighborhoodsizeof3anderroradditionshowssuperior
performance. ForRotatedMNIST,thepoolinganderroradditionapproachwithaneighborhoodsize
of3leadsinperformance. Theseperformancesareachievedwithsignificantlyfewerparameters.
NotethatthenumbersforCIFAR-10aremuchlowerthanstate-of-the-artresults,whichweattribute
tothesignificantlylowerparametercounts.
5 Conclusion
In this paper, we presented the development of a novel formalism (GM-CNNs) for constructing
equivariantnetworksforgeneraldiscretegroupsusinggroupmatrices,generalizingalltheelementary
operations of classical CNNs. GM-CNNs employ the use of a novel family of symmetry-based
structuredmatrices,alsodevelopedviaourformalism,whichfacilitatestheconstructionoflightweight
equivariantNNs. Further,wepresentedaprincipledimplementationofapproximatelyequivariant
groupCNNsusingourformalism. Connectinggroupmatricestoclassicaldisplacementstructure
theory,weprovideageneralizationofthetheoryforLDRmatriceswithcyclicstructuretodiscrete
groups. Movingbeyonddiscretegroups,weprovideanextensionofGM-CNNstohomogeneous
spaces and infinite discrete groups. Finally, we tested our proposed formalism on a variety of
differenttasks,andshowthatGM-CNNscanbeconsistentlycompetitive,whilebeingsignificantly
moreparameterefficient,comparedtoapproximatelyequivariantNNsandstructuredmatrix-based
frameworks. Forfuturework,itwouldbeinterestingtoextendourformulationforcontinuousgroups,
andenablingoursetuptobesteerable. Further,exploringthegrouptensorizationoperationsproposed
in[68](e.g. theorem1.1),couldhelpimprovethescalabilityofourmethod.
10Method↑ M-bg-rot M-noise CIFAR-10 NORB SmallNORB Rect Rot-MNIST
LDR-TD(r=1) 45.81 78.45 45.33 62.75 83.23 98.53 79.82
14122 14122 18442 14342 14122 14122 14122
GM(n=3) 30.07 80.14 58.31 54.63 79.01 99.60 83.78
12483 13441 13681 12967 12725 12483 48.7k
GM(n=3,E) 49.07 82.55 58.29 70.59 85.22 99.31 83.26
24734 25213 25213 24734 24546 24244 24734
GM(n=1,P+E) 55.29 90.20 58.07 67.84 80.90 99.89 78.77
5915 5915 6003 5687 5573 5915 5915
GM(n=2,P+E) 53.94 88.84 55.10 67.72 78.61 99.86 79.06
8701 8701 8819 8503 8359 8701 8731
GM(n=3,P+E) 53.88 84.92 57.14 66.99 83.02 99.67 84.50
14747 14747 14482 14519 14482 14747 14747
Low-rank[67](r=4) 35.67 52.25 32.28 43.66 78.05 87.48 54.87
14122 14122 18442 14342 6916 7842 14122
Fastfood[68] 38.13 63.55 39.64 59.02 73.38 89.81 58.14
10202 10202 13322 9222 5380 10202 10202
Circulant[69] 34.46 65.35 34.28 46.45 71.23 88.92 52.22
8634 8634 11274 7174 3456 8634 8634
Table3: TestaccuracyandparametercountforGM-CNNs. Inthe"Method"column,ndenotesthe
neighbourhoodsize,PsuggeststheuseofpoolingandEsuggestserroraddition. Thebestaccuracy
isinblue,andthesecondbestinred. IfGM-CNNachievesbestorsecond-bestaccuracywithfewer
parameters,thenwemarkthenumberofparametersinbold. Clearly,weseethatGM-CNNmethods
consistentlyprovidethemostaccurateandusuallythelowestparametercount. Incaseswherethe
best GM-CNN model has the best accuracy, but not the lowest parameter count, the second best
GM-CNNmodelhascomparableaccuracy,butsignificantlylowerparametercount.
AcknowledgmentsandDisclosureofFunding
ThisworkissupportedbytheDeepSkiesCommunity(deepskieslab.com),whichhelpedinbringing
togethertheauthors. MPthankstheCentroNacionaldeInteligenciaArtificialinChile,aswellas
acknowledgesFondecytRegulargrantnumber1210462titled“Rigidity,stabilityanduniformityfor
largepointconfigurations”forsupportinghisresearch.
Notice: ThisworkwasproducedbyFermiResearchAlliance,LLCundercontractNo. DEAC02-
07CH11359withtheU.S.DepartmentofEnergy,OfficeofScience,OfficeofHighEnergyPhysics.
The United States Government retains and the publisher, by accepting the work for publication,
acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable,
world-widelicensetopublishorreproducethepublishedformofthiswork,orallowotherstodo
so,forUnitedStatesGovernmentpurposes. TheDepartmentofEnergywillprovidepublicaccess
to these results of federally sponsored research in accordance with the DOE Public Access Plan
(http://energy.gov/downloads/doe-public-access-plan).
11References
[1] TacoCohenandMaxWelling. Groupequivariantconvolutionalnetworks. InICML,2016.
[2] IlyesBatatia,MarioGeiger,JoseMunoz,TessSmidt,LiorSilberman,andChristophOrtner. A
generalframeworkforequivariantneuralnetworksonreductiveliegroups. AdvancesinNeural
InformationProcessingSystems,36,2024.
[3] ErikJ.Bekkers. B-splinecnnsonliegroups. In8thInternationalConferenceonLearning
Representations,ICLR2020,AddisAbaba,Ethiopia,April26-30,2020.OpenReview.net,2020.
[4] TacoSCohen,MarioGeiger,andMauriceWeiler. AgeneraltheoryofequivariantCNNson
homogeneousspaces. InH.Wallach,H.Larochelle,A.Beygelzimer,F.dAlché-Buc,E.Fox,
andR.Garnett,editors,AdvancesinNeuralInformationProcessingSystems,volume32.Curran
Associates,Inc.,2019.
[5] Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant
convolutionalnetworksandtheicosahedralcnn. InICML,2019.
[6] MarcFinzi,MaxWelling,andAndrewGordonWilson. Apracticalmethodforconstructing
equivariantmultilayerperceptronsforarbitrarymatrixgroups. ArXiv,abs/2104.09459,2021.
[7] RisiKondorandShubhenduTrivedi. Onthegeneralizationofequivarianceandconvolutionin
neuralnetworkstotheactionofcompactgroups. InICML,2018.
[8] EitanLevinandMateoDíaz. Any-dimensionalequivariantneuralnetworks. InInternational
ConferenceonArtificialIntelligenceandStatistics,pages2773–2781.PMLR,2024.
[9] HaggaiMaron,HeliBen-Hamu,NadavShamir,andYaronLipman. Invariantandequivariant
graphnetworks. ArXiv,abs/1812.09902,2019.
[10] MirceaMironencoandPatrickForré. Liegroupdecompositionsforequivariantneuralnetworks.
arXivpreprintarXiv:2310.11366,2023.
[11] EdwardPearce-Crump.Brauer’sgroupequivariantneuralnetworks.InInternationalConference
onMachineLearning,pages27461–27482.PMLR,2023.
[12] SiamakRavanbakhsh,JeffG.Schneider,andBarnabásPóczos. Equivariancethroughparameter-
sharing. InDoinaPrecupandYeeWhyeTeh,editors,Proceedingsofthe34thInternational
Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017,
volume70ofProceedingsofMachineLearningResearch,pages2892–2901.PMLR,2017.
[13] MauriceWeiler, PatrickForré, ErikP.Verlinde, andMaxWelling. Coordinateindependent
convolutionalnetworks-isometryandgaugeequivariantconvolutionsonriemannianmanifolds.
ArXiv,abs/2106.06020,2021.
[14] Yinshuang Xu, Jiahui Lei, Edgar Dobriban, and Kostas Daniilidis. Unified fourier-based
kernelandnonlinearitydesignforequivariantnetworksonhomogeneousspaces. InKamalika
Chaudhuri,StefanieJegelka,LeSong,CsabaSzepesvári,GangNiu,andSivanSabato,editors,
International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,
Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 24596–
24614.PMLR,2022.
[15] BenBlum-SmithandSoledadVillar. Machinelearningandinvarianttheory. Noticesofthe
AmericanMathematicalSociety,70:1205–1213,2022.
[16] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d
roto-translation equivariant attention networks. Advances in neural information processing
systems,33:1970–1981,2020.
[17] David Romero, Erik Bekkers, Jakub Tomczak, and Mark Hoogendoorn. Attentive group
equivariantconvolutionalnetworks. InInternationalConferenceonMachineLearning,pages
8188–8199.PMLR,2020.
12[18] Michael J Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont, Yee Whye Teh,
andHyunjikKim. Lietransformer: Equivariantself-attentionforliegroups. InInternational
ConferenceonMachineLearning,pages4533–4543.PMLR,2021.
[19] Raphael J. L. Townshend, Stephan Eismann, Andrew M. Watkins, Ramya Rangan, Maria
Karelina, Rhiju Das, and Ron O. Dror. Geometric deep learning of rna structure. Science,
373:1047–1051,2021.
[20] MinkyungBaek,FrankDimaio,IvanV.Anishchenko,JustasDauparas,SergeyOvchinnikov,
Gyu Rie Lee, Jue Wang, Qian Cong, Lisa N. Kinch, R. Dustin Schaeffer, Claudia Millán,
HahnbeomPark,CarsonAdams,CalebR.Glassman,AndyM.DeGiovanni,JoseH.Pereira,
AndriaV.Rodrigues,AlberdinaAikevanDijk,AnaCEbrecht,DiederikJohannesOpperman,
Theo Sagmeister, Christoph Buhlheller, Tea Pavkov-Keller, Manoj K. Rathinaswamy, Udit
Dalwadi,CalvinK.Yip,JohnE.Burke,K.ChristopherGarcia,NickV.Grishin,PaulD.Adams,
RandyJ.Read,andDavidBaker. Accuratepredictionofproteinstructuresandinteractions
usingathree-trackneuralnetwork. Science,373:871–876,2021.
[21] VictorGarciaSatorras,E.Hoogeboom,F.Fuchs,I.Posner,andM.Welling. E(n)equivariant
normalizingflowsformoleculegenerationin3d. ArXiv,abs/2105.09016,2021.
[22] BrandonAnderson,TruongSonHy,andRisiKondor. Cormorant: Covariantmolecularneural
networks. InH.Wallach,H.Larochelle,A.Beygelzimer,F.dAlché-Buc,E.Fox,andR.Garnett,
editors,AdvancesinNeuralInformationProcessingSystems,volume32.CurranAssociates,
Inc.,2019.
[23] JohannesKlicpera,JanekGroß,andStephanGünnemann. Directionalmessagepassingfor
moleculargraphs. ArXiv,abs/2003.03123,2020.
[24] MarysiaWinkelsandTacoCohen. Pulmonarynoduledetectioninctscanswithequivariant
cnns. Medicalimageanalysis,55:15–26,2019.
[25] RisiKondor,ZhenLin,andShubhenduTrivedi. Clebsch–gordannets: afullyfourierspace
sphericalconvolutionalneuralnetwork. AdvancesinNeuralInformationProcessingSystems,
31,2018.
[26] Jonathan Gordon, David Lopez-Paz, Marco Baroni, and Diane Bouchacourt. Permutation
equivariantmodelsforcompositionalgeneralizationinlanguage. InICLR,2020.
[27] I.Sosnovik,A.Moskalev,andA.Smeulders. Scaleequivarianceimprovessiamesetracking.
2021IEEEWinterConferenceonApplicationsofComputerVision(WACV),pages2764–2773,
2021.
[28] StephanEismann,RaphaelJ.L.Townshend,NathanielThomas,MilindJagota,BowenJing,
andRonO.Dror. Hierarchical,rotation-equivariantneuralnetworkstoselectstructuralmodels
ofproteincomplexes. Proteins: Structure,89:493–501,2020.
[29] JenniferC.WhiteandRyanCotterell. Equivarianttransductionthroughinvariantalignment. In
Proceedingsofthe29thInternationalConferenceonComputationalLinguistics,pages4651–
4663, Gyeongju, RepublicofKorea, Oct.2022.InternationalCommitteeonComputational
Linguistics.
[30] XupengZhu,DianWang,OndrejBiza,GuanangSu,RobinWalters,andRobertPlatt. Sample
efficientgrasplearningusingequivariantmodels. CoRR,abs/2202.09468,2022.
[31] ManzilZaheer,SatwikKottur,SiamakRavanbakhsh,BarnabasPoczos,RussRSalakhutdinov,
andAlexanderJSmola. Deepsets. Advancesinneuralinformationprocessingsystems,30,
2017.
[32] RisiKondor,HyTruongSon,HoracePan,BrandonAnderson,andShubhenduTrivedi. Covari-
antcompositionalnetworksforlearninggraphs. arXivpreprintarXiv:1801.02144,2018.
[33] XiangyuChenandMinYe. Cyclicallyequivariantneuraldecodersforcycliccodes. arXiv
preprintarXiv:2105.05540,2021.
13[34] XiaoxunGong,HeLi,NianlongZou,RunzhangXu,WenhuiDuan,andYongXu. General
framework for e (3)-equivariant neural network representation of density functional theory
hamiltonian. NatureCommunications,14(1):2848,2023.
[35] Tristan Maxson and Tibor Szilvási. Transferable water potentials using equivariant neural
networks. TheJournalofPhysicalChemistryLetters,15(14):3740–3747,2024.
[36] KoenMinartz,YoeriPoels,SimonKoop,andVladoMenkovski. Equivariantneuralsimulators
forstochasticspatiotemporaldynamics. AdvancesinNeuralInformationProcessingSystems,
36,2024.
[37] Sophie Baker, Joshua Pagotto, Timothy T Duignan, and Alister J Page. High-throughput
aqueouselectrolytestructurepredictionusingionsolvrandequivariantgraphneuralnetwork
potentials. TheJournalofPhysicalChemistryLetters,14(42):9508–9515,2023.
[38] MarcFinzi,GregoryBenton,andAndrewGWilson. Residualpathwaypriorsforsoftequiv-
arianceconstraints. InM.Ranzato,A.Beygelzimer,Y.Dauphin,P.S.Liang,andJ.Wortman
Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages
30037–30049.CurranAssociates,Inc.,2021.
[39] DavidWRomeroandSuhasLohit. Learningpartialequivariancesfromdata. Advancesin
NeuralInformationProcessingSystems,35:36466–36478,2022.
[40] TychoF.A.vanderOuderaa,DavidW.Romero,andMarkvanderWilk. Relaxingequivariance
constraintswithnon-stationarycontinuousfilters.InAdvancesinNeuralInformationProcessing
Systems(NeurIPS),volume35,Dec2022.
[41] RuiWang,RobinWalters,andRoseYu. Approximatelyequivariantnetworksforimperfectly
symmetricdynamics. InInternationalConferenceonMachineLearning,pages23078–23091.
PMLR,2022.
[42] Daniel McNeela. Almost equivariance via lie algebra convolutions. arXiv preprint
arXiv:2310.13164,2023.
[43] RuiWang,RobinWalters,andTessESmidt. Relaxedoctahedralgroupconvolutionforlearning
symmetrybreakingin3dphysicalsystems. arXivpreprintarXiv:2310.02299,2023.
[44] Tycho van der Ouderaa, Alexander Immer, and Mark van der Wilk. Learning layer-wise
equivariances automatically using gradients. Advances in Neural Information Processing
Systems,36,2024.
[45] KaitlinMaile,DennisGWilson,andPatrickForré. Equivariance-awarearchitecturaloptimiza-
tionofneuralnetworks. arXivpreprintarXiv:2210.05484,2022.
[46] AlonsoUrbanoandDavidWRomero. Self-superviseddetectionofperfectandpartialinput-
dependentsymmetries. arXivpreprintarXiv:2312.12223,2023.
[47] NingyuanHuang,RonLevie,andSoledadVillar. Approximatelyequivariantgraphnetworks.
ArXiv,abs/2308.10436,2023.
[48] MirceaPetracheandShubhenduTrivedi. Approximation-generalizationtrade-offsunder(ap-
proximate)groupequivariance. InA.Oh,T.Naumann,A.Globerson,K.Saenko,M.Hardt,
andS.Levine,editors,AdvancesinNeuralInformationProcessingSystems,volume36,pages
61936–61959.CurranAssociates,Inc.,2023.
[49] Atri Rudra. Arithmetic circuits, structured matrices and (not so) deep learning. Theory of
ComputingSystems,67(3):592–626,2023.
[50] TriDao,BeidiChen,NimitSSohoni,ArjunDesai,MichaelPoli,JessicaGrogan,Alexander
Liu,AniruddhRao,AtriRudra,andChristopherRé. Monarch: Expressivestructuredmatrices
forefficientandaccuratetraining. InInternationalConferenceonMachineLearning,pages
4690–4721.PMLR,2022.
14[51] TriDao,AlbertGu,MatthewEichhorn,AtriRudra,andChristopherRé.Learningfastalgorithms
forlineartransformsusingbutterflyfactorizations. InInternationalconferenceonmachine
learning,pages1517–1527.PMLR,2019.
[52] TriDao,NimitSSohoni,AlbertGu,MatthewEichhorn,AmitBlonder,MeganLeszczynski,
AtriRudra,andChristopherRé. Kaleidoscope: Anefficient,learnablerepresentationforall
structuredlinearmaps. arXivpreprintarXiv:2012.14966,2020.
[53] VikasSindhwani,TaraSainath,andSanjivKumar. Structuredtransformsforsmall-footprint
deeplearning. AdvancesinNeuralInformationProcessingSystems,28,2015.
[54] Anna Thomas, Albert Gu, Tri Dao, Atri Rudra, and Christopher Ré. Learning compressed
transformswithlowdisplacementrank. Advancesinneuralinformationprocessingsystems,31,
2018.
[55] VictorYPan. Structuredmatricesandpolynomials: unifiedsuperfastalgorithms. Springer
Science&BusinessMedia,2012.
[56] ThomasKailath,Sun-YuanKung,andMartinMorf. Displacementranksofmatricesandlinear
equations. JournalofMathematicalAnalysisandApplications,68(2):395–407,1979.
[57] Thomas Kailath and Ali H Sayed. Displacement structure: theory and applications. SIAM
review,37(3):297–386,1995.
[58] ThomasKailathandJoohwanChun. Generalizeddisplacementstructureforblock-toeplitz,
toeplitz-block,andtoeplitz-derivedmatrices.SIAMJournalonMatrixAnalysisandApplications,
15(1):114–128,1994.
[59] Roger Chalkley. Information about group matrices. Linear Algebra and its Applications,
38:121–133,1981.
[60] PaulDGader. Displacementoperatorbaseddecompositionsofmatricesusingcirculantsor
othergroupmatrices. LinearAlgebraanditsApplications,139:111–131,1990.
[61] WilliamCWaterhouse. Displacementoperatorsrelativetogroupmatrices. Linearalgebraand
itsapplications,169:41–47,1992.
[62] PhilippMHoll,KiwonUm,andNilsThuerey. phiflow: Adifferentiablepdesolvingframework
fordeeplearningviaphysicalsimulations. InWorkshoponDifferentiableVision,Graphics,and
PhysicsinMachineLearningatNeurIPS,2020.
[63] MauriceWeilerandGabrieleCesa. GeneralE(2)-EquivariantSteerableCNNs. InConference
onNeuralInformationProcessingSystems(NeurIPS),2019.
[64] MarcFinzi,GregoryBenton,andAndrewGWilson. Residualpathwaypriorsforsoftequiv-
arianceconstraints. AdvancesinNeuralInformationProcessingSystems, 34:30037–30049,
2021.
[65] DianWang,RobinWalters,XupengZhu,andRobertPlatt. Equivariant$q$learninginspatial
actionspaces. In5thAnnualConferenceonRobotLearning,2021.
[66] YannLeCun,FuJieHuang,andLeonBottou. Learningmethodsforgenericobjectrecognition
with invariance to pose and lighting. In Proceedings of the 2004 IEEE Computer Society
ConferenceonComputerVisionandPatternRecognition,2004.CVPR2004.,volume2,pages
II–104.IEEE,2004.
[67] HugoLarochelle,DumitruErhan,AaronCourville,JamesBergstra,andYoshuaBengio. An
empirical evaluation of deep architectures on problems with many factors of variation. In
Proceedingsofthe24thinternationalconferenceonMachinelearning,pages473–480,2007.
[68] HarmDerksen. Onthenuclearnormandthesingularvaluedecompositionoftensors. Founda-
tionsofComputationalMathematics,16(3):779–811,2016.
15A Proofs
A.1 ProofofLemma2.3
Proof. Direct by definition (1). For h,h′ ∈ H and g ∈ G, the entry of B at position (h′,g) is
h
δ(g =h′h). SinceH isclosedunderthegroupoperation,g =h′h⇒g ∈H.
A.2 ProofofProposition2.5
Proof. The result follows by linearity from the following explicit properties of group diagonals,
whicharedirecttocheckfromthedefinitions:
BT =B−1 =B , B B =B . (15)
g g g−1 g1 g2 g1g2
Finally,thecaseofKroneckerproductfollowsfrom(6).
A.3 ProofofProposition3.1
Proof. ForamatrixM letM beM’sprojection,i.e. thegroupmatrixrealizingthedistancetoGM.
0
Notethat∥M ∥≤∥M∥isaconsequenceofFrobeniusnormcomingfromaninnerproduct.
0
SinceforallmatricesX wehave∥X∥=∥XT∥,andsinceGMisclosedundertranspose,wehave
dist(M,GM) = ∥M −M ∥ = ∥MT −MT∥ ≥ dist(MT,GM). Byinterchangingtherolesof
0 0
M,MT wealsofindtheotherinequality,andthusproveitem1.
For item 2, note that for all matrices X,Y we have ∥XY∥ ≤ ∥X∥∥Y∥. Then we use the clo-
surepropertyofGMunderproduct,andtriangleinequality,inordertowritedist(MN,GM) ≤
∥MN −M N ∥ = ∥M(N −N )+(M −M )N ∥ ≤ ∥M∥∥N −N ∥+∥M −M ∥∥N ∥ ≤
0 0 0 0 0 0 0 0
max{∥M∥,∥N ∥}(∥M −M ∥+∥N −N ∥),whichallowstoconcludebynoting∥N ∥≤∥N∥.
0 0 0 0
For item 3, note that for all X,Y we have ∥X ⊗Y∥ ≤ ∥X∥∥Y∥. Then we proceed as for item
2,withtensorproductreplacingmatrixproduct,keepinginmindthat,duetoProp. 2.5,ifM ∈
0
GMG,N ∈GMH thenM ⊗N ∈GMG×H.
0 0 0
.
B Extendeddiscussiononnotionsofdisplacementrankforgroupmatrices
B.1 Comparisontoclassicaldisplacementdimensionforgroupmatrices
Inthissection,wereviewpreviousnotionsofdisplacementrankfrom[61,60]. Theseworkswere
motivatedbyaimingtogeneralizedifferentaspectsofLDRtheoryfromthecaseofcirculantmatrices,
formulatedverysimilarlytoourExample2.2. Inthecaseofgroupmatrices,itwasrealizedthatthe
propertyofcirculantmatricesofbeingconstantalongdiagonalscanbegeneralizedtothenotionof
groupdiagonals(whosenameincludestheterm"diagonals"forthisreason). Thenthenaturalideais
thatdisplacementneededtohavethepropertyofvanishingongroupmatricesasin[60,61].
In a formula extending the Stein-type displacement operator ∆ M = M −AMB, which we
A,B
definedinSection1,[60]definedadisplacementoperatorG(M):=M −PQ(M),inwhichP is
thepermutationmatrixcorrespondingtothecyclicpermutationofbasisvectorse (cid:55)→e (cid:55)→···(cid:55)→
1 2
e (cid:55)→e ,andQ(M)isaninvolvedoperationthatcyclicallypermuteselementsofM ofeachgiven
|G| 1
groupdiagonalpatternseparately. ThedisplacementoperationG(M)fromtheworkof[60]was
further rationalized and made more elegant in [61], which replaced the term PQ(M) by a more
generalformT(M)inwhichthepermutationP canbechosenarbitrarily. Inbothcases,operator
G(M)subtractselementsofM whichareshiftedalonggroupdiagonalpatternsB ,andastheshift
g
isdoneviaacyclicpermutation,wehavethepropertythatifallsuchdifferencesarezero,thenthe
entriesofM areconstantalonggroupdiagonals.
Ourcontributionistosimplifytheexpressionsfurther,byintroducingtheintermediatereordering
F(M),whichwasnotpresentinpreviouswork. Thisallowstoeasilyoperationalizetheimplementa-
tionthatisthefocusofthiswork. Ourdefinition(9),andthemorecomplexone(10),areindirect
paralleltothetreatmentfrom[60],[61]respectively. Themainjustificationforusing(10),isthatit
willbehelpfulintheproofofPropositionB.1below.
16B.2 Displacementdimensionchangesunderelementaryoperations
InthissectionweconsiderdisplacementerrorD (M)asdefinedin(10),andthederivednotionof
P⃗
displacementdimensiondim (M)from(11).Forthelatter,weareinterestedinhowitbehavesunder
D
elementaryoperationslikethosefrom[54,55]. Notethatinpreviousworkonclassicaldisplacement
rank[54],similarresultsareformulatedtostudytheclosurepropertiesofclassicalstructuredmatrices,
however,wewerenotabletoadaptdisplacementrank(12)similarlytoourcase(asalsomentioned
inSection3,aboveproposition3.1)forthefollowingreasons: (1)theresultsvalidforclassicalLDR
structured matrices are not valid in our case, (2) the neural network interpretation as number of
degreesoffreedomasquantifiedintheerrortopreciseequivarianceismoreimportanttooursetup.
Wesummarizeourresultsinthefollowing:
PropositionB.1. LetG,H betwogroups. Further,considerclassesof|G|×|G|-matricesM,M′
andaclassof|H|×|H|-matricesN,withD-dimensionswithrespecttothecorrespondinggroups
denotedrespectivelyasd ,d d . Thenthefollowingholds:
M M′ N
1. TheclassMT ={MT : M ∈M}hasdim (MT)=d .
D M
2. ThesetM+M′ :={M+M′ : M ∈M,M′ ∈M′}hasdim (M+M′)=d +d .
D M M′
3. ConsiderthesetM⊗N :={M⊗N : M ∈M,N ∈N}. Thenwithrespecttothegroup
diagonalsfromadirectproductgroupG×H,wehavedim (M⊗N)≤d +d .
D M N
Proof. Inthesettingofitem1,wefirstclaimthat
F(MT) =F(M) . (16)
gh g−1,g−1h
Oncethisisproved,wenotefirstthatF(MT)’srowscanbere-labeledbyg−1 (cid:55)→g,toobtainthe
matrixF(cid:101)(M)withentriesF(M) g,gh. Then,ifτ g(h):=gh,andσ
g
wasthepermutationusedinthe
definitionofD P⃗(M)in(10),thenfortherowsofF(cid:101)(M)wecanusecyclicpermutationτ g−1σ gτ g.
AfterthischangecoordinatesF(cid:101)(M)(cid:55)→F(MT). Sincealltheappliedtransformationsdonotdepend
onthechoiceofM ∈M,anddonotchangethedimensioncounts(since,asobservedbefore,dim
D
doesnotdependonthechoicesofσ ’s),thismeansthatM,MT havethesameD-dimension.
g
Itnowsufficestoprove(16). Forthis,wefirstnotethefollowingcommutationrelationofvectors
withgroupdiagonalmatricesB :
g
B diag(v)=diag(w)B , where w =v ,h∈G. (17)
g g h g−1h
Theabovefollowsdirectlyusingthedefinition(B ) =δ(h=gh′).
g h,h′
RecallthatB isapermutationmatrix,thusBT = B−1 andwecandirectlyverifyB B = B ,
g g g g h gh
B =Id,thus
id
BT =B−1 =B . (18)
g g g−1
Using(17),(18)andachangeofvariableg (cid:55)→g−1inthesums,wenowget,denotingF =F(M),
(cid:88) (cid:88) (cid:88)
MT = BTdiag((F ) )= diag((F ) B−1 = diag((F ) )B .
g gh h g,gh h g g−1,g−1h h g
g g g
Thisdirectlyimplies(16),asdesired.
Item2followsdirectlyfromlinearityandusingthepropertythatdim(V +W)≤dimV +dimW
foranyvectorsubspacesV,W ⊆R|G|×|G|.
Item3followsfromthedefinitionofKroneckerproduct,observingthatF(M ⊗N) = F(M)⊗
F(N).
C GeneralizationofFrameworktoHomogeneousSpacesandInfiniteGroups
C.1 Actionsonhomogeneousspaces.
Inmostmachinelearningscenarios,theinputismorefrequentlydefinednotasafunctionovera
group,butratheronaspaceonwhichthegroupacts[7]. Thatis,thespaceX thatencodestheinputs,
17isnotidentifiablewiththesymmetrygroupG,butratherisahomogeneousspaceofG,i.e. itcan
beidentifiedwithaquotientbyasubgroupH ⊆G,denotedX =G/H. Elementsx∈X arethen
identifiedwithsubsetsofthepartitionintorightcosets[x]:=xH ={x¯∈G: ∃h∈H, x¯=xh}.
Ingeneral,G/H stillhasanaturalactionofG,givenbyg·[x]:=[gx].
ConvolutionoverG/H.Theconvolutionoperationscanbeencodedinthiscaseasfollows.Consider
akernelϕ:G→Randachannelf :G/H →R,wedefine
(cid:88) (cid:88)
ϕ⋆f([x]):= ϕ(g)f(g−1[x])= ϕ(g)f([y])δ([y]=[g−1x]). (19)
g∈G g∈G,[y]∈G/H
Theaboveoperationcanbeexpressedusinggroupmatricesbyobservingthefollowing
δ(cid:0) [y]=[g−1x](cid:1) = δ(∃h∈H : yh=g−1x)= (cid:88) δ(yh=g−1x)= (cid:88) δ(x=gyh)
h∈H h∈H
(cid:88)
= (B ) =[B 1 ] , (20)
gy xh gy H x
h∈H
inwhich1 isthecharacteristicvectorofH. Onecanverifythat(B 1 )(x) = (B 1 )(x′)
H gy H gy′ H
whenever[x]=[x′]and[y]=[y′],thusexpression(20)doesnotdependonthechoicesofrepresen-
tativesx,y. Usingthisobservation,wecanfixachoiceofrepresentatives,namely
X ⊂G, suchthat ∀gH ∈G/H, |gH ∩X|=1.
Thenwecanequivalentlyworkwithf,ϕ⋆f :X →R. Wethenwrite(19)intermsofgroupdiagonal
matricesandrepresentativesfromX asfollows:
(cid:88)
ϕ⋆f(x)= ϕ(g)f([y])(B 1 )(x). (21)
gy H
g∈G,y∈X
C.2 Infinitediscretegroupsandpadding.
Althoughforeaseofexpositioninsection2weusedperiodictranslations,moretraditionallyCNNs
usethegroupG=Z2,whichisinfinite. Then,operation(2)involvesaninfinitesum,andisthusnot
computable. Thisisnaturallytakencareofbythefollowingadjustments,whichwestatedirectlyfor
generalgroupsG:
1. WeworkonlywithinputsψofsupportcontainedinafixedfinitesetX (forCNNs,X is
in in
asquare{0,...,n−1}2 ⊂Z2).
2. WeusekernelsϕofsupportconstrainedtoafinitesetN,typicallyaradius-k(intheword
metricinducedbyasetofgenerators)neighborhoodoftheidentity. WefurtherassumeN
k
tobesymmetric(weuseasymmetricsetofgenerators),inwhich"symmetric"meansclosed
undertakinginverses: g ∈N ⇔g−1 ∈N.
3. Whileapplyingϕ⋆ψ(x),x∈X ,thecomputation(2)involvestermsψ(g−1x),inwhich
in
sometimesg−1x∈/ X . Wethenhavetoextendϕbyzeroontheset(X ) ,wherewe
in in N
usenotation
(A) :={b−1a: a∈A,b∈B}=supp(1 ⋆1 ).
B B A
NotethatifN istheradius-kword-distanceballaroundtheidentity,then(A) alsocanbe
N
describedasthesetofallelementsatword-distance≤kfromA.
Theaboveoperationscanbesummarizedasfollows:
(cid:26)
ψ(x) ifx∈X ,
Pad(ψ)(x) := in
0 ifx∈∂ X :=(X ) \X ,
N in in N in
(cid:88)
[ϕ ⋆ Pad(ψ)](x) := ϕ(g)Pad(ψ(g−1x)) for x∈X . (22)
in
g∈N
Formally,extensionbyzerocorrespondstoasubspaceimmersiongivenbyamatrixmultiplication
E : RXin → R(Xin)N with entries E
x,y
= δ(x = y),x ∈ X in,y ∈ (X in) N, and then we can
defineanaloguesofgroupmatricesB(Xin)N,g ∈N bythesameformula(1)withtherestrictionof
g
h,h′ ∈(X ) only. Afterthis,theformulafor(22)(analogousto(3)forthiscase)reads
in N
 
C(cid:93) onv ϕ→− ψ :=ET (cid:88) ϕ(g)B g(Xin)NE →− ψ. (23)
g∈N
18C.3 Errortoequivariance.
(cid:93) →−
NotethatoperationConv isnotequivariantunderG-action,simplybecause ψ hasentriesinX ,
ϕ in
which in general is not a union of G-action orbits, and thus is not invariant under the G-action.
Inotherwords, therestrictionmapencodedinIT in(23)is"theculprit"responsiblefortheloss
of equivariance, whereas the term in parenthesis in (23) is actually equivariant when applied to
elementsoftheimageofE (correspondingtofunctionsover(X ) thatarezerooutsideX ). A
in N in
(cid:93)
benefitof(23)isthatitmakesitstraightforwardforustomeasuretheequivarianceerrorofConv,
byextendingthetheoryfromSection3. WorkingontheimageoftheextensionmapI,wedefine
a version of the displacement operator, denoted D(cid:101), as follows. Consider M(cid:102) := EETM where
(cid:16) (cid:17)
M := (cid:80) ϕ(g)B(Xin)N : following(8),wecanencodethismatrixas
g∈N g
(cid:88)
M(cid:102)= diag(F(cid:101)g)B g(Xin)N, where (F(cid:101)g)
x
:=M(cid:102)x,xg−1. (24)
g∈N
Then we form a matrix F(M(cid:102)) with rows F(cid:101)g,g ∈ N and define D(M(cid:102)) as in (9). We can bound
thedisplacementdimensionofmatricesoftheform(24)bynotingthattheM definedearlierhas
D(M)=0andbylinearityofD,onlycolumnsofF(cid:101)correspondingtoelementsx∈X insuchthat
thereexistsg ∈ N suchthatg−1x′ ∈ ∂ NX
in
cancontributetodegreesoffreedomofD(M(cid:102)). We
thusfindthatforM:={M(cid:102)asin(24)}itholdsthat:
dim (M)≤|(X ) \X ||N|, DR(M)≤|(X ) \X |. (25)
D in N in in N in
Theintuitiveexplanationfor(25)inthecaseofpaddingforclassicalCNNsisthatthenumberof
paddingpixelsprovidesrankcontrolfortheerrortopreciseequivarianceineachlayer.
D EquivarianceErrorAnalysis:
In[41],theauthorsusetheequivarianceerrorasametrictoquantifythedegreetowhichamodel
deviatesfromperfectsymmetryundergrouptransformations. Thismeasureisrelevantforapproxi-
matelyequivariantnetworks,whichaimtobalancethesymmetryconstraintsandmodelflexibility.
Theauthorsarguethatapproximatelyequivariantnetworkscanbettercapturetheimperfectsymme-
triesinreal-worldscenariosviaaddingsoftequivarianceregularizationduringtraining. However,
equivarianceerroranalysisforGM-CNNsuggeststhatwhileitdoesnotachievetheoptimalbalance
betweendataandmodelequivarianceerror,itconsistentlyoutperformsmostothermethods,including
ConvNet,RPP,LiftasseeninFigure1. Ouranalysisraisesthepossibilitythatthebalancebetween
modelanddataequivarianceerroranditsrelationtooverallperformanceneedsfurtheranalysisto
accuratelyquantify. GM-CNN’sabilitytocapturerelevantsymmetriesandofferstrongpredictiveca-
pabilitiesindicatethatcertainapplicationsmaybenefitfromamoreflexibleapproachtoequivariance.
DespiteRSteershowingequivarianceerrorclosetooptimallevel,GM-CNNoffersacompetitive
balanceandapromisingframeworkforreal-worldapplicationswherebothsymmetryandmodel
flexibilityarecrucial.
E BroaderImpact
Thispaperpresentsanovelformulationforthedesignoflight-weightapproximatelyequivariant
CNNs. Thedevelopmentcombinestwodifferentthreadsofresearchintooneframework. Italso
presentsatheorygeneralizingclassicalLDRtheoryfromcyclincgroupstogeneraldiscretegroups.
Assuch, whilethecontributionhasamethodologicalorientation, itisprimarilyintendedtobea
conceptualcontribution. Wedonotforeseeimmediatesocietalbroaderimpactofthiswork. However,
iftheformulationpresentedisscaledup,itcouldprovideveryefficientequivariantnetworksthat
couldpossiblyrunonmobiledevices.
19Figure1: Equivarianceerroranalysisonsyntheticsmokeplumewithdifferentlevelsofrotational
equivarianceasdescribedin[41].
20