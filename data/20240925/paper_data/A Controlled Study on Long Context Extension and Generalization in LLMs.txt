A Controlled Study on Long Context Extension
and Generalization in LLMs
YiLu2∗ JingNathanYan1∗ SonglinYang3 JustinT.Chiu1
SiyuRen2 FeiYuan2 WentingZhao1 ZhiyongWu2+ AlexanderM.Rush1
1CornellUniversity,2ShanghaiAILab,3MassachusettsInstituteofTechnology
Abstract
Broadtextualunderstandingandin-contextlearningrequirelanguagemodelsthat
utilizefulldocumentcontexts. Duetotheimplementationchallengesassociated
withdirectlytraininglong-contextmodels,manymethodshavebeenproposedfor
extendingmodelstohandlelongcontexts. However,owingtodifferencesindata
andmodelclasses,ithasbeenchallengingtocomparetheseapproaches,leadingto
uncertaintyastohowtoevaluatelong-contextperformanceandwhetheritdiffers
fromstandardevaluation. Weimplementacontrolledprotocolforextensionmeth-
odswithastandardizedevaluation,utilizingconsistentbasemodelsandextension
data.Ourstudyyieldsseveralinsightsintolong-contextbehavior.First,wereaffirm
thecriticalroleofperplexityasageneral-purposeperformanceindicatorevenin
longer-contexttasks. Second,wefindthatcurrentapproximateattentionmethods
systematicallyunderperformacrosslong-contexttasks. Finally,weconfirmthat
exactfine-tuningbasedmethodsaregenerallyeffectivewithintheirextensionrange,
whereasextrapolationremainschallenging.Allcodebases,models,andcheckpoints
aremadeavailableopen-sourceviahttps://github.com/Leooyii/LCEG,pro-
moting transparency and facilitating further research in this critical area of AI
development.
1 Introduction
Thepretrainingdatascaleoflargelanguagemodels(LLMs)hasexpandedgreatlyinrecentyears
withopenmodelstrainedupto15Ttokens[AI@Meta,2024]. Implementationchallengesmakeit
difficulttofullytrainmodelswithlongercontextwindowsduringpretraining[Liuetal.,2023a]. Still,
long-contextwindowsareconsideredcentral,astheyenableLLMstoperformtasksthatrequiremore
extensivetextualunderstanding,suchasutilizinginformationfromtextbooks[Tanzeretal.,2024],
summarizingnovels[Krys´cin´skietal.,2022],andengaginginmany-shotlearning[Bertschetal.,
2024,Lietal.,2023a].
Asatrade-off,researchershaveproposedcontextextension,whereanLLMinitiallypretrainedon
standardsequencesisadaptedforsignificantlylongercontextlengths[Chenetal.,2023a,Pengetal.,
2023, Han et al., 2023, bloc97, 2023]. These methods differ in the type of attention used and in
post-trainingadaptationtechniques. Theyvaryincomplexity,trainingrequirements,andqualitatively
exhibitsignificantlydifferentperformanceprofiles.
Unfortunately,thereisarelativelypoorunderstandingofthequantitativerankingsofthesedifferent
methodologies. Owingtotheperceivedchallengesofevaluation,severalnewmetrics,suchaslong
contextperplexity[Chenetal.,2023a,b,Hanetal.,2023],andretrievalaccuracy[Mohtashamiand
Jaggi, 2023, gkamradt, 2023] have been introduced [Bai et al., 2023, An et al., 2023]. However,
thedifferencesinlong-contextextensionproceduresmakeithardtocalibratethesemetricswhile
controllingforotherfactors.
∗Equalcontribution.+Correspondenceauthor
Preprint.Underreview.
4202
peS
32
]LC.sc[
2v18121.9042:viXraInthiswork,weimplementacontrolledprotocolforcontextextension.Theaimistocomparecontext
extensionwhileremovingspuriousfactorsthatimpactLLMability.
Modeling:Westandardizeonthesamebasemodelforallexperiments. Differentbasemodelsbehave
significantlydifferently,makingitchallengingtodrawgeneralconclusions. Forinstance,pastwork
evaluatesLM-Infinite[Hanetal.,2023]onLongBench[Baietal.,2023]usingdifferentbasemodels
[Xiaoetal.,2024,Luetal.,2024].
Extensions: Weimplementarangeofcontextextensionmethodswithinthesameframework. We
useastandardizedrecipetoeliminatepotentialgainsfromtailoredhyperparameters. Weadditionally
fixthepost-trainingdataforeachmethod,utilizinganidenticalandopen-sourcedtrainingcorpus[Fu
etal.,2024,Chenetal.,2023b].
Metrics: We look at both intrinsic metrics, such as perplexity, and extrinsic properties, such as
downstreamtaskperformance[Hsiehetal.,2024,gkamradt,2023,Baietal.,2023]. Weconsider
metricswithintheextensionlengthaswellasanextrapolationtolongercontexts.
Ourstudyidentifiesseveraltakeawaysforfutureresearch.First,contrarytosomeoftheargumentsfor
needingnewmetrics,ourfindingsindicateastrongcorrelationbetweenperplexityanddownstream
task performance for exact fine-tuned methods in controlled studies. While some approximate
attentionmethodsareunbalancedbetweenperplexityanddownstreamtaskperformance,thereisa
strongcorrelationbetweenthetwoformostextensiontasks.
Second,wefindrelativelypoorresultsforapproximateattentionmethods. Whiletheycanhandle
longer length contexts, there generally is a trade-off in terms of accuracy for most of our bench-
marks. Exactfrozenmethodsalsotendtodegrademodelperformance,showinghighsensitivityto
hyperparametersandoftenfailingwithageneraltrainingrecipe.
Finally, continual fine-tuning with exact attention generally works well, particularly within the
extended context length. Specifically, Dynamic NTK [emozilla, 2023] works best among these
methods. Extrapolationtolongerlengthsremainsachallengingproblem. Ourtrainingcode,models,
andcheckpointswillbeopen-sourcedtosupportfurtherresearchinthisarea.
2 RelatedWork
LongContextMethods Wedivideextensionmethodsintothreebroadclasses: exactattention,
approximateattention,andcontextcompression. Exactattentionmethodsaugmenttheparameter-
ization of attention. Position interpolation (PI) [Chen et al., 2023a], NTK-aware [bloc97, 2023],
DynamicNTK[emozilla,2023],YaRN[Pengetal.,2023],andCLEX[Chenetal.,2024],allbased
onRoPE[Suetal.,2021],designpositionembeddingsforlengthextension. Thesemethodsmaybe
appliedwithfine-tuningortofrozenmodels. Otherexactattentionmethodsfocusontraining-time
improvements,suchascontrastivetraining[Tworkowskietal.,2023]. Approximateattentionmeth-
odsintroducestructuredattentionapproximationsthatminimizethecomputationalcostoflength
extension. Chenetal.[2023b]introducedtheuseofLoRA[Huetal.,2021]andaspecializedlocal
attentionmechanismtoreducefurtherthecomputationaloverheadoffurtherfine-tuningwithlong
context. Otherapproachesbreakthetextintochunksandutilizeawell-designed"chunkrepresenta-
tion"toretrieverelevantchunksforattention[MohtashamiandJaggi,2023,Xiaoetal.,2024,Lu
etal.,2024]. LM-InfiniteandStreamLLM[Hanetal.,2023,Xiaoetal.,2023]retainonlyafew
tokensfromthebeginningofthetextandalocalwindowtokeeptheattentionwindowwithinthe
pretrainedlength. Xuetal.[2024]focusesonusingretrieverstoretrieverelevantblocksfromlong
documents. Finally,contextcompressionmethods,whichwedonotexploreinthiswork,reduce
lengthextensiontolengthcompressionviaasummarizationstep[Jiangetal.,2023,Lietal.,2023b].
LongContextEvaluationBenchmarks TheLongRangeArena(LRA)[Tayetal.,2020]isanearly
effortsinevaluatingtheproficiencyofprocessinglongcontextsindifferentmodalities. Sincethen,a
growingnumberofbenchmarkshaveemerged,includingLongBench[Baietal.,2023],LEval[An
etal.,2023],andLooGLE[Lietal.,2023c]. Thesebenchmarksareamixtureofdiversedownstream
tasksexplicitlytailoredtoassessthecapabilitiesofLLMsinunderstandingandgeneratinglengthy
contexts. Amongthesebenchmarks,LongBenchstandsoutforitsinclusionofdiversesequences
with varying lengths, distributions, patterns, languages, and domains, enabling a comprehensive,
nuancedevaluation. InadditiontoevaluatingLLMs’performanceondownstreamNLPtasks,thereis
2anotherlineofbenchmarksthatspecificallyfocusesonassessingparticularaspectsoflongcontext
processingability[Liuetal.,2023b,Hsiehetal.,2024]. Forinstance,MohtashamiandJaggi[2023]
proposethepasskeyretrievaltasktochallengealanguagemodeltoaccuratelylocateandretrieve
asimplepasskey(afive-digitrandomnumber)inalongcontextsequence. Similarly,theNeedle
inaHaystack[gkamradt,2023]testrequiresthemodeltoaccuratelyrecitetheinformationfroma
specifiedsentence(the"needle")fromalongdocument. However,mostexistingworksmainlyfocus
onevaluatingmainstreamcommercialmodels(e.g. GPT-4andClaude),open-sourcebasemodels,or
justperformindividualevaluationsofafewlongcontextmethods. Thereisalackofcomprehensive,
yetcontrolledevaluationonlong-contextextensiontechniquesthemselves.
3 ContextExtensionMethods
3.1 Background: AttentionandRoPE
ThebottleneckinlongcontextmodelinginTransformersisattention. AttentionisdefinedoverC
embeddings X = [x ,x ,...,x ]⊤ ∈ RC×d where d is the model dimension. Learned weight
1 2 C
matricesW
v
∈Rd×dk,W
q
∈Rd×dk,andW
k
∈Rd×dk areusedtotransformtheseinputswhere
d istheprojectedhiddendimension. Theattentionmechanismitselfcomputestheattentionmatrix
k
andappliesittoproduceaweightedsumofthevaluevectors:
(cid:18) QK⊤(cid:19)
Attention(Q,K,V)=AV=softmax √ V. (1)
d
k
Basicattentionwasoriginallydefinedwith: Q = XW ,K = XW ,V = XW . However,this
q k v
approachdoesnotdirectlyencodetherelativepositionofkeysandvalues.
RotaryPositionEmbeddings(RoPE)[Suetal.,2024]encodepositionalinformationbyapplyinga
phaserotationtoeachelementoftheembeddingvectors. Formally,wedefineatransformationf:
f (x ,θ)=R(θ,i)W⊤x (2)
W i i
Here x
i
∈ Rdk is an embedding for position i, W is a projection matrix, and θ ∈ Rdk/2 is a
frequencybasis. Thefunctionisdefinedbasedontherotarypositionmatrix:
cosiθ −siniθ ··· 0 0 
1 1
siniθ cosiθ ··· 0 0
 1 1 
 . 
R(θ,i)= . .  (3)
 
 0 0 ··· cosiθ −siniθ 
 dk dk
2 2
0 0 ··· siniθ cosiθ
dk dk
2 2
Due to the arrangement of frequencies, this matrix has the property that R(θ,n − m) =
R(θ,m)⊤R(θ,n) by Ptolemy’s identity. We redefine the query-key product between two posi-
tionsmandnas,
q⊤k =f (x ,θ)⊤f (x ,θ) (4)
m n Wq m Wk n
=(cid:0) R(θ,m)W⊤x (cid:1)⊤(cid:0) R(θ,n)W⊤x (cid:1) (5)
q m k n
=x⊤W R(θ,n−m)W⊤x (6)
m q k n
Inthisway,therelativepositionalinformationn−misimplicitlyinjectedintothequeryandkey
product,thustheattentionscore.
ThestandardRoPEtransformation,f W(x i,θ),setsthecomponentsθ
j
=b− d2 kj withbaseb=10000.
33.2 AdjustingtheFrequencyofRoPEforLongContextExtension
WeconsiderfourmethodsforperforminglengthextensiononRoPEembeddings: PositionInter-
polation (PI) [Chen et al., 2023a], NTK-RoPE [emozilla, 2023], YaRN [Peng et al., 2023] and
CLEX[Chenetal.,2024]. Inthissectionourgoalistoextendamethodtrainedtoextendposition
embeddings for context length C to length C′ >> C. The methods in this section perform this
extensionbyscalingthefrequencieswiththebasescalingvectorα∈Rd 2k:
f (x )=f(x ,α⊙θ). (7)
W i i
LinearPositionInterpolation(PI)decreasesthefrequenciesofthebasisfunctionssothatmore
tokensfitwithineachperiod. PIisimplementedbysettingthecomponentsofthebasescalingvector
to
C 1
αPI = = . (8)
j C′ t
wheret= C′ istargetlengthratio. PIhasbeenintegratedintomanyopen-sourcemodelssuchas
C
LLaMA2-7B-32K[Together.AI,2023],Vicuna-7B-v1.5[Chiangetal.,2023],andLongAlpaca[Chen
etal.,2023b].
NeuralTangentKernelInterpolationRoPE(NTK-RoPE) buildsonlinearpositioninterpolation
byintroducingaper-dimensionscalingfactor. InspiredbyfindingsfromtheNTKliteraturethatshow
thathigh-frequencyfeaturesaredifficultforMLPstolearn,NTK-RoPEpreserveshigh-frequency
featureswhileextendingtheperiodoflow-frequencyfeatures. Thisisaccomplishedviaadimension-
dependentbasescalingvectorα:
α jNTK-RoPE =κ− d2 kj , (9)
dk
where κ = (t)dk−2 so that the lowest frequency is scaled to match PI and the highest frequency
remainsthesameasinRoPE.
Anextensiontothisapproach,DynamicNTK-RoPEsuggeststhatinsteadoffixingscalingbasedon
asetratiosforallexamplesduringinference,theformulashouldadapttothecurrentcontextlength
foraspecificexample. WefollowedthesetupofFuetal.[2024]forDynamicNTK-RoPE.More
detailscanbefoundintheAppendix9.2.
YaRN,anotherRoPEextensionmethod,uses“NTK-by-parts"interpolationstrategiesacrossdifferent
dimensions of the embedding space and introduces a temperature factor to adjust the attention
distributionforlonginputs.
1 √
αYaRN =((1−γ ) +γ )/ T (10)
j j t j
Weusearampvectorγ todeterminetheinterpolationbetweenthe 1 andtheoriginalfrequencybase.
t
Theinterpolationgatingissetbasedonthefrequencyforthedimensionj.

0, ifθ <p,
 j
γ = 1, ifθ >q, (11)
j j
θj−p, otherwise.
q−p
Thevaluesofp,q,T canbetunedasneeded.
OthermethodssuchasCLEXChenetal.[2024]modelsthescalingvectorsasadynamicalsystem,
intendingtolearntarget-lengthdependentscalingvectors.
3.3 ApproximateAttention
Analternativeapproachistomodifytheattentionfunctionitself. Insteadofexactlycomputingeach
longerattention,thesemethodsselectasubsetofpositionstoattendto. Weconsiderfourwell-known
methodsbasedonthreedifferentattentionmechanisms: sparseattention,slidingwindowattention,
andretrievalattention.
4LongLoRA [Chen et al., 2023b] avoids computing attention ranges over C′ by only computing
theblock-diagonlpartofattention. Formally,givenasequencelengthofC′,LongLoRAdividesit
intoM blocksofsizeB,resultinginasparseattentionmatrixA∈RC′×C′ withablock-diagonal
structure:
A 0 ··· 0 
1
0 A ··· 0
 2 
A=  . . . . . . ... . . .   (12)
0 0 ··· A
M
whereA ∈RB×B istheattentionmatrixforthei-thblock. Inaddition,theyshifttheblocksforhalf
i
oftheheadsenablingtheinformationflowbetweengroupsviashifting. Notably,whiletheyemploy
localattentionduringthefine-tuningphase,fullattentionisstilladoptedduringtheinferencestage.
Landmark Attention [Mohtashami and Jaggi, 2023] addresses the challenge of attending over
longsequencesbybreakingtheinputsequenceintochunksandusingtrainable“landmark"tokens
tosummarizethesechunks. Theattentionprocessiscarriedoutintwostages. Givenasequence
of C′ embeddings, divided into M chunks, each of length B, the first step is to compute global
attentionbetweenthequeryvectorsQ ∈ RC′×dk (correspondingtoallinputembedding)andthe
landmarkvectorsL ∈ RM×dk (whichrepresentthechunks). Fromthisglobalattention, asetof
n-mostattended-tochunksisselectedforfurtherprocessing. Next,alocalattentionmechanismis
appliedwithineachoftheselectedchunks. Forthen-thselectedchunk,thekeymatrixforthechunk
isdenotedasK
n
∈RB×dk andQ
n
∈RB×dk. Theattentionmatricesarethencomputedasfollows:
(cid:18) QLT(cid:19)
A1 =softmax √ ∈RC′×M, (13)
d
k
(cid:18)
Q
KT(cid:19)
A2,n =softmax √n n ∈RB×B, (14)
d
k
Thefinalattentionforeachembeddingisacombinationofthesetwoattention.Thismethodefficiently
scalesattentionmechanismsforlongsequencesbyfocusingonlandmarktokensthatsummarize
largepartsofthesequence,followedbylocalattentionwithintherelevantchunks.
LM-Infinite[Hanetal.,2023](a.k.a.,SlidingWindowAttention)maintainsaslidinglocalwindowof
sizeM alongwithafixedglobalmemoryofGpositionsatthestartingpointofthegivenembedding.
GivenC′ embeddings, attentioniscomputedovertheM embeddingsinitslocalwindowandG
embeddings in global memory. LM-Infinite replaces relative positional information n−m with
min(n−m,C)whilecomputingthequeryandkeyproductinEq4. Altogether,LM-Infinitereduces
thecomplexityfromO((C′)2)toO(C′(M +G))withouttheneedtoscalepositionalencoding.
Self-Extend[Jinetal.,2024]mapstheunseenpositionsinextendedcontextlengthC′topositions
withinthepretrainingcontextlengthC toavoidtraining. Foreachembeddings,Self-Extendchooses
closestM embeddingsandanyembeddingsbeyondaredividedintomultiplegroups. Eachgroup
containsN embeddings. Whenperformingquery-keyproductbetweentwopositionsmandnin
Equation4,therelativepositionalinformationn−misreplacedbyrwhichiscomputedbyscaling
n−mw.r.tM andN:
(cid:26)
n−m, n−m≤M,
r =
M
+(cid:4)n−m(cid:5) −(cid:4)M(cid:5)
, n−m>M.
(15)
N N
where⌊·⌋denotesthefloordivision. ThemaximumextendedcontextlengthC′is(C−M)·N+M.
4 Long-ContextExtensionProtocol
BaseModelAllmodelsstartfromanidenticalLLaMA2-7Bbasecheckpoint[Touvronetal.,2023].
Thisapproachremovespotentialbiasesfromthecheckpointbasis. Notethattostudytheinfluenceof
basemodel,particularlythesizeofbasemodel,wealsousePhi-2-basecheckpoint[Javaheripietal.,
2023]forcontextextension,toverifywhetherthetrendsandanalysesweobservedareconsistent
acrossdifferentbasemodels,therebyavoidingpotentialover-generalization.TheresultsofPhi-2-base
arereportedinAppendix9.1.
5Fine-TuningWesample1Btokensfromalong-contextdatamixturefromFuetal.[2024]toachieve
state-of-the-art context extension performance. The details of the data are reported in Appendix
9.3. Wefocusonextendingthecontextwindowfrom4kto32k,asmostdownstreamtasksrequire
contextsunder32k.
Wemaintainafixedtrainingrecipetoensureconsistencyacrossallmodels[Chenetal.,2023b]. We
followexistingpracticesbykeepinganexponentialmovingaverage(EMA)ofmodelweightswitha
constantdecay. Mosttraininghyperparametersarebasedon[Fuetal.,2024],withthelearningrate
setto2×10−5. Weusealinearlearningratewarm-upandzeroweightdecay,utilizing8NVIDIA
A100GPUs.
For LongLora, wefine-tune theweightsoftheLoRA adapterwithtrainableembeddingand nor-
malization,thenmergethesetrainableweightswiththeLLaMA2basemodelforevaluation. For
LandmarkAttention,thetrainingcontextlengthissetto512,withablocksizeof64. ForCLEX,we
setthemaxscalefactorto32andusetheSiLUactivationfunction.
WereusetheoriginalscalefactortomaintainconsistencyforNTK,YaRN,andPositionInterpolation
methods. However,thisbasefactorsignificantlydegradescontinualfine-tunedmodels,particularly
causing performance deterioration in shorter sequences. Therefore, we conduct a grid search to
determine a better scale factor for different input lengths for NTK-RoPE method. Based on our
findings,wefollowandimproveuponFuetal.[2024]tosetthescalefactorforNTK-RoPEmethod.
ThescalefactoranditsrelationshipwithperplexityarereportedintheAppendix9.5. Pleasereferto
theAppendix9.2fordetailedhyperparametersetups.
Metrics We consider two sets of intrinsic metrics. The first is based on perplexity. We use the
bookcorpusPG19 [Raeetal.,2019]andtheProof-piledataset[Azerbayevetal.,2023]toevaluate
thelongsequencelanguagemodelingperformances. FollowingPressetal.[2022],allperplexity
evaluationsarecalculatedusingaslidingwindowwithawindowsizeof256.
Thesecondisbasedonretrieval.Wefocusontheneedleinthehaystacktask[gkamradt,2023](NIAH).
NIAH involves identifying a specific, relevant piece of information (the "needle") within a large
setofirrelevantdata(the"haystack"). Thistaskiscommonlyusedtotesttheprecisionandrecall
capabilitiesofLLMsinscenarioswheretherelevantdataissparseandsurroundedbyasignificant
amountofnoise. Additionally,weevaluatewithRULER[Hsiehetal.,2024]. RULERenhances
thestandardNIAHtestbyincorporatingvariationswithdifferenttypesandquantitiesofneedles.
Additionally,itintroducesnewtaskcategories,suchasmulti-hoptracingandaggregation,toevaluate
behaviorsbeyondsimplecontext-basedsearching.
Forextrinsicmetrics,weconsideracollectionoftasks. LongBench[Baietal.,2023]isafamilyof
bilingual,multitaskevaluationsforlong-contextunderstandingwidelyusedinmeasuringthelong-
contextabilitiesofLLMs[Jinetal.,2024,Xiaoetal.,2024,Luetal.,2024]. LongBenchincludes
single-documentquestionanswering,multi-documentQA,summarization,few-shotlearning,and
codecompletion. WefollowBaietal.[2023]toevaluatethemodelson32kcontextwindowsizesby
truncatingthepromptfromthemiddlewhenthetasklengthexceedsadesignatedcontextwindow
size. WealsoconsidertheManyShotstasks, wherethelong-contextmodelwillbegivenseveral
examplesasprompts. WeusetheTrecNews[Kontonisetal.,2024]datasetforthistask.
5 ExperimentalResults
5.1 ResultOverview
Table 1 overviews the results across both types of evaluation. The main result demonstrate that
fine-tunedexactattentionmethodsforlongcontexts, suchasNTK-32KandYARN,consistently
outperformapproximateattentionmethodsandfrozenmethodsbyasignificantmargin. Thissuggests
thattradingaccuracyforspeedinapproximateattentionmethodscanresultinalossofimportant
reasoningcapabilities,particularlyforretrieval-basedtasks. Theperformancedisparityhighlights
theimportanceofexactattentionmechanismsinmaintaininghighaccuracyoverextendedcontexts,
emphasizingtheneedforcarefulconsiderationofattentionmechanismsinmodeldesignforlong-
contexttasks. Wenowconsidereachtypeofresultinmoredetail.
6Table1: Overviewofresultsacrossdifferentextensiontypes.
AttentionMechanisms Model PPL Needle Mshots LongB RULER
Frozen NTK-F 14.52 18.8 64.5 25.54 0.72
Exact PI 5.85 42.1 75.5 33.48 57.66
Attention YaRN 5.85 46.7 75.0 33.45 36.95
Fine-Tuned
CLEX 5.82 71.1 74.0 33.48 52.17
NTK-32K 5.79 83.7 71.0 35.32 59.42
NTK-64K 5.93 69.1 73.0 34.30 60.03
Frozen LM-Infinite 6.71 23.9 61.5 25.84 12.34
Approxi.
Self-Extend 6.11 25.8 72.0 33.62 29.50
Attention
Fine-tuned LongLora 9.89 20.3 55.5 23.30 3.53
Landmark 8.13 50.9 50.0 28.19 13.56
Llama2 LM-Infinite Self-Extend
0%
33%
67%
100%
1K 8k 16k 32k 64K 1K 8k 16k 32k 64K 1K 8k 16k 32k 64K
Landmark LongLora PI
0%
33%
67%
100%
1K 8k 16k 32k 64K 1K 8k 16k 32k 64K 1K 8k 16k 32k 64K
YaRN CLEX NTK-F
0%
33%
67%
100%
1K 8k 16k 32k 64K 1K 8k 16k 32k 64K 1K 8k 16k 32k 64K
NTK-32k NTK-64k NTK-64k-2B
0%
33%
67%
100%
1K 8k 16k 32k 64K 1K 8k 16k 32k 64K 1K 8k 16k 32k 64K
Figure1: NeedleinaHaystackevaluation. Greensquaresindicatesahighretrievalsuccessrate,the
whitedashedlinedenotesthelongestlengthexamplesseenattrainingorfinetuning,andtheY-axis
representsthedistancetotheretrievedtarget.
5.2 Intrinsictasks
Perplexity Table 2 shows perplexity scores across length. We see that continuous fine-tuning
methodslikePI,YaRN,andLongLoraeffectivelykeeplowperplexityscoreswithinthepre-training
contextlength. However,whenthecontextlengthexceedsperplexityscoresescalateoncethecontext
surpassesthepre-trainedwindow. OnlyNTKandCLEXcangeneralizetounseensequencelengthin
bothpretrainingandcontinualfinetuning. Additionally,wefindthatexactattentionmaintainsbetter
perplexitythanLoRA,whichmayreduceLongLora’sability. WealsonotethatresultsonbothPG19
andProof-filegavenearlyconsistentconclusions.
7
eldeeN
fo
htpeDTable2: PerplexityresultsofdifferentmethodsonPG19andProof-file. NTK-32KandNTK-64K
refertoNTK-Dynamic,whichrequiresfinetuningonlongertext. Lenreferstothelongest-length
examplesseenattrainingorfine-tuning. Exreferstotheexactattention. Allresultsareproducedby
ourexperiments.
ModelDetails EvalLength
Len Ex Methods 2k 4k 8k 16k 32k 64k
PG19
4k ✓ LLaMA2 6.61 6.30 - - - -
Frozen 4k LM-Infinite 6.61 6.30 6.25 6.45 6.71 8.49
4k ✓ NTK-Frozen 6.61 6.30 6.82 7.94 14.52 -
4k Self-Extend 6.61 6.32 6.15 6.07 6.11 7.15
32k ✓ PI 6.88 6.52 6.27 6.08 5.95 -
32k ✓ NTK-32K 6.63 6.32 6.09 5.92 5.79 5.76
32k ✓ YaRN 6.70 6.39 6.16 6.01 5.93 -
Finetuned
32k ✓ CLEX 6.85 6.62 6.14 5.93 5.82 5.79
32k LongLora 12.80 11.52 10.70 10.18 9.89 -
32k Landmark 8.15 8.14 8.14 8.11 8.13 8.15
64k ✓ NTK-64K 6.83 6.49 6.25 6.07 5.93 5.85
Proof-file
4k ✓ LLaMA2 3.34 3.04 - - - -
Frozen 4k LM-Infinite 3.34 3.04 2.94 3.02 3.11 3.12
4k ✓ NTK-Frozen 3.34 3.04 2.91 3.09 4.06 12.65
4k Self-Extend 3.35 3.06 2.88 2.78 2.75 2.90
32k ✓ PI 3.34 3.03 2.83 2.68 2.58 -
32k ✓ NTK-32K 3.27 2.98 2.78 2.64 2.54 2.48
32k ✓ YaRN 3.29 3.00 2.81 2.68 2.59 106.38
Finetuned
32k ✓ CLEX 3.37 3.10 2.80 2.65 2.55 2.48
32k LongLora 5.97 5.10 4.58 4.27 4.13 -
32k Landmark 4.51 4.50 4.48 4.49 4.49 4.49
64k ✓ NTK-64K 3.33 3.03 2.83 2.69 2.58 2.51
Needle-in-the-haystack NIAHresultsareshowninFigure1. Continuousfinetuningapproaches
suchasNTK,PI,andYaRNhavesuccessfullyretrievedthe"needle"withinthepretraininglength.
Yet,onlytheNTKandCLEXmethodcanretrievetheneedlebeyondthepretraininglength,aligning
withtheperplexityresults. TheperformanceoftheExactAttentionMethodgenerallysurpassesthat
oftheApproximateAttentionMethods. LM-InfiniteandLandmarkExcelareonlywithinthelocal
window,andtheystruggletoretrievetheintermediatetextaccurately. RegardingtheDynamicNTK
method,NTK-Fexhibitsweakgeneralizationwhennottrained. Whentrainedonthesameamountof
data(1B),NTK-32KoutperformsNTK-64K.Whentrainedon2Btokens,NTK-64Kdemonstrateda
significantperformanceimprovement,detailsareinAppendix9.4.
RULER Wetestallmodelson13diversetasksfromthefourRULERcategories[Hsiehetal.,2024].
Eachmodelisevaluatedwith500examplesforlengthsof4k,8k,16k,32k,and64k. Resultsare
comparedwiththeLlama2-7BbaselineinTable3. WeobservedasimilartrendasintheNIHKtask,
whereNTKfamilymethodsperformedthebest. NTK-32kmaintainedrelativelygoodperformance
comparedtoothermethodsfinetunedwithalengthcapof32k. Performanceofmodelsondifferent
lengthandbreakdownby13subtaskscanbefoundinAppendix9.8.
5.3 Extrinsictasks
LongBench TheevaluationresultsofmostmethodsonLongBencharepresentedinTable4,and
resultsonallmethodsarepresentedinAppendix9.7. BothLM-InfiniteandLandmarkAttention
8Table3: RULERevaluation. Performanceofmodelsevaluatedatlengthfrom4kto64k. Eachscore
iscomputedbyaveragingtheaccuracyof13tasks. TrainLenreferstothelongest-lengthexamples
seenatcontinuousfinetuning.
Train
Models 4k 8k 16k 32k 64k
Len
LLaMA2 4k 80.94 - - - -
Frozen LM-Infinite 4k 81.05 30.01 18.02 12.34 10.56
NTK-Frozen 4k 81.14 44.45 14.79 0.72 0.91
Self-Extend 4k 65.03 50.73 44.02 29.50 9.34
PI 32k 84.56 76.04 69.64 57.66 0.00
NTK-32K 32k 86.58 77.75 70.01 59.42 46.26
YaRN 32k 79.12 65.60 54.21 36.95 0.00
Finetuned
CLEX 32k 50.18 63.93 64.35 52.17 30.61
LongLora 32k 10.58 6.37 3.67 3.53 0.00
Landmark 32k 22.37 17.52 16.31 13.56 14.15
NTK-64K 64k 86.60 76.34 69.56 60.03 49.31
Table 4: LongBench results. N-32 and N-64 refer to NTK finetuned on 32K and 64K context
lengthsrespectively. SEreferstoSelf-Extend. YNreferstoYaRN.CXreferstoCLEX.LLRrefers
toLongLora. Lenreferstoaveragelengthofthedatasets. TrainLenreferstothelongestlength
examplesseenattrainingorfinetuning. EvalLenreferstothemaximumlengthoftheinputprompt.
✓referstowhetherthemethodisexactattention.
Frozen Finetuned
Len Base N-F SE PI N-32 YN CX LLR Land N-64
✓ ✓ ✓ ✓ ✓ ✓
TrainLen 4k 4k 4K 32k 32k 32k 32k 32k 32k 64k
EvalLen 4k 32k 32K 32k 32k 32k 32k 32k 32k 32k
NQA 18k 21.09 3.88 23.49 23.02 23.73 19.82 24.19 12.07 12.47 24.31
QAP 4k 26.94 26.79 28.75 25.85 27.50 26.98 23.36 20.15 19.06 24.97
MQA 5k 32.42 29.82 32.66 35.10 38.22 37.11 40.83 24.50 21.86 40.60
HQA 9k 31.23 32.10 37.63 36.98 41.56 38.60 35.59 27.41 33.66 41.47
WQA 5k 25.75 22.34 30.70 29.38 31.58 30.63 28.24 21.46 24.94 28.62
MSQ 11k 10.55 8.84 15.73 16.80 17.41 22.08 17.12 11.46 11.41 18.24
GR 9k 17.32 17.87 13.15 25.61 28.27 20.98 24.68 24.05 17.20 24.37
QSM 11k 21.28 15.35 20.20 21.19 21.52 20.66 21.55 17.66 18.83 21.65
MWS 2k 3.44 9.30 1.50 10.55 22.13 8.91 16.96 21.19 19.43 25.02
TRE 5k 66.00 67.50 69.00 71.00 69.00 69.00 67.50 50.00 49.00 69.00
TQA 8k 87.89 18.69 88.44 88.55 88.86 89.63 89.36 12.28 74.75 88.65
SMS 6k 41.70 32.46 43.76 43.35 42.21 44.25 43.02 13.45 40.38 41.59
PSC 11k 2.10 2.67 0.00 1.50 2.68 1.05 2.50 4.57 0.64 2.09
PSR 9k 9.00 3.77 4.50 4.50 4.62 3.79 8.50 3.50 2.50 6.50
LCC 1k 68.22 63.64 68.47 55.05 56.78 54.06 49.45 57.12 56.70 52.04
REP 4k 61.73 53.69 59.99 47.26 49.09 47.60 42.84 51.92 48.23 39.68
Avg. 7k 32.92 25.54 33.62 33.48 35.32 33.45 33.48 23.30 28.19 34.30
exhibitsignificantperformancedegradationcomparedtothebasemodel. Incontrast,theNTK,PI,
andYaRNmethodshavesuccessfullymaintainedtheirperformanceat32k,demonstratingcomparable
resultsamongthesemethods. ThissuggeststhatPIandYaRNperformsimilarlyindownstreamtasks,
whiletheNTKfamilyofmodelsremainsstable.
Notably, the LongLoRA method, which utilizes LoRA, also experiences a performance decline
relativetothebasecheckpoint,LLaMA2. Wearguethatthismaybeduetothesensitivityofthe
trainingproceduresforLongLoRA,andweacknowledgethisinourlimitationdiscussionsection.
910 PI 10 PI 10 PI
YaRN YaRN YaRN
9 N NT TK K- -3 62 4K K 9 N NT TK K- -3 62 4K K 9 N NT TK K- -3 62 4K K
LongLora LongLora LongLora
8 L Ca Ln Ed Xmark 8 L Ca Ln Ed Xmark 8 L Ca Ln Ed Xmark
Linear Fit Linear Fit Linear Fit
7 7 7
6 6 6
40 50 60 70 80 24 26 28 30 32 34 10 20 30 40 50 60
Needle in a Haystack LongBench RULER
Figure4: PerplexityandaverageddownstreamtaskaccuracyforNeedleinahaystack,LongBench
andRULER.
Furthermore,theoverallperformanceonLongBenchhasnotshownsignificantimprovementover
LLaMA2.WehypothesizethatthisisduetotheaveragelengthofLongBenchtestdata(approximately
7.5k)beingconsiderablyshorterthanthe32kcontextwindowofthelong-contextmethods.
Many-shot In-Context Learning with Trec
News WeevaluateTRECNews[LiandRoth,
80 LM-Infinite
2002]with1to1000in-contextexamples. Per- NTK-Frozen
PI
formanceimproveswithmoreexamplesinFig- 70 Y La oR ngN Lora
Landmark
ure2. ExactAttentionmethodsshowsignificant 60 N NT TK K- -3 62 4k k
Self-Extend
gainsfrom10to50examples(+44.0%)and100 CLEX
50
to1000examples(+25.9%),withslowergrowth
from50to100examples(+5.7%).Approximate 40
Attention methods consistently underperform.
30
Performance gains align with model perplex-
20
ityinlongercontexts;NTK-Frozenexcelswith
fewerexamplesbutunderperformswithmore. 10
0
1 5 10 25 50 75 100 200 300 400 500 625 750 875 1000
Number of Examples In-Context
6 Analysis
Figure2: Many-shotin-contextlearningaccuracy
onTRECNews.
PerplexityandDownstreamTasks Several
existing studies [Sun et al., 2021, An et al., 2023] suggest that perplexity may not consistently
correlate with performance on long-range tasks. In Figure 4, we plot the perplexity of models
fromourevaluatedbenchmarksagainsttheirperformance. Thefigureshowsageneralcorrelation
betweenperplexityandmodelperformanceacrossvarioustasksforexactattentionmethods.However,
approximateattentionmethods,LongLora,andLandmarkonRULER,exhibitslightdeviationsfrom
this trend but still roughly fit into the linear relationship. We hypothesize that previous studies
mentionedthisdisconnectionduetothelackofcontrolledstudiesandthepresenceofnoisydata.
AsdepictedinFigure1,althoughLM-Infinite
achievesagoodperplexityscoreat32k,itfails
togeneralizebeyond4k. Thislimitationarises
because the LM-Infinite method focuses on a
context window of only 4k, resulting in poor
retrieval ability for longer contexts. This sug-
geststhatweshouldconsiderdownstreamtasks
atdifferentlengthswhenevaluatingperplexity
forapproximateattentionmethods.
Contextextensionhurtsintheshorttermand
gainsinthelongterm Whilecontextexten-
sionseemstoimproveperplexity,inTable4,we
donotnoticeasignificantgaininperformance.
Wehypothesizethatwhilethisdatasetcontains Figure3: Averagednegativelog-likelihoodofdif-
longtasks, theaveragelengthismuchshorter ferentmodelsbrokendownbycontextposition.
than32K.Thesemethodsseemtoimprovethe
abilitytomodellanguageoverthelongtermbuthurtintheshortterm. Tounderstandthisbetterwe
10
K23@LPP
ycaruccAcomputetheaveragednegativelikelihoodofeachpositionofYaRN,LLaMa2,andNTK-32Kper
position(withLLaMa2seeingjusttokensevery4kchunks)inFigure3.
NTKGeneralizesBeyond32k InFigure1,weobservethatNTK-32Ksuccessfullygeneralizesto
unseensequencelengthsbeyond32kinbothNeedle-in-the-HaystackandRULERtasks,performing
onparwithNTK-64K.Incontrast,NTK-Fdemonstratesgeneralizationupto8kbutfailstoextend
further. ThissuggeststhatwhileNTKmethodsmaypossessthecapabilitytogeneralizetolonger
unseensequences,theireffectivenessiscontingentuponconditionssuchascontinualfine-tuning.
Wefindthatupuntil4KtheyallimproveasexpectedwithLLaMa2havingthebestNLL.After4K
theyallfluctuateinaverage,butweseeaclearseparationwithYarnandNTKtakingintoaccountthe
longcontext. AtextremelylongcontextNTKremainsastrongmodelwhereasYarnbecomesreverts
toasimilarperformanceasLLaMa2.
7 LimitationsandBroaderImpacts
LimitationsAswearelimitedbycomputingbudget,weonlyexperimentwithLlama-2-7Basour
basemodel. Thefindingsinthispapermaynotgeneralizetoother,possiblylarger,basemodels. We
onlyfine-tunedmodelstocontextsizesof32k,andgeneralizationbehaviorstolongercontextsmay
differwhentrainingcontextsarelonger. Wealsoacknowledgethatthestandardizedtrainingrecipe
withfixedhyperparametersmaybiassomemodelsmorethantheothermodels.
Broader Impacts This paper provides an in-depth evaluation of various methods for extending
the context lengths of LLMs, offering insights into their performance in long-range settings. By
standardizingevaluationswithaconsistentbasemodelandextensiondata,thisworkprovidesclearer
benchmarksforfuturework. Wealsoopen-sourcecodeandmodelstopromotetransparency. Our
modelssharesimilarrisksasstandardLLMs,wheretheymaybeusedtogenerateharmfulcontent
andmisinformation.
8 Conclusion
Inthispaper,weusedastandardizedapproachtoassesstheperformanceofvariouslong-context
methodsinLLMs. Ourstudyunderscorestheroleofperplexityasacrucial,performanceindicator
atlengthandhighlightsthetrade-offsinherentindifferentattentionmechanisms. Weshedlighton
thestrengthsandweaknessesofvariousapproaches,providingvaluableinsightsforfutureresearch.
Aspartofourcommitmenttoopenscience, allourresources, includingcodebases, models, and
checkpoints,willbeopen-sourceduponacceptance,fosteringfutureadvancementsinthispivotal
areaofAIresearch.
9 Acknowledgements
WethankYaoFu,YueYu,TianyuGao,CelineLee,WoojeongKim,JackMorris,JunxiongWang,and
OscarYinfortheirsuggestionsandfeedback. ThisworkwassupportedbyNSFCAREER#2037519
andNSF#1901030.
References
AI@Meta. Llama3modelcard. 2024. URLhttps://github.com/meta-llama/llama3/blob/
main/MODEL_CARD.md.
HaoLiu,MateiZaharia,andPieterAbbeel. Ringattentionwithblockwisetransformersfornear-
infinitecontext. InNeurIPS2023FoundationModelsforDecisionMakingWorkshop,2023a. URL
https://openreview.net/forum?id=fXugVDtCQO.
GarrettTanzer,MiracSuzgun,ElineVisser,DanJurafsky,andLukeMelas-Kyriazi. Abenchmark
forlearningtotranslateanewlanguagefromonegrammarbook. InTheTwelfthInternational
ConferenceonLearningRepresentations,2024. URLhttps://openreview.net/forum?id=
tbVWug9f2h.
11WojciechKrys´cin´ski,NazneenRajani,DivyanshAgarwal,CaimingXiong,andDragomirRadev.
Booksum: A collection of datasets for long-form narrative summarization. In Findings of the
AssociationforComputationalLinguistics: EMNLP2022,pages6536–6558,2022.
AmandaBertsch, MaorIvgi, UriAlon, JonathanBerant, MatthewRGormley, andGrahamNeu-
big. In-context learning with long-context models: An in-depth exploration. arXiv preprint
arXiv:2405.00200,2024.
MukaiLi,ShansanGong,JiangtaoFeng,YihengXu,JunZhang,ZhiyongWu,andLingpengKong.
In-contextlearningwithmanydemonstrationexamples. arXivpreprintarXiv:2302.04931,2023a.
ShouyuanChen,ShermanWong,LiangjianChen,andYuandongTian. Extendingcontextwindowof
largelanguagemodelsviapositionalinterpolation,2023a.
BowenPeng,JeffreyQuesnelle,HongluFan,andEnricoShippole. Yarn: Efficientcontextwindow
extensionoflargelanguagemodels,2023.
ChiHan,QifanWang,WenhanXiong,YuChen,HengJi,andSinongWang. Lm-infinite: Simple
on-the-flylengthgeneralizationforlargelanguagemodels,2023.
bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) con-
text size without any fine-tuning and minimal perplexity degradation, 2023. URL
https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_
rope_allows_llama_models_to_have/.
YukangChen,ShengjuQian,HaotianTang,XinLai,ZhijianLiu,SongHan,andJiayaJia. Longlora:
Efficientfine-tuningoflong-contextlargelanguagemodels,2023b.
AmirkeivanMohtashamiandMartinJaggi. Landmarkattention: Random-accessinfinitecontext
lengthfortransformers,2023.
gkamradt. Needle in a haystack - pressure testing llms, 2023. URL https://github.com/
gkamradt/LLMTest_NeedleInAHaystack.
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,
XiaoLiu,AohanZeng,LeiHou,YuxiaoDong,JieTang,andJuanziLi. Longbench: Abilingual,
multitaskbenchmarkforlongcontextunderstanding,2023.
ChenxinAn,ShansanGong,MingZhong,XingjianZhao,MukaiLi,JunZhang,LingpengKong,and
XipengQiu. L-eval: Institutingstandardizedevaluationforlongcontextlanguagemodels,2023.
ChaojunXiao,PengleZhang,XuHan,GuangxuanXiao,YankaiLin,ZhengyanZhang,ZhiyuanLiu,
SongHan,andMaosongSun. Infllm: Unveilingtheintrinsiccapacityofllmsforunderstanding
extremelylongsequenceswithtraining-freememory. arXivpreprintarXiv:2402.04617,2024.
YiLu,XinZhou,WeiHe,JunZhao,TaoJi,TaoGui,QiZhang,andXuanjingHuang. Longheads:
Multi-headattentionissecretlyalongcontextprocessor. arXivpreprintarXiv:2402.10685,2024.
YaoFu,RameswarPanda,XinyaoNiu,XiangYue,HannanehHajishirzi,YoonKim,andHaoPeng.
Dataengineeringforscalinglanguagemodelsto128kcontext,2024.
Cheng-PingHsieh,SimengSun,SamuelKriman,ShantanuAcharya,DimaRekesh,FeiJia,Yang
Zhang,andBorisGinsburg. Ruler: What’stherealcontextsizeofyourlong-contextlanguage
models?,2024.
emozilla. DynamicallyScaledRoPEfurtherincreasesperformanceoflongcontextLLaMAwithzero
fine-tuning, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/
dynamically_scaled_rope_further_increases/.
GuanzhengChen,XinLi,ZaiqiaoMeng,ShangsongLiang,andLidongBing. Clex: Continuous
lengthextrapolationforlargelanguagemodels, 2024. URLhttps://arxiv.org/abs/2310.
16450.
JianlinSu,YuLu,ShengfengPan,BoWen,andYunfengLiu. Roformer: Enhancedtransformerwith
rotarypositionembedding. CornellUniversity-arXiv,CornellUniversity-arXiv,Apr2021.
12SzymonTworkowski,KonradStaniszewski,MikołajPacek,YuhuaiWu,HenrykMichalewski,and
PiotrMiłos´. Focusedtransformer: Contrastivetrainingforcontextscaling,2023.
EdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,andWeizhu
Chen. Lora: Low-rankadaptationoflargelanguagemodels. CoRR,abs/2106.09685,2021. URL
https://arxiv.org/abs/2106.09685.
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming
languagemodelswithattentionsinks,2023.
PengXu,WeiPing,XianchaoWu,LawrenceMcAfee,ChenZhu,ZihanLiu,SandeepSubramanian,
EvelinaBakhturina,MohammadShoeybi,andBryanCatanzaro. Retrievalmeetslongcontext
largelanguagemodels,2024.
HuiqiangJiang,QianhuiWu,XufangLuo,DongshengLi,Chin-YewLin,YuqingYang,andLiliQiu.
Longllmlingua:Acceleratingandenhancingllmsinlongcontextscenariosviapromptcompression,
2023.
YuchengLi,BoDong,ChenghuaLin,andFrankGuerin. Compressingcontexttoenhanceinference
efficiencyoflargelanguagemodels,2023b.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
LiuYang,SebastianRuder,andDonaldMetzler. Longrangearena: Abenchmarkforefficient
transformers,2020.
JiaqiLi,MengmengWang,ZilongZheng,andMuhanZhang. Loogle: Canlong-contextlanguage
modelsunderstandlongcontexts?,2023c.
NelsonF.Liu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,and
PercyLiang. Lostinthemiddle: Howlanguagemodelsuselongcontexts,2023b.
JianlinSu,MurtadhaAhmed,YuLu,ShengfengPan,WenBo,andYunfengLiu.Roformer:Enhanced
transformerwithrotarypositionembedding. Neurocomputing,568:127063,2024.
Together.AI. Llama-2-7b-32k-instruct—andfine-tuningforllama-2modelswithtogetherapi,2023.
URLhttps://www.together.ai/blog/llama-2-7b-32k-instruct.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:
Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality,March2023. URLhttps:
//lmsys.org/blog/2023-03-30-vicuna/.
HongyeJin,XiaotianHan,JingfengYang,ZhimengJiang,ZiruiLiu,Chia-YuanChang,Huiyuan
Chen,andXiaHu. Llmmaybelonglm: Self-extendllmcontextwindowwithouttuning,2024.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,LukasBlecher,Cris-
tianCantonFerrer, MoyaChen, GuillemCucurull, DavidEsiobu, JudeFernandes, JeremyFu,
WenyinFu, BrianFuller, CynthiaGao, VedanujGoswami, NamanGoyal, AnthonyHartshorn,
SagharHosseini,RuiHou,HakanInan,MarcinKardas,ViktorKerkez,MadianKhabsa,Isabel
Kloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,JenyaLee,
DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov,PushkarMishra,
IgorMolybog,YixinNie, AndrewPoulton, JeremyReizenstein, RashiRungta, KalyanSaladi,
AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,Binh
Tang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
SergeyEdunov, andThomasScialom. Llama2: Openfoundationandfine-tunedchatmodels,
2023.
Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio
CésarTeodoroMendes,WeizhuChen,AllieDelGiorno,RonenEldan,SivakanthGopi,Suriya
Gunasekar, Mojan Javaheripi, Piero Kauffmann, Yin Tat Lee, Yuanzhi Li, Anh Nguyen, Gus-
tavodeRosa,OlliSaarikivi,AdilSalim,ShitalShah,MichaelSantacroce,HarkiratSinghBehl,
AdamTaumannKalai,XinWang,RachelWard,PhilippWitte,CyrilZhang,andYiZhang. Phi-2:
Thesurprisingpowerofsmalllanguagemodels,2023.
13Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. Compressive
transformersforlong-rangesequencemodelling,2019.
ZhangirAzerbayev,BartoszPiotrowski,HaileySchoelkopf,EdwardW.Ayers,DragomirRadev,and
JeremyAvigad. Proofnet: Autoformalizingandformallyprovingundergraduate-levelmathematics,
2023.
OfirPress,NoahSmith,andMikeLewis. Trainshort,testlong: Attentionwithlinearbiasesenables
inputlengthextrapolation. InInternationalConferenceonLearningRepresentations,2022. URL
https://openreview.net/forum?id=R8sQPpGCv0.
VasilisKontonis,MingchenMa,andChristosTzamos. Activelearningwithsimplequestions,2024.
Xin Li and Dan Roth. Learning question classifiers. In Proceedings of the 19th international
conference on Computational linguistics -, Jan 2002. doi: 10.3115/1072228.1072378. URL
http://dx.doi.org/10.3115/1072228.1072378.
SimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer. Dolong-rangelanguage
modelsactuallyuselong-rangecontext?,2021.
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel
Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and
deduplicated version of RedPajama. https://www.cerebras.net/blog/
slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama,
2023. URLhttps://huggingface.co/datasets/cerebras/SlimPajama-627B.
14Appendix
9.1 Phi-2forContextExtension
Weuseanotheropen-weightmodel,Phi-2-baseJavaheripietal.[2023]asthebasepointforcontext
extension,toverifywhetherthetrendsandanalysesweobservedareconsistentacrossdifferentbase
models. Usinganidenticaltrainingrecipe,were-trainandre-evaluatesevenmodelswithPhi-2-base.
9.1.1 PerplexityonProof-fileofPhi-2-base
We evaluate the perplexity score of Phi-2-base on seven models in Table 5. Consistent with our
observationsintheoriginalsubmission,continuousfine-tuningmethodslikePI,YaRN,andLongLora
effectivelymaintainlowperplexityscoreswithinthepre-trainingcontextlength. However,perplexity
scoresescalateoncethecontextlengthexceedsthepre-trainedwindow. Notably,onlyNTKcould
generalizetounseensequencelengthsduringbothpre-trainingandcontinualfine-tuning.
Table5: PerplexityresultsofdifferentmethodsonProof-filewithPhi-2-base. NTK-32KandNTK-
64KrefertoNTK-Dynamic,whichrequiresfinetuningonlongertext. Lenreferstothelongest-length
examplesseenattrainingorfine-tuning. Exreferstotheexactattention. Allresultsareproducedby
ourexperiments.
ModelDetails EvalLength
Len Ex Methods 2k 4k 8k 16k 32k 64k
2k ✓ Phi-2-base 4.02 25.72 175.05 - - -
Frozen 2k ✓ NTK-Frozen 4.02 3.73 4.07 5.49 12.58 36.68
2k Self-Extend 4.08 3.70 3.48 3.42 3.48 3.73
32k ✓ PI 7.53 6.75 6.25 5.97 5.83 45.00
Finetuned 32k ✓ NTK-32K 4.24 3.81 3.51 3.32 3.18 3.20
32k ✓ CLEX 5.53 4.32 3.78 3.51 3.42 3.60
64k ✓ NTK-64K 4.63 4.14 3.82 3.61 3.47 3.38
9.1.2 RULERofPhi-2-base
We test all models on 12 diverse tasks (except QA-2) from the four Ruler Hsieh et al. [2024]
categoriesinTable6. Consistently,NTK-32kmaintainsrelativelystrongperformancecomparedto
othermethodsfine-tunedwithalengthcapof32k. TheonlyexceptionwasCLEXandNTK-32kat
64k,showingaslightdropinperformance.
Table6: RULERevaluationonsevenmethodswithPhi-2-base. Performanceofmodelsevaluated
atlengthfrom2kto64k. Eachscoreiscomputedbyaveragingtheaccuracyof12tasks. TrainLen
referstothelongest-lengthexamplesseenatcontinuousfinetuning.
Train
Models 2k 4k 8k 16k 32k 64k
Len
Phi-2-base 2k 83.73 - - - - -
Frozen NTK-Frozen 2k 83.98 52.95 18.09 4.07 0.06 0.00
Self-Extend 2k 68.55 50.82 36.65 22.00 7.83 2.32
PI 32k 25.51 23.19 16.88 14.99 4.78 0.00
Finetuned NTK-32K 32k 81.18 66.90 52.57 46.53 32.06 12.84
CLEX 32k 75.33 72.66 53.56 46.23 25.46 13.03
NTK-64K 64k 78.73 59.87 47.56 41.87 25.66 17.69
159.2 ImplementationDetails
Training Tomaintainconsistencyacrossallmodels,weuseafixedtrainingprotocol[Chenetal.,
2023b]. Weadoptstandardpracticesbyapplyinganexponentialmovingaverage(EMA)tothemodel
weightswithaconstantdecayrate. Themajorityofourtraininghyperparametersarederivedfrom
[Chenetal.,2023b],includingalearningrateof2×10−5. Weimplementalinearwarm-upforthe
learningrateandsettheweightdecaytozero,utilizing8NVIDIAA100GPUs.
ForLongLora,wefine-tunetheLoRAadapterweightsalongwithtrainableembeddingsandnormal-
ization,subsequentlyintegratingthesetrainedweightsintotheLLaMA2basemodelforevaluation.
ForLandmarkAttention,thetrainingcontextlengthis512,withablocksizeof64. ForCLEX,we
setthemaxscalefactorto32andusetheSiLUactivationfunction. Wepresentthehyperparameter
settingsfordifferentmethodsduringthetrainingstageinTable7.
Inference WelistthescalefactorsusedfordifferentlengthrangesduringinferenceinTable8. For
NTK-RoPE,giventhemaximumobservedlengthduringtrainingorinference,C ,andthescaling
test
hyperparameters,wefollowFuetal.[2024]inreplacingtwiths·
max(C′,Ctest)
−(s−1),andset
C
thehyperparametersto C′ duringbothtrainingandinference. ForLM-infinite,wesettheglobal
2C
memoryG = 10andthelocalwindowM = 4096. ForLandmarkAttention,thetrainingcontext
lengthissetto512,withablocksizeof64. ForSelf-Extend,wesetthelocalwindowsizeM for
neighbortokensto1024andthegroupsizeN to64.
Table7: HyperparametersforDifferentLongSequenceMethodsinTraining.
Train Train
Models α bsz lr
Len Tokens
PI 32k 1B 8.0 32 2e-5
NTK-32K 32k 1B 29.0 32 2e-5
YaRN 32k 1B 8.0 32 2e-5
LongLora 32k 1B 8.0 32 2e-5
Landmark 32k 1B - 32 2e-5
NTK-64K 64k 1B 57.0 32 2e-5
Table8: HyperparametersfortheScaleFactorDifferentLong-contextMethodsinInference.
Models 4k 8k 16k 32k 64k
Llama2 - - - - -
LM-Infinite - - - - -
NTK-Frozen 1.0 3.0 7.0 15.0 31.0
PI 8.0 8.0 8.0 8.0 8.0
NTK-32K 29.0 29.0 29.0 29.0 61.0
YaRN 8.0 8.0 8.0 8.0 8.0
LongLora 8.0 8.0 8.0 8.0 8.0
Landmark - - - - -
NTK-64K 57.0 57.0 57.0 57.0 57.0
9.3 TrainingDataConstruction
We sample 1B tokens from a long-context data mixture following Fu et al. [2024]. We use the
SlimPajama [Soboleva et al., 2023] dataset for continuous finetuning. This dataset serves as an
open-sourcereplicationoftheLLaMA[Touvronetal.,2023]pretrainingdatamixture. Itcomprises
82% web data (sourced 67% from CommonCrawl and 15% from C4), 4.5% code data (Github),
4.5%Wikipediacontent,4.5%books,2.5%Arxivpapers,and2.0%StackExchangecontent. Weuse
per-sourcelength-upsamplingtosample1Btokensfromthedatasets,whichincreasestheportion
oflongsequenceswhilekeepingthedomainmixturethesame. Wepackedallsampleddatainto
16NTK-32k NTK-64k NTK-64k-2B
0%
33%
67%
100%
1K 8k 16k 32k 64K 1K 8k 16k 32k 64K 1K 8k 16k 32k 64K
Figure5: NeedleinaHaystackevaluation. “NTK-64-2B”representstheNTK-64Kmodeltrained
with2Btokens. Greensquaresindicatesahighretrievalsuccessrate,thewhitedashedlinedenotes
thelongestlengthexamplesseenattrainingorfinetuning,andtheY-axisrepresentsthedistanceto
theretrievedtarget.
chunksofthecorrespondingtraininglength,regardlessofdocumentboundaries,followingcommon
practiceTouvronetal.[2023],Fuetal.[2024].
9.4 LongerModelNeedsmoreTrainingTokens
WeobservethattheperformanceofNTK-64KisnotasgoodasNTK-32K.Consequently,wefurther
sample2Btokensfromalong-contextdatamixturefromFuetal.[2024]fortrainingandevaluatethe
modelonthe"NeedleinAHaystack"task,asshowninFigure5. OurNTK-64Kmodeldemonstrates
asignificantperformanceimprovementwhentrainedwithmoretokens,indicatingthatlongermodels
requiremoretokensforeffectivetraining.
9.5 RoPEScaleFactorforDynamicNTK
WeobservethatthescalefactorsignificantlydegradesNTK-Dynamicmodels,particularlycausing
performancedeteriorationinshortersequences. Therefore,weconductagridsearchtodeterminea
betterscalefactorfordifferentinputlengths. Thescalefactoranditsrelationshipwithperplexityon
PG19arereportedinTable9.
Table 9: The scale factor and its relationship with perplexity on PG19. We only use the first 2
documentsofPG19tocalculatetheperplexity.
Models ScaleFactor 4k 8k 16k 32k 64k
1 7.65 118.82 NaN NaN NaN
3 8.19 7.99 57.15 386.02 NaN
7 9.39 9.26 9.61 72.62 486.13
NTK-Frozen
15 11.53 12.04 12.98 20.15 180.59
31 16.18 20.66 26.67 40.06 69.01
63 30.22 48.78 69.89 89.75 118.59
1 12.64 NaN NaN NaN NaN
5 7.84 7.638 10.36 NaN NaN
13 7.686 7.459 7.25 8.35 NaN
NTK-32K
29 7.689 7.457 7.24 6.82 9.11
61 7.8 7.565 7.34 6.91 6.63
125 7.99 7.774 7.57 7.13 6.83
1 19.16 NaN NaN NaN NaN
9 8.02 7.79 7.63 22.6 NaN
NTK-64K 25 7.89 7.65 7.443 7.04 14.02
57 7.922 7.67 7.44 7.01 6.75
121 8.016 7.75 7.51 7.06 6.77
17
eldeeN
fo
htpeD9.6 LongLoraValidation
To validate our LongLora Chen et al. [2023b] implementation, we reproduced their Llama-2-7b-
longlora-32k model following LongLora’s training data and training recipe. We evaluated the
perplexityforthecorrespondinglengthonPG19andProof-fileinTable10.
Table10: PerplexityresultsofLongLorareportedandourreproductiononPG19andProof-file.
Method 2k 4k 8k 16k 32k
PG19
Llama-2-7b-longlora-32k 8.29 7.83 7.54 7.35 7.22
OurReproduction 8.10 7.69 7.43 7.28 7.32
Proof-file
Llama-2-7b-longlora-32k 3.35 3.01 2.78 2.61 2.50
OurReproduction 3.33 3.01 2.80 2.67 2.61
9.7 LongBenchResults
TheevaluationresultsofallmethodsonLongBencharepresentedinTable11.
Table 11: LongBench results. N-32 and N-64 refer to NTK finetuned on 32K and 64K context
lengthsrespectively. InfreferstoLM-Infinite. SEreferstoSelf-Extend. LLRreferstoLongLora.
AvgLenreferstoaveragelengthofthedatasets. TrainLenreferstothelongestlengthexamplesseen
attrainingorfinetuning. EvalLenreferstothemaximumlengthoftheinputprompt. ✓refersto
whetherthemethodisexactattention.
Exact AvgLen Frozen Finetuned
Base Inf N-F SE PI N-32 YaRN CLEX LLR Land N-64
✓ ✓ ✓ ✓ ✓ ✓
TrainLen 4k 4k 4k 4K 32k 32k 32k 32k 32k 32k 64k
EvalLen 4k 32k 32k 32K 32k 32k 32k 32k 32k 32k 32k
NQA 18,409 21.09 10.39 3.88 23.49 23.02 23.73 19.82 24.19 12.07 12.47 24.31
QAPR 3,619 26.94 22.58 26.79 28.75 25.85 27.50 26.98 23.36 20.15 19.06 24.97
MFQA 4,559 32.42 26.19 29.82 32.66 35.10 38.22 37.11 40.83 24.50 21.86 40.60
HPQA 9,151 31.23 16.13 32.10 37.63 36.98 41.56 38.60 35.59 27.41 33.66 41.47
WMQA 4,887 25.75 20.64 22.34 30.70 29.38 31.58 30.63 28.24 21.46 24.94 28.62
MSQ 11,214 10.55 5.26 8.84 15.73 16.80 17.41 22.08 17.12 11.46 11.41 18.24
GR 8,734 17.32 13.43 17.87 13.15 25.61 28.27 20.98 24.68 24.05 17.20 24.37
QMSM 10,614 21.28 6.10 15.35 20.20 21.19 21.52 20.66 21.55 17.66 18.83 21.65
MNWS 2,113 3.44 3.63 9.30 1.50 10.55 22.13 8.91 16.96 21.19 19.43 25.02
TREC 5,177 66.00 61.00 67.50 69.00 71.00 69.00 69.00 67.50 50.00 49.00 69.00
TRVQA 8,209 87.89 81.40 18.69 88.44 88.55 88.86 89.63 89.36 12.28 74.75 88.65
SMSM 6,258 41.70 15.07 32.46 43.76 43.35 42.21 44.25 43.02 13.45 40.38 41.59
PSC 11,141 2.10 1.62 2.67 0.00 1.50 2.68 1.05 2.50 4.57 0.64 2.09
PSR 9,289 9.00 4.00 3.77 4.50 4.50 4.62 3.79 8.50 3.50 2.50 6.50
LCC 1,235 68.22 67.68 63.64 68.47 55.05 56.78 54.06 49.45 57.12 56.70 52.04
REPO 4,206 61.73 58.27 53.69 59.99 47.26 49.09 47.60 42.84 51.92 48.23 39.68
Average 7,425 32.92 25.84 25.54 33.62 33.48 35.32 33.45 33.48 23.30 28.19 34.30
9.8 RULERSubtasksResult
Theperformanceofmodelsondifferentlengthsandbreakdownsby13subtasksarereportedinTable
12(RULERon4k),Table13(RULERon8k),Table14(RULERon16k),Table15(RULERon32k)
andTable16(RULERon64k).
18Table12: Rulerresultson4kcontextlength. N-32andN-64refertoNTKfinetunedon32Kand
64Kcontextlengthsrespectively. InfreferstoLM-Infinite. SEreferstoSelf-Extend. LLRrefersto
LongLora. TrainLenreferstothelongestlengthexamplesseenattrainingorfinetuning. EvalLen
referstothemaximumlengthoftheinputprompt. ✓referstowhetherthemethodisexactattention.
Exact Frozen Finetuned
Base Inf N-F SE PI N-32 YaRN CLEX LLR Land N-64
✓ ✓ ✓ ✓ ✓ ✓ ✓
TrainLen 4k 4k 4k 4k 32k 32k 32k 32k 32k 32k 64k
EvalLen 4k 4k 4k 4k 4k 4k 4k 4k 4k 4k 4k
NIAH_S1 100.00 100.00 100.00 100.00 98.00 100.00 99.60 100.00 0.00 49.00 100.00
NIAH_S2 100.00 100.00 100.00 100.00 99.80 100.00 88.60 100.00 0.00 20.60 100.00
NIAH_S3 99.20 95.80 98.80 89.80 99.80 94.20 53.00 89.60 0.00 10.00 97.20
NIAH_M1 99.20 98.80 99.20 79.00 99.20 99.20 62.60 95.80 0.00 10.60 98.00
NIAH_M2 88.00 88.00 88.20 26.00 95.40 97.40 14.00 83.20 0.00 6.80 97.00
NIAH_M3 61.40 62.00 61.60 14.40 78.00 68.20 8.20 53.80 0.00 1.20 84.80
NIAH_MV 83.55 90.45 86.60 82.10 95.45 96.40 50.25 95.10 0.05 10.80 96.15
NIAH_MQ 95.45 96.15 96.00 90.70 96.95 97.00 62.00 96.20 0.00 5.35 98.25
VT 57.72 58.56 56.48 8.92 96.64 98.16 25.68 85.72 0.00 2.92 97.00
CWE 78.20 75.90 78.20 73.56 81.38 80.86 58.78 82.60 64.70 23.16 74.26
FWE 84.33 84.20 84.93 80.07 58.40 85.53 26.20 52.60 18.53 84.93 81.40
QA_1 62.20 60.40 62.40 60.60 57.80 62.40 60.20 55.80 26.20 37.20 55.80
QA_2 43.00 43.40 42.40 40.20 42.40 46.20 43.20 38.20 28.00 28.20 46.00
Avg. 80.94 81.05 81.14 65.03 84.56 86.58 50.18 79.12 10.58 22.37 86.60
Table13: Rulerresultson8kcontextlength.
Exact Frozen Finetuned
Base Inf N-F SE PI N-32 YaRN CLEX LLR Land N-64
✓ ✓ ✓ ✓ ✓ ✓ ✓
TrainLen 4k 4k 4k 4k 32k 32k 32k 32k 32k 32k 64k
EvalLen 4k 4k 4k 4k 4k 4k 4k 4k 4k 4k 4k
NIAH_S1 - 46.00 61.60 100.00 99.00 99.80 100.00 100.00 0.00 46.00 100.00
NIAH_S2 - 36.60 59.40 98.80 100.00 100.00 99.40 100.00 0.00 7.20 100.00
NIAH_S3 - 20.80 51.00 88.60 99.20 94.20 96.00 97.80 0.00 3.80 99.20
NIAH_M1 - 27.80 46.00 69.40 98.00 94.20 86.60 90.20 0.00 7.60 95.20
NIAH_M2 - 4.40 11.00 8.20 91.60 86.20 60.60 66.00 0.00 1.60 86.60
NIAH_M3 - 2.60 4.00 3.20 48.40 52.20 34.60 11.80 0.00 0.00 47.40
NIAH_MV - 30.35 41.35 52.95 65.50 85.95 70.40 61.25 0.00 6.25 84.75
NIAH_MQ - 30.15 50.40 78.70 93.25 95.20 92.45 86.95 0.00 3.35 94.95
VT - 4.88 69.88 1.48 91.20 96.16 77.52 48.16 0.00 3.08 94.36
CWE - 65.08 40.30 30.82 45.66 45.76 44.72 32.72 18.92 22.08 40.80
FWE - 56.73 64.87 59.00 65.07 70.13 10.53 45.40 16.73 76.60 54.13
QA_1 - 35.80 44.40 31.00 50.80 49.20 43.20 48.20 22.80 25.00 50.20
QA_2 - 29.00 33.60 37.40 40.80 41.80 36.80 42.60 24.40 25.20 44.80
Avg. - 30.01 44.45 50.73 76.04 77.75 65.60 63.93 6.37 17.52 76.34
19Table14: Rulerresultson16kcontextlength.
Exact Frozen Finetuned
Base Inf N-F SE PI N-32 YaRN CLEX LLR Land N-64
✓ ✓ ✓ ✓ ✓ ✓ ✓
TrainLen 4k 4k 4k 4k 32k 32k 32k 32k 32k 32k 64k
EvalLen 4k 4k 4k 4k 4k 4k 4k 4k 4k 4k 4k
NIAH_S1 - 21.00 14.20 99.80 97.20 99.40 100.00 99.80 0.00 42.40 99.80
NIAH_S2 - 17.00 17.40 93.40 100.00 100.00 99.20 100.00 0.20 6.80 100.00
NIAH_S3 - 11.60 8.20 77.00 99.60 98.60 89.60 99.60 0.00 3.60 100.00
NIAH_M1 - 15.80 9.20 60.00 97.80 93.20 83.40 89.40 0.00 5.60 90.80
NIAH_M2 - 0.00 0.60 3.80 82.80 79.80 19.60 72.00 0.00 0.80 67.60
NIAH_M3 - 1.00 0.00 1.80 34.20 18.20 7.40 15.00 0.00 0.00 29.60
NIAH_MV - 8.40 6.90 38.85 77.55 81.95 58.75 62.40 0.00 4.80 83.50
NIAH_MQ - 8.85 7.95 59.30 90.95 86.20 85.15 81.60 0.00 2.75 90.35
VT - 6.56 11.28 1.16 68.84 83.56 47.12 48.16 0.00 2.52 88.68
CWE - 19.94 28.36 17.80 27.26 26.32 23.72 28.60 0.62 11.90 21.20
FWE - 77.13 25.80 59.80 47.93 61.73 10.13 57.33 12.93 81.60 51.73
QA_1 - 22.80 36.40 28.00 46.00 45.20 43.20 49.20 13.20 23.00 45.00
QA_2 - 24.20 26.00 31.60 35.20 36.00 37.40 33.40 20.80 26.20 36.00
Avg. - 18.02 14.79 44.02 69.64 70.01 54.21 64.35 3.67 16.31 69.56
Table15: Rulerresultson32kcontextlength.
Exact Frozen Finetuned
Base Inf N-F SE PI N-32 YaRN CLEX LLR Land N-64
✓ ✓ ✓ ✓ ✓ ✓ ✓
TrainLen 4k 4k 4k 4k 32k 32k 32k 32k 32k 32k 64k
EvalLen 4k 4k 4k 4k 4k 4k 4k 4k 4k 4k 4k
NIAH_S1 - 7.80 0.00 83.00 97.20 99.00 85.80 85.60 0.00 33.80 100.00
NIAH_S2 - 7.00 0.00 68.40 99.00 100.00 81.00 94.00 0.00 3.20 99.20
NIAH_S3 - 6.40 0.00 42.80 97.00 99.40 62.40 97.20 0.00 2.60 96.40
NIAH_M1 - 8.80 0.00 29.40 93.40 90.80 63.20 78.40 0.00 5.40 82.60
NIAH_M2 - 0.00 0.00 2.40 48.80 39.40 6.40 40.40 0.00 0.20 36.60
NIAH_M3 - 0.00 0.00 1.40 5.80 8.60 1.20 8.00 0.00 0.00 7.20
NIAH_MV - 3.75 0.00 24.65 61.30 68.20 37.95 60.75 0.00 2.80 82.20
NIAH_MQ - 2.05 0.00 20.35 68.65 78.25 46.95 67.80 0.05 2.35 85.80
VT - 2.08 0.00 2.32 56.68 43.28 22.00 30.08 0.00 2.52 71.28
CWE - 4.48 0.02 17.46 26.72 11.78 11.38 22.70 13.26 3.68 7.34
FWE - 72.67 2.93 46.73 31.00 64.53 15.13 34.67 13.93 72.47 51.53
QA_1 - 20.20 5.20 20.60 33.40 34.40 23.00 28.00 6.00 22.60 27.00
QA_2 - 25.20 1.20 24.00 30.60 34.80 24.00 30.60 12.60 24.60 33.20
Avg. - 12.34 0.72 29.50 57.66 59.42 36.95 52.17 3.53 13.56 60.03
20Table16: Rulerresultson64kcontextlength.
Exact Frozen Finetuned
Base Inf N-F SE PI N-32 YaRN CLEX LLR Land N-64
✓ ✓ ✓ ✓ ✓ ✓ ✓
TrainLen 4k 4k 4k 4k 32k 32k 32k 32k 32k 32k 64k
EvalLen 4k 4k 4k 4k 4k 4k 4k 4k 4k 4k 4k
NIAH_S1 - 3.20 0.00 71.80 0.00 83.60 0.00 40.60 0.00 40.00 98.00
NIAH_S2 - 3.80 0.00 0.20 0.00 95.60 0.00 68.80 0.00 3.00 98.00
NIAH_S3 - 5.40 0.00 0.00 0.00 95.40 0.00 70.40 0.00 3.00 95.80
NIAH_M1 - 5.40 0.00 0.00 0.00 76.80 0.00 55.40 0.00 5.20 67.20
NIAH_M2 - 0.00 0.00 2.60 0.00 15.20 0.00 15.80 0.00 0.00 25.80
NIAH_M3 - 0.00 0.00 0.20 0.00 1.20 0.00 1.00 0.00 0.00 4.00
NIAH_MV - 4.45 0.00 0.20 0.00 51.70 0.00 36.40 0.00 3.70 51.20
NIAH_MQ - 4.45 0.00 0.05 0.00 56.60 0.00 43.50 0.00 2.45 65.40
VT - 1.28 0.00 12.20 0.00 34.28 0.00 0.00 0.00 2.40 41.48
CWE - 0.76 0.00 6.85 0.00 6.58 0.00 9.72 0.00 1.70 7.88
FWE - 72.20 11.47 26.47 0.00 25.27 0.00 11.73 0.00 82.67 27.73
QA_1 - 16.20 0.20 0.80 0.00 30.80 0.00 25.60 0.00 19.60 29.20
QA_2 - 20.20 0.20 0.00 0.00 28.40 0.00 19.00 0.00 20.20 29.40
Avg. - 10.56 0.91 9.34 0.00 46.26 0.00 30.61 0.00 14.15 49.31
21