PieClam: A Universal Graph Autoencoder Based on
Overlapping Inclusive and Exclusive Communities
Daniel Zilberg and Ron Levie
Faculty of Mathematics, Technion - Israel Institute of Technology
Abstract
We propose PieClam (Prior Inclusive Exclusive Cluster Affiliation Model): a probabilistic
graphmodelforrepresentinganygraphasoverlappinggeneralizedcommunities. Ourmethodcan
be interpreted as a graph autoencoder: nodes are embedded into a code space by an algorithm
that maximizes the log-likelihood of the decoded graph, given the input graph. PieClam is a
communityaffiliationmodelthatextendswell-knownmethodslikeBigClamintwomainmanners.
First,insteadofthedecoderbeingdefinedviapairwiseinteractionsbetweenthenodesinthecode
space, we also incorporate a learned prior on the distribution of nodes in the code space, turning
our method into a graph generative model. Secondly, we generalize the notion of communities by
allowingnotonlysetsofnodeswithstrongconnectivity,whichwecallinclusivecommunities,but
alsosetsofnodeswithstrongdisconnection,whichwecallexclusivecommunities. Tomodelboth
types of communities, we propose a new type of decoder based the Lorentz inner product, which
weprovetobemuchmoreexpressivethanstandarddecodersbasedonstandardinnerproductsor
normdistances. Byintroducinganewgraphsimilaritymeasure,thatwecallthelogcutdistance,
we show that PieClam is a universal autoencoder, able to uniformly approximately reconstruct
any graph. Our method is shown to obtain competitive performance in graph anomaly detection
benchmarks.
1 Introduction
In recent years, considerable research has concentrated on graph representation learning, aiming to
develop vector representations for graph entities, including nodes, edges, and subgraphs [24, 6]. In
graph autoencoders, e.g., [30, 22, 55, 41], the vertices of a graph are embedded in a code space, where
edges are inferred from the locations of the vertices in this space. Encoding graphs into a standard
space has a number of advantages. While different graphs can have different sizes and topology, the
code space is fixed with a fixed dimension. This helps when learning downstream tasks, where the
representation of the graph in the code space can be processed. For example, in link prediction, one
infers unknown edges by defining [30] or learning [32] a function that takes pairs of nodes in the code
space and predicts if there is an edge between them. In anomaly detection, one defines [10, 14] or
learns [7, 9], a function that takes the representation of a node and its neighborhood and predicts if
this node is normal or an anomaly. In graph and node classification, the graph is represented in the
code space, and one learns a model that predicts from this representation the classes of the nodes or
of the graph [31, 18].
Our Contribution. In this paper, we derive a new graph autoencoder from a statistical model of
graphs. In our model, similarly to SBM [46, 34] or community-based statistical models [2, 64], graphs
aregeneratedfromacombinationofintersectingcommunities/cliques. Namely,eachnodebelongstoa
differentsubsetofapredefinedsetofcommunities,andtheaffiliationsofthenodestothedifferentcom-
munities determine the probabilities of the edges of the graph. Here, the estimation of the community
affiliationsisseenasanencoderofgraphstoacommunity affiliation space,andthecomputationofthe
correspondingedgeprobabilitiesisseenasadecoder. Asopposedtopastworks,weconsidertwotypes
of generalized communities. First, standard inclusive communities, where any two nodes in the same
community are likely to be connected. Second, we propose exclusive communities, where belonging to
1
4202
peS
81
]GL.sc[
1v81611.9042:viXrathe same community reduces the probability of nodes being connected. For illustration, consider a so-
cial network of employers/recruiters and employees/job-seekers. Such a network has roughly bipartite
components, where job-seekers do not tend to connect to other seekers, and employers do not connect
tootheremployers, butemployersandseekersofthesamesubsectortendtoconnect. Hence, inclusive
communities for such a graph can correspond to job titles or subsectors, and exclusive communities
can correspond to sets of job-seekers and sets of employees from the same sector.
To formalize the above ideas, we propose the Prior Inclusive Exclusive Cluster Affiliation Model
(PieClam). This model represents graphs as overlapping inclusive and exclusive communities. More-
over, instead of embedding the nodes of a given graph in the community code space, our model also
learns a prior on the code space, so new graphs can be generated from the learned model. This makes
PieClam a graph generative model, like, e.g., [30, 22, 55, 41, 58].
Tomodelbothtypesofcommunities,weproposeanewtypeofdecoderbasedontheLorentz inner
product, in which case the code space is typically called a pseudo Euclidean space [21]. The addition
of exclusive communities in our model is not merely aimed at improving it heuristically for special
graphslikesocialnetworks. Rather,weproveinTheorems10and11thatusingexclusivecommunities
(via the Lorentz inner product) makes our model universal, namely, able to approximate with a fixed
budget of parameters any graph. This is in contrast to standard decoders based on only inclusive
communities, which we show are unable to represent many graphs.
To formalize this universality property, we propose a new similarity measure between graphs with
edge probabilities, that we call the log cut distance. We formalize the universality of our model as
follows: one can choose the dimension of the code space (the number of communities) a priori, and
guarantee that any graph of any size and topology can be approximated up to small log cut distance
by decoding points from this fixed space. We show that other related decoders do not satisfy this
universality property.
InSection4wesupportourconstructionwithexperiments. Wefirstconductsometoyexperiments
that illustrate the merits of our method. We then use PieClam to perform graph anomaly detection.
Here, PieClam learns a probabilistic model corresponding to a given graph, and this model can be
directlyusedforinspectingtheprobabilitiesofdifferentnodesinthisgraph. Nodeswithlowprobabil-
ities are deemed to be anomalies. Our models achieve competitive performance with respect to state
of the art anomaly detection methods.
Appendix B offers an extended discussion on related work.
2 Community Affiliation Models With Prior
Our model PieClam is best understood as an extension of the BigClam model [63]. Hence, after
introducing basic notations, we start with a detailed exposition of BigClam, before introducing our
novel constructions.
2.1 Notations
We denote by R the real line, and by R the non-negative real numbers. We denote “and” by ∧.
+
We denote matrices as boldface capital letter B = {b } , vectors as boldface lowercase letters
n,m n,m
b={b } , and scalars as regular lowercase letters b. Vectors b∈RN are always column vectors, and
n n
their transpose b⊤ row vectors. The rows of a matrix B ∈ RN×C are denoted by the same letter in
lowercase b⊤, where b ∈ RC, where we write in short b⊤ ∈ RC. A diagonal matrix D ∈ RN×N
n n n
with diagonal entries d is denoted by diag(d)=diag(d ,...,d ). The ℓ2 norm of a vector b∈RN is
1 N
defined to be ∥b∥=((cid:80)N b2)1/2.
n=1 n
AgraphisdenotedbyG=([N],E,A),where[N]={1,...,N}isthesetofN nodes,E ⊆[N]×[N]
is the set of edges, and A∈{0,1}N×N is the adjacency matrix. For weighted graphs, A∈[0,1]N×N,
where a is the edge weight of (n,m). In this work we focus on undirected graphs, for which
n,m
(m,n) ∈ E ⇔ (n,m) ∈ E, and A = A⊤. Any pair (n,m) ∈ [N] × [N] is called a dyad. The
neighborhood of a node n ∈ [N] is N(n) = {m ∈ [N] | (m,n) ∈ E}. A graph-signal is a graph with
node features G=([N],E,A,X) where X={x⊤}N ∈RN×D, x ∈RD, and D is called the feature
n n=1 n
dimension. Arandomgraphisagraph-valuedrandomvariable. Givenarandomgraphwithnodes[N],
we denote the event in which (n,m)∈E by n∼m, and the event (n,m)∈/ E by ¬(n∼m). A graph
2is bipartite if its vertex set [N] can be partitioned into two disjoint sets U and V, with U ∪V = [N],
such that every edge has one endpoint in U and the other in V.
2.2 BigClam
The code space in BigClam is RC, where each axis is interpreted as a community. Each entry fc of a
+
point f = (f1,...,fC) ∈ RC is interpreted as how much the point belongs to community c, where 0
+
means “not included in c” and 1 “included.” In this paper we call RC the affiliation space (AS), and
+
call any point in the affiliation space an affiliation feature (AF).
BigClam is a model where a simple random graph is decoded from a sequence of AFs F = {f =
n
(f1,...,fC)}N in the AS. For each pair of nodes n,m∈[N], the probability of the event ¬(n∼m)
n n n=1
(no edge between n and m), given their amount of membership in the same community c, is defined
to be
P(¬(n∼m)|f nc,f mc)=e−f ncf mc .
BigClam makes two assumptions of independence. First, the membership in one community does not
affect the membership in another community. Hence,
P(¬(n∼m))|F)=P(¬(n∼m))|f n,f m)=e−f n⊤fm, (1)
P(n∼m)|F)=P(n∼m|f n,f m)=1−e−f n⊤fm.
Due to this formula, BigClam is a so called Bernoulli-Poisson model (see Appendix B.4 for more
details). Thesecondassumptionisthattheeventsofhavingandedgebetweendifferentpairsofnodes
are independent. As a result, the probability of the entire graph ([N],E) conditioned on the AFs F is
(cid:32) (cid:33)
(cid:115)
(cid:89) (cid:89) (cid:89)
P(E|F)= P(n∼m|F) P(¬(n∼m)|F) (2)
n∈[N] m∈N(n) m∈/N(n)
Here, the square root is taken because the product considers each edge twice.
BigClamconsistofadecoder, decodingfromtheAFsFtherandomgraphG(F)withnodeset[N]
and independent Bernoulli distributed edges with probabilities P={P(n∼m|f ,f )}N .
n m n,m=1
From the encoding side, given a simple graph ([N],E), BigClam encodes the graph into the affili-
ation space by maximizing the log likelihood with respect to the AFs F
l(F)= 1
2
(cid:88) (cid:16) (cid:88) log(1−e−f n⊤fm)− (cid:88) f n⊤f m(cid:17) . (3)
n∈[N] m∈N(n) m∈/N(n)
BigClam is optimized by gradient descent, with update at iteration i
F(i+1) =F(i)+δ∇ l(F(i)), (4)
F
for some learning rate δ > 0. In order to implement the above iteration with O(|E|) operations at
each step, instead of O(N2), the loss can be rearranged as
2l(F)= (cid:88) (cid:16) (cid:88) log(ef n⊤fn −1)−f n⊤ (cid:88) f m+∥f n∥2(cid:17) .
(5)
n∈[N] m∈N(n) n∈[N]
The gradient of the loss is now
∇ fnl= (cid:88) f m(cid:0) 1−e−f n⊤fm(cid:1)−1 − (cid:88) f m+f n. (6)
m∈N(n) n∈[N]
Since the global term needs to be calculated only once, the number of operations is O(|E|) instead of
O(N2).
We observe that the optimization process is a message passing scheme. Looking at the dynamics
of the optimization process, we see that every node is pushed in the direction of a weighted average of
all of its neighbors, and pushed in the opposite direction by an average of all of the nodes. The sum
of both forces tends to drive communities towards the axes in the optimization dynamics.
32.3 Inclusive-Exclusive Cluster Affiliation Model
The BigClam decoder has a limitation due to a “triangle inequality type” behavior. Namely, suppose
that we would like to construct two features f and f with strong connectivity to a third feature f ,
1 2 3
and we are limited by a fixed affiliation feature dimension C. A naive approach to achieve this would
be to put f and f close to f so they have large inner products f⊤f and f⊤f . This would mean
1 2 3 1 3 2 3
that f⊤f would also be large, so f would be strongly connected to f under the BigClam model.
1 2 1 2
However, some graphs, like bipartite graphs, do not exhibit this triangle inequality type behavior. For
a rigorous treatment, see Section 3.4 and Appendix A.1. Next, we build the IeClam decoder, that
allows decoding any graph, that may have bipartite components without being limited by a triangle
inequality-type behavior.
Our Inclusive Exclusive Cluster Affiliation Model (IeClam) can be extended from BigClam by
replacing the inner product in the non-edge probability (1) by the more expressive Lorentz inner
product, which doesnot enforcea triangle inequality-typebehavior. Forthat, weextend the affiliation
space (AS) to be R2C, with two types of communities. The first C axes are called the inclusive
communities, and their corresponding features are called inclusive affiliation features (IAF), denoted
by t ∈ RC. The last C axes are called the exclusive communities, with exclusive affiliation features
(EAF),denotedbys∈RC.1 Wedefinetheconcatenatedaffiliationfeaturebyf =(t,s)∈R2C. Given
a sequence of affiliation features F = {f }N ∈ RN×2C, IeClam defines the probability of a single
n n=1
edge by
P(n∼m|f ,f )=1−exp(cid:0) −t⊤t +s⊤s (cid:1)
n m n m n m
(7)
=1−exp(cid:0) −f⊤Lf (cid:1) ,
n m
whereL=diag(1,...1,−1,...−1). Thebilinearform(u,v)(cid:55)→u⊤LviscalledtheLorentzinnerproduct,
andhasitsrootsinspecialrelativity. SeeAppendixB.8formoredetailsontheLorentzinnerproduct.
Note that L is not positive-definite, so it does not actually define an inner product. Moreover,
f⊤Lf can be negative even if f ,f ∈ RC. To guarantee that (7) defines a proper probability
n m n m +
between 0 and 1, we limit the affiliation space as follows.
Definition 1. A cone of non-negativity is a subset C of R2C such that for every f,g ∈ C we have
f⊤Lg≥0.
If we limit the affiliation space to be a cone of non-negativity, then IeClam gives well defined
probabilities in [0,1]. In our experiments, we restrict ourselves to the following simple construction of
a cone of non-negativity, noting that it is not the only possible construction.
Definition 2. The pairwisecone T is defined to be the set of affiliation features f =(t,s)∈R2C such
that for every c∈[C] we have −tc ≤sc ≤tc.
It is easy to see that T is a cone of non-negativity. Indeed, for any f =(t ,s ),f =(t ,s )∈T,
1 1 1 2 2 2
we have
∀c∈[C]: tctc −scsc ≥0,
1 2 1 2
so
C
(cid:88)
f⊤Lf = tctc −scsc ≥0.
1 2 1 2 1 2
c=1
AsopposedtotheBigClamdecoder,theIeClammodelcanapproximateabipartitegraphwithasmall
number of communities. Namely, a bipartite graph with two (disjoint) sides U,V ⊂[N] and constant
probability for edges between U and V can be represented using C = 1. Here, all of the nodes in U
are encoded to (a,a), and all of the nodes of V are encoded to (a,−a), for some a≥0. This gives zero
probability for edges within each part, and probability 1−e−2a2 for edges between the parts.
Remark 3. The above construction gives an interpretation for each pair of axes (tc,sc) in T as a
generalized community which can model anything between a clique and a bipartite component.
1Moregenerally,onecandefineadifferentnumberofinclusiveandexclusivecommunities.
4Given AFs F in a cone, IeClam is seen as a decoder, by decoding F into the random graph
G(F) with node set [N] and independent Bernoulli distributed edges with probabilities P = {P(n ∼
m|f ,f )}N .
n m n,m=1
For affiliation features F∈TN, the probability of the graph ([N],E) given F of IeClam is
P(E|F)
(cid:32) (cid:33)
(cid:115)
(cid:89) (cid:89) (cid:89) (8)
= (1−e−f n⊤Lfm) e−f n⊤Lfm .
n∈[N] m∈N(n) m∈/N(n)
Like BigClam, IeClam is optimized by maximizing the log likelihood with gradient descent
2l(F)=
(cid:88) (cid:16) (cid:88) log(1−e−fn⊤Lfm)− (cid:88) f⊤Lf (cid:17) (9)
n m
n∈[N] m∈N(n) m∈/N(n)
This loss can be efficiently implemented on sparse graph by the formulation
2l(F)=
(cid:88) (cid:16) (cid:88) log(ef n⊤Lfm −1)−f n⊤L (cid:88) f m+f n⊤Lf n(cid:17) . (10)
n∈[N] m∈N(n) n∈[N]
The gradient of the loss for node n is
(cid:18) (cid:19)
∇ fnl=L (cid:88) f m(cid:0) 1−e−f n⊤Lfm(cid:1)−1 − (cid:88) f m+f
n
. (11)
m∈N(n) n∈[N]
Notice that all of the calculations are the same as BigClam, up to replacing the dot product by the
Lorenz inner product.
2.4 Community Affiliation Models With Prior
BigClam and IeClam are not generative graph models. Indeed, these methods only fit a conditional
probabilityofthegraph,conditionedontheAFvalues,butthemethodsdonotlearntheprobabilityof
theAFsovertheaffiliationspace. Hence,thetotalprobabilityofE∧Fisnotdefined. ToextendIeClam
(andsimilarlyBigClam)intoprobabilisticgenerativemodels,wedefineapriorprobabilitydistribution
over the affiliation cone space C ⊂R2C, with probability density function p:R2C →[0,∞) supported
on C. Using Bayes law, we now obtain the joint probability P(E ∧F) of the edges and community
affiliation features via the probability density function
p(E,F)=P(E|F)p(F).
We assume that the prior probabilities of all nodes are independent, namely,
(cid:89)
p(F)= p(f ).
n
n∈[N]
Hence, the probability densities that a dyad (n,m) is an edge or non-edge are
p(n∼m,f ,f )=p(f )p(f
)(1−e−fn⊤Lfm),
n m n m
p(¬(n∼m),f ,f )=p(f )p(f
)e−fn⊤Lfm.
n m n m
As before, we assume that the probabilities of different edges are independent, which gives
(cid:115)
(cid:89) (cid:89) (cid:89)
p(E,F)= p(f ) P(n∼m|F ) P(¬(n∼m)|f ,f ).
n m n m (12)
n∈[N] m∈N(n) m∈/N(n)
Now, the log likelihood loss is
5(cid:32) (cid:33)
l(F)= (cid:88) log(p(f n))+ 1 2(cid:16) (cid:88) log(ef n⊤Lfm −1)−f n⊤L (cid:88) f m+f n⊤Lf n(cid:17) . (13)
n∈[N] m∈N(n) n∈[N]
WecallthisextensionofIeClamPieClam(PriorInclusiveExclusiveClusterAffiliationModel). We
similarly extend BigClam to PClam (Prior Cluster Affiliation Model) by replacing L in (13) with the
identity matrix.
Observe that the PieClam loss is similar to the IeClam loss only, with the addition of the prior,
acting as a per node regularization term. The prior attracts all nodes to areas of higher probability
during the optimization dynamics.
In order to sample from the above generative models, we first sample features {f } according
n n∈[N]
to p, and then connect them using either the BigClam or IeClam conditional probability. To model
the prior in practice, we use realNVP, which is a normalizing flow neural network model [12]. For
more details on normalizing flows, see Appendix B.7.
PieClamforgraphswithnodefeatures. Sofar,wehaveusedonlythetopologyofthegraph,not
considering node features. We extend PieClam (and PClam) to graph-signals ([N],E,X) as follows.
Concatenate the feature space of X with the affiliation space, and learn the prior on this combined
space. This only affects the prior p. The conditional edge probabilities are defined only in terms of
the affiliation features, as before.
3 Universality of PieClam and IeClam
In this section, we define the universality of graph autoencoders, and prove that IeClam and PieClam
are universal, while BigClam and PClam are not. The motivation behind the universality definition is
that we would like to uniformly choose the dimension of the code space, such that every graph can be
approximated by decoding some points in this fixed code space. Namely, we would like one universal
decoder that works for all graph, as opposed to choosing the dimension of the code space depending
on the graph.
3.1 General Graph Autoencoders
Next, we define a general decoder that defines edge probabilities by operating on pairs of points in a
code space.
Definition 4. A pairwaise decoder over the code spaces RM is a mapping D :R2M →[0,1]. Given
M
N points in the code space z = {z ∈ RM}N , the decoded graph G (z) is the weighted graph with
n n=1 N
adjacency matrix
(cid:0) (cid:1)N
D (z)= D (z ,z ) .
M M n k n,k=1
Clam models are special cases of pairwise decoders.
3.2 Log Cut Distance
Our definition of universality has the following form: for every error tolerance ϵ>0, there is a choice
of the dimension M(ϵ) of the code space such that every graph can be approximated up to error ϵ by
decoding some points in this space. To formalize the “up to error ϵ” statement, we present in this
section a new graph similarity measure which we call the log cut distance. Our construction is based
on a well-known graph similarity measure called the cut norm.
Definition 5. The cut norm of a matrix X∈RN×N is defined to be
1 (cid:12)(cid:88)(cid:88) (cid:12)
∥X∥□ :=
N2
sup (cid:12)
(cid:12)
x i,j(cid:12) (cid:12). (14)
U,V⊂[N]
i∈Uj∈V
6Thecut metric ∥A−B∥□ betweentwoadjacencymatricesAandBisinterpretedasthedifference
between the edge densities of A and B on the block U ×V on which their edge densities are the most
different.
The following graph similarity measure modifies the cut norm, making it appropriate for graphs
with random edges over a fixed node set.
Definition 6. Given two random graphs over the nodes set [N], with independently Bernoulli dis-
tributed edges, with probabilities P={p } and Q={q } respectively, their log cut
n,m n,m∈[N] n,m n,m∈[N]
distance is defined to be
(cid:32) (cid:33)
D□(P||Q):= 0<i en ,df
≤1
e+d+ N1
2
U,Vsu ⊂p [N](cid:12) (cid:12) (cid:12)log(cid:16) (cid:89) (cid:89) 1 1− −( (1 1− −e d) )p
q
nn ,, mm(cid:17)(cid:12) (cid:12)
(cid:12)
. (15)
n∈Um∈V
The second term in (15) is the cut distance ∥P˜ −Q˜∥□ between the matrix P˜ with entries
p˜ =−log(1−(1−e)p )
n,m n,m
and the matrix Q˜ with entries
q˜ =−log(1−(1−d)q ).
n,m n,m
Namely, the cut distance between the log likelihoods of non-edges. The parameters e and d make the
[0,1]-valued probabilities valid inputs to the log. The goal of e,d is to regularize the probability of the
edges, where higher regularization is penalized via the additive term e+d in (15).
For each choice of a cut U,V ⊂[N], the term
1 log(cid:16) (cid:89) (cid:89) 1−(1−e)p n,m(cid:17)
(16)
N2 1−(1−d)q
n,m
n∈Um∈V
is somewhat similar in structure to an un-normalized KL divergence, or distance of log likelihoods,
between the non-edge probabilies of the graphs P and Q over the dyads between U and V. Here,
“un-normalized” means that the dyads are drawn uniformly with probabilities 1/N2, but the sum
of probabilities is |U|·|V|/N2 and not 1. The un-normalized uniform distribution discourages the
supremum inside the definition of D□ from choosing small blocks for maximizing (16). Note that
normalizeduniformdistributionswouldleadD□tochoosesmallblocks,whichdonotreflectmeaningful
empiricalestimatesoftheedgestatistics(theedgedensitiesofsmallblockswouldnotbeinterpretable
as expected number of edges). To conclude, D□(P||Q) is interpreted as the maximal divergence
between P and Q over all blocks, up to the best regularizers e,d.
In our analysis we compute the log cut distance between the random decoded graph P and the
deterministictargetgraphA. WhileAhasedgeprobabilitiesin{0,1},Phasedgeprobabilitiesin[0,1)
for Clam models. Therefore, we only require regularization for A. We hence consider the following
modified version of Definition 6.
Definition 7. Given an unweighted graph with adjacency matrix A∈{0,1}N×N and a random graph
over the nodes set [N], with independently Bernoulli distributed edges with probabilities P = {0 ≤
p <1} , the log cut distance between P and A is defined to be
n,m n,m∈[N]
(cid:32) (cid:33)
D□(P||A):= 0<in d≤f
1
d+ N1
2
U,Vsu ⊂p [N](cid:12) (cid:12) (cid:12)log(cid:16) (cid:89) (cid:89) 1−1 (1− −p n d, )m
a
n,m(cid:17)(cid:12) (cid:12)
(cid:12)
.
n∈Um∈V
Lastly, in case both P and Q are [0,1)-valued, a simple version of the log cut distance is (15) with
the choice e=d=0, namely,
D0(P||Q):=
1
sup
(cid:12) (cid:12)log(cid:16) (cid:89) (cid:89) 1−p n,m(cid:17)(cid:12)
(cid:12). (17)
□ N2 (cid:12) 1−q (cid:12)
U,V⊂[N] n,m
n∈Um∈V
73.3 Universal Graph Autoencoders
We are now ready to define the universality of general pairwise autoencoders. Motivated by the
fact that a Clam autoencoder is actually a family of autoencoders, parameterized by the number of
communities, we also define general pairwise decoders as families.
Definition 8. A family of code spaces RM and corresponding pairwaise decoders D :R2M →[0,1],
M
parametrized by M ∈ N, is called universal if for every ϵ > 0 there is M ∈ N (which depends only
on ϵ) such that for every N ∈ N and every graph with adjacency matrix A and N nodes there are N
points in the code space {z ∈RM}N such that
n n=1
D□(D M(z)||A)<ϵ.
3.4 BigClam and PClam are Not Universal
We now show that BigClam (and hence also PClam) is not a universal autoencoder since it cannot
approximate bipartite graphs. Consider the bipartite graph B with N nodes at each part, and prob-
ability 1−e−a2 for an edge between the two parts, and 0 within each part. Since in this case all
probabilities are less than 1, we can use (17) as the definition of the log cut distance. The analysis for
Definition 7 extends naturally.
Let P be a decoded BigClam graph. Our goal is to show that there is no way to make D0(P||Q)
□
smallbychoosingthedimensionC uniformlywithrespecttoN. Infact,wewillshowBigClamcannot
approximate a bipartite graph at all.2
Claim 9. Under the above construction,
a2
D0(P||B)≥ .
□
16
As a result, BigClam is not a universal autoencoder.
The proof is given in Appendix A.1. We note that one can similarly show that BigClam is not
universal also with respect to the log cut distance of Definition 7.
3.5 Universality of IeClam and PieClam
We are now ready to show that IeClam (and hence also PieClam) is a universal autoencoder. The
proofs of the following two theorems are in Appendix A.3 and A.4.
Wegivetwoversionsfortheuniversalityresult. Thefirstiswithoutaconerestriction,andrequires
a relatively small number of communities for the given error tolerance. The corresponding decoder
produces edge weights that can be negative. The second theorem is restricted to the pairwise cone
of non-negativity, and has pessimistic asymptotics for the required number of communities given an
error tolerance. This decoder is guaranteed to produce proper edge probabilities in [0,1).
Theorem 10. For every epsilon ϵ > 0, every N ∈ N, and every adjacency matrix A ∈ [0,1]N×N,
thereareN affiliationfeaturesF∈R2K ofdimensionK =−9log(ϵ/2)2/ϵ2 suchthatthecorresponding
IeClam model P={P(n∼m|f ,f )}N satisfies
n m n,m=1
[D□(P||A)<ϵ.
Here, the log cut distance is from Definition 7. As a result, IeClam and PieClam are universal au-
toencoders with code space R2K.
Theorem 11. For every epsilon ϵ > 0, every N ∈ N, and every adjacency matrix A ∈ [0,1]N×N,
there are N affiliation features F in the cone of pairwise non-negativity T ⊂ R2C of dimension C =
24⌈−log(ϵ/2)2/ϵ2⌉ such that the corresponding IeClam model P={P(n∼m|f ,f )}N satisfies
n m n,m=1
D□(P||A)<ϵ.
Here, the log cut distance is from Definition 7. As a result, IeClam and PieClam are universal au-
toencoders with code space T.
2InAppendixA.2weshowthatonecanapproximateabipartitegraphof2N nodesusingC=N2classesinBigClam
ifthemodelignoresself-loops.
84 Experiments
4.1 Reconstructing Synthetic Priors
We consider a ground-truth synthetic prior p : RC → [0,∞) in PClam. We sample N = 500 points
from the prior, decode the corresponding PClam graph, and sample a simple graph from the random
Bernoulli edges. Then, given these sampled graph, we fit to it a PClam and model. In Figure 1 we
compare the ground-truth prior to the reconstructed prior. We observe that even though our method
is unsupervised, it still manages to capture the prior qualitatively well. More details are given in
Appendix D.
Figure 1: Left to right: Synthetic prior in an affiliation space of two inclusive communities. Recon-
structed prior by PClam with normalizing flow. Reconstructed affiliation features by PClam.
4.2 Reconstructing Synthetic SBMs
In Figures 2 and 3 we consider a synthetic SBM, and sample simple graph with N =210 nodes from
it. We then fit A PClam and PieClam model to it. The SBM is not dominant diagonal, so it cannot
be well approximated by the PClam model (which finds a nested community structure), while the
PieClam model approximates it well qualitatively. Additional details are given in Appendix D.
Figure 2: Left to right: Adjacency matrix sampled from SBM with three classes and 9 blocks. Adja-
cencymatrixofthefittedPieClamgraph,withtwoinclusiveandtwoexclusivecommunities. Affiliation
features of the PieClam matrix in T projected to (t1,s1) and (t2,s2).
9Figure 3: Left to right: Adjacency matrix sampled from SBM. Fitted PClam adjacency matrix based
on two inclusive communities. Affiliations features of the PCLam matrix.
4.3 Anomaly Detection
In unsupervised node anomaly detection, one is given a graph with node features, where some of the
nodes are unknown anomalies. The goal is to detect these anomalous nodes, without supervising on
any example of normal or anomalous nodes, by only using the structure of the graph and the node
features. WeuseClammodelsfornodeanomalydetectionbyfittingtheClammodeltothegraphand
flagging nodes as anomalous if they satisfy the following different criteria.
• (S)Star probability: GivenanyClammodel, anodeniscalledanomalousif(cid:81) P(n∼
m∈N(n)
m|F)<δ.
• (P) Prior probability: Given any Clam model with prior, a node n is called anomalous if
p(f )<δ.
n
• (PS) Prior star probability: Given any Clam model withprior, a node n is called anomalous
(cid:81)
if p(f ) P(n∼m|F)<δ.
n m∈N(n)
We reduce the dimension of the node features of the input graphs to 100 using truncated SVD, unless
the dimension of the features is smaller than 100 in which case we only normalize them to have zero
mean and standard deviation one. We use an affiliation space embedding dimension of 30 for F for
PieClam and IeClam, and 24 for BigClam. Every Clam method starts with a random embedding F.
Clam models with prior are trained with the following steps. (F-t): given a fixed prior p, optimize
only F for t steps. (p-t): given a fixed embedding F, optimize only p for t steps. For regularization, in
each iteration of of p we add Gaussian noise with amplitude 0.01 to the affiliation features. We train
PieClam with scheme F-500 → p-1300 → F-500 p-1300 with learning rate of 2e−6 on F and 1e−6 on
p. For the models which only optimize F (IeClam and BigClam) we use the following configurations:
We train IeClam on 2500 iterations with learning rate 1e-6 using 30 communities. we train BIGClam
on 2200 iterations with learning rate 1e-6. More details on hyper-parameters are given in Appendix
D.
InTable1wecomparetheperformanceofClammethodstoDOMINANT[10], AnomalyDAE[14],
OCGNN [62], AEGIS [9], GAAN [7] and TAM [52] on the datasets Reddit, Elliptic, and Photo. The
hyper-parametersofthecompetingmethodsaretakenastherecommendedvaluesfromtherespective
papers. The results are taken from Table 1 in [53]. We observe that our methods are first and second
place on all datasets. Moreover, S- IeClam, PS-PieClam and S BigClam each beats the competing
methods in two out of the three datasets.
10Method Reddit Elliptic Photo
(S)- IeClam 0.639 0.440 0.592
(S) - PieClam 0.610 0.435 0.556
(P) - PieClam 0.567 0.610 0.425
(PS) - PieClam *0.612 0.551 0.507
(S) - BigClam 0.637 0.434 0.581
DOMINANT 0.511 0.296 0.514
AnomalyDAE 0.509 *0.496 0.507
OCGNN 0.525 0.258 0.531
AEGIS 0.535 0.455 0.552
GAAN 0.522 0.259 0.430
TAM 0.606 0.404 *0.568
Table 1: Comparison of Clam anomaly detectors with competing methods. First place in boldface,
second with underline, third with *star. We observe that our methods are first and second place on
all datasets. Moreover, S- IeClam, PS-PieClam and S BigClam each beats the competing methods in
two out of the three datasets. The accuracy metric is areas under curve (AUC).
5 Conclusion
We introduced PieClam, a new probabilistic graph generative model. PieClam models graphs via
embedding the nodes into an inclusive and exclusive communities space, learning a prior distribution
inthisspace,anddecodingpairsofpointsinthisspacetoedgeprobabilities,suchthatpointsaremore
likely to be connected the more inclusive communities and the less exclusive communities they share.
WeshowedthatPieClamisauniversalautoencoder,abletoapproximateanygraph,wherethebudget
of parameters (the number of communities) can be predefined, irrespective of any property of specific
graph,noteventhenumberofnodes. OurexperimentsshowthatPieClamachievescompetitiveresults
when used in graph anomaly detection.
One limitation of PieClam is that, for attributed graphs, it only models the node features through
the prior in the community affiliation space, but not via the conditional probabilities of the edges
(given the community affiliations). Future work will deal with extending PieClam to also include the
node(oredge)featuresintheedgeconditionalprobabilities. Anotherlimitationofouranalysisisthat
the log cut distance is mainly appropriate for dense graphs. Future work will extend this metric to
sparse graphs. This can be done, e.g., similarly to the sparse constructions in [15].
Acknowledgements
This research was supported by the Israel Science Foundation (grant No. 1937/23)
References
[1] Christopher Aicher, Abigail Z Jacobs, and Aaron Clauset. Learning latent block structure in
weighted networks. Journal of Complex Networks, 3(2):221–248, 2015.
[2] Edo M Airoldi, David Blei, Stephen Fienberg, and Eric Xing. Mixed membership stochastic
blockmodels. Advances in neural information processing systems, 21, 2008.
[3] Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zu¨gner, and Stephan Gu¨nnemann. Netgan:
Generating graphs via random walks. In Proceedings of the 35th International Conference on
Machine Learning (ICML), pages 610–619. PMLR, 2018.
[4] ChristianBorgs,JenniferT.Chayes,L´aszl´oMikl´osLov´asz,VeraT.S´os,andKatalinVesztergombi.
Convergent sequences of dense graphs i: Subgraph frequencies, metric properties and testing.
Advances in Mathematics, 219:1801–1851, 2007.
11[5] SandroCavallari,VincentWZheng,HongyunCai,KevinChen-ChuanChang,andErikCambria.
Learning community embedding with community detection and node embedding on graphs. In
Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages
377–386, 2017.
[6] Fenxiao Chen, Yun-Cheng Wang, Bin Wang, and C.-C Kuo. Graph representation learning: a
survey. APSIPA Transactions on Signal and Information Processing, 9, 05 2020.
[7] ZhenxingChen,BoLiu,MeiqingWang,PengDai,JunLv,andLiefengBo. Generativeadversarial
attributed network anomaly detection. In Proceedings of the 29th ACM International Conference
on Information & Knowledge Management, pages 1989–1992, 2020.
[8] Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular
graphs. arXiv preprint arXiv:1805.11973, 2018.
[9] Kaize Ding, Jundong Li, Nitin Agarwal, and Huan Liu. Inductive anomaly detection on at-
tributed networks. In Proceedings of the twenty-ninth international conference on international
joint conferences on artificial intelligence, pages 1288–1294, 2021.
[10] KaizeDing,JundongLi,RohitBhanushali,andHuanLiu. Deepanomalydetectiononattributed
networks. In Proceedings of the 2019 SIAM international conference on data mining, pages 594–
602. SIAM, 2019.
[11] Meiqi Ding, Quanshi Yao, Qiang Zhang, Peng Cui, Wenwu Zhu, and Jianmin Shen. Data aug-
mentation for deep graph learning: A survey. arXiv preprint arXiv:1909.07251, 2019.
[12] LaurentDinh,JaschaSohl-Dickstein,andSamyBengio. Densityestimationusingrealnvp. arXiv
preprint arXiv:1605.08803, 2016.
[13] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel,
Al´anAspuru-Guzik,andRyanPAdams. Convolutionalnetworksongraphsforlearningmolecular
fingerprints. Advances in neural information processing systems, 28, 2015.
[14] Haoyi Fan, Fengbin Zhang, and Zuoyong Li. Anomalydae: Dual autoencoder for anomaly detec-
tiononattributednetworks. InICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 5685–5689. IEEE, 2020.
[15] Ben Finkelshtein, I˙smail I˙lkan Ceylan, Michael Bronstein, and Ron Levie. Learning on large
graphs using intersecting communities, 2024.
[16] Santo Fortunato and Darko Hric. Community detection in networks: A user guide. Physics
reports, 659:1–44, 2016.
[17] Alan M. Frieze and Ravi Kannan. Quick approximation to matrices and applications. Combina-
torica, 1999.
[18] JustinGilmer,SamuelS.Schoenholz,PatrickF.Riley,OriolVinyals,andGeorgeE.Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning - Volume 70, ICML’17, page 1263–1272. JMLR.org, 2017.
[19] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International conference on machine learning, pages
1263–1272. PMLR, 2017.
[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
AaronCourville,andYoshuaBengio. Generativeadversarialnets. Advancesinneuralinformation
processing systems, 27, 2014.
[21] Werner Greub. Linear Algebra. Springer-Verlag, New York, 2nd edition, 1963. Pseudo-Euclidean
Spaces.
[22] Aditya Grover, Aaron Zweig, and Stefano Ermon. Graphite: Iterative generative modeling of
graphs. In International conference on machine learning, pages 2434–2444. PMLR, 2019.
12[23] XiaojieGuoandLiangZhao. Asystematicsurveyondeepgenerativemodelsforgraphgeneration.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(5):5370–5390, 2022.
[24] William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods
and applications. arXiv preprint arXiv:1709.05584, 2017.
[25] GM Harshvardhan, Mahendra Kumar Gourisaria, Manjusha Pandey, and Siddharth Swarup
Rautaray. A comprehensive survey and analysis of generative models in machine learning. Com-
puter Science Review, 38:100285, 2020.
[26] Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels:
First steps. Social networks, 5(2):109–137, 1983.
[27] JohnIngraham,VikasGarg,ReginaBarzilay,andTommiJaakkola. Generativemodelsforgraph-
based protein design. Advances in neural information processing systems, 32, 2019.
[28] Jonathan Q Jiang. Stochastic block model and exploratory analysis in signed networks. Physical
Review E, 91(6):062805, 2015.
[29] DP Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
[30] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint
arXiv:1611.07308, 2016.
[31] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In5th International Conference on Learning Representations, ICLR 2017,Toulon, France,
April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
[32] Ajay Kumar, Shashank Sheshar Singh, Kuldeep Singh, and Bhaskar Biswas. Link prediction
techniques, applications, and performance: A survey. Physica A: Statistical Mechanics and its
Applications, 553:124289, 2020.
[33] Pierre Latouche, Etienne Birmel´e, and Christophe Ambroise. Variational bayesian inference and
complexity control for stochastic block models. Statistical Modelling, 12(1):93–115, 2012.
[34] Clement Lee and Darren J Wilkinson. A review of stochastic block models and extensions for
graph clustering. Applied Network Science, 4(1):1–50, 2019.
[35] Ron Levie. A graphon-signal analysis of graph neural networks. In Thirty-seventh Conference on
Neural Information Processing Systems, 2023.
[36] Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing flows.
Advances in Neural Information Processing Systems, 32, 2019.
[37] L´aszl´oMikl´osLov´asz. Largenetworksandgraphlimits. Involume 60 of Colloquium Publications,
2012.
[38] L´aszl´o Mikl´os Lov´asz and Bal´azs Szegedy. Szemer´edi’s lemma for the analyst. GAFA Geometric
And Functional Analysis, 2007.
[39] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and Le-
man Akoglu. A comprehensive survey on graph anomaly detection with deep learning. IEEE
Transactions on Knowledge and Data Engineering, 35(12):12012–12038, 2021.
[40] Kaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago, and Motoki Abe. Graphnvp: An
invertible flow model for generating molecular graphs. arXiv preprint arXiv:1905.11600, 2019.
[41] Nikhil Mehta, Lawrence Carin Duke, and Piyush Rai. Stochastic blockmodels meet graph neural
networks. In International Conference on Machine Learning, pages 4466–4474. PMLR, 2019.
[42] Kurt Miller, Michael Jordan, and Thomas Griffiths. Nonparametric latent feature models for
link prediction. In Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, editors,
Advances in Neural Information Processing Systems, volume 22. Curran Associates, Inc., 2009.
13[43] Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv
preprint arXiv:1610.03483, 2016.
[44] MortenMørup, MikkelNSchmidt, andLarsKaiHansen. Infinitemultiplemembershiprelational
modeling for complex networks. In 2011 IEEE International Workshop on Machine Learning for
Signal Processing, pages 1–6. IEEE, 2011.
[45] MarkEJNewman.Thestructureandfunctionofcomplexnetworks.SIAMReview,45(2):167–256,
2003.
[46] Krzysztof Nowicki and Tom A B Snijders. Estimation and prediction for stochastic blockstruc-
tures. Journal of the American statistical association, 96(455):1077–1087, 2001.
[47] Krzysztof Nowicki and Tom A.B. Snijders. Estimation and prediction for stochastic blockstruc-
tures. Journal of the American Statistical Association, 96(455):1077–1087, 2001.
[48] KonstantinaPalla,DavidA.Knowles,andZoubinGhahramani. Aninfinitelatentattributemodel
for network data. In Proceedings of the 29th International Coference on International Conference
on Machine Learning, ICML’12, page 395–402, Madison, WI, USA, 2012. Omnipress.
[49] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for
anomaly detection: A review. ACM computing surveys (CSUR), 54(2):1–38, 2021.
[50] Judea Pearl. Reverend bayes on inference engines: A distributed hierarchical approach. In
Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 133–136. AAAI
Press, 1982.
[51] W. W. Peterson, T. G. Birdsall, and W. C. Fox. The theory of signal detectability. Transactions
of the IRE Professional Group on Information Theory, 4(4):171–212, 1954.
[52] HezheQiaoandGuansongPang. Truncatedaffinitymaximization: One-classhomophilymodeling
for graph anomaly detection. Advances in Neural Information Processing Systems, 36, 2024.
[53] Hezhe Qiao, Qingsong Wen, Xiaoli Li, Ee-Peng Lim, and Guansong Pang. Generative semi-
supervised graph anomaly detection. arXiv preprint arXiv:2402.11887, 2024.
[54] A Radford. Unsupervised representation learning with deep convolutional generative adversarial
networks. arXiv preprint arXiv:1511.06434, 2015.
[55] Bidisha Samanta, Abir De, Gourhari Jana, Vicena§ Gamez, Pratim Chattaraj, Niloy Ganguly,
and ManuelGomez-Rodriguez. Nevae: A deep generativemodel for moleculargraphs. Journal of
Machine Learning Research, 21(114):1–33, 2020.
[56] OleksandrShchurandStephanGu¨nnemann. Overlappingcommunitydetectionwithgraphneural
networks. arXiv preprint arXiv:1909.12201, 2019.
[57] Tom A.B. Snijders and Krzysztof Nowicki. Estimation and prediction for stochastic blockmodels
for graphs with latent block structure. Journal of Classification, 14(1):75–100, 1997.
[58] Fan-Yun Sun, Meng Qu, Jordan Hoffmann, Chin-Wei Huang, and Jian Tang. vgraph: A genera-
tive model for joint community detection and node representation learning. Advances in Neural
Information Processing Systems, 32, 2019.
[59] Michael Tschannen, Olivier Bachem, and Mario Lucic. Recent advances in autoencoder-based
representation learning. arXiv preprint arXiv:1812.05069, 2018.
[60] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua
Bengio, et al. Graph attention networks. stat, 1050(20):10–48550, 2017.
[61] Hongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie,
and Minyi Guo. Graphgan: Graph representation learning with generative adversarial nets. In
Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.
14[62] Xuhong Wang, Baihong Jin, Ying Du, Ping Cui, Yingshui Tan, and Yupu Yang. One-class graph
neuralnetworksforanomalydetectioninattributednetworks. Neuralcomputingandapplications,
33:12073–12085, 2021.
[63] JaewonYangandJureLeskovec.Structureandoverlapsofcommunitiesinnetworks.InSNAKDD,
2012.
[64] JaewonYangandJureLeskovec. Overlappingcommunitydetectionatscale: anonnegativematrix
factorization approach. In Proceedings of the sixth ACM international conference on Web search
and data mining, pages 587–596, 2013.
[65] JaewonYangandJureLeskovec. Structureandoverlapsofground-truthcommunitiesinnetworks.
ACM Transactions on Intelligent Systems and Technology (TIST), 5(2):1–35, 2014.
[66] JiaxuanYou,RexYing,XiangRen,WilliamHamilton,andJureLeskovec. Graphrnn: Generating
realisticgraphswithdeepauto-regressivemodels.InInternationalconferenceonmachinelearning,
pages 5708–5717. PMLR, 2018.
[67] M. Zhou. Infinite edge partition models for overlapping community detection and link predic-
tion. Proceedings of the 18th International Conference on Artificial Intelligence and Statistics
(AISTATS), pages 1135–1143, 2015.
Appendix
A Proofs
A.1 Proof That BigClam Is Not Universal
In this subsection we Prove Claim 9. Consider the bipartite graph B with N nodes at each part, and
probability 1−e−a2 for an edge between the two parts, and 0 within each part. Consider (17) as the
definition of the log cut distance. Let P be a decoded BigClam graph from the affiliation features F.
Denote by P˜ the matrix with entries
p˜ =−log(1−p )=f⊤f ,
n,m n,m n m
and by B˜ the matrix with entries ˜b = −log(1−b ). We show that there is no way to make
n,m n,m
∥P˜ −Q˜∥□ small.
Claim 12. Under the above construction,
a2
D0(P||B)≥ .
□
16
As a result, BigClam is not a universal autoencoder.
Proof. Note that ˜b = a2 if n,m are in opposite parts, and ˜b = 0 if n,m are on the same side.
n,m n,m
We index the nodes such that [N] is the first side of the graph, and [N]+N is the second side. For
n∈[N] we denote q =f and y =f . Next, we use the identity
n n n n+N
D □0(P||Q)=∥P˜−B˜∥□,
and bound the right-hand-side from below.
First, by the definition of cut norm, for every U,V ⊂[2N],
∥P˜−B˜∥□ ≥(cid:12) (cid:12) (cid:12)4N1
2
(cid:88) (cid:88) (p˜ n,m−˜b n,m)(cid:12) (cid:12) (cid:12).
n∈Um∈V
Hence, for U = V = [N], U = V = [N]+1, U = [N],V = [N]+N, and U = [N]+N,V = [N],
1 1 2 2 3 3 4 4
we have
∥P˜−B˜∥□ ≥
151 (cid:88)4 (cid:12) (cid:12) (cid:88) (cid:88) (p˜ −˜b )(cid:12) (cid:12)
16N2 (cid:12) n,m n,m (cid:12)
j=1 n∈Ujn∈Vj
N N N N
1 (cid:88) (cid:88) 1 (cid:88) (cid:88)
= q⊤q + y⊤y
16N2 n m 16N2 n m
n=1m=1 n=1m=1
N N
1 (cid:88) (cid:88)
+ (a2−q⊤y ). (18)
8N2 n m
n=1m=1
Denote
N N
1 (cid:88) 1 (cid:88)
q= q , y= y .
N n N n
n=1 n=1
With these notations, (18) can be written as
∥P˜−B˜∥□ ≥
1 1 1
q⊤q+ y⊤y+ (a2−q⊤y)
16 16 8
1 (cid:16) (cid:17) a2
= a2+(q⊤−y⊤)(q−y) ≥ .
16 16
We note that one can similarly show that BigClam is not universal also with respect to the log cut
distance of Definition 7.
A.2 BigClam With No Self Loops Approximating Bipartite Graphs
Consider the above bipartite graph B with N nodes at each part, and probability 1−e−a2 for an
edge between the two parts, and 0 within each part. If we redefine the BigClam decoder to have no
self-loops, namely, P has entries p = P(n ∼ m|f ,f ) for n ̸= m, and p = 0 for n = m, then
n,m n m n,m
one can obtain a bipartite P with C =N2 communities as follows.
In the following analysis, an addition or multiplication of a set by a scalar is defined to be the
additionormultiplicationofeveryelementinthesetbythisscalar. Encodeeachnoden∈[N]inpart
1 to f with fc = a for c ∈ [N]+n(N −1) and fc = 0 otherwise. Encode every node n ∈ [N]+N
n n n
from side 2 to f with fc =a for c∈N([N]−1)+n and fc =0 otherwise. It is easy to see that the
n n n
corresponding P is bipartite with edge probability between the parts being 1−e−a2.
A.3 Proof of the Universality of PieClam
The proof is based on a version of the weak regularity lemma for intersecting communities. While
the standard weak regularity lemma [17, 38] partitions the graph into disjoint communities, it is well
known that allowing the communities to overlap allows using much less communities, which improves
theasymptoticsoftheapproximation. Theregularitylemmawasusedinthecontextofgraphmachine
learning in [35, 15]. To formalize the relevant version of the weak regularity theorem for our analysis,
we first need to cite a definition from [15].
Definition 13. A (hard) intersecting community graph (ICG) with N nodes and K communities is
a matrix C ∈ RN×N of the following form. There exist K column vectors Q = (cid:0) q ∈ {0,1}N(cid:1) )K ∈
k k=1
{0,1}N×K and k coefficients r=(r ∈R)K ∈RK such that
k k=1
C=Qdiag(r)Q⊤.
Thefollowingisaspecialcaseoftheweakregularitylemmafrom[15],uptothesmallmodification
to the adjacency matrix, allowing it to have values in [0,R] instead of [0,1].
16Theorem 14. Let A∈[0,R]N×N be an adjacency matrix of a graph with N nodes. Let ϵ>0. Denote
K = 9R2. Then, there exists a hard ICG C with K communities such that
4ϵ2
∥A−C∥□ ≤ϵ. (19)
Proof of Theorem 10. Let ϵ>0. Let A∈[0,1]N×N be an adjacency matrix. Let 0<d≤1.
Consider the matrix A˜ with entries
a˜ =−log(1−(1−d)a ).
n,m n,m
In the following construction, we build IeClam affiliation features F and we want
1−exp(−f⊤Lf )≈(1−d)a .
n m n,m
Note that −log(1−(1−d)a ) is increasing in (1−d)a . For (1−d)a = 0 the value of this
n,m n,m n,m
function is 0, and for (1−d)a =1−d it is R=−log(d). Choose d=ϵ/2. For this specific choice
n,m
of d, if we replace d by ϵ/2 in the definition of D□ and omit the infimum, we get an upper bound of
D□(P,A).
Using the overlaping weak regularity lemma, we approximate A˜ by an ICG with
−9log(ϵ/2)2
K =
ϵ2
communities,
C=Qdiag(r)Q⊤,
such that
∥A˜ −C∥□ ≤ϵ/2.
Let r =ReLU(r) and r =ReLU(−r). Denote
+ −
√
C =Qdiag( r )
+ +
and
√
C =Qdiag( r ).
− −
Denote the rows of C by t and the rows of C by s , for n = 1,...,N. For each n ∈ [N] we
+ n + n
concatenate(t ,s )todefinetheaffiliationfeaturef ,withthefirstK coordinatesbeingtheinclusive
n n n
communities, and the last K coordinates being the exclusive communities. Denote the corresponding
IeClam matrix by P.
It is easy to see that f⊤Lf is the (n,m) entry of Qdiag(r)Q⊤. This also proves that
n m
∥−log(1−(1−ϵ)A)+log(1−P)∥□
=∥A˜ −C∥□ ≤ϵ/2.
We can now summarize
D□(P||A)≤
ϵ/2+ 1 sup (cid:12) (cid:12)log(cid:16) (cid:89) (cid:89) 1−p n,m (cid:17)(cid:12) (cid:12)
N2 (cid:12) 1−(1−ϵ/2)a (cid:12)
U,V⊂[N] n,m
n∈Um∈V
=ϵ/2+
1
sup
(cid:12) (cid:12)(cid:88) (cid:88) (cid:16) −log(cid:0)
1−(1−ϵ)a
(cid:1)
N2 (cid:12) n,m
U,V⊂[N]
n∈Um∈V
+log(cid:0) 1−p
)(cid:1)(cid:17)(cid:12)
(cid:12)
n,m (cid:12)
=ϵ/2+∥P˜ −C∥□ =ϵ.
17A.4 Proof of the Universality of IeClam in the Pairwise Cone of Non-
negativity
For this result, we use the standard weak regularity lemma for non-intersecting classes. It is based on
the weak regularity lemma from [17, 38], see also Lemma 9.3 and Corollary 9.13 from [37].
Definition 15. A block matrix B with K classes is a symmetric matrix B ∈ [0,∞)N×N for which
there exists a partition of [N] into K disjoint sets, called classes, C ,...,C (with ∪C = [N]), such
1 K j
that for every pair of classes i,j ∈ [K], there is a constant c ≥ 0 such that b = c for any two
i,j n,m i,j
nodes n∈C and m∈C .
i j
Theorem 16. Let A∈[0,R]N×N be an adjacency matrix of a graph with N nodes. Let ϵ>0. Denote
K =22⌈R2/ϵ2⌉. Then, there exists a block matrix B with K (disjoint) classes such that
∥A−B∥□ ≤ϵ. (20)
We stress that Theorem 16 guarantees non-negative block values of B, while in Theorem 14, in
general, the matrix C may have negative entries.
Proof of Theorem 11. We start similarly to the proof of Theorem 10. Let ϵ > 0. Let A ∈ [0,1]N×N
be an adjacency matrix. Consider the matrix A˜ ∈[0,−log(ϵ/2)] with entries
a˜ =−log(1−(1−ϵ/2)a ).
n,m n,m
In the following, we build IeClam affiliation features F such that
1−exp(−f⊤Lf )≈(1−ϵ/2)a .
n m n,m
By the weak regularity lemma (Theorem 16), we approximate there is a non-negative block matrix
B with K =22⌈−log(ϵ/2)2/ϵ2⌉ classes C ,...,C , such that
1 K
∥A˜ −B∥□ ≤ϵ.
We now take the affiliation space to have C = K2 inclusive communities and C = K2 exclusive
communities.
For each n ∈ [N], let k be the class such that c ∈ C . For each pair of classes i,j ∈ [K], let
n nk
c ≥0 denote the edge weight between C and C .
i,j i j √
The feature of each n ∈ [N] at the inclusive channel c = (K −1)k +k is tc = c . It is
n n n kn,kn
(cid:112) (cid:112)
tc = c /4 at inclusive channels c=(K−1)k +j and c=(K−1)j+k , and sc =− c /4 at
n kn,j n n n kn,j
exclusive channels c=(K−1)k +j and c=(K−1)j+k . In all other channels tc and sc are zero.
n n n n
Note that f belongs to the cone of pairwise non-negativity T ⊂R2K2.
n
It is now direct to see that f ⊤Lf =c . As a result, as in the proof of Theorem 10, we get
n m kn,km
D□(P||A)≤
ϵ/2+ 1 sup (cid:12) (cid:12)log(cid:16) (cid:89) (cid:89) 1−p n,m (cid:17)(cid:12) (cid:12)
N2 (cid:12) 1−(1−ϵ/2)a (cid:12)
U,V⊂[N] n,m
n∈Um∈V
=ϵ/2+
1
sup
(cid:12) (cid:12)(cid:88) (cid:88) (cid:16) −log(cid:0)
1−(1−ϵ)a
(cid:1)
N2 (cid:12) n,m
U,V⊂[N]
n∈Um∈V
+log(cid:0) 1−p
)(cid:1)(cid:17)(cid:12)
(cid:12)
n,m (cid:12)
=ϵ/2+∥P˜ −B∥□ =ϵ.
18B Extended Related Work
B.1 Message Passing Algorithms and Networks
The message passing algorithm is a general architecture for processing graph-signals. An MPNN
operates on the graph data by aggregating the features in the neighborhood of each node, allowing
for information to travel along the edges. The first example of this scheme (Message Passing) was
originally suggested by Pearl et al [50], and was combined with a neural network in [13], and later
generalized in [19]. Most graph neural networks applied in practice are specific instances of MPNNs
[18, 31, 60]. In MPNNs, information is exchanged between nodes along the graph’s edges. Each
node combines the incoming messages from its neighbors using an aggregation scheme, with common
methods being summing, averaging, or taking the coordinate-wise maximum of the messages. Let
T ∈Nrepresentthenumberoflayers, anddefinetwosequencesofpositiveintegers(c )T and(d )T
t t=0 t t=0
representing the feature dimensions in the hidden layers {G t}T
t=0
={Rct}T
t=0
and {S t}T
t=0
={Rdt}T t=0.
Define the message functions as Mt :S ×S →G and unpdate functions as Ut :G ×S →S . The
t t t t t t+1
features ft+1 ∈S at layer t+1 of the nodes n∈[N] are computed from the features ft ∈S by
n t+1 m t
(cid:88)
mt = Mt(ft,ft)
n n k
k∈N(n)
ft+1 =Ut(mt,ft).
n n n
Here, Mt, Ut and mt are called the message function, update function and mail at time t re-
spectively. The summation over the messages can also be replaced for any node by any function
Aggt :(cid:81) G →G which is permutation invariant.
n |N(n)| t t
AtstepT areadoutspacecanbedefinedR
T
=RbT,withapermutationinvariantreadoutfunction
RT, e.g., summing, averaging, or taking the max of all of the nodes of the graph. This produces a
vector representation of the whole graph.
B.2 Deep Generative Models
Generative models in machine learning assume that training data is generated by some underlying
probability distribution. One goal in this context is to approximate this distribution, or build a model
that approximates a random sampler of data points from this distribution. Hence, generative models
can be used to generate synthetic data, mimicking training data by sampling from the distribution
[25, 12, 43], or to infer the probability of unseen data by substituting it into the probability function.
The latter can be useful in tasks like anomaly detection [49] in which the model can asses whether
a sample is probable under the learned model. Two examples of generative models are Generative
AdversarialNetworks(GANs)[20,54]andVariationalAutoencoders(VAEs)[29],whichareusedboth
for inference and generation.
VAEmodelsconsistofanencoderandadecoder. Theencodermapsdatafromahigh-dimensional
dataspace toalower-dimensional,simpler,codespace. Thedecoderreversesthisprocess,transforming
data from the code space back into the data space. The code space serves as a bottleneck, capturing
theessentialfeaturesofthetrainingdata,whichisoftenhigh-dimensional(e.g.,images,socialnetwork
graphs) and thus more complex than the code space. If the model trains by encoding followed by
decoding, then minimizing the difference between the input and its decoded version, it is called an
autoencoder.
In a VAE, training involves encoding each data point to a known distribution (typically Gaus-
sian), sampling from this distribution, decoding it, and then minimizing the difference between the
distribution of the original data and the decoded data. For a survey on VAEs, see [59].
B.3 Graph Generative Models
Graph generative models learn a probability distributions of graphs. Such models allow for various
tasks where the goal is not only to analyze existing graphs but also to predict or simulate new graph
data.
Someclassicalgenerativemodelsarepre-definedprobabilisticmodels,e.g.,theErd˝os–R´enyimodel,
Preferential attachment, Watts–Strogatz model, and more. See [45] for a review. Other graph gen-
erative models are learned from data, e.g., [34, 30, 66, 3]. Applications of generative graph models
19include social network analysis [61, 22, 25]. anomaly detection B.6, graph synthesis [66, 23], data
augmentation [11], and protein interaction modeling (e.g. for drug manufacturing) [8, 27], to name a
few. For a review see [39].
Deep Graph Autoencoders. In graph deep learning-based autoencoders, one estimates the data
distribution by learning to embed the nodes to a code space, in which the data distribution is defined
to be some standard distribution, e.g., Gaussian, in such a way that the encoded nodes can be recun-
structed back to the graph with small error. Graph VAEs [30, 22, 55, 41] embed the data into the
code space by minimizing the evidence lower bound loss comprised of the decoding loss and the KL
divergence between the encoded distribution and a Gaussian prior.
GAN-based Graph generative models. In [61], a GAN method for graphs generates a neigh-
borhood for each of the nodes and the discriminator gives a probability score for each edge. This
methodalsoformulateagraphversionofsoftmax, andofferarandomwalkbasedgeneratingstrategy.
Another GAN model that is used for anomaly detection is GAAN [7]. In GAAN, the ground truth
and generated node attributes are encoded into a latent space from which the adjacency between any
two nodes is decoded using a sigmoid of the inner product between their latent features.
Normalizing Flows-based Graph Models. Normalizing flows models [12] also have adaptations
for graph data. For example, the work of [36, 40] offers a version of coupling blocks that use message
passing neural networks. See more details on normalizing flows in Section B.7.
B.4 Stochastic block models
A stochastic block model (SBM) is a generative model for random graphs. A basic stochastic block
modelisdefinedbyspecifyinganumberofclasses K,theprobabilityp ofarandomnodebeinginblock
k
k, for k ∈ [K], where (cid:80) p = 1, and an array of values C = {c }J ∈ [0,1]J×K indicating edge
k k k,l k,l=1
probabilities between classes. Each node of a randomly generated graph of N nodes is independently
chosen to belong to one of the classes at random, with probabilities {p } . Then, the edges of
k k∈[K]
the graph are chosen independently at random according to the following rule. For each n ∈ [N],
denote by k ∈ [K] the class of n. Each dyad (n,m) ∈ [N]2 is chosen to be an edge in probability
n
c . Namely,theentriesa oftheadjacencymatrixoftherandomgraphareindependentrandom
kn,km n,m
Bernoulli variables. See [34] for a review on SBMs.
For each node n, denote by f ∈{0,1}K the vector such that fc =1 if and only if k =c. Hence,
n n n
in a basic SBM, the presence of an edge between nodes n and m follows a Bernoulli distribution
with parameter P(n ∼ m|f ,f ,C) = f ⊤Cf , where the adjacency matrix is P = FCF⊤, where
m m n m
F ∈ RN×K is the matrix where each row n has the feature f [46]. This model can be extended to
n
intersecting classes, where now f can have more than one nonzero entry [44, 42, 48].
n
In a Bernouli-Poisson SBM [64, 67, 56], the probability for a non-edge is modeled by a Poisson
distribution. The idea is that the more classes n and m share, the higher the probability that there is
an edge between them. Hence,
P(n∼m|f ,f
,C)=1−e−fn⊤Cfm,
(21)
m m
with the expected number of edges being f ⊤Cf .
n m
In both the Bernouli and Bernouli-Poisson models, the probabilisic model of the entire graph is
given by a product of the probabilities of all of the events
P(E|F,C)=
(cid:115) (cid:89) (cid:16) (cid:89) (cid:89) (cid:17)
P(n∼m) P(¬(n∼m)) .
n∈[N] m∈N(n) m∈/N(n)
Here, the square root is taken since we assume that the graph is undirected so the product goes over
all of the edges twice [1, 44].
When fitting an SBM to a graph, both the class affiliations of nodes and the block structure C are
learned [57, 47, 33].
20B.5 Community Affiliation Models
Community detection is a fundamental task in network analysis, aiming to identify groups of nodes
that are more densely connected internally than with the rest of the network. This process is useful
for understanding the structure and function of complex networks, such as social, biological, and
information networks [16].
Although there exist models in which each node belongs to only one community as in traditional
SBM models [26], and some relatively new deep learning models such as [5], it was shown that real-
world networks often exhibit overlapping communities [65, 64], where nodes belong to multiple groups
and the probability of connectivity increases the more communities two nodes share. This indeed
makes intuitive sense when looking at, e.g., social networks, where the more common interests and
social circles people share the more they are likely to connect.
An example of a community affiliation model that can generate new graphs is AGM [63], which
classifiesallofthenodesinagraphintoseveralcommunities,whereeachcommunitykhasaprobability
p fortwomembernodestoconnect. IfwedenotebyC thesetofcommunitiestwonodesn,m∈[N]
k n,m
share, then the probability of an edge between n and m is
(cid:89)
p(n∼m)=1− (1−p ).
k
k∈Cnm
To sample a new graph from a trained model, nodes are sampled and assigned communities based on
therelativesizesofcommunitiesinthetraininggraph, andedgesareconnectedbasedontheirmutual
community memberships.
Community affiliations can also be continuous, where nodes have varying degrees of membership
in multiple communities. Community Affiliation models with continuous communities include Mixed
Membership SBM (MMSBM) [2] which is an SBM which allows nodes to have mixed memberships
in multiple communities, and BigClam [64], which is a Bernouli Poisson model model that scales
efficiently for sparse graphs.
ItisalsoworthmentioningtheNOCDmodelpresentedin[56]whichusestheBernouliPoissonloss,
but embeds the nodes of the graph into the code space using a learned GNN. Another related paper
is [58], which models the features and communities separately, learning latent features from which it
estimates the community affiliation.
While community affiliations are typically non-negative, there are models where affiliations can be
negative. An example is the Signed Stochastic Block Model (SSBM) [28].
B.6 Anomaly Detection
Anomaly detection in graphs aims to identify nodes or subgraphs that deviate significantly from the
typical patterns within the graph. This is useful in various applications such as network security,
fraud detection, and social network analysis. In the unsupervised formulation of the problem, there
is no labeled data in the training process, We highlight five works in this direction. GAAN [7] and
AEGIS [9] use a generative adversarial approach, training a discriminator to distinguish real and fake
nodes. Dominant [10] and AnomalyDAE [14] identify anomalies via reconstruction errors of a graph
autoencoder.
Another work is [53], which considers a semi-supervised setting. Here, there is a relatively small
numberofavailablelabelednormalnodesduringtraining(nodesthatareknowntonotbeanomalies),
and the goal is to predict the anomalies in the unknown nodes.
B.7 Normalizing Flows
Normalizing flows are deep learning algorithms that estimate probability distributions, and allow
an efficient sampling from this distribution. They do so by constructing an invertible coordinate
transformation between the unknown target probability space and a standard probability space with
a well known distribution (e.g. Gaussian). This transformation is modeled as a deep neural network,
composed of a series of basic transformations called flows.
21Notations. Let F represent the space of the target data, with an unknown probability density
function p :F →[0,∞). We denote the elements of F by f.
F
Let Z represent a latent space with a known probability density funcion p : Z → [0,∞). We
Z
denote the elements of Z by z. The density function p is often chosen to be a standard isotropic
Z
Gaussian with zero mean and identity covariance, denoted by N(0,I).
Goal. ThegoalinnormalizingflowsistolearnaninvertibletransformationT :F →Z (parameter-
θ
ized by θ) which maps the target space F to the latent space Z, and preserves probabilities. Namely,
T should satisfy: for every measurable subset F ⊂F we have
θ
(cid:90) (cid:90)
T (f)p (f)df = p (z)dz.
θ F Z
F Tθ(F)
Here T (F) = {z ∈ Z | ∃f ∈ F such that T (f) = z}. Such a transformation can be seen as a change
θ θ
of variable.
Density Transformation. Given an invertible transformation T , the target density p (f) can be
θ F
expressed in terms of the latent density p (z). By upholding the constraint that either probability
Z
density function has integral 1 over their respective spaces, one can deduce the change of variable
formula
(cid:12) (cid:18) (cid:19)(cid:12)
p F(f)=p Z(T θ(f))(cid:12) (cid:12) (cid:12)det ∂T ∂θ f(f) (cid:12) (cid:12) (cid:12).
Here, ∂Tθ(f) denotestheJacobianmatrixofthetransformationT withrespecttoFanddet(·)denotes
∂f θ
its determinant.
SequentialCompositionofFlows. Inpractice,thetransformationT ismodeledasacomposition
θ
of L ∈ N simpler invertible transformations {T }L between the consecutive spaces {Zi} where
θi i=1
ZL = F and Z1 = Z in the previous notations. The invertible mappings T : Zi → Zi−1 are called
θi
flows, and we have
T =T ◦T ◦···◦T
θ θL θL−1 θ1
where ◦ denotes function composition. The overall density function for f then becomes
p F(f)=p Z(T
θ(f))(cid:89)L (cid:12) (cid:12)
(cid:12)
(cid:12)det(cid:18) ∂T ∂θi f(f i)(cid:19)(cid:12) (cid:12)
(cid:12) (cid:12).
i
i=1
Here,f istheintermediaterepresentationafterapplyingthefirsti−1flows. Thealgorithmisoptimized
i
by maximum log likelihood, namely, by maximizing
log(cid:0)
p
F(f)(cid:1) =log(cid:0)
p Z(T
θ(f))(cid:1) +(cid:88)L log(cid:32)(cid:12) (cid:12)
(cid:12)
(cid:12)det(cid:18) ∂T ∂θi f(f i)(cid:19)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:33)
,
i
i=1
using gradient descent.
Since T and every Ti are invertible, information can also flow in the opposite direction in order
θ θ
to sample data. First, a point z is sampled z ∼ p and the inverse transformation T−1 is applied to
Z θ
map the sample z into the data space F. Since T is composed of a series of flows, the generated point
in the data space is
f =T−1(z)=T−1◦T−1◦···◦T−1(z).
θ θ1 θ2 θL
RealNVP. One popular method of implementing normalizing flow is the Real Valued Non-Volume
Preserving (RealNVP). In this model, the flows T are modeled as mappings called coupling blocks.
θi
In a coupling block, the input vector fi (or zi, in the generative direction) is split into two vectors: fi
A
and fi, each the same dimension. Namely, fi =(fi,fi). The flow T is then defined to be
B A B θi
fi−1 =fi (22)
A A
fi−1 =fi ⊙s (fi)+t (fi) (23)
B B θi A θi A
22Where (s (·),t (·)) are Multi Layer Perceptrons (MLPs) parameterized by θi, and ⊙ is elementwise
θi θi
product. In addition, the elements of fi are permuted at every i before applying the coupling block,
using predefined permutations, that are parts of the hyperparameters of the model. It is easy to see
(cid:18) (cid:19)
that the Jacobian ∂T ∂θi f( ifi) is a diagonal matrix with 1 for the first dim(f Ai) elements, and s θi(f Ai) for
the last dim(fi) elements. This makes the log of the determinant of the Jacobian
B
log(cid:32)(cid:12) (cid:12) (cid:12) (cid:12)det(cid:18) ∂T ∂θi f(f i)(cid:19)(cid:12) (cid:12) (cid:12) (cid:12)(cid:33) =dim (cid:88)(f Bi) log(cid:16) s θi(f Ai) j(cid:17) . (24)
i
j=0
Hence, this determinant is easily computed in practice.
For generation, the inverse transformation can be calculated easily as
zi+1 =zi
A A
zi −t(zi )
zi+1 = B A
B s(zi )
A
wherethedivisioniselementwise. Here,thelogdeterminantoftheJacobiancanbecalculatedsimilarly
to (24), applied to 1/s(zi ).
A
B.8 The Lorentz Inner Product
The Lorentz inner product is a bilinear form used in the context of special relativity to describe the
spacetime structure. In a four-dimensional spacetime, the Lorentz inner product between two vectors
v and w is given by:
⟨v,w⟩=v w −v w −v w −v w
0 0 1 1 2 2 3 3
Here,v andw representthe”time”components,whilev ,v ,v andw ,w ,w representthe”spatial”
0 0 1 2 3 1 2 3
components. ThenegativesigninfrontofthetimecomponentiswhatdistinguishestheLorentzinner
product from the standard Euclidean inner product, making it suitable for modeling the geometry of
spacetime where time and space are treated differently. Note that due to the subtraction, the Lorenz
inner product is in fact not an inner product as it is not positive definite. In the context of special
relativity, the points for which the Lorenz product remain positive define the so called ”light cone”
structure of spacetime, separating events into those that are causally connected and those that are
not.
The Lorentz inner product is a specific example of a broader class of inner products known as
pseudo-Euclidean inner products. In a pseudo-Euclidean space, the inner product can have
a mixture of positive and negative signs, leading to different geometric properties. These spaces
generalize the concept of Euclidean space by allowing for non-positive definite metrics.
C PClam and PieClam as Graphons
A graphon is a model which can be seen as a graph generative model that extends SBMs. A graphon
[4, 37] can be seen as a weighted graph with a “continuous” node set [0,1].
Definition 17. The space of graphons W is defined to be the set of all measurable function W :
[0,1]2 →[0,1] which are symmetric, namely W(x,y)=W(y,x).
The edge weight W(x,y) of a graphon W ∈ W can be seen as the probability of having an edge
betweennodexandnodey. GivenagraphonW,arandomgraphisgeneratedbysamplingindependent
uniform nodes {X } from the graphon domain [0,1], and connecting each pair X ,X in probability
n n m
W(X ,X ) to obtain the edges of the graph.
n m
We next show that Clam models with prior are special cases of graphon models. Note that the
prior p defines a standard atomless probability spaces over the community affiliation space RK. Since
allstandardatomelessprobabilityspacesareequivalent,thereisaprobabilitypreservinga.e. bijection
ξ : [0,1] → RK that maps the prior probability to the uniform probability over [0,1]. Now, the
p
PieClam model for generating a graph of N nodes can be written as follows.
23• Sample N points {X }N ⊂ [0,1] uniformly. Observe that {ξ (X )}N ⊂ RK are indepndent
n n=1 p n n=1
samples via the probability density p.
• Connect the points according to the BigClam of IeClam models P(n ∼ m|ξ (X ),ξ (X )) (1)
p n p m
or (7).
This shows that PClam and PieClam coincide with the generative graphon model W(x,y) = P(n ∼
m|ξ (X),ξ (Y)) where P is defined either by (1) or by (7).
p p
D Extended Details on Experiments
All of the experiments were run on Nvidia GeForce RTX 4090 and Nvidia L40 GPUs. Our code can
be found in:
https://anonymous.4open.science/r/PieClam-49C4
D.1 Additional Architecture Details in PieClam and PClam
Added Affiliation Noise. When training the prior, overfitting may cause the probability to spike
aroundcertainareasintheaffiliationspace, e.g., aroundtheaffiliationfeaturesofthenodes. Toavoid
thisissue,weaddgaussiannoisetotheaffiliationvectorasaregularizationateachstepoftrainingthe
normalizingflowpriormodel. Thenormalizingflowmodelthentransformsthesamepointtoaslightly
differentlocationinthecodespace. Thenoiseoptimizationthereforeprovidesaresolutiontotheprior,
smoothingthedistributionintheaffiliationspace. Noiseadditionistheprimaryregularizationmethod
we have used when training the prior in all of our experiments.
Densification. For very sparse datasets, Clam models may not behave well. In such cases, the
community structure is sometimes unstable, where the same node can find itself in different com-
munities based on slightly different initial conditions. In order to strengthen the connections within
communities, we apply a two-hop densification scheme on very sparse graphs. Namely, we connect
two disjoint nodes n,m with an edge if there is a third node k for which n ∼ k and m ∼ k. We find
that this scheme improves anomaly detection on Elliptic, Photo and Reddit datasets. Densification
may strengthen the community structure in some datasets, but can also destroy it in others. In the
experiments on synthetic datasets we did not use densification, as these graphs are relatively dense.
D.2 Anomaly detection
Metric. In order to measure the accuracy of the anomaly detection classification we use the area
undertheReceiverOperatingCharacteristic(ROC)curve[51]. Thecurveisaplotofthetruepositive
rate(TPR)againstthefalsepositiverate(FPR)fortherangeofthethresholdvaluesbetweenthevalue
that classifies all samples as true and the value that classifies all as false. The area under the curve
signifies the general tendency of the curve toward the point (0,1) for which FPR=0 and TPR=1.
Additional experiment. We add additional experiments on anomaly detection using PieClam,
where we averaged the accuracy over 10 rounds and included error bars.
In this setup we compose three iteration steps of the form F-500 → p-1300 with initial learning
ratesof3e−6forthefeaturesand2e−6fortheprior,andinitialnoiseamplitudeof0.05. Thelearning
rates and noise amplitude were halved after every F→p iteration. The results are in Table 2.
Furthermore, in this section we also run our anomaly detection methods without using the node
features, namely, only using the graph structure. We find that we comparable results on Photo and
Reddit with or without node features, but that the detection on Elliptic decreases significantly. Still,
anomaly detection on Elliptic without node features is competitive with state of the art models that
do use node features. The results are in Table 3.
We proceed to compare the optimization that includes the node features to the optimization that
didn’t. The comparison is presented in
24Method Reddit Elliptic Photo
(S)- IeClam 0.639 0.440 0.592
(S) - PieClam 0.6320 ± 0.0054 0.4354 ± 0.0005 *0.5727 ± 0.0151
(P) - PieClam 0.5089 ± 0.0219 0.6201 ± 0.0176 0.4252 ± 0.0074
(PS) - PieClam *0.6329 ± 0.0052 0.5294 ± 0.0089 0.5289 ± 0.0127
(S) - BigClam 0.637 0.434 0.581
DOMINANT 0.511 0.296 0.514
AnomalyDAE 0.509 *0.496 0.507
OCGNN 0.525 0.258 0.531
AEGIS 0.535 0.455 0.552
GAAN 0.522 0.259 0.430
TAM 0.606 0.404 0.568
Table 2: Comparison of Clam anomaly detectors with competing methods. First place in boldface,
second with underline, third with *star. We observe that our methods are first place on all datasets.
Moreover, S- IeClam, PS-PieClam and S BigClam each beats the competing methods in two out of
the three datasets . The accuracy metric is areas under curve (AUC).
Method Reddit Elliptic Photo
(S) - PieClam 0.6320 ± 0.0054 0.4354 ± 0.0005 0.5727 ± 0.0151
(P) - PieClam 0.5089 ± 0.0219 0.6201 ± 0.0176 0.4252 ± 0.0074
(PS) - PieClam *0.6329 ± 0.0052 0.5294 ± 0.0089 0.5289 ± 0.0127
(S) - PieClam (No Attr) 0.6346 ± 0.0015 0.4349 ± 0.0009 *0.5696 ± 0.0065
(P) - PieClam (No Attr) 0.4530 ± 0.0142 *0.4826 ± 0.0134 0.4977 ± 0.0747
(PS) - PieClam (No Attr) 0.6351 ± 0.0015 0.4349 ± 0.0009 0.5697 ± 0.0065
Table 3: Comparison of attributed and unattributed PieClam anomaly detection.
D.2.1 T-Test Comparison Summary.
Weconductedt-teststodetermineifnodefeatures/attributes(”WithFeatures”)significantlyimprove
performance compared to not using attributes (”Without Features”), across the Reddit, Elliptic, and
Photo datasets for each method (S, P, PS). Even though S method is not affected directly by the
affiliation vectors, the latter can affect the convergence of the prior in the affiliation space and have
an indirect effect.
• Reddit Dataset: Attributes did not significantly improve performance for the ”Star” (S) and
”Prior Star” (PS) methods. However, ”Prior” (P) did show a significant improvement with
attributes (p-value=0.0002).
• Elliptic Dataset: ”Prior” (P) and ”Prior Star” (PS) both showed significant improvements
with attributes (p-value<0.0001). However, the ”Star” (S) method did not significantly benefit
from attributes.
• Photo Dataset: The ”Star” (S) method did not show significant difference (p-value=0.6864).
The ”Prior Star” (PS) method, however, did show a significant benefit from attributes (p-
value<0.0001), and ”Prior” (P) had a borderline significant result (p-value=0.0529).
In conclusion, attributes did not make a difference for the ”Star” (S) method across all datasets
(as would be expected), and for the ”Prior Star” (PS) method in the Reddit dataset. However, the
”Prior” (P) and ”Prior Star” (PS) methods mostly showed improvement in the Elliptic dataset.
D.3 SBM Reconstruction And Distance Convergence
We extend the experiment in Section 4.2 by considering another synthetic SBM, which is off-diagonal
dominant. As in Section 4.2, we sample a simple graph with N =210 nodes from the SBM. The non
zero probability blocks have a probability of 0.5. We estimate the log cut distance between the Clam
models(BigClamandIeClam)andtheSBMduringtraining. Weconsiderforbothmethodsaffiliation
25spaces of dimensions 2,4 and 6. In addition, we calculate the cut distance and l2 errors between the
Clam models and the SBM. The results are shown in Figures 4–9.
Figure4: Lefttoright: TargetSBM.FittedBigClamgraphwithtwocommunities. Errorasafunction
of optimization iteration where error is, left to right, log cut distance, cut distance, l2 distance. After
convergence, the log cut distance between the SBM and BigClam is 0.0776.
Figure5: Lefttoright: TargetSBM.FittedBigClamgraphwithfourcommunities. Errorasafunction
of optimization iteration where error is, left to right, log cut distance, cut distance, l2 distance. After
convergence, the log cut distance between the SBM and BigClam is 0.0775.
26Figure6: Lefttoright: TargetSBM.FittedBigClamgraphwithsixcommunities. Errorasafunction
of optimization iteration where error is, left to right, log cut distance, cut distance, l2 distance. After
convergence, the log cut distance between the SBM and BigClam is 0.0776.
Figure 7: Left to right: Target SBM. Fitted IeClam graph with two communities. Error as a function
of optimization iteration where error is, left to right, log cut distance, cut distance, l2 distance. After
convergence, the log cut distance between the SBM and IeClam is 0.0662.
27Figure 8: Left to right: Target SBM. Fitted IeClam graph with four communities. Error as a function
of optimization iteration where error is, left to right, log cut distance, cut distance, l2 distance. After
convergence, the log cut distance between the SBM and IeClam is 0.0312.
Figure 9: Left to right: Target SBM. Fitted IeClam graph with six communities. Error as a function
of optimization iteration where error is, left to right, log cut distance, cut distance, l2 distance. After
convergence, the log cut distance between the SBM and IeClam is 0.0309.
28