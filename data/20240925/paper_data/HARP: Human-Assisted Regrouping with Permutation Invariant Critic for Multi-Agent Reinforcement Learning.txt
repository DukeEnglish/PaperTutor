HARP: Human-Assisted Regrouping with Permutation Invariant Critic
for Multi-Agent Reinforcement Learning
Huawen Hu1, Enze Shi1, Chenxi Yue1, Shuocun Yang1,
Zihao Wu2, Yiwei Li2, Tianyang Zhong1, Tuo Zhang1, Tianming Liu2,∗, Shu Zhang1,∗
Abstract—Human-in-the-loop reinforcement learning inte-
grateshumanexpertisetoaccelerateagentlearningandprovide
critical guidance and feedback in complex fields. However,
many existing approaches focus on single-agent tasks and
require continuous human involvement during the training
process, significantly increasing the human workload and lim-
iting scalability. In this paper, we propose HARP (Human-
Assisted Regrouping with Permutation Invariant Critic), a
multi-agent reinforcement learning framework designed for
group-oriented tasks. HARP integrates automatic agent re-
grouping with strategic human assistance during deployment,
enabling and allowing non-experts to offer effective guidance
withminimalintervention.Duringtraining,agentsdynamically
adjust their groupings to optimize collaborative task comple-
tion. When deployed, they actively seek human assistance and
utilize the Permutation Invariant Group Critic to evaluate and
refine human-proposed groupings, allowing non-expert users
to contribute valuable suggestions. In multiple collaboration
scenarios, our approach is able to leverage limited guidance
from non-experts and enhance performance. The project can
Fig. 1. HARP automatically forms groups during training to achieve
be found at https://github.com/huawen-hu/HARP.
collaborative task completion. In the deployment phase, it actively seeks
assistancefromhumans,evaluatestheirsuggestions,andprovidesfeedback
I. INTRODUCTION
onthegroupsreceived.
Inthefieldofmulti-agentsystems,reinforcementlearning
has shown great promise in fostering cooperation among
agents, enabling them to solve complex tasks beyond the
information-rich signals compared to fully automated meth-
capabilitiesofindividualagents[1]–[5].Researchhasshown
ods. This collaboration between human insight and algorith-
that group division is an effective means of promoting
micpowernotonlyaccelerateslearningbutalsoenhancesthe
collaboration, both in natural ecosystems [6] and in multi-
system’s ability to generalize in complex tasks. As a result,
agent systems [7] within artificial intelligence. Breaking
HITL-RL offers a transformative approach where human
teams into smaller units can facilitate more detailed learning
input is not just a supplement, but a critical component for
processes while providing increased opportunities for inte-
achieving higher performance and efficiency in real-world
gratinginformation-richsignalsderivedfromgrouplearning.
applications [11]–[13].
While reinforcement learning has proven effective for au-
Despite these advancements, existing human-in-the-loop
tonomous problem-solving and fostering cooperation among
methods predominantly focus on single-agent scenarios,
agents,itoftenstruggleswithlowsampleefficiencyandpoor
while the integration of human guidance in multi-agent
generalization in intricate environments [8], [9].
reinforcement learning settings remains largely unexplored.
Human-in-the-loop reinforcement learning (HITL-RL)
Extending these methods to multi-agent systems presents
represents a crucial advancement in overcoming the limi-
unique challenges, as guidance from human experts is ex-
tations of traditional reinforcement learning, particularly in
pensive and rare. The core challenge lies in how humans
complexmulti-agentsystems[10].HITL-RLaddressesthese
can effectively provide guidance to multiple agents simulta-
issues by incorporating human expertise directly into the
neously, considering the complex dynamics and interactions
learning process. Human intuition and domain knowledge
within these systems [14]–[16].
provide essential guidance, enabling more accurate correc-
tions in agent behavior and more effective integration of To address these challenges, we propose a novel frame-
workforhuman-in-the-loopmulti-agentreinforcementlearn-
1NorthwesternPolytechnicalUniversity,Xi’an710072,China.
ingwithdynamicgroupingasshowninFig.1.Ourapproach
2UniversityofGeorgia,Athens,GA30602,USA.
introduces a mechanism for readjusting and reevaluating
*Corresponding author: Tianming Liu, tliu@cs.uga.edu; Shu Zhang,
shu.zhang@nwpu.edu.cn. agent groupings during the deployment phase, guided by
4202
peS
81
]GL.sc[
1v14711.9042:viXranon-expert human input. It mitigates the burden of con- the limited availability of human expert advice, RCMP [25]
tinuous human involvement in the reinforcement learning introduces a selective guidance strategy based on cognitive
trainingprocess,enablesnon-experthumanstoimprovetheir uncertainty. This approach requests human input only when
suggestion-making skills over time through the reevaluation the agent’s uncertainty is high, employing specialized tech-
of their guidance, and enhances the adaptability of multi- niques to assess and quantify this uncertainty. The idea is
agent systems to complex, dynamic environments. Our main further expanded by HULA [26], which integrates human
contributions are as follows: expert assistance during the deployment phase. In this ap-
1. We propose a novel human-in-the-loop grouped multi- proach, the agent actively seeks human recommendations
agent reinforcement learning framework where agents ac- when the return variance exceeds a predefined threshold.
tively seek human assistance during the deployment phase, Recently, OpenAI’s o1 model [27] represents a significant
while requiring no human guidance during training. advancement in natural language artificial intelligence (AI),
2. We implement a permutation invariant grouping eval- leveraginghuman-in-the-loopreinforcementlearning.Byin-
uation method that, during the deployment phase, utilizes corporating human feedback into its learning process, o1
non-expert human guidance to improve agent grouping and demonstrates remarkable improvements in response accu-
decision-making through regrouping and reevaluation. racy, and adaptability across various tasks, showcasing the
3. Our experimental results on cooperative problems power of human-guided AI optimization.
across three difficulty levels of StarCraft II [17] demonstrate In this paper, we propose a novel approach that integrates
that with limited human guidance during the deployment multi-agentreinforcementlearninggroupingtechniqueswith
phase, HARP can significantly improve agent performance human-in-the-looplearning.Ourmethodintroducesamecha-
by over 10%. nismforreadjustingandreevaluatingagentgroupingsduring
the deployment phase, leveraging guidance from non-expert
II. RELATEDWORK
humans. This approach mitigates the burden of continuous
The advancements in multi-agent reinforcement learning human involvement throughout the reinforcement learning
(MARL) have significantly enhanced learning efficiency and training process. Furthermore, it enables non-expert humans
performance through innovative approaches to agent group- to learn how to provide increasingly effective suggestions
ingandroleassignment.Forinstance,ROMA[18]introduces over time.
a role-oriented methodology that optimizes conditional mu-
III. METHOD
tual information to ensure precise alignment between roles
and trajectories. RODE [19] expands upon this by decon- A. Preliminary
structing the joint action space and incorporating action ef- In this paper, cooperative tasks are considered which
fectsintorolepolicies.Unlikerole-basedmethods,VAST[7] involving n agents, denoted as A = {a ,...,a }, framed as
1 n
examinestheinfluenceofsubgroupsonvaluedecomposition, adecentralizedpartiallyobservableMarkovdecisionprocess
employing variable subteams to aggregate local Q-functions (Dec-POMDP)[28].TheprocessisdefinedbythetupleG=
into a group Q-function, which is then synthesized into a ⟨S,U,P,r,Z,O,n,γ⟩. The environment is characterized by
globalQ-functionviavaluefunctionfactorization.SOG[20] aglobalstates∈S.Ateachtimestept,eachagentaselects
proposes a dynamic grouping framework where designated an action ut from its own action space U , forming a joint
a a
commanders extend team invitations, allowing agents to se- actionut ∈Un =U ×···×U .Thejointactiondetermines
1 n
lectpreferredcommanders.GoMARL[21]employsa”select the state transition according to the probability distribution
andkick-out”strategyforautomatedgroupingandintegrates P(st+1|st,ut) : S ×Un ×S → [0,1]. A shared reward is
hierarchical control within policy learning. This approach provided by the function r(s,u):S×Un →R, and future
achieves efficient cooperation without necessitating domain rewards are discounted by a factor γ ∈[0,1).
knowledge.Othergroupingmethodsincludeapproacheslike
B. Automatic Grouping Mechanism
GACG [22], which presents a graph-based technique that
models the multi-agent environment as a graph, calculating Consider a cooperative task involving a set of n agents.
cooperation demands between agent pairs and capturing We can partition these agents into a series of groups G =
dependencies at the group level. {g ,...,g }, where 1≤m≤n. Each group g contains a
1 m j
Recently, human-in-the-loop reinforcement learning has subset of agents: g ={a1,...,anj}⊆A. The union of all
j j j
emerged as a transformative approach to enhance policy groups covers the entire set of agents:
(cid:83)m
g = A. Also,
j=1 j
learning efficiency. Various methodologies have been pro- the groups are mutually exclusive, meaning g ∩g =∅ for
j k
posed to integrate human expertise into the learning frame- j,k ∈{1,2,...,m} and j ̸=k.
work. TAMER [23] incorporates human experts into the In this section, we focus on developing an effective learn-
agent’s learning loop, allowing them to provide reward ing method for dynamic group adjustment. Our objective is
signals and minimizing discrepancies between the agent’s to learn a grouping function f : A → G that maps agents
g
policy and the human reinforcement function. COACH [24] to groups.
enables non-expert humans to offer actionable advice during To achieve this, we introduce an automatic grouping
interactions with continuous action environments, using bi- mechanism, as illustrated in Fig. 2. Following the value
nary correction signals to refine agent actions. Recognizing functiondecompositionapproach[29],[30],werepresenttheFig.2. TheoverallframeworkofHARP.TheAgentNetworkusesgaterecurrentunit(GRU)tocapturelong-termdependenciesinpastsequencesand
encodeshiddenlayerstatestoobtainstaterepresentations.TheAutomaticGroupingsectionutilizesSelectandKickalongwithhypernetworkstoachieve
dynamicgrouping.TherightmostpartshowstheMixernetworkandhumanparticipationcomponent,includingthePermutationInvariantGroupCritic.
group Q-value Q as an aggregation of individual agent Q- C. Permutation Invariant Group Critic
g
values Qi g within the group: In multi-agent reinforcement learning scenarios, each
agent i in a system possesses a unique state representation
Q g =Aggregate(Q1 g,Q2 g,...,Qn gj) (1) s i. A critical challenge is to compute an accurate joint value
This mechanism allows agents to dynamically adjust their function V(s 1,s 2,...,s n) based on these individual states.
groupingsbasedonthelearnedQ-values,potentiallyleading Conventionally, this is achieved by concatenating the states
to more effective cooperation in complex tasks. into a single vector:
Given the hidden state ha for each agent, we employ a
t s=[s 1,s 2,...,s n] (5)
group weight generator (hyper w ) to learn the contribution
1
Thisvectoristheninputintothecriticfunction.However,
w of each agent to the total Q-value of the current group.
1
this method is sensitive to the ordering of agent states,
We then apply a “Select and Kick” strategy to adjust the
leading to the permutation non-invariance problem:
groupings. Consider two groups g and g , with a set of
1 2
weights w 1 = {{w 1i},{w 1j}}, where i represents agents Critic([s 1,s 2,...,s n])̸=Critic(π([s 1,s 2,...,s n])) (6)
in group g and j represents agents in group g . We first
1 2
where π(·) denotes any permutation of the state sequence.
calculate the threshold τ for group g as the average of
1 1
This issue is even more critical in grouping cause both
{wi}:
1 1 (cid:88) within-grouppermutationinvarianceandbetween-groupper-
τ = wi (2)
1 |g | 1 mutation invariance need to be satisfied.
1
i∈g1 Inspired by Liu et al. [31], we propose the Permutation
This threshold is then used to reassign agents between Invariant Group Critic (PIGC). In this approach, the multi-
groups. Agents in g 1 with weights below τ 1 are moved to agentenvironmentisrepresentedasagraph,whereagentsare
g 2, resulting in updated group compositions: modeled as nodes and their interactions as edges. Given the
g′ ={i∈g |wi ≥τ } (3) group index information, we construct the graph adjacency
1 1 1 1
matrix,whereforagentswithinthesamegroup,weconstruct
g′ =g ∪{i∈g |wi <τ } (4)
2 2 1 1 1 an edge between any two agents, and for agents in different
The process is then repeated for the updated group g′. groups,wedonotconstructanyconnections,i.e.eachgroup
2
This dynamic process adjusts group compositions based on is an independent subgraph. Meanwhile, we encode the
eachagent’scontributiontothegroup,potentiallyimproving hidden states of each agent as the node embedding of the
overall system performance in multi-agent scenarios. graph, as shown in Fig. 3.
After obtaining group indices based on w , we utilize Tocomputetheoutputofthegroupcritic,weuseaL-layer
1
hyper w to generate intra-group feature weights. For each graph convolutional network GCN = {f(1) ,...,f(L) }.
2 GCN GCN
group, we derive the group state by pooling the agent infor- Graph convolutional network (GCN) layer processes input
mation within the group and apply a multi-layer perceptron dataintheformofnodefeaturesandthegraph’sconnectivity
to compute the Q-value for each group. Subsequently, we structure, typically represented by an adjacency matrix.
employaMixer[21]totransformthesegroup-levelQ-values
into an overall Q-value. h(l) =f(l) (h(l−1)):=σ(Aˆ h(l−1)W(l)) (7)
GCN adjIn this paper, we adopt a proactive approach to seeking
help. During the deployment phase, we define ”the variance
of group return”. When this variance exceeds the maximum
valueinthehistoricalqueue,theagentsactivelyseekhuman
assistance to adjust the grouping in a timely manner. How-
ever, guidance from human experts is expensive and rare.
To address this, we propose a regrouping and reevaluation
strategy as shown in the red line in Fig. 2 that fully utilizes
non-expert human guidance. By reevaluating non-expert hu-
man guidance, we can reasonably identify shortcomings in
the current grouping strategy, provide timely feedback to
humans, and make adjustments. This achieves a two-way
Fig.3. PermutationInvariantGroupCritic learningprocesswheretheagentgetshelpwhilehumanscan
also learn how to come up with more effective groupings.
where Aˆ = A + I is the graph adjacency matrix
adj adj N (cid:88) 1 (cid:88) 1 (cid:88)
with self-connections, I is the identity matrix, W(l) is a Var =α (Qa−Q )2+β (Q −Q)2 (8)
N |g| g g |G| g
weight matrix, σ is the activation function. Then we use a g∈G a∈g g∈G
fully connected layer to obtain the Q value of each group.
where α is the coefficient for intra-group variance, and β is
These Q-values are compared with the group-level Q-values
the coefficient for inter-group variance.
obtained from the automatic grouping process, and their L2
We comprehensively consider both intra-group and inter-
loss is computed. This loss is incorporated as a component
group variances as the basis for determining whether as-
of the overall loss function.
sistance is needed. By setting coefficients for intra-group
D. Human Participation in Group Adjustment and inter-group variances, we can effectively adjust the
composition of agent teams. In homogeneous agent tasks,
Relying solely on automatic grouping during training has
wefocusmoreonbothintra-groupandinter-groupvariances.
limitedperformanceatdeploymenttime.Indynamicscenar-
In heterogeneous agent tasks, we place greater emphasis on
ios, while fixed groupings may achieve good performance
inter-group variance. This approach allows us to tailor our
during training, a single grouping strategy cannot satisfy all
groupingstrategytothespecificcharacteristicsofthetaskat
states. Let’s use playing soccer as an example. Although
hand. When dealing with both intra-group and inter-group
we can train players to automatically learn teamwork and
variances simultaneously, we normalize each component
completetasks,seekingoptimalgroupings,inactualmatches
using values from the historical variance queue.
(i.e., the deployment phase), a single grouping strategy
cannot work effectively from start to finish. Therefore, a IV. RESULTS
coach needs to pause the game appropriately and make
A. Experiment Settings
timely adjustments to the grouping strategy.
We conducted our experiments on six maps in the Star-
Craft II Multi-Agent Challenge environment, encompassing
Algorithm 1 HARP Deployment
three difficulty levels: Easy (8m, MMM), Hard (8m vs 9m,
while not terminate do
5m vs 6m), and Super Hard (MMM2, corridor). Agents
Get h and Qa using agent network
t controlled by algorithms compete against those controlled
Load group index G
by systems to gain rewards. The one that eliminates the
Compute variance of group Var(G) using Eq. (8)
opponent first achieves victory. Throughout the training
if Var(G) ≥ ϵ then
process,wemaintainedconsistentparametersettingsandthe
while not done do
length of historical queue was set to 10. Our experiments
GetnewgroupindexG andactionsu
human human wereexecutedonanNVIDIAGeForceRTX3090GPUwith
from human
24GB memory.
Get new group Q values QGhuman using Permuta-
tion Invariant Group Critic B. Comparison with Other Methods
if QGhuman >QG then
WeselectedVASTandGACGasourcomparisonmethods.
done
VAST is similar to our work, which is also based on
end if
value decomposition and transforms local Q-functions of
end while
individual subgroups into group Q-functions through linear
Execute u
human aggregation. GACG employs a graph theory algorithm to
else
achieveagentgrouping.Wealsocomparedabaselinemodel,
Execute actions u=argmax(Qa)
HARP without human feedback (Baseline), which uses a
end if
greedy strategy during training by selecting the action with
end while
the highest Q-value at each step.Fig.4. Groupingvisualizationduringtraininganddeploymentphases.(a)and(b)showthevisualizationandinterpretabilityanalysisofautomaticgrouping
results during the training process, while (c) and (d) present the visualization of human-assisted results during the deployment phase. The ’m’ in these
mapsreferstoMarine.’MMM’representsabattleconfigurationof1Medivac,2Marauders,and7Marinesoneachside,while’MMM2’with1Medivac,
2Marauders,and7Marinesagainst1Medivac,3Marauders,and8Marines.Inthe’corridor’map,playerscontrol6Zealotsfacing24Zerglings.
We compared the test win rate and the average return both maps present a scenario where allies are outnumbered
across these different methods. As shown in Fig. 5, to by one, the performance on 8m vs 9m is notably better than
enablehumanstobetterguidethebehaviorofthemulti-agent on 5m vs 6m. We posit that as the number of agents in-
system,weimplementedagraphicaluserinterface(GUI)that creases,themodelbecomesmoreadeptatgrouping,allowing
displayspartialinformationabouttheenvironment,including forclearertaskallocationwithineachgroup.Onsuperhard-
the relative positions between agents and their health status. typemaps,onlythebaselinesandHARPmanagetomaintain
Based on this information, humans can determine how to high success rates, while VAST and GACG perform poorly
improve the grouping strategy and provide guidance for the or even fail to function. Notably, HARP achieves a 100%
agents’ actions. The experimental results are shown in Table win rate across all six maps of varying difficulties.
I and Table II. Fig. 4 (a) and Fig. 4 (b) show partial results from the
trainingprocess,whichweanalyzedforinterpretabilitybased
TABLEI
on the learned groupings. It can be observed that different
COMPARISONOFHARPWITHOTHERMETHODSONTESTWINRATE(%).
groupingsexhibitdistinctstrategies.OntheMMM2map,the
grouping consists of three parts: based on their performance
Method 8m MMM 5mvs6m 8mvs9m MMM2 Corridor
VAST 98.1 94.4 71.2 91.9 43.7 0 in the game, we can categorize them as the Medic Guard
GACG 98.7 98.7 53.1 90.6 48.7 0 which protect the Medivac, the Tank Squad responsible for
Baseline 96.0 100 65.6 90.6 93.7 93.7
drawing fire and attacking, and a small group consisting of
HARP 100 100 100 100 100 100
only two Marines (Strike Team). This Strike Team switches
between the roles of the other two; when the number of
TABLEII MedicGuarddecreases,theStrikeTeamtakesonpartofthe
COMPARISONOFHARPWITHOTHERMETHODSONTESTMEAN responsibilityforprotectingtheMarines.Asimilarresultcan
RETURN. be observed on the corridor map, where the Decoy Squad is
responsible for drawing the majority of the Zerglings, while
Method 8m MMM 5mvs6m 8mvs9m MMM2 Corridor theBackupTeamandFrontlineUnithandlethesmallportion
VAST 19.8 19.7 17.2 19.4 15.8 11.0
ofenemiesnotdrawnaway.Amongthese,theFrontlineUnit
GACG 20.7 21.7 15.5 19.3 16.3 10.9
Baseline 19.8 20.3 16.5 19.3 19.3 19.8 is primarily responsible for attacking, and the Backup Team
HARP 20.0 20.1 20.0 20.0 20.1 20.2 finishes off any remaining enemies.
C. Human Participation Rate in Deployment Phase
On easy-type tasks, each method achieves satisfactory
performance, with win rates approaching or reaching 100%. In this section, we investigated the proportion of human
However, on hard-type tasks, the performance gap widens involvement during the deployment phase across different
significantly. On the 5m vs 6m map, methods other than types of maps. We repeated the experiments five times on
HARP only achieve win rates of around 50% to 70%. Inter- each map, with each experiment consisting of 31 episodes.
estingly, on the 8m vs 9m map, this gap narrows. Although We recorded the total number of steps and the number ofIn this section, we examine why limited human guidance
during the deployment phase significantly improves the suc-
cess rate of the game. As observed in Table I, all baselines
performpoorlyonthe5m vs 6mmap,showingasubstantial
performance gap compared to the 8m vs 9m map of similar
difficulty.However,activelyseekinghumanassistanceduring
the testing phase leads to a marked improvement in perfor-
mance. We aim to understand the specific role of human
assistance in this context.
Table III outlines the groupings learned by the automatic
grouping algorithm across various maps. On the 5m vs 6m
Fig.5. TheGUIinterfaceshowntohumansduringthedeploymentphase, and 8m vs 9m maps, the automatic grouping algorithm
includingtherelativepositionsofeachagentandtheirhealthpercentages.
places all agents in a single group for coordination and
cooperation.Thisapproachresultsinalackofcleardivision
of labor among agents. As illustrated in Fig. 4 (c) and Fig.
4 (d), the dashed lines represent human-assisted groupings.
Duringtesting,thereisevidentdivisionoflaborandstrategy
among agents. At certain moments, agents exhibit spatial
convergence, forming groups. Simultaneously, in terms of
strategy,agentswithhigherhealthpointspositionthemselves
more forward, while those with lower health points retreat.
This phenomenon is also observable in Fig. 5. During the
human assistance process, these phenomena and strategies
are typically leveraged to dynamically adjust the behavior
and grouping of multiple agents. This adaptive approach,
guided by human insight, appears to be a key factor in the
improved performance observed when human assistance is
incorporated into the deployment phase.
Fig.6. Thenumberoftimesagentsactivelyseekhumanassistanceduring V. CONCLUSIONS
thedeploymentphaseondifferenttypesofmaps,expressedasapercentage
(totalhumaninterventions/totalsteps). In this work, we propose an effective multi-agent rein-
forcement learning framework that actively seeks human
assistance for grouping during the deployment phase. As
human interventions, using their ratio as an indicator of real-timehumaninvolvementduringtrainingcanbecumber-
humaninvolvement.TheresultsareshowninFig.6.Overall, some and time-consuming, we shift human participation to
theproportionofhuman-assistedagentreorganizationacross the deployment stage. To enhance the quality of non-expert
all maps was less than 25%. On simpler maps, the agents guidance,weintroducearegroupingandreevaluationmethod
requested human assistance less frequently, accounting for for group critics based on group invariance. By evaluating
only 2.61% on the 8m map. However, in tasks that agents human-proposed groupings, we maximize the utilization of
didnothandlewell,suchas5m vs 6mand8m vs 9m,where human suggestions. We tested our approach on six StarCraft
the agents’ win rates were only 65.6% and 90.6%, respec- II maps across three difficulty levels. Compared to scenarios
tively, the number of times agents sought human assistance without human assistance, our method improved success
significantly increased during the deployment phase, rising rates by an average of 10%. On some more challenging
to around 20%. With HARP’s limited human involvement maps, we increased success rates from 65% to 100%. Our
in the deployment phase, we can significantly improve the approachhasthepotentialtobeextendedtoothertaskssuch
agent’s performance. as human-machine collaboration and sim-to-real transfer
[32]–[35]. One promising direction is its integration with
D. Impact of Human Assistance on Challenging Scenarios
multimodal LLMs for complex reasoning tasks [36]. By
dynamically grouping agents to specialize in different data
TABLEIII modalitiesandadaptivelyregroupingbasedontaskdemands,
GROUPINFORMATIONLEARNEDTHROUGHAUTOMATICGROUPING the framework enhances multimodal alignment and balances
the permutations of inputs. Incorporating real-time human
Maps Groups
8m [01345][67][2] feedback further improves decision-making, benefiting ap-
MMM [012345] plications such as autonomous vehicles processing diverse
5mvs6m [01234]
sensor data or healthcare systems integrating various patient
8mvs9m [01234567]
MMM2 [24568][790][31] information. This integration helps make multimodal LLMs
corridor [12][045][3] morerobustandadaptabletocomplex,real-worldchallenges.REFERENCES [21] Y.Zang,J.He,K.Li,H.Fu,Q.Fu,J.Xing,andJ.Cheng,“Automatic
groupingforefficientcooperativemulti-agentreinforcementlearning,”
[1] M.Kouzeghar,Y.Song,M.Meghjani,andR.Bouffanais,“Multi-target AdvancesinNeuralInformationProcessingSystems,vol.36,2024.
pursuitbyadecentralizedheterogeneousuavswarmusingdeepmulti-
[22] W. Duan, J. Lu, and J. Xuan, “Group-aware coordination graph for
agentreinforcementlearning,”in2023IEEEInternationalConference
multi-agentreinforcementlearning,”arXivpreprintarXiv:2404.10976,
onRoboticsandAutomation(ICRA). IEEE,2023,pp.3289–3295.
2024.
[2] W.Wang,L.Mao,R.Wang,andB.-C.Min,“Multi-robotcooperative
[23] W. B. Knox and P. Stone, “Tamer: Training an agent manually via
socially-aware navigation using multi-agent reinforcement learning,” evaluativereinforcement,”in20087thIEEEinternationalconference
in2024IEEEInternationalConferenceonRoboticsandAutomation
ondevelopmentandlearning. IEEE,2008,pp.292–297.
(ICRA). IEEE,2024,pp.12353–12360.
[24] C. Celemin and J. Ruiz-del Solar, “An interactive framework for
[3] Y.Feng,C.Shi,J.Du,Y.Yu,F.Sun,andY.Song,“Variableadmittance
learning continuous actions policies based on corrective feedback,”
interactioncontrolofuavsviadeepreinforcementlearning,”in2023
JournalofIntelligent&RoboticSystems,vol.95,pp.77–97,2019.
IEEEInternationalConferenceonRoboticsandAutomation(ICRA).
[25] F. L. Da Silva, P. Hernandez-Leal, B. Kartal, and M. E. Taylor,
IEEE,2023,pp.1291–1297.
“Uncertainty-aware action advising for deep reinforcement learning
[4] A. Agrawal, A. S. Bedi, and D. Manocha, “Rtaw: An attention agents,” in Proceedings of the AAAI conference on artificial intelli-
inspiredreinforcementlearningmethodformulti-robottaskallocation gence,vol.34,no.04,2020,pp.5792–5799.
inwarehouseenvironments,”in2023IEEEInternationalConference
[26] S. Singi, Z. He, A. Pan, S. Patel, G. A. Sigurdsson, R. Piramuthu,
onRoboticsandAutomation(ICRA). IEEE,2023,pp.1393–1399.
S. Song, and M. Ciocarlie, “Decision making for human-in-the-loop
[5] I. Igbinedion and S. Karaman, “Learning when to ask for help: roboticagentsviauncertainty-awarereinforcementlearning,”in2024
Efficient interactive navigation via implicit uncertainty estimation,” IEEEInternationalConferenceonRoboticsandAutomation(ICRA).
in2024IEEEInternationalConferenceonRoboticsandAutomation
IEEE,2024,pp.7939–7945.
(ICRA). IEEE,2024,pp.9593–9599.
[27] OpenAI, “Introducing OpenAI o1-preview,” https://www.openai.com/
[6] G.WittemyerandW.M.Getz,“Hierarchicaldominancestructureand
blog/introducing-openai-o1,n.d.,accessed:2024-09-12.
social organization in african elephants, loxodonta africana,” Animal [28] F.A.Oliehoek,C.Amatoetal.,Aconciseintroductiontodecentral-
Behaviour,vol.73,no.4,pp.671–681,2007.
izedPOMDPs. Springer,2016,vol.1.
[7] T.Phan,F.Ritz,L.Belzner,P.Altmann,T.Gabor,andC.Linnhoff-
[29] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
Popien, “Vast: Value function factorization with variable agent sub- M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls et al.,
teams,”AdvancesinNeuralInformationProcessingSystems,vol.34,
“Value-decomposition networks for cooperative multi-agent learning
pp.24018–24032,2021. based on team reward,” in Proceedings of the 17th International
[8] Y. Yu, “Towards sample efficient reinforcement learning.” in IJCAI, ConferenceonAutonomousAgentsandMultiAgentSystems,2018,pp.
2018,pp.5739–5743.
2085–2087.
[9] Z. Zhang, Y. Chen, J. D. Lee, and S. S. Du, “Settling the sample
[30] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi, “Qtran:
complexity of online reinforcement learning,” in The Thirty Seventh
Learningtofactorizewithtransformationforcooperativemulti-agent
Annual Conference on Learning Theory. PMLR, 2024, pp. 5213– reinforcementlearning,”inInternationalconferenceonmachinelearn-
5219. ing. PMLR,2019,pp.5887–5896.
[10] T. Mandel, Y.-E. Liu, E. Brunskill, and Z. Popovic´, “Where to add
[31] I.-J.Liu,R.A.Yeh,andA.G.Schwing,“Pic:permutationinvariant
actionsinhuman-in-the-loopreinforcementlearning,”inProceedings
criticformulti-agentdeepreinforcementlearning,”inConferenceon
oftheAAAIConferenceonArtificialIntelligence,vol.31,no.1,2017.
RobotLearning. PMLR,2020,pp.590–602.
[11] M.Torne,M.Balsells,Z.Wang,S.Desai,T.Chen,P.Agrawal,and
[32] D. Liu, Y. Chen, and Z. Wu, “Digital twin (dt)-cyclegan: En-
A. Gupta, “Breadcrumbs to the goal: goal-conditioned exploration ablingzero-shotsim-to-realtransferofvisualgraspingmodels,”IEEE
fromhuman-in-the-loopfeedback,”inProceedingsofthe37thInter-
RoboticsandAutomationLetters,vol.8,no.5,pp.2421–2428,2023.
nationalConferenceonNeuralInformationProcessingSystems,2023,
[33] D. Liu, Z. Li, Z. Wu, and C. Li, “Dt/mars-cyclegan: Improved
pp.63222–63258. object detection for mars phenotyping robot,” arXiv preprint
[12] S. Holk, D. Marta, and I. Leite, “Polite: Preferences combined with arXiv:2310.12787,2023.
highlights in reinforcement learning,” in 2024 IEEE International
[34] J. Wu, Y. Zhou, H. Yang, Z. Huang, and C. Lv, “Human-guided
Conference on Robotics and Automation (ICRA). IEEE, 2024, pp.
reinforcementlearningwithsim-to-realtransferforautonomousnav-
2288–2295. igation,”IEEETransactionsonPatternAnalysisandMachineIntelli-
[13] M. Mitra, G. Kumar, P. P. Chakrabarti, and P. Biswas, “Enhanced gence,2023.
human-robot collaboration with intent prediction using deep inverse
[35] S. W. Abeyruwan, L. Graesser, D. B. D’Ambrosio, A. Singh,
reinforcement learning,” in 2024 IEEE International Conference on
A.Shankar,A.Bewley,D.Jain,K.M.Choromanski,andP.R.Sanketi,
RoboticsandAutomation(ICRA). IEEE,2024,pp.7880–7887.
“i-sim2real:Reinforcementlearningofroboticpoliciesintighthuman-
[14] J. Chung, A. Luo, X. Raffin, and S. Perry, “Battlesnake challenge: robotinteractionloops,”inConferenceonRobotLearning. PMLR,
Amulti-agentreinforcementlearningplaygroundwithhuman-in-the-
2023,pp.212–224.
loop,”arXivpreprintarXiv:2007.10504,2020.
[36] J.Wang,Z.Wu,Y.Li,H.Jiang,P.Shu,E.Shi,H.Hu,C.Ma,Y.Liu,
[15] C.O.Retzlaff,S.Das,C.Wayllace,P.Mousavi,M.Afshari,T.Yang, X. Wang et al., “Large language models for robotics: Opportunities,
A.Saranti,A.Angerschmid,M.E.Taylor,andA.Holzinger,“Human- challenges,andperspectives,”arXivpreprintarXiv:2401.04334,2024.
in-the-loopreinforcementlearning:Asurveyandpositiononrequire-
ments,challenges,andopportunities,”JournalofArtificialIntelligence
Research,vol.79,pp.359–415,2024.
[16] Z.Qin,H.-N.Wu,andJ.-L.Wang,“Proactivecooperativeconsensus
control for a class of human-in-the-loop multi-agent systems with
humantime-delays,”Neurocomputing,vol.581,p.127485,2024.
[17] M. Samvelyan, T. Rashid, C. S. de Witt, G. Farquhar, N. Nardelli,
T. G. J. Rudner, C.-M. Hung, P. H. S. Torr, J. Foerster, and
S. Whiteson, “The StarCraft Multi-Agent Challenge,” CoRR, vol.
abs/1902.04043,2019.
[18] T.Wang,H.Dong,V.Lesser,andC.Zhang,“Roma:Multi-agentrein-
forcementlearningwithemergentroles,”inInternationalConference
onMachineLearning. PMLR,2020,pp.9876–9886.
[19] T.Wang,T.Gupta,A.Mahajan,B.Peng,S.Whiteson,andC.Zhang,
“Rode:Learningrolestodecomposemulti-agenttasks,”arXivpreprint
arXiv:2010.01523,2020.
[20] J.Shao,Z.Lou,H.Zhang,Y.Jiang,S.He,andX.Ji,“Self-organized
group for cooperative multi-agent reinforcement learning,” Advances
in Neural Information Processing Systems, vol. 35, pp. 5711–5723,
2022.