XP-MARL: Auxiliary Prioritization in Multi-Agent Reinforcement
Learning to Address Non-Stationarity
Jianye Xu1 , Student Member, IEEE, Omar Sobhy1 , Bassam Alrifaee2 , Senior Member, IEEE
Abstract—Non-stationarityposesafundamentalchallengein challengesduetothedynamicinterdependenceamongagents
Multi-Agent Reinforcement Learning (MARL), arising from that single-agent RL techniques cannot effectively handle.
agents simultaneouslylearning andaltering theirpolicies. This
One of the fundamental challenges in MARL is non-
creates a non-stationary environment from the perspective of
stationarity. In MARL, each agent considers other agents
each individual agent, often leading to suboptimal or even
unconverged learning outcomes. We propose an open-source as a part of the environment. As other agents learn and
framework named XP-MARL, which augments MARL with update their policies continuously, the environment changes
auxiliaryprioritizationto addressthischallengein cooperative over time and becomes non-stationary from the perspective
settings. XP-MARL is 1) founded upon our hypothesis that
of each agent, rendering its previously learned policies less
prioritizing agents and letting higher-priority agents establish
effective or obsolete. Therefore, the optimal policy for one
their actions first would stabilize the learning process and
thus mitigate non-stationarity and 2) enabled by our proposed agent dynamically changes, leading to a situation where
mechanism called action propagation, where higher-priority each agent is essentially chasing a “moving target” [4]. As
agentsactfirstandcommunicatetheiractions,providingamore traditional single-agent RL techniques often fall short in
stationaryenvironmentforothers.Moreover,insteadofusinga
effectively handling this non-stationarity because they rely
predefined or heuristic priority assignment, XP-MARL learns
ontheassumptionofastableenvironment,thereisapressing
priority-assignment policies with an auxiliary MARL problem,
leading to a joint learning scheme. Experiments in a motion- need to develop methods that address non-stationarity in
planningscenarioinvolvingConnectedandAutomatedVehicles MARL.
(CAVs) demonstrate that XP-MARL improves the safety of a
baseline model by 84.4% and outperforms a state-of-the-art A. Related Work
approach, which improves the baseline by only 12.8%.
Various works have been proposed to handle non-
Code: github.com/cas-lab-munich/sigmarl stationarity in MARL, see survey articles [5] and [6]. We
detail two categories: centralized critic and opponent mod-
I. INTRODUCTION eling.
1) Centralized Critic: Actor-critic algorithms, initially
Reinforcement Learning (RL) has seen a substantial rise
proposed for single-agent settings [7], have been effectively
in both interest and application recently, primarily due to
adapted for multi-agent systems to address non-stationarity
advancements in deep learning. The integration of deep
in MARL. The centralized critic approach is particularly
learning with traditional RL algorithms has enabled the
appealing, leveraging centralized training with decentralized
development of agents capable of operating in complex
execution. During training, agents access global information
environments,leadingtosignificantbreakthroughsinvarious
thatconsidersthestatesofallagents,whileduringexecution,
domains, including autonomous driving [1], robotic control
theyoperateindependentlyusingonlylocalinformation.This
[2], and strategy games [3].
approachstabilizesthelearningprocessbyincorporatingthis
While single-agent RL has achieved considerable success, global information. One of the pioneering works using this
manyreal-worldproblemsinherentlyinvolvemultipleagents approach is MADDPG [8], which has been widely adopted
interacting within a shared environment. Such multi-agent and extended by other studies, such as integrating recurrent
systems are crucial in a wide range of applications such neural networks to handle partial observability with limited
as cooperative robots and Connected and Automated Ve- communication [9], an MADDPG-based resource manage-
hicles (CAVs) navigating traffic. The increasing complexity ment scheme for vehicular networks [10], and M3DDPG—a
and interactivity of these environments have given rise to minimax extension for robust learning [11]. Our approach
Multi-AgentReinforcementLearning(MARL),aspecialized can complement this category to address non-stationarity
subfield of RL that addresses the unique challenges in further.
multi-agent systems. However, MARL introduces additional 2) Opponent Modeling: Opponent modeling addresses
non-stationarity by predicting and adapting to the intentions
ThisresearchwassupportedbytheBundesministeriumfu¨rDigitalesund and policies of other agents. Early approaches focused on
Verkehr (German Federal Ministry for Digital and Transport) within the
specific domains such as Poker games [12], while recent
project“HarmonizingMobility”(grantnumber19FS2035A).
1The authors are with the Chair of Embedded Software research has shifted toward more generalized techniques.
(Informatik 11), RWTH Aachen University, Germany, {xu, Studies [13] and [14] introduced an auxiliary network that
sobhy}@embedded.rwth-aachen.de.
predicted opponents’ actions based on observations, whose
2TheauthoriswiththeDepartmentofAerospaceEngineering,University
oftheBundeswehrMunich,Germany,bassam.alrifaee@unibw.de. hiddenlayerswereincorporatedintoaDQNtoconditionfor
ThisworkhasbeensubmittedtotheIEEEforpossiblepublication.Copyrightmaybetransferredwithoutnotice,afterwhichthisversionmaynolongerbeaccessible.
4202
peS
81
]OR.sc[
1v25811.9042:viXraa better policy. In contrast, [15] proposed agents predicting II. PROBLEMFORMULATION
the behavior of others using their own policies. While
A Markov decision process is a mathematical framework
effectiveinhomogeneoussettings,thisapproachcandegrade
for modeling decision-making in discrete time settings with
against heterogeneous opponents. In [16], the authors sug-
a single agent. Extending this concept, a Markov Game
gested an approach that enabled agents to utilize recursive
(MG) involves multiple agents, where each agent’s payoff
reasoning, even against similarly capable opponents. Other
depends on the actions of all agents. In our work, we target
workssuchas[17]and[18]focusedinsteadonmodifyingthe
Partially Observable MGs (POMGs), which allow for only
optimization function in policy gradient methods to account
partialobservabilityofthestate,requiringagentstobasetheir
foropponentlearningduringtraining.Wewillunderscoreour
decisions on incomplete information. Formally, we define a
approach’s effectiveness by comparing it with an opponent
POMG as follows (adapted from [33]).
modeling approach similar to [15].
Definition 1. A POMG is defined by a tuple
Other promising approaches have also been proposed
(N,S,{A(i)} ,{O(i)} ,P,{R(i)} ,γ), where
to address non-stationarity in MARL, including indepen- i∈N i∈N i∈N
N = {1,··· ,N} denotes the set of N > 1 agents, S
dentlearning[19]–[21],sequentiallearning[22]–[25],multi-
denotes the state space shared by all agents, A(i) and O(i)
timescalelearning[26]–[28],andinter-agentcommunication
denote the action and observation spaces of each agent
[29]–[31].
i ∈ N, respectively. Let A := A(1) × ··· × A(N) and
O := O(1) ×···×O(N) denote the joint action and joint
B. Paper Contributions
observation spaces, then P : S × A → ∆(S) × ∆(O)
The main contributions of this work are threefold: denotes the transition probability from any state s ∈ S
1) It proposes the hypothesis that prioritizing agents and and any given joint action a := (a(1),...,a(N)) ∈ A to
letting higher-priority ones establish actions first would any new state s′ ∈ S while receiving a joint observation
mitigate non-stationarity in cooperative MARL. o := (o(1),...,o(N)) ∈ O; R(i) : S × A × S → R is
2) It introduces an open-source framework named the reward function that determines the immediate reward
XP-MARL, which augments MARL with auxiliary received by the agent i for a transition from (s,a) to s′;
prioritization to validate the above hypothesis. γ ∈ [0,1) is the discount factor balancing the immediate
3) It proposes a mechanism called action propagation as and future rewards.
a key ingredient of XP-MARL, enabling lower-priority
In this setting, agents have access only to partial state
agents to condition their actions on the communicated
of their environment. At each time step t, each agent i ∈
actions of higher-priority ones, thereby providing a
N executes an action a(i) based on its partial observation
more predictable environment. t
o(i) ∈ O(i). The environment then transitions from the
t
Our work appears to be pioneering in addressing non-
current state s ∈S to a new state s ∈S, and the agent
t t+1
stationarityinMARLthroughlearning-baseddynamicpriori- receives a reward R(i)(s ,a ,s ), where a ∈ A is the
t t t+1 t
tization.Acloselyrelatedstudy[32],whichtargetedhighway
joint action of all agents. The objective of agent i is to find
merging, also incorporated priorities in MARL. However, a policy π(i) : O(i) → ∆(A(i)), a mapping from its partial
it used handcrafted heuristics to assign priorities, limiting observation o(i) to a probability distribution ∆(A(i)) over
its application to this specific scenario and possibly result- its action space A(i), such that a(i) ∼ π(i)(· | o(i)) and
ing in inappropriate prioritization in unforeseen situations. t t
its expected cumulative reward—termed value function—is
In comparison, our framework extends to general MARL
maximized. Note that ∼ denotes the sampling of an action
environments and avoids manually crafted heuristics by
a from a policy π.
autonomouslylearningeffectivepriority-assignmentpolicies.
Toformallyillustratenon-stationarityinMARL,weadapt
the formulation in [34]. The value function for each agent
C. Notation
i∈N is defined as:
Avariablexismarkedwithasuperscriptx(i) ifitbelongs V(i)(s)=(cid:88) π(a|s) (cid:88) P(cid:0) s′ |s,a(i),a(−i)(cid:1) ×
toagenti.Allotherinformationispresentedinitssubscript, π
a∈A s′∈S
e.g., the value of x at time t is written as x . If multiple
t (cid:2) R(i)(cid:0) s,a(i),a(−i),s′(cid:1) γV(i)(s′)(cid:3)
, (1)
pieces of information need to be conveyed in the subscript,
they are separated by commas. The cardinality of any set X where π(a | s) = (cid:81) π(j)(cid:0) a(j) |s(cid:1) represents the joint
j∈N
is denoted by |X|. If x represents a tuple, appending a new policy and a=(a(i),a(−i)) represents the joint action, with
element a to it is denoted by x←(x,a).
−i = N \{i} denoting the set of all agents except i. The
joint policy could be equivalently expressed as:
D. Paper Structure
(cid:16) (cid:17) (cid:89) (cid:16) (cid:17)
π(a|s)=π(i) a(i) |s · π(j) a(j) |s
Section II formally formulates the problem. Section III
j∈−i
presents our framework as a solution. Section IV details ex-
(cid:124) (cid:123)(cid:122) (cid:125)
perimentsanddiscussesthelimitationofourwork.SectionV (cid:16) (cid:17) (cid:16) (cid:17)
=π(i) a(i) |s ·π(−i) a(−i) |s . (2)
draws conclusions and outlines future research.Problem 2 (Priority-Assignment Problem). Find a priority-
assignment policy such that at any state s∈S,
(cid:88) V(i)(s)> (cid:88) V(i)(s) (4)
πˆ π
i∈N i∈N
Fig. 1: A navigation game with two agents. Each agent i∈ holds, where πˆ ∈ Ω and π ∈ Ω denote the joint policy
{1,2} has three actions: turn left a(i), go straight a(i) , learned with and without prioritizing agents, respectively.
left straight
and turn right a(i) . Right side shows team rewards.
right III. OURXP-MARLFRAMEWORK
This section presents our framework named XP-MARL,
The optimal policy of each agent i, denoted as π∗(i), is the depictedinFig.2.XP-MARLconsistsoftwostages,priority
assignmentanddecision-making,witheachcorrespondingto
one that maximizes its value function at any state s while
anMARLproblem,thusnamedbi-stageMARLproblem.We
considering the interaction with all opponents’ policies,
i.e., π∗(i) := argmax V(i) (s). Consequently, this describethebi-stageMARLprobleminSec.III-A,detailthe
π(i) π(i),π(−i) two stages in Sec. III-B and III-C, and overview the overall
optimal policy depends on the opponents’ policies, which
framework in Sec. III-D.
are non-stationary as the opponents learn and update their
policies over time. This dynamic nature causes the optimal A. Bi-Stage MARL Problem
policy for each agent to change.
While heuristic priority assignment has shown effective-
In our work, we target team POMG, a type of team
ness in priority-based decision-making, they are often tai-
game, where all agents share the same reward. Formally,
lored for specific scenarios or may lack the ability to handle
weformulatethestandardMARLprobleminateamPOMG
complexsituations[32],[37].Inourwork,welearnpriority-
as follows (adapted from [35] and [36]).
assignmentpoliciesbyintroducinganauxiliaryMARLprob-
Problem 1 (Team POMG). Find a team-optimal Nash equi- lem alongside the primary one that learns decision-making
librium—a joint policy π∗ :=(π∗(1),...,π∗(N))—such that policies.Thisapproachresultsinajointlearningschemeand
at any state s∈S, abi-stageMARLproblem.Wenametheactionsgeneratedby
the priority-assignment policies as priority scores and those
(cid:88) V π( ∗i)(s)≥ (cid:88) V π(i)(s),∀π ∈Ω (3) by the decision-making policies as decisions. When context
i∈N i∈N allows, we sometimes refer to both as actions for simplicity.
In the remainder of the paper, variables will be annotated
holds, where Ω denotes the joint policy space.
withsubscriptstoindicatetheirassociations:x forvariables
P
While solving Problem 1 is challenging due to non- related to the priority-assignment stage and x for those
D
stationarity,addressingitwouldimprovethesolutionquality. associated with the decision-making stage.
A. Hypothesis on Prioritization B. Priority-Assignment Stage
The priority-assignment stage prioritizes agents and gen-
Wehypothesizethatprioritizingagentsandlettinghigher-
erates a priority rank according to which agents act sequen-
priority agents establish their actions first can help address
tially. This establishes a sequential decision-making scheme
non-stationarity.Ourhypothesisstemsfromtheintuitionthat
that serves as the backbone for the subsequent decision-
if higher-priority agents could establish their actions first
making stage.
and make these actions known to lower-priority agents, the
We propose Algorithm 1 for this stage, executed at each
former would create a more predictable environment for the
timestep(timeargumentsomitted).Atthebeginningofeach
latter. Consider a simple navigation game with two agents,
time step, for each agent i, the priority-assignment policy
depicted in Fig. 1, where they learn to bypass each other.
The optimal reward of 10 is issued if only one evades, π P(i) generates a priority score a( Pi) based on its observation
while the worst reward of -10 occurs if they collide. If both o( Pi) (line3).Thesescoresarethenappendedintoatuplea P,
evade to different sides, they receive a suboptimal reward sortedindescendingorder,andthecorrespondingagentsare
of 5. Suppose in the previous time step, both agents tried arrangedinorderwithinanothertupleR P,whichwecallthe
to yield and secured a suboptimal reward. Therefore, at the priority rank (lines 4 to 6). The entire stage is visualized in
currenttimestep,theybothadjusttheirstrategiestobemore Fig. 2 (left side).
aggressive and decide to go straight, which unfortunately
C. Decision-Making Stage
leads to a collision and a negative reward of -10. This
After the priority-assignment stage, the system transitions
situation changes if one agent has a higher priority: if agent
to the decision-making stage (right side of Fig. 2).
1 acts first and makes its decision known to agent 2, agent
Opponent modeling approaches incorporate other agents’
2 can then choose the action that maximizes the reward.
intentions, often by predicting their actions, which has
How to assign appropriate priorities to agents remains a
shown effectiveness in stabilizing learning and mitigating
question. Formally, we formulate a new problem as follows.
non-stationarity [12]–[18]. Thus, we hypothesize that usingFig.2:OurXP-MARLframework,timeargumentsomitted.o(i)/a(i)/π(i)/R :observation/action/policy/priorityrank
P,i
of agent i, i∈N ={1,...,N}. argsort: returns the indices that sort the priority scores (a(i)) in descending order.
P i∈N
Algorithm 1 PriorityAssignment: Generate Priority Rank Algorithm 2 details the overall framework, executed at
Input: joint observation: o := (o(i)) , joint policy: each time step (time arguments omitted). The time step
π :=(π(i)) P P i∈N begins by assigning priorities to agents and determining a
P P i∈N priority rank R (line 1). Thereafter, agents act sequentially
Output: priority rank of agents: R ∈NN P
P in this ranked order. For each agent i, its observable higher-
1: a P ←() ▷ Initialize a tuple to store priority scores priority agents, denoted by N(i←), are identified (lines 4
2: for i=1 to N do obs.
3: a( Pi) ∼π P(i)(·|o( Pi)) ▷ Call priority-assignment t to he6 n). umN bo ete
r
t oh fat agw ee ntsm to hd ae tl ep aa cr htia al geo nb tse cr av nab oi bli st ey rvb ey
,
l ri em fei rti rn eg
d
policy
4: a P ←(a P,a( Pi)) ▷ Append new priority score t oo bsa es rvao bb ls eer hva igb hle er-a pg rie on rt is tyhe an gc ee nf to s,rth d. enT oh tee da ac stio Ans (i)of t ,he as ree
5: end for D,prop.
6: R P ←argsort(a P,descending) ▷ Get indices that sort propagated and appended to the agent’s observation o( Di),
a tuple yieldingamodifiedobservationo′( Di) (line8).Conditionedon
7: return R P this modified observation, the agent generates an action a( Di)
using its decision-making policy π(i) (line 9). This action
D
is added to the joint action a (line 10) to be later used
D
actual actions instead of predictions could further stabilize by subsequent agents for action propagation. The overall
learning. Consequently, we propose a mechanism called ac- framework is depicted in Fig. 2.
tion propagation, where higher-priority agents communicate Note that at the end of Algorithm 1 and Algorithm 2,
their actions to lower-priority agents, enabling the latter to the joint action of each MARL problem is fed into its
conditiontheirdecisionsontheseactions.Thisresultsinase- environment to update the environment state and generate
quential decision-making scheme where agents act one after reward signals. However, we omit this step for simplicity.
another according to the priority rank determined during the
Remark 1. We present Algorithm 1 and Algorithm 2 as
priority-assignment stage. This structured decision-making
centralized algorithms for simplicity. They can be easily
stage reduces the complexity of interactions and enhances
adapted to decentralized algorithms. As such, agents need
learning, thereby mitigating non-stationarity and improving
to communicate their priority scores during the priority-
coordination among agents.
assignment stage.
The action propagation mechanism is enabled by the
assumption that agents can communicate their actions with
IV. EXPERIMENTS
a communication delay of less than one time step. Note
this assumption does not violate partial observability, which We evaluate our framework through numerical experi-
referstoagents’inabilitytofullyobservethestateoftheen- ments within the open-source SigmaRL [38]—a sample-
vironment,whichisdistinctfromtheabilitytocommunicate efficient and generalizable MARL framework for motion
specific information such as actions. planningofCAVs.SigmaRLprovidesvariousbenchmarking
trafficscenarios,whereagentsneedtocooperatetomaximize
D. Overview of XP-MARL
trafficsafetyandefficiency.Weselectascenariothatmirrors
Integrating the above two stages leads to our framework, the real-world conditions in the Cyber-Physical Mobility
XP-MARL, which enables the joint learning of the priority- (CPM) Lab [39], referred to as CPM scenario thereafter.
assignment and decision-making policies. As depicted in Fig. 3, the CPM scenario features an eight-Algorithm 2 Overall Algorithm of XP-MARL algorithm—to learn both policies. Each MAPPO instance
Input: joint observations: o := (o(i)) and o := consists of a centralized critic and multiple decentralized
D D i∈N P actors, where each actor learns a policy for one vehicle. The
(o(i)) , joint policies: π := (π(i)) and π :=
P i∈N D D i∈N P inputofthepriority-assignmentactorincludestheunderlying
(π(i)) , observable agents of each agent: {N(i)} vehicle’s states such as speed, observable vehicles’ states
P i∈N obs. i∈N
such as speeds and positions, and the lane information
Output: joint action for decision-making: a ∈A
D D
related to centerlines and boundaries. The decision-making
1: R P ←PriorityAssignment(o P,π P) ▷ Call Algorithm 1 actor uses the same input but also includes the propagated
2: a D ←() ▷ Initialize joint action actions of observable higher-priority vehicles. Each critic’s
3: for each i∈R P do ▷ Iterate over in ranked order input is a concatenation of all actors’ inputs in its MAPPO
instance.Sincevehiclesarehomogeneous,weenablepolicy-
4: k ←where(R P(k)≡i}) ▷ Find index
parameter sharing to enhance learning efficiency. Note that
5: N(i←) ←R P(1:k−1) ▷ Higher-priority agents our framework can also be applied to heterogeneous multi-
6: N(i←) ←N(i←)∩N(i) ▷ Observable agent systems. We model partial observability by allowing
obs. obs.
each vehicle to observe only its two nearest neighbors.
higher-priority agents
WetrainfivemodelsfromM toM .ModelM employs
7: A( Di ,) prop. ←{a( Dj) |j ∈N o( bi s← . )} our framework, XP-MARL; M1 odel M5 2 employs1 standard
8: o′( Di) ←o D(i)∪A( Di ,) prop. ▷ Action propagation MAPPO serving as a baseline; Model M 3 enhances this
9: a( Di) ∼π D(i)(·|o′( Di)) ▷ Call decision-making policy baselineusingastate-of-the-artapproach,lettingagentspre-
dict the actions of others using their own policies—a variant
10: a D ←(a D,a( Di)) ▷ Incrementally store actions of opponent modeling inspired by [15]. Since agents share
11: end for policy parameters in our experiments, they can perfectly
12: return a D model others, hence termed perfect opponent modeling.
Model M uses XP-MARL with random priorities to assess
4
the effectiveness of the priority-assignment policies learned
in Model M . Model M also uses XP-MARL but injects
1 5
noise into communicated actions, modeled by a normal
distribution with a variance of 10% of the maximum action
values, to test XP-MARL’s robustness.
B. Testing Process
After training, we test the models on the entire map with
15 agents rather than in the original training environment.
This way, we also challenge the learned policies’ ability to
generalizetounseenenvironments.Weconduct32numerical
experiments for each model, with each lasting one minute,
Fig. 3: CPM scenario. Train only on the intersection (gray corresponding to 1200 time steps since each time step spans
area) with 4 agents. Test on the entire map with 15 agents. 50ms. For each experiment, we evaluate two performance
metrics:1)collisionrate,whichistheproportionofthetime
steps involving a collision, and 2) relative average speed,
lane intersection, a loop-shaped highway, and multiple on- which is the average speed of all agents relative to the
and off-ramps, presenting diverse challenging traffic scenes. maximum allowed speed. These two metrics correspond to
Our open-source repository1 contains code for reproducing traffic safety and efficiency, respectively.
experimental results and a video demonstration.
C. Experimental Results
We detail training and testing processes in Sec. IV-A and
IV-B,presentandinterpretexperimentalresultsinSec.IV-C Figure 4a depicts the episode mean reward during train-
and IV-D. ing. Model M 1 secures the highest reward, indicating the
best learning efficiency. This suggests that our framework
A. Training Process effectively mitigates non-stationarity in our experiments. All
We conduct training in the intersection area of the CPM other four models demonstrate similar learning efficiency,
scenario with four vehicles, visualized by the gray area with Model M 3 exhibiting slightly worse performance.
in Fig. 3. Each vehicle is equipped with two learning Figure 4b presents the collision rate during testing.
agents: one for the priority-assignment policy and one for The baseline model, M 2, exhibits a median collision
the decision-making policy. We employ Multi-Agent Proxi- rate of 1.09%. Remarkably, Model M 1, trained with our
mal Policy Optimization (MAPPO) [8]—a standard MARL XP-MARL,lowersthisrateby84.4%,outperformingModel
M , which uses perfect opponent modeling and achieves an
3
1github.com/cas-lab-munich/sigmarl improvementofonly12.8%.Theunsatisfactoryperformancehigh
2 priority
1 2
2
1
1
low
priority
(a) Approaching (b) Switching (c) Agent 2
an on-ramp. priorities. yields.
Fig. 5: Two agents avoiding a collision at an on-ramp by
dynamically switching their priorities.
(a) Episode mean reward during training. Smoothed with a sliding
aggressive policy. Both agents will predict that the other
window spanning five episodes.
will go straight and decide to evade, leading to either a
2 collision if they evade to the same side or a suboptimal
reward if they evade to different sides. In contrast, using
1 our XP-MARL, even with random priorities, allows them to
achieve the optimal reward in all cases. This may explain
Our: 0.17%
why Models M and M outperform Model M in terms of
1 4 3
0
M1 M2 M3 M4 M5 safety in our experiments.
Figure 5 exemplifies two agents approaching an on-ramp
(b) Collision rate during testing.
during our experiments. Before entering the on-ramp, agent
100 1 holds a lower priority than agent 2. Upon entering the on-
ramp, they switch priorities, making agent 2 responsible for
90 Our: 86.63%
collisionavoidance.Thisway,agent2sacrificesitsindividual
80 short-term reward for a better team benefit.
70 V. CONCLUSIONS
M1 M2 M3 M4 M5
In our work, we proposed an open-source framework
(c) Relative average speed during testing.
named XP-MARL, which augmented MARL with auxiliary
Fig.4:Trainingcurvesandtestingresultsof32experiments. prioritization and mitigated the non-stationarity challenge
in MARL. It was founded upon our hypothesis that pri-
oritizing agents and letting higher-priority agents establish
of Model M 3 may result from its inability to handle non- their actions first would mitigate this challenge. It incorpo-
stationarity as effectively as our framework, which we will rates a mechanism called action propagation, which prop-
elaborate on in Sec. IV-D. Further, Model M 4 improves the agates the actions of high-priority agents, thus stabilizing
baselineby22.0%,suggestingthatincorporatingourprioriti- the environment for lower-priority agents. Instead of using
zation scheme in MARL can mitigate non-stationarity, even predefined or heuristic priority assignment, it jointly learns
with random priorities. Additionally, Model M 1 surpasses priority-assignment policies and decision-making policies.
Model M 4, demonstrating effective learning of priority- We evaluated its effectiveness with SigmaRL—an MARL-
assignment policies. Moreover, even with 10% communi- based motion-planning framework for CAVs. It improved
cationnoise,ModelM 5 stillreducesthecollisionrateofthe thesafetyofabaselinemodelby84.4%andoutperformeda
baseline by 35.8% and outperforms Model M 3, validating state-of-the-artapproachemployingperfectopponentmodel-
the robustness of our approach. ing,whichimprovedthebaselinebyonly12.8%.Moreover,
Figure 4c depicts the average speeds, reflecting traffic even with 10% communication noise, it still maintained an
efficiency.Withreducednon-stationarity,theagentsinModel improvement of 35.8%, demonstrating its robustness. These
M 1 have learned to brake properly to avoid collisions rather results validated our hypothesis and suggested that prioritiz-
thanrecklesslycruiseatahighspeed.Asaresult,ModelM 1 ing agents might be a promising approach to mitigating the
achieves the lowest traffic efficiency. Nevertheless, it only non-stationarity challenge in MARL.
marginally reduces traffic efficiency by 2.2% compared to The sequential planning scheme of our XP-MARL may
the baseline, while improving its safety by 84.4%. resultinprolongedidletimesforlower-priorityagents,since
they need to wait for the communicated actions of higher-
D. Interpretation of the Experimental Results
priority agents, potentially limiting its applicability to large-
Recall the navigation example in Fig. 1. Assume agents scale systems. In future work, we plan to integrate our
can perfectly predict the actions of their teammates, as they previous works [40] and [41], which leverage graph theory
do in Model M . They may still fail to secure the optimal and reachability analysis, into XP-MARL to address this
3
reward: if at the current time step, they follow the same limitation.
]%[
etar
noisilloC
]%[
.dps
.gva
.leRREFERENCES [24] M. Lanctot, V. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls,
J. Pe´rolat, D. Silver, and T. Graepel, “A unified game-theoretic
[1] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multi- approach to multiagent reinforcement learning,” Advances in Neural
agent,reinforcementlearningforautonomousdriving,”arXivpreprint
InformationProcessingSystems,vol.30,2017.
arXiv:1610.03295,2016.
[25] D.Bertsekas,“Multiagentreinforcementlearning:Rolloutandpolicy
[2] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, iteration,”IEEE/CAAJournalofAutomaticaSinica,vol.8,no.2,pp.
D.Silver,andD.Wierstra,“Continuouscontrolwithdeepreinforce-
249–272,2021.
mentlearning,”arXivpreprintarXiv:1509.02971,2019.
[26] C. Daskalakis, D. J. Foster, and N. Golowich, “Independent policy
[3] O.Vinyals,I.Babuschkin,W.M.Czarnecki,M.Mathieu,A.Dudzik, gradient methods for competitive reinforcement learning,” Advances
J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev et al., in Neural Information Processing Systems, vol. 33, pp. 5527–5540,
“Grandmaster level in StarCraft II using multi-agent reinforcement
2020.
learning,”Nature,vol.575,no.7782,pp.350–354,2019.
[27] M. Sayin, K. Zhang, D. Leslie, T. Basar, and A. Ozdaglar, “Decen-
[4] K.TuylsandG.Weiss,“Multiagentlearning:Basics,challenges,and tralizedQ-learninginzero-sumMarkovgames,”AdvancesinNeural
prospects,”AIMagazine,vol.33,no.3,pp.41–41,2012.
InformationProcessingSystems,vol.34,pp.18320–18334,2021.
[5] P. Hernandez-Leal, M. Kaisers, T. Baarslag, and E. M. De Cote, “A
[28] H. Nekoei, A. Badrinaaraayanan, A. Sinha, M. Amini, J. Rajendran,
survey of learning in multiagent environments: Dealing with non-
A. Mahajan, and S. Chandar, “Dealing with non-stationarity in de-
stationarity,”arXivpreprintarXiv:1707.09183,2017.
centralized cooperative multi-agent deep reinforcement learning via
[6] G.Papoudakis,F.Christianos,A.Rahman,andS.V.Albrecht,“Deal- multi-timescale learning,” in Proceedings of The 2nd Conference on
ingwithnon-stationarityinmulti-agentdeepreinforcementlearning,” LifelongLearningAgents,Nov.2023,pp.376–398.
arXivpreprintarXiv:1906.04737,2019.
[29] J.N.Foerster,Y.M.Assael,N.deFreitas,andS.Whiteson,“Learning
[7] V. Konda and J. Tsitsiklis, “Actor-critic algorithms,” Advances in
to communicate to solve riddles with deep distributed recurrent Q-
NeuralInformationProcessingSystems,vol.12,1999.
networks,”arXivpreprintarXiv:1602.02672,2016.
[8] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mor-
[30] A.Singh,T.Jain,andS.Sukhbaatar,“Learningwhentocommunicate
datch, “Multi-agent actor-critic for mixed cooperative-competitive at scale in multiagent cooperative and competitive tasks,” arXiv
environments,” Advances in Neural Information Processing Systems, preprintarXiv:1812.09755,2018.
vol.30,2017. [31] S.Sukhbaatar,R.Fergusetal.,“Learningmultiagentcommunication
[9] R. E. Wang, M. Everett, and J. P. How, “R-MADDPG for partially with backpropagation,” Advances in Neural Information Processing
observableenvironmentsandlimitedcommunication,”arXivpreprint
Systems,vol.29,2016.
arXiv:2002.06684,2020.
[32] D. Chen, M. R. Hajidavalloo, Z. Li, K. Chen, Y. Wang, L. Jiang,
[10] H. Peng and X. Shen, “Multi-agent reinforcement learning based
andY.Wang,“Deepmulti-agentreinforcementlearningforhighway
resourcemanagementinMEC-andUAV-assistedvehicularnetworks,” on-ramp merging in mixed traffic,” IEEE Transactions on Intelligent
IEEE Journal on Selected Areas in Communications, vol. 39, no. 1, TransportationSystems,vol.24,no.11,pp.11623–11638,2023.
pp.131–141,2020.
[33] E.Hansen,D.Bernstein,andS.Zilberstein,“Dynamicprogramming
[11] S. Li, Y. Wu, X. Cui, H. Dong, F. Fang, and S. Russell, “Robust forpartiallyobservablestochasticgames,”AAAIWorkshop-Technical
multi-agent reinforcement learning via minimax deep deterministic Report,062004.
policygradient,”inProceedingsoftheAAAIconferenceonartificial
[34] P.Hernandez-Leal,B.Kartal,andM.E.Taylor,“Asurveyandcritique
intelligence,vol.33,no.01,2019,pp.4213–4220.
of multiagent deep reinforcement learning,” Autonomous Agents and
[12] D.Billings,D.Papp,J.Schaeffer,andD.Szafron,“Opponentmodel- Multi-AgentSystems,vol.33,no.6,pp.750–797,Nov.2019.
inginpoker,”AAAI/IAAI,vol.493,no.499,p.105,1998.
[35] K. Zhang, Z. Yang, and T. Bas¸ar, “Multi-agent reinforcement learn-
[13] H. He, J. Boyd-Graber, K. Kwok, and H. Daume´ III, “Opponent ing: A selective overview of theories and algorithms,” Handbook of
modelingindeepreinforcementlearning,”inInternationalConference
ReinforcementLearningandControl,pp.321–384,2021.
onMachineLearning,2016,pp.1804–1813.
[36] X. Wang and T. Sandholm, “Reinforcement learning to play an
[14] Z.-W. Hong, S.-Y. Su, T.-Y. Shann, Y.-H. Chang, and C.-Y. Lee, optimalNashequilibriuminteamMarkovgames,”AdvancesinNeural
“A deep policy inference Q-network for multi-agent systems,” in InformationProcessingSystems,vol.15,2002.
Proceedings of the 17th International Conference on Autonomous
[37] P. Scheffe, G. Dorndorf, and B. Alrifaee, “Increasing Feasibility
AgentsandMultiAgentSystems,2018,p.1388–1396.
withDynamicPriorityAssignmentinDistributedTrajectoryPlanning
[15] R. Raileanu, E. Denton, A. Szlam, and R. Fergus, “Modeling others for Road Vehicles,” in IEEE International Conference on Intelligent
using oneself in multi-agent reinforcement learning,” in Proceedings TransportationSystems(ITSC),2022,pp.3873–3879.
ofthe35thInternationalConferenceonMachineLearning,Jul.2018,
[38] J.Xu,P.Hu,andB.Alrifaee,“SigmaRL:Asample-efficientandgen-
pp.4257–4266.
eralizable multi-agent reinforcement learning framework for motion
[16] X.Yu,J.Jiang,W.Zhang,H.Jiang,andZ.Lu,“Model-basedoppo- planning,”arXivpreprintarXiv:2408.07644,2024.
nentmodeling,”AdvancesinNeuralInformationProcessingSystems,
[39] M.Kloock,P.Scheffe,J.Maczijewski,A.Kampmann,A.Mokhtarian,
vol.35,pp.28208–28221,Dec.2022.
S. Kowalewski, and B. Alrifaee, “Cyber-physical mobility lab: An
[17] C.ZhangandV.Lesser,“Multi-agentlearningwithpolicyprediction,”
open-source platform for networked and autonomous vehicles,” in
in Proceedings of the AAAI Conference on Artificial Intelligence, 2021EuropeanControlConference(ECC),2021,pp.1937–1944.
vol.24,no.1,2010,pp.927–934.
[40] P. Scheffe, J. Xu, and B. Alrifaee, “Limiting computation levels
[18] J. N. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, in prioritized trajectory planning with safety guarantees,” in 2024
andI.Mordatch,“Learningwithopponent-learningawareness,”arXiv
EuropeanControlConference(ECC),2024,pp.297–304.
preprintarXiv:1709.04326,2017.
[41] P. Scheffe, J. Kahle, and B. Alrifaee, “Reducing computation time
[19] A.Tampuu,T.Matiisen,D.Kodelja,I.Kuzovkin,K.Korjus,J.Aru, with priority assignment in distributed mpc,” TechRxiv preprint
J.Aru,andR.Vicente,“Multiagentcooperationandcompetitionwith TechRxiv:10.36227/techrxiv.20304015.v2,2023.
deepreinforcementlearning,”PloSOne,vol.12,no.4,p.e0172395,
2017.
[20] J.Foerster,N.Nardelli,G.Farquhar,T.Afouras,P.H.Torr,P.Kohli,
and S. Whiteson, “Stabilising experience replay for deep multi-
agentreinforcementlearning,”inInternationalConferenceonMachine
Learning,2017,pp.1146–1155.
[21] C.S.DeWitt,T.Gupta,D.Makoviichuk,V.Makoviychuk,P.H.Torr,
M.Sun,andS.Whiteson,“Isindependentlearningallyouneedinthe
StarCraft multi-agent challenge?” arXiv preprint arXiv:2011.09533,
2020.
[22] G. W. Brown, “Iterative solution of games by fictitious play,” Act.
Anal.ProdAllocation,vol.13,no.1,p.374,1951.
[23] J. Heinrich, M. Lanctot, and D. Silver, “Fictitious self-play in
extensive-form games,” in International Conference on Machine
Learning,2015,pp.805–813.