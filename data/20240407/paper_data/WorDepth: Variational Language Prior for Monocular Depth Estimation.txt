WorDepth: Variational Language Prior for Monocular Depth Estimation
ZiyaoZeng1 DanielWang1 FengyuYang1 HyoungseobPark1 YangchaoWu2
StefanoSoatto2 Byung-WooHong3 DongLao2 AlexWong1
1YaleUniversity 2UniversityofCalifornia,LosAngeles Chung-AngUniversity3
1{ziyao.zeng, daniel.wang.dhw33, fengyu.yang, hyoungseob.park, alex.wong}@yale.edu
2 wuyangchao1997@g.ucla.edu 2{soatto,lao}@cs.ucla.edu 3hong@cau.ac.kr
Abstract
Infinitely many 3D scenes
Three-dimensional(3D)reconstructionfromasingleim-
age is an ill-posed problem with inherent ambiguities, i.e.
scale. Predicting a 3D scene from text description(s) is
Metric-scale depth map
similarlyill-posed,i.e. spatialarrangementsofobjectsde-
scribed. Weinvestigatethequestionofwhethertwoinher-
ently ambiguous modalities can be used in conjunction to
produce metric-scaled reconstructions. To test this, we fo- Infinitely many 3D scenes
cusonmonoculardepthestimation,theproblemofpredict-
ing a dense depth map from a single image, but with an
additional text caption describing the scene. To this end,
webeginbyencodingthetextcaptionasameanandstan-
darddeviation;usingavariationalframework,welearnthe
distribution of the plausible metric reconstructions of 3D
Figure 1. Language as a prior for depth estimation. Depth
scenes corresponding to the text captions as a prior. To
estimationfromasingleimageisanill-posedproblem(i.e.,scale),
“select”aspecificreconstructionordepthmap,weencode
andlikewisefromtextcaptions(i.e.,layout). Cantwoinherently
thegivenimagethroughaconditionalsamplerthatsamples
ambiguousmodalitiesresolvemetric-scaleddepthestimates?
fromthelatentspaceofthevariationaltextencoder,which
is then decoded to the output depth map. Our approach is
guity,suchasthescaleofthereconstruction. Consequently,
trainedalternatinglybetweenthetextandimagebranches:
inductionisnecessary,anddepthestimationbecomesdraw-
inoneoptimizationstep,wepredictthemeanandstandard
ing a scene with maximum likelihood from the distribu-
deviationfromthetextdescriptionandsamplefromastan-
tionofallpossiblescenes,conditionedontheimage. This
dard Gaussian, and in the other, we sample using a (im-
conditional scene distribution is learned by a deep neural
age)conditionalsampler. Oncetrained,wedirectlypredict
network on a chosen training set. While an ideal training
depthfromtheencodedtextusingtheconditionalsampler.
setshouldaccuratelyreflectthisdistribution,practicalchal-
Wedemonstrateourapproachonindoor(NYUv2)andout-
lenges arise due to the scarcity of well-established large-
door(KITTI)scenarios, whereweshowthatlanguagecan
scale depth datasets. A crucial question arises: Can any
consistentlyimproveperformanceinboth. Code: https:
priors, otherthanthetrainingset, beleveragedtocalibrate
//github.com/Adonis-galaxy/WorDepth.
thelearnedscenedistributiontotruereal-worldstatistics?
Thesepriorsmaycomeinmanyforms,fromgenericpri-
ors such as local smoothness and connectivity [19, 22, 67,
1.Introduction
102] or object orientation [15] that may be imposed as a
Theprocessofimagingisasurjectionfroma3Dscene partofthetrainingobjective(regularizer)tospecificinduc-
tothe2Dimagedomain, whereinfinitelymany3Dscenes tivebiasesrealizedasarchitecturaldesigns(layers)[65]or
canmaptothesameimage. Itsinverseproblem,estimating a collection object shapes [14]. While generic priors are
the3Dscenestructurefromasingleimage,i.e.,monocular suitable for a wide variety of scenes, they typically lack
depthestimation,isthereforeill-posedwithinherentambi- specificity, i.e., size or shape of objects within a specific
4202
rpA
4
]VC.sc[
1v53630.4042:viXra3Dscene. Ontheotherhand,specificnetworkdesignsmay processbetweenthe(text-)VAEandconditionalsampler.
backfire when the assumption motivating the design does Inonealternation,onewouldpredictthemeanandstan-
not hold, i.e., using specifics about camera parameters for darddeviationfromthetextcaptionandoptimizethetext-
reconstruction. We consider a more flexible source of pri- VAE branch for depth by minimizing a loss with respect
ors – language – that is closely tied to semantics, and of- togroundtruthonthedepthmapsampledusingastandard
ten shape (and functionality) [4, 31, 32]. Consider a text Gaussian (similar to traditional VAEs). In the other alter-
description of “A bedroom with a bed and a table” as in nation,onewouldstillusethemeanandstandarddeviation
Fig.1: Onecanimagineaprobable3Dscenecontaininga predictedbythetext-VAE,butinstead, usetheconditional
bed and a table as the primary objects. In fact, there exist sampler to “select” a specific depth map compatible with
infinitelymany3Dscenescompatiblewiththedescription, theimage,andagain,minimizealossontheoutputdepth.
asthereareambiguitiesintermsofthescenelayoutandthe Note:thatdependingonthealternation,eitherthetext-VAE
precise shape of the bed and table. Yet, one may surmise or the conditional sampler is frozen. At test-time, one no
that the scale of the scene is closely related to the objects longerneedstosamplefromtheGaussianandmaydirectly
(andtheirtypicalsizes)populatingit. Thislendstoaprior predictdepthusingthetext-VAEwiththeconditionalsam-
thatisspecificforagivenscene,yet,genericenoughwith- pler (see Fig. 2). In another mode, one may use the text-
outassumptionsonthecamerausedortheshapeswithinthe VAEalonetogenerateplausiblescenesforagivencaption.
imaged3Dscene. Ourcontributionsareasfollows:(i)Weproposeavari-
Hence,thequestionathandbecomeswhethertwoinher- ational framework that leverages complementary strengths
entlyambiguousmodalities(cameraimageandtextdescrip- of two inherently ambiguous modalities for monocular
tions) can be exploited for their complementary strengths: depthestimation;wetermourapproach,WorDepth.(ii)We
Intheimage,onecanobservethelayoutandobjectshapes introduce an image-based conditional sampler that models
populating the 3D scene; in a text caption, one has strong theuseoflanguageasaconditionalprior. (iii)Weachieve
priorsaboutthescale(andcoarseshapes)ofthescene. Our the state-of-the-art on indoor (NYU Depth V2 [58]) and
workaimstoresolvetherespectiveambiguitiesofthetwo outdoor (KITTI [20]) benchmarks. (iv) To the best of our
modalities by using language to reduce the solution space knowledge,wearethefirsttotreatlanguageasavariational
toyieldmetric-scaledreconstructionsas2.5Ddepthmaps. priorformonoculardepthestimation.
To test the feasibility of this approach, we consider the
2.RelatedWork
ill-posed inverse problem of monocular depth estimation,
where one predicts a depth map from a single image. In- Monoculardepthestimation trainsbyminimizingloss
steadofusingjustanimage,wealsoassumeatextdescrip- between depth predictions and ground-truth depth maps
tionorcaptiondescribingthe3Dscenecapturedwithinthe [2,7,17,35,46,52,54,61,66,78,80,84,86].Specifically,
image. Note that we do not make any assumption regard- DORN [16] employs a spacing-increasing discretization
ingthesourceofthedescription, i.e., itcanbedictatedby strategyfordepthestimationasanordinalregressionprob-
humans or generated by a model. But for practicality, we lem. AdaBins [2] introduces a transformer block that seg-
useanimagecaptioner(ExpansionNetv2[25])togenerate ments the depth range into adaptive bins. ASTransformer
abrief,concisedescriptionoftheimage. [7] incorporates an Attention-based Up-sample Block to
Toexploittheinherentambiguityoftextcaptions,where enhance detailed texture features. DepthFormer [40] em-
asingledescriptioncangenerateinfinitelymany3Dscenes, ploys hierarchical aggregation and heterogeneous interac-
we choose to encode the caption using a variational auto- tion modules for effective feature affinity and modeling.
encoder (VAE) as a mean and standard deviation of the RPSF [47] presents a differentiable model of the aper-
plausible scene layout distribution. By sampling a noise ture mask. However, deriving semantics solely from vi-
vector from a standard Gaussian and using the reparame- sualcuesischallengingbecauseofscaleambiguityandthe
terization trick customary in VAEs, we can draw from the limited size of fully annotated training datasets. We use
latent distribution and decode it into a metric-scaled depth language as a prior to ground predictions to metric scale.
map. Yet, to choose a particular depth map amongst the When ground-truth depth is not available, self-supervised
many possible, one must rely on the image. This is facili- approaches [3, 15, 27, 36, 51, 62–64, 70, 85, 94, 96, 100]
tatedbyaconditionalsamplerthatpredictsthenoisevector rely on geometric constraints, often established via from
from the given image in place of the one sampled from a various modalities, including lidar [44, 50, 67–69, 72, 79]
Gaussian to be used in the reparameterization step. Con- and radar [59], or through deliberate design. Arising from
sequently, thissubstitutionenablesonetosamplethemost training,ifdoneatalargescale,isaprioronthescenethat
probabledepthmap,adheringtothescenearrangementand canbeexploitedforsemantictasks[33]. Ontheotherhand,
objectshapesobservedintheimage, fromthelearneddis- weconsiderlanguageasasemanticpriortoenhancetheef-
tribution.Thisnaturallylendstoanalternatingoptimization fectivenessofmonoculardepthestimation.Variational and generative methods focus on the am- abletokensinplaceofdiscretehuman-languagewords.Ad-
biguous nature of monocular depth estimation, many in- ditionally,VPD[97]exploitsthehigh-fidelityembeddingof
volving Diffusion or VAE models for modeling this ambi- a pre-trained text-to-image diffusion model in monocular
guity[5,10,41,56,57,73,83].DepthGen[56]usesadepth depthestimation. However,existingmethodsusingvision-
pre-traineddiffusionmodel,whichgeneratesdepthestima- language models rely on implicit modeling. Conversely,
tions conditioned on images, and shows that the model is WorDepth explicitly models language as a prior for depth
capable of generating multiple plausible depth maps when estimation and exploits strong priors regarding the size of
depth is ambiguous. DDVM [57] uses a similar approach objectsdescribedintextcaptionstobettergroundmonocu-
anddesignedatrainingpipelinethatcanproducebothdepth lardepth(oftenscaleless)tometricscale.
mapsandopticalflowoutputswithadiffusionmodel. [73]
trainedaVAEmodelthatoutputsaprobabilitydistribution 3.Method
over scene depth given an image, which can then be com-
Given an RGB image x : Ω ⊂ R2 → R3, monocu-
bined with additional inputs for more accurate depth esti-
lar depth estimation aims to infer a dense depth map y :
mations. VDN[10]modelsdepthasadistributionwithits
Ω ⊂ R2 → R using a parameterized function h realized
varianceinterpretedasuncertainty. TheCodeSLAMmodel +
as a neural network, i.e., y := h(·). We consider a super-
[5] also employed a VAE conditioned on image intensities
vised dataset D = {x(m),t(m),y∗(m)}M with M sam-
for depth estimation. However, although these work ex- m=1
ples, where y∗ : Ω ⊂ R2 → R denotesthe ground-truth
ploredtheideaofuncertaintyindepthestimation,andeven +
depthmap,andtthetextcaptiondescribingtheimage.
combined other modalities of inputs [73], none have ex-
perimentedwithlanguagepriors,andmostVAE-basedap-
3.1.Textvariationalauto-encoder
proachesuseimagestoobtainthemeanofthemodeleddis-
tribution,whichisfundamentallydifferentfromWorDepth. To incorporate language priors to monocular depth es-
timation, we first design a variational auto-encoder (VAE)
Foundationmodels[6,21,23,37,38,48,49,53,77,98,
to learn the latent distribution of possible depth maps as
104]acquireacomprehensiveunderstandingoflanguages,
described by the text caption. This VAE is comprised of
images, and other data types through pre-training under
thetextencoderfromapre-trainedvision-languagemodel,
substantial and diverse datasets, thus forming an effective
CLIP [53], which by default offers a shared latent space
baseline for downstream tasks [2, 8, 12, 39, 42, 71, 74–
between vision and text embeddings, followed by a multi-
76, 81, 82, 89, 92, 93, 95]. To leverage foundation mod-
layer perceptron (MLP) to estimate the mean µˆ ∈ Rd and
els for monocular depth estimation, TADP [30] uses cap-
standard deviation σˆ ∈ Rd of the latent distribution of
tionscreatedbyAItoenhancethecorrelationbetweentext
plausible scenes based on the text encoding. Note that the
and images in diffusion-based vision models. VPD [97]
CLIPtextencoderisfrozenatalltimesandneverupdated
leveragesadiffusion-basedpipelinewithcross-attentionbe-
when training WorDepth. Specifically, given a text cap-
tweentextandimages. Dinov2[48]trainsaViT[11]with
tiont = {t ,t ,...},wefirstencodeitusingtheCLIPtext
1B parameters using an automatically built image dataset 1 2
encoder and estimate the mean and standard deviation as
undercontrastivelearningobjectives. Unlikemethodsthat
(µˆ,σˆ) = g (t) ∈ R2×d using a multi-layer perceptron
relyonfoundationmodelsforfeatureextraction,WorDepth ψ
(MLP). To sample from the distribution parameterized by
ispotentiallymoreefficientforindustrialapplications.
µˆ and σˆ, we first draw a noise vector ϵ ∈ Rd from a stan-
Vision-language models are designed to build connec- dardGaussianϵ∼N(0,1). Then,weuseϵtosamplefrom
tions between visual and language inputs. CLIP [53] con-
thelatentdistributionviathereparameterizationtrick[29],
ducts contrastive learning between text-image pairs, em- zˆ= µˆ+ϵ·σˆ. Werefertothismoduleasatextvariational
powering various tasks like few-shot image classification auto-encoder (text-VAE). To generate a depth map yˆfrom
[18,87,88,101],imagesegmentation [55,99],objectde- thesamplezˆ∈Rd,wefirstduplicatezˆalongthehorizontal
tection [55,103], and3Dperception[26,90,91,105]. In andverticalaxestoyieldad×h×wlatent(choiceofde-
light of the powerful emerging ability brought by recent
signtobediscussedbelowinSec.3.2)andfeeditthrough
vision-language models, some works have tried to apply a depth decoder to yield yˆ = h (zˆ) ∈ RH×W, where we
ϕ +
thevision-languagemodelformonoculardepthestimation. overloadzˆasthespatiallyduplicatedlatent,andH andW
DepthCLIP [91] leverages the semantic depth response of
denotetheheightandwidthofthedepthmap,presetashy-
CLIP[53]withadepthprojectionschemetoconductzero-
perparameterstomatchthedesiredimagedimensions.
shot adaptation from the semantic language response to
Totrainourtext-VAEanddepthdecoder,weminimize
monocular depth estimation. Furthermore, [26] extends
DepthCLIPwithlearnablepromptsanddepthcodebookto L =L (y∗,yˆ)+α·L (µˆ,σˆ) (1)
VAE SI KL
narrowthedepthdomaingapamongdifferentscenes. Like-
wise,[1]modifiesDepthCLIP[91]usingcontinuouslearn- withrespecttoψandϕ,whereL isthescaleinvariantloss
SIAlternating witha ratiop Prior Depth
“A living room
with a couch and text-VAE 𝜇̂,𝜎+ ∈𝑅! × 𝑅! 𝑧̂ =𝜇̂ +𝜖*𝜎+ Depth
Decoder
a ceiling fan.” Ground Truth
TextDescription
Detached KL-Divergence 𝜖∼N(0,1) Sharedweights Pixel-wise
gradient with N(0,1) depth loss
Conditional 𝜖̃∈𝑅!×#×$ 𝑧̃ =𝜇̂ +𝜖̃*𝜎+ Depth
Sampler Decoder
RGBImage Predicted Depth
Figure2.TrainingWorDepth.Webeginwithoptimizingtext-VAEbypredictingthemeanandstandarddeviationofthelatentdistribution
ofdepthmapscorrespondingtoatextcaption.Wethensamplezˆfromthedistributionusingthereparameterizationtrickwithϵ∼N(0,1)
anddecodeitintoadepthmapforlosscomputation.Wethenoptimizeaconditionalsamplerbypredictingpatch-wiseϵ˜fromanimageto
samplez˜fromthelatenttoyieldoutputdepthforthelosscomputation.Thedepthdecoderisupdatedinbothalternatingsteps.
(Eq.(3)), L theKLdivergenceloss(Eq.(4))asdetailed samesize,butpopulatedwithzerosinstead.
KL
inSection3.3,andαtheweightoftheKLdivergenceterm. Totraintheconditionalsampler,weminimizethesame
loss(Eq.(1))asthatoftext-VAE:
3.2.Image-basedconditionalsampler
L =L (y∗,y˜)+β·L (µ˜,σ˜) (2)
CS SI KL
While our text-VAE can predict plausible metric-scaled
withrespecttoφandϕ. Withabatchsizeofb,thenumber
depth maps from text captions, we are interested in the
ofϵ˜isb×h×w,whileµ˜ andσ˜ arethesamplemeanand
depthmapcorrespondingtoaspecificimage. Todoso,we
standard deviation of ϵ˜over a batch. We impose a KL di-
treattext-VAEasthelatentpriordistributionoftheplausi-
vergencelossasregularizationsothattheestimatedϵ˜does
ble scene layouts. Predicting depth y˜for a specific image
not drift from the standard Gaussian, which also serves to
x requires sampling the latent corresponding to the depth
improvetrainingstability.
mapofthe3Dscenelayoutwiththehighestlikelihoodtobe
compatiblewiththeobservedimage,i.e.,priorconditioned 3.3.TrainingLoss
on the image. To this end, we introduce an image-based
Scaleinvariantloss. Weminimizeasupervisedlossus-
conditional sampler that will predict the sample ϵ˜in place
ing ground truth y∗. To improve training stability over di-
of ϵ ∼ N(0,1) drawn from the standard Gaussian. Using
versescenes,weusethescale-invariantdepthloss[13]:
thereparameterizationtrickasbefore,wewilluseϵ˜toselect
thelatentvectorz˜tobedecodedbythedepthdecoder. 1 (cid:88) γ (cid:88)
L (y,y∗)= e(i,j)2− ( e(i,j))2,
Specifically, our image-based conditional sampler uti- SI N N2
e e
lizes a Swin-L transformer backbone to encode an image (i,j)∈Ω (i,j)∈Ω
(3)
x ∈ R3×H×W. We chose this design to exploit the local-
wheree(i,j)=logy(i,j)−logy∗(i,j),Ωdenotestheim-
ityofthetokensproducedbySwin-L.Thetokensarethen
agespace, N thenumberofpixels, y thepredicteddepth,
encodedintoh×w numberoflocalsamplesϵ˜∈ Rd×h×w e
andγthescalingfactortocontrolthesensitivityoftheloss.
tobeusedtosamplefromthelatentdistributionofourtext-
Kullback-Leibler (KL) divergence loss. Following
VAE; in other words, we perform “patch-wise” selection
[29], we employ the KL Divergence loss as a regularizer,
from latent distribution for more granular predictions. To
which biases the predicted latent distribution (parameter-
doso, weadditionallyincludeµˆ andσˆ aspartofitsinput.
ized by mean µ and standard deviation σ) towards a stan-
Wenotethatµˆandσˆhavebeendetachedfromthecomputa-
dardGaussiandistribution. WeapplytheKullback-Leibler
tionalgraphandtreatedasinput. Werefertothismoduleas
divergencelosstoµandσasfollows:
ourconditionalsamplerϵ˜= f (x,µˆ,σˆ),whichaimstoes-
φ
timatethemostprobablelatentvariableoftext-VAE.Thus, σ2+µ2 1
L (µ,σ)=−log(σ)+ − . (4)
thescenelayoutlatentvectorisnowgivenbyz˜=µˆ+ϵ˜·σˆ, KL 2 2
andthepredicteddepthy˜ = h (z˜). Asanimplementation
ϕ 3.4.OptimizingWordepth
detail, we note that skip connections from the encoder f
φ
are injected into h by concatenation; when training text- Training Wordepth involves optimizing text-VAE with
ϕ
VAE(Sec.3.1),featuremapsofskipconnectionsareofthe our conditional sampler: One may choose to first trainImage Text Ours Ours Error Adabins Adabins Error Ground Truth
Depth
A store with Value (m)
chairs and
tables in it.
An unmade bed
in a bedroom
with a window.
A bathroom with a
toilet, a sink and a
shower curtain. Error
(Abs Rel)
A man standing
in a kitchen next
to a counter.
A classroom with
a group of desks.
Figure 3. Qualitative results on NYU Depth V2. We compare WorDepth with AdaBins [2]. Text descriptions are generated using
ExpansionNetv2[25]. Overall,WorDepthimprovesuniformlyacrosstheimage(darkerinerrormap),implyingbetterscale. WorDepth
alsopredictsmoreaccuratedepthinregionscorrespondingto“chairs”,“window”,“showercurtain”,“man”,and“desks”,whichareobjects
specifiedbytextdescriptions.Note:Zerosinthegroundtruthdepthmapindicatetheabsenceofvaliddepthvalues.
text-VAEuntilconvergence(i.e.,optimizeforψ∗,ϕ∗),then fromastandardGaussianinstead.
freezeψ∗,ϕ∗,andfinallytraintheimage-basedconditional
sample (i.e., optimize for φ∗). However, we find that do- 4.Experiments
ingsooftenresultsintheconditionalsamplerbeingtrapped
inasuboptimallocalminimum. Moreover, thisintroduces Datasets. We evaluate our method on indoor (NYU
theinconvenienceofanextrastageoftraining. Instead,we DepthV2[58])andoutdoor(KITTI[20])scenarios. NYU
proposeanalternatingoptimizationschemetotrainthetext- DepthV2contains480×640imageswithdepthvaluesfrom
VAEwithconditionalsampler. Inonealternatingstep, we 1×10−3 to10meters. Wefollow[34]forthedatasetpar-
freeze the conditional sampler and train the text-VAE and tition,whichcontains24,231trainimagesand654testim-
depthdecoderfollowingtheprocedureinSec.3.1,i.e.,pre- ages.KITTIcontains352×1216imageswheredepthvalues
dictingµˆ andσˆ fromtextcaptiontandusingthereparam- from1×10−3 to80meters. WeadopttheEigenSplit[13]
eterizationtrickwithanϵdrawnfromastandardGaussian consisting of 23,488 training images and 697 testing im-
tosamplethelatentvector. Inthenextalternatingstep,we ages. Following[2,86],aftercleaningoutsampleswithout
freezetext-VAEandtraintheconditionalsamplerwiththe validgroundtruth,wehave652validimagesfortesting.
depth decoder following Sec. 3.2, i.e., predicting µˆ and σˆ NetworkArchitecture. WeusetheResNet-50[24]ver-
using the frozen text-VAE and sample from the latent dis- sionofCLIP[53]textencodertoextracttextfeatures. We
tributionusingϵ˜predictedfromtheimage. Thesealternat- use ExpansionNet-v2 [25] for captioning images for effi-
ingstepsarerepeatedwitharatioofp(foroptimizingtext- ciency. We set the dimension d of the latent space of the
VAE)to1−p(foroptimizingtheconditionalsampler). text-VAE and image-based conditional sampler to be 128.
Asfortheimage-basedconditionalsampler,weuseaSwin-
Inference. Oncetrained, wenolongerrequiredrawing LTransformerbackbone[45]pre-trainedonImageNet[9].
ϵfromastandardGaussian. Instead,attesttime,theinfer- Forthetext-VAE,givenCLIPfeaturesofsize1024,weuse
encestepsimplyfollowsSec.3.2. Inanothermode,ifone a3-layerMLPwithhiddendimensionsof512,256,and128
wants to generate depth maps from text captions, one can to encode text features. For the depth decoder, there are 3
discardtheconditionalsamplerbranchanddirectlysample convolutionalup-samplingandrefinementlayers.FordepthMethod Backbone δ<1.25↑ δ<1.252 ↑ δ<1.253 ↑ AbsRel↓ log ↓ RMSE↓
10
DepthCLIP[91] CLIP(zero-shot) 0.394 0.683 0.851 0.388 0.156 1.167
CLIPMDE[1] CLIP 0.465 0.776 0.922 0.319 0.139 0.970
GeoNet[52] ResNet-50 0.834 0.960 0.990 0.128 0.057 0.569
DORN[16] ResNet-101 0.828 0.965 0.992 0.115 0.051 0.509
Yinetal.[80] ResNeXt-101 0.875 0.976 0.994 0.108 0.048 0.416
TransDepth[78] ViT-B 0.900 0.983 0.996 0.106 0.045 0.365
ASN[46] HRNet-48 0.890 0.982 0.996 0.101 0.044 0.377
BigtoSmall[35] DenseNet-161 0.885 0.978 0.994 0.110 0.047 0.392
DPT-Hybird[54] ViT-B 0.904 0.988 0.998 0.110 0.045 0.357
ASTransformer[7] ViT-B 0.902 0.985 0.997 0.103 0.044 0.374
AdaBins[2] EffNet-B5+ViT-mini 0.903 0.984 0.997 0.103 0.044 0.364
NeWCRFs[86] Swin-L 0.922 0.992 0.998 0.095 0.041 0.331
Yuetal.[84] Swin-L 0.921 0.990 0.998 0.093 0.040 0.331
DepthFormer[40] Swin-L 0.923 0.989 0.997 0.094 0.040 0.329
Baseline Swin-L 0.910 0.990 0.998 0.098 0.043 0.351
WorDepth Swin-L 0.932 0.992 0.998 0.088 0.038 0.317
%Improvement - +2.42% +0.02% +0.00% -10.20% -11.63% -9.69%
Table1.QuantitativeresultsonNYUDepthV2.ThebaselinemethodistodirectlytrainaSwin-Limageencoderandthedepthdecoder
withoutthehelpoflanguageprior.ImprovementreferstotheperformanceenhancementrelativetotheBaseline.
prediction, we attach 3 skip connections from the condi- specific range. We note that while existing methods often
tionalsamplertothedepthdecoderbetweencorresponding produce high fidelity shapes (i.e., ordinal relationships of
layers. When optimizing for text-VAE by our alternating points)indepthmaps,thescaletendstobeoff–leadingto
optimization scheme (Sec. 3.4), we sample ϵ ∼ N(0,1) lowerδ < 1.25. Ourgainintheδ < 1.25metricindicates
from a standard Gaussian; as an implementation detail, all thatagreaterproportionofdepthestimationsalignclosely
valuespassedfromtheskipconnectionsaresettobezero. withthegroundtruth,thankstobetterscaleestimatedbased
Hyperparameters. We use the Adam [28] optimizer on objects that populate the scene, thereby yielding depth
without weight decay. The learning rate is reduced from valuesinrangesclosertothatofgroundtruth.
3 × 10−5 to 1 × 10−5 by a cosine learning rate sched- Tab.2showstheresultsontheKITTIdataset,usingthe
uler. The model is trained for 50 epochs for both KITTI Eigen Split [13] partition. WorDepth also achieves state-
and NYU Depth V2 under this scheduler. γ for scale- of-the-art performance. Like NYU Depth V2, WorDepth
invariantlossissetto0.85,andtheweightsαandβforKL- improvesthethresholdaccuracyδ <1.25,however,therel-
Divergencearesetto1×10−3. Wesettheprobabilitypto ativeperformancegainonthismetricisnotaspronounced
optimizingtext-VAEbranchto1%. Dataaugmentationin- as on NYU Depth V2. This difference can be due to the
cludesrandomgammawithin[0.9,1.1],randombrightness wider range of object sizes and shapes that may populate
within [0.75,1.25] for NYU Depth V2 [58] and [0.9,1.1] anoutdoorscenethatareattributedtothesameequivalence
forKITTI[20],randomcolorintensitywithin[0.9,1.1]for class of an object. For example, the term “car” may refer
eachchannel, randomhorizontalflippingwith50%proba- to a sedan, a coupe, or a hatchback – all exhibit different
bility,andrandomrotationswithin[−2.5,2.5]degrees. sizes (coupes are smaller than sedans) and shapes (hatch-
Evaluation metrics. Following [7, 43], we evaluate backshaveanelevatedandconnecttrunk). Whiletextcap-
WorDepthandbaselinemethodsquantitativelyusingmean tionsgiveflexibilitybetweengeneralityandspecificityasa
absolute relative error (Abs Rel), root mean square error prior,incaseswherecaptionstendtobevague,theexplicit
(RMSE), absolute error in log space (log ), logarithmic reliance(bymodelingasaconditionalprior)onthemmay
10
root mean square error (RMSE ) and threshold accuracy backfire,leadingtoincorrectshapesandsizes.Nonetheless,
log
(δ ). The evaluation metrics are summarized in the Supp. conditioning on the image resolve such cases to a degree
i
Mat. Forqualitativeresultsandcomparisons,seeFig.3and andusageofthepriorleadstomorebenefitsthanharm.
4,wheretheerrormapshowstheabsoluterelativeerror. Qualitativecomparisons. Weshowrepresentativevi-
Quantitative results. We show results on NYU Depth sualexamplescomparingWorDepthwithabaselinemethod
V2 in Tab. 1, where we improve over the baseline and AdaBins [2] on the NYU Depth V2 dataset in Fig. 3, to
existing works across all evaluation metrics. We want to highlightthebenefitofthelanguageprior.
highlightthatWorDepthsignificantlyexcelsintermsofthe From the error map where brighter regions indicate
threshold accuracy δ < 1.25, which measures the propor- largererrors,itisevidentthatWorDepthpredictsmoreac-
tionofpredictionsdeviatingfromthegroundtruthwithina curate depth for objects mentioned in the text description,Depth
Image Value (m)
An empty street with trees A red truck is driving down a A group of cars parked in
Text
and a wall. road with trees. front of a large building.
Ours
Ours
Error
Error
(Abs Rel)
AdaBins
AdaBins
Error
Ground
Truth
Figure4. VisualizationofdepthestimationsonKITTI.ComparedwithAdaBins[2],WorDepthimprovesuniformlyacrosstheimage
(darker in error map), implying better scale. WorDepth also predicts more accurate depth in regions corresponding to “wall”, “trees”,
“building”,whichareobjectsspecifiedbytextdescriptions.Note:Zerosingroundtruthdepthindicatetheabsenceofvaliddepthvalues.
like “chairs and tables” in the first row, “a window” in the specificrangewhileignoringitwhenitfallswithinanother
second row, “a shower curtain” in the third row, “a man” range. Theresultingpriorembeddedinthetextdescription
in the fourth row, and “a group of desks” in the last row. mayconveymorescaleinformationthaninitiallyapparent.
Note that errors maps of WorDepth shows improvement Optimizing with different alternation ratios. As a
uniformly across the image regions; this implies that our sensitivitystudy,weinvestigatehowdifferentratiosofalter-
methodestimatesabetterscalethanexistingones,thanksto natingoptimizationstepsbetweentext-VAEandconditional
priorsaboutobjectsize(andcoarseshapes)fromtextcap- sampler have an effect on the performance of WorDepth.
tions. Knowingthatacertainobjectexistswithinanimage We find that optimizing text-VAE with a lower ratio will
reducestheproblemto“placing”theobjectinthe3Dscene lead to a more deterministic model, which is anticipated.
based on its shape and location in the image. We showed On the other hand, optimizing text-VAE more frequently
thatscalecanbeinferredfromlanguage,whichcannarrow enables the model to learn a better variational prior on the
downthesolutionspaceofdepthprediction,leadingtoim- depthmapscorrespondingtotextcaptions,whichfacilitates
provedaccuracy. the generation of diverse prior depth maps. However, this
A similar pattern is also evident in KITTI (Fig. 4). Ex- comes at the cost of training time as the conditional sam-
amplesincludeimprovedaccuracyfor“awall”showninthe pler takes more steps to converge and, given a fixed num-
firstcolumn,“trees”inthesecondcolumn,and“agroupof ber of steps, results in more blurry predictions. We iden-
cars”alongside“alargebuilding”inthelastcolumn. This tifytheratioat1%inupdatingtext-VAEtobethebestem-
observation is intriguing because, for example, the text “a pirically (Tab. 3). Ratios exceeding 10% notably degrades
wall” is ambiguous by itself, especially in outdoor scenes, performancegivenafixedtraininglengthbecauseoffewer
wherethewallcouldbeanysizeordistanceawayfromthe updates to the sampler. Note that at 100%, where we do
camera, 1 or 100 meters. However, the text description of not condition the image, caption to depth generation still
a scene, either from a human annotator or a deep neural yieldsreasonableresultsasthetextcaptionsproduceplausi-
network, inherently carries biases that emphasize “a wall” blestatisticsthatmatchthegroundtruthdepth.Ontheother
when its size (tall or wide enough) or depth falls within a hand, withoutthemodelinglanguageasavariationalpriorMethod Backbone δ<1.25↑ δ<1.252 ↑ δ<1.253 ↑ AbsRel↓ RMSE ↓ RMSE↓
log
CLIPMDE[1] CLIP 0.550 0.830 0.938 0.303 0.119 6.322
DORN[16] ResNet-101 0.932 0.984 0.995 0.072 0.120 2.727
Yinetal.[80] ResNeXt-101 0.938 0.990 0.998 0.072 0.117 3.258
TransDepth[78] ViT-B 0.956 0.994 0.999 0.064 0.098 2.755
BigtoSmall[35] DenseNet-161 0.955 0.993 0.998 0.060 0.096 2.798
DPT-Hybird[54] ViT-B 0.959 0.995 0.999 0.062 0.092 2.573
ASTransformer[7] ViT-B 0.963 0.995 0.999 0.058 0.089 2.685
AdaBins[2] EffNet-B5+ViT-mini 0.964 0.995 0.999 0.058 0.089 2.360
NeWCRFs[86] Swin-L 0.974 0.997 0.999 0.052 0.079 2.129
Yuetal.[84] Swin-L 0.972 0.996 0.999 0.054 0.081 2.134
DepthFormer[40] Swin-L 0.975 0.997 0.999 0.052 0.079 2.143
Baseline Swin-L 0.969 0.996 0.999 0.054 0.085 2.343
WorDepth Swin-L 0.979 0.998 0.999 0.049 0.074 2.039
%Improvement - +1.03% +0.20% +0.00% -9.26% -12.94% -12.97%
Table2. QuantitativeresultsonKITTIEigenSplit. ThebaselinemethodistodirectlytrainaSwin-Limageencoderandthedepth
decoderwithoutthehelpoflanguageprior.ImprovementistherelativeperformancegaincomparedwiththeBaseline.
p δ<1.25↑δ<1.252↑δ<1.253↑AbsRel↓log ↓RMSE↓ Method δ<1.25↑δ<1.252↑δ<1.253↑AbsRel↓log ↓RMSE↓
10 10
0% 0.929 0.990 0.998 0.091 0.039 0.323 Adabins 0.771 0.944 0.983 0.159 0.068 0.476
1% 0.932 0.992 0.998 0.088 0.038 0.317 DepthFormer 0.815 0.970 0.993 0.137 0.059 0.408
10% 0.930 0.991 0.998 0.090 0.039 0.322 Baseline 0.803 0.965 0.990 0.141 0.062 0.427
50% 0.763 0.942 0.987 0.163 0.068 0.527 WorDepth 0.833 0.976 0.994 0.123 0.054 0.376
90% 0.642 0.906 0.975 0.211 0.089 0.687
100% 0.590 0.889 0.973 0.225 0.097 0.746 Table4. Zero-shotgeneralizationtoSUN-RGBD.Themodels
aretrainedontheNYUDepthV2andtestingontheSun-RGBD
Table3. Sensitivitytodifferentratiosofalternatingoptimiza- withoutanyfine-tuning.
tionstepsbetweentext-VAEandconditionalsampleronNYU
DepthV2. pdenotesprobabilityofoptimizingtext-VAE.While
morestepsspentontext-VAEwillyieldbettergenerativeresults, motionproblems.Theapproachisafirstinleveragingcom-
itcomesatthecostofslowerconvergenceforthesampler. plementary properties of two modalities with inherent am-
biguitiesforthe3Dreconstruction,toaddressthedeficitsin
(at0%,wherewetrainbothtext-VAEandconditionalopti- one another. We show that by exploiting the layout/scene
mizerjointlyasadirectmapfromsingleimageandcaption ambiguityinlanguageasastrengthviaourvariationalap-
todepth),performancedegradetodothelackoftheprior. proach, we can ground predictions to metric scale. This
Zero-shot Generalization. Given the smaller domain opens up new avenue in how one can address the issue of
gap in language descriptions across different scenes com- scalein3Dreconstructionaswellasprovideadirectframe-
pared to images, we conduct a zero-shot transfer experi- worktoextendingthemanyworksthatcurrentlyarelimited
ment to highlight our improved generalization ability. We torelativeorscalelessdepthpredictions.
train the model on the NYU Depth V2 [58] and test it
Limitations. Generic regularizers typically yield little
on the Sun-RGBD [60] without fine-tuning. As shown in
gains, but do little harm; specific regularizers can provide
Tab.4, WorDepthoutperformsbaselinemethodsbyasub-
larger boosts but are limited in their applications. While
stantial margin, indicating the transferability of language
usinglanguageasapriorgivesflexibilitybetweenthetwo,
priorswhichunderscorestherobustnessoftextdescriptions
specificity in the caption controls the degree of regulariza-
in handling scene variability. This suggests that language
tion imposed. Naturally, vague captions give little to no
descriptions may offer a more stable basis for generaliza-
information on object shape or size, so there is little to be
tionacrossdiversedatadomainsthandirectvisualsignals.
gained;specific,butincorrectcaptionsmaymisfire,barring
anymaliciousintent. AsWorDepthreliesonthequalityof
5.Discussion
thecaption,itissusceptibletoinaccuraciesstemmingfrom
descriptions provided by the image captioner. Its ease of
Inthisstudy,weseektoanswerthequestionofwhether
usealsoopensupvulnerabilitiesfrommalicioususerswho
languagecanbeusedtocalibratethelearnedscenedistribu-
maychoosecaptionstosteerpredictionsincorrectly.
tion to true real-world statistics. The answer is yes, which
is valuable for circumventing the long-standing problem Acknowledgments. This work was supported by NSF
of scale ambiguity in monocular depth or structure-from- 2112562.References [13] David Eigen, Christian Puhrsch, and Rob Fergus. Depth
map prediction from a single image using a multi-scale
[1] DylanAutyandKrystianMikolajczyk.Learningtoprompt
deepnetwork. Advancesinneuralinformationprocessing
clipformonoculardepthestimation:Exploringthelimitsof
systems,27,2014. 4,5,6,1
humanlanguage.InProceedingsoftheIEEE/CVFInterna-
[14] XiaohanFeiandStefanoSoatto. Visual-inertialobjectde-
tionalConferenceonComputerVision, pages2039–2047,
tectionandmapping. InProceedingsoftheEuropeancon-
2023. 3,6,8
ferenceoncomputervision(ECCV),pages301–317,2018.
[2] ShariqFarooqBhat,IbraheemAlhashim,andPeterWonka.
1
Adabins:Depthestimationusingadaptivebins.InProceed-
[15] Xiaohan Fei, Alex Wong, and Stefano Soatto. Geo-
ingsoftheIEEE/CVFConferenceonComputerVisionand
supervisedvisualdepthprediction. IEEERoboticsandAu-
PatternRecognition,pages4009–4018,2021. 2,3,5,6,7,
tomationLetters,4(2):1661–1668,2019. 1,2
8,1
[16] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
[3] Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Zhichao
manghelich, and Dacheng Tao. Deep ordinal regression
Li,LeZhang,ChunhuaShen,Ming-MingCheng,andIan
networkformonoculardepthestimation.InProceedingsof
Reid. Unsupervised scale-consistent depth learning from
theIEEEconferenceoncomputervisionandpatternrecog-
video. International Journal of Computer Vision, 129(9):
nition,pages2002–2011,2018. 2,6,8
2548–2564,2021. 2
[17] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
[4] IrvingBiedermanandGinnyJu.Surfaceversusedge-based
manghelich, and Dacheng Tao. Deep ordinal regression
determinantsofvisualrecognition. Cognitivepsychology,
networkformonoculardepthestimation.InProceedingsof
20(1):38–64,1988. 2
theIEEEconferenceoncomputervisionandpatternrecog-
[5] Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan nition,pages2002–2011,2018. 2
Leutenegger,andAndrewJDavison. Codeslam—learning
[18] PengGao, ShijieGeng, RenruiZhang, TeliMa, Rongyao
a compact, optimisable representation for dense visual
Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.
slam. InProceedingsoftheIEEEconferenceoncomputer
Clip-adapter: Better vision-language models with feature
visionandpatternrecognition,pages2560–2568,2018. 3
adapters. arXivpreprintarXiv:2110.04544,2021. 3
[6] MathildeCaron,HugoTouvron,IshanMisra,Herve´Je´gou,
[19] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian
Julien Mairal, Piotr Bojanowski, and Armand Joulin.
Reid. Unsupervisedcnnforsingleviewdepthestimation:
Emergingpropertiesinself-supervisedvisiontransformers.
Geometrytotherescue. InComputerVision–ECCV2016:
InProceedingsoftheIEEE/CVFinternationalconference
14th European Conference, Amsterdam, The Netherlands,
oncomputervision,pages9650–9660,2021. 3
October 11-14, 2016, Proceedings, Part VIII 14, pages
[7] Wenjie Chang, Yueyi Zhang, and Zhiwei Xiong. 740–756.Springer,2016. 1
Transformer-based monocular depth estimation with
[20] AndreasGeiger,PhilipLenz,andRaquelUrtasun. Arewe
attention supervision. In 32nd British Machine Vision
readyforautonomousdriving? thekittivisionbenchmark
Conference(BMVC2021),2021. 2,6,8,1 suite. In 2012 IEEE conference on computer vision and
[8] Jiaben Chen, Renrui Zhang, Dongze Lian, Jiaqi Yang, patternrecognition,pages3354–3361.IEEE,2012.2,5,6,
ZiyaoZeng,andJianboShi. iquery:Instrumentsasqueries 1
for audio-visual sound separation. In Proceedings of the [21] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat
IEEE/CVF Conference on Computer Vision and Pattern
Singh,KalyanVasudevAlwala,ArmandJoulin,andIshan
Recognition,pages14675–14686,2023. 3 Misra. Imagebind: Oneembeddingspacetobindthemall.
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, InProceedingsoftheIEEE/CVFConferenceonComputer
and Li Fei-Fei. Imagenet: A large-scale hierarchical im- VisionandPatternRecognition,pages15180–15190,2023.
agedatabase. In2009IEEEconferenceoncomputervision 3
andpatternrecognition,pages248–255.Ieee,2009. 5 [22] Cle´ment Godard, Oisin Mac Aodha, and Gabriel J Bros-
[10] Georgi Dikov and Joris van Vugt. Variational depth net- tow. Unsupervised monocular depth estimation with left-
works:Uncertainty-awaremonocularself-superviseddepth rightconsistency.InProceedingsoftheIEEEconferenceon
estimation. InEuropeanConferenceonComputerVision, computer vision and pattern recognition, pages 270–279,
pages43–60.Springer,2022. 3 2017. 1
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, [23] ZiyuGuo,RenruiZhang,XiangyangZhu,YiwenTang,Xi-
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, anzhengMa,JiamingHan,KexinChen,PengGao,Xianzhi
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Li, Hongsheng Li, et al. Point-bind & point-llm: Align-
Sylvain Gelly, et al. An image is worth 16x16 words: ing point cloud with multi-modality for 3d understand-
Transformersforimagerecognitionatscale.arXivpreprint ing, generation, andinstructionfollowing. arXivpreprint
arXiv:2010.11929,2020. 3 arXiv:2309.00615,2023. 3
[12] YimingDou,FengyuYang,YiLiu,AntonioLoquercio,and [24] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
AndrewOwens.Tactile-augmentedradiancefields.InPro- Deepresiduallearningforimagerecognition. InProceed-
ceedingsoftheIEEE/CVFConferenceonComputerVision ingsoftheIEEEconferenceoncomputervisionandpattern
andPatternRecognition,2024. 3 recognition,pages770–778,2016. 5[25] JiaChengHu,RobertoCavicchioli,andAlessandroCapo- and Cewu Lu. From isolated islands to pangea: Unify-
tondi. Expansionnet v2: Block static expansion in fast ingsemanticspaceforhumanactionunderstanding. arXiv
end to end training for image captioning. arXiv preprint preprintarXiv:2304.00553,2023. 3
arXiv:2208.06551,2022. 2,5 [40] ZhenyuLi, ZehuiChen, XianmingLiu, andJunjunJiang.
[26] XuetingHu,CeZhang,YiZhang,BowenHai,KeYu,and Depthformer: Exploiting long-range correlation and local
ZhihaiHe. Learningtoadaptclipforfew-shotmonocular informationforaccuratemonoculardepthestimation.arXiv
depthestimation. arXivpreprintarXiv:2311.01034,2023. preprintarXiv:2203.14211,2022. 2,6,8
3 [41] Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-
[27] PanJi,RunzeLi,BirBhanu,andYiXu. Monoindoor: To- Phuoc, Douglas Lanman, James Tompkin, and Lei Xiao.
wardsgoodpracticeofself-supervisedmonoculardepthes- Gaufre:Gaussiandeformationfieldsforreal-timedynamic
timation for indoor environments. In Proceedings of the novelviewsynthesis,2023. 3
IEEE/CVF International Conference on Computer Vision, [42] Yiqing Liang, Eliot Laidlaw, Alexander Meyerowitz, Sri-
pages12787–12796,2021. 2 nathSridhar,andJamesTompkin. Semanticattentionflow
[28] DiederikPKingmaandJimmyBa. Adam: Amethodfor fields for monocular dynamic scene decomposition. In
stochastic optimization. arXiv preprint arXiv:1412.6980, ProceedingsoftheIEEE/CVFInternationalConferenceon
2014. 6 ComputerVision(ICCV),2023. 3
[43] Ce Liu, Suryansh Kumar, Shuhang Gu, Radu Timofte,
[29] DiederikPKingmaandMaxWelling. Auto-encodingvari-
ational bayes. arXiv preprint arXiv:1312.6114, 2013. 3, and Luc Van Gool. Va-depthnet: A variational ap-
proach to single image depth prediction. arXiv preprint
4
arXiv:2302.06556,2023. 6,1
[30] Neehar Kondapaneni, Markus Marks, Manuel Knott,
[44] Tian Yu Liu, Parth Agrawal, Allison Chen, Byung-Woo
Roge´rioGuimara˜es,andPietroPerona. Text-imagealign-
Hong,andAlexWong. Monitoreddistillationforpositive
ment for diffusion-based perception. arXiv preprint
congruent depth completion. In European Conference on
arXiv:2310.00031,2023. 3
ComputerVision,pages35–53.Springer,2022. 2
[31] BarbaraLandau, LindaBSmith, andSusanSJones. The
[45] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng
importance of shape in early lexical learning. Cognitive
Zhang,StephenLin,andBainingGuo. Swintransformer:
development,3(3):299–321,1988. 2
Hierarchicalvisiontransformerusingshiftedwindows. In
[32] Barbara Landau, Linda Smith, and Susan Jones. Object
ProceedingsoftheIEEE/CVFinternationalconferenceon
shape,objectfunction,andobjectname.Journalofmemory
computervision,pages10012–10022,2021. 5
andlanguage,38(1):1–27,1998. 2
[46] XiaoxiaoLong,ChengLin,LingjieLiu,WeiLi,Christian
[33] DongLao,FengyuYang,DanielWang,HyoungseobPark,
Theobalt, Ruigang Yang, and Wenping Wang. Adaptive
SamuelLu,AlexWong,andStefanoSoatto. Ontheviabil-
surfacenormalconstraintfordepthestimation.InProceed-
ityofmonoculardepthpre-trainingforsemanticsegmenta-
ings of the IEEE/CVF international conference on com-
tion. arXivpreprintarXiv:2203.13987,2022. 2
putervision,pages12849–12858,2021. 2,6
[34] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and
[47] Mazen Mel, Muhammad Siddiqui, and Pietro Zanut-
IlHongSuh. Frombigtosmall: Multi-scalelocalplanar
tigh. End-to-end learning for joint depth and image
guidance for monocular depth estimation. arXiv preprint
reconstruction from diffracted rotation. arXiv preprint
arXiv:1907.10326,2019. 5
arXiv:2204.07076,2022. 2
[35] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and [48] MaximeOquab,Timothe´eDarcet,The´oMoutakanni,Huy
IlHongSuh. Frombigtosmall: Multi-scalelocalplanar
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
guidance for monocular depth estimation. arXiv preprint
DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal.
arXiv:1907.10326,2019. 2,6,8
Dinov2: Learning robust visual features without supervi-
[36] Boying Li, Yuan Huang, Zeyu Liu, Danping Zou, and sion. arXivpreprintarXiv:2304.07193,2023. 3
Wenxian Yu. Structdepth: Leveraging the structural reg- [49] Zixuan Pan, Zihao Wei, and Andrew Owens. Efficient
ularities for self-supervised indoor depth estimation. In vision-language pre-training by cluster masking. In Pro-
ProceedingsoftheIEEE/CVFInternationalConferenceon ceedingsoftheIEEE/CVFConferenceonComputerVision
ComputerVision,pages12663–12673,2021. 2 andPatternRecognition,2024. 3
[37] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. [50] Hyoungseob Park, Anjali Gupta, and Alex Wong. Test-
Blip: Bootstrapping language-image pre-training for uni- time adaptation for depth completion. In Proceedings of
fied vision-language understanding and generation. In theIEEE/CVFConferenceonComputerVisionandPattern
International Conference on Machine Learning, pages Recognition,2024. 2
12888–12900.PMLR,2022. 3 [51] RuiPeng,RonggangWang,YawenLai,LuyangTang,and
[38] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Yangang Cai. Excavating the potential capacity of self-
Blip-2: Bootstrapping language-image pre-training with supervisedmonoculardepthestimation. InProceedingsof
frozen image encoders and large language models. arXiv the IEEE/CVF International Conference on Computer Vi-
preprintarXiv:2301.12597,2023. 3 sion,pages15560–15569,2021. 2
[39] Yong-Lu Li, Xiaoqian Wu, Xinpeng Liu, Yiming Dou, [52] XiaojuanQi,RenjieLiao,ZhengzheLiu,RaquelUrtasun,
YikunJi,JunyiZhang,YixingLi,JingruTan,XudongLu, andJiayaJia. Geonet: Geometricneuralnetworkforjointdepthandsurfacenormalestimation. InProceedingsofthe depthprediction. InProceedingsoftheIEEE/CVFConfer-
IEEEConferenceonComputerVisionandPatternRecog- ence on Computer Vision and Pattern Recognition, pages
nition,pages283–291,2018. 2,6 5644–5653,2019. 2
[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya [65] AlexWongandStefanoSoatto. Unsuperviseddepthcom-
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, pletionwithcalibratedbackprojectionlayers. InProceed-
AmandaAskell,PamelaMishkin,JackClark,etal. Learn- ings of the IEEE/CVF International Conference on Com-
ingtransferablevisualmodelsfromnaturallanguagesuper- puterVision,pages12747–12756,2021. 1
vision. In International conference on machine learning, [66] AlexWong,SafaCicek,andStefanoSoatto. Targetedad-
pages8748–8763.PMLR,2021. 3,5 versarialperturbationsformonoculardepthprediction.Ad-
[54] Rene´Ranftl,AlexeyBochkovskiy,andVladlenKoltun.Vi- vancesinneuralinformationprocessingsystems,33:8486–
siontransformersfordenseprediction. InProceedingsof 8497,2020. 2
the IEEE/CVF international conference on computer vi-
[67] Alex Wong, Xiaohan Fei, Stephanie Tsuei, and Stefano
sion,pages12179–12188,2021. 2,6,8 Soatto. Unsupervised depth completion from visual iner-
[55] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yan- tial odometry. IEEE Robotics and Automation Letters, 5
song Tang, Zheng Zhu, Guan Huang, Jie Zhou, and (2):1899–1906,2020. 1,2
Jiwen Lu. Denseclip: Language-guided dense pre-
[68] Alex Wong, Safa Cicek, and Stefano Soatto. Learning
diction with context-aware prompting. arXiv preprint
topologyfromsyntheticdataforunsuperviseddepthcom-
arXiv:2112.01518,2021. 3
pletion.IEEERoboticsandAutomationLetters,6(2):1495–
[56] SaurabhSaxena,AbhishekKar,MohammadNorouzi,and
1502,2021.
DavidJFleet. Monoculardepthestimationusingdiffusion
[69] AlexWong, XiaohanFei, Byung-WooHong, andStefano
models. arXivpreprintarXiv:2302.14816,2023. 3
Soatto. Anadaptiveframeworkforlearningunsupervised
[57] SaurabhSaxena,CharlesHerrmann,JunhwaHur,Abhishek
depthcompletion. IEEERoboticsandAutomationLetters,
Kar,MohammadNorouzi,DeqingSun,andDavidJFleet.
6(2):3120–3127,2021. 2
Thesurprisingeffectivenessofdiffusionmodelsforoptical
[70] Cho-Ying Wu, Jialiang Wang, Michael Hall, Ulrich Neu-
flowandmonoculardepthestimation. AdvancesinNeural
mann, andShuochenSu. Towardpracticalmonocularin-
InformationProcessingSystems,36,2024. 3
door depth estimation. In Proceedings of the IEEE/CVF
[58] NathanSilberman,DerekHoiem,PushmeetKohli,andRob
Conference on Computer Vision and Pattern Recognition,
Fergus. Indoor segmentation and support inference from
pages3814–3824,2022. 2
rgbdimages. InComputerVision–ECCV2012:12thEuro-
[71] ShaokaiWuandFengyuYang.Boostingdetectionincrowd
peanConferenceonComputerVision,Florence,Italy,Oc-
analysisviaunderutilizedoutputfeatures.InProceedingsof
tober7-13,2012,Proceedings,PartV12,pages746–760.
theIEEE/CVFConferenceonComputerVisionandPattern
Springer,2012. 2,5,6,8,1
Recognition(CVPR),pages15609–15618,2023. 3
[59] Akash Deep Singh, Yunhao Ba, Ankur Sarker, Howard
[72] Yangchao Wu, Tian Yu Liu, Hyoungseob Park, Stefano
Zhang,AchutaKadambi,StefanoSoatto,ManiSrivastava,
Soatto,DongLao,andAlexWong. Augundo: Scalingup
and Alex Wong. Depth estimation from camera image
augmentations for unsupervised depth completion. arXiv
and mmwave radar point cloud. In Proceedings of the
preprintarXiv:2310.09739,2023. 2
IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages9275–9285,2023. 2 [73] ZhihaoXia,PatrickSullivan,andAyanChakrabarti. Gen-
erating and exploiting probabilistic monocular depth esti-
[60] ShuranSong, SamuelPLichtenberg, andJianxiongXiao.
mates. In Proceedings of the IEEE/CVF Conference on
Sunrgb-d: Argb-dsceneunderstandingbenchmarksuite.
Computer Vision and Pattern Recognition, pages 65–74,
InProceedingsoftheIEEEconferenceoncomputervision
2020. 3
andpatternrecognition,pages567–576,2015. 8
[61] RishiUpadhyay,HowardZhang,YunhaoBa,EthanYang, [74] FengyuYangandChenyangMa. Sparseandcompletela-
Blake Gella, Sicheng Jiang, Alex Wong, and Achuta tentorganizationforgeospatialsemanticsegmentation. In
Kadambi. Enhancing diffusion models with 3d perspec- ProceedingsoftheIEEE/CVFConferenceonComputerVi-
tivegeometryconstraints. ACMTransactionsonGraphics sionandPatternRecognition,pages1809–1818,2022. 3
(TOG),42(6):1–15,2023. 2 [75] Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu,
[62] RuoyuWang, ZehaoYu, andShenghuaGao. Planedepth: WenzhenYuan,andAndrewOwens. Touchandgo:Learn-
Self-superviseddepthestimationviaorthogonalplanes. In ing from human-collected vision and touch. Neural In-
ProceedingsoftheIEEE/CVFConferenceonComputerVi- formation Processing Systems (NeurIPS) - Datasets and
sion and Pattern Recognition, pages 21425–21434, 2023. BenchmarksTrack,2022.
2 [76] FengyuYang,JiachengZhang,andAndrewOwens. Gen-
[63] Youhong Wang, Yunji Liang, Hao Xu, Shaohui Jiao, and eratingvisualscenesfromtouch. InternationalConference
HongkaiYu. Sqldepth: Generalizableself-supervisedfine- onComputerVision(ICCV),2023. 3
structured monocular depth estimation. arXiv preprint [77] FengyuYang,ChaoFeng,ZiyangChen,HyoungseobPark,
arXiv:2309.00526,2023. Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit
[64] AlexWongandStefanoSoatto. Bilateralcyclicconstraint Gangopadhyay, Andrew Owens, and Alex Wong. Bind-
and adaptive regularization for unsupervised monocular ingtouchtoeverything: Learningunifiedmultimodaltac-tilerepresentations. InProceedingsoftheIEEE/CVFCon- recognition with high-frequency fusion. arXiv preprint
ferenceonComputerVisionandPatternRecognition,2024. arXiv:2111.10332,2021. 3
3 [90] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-
[78] Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, and pengMiao, BinCui, YuQiao, PengGao, andHongsheng
Elisa Ricci. Transformer-based attention networks for Li. Pointclip: Pointcloudunderstandingbyclip. InPro-
continuous pixel-wise prediction. In Proceedings of the ceedingsoftheIEEE/CVFConferenceonComputerVision
IEEE/CVF International Conference on Computer vision, andPatternRecognition,pages8552–8562,2022. 3
pages16269–16279,2021. 2,6,8 [91] Renrui Zhang, Ziyao Zeng, Ziyu Guo, and Yafeng Li.
[79] Yanchao Yang, Alex Wong, and Stefano Soatto. Dense Can language understand depth? In Proceedings of the
depthposterior(ddp)fromsingleimageandsparserange. 30thACMInternationalConferenceonMultimedia,pages
InProceedingsoftheIEEE/CVFConferenceonComputer 6868–6874,2022. 3,6
VisionandPatternRecognition,pages3353–3362,2019. 2 [92] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
[80] WeiYin,YifanLiu,ChunhuaShen,andYouliangYan. En- ShilinYan,PanLu,HongshengLi,PengGao,andYuQiao.
forcing geometric constraints of virtual normal for depth Llama-adapter: Efficient fine-tuning of language models
prediction. InProceedingsoftheIEEE/CVFInternational withzero-initattention. ICLR2024,2023. 3
ConferenceonComputerVision, pages5684–5693, 2019. [93] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin,
2,6,8 Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei
[81] ChenyuYou,WeichengDai,FenglinLiu,YifeiMin,Hao- Chang,PengGao,etal.Mathverse:Doesyourmulti-modal
ran Su, Xiaoran Zhang, Xiaoxiao Li, David A Clifton, llmtrulyseethediagramsinvisualmathproblems? arXiv
Lawrence Staib, and James S Duncan. Mine your own preprintarXiv:2403.14624,2024. 3
anatomy: Revisitingmedicalimagesegmentationwithex- [94] ChaoqiangZhao,YouminZhang,MatteoPoggi,FabioTosi,
tremely limited labels. arXiv preprint arXiv:2209.13476, Xianda Guo, Zheng Zhu, Guan Huang, Yang Tang, and
2022. 3 Stefano Mattoccia. Monovit: Self-supervised monocular
[82] Chenyu You, Weicheng Dai, Yifei Min, Lawrence Staib, depthestimationwithavisiontransformer. In2022Inter-
and James S Duncan. Implicit anatomical rendering for nationalConferenceon3DVision(3DV),pages668–678.
medicalimagesegmentationwithstochasticexperts. InIn- IEEE,2022. 2
ternationalConferenceonMedicalImageComputingand [95] HanbinZhao, FengyuYang, XingheFu, andXiLi. Rbc:
Computer-AssistedIntervention,pages561–571.Springer, Rectifying the biased context in continual semantic seg-
2023. 3 mentation. ECCV,2022. 3
[83] ChenyuYou,WeichengDai,YifeiMin,FenglinLiu,David [96] WangZhao,ShaohuiLiu,YezhiShu,andYong-JinLiu.To-
Clifton, S Kevin Zhou, Lawrence Staib, and James Dun- wardsbettergeneralization:Jointdepth-poselearningwith-
can. Rethinkingsemi-supervisedmedicalimagesegmenta- outposenet. InProceedingsoftheIEEE/CVFConference
tion:Avariance-reductionperspective.AdvancesinNeural onComputerVisionandPatternRecognition,pages9151–
InformationProcessingSystems,36,2024. 3 9161,2020. 2
[84] Shangbin Yu, Renyan Zhang, Shuaiye Ma, and Xinfang [97] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu,
Jiang. Monoculardepthestimationnetworkbasedonswin Jie Zhou, and Jiwen Lu. Unleashing text-to-image dif-
transformer.InJournalofPhysics:ConferenceSeries,page fusion models for visual perception. arXiv preprint
012019.IOPPublishing,2023. 2,6,8 arXiv:2303.02153,2023. 3
[85] ZehaoYu,LeiJin,andShenghuaGao.P2net:Patch-match [98] Chenhao Zheng, Jieyu Zhang, Aniruddha Kembhavi, and
andplane-regularizationforunsupervisedindoordepthes- Ranjay Krishna. Iterated learning improves composition-
timation. In European Conference on Computer Vision, ality in large vision-language models. In Proceedings of
pages206–222.Springer,2020. 2 theIEEE/CVFConferenceonComputerVisionandPattern
[86] WeihaoYuan,XiaodongGu,ZuozhuoDai,SiyuZhu,and Recognition,2024. 3
PingTan.Neuralwindowfully-connectedcrfsformonocu- [99] Chong Zhou, Chen Change Loy, and Bo Dai. Dense-
lardepthestimation.InProceedingsoftheIEEE/CVFCon- clip: Extract free dense labels from clip. arXiv preprint
ferenceonComputerVisionandPatternRecognition,pages arXiv:2112.01071,2021. 3
3916–3925,2022. 2,5,6,8 [100] JunshengZhou,YuwangWang,KaihuaiQin,andWenjun
[87] Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Zeng. Moving indoor: Unsupervised video depth learn-
Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. ing in challenging environments. In Proceedings of the
Tip-adapter: Training-free clip-adapter for better vision- IEEE/CVF International Conference on Computer Vision,
language modeling. arXiv preprint arXiv:2111.03930, pages8618–8627,2019. 2
2021. 3 [101] KaiyangZhou,JingkangYang,ChenChangeLoy,andZi-
[88] RenruiZhang,LongtianQiu,WeiZhang,andZiyaoZeng. weiLiu. Learningtopromptforvision-languagemodels.
Vt-clip: Enhancing vision-language models with visual- arXivpreprintarXiv:2109.01134,2021. 3
guidedtexts. arXivpreprintarXiv:2112.02399,2021. 3 [102] Tinghui Zhou, Matthew Brown, Noah Snavely, and
[89] RenruiZhang,ZiyaoZeng,ZiyuGuo,XinbenGao,Kexue DavidGLowe. Unsupervisedlearningofdepthandego-
Fu, and Jianbo Shi. Dspoint: Dual-scale point cloud motionfromvideo. InProceedingsoftheIEEEconferenceon computer vision and pattern recognition, pages 1851–
1858,2017. 1
[103] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Phillip
Kra¨henbu¨hl, andIshanMisra. Detectingtwenty-thousand
classes using image-level supervision. arXiv preprint
arXiv:2201.02605,2022. 3
[104] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui,
HongFaWang,YatianPang,WenhaoJiang,JunwuZhang,
Zongwei Li, et al. Languagebind: Extending video-
language pretraining to n-modality by language-based se-
manticalignment. arXivpreprintarXiv:2310.01852,2023.
3
[105] XiangyangZhu,RenruiZhang,BoweiHe,ZiyuGuo,Ziyao
Zeng,ZipengQin,ShanghangZhang,andPengGao.Point-
clipv2:Promptingclipandgptforpowerful3dopen-world
learning. In Proceedings of the IEEE/CVF International
ConferenceonComputerVision, pages2639–2650, 2023.
3WorDepth: Variational Language Prior for Monocular Depth Estimation
Supplementary Material
A.Evaluationmetrics. Method δ<1.25↑ δ<1.252↑ δ<1.253↑ AbsRel↓ log ↓ RMSE↓
10
d=32 0.925 0.990 0.998 0.093 0.039 0.327
Drawingon[7,43], ourevaluationofWorDepthalong- d=64 0.928 0.990 0.998 0.090 0.039 0.325
sidecomparisonmethodsinvolvesaquantitativeassessment d=128 0.932 0.992 0.998 0.088 0.038 0.317
d=256 0.930 0.991 0.998 0.089 0.039 0.323
throughseveralmetrics. Theseincludemeanabsoluterela-
d=512 0.929 0.990 0.998 0.089 0.039 0.324
tiveerror(AbsRel),rootmeansquareerror(RMSE),abso- d=1024 0.926 0.989 0.998 0.091 0.039 0.325
luteerrorinlogspace(log ),logarithmicrootmeansquare
10
error (RMSE ) and threshold accuracy (δ ). The evalua-
log i Table6.Sensitivitytodifferentnumbersofhiddenvariablesd.
tionmetricsaresummarizedinTable5fordetails.
ExperimentsareconductedonNYUDepthV2.disthenumberof
hiddenvariablesdofthetext-VAE.
B.AblationonModelArchitecture
C.AdditionalVisualizationonNYUDepthV2
Weevaluatedvaryinghiddenvariablesdoftext-VAEus-
Inthissection,asillustratedinFigure5,Wepresentad-
ing the NYU Depth V2 dataset[58], shown in Table 6. A
ditionalvisualizationscomparingWorDepthwithabaseline
key consideration was ensuring the hidden space was suf-
method AdaBins [2] on the NYU Depth V2 [58] dataset,
ficiently large to encode the necessary structural and ge-
emphasizing the advantages gained from integrating the
ometric features for reconstructing depth maps. This size
language prior. Compared with AdaBins, the error map,
requirementarisesfromtheneedtopreserveessentialfea-
with its brighter regions highlighting larger errors, clearly
turesaboutthescene’sobjectsandlayoutderivedfromtext
demonstrates that WorDepth achieves more precise depth
featuresencodedbytext-VAE.
predictionsforobjectsidentifiedinthetextdescription. For
However, it’s equally crucial to avoid excessively large
instance: “a sink and a bath tub” in the first row, “a white
hidden variables. A relatively constrained dimensionality
bathtub”inthesecondrow,“awoodendresser”inthethird
actsasaformofregularization,compellingthetext-VAEto
row, “a bed” in the fourth row, “a bunk bed” in the fifth
focusonextractingfeaturescrucialfordepthdecoding.Ad-
row,“anunmadebedwithclothesontopofit”inthesixth
ditionally, a limited hidden dimension prompts the model
row,“acouchandatable”intheseventhrow,“atableand
tolearnnotjustthedistributionmeanbutalsoitsvariance.
chairs” in the eighth row, “a blender on a counter” in the
This aspect is particularly important when mapping a text
ninthrow,“chairs”inthetenthrow,and“machineontopof
description to multiple scenes, such scenes’ text features
awoodentable”inthelastrow.
are encoded with identical distribution means but exhibit
significantvariance.
We established hidden variables d of 32, 64, 128, 256,
512,and1024fortrainingWorDepth. Itwasobservedthat
D.AdditionalVisualizationonKITTI
the optimal hidden dimension is 128, striking a balance
between capturing sufficient geometric features of scenes
Thissection,depictedinFigure6,showcasesvisualiza-
whilemaintainingeffectiveregularization. Deviatingfrom
tions of Monocular Depth Estimation in outdoor scenarios
this optimal size, either too small or too large, adversely
withtheKITTIdataset[20]usingEigenSplit[13],compar-
impactsperformance.
ingwithAdabins[2]. Duetothelimitedvarietyofobjects
inoutdoorscenes,ourmethodcapturesfewerobjectscom-
Metric Formulation
paredtoindoorscenes. However,whensalientobjectsand
AbsRel N1 e(cid:80) (i,j)∈Ω|y∗(i y,j ∗) (− i,jy )(i,j)| scenesarepresentoutdoors,ourmethodgainsapreliminary
RMSE (cid:113) 1 (cid:80) (y∗(i,j)−y(i,j))2 understandingoftheirscale. Thisunderstandingaidsinen-
Ne (i,j)∈Ω
hancingmonoculardepthestimationfortheseobjects. The
log 1 (cid:80) |log (y∗(i,j))−log (y(i,j))|
10 Ne (i,j)∈Ω 10 10 errormap’sbrighterregions,whichemphasizegreaterabso-
RMSElog (cid:113) N1 e(cid:80) (i,j)∈Ω(ln(y∗(i,j))−ln(y(i,j)))2 luterelativeerrors,unequivocallyshowthatWorDepthout-
δ %ofy(i,j)s.t. max(y(i,j),y∗(i,j))<thr∈[1.25,1.252,1.253] performs AdaBins in making more accurate depth predic-
y∗(i,j) y(i,j)
tions for objects and scenes mentioned in the text descrip-
Table5. Evaluationmetricformonoculardepthestimation. y tion. Forinstance: “twowhitetrucks”intheupperright,“a
denotespredictionsandy∗denotesgroundtruth. womanridingascooter”inthelowerleft,“buildings”inthe
lowermiddle,and“forestwithtree”inthelowerleft.Image Text Ours Ours Error Adabins Adabins Error Ground Truth
A bathroom
with a sink and
a bath tub.
A white bath tub
sitting in a
bathroom.
Depth
Value (m)
A television sitting
on top of a
wooden dresser.
A man standing
next to a bed in a
bedroom.
A bunk bed
with a table in
a room.
An unmade bed
with clothes on
top of it.
A living room
with a couch
and a table. Error
(Abs Rel)
A dining room
with a table
and chairs.
A kitchen with
a blender on a
counter.
A living room
with chairs
and a table.
A machine sitting
on top of a
wooden table.
Figure5.AdditionalvisualizationofmonoculardepthestimationonNYUDepthV2.Image
A group of cars parked on the A street with cars parked on Two white trucks driving down
Text
side of a road. the side of it. a street with a building.
Ours
Depth
Value (m)
Ours
Error
AdaBins
AdaBins
Error
Ground
Truth
Image
A woman riding a scooter
Text An empty street with buildings A forest with trees and a dirt road.
down a street.
and a car parked on it.
Error
Ours
(Abs Rel)
Ours
Error
AdaBins
AdaBins
Error
Ground
Truth
Figure6.AdditionalvisualizationofmonoculardepthestimationonKITTIEigensplit.