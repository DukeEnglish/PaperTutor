1
Exploring Emotions in Multi-componential Space
using Interactive VR Games
Rukshani Somarathna, Member, IEEE, and Gelareh Mohammadi, Member, IEEE
Abstract—Emotionunderstandingisacomplexprocessthatinvolvesmultiplecomponents.Theabilitytorecogniseemotionsnotonly
leadstonewcontextawarenessmethodsbutalsoenhancessysteminteraction’seffectivenessbyperceivingandexpressingemotions.
Despitetheattentiontodiscreteanddimensionalmodels,neuroscientificevidencesupportsthoseemotionsasbeingcomplexand
multi-faceted.OneframeworkthatresonatedwellwithsuchfindingsistheComponentProcessModel(CPM),atheorythatconsiders
thecomplexityofemotionswithfiveinterconnectedcomponents:appraisal,expression,motivation,physiologyandfeeling.However,
therelationshipbetweenCPManddiscreteemotionshasnotyetbeenfullyexplored.Therefore,tobetterunderstandemotions’
underlyingprocesses,weoperationalisedadata-drivenapproachusinginteractiveVirtualReality(VR)gamesandcollectedmultimodal
measures(self-reports,physiologicalandfacialsignals)from39participants.WeusedMachineLearning(ML)methodstoidentifythe
uniquecontributionsofeachcomponenttoemotiondifferentiation.Ourresultsshowedtheroleofdifferentcomponentsinemotion
differentiation,withthemodelincludingallcomponentsdemonstratingthemostsignificantcontribution.Moreover,wefoundthatat
leastfivedimensionsareneededtorepresentthevariationofemotionsinourdataset.Thesefindingsalsohaveimplicationsforusing
VRenvironmentsinemotionresearchandhighlighttheroleofphysiologicalsignalsinemotionrecognitionwithinsuchenvironments.
IndexTerms—Emotion,componentprocessmodel,physiologicalresponses,computationalmodelling
✦
1 INTRODUCTION
UNDERSTANDING emotions and their formation is a dimensionalmodelshavegainedwidespreadrecognitionin
fundamental aspect of human social interactions, and researchonemotionformation[7],[8].Discretemodelssug-
ithasextensiveimplicationsinmanyfields,suchaspsychol- gestthatemotionsaredistinct,separateentitiesthatcanbe
ogy, computer science, and human-computer interaction. easily distinguished from one another [7], [9]. Dimensional
Emotions are complex cultural and psychobiological states models postulate that emotions can be differentiated along
thatareanimportantaspectofhumanexperienceandplay continuousdimensionssuchasvalenceandarousal[5],[7],
acentralroleinoursocialinteractionsanddecision-making [10],[11].
[1]. Therefore, emotion understanding enables individuals Appraisal models theorise that emotions are the re-
to comprehend and respond appropriately to events and sponses of an individual’s cognitive evaluation of an event
regulate their emotions. Additionally, research on emotion [12], [13], [14].Appraisal models assume the cognitivepro-
understandinghasthepotentialtoinformthedevelopment cesses, the role of individual differences, and contextual
ofsystemsthatpossesscontextawarenessandtheabilityto awareness. Nevertheless, discrete models may not ade-
perceiveand expressemotions [2].Recent developmentsin quately capture the complexity of emotion formation, as
the field have led to numerous innovations and advances they treat emotions as distinct categories [15], [16] rather
in Artificial Intelligence (AI) systems such as virtual assis- than considering the continuity of emotional experiences.
tance, chatbots, Virtual Reality (VR) environments, cross- Dimensional models address this to some extent by recog-
culturalmodelsthatcanrecogniseandrespondtoemotions nising the continuity, however mostly limited to basic di-
across different cultures, wearable devices, and smart ap- mensions [5], [7]. Further, both discrete and dimensional
plicationsthatbenefitfromemotionalawareness.Although models tend to focus on the feeling component without
discrete and dimensional models have received significant explainingtheinteractiveeffectsofsub-processes[1].While
attention, neuroscientific evidence suggests that emotions appraisal models acknowledge the role of cognitive pro-
are complex, multi-faceted in nature, and show multiple cesses in emotion formation, there is limited data-driven
brain processes subserving emotional experience [3], [4]. research [7], [13], [17]. One possible reason may be the
Affective Computing (AC) research is mainly attributed to complexityofoperationalisingandassessing,astheyrelyon
Discrete,Dimensional,andAppraisal[5],[6]modelsindata- subjective evaluations of events. However, as the appraisal
drivenanalysis.Eachofthesemodelsapproachesemotions model recognises the complexity of emotions, encompass-
from different perspectives; however, only discrete, and ing a range of cognitive, physiological, and behavioural
processes [5], [18], it is crucial to consider a process-based
model to understand the mechanisms behind emotion for-
Rukshani Somarathna is with the School of Computer Science and mation.
•
Engineering, University of New South Wales, Australia, e-mail:
Theappraisalmodelhasinfluencedasignificantamount
r.somarathna@student.unsw.edu.au.
Gelareh Mohammadi is with the School of Computer Science and ofresearch.TheComponentProcessModel(CPM)[1],[12],
•
Engineering, University of New South Wales, Australia, e-mail: [18],avariantoftheappraisalmodel,hasalsosignificantly
g.mohammadi@unsw.edu.au.
shapedthisresearch.CPMassumestheprocess-basedmod-
4202
rpA
4
]CH.sc[
1v93230.4042:viXra2
ellingbetweenfivemaincomponents:appraisal,motivation,
expression,physiology,andfeeling[1].Renewedinterestin
the CPM has led to data-driven analyses. However, these
studieshaveoftenbeenlimitedintheiruseofactivepartic-
ipation [12], [19], objective measures of the physiology and
expressioncomponents[17],andtheinductionofabroader
range of emotions [12], [20]. Despite a few existing studies
on CPM, there is a need for more research using enhanced
data-driven approaches that involve active participation
and objective measures to gain a better understanding of
emotions. Fig.1.SimpleComponentProcessModel(CPM)basedon[18].
Inthisstudy,weoperationaliseaVR-basedsettingtoex-
plore the relationship between CPM and discrete emotions
bycreatingamorerealisticemotionalinduction[21].Inthis 5concludeswithasummary.
research,ouremphasiswasonCPMduetotheavailability
of a standard questionnaire to measure components and
operationalise them. We induced a more comprehensive 2 BACKGROUND AND RELATED WORKS
rangeofemotionsusinginteractiveVRgamesandcollected
Thepurposeofthissectionisfirsttoprovideabackground
multimodalmeasures:self-reports,physiological,andfacial
onthetopicofemotionunderstandingfromCPMandthen
signals. Our goal is to better understand the role of each
toreviewrelevantresearchinthefield.
CPM component in capturing emotions using self-reports
and physiological and facial signals. We also explore the
potential of VR games to induce a range of emotions with
2.1 ComponentProcessModel(CPM)
different intensities. It’s worth noting the main reason for
employing VR games is that they provide an immersive TheCPMisatheoreticalframeworkcomposedoffivemain
and more ecological medium for eliciting emotions, which components: appraisal, motivation, physiology, expression,
goes beyond the previous studies that used still images and feeling, which are used to understand and interpret
or video clips that result in a more passive experience of emotionalexperiences[1],[18].RefertoFigure1foravisual
emotions. Therefore, our analyses in this manuscript are representation of these components. CPM is based on the
confinedtounderstandingemotionsratherthantheimpact ideathatemotionsarecomplexandmulti-facetedinnature,
on VR gameplay. Additionally, we examine latent dimen- withvariouscognitive,physiological,andbehaviouralpro-
sionsthathelpunderstanddistinctemotionalfeaturesusing cessesconvergingtocreateanemotionalexperience.
CPMself-reports.OurfindingsinformtheroleofeachCPM The CPM’s appraisal component consists of cognitive
component in capturing emotions. These insights can be processes to evaluate an event. It has four objectives that
used for enhanced context awareness, system interaction, evaluate an event as 1) Relevance: “Is this event relevant?”,
and system adaptations in domains including healthcare, 2) Implications: “What will be the impact of this event?”, 3)
VRenvironments,education,gaming,andinterfaces.More- Coping potential: ”What are the chances of me being able to
over, it uses a data-driven approach to investigate emotion overcome the potential consequences of this event?” and 4)
formationpositingthatemotionsaresubjectiveexperiences. Normative significance: “Does this event align with my values
This manuscript builds upon previous research in this and beliefs?”. The outcome of these assessments activates
area [22], [23], [24] by presenting more comprehensive ex- mutually dependent processes on other components. The
perimentsandanalyseswithfurtherdatasamplesandgen- motivationcomponentreferstotheactiontendencies,goals,
eralised ML models using more rigorous evaluation meth- and motivations triggered by an individual’s appraisal of
ods (e.g. Leave-One-Subject-Out (LOSO) cross-validations). an event and drives their emotional reactions. Driven by
This paper addresses the following research questions: (i) appraisal and motivation outputs, the expression compo-
HowareCPManddiscreteemotionsrelatedwhenemotions nent initiates verbal or non-verbal behaviour to convey the
are induced through interactive VR games? (ii) What are emotional experience. Similarly, the physiology component
the latent dimensions that underlie emotional experience? begins appropriate changes in bodily functions to adjust to
and (iii) What is the contribution of each CPM component the situation. Finally, the feeling component represents the
in capturing discrete emotions? This study makes several conscious and non-conscious subjective emotional experi-
significant contributions to the field of affective computing ence.
and emotion recognition, which can be summarised as (i) The suggestive features and functions of the CPM in
Exploringthecomponentialprocessmodelinaninteractive emotion formation can be evaluated by the GRID instru-
VR environment with a much broader scope than previous ment [25] or by the shorter versions, CoreGRID and Mini-
works, (ii) Providing new insights into the relationship GRID[26],whichdescribethemeaningsofemotionwords.
between discrete emotions and CPM, and (iii) Exploring Unfolding the emotional experiences by CPM can be done
theimportanceofeachcomponentindifferentiatingdiscrete by either selecting full or a combination of components.
emotions. Accordingly, selecting proper GRID items that correspond
Thepaperisorganisedasfollows:Section2presentsthe- tothechosenstimuliandselectingappropriatedata-analytic
oreticalfoundations,Section3coversourresearchmethod- procedures for operationalising the selected CPM compo-
ology,Section4detailsresultsandimplications,andSection nentsiscritical[25].3
2.2 Relatedworks relaxingduetothefMRIsetupandpassiveemotioninduc-
tion due to the use of films [21]. Moreover, some studies
Inliterature,CPM-baseddata-drivenresearchaimstoiden-
haveusedfacialsignalsformeasuringemotions.
tify the structure of discrete emotions within the compo-
Facialelectromyography(fEMG)isanon-invasivetech-
nential space using data. Further, the exploration of com-
nique that uses electrodes to measure facial muscle acti-
putational modelling in understanding emotions has been
vations [40], [41]. Studies showed that fEMG can reflect
a recurring theme in diverse research endeavors [14], [27],
changesinthedimensionalmodelofemotion[40],[42],[43],
[28].Theseapproachesusecollecteddataandcomputational
[44], discrete model of emotion [45], [46], [47], and facial
modelstoprovidebottom-upinsightsintodiscreteemotions
expressions [41], [48], [49]. Several studies have focused on
fromaCPMperspective.Itsmeritliesinminimisingreliance
individual components or processes of the CPM [30], [31],
on assumptions about the relationships between variables,
but to our knowledge, none of these studies has examined
enabling data to guide the interpretation of emotions from
thefullCPMusingfEMGasanobjectivemeasureofexpres-
aCPMperspective.
sion.
Most of the data-driven research has been conducted Overall, the CPM is progressively getting more recog-
using individual or a subset of CPM components [20], nition in AC to understand the multi-faceted nature of
[29], [30], [31], [32], [33]. However, this narrow scope can emotions and underlying processes. The CPM can also be
also limit our comprehensive understanding of emotions, consideredasacomprehensivemodelthattakescontextinto
leading to less reliable conclusions being drawn by com- accountandgoesbeyondusingonlytheexpression.Current
putational models. Therefore, considering a full CPM with researchhasprovidedvaluableinsightsintoemotionforma-
five components can potentially improve the consistency tion’s cognitive, physiological, and behavioural processes.
of computational models and expand the current under- However, further research is yet needed to provide deeper
standing of emotions [18] and their representation within insightsintounderlyingmechanisms,theroleofphysiology,
different components. For example, in a data-driven study andothercomponentsinamoreimmersiveandnaturalistic
by Mohammadi and Vuilleumier [12], the authors used the setting.
fullCPMtoinvestigatetherelationshipbetween10discrete Many of the previous studies in this field have utilised
emotions and their corresponding CoreGRID responses us- passivemethods,suchasfilms,forelicitingemotions.How-
ing films as stimuli. The researchers discovered a clear ever,thesemethodshavebeenshowntobelesseffectivein
hierarchy between positive and negative emotions within elicitingemotionsthanactivemethods.Whilesomestudies
the CPM space. They also found that six dimensions were have used active methods, such as VR, they have often
necessarytocapturethedifferencesbetweenemotions,and restricted their stimulus selection to a narrower spectrum
ML can be used to differentiate emotion features. Another of emotions, relied solely on self-reporting, and have not
data-driven study used VR to induce emotions actively fullyinvestigatedtheobjectivelymeasurablecomponentsof
and examined the full CPM [17] but focused more on the emotions or the contributions of each component. Further-
appraisal component. Also, due to the limited selection of more,moststudiestendtooverlookthesubjectivenatureof
only7VRgames,abroaderrangeofemotionsmaynothave emotions and pre-label emotional situations with fixed la-
been effectively induced. Additionally, their results were bels.Therefore,weproposeadata-drivenapproach,postu-
influenced by the novelty factor associated with experienc- latingemotionalexperienceassubjective,tounderstanding
ing VR, as not all participants had prior exposure to VR. emotionsfromacomponentialperspective.Wewillemploy
Collectively, both of these data-driven studies have solely activeemotionelicitationtechniquesandcollectmultimodal
depended on measuring CoreGRID subjective annotations objectiveandsubjectivemeasurestoassessthecontribution
forinterpretingtheobjectivelymeasurablecomponents(ex- ofeachcomponentincapturingdiscreteemotions.
pressionandphysiology)oftheCPM.
Physiological signals have been used in AC research
3 METHODOLOGY
[11], [34], [35] to measure emotion responses, providing
Thissectionoutlinestheresearchdesign,materialselection,
details about the non-conscious and conscious experience
anddatacollectionproceduresemployedinthisstudy.
[36], [37], intensity, valence, and temporal variations. Re-
cent technological advancements have allowed it to collect
physiological data non-invasively with minimal setup and 3.1 MaterialandAssessment
configuration [9], [38], [39]. Researchers have focused on
Materialselectionisasignificantsteptowardthereliability
usingphysiologicalsignalstostudydifferentemotions,but
of research conclusions. Therefore, we used interactive VR
relatively little attention has been given to using these sig-
games to create a more realistic and immersive emotion
nalsinstudiesbasedonthefullCPM.Onesuchexampleis
induction paradigm as opposed to passive paradigms [21],
adata-drivenstudybyMene´trey,etal.[19],whichhasbeen [50].Weselected96VRgamesfromSteam1usedinprevious
expanded to include heart activity, skin conductance, and
works[17],[51],[52]andwhichwereratedongamereview
respirationotherthantheCoreGRIDannotations.Usingthe
websites.Fromthat,wechose27VRgames(refertoSupple-
datacollectedinsideafunctionalMagneticResonanceImag-
mentaryTableS1)basedonthegamemechanics’simplicity,
ing (fMRI) machine, researchers confirmed ML’s ability to
clarity, diversity, and the possibility to generate emotions
differentiate emotions and assess synchronisation between
withinthreeminutes.Forinstance,thechosengamesfeature
thecomponentsduringanemotionalepisode.ThefullCPM
VR scenarios like shooting and zombie encounters mainly
showedbetterwhenallcomponentswereconsideredrather
than just one. However, this experiment setup may be less 1.https://store.steampowered.com/4
HTC VIVE Pro Self-assessments Wearable devices 3.3 Procedure
with EmteqPro
TheresearchwasapprovedbytheUniversityofNewSouth
Speech recorder MSSQ HTC VIVE Pro
To screen To present VR games Wales, Human Research Ethics Committee (HC200809).
Shimmer sensor Pre-experimental survey EmteqPro
Empatica To assess demography To collect facial EMG The dataset was collected from 39 participants (18 fe-
E4 Big Fi Tv oe a I sn sv ee sn st o pr ey rsonality Speec Th o cre oc lleo crd t ve or ice males, 21 males, mean age = 25 & SD = 5.42 years) who
Brief Mood Introspection Scale Shimmer sensor were first screened by the Motion Sickness Susceptibility
To assess mood To collect biosignals
GEW Empatica E4 Questionnaire (MSSQ) [57] for motion sickness. The inclu-
IMUs CoreGTo R Ia Dssess emotions Foot IT Mo U c sollect biosignals sion/exclusion criteria to recruit participants were an age
To assess CPM To collect foot moves rangeof18-40years,withoutanytypeofpriorpsychological
or prior neurological disorders, fluent in the English lan-
Fig.2.AparticipantplayingVRgameswhilewearingwearabledevices, guage,notwearingglassesorwithcorrectedrefractiveerror,
providingsubjectiveassessments. and without prior vertigo, hearing, or vestibular problems.
Each participant went through a one-hour training session
andthree100-minuteactualdatacollectionsessionsaspart
at evoking emotions such as fear, disgust, and hate. Ad-
of our data collection process. Each participant received
ditionally, VR experiences like slingshotting, slicing fruits,
$150 as reimbursement for their participation. The training
and slashing blocks are included to evoke feelings of joy,
session began with an explanation of the experiment setup
pleasure, and interest. To ensure the selection encompasses
and instructions for the participants to report their own
a wide range of emotions, games were labelled with a
experiences rather than what they thought should be felt
dominant emotion only for selection purposes using the
whileplayingthegamesinactualdatacollectionsessions.
Geneva Emotion Wheel (GEW) [53]; however, these labels
We developed an app using PsychoPy [58] to present
werenotusedinthefollowinganalysis.Weusedallthe20
thegames,administerthequestionnaires,synchroniseeach
emotions from the GEW, which are defined as positive (in-
event,andsaveourdata.Toensureabalanceddistribution
terest, amusement, pride, joy, pleasure, contentment, admi-
ofemotionalexperiencesacrossthethreedatacollectionses-
ration,love,relief,compassion)andnegative(sadness,guilt,
sions, we randomised the sequence of pre-labelled games.
regret,shame,disappointment,fear,disgust,contempt,hate,
We minimised repeating any similar experiences within a
anger). Given that VR games are primarily developed for
single session. Additionally, we randomised the presenta-
entertainment purposes, it was difficult to identify games
tionofeachsurveyitemtoreduceanyeffectoftheorder.We
thatcouldelicitnegativeemotionsandsomeofthepositive
collected each participant’s demography (pre-experimental
emotions within a three-minute time frame. Consequently,
survey),personality(BigFiveInventorysurvey),andmood
an uneven distribution of emotions was observed in the
(BriefMoodIntrospectionScalesurvey)throughsurveys.
selectedgames,asindicatedinSupplementaryTableS1.
Forthecalibrationofwearabledevices,wefirstcollected
Welabelledseven games basedonpreviousworks[17],
neutral expressions. For the calibration of emteqPRO, we
[51],[52]andtherestofthe20gamesbasedonresearchers’
furthercollectedseveralmaximumsmiles,frowns,andeye-
gameplay experience and game review ratings and com-
brow raises expressions. To instruct the participants in the
ments.Whilewetermedeachgamewithprimaryemotion,
calibration phase, we developed 3D scenes using Blender
we did not expect all participants to experience the same
[59],whichweredeployedtoVRenvironmentsusingHAR-
emotionsduetosubjectivevariations[20],[35],[54].There-
FANG®3D [60]. Then each participant played a VR game
fore,weusedtheGEWandCoreGRIDassurveystocollect
forthreeminutesandcompletedGEWandCoreGRID(refer
participants’emotionalexperiencesfollowingeachgame.
to Supplementary Tables S2 and S3) surveys subsequently,
In the experimental setup, we organised a training ses-
wheretheymarkedemotionalexperienceona5-pointLikert
siontotraintheparticipantstoVRandtoreducethenovelty
scale (1-Not at all, 5-Strongly). This process was repeated
effect of VR. For that, we used the SteamVR tutorial appli-
until each participant had completed all 27 games in three
cation, NVIDIA® VR Funhouse, Fantasynth One, Expedia
data collection sessions. We collected 1053 observations (27
CenoteVR,Luna,andGoogleSpotlightStories:AgeofSail
games 39 participants) from all participants but only
games. ×
considered 1041 observations in our analysis. The rest of
the observations were removed due the technical concerns
such as network errors, game updates, and headset dis-
3.2 Proposedsystem
connections. Lastly, we post-processed our collected facial
Figure 2 shows our experimental setup, where we used an
EMGdatausingtheSuperVision[55]applicationtogetthe
HTC VIVEPro headset withcontrollers for gamepresenta-
frequency and intensity of facial expressions such as smile,
tion. We used emteqPRO [55] wearable device to measure
frown, eyebrow raises and neutral, arousal, and valence
the facial EMG activations and Empatica E4 wristband
insights.
to record Heart Rate (HR), Electrodermal Activity (EDA),
Blood Volume Pulse (BVP), Inter Beat Intervals (IBI), skin
temperature, and acceleration. We also used Shimmer sen- 4 RESULTS AND DISCUSSION
sorstomeasurerespirationandElectrocardiography(ECG) Thefollowingsubsectionsdescribeourresultsandimplica-
and an off-the-shelf Inertial Measurement Unit (IMU) [56] tions. The first section, 4.1, discusses the different discrete
to track body movements. However, for the purpose of the emotions experienced in VR, while section 4.2 explores the
presentstudy,ourfocuswaslimitedtotheanalysisoffacial componential features of these emotions. In section 4.3,
EMG,BVP,skintemperature,EDA,andrespiration. the focus is on finding the emotional dimensions, while5
Hate intricate nature of some of these emotions, which are often
Disgust referredtoassocialorself-consciousemotions(shame,guilt,
Angry
Fear and contempt [6], [61], [62]). Furthermore, we observed a
Disappointment
Sadness discrepancybetweenourinitialcategorisation(Supplemen-
ons S Rh ea gm ree
t
tary Table S1) of emotions, which was based on previous
oti Guilt works, the experience of the researchers, and ratings and
em Contempt
comments from game review sites. The outcomes of this
e Pleasure
Discret
Amuse Pm
rJ
e
ido
n
ey
t
a sin mal iy las ris sis tu ug ag tie os nt st .h Cat onin sd eqiv ui ed nu ta lyls ,te hx eh rib ai tt inv ga sri fe od re thm eo st eio len cs tei dn
Admiration games and their content reveal the presence and complex
Contentment
Relief natureofappraisalobjectivestoreacttoaneventadaptively.
Interest
Moreover, the figure highlights the presence of complex
Compassion
Love emotional structures and mixed feelings in each game,
which makes the current emotion recognition models that
BeatSaber FruitNinja AngryBirds GoogleEarth TiltBrush Richies WaltzWizard Fujii OceanRift EggTime Futurejam Moss Meditation Cartoon MSI VirusPop Toran Doctor Kitty CatchReleas Eve erybodyssad Fishermans CosmicFl Bo ow okofDistance DeadlyHunter Propagation Brookhaven u
T
ms
h
eu
e
aa nsl tl sy
u
odl fa yb
e’s
le il
cre
ia
ts
in
u nl
ge tv
s
ae in
n
rat
d
nw
ic
gai et th
e
ofo thn
ea
mly
t
ooo
tu
in ore nVe sRm (to
g
ht
a
oi mo un
ge
hsle
a
is trs
e
ise ef uff fe sec uct
t
ai iv
lv
le ye.
Games
mixedemotions),whichcanbeusedtoaddressourresearch
questions.
Fig.3.Participant’saverageintensityratingsfordiscreteemotionsbyVR
games.Thesizeofthebubbleisproportionaltotheaverageemotional
intensity.Forbetterrepresentationofthedifferenceacrossgames,the
gamesweresortedbasedontheaverageratingofJoyemotion. 4.2 ComponentialEmotionFeaturesinVR
We performed a cluster analysis on the CPM profile of
different emotion categories to unfold the hierarchical or-
section 4.4 explains therole of facial EMG in theemotional
ganisation of each emotion in the CPM space. Here, our
experience. Finally, section 4.5 discusses the use of ML in
analyseswerebasedontheself-assessmentof51CoreGRID
modelling emotions and investigates the role of each com-
and20GEWitems,withtheCoreGRIDitemsservingasrep-
ponent. Together, these sections provide a comprehensive
resentativeswithintheCPM.Theformulafortheweighted
understandingofthevariousfacetsofemotionalexperience
average CPM profile (CP) for emotion term j is given as
inVR. j
follows:
n w x
4.1 DiscreteEmotionsinVR CP = i=1 ij i
j n w
Given the novelty of VR to emotional studies, the efficacy P i=1 ij
ofVRgamesininducingawiderrangeofemotionsshould w
ij
is the score of emotioPn term j in sample i, x
i
is the
bevalidatedtoanswerourresearchquestions.Therefore,in CoreGRIDscorevectorforsampleiandnisthenumberof
Figure3,weillustratedthe39participant’saverageintensity samples. We conducted a Ward method-based hierarchical
scores of each discrete emotion (defined by the 20 GEW clustering using the Euclidean distance metric [23], [63].
terms)whenplayingtheselectedVRgames.Thisplotmay Thehierarchical organisationillustratedin Figure4depicts
beimportanttoresearchersinterestedinselectingVRgames theliterature’sclassicaldefinitionsofpositiveandnegative
that evoke specific emotions. It can provide insight into emotions [53]. Further, it can be explained by the CPM’s
whichVRgamesaremosteffectiveatelicitingspecificemo- intrinsic (un)pleasantness, goal obstructiveness, and goal
tions when designing future studies and aid in identifying relevance [20]. Lower-level branching can also be identi-
which emotions are most elicited by VR games in general, fied as eight meaningful sub-clusters: anger (hate, disgust,
which can inform the development of VR games that are anger),fear,sadness(disappointment,sadness),embarrass-
moreemotionallyengaging. ment (shame, regret, guilt, contempt), happiness (pleasure,
It is apparent from this figure that all the selected emo- joy, amusement, pride), satisfaction (admiration, content-
tions were experienced to a certain intensity by partici- ment), serenity (relief, interest) and affection (compassion,
pants. Regardless of the negative valence of some games, love).
positive emotions such as interest, amusement, pride, joy, In our negative cluster in the CPM space, we found
pleasure, contentment, and admiration are highly rated. anger, fear, sadness, and embarrassment as the main sub-
This is supported by the high immersion and pleasantness levels. These hierarchically organised clusters demonstrate
in VR games that increase interaction and engagement. the high arousal activation of anger and fear against low-
Additionally,love,relief,compassion,disappointment,fear, arousal emotions such as sadness [64]. Also, shame, regret,
and anger have been triggered by some games. However, and guilt clustered in terms of embarrassment suggest the
the intensity of sadness, guilt, regret, shame, disgust, con- clustering of low power and negative valence emotions,
tempt,andhateisrelativelylow.AsVRgamesaregenerally as depicted in GEW [53]. However, contempt, which is
targeted for entertainment, it was not easy to find games clustered in the higher-level branch of the embarrassment
that elicit such complex emotions. Our results align with cluster,isdefinedasanegativevalencedandhigh-powerin
previoussurveys[21]andstudiesthatfoundsadness,guilt, GEW[53].Butcontemptclusteredherewithshameandguilt
and contempt [17] to be rated as low-intensity emotions in potentially due to the complex nature of these emotions as
generalinlaboratoryexperiments.Thiscouldbeduetothe socialorself-consciousemotions[62].Inourpositivecluster,6
ysisonthe51CoreGRIDitems,examiningdimensionsfrom
acomponentialperspective.Weperformedafactoranalysis
followed by a Varimax rotation on the z-score normalised
51CoreGRIDitems[68]andidentifiedfivefactorsbasedon
a scree plot. We found five meaningful factors explaining
43.84% of the total variance, capturing aspects related to
body changes and expressions associated with suddenness
and novelty (14.20%), valence (10.89%), agency (6.94%),
novelty (5.42%), and norms (6.40%). Our results align with
theindividualresearch[12],[66],[68],[69],suggestingmore
than the commonly considered two dimensions of valence
and arousal. As an example, the full CPM data-driven
study by [12] identified action tendency, (un)pleasantness,
novelty, norms, arousal, and goal relevance as dimensional
representations of emotions. Some of the previous works
Fig. 4. Results of the componential hierarchical clustering of discrete
emotionterms.Twodistinctclustersofnegative(redcluster)andpositive have used a few components [33], emotional terms, and
(greencluster)emotionswithmeaningfulsub-clusterscanbeobserved. facialexpressions[65],[70],[71],sotheresultswerelimited
to a few dimensions. Nevertheless, our assumption of full
CPM with CoreGRID items expands the dimensions that
we found happiness, satisfaction, serenity, and affection
candifferentiateamongemotionwords.Moreover,amulti-
as the main sub-levels. The happiness cluster consists of
componential architecture of emotion aligns broadly with
high power and positive valence emotions agreeing to the
ourfive-factoranalysis.
independent research [12], [20]. Similarly, the hierarchical
organisation of the satisfaction cluster agrees with GEW’s
4.4 RoleofFacialEMGintheEmotionalExperience
lowpowerandpositivevalencedemotions[53].However,a
Facial expressions are a prominent form of non-verbal
separateserenityclusterisobservedwithinterestandrelief,
communication of emotions [41], [49]. However, when face
which are positive valence but opposed poled with power
monitoring is not feasible, facial EMG has been used to
dimension regarding GEW [53]. This observation contrasts
measure muscle movements using surface EMG sensors
with the previous findings where interest and relief were
[72]. Therefore, we investigated the power of facial EMG
observed in two different clusters in CPM space [17]. Our
indifferentiatingself-reportedemotional(20emotionterms
observationmaybepossiblyduetothehighimmersionand
from GEW) experiences to gain insights into the role of
pleasantness in selected VR game experiences. The hierar-
CPMexpressioncomponentsinemotionsandtheircorrela-
chical organisation of affection cluster in the CPM space
tion.Weuploadedourcalibrationdata(neutral,maxsmile,
agreeswiththelowpowerandpositivevalencedimensions,
max frown, max eyebrow raises) and gameplay data to
including attachment-related emotions such as compassion
SuperVision to get facial EMG expressions (smile, frown,
andlove[36],[65].
eyebrow raise) and their intensities. Figure 5 shows the
The vector values of the weighted average CPM profile
Spearman correlation of three facial EMG expressions with
(CP)ofeachdiscreteemotiontermarepresentedinSupple-
j 20emotions.
mentary Figure S2. Accordingly, the intensity of different
The results showed a significant positive correlation of
CoreGRID items varied in response to different discrete
“smile emg” with interest, joy, pleasure (p<0.001), amuse-
emotions or combinations of them. This illustrates the im-
ment (p<0.01), and pride, fear (p<0.05), indicating activa-
pactingfactorsforeachemotionandsupportstheprevious
tionofzygomaticusmusclesinpleasantscenariosexceptfor
clusteringresults.
fear[30],[42],[43],[49].However,thepositivecorrelationof
Overall, our results’ validity and meaningful clustering
EMG between fear, shame, hate, and anger may be due to
forawiderangeofemotionsshowthatVRisapowerfultool
lip tightener, lip stretcher, and jaw drop [73], which are re-
for emotion research, offering a more realistic environment
latedtomouthareas.Asignificantnegativecorrelationwas
thantraditionalmethodssuchasfilms[12],[20].
observedwithadmiration,sadness,andcontempt(p<0.05),
possibly due to lower arousal [53] and complexity [62],
4.3 LabelingtheEmotionalDimensions [74] of these emotions, leading to lower activation in the
Emotion terms are differentiated by various dimensional zygomaticusmuscle.The“frown emg”waspositivelycorre-
models of emotion. While the basic valence and arousal latedwithmostofthenegativeemotionsdisgust(p<0.001),
dimensional model is generally recognised, there are com- hate(p<0.01),fear,andanger(p<0.05).Further,thepositive
plications in rationalising some emotions [66]. For exam- emotions except admiration and compassion were signif-
ple, intense anger is considered high arousal, and intense icantly negatively correlated with “frown emg”. The high
sadness is considered low arousal [67]. Given that, the ba- activationofcorrugatorsupercillifacialmusclesinnegative
sic dimensional model cannot distinguish between intense experiences can explain the observation [43], [72], [75].To
and less intense sadness due to low arousal in both cases. further note, the significant correlation between the corru-
Therefore,thecurrentscopeofthebasicdimensionalmodel gator or “frown emg” and positive/negative emotions can
createslimitationsindifferentiatingcertainemotions. be attributed to its positioning below the frontalis muscle,
Toexplorethelatentdimensionsandaddressoursecond whichisinvolvedinfacialexpressionssuchasfrowningand
researchquestion,weconductedanexploratoryfactoranal- eyebrow-raising[72].7
Fig.5.ThecorrelationmatrixdisplaystheaverageintensityofthreefacialEMGexpressions(“smile emg”,“frown emg”,“eyebrowraise emg”)in
blackfontandtheGEWemotionsinbluefont.Positivecorrelationsareindicatedbya(+)symbol,anda(-)symbolindicatesnegativecorrelations.
Asterisksindicatethestatisticalsignificanceoftheresultsatap-valueof:*p<0.05,**p<0.01,***p<0.001.
The pattern of “eyebrowraise emg” with emotions was
ambiguous, given that it showed a significant positive
correlation with “frown emg” (p<0.001), admiration, and
compassion (p<0.01). The positive correlations with admi-
ration and compassion, typically associated with positive
emotions, suggest that “eyebrowraise emg” may not be a
definitive indicator of positive emotions. The significant
negativecorrelationof“eyebrowraise emg”withinterestand
pleasure (p<0.01) may be due to the high arousal of these
emotions showing lower activations of frontalis EMG [42].
Althoughmostwerenon-significant,thepositivecorrelation
between“eyebrowraise emg”andemotions,exceptforseven,
suggestambiguousvalence[76]andtheroleofnovelty[66],
morelikeinsurpriseelicitation.
Similarly, we did a Spearman correlation but with the
Fig. 6. The correlation matrix presents the average intensity of
eight CoreGRID expression component items and three
three facial EMG expressions (“smile emg”, “frown emg”, “eye-
facial EMG activations. As given in Figure 6, a posi- browraise emg”)inblackfontandtheCoreGRIDexpressioncomponent
tive correlation of “smile emg” with “smile?”, “shout or ex- in blue font. Positive correlations are denoted by (+), while negative
claim?”(p<0.001),“eyebrowsgoup?”(p<0.01),and“frown?” correlationsarerepresentedby(-).Asterisksindicatethestatisticalsig-
nificanceoftheresultsatap-valueof:*p<0.05,**p<0.01,***p<0.001.
(p<0.05) were visible. The “smile emg” correlation between
“smile?”, “shout or exclaim?” was expected due to the zy-
gomaticus muscle movement caused by an actual smile with “frown emg” (p<0.001) and negative correlations with
[42], [77] and involuntary speech or orofacial movements “smile?” (p<0.05). The negative correlations suggest that
[49], respectively, where SuperVision cannot differentiate raised eyebrows are less likely to occur with smiling, pos-
between two expressions linked to similar muscle areas. sibly indicating a more subdued emotional state or a lack
The correlation of “smile emg” with self-reported “frown?” of intensity in the emotional response. However, the corre-
may be due to the activations in the cheek region, such lation between EMG and self-reported eyebrow activations
as nose wrinkling, upper lip raising, lip corner depression, was negative and non-significant, perhaps due to certain
andlipstighteninginunpleasantoutcomes[30].Incontrast, oversights in the SuperVision application or individuals’
“smile emg”wasnegativelycorrelatedwith“cry?”(p<0.01), perceptionofeyebrowsraisingisnotnecessarilyreliable.
which is consistent with the lower activation of the zygo- In any case, the results demonstrate the possibility of
maticus muscle previously observed in other studies [78]. facial EMG in monitoring facial expressions to detect emo-
Given the differences in individual experiment settings, tions.Further,itemphasisestheroleoftheexpressioncom-
insights from SuperVision may be more general, so a cus- ponentinemotionformationaftereventappraisal.
tomisedmodelmayimproveinterpretations.
Asthenextobservation,“frown emg”showedapositive 4.5 ModellingusingMachineLearning
correlation with “frown?” (p<0.05) and a negative correla- Totestforanyprofoundrelationshipsofemotionswithfull
tion with “smile?” (p<0.01). This may be possibly due to CPM,includingsignals,weusedMLclassifiers.Thissection
the high and opposite activation of corrugator supercilli focuses specifically on our third research question, which
muscles[79],[80]respectivelyorduetotheinvolvementof seeks to determine the extent to which each component of
corrugatormuscleinbothnegativeandnon-negativefacial the CPM contributes to the identification of discrete emo-
expressions [72]. This finding aligns with the obstructive tions. For that, we first preprocessed the data and applied
andconduciveappraisalinthebrowregion[30]. digitalsignalprocessing.EMGsignalsaretypicallysampled
The “eyebrowraise emg” showed a positive correlation atarateof1000Hztocaptureandpreservethesignal’sfull8
rangeoffeaturesandnuances[41].Therefore,weresampled for each emotion against the chance level (majority class
all the signals to 1000 Hz to ensure that the signals are prediction) are shown in Figure 7. In our analysis, we
evenly sampled in time and allow for accurate comparison generated the baseline using both the chance level and a
andanalysisofthesignals.TheBVPsignalwasup-sampled nonparametric chance distribution through permutations.
from64Hzto1000Hz,andamedianfilterwasappliedtore- However,sincebothbaselineswerenearlysimilar,weopted
ducethenoise[81].BothskintemperatureandEDAupsam- tousethechancelevelasthebaselineforallofouranalyses.
pledfrom4Hzto1000Hz.Thenapplied,aSavitzky-Golay Inallcasesexceptfour(shame,regret,guilt,andcontempt),
(SG) filter [82] for skin temperature (window length=9 and the model exceeds the chance level. However, according
polyorder=5) and EDA (window length=11 and polyorder=5) to the one-sample t-tests, accuracy was significantly higher
[83] for smoothing the signal. The respiration signal was (p<0.001)forinterest,amusement,pride,joy,pleasure,con-
up-sampled from 256 Hz to 1000 Hz and applied an SG tentment, admiration, love, and fear. Accuracy of relief,
filter (window length=11 and polyorder=5) [83]. Seven EMG compassion,andangeraremoderatelysignificant(p<0.05).
amplitude channels (right and left frontalis, right and left The lower performance for the remaining emotions can
orbicularis, right and left zygomaticus, corrugator) were be attributed to the skewed distribution of the emotion
downsampledto1000Hzfrom2000Hz. ratings, resulting in fewer representative examples of the
For feature extraction, we employed Discrete Wavelet underrepresented class, and making it more difficult for
Transformations (DWT). DWT is beneficial in this context themodeltoidentifydistinguishingpatterns.Forexample,
because it allows for localisation in both the frequency and guilt, disgust, and shame are heavily skewed towards the
time domains and provides a multi-resolution analysis at lowerend,possiblyduetothecomplexityoftheseemotions
different frequencies or scales. Through experimentation, [74]. As VR games are primarily designed for entertain-
wefoundthattheDaubechieswaveletfamilyandfivelevels ment, it is not easy to find games that effectively elicit
of frequency decomposition yielded the best results. We these emotions. Additionally, the margin of improvement
usedthecoefficients’mean,variance,andmedianresulting forsomesignificantmodelsissmallerduetothesamecause.
fromthefivedecompositionlevelsasfeatures.Accordingly, Overall,theeffectivenessofselectedphysiologicalmeasures
we used 51 CoreGRID items, three statistical features of andCoreGRIDitemsindifferentiatingemotionalfeaturesis
DWT of BVP (15 features), skin temperature (15 features), depicted.
EDA(15features),respiration(15features),andsevenEMG
channels(105features).Apartfromthat,wefedtheaverage 4.5.2 Role of each CPM Component and Physiological
intensitiesofsmile,frown,andeyebrowraiseretrievedfrom ChangesinDifferentiatingDiscreteEmotions
theSuperVisionapplication,resultingin219inputfeatures. To investigate the contribution of each CPM component
We trained ML classifiers treating the CoreGRID items and physiological measures, we trained ML models as
and signal features as input representing the CPM inter- previouslydescribedbutusingtherelevantcombinationof
pretations. We used each discrete emotion’s high and low features for each component. (For example, the appraisal
values as split by the first quartile output. Due to the component includes 17 appraisal CoreGRID items, and the
imbalance between high and low values, we selected the physiology component includes 8 physiology CoreGRID
first quartile as the threshold instead of the median. We items and BVP, skin temperature, EDA, and respiration).
within-subjectz-normalisedtheinputfeaturesandsampled The results for 20 emotions and the statistical significance
thetrainingdatasetusingtheSyntheticMinorityOversam- level for using either individual components or full CPM
pling Technique (SMOTE) [84] to reduce the imbalance in compared to chance level (majority class prediction) are
the dataset. For better generalisability, we report the LOSO given in Table 1. In Table 2, we provide the results when
cross-validation results, taking the average after all itera- removing one component at a time and training with the
tions. remaining components. Using this approach, we can deter-
mine the additional information provided by each compo-
4.5.1 Interpretation of Emotional Experience through the nentindistinguishingdiscreteemotions.
CoreGRIDandPhysiologicalChanges Accordingly,theappraisalcomponentwassignificantfor
To understand the emotional experience through the Core- amusement,pride,joy,pleasure,andcontentment(p<0.001)
GRID items and physiological changes, we trained sev- and interest, love (p<0.01). This correlation between the
eral commonly used ML classifiers such as Support Vec- appraisal component and most positive emotions may be
tor Machine (SVM), Random Forest (RF), eXtreme Gradi- due to the compatibility with the participant’s goals, their
ent Boosting (XGB) and Light Gradient Boosting Machine ability to cope with the experience, and the intrinsic pleas-
(LGBM). We followed the process outlined in Section 4.5 antness [20], [85] of the VR experience. The motivation
and evaluated the performance of our model using LOSO component showed significant performance for interest,
cross-validation,repeatingtheprocess.Althoughwetrained amusement, pride, joy, pleasure (p<0.001), contentment,
severalclassificationmodels,wefoundthattheLGBMhad and fear (p<0.01). It appears that motivational goals such
thebestperformance,sowepresentedtheresultsfromthat as information seeking, savouring, and wishful thinking
model. [85] have impacted the experience of positive emotion.
We initially assessed model performance using Core- In contrast, fear correlations represented avoidance action
GRID items alone and in combination with physiological tendencies [61]. Next, for the CPM expression component,
measures. The combined approach showed an average in- first, we trained classifiers only using the self-reports and
creaseof1.61%,rangingfrom0.22%to4.48%.Therefore,the both self-reports and facial signal features. We reached the
resultsoftrainingtheLGBMclassifierusingbothmeasures conclusion that models incorporating both inputs consis-9
*** *** *** *** *** *** *** *** ***
100 * * *
80
60
40
Hate Disgust Angry Fear appointment Sadness Shame Regret Guilt Contempt Pleasure Joy Amusement Pride Admiration Contentment Relief Interest Compassion Love
s
Di
Chancelevel LGBM
Discreteemotions
Fig.7.AccuracyoftheLGBMbinaryclassifiersfordifferentiatingemotions.Accuracyiscomparedwiththechancelevel(majorityclassportion).
Errorbarsrepresentthestandarddeviation.Asterisksindicatethesignificanceofresultsatp-value:*p<0.05,**p<0.01,***p<0.001.
TABLE1
AccuracyoftheLGBMbinaryclassifiersforthedifferentiationofeachemotionusingindividualcomponents.Asterisksindicatethesignificanceof
resultsatp-value:*p<0.05,**p<0.01,***p<0.001.Thechancelevelisusedasthebaseline.Appraisal,motivation,andfeelingcomponents
includeonlytherelevantCoreGRIDitems.ThephysiologycomponentincludesCoreGRIDitemsandBVP,skintemperature,EDA,andrespiration.
TheexpressioncomponentcontainsCoreGRIDitems,EMGamplitudesignals,andthreefacialexpressionvalues.FullCPMincludesall
CoreGRIDitemsandsignaldata.
Appraisal Motivation Expression Physiology Feeling FullCPM Baseline
Hate 77.99 80.25 72.11 68.56 72.73 82.23 79.84
Disgust 74.83 77.59 77.98 70.62 68.82 83.60 81.26
Angry 70.09 70.14 68.84 66.08 69.53 75.52* 71.12
Fear 73.52 77.30** 73.87 75.77* 76.02** 81.14*** 72.14
Disappointment 66.13 59.53 57.16 58.61 59.96 65.22 62.21
Sadness 70.45 68.16 73.91 68.48 69.29 78.70 78.01
Shame 70.97 70.62 75.05 70.24 66.58 78.46 81.16
Regret 72.10 65.79 61.01 61.95 66.48 69.15 73.76
Guilt 77.27 76.88 82.03 76.09 66.41 84.64 85.71
Contempt 67.55 68.53 61.86 63.52 61.32 69.22 71.83
Pleasure 73.64*** 71.83*** 74.34*** 60.45* 75.61*** 76.81*** 56.23
Joy 73.66*** 71.40*** 76.36*** 61.66** 75.47*** 76.96*** 55.83
Amusement 68.40*** 70.43*** 70.08*** 56.14 69.79*** 72.41*** 54.71
Pride 69.22*** 62.91*** 64.83*** 60.96*** 68.52*** 68.21*** 50.15
Admiration 61.83 60.50 63.25** 60.78 65.77*** 66.96*** 58.36
Contentment 69.89*** 65.96** 65.25** 62.93 73.93*** 73.57*** 59.68
Relief 55.20 47.32 57.96 58.00 58.10 60.74* 55.93
Interest 64.99** 68.24*** 67.44*** 56.80 67.63*** 71.90*** 58.87
Compassion 58.83 60.84 52.46 56.70 59.95 64.01* 58.46
Love 58.06** 54.68 62.09*** 58.21** 58.95** 62.67*** 52.08
tentlyoutperformedunimodalmodels,withimprovements In that study, it was observed that only the expression and
ranging from 0.02% to 12.77%. Therefore, this paper only feeling components held significance for the emotions of
presents the results for these multimodal models. Accord- joy, love, and satisfaction. One potential explanation could
ingly, the expression component was significant for inter- be that our study, which employed both subjective and
est, amusement, pride, joy, pleasure, love (p<0.001), con- objectivemeasures,possessedmoreinformativefeatures.
tentment, and admiration (p<0.01). This finding suggests Lastly, the feeling component is significant for interest,
that facial expressions, the most natural form of nonverbal amusement, pride, joy, pleasure, contentment, admiration
communication[41],areeffectivelycapturedbyfacialEMG (p<0.001), love, and fear (p<0.01). However, the feeling
signalsandCoreGRIDitems. component, traditionally the focus of previous research,
Like the expression component, a better performance demonstratedminimalperformanceinpredictingmostneg-
(0.45% - 8.27% improvement) was observed for the mul- ative emotions. This less prediction capacity for negative
timodal physiological component with a combination of emotions can be partially explained by the skewness of
self-reports and signal features. In parallel with previous the emotion ratings towards the lower end because VR
research, the physiology component showed ambiguous experiences were often perceived as pleasant even when
patternsforemotiondifferentiation[19],[36].Consequently, they involved challenging or fearful content [17]. Overall,
emotions such as pride (p<0.001), joy, love (p<0.01), plea- similartousingfullCPM,usingsinglecomponentsdemon-
sure, and fear (p<0.05) showed significance. Although fear stratesbetterdistinctionpowersforpositiveemotionsinour
showed similar behaviour, the remaining significant obser- experiment.
vations diverged from those seen in a previous study [63]. Next, we compared the results of training ML models
ycaruccA10
TABLE2
AccuracyoftheLGBMbinaryclassifiersforthedifferentiationofeachemotionwhenremovingonecomponentatatime.Asterisksindicatethe
significanceofresultsatp-value:*p<0.05,**p<0.01,***p<0.001.ThefullCPMisusedasthebaseline.Forclassificationwithoutappraisal,
motivation,andfeelingcomponents,onlytherelevantCoreGRIDitemswereremoved.Forclassificationexcludingthephysiologycomponent,
CoreGRIDphysiologyquestions,BVP,skintemperature,EDA,andrespirationwereremoved.Forclassificationwithouttheexpressioncomponent,
CoreGRIDexpressionquestions,EMGamplitudesignals,andthreefacialexpressionvalueswereremoved.FullCPMincludesallCoreGRID
itemsandsignaldata.(WO:Without)
WOAppraisal WOMotivation WOExpression WOPhysiology WOFeeling FullCPM
Hate 81.34 81.27 82.12 81.49 82.01 82.23
Disgust 83.76 80.37** 81.78* 84.32 83.92 83.60
Angry 75.03 72.87* 76.15 75.42 74.34 75.52*
Fear 81.16 79.55 81.12 80.63 80.26 81.14***
Disappointment 65.62 65.77 64.00 63.32 67.45 65.22
Sadness 76.16** 76.47* 75.86* 77.80 76.23 78.70
Shame 79.05 77.94 75.79* 78.09 77.83 78.46
Regret 70.17 69.94 72.4** 69.64 68.76 69.15
Guilt 84.16 83.12 83.76 84.23 85.28 84.64
Contempt 70.83 65.33 70.67 69.06 68.96 69.22
Pleasure 76.06 77.70 75.28 77.60 75.61 76.81***
Joy 78.37 78.09 77.98 77.16 77.51 76.96***
Amusement 71.30 70.43 70.95 71.42 72.68 72.41***
Pride 66.35 68.42 68.20 69.12 68.10 68.21***
Admiration 66.38 66.10 67.91 65.03 66.31 66.96***
Contentment 72.28 73.19 72.48 72.88 69.95* 73.57***
Relief 61.45 59.65 59.53 55.40* 59.98 60.74*
Interest 71.49 69.65 69.55 69.88* 70.38 71.90***
Compassion 60.14* 62.50 62.37 62.16 61.83 64.01*
Love 59.38 61.33 59.88 60.34 61.46 62.67***
usingsinglecomponents(Table1)withresultsfromexclud- catesthatthefeelingcomponentprovidessignificantdetails
ing one component (Table 2). To better understand the role forcontentment.
and importance of each component, in this experiment, we A different observation was made for regret, where
comparedtheaccuracyofeachmodelwithfullCPMasthe neither the single components nor the full CPM were
baseline (Table 2), as our objective here is to show the rela- significant, as shown in Table 1. However, removing the
tive importance of each component in the full CPM model. expressioncomponent(p<0.01)inTable2ledtoanaccuracy
Weexpectthatremovingmoreinformativecomponentswill improvement. This implies that the information encoded
resultinhigherreductionsinthepredictionpower. in the expression component might be just redundant. On
Table 1 showed that emotions like pleasure, joy, pride, the other hand, for the emotions of hate, disappointment,
fear, amusement, and admiration were significant when guilt,andcontempt,nomodelwassignificantinbothtables.
using single components or with the full CPM. However, In summary, this overall comparison suggests that while
in Table 2, removing each component did not lead to somecomponentshavecomplementarycounterparts,others
any significant difference when comparing these emotions containveryspecificorcomplementaryinformationforeach
(pleasure,joy,pride,fear,amusement,andadmiration)with emotion.
the full CPM results. This suggests that each component As the last analysis, we report the feature importance
can complement specific emotion differentiation, and the scores for the LGBM model while using the full CPM for
information from removed components is already encoded emotion prediction. Supplementary material Figures S3-S7
inothercomponents. illustratethescoresforeachcomponentwhiletrainedusing
AsshowninTable1,whencomparingaccuracywiththe the full CPM. Accordingly, a clear differentiation can be
chancelevelforemotionslikedisgust,sadness,andshame, observedamongtheclustersdepictedinFigure4.
no scenario was significant. However, removing different Overall, the model that includes all CPM self-reports
combinationsofmotivation,expression,andappraisalcom- and DWT signal features performs well in distinguishing
ponents resulted in a significant accuracy reduction com- emotions. Our results reveal that each component of the
pared to the full CPM performance (Table 2). This implies CPM may have a specific role in emotion differentiation.
that although these components may not directly impact Additionally, considering only one component of the CPM
the results, they provide complementary information not may limit our understanding of emotion formation. These
providedbyothercomponentsorthefullCPM. findingssuggestthatacompleteunderstandingofemotion
For interest, the full CPM (p<0.001) and single com- formationmayrequireconsideringtheinterplaybetweenall
ponents, except physiology, were significant. However, a fivecomponentsoftheCPM.
significantperformancereductionwasobservedwhenusing The accuracy improvements observed when using both
all components except physiology, suggesting that even subjective and objective measures highlight the value of
though it may not be important as a single component, incorporating objective measures in addition to subjective
it encodes relevant complementary information for inter- evaluations, particularly for components of emotion that
est. Similarly, for contentment, including only the feeling individualsmaynotbeawareof.Thissuggeststhatrelying
component(p<0.001)andremovingthefeelingcomponent solelyonsubjectiveevaluationsmaynotprovideacomplete
(p<0.01) both showed significant performance. This indi- understandingofemotionalstates.11
5 CONCLUSION researchworks.
Our manuscript aimed to study CPM as a framework to
examine the interconnected components and sub-processes
REFERENCES
that contribute to the formation of emotions. We opera-
tionalised a data collection using 27 interactive VR games
[1] K. R. Scherer, “Emotions are emergent processes: they require a
and measured the subjective ratings and physiological and
dynamiccomputationalarchitecture,”PhilosophicalTransactionsof
facialsignalsfrom39participants.Thisstudymakesseveral theRoyalSocietyB:BiologicalSciences,vol.364,no.1535,pp.3459–
significant contributions to the field of affective computing 3474,2009.
[2] R.W.Picard,Affectivecomputing. Cambridge:MITpress,2000.
and emotion recognition: (i) Using VR as an immersive
[3] H. Kober, L. F. Barrett, J. Joseph, E. Bliss-Moreau, K. Lindquist,
environment to induce emotions as a more ecologically
and T. D. Wager, “Functional grouping and cortical–subcortical
valid setup, (ii) It provides insights into the underlying interactionsinemotion:ameta-analysisofneuroimagingstudies,”
mechanismsthatlinkCPMandemotionalexperienceusing Neuroimage,vol.42,no.2,pp.998–1031,2008.
[4] K. A. Lindquist, T. D. Wager, H. Kober, E. Bliss-Moreau, and
a realistic emotion induction paradigm without assuming
L.F.Barrett,“Thebrainbasisofemotion:ameta-analyticreview,”
any pre-labels for emotional experience. This allows for Behavioralandbrainsciences,vol.35,no.3,pp.121–143,2012.
a better understanding of how these factors are related [5] J. J. R. Fontaine, Dimensional, basic emotion, and componential ap-
in a naturalistic setting and takes into account individual proaches to meaning in psychological emotion research1. Oxford:
OxfordUniversityPress,2013.
differences, (iii) Provides a comprehensive analysis of the
[6] D. Grandjean, D. Sander, and K. Scherer, “Conscious emotional
underlying dimensions that describe the emotional experi- experience emerges as a function of multilevel, appraisal-driven
ence, (iv) The study examines the contribution of different responsesynchronization,”Consciousnessandcognition,vol.17,pp.
CPM components and modalities to emotional experience, 484–95,2008.
[7] E. Harmon-Jones, C. Harmon-Jones, and E. Summerell, “On the
providing a nuanced and detailed understanding of their
importanceofbothdimensionalanddiscretemodelsofemotion,”
respectiveroles,and(v)Thestudyemploysalargersample Behavioralsciences,vol.7,no.4,p.66,2017.
sizeandgeneralisedMLmodelstoexamineemotionforma- [8] T.Song,W.Zheng,C.Lu,Y.Zong,X.Zhang,andZ.Cui,“Mped:A
tion,whichallowsformoreaccurateandreliablefindings. multi-modalphysiologicalemotiondatabasefordiscreteemotion
recognition,”IEEEAccess,vol.7,pp.12177–12191,2019.
Our study found that the CPM provides a good frame-
[9] P.J.Bota,C.Wang,A.L.N.Fred,andH.P.D.Silva,“Areview,
workforunderstandingemotionformationwhenemotions currentchallenges,andfuturepossibilitiesonemotionrecognition
are induced through interactive VR games, and it can help using machine learning and physiological signals,” IEEE Access,
vol.7,pp.140990–141020,2019.
differentiatebetweendiscreteemotions.WechoseVRgames
[10] J.A.M.Correa,M.K.Abadi,N.Sebe,andI.Patras,“Amigos:A
duetotheirimmersivenature,providingaricheremotional dataset for affect, personality and mood research on individuals
experience compared to still images or video clips. Thus, and groups,” IEEE Transactions on Affective Computing, pp. 1–1,
our focus in this manuscript is solely on understanding 2018.
[11] S.Koelstra,C.Muhl,M.Soleymani,J.Lee,A.Yazdani,T.Ebrahimi,
emotionsratherthananalysingtheimpactonVRgameplay.
T.Pun,A.Nijholt,andI.Patras,“Deap:Adatabaseforemotion
Each component of the CPM framework plays a specific analysis;usingphysiologicalsignals,”IEEETransactionsonAffec-
roleincapturingtheseemotions,andacombinationofself- tiveComputing,vol.3,no.1,pp.18–31,2012.
reportandobjectivemeasuresiseffectiveinunderstanding [12] G. Mohammadi and P. Vuilleumier, “A multi-componential ap-
proachtoemotionrecognitionandtheeffectofpersonality,”IEEE
theirunderlyingprocesses.Weidentifiedfivelatentdimen-
TransactionsonAffectiveComputing,pp.1–1,2020.
sions that underlie emotional experiences induced through [13] S. Ojha, J. Vitale, and M.-A. Williams, “Computational emotion
interactiveVRgames,whichmaybeuniversaltothehuman models:Athematicreview,”InternationalJournalofSocialRobotics,
2020.
experience of emotions. These findings have implications
[14] K. R. Scherer, “Towards a prediction and data driven computa-
forthedesignofsystemsthataimtorecogniseandrespond
tional process model of emotion,” IEEE Transactions on Affective
to emotions, such as in healthcare, education, and gaming. Computing,vol.12,no.2,pp.279–292,2021.
Furtherresearchisneededtofullyunderstandthecomplex [15] P.Ekman,BasicEmotions,2005,pp.45–60.
processesunderlyingemotionformationanddevelopmore [16] R.Plutchik,“Apsychoevolutionarytheoryofemotions,”vol.21,
no.4-5,pp.529–553,1982.
effectivemethodsforcapturingandanalysingemotions.
[17] B.MeulemanandD.Rudrauf,“Inductionandprofilingofstrong
It’simportanttoacknowledgeseverallimitationsinthis multi-componentialemotionsinvirtualreality,”IEEETransactions
study. Firstly, the imbalanced distribution of emotions is onAffectiveComputing,vol.PP,pp.1–1,2018.
[18] K. R. Scherer, “The dynamic architecture of emotion: Evidence
likelyinfluencedbytheoverallpleasantnessandimmersive
forthecomponentprocessmodel,”CognitionandEmotion,vol.23,
nature of the VR experience. Additionally, we collected no.7,pp.1307–1351,2009.
various other signals that were not utilised in our current [19] M. Q. Mene´trey, G. Mohammadi, J. Leita˜o, and P. Vuilleumier,
analysis. Furthermore, increasing the sample size could “Emotionrecognitioninamulti-componentialframework:therole
ofphysiology,”Frontiersincomputerscience,vol.4,p.773256,2022.
enhance the robustness of our interpretations. Moreover,
[20] B.MeulemanandK.R.Scherer,“Nonlinearappraisalmodeling:
we recognise the challenge of accurately capturing and re- An application of machine learning to the study of emotion
porting facial expressions lasting mere milliseconds within production,”IEEETransactionsonAffectiveComputing,vol.4,no.4,
a 3-minute period, along with participants’ difficulty in pp.398–411,2013.
[21] R.Somarathna,T.Bednarz,andG.Mohammadi,“Virtualreality
recalling and reporting these subtle expressions compre-
for emotion elicitation - a review,” IEEE Transactions on Affective
hensively. In future research, investigating the impact of Computing,pp.1–21,2022.
VR games on emotions, exploring the dynamic nature of [22] ——,“Anexploratoryanalysisofinteractivevr-basedframework
emotional responses, and understanding brain processes formulti-componentialanalysisofemotion,”in2022IEEEInterna-
tionalConferenceonPervasiveComputingandCommunicationsWork-
couldoffervaluableinsights.Movingforward,werecognise
shops and other Affiliated Events (PerCom Workshops), Conference
theimportanceofaddressingtheselimitationsinourfuture Proceedings,pp.353–358.12
[23] ——,“Multi-componentialanalysisofemotionsusingvirtualre- reactionstoemotionalfacialexpressions,”vol.11,no.1,pp.86–89,
ality,”inProceedingsofthe27thACMSymposiumonVirtualReality 2000.
SoftwareandTechnology. AssociationforComputingMachinery, [44] W. Sato, K. Murata, Y. Uraoka, K. Shibata, S. Yoshikawa, and
ConferenceProceedings,p.Article85. M. Furuta, “Emotional valence sensing using a wearable facial
[24] R. Somarathna, A. Quigley, and G. Mohammadi, “Multi- emgdevice,”ScientificReports,vol.11,no.1,p.5757,2021.
componential emotion recognition in vr using physiological sig- [45] G. Bernal, T. Yang, A. Jain, and P. Maes, “Physiohmd: A con-
nals,”ser.AI2022:AdvancesinArtificialIntelligence. Springer formable, modular toolkit for collecting physiological data from
InternationalPublishing,2022,ConferenceProceedings,pp.599– head-mounteddisplays,”inProceedingsofthe2018ACMInterna-
613. tionalSymposiumonWearableComputers,2018,p.160–167.
[25] J.J.R.Fontaine,K.R.Scherer,andC.Soriano,Thewhy,thewhat, [46] V. Kehri and R. Awale, “A facial emg data analysis for emotion
andthehowoftheGRIDinstrument1. Oxford:OxfordUniversity classificationbasedonspectralkurtogramandcnn,”International
Press,2013. Journal of Digital Signals and Systems, Smart, vol. 4, no. 1-3, pp.
[26] K. R. Scherer, J. R. F. Fontaine, and C. Soriano, CoreGRID and 50–63,2020.
MiniGRID: Development and validation of two short versions of the [47] J. Perdiz, G. Pires, and U. J. Nunes, “Emotional state detection
GRIDinstrument1. Oxford:OxfordUniversityPress,2013. basedonemgandeogbiosignals:Ashortsurvey,”in2017IEEE
[27] S. Marsella, J. Gratch, P. Petta et al., “Computational models of 5th Portuguese Meeting on Bioengineering (ENBENG), Conference
emotion,” A Blueprint for Affective Computing-A sourcebook and Proceedings,pp.1–4.
manual,vol.11,no.1,pp.21–46,2010. [48] L.Boot,“Facialexpressionsineeg/emgrecordings,”Thesis,2009.
[28] N.YongsatianchotandS.Marsella,“Computationalmodelsofap- [49] L.Schilbach,S.B.Eickhoff,A.Mojzisch,andK.Vogeley,“What’s
praisaltounderstandtheperson-situationrelation,”inMeasuring in a smile? neural correlates of facial embodiment during social
andModelingPersonsandSituations. Elsevier,2021,pp.651–674. interaction,”SocialNeuroscience,vol.3,no.1,pp.37–50,2008.
[29] D. Dupre´, A. Tcherkassof, and M. Dubois, “Emotions triggered [50] T.Luong,A.Lecuyer,N.Martin,andF.Argelaguet,“Asurveyon
byinnovativeproducts:Amulti-componentialapproachofemo- affectiveandcognitivevr,”IEEETransactionsonVisualizationand
tions for user experience tools,” in 2015 International Conference Graphics,Computer,2021.
onAffectiveComputingandIntelligentInteraction(ACII),Conference [51] C. Bassano, G. Ballestin, E. Ceccaldi, F. I. Larradet, M. Mancini,
Proceedings,pp.772–777. E. Volta, and R. Niewiadomski, “A vr game-based system for
multimodal emotion data collection.” New York, NY, USA:
[30] K. Gentsch, U. Beermann, L. Wu, S. Trznadel, and K. Scherer,
AssociationforComputingMachinery,2019.
“Temporal unfolding of micro-valences in facial expression
[52] I. Shumailov and H. Gunes, “Computational analysis of valence
evokedbyvisual,auditory,andolfactorystimuli,”AffectiveScience,
andarousalinvirtualrealitygamingusinglowerarmelectromyo-
vol.1,2020.
grams,” in 2017 Seventh International Conference on Affective Com-
[31] J.Leita˜o,B.Meuleman,D.VanDeVille,andP.Vuilleumier,“Com-
putingandIntelligentInteraction(ACII),2017,ConferenceProceed-
putational imaging during video game playing shows dynamic
ings,pp.164–169.
synchronizationofcorticalandsubcorticalnetworksofemotions,”
[53] V. Shuman, K. Schlegel, and K. Scherer, Geneva Emotion Wheel
PLOSBiology,vol.18,no.11,p.e3000900,2020.
RatingStudy,2015.
[32] K.Scherer,A.Dieckmann,M.Unfried,H.Ellgring,andM.Mor-
[54] W. Sato, T. Kochiyama, and S. Yoshikawa, “Physiological corre-
tillaro,“Investigatingappraisal-drivenfacialexpressionandinfer-
latesofsubjectiveemotionalvalenceandarousaldynamicswhile
enceinemotioncommunication,”Emotion,2019.
viewingfilms,”BiologicalPsychology,vol.157,p.107974,2020.
[33] C. van Reekum, T. Johnstone, R. Banse, A. Etter, T. Wehrle, and
[55] M. Gnacek, J. Broulidakis, I. Mavridou, M. Fatoorechi, E. Seiss,
K. Scherer, “Psychophysiological responses to appraisal dimen-
T.Kostoulas,E.Balaguer-Ballester,I.Kiprijanovska,C.Rosten,and
sionsinacomputergame,”CognitionandEmotion,vol.18,no.5,
C. Nduka, “emteqpro—fully integrated biometric sensing array
pp.663–688,2004.
for non-invasive biomedical research in virtual reality,” vol. 3,
[34] S. Katsigiannis and N. Ramzan, “Dreamer: A database for emo-
2022.
tion recognition through eeg and ecg signals from wireless low-
[56] D.S.Elvitigala,D.J.C.Matthies,andS.Nanayakkara,“Stressfoot:
cost off-the-shelf devices,” IEEE Journal of Biomedical and Health
Uncovering the potential of the foot for acute stress sensing in
Informatics,vol.22,no.1,pp.98–107,2018.
sittingposture,”vol.20,no.10,p.2882,2020.
[35] R.Subramanian,J.Wache,M.K.Abadi,R.L.Vieriu,S.Winkler,
[57] S. Lamb and K. Kwok, “Mssq-short norms may underestimate
andN.Sebe,“Ascertain:Emotionandpersonalityrecognitionus-
highlysusceptibleindividuals,”HumanFactors:TheJournalofthe
ingcommercialsensors,”IEEETransactionsonAffectiveComputing,
HumanFactorsandErgonomicsSociety,vol.57,2014.
vol.9,no.2,pp.147–160,2018.
[58] J.W.Peirce,“Psychopy—psychophysicssoftwareinpython,”Jour-
[36] S.D.Kreibig,“Autonomicnervoussystemactivityinemotion:A nalofneurosciencemethods,vol.162,no.1-2,pp.8–13,2007.
review,”BiologicalPsychology,vol.84,no.3,pp.394–421,2010.
[59] T.Mullen,Masteringblender. JohnWiley&Sons,2011.
[37] P.Schmidt,A.Reiss,R.Du¨richen,andK.V.Laerhoven,“Wearable- [60] [Online].Available:https://www.harfang3d.com/en US/
based affect recognition—a review,” Sensors, vol. 19, no. 19, p. [61] C.E.Izard,“Basicemotions,naturalkinds,emotionschemas,and
4079,2019. anewparadigm,”vol.2,no.3,pp.260–280,2007.
[38] G.Chen,X.Zhang,Y.Sun,andJ.Zhang,“Emotionfeatureanalysis [62] ——, “Emotion theory and research: Highlights, unanswered
andrecognitionbasedonreconstructedeegsources,”IEEEAccess, questions, and emerging issues,” Annual review of psychology,
vol.8,pp.11907–11916,2020. vol.60,pp.1–25,2009.
[39] J.TeoandJ.T.Chia,“Deepneuralclassifiersforeeg-basedemo- [63] G.Mohammadi,K.Lin,andP.Vuilleumier,“Towardsunderstand-
tionrecognitioninimmersiveenvironments,”in2018International ingemotionalexperienceinacomponentialframework,”in2019
ConferenceonSmartComputingandElectronicEnterprise(ICSCEE), 8th International Conference on Affective Computing and Intelligent
2018,ConferenceProceedings,pp.1–6. Interaction(ACII),2019,ConferenceProceedings,pp.123–129.
[40] I. Mavridou, E. Seiss, M. Hamedi, E. Balaguer-Ballester, and [64] J.A.Russell,M.Lewicka,andT.Niit,“Across-culturalstudyof
C.Nduka,“Towardsvalencedetectionfromemgforvirtualreality acircumplexmodelofaffect,”Journalofpersonalityandpsychology,
applications,”in12thInternationalConferenceonDisability,Virtual social,vol.57,no.5,p.848,1989.
RealityandAssociatedTechnologies(ICDVRAT2018). ReadingUK: [65] X.S.P.P.AlexandreSchaefer,Fre´de´ricNils,“Assessingtheeffec-
ICDVRAT,UniversityofReading,September2018. tivenessofalargedatabaseofemotion-elicitingfilms:Anewtool
[41] M.Perusqu´ıa-Herna´ndez,M.Hirokawa,andK.Suzuki,“Sponta- foremotionresearchers.”Cognition&Emotion,vol.24,pp.1153–
neousandposedsmilerecognitionbasedonspatialandtemporal 1172,2010.
patternsoffacialemg,”in2017SeventhInternationalConferenceon [66] J.R.Fontaine,K.R.Scherer,E.B.Roesch,andP.C.Ellsworth,“The
AffectiveComputingandIntelligentInteraction(ACII). IEEE,2017, world of emotions is not two-dimensional,” Psychol Sci, vol. 18,
ConferenceProceedings,pp.537–541. no.12,pp.1050–7,2007.
[42] J. T. Cacioppo, R. E. Petty, M. E. Losch, and H. S. Kim, “Elec- [67] K. R. Scherer, V. Shuman, J. J. R. Fontaine, and C. Soriano, The
tromyographicactivityoverfacialmuscleregionscandifferentiate GRID meets the Wheel: Assessing emotional feeling via self-report1.
thevalenceandintensityofaffectivereactions.”Journalofperson- Oxford:OxfordUniversityPress,2013.
alityandsocialpsychology,vol.50,no.2,p.260,1986. [68] G.Mohammadi,D.VanDeVille,andP.Vuilleumier,“Brainnet-
[43] U.Dimberg,M.Thunberg,andK.Elmehed,“Unconsciousfacial workssubservingfunctionalcoreprocessesofemotionsidentified13
withcomponentialmodeling,”CerebralCortex,vol.33,no.12,pp. GelarehMohammadiisaSeniorLecturer(As-
7993–8010,2023. sistantProf.)andheadofHuman-CentredCom-
[69] U. Beermann, G. Hosoya, I. Schindler, K. R. Scherer, M. Eid, puting (HCC) research group at the School
V. Wagner, and W. Menninghaus, “Dimensions and clusters of of Computer Science and Engineering, UNSW,
aestheticemotions:Asemanticprofileanalysis,”vol.12,no.1949, Sydney.HermainresearchinterestsareinAffec-
2021. tive Computing and Behavior Analysis with the
[70] R.ReisenzeinandC.Spielhofer,“Subjectivelysalientdimensions aimofbetterunderstandinghumansanddesign-
ofemotionalappraisal,”MotivationandEmotion,vol.18,no.1,pp. ing human-centred technologies. She received
31–77,1994. her Ph.D. in Electrical Engineering from EPFL,
[71] J. A. Russell, Reading emotions from and into faces: Resurrecting a Switzerland.Shewasapost-doctoralresearcher
dimensional-contextualperspective,ser.Studiesinemotionandsocial atIdiapResearchInstituteandlateratLabora-
interaction,2ndseries. Paris,France:EditionsdelaMaisondes toryforNeurologyandImagingofCognition,UniversityofGeneva.She
Sciencesdel’Homme,1997,pp.295–320. hasinvestigatedapproachesforpersonalityperception,emotionrecog-
[72] M. Gjoreski, I. Kiprijanovska, S. Stankoski, I. Mavridou, M. J. nition from a componential perspective, and understanding the neural
Broulidakis, H. Gjoreski, and C. Nduka, “Facial emg sensing basis of emotions. Gelareh has authored and co-authored more than
for monitoring affect using a wearable device,” Scientific reports, 70publications,includingseveralbookchapters.Shewasarecipientof
vol.12,no.1,pp.1–12,2022. the”GoogleAnitaBorgaward”(a.k.a.WomenTechmakers)in2013and
[73] P.EkmanandW.V.Friesen,“Facialactioncodingsystem,”Envi- hasbeenselectedasthefieldleaderinthehuman-computer-interaction
ronmentalPsychology&NonverbalBehavior,1978. domaininTheAustralian’sResearchReport,2019.
[74] J.KoryandS.K.D’Mello,“Affectelicitationforaffectivecomput-
ing,”ConferenceProceedings.
[75] M.Granato,D.Gadia,D.Maggiorini,andL.A.Ripamonti,“An
empiricalstudyofplayers’emotionsinvrracinggamesbasedon
adatasetofphysiologicaldata,”MultimediaToolsandApplications,
2020.
[76] M.J.Kim,A.M.Mattek,R.H.Bennett,K.M.Solomon,J.Shin,and
P. J. Whalen, “Human amygdala tracks a feature-based valence
signalembeddedwithinthefacialexpressionofsurprise,”Journal
ofNeuroscience,vol.37,no.39,pp.9510–9518,2017.
[77] M.G.Frank,P.Ekman,andW.V.Friesen,“Behavioralmarkersand
recognizabilityofthesmileofenjoyment,”Journalofpersonalityand
psychology,social,vol.641,pp.83–93,1993.
[78] C.F.Lima,P.Arriaga,A.Anikin,A.R.Pires,S.Frade,L.Neves,
and S. K. Scott, “Authentic and posed emotional vocalizations
trigger distinct facial responses,” Cortex, vol. 141, pp. 280–292,
2021.
[79] A. Gruebler, V. Berenz, and K. Suzuki, “Emotionally assisted
human–robot interaction using a wearable device for reading
facial expressions,” Advanced Robotics, vol. 26, no. 10, pp. 1143–
1159,2012.
[80] L.Inzelberg,D.Rand,S.Steinberg,M.David-Pur,andY.Hanein,
“A wearable high-resolution facial electromyography for long
term recordings in freely behaving humans,” Scientific reports,
vol.8,no.1,pp.1–9,2018.
[81] V. Chandra, A. Priyarup, and D. Sethia, “Comparative study
of physiological signals from empatica e4 wristband for stress
classification,”inInternationalConferenceonAdvancesinComputing
andDataSciences. Springer,ConferenceProceedings,pp.218–229.
[82] A.SavitzkyandM.J.E.Golay,“Smoothinganddifferentiationof
databysimplifiedleastsquaresprocedures,”AnalyticalChemistry,
vol.36,no.8,pp.1627–1639,1964.
[83] M. Sevil, M. Rashid, I. Hajizadeh, M. R. Askari, N. Hobbs,
R.Brandt,M.Park,L.Quinn,andA.Cinar,“Discriminationofsi-
multaneouspsychologicalandphysicalstressorsusingwristband
biosignals,” Computer Methods and Biomedicine, Programs in, vol.
199,p.105898,2021.
[84] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer,
“Smote: synthetic minority over-sampling technique,” Journal of
artificialintelligenceresearch,vol.16,pp.321–357,2002.
[85] J.Yih,L.D.Kirby,andC.A.Smith,“Profilesofappraisal,moti-
vation,andcopingforpositiveemotions,”CognitionandEmotion,
vol.34,no.3,pp.481–497,2020.
RukshaniSomarathnaiswiththeUNSW,Syd-
ney, where she is a Doctoral student at the
School of Computer Science and Engineering.
ShereceivedherBScHonsinInformationTech-
nology from the University of Moratuwa, Sri
Lanka,in2017.From2017to2019,shewasa
SoftwareEngineerwhowasinvolvedinthede-
velopmentofaHealthcareSystem.Hercurrent
research interests include Affective Computing,
ArtificialIntelligence,SignalProcessing,andVir-
tualReality.1
Exploring Emotions in Multi-componential Space
using Interactive VR Games
Rukshani Somarathna, Member, IEEE, and Gelareh Mohammadi, Member, IEEE
✦
1 SUPPLEMENTARY MATERIALS
TABLES1
Commerciallyavailabletwenty-sevenVRgamesfromtheSteam
platformusedinthestudywiththepre-taggeddominantemotion.
Dominant Games
emotion
Fear TheBrookhavenExperiment,KittyRescue,
PropagationVR,RichiesPlankExperience
Interest DoctorWho:therunway,Toran,Waltzofthe
Wizard(Legacy)
Amusement AngryBirdsVR:IsleofPigs,FruitNinjaVR,
Moss,Fujii
Joy CartoonNetworkJourneysVR,BeatSaber,
Futurejam, Tilt Brush, MSI Electric City:
CoreAssault
Contentment AFisherman’sTale,VirusPopper,EggTime
Admiration OceanRift
Relief Meditation VR, Catch & Release, Google
EarthVR,CosmicFlow
Compassion TheBookofDistance
Sadness Everybody’ssad
Angry DeadlyHunterVR
TABLES2
GenevaEmotionWheelquestionnaire
Whileplayingthisgame,Ifelt...
Interest/Alert/Curious Sadness/Downhearted/Unhappy
Amusement/Awe/Wonder Guilt/Repentant
Pride/Confident/Self-assured Regret
Joy/Glad/Happy Shame/Humiliated/Disgraced
Pleasure Disappointment
Contentment Fear/Scared/Afraid
Admiration Disgust/Distaste
Love/Closeness/Trust Contempt
Relief Hate/Distrust
Compassion Angry/Irritated/Annoyed
4202
rpA
4
]CH.sc[
1v93230.4042:viXra2
1,000
500
1-Notatall 2- 3- 4- 5-Strongly
0
Interest Amusement Pride Joy Pleasure Contentment Admiration Love Relief Compassion Sadness Guilt Regret Shame appointment Fear Disgust Contempt Hate Angry
s
Di
Discreteemotions
Fig.S1.Distributionofscoringsin20emotions.Colourscalescorrespondtotheproportionofsamples.
stnuoC3
powerless in the situation_A
power over the consequences of the event_A
incongruent with your standards/ideals_A
outcomes were a result of your behaviour_A
feel in control over the outcome_A
no urgency in the situation_A
unpredictable_A
pleasant_A
uncontrollable_A
important for and relevant your goals or needs_A
event was caused by a virtual agent_A
demanded an immediate response_A
events occurred suddenly/without warning_A
violated laws/social norms_A
caused by chance_A
caused negative or undesirable consequences_A
experience was predictable_A
want to sing and dance_M
want to run away (virtually)_M
motivated to tackle the situation at hand_M
urge to resist or oppose someone or something_M
urge to lash out verbally or physically_M
bored or unmotivated_M
motivated to overcome an obstacle_M
want to reverse what was happening_M
want the experience to stop_M
want the experience to continue_M
speech disturbances_E
shout or exclaim_E
cry_E
shut your eyes_E
frown_E
eyebrows go up_E
jaw drop_E
smile_E
sweat_P
feel warm_P
breathing getting faster_P
breathing slowing down_P
muscle tension_P
elevated heart rate_P
stomach clench up_P
feel nervous or weak in the knees_P
feel weak_F
feel bad_F
feel calm_F
feel energised_F
feel tired_F
feel good_F
prolonged heightened emotion_F
intense emotion_F
Hate Disgust Angry Fear appointment Sadness Shame Regret Guilt Contempt Pleasure Joy Amusement Pride Admiration Contentment Relief Interest Compassion Love
s
Di
Fig.S2.EmotionprofilesinCoreGRIDspacederivedfromhierarchicalclustering.Thesizeofthebubbleisproportionaltothevalueoftheweighted
averageCPMprofileforeachemotiontermafterscalingforvisualisation.4
powerless in the situation_A
power over the consequences_A
incongruent with your ideals_A
TABLES3 outcomes from your behaviour_A
feel in control over the outcome_A
CoreGRIDquestionnaire
no urgency in the situation_A
unpredictable_A
pleasant_A
Whileplayingthisgame,didyou/r... uncontrollable_A
CoreGRIDitem Component important for your goals/needs_A
experienceanintenseemotionalstate? Feeling caused by a virtual agent_A
demanded immediate response_A
experience a prolonged heightened emotional Feeling events occurred suddenly_A
state? violated laws/social norms_A
feelgood? Feeling caused by chance_A
caused negative consequences_A
feeltired? Feeling experience was predictable_A
feelenergised? Feeling
f
f
f
fe
e
e
ee
e
e
el
l
l
lc
b
w
na
a
eel
d
rm
a
v?
k
o?
?
usorweakintheknees?
F
PF Fee
e
hee
e
yl
l
l
sii in
n
n
iog
g
g
logy
Hate Disgust Angry Fear Disappointment Sadness Shame Regret Guilt Contempt Pleasure Joy Amusement Pride Admiration Contentment Relief Interest Compassion Love
stomachclenchup? Physiology
experienceanelevatedheartrate? Physiology
Fig. S3. Appraisal component feature importance obtained from the
experiencemuscletension? Physiology
LGBM model in section 4.5.2. The size of the bubble is proportional
feelyourbreathingslowingdown? Physiology
tothefeature’simportance.Resultshavebeenscaledforvisualisation.
feelyourbreathinggettingfaster? Physiology
feelwarm? Physiology
sweat? Physiology
smile? Expression
jawdrop? Expression
eyebrowsgoup? Expression
frown? Expression
shutyoureyes? Expression
cry? Expression want to sing and dance_M
want to run away (virtually)_M
shoutorexclaim? Expression
motivated to tackle situation_M
experiencespeechdisturbances? Expression urge to resist or oppose_M
wanttheexperiencetocontinue? Motivation urge to lash out_M
wanttheexperiencetostop? Motivation bored or unmotivated_M
wanttoreversewhatwashappening? Motivation motivated to overcome obstacle_M
feelmotivatedtoovercomeanobstacle? Motivation want to reverse_M
want to stop_M
feelboredorunmotivated? Motivation
want to continue_M
feelanurgetolashoutverballyorphysically? Motivation
f
s
f
fe
e
eoe
e
eml
l
le
m
ta htn
h
o
ai ttn
iu yvgr oa?g
t
ue
ed
wto
t ao
nr tte
a
ts
c
oi ks rlt
ue
no thr
ae
wo sp
ait
yp uo
a
(s vte
i io
rtns uo
a
am
t llh
yeo
)a
?n ne d?or MMM ooo
t
tt iii
v
vv aaa ttt iii
o
oo nnn Hate Disgust Angry Fear Disappointment Sadness Shame Regret Guilt Contempt Pleasure Joy Amusement Pride Admiration Contentment Relief Interest Compassion Love
wanttosinganddance? Motivation
feelthattheexperiencewaspredictable? Appraisal
Fig. S4. Motivation component feature importance obtained from the
think that the experience caused negative or Appraisal
LGBM model in section 4.5.2. The size of the bubble is proportional
undesirableconsequencesforyou?
tothefeature’simportance.Resultshavebeenscaledforvisualisation.
feeltheexperiencecausedbychance? Appraisal
feel that the experience violated laws/social Appraisal
norms?
feel the events occurred suddenly/without Appraisal
warning?
feel the experience demanded an immediate Appraisal
response?
eventwascausedbyavirtualagent? Appraisal
feeltheeventwasimportantforandrelevantto Appraisal
yourgoalsorneeds? sweat_P
feelliketheexperiencewasuncontrollable? Appraisal feel warm_P
breathing getting faster_P
thinktheexperiencewaspleasant? Appraisal
breathing slowing down_P
feelthattheexperiencewasunpredictable? Appraisal muscle tension_P
feelthattherewasnourgencyinthesituation? Appraisal elevated heart rate_P
feelincontrolovertheoutcome? Appraisal stomach clench up_P
feel that the outcomes were a result of your Appraisal nervous or weak in the knees_P
b
f
y
f
qe
ee
o
ue
eh
u
el
la
nrt
tv
ch
s
h
ei tao
aa
st
tu
n
or
t
d
y
f?
h
a
o
te
r hud
ee sx
h
e/p
a
vie
d
d
er nei
a
t
te
h
?ln
s
ec ?e pw owas erin oc vo en rgr tu he en ct ow nsit eh
-
A Ap pp pr ra ai is sa al
l
Hate Disgust Angry Fear Disappointment Sadness Shame Regret Guilt Contempt Pleasure Joy Amusement Pride Admiration Contentment Relief Interest Compassion Love
feelpowerlessinthesituation? Appraisal
Fig. S5. Physiology component feature importance obtained from the
LGBMmodelinsection4.5.2.Thesizeofthebubbleisproportionalto
thefeature’simportance.Resultshavebeenscaledforvisualisation.5
speech disturbances_E
shout or exclaim_E
cry_E
shut your eyes_E
frown_E
eyebrows go up_E
jaw drop_E
smile_E
Hate Disgust Angry Fear Disappointment Sadness Shame Regret Guilt Contempt Pleasure Joy Amusement Pride Admiration Contentment Relief Interest Compassion Love
Fig. S6. Expression component feature importance obtained from the
LGBMmodelinsection4.5.2.Thesizeofthebubbleisproportionalto
thefeature’simportance.Resultshavebeenscaledforvisualisation.
feel weak_F
feel bad_F
feel calm_F
feel energised_F
feel tired_F
feel good_F
heightened emotion_F
intense emotion_F
Hate Disgust Angry Fear Disappointment Sadness Shame Regret Guilt Contempt Pleasure Joy Amusement Pride Admiration Contentment Relief Interest Compassion Love
Fig.S7.FeelingcomponentfeatureimportanceobtainedfromtheLGBM
model in section 4.5.2. The size of the bubble is proportional to the
feature’simportance.Resultshavebeenscaledforvisualisation.