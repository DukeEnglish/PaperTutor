MEDIATE: Mutually Endorsed Distributed Incentive
Acknowledgment Token Exchange
PhilippAltmann1 KatharinaWinter1 MichaelKo¨lle1 MaximilianZorn1 ThomyPhan2
ClaudiaLinnhoff-Popien1
Abstract used(Dawes,1980). Yet,theavailabilityofcommunication
andexchangeisvitaltofosteringcooperationbetweenself-
Recent advances in multi-agent systems (MAS)
interestedindividuals. However, besidestheautonomous have shown that incorporating peer incentiviza-
interactionwithanenvironment,increasedprivacyrequire-
tion(PI)mechanismsvastlyimprovescooperation.
ments might require instances to conceal information re-
Especiallyinsocialdilemmas,communicationbe-
garding their current state (Tawalbeh et al., 2020). Peer
tweentheagentshelpstoovercomesub-optimal
incentivization(PI)isarecentbranchofresearchofferinga
Nashequilibria. However,incentivizationtokens
distinctsolutionforemergentcooperationbetweenagents.
needtobecarefullyselected. Furthermore,real-
Atitscore,PIenablesagentstoshapeeachother’sbehavior
worldapplicationsmightyieldincreasedprivacy
by exchanging reward tokens in addition to the environ-
requirements and limited exchange. Therefore,
mental reward (Phan et al., 2022; Lupu & Precup, 2020).
weextendthePIprotocolformutualacknowledg-
However,forproperintegrationandeffectiveincentiviza-
menttokenexchange(MATE)andprovideaddi-
tion, thoseexchangedtokensneedtobecarefullyconsid-
tionalanalysisontheimpactofthechosentokens.
ered,regardlessofwhethertheirvalueissetdynamicorasa
Buildinguponthoseinsights,weproposemutu-
hyperparameter. Fortherobustandscalableapplicabilityof
allyendorseddistributedincentiveacknowledg-
PImechanismsindecentralizedlearningscenarios,adaptive
menttokenexchange(MEDIATE),anextended
incentivizationtokensandmechanismstoagreeuponcom-
PIarchitectureemployingautomatictokenderiva-
montokenvaluesarerequired. Yet,currentapproachesare
tion via decentralized consensus. Empirical re-
missingsaidcoordinatedadaptability. Toovercomethese
sults show the stable agreement on appropriate
shortcomings,weprovidethefollowingcontributions:
tokensyieldingsuperiorperformancecompared
tostatictokensandstate-of-the-artapproachesin
differentsocialdilemmaenvironmentswithvari-
• We evaluate the effect of different centralized (com-
ousrewarddistributions.
mon)anddecentralized(varying)valuesfortheincen-
tivizationtoken.
1.Introduction
Recentadvancesinusingreinforcementlearning(RL)in • We propose mutually endorsed distributed incentive
multi-agentsystems(MAS)demonstratedtheirfeasibility acknowledgmenttokenexchange(MEDIATE),amech-
forreal-worldmulti-agentreinforcementlearning(MARL) anism for automatic token derivation based on the
applications. Those applications range from smart grids agents’ value estimate, and a consensus mechanism
(Omitaomu&Niu,2021)andfactories(Kimetal.,2020) tomediateaglobaltokenmaintaininglocalprivacy.
tointelligenttransportationsystems(Qureshi&Abdullah,
2013). Toassesstheagents’cooperationcapabilities,social
dilemmas producing tensions between the individual and • We provide ablation studies of the introduced token
collectiverewardmaximization(socialwelfare)areoften derivationandtheconsensusmechanismoverusinga
statictoken. Benchmarkcomparisonstostate-of-the-
1LMU Munich, Germany 2University of Southern Califor-
artPIapproachesshowthatMEDIATEcannegotiate
nia, Los Angeles, USA. Correspondence to: Philipp Altmann
appropriatetokensthatyieldimprovedcooperationand
<philipp.altmann@ifi.lmu.de>.
socialwelfareinvarioussocialdilemmaswithdifferent
Preprint.Underreview. rewardlandscapes.
1
4202
rpA
4
]AM.sc[
1v13430.4042:viXraMEDIATE:MutuallyEndorsedDistributedIncentiveAcknowledgmentTokenExchange
2.Background utilizes SSDs to analyze and experiment with the social
behaviorofdifferentlearningstrategies(Leiboetal.,2017).
2.1.ProblemFormulation
Toassesstheemergenceofcooperation,weemploytheIt-
We formulate our problem of a MAS as a stochastic eratedPrisoner’sDilemma(IPD),wheremutualdefection
gameM = ⟨D,S,Z,A,P,R⟩,withthesetofallagents constitutesaNashequilibrium(Axelrod,1980;Sandholm
D = {1,...,N}, a set S of states s at time step t, a set &Crites,1996). Toevaluatethescalabilityofourapproach,
t
A = ⟨A ,...,A ⟩ of joint actions a = ⟨a ⟩ , the weusetheCoinGamewithtwo,four,andsixagents(Lerer
1 N t t,i i∈D
transitionprobabilityP(s |s ,a ),andthejointreward &Peysakhovich,2017). Additionally,weusetheRescaled
t+1 t t
R(s ,a )=⟨r ⟩ ∈R. Furthermore,weassumeeach CoinGamewithtwoagentstoassesstherobustnessw.r.t.
t t t,i i∈D
agentitohaveaneighborhoodN ⊆D\{i},bounding varyingrewardlandscapes. Therateofowncoinsversusto-
t,i
itssetoflocalobservationsz =⟨z ⟩ ∈ZN,and talcoinscollectedreflectsoverallcooperation. Forinsights
t+1 t+1,i i∈D
the agents’ experience tuple ⟨τ ,a ,r ,z ⟩, where onlong-termcooperation,weuseHarvest,posingariskof
t,i t,i t,i t+1,i
τ ∈ (Z ×A ) is the agent’s history. Agent i selects thetragedyofthecommonstoself-interestedagents(Perolat
t,i i t
the next action based on a stochastic policy π (a |τ ). etal.,2017;Phanetal.,2022). Forfurtherdetailsaboutthe
i t,i t,i
Simultaneouslylearningagentscausenon-stationary, i.e., employedenvironments,pleaserefertoAppendixA.
varyingtransitionprobabilitiesovertime. Thegoalofeach
self-interested agent i is to find a best response π∗ that 2.3.PeerIncentivization
i
maximizestheexpectedindividualdiscountedreturn:
In MAS, cooperation connotes the joining of individual
∞ problem-solvingstrategiesofautonomousagentsintoacom-
(cid:88)
G t,i = γkr i,t+k, (1) binedstrategy(Crainic&Toulouse,2007).Theemergentco-
k=0 operationoflearningagentsnecessitatescoordination(Noe¨,
2006),whichposesavitalchallengetocurrentcommuni-
withadiscountfactorγ ∈[0,1). Fromtheperspectiveofan
cationprotocolsindecentralizedMARLscenarios(Jaques
agent,otheragentsarepartofitsenvironment,andpolicy
etal.,2019). PIisarecentbranchofresearch,focussingon
updatesbyotheragentsaffecttheperformanceofanagent’s
agentslearningtoactivelyshapethebehaviorofothersby
ownpolicy(Laurentetal.,2011). Theperformanceofπ
i sendingrewardsorpenalties(Phanetal.,2022;Yangetal.,
isevaluatedusingavaluefunctionV (s )=E [G |s ]for
i t π t,i t 2020). Thesepeerrewardsareprocessedlikeenvironment
all s ∈ S, with the joint policy π = ⟨π ⟩ (Bus¸oniu
t j j∈D rewards,enablingtheemergenceofcooperation. However,
etal.,2010). Boththepoliciesπ andthevaluefunctions
newdynamicsarisethroughtheincreasedinter-dependency,
V are approximated by independent neural networks pa-
whichcomeswithnewchallenges. Carefullydesigningthis
rameterized by θ and ω respectively. For simplicity we
rewardmechanismisessentialtoachievingagoodoutcome
omitthoseforthefollowingandusetheabbreviatedforms
(Lupu&Precup,2020).
V
i
= V iω(τ t,i) ≈ Vπi(s t) and π
i
= π iθ respectively. To
measure efficiency U of the whole MAS we furthermore
2.4.ConsensusinMulti-AgentSystems
considerthesocialwelfare(Sandholm&Crites,1996),mea-
sured by the sum of undiscounted returns over all agents Distributedsystemsuseconsensusalgorithmstodeducta
withinanepisodeuntiltimestepT: global average of local information (Schenato & Gamba,
2007). ForMAS,consensusdescribestheconvergenceof
T−1
(cid:88)(cid:88) agents on a mutual value via communication (Li & Tan,
U = r (2)
t,i
2019). Aconsensusalgorithmspecifiestheexecutionsteps
i∈D t=0
to reach consensus (Han et al., 2013). Bee swarms, bird
flocks,andothergroup-coordinatedspeciesshownaturalbe-
2.2.SocialDilemmas
havior(Amirkhani&Barshooi,2022)thatinspiresfurther
GameTheoryanalyzesbehavioramongrationalagentsinco- underlying concepts like leadership, voting, or decision-
operativeandcompetitivesituations(Russell,2010;Littman, making(Conradt&Roper,2005). Twomainapplicationar-
2001). Social dilemmas are Markov games that inhibit a easforconsensusalgorithmsaresensornetworks(Yuetal.,
specific reward structure, which creates tension between 2009) and blockchain technology (Monrat et al., 2019),
individual and collective reward maximization. Sequen- whichhasplayedanintegralroleincryptocurrenciesand
tialsocialdilemmas(SSD)aretemporallyextendedsocial provides promising solutions for IoT applications. Con-
dilemmas,inwhichthegamerepeatsoverseveraltimesteps sensusinsensornetworksmainlydealswiththefusionof
(Leibo et al., 2017). The Nash equilibrium is a situation distributeddata,especiallyfortime-criticaldata(Schenato
wherenoagentcanincreaseitsindividualrewardbychang- & Gamba, 2007) and uncertainty in large-scale networks
ing its strategy if all other agents maintain their current (Olfati-Saber & Shamma, 2005). Research in cryptocur-
strategy(Littman,2001;Sandholm&Crites,1996). MARL rencyandIoTfocusesonsynchronization(Caoetal.,2019),
2MEDIATE:MutuallyEndorsedDistributedIncentiveAcknowledgmentTokenExchange
agreement(Salimitari&Chatterjee,2018),andverification DIATE,LIOrequiresanadditionalmodeltobelearnedfor
ofactions(Lashkari&Musilek,2021)betweenentitiesin predictingthisvalue,causingadditionaloverhead. Learn-
distributedsystems. Thenumberofsophisticatedconsensus ingtoshare(LToS)alsoimplementstwopolicies,onefor
algorithmsisgrowingthroughtherisingimportanceofde- localobjectivessetbyahigh-levelpolicy(Yietal.,2021).
centralizedcoordinationmechanisms(Lashkari&Musilek, Peer-evaluation-baseddual-DQN (PED-DQN)letsagents
2021) in an increasingly digitally connected world. Our evaluatetheirreceivedpeersignalsw.r.t. theirenvironment
approach utilizes the cryptographic technique of additive rewardswithanadditionalDQNnetwork(Hostalleroetal.,
secretsharing,solvingtheaverageconsensusproblemfor 2020). Learningtoinfluencethroughevaluativefeedback
privacy-critical tasks (Li et al., 2019). MARL research (LIEF) learns to reconstruct the reward function of peers
onconsensusalgorithmshasbeenincreasingrecently,in- viafeedback. Theauthorscallforaninvestigationbetween
tendingtoreachanoptimaljointpolicyinadecentralized amanual,systematic,andlearnedconstructionofrewards
systemthatisrobusttounreliableagentsoradversarialat- (Merhej&Chetouani,2021). Fayad&Ibrahim(2021)use
tacks(Figuraetal.,2021). Toourknowledge,noresearch counterfactualsimulationstoderiveinfluentialactions. The
existsconcerningconsensusalgorithms,PIandRL. aboveconceptsmodifytheagentmodelsortheactionspace
to derive the intrinsic rewards. Rather than altering the
agentsthemselves,weutilizeanadditionalprotocollayer,
3.RelatedWork
servingasatoolforagents,yieldingincreasedflexibility.
Variousconceptsaffecttheachievementofemergentcoop-
MATE Mutualacknowledgmenttokenexchange(MATE)
erationinMAS.Learningwithopponent-learningaware-
proposed by Phan et al. (2022) controls the exchange of
ness (LOLA) (Foerster et al., 2018) and stable opponent
incentivesviaatwo-phasecommunicationprotocol. Inthe
shaping(SOS)(Letcheretal.,2019)considerthelearning
request phase of each time step, all agents evaluate their
processofotheragentsandshapethepolicyupdatesofop-
monotonicimprovement(MI)andpotentiallysendacknowl-
ponents. Natureandhumansocialbehavioralsoinspired
edgment tokens to all neighbors. In the response phase,
many concepts. Wang et al. (2019) developed an evolu-
agentsevaluatetheirMIw.r.t. thesumofenvironmentre-
tionaryapproachtocreateagentswithsocialbehaviorby
wardsandthereceivedtokenandrespondwithapositive
naturalselection. Otherworkfocusesonprosocialagents
ornegativetoken. Thistwo-wayhandshakeallowsagents
and intrinsic motivation thriving for the manifestation of
to give feedback to other agents when incentives are re-
socialnorms(Jaquesetal.,2019). Ecclesetal.(2019)di-
ceived, which fosters cooperation and outperforms na¨ıve
videdagentsintoinnovators,learningapolicy,andimitators,
learningandotherPIapproaches,likeLIOandGifting,in
whichreciprocateinnovators. Baumannetal.(2020)insert
variousbenchmarksregardingefficiencyandequalitymet-
anexternalplanningagentintotheenvironment,whichcan
rics(Phanetal.,2022). MATEusesacommunicationlayer
observeallagentsanddistributerewards.Overall,wedivide
andthusprovidesalightweightsolutionwithminimalin-
approachesfosteringemergentcooperationintoconstructed
terferencewiththeagentmodel. Duetothisflexibleand
artificialsocialassemblies,addedintrinsicmotivation,and
privacy-conservingdesign,weevaluateourapproachasan
externaloptimizationtechniques. Ourapproachcombines
extensionofMATE.However,notethatotherprotocolPI
thoseconcepts,usingsociallyinspiredmutualacknowledg-
solutions can also utilize MEDIATE. Overall, we aim to
menttoshapetheenvironmentalrewards.
eliminate the need for setting the exchange token before-
AlargecorpusinPIresearchfocusesonsimilarapproaches hand,whichdenotesacentrallimitationofMATE.
to learning incentives integrated into the model. Gifting
Giventheirdirectcombinationwiththeexternalreward,we
integratesthereward-giftingcapabilityintoagents’policies
arguethatincentivizationtokensaresensitiveparameters
asanadditionalaction. Differentrewardmechanismscan
to be carefully considered. Kuhnle et al. (2023) analyze
builduponthisconcept. Inzero-sumgifting,agentsreceive
the Harsanyi-Shapley value to determine the weight of a
apenaltyforeachsentrewardtobalancethetotalsumofre-
sidepaymentbasedonthestrategicstrengthofaplayerin
wards. Giftingcanalsobeonlyalloweduptoafixedbudget
two-playerscenarios. Valuedecompositionnetworks(Sune-
perepisodeasanalternativetopenalization. Withareplen-
hagetal.,2018),QMIX(Rashidetal.,2020),andQTRAN
ishablebudget,thereceptionofenvironmentrewardscan
(Sonetal.,2019)decomposethejointaction-valuefunction
rechargethisbudget(Lupu&Precup,2020). Learningto
intoagent-basedvaluefunctionstoachievecooperationand
incentivizeotherlearningagents(LIO)isanotherapproach
maximizesocialwelfare. Theseapproachesarebasedon
thatusesanincentivefunctiontolearnappropriatepeerre-
acentralizedvaluefunction,whereasourworkfocuseson
wards. Selectingarewardisnotpartoftheactionspacebut
independentlearnersinafullydecentralizedsetting. ME-
islearnedseparatelybyasecondmodel(Yangetal.,2020).
DIATEalsousesthevaluefunctiontoautomaticallyderive
Like LIO, MEDIATE derives incentives from the agents’
token values to be mixed with the environmental reward,
expectedenvironmentalreturn. However,incontrasttoME-
posingalightweightandefficientsolution.
3MEDIATE:MutuallyEndorsedDistributedIncentiveAcknowledgmentTokenExchange
(a)CentraltokenvaluesforCoinGame-2,-4,and-6 (b)DecentralizedTokenValuesfor
CoinGame-2
Figure1. Rateofowncoinsfordifferenttokenswhendeterminedcentralized(1a)anddecentralized(1b)
4.ImpactofIncentivizationTokens Toprovidefurtherinsightsintothedynamicsintroducedby
thechoiceofincentivizationtokenvalue,wemodifiedthe
As MATE was previously only evaluated with token val-
protocoltoallowtheagentstoexchangedisparatetokens.
ues of 1, we first aim to provide additional insights into
Werefertothismodeasdecentralized. Notethatusingau-
the impact of the incentivization token, supplying an ex-
tomatedtokenderivationinadecentralizedsettingwithout
tensive hyperparameter analysis across various choices,
amechanismforcoordinationorconsensusmightresultin
both per-agent (decentralized) and globally (centralized).
suchvaryingtokenvalues. Fig.1bmapstheinterpolated
Fig. 1a displays the level of cooperation measured by
cooperationlevelsinthetwo-agentCoinGamewiththeto-
the rate of own coins collected for different token val-
kensT ∈[0.25,0.5,1,2,4],asvaluesbetween1and2have
uesT ∈[0,0.25,0.5,0.75,1,1.5,2,2.5,3,4,8]intheCoin
previouslyshowntobesufficientcentraltokens,employed
Game with two, four, and six agents, as well as the two-
by both agents, measured by the rate of own coins. The
agent Coin Game with scaled rewards. We averaged all
results reveal that the token combinations (1,1) and (2,2)
results over five random seeds. The graphs display high
yieldthehighestcooperationrates. Bothtokenvaluesare
averagelevelsofcooperationforvalue1inallsettings,ex-
positionedintheappropriatetokenrangeinthecentralized
ceptforthedown-scaledCoinGame,wheretoken1fails,
comparison(cf.Fig.1a)andthecombinationscontainequal
indicatingthatthetokenvalueishighlydependentonthe
values, which appears to be a significant criterion in this
rewardlandscape. Insufficient(inferior)tokenvaluesfail
context. Althoughthecombination(1,2)includestwoap-
toachievethecollectiveobjective,causingself-interested
propriatevalues,thecooperationisdecreasedcomparedto
behavior. Conversely,over-exploitative(intemperate)token
the equal-valued exchange. With increasing discrepancy
valueslikewisefailtoyieldcooperativebehavior.Increasing
between the token values, cooperation further decreases,
thenumberofagents,avalueof1.5appearstobeoptimal
suggestingacorrelationbetweenthedegreeofvalueequal-
withinthepresentedrange, buttherequiredprecisionfor
ity and cooperation. Agents with over-exploitative token
successfulcooperationvaries. Alsotherangeoftokenval-
valuescanexerthighimpactonotheragents,especiallyon
uesthatyieldhighcooperationnarrows,retainingitsrelative
those with limited social influence due to smaller tokens,
positionbutexhibitinganincreasedsensitivitytothebound-
leadingtoamanipulativeformofcooperation. Equalbut
ariesofthatrange. Thediscrepancybetweentheoptimal
inappropriatetokenvaluesexhibitlowperformanceandco-
tokenvalueof1.5andvalue1increasesinthesix-agentCoin
operation,whichminimizesfor(0.25,0.25). Overall,this
Game. The analysis implies that factors like the domain,
evaluationsuggeststhattheexchangeofdecentralizedtoken
therewardlandscape,andthenumberofagentsinfluence
valuesmustbeappropriateandequaltoprovidefairnessand
incentiverewards. Therangeoftokenswithdistinctively
induceequalcooperation.
high cooperation is solely a function of the environment
rewardsbutdependsonthespecificdynamicsofthegame, Nevertheless,therateofowncoinscollectedforalltested
makingitchallengingtopredict. Afixedtokenvaluelacks tokensexcelstheperformanceofna¨ıvelearning,reflectedby
theadaptabilityrequiredfordiversesettings,makingapri- tokenvalue0. Conceptually,theseprospectsofMATEarise
oripredictionbasedonparametersettingsacomplextask. fromenablingagentstosharetheirsuccess, providedthe
Itbecomesevidentthatrewardstructuresarenotthesole benefitsaremutual. Asshownbefore,however,exchanging
determinants for selecting appropriate token weights and tokensofvalueT = 1mightnotalwaysbethesufficient
maynotevenbereliablyindicativeacrossallscenarios. choiceforanygivenenvironment.
4MEDIATE:MutuallyEndorsedDistributedIncentiveAcknowledgmentTokenExchange
5.MEDIATE Thisinitializationallowsfortheimmediateincorporationof
theMATEmechanism. WeimplementedMEDIATEextend-
ToelevatePItokenvaluesfromstatichyperparameterstody-
ingMATE-TD,whichemploystemporal-difference-based
namicallyadaptabledomain-specificquantities,wepropose
MIevaluationrootedinthevaluefunction. Toensureanap-
mutually endorsed distributed incentive acknowledgment
propriateacceptance-/rejection-ratioandthusanappropriate
tokenexchange(MEDIATE),combiningtwoprogressions
impactonthebehaviorofotheragents,thetokenvaluemust
(cf. Fig.2): First,weprovideanautomatedmechanismto
be proportional to the value function. Thus, we suggest
derivedynamicagent-basedincentivizationtokensT . To
i incrementingtokensbytherelativedifferencebetweenthe
ensureglobalconvergenceofsaidtokens,wesecondlypro-
meanstatevalueestimatesacrossconsecutiveepochs. By
videaconsensusmechanismthatensurestheprivacyofthe
doingso,MEDIATEtailorstokenstotheuniquedynamics
agents’localinformation.
ofeachdomain,therebyfosteringequalcooperationacross
diversesettings. Asameasureoftheprofit,wederivethe
MEDIATE Request Response mean accumulated value V¯ of an episode τ of length T
Token Derivation Consensus similartotheundiscountedreturn(cf. Eq.(1)):
(cid:80)T
V (τ )
V¯(τ)= t=0 i t,i (3)
i T
V referstothecurrentvalueapproximationofagenti. Fur-
i
thermore,weusethemedianofthemeanvaluesV¯ overan
epochofepisodestoimprovestability. ThelocaltokensT
i
are adjusted every epoch based on the difference (∆) be-
tweenthecurrentmedianofthemeanvalues(median(V ))
i
Figure2. MEDIATEArchitecture andthepreviousmedianofthemeanvalueV(cid:101)i:
Astheexchangedtokensacknowledgemutualbenefit,using
∇ =α·
∆(V(cid:101)i,median(V i))
·|rmin|, (4)
V itomeasurethevalueofastateforagentiisthemostnat- Ti
V(cid:101)i
i
uralchoiceforitsautomation. Furthermore,thisallowsus
toprovidealightweightextension,notrelyingonadditional withα=0.1asaconstantcomparabletoalearningrateand
models to be learned (in contrast to previous automatic theabsolutevalueofthelowestencounteredenvironmental
incentivizationapproaches). MEDIATEoperatesdecentral- rewardrmin (cf. Alg.1)asascalingfactor. Furthermore,
i
ized,individuallycalculatingatokenvalueforeachagent weusethepreviousmedianofthemeanvalueV(cid:101)ifornormal-
basedontheirrespectivevaluefunctions. Anagentdoesnot ization. Consequently,sufficientlylargenegativestatevalue
knowthevaluefunctionofotheragents,butweassumeover- estimates can cause positive tokens, which rise when the
allsimilarvaluefunctions. Thisassumptionensuresboth valuefurtherdecreases. Fornegativevalues,thetokenthus
independenceanddecentralizationbyenablinganagentto remainsproportionatetotheabsolutemagnitudeofthevalue
operatesolelybasedonitsdomain-specificmetricsandvari- function. Furthermore,theresultingtokenvalueisclamped
ables. Alg.1depictstheproposedmechanismforderiving topositivevaluesusingthemaxoperation(cf.Alg.1),send-
andupdatingindividualtokens: Allagentsinitiallysettheir ingazerotokenotherwise. ResemblingtheuseofaReLU
tokentoasmallbutnon-zerovalueof0.1todifferentiateit activationfunction(Agarap,2018),thisforcestheagentto
fromazero-valuedtokenthatwouldequatetona¨ıvelearning. sendnoincentivewhenunabletosendapositive. Bythis,
agentsadheretotheprincipleofNiceness,whichisacore
principleforthereciprocalstrategyofMATE,implyingno
Algorithm1Agent-wiseTokenDerivationwithMEDIATE
intentofdefectionintherequest(Phanetal.,2022).
SetupforAgenti∈D: T
i
←0.1;r imin ←∞;V(cid:101)i ←0
However,besidesusingappropriatetokens,findingsfrom
forEpochϵinEpochs;Agenti∈D do
theanalysisofdecentralizedtokensalsodemonstratedthe
V ←{} ▷Initializemeanvaluesforepoch
i needforequaltokenvaluesinthemutualexchange. There-
forRollout⟨τ ,a ,r ,...,τ ,a ,r ⟩inϵdo
0,i 0,i 0,i T,i T,i T,i fore,weextendMEDIATEwithaconsensusmechanismto
rmin ←min(rmin,⟨r ⟩)
i i 0...T,i reachanagreementonamutualtoken,increasingequality
V ←V ∪V¯(τ) ▷Calculatemeanvalue(3)
i i i andreducingtheimpactofoutlierswhilepreservingthepri-
endfor
vacyoftheagents’confidentialinformationusingadditive
T ←max(T(∗)+∇ ,0) ▷Updatelocaltoken(4)
i (i) Ti secretsharing. Allagentssetuptheconsensusexchangeby
V(cid:101)i ←median(V i) dividingtheirtokenvaluesintosharesforallagentsintheir
endfor neighborhoodN,reservingoneshareforprivacyreasons.
5MEDIATE:MutuallyEndorsedDistributedIncentiveAcknowledgmentTokenExchange
Thetokenisonlyreconstructablewhenaccountingforall Training is conducted for 5000 epochs, comprising ten
shares, whichprovidessecurityagainstprivacydefectors. episodes each. We averaged all of the following results
In the request phase, all agents i send the corresponding over eight random seeds. If not stated otherwise, all im-
shares [T△,...,T△] to all n neighbors. Each receiving plementationsusetheirdefaulthyperparametersfromthe
i,1 i,n
agentj accumulatesitsreceivedshares[T△,...,T△ ] correspondingsource. Pleaserefertotheappendixforfur-
j,1 j,m+1
fromitsmneighbors,includingitsreservedshare. Inthe therenvironment-andimplementationdetails.
responsephase,eachagentj sendstheaccumulatedshares
to all its neighbors. Each receiving agent i obtains the 6.1.EvaluationofMEDIATE
accumulatedsharesfromallneighbors,whichitaverages
Fig. 3 shows the evaluation results. The graphs indicate
overthenumberofshares,i.e.,thenumberofagentsN,to
thateithersynchronizedorisolatedMEDIATEupdatescon-
obtainthereconstructedconsensustokenT∗:
sistentlyachieveefficiencyandcooperationlevelsatleast
equivalenttoMATEinallexperimentalsettings,whichle-
(cid:80) (cid:80) T△
T∗ = i∈N j∈N i,j (5) gitimates their further investigation. As expected, na¨ıve
N learningfailstoreachemergentcooperation,againshowcas-
ingthecomparedenvironments’intricacy.
In domains like Harvest, with only partially connected
agentsandchangingtopologies,theconsensusprotocolin- Ingeneral,MEDIATEenhancestheperformanceofAuto-
cludesamulti-iterationresponsephase.Eachsummedshare MATE across all settings, except for the two-agent Coin
istaggedwithanID,senttoallneighbors,andforwarded Gamescenario,whereisolatedupdatesneitherimprovenor
overmultipletimestepstoensurenetwork-wideinformation deteriorate cooperation. The results imply that the com-
dissemination. Tointegratethereconstructedtokenintothe binedautomaticanddecentralizedmechanism-introduced
tokenderivationmechanism,weproposetwodifferentup- by MEDIATE - provides sufficient tokens to replace the
datemechanisms: IsolatedupdatesthelocaltokenT based original MATE token value 1. Furthermore, Figs. 3d-3f
i
onthepreviouslocaltoken,whichissharedindependently showthatallautomaticallyderivedtokensconvergewithin
viatheconsensusprotocol: max(T +∇ ,0). Incontrast, theinitial1000epochs,indicatingthepurposefulnatureof
i Ti
synchronizedreplacesthelocaltokenwiththereconstructed theproposedarchitecture.Incomparison,thecorresponding
token T∗ after the consensus phase: max(T∗ +∇ ,0). tokensofAutoMATEandMEDIATEallconvergetohigher
Ti
Consequently,onlythetokenupdate(cf. Alg.1)isaffected, tokenvaluesthanMATE,which,accordingtoourprelim-
eithersynchronizedwiththeconsensustokenT∗ordrifting inarystudies,aremoreoptimaltokens. Widerconfidence
independently. We will refer to the resulting variants as intervalsintokenconvergencearegenerallyassociatedwith
MEDIATE-IandMEDIATE-S. reducedefficiencyandcooperation,butintheCoinGame-4,
AutoMATEtokensconvergetoequivalentvaluesasthose
with isolated updates. However, although its confidence
6.ExperimentalResults
intervalisnarrower,AutoMATE’sperformanceisinferior
Toassesstheeffectoftheintroducedtokenderivationmech- duetothemissingtokencoordinationbetweentheagents.
anismandtheproposedconsensusarchitecture,weraneval- Comparing the two MEDIATE variants, isolated updates
uations comparing isolated and synchronized MEDIATE performbetterinbothCoinGamesettings.
in the IPD, CoinGame-2, and CoinGame-4. As an addi-
Inthenegative-valuedIPDdomain,synchronizedupdates
tional ablation, we use a reduced version with only the
showadvantages. Overall,incombinationwiththetoken
automateddecentralizedtokenderivation(cf. Alg.1)but
plots,theresultsshowthattheupdatevariantconvergingto
without any consensus mechanism, which we refer to as
asmallervalue,i.e.,therespectivelylessoptimisticvariant,
AutoMATE.Additionally, wecomparetheabovetona¨ıve
providessuperiortokensandthusyieldsimprovedefficiency
learningandMATEwithafixedtokenof1. Wemeasure
andcooperation. Giventheabsenceofadefinitivesuperior
cooperationinallCoinGameenvironmentsbytheratiobe-
optionbetweenthetwoMEDIATEvariants,weincludeboth
tweenowncoinscollected(occ)andtotalcoinscollected
inthebenchmarkcomparisons.
(tcc): owncoins = |occ|. Wecomparetheperformance
|tcc|
in the IPD and Harvest by the approaches’ efficiency (cf.
6.2.BenchmarkComparisons
Eq. (2)), as a metric for social welfare. Additionally, we
compareallMEDIATEablationsw.r.t. theconvergenceof Fig.4showsthebenchmarkresults. Table1summarizes
theirtokenvalue.TotestthescalabilityofMEDIATEandits thefinalperformancemetrics. Thetwo-agentCoinGame
robustnesstovaryingrewarddistributions,weprovidefur- featuresdown-scaledrewards(RCG-2),requiringagentsto
therevaluationsintheRescaledCoinGame-2,CoinGame-6, learncooperationunderminimalpositiveandnegativeenvi-
and Harvest, including benchmark comparisons to zero- ronmentrewards. Incontrasttothecomparedapproaches,
sum-andbudget-giftingandLIO. both MEDIATE variants achieve significantly higher re-
6MEDIATE:MutuallyEndorsedDistributedIncentiveAcknowledgmentTokenExchange
MEDIATE-S MEDIATE-I AutoMATE MATE Naive Learning
(a)IPD—Efficiency (b)CoinGame(2agents)—OwnCoins (c)CoinGame(4agents)—OwnCoins
(d)IPD—TokenValues (e)CoinGame(2agents)—TokenValues (f)CoinGame(4agents)—TokenValues
Figure3.MEDIATEEvaluation:ComparingthemeanEfficiency(Fig.3a)andrateofOwnCoins(Fig.3b,3c)ofNa¨ıveLearning(grey),
MATE(blue),AutoMATE(lightblue),MEDIATE-I(orange),andMEDIATE-S(green),andtheMeanTokenValue(Fig.3d,3e,3f)inthe
IPD(Fig.3a,3d),CoinGame-2(Fig.3b,3e),andCoinGame-4(Fig.3c,3f).Theshadedareasmarkthe95%confidenceintervals.
wardsandmasterthetask. Yet,isolatedupdatesexhibita privacyovertheagents’localvalueinformation.
slight performance advantage over synchronized updates.
MATE demonstrates moderate cooperation, slightly im-
Table1.FinalaverageoftherateofOwnCoinsintheRescaled
proving upon LIO. In contrast, the gifting methods and
CoinGame-2(RCG-2)andCoinGame-6(CG-6),andtheEfficiency
na¨ıvelearningonlyshowmarginalcooperation, although
inHarvestforsynchronizedandisolatedMEDIATE(MEDIATE-
Gifting-Budgetperformscomparablybetter. Theseresults S,MEDIATE-I),AutoMATE,MATE,LIO,Budget-andZerosum
againhighlightthesuperioradaptabilityofMEDIATEto Gifting(Budget-G,Zerosum-G),andNa¨ıveLearning.
unconventional, potentially challenging reward scenarios
thatyieldimprovedapplicabilitytovaryingtasks. RCG-2 CG-6 Harvest
MEDIATE-S 0.93±0.08 0.50±0.16 1212±20
In the six-agent Coin Game (CG-6), na¨ıve learning per-
MEDIATE-I 0.97±0.02 0.41±0.16 1232±17
formsworstalongsideGifting-ZerosumandGifting-Budget.
AutoMATE 0.86±0.08 0.18±0.09 1204±35
WhileLIOshowsamarginalimprovement,itstilllackssig-
MATE 0.69±0.01 0.39±0.03 1177±20
nificantlybehindMATEandMEDIATEregardingstrategic
LIO 0.69±0.10 0.17±0.11 1192±20
cooperation.MEDIATE-IperformssimilartoMATE,which
Budget-G 0.54±0.03 0.16±0.02 1232±23
potentiallycanbeattributedtothelimitedcapabilityofiso-
Zerosum-G 0.50±0.01 0.16±0.01 1230±20
latedupdatestomanagenegativereturns. MATEinitially
Na¨ıveL. 0.50±0.01 0.16±0.01 1220±25
demonstratesanoptimallearningcurvebutdeterioratesin
performanceafterward. Intermsofcooperation,MEDIATE
Overall,evaluationsdemonstratedthatemergentcoopera-
withsynchronizedupdatesemergesasperformingbest.
tionbetweenagentsfostersoptimalsocialwelfare. Appro-
Harvest demonstrates the ability of MEDIATE to benefit priaterewardweightscanboostequalcooperationinsocial
inpartiallyconnectedtopologies. Here,MEDIATEranks dilemmas, but such weights’ appropriateness depends on
among the top-performing approaches and enhances the thedomain,thenumberofagents,therewardstructure,or
performanceofMATEbyprovidinganappropriateincen- otherfactors. Involvingahighernumberofagentswithina
tivizationtoken.Itthusdemonstratesitsefficacyinfunction- domainincreasestherequiredprecision. Ourexperiments
ingevenwithinunreliableenvironmentswhilepreserving showthatatokenvalueof1-asproposedforMATE-is
notuniversallyappropriateinalldomainsorsettings. Inthe
7MEDIATE:MutuallyEndorsedDistributedIncentiveAcknowledgmentTokenExchange
MEDIATE-S MEDIATE-I MATE LIO
Gifting-Budget Gifting-Zerosum Naive Learning
(a)RCG-2—OwnCoins (b)CG-6—OwnCoins (c)Harvest(6Agents)—Efficiency
Figure4.BenchmarkComparison:MeanrateofOwnCoins(Fig.4a,4b)andEfficiency(Fig.4c)ofMEDIATE-S(green),MEDIATE-I
(orange),MATE(blue),LIO(red),Budget-Gifting(purple),Zerosum-Gifting(pink)andNa¨ıveLearning(grey)intheRescaledCoinGame-
2(RCG-2)(Fig.4a),CoinGame-6(CG-6)(Fig.4b),andHarvest(Fig.4c).Theshadedareasmarkthe95%confidenceintervals.
down-scaledtwo-agentCoinGame,tokenvalue1isinappro- capable of finding appropriate tokens. Computationally,
priateandinthesix-agentCoinGame,itdoesnotachieve MEDIATEiscomparabletoMATEwhileovercomingits
optimal cooperation. Yet across all domains, MEDIATE centrallimitationofstatictokenvalues. Theonlyaddition
exhibits strong adaptability while consistently delivering of deriving consented tokens at each update is a sum of
good performance, even in challenging cooperative tasks constant values with linear complexity. Furthermore, the
suchasthesix-agentCoinGame,scenarioswithcomplex tokenextendsonthevalueapproximation. Thus,compared
rewardlandscapes,orunreliableenvironmentswithpartially toLIO,noadditionalmodelneedstobelearned.
connectedneighborhoods,likeHarvest.
Yet, even though not apparent in the evaluated social
dilemmaenvironments,thisdependenceonarobustvalue
7.Conclusion estimatealsodepictsacentrallimitationofMEDIATE.Fur-
thermore,theevaluatedupdatemechanismsshowedpoten-
In this work, we proposed mutually endorsed distributed
tially unstable and prone to outliers. Thus, future work
incentive acknowledgment token exchange (MEDIATE).
shouldfocusonproducingmoreaccuratetokens,especially
MEDIATE introduces automated PI tokens in decentral-
foranincreasednumberofagents,makingtheoverallalgo-
ized MAS with a consensus architecture and two agent-
rithmmorereliableinprecision-requiringdomainslikethe
individualupdatemechanisms.
RescaledCoinGame. Also,whileshownrobusttoscaled
Tokendecentralizationallowsagentstousedifferenttokens rewardlandscapes,increasingnumbersofagents,andlong-
in the exchange. Experiments on the impact of different termcooperationscenarioslikeHarvest,MEDIATEshould
tokensinsocialdilemmassuggestthatequalandappropri- betestedforunreliableconnectionsordefectivescenarios.
atetokenvaluesfosterimprovedsocialwelfare. MEDIATE
Overall,MEDIATEprovidesalightweightandrobustframe-
integratesthegradientoftheagents’localvaluefunction
worktoassesscommunicationconsensusmechanismswith
approximation to derive appropriate tokens matching the
automatedpeerincentivesforcreatingemergentcooperation
externalrewards. Toachieveconsensusonequaltokens,we
invariousscenariosofsocialdilemmas.
propose extending the MATE protocol based on additive
secretsharing,enablingtheidentificationofthetokenaver-
ImpactStatement
agethroughthetokenexchangewhileadheringtoprivacy
requirements. The consensus protocol is independent of
MEDIATErepresentsaprotocoladditiontoderivedecentral-
theunderlyingalgorithmfortokenderivation. Wefurther-
izedtokenvaluesforcurrentpeerincentivizationapproaches
moreevaluatetwotoken-updatevariations: Asynchronized
using a privacy-preserving consensus mechanism. Using
mechanismbasedonthereconstructedglobaltokenandan
thoseimprovedtokenvaluesoverallincreasessystematic
isolatedmechanismusingthepreviouslocaltoken.
cooperationbetweenself-interestedagents. Eventhough,in
Benchmark evaluations showed that MEDIATE achieves theory,thesystemisdesignedtoberobustw.r.t. defectors,
highsocialwelfareinalltesteddomains. Inallevaluated additionalevaluationswithadversarieswouldbehelpful.
settings, MEDIATE improves the performance of MATE
andevenoutperformsormatchesthebest-performingbase-
lines. MEDIATErepresentsarobustandadaptivesolution
8MEDIATE:MutuallyEndorsedDistributedIncentiveAcknowledgmentTokenExchange
References Han,Y.,Lu,W.,andChen,T. Clusterconsensusindiscrete-
timenetworksofmultiagentswithinter-clusternonidenti-
Agarap, A. F. Deep learning using rectified linear units
calinputs. IEEETransactionsonNeuralNetworksand
(relu). arXivpreprintarXiv:1803.08375,2018.
LearningSystems,24(4):566–578,2013.
Amirkhani, A. and Barshooi, A. H. Consensus in multi-
Hostallero,D.E.,Kim,D.,Moon,S.,Son,K.,Kang,W.J.,
agentsystems: areview. ArtificialIntelligenceReview,
andYi,Y.Inducingcooperationthroughrewardreshaping
55(5):3897–3935,2022.
basedonpeerevaluationsindeepmulti-agentreinforce-
mentlearning. InProceedingsofthe19thInternational
Axelrod, R. Effective choice in the prisoner’s dilemma.
ConferenceonAutonomousAgentsandMultiAgentSys-
Journalofconflictresolution,24(1):3–25,1980.
tems,pp.520–528,2020.
Baumann,T.,Graepel,T.,andShawe-Taylor,J. Adaptive
Jaques,N.,Lazaridou,A.,Hughes,E.,Gulcehre,C.,Ortega,
mechanismdesign: Learningtopromotecooperation. In
P., Strouse, D., Leibo, J.Z., andDeFreitas, N. Social
2020InternationalJointConferenceonNeuralNetworks
influence as intrinsic motivation for multi-agent deep
(IJCNN),pp.1–7.IEEE,2020.
reinforcementlearning. InInternationalconferenceon
machinelearning,pp.3040–3049.PMLR,2019.
Bus¸oniu,L.,Babusˇka,R.,andDeSchutter,B. Multi-agent
reinforcement learning: An overview. Innovations in
Kim, Y. G., Lee, S., Son, J., Bae, H., and Do Chung, B.
multi-agent systems and applications-1, pp. 183–221,
Multi-agentsystemandreinforcementlearningapproach
2010.
fordistributedintelligenceinaflexiblesmartmanufac-
turing system. Journal of Manufacturing Systems, 57:
Cao,B.,Li,Y.,Zhang,L.,Zhang,L.,Mumtaz,S.,Zhou,Z.,
440–450,2020.
andPeng,M. Wheninternetofthingsmeetsblockchain:
Challengesindistributedconsensus. IEEENetwork,33
Kuhnle, A., Richley, J., and Perez-Lavin, D. Learning
(6):133–139,2019.
strategicvalueandcooperationinmulti-playerstochas-
tic games through side payments. arXiv preprint
Conradt,L.andRoper,T.J. Consensusdecisionmakingin
arXiv:2303.05307,2023.
animals. Trendsinecology&evolution,20(8):449–456,
2005. Lashkari, B. and Musilek, P. A comprehensive review
ofblockchainconsensusmechanisms. IEEEAccess,9:
Crainic,T.G.andToulouse,M.Explicitandemergentcoop-
43620–43652,2021.
erationschemesforsearchalgorithms. InInternational
ConferenceonLearningandIntelligentOptimization,pp. Laurent,G.J.,Matignon,L.,Fort-Piat,L.,etal. Theworld
95–109.Springer,2007. ofindependentlearnersisnotmarkovian. International
JournalofKnowledge-basedandIntelligentEngineering
Dawes,R.M. Socialdilemmas. Annualreviewofpsychol-
Systems,15(1):55–64,2011.
ogy,31(1):169–193,1980.
Leibo, J. Z., Zambaldi, V., Lanctot, M., Marecki, J., and
Eccles, T., Hughes, E., Krama´r, J., Wheelwright, S., and
Graepel, T. Multi-agent reinforcement learning in se-
Leibo,J.Z. Learningreciprocityincomplexsequential quential social dilemmas. In Proceedings of the 16th
socialdilemmas. arXivpreprintarXiv:1903.08082,2019. ConferenceonAutonomousAgentsandMultiAgentSys-
tems,pp.464–473,2017.
Fayad,A.andIbrahim,M. Influence-basedreinforcement
learningforintrinsically-motivatedagents.arXivpreprint Lerer,A.andPeysakhovich,A. Maintainingcooperation
arXiv:2108.12581,2021. in complex social dilemmas using deep reinforcement
learning. arXivpreprintarXiv:1707.01068,2017.
Figura,M.,Kosaraju,K.C.,andGupta,V. Adversarialat-
tacksinconsensus-basedmulti-agentreinforcementlearn- Letcher,A.,Foerster,J.,Balduzzi,D.,Rockta¨schel,T.,and
ing. In2021AmericanControlConference(ACC),pp. Whiteson,S. Stableopponentshapingindifferentiable
3050–3055.IEEE,2021. games. InInternationalConferenceonLearningRepre-
sentations,2019.
Foerster, J., Chen, R. Y., Al-Shedivat, M., Whiteson, S.,
Abbeel, P., and Mordatch, I. Learning with opponent- Li, Q., Cascudo, I., and Christensen, M. G. Privacy-
learningawareness. InProceedingsofthe17thInterna- preserving distributed average consensus based on ad-
tionalConferenceonAutonomousAgentsandMultiAgent ditivesecretsharing. In201927thEuropeanSignalPro-
Systems,pp.122–130,2018. cessingConference(EUSIPCO),pp.1–5.IEEE,2019.
9MEDIATE:MutuallyEndorsedDistributedIncentiveAcknowledgmentTokenExchange
Li,Y.andTan,C. Asurveyoftheconsensusformulti-agent Salimitari,M.andChatterjee,M. Asurveyonconsensus
systems. SystemsScience&ControlEngineering,7:468 protocolsinblockchainforiotnetworks. arXivpreprint
–482,2019. arXiv:1809.05613,2018.
Littman, M.L. Value-functionreinforcementlearningin Sandholm,T.W.andCrites,R.H.Multiagentreinforcement
markovgames. Cognitivesystemsresearch,2(1):55–66, learningintheiteratedprisoner’sdilemma. Biosystems,
2001. 37(1-2):147–166,1996.
Lupu,A.andPrecup,D. Giftinginmulti-agentreinforce- Schenato,L.andGamba,G. Adistributedconsensusproto-
mentlearning. InProceedingsofthe19thInternational colforclocksynchronizationinwirelesssensornetwork.
Conference on autonomous agents and multiagent sys- In200746thieeeconferenceondecisionandcontrol,pp.
tems,pp.789–797,2020. 2289–2294.IEEE,2007.
Son, K., Kim, D., Kang, W.J., Hostallero, D.E., andYi,
Merhej,R.andChetouani,M. Lief: Learningtoinfluence
Y. Qtran: Learningtofactorizewithtransformationfor
throughevaluativefeedback. InAdaptiveandLearning
cooperativemulti-agentreinforcementlearning. InInter-
AgentsWorkshop(AAMAS2021),2021.
nationalconferenceonmachinelearning,pp.5887–5896.
Monrat, A. A., Schele´n, O., and Andersson, K. A sur- PMLR,2019.
veyofblockchainfromtheperspectivesofapplications,
Sunehag,P.,Lever,G.,Gruslys,A.,Czarnecki,W.M.,Zam-
challenges,andopportunities. IEEEAccess,7:117134–
baldi,V.,Jaderberg,M.,Lanctot,M.,Sonnerat,N.,Leibo,
117151,2019.
J.Z.,Tuyls,K.,andGraepel,T. Value-decompositionnet-
Noe¨, R. Cooperation experiments: coordination through worksforcooperativemulti-agentlearningbasedonteam
communicationversusactingaparttogether. Animalbe- reward. InProceedingsofthe17thInternationalConfer-
haviour,71(1):1–18,2006. enceonAutonomousAgentsandMultiAgentSystems,pp.
2085–2087,2018.
Olfati-Saber, R. and Shamma, J. S. Consensus filters for
Tawalbeh,L.,Muheidat,F.,Tawalbeh,M.,andQuwaider,
sensor networks and distributed sensor fusion. In Pro-
M. Iotprivacyandsecurity: Challengesandsolutions.
ceedingsofthe44thIEEEConferenceonDecisionand
AppliedSciences,10(12):4102,2020.
Control,pp.6698–6703.IEEE,2005.
Wang,J.X.,Hughes,E.,Fernando,C.,Czarnecki,W.M.,
Omitaomu,O.A.andNiu,H. Artificialintelligencetech-
Due´n˜ezGuzma´n,E.A.,andLeibo,J.Z. Evolvingintrin-
niquesinsmartgrid: Asurvey. SmartCities,4(2):548–
sicmotivationsforaltruisticbehavior. InProceedingsof
568,2021.
the18thInternationalConferenceonAutonomousAgents
Perolat, J., Leibo, J. Z., Zambaldi, V., Beattie, C., Tuyls,
andMultiAgentSystems,pp.683–692,2019.
K.,andGraepel,T. Amulti-agentreinforcementlearning
Yang, J., Li, A., Farajtabar, M., Sunehag, P., Hughes, E.,
modelofcommon-poolresourceappropriation,2017.
andZha,H. Learningtoincentivizeotherlearningagents.
AdvancesinNeuralInformationProcessingSystems,33:
Phan, T., Sommer, F., Altmann, P., Ritz, F., Belzner, L.,
15208–15219,2020.
and Linnhoff-Popien, C. Emergent cooperation from
mutual acknowledgment exchange. In Proceedings of
Yi, Y., Li, G., Wang, Y., and Lu, Z. Learning to share
the21stInternationalConferenceonAutonomousAgents
in multi-agent reinforcement learning. arXiv preprint
andMultiagentSystems,pp.1047–1055,2022.
arXiv:2112.08702,2021.
Qureshi,K.N.andAbdullah,A.H. Asurveyonintelligent Yu,W.,Chen,G.,Wang,Z.,andYang,W. Distributedcon-
transportationsystems. Middle-EastJournalofScientific sensusfilteringinsensornetworks. IEEETransactions
Research,15(5):629–642,2013. onSystems,Man,andCybernetics,PartB(Cybernetics),
39(6):1568–1577,2009.
Rashid, T., Samvelyan, M., De Witt, C. S., Farquhar, G.,
Foerster,J.,andWhiteson,S. Monotonicvaluefunction
factorisationfordeepmulti-agentreinforcementlearning.
TheJournalofMachineLearningResearch,21(1):7234–
7284,2020.
Russell, S. J. Artificial intelligence a modern approach.
PearsonEducation,Inc.,2010.
10MEDIATE:MutuallyEndorsedDistributedIncentiveAcknowledgmentTokenExchange
A.Environments
Iterated Prisoner’s Dilemma The Iterated Prisoner’s
Table2.Prisoner’sDilemmarewardallocation. Eachcellcon-
Dilemma (IPD) is the repeated game of the Prisoner’s
tainstherespectivepayoffsforeachofthetwoplayersbasedon
Dilemma, depicted in Table 2. At each time step, the two
theirchoiceofcooperationordefection.
players must choose between cooperation and defection to
maximizetheirpayoff(Axelrod,1980;Hostalleroetal.,2020).
Cooperate Defect
Mutual defection constitutes a Nash equilibrium. If both
Cooperate (-1,-1) (-3,0)
agentsdefect,noagentisincentivizedtochangeitsstrategy
Defect (0,-3) (-2,-2)
to cooperation in the next step if the other agent remains a
defector. Ifbothagentsswitchedtheirstrategytocooperate,
bothwouldreceivealowerpenalty.
CoinGame CoinsorCoinGameisanSSDconceptualizedbyLerer&Peysakhovich(2017).TheCoinGame-N comprises
N ∈ {2,4,6}agentsona3x3, 5x5, and7x7gridrespectively(cf. Figs.5a-5b). Adistinctcoloridentifieseachagent.
Initially,allN agentsandonerandom-coloredcoinspawnatrandompositions. Thecolorofthecoinmatchesoneofthe
agents. Anagentcandistinguishwhetherthecoinmatchesitsowncolorornot. Theactionspaceofeachagentcomprises
fourdirectionsofmovementA∈{left,right,up,down}. Acoiniscollectedwhenanagentmovestoitsposition. The
environmentdiscardsactionsviolatingitsbounds. Ifanagentcollectsanycoin,itreceivesarewardof+1. Ifthecolor
matchesadifferentagent,thatagentispenalizedwith−2. Ifmultipleagentscollectacoinsimultaneously,thematching
agentreceivesapenaltyof−1. Onceacoiniscollected,anewcoinspawns. Toevaluatevaryingrewardscales,weadded
theRescaledCoinGame-2variationwithdownsizedrewards(i.e.,scaledby0.1),suchthatthepositiverewardbecomes
+0.1andthepenaltyweighs−0.2. Theratiobetweenrewardandpenaltyremainsunchanged. Self-interestedagentswill
collectallcoinsregardlessofcolorsincethisstrategyimposesonlypositiverewardsonthemselves. TheNashequilibriumis
reachedifallagentsfollowthisstrategysincerefrainingfromcollectingotheragents’coinsonlyreducesanagent’sown
rewardswithoutmitigatingthepenaltiesincurredfromtheactionsofotheragents. However,ifallagentscollecttheirown
coins,eachagentprofitsfromthereducedpenalties,andsocialwelfarecanbemaximized. Tomeasurethelevelofstrategic
cooperationinthisdomain,weevaluatetherateofowncoinsw.r.t. tothetotalofcollectedcoins.
Harvest TheCommonsgameisconceptualizedbyPerolatetal.(2017)andadaptedbyPhanetal.(2022),whereitis
named Harvest. In Harvest, agents move on a 25x9-sized grid to collect apples. The Harvest grid, including the fixed
positioningoftheapples,isdisplayedinFig.5d. Appleshavearegrowthrate,whichdependsonthenumberofexisting
applesinthelocalarea. Moreapplesintheareacauseahigherregrowthrateofcollectedapples. Ifnoapplesremaininthe
area,noapplesregrow. Self-interestedagentsmaximizetheirownappleharvest,butinaMAS,agentshavetorefrainfrom
simultaneousapplecollectiontoavoidtheultimatedepletionofresources(thetragedyofthecommons). Thisrequirementis
theNashequilibriumofHarvest,asasingleagentcannotimproveitsrewardsbyrefrainingfromapplecollectionwhen
otheragentswillcontinuetodiminishtheresources. Onlyifallagentscooperatetheycanmaximizetheirlong-termrewards.
Agentscantagotheragentstoremovethemfromthegamefor25timesteps. (Perolatetal.,2017). Inadditiontoapositive
rewardof+1foranappleharvest,eachtimestepposesatimepenaltyof−0.1. Furthermore,agentsonlyhaveaccesstoa
partialobservationsurroundingtheirposition. Agentscanonlycommunicatewithagentsintheirneighborhoodinanareaof
7x7tiles. Inadditiontomovinginfourdirections(asforthecoingame),theactionspacecomprisesfouractionstotagall
neighboragentsinthefourdirections. Movingtowardaboundaryresultsinnomovement. Onlyoneagentcanharvestan
appleortaganotheragentatatime. Theorderofactionsateachtimestepisrandom.
(a)CoinGame-2 (b)CoinGame-4 (c)CoinGame-6 (d)Harvest(6agents)
Figure5. EvaluationEnvironments
11MEDIATE:MutuallyEndorsedDistributedIncentiveAcknowledgmentTokenExchange
B.ImplementationDetails
Weusethefollowinghyperparameters:
Table3. EnvironmentParameters
IPD CoinGame2 CoinGame4 CoinGame6 Harvest
Numactions|A| 2 4 4 4 9
Observationdimension|Z| 2 36 100 196 196
Discountfactorγ 0.95 0.95 0.95 0.95 0.99
TimeLimit 150 150 150 150 250
LIOmaxincentive 3 2 2 2 0.25
Table4. TrainingParameters
Name Value
Epochs 5000
Episodesperepoch 10
Networkarchitecture Sharedfullyconnectedprepossessingnet: |Z|(cid:55)→64
Activationfunction ExponentialLinearUnit(ELU)
Hiddensize {64,64}
Actorhead 64(cid:55)→|A|(Softmax)
Critichead 64(cid:55)→1
Optimizer Adam
Learningrate 0.001
Clippingnorm 1
Historylength 1
12