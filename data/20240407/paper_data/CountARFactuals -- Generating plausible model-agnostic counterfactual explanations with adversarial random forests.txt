COUNTARFACTUALS – GENERATING PLAUSIBLE
MODEL-AGNOSTIC COUNTERFACTUAL EXPLANATIONS WITH
ADVERSARIAL RANDOM FORESTS
SusanneDandl∗,1,KristinBlesch*,2,3,TimoFreiesleben*,5,GunnarKönig*,6,
JanKapar2,3,BerndBischl1,andMarvinN.Wright3,4,5
1MunichCenterforMachineLearning(MCML)andDepartmentofStatistics,LMUMunich
2LeibnizInstituteforPreventionResearch&Epidemiology–BIPS
3FacultyofMathematicsandComputerScience,UniversityofBremen
4DepartmentofPublicHealth,UniversityofCopenhagen
5Cluster: MachineLearningforScience,UniversityofTübingen
6TübingenAICenterandUniversityofTübingen
wright@leibniz-bips.de
ABSTRACT
Counterfactualexplanationselucidatealgorithmicdecisionsbypointingtoscenariosthatwouldhave
led to an alternative, desired outcome. Giving insight into the model’s behavior, they hint users
towardspossibleactionsandgivegroundsforcontestingdecisions. Asacrucialfactorinachieving
thesegoals,counterfactualsmustbeplausible,i.e.,describingrealisticalternativescenarioswithinthe
datamanifold. Thispaperleveragesarecentlydevelopedgenerativemodelingtechnique–adversarial
randomforests(ARFs)–toefficientlygenerateplausiblecounterfactualsinamodel-agnosticway.
ARFs can serve as a plausibility measure or directly generate counterfactual explanations. Our
ARF-basedapproachsurpassesthelimitationsofexistingmethodsthataimtogenerateplausible
counterfactualexplanations:Itiseasytotrainandcomputationallyhighlyefficient,handlescontinuous
and categorical data naturally, and allows integrating additional desiderata such as sparsity in a
straightforwardmanner.
Keywords counterfactualexplanations·explainableartificialintelligence·interpretablemachinelearning·adversarial
randomforest·tabulardata·plausibility·model-agnostic.
1 Introduction
Machinelearning(ML)algorithmsareincreasinglyusedinhigh-stakesscenarios. Forexample,theyhelptodecide
whetheryoureceivealoan,ifyouaresuitableforajob,orevenwhichdiseaseyouarediagnosedwith. WhileML-based
systemsarepowerfulatdetectingcomplexpatternsindata,thereasoningbehindtheirpredictionsisoftennoteasy
todiscernforhumans. ManyMLmodelsareblackboxeswithacomplexmathematicalstructurethatdonotfollow
transparentlogicalrules[1].
Theemergingfieldofinterpretablemachinelearning(IML)(alsoknownasexplainableartificialintelligenceorXAI
forshort)promisestoopenuptheseblackboxesandaimstomakethedecisionsofMLmodelstransparenttohumans
(see[2,3]foroverviews). Aparticularlysimpleapproachistoexplainalgorithmicdecisionstoend-usersviaso-called
counterfactualexplanations[4].
Example: Imagineyouapplyforaloan. Youentercharacteristicssuchasyourage,salary,loanamount,etc.
intheonlineapplicationformandafterafewsecondsyoureceivethedecision–yourloanapplicationhas
∗Equalcontributionasfirstauthors.
4202
rpA
4
]LM.tats[
1v60530.4042:viXraCountARFactuals
beendenied. Acounterfactualexplanationcouldbe: Ifyoursalaryhadbeene5,000higher,yourloanwould
havebeenapproved.
Moregenerally,acounterfactualexplanationpointstoaclosealternativescenario(theso-calledcounterfactual)that,
incontrasttotheactualscenario,wouldhaveresultedinthedesiredoutcome. Counterfactualexplanationsmaybe
employedforvariouspurposes,suchashelpingtoguideaperson’sactions[5,6],enablingthemtocontestadverse
decisions[7],andprovidinginsightsintothedecisionbehaviorofthemodel[8]. Forallthesegoals,counterfactuals
must be plausible, which means the alternative scenarios they depict are realistic. For instance, in the example
above,suggestinganegativeloanamountorarealestateloanwithanamountofe500wouldnotbeveryplausible
counterfactuals.
Whenaddingplausibilityasanotherobjectiveforgeneratingcounterfactuals,itstrade-offwithproximity,i.e.,that
the counterfactual is close to the point of interest, should be taken into account. Dandl et al. [9] were one of the
firsttoaddressthistrade-offbyframingthecounterfactualsearchasamulti-objectiveoptimizationproblem. Their
approach–multi-objectivecounterfactualexplanations(MOC)–returnsnotjustasinglecounterfactual,butaPareto
setofcounterfactuals,whichisadvisabletoaccountfortheRashomoneffect,i.e.,thatmultiple,diverse,equallygood
counterfactualsmayexist[10].
Anintuitiveapproachtoplausibilityissearchingforonlythosecounterfactualsthatareclosetoactualinstancesin
thedataset[11]. Tooperationalizethisgoal,oneobjectiveinMOCminimizesthedistancebetweencounterfactuals
and the actual instances. However, as presented in Section 3.1, this approach has its limitations if, for example,
there are low-density gaps close to x∗ between high-density regions. Other approaches model plausibility via the
joint probability density. They rely on computationally intensive neural network architectures such as variational
autoencoders(VAEs)[12,13,14,15]orgenerativeadversarialnetworks(GANs)[16,17]. Whilethesearchitectures
have merits for high-dimensional tensor data (e.g., images or text), they are less suited for tabular data (see our
discussioninSection3.2).
Contributions Weleverageatree-basedtechniquefromgenerativemodelingcalledadversarialrandomforests(ARF)
[18]togenerateplausiblecounterfactualsinamixed(i.e.,categoricalandcontinuous)tabulardatasetting. Wecall
thesecountARFactualsandproposetwomodel-agnosticalgorithmstogeneratethem:
1. WeintegrateARFintothemulti-objectivecounterfactualexplanation(MOC)framework[9]tospeedupthe
counterfactualsearchandfindmoreplausiblecounterfactuals(seeSection4.1).
2. We tailor ARF to directly generate plausible counterfactuals without an optimization algorithm (see Sec-
tion4.2).
AsimulationstudyshowstheadvantagesinplausibilityandefficiencyofourARF-basedapproachescomparedto
competingmethods(Section5). Moreover,weapplyourmethodonareal-worlddataset,namelytoexplaincoffee
qualitypredictions(Section6).
2 RelatedWork
Thereiswidespreadagreementinthecounterfactualcommunitythatplausibilityisanimportantconcern[19,11,5,20,
21,22]. Varioussuggestionshavebeenmadetoincorporateplausibilityintothecounterfactualsearch,forexample
usingcausalknowledge[6,14], case-basedreasoning[23], outlierdetectors[24], restrictingthesearchspace[25],
imputingfeaturecombinationsfromrealinstances[26],respectingpathsbetweendatapoints[27],or,asdescribed
above,stayingclosetothetrainingdata[9].
Manydefineplausibilitytheoreticallythroughthejointprobabilitydensity[22]. SomeworksrelyonVAEsorstandard
autoencoders: theydirectlygeneratecounterfactuals[14,15],useVAEsintheoptimization[13]orjustformeasuring
plausibility[12]. OtherworksrelyonGANstogeneratecounterfactuals[17,16]. However,theseapproachesdiffer
substantiallyfromourwork,astheyaretailoredforneuralnetworkmodels[14],focusonlyonplausibilitythereby
ignoringotherobjectiveslikesparsity[14,15](seeSection3.1),orworkonlyforcontinuousdata[13,16]. Theclosest
works to ours are Brughmans et al. [12] and Dandl et al. [9]. Both are designed to generate plausible and sparse
counterfactualsinmixedtabulardatasettings. Brughmansetal. [12]usetheautoencoderreconstructionlossasa
plausibilitymeasureandDandletal. [9]usethedistancetothek−nearestneighborstoevaluateplausibility. Weshow
inourexperimentsinSection5thatutilizingARFtogeneratecounterfactualsimprovesplausibilitycomparedtothose
approacheswhilebeingcomputationallyfast.
2CountARFactuals
3 Background
Before we present our approaches, we provide background on the two methods we build upon: multi-objective
counterfactualexplanations(MOC)[9]andadversarialrandomforests(ARF)[18].
Weconsiderasupervisedlearningsetupwithabinaryclassificationorregressionproblem.2 X denotesap-dimensional
feature space. The respective vector X := (X ,...,X )T of random variables may contain both continuous and
1 p
categoricalfeatures. WithY ∈R,wedenotearandomvariablereflectingtheoutcome. Incaseofabinaryclassification
model,werestrictY to{0,1}.
TopredictY fromX,wetrainedanMLmodelfˆ:X →RonadatasetD
train
:={(x(1),y(1)),...,(x(ntrain),y(ntrain))}
withn observations.Forbinaryclassification,themodeloutputisrestrictedtofˆ(x)∈[0,1],reflectingtheprobability
train
forY = 1. Mostcounterfactualexplanationmethodsrequireaccesstoadatasetforgeneratingcounterfactuals. To
reflectthatthisdatasetcandiffertoD ,wedenoteitasDinthefollowingandassumeittoconsistofnobservations.
train
3.1 Multi-objectivecounterfactualexplanations
Supposewewanttoexplainwhyacertaindatapointofinterestx∗waspredictedasfˆ(x∗)insteadofadesiredprediction
withinY ⊂R. Wachteretal. [4]definecounterfactualsastheclosestpossibleinputvectorxcf tox∗accordingto
des
somedistanceonX suchthatfˆ(xcf)∈Y . Thisdefinitiondoesnotexplicitlydemandsparseorplausiblechanges.
des
Whenintegratingallthesedesiderataintoanobjectivetogeneratecounterfactuals,trade-offsbetweenthedifferent
objectivesmustbetakenintoaccountsincetheobjectivesconflicteachother. Figure1aillustratesthisfortheproperties
plausibilityandproximitytotheoriginalinstancex∗.Ifallhigh-densityregionsarefarawayfromthedecisionboundary,
enforcingproximityleadstounrealisticcounterfactuals.
(a)Plausibility-proximitytrade-off (b)LimitationofMOC’splausibility
Figure1: (a)Proximityandplausibilitycanbeconflictingobjectives[9];enforcingproximitymayleadtounrealistic
counterfactuals. (b)Tohavehighproximity(i.e.,lowo inEquation(2))andhighplausibility(i.e.,lowo in
prox plaus
Equation(3),withk =1),thecounterfactualmaybeinalow-densityregion.
Toconsiderthesetrade-offs,Dandletal. [9]turnedthesearchforcounterfactualsintoamulti-objectiveoptimization
problem:
(cid:16) (cid:17)
xcf ∈argmin o (fˆ(x),Y ),o (x,x∗),o (x,D),o (x,x∗) . (1)
valid des prox plaus sparse
x∈X
Thedifferentobjectivesdenote:
1. Validity: CounterfactualsshouldhaveapredictedoutcomeinY
des
o (fˆ(x),Y ):= inf |fˆ(x)−y|.
valid des
y∈Ydes
2. Proximity: Counterfactualsshouldbeclosetox∗accordingtotheGowerdistanced [28]
Gower
o (x,x∗):=d (x,x∗). (2)
prox Gower
2Ourframeworkalsogeneralizestomulti-classproblems;werestrictourselveshereonlyforthesakeofsimplicityandnotation.
3CountARFactuals
3. Plausibility: Counterfactuals should describe a realistic data instance, with x[1],...,x[k] indicating the
k−nearestneighborstoxwithindataDandw
denotingweightswith(cid:80)k
w =1
i i=1 i
k
(cid:88)
o (x,D):= w d (x,x[i]). (3)
plaus i Gower
i=1
4. Sparsity: Counterfactualsshouldvaryfromx∗inonlyafewfeatures
p
1(cid:88)
o (x,x∗):=∥x−x∗∥ = 1 .
sparse 0 p xj̸=x∗ j
j=1
Dandletal. [9]adaptedthenondominatedsortinggeneticalgorithmorshortNSGA-IIofDebetal. [29]tosolvethe
multi-objectiveoptimizationproblem. Thisalgorithmfollowsthreesteps:
1. Itgeneratesasetofcandidateinstancesclosetothepointofinterestx∗. Amongthese,itrecombinesand
mutatesthecandidatesthatperformbestaccordingtotheabovecriteria. Perdefault,themutatordoesnot
takefeaturedependenciesintoaccount. Toenhanceplausibility,mutationcanbeoptionallyperformedby
samplingfromconditionaldistributionslearnedonDbyconditionaltrees[30]–werefertothisMOCversion
asMOCCTREE.
2. Bothnewandoldcandidatesarerankedusingnondominatedandcrowdingdistancesorting. Nondominated
sortingranksaccordingtooptimalitywithrespecttotheaboveobjectives(withtheoptiontopenalizeinvalid
counterfactuals)andcrowdingdistanceranksaccordingtodiversity.
3. Basedontheserankings,optimalanddiversecandidatesareselectedforthenextiteration. Thesearchfor
counterfactualsendsaftereitherafixednumberofpredefinediterationsorwhenthegeneratedcounterfactuals
arenotsignificantlybetteraccordingtothehypervolumeoftheobjectivesabove. Asafinalstep,thealgorithm
outputstheParetooptimalsetofcounterfactualsoverthegenerations.
TheconceptualizationofplausibilityasEquation(3)hasitslimitationsas,e.g.,illustratedinFigure1b: Withk =1
(thedefaultinMOC),counterfactualswithlowvaluesinEquation(3)mightstillendupinlow-densityregions.
3.2 Generativemodelingandadversarialrandomforests
Generative modeling is concerned with models that generate synthetic data D˜ that mimic the appearance of real
dataD. Awell-knownapproachareVAEs[31],whichencodeoriginaldatainstancesintoasetoflow-dimensional
distribution parameters and then reconstruct these instances with a decoder neural network from samples of these
distributions. AnothercommontechniqueareGANs[32],wheretwodifferentneuralnetworkmodelsplayazero-sum
game–thegeneratornetworkaimstogeneraterealisticinstances,andthediscriminatornetworkaimstodiscriminate
theseinstancesfromrealdata. Othergenerativemodelsbasedonneuralnetworksincludenormalizingflows[33],
diffusionprobabilisticmodels[34]andtransformer-basedmodels[35](see[36,37]foroverviews). Whilethereexist
adaptionsofneuralnetworkmodelstotabulardata,tree-basedapproachesmaybebettersuited[38,39,40].
ARFsareatree-basedprocedureforgenerativemodeling[18]. TheARFapproachissimilartotheapproachofGANs,
however,insteadofaneuralnetworkasabaselearner,ARFreliesonrandomforests. AnARFistrainedinthreesteps:
(1)Fittinganunsupervisedrandomforest[41],whichgeneratesanaivesyntheticdatasetD˜ andsubsequentlytrainsa
1
randomforestgˆ todistinguishbetweenDandD˜ . (2)Samplingfeaturevaluesmarginallyfromtheinstancesinthe
1 1
leavesofgˆ toobtainamorerealisticsyntheticdatasetD˜ . Anotherrandomforestgˆ istrainedtodistinguishbetween
1 2 2
D andD˜ . (3)Thisprocessisrepeateduntiltherandomforestclassifiercannolongerdistinguishsyntheticfrom
2
realdata. WedenotethefinalARFmodelasgˆ∗. AsopposedtoGANs,ARFsallowforbothdensityestimationand
generativemodeling. Thetwoalgorithmsarecalledforestsfordensityestimation(FORDE)andforestsforgenerative
modeling(FORGE),respectively.
DensityestimationwithFORDE leveragesthemutualindependenceacrossfeaturesintheleavesafteralgorithm
convergence,whichallowstomodelthejointdensityp(x)asamixtureofunivariatefeaturedensities:
p
(cid:88) (cid:89)
FORDE(x):=pˆ(x)= π pˆ (x ), (4)
l l,j j
l:x∈Xl j=1
whereX isthehyperrectangledefinedbythel-thleaf,thecorrespondingmixtureweightsπ arecalculatedastheshare
l l
ofrealdatapointsthatfallintoleaflnormalizedoveralltrees,andpˆ are(locally)estimatedunivariatedensity/mass
l,j
4CountARFactuals
functionsforthej-thfeatureinleafl. TheconvergenceofFORDEtotherealdatadistributionofXforinfinitedatais
provenundersomemildconditionsinWatsonetal. [18]. AconditionaldensityunderasetofconditionsC,e.g.,fixed
valuesorintervalsforcertainfeaturesC ⊆{1,...,p},canbederivedfromEquation(4)inthefollowingway:
p
(cid:88) (cid:89)
FORDE(x|C):=pˆ(x|C)= π′ pˆ (x |C ), (5)
l l,j j j
l:x∈Xl j=1
whereC ⊆C denotesthesubsetofconditionsconcerningfeaturej ∈C,andthemixtureweightsπ′ areupdatedto
j l
reflecthowlikelytheircorrespondingleavesfulfillthecondition. Moreformally,themixtureweightsareupdatedand
normalizedusingtheunivariatemarginalsby
π
(cid:81)p
pˆ (C )
π′ := l j=1 l,j j
l (cid:80) π (cid:81)p pˆ (C )
m:x∈Xm m j=1 m,j j
ifthedenominatordoesnotequal0andbyπ′ :=0otherwise. Notethatinthecaseofconditioningonafixedvalueor
l
intervalforacontinuousfeaturej,theunivariatedensitiespˆ collapsetotheindicatorfunction1 ortheunconditional
l,j Cj
densitiestruncatedontheconditioninginterval,respectively.
GenerativemodelingwithFORGE isbasedondrawingaleaflfromtheforestaccordingtothemixtureweightsin
FORDEandsamplingfeaturevaluesfromtheestimatedunivariate(conditional)densitiespˆ . Thereby,FORGEallows
l,j
todrawsamplesthatadheretoFORDEasanapproximationtotherealdistributionofXorX|C.
4 Methods
OurproposalistoleverageARFfortheefficientgenerationofcounterfactualexplanations,i.e.,countARFactuals,in
mixedtabulardatasettings. Morespecifically,weuseandmodifyARFtoaccountforthedesideratathatwediscussed
inSection3.1:
1. Validity: WetrainARFonDbutreplacethetargetY withthepredictionsYˆ. Here,Yˆ istreatedjustasany
otherfeatureinthedata. SinceFORGEallowsforconditionalsampling,wecansamplefromXconditioned
onourdesiredoutcomesYˆ ∈ Y . Note,however,thatARFmaynotlearnaperfectrepresentationofthe
des
predictionfunctionYˆ :=fˆ(X). ItthereforeisnotguaranteedthatARF-samplesarevalid,itonlybecomes
morelikely. Inouralgorithms,weonlyreturnthosecandidateswithpredictionsinY .
des
2. Proximity: WerestricttheoutputofourtwomethodstothosecounterfactualsintheParetoset,definedover
thefourobjectivesofSection3.1,includingproximity(Equation(2)). Inthefirstalgorithmdescribedbelow,
weadditionallyuseARFcombinedwithMOC,whichaccountsforproximity,asdescribedinSection3.1.
3. Plausibility: ARFallowsustobothevaluatetheplausibilityofdatapointsusingFORDE(whichisalsoused
todeterminethereturnedParetoset)andefficientlygenerateplausibledatawithFORGE.
4. Sparsity: FORGEallowstosamplefeaturevaluesX basedontheobservationX =x . Byfixingcertain
S C C
featuresC tothevalueofx∗,weonlychangefeaturevaluesinthesparsesetS :={1,...,p}\C.
C
Withthedesideratainplace,severaldecisionsneedtobemade: Shouldweintegrateplausibilityviadensityestimation
(FORDE)orgenerativemodeling(FORGE)?Whatisanoptimaltrade-offbetweenproximityandotherobjectives,such
asplausibilityandsparsity? HowshouldwesearchfortheconditioningsetC forfeaturesthatshouldnotbechanged?
Inthefollowing,weprovidetwoalgorithmsthatdecideonthesequestionsindifferentways. ThefirstintegratesARF
intoMOC(Section4.1). ThesecondusesARFasastandalonecounterfactualgenerator(Section4.2).
4.1 Algorithm1: IntegratingARFintoMOC
InMOC’soptimizationproblem(Equation(1)),wesubstitutetheplausibilitymeasure(Equation(3))bythedensity
estimatorofFORDE(Equation(4)). SincetheindividualobjectivesinMOCmustmaptoazero-oneinterval(withlow
valuesdenotingdesiredproperties),wetransformpˆ(x),asestimatedbyFORDE,withthenegativeexponentialfunction
o∗ (x):=e−pˆ(x). (6)
plaus
WeuseFORGEasdescribedabovetosampleplausiblecandidatesinMOCinthemutationstepoftheNSGA-II.This
isastrategytoefficientlylimitthesearchspaceofMOCtoplausiblecounterfactuals. Concerningsparsity,wefind
theconditioningsetC throughiteratedmutationandrecombination,justlikeinMOC,andweselectcandidatesusing
NSGA-IIaccordingtooptimalityanddiversity. Atlast,theoutputcomprisesonlythevalidPareto-setofcounterfactuals
overthegenerations,i.e.,counterfactualsthathaveapredictioninY andarenotdominatedbyothercandidatesthat
des
weregenerated. Fordetails,werefertothepseudocodeinAppendixA.
5CountARFactuals
4.2 Algorithm2: ARFisallyouneed
For this algorithm, we leverage the ability of our modified ARF sampler to directly and efficiently generate many
relevantcounterfactuals. Asdescribedabove,themodifiedFORGEmethodallowstogenerateplausibledatapoints. To
enforcesparsity,wesamplemfeatureswithprobabilitiesaccordingtotheirlocalfeatureimportance,calculatedasthe
standarddeviationoftheindividualconditionalexpectation(ICE)curve[9,42]. Themselectedfeaturesdescribethe
featuresS weaimtochangebecausethey,accordingtothelocalfeatureimportance,impactthepredictionthemost.
TheremainingfeaturesthenformtheconditioningsetC ={1,...,p}\S.
AsforAlgorithm1,weoutputonlythevalidandPareto-optimalsetofcounterfactuals. Thepseudocodeforthismethod
isgiveninAppendixB.
5 Experiments
Weevaluatethequalityofourproposedmethodswithrespecttothefollowingresearchquestions:
RQ(1) DoourproposedARF-basedmethodsgeneratemoreplausiblecounterfactualscomparedtocompeting
methodswithoutmajorsacrificesinsparsity(o ),proximity(o )andtheruntime?
sparse prox
RQ(2) Doeso∗ (Equation(6))betterreflectthetrueplausibilitycomparedtoo (Equation(3))?
plaus plaus
Toobjectivelyevaluatetheplausibilityofthegeneratedcounterfactuals,werequireaccesstotheground-truthlikelihood.
Becauseground-truthlikelihoodsareusuallyunavailableforreal-worlddata,weevaluateourmethodsonsyntheticdata.
Anillustrativereal-worldapplicationfollowsinSection6.
5.1 Data-generatingprocess
Fortheexperiments,weconstructedthreeillustrativetwo-dimensionaldatasets,namelycassini(inspiredby[43]),
two sines(inspiredbythetwomoonsdataset),andthree blobs(inspiredby[15]). Moreover,wegeneratedfour
datasetsfromrandomlysampledBayesiannetworksofdimensionality5,10,and20,namelybn_5,bn_10,andbn_20,
whichallincludebothcontinuousandcategoricalfeaturesaswellasnonlinearrelationships. AnXGBoostmodelwas
fittedonsampleddatasetsD ofsize5000[44]. Foreachdata-generatingprocess(DGP),tenadditionalpointswere
train
sampledasinstancesofinterestx∗. Thecounterfactualgenerationmethodsreceivedaccesstonewlysampleddatasets
D ofsize5000. DetailsonthedatasetgenerationandmodelfitcanbefoundinAppendixCandintherepository
accompanyingthispaper.3
5.2 Competingmethods
WecompareourproposedMOCversionbasedonARFofSection4.1(referredtoasMOCARF)andthestandalone
ARFgeneratorofSection4.2(referredtoasARF)tothefollowingcompetitors: MOCandMOCCTREE(MOCwitha
conditionalsampler,seeSection3.1)[9]andNICE[12]withaplausibilityrewardfunction(seeEquation(4)in[12]).
NICEgeneratescounterfactualsbyiterativelyreplacingonefeatureaftertheotherinx∗bythevaluesofxnn,which
denotesanearestneighborofx∗ inDwithfˆ(xnn)∈Y . Ineachiteration,thealgorithmkeepsthefeaturechange
des
withthehighestplausibilityreward.
To allow for a fair comparison, all methods generate a set of counterfactual candidates. For NICE, we apply the
extensionofDandletal. [45];insteadofstoppingthesearchoncethepointwiththehighestrewardhasapredictionin
Y ,thesearchcontinuesuntilxnnisrecoveredandallintermediateinstanceswithpredictionsinY arereturned.
des des
Ifpossible,weselectedthehyperparametersforthemethodssuchthateachmethodgeneratedanequalnumberof
candidates–namely,1000.4 ARFrequiresamaximumsetsizeforS,reflectinghowmanyfeaturesaremaximally
√
allowedtobechanged. Wesetitaccordingtothenumberoffeaturespasm :=min(⌈ p+3⌉,p). Sincealsofor
max
allMOC-basedmethodsthemaximumnumbercanbespecified,weusedthesamem forMOC,MOCARFand
max
MOCCTREE.Fortheevaluation,wefocusedonlyontheuniquecounterfactualsthathavepredictionsinY . We
des
furtherreducedthissettotheParetoset,i.e.,thesetofcounterfactualsthatarenondominatedaccordingtoproximity
(o ),sparsity(o )andplausibility. Thedefinitionoftheplausibilityobjectivedifferedbetweenthemethods,with
prox sparse
o⋆ forARFandMOCARF,oplausforMOCandMOCCTREE,andtheautoencoderreconstructionerrorforNICE(as
plaus
proposedby[12]).
3https://github.com/bips-hb/countARFactuals.
4SpecifyingtheexactnumberwaspossibleforallmethodsbesidesNICE[45].
6CountARFactuals
5.3 Evaluationcriteria
ToanswerRQ(1),weevaluatedthegeneratedcounterfactualswithrespecttotheground-truthlikelihood(denotedas
plausibility,inthefollowing),validityo ,proximityo andsparsityo (seeSection3.1). Weaggregatedthe
valid prox sparse
resultspermethod,datasetandinstanceofinterestx⋆bycomputing(scaled)dominatedhypervolumes[46]. Wealso
measuredthenumberofnondominatedcounterfactualsandtheruntime. Toinvestigatethetrade-offbetweenplausibility
andproximity,wealsocomputedmedianattainmentsurfacesaccordingtoLópez-Ibáñezetal. [47]foreachmethod
anddataset. Itrevealshowthetwoobjectivesaredistributedonaverageoverthedifferentx∗. ToanswerRQ(2),all
generatedcounterfactualswereevaluatedwithrespecttoo⋆ ando . Permethod,datasetandx⋆,wecomputed
plaus plaus
Spearman-rankcorrelationsbetweenthetrueplausibilityando⋆ andbetweenthetrueplausibilityando . Witha
plaus plaus
Wilcoxonsignedranktest,wetestedwhethero∗ hashighercorrelationstothetrueplausibilitythano .
plaus plaus
5.4 Results
Figure2presentstheresultsforRQ(1)andshowstheobjectivevaluespercounterfactualsaswellasthehypervolume,
numberofcounterfactualsandruntime. Onaverage,ARFandMOCARFgeneratedmoreplausiblecounterfactuals
comparedtotheotherMOC-basedapproachesandNICE.Inalignmentwithpreviousliterature[9,48],ourresults
suggestthathigherplausibilitymightbeassociatedwithlowerproximityandsparsity. Forfurtherinvestigationsonthe
trade-offs,Figure4andFigure5inAppendixDdetailthemedianattainmentsurfacesperdatasetandmethod. The
plotsrevealthatARFandMOCARFonaveragedominatetheothermethodsinproximity,sparsityandplausibility,with
thedifferencesbeinggreatestinplausibility. Thehypervolumewasonaveragesimilarforthedifferentmethodsfor
low-dimensionaldatasets(ARFhadlowerhypervolumesincassiniduetoitsinferiorityinproximityandsparsity),
forhigher-dimensionaldatasets(bn_10andbn_20),ARFandMOCARFperformedbetterthanthecompetingmethods.
Concerningruntime,ARFgeneratedcounterfactualsthefastestonaverage,followedbyNICEandMOC.MOCARF
wasfasterthanMOCCTREEfordatasetswithmorethantwofeatures. Theruntimedifferencesincreasedwithhigher
dimensionaldata. Onaverage,ARFandMOCARFgeneratedthelargestsetofnondominatedcounterfactualscompared
totheothermethods.
Considering RQ (2), the Wilcoxon rank sum test had a p-value close to 0 (7.16e−06), i.e., the correlation of our
proposedplausibilitymeasureo∗ tothetrueplausibilitywassignificantlyhigherthanthatofo . Themedian
plaus plaus
correlationtothetrueplausibilityoverallmethodsanddatasetswas0.84foro∗ and0.69foro .
plaus plaus
Overall,ourstudyshowsthatonaverageourproposedmethods–ARFandMOCARF–generateamoreplausible
setofcounterfactualscomparedtoourcompetitorswithoutmajorsacrificesinsparsityandproximity. Notably,ARF
achievesthiswithsuperiorityinruntimes.
6 RealDataExample
We illustrate our approach on the publicly available coffee quality dataset5. The data details the characteristics of
severalArabicacoffeebeans,suchasthecountryoforiginandaltitudeatwhichthebeanswerecultivated. Further,the
datasetincludesinformationonaqualityreviewscore(cuppoints)specifiedbyanexpertjurywithintheCoffeeQuality
Institute[49].
Inthisexample,weusearandomforesttopredictcoffeequalityfromselected,actionablecharacteristicsofthecoffee
beans. Forsimplicity,webinarizethetargetscorecuppoints. Aimingforbalancedclassesofgoodandbadquality,we
usethedataset’smedianvalueofcuppointsasacut-offpoint,i.e.,
(cid:26)
good ifcuppoints≥median(cuppoints)
quality= (7)
bad otherwise.
Forillustration,wegeneratecounterfactualexplanationsforaninstanceofbadcoffeequality,answeringthequestion:
Whichcharacteristicswouldneedtobechangedtorateasgoodqualitycoffee?
Thisexampleillustratestheimportanceoftakingintoaccountthemultipleobjectivesofcounterfactualexplanations,
suchassparsityandplausibility. Forexample,acompanythataimstoimprovethequalityoftheircoffeemaywantto
makeassparsechangestothecoffeecharacteristicsaspossibleforeconomicreasons. Similarly,somechangesmight
notbeplausible,thinkofchangingthecountryoforiginindependentlyofthealtitudeofthecoffeeplantationsorthe
varietyofbeanscultivatedintherespectivecountry(sincethevarietymustsuitthenaturalconditionsintherespective
country).
5https://github.com/jldbc/coffee-quality-database
7CountARFactuals
Figure2:Boxplotsoftheplausibility,proximity(1−o ),sparsity(1−o ),hypervolume,numberofcounterfactuals
prox sparse
andruntimeforeachmethodanddataset. Highervaluesarebetter,exceptforruntime.
ThegenerationofcounterfactualexplanationsinthisexampleisperformedusingAlg. 2detailedinSection4.2. In
Figure3,wepresentasetofthegeneratedcountARFactualsexplanationsforaninstanceofcoffeebeansbelonging
tothebadclassthatoriginatefromTaiwan. Figure3illustratesthatcountARFactualsyieldplausiblecounterfactual
explanations. Forinstance,forcountARFactual#3,boththecountryoforiginischangedfromTaiwantoColombiaand
thevarietyfromTypicatoCaturra. ThisseemsreasonablebecauseTypicawasgrowninonlyfewColombianinstances
inthetrainingdataset,andinstead,CaturrawasthemostfrequentlygrownvarietyinColombia. Further,thealtitudeat
whichthebeansaregrowniselevatedonlyalittlewithinTaiwanesecountARFactuals(#4-12),butmoredrasticallyfor
countriesthat–giventhedata–growcoffeeonhigheraltitudesonaverage,suchasMexico(#1)andColombia(#3).
7 Discussion
Inthispaperweshowthatadversarialrandomforests(ARF)canbemodifiedtogenerateplausiblecounterfactuals,both
asasubroutinetomulti-objectivecounterfactualexplanations(MOC)andasastandaloneapproach. Ourexperimentsin
Section5demonstratethatARFcanimprovetheplausibilityofcounterfactualsandtheefficiencyintheirgeneration
withoutsubstantiallysacrificingotherdesideratasuchasproximityandsparsity. Incontrasttoothergenerativemodeling
approachesforplausiblecounterfactuals,ARFhandlesmixedtabulardatadirectlywithout,e.g.,one-hot-encoding
categoricalfeatures,therebyimprovingdata-efficiency. Moreover,ARF-basedcounterfactualgenerationallowsfor
sparsity via conditional sampling and is an off-the-shelf methodology that requires minimal efforts in tuning and
computationalresources.
8CountARFactuals
country of origin harvest year variety processing method moisture altitude mean meters quality
point of interest x* Taiwan 2014 Typica Washed / Wet 0.10 800.00 bad
countARFactual #1 Mexico 2013 Typica Washed / Wet 0.11 1498.68 good
countARFactual #2 Taiwan 2014 Typica Washed / Wet 0.07 1172.96 good
countARFactual #3 Colombia 2013 Caturra Washed / Wet 0.01 1735.77 good
countARFactual #4 Taiwan 2013 Typica Washed / Wet 0.11 1268.73 good
countARFactual #5 Taiwan 2014 Typica Washed / Wet 0.11 1013.48 good
Taiwan 2014 Typica Washed / Wet 0.11 1300.71 good
countARFactual #11 Taiwan 2014 Typica Washed / Wet 0.10 952.60 good
countARFactual #12 Taiwan 2014 Typica Washed / Wet 0.10 965.36 good
Figure3: ExemplarycountARFactualsforaninstanceofbadcoffeequality. Arrowsindicatechangesincomparisonto
x∗,i.e.,afeature’svalueincrease↑,decrease↓orchangeincategory↔.
Our work faces some limitations. For example, we define the plausibility of counterfactuals via the joint density.
However,ashighlightedbyKeaneetal. [20],therearedifferentconceptualizationsofplausibility,forexample,based
onthefeasibilityofactionsoruserperceivedplausibility[5,6,27]. Onemightevenquestionifstayinginthemanifold
isalwaysdesirable,e.g.,ifchangingtheclassrequiresextrapolationsoshouldourcounterfactuals. Itshouldbenoted
thatplausiblecounterfactuals,ingeneral,cannotbeinterpretedasactionrecommendations. Althoughtheyprovide
hintsaboutwhichalternativefeaturevalueswouldyieldacceptancebythepredictor,theydonotguidetheuseron
whichinterventionsyieldthedesiredchangeintherealworld. Toguideaction,causalknowledgeisrequired[50].
Furthermore,inthecontextofrecourse,improvementoftheunderlyingtargetismoredesirablethanacceptancebya
specificpredictor,whichcounterfactualexplanationsdonottarget[6].
Proximityandplausibilityareconflictingobjectives[12,9]. Oftentimes,thereisonlylittledataclosetothedecision
boundary,andjumpingjustovertheboundarycanleadtoimplausiblecounterfactuals[19]. Atrade-offbetweenthetwo
objectivesisdesirable,whichweimplicitlyaddressbygeneratingaPareto-optimalsetofdiversecounterfactuals. In
futurework,onecouldalreadyincorporatesuchtrade-offsinthecounterfactualgeneration,e.g.,byaparameterthat
directlycontrolsfortheproximity-plausibilitytrade-off. Oneoptionwouldbetosetathresholdforplausibilityinstead
ofatrade-offparameter,assuggestedbyBrughmansetal. [12].
Likeallworksoncounterfactualexplanations,wefacetheRashomoneffect: Thereexistmanyplausiblecounterfactuals
thatexplainthesamedatapoint. Thisraisesthequestionofwhichoneweshouldshowtotheuser[20,22]? Asabottom
line, we returnonly thePareto-optimal setof counterfactuals, which atleast guaranteesthat nostrictly dominated
optionisshown. Infuturework,integratinguserpreferencesorconsideringadditionalobjectivesmayimprovethefinal
selection.
Ourframeworkistailoredformixedtabulardatasettings.Forotherdatamodalitieslikeimageortextdata,weadvisefor
usingneuralnetworkbasedapproachesfordensityestimationandgenerativemodelingsuchasVAEsandGANs.Finally,
ourframeworkisdesignedforbinaryclassificationandregressionbutcanbeextendedtomulti-classclassification.
Infuturework,weplantoinvestigatetheroleoftheMLmodelintheARFapproachtocounterfactuals. Wecould
alsogeneratecounterfactualswithARFwithoutthemodelbydirectlytrainingARFonY ratherthanthepredictions
Yˆ. Wewouldthengetplausiblecounterfactualsthathinttowardsimprovementinsteadofacceptance[6]. Whilesuch
counterfactualsappeardifferentfromthosediscussedintheXAIliteraturesofar,infact,theyessentiallyjustturnthe
generativemodelthatconditionsonX=xintoapredictionalgorithm.
Acknowledgments
MNWandKBweresupportedbytheGermanResearchFoundation(DFG),GrantNumber437611051. MNWwas
supportedbytheGermanResearchFoundation(DFG),GrantNumber459360854. KBwassupportedbyaPhDgrant
oftheMinds,Media,MachinesIntegratedGraduateSchoolBremen. MNWandJKweresupportedbytheUBremen
Research Alliance/AI Center for Health Care, financially supported by the Federal State of Bremen. GK and TF
weresupportedbytheGermanResearchFoundationthroughtheClusterofExcellence“MachineLearning-New
9
1
… … … …CountARFactuals
PerspectivesforScience"(EXC2064/1number390727645). TFhasbeensupportedbytheCarlZeissFoundation
throughtheproject“CertificationandFoundationsofSafeMachineLearningSystemsinHealthcare”.
References
[1] JennaBurrell. Howthemachine‘thinks’: Understandingopacityinmachinelearningalgorithms. BigData&
Society,3(1):2053951715622512,2016.
[2] AminaAdadiandMohammedBerrada.Peekinginsidetheblack-box:asurveyonexplainableartificialintelligence
(XAI). IEEEaccess,6:52138–52160,2018.
[3] ChristophMolnar. InterpretableMachineLearning. 2edition,2022.
[4] SandraWachter,BrentMittelstadt,andChrisRussell. Counterfactualexplanationswithoutopeningtheblackbox:
AutomateddecisionsandtheGDPR. Harv.JL&Tech.,31:841,2017.
[5] Amir-HosseinKarimi,GillesBarthe,BernhardSchölkopf,andIsabelValera. Asurveyofalgorithmicrecourse:
Contrastiveexplanationsandconsequentialrecommendations. ACMComputingSurveys,55(5):1–29,2022.
[6] GunnarKönig,TimoFreiesleben,andMoritzGrosse-Wentrup. Improvement-focusedcausalrecourse(ICR). In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume37,pages11847–11855,2023.
[7] HenriettaLyons,EduardoVelloso,andTimMiller. Conceptualisingcontestability: Perspectivesoncontesting
algorithmicdecisions. ProceedingsoftheACMonHuman-ComputerInteraction,5(CSCW1):1–25,2021.
[8] Brent Mittelstadt, Chris Russell, and Sandra Wachter. Explaining explanations in ai. In Proceedings of the
conferenceonfairness,accountability,andtransparency,pages279–288,2019.
[9] SusanneDandl,ChristophMolnar,MartinBinder,andBerndBischl. Multi-objectivecounterfactualexplanations.
InInternationalConferenceonParallelProblemSolvingfromNature,pages448–469.Springer,2020.
[10] LeoBreiman. Statisticalmodeling: Thetwocultures(withcommentsandarejoinderbytheauthor). Statistical
Science,16(3):199–231,2001.
[11] RiccardoGuidotti. Counterfactualexplanationsandhowtofindthem: Literaturereviewandbenchmarking. Data
MiningandKnowledgeDiscovery,pages1–55,2022.
[12] DieterBrughmans,PieterLeyman,andDavidMartens. NICE:Analgorithmfornearestinstancecounterfactual
explanations. DataMiningandKnowledgeDiscovery,pages1–39,2023.
[13] Shalmali Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. Towards real-
istic individual recourse and actionable explanations in black-box decision making systems. arXiv preprint
arXiv:1907.09615,2019.
[14] DivyatMahajan,ChenhaoTan,andAmitSharma. Preservingcausalconstraintsincounterfactualexplanationsfor
machinelearningclassifiers. arXivpreprintarXiv:1912.03277,2019.
[15] MartinPawelczyk,KlausBroelemann,andGjergjiKasneci. Learningmodel-agnosticcounterfactualexplanations
fortabulardata. InProceedingsofthewebconference2020,pages3126–3132,2020.
[16] DanielNemirovsky,NicolasThiebaut,YeXu,andAbhishekGupta. Countergan: Generatingcounterfactuals
forreal-timerecourseandinterpretabilityusingresidualgans. InUncertaintyinArtificialIntelligence,pages
1488–1497.PMLR,2022.
[17] Arnaud Van Looveren, Janis Klaise, Giovanni Vacanti, and Oliver Cobb. Conditional generative models for
counterfactualexplanations. arXivpreprintarXiv:2101.10123,2021.
[18] David S Watson, Kristin Blesch, Jan Kapar, and Marvin N Wright. Adversarial random forests for density
estimationandgenerativemodeling. InProceedingsofthe26thInternationalConferenceonArtificialIntelligence
andStatistics,pages5357–5375.PMLR,2023.
[19] TimoFreiesleben. Theintriguingrelationbetweencounterfactualexplanationsandadversarialexamples. Minds
andMachines,32(1):77–109,2022.
[20] MarkTKeane,EoinMKenny,EoinDelaney,andBarrySmyth. Ifonlywehadbettercounterfactualexplanations:
FivekeydeficitstorectifyintheevaluationofcounterfactualXAItechniques. arXivpreprintarXiv:2103.01035,
2021.
[21] IliaStepin,JoseMAlonso,AlejandroCatala,andMartínPereira-Fariña.Asurveyofcontrastiveandcounterfactual
explanationgenerationmethodsforexplainableartificialintelligence. IEEEAccess,9:11974–12001,2021.
10CountARFactuals
[22] SahilVerma,VarichBoonsanong,MinhHoang,KeeganEHines,JohnPDickerson,andChiragShah. Counterfac-
tualexplanationsandalgorithmicrecoursesformachinelearning: Areview. arXivpreprintarXiv:2010.10596,
2020.
[23] Mark T Keane and Barry Smyth. Good counterfactuals and where to find them: A case-based technique for
generatingcounterfactualsforexplainableai(XAI). InCase-BasedReasoningResearchandDevelopment: 28th
InternationalConference,ICCBR2020,Salamanca,Spain,June8–12,2020,Proceedings28,pages163–178.
Springer,2020.
[24] KentaroKanamori,TakuyaTakagi,KenKobayashi,andHirokiArimura. Dace: Distribution-awarecounterfactual
explanationbymixed-integerlinearoptimization. InIJCAI,pages2855–2862,2020.
[25] AndréArteltandBarbaraHammer.Convexdensityconstraintsforcomputingplausiblecounterfactualexplanations.
InArtificialNeuralNetworksandMachineLearning–ICANN2020: 29thInternationalConferenceonArtificial
NeuralNetworks,Bratislava,Slovakia,September15–18,2020,Proceedings,PartI29,pages353–365.Springer,
2020.
[26] YashGoyal,ZiyanWu,JanErnst,DhruvBatra,DeviParikh,andStefanLee. Counterfactualvisualexplanations.
InInternationalConferenceonMachineLearning,pages2376–2384.PMLR,2019.
[27] RafaelPoyiadzi,KacperSokol,RaulSantos-Rodriguez,TijlDeBie,andPeterFlach. Face:feasibleandactionable
counterfactual explanations. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pages
344–350,2020.
[28] JohnCGower. Ageneralcoefficientofsimilarityandsomeofitsproperties. Biometrics,27(4):857–871,1971.
[29] KalyanmoyDeb,AmritPratap,SameerAgarwal,andTAMTMeyarivan. Afastandelitistmultiobjectivegenetic
algorithm: NSGA-II. IEEEtransactionsonevolutionarycomputation,6(2):182–197,2002.
[30] TorstenHothornandAchimZeileis. Predictivedistributionmodelingusingtransformationforests. Journalof
ComputationalandGraphicalStatistics,30(4):1181–1196,March2021.
[31] DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. InProceedingsofthe2ndInternational
ConferenceonLearningRepresentations,2014.
[32] IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,AaronCourville,
and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems,
volume27,2014.
[33] DaniloRezendeandShakirMohamed. Variationalinferencewithnormalizingflows. InProceedingsofthe32th
InternationalConferenceonMachineLearning,pages1530–1538.PMLR,2015.
[34] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InAdvancesinNeural
InformationProcessingSystems,volume33,pages6840–6851,2020.
[35] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,and
IlliaPolosukhin. Attentionisallyouneed. InAdvancesinNeuralInformationProcessingSystems,volume30,
2017.
[36] SamBond-Taylor,AdamLeach,YangLong,andChrisGWillcocks. Deepgenerativemodelling: Acomparative
review of VAEs, GANs, normalizing flows, energy-based and autoregressive models. IEEE Transactions on
PatternAnalysisandMachineIntelligence,44(11):7327–7347,2021.
[37] DavidFoster. Generativedeeplearning: Teachingmachinestopaint,write,compose,andplay. O’ReillyMedia,
Inc.,2ndedition,2022.
[38] VadimBorisov,TobiasLeemann,KathrinSeßler,JohannesHaug,MartinPawelczyk,andGjergjiKasneci. Deep
neuralnetworksandtabulardata: Asurvey. IEEETransactionsonNeuralNetworksandLearningSystems,2022.
[39] LéoGrinsztajn,EdouardOyallon,andGaëlVaroquaux. Whydotree-basedmodelsstilloutperformdeeplearning
ontypicaltabulardata? InAdvancesinNeuralInformationProcessingSystems,volume35,pages507–520,2022.
[40] RavidShwartz-ZivandAmitaiArmon. Tabulardata: Deeplearningisnotallyouneed. InformationFusion,
81:84–90,2022.
[41] TaoShiandSteveHorvath. Unsupervisedlearningwithrandomforestpredictors. JournalofComputationaland
GraphicalStatistics,15(1):118–138,2006.
[42] Alex Goldstein, Adam Kapelner, Justin Bleich, and Emil Pitkin. Peeking inside the black box: Visualizing
statisticallearningwithplotsofindividualconditionalexpectation. JournalofComputationalandGraphical
Statistics,24(1):44–65,January2015.
11CountARFactuals
[43] FriedrichLeischandEvgeniaDimitriadou. mlbench: Machinelearningbenchmarkproblems,2021. Rpackage
version2.1-3.1.
[44] TianqiChenandCarlosGuestrin. XGBoost: Ascalabletreeboostingsystem. InProceedingsofthe22ndACM
SIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining,KDD’16,pages785–794,New
York,NY,USA,2016.ACM.
[45] SusanneDandl,AndreasHofheinz,MartinBinder,BerndBischl,andGiuseppeCasalicchio. counterfactuals: An
Rpackageforcounterfactualexplanationmethods,2023.
[46] EckartZitzlerandLotharThiele. Multiobjectiveoptimizationusingevolutionaryalgorithms—Acomparative
casestudy,page292–301. SpringerBerlinHeidelberg,1998.
[47] Manuel López-Ibáñez, Luís Paquete, and Thomas Stützle. Exploratory analysis of stochastic local search
algorithmsinbiobjectiveoptimization,page209–222. SpringerBerlinHeidelberg,2010.
[48] JavierDelSer,AlejandroBarredo-Arrieta,NataliaDíaz-Rodríguez,FranciscoHerrera,AnnaSaranti,andAndreas
Holzinger. Ongeneratingtrustworthycounterfactualexplanations. InformationSciences,655:119898,2024.
[49] CoffeeQualityInsitute. https://www.coffeeinstitute.org/. Lastaccessed: 2024-03-12.
[50] Amir-Hossein Karimi, Bernhard Schölkopf, and Isabel Valera. Algorithmic recourse: From counterfactual
explanations to interventions. In Proceedings of the 2021 ACM conference on fairness, accountability, and
transparency,pages353–362,2021.
12CountARFactuals
A Algorithm1: IntegratingARFintoMOC
ThefollowingpseudocodeisbasedonAlgorithm1in[45]. Bluelineshighlightthestepsthatdifferfromtheoriginal
MOCalgorithmproposedby[9].
Algorithm1MOCwithARF-basedSamplerandEvaluation
Inputs:
Datapointtoexplainpredictionforx⋆ ∈X
Desiredoutcome(range)Y
des
Predictionfunctionfˆ:X →R
ObserveddataD
ARFgˆ∗trainedon(x ,fˆ(x ))n withx ∈D
i i i=1 i
Numberofgenerationsn
generations
Sizeofpopulationµ
Recombinationandmutationmethodsincludingprobabilities
Selectionmethodforfeaturesintheconditioningsetandinitializationmethod
Stoppingcriterion
(Additionaluserinputs,e.g.,rangeofnumericalfeatures,immutablefeatures,distancefunction,see[9])
1: InitializepopulationP with|P |=µ(ICE-curve-based,see[9])
0 0
2: Evaluatecandidatesaccordingtofourobjectives:
• Validity(L )
1
• Sparsity(L )
0
• Proximity(Gowerdistance)
• Plausibility(ARF-basedlikelihoodtransformedwithe−x)
3: Sett=0
4: forr ∈{1,...,n }
iterations
5: C =create_offspring(P ),|C |=µwithgivenprobabilities
t t t
1. Selectbestcandidates(acc. tovalidityobjective)
2. Recombinethesepairwise
3. Mutatevaluesjointlyusinggˆ∗: generatenewdatapointswithFORGE
6: CombineparentsandoffspringR =C ∪P
t t t
7: Assigncandidatestoafrontaccordingtotheirobjectivevalues:
(F ,F ,...,F )= nondominated_sorting(R )
1 2 m t
8: fori=1,...,m
9: Sortcandidatesacc. todiversity(objectiveandfeaturespace):
F˜ =crowding_distance_sort(F )
i i
10: endfor
11: SetP =∅andi=1
t+1
12: while|P |+|F˜|≤µ
t+1 i
13: P =P ∪F˜
t+1 t+1 i
14: i=i+1
15: endwhile
16: Choosefirstµ−|P |elementsofF˜: P =P ∪F˜[1:(µ−|P |)]
t+1 i t+1 t+1 i t+1
17: t=t+1
18: endfor
19: Returnunique,non-dominatedcandidatesof(cid:83)t P \x⋆withfˆ(x )∈Y
k=0 k CF des
13CountARFactuals
B Algorithm2: ARFisallyouneed
Algorithm2ARF-basedCounterfactualGenerator
Inputs:
Datapointtoexplainpredictionforx⋆ ∈X
Desiredoutcome(range)Y
des
Predictionfunctionfˆ:X →R
ObserveddataD
ARFgˆ∗trainedondata(x ,fˆ(x ))n withx ∈D
i i i=1 i
Maximumnumberoffeaturechangesm
max
Numberofiterationsn
iterations
Numberofsamplesgeneratedineachiterationn
synth
(Additionaluserinputs,e.g.,immutablefeatures)
1: Derivelocalimportances(fi )p foreachfeaturej ∈{1,...,p}(ICE-curve-based,see[9])
j j=1
2: forr ∈{1,...,n }
iterations
3: m←sample(1,...,m )
max
4: SelectsetC ⊂{1,...,p}byrandomlysamplingmfeatureswithprobability
proportionaltohowunimportantfeatureis
5: CF ←samplen observationswithFORGEderivedfromgˆ∗under
synth
conditionthat∀j ∈C :X =x⋆&Yˆ ∈Y
j j des
6: X ←(X ,CF)
CF CF
7: endfor
8: Returnunique,nondominatedcandidatesx ∈X withfˆ(x )∈Y
CF CF CF des
14CountARFactuals
C SyntheticData
Asfollows,wedescribethethreeillustrativedatasetsaswellasthesamplingoftherandomlygenerateddata-generating
processes. Thecodethatwasusedtogeneratethedatasetsandpairplotsvisualizingtheirdistributioncanbefoundin
therepositoryaccompanyingthepaper(https://github.com/bips-hb/countARFactuals).6
C.1 Illustrativedatasets
Cassini TheDGP,inspiredby[43],isdefinedasfollows:
Y ∼Y +Y Y with Y ∼Bern(2/3), Y ∼Bern(0.5)
1 1 2 1 2
(cid:26)
N(0,0.2), ifY =0
X |Y ∼ 1
1 1 N(0,0.5), otherwise
(cid:26)
N(0,0.2) ifY =0
X |X ,Y ,Y ∼ 1
2 1 1 2 N((−1)Y2cos(X 1),0.2) otherwise
TwoSines TheDGP,inspiredbythetwomoonsdataset,isspecifiedas:
Y ∼Bern(0.5), X |Y ∼N(Y,3.0), X |Y,X ∼N(sin(X )−2Y +1,0.3)
1 2 1 1
Pawelczyk TheDGP,takenfrom[15],isdefinedbelow,whereI referstothe2x2identitymatrixandCat(1,1,1)
2 3 3 3
totheuniformcategoricaldistributionwithvalues0,1,2.
L∼Cat(1,1,1)
3 3 3
 (−10,5)T ifL=0

X|µ∼N(µ,I ), µ|L= (0,5)T ifL=1
2
(0,0)T otherwise
Y(X):=X >6
2
C.2 RandomlygeneratedDGPs
Forthegenerationofbn_5,bn_10,andbn_20,werandomlysampleBayesiannetworkswithcategoricalandcontinuous
distributionsaswellaslinearandnonlinearrelationships.
1. First,werandomlysampleaDirectedAcyclicGraph(DAG)usingthenetworkxpackage. WeselectY asthe
rootnode. TomakesurethatY isrelatedtomanyofthefeatures,foreachnodethatisnotdirectlyneighboring
Y,adirectededgeisaddedwithprobability0.5(directedsuchthatthegraphremainsacyclic).
2. Fromallnodes,20%arerandomlyselectedtobecategoricalnodes;Y isalwaysselectedtobeacategorical
node.
3. Foreverynodej,anaggregationfunctiong issampledthatmapstheparentvaluesx toanaggregate,
pa(j)
whichthenparameterizesthedistributionoftherespectivenode.
 
(cid:88)
g(x)=β+β 1h(x)+β 2h(x)2 with h(x)=sin w ix i
i∈pa(j)
TheweightswaresampledfromUnif(−1,1)+3Bern(3/d). TomakeitmorelikelythatY canbepredicted
wellfromitscovariates,weightsconcerningY areincreasedbyd ∼ Unif(3,4)withprobability0.1. The
weightvectorwisnormalized. Thecoefficientsβ aresampledfromUnif(−1,1).
4. Ifthefeatureiscategorical,therespectiveBernoulliisparameterizedwiththesigmoidoftheaggregateofthe
parentsBern(ς(g(x))). ContinuousfeaturesfollowN(µ,σ)withµ∼N(g(x),1)andσ ∼N(0,2).
To ensure that a prediction model fitted on the data can discriminate between the classes and that changing the
predictiontothedesirableclassisfeasible,werandomlygenerateddatasetsuntilwefoundonewithbalancedlabels
(0.4 < E[Y] < 0.6),andwhereaxgboostmodeldemonstratedgoodaccuracy(> 0.95)andbalancedpredictions
(0.3<E[Yˆ]<0.7).
6Foranexplanationofhowtorunthecode,werefertopython/README.md. Thevisualizationcanbefoundinthefolder
python/visualizations/.
15CountARFactuals
D Additionalempiricalresults
ARF ARF
MOC MOC
MOCARF MOCARF
MOCCTREE MOCCTREE
NICE NICE
ARF
MOC
MOCARF
MOCCTREE
NICE
−2.8 −2.2 −1.6 −1 −0.6 0 −0.03 −0.02 −0.01 0 −0.18 −0.14 −0.1 −0.06 −0.02
neg. plausibility neg. plausibility neg. plausibility
(a)cassini (b)pawelczyk (c)two_sines
ARF ARF ARF
MOC MOC MOC
MOCARF MOCARF MOCARF
MOCCTREE MOCCTREE MOCCTREE
NICE NICE NICE
−0.035 −0.025 −0.015 −0.005 −0.002 −0.001 0 0.0005 −4e−08 −1e−08 1e−08 3e−08
neg. plausibility neg. plausibility neg. plausibility
(d)bn_5 (e)bn_10 (f)bn_20
Figure4: Medianempiricalattainmentfunction[47]forthenegativeplausibilityandnegativeproximity. Lowervalues
arebetter.
ARF ARF ARF
MOC MOC MOC
MOCARF MOCARF MOCARF
MOCCTREE MOCCTREE MOCCTREE
NICE NICE NICE
−3 −2.5 −2 −1.5 −1 −0.5 0 −0.035 −0.025 −0.015 −0.005 −0.35 −0.25 −0.15 −0.05
neg. plausibility neg. plausibility neg. plausibility
(a)cassini (b)pawelczyk (c)two_sines
ARF ARF ARF
MOC MOC MOC
MOCARF MOCARF MOCARF
MOCCTREE MOCCTREE MOCCTREE
NICE NICE NICE
−0.035 −0.025 −0.015 −0.005 −0.0014 −0.001 −0.0006 0 −2e−08 −1.4e−08 −8e−09 −2e−09
neg. plausibility neg. plausibility neg. plausibility
(d)bn_5 (e)bn_10 (f)bn_20
Figure5: Medianempiricalattainmentfunction[47]forthenegativeplausibilityandnegativesparsity. Lowervalues
arebetter.
16
ytimixorp
.gen
ytimixorp
.gen
ytisraps
.gen
ytisraps
.gen
57.0−
58.0−
59.0−
1−
5.0−
7.0−
9.0−
0
1.0−
3.0−
5.0−
0
2.0−
4.0−
6.0−
ytimixorp
.gen
ytimixorp
.gen
ytisraps
.gen
ytisraps
.gen
4.0−
6.0−
8.0−
1−
57.0−
58.0−
59.0−
0
1.0−
3.0−
5.0−
3.0−
5.0−
7.0−
9.0−
ytimixorp
.gen
ytimixorp
.gen
ytisraps
.gen
ytisraps
.gen
6.0−
7.0−
8.0−
9.0−
88.0−
29.0−
69.0−
1−
0
1.0−
3.0−
5.0−
2.0−
4.0−
6.0−
8.0−