MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation
HanzheHu1* ZhizhuoZhou2* VarunJampani3 ShubhamTulsiani1
1 CarnegieMellonUniversity 2 StanfordUniversity 3 StabilityAI
https://mvd-fusion.github.io/
Input View Synthesized Novel Views Input View Synthesized Novel Views
Figure1. Single-view3DInference. GivenaninputRGBimage,MVD-Fusionallowssynthesizingmulti-viewRGB-Dimagesusinga
depth-guidedattentionmechanismforenforcingmulti-viewconsistency. WevisualizetheinputRGBimage(left)andthreesynthesized
novelviews(withgenerateddepthininset).
Abstract gleRGBinputimageandleveragethe(intermediatenoisy)
depth estimates to obtain reprojection-based conditioning
We present MVD-Fusion: a method for single-view 3D to maintain multi-view consistency. We train our model
inference via generative modeling of multi-view-consistent using large-scale synthetic dataset Obajverse as well as
RGB-D images. While recent methods pursuing 3D infer- the real-world CO3D dataset comprising of generic cam-
enceadvocatelearningnovel-viewgenerativemodels,these era viewpoints. We demonstrate that our approach can
generationsarenot3D-consistentandrequireadistillation yield more accurate synthesis compared to recent state-of-
process to generate a 3D output. We instead cast the task the-art,includingdistillation-based3Dinferenceandprior
of 3D inference as directly generating mutually-consistent multi-view generation methods. We also evaluate the ge-
multipleviewsandbuildontheinsightthatadditionallyin- ometryinducedbyourmulti-viewdepthpredictionandfind
ferring depth can provide a mechanism for enforcing this thatityieldsamoreaccuraterepresentationthanotherdi-
consistency. Specifically, we train a denoising diffusion rect3Dinferenceapproaches.
model to generate multi-view RGB-D images given a sin-
*Equalcontribution.
1
4202
rpA
4
]VC.sc[
1v65630.4042:viXra1.Introduction held out objects as well as scanned real world data. We
show that our approach allows more accurate generation
The task of recovering 3D from a single 2D image has
comparedtopriorstateoftheartwhilealso(directly)gener-
witnessed a recent wave of generative-modeling based ap-
atingplausiblegeometryviathesynthesizeddepthimages.
proaches. Inparticular,whileinitial3Dpredictionmethods
Finally,wealsohighlighttheabilityofourmethodtosam-
pursuedinferenceofvolumetric[1,6],pointclouds[5,47],
plediverseoutputsanditsabilitytogeneralizezero-shotto
or meshes [7, 13] representations, a class of recent ap-
in-the-wildgenericobjects.
proaches [20, 23, 54] instead formulate the task as learn-
ing (conditional) generation of novel views. By adapt-
2.RelatedWork
ing large-scale pre-trained generative models, these meth-
odscanlearngeneralizableviewsynthesisthatperformsre- Single-view 3D Prediction. The task of inferring 3D
markably well even for generic objects in-the-wild. How- from2Dimagesisalong-standingoneincomputervision,
ever, the synthesized novel views are not mutually consis- andthepre-dominant learning-basedapproachhasbeen to
tent and these 2D generative methods rely on a (costly) frameitasapredictiontaskwhereadata-drivenpredictoris
‚Äòscore distillation‚Äô [28] based optimization to then recover trainedtooutputa3Drepresentationgivenimageinput. In
aconsistent3D.Whilethisprocesscanyieldimpressivere- particular,severaldeeplearningbasedmethodspursuedthis
sults,thesecomeatthecostofareductioninboththeeffi- task by inferring a plethora of 3D representations such as
ciencyofinferenceandthediversityofthegenerations. volumetric3D[1,6,40,50],meshes[7,13,15,16,43,49],
Inthiswork,weseektoovercometheselimitations,and pointclouds[5,26,47],orneuralimplicitfields[17,24,41].
pursue an approach that allows directly generating diverse Whiletheseapproacheshaveshownpromisingresults,they
outputs. We do so by re-formulating the task of 3D in- oftenstruggletogeneratedetailedoutputsforcomplexob-
ference as that of generating a set of (mutually consistent) jectsowingtotheambiguityinunobservedregions.Indeed,
multipleviews,andlearna(conditional)generativepriorto any such regression-based approach fundamentally cannot
modelthisjointdistribution. Whilesomerecent(andcon- model the inherent uncertainty in single-view reconstruc-
current)methodsdosimilarly‚Äòco-generate‚Äômultipleviews tion. In contrast, our generative modeling-based approach
givenasingleinputimage[35,36], thesearetypicallynot can synthesize and generate high-fidelity outputs and also
geometricallyconsistent. Instead, ourapproachisinspired yieldmultiplemodes.
bytherecentworkfromLiuetal.[21]whichincorporates
a 3D bottleneck with unprojection and reprojection as an 3D Inference via 2D Diffusion. Instead of directly pre-
inductive bias for ensuring geometric consistency across dicting3Dshapesinafeed-forwardway, thislineofwork
views. Inthiswork,weexploreanalternatemechanismfor utilizes 2D diffusion prior to facilitating 3D inference. In
enforcing such consistency. In particular, we formulate a particular, DreameFusion [28] and SJC [42] formulated a
depth-generation-guidedapproachthat: a)allowsimproved ‚Äòscoredistillation‚Äôobjectivethatenabledincorporatingpre-
generationviadepth-basedreprojection,andb)enablesdi- traineddiffusionasapriorforoptimization, andleveraged
rectlyproducinganestimateofthe3Dgeometryviathein- ittodistilla2Dstablediffusionmodelforthetext-to-3Din-
ferredmulti-view2.5Drepresentation. ference.Inspiredbythis,severalworks[3,8,23,29,33,37,
Our approach for enforcing multi-view consistency 48] adopt this pipeline to optimize a neural radiance field
stems from a simple question: what does it mean for im- (NeRF[25])forthesingle-viewreconstructiontask. Forin-
agestobe3D-consistent? Drawinginspirationfromclassi- stance, RealFusion[23]extractsfromasingleimageofan
cal3Dreconstructionmethods,oneanswertothisquestion object a 360‚ó¶ 3D reconstruction by leveraging a 2D diffu-
is that if a pixel in one image corresponds to a point that sionmodelwithasingle-imagevariantoftextualinversion
is also visible inanother, then the local appearance should whereas NeuralLift-360 [48] utilizes a depth-aware NeRF
match. However, how can we know where the 3D point and learns to craft the scene by denoising diffusion mod-
corresponding to a pixel in one image may project in the els. However, as these methods only use pre-trained im-
other? Thisinspiresoursolutionforgeneratingmulti-view agediffusionmodelsfor3Dinference,theycansufferfrom
consistentimages,wherewenotonlygeneratetheRGBim- implausible3Doutputse.g.januseffect anddonotalways
agesbutalsoreasonaboutthecorrespondingdepthforthese preservethedetailsintheobservedimage.
generations(andthusallowsuchreprojection-basedmulti- Tocircumventthis,SparseFusion[54]proposedtolearn
viewconsistency). Morespecifically,weadoptexisting2D a novel-view diffusion model using epipolar feature trans-
diffusion models to generate RGB-D images while adding former to build a view-conditioned features model for
multi-view projection based on (noisy) depth estimates to sparse-view reconstruction and distilled it to obtain more
enforce3Dconsistency. accurate3Dreconstructions.Moreover,DiffusionwithFor-
Wetrainoursystemusingalarge-scalesyntheticdataset, wardModels[39]applied2Ddiffusionnetworkstodenoise
andempiricallydemonstratetheefficacyofourapproachon a consistent 3D scene. Our approach builds on Zero-1-to-
2Depth-aware 3D attention
p =ùúã‚àí1(ùë¢,ùëën)
ùë°
=ùúè
u
Feature Aggregation
Input View & Noisy Target Views ùíô ùë°1:ùëÅ Multi-view Aware Features ùíõ ùë°1:ùëÅ
Cross Attention
Noisy Target View ùíôùëõ Input View Diffusion U-Net Denoised Target View ùíôùëõ
ùë° ùë°‚àí1
Figure 2. Approach Overview. MVD-Fusion learns a denoising diffusion model for generating multi-view RGB-D images given an
inputRGBimage. Ateachdiffusiontimestept,MVD-Fusionusesthecurrent(noisy)depthestimatestocomputedepth-projection-based
multi-viewawarefeatures(top).Anovel-viewdiffusionbasedU-Netismodifiedtoleveragethesemulti-viewawarefeaturesasadditional
conditioningwhileproducingdenoisedestimatesofboth,RGBanddepth(bottom).
3 [20], which demonstrated that a large pre-trained image dant information. Finally, there have been several promis-
diffusion model can be finetuned for novel-view genera- ing concurrent works which also pursue multi-view infer-
tion using a large-scale 3D dataset to achieve better gen- ence[11,14,19,22,34,46]butwebelievethatourmethod
eralizationability. Whilethesemethodsareabletoproduce of depth-guided multi-view diffusion represents a comple-
high-quality predictions, the reliance on score distillation mentaryadvancetothetechniquesproposedinthese.
samplingrestrictsthemfromobtainingdiverseresultswith
single-view3Dpredictionasanunder-constrainedtask. 3.Method
Given a single RGB image, our method generates a set of
Multi-view Image Generation. Unlike novel-view gen-
multi-view consistent RGB-D predictions. In addition to
eration models which model the distribution over a sin-
allowing the synthesis of the object from any desired set
gle view given a reference image, many recent works
ofviews,thegeneratedmulti-viewdepthmapsalsoconve-
haveinvestigatedgeneratingmulti-viewimagesatthesame
nientlyyielda(coarse)pointcloudrepresentationofthege-
timebyusingdiffusionmodels,includingtext-basedmeth-
ometry. To ensure multi-view consistency among the gen-
ods[35,38]andimage-basedmethods[21,36].Givenatext
erated images, we model the joint distribution over a set
conditioning, MVDiffusion [38] simultaneously generates
ofposedimagesbyaddingdepth-guided3Dcross-attention
allimageswithaglobaltransformertofacilitatecross-view
layersontopofpre-trainedlatentdiffusionbackbonefrom
interactions. Similarly, MVDream [35] produces multi-
Stable Diffusion [32] and Zero-1-to-3 [20]. We first for-
viewimagesviamulti-viewdiffusionandleveragesaself-
malizethistaskofmulti-viewgenerationviadenoisingdif-
attentionlayertolearncross-viewdependencyandencour-
fusion (Section 3.1), and then detail our specific approach
age multi-view consistency. While these methods rely on
toenforcingmulti-viewconsistency(Section3.2)and2.5D
textinput,ViewsetDiffusion[36]adoptsasimilarapproach
image generation (Section 3.3). A diagram of our method
forgeneratingamulti-viewimagesetgivenaninputimage
isinFigure2.
and subsequently infers a radiance field to ensure consis-
tent geometry. While these methods, similar to our goal,
3.1.Multi-viewDenoisingDiffusion
canmodelthedistributionovernovelviews,theydolever-
age any geometric mechanism to enforce multi-view con- A conditional denoising diffusion model can capture the
sistency. Perhaps most closely related to our work, Sync- distribution over a variable of interest x given some con-
Dreamer [21] proposes to use a 3D-aware feature atten- ditioningc. Inparticular, bylearningafunctionœµ(x ,c,t)
t
tion mechanism that correlates the corresponding features thatlearnstodenoiseaninputwithtime-dependentcorrup-
across views to enforce multi-view consistency. Different tion,diffusionmodelscanallowsamplingfromthedistribu-
from [21],weutilizedepthinformationtolearnconsistency tionp(x|c). Towardsourgoalofmulti-viewgeneration,we
acrossviewsinsteadofa3Dbottleneckthatcontainsredun- are interested in an instantiation of this framework where
3knowledgeofobjectsurfaces.Incontrast,weproposetoex-
plicitlyreasonaboutthesurfacebyadditionallygenerating
depthandbiasingsamplingnearthepossiblesurface.
Givenatargetviewxi,weobtainfeaturefrustumzi by
t t
shooting rays and sampling features at 3D locations along
therays. Foreachray, wesampleD depthvaluesnearthe
Input View T=999 T=0
expectedsurfaceandaggregateprojectedfeaturesfromtar-
Figure3. Wevisualizetheunprojectedpointcloudobtainedfrom getviews{xn}andinputviewy. Let(zi) bethefeature
t t md
asetofnoisyRGB-Dimagesatdifferenttimestepsduringinfer- forthem-thrayatd-thdepthinzi.Fora3Dpointpi cor-
t md
ence. Weobservethegradualdenoisingofgeometryfromaran- respondingtothefeature(zi) ,wesampleN+1features
t md
dompointcloudtoapointcloudthatmatchestheinputobject. c from{xn}andy.Wealsoincludethepluckerembed-
mdn t
ding of query ray q and reference rays r from pi
m mdn md
to N +1 camera centers along with the sampled features
theconditioningcorrespondstoanobservedRGBimagey
as input into the transformer f that predicts view-aligned
and a set of desired novel viewpoints {œÄn}. Given these Œ∏
multi-viewawarefeatures:
as input, we aim to generate a (mutually consistent) set of
novelviews{xn}correspondingtotheconditioningview- N+1
(cid:88)
points and thus seek to learn a denoising diffusion model (zi t) md = w Œ∏(v mdn,t)f Œ∏(v mdn,t)
(3)
thatcapturesp({xn}|y,{œÄn}). n=0
To learn such a diffusion model, we need to formulate wherev mdn ={c mdn,r mdn,q m}
anapproachthatcanpredictthenoiseaddedtoasetofcor-
Here, w (v ,t) represents (normalized) weights pre-
Œ∏ mdn
ruptedmulti-viewimages:
dicted by the transformer, which are then used to aggre-
‚àö ‚àö gatethemulti-viewfeaturestoobtainthepointwisefeature
{xn}={ Œ±¬Ø xn+ 1‚àíŒ±¬Ø œµ }
t t t n (zi) .
œµ =f({xn},y,t) t md
pred t Naively, we can sample a large number of depth points
alongeachraylinearlyspacedthroughoutthescenebound;
Instead of learning such a prediction model from scratch,
however, such exhaustive sampling quickly becomes a
we propose to adapt a pre-trained novel-view generative
memoryconstraintwhilepossiblymakingthelearningtask
modelfromZero-1-to-3[20]. Specifically,thismodelcap-
more difficult as the network may also observe features
tures the distribution over a single novel view p(x|y,œÄ)
away from the surface. Thus, we sample D = 3 depths
given an RGB input by learning a denoising function
from a Gaussian distribution centered around an unbiased
œµ (y,x ,œÄ,t). While we aim to leverage this pre-trained
œï t estimateofdepthgiventhenoisydepthd andascaledver-
large-scalemoduleforefficientlearningandgeneralization, t
sionofthedenoisingdiffusionmodelvarianceschedulein
it only models the distribution over a single novel view
equationEq.4.
whereas we aim to model the joint distribution over mul-
‚àö
tipleviews. Toenableefficientlyadaptingthis,wepropose d‚àºN(E[d ],k‚àö Œ±¬Ø t )
a first learn a separate module that computes view-aligned 0 1‚àíŒ±¬Ø
multi-view aware features {zn}. We then modify the pre- ‚àö ‚àö t
t whered = Œ±¬Ø d + 1‚àíŒ±¬Ø œµ (4)
t t 0 t
trainedsingle-viewdiffusionmodeltoadditionallyleverage
d
thismulti-viewawareconditioning: andE[d ]= ‚àöt
0 Œ±¬Ø
t
zn t =f Œ∏(y,{xn t},{œÄn},t) (1) We then use these multi-view aware features {zn} as
t
œµ
pred
={œµ œï‚Ä≤(y,xn t,œÄn,zn t,t)} (2) conditioning input into our latent diffusion model in Sec-
tion3.3.
3.2.Depth-guidedMulti-viewConsistency 3.3.LearningMulti-view2.5DDiffusion
Generatingasetofconsistentimagesrequiresthenetwork InspiredbythesuccessoffinetuningpretrainedStableDif-
to attend across different images within the set. Sync- fusionmodels,weadaptZero-1-to-3[20]asourmulti-view
Dreamer [21] proposes a way of achieving multi-view at- novel view synthesis backbone œµ . While Zero-1-to-3 is
œï
tention by unprojecting features from the set of images designed to only model single-view distributions and gen-
{x0,x1,...xN}ontoa3DvolumeV andinterpolatingcon- erateRGBoutput,weadaptittopredictanadditionaldepth
t t t
ditioning feature frustums {z0,z1,...zN}. However, inter- channelandcross-attendtothemulti-viewawarefeatures.
t t t
polating feature frustums linearly spaced across the whole First,weincreasetheinputandoutputchannelsofthela-
3Dvolumeisanexpensiveoperationthatassumesnoprior tentdiffusionUNetbackbonetopredictnormalizeddepth.
4Input View Zero-1-to-3 SyncDreamer MVD-Fusion Ground Truth
Figure 4. Qualitative results for novel view synthesis on instances from Objaverse (top) and Google Scanned objects (bottom).
We compare our method with Zero-1-to-3 [20] and SyncDreamer [21]. We show the input image and two novel views generated by
each method. Zero-1-to-3 independently generates novel views which are not consistent (e.g. the person in Objaverse). While both,
SyncDreamer and MVD-Fusion yield consistent generations, we find that MVD-Fusion can generate more plausible output (e.g. the
Androidimage)andismorefaithfultodetailsintheinput(e.g.thethreecars).
Whiletheimagelatentscanbedecodedintohigh-resolution During training, we finetune all the parameters of our
images,ourpredicteddepthmapremainsatthelowerreso- network and follow [10] to use a simplified variational
lution.Thismulti-resolutionapproachtopredictingRGB-D lowerboundobjectiveinEq.5.Duringinference,wefollow
letsususethefrozenStableDiffusionVAEtodecodehigh- [21]anduseaclassifierfreeguidanceof2.0.
resolutionRGBimages.Moreover,weaddadditionalresid-
ual cross-attention layers at multiple levels of the UNet to
L =E [||œµn‚àí{œµ (y,xn,œÄn,zn,t)}||]
attendtoourmulti-viewawarefeatures. Finally,wemodify DM xn 0,œµn ‚àö,t ‚àöœï‚Ä≤ t t
(5)
thecameraparameterizationusedinZero-1-to-3froma3- wherex = Œ±¬Øx + 1‚àíŒ±¬Øœµ; œµ‚àºN(0,1)
t t 0 t
DoFazimuth,elevation,andradiusparameterizationtouse
4.Experiments
thefullperspectivecameramatrix. Thismakesourmethod
capableofhandlingarbitrarycameraposesinrealdatasets
WetrainMVD-Fusion usingalarge-scalesyntheticdataset
suchasCO3D.
(Section 4.1), and evaluate it on both, synthetic and real-
5Table 1. Results for novel view synthesis on the Objaverse
dataset.Wecompareourmethodwithtwootherbaselineson100
instancesfromthetestsetofObjaversedataset, sameasin[20].
Ourmethodoutperformsexistingbaselinesoverthreecommonly
usedmetrics:PSNR,SSIMandLPIPS.
Method PSNR‚Üë SSIM‚Üë LPIPS‚Üì
Zero123[20] 17.37 0.783 0.211
SyncDreamer[21] 19.22 0.817 0.176
Input View Sample 1 Sample 2 Sample 3
MVD-Fusion 21.19 0.835 0.146
Figure 5. Sample Diversity. MVD-Fusion is capable of gener-
ating diverse samples given the same input. We show the input
imagesfromnovelviewpoints. BuiltonZero-1-to-3,Sync-
image(left)followedbyviewssynthesizedinthreerandomlygen-
Dreamercansimultaneouslygeneratemultipleimagesfrom
eratedsamples. Weobservethatthereismeaningfulvariationin
uncertainregionse.g.theeyesofthecharacterandthecolorson different viewpoints with 3D consistency. For CO3D, we
thescreenvaryacrosssamples. compare against PixelNeRF [51] as both Zero-1-to-3 and
SyncDreamerarerestrictedto3-DoFcameravariation.
For 3D reconstruction, we compare our method with
world objects for view synthesis (Section 4.2) and 3D the aforementioned two methods together with RealFu-
reconstruction (Section 4.3). We show that our results sion [23], Magic 123 [29], One-2-3-45 [18], Point-E [27]
achieves more accurate view synthesis compared to state- and Shape-E [12]. Note that the diffusion-based methods
of-the-artaswellasyieldsbetter3Dpredictionscompared requireneuralfieldoptimizationusingeitherrenderingob-
to prior direct 3D inference methods. Finally, we present jectives or distillation objectives (e.g. Zero-1-to-3 requires
qualitativeresultsonin-the-wild-objects(Section4.4). a SDS distillation for extracting geometry whereas Sync-
DreamerreliesontrainingNeus[44]),whereasourmethod
4.1.ExperimentalSetup allows‚Äòdirectly‚Äôcomputingthegeometryviaun-projecting
thepredicteddepthmaps. Tohighlightthisdistinction,we
Datasets. Weusethelarge-scale3DdatasetObjaverse[2]
categorize the reconstruction approaches as direct (One-2-
fortraining. SinceObjaverseislarge(800Kinstances)and
3-45 [18], Point-E [27], Shape-E [12], and MVD-Fusion)
contains several instances with poor texture, we filter the
or optimization based (RealFusion [23], Magic 123 [29],
dataset with CLIP score to remove instances that match a
Zero-1-to-3[20],andSyncDreamer[21]).
set of hand-crafted negative text prompts. Our filtered set
containsaround400Kinstances. Foreachinstance,weren-
Metrics. For the novel view synthesis, we adopt com-
der 16 views from an elevation of 30 degrees and azimuth
monly used metrics: PSNR, SSIM [45], and LPIPS [53].
linearly spaced across 360 degrees. Additionally, we hold
For the 3D reconstruction task, we report Chamfer Dis-
outasubsetofObjaverseinstancesfollowing[20]foreval-
tancesbetweenground-truthandpredictedpointclouds.
uation,whichconsistsofabout4kinstances.
Beyond evaluating on these held-out Objaverse in- ImplementationDetails. Wetrainourmodelonafiltered
stances, we also evaluate our method with the Google version of the Objaverse dataset, which consists of about
ScannedObjectdataset(GSO)[4],whichconsistsofhigh- 400kinstances. Duringtraining,foreachinstance,weran-
qualityscannedhouseholditems. Foreachobject, weren- domly sample 5 views and choose the first one as the in-
der 16 views evenly spaced in azimuth from an elevation put view. We train the model for 400k iterations with 4
of 30 degrees and choose one of them as the input image. 40G A100 GPUs using a total batch size of 16. We use
For quantitative results, we randomly chose 30 objects to Adamoptimizerwithalearningrateof1e-5. Eventhough
computethemetrics. Finally,toshowtheflexibilityofour weonlytrainwith5views, wecansampleanarbitraryset
approachinmodelingreal-worlddatasetswithgeneralper- duringinferenceasourdepth-basedprojectionfollowedby
spectivecameras,asopposedtothecommon3DoFcameras transformer-basedaggregationtriviallygeneralizestoincor-
used in Objaverse and GSO, we finetune and evaluate our poratemoreviews. Inourexperiments,werender16views
model on CO3D [31]. We follow [52] to train on 41 cate- foreachinstanceforevaluation.
goriesandevaluateontheheld-outsetofall51categories.
4.2.NovelViewSynthesis
Objaverse and Google Scanned Objects. We report
Baselines. For the novel view synthesis task, we adopt quantitative results on the Objaverse dataset and GSO
Zero-1-to-3 [20] and SyncDreamer [21] as our baseline dataset in Table 1 and Table 2, respectively. For the Ob-
methods. Givenaninputimage,Zero-1-to-3cansynthesize javerse dataset, we use the held-out test set for evaluation
6Table2.ResultsfornovelviewsynthesisontheGoogleScanned
Objects(GSO)dataset. Wecompareourmethodwithtwoother
baselineson100instancesrandomlychosenfromtheGSOdataset.
Ourmethodachievesconsistentimprovementoverbaselinemeth-
odsonPSNRandLPIPS,whileslightlyworsethanSyncDreamer
onSSIM.
Method PSNR‚Üë SSIM‚Üë LPIPS‚Üì
Zero123[20] 17.42 0.756 0.207 Input PixelNeRF MVD-Fusion
SyncDreamer[21] 18.95 0.796 0.176
Figure7.Qualitativeresultsfornovelviewsynthesisoninstances
MVD-Fusion 19.53 0.790 0.175
fromCO3D.MVD-Fusionisabletopredictaccurateandrealistic
novelviewsonreal-worlddatasetswithperspectivecameraposes.
Table3.Resultsfor51categorynovelviewsynthesisonCO3D.
WesignificantlyoutperformPixelNeRFonperceptualquality. baselines,wealsohighlighttheabilityofMVD-Fusion to
generate multiple plausible outputs. In particular, since
Method PSNR‚Üë SSIM‚Üë LPIPS‚Üì novel view synthesis from a single image is an under-
constrained task, using the diffusion model can effectively
PixelNeRF[51] 17.64 0.484 0.378
generate more diverse samples given a single input image.
MVD-Fusion 17.16 0.701 0.220
As shown in Figure 5, MVD-Fusion is able to generate
diverse plausible samples with different random seeds e.g.
varyingtexturesinthefrontofthebus.
MVD-Fusion 56.1% 43.9% Zero-123
MVD-Fusion 56.3% 43.7% SyncD. User Study. We run a user study by randomly selecting
40instancesfromObjaverseandGSOtestsetandasking43
SyncD. 17.9% 82.1% G.T.
users to make 860 pairwise comparisons (users are shown
aninputimageandtwogeneratednovelviewspermethod).
Zero-123 24.8% 75.2% G.T.
WeshowresultsinFigure6.Ourmethodtendstobechosen
MVD-Fusion 31.4% 68.6% G.T. overZero-123andSyncDreamer,andisalsomorecompet-
0 25 50 75 100 itivewithGTcomparedtothesebaselines.
Win Rate Percentage
Figure 6. User preference percentage of MVD-Fusion against
Common Objects in 3D. Real-world data often have
Zero-123,SyncDreamer(SyncD.),andgroundtruth(G.T.).
cameras that are not origin facing, making methods that
model3DoForiginfacingcameras[20,21]notsuitablefor
real-worldinference. WefinetuneourmodelonCO3Dand
whereas we use a subset of 30 random objects from GSO.
shownovelviewsynthesisresultsinTable3.Wealsotraina
Wefindthatourmethodachievesconsistentimprovements
cross-categoryPixelNeRF[51]modelasabaseline. While
overthebaselinesacrossmetricsonboth,thein-distribution
it is slightly better in PSNR (perhaps due to blurry mean
Objaverse dataset and the out-of-distribution GSO dataset.
predictions being optimal under uncertainty), our method
We also provide qualitative comparisons on the Objaverse
vastly outperforms PixelNeRFin perceptual metrics SSIM
dataset and GSO dataset in Figure 4. Although Zero-1-to-
andLPIPS(seeFigure7).
3[20]producesvisuallyreasonableimages,itsuffersfrom
multi-view inconsistency across generated viewpoints. In
4.3.Single-viewReconstruction
contrastSyncDreamerandMVD-Fusion,areabletoobtain
multi-viewconsistencyamonggeneratedimages. However, Unlike previous methods such as Zero-1-to-3 and Sync-
SyncDreamer struggles to obtain high alignment with the Dreamer, which have to fit a radiance field from gener-
input image, sometimes leading to consistent multi-view ated multi-view images to obtain the 3D shape, MVD-
imageswithunreasonableappearance. Ourmethod,onthe Fusion can directly obtain the point cloud. With multi-
other hand, is able to generate multi-view consistent im- view RGB-D generations, we can simply unproject the
ages with better alignment with the image input and more foreground pixels and obtain the object point cloud. We
plausiblecompletionsinunobservedregions.Moreover,we showquantitativeresultsinTable4,wherewecompareour
also note that SyncDreamer is trained on the whole Obja- methodagainstpreviousmethodsontheGSOdatasetusing
verse dataset and may have actually seen these instances chamfer distance. We see that our method outperforms all
whereastheserepresentheldoutinstancesforboth,MVD- ofthemethodsthatdirectlyinfer3Dshapesandmostofthe
Fusion andZero-1-to-3. methods that require further optimization steps to get the
In addition to visualizing the comparative results with 3Dshapes.
7Input View Synthesized Novel Views Geometry Input View Synthesized Novel Views Geometry
Figure8.In-the-wildGeneralization.WevisualizethepredictionfromMVD-Fusiononin-the-wildinternetimages.WefindthatMVD-
Fusion isabletopreservetherichtextureintheinputimagesandmodeltheroughgeometrywithoutpost-processing.
Table4. Resultsfor3DreconstructionontheGoogleScanned 5.Discussion
Objects(GSO)dataset. ‚ÄòOptimization‚Äôdenotesmethodsthatre-
quireadditionaltrainingsuchasfittinganoccupancyfieldtoob- In this work, we presented MVD-Fusion, which allowed
tain3Dshapes. ‚ÄòDirect‚Äôdenotesmethodsthatcandirectlyoutput co-generating multi-view images given a single input im-
3Dpredictions. Following [21],wereportChamferDistanceon age. Our approach allowed adapting a pre-trained large-
thesame30instancesfromtheGSOdataset.Ourmethoddemon- scalenovel-viewdiffusionmodelforgeneratingmulti-view
stratesconsistentimprovementover‚Äòdirect‚Äômethodsandoutper- RGB-Dimages,andenforcedconsistencyamongthesevia
formsmostofthe‚Äòoptimization‚Äômethods.
depth-guided projection. While our results showed im-
provementsoverpriorstate-of-the-artacrossdatasets,there
3DExtraction Method ChamferDist‚Üì
are several challenges that still remain. First, the multi-
RealFusion[23] 0.082 view consistency is encouraged via inductive biases in the
Magic123[29] 0.052 network design but is not guaranteed, and the network
Optimization
Zero123[20] 0.034 maygenerate(slightly)inconsistentmulti-viewpredictions.
SyncDreamer[21] 0.026 Moreover, while our inferred multi-view depth maps can
One-2-3-45[18] 0.063 yield a point cloud representation that captures the coarse
Point-E[27] 0.043 geometry, these coarse depth maps do not capture the fine
Direct
Shap-E[12] 0.044 details visible in the generated views and an optimization-
MVD-Fusion 0.031 basedproceduremayhelpextractthesebetter. Finally,our
approach has been trained on clean unoccluded instances
andwouldnotbedirectlyapplicableunderclutteredscenes
with partially visible objects, and it remains an open re-
4.4.In-the-wildGeneralization
search question to build systems that can deal with such
challengingscenarios.
We also demonstrate the generalization ability of MVD-
Fusionforreconstructingin-the-wildimagesfromtheinter-
Acknowledgements
net. Weshowqualitativeresultsdepictinggeneratednovel
views and recovered point clouds in Figure 8. With chal- We thank Bharath Raj, Jason Y. Zhang, Yufei (Judy) Ye,
lengingout-of-domainimagesasinput,MVD-Fusionisstill YanboXu, andZifanShiforhelpfuldiscussionsandfeed-
capableofgeneratingconsistentnovel-viewimagesandrea- back. This work is supported in part by NSF GRFP Grant
sonable3Dshapesfromsingle-viewobservation. No. (DGE1745016,DGE2140739).
8References [17] Chen-HsuanLin,ChaoyangWang,andSimonLucey. Sdf-
srn: Learningsigneddistance3dobjectreconstructionfrom
[1] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin
staticimages. InNeurIPS,2020. 2
Chen,andSilvioSavarese. 3d-r2n2: Aunifiedapproachfor
[18] MinghuaLiu,ChaoXu,HaianJin,LinghaoChen,Mukund
single and multi-view 3d object reconstruction. In ECCV,
VarmaT,ZexiangXu,andHaoSu. One-2-3-45:Anysingle
2016. 2
imageto3dmeshin45secondswithoutper-shapeoptimiza-
[2] MattDeitke, DustinSchwenk, JordiSalvador, LucaWeihs, tion. InNeurIPS,2023. 6,8
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana [19] MinghuaLiu, RuoxiShi, LinghaoChen, ZhuoyangZhang,
Ehsani,AniruddhaKembhavi, andAliFarhadi. Objaverse: Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Ji-
Auniverseofannotated3dobjects. InCVPR,2023. 6 ayuan Gu, and Hao Su. One-2-3-45++: Fast single image
[3] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, to 3d objects with consistent multi-view generation and 3d
Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al. diffusion. InCVPR,2024. 3
Nerdi:Single-viewnerfsynthesiswithlanguage-guideddif- [20] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
fusionasgeneralimagepriors. InCVPR,2023. 2 makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:
[4] LauraDowns,AnthonyFrancis,NateKoenig,BrandonKin- Zero-shotoneimageto3dobject. InICCV,2023. 2,3,4,5,
man,RyanHickman,KristaReymann,ThomasBMcHugh, 6,7,8,1
and Vincent Vanhoucke. Google scanned objects: A high- [21] YuanLiu,ChengLin,ZijiaoZeng,XiaoxiaoLong,Lingjie
quality dataset of 3d scanned household items. In ICRA, Liu, Taku Komura, and Wenping Wang. Syncdreamer:
2022. 6 Learning to generate multiview-consistent images from a
[5] Haoqiang Fan, Hao Su, and Leonidas J. Guibas. A point single-viewimage. InICLR,2024. 2,3,4,5,6,7,8
set generation network for 3d object reconstruction from a [22] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,
singleimage. InCVPR,2017. 2 Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,
MarcHabermann,ChristianTheobalt,etal. Wonder3d:Sin-
[6] RohitGirdhar, DavidFFouhey, MikelRodriguez, andAb-
gle image to 3d using cross-domain diffusion. In CVPR,
hinav Gupta. Learning a predictable and generative vector
2024. 3
representationforobjects. InECCV,2016. 2
[23] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and
[7] GeorgiaGkioxari,JitendraMalik,andJustinJohnson. Mesh
AndreaVedaldi. Realfusion: 360degreconstructionofany
r-cnn. InICCV,2019. 2
objectfromasingleimage. InCVPR,2023. 2,6,8
[8] JiataoGu,AlexTrevithick,Kai-EnLin,JoshuaMSusskind,
[24] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.
bastianNowozin,andAndreasGeiger.Occupancynetworks:
Nerfdiff: Single-imageviewsynthesiswithnerf-guideddis- Learning 3d reconstruction in function space. In CVPR,
tillationfrom3d-awarediffusion. InICML,2023. 2
2019. 2
[9] Jonathan Ho and Tim Salimans. Classifier-free diffusion [25] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
guidance. arXivpreprintarXiv:2207.12598,2022. 1 JonathanT.Barron,RaviRamamoorthi,andRenNg. Nerf:
[10] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu- Representingscenesasneuralradiancefieldsforviewsyn-
sionprobabilisticmodels. InNeurIPS,2020. 5 thesis. InECCV,2020. 2
[11] Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, [26] KLNavaneet,AnsuMathew,ShashankKashyap,Wei-Chih
YangguangLi,XinyuanChen,Yan-PeiCao,DingLiang,Yu Hung,VarunJampani,andRVenkateshBabu. Fromimage
Qiao, Bo Dai, and Lu Sheng. Epidiff: Enhancing multi- collections to point clouds with self-supervised shape and
view synthesisvia localizedepipolar-constrained diffusion. posenetworks. InCVPR,2020. 2
InCVPR,2024. 3 [27] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
[12] Heewoo Jun and Alex Nichol. Shap-e: Generat-
ing3dpointcloudsfromcomplexprompts. arXivpreprint
ing conditional 3d implicit functions. arXiv preprint
arXiv:2212.08751,2022. 6,8
arXiv:2305.02463,2023. 6,8
[28] BenPoole,AjayJain,JonathanT.Barron,andBenMilden-
[13] AngjooKanazawa,ShubhamTulsiani,AlexeiAEfros,and
hall. Dreamfusion: Text-to-3dusing2ddiffusion. InICLR,
JitendraMalik. Learningcategory-specificmeshreconstruc-
2023. 2
tionfromimagecollections. InECCV,2018. 2
[29] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,
[14] Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng
Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-
Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey
rokhodov,PeterWonka,SergeyTulyakov,etal. Magic123:
Tulyakov,IgorGilitschenski,andAliaksandrSiarohin.Spad
One image to high-quality 3d object generation using both
:Spatiallyawaremultiviewdiffusers. InCVPR,2024. 3
2dand3ddiffusionpriors. InICLR,2024. 2,6,8
[15] NileshKulkarni, JustinJohnson, andDavidFFouhey. Di- [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
rectedraydistancefunctionsfor3dscenereconstruction. In Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
ECCV,2022. 2 Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
[16] Nilesh Kulkarni, Linyi Jin, Justin Johnson, and David F Krueger, and Ilya Sutskever. Learning transferable visual
Fouhey. Learning to predict scene-level implicit 3d from modelsfromnaturallanguagesupervision. InICML,2021.
posedrgbddata. InCVPR,2023. 2 1
9[31] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, and Jun Zhu. Crm: Single image to 3d textured mesh
LucaSbordone,PatrickLabatut,andDavidNovotny. Com- with convolutional reconstruction model. arXiv preprint
mon objects in 3d: Large-scale learning and evaluation of arXiv:2403.05034,2024. 3
real-life3dcategoryreconstruction. InICCV,2021. 6 [47] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph
[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Feichtenhofer, and Georgia Gkioxari. Multiview compres-
PatrickEsser,andBjo¬®rnOmmer.High-resolutionimagesyn- sivecodingfor3dreconstruction. InCVPR,2023. 2
thesiswithlatentdiffusionmodels. InCVPR,2022. 3,1 [48] DejiaXu,YifanJiang,PeihaoWang,ZhiwenFan,YiWang,
[33] QiuhongShen,XingyiYang,andXinchaoWang. Anything- andZhangyangWang.Neurallift-360:Liftinganin-the-wild
3d:Towardssingle-viewanythingreconstructioninthewild. 2dphototoa3dobjectwith360degviews. InCVPR,2023.
arXivpreprintarXiv:2304.10261,2023. 2 2
[34] RuoxiShi,HanshengChen,ZhuoyangZhang,MinghuaLiu, [49] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir
ChaoXu,XinyueWei,LinghaoChen,ChongZeng,andHao Mech, and Ulrich Neumann. Disn: Deep implicit surface
Su. Zero123++:asingleimagetoconsistentmulti-viewdif- network for high-quality single-view 3d reconstruction. In
fusionbasemodel. arXivpreprintarXiv:2310.15110,2023. NeurIPS,2019. 2
3 [50] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. Shelf-
[35] YichunShi,PengWang,JianglongYe,MaiLong,KejieLi, supervisedmeshpredictioninthewild. InCVPR,2021. 2
andXiaoYang. Mvdream:Multi-viewdiffusionfor3dgen- [51] AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa.
eration. InICLR,2024. 2,3 pixelnerf:Neuralradiancefieldsfromoneorfewimages. In
[36] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea CVPR,2021. 6,7
Vedaldi.Viewsetdiffusion:(0-)image-conditioned3dgener- [52] JasonY.Zhang,DevaRamanan,andShubhamTulsiani.Rel-
ativemodelsfrom2ddata. InICCV,2023. 2,3 Pose:Predictingprobabilisticrelativerotationforsingleob-
[37] JunshuTang,TengfeiWang,BoZhang,TingZhang,RanYi, jectsinthewild. InECCV,2022. 6
LizhuangMa,andDongChen.Make-it-3d:High-fidelity3d [53] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,
creationfromasingleimagewithdiffusionprior. InICCV, and Oliver Wang. The unreasonable effectiveness of deep
2023. 2 featuresasaperceptualmetric. InCVPR,2018. 6
[38] ShitaoTang,FuyangZhang,JiachengChen,PengWang,and [54] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-
YasutakaFurukawa. Mvdiffusion: Enablingholisticmulti- tillingview-conditioneddiffusionfor3dreconstruction. In
viewimagegenerationwithcorrespondence-awarediffusion. CVPR,2023. 2
InNeurIPS,2023. 3
[39] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon
Rezchikov,JoshuaB.Tenenbaum,Fre¬¥doDurand,WilliamT.
Freeman, and Vincent Sitzmann. Diffusion with forward
models: Solvingstochasticinverseproblemswithoutdirect
supervision. InNeurIPS,2023. 2
[40] Shubham Tulsiani, Tinghui Zhou, Alexei A Efros, and Ji-
tendraMalik. Multi-viewsupervisionforsingle-viewrecon-
structionviadifferentiablerayconsistency. InCVPR,2017.
2
[41] KalyanAlwalaVasudev,AbhinavGupta,andShubhamTul-
siani. Pre-train,self-train,distill: Asimplerecipeforsuper-
sizing3dreconstruction. InCVPR,2022. 2
[42] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
andGregShakhnarovich. Scorejacobianchaining: Lifting
pretrained2ddiffusionmodelsfor3dgeneration. InCVPR,
2023. 2
[43] NanyangWang,YindaZhang,ZhuwenLi,YanweiFu,Wei
Liu,andYu-GangJiang. Pixel2mesh: Generating3dmesh
modelsfromsinglergbimages. InECCV,2018. 2
[44] PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,Taku
Komura,andWenpingWang.Neus:Learningneuralimplicit
surfacesbyvolumerenderingformulti-viewreconstruction.
InNeurIPS,2021. 6
[45] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P
Simoncelli. Imagequalityassessment: fromerrorvisibility
tostructuralsimilarity. InTIP,2004. 6
[46] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xi-
ang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su,
10A.ArchitectureDetails Table 5. Ablation study on Google Scanned Objects (GSO)
dataset. Weablatetheeffectoftheclassifier-freeguidancescale
Our network consists of modifications on top of Zero123 during inference. We randomly chose 30 instances from the
[20]. Wedescribeeachcomponentofournetworkindetail. datasetforevaluation. ‚ÄòScale‚Äôdenotestheclassifier-freeguidance
scale.
VAE.WeusethepretrainedVAEfromStableDiffusion1.4
[32]. WefreezetheVAE.
Scale PSNR‚Üë SSIM‚Üë LPIPS‚Üì
UNet. We initialize our UNet with weights from Zero123
[20]. Zero123hasanovelviewsynthesisUNetthataccepts 1.0 19.34 0.775 0.199
2.0 19.91 0.787 0.184
oneinputimage(4channellatents)andonetargetnoisyim-
3.0 19.38 0.778 0.189
age(4channellatents)alongwithcameraposeandpredicts
5.0 18.66 0.771 0.194
anovelviewimagelatent(4channels).Wemodifytheinput
andoutputblockstoaccommodatethepredictionofanad-
ditionaldepthchannel.OurUNethas10inputchannelsand
method, we use the classifier-free guidance scale œâ
5 output channels. For all experiments, we use only RGB
to control the contribution of the classifier-free model:
images as input (4 channel latents) and pad the additional
œµÀÜ (y,xn,œÄn,zn,t) = œâœµ (y,xn,œÄn,zn,t) + (1 ‚àí
channelwithzeros. Thenoisytargettargetimageisalways œï‚Ä≤ t t œï‚Ä≤ t t
œâ)œµ (xn,t), where œµ (y,xn,œÄn,zn,t) is our proposed
5channels. œï‚Ä≤ t œï‚Ä≤ t t
multi-view diffusion model. In practice, we notice that a
CLIPandCameraPoseEmbedding. WefollowZero123
higher classifier-free guidance scale leads to better multi-
[20] to use the frozen CLIP [30] image encoder along
viewconsistency. AsshowninTable5,wefindthatadopt-
with camera information as one of the inputs to the cross-
ingascaleof2yieldsthebestperformance. Therefore,we
attentionlayersinStableDiffusion.However,insteadofus-
usethisclassifier-freeguidancescaleforinference.
ingazimuthandelevationanglerepresentation,wedirectly
useaflattenedcameramatrixasinput. Weuse3fullycon-
nectedlayerstomapCLIPimageembeddingandflattened
cameramatrixintocross-attentioninputofdimension768.
Depth-guided Multi-view Attention. After each of the
existing cross-attention layers in the UNet, we add addi-
tionalcrossattentionlayersthatattendtoview-alignedfea-
ture frustums sampled from our depth-guided multi-view
attention module. Our depth-guided attention module is a
3layerstransformerthataggregatesinformationacrossthe
noisytargetlatentsfromthecurrenttimestepandalsoinput
image latents. For each target view, we generated a fea-
turefrustumofshape(1,256,3,32,32),wherethefeature
mapis32by32, with3depthsamplesandfeaturedimen-
sion 256. The depth dimension represents the number of
depth points sampled along each ray and can be reduced
down to just 1. Our transformer uses a hidden dimension
of256with8heads. Weuseanadditionalfullyconnected
layer to project our features into 768 dimensions, making
themcompatiblewithexistingcrossattentionlayers. Akey
difference between our multi-view cross attention and text
crossattentionisthat,inourmulti-viewattention,eachla-
tentpatchindependentlyattendstothecorrespondingpatch
inthefeaturefrustum.
B.AdditionalResultsandVisualizations
Ablating Classifier-free Guidance Scale. We
further conduct experiments to ablate the effect of
the classifier-free guidance scale. Proposed in [9],
classifier-free guidance controls the faithfulness of the
generated output to the conditional input. In our
1