[
    {
        "title": "OW-VISCap: Open-World Video Instance Segmentation and Captioning",
        "authors": "Anwesa ChoudhuriGirish ChowdharyAlexander G. Schwing",
        "links": "http://arxiv.org/abs/2404.03657v1",
        "entry_id": "http://arxiv.org/abs/2404.03657v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03657v1",
        "summary": "Open-world video instance segmentation is an important video understanding\ntask. Yet most methods either operate in a closed-world setting, require an\nadditional user-input, or use classic region-based proposals to identify never\nbefore seen objects. Further, these methods only assign a one-word label to\ndetected objects, and don't generate rich object-centric descriptions. They\nalso often suffer from highly overlapping predictions. To address these issues,\nwe propose Open-World Video Instance Segmentation and Captioning (OW-VISCap),\nan approach to jointly segment, track, and caption previously seen or unseen\nobjects in a video. For this, we introduce open-world object queries to\ndiscover never before seen objects without additional user-input. We generate\nrich and descriptive object-centric captions for each detected object via a\nmasked attention augmented LLM input. We introduce an inter-query contrastive\nloss to ensure that the object queries differ from one another. Our generalized\napproach matches or surpasses state-of-the-art on three tasks: open-world video\ninstance segmentation on the BURST dataset, dense video object captioning on\nthe VidSTG dataset, and closed-world video instance segmentation on the OVIS\ndataset.",
        "updated": "2024-04-04 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03657v1"
    },
    {
        "title": "CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching",
        "authors": "Dongzhi JiangGuanglu SongXiaoshi WuRenrui ZhangDazhong ShenZhuofan ZongYu LiuHongsheng Li",
        "links": "http://arxiv.org/abs/2404.03653v1",
        "entry_id": "http://arxiv.org/abs/2404.03653v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03653v1",
        "summary": "Diffusion models have demonstrated great success in the field of\ntext-to-image generation. However, alleviating the misalignment between the\ntext prompts and images is still challenging. The root reason behind the\nmisalignment has not been extensively investigated. We observe that the\nmisalignment is caused by inadequate token attention activation. We further\nattribute this phenomenon to the diffusion model's insufficient condition\nutilization, which is caused by its training paradigm. To address the issue, we\npropose CoMat, an end-to-end diffusion model fine-tuning strategy with an\nimage-to-text concept matching mechanism. We leverage an image captioning model\nto measure image-to-text alignment and guide the diffusion model to revisit\nignored tokens. A novel attribute concentration module is also proposed to\naddress the attribute binding problem. Without any image or human preference\ndata, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.\nExtensive experiments show that CoMat-SDXL significantly outperforms the\nbaseline model SDXL in two text-to-image alignment benchmarks and achieves\nstart-of-the-art performance.",
        "updated": "2024-04-04 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03653v1"
    },
    {
        "title": "Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra",
        "authors": "Darioush KevianUsman SyedXingang GuoAaron HavensGeir DullerudPeter SeilerLianhui QinBin Hu",
        "links": "http://arxiv.org/abs/2404.03647v1",
        "entry_id": "http://arxiv.org/abs/2404.03647v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03647v1",
        "summary": "In this paper, we explore the capabilities of state-of-the-art large language\nmodels (LLMs) such as GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra in solving\nundergraduate-level control problems. Controls provides an interesting case\nstudy for LLM reasoning due to its combination of mathematical theory and\nengineering design. We introduce ControlBench, a benchmark dataset tailored to\nreflect the breadth, depth, and complexity of classical control design. We use\nthis dataset to study and evaluate the problem-solving abilities of these LLMs\nin the context of control engineering. We present evaluations conducted by a\npanel of human experts, providing insights into the accuracy, reasoning, and\nexplanatory prowess of LLMs in control engineering. Our analysis reveals the\nstrengths and limitations of each LLM in the context of classical control, and\nour results imply that Claude 3 Opus has become the state-of-the-art LLM for\nsolving undergraduate control problems. Our study serves as an initial step\ntowards the broader goal of employing artificial general intelligence in\ncontrol engineering.",
        "updated": "2024-04-04 17:58:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03647v1"
    },
    {
        "title": "WorDepth: Variational Language Prior for Monocular Depth Estimation",
        "authors": "Ziyao ZengDaniel WangFengyu YangHyoungseob ParkYangchao WuStefano SoattoByung-Woo HongDong LaoAlex Wong",
        "links": "http://arxiv.org/abs/2404.03635v1",
        "entry_id": "http://arxiv.org/abs/2404.03635v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03635v1",
        "summary": "Three-dimensional (3D) reconstruction from a single image is an ill-posed\nproblem with inherent ambiguities, i.e. scale. Predicting a 3D scene from text\ndescription(s) is similarly ill-posed, i.e. spatial arrangements of objects\ndescribed. We investigate the question of whether two inherently ambiguous\nmodalities can be used in conjunction to produce metric-scaled reconstructions.\nTo test this, we focus on monocular depth estimation, the problem of predicting\na dense depth map from a single image, but with an additional text caption\ndescribing the scene. To this end, we begin by encoding the text caption as a\nmean and standard deviation; using a variational framework, we learn the\ndistribution of the plausible metric reconstructions of 3D scenes corresponding\nto the text captions as a prior. To \"select\" a specific reconstruction or depth\nmap, we encode the given image through a conditional sampler that samples from\nthe latent space of the variational text encoder, which is then decoded to the\noutput depth map. Our approach is trained alternatingly between the text and\nimage branches: in one optimization step, we predict the mean and standard\ndeviation from the text description and sample from a standard Gaussian, and in\nthe other, we sample using a (image) conditional sampler. Once trained, we\ndirectly predict depth from the encoded text using the conditional sampler. We\ndemonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where\nwe show that language can consistently improve performance in both.",
        "updated": "2024-04-04 17:54:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03635v1"
    },
    {
        "title": "Standardizing Knowledge Engineering Practices with a Reference Architecture",
        "authors": "Bradley P. AllenFilip Ilievski",
        "links": "http://arxiv.org/abs/2404.03624v1",
        "entry_id": "http://arxiv.org/abs/2404.03624v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03624v1",
        "summary": "Knowledge engineering is the process of creating and maintaining\nknowledge-producing systems. Throughout the history of computer science and AI,\nknowledge engineering workflows have been widely used given the importance of\nhigh-quality knowledge for reliable intelligent agents. Meanwhile, the scope of\nknowledge engineering, as apparent from its target tasks and use cases, has\nbeen shifting, together with its paradigms such as expert systems, semantic\nweb, and language modeling. The intended use cases and supported user\nrequirements between these paradigms have not been analyzed globally, as new\nparadigms often satisfy prior pain points while possibly introducing new ones.\nThe recent abstraction of systemic patterns into a boxology provides an opening\nfor aligning the requirements and use cases of knowledge engineering with the\nsystems, components, and software that can satisfy them best. This paper\nproposes a vision of harmonizing the best practices in the field of knowledge\nengineering by leveraging the software engineering methodology of creating\nreference architectures. We describe how a reference architecture can be\niteratively designed and implemented to associate user needs with recurring\nsystemic patterns, building on top of existing knowledge engineering workflows\nand boxologies. We provide a six-step roadmap that can enable the development\nof such an architecture, providing an initial design and outcome of the\ndefinition of architectural scope, selection of information sources, and\nanalysis. We expect that following through on this vision will lead to\nwell-grounded reference architectures for knowledge engineering, will advance\nthe ongoing initiatives of organizing the neurosymbolic knowledge engineering\nspace, and will build new links to the software architectures and data science\ncommunities.",
        "updated": "2024-04-04 17:46:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03624v1"
    }
]