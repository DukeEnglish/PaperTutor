[
    {
        "title": "Laser Learning Environment: A new environment for coordination-critical multi-agent tasks",
        "authors": "Yannick MolinghenRaphaël AvalosMark Van AchterAnn NowéTom Lenaerts",
        "links": "http://arxiv.org/abs/2404.03596v1",
        "entry_id": "http://arxiv.org/abs/2404.03596v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03596v1",
        "summary": "We introduce the Laser Learning Environment (LLE), a collaborative\nmulti-agent reinforcement learning environment in which coordination is\ncentral. In LLE, agents depend on each other to make progress\n(interdependence), must jointly take specific sequences of actions to succeed\n(perfect coordination), and accomplishing those joint actions does not yield\nany intermediate reward (zero-incentive dynamics). The challenge of such\nproblems lies in the difficulty of escaping state space bottlenecks caused by\ninterdependence steps since escaping those bottlenecks is not rewarded. We test\nmultiple state-of-the-art value-based MARL algorithms against LLE and show that\nthey consistently fail at the collaborative task because of their inability to\nescape state space bottlenecks, even though they successfully achieve perfect\ncoordination. We show that Q-learning extensions such as prioritized experience\nreplay and n-steps return hinder exploration in environments with\nzero-incentive dynamics, and find that intrinsic curiosity with random network\ndistillation is not sufficient to escape those bottlenecks. We demonstrate the\nneed for novel methods to solve this problem and the relevance of LLE as\ncooperative MARL benchmark.",
        "updated": "2024-04-04 17:05:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03596v1"
    },
    {
        "title": "No Panacea in Planning: Algorithm Selection for Suboptimal Multi-Agent Path Finding",
        "authors": "Weizhe ChenZhihan WangJiaoyang LiSven KoenigBistra Dilkina",
        "links": "http://arxiv.org/abs/2404.03554v1",
        "entry_id": "http://arxiv.org/abs/2404.03554v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03554v1",
        "summary": "Since more and more algorithms are proposed for multi-agent path finding\n(MAPF) and each of them has its strengths, choosing the correct one for a\nspecific scenario that fulfills some specified requirements is an important\ntask. Previous research in algorithm selection for MAPF built a standard\nworkflow and showed that machine learning can help. In this paper, we study\ngeneral solvers for MAPF, which further include suboptimal algorithms. We\npropose different groups of optimization objectives and learning tasks to\nhandle the new tradeoff between runtime and solution quality. We conduct\nextensive experiments to show that the same loss can not be used for different\ngroups of optimization objectives, and that standard computer vision models are\nno worse than customized architecture. We also provide insightful discussions\non how feature-sensitive pre-processing is needed for learning for MAPF, and\nhow different learning metrics are correlated to different learning tasks.",
        "updated": "2024-04-04 16:06:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03554v1"
    },
    {
        "title": "MEDIATE: Mutually Endorsed Distributed Incentive Acknowledgment Token Exchange",
        "authors": "Philipp AltmannKatharina WinterMichael KölleMaximilian ZornThomy PhanClaudia Linnhoff-Popien",
        "links": "http://arxiv.org/abs/2404.03431v1",
        "entry_id": "http://arxiv.org/abs/2404.03431v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03431v1",
        "summary": "Recent advances in multi-agent systems (MAS) have shown that incorporating\npeer incentivization (PI) mechanisms vastly improves cooperation. Especially in\nsocial dilemmas, communication between the agents helps to overcome sub-optimal\nNash equilibria. However, incentivization tokens need to be carefully selected.\nFurthermore, real-world applications might yield increased privacy requirements\nand limited exchange. Therefore, we extend the PI protocol for mutual\nacknowledgment token exchange (MATE) and provide additional analysis on the\nimpact of the chosen tokens. Building upon those insights, we propose mutually\nendorsed distributed incentive acknowledgment token exchange (MEDIATE), an\nextended PI architecture employing automatic token derivation via decentralized\nconsensus. Empirical results show the stable agreement on appropriate tokens\nyielding superior performance compared to static tokens and state-of-the-art\napproaches in different social dilemma environments with various reward\ndistributions.",
        "updated": "2024-04-04 13:24:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03431v1"
    },
    {
        "title": "MARL-LNS: Cooperative Multi-agent Reinforcement Learning via Large Neighborhoods Search",
        "authors": "Weizhe ChenSven KoenigBistra Dilkina",
        "links": "http://arxiv.org/abs/2404.03101v1",
        "entry_id": "http://arxiv.org/abs/2404.03101v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03101v1",
        "summary": "Cooperative multi-agent reinforcement learning (MARL) has been an\nincreasingly important research topic in the last half-decade because of its\ngreat potential for real-world applications. Because of the curse of\ndimensionality, the popular \"centralized training decentralized execution\"\nframework requires a long time in training, yet still cannot converge\nefficiently. In this paper, we propose a general training framework, MARL-LNS,\nto algorithmically address these issues by training on alternating subsets of\nagents using existing deep MARL algorithms as low-level trainers, while not\ninvolving any additional parameters to be trained. Based on this framework, we\nprovide three algorithm variants based on the framework: random large\nneighborhood search (RLNS), batch large neighborhood search (BLNS), and\nadaptive large neighborhood search (ALNS), which alternate the subsets of\nagents differently. We test our algorithms on both the StarCraft Multi-Agent\nChallenge and Google Research Football, showing that our algorithms can\nautomatically reduce at least 10% of training time while reaching the same\nfinal skill level as the original algorithm.",
        "updated": "2024-04-03 22:51:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03101v1"
    },
    {
        "title": "Traffic Divergence Theory: An Analysis Formalism for Dynamic Networks",
        "authors": "Matin MacktoobianZhan ShuQing Zhao",
        "links": "http://dx.doi.org/10.1109/ACCESS.2024.3383436",
        "entry_id": "http://arxiv.org/abs/2404.03066v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03066v1",
        "summary": "Traffic dynamics is universally crucial in analyzing and designing almost any\nnetwork. This article introduces a novel theoretical approach to analyzing\nnetwork traffic dynamics. This theory's machinery is based on the notion of\ntraffic divergence, which captures the flow (im)balance of network nodes and\nlinks. It features various analytical probes to investigate both spatial and\ntemporal traffic dynamics. In particular, the maximal traffic distribution in a\nnetwork can be characterized by spatial traffic divergence rate, which reveals\nthe relative difference among node traffic divergence. To illustrate the\nusefulness, we apply the theory to two network-driven problems: throughput\nestimation of data center networks and power-optimized communication planning\nfor robot networks, and show the merits of the proposed theory through\nsimulations.",
        "updated": "2024-04-03 21:13:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03066v1"
    }
]