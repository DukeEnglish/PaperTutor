[
    {
        "title": "CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching",
        "authors": "Dongzhi JiangGuanglu SongXiaoshi WuRenrui ZhangDazhong ShenZhuofan ZongYu LiuHongsheng Li",
        "links": "http://arxiv.org/abs/2404.03653v1",
        "entry_id": "http://arxiv.org/abs/2404.03653v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03653v1",
        "summary": "Diffusion models have demonstrated great success in the field of\ntext-to-image generation. However, alleviating the misalignment between the\ntext prompts and images is still challenging. The root reason behind the\nmisalignment has not been extensively investigated. We observe that the\nmisalignment is caused by inadequate token attention activation. We further\nattribute this phenomenon to the diffusion model's insufficient condition\nutilization, which is caused by its training paradigm. To address the issue, we\npropose CoMat, an end-to-end diffusion model fine-tuning strategy with an\nimage-to-text concept matching mechanism. We leverage an image captioning model\nto measure image-to-text alignment and guide the diffusion model to revisit\nignored tokens. A novel attribute concentration module is also proposed to\naddress the attribute binding problem. Without any image or human preference\ndata, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.\nExtensive experiments show that CoMat-SDXL significantly outperforms the\nbaseline model SDXL in two text-to-image alignment benchmarks and achieves\nstart-of-the-art performance.",
        "updated": "2024-04-04 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03653v1"
    },
    {
        "title": "AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent",
        "authors": "Hanyu LaiXiao LiuIat Long IongShuntian YaoYuxuan ChenPengbo ShenHao YuHanchen ZhangXiaohan ZhangYuxiao DongJie Tang",
        "links": "http://arxiv.org/abs/2404.03648v1",
        "entry_id": "http://arxiv.org/abs/2404.03648v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03648v1",
        "summary": "Large language models (LLMs) have fueled many intelligent agent tasks, such\nas web navigation -- but most existing agents perform far from satisfying in\nreal-world webpages due to three factors: (1) the versatility of actions on\nwebpages, (2) HTML text exceeding model processing capacity, and (3) the\ncomplexity of decision-making due to the open-domain nature of web. In light of\nthe challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web\nnavigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns,\nwe design an HTML simplification algorithm to represent webpages, preserving\nvital information succinctly. We employ a hybrid human-AI method to build web\nbrowsing data for curriculum training. Then, we bootstrap the model by\nreinforcement learning and rejection sampling to further facilitate webpage\ncomprehension, browser operations, and efficient task decomposition by itself.\nFor testing, we establish a bilingual benchmark -- AutoWebBench -- for\nreal-world web browsing tasks. We evaluate AutoWebGLM across diverse web\nnavigation benchmarks, revealing its improvements but also underlying\nchallenges to tackle real environments. Related code, model, and data will be\nreleased at \\url{https://github.com/THUDM/AutoWebGLM}.",
        "updated": "2024-04-04 17:58:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03648v1"
    },
    {
        "title": "Locating and Editing Factual Associations in Mamba",
        "authors": "Arnab Sen SharmaDavid AtkinsonDavid Bau",
        "links": "http://arxiv.org/abs/2404.03646v1",
        "entry_id": "http://arxiv.org/abs/2404.03646v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03646v1",
        "summary": "We investigate the mechanisms of factual recall in the Mamba state space\nmodel. Our work is inspired by previous findings in autoregressive transformer\nlanguage models suggesting that their knowledge recall is localized to\nparticular modules at specific token locations; we therefore ask whether\nfactual recall in Mamba can be similarly localized. To investigate this, we\nconduct four lines of experiments on Mamba. First, we apply causal tracing or\ninterchange interventions to localize key components inside Mamba that are\nresponsible for recalling facts, revealing that specific components within\nmiddle layers show strong causal effects at the last token of the subject,\nwhile the causal effect of intervening on later layers is most pronounced at\nthe last token of the prompt, matching previous findings on autoregressive\ntransformers. Second, we show that rank-one model editing methods can\nsuccessfully insert facts at specific locations, again resembling findings on\ntransformer models. Third, we examine the linearity of Mamba's representations\nof factual relations. Finally we adapt attention-knockout techniques to Mamba\nto dissect information flow during factual recall. We compare Mamba directly to\na similar-sized transformer and conclude that despite significant differences\nin architectural approach, when it comes to factual recall, the two\narchitectures share many similarities.",
        "updated": "2024-04-04 17:58:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03646v1"
    },
    {
        "title": "WorDepth: Variational Language Prior for Monocular Depth Estimation",
        "authors": "Ziyao ZengDaniel WangFengyu YangHyoungseob ParkYangchao WuStefano SoattoByung-Woo HongDong LaoAlex Wong",
        "links": "http://arxiv.org/abs/2404.03635v1",
        "entry_id": "http://arxiv.org/abs/2404.03635v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03635v1",
        "summary": "Three-dimensional (3D) reconstruction from a single image is an ill-posed\nproblem with inherent ambiguities, i.e. scale. Predicting a 3D scene from text\ndescription(s) is similarly ill-posed, i.e. spatial arrangements of objects\ndescribed. We investigate the question of whether two inherently ambiguous\nmodalities can be used in conjunction to produce metric-scaled reconstructions.\nTo test this, we focus on monocular depth estimation, the problem of predicting\na dense depth map from a single image, but with an additional text caption\ndescribing the scene. To this end, we begin by encoding the text caption as a\nmean and standard deviation; using a variational framework, we learn the\ndistribution of the plausible metric reconstructions of 3D scenes corresponding\nto the text captions as a prior. To \"select\" a specific reconstruction or depth\nmap, we encode the given image through a conditional sampler that samples from\nthe latent space of the variational text encoder, which is then decoded to the\noutput depth map. Our approach is trained alternatingly between the text and\nimage branches: in one optimization step, we predict the mean and standard\ndeviation from the text description and sample from a standard Gaussian, and in\nthe other, we sample using a (image) conditional sampler. Once trained, we\ndirectly predict depth from the encoded text using the conditional sampler. We\ndemonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where\nwe show that language can consistently improve performance in both.",
        "updated": "2024-04-04 17:54:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03635v1"
    },
    {
        "title": "Training LLMs over Neurally Compressed Text",
        "authors": "Brian LesterJaehoon LeeAlex AlemiJeffrey PenningtonAdam RobertsJascha Sohl-DicksteinNoah Constant",
        "links": "http://arxiv.org/abs/2404.03626v1",
        "entry_id": "http://arxiv.org/abs/2404.03626v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03626v1",
        "summary": "In this paper, we explore the idea of training large language models (LLMs)\nover highly compressed text. While standard subword tokenizers compress text by\na small factor, neural text compressors can achieve much higher rates of\ncompression. If it were possible to train LLMs directly over neurally\ncompressed text, this would confer advantages in training and serving\nefficiency, as well as easier handling of long text spans. The main obstacle to\nthis goal is that strong compression tends to produce opaque outputs that are\nnot well-suited for learning. In particular, we find that text na\\\"ively\ncompressed via Arithmetic Coding is not readily learnable by LLMs. To overcome\nthis, we propose Equal-Info Windows, a novel compression technique whereby text\nis segmented into blocks that each compress to the same bit length. Using this\nmethod, we demonstrate effective learning over neurally compressed text that\nimproves with scale, and outperforms byte-level baselines by a wide margin on\nperplexity and inference speed benchmarks. While our method delivers worse\nperplexity than subword tokenizers for models trained with the same parameter\ncount, it has the benefit of shorter sequence lengths. Shorter sequence lengths\nrequire fewer autoregressive generation steps, and reduce latency. Finally, we\nprovide extensive analysis of the properties that contribute to learnability,\nand offer concrete suggestions for how to further improve the performance of\nhigh-compression tokenizers.",
        "updated": "2024-04-04 17:48:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03626v1"
    }
]