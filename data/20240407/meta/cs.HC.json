[
    {
        "title": "Creator Hearts: Investigating the Impact Positive Signals from YouTube Creators in Shaping Comment Section Behavior",
        "authors": "Frederick ChoiCharlotte LambertVinay KoshySowmya PratipatiTue DoEshwar Chandrasekharan",
        "links": "http://arxiv.org/abs/2404.03612v1",
        "entry_id": "http://arxiv.org/abs/2404.03612v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03612v1",
        "summary": "Much of the research in online moderation focuses on punitive actions.\nHowever, emerging research has shown that positive reinforcement is effective\nat encouraging desirable behavior on online platforms. We extend this research\nby studying the \"creator heart\" feature on YouTube, quantifying their primary\neffects on comments that receive hearts and on videos where hearts have been\ngiven. We find that creator hearts increased the visibility of comments, and\nincreased the amount of positive engagement they received from other users. We\nalso find that the presence of a creator hearted comment soon after a video is\npublished can incentivize viewers to comment, increasing the total engagement\nwith the video over time. We discuss the potential for creators to use hearts\nto shape behavior in their communities by highlighting, rewarding, and\nincentivizing desirable behaviors from users. We discuss avenues for extending\nour study to understanding positive signals from moderators on other platforms.",
        "updated": "2024-04-04 17:34:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03612v1"
    },
    {
        "title": "Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work",
        "authors": "Somin ParkCarol C. MenassaVineet R. Kamat",
        "links": "http://arxiv.org/abs/2404.03498v1",
        "entry_id": "http://arxiv.org/abs/2404.03498v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03498v1",
        "summary": "In the construction industry, where work environments are complex,\nunstructured and often dangerous, the implementation of Human-Robot\nCollaboration (HRC) is emerging as a promising advancement. This underlines the\ncritical need for intuitive communication interfaces that enable construction\nworkers to collaborate seamlessly with robotic assistants. This study\nintroduces a conversational Virtual Reality (VR) interface integrating\nmultimodal interaction to enhance intuitive communication between construction\nworkers and robots. By integrating voice and controller inputs with the Robot\nOperating System (ROS), Building Information Modeling (BIM), and a game engine\nfeaturing a chat interface powered by a Large Language Model (LLM), the\nproposed system enables intuitive and precise interaction within a VR setting.\nEvaluated by twelve construction workers through a drywall installation case\nstudy, the proposed system demonstrated its low workload and high usability\nwith succinct command inputs. The proposed multimodal interaction system\nsuggests that such technological integration can substantially advance the\nintegration of robotic assistants in the construction industry.",
        "updated": "2024-04-04 14:56:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03498v1"
    },
    {
        "title": "Agora Elevator Bodily Sensation Study -- a report",
        "authors": "Rebekah Rousi",
        "links": "http://arxiv.org/abs/2404.03356v1",
        "entry_id": "http://arxiv.org/abs/2404.03356v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03356v1",
        "summary": "This study set out to examine the relationship between expressed social\nemotions (i.e. that what people say they are feeling) and physical sensations,\nthe connection between emotion and bodily experience. It additionally provided\nthe opportunity to investigate how the neurological findings of gender\ndifferences can be observed in practice, what difference does it make in\nbehaviour and judgment that we have varying levels of mirror neuron activity?\nThe following report documents the study, procedure, results and findings.",
        "updated": "2024-04-04 10:50:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03356v1"
    },
    {
        "title": "Influence of Gameplay Duration, Hand Tracking, and Controller Based Control Methods on UX in VR",
        "authors": "Tanja KojićMaurizio VergariSimon KnuthMaximilian WarsinkeSebastian MöllerJan-Niklas Voigt-Antons",
        "links": "http://dx.doi.org/10.1145/3652212.3652222",
        "entry_id": "http://arxiv.org/abs/2404.03337v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03337v1",
        "summary": "Inside-out tracking is growing popular in consumer VR, enhancing\naccessibility. It uses HMD camera data and neural networks for effective hand\ntracking. However, limited user experience studies have compared this method to\ntraditional controllers, with no consensus on the optimal control technique.\nThis paper investigates the impact of control methods and gaming duration on VR\nuser experience, hypothesizing hand tracking might be preferred for short\nsessions and by users new to VR due to its simplicity. Through a lab study with\ntwenty participants, evaluating presence, emotional response, UX quality, and\nflow, findings revealed control type and session length affect user experience\nwithout significant interaction. Controllers were generally superior,\nattributed to their reliability, and longer sessions increased presence and\nrealism. The study found that individuals with more VR experience were more\ninclined to recommend hand tracking to others, which contradicted predictions.",
        "updated": "2024-04-04 10:06:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03337v1"
    },
    {
        "title": "Exploring Emotions in Multi-componential Space using Interactive VR Games",
        "authors": "Rukshani SomarathnaGelareh Mohammadi",
        "links": "http://arxiv.org/abs/2404.03239v1",
        "entry_id": "http://arxiv.org/abs/2404.03239v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03239v1",
        "summary": "Emotion understanding is a complex process that involves multiple components.\nThe ability to recognise emotions not only leads to new context awareness\nmethods but also enhances system interaction's effectiveness by perceiving and\nexpressing emotions. Despite the attention to discrete and dimensional models,\nneuroscientific evidence supports those emotions as being complex and\nmulti-faceted. One framework that resonated well with such findings is the\nComponent Process Model (CPM), a theory that considers the complexity of\nemotions with five interconnected components: appraisal, expression,\nmotivation, physiology and feeling. However, the relationship between CPM and\ndiscrete emotions has not yet been fully explored. Therefore, to better\nunderstand emotions underlying processes, we operationalised a data-driven\napproach using interactive Virtual Reality (VR) games and collected multimodal\nmeasures (self-reports, physiological and facial signals) from 39 participants.\nWe used Machine Learning (ML) methods to identify the unique contributions of\neach component to emotion differentiation. Our results showed the role of\ndifferent components in emotion differentiation, with the model including all\ncomponents demonstrating the most significant contribution. Moreover, we found\nthat at least five dimensions are needed to represent the variation of emotions\nin our dataset. These findings also have implications for using VR environments\nin emotion research and highlight the role of physiological signals in emotion\nrecognition within such environments.",
        "updated": "2024-04-04 06:54:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03239v1"
    }
]