FUNCTION-SPACE MCMC FOR BAYESIAN WIDE NEURAL
NETWORKS
APREPRINT
LuciaPezzetti StefanoFavaro
ETHAICenter UniversityofTorino
lucia.pezzetti@inf.ethz.ch andCollegioCarloAlberto
stefano.favaro@unito.it
StefanoPelucchetti
CogentLabs
spelucchetti@cogent.co.jp
August27,2024
ABSTRACT
Bayesian Neural Networks represent a fascinating confluence of deep learning and probabilistic
reasoning, offeringacompellingframeworkforunderstandinguncertaintyincomplexpredictive
models. Inthispaper,weinvestigatetheuseofthepreconditionedCrank-Nicolsonalgorithmandits
Langevinversiontosamplefromthereparametrisedposteriordistributionoftheweightsasthewidths
ofBayesianNeuralNetworksgrowlarger. Inadditiontobeingrobustintheinfinite-dimensional
setting,weprovethattheacceptanceprobabilitiesoftheproposedmethodsapproach1asthewidthof
thenetworkincreases,independentlyofanystepsizetuning. Moreover,weexamineandcomparehow
themixingspeedsoftheunderdampedLangevinMonteCarlo,thepreconditionedCrank-Nicolson
andthepreconditionedCrank-NicolsonLangevinsamplersareinfluencedbychangesinthenetwork
width in some real-world cases. Our findings suggest that, in wide Bayesian Neural Networks
configurations,thepreconditionedCrank-Nicolsonmethodallowsformoreefficientsamplingofthe
reparametrisedposteriordistribution,asevidencedbyahighereffectivesamplesizeandimproved
diagnosticresultscomparedwiththeotheranalysedalgorithms.
1 Introduction
NeuralNetworks(NNs)havebecomeverypopularintherecentyears,duetothesignificantresultsachievedinvarious
artificialintelligencetasksfromspeechrecognitionandimageclassification,tostockmarketprediction,healthcare
andweatherforecasting. Despitethiswidespreadapplicabilityandindisputablesuccess,NNsarehinderedbyintrinsic
shortcomings. ThelargenumberofparametersthatmakesNNspowerfulfunctionapproximations,isalsothereason
whyNNsarepronetooverfittingandoverconfidenceintheirpredictions. Moreover, theblack-boxnatureofNNs
makes interpretability hard. Bayesian Neural Networks (BNNs) address some of these limitations by providing a
principledwaytoincorporateuncertaintyintocomplexpredictivemodels. AtitsheartliestheframeworkofBayesian
Inference,specifically,theweightsorparametersofthenetworkareconsideredtoberandomvariables,insteadof
deterministicquantities,whosepriorbeliefsareupdatedasdataaregatheredaccordingtotheBayesianlearningrule
[1,2,3,4,5,6,7].
Despiteallthesepromisingadvantagesandsomerecentprogress,mainlyunderGaussianinitializationsoftheweights,
BNNshavereachedfarlesspopularitythanNNs,duetohighercomputationalrequirementsandlimitedtheoretical
understanding. Oneofthemajortheoreticalchallengesconcernsthecomprehensionoftheparameter-spacebehavior
ofBNNs,inspiteofthefunctionone. WethereforeexploresamplingfromtheposteriordistriutionofwideBNNs,
focusing on understanding its behavior and properties from a parameter-space perspective. In function space, in
fact, under Gaussian initializations of the network’s weights, the distribution over the functions induced by wide
4202
guA
62
]GL.sc[
1v52341.8042:viXraarXivTemplate APREPRINT
Figure1: Comparisonoftheacceptanceprobabilityobtainedusing: i)theunderdampedLMCalgorithm(orMetropolis
Adjusted Langevin Algorithm: MALA); ii) the pCN algorithm; iii) the pCNL method. The neural network archi-
tecture used is a fully-connected with one hidden layer, and the layer width varies among the following values:
128,512,1024,2096,4192. TheCIFAR-10datasetisused,withthesamplesizefixedatn=256. Thestepsizeβ of
theproposalstepforallthemethodsisfixedat0.1. TheacceptancerateofthepCN(orangeline)increasessteadily
asthewidthoftheBNNgrows,suggestingimprovedperformanceinwideBNNsandhighlightingitsdimensional
independenceproperties. Incontrast,theLMC(blueline)initiallyshowsarapidaccelerationinperformanceduetothe
effectofreparametrization. Asthewidthofthenetworkincreases,thesamplingdistributionbecomesmoresimilartoan
isotropicGaussiandistribution,whichmakesitbetter-behaved. However,asthewidthincreasesfurther,thesampler’s
non-robustnessinhigh-dimensionalsettingsleadstoaprogressivedeteriorationinperformance. Thisconfirmstheneed
todecreasethestepsizeinordertomaintainaconstantacceptancerate,whichintroducesfurthercorrelationsbetween
thesamples. ThepCNLmethod(greenline)showsaconsistentacceptancerateacrossalldimensions,indicatinga
robustnesstothedimensionalscalingoftheBNN.ThisstabilitymaysuggestthatthepCNLalgorithmeffectively
combinesthestrengthsofthepCNandLMCmethods.
BNNshasbeenprovedtoconvergetotheneuralnetworkGaussianprocess(NNGP)limit[8,9,10,11,12,13,14].
Recently, [15] provided a counterpart of this result in the parameter space. Specifically, they proved that, after a
reparametrizationϕ=T−1(θ)oftheflattenedNNweightsθ ∈Rd,theposteriordistributionp(ϕ|D)convergesinthe
KL-divergencetothestandardGaussiandistributionN(0,I )asthewidthoftheBNNincreases. Thisresultnotonly
d
givesacharacterizationofthewide-limitparameterdistribution,buttheclosenesstothewell-behavedstandardnormal
distributionalsosuggeststhatanimprovementinthemixingtimeofMarkovchainMonteCarlo(MCMC)sampling
proceduresispossible. However,standardMCMCproceduresarenotablynon-robustintheinfinite-dimensionalsetting,
sincetheirproposalstepneedstoapproachzerofortheacceptanceratetoremainconstant,asthedimensionofthe
spaceincreases.
Inthispaper,weinvestigateandprovidetheoreticalguaranteesfortheuseofalgorithmsthatarestableundermesh
refinement,namelythepreconditionedCrank-Nicolson(pCN)andCrank-NicolsonLangevin(pCNL),tosamplefrom
thereparametrisedposteriordistributionoftheweightsasthewidthsofBNNsgrowlarger. Weshowthattheacceptance
probabilityofbothsamplers, underreparametrizationT, convergesto1asthewidthoftheBNNlayersincreases,
independentlyofthechosenfixedstepsize. Thisresultpavesthewayforafasterandmoreefficientsamplingprocedure
inwideBNNs. Specifically,notonlydothepCNandpCNLproposalsavoidsufferingfromthecurseofdimensionality,
2arXivTemplate APREPRINT
but the performance of the samplers intrinsically improves as the BNN becomes larger, without introducing any
additionalautocorrelationamongthesamplers. Thislessenstheneedforlongerburn-inperiodsandmoresamplesfor
adequateconvergencediagnostics,whichaffectsstandardMCMCmethods.
ThetheoreticalguaranteesfortheuseofthepCNandpCNLproposalsinwideBNNconfigurationsareempirically
supportedbyahighereffectivesamplesizeandimproveddiagnosticresultscomparedwiththeLMCalgorithm. Froma
computationalpointofview,thepCNprovidesmorescalabilitywithrespecttoderivative-basedMCMCalgorithms
and can scale well to large datasets and complex model. For this reason, the pCN method seems to effectively
combineasmallcomputationalcostwithasignificantimprovementinthequalityofthecollectedsamples,wherease
the performancce of the pCNL procedure is not striking enough to justify its high computational cost. Notably,
improvementsinperformanceincreasewiththewidthsizebutareevidentevenfarfromtheNNGPregime.
Figure 1 shows the potential of our approach. The acceptance probabilities of the underdamped Langevin Monte
Carlo(LMC)[16]thepCNandthepCNL,appliedtotheposteriorofthereparametrizedweights,areplottedfora
fully-connectednetwork(FCN)with1hiddenlayer. Inthesamplersafixedstepsize,β =0.1,intheproposalstephas
beenused. AsthewidthoftheBNNgrowslarge,theacceptancerateofthepCNalgorithmsteadilyincreases,whilst
theoneofLMCdecreasesaswegoinhigherdimension. ThepCNLmethod,instead,showsaconsistentacceptance
probabilityacrossthedimensions. Worth-notingistheinitialdramaticincreaseoftheLMCacceptancerate,dueto
thereparametrizationeffectthatbringtheposteriorclosetothewell-behavedstandardGaussiandistribution. Further
investigationswillbeprovidedinSection4.1.
2 Method
2.1 Notationsandpreliminaries
LetadatasetD = (x ,y ),...,(x ,y ),wherex ∈ Rm andy ∈ R. Letparametersθbelongtosomeparameter
1 1 n n i i
spaceΘ ⊂ RD,forsomeD ≥ 1. Thepriordistributionp(θ)representsourbeliefspriortoobservingthedata,the
beliefsarethenupdatedaccordinglytoBayes’ruleasdataaregathered:
p(θ)p(y|θ,x)
p(θ|D)= ,
p(y|x)
wherep(y|θ,x)isthelikelihoodfunction.
ABNNisaparametricfunctionf :=f thattakesaninputxandproducesanoutputy. Toclarifythenotation,we
θ
explicitlydefinetherecursionforafullyconnectedL-hiddenlayernetworkwithalinearreadoutlayer,i.e.,
σ(1)
f(0)(x)= √W xW(1)+σ(1)1(0)b(1)
d(0) b
g(l)(x)=ψ(f(l)(x)), l=0,...,L, (1)
and
σ(l+1)
f(l+1)(x)= √W g(l)(x)W(l+1)+σ(l+1)1lb(l+1), l=0,...,L
d(l) b
Thus,y=fL+1(x)istheoutputofthenetworkgiventheinputx. The(non-linear)activationfunctionisdenotedbyψ,
d(l)denotesthewidthofthelthlayer(withd(0)beingtheinputsizem)andW(l) ∈Rd(l−1)×d(l) isitsweightmatrix.
Withoutlossofgenerality,inthefollowingwewillnotmakeexplicitthepresenceofthebiasb(l) ∈Rd(l),thoughwe
includeitintheweightmatrixbyaddingaconstantdimensiontotheoutputsofthepreviouslayersotosimplifythe
√
notation. Thefactorσl+1/ dlistobeattributedtotheNTKparametrization[17],anditallowstoconsideranisotropic
W
Gaussianpriorfortheweights,removingitsusualdependenceonthewidth. Precisely,wedenote
θ(l) ∈Rd(l) , l=1,...,L+1, θ(l) ∼N(0,I )
d(l)
theflattenedweightsofthelth layer,biasincluded,assumedtobe,apriori,independentstandardGaussianrandom
variables. Withθ ∈RD weconsideralltheflattenedweightsθ =[θ(l)] ∼N(0,I ). WeassumeaGaussian
l=0,...,L D
likelihoodp(y|θ,x)∼N(f(L+1)(x),σ2I )fortheobservationvarianceσ2 >0. Moreover,weassumethat|ψ(x)|is
D
boundedbyafunctionoftheformexp(C∥x∥2−ϵ−c),forsomeC,c,ϵ>0;wereferto[12]forsimilarassumptions
ontheactivationfunction.
3arXivTemplate APREPRINT
Weareinterestedintheoreticalguaranteesforsamplingfromtheposteriordistributionoftheweightsasthewidth
grows. [15]proposedthefollowingdata-dependentreparametrizationofthereadoutlayerweightsofBNNs:
 Σ−1/2(cid:0) θ(l)−µ(cid:1)
l=L+1

ϕ(l) = (2)
θ(l) otherwise
where,definingΨ= σ √W(L+1) g(L)(x),wehave
d(L)
Σ=(cid:0) I +σ−2ΨTΨ(cid:1)−1 , µ=σ−2ΣΨTy. (3)
dL
Thereparametrizedposteriordistribution,whosedensityfunctionisdenotedbyp(ϕ|D),isshowntoconvergeinthe
KL-divergencetoastandardGaussianN(0,I )asthewidthgoestoinfinity. Inparticular,theconvergencetoasimple
D
isotropicGaussiansuggestspotentialimprovementsinthemixingspeedofMCMCprocedures,comparedtosampling
from the notably arduous BNN posterior. Nevertheless, standard MCMC algorithms, such as the Random Walk
Metropolis-HastingsAlgorithm(RW-MH)andtheMetropolisAdjustedLangevinAlgorithm(LMC),arenotoriously
ill-suited for the infinite-dimensional setting and must be carefully re-tuned as the dimension increases to avoid
degeneracyintheacceptanceprobability. Indeed,theoptimalscalingpropertiesoftheRW-MHandLMCalgorithms
havebeenderivedin[18]and[19],respectively. There,itisshownthattheproposalmovementsneedtovanishasthe
dimensionoftheparameterspacegrowsunboundedforfactorialtargetdistributions. Consequently,theefficiencyofthe
RW-MHandLMCalgorithmsvanishesaswell.
2.2 Function-SpaceMCMC
Since our interest lies in analyzing the behavior of wide networks, we need methods that are robust in the infinite
dimensionalsetting. AclassofsuchMCMCalgorithmshavebeenearlyderivedanddiscussedin[20]. Inthissection,
wefocusonthepCNandpCNLmethodsandbrieflycontrastthemwiththeirtraditionalcounterparts: theRW-MHand
theLMCalgorithms. TheslightmodificationsintroducedinpCNandpCNLmadethemrobustandwell-definedin
highdimensionalsettings,contrastingthewellknowndegeneracyofthestandardMCMCacceptanceprobabilityasthe
dimensionincreases.
RH-MCemploysasymmetricproposal,typicallyGaussian,centeredatthecurrentstatetoexploretheparameterspace,
i.e.,
v =u+βw, w ∼N(0,C), β ∈(0,1]
Instead,thepCNalgorithmreliesonthefollowingmodificationofthe(standard)randomwalkproposal:
(cid:112)
v = 1−β2u+βw, w ∼N(0,C), β ∈(0,1] (4)
whereCisthecovarianceoperatorofthepriorGaussianmeasure. See[20]andreferencesthereinfordetailsonRH-MC
andpCNproposals.
The pCN algorithm is reversible with respect to an arbitrary Gaussian measure, and hence admits such Gaussian
measureasinvariantdistribution. Specifically,inourcontext,wesettheinvariantGaussianmeasuretobeequaltoan
isotropicGaussiandistribution(theprior). Theacceptanceprobabilityreducesto:
a(v|u)=min{1,exp(−ℓ(u)+ℓ(v))}. (5)
whereℓisthelog-likelihood. ThestepsofthepCNproceduresarelistedinAlgorithm1.
Despitebeingagnosticaboutwhichpartsofthestatespacearemoreprobable,thepCNproposalis,byconstruction,
well-definedintheinfinite-dimensionalsetting. Thismotivatedustostudytheproceduretosamplefromtheabove
reparametrizedweightsposteriordistribution. Inourcontext,theproposal4is
(cid:112) (cid:112)
ϕ∗ = 1−β2ϕ+βw w ∼N(0,I ) =⇒ q(ϕ∗|ϕ)=N( 1−β2ϕ,β2I ),
D D
where β ∈ [0,1). Now, by including this expression in the acceptance probability 5, we can prove the following
theorem.
Theorem2.1 ConsidertheBNNmodelwiththereparametrisation2. Then,theacceptanceprobabilityofthepCN
algorithmtosamplefromthereparametrisedweightposterior,foranyβ ∈[0,1),convergesto1asthewidthofthe
networkincreases. Ifd isthesmallestamongthenetwork’slayerwidths,then
min
(cid:26) p(ϕ∗|D)q(ϕ|ϕ∗)(cid:27)
a(ϕ∗|ϕ)=min 1, →1 as d →∞
p(ϕ|D)q(ϕ∗|ϕ) min
4arXivTemplate APREPRINT
Algorithm1PreconditionedCrank-Nicolson(pCN)Algorithm
1: Initializeu(0),setnumberofiterationsN,chooseβ ∈(0,1]
2: forn=0toN −1do
(cid:112)
3: Proposev = 1−β2u(n)+βN(0,C)
4: Calculateacceptanceprobabilitya(v|u(n))=min{1,exp(−ℓ(u(n))+ℓ(v))}
5: Drawη ∼Uniform(0,1)
6: ifη ≤a(v|u(n))then
7: Setu(n+1) =v
8: else
9: Setu(n+1) =u(n)
10: endif
11: endfor
SeeAppendixAfortheproofofTheorem2.1.
TheMALAalgorithmincorporatesgradientinformationfromthelog-posteriordistributionintheproposalmechanism
to guide the sampling process. More precisely, this method involves simulating Langevin diffusion such that the
solutiontothetimeevolutionequationisastationarydistributionthatequalsthetargetdensity(inBayesiancontexts,
theposteriordistribution). Thisapproachisparticularlyeffectiveinexploringcomplex,high-dimensionalprobability
distributions by making proposals that are more informed and therefore likely to be accepted. The mathematical
expressionoftheproposalis:
ϵ2
v =u+ ∇logp(u|D)+ϵw, w ∼N(0,I)
2
wherevisthecurrentpositionofthealgorithmintheparameterspace,andϵisthestepsize,whichisatuningparameter
thatallowstocontrolthescaleoftheupdates. Inparticular,∇logp(ϕ|D)isthegradientofthelog-posteriordensity,
whichprovidesthedirectiontowardshigherprobabilitydensities.
WhileMALAprovidesanefficientwaytoexploretheparameterspace, itcanstillstruggleinscenarioswherethe
parameter space is high-dimensional. This led to the development of the pCNL algorithm, which introduces little
modificationstobetterhandlethesechallenges.Itsproposalisderivedbydiscretizationofastochasticpartialdifferential
equation(SPDE)whichisinvariantfortargetmeasureandisgivenby:
1 (cid:104) √ (cid:105)
v = (2−δ)u+2δCDℓ(u)+ 8δw , w ∼N(0,C), δ ∈(0,2) (6)
2+δ
whereC isthecovarianceoperatoroftheGaussianpriormeasure,ℓisthelog-likelihood,andDℓdenotesitsFréchet
derivative. NotethattheparameterδisdirectlyrelatedtothepCNstepsizeβ bytherelationshipβ2 =8δ/(2+δ)2.
Throughoutthepaper,werefertothestepsizeasβ,andwekeepintoaccountthisrelationshipwhendealingwiththe
pCNLproposal. WhenthepCNLisusedasaproposalforaMHscheme,theacceptanceprobabilityisgivenby
a(v|u)=min{1,exp(ρ(u,v))−ρ(v,u)} (7)
,where
1 δ δ √
ρ(u,v)=−ℓ(u)− ⟨v−u,Dℓ(u)⟩− ⟨u+v,Dℓ(u)⟩+ ∥ CDℓ(u)∥2
2 4 4
ThesemodificationsensurethatpCNLretainstheefficiencyofMALAintermsofinformedproposalswhileenhancing
itsapplicabilityandrobustnessinmorechallenginghigh-dimensionalsettings. ThestepsofthepCNLproceduresare
presentedinAlgorithm2.
ItisthereforenaturaltoaskwhetheralsotheacceptanceprobabilityofthepCNLalgorithmconvergestooneasthe
networkwidthincreases.
FirstlyrecallthepCNLproposal6that,usingthenotationintroducedwiththereparametrisation2,canbewrittenas:
1 (cid:104) √ (cid:105)
ϕ∗ = (2−δ)ϕ+2δCDℓ(ϕ)+ 8δZ , Z ∼N(0,C)
2+δ
whereδ ∈(0,2).
5arXivTemplate APREPRINT
Algorithm2PreconditionedCrank-NicolsonLangevin(pCNL)Algorithm
1: Initializeu(0),setnumberofiterationsN,chooseδ ∈(0,2)
2: forn=0toN −1do
(cid:104) √ (cid:105)
3: Proposev = 1 (2−δ)u(n)+2δCDℓ(u(n))+ 8δN(0,C)
2+δ
(cid:110) (cid:111)
4: Calculateacceptanceprobabilitya(v|u(n))=min 1, p(v|D)
p(u(n)|D)
5: Drawη ∼Uniform(0,1)
6: ifη ≤a(v|u(n))then
7: Setu(n+1) =v
8: else
9: Setu(n+1) =u(n)
10: endif
11: endfor
Then,whenthepCNLproposalisappliedintheMHprocedure,theacceptanceprobability7assumesthefollowing
expression:
acc =min{1,exp(ρ(ϕ,ϕ∗)−ρ(ϕ∗,ϕ))}
pCNL
where
1 δ δ √
ρ(ϕ,ϕ∗)=−ℓ(ϕ)− ⟨ϕ∗−ϕ,Dℓ(ϕ)⟩− ⟨ϕ+ϕ∗,Dℓ(ϕ)⟩+ ∥ CDℓ(ϕ)∥2
2 4 4
and,inourcase,C =I.
ThefollowingtheoremprovidestheconvergenceoftheacceptanceprobabilityofthepCNLalgorithmtooneasthe
networkwidthincreases.
Theorem2.2 ConsidertheBNNmodelwiththereparametrisation2. Then,theacceptanceprobabilityofthepCNL
algorithmtosamplefromthereparametrisedweightposterior,foranyδ ∈(0,2),convergesto1asthewidthofthe
networkincreases. Thatistosay,ifd isthesmallestamongthenetwork’slayerwidths,then
min
(cid:26) p(ϕ∗|D)q(ϕ|ϕ∗)(cid:27)
a(ϕ∗|ϕ)=min 1, →1 as d →∞
p(ϕ|D)q(ϕ∗|ϕ) min
SeeAppendixBfortheproofofTheorem2.2.
3 Marginal-ConditionalDecomposition
AnalternativeapproachtoTheorem2.1involvesmarginalizingtheweightsofthenetwork’sfinallayerandperformthe
samplingprocedureonlyontheweightsofthenetwork’sinnerlayers. Thisideaiseffectivesinceitacknowledgesthat
exactsamplingcanbeperformedfromtheposteriordistributionofthereparametrizedweightsofthelastlayer,oncethe
weightsfromallprecedinglayersareknown,i.e.,
p(ϕ|D)=p(ϕ(L+1)|ϕ(≤L),D)p(ϕ(≤L)|D)
=p(ϕ(L+1)|θ(≤L),D)p(θ(≤L)|D)
where the last equality follows directly from the reparametrisation definition and from the fact that
p(ϕ(L+1)|θ(≤L),D)∼N(0,I )foranyfixedvalueofθ(≤L). TheideaisthentosimplyperformpCNsamplingon
d(L)
theposteriordistributionovertheinner-layersweightsπ(θ≤L|D). Then,oncethesamples
(cid:104) (cid:105)
θ(≤L)
i
i=1,...,n
havebeencollected,wedraw,∀i,ϕ(L+1) ∼ N(0,I ),toobtainasampleofthefullposteriordistributionofthe
i d(L)
reparametrisedweights. OnthepCNalgorithm,thenexttheoremisacounterpartofTheorem2.1:
Theorem3.1 ConsidertheBNNmodelwiththereparametrisation2,andsetp(θ|D)=p(ϕ(L+1)|θ(≤L),D)p(θ(≤L)|D)
andθ(≤L) = W. Then,theacceptanceprobabilityofthepCNalgorithm,foranyβ ∈ [0,1),appliedtop(θ(≤L)|D)
6arXivTemplate APREPRINT
convergesto1asthewidthofthenetworkincreases. Ifd isthesmallestamongthenetwork’slayerwidths,then
min
(cid:26) p(W∗|D)q(W|W∗)(cid:27)
a(ϕ∗|ϕ)=min 1, →1 as d →∞
p(ϕ|D)q(W∗|W) min
SeeAppendixCfortheproofofTheorem3.1.
4 NumericalExperiments
Wetestourframeworktovalidatethetheoryandhighlightitsapplicability. InthesettingofTheorems2.1and2.2,
wecomparetheperformanceofthepCNalgorithm,theunderdampedLMCsamplerandthepCNLmethodsonthe
reparametrizedposteriorofafullyconnectedfeed-forwardBNN.Wereplicatethesettingusedby[15]: theCIFAR-10
dataset,withone-hotencodingand−1/10labelshifting,isusedinalltheexperiments,thelikelihoodisGaussianwitha
fixedstandarddeviationofσ =0.1andsamplethinningisconsistentlyapplied. Afaircomparisonbetweenthedifferent
samplersisachievedbyequalizingtheirstepsizeβ,thatinourcontextalwaysreferstothecoefficientmultiplyingthe
noiseintheproposalstep. Thecodeisavailableatgithub.com/lucia-pezzetti/Function-Space-MCMC-for-Wide-BNNs.
4.1 AcceptanceRateConvergence
ThefirstresultwereportinFigure2isthenumericalillustrationofthebehaviorofthepCN,LMCandpCNLacceptance
ratesasthewidthoftheBNNgrowslarge. Tobemoreprecise,theacceptanceprobabilitiesoftheunderdampedLMC
thepCNandthepCNL,appliedtotheposteriorofthereparametrizedweights,areplottedforaFCNwith1hidden
layer. Foreachsubplotafixedstepsize,respectivelyβ = 0.2,β = 0.1andβ = 0.01,hasbeenusedintheproposal
step. Thefigureclearlydisplaysthat,forallstepsizes,theacceptancerateofthepCNalgorithmsteadilyincreaseswith
thelayerwidth,empiricallyshowcasingthetheoreticalresultpresentedinTheorem2.1. Evenifitsacceptancerate
increasesasthewidthgrows,asexpectedfromwhatwewilldemonstrateinTheorem2.2,theconvergencerateseems
tobeheavilyaffectedbythestepsize. Indeed,forthelargeststepsize,β =0.2,thetrendisquiteevident,whereasfor
theothertwostepsizestheacceptancerateincreasesonlymarginallywhenthenumberofhiddenunitsvaryfrom512
to4192andappearsalmostconstant. Apossibleexplanationforthisunexpectedfindingscanlieinthefactthatthe
convergenceofthepCNLacceptancerateismoreelaboratedthanthepCNone,asclearfromtheProofBandthismay
leadtounpredictedconsequencesinitsempiricalresults. Finally,theprofileoftheLMCacceptanceratesconfirmand
reflectthereasonwhywestartedourinvestigations: theMCMCmethods’non-robustnessinhigh-dimensionalsettings
leadstoaprogressivedeclineofitsacceptancerateasthewidthincreasesthatshouldbecompensatedbydecreasingthe
methodstepsize.
Wethenproceedbyanalyzingthequalityofthesamplersusingtwokeydiagnostictools: theeffectivesamplesizeand
theGelman-Rubinstatistic.
4.2 Diagnostic: EffectiveSampleSize
WeassesstheperformanceoftheMCMCusingtheeffectivesamplesize(ESS).Inparticular,foranMCMC-based
estimator,theESSestimatesthenumberofindependentsamplesthatareequivalent,intermsoftheirstandardvariance,
to the correlated Markov chain samples collected. In practical implementations, we use TensorFlow Probability’s
built-infunction,tfp.mcmc.effective_sample_size,tocomputetheESS[21].:
N
ESS(N)= ,
1+2(cid:80)N−1(1− i )R
i=1 N i
wheretheresultingsequenceofauto-correlationsistruncatedafterthefirstappearanceofatermthatislessthanzero.
Inparticular,theper-stepESSwillbecalculatedbydividingtheaboveexpressionbyN.
4.3 Diagnostic: Gelman-RubinStatistic
TheGelman-Rubinstatisticorpotentialscalereductionfactor,Rˆ,[22,23]complementstheESSanalsysbyassessing
howthedifferentsamplesaffecttherateofconvergenceofthecollectedMarkovchain. Moreprecisely,themetricis
evaluatedbyrunningmultipleindependentchainspereachsampleandthenexploitingcomparisonsofthevariance
betweenmultiplechainstothevarianceswithinthesinglechainstogiveameasureofthedegreetowhichvariance(of
themeans)betweenchainsexceedswhatonewouldexpectifthechainswereidenticallydistributed. Mathematically,
7arXivTemplate APREPRINT
Stepsize = 0.2 Stepsize = 0.1 Stepsize = 0.01
1.00 1.00 1.00
pCN pCN
LMC LMC
pCNL pCNL
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
pCN
LMC
pCNL
0.00128 512 1024 2096 4192 0.00128 512 1024 2096 4192 0.00128 512 1024 2096 4192
Dimension Dimension Dimension
Figure2: Comparisonatdifferentstepsizes(β = 0.2,0.1,0.01)oftheacceptanceprobabilityobtainedusing: i. the
underdampedLMCalgorithm(orMetropolisAdjustedLangevinAlgorithm: MALA);ii. thepCNalgorithm;iii. the
pCNLmethod. Theneuralnetworkarchitectureusedisafully-connectedwithonehiddenlayer,andlayerwidththat
variesamongthefollowingvalues: 128,512,1024,2096,4192. TheCIFAR-10datasetisused,withthesamplesize
fixedatn = 256. TheacceptancerateofthepCN(orangeline)increasessteadilyasthewidthoftheBNNgrows
ateverystepsize,suggestingimprovedperformanceinwideBNNs,highlightingitsrobustnessinhigh-dimensional
settingsandempiricallyconfirmingourtheoreticalanalysis. Incontrast,theLMC(blueline)initiallyshowsarapid
acceleration in performance due to the reparametrization effect, indeed as the width of the network increases, the
samplingdistributionbecomesmoresimilartoanisotropicGaussiandistribution,whichmakesitbetter-behavedand
the sampling procedure more effective. However, as the width increases further, the sampler’s non-robustness in
high-dimensionalsettingsleadstoaprogressivedeclineofitsacceptancerate. Thisconfirmstheneed,instandard
MCMCprocedures,tocarefullydecreasethestepsizeinordertomaintainaconstantacceptancerate,which,inevitably,
introducesfurthercorrelationsbetweenthesamples. Thisbehaviorislessvisible,butslightlypresent,alsoforthe
smallest stepsize, β = 0.01 for which the LMC acceptance rate remains constantly over 0.99. Finally, the pCNL
acceptancerate(greenline)exhibitsaslowerprogresstoward1asthewidthincreases,comparedtothepCN,andthe
rateseemstobealsoheavilydependentonthestepsize. Indeed,whereastheconvergenceisquiteevidentforβ =0.2,
forboththeotherstepsizestheacceptancerateincreasesonlymarginallyasthewidthgrowsandappearasalmost
constant.
theGelman-Rubinstatisticisdefinedasfollows: SupposewehaverunM chainsinparallelwithdifferentstarting
valuesandthatwehavecollectedN samplesforeachchain. Denotebyx(m)thei-thsampleofthem-thchain. The
i
within-chainvarianceandthebetween-chainvariancearedefinedas:
M M
1 (cid:88) N (cid:88)
W = s2 , B = (x¯(·)−x¯(m))2,
M m M −1
m=1 m=1
wheres2 isthesamplevarianceofthem-thchain, x¯(m) itsmeanandx¯(·) isthegrandmean. TheGelman-Rubin
m
statisticisthendefinedas:
N−1W + 1B
Rˆ = N N .
W
Now,sinceatconvergencealltheparallelchainsareindistinguishable,theGelman-Rubinstatisticshouldbecloseto1.
Inpractice,avalueofRˆsmallerthan1.2isconsideredasagoodindicatorofapproximateconvergence[22].
In all the following experiments we make use of the TensorFlow Probability’s built-in function,
tfp.mcmc.potential_scale_reduction,tocomputeRˆ.
4.4 RealWorldApplication
WeexamineandcomparehowtheefficiencyoftheLMC,thepCNandthepCNLsamplersareinfluencedbychangesin
networkwidth,usingper-stepESSastheperformancemetric. Ouranalysiswasconductedusingasinglechainoverone
millionsteps,excludingthefirst20,000stepsasburn-inforeachconfigurationtested,andwethinbycollectingevery
25thstepfromthechain. Inallexperiments,weusedasingle-layer,fullyconnectedfeed-forwardneuralnetworkwith
GELUactivationfunctions[24]. WeadheredtotheweightandbiasscalingdescribedinSection1.1of[15],setting
thescalingfactorstoσ2 =2andσ2 =0.01acrossalllayersexceptfortheoutputlayer,whereσ2 wassetto1to
W b W
maintainunitvarianceofthenetwork’sinitialpredictionsunderthepriordistribution.
8
etaR
ecnatpeccA
etaR
ecnatpeccA
etaR
ecnatpeccAarXivTemplate APREPRINT
Inthissetting,wefocusedonevaluatingthesamplers’performanceasthenetworkwidthincreased,seekingempirical
validationofourtheory. Wefixedthedatasetsizeatn=256andallowedthenetworkwidthtorangefrom128to4192,
increasingaccordingtopowersof2. Reparametrization2wasusedforallsamplers.
EmpiricalfindingsareshowninFigure3,whichcomparestheefficiencyofthesamplersusingtheper-stepESSmetric
acrossvariouswidths. Specifically,thesolidlinesrepresenttheaverageESSperstepforeachwidth,whiletheshaded
regionsindicatetheminimumandmaximumvaluesoverthewholerun. Thereportedplotscorrespondtothestepsizes
β =0.2(above),β =0.1(inthemiddle)andβ =0.01(below).
Weanalyzetheplotsforthethreestepsizesseparatelybeforegivingageneraloverviewoftheresults. Forβ =0.2,the
LMCsamplerexhibitsanextremelylowper-stepESSacrossallwidths,reflectingitssuboptimalacceptancerate. In
particular,themetricseemstoreachzerowithalmostnovariabilityforthelargerwidths. ThepCNsampler,onthe
otherhand,ischaracterizedbyasteadyincreaseinitsper-stepESS,underlininghowtheincreaseinacceptancerate
actuallyhelpstoboosttheperformanceandtheefficiencyofthesampler. Indeed,theriseintheacceptanceratefora
constantstepsizedoesnotintroducefurthercorrelationamongthesamples,butratherallowsthesamplertoexplorethe
spacemoreeffectively. ThepCNL,asweexpectedfromthetheoreticalanalysis,behavessimilarlytothepCN,although
theincreaseintheESSislesspronounced. Moreover,thepCNLmaintainsthehighestper-stepESSinallbutthelargest
dimension,wherethepCNeventuallyreachesit. This,togetherwiththefactthatthepCNLiscomputationallymore
demandingthanthepCN,startstosuggestthatitmaybealessattractiveoptionforwideBNNs.
For β = 0.1, the performances of LMC and pCN described for the previous stepsize are even more accentuated.
The LMC procedure is characterized by an initial significant rise in per-step ESS as the network width increases,
thatcanbeattributedtothereparametrizationeffect: thesamplingdistributionbecomesmoresimilartoanisotropic
Gaussianleadingtoanincreaseintheperformanceofthesamplerandintheacceptancerate. However,asthewidth
increasesfurther,thewell-knownMCMCmethods’non-robustnessinhigh-dimensionalsettingsleadstoaprogressive
deteriorationinperformance,thusconfirmingtheneedtocarefullydecreasethestepsizefortheefficacytonotdiminish.
ThepCNproposal,instead,isstillassociatedtoadramaticincreaseintheESSasthenetworkwidthgrowsandthesame
analysisasbeforecanbeapplied. Worth-notingisthat,asexpected,biggerstepsizesintroducelessautocorrelation
amongthecollecteddatapointsandhenceleadtohigherESSvalues. Finally,thepCNLmethodshowsamorestable
performance,withevenanunexpectedslightdeclineintheESSasthewidthbecomeslarger. Thismaybejustifiedby
thefactthattheconvergenceofthepCNLacceptanceratedependsonmorefactorsthanthepCNone,thismayleadto
unpredictedconsequencesinitsempiricalresults. Nevertheless,thepCNLmaintainsthehighestper-stepESSinall
widthsbutthelargestone.
Forthesmalleststepsize,β =0.01,theoutcomesofthesimulationsareratherdifferent. BoththepCNandthepCNL
algorithmsexhibitastrongautocorrelationamongthesamples,duetothetoosmallstepsizeandtheconstructionitself
oftheproposals. Thisleadstoanextremelylowper-stepESSdespitethehighacceptancerate. TheLMCmethod,
converselyandsurprisingly,showssignificantvaluesofper-stepESS,evenifdecreasingasthenetworkwidthincreases.
This is particularly striking if these values are compared to the ones obtained for β = 0.1, for which the LMC’s
acceptanceratewasclosertothesuggestedoptimalone[19]. Despitethisunexpectedbehavior,theefficacyofthe
samplingprocedureisstilldecliningasthedimensiongrowsand,sincetheacceptancerateremainsalmostconstant,
maintainingitstabledoesn’tseemaneasytasktoachieve.
Ingeneral,theresultsoftheexperimentsareconsistentwiththetheoreticalanalysis. ThepCNsampler,inparticular,
emergesasthemostappropriatemethodforsamplingfromthereparametrizedposteriorofwideBNNs. Nevertheless,
itiscrucialtonotsetthestepsizetoosmall,asthiswouldleadtoasignificantincreaseintheautocorrelationamong
thesamplesandaconsequentdecreaseintheESS.Atlowerwidths,instead,tuningthestepsizeoftheLMCmethod
toreachanacceptancerateover98%istheoptionthatguaranteesthebestresults. Finally,thepCNLmethodseems
nottohavemeaningfulenoughoutcomestojustifyitsuseinthiscontext,especiallyifwetakeintoaccountitsheavy
computationaldemands.
WenowshiftourfocustotheGelman-RubinstatisticinordertoassesstheconvergenceofMarkovchainstothetarget
distribution. WeexaminetheRˆvaluesforthepCN,LMC,andpCNLsamplers,consideringtheirevolutionasafunction
ofthenumberofsteps[5000,10000,15000,20000]. Thestepsizeisfixedatβ =0.1. Foreachsampler,werunthree
independentchainsforeachoftheBNNwidthsamong{512,1024,2096,4192},wethenusethecollectedsamples
tocalculatethemetricvalues. Morespecifically,ourfocusisoninvestigatinghowtheRˆ valuesfordifferentwidths
oftheBNNchangeasthenumberofstepsincreasesforthedifferentsamplers. Thisanalysisallowsustoevaluate
andcomparetheconvergenceefficiencyofthedifferentsamplersacrossthesevaryingnetworksizes. Theresultsare
presentedinFigure4,wherewealsoplotthereferencelineofRˆ =1.2toindicatethestandardempiricalthresholdfor
determiningconvergence.
9arXivTemplate APREPRINT
ESS of weights per width ESS of predictions per width
LMC LMC
0.10
pCN pCN
0.25
pCNL pCNL
0.08
0.20
0.06
0.15
0.04 0.10
0.02 0.05
0.00 0.00
128 512 1024 2048 4096 128 512 1024 2048 4096
Width Width
ESS of weights per width ESS of predictions per width
0.05
LMC 0.14 LMC
pCN pCN
0.04 pCNL 0.12 pCNL
0.10
0.03
0.08
0.02 0.06
0.04
0.01
0.02
0.00 0.00
128 512 1024 2048 4096 128 512 1024 2048 4096
Width Width
ESS of weights per width ESS of predictions per width
0.10 LMC 0.175 LMC
pCN pCN
pCNL 0.150 pCNL
0.08
0.125
0.06 0.100
0.04 0.075
0.050
0.02
0.025
0.00 0.000
128 512 1024 2048 4096 128 512 1024 2048 4096
Width Width
Figure3: ESSanalysisoftheLMC,pCNandpCNLalgorithmsasafunctionofthe1-layerFCN’swidthforstepsizes
β =0.2(above),β =0.1(middle)andβ =0.01(below). Thesolidlinesrepresenttheaverageper-stepESS,whereas
theshadedareasindicatethevariabilityoftheper-stepESSdelineatedbyitsminimumandmaximumvalues. The
settingusedintheexperimentsisthesameasthesettingofFigure2: thelayerwidthoftheBNNvariesamongthe
followingvalues: {128, 512, 1024, 2096, 4192}. TheCIFAR-10datasetisused,withsamplesizefixedatn=256.
Beginningwiththehigheststepsize,β = 0.2,theLMCperformanceisverypooramongalldimensionsduetothe
suboptimalacceptanceprobability. ThepCNsampler,instead,showsaconstantgrowthinESSasthenetworkwidth
increases,indicatingthatenhancementsinacceptanceratecontributepositivelytoefficiencyandperformance. The
pCNLapproachshowsasimilarbutlesspronouncedbehavior, maintainingthehighestper-stepESSinallbutthe
largestdimensionswherepCNeventuallyreachesit. Forβ =0.1,theLMCmethodinitiallydemonstratesasignificant
riseinESSasthenetworkwidthincreases,reflectingtheincreaseinacceptancerateduetoreparametrization. However,
itseffectivenesssharplydecreasesinhigher-dimensionalsettingsduetoitsinherentlimitations. pCNLdisplaysamore
stableperformance,althoughtheESSslightlydecreaseswithincreasingdimensionality. Conversely,pCNconsistently
demonstratessignificantimprovementsinESS,reaffirmingitsrobustnessinlargerBNNarchitectures. Finally,the
smallest stepsize, β = 0.01 heavily affects the behavior of both the pCN and the pCNL samplers which exhibit
strongautocorrelationamongthesamplers. Instead,whileLMC’sESSdecreases,itremainsrelativelyrobustacross
alldimensions. Thisunderscoresthedistinctionbetweenprovinganacceptanceprobabilityapproachingunityfor
everyfixedstepsizeandachievingsuchaprobabilitythroughstepsizereduction. Smallerstepsizesinevitablyleadto
highercorrelations,reducedESS,anddiminishedperformance. Maintainingalargerstepsizeisbeneficialasitavoids
introducingadditionalcorrelations.
10
pets/SSE
pets/SSE
pets/SSE
pets/SSE
pets/SSE
pets/SSEarXivTemplate APREPRINT
R-hat per number of steps: pCN R-hat per number of steps: LMC R-hat per number of steps: pCNL
1.25 1.4 1.04
Dim 512 Dim 512 Dim 512
Dim 1024 Dim 1024 Dim 1024
Dim 2048 Dim 2048 Dim 2048
1.20 1.03
Dim 4096 1.3 Dim 4096 Dim 4096
Threshold (1.2) Threshold (1.2) Threshold (1.2)
1.15 1.02
1.2
1.10 1.01
1.1
1.05 1.00
1.0
1.00 0.99
0.95 0.9 0.98
5000 10000 15000 20000 5000 10000 15000 20000 5000 10000 15000 20000
Number of Steps Number of Steps Number of Steps
Figure4: EvolutionoftheGelman-Rubinstatistic,evaluatedusingthreeindependentchains,oftheLMC,pCNand
pCNLasafunctionofthenumberofstepsforthestepsizeβ =0.1. ThesolidlinesrepresenttheaverageGelman-Rubin
statistic, whereas the shaded areas indicate their standard deviation. Again, the CIFAR-10 dataset, with sample
sizefixedatn = 256,isused. Sincethemetricshouldbecloseto1forallchainstobeconsideredconverged,we
traceahorizontallineatRˆ = 1.2,indicatingthestandardempiricalthresholdfordeterminingconvergence. Forall
samplesthechosenburn-inof20,000stepsappearstobesufficienttoensurethatthechainshavereachedthetarget
distribution. However, we can still notice some peculiarities. The metric for pCN sampler improves as the width
increases,convalidatingevenmoretherobustnessofthemethodinhigh-dimensionalsettings. TheLMCmethodshows
complementaryresults,withanincreaseintheGelman-Rubinstatisticasthenetworkwidthgrows. Finally,thepCNL
approachexhibitsstraordinarygoodresultsamongallthedimensions.
Inthefirstplaceweobservethatforallsamplersthechosenburn-inof20,000stepsappearstobeadequatetoensurethat
thechainshavereachedstationarity. Nevertheless,thesamplerspresentsomepeculiaritiesthatareworthtodiscuss. The
pCNsampler,showsalessvariableandcloserto1Gelman-Rubinstatisticasthenetworkwidthincreases,suggesting
thatthechainattainsstationarityquickerandmoresteadilyforlargenetworkwidths. Thisisinlinewiththepreviously
obtainedresultsandconfirmsonceagainpCNsuitabilityinwideBNNssettings. Notsurprisingly,theLMCresults
seemsthenegativeversionofthepCNones. Indeed,itsRˆvaluesreflectsthebetterperformanceofthesamplerinlower
dimensions,asunderlinedbyitsincreaseinboththeaverageandstandarddeviationasthewidthgrows. Finally,the
pCNLprocedureexhibitsexceptionalygoodresultsamongalldimensions,withtheRˆvaluesthatarealwayscloseto
1andhavesmallvariability. Thisisaveryinterestingresult,sinceithighlightstheabilityofthesampletoproperly
explorethespaceandquicklyapproachthetargetdistribution.
5 Conclusion
Inthispaper,weinvestigatedtheeffectivenessofthepCNandpCNLsamplersinsamplingwideBayesianNeural
Networks(BNNs),andweprovidedinsightsintothescalabilityandefficiencyofthesemethods;particularlywhen
comparedtotheunderdampedLMCsampler. Ourtheoreticalfindings,groundedintheunderstandingofthereparame-
terizedposteriordistribution,demonstratedthattheacceptanceprobabilityofthepCNproposalconvergesto1asthe
networkwidthgoestoinfinity,thusensuringefficientsampling. Empiricalresultsfurthercorroboratedthistheoretical
prediction. Whenappliedtofullyconnectednetworkswithvaryingwidths,pCNconsistentlydemonstratedhigher
per-stepESScomparedtoLMC,especiallyasdimensionalityincreased. Thisfindingiscrucialasitunderscoresthe
advantageofthepCNapproachinwideBayesianneuralnetworks,wherestandardMCMCtechniquesoftenstruggle
duetohighautocorrelationamongsamples. OurresultsindicatethatthepCNsampler,undertheproposedframework,
offersareliableandscalablesolutionforBayesianinferenceinwideneuralnetworks, ultimatelyleadingtobetter
diagnosticperformanceandimprovedcomputationalefficiency. ThedimensionalrobustnessofthepCNLmethod,
alongwiththeincorporationofgradientinformationwithintheMHproposal,makesiteffectiveandstableforposterior
samplingfromBNNs. However,themethodissignificantlymoreexpensivecomputationallycomparedtopCNand
therefore,giventheabsenceofsignificantincreasesinperformanceasthenetworkwidthgrowslarger,thepCNversion
ispreferredinverylargewidthregimes.
11
tah-R tah-R tah-RarXivTemplate APREPRINT
Limitations
Duetoconstraintsinourcomputationalresources,wewerenotabletorunexperimentsonBNNswithwiderlayers.
However,basedonourcurrentanalyses,aspresentedintheacceptanceprobabilityanalysisofFigure2andintheESS
analysisofFigure3,weanticipateawideningperformancegapbetweentheLMCandpCNsamplerswithincreased
layerwidth. Theoretically, we showed thattheacceptance ratesfor pCNandpCNL converge to1, supportingthe
scalabilityandrobustnessofthealgorithms. Incontrast, theLMCsampler’sacceptancerateisknowntodecrease
furtheraslayerwidthincreases. Thisanticipatedbehaviorsuggestsanincreaseinautocorrelationamongsamples
fromtheLMC,andadecreaseinautocorrelationforthepCNsamples. Theseprojectionsunderscorethepotential
limitationsofLMCsamplersinhandlinglargerBNNs,highlightingtheneedformorerobustsamplingmethodsin
extensivecomputationalsettings. Finally,itwouldbeinterestingtoextendtheseempiricalanalysestootherneural
networkarchitecturesandtodeepneuralnetworks.
References
[1] MohammadEmtiyazKhanandDidrikNielsenandVootTangkarattandWuLinandYarinGalandAkashSrivastava.
FastandScalableBayesianDeepLearningbyWeight-PerturbationinAdam. arXivpreprintarXiv:1806.04854,
2018.
[2] WesleyMaddoxandTimurGaripovandPavelIzmailovandDmitryVetrovandAndrewGordonWilson. ASimple
BaselineforBayesianUncertaintyinDeepLearning. arXivpreprintarXiv:1902.02476,2019.
[3] Kazuki Osawa and Siddharth Swaroop and Anirudh Jain and Runa Eschenhagen and Richard E. Turner and
RioYokotaandMohammadEmtiyazKhan. PracticalDeepLearningwithBayesianPrinciples. arXivpreprint
arXiv:1906.02506,2019.
[4] MichaelW.DusenberryandGhassenJerfelandYemingWenandYi-AnMaandJasperSnoekandKatherineHeller
andBalajiLakshminarayananandDustinTran. EfficientandScalableBayesianNeuralNetswithRank-1Factors.
arXivpreprintarXiv:2005.07186,2020.
[5] ErikDaxbergerandEricNalisnickandJamesUrquhartAllinghamandJavierAntoránandJoséMiguelHernández-
Lobato. BayesianDeepLearningviaSubnetworkInference. arXivpreprintarXiv:2010.14689,2022.
[6] PavelIzmailovandSharadVikramandMatthewD.HoffmanandAndrewGordonWilson. WhatAreBayesian
NeuralNetworkPosteriorsReallyLike? arXivpreprintarXiv:2104.14421,2021.
[7] RadfordM.Neal. BayesianLearningforNeuralNetworks. Springer-Verlag,Berlin,Heidelberg,1996.
[8] AlexanderG.deG.MatthewsandMarkRowlandandJiriHronandRichardE.TurnerandZoubinGhahramani.
GaussianProcessBehaviourinWideDeepNeuralNetworks. arXivpreprintarXiv:1804.11271,2018.
[9] JaehoonLeeandYasamanBahriandRomanNovakandSamuelS.SchoenholzandJeffreyPenningtonandJascha
Sohl-Dickstein. DeepNeuralNetworksasGaussianProcesses. arXivpreprintarXiv:1711.00165,2018.
[10] RomanNovakandLechaoXiaoandJaehoonLeeandYasamanBahriandGregYangandJiriHronandDanielA.
AbolafiaandJeffreyPenningtonandJaschaSohl-Dickstein. BayesianDeepConvolutionalNetworkswithMany
ChannelsareGaussianProcesses. arXivpreprintarXiv:1810.05148,2020.
[11] AdriàGarriga-AlonsoandCarlEdwardRasmussenandLaurenceAitchison. DeepConvolutionalNetworksas
shallowGaussianProcesses. arXivpreprintarXiv:1808.05587,2019.
[12] Greg Yang. Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are
GaussianProcesses. arXivpreprintarXiv:1910.12478,2021.
[13] Jiri Hron and Yasaman Bahri and Roman Novak and Jeffrey Pennington and Jascha Sohl-Dickstein. Exact
posteriordistributionsofwideBayesianneuralnetworks. arXivpreprintarXiv:2006.10541,2020.
[14] DanieleBracaleandStefanoFavaroandSandraFortiniandStefanoPeluchetti.Large-widthfunctionalasymptotics
fordeepGaussianneuralnetworks. arXivpreprintarXiv:2102.10307,2021.
[15] JiriHronandRomanNovakandJeffreyPenningtonandJaschaSohl-Dickstein. WideBayesianneuralnetworks
haveasimpleweightposterior: theoryandacceleratedsampling. arXivpreprintarXiv:2206.07673,2022.
[16] RosskyPJ,DollyJD,FriedmanHL. BrownianDynamicsasSmartMonteCarloSsimulation. J.CHEM.PHYS.,
USA,vol.69,no.10,pp.4628-4633,1978.
[17] ArthurJacotandFranckGabrielandClémentHongler. NeuralTangentKernel: ConvergenceandGeneralization
inNeuralNetworks. arXivpreprintarXiv:1806.07572,2020.
12arXivTemplate APREPRINT
[18] G. O. Roberts and A. Gelman and W. R. Gilks. Weak Convergence and Optimal Scaling of Random Walk
MetropolisAlgorithms. TheAnnalsofAppliedProbability,vol.7,no.1,pages110–120,1997.
[19] GarethO.RobertsandJeffreyS.Rosenthal. OptimalScalingofDiscreteApproximationstoLangevinDiffusions.
JournaloftheRoyalStatisticalSociety.SeriesB(StatisticalMethodology),vol.60,no.1,pages255–268,1998.
[20] S.L.CotterandG.O.RobertsandA.M.StuartandD.White. MCMCMethodsforFunctions: ModifyingOld
AlgorithmstoMakeThemFaster. StatisticalScience,28(3): http://dx.doi.org/10.1214/13-sts421,August2013.
[21] JoshuaV.DillonandIanLangmoreandDustinTranandEugeneBrevdoandSrinivasVasudevanandDaveMoore
andBrianPattonandAlexAlemiandMattHoffmanandRifA.Saurous. TensorFlowDistributions. arXivpreprint
arXiv:1711.10604,2017.
[22] AndrewGelmanandDonaldB.Rubin. InferencefromIterativeSimulationUsingMultipleSequences. Statistical
Science,vol.7,no.4,pages457–472,1992,DOI:10.1214/ss/1177011136.
[23] StephenP.BrooksandAndrewGelman. GeneralMethodsforMonitoringConvergenceofIterativeSimulations.
JournalofComputationalandGraphicalStatistics,vol.7,no.4,pages434–455,1998.
[24] DanHendrycksandKevinGimpel. GaussianErrorLinearUnits(GELUs). arXivpreprintarXiv:1606.08415,
2023.
[25] CarlEdwardRasmussenandChristopherK.I.Williams. Gaussianprocessesformachinelearning. MITPress,
2006,pagesI-XVIII,1-248.
A ProofofTheorem2.1
LettheassumptionsofTheorem2.1hold. WestartbyanalysingthegeneralexpressionoftheMCMCacceptance
probability:
(cid:26) p(ϕ∗|D)q(ϕ|ϕ∗)(cid:27)
a=min 1, .
p(ϕ|D)q(ϕ∗|ϕ)
(cid:112)
Wehavealreadyshownthatq(ϕ∗|ϕ)=N( 1−β2ϕ,β2I ). Regardingthereparamatrizedweightposteriorofthe
D
network,weobservethat[15]
p(ϕ|D)=p(ϕ(L+1)|ϕ(≤L),D)p(ϕ(≤L)|D)
(cid:18) (cid:19)
(cid:112) 1
∝p(ϕ(L+1)|ϕ(≤L),D) det(Σ)exp yT(σ2I +ΨΨT)−1y
2 n
where p(ϕ(L+1)|ϕ(≤L),D) ∼ N(0,I ) is assured by the reparametrisation. It is then crucial to recognize the
d(L)
empiricalNNGPkernelKˆ =σ2I +ΨΨT [25]andobservethatdet(Σ)∝det(Kˆ ).
σ2 n σ2
Insertingeverythingintheexpressionoftheacceptanceprobabilitywehave:
p(ϕ∗|D)q(ϕ|ϕ∗) p(ϕ∗(≤L)|D)p(ϕ∗(L+1)|ϕ∗(≤L),D)q(ϕ|ϕ∗)
=
p(ϕ|D)q(ϕ∗|ϕ) p(ϕ(≤L)|D)p(ϕ(L+1)|ϕ(≤L),D)q(ϕ∗|ϕ)
p(ϕ∗(≤L))(cid:112) det(Σ∗)exp(cid:0)1yT(σ2I +Ψ∗Ψ∗T)−1y(cid:1) p(ϕ∗(L+1)|ϕ∗(≤L),D)q(ϕ|ϕ∗)
= 2 n
p(ϕ(≤L))(cid:112) det(Σ)exp(cid:0)1yT(σ2I +ΨΨT)−1y(cid:1)
p(ϕ(L+1)|ϕ(≤L),D)q(ϕ∗|ϕ)
2 n
WherewedenotewithΣ∗andΨ∗thecovariancematrixandscaledinputmatrixoftheredoutlayerinequation3,but
foranetworkwithweightsϕ∗. Now:
(cid:18) (cid:19)
1 (cid:112)
q(ϕ|ϕ∗)∝exp − ||ϕ− 1−β2ϕ∗||2
2β2
(cid:32) (cid:112) (cid:33)
1 (1−β2) 1−β2
=exp − ||ϕ||2− ||ϕ∗||2+ ϕTϕ∗
2β2 2β2 β2
(cid:32) (cid:112) (cid:33)
1 1 1 1−β2
=exp − ||ϕ||2− ||ϕ∗||2+ ||ϕ∗||2+ ϕTϕ∗
2β2 2β2 2 β2
13arXivTemplate APREPRINT
Fromwhich
q(ϕ|ϕ∗) (cid:18) 1 1 (cid:19)
=exp ||ϕ∗||2− ||ϕ||2
q(ϕ∗|ϕ) 2 2
andsince
(cid:18) (cid:19) (cid:18) (cid:19)
1 1
p(ϕ∗(≤L))p(ϕ∗(L+1)|ϕ∗(≤L),D)∝exp − ||ϕ∗(≤L)||2 exp − ||ϕ∗(L+1)||2
2 2
(cid:18) (cid:19)
1
=exp − ||ϕ∗||2
2
weobtain
p(ϕ∗|D)q(ϕ|ϕ∗)
exp(cid:0) −1||ϕ∗||2(cid:1) exp(cid:0)1||ϕ∗||2(cid:1)(cid:112) det(Σ∗)exp(cid:0)1yT(σ2I +Ψ∗Ψ∗T)−1y(cid:1)
= 2 2 2 n
p(ϕ|D)q(ϕ∗|ϕ) exp(cid:0) −1||ϕ||2(cid:1) exp(cid:0)1||ϕ||2(cid:1)(cid:112) det(Σ)exp(cid:0)1yT(σ2I +ΨΨT)−1y(cid:1)
2 2 2 n
(cid:112) det(Σ∗)exp(cid:0)1yT(σ2I +Ψ∗Ψ∗T)−1y(cid:1)
= 2 n
(cid:112) det(Σ)exp(cid:0)1yT(σ2I +ΨΨT)−1y(cid:1)
2 n
(cid:113) (cid:16) (cid:17)
det(Kˆ∗ )exp 1yT(Kˆ∗ )−1y
σ2 2 σ2
∝
(cid:113) (cid:16) (cid:17)
det(Kˆ )exp 1yT(Kˆ )−1y
σ2 2 σ2
Toconclude,weexploittheknownconvergenceoftheempiricalNNGPkerneltoaconstantindependentofϕ≤L =θ≤L
Kˆ →K as d →∞
σ2 σ2 min
Thisprovesthatthenumeratorandthedenominatorconvergetothesamequantityand,consequently,thattheirratio
convergesto1.
Implyingthethesis
p(ϕ∗|D)q(ϕ|ϕ∗)
a=1∧ →1∧1=1 ford →∞
p(ϕ|D)q(ϕ∗|ϕ) min
B ProofofTheorem2.2
TheproofofTheorem2.2reliesheavilyonresultsobtainedalongtheproofofTheorem2.1.
Let’sstartbywritingexplicitelytheexpressionsoftheacceptanceprobabilityforthepCNLalgorithm:
ρ(ϕ,ϕ∗)−ρ(ϕ∗,ϕ)=
1 δ δ √
=−ℓ(ϕ)− ⟨ϕ∗−ϕ,Dℓ(ϕ)⟩− ⟨ϕ+ϕ∗,Dℓ(ϕ)⟩+ ∥ CDℓ(ϕ)∥2
2 4 4
1 δ δ √
+ℓ(ϕ∗)+ ⟨ϕ−ϕ∗,Dℓ(ϕ∗)⟩+ ⟨ϕ+ϕ∗,Dℓ(ϕ∗)⟩− ∥ CDℓ(ϕ∗)∥2
2 4 4
1
=−ℓ(ϕ)+ℓ(ϕ∗)− ⟨ϕ−ϕ∗,Dℓ(ϕ)+Dℓ(ϕ∗)⟩
2
−δ ⟨ϕ+ϕ∗,Dℓ(ϕ)−Dℓ(ϕ∗)⟩+ δ (cid:0) ∥Dℓ(ϕ)∥2−∥Dℓ(ϕ∗)∥2(cid:1)
4 4
Now,wecanexploitpreviousresultstoevaluatethelikelihooddistribution:
p(ϕ|D)p(D)
exp{ℓ(ϕ)}= =
p(ϕ)
C
(D)p(ϕ(L+1)|ϕ(≤L),D)p(ϕ(≤L))(cid:112) det(Σ)exp(cid:0)1yT(σ2I +ΨΨT)−1y(cid:1)
= 1 2 n
p(ϕ)
(cid:18) (cid:19)
(cid:112) 1
=C (D) det(Σ)exp yT(σ2I +ΨΨT)−1y
1 2 n
14arXivTemplate APREPRINT
where C is a suitable distribution depending only on data. The last equality is due to the fact that
1
p(ϕ(L+1)|ϕ(≤L),D)p(ϕ(≤L))∝exp(cid:0) −1∥ϕ∥2(cid:1)
andthepriorp(ϕ)isassumedtobeastandardGaussian.
2
AsalreadydoneintheproofforpCNsamplerwerecognizetheempiricalNNGPkernelKˆ ,andwerecallthatasthe
σ2
minimumlayerwidthgoesto∞,itconvergestoaconstantindependentoftheweightsK .
σ2
Hence,notingthattheFréchetderivativeisaboundedlinearoperator(thuscontinuous),bycontinuousmappingtheorem
wecanconcludethatDℓ(ϕ)→0asthelayerwidthgoesto∞. SincethesamegoesforDℓ(ϕ∗),wecanconcludethat
inthewide-widthlimit,allthetermscontainingFréchetderivativesofthelog-likelihood,evaluatedatanypoint,vanish.
Asaresult,inthewide-widthregime,theacceptancerateofthepCNLsamplerconvergestotheoneofpCNand,thus,
convergesto1.
C ProofofTheorem3.1
Sincewearesamplingonlyfromtheinner-weightsθ(≤L) =W oftheBNN,theacceptanceprobabilitybecomes
p(W|D)q(W|W∗)
a=1∧
p(W|D)q(W∗|W)
Bydefinition2,p(W|D)=p(ϕ(≤L)|D)andthuswecanexploittheknownexpressionsfromAppendixA:
(cid:18) (cid:19)
(cid:112) 1
p(W|D)=p(W) det(Σ)exp yT(σ2I +ΨΨT)−1y
2 n
(cid:18) (cid:19) (cid:18) (cid:19)
1 (cid:112) 1
∝exp − ||W||2 det(Σ)exp yT(σ2I +ΨΨT)−1y
2 2 n
Moreover
(cid:18) (cid:19)
1 (cid:112)
q(W|W∗)∝exp − ||W − 1−β2W∗||2
2β2
q(W|W∗) (cid:18) 1 1 (cid:19)
=⇒ =exp ||W∗||2− ||W||2
q(W∗|W) 2 2
Puttingalltogetherandsimplifyingweget:
π(W∗|D)q(W|W∗)
a=1∧
π(W|D)q(W∗|W)
(cid:112) det(Σ∗)exp(cid:0)1yT(σ2I +Ψ∗Ψ∗T)−1y(cid:1)
=1∧ 2 n
(cid:112) det(Σ)exp(cid:0)1yT(σ2I +ΨΨT)−1y(cid:1)
2 n
ThatcorrespondstothesameexactexpressionfoundinAppendixAandhenceconvergenceto1asthelayerswidth
increasesisgrantedbythesamearguments.
15