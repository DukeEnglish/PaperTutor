Transient-CoMLSim AIP/123-QED
A domain decomposition-based autoregressive deep learning model for unsteady and
nonlinear partial differential equations
S. Nidhan,1 H. Jiang,1 L. Ghule,2 C. Umphrey,3 R. Ranade,4 and J. Pathak1
1)Ansys,Inc.,SanJose,CA
2)Ansys,Inc.,Canonsburg,PA
3)Ansys,Inc.,SaltLakeCity,UT
4)NVIDIA,SantaClara,CA
(*Electronicmail: sheel.nidhan@ansys.com)
(Dated: 27August2024)
Inthispaper,weproposeadomain-decomposition-baseddeeplearning(DL)framework,
namedtransient-CoMLSim,foraccuratelymodelingunsteadyandnonlinearpartialdifferen-
tialequations (PDEs). The framework consistsof two keycomponents: (a)a convolutional
neuralnetwork(CNN)-basedautoencoderarchitectureand(b)anautoregressivemodelcom-
posedoffullyconnectedlayers. Unlikeexistingstate-of-the-artmethodsthatoperateonthe
entirecomputationaldomain,ourCNN-basedautoencodercomputesalower-dimensional
basis for solution and condition fields represented on subdomains. Timestepping is per-
formedentirelyinthelatentspace,generatingembeddingsofthesolutionvariablesfrom
the time history of embeddings of solution and condition variables. This approach not
onlyreducescomputationalcomplexitybutalsoenhancesscalability,makingitwell-suited
for large-scale simulations. Furthermore, to improve the stability of our rollouts, we em-
ploya curriculumlearning(CL) approachduringthetraining oftheautoregressivemodel.
The domain-decomposition strategy enables scaling to out-of-distribution domain sizes
whilemaintainingtheaccuracyofpredictions–afeaturenoteasilyintegratedintopopular
DL-basedapproachesforphysicssimulations. Webenchmarkourmodelagainsttwowidely-
usedDLarchitectures,FourierNeuralOperator(FNO)andU-Net,anddemonstratethatour
frameworkoutperformsthemintermsofaccuracy,extrapolationtounseentimesteps,and
stabilityforawiderangeofusecases.
1
4202
guA
62
]GL.sc[
1v16441.8042:viXraTransient-CoMLSim
I. INTRODUCTION
Inrecentyears,theintersectionofdeeplearning(ML)andphysics-basedmodelinghasgained
significantattention,fueledbythepotentialofdata-driventechniquestoenhanceourunderstanding
and prediction of complex physical phenomena. Several methods, both data-driven and physics-
informed1–7, have been developed to create surrogates for solving partial differential equations
(PDEs). Thesedeeplearning(DL)modelsofferthepromiseofreducingthecomputationaltime
requiredtosolvehighlynonlinearPDEs. Inadditiontodevelopingpurelydata-drivenDLmodels,
therehavebeeneffortstocreatehybridmodelsthatcombinethestrengthsoftraditionalPDEsolvers
with the capabilities of DL methods8–13. These hybrid approaches leverage the generalization
abilityofPDEsolverswhileusingDLtechniquestoeffectivelymodelhigh-dimensionalmanifolds
by identifying repeated patterns in the solution and geometry fields. As a result, they serve as
an augmentation to traditional PDE solvers14–22. These DL-based models have demonstrated
significantpotentialinfieldswherePDEmodelingisessential,suchasweatherprediction23,24,3-D
aerodynamicspredictions25,etc.
Amongthevariousframeworksdevelopedforphysics-basedproblems,physics-informedneural
networks (PINNs)1 and Fourier neural operators (FNOs)2,25 are two of the most prevalent, both
withdistinctstrengthsandapplications. PINNsleverageunderlyingPDEsbyincorporatingresidual
governing equationsdirectly into theloss functionduring training. Thisapproach allowsthem to
solvebothforwardandinverseproblemswhilerespectingphysicallaws,makingthemparticularly
usefulinscenarioswheredataisscarcebutthephysicsiswellunderstood. Incontrast,FNOstakea
different approach by learning operators in the frequency domain. By using Fourier transforms,
FNOscaptureglobalinformationandefficientlyhandlecomplex,high-dimensionalproblems. In
recentyears,graph-basedapproachesusinggraphneuralnetworks(GNNs)havealsoseensignificant
growth, as demonstrated by the works of Brandstetter et al.26, Xu et al.27, and Boussif et al.28.
One of the key advantages of GNN-based methods is the seamless formulations of unstructured
simulations,prevalantinPDEmodelingofcomplexphysics29,ongraphs.
Oneofthekeydistinctionsofexistingdeeplearningmethods,suchasthosementionedearlier,
is that most of them are global in nature. By ‘global’, we mean that they learn the solution
distributionacrosstheentirecomputationaldomainsimultaneously. Thiscontrastswithtraditional
PDE solvers, which derive solutions at computational grid points based on local differentials.
Dueto theirglobal learning approach, these deeplearning modelsoftenstruggle withaccurately
2Transient-CoMLSim
resolvingsmall-scalefeaturesorextrapolatingtoout-of-distributiondomainsizes. Moreover,they
face additional challenges such as the inability to represent complex geometries, difficulty in
scaling to high-dimensional problems with thehigh-resolution meshes that arequite common in
industrial simulations, exorbitantly large training times, prohibitive GPU memory requirements,
andlimited generalizationtoout-of-distributioninput spaces. Theseissuessignificantly hinderthe
practical application of deep learning in real-world problems. Inspired by the local approach of
traditionalPDEsolvers,Ranadeetal.8 introducedtheComposableMachineLearningSimulator
(CoMLSim)frameworkforsteady-statePDEs. Unlikeglobaldeeplearningmethods,CoMLSim
focuseson learningsolutiondistributions andconditionsin low-dimensionalspaceswithin local
subdomainsratherthanacrosstheentirecomputationaldomain. Itthenstitchestheselocalsolutions
together by ensuring consistency and continuity across subdomains using flux schemes learned
with neural networks. This approach integrates two key features: (a) a local-learning strategy
and (b) a low-dimensional learning. Ranade et al. demonstrated that CoMLSim outperformed
DLmethodslearningonthegloballevel,suchasFNO2,DeepONet4,andU-Net30 onarangeof
steady-state PDEs, from simple Poisson’s equations to industrial applications like 3D Reynolds
AveragedNavier-Stokes(RANS)equationsandchipcooling9.
Inthiswork,weextendtheCoMLSimframeworktohandletransientPDEs,introducingwhatwe
callthetransient-CoMLSim. Thepaperisorganizedasfollows: SectionIIoutlinesthemethodology
behind the transient-CoMLSim approach. Section III presents the results, comparing transient-
CoMLSimwithbenchmarkmodelsandevaluatingitsextrapolationcapabilitiesacrossfourdifferent
PDEs. Additionally,weincludeablationstudiesforthetimeintegratornetworkinSectionIII.The
studyisconcludedinSectionIV.
II. METHODOLOGY
Letusassumeasolutionvariablesandaconditionvariablecdefinedonaglobalcomputational
domainΩ . Forexample,inanunsteadyheatdiffusionequationwithamovinglasersource,the
G
temperatureT servesasthesolutionvariable,whilethelaserpowerQactsastheconditionvariable.
Our methodology entails first decomposing Ω into multiple smaller subdomains Ω ,Ω ,···,Ω
1 2 N
suchthatΩ ∪Ω ···∪Ω =Ω . Thereafter,solutionandconditiononeachofthosesubdomains
1 2 N G
areencodedusingaconvolutionalneuralnetwork(CNN)basedautoencoder,namedAEhereafter,
toobtainlatentencodings{ηs,ηs,···,ηs}and{ηc,ηc,···,ηc}forthesolutionandthecondition
1 2 N 1 2 N
3Transient-CoMLSim
i
j
Unroll for all future timesteps
over all subdomains
ηs ,ηc at (during each timestep)
in the latent (η) space
t c−th,t c−(th−1),⋯,t
c
s,c at t c−th,t c−(th−1),⋯,t
c
to
build
bS acti kt c th
h
et o og re igth ine ar l s cu ob md po um ta ai tn ios n al domain
t
c+Δt,s ⋯i,j ,a tt
c +TΔt
s at t c+Δt,t c+2Δt,⋯,t c+TΔt
FIG.1. Schematicofthetransient-CoMLSimframeworkfora2-Dproblemsetup. Es andEc aresolution
and condition encoders. Ds refers to the solution decoder. TI is the time-integrator network. ηs and ηc
correspondtosolutionandconditionlatentembeddings. ModelpredictsuptoT timestepsintothefuture. For
3-Dproblemsetup,thesetupcanbeeasilyextendedtoinclude7neighbors(left,right,top,bottom,front,
back).
variable,respectively.
ηs =Es(s ),ηc =Ec(c ), (1)
n n n n
sˆ =Ds(ηs),cˆ =Dc(ηc). (2)
n n n n
Here, E and D are the encoder and decoder components of our CNN-based autoencoder, i.e.,
sˆ =AEs(s )=Ds(Es(s )) and cˆ =AEc(c )=Dc(Ec(c )). Here, n denotes the nth subdomain.
n n n n n n
Our CNN-based autoencoder, described in detail in section IIA, encodes the subdomains into
low-dimensional latent embeddings. Representing solutions in a non-linear lower-dimensional
basisoffersbenefitssuchasreducedcomputationalcostsandmemoryusagefortimeintegration,as
well as an efficient and dense representation of solution and condition fields, that enables accurate
learning. Thereafter, the time integrator network, named TI hereafter, learns a robust mapping
4Transient-CoMLSim
tothe futuretimestep inthelatent spacefor agivensubdomain from: (a) the historyof previous
timestepsofsolutionandconditionand(b)informationofthesurroundingsubdomains:
ηs =TI(ηs ,ηc ;θ). (3)
n,tc+∆t I,T
s
I,T
c
Here, I denotes the collection of subdomains in the neighborhood of the subdomain n and
T denotes the time history (consisting of previousth timesteps) of all the previous embeddings
foranysubdomaininthevicinityofn(includingnitself),i.e.,T ≡(t ,t −∆t,···,t −th∆t)and
s c c c
T ≡(t ,t −∆t,···,t −th∆t). ForPDEswithoutanytimevaryingconditionfield,onecanonly
c c c c
considerthesolutionencodings(ηs)intheequation3. Likewise,multiplesolutionandcondition
fieldscan behandled byincluding theirrespectiveη inthe input(for solutionand conditionfields)
andtheoutput(forsolutionfieldsonly). Abroadoverviewofthemodelisdepictedinfigure1. In
thesubsequentsubsections,wewilldelvedeeperintothedetailsoftheCNN-basedautoencoder
AEandtimeintegratorTI.
A. Locallearningonastructuredcomputationaldomain
i
s
i,j
CNN Encoder ηs
s
i,j
j E
Stitch
together
subdo
mains s î
,j
CNN Decoder
s
D
FIG.2. SchematicofthestructuredCNN-basedautoencoderusedtoencodethephysicalfieldstothelatent
space η and to decode back from the latent space η to the physical space. Encoding and decoding of
conditionvariablesareperformedinasimilarfashion. For3-Ddatasets,a3-DCNNisused.
Our methodology decomposes the global computational domain Ω into uniform stuctured
G
subdomains Ω ,Ω ,···,Ω such that Ω ∪Ω ···∪Ω = Ω . Each subdomain represents a
1 2 N 1 2 N G
5Transient-CoMLSim
constant physical size and denotes the local distribution of solution and condition of the PDE
distribution,s andc ,respectively. ACNN-basedautoencoderisusedtolearntoencodeanddecode
n n
s andc . Thetrained CNN-based autoencoderdetermines the compressedlatent representations
n n
ηs and ηc of the local distribution in each structured subdomain Ω . The encoder network E
n n n
learns to compress the input distribution in a latent representation as shown in equation 1. The
decoder of the network D learns to reconstruct the original local distribution based on the latent
representation as shown in equation 2. The encoder and decoder networks are trained together
usingareconstructionobjectivefunctionwithsolution/conditionfieldsrepresentedonsubdomains.
Meansquarederror(MSE)isusedasthelossfunctionforoptimization. Duringtheinferencetime,
encoderanddecoderareusedseperatelytoencode{s ,c }into{ηs,ηc}andtodecodeηs obtained
n n n n n
fromthetime-integratornetworktogetbacktophysicalspacesˆ . Wealsonotethatthecondition
n
and solution autoencoders are trained independently, with each field utilizing its own dedicated
autoencoderinstanceifmultiplefieldsareinvolved. TheschematiceoftheCNN-basedautoencoder
operatingonsubdomainsisdepictedinfigure2.
B. Robusttransientlearningwithcurriculumlearning
γ
t c−th
Shared MLP γ
across all
t c−(th−1)
⋮
timesteps t
γ
t
c
Operates individually on collection
of subdomains at each t to obtain γ
t
ηs ,ηc at
t∈{t c−th,t c−(th−1),⋯,t c}
MLP
FIG.3. Schematicofthetimeintegratoroperatinginthelatentspaceη.
TIfocusesonlearningthechangeofdistributionsofsolutionlatentembeddingsinasubdomain
fromthegiventimehistoryofsolutionandconditionlatentembeddingsinitssurroundings. The
6Transient-CoMLSim
networkfollowsalocallearningprinciple,i.e.,ittakesafixedlengthT =thandT =thofthe
s c
historyofthelocaldistributionofsolutionandconditioninthelatentspace,{ηs ,ηc }along
i,j,T
s
i,j,T
c
with collection of its neighboring subdomains I, as the input. Figure 3 presents a 2-D physical
domain for the illustration purpose. Here i, j refer to the index of the subdomain in the x and
y direction respectively. A subdomain with index i, j has four neighbors at (i−1, j), (i+1, j),
(i, j−1),and(i, j+1).
TheTI networkconsists of twodeep neural networks, eachcomposed of multi-layer perceptons
(MLP)asshowninfigure3. Weutilizethefirstnetworktofusethelocaldistributionofneighboring
subdomainsofsolutionandconditiontogeneratevecotorsγ foreachtimestepinthetimehistory
t
t ∈ {t −th∆t,t −(th−1)∆t,...,t }, i.e., γ = F (ηs ,ηc ;θ ). The second network
c c c t spatial I,t I,t F spatial
is used to predict the latent embedding of the solution, ηs , for the next time step t +∆t
i,j,tc+∆t c
fromthegeneratedγT
c
forallprevioustimesteps,i.e.,η is
,j,t+∆t
=F temporal(γT c;θ
F
temporal). Weagain
emphasize that for each timestep, all the subdomains in the computational domain are updated
beforemovingtothenexttimestep.
FortrainingthetimeintegratorTI,weutilizeamulti-stepobjectivefunctiontofurtherincrease
therobustnesofthetrainingprocess. Ratherthanupdatingweightsaggresivelybasedonprediction
error of one time step, we let TI accomplish forward pass on multiple steps and then compute
the error of these steps together for weight updates. For our runs, we find that a weight update
afteraccumulationoflossfor10timestepsprovidesagoodcompromisebetweentherobustnessof
the model and GPU memory requirement for the training. Furthermore, we utilize a curriculum
learning(CL)basedapproachforrobustweightupdates. TheCLtrainingmechanicswasoriginally
proposed by Bengio et al.31 in the context of natural language procesisng to mitigate the error
accumulationinautoregressivemodels. ResultsfromGhuleetal.32 showedpreliminarysuccces
in using the CL approach for transient PDE modeling, with improvement in generalization and
exptrapolationbyadaptingCLtoFNOs. Inshort,theCLtrainingmechanicrandomlypickseither
s,true
the latent embeddings encoded from true solution distribution, η , or the latent embeddings
I
basedon theTIprediction,ηs , astheinput forany timestepinthe timehistory. Theprobability
I
with which the model’s prediction is selected for a given timestep is gradually ramped up as the
modeltrainingprogresses. Initially,the modelusesgroundtruthdataasinputfor stabletraining
forcertainnumberofepochs. Astrainingadvances,themodelprogressivelystartsusingitsown
predictionsasinput.
7Transient-CoMLSim
C. Detailsofinferencemethdology
FortheinferenceofeachtransientPDEsample,westartwithlatentembeddingsofinitialhistory
solutions and condtions, {ηs,ηc }, forth history time steps,t ∈{0,∆t,...,th∆t}, encoded by the
t t
trained solution and condition encoders, Es and Ec, respectively. We apply the forward pass on
eachsubdomainattimestept sothatthesolutionlatentembeddingsηs ofsudomainsinthe
c tc+∆t
entirecomputationaldomain(Ω )arepredicted. Withthelatestpredictedηs addedtothetime
G tc+∆t
history solutions, we continue unrolling in time to predict the solutions for the next timesteps.
The unrolling of solutions over time occurs entirely in the latent space, providing users with the
flexibilitytodecodeonlythetimestepsofinterest.
AEandTIaretrainedseparatelyfollowingourdescriptionsinsectionIIAandIIB,respectively.
AsTIlearnsinthelatentspaceofPDEsolutions,wefirsttraintheAEanduseitforencodingand
decodingsolutionfieldswhiletrainingtheTI.Theinferencemethdologyisexplainedindetailin
algorithm 1. The methodology startswith the intialcondition andsolution distributions ofthe first
thtimesteps. Thetransient-CoMLSimdiscretizestheentirephysicaldomainΩ intosubdomains
G
Ω ,Ω ,...,Ω . ThetrainedsolutionencoderEs andconditionencoderEc areusedtoencodethe
1 2 N
solutionandconditiondistributionofeachsubdomainΩ ,n∈{1,2,...,N}intoηs andηc ,forthe
n n,t n,t
firstthtimestepst ∈{0,1,...,th}. Afterobtainingallthelatentembeddingsoftheinitialhistoryof
timesteps,theTIcanpredictthesolutiondistributionofthenextstepsinanautoregressivewayuntil
thelasttimestep. Foreachtimestep,thetrainedencoderproducestheconditionembeddingηcfrom
theinputdata. ThetrainedTIpredictsthesolutiondistributionofeachsubdomainsforthenexttime
stept +∆t followingequation3. Inequation3,thespatialinformationneededtoupdatethecentral
c
subdomain for the next timestep is obtained from neighboring subdomains, as illustrated in figure
3. Ateachtimestep,wepredictthesolutionencodingacrossallsubdomainsinthecomputational
domain. Thismethodfacilitatesthepropagationofinformationthroughouttheentiredomain. After
obtainingtheencodingsoftheentire timesequence,thedecoderDs decodesthelatentembeddings
ηs toηs intosolutiondistributionpatchesandthesubdomainsareputbackintotheoriginal
n,th n,th+T∆t
computational domain to compose the solution predictions at timestepsth,th+∆t,···,th+T∆t.
At inference, one can also impose periodic and Dirichlet boundary condition in the latent space,
respectively,asfollows:
ηs =ηs , (4)
start,tc end,tc
8Transient-CoMLSim
ηs =ηs =ηs. (5)
start,tc start,tc 0
Intheequationsabove,startandenddenotethefirstandlastsubdomainsalongtheaxiswhere
theboundaryconditionisapplied.
Algorithm1Inferencemethodologyoftransient-CoMLSim.
DomainDecomposition: DomainΩ intoΩ ,Ω ,···,Ω
G 1 2 N
fortimestept ∈{0,1,...,th}do ▷Initializewiththeconditionandsolutionhistory
forallsubdomainn∈{1,2,...,N}do
Encodeinitialconditions: ηc =Ec(c )
n,t n,t
Encodeinitialsolutions: ηs =Es(s )
n,t n,t
t ←th
c
whiletimestept <finaltimestepth+T∆t do
c
forsubdomainsΩ ∈Ω do
i G
FetchhistoryoflatentvectorsinneighborhoodofΩ : ηs ,ηc
n I,T
s
I,T
c
ComputesolutionencodingsofΩ att +∆t byTI:ηs =TI(ηs ,ηc ;θ)
n c n,tc+∆t I,T s I,T c
t ←t +∆t
c c
DecodePDEsolutionsafterinferenceinthelatentspaceforthtoth+T∆t.
D. Descriptionofdatasets
TABLE I. Summary of datasets employed in this study. N ,N denote the number of dimensions and
d var
numberofvariables,respecitvely. N ,N ,N ,N correspondtothenumberofgridpointsinx,y,z,t directions,
x y z t
respectively. N andN denotethenumberoftrainingandtestsamples,respectively.
train test
Dataset N N N N N N N N
d var x y z t train test
2-DShallowWater 2 1 128 128 – 100 800 100
2-DDiffusion-Reaction 2 2 128 128 – 100 800 100
2-Dvortex-flow 2 1 64 64 – 100 800 100
3DAdditiveManufacturing 3 2 16 200 200 400–1000 48 6
9Transient-CoMLSim
Wetestourtransient-CoMLSimmodelonfourdifferentdatasets,eachofwhicharegovernedby
adifferentsetofPDEs,asshownintableII.Abriefdescriptionofeachdatasetisprovidedbelow:
2-D Shallow-Water Equation: The shallow water equation (SWE) dataset is sourced from
PDEBench33 and offers a framework for simulating free surface flows. The dataset represents a
2-D radial dam break scenario, initializing the circular bump’s initial height at the center of the
domainas:
h(t =0,x,y)=1.0∀r≥r andh(t =0,x,y)=2.0∀r<r , (6)
c c
where r is uniformly sampled from the uniform distribution U (0.3,0.7). These simulations
c
accuratelyportrayaradiallypropagatingshockfrontastimet progresses. Thedatasetconsistsofa
singlevariableh,denotingtheheightofthewaterlevelinthe2-Ddomainatdifferenttimestepst.
The2-D SWEPDEisused tomodelcomplexphenomenalike tsunamis andfloodingevents, where
massandmomentumconservationmustbemaintained,evenacrossshocks. Thesecharacteristics
make2-DSWEdatasetparticularlydifficult,asitsrequiresaccuratehandlingofnonlineardynamics
anddiscontinuitiesinthetemporal2-Dsolution.
2-D Diffusion-Reaction Equation: The 2-D diffusion-reaction equation, also taken from
PDEBench33, consists of two non-linearly coupled variables, u(x,y,t) and v(x,y,t), that evolve
accordingtothefollowingparabolicPDEequations,
∂u (cid:16)∂2u ∂2u(cid:17) ∂v (cid:16)∂2v ∂2v(cid:17)
=D + +R , =D + +R . (7)
u u v v
∂t ∂x2 ∂y2 ∂t ∂x2 ∂y2
Here,R =u−u3−k−vandR =u−vwithk=5×10−3,asgiveninTakamotoetal.33. The
u v
intial conditions for both u and v are generated as Gaussian noise sampled from N (0,1). Thus,
thedatasetconsistsoftwovariablesthatevolvetogetherparabolicallyinspaceandtime. The2-D
diffusion-reactionproblemisparticularlychallengingbecauseitdemandsthatamachinelearning
modelaccuratelycapturethenonlinear,time-dependentcouplingbetweentwointeractingvariables.
2-D vortex-flow: The 2-D vortex-flow dataset represents spatiotemporally evolving 2-D Navier
Stokesequationsonaperioddomain:
∂ω ∂ω ∂ω 1 (cid:16)∂2ω ∂2ω(cid:17)
+u +v = + + f(x,y), (8)
∂t ∂x ∂y Re ∂x2 ∂y2
∂u ∂v
+ =0, (9)
∂x ∂y
10Transient-CoMLSim
where ω is the vorticity and u,v are the velocity components. In our work, we set f =
0.1(sin(2π(x+y))+cos(2π(x+y)) and Re = 1000. The vorticity field ω (x,y) is initialized
0
att =0usingGaussianrandomfield fortheentiredataset. Inourdatasetforthe 2-Dvortex-flow,
wegeneratedataat256×256resolutionona[0,1]×[0,1]squareboxanddownsampleitto64×64
grid. Weareinterestedinobtainingthespatiotemporalevolutionofω(x,y,t). The2-Dvortexflow
isacanonicaldatasetusedwidelyintestingphysics-basedMLmodels2,14,15,34.
3-DAdditive Manufacturing: The3Dadditive manufacturingdatasetillustratesan industrial
scenariowherethetemperature(T)distributionofametalplateundergoeschangesasalasersource
movesoveritduringthe3-Dprintingprocess. Thisevolvingtemperaturefield,influencedbylaser
motion,caninducerapidheatingandcooling,resultinginthegenerationofstrainandstresswithin
amanufacturedpart. Consequently,thesefactorscansignificantlyimpactthestructuralintegrityof
thefinalproduct. ThePDEequationthatgovernsthetemperaturevariationis:
∂T (cid:16)∂2T ∂2T ∂2T(cid:17)
ρC =k + + +Q(x,y,z,t), (10)
∂t ∂x2 ∂y2 ∂z2
where Q(x,y,z,t) represents the moving laser source and α =k/ρC is the thermal diffusivity
of the metal. Here, k,ρ and C are the thermal conductivity, density, and specific heat, of the
material. In our work, k,ρ,C, and laser power parameters stay constant while the laser motion
patternchangesacrossthedifferentsamples. Hence,N variesacrossdifferentsamples. Fortraining,
t
we keep N =400 across all training samples. However, at inference, we let the model run for
t
the entire duration of the actual simulation. This can lead to N going as large at 1500−1600
t
at inference – approximately 4× the temporal window seen by the model during training. It is
important to note that, given the three-dimensional nature of the additive manufacturing dataset,
it is prohibitively expensive to generate N or N of ∼O(100). Hence, during training, we
train test
augment12simulationstogenerate48trainingsamplesbyperformingtworeflectionsalongthe
x−zandy−zplanesandone90◦ rotationaboutthez-axis. Duringinference,wetestthemodel’s
performanceon6non-augmentedsimulationswithvaryingtimehistoriesandspatialdomainsizes,
asmentionedintableII.
E. Descriptionofbaselinemodelsanderrormetric
Ourtransient-CoMLSimiscomparedagainstthefollowingbaselinemodelstoassessitsperfor-
mance.
11Transient-CoMLSim
TABLEII.Summaryofsimulationparametersfortestsamplesof3-Dadditivemanufacturingdataset.
Index N N N N
x y z t
AM1 200 200 16 298
AM2 200 200 16 668
AM3 200 200 16 417
AM4 296 296 16 1600
AM5 544 544 16 601
AM6 104 1320 16 601
U-Net: The U-Netmodel30 along with pushforwardtechnique26 is used, similar to the imple-
mentationofTakamotoetal.33. AsdiscussedinTakamotoetal.33,theautoregressivemethodresults
intotraininginstabilityfor U-Net. Toavoid this,wehaveusedthe pushforwardtechniquetotrain
themodel, followingTakamoto etal.33. In ourstudy,weuse apushforward of20timesteps. The
gradientsarecalculated onlyforthelast 20timesteps. At training,themodel rollsoutpredictions
similar to an autoregressive model, but only the last 20 steps are used for backpropagation. The
U-Netmodelhas7765057trainableparameters. TheU-Netmodeltakesatimehistoryofth=10
stepsasinputtopredictthenexttimestep.
FourierNeuralOperator: FNO2 isan established physics-basedMLapproachthat learnsthe
forwardpropagatorforPDEs. TheimplementationofFNOinthisstudyissameasinTakamotoet
al.33 withinitial10timestepsusedasinputtothemodelwhilethemodelunrollsfortheremaining
N −10timestepsautoregressively. Themodelhas465557parametersthatcanbetrained.
t
Weusethenormalizedrootmeansquarederror(nRMSE)toquantifytheaccuracyofpredictions
fordifferentPDEsacrossvariousmodels.
(cid:13) (cid:13)
nRMSE=
1 N ∑test N ∑var ∑Nt (cid:13)u pred,t,nvar,n−u gt,t,nvar,n(cid:13)
2 (11)
(cid:13) (cid:13)
N test·(N t−th)·N var n=1nvar=1t=th+1 (cid:13)u gt,t,nvar,n(cid:13) 2
nRMSE normalizes the RMSE based on the ℓ2 norm of the ground truth data, making it
dimensionlessandcomparableacrossdifferentdatasets. Thisensuresthattheerrormetricremains
consistent,irrespectiveofthescaleofthedata,providingamoremeaningfulassessmentofmodel
performance.
12Transient-CoMLSim
III. RESULTS
This sectionpresents results obtained from ourexperiments,focusing on theevaluationof the
accuracy,generalizability,andextrapolationcapabilityofourframework. Weconductcomparisons
withthebenchmarkmodelsoutlinedintheprecedingsection.
A. Accuracycomparisonwithbaselinesfortheinterpolationtask
TABLEIII.ComparisonofnRMSEamongtransient-CoMLSim,FNOandU-Netmodelsonthefourdatasets.
For FNO and transient-CoMLSim, gradient updates are performed after every 10 timesteps. FNO and
transient-CoMLSimbotharetrainedwith10timestepsashistoryandU-Netistrainedwithapushforward
timestepwindowof20.
Dataset Transient-CoMLSim FNO U-Net
2-DShallowWater 1.96×10−3 3.10×10−3 8.54×10−2
2-DDiffusion-Reaction 5.32×10−2 2.07×10−1 3.73×10−1
2-DVortexFlow 9.67×10−2 2.09×10−1 6.31×10−1
3-DAdditiveManufacturing 5.47×10−2 − −
TableIIIpresentsthenRMSEaveragedoverallinferredtimestepsanddatasamplesforthefour
PDEs described in the previous section. For all the PDEs, we observe that transient-CoMLSim
performssignificantlybetterthanboth theFNOandU-Netbaselines. Eachdataset presentsunique
challenges: (a) the 2-D SWE dataset requires capturing a propagating wave front, (b) the 2-D
diffusion-reaction dataset involves complex non-linear coupling between the u and v variables, (c)
the 2-D vortex-flow dataset is chaotic in nature, and (d) the 3-D additive manufacturing dataset
involves capturing the complex laser motion patterns for extended time durations. Transient-
CoMLSimoutperformsthebenchmarkmodelsinallthesecases. Animportantpointtonoteisthat
both transient-CoMLSim and FNO perform optimization steps after accumulating gradients for
10timesteps. WhileFNOoutperformedtransient-CoMLSimonthe2-Dvortex-flowdatasetwhen
FNO’sweightoptimizationwasperformedafteraccumulatinggradientsforupto100timesteps,this
approachmightberestrictiveon(a)architectureswithlimitedGPUmemoryand(b)forPDEsthat
arethree-dimensionalinnature. Insuchcases,transient-CoMLSimremainsamoreattractiveoption,
13Transient-CoMLSim
(a) (b)
2-D SWE
2-D Vortex Flow
(c) (d)
2-D Diffusion-Reaction
3-D Additive Manufacturing
FIG. 4. Evolution of nRMSE as a function of timesteps for the four datasets. For panels (a)−(c), all
modelsaretraineduntilT =100timestepsandareinferredfromt =11to100onthetestsamples. Inpanel
(d),modelsaretrainedfor400timestepsandinferreduntiltheendofthetimehorizonfortherespective
simulation. Asmentionedinthetext,wewereunabletofitFNOorU-Netontheadditivemanufacturing
datasetusingourcomputeresources. Therefore,panel(d)onlyshowsthenRMSEevolutionforthetransient-
CoMLSim.
eventhoughFNOcouldpotentiallyachievebetterperformancewithlargergradientaccumulation.
Forotherdatasets,FNOunderperformedcomparedtotransient-CoMLSimevenwhenFNOweight
optimizationwasperformedaftergradientaccumulationfor100timesteps(notshownhere). The
subdomainsizeforalltherunsoftransient-CoMLSimissetas8×8for2-Ddatasetsand8×8×8
for 3-D dataset. The size of the latent embedding is set to 16 for 2-D datasets and 8 for the 3-D
additive manufacturing dataset. In the limit of larger subdomain sizes, our framework begins to
resembleFNOorU-Net-likearchitectures,capturingbroaderspatialfeaturesbutlosingsomeof
thegranularitythatsmallersubdomainsprovide. Conversely,asthesubdomain size decreases,the
14Transient-CoMLSim
modelincreasinglybehaveslikeatraditionalsolver,effectivelypreservinglocaldetailsbutatthe
costofreduceddatacompression. Throughourexperiments,wefoundthatasubdomainsizeof8
ineachdirectionoffersanoptimalbalance,compressingthedatasetefficientlywhilemaintaining
accuracy. The subdomain size of 8 in each direction effectively captured the essential dynamics of
thesystemwithoutexcessivelysmoothingthesmall-scalefeatures
Due to the significantly larger domain size and the three-dimensional nature of the additive
manufacturing dataset compared to the 2-D datasets, fitting the FNO and U-Net model and data
on the available GPU was challenging. Furthermore, the laser power source in the 3-D additive
manufacturingdatasetishighlylocalizedforanygiventimestep,makingitunsuitableforspatial
subsamplingtofitonavailablecomputeresourcesfortheFNOandU-Netmodels. Themethodology
usedinPDEBench33 calculatesthelossforeachtimestepandaccumulatesittocomputegradients
aftertheentiretrajectoryisrolledout. Giventhelargedomainsizeandnumberofroll-outsteps,we
encounteredout-of-memory(OOM)errorsforbothFNOandU-Net,evenwithabatchsizeofone.
Hence,withtheavailableresources,wecouldnottrainthebaselinesaspertheabovemethodology
forthe 3-Dadditivemanufacturing dataset. Toovercome theOOMerror, wemodifiedthe training
mechanism by calculating gradients for each time step instead of all at once. This adjustment
allowedustotrainthemodelswithabatchsizeoftwosamples. Theperepochtimeforthissetupis
approximately50minutes,makingtrainingfor500epochs,assuggestedinFNO2,impractical. In
contrast,themethodproposedinourpaperiswell-suitedtohandlesuchindustrialusecases. Hence,
wecouldnotreportthebaselinecomparisonforthe3-DadditivemanufacturingdatasetintableIII.
Figure 4(a,b,c) show the evolution of nRMSE overtime for 2-D datasets when all themodels
aretraineduntilT =100timesteps. Forthisplot,weaveragenRMSEoverallthetestdatasamples
foragiventimet. Transient-CoMLSimandFNObotharetrainedwithtimehistoryof10timesteps.
BetweentheU-NetwithpushforwardandFNO,FNOperformsbetterandaccumulateslesserror.
However,transient-CoMLSimsignificantlyoutperformsbothonthetestdataset,demonstratingthat
itisabettertemporalinterpolatorframeworkcomparedtothebaselines.
Since the 3-D additive manufacturing dataset includes test samples that evaluate the model’s
extrapolation capability to larger domain sizes and longer timesteps, we present the nRMSE
evolutionforalltestcasesinfigure4(d). Itcanbeobservedthatourmodelconsistentlypredicts
within a5−10%error rangefor all domainsizes and timesteps, eventhough it wastrained on only
400timesteps.
Figures5,6, 7presentthreerandomlyselectedcontoursobtained usingtransient-CoMLSimat
15Transient-CoMLSim
FIG.5. Randomlyselectedthreetestsamplesatt =100randomlyselectedfrom2-Dshallowwaterdataset.
lasttimestepsfromthe2-Dshallowwater,2-Dvortex-flow,and2-Ddiffusion-reaction,respectively.
Fromthecontours,itisevidentthatthetransient-CoMLSimcapturesthephysicsoftherespective
2-Ddatasetsquiteaccuratelythroughoutthecourseoftheirrespectivetemporalevolution.
B. Extrapolationtounseentimestepsandbiggerdomainsizes
To test the performance on unseen timesteps, we train the transient-CoMLSim model on
timesteps t ∈ [10,70] and unroll the model up to T = 100 at inference. Table IV presents the
nRMSE, calculated from 11, for SWE, diffusion-reaction, and vortex-flow datasets. Transient-
CoMLSim is compared against FNO. We do not perform the extrapolation runs for the U-Net
16Transient-CoMLSim
FIG.6. Randomlyselectedthreetestsamplesatt =100from2-Dvortex-flowdataset.
because,fromtableIII,weinferthatFNOoutperformsU-Netintermsofaccuracy. Similartotable
III,timehistoryandnumberoftimestepsoverwhichgradientaccumulationisdonearesetto10for
bothmodels. TableIVshowsthatthetransient-CoMLSimoutperformsFNOintermsofitsability
toextrapolatetounseentimestepsduringinference.
Table V presents the nRMSE of the transient-CoMLSim model on the six test datasets of the
3-D additive manufacturing case. The number of timesteps differ from that used during training
– N =400 duringtraining forthe 3-Dadditivemanufacturing case. Additionally, datasetsAM4,
t
AM5,andAM6alsotestthemodel’sabilitytoextrapolatetolargerspatialdomains,asdiscussedin
thecontextoftableII.TableVdemonstratesthatthemodelshowsremarkableaccuracyforallsix
testsamples,effectivelyextrapolatingtounseentimestepsanddomainsizes. Figure8presentsthe
17Transient-CoMLSim
u v
FIG.7. Randomlyselectedthreetestsamplesatt =100from2-Ddiffusion-reactiondataset.
TABLEIV.Performanceofthedifferentmodelsonunseentimesteps. Modelsaretrainedfort ∈[10,70]and
areinferredont ∈[10,100].
Dataset Transient-CoMLSim FNO
2-DShallowWater 6.42×10−3 1.40×10−2
2-DDiffusion-reaction 3.97×10−2 5.65×10−2
2-Dvortex-flow 8.37×10−2 1.31×10−1
temperature contours at the top-most plane for all six test samples, at the end of their respective
simulation time. The model faithfully captures the spatial distribution of the temperature in the
3-Dadditivemanufacturingcase,similartoits2-Dcounterpartsthatarediscussedintheprevious
section. Figure9showsthetemporalevolutionofthemeltpooldepth,aglobalquantity,forallsix
testsamples. Inthecontextofadditivemanufacturing,themeltpooldepthisthedepthtowhich
thetemperaturegoesbeyondmeltingpointduringsimulation. Transient-CoMLSimdemonstrates
remarkableaccuracyincapturingthetemporaltrendofthemeltpooldepthforallsixtestcases.
18Transient-CoMLSim
AM1
(a))
AM2
(b))
(c) AM3
(d) AM4
AAM5
(e)
AM6
(f)
FIG.8. Testsamplesattheendoftheirrespectivesimulationtimesforthe3-Dadditivemanufacturingcase.
Temperaturecontouratthetop-mostplaneisshown.
19Transient-CoMLSim
TABLEV.Performanceofthetransient-MLSolveron3-Dadditivemanufacturingdataset. Modelperforms
temporalextrapolationforsamplesAM2andAM3andbothspatialandtemporalextrapolationforAM4−6,
asdiscussedintableII.
Index N Transient-CoMLSim
t
AM1 298 4.24×10−2
AM2 668 7.02×10−2
AM3 417 5.07×10−2
AM4 1600 5.16×10−2
AM5 601 3.97×10−2
AM6 601 5.80×10−2
(a) (b) (c)
AM1 AM2 AM3
(d) (e) (f)
AM4 AM5 AM6
FIG.9. Comparisonbetweenthegroundtruthandtransient-CoMLSimpredictionofmeltpooldepthasa
functionoftimestepsfortestdatasamplesof3-Dadditivemanufacturingcase.
C. Ablationstudiesfortransient-CoMLSimon2-DSWEdataset
Wenotethatthetimeintegratorandautoencodersaretheprimaryworkhorseofthetransient-
CoMLSimmodel. Tothisend,tableVIpresentstheresultsofvaryingthreeimportanthyperparam-
eters: (a) latent size, (b) time history, and (c) training window, on the quality of the results. We
chose2-DSWEfortheablationstudy. Foreachsetofrunswhereagivenhyperparameterisvaried,
20Transient-CoMLSim
TABLEVI.AblationstudiesbasedonnRMSElossdemonstratingtheimpactofvaryingparameterson2-D
SWE:latentsize,trainingtimewindow,andtimehistory.
Latentsize(l) l=8 l=16 l=24
TestError 1.98×10−3 1.96×10−3 1.95×10−3
Timehistory(th) th=5 th=10 th=20
TestError 2.21×10−3 1.96×10−3 1.52×10−3
Trainingwindow t ∈[10,100] t ∈[10,70] t ∈[10,40]
TestError 1.96×10−3 6.42×10−3 5.00×10−2
thedefaultvaluesforotherhyperparametersareasfollows: latentsizel =8,timehistoryth=10,
andtrainingwindowt ∈[10,100].
Intheablationstudyonlatentsize,weobservethatwhilethetesterrordecreasesslightlywith
increasinglatentsize,thesensitivityofthemodelperformanceisnotdrasticfortheSWEcase. This
suggeststhattheoptimallatentsizemayvarydependingonthespecificapplication. Inthisstudy,
allthreetestedsizes–8,16,and24–performedsimilarlywell. Fromamodelreductionperspective,
a larger latent size may introduce noise into the embedding, while a too-small latent size might
overlookimportantlarge-scalefeatures. Hence,choosingtherightlatentsizerequiresabalanceto
capturecriticalfeatureswithoutaddingunnecessarycomplexityandisveryspecifictothedynamics
of a dataset. For time history, table VI shows a clear trend: as the time history included in the
trainingprocessincreases,themodelaccuracyimproves. Alongertimehistoryprovidesthemodel
withmoretemporalinformationofthehistory,enablingittocapturethedynamicsofthesystem
more effectively. By understanding the evolution of the system over an extended period of time
history,themodelcanbetteranticipatefuturestates,leadingtoimprovedpredictions. Conversely,
when the training time window is reduced from t ∈ [10,100] to t ∈ [10,70] to t ∈ [10,40], the
predictionaccuracyofthemodeldeteriorates. Thisdeclineoccursbecausetheshortertimewindow
limitsthe amountoftemporal informationavailable, makingitharder forthemodel toaccurately
capturetheunderlyingpatternsandpredictfuturebehavior.
21Transient-CoMLSim
IV. CONCLUSIONS
In this work, we have proposed a domain decompisition-based deep learning framework to
accuratelymodeltransientandnonlinearPDEs,buildinguponitspredecessorCoMLSim8. Hence,
we name the framework transient-CoMLSim. The transient-CoMLSim framework utilizes a
CNN-based autoencoder architecture to obtain latent embeddings of the solution and condition
variables. Unlikealotofexistingphysics-MLframeworks,theautoencoderarchitectureoperates
onsubdomainsinsteadoftheentiretyofthecomputationaldomain. Eachofthesesubdomainsisa
collection of computational grid points, e.g., a subdomain of dimension 8×8×8, comprises of
512computational gridpoints. Thereafter,anautoregressivemodelcomposed offullyconnected
layers process these embeddings to obtain latent embeddings for the future timesteps. Both the
autoencoder and autoregressive model are trained separately and are used in conjunction during
the inference. Derivinginspiration fromthe successof previousworks31,32, weemploy curriculum
learning(CL)techniquewhiletrainingtheautoregressivemodelthatgraduallyallowsthemodel
to use its own predictions as an input, resulting in increased robustness and accuracy for long
timehorizonunrolling. TheframeworkalsoallowsforimposingDirichletandperiodicboundary
conditionsinthelatentspace.
Wetestedthetransient-CoMLSimonfourdatasetsthatexhibitdifferentphysicaldynamics: the2-
Dshallowwaterequation,2-Ddiffusion-reaction,2-Dvortexflow,and3-Dadditivemanufacturing.
The last dataset, 3-D additive manufacturing, is an industrial use case designed to evaluate the
transient-CoMLSim’sabilitytoaccuratelypredicton(a)out-of-distributiondomainsizesand(b)
timehorizonsmuchlargerthanthoseseenduringtraining. Forthe2-Ddatasets,wedemonstratethat
ourmodeloutperformstwopopularmodelswidelyusedinphysics-baseddeeplearning: U-Net30
andtheFourierNeuralOperator(FNO)2,inbothtemporalinterpolationandextrapolationtasks.
Forthe3-Dadditivemanufacturingdataset,weencounteredmemorylimitationswhenattempting
tofitFNOorU-NetonasingleGPU(32GBcapacity),duetothe3-Dnatureoftheproblemand
the large number of rollout steps required during training. However, the transient-CoMLSim’s
ability to operate (a) in the latent space and (b) on local subdomains allows us to successfully
trainandinferonthe3-Dadditivemanufacturingdataset. Transient-CoMLSimaccuratelypredicts
the temperature distribution resulting from a moving laser source for time horizons 3−4 times
longer than those used during training (see figures 9 and 4d). Additionally, due to its capability
tooperate andtrain onsubdomains, transient-CoMLSimcan betrained onsimulationsof smaller
22Transient-CoMLSim
computationaldomainsizesandthenusedinferonsimulationswithlargercomputationaldomain
sizes(seefigures8and9).
We also performed ablation studies on the time integrator and autoencoder network for the
2-D SWE dataset, varying key parameters such as the latent embedding size, the time history
(th) provided to the TI network during training, and the time window seen by the model during
training. Our findings indicate that increasing the time history (th) leads to an improvement in
the model accuracy, as it allows the network to capture more temporal dependencies and time
history information. Similarly, expanding the training time window improves model accuracy,
suggestingthat abigger temporalwindow enablesthemodel tobetter understandandpredict the
evolutionofcomplex systems. These resultsemphasizetheimportanceofcarefullyselectingTI
hyperparameterstooptimizemodelperformance. Forthe2-DSWEcase,wefoundthatthemodel’s
performance wasnot particularly sensitive to thelatent size. However,this sensitivity could vary
withdifferentdatasets.
Infuturework,thecurrentframeworkcanbeenhancedbyintegratingRunge-Kuttamethodsfor
moreaccuratetime-steppingandincorporatingNeumannboundaryconditionstobroadenthescope
ofapplicableproblems. Wealsoplantoexplorethe possibility oftrainingthetimeintegratorand
autoencoderinconjunction,therebycreatingamoretightlycoupledframeworkthatcouldimprove
overallpredictionaccuracyandstability. Additionally,extendingthisapproachtounstructuredgrids
is a natural progression, allowing for the modeling of more complex geometries and real-world
scenarios. Theseadvancementswillbothincreasetherobustnessoftheframeworkandexpandits
applicabilitytoawiderrangeofchallengingcomputationalproblems.
DATAAVAILABILITYSTATEMENT
Thedatathatsupportthefindingsofthisstudyareavailablefromthecorrespondingauthorupon
reasonablerequest.
REFERENCES
1M.RaissiandG.E.Karniadakis,“Hiddenphysicsmodels: Machinelearningofnonlinearpartial
differentialequations,”JournalofComputationalPhysics357,125–141(2018).
23Transient-CoMLSim
2Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anand-
kumar, “Fourier neural operator for parametric partial differential equations,” arXiv preprint
arXiv:2010.08895 (2020).
3Z. Li, D. Z. Huang, B. Liu, and A. Anandkumar, “Fourier neural operator with learned de-
formationsforPDEsongeneralgeometries,”JournalofMachineLearningResearch24,1–26
(2023).
4L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis, “Learning nonlinear operators via
deeponetbasedontheuniversalapproximationtheoremofoperators,”Naturemachineintelligence
3,218–229(2021).
5T. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. W. Battaglia, “Learning mesh-based
simulationwithgraphnetworks,”arXivpreprintarXiv:2010.03409 (2020).
6A.Sanchez-Gonzalez,J.Godwin,T.Pfaff,R.Ying,J.Leskovec, andP.Battaglia,“Learningto
simulatecomplexphysicswithgraphnetworks,”inInternationalconferenceonmachinelearning
(PMLR,2020)pp.8459–8468.
7K. Stachenfeld, D. B. Fielding, D. Kochkov, M. Cranmer, T. Pfaff, J. Godwin, C. Cui, S. Ho,
P.Battaglia, andA.Sanchez-Gonzalez,“Learnedcoarsemodelsforefficientturbulencesimula-
tion,”arXivpreprintarXiv:2112.15275 (2021).
8R. Ranade, C. Hill, L. Ghule, and J. Pathak, “A composable machine-learning approach for
steady-statesimulationson high-resolutiongrids,”Advancesin NeuralInformationProcessing
Systems35,17386–17401(2022).
9R.Ranade,H.He,J.Pathak,N.Chang,A.Kumar, andJ.Wen,“Athermalmachinelearningsolver
forchipsimulation,”inProceedingsofthe2022ACM/IEEEWorkshoponMachineLearningfor
CAD(2022)pp.111–117.
10R.Ranade,C.Hill, andJ.Pathak,“DiscretizationNet: Amachine-learningbasedsolverfornavier–
stokesequationsusingfinitevolumediscretization,”ComputerMethodsinAppliedMechanics
andEngineering378,113722(2021).
11H. He, N. Chang, J. Yang, A. Kumar, W. Xia, L. Lin, and R. Ranade, “Solving fine-grained
static3DICthermalwithmlthermalsolverenhancedwithdecaycurvecharacterization,”in2023
IEEE/ACMInternationalConferenceonComputerAidedDesign(ICCAD)(IEEE,2023)pp.1–7.
12E.Zhang,A.Kahana,E.Turkel,R.Ranade,J.Pathak, andG.E.Karniadakis,“Ahybriditerative
numerical transferable solver(HINTS) forPDEs basedon deepoperator networkand relaxation
methods,”arXivpreprintarXiv:2208.13273 (2022).
24Transient-CoMLSim
13A.Kahana,E.Zhang,S.Goswami,G.Karniadakis,R.Ranade, andJ.Pathak,“Onthegeometry
transferabilityofthehybriditerativenumericalsolverfordifferentialequations,”Computational
Mechanics72,471–484(2023).
14Y.Bar-Sinai,S.Hoyer,J.Hickey, andM.P.Brenner,“Learningdata-drivendiscretizationsfor
partial differential equations,” Proceedings of the National Academy of Sciences 116, 15344–
15349(2019).
15D.Kochkov,J.A.Smith,A.Alieva,Q.Wang,M.P.Brenner, andS.Hoyer,“Machinelearning–
accelerated computational fluid dynamics,” Proceedings of the National Academy of Sciences
118,e2101784118(2021).
16K. Um, R. Brand, Y. R. Fei, P. Holl, and N. Thuerey, “Solver-in-the-loop: Learning from
differentiablephysicstointeractwithiterativePDE-solvers,”AdvancesinNeuralInformation
ProcessingSystems33,6111–6122(2020).
17N.Tathawadekar,N.Doan, C.F.Silva, andN. Thuerey, “HybridneuralnetworkPDE solvers for
reactingflows,”arXiv: 2111.11185 (2021).
18S.BrahmacharyandN.Thuerey,“Unsteadycylinderwakesfromarbitrarybodieswithdifferen-
tiablephysics-assistedneuralnetwork,”PhysicalReviewE109,055304(2024).
19B.Ramos,F.Trost, andN.Thuerey,“Controloftwo-waycoupledfluidsystemswithdifferentiable
solvers,”arXivpreprintarXiv:2206.00342 (2022).
20B. List, L.-W. Chen, andN. Thuerey, “Learned turbulencemodelling with differentiable fluid
solvers: physics-based loss functions and optimisation horizons,” Journal of Fluid Mechanics
949,A25(2022).
21S. Coros, M. Macklin, B. Thomaszewski, and N. Thürey, “Differentiable simulation,” in SIG-
GRAPHAsia2021Courses(2021)pp.1–142.
22P.Holl,V.Koltun, andN.Thuerey,“LearningtocontrolPDEswithdifferentiablephysics,”arXiv
preprintarXiv:2001.07457 (2020).
23J.K.GuptaandJ.Brandstetter,“Towardsmulti-spatiotemporal-scalegeneralizedPDEmodeling,”
arXivpreprintarXiv:2209.15616 (2022).
24R. Lam, A. Sanchez-Gonzalez, M. Willson, P. Wirnsberger, M. Fortunato, F. Alet, S. Ravuri,
T.Ewalds,Z.Eaton-Rosen,W.Hu,etal.,“GraphCast: Learningskillfulmedium-rangeglobal
weatherforecasting,”arXivpreprintarXiv:2212.12794 (2022).
25Z.Li,N.Kovachki,C.Choy,B.Li,J.Kossaifi,S.Otta,M.A.Nabian,M.Stadler,C.Hundt,K.Az-
izzadenesheli,etal.,“Geometry-informedneuraloperatorforlarge-scale3DPDEs,”Advancesin
25Transient-CoMLSim
NeuralInformationProcessingSystems36(2024).
26J. Brandstetter, D. Worrall, and M. Welling, “Message passing neural PDE solvers,” arXiv
preprintarXiv:2202.03376 (2022).
27J. Xu, A. Pradhan, and K. Duraisamy, “Conditionally parameterized, discretization-aware
neuralnetworksformesh-basedmodelingofphysicalsystems,”AdvancesinNeuralInformation
ProcessingSystems34,1634–1645(2021).
28O. Boussif, Y. Bengio, L. Benabbou, and D. Assouline, “Magnet: Mesh agnostic neural PDE
solver,”AdvancesinNeuralInformationProcessingSystems35,31972–31985(2022).
29D.Mavriplis,“Unstructuredgridtechniques,”AnnualReviewofFluidMechanics29,473–514
(1997).
30O.Ronneberger,P.Fischer, andT.Brox,“U-Net: Convolutionalnetworksforbiomedicalimage
segmentation,” in Medical image computing and computer-assisted intervention–MICCAI 2015:
18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18
(Springer,2015)pp.234–241.
31S.Bengio,O.Vinyals,N.Jaitly, andN.Shazeer,“Scheduledsamplingforsequenceprediction
withrecurrentneuralnetworks,”Advancesinneuralinformationprocessingsystems28(2015).
32L.Ghule,R.Ranade, andJ.Pathak,“NLPinspiredtrainingmechanicsformodelingtransient
dynamics,”.
33M.Takamoto,T.Praditia,R.Leiteritz,D.MacKinlay,F.Alesiani,D.Pflüger, andM.Niepert,
“PDEBench: An extensive benchmark for scientific machine learning,” Advances in Neural
InformationProcessingSystems35,1596–1611(2022).
34N.Kovachki,Z.Li,B.Liu,K.Azizzadenesheli,K.Bhattacharya,A.Stuart, andA.Anandkumar,
“Neuraloperator: Learningmaps betweenfunction spaceswithapplications toPDEs,”Journal of
MachineLearningResearch24,1–97(2023).
26