Center Direction Network for Grasping Point Localization on Cloths
Domen Tabernik∗1, Jon Muhovicˇ1, Matej Urbas1 and Danijel Skocˇaj1
Abstract—Object grasping is a fundamental challenge in
robotics and computer vision, critical for advancing robotic
manipulation capabilities. Deformable objects, like fabrics and
cloths,poseadditionalchallengesduetotheirnon-rigidnature.
In this work, we introduce CeDiRNet-3DoF, a deep-learning
modelforgrasppointdetection,withaparticularfocusoncloth
objects. CeDiRNet-3DoF employs center direction regression
alongside a localization network, attaining first place in the
perceptiontaskofICRA2023’sClothManipulationChallenge.
Recognizing the lack of standardized benchmarks in the lit-
erature that hinder effective method comparison, we present
the ViCoS Towel Dataset. This extensive benchmark dataset
comprises 8,000 real and 12,000 synthetic images, serving as
a robust resource for training and evaluating contemporary
data-drivendeep-learningapproaches.Extensiveevaluationre-
Fig.1. TheproposedCeDiRNet-3DoFarchitecture.
vealedCeDiRNet-3DoF’srobustnessinreal-worldperformance,
outperforming state-of-the-art methods, including the latest
transformer-based models. Our work bridges a crucial gap,
offeringarobustsolutionandbenchmarkforclothgraspingin absence of a dedicated benchmark hinders effective method
computer vision and robotics. Code and dataset are available comparison. Our motivation stems from this critical need,
at: https://github.com/vicoslab/CeDiRNet-3DoF. drivingustoproposeanovelbenchmarkdatasetandanovel
I. INTRODUCTION method designed to detect grasp points in cloth objects. In
this paper, we focus on grasp points for towel manipulation
In robotics and computer vision, object grasping is a
tasks (folding/unfolding) following the definition of grasp
fundamental challenge crucial for advancing manipulation
points from [10], where only visible physical corners are
capabilities. The variability in shapes of real-world objects
considered valid graspable points while grasp orientation
requires advanced perception algorithms for precise grasp
is a single dimensional angle-of-approach defined relative
point detection, particularly challenging with deformable
to the image plane. Our contributions in this paper are
materials like fabrics due to their non-rigid nature, leading
twofold. First and foremost, we introduce a deep-learning
to non-rigid transformations and self-occlusions.
methodology for identifying grasp points on deformable ob-
In addressing cloth manipulation, traditional robotics re-
jects by extending our previously introduced CeDiRNet [11]
search often focuses on physical grasping or manipulation
with3-DoFextension.TheproposedCeDiRNet-3DoF,which
strategies (e.g., folding/unfolding) and simplifies the envi-
achieved the first place in perception task at the 2nd Cloth
ronment using conventional computer vision techniques [1],
Manipulation and Perception Competition from ICRA 2023,
stillpresentinrecentworks[2].Recently,morerobustdeep-
enables accurate grasp point detection through regression of
learning techniques have been employed to address percep-
center-direction and a separate localization network as part
tion. Some approaches manage it by directly regressing one
of the original CeDiRNet (see, Fig. 1), while the proposed
or two grasp points using neural networks [3]–[5], while
3-DoFextensionenablesestimationofoptimalgrasporienta-
others treat grasp point estimation as a segmentation prob-
tion (i.e., angle-of-approach) and simultaneously improving
lem[6],[7]foredges,hems,collars,orsleeves.Asignificant
grasp point detection.
limitation in current research is the lack of standardized
To address the dearth of benchmark datasets in this
benchmarking, as each study evaluates its solution on a
domain, our second contribution is an extensive, publicly
private dataset, hindering comparison between approaches.
availabledatasetcomprising8,000realimagesbuiltusingten
Efforts to standardize benchmarking [8], [9], such as the
diversetowelsfromHouseholdClothObjects[9]and12,000
recent Cloth Manipulation and Perception Competition [10]
synthetic ones. The proposed dataset, termed the ViCoS
from ICRA, still lack a comprehensive dataset for training
Towel Dataset, encompasses images of diverse towel con-
and evaluating modern data-driven deep-learning methods.
figurations, backgrounds, lighting conditions, and occluding
While computer vision algorithms have demonstrated re-
object clutter captured on a tabletop from a viewpoint sim-
markable capabilities in addressing grasping challenges, the
ulating the robotic perception, thus serving as a benchmark
∗Correspondingauthor.
for training and evaluating modern deep-learning methods
1Members of the Faculty of Computer and Information Sci-
in grasp point detection. The proposed dataset facilitates a
ence, University of Ljubljana, Vecˇna pot 113, Ljubljana, Slovenia
domen.tabernik@fri.uni-lj.si. rigorous evaluation of CeDiRNet-3DoF and its comparison
4202
guA
62
]VC.sc[
1v65441.8042:viXrawith ten state-of-the-art deep-learning methods, including with specific robotic systems that hinder replication of re-
several recent transformer-based approaches. sults. Many methods are evaluated on proprietary datasets,
The remainder of this paper is structured as follows: limiting accessibility and scalability for contemporary deep-
Section II discusses related work, and Section III presents learning techniques. Addressing these challenges is an im-
the methodology. The ViCoS Towel Dataset is introduced portantobjectiveaswestrivetoprovidesolutionsforbench-
in Section IV and used for evaluation and benchmarking in marking and comparing deep-learning methodologies in the
Section V. We conclude with a discussion in Section VI. field.
II. RELATEDWORK III. METHODS
In the field of grasp point estimation for deformable ob- This section details our proposed method for grasp point
jects,traditionalapproacheshavehistoricallyreliedonhighly localization termed CeDiRNet-3DoF that leverages our pre-
engineered features [12]. Grasp point estimation methods viously introduced CeDiRNet [11]. Method adapted specif-
wereemployedincombinationswithBagofFeatures,sliding ically for the regression of graspable points and their cor-
windows, SVM, and engineered measures to define “grasp responding angles-of-approach for robotic arms. We start
goodness” [13]. Others also used Difference-of-Gaussians by presenting the formulation of the original CeDiRNet as
and Gabor features coupled with Harris corners [14]. Tradi- a reference (previous work), then define our novel 3-DoF
tional approaches also persist in recent work for estimating extension designed for the regression of grasp points and
grasp points from point cloud segmentation using wrinkled- angles.
ness measures [2].
A. CeDiRNet
While traditional approaches are known for not requiring
The original Center Direction Network from [11] com-
extensive training data, they often lack robustness for real-
prises of two network modules: i) a dense regression of
world applications. Consequently, deep-learning methods
center-directions and ii) a localization network. We offer a
havebeenincreasinglyexplored.Yamazakietal.[15]applied
conciseoverviewofeachmoduleandreferthereaderto[11]
deep-learning segmentation and VGG19 features for gripper
for a more comprehensive description.
position estimation in towel folding, albeit with a small
a) Dense regression of center-directions: In the first
dataset of 275 images. Demura et al. [16] employed pre-
module, the network regresses directions to the closest
trained YOLO and transfer learning for grasp point estima-
target point from all its surrounding pixel locations. A
tion on folded towels with a dataset of 200 images. Seita et
direction angle ϕ =
tan(cid:0)m−j(cid:1)
is defined for each
al. [5] also used YOLO with additional grasp point layers i,j n−i
pixel location x=(i,j) pointing towards the closest target
for the bed-making task, achieving superior performance
point y=(n,m). A direction angle ϕ is not regressed
compared to the Harris corner detector on a larger dataset
directly but instead predicted as a re-parameterized angle in
of 2,000 images. Corona et al. [4] used a hierarchical tree
C =sin(ϕ) and C =cos(ϕ), therefore regressing two
of CNNs for cloth classification and estimating the 1st and sin sin
fields:
2nd grasp points. They used 2,000 real and 6,000 synthetic
imagesgeneratedfromfourgarments;however,theirdataset C ,C =DenseRegNet(I), (1)
sin cos
lackspublicavailability.Saxenaetal.[3]employedsemantic
segmentation of garments and specialized CNN for each where I ∈ RN×M is the input image, DenseRegNet
garment for grasp point estimation. They proposed a large- is a regression network with two dense output fields and
scale dataset comprising 18k real and 95k synthetic images C sin,C cos ∈RN×M are two dense output fields correspond-
but did not make it publicly available. ing to sin(·) and cos(·) of direction angle ϕ as depicted
in top middle images in Fig. 1. As proposed in [11], the
More recent works continue to leverage deep-learning
original regression network uses an encoder-decoder model,
techniques for grasp point estimation. Ren et al. [6] learned
with ResNet-101 for the encoder backbone and Feature
amodelwithoutrealdatausingsyntheticdepthdataanddo-
Pyramid Network for the decoder, where the last part of
main adaptation, focusing on segmenting graspable regions.
decoder contains a dense output head with convolutions and
Zhu et al. [17] proposed BiFCNet for semantic segmenta-
upsamplingfortheincreasedresolutionoutput(Conv2D3x3,
tion of graspable regions, training on NYU-Depth v2, and
GroupNorm, ReLu, Bilinear upsampling, Conv2D 3x3).
evaluating on a lab coat. Garcia-Camacho et al. [8], [9]
The center-direction module is learned with an L1 loss
recentlyproposedtheHouseholdClothObjectSetforbench-
function:
marking deformable object manipulation but lacks suitable
datasets for training and evaluation. Lips et al. [18] recently L =(cid:88) W (x )·|C (x )−Cˆ (x )|+
ϕ ε i,j sin i,j sin i,j
introduced a large-scale dataset for learning and evaluating
i,j
(2)
modern deep-learning approaches. However, only 2,000 real (cid:88) W (x )·|C (x )−Cˆ (x )|,
images are proposed, leaving only synthetic images of suf- ε i,j cos i,j cos i,j
i,j
ficient quantity for training, albeit generated for high-level
photo-realism with Blender. where the groundtruth data are two dense fields Cˆ and
sin
A notable limitation in the literature is the absence of a Cˆ constructedfromsin(·)andcos(·)respectivelyforevery
cos
comprehensivecomparisonamongapproaches,oftencoupled pixel location, indicating the direction of the pixel towardsthe closest point to be detected. The loss function has an shared except for the final dense output heads. The loss
additional per-pixel weight W (x ) with a cutoff distance function L consists of a loss for center-direction L and a
ε i,j ϕ
thresholdεtobalancethelossbetweendifferentground-truth loss for angle-of-approach L . We additionally balance both
θ
points and between ground-truth points and the background losses using uncertainty weighing [19], resulting in the final
to ensure pixels for every ground-truth point have equal loss:
importance in the learning process. 1 1
L= L + L +logσ σ , (4)
b) Localization from center-directions: The sec- 2σ2 ϕ 2σ2 θ ϕ θ
ϕ θ
ond module is used to extract the exact point loca-
tions from the regressed center-directions fields, which where weighting values σ ϕ and σ θ are additional learn-
is implemented as an additional deep neural network able parameters defining the uncertainty weight for each
O =LocNet(C ,C ),outputtingprobabilitymapfor corresponding loss, thus eliminating the need for manually
cent sin cos
grasp points. Finding local-maxima values in the resulting setting additional hyperparameter weights and preventing
O yields a list of locations of detected points. the domination of gradients from one task to negatively
cent
LocNet is implemented as a lightweight hourglass archi- influencing the other task.
tectureconsistingoffourlevelsinbothencoderanddecoder Bothlosses,L ϕandL θ,useL1 followingtheEq.2,where
and a small number of channels (16 or 32). The localization L θ is applied to (D sin,D cos) fields instead of (C sin,C cos);
network is domain agnostic and can be trained on synthetic however, loss for angle-of-approach L θ is not computed in
data since each problem domain is always effectively trans- the background as is in L ϕ. Instead, we compute L θ only
lated to a common appearance of re-parameterized center within a cutoff distance ε area around ground-truth grasp
direction angles that are the same regardless of the actual points(30×30pixels)andignorelossforpixelsfurtheraway
visualappearanceoftheobjects.Formoredetails,thereader from ground-truth grasp points since they are not relevant
is referred to [11]. andwouldonlydistractfromlearningrelevantpixelsaround
grasppoints.Notethatweignorethebackgroundonlyduring
B. CeDiRNet-3DoF for grasp point localization training; during inference, we regress for all pixels but use
We now detail the proposed 3-DoF extension for the only ones at detected grasp points.
CeDiRNet, resulting in the CeDiRNet-3DoF formulation Wetrainonlytheregressionnetworkanduseapre-trained
that enables the application to the grasp-point localization generic localization network from [11] since the appearance
problem. Grasp points are defined with their location y in of regressed center-direction angles in sin/cos form (which
the image and their single-dimensional angle-of-approach θ isinputforlocalizationnet)doesnotchangeacrossdifferent
for the robotic manipulator. Finding grasp point location is problemdomains,thereforeeliminatingtheneedtofine-tune
implemented as regression of center-direction angles ϕ in localization network for the specific domain, as proposed
sin/cosformpointingtowardsyusingtheoriginalCeDiRNet in [11].
architecture,whilewenowproposetoreusethesameregres- b) Inference: During inference, we apply generic lo-
sion model to also predict the angle-of-approach θ for each calization network to the regressed center-direction fields
predicted grasp point. For every pixel location x =(i,j), (C ,C ) to predict grasp point locations. For each pre-
i,j sin cos
weassignanangle-of-approachvalueθ basedonitsclosest dictedgrasppoint,weestimateitsangle-of-approachbycon-
i,j
corresponding ground-truth grasp point. Similarly to center- vertingtheregressedangle-of-approachvalueatthedetected
(cid:16) (cid:17)
direction, we do not directly regress angle-of-approach θ location from sin/cos to actual angle θˆ=tan−1 Dcos .
but instead re-parameterize it with D sin = sin(θ) and c) Comparison to the original CeDiRNet: CD os min pared
D cos = cos(θ) to avoid regressing periodic angle values to the original CeDiRNet [11], the overall architecture of
while also normalizing to [−1,1]. This follows the same re- CeDiRNet-3DoF remains the same with only two main net-
parametrization from the original CeDiRNet for regressing works:a)aregressionnetworkandb)alocalizationnetwork.
center directions [11]. The regression network now predicts However, as part of our 3-DoF extension, we introduce
two additional fields D related to θ: severalchangestotheregressionnetwork:i)replaceResNet-
101 encoder backbone with ConvNext, ii) add additional
denseoutputheadforregressingsin/cosofangle-of-approach
(C ,C ),(D ,D )=DenseRegNet(I), (3)
sin cos sin cos
θ,andiii)adduncertaintyweights[19]forbalancingcenter-
where C sin,C cos ∈ RN×M are dense output fields for direction L ϕ and angle-of-approach L θ losses.
center-directionanglesϕ,andD ,D ∈RN×M aredense
sin cos
output fields for angle-of-approach θ.
IV. THEVICOSTOWELDATASET
a) Training: TheregressionnetworkDenseRegNetis We propose a novel benchmark dataset for grasp point lo-
trained to simultaneously predict all four dense output fields calizationonclothobjects,termedtheViCoSTowelDataset.
using the same network. However, we use a separate dense The dataset consists of images depicting various towels on
output head for angle-of-approach θ to allow for a certain top of the desk from the perspective of a robotic view
degree of specialization since the output of center direction that was systematically captured in various conditions using
angle ϕ significantly differs from the output of angle-of- Kinect V2. This resulted in 8,000 annotated high-resolution
approach θ. Therefore, all encoder and decoder weights are images (1920 × 1080) with depth information (RGB-D),Fig.2. OverviewofimagesintheViCoSTowelDatasetwithdifferenttowels,cornerconfigurations,backgrounds,clutter,andlightning.
TABLEI
but the examples are more crumpled. All configurations are
CONFIGURATIONS(*OBJECTSNOTPRESENTINTHETRAININGSET).
listedinthesecondrowinTableIanddepictedinthesecond
row in Fig. 2.
bigtowel;smalltowel;checkeredragbig;
c) Backgrounds: Five different tabletop cloths or ob-
Object/cloth checkeredragmedium;checkeredragsmall*;
types cottonnapkin*;linenrag;towelrag; jects were used to increase the variance of the background
wafflerag;waffleragstripes as listed in the third row in Table I and shown in the third
4ontable;2onobject2ontable;2onobject; row in Fig. 2.
Object ontable;1ontableon1onobject;1onobject d) Lighting conditions: Dataset images were captured
positions 1ontable;3onobject1ontable;2onobject;
1xfolded(2visible);2xfolded(1visible) under eight different lighting conditions. We used several
lights positioned around the table (two lights on the right,
festivetablecloth*;greencheckeredtablecloth;
Backgrounds whitedesk;poster;redtablecloth a strong and a weak one, and one on the left), which
were turned on and off in different combinations to obtain
nolights;leftlight;strongrightlight;
Lighting
strongrightlight+leftlight;weakrightlight;weak severaldifferentsceneswithi)poorlights,ii)mildshadows,
conditions
rightlight+leftlight;bothrightlights;alllights iii) strong shadows, and iv) fully illuminated scene without
shadows.LightconditionsarelistedinthelastrowinTableI
and depicted in the last row in Fig. 2. Images with different
which is sufficient for both training deep-learning models lighting conditions were captured automatically; therefore,
and for extensive benchmarking under various conditions. the same scene with the exact same object position, back-
Examples are depicted in Fig. 2, including a depth in the ground, and clutter was captured in eight different lighting
third row. settings.
e) Clutter: Finally, we captured images with and with-
A. Dataset construction
out additional clutter in the scene. Various clutter objects
We systematically varied various cloth objects (i.e., tow-
wereaddedeitheronthedeskandaroundtheclothobjector
els),configurationsofvisiblegrasppoints(i.e.,cornersofthe
directly on the cloth themselves to create additional difficult
towels), backgrounds, lighting conditions, and the presence
sceneswithocclusionofthecloth(seerightsideofthethird
of clutter in the background in all possible combinations. A
row in Fig. 2).
detailed description of each condition is provided below.
a) Objects: Proposed dataset contains objects from the B. Annotations
Household Cloth Object Set [20]. In particular, we included All visible corners of the cloth object were manually
10 towels that were part of the 2nd Cloth Manipulation and annotatedinallimages.Cornerswereannotatedwithapoint
Perception Challenge at ICRA 2023 (see the first row in label and represented a potential grasping point that needs
TableIandFig.2).Weconsidereach visiblephysicalcorner to be detected and localized. We also annotated the angle-
of the towel as a potential graspable point that is the subject of-approach for each corner at 45° outwards relative to the
of detection and localization, according to the rules of the clothsideedges.Thisresultedinatotalof20,784annotation
challenge. points and angles.
b) Object position: Each object was captured in 10
C. Training and testing splits
different positions lying on the tabletop, which simulated
different levels of visibility of individual grasp points. This We held out two towels (checkered rag small and cotton
included fully spread-out objects with all corners visible, napkin) and one background (festive tablecloth) object from
crumpled objects with some corners hidden, and folded the training set, which are then used exclusively for the
objects with overlapping corners. Note that 1 on cloth, 1 on testing, thus ensuring fair evaluation with objects that were
table has a similar configuration to 1 on table, 1 on cloth, notpresentintraining.Allremainingconfigurations(corners,TABLEII
lighting positions, w/ and w/o clutter) are included in both
CEDIRNET-3DOFONVICOSTOWELDATASETWITHABLATIONSTUDY.
the training and testing set. This resulted in 5,120 training
and 2,880 testing images.
Uncertaintyweighing ✓ ✓ ✓ ✓
3-DoFdenseoutputhead ✓ ✓ ✓ ✓ ✓
D. Synthetic training data
Regressingangle-of-approach ✓ ✓ ✓ ✓ ✓ ✓
We also provide 12,000 additional synthetic images as Withpre-trainedsyntheticdata ✓ ✓ ✓ ✓ ✓
RGB-D ✓ ✓ ✓ ✓ ✓
part of the proposed ViCoS Towel Dataset that can be
used to increase the diversity of the training data. Synthetic Precision[%]↑ 79.4 82.4 83.2 83.0 81.5 79.5 80.4
images were generated using MuJoCo* simulation environ- Recall[%]↑ 80.1 82.5 84.3 84.6 80.2 73.0 72.0
F1[%]↑ 77.0 80.0 81.2 81.4 78.0 73.0 72.7
ment, where each image depicts a towel positioned on the Localizationerror[px]↓ 1.7 1.7 1.7 1.6 1.7 1.9 1.7
table at 80 cm distance at 70-90° viewing angle in random Orientationerror[°]↓ - 6.5 6.5 6.4 6.7 6.5 6.5
configurations based on physics simulation, using random
image texture for both towel and backgrounds (12 towel
and 10 background textures). Each towel was resized to is correctly detected if the prediction is within 20 pixels of
between 24 and 45 cm in each dimension and simulated the ground truth, corresponding to 2-4 cm for objects in the
withgrid-positionedellipsoidelementsat3cmintervals.We dataset. If multiple predictions are near one ground truth,
determineditsfinalpositionbysimulatingadropfrom70cm only one is counted as correct; the others are false positives.
abovetheground,randomlyapplyingforcestotheedgesand For correctly detected grasp points, we report localization
corners to crumple the cloth into a complex shape. For a errorinpixels(translationerror)andangle-of-approacherror
diverse set of shadows, directional lighting was positioned in degrees (rotation error).
1 m above the ground with random directions and color
temperatures between 2000° and 6500° K. C. Overall results
TableIIpresentstheresultsofCeDiRNet-3DoF’sapplica-
V. EXPERIMENTS
tion to the ViCoS Towel Dataset for grasp point estimation.
The evaluation was carried out on the ViCoS Towel
Whenusingproposed3-DoFextensionwithRGB-Ddataand
Dataset, and the method was benchmarked against ten state-
pre-trained on synthetic data, our optimal variant achieves
of-the-art deep-learning approaches. We also conducted an
a precision of 83.0%, recall of 84.6%, and an F1 measure
ablation study to substantiate our design choices and the
of 81.4% in grasp point detection. With smaller distance
advantages of incorporating synthetic data and depth infor-
tolerances, F1 reduces slightly to 80.4% at 10 px (1-2 cm),
mation.
and 76.1% at 5 px (0.5-1 cm). The localization error is
A. Implementation details approximately 1.6 pixels, with an angle-of-approach error
ranging between 6-7°. If exclusively testing on novel cloths
We trained separately on synthetic and real data, using
and backgrounds (360 images), the performance mirrors
weights from the synthetic training as initialization for real-
the one achieved on the entire test set, maintaining an
datatraining.WetrainedwithasingleGPU,abatchsizeof4,
F1 measure of around 81.1%. Several examples of good
and a learning rate of 10−4 (Adam optimizer) for 10 epochs
performance in difficult situations are depicted in Fig. 3.
using a polynomial decay (power of 0.9). Fixed image sizes
a) Ablation study: We also assessed several design
were used for training and testing: 512×512 for synthetic
choices (uncertainty weights, dense output head for 3-DoF,
data and 768×768 for real data, to approximately match
disabling angle-of-approach regression) and performance
the difference in object scales in different scenes. Two data
without synthetic data and depth information. Results in
augmentation techniques were also applied—Gaussian blur
the left side of Table II show our design choices’ positive
(σ =[0.5,2]) and color jittering (randomly selected in range
impact: uncertainty weights improve the F1 measure by 0.2
[−0.3,0.3] for brightness, contrast, saturation, and hue),
percentage points (pp),and the dense outputhead for 3-DoF
each with a probability of 0.5. For the backbone, we used
adds1.2ppcomparedtousingasharedheadbetweencenter-
the ConvNext-B model in all experiments unless explicitly
directions and angle-of-approach. Not regressing the angle-
stated otherwise. Where applicable, we also normalized the
of-approachreducestheF1measureby3pp,suggestingthat
depth values to the range [0,1]. CeDiRNet-3DoF achieves
angle-of-approach regression positively influences center-
an inference speed of approximately 20 FPS for half HD
direction regression, possibly acting as regularization and
resolution on NVIDIA A100, and surpasses 8 FPS for full
preventingoverfitting.Combiningallourdesignchoicesalso
HD resolution.
slightly positively influences localization error (+0.1 pixels)
B. Evaluation metrics and the orientation error (+0.1°).
The right side of Table II shows the benefits of synthetic
The evaluation focused on point localization performance
data and depth information. Excluding these leads to a
inthecontextofgrasping.Wemeasuredtwosetsofmetrics:
significantperformancedrop.Withoutsyntheticpre-training,
i) metrics for grasp point detection and ii) metrics for
the F1 measure decreases by 8-9 percentage points (pp),
localizationerrorandangle-of-approacherror.Agrasppoint
and the positive impact of depth information is most evident
*https://mujoco.org/ when synthetic data is used.Fig.3. Severalexamplesofcorrectgrasppointdetection(greencross)andestimationofangle-of-approach(darklinespredicted,lightlinesground-truth).
TABLEIII
PARAMETRICSTUDYFORCEDIRNET-3DOFWITHRGB-D.
(*OBJECTSNOTPRESENTINTHETRAININGSET;CLTR=CLUTTER)
Prec. Recall F1 Loc. Orient.
[%] [%] [%] [px] [°]
4ontable 99.9 95.9 97.6 1.3 4.7
2oncloth,2ontable 96.6 81.1 86.9 1.7 6.7
2oncloth,1ontable 90.9 84.8 86.7 1.5 7.7
2ontable 83.7 93.4 86.6 1.4 7.5
1ontable,1oncloth 90.2 84.1 85.3 1.6 7.0
3oncloth,1ontable 90.2 77.7 81.5 2.3 5.6
1oncloth,1ontable 82.9 77.4 77.6 1.5 8.9
Fig. 4. Examples of false and missed grasp point detections (true as a
2oncloth 82.3 75.2 76.2 1.8 7.2
greencross,falseasaredcross,andground-truthasagreendot).
1xfolded(2visible) 64.2 82.6 70.5 1.3 6.1
2xfolded(1visible) 62.4 87.8 70.4 1.6 4.1
D. Parametric performance study Towelrag 86.2 96.1 89.6 1.6 7.6
Checkeredragmed. 92.0 90.4 89.6 1.5 7.1
We conducted a supplementary parametric performance Smalltowel 94.7 85.3 87.7 1.8 8.1
Checkeredragbig 92.2 85.9 86.7 2.0 5.2
study on individual towels, corner configurations, back-
Bigtowel 85.0 91.4 86.0 1.6 7.6
grounds,andclutter.Results,sortedbyF1score,arereported Wafflerag 86.6 87.2 83.7 1.5 7.4
in Table II. CeDiRNet-3DoF performs best when the cloth Linenrag 78.5 88.3 80.9 1.4 6.0
Waffleragstripes 82.2 83.3 80.7 1.6 5.8
is laid flat with all corners visible and worst with folded
Cottonnapkin* 80.4 84.7 80.4 1.7 5.6
cloths, as folded edges appear as corners but should not be Checkeredragsmall* 81.5 77.8 76.7 1.5 6.3
detected. Different towels and background types result in
Festivetablecloth* 87.1 86.4 84.5 1.6 6.7
smaller variances compared to corner configurations. The Greencheckered
85.8 85.7 83.7 1.7 5.5
highest performance is with towel rag or checkered rag tablecloth
Whitedesk 83.1 80.7 78.7 1.7 5.6
medium towels and the festive tablecloth background, while
Poster 77.7 80.1 76.4 1.5 6.5
thecheckeredragsmalltowelandredtableclothbackground Redtablecloth 74.8 77.0 73.1 1.6 6.7
perform worst. Interestingly, the best background was not
yes 82.4 83.3 80.4 1.7 6.6
in the training set, highlighting the absence of overfitting. no 84.4 85.3 82.6 1.6 6.3
Minimal overfitting is also apparent with the two towels
not in the training set, attaining 77-80% F1 score, which
is close to 81.4% in all test samples, indicating sufficient
of recently proposed key-point detectors tailored for cloth
diversityoftowelsinthedatasettolearnageneralizedmodel.
grasping. In this benchmark, we focus solely on grasp point
Clutter shows minimal impact, demonstrating the robustness
detection, omitting angle-of-approach accuracy assessment
to background clutter, which can occlude grasp points.
since most of the related approaches do not address it. All
E. Comparison with related methods results are presented in Table IV.
Finally, we conduct a comprehensive benchmark of All related methods underwent the same training proce-
CeDiRNet-3DoF and several recent state-of-the-art mod- dure as CeDiRNet-3DoF: pre-training on synthetic images
els. The evaluation encompasses two categories of ap- for 10 epochs, followed by at least 10 additional epochs
proaches that are suitable for grasp point detection: i) on real images. In certain cases, longer training was nec-
point-supervision approaches similar to CeDiRNet and ii) essary for successful convergence. Where applicable, we
bounding box approaches. For evaluation, we selected the also consider different backbones for a fair comparison. The
best-performing recent general methods and two variants methodswereexclusivelyappliedtoRGBdata,astheywere
noitarugfinocrenroC
slewoT
dnuorgkcaB
.rtlCTABLEIV
locations.Inourcase,thedetectionfocusesonamaximumof
RELATEDWORKBENCHMARKONVICOSTOWELDATASET.
fourpointsineachimage,creatingasignificantlyunbalanced
set of points compared to the number of background pixels.
Train Prec. Recall F1
Model Backbone Epoch Depth [%] [%] [%] Purpose-build key-points methods for grasp point detec-
tion on cloths [18], [26] also demonstrated subpar perfor-
CeDiRNet-3DoF ConvNext-B 10 ✓ 83.0 84.6 81.4
(our) ConvNext-L 10 ✓ 82.3 84.5 80.8 mance. A variant from ICRA 2022 workshop [26] with U-
Net achieved only 30% in F1 measure, while the latest RA-
CeDiRNet-3DoF ConvNext-L 10 × 79.7 82.5 78.4
L 2024 journal variant [18] achieved 50%. However, the
(our) ConvNext-B 10 × 81.5 80.2 78.0
best performance was achieved with further changes to the
DINO[21] ConvNext-B 12 × 72.5 79.2 72.7
backbone,attaining66%F1measurewhenusingConvNext-
DeformDETR ConvNext-B 50 × 74.4 75.2 71.6 B.Nevertheless,thisstilllagsbehindCeDiRNet-3DoFby12
ResNet101 50 × 68.8 67.4 65.3
[22] pp.
ResNet50 50 × 68.9 64.3 63.2
b) Bounding box approaches: As bounding box meth-
ConvNext-B 500 × 65.7 62.2 61.2
DETR[23] ResNet101 500 × 64.5 59.6 59.2 ods we included Faster R-CNN [24], YOLO v7 [25],
DETR[23],DeformDETR[22],andDINO[21],withthelat-
ResNext101 10 × 70.0 73.1 68.3
FasterR-CNN ResNet101 10 × 68.6 66.7 64.4 terthreerepresentingstate-of-the-arttransformerapproaches.
[24]
ResNet50 10 × 66.6 71.3 65.8 As a training bounding box, we used a fixed-sized rectangle
YOLOv7[25] yolov7x 20 × 44.8 61.0 48.3 (50 × 50 pixels) centered over the annotation point. For
testing,thecenterofthepredictedboundingboxwasutilized
Clothgrasping ConvNext-B 50 × 68.6 70.6 65.7
keypoints ViT-T[18] 50 × 50.5 57.7 50.6 as the final point prediction.
[18],[26] UNet[26] 50 × 29.3 39.5 30.7 Theleast-performingboundingboxmethodwasYOLOv7
P2PNet[27] VGG16 20 × 46.7 53.8 46.2 (48% F1), comparable to CenterNet++ but significantly lag-
ResNext101 24 × 50.6 56.2 49.4 ging behind CeDiRNet-3DoF (78% F1). Faster R-CNN with
CenterNet++ ResNet101 24 × 48.0 45.5 43.0 a ResNext101 backbone yielded much better results (68%
[28] ResNet50 36 × 39.5 42.4 37.0
F1). Among transformer-based methods using ConvNext-B,
Swin-L 24 × 43.9 33.1 35.1
DETR achieved a maximum F1 measure of 61%, while De-
ResNet101 150 × 30.7 37.7 30.6
CenterNet formDETRandDINOdemonstratedslightlysuperiorperfor-
ResNet101 50 × 26.6 32.8 26.4
[29]
Hourglass-101 50 × 25.2 30.8 24.9 mancecomparedtoFasterR-CNN,withF1measuresof71%
and 72%, respectively. Notably, CeDiRNet-3DoF, employ-
PET[30] VGG16 20 × 32.0 19.1 22.2
ing the same ConvNext-B backbone, outperformed all re-
latedmethods,surpassingthebest-performingrelatedworks,
DINOandDeformDETR,by6-7pp.Furthermore,thissupe-
specifically designed for this type of data. To align with our
riorperformancewasachievedwithonly10epochs,whereas
method, we apply the same augmentation from CeDiRNet-
DeformDETR required 50 epochs, and DETR needed 500
3DoF to all related methods, utilizing 768×768 images for
epochstoconverge.Notably,theperformanceimproveseven
both training and testing.
further when depth data is added to CeDiRNet-3DoF.
a) Point-supervision approaches: We assessed the per-
formance of CenterNet [29], CenterNet++ [28], PET [30], VI. CONCLUSION
and P2PNet [27] as general methods, and two variants of In this work, we introduced a novel approach for grasp
a specialized method designed for grasp point detection on pointdetectionondeformableclothobjectsusingCeDiRNet-
clothsusingkey-points[18],[26].Theresultsaredetailedin 3DoF. Leveraging a re-parameterization of center directions
thelowerrowsofTableIV.CenterNetandCenterNet++,akin and a dedicated localization module, the proposed method
to CeDiRNet-3DoF, focus on regressing the center positions demonstratedrobustperformanceacrossdiversetoweltypes,
of objects. Their capacity to regress bounding boxes was configurations, backgrounds, and clutter scenarios. A signif-
not utilized in our benchmark. Both related methods exhibit icant contribution of this work is the introduction of the
significantly inferior performance compared to CeDiRNet- publicly available ViCoS Towel Dataset, comprising 8,000
3DoF, with 30% and 49% in F1 measure for CenterNet and real-world and 12,000 synthetic images, which serves as a
CenterNet++, respectively, while CeDiRNet-3DoF surpasses valuable resource for training and evaluating data-dependent
them with 78%. The change in backbones to Swin-L or deep-learning approaches.
Hourglass-101 did not yield improvements. The proposed method has been extensively evaluated
PETandP2PNet,designedforpointlocalizationincrowd on the ViCoS Towel Dataset. Through an ablation study,
counting applications, demonstrate similarly subpar results, we highlighted the benefits of our design choices and the
with 22% and 46% F1 measure, respectively, for PET and significance of synthetic data and depth information. Our
P2PNet,respectively,markedlyworsethanCeDiRNet-3DoF. parametric performance study provided further nuanced in-
Weattributethepoorresultstothecombinationoftheback- sights into performance across different towel types, corner
bones(VGG16)andthemethods’designforcrowdcounting, configurations, backgrounds, and clutter scenarios. Notably,
where the loss function anticipates a large number of object the method excelled in scenarios where cloth is laid flaton the table with all four corners well-visible, achieving [10] I.Garcia-camacho,B.Calli,A.Norton,andG.Aleny,“ClothManipu-
over97%F1.Inacomprehensivebenchmarkagainstseveral lationandPerceptionCompetition,”in2ndWorkshoponRepresenting
andManipulatingDeformableObjects,2022.
related methods, CeDiRNet-3DoF outperformed both point-
[11] D. Tabernik, J. Muhovicˇ, and D. Skocˇaj, “Dense center-direction
supervision and bounding box approaches, including the regressionforobjectcountingandlocalizationwithpointsupervision,”
latest transformer-based models, DINO and DeformDETR, PatternRecognition,vol.153,2024.
[12] J.Maitin-Shepard,M.Cusumano-Towner,J.Lei,andP.Abbeel,“Cloth
andapurpose-buildkey-pointdetectorforcloth,highlighting
grasp point detection based on multiple-view geometric cues with
its effectiveness and efficiency. applicationtorobotictowelfolding,”Proceedings-IEEEInternational
Our findings underscore the potential of CeDiRNet-3DoF ConferenceonRoboticsandAutomation,pp.2308–2315,2010.
[13] A. Ramisa, G. Alenya`, F. Moreno-Noguer, and C. Torras, “Using
for real-world robotic manipulation tasks. Despite achieving
depthandappearancefeaturesforinformedrobotgraspingofhighly
excellent results on towels with well-visible corners, this wrinkled clothes,” Proceedings - IEEE International Conference on
work highlights challenges for folded and crumpled objects RoboticsandAutomation,pp.1703–1708,2012.
[14] A. Doumanoglou, J. Stria, G. Peleka, I. Mariolis, V. Petrik, A. Kar-
where results are still not perfect. Method may also face
gakos,L.Wagner,V.Hlavac,T.K.Kim,andS.Malassiotis,“Folding
limitations due to the need for high output resolution when clothes autonomously: A complete pipeline,” IEEE Transactions on
grasppointsareclosetogether,potentiallymakingthemhard Robotics,vol.32,no.6,pp.1461–1478,2016.
[15] K.Yamazaki,“GrippingPositionsSelectionforUnfoldingaRectan-
todistinguishinsin/cosfields.Anotherconstraintisitsfocus
gular Cloth Product,” IEEE International Conference on Automation
on point detection, with the dataset featuring grasp points ScienceandEngineering,vol.2018-Augus,pp.606–611,2018.
exclusively at physical corners, neglecting edges useful for [16] S.Demura,K.Sano,W.Nakajima,K.Nagahama,K.Takeshita,and
K.Yamazaki,“PickingupOneoftheFoldedandStackedTowelsby
cloth manipulation. These challenges present opportunities
aSingleArmRobot,”inIEEEInternationalConferenceonRobotics
for future research. We aim to expand the dataset by an- andBiomimetics. IEEE,2018,pp.1551–1556.
notating towel edges in upcoming studies, thereby providing [17] X.Zhu,X.Wang,J.Freer,H.J.Chang,andY.Gao,“ClothesGrasping
and Unfolding Based on RGB-D Semantic Segmentation,” Proceed-
furtherdataforadvancinglearning-basedmethodsforgrasp-
ings - IEEE International Conference on Robotics and Automation,
ing point localization on cloth. vol.2023-May,pp.9471–9477,2023.
[18] T.Lips,V.-L.DeGusseme,andF.Wyffels,“LearningKeypointsfor
ACKNOWLEDGMENTS Robotic Cloth Manipulation using Synthetic Data,” IEEE Robotics
andAutomationLetters,vol.9,no.7,pp.6528–6535,2024.[Online].
Acknowledgments: This work was in part supported by
Available:http://arxiv.org/abs/2401.01734
the ARIS research projects J2-3169 (MV4.0) and J2-4457 [19] A. Kendall, Y. Gal, and R. Cipolla, “Multi-Task Learning Using
(RTFM) as well as by research programme P2-0214. Uncertainty to Weigh Losses for Scene Geometry and Semantics,”
inComupterVisionandPatternRecognition,2018,pp.7482–7491.
REFERENCES [20] I. Garcia-Camacho, J. Borras, B. Calli, A. Norton, and G. Alenya,
“HouseholdClothObjectSet:FosteringBenchmarkinginDeformable
[1] Y. C. Hou, K. S. Mohamed Sahari, L. Y. Weng, D. N. T. How, and ObjectManipulation,”IEEERoboticsandAutomationLetters,vol.7,
H. Seki, “Particle-based perception of garment folding for robotic no.3,pp.5866–5873,2022.
manipulation purposes,” International Journal of Advanced Robotic [21] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni,
Systems,vol.14,no.6,pp.1–14,2017.
and H.-Y. Shum, “DINO: DETR with Improved DeNoising Anchor
[2] A.CaporaliandG.Palli,“Pointcloud-basedIdentificationofOptimal BoxesforEnd-to-EndObjectDetection,”inInternationalConference
GraspingPosesforCloth-likeDeformableObjects,”inIEEEInterna- on Learning Representations Workshop, 2023. [Online]. Available:
tionalConferenceonEmergingTechnologiesandFactoryAutomation,
http://arxiv.org/abs/2203.03605
ETFA,vol.2020-Septe,no.870133,2020,pp.581–586.
[22] X.Zhu,W.Su,L.Lu,B.Li,X.Wang,andJ.Dai,“DeformableDetr,”
[3] K.SaxenaandT.Shibata,“GarmentRecognitionandGraspingPoint in International Conference on Learning Representations, 2021, pp.
Detection for Clothing Assistance Task using Deep Learning,” Pro- 1–11.
ceedingsofthe2019IEEE/SICEInternationalSymposiumonSystem
[23] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and
Integration,SII2019,pp.632–637,2019.
S. Zagoruyko, “End-to-End Object Detection with Transformers,” in
[4] E. Corona, G. Alenya`, A. Gabas, and C. Torras, “Active garment EuropeanConferenceonComputerVision,vol.12346,2020,pp.213–
recognition and target grasping point detection using deep learning,” 229.
PatternRecognition,vol.74,pp.629–641,2018.[Online].Available:
[24] K. He, G. Gkioxari, P. Dolla´r, and R. Girshick, “Mask R-CNN,” in
https://doi.org/10.1016/j.patcog.2017.09.042 InternationalConferenceonComputerVision,2017,pp.2961–2969.
[5] D. Seita, N. Jamali, M. Laskey, A. K. Tanwani, R. Berenstein, [25] C.-Y.Wang,A.Bochkovskiy,andH.-Y.M.Liao,“YOLOv7:Trainable
P. Baskaran, S. Iba, J. Canny, and K. Goldberg, “Deep Transfer bag-of-freebiessetsnewstate-of-the-artforreal-timeobjectdetectors,”
LearningofPickPointsonFabricforRobotBed-Making,”inRobotics inComputerVisionandPatternRecognition,2023,pp.7464–7475.
Research - The 19th International Symposium ISRR 2019, 2018, pp. [26] T. Lips, V.-L. De Gusseme, and F. Wyffels, “Learning Keypoints
275–290.[Online].Available:http://arxiv.org/abs/1809.09810 fromSyntheticDataforRoboticClothFolding,”in2ndworkshopon
[6] R.Ren,M.G.Rajesh,J.Sanchez-Riera,F.Zhang,Y.Tian,A.Agudo, Representing and Manipulating Deformable Objects - ICRA, 2022.
Y.Demiris,K.Mikolajczyk,andF.Moreno-Noguer,“Grasp-Oriented [Online].Available:http://arxiv.org/abs/2205.06714
Fine-grained Cloth Segmentation without Real Supervision,” in [27] Q.Song,C.Wang,Z.Jiang,Y.Wang,Y.Tai,C.Wang,J.Li,F.Huang,
Proceedings of the 2023 6th International Conference on Machine
and Y. Wu, “Rethinking Counting and Localization in Crowds: A
Vision and Applications, 10 2021, p. 147–153. [Online]. Available: PurelyPoint-BasedFramework,”inInternationalConferenceonCom-
http://arxiv.org/abs/2110.02903 puterVision,2021,pp.3345–3354.
[7] J. Qian, T. Weng, L. Zhang, B. Okorn, and D. Held, “Cloth region [28] K.Duan,S.Bai,L.Xie,H.Qi,Q.Huang,andQ.Tian,“CenterNet++
segmentationforrobustgraspselection,”inIEEEInternationalCon- for Object Detection,” IEEE Transactions on Pattern Analysis and
ferenceonIntelligentRobotsandSystems,2020,pp.9553–9560. MachineIntelligence,vol.1,pp.1–14,2023.
[8] I. Garcia-Camacho, G. Alenya, D. Kragic, M. Lippi, M. C. Welle, [29] X. Zhou, D. Wang, and P. Kra¨henbu¨hl, “Objects as Points,” 2019.
H.Yin,R.Antonova,A.Varava,J.Borras,C.Torras,andA.Marino, [Online].Available:http://arxiv.org/abs/1904.07850
“Benchmarking Bimanual Cloth Manipulation,” IEEE Robotics and [30] C.Liu,H.Lu,Z.Cao,andT.Liu,“Point-QueryQuadtreeforCrowd
AutomationLetters,vol.5,no.2,pp.1111–1118,2020. Counting, Localization, and More,” in International Conference
[9] I. Garcia-Camacho, J. Borras, B. Calli, A. Norton, and G. Alenya, on Computer Vision, 2023, pp. 1676–1685. [Online]. Available:
“HouseholdClothObjectSet:FosteringBenchmarkinginDeformable https://github.com/cxliu0/PET.
ObjectManipulation,”IEEERoboticsandAutomationLetters,vol.7,
no.3,pp.5866–5873,2022.