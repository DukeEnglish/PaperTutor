Dense Center-Direction Regression for Object Counting and Localization with Point
Supervision
DomenTabernika,JonMuhovicˇa,DanijelSkocˇaja
aFacultyofComputerandInformationScience,UniversityofLjubljana,Vecnapot113,Ljubljana,Slovenia
Abstract
Object counting and localization problems are commonly addressed with point supervised learning, which allows the use of less
labor-intensivepointannotations.However,learningbasedonpointannotationsposeschallengesduetothehighimbalancebetween
thesetsofannotatedandunannotatedpixels,whichisoftentreatedwithGaussiansmoothingofpointannotationsandfocalloss.
However, these approaches still focus on the pixels in the immediate vicinity of the point annotations and exploit the rest of
the data only indirectly. In this work, we propose a novel approach termed CeDiRNet for point-supervised learning that uses a
denseregressionofdirectionspointingtowardsthenearestobjectcenters,i.e. center-directions. Thisprovidesgreatersupportfor
each center point arising from many surrounding pixels pointing towards the object center. We propose a formulation of center-
directionsthatallowstheproblemtobesplitintothedomain-specificdenseregressionofcenter-directionsandthefinallocalization
task based on a small, lightweight, and domain-agnostic localization network that can be trained with synthetic data completely
independent of the target domain. We demonstrate the performance of the proposed method on six different datasets for object
counting and localization, and show that it outperforms the existing state-of-the-art methods. The code is accessible on GitHub
athttps://github.com/vicoslab/CeDiRNet.git.
Keywords: Point-Supervision,ObjectCounting,ObjectLocalization,Center-PointPrediction,Center-DirectionRegression,
CeDiRNet
1. Introduction
Object counting is a computer vision task with the goal of Dense
center-direction
countingthenumberofvisualobjectsofaspecificcategoryin regression
visualdata.Countingisoftenaddressedwithadirectregression
of the count number [1] or the density estimation map [2–5]. Sine Cosine
However,returningonlythecountoradensitymapisnotsuf-
ficientformanyapplicationswhereinformationabouttheloca-
tionoftheobjectisalsorequiredforadditionalverificationand
explanation,suchasindustrialqualitycontrolapplicationsorin Localization
someremotesensingdomains(countingvehicles,buildings,or
ships). In this work we focus on the problem of simultaneous Visualized
Locations directions
countingandlocalizationofobjectsfromvisualdata.
One solution would be to leverage existing state-of-the-art Figure1:OverviewoftheproposedCeDiRNetapproach.
generic object detectors, e.g., Mask R-CNN [6], YOLO [7],
FCOS[8],orAccLoc[9],andcountthenumberofdetections.
Moreover,usingonlypointlabelsmakestheannotationpro-
However, generic detectors focus on finding accurate bound-
cesssignificantlylesslaborintensivecomparedtodetailedan-
ariesofobjects,whichisnotrequiredforcountingapplications,
notation with bounding boxes. Studies comparing annotation
andthususetoomuchmodelcapacitytoachievegoodbound- effort have found that annotation with center points is 77%
ary accuracy instead of focusing on counting accuracy. They
fasterthanannotationwithboundingboxes,andmorethan3.5
are also prone to errors in densely packed scenes [10] due to
timesfasterthanannotationwithfour-pointpolygons. Thisbe-
overlapofboundingboxes,whiletheyalsorequiredetailedan-
comes even more important when there are a large number of
notations (masks or bounding boxes) that are not required for
objectsintheimage,asisthecaseinthecountingproblem.For
accurate counting and localization. For counting, localization
thisreason,recentapproachestocountingandlocalization [11–
withasinglepointissufficient.
Tabernik),jon.muhovic@fri.uni-lj.si(JonMuhovicˇ),
Emailaddresses:domen.tabernik@fri.uni-lj.si(Domen danijel.skocaj@fri.uni-lj.si(DanijelSkocˇaj)
PreprintsubmittedtoElsevier August27,2024
4202
guA
62
]VC.sc[
1v75441.8042:viXra13]employmethodsbasedonpoint-supervisionthatpredictthe directions can be viewed as normalized offsets that, unlike di-
objectlocationasasinglepoint.Thisreducestheamountofan- rect offsets, are invariant to changes in scale (the network can
notationrequiredandalsoleadstothereplacementofbounding learn the same value regardless of image or object size) while
box location regression or segmentation mask estimation with also avoiding the unbalanced learning issues between the val-
objectcenterprobabilityprediction. ues that are near and those that are farther from the center
In most such approaches [12, 13], the prediction of an ob- thatoccurwithdirectoffsetregression. Wealsoformulatethe
ject’s center is done by directly learning the probability for center-directions such that they provide a specific signature at
every pixel in an image that it belongs to the object center. thecenteroftheobject(seeFig.1), makingitpossibletofind
Thiscanbeimplementedasabinarysegmentationproblemus- theobjectcenterwithasmall,lightweightlocalizationnetwork
ing cross-entropy loss, with only a single object center point applied to the regressed center-direction thus avoiding com-
markedasforeground.However,thisalsoleadstoanextremely plex post-processing method such as clustering or Hough vot-
unbalancedproblemduetoalargenumberofbackgroundpix- ing. Moreover, the appearance of the center-directions is not
els compared to the number of foreground pixels. Most ap- domain-specific, but is similar for different categories, allow-
proaches address this problem with focal loss [14] and with ingtheuseofadomain-agnosticlocalizationnetworkthatcan
Gaussian blur to spread the center point and expand the fore- be trained with synthetically generated input data completely
ground region, while some [11] avoid this problem by replac- independentofaspecificdomain.
ingthecrossentropylosswiththeweightedHausdorffdistance. The contribution of the proposed approach is threefold: (a)
Nevertheless,thesemethodsstillusedirectregressionofaper- the dense prediction of the center-directions allows for large
pixellikelihoodorsaliencymapofthecenterpointthusregress- support from pixels farther away from the center, (b) only
ing to zero values in the background. However, regression to thecenter-directionregressionistrainedonthetargetdomain,
zerovaluesawayfromthecenterdoesnotencouragethemodel whilethelocalizationnetworkisfullydomain-agnostic,trained
tofindusefulinformationfartherawayfromthecenter, which once using synthetic data and shared among all domains, and
may still be useful for predicting the center, especially if the (c)point-supervisionreducestheannotationeffort. Wedemon-
objectislessprominentininferencetimethanduringtraining. strate the benefits with an extensive evaluation on six differ-
Some information may be found, but this is not explicitly en- ent datasets, showing that our approach is able to outperform
couragedbytheloss. Addingregressionofdirectoffsetvalues existing state-of-the-art approaches for counting and localiza-
tothecentersfromthebackgroundlocationwouldaddressthis tion. Our extensive ablation study provides further details on
problem,butsuchmethodsdonotregressthevaluesoutsidethe our design choices and also demonstrates a possibility of us-
centers[15]oroutsidetheinstancemask[16]. ing a small, non-learnable, hand-crafted localization network
Moreover, when regressing the offsets to centers with gra- that still out-competes many existing methods, while a learn-
dient descent methods, this results in a small gradient for the able domain-agnostic network trained solely on synthetic data
former and a large gradient for the latter, creating an imbal- isabletoimproveuponthisandachievestate-of-the-artresults.
ancebetweenthemduringtraining. Effectively,offsetsthatare Theremainderofthispaperisstructuredasfollows: inSec-
farther away suppress learning of offsets that are closer to the tion 2 we present existing literature on counting and local-
center,eventhoughthecloseroffsetstendtobemoreimportant. ization, in Section 3 we detail our proposed approach using
Also,directoffsetvaluesarenotinvarianttoscalechangesand center-direction regression together with the localization net-
the network must regress different values depending on image work, while in Section 4, we demonstrate the performance of
sizeorobjectscale. InCenterNet[15],thisproblemisavoided our method on several datasets and with a detailed ablation
by learning regressions only in locations around the centers, study. WeconcludewithadiscussioninSection5.
while[16,17]addascalefactortoreducethesizeofthevec-
tors. Nevenetal.[16]alsoperformstheregressiononlywithin
2. Relatedwork
theinstancemask,whichmitigatesthisproblem.Currentmeth-
ods that regress direct offsets from all surrounding pixels also Object counting has been well-explored in the literature.
require complex post-processing to infer centers from the off- While early approaches were based on hand-crafted fea-
sets,suchasclustering[16]orHoughvoting[17],makingthem tures [1], machine learning [18] and simple neural net-
unsuitable for locating a large number of objects common in works[19],morerecentapproachesrelyheavilyondeeplearn-
counting. ing. Modernapproachesthatrelyondeeplearningcanbesplit
We propose a novel point-supervision approach for object intotwogroups:countingbyregressionandcountingbydetec-
countingandlocalizationtermedCeDiRNet. Theproposedap- tion. Inthefirstgroup,objectsarecountedbydirectlyorindi-
proachdenselyregressesdirectionspointingtothenearestob- rectlyregressingthecountnumber,whileinthesecondgroup,
ject center, rather than relying on a direct regression of the the count number is obtained by detecting each individual ob-
center’sprobabilitymapordirectoffsets. Werefertosuchre- ject.
gressed directions as center-directions. Regressing the center-
direction for all pixel locations in the image encourages the Counting by regression. Many modern state-of-the-art ap-
modeltofindfeaturesthatarecorrelatedwiththecenter, even proaches to object counting employ regression of density es-
if they are farther from the center, and thus providing greater timationasanintermediatesteptoobjectcounting. Insteadof
support for the center from a larger surrounding area. Center- directly regressing the count number, they estimate the spatial
2ResNet 101 Feature Pyramid Network
for segmentation
dilate=1
dilate=4
+
32 128
32 dilate=8 32
16 32
16 16 Localization map
2 16 1
dilate=12 Local-maxima
Concatenate
Conv2d (3x3,stride=1)
Conv2d (3x3,stride=2)
DeConv2d (5x5,stride=2)
Input image Center-direction fields Output detections
Center-direction regression network Localization network
Figure2:Architectureoverviewofdensecenter-directionsregressionandlocalizationnetworks.
probabilitymapofobjectsandreturnthecountnumberbyin- in dense scenes, recent methods directly predict object center
tegratingtheestimateddensity[20]. Inmodernapproaches,di- positions instead of predicting bounding boxes. Such meth-
rectregressionofthecountnumberisoccasionallyusedonlyas ods also significantly reduce the required labeling effort since
anauxiliaryloss[16,21].Fortraining,onlypointsupervisionis only point annotations are needed. Wang et al. [12] predict
required,butcertainapproaches[5,22]alsoutilizeonlyimage- the probability map of object center location from a Gaussian
level annotations of the number of objects. Density estima- smoothedcenterpointsandreliesonlyonpoint-supervisionby
tionapproachesarewellsuitedforcountingindenselycrowded usingaself-trainingapproachthatestimatesandinferspseudo-
scenes[4,23], particularlywhencombinedwithlargedilation groundtruthsizesfrompointannotations.Riberaetal.[11]pro-
factorstoincreasethereceptivefieldsize[24].However,sparse posed not to use object sizes during training at all and instead
and background regions often present challenges for density employedtheweightedHausdorffdistancetolearnanobjectlo-
estimation. Liu et al. [2] proposed to address these issues cationprobabilitymap. Forobjectcounting, theirmethodfur-
withtheattentionguidednetworkthatcombinesdensityestima- theremployeddirectregressionofthecountingnumberfromU-
tionwithcounting-by-detectionforregionswithfewerobjects. Netfeaturesmergedwiththepredictedcenterprobabilitymap.
Rong et al. [3] also addressed problems of estimating density Tongetal.[13]alsousedlesslabor-intensivepoint-supervision
in background regions by proposing a background-aware loss forpredictingobjectlocationswhilealsoestimatingsegmenta-
thateffectivelyaddsafrom-coarse-to-fineprogressiveattention tion map for counting and segmentation of trees from drone-
mechanism.Whileimprovedperformanceinsparseregionshas basedimagery.Themostrecentwork[26]employedapyramid
allowed density estimation methods to achieve state-of-the-art poolingmodelandmulti-sigmarefinementapproachtoachieve
result for dense crowd counting [3, 4], such methods remain the state-of-the-art results for counting and localization from
limited to applications where localization of the counted ob- pointsupervision.
jects is not required. This is a direct consequence of relying
onthedensitymap,wherethecountnumberisencodedovera Center-pointpredictionmethods. Ourproposedapproachalso
largerspreadoftheprobabilitylocations. employs point supervision and can be considered a counting-
by-detectionmethod,however,itdiffersfromexistingmethods
Countingbydetection. Ontheotherhand,approachesthatem- in the regression of the center locations. Instead of directly
ploy counting-by-detection explicitly detect all objects being regressing the probability map of object center locations, as
counted, and are thus capable of performing both localization doneinrelatedmethods[11–13,26],weproposetoregressonly
andcounting. Applyinggeneralobjectdetectorswithbounding thecenterdirectionandthenusealightweightdomain-agnostic
boxpredictionscanbeasimplesolution,butsuchdetectorsare networktofindtheobjectcenters.
sensitive to higher-density scenes due to overlapping bound- Regression of center locations has also been used in object
ing boxes [10]. This can be improved by using Soft-IoU and detection [12, 15, 27? ], segmentation [16] and human pose
Gaussianmixturemodeltoreducetheoverlapbetweenthede- estimation [17]. CenterNet [15], CenterNet++ [? ] and Cor-
tectedboundingboxesasshownbyGoldmanetal.[10]. Hsieh nerNet [27] perform regression of the object’s center position
et al. [25] also explored extending a region proposal-based as well as additional key-points to define the object bounding
methodwithalayoutproposalnetworktocapturethestatistics box. The same approach was also applied to 3D object de-
inneighboringobjectlocationsfordrone-basedscenes. tectionfrompointclouds[12]. Whilethesemethodswerede-
Tocompletelyeliminatetheproblemofoverlappingregions signedforothertasks,theirfirststagecanstillbeusedforcenter
3detectioninpoint-supervision. CenterNet[15]andthemethod Sine Cosine Direction angle
proposed by Zhou et al. [17] also perform direct regression of
offsetstothecenterposition,nexttoregressionofbinaryprob-
ability map, however, they propose this only as a refinement
ofalocationfoundinitiallythroughthecenterlikelihoodmap.
In CenterNet, offsets are trained only at locations where cen-
ters are positioned, while ignoring regression of offsets from
surrounding locations as we do. Zhou et al. [17], as well as
1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 180 0 180
Nevenetal.[16],ontheotherhandperformpredictionofvec-
Figure 3: Example of center-directions fields in re-parametrized form: Csin
torsthatpointtothecenterfromneighboringpixels,similarto (left)andCcos (middle). Lastcolumnisavisualizationoftheactualangleϕ
ourapproach. Howeverin[16],centerpositionsarenotlearned obtainedbyatan2(·)
directly but through an IoU-loss and only within the instance
mask,makingthismethodincompatiblewithpoint-supervision
Are-parametrizedcenter-directionaffordsmultiplebenefits.
training as it requires pixel-wise segmentation labels for the
It eliminates the issue of regressing cyclical values in angle
groundtruth. MethodsbyZhouetal.[17]andNevenetal.[16]
while also translates values into bounded range of [−1,+1].
also do not formulate the center direction as we do to enable
Thisismoresuitablefordirectregressionthanusingunbounded
localizationwithlightweightnetwork,butinsteadrequirecom-
vectors in Cartesian space [16], since all values will be of the
plexpost-processingwithclustering[16]orHoughvoting[17],
sameordersofmagnitude, thushavingsimilargradients. This
makingthemunsuitableforlocalizationoflargenumberofob-
alsoeliminatestheuseofanyadditionalscalingfactorthatisof-
jectscommonincounting.
tenusedtomitigatetheissueofunboundedrangeinCartesian
space.
3. Objectcountingandlocalizationwithcenter-directions Wecanalsore-obtaintheoriginalangleϕfromtheregressed
Cˆ andCˆ byapplyinginversetangentfunction. Formathe-
sin cos
We propose a novel method for object counting and local- maticalstability,weuseatan2(·)function,however,tofindthe
ization using a dense prediction of directions to object cen- final object location we do not need angle directly and use it
ters. Instead of directly predicting a binary probability of a onlyforvisualizationpurposes.
center for each location, as is commonly performed in other
approaches [12], we propose to first predict center-directions Architecture. To learn center-direction regression for every
andthenusedomain-agnosticlocalizationnetworktocountob- pixellocation, weuseadeepnetworkfordenseprediction. In
jectsandfindtheirposition(c.f. Fig.2). Wetermtheproposed general,anynetworkfordensepredictionwouldsuffice,suchas
approachwithCenter-DirectionRegressionasCeDiRNet. hourglassarchitecturesorencoder-decodermodels. Inthispa-
per,weuseanencoder-decodermodelwithResNet-101asen-
3.1. Denseregressionofcenter-directions coder and Feature Pyramid Network [28, 29] as decoder. The
architecture is implemented in PyTorch based on [30], where
Parametrization. Weproposetofirstpredictadirectiontothe
FPNconsistsoffourlevelsofthefeaturepyramidwithanaddi-
closest center position for each object in an image from all of
tionalsegmentationheadconsistingoffourlayersthatperform
its surrounding pixel locations, i.e., for each pixel location in
thefinalup-samplingandregression1.
animage,wepredictadirectionthatpointstowardstheclosest
object center. For each pixel location x = (i, j), we can de-
i,j
Learning. The regression model for center-directions, i.e.,
fineϕ asadirectionanglepointingtowardstheclosestobject
i,j
centery=(n,m): ConvNet cos and ConvNet sin, is learned in a fully supervised
manner. For each regressed location x , we define a ground
i,j
truthregressiontargetasacenter-directiontotheclosestobject
(cid:18)m− j(cid:19)
in a re-parametrized form, C (x ) and C (x ). The final
ϕ =tan . (1) sin i,j cos i,j
i,j n−i lossfunctionisformulatedusingtheper-pixelL distance:
1
(cid:88)
We propose a re-parameterization of the direction angle L= W(x )·|Cˆ (x )−C (x )|+ (3)
ϕ using sin(·) and cos(·), and we term ϕ in this form as i,j sin i,j sin i,j
i,j
center-direction. Direction angle and their corresponding re- (cid:88)
W(x )·|Cˆ (x )−C (x )|, (4)
parameterized fields are depicted in Fig. 3. A deep learning i,j cos i,j cos i,j
modelfordensepredictionisthenusedtoperformadensere- i,j
gression of center-direction for every pixel location in the im-
whereW(x )definesaper-pixelweightassignedforeachloca-
i,j
age. Ourproposedmodelcanthereforebedescribedasregres-
tionx ,whichalsoactsasanaveragingterm,and|·|represents
sionoftwodensefieldsCˆ ,Cˆ ∈RN×M fromaninputimage i,j
sin cos a per-pixel absolute error. We use the per-pixel weight to bal-
I ∈ RN×M, corresponding to sin(ϕ) and cos(ϕ) of the closest
ancethelossesbetweenthepixelsclosertotheobjectandones
centerpoint:
Cˆ =ConvNet (I), Cˆ =ConvNet (I). (2)
sin sin cos cos 1https://github.com/vicoslab/CeDiRNet.git
4fartheraway.Sincethenumberofpixelsfartherfromthecenter wecanconstructanetworkforConvNet thatoutputsahigh
cent
will always be much larger than the number of pixels around probability at the exact center position and a low probability
thecenter,thiswillresultinalargerinfluenceofdistantpixels, everywhereelse. Findinglocal-maximavaluesintheresulting
thus reducing the speed of learning for pixels around the cen- Oˆ yieldsthenumberofobjectsinanimageandtheircenter
cent
ter. However, during learning more emphasis should be given positions,whichisthenusedasthefinaloutputofourmodel:
toproperregressionofcenter-directionsaroundtheobjectcen-
(cid:104) (cid:105)
ter. Were-balancethisbyassigninglargerweightstothepixels positions(I)=argmax LocalMax(Oˆ cent) , (9)
aroundtheobjectcenters: i,j
(cid:88)(cid:104) (cid:105)
W(x
)=
K·1
|F k| ifx i,j ∈F k
(5)
count(I)=
i,j
LocalMax(Oˆ cent) . (10)
i,j |B1
|
ifx
i,j
∈B, A simple local maxima detection is sufficient to extract
the locations from the regressed output heatmap. This stems
where B represents the set of distant pixels, F represents the directly from applying the localization network to center-
k
directions fields with the limited range of values and a unique
set of pixels around the k-th object center and K is the total
visualsignaturearoundeachobjectcenterthusresultinginthe
number of objects in an image. Division between pixels be-
longingtoF andBiscompletelyindependentfromtheactual localizationoutputOˆ cent withminimalnoiseevenforlocations
k
where regressed center-directions are somewhat noisy. This
objects’visualboundariessinceweonlyrequirelooselydefined
areas of higher importance. We define F and B based on the removes any need for an additional complex post-processing
k
distancetotheclosestcentery usingacutoffthresholdϵ: heuristicswiththresholding,smoothingorclustering.
k
F ={x |(x −y )2 <ϵ}, (6) Architecture. For the center localization network, we use an
k i,j i,j k
hourglassarchitectureconsistingoffourlevelsinbothencoder
B={x |∀k∈{1..K},(x −y )2 ≥ϵ}. (7)
i,j i,j k and decoder, i.e., eight levels in total. Each level consists of
Thecutoffdistancethresholdϵcouldbesetrelativetotheimage a single convolution block (Conv2D with batch normalization
sizeandobjectdensity,butinourexperimentsweusedasingle and ReLu) with 16 or 32 channels. To increase the receptive
fixed ϵ, which has proven sufficient for different datasets. We field size, we add a middle layer between the encoder and de-
providemoredetailsondifferentϵ valuesintheablationstudy. codercontainingmultipleparallelconvolutions,eachwithadif-
The proposed weighing scheme effectively normalizes the ferentdilationfactorsimilartoASPPmoduleinDeepLab[31].
lossesbetweenthepixelsaroundtheobjectcenters,F ∪F ∪ Weusethedilationfactorsof1,4,8and12. Thearchitectureis
1 2
..∪F ,andmoredistantones,B,aswellasbetweentheindi- depictedinFig.2.
K
vidualsetsF . Thisisimplementedasnormalizationwiththe
k
numberofpixelsinBsetforx ∈B,andasnormalizationwith Learning. The localization network is learned independently
i,j
the number of pixels in each F , averaged over the number of of the center-direction regression. Localization network is
k
objectsK,forx ∈F ∪F ∪..∪F . learned with a per-pixel loss function. We define a per-pixel
i,j 1 2 K
groundtruthregressiontargetO (x )asaGaussiandistance
cent i,j
3.2. Objectlocalizationfromcenter-directions totheclosestobjectcenterpointy:
Although a single regressed center-direction value does not (cid:32) (cid:32) (cid:33)(cid:33)
|x −y|−ξ
have enough information to exactly locate the object’s posi- O (x )=min 1.0,exp − i,j , (11)
cent i,j 2σ2
tion,theinformationabouttheexactcenterlocationariseswhen
center-directionsfromsurroundingpixelsareconsidered. The-
wherewesetσ=2.5andξ =1toensuremorethanonepixelin
oretically,theobjectcentercanbefoundasanintersectionpoint
thecenterhavemaximumvalueandtoadditionallyincreasethe
between multiple lines obtained from the regressed direction
extent of the Gaussian. The network parameters ConvNet
anglesfrommultiplesurroundingpixellocations. However,di- cent
arethenoptimizedusingL loss:
rectlyapplyingananalyticalsolutiontofindintersectionpoints 1
from a large set of lines (i.e., a large image will contain hun-
dreds to millions of lines) would not be efficient nor robust, 1 (cid:88)
L = W (x )·|Oˆ (x )−O (x )|, (12)
particularly when a large number of objects is present in the cent N·M cent i,j cent i,j cent i,j
i,j
image.
Weinsteadproposetofindobjectcentersbyapplyinganad- whereN·Misthenumberofpixelsinanimage,W (x )isan
cent i,j
ditional small network directly to the re-parameterized center- additionalper-pixelweightand|·|isaper-pixelabsoluteerror.
direction fields Cˆ sin,Cˆ cos, and output a dense probability map Since the number of background pixels is significantly larger
ofobjectcentersOˆ ∈RN×M:
than the number of foreground pixels, we increase the weight
fornon-backgroundpixelsusingw >1:
Oˆ =ConvNet (Cˆ ,Cˆ ). (8) fg
cent cent sin cos

SinceC sinandC cosresultinaheatmapimagewithextremely W (x )=w fg ifO cent(x i,j)>0 (13)
uniquevisualsignaturearoundeachobjectcenter(c.f. Fig.3),
cent i,j 1
otherwise.
5Domain-agnostic learning using synthetic data. The localiza- cars that are split into 989 training and 495 testing images.
tionnetworkislearnedinafullydomain-agnosticmannerwith- PUCPR+ dataset consists of 125 high-resolution images con-
outtheneedforrealdatasets. Ourapproachtranslatesdomain- taining 16,456 annotated cars that are split into 100 training
specific images into a domain-agnostic center-directions map and25testingimages. AllimagesinCARPKdatasetarecap-
(C andC ),effectivelyseparatingthelocalizationtaskfrom turedfromthetop-downperspectivetakenwithanUAVdrone,
sin cos
domain-specific features. This allows independent training while images in PUCPR+ dataset are taken by a fixed camera
across domains using synthetic data, simplifying the overall mounted in front of a parking lot. Some examples of images
learningprocedureasthelocalizationnetworknolongerrelies are shown in Figs. 6 and 5. In both datasets objects are anno-
on domain-specific datasets. Simultaneously, leveraging syn- tatedwithboundingboxes,however,inourexperimentsweuse
theticdatamitigatesoverfittingissuesbydiversifyingthetrain- only the center of the bounding box and discard object sizes.
ing dataset, as it encompasses random generation of the num- Since a validation set is not provided, we selected a subset of
ber and position of objects in the input data. This approach trainingimagesforvalidationpurposes. ForPUCPR+,weran-
prevents overfitting associated with a fixed number of objects domlyselected10images,whileforCARPK,weselectedaset
and locations in real data, leading to enhanced and more ro- of all training images that originated from the same video se-
bust performance. Additionally, we introduce Gaussian noise quencetoensureminimalintersectionbetweenthetrainingand
andocclusionstothesyntheticdatatoenhancerobustness. Our thevalidationsets.Thisresultedinusing155validationimages
ablationstudydemonstratesthatthisapproachyieldsbetterper- forCARPKdataset. Nevertheless,thevisualappearanceofthe
formancecomparedtofinetuningonaspecificdomainwithreal validation set is much closer to the appearance of the training
datathuscompletelyeliminatingtheneedforrealdata. setthanthetestingset,makingthevalidationsetlessthanideal.
Acacia and Oil palm datasets. Acacia and Oil palm datasets
4. Experiments consistsofthreedatasetsprovidedbyTongetal.[13]: Acacia-
6, Acacia-12andOilpalm. Eachdatasetconsistofalargeor-
In this section, we report results of an extensive evalua- thoimage of tree plantations. Images are of 31,644×26,420
tion of the proposed method. Evaluation is performed on pixelsinsizeforAcacia-6,36,521×31,181pixelsforAcacia-
six datasets for object counting in remote sensing domains: 12 and 64,273 × 27,839 pixels for Oil palm. The three
Sorghum plant [11], CARPK [25], PUCPR+ [25], Acacia- datasetscontain18,528,25,691and130,035annotatedobjects
6 [13], Acacia-12 [13] and Oil palm [13]. We first report re- for Acacia-6, Acacia-12 and Oil palm, respectively, and each
sults obtained on all these datasets and compare the proposed object is annotated with a single center point only. Since au-
methodwiththerelatedwork. Thisisfollowedbyanablation thorsdonotprovidecroppedimagesnorthetrain/testsplitsthat
study,analyzingtheinfluenceofdifferenthyperparametersand
were used in their experiments, we first extracted sliding win-
designchoiceswemade. dowpatchesandthensplitthemintothreenon-overlappingsets
for3-foldcrossvalidation.Slidingwindowpatchesof512×512
4.1. Datasets pixels in size were extracted with a step of 384 pixels, while
ignoring patches that do not contain any valid image pixels.
Sorghum plant dataset. Sorghum plant dataset provided by
This resulted in 1779, 2495 and 11,867 images for Acacia-6,
Riberaetal.[11]consistsof60,000imagesofsorghumplants
Acacia-12 and Oil palm, respectively. Some examples of im-
captured from a top-down perspective from a UAV and point-
agepatchesareshowninFig.7. Wethenassignedthecropped
annotationofeachplant. Thedatasetcontainsimagesofasin-
images into three non-overlapping sets based on the original
glephysicalsorghumfieldthatwerefirststitchedtogetherinto
largeimagethatissplitintothreeregions. Foreachfoldinthe
a single 6000×12000 orthoimage and then split into separate
cross-validationthatcorrespondstooneregion,thetestingim-
training, validation and testing regions. Final images are then
ages corresponded to all patches that contain only pixels from
randomlycroppedpatchesfromeachregion,resultingin50,000
that region, while the remaining patches that do not contain
training,5,000validationand5,000testingimages. Theheight
any pixels from that region were used for training and valida-
and width of the random crops are uniformly distributed be-
tion. For each dataset, the original large image was split into
tween 100 and 600 pixels. Some of these image crops may
three regions either in vertical or horizontal orientation to en-
overlap,buttheyarerestrictedtotheirarea. Imagepatchesre-
suresimilarimagecountforeachfold. Weperformevaluation
sized to 256×256 pixels are also provided, however, we use
onthisdatasetbymergingtheresultsfromeachtestpatchfrom
onlyoriginallycroppedimageswithpreservedaspectratiosin
allfoldsintotheoriginalimage.
ourexperiments, andadditionallycrop/padthemto256×768
pixels for training without changing the aspect ratios. Some
4.2. Evaluationmetrics
examplesofimagesareshowninFigs.4and5.
Weusestandardmetricsforevaluatingobjectcountingmeth-
ods. This includes the mean absolute error (MAE) and root
CARPK and PUCPR+ datasets. Car Parking Lot (CARPK)
meansquarederror(RMSE):
and the Pontifical Catholic University of Parana+ (PUCPR+)
(cid:118)(cid:117)(cid:116)
a pr re ovc ia dr e- dcou bn ytin Hg sid ea hta es tets alt .ha [t 25c ]o .ntai Cn Aim RPag Kes do af tap sea trk cin og nsl io st ts
s MAE = N1
(cid:88)N
(cid:12) (cid:12) (cid:12)c i−cˆ i(cid:12) (cid:12) (cid:12), RMSE = N1
(cid:88)N
(cid:0) c i−cˆ i(cid:1)2, (14)
of 1,444 high-resolution images containing 89,777 annotated i=1 i=1
6wherec iistheactualnumberofobjectsinthei-thimageandcˆ i Method τ[px] prec[%] recall[%] F1[%] MAE RMSE
isthepredictednumberofobjects. Weadditionallyevaluatelo-
Tongetal.[13] - 92.60 93.00 92.60 2.33 3.43
calization performance since the proposed method returns not
CenterNet[15] 40 95.18 96.22 95.58
only the number of objects but also the exact object location. CenterNet[15] 30 95.11 96.16 95.52
2.16 3.41
Localization metrics are often stricter than MAE or RMSE, CenterNet[15] 20 95.06 96.10 95.47
CenterNet[15] 15 95.02 96.06 95.43
sincealowcounterrorcanbeachievedevenwithalargenum-
CenterNet[15] 5 89.53 90.50 89.91 2.16 3.41
beroferroneousdetectionsifthenumberoffalsepositivesand
FasterR-CNN[11] 5 86.60 78.30 82.20 9.40 17.70
false negatives balance out. As a localization metric, we thus Riberaetal.[11] 5 88.10 89.20 88.60 1.90 2.70
measureF ,precisionandrecallaveragedoverallimages: CeDiRNet(our) 5 89.76 91.03 90.32 1.78 2.99
1
CeDiRNet(our) 15 95.80 97.17 96.41
F = 1 (cid:88)N 2TP i , (15) C Ce eD Di iR RN Ne et t( (o ou ur r) ) 2 30 0 9 95 5. .8 83 6 9 97 7. .2 20 4 9 96 6. .4 44 7 1.78 2.99
1 N i=1 2TP i+FP i+FN i CeDiRNet(our) 40 95.91 97.29 96.52
Table1:ResultsonSorghumdatasetwithdifferentτvalues.Notethat[13]do
precision=
1
(cid:88)N
TP
i , recall=
1
(cid:88)N
TP
i ,
τno =te 4x 0p fl oic rit oly there rp do ar tt aw seh tsic .hτvaluewasusedforSorghumdatasetbuttheyuse
N TP +FP N TP +FN
i=1 i i i=1 i i
(16)
arechosenuniformlyfrom[0.8,1.2],whilehuejitterischosen
whereTP,TN ,FP and FN arecalculatedforthei-thimage uniformlyfrom[−0.2,0.2].
i i i i
ataspecificscorethreshold,whichisobtainedasthethreshold
achievingthebestF andMAEscoreonthevalidationset.True
1 Syntheticdataforlearninglocalizationnetwork. Forthelocal-
detectionsareconsideredonlywhenthedistancetotheground
ization network, we used synthetic training images that were
truthlocationislessthanτandiscalculatedasalinearsumas-
randomly generated during each iteration. For each sample,
signmentproblem[32]betweenthegroundtruthlocationsand
we randomly assigned a number of center points and gener-
thepredictedobjectlocations. Sincedifferentrelatedmethods
atedC ,C as input images, each 512×512 pixels in size.
use different τ thresholds, we report metrics at multiple τ for sin cos
The number of points were randomly selected from the uni-
faircomparison. However,weobservedthatsomemethodsre-
formdistributionofU(5,50). Wealsoaddedadditionalcenter
portvaluesatsmallτ=5,whichconsideringimagesofaround
points around the originally generated ones. Around each ex-
512×512isunrealisticallysmallmarginoferror. Wefindthe
isting point, we added additional points with a probability of
metricataround20≤τ≤40tobeamorerealisticreflectionof
0.25, while the number of added points around each one was
trueperformance,particularlyinhigh-resolutionimages.
randomly sampled from the uniform distribution of U(2,5).
We additionally simulated degradation of input data to mini-
4.3. Implementationdetails
mizetheproblemofoverfittingtothesyntheticappearance.We
Learning. BothnetworkarchitecturesweretrainedwithAdam added zero-mean Gaussian noise with the probability of 0.25,
optimizer using the learning rate of 10−4. Additionally, the where the standard deviation of the noise was randomly se-
learningratedecaywasappliedatthestartofeachepochwitha lected from the uniform probability of U(0.1,2). Noise was
factorof(1−n)0.9,wherenisthefractionofcompletedepochs. additionally smoothed with a Gaussian blur using σ = 3. Fi-
For the center-direction regression network, we used a batch nally, we simulated occlusion of the center-directions around
size of 32 and trained the network for 200 epochs, except for the center point. Occlusions were added to each center point
the Sorghum dataset, where we used a batch size of 128 and independently with the probability of 0.75. We used circular
traineditfor50epochsduetosmallerimagesbutlargerdataset. occlusions positioned around the existing center points. Posi-
WeusedResNetbackbonepre-trainedonImageNetdatasetas tionswerealsorandomlyperturbedwiththeprobabilityof0.5.
providedbyPyTorchframework,whileFPNlayerswereinitial- Thesizeofthecircularocclusionswasrandomlyselectedina
√
izedtothenormaldistributionbasedonGlorotandBengio[33] rangeof[5,0.025·D]pixels,where D = W2+H2 isthedi-
methodwithagainvalueof0.1. Foralldatasets,trainingwas
agonalofthegeneratedimage. Theselectionofsizeswasalso
distributed over 8 GPUs. For the center localization network,
slightlyskewedtowardslargerimages.
we used a batch size of 768 and trained the network for 200
epochson16GPUs. Wesetw = 50,whichwefoundtoper-
fg
formthebest. Weused5000syntheticallygeneratedimagesin Hardsamplesminingwithinepoch. Weadditionallyenhanced
eachepoch. thelearningprocesswithamechanismthatgivesmoreempha-
sis on the learning of hard samples. We implemented this by
Dataaugmentation. Foralldatasets,weapplydataaugmenta- augmenting the training batch with samples that are selected
tionusingrandomimagerotationandcolorpixeljittering. Ro- fromasetofhardexamples. Atthestartofeachepoch,atmin-
tational augmentation is implemented as a vertical and a hori- imum 10% of the most difficult samples are selected as hard
zontalimageflip,eachbeingperformedrandomlywithaproba- samplesandduringeachiterationN %ofthebatchsizeisran-
hr
bilityof0.5.Wethenapplyrandomcolorjitteringwithaproba- domly selected only from those hard samples. The degree of
bilityof0.5,whereaddedbrightness,colorandsaturationjitter difficulty of each sample is measured by its loss value as well
7Sine Cosine Sine Cosinceosine
Visualized Locations Visualized directions Locations
directions
Figure4: Examplesofregressedcenter-directionsandlocalizationdetectionsonSorghumdataset. Greenandredcrossesarecorrectandincorrectdetections,
respectively,shownbasedonτ=40,whilegroundtruthsareshownwithgreendots.
asbyusingtheactualnumberoffalsepositivesandfalsenega-
tives,whilegivingslightlymoreemphasisonfalsenegatives..
Hyperparameteroptimizationonvalidation. Foreachdataset,
we selected the model at the best performing epoch and score
threshold,whilewealsooptimizedfreehyperparameters. Both
optimizationswereperformedonthevalidationset.Twohyper-
parameters were optimized: (a) learning weight decay and (b)
percentofbatchsizeusedforhardsamples(N ). Ahyperpa-
hr
rameter search was performed for each dataset independently,
and a model with the specific combination of parameters that
performed the best was then used for evaluation on the test-
ingset. Fortheweightdecay, weexploredusingvalues0and
10−4,whileforhardsamplesmining,weexploredusingvalues Figure5:ExamplesoftrueandfalsedetectionsonSorghumdatasetusingτ=5,
N =0andN =50%. ontheleft,anddetectionsonPUCPR+datasetusingτ=40,ontheright.
hr hr
4.4. Results
offsets to the centers. Although CenterNet was designed for
EvaluationresultsarereportedinTable1forSorghumplant
bounding box detection, we use only the first stage for center
dataset, in Table 2 for CARPK and PUCPR+ datasets, and
detection,whichcanbetrainedonpointannotationsonly. For
in Table 3 for Acacia-06, Acacia-12 and Oil palm datasets.
faircomparison, weusethesamebackboneandaugmentation
Theproposedmodelisshowntooutperformallexistingstate-
as for CeDiRNet (i.e., ResNet-101, color jittering and image
of-the-art models. All results reported in those tables were
mirroring), while following the same evaluation protocol. We
achieved with a generic localization network that was trained
alsoperformhyper-parameteroptimizationforthelearningrate
onlyonsyntheticdata. Domain-specificdatawereusedonlyto
(1.25·104and5·104),learningdecay(nodecay,orstep-down
trainthecenter-directionregressionnetwork.
at65%and85%),thebestepochandthethreshold. Resultsare
Related methods. We compare the proposed method with a reportedwithhyper-parametersselectedonthevalidationset.
number of related approaches that are currently the best per-
forming methods for each dataset as reported in the literature. Sorghum. On Sorghum dataset the proposed model outper-
We also include CenterNet [15] as a closely related approach forms existing state-of-the-art point supervision approaches,
thatcombinestwomostcommonapproachesforcounting: (a) both in terms of counting and in localization, as shown in Ta-
direct prediction of center locations as a heatmap trained us- ble1. Intermsofcounting, theCeDiRNetoutperformsSOTA
ing the binary cross-entropy loss, and (b) direct regression of with MAE of 1.78 versus MAE of 1.90 for the second-best
8Figure6:Examplesofdetections(top)andregressedcenter-directions(bottom)onCARPKdatasetbasedonτ=40
model (lower is better), while CenterNet laggs behind with CARPK PUCPR+
Method
MAE of 2.16. In RMSE metric, CeDiRNet slightly lags be- MAE RMSE MAE RMSE
hindtheSOTAmodel,butstilloutperformsFasterR-CNN,the
RetinaNet[25] 16.62 22.3 24.58 33.12
methodbyTongetal.[13]andCenterNet. SinceRMSEpenal-
Goldmanetal.[10] 6.77 8.52 7.16 12
izeslargererrorsmore, thismaybeanindicationthatthepro-
Lietal.[29] 5.24 7.38 3.92 5.06
posedmodelhasalargervarianceoferrors, i.e., fewerimages
Caietal.[34] 4.6 6.55 3.68 5.47
with count errors, but each error is slightly larger. CeDiRNet
Wangetal.[12] 4.95 7.09 3.2 4.83
outperforms all SOTA models when considering a less biased
MSdeArrudaetal.[26] 4.45 6.18 3.16 4.39
localizationmetric,wherethecenter-directionmodelisableto CenterNet[15] 6.65 8.44 1.72 2.52
achieve F 1 of 90.32% while Ribera et al. [11] achieves F 1 of CeDiRNet(our) 4.43 6.33 1.68 2.61
88.10%. CeDiRNet also outperforms CenterNet, with Center-
Net lagging slightly behind at F of 89.92%. Localization er- Table2:ResultsonCARPKandPUCPR+[25]datasets.
1
rorsarereportedatτ = 5,butwealsoreportvaluesatlargerτ
for easier comparison with other models. Tong et al. [13] did
plants at the edge of the field that are not annotated), due to
not explicitly report which τ was used for their Sorghum ex-
overlapofmultipleplantsmaking itdifficulttoseparatethem,
periment,buttheyreportusingτbetween20and40intheirex-
orinsomecases,duetotruncatedplantsattheedgeoftheim-
periments.CeDiRNetachievesbetterlocalizationthanreported
age,whichareoftennotannotated. Truncatedandsimilarplant
resultsalreadyatτ ≥ 15. Atalargerτ,CeDiRNetalsooutper-
casesareparticularlyproblematicforthisdataset,sincetheyare
formsCenterNetbyaround1percentagepointinallcases.
notalwaysannotatedandmaybesimilartoannotatedplants,re-
Some detections are visualized in Fig. 4 and 5 with green sultinginbothconflictingtrainingsignalaswellasinincorrect
and red crosses representing true and false detections, respec- detections. In all cases, missing and incorrect detections were
tively,whilegroundtruthcentersaredepictedusinggreendots. the result of incorrect regression of center-direction, while er-
Fig.4alsoshowsregressedcenter-directionfieldsCˆ andCˆ rors in the localization network were never the main cause of
sin cos
aswellasthecorrespondingdirectionangleϕ. Probabilityout- issues. Visualization of center-directions also shows that re-
put map from the localization network is also visualized. De- gression of the closest object can be achieved even for pixels
picted detections indicate that CeDiRNet performs really well farther away from objects thus resulting in greater support for
inmajorityofcases,whilefailurecasesareoftenambiguousex- eachdetection.
amples due to similarity to the sorghum plant (detecting other Comparingtheresultswithτ = 5andτ ≥ 15alsoindicates
9Acacia-06 Acacia-12 Oilpalm
Method Annotation
prec[%] recall[%] F [%] prec[%] recall[%] F [%] prec[%] recall[%] F [%]
1 1 1
FasterR-CNN[35] boundingbox 97.8 97.2 97.5 94.2 96.5 95.3 98.9 98.3 98.5
FPN[28] boundingbox 97.6 97.4 97.6 95.1 96.6 96.2 98.8 97.9 98.4
WSDDN[36] imagelevel 77.6 70.2 71.5 74.3 71.8 72.9 75.8 73.6 97.5
PCL[37] imagelevel 78.5 75.1 77.3 79.2 76.8 78.6 76.4 74.7 75.9
C-MIL[38] imagelevel 87.9 82.6 86.8 86.5 83.9 53.0 86.4 84.7 85.8
Tongetal.[13] pointlevel 98.3 97.5 97.9 95 96.7 95.8 99.5 99.2 99.4
CeDiRNet(our) pointlevel 98.74 98.21 98.48 95.46 97.54 96.49 99.94 99.08 99.51
Table3:ResultsonAcacia-06,Acacia-16andOilpalmdatasets.
thatusingonly5pixelerrordistanceforatruepositivemaybe center-direction approach outperforms all related methods, as
toorestrictiveandlargerτwouldbemoreappropriate,particu- showninTable3. Forfaircomparison,resultsarereportedus-
larlysincemostobjectsinthisdatasetarealwayslargerthan10 ing the same τ as in [13], i.e., using τ = 40 for Acacia-06/12
pixels. ThisisalsowellshownindetectionsdepictedinFig.5, datasets and using τ = 30 for Oil palm dataset. CeDiRNet
where several detections are marked as false positives despite outperforms the state-of-the-art point supervision model [13]
thecorrectdetectionofplants. by around 0.5 - 1 percentage points. Despite a small percent-
age point difference, this results in over hundreds of fewer er-
CARPKandPUCPR+. ResultsforCARPKandPUCPR+are
rorssinceAcaciadatasetscontainaround20,000objects,while
shown in Table 2. In this table we report only MAE and
Oilpalmdatasetcontainsover130,000objects. Theproposed
RMSE metrics since all related literature evaluating on those
center-direction method also outperforms methods that were
datasets reports only counting errors. We report localization
trainedwithonlyimage-level annotationsaswellasbounding
metrics with τ = 40 in Table 6. The results show that the
boxmethods,suchasFasterR-CNN[35]andFPN[28].
proposedCeDiRNetoutperformsthestate-of-the-artmodelson
Examples of some correct and incorrect detections are de-
both car datasets. Best performing SOTA model MS de Ar-
picted in Fig. 7. False detections occur mostly on other trees,
ruda et al. [26] is comparable to CeDiRNet on CARPK since
which are not part of the plantation and are visually similar,
SOTA is better in RMSE metric (6.33 versus 6.18, for ours while missed detections can arise from smaller/younger trees
andSOTArespectively), butCeDiRNetisbetterinMAEmet-
thatareeitherpartlyoccludedbyshadeorarevisuallysignifi-
ric (4.43 versus 4.45, for ours and SOTA respectively). This cantlydifferentfromothertrees.
discrepancyinMAEandRMSEmetricsmayindicatefewerer-
rorsbutalargervarianceinCeDiRNet,similartotheresultson
Sorghum dataset. Nevertheless, CeDiRNet significantly out- 4.5. Ablationstudy
performs all remaining models in CARPK dataset, including
Weperformedanextensiveablationstudythatprovidesfur-
CenterNet, which achieved MAE of only 6.65. On PUCPR+
therinformationonourdesignchoices,whilealsodemonstrat-
dataset, CeDiRNet slightly outperformed CenterNet, but sig-
ingadditionalbenefitsoftheproposedmethod.
nificantlyoutperformedthemodelbyMSdeArrudaetal.[26],
bothinMAE(1.68versus3.16,foroursand[26]respectively)
Contributionofbackgroundpixels. Inthefirstexperiment,we
andRMSE(2.61and4.39foroursand[26]respectively)met-
wantedtoverifywhetherregressionofcenter-directionsinpix-
rics.
elslocatedfartherawayfromthecenterandoftheobjectitself
Some examples of detections are also depicted in Fig. 6 for
CARPK and in Fig. 5 for PUCPR+. Both figures also show provesuseful. Inthisexperiment,thecentersareregresseddur-
some difficult occluded examples behind the trees and at the ingtrainingonlyinthepixelsthatareinsidetheobjects’bound-
edge of the image, which would be difficult to detect even for ing boxes. The other (background) pixels are then regressed
human observer. In PUCPR+, some truncated examples are towards0andthereforenotuseddirectlyforcenterprediction
(as in most related works). We performed this experiment us-
alsoannotatedinconsistently,causingincorrectfalsedetections.
ingtheCARPKandPUCPR+datasets,sincethesetwodatasets
Visualizationofregressedcenter-directionsfortherightimage
provide bounding boxes for the objects to be detected. In Ta-
in Fig. 6 also reveals the extent at which center-directions can
ble 4, we can see that the performance decreases compared to
beregressed.Thisextendssignificantlybeyondtheboundaryof
themodelthatalsoregressesthecenter-directionsintheback-
theobjectresultinginalargersupportforeachcenterpoint.Al-
ground pixels. This clearly shows the advantage of regressing
thoughtheregressedvaluesinverydistantpixelsmaybefairly
center-directionseveninthepixelsoutsidetheboundingboxes.
noisy,thisdoesnotcauseanyfalsedetectionsinthisregiondue
They do contain information that proves useful for estimating
to the specific visual appearance that the localization network
the centers in the second stage of the proposed approach. In
istunedto.
addition,wealsoevaluatedamodelthatcompletelyignoresthe
Acacia-06/12 and Oil palm. Comparing to the related works pixels outside the bounding boxes, i.e., does not include them
that reported results on tree plantation datasets, the proposed inthecomputationoftheloss,whichledtoevenworseresults
10Figure7: Examplesofregressedcenter-directionsandlocalizationdetectionsonAcacia-06,Acacia-12andOilpalmdatasetsintop,middleandbottomrows,
respectively.
sincetheinferenceinthebackgroundpixelsbecomesquiteun- CARPK PUCPR+
predictable. CeDiRNet BBonly noBG CeDiRNet BBonly noBG
prec[%] 98.23 98.44 81.98 99.57 96.81 69.27
recall[%] 94.69 94.07 84.11 98.41 98.11 93.27
Balancing foreground and background losses. We also ana- F 1[%] 96.21 95.88 80.76 98.98 97.11 71.52
lyzed how sensitive is the proposed weighting scheme for re- MAE 4.43 5.50 14.42 1.68 1.72 23.72
RMSE 6.33 8.15 19.62 2.61 2.75 26.22
gressing center-directions (Eq. 6). The weighting scheme bal-
ancesthelearningofsmallnumberofcenter-directionsaround
Table4: Denseregressionofcenter-directions-CeDiRNet: asproposed;BB
objectsversusthelearningoflargernumberofcenter-directions only:onlyfromtheobjectboundingboxesandregressingtheotherpixelsto0;
furtherawayfromobjects. Toanalyzethesensitivity,weevalu- noBG:completelyignoringthebackgroundpixels.
atedamodelusingvariouscutoffdistancethresholdvaluesfor
theweightingscheme. Thisalsoincludedmodelwithϵ =0,ef-
fectively disabling the weighting scheme. All models for this alsobechosenforeachdomainspecificallybasedonexpected
analysis were evaluated on Sorghum validation set to avoid proximity of objects in that domain, which would reduce free
overfitting ϵ, as selection of ϵ for all other experiments was hyperparametersfortheproposedcenter-directionlearning. A
basedonthisanalysis. distributionofdistancestothenearestobjectinSorghumtrain-
Results with various ϵ values are shown in Table 5. Note ing set is shown in Fig. 8. Taking a median would result in
thatthetablereports2ϵasthisrepresentsthediameterofpixels 2ϵ = 32 for this dataset, which is similar to what we used.
Moreover,thesevaluescouldalsobesetwithamorecomplex
around the object center instead of a radius. Results demon-
stratethatwellperformingmodelshave2ϵ between10and30 schemes, such as ones based on density of objects in each in-
dividual image, however, we leave this for a potential future
pixels. Smaller values had slightly lower results, while com-
researchopportunity.
pletelydisablingtheweightingschemealsohadanegativeef-
fect, with slightly worse localization error. However, 2ϵ val-
ueslargerthan40pixelsresultedinsignificantlyworseperfor-
mance,havingover5-10percentagepointsworselocalization L loss for localization network. For training the localization
1
error. Prioritizing too large area around the object center may network, we evaluate the use of L loss against the focal loss,
1
startincludingtoomanybackgroundpixelsthatoverwhelmthe which has been commonly used in the literature for learning
learningaroundtheactualobjects. probability maps of centers. For this experiment, we imple-
Basedonthisanalysis,wefixed2ϵ = 30forallexperiments mentthefocallossbyreplacingL learninglossfunctionL
1 cent
on this dataset as well as on other datasets. Note that ϵ may (Eq.12)withthefocal-lossimplementationcommonlyusedin
11
60-aicacA
21-aicacA
mlap
liO2ϵ prec[%] recall[%] F [%] MAE RMSE Hand-crafted localization network. We also evaluated an al-
1
ternative to the localization network, where we used a sim-
0px 90.16 89.95 90.00 1.44 2.18
ple non-learnable hand-crafted network instead of a learnable
5px 90.47 91.01 90.67 1.52 2.43
one. Since the appearance of center-directions changes grad-
10px 91.77 91.98 91.83 1.37 2.01
ually from -1 to 1 in horizontal and vertical directions, we
20px 91.46 92.17 91.76 1.40 2.19
can replace the 9-layer hourglass network with one layer of
30px 91.05 91.29 91.12 1.33 2.02
40px 83.98 84.64 84.24 1.61 2.60 1D convolutions applied in the vertical direction for C sin and
50px 78.45 78.68 78.52 1.42 2.09 the horizontal direction for C cos channels. Instead of learn-
ing, the convolution weights can be pre-defined to activate on
Table5: Sensitivityanalysisofthecutoffdistancethresholdϵ thatisusedin edges using [−1,0,1] filters. We used seven different sizes of
W(xi,j)toemphasizethelearningofforegroundpixels. Resultsarereported kernels, [3,9,15,21,31,51,65], toaccountfordifferentobject
onSorghumvalidationset. Notethatwereport2ϵ whichcorrespondstothe
scales. Although fairly large convolution weights were used,
diameteraroundtheannotationpoint.
theyareonlysingledimensionalandcanbeimplementedeffi-
cientlywithsignificantlysmallercomputationalcostthananor-
Distribution of nearest neighbors
malneuralnetwork. Responsestoallfilterswerethensummed
to form the final response, where local maxima were located.
300K
For this experiment, we also optimized hyperparameters for
250K
center-direction regression network separately from the previ-
200K
ousexperiments,sincetheoptimizationprocessisdependenton
150K thefinalscoringthatisprovidedbythelocalizationnetwork.
100K Resultswithhand-craftedlocalizationnetworksareshownin
50K thelastcolumnofTable6,wherewereporttheresultsforallsix
0 datasets on respective testing set. Although hand-crafted net-
20 30 40 50 100
work is being out-competed by the learnable network with L
Distance [px] 1
loss, the results with hand-crafted network are still better than
Figure8: DistributionofdistancestonearestneighboronSorghumtraining otherstate-of-the-artmodelsonalldatasetsexceptforCARPK.
dataset.
The hand-crafted network even slightly outperforms learnable
localization model on Acacia-12 dataset in both F and preci-
1
otherpoint-supervisionmethods[12]: sion(F 1 andprecisionof96.60%and95.80%,respectively,vs
96.49% and 95.46%), while in PUCPR+ it outperformslearn-
L cent =− NW ·ce Mnt (cid:88)
i,j
  (cid:16) A1 (cid:16) 1− −Oˆ Ocen (cid:17)t δ(cid:17) Oγ ˆl γ co eng t(cid:16) loOˆ gc (cid:16)e 1nt(cid:17) −, Oˆ cent(cid:17) , oif thO er= w1 is, e a m 1b .6ele t 8ril aco nsc da (Ml 2iz .A 6a 1t Ei )on a bn um d
t
no R od M tel iS ni En looc co f au l1 in z.6t ai 8 tn ig oa nne dr er ro r2 or .5 rb 1 (a , Fs re 1ed s op fo en 9c 8tc i .v 4o e 0u l %n y,tin vvg ss
(17) 98.98%).
Such good performance compared to other state-of-the-art
whereOˆ andO arepredictedprobabilityandtheground models demonstrates that regression of center-directions ex-
cent cent
truthofacenter,respectively. Lossisappliedper-pixelbutwe poses information on the location of objects in a manner that
omit location (x ) in both terms for clarity. Hyperparameters canbeextractedbyafairlysimpleprocessof1Dconvolutions.
i,j
W ,A,δandγweresetforbestperformancebyexperiment- Moreover, this can be achieved at a low computational cost
cent
ing with four different models based on four different combi- since1Dconvolutioncanbeimplementedefficientlywithsig-
nationsofparameters. Asadefaultversion, wesetW = 1, nificantly smaller cost than regular convolutional network. A
cent
δ=4,γ=5andA=1/16basedon[12],andthenexperimented solutionwithhand-craftedlocalizationnetworkmaybepartic-
withchangingW =10aswellaschangingsettingsγ=2and ularlysuitableforapplicationswherecomputationalcostplays
cent
A = 1/50(correspondingtow
fg
= 50thatwasusedinourorig- animportantfactor,whileinapplicationswherecomputational
inal L -based loss). Four combinations of focal hyperparam- costislessofanissue,slightperformancegaincanbeachieved
1
eterswere optimizedtogether withother twohyperparameters byusingthelearnablenetwork.
forcenter-directionregression(weightdecay, percentofbatch
size for hard samples) and the best performing model was se- Domain-specificfine-tuningofthelocalizationnetwork. Next,
lectedonthevalidationsetforeachdatasetindependently. All wetestedwhetheragenericlocalizationnetworkcouldbefur-
focal-loss based models were learned with a learning rate of ther improved by fine-tuning it on a specific domain. In this
0.01 using the same procedure as used for the L loss model, experiment,weusedapre-trainedgenericlocalizationnetwork
1
e.g.,learningfor200epochswithabatchsizeof768andusing forinitializationandthenlearnedthelocalizationmodelsimul-
5000syntheticallygenerateimages.Resultsforthelocalization taneouslywiththecenter-directionregressionnetworkusinga
networklearnedwiththefocallossareshowninTable6,where real output data of center-direction regressions instead of the
wereporttheresultsforallsixdatasetsonthetestingset. Inall syntheticone. Syntheticdatawasusedonlywhenlearningthe
cases,theproposedL lossmodelsoutperformedthefocal-loss pre-trainedlocalizationmodel.
1
models. The results on the domain-specific fine-tuning are shown in
12
tnuoCL loss focal-loss hand-crafted L loss focal-loss hand-crafted L loss focal-loss hand-crafted
1 1 1
Sorghum CARPK PUCPR+
prec[%] 89.76 89.49 89.50 98.23 96.16 98.22 99.57 99.06 98.79
recall[%] 91.03 90.99 91.22 94.69 94.39 94.17 98.41 98.37 98.02
F [%] 90.32 90.16 90.28 96.21 94.74 95.94 98.98 98.69 98.40
1
MAE 1.78 1.83 1.86 4.43 4.66 4.84 1.68 1.96 1.68
RMSE 2.99 3.09 3.14 6.33 6.66 7.09 2.61 2.69 2.51
Acacia-06 Acacia-12 Oilpalm
prec[%] 98.74 98.66 98.60 95.46 95.05 95.80 99.94 99.44 99.94
recall[%] 98.21 98.16 98.25 97.54 97.36 97.41 99.08 99.13 99.07
F [%] 98.48 98.41 98.42 96.49 96.19 96.60 99.51 99.29 99.50
1
Table6:AblationstudyonCeDiRNetwithdifferentlocalizationnetworkvariants.
synthetic domain prec[%] recall[%] F [%] MAE RMSE values with various levels of occlusion and running corrupted
1
imagesthroughthelocalizationnetwork. Wecreatedanocclu-
89.50 91.22 90.28 1.86 3.14
✓ 89.76 91.03 90.32 1.78 2.99 sion noise by uniformly sampling values for each pixel, then
✓ ✓ 89.26 90.90 90.00 1.84 3.03 blurringthemwiththeGaussiankernelusingσ=5andfinally
thresholding them to get small occluded regions. By varying
Table7:Ablationstudyonusingdifferenttrainingdatatypesforthelocalization thethreshold,differentpercentagesofoccludedpixelswereob-
networksonSorghumtestingdataset.Resultsareshownfor:(a)non-learnable
tained,ascanbeseenatthebottominFigure9.
hand-craftednetworkinthefirstrow,(b)genericnetworktrainedonlyonsyn-
theticdatainthesecondrow, and(c)additionaldomain-specificfine-tuning We applied occlusion masks to the groundtruth data of
appliedtothegenericnetworkinthethirdrow. CARPKandPUCPR+datasets. ResultsareshowninFigure9.
We can observe robust detection even in the presence of seri-
ousocclusions. Bothnetworksarefairlyrobusttoupto30%of
Table 7, where we report the results for Sorghum dataset on
imageocclusion. Thelearnedlocalizationnetworkhasproven
the testing set. Comparison of the second and the third row
tobesignificantlymoreresilientthanthehand-craftednetwork
thatshowlearnablelocalizationmodelswithoutandwithfine-
as it is able to maintain performance even at an extreme level
tuning, respectively, indicates that the domain-specific fine-
of occlusion of up to 60%, as depicted in Figure 9. Possible
tuningdidnotimproveperformanceinanymetric. Observing
explanation for such robustness can be found in augmentation
aperformancedeclineduringfine-tuningmayseemcounterin-
withrandomGaussiannoiseappliedduringthetrainingofthe
tuitive, given the expectation of added benefits from domain-
localizationnetwork.
specific information. This may be attributed to the risk of
overfitting to specific visual appearances in real training data Inference running times. In Table 8, we also report the in-
stemmingfromlimiteddiversityincenter-directions(e.g.,their ference runtime analysis. Running times were calculated on
count and dispersion) and the absence of augmentation (e.g., images of 1280 × 736 pixels in size using a single NVIDIA
synthetic occlusions and noise). These factors pose more sig- GeForceRTX2080TiGPUandIntelXeonSilver4214CPU.
nificant constraints than training on synthetic data where ran- Wereportaveragedmeasurementsrepeatedtentimes,eachtime
domly generated locations ensure uniqueness for each batch. performing a warm-up with 100 runs before measuring actual
Thissuggeststhatthevisualappearanceofcenter-directionsre- runtime. We report results for CeDiRNet model with both lo-
mainssufficientlyconstrained,facilitatingeffectiveuseofsyn-
calization networks and use CenterNet model for comparison.
theticdatafortrainingthelocalizationnetworkandsimplifying Regressionofcenter-directionsinCeDiRNetrequiredanaddi-
applicationtodiversedomains,whereonlythecenter-direction tional 20% of the total time compared to the CenterNet. This
regressionmodelneedstobelearned. canbecontributedtotheFPNdecoderusedbyCeDiRNet,since
Comparedtonon-learnablehand-craftedmodel,thedomain- CenterNetdoesnotuseit. ThelocalizationnetworkinCeDiR-
specificfine-tuningisstillslightlybetterintermsofcounterror, Netthentookadditional10msforthelearnednetwork, while
butintermsofthelocalizationerroritperformedworseinboth hand-crafted network took only 2 ms. In both cases the time
precision and recall. Nevertheless, the differences between all needed for post-processing was almost negligible. The pro-
three models are fairly small, and domain-specific fine-tuning posed method is therefore quite fast, operating at around 13 -
alsooutperformsthebeststate-of-the-artmodelreportedonthis 15FPSonHDimages.
dataset.
5. Conclusion
Robustnessanalysisofthelocalizationnetwork. Weperformed
additionalanalysisofthelocalizationnetworktoassessthero- In this work we presented a novel method for counting and
bustnesstofailuresoftheregressedcenter-directions. Weper- localizationbasedonpointsupervisionbyusinganovelcenter-
formedanexperimentbyartificiallycorruptingcenter-direction direction approach termed CeDiRNet. We proposed to split
13Learned net Hand-crafted net
Robustness of localization network + ++ + + ++++ ++++
100 + + ++ ++
+ ++ + +++
90
++ ++++ + + +++ ++++++ + +
+++ +++ +++ +++
80
+++ + ++++ +
70 + + Learned net (CARPK)
Learned net (PUCPR+) +
60
Hand-crafted net (CARPK) + +
Hand-crafted net (PUCPR+)
50 + + +
0 10 20 30 40 50 60 70
Percentage of image occlusion [%] +
0% 10% 20% 30% 40% 50% 60% 70%
Figure9:Performanceoflocalizationnetworkatvariouslevelsofocclusionnoiseontheleft,withoneexampleontherightshowingcosineofcenter-directionat
0.4and0.7occlusionnoiseforlearnedandhand-craftednetwork,andvariousdegreesofocclusionnoiseatthebottom.
the problem into first performing dense regression of center- onsyntheticdata,and(c)simplifiedlearningwithfewerhyper-
directions and then performing localization of centers from parametersandnoadditionalpost-processingheuristics.
densely regressed center-direction fields. Formulating center-
Despitedemonstratedstate-of-the-artperformanceonseveral
directionsasare-parametrizationofsineandcosinevaluesen-
remotesensingdomainstherearecertainlimitationsofthepro-
ableddetectionofobject’scenterlocationwithalargersupport
posed method. CeDiRNet method can be sensitive to certain
from multiple surrounding pixels. At the same time, it also
typesofocclusions,particularlywhentwoormoreobjectsare
pawed the way for using generic non-complex model for lo-
positioned such that they have the same center point and sur-
calization of centers, which can be not only trained from syn- rounding pixels from different objects point to the same loca-
thetic data but also does not require any domain-specific fine-
tionthusmakingimpossibleforthelocalizationnetworktodis-
tuning. Thisfacilitatesapplicationtodifferentdomainsbysim-
tinguishesthem(e.g.,alargeshipcarryingasmallerone). An
plylearningthecenter-directionregressionmodel.
additional limitation can also be found in dealing with highly
Wehavedemonstratedonsixdifferentdatasetsforcounting crowded scenes, where small objects are densely packed to-
and localization in the remote sensing domain that CeDiRNet gether,resultinginasmallareaaroundeachobjectcenter. This
outperforms all existing state-of-the-art models for counting limitsthenumberofpixelsthatpointtoaspecificcenterandcan
and localization that are trained with point supervision. Re- resultinblendingtheneighboringdirectionvectors,makinglo-
gressionofcenter-directionsisthusshowntobeaviablesolu- calizationnetworkdifficulttodistinguishbetweenobjects.This
tionforlearningdensepredictionofcenterpointsfromasetof limitation could be addressed by increasing the resolution of
sparseannotations,whichinotherapproachesisaddressedwith bothhigh-levelfeaturemapsaswellasresolutionoftheoutput
focallossandfine-tuningoftheirhyperparametersforeachdo- directionsthusprovidingmorepixelsthatwouldpointtowards
main. Withcenter-directionsthisproblemcanbeconfinedonly thecenter.
tothelearningofonegenericlocalizationnetwork, whichcan
The concept of regressing center-directions is also not lim-
betrainedonceonsyntheticdata,whileineachdomainwesim-
ited to counting or localization and may be applied to other
plylearndenseregressionofcenter-directions. Evenapplying
tasksaswell. Infuturework,weplanonextendingCeDiRNet
focal lossto thelearning of ourlocalization networkis shown
beyondpoint-supervisedlocalizationbyapplyingittoinstance
asnotneeded. Thisalsosimplifieslearningofthelocalization
segmentation task. In instance segmentation, background pix-
network,andallowstouseasimplelocalmaximaforlocating
els could be learned as zero-magnitude center-directions, thus
thefinalcentersinthelocalizationheatmapwithouttheneedto
servingdualpurposeofencodingsegmentationmaskandpro-
applyadditionalpost-processingheuristics.
viding direction to the centers. Each instance could then be
The proposed CeDiRNet also has a high applicative value. determined from a group of regressed center-directions that
This stems directly from the concept of pointing towards the all point towards the same center. Since the same model
centers,whichenables: (a)trainingwithalow-costpointanno- can also be used for point-supervised localization, this pro-
tations,(b)learningofonlycenter-directionregressionfordif- videsanelegantapproachforlearningwithmixed-supervision
ferent domains and using generic localization network trained that would provide a way to reduce the annotation effort, i.e.,
14
]%[
erusaem
- 1F
snoisulcco
%04
snoisulcco
%07Method Localizationnetwork Regression Localization Post-processing
Learned 64.7±0.3ms 10.6±0.2ms 0.6±0.0ms
CeDiRNet(our)
Hand-crafted 64.4±0.6ms 1.9±0.0ms 0.5±0.0ms
CenterNet / 53.6±0.3ms 1.3±0.1ms
Table8:Inferenceruntimesreportedonanimageofsize1280×736usingasingleGPU.
enable using instance annotation for a smaller subset of im- [12] Q.Wang,J.Chen,J.Deng,X.Zhang,3D-CenterNet:3Dobjectdetection
ages and point annotation for others. Our preliminary results networkforpointcloudswithcenterestimationpriority,PatternRecogni-
tion115(2021)107884.
also showed promising results when adding perturbations to
[13] P.Tong,P.Han,S.Li,N.Li,S.Bu,Q.Li,K.Li,Countingtreeswith
groundtruth centers, which may merit more research focus in
point-wisesupervisedsegmentationnetwork, EngineeringApplications
the future. Finally, the proposed concept of regressing center- ofArtificialIntelligence100(January)(2021).
directionsisalsoagnostictoanyspecificbackbonearchitecture [14] T.Y.Lin,P.Goyal,R.Girshick,K.He,P.Dollar,FocalLossforDense
ObjectDetection,in: ProceedingsoftheIEEEInternationalConference
and could be implemented with any modern deep learning ar-
onComputerVision,2017,pp.2999–3007.
chitecturefordenseprediction. WhileweusedResNet-FPNin
[15] X.Zhou,D.Wang,P.Kra¨henbu¨hl,ObjectsasPoints(2019).
our work, we already found promising results with ConvNext URLhttp://arxiv.org/abs/1904.07850
and transformer-based backbones, but we leave further opti- [16] D.Neven,B.D.Brabandere,M.Proesmans,L.VanGool,Instanceseg-
mentationbyjointlyoptimizingspatialembeddingsandclusteringband-
mizationforbothregressionandlocalizationnetworksasfuture
width,in:ProceedingsoftheIEEE/CVFConferenceonComputerVision
researchopportunities. andPatternRecognition,2019,pp.8829–8837.
[17] T.Zhou,W.Wang,S.Liu,Y.Yang,L.VanGool,DifferentiableMulti-
GranularityHumanRepresentationLearningforInstance-AwareHuman
Acknowledgments SemanticParsing,in:ProceedingsoftheIEEE/CVFConferenceonCom-
puterVisionandPatternRecognition,2021,pp.1622–1631.
[18] H.Foroughi,N.Ray,H.Zhang,Robustpeoplecountingusingsparserep-
This work was in part supported by the ARRS research
resentationandrandomprojection, PatternRecognition48(10)(2015)
project J2-3169 (MV4.0) and J2-4457 (RTFM) as well as by 3038–3052.
researchprogrammeP2-0214. [19] A.J.Schofield,P.A.Mehta,T.J.Stonham,Asystemforcountingpeople
invideoimagesusingneuralnetworkstoidentifythebackgroundscene,
PatternRecognition29(8)(1996)1421–1428.
[20] Y.Zhang,D.Zhou,S.Chen,S.Gao,Y.Ma,Single-imagecrowdcount-
References
ingviamulti-columnconvolutionalneuralnetwork, in: Proceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,
[1] A. B. Chan, Z.-S. J. Liang, N. Vasconcelos, Privacy preserving crowd 2016,pp.589–597.
monitoring:Countingpeoplewithoutpeoplemodelsortracking,in:IEEE [21] H.T.Nguyen,C.W.Ngo,W.K.Chan,SibNet: Foodinstancecounting
ConferenceonComputerVisionandPatternRecognition,2008. andsegmentation,PatternRecognition124(2022)108470.
[2] J.Liu,C.Gao,D.Meng,A.G.Hauptmann,DecideNet:CountingVary- [22] H.Cholakkal,G.Sun,F.ShahbazKhan,L.Shao,Objectcountingand
ingDensityCrowds,in: ComputerVisionandPatternRecognitionCon- instancesegmentationwithimage-levelsupervision,Proceedingsofthe
ference,2018,pp.5197–5206. IEEE/CVF Conference on Computer Vision and Pattern Recognition
[3] L. Rong, C. Li, Coarse- And fine-grained attention network with (2019)12389–12397.
background-awarelossforcrowddensitymapestimation,in: Proceed- [23] B.Wang,H.Liu,D.Samaras,M.Hoai,Distributionmatchingforcrowd
ings-2021IEEEWinterConferenceonApplicationsofComputerVi- counting, Advances in Neural Information Processing Systems 2020-
sion,2021,pp.3674–3683. Decem(NeurIPS)(2020)1–13.
[4] J.Wan,A.B.Chan,Modelingnoisyannotationsforcrowdcounting,Ad- [24] Y.Li,X.Zhang,D.Chen,CSRNet: DilatedConvolutionalNeuralNet-
vancesinNeuralInformationProcessingSystems33(NeurIPS)(2020) worksforUnderstandingtheHighlyCongestedScenesUniversityofIlli-
3386–3396. noisatUrbana-ChampaignBeijingUniversityofPostsandTelecommu-
[5] Y.Lei,Y.Liu,P.Zhang,L.Liu,Towardsusingcount-levelweaksupervi- nications, in: Proceedings of the IEEE/CVF Conference on Computer
sionforcrowdcounting,PatternRecognition109(2021). VisionandPatternRecognition,2018,pp.1091–1100.
[6] K.He,G.Gkioxari,P.Dolla´r,R.Girshick,MaskR-CNN,in: Proceed- [25] M. R. Hsieh, Y. L. Lin, W. H. Hsu, Drone-Based Object Counting by
ingsoftheIEEEInternationalConferenceonComputerVision,2017,pp. Spatially Regularized Regional Proposal Network, in: Proceedings of
2961–2969. theIEEEInternationalConferenceonComputerVision,2017,pp.4165–
[7] J.Redmon,A.Farhadi,YOLO9000: Better,Faster,Stronger,in: IEEE 4173.
Conference on Computer Vision and Pattern Recognition, 2017, pp. [26] M.d.S.deArruda,L.P.Osco,P.R.Acosta,D.N.Gonc¸alves,J.Marcato
6517–6525. Junior, A.P.M.Ramos, E.T.Matsubara, Z.Luo, J.Li, J.d.A.Silva,
[8] Z.Tian,C.Shen,H.Chen,T.He,FCOS:FullyConvolutionalOne-Stage W.N.Gonc¸alves,Countingandlocatinghigh-densityobjectsusingcon-
ObjectDetection,in: ProceedingsoftheIEEEInternationalConference volutionalneuralnetwork,ExpertSystemswithApplications195(May
onComputerVision,2019. 2020)(2022)116555.
[9] Z.Piao,J.Wang,L.Tanga,B.Zhao,W.Wang,AccLoc:Anchor-Freeand [27] H.Law,J.Deng,CornerNet:DetectingObjectsasPairedKeypoints,In-
two-stagedetectorforaccurateobjectlocalization, PatternRecognition ternationalJournalofComputerVision128(3)(2020)642–656.
126(2022)108523. [28] T.-Y.Lin,P.Dollar,R.Girshick,K.He,B.Hariharan,S.Belongie,Fea-
[10] E.Goldman,R.Herzig,A.Eisenschtat,J.Goldberger,T.Hassner,Precise ture Pyramid Networks for Object Detection, in: Proceedings of the
detection in densely packed scenes, in: Proceedings of the IEEE/CVF IEEE/CVF Conference on Computer Vision and Pattern Recognition,
Conference on Computer Vision and Pattern Recognition, 2019, pp. IEEE,2017,pp.936–944.
5222–5231. [29] W.Li, H.Li, Q.Wu, X.Chen, K.N.Ngan, SimultaneouslyDetecting
[11] J.Ribera,D.Guera,Y.Chen,E.J.Delp,Locatingobjectswithoutbound- andCountingDenseVehiclesFromDroneImages,IEEETransactionson
ingboxes, in: ProceedingsoftheIEEE/CVFConferenceonComputer IndustrialElectronics66(12)(2019)9651–9662.
VisionandPatternRecognition,2019,pp.6472–6482.
15[30] P.Yakubovskiy,SegmentationModelsPytorch,https://github.com/
qubvel/segmentation_models.pytorch(2020).
[31] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, A. L. Yuille,
DeepLab:SemanticImageSegmentationwithDeepConvolutionalNets,
AtrousConvolution,andFullyConnectedCRFs,IEEETransactionson
PatternAnalysisandMachineIntelligence40(4)(2018)834–848.
[32] H.W.Kuhn,TheHungarianmethodfortheassignmentproblem,Naval
ResearchLogisticsQuarterly2(1-2)(1955)83–97.
[33] X.Glorot,Y.Bengio,Understandingthedifficultyoftrainingdeepfeed-
forwardneuralnetworks,in: Proceedingsofthe13thInternationalCon-
ferenceonArtificialIntelligenceandStatistics,Vol.9,2010,pp.249–256.
[34] Y.Cai,D.Du,L.Zhang,L.Wen,W.Wang,Y.Wu,S.Lyu,GuidedAtten-
tionNetworkforObjectDetectionandCountingonDrones,in:MM2020
-Proceedingsofthe28thACMInternationalConferenceonMultimedia,
2020,pp.709–717.
[35] S.Ren,K.He,R.Girshick,J.Sun,FasterR-CNN:TowardsReal-Time
ObjectDetectionwithRegionProposalNetworks,NIPS(2015)1–10.
[36] H.Bilen,A.Vedaldi,WeaklySupervisedDeepDetectionNetworks,Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(2016)2846–2854.
[37] P.Tang,X.Wang,S.Bai,W.Shen,X.Bai,W.Liu,A.Yuille,PCL:Pro-
posalClusterLearningforWeaklySupervisedObjectDetection, IEEE
TransactionsonPatternAnalysisandMachineIntelligence42(1)(2020)
176–191.
[38] F.Wan,C.Liu,W.Ke,X.Ji,J.Jiao,Q.Ye,C-MIL:ContinuationMultiple
InstanceLearningforWeaklySupervisedObjectDetection,in: CVPR,
Vol.1,2019,pp.2199–2208.
16