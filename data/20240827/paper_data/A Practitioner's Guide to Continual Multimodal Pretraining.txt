A Practitioner’s Guide to Continual Multimodal Pretraining
Karsten Roth1,2,6∗ Vishaal Udandarao1,3∗ Sebastian Dziadzio1◦ Ameya Prabhu1◦
Mehdi Cherti4 Oriol Vinyals5 Olivier H´enaff5
Samuel Albanie† Matthias Bethge1† Zeynep Akata2,6,7†
1Tu¨bingen AI Center, University of Tu¨bingen 2Helmholtz Munich 3University of Cambridge 4LAION, Ju¨lich
Supercomputing Center (JSC), Research Center Ju¨lich (FZJ), Helmholtz Association 5Google DeepMind
6Munich Center for ML 7Technical University of Munich
∗equal project lead, order interchangeable ◦core contributors †equal supervision.
Abstract
Multimodal foundation models serve numerous applications at the intersection of vision and language.
Still,despitebeingpretrainedonextensivedata,theybecomeoutdatedovertime. Tokeepmodelsupdated,
research into continual pretraining mainly explores scenarios with either (1) infrequent, indiscriminate
updates on large-scale new data, or (2) frequent, sample-level updates. However, practical model
deployment often operates in the gap between these two limit cases, as real-world applications often
demand adaptation to specific subdomains, tasks or concepts — spread over the entire, varying life cycle
of a model. In this work, we complement current perspectives on continual pretraining through a research
test bed as well as provide comprehensive guidance for effective continual model updates in such scenarios.
WefirstintroduceFoMo-in-Flux,acontinualmultimodalpretrainingbenchmarkwithrealisticcompute
constraints and practical deployment requirements, constructed over 63 datasets with diverse visual
and semantic coverage. Using FoMo-in-Flux, we explore the complex landscape of practical continual
pretraining through multiple perspectives: (1) A data-centric investigation of data mixtures and stream
orderings that emulate real-world deployment situations, (2) a method-centric investigation ranging from
simple fine-tuning and traditional continual learning strategies to parameter-efficient updates and model
merging,(3)metalearningrateschedulesandmechanisticdesignchoices,and(4)theinfluenceofmodeland
compute scaling. Together, our insights provide a practitioner’s guide to continual multimodal pretraining
for real-world deployment. Our benchmark and code is here: github.com/ExplainableML/fomo in flux.
1 Introduction 2 6 Continual Pretraining: General Training
Recipes 16
2 Categorizing Continual Pretraining: A Version- 6.1 LearningRates,SchedulesandMeta-Schedules 17
ing Perspective 3 6.2 ScalingupModelandComputeBudgets . . . . 19
6.3 Model-specific tuning choices in compute-
3 The FoMo-in-Flux Benchmark 5 restrictedscenarios . . . . . . . . . . . . . . . . 21
3.1 Creation . . . . . . . . . . . . . . . . . . . . . . 6 6.4 SoftmaxTemperaturesforContrastiveLosses—
3.2 Pipeline,ComputeBudgetingandDataRestric- Not Too Hot! . . . . . . . . . . . . . . . . . . . 21
tions . . . . . . . . . . . . . . . . . . . . . . . . 8
3.3 DesigningData-CentricTask-Sequences . . . . 9 7 Continual Pretraining: A Data-Centric Perspec-
3.4 Verifying Downstream Datasets: Finetuning tive 22
mustimprovePerformance. . . . . . . . . . . . 11 7.1 Deploymentscenariosimpactcontinualpretrain-
ability . . . . . . . . . . . . . . . . . . . . . . . 22
4 Experimental Setup 11 7.2 Datamixturesinformknowledgeaccumulation
andzero-shotretention. . . . . . . . . . . . . . 24
5 Continual Pretraining: A Method Perspective 12 7.3 Choiceofpretrainingdatapoolsignificantlyim-
5.1 Parameter-efficient Finetuning and Continual pactszero-shotretention . . . . . . . . . . . . . 24
Learning . . . . . . . . . . . . . . . . . . . . . . 13
5.2 OntheBenefitsofModelMergingTechniques . 15 8 Conclusion 25
1
4202
guA
62
]VC.sc[
1v17441.8042:viXra1 Introduction
Foundation models [14]—whether unimodal or multimodal—are widely deployed, but remain expensive to
train [143, 29], requiring vast datasets and significant computational resources. Despite the substantial
investments, these models often have limited knowledge and concept coverage [181], and can quickly become
outdated as new tasks and subdomains emerge. To maintain relevance, they need continual pretraining. On a
high-level, continual pretraining methods fall into two categories: (1) infrequent, large-scale updates that
require substantial new data and computing power [49, 77], and (2) frequent, but minimal updates that
target specific pieces of information, often through knowledge editing or by updating the knowledge base in
retrieval-augmented systems [28, 192, 136, 58]. However, many real-world applications operate in the large
and complex gap between these limit cases; calling for specialized knowledge—such as fine-grained expert
knowledge or semantic and visual distribution shifts [88, 217, 180, 165, 118, 140, 157, 56, 156, 226, 47, 138]—
that goes beyond simple, localized edits. Information of such model shortcomings appears throughout the
entirelifecycleofamodelasnewdeploymentscenariosoccur, andgenerallydon’tjustifyretrainingtheentire
model from scratch. Using the terminology of the semantic versioning framework [144, 141], such specialized,
minor updates exceed the scope of simple patches, but do not warrant a major version update.
In this work, we provide a new research framework to emulate these complex practical deployment scenarios
for vision-language foundation models in a controllable environment, and study the different requirements for
continual pretraining to succeed under these circumstances. Our contributions are outlined as follows:
Creating a Suitable Benchmark. Tocontrollablystudydifferentspecialized(minor)updatesofmultimodal
models over a long model life cycle, we introduce FoMo-in-Flux (Foundation-Models-in-Flux, Fig. 1).
FoMo-in-Flux builds on 63 image classification and image-text retrieval datasets (publically available or
part of this work), enhanced with captions to enable multimodal pretraining. Unlike monolithic, noisy
web-crawl datasets like TiC-RedCaps and DataComp [49, 45], FoMo-in-Flux comprises curated, high-quality
samples with fine-grained class information and precise control over data streams spanning different visual
and semantic domains like natural and synthetic images, abstractions, or procedurally generated data.
Realistic Continual Pretraining. Unlike traditional continual learning research, we avoid the practically
unnecessary restriction of limited storage [136, 137], and allow unrestricted access to both pretraining and
adaptation data. Recognizing that deployment cost and practicability of potential remedies are primarily a
function of compute requirements, we only impose a restriction on the compute budgets. To avoid skewed
compute metrics [38, 119], we enforce constraints using Memory-Adjusted FLOPs (MAFs), which take into
account FLOP counts for forward and backward passes, as well as peak device (accelerator) memory required.
Which Methods are Effective for Continual Pretraining? Using FoMo-in-Flux, we determine the
sustainability of current research strategies for multiple sequential, minor continual pretraining updates
— ranging from existing continual learning (CL) regularization-based strategies like EWC [87] and SI [210],
simple finetuning, parameter-efficient adaptation like LoRA [73] and VeRA [89], to model merging [79].
On the Importance of Continual Pretraining Recipes. We showcase the importance of continual
pretraining strategies beyond simple method choices, such as learning rate scheduling, and propose task-
dependent meta schedules to facilitate long-term continuous, controlled model updates. Moreover, we study
both the impact of model and compute scaling on continual model pretrainability, and give an overview of
important experimental design choices when setting up a continual multimodal pretraining pipeline.
A Data-centric Perspective on Continual Pretraining. Lastly, the concepts and tasks that a model
shouldimproveonoftenariseinsequence,drivenbytheuse-casesitisdeployedfor,andtheongoingdiscovery
offundamentalmodelshortcomingsfromfeedbackloops[46]. Retainingfine-grainedcontroloverthesequence
of semantic and visual concepts allows us to create realistic, data-centric streams. This makes it possible to
better understand how different orderings of concepts and tasks affect the balance between accumulating new
knowledgeandretainingexistinginformation. Tothisend, westudysixvarieddata-streamorderings: (i)easy
to hard ordering, (ii) concept frequency ordering, (iii) concept similarity ordering, (iv) chronological ordering,
(v) dataset-incremental ordering and (vi) random ordering. Moreover, we provide insights into the impact of
data mixtures on the accumulation and retention trade-off as new concepts and subdomains are introduced.
2Practical Insights. Our study is intended to assist practitioners in understanding how various factors, such
as deployment scenarios, data limitations, continual learning and finetuning strategies, and constraints on
computing power or model capacity affect the ability to carry out long-term, controlled model updates. Using
FoMo-in-Flux, we provide a first set of key insights for real-world continual multimodal pretraining:
A Concise Practitioner’s Guide to Continual Multimodal Pretraining.
Method Choices. Under practical update scenarios and compute constraints, continual learning
methods and parameter-efficient fine-tuning techniques favor knowledge retention (stability) while
simple fine-tuning focuses on adaptation (plasticity). However, in combination with model merging,
fine-tuningsufficientlyaddressesthistrade-off,allowingforstrongknowledgeretentionandadaptation.
Meta Learning Rate Schedules. Learning rates matter, and can naturally be accounted for in
long-horizon continual pretraining via meta learning rate schedules across incoming tasks. These help
reduce the loss of pretraining knowledge while preserving high adaptation performance. Maintaining
the same learning rate schedule between pretraining and continual updating is much less important.
Model and Compute Scaling. Simple fine-tuning does not scale well with increased compute
resources or more frequent updates, unlike parameter-efficient fine-tuning, and particularly fine-tuning
with model merging. On the other hand, increasing model size helps it acquire new knowledge
while retaining its foundational properties, even within the same compute budget.
Data-centric Stream Orderings. The order in which data updates are applied significantly
impacts the model’s ability to learn new information and retain its zero-shot capabilities. This is
important to account for during deployment. However, when underlying data distributions are the
same, models converge to comparable final performance across update sequences.
Data mixture ratio. The ratio between pretraining-, update-, and buffer data affects the model’s
final performance, and “IID-fying” knowledge accumulation is crucial. Specifically, replaying previous
adaptation task data helps the model adapt better, while replaying pretraining data is less critical.
However, the choice of pretraining data pool can influence how well the model retains knowledge.
2 Categorizing Continual Pretraining: A Versioning Perspective
Traditional continual learning has been categorized into class-, domain-, and task-incremental settings [182].
However, continual pretraining benchmarks do not fit these categories, as they exhibit high-overlaps in
captions as opposed to disjoint classes [77, 15, 103], and time-varying gradual class and domain shifts
[49, 102, 21, 136, 104, 190]. Similarly, continual learning strategies are typically grouped [35, 135] into replay
[25,20],regularization[122,87,24],andparameter-isolationmethods[225,3,228],withmorerecentadditions
like prompt-tuning [194, 195, 169, 142], fixed-representation [117, 223, 139], and model-mixture methods
[115, 79] (see [224] for a survey). However, continual foundation model updates are dominated by replay
[137, 49], parameter-efficient finetuning [63] and retrieval-augmented methods [186, 136, 58], as traditional
methods do not help under computational constraints [64, 184, 136] and do not outperform simple baselines
[139, 117, 137, 216]. Hence, we provide a new categorization suitable for continual pretraining literature.
Ourcategorizationforcontinualpretrainingliteratureisinspiredbythesemanticsoftwareversioningframework
[144]. We believe that different scopes of updates require distinct strategies, indicating that no single solution
fitsallcontinualpretrainingscenarios(see[199]forasurvey,andTab.1foranoverviewofrelatedbenchmarks
under the semantic versioning umbrella). We believe foundation models require distinct update strategies,
similar to major, minor, and patch updates in software versioning:
Major Updates. Large-scale continual pretraining over extensive compute, data, and time resources that
substantially alter overall performance. Methods focusing on significant updates [49, 77, 51] consistently
employcontinualfine-tuningofthemodel,whichhasbeenfoundtobetheprimarystrategythroughextensive
3Benchmark #Samples #Tasks Ordering Domains Update Multi- Zero-Shot Compute- Data- RealWorld
Style modal Retention Bound Mixtures StreamVariants
CORe50[107] 165K 9 Class-/Data-Inc Objects Major
Split-ImageNet[196] 1.2M 10 Class-Inc WebImages Major × × × × ×
PTM-Adaptation[174] 30K-100K 5-20 Class-Inc WebImages Minor × × × × ×
CLAD[185] 23K 2000 Time-Inc Synthetic Patch × × × × ×
OAK[190] 326K ∼2000 Time-Inc Egocentric Patch × × × × ×
Inc-PASCAL[120] 11K ∼2-6 Class-Inc WebImages Major × × × × ×
Inc-ADE20K[22] 20K 2-6 Class-Inc SceneParsing Major × × × × ×
StreamingQA[104] 100K 6 Time-Inc Text Major × × × × ×
TemporalWiki[83] 32M 4 Time-Inc Text Major × ✓× × × ×
CKL[82] 30K 2 Task-Inc Text Minor × ✓ × × ×
CTrL[183] 300K 100 Task-Inc Objects Major × × × ×
CLEAR[102] 7.8M 10 Time-Inc WebImages Minor × × × × ×
ImageNet2K[137] 1.2M 20-200 Class-/Data-Inc WebImages Major × × ✓× × ×
Offline-CGLM[137] 500K 20-200 Time-Inc WebImages Major × × ✓ × ×
In1K-P365-LT[63] 62K 5 Class-/Data-Inc WebImages Minor × × ✓ × ×
NEVIS[15] 8M 79 Task-Inc Mixed Major × × ✓ × ×
CLOC[21] 39M 39M Time-Inc Geolocation Patch × × ✓ × ×
CGLM[136] 500K 500K Time-Inc Landmarks Patch × × ✓ × ×
CLiMB[172] 1.3M 4 Task-Inc Mixed Minor ✓× ✓× × ×
MTIL[221] 250K 5-20 Class-Inc Mixed Minor ✓ × × ×
Ctl-M2D2[205] 6.6B 160 Domain-Inc Text Minor ✓× × × ×
TiC-DataComp[49] 100M/1B/12B 6 Time-Inc WebImages Major ✓× ✓ ✓× × ×
× ×
FoMo-in-Flux(Ours) 2.5M 20+ Data-Centric Mixed Minor ✓ ✓ ✓ ✓ ✓
Table 1: FoMo-in-Flux comparison to existing benchmarks used in traditional continual learning and
continual pretraining studies: It features large timesteps and data-centric streams across various subdomains,
provides image-text pairs, a minor-update style, measures zero-shot retention, and is compute-constrained.
comparisons with other works [49, 193, 137, 27]. Currently explored topics include continual LR scheduling
[60, 77, 212, 128, 75] to minimize the stability gap [36].
Patch Updates.Frequentbutminor,targetedupdatesinwhichcontinualfine-tuningleadstopoorzero-shot
capabilityretentionwithlittlenewknowledgegained. Thesearebestmanagedbycontinualknowledgeediting
[28, 192] or sample-wise updates using a fixed backbone [136, 229, 58, 117, 52].
Minor Updates. Adaptations to whole subdomains and general concepts out of scope for knowledge edits,
but without the need for large-scale major updates. Some examples are: updating specific parts of a model
with LoRA [63, 12, 110, 197], model merging [79, 175, 188], instruction tuning [65, 219, 26], incorporating
expertknowledgeonparticularsubdomainsorspecializedvisualdistributionshifts[88,217,180,165,118,140,
157, 56, 226, 47, 138]). Real-world situations that might warrant a minor update include incorporating new
tasks, such as visual reasoning over fine-grained object categories [9, 187, 80, 131, 126, 170], or new domains
like sketches [30, 130], drawings [130, 100], or synthetic [19, 116] and medical imagery [78, 41]. Within our
multimodal setup, these minor updates can also jointly involve new or infrequently encountered concepts [19,
116], s.a. aforementioned fine-grained expert knowledge, medical applications or new compositions [81].
Overview. To understand the practical extent of continual minor version updates for foundation models,
our work is structured as follows: (1) We introduce FoMo-in-Flux, our benchmark for controlled continual
multimodal pretraining in Sec. 3, where we detail the datasets covered, the captioning process, and the
overall coverage. (2) Sec. 3.1.1 introduces our artificial obscure datasets, which focus on long-tail visual
and semantic concepts while simulating the increase of AI-generated content in future pretraining data. (3)
Sec. 3.2 and Sec. 3.3 then outline the overall training and evaluation pipeline within FoMo-in-Flux, our
memory-adjusted FLOPs metric, the corresponding compute budgets, and our streaming sequences emulating
different real-world minor update scenarios. (4) Sec. 4 provides experimental details. (5) Sec. 5 studies the
extent to which parameter-efficient finetuning, continual learning methods (Sec. 5.1), and model merging
(Sec. 5.2) can facilitate continual pretraining. (6) Sec. 6 then looks into the impact of (meta-)learning rate
schedules (Sec. 6.1), alongside other general training choices (Sec. 6.4, Sec. 6.3), followed by (7) Sec. 7, which
begins our data-centric investigation into continual minor model updates: Sec. 7.1 explores the different
streaming orderings, Sec. 7.2 looks into mixture ratios between adaptation, pretraining, and buffer data, and
Sec. 7.3 examines the influence of replaying on various pretraining pools.
4Table 2: Adaptation-only datasets over various visual and textual domains like diagrams, paintings,
natural, synthetic or generative images, remote sensing, art styles, traffic signs or textural data; with datasets
from Radford et al. [143] with lower zero-shot performance, common transfer or aggregation benchmark
datasets such as DomainNet [130] or VTAB [211] and specialized datasets like MVTec-AD [10].
Dataset #Train #Test #Classes Domain License Captions
Classification-based
AI2Diagrams[85] 2720 681 15 diagrams CCBY-SA generated
ArtBench10[100] 47531 11883 1870 paintings FairUse generated
Birdsnap[9] 31905 7977 500 finegrained,natural Unspecified,butacademicusage generated
Cifar100[94] 50000 10000 100 natural Unspecified,butacademicusage generated
CLEVR[84] 55931 13983 217 synthetic CCBY4.0 generated
CLRS[152] 13525 1475 25 remotesensing Academicpurposes[152] generated
Country211[143] 31650 21100 211 natural variousCC generated
CUB200-2011[187] 5994 5794 200 finegrained,natural customnon-commercial generated
DF20-mini[132] 32724 3637 179 finegrained,natural customnon-commercial generated
Dollarstreet[153] 13555 4103 1701 finegrained,natural CCBY-SA4.0 generated
Domainnet-Clipart[130] 33525 14604 345 illustrations customnon-commercial generated
Domainnet-Infograph[130] 36023 15582 345 diagrams customnon-commercial generated
Domainnet-Painting[130] 50416 21850 344 paintings customnon-commmerical generated
Domainnet-Sketch[130] 48212 20916 345 sketch customnon-commercial generated
Dsprites[116] 75000 25000 27 synthetic Apache2.0 procedural
DTD[31] 1880 1880 47 textural customnon-commercial generated
FGVCAircraft[111] 3334 3333 100 finegrained,natural customnon-commercial generated
Flowers102[126] 6149 1020 102 finegrained,natural Unspecified,butacademicusage generated
FRU92[70] 55814 9200 92 finegrained,natural Apache2.0 generated
iNaturalist2021[80] 125000 25000 2500 finegrained,natural customnon-commercial generated
Isicmelanoma[41] 2245 562 7 medical CC-BY-NC generated
Mitstates[81] 43002 10751 1959 finegrained,natural Unspecified,butacademicusage generated
Mtsd[44] 59978 8737 227 finegrained,trafficsigns CCBY-NC-SA4.0 generated
MVTec-AD(Base)[10] 2903 726 15 high-resolution,industrial CCBY-NC-SA4.0 generated
MVTec-AD(Faults)[10] 1380 345 88 high-resolution,industrial CCBY-NC-SA4.0 generated
ObjectNet[7] 40134 10000 313 natural CCBY4.0 generated
ObscureAnimals 17000 4238 74 generative MIT custom
ObscureThings 19128 4758 84 generative MIT custom
OpenImages[91] 115333 8593 589 natural Apache2.0 available
PatternNet[227] 26600 3800 38 remotesensing customnon-commercial generated
Places365[222] 120231 36499 365 natural customnon-commercial generated
Plantvillage[76] 43444 10681 38 finegrained,natural CC0 generated
Quilt-1M[78] 95862 23966 157 medical Academicpurposes available
Resisc45[69] 18900 6300 45 remotesensing Unspecified,butacademicusage generated
Shapes3D[19] 75000 25000 864 synthetic Apache2.0 procedural
SnakeCLEF2023[131] 151031 14117 1599 finegrained,natural customnon-commercial generated
SUN397[203] 15880 19850 397 natural customnon-commercial generated
SynthCLIP106[61] 84800 13886 106 generative CCBY-NC4.0 generated
Veg200[70] 61117 20000 200 finegrained,natural Apache2.0 generated
Zappos50k[206] 37829 9458 1847 finegrained,object customnon-commerical generated
Retrieval-based
FSCOCO[30](avgT2I/I2TR@5) 7105 1777 115 sketch CCBY-NC4.0 Available
Total 1759782 453020 18449
3 The FoMo-in-Flux Benchmark
WeintroduceFoMo-in-Flux(Foundation-Models-in-Flux),abenchmarkforcontrolledcontinualmultimodal
pretraining. We extend the study of continual pretraining beyond monolithic pretraining datasets, such as
TiC-RedCaps/TiC-DataComp[49], tospecializedsubdomainswithfine-grainedcontroloverdatastreamsand
adaptation over long task horizons. A more extensive comparison of FoMo-in-Flux to related benchmarks
can be found in Tab. 1, presenting key features of FoMo-in-Flux that distinguish it from existing works.
5Table3: FoMo-in-Flux Evaluation-only Datasets. Weutilizeasubsetofstandardevaluationdatasetsused
in Radford et al. [143], as well as an array of ImageNet-like variations (including the original ImageNet) to
probe different aspect of vision-language understanding and alignment. Moreover, datasets like Food101 [18]
or OxfordPets [127] were selected due to their high initial zero-shot performance scores.
Dataset #Train #Test #Classes Domain License Captions
Classification-based
Caltech101[95] 6026 2651 101 natural CCBY4.0 generated
Caltech256[55] 21307 9300 257 natural CCBY4.0 generated
Cars196[170] 8144 8041 196 finegrained,natural customnon-commercial generated
Cifar10[92] 50000 10000 10 natural,low-res Unspecified,butacademicusage generated
Domainnet-Quickdraw[130] 60375 25875 345 sketch customnon-commercial generated
EuroSAT[66] 18900 8100 10 RemoteSensing MIT generated
FashionMNIST[202] 60000 10000 10 b&w,low-res MIT generated
Food101[18] 75750 25250 101 finegrained,natural Unspecified,butacademicusage generated
GTSRB[72] 18635 8005 43 trafficsigns CC0 generated
ImageNet[39] 0 50000 1000 natural customnon-commercial generated
ImageNet-A[68] 0 7500 200 adversarial,natural MIT generated
ImageNet-D[215] 0 4835 103 generative MIT generated
ImageNet-R[67] 0 30000 200 renditions(e.g. sketch,paintings) MIT generated
ImageNet-S[189] 0 50889 1000 sketch MIT generated
ImageNet-V2[151] 0 10000 1000 natural MIT generated
MNIST[40] 60000 10000 10 b&w,low-res CCBY-SA3.0 generated
Monkeys10[2] 1097 272 10 natural CC0 generated
OxfordPets[127] 3680 3669 37 natural CCBY-SA4.0 generated
STL10[32] 5000 8000 10 natural,low-res customnon-commercial generated
SVHN[123] 73257 26032 10 natural,low-res customnon-commercial generated
Retrieval-based
MSCOCO[101](avgT2I/I2TR@5) 0 5000 0 natural CCBY4.0 available
Flickr30k[133](avgT2I/I2TR@5) 0 1000 0 natural CC0 available
Total 462171 314419 4653
3.1 Creation
Breakdown. FoMo-in-Flux consists of 63 classification and retrieval datasets—either publicly available or
introduced as part of this work—for a total of over 2.53M samples grouped into 23,045 concepts spanning
diverse visual domains such as natural images, sketches, abstractions, synthetic imagery or generative data.
Building concept-first allows experimentation with very precise and controlled ordering on the type of data
encounteredateachcontinualpretrainingstage. Moreover, byoperatingonmuchcleanerdatabuildingblocks
than web-crawled datasets like TiC-RedCaps or DataComp [49, 45], we ensure cleaner alignment between
concepts and images. The 63 datasets are divided into 41 datasets used for adaptation only, and 22 hold-out
datasets to probe retention of initial zero-shot generalization. See Tabs. 2 and 3 for a more detailed overview
of datasets and the exact split. For each dataset, we provide the number of trainable and evaluation samples
(though irrelevant for our evaluation-only split, these may prove useful for future dataset mix-and-matching
studies), the assigned domain, and the information on how its captions were produced.
Captioning. As classification datasets lack image-caption pairs necessary for vision-language model pre-
training, we provide captions for each image. More precisely, we introduce high-quality class-specific captions
through three different methods: (1) A scalable two-stage captioning mechanism, which uses BLIP-2 [97] to
generate general captions for each image and CapsFusion [208] (T5-XL) to merge and align captions with
available information on ground-truth class names (c.f. Fig. 2). (2) Procedural generation for a few specific
datasets (such as Shapes3D [19] and DSprites [116]) using available dataset-specific information, such as
image latents or descriptors (c.f. Fig. 3), creating captions that for example contain information about the
approximate location of the object, its orientation, size or shape. These captions are then adjusted at random
based on captions generated by GPT-4 [4], with some being complete, and some only including the basic
information. (3)Captionsalreadyprovidedalongsideclasslabelsaspartofthedataset(e.g.,OpenImages[91]
or our obscure datasets, see Sec. 3.1.1 and Fig. 4).
Coverage. Tabs. 2 and 3 highlight the diversity of domains and concepts covered in FoMo-in-Flux—ranging
6Pretraining Update step 1 Update step 2 Update step T
add to buffer add to buffer add to buffer
…
pretrain sample mixture sample mixture sample mixture
train train train
MAFs MAFs MAFs
time
…
Legend: Pretraining Update data pool Memory CLIP model
Dataset at each step buffer (cont. updated)
Figure 1: FoMo-In-Flux pipeline. (Pretraining) We start from a static pretrained CLIP model θ and its
0
pretraining data-pool . (Update steps) To continually update θ , at each update step t, we sample training
0
P
instances from the pretraining data-pool , current update pool , and memory buffer (containing all
t t
S P D B
past s), and train the model for a fixed compute budget, i.e. F MAFs. Each operation within an update
t
D
step (sampling, training, evaluation) counts towards the compute budget F. Refer Sec. 3 for more details.
from diagrams and paintings, natural high- and low-resolution images, to synthetic and generative images,
covering fine-grained and specialized domains, such as remote sensingand medical images. On the language
side, concept and classes covered also vary noticeably, with e.g. ArtBench10 built around art-style and artist
classification (as reflected in the captions), Quilt-1M introducing medical captions for histopathological image
data, or our synthetic Obscure datasets introducing rare and fantastical concepts with corresponding image
captions. Dataset licenses are provided in both tables, all of which permit academic re-use. We provide
references to original publications, most of which contain information how to download each dataset. To
facilitate reproduction, our codebase comes with automatic download mechanisms for datasets where possible,
and manual instructions otherwise.
3.1.1 Creating our Obscure Datasets
To improve diversity and increase the number of synthetic samples in our benchmark, we created the Obscure
Animals and Obscure Things datasets using text-to-image models. An additional motivation for creating
these datasets was to include classes that are systematically seen as obscure or not commonly occurring
in the wild. The goal was both to mimic tail-ends of image and concept distributions, as well as the issue
of more AI-generated content making its way into model training data, potentially misrepresenting some
concepts (see e.g., Fig 4). We first query ChatGPT to produce a set of 100 obscure animal names and 100
obscure object names. We then ask ChatGPT again to produce diverse prompts for each class name to be
used as text prompts to feed into a text-to-image generation model.
We manually reviewed the quality of the text prompts for veracity and faithfulness to real world contexts.
We then used the Kandinsky-2.1 [148], Stable Diffusion-2.1 [154], and Dreamlike-PhotoReal [1] text-to-image
models to generate images for each classname using the curated text prompts. Finally, for each class we
manually cleaned and filtered the images to ensure faithfulness. To create as clean a test set as possible, we
conservatively removed an entire class if more than 30% of its images were ambiguous, unclear or outright
unfaithful to the class—we used reference images from Google Images for this manual verification. Examples
7Class: Thayers Gull Class: Industrial Area Class: Gray Kingbird Class: Ruin Class: Violet green Swallow
Generated Caption: Generated Caption: Generated Caption: Generated Caption: Generated Caption:
“A photo of a Thayers Gull, a bird, “The photo depicts an “The Gray Kingbird, a “A photo of a temple built on a “In the clear blue sky, a photo
standing on a rock in a stream of water.” industrial area with a small bird, can be hillside in the desert captures captures the Violet green Swallow
cooling tower emitting seen in a photo the essence of a ruin.” perched on a wire.”
billowing smoke.” standing on top of a
tall stem.”
Figure 2: Visualisation of generated captions. We showcase some sample captions generated using our
two-stage pipeline for fine-grained classes (birds from Birdsnap [9]), and general, coarse classes (taken from
SUN397 [203]). The generated captions combine both image descriptions as well as important semantic class
information.
Caption: Caption: Caption: Caption: Caption:
“A black-and-white photo “A black-and-white photo “Captured from a “A pink sphere “From the 17.14
of a square located in of a heart located in the -30.00 degree angle: (larger size), degree direction,
the top left (exact top right: Located at x = a red, small cube displayed on blue the orange pill
position: x = 0.097 (0: 0.839 (with 0 as left, 1 against a blue wall flooring with a makes a striking
left, 1: right) and y = as right) and y = 0.032 on a blue flooring.” orange background, impression against
0.097 (0: top, 1: (0 top, 1 bottom).” viewed from a 17.14 a simple backdrop.”
bottom).” degree angle.”
Figure 3: Visualisation of programmatically generated captions for Shapes3D [19] (right) and
DSprites [116] (left, black and white). Chosen at random, some captions are complete with exact details,
while some only have more generic descriptors. Caption style leverages templates generated by GPT-4. The
default resolution of these images is 64 64, hence the low-resolution appearance.
×
are visualized in Fig. 4. We provide download links here for obscure animals and obscure things.
3.2 Pipeline, Compute Budgeting and Data Restrictions
We illustrate the general FoMo-in-Flux training and evaluation pipeline in Fig. 1. We start with a model θ
0
trained on a large pretraining dataset , and an empty buffer .
P B
ContinualPretrainingUpdates. Withintheallocatedupdatebudget,ateachupdatestepj 1,2,...,T ,
∈{ }
the following happens in order:
1. The stream reveals a task update pool of n image-text pairs = (ij,tj) nj spanning concepts.
j Dj { k k }k=1 Cj
2. We create the training data mixture by sampling from the pretraining data , buffer , and current
j
S P B
task data with respective ratios λ ,λ ,and λ , such that λ +λ +λ = 1. If samples in are
j P B D P B D
D B
insufficient (particularly at the start of task adaptation), we oversample from , with λ fixed.
j D
D
3. We apply a continual update method with a fixed compute budget F: θ =train( , ,θ ). This
j j j−1
M MD
compute budget F also determines the overall number of update steps conducted.
4. We add samples from the update pool to the unrestricted buffer . However, while all samples can be
j
D B
stored in buffer , they cannot all be sampled for training set , as the compute budget F imposes an
B S
implicit memory restriction [137].
8Thing: Khopesh Thing: Matryoshka Doll Thing: Tessellation Animal: Kakapo Animal: Ichthyosaur
Figure 4: Examples of our generated obscure things and animals along with captions, covering
100 rare and uncommonly occurring things and animals. For each class, images are generated using either
Kandinsky-2.1 [148], Stable Diffusion 2.1 [154] or Dreamlike-PhotoReal [1].
How to Measure Continual Pretraining Computational Cost? To keep our setting practical and
ensure a fair comparison, we impose a fixed computation cost budget for each time step to account for the
efficiency of each method. However, there is no universally adopted measure of computational cost. Recent
works use the number of iterations (forward/backward passes) [137, 49], number of parameters updated
[89, 13, 119], FLOPs [50], and time/throughput [119]. However, a single metric does not paint a complete
picture of efficiency that is releveant in practice [38, 119].
To account for this, we introduce Memory-Adjusted-FLOPs (MAFs), a novel metric that highlights two
aspects most relevant from a practitioner’s perspective: the total number of FLOPs per iteration and the
maximum utilization of device memory. To compute MAFs, we multiply the FLOPs count of each method
by a memory multiplier, the ratio of that method’s maximum memory utilization to the maximum memory
utilization of a full fine-tuning of the base model. The total amount of MAFs for each method and backbone
determines the allowed number of update steps each method can take during each adaptation task.
Data Restrictions. We allow unrestricted access to pretraining data (e.g., LAION-400M [161]), and an
unlimited replay buffer , as data storage is a negligible contributor to real-world cost [136, 137], and
B
buffer memory is only utilized during the continual pretraining process. To study different retraining
data pools, we use four popular image-text pretraining datasets of varying sizes, quality and curation
strategies—LAION-400M [161], CC-12M [23], CC-3M [162], and DataComp-Small [45].
3.3 Designing Data-Centric Task-Sequences
In addition to studying different pretraining sets and data mixture ratios (λ ,λ ,λ ), we also investigate
P B D
P
different realistic orderings by breaking down the FoMo-in-Flux datasets into individual concepts, which
are then ordered according to a chosen criterion (including the option to study reverse orderings). This is
visualized in Fig. 5. In order to do so, having a controlled set of image-caption pairs is critical, as it allows
for well-defined and meaningful arrangement of concepts into sequences according to an ordering π( ). Each
C
ordering π divides the set of samples into T disjoint subsets ,..., of concepts sampled without
1 T
(cid:84) D {D D } C
replacement, i.e. =ϕ, i,j. We define and motivate six different orderings below:
i j
C C ∀
1. Easy-To-Hard Ordering (performance) is motivated by curriculum learning [59, 155, 166, 171, 209],
assuming users deploying their model to easier concepts and usecases first, with incremental movement
towards to harder concepts.
Implementation. We approach the notion of “easy” vs. “hard” samples by ordering them according to base
model performance. For each concept, we select 50 random image-text pairs and then randomly sample
further 50 image-text pairs from the CC-3M dataset to represent random samples from CLIP’s pretraining
data pool [29]. For each of the 100 image-text pairs, we compute the sample-wise contrastive loss using a
CLIP ViT-L-14 model, and average it over concepts. The lower the mean loss per concept, the easier it is.
We then sort all the concepts by their mean loss in ascending order, and consider that to be the data stream
ordering.
9Pretraining concept frequency …
images
Sequence of 1 2 … … … … …
next n … …
semantically
most similar Sample id CLIP-Loss
concept
captions
concepts Dataset [2008] Dataset [2022] Dataset [2016]
Generated Stream Orderings
(1) Easy Samples … (2) Concept … … … …
< < <
First Ordering Frequency 1 2 5 3 9 4 3 5 1 8
(3) Concept … … … … (4) Time
[2008] [2016] [2022]
Similarity 1 2 1 2 1 2 1 2 1 2 incremental
(5) Dataset … … … …
(6) Random
incremental 3 5 1 2 5 3 9 4 1 8
Figure 5: Pictographic visualization of different data stream orderings included within the
FoMo-in-Flux benchmark setup.
2. Concept Frequency Ordering (concept-frequency) draws motivation from Udandarao et al. [181],
with user requests for model improvement starting from least frequent concepts first (as these constitute
edge cases that are most likely to cause undesired performance drops) and incrementally extending to more
frequent concepts, which are already represented well in the pretraining pool.
Implementation. WeusetheWhat’s In My Big Data [43]tool’selasticsearchindextosearchforthefrequency
of occurrence of each of the class names in the C4 [145] dataset. We compute the frequencies of each of the
classes, and order them such that the least frequent concepts (long-tail) occur first and the most frequent
ones (head-concepts) are at the end.
3. Concept Similarity Ordering (similarity), inspired by Yıldız et al. [205], is based on the hypothesis
that training on conceptually similar tasks allows users to minimize catastrophic forgetting over tasks.
Implementation. To find a trajectory with the highest semantic similarity between subsequent concepts, we
start with a similarity matrix containing the pairwise similarities between all the class names (via CLIP
ViT-L-14 text embeddings of templated text captions of the respective classes). Defining each class as a
node in a graph, with weights between the classes being their similarity, the problem reduces to finding the
minimum spanning path. We use a simple greedy algorithm: pick a starting class, find its closest neighbour
from the remaining set of classes, and keep repeating until we exhaust all classes. We repeat this procedure
for every class as a starting point and pick the path with the smallest total weight across all starting classes.
4. Time-incremental Ordering (time), inspired by [15, 74, 21, 136, 49], arranges in chronological order.
Implementation. As we only have reliable time information about datasets (via release dates of corresponding
publications or the official dataset upload date), concepts are ordered on a dataset-level [15]. These year-level
groupsarearrangedfromoldesttomostrecent,assumingthatolderdatasetsaremorelikelytobeconceptually
integrated within the pretraining data. Within each year, concepts are randomly ordered. Alongside the
above orderings, we compare with two baseline methods popular in continual learning, to better understand
the trade-offs made by these data-centric orderings:
5. Dataset-Incremental Ordering (dataset) is motivated by [149, 112, 113, 191, 207], but extended to a
larger sequence of datasets. To set up dataset, we simply randomly sample datasets from Tab. 2 to create a
dataset-incremental concept sequence. This sequence is then broken down into the desired number of tasks T.
6. Random Ordering (random), a baseline class-incremental ordering widely used across continual learning
setups [150, 201, 71, 137], mimics a scenario where user requests for model improvement are unstructured.
For this ordering, we simply shuffle class names at random.
103.4 Verifying Downstream Datasets: Finetuning must improve Performance
In order to estimate a reference upper bound on adaptation performance, verify the quality of generated
captions, and perform a sanity-check on our training pipeline, we fine-tune CLIP-ViT-B/32 and CLIP-ViT-
B/16 individually on each dataset in our training split, as well as all the evaluation-only datasets which come
with training samples. We fine-tune the models on each dataset for 10 epochs, with exact results and training
details shown in Supp. Tab. 5. For all datasets, we find that finetuning a pretrained CLIP model on our
generated captions consistently, and in parts very significantly, improves initial zero-shot performance. This
showcases the validity of our generated captions, and supports the inclusion of each listed dataset in the
FoMo-in-Flux benchmark.
4 Experimental Setup
We detail the default models, compute budgets, metrics, training schedules, and data mixtures used here.
Pretrained Models. We conducted our main experiments using a ViT-B-16 CLIP model pretrained on
the LAION-2B dataset [160]. We also conducted some additional ablation experiments with a ViT-B-32
CLIP model (to understand the effects of different patch resolution) and ViT-S/16, ViT-L/14, ViT-H/14 and
ViT-g/14 models. All our CLIP models are pretrained on LAION-2B, except for the ViT-S/16 model which
is pretrained on the DataComp-1B dataset [45].
Default Continual Pretraining Settings. Unless otherwise specified, we always train each continual
pretraining method for 20 update steps, T=20 (we test longer sequences with T= 50,200 in Supp. Fig. 19).
{ }
Each update step comprises of continually training a CLIP model for a fixed number of samples derived
by the computational budget outlined above. We fix the compute budgets per update step by taking the
DataComp-Small total FLOP budget, i.e., 1.8 109 GFLOPs and dividing it by the total number of update
×
steps. The exact number of update steps for each method is provided in Supp. Tab. 4. By default, we
use a random 2M subset of LAION-400M as our pretraining data pool and operate with uniform mixing
P
ratios λ =0.33,λ =0.34,λ =0.33 . For our reference upper bound performance, we train a CLIP model
P D B
{ }
initialized from the same open clip checkpoints jointly on all 41 adaptation datasets (with the samples
randomly shuffled). We do this training for a compute budget of T F MAFs, equivalent to the overall
×
compute budget available for the entire continual pretraining process.
Training Details. We train all continual pretraining methods with the CLIP contrastive loss [143, 54] and
learnable temperature τ, initialized to 0.01 (we provide ablations for the impact of τ initialization in Sec. 6.4).
We select the best-reported hyperparameters for each method from previous literature, only tuning the peak
learningrateforeachmethod. Weusecosine-decayLR-schedulingwithlinearwarmupof10%(westudymore
LR-schedules in Sec. 6.1), with an AdamW optimizer [108], a batch-size of 512 [108], and clip gradients with
norm higher than 1. We run all experiments using PyTorch [129]. To truly study updates in both vision and
language space, we update both encoders jointly (following Zhai et al. [213], we ablate this choice in Sec. 6.3).
Finally, the exact reflections of MAFs in method updates steps are provided in the supplementary, alongside
individual reference scores finetuning CLIP on each dataset individually.
Metrics. From a model updating perspective, there are two main quantities of interest: the degree of
adaptation to new data and the retention of pretraining knowledge. For all experiments, we therefore report
two main metrics: Knowledge Accumulation ( ), the average accuracy (or recall@5 for retrieval) over all
KA
A
concepts in the 41 adaptation datasets, and Zero-Shot Retention ( ), the zero-shot transfer accuracy (or
ZS
A
recall@5 for retrieval) on the held-out set of 22 datasets.
Plotting Style. In most plots showing our main experimental result, we depict the zero-shot baseline as
a black star and the joint training upper-bound as a golden star, with a dotted line connecting the two to
approximate the joint training trajectory on the - plane. Every other trajectory depicts the training
KA ZS
A A
progressionofindividualexperimentalruns. Notethatthesetrajectoriesalwaysbeginatthezero-shotbaseline
(black star).
115 Continual Pretraining: A Method Perspective
Main Findings
1. ModelMergingtechniquesexhibitaunique,promisingcontinualpretrainingdynamic(Fig.6)—
showing improved base generalization performance for shorter continual pretraining horizons
and better retention across full continual pretraining sequence, while also achieving substantial
gains in knowledge accumulation beyond that achieved by parameter-efficient tuning techniques
or full finetuning.
2. Parameter-efficienttuningtechniqueslikeLoRA,DoRAorVeRAfacesignificantplasticityissues,
meaning they sacrifice the capacity necessary to adapt effectively in a bid to improve knowledge
retention (Fig. 7 left, right). This behaviour is significantly exacerbated in parameter-selective
tuning techniques like LNFit and BitFit. Low-rank approximations on gradient updates, as
done in GaLore [220], appear to provide a simple middle ground in knowledge accumulation and
retention between full finetuning and parameter-efficient finetuning.
3. Continual learning regularization strategies under compute-restricted circumstances show
strong plasticity issues when the degree of regularization is high (EWC), but have minimal and
negative effect (SI) when it is low.
[TL;DR] Simple continual finetuning coupled with model merging appears to offer the most promise
for continual model pretraining across longer update cycles.
Webeginbyexploringhowdifferentcontinuallearningandfinetuningstrategiesaffectknowledgeaccumulation
and zero-shot retention at the model level, with the goal of understanding their trade-offs from a practical
perspective. We study several promising directions for continual pretraining of foundation models:
• Naive continual finetuning [49, 137, 77], which has emerged as a dominant approach for major updates
on realistic large-scale benchmarks, making it a contender for handling minor updates as well.
• Parameter-efficient tuning methods like LoRA [73], which have become a method of choice for minor
updates on a smaller scale or for adapting to new tasks with reduced memory requirements [63, 110,
197, 167, 168, 48, 99] through the use of low-rank weight approximations. In a related fashion, recent
work by Zhao et al. [220] has shown promise for model finetuning through low-rank approximations on
the optimization gradients (GaLore).
• Parameter-selective tuning methods such as BitFit [8] or LNFit [37], which only tune and update
particular parameter subsets in the pretrained model such as bias or normalization terms.
• Traditional regularization strategies from continual learning literature [87, 210], which have yielded
surprisingly strong performance in recent studies both in parameter [96, 218] and feature space [122],
despite being developed and tested in small-scale scenarios where the model is trained from scratch.
• Model merging, which has gained popularity [198, 79, 147] in non-continual learning scenarios as a
means to aggregate models tuned across different tasks, and has been studied in some recent [173, 115]
and concurrent works [90, 114] as a method to facilitate continual pretraining over longer adaptation
periods.
We excluded certain conceptual approaches from our investigation due to limited capacity and prior evidence
strongly suggesting they might not be effective. These include prompt-tuning-based continual learning
methods, which often collapse to a single prompt [176] or near-chance performance over a longer time horizon
12TheLandscapeofDifferentCPTMethods Shouldyoumergeyourweights?
60 full-ft 60 zs-merge,w=0.9
lnfit ft-merge,w=0.9
lora,r=4 ema-merge,w=0.95
55 ewc 55 ema-merge,w=0.85
si ema-merge,w=0.9
galore,r=16 Zero-Shot
Zero-Shot JointUpper-Bound
50 JointUpper-Bound 50
45 45
40 40
64 66 68 70 72 64 66 68 70 72
Zero-ShotRetention( AZS) Zero-ShotRetention( AZS)
Figure 6: Which methods should you opt for in continual pretraining over longer update cycles?
(Left) An in-depth study across five different method families: naive continual finetuning (Full-FT [77]),
parameter-selective tuning (LNFit [37]), parameter-additive tuning (LoRA), continual learning with regulariza-
tion (EWC [87], SI [210]), and low-rank approximations on the model update gradients (GaLore [220]). Naive
continual finetuning and parameter-selective tuning provide the extreme points in knowledge accumulation
and retention of original zero-shot performance. Switching from GaLore to parameter-efficient tuning with
e.g.,LoRAprovidesnearlinearinterpolationpointsbetweenboth. (Right) Judiciouslymergingmodelweights
exhibitsuniquelong-horizoncontinualpretrainingbehaviour,allowingforsignificantandconsistentknowledge
accumulation across update tasks with minimal forgetting (even allowing for improved initial generalization
performance across the first set of update tasks). While the exact trade-off between merging mechanisms
differs, we find that this unique overall trade-off persists—breaking with the hypothetical linear trade-off line
between original zero-shot performance and the joint finetuning upper-bound!
[139]. Similarly, we do not include distillation-based CL methods, as they do not show improvements when
memory is unrestricted [137]. For a detailed description of each of our tested methods, please see Appx. A.
5.1 Parameter-efficient Finetuning and Continual Learning
In this section, we leverage FoMo-in-Flux to understand the applicability of popular parameter-efficient
tuning methods to the continual pretraining setting. In particular, we investigate both parameter-additive
methods (LoRA [73], VeRA [89] and DoRA [105]) and parameter-selective approaches tuning only particular
weightsubsets(LNFit[37]andBitFit[8]). Finally, wealsostudyrecentlyproposedlow-rankapproximations
to model gradient updates (GaLore [220]). Additionally, we examine the extent to which methods developed
under smalls-scale continual learning scenarios such as Elastic Weight Consolidation (EWC, [87]) or Synaptic
Intelligence (SI, [210]) can be utilized to provide a favourable trade-off between accumulation and retention.
We refer to the supplementary for a detailed description of all methods.
Figure 7 showcases the comparison of all methods under our default 20-update step setting on the random
data ordering stream. To begin, we find two extreme points:
1. Strongest accumulation, weakest retention. Naive contrastive finetuning (in orange, Fig. 6
left) which achieves strongest knowledge accumulation across a full update cycle, at the cost of a
KA
A
significant drop in zero-shot retention even when leveraging learning rate rewarming as suggested
ZS
A
in [77]. Note that for our continual contrastive finetuning, we follow best practices sketched out in
[54], which recommends using the same objective for both continual and initial pretraining. Moreover,
we update both the image and language branch of the model, and initialize from the pretraining
temperature (see Sec. 6.4 for more details).
2. Weakest accumulation, strongest retention. On the other hand, parameter-selective update
methods such as LNFit (green, Fig. 7 center) and BitFit (blue, Fig. 7 center) exhibit good knowledge
13
)AKA(noitalumuccAegdelwonK )AKA(noitalumuccAegdelwonKDifferentRanksforPEFTMethods Tuningbiasesorlayer-norms? WhichPEFTmethodisbest?
60 lora,r=4 60 lnfit 60 lora,r=4
lora,r=64 bitfit dora,r=4
Zero-Shot Zero-Shot vera,r=4
55 JointUpper-Bound 55 JointUpper-Bound 55 Z Joe ir no t-S Uh po pt er-Bound
50 50 50
45 45 45
40 40 40
66 67 68 69 70 66 67 68 69 70 66 67 68 69 70
Zero-ShotRetention( AZS) Zero-ShotRetention( AZS) Zero-ShotRetention( AZS)
Figure7: More Detailed Method Ablations. (Left)Impactofdifferentranksoncontinualpretrainability;
favouring lower rank values (r =4) over large rank values (r =64) when contrasted against the hypothetical
linear tradeoff line between original zero-shot behaviour and performance when finetuned over all data at
once. (Center) Comparison between parameter-selective LNFit [37] and BitFit [8]. Both exhibit similar
behaviour: strongly limited ability to continuously incorporate new context, with correspondingly minimal
deviation in original zero-shot behaviour. (Right) Overview of adaptation versus evaluation trajectories for
different PEFT methods: LoRA [73], DoRA [105] and VeRA [89]. LoRA and DoRA behave comparably, with low
adaptable parameter counts in VeRA heavily limiting the ability to accumulate new knowledge.
retention, but minimal capacity for the accumulation of new knowledge across longer and complex data
streams.
Importantly, we find that naive continual finetuning strongly falls victim to “longer-horizon” stability gap
issues [36], where forgetting is high and achievable knowledge gain is strongly limited across the first number
of update steps (with each update step being a whole compute-budgeted training cycle over a data chunk,
c.f. Sec. 3.2).
Allothertestedmethodsoperatebetweenthesetwoendsofthespectrum,tradingoffknowledgeaccumulation
approaching that of simple finetuning, and knowledge retention to the degree of parameter-selective updates:
1. Strong accumulation, weak retention. By retaining the forward pass of the model and only
modifying the naturally lower-rank gradient updates during model training, GaLore (olive green, Fig. 6
left) offers a moderate balance between the ability to effectively incorporate new knowledge within a
given compute budget, and retaining original zero-shot generalization behaviour.
2. Decent accumulation, decent retention. Parameter-efficient tuning methods such as LoRA (blue,
Fig. 6 left) and DoRA (pink, Fig. 7 right) provide an effectively linear reduction in both knowledge
accumulation and forgetting (particularly with respect to full finetuning) compared to GaLore. This
conceptually also aligns with recent insights on LoRA effectively both learning and forgetting less even
in single domain finetuning tasks [11]. However, VeRA (dark blue, Fig. 7 right), which significantly
reduces the number of tunable parameters, behaves closely to parameter-selective tuning methods,
offering very little knowledge gain across long and complex data streams.
For parameter-efficient tuning, the scaling between the accumulation-forgetting trade-off and the tunable
parameter count is also unsurprisingly reflected when adjusting the rank of LoRA (Fig. 7 left)—though the
loss in original generalization performance outweighs the achievable knowledge accumulation when contrasted
against the hypothetical trade-off line between initial zero-shot behaviour and joint finetuning.
Finally, for continual learning regularization methods we find that while EWC (pink, Fig. 6 left) significantly
improveszero-shotretention,italsooffersextremelylimited comparedtotheinitialzero-shotperformance.
KA
A
On the other hand, the popular regularisation method SI (purple, Fig. 6 left) effectively offers no benefits
over standard finetuning, either in or . The poor performance of regularisation-based methods is
KA ZS
A A
curious as prior work has hinted at their benefits at scale [122, 86]. However, our fine-grained, and most
importantly compute-controlled FoMo-In-Flux helps verify these claims, as these regularization mechanisms
are both compute- and memory-expensive.
14
)AKA(noitalumuccAegdelwonK )AKA(noitalumuccAegdelwonK )AKA(noitalumuccAegdelwonKzs zs
train train
train
ema
ema
1-w ft 1-w … ft 1-w
ft ft
w merge w merge w merge
ema ema
zs zs
Pretrain Update step 1 Update step 2 Update step T
Legend , …, model at different steps t merged model Methods: [ema] EMA-Merge [zs] ZeroShot-Merge[ft] Finetune-Merge
Figure 8: Different model merging strategies explored in this work. We use θ′ to denote weights θ
finetuned after a respective task. Merging θ and θ′ then results in the merged outputs weights for task t,
t−1 t
θ . EMA-Merge, or exponential moving average merging, merges previously merged weights θ with current
t t−1
task weights θ′ produced by tuning the same previously merged θ on task t. ZeroShot-Merge always
t t−1
tunes the original pretraining weights θ on each task, then weight-interpolates between the finetuned θ′ and
0 t
the previously merged θ . Finetune-Merge always interpolates between the original pretraining weights θ
t−1 0
and the finetuned weights θ′. To arrive at θ′, the previously merged model θ is trained on task t.
t t t−1
5.2 On the Benefits of Model Merging Techniques
Recently, model merging has emerged as a promising avenue for adapting foundation models [198, 79, 173],
enabling efficient aggregation of multiple expert models [204, 158, 34, 5]. Initial work [173] also highlights its
potential benefits in small-scale, classification-based continual learning settings. To study their benefits at
scale, we investigate three forms of model merging. Denoting the model weights going into task t as θ ,
t−1
the finetuned weights after task t as θ′, and the final model-merged output after task t as θ , we define (c.f.
t t
Fig. 8 for details):
1. Exponential-moving averaging (EMA-merge), as adopted in Stojanovski et al. [173], which tunes the
previously merged task weights θ on task t to produce the finetuned weights θ′, and then merges
t−1 t
θ with θ′ to produce θ .
t−1 t t
2. Continual fine-tuning and merging (Finetune-merge) derived from multi-model patching in Ilharco
et al. [79]), which produces θ by merging the original pretraining weights θ and the finetuned weights
t 0
θ′. Toobtainθ′, Finetune-mergetunesthepreviouslymergedmodelweightsθ , sameasEMA-merge.
t t t−1
3. Continual zero-shot merge (ZeroShot-merge), a simple ablative merging protocol, which tunes the
original pretraining weights θ during each task t and produces θ by merging θ and the finetuned θ′.
0 t t−1 t
Each merge method uses an old-new weight mixing coefficient w, which we ablate over w= 0.85,0.9,0.95 .
{ }
As shown in Fig. 6 (right), we surprisingly find that the EMA-merge (blue) and ZS-merge (green), for the first
time, provide impressive boosts in zero-shot retention rates during the first update tasks, and retain
ZS
A
slight gains over the entire update cycle.
Moreover,thisiscoupledwithstrongknowledgeaccumulation ,thoughnotyetatthelevelofstandard
KA
A
finetuning. As expected, ablating the mixing weight w yields a trade-off between zero-shot retention and
knowledgeaccumulation—higherwsprovidebetterzero-shotretentioncapabilitieswhilecompromisingonthe
accumulation . However, across both ablated mixing ratios, as well as the merging mechanism, we find
KA
A
thatthehigh-levelcontinualpretrainingdynamicsremainthesame—atworstlimitedloss(andatbestnotable
gains) in zero-shot retention coupled with strong accumulation capacities, while also breaking favorably
with the hypothetical linear trade-off between the initial zero-shot performance and the joint finetuning
upper-bound. This strongly contrasts with the method families studied in the previous section, which trade
any acquired knowledge accumulation for a strong reduction in zero-shot generalization capabilities.
156 Continual Pretraining: General Training Recipes
Main Findings
1. Learning rates and schedules matter in continual pretraining over long update cycles.
While the specific choice of schedule for each update task has limited impact, correctly defining
meta-schedules modifying each task-specific schedule as a function of the deviation from the
initial pretraining weights can significantly break forgetting while allowing for nearly the same
degree of knowledge accumulation! Importantly, such meta schedules can be naturally derived
without the inclusion of additional hyperparameters.
2. Model size matters for continual pretraining. By increasing the model size, retention
of generalization performance becomes much less a trade-off with knowledge accumulation.
Increased capacity allows the model to acquire high degree of new knowledge without incurring
highratesofforgetting;andevenallowingforadditionalpositivebackwardtransfer. Consequently,
when expecting longer model update cycles, accounting for the higher “future-proofness” of
larger models even at higher initial training cost may be crucial.
3. Compute scaling matters (for some methods) for continual pretraining. For a fixed
model size, increasing the compute budget does not come with a more favorable accumulation-
versus-forgetting trade-off when simply finetuning. However, in conjunction with model merging,
additional increases in the allocated compute budget actually come with an improved accumula-
tion and forgetting trade-off!
4. Full model tuning beats locked image or text encoder training over long update cycles.
5. Initial stability gap issues are strongly mitigated by calibration – matching the pretraining
and subsequent continual pretraining softmax temperatures.
[TL;DR] Learning rate schedules should account for the update cycle duration. Larger models and
compute budgets (particularly alongside model merging) allow for knowledge accumulation with
reduced impact on initial knowledge retention, and an overall better accumulation-retention tradeoff.
This section studies the other degrees of freedom orthogonal to particular methodological update strategies
that co-occur with the design of a continual pretraining pipeline, particular across our studied longer minor
update cycles. In particular, this sections investigates the following pipeline properties:
1. TheimportanceofthelearningrateanditsschedulinginSec.6.1asnotedalreadyine.g.,[77]-covering
the need for matching inital and continual pretraining schedules and the option for meta-learning rate
schedules.
2. The impact of both model and compute scaling as independent axes to optimize and account for when
planning to deploy a model over longer minor update cycles. More precisely, Sec. 6.2 evaluates the
impact on the knowledge accumulation and the zero-shot retention trade-off as a function of both
increasedmodelsizeswithinthesamemodelfamily,aswellasincreasesintheallocatedcomputebudget
within a fixed model size.
3. The relevance of joint image and text encoder tuning in Sec. 6.3 when contrasted against locked image
or text encoder training.
4. The importance of aligning initial and continual pretraining softmax temperature in order to minimize
stability gap issues highlighted in Sec. 6.4.
166.1 Learning Rates, Schedules and Meta-Schedules
On the Influence of Learning Rate
Effectofchangingthelearningrate
Choices for Continual Pretraining.
60 full-ft(lr=1e-04)
To define the learning rate of choice for full-ft(lr=1e-05) 64.0
our continual pretraining problem, we 55 full-ft(lr=1e-06)
full-ft(lr=1e-07)
derive it directly from the original pre- 50 Zero-Shot 60.0
JointUpper-Bound(lr=1e-04)
training values in Cherti et al. [29] (1e-
JointUpper-Bound(lr=1e-05)
45
3). We note that the exact peak val- JointUpper-Bound(lr=1e-06) 56.0
JointUpper-Bound(lr=1e-07)
ues are corrected for our practical dif- 40
ferences in compute availability (operat-
52.0
35
ing on a batch-size of b = 512 in-
ours 48.0
stead of b = 88064); testing both 30
openclip
44.0
the commonly utilized linear resizing [53]:
25 32.0 36.0 40.0
λ spsc ea cle td iv=
e
sb qou ur as/ rb eo -p re onc oli tp· rλ eso ip ze in nc glip [9a 3n ]d (gth ive inre g- 40 45 Ze5 r0 o-ShotRe5 t5
ention(
A6 Z0
S)
65 70
5.81e 6and7.625e 5, respectively). In Figure 9: The effect of the base learning rate on continual
− −
preliminary experiments, we found that pretraining. The learning trajectory is shown for each value of
rounding up the linearly resized reference the learning rate, with the joint training performance as an upper
(toλ scaled =1e 5)workedslightlybetter bound. The contour lines show the geometric mean of knowledge
−
than both options, and provides a much accumulationandzero-shotretention(√ ). Alearning
KA ZS
cleaner entry point. As such, we chose rate of 1e 5 derived from the inital pA retra× iniA ng learning rate
to utilize 1e 5 as our learning rate ref- achieves th− e highest final knowledge accumulation and provides
−
erence value. As we find in Fig. 9, this the optimal balance between and .
KA ZS
(mostly) direct re-use of the maximum A A
learning rate has most importantly the highest degree of knowledge accumulation, but also achieves the
highest base joint tradeoff with respect to zero-shot retention. Larger learning rates incur significantly higher
rates of particularly early-task forgetting, while smaller learning rates limit the amount of knowledge gained.
As such, we set λ =1e 5 as our base learning rate.
scaled
−
Continual Pretraining Learning Rate Schedules. By default, LR schedules are applied on each task
individually [20, 163, 16, 173, 109]. As open clip models are trained using cosine schedules, we first study
the impact of re-applying the same cosine schedule for each task:
(cid:40) η + n (η η ) n<N
η = min Nwarm max − (cid:16)min (cid:16) warm (cid:17)(cid:17) (1)
n η min+ 21(η max −η min) 1+cos Ntn a− skN −w Na wrm armπ
with η [η ,η ] the learning rate at step n, and N the number of update steps for a given task. As
n min max task
∈
recommended in e.g. Ibrahim et al. [77], we utilize linear warmup to the initial pretraining peak learning rate
η used in Cherti et al. [29] for N iterations.
max warm
Tostudytheimpactofalearningratescheduleswitchtoe.g. infinitelearningratevariantsforpotentiallymore
flexibility down the line, we investigate a switch towards reciprocal square root schedule (rsqrt) introduced in
Zhai et al. [212]
 η + n (η η ) n N
 min N√warm max − min ≥ warm
η n = ηη max
·
√ n+N Nwa wr am
Nrm
task−(n+Nwarm)
en
ls∈
e[N warm,N task −N cool] (2)
Ntask−Ncool · Ncool
Note that rsqrt scheduling includes a separate cooldown section, wherein the last N steps are used to
cool
linear cooldown the previously decayed learning rate.
17
)AKA(noitalumuccAegdelwonKBasic Cosine Schedules Autoregressive Cosine Schedule Autoregressive Infinite Schedule
1.0 Cosine Cosine + Warmup References Rsqrt
Cosine + Warmup + Autoregressive + Peaks match Rsqrt
0.8 + Continued Dynamic + Autoregressive
+ Continued Dynamic
0.6
0.4
0.2
0.0
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
Exemplary Updates. One task = 1500 steps. Exemplary Updates. One task = 1500 steps. Exemplary Updates. One task = 1500 steps.
Figure 10: Visualization of different deployed learning rate schedules, from task-independent cosine
and infinite learning rate schedules (Rsqrt), to task-dependent meta learning rate schedule variations.
BothschedulesarevisualizedinFig.10(leftandright)overmultipletasks,andtheresultofeitherapplication
(matching and changing the pretraining learning rate scheduler) to our 20 task update cycle stream is
visualized in Fig. 11 (center). As can be seen, there is a negligible change in knowledge accumulation
and knowledge retention for either learning rate scheduler; highlight that across longer update cycles,
KA
A
matching the original pretraining scheduler is of lesser importance.
Meta Learning Rate Schedules. In the previous case, by default, each intermediate update is treated
independently (see the scheduler visualization in Fig. 10 (left)) - meaning each task rewarms and cools down
to the same learning rate and with the same decay and cooldown dynamics. However, as these continual
pretraining updates appear in succession, catastrophic forgetting of previously seen tasks has to also be
accounted for, going beyond just the loss in initial foundational knowledge. On top of that, with every task
update, the model is encouraged to move further away from its pretraining starting point.
To reduce the impact of task-level forgetting and the increased shift from pretraining, we introduce meta
LR scheduling - task-level schedules over each task-specific, iteration-level LR schedule to account for task
continuity. These derive naturally and hyperparameter-free from hypothetical scenarios wherein the previous
task schedule is simply extended across all the new tasks (see gray hypothetical schedules in Fig. 10 (center)).
In particular, we explore four meta-schedules: (i) autoregressive cosine scheduling, which selects η for each
max
task-schedule by building a hypothetical cosine schedule with warmup across the current and all seen tasks
and sets it to the intersection point with the warmup process of each respective task (c.f. Fig. 10 center):
ηT =ηcos(n′ =NT +(cid:80)T−1Nt ,N′ =(cid:80)T Nt ) (3)
max warm t task task t task
where ηcos(, ) defines the LRreturnedby the standardcosine LRschedule withwarmup atpoint n′ for N′
· · task
total iterations. Using the same formulation, we also test (ii) autoregressive continued dynamic schedule,
which warms up to the same ηT , but continues the schedule following the hypothetical cosine schedule over
max
all total previous steps N and the current post-warmup steps N . This autoregressive scheduling
previous warm
is naturally extended to the (iii) autoregressive rsqrt schedule, which simply sets η = ηrsqrt(n′,N′ ),
max task
and (iv) which similarly continues the dynamics of a hypothetically extended base schedule (“Continued
Dynamic”). Finally,wealsointroduce(v)“Peaks match Rsqrt”,whererespectiveη matchesthecontinued
max
dynamics while continuing with a standard rsqrt schedule.
The impact of task- and meta-level learning rate schedules for continual model updates are
visualized in Fig. 11 on the default 20-task variation of FoMo-in-Flux using simple continual finetuning as
our reference approach. Indeed, for longer continual pretraining sequences, switching from task-indepedent to
meta learning rate schedules notably changes the accumulation versus retention tradeoff behaviour. While
18
reilpitluM
etaR
gninraeLHowtorewarmyourCosineLR-Scheduler? Cosinevs.R-SqrtLR-Schedule HowtorewarmyourRsqrtLR-Scheduler?
60 cosine 60 rsqrt 60 rsqrt
cosine+warmup cosine CD-rsqrt
AR-cosine+warmup Zero-Shot AR-rsqrt CD-cosine+warmup JointUpper-Bound rsqrt-peaks-match
55 Zero-Shot 55 55 Zero-Shot
JointUpper-Bound JointUpper-Bound
50 50 50
45 45 45
40 40 40
64 65 66 67 68 69 70 71 64 65 66 67 68 69 70 71 64 65 66 67 68 69 70 71
Zero-ShotRetention( AZS) Zero-ShotRetention( AZS) Zero-ShotRetention( AZS)
Figure11: Meta-schedulingtask-specificLRschedulerhassignificantimpactontheknowledgeaccumulation
and retention trade-off, with meta-schedules derived from infinite LR schedules showing significant transitions
across the zeroshot vs finetuning threshold; moving close to accumulation performance of task-independent
scheduling, but retaining significantly more pretraining knowledge.
within different meta-schedules variations there is limited difference, as shown in Fig. 11 (left and right),
meta-learning rate schedules allow for significantly better retention of initial zero-shot transfer performance.
In the case of meta-schedules deriving from cosine learning rate schedules however, there is a severe reduction
in accumulated new knowledge due to the fast reduction in the learning rate (Fig. 10 left).
On the opposite end, meta-schedules deriving from infinite learning rate schedules such as the rsqrt schedule
lend themselves much better to longer-horizon continual pretraining tasks due to the much less aggressive
decay in learning rate within tasks: As shown in Fig. 10 (right), the autoregressive rsqrt meta-schedule
achievesnearlythesamegainin ,whilevastly increasing the amount of retained knowledge andexceeding
KA
A
the hypothetical linear zero-shot vs joint finetuning trade-off line.
6.2 Scaling up Model and Compute Budgets
To understand the impact of both model and compute scaling on the ability to continual pretrain over longer
update cycles, we adjust either the underlying vision transformer size (keeping the number of update steps
and task iterations fxied, and covering ViT-S/16 [62.3M], B/16 [149.6M], L/14 [427.62M], H/14 [986.11M]
and g/14 [1366.68M] taken from [29]) or the allocated compute budget for a fixed model size (selecting our
default ViT-B/16 and the default derived finetuning compute budget of 1.8 109 FLOPs as reference, see
×
also Sec. 4). Results for both studies are provided in Fig. 12 left and right, respectively.
Scaling Model Size. As can be seen, we find that with a controlled increase of model size, the ability
to continually pretrain over longer minor update cycles improves. While the absolute change in knowledge
accumulation remains rather consistent (within the interval of 8% and 10%), zero-shot retention
KA ZS
A A
improves - where both for the joint finetuning upper bound and continual pretraining, we see improved
knowledge retention, and in parts even slight positive backward transfer for ViT-L14 (roughly tripling the
parameter count with respect to ViT-B/16).
ForViT-B/16,weseea∆ 9.0%andnegativezero-shotretentionchange∆ 3.2%,whileforlarger
KA ZS
A ≈ A ≈
L/14,H/14and(overamagnitudebigger)g/14wefind(∆L/14 9.4,∆L/14 0.8),(∆H/14 10.1%,∆H/14
KA ≈ ZS ≈ KA ≈ ZS ≈
1.5%) and (∆g/14 9.8%,∆g/14 0.05%). Even with higher initial generalization performance, the
− KA ≈ ZS ≈ −
rate of knowledge accumulation remains roughly the same or even increases, while the ability to maintain its
initial generalization capabilities through the longer update cycles in parts notably improves.
These results suggest that model scaling can benefit long-term re-use and the opportunity to maintain
and consistently improve the base model over longer minor update cycles, suggesting model scaling helps
mitigate forgetting [146]. Our results partly contrasts works in the continual learning domain (though with
models trained from scratch) such as [57], which note that at least width alone does not encourage improved
knowledge retention. Given our exploratory insights, we believe that our experimental insights warrant
further and more controlled inspection into this phenomenon.
19
)AKA(noitalumuccAegdelwonK )AKA(noitalumuccAegdelwonK )AKA(noitalumuccAegdelwonKScalingModelSizes ScalingtheComputeBudget
62
S/16
60 B/16
L/14
H/14 60
55 g/14
Zero-Shot
58 full-ft
50 lora,r=4
lora,r=64
56 ema-merge,w=0.9
45
54
40
35 66 68 70 72 74 76 78 52 109 1010
Zero-ShotRetention( AZS) Computebudget(GFLOPs)
Figure 12: Model and Compute Scaling for Continual Pretraining. (Left) Impact on continual
pretrainability when increasing the underlying model size; going from ViT S/16 to ViT g/14. As the initial
zero-shot performance scales with increased model sizes, we find that the ability to acquire new knowledge
remains consistent. However, incorporating new context comes at a reduced loss of the original zero-shot
performance; in parts also encouraging positive backward transfer. (Right) For continual finetuning (with
and without model merging), as well as two different rank LoRA adapters, we measure the trade-off between
knowledge accumulation and retention as a function of a consistent increase in the allocated
KA ZS
A A
compute budget (within a fixed B/16 model size). For normal continual finetuning, a trade-off optimum is
reached early, and additional compute allocation disproportionately encourages model forgetting. However,
with model merging, we see a log-linear increase in performance when allowing for additional compute!
Scaling Compute Budgets. Instead of investing into compute increases through larger model sizes, one
can also adjust the directly allocated compute budgets; changing for example the number of update steps and
task iterations. For our reference model B/16 and its associated compute budget of 1.8 109 FLOPs, we thus
×
conduct 2 , 4 and 6 increases, as well as 0.5 and 0.25 reductions to understand how the continual
× × × × ×
pretraining abilities vary as a function of associated compute budgets and the applied continual pretraining
strategies of choice.
AsseeninFig.12(right)whichaggregatesknowledgeaccumulation andzero-shotretention through
KA ZS
A A
their geometric mean, simple continual finetuning (brown) can not consistently leverage increased compute
budgets; having to trade off increased knowledge accumulation with a disproportionate loss in the models
initial generalization capabilities. However, coupled with simple model merging, we find that models become
much better at effectively utilizing the additional budget increase; exhibit a log-linear budget-performance
relation. With much lower aggregate accumulation-retention performance, we also find a similar, slightly
weaker compute scaling behavior for adapter-based continual pretraining. While the ability to accumulate
knowledge, as also indicated in Fig. 7, is limited, adapter-based continual pretraining is much more consistent
in retaining initial zero-shot performance than simple finetuning.
20
)AKA(noitalumuccAegdelwonK
AKA×SZA√6.3 Model-specific tuning choices in compute-restricted scenarios
Finally, we highlight the relevance of freezing either image
Whichencodershouldyoutune?
ortextencoderinpracticallycompute-restrictedcontinual
60 full-ft
pretraining in Fig. 13. As freezing either the image or locked-image
locked-text Zero-Shot
languageencodercanallowforsignificantincreases(overa 55 JointUpper-Bound
magnitude)inthetuningstepbudget(astotalFLOPsand
memory use go down), we find that within the compute- 50
restricted continual multimodal pretraining scenario, tun-
45
ing both encoders still remains beneficial (aligning with
insightsprovidedinGoyaletal.[54]forsimplefinetuning).
40
While there is negligible difference when freezing each
encoder respectively (despite the substantial difference
64 66 68 70 72
in FLOPs reduction based on tuning the image-encoder Zero-ShotRetention( AZS)
Figure 13: To freeze or not to freeze. Tuning
alone vs. tuning the text-encoder alone), updating the
both encoders beats single encoder tuning in line
vision-language model as a joint system incurs a more
with finetuning insights from Goyal et al. [54].
favorable trade-off between knowledge accumulation and
zero-shot retention for each update.
6.4 Softmax Temperatures for Contrastive Losses—Not Too Hot!
Recall that CLIP’s contrastive loss uses a temperature pa-
ImpactofSoftmaxTemperature
rameter τ, and it is typically learnable during pretraining. 60
τ=0.01
At the beginning of training, it is initialized to 0.07 [143]. τ=0.1
τ=0.5
Further, to prevent training instabilities, the temperature 50 τ=0.75
τ=1.0
isclippedtoavoidbecomingsmallerthan0.01. Posttrain- Zero-Shot
40 JointUpper-Bound
ing, the learned temperature for all CLIP models consid-
ered in this study are found to be exactly 0.01. Moreover, 30
most works that fine-tune a pretrained CLIP model for
different downstream tasks, use exactly this learned tem- 20
perature [54, 178, 179, 198, 42, 79, 62]. Across our main
10
experiments, we follow this standard practice of initializ-
ing τ to 0.01 and setting it to be a learnable parameter 10 20 30 40 50 60 70
during continual pretraining. We now explore the impact
Zero-ShotRetention( AZS)
of different initializations for τ, and sweep over 5 different Figure 14: The softmax temperature for the
temperature values, 0.01,0.1,0.5,0.75,1.0 . From Fig. 14, contrastive lossiscrucialforcontinualpretrain-
{ }
we observe that τ plays a crucial role for continual pre- ing optimization. The learned temperature af-
training. As we increase the temperature from 0.01 to 0.1, ter CLIP pretraining is 0.01 (brown trajectory)—
zero-shot retention ZS gets impacted by 20% while also highertemperaturesthantheoptimal0.01hinder
A
noting modest drops on knowledge accumulation KA, as continual pretraining optimization and degrade
A
stability gap issues are excacerbated. Further increasing model weights.
τ, degrades both A and A even more greatly, with
ZS KA
the model degenerating to very poor performance. Such drastic changes in model behaviour were also
observed in prior work investigating CLIP fine-tuning for downstream tasks [178, 98, 33]—fine-tuning at
higher temperatures leads to a decrease in the modality gap between the image and text embedding spaces
on the CLIP embedding hypersphere, and hence very quickly degrades the quality of the embedding space for
performing downstream tasks [159, 164, 98]. We reproduce and extend the findings of these previous works
for the continual pretraining regime, and emphasise the importance of retaining low temperature values for
providing optimal and .
ZS KA
A A
21
)AKA(noitalumuccAegdelwonK
)AKA(noitalumuccAegdelwonK7 Continual Pretraining: A Data-Centric Perspective
Main Findings
1. Update cycle and deployment scenarios matter. Within the same overall dataset broken
down into continual pretraining updates, trajectors within the accumulation and retention
space can significantly differ. If an option, continual updates should be designed as “i.i.d” as
possible; ordering based on pretraining concept frequency, concept similarity or loss can result in
performance drops particularly during the initial set of updates. However, we find that so long
as update cycles operate over the same underlying data distribution that continual pretraining
endpoints end up highly similar within the accumulation and retention space.
2. Retaining a continual pretraining buffer is essential. Compared to training on currently
streamed data and a buffer populated with previously seen streaming data, replaying on
pretraining data has much less relative impact. However, the form of subsampling from the
pretraining data can notably impact knowledge retention. Together, it is clear that finding ways
to “i.i.d”-fy the continual pretraining process is crucial.
[TL;DR] “IID”-fying both the sequence of updates as well as the samples presented at each iteration
make the continual pretraining process most effective.
This section provides an important data-centric perspective on continual multimodal pretraining. We study
how fine-grained constraints on the sequence of tasks within an update cycle π (Sec. 7.1), specific data-pool
choicesandmixingratiosbetweenstreaming,bufferandpretrainingdata( / / andλ ,λ ,λ ,respectively,
D B P
D B P
in Sec. 7.2), and subsampling over the pretraining data for replay influence favorable trade-offs between
between knowledge accumulation and zero-shot retention (Sec. 7.3).
KA ZS
A A
7.1 Deployment scenarios impact continual pretrainability
Results on the impact of different deployment scenarios on continual pretrainability over a longer sequence
of minor updates are visualized in Fig. 15 for the following scenarios (Sec. 3.3): (1) performance sorted
- transition from easy to hard concepts, (2) concept-frequency sorted - rare pretraining concepts first,
(3) concept-similarity sorted - each update contains concepts semantically related to the preceding
update, and (4) random sorting. Dataset-incremental as well as time-incremental minor updates are studied
separately due to their different structure in Sec. 7.1, and reverse streams are investigated in Sec. 7.1.
Concept- and Sample-based Deployment Scenarios. Across the deployment scenarios in Fig. 15
(leftmost), while the concept-frequency stream (in green) has the marginally best – tradeoff with
KA ZS
A A
=55.2, =65.6, and performance (in pink) performs worst ( =53.8, =64.3), we find that
KA ZS KA ZS
A A A A
convergence end-points are surprisingly similar - especially w.r.t. the initial zero-shot and the joint finetuning
upper bound reference points. However, while endpoints are remarkably similar, different orderings π induce
significantly different trajectories in the accumulation-retention space, with similarity the most sample
inefficient ordering, while random produces the most favorable trajectories. This aligns with prior work from
curriculum learning and active learning that have suggested the efficacy of random curriculums [121, 200],
which we find extends itself well into the domain of longer-horizon continual pretraining over minor updates.
These insights mean that for longer update trajectories and a shared total space of subdomains and tasks
of interest, the type and order of continual minor model updates primarily impact initial model versions.
This is crucial to account for with respect to the model release horizon and the expected time frame before
conductinglarge-scalecontinualpretrainingupdates. However, italsomeansthatacrosslongupdatehorizons
irrespective of particular task orders, continually pretrained models arrive at similar performance breakpoints.
22Howtoorderyourdatastreamπ? BehaviourofTime-andDataset-IncrementalStreams Effectofreversedatastreams(invertingπ)
60 Similarity 60 Random 60 StreamType
Frequency Time Reverse
Random Dataset Forward
55 P Zee rr ofo -Srm hoa tnce 55 Z Joe ir no t-S Uh po pt er-Bound 55
JointUpper-Bound
Streams
50 50 50 Similarity
Frequency
Random
45 45 45 Performance
40 40 40
62 64 66 68 70 72 62 64 66 68 70 72 62 64 66 68 70 72
Zero-ShotRetention( AZS) Zero-ShotRetention( AZS) Zero-ShotRetention( AZS)
Figure 15: A Data-centric Perspective on Continual Pretraining. (Left) We study four concept-level
stream orderings π motived by potential update cycle scenarios (c.f. Sec. 3.3 for more details). Our results
indicate that deployment scenarios heavily impact the trajectory of the model through the accumulation and
retention landscape; however when update cycles operate over shared underlying data distributions, continual
pretraining endpoints end up highly similar. (Center) Dataset-level (random or time-incremental) update
cycles exhibit less stable deployment trajectories due to high dataset biases [177, 106], and should be avoided
for consistent continual model updates. (Right) Reversing our concept-level datastreams reveals significant
trajectory changes in similarity, in which dissimilar concepts at the sequence start incur increases task
difficulty which facilitates consistent knowledge accumulation and retention tradeoffs in early update stages.
More importantly, we find that even through streaming reversals, the end point similarity still persists.
Dataset- and Time-based Deployment Scenariosdifferfromthepreviousscenarios, inthateachupdate
stepgenerallycontainsmuchmoresemanticallygroupedsamples. Aswefindforbothcases(randomlyordering
datasets in dataset or time-ordering in time), such an update format induces significantly higher trajectory
variance, with much less trajectory coherence when compared to the other four streaming orderings studied
above. This is expected given prior work suggesting that visual datasets encode heavy biases [177, 106], and
hence tasks that explicitly separate these datasets cause much larger distribution shifts than tasks that (more
or less) smoothly mix data samples across the datasets on a concept-level. Still, the degree of accumulation
remains comparable, though we find that zero-shot retention is impacted disproportionately higher when
orderings π or designed on a dataset-level (down to 62.8%, compared e.g. random 64.4% and
frequency 65.5% in the best case). This is importanA t Z toS a≈ ccount for when designingA mZS inor≈ updates with
AZS ≈
the goal of retaining original zero-shot performance.
What Happens if We Reverse these Deployment Scenarios? Each sequence introduced in Sec. 3.3
introduces its own particular deployment scenario. Naturally, these scenarios may also either occur or be
designed to occur in reverse; updating the model for example with hardest examples first, or choosing highly
unrelated concepts before honing in on one specific ordering of similar concepts (by reversing similarity).
These scenarios do not have to be related to their precursors, and can present their own unique update
cycle. Evaluating Fig. 15 (right), random remains consistent. The prevalent difference we find in reversing
similarity; starting with a stream of unrelated concepts (more so than just random subsampling) and
then moving towards a stream of more related concepts. Effectively, early task composition becomes forcibly
harder. In doing so, the loss in retention along the trajectory comes with increased knowledge accumulation1.
This allows the trajectory to remain consistent and close to the hypothetical linear trade-off line between
the initial zero-shot behavior and the finetuning upper bound - more so even than random streams. Both
cases however point towards high variation in the presented concepts during each update step being very
beneficialforcontinualpretrainingoverlongerupdatecycles,especiallywhentryingtoretainconsistentmodel
behaviourforeachupdate. Still,evenwhenalsoaccountingforthereversedperformanceordering,end-points
converge to comparable end points! We find the only outlier to this to be the reverse frequency stream.
As head concepts are encountered early, knowledge accumulation is lower, while the controlled placement
of long-tailed, rare concepts towards the end of the update cycle, result in disproportionate forgetting of
frequent concepts crucial for achieving and retaining overall accumulation and retention performance.
1Bycomposinghardertasks,batchcompositionbecomesalsomoredifficult,whichhasbeenalignedwithimprovedvision-
languagerepresentationlearningine.g.,Zhaietal.[214]. Thoughbyreversingsimilarityinourcase,theaggregationofsimilar
conceptstowardstheendofthestreamresultsindiminishedknowledgeaccumulationtowardstheendofthesequence.
23
)AKA(noitalumuccAegdelwonK )AKA(noitalumuccAegdelwonK )AKA(noitalumuccAegdelwonK7.2 Data mixtures inform knowledge accumulation and zero-shot retention
Data control is also reflected in the use of different mixing ratios λ , which we study in Fig. 16. The
P/D/B
particular ratios investigated are motivated as follows (note that the baseline reference ratios we use for all
our experiments are λ =0.33,λ =0.34,λ =0.33 (in orange)):
P D B
{ }
No Buffer λ =0.5,λ =0.5,λ =0 (in pink) significantly degrades both accumulation and retention,
P D B
{ }
hampering the – tradeoffs ( 14% and 2.5% compared to the reference).
KA ZS KA ZS
A A − A − A
Pretrain-heavy λ =0.8,λ =0.1,λ =0.1 (in blue)
P D B
{ } DifferentData-MixingRatiosAcrossPools
also does not improve over the reference, since at each up-
60 λ=0.33,λ=0.34,λ=0.33
datestep,weinputfewerupdatesamplesfrom ,limiting { λP=0.05,λD=0.48,λB=0.47}
D { λP=0.8,λD=0.1,λ=B0.1 }
the accumulation capacity. 55 { {λ λP P= =0 0. ,λ5,λ =D D0= .0 1. ,λ5,λ =B B0= .0 9}}
Ibrahim et al. [77] λ =0.05,λ =0.48,λ =0.47 (in { ZerPo-ShoDt B }
green) defines the mix{ tuP re ratio uD sed in pasB t CPT} work 50 JointUpper-Bound
operatingonLLMs. Wereproducethefindingsof[77],find-
45
ing a 5% pretraining replay suffices to provide a better ac-
cumulationtradeoffcomparedtothereference(+2.2%
KA
A 40
and 0.3% ), suggesting that replaying pretraining
ZS
− A
data is less essential for optimal performance.
62 64 66 68 70 72
IIDify λ P=0,λ D=0.1,λ B=0.9 (in violet). Inspired
Zero-ShotRetention( AZS)
{ }
by the previous result of [77], the question arises on the Figure 16: Different Data Mixture Ra-
importance of the overall pretraining pool . Extending tios λ between pretraining , update
P D/P/B P D
findings in Prabhu et al. [137], we jointly also increase and buffer pool yield significantly different
B
the buffer mixing ratio to encourage more IID training adaptation-retention behaviour. “IID-fying” the
distributions at each update step from the full and continual pretraining process through frequent
D B
pools. Doing so provides the favored tradeoff compared to streaming buffer replay is most crucial.
all the previous mixtures, corroborating findings in [137].
7.3 Choice of pretraining data pool significantly impacts zero-shot retention
While the overall relevance of replay on pretraining
EffectofDifferentPretrainingData-Pools
data may be smaller than suitable buffer choices,
60 cc3m
we complete the previous study by investigating the cc12m
laion400m
i mm op da ec lt
.
o Wf eth ee xp pr ee rt imra ein ni tng wid thata thp reo eol oP theo rn pt rh ee tre an ind
-
55 d Za et ra oc -So hm op t-small
JointUpper-Bound
ing data pools of diverse volumes, caption-sources,
50
curation strategies, and quality measurements—
CC-3M [162], CC-12M [23], DataComp-Small [45]—
45
beyond our reference pool LAION-400M. For a fair
comparison, we randomly subsample each pretrain-
40
ing data pool to a total size of 2M samples, and use
this subset as our final pretraining pool P. Here
too, we use the reference mixture ratio setting of 62 64 66 68 70 72
λ =0.33,λ =0.34,λ =0.33 . From Fig. 17, it is
Zero-ShotRetention( AZS)
P D B
{ } Figure 17: Quality and Diversity of the Pretrain-
immediately evident that the choice of the pretrain-
ing Pool can matter significantly for retention of
ing data pool has a relevant impact on the KA– P
A initial zero-shot performance, but have limited impact
tradeoffs. While adaptation capabilities are
ZS
A on the ability to accumulate new knowledge.
barely impacted, using DataComp-Small (in pink)
yields significantly better zero-shot retention properties, (upto 2.4% ) gains). We speculate that this
ZS
A
could be attributed to the purely English-centric nature of the CC/LAION pools compared to the unfiltered
DataComp-Small which has a significantly higher multilingual and cultural diversity, which has been shown
to be beneficial for downstream performance previously [124, 125, 134].
24
)AKA(noitalumuccAegdelwonK
)AKA(noitalumuccAegdelwonK8 Conclusion
ThisworkintroducesFoMo-In-Flux-anovel, large-scale, fine-grainedcontrollableandlonghorizoncontinual
pretraining benchmark. It pools 63 standard classification and image-text retrieval datasets into a continual
pretraining setup suitable for vision-language training through image recaptioning and combination with web-
scalepretrainingdatasets. UsingFoMo-In-Flux,weconductanextensiveinvestigationintohowtocontinually
pretrain contrastive multimodal models, from a data-centric, method-centric, and training-recipe-centric
perspective. Key findings show that
• model merging strategies are most promising for a successful trade-off between the acquisition of new
knowledge and retaining pretraining knowledge,
• that learning rates matter; particular through the utilisation of learning rate schedules which ideally
account for the update cycle through meta scheduling,
• that increased model size makes it easier to incorporate new knowledge without overwriting pretraining
context,
• that simple compute scaling through e.g. more update steps does not benefit all methods equally - with
model merging again exhibiting the most favorable properties,
• that the order of updates impact the models trajectory in knowledge accumulation-retention space, but
only marginally impact the streaming endpoints,
• and that replaying on buffer data during streaming is generally more important than replaying on
(various subsets of) the original pretraining data.
By conducting our studies and comparisons across different model families within uniform and realistic
compute budgets, we believe that this paper allows us to provide several practical guidelines for real-world
deployment of multimodal continual pretraining systems, and that FoMo-In-Flux can provide a meaningful
testbed to better understand continual pretraining.
Limitations. Inthiswork, ouraimwastocreateameaningfulbenchmark, providepracticalguidelines, and
offer insights into various multimodal continual pretraining scenarios. We focused on continual, controlled,
minor model updates. We developed FoMo-in-Flux to include many publicly accessible datasets covering a
wide range of potential adaptation sub-domains. However, our findings on knowledge accumulation and
KA
A
zero-shot retention are tied to our chosen adaptation and evaluation datasets. Consequently, though
ZS
A
unlikely, various sub-domains relevant for future applications might not be sufficiently covered. Additionally,
our methods were based off of default hyperparameter ranges from original publications (LoRA, VeRA, DoRA,
BitFit, LNFit, FS-Merge, EMA-Merge) or continual learning repositories (mammoth [17]). While we tested
the validity of each method and the chosen hyperparameters to elicit meaningful finetuning responses on
respective single datasets (as highlighted e.g., for normal full-finetuning in Tab. 5), it overall means that our
conclusions rely on the optimality of these provided hyperparameter ranges.
Future Work. Our benchmark and findings provide a crucial starting point reference for further research
into continual multimodal pretraining. We sketch a few important and immediate future research directions:
• (Meta-) Learning Rate Schedules and Beyond: Our experiments show the importance of learning
rate schedules (and meta-variants) designed for longer horizon continual (minor) model updates. We
used a default cosine learning rate schedule and one infinite learning rate schedule (rsqrt), along with
five meta-schedule variants, but our results showcase that there is a lot of potential in further exploring
infinite schedules, as well as extensions into task- and order-conditioned learning rate schedules to allow
for continual model pretraining and model updates.
25• FurtherScalingUpComputeandModels: Westudiedcontinuallearningunderrealisticconstraints
(MAFs), with compute budgets derived from DataComp-small. Investigating other computational
budgets including over-training, and extending budgets to be potentially task-order dependent could
have practical relevance. Extending our insights to even larger model scales (ViT-bigG/14 and beyond)
can offer further practical guidance. We have investigated the effect of model and compute scaling
(see Fig. 12) independently and to a first degree, however we believe there is a lot more exciting future
work to be done.
• Text-to-Image Generative Models: Besides vision-language representation learning, FoMo-in-Flux
can be used to study continuous minor updates of text-to-image generative models (such as generative
diffusion models) on a fine-grained class and concept level, leveraging its diverse set of captions and
information about respective image concepts.
• Optimal Training Mixtures: Our results indicate that knowledge retention during minor updates
depends heavily on replaying data from previous tasks, guided towards “iid”-fying the learning task.
This process helps prevent knowledge forgetting related to pretraining. However, there is room to
better understand optimal training mixtures within limited compute budgets. Finding the best ways to
allocate FLOPs and memory for replay on large pretraining data is crucial.
Broader Impact. Better continual model pretraining and the ability to minimize the need for large-scale
model retraining can have significant impact on cost, compute and consequently environmental footprint. By
encouraging research into extending the re-usability of large-scale pretrained models before a major continual
model update or even full retraining from scratch is needed, we believe our work will lead to more economical
and ecological utilization of foundation models. We do not believe that there are any immediate negative
societal consequences as a result of this work, but we outline the limitations of our datasets in Appx. F.
Acknowledgements
The authors would like to thank Lukas Thede, Shaswat Goel and Shyamgopal Karthik for helpful feedback.
VU, KR and SD thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS).
VU, KR and SD also thank the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD
program for support. SA is supported by a Newton Trust Grant. MB acknowledges financial support via the
Open Philanthropy Foundation funded by the Good Ventures Foundation. MB is a member of the Machine
Learning Cluster of Excellence, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research
Foundation) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645. ZA
acknowledgesthesupportfromtheGermanResearchFoundation(DFG):SFB1233, RobustVision: Inference
Principles and Neural Mechanisms, project number: 276693517 and ERC Grant DEXIM, project number:
853489.
26References
[1] Dreamlike photoreal v2.0. https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0. 7, 9
[2] 10 monkey species. URL https://www.kaggle.com/datasets/slothkong/10-monkey-species. 6, 8
[3] Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, and Babak Ehteshami
Bejnordi. Conditional channel gated networks for task-aware continual learning. In CVPR, 2020. 3
[4] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774, 2023. 6
[5] Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary optimization of model merging
recipes. arXiv preprint arXiv:2403.13187, 2024. 15
[6] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. 2
[7] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum,
and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition
models. InH.Wallach,H.Larochelle,A.Beygelzimer,F.d'Alch´e-Buc,E.Fox,andR.Garnett,editors,Advances
inNeuralInformationProcessingSystems,volume32.CurranAssociates,Inc.,2019.URLhttps://proceedings.
neurips.cc/paper_files/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf. 5, 8
[8] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine-tuning for
transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio,
editors, ACL (Short), 2022. 12, 13, 14, 2
[9] Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L. Alexander, David W. Jacobs, and Peter N. Belhumeur.
Birdsnap: Large-scale fine-grained visual categorization of birds. In 2014 IEEE Conference on Computer Vision
and Pattern Recognition, pages 2019–2026, 2014. doi: 10.1109/CVPR.2014.259. 4, 5, 8
[10] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad – a comprehensive real-world
dataset for unsupervised anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), June 2019. 5, 8
[11] Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel
King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, and John P. Cunningham. Lora learns
less and forgets less, 2024. URL https://arxiv.org/abs/2405.09673. 14
[12] Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel
King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns less and forgets less. arXiv preprint
arXiv:2405.09673, 2024. 4
[13] Massimo Bini, Karsten Roth, Zeynep Akata, and Anna Khoreva. Ether: Efficient finetuning of large-scale
models with hyperplane reflections, 2024. URL https://arxiv.org/abs/2405.20271. 9
[14] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von Arx, Michael S.
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas
Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya
Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin
Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby
Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong,
Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti,
Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith
Kuditipudi, and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021. URL
http://dblp.uni-trier.de/db/journals/corr/corr2108.html. 2
[15] Jorg Bornschein, Alexandre Galashov, Ross Hemsley, Amal Rannen-Triki, Yutian Chen, Arslan Chaudhry,
Xu Owen He, Arthur Douillard, Massimo Caccia, Qixuan Feng, et al. Nevis’ 22: A stream of 100 tasks sampled
from 30 years of computer vision research. JMLR, 2023. 3, 4, 10, 1
27[16] Matteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, Angelo Porrello, and Simone Calderara. Class-incremental
continual learning into the extended der-verse. TPAMI, 2022. 17
[17] Matteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, Angelo Porrello, and Simone Calderara. Class-incremental
continuallearningintotheextendedder-verse. IEEETransactionsonPatternAnalysisandMachineIntelligence,
page1–16, 2022. ISSN1939-3539. doi: 10.1109/tpami.2022.3206549. URLhttp://dx.doi.org/10.1109/TPAMI.
2022.3206549. 25, 3
[18] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components with
random forests. In European Conference on Computer Vision, 2014. 6, 8
[19] Chris Burgess and Hyunjik Kim. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/, 2018. 4,
5, 6, 8
[20] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for
general continual learning: a strong, simple baseline. In NeurIPS, 2020. 3, 17
[21] Zhipeng Cai, Ozan Sener, and Vladlen Koltun. Online continual learning with natural distribution shifts: An
empirical study with visual data. In ICCV, 2021. 3, 4, 10
[22] Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bulo, Elisa Ricci, and Barbara Caputo. Modeling the
background for incremental learning in semantic segmentation. In CVPR, 2020. 4
[23] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale
image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. 9, 24
[24] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning
with a-gem. In ICLR, 2019. 3
[25] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania,
Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint
arXiv:1902.10486, 2019. 3
[26] Cheng Chen, Junchen Zhu, Xu Luo, Hengtao Shen, Lianli Gao, and Jingkuan Song. Coin: A benchmark of
continual instruction tuning for multimodel large language model. arXiv preprint arXiv:2403.08350, 2024. 4
[27] Jie Chen, Zhipeng Chen, Jiapeng Wang, Kun Zhou, Yutao Zhu, Jinhao Jiang, Yingqian Min, Wayne Xin Zhao,
Zhicheng Dou, Jiaxin Mao, et al. Towards effective and efficient continual pre-training of large language models.
arXiv preprint arXiv:2407.18743, 2024. 4
[28] Qizhou Chen, Taolin Zhang, Dongyang Li, Longtao Huang, Hui Xue, Chengyu Wang, and Xiaofeng He.
Lifelong knowledge editing for llms with retrieval-augmented continuous prompt learning. arXiv preprint
arXiv:2405.03279, 2024. 2, 4
[29] MehdiCherti,RomainBeaumont,RossWightman,MitchellWortsman,GabrielIlharco,CadeGordon,Christoph
Schuhmann,LudwigSchmidt,andJeniaJitsev.Reproduciblescalinglawsforcontrastivelanguage-imagelearning.
arXiv preprint arXiv:2212.07143, 2022. 2, 9, 17, 19
[30] Pinaki Nath Chowdhury, Aneeshan Sain, Ayan Kumar Bhunia, Tao Xiang, Yulia Gryaditskaya, and Yi-Zhe
Song. Fs-coco: Towards understanding of freehand sketches of common objects in context. In ECCV, 2022. 4, 5
[31] M.Cimpoi,S.Maji,I.Kokkinos,S.Mohamed,,andA.Vedaldi. Describingtexturesinthewild. InProceedings
of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. 5, 8
[32] Adam Coates, Andrew Ng, and Honglak Lee. An Analysis of Single Layer Networks in Unsupervised Feature
Learning. In AISTATS, 2011. https://cs.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf.
6, 8
[33] GuillaumeCouairon,MatthijsDouze,MatthieuCord,andHolgerSchwenk. Embeddingarithmeticofmultimodal
queries for image retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 4950–4958, 2022. 21
28[34] Nico Daheim, Thomas M¨ollenhoff, Edoardo Ponti, Iryna Gurevych, and Mohammad Emtiyaz Khan. Model
merging by uncertainty-based gradient matching. In ICLR, 2024. 15
[35] Matthias De Lange and Tinne Tuytelaars. Continual prototype evolution: Learning online from non-stationary
data streams. In ICCV, 2021. 3
[36] Matthias De Lange, Gido van de Ven, and Tinne Tuytelaars. Continual evaluation for lifelong learning:
Identifying the stability gap. arXiv preprint arXiv:2205.13452, 2022. 4, 14, 5
[37] Thomas De Min, Massimiliano Mancini, Karteek Alahari, Xavier Alameda-Pineda, and Elisa Ricci. On the
effectiveness of layernorm tuning for continual learning in vision transformers. In ICCV-W, 2023. 12, 13, 14, 2
[38] Mostafa Dehghani, Yi Tay, Anurag Arnab, Lucas Beyer, and Ashish Vaswani. The efficiency misnomer. In
ICLR, 2022. 2, 9
[39] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
imagedatabase. In2009 IEEE conference on computer vision and pattern recognition,pages248–255.Ieee,2009.
6
[40] LiDeng. Themnistdatabaseofhandwrittendigitimagesformachinelearningresearch. IEEE Signal Processing
Magazine, 29(6):141–142, 2012. 6, 8
[41] Nick DiSanto. Isic melanoma dataset, 2023. URL https://dx.doi.org/10.21227/9p2y-yq09. 4, 5, 8
[42] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Shuyang Gu, Weiming Zhang, Lu Yuan, Dong Chen,
Fang Wen, and Nenghai Yu. Clip itself is a strong fine-tuner: Achieving 85.7% and 88.0% top-1 accuracy with
vit-b and vit-l on imagenet. arXiv preprint arXiv:2212.06138, 2022. 21
[43] YanaiElazar,AkshitaBhagia,IanMagnusson,AbhilashaRavichander,DustinSchwenk,AlaneSuhr,PeteWalsh,
Dirk Groeneveld, Luca Soldaini, Sameer Singh, et al. What’s in my big data? arXiv preprint arXiv:2310.20707,
2023. 10
[44] Christian Ertler, Jerneja Mislej, Tobias Ollmann, Lorenzo Porzi, Gerhard Neuhold, and Yubin Kuang. The
mapillary traffic sign dataset for detection and classification on a global scale, 2020. 5, 8
[45] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan
Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of
multimodal datasets. NeurIPS, 2023. 2, 6, 9, 11, 24, 5
[46] Irena Gao, Gabriel Ilharco, Scott Lundberg, and Marco Tulio Ribeiro. Adaptive testing of computer vision
models. InProceedings of the IEEE/CVF International Conference on Computer Vision,pages4003–4014,2023.
2
[47] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.
Clip-adapter: Better vision-language models with feature adapters. IJCV, 2024. 2, 4
[48] Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang, Bernard Ghanem, and Jian Zhang. A unified
continual learning framework with general parameter-efficient tuning. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 11483–11493, 2023. 12
[49] SaurabhGarg,MehrdadFarajtabar,HadiPouransari,RavitejaVemulapalli,SachinMehta,OncelTuzel,Vaishaal
Shankar, and Fartash Faghri. Tic-clip: Continual training of clip models. In ICLR, 2024. 2, 3, 4, 5, 6, 9, 10, 12,
1
[50] Yasir Ghunaim, Adel Bibi, Kumail Alhamoud, Motasem Alfarra, Hasan Abed Al Kader Hammoud, Ameya
Prabhu, Philip HS Torr, and Bernard Ghanem. Real-time evaluation in online continual learning: A new
paradigm. In CVPR, 2023. 9
[51] Evangelia Gogoulou, Timoth´ee Lesort, Magnus Boman, and Joakim Nivre. A study of continual learning under
language shift. arXiv preprint arXiv:2311.01200, 2023. 3
29[52] Dipam Goswami, Yuyang Liu, Bart(cid:32)lomiej Twardowski, and Joost van de Weijer. Fecam: Exploiting the
heterogeneity of class distributions in exemplar-free continual learning. NeurIPS, 2023. 4
[53] Priya Goyal, Piotr Dolla´r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,
Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour, 2018. 17
[54] Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you pretrain:
Improved finetuning of zero-shot vision models, 2022. 11, 13, 21
[55] Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 Object Category Dataset. Mar 2007. 6, 8
[56] Yanan Gu, Xu Yang, Kun Wei, and Cheng Deng. Not just selection, but exploration: Online class-incremental
continual learning via dual view consistency. In CVPR, 2022. 2, 4
[57] Etash Kumar Guha and Vihan Lakshman. On the diminishing returns of width for continual learning. In
Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=
Ld255Mbx9F. 19
[58] ZhongruiGui,ShuyangSun,RunjiaLi,JianhaoYuan,ZhaochongAn,KarstenRoth,AmeyaPrabhu,andPhilip
Torr. knn-clip: Retrieval enables training-free segmentation on continually expanding large vocabularies. arXiv
preprint arXiv:2404.09447, 2024. 2, 3, 4
[59] Guy Hacohen and Daphna Weinshall. On the power of curriculum learning in training deep networks. In
Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on
Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2535–2544. PMLR, 09–15
Jun 2019. URL https://proceedings.mlr.press/v97/hacohen19a.html. 9
[60] AlexanderHa¨gele,ElieBakouch,AtliKosson,LoubnaBenAllal,LeandroVonWerra,andMartinJaggi. Scaling
laws and compute-optimal training beyond fixed training durations. arXiv preprint arXiv:2405.18392, 2024. 4
[61] Hasan Abed Al Kader Hammoud, Hani Itani, Fabio Pizzati, Philip Torr, Adel Bibi, and Bernard Ghanem.
Synthclip: Are we ready for a fully synthetic clip training?, 2024. 5, 8
[62] JinweiHan, ZhiwenLin, ZhongyisunSun, YingguoGao, KeYan, ShouhongDing, YuanGao, andGui-SongXia.
Anchor-based robust finetuning of vision-language models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 26919–26928, 2024. 21
[63] Md Yousuf Harun and Christopher Kanan. Overcoming the stability gap in continual learning. arXiv preprint
arXiv:2306.01904, 2023. 3, 4, 12
[64] Md Yousuf Harun, Jhair Gallardo, Tyler L Hayes, and Christopher Kanan. How efficient are today’s continual
learning algorithms? CVPR-W, 2023. 3
[65] Jinghan He, Haiyun Guo, Ming Tang, and Jinqiao Wang. Continual instruction tuning for large multimodal
models. arXiv preprint arXiv:2311.16206, 2023. 4
[66] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep
learning benchmark for land use and land cover classification, 2017. 6, 8
[67] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,
Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of
robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. 6
[68] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples.
CVPR, 2021. 6
[69] Haikel Hichri. NWPU-RESISC45 Dataset with 12 classes. 9 2021. doi: 10.6084/m9.figshare.16674166.v1. URL
https://figshare.com/articles/dataset/NWPU-RESISC45_Dataset_with_12_classes/16674166. 5, 8
[70] SaihuiHou,YushanFeng,andZileiWang.Vegfru: Adomain-specificdatasetforfine-grainedvisualcategorization.
In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017. 5, 8
30[71] SaihuiHou,XinyuPan,ChenChangeLoy,ZileiWang,andDahuaLin. Learningaunifiedclassifierincrementally
via rebalancing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages
831–839, 2019. 10
[72] Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of traffic
signs in real-world images: The German Traffic Sign Detection Benchmark. In International Joint Conference
on Neural Networks, number 1288, 2013. 6, 8
[73] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning
Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. 2, 12, 13, 14
[74] Hexiang Hu, Ozan Sener, Fei Sha, and Vladlen Koltun. Drinking from a firehose: Continual learning with
web-scale natural language. IEEE Transactions on Pattern Analysis and Machine Intelligence, to appear, 2022.
10
[75] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang
Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training
strategies. arXiv preprint arXiv:2404.06395, 2024. 4
[76] David P. Hughes and Marcel Salath´e . An open access repository of images on plant health to enable the
developmentofmobilediseasediagnosticsthroughmachinelearningandcrowdsourcing. CoRR,abs/1511.08060,
2015. URL http://arxiv.org/abs/1511.08060. 5, 8
[77] Adam Ibrahim, Benjamin Th´erien, Kshitij Gupta, Mats L Richter, Quentin Anthony, Timoth´ee Lesort, Eugene
Belilovsky, and Irina Rish. Simple and scalable strategies to continually pre-train large language models. arXiv
preprint arXiv:2403.08763, 2024. 2, 3, 4, 12, 13, 16, 17, 24
[78] Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva, Fatwir Sheikh
Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text
pairs for histopathology. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and
Benchmarks Track, 2023. URL https://openreview.net/forum?id=OL2JQoO0kq. 4, 5, 8
[79] GabrielIlharco,MitchellWortsman,SamirYitzhakGadre,ShuranSong,HannanehHajishirzi,SimonKornblith,
Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. In NeurIPS,
2022. 2, 3, 4, 12, 15, 21
[80] iNaturalist 2021 competition dataset. iNaturalist 2021 competition dataset. https://github.com/visipedia/
inat_comp/tree/master/2021, 2021. 4, 5, 8
[81] PhillipIsola,JosephJ.Lim,andEdwardH.Adelson. Discoveringstatesandtransformationsinimagecollections.
In CVPR, 2015. 4, 5, 8
[82] Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi,
and Minjoon Seo. Towards continual knowledge learning of language models. arXiv preprint arXiv:2110.03215,
2021. 4
[83] Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, and
Minjoon Seo. Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models.
arXiv preprint arXiv:2204.14211, 2022. 4
[84] JustinJohnson,BharathHariharan,LaurensvanderMaaten,LiFei-Fei,C.LawrenceZitnick,andRossGirshick.
Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 5, 8
[85] Aniruddha Kembhavi, Michael Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A
diagram is worth a dozen images. ArXiv, abs/1603.07396, 2016. URL https://api.semanticscholar.org/
CorpusID:2682274. 5, 8
31[86] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and
FahadShahbazKhan.Self-regulatingprompts: Foundationalmodeladaptationwithoutforgetting.InProceedings
of the IEEE/CVF International Conference on Computer Vision, pages 15190–15200, 2023. 14
[87] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic
forgetting in neural networks. PNAS, 2017. 2, 3, 12, 13
[88] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani,
Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness,
Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson,
SergeyLevine,ChelseaFinn,andPercyLiang. Wilds: Abenchmarkofin-the-wilddistributionshifts,2021. 2,4
[89] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix adaptation.
In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/
forum?id=NjNfLdxr3A. 2, 9, 13, 14
[90] Jedrzej Kozal, Jan Wasilewski, Bartosz Krawczyk, and Michal Wozniak. Continual learning with weight
interpolation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
4187–4195, 2024. 12
[91] Ivan Krasin, Tom Duerig, Neil Alldrin, Andreas Veit, Sami Abu-El-Haija, Serge Belongie, David Cai, Zheyun
Feng,VittorioFerrari,VictorGomes,AbhinavGupta,DhyaneshNarayanan,ChenSun,GalChechik,andKevin
Murphy. Openimages: A public dataset for large-scale multi-label and multi-class image classification. Dataset
available from https://github.com/openimages, 2016. 5, 6, 8
[92] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. 6, 8
[93] Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks, 2014. URL http://arxiv.
org/abs/1404.5997. cite arxiv:1404.5997. 17
[94] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (canadian institute for advanced research). URL
http://www.cs.toronto.edu/~kriz/cifar.html. 5, 8
[95] Fei-Fei Li, Marco Andreeto, Marc’Aurelio Ranzato, and Pietro Perona. Caltech 101, Apr 2022. 6, 8
[96] Haoran Li, Jingfeng Wu, and Vladimir Braverman. Fixed design analysis of regularization-based continual
learning. In Conference on Lifelong Learning Agents, pages 513–533. PMLR, 2023. 12
[97] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-imagepre-trainingwith
frozen image encoders and large language models, 2023. 6, 9, 12
[98] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. Mind the gap:
Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural
Information Processing Systems, 35:17612–17625, 2022. 21
[99] Yan-Shuo Liang and Wu-Jun Li. Inflora: Interference-free low-rank adaptation for continual learning. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23638–23647,
2024. 12
[100] Peiyuan Liao, Xiuyu Li, Xihui Liu, and Kurt Keutzer. The artbench dataset: Benchmarking generative models
with artworks. arXiv preprint arXiv:2206.11404, 2022. 4, 5, 8
[101] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro
Perona, Deva Ramanan, Piotr Doll’a r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context.
CoRR, abs/1405.0312, 2014. URL http://arxiv.org/abs/1405.0312. 6
[102] ZhiqiuLin,JiaShi,DeepakPathak,andDevaRamanan. Theclearbenchmark: Continuallearningonreal-world
imagery. In NeurIPS, 2021. 3, 4
[103] Zhiqiu Lin, Deepak Pathak, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Continual learning with evolving
class ontologies. Advances in Neural Information Processing Systems, 35:7671–7684, 2022. 3
32[104] Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, D’Autume
Cyprien De Masson, Tim Scholtes, Manzil Zaheer, Susannah Young, et al. Streamingqa: A benchmark for
adaptation to new knowledge over time in question answering models. In International Conference on Machine
Learning, pages 13604–13622. PMLR, 2022. 3, 4
[105] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng,
and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation, 2024. 13, 14, 2
[106] Zhuang Liu and Kaiming He. A decade’s battle on dataset bias: Are we there yet?, 2024. URL https:
//arxiv.org/abs/2403.08632. 23
[107] VincenzoLomonacoandDavideMaltoni.Core50: anewdatasetandbenchmarkforcontinuousobjectrecognition.
In Conference on robot learning, pages 17–26. PMLR, 2017. 4
[108] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,
2017. 11
[109] YilinLyu,LiyuanWang,XingxingZhang,ZichengSun,HangSu,JunZhu,andLipingJing. Overcomingrecency
bias of normalization statistics in continual learning: Balance and adaptation. In Thirty-seventh Conference on
Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Ph65E1bE6A. 17
[110] Simone Magistri, Joost van de Weijer, Andew D Bagdanov, et al. An empirical analysis of forgetting in
pre-trained models with incremental low-rank updates. arXiv preprint arXiv:2405.18069, 2024. 4, 12
[111] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft.
Technical report, 2013. 5, 8
[112] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning.
In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT,
USA, June 18-22, 2018, pages 7765–7773. Computer Vision Foundation / IEEE Computer Society, 2018.
doi: 10.1109/CVPR.2018.00810. URL http://openaccess.thecvf.com/content_cvpr_2018/html/Mallya_
PackNet_Adding_Multiple_CVPR_2018_paper.html. 10
[113] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by
learning to mask weights. In Proceedings of the European Conference on Computer Vision (ECCV), September
2018. 10
[114] Daniel Marczak, Bartl(cid:32)omiej Twardowski, Tomasz Trzcin´ski, and Sebastian Cygert. Magmax: Leveraging model
merging for seamless continual learning. arXiv preprint arXiv:2407.06322, 2024. 12
[115] Imad Eddine Marouf, Subhankar Roy, Enzo Tartaglione, and St´ephane Lathuili`ere. Weighted ensemble models
are strong continual learners. arXiv preprint arXiv:2312.08977, 2023. 3, 12
[116] LoicMatthey,IrinaHiggins,DemisHassabis,andAlexanderLerchner. dsprites: Disentanglementtestingsprites
dataset. https://github.com/deepmind/dsprites-dataset/, 2017. 4, 5, 6, 8
[117] Mark D McDonnell, Dong Gong, Amin Parveneh, Ehsan Abbasnejad, and Anton van den Hengel. Ranpac:
Random projections and pre-trained models for continual learning. In NeurIPS, 2023. 3, 4
[118] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. In The
Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?
id=jlAjNL8z5cs. 2, 4
[119] Otniel-Bogdan Mercea, Alexey Gritsenko, Cordelia Schmid, and Anurag Arnab. Time-, memory-and parameter-
efficient visual adaptation. arXiv preprint arXiv:2402.02887, 2024. 2, 9
[120] Umberto Michieli and Pietro Zanuttigh. Incremental learning techniques for semantic segmentation. In
Proceedings of the IEEE/CVF international conference on computer vision workshops, pages 0–0, 2019. 4
[121] Sudhanshu Mittal, Maxim Tatarchenko, O¨zgu¨n C¸i¸cek, and Thomas Brox. Parting with illusions about deep
active learning, 2019. 22
33[122] Jishnu Mukhoti, Yarin Gal, Philip HS Torr, and Puneet K Dokania. Fine-tuning can cripple your foundation
model; preserving features may be the solution. arXiv preprint arXiv:2308.13320, 2023. 3, 12, 14
[123] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. 2011. 6, 8
[124] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality not quantity:
On the interaction between dataset design and robustness of clip. Advances in Neural Information Processing
Systems, 35:21455–21469, 2022. 24
[125] Thao Nguyen, Matthew Wallingford, Sebastin Santy, Wei-Chiu Ma, Sewoong Oh, Ludwig Schmidt, Pang Wei
Koh, and Ranjay Krishna. Multilingual diversity improves vision-language representations. arXiv preprint
arXiv:2405.16915, 2024. 24
[126] Maria-ElenaNilsbackandAndrewZisserman. Automatedflowerclassificationoveralargenumberofclasses. In
2008 Sixth Indian Conference on Computer Vision, Graphics and Image Processing, pages 722–729, 2008. doi:
10.1109/ICVGIP.2008.47. 4, 5, 8
[127] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. The oxford-iiit pet dataset. URL
https://www.robots.ox.ac.uk/~vgg/data/pets/. 6, 8
[128] Jupinder Parmar, Sanjev Satheesh, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Reuse, don’t
retrain: A recipe for continued pretraining of language models. arXiv preprint arXiv:2407.07263, 2024. 4
[129] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems, 32, 2019. 11
[130] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for
multi-source domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer
Vision (ICCV), October 2019. 4, 5, 6, 8
[131] Luka´sPicek,MilanSulc,Jir´ıMatas,andJacobHeilmann-Clausen. Overviewoffungiclef2022: Fungirecognition
as an open set classification problem. In Guglielmo Faggioli, Nicola Ferro, Allan Hanbury, and Martin Potthast,
editors, Proceedings of the Working Notes of CLEF 2022 - Conference and Labs of the Evaluation Forum,
Bologna, Italy, September 5th - to - 8th, 2022, volume 3180 of CEUR Workshop Proceedings, pages 1970–1981.
CEUR-WS.org, 2022. URL https://ceur-ws.org/Vol-3180/paper-157.pdf. 4, 5, 8
[132] Luk´aˇs Picek, Milan Sˇulc, Jiˇr´ı Matas, Jacob Heilmann-Clausen, Thomas S. Jeppesen, Thomas Læssøe, and
Tobias Frøslev. Danish fungi 2020 - not just another image recognition dataset. 2021. 5, 8
[133] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana
Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.
In Proceedings of the IEEE International Conference on Computer Vision (ICCV), December 2015. 6
[134] Ang´eline Pouget, Lucas Beyer, Emanuele Bugliarello, Xiao Wang, Andreas Peter Steiner, Xiaohua Zhai, and
Ibrahim Alabdulmohsin. No filter: Cultural and socioeconomic diversityin contrastive vision-language models.
arXiv preprint arXiv:2405.13777, 2024. 24
[135] AmeyaPrabhu,PhilipHSTorr,andPuneetKDokania. Gdumb: Asimpleapproachthatquestionsourprogress
in continual learning. In ECCV, 2020. 3
[136] Ameya Prabhu, Zhipeng Cai, Puneet Dokania, Philip Torr, Vladlen Koltun, and Ozan Sener. Online continual
learning without the storage constraint. arXiv preprint arXiv:2305.09253, 2023. 2, 3, 4, 9, 10
[137] Ameya Prabhu, Hasan Abed Al Kader Hammoud, Puneet Dokania, Philip HS Torr, Ser-Nam Lim, Bernard
Ghanem, and Adel Bibi. Computationally budgeted continual learning: What does matter? In CVPR, 2023. 2,
3, 4, 8, 9, 10, 12, 13, 24
[138] Ameya Prabhu, Hasan Abed Al Kader Hammoud, Ser-Nam Lim, Bernard Ghanem, Philip HS Torr, and
Adel Bibi. From categories to classifier: Name-only continual learning by exploring the web. arXiv preprint
arXiv:2311.11293, 2023. 2, 4
34[139] Ameya Prabhu, Shiven Sinha, Ponnurangam Kumaraguru, Philip HS Torr, Ozan Sener, and Puneet K Dokania.
Randumb: A simple approach that questions the efficacy of continual representation learning. arXiv preprint
arXiv:2402.08823, 2024. 3, 13
[140] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating customized
prompts for zero-shot image classification. In Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV), pages 15691–15701, October 2023. 2, 4
[141] Tom Preston-Werner. Semantic versioning 2.0.0, 2013. URL https://semver.org/. 2
[142] Gao Qiankun, Zhao Chen, Sun Yifan, Xi Teng, Zhang Gang, Ghanem Bernard, and Zhang Jian. A unified
continual learning framework with general parameter-efficient tuning. ICCV, 2023. 3
[143] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language
supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 2, 5, 6, 11, 21, 8
[144] Colin Raffel. A call to build models like we build opensource software, 2021. https://colinraffel.com/
blog/a-call-to-build-models-like-we-build-open-source-software.html. 2, 3
[145] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei
Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of
machine learning research, 21(140):1–67, 2020. 10
[146] Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. Effect of scale on catastrophic forgetting in
neural networks. In International Conference on Learning Representations, 2021. 19
[147] Alexandre Ram´e, Nino Vieillard, L´eonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and
JohanFerret. Warm: Onthebenefitsofweightaveragedrewardmodels. arXiv preprint arXiv:2401.12187,2024.
12
[148] Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov,
Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, and Denis Dimitrov. Kandinsky: an improved
text-to-image synthesis with image prior and latent diffusion. arXiv preprint arXiv:2310.03502, 2023. 7, 9
[149] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual
adapters. Advances in neural information processing systems, 30, 2017. 10
[150] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental
classifier and representation learning. In CVPR, 2017. 10
[151] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to
imagenet? In International Conference on Machine Learning, pages 5389–5400, 2019. 6
[152] Jonathan Roberts, Kai Han, and Samuel Albanie. Satin: A multi-task metadataset for classifying satellite
imagery using vision-language models, 2023. 5, 8
[153] William A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody
Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the
world. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track,
2022. URL https://openreview.net/forum?id=qnfYsave0U4. 5, 8
[154] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjo¨rnOmmer. High-resolutionimage
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pages 10684–10695, June 2022. 7, 9
[155] Karsten Roth, Timo Milbich, and Bjorn Ommer. Pads: Policy-adapted sampling for visual similarity learning.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
9
35[156] Karsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent, and Diane Bouchacourt. Disentanglement of
correlated factors via hausdorff factorized support. In The Eleventh International Conference on Learning
Representations, 2023. URL https://openreview.net/forum?id=OKcJhpQiGiX. 2
[157] KarstenRoth,JaeMyungKim,A.SophiaKoepke,OriolVinyals,CordeliaSchmid,andZeynepAkata. Waffling
around for performance: Visual classification with random words and broad concepts. In Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV), pages 15746–15757, October 2023. 2, 4
[158] Karsten Roth, Lukas Thede, A. Sophia Koepke, Oriol Vinyals, Olivier J Henaff, and Zeynep Akata. Fantastic
gains and where to find them: On the existence and prospect of general knowledge transfer between any
pretrained model. In The Twelfth International Conference on Learning Representations, 2024. URL https:
//openreview.net/forum?id=m50eKHCttz. 15
[159] Simon Schrodi, David T Hoffmann, Max Argus, Volker Fischer, and Thomas Brox. Two effects, one trigger: On
the modality gap, object bias, and information imbalance in contrastive vision-language representation learning.
arXiv preprint arXiv:2404.07983, 2024. 21
[160] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for
trainingnextgenerationimage-textmodels.AdvancesinNeuralInformationProcessingSystems,35:25278–25294,
2022. 11
[161] Cristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Jenia Jitsev,
andAranKomatsuzaki.LAION-400M:OpendatasetofCLIP-filtered400millionimage-textpairs.InProceedings
of Neurips Data-Centric AI Workshop, 2021. 9
[162] PiyushSharma,NanDing,SebastianGoodman,andRaduSoricut.Conceptualcaptions: Acleaned,hypernymed,
image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 2556–2565, 2018. 9, 24
[163] Guangyuan Shi, Jiaxin Chen, Wenlong Zhang, Li-Ming Zhan, and Xiao-Ming Wu. Overcoming catastrophic
forgetting in incremental few-shot learning by finding flat minima, 2021. 17
[164] Peiyang Shi, Michael C Welle, M˚arten Bjo¨rkman, and Danica Kragic. Towards understanding the modality gap
in clip. In ICLR 2023 Workshop on Multimodal Representation Learning: Perks and Pitfalls, 2023. 21
[165] YangShu,XingzhuoGuo,JialongWu,XimeiWang,JianminWang,andMingshengLong.CLIPood: Generalizing
CLIPtoout-of-distributions. InAndreasKrause, EmmaBrunskill,KyunghyunCho, BarbaraEngelhardt,Sivan
Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning,
volume 202 of Proceedings of Machine Learning Research, pages 31716–31731. PMLR, 23–29 Jul 2023. URL
https://proceedings.mlr.press/v202/shu23a.html. 2, 4
[166] Samarth Sinha, Animesh Garg, and Hugo Larochelle. Curriculum by smoothing. In H. Larochelle, M. Ranzato,
R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,
pages 21653–21664. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/
paper/2020/file/f6a673f09493afcd8b129a0bcf1cd5bc-Paper.pdf. 9
[167] James Seale Smith, Paola Cascante-Bonilla, Assaf Arbelle, Donghyun Kim, Rameswar Panda, David Cox, Diyi
Yang, Zsolt Kira, Rogerio Feris, and Leonid Karlinsky. Construct-vl: Data-free continual structured vl concepts
learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
14994–15004, 2023. 12
[168] JamesSealeSmith,Yen-ChangHsu,LingyuZhang,TingHua,ZsoltKira,YilinShen,andHongxiaJin.Continual
diffusion: Continualcustomizationoftext-to-imagediffusionwithc-lora. arXiv preprint arXiv:2304.06027,2023.
12
[169] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle,
RameswarPanda,RogerioFeris,andZsoltKira.Coda-prompt: Continualdecomposedattention-basedprompting
for rehearsal-free continual learning. In CVPR, 2023. 3
36[170] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured
feature embedding. In Computer Vision and Pattern Recognition (CVPR), 2016. 4, 6, 8
[171] Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. Curriculum learning: A survey. Int. J.
Comput. Vision, 130(6):1526–1565, jun 2022. ISSN 0920-5691. doi: 10.1007/s11263-022-01611-x. URL
https://doi.org/10.1007/s11263-022-01611-x. 9
[172] Tejas Srinivasan, Ting-Yun Chang, Leticia Pinto Alva, Georgios Chochlakis, Mohammad Rostami, and Jesse
Thomason. Climb: A continual learning benchmark for vision-and-language tasks. Advances in Neural
Information Processing Systems, 35:29440–29453, 2022. 4
[173] Zafir Stojanovski, Karsten Roth, and Zeynep Akata. Momentum-based weight interpolation of strong zero-shot
models for continual learning. arXiv preprint arXiv:2211.03186, 2022. 12, 15, 17, 3
[174] Hai-Long Sun, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Pilot: A pre-trained model-based continual
learning toolbox. arXiv preprint arXiv:2309.07117, 2023. 4
[175] YiSun,XinXu,JianLi,GuangleiXie,YifeiShi,andQiangFang. Continuallearningthroughnetworkssplitting
and merging with dreaming-meta-weighted model fusion. arXiv preprint arXiv:2312.07082, 2023. 4
[176] Lukas Thede, Karsten Roth, Olivier J. H´enaff, Matthias Bethge, and Zeynep Akata. Reflecting on the state of
rehearsal-free continual learning with pretrained models, 2024. URL https://arxiv.org/abs/2406.09384. 12
[177] Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521–1528, 2011.
doi: 10.1109/CVPR.2011.5995347. 23
[178] Vishaal Udandarao. Understanding and fixing the modality gap in vision-language models. Master’s thesis,
University of Cambridge, 2022. 21
[179] Vishaal Udandarao, Max F Burg, Samuel Albanie, and Matthias Bethge. Visual data-type understanding
does not emerge from scaling vision-language models. In The Twelfth International Conference on Learning
Representations, 2023. 21
[180] Vishaal Udandarao, Ankush Gupta, and Samuel Albanie. Sus-x: Training-free name-only transfer of vision-
language models. In ICCV, 2023. 2, 4
[181] VishaalUdandarao,AmeyaPrabhu,AdhirajGhosh,YashSharma,PhilipH.S.Torr,AdelBibi,SamuelAlbanie,
and Matthias Bethge. No ”zero-shot” without exponential data: Pretraining concept frequency determines
multimodal model performance, 2024. 2, 10
[182] Gido M van de Ven and Andreas S Tolias. Three scenarios for continual learning. In NeurIPS-W, 2018. 3
[183] Tom Veniat, Ludovic Denoyer, and Marc’Aurelio Ranzato. Efficient continual learning with modular networks
and task-driven priors. arXiv preprint arXiv:2012.12631, 2020. 4
[184] Eli Verwimp, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L Hayes, Eyke
Hu¨llermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H Lampert, et al. Continual learning:
Applications and the road forward. arXiv preprint arXiv:2311.11908, 2023. 3
[185] Eli Verwimp, Kuo Yang, Sarah Parisot, Lanqing Hong, Steven McDonagh, Eduardo P´erez-Pellitero, Matthias
DeLange,andTinneTuytelaars. Clad: Arealisticcontinuallearningbenchmarkforautonomousdriving. Neural
Networks, 161:659–669, 2023. 4
[186] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny
Zhou, Quoc Le, et al. Freshllms: Refreshing large language models with search engine augmentation. arXiv
preprint arXiv:2310.03214, 2023. 3
[187] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-ucsd birds-200-2011 (cub-200-2011).
Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. 4, 5, 8
[188] Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. Knowledge fusion of large
language models. arXiv preprint arXiv:2401.10491, 2024. 4
37[189] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by
penalizing local predictive power. In Advances in Neural Information Processing Systems, pages 10506–10518,
2019. 6
[190] Jianren Wang, Xin Wang, Yue Shang-Guan, and Abhinav Gupta. Wanderlust: Online continual object
detectionintherealworld. InProceedings of the IEEE/CVF international conference on computer vision,pages
10829–10838, 2021. 3, 4
[191] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual learning: Theory,
method and application, 2024. URL https://arxiv.org/abs/2302.00487. 10
[192] Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun
Chen. Wise: Rethinking the knowledge memory for lifelong model editing of large language models. arXiv
preprint arXiv:2405.14768, 2024. 2, 4
[193] Xiao Wang, Yuansen Zhang, Tianze Chen, Songyang Gao, Senjie Jin, Xianjun Yang, Zhiheng Xi, Rui Zheng,
YichengZou,TaoGui,etal. Trace: Acomprehensivebenchmarkforcontinuallearninginlargelanguagemodels.
arXiv preprint arXiv:2310.06762, 2023. 4
[194] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su,
Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning.
In European Conference on Computer Vision (ECCV), 2022. 3
[195] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot,
Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), 2022. 3
[196] Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to efficient ensemble and
lifelong learning. arXiv preprint arXiv:2002.06715, 2020. 4
[197] Martin Wistuba, Prabhu Teja Sivaprasad, Lukas Balles, and Giovanni Zappella. Continual learning with low
rank adaptation. arXiv preprint arXiv:2311.17601, 2023. 4, 12
[198] MitchellWortsman,GabrielIlharco,JongWookKim,MikeLi,SimonKornblith,RebeccaRoelofs,RaphaelGon-
tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of
zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pages 7959–7971, June 2022. 12, 15, 21, 3
[199] Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. Continual
learning for large language models: A survey. arXiv preprint arXiv:2402.01364, 2024. 3
[200] Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. When do curricula work? In International Conference on
Learning Representations, 2021. URL https://openreview.net/forum?id=tW4QEInpni. 22
[201] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale
incremental learning. In CVPR, 2019. 10
[202] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms, 2017. 6, 8
[203] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale
scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, pages 3485–3492, 2010. doi: 10.1109/CVPR.2010.5539970. 5, 8
[204] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. TIES-merging: Resolving
interference when merging models. In Thirty-seventh Conference on Neural Information Processing Systems,
2023. URL https://openreview.net/forum?id=xtaX3WyCj1. 15
[205] C¸ag˘atayYıldız,NishaanthKannaRavichandran,PrishruitPunia,MatthiasBethge,andBeyzaErmis. Investigat-
ing continual pretraining in large language models: Insights and implications. arXiv preprint arXiv:2402.17400,
2024. 4, 10
38[206] Aron Yu and Kristen Grauman. Fine-grained visual comparisons with local learning. In 2014 IEEE Conference
on Computer Vision and Pattern Recognition, pages 192–199, 2014. doi: 10.1109/CVPR.2014.32. 5, 8
[207] Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Ping Hu, Dong Wang, Huchuan Lu, and You He. Boosting continual
learningofvision-languagemodelsviamixture-of-expertsadapters. InProceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pages 23219–23230, June 2024. 10
[208] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu.
Capsfusion: Rethinking image-text data at scale. arXiv preprint arXiv:2310.20550, 2023. 6, 9, 12
[209] Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang Zhu. From easy to hard: Learning language-guided
curriculumforvisualquestionansweringonremotesensingdata. IEEE Transactions on Geoscience and Remote
Sensing, 60, 2022. 9
[210] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In ICML,
2017. 2, 12, 13, 3
[211] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip
Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael
Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of
representation learning with the visual task adaptation benchmark, 2020. 5
[212] XiaohuaZhai,AlexanderKolesnikov,NeilHoulsby,andLucasBeyer. Scalingvisiontransformers. InProceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12104–12113, June
2022. 4, 17
[213] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas
Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pages 18123–18133, 2022. 11
[214] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-
training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11975–11986,
2023. 23
[215] Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, and Chengzhi Mao. Imagenet-d: Benchmarking neural
network robustness on diffusion synthetic object. CVPR, 2024. 6
[216] Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, and Yunchao Wei. Slca++: Unleash the power of
sequentialfine-tuningforcontinuallearningwithpre-training,2024. URLhttps://arxiv.org/abs/2408.08295.
3
[217] Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.
Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930,
2021. 2, 4
[218] YuanhangZhang,ZhidiLin,YiyongSun,FengYin,andCarstenFritsche. Regularization-basedefficientcontinual
learning in deep state-space models. arXiv preprint arXiv:2403.10123, 2024. 12
[219] Zihan Zhang, Meng Fang, Ling Chen, and Mohammad-Reza Namazi-Rad. Citb: A benchmark for continual
instruction tuning. arXiv preprint arXiv:2310.14510, 2023. 4
[220] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore:
Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507, 2024. 12, 13
[221] ZangweiZheng,MingyuanMa,KaiWang,ZihengQin,XiangyuYue,andYangYou.Preventingzero-shottransfer
degradation in continual learning of vision-language models. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 19125–19136, 2023. 4
[222] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image
database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. 5, 8
39[223] Da-WeiZhou,Han-JiaYe,De-ChuanZhan,andZiweiLiu. Revisitingclass-incrementallearningwithpre-trained
models: Generalizability and adaptivity are all you need. arXiv preprint arXiv:2303.07338, 2023. 3
[224] Da-WeiZhou,Hai-LongSun,JingyiNing,Han-JiaYe,andDe-ChuanZhan. Continuallearningwithpre-trained
models: A survey. arXiv preprint arXiv:2401.16386, 2024. 3
[225] Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, and De-Chuan Zhan. Expandable subspace ensemble for pre-trained
model-based class-incremental learning. arXiv preprint arXiv:2403.12030, 2024. 3
[226] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiweiLiu. Learningtopromptforvision-languagemodels.
International Journal of Computer Vision, 130(9):2337–2348, 2022. 2, 4
[227] Weixun Zhou, Shawn Newsam, Congmin Li, and Zhenfeng Shao. Patternnet: A benchmark dataset for
performance evaluation of remote sensing image retrieval. ISPRS Journal of Photogrammetry and Remote
Sensing, 145:197–209, November 2018. ISSN 0924-2716. doi: 10.1016/j.isprsjprs.2018.01.004. URL http:
//dx.doi.org/10.1016/j.isprsjprs.2018.01.004. 5, 8
[228] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James
Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing
Systems, 35:7103–7114, 2022. 3
[229] Zhen Zhu, Weijie Lyu, Yao Xiao, and Derek Hoiem. Continual learning in open-vocabulary classification with
complementary memory systems. arXiv preprint arXiv:2307.01430, 2023. 4
40Part I
Appendix
Table of Contents
A Method and Schedule Details 2
A.1 Adaptation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
A.2 Standard Continual Learning Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
A.3 Model Merging Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
B Differentiating Factors: FoMo-in-Flux with TiC-CLIP [49] and NEVIS [15] 4
C Additional Experimental Details and Results 5
D FoMo-in-Flux: Datasets 6
D.1 Finetuning verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
E FoMo-in-Flux: Caption Pipeline 9
F Data Statement 11
F.1 Executive Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
F.2 Curation Rationale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
F.3 Documentation for Source Datasets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
F.4 Language Varieties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
F.5 Speaker Demographic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
F.6 Annotator Demographic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
F.7 Speech Situation and Text Characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
F.8 Preprocessing and Data Formatting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
F.9 Capture Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
F.10 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
F.11 Broad Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
F.12 Metadata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
F.13 Disclosures and Ethical Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
F.14 Other . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
F.15 Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1A Method and Schedule Details
In the main paper, we study and reference different methods for their ability to encourage better continual
multimodal pretraining on FoMo-in-Flux. In this section, we provide details on the methods utilized,
alongside information not included in the main text with respect to the utilized learning rate schedules.
A.1 Adaptation Methods
LoRA [73] is the most commonly deployed form of parameter-efficient finetuning based on Low-rank
Adaptation, which avoids explicitly changing pretrained weights, but instead recommends weight updates to
be of the form
W′ =W +BA
0
with pretrained weights W , where B, A are two low-rank matrices, i.e., where W Rd×f, A Rr×f and
0
B Rd×r. By choosing r <<min(d,f), memory requirements during finetuning can b∈ e significan∈ tly reduced.
∈
Moreover, any learned adapter weights can be absorbed into the pretraining weights. Note however that
while memory is reduced, total FLOPs for backward and forward pass are commonly increased over simple
finetuning, as full backpropagation still needs to be conducted, as noted in Mercea et al. [119] and as
consequently seen in the final MAFs breakdown (see Tab. 4). By default, LoRA (as well as its subsequent
variations VeRA and DoRA, see below) introduces an additional weighting α over the weight update BA,
which we set to a constant α=1 [73, 89]; as it only acts as an implicit change in learning rate. As noted in
Hu et al. [73], the rank r is the essential hyperparameter to define for optimal changes in behaviour.
VeRA [89] introduces a simple variation over LoRA by randomly initializing and freezing A, B into fixed
low-rank projections, and instead learning simple learnable vectors Λ and Λ such that
B A
W′ =W +Λ BΛ A
0 B A
where Λ Rf and Λ Rr (utilizing the same dimensional notation as above). This reduces the total
B A
∈ ∈
number of tunable parameters significantly (though also mitigating possible adaptation capabilities), but
similar to LoRA, does not positively impact FLOPs counts for backward and forward passes together.
DoRA [105] minimally alters LoRA by disentangling norm and directions of the introduced adapter
matrices to encourage increased stability, and moving training dynamics of LoRA-style approaches closer to
those of simple finetuning. Effectively, this defines the DoRA adaptation step as
W +BA
W′ =m 0
· W +BA
0
∥ ∥
with magnitude vector m R1×f, where m is initialized as W , before being jointly updated during
∈ ∥ 0 ∥c
finetuning alongside the directional (through normalization) updates induced by B and A.
BitFit [8] introduces parameter-selective model finetuning by only updating bias-terms in the model (and
retaining remaining (kernel) weights as frozen). In doing so, changes to the model behaviour are supposed to
be kept minimal, will still introducing several degrees of freedom for finetuning. Note however that similar
to LoRA, while GPU peak memory is reduced, FLOPs are still high, as backpropagation through the full
network still has to occur.
LNFit[37] succeeds in the spirit of BitFit, by recommending to only tune scale and bias parameters in
model architectures that leverage LayerNorm [6] layers, showcasing particular success on small continual
learning benchmarks.
2A.2 Standard Continual Learning Methods
EWC [87] (Elastic Weight Consolidation) is a regularization scheme on weight updates initially introduced
to tackle rehearsal-free continual learning from scratch. The core motivation behind EWC is the assumption
that for each continual task, deviation from “task-optimal” weights learned in preceding tasks should be kept
meaningfully minimal. In particular, Kirkpatrick et al. [87] argue that deviation should be individual to each
model parameter. Assuming full model weights θ after task t, EWC tries to approximate the curvature in
parameter-loss space around θ via the Fisher Information Matrix t. To estimate t, several forward and
t
F F
backward passes have to be conducted, with the final regularization during training in task t+1 defined as
λ (cid:88)
total(θ)= (θ) t(θ θ )2
Lt+1 Lt+1 − 2 Fk k − t,k
k∈|θ|
with penalty weight λ, loss function for task t+1, , θ the weights from the previous task, and k the
t+1 t
L
parameter index. Note that for more than two tasks, is commonly estimated through a rolling average, as
F
done in implementation, borrowing from the mammoth codebase [17].
SI[210] (SynapticIntelligence)followsamotivationconceptuallyrelatedtothatofEWC,inthatparameters
defined as more influential (by some measure) are regularized more strongly to minimize change. However,
unlike EWC which computes one single point estimate using final parameter values after each task, SI
computes importance measures used for regularization along the entire training trajectory. By tracking past
and current parameter values, an online importance estimate is computed and incorporated as regularization
as follows:
(cid:32) (cid:33)
(θ)= (θ)+c (cid:88) (cid:88) ω kτ (cid:0) θt θ (cid:1)2 .
Lt+1 Lt+1 · (∆τ)2+ζ k− k
k∈|θ| τ<t k
with final task weights θt from the previous task. Here, ωtau is regarded as the per-parameter contribution to
k
changes in the total loss, approximated as the running sum of the product between gradient g (s)= δL and
k δθk
parameter update θ k′(s)= δ δθ sk (with within-task update step s). Finally, ∆τ
k
=θ k(sτ) −θ k(sτ−1) estimates
how much a particular parameter has moved. Alongside a simple regularization term ζ to avoid division by
zero, this defines the online importance term in SI.
A.3 Model Merging Methods
FT-Merge [198, 79] introduces a simply model merging recipe, in which different finetuned variants of
a same base pretrained model are linear interpolated (using interpolation coefficient α) into a final, more
general new base model. While this was initially not introduced for continual learning / pretraining tasks,
this form of interpolation can be naturally extended to our problem scenario. After each task, given an
interpolation coefficient alpha, we interpolate pre- and post-task weights (θ and θ , respectively). These
t−1 t
updated weights are then passed to the subsequent task t+1. Note that we incorporate the interpolation
process into the overall MAF compute budget as well.
EMA-Merge [173] extendsIlharcoetal.[79],butshowshowasimpleexponentialmovingaveragecanachieve
promising regularization beyond implicit learning rate changes for small, toy-ish continual learning image
classification benchmarks. Similar to FT-Merge, EMA-Merge introduces an interpolation coefficient α, and
each interpolation step is account for in the overall compute budget.
ZS-Merge operates in a fashion close to both merging methods - with the only differentiating factor being
that after each task, interpolation occurs not with respect to preceding model weights, but instead to the
initial zero-shot baseline.
3B Differentiating Factors: FoMo-in-Flux with TiC-CLIP [49] and
NEVIS [15]
Inthissection,weelaborateonthedetailspresentedinTable1ofthemainpaper. Wehighlightthedistinctive
features of our benchmark, FoMo-in-Flux, in comparison to two closely related benchmarks: NEVIS and
TiC-CLIP.
NEVIS. NEVIS [15], like our work, studies long-horizon continual learning with changing data distribu-
tions. However, NEVIS focuses on improving performance in a task-incremental setup, where task separation
is based on dataset creation timestamps, and concentrates on performance for the current, ongoing task. In
contrast, FoMo-in-Flux studies the ability for continual knowledge aggregation, while balancing the retention
of good downstream zero-shot performance; measuring open-ended performance in both cases and not limited
to a fixed set of classes. We also tackle multimodal vision-language tasks like image-text retrieval, which are
more complex to formulate than vision-only tasks. Moreover, FoMo-in-Flux allows as to study the impact
of different concept and class streams to emulate task orderings that can potentially be encountered when
realistically deployed.
TiC-CLIP. The TiC-Datacomp benchmark [49] evaluates the best methods for continual learning over
major updates, using pretraining budgets similar to those used for pretraining CLIP. In contrast, our
work focuses on minor updates, utilizing sample and compute scales that are 20 100 lower than the
×− ×
corresponding pretraining budgets. Furthermore, TiC-CLIP operates with only six timesteps and uses
large, monolithic time-incremental batches of image-text pairs. Our experiments, however, extend up to
200 timesteps and involve four carefully controlled fine-grained data-centric streams across a variety of
subdomains, including medical and remote sensing images. Our study provides insights into how models can
be pretrained continually over time, in scenarios working with far smaller sample and compute budgets and a
larger number of timesteps, ensuring efficiency and scalability across different subdomains. Moreover, we are
able to cover and study different data-centric deployment scenarios, alongside a wide array of methods and
their trajectority in the knowledge aggregation and retention space. Together, FoMo-in-Flux allows us to
provide the transitional benchmark towards the much more compute-intensive major updates as studied in
TiC-Datacomp.
4C Additional Experimental Details and Results
Code and datasets are provided for download using the following link (alongside generated captions). The
totalsizeis2.4Gfordownload. TheprovidedcodecoversallrelevantdetailsthatmakeupFoMo-in-Flux: All
dataset loaders, method implementations, streaming files and all generated captions for every single dataset
image (c.f. data lib/00 info). The full version of the code also comes with an automated downloader for
preprocessed versions of each utilized dataset. We also provide our full code, datasets, download pipeline,
and experimental results here: github.com/ExplainableML/fomo in flux.
Compute cluster and run details. For all our experiments, we used a compute cluster with 8 40GB
×
A100 nodes. For most of our ViT-B-16 runs, we used 2 GPUs from these nodes which was sufficient for all
our method implementations. To ensure memory efficiency, we optimised our implementations to use CPU
offloading for model weights where possible (for e.g., for the EWC, SI and Merge methods). For comparability
and reproducibility, all runs and methods share the same seed and equivalent overall experiment setting, with
changes in e.g., data stream ordering, modified compute budgets, method or data-mixtures only done when
explicited noted.
Justification for CLIP models used. To ensure that our experiments were most relevant to the
community, we further verified that the choice of our base CLIP models were validated by practitioner usage.
On Huggingface, the open clip models that were downloaded the most were CLIP ViT-B-32-laion2b (6.11M
times), CLIP-ViT-H-14-laion2b (4M times), and CLIP-ViT-B-16 (2M times). Hence, we investigate these
models - particularly as ViT-B/16 has been used in other studies on continual major model updates such
as Garg et al. [49].
Exact number of update steps, MAFs and samples seen. We provide the full breakdown of how we
compute MAFs per time step for each of the methods, and the total compute budget in terms of samples
seen per method (in Appendix Tab. 4). We use the datacomp-small [45] compute budgets as our reference.
Hence, this means that our total compute budget for the full continual pretraining is set to 5.7 108 GFlops
for the ViT-B-32 architecture and 1.8 109 GFlops for the ViT-B-16 architecture.2 ×
×
Variance across seeds. To ensure that our results are statistically valid and generalizable, we re-run our
canonical continual pretraining experiment with a ViT-B/16 backbone on the 20-task random data stream,
with three different seeds. Fig. 18 showcases that the three trajectories across the different seeds result in
very similar patterns and low variance across runs. This validates that all our main results are generalizable
across seeds.
Additional Experiment Results. Finally, we augment our suite of experiments conducted in the main
paper.
Fig. 19 provides additional higher-level experiment insights and verification, covering changes in backbone
architecture, compute budget and total update steps / task counts. More precisely, Fig. 19 (left) shows the
impactanincreaseordecreaseinoverallcomputebudgethas. Ascanbeseen, alltrajectoriesbehavesimilarly
on a qualitative level - experiencing forgetting and stability gap [36] issues at the beginning, before recovering
towards the linear zeroshot-finetuning trend line. Comparing end points, we do find that larger compute
budgets encourage slightly increased knowledge accumulation gains, but at the cost of disproportionately
larger losses in knowledge retention. This means that in practice, large compute budgets may be less favoured
even from a performance standpoint to incorporate minor model updates and bridge time between large,
major model updates. On top of that, Fig. 19 (right) highlights that under a fixed compute budget, in order
to bridge time to large model updates, keeping the number of minor model updates small, while maximizing
the size of each respective minor update, is preferable from both a knowledge accumulation and retention
perspective. Further, we note the strong robustness of model merging even under very long task streams,
further strengthening their applicability for long-step continual pretraining.
Fig. 19 (center) augments our results on the impact of different data-centric deployment scenarios
2Notethatthecomputebudgetsoutlinedintheoriginalpaper[45]wereinGMacs—weconvertthesenumberstoGFlopsby
multiplyingby2(seehereforreference.)
5Table4: Compute Budgets used in all ViT-B-16 experiments. Weprovidethe totalnumberof GFlops
taken per task for each of the methods in the Per-Task GFlops column. We also showcase the maximum
GPU memory requirements for each method in the Max. Memory Reqd. column—we convert this into a
memory multiplier for each method by dividing with respect to the reference full-ft max memory required.
Finally, for each method the Per-Task MAFs are computed as the product of the Per-Task GFlops and the
Memory Multiplier. Then, we show the total number of gradient update steps that are allowed for these
compute budgets per update step t, for the four total number of time step settings, T= 20,50,100,200 .
{ }
Finally, we also show the total number of gradient steps used (Total Num. steps) and the total number of
samplesseen(Total Num. samples seen)forthefullcontinualpretrainingprocess—ourjointupper-bound
oracle also uses this total compute budget.
Method Per-TaskGFlopsMaxMemoryReqd.MemoryMultiplierPer-TaskMAFsNum.stepsNum.stepsNum.stepsNum.stepsTotalNum.TotalNum.
(wrtfull-ft) (T=20) (T=50) (T=100) (T=200) steps samplesseen
full-ft 63394.7585 46.5917 1 63394.7585 1420 568 284 142 28,400 14,540,800
locked-text 57254.6183 37.5761 0.8064 46170.1241 1949 780 390 195 39,000 19,968,000
locked-image27176.6698 11.8847 0.2551 6932.7684 12982 5193 2596 1298 259,600 132,915,200
LNFit 43165.5968 30.5566 0.6558 28307.9983 3179 1272 636 318 63,600 32,563,200
BitFit 43165.5968 30.5546 0.6558 28307.9983 3179 1272 636 318 63,600 32,563,200
LoRA, r=4 54479.2515 40.5449 0.8702 47407.8446 1898 759 380 190 38,000 19,456,000
LoRA, r=64 54505.0151 40.6757 0.873 47582.8781 1891 757 378 189 37,800 19,353,600
DoRA, r=4 54479.8241 40.6582 0.8726 47539.0945 1893 757 379 189 37,800 19,353,600
DoRA, r=64 54514.1754 40.7871 0.8754 47721.7091 1886 754 377 189 37,800 19,353,600
VeRA, r=4 54479.3393 40.5449 0.8702 47407.921 1898 759 380 190 38,000 19,456,000
VeRA, r=64 54507.8336 40.5742 0.8708 47465.4214 1896 758 379 190 38,000 19,456,000
EWC 6276081.094 47.207 1.0132 6358925.364 14 6 3 1 200 102,400
SI 63394.7585 46.6523 1.0013 63477.1716 1418 567 284 142 28,400 14,540,800
ZS-Merge 63394.7585 46.5917 1 63394.7585 1420 568 284 142 28,400 14,540,800
FT-Merge 63394.7585 46.5917 1 63394.7585 1420 568 284 142 28,400 14,540,800
EMA-Merge 63394.7585 46.5917 1 63394.7585 1420 568 284 142 28,400 14,540,800
for continual minor model updates, under a different patch resolution for the vision-transformer. In this
experiment, we continually pretrain ViT-B-32 image-encoder models instead of the standard ViT-B-16
image-encoder. We note that the overall trends from this experiment closely match those of the original
ViT-B-16 experiments ( Fig. 15), suggesting the overall robustness of our main data-centric results to the
patch-resolution of the input images.
D FoMo-in-Flux: Datasets
D.1 Finetuning verification
In order to estimate a reference upper bound on adaptation performance, we verify the quality of generated
captions,andperformasanity-checkonourtrainingpipeline,wefine-tuneViT-B/32andViT-B/16individually
on the datasets in our training split, as well as the evaluation-only datasets which come with training samples.
We fine-tune the model on each dataset for 10 epochs with the same learning rate scheduling and the results
are shown in table Tab. 5. As can be seen, we find a consistent, and in parts significant improvements
conducting CLIP-style training across all individual benchmarks—highlighting the validity of our generated
captions, and support for each benchmark to be included in FoMo-in-Flux.
6RobustnessAcrossSeeds
60 seed=0
seed=1
seed=2
55 Zero-Shot
JointUpper-Bound
50
45
40
62 64 66 68 70 72
Zero-ShotRetention( AZS)
Figure 18: Our continual pretraining insights are robust across different random seeds—the variance in
trajectories across three different seeds is minimal.
Effectofchangingthecomputebudgetforfull-finetuning Sensitivitytopatch-sizeinimage-encoder:ViT-B-32 ChangingthetotalnumberofupdatestepsT
60 standard-budget 60 Similarity 60 full-ft(T=20)
double-compute Frequency full-ft(T=50)
half-compute Random full-ft(T=200)
55 Z Joe ir no t-S Uh po pt er-Bound 55 P Z Joee irr nofo t-Sr Um h poa ptn ec re -Bound 55 e e em m ma a a- - -m m me e er r rg g ge e e( ( (T T T= = =2 5 20 0 0) ) 0)
50 50 50 Z Joe ir no t-S Uh po pt er-Bound
45
45 45
40
40 40
35
62 64 66 68 70 72 56 58 60 62 64 66 68 70 72 62 64 66 68 70 72
Zero-ShotRetention( AZS) Zero-ShotRetention( AZS) Zero-ShotRetention( AZS)
Figure 19: We provide additional experiment insights and verifications, covering changes in backbone
architecture, compute budget and update steps. (Left) Changing the available compute budget noticeably
affects knowledge retention, however with limited gains in knowledge accumulation. (Center) Replacing our
default patch-size of 16 16 to 32 32 (i.e., ViT-B-16 to ViT-B-32) for ablating the effect of lower the patch-
× ×
resolution of our vision-transformer backbones, retains comparable behaviour across different deployment
scenarios, withsurprisinglysimilartrajectoryendpoints, andcomparableaccumulationperformance. (Right)
Changing the number of tasks the data stream (referred to as update steps T) is divided into, we find drops
in both knowledge retention and accumulation. Correspondingly, these results generally recommend to keep
the number of minor updates as small as possible, and the respective sizes as large as can be. Note that each
trajectory has been uniformly subsampled to visualize the same number of trajectory points for better visual
readability. Additionally, note that the robustness of the EMA-Merge method extends to longer task streams,
reinforcing its potential as a strong approach for continual pretraining.
7
)AKA(noitalumuccAegdelwonK
)AKA(noitalumuccAegdelwonK
)AKA(noitalumuccAegdelwonK )AKA(noitalumuccAegdelwonKTable 5: Per-dataset fine-tuning results for the ViT-B/32 and ViT-B/16 backbone. FT Performance is the
maximum accuracy over 10 epochs. Delta to ZS is the difference between FT Performance and the initial
zero-shot accuracy.
ViT-B-16 ViT-B-32
Dataset
FTPerformanceDeltatoZSFTPerformanceDeltatoZS
Ai2Diagrams[85] 88.00 10.67 83.67 12.33
ArtBench10[100] 22.86 11.64 21.20 9.08
Birdsnap[9] 63.70 13.30 57.60 10.00
Caltech101[95] 93.33 1.33 93.67 1.67
Caltech256[55] 93.97 1.39 92.61 2.61
Cars196[170] 93.88 5.07 90.56 2.25
Cifar100[94] 90.33 15.83 91.33 15.93
Cifar10[92] 99.67 4.67 99.00 4.70
CLEVR[84] 71.05 67.19 55.87 52.94
CLRS[152] 92.67 29.33 91.33 30.00
Country211[143] 20.38 3.74 20.38 6.11
CUB200[187] 80.50 10.38 74.00 10.27
DF20mini[132] 50.84 49.46 43.30 41.64
DollarStreet[153] 18.31 11.88 17.96 12.26
DomainNet-Clipart[130] 83.62 3.14 81.74 3.93
DomainNet-Infograph[130] 61.16 3.71 54.93 2.55
DomainNet-Painting[130] 74.64 3.61 71.72 1.47
DomainNet-Quickdraw[130] 66.81 48.45 66.52 48.24
DomainNet-Sketch[130] 78.26 3.94 76.96 4.89
Dsprites[116] 100.00 88.16 100.00 88.36
DTD[31] 68.00 16.00 66.33 11.33
EuroSAT[66] 99.67 43.62 99.33 47.85
FashionMNIST[202] 96.33 16.93 94.67 18.07
FGVCAircraft[111] 48.67 22.24 39.33 14.41
Flowers102[126] 95.67 21.33 94.67 21.33
Food101[18] 90.67 5.08 88.00 5.66
FRU92[70] 91.67 42.97 88.33 39.64
GTSRB[72] 99.33 49.46 100.00 56.12
iNaturalist2021[80] 50.40 44.76 43.10 37.80
Isicmelanoma[41] 59.33 51.00 56.00 40.33
MITStates[81] 28.30 4.75 26.35 3.02
MNIST[40] 100.00 34.70 99.67 30.57
Monkeys10[2] 97.79 15.07 96.69 13.97
MTSD[44] 90.97 72.41 90.75 70.93
MVTec-AD(Base)[10] 100.00 27.67 100.00 21.00
MVTec-AD(Faults)[10] 52.33 38.67 38.00 20.67
ObjectNet[7] 54.63 16.75 48.88 16.98
ObscureAnimals 89.67 27.49 89.33 33.78
ObscureThings 73.33 17.54 68.67 14.98
OpenImages[91] 58.64 0.00 59.40 0.38
OxfordPets[127] 95.00 4.29 90.67 0.23
PatternNet[227] 99.67 30.72 99.67 34.14
Places365[222] 48.49 6.62 49.86 7.22
Plantvillage[76] 100.00 80.02 99.67 76.55
Quilt-1M[78] 66.45 65.45 67.10 66.80
Resisc45[69] 94.33 25.60 93.33 30.16
Shapes3d[69] 100.00 87.16 100.00 85.68
SnakeCLEF2023[131] 22.17 21.98 16.51 16.45
SUN397[203] 75.69 6.22 73.93 5.62
STL10[32] 100.00 3.25 98.67 1.42
SVHN[123] 99.33 46.32 99.00 57.01
SynthClip106[61] 46.67 5.46 44.00 4.30
VEG200[70] 84.75 53.90 79.50 46.70
Zappos50k[206] 35.14 22.36 31.29 18.25
8E FoMo-in-Flux: Caption Pipeline
As part of our FoMo-in-Flux pipeline, we converted 63 different classification and retrieval datasets into a
format that made them amenable for contrastive language-image pretraining. This entailed providing text
captions for each of the images in the classification datasets. For this, our main aims were to ensure: (1)
scalability of the captioning pipeline, (2) that the captions captured real-world and fine-grained details about
the image, (3) that the captions were not verbose so that they would fit into the context length of CLIP’s
text encoder (77 tokens), and (4) that the captions contained the true classname of each of the images from
the classification datasets.
To this end, we proceeded to caption the images in a three-stage manner—(1) We first used a BLIP-2
model [97] using a T5-XL decoder to ensure high captioning performance along with scalability to provide
initial seed synthetic captions for each of the images, (2) we next generated templated captions for each of the
images using the classnames, for e.g., for an image of a tench in the ImageNet dataset, we use a templated
caption, “A photo of a tench” and similarly for an image of a manted howler in the Monkeys10 dataset,
we use a templated caption, “A photo of a mantled howler, a type of monkey.”, and finally (3) we merge
both the templated and seed synthetic captions using the Capsfusion [208] model—a LLaMA model that is
finetuned to take in two captions for an image, and return a merged caption capturing the key aspects of
both the captions. Using our three-stage pipeline, we are able to generate diverse yet faithful captions for
each of the images in our set of 63 datasets. We showcase a visualisation of our generated captions for some
of our constituent datasets in Fig. 20.
9Dataset: AI2Diagrams Dataset: ArtBench10 Dataset: Birdsnap Dataset: Caltech101 Dataset: Cars196
Class caption: “A Class caption: “A Class caption: “A Class caption: “A Class caption: “A
photo of a circuits painting of the photo of the bird photo of a camera.” photo of a GMC
diagram.” artist adam Acadian Synthetic caption: Terrain SUV 2012, a
Synthetic caption: baltatu.” Flycatcher.” “the canon eos dslr type of car.”
“the main parts of Synthetic caption: Synthetic caption: has a flash and Synthetic caption:
the model of the “two men in a boat “small green bird lens attached to “gmc terrain slt.”
circuit are :” sitting on a on the branch.” the front.” Merged caption: “The
Merged caption: “The cliff.” Merged caption: “On Merged caption: “The GMC Terrain SUV
main parts of the Merged caption: “A the branch, there Canon EOS DSLR, 2012 is a type of
model of the painting by the is a small green depicted in a car, specifically
circuit are artist Adam Baltatu bird known as the photo, features a the SLT model.”
depicted in a photo depicts two men Acadian flash and a lens
of a circuit sitting in a boat Flycatcher.” attached to the
diagram.” on a cliff.” front.”
Dataset: CLRS Dataset: Dollarstreet Dataset: DF20-mini Dataset: DomainNet-C Dataset: DomainNet-S
Class caption: “A Class caption: “A Class caption: “A Class caption: “A Class caption: “A
satellite image of photo of a washing photo of the fungi clipart of a sketch of a dog.”
a airport.” machine from Agaricus arvensis.” butterfly.” Synthetic caption: “a
Synthetic caption: “a Pakistan.” Synthetic caption: Synthetic caption: “a sketch of an
google satellite Synthetic caption: “the image shows colorful butterfly american bulldog
image of the “image of washing three white and is shown on a white standing”
airport and parking machine” black mushrooms” background” Merged caption: “A
lot” Merged caption: “The Merged caption: “The Merged caption: “A detailed sketch of
Merged caption: “A image depicts a image depicts three colorful butterfly an American Bulldog
Google satellite washing machine Agaricus arvensis is depicted in a standing.”
image of the from Pakistan.” mushrooms, which clipart, set
airport and parking are characterized against a white
lot provides a by their white and background.”
detailed real-world black coloration.”
view.”
Dataset: DTD Dataset: FGVCAircraft Dataset: Flowers102 Dataset: FRU92 Dataset: Monkeys10
Class caption: “A Class caption: “A Class caption: “A Class caption: “A Class caption: “A
photo of a waffled photo of a EMB-120, photo of a passion photo of a candied photo of a bald
texture.” a type of flower, a type of date, a type of uakari, a type of
Synthetic caption: “i aircraft.” flower.” fruit.” monkey.”
made chocolate chip Synthetic caption: “The Synthetic caption: “a Synthetic caption: Synthetic caption: “a
banana waffles and airplane is blue purple flower with “nuts in small bag monkey holds a
ate them with milk” and white and an orange bud” with chinese and piece of food, and
Merged caption: “I sitting on the Merged caption: “A chinese chinese is eating it”
made chocolate chip runway” photo of a passion written” Merged caption: “A
banana waffles and Merged caption: “A flower, which is a Merged caption: “A photo captures a
enjoyed them with photo of an type of flower photo of a candied bald uakari, a type
milk, capturing a EMB-120, a type of characterized by date, a type of of monkey, holding
photo of their aircraft, shows a its purple color fruit, along with a piece of food and
delightful waffled blue and white and orange bud.” nuts in a small bag enjoying a meal.”
texture.” airplane parked on with Chinese
the runway.” characters written
on it.”
Figure20: Random Samples from FoMo-In-Flux. Weshowcasesomesamplecaptionsgeneratedusingour
three-stage pipeline for a few of the datasets in FoMo-In-Flux. The Class caption is the templated caption
using the class-name, Synthetic caption is the caption generated using BLIP-2, and the Merged caption
is the final merged caption using Capsfusion (merging both Class caption and Synthetic caption).
10F Data Statement
Dataset Title: FoMo-in-Flux
Dataset Curator(s): N/A
Dataset Version: 1.0
Dataset Citation: N/A
Data Statement Authors: N/A
Data Statement Version: 1.0
Data Statement Citation and DOI: N/A
F.1 Executive Summary
FoMo-in-Flux is an aggregate benchmark comprising over 2.53M images from 63 classification and retrieval
datasets, including 61 existing datasets and 2 newly introduced ones, described in Sec. 3.1.1. On top of
image and labels provided by the original datasets, we provide a caption for each image, generated using the
pipeline described in Appx. E.
F.2 Curation Rationale
Fomo-in-Fluxisabenchmarkforcontinualmultimodalpretrainingthatemphasizesadaptationacrossdistinct
subdomains over long time horizons, while allowing for finegrained controllability of particular concepts and
classespresentedatrespectiveupdatestepsforadata-centricperspectiveoncontinualmultimodalpretraining.
The constituent datasets were selected based on availability, licensing, quality of labels, diversity of data
domains, quality of the resulting captions, and the degree of adoption in the computer vision and machine
learning research communities.
F.3 Documentation for Source Datasets
The licensing information for source datasets, as well as relevant citations, are provided in Tab. 2 and Tab. 3.
We release the captions, as well as the Obscure Animals and Obscure Things datasets under the MIT license
(https://opensource.org/license/mit).
F.4 Language Varieties
All the class labels and captions are in English.
F.5 Speaker Demographic
N/A
F.6 Annotator Demographic
The captions were created using an automated pipeline and based on original class labels, as outlined in
Appx. E. For selected simpler datasets, we use the templated captions directly, as shown in Tab. 2 and Tab. 3.
For the information about annotators of source datasets, please see the references in Tab. 2 and Tab. 3.
F.7 Speech Situation and Text Characteristics
N/A
F.8 Preprocessing and Data Formatting
The class labels are used as-is with no modification. All images are resized to 224x224 pixels.
11F.9 Capture Quality
N/A
F.10 Limitations
Although great care was taken to ensure the correctness of the dataset and random samples of the captions
were manually inspected for a quality check, we did not verify the captions for all 2.53M samples. Given the
dependence on BLIP-2 [97] and Capsfusion [208], the captions might reflect the biases and idiosyncracies of
these models.
Moreover, as an aggregate benchmark, Fomo-in-Flux reflects the data collection and annotation biases of
the source datasets. However, by pooling diverse sources of data, we avoid a systematic dataset-wide curation
bias.
F.11 Broad Impact
Our dataset helps assess the continual multimodal pretraining performance across various methods, data
stream orderings, learning rate schedulers, and compute budgets. The insights gained will help optimize
continual pretraining, facilitating fewer large-scale model updates. This optimization, in turn, will help
decrease energy consumption and lower carbon emissions associated with continual adaptation of foundation
models, and overall encourage a more economical and ecological treatment of these large architectures.
F.12 Metadata
License: https://opensource.org/license/mit
Annotation Guidelines: N/A
Annotation Process: Automatic
Dataset Quality Metrics: N/A
Errata: N/A
F.13 Disclosures and Ethical Review
N/A
F.14 Other
N/A
F.15 Glossary
N/A
About this data statement
A data statement is a characterization of a dataset that provides context to allow developers and users to
better understand how experimental results might generalize, how software might be appropriately deployed,
and what biases might be reflected in systems built on the software.
This data statement was written based on the template for the Data Statements Version 2 Schema. The
template was prepared by Angelina McMillan-Major, Emily M. Bender, and Batya Friedman and can be
found at http://techpolicylab.uw.edu/data-statements.
12