A quasi-Bayesian sequential approach to deconvolution
density estimation
Stefano
Favaro∗1
and Sandra
Fortini†2
1
Department of Economics and Statistics, University of Torino and Collegio Carlo Alberto, Italy
2Department of Decision Sciences, Bocconi University, Italy
August 27, 2024
Abstract
Density deconvolution addresses the estimation of the unknown (probability) den-
sity function f of a random signal from data that are observed with an independent
additive random noise. This is a classical problem in statistics, for which frequen-
tist and Bayesian nonparametric approaches are available to deal with static or batch
data. In this paper, we consider the problem of density deconvolution in a streaming
or online setting where noisy data arrive progressively, with no predetermined sample
size, and we develop a sequential nonparametric approach to estimate f. By relying
on a quasi-Bayesian sequential approach, often referred to as Newton’s algorithm, we
obtain estimates of f that are of easy evaluation, computationally efficient, and with
a computational cost that remains constant as the amount of data increases, which is
critical in the streaming setting. Large sample asymptotic properties of the proposed
estimates are studied, yielding provable guarantees with respect to the estimation of
f at a point (local) and on an interval (uniform). In particular, we establish local and
uniform central limit theorems, providing corresponding asymptotic credible intervals
and bands. We validate empirically our methods on synthetic and real data, by con-
sidering the common setting of Laplace and Gaussian noise distributions, and make a
comparison with respect to the kernel-based approach and a Bayesian nonparametric
approach with a Dirichlet process mixture prior.
Keywords: Credible intervals and bands; deconvolution; Gaussian central limit theorem; non-
parametric density estimation; quasi-Bayes; sequential estimation; streaming data
1 Introduction
Density deconvolution addresses the estimation or recovery of the unknown (probability) density
function of a random signal X from data observed with an independent additive random noise Z.
∗stefano.favaro@unito.it
†sandra.fortini@unibocconi.it
1
4202
guA
62
]EM.tats[
1v20441.8042:viXraTo be more precise, consider n ≥ 1 one-dimensional observations Y ,...,Y such that
1 n
Y = X +Z i = 1,...,n,
i i i
where the signal components X ’s are independent of the noise components Z ’s, with the latter
i i
being independent and identically distributed (i.i.d.) with known density function f . Based on
Z
the noisy observations Y ’s, the object of interest is the density function f of the X ’s, which is
i X i
related to the density function f of Y through the convolution of f and f , i.e. f = f ∗f .
Y X Z Y X Z
Density deconvolution arises in many fields, typically in connection to applications involving data
with measurement errors, or data subject to blurring or distortion constraints (Carroll et al., 2006;
Buonaccorsi, 2010). For instance, in medicine, to reconstruct images from blurred data obtained
through magnetic resonance imaging or positron emission tomography. In econometrics, to analyze
theriskofinvestmentsbystochasticvolatilitymodelswitherrors-in-variables. Inbioinformaticsand
chemistry, to correct the (background) effects induced by measurement errors, before performing
further statistical analysis. In astronomy, to reconstruct images of celestial objects from contami-
nated radio interferometric data, as well as to verify the galaxy formation from perturbed veloci-
ties of stars. Further, density deconvolution appears in epidemiology, remote sensing, seismology,
marketing, and, recently, in privacy-preserving data analysis, where it enables for reconstructing
meaningful insights while respecting suitable privacy constraints.
The estimation of f has received a lot of attention in the frequentist literature, leading to sev-
X
eral nonparametric approaches to estimate f , e.g. kernel-based (Carroll and Hall, 1988), wavelets
X
(Pensky and Vidakovic, 1999) and iterative approaches (Hesse and Meister, 2004). It is well known
that the optimal minimax rate at which f can be estimated depends on the smoothness of f as
X X
well as on the smoothness of f (Butucea and Tsybakov, 2008a,b). See Yi et al. (2021, Chapter
Z
10-14) for an overview with emphasis on kernel methods. On the Bayesian side, there is a recent
literature on frequentist properties of Bayesian nonparametric estimation of f , namely posterior
X
consistency and contraction rates, which shows how density deconvolution poses new challenges
with respect to the use of the celebrated strategy of Ghosal et al. (2000). By assuming a Laplace
distributed noise Z and a Dirichlet process mixture of Gaussian densities as a prior for f , Gao
X
and Van der Vaart (2016) developed a strategy to obtain posterior contraction rates for estimating
f , showing a match with minimax rates under a suitable smoothness for f ; see also Scricciolo
X X
(2018). As a novelty, it emerges the use of an inversion inequality that, roughly, it allows to “trans-
late” posterior contraction rates for estimating f into corresponding rates for the estimation of f
Y X
(Nguyen, 2013). Rousseau and Scricciolo (2023) further developed early ideas through the proposal
of a novel, and sharper, inversion inequality that holds for any ordinary-smooth noise distribution,
improving over the previous posterior contraction rates, and also developing adaptive rates.
1.1 Our contributions
We consider density deconvolution in a streaming or online setting where noisy data arrive progres-
sively, with no predetermined sample size n, and we develop a sequential nonparametric approach
to estimate f . We assume that f is a finite mixture of a known strictly positive kernel k(· | θ),
X X
for θ ∈ Θ, with unknown mixing density function g with respect to the counting measure µ on a
finite set Θ. For notational convenience, we denote the finite mixture as an integral with respect
to µ, i.e.
(cid:90)
f (x) = k(x | θ)g(θ)µ(dθ). (1)
X
Θ
This implies that f is itself a finite mixture, with known kernel k ∗f and mixing the same g
Y Z
as f . Under the model f , we follow a procedure proposed by Smith and Makov (1978), often
X Y
2referred to as Newton’s algorithm (Newton et al., 1998), to estimate g sequentially, as the noisy
observations Y ’s arrive in the stream. Starting with an initial guess g˜ for g and a sequence
n 0
(α˜ ) of real numbers in (0,1), for any n ≥ 1 Newton’s algorithm provides a recursive estimate
n n≥1
g˜ of g as a convex linear combination between g˜ and an updated version of g˜ through Y ,
n n−1 n−1 n
with coefficient or weight α˜ . Under standard assumptions of regularity for k and f , we rely on
n Z
stochastic approximation to show that g˜ has a large n almost-sure limit that coincides with the
n
limit that would be obtained if we could observe the X ’s, i.e. the direct problem. As an estimate
n
(X)
of f , we consider the plug-in estimate f obtained from (1) by replacing g with g˜ . Besides
X n n
its ease of evaluation, the proposed estimate is efficient, with a computational cost that remains
constant as data increases, which is critical in the streaming setting.
We study large n asymptotic properties of f(X) as a sequential estimate of f at a point x ∈ R,
n X
(X)
i.e. localorpointwiseestimation; inparticular,weestablishalocalcentrallimittheoremforf at
n
x,characterizingthelimitingGaussiandistributionandprovidingasymptoticcredibleintervals. We
(X)
also investigate large n asymptotic properties of f as a sequential estimate of f on a bounded
n X
interval I ⊂ R, i.e. global or uniform estimation; in particular, we establish a uniform central
(X)
limit theorem for f on I, characterizing the limiting Gaussian process and providing asymptotic
n
credible bands. For typical values of the learning rate α˜ of Newton’s algorithm, the convergence
n
−1/2
rate to the Gaussian limits is α˜ , with the noise distribution affecting credible intervals and
n
bands through the variance of the limiting distribution. Our asymptotic analysis is of interest also
for the direct density estimation problem, where only central limit theorems for the estimation of
the mixing g are available (Martin and Ghosh, 2008; Fortini and Petrone, 2020). To validate our
methods we consider the setting in which f is a location-scale mixture of Gaussian densities, and
X
f is a Laplace density function (ordinary-smooth noise) or a Gaussian density function (super-
Z
smooth noise). We present numerical illustrations on synthetic and real data, comparing our
approach with the kernel-based approach and a Bayesian nonparametric approach with a Dirichlet
process mixture prior. We discuss the choice of α˜ , proposing an empirical procedure to calibrate
n
α˜ with respect to direct density estimation problem (Fortini and Petrone, 2020).
n
1.2 Related works
Newton’s algorithm has been extensively investigated in the direct density estimation problem,
including computational aspects (Newton and Zhang, 1999; Quintana and Newton, 2000), connec-
tions with stochastic approximation (Martin and Ghosh, 2008), consistency properties (Tokdar et
al., 2009; Martin, 2012), and interplay with Bayesian nonparametrics (Favaro et al., 2012; Han et
al., 2018; Fortini and Petrone, 2020). See Martin (2019) for a thoughtful review. Our work is the
first to propose Newton’s algorithm in density deconvolution. In general, we are not aware of any
work providing a principled framework for density deconvolution in the streaming setting. Existing
approaches are developed for static or batch data, and their adaptations to streaming data are typ-
ically achieved at a loss of ease of evaluation and/or computational efficiency. The use of Newton’s
algorithm makes our approach neither frequentist nor Bayesian, though it admits an interpretation
as a quasi-Bayesian approach, in the sense that the large sample behaviour of Newton’s algorithm
is that of a Bayesian predictive rule (Fortini and Petrone, 2020; Fong et al., 2023). To some extent,
our approach is related to a Bayesian nonparametric approach with a Dirichlet process mixture
prior, for which MCMC algorithms with static data are proposed in Beraha et al. (2023). In princi-
ple, MCMC algorithms can be adapted to streaming data, by resorting to sequential Monte Carlo
and variational Bayes methods. Though, the resulting sequential posterior inferences have a larger
computational cost than our approach, as well as issues in scalability.
31.3 Organization of the paper
The paper is structured as follows. In Section 2 we present Newton’s algorithm for density decon-
volution, and its almost-sure asymptotic properties. Section 3 contains local and uniform central
limit theorems for our estimate, including asymptotic credible intervals and bands. In Section 4
we validate our methods on synthetic and real data, and in Section 5 we discuss some directions
for future research. Appendix B contains new results on stochastic approximation, which are of
independent interest, whereas in Appendix C and D we recall some technical results on almost-
sure conditional convergence for martingales and concentration inequalities for the supremum of
a Gaussian process. Appendix E and Appendix F contain the proofs of our esults. Additional
illustrations are in Appendix G
2 Newton’s algorithm for density deconvolution
We consider a stream of observations (Y ) such that Y = X +Z for n ≥ 1, where: i) the
n n≥1 n n n
X ’s have density function f defined as the mixture model (1), with respect to a Baire measure
n X
λ; ii) the Z ’s are i.i.d. with density function f with respect to λ, and independent of the X ’s.
n Z n
Then, the Y ’s have density function
n
(cid:90)
f (y) = (f ∗f )(y) = k˜(y | θ)g(θ)µ(dθ), (2)
Y Z X
Θ
where
k˜(y | θ) = (f ∗k(· | θ))(y).
Z
Under the mixture model (2), we apply Newton’s algorithm to estimate g sequentially, as the Y ’s
n
arrive in the stream. Let g˜ be an initial guess for g, and (α˜ ) be a sequence of real numbers in
0 n n≥1
(0,1) such that (cid:80) α˜ = +∞ and (cid:80) α˜2 < +∞. For n ≥ 0, Newton’s algorithm recursively
n≥1 n n≥1 n
estimate g as
g˜ (θ) = (1−α˜ )g˜ (θ)+α˜ g˜ (θ | Y ), (3)
n+1 n+1 n n+1 n n+1
where
k˜(y | θ)g˜ (θ)
n
g˜ (θ | y) = . (4)
n (cid:82) k˜(y | θ′)g˜ (θ′)µ(dθ′)
Θ n
The choice of the learning rate α˜ will be discussed in Section 4, providing an empirical procedure
n
to calibrate α˜ with respect to direct density estimation problem.
n
Newton’s algorithm (3) admits an interpretation as a quasi-Bayesian learning procedure, with
a statistical model that changes at each n ≥ 1, according to the estimate g˜ of g (Fortini and
n
Petrone, 2020). For each n ≥ 1, the statistical model for the next observation Y is the mixture
n+1
model (2) with g being replaced by g˜ . Under this learning procedure, future observations are con-
n
ditionally identically distributed, given current information (Berti et al., 2004), and the conditional
distribution of Y , given (Y ,...,Y ), has density function
n+1 1 n
(cid:90)
f(Y)(y) = k˜(y | θ)g˜ (θ)µ(dθ) (5)
n n
Θ
with respect to λ. The recursive equations (3) and (5) define a probabilistic learning procedure,
that we encode in a probability space (Ω,F,P) on which all the random variables are assumed to
be defined. We denote by E the conditional expectation, given the sigma-algebra G generated by
n n
(Y ,...,Y ), for n ≥ 1. Although our learning procedure is built on the sequence of the observed
1 n
4random variables (Y ) , we can assume that there exist two sequences of random variables
n n≥1
(X ) and (Z ) such that for any n ≥ 1
n n≥1 n n≥1
Y = X +Z , (6)
n n n
where the Z ’s are i.i.d with a known density function f and independent of the X ’s. Let
n Z n
Y = (Y ,...,Y ), X = (X ,...,X ) and Z = (Z ,...,Z ).
1:n 1 n 1:n 1 n 1:n 1 n
Proposition2.1. Thereexistsaprobabilityspace(Ω,F,P)andthreesequencesofrandomvariables
(X ) , (Y ) and (Z ) , which are defined on (Ω,F,P), such that (6), (3), (4) and (5) hold,
n n≥1 n n≥1 n n≥1
the Z ’s are independent of the X ’s and i.i.d. with known density f with respect to λ, and the
n n Z
conditional distribution of X , given (X ,Y ,Z ) has density function
n+1 1:n 1:n 1:n
(cid:90)
f(X)(·) = k(· | θ)g˜ (θ)µ(dθ) (7)
n n
Θ
with respect to λ. The conditional distribution of X given (X ,Y ,Z ) is uniquely defined
n+1 1:n 1:n 1:n
by (6), (3), (4) and (5), as well as the assumptions on (Z ) .
n n≥1
See Appendix E for the proof of Proposition 2.1. Since f and f share the same mixing g, we
Y X
(X)
consider f in (7) as a sequential estimate of f , namely a plug-in estimate obtained from (1) by
n X
replacing g with g˜ . We characterize the large n asymptotic behaviour of g˜ under the following
n n
assumptions:
A1) Θ is a finite set, µ is the counting measure and g (θ) > 0 for every θ ∈ Θ;
0
A2) λ is the Lebesgue measure on R;
A3) for x ∈ R and θ ∈ Θ, the kernel k(x | θ) is strictly positive, bounded uniformly with respect
to x and θ, continuously differentiable with respect to x, with sup |k′(x | θ)| < +∞,
θ∈Θ,x∈R
where k′(x | θ) is the derivative of k(x | θ) with respect to x;
(cid:82)
A4) k(· | θ)g(dθ)µ(dθ) identifies g;
Θ
A5) f is strictly positive, bounded and uniformly continuous on R.
Z
These assumptions imply that k˜(y | θ) > 0 for every y ∈ R and every θ ∈ Θ, (cid:82) k(x | θ)g(θ)µ(dθ)
Θ
and (cid:82) k˜(y | θ)g(θ)µ(dθ) are bounded and uniformly continuous on R, for any g, and that (cid:82) k˜(· |
Θ Θ
θ)g(θ)µ(dθ) identifies g. Further, A3)-A5) hold, for instance, if k is the Gaussian kernel with
boundedexpectationandbounded,awayfromzerovariance,andf istheLaplacedensityfunction.
Z
The assumption that k(x | θ) is strictly positive for every x ∈ R may be relaxed, allowing the
support of k(· | θ) to depend on θ. Then, the estimation of f by (7) still applies, and the large n
X
asymptotic properties of Section 3 hold in regions where f is strictly positive. Here we keep the
X
assumption k(x | θ) > 0 for every x and θ to simplify the statements of our results.
Under the assumptions A1)-A5), by Fortini and Petrone (2020, Theorem 1), the distribution
function G˜ defined as G˜ (θ) = (cid:82) g˜ (θ′)µ(dθ′) converges weakly, P-a.s., to a discrete (almost
n n θ′≤θ n
surely) random distribution function G˜. Since G˜ and G˜ are discrete (almost surely) distributions
n
on the finite set Θ, then there exists a random probability mass function g˜ such that, as n → +∞
P-a.s.
sup|g˜ (θ)−g˜(θ)| −→ 0. (8)
n
θ∈Θ
5In addition to (8), we show, under mild assumptions on {k(· | θ) : θ ∈ Θ}, that g˜ has P-a.s. limit
n
that coincides (almost surely) with the P-a.s. limit that would be obtained if we could observe the
X ’s. If we could observe the X ’s, then for n ≥ 0 Newton’s algorithm would recursively estimate
n n
g as follows:
k(X | θ)g (θ)
n+1 n
g (θ) = (1−α )g (θ)+α , (9)
n+1 n+1 n n+1(cid:82) k(X | θ′)g (θ′)µ(dθ′)
Θ n+1 n
where g is an initial guess for g, and (α ) is a sequence of real numbers in (0,1) such that
0 n n≥1
(cid:80) α = +∞ and (cid:80) α2 < +∞. By relying on stochastic approximation, the next theorem
n≥1 n n≥1 n
establishes a merging (almost surely) between the P-a.s. limits of the estimate g˜ and the estimate
n
g , as n → +∞.
n
Theorem 2.2. Let g˜ and g as in (9) and (3), respectively, with g = g˜ . Under A1)-A5) and
n n 0 0
(cid:82) k(x|θ1)2
k(x | θ )λ(dx) < +∞ for every θ ,θ ,θ ∈ Θ, as n → +∞
R k(x|θ2)2 3 1 2 3
P-a.s.
sup|g (θ)−g˜ (θ)| −→ 0.
n n
θ∈Θ
See Appendix E for the proof of Theorem 2.2. Given our assumptions, g˜ converges uniformly
n
on Θ to g˜, P-a.s. Consequently, the thesis of Theorem 2.2 is equivalent to the P-a.s. convergence
of g towards g˜. The proof of Theorem 2.2 is inspired by Martin and Ghosh (2008, Theorem 3.4),
n
where stochastic approximation is employed to prove that Newton’s algorithm gives a consistent
estimator of a “true” (non random) mixing measure, say g . In our framework, however, the
true
limit g˜ is a random probability measure, which prevents the use of classical results on stochastic
approximation. To deal with the randomness of the limit, we developed new results on stochastic
approximation, which are of independent interest. See Appendix B for details. Differently from
Theorem 2.2, Martin and Ghosh (2008, Theorem 3.4) only assumes that k is strictly positive. As
we discussed in Remark E.1, we argue that the proof of Martin and Ghosh (2008, Theorem 3.4)
is not complete, and that our additional assumpition on k allows to complete it. The assumption
holds, for instance, if k is a Gaussian kernel with bounded expectation µ and variance σ2 ∈ [h ,h ]
1 2
with h > 0 and h < 3/2h , or when k is a truncated Gaussian kernel with bounded mean and
1 2 1
variance. Since the implementation of Newton’s algorithm requests some sort of truncation, this
assumption has no impact from a practical point of view. Note that Theorem 2.2 holds even if
g ̸= g˜ , provided that g (θ) > 0 for every θ > 0.
0 0 0
3 Asymptotic credible intervals and bands
(X)
We study large n asymptotic properties of the sequential estimate f in (7), with respect to
n
both local and uniform estimation of f . We establish local and uniform central limit theorems,
X
providing corresponding asymptotic credible intervals and bands. These results rely on an almost-
sure conditional convergence for martingales (Crimaldi, 2009). See Appendix C for the definition
of almost sure conditional convergence for Polish spaces, and for related technical results.
3.1 Local estimation
We characterize the large n Gaussian behaviour of f(X) as an estimate of f at a point x ∈ R, i.e.
n X
local or pointwise estimation. Let
(cid:90)
f(X)(·) = k(· | θ)g˜(θ)µ(dθ), (10)
Θ
6where the random distribution g˜ is the P-a.s. limit of g˜ , as defined in (8). Since
n
(cid:16) (cid:17)
(cid:18)(cid:90) (cid:19)
E f(X)(x) = E k(x | θ)g˜ (θ)µ(dθ)
n−1 n n−1 n
Θ
(cid:90)
(X)
= k(x | θ)g˜ (θ)µ(dθ) = f (x),
n−1 n−1
Θ
(X) (X)
and f (x) ≤ sup k(x | θ) < ∞, then (f (x)) is a bounded martingale, converging P-
n x∈R,θ∈Θ n n≥1
a.s. and L1 to f(X)(x). Define the limiting density function of (Y ) and the limiting conditional
n n≥1
density function of (X ) , given (Y ) , as
n n≥1 n n≥1
(cid:90)
f(Y)(y) = k˜(y | θ)g˜(θ)µ(dθ), (11)
Θ
and
(cid:90)
f(X)(x | y) = k(x | θ)g˜(θ | y)µ(dθ), (12)
Θ
respectively, with
k˜(y | θ)g˜(θ)
g˜(θ | y) = . (13)
f(Y)(y)
By A1), A2) and A5), f(X)(x), f(Y)(y) and f(X)(x | y) are strictly positive for every x and y. The
(X)
next theorem provides a local central limit theorem for f (x).
n
Theorem 3.1. For x ∈ R, let f(X) (x) and f(X)(x) be as in (7) and (10), respectively. Under
n
A1)-A5), if (b ) is a sequence of strictly positive numbers with
n
(cid:88) (cid:88)
b ↑ +∞, α˜4b2 < +∞, b supα˜2 → 0, b α˜2 → 1, (14)
n n n n k n k
k≥n
n≥1 k≥n
then, as n → +∞, b1/2 (f(X) (x) − f(X)(x)) converges, in the sense of almost-sure conditional
n n
convergence, to a centered Gaussian kernel with random variance function
(cid:90) (cid:16) (cid:17)2
v(x) = f(X)(x | y)−f(X)(x) f(Y)(y)λ(dy), (15)
with f(X)(· | y), f(X) and f(Y) as in (12), (10) and (11), respectively. If α = ( α )γ, with α > 0
n α+n
and γ ∈ (1/2,1], then the convergence holds with b = 2γ−1n2γ−1.
n α2γ
See Appendix F for the proof of Theorem 3.1. If (α˜ ) is non increasing, then the condition
n
b sup α˜2 → 0 follows from (cid:80) α˜4b2 < +∞. A typical choice for (α˜ ) in Newton’s algorithm
n k≥n k n≥1 n n n
is α˜ = 1/(1+n). In this case, b = α˜−1.
n n n
WedenotebyΦ theGaussiancumulativedistributionfunctionwithmean0andvarianceσ2,
0,σ2
with the proviso that Φ is a distribution degenerate in zero. Under the assumptions of Theorem
0,0
3.1, for every z ̸= 0, as n → +∞
(cid:16) (cid:17)
P b1/2(f(X)(x)−f(X)(x)) ≤ z P −- →a.s. Φ (z),
n n n 0,v(x)
with b in (14). Since the random variable v(x) is not a function of Y , this result can not be
n 1:n
directly applied to construct credible intervals for f(X)(x). Then, we set
(cid:90) (cid:16) (cid:17)2
v (x) = f(X)(x | y)−f(X)(x) f(Y)(y)λ(dy), (16)
n n n n
7(X)
with f as in (7),
n
(cid:90)
f(Y)(y) = k˜(y | θ)g˜ (θ)µ(dθ), (17)
n n
Θ
(cid:90)
f(X)(x | y) = k(x | θ)g˜ (θ | y)µ(dθ), (18)
n n
Θ
and
k˜(y | θ)g˜ (θ)
n
g˜ (θ | y) = . (19)
n
(Y)
f (y)
n
In particular, v (x) converges to v(x), P-a.s. We refer to Lemma F.1 for details.
n
By means of Theorem 3.1, if (b ) is an increasing sequence of strictly positive numbers such
n
that (14) holds true, then, for every s ,s ∈ R, as n → +∞
1 2
E
n(cid:18) eis1bn1/2(cid:16) f(X)(x)−fn(X)(x)(cid:17) +is2vn(x)(cid:19)
P −- →a.s. eis2 1v(x)+is2v(x).
Therefore, by Portmanteau theorem, for every z > 0 and every ϵ > 0, it holds that
(cid:16) (cid:12) (cid:12) (cid:17)
liminfP b1/2(cid:12)f(X)(x)−f(X)(x)(cid:12) < z(max(v (x),ϵ))1/2 ≥ 2Φ (z)−1.
n n (cid:12) n (cid:12) n 0,1
n→+∞
If z is the (1−β/2)-quantile of the standard Gaussian distribution, then, as n → +∞
1−β/2
(cid:16)(cid:12) (cid:12) (cid:17)
liminfP (cid:12)f(X)(x)−f(X)(x)(cid:12) ≤ b−1/2z (max(v (x),ϵ)1/2 ≥ 1−β,
n (cid:12) (cid:12) n 1−β/2 n
n→+∞
i.e., for every ϵ > 0,
f(X)(x)±b−1/2z (max(v (x),ϵ)1/2 (20)
n n 1−β/2 n
is an asymptotic credible interval for f(X)(x) with confidence level at least (1−β).
3.2 Uniform estimation
(X)
We characterize the large n Gaussian behaviour of f as an estimate of f on a bounded interval
n X
I ⊂ R, i.e. global or uniform estimation. Let C(R) be the space of continuous functions with the
topology of uniform convergence on compact sets. Under our assumptions, the density function
(X)
f (ω) is continuous for every ω ∈ Ω. Accordingly, as n → +∞
n
f(X) P −- →a.s f(X),
n
i.e., for every compact set I,
(cid:12) (cid:12)
sup(cid:12)f(X)(x)−f(X)(x)(cid:12) P −- →a.s. 0,
(cid:12) n (cid:12)
x∈I
as n → ∞. The next theorem provides a refinement of this asymptotic result in terms of a uniform
(X)
central limit theorem for f . We show that, if (b ) is an sequence of strictly positive numbers
n n
such that (14) holds true, then the random conditional distribution of b1/2 (f(X) − f(X)), given
n n
the σ-algebra G converges P-a.s. to a centered Gaussian process kernel. See Appendix C for the
n
definition.
8Theorem 3.2. Let f(X) and f(X) be as in (7) and (10), respectively. Under A1)-A5), if (b ) is a
n n
sequence of strictly positive numbers as in Theorem 3.1, i.e.,
(cid:88) (cid:88)
b ↑ +∞, α˜4b2 < +∞, b supα˜2 → 0, b α˜2 → 1,
n n n n k n k
k≥n
n≥1 k≥n
then, as n → +∞, b1/2 (f(X) −f(X)) converges, in the sense of almost-sure conditional convergence
n n
in C(R), to a centered Gaussian process kernel, with random covariance function
(cid:90)
R(x ,x ) = (f(X)(x | y)−f(X)(x ))(f(X)(x | y)−f(X)(x ))f(Y)(y)dy, (21)
1 2 1 1 2 2
(cid:16) (cid:17)γ
with f(X)(· | y), f(X) and f(Y) as in (12), (10) and (11), respectively. If α˜ = α , with α > 0
n α+n
and γ ∈ (1/2,1], then the convergence holds with b = 2γ−1n2γ−1.
n α2γ
See Appendix F for the proof of Theorem 3.2. The random covariance function R is measurable
with respect to the sigma-algebra generated by g˜, and positive semi-definite for every g˜. Indeed,
for every J ≥ 1, x ,...,x ∈ R and u ,...,u ∈ R,
1 J 1 J
(cid:88)J (cid:90) (cid:16) (cid:17)(cid:16) (cid:17)
u u f(X)(x | y)−f(X)(x ) f(X)(x | y)−f(X)(x ) f(Y)(y)λ(dy)
i j i i j j
R
i,j=1
(cid:90) (cid:32) J (cid:33)2
(cid:88)
= u (f(X)(x | y)−f(X)(x )) f(Y)(y)λ(dy) ≥ 0.
i i i
R
i
For each possible realization g of g˜, let G be a Gaussian process in C(R), independent of g˜, and
g
with covariance function as in (21) with g in the place of g˜. Now, based on Theorem 3.2, the next
theorem provides an upper bound for the fluctuations of |G | on a bounded interval I. Let σ(I)
g˜
and ψ(z) be defined as:
(cid:18) (cid:90) (cid:19)1/2
σ(I) = sup (f(X)(x | y)−f(X)(x))2f(Y)(y)λ(dy) (22)
x∈I
and
(cid:18)(cid:90) (cid:19)1/2
ψ(z) = sup (f(X)(x | y)−f(X)(x | y))2f(Y)(y)λ(dy)−(f(X)(x )−f(X)(x ))2 .
1 2 1 2
x ,x ∈ I,
1 2
|x −x | < z
1 2
(23)
Moreover, let
ψ−1(t) = inf{z : ψ(z) > t}. (24)
The upper bound for the fluctuations of |G | is obtained by first bounding E(sup G (x) | g˜),
g˜ x∈I g˜
and then using concentration inequalities to bound G (x), uniformly in x ∈ I, with respect to its
g˜
conditional distribution, given g˜.
Theorem 3.3. For every β ∈ (0,1),
(cid:32) (cid:33)
(cid:90) σ(I)(cid:18) (cid:18)
λ(I)
(cid:19)(cid:19)1/2
(cid:112)
P sup|G (x)| ≤ 12 log 1+ dz+σ(I) 2|log(β/2)| | g˜ ≥ 1−β.
g˜ 2ψ−1(z/2)
x∈I 0
9SeeAppendixFfortheproofofTheorem3.3. Now,weareinthepositiontoprovideaconfidence
(X)
band for f . In particular, according to Theorem 3.2,
n
(cid:18) (cid:12) (cid:12) (cid:19)
P supb1/2(cid:12)f(X)(x)−f(X)(x)(cid:12) ∈ ·
n n (cid:12) n (cid:12)
x∈I
(cid:18) (cid:19)
−w → P sup|G (x))| ∈ · | g˜ P-a.s.
g˜
x∈I
On the other hand, it follows directly from Theorem 3.3 that, for every β ∈ (0,1),
(cid:18) (cid:19)
P sup|G (x)| ≤ v (I,β)|g˜ ≥ 1−β,
g˜ g˜
x∈I
where
(cid:90) σ(I)(cid:18) (cid:18)
λ(I)
(cid:19)(cid:19)1/2
(cid:112)
v (I,β) = 12 log 1+ dz+σ(I) 2|log(β/2)|, (25)
g˜ 2ψ−1(z/2)
0
with β ∈ (0,1), σ(I) as in (22) and ψ−1 as in (24), with ψ as in (23). Consider the estimator of
v (I,β) defined as
g˜
(cid:90) σn(I)(cid:18) (cid:18)
λ(I)
(cid:19)(cid:19)1/2
(cid:112)
v (I,β) = 12 log 1+ dz+σ (I) 2|log(β/2)|, (26)
n 2ψ−1(z/2) n
0 n
where the quantities σ (I) and ψ are defined as in (22) and (23), respectively, by replacing
n n
f(X)(· | y) with f(X) (· | y), f(X) with f(X) and f(Y) with f(Y) .
n n n
Theorem 3.4. Let v (I,β) and v (I,β) be as in (26) and (25), respectively. As n → ∞,
n g˜
P-a.s.
v (I,β) −→ v (I,β).
n g˜
See Appendix F for the proof of Theorem 3.4. From Theorem 3.4, for every ϵ > 0 and every
β ∈ (0,1),
(cid:18) (cid:12) (cid:12) (cid:19)
liminfP sup(cid:12)f(X)(x)−f(X)(x)(cid:12) < b−1/2max(v (I,β),ϵ) ≥ 1−β, (27)
n (cid:12) n (cid:12) n n
n→∞ x∈I
with (b ) as in (14). See Appendix F for details on Equation (27). In particular, it follows by a
n
direct application of Theorem 27 that, for every ϵ > 0 and every β ∈ (0,1),
f(X)±b−1/2max(v (I,β),ϵ) (28)
n n n
is an asymptotic credible band for f(X), on I, with confidence level at least 1−β.
4 Numerical illustrations
To validate our methods, both on synthetic data and real data, we consider signal components
(X ) that have density function f defined in terms of the mixture model (1) with a Gaussian
n n≥1 X
kernel k. Let ϕ(· | θ) be a Gaussian density function with parameter θ = (µ,σ2), such that µ ∈ R
is the mean and σ2 ∈ R+ is the variance, and for K > 1 let
K
(cid:88)
G(·) = p δ (·)
k θ
k
k=1
10be a discrete distribution on the set {θ ,...,θ } ∈ R×R+, i.e. p ∈ (0,1) for all k = 1,...,K and
1 K k
(cid:80)
p = 1. Throughout this section, we assume that
1≤k≤K k
(cid:90) K
(cid:88)
f (·) = ϕ(· | θ)G(dθ) = p ϕ(· | θ ), (29)
X k k
R×R+
k=1
wheretheGaussiankernelϕ(· | θ)isknownandthemixingprobabilityg = (p ,...,p )isunknown.
1 K
Further, we assume the noise components (Z ) to have Laplace distribution with mean 0 and
n n≥1
variance σ2, i.e. for z ∈ R
l
(cid:40) (cid:115) (cid:41)
1 2
f (z) = exp − |z| .
Z (cid:113) σ2
2σ2 l
l
We also consider f to be a Gaussian density function with mean 0 and variance σ2. The Laplace
Z g
distribution and the Gaussian distribution are the most common ordinary-smooth and super-
smooth noise distributions, respectively.
Remark 4.1. Ordinary smoothness and super smoothness are defined in terms of the charac-
teristic function of the noise distribution. If |E(eitZ)| = O(t−β) as t → +∞, for β ≥ 0, then
the distribution of Z is referred to as ordinary-smooth. Examples of ordinary-smooth distribu-
tions include Gamma, symmetric Gamma, Laplace or double Exponential, and their mixtures. If
|E(eitZ)| = O(tβ0)exp{−tβ/γ} as t → +∞, for β > 0, γ > 0 and real number β 0, then the distribu-
tion of Z is referred to as super-smooth. Examples of super-smooth distributions include Gaussian,
Cauchy, and their mixtures.
4.1 Synthetic data
We consider a unimodal f defined as the Gaussian mixture model (29). In particular, for fixed
X
n = 1000, 2000, 4000, we generate the synthetic signal components (X ,...,X ) as i.i.d. with
1 n
density function
f (x) = 0.3ϕ(x | −1, 2)+0.7ϕ(x | 3, 1.5) x ∈ R, (30)
X
and then we generate the noise components (Z ,...,Z ), independently of (X ,...,X ), as i.i.d.
1 n 1 n
with Laplace density function of mean 0 and standard deviation σ = 0.25, 0.50. From the X ’s and
l i
Z ’s, we generate (Y ,...,Y ) as Y = X +Z , for i = 1,...,n. Based on the noisy observations
i 1 n i i i
(X)
(Y ,...,Y ), we apply Newton’s algorithm to obtain the sequential estimate f of the density
1 n n
function f , as we described in Section 2, as well as to construct credible intervals and bands, as
X
described in Section 3. We set (or initialize) Newton’s algorithm as follows: i) the mean µ and the
variance σ2 of the Gaussian kernel ϕ(· | θ) take values in the sets R = {−40, −40.1,...,40.9, 40}
µ
and R = {0.01, 0.02,...,4.99, 5}, respectively, i.e. θ takes values in R ×R ; ii) the number K
σ2 µ σ2
of mixture components is the cardinality of the set R ×R , i.e. K = 80500; iii) the initial guess
µ σ2
g˜ for the mixing probability g = (p ,...,p ) is the Uniform distribution on the set R ×R ; iv)
0 1 K µ σ2
the learning rate is defined a α˜ = 1/(1+n). Newton’s algorithm (3) provides the estimate g˜ of
n n
(X)
g, from which we obtain the plug-in estimate f by replacing g with g˜ in (29). Then, 95%-level
n n
(X)
credible intervals and bands for f follow from (20) and (28), respectively. Results are reported
n
in Figure 1.
As a second example, we consider a bimodal f defined as the Gaussian mixture model
X
(29). In particular, for fixed n = 1000, 2000, 4000, we generate the synthetic signal components
11Figure 1: Unimodal f displayed in (30) (dashed black). Panel A and B: estimate f(X) of f for
X n X
n = 1000 (solid red), n = 2000 (solid green) and n = 4000 (solid blue), with σ = 0.25 (A) and
l
(X)
σ = 0.5 (B). Panel C: estimate f for n = 1000 and σ = 0.5 (solid red), and 95%-level credible
l n l
(X)
interval (dashdot red). Panel D: estimate f for n = 1000 and σ = 0.5 (solid red), and 95%-level
n l
credible band (dashdot red).
(X ,...,X ) as i.i.d. with density function
1 n
f (x) = 0.4ϕ(x | −2, 1)+0.5ϕ(x | 4, 1) x ∈ R, (31)
X
and then we generate the noise components (Z ,...,Z ), independently of (X ,...,X ), as i.i.d.
1 n 1 n
with Laplace density function of mean 0 and standard deviation σ = 0.25, 0.50. Finally, we
l
compute(Y ,...,Y )asY = X +Z , fori = 1,...,n. Basedonthenoisyobservations(Y ,...,Y ),
1 n i i i 1 n
(X)
weapplyNewton’salgorithmtoobtainthesequentialestimatef off ,aswedescribedinSection
n X
2, and to construct credible intervals and bands, as described in Section 3. The setting of Newton’s
algorithm is assumed to be the same of the previous unimodal example. Results are reported in
Figure 2.
(X)
Panel A and B of Figure 1 and Figure 2 display the estimate f of f for σ = 0.25 (panel A)
n X l
and σ = 0.5 (panel B). The density function f is the dashed black curve, whereas the estimate
l X
(X)
f is the solid red (n = 1000), solid green (n = 2000) and solid blue (n = 4000) curves. In all the
n
12Figure 2: Bimodal f displayed in (31) (dashed black). Panel A and B: estimate f(X) of f for
X n X
n = 1000 (solid red), n = 2000 (solid green) and n = 4000 (solid blue), with σ = 0.25 (A) and
l
(X)
σ = 0.5 (B). Panel C: estimate f for n = 1000 and σ = 0.5 (solid red), and 95%-level credible
l n l
(X)
interval (dashdot red). Panel D: estimate f for n = 1000 and σ = 0.5 (solid red), and 95%-level
n l
credible band (dashdot red).
(X)
scenarios,f reasonablyreconstructsthebulkoftheprobabilitymassoff . WerefertoAppendix
n X
G.1 and Appendix G.2 for a more extensive analysis that considers σ = 0.25, 0.50, 12, 3, 4 and
l
n = 1000, 2000, 4000, 8000, and for the same analysis under the assumption of a Gaussian noise
distribution with mean 0 and standard deviation σ = 0.25, 0.50, 12, 3, 4. Not surprisingly, the
g
reconstruction of f improves as the variance of the Laplace noise distribution decreases, as well as
X
the sample size increases. There are no remarkable differences between the Laplace and Gaussian
noise distributions, though for large values of σ = σ the estimates obtained under the Laplace
l g
noise distribution are slightly better than those under the Gaussian noise distribution. Panel C
and D of Figure 1 and Figure 2 focus on the scenario σ = 0.5 and n = 1000, which is the “worst”
l
case scenario in terms of variance of the noise distribution and sample size, and display 95%-level
credible intervals (panel C) and 95%-level credible band with the dashdot curve (panel D). For the
interval (−4, 6), which includes the bulk of the probability mass of f , the 95%-level credible band
X
include f . We refer to Appendix G.3 for an additional example that considers a multimodal f
X X
defined as the Gaussian mixture model (29), with both Laplace and Gaussian noise distribution.
134.2 Calibrating the learning rate α˜
n
We present an empirical procedure to calibrate the learning rate α˜ of Newton’s algorithm. Let
n
g be the estimate of g that would be obtained from Newton’s algorithm if we could observe the
n
X ’s, i.e. (9), and let g˜ be the estimate of g obtained from Newton’s algorithm by using the Y ’s,
n n n
i.e. (3). The proposed calibration of α˜ is based on matching the sizes of the updates of g and
n n
g˜ . For Newton’s algorithm based on the X ’s, we assume α = α/(α+n), for α > 0. This is
n n n
motivated by the interplay between Newton’s algorithm and the Bayesian nonparametric approach
with a Dirichlet process mixture as a prior model (Fortini and Petrone, 2020). The updates or
innovations of the sequence (g ) are
n
α
I (θ) = g (θ)−g (θ) = (g (θ | X )−g (θ)) i ≥ 1. (32)
i i i−1 i−1 i i−1
α+i
Further, for Newton’s algorithm based on the Y ’s, we assume that α˜ is in the class
n n
(cid:26)(cid:18)
α
(cid:19)γ
1
(cid:27)
: < γ ≤ 1 .
α+n 2
Accordingly, the updates or innovations of he sequence (g˜ ) are given by
n
(cid:18)
α
(cid:19)γ
I˜(θ,γ) = g˜(θ)−g˜ (θ) = (g˜ (θ | Y )−g˜ (θ)) i ≥ 1. (33)
i i i−1 i−1,γ i i−1,γ
α+i
Other choices for α and α˜ are possible, though what is presented in the most common . See
n n
Martin and Ghosh (2008) and Fortini and Petrone (2020) for details.
We calibrate the parameter γ by matching the sizes of the updates. For i ≥ 1, we set
∆ (θ,γ) = log|g (θ | X )−g (θ)|−log|g˜ (θ | Y )−g˜ (θ)|,
i i−1 i i−1 i−1,γ i i−1,γ
such that
(cid:18) (cid:19)
i
log|I˜(θ,γ)|−log|I (θ)| = ∆ (θ,γ)−(1−γ)log 1+ .
i i i
α
For the matching, we need to find the value of γ ∈ (1,1] that minimizes, for some M ∈ N,
2
(cid:88)M (cid:90) (cid:18) (cid:18) i(cid:19)(cid:19)2
E ∆ (θ,γ)−(1−γ)log 1+ g˜ (θ)µ(dθ).
i−1 i i−1,γ
α
Θ
i=1
We solve the problem numerically through a Monte Carlo simulation, for γ in a grid, by means of
(γ) (γ)
random sampling (θ ,...,θ ). See Algorithm 1 for details.
0 M
14Algorithm 1: Calibrating the learning rate α˜ =
(cid:0)
α
(cid:1)γ
n α+n
Input: N, L; M; Θ = {θ ,...,θ }; B = {b ,...,b }; g ; k; k˜;
1 N 1 L 0
for l = 1 to L do
Set g˜ = g
0 0
for i = 1 to M do
Sample θ ∼ g˜ (·); X ∼ k(· | θ ); Z ∼ f (z)
i i−1 i i i Z
Compute Y = X +Z
i i i
for j = 1 to N do
Compute g˜ (θ | Y ) =
k˜(Yi|θj)g˜i−1(θj)
i−1 j i (cid:80)N l=1k˜(Yi|θ l)g˜i−1(θ l)
Compute g (θ | X ) =
k(Xi|θj)gi−1(θj)
i−1 j i (cid:80)N l=1k(Xi|θ l)gi−1(θ l)
Compute g˜(θ ) = g˜ (θ )+
(g˜i−1(θj|Yi)−g˜i−1(θj))
i j i−1 j (1+i)γl
α
Compute g (θ ) = g (θ )+
(gi−1(θj|Xi)−gi−1(θj))
i j i−1 j 1+i
α
end
Return ∆ (γ ) = log
|gi−1(θi|Xi)−gi−1(θi)|
i l |g˜i−1(θi|Yi)−g˜i−1(θi)|
end
end
Output: γˆ = argmin
(cid:80)M (cid:0)
∆ (γ )−(1−γ
)log(cid:0)
1+
i(cid:1)(cid:1)2
l=1,...,L i=1 i l l α
We apply Algorithm 1 with k being the Gaussian kernel ϕ(· | θ), such that θ = (µ,σ2), and as-
suming: i)theLaplacenoisedistribution,withmean0andstandarddeviationσ = 0.25, 0.50, 12, 3, 4;
l
ii) the Gaussian noise distribution, with mean 0 and standard deviation σ = 0.25, 0.50, 12, 3, 4.
g
For M = 1000, 2000, 4000, we set (or initialize) Algorithm 1 as follows: i) the mean µ and the
variance σ2 of the Gaussian kernel ϕ(· | θ) take values in the sets R = {−40, −40.1,...,40.9, 40}
µ
and R = {0.01, 0.02,...,4.99, 5}, respectively, i.e. Θ = R ×R with cardinality N = 80500; ii)
σ2 µ σ2
B = {0.501, 0.502,...,0.999, 1} with cardinality L = 500; iii) g is the Uniform distribution on the
0
set Θ; iv) α = 1. Table 1 and Table 2 report the Monte Carlo estimate γˆ of γ under the Laplace
noise distribution and the Gaussian noise distribution, respectively. The results confirm what was
expected for the behaviour of γˆ, and hence for the calibrated learning rate α˜ , as a function of the
n
variance of the noise distribution and M: i) for a fixed M, the larger the variance the smaller γˆ;
ii) for a fixed variance, the larger M the larger γˆ. There are no remarkable differences between
the Laplace and Gaussian noise distributions, though for large values of σ = σ the estimate γˆ
l g
obtained under the Laplace noise distribution are slightly smaller than those under the Gaussian
noise distribution. For σ = 0.25 and σ = 0.5, which are the values considered in Figure 1 and
l l
Figure 2, the estimate of γ is very close to 1. Therefore, we expect that the use of the calibrated
learning rate to estimate f in (30) and (31) will produce results similar to those in Figure 1 and
X
Figure 2. Even for larger values of σ , the estimates γˆ are not far from 1.
l
4.3 Real data
We consider the Shapley galaxy data, which are available at https://rdrr.io/cran/spatstat.
data/man/shapley.html. The dataset comes from a survey of the Shapley Supercluster, one of
the most massive concentrations of galaxies in the local universe. See Drinkwater et al. (2004) and
references therein. The data provide the velocities (in km/s with respect to the Earth) of n = 4206
galaxies observed using the FLAIR-II spectrograph on the UK Schmidt Telescope (UKST). We
15Table 1: Laplace noise distribution: Monte Carlo estimates γˆ of γ
M = 1000 M = 2000 M = 4000
σ = 0.25 0.9841 0.9902 0.9906
l
σ = 0.50 0.9809 0.9840 0.9848
l
σ = 1 0.9743 0.9782 0.9775
l
σ = 2 0.9505 0.9598 0.9635
l
σ = 3 0.9248 0.9342 0.9438
l
σ = 4 0.8970 0.9188 0.9148
l
Table 2: Gaussian noise distribution: Monte Carlo estimates γˆ of γ
M = 1000 M = 2000 M = 4000
σ = 0.25 0.9864 0.9884 0.9912
g
σ = 0.50 0.9830 0.9822 0.9856
g
σ = 1 0.9707 0.9791 0.9737
g
σ = 2 0.9477 0.9656 0.9612
g
σ = 3 0.9307 0.9323 0.9431
g
σ = 4 0.8900 0.9002 0.9117
g
assume the data to be modeled as random variables (X ,...,X ) with density function f defined
1 n X
as the Gaussian mixture model (29). Then, the noisy data are obtained by adding to the X ’s
i
the noise components Z ’s, independent of the X ’s and i.i.d. with Laplace density function of
i i
mean 0 and standard deviation σ = 0.50, 1, 2, 3. Based on the noisy data, we apply Newton’s
l
(X)
algorithm to obtain the recursive estimate f of f , as described in Section 2, and to construct
n X
credible intervals and credible bands, as described in Section 3. The setting of Newton’s algorithm
is the same as in the previous examples with synthetic data. In particular, Newton’s algorithm (3)
(X)
provides the estimate g˜ of g, from which we obtain the plug-in estimate f by replacing g with
n n
(X)
g˜ in (29). Then, 95%-level credible bands for f follow from an application of (20) and (28),
n n
respectively. Results are reported in Figure 3 and Figure 4.
Panel A, B, C and D of Figure 3 display the estimates of f for σ = 0.50 (panel A), σ = 1
X l l
(panelB),σ = 2(PanelC)andσ = 3(panelD).ThehistogramrepresentstheShapleygalaxydata,
l
whereas the solid lines are the estimates obtained with Newton’s algorithm (red), kernel deconvo-
lution (green) and Bayesian nonparametric deconvolution (blue). For the kernel deconvolution, we
applied the fast Fourier transform algorithm with optimal bandwidth computed using the plug-in
methodsbyminimizingthemeanintegratedsquarederror. Thisavailableinthepackage”decon”at
16Figure 3: Shapley galaxy data. Panel A, B, C and D: estimate f(X) (solid red), estimate via kernel
n
deconvolution (solid green) and estimate via Bayesian nonparametric deconvolution (solid blue),
with σ = 0.5 (A), σ = 1 (B), σ = 2 (C) and σ = 3 (D).
l l l l
https://cran.r-project.org/web/packages/decon/. For the Bayesian nonparametric deconvo-
lution, we considered a Dirichlet process mixture prior for f and applied the conditional MCMC
X
algorithm or slice-sampling of Beraha et al. (2023); the marginal MCMC algorithm can be also
applied, though it is more expensive in terms of time of computation. Differently from Newton’s
algorithm, the fast Fourier transform and the conditional MCMC algorithm are not sequential,
meaning that they do not allow to update estimates as a new data point arrives, and therefore
must be reloaded on the updated data. Panel A, B, C and D of Figure 4 display the estimate of
f obtained with Newton’s algorithm (solid red) and 95%-level credible bands (dashdot red) for
X
σ = 0.55 (panel A), σ = 1 (panel B), σ = 2 (Panel C) and σ = 3 (panel D). See Appendix G.4 for
l l l
the same analysis under the assumption of a Gaussian noise distribution with mean 0 and standard
deviation σ = 0.50, 1, 2, 3.
g
5 Discussion
We developed a nonparametric approach to density deconvolution in a streaming or online setting
where noisy data arrive progressively, with no predetermined sample size. This is the first use of
17Figure 4: Shapley galaxy data. Panel A, B, C and D: estimate f(X) (solid red) and 95%-level
n
credible bands (dashdot red), with σ = 0.5 (A), σ = 1 (B), σ = 2 (C) and σ = 3 (D).
l l l l
Newton’s algorithm in density deconvolution, providing a principled, and practical, quasi-Bayesian
framework to deal with streaming data. As the n-th noisy observation arrives in the stream, our
(X)
approach produces the estimate f of the unknown density function f of the signal, as well as
n X
asymptotic credible intervals and bands, that are of easy evaluation, computationally efficient, and
with a computational cost that remains constant as the amount of data increases, which is critical
in the streaming setting. Our results are based on standard assumptions on regularity for f
X
and the noise distribution, including the common setting in which f is a location-scale mixture
X
of Gaussian densities, and the noise distribution is the Laplace (ordinary-smooth noise) or the
Gaussian (super-smooth noise) distribution. Though, more general kernels and noise distributions
can be considered. Numerical illustrations show that our approach performs well, and, for static
data, it competes with the kernel-based approach and a Bayesian nonparametric approach with a
Dirichletprocessmixtureprior. Whilewefocusedonone-dimensionaldata,ourresultsstillworkfor
multi-dimensional data, at least with respect to estimation and uncertainty quantification through
credible intervals. Instead, we expect the dimension to affect the asymptotic credible bands.
This work opens several opportunities for further research. In the future one may consider
developinganin-depthanalysisforthecalibrationofthelearningrateofNewton’salgorithm,which
herehasbeenaddressedonlyempirically,anddevelopageneralframeworkinwhichthelearningrate
18depends on the noise distribution. This is suggested by our calibration procedure, though different
proceduresmaybeconsidered(FortiniandPetrone,2020). Further,onemayinvestigateconsistency
and rates of convergence of Newton’s algorithm in density deconvolution, which may be pursued in
the frequentist sense, by assuming a true model for the data, or under the quasi-Bayesian model
developed in Fortini and Petrone (2020). We expect consistency to follow easily from previous
works on Newton’s algorithm in the direct density estimation problem, while obtaining rates of
convergence is more challenging (Tokdar et al., 2009; Martin, 2012). Because of the quasi-Bayesian
interpretation of Newton’s algorithm, one may consider properties of merging with respect to the
Bayesian nonparametric approach with a Dirichlet process mixture prior (Rousseau and Scricciolo,
2023; Beraha et al., 2023). Extensions of our study to the class of algorithms that were proposed
by Han et al. (2018), as well as developments for multivariate mixtures and dependent mixture
models, present interesting directions for future research.
Acknowledgement
Stefano Favaro wishes to thank Mario Beraha for the stimulating discussions on Bayesian deconvo-
lution, in connection with differential privacy, and for help with the code. Stefano Favaro received
funding from the European Research Council (ERC) under the European Union’s Horizon 2020
research and innovation programme under grant agreement No 817257.
References
Beraha, M., Favaro, S. and Rao, V.A. (2023). MCMC for Bayesian nonparametric mixture
modeling under differential privacy. Preprint arXiv:2310.09818.
Berti, P., Pratelli, L. and Rigo, P.(2004).Limittheoremsforaclassofidenticallydistributed
random variables.The Annals of Probability 32, 2029–2052.
Billingsley, P. (1999). Convergence of Probability Measures. John Wiley, New York.
Buonaccorsi, J.P. (2010). Measurement error: models, methods, and applications. CRC Press.
Butucea, C. and Tsybakov, A.B. (2007). Sharp optimality in density deconvolution with
dominating bias. I. Theory of Probability and Its Applications 52, 111–128.
Butucea, C. and Tsybakov, A.B. (2008). Sharp optimality in density deconvolution with
dominating bias. II. Theory of Probability and Its Applications 52, 237–249.
Carroll, R.J. and Hall, P. (1988). Optimal rates of convergence for deconvolving a density.
Journal of the American Statistical Association 18, 1184–1186.
Carroll, R.J., Ruppert, D., Stefanski, L. and Crainiceanu, C.(2006).Measurement error
in nonlinear models: a modern perspective. CRC Press.
Crimaldi, I. (2009). An almost-sure conditional convergence result and an application to a gen-
eralized P´olya urn. International Mathematical Forum 23, 1139–1156.
Crimaldi, I., Dai Pra, P. and Minelli, I.G. (2016). Fluctuation theorems for synchronization
of interacting P´olya’s urns. Stochastic Processes and their Applications 126, 930–947.
19Drinkwater, M.J., Parker, Q.A., Proust, D., Slezak, E. and Quintana, H (2004). The
large scale distribution of galaxies in the Shapley Supercluster. Publications of the Astronomical
Society of Australia 21, 89–96
Dunford, N. and Schwartz, J.T. (1988). Linear Operators, Part 1. John Wiley, New York.
GaoF.andVanderVaart, A.(2016).PosteriorcontractionratesfordeconvolutionofDirichlet-
Laplace mixtures. Electronic Journal of Statistics 10, 608–627.
Favaro, S., Guglielmi, A. and Walker, S.G. (2012) A class of measure-valued Markov chains
and Bayesian nonparametrics. Bernoulli 18, 1002–1030.
Fong, E., Holmes, C. and Walker, S.G. (2023). Martingale posterior distributions. Journal
of the Royal Statistical Society Series B 85, 1357–1391.
Fortini, S. and Petrone, S. (2020). Quasi-Bayesian properties of a procedure for sequential
learning in mixture models. Journal of the Royal Statistical Society Series B 82, 1087–1114.
Ghosal, S., Ghosh, J.K. and van der Vaart, A.W. (2000). Convergence rates of posterior
distributions. The Annals of Statistics 28, 500–531.
Hahn, P.R., Martin, R. and Walker, S.G. (2018). On recursive Bayesian predictive distribu-
tions. Journal of the American Statistical Association 113, 1085–1093.
Hesse, C.H. and Meister, A. (2004). Optimal iterative density deconvolution. Journal of Non-
parametric Statistics 56, 19–47.
Kallenberg, O. (1978). Foundations of modern probability. Springer-Verlag, New York.
Kushner, H.J. and Clarke, D.S. (1978). Stochastic Approximation Methods for Constrained
and Unconstrained Systems. Springer, New York.
Martin, R. (2012). Convergence rate for predictive recursion estimation of finite mixtures. Statis-
tics and Probability Letters 82, 378–384.
Martin, R. (2019). A survey of nonparametric mixing density estimation via the predictive recur-
sion algorithm. Sankhya B 83, 97–121.
Martin, R. and Ghosh, J.K. (2008). Stochastic approximation and Newton’s estimate of a
mixing distribution. Statistical Science 23, 365–382.
Massart, P. (2007) Concentration Inequalities and Model Selection. Lecture Notes in Mathemat-
ics, Springer-Verlag, Berlin.
Newton, M.A., Quintana, F.A. and Zhang, Y. (1998). Nonparametric Bayes methods us-
ing predictive updating. In Practical Nonparametric and Semiparametric Bayesian Statistics,
Springer.
Newton, M.A. and Zhang, Y. (1999). A recursive algorithm for nonparametric analysis with
missing data. Biometrika 86, 15–26.
Nguyen, X. (2013). Convergence of latent mixing measures in finite and infinite mixture models.
The Annals of Statistics 41, 370–400.
20Pensky, M. and Vidakovic, B. (1999). Adaptive wavelet estimator for nonparametric density
deconvolution. The Annals of Statistics 27, 2033–2053.
Quintana, F.A. and Newton, M.A. (2000). Computational aspects of nonparametric Bayesian
analysiswithapplicationstothemodelingofmultiplebinarysequences.JournalofComputational
and Graphical Statistics 9, 711–737.
Roeder, K (1990). Density estimation with confidence sets exemplified by superclusters and voids
in the galaxies. Journal of the American Statistical Association 85, 617-624.
Rousseau, J. and Scricciolo, C. (2023). Wasserstein convergence in Bayesian and frequentist
deconvolution models. The Annals of Statistics, to appear.
Scricciolo, C.(2018).BayesandminimaxlikelihoodforL1-WassersteindeconvolutionofLaplace
mixtures. Statistical Methods and Applications 27, 333–362.
Smith, A.F.M. and Makov, U.E. (1978). A quasi-Bayes sequential procedure for mixtures.
Journal of the Royal Statistical Society Series B 40, 106–112.
Tokdar, S.T., Martin, R. and Ghosh, J.K. (2009). Consistency of a recursive estimate of
mixing distributions. The Annals of Statistics 37, 2502–2522.
Yi, G.Y., Delaigle, A. and Gustafson, P. (2021). Handbook of measurement error models.
CRC Press.
Appendix
B A stochastic approximation with random limit
In this Section, we report a stochastic extension of Kushner and Clarke (1978, Theorem 2.3.1),
dealing with the almost-sure convergence of a sequence of random variables (X ) defined on a
n
probability space (Ω,F,P), taking values in Rd, for some d ∈ N, and satisfying, for almost all ω
and for every n ≥ 0,
X (ω) = X (ω)+a h (X (ω))+a β (ω)+a ξ (ω), (B.1)
n+1 n n ω n n n n n
with assumptions that are detailed below.
SA1 The sequence of real numbers (a ) satisfies: a > 0 for every n ≥ 0, a → 0 as n → ∞, and
n n n
(cid:80)
a = +∞.
n n
SA2 (X ) takes values in a compact, convex set F, a.s.
n
SA3 h (x) is measurable with respect to ω for every x ∈ F and is a continuous function with
ω
respect to x ∈ F, for almost all ω.
SA4 (β ) is a bounded (with probability one) sequence of random variables such that β → 0 a.s.
n n
21SA5 (ξ ) is a sequence of Rd valued random variables such that, for every ϵ > 0,
n
m
(cid:88)
lim P(sup || a ξ || ≥ ϵ) = 0.
i i
n→∞ m≥n
i=n
Remark B.1. Under SA1, if supE(||ξ ||2) < ∞ and there exists a filtration (G ) to which (ξ ) is
n n n
a adapted and such that
E(ξ | G ) = 0
n+1 n
for every n ≥ 0, then SA5 holds. Indeed, by Doob martingale inequality,
m ∞
(cid:88) (cid:88)
lim P(sup || a ξ || ≥ ϵ) ≤ lim a2E(||ξ ||2) = 0.
i i i i
n→∞ m≥n n→∞
i=n i=n
We now introduce some definitions. For every n ≥ 1, let
n−1
(cid:88)
t = a (n ≥ 1).
n i
i=0
Moreover, for t > 0, let
(cid:26)
max{n : t ≤ t}, t ≥ 0
m(t) = n
0 t < 0.
Set
n−1 n−1
(cid:88) (cid:88)
X(0)(t ,ω) = X (ω), B(0)(t ,ω) = a β (ω), M(0)(t ,ω) = a ξ (ω),
n n n i i n i i
i=0 i=0
and consider the piecewise linear interpolations of the processes X(0), B(0) and M(0):
t −t t−t
X(0)(t,ω) = n+1 X(0)(t ,ω)+ n X(0)(t ,ω) t ∈ (t ,t )
n n+1 n n+1
a a
n n
t −t t−t
B(0)(t,ω) = n+1 B(0)(t ,ω)+ n B(0)(t ,ω) t ∈ (t ,t ) (B.2)
n n+1 n n+1
a a
n n
t −t t−t
M(0)(t,ω) = n+1 M(0)(t ,ω)+ n B(0)(t ,ω) t ∈ (t ,t ).
n n+1 n n+1
a a
n n
In the following, also the piecewise constant, right continuous interpolation of X(0) will be consid-
ered:
(0)
X (t,ω) = X (ω) for t ∈ [t ,t ).
n n n+1
The assumption SA5 implies that, for every ϵ > 0,
P({ω ∈ Ω : sup ||M(0)(s,ω)−M(0)(t,ω)|| ≥ ϵ}) → 0 as n → +∞. (B.3)
|t−s|≤T,t,s≥n
The following stronger result holds.
Lemma B.2 (Kushner and Clarke (1978), Lemma 2.2.1). Under SA1 and (B.3), for almost all ω,
M(0)(·,ω) is uniformly continuous on [0,+∞), and, for every T < +∞,
sup ||M(0)(t+s,ω)−M(0)(t,ω)|| → 0, (B.4)
|s|≤T
as t → +∞. If in addition SA5 holds, then
(cid:80)n
a ξ converges a.s.
i=0 i i
22We now define the left shifts of X(n), B(n) and M(n) on (−∞,+∞):
(cid:26) X(0)(t+t ,ω) t ≥ −t
X(n)(t,ω) = n n
X (ω) t < −t ,
0 n
(cid:26) B(0)(t+t ,ω)−B(0)(t ,ω) t ≥ −t
B(n)(t,ω) = n n n
−B(0)(t ,ω) t < −t ,
n n
(cid:26) M(0)(t+t ,ω)−M(0)(t ,ω) t ≥ −t
M(n)(t,ω) = n n n
−M(0)(t ,ω) t < −t .
n n
Beforestatingthemainresultofthissection,weremindthedefinitionofstabilityofthesolutions
of differential equations. For any vector x˜ and any ϵ > 0, let N (x˜) = {x ∈ Rr : ||x−x˜|| < ϵ}.
ϵ
Denote by x(t,x ) a solution of x˙ = h(x) satisfying x(0,x ) = x .
0 0 0
Definition B.3. A vector x is a locally asymptotically stable solution of a differential equation
∗
x˙ = h(x) in a set D if every solution x(t,y) → x as t → ∞ for every y ∈ D, and for each ϵ > 0
∗
there exists δ > 0 such that x ∈ N (x ) implies that x(t,x ) ∈ N (x ) for every t > 0. The set D
0 δ ∗ 0 ϵ ∗
is called the domain of attraction of x and denoted by DA(x ).
∗ ∗
Theorem B.4. Let (X ) be defined as in (B.1), and let SA1-SA5 hold. Then, there exists a null
n
set N such that:
(i) For every ω ∈ Nc, (X(n)(·,ω)) is equicontinuous, and the limit of any convergent subsequence
of (X(n)(·,ω)) satisfies the ordinary differential equation
x˙ = h (x), (B.5)
ω
with x ∈ F.
(ii) If x is a locally asymptotically stable solution to (B.5) with domain of attraction DA(x )
ω ω
and if there exists a compact set A ⊂ DA(x ) such that X (ω) ∈ A for infinitely many
ω ω n ω
n ≥ 0, then X (ω) → x , as n → ∞.
n ω
Proof. Let N be the null set where (X ) is not in F for all n, or (β ) does not converge to zero or
n n
(B.4) does not hold. Fix ω ∈ Nc.
Proof of (i). Equation (B.1) can be rewritten as
(cid:90) t
X(0)(t,ω) = X (ω)+ h (X(0) (s,ω))ds+M(0)(t,ω)+B(0)(t,ω).
0 ω
0
Then,
(cid:90) t
X(n)(t,ω) = X(n)(0,ω)+ h (X(0) (t +s,ω))ds+M(n)(t,ω)+B(n)(t,ω).
ω n
0
By SA2, X(0)(·,ω) is bounded on [0,+∞). Moreover, by SA4 and Lemma B.2, B(n)(·,ω) and
M(n)(·,ω) are uniformly continuous on (−∞,+∞), bounded on finite intervals in (−∞,+∞), and
tend to zero as n → +∞, uniformly on bounded intervals. Hence, X(n)(·,ω) is bounded and
equicontinuous on (−∞,+∞). Let (∆(n)(t,ω)) be such that
(cid:90) t (cid:90) t
h (X(0) (t +s,ω))ds = h (X(n)(s,ω))ds+∆(n)(t,ω).
ω n ω
0 0
23Then, ∆(n)(·,ω) → 0 as n → ∞ uniformly on finite intervals, and
X(n)(t,ω)
(cid:90) t
= X(n)(0,ω)+ h (X(n)(s,ω))ds+M(n)(t,ω)+B(n)(t,ω)+∆(n)(t,ω).
ω
0
By Ascoli-Arzel`a theorem (Dunford and Schwartz (1988, Theorem IV.6.7)), for each subsequence
X(n′)(·,ω) there exists a further subsequence (X(n′′)(·,ω)) converging uniformly on finite intervals
toacontinuousandboundedfunctionX(·,ω). Since∆(n′′)(·,ω)+M(n′′)(·,ω)+B(n′′)(·,ω)converges
to zero uniformly on bounded intervals, then
(cid:90) t
X(t,ω) = X(0,ω)+ h (X(s,ω))ds.
ω
0
It follows that X(·,ω) is a solution to (B.5).
Proof of (ii). Fix ω ∈ Nc and let ϵ > ϵ > 0 be arbitrarily small real numbers. Let A ⊂
2 1 ω
DA(x ) be a compact set and (n′) (depending on ω) be an increasing sequence in N such that
ω
A ⊃ N (x ) and X (ω) ∈ A for all n′. Let (X(n′′)(·,ω)) be a convergent subsequence of
ω ϵ2 ω n′ ω
(X(n′)(·,ω)) and let Xˆ(·,ω) be its limit. Then Xˆ(·,ω) satisfies (B.5) and, since A is compact,
ω
then Xˆ(0,ω) = lim X (ω) ∈ A . Since x is asymptotically stable in DA(x ), and since
n′′→+∞ n′′ ω ω ω
A ⊂ DA(x ), then Xˆ(t,ω) → x , as t → +∞. Since X(n′′)(·,ω) converges to Xˆ(·,ω) uniformly on
ω ω ω
bounded intervals as n′′ → +∞ and since lim lim X(0)(t+t ,ω) = x , then X (ω) =
t→+∞ n′′→+∞ n ω n
X(0)(t ,ω) ∈ N (x ) for infinitely many n. We now prove that X (ω) ∈ N (x ) ultimately. By
n ϵ1 ω n ϵ2 ω
contradiction, suppose that X (ω) ∈ N (x )c for infinitely many n. Since X (ω) = X(0)(t ,ω) ∈
n ϵ2 ω n n
A for infinitely many n, then there exists a sequence ((t (ω),t (ω))) such that t (ω) < t (ω) <
ω j j j j
t (ω) for every j ≥ 1, X(0)(t (ω),ω) ∈ ∂N (x ), X(0)(t (ω),ω) ∈ ∂N (x ) for every j ≥ 1 and
j+1 j ϵ1 ω j ϵ2 ω
X(0)(t,ω) ∈ N (x )\N (ω) for t ∈ [t (ω),t (ω)]. We now prove that this leads to a contradiction
ϵ2 ω ϵ1 j j
distinguishing two cases:
a) t (ω)−t (ω) → T(ω) < +∞ for some subsequence (j′);
j′ j′
b) t (ω)−t (ω) → +∞ as j → +∞.
j j
a) Take a convergent subsequence of the segment of X(0)(·,ω) corresponding to (t (ω),t (ω)), and
j′ j′
let X˜(·,ω) be its limit. Then X˜(·,ω) satisfies (B.5), X˜(0,ω) ∈ ∂N (x ), and X˜(T,ω) ∈ ∂N (x ).
ϵ1 ω ϵ2 ω
By the arbitrariness of ϵ and the stability of x this is impossible.
1 ω
b) The restrictions of X(0)(·,ω) to [t (ω),+∞) is bounded and equicontinuous. Let X˜(·,ω) be a
j
convergent subsequence. Then X˜(·,ω) satisfies (B.5), X˜(0,ω) ∈ ∂N (x ) and X˜(t,ω) ∈ N (x )\
ϵ1 ω ϵ2 ω
N (x ) for every t ≥ 0. This again contradicts the stability property of x .
ϵ1 ω ω
C Almost-sure conditional convergence
In this Section, we report some definitions and results, that are critical to Section 3. Given a
probability space (Ω,F,P) and a Polish space X with its Borel sigma-algebra X, a kernel on X is
a function K : Ω×X satisfying:
˙
(i) for every ω ∈ Ω, K(ω,) is a probability measure on X;
(ii) for each B ∈ X, the function K(·,B) is measurable with respect to F.
24AKernelK onRiscalledaGaussiankernelifK(ω,·)isaGaussiandistributionwithmeanµ(ω)and
varianceσ2(ω), whereµandσ2 arerandomvariablesdefinedon(Ω,F,P). WedenotetheGaussian
kernel by N(µ,σ2) and interpret the Gaussian distribution with zero variance as the degenerate
law centered on the mean. A kernel K on the Polish space C(R), with the topology of uniform
convergence on compact sets, is called a Gaussian process kernel if K(ω,·) is the distribution of
a Gaussian process in C(R), with mean function µ(ω) ∈ C(R) and covariance function R(ω) ∈
C(R×R). If µ(ω) = 0 for all ω, then the Gaussian process kernel is said to be centered.
We now extend the definition of almost sure conditional convergence, given by Crimaldi (2009)
for real sequences, to sequences taking values in Polish spaces.
Definition C.1. Let (X ) be a sequence of random variables adapted to a filtration (G ) and taking
n n
values in a Polish space X with its Borel sigma-algebra X. For every n ≥ 0, let K denote a regular
n
version of the conditional distribution of X , given G . If there exists a kernel K such that the
n+1 n
sequence (K (ω,·)) converges weakly to K(ω,·) for almost every ω ∈ Ω, then we say that the
n n
sequence (X ) converges to K in the sense of almost-sure conditional convergence.
n
Next results are the main tool in the proof of Theorem 3.1.
Theorem C.2 (Crimaldi (2009), Theorem A.1). For each n ≥ 1, let (M ) be a real valued
n,j j≥1
martingale with respect to the filtration (F ) , satisfying M = 0, and converging in L1 to a
n,j j≥1 n,0
random variable M . Set
n,∞
(cid:88)
X = M −M for j ≥ 1, U = X2 , X∗ = sup|X |.
n,j n,j n,j−1 n n,j n n,j
j≥1
j≥1
Assume that
(a) (X∗) is dominated in L1 and converges to zero a.s.
n n
(b) (U ) converges a.s. to a non-negative random variable U.
n n
Then the the sequence (M ) converges to the Gaussian kernel N(0,U) in the sense of almost-sure
n,∞
conditional convergence.
Remark C.3. Requesting that the almost sure limit of (U ) is non-negative guarantees the well-
n
definedness of the kernel N(0,U) for every ω ∈ Ω. Theorem A.1 in Crimaldi (2009) specifies that
U should be a “positive random variable”. Although not explicitly stated, this requirement should
be understood as U ≥ 0, as becomes evident upon a careful examination of the proof.
Theorem C.4 (Crimaldi et al. (2016), Lemma 4.1). Let (Z ) be a sequence of real valued random
n
variables adapted to the filtration (G ) and such that E(Z | G ) → Z a.s. for some random
n n+1 n
variable Z. Moreover, let (a ) and (b ) be sequences of real numbers such that
n n
(cid:88)∞ E(Z2)
b ↑ +∞, k < +∞.
n a2b2
k=1 k k
Then we have:
(a) If 1 (cid:80)n 1 → c for some constant c, then 1 (cid:80)n Z k → cZ a.s.
bn k=1 a
k
bn k=1 a
k
(b) If b (cid:80) 1 → c for some constant c, then b (cid:80) Z k → cZ a.s.
n k≥n a b2 n k≥n a b2
k k k k
25D A concentration inequality for Gaussian processes
Let (T,d) be a totally bounded pseudometric space, i.e. d satisfies the axioms of a metric except
that does not necessarily separate points. For every δ > 0, a δ-packing of T is a finite subset T of
0
T such that for every t ,t ∈ T , d(t ,t ) > δ. A δ-covering of T is a finite subset T of T whose
1 2 0 1 2 1
covering radius r satisfies r ≤ δ, where the covering radius of T is defined as
1
r = inf{s > 0 : for every t ∈ T there exists t ∈ T with d(t,t ) ≤ s}.
1 1 1
The maximal cardinality of a δ-packing of T is called the δ-packing number of T and denoted by
N(δ,T). The minimal cardinality of a δ-covering of T is called the δ-covering number of T, and
denoted by N′(δ,T). It follows from the definitions that
N′(δ,T) ≤ N(δ,T) ≤ N′(δ/2,T).
The δ-entropy number of T is defined as
H(δ,T) = log(N(δ,T)).
The function H(·,T) is called the metric entropy of (T,d). The covariance pseudometric of a
centered Gaussian process (X(t)) is defined as
t∈T
d(s,t) = E((X(s)−X(t))2)1/2 = (R(s,s)−2R(s,t)+R(t,t))1/2,
where R is the covariance function of (X(t)) .
t∈T
Theorem D.1 (Massart (2007), Theorem 3.18). Let (X(t)) be a centered Gaussian process and
t∈T
let d be the covariance pseudometric of (X(t)) . Assume that (T,d) is totally bounded and denote
t∈T
(cid:112)
by H(δ,T) the δ-entropy number of (T,d), for all positive δ. If H(·,T) is integrable at 0, then
(X(t)) admits a version which is almost surely uniformly continuous on (T,d). Moreover, if
t∈T
(X(t)) is almost surely continuous on (T,d), then
t∈T
(cid:90) σ
(cid:112)
E(supX(t)) ≤ 12 H(z,T)dz,
t∈T 0
where σ = (sup E(X2(t)))1/2.
t∈T
Thefollowingconcentrationinequalityallowstogiveanupperboundinprobabilityforsup X(t).
t∈T
Theorem D.2 (Massart (2007), Proposition 3.19). Let (X(t)) be an almost surely continuous
t∈T
centered Gaussian process on the totally bounded set (T,d), and let
(cid:18) (cid:19)1/2
σ = supE(X2(t)) .
t∈T
If σ > 0, then, for every z > 0,
√
P(supX(t) ≥ E(supX(t))+σ 2z) ≤ e−z.
t∈T t∈T
If σ = 0, then sup X(t) = E(sup X(t)) = 0. Hence the above inequality does not hold.
t∈T t∈T
However, in such a case, for every ϵ > 0, P(sup X(t) ≥ ϵ) = 0. Therefore, whatever σ ≥ 0, we
t∈T
can write that
√
P(supX(t) ≥ max(ϵ,E(supX(t))+σ 2z)) ≤ e−z,
t∈T t∈T
for every ϵ > 0 and z > 0.
26E Proofs: Newton’s algorithm for density deconvolu-
tion
E.1 Proof of Proposition 2.1
Define recursively the law of (X ) and (Z ) for n ≥ 1 by setting the conditional density of X ,
n n n+1
givenX ,Z ,Y asin(7), theconditionaldensityofZ , givenX ,Z ,Y , asf . Then
1:n 1:n 1,n n+1 1:n+1 1:n 1,n Z
define Y = X +Z and g˜ as in (3). Let (Ω,F,P) be a probability space on which all
n+1 n+1 n+1 n+1
the random variables (X ,Y ,Z ) are defined. Denoting by E the conditional expectation given
n n n n−1
(X ,Y ,Z ), we can write that
1:n−1 1:n−1 1:n−1
(cid:90)
E (eisYn) = φ (s) φ (s | θ)g˜ (θ)µ(dθ)
n−1 fZ k n−1
(cid:90)
= φ (s | θ)φ (s)g˜ (θ)µ(dθ)
k fZ n−1
(cid:90)
= φ (s | θ)g˜ (θ)µ(dθ),
k˜ n−1
where φ and φ (s | θ) denote the Fourier transforms of f and of k˜(· | θ) = k(· | θ) ∗ f ,
fZ k˜ Z Z
respectively. Hence (5) holds. Moreover, the conditional distribution of X , given X ,Z ,Y
n+1 1:n 1:n 1,n
is uniquely identified by
E (eisYn+1) = E (eisZn+1eisXn+1)
n n
= φ (s)E (eisXn+1).
fZ n
E.2 Proof of Theorem 2.2
The proof is based on Theorem B.4 on stochastic approximation with random limits. Let Θ =
{θ ,...,θ }. It is immediate to verify from the definition that g˜ is the a.s. limit of the bounded
1 d+1
martingale (g˜ ). Moreover, by assumption, g (θ ) > 0 for every i = 1,...,d + 1, and a simple
n 0 i
calculation shows that g (θ ) > 0 for every n ≥ 1 and every i = 1,...,d+1. The functions g˜, g˜
n i n
and g are densities with respect to the counting measure on Θ, then it is sufficient to show that
n
g (θ )−g˜ (θ ) converges to zero for every i = 1,...,d. We can view g˜ and g as d-dimensional
n i n i n n
vectors taking values in
d
(cid:88)
∆d = {(g ,...,g ) ∈ Rd : g ≥ 0, g ≤ 1}.
1 d i i
i=1
Moreover, since g˜ converges to g˜ a.s., the thesis holds if
n
g (θ ) → g˜(θ ) a.s. for every i = 1,...,d.
n i i
We denote by ◦ the coordinate-wise product in Rd+1 and introduce the notation
g˜ := g˜(ω), k˜ := k˜(y | ·), k := k(x | ·).
ω y x
Since
(cid:82)
k gdµ =
(cid:80)d
k (θ )g(θ )+k (θ
)(1−(cid:80)d
g(θ )) , then we can see the integral as a
Θ x i=1 x i i x d+1 j=1 j
function of g ∈ ∆d.
27By definition,
(cid:20) (cid:21)
g ◦k
g = g +α
n−1 Xn
−g . (E.6)
n n−1 n (cid:82) n−1
k g dµ
Θ Xn n−1
We can rewrite (E.6) as
g (ω) = g (ω)+α h (g (ω))+α ∆M (ω)+α u (ω), (E.7)
n n−1 n ω n−1 n n n n
where
(cid:90) (cid:20) (cid:82) k g˜ dµ (cid:21)
h (g (ω)) = g (ω)◦ Θ x ω −1 k λ(dx),
ω n−1 n−1 (cid:82) x
k g (ω)dµ
Θ x n−1
(cid:20) k (cid:90) (cid:82) k g˜ (ω)dµ (cid:21)
∆M (ω) = g (ω)◦ Xn − Θ x n−1 k λ(dx) ,
n n−1 (cid:82) (cid:82) x
k g (ω)dµ k g (ω)dµ
Θ Xn(ω) n−1 Θ x n−1
(cid:90) (cid:82) k (g˜ (ω)−g˜ )dµ
u (ω) = g (ω)◦ Θ x n−1 ω k λ(dx)
n n−1 (cid:82) x
k g (ω)dµ
Θ x n−1
We first show the validity of the assumptions of Theorem B.4 for the stochastic approximation
(E.7):
D1 The sequence of real numbers (α ) satisfies: α > 0 for every n ≥ 0, α → 0 as n → ∞, and
n n n
(cid:80)
α = +∞.
n n
D2 (g ) takes values in a compact convex set F, a.s.
n
D3 h (g) is measurable with respect to ω for every g ∈ F and continuous with respect to g ∈ F
ω
for almost all ω.
D4 (u ) is a bounded (with probability one) sequence of random vectors such that u → 0 a.s.
n n
D5 (∆M ) is a sequence of random vectors such that, for every ϵ > 0,
n
m
(cid:88)
lim P(sup || α ∆M || ≥ ϵ) = 0.
i i
n→∞ m≥n
i=n
D1 and D2 hold trivially in the present case, with F = ∆d. To prove D4, notice that
(cid:90) (cid:82) k (g˜ −g˜)dµ
Θ x n−1 k (θ)g (θ)λ(dx)
(cid:82) x n−1
k g dµ
Θ x n−1
(cid:90) (cid:90)
k (θ)g (θ) dµ
≤ sup|g˜ (θ′)−g˜(θ′)|µ(Θ) x n−1 k λ(dx)
n−1 (cid:82) x
k g dµ µ(Θ)
θ′ Θ x n−1 Θ
(cid:90) (cid:90)
dµ
≤ sup|g˜ (θ′)−g˜(θ′)|µ(Θ) k λ(dx)
n−1 x
µ(Θ)
θ′ Θ
≤ µ(Θ)sup|g˜ (θ′)−g˜(θ′)|.
n−1
θ′
Hence the sequence (u ) is bounded and converges to zero a.s. To prove D5, notice that (∆M ) is
n n
a martingale difference. Moreover, for every n ≥ 1, ∆M is a differences of densities, with respect
n
to µ. Therefore,
supE(cid:0)
||∆M
||2(cid:1)
< +∞. (E.8)
n
n
28By(E.8)andDoobinequality,D5holds. ThemeasurabilityconditioninD3holdstrivially. Toprove
that h is a continuous function of g in ∆d, we fix a sequence g∗ ∈ ∆d converging to g∗ ∈ ∆d and
ω n
prove that h(g∗) converges to h(g∗). At this aim, it is sufficient to show that h (g∗(θ)) converges
n ω n
to h (g∗(θ)) for every θ ∈ Θ. We can write that
ω
(cid:90) k g∗ (cid:90)
h (g∗) = x n k g˜ dµλ(dx)−g∗.
ω n (cid:82) k g∗dµ x ω n
Θ x n Θ
The functions k g∗/(cid:82) k g∗dµ can be seen as densities with respect to µ. Therefore, they are
uniformly boundx edn . MΘ orex ovn er, they converge to k g∗/(cid:82) k g∗dµ. On the other hand, the function
(cid:82) k g˜ dµ is a density with respect to λ. By domx inatedΘ cox nvergence theorem, h(g∗(θ)) converges
Θ x ω n
to h(g∗(θ)) for every θ.
We can therefore apply Theorem B.4 to the sequence g . Consider the differential equations
n
g˙ = h (g), (E.9)
ω
with g ∈ ∆d. Notice that (cid:82) g dµ = 1 along any solution of (E.9) since
Θ t
(cid:90) (cid:90)
d
g dµ = h (g )dµ = 0.
t ω t
dt
Θ Θ
Let us define
(cid:90) (cid:18) (cid:19)
g˜
ω
↕ (g) = log g˜ dµ.
ω ω
g
Θ
We will prove that g˜ is the only asymptotically stable point of (E.9) in
ω
∆d = {(g ,...,g ) : ↕ (g) < +∞}
ω 1 d ω
(cid:32) d (cid:33)
(cid:88)
= int(∆d)∪ ∩ {g ∈ ∆d : g = 0}∩ {g ∈ ∆d : g = 1} ,
i:g˜(θi)=0 i g˜(θ d+1)=0 i
i=1
where int(∆d) denotes the interior of ∆d. The function ↕ satisfies ↕ (g) ≥ 0 for every g ∈ ∆d, and
ω ω
↕ (g) = 0 if and only if g = g˜ . The time derivative of ↕ along a solution g of (E.9) is equal to
ω ω ω t
(cid:90)
˙
↕ (g ) = (∇ ↕ (g ))g˙ dµ
ω t g ω t t
Θ
(cid:90)
= (∇ ↕ (g ))h (g )dµ
g ω t ω t
Θ
(cid:90) g˜ (cid:18)(cid:90) k g (cid:82) k g˜ dµ (cid:19)
= − ω x t Θ x ω dλ−g dµ
(cid:82) t
g k g dµ
Θ t Θ x t
(cid:90) (cid:82) k g˜ dµ(cid:82) k g˜ dµ
= − Θ x ω Θ x ω dλ+1
(cid:82)
k g dµ
Θ x t
(cid:18)(cid:90) (cid:82) k g dµ(cid:82) k g˜ dµ (cid:19)−1
≤ 1− Θ x t Θ x ω dλ = 0,
(cid:82)
k g˜ dµ
Θ x ω
where the equality holds if and only if
(cid:90) (cid:90)
k gdµ = k g˜dµ. (E.10)
x x
Θ Θ
29We are assuming that k identifies the mixture. Hence (E.10) holds if and only if g = g˜. We have
x t
thus proved that the function ↕ is decreasing along the solutions of (E.9). Hence, whatever the
ω
initial state g∗ ∈ ∆d, the solution of (E.9) lies in the subset of ∆d
0 ω ω
C = {g ∈ ∆d : ↕ ≤ ↕ (g) ≤ ↕ (g∗) < +∞},
0 ω ω 0
˙
for some ↕ ≥ 0. We now prove that ↕ = 0. Suppose by contradiction that ↕ > 0. Since ↕ is
0 0 0 ω
continuous and bounded away from zero on C, then there exists L > 0 such that
˙
sup↕ (g) = −L.
ω
g∈C
Then, for any t > 0,
(cid:90) t
˙
↕ (g ) = ↕ (g )+ ↕ (g )dt ≤ ↕ (g )−Lt,
ω t ω 0 ω t ω 0
0
which leads to a contradiction since ↕ (g ) − Lt < 0 for t > ↕ (g )/L. We have thus proved
ω 0 ω 0
that ↕ is decreasing along the solutions of (E.9), whatever the initial state g∗ ∈ ∆d, and that
ω 0 ω
lim ↕ (g ) = 0. We now show that this implies that g˜ is asymptotically stable with domain
t→+∞ ω t ω
of attraction ∆d. Let m = inf g˜ (θ). Since Θ is a finite set, then m > 0. Let ϵ ∈ (0,1)
ω ω θ∈Θ:g˜ω(θ)>0 ω ω
be a fixed number, and let δ = ϵ2m /2. If ||g∗−g˜ || < δ, then, for every θ such that g˜ (θ) > 0
ω 0 ω 2 ω
g∗(θ) ≥ g˜ (θ)−δ ≥ m −ϵ2m /2 ≥ m /2.
0 ω ω ω ω
Thus, |logg˜ (θ)−logg∗(θ)| ≤ 2 |g˜ (θ)−g∗(θ)|, which entails
ω 0 mω ω 0
(cid:90)
↕ (g∗) = (logg˜ −logg∗)g˜ dµ
ω 0 ω 0 ω
{θ:g˜ω(θ)>0}
(cid:90)
2
≤ |g˜ −g∗|g˜ dµ
m ω 0 ω
ω {θ:g˜ω(θ)>0}
2
≤ ||g∗−g˜|| < ϵ2.
m 0 2
ω
Since ↕ is decreasing along the solutions g of (E.9), then, for every t ≥ 0, ↕ (g ) < ϵ2. Hence
ω t ω t
(cid:90) (cid:16)√ (cid:112) (cid:17)2 (cid:90) (cid:16) (cid:112) (cid:17)
||g −g˜||2 ≤ g − g˜ dµ = 2 1− g˜ g dµ
t 2 t ω ω t
(cid:90) (cid:18) (cid:114) (cid:19) (cid:90) (cid:18)(cid:114) (cid:19)
g g
t t
= 2 1− g˜ dµ ≤ −2 log g˜ dµ
ω ω
g˜ g˜
ω ω
(cid:90) (cid:18) (cid:19)
g˜
= log ω g˜ dµ = ↕ (g ) < ϵ2.
ω ω t
g
t
It follows that, if the initial state g∗ of (E.9) satisfies ||g∗−g˜ || < δ, then ||g −g˜ )|| < ϵ for every
0 0 ω 2 t ω 2
t ≥ 0. Moreover, since lim ↕ (g ) = 0, then lim ||g −g˜ || = 0. This proves that g˜ is
t→+∞ ω t t→+∞ t ω 2 ω
the only asymptotically stable point for g˙ = h (g) in ∆d.
ω ω
We now prove that, for ω in a set with probability one, (↕ (g (ω))) is bounded, uniformly
ω n n
with respect to n, which implies that there exists a compact set A ⊂ ∆d such that g (ω) ∈ A
ω ω n ω
for infinitely many n. In turn, based on Theorem B.4, this implies that g˜ is the only limit point
ω
of g (ω). By assumption, ↕ (g ) < +∞ a.s. for every n ≥ 0. By contradiction, suppose that there
n ω n
30existsM withP(M) > 0suchthat↕ (g (ω)) → +∞foreveryω ∈ M. Thenliminf d(g (ω),g˜ ) >
ω n n n ω
0 for ω ∈ M, and E(↕ (g )) → ∞ as n → +∞. Let g∗ = δg +(1−δ)g be such that
ω n n n n+1
(cid:90)
↕ (g ) = ↕ (g )+ ∇ ↕ (g )(g −g )dµ
ω n+1 ω n g ω n n+1 n
Θ
(cid:90) (cid:18)(cid:90) (cid:19)
+ (g −g ) ∇2↕ (g∗))(g −g )dµ dµ
n+1 n g ω n n+1 n
Θ Θ
(cid:90) (cid:90)
g˜ g˜
= ↕ (g )− ω (g −g )dµ+ ω (g −g )2dµ
ω n g n+1 n (g∗)2 n+1 n
Θ n Θ n
(cid:90) (cid:90)
g˜ g˜ −g˜
n n ω
= ↕ (g )− (g −g )dµ+ (g −g )dµ
ω n n+1 n n+1 n
g g
Θ n Θ n
(cid:90)
g˜
+ ω (g −g )2dµ
(g∗)2 n+1 n
Θ n
Notice that
(cid:18)(cid:90) (cid:19) (cid:18)(cid:90) (cid:19)
g˜ g˜
n n
E (g −g )dµ = α E (h (g )+∆M +u )dµ ,
n+1 n n+1 ω n n+1 n+1
g g
Θ n Θ n
with
(cid:18)(cid:90) (cid:19) (cid:90) (cid:18) (cid:19)
g˜ g˜
n n
E ∆M dµ = E E (∆M ) dµ = 0,
n+1 n n+1
g g
Θ n Θ n
(cid:18)(cid:90) (cid:19) (cid:18)(cid:90) (cid:19)
g˜ g˜
n n
E u dµ = E E (u )dµ = 0,
n+1 n n+1
g g
Θ n Θ n
and
(cid:18)(cid:90) g˜ (cid:19) (cid:18)(cid:90) g˜ (cid:18)(cid:90) k g (cid:82) k g˜ dµ (cid:19) (cid:19)
liminfE n h (g )dµ = liminfE n x n Θ x ω dλ−g dµ
ω n (cid:82) n
n Θ g n n Θ g n Θk xg ndµ
(cid:18)(cid:90) (cid:82) k g˜ dµ(cid:82) k g˜ dµ (cid:19)
= liminfE Θ x n Θ x ω dλ−1
(cid:82)
n Θk xg ndµ
(cid:18) (cid:18)(cid:90) (cid:82) k g˜ dµ(cid:82) k g˜ dµ (cid:19)(cid:19)
= liminfE E Θ x n Θ x ω dλ−1
n (cid:82)
n Θk xg ndµ
(cid:18)(cid:90) (cid:82) k g˜ dµ (cid:90) (cid:19)
= liminfE Θ x n k g˜ dµdλ−1
(cid:82) x n
n Θk xg ndµ
Θ
(cid:32) (cid:18)(cid:90) (cid:82) k g dµ (cid:90) (cid:19)−1 (cid:33)
> liminfE Θ x n k g˜ dµdλ −1 = 0,
(cid:82) x n
n Θk xg˜ ndµ
Θ
where the strict inequality holds since, under the contradiction assumption, P(g ̸→ g˜) > 0. Thus
n
there exists a constant c > 0 such that
(cid:18) (cid:90) (cid:19)
g˜
n
E − (g −g )dµ < −cα
n+1 n n
g
Θ n
for n large enough. On the other hand, since sup
(cid:82) k(x|θ1)2
k(x | θ )λ(dx) < +∞, then
θ1,θ2,θ3 Θ k(x|θ2)2 3
sup
(cid:82) k(x|θ1)
k(x | θ )λ(dx) < +∞. Hence,
θ1,θ2,θ3 Θ k(x|θ2) 3
(cid:12) (cid:18)(cid:90) (cid:19)(cid:12)
(cid:12) g˜ n−g˜ ω (cid:12)
(cid:12)E (g n+1−g n)dµ (cid:12)
(cid:12) g (cid:12)
Θ n
31(cid:18) (cid:18)(cid:90) (cid:12)
k
(cid:12) (cid:19)(cid:19)
≤ α n+1E ||g˜ n−g˜|| ∞E n
(cid:12)
(cid:12)(cid:82)
Xn+1 −1(cid:12)
(cid:12)dµ
(cid:12) k g dµ (cid:12)
Θ Θ Xn+1 n
(cid:18) (cid:18)(cid:90) (cid:18)(cid:90) (cid:18)(cid:90) (cid:19) (cid:19) (cid:19)(cid:19)
k
x
≤ α E ||g˜ −g˜|| k g˜ dµ dµ λ(dx)+1
n+1 n ∞ (cid:82) x n
k g dµ
Θ Θ x n Θ
(cid:18) (cid:18)(cid:90) (cid:18)(cid:90) (cid:19) (cid:19)(cid:19)
k(x | θ )
1
≤ α E ||g˜ −g˜|| k(x | θ )g (θ )g˜ (θ )dµ(θ )dµ(θ )dµ(θ ) λ(dx)+1
n+1 n ∞ 3 n 2 n 3 1 2 3
k(x | θ )
Θ×Θ×Θ 2
(cid:32) (cid:33)
(cid:90)
k(x | θ )
1
≤ α E(||g˜ −g˜|| ) 1+µ(Θ) sup k(x | θ )λ(dx)
n+1 n ∞ 3
k(x | θ )
θ1,θ2,θ3 2
= o(α ),
n
and
(cid:18)(cid:90) (cid:19) (cid:18)(cid:90) (cid:19) (cid:18)(cid:90) (cid:19)
g˜ g˜ g˜
E ω (g −g )2dµ ≤ E ω (g −g )2dµ +E ω (g −g )2dµ
(g∗)2 n+1 n g2 n+1 n g2 n+1 n
Θ n Θ n Θ n+1
(cid:32) (cid:33)
(cid:90) (cid:18) k (cid:19)2
≤ α2 E g˜ Xn+1 −1 dµ
n+1 ω (cid:82) k g dµ
Θ Θ Xn+1 n
 (cid:18) (cid:19)2 
kXn+1
−1
+α2
E (cid:90)
g˜
(cid:82) kXn+1gndµ dµ

n+1  ω(cid:18) (cid:18) (cid:19)(cid:19)2 
 Θ
1+α
kXn+1
−1

n+1 (cid:82) kXn+1gndµ
(cid:32) (cid:32) (cid:33)(cid:33)
(cid:90) (cid:18) k (cid:19)2
≤ α2 E 1+E Xn+1 dµ
n+1 n (cid:82) k g dµ
Θ Θ Xn+1 n
(cid:32) (cid:32) (cid:33)(cid:33)
α2 (cid:90) (cid:18) k (cid:19)2
+ n+1 E 1+E Xn+1 dµ
(1−α )2 n (cid:82) k g dµ
n+1 Θ Θ Xn+1 n
α2 (cid:18) (cid:90) (cid:90) k(x | θ )2 (cid:19)
≤ 2 n+1 E 1+ 1 k(x | θ )g (θ )g˜ (θ )dµ(θ )dµ(theta )dµ(θ )λ(dx)
(1−α )2 k(x | θ )2 3 n 2 n 3 1 2 3
n+1 Θ×Θ×Θ 2
(cid:32) (cid:33)
α2 (cid:90) k(x | θ )2
≤ 2 n+1 1+µ(Θ) sup 1 k(x | θ )λ(dx)
(1−α )2 k(x | θ )2 3
n+1 θ1,θ2,θ3 2
= o(α ).
n
It follows that,
E(↕ (g )) = E(↕ (g ))
ω n+1 ω n
(cid:18)(cid:90) (cid:19) (cid:18)(cid:90) (cid:19)
g˜ g˜ −g˜
n n ω
−E (g −g )dµ +E (g −g )dµ
n+1 n n+1 n
g g
n n
(cid:18)(cid:90) (cid:19)
g˜
+E ω (g −g )2dµ
(g∗)2 n+1 n
n
≤ E(↕ (g )),
ω n
for n large enough, which contradicts E(↕ (g )) → ∞.
ω n
Remark E.1. The proof of Theorem 2.2 is done along the same lines of reasoning as the proof of
Martin and Ghosh (2008, Theorem 3.4), with some differences due to the randomness of the limit
32and to the more complex probability distribution of the X ’s. In Martin and Ghosh (2008), it is
n
proved that the true mixing density, say g , is the unique stable point, in the set {g : g ≪ g},
true true
of the ordinary differential equation
(cid:90) (cid:90)
k
x
g˙ = g◦ k g dµλ(dx)−g,
(cid:82) x true
k gdµ
Θ x Θ
derived from Newton’s algorithm, seen as a stochastic approximation. It is clear from the the
above equation that the set C = {g : g ̸≪ g} is stable for the differential equation. Hence,
true
in order to conclude, it is necessary to show that, no point g in C can be a limit point of the
stochastic approximation. Notice that the condition g (θ) > 0 for every θ implies that g ̸∈ C.
n n
However, in the ODE method, this is not sufficient to exclude that C contains limit points of the
stochastic approximation. In our context, to prove that g˜(ω) is the only limit point of the stochastic
approximation, we show that, under a suitable condition on k, P(↕ (g (ω)) → +∞) = 0.
ω n
F Proofs: asymptotic credible intervals and bands
F.1 Proof of Theorem 3.1
LemmaF.1. Letv(x)andv (x)bedefinedasin (15)and (16), respectively. Then(v )isuniformly
n n
bounded and, for every x ∈ R, v (x) is strictly positive and converges to v(x), P-a.s.
n
Proof. Let us denote by k (·) = k(x | ·) and k˜ (·) = k˜(y | ·). Since
x y
(cid:90) (cid:18)(cid:90) (cid:19)2(cid:18)(cid:90) (cid:19)
v (x) = k(x | θ)(g˜ (θ | y)−g˜ (θ))µ(dθ) k˜ g˜ dµ λ(dy),
n n n y n
Θ Θ
thenthesequence(v )isuniformlyboundedbyassumptionsA1, A2andA5. Toshowthatv (x) ̸=
n n
(cid:82)
0 P-a.s. we proceed by contradiction. If v (x) = 0, then k(x | θ)(g˜ (θ | y)−g˜ (θ))µ(dθ) = 0
n Θ n n
for λ-almost every y. By the assumption A4, the last equality entails g˜ (θ | y) = g˜ (θ) for every
n n
θ and λ-almost every y. On the other hand, by the assumptions A1, A2 and A5, g˜ (θ) ̸= 0 for
n
every θ, which implies k˜(· | θ) ̸= (cid:82) k˜(· | θ′)g˜ (θ′)µ(dθ′), leading to g˜ (· | y) ̸= g˜ (·) for y in a set of
n n n
λ-positive measure.
To prove that v (x) converges to v(x), P-a.s., we can write that
n
v (x)
n
(cid:90)
(cid:34)
(cid:90)
(cid:32)
k˜(y | θ)g˜ (θ)
(cid:33) (cid:35)2(cid:18)(cid:90)
(cid:19)
= k(x | θ) n −g˜ (θ) µ(dθ) k˜ g˜ dµ λ(dy)
(cid:82) k˜ g˜ dµ n y n
Θ Θ y n Θ
 
(cid:34) (cid:32) (cid:33) (cid:35)2
(cid:90) (cid:90) (cid:90) k˜(y | θ)g˜ (θ)
=  k(x | θ) (cid:82) k˜ g˜ n dµ −g˜ n(θ) µ(dθ) g˜ n(θ′)k˜(y | θ′)λ(dy)µ(dθ′).
Θ Θ Θ y n
Since k˜(y | θ)g˜ (θ)/(cid:82) k˜ g˜ dµ is a density with respect to the counting measure on Θ, then it is
n Θ y n
bounded. Hence, the function of (θ′,y)
(cid:34) (cid:32) (cid:33) (cid:35)2
(cid:90) k˜(y | θ)g˜ (θ)
k(x | θ) n −g˜ (θ) µ(dθ) g˜ (θ′)
(cid:82) k˜ g˜ dµ n n
Θ Θ y n
33is bounded and converges P-a.s. to
(cid:34) (cid:32) (cid:33) (cid:35)2
(cid:90) k˜(y | θ)g˜(θ)
k(x | θ) −g˜(θ) µ(dθ) g˜(θ′).
(cid:82) k˜ g˜dµ
Θ Θ y
Since k˜(y | θ′)λ(dy)µ(dθ′) is a finite measure, then, by dominated convergence theorem,
v (x)
n
 
(cid:34) (cid:32) (cid:33) (cid:35)2
(cid:90) (cid:90) (cid:90) k˜(y | θ)g˜(θ)
P −− →a.s.  k(x | θ) (cid:82) k˜ g˜dµ) −g˜(θ) µ(dθ) g˜(θ′)k˜(y | θ′)λ(dy)µ(dθ′)
Θ Θ Θ y
(cid:34) (cid:32) (cid:33) (cid:35)2
(cid:90) (cid:90) k˜(y | θ)g˜(θ) (cid:90)
= k(x | θ) −g˜(θ) µ(dθ) k˜(y | θ′)g˜(θ′)µ(dθ′)λ(dy)
(cid:82) k˜ g˜dµ
Θ Θ y Θ
(cid:90) (cid:20)(cid:90) (cid:21)2(cid:18)(cid:90) (cid:19)
= k(x | θ)(g˜(θ | y)−g˜(θ))µ(dθ) k˜ g˜dµ λ(dy)
y
Θ Θ
(cid:90) (cid:104) (cid:105)2
= f(X)(x | y)−f(X)(x) f(Y)(y)λ(dy) = v(x).
Proof of Theorem 3.1. The proof is based on Crimaldi (2009, Theorem A.1) (see Theorem C.2).
Fix x ∈ R and define, for each n ≥ 0,
F = G , F = G for j ≥ 1,
n,0 n n,j n+j−1
M = 0, M =
b1/2(f(X)(x)−f(X)
(x)) for j ≥ 1.
n,0 n,j n n n+j−1
For every n ≥ 0, the sequence (M ) is a bounded martingale with respect to the filtration
n,j j≥0
(F ) , converging in L1 to
n,j j≥0
M = b1/2(f(X)(x)−f(X)(x)).
n,∞ n n
Let
X = M −M =
b1/2(f(X) (x)−f(X)
(x)),
n,j n,j n,j−1 n n+j−1 n+j
X∗ = sup|X | = b1/2sup|f(X) (x)−f(X) (x)|,
n n,j n n+j n+j−1
j≥1 j≥1
and
∞
U = (cid:88) X2 = b (cid:88) (f(X) (x)−f(X) (x))2.
n n,j n k+1 k
j=0 k≥n
The thesis follows from Theorem C.2, if we can verify that
(a) the sequence (X∗) defined by X∗ = b1/2 sup |f(X) (x)−f(X) (x)| is dominated in L1 and
n n n k≥n k+1 k
converges to zero P-a.s. as n → +∞.
(b) U = b (cid:80) (f(X) (x)−f(X) (x))2 → v(x), P-a.s. as n → +∞.
n n k≥n k k−1
34Notice that
(cid:90)
(X) (X)
f (x)−f (x) = k(x | θ)(g˜ (θ)−g˜ (θ)µ(dθ)
k k−1 k k−1
(cid:32) (cid:33)
(cid:90) k˜(Y | θ)g˜ (θ)
k k−1
= α˜ k(x | θ) −g˜ (θ) µ(dθ)
k (cid:82) k˜(Y | θ′)g˜ (θ′)µ(dθ′) k−1
Θ k k−1
(cid:90)
= α˜ k(x | θ)(g˜ (θ | Y )−g˜ (θ))µ(dθ),
k k−1 k k−1
where g˜ (θ | Y ) := k˜(Y | θ)g˜ (θ)/(cid:82) k˜(Y | θ′)g˜ (θ′)µ(dθ′) is a density with respect to µ.
k−1 k k k−1 Θ k k−1
Thus,
(cid:90)
X∗ ≤ b1/2supα˜ k(x | θ)|g˜ (θ | Y )−g˜ (θ)|µ(dθ),
n n k k−1 k k−1
k≥n
is uniformly bounded and converges to zero P-a.s., since b sup α˜2 → 0 by assumption, and the
n k≥n k
integrand is bounded uniformly with respect to n and µ, which is a finite measure. To prove (b),
we apply Crimaldi et al. (2016, Lemma 4.1 (b)) (see Theorem C.4 (b)), with
Z = α˜−2(f(X)(x)−f(X) (x))2,
n n n n−1
and a = (b α )−2. We can write that
k k k
(cid:16) (cid:17)
E (Z ) = E α˜−2(f(X)(x)−f(X) (x))2
n−1 n n−1 n n n−1
(cid:90) (cid:20)(cid:90) (cid:21)2
(Y)
= k(x | θ)(g˜ (θ | y)−g˜ (θ))µ(dθ) f (y)λ(dy)
n−1 n−1 n−1
Θ
(cid:90) (cid:104) (cid:105)2
(X) (X) (Y)
= f (x | y)−f (x) f (y)λ(dy) = v (x).
n−1 n−1 n−1 n
Thus, by Lemma F.1,
E (Z ) → v(x), P −a.s.
n−1 n
Moreover, b ↑ +∞ by assumption, and
n
(cid:88)∞ E(Z k2)
=
(cid:88)∞ α˜4b2E(cid:18) (cid:104)
f(X)
(x | Y
)−f(X)
(x)(cid:105)4(cid:19)
< +∞.
a2b2 k k k−1 k k−1
k=1 k k k=1
Since, by assumption, b (cid:80) a−1b−2 → 1, then by Theorem C.4 (b),
n k≥n k k
b (cid:88) (f(X) (x)−f(X) (x))2 = b (cid:88) Z k → v(x), P −a.s.
n k k−1 n a b2
k≥n k≥n k k
It follows by Theorem C.2 that M = b1/2 (f(X) (x) − f(X)(x)) converges to N(0,v(x)) in the
n,∞ n n
sense of almost-sure conditional convergence.
Now suppose that α˜ = ( α )γ. With b = 2γ−1n2γ−1, the conditions (14) hold. Indeed, b ↑ +∞,
n α+n n α2γ n
(cid:88)∞ (cid:88)∞ α (2γ −1)2
α˜4b2 = ( )4γ n4γ−2 < +∞,
n n α+n α2γ
n=1 n=1
2γ −1
(cid:18)
α
(cid:19)2γ
b supα˜2 = n2γ−1 → 0,
n k α2γ α+n
k≥n
35and
(cid:88) 2γ −1
(cid:88)(cid:18)
α
(cid:19)2γ
b α˜2 = n2γ−1 → 1.
n k α2γ α+k
k≥n k≥n
F.2 Proof of Theorem 3.2
Since a stochastic process taking values in C(R), with the topology of uniform convergence on
compact sets, converges in distribution if and only if its restrictions to compact sets converge (see
e.g. Kallenberg (2002), Proposition 16.6), then, in this section, we consider, for each n ≥ 1, the
restriction of b1/2 (f(X) −f(X)) to a fixed compact interval I = [a,b]. With an abuse of notation,
n n
we use the same symbol b1/2 (f(X) −f(X)) irrespective of its domain. Let ν denote the (random)
n n n
conditionaldistributionofb1/2 (f(X) −f(X)),asanelementinC(I),givenG . Toprovethetheorem,
n n n
we need to show that ν is tight and that the conditional finite dimensional distributions of ν
n n
converge almost surely to those of a centered Gaussian process G with covariance function (21)
g˜
(see Theorem 7.1 in Billingsley (1999)).
Lemma F.2. Let ν denote the (random) conditional distribution of b1/2 (f(X) −f(X)), with (b )
n n n n
an increasing sequence of strictly positive numbers satisfying (14). Then (ν ) is P-a.s. tight.
n
Proof. By Billingsley (1999, Theorem 7.3), it is sufficient to show that there exists a set N with
P(N) = 0 such that for every ω ̸∈ N the following conditions hold:
(i) For each positive η, there exist an k = k(ω) > 0 and an n = n (ω) such that
0 0
sup P (b1/2(f(X)(a)−f(X)(a))| ≥ k)(ω) ≤ η. (F.11)
n n n
n≥n0
(ii) For every ϵ,η > 0 there exists δ = δ(ω) ∈ (0,1) and n = n (ω) such that
0 0
sup P (b1/2 sup |f(X)(s)−f(X)(s)−f(X)(t)+f(X)(t)| ≥ ϵ)(ω) ≤ η. (F.12)
n n n n
n≥n0 |t−s|<δ
The condition (F.11) holds, since b1/2 (f(X) (a) − f(X)(a)) converges in the sense of almost-sure
n n
conditional convergence. To prove (F.12), let c = sup ∂ k(x | θ). Then c < +∞, and
x,θ ∂x
(cid:32) (cid:33)
sup P b1/2 sup |f(X)(s)−f(X)(s)−f(X)(t)+f(X)(t)| > ϵ
n n n n
n≥n0t |t−s|≤δ
(cid:32) (cid:33)
(cid:90)
≤ sup P b1/2 sup |k(s | θ)−k(t | θ)||g˜ (θ)−g˜(θ)|µ(dθ) > ϵ
n n n
n≥n0 |t−s|≤δ Θ
(cid:18) (cid:90) (cid:19)
≤ cδ sup P b1/2 |g˜ (θ)−g˜(θ)|µ(dθ) > ϵ
n n n
n≥n0 Θ
(cid:18) (cid:90) (cid:19)
cδ
≤ sup E b1/2 |g˜ (θ)−g˜(θ)|µ(dθ)
ϵ n n n
n≥n0 Θ
cδµ(Θ)
(cid:18) (cid:18) (cid:90) µ(dθ)(cid:19)(cid:19)1/2
≤ sup E b (g˜ (θ)−g˜(θ))2 .
n n n
ϵ µ(Θ)
n≥n0 Θ
36Since Θ is finite, then (F.12) holds if, for every θ ∈ Θ,
limsupb E ((g˜ (θ)−g˜(θ))2) < +∞,
n n n
n
P-a.s. To prove it, we can write that
limsupb E ((g˜ (θ)−g˜(θ))2)
n n n
n
(cid:88)
= limsupb E ((g˜ (θ)−g˜ (θ))2)
n n k k+1
n
k≥n
(cid:88)
= limsupb α˜2E ((g˜ (θ | Y )−g˜ (θ))2),
n k n k k+1 k
n
k≥n
where g˜(θ | Y ) = k˜(Y | θ)g˜ (θ)/(cid:82) k˜(Y | θ′)g˜ (θ′)µ(dθ′). Since g˜ (θ) and g˜ (θ | Y )
k+1 K+1 k Θ K+1 k k k K+1
are bounded by one, and since b (cid:80) α˜2 → 1 by (14), then (F.12) holds. It follows that the
n k≥n k
sequence (ν ) is tight, on a set of probability one.
n
Lemma F.3. Let (b ) be an increasing sequence of strictly positive numbers satisfying (14). Then,
n
for every J ≥ 1, x ,...,x ∈ I and u ,...,u ∈ R, b1/2(cid:80)J u (f(X) (x )−f(X)(x )), converges
1 J 1 J n i=1 i n i i
in the sense of almost sure conditional convergence, to a Gaussian kernel, with zero mean and
variance
(cid:80)J
u u R(x ,x ), with R as in (21).
i,j=1 i j i j
Proof. Withoutlossofgenerality, wecanassumethat(cid:80)J u2 ≤ 1. TheproofisbasedonCrimaldi
j=1 i
(2009, Theorem A.1) (see Theorem C.2). Define, for each n ≥ 0,
F = G , F = G for j ≥ 1,
n,0 n n,j n+j−1
J
M = 0, M = b1/2(cid:88) u (f(X)(x )−f(X) (x )) for j ≥ 1.
n,0 n,j n i n i n+j−1 i
i=1
For every n ≥ 0, the sequence (M ) is a bounded martingale with respect to the filtration
n,j j≥0
(F ) , converging in L1 to
n,j j≥0
J
(cid:88)
M = b1/2 u (f(X)(x )−f(X)(x )).
n,∞ n i n i i
i=1
Let
J
X = M −M =
b1/2(cid:88)
u
(f(X)
(x
)−f(X)
(x )),
n,j n,j n,j−1 n i n+j−1 i n+j i
i=1
J
X∗ = sup|X | = b1/2sup|(cid:88) u (f(X) (x )−f(X) (x ))|,
n n,j n j n+j i n+j−1 i
j≥1 j≥1
i=1
and
∞ J
U = (cid:88) X2 = b (cid:88) ((cid:88) u (f(X) (x)−f(X) (x)))2.
n n,j n i k+1 k
j=0 k≥n i=1
The thesis follows from Theorem C.2, if we can verify that
37(a) the sequence (X∗) defined by X∗ = b1/2 sup |(cid:80)J u (f(X) (x )−f(X) (x )| is bounded in
n n n k≥n i=1 i k+1 i k i
L1 and converges to zero P-a.s. as n → +∞.
(b) U = b (cid:80) ((cid:80)J u (f(X) (x )−f(X) (x )))2 converges P-a.s., as n → +∞, to
n n k≥n i=1 i k i k−1 i
(cid:88)J (cid:90) (cid:16) (cid:17)(cid:16) (cid:17)
u u f(X)(x |y)−f(X)(x ) f(X)(x | y)−f(X)(x ) f(Y)(y)λ(dy)
i j i i j j
R
i,j=1
(cid:90) (cid:32) J (cid:33)2
(cid:88)
= u (f(X)(x |y)−f(X)(x )) f(Y)(y)λ(dy).
i i i
R
i=1
Notice that, for every i = 1,...,J,
(cid:90)
(X) (X)
f (x )−f (x ) = k(x | θ)(g˜ (θ)−g˜ (θ))µ(dθ)
k i k−1 i i k k−1
(cid:32) (cid:33)
(cid:90) k˜(Y | θ)g˜ (θ)
k k−1
= α˜ k(x | θ) −g˜ (θ) µ(dθ)
k i (cid:82) k˜(Y | θ′)g˜ (θ′)µ(dθ′) k−1
Θ k k−1
(cid:90)
= α˜ k(x | θ)(g˜ (θ | Y )−g˜ (θ))µ(dθ),
k i k−1 k k−1
where g˜ (θ | Y ) := k˜(Y | θ)g˜ (θ)/(cid:82) k˜(Y | θ′)g˜ (θ′)µ(dθ′) is a density with respect to µ.
k−1 k k k−1 Θ k k−1
Thus,
J (cid:90)
(cid:88)
X∗ ≤ b1/2supα˜ |u | k(x | θ)|g˜ (θ | Y )−g˜ (θ)|µ(dθ),
n n k i i k−1 k k−1
k≥n
i=1
is bounded and converges to zero P-a.s., since b sup α˜2 → 0 by assumption, and the integrand
n k≥n k
is bounded uniformly with respect to n and µ, which is a finite measure. To prove (b), we apply
Crimaldi et al. (2016, Lemma 4.1 (b)) (see Theorem C.4 (b)), with
(cid:32) J (cid:33)2
Z = α˜−2 (cid:88) u (f(X)(x )−f(X) (x )) .
n n i n i n−1 i
i=1
We can write that

(cid:32) J
(cid:33)2
E n−1(Z n) = E n−1α˜ n−2 (cid:88) u i(f n(X)(x i)−f n(X −1) (x i)) 
i=1
(cid:90) (cid:34) J (cid:90) (cid:35)2
(cid:88) (Y)
= u k(x | θ)(g˜ (θ | y)−g˜ (θ))µ(dθ) f (y)λ(dy)
i i n−1 n−1 n−1
Θ
i=1
J (cid:90)
a →.s. (cid:88) u u (f(X)(x | y)−f(X)(x ))(f(X)(x | y)−f(X)(x ))f(Y)(y)dy.
i j i i j j
i,j=1
Moreover, b ↑ +∞ by assumption, and, defining a = α˜−2b−2, we obtain
n n n n
(cid:88)∞ E a( 2Z b2k2)
=
(cid:88)∞
α˜ k4b2
kE
(cid:34) (cid:88)j
u i(f k(X −1) (x i | Y k)−f k(X −1) (x
i))(cid:35)4
 < +∞.
k=1 k k k=1 i=1
38Since, by assumption, b (cid:80) a−1b−2 → 1, then by Theorem C.4 (b),
n k≥n k k
J
b (cid:88) Z k = b (cid:88) ((cid:88) u (f(X) (x )−f(X) (x )))2
n a b2 n i k i k−1 i
k≥n k k k≥n i=1
J (cid:90)
a →.s. (cid:88) u u (f(X)(x | y)−f(X)(x ))(f(X)(x | y)−f(X)(x ))f(Y)(y)dy.
i j i i j j
i,j=1
It follows by Theorem C.2 that M = b1/2(cid:80)J u (f(X) (x) − f(X)(x)) converges in the sense
n,∞ n i=1 i n
of almost-sure conditional convergence to a Gaussian kernel with variance (cid:80)J u u (cid:82) (f(X)(x |
i,j=1 i j i
y)−f(X)(x ))(f(X)(x | y)−f(X)(x ))f(Y)(y)dy.
i j j
Proof of Theorem 3.2. The proof of Theorem 3.2 is a direct consequence of Lemma F.2, Lemma
F.3 and Theorem 7.1 in Billingsley (1999).
F.3 Proof of Theorem 3.3
Toprovethetheorem, wewillfirstprovideaboundforE(sup G (x))andthenuseconcentration
x∈I g˜
inequalities to bound G (x) uniformly with respect to x ∈ I
g˜
Lemma F.4. Let G be a centered Gaussian process with random covariance function (21). Then
g˜
(cid:90) σ(I)(cid:18) (cid:18)
λ(I)
(cid:19)(cid:19)1/2
E(supG (x) | g˜) ≤ 12 log 1+ dz,
g˜ 2ψ−1(z/2)
x∈I 0
where σ(I) and ψ are defined as in (22) and (23), respectively.
Proof. The proof is based on Massart (2007, Theorem 3.18) (see Theorem D.1). Consider the
pseudometric on I induced by R
d(x ,x ) = (R(x ,x )+R(x ,x )−2R(x ,x ))1/2
1 2 1 1 2 2 1 2
(cid:18)(cid:90) (cid:19)1/2
= (f(X)(x | y)−f(X)(x )−f(X)(x | y)+f(X)(x ))2f(Y)(y)λ(dy)
1 1 2 2
(cid:18)(cid:90) (cid:19)1/2
= (f(X)(x | y)−f(X)(x | y))2f(Y)(y)λ(dy)−(f(X)(x )−f(X)(x ))2 .
1 2 1 2
Let ψ and ψ−1 be defined as in (23) and (24). If |x −x | ≤ ψ−1(ϵ/2), then d(x ,x ) ≤ ϵ/2. Hence
1 2 1 2
a uniform grid on I of step 2ψ−1(ϵ/2) allows to build a ϵ/2-covering of I. It follows that
(cid:108) λ(I) (cid:109) λ(I)
N(ϵ,I) ≤ N′(ϵ/2,I) ≤ ≤ +1.
2ψ−1(ϵ/2) 2ψ−1(ϵ/2)
Notice that
(cid:18)(cid:90) (cid:19)1/2
d(x ,x ) ≤ (f(X)(x | y)−f(X)(x | y))2f(Y)(y)λ(dy)
1 2 1 2
(cid:18)(cid:90) (cid:90) (cid:19)1/2
= (k(x | θ)−k(x | θ))2g˜(θ | y)2µ(dθ)f(Y)(y)λ(dy)
1 2
Θ
39≤ k′(I)|x −x |,
1 2
with k′(I) defined as
k′(I) = sup |k′(x | θ)| < ∞.
x∈I,θ∈Θ
Hence ψ(z) ≤ k′(I)z, and
ϵ
ψ−1(ϵ/2) ≥ .
2k′(I)
It follows that
(cid:112) (cid:112) (cid:115) (cid:18) λ(I) (cid:19) (cid:114) λ(I)k′(I)
H(ϵ,I) = logN(ϵ,I) ≤ log 1+ ≤ ,
2ψ−1(ϵ/2) ϵ
(cid:112)
which implies that H(·,I) is integrable in 0. Moreover,
(cid:90) σ(I)
(cid:112)
E(supG (x) | g˜) ≤ 12 H(z,I)dz
g˜
x∈I 0
(cid:90) σ(I)
(cid:112)
≤ 12 log(N(z,I)dz
0
(cid:90) σ(I)(cid:18) (cid:18)
λ(I)
(cid:19)(cid:19)1/2
≤ 12 log 1+ dz.
2ψ−1(z/2)
0
Proof of Theorem 3.3. The proof is based on Lemma F.4 and on the following concentration in-
equality (see Theorem D.2): for every z > 0,
√
P(supG (x) ≥ E(supG (x) | g˜)+σ(I) 2z | g˜) ≤ e−z,
g˜ g˜
x∈I x∈I
where σ(I) is defined as in (22). Taking into account the symmetry of the distribution of G , we
g˜
can write that
√ √
P((supG (x) ≥ E(supG (x) | g)+σ(I) 2z)∪(inf G (x) ≤ −E(supG (x)|g)−σ(I) 2z)|g˜)
g˜ g˜ g˜ g˜
x∈I x∈I x∈I x∈I
≤ 2e−z.
Setting 2e−z = β, we obtain that
(cid:90) σ(I)(cid:18) (cid:18)
λ(I)
(cid:19)(cid:19)1/2
(cid:112)
P(sup|G (x)| < 12 log 1+ dz+σ(I) 2|log(β/2)| |g˜)
g˜ 2ψ−1(z/2)
x∈I 0
(cid:112)
≥ P(sup|G (x)| < E(supG (x)|g˜)+σ(I) 2|log(β/2)| |g˜)
g˜ g˜
x∈I x∈I
≥ 1−β.
40F.4 Proof of Theorem 3.4
The proof is split into several lemmas.
Lemma F.5. As n → ∞, σ (I) converges to σ(I), P-a.s.
n
Proof. We can write that
|σ (I)2−σ(I)2|
n
(cid:90) (cid:90)
≤ sup| (f(X)(x|y)−f(X)(x))2f(Y)(y)λ(dy)− (f(X)(x|y)−f(X)(x))2f(Y)(y)λ(dy)|
n n n
x∈I
(cid:90) (cid:90) (cid:16) (cid:17)
≤ supk2(x|θ) (g˜ (θ|y)−g˜ (θ))2f(Y)(y)−(g˜(θ|y)−g˜(θ))2f(Y)(y) µ(dθ)λ(dy)
n n n
Θ x∈I
≤ sup k2(x|θ)
x∈I,θ
(cid:90) (cid:90) (cid:90)
(cid:0) (g˜ (θ|y)−g˜ (θ))2g˜ (θ′)−(g˜(θ|y)−g˜(θ))2g˜(θ′)(cid:1) µ(dθ)k˜(y|θ′)λ(dy)µ(dθ′),
n n n
Θ Θ
which converges to zero P-a.s., since (g˜ (θ|y) − g˜ (θ))2g˜ (θ′) ≤ 1 and converge to (g˜(θ|y) −
n n n
g˜(θ))2g˜(θ′), P-a.s.
Lemma F.6. For every z ∈ [0,λ(I)], ψ (z) converges to ψ(z), P-a.s. as n → ∞,
n
Proof. Since f(X) (x), f(X) (x|y) and g˜ (θ) are uniformly bounded, and since k˜(y | θ)λ(dy)µ(dθ) is
n n n
a finite measure on R×Θ, then
(cid:90) (cid:16) (cid:17)2
f(X)(x | y)−f(X)(x | y) f(Y)(y)λ(dy)
n 1 n 2 n
(cid:90) (cid:90) (cid:16) (cid:17)2
= f(X)(x | y)−f(X)(x | y) g˜ (θ)k˜(y | θ)λ(dy)µ(dθ)
n 1 n 2 n
Θ
(cid:90) (cid:90) (cid:16) (cid:17)2
→ f(X)(x | y)−f(X)(x | y) g˜(θ)k˜(y | θ)λ(dy)µ(dθ),
1 2
Θ
P-a.s. as n → ∞ .
Lemma F.7. As n → ∞,
(cid:90) σn(I)(cid:18) (cid:18)
λ(I)
(cid:19)(cid:19)1/2 (cid:90) σ(I)(cid:18) (cid:18)
λ(I)
(cid:19)(cid:19)1/2
log 1+ dz → log 1+ dz
2ψ−1(z/2) 2ψ−1(z/2)
0 n 0
P-a.s.
Proof. For P-a.s. every ω ∈ Ω, the functions ψ (·)(ω) and ψ(·)(ω) are continuous and monotone
n
non decreasing on the closed interval [0,λ(I)]. Hence, by Lemma F.6, we can find a set N with
P(N) = 0 such that for every ω ∈ Nc, ψ (·)(ω) converges to ψ(·)(ω) in C([0,λ(I)]. In the rest of
n
the proof we restrict to Nc. It can be proved as in Lemma F.4 that
(cid:18) (cid:18)
λ(I)
(cid:19)(cid:19)1/2 (cid:18) (cid:18)
λ(I)
(cid:19)(cid:19)1/2
log 1+ and log 1+
2ψ−1(z/2) 2ψ−1(z/2)
n
are integrable in a right neighborhood of the origin. Moreover
(cid:90) σn(I)(cid:18) (cid:18)
λ(I)
(cid:19)(cid:19)1/2 (cid:90) ψn−1(σn(I)/2)(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2
log 1+ dz = 2 log 1+ dψ (t)
2ψ−1(z/2) 2t n
0 n 0
41and
(cid:90) σ(I)(cid:18) (cid:18)
λ(I)
(cid:19)(cid:19)1/2 (cid:90) ψ−1(σ(I)/2)(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2
log 1+ dz = 2 log 1+ dψ(t),
2ψ−1(z/2) 2t
0 0
where dψ and dψ denote the measure associated to the monotone nondecrasing and continuous
n
function ψ and ψ, respectively, and ψ−1 and ψ−1 are defined as in (24).
n n
Let u ,u ∈ [0,λ(I)] be such u ≤ u and ψ(u) = σ(I)/2 for every u ∈ [u ,u ]. If u < u
1 2 1 2 1 2 1
and u > u , then ψ(u) < σ(I)/2 and ψ(u) > σ(I)/2, which implies that ψ (u) < σ(I)/2 and
2 n
ψ (u) > σ(I)/2 for n large enough. Since ψ converges weakly to ψ, then, for n large enough,
n n
(cid:90) u(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2 (cid:90) ψn−1(σn(I)/2)(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2
log 1+ dψ(t) ≤ log 1+ dψ (t)
n
2t 2t
0 0
and
(cid:90) u(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2 (cid:90) ψn−1(σn(I)/2)(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2
log 1+ dψ(t) ≥ log 1+ dψ (t).
n
2t 2t
0 0
Hence, for every u < u ≤ u < u,
1 2
(cid:90) u(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2 (cid:90) ψn−1(σn(I)/2)(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2
log 1+ dψ(t) ≤ liminf log 1+ dψ (t)
n
2t n 2t
0 0
(cid:90) ψn−1(σn(I)/2)(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2 (cid:90) u(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2
≤ limsup log 1+ dψ(t) ≤ log 1+ dψ(t).
2t 2t
n 0 0
Since the integrals are continuous with respect the endpoints,
(cid:90) u1(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2 (cid:90) ψn−1(σn(I)/2)(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2
log 1+ dψ(t) ≤ liminf log 1+ dψ (t)
n
2t n 2t
0 0
(cid:90) ψn−1(σn(I)/2)(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2 (cid:90) u2(cid:18) (cid:18) λ(I)(cid:19)(cid:19)1/2
≤ limsup log 1+ dψ(t) ≤ log 1+ dψ(t).
2t 2t
n 0 0
Since ψ is constant over the interval [u ,u ], then the first and last terms of the above equation are
1 2
equal. Moreover, by (24) and continuity of ψ, u = ψ−1(σ(I)/2).
2
Proof of Theorem 3.4. The proof is an immediate consequence of Lemma F.5 and Lemma F.7.
F.5 Proof of Equation 27
Since v (I,β) is measurable with respect to Y , v (I,β) is measurable with respect to g˜, and
n 1:n g˜
v (I,β) converges to v (I,β) P-a.s., then, for every s ,s ∈ R,
n g˜ 1 2
E (exp(i(s v (I,β)+s supb1/2|f(X)(x)−f(X)(x)|)))
n 1 n 2 n n
x∈I
P −− →a.s. E(exp(i(s v (I,β)+s sup|G (x))|))|g˜).
1 g˜ 2 g˜
x∈I
Inotherwords,wehavethat(v (I,β),sup b1/2 |f(X) (x)−f(X)(x)|)convergesto(v (I,β),sup |G (x)|)
n x∈I n n g˜ x∈I g˜
in the sense of almost-sure conditional convergence. By Portmanteau theorem,
liminfP (sup|b1/2(f(X)(x)−f(X)(x))| < max(v (I,β),ϵ))
n n n n
n→∞ x∈I
≥ P(sup|G | < max(v (I,β),ϵ) | g˜)
g˜(ω) g˜
x∈I
≥ 1−β.
42G Additional numerical illustrations
G.1 Numerical illustrations: synthetic data, unimodal f (30)
X
Wepresentamoredetailedanalysisfortheestimationoff in(30), whichcomplementstheresults
X
reported in Figure 1. For the Laplace noise distribution, we expand the analysis of Section 3 by
considering larger values for the variance σ2 of the noise distribution, as well as larger values of the
l
sample size n. See Figure 5, Figure 6 and Figure 7. Further, we present the same analysis under
a Gaussian noise distribution with mean 0 and standard deviation σ = 0.25, 0.50, 12, 3, 4. See
g
Figure 8, Figure 9 and Figure 10 below.
G.1.1 Laplace noise distribution
Figure 5: Unimodal f displayed in (30) (dashed black) and estimate f(X) of f for n = 1000
X n X
(solid red), n = 2000 (solid green), n = 4000 (solid blue), n = 8000 (solid magenta): σ = 0.25 (A),
l
σ = 0.5 (B), σ = 1 (C), σ = 2 (D), σ = 3 (E), σ = 4 (F).
l l l l l
43Figure 6: Unimodal f in (30) (dashed black). Panel A and B: estimate f(X) for n = 1000 and
X n
σ = 0.5 (solid red), with 95%-level credible interval (dashdot red) (A) and band (dashdot red)
l
(X)
(B). Panel C and D: estimate f for n = 2000 and σ = 0.5 (solid green), with 95%-level credible
n l
(X)
interval (dashdot green) (C) and band (dashdot green) (D). Panel E and F: estimate f for
n
n = 4000 and σ = 0.5 (solid blue), with 95%-level credible interval (dashdot blue) (E) and band
l
(dashdot blue) (F).
G.1.2 Gaussian noise distribution
G.2 Numerical illustrations: synthetic data, bimodal f (31)
X
Wepresentamoredetailedanalysisfortheestimationoff in(31), whichcomplementstheresults
X
reported in Figure 2. For the Laplace noise distribution, we expand the analysis of Section 3 by
considering larger values for the variance σ2 of the noise distribution, as well as larger values of
l
the sample size n. See Figure 11, Figure 12 and Figure 13. Further, we present the same analysis
under a Gaussian noise distribution with mean 0 and standard deviation σ = 0.25, 0.50, 12, 3, 4.
g
See Figure 14, Figure 15 and Figure 16 below.
44Figure 7: Unimodal f in (30) (dashed black). Panel A and B: estimate f(X) for n = 1000 and
X n
σ = 1 (solid red), with 95%-level credible interval (dashdot red) (A) and band (dashdot red) (B).
l
(X)
PanelCandD:estimatef forn = 2000andσ = 1(solidgreen), with95%-levelcredibleinterval
n l
(X)
(dashdot green) (C) and band (dashdot green) (D). Panel E and F: estimate f for n = 4000 and
n
σ = 1 (solid blue), with 95%-level credible interval (dashdot blue) (E) and band (dashdot blue)
l
(F).
G.2.1 Laplace noise distribution
G.2.2 Gaussian noise distribution
G.3 Numerical illustrations: synthetic data, multimodal f
X
We complete our analysis by considering a multimodal f defined as the Gaussian mixture model
X
(29). In particular, for fixed n = 1000, 2000, 4000, 8000, we generate the synthetic signal compo-
nents (X ,...,X ) as i.i.d. with density function
1 n
f (x) = 0.1ϕ(x | −3, 0.2)+0.3ϕ(x | 0, 0.1) (G.13)
X
+0.2ϕ(x | 2, 0.1)+0.2ϕ(x | 1, 0.1)+0.2ϕ(x | 4, 0.05) x ∈ R,
and then we generate the noise components (Z ,...,Z ), independently of (X ,...,X ), as i.i.d.
1 n 1 n
with Laplace density function of mean 0 and standard deviation σ = 0.25, 0.50, 12, 3, 4. We also
l
45Figure 8: Unimodal f displayed in (30) (dashed black) and estimate f(X) of f for n = 1000
X n X
(solid red), n = 2000 (solid green), n = 4000 (solid blue), n = 8000 (solid magenta): σ = 0.25 (A),
g
σ = 0.5 (B), σ = 1 (C), σ = 2 (D), σ = 3 (E), σ = 4 (F).
g g g g g
consider generating (Z ,...,Z ), independently of (X ,...,X ), as i.i.d. with Gaussian density
1 n 1 n
function of mean 0 and standard deviation σ = 0.25, 0.50, 12, 3, 4. From (X ,...,X ) and
g 1 n
(Z ,...,Z ), we generate (Y ,...,Y ) as Y = X +Z , for i = 1,...,n. Under the Laplace noise
1 n 1 n i i i
distribution, the results are reported in Figure 17, Figure 18 and Figure 19, whereas under the
Gaussian noise distribution, the results are reported in Figure 20, Figure 21 and Figure 22.
G.3.1 Laplace noise distribution
G.3.2 Gaussian noise distribution
G.4 Numerical illustrations: real data
G.4.1 Shapley galaxy data: Gaussian noise distribution
We present the analysis under a Gaussian noise distribution with mean 0 and standard deviation
σ = 0.25, 0.50, 12, 3, 4. See Figure 23 and Figure 24 below for details.
g
46Figure 9: Unimodal f in (30) (dashed black). Panel A and B: estimate f(X) for n = 1000 and
X n
σ = 0.5 (solid red), with 95%-level credible interval (dashdot red) (A) and band (dashdot red)
g
(X)
(B). Panel C and D: estimate f for n = 2000 and σ = 0.5 (solid green), with 95%-level credible
n g
(X)
interval (dashdot green) (C) and band (dashdot green) (D). Panel E and F: estimate f for
n
n = 4000 and σ = 0.5 (solid blue), with 95%-level credible interval (dashdot blue) (E) and band
g
(dashdot blue) (F).
References
Billingsley, P. (1999). Convergence of Probability Measures. John Wiley, New York.
Crimaldi, I. (2009). An almost-sure conditional convergence result and an application to a gen-
eralized P´olya urn. International Mathematical Forum 23, 1139–1156.
Crimaldi, I., Dai Pra, P. and Minelli, I.G. (2016). Fluctuation theorems for synchronization
of interacting P´olya’s urns. Stochastic Processes and their Applications 126, 930–947.
Dunford, N. and Schwartz, J.T. (1988). Linear Operators, Part 1. John Wiley, New York.
Kallenberg, O. (1978). Foundations of modern probability. Springer-Verlag, New York.
47Figure 10: Unimodal f in (30) (dashed black). Panel A and B: estimate f(X) for n = 1000 and
X n
σ = 1 (solid red), with 95%-level credible interval (dashdot red) (A) and band (dashdot red) (B).
g
(X)
PanelCandD:estimatef forn = 2000andσ = 1(solidgreen),with95%-levelcredibleinterval
n g
(X)
(dashdot green) (C) and band (dashdot green) (D). Panel E and F: estimate f for n = 4000 and
n
σ = 1 (solid blue), with 95%-level credible interval (dashdot blue) (E) and band (dashdot blue)
g
(F).
Kushner, H.J. and Clarke, D.S. (1978). Stochastic Approximation Methods for Constrained
and Unconstrained Systems. Springer, New York.
Martin, R. and Ghosh, J.K. (2008). Stochastic approximation and Newton’s estimate of a
mixing distribution. Statistical Science 23, 365–382.
Massart, P. (2007) Concentration Inequalities and Model Selection. Lecture Notes in Mathemat-
ics, Springer-Verlag, Berlin.
48Figure 11: Bimodal f displayed in (31) (dashed black) and estimate f(X) of f for n = 1000
X n X
(solid red), n = 2000 (solid green), n = 4000 (solid blue), n = 8000 (solid magenta): σ = 0.25 (A),
l
σ = 0.5 (B), σ = 1 (C), σ = 2 (D), σ = 3 (E), σ = 4 (F).
l l l l l
49Figure 12: Bimodal f in (31) (black dashed). Panel A and B: estimate f(X) for n = 1000 and
X n
σ = 0.5 (solid red), with 95%-level credible interval (dashdot red) (A) and band (dashdot red)
l
(X)
(B). Panel C and D: estimate f for n = 2000 and σ = 0.5 (solid green), with 95%-level credible
n l
(X)
interval (dashdot green) (C) and band (dashdot green) (D). Panel E and F: estimate f for
n
n = 4000 and σ = 0.5 (solid blue), with 95%-level credible interval (dashdot blue) (E) and band
l
(dashdot blue) (F).
50Figure 13: Bimodal f in (31) (dashed black). Panel A and B: estimate f(X) for n = 1000 and
X n
σ = 1 (solid red), with 95%-level credible interval (dashdot red) (A) and band (dashdot red) (B).
l
(X)
PanelCandD:estimatef forn = 2000andσ = 1(solidgreen), with95%-levelcredibleinterval
n l
(X)
(dashdot green) (C) and band (dashdot green) (D). Panel E and F: estimate f for n = 4000 and
n
σ = 1 (solid blue), with 95%-level credible interval (dashdot blue) (E) and band (dashdot blue)
l
(F).
51Figure 14: Bimodal f displayed in (31) (dashed black) and estimate f(X) of f for n = 1000
X n X
(solid red), n = 2000 (solid green), n = 4000 (solid blue), n = 8000 (solid magenta): σ = 0.25 (A),
g
σ = 0.5 (B), σ = 1 (C), σ = 2 (D), σ = 3 (E), σ = 4 (F).
g g g g g
52Figure 15: Bimodal f in (31) (black dashed). Panel A and B: estimate f(X) for n = 1000 and
X n
σ = 0.5 (solid red), with 95%-level credible interval (dashdot red) (A) and band (dashdot red)
g
(X)
(B). Panel C and D: estimate f for n = 2000 and σ = 0.5 (solid green), with 95%-level credible
n g
(X)
interval (dashdot green) (C) and band (dashdot green) (D). Panel E and F: estimate f for
n
n = 4000 and σ = 0.5 (solid blue), with 95%-level credible interval (dashdot blue) (E) and band
g
(dashdot blue) (F).
53Figure 16: Bimodal f in (31) (dashed black). Panel A and B: estimate f(X) for n = 1000 and
X n
σ = 1 (solid red), with 95%-level credible interval (dashdot red) (A) and band (dashdot red) (B).
g
(X)
PanelCandD:estimatef forn = 2000andσ = 1(solidgreen),with95%-levelcredibleinterval
n g
(X)
(dashdot green) (C) and band (dashdot green) (D). Panel E and F: estimate f for n = 4000 and
n
σ = 1 (solid blue), with 95%-level credible interval (dashdot blue) (E) and band (dashdot blue)
g
(F).
54Figure17: Multimodalf displayedin(G.13)(dashedblack)andestimatef(X) off forn = 1000
X n X
(solid red), n = 2000 (solid green), n = 4000 (solid blue), n = 8000 (solid magenta): σ = 0.25 (A),
l
σ = 0.5 (B), σ = 1 (C), σ = 2 (D), σ = 3 (E), σ = 4 (F).
l l l l l
55Figure 18: Multimodal f in (G.13) (black dashed). Panel A and B: estimate f(X) for n = 1000
X n
and σ = 0.5 (solid red), with 95%-level credible interval (dashdot red) (A) and band (dashdot
l
(X)
red) (B). Panel C and D: estimate f for n = 2000 and σ = 0.5 (solid green), with 95%-level
n l
(X)
credible interval (dashdot green) (C) and band (dashdot green) (D). Panel E and F: estimate f
n
for n = 4000 and σ = 0.5 (solid blue), with 95%-level credible interval (dashdot blue) (E) and
l
band (dashdot blue) (F).
56Figure 19: Multimodal f in (G.13) (dashed black). Panel A and B: estimate f(X) for n = 1000
X n
and σ = 1 (solid red), with 95%-level credible interval (dashdot red) (A) and band (dashdot red)
l
(X)
(B). Panel C and D: estimate f for n = 2000 and σ = 1 (solid green), with 95%-level credible
n l
(X)
interval (dashdot green) (C) and band (dashdot green) (D). Panel E and F: estimate f for
n
n = 4000 and σ = 1 (solid blue), with 95%-level credible interval (dashdot blue) (E) and band
l
(dashdot blue) (F).
57Figure20: Multimodalf displayedin(G.13)(dashedblack)andestimatef(X) off forn = 1000
X n X
(solid red), n = 2000 (solid green), n = 4000 (solid blue), n = 8000 (solid magenta): σ = 0.25 (A),
g
σ = 0.5 (B), σ = 1 (C), σ = 2 (D), σ = 3 (E), σ = 4 (F).
g g g g g
58Figure 21: Multimodal f in (G.13) (black dashed). Panel A and B: estimate f(X) for n = 1000
X n
and σ = 0.5 (solid red), with 95%-level credible interval (dashdot red) (A) and band (dashdot
g
(X)
red) (B). Panel C and D: estimate f for n = 2000 and σ = 0.5 (solid green), with 95%-level
n g
(X)
credible interval (dashdot green) (C) and band (dashdot green) (D). Panel E and F: estimate f
n
for n = 4000 and σ = 0.5 (solid blue), with 95%-level credible interval (dashdot blue) (E) and
g
band (dashdot blue) (F).
59Figure 22: Multimodal f in (G.13) (dashed black). Panel A and B: estimate f(X) for n = 1000
X n
and σ = 1 (solid red), with 95%-level credible interval (dashdot red) (A) and band (dashdot red)
g
(X)
(B). Panel C and D: estimate f for n = 2000 and σ = 1 (solid green), with 95%-level credible
n g
(X)
interval (dashdot green) (C) and band (dashdot green) (D). Panel E and F: estimate f for
n
n = 4000 and σ = 1 (solid blue), with 95%-level credible interval (dashdot blue) (E) and band
g
(dashdot blue) (F).
60Figure 23: Shapley galaxy data. Panel A, B, C and D: estimate f(X) (solid red), estimate via
n
kernel deconvolution (solid green) and estimate via Bayesian nonparametric deconvolution (solid
blue), with σ = 0.5 (A), σ = 1 (B), σ = 2 (C) and σ = 3 (D).
g g g g
61Figure 24: Shapley galaxy data. Panel A, B, C and D: estimate f(X) (solid red) and 95%-level
n
credible bands (dashdot red), with σ = 0.5 (A), σ = 1 (B), σ = 2 (C) and σ = 3 (D).
g g g g
62