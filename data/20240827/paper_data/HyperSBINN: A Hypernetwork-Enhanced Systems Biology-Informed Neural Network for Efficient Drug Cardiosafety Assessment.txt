HyperSBINN: A Hypernetwork-Enhanced Systems
Biology-Informed Neural Network for Efficient Drug
Cardiosafety Assessment
Inass Soukarieh2,3, Gerhard Hessler1, Herv´e Minoux2,
Marcel Mohr1, Friedemann Schmidt1, Jan Wenzel1,
Pierre Barbillon3, Hugo Gangloff3, Pierre Gloaguen4,∗.
1 Sanofi, R&D Preclinical Safety, Industriepark Hoechst,
Frankfurt am Main, Germany
2 Sanofi, Digital R&D, 13 Quai Jules Guesde,
Vitry-Sur-Seine, France
3 Universit´e Paris-Saclay, AgroParisTech, INRAE,
UMR MIA Paris-Saclay, 91120, Palaiseau, France.
4 Universit´e Bretagne Sud, UMR CNRS 6205,
LMBA, F-56000 Vannes, France
∗pierre.gloaguen@univ-ubs.fr;
August 27, 2024
Abstract
Mathematical modeling in systems toxicology enables a comprehensive understanding
of the effects of pharmaceutical substances on cardiac health. However, the complexity of
these models limits their widespread application in early drug discovery. In this paper, we
introduce a novel approach to solving parameterized models of cardiac action potentials
by combining meta-learning techniques with Systems Biology-Informed Neural Networks
(SBINNs). The proposed method, HyperSBINN, effectively addresses the challenge of pre-
dictingtheeffectsofvariouscompoundsatdifferentconcentrationsoncardiacactionpoten-
tials, outperforming traditional differential equation solvers in speed. Our model efficiently
handles scenarios with limited data and complex parameterized differential equations. The
HyperSBINN model demonstrates robust performance in predicting APD90 values, indi-
cating its potential as a reliable tool for modeling cardiac electrophysiology and aiding in
preclinicaldrugdevelopment. Thisframeworkrepresentsanadvancementincomputational
modeling,offeringascalableandefficientsolutionforsimulatingandunderstandingcomplex
biological systems.
Keywords: System-Biology informed neural network, Hypernetwork and Cardiac action
potential.
1 Introduction
The heart is one of the most important target organs of human drug toxicity. Consequently,
modern drug discovery strives to screen out potential hazards early amongst multiple drug can-
1
4202
guA
62
]LM.tats[
1v66241.8042:viXradidates,beforeanoptimizeddrugcandidateisselectedandentersclinicaltrials. Toxicitytesting
hashistoricallyreliedonevaluationoftheadverseeffectsofdrugsindedicatedanimalstudiesto
predicthumansafetyofdrugexposure. Withtheimplementationofnovelinsilicopredictiveap-
proaches, animal-free and cost-effective approaches are increasingly being developed to augment
and ultimately replace animal experimentation (Jenkinson et al. (2020)). Amongst others, the
cardiac action potential (CAP) (Andr´as et al. (2021)) is considered as a key parameter to mon-
itor closely for undesired drug effects. CAP is a periodic and transient change of the membrane
potential of heart cells. It is instantiated by a group of specialized pacemaker cells, which have
automaticactionpotentialgenerationcapabilities. Thevoltagechangeiscreatedbytransportof
charged ions through ion channels within the membrane. In healthy and undisturbed condition,
the action potential passes along the cell membrane causing heart cells to contract periodically.
Upon exposure to certain drugs, the morphology and frequency of the cardiac action potential
can be disturbed, making this parameter an important marker for drug toxicity to the heart.
CAPistightlycontrolledbyproperfunctioningofmultipleionchannelsexpressedoncardiacmy-
ocytes. The ionic currents through channels maintain regular heart beat. Mathematical models
of cardiac electrophysiology usually start from the modeling of ionic current dynamics in single
cells, coupled with models for the propagation of an action potential in tissue, and ultimately
also mechanical parameters, such as contractility, cardiac output or blood pressure. The general
approach to cardiac electrophysiology applies a set of coupled differential equations (Trayanova
et al. (2024); Mayourian et al. (2018)). Drugs may have the potential to alter the ion fluxes
through undesired blocks of one or even multiple channels. These ”off-target” effects have been
found to be quite common, particularly with the human Ether-`a-go-go-Related Gene (hERG)
potassium channel( Lee et al. (2019)). Effects of drugs on ion channels can be mathematically
expressed by partial block of the channels, leading to a reduced or stopped ionic current, and
eventually implying alterations of the cardiac action potential(Gerlach (2001); Carmeliet and
Mubagwa (1998); Andr´as et al. (2021)).
CAP modeling often employs systems of ordinary (ODEs) or partial differential equations
(PDEs) such as the Hodgkin-Huxley equations or the Fitzhugh-Nagumo equations Corrias et al.
(2011); Williams and Mirams (2015). However, these models can be computationally expen-
sive, limiting their practical utility in the early drug development process where numerous drug
candidatesorevendrugcandidatelibrarieshavetobeassessedinparalleltoguidedrugdevelop-
mentoptimally. Toaddressthischallenge,weproposetestingPhysics-InformedNeuralNetworks
(PINNs)asaninnovativedeeplearningtoolforsolvingCAPmodel(Raissietal.(2019);Cuomo
et al. (2022); Krishnapriyan et al. (2021))). PINNs combine the precision of governing physical
laws with the efficiency of deep learning.
Classical PINNs solve differential equations by embedding physical laws into the neural net-
work framework through a specialized loss function. This integration allows PINNs to leverage
the strengths of machine learning, such as automatic differentiation and optimization, to solve
complex scientific and engineering problems Raissi et al. (2019). A significant advancement in
the application of PINNs is their use in handling stiff ODE systems, which are characterized by
solutions that can change rapidly and require careful numerical treatment(Yazdani et al. (2020);
Jietal.(2021);O’Learyetal.(2022). TraditionalnumericalmethodsforstiffODEs,likeimplicit
solvers, can be computationally expensive and challenging to implement. PINNs offer a promis-
ingalternativebydirectlyincorporatingthestiffnessofthesystemintothetrainingprocess,thus
providing a more efficient and scalable solution.
Recent work has started to explore the use of PINNs to analyze cardiac electrophysiology
models. Forinstance,SahliCostabaletal.(2020)employedPINNstosolvetheisotropicdiffusion
equation using in silico data, estimating action potential arrival times and conduction velocity
maps. Grandits et al. (2021) advanced this work by solving the anisotropic eikonal equation
2to learn about fiber orientations and conductivity tensors, though their study was based on
synthetic data and data from only one patient. In a closely related study, Herrero Martin et al.
(2022) applied PINNs with the monodomain equation to estimate electrophysiology parameters
from sparse transmembrane potential maps, utilizing the Aliev-Panfilov model as described by
Aliev and Panfilov (1996), which lacks biological mechanism. Most recently, Chiu et al. (2024)
proposed a novel PINNs framework to predict how anti-arrhythmic drugs modulate myocardial
electrophysiology parameters using in vitro optical mapping data and a biophysically detailed
Fenton-Karma model.
Though algorithmic progress in machine learning is undisputed, the acquisition of suitable
training data remains a challenge, leading to limited information available to inform models.
Data-driven machine learning techniques, such as deep neural networks, tend to fail under these
circumstances. Physics-Informed Machine Learning (PiML) addresses this by blending data-
driven machine learning with physics-based simulations. This hybrid approach is transforming
problem-solving in science, engineering, and medicine Alber et al. (2019). At the core of PiML
are PINNs, a novel ”meshfree” approach that leverages deep learning to solve PDEs commonly
used to describe various phenomena in science and engineering. PINNs incorporate physical
domain knowledge as soft constraints on an empirical loss function, optimized using existing
machine learning training methodologies.
However, standard PINNs typically solve for a single set of parametrizations, meaning they
areconfiguredtosolvethedifferentialequationswithspecificfixedparameters. Therefore, when
working in domains that require evaluating solutions across multiple parameter values, it is
essential to adjust the approach to accommodate varying parameters. Two primary strategies
for handling varying parametrizations in PINNs are prevalent:
1. Retrainingthenetwork: ThisinvolvestraininganewPINNforeachdistinctsetofparametriza-
tions, which is costly due to the time-intensive nature of training.
2. Including parameters as inputs: This approach increases the complexity of the PINN by
directly propagating parametrizations into the network, resulting in decreased operational
speed.
To solve this problem, this study introduces the use of HyperPINN in conjunction with
a highly validated mathematical CAP model presented by Mohr et al. (2022). HyperPINN,
developed by de Avila Belbute-Peres et al. (2021), adopts a meta-learning approach, utilizing
a separate network (hypernetwork) to understand how parameters influence the equation. This
allows the primary network responsible for solving the equation to remain simple and efficient.
In our specific case, the parametrization variables of the ODE systems pertain to the com-
pound’shalf-maximalinhibitoryconcentration(IC50)foraselectionoffourionicchannels: hERG
potassium channel, hKv4.3 potassium channel, hCav1.2 L-type calcium channel, and hNav1.5
sodium channel. Each set of these four parameters characterizes the inhibitory potency of an
individual compound on these selected channels. Given our objective of testing multiple com-
pounds simultaneously, we encounter challenges in parametrized physics-informed neural net-
works,suchassolvingcomplexdifferentialequationswithahighnumberofparameters,handling
high-dimensional parameters space, and ensuring accurate predictions across various parameter
combinations .
This paper presents results utilizing HyperPINN, referred to as HyperSBINN, integrating
hypernetworksandsystembiology-informedneuralnetworks(SBINNs). Ourapproachhighlights
the significance and effectiveness of employing hypernetworks to train the parameter space of
IC50 values. We compare our findings with other machine learning techniques, to demonstrate
the efficacy of HyperSBINN.
32 Cardiac action potential model
Our study focuses on the modelling of the cardiac action potential (CAP) under different drug
configurations. We focus on the model proposed by Mohr et al. (2022), which we briefly recall
here.
In our context, the effect of a drug on CAP is supposed to depend only on its concentration
(denoted D) and on the inhibitory effect of the drug on J ionic channels. The inhibitory effect
of the j-th (1⩽j ⩽J) ionic channel is denoted by IC50 . We denote X={D,IC50 ,...,IC50 }
j 1 J
the controlled characteristics of the drug. The CAP V(t,X) at time t under the configuration X
is related to J = 8 ionic currents (listed in the Appendix A), denoted by I (t,X) (1 ⩽ j ⩽ 8)
j
through the ordinary differential equation (ODE) 1
 
8
dV (cid:88)
dt
(t,X)=−I stim(t)+ I j(t,X) , (1)
j=1
where I (t) is a known externally applied stimulus current (see Appendix). The generic for-
stim
mulation of an ionic current I (t,X) from a channel j is based on Ohm’s law and can be stated
j
as
I (t,X)=G ·k (D,IC50 )·g (t,V(t,X))·(V(t,X)−E ), (2)
j j j j j j
where:
• E and G is the known reversal potential and the maximal conductance, respectively, for
j j
channel j;
• k (D,IC50 ) is a function describing the inhibition factor for channel j. This function is
j j
eitheralways1(noinhibition),ortakesthefollowingformforthefourionicchannelslisted
in the Appendix:
(cid:18)
D
(cid:19)−1
k (D,IC50 )= 1+ .
j j IC50
j
• g (t,V(t,X)) is the fraction of conducting channels, which depends on voltage and a set of
j
kinetic parameters. It depends on V(t,X) through ODEs (resulting in 13 ODEs, all listed
in the Appendix A).
The complexity of the proposed models lies in the expressions of V(t,X) and the different
g (t,V(t,X)), which requires solving a (nonlinear) system of 14 ODEs, whose solution has a
j
nonlinear dependence on X, to obtain a CAP curve for each drug configuration. We denote by
u(t,X) the function taking values in R14 which is the solution of our system of ODE. The next
section depicts the framework to obtain a fast approximation of this function for every t and X
without using an ODE solver for each configuration X.
3 System Biology-Informed Neural networks
3.1 Overview of Physics-Informed Neural Networks
Inthisfirstpart, wefocusontheapproximationofthesolutionforagivenX. Thusthisvariable
is omitted in the notation. Consider the problem of approximating a function u(t) ∈ Rd, for
1Thisistheclassicalequationforelectricalpropertiesofasinglecardiaccell,wherewesetthecellcapacitance
to1,asinMohretal.(2022).
4t∈[0,T] (where T is a fixed time horizon), such that u(t) is the solution of a nonlinear system
of differential equations, expressed as:
u(0) =u
0 (3)
du(t) =f(u(t)),
dt
where f(·) is a known non linear function and u is the known initial condition. We suppose
0
moreover that we have a set of measurements y := y ,...,y from the target function2 at
1:n 1 n
times t :=t ,...,t .
1:n 1 n
Physics-informed neural networks (PINNs, Raissi et al., 2019), approximates the solution u
by a function v(t,θ), which is a neural network parameterized by a set of weights and biases
θ. Neural networks provide a flexible set of arbitrarily complex functions that performs well in
approximating non linear functions and whose derivatives (with respect to t or θ) can be easily
computedbyautomaticdifferentiationinstandardmachinelearningsoftwares. Foragivenneural
networkarchitecture3,theapproximationproblemboilsdowntolearnθ thatminimizesomeloss
function L(θ). In our context, a ”good” approximation is a function v(t,θ) such that:
• at observation times t ,...,t , the observations are well approximated, i.e., for all 1⩽i⩽
1 n
n, v(t ,θ)≈y .
i i
• the approximation satisfies the initial condition, i.e. v(0,θ)≈u .
0
• the approximation satisfies Equation (3), i.e., for all t∈[0,T],
dv
(t,θ)≈f(v(t,θ)).
dt
These three objectives result in the following loss for the PINN:
LPINN(θ)=wDataLData(θ)+wICLIC(θ)+wODELODE(θ), (4)
where wData,wIC and wODE are weights chosen by the user and:
n
LData(θ)= 1 (cid:88) ∥v(t ,θ)−y ∥2 , Data loss
n i i
i=1
LIC(θ)=∥v(0,θ)−u ∥2 , Initial condition loss
0
LODE(θ)= m1 (cid:88)m (cid:13) (cid:13) (cid:13) (cid:13)d dv t(τ j,θ)−f(v(τ j,θ))(cid:13) (cid:13) (cid:13) (cid:13)2 . ODE loss
j=1
The first two losses are mean squared errors at the observed values of the function. The third
loss, evaluates the adequacy of the derivative of the approximation to the ODE model, at times
τ ,...,τ that are chosen by the user. These times are the analogous of the discretization times
1 m
in a scheme use by a traditional ODE solver.
By optimizing θ (typically using gradient descent), one can therefore find a model that both
fits the data and satisfies the physics equation. However, so far, the training of θ must be
done separately for each drug configuration X. We now depict a unified framework for all drug
configurations.
NotethatthechoiceoftheweightswData,wIC andwODE willaffectthetraining. Thischoice
is model-driven and is discussed for our case study in Section 5.2. Note that we wrote Equation
(3) with a common weight for all ODEs, but it is straightforward to have different weights for
the different ODEs (which turned out to be necessary in our case-study).
2Still,inthissection,observationswouldcomefromasingledrugconfigurationX.
3Definedbythenumberoflayers, thenumberofneuronsperlayers, theconnectionsbetweenlayers, andthe
activationfunctions.
53.2 HyperPINN
We now come back to our original objective which is to learn the function u(t,X) given by the
CAP model of Section 2, for the drug configuration X.
OnceagoodPINNarchitecturehasbeenchosenanaturalapproachwouldbetolearnamap-
pingwhich,foraconfigurationX,givestheparametersθ(X)forwhichv(t,θ(X))isagoodapprox-
imationofu(t,θ). Thisapproach,namedhyperPINNs,wasintroducedbydeAvilaBelbute-Peres
etal.(2021). Itcombinesphysics-informedneuralnetworkarchitectureswithhypernetworks(Ha
et al., 2016). The main idea is then to build a neural network v (x,Γ), parameterized by
hyper
Γ, that takes as input a drug configuration and outputs a parameter θ which serves at the
weights for a neural network v (t,·). Formally, we approximate u(t,X) by setting, for every
PINN
(t,X)∈[0,T]×R5:
θ(X):=v (X,Γ) ,
hyper
uˆ(t,X):=v (t,θ(X)) .
PINN
ThelearningobjectivethenbecomestooptimizethelossinΓ. Supposedthatwehaveatdisposal
a set of q drug configurations X ,...,X , and that for each configuration, we have a specific set
1 q
of observations and times for the ODE loss (defined in Section 3.1). The loss for the hyperPINN
then becomes:
q
(cid:88)
LhyperPINN(Γ)= LPINN(v (X ,Γ)).
hyper i
i=1
where LPINN is defined in Equation (4). Figure 1 depicts graphically the framework of hyper-
PINNs.
ODE
Hypernetwork Gradient
Parameters Minimize
X
v hyper(X,Γ) update
LPINN(θ)
=
Output
θ(X) LIC(θ)
+
Input PINN Solutions Automatic
LODE(θ)
u 0,y 1:n,t 1:n v PINN(t,θ(X)) uˆ(t,X) Differentiation
+
LData(θ)
Figure1: Thetwo-stageneuralnetworkarchitectureofHyperPINNs. Thehypernetworktakesa
set of configuration X as input and outputs the weights and biases for the main PINNs network.
The PINNs network then takes a time coordinate as input and makes predictions based on the
problem defined by the hypernetwork output.
4 Experimental settings
In our approach, called HyperSBINN, we combine systems biology-informed neural networks,
defined in Yazdani et al. (2020), with hypernetworks to predict CAP at various concentrations.
6Our input X is a vector of size 5, stacking the IC50 values of four cardiac ion channel targets of
interest (hERG, hKv3.4, hCav1.2, and hNav1.5), and the drug concentration. The model then
outputs 14 curves, and in particular the curve of V(t), which is our main target.
We assess the performance of our approach by first comparing the curves outputted by the
HyperSBINN to the curves outputted by the solver and second, by computing a quantity of
interest (biomarker) from the voltage curve, namely APD90. Since this quantity of interest is
scalar, we can compare our HyperSBINN approach with standard machine learning techniques.
4.1 Assessing performance of the hyperSBINN model
We asses the performance of the hyperSBINN model to retrieve the solution of the ODE system
for different set of drug configurations when the train is performed using our different losses. As
observations, we provide outputs of a SBINN solver at random times (uniformly in [0, 500ms])
for random drug configurations. we uniformly randomly generated a total of 500 different drug
configurations. TheseconfigurationsincludeuniformlysimulatedIC50valuesforfourionicchan-
nels, ranging from 0.1µM to 100µM , and drug concentrations D varying between 0µM and
4µM.
4.2 Learning specific biomarker of the CAP
As mentioned previously, the curve V(t) is the target output. From this curve, we can extract
significant biomarker responses such as the APD50, the peak value, and others. An important
biomarkeristheAPD90response,whichisdefinedasthetimeittakesforthemembranepotential
to repolarize to 90% of its peak value during an action potential. Mathematically, it can be
expressed as:
APD90=t −t ,
90 peak
where t is the time at which the membrane potential V(t) repolarizes to 90% of its peak
90
value, and t is the time at which V(t) reaches its peak value. These values are obtained
peak
directly from the V(t) curves. To evaluate the effectiveness of the HyperSBINN model, we will
compare its predictions of APD90 values across a set of the same drug configurations as in the
previous part. The HyperSBINN model was used to predict the voltage curves V(t), from which
the APD90 values were subsequently extracted. The absolute difference with the numerical
solver (considered as the ground truth) is then computed for each configuration in the test set.
Additionally, we will compare the APD90 predicted by the HyperSBINN with those obtained
from a Random Forest model, a standard machine learning technique which has provided the
best performances in this case. While the methodologies differ, the comparison aims to evaluate
performanceofpredictingtheAPD90. TheRandomForestmodelwillbetrainedusingthesame
configuration set as the hyperSBINN (see Section 2), and will predict APD90 values directly,
bypassing the voltage curve V(t) and the CAP model.
5 Implementation details
In this section, we provide details of our implementation HyperSBINN. Python (3.10) was used
for all numeric computations. HyperSBINN model was designed, trained and deployed using
the Python library Jinns 4. Note that the training of the HyperSBINN is time demanding, its
evaluation (once trained) is fast.
4https://github.com/mia-jinns/jinns
75.1 HyperSBINN architecture and training
For the SBINN, a Multi-Layer Perceptron (MLP) architecture with one input the time space t
, five hidden layers with 50 neurons each, and an output layer of size 14. The hypernetwork
is also a MLP with input of size 5, and five hidden layers of 46 neurons. The output size
corresponds to the required number of parameters for the SBINN network. The choice of these
architectures was guided by different experimentations, where we tested different combinations
tofindtheoptimalconfigurationthatbalancescomputationalefficiencyandmodelperformance.
The specific architecture was selected based on its ability to capture the complexity of the CAP
model while maintaining manageable computational costs.
Regarding the activation function, we use the hyperbolic tangent activation function for all
layers except the last one. The output layer matches the possible values taken by the solutions
of the ODEs, which is [0, 1] for the 13 gate functions and ]−∞;+∞[ for the voltage func-
tion. The match is made by using sigmoid and linear activation functions respectively. For the
hypernetwork, we consistently use the hyperbolic tangent activation function.
For stochastic gradient descent, we used the Adamax optimizer (Kingma and Ba, 2014), a
variantoftheAdamoptimizer,withalearningratebeingconstantat10−4 for10,000iterations,
then decreasing linearly to 10−6 until iteration 50000, and being constant to this value for re-
mainingiterations. Thisoptimizerwaschosenduetoitssuitabilityforproblemswithpotentially
very large gradients, which can be common in deep learning tasks. Unlike Adam, which uses an
exponentially decaying average of squared gradients, Adamax utilizes the infinity norm (maxi-
mum absolute value) of past gradients. In our case, this optimizer was essential as it effectively
handledgradientsthatfluctuatesignificantly, leadingtomorestableandefficientlearninginour
model.
5.2 Losses weights
TheweightsdiscussedinSection3.1weremeticulouslyselectedtoensurebalancedcontributions
from all terms in the loss function. The weights wODE were chosen so that all terms in the
ODElossfunctioncontributeatasimilarscale, preventinganysingleODEfromdominatingthe
optimization process and ensuring the model learns from all aspects of this loss function. The
detailed method used for selecting these weights is provided in Appendix B.
The weight wIC for the initial condition loss term was set to 1 to prioritize satisfying the
initial points during training, ensuring the model adheres to the starting state of the system.
Additionally, the weights for the observational data loss wData were also set to 1. This config-
uration allows the model to learn from the physical dynamics and governing ODEs while still
incorporating observations, but without giving them undue influence over the training process.
5.3 Collocation points
To avoid overfitting to specific collocation points, the ODE loss is computed with batches of
500 times (in [0, 500]) which where uniformly sampled at each iteration. This sampling strategy
certainly avoids overfitting, but could be improved by sampling in times zones where the model
performs poorly.
For readers who want to reproduce our study, it is worth noting that a preprocessing step
is necessary to ensure efficient neural network training. We address this by employing a linear
scaling mechanism for the time variable t. This involves normalization by dividing t by the
maximum time domain value, T . This transformation t = t/T compresses the t scale to
max max
a more manageable range, typically around O(1), for optimal NN performance.
85.4 Observational Data points
As we stated in Section 4, we will add some observational data (y ) to the network. We derive
1:n
these observations by extracting configurations X from the hypernetwork, and then we used a
numerical solver to generate the solution of the CAP model given these configurations. For the
numerical solver, we use the same initial conditions provided to the HyperSBINN model and
described above.
Generated data are provided to the network as an input and used to minimise the data
discrepancy loss function, and introduced in the loss function l in Equation (4). From the
Data
extensive dataset, we randomly select 0.6 % data points of the total generated data, injecting
them into the hyperSBINN along with their corresponding time and configuration X. This
augmentation strategy enhances the robustness and generalization capabilities of our network
architecture.
6 Results
6.1 Performance of the hyperSBINN model
Thissectionpresentsaseriesofexperimentalresults. Figure2presentstheconvergencebehavior
of the loss components described in Section 3.1 during the training of the HyperSBINN model.
The graph demonstrates the training dynamics of the HyperSBINN, with a significant initial
drop in the dynamic loss (LODE) indicating rapid learning of the underlying ODEs. The steady
decrease and eventual stabilization of the initial condition loss and observation loss suggest
effective learning of initial conditions and a good fit to the observed data. One can notice
high variability of the ODE loss, which makes the learning highly sensitive to the learning rate
schedule. The training of the HyperSBINN takes around two hours but once it is trained, an
evaluation takes only a few seconds which is twenty times faster than using the ODE solver.
The results from the HyperSBINN model presented in 3 closely match those obtained from
the numerical solver across all variables, with minor differences observed in transition phases.
This close alignment indicates that the HyperSBINN effectively captures the system’s dynamics
andprovidescomparableperformancetothetraditionalsolver. Theslightdiscrepanciesinsignal
with low range of variations (such as h , h or h might be corrected by adjustment of
K-S Ca-L To-S
the different ODE weights.
Figure 3 also presents a comparative analysis between the traditional solver and the Hy-
perSBINN model for two sets of parameterizations over a time span of 500 milliseconds. The
HyperSBINN model demonstrates high accuracy in replicating the results from the traditional
solver across all variables. This can be seen in Table 1 which provides the normalized squared
differencesbetweencurvesoutputtedbythenumericalsolverandcurvesprovidedbytheHyper-
SBINN on a test set of drug configurations. The order of magnitude is low for a vast majority
of configurations. However, a few configurations (two over 500 in our test set) present a strong
discrepancy.
Wehavenoticedtheimportanceofobserveddata,asthetrainingwithouttheobservationloss
would lead, in most of the time to solution which are constant outside of the initial conditions
(not shown here). Therefore, to ensure the model captures the full range of possible solutions, it
is crucial to incorporate additional data points that represent the solutions of the ODE system
at different time points t, beyond just the initial conditions and ODE loss.
91e+05
LODE
LIC
LObs
LhyperSBINN
1e+02
1e-01
25000 50000 75000
Iteration
Figure 2: Convergence behavior of different loss components (in log scale) during the training of
HyperSBINN.Toeasetheread,lossvalueshavebeensmoothedusingamovingaverageover500
iterations. The learning is constant and equal to 10−4 until iteration 10000 (first dashed line),
then decreases linearly to 10−6 (second dashed line), and remains constant afterwards.
6.2 Learning specific biomarker of the CAP
Table2showstheabsoluteerrorofpredictionofAPD90whenthisquantityiscomputedusingthe
voltagecurveoutputbythehyperSBINN.Overall,thepredictionissatisfying,asthediscrepancy
between target and prediction is less than 3 ms for 75% of predictions.
Itisworthnotingthatifthespecificgoalisthepredictionofthisquantity,andiftheuserhas
atrainingsetofAPD90sfordifferentdrugconfigurations,aclassicalmachinelearningalgorithm
(thatfocusesonthisspecificquantity)mightbeabetteroption. Forinstance,Table2illustrates
thatadedicatedrandomforestperformsquitewellforthistask. Thisisnotasurpriseasaperfect
prediction of the APD90 from the voltage curves requires a perfect approximation of the lowest
value, the highest value, and the decreasing rate. A small error on those three quantities could
accumulate when predicting the APD90. Moreover, as shown before, the hyperSBINN model
is susceptible to extreme values in the error. Having a specific sampling design for problematic
drug configurations (instead of the uniform sampling chosen here) is certainly a lead for future
work.
10
eulav
ssoLCurve Min Q1 Median Mean Q3 Max
V(t) 0.01 0.07 0.16 1.71 0.59 161.49
f 0.02 0.05 0.10 1.21 0.42 173.76
Ca-L
h 0.08 0.23 0.54 2.36 1.41 178.18
Ca-L
f 0.19 0.27 0.46 4.61 1.61 436.78
Na-F
f 0.12 0.30 0.42 0.99 0.72 43.92
Na-L
h 0.00 0.03 0.10 1.46 0.55 163.84
Na-f
h 0.01 0.03 0.08 1.26 0.40 185.51
Na-L
f 0.03 0.05 0.10 1.04 0.30 228.93
To-F
f 0.07 0.11 0.15 1.25 0.35 250.48
To-S
h 0.11 0.16 0.20 1.24 0.41 185.34
To-F
h 7.40 13.36 14.67 17.33 16.83 219.86
To-S
f 0.04 0.09 0.15 1.40 0.40 183.74
K-R
f 0.10 0.23 0.47 1.83 1.17 193.82
K-S
h 0.92 1.55 1.84 3.37 2.97 144.10
K-S
Table 1: Normalized squared differences between curves outputted by the solver and curves
outputted by the SBINN. All metrics were multiplied by 1000, for instance, a value of 200
corresponds to a normalized squared difference of 0.2.
Method Min Q1 Median Mean Q3 Max
hyperSBINN 0.000 0.600 1.300 2.972 2.900 90.800
RF 0.000 0.071 0.189 0.627 0.454 25.613
Table 2: Absolute difference (in ms) between predicted and APD90 obtained by the python
solver. Predictions are made over 500 drug configurations.
7 Conclusion and perspectives
Inthiswork,weintroduceanewframeworkformodelingcardiacelectrophysiologyusingSystem
Biology-Informed Neural Networks (SBINNs) enhanced with hypernetworks. This approach ad-
dresses the limitations of traditional SBINNs by incorporating hypernetworks for parameterized
differential equations, providing a more robust and scalable model of cardiac action potentials.
Our HyperSBINN approach successfully integrates systems biology-informed neural networks
with the flexibility and efficiency of hypernetworks, enabling the simultaneous prediction of car-
diac action potentials across drugs at variable concentrations.
The complexity of our HyperSBINN model stems from various factors. The system involves
numerousinteractingequations,makingthetrainingprocesslongerandmoreresource-intensive.
Additionally, the model features a high number of non-fixed parameters (5 parameters that
represent the drug configuration X) with a large range of the IC50 values ([0.1−100]). This
significantly expands the search space, requiring the optimizer to explore extensively to find the
optimal solution, potentially leading to slow training times. The numerous factors mentioned
abovecontributetoasloweroptimizationprocess. Theoptimizermighttakealongtimetofinda
goodsolutionorriskgettingstuckinlocalminima. Withacomplexdifferentialequationsystem
and many parameters, there is a higher risk of the model overfitting to the training data. This
means the model might perform well on the training data but fail to generalize to unseen data.
The hyperparameters of the neural network become even more crucial for successful training.
Careful tuning is necessary to balance exploration and exploitation during optimization.
The results indicate that our model might achieve high accuracy in replicating results from
11V(t) fK−R
1.00
50 hyperSBINN 0.75
0 Solver 0.50
-50 0.25
0.00
0 100 200 300 400 500 0 100 200 300 400 500
hK−S fK−S
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
0 100 200 300 400 500 0 100 200 300 400 500
hCa−L fCa−L
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
0 100 200 300 400 500 0 100 200 300 400 500
hNa−F fNa−F
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
0 100 200 300 400 500 0 100 200 300 400 500
hNa−L fNa−L
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
0 100 200 300 400 500 0 100 200 300 400 500
hTo−F fTo−F
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
0 100 200 300 400 500 0 100 200 300 400 500
hTo−S fTo−S
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
0 100 200 300 400 500 0 100 200 300 400 500
Time (ms)
Figure 3: Comparison of the solver and hyperPINN curves for 2 different drug configurations
neverseenonthetrainingset(onecolorperconfiguration)foreach14solutionsofthebiological
system. Significance of all signals names is depicted in Appendix. Our main signal of interest is
V(t) (top left).
traditionalsolvers. Thisclosealignmentunderscoresthemodel’scapabilitytoeffectivelycapture
the dynamics of complex systems. Furthermore, the HyperSBINN model demonstrated robust
performanceinpredictingAPD90values,showingcomparableaccuracytoawell-tunedRandom
Forest model.
In our work, we leverage Data-driving learning alongside the HyperSBINN model for several
key reasons. First, incorporating observations, even a small portion, significantly improves the
accuracyandconvergence,especiallywhendealingwith1024differentcurvesolutionsduringthe
12
noitulos
s'EDO
fo
eulaVtraining. This is particularly beneficial for complex systems of Ordinary Differential Equations
(ODEs)wherethemodelcanencounterchallengesidentifyingthecorrectsolutionmanifoldfrom
scratch. Also, these observations act as guideposts, steering the HyperSBINN towards the phys-
ically relevant solutions within the vast solution space. Compared to literature findings where
10% to 20% of data might be used for a single curve solution, our approach shows remarkable
efficiency. Weachievecomparableaccuracyusingonly0.6%ofthetotalobservationsdatapoints
for1024drugsconfigurationsusedfortraining. Thisispossibleduetothemodel’sabilitytolearn
from limited data and generalize across different configurations. This efficiency is particularly
important when data acquisition is expensive.
Tothebestofourknowledge,thisisthefirstinstancewhereasystemsbiology-informedneural
network is combined with a hypernetwork for the study of cardiac action potentials. Looking
forward,furtherrefinementandvalidationoftheHyperSBINNmodelonmorespecificanddiverse
datasets are necessary to fully realize its potential. Expanding the framework with real clinical
and in vitro data will be crucial for translating the model advancements into pharmaceutical
applications. Additionally, improving the selection of observational data ( explained in Section
5.4), ensuring that data points are more precise and representative, will enhance the model
accuracy. Forinstance,incorporatingmoretargetedobservationsatcriticaltimepointsorunder
specific conditions could provide better guidance for the model. Furthermore, in our study,
we compared our results with the solutions of the ODEs derived from a numerical solver. It
would be interesting to also compare these results with real cardiac voltage curves and ionic
currentsderivedfrombiologicalassays. Includingrealobservationdataforthegatesandvoltages
would also be beneficial. Given that real data often contains noise, methodologies such as those
proposedintheworkLinkaetal.(2022). Additionally,itcanbeinterestingtoadaptivelymanage
collocation points based on model residuals, which could enhance the efficiency and accuracy of
the training process as inNguyen et al. (2023).
Overall,theframeworkproposedinthisworkrepresentsapromisingsteptowardmoreefficient
and reliable computational AI models in cardiac electrophysiology, potentially accelerating the
development of safer and more effective drugs.
Acknowledgement
We thank H. Matter and C. Grebner for thoughtful discussions at the beginning of this project.
Author Disclosure Statement
G.H., H.M., M.M., F.S. and J.W. are employees of Sanofi and may hold shares in the company.
Funding Information
This research was supported by Sanofi.
References
Alber, M., Buganza Tepole, A., Cannon, W. R., De, S., Dura-Bernal, S., Garikipati, K., Karni-
adakis, G., Lytton, W. W., Perdikaris, P., Petzold, L., etal. Integrating machine learning and
multiscalemodeling—perspectives,challenges,andopportunitiesinthebiological,biomedical,
and behavioral sciences. NPJ digital medicine, 2(1):115, 2019.
13Aliev, R. R. and Panfilov, A. V. A simple two-variable model of cardiac excitation. Chaos,
Solitons & Fractals, 7(3):293–301, 1996.
Andr´as, V., Tomek, J., Nagy, N., Vir´ag, L., Passini, E., Rodriguez, B., and Baczk´o, I. Cardiac
transmembrane ion channels and action potentials: cellular physiology and arrhythmogenic
behavior. Physiological reviews, 2021.
Carmeliet, E. and Mubagwa, K. Antiarrhythmic drugs and cardiac ion channels: mechanisms of
action. Progress in biophysics and molecular biology, 70(1):1–72, 1998.
Chiu, C.-E., Pinto, A. L., Chowdhury, R. A., Christensen, K., and Varela, M. Characterisation
of anti-arrhythmic drug effects on cardiac electrophysiology using physics-informed neural
networks. arXiv preprint arXiv:2403.08439, 2024.
Corrias,A.,Giles,W.,andRodriguez,B. Ionicmechanismsofelectrophysiologicalpropertiesand
repolarization abnormalities in rabbit purkinje fibers. American Journal of Physiology-Heart
and Circulatory Physiology, 300(5):H1806–H1813, 2011.
Cuomo, S., Di Cola, V., Giampaolo, F., and Rozza, G. Scientific machine learning through
physics-informed neural networks: Where we are and what’s next. Journal of Scientific Com-
puting, 92:1–27, 2022.
de Avila Belbute-Peres, F., Chen, Y.-f., and Sha, F. Hyperpinn: Learning parameterized differ-
ential equations with physics-informed hypernetworks. In The symbiosis of deep learning and
differential equations, 2021.
Gerlach, U. channel blockers: potential antiarrhythmic agents. Drugs of the Future, 26(5):
473–484, 2001.
Grandits, T., Pezzuto, S., Costabal, F. S., Perdikaris, P., Pock, T., Plank, G., and Krause,
R. Learning atrial fiber orientations and conductivity tensors from intracardiac maps using
physics-informed neural networks. In International Conference on Functional Imaging and
Modeling of the Heart, pages 650–658. Springer, 2021.
Ha, D., Dai, A., and Le, Q. V. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.
Herrero Martin, C., Oved, A., Chowdhury, R. A., Ullmann, E., Peters, N. S., Bharath, A. A.,
and Varela, M. Ep-pinns: Cardiac electrophysiology characterisation using physics-informed
neural networks. Frontiers in Cardiovascular Medicine, 8:768419, 2022.
Jenkinson, S., Schmidt, F., Ribeiro, L. R., Delaunois, A., and Valentin, J.-P. A practical guide
to secondary pharmacology in drug discovery. Journal of Pharmacological and Toxicological
Methods, 105:106869, 2020.
Ji, W., Qiu, W., Shi, Z., Pan, S., and Deng, S. Stiff-pinn: Physics-informed neural network for
stiff chemical kinetics. The Journal of Physical Chemistry A, 125(36):8098–8106, 2021.
Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Krishnapriyan, A., Gholami, A., Zhe, S., et al. Characterizing possible failure modes in physics-
informed neural networks. In NeurIPS, 2021.
14Lee,H.-M.,Yu,M.-S.,Kazmi,S.R.,Oh,S.Y.,Rhee,K.-H.,Bae,M.-A.,Lee,B.H.,Shin,D.-S.,
Oh, K.-S., Ceong, H., et al. Computational determination of herg-related cardiotoxicity of
drug candidates. BMC bioinformatics, 20:67–73, 2019.
Linka, K., Sch¨afer, A., Meng, X., Zou, Z., Karniadakis, G. E., and Kuhl, E. Bayesian physics
informed neural networks for real-world nonlinear dynamical systems. Computer Methods in
Applied Mechanics and Engineering, 402:115346, 2022.
Mayourian, J., Sobie, E. A., and Costa, K. D. An introduction to computational modeling
of cardiac electrophysiology and arrhythmogenicity. Experimental Models of Cardiovascular
Diseases: Methods and Protocols, pages 17–35, 2018.
Mohr, M., Chambard, J.-M., Ballet, V., and Schmidt, F. Accurate in silico simulation of the
rabbit purkinje fiber electrophysiological assay to facilitate early pharmaceutical cardiosafety
assessment: Dream or reality? Journal of Pharmacological and Toxicological Methods, 115:
107172, 2022.
Nguyen, T. N. K., Dairay, T., Meunier, R., Millet, C., and Mougeot, M. Fixed-budget on-
line adaptive learning for physics-informed neural networks. towards parameterized problem
inference. In International Conference on Computational Science, pages 453–468. Springer,
2023.
O’Leary, J., Paulson, J. A., and Mesbah, A. Stochastic physics-informed neural ordinary differ-
ential equations. Journal of Computational Physics, 468:111466, 2022.
Raissi, M., Perdikaris, P., and Karniadakis, G. E. Physics-informed neural networks: A deep
learning framework for solving forward and inverse problems involving nonlinear partial dif-
ferential equations. Journal of Computational physics, 378:686–707, 2019.
Sahli Costabal, F., Yang, Y., Perdikaris, P., Hurtado, D. E., and Kuhl, E. Physics-informed
neural networks for cardiac activation mapping. Frontiers in Physics, 8:42, 2020.
Trayanova, N. A., Lyon, A., Shade, J., and Heijman, J. Computational modeling of cardiac
electrophysiology and arrhythmogenesis: toward clinical translation. Physiological Reviews,
104(3):1265–1333, 2024.
Williams, G. and Mirams, G. R. A web portal for in-silico action potential predictions. Journal
of pharmacological and toxicological methods, 75:10–16, 2015.
Yazdani, A., Lu, L., Raissi, M., and Karniadakis, G. E. Systems biology informed deep learning
for inferring parameters and hidden dynamics. PLoS computational biology, 16(11):e1007575,
2020.
A Full cardiac action potential model
This section completes Section 2, we give the full equation details for the 8 considered ionic
channelsintheCAPmodel. AllconstantsinvolvedinthefollowingequationsarelistedonTable
3. In the following, we denote σ(z) the sigmoid function
σ : R → ]0;1[
z (cid:55)→ σ(z)= 1 .
1+e−z
To lighten the notation, the dependence on X of the voltage function is omitted (i.e. V(t) is
written instead of V(t,X).
15A.1 Channel directly impacted by the drug characteristics
We first depict the equations for the four currents whose conductance depend on the IC50 value
of the tested drug, i.e. currents for which the k (·,·) function is not equal to 1 in Equation (2).
j
A.1.1 Fast sodium current I
Na-F
(cid:18)
D
(cid:19)−1
I (t,X)=G × 1+ ×g (t)×(V(t)−E ),
Na-F Na-F IC50 Na-F Na-F
Na-F
where
g (t)=f (t)×h (t)
Na-F Na-F Na-F
with these two functions satisfying the following ODEs:
(cid:16) (cid:17)
σ V(t)+25 −f (t)
df Na-F(t)= 5 Na-F ,
dt 0.005
(cid:16) (cid:17)
σ −V(t)+69 −h (t)
dh Na-F(t)= 3.96 Na-F .
dt 2
A.1.2 L-type calcium current I
Ca
(cid:18)
D
(cid:19)−1
I (t,X)=G × 1+ ×g (t)×(V(t)−E ),
Ca Ca IC50 Ca Ca
Ca
where
g (t)=f (t)×h (t)
Ca Ca Ca
with these two functions satisfying the following ODEs:
(cid:16) (cid:17)
σ V(t)+14.6 −f (t)
df Ca(t)= 5.5 Ca ,
dt 0.7
(cid:16) (cid:17)
σ −V(t)+31 −h (t)
dh Ca(t)=(cid:16) 0.7e−0.0337×(V(t)+14.5)2 +0.04(cid:17)
×
5.54 Ca
.
dt 25.1
A.1.3 Fast transient outward potassium current I
TO-F
(cid:18)
D
(cid:19)−1
I (t,X)=G × 1+ ×g (t)×(V(t)−E ),
TO-F TO-F IC50 TO-F K
TO-F
where
g (t)=f (t)×h (t)
TO-F TO-F TO-F
16with these two functions satisfying the following ODEs:
(cid:16) (cid:17)
σ V(t)+3 −r (t)
df TO-F(t)= 15 TO-F ,
dt 3.5×e− 9V 02 0 +1.5
(cid:16) (cid:17)
σ −V(t)+33.5 −s (t)
dh TO-F(t)= 10 TO-F .
(cid:16) (cid:16) (cid:17) (cid:17)
dt 20× σ −V(t)+33.5 +1
10
A.1.4 Rapidly activating delayed rectifier potassium I
K-R
(cid:18)
D
(cid:19)−1
I (t,X)=G × 1+ ×g (t)×(V(t)−E ),
K-R K-R IC50 K-R K
K-R
where
(cid:18) (cid:19)
V +33
g (t)=f (t)×σ −
K-R K-R 22.4
with the function f satisfying the following ODE:
K-R
(cid:18) (cid:19)
df 0.00138×(V(t)+7) 0.00061×(V(t)+10)
K-R(t)= −
dt 1−e−0.123×(V(t)+7) 1+e0.145×(V(t)+10)
(cid:18) (cid:18) (cid:19) (cid:19)
V(t)+50
× σ −f (t) .
7.5 K-R
A.2 Channel indirectly impacted by the drug
A.2.1 Late sodium current I
Na-L
I (t,X)=G ×g (t)×(V(t)−E ),
Na-L Na-L Na-L Na-L
where
g (t)=f (t)×h (t)
Na-L Na-L Na-L
with these two functions satisfying the following ODEs:
(cid:16) (cid:17)
σ V(t)+30 −f (t)
df Na-L(t)= 5 Na-L ,
dt 15
(cid:16) (cid:17)
σ −V(t)+75.6 −h (t)
dh Na-L(t)= 6.3 Na-L .
dt 120+eV(t) 2+ 5100
17A.2.2 Sustained transient outward potassium current I
TO-S
I (t,X)=G ×g (t)×(V(t)−E ),
TO-S TO-S TO-S K
where
(cid:18) (cid:18) (cid:19)(cid:19)
1 V +33.5
g (t)=f (t)× h (t)+ σ −
TO-S TO-S TO-S 2 10
with these two functions satisfying the following ODEs:
(cid:16) (cid:17)
σ V(t)+3 −f (t)
df TO-S(t)= 15 TO-S ,
(cid:16) (cid:17)
dt 9×σ −V(t)+3 +0.5
15
(cid:16) (cid:17)
σ −V(t)+33.5 −h (t)
dh TO-S(t)= 10 TO-S .
(cid:16) (cid:17)
dt 3000×σ −V(t)+60 +30
10
A.2.3 Slowly activating delayed rectifier potassium Current I
K-L
I (t,X)=G ×g (t)×(V(t)−E ),
K-L K-L K-L K
where
g (t)=f (t)×h (t)
K-L K-L K-L
with these two functions satisfying the following ODEs
df (cid:18) 7.19·10−5×(V +30) 1.31·10−4×(V +30) (cid:19)
K-L(t)= +
dt 1−exp(−0.148·(V +30)) exp(0.0687·(V +30))−1
(cid:18) (cid:18) (cid:19) (cid:19)
V(t)−1.5
× σ −f (t) ,
16.7 K-L
dh 1 (cid:18) 7.19·10−5×(V +30) 1.31·10−4×(V +30) (cid:19)
K-L(t)= × +
dt 4 1−exp(−0.148·(V +30)) exp(0.0687·(V +30))−1
(cid:18) (cid:18) (cid:19) (cid:19)
V(t)−1.5
× σ −h (t) .
16.7 K-L
A.2.4 Inward rectifier current I
K-I
(cid:18)
D
(cid:19)−1
I (t,X)=G × 1+ ×g (t)×(V(t)−E ),
K-I K-I IC50 K-I K
K-I
where
(cid:18) (cid:19)
V(t)+92
g (t)=σ − .
K-I 10
18Constant Value Units
E 40 mV
Ca
E 74 mV
Na
E -85 mV
K
G 0.078 nS
Ca
G 0.03 nS
Nal
G 16.52 nS
Na-F
G 0.03 nS
Kr
G 0.1505 nS
Ks
G 0.29 nS
K1
G 0.06 nS
TO-F
G 0.02 nS
TO-S
Table 3: Constant values for the ionic currents models.
B ODE Weights wODE Initialization Method
Several methods exists for initializing weights in neural networks. To effectively balance the
contributions of each ODE in the loss function, we implement a specific weighting scheme. This
method ensures that each ODE contributes proportionately to the ODE loss, facilitating stable
and efficient training. Below is a step-by-step description of the method used:
• Initial Weight Setting: Begin by setting all weights to one. This provides a uniform
starting point for the weight adjustment process.
• Calculate initial ODE loss: Thenetworkisrunforasingleforwardpass,andtheinitial
ODE loss is computed. This is done by calculating the loss after zero epochs.
• Adjust weights for balance: Adjust the weights such that the contributions from each
ODE to the total loss are of the same order of magnitude.
Our system consists of 14 ODEs, each contributing differently to the ODE loss due to the
inherent differences in their dynamics and scales. Using the method described above, that aims
to address these disparities, the value of wODE found vary significantly in a range of 0.01 and
10,000. Withoutsuchweighting,certainODEswithnaturallylargermagnitudescoulddominate
the loss, leading to suboptimal training and convergence. The chosen weights reflect the relative
importance and scale of each ODE in the system. By balancing the contributions of each ODE,
thetrainingprocessbecomesmorestableandefficient. Thisbalanceensuresthattheoptimization
algorithmdoesnotfocusdisproportionatelyonanysingleODE,leadingtomoreuniformupdates
across the entire system.
19