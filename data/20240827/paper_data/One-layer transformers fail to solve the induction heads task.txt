One-layer transformers fail to solve the induction heads task
Clayton Sanford∗ Daniel Hsu† Matus Telgarsky‡
August 27, 2024
Abstract
A simple communication complexity argument proves that no one-layer transformer can
solve the induction heads task unless its size is exponentially larger than the size sufficient for
a two-layer transformer.
1 Introduction
The mechanistic interpretability studies of Elhage et al. (2021) and Olsson et al. (2022) identified
the ubiquity and importance of so-called “induction heads” in transformer-based language mod-
els (Vaswani et al., 2017; Radford et al., 2019; Brown et al., 2020). The basic task performed by
an induction head is as follows.
• The input is an n-tuple of tokens (σ ,...,σ ) from a finite alphabet Σ.
1 n
• The output is an n-tuple of tokens (τ ,...,τ ) from the augmented alphabet Σ∪{⊥}, where
1 n
the i-th output τ is equal to the inputtoken that immediately follows the rightmost previous
i
occurrence of the inputtoken σ , or ⊥ if there is nosuch previous occurrence. Thatis: τ = ⊥
i i
if σ 6= σ for all j < i, and otherwise τ = σ where j = max{j : j < i∧σ = σ }.
j i i ji+1 i j i
Sanford et al. (2024) call this the “1-hop induction heads task”; they also define and study gener-
alizations of the task with increasing difficulty, which they call “k-hop” (for k ∈ N). The special
case of 2-hop is related to the function composition task defined and studied by Peng et al. (2024).
Bietti et al. (2023) gave an explicit construction of a transformer for solving the (1-hop) induc-
tion heads task. Their construction is a two-layer transformer with a single attention head in each
layer. They also empirically found it difficult to train one-layer transformers to successfully solve
theinductionheadstaskunderacertaindatagenerationmodel,buttrainingtwo-layer transformers
was possible. Indeed, Elhage et al. (2021) noted: “In small two-layer attention-only transformers,
composition seems to be primarily used for one purpose: the creation of [...] induction heads.”
In this note, we prove that a one-layer transformer cannot solve the induction heads task unless
the size of the transformer is very large. By “size”, we mean the product of the number of self-
attention heads h, the embedding dimension m, and the number of bits of precision p used by the
transformer. By “very large”, we mean that hmp = Ω(n), where n is the size of the input. We
∗Columbia University,clayton@cs.columbia.edu.
†Columbia University,djhsu@cs.columbia.edu.
‡New York University,mjt10041@nyu.edu.
1
4202
guA
62
]GL.sc[
1v23341.8042:viXranote that when |Σ| ≤ n, there is a two-layer transformer that solves the induction heads task with
h = O(1), m = O(1), and p = O(log(n))(Bietti et al.,2023;Sanford et al.,2024). Soour size lower
boundfor one-layer transformers is exponentially larger than the size that is sufficient for two-layer
transformers.
The proof is based on a simple communication complexity argument. Lower bounds on the size
of one-layer transformers that solve related tasks were given by Sanford et al. (2023) using similar
arguments. Conditional lower bounds for the k-hop (for general k, mentioned above) were given
by Sanford et al. (2024) for Ω(logk)-layer transformers, assuming the 1-vs-2 cycle conjecture in the
Massively Parallel Computation model (Im et al., 2023). Peng et al. (2024) prove an average-case
lower bound for one-layer transformers to solve their function composition task (which resembles
the 2-hop), again using a communication complexity argument.
2 Transformer model
In this section, we give a generic definition of one-layer transformers that allows for general forms
of token embeddings and positional encodings. A self-attention head with embedding dimension
m ∈ N is a triple (Q,K,V) where Q: N×Rm → Rm, K: N×Rm → Rm, and V : N×Rm → Rm
are, respectively, called the query, key, and value embedding functions, and the first arguments to
Q,K,V enable the commonly-used positional encoding. The self-attention head defines a mapping
SA : Rn×m → Rn×m as follows. On input X = [X ,...,X ]T ∈ Rn×m, the output Y =
Q,K,V 1 n
[Y ,...,Y ]T = SA (X) ∈ Rn×m is defined by
1 n Q,K,V
n
Y = α V(j,X )
i i,j j
j=1
X
where (α ,...,α )= softmax(hQ(i,X ),K(1,X )i,...,hQ(i,X ),K(n,X )i)
i,1 i,n i 1 i n
(exp(hQ(i,X ),K(1,X )i),...,exp(hQ(i,X ),K(n,X )i))
i 1 i n
= .
n
exp(hQ(i,X ),K(j,X )i)
j=1 i j
A one-layer transformer with h self-attention hPeads and embedding dimension m is a collection of
h self-attention heads (Q ,K ,V )h each with embedding dimension m, together with an input
t t t t=1
encoding function ψ : N × Σ → Rm and an output decoding function ψ : N × Rm → Σ .
in in out out
Here, the input alphabet Σ and the output alphabet Σ are finite sets. The transformer defines
in out
a mapping TF ((Qt,Kt,Vt)h t=1,ψin,ψout): Σn in → Σn out as follows. On input σ = (σ 1,...,σ n) ∈ Σn in, the
output τ = (τ 1,...,τ n) = TF ((Qt,Kt,Vt)h t=1,ψin,ψout)(σ) ∈ Σn out is defined by
h
τ = ψ (i,ψ (i,X )+Z ) where [Z ,...,Z ]T = SA ([ψ (1,X ),...,ψ (n,X )]T).
i out in i i 1 n Qt,Kt,Vt in 1 in n
t=1
X
We say that a transformer uses p bits of precision if the outputs of all embedding functions (Q ,
t
K , V , ψ ) and the self-attention heads may be rounded to rational numbers with at most p bits
t t in
of precision without changing the behavior of the mapping TF ((Qt,Kt,Vt)h t=1,ψin,ψout).
23 Size of one-layer transformers for the induction heads task
Theorem 1. If a one-layer transformer with h self-attention heads, embedding dimension m, and p
bits of precision solves the induction heads task for input sequences of length n over a three-symbol
alphabet, then hmp = Ω(n).
Proof. Wegiveareductionfromtheone-waycommunicationproblemINDEX(Kushilevitz and Nisan,
1997, Example 4.19). In this problem, Alice is given a bit string f = (f ,...,f ) ∈ {0,1}k, and
1 k
Bob is given an index i∗ ∈ [k]. Alice can send a message to Bob, and after receiving it, Bob has
to output f i∗. By the pigeonhole principle, in any protocol for INDEX, Alice must send at least k
bits to Bob.
Supposethere is a one-layer transformer (with h self-attention heads and embedding dimension
m, using p bits of precision) that solves the induction heads task with a three-symbol alphabet
Σ = {0,1,?} (so Σ = Σ and Σ = Σ∪{⊥}). We show that it specifies a one-way communication
in out
protocol for INDEX. Consider the input n-tuple σ, with n = 2k+1, defined by
σ = (e ,f ,e ,f ,...,e ,f ,?) ∈ {0,1,?}2k+1
1 1 2 2 k k
where f ,...,f are taken from Alice’s input, and
1 k
? if i = i∗,
e =
i
(0 if i 6= i∗,
which is based on Bob’s input. The (2k+1)-th output of the transformer on input σ is f i∗, which
is exactly the correct output for INDEX. We show that Alice can send a message to Bob such that,
subsequently, Bob can compute the (2k+1)-th output of the transformer and hence determine f i∗.
Consider one of the self-attention heads (Q,K,V), and define Q˜ := Q◦ψ , K˜ = K ◦ψ , and
in in
V˜ := V ◦ψ . (We leave out the positional arguments in Q,K,V,ψ for brevity.) The (2k+1)-th
in in
output of SA ((ψ (σ ),...,ψ (σ ))) is
Q,K,V in 1 in n
k k
exp(Q˜(?)TK˜(e ))V˜(e )+ exp(Q˜(?)TK˜(f ))V˜(f )+exp(Q˜(?)TK˜(?))V˜(?)
i i i i
i=1 i=1
X X knowntoboth
knowntoBob knowntoAlice
Y = .
2k+1 | {z }
k k
| e{ xz p(Q˜(?)TK˜(e} ))| + exp(Q˜{ (z ?)TK˜(f ))+} exp(Q˜(?)TK˜(?))
i i
i=1 i=1
X X knowntoboth
knowntoBob knowntoAlice
| {z }
In order for Bob to compute Y to p bits of precision, it suffices for Alice to send the values
| 2{kz+1 } | {z }
k exp(Q˜(?)TK˜(f ))V˜(f ) k
i=1 i i ∈Rm and log exp(Q˜(?)TK˜(f )) ∈ R
P
k i=1exp(Q˜(?)TK˜(f i))
i=1
i !
X
roundedtoO(pP)bitsof precision toBob inamessageof size Cmpbitsfor someconstantC > 0(see
Appendix A for details). Such messages can be sent for all h self-attention heads simultaneously;
in total, Alice sends just Chmp bits to Bob, and after that, Bob computes the output of the
transformer and hence determines f i∗, thereby solving the INDEX problem. Since every protocol
for INDEX must require Alice to send at least k bits, we have
k n−1
hmp ≥ = = Ω(n).
C 2C
3References
Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a
transformer: A memory viewpoint. In Advances in Neural Information Processing Systems 36,
2023.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler,JeffreyWu,ClemensWinter,ChristopherHesse,MarkChen,EricSigler,MateuszLitwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
IlyaSutskever, andDarioAmodei. Languagemodelsarefew-shotlearners. InAdvances in Neural
Information Processing Systems 33, 2020.
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep
Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,
Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and
Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread,
2021. https://transformer-circuits.pub/2021/framework/index.html.
Sungjin Im, Ravi Kumar, Silvio Lattanzi, Benjamin Moseley, and Sergei Vassilvitskii. Massively
parallel computation: Algorithms and applications. Foundations and Trends® in Optimization,
5(4):340–417, 2023.
Eyal Kushilevitz andNoam Nisan. Communication Complexity. CambridgeUniversity Press, 1997.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,
Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli,
Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane
Lovitt, KamalNdousse,DarioAmodei,TomBrown,JackClark,JaredKaplan,SamMcCandlish,
and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022.
https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
Binghui Peng, Srini Narayanan, and Christos Papadimitriou. On limitations of the transformer
architecture. arXiv preprint arXiv:2402.08164, 2024.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. Technical report, OpenAI, 2019.
Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of
transformers. In Advances in Neural Information Processing Systems 36, 2023.
Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Transformers, parallel computation, and log-
arithmic depth. In Forty-First International Conference on Machine Learning, 2024.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
LukaszKaiser,andIlliaPolosukhin. Attention isallyouneed. InAdvances in Neural Information
Processing Systems 30, 2017.
4A Precision details
The j-th component of Y can be expressed as (A+B)/(Z +Z ) where
2k+1 A B
k k
A= exp((Q˜(?)TK˜(f ))V˜(f ) , B = exp(Q˜(?)TK˜(?))V˜(?) + exp(Q˜(?)TK˜(e ))V˜(e ) ,
i i j j i i j
i=1 i=1
X X
k k
Z = exp(Q˜(?)TK˜(f )), Z = exp(Q˜(?)TK˜(?))+ exp(Q˜(?)TK˜(e )).
A i B i
i=1 i=1
X X
Define r := A/Z and s := logZ . Alice’s message to Bob contains rˆ and sˆ, which are obtained
A A
by rounding r and s, respectively, to 3p bits of precision. Hence, rˆ and sˆ satisfy |r −rˆ| ≤ ǫ and
|s−sˆ| ≤ ǫ where ǫ := 2−3p. It suffices to show that Bob can approximate (A+B)/(Z +Z ) up
A B
to error 2−p using
rˆesˆ+B
,
esˆ+Z
B
which only depends on rˆ, sˆ, B, and Z . Observe that
B
rˆesˆ+B A+B esˆ es (rˆ−r)esˆ B es−esˆ
− = r − + + .
esˆ+Z Z +Z esˆ+Z es +Z esˆ+Z Z +Z esˆ+Z
B A B B B B A B B
(cid:18) (cid:19) (cid:18) (cid:19)
T1 T2 T3
Byassumptionontheembeddin| gfunctions{ ,z wemayass} um| ewi{ tz hou} tlos| sofgener{ az litythat|} V˜(σ ) |≤
i j
2p for all σ ∈ Σ. Therefore
i
|B| |B|
|r|≤ max|V˜(f ) |≤ 2p and ≤ ≤ max |V˜(?) |,max|V˜(e ) | ≤ 2p.
i j j i j
i∈[k] Z A+Z B Z B i∈[k]
(cid:26) (cid:27)
We now bound each of T , T , and T in magnitude. To bound |T |, we use the (1/4)-Lipschitzness
1 2 3 1
of the sigmoid function:
esˆ es |r||sˆ−s| 2pǫ
|T |= |r| − ≤ ≤ .
1 esˆ+Z es +Z 4 4
(cid:12) B B(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
To bound |T 2|: (cid:12) (cid:12)
esˆ
|T | = |rˆ−r| ≤|rˆ−r|≤ ǫ.
2 esˆ+Z
B
Finally, to bound |T |, we use the approximation et(et−1) ≤ 1.25t for t ∈ [0,1/8]:
3
|B| es−esˆ |B| Z e|sˆ−s|−1
|T | = ≤ · A · ≤ 2p·1.25ǫ.
3 Z +Z esˆ+Z Z +Z Z +Z e−|sˆ−s|
A B(cid:12) B(cid:12) A B A B
(cid:12) (cid:12)
(cid:12) (cid:12)
Therefore
(cid:12) (cid:12)
rˆesˆ+B A+B 3
− ≤ |T |+|T |+|T | ≤ ·2p+1 ǫ ≤ 2−p.
esˆ+Z Z +Z 1 2 3 2
(cid:12) B A B(cid:12) (cid:18) (cid:19)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
5