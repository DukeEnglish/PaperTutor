Sparsity-Aware Hardware-Software Co-Design of
Spiking Neural Networks: An Overview
Ilkin Aliyev∗, Kama Svoboda∗, Tosiron Adegbija∗, Jean-Marc Fellous†
∗Department of Electrical & Computer Engineering
†Departments of Psychology and Biomedical Engineering
University of Arizona, Tucson, AZ, USA
Email: {ilkina,ksvoboda,tosiron,fellous}@arizona.edu
Abstract—SpikingNeuralNetworks(SNNs)areinspiredbythe this translates to fewer necessary connections and routing
sparse and event-driven nature of biological neural processing, paths, which can simplify the accelerator’s physical layout
and offer the potential for ultra-low-power artificial intelligence.
and reduce the energy costs associated with data transfer and
However, realizing their efficiency benefits requires specialized
storage. Furthermore, the high reliability of digital hardware
hardware and a co-design approach that effectively leverages
sparsity. We explore the hardware-software co-design of sparse (comparedtobiologicalneurons)makesitpossibletoincrease
SNNs, examining how sparsity representation, hardware archi- the sparsity of the computations beyond what is observed in
tectures, and training techniques influence hardware efficiency. the brain to potentially achieve even more energy efficiency.
Weanalyzetheimpactofstaticanddynamicsparsity,discussthe
However, translating this theoretical efficiency into tangible
implications of different neuron models and encoding schemes,
gains on real-world hardware remains a critical challenge.
and investigate the need for adaptability in hardware designs.
Ourworkaimstoilluminatethepathtowardsembeddedneuro- Specialized hardware platforms that explicitly exploit the
morphicsystemsthatfullyexploitthecomputationaladvantages characteristicsofSNNworkloadsarenecessarytoreapthefull
of sparse SNNs. benefitsofsparseSNNcomputations[7],[8].Assuch,theco-
Index Terms—Spiking Neural Networks (SNNs), hardware- designofhardwareandsoftwareholdsthekeytounlockingthe
software co-design, sparsity, energy efficiency, event-driven pro-
energy-savingpotentialofSNNs.Algorithmsandmodelsmust
cessing
be tailored to work in synergy with hardware architectures
optimized to handle the unique computational characteristics
I. INTRODUCTION
of sparse SNNs.
Energy-efficient and high-performance computing architec- Key considerations in this co-design process involve how
tures have become more essential than ever in the era of per- sparsity is represented and how it interacts with the underly-
vasive machine learning (ML) and artificial intelligence (AI). ing hardware. Sparse operations in SNN computations often
Spiking Neural Networks (SNNs), which mimic the event- require different approaches for hardware acceleration than
drivencommunicationofbiologicalneurons,holdthepromise dense operations. Additionally, model configurations, such as
of surpassing the energy efficiency of conventional Artificial the synaptic connectivity patterns, neuron models, encoding
Neural Networks (ANNs) [1]. An important reason for the schemes, and the balance between different kinds of sparsity,
potential efficiency of SNNs is that they exploit the inherent including static and dynamic sparsity, can have profound
sparsity observed in biological neural systems, characterized impacts on hardware efficiency. For example, static sparsity,
by sparse coding [2], [3] and sparse connectivity [4], [5], and which refers to a fixed pattern of zero-valued weights in
computation using partial synchrony instead of firing rate [6]. the SNN model, allows for predetermined optimizations like
This sparsity translates directly into potential computational memory compression and skipping of computations with zero
savingsinhardwareimplementations,especiallyifthesparsity weights. On the other hand, dynamic sparsity, referring to
is explicitly exploited [7], [8]. the temporal event-based neuron activations, offers potential
In sparse coding, only a fraction of neurons are activated for further efficiency, but requires flexible hardware to handle
at a time. As a result, hardware designs that explicitly exploit variable, irregular, and unpredictable computational loads.
sparse coding in SNN models can conserve energy by pow- This paper provides an overview of the multifaceted field
ering down inactive neurons, thus only consuming power for of hardware-software co-design for sparse SNNs, emphasiz-
processing active signals on demand. This selective activation ing the critical role of sparsity in achieving energy-efficient
aligns well with event-driven processing, where computations neuromorphic computing. We investigate the dynamic na-
are performed only when events (spikes) occur, reducing the ture of sparsity, exploring its dependency on various factors
overall energy consumption. Moreover, sparse connectivity such as training hyperparameters, neuron models, and input
implies that each neuron is connected to only a subset of encoding methods. Through empirical analysis and detailed
other neurons, rather than a fully connected network. This exploration, we quantify the impact of these factors on
reduces the complexity of the inter-neuronal communication sparsity, offering valuable insights for optimizing SNNs for
infrastructure required. For an SNN hardware accelerator, hardware implementation. We also address the challenges
4202
guA
62
]RA.sc[
1v73441.8042:viXraSNNs for applications in sequence recognition, temporal pre-
x1 w1
diction, and adaptive behavior within dynamic environments.
x2 w2 Σ y
Furthermore, the inherent temporal sparsity of SNNs, where
xn w3 neurons only fire when necessary, contributes significantly to
their energy efficiency [15].
Fig. 1: SNN neurons integrate incoming spikes x with corre-
3) Learning in SNNs: Although backpropagation [16] has
spondingsynapticweightswtogenerateoutputspikesy every
become a workhorse for effectively training ANNs in prac-
time the integrated membrane potential reaches a threshold.
tice, training SNNs presents unique challenges due to the
non-differentiability of spike-based signals. Several training
methods address this, including ANN-to-SNN conversion [17],
inherentinhardware-softwareco-designofSNNs,highlighting
where a conventional ANN is trained and then converted to
the need for specialized hardware architectures and sparsity-
an SNN, potentially sacrificing accuracy and efficiency gains.
aware training techniques. Furthermore, we survey existing
UnsupervisedmethodsutilizeSTDP[14],butoftensufferfrom
hardware architectures and techniques specifically designed
slow convergence, high sensitivity to noise and high sensitiv-
to exploit sparsity, showcasing the potential for significant
ity to parameter setting. More recently, supervised learning
performance and energy efficiency gains in SNN accelerators.
with surrogate gradients [18] has shown promise by using
Bybridgingthegapbetweentheoreticalpotentialandpractical
differentiable surrogate functions during backpropagation-like
implementation, this paper aims to contribute to the advance-
training,allowingoptimizationofSNNsforbothaccuracyand
ment of neuromorphic computing and pave the way for a new
hardware efficiency.
generation of energy-efficient AI applications.
4) Inputencoding: InputencodinginSNNs,thetranslation
of real-world data into spikes, significantly affects informa-
II. BACKGROUNDONSPIKINGNEURALNETWORKS
tion representation, network sparsity, robustness to noise, and
While both SNNs and ANNs ultimately map input patterns hardware efficiency. Different encoding methods offer distinct
to outputs, their computational models differ significantly. trade-offs. For example, rate coding [19] encodes information
ANNsrelyoncontinuous-valuedactivationfunctions,whereas intheaveragefiringrateovertime,offeringhighperformance
SNNs utilize discrete binary spikes within the temporal do- in deep networks (e.g., VGG9, VGG11) [20], but often at the
main to represent information [9]. As depicted in Figure 1, cost of reduced sparsity due to the high spike rate. Temporal
using the integrate-and-fire neuron model [10] as an example, coding[21],conversely,focusesontheprecisetimingofspikes
SNN neurons accumulate incoming spikes, integrating their orpatternsofspikeswithinshorttimeframes[22].Whilegen-
weighted influence over time. A neuron fires an output spike erallysparserthanratecoding,itcansometimesleadtolower
only when its membrane potential surpasses a defined thresh- accuracy, though methods like time-to-first-spike (TTFS) have
old. In this section, we present a brief overview of SNNs achievedhighaccuracyincertainapplications[23],[24].Delta
underpinned by the importance of sparsity as a core feature. encoding [25] strikes a balance by using the temporal change
1) Neuron models: At the heart of an SNN lies the indi- of input features to generate spikes, offering a compromise
vidual neuron model and its synapses which determine the between sparsity and accuracy. Radix encoding [26] aims for
network’s learning dynamics. Simple neuron models like the ultra-shortspiketrains,achievinghighaccuracywithfewtime
Leaky Integrate-and-Fire (LIF) [11] mimic the thresholding steps, but may require specialized hardware. Direct coding
behaviorofneurons,i.e.,spikesaregeneratedwhentheirmem- [27] bypasses explicit input encoding, allowing the training
brane potential exceeds a threshold. More complex models, algorithmtolearntheoptimalmappingofinputdatatospiking
such as the Hodgkin-Huxley [12], do not have an explicit patterns. The choice of encoding scheme depends on various
threshold but detail the dynamics of membrane ion channels factors, including input data characteristics, neuron models,
for greater biological realism and introduce computational and the target application.
overhead.Thechoiceofneuronmodelprofoundlyimpactsthe 5) Applications: SNNsarewell-suitedfortaskswheretem-
efficiency obtained from network sparsity, learning dynamics, poraldynamicsandefficientprocessingarevital(e.g.,machine
and the suitability for different hardware implementations. learning implementations on resource-constrained devices).
2) Spatiotemporal dynamics: SNNs fundamentally differ They are particularly well-suited for processing data from
from traditional ANNs in their approach to information pro- event-based sensors (such as neuromorphic vision sensors
cessing. While both utilize the activation patterns of neurons or dynamic audio sensors) [28], where the sensor output
to encode information, SNNs introduce the precise timing of aligns naturally with the sparse, spike-based communication
neuronal spikes as an additional dimension [13]. This timing in SNNs. This enables low-power, real-time processing in
allows neurons to convey information through single spikes, resource-constrained edge computing systems. SNNs also
bursts, or complex temporal patterns. Learning mechanisms show promise in embedded pattern recognition tasks [29]
like Spike Timing Dependent Plasticity (STDP) [14], which where stringent power constraints must be adhered to. Their
modifysynapticstrengthsbasedontherelativetimingbetween ability to learn temporal patterns makes them applicable to
pre- and post-synaptic spikes, enable SNNs to learn both tasks such as gesture recognition [30], anomaly detection
spatial and temporal patterns. This unique capability positions in time-series data [31], or adaptive control systems [32].Additionally, the biological plausibility of SNNs opens up Due to its simplicity, sparsity emerges naturally in the
opportunitiesforcomputationalandexperimentalneuroscience Lapicque model. If the input is insufficient to push the
research, enabling the modeling of specific computations im- membrane potential above the threshold, the neuron remains
plemented by a given brain region and the investigation of silent. Sparsity in this model is primarily determined by the
learning and memory mechanisms. distribution of input weights and the chosen threshold value.
However, its lack of temporal dynamics limits the complexity
III. NEUROBIOLOGICALFOUNDATIONSOFSPARSITY
of sparsity patterns it can exhibit.
Neuroscience research reveals that sparsity may be funda-
The LIF model extends the Lapicque model by introducing
mental to the brain’s organization and function, influencing
a “leak” term, simulating the gradual decay of the membrane
storage [2], energy consumption [33], robustness to noise
potentialtowardsitsrestingstate.Theinterplaybetweeninput
[34], and processing efficiency [35]. This sparsity manifests
strength, the membrane potential’s leak term, and the firing
in various ways:
threshold governs the neuron’s spiking behavior. The leak’s
1) Sparse neural coding: The brain employs a sparse dis-
time constant influences how quickly the neuron “forgets”
tributed coding scheme, where only a small subset of neurons
previous inputs, impacting sparsity. A shorter time constant
are active in response to specific stimuli or tasks, enhancing
leads to a more rapid decay of the membrane potential in the
energy efficiency and robustness to noise [36], [37].
absence of new inputs and effectively increases the amount
2) Structural sparsity: The brain exhibits a high degree
of input current required to reach the firing threshold, thereby
of sparse connectivity—i.e., neurons form connections with
leading to sparser activity. The LIF neuron’s characteristics
onlyafractionofotherneurons[5].Thisminimizesmetabolic
can be expressed as:
wiring costs and promotes modular and specialized subnet-
works for efficient processing. (cid:88)
u [t+1]=β·u [t]+ w ·s [t]−s [t]·θ (2)
j j ij i j
3) Sparsity, plasticity, and learning: Sparsity interacts dy-
i
namically with learning mechanisms such as STDP [38],
(cid:40)
allowingforflexiblesynapticmodificationsandsynapticprun-
s [t]=
1, if u j[t]>θ
(3)
j
ing, which refines network representation during development 0, otherwise
and learning [39].
whereβ (decayfactor)controlsthemembranepotentialdecay
4) Computational models of sparsity: Theoretical models
rate, and impacts how the previous potential u [t] affects the
suggest that sparsity enhances brain computing power by j
current potential u [t+1]. θ represents the firing threshold to
reducing redundancy and facilitating pattern separation [40], j
produce a spike s [t]. A higher β and θ can lead to sparser
aiding in classification tasks [41]. j
firing.Morecomplexneuronmodelscansimilarlybeanalyzed
Understanding the biological basis of sparsity is crucial
based on their configurable parameters.
for developing neuromorphic computing systems that aim to
mimic the brain’s efficiency and low power consumption. B. Practical impacts of model hyperparameters on sparsity
Insights from biological sparsity can inspire the design of
Model hyperparameters can significantly influence SNN
algorithms,hardwareoptimizations,andplasticitymechanisms
sparsity and hardware efficiency. For example, a previous
for more efficient AI in resource-constrained systems.
study [43] showed that using the fast sigmoid surrogate
IV. UNDERSTANDINGTHEDYNAMICSOFSPARSITYIN gradient function instead of arctan increased sparsity and
PRACTICALSNNS improved frames per second/watt (FPS/W) by 11%. Fine-
tuning neuronal parameters like decay rate and threshold
The inherent sparsity of SNNs is key to their energy
further reduced latency by 48% with minimal accuracy loss.
efficiency. Sparsity is a dynamic property influenced by var-
TofurtherexaminetheLIFandLapicqueneuronmodels,we
ious factors like the network’s training algorithms, neuron
performedexperimentstoexploretheimpactsoftheirdifferent
models, input encoding methods, and dataset characteristics.
hyperparameters on sparsity. These experiments underscore
This section explores the impact of neuron models, their
theimportanceofsparsity-awarehardware-softwareco-design
hyperparameters, and encoding methods on sparsity.
inthedevelopmentofSNNs,illustratingtheneedforcarefully
A. Sparsity in the LIF and Lapicque neuron models
balancing the trade-offs between accuracy and sparsity.
To examine the sparsity characteristics of neuron models, We used snnTorch [44] to build spiking neuron models
we consider two simple models: Lapicque [42] and leaky and PyTorch to train a convolutional SNN (CSNN) on the
integrate-and-fire(LIF)[11].TheLapicquemodel,introduced Street View House Numbers (SVHN) dataset. We used a
in1907,representsaneuronasasinglepointwithamembrane VGG-9-based [45] CSNN architecture with the structure:
potential that evolves in response to incoming inputs (I(t)). 64C3-P1-112C3-P1-192C3-P1-216C3-P1-480C3-P1-504C3-
If the membrane potential (u (t)) exceeds a threshold (θ), the P1-560C3-P1-1064FC-P1-5000FC-P1-Dropout, where xCy
j
neuron fires a spike and resets to its resting value (u ): denotes convolutional layers with x filters of size y × y.
rest
Depending on the neuron model employed, P1 represents
(cid:40)
du j(t)
=
I(t) if u j(t)<θ
(1)
either the LIF or Lapicque layer. xFC is a fully connected
dt u if u (t)≥θ layer containing x neurons. Training was performed for
rest jTABLEI:AccuracyandsparsityforLIFandLapicquemodels
across different encoding methods.
Encoding Model Accuracy Spikes
LIF 77.99% 1,091,195
Rate
Lapicque 75.88% 690,700
LIF 38.40% 79,969
Delta
Lapicque 40.53% 34,246
LIF 94.46% 92,069
Direct
Lapicque 93.23% 61,761
figuresillustratesthetrade-offsbetweensparsityandaccuracy.
While high sparsity, and in effect, hardware efficiency, can
be achieved by increasing the threshold and decay factor, it
may come at a cost to accuracy. As such, a balance must
(a) LIF Accuracy
be found where accuracy remains high without significantly
compromising on sparsity. In this case, for example, the
sparsest configuration (β = 0.9, θ = 2) might represent a
satisfactory balance when hardware efficiency is the priority.
This configuration increased sparsity by 13.5% compared to
thebestaccuracyconfiguration,withonlya0.83%decreasein
accuracy,indicatingthatthisconfigurationisefficientinterms
of sparsity while maintaining high performance.
To illustrate the diversity in the sparsity of different neuron
models and the need for detailed exploration of software and
hardwareconfigurations,Figure3depictsasimilarexploration
for the Lapicque neuron. In Figure 3a, we observe a notable
decline—more so than for the LIF neuron—in accuracy as
θ increases, particularly at lower β values. Interestingly, the
(b) LIF Sparsity accuracy remains relatively robust at higher decay factors
Fig. 2: LIF neuron model cross sweep results for β and θ (β =0.95), even with an increase in threshold, maintaining a
parameters. minimum accuracy of 86.99%.
AswiththeLIFneuron,thesparsityfortheLapicqueneuron
increasesasβ andθ increase.Inthiscase,theoptimalbalance
200 epochs. Given prior research on its ability to enhance for the Lapicque neuron model was found with θ at 2.0 and
sparsity [43], we used the fast sigmoid surrogate gradient β at 0.7, achieving 93.23% accuracy and 61,761 spikes. This
function for training the network. Network parameters were configuration increased sparsity by 33.0% compared to the
updated via the Adam optimizer with an initial learning rate bestaccuracyconfiguration,witha1.53%accuracyloss.These
of 5.0×10−3. results suggest that while the Lapicque neuron might be more
1) Beta-threshold exploration: We started by performing sensitive to changes in β and θ than the LIF neuron, the
a detailed exploration of β (the leakage factor) and θ (the Lapicque neuron might be more efficient regarding sparsity
firingthreshold)forboththeLIFandLapicqueneuronmodels than the LIF model while maintaining comparable accuracy.
using direct encoding, evaluating accuracy and sparsity. The 2) Impacts of encoding methods: To investigate how dif-
Lapicque neuron is implemented using RC circuit parameters, ferent encoding approaches affect accuracy and sparsity, we
with β = e− R1 C. R (resistance) defaults to 1 and C (capaci- used the optimal β and θ (highlighted in Figures 2 and 3)
tance)isinferredbasedonthevalueofβ.Theresultsofthese for the LIF and Lapicque neurons, respectively, and trained
explorations can be seen in Figures 2 and 3. the model using each encoding methods. Table I presents a
In Figure 2a, we observe that the accuracy using the LIF comparative analysis of the accuracy and sparsity of LIF and
neuron remains relatively high across various values of β and Lapicque neuron models across three encoding methods: rate
θ, with a maximum accuracy of 95.29% achieved at β =0.15 encoding, delta encoding, and direct encoding.
and θ =2. Figure2b shows the LIFsparsity, measured by the The direct encoding method yields the highest accuracy for
number of spikes. Here, we see that in general, the sparsity bothmodels,withLIFachieving94.46%andLapicqueachiev-
increases (fewer spikes) as β and θ increase. The sparsest ing 93.23%, while also demonstrating a moderate amount
configuration, with 92,069 spikes, occurs at β = 0.9 and of sparsity with 92,069 spikes for LIF and 61,761 spikes
θ = 2. These observations suggest that higher thresholds for Lapicque. This indicates a good performance in terms
and decay factors encourage more selective neuronal activity, of both accuracy and sparsity. In this case, the Lapicque
leading to higher sparsity. Combining insights from both model performs similarly to the LIF model, with slightlyV. CHALLENGESOFHARDWARE-SOFTWARECO-DESIGN
OFSNNS
While the potential benefits of SNNs are substantial, real-
izing these advantages in the real world necessitates careful
co-design of the algorithms and the specialized hardware
that supports them. In this section, we identify several key
challenges that must be addressed to enable successful co-
design for SNNs.
1) Mapping algorithms to hardware: The complexity of
the chosen neuron model has profound implications for hard-
ware design. Complex models (e.g., Hodgkin-Huxley) might
demand a large number of computations per timestep [46],
straining embedded neuromorphic devices. Implementing on-
(a) Lapicque Accuracy chip learning rules, especially those beyond simple STDP,
adds complexity in terms of memory technologies, update
mechanisms, and potential trade-offs between flexibility and
power consumption. Input encoding also plays a crucial role
onthechallengeofmappingSNNalgorithmstohardware.For
example,rate-basedencodingcanleadtodenseactivity,reduc-
ing the benefits of hardware-level sparsity support [47], while
temporal encoding might necessitate specialized hardware for
spike-time processing [48].
2) Scalability and network architectures: Building large-
scale SNNs necessitates the efficient routing of the poten-
tially massive number of spike events, requiring specialized
routing fabrics or memory-centric architectures that reduce
communication overhead [49]. Implementing diverse SNN
topologies introduces unique challenges at both the software
(b) Lapicque Sparsity
and hardware levels. For example, deep convolutional SNNs
Fig. 3: Lapicque neuron model exploration for β and θ. need efficient distribution of convolutional kernels and man-
agement of spike-based data [8]. Replicating the connectivity
lower accuracy and fewer spikes. This makes it an attractive of large-scale brain regions onto resource-constrained neuro-
optionforscenarioswithstricterhardwarerequirementswhere morphic platforms might demand simplifying assumptions or
minimizing the number of spikes is crucial. distributed implementation strategies. Furthermore, real-time
In contrast, rate encoding shows moderate accuracy levels systems require optimized hardware-software mappings for
(77.99% for LIF and 75.88% for Lapicque) but results in real-time performance and sparsity [50].
muchhigherspikecounts,particularlyfortheLIFmodelwith 3) Accuracy vs. efficiency trade-offs: SNN optimizations
1,091,195 spikes, indicating lower sparsity. Delta encoding for efficiency, like reducing the bit precision of weights and
exhibits the lowest accuracy for both models (38.40% for activations (i.e., quantization) [51], can significantly increase
LIFand40.53% forLapicque)andmaintains thelowestspike sparsity. However, such optimizations for efficiency might
counts (79,969 for LIF and 34,246 for Lapicque). Although also carry the risk of severe accuracy degradation. Finding
delta encoding achieved the highest amount of sparsity, this hardware-aware, optimal quantization strategies is important.
was at the cost of a significant loss of accuracy. Similarly, although pruning away weights creates sparsity,
In summary, the choice of neuron model and encoding different SNN architectures might exhibit varying degrees
method significantly impacts the accuracy and sparsity of of sensitivity to pruning. In addition, changes to the SNN
SNNs.Directencodingconsistentlyoutperformsrateanddelta architecture can create new trade-off considerations for dif-
encoding in terms of accuracy, with the Lapicque neuron ferent workloads. For instance, expanding an integrate-and-
model exhibiting slightly lower accuracy but higher sparsity fire neuron model to a more bio-realistic leaky mechanism
compared to the LIF model. While rate encoding offers mod- might increase the area overhead [47], leading to important
erate accuracy, it suffers from low sparsity. Delta encoding, workload-specific trade-off considerations regarding the effi-
despite achieving the highest sparsity, is impractical due to ciency impacts of the more realistic neuron model.
its significantly lower accuracy. The optimal choice depends 4) Neuromorphic hardware heterogeneity: Analog neuro-
on the specific application requirements, with direct encoding morphicchips[52],[53]mightoffersuperiorenergyefficiency
being preferable for high accuracy and the Lapicque model but can suffer from device mismatch and noise, impacting
being advantageous for hardware-constrained scenarios prior- accuracy. Digital platforms offer flexibility but could demand
itizing sparsity. morecomplexcircuitrytoachieveequivalentsparsitybenefits.TABLE II: Summary of hardware architectures for SNNs, with a focus on techniques for exploiting sparsity to enhance
performance and energy efficiency
Work KeyFocus NotableFeatures Strengths
Cerebron[56] Spatiotemporal Sparsity Exploita- Onlineworkloadscheduling,datareuse Broadhandlingofsparsity,reduced
tion computationtime
MISS[57] IrregularSparsitySupport Unstructured pruning, sparsity-stationary Efficient memory usage, handles
dataflow complexsparsitypatterns
ESSA[58] Inference Throughput Optimiza- Adaptivespikecompression,flexiblefan-in/fan- Highthroughput,handlesbothtem-
tion out poral&spatialsparsity
SATA[7] Sparsity-AwareTraining Systolicarchitecture,exploitssparsityinspikes, Improved training energy effi-
gradients,membranepotentials ciency
MF-DSNN[59] Temporal Coding for Biological Multiplication-freecoding,parallelneuroncom- Enhanced efficiency, aligns with
Realism putations biologicalmechanisms
Kuangetal.[60] InputSparsityHandling On-chip sparse weight storage, adaptive spike Optimized for sparse inputs, high
compression/decompression computationalefficiency
SpinalFlow[61] DataflowOptimization Compressedtime-stampedinputspikes Reduced storage overheads, lower
energyconsumption
Aliyevetal.[62] DesignSpaceExploration Hardware&modelparameteralignment Peak performance optimization,
valueofexploration
The diversity of hardware also means navigating specialized reductions in prediction energy and faster processing, high-
programming tools and abstractions, potentially creating ven- lighting the importance of exploiting sparsity for neuromor-
dor lock-in and hindering the portability of SNN solutions. phic computing.
5) Lackofstandardizedtoolsandbenchmarks: Comparing
Liu et al. [57] introduced the MISS (Memory-based Ir-
theperformanceofSNNsfairlyacrossdifferentalgorithmsand
regular Sparsity Support) framework to tackle irregular spar-
hardware platforms is hindered by a lack of standardization.
sity with a combination of software and hardware optimiza-
Several current benchmarks focus on simple datasets (e.g.,
tions. The framework applies unstructured pruning to synap-
MNIST, FashionMNIST) [54], which do not fully capture the
tic weights for increased efficiency. The hardware utilizes
strengthsofSNNsinhandlingtemporalorspikingsensordata
a sparsity-stationary data flow to optimize memory usage
(e.g., neuromorphic vision sensors, audio). While there is a
and minimize processing overheads associated with sparsity,
growing body of work focused on developing neuromorphic
improving energy efficiency and speed. The MISS framework
datasets (e.g., [55]) the development of SNNs would benefit
achievedanaverageof36%improvementinenergyefficiency
from more comprehensive benchmarks that include tasks like
and 23% speedup over baseline SNN accelerators by ex-
dynamic object recognition, spatiotemporal pattern analysis,
ploiting irregular sparsity in both input spikes and synaptic
and processing event-based sensor data, reflecting real-world
weights. Kuang et al. [58] presented an accelerator called
scenarios where SNNs could excel. Additionally, software
ESSA (Efficient Sparse SNN Accelerator) that targets both
frameworks for SNNs [44] currently lack the maturity of
temporal sparsity (in spike events) and spatial sparsity (in
debugging,profiling,andhardwaremappingtoolsavailablefor
weights) for enhanced SNN inference throughput. Key de-
ANNs,potentiallyslowingdowntheresearchanddeployment
sign features include adaptive spike compression for efficient
cycle of SNNs.
handling of sparse spike patterns and a flexible fan-in/fan-
VI. SURVEYOFSPARSITY-AWAREHARDWARE outtrade-offtoworkwithinneuromorphicsystemconstraints.
ARCHITECTURESFORSNNS Results showed that ESSA achieved a performance equivalent
of 253.1 GSOP/s and an energy efficiency of 32.1 GSOP/W
Addressing the challenges in hardware-software co-design
for 75% weight sparsity on a Xilinx Kintex Ultrascale FPGA,
for sparsity-aware SNNs demands specialized hardware that
showing significant improvements in throughput and energy
can seamlessly handle the unique computational demands of
savings compared to other neuromorphic processors.
these networks. The potential for extreme energy efficiency
hinges on architectures explicitly designed to exploit irregular Unlike the prior works, which focused on inference, Yin
sparsity patterns and event-driven communication. Table II et al. [7] proposed a sparsity-aware accelerator for training
summarizes several innovative designs and approaches that called SATA. SATA (Sparsity-Aware Training Accelerator
have emerged. This section surveys some of the recent ad- for SNNs) focuses on making SNN training more efficient
vancesinthisfield,providingarepresentativeoverviewofthe usingbackpropagationthroughtime(BPTT).Itssystolic-based
strategiesbeingexploredtounlockthepowerofsparsity-aware accelerator architecture exploits various forms of sparsity (in
SNN hardware. spikes, firing function gradients, and membrane potentials) to
One notable sparsity-aware implementation is Cerebron, a improve training energy efficiency. SATA’s analysis demon-
reconfigurablearchitecturethateffectivelyhandlesbothspatial strates that SNN training can be less energy-intensive than
and temporal sparsity in SNNs [56]. It utilizes an online traditional ANN training. The analysis showed that although
channel-wise workload scheduling strategy to maximize data SNN training consumed approximately 1.27 times more total
reuse and reduce computation time. This leads to significant energy than ANNs when considering sparsity, it improvedcomputational energy efficiency by 5.58 times over non- ArizonabytheArizonaBoardofRegents(ABOR)andbythe
sparsity exploiting methods. National Science Foundation under grant CNS-1844952.
Another sparsity-aware implementation, the MF-DSNN ac-
REFERENCES
celerator [59], focuses on a temporal coding scheme that
removes the need for multiplication, enhancing biological [1] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and
A.Maida,“Deeplearninginspikingneuralnetworks,”Neuralnetworks,
realism. In concert with minimizing weight data access, this
vol.111,pp.47–63,2019.
design achieves superior performance and energy efficiency, [2] P.Foldiak,“Sparsecodingintheprimatecortex,”Thehandbookofbrain
exemplifying the advantage of leveraging temporal sparsity. theoryandneuralnetworks,2003.
Further exploring optimizations, an event-driven SNN ac- [3] A.SpanneandH.Jo¨rntell,“Questioningtheroleofsparsecodinginthe
brain,”Trendsinneurosciences,vol.38,no.7,pp.417–427,2015.
celerator by Kuang et al. [60] features on-chip sparse weight
[4] F.Faghihi,S.Cai,andA.A.Moustafa,“Aneuroscience-inspiredspik-
storage and a self-adaptive spike compression/decompression ing neural network for eeg-based auditory spatial attention detection,”
mechanism, optimizing its handling of input spike sparsity.
NeuralNetworks,vol.152,pp.555–565,2022.
[5] H. Eavani, T. D. Satterthwaite, R. Filipovych, R. E. Gur, R. C. Gur,
This enhances both speed and computational efficiency, evi-
andC.Davatzikos,“Identifyingsparseconnectivitypatternsinthebrain
denced by its high GSOPs/s performance even under elevated usingresting-statefmri,”Neuroimage,vol.105,pp.286–299,2015.
weightsparsity.TheSpinalFlowarchitecture[61]tacklesSNN [6] R. Brette, “Computing with neural synchrony,” PLoS computational
biology,vol.8,no.6,p.e1002561,2012.
efficiency through a novel data flow strategy that processes
[7] R. Yin, A. Moitra, A. Bhattacharjee, Y. Kim, and P. Panda, “Sata:
compressed, time-stamped sequences of input spikes. This Sparsity-aware training accelerator for spiking neural networks,” IEEE
Transactions on Computer-Aided Design of Integrated Circuits and
substantially reduces storage overheads and computational
Systems,2022.
cost, demonstrating the power of optimized data handling in
[8] I. Aliyev and T. Adegbija, “Pulse: Parametric hardware units
lowering energy consumption and boosting performance. for low-power sparsity-aware convolution engine,” arXiv preprint
In the domain of sparsity-aware design space exploration, arXiv:2402.06210,2024.
[9] F. Ponulak and A. Kasinski, “Introduction to spiking neural networks:
Aliyev et al. [62] focused on exploring the vast design
Informationprocessing,learningandapplications,”Actaneurobiologiae
space of sparsity-aware SNN accelerators. Their work sought experimentalis,vol.71,no.4,pp.409–433,2011.
configurations that provide peak performance by carefully [10] A. N. Burkitt, “A review of the integrate-and-fire neuron model: I.
homogeneous synaptic input,” Biological cybernetics, vol. 95, pp. 1–
aligning hardware and SNN model parameters. The proposed
19,2006.
hardware leverages SNN sparsity for significant reductions in [11] P. Lansky, P. Sanda, and J. He, “The parameters of the stochastic
resourceusageandincreasedspeed.Thisworkunderscoresthe leaky integrate-and-fire neuronal model,” Journal of Computational
Neuroscience,vol.21,pp.211–223,2006.
value of thorough design space exploration in creating highly
[12] M. Nelson and J. Rinzel, “The hodgkin-huxley model,” The book of
efficient SNN accelerators. The proposed sparsity-aware SNN genesis,vol.2,1995.
accelerator designs achieved up to 76% reduction in hardware [13] X. She, S. Dash, D. Kim, and S. Mukhopadhyay, “A heterogeneous
spiking neural network for unsupervised learning of spatiotemporal
resourcesandaspeedincreaseofupto31.25times,validating
patterns,”FrontiersinNeuroscience,vol.14,p.615756,2021.
the effectiveness of tailoring hardware to specific sparsity- [14] T. Serrano-Gotarredona, T. Masquelier, T. Prodromakis, G. Indiveri,
aware configurations for optimal performance. and B. Linares-Barranco, “Stdp and stdp variations with memristors
forspikingneuromorphiclearningsystems,”Frontiersinneuroscience,
Collectively, these advancements showcase the rapid
vol.7,p.2,2013.
progressinSNNacceleratordesign.Bystrategicallyexploiting [15] B. Han and K. Roy, “Deep spiking neural network: Energy efficiency
different dimensions of sparsity, more efficient, effective, and through time based coding,” in European Conference on Computer
Vision. Springer,2020,pp.388–404.
biologically plausible computing models can be created.
[16] R. Rojas and R. Rojas, “The backpropagation algorithm,” Neural net-
works:asystematicintroduction,pp.149–182,1996.
VII. CONCLUSION
[17] T. Bu, W. Fang, J. Ding, P. Dai, Z. Yu, and T. Huang, “Optimal ann-
We provided an overview of the hardware-software co- snn conversion for high-accuracy and ultra-low-latency spiking neural
design of sparse SNNs, emphasizing the critical role of spar- networks,”arXivpreprintarXiv:2303.04347,2023.
[18] E. O. Neftci, H. Mostafa, and F. Zenke, “Surrogate gradient learning
sity in achieving energy-efficient neuromorphic computing.
inspikingneuralnetworks:Bringingthepowerofgradient-basedopti-
Keytakeawaysincludetheunderstandingthatsparsityisady- mizationtospikingneuralnetworks,”IEEESignalProcessingMagazine,
namic property, influenced by various factors such as network vol.36,no.6,pp.51–63,2019.
[19] N.Rathi,G.Srinivasan,P.Panda,andK.Roy,“Enablingdeepspiking
architecture, training algorithms, neuron models, and input
neural networks with hybrid conversion and spike timing dependent
encodingmethods.Theexplorationofdifferentsparsity-aware backpropagation,”arXivpreprintarXiv:2005.01807,2020.
hardware architectures reveals the potential for significant [20] Z. Kang, L. Wang, S. Guo, R. Gong, S. Li, Y. Deng, and W. Xu,
“Asie:Anasynchronoussnninferenceengineforaereventsprocessing,”
performance and energy efficiency gains through specialized
ACMJournalonEmergingTechnologiesinComputingSystems(JETC),
designsthatexploitirregularsparsitypatternsandevent-driven vol.16,no.4,pp.1–22,2020.
communications.Theinsightspresentedinthispaperpavethe [21] S. Zhou, X. Li, Y. Chen, S. T. Chandrasekaran, and A. Sanyal,
“Temporal-coded deep spiking neural network with easy training and
way for future research in developing neuromorphic systems
robustperformance,”inProceedingsoftheAAAIconferenceonartificial
that fully exploit the computational advantages of sparse intelligence,vol.35,no.12,2021,pp.11143–11151.
SNNs, enabling highly energy-efficient artificial intelligence [22] J.-M.Fellous,P.H.Tiesinga,P.J.Thomas,andT.J.Sejnowski,“Dis-
coveringspikepatternsinneuronalresponses,”JournalofNeuroscience,
in resource-constrained systems.
vol.24,no.12,pp.2989–3001,2004.
ACKNOWLEDGMENT
This work was partially supported by the Technology and
Research Initiative Fund (TRIF) provided to the University of[23] J.Sommer,M.A.O¨zkan,O.Keszocze,andJ.Teich,“Efficienthardware [44] J.K.Eshraghian,M.Ward,E.Neftci,X.Wang,G.Lenz,G.Dwivedi,
acceleration of sparsely active convolutional spiking neural networks,” M. Bennamoun, D. S. Jeong, and W. D. Lu, “Training spiking
IEEE Transactions on Computer-Aided Design of Integrated Circuits neural networks using lessons from deep learning,” arXiv preprint
andSystems,vol.41,no.11,pp.3767–3778,2022. arXiv:2109.12894,2021.
[24] D.Lew,K.Lee,andJ.Park,“Atime-to-first-spikecodingandconversion [45] Y. Dong, R. Ni, J. Li, Y. Chen, J. Zhu, and H. Su, “Learning accu-
awaretrainingforenergy-efficientdeepspikingneuralnetworkprocessor rate low-bit deep neural networks with stochastic quantization,” arXiv
design,” in Proceedings of the 59th ACM/IEEE Design Automation preprintarXiv:1708.01001,2017.
Conference,2022,pp.265–270. [46] B.A.yArcas,A.L.Fairhall,andW.Bialek,“Computationinasingle
[25] S. Y. A. Yarga, J. Rouat, and S. Wood, “Efficient spike encoding neuron: Hodgkin and huxley revisited,” Neural computation, vol. 15,
algorithmsforneuromorphicspeechrecognition,”inProceedingsofthe no.8,pp.1715–1749,2003.
InternationalConferenceonNeuromorphicSystems2022,2022,pp.1– [47] N.AbderrahmaneandB.Miramond,“Informationcodingandhardware
8. architecture of spiking neural networks,” in 2019 22nd Euromicro
[26] Z. Wang, X. Gu, R. S. M. Goh, J. T. Zhou, and T. Luo, “Efficient ConferenceonDigitalSystemDesign(DSD). IEEE,2019,pp.291–298.
spiking neural networks with radix encoding,” IEEE Transactions on [48] S. Oh, D. Kwon, G. Yeom, W.-M. Kang, S. Lee, S. Y. Woo, J. S.
NeuralNetworksandLearningSystems,2022. Kim, M. K. Park, and J.-H. Lee, “Hardware implementation of spik-
[27] Y.Wu,L.Deng,G.Li,J.Zhu,Y.Xie,andL.Shi,“Directtrainingfor ing neural networks using time-to-first-spike encoding,” arXiv preprint
spiking neural networks: Faster, larger, better,” in Proceedings of the arXiv:2006.05033,2020.
AAAI conference on artificial intelligence, vol. 33, no. 01, 2019, pp. [49] S.Carrillo,J.Harkin,L.McDaid,S.Pande,S.Cawley,andF.Morgan,
1311–1318. “Adaptive routing strategies for large scale spiking neural network
[28] S. Singh, A. Sarma, S. Lu, A. Sengupta, V. Narayanan, and C. R. hardwareimplementations,”inArtificialNeuralNetworksandMachine
Das,“Gesture-snn:Co-optimizingaccuracy,latencyandenergyofsnns Learning–ICANN 2011: 21st International Conference on Artificial
for neuromorphic vision sensors,” in 2021 IEEE/ACM International NeuralNetworks,Espoo,Finland,June14-17,2011,Proceedings,Part
Symposium on Low Power Electronics and Design (ISLPED). IEEE, I21. Springer,2011,pp.77–84.
2021,pp.1–6. [50] B. Meftah, O. Lezoray, and A. Benyettou, “Segmentation and edge
[29] H.Kim,S.Hwang,J.Park,S.Yun,J.-H.Lee,andB.-G.Park,“Spiking detection based on spiking neural network model,” Neural Processing
neuralnetworkusingsynaptictransistorsandneuroncircuitsforpattern Letters,vol.32,pp.131–146,2010.
recognitionwithnoisyimages,”IEEEElectronDeviceLetters,vol.39, [51] C.Li,L.Ma,andS.Furber,“Quantizationframeworkforfastspiking
no.4,pp.630–633,2018. neuralnetworks,”FrontiersinNeuroscience,vol.16,p.918793,2022.
[30] A. Amir, B. Taba, D. Berg, T. Melano, J. McKinstry, C. Di Nolfo, [52] J.Schemmel,S.Billaudelle,P.Dauer,andJ.Weis,“Acceleratedanalog
T. Nayak, A. Andreopoulos, G. Garreau, M. Mendoza, J. Kusnitz, neuromorphic computing,” in Analog Circuits for Machine Learning,
M.Debole,S.Esser,T.Delbruck,M.Flickner,andD.Modha,“Alow Current/Voltage/TemperatureSensors,andHigh-speedCommunication:
power,fullyevent-basedgesturerecognitionsystem,”inProceedingsof AdvancesinAnalogCircuitDesign2021. Springer,2021,pp.83–102.
theIEEEconferenceoncomputervisionandpatternrecognition,2017, [53] D. Miyashita, S. Kousai, T. Suzuki, and J. Deguchi, “A neuromorphic
pp.7243–7252. chipoptimizedfordeeplearningandcmostechnologywithtime-domain
[31] B. Yusob, Z. Mustaffa, and J. Sulaiman, “Anomaly detection in time analoganddigitalmixed-signalprocessing,”IEEEJournalofSolid-State
series data using spiking neural network,” Advanced Science Letters, Circuits,vol.52,no.10,pp.2679–2689,2017.
vol.24,no.10,pp.7572–7576,2018. [54] Y. Sakemi, K. Yamamoto, T. Hosomi, and K. Aihara, “Sparse-firing
[32] E.Nichols,L.J.McDaid,andN.Siddique,“Biologicallyinspiredsnn regularization methods for spiking neural networks with time-to-first-
forrobotcontrol,”IEEEtransactionsoncybernetics,vol.43,no.1,pp. spikecoding,”ScientificReports,vol.13,no.1,p.22897,2023.
115–128,2012. [55] W.He,Y.Wu,L.Deng,G.Li,H.Wang,Y.Tian,W.Ding,W.Wang,
[33] L. Yu and Y. Yu, “Energy-efficient neural information processing in andY.Xie,“Comparingsnnsandrnnsonneuromorphicvisiondatasets:
individual neurons and neuronal networks,” Journal of Neuroscience Similarities and differences,” Neural Networks, vol. 132, pp. 108–120,
Research,vol.95,no.11,pp.2253–2266,2017. 2020.
[34] T. Bricken, R. Schaeffer, B. Olshausen, and G. Kreiman, “Emergence [56] Q. Chen, C. Gao, and Y. Fu, “Cerebron: A reconfigurable architecture
ofsparserepresentationsfromnoise,”2023. forspatiotemporalsparsespikingneuralnetworks,”IEEETransactions
[35] N. Schweighofer, K. Doya, and F. Lay, “Unsupervised learning of on Very Large Scale Integration (VLSI) Systems, vol. 30, no. 10, pp.
granule cell sparse codes enhances cerebellar adaptive control,” Neu- 1425–1437,2022.
roscience,vol.103,no.1,pp.35–50,2001. [57] F. Liu, Z. Wang, W. Zhao, Y. Chen, T. Yang, X. Yang, and L. Jiang,
[36] Y. Xu, Z. Xiao, and X. Tian, “A simulation study on neural ensemble “Randomizeandmatch:Exploitingirregularsparsityforenergyefficient
sparsecoding,”in2009InternationalConferenceonInformationEngi- processing in snns,” in 2022 IEEE 40th International Conference on
neeringandComputerScience. IEEE,2009,pp.1–4. ComputerDesign(ICCD). IEEE,2022,pp.451–454.
[37] X. Li, X. Lu, and H. Wang, “Robust common spatial patterns with [58] Y.Kuang,X.Cui,Z.Wang,C.Zou,Y.Zhong,K.Liu,Z.Dai,D.Yu,
sparsity,” Biomedical Signal Processing and Control, vol. 26, pp. 52– Y. Wang, and R. Huang, “Essa: Design of a programmable efficient
57,2016. sparsespikingneuralnetworkaccelerator,”IEEETransactionsonVery
[38] C. D. Hassall, P. C. Connor, T. P. Trappenberg, J. J. McDonald, and LargeScaleIntegration(VLSI)Systems,vol.30,no.11,pp.1631–1641,
O. E. Krigolson, “Learning what matters: a neural explanation for the 2022.
sparsitybias,”InternationalJournalofPsychophysiology,vol.127,pp. [59] Y.Zhang,S.Wang,andY.Kang,“Mf-dsnn:Anenergy-efficienthigh-
62–72,2018. performance multiplication-free deep spiking neural network accelera-
[39] R. C. Gerum, A. Erpenbeck, P. Krauss, and A. Schilling, “Sparsity tor,”in2023IEEE5thInternationalConferenceonArtificialIntelligence
throughevolutionarypruningpreventsneuronalnetworksfromoverfit- CircuitsandSystems(AICAS). IEEE,2023,pp.1–4.
ting,”NeuralNetworks,vol.128,pp.305–312,2020. [60] Y.Kuang,X.Cui,C.Zou,Y.Zhong,Z.Dai,Z.Wang,K.Liu,D.Yu,and
[40] E.HerbertandS.Ostojic,“Theimpactofsparsityinlow-rankrecurrent Y.Wang,“Anevent-drivenspikingneuralnetworkacceleratorwithon-
neural networks,” PLOS Computational Biology, vol. 18, no. 8, p. chipsparseweight,”in2022IEEEInternationalSymposiumonCircuits
e1010426,2022. andSystems(ISCAS). IEEE,2022,pp.3468–3472.
[41] K. D. Harris, “Additive function approximation in the brain,” arXiv [61] S. Narayanan, K. Taht, R. Balasubramonian, E. Giacomin, and P.-
preprintarXiv:1909.02603,2019. E. Gaillardon, “Spinalflow: An architecture and dataflow tailored for
[42] N.BrunelandM.C.VanRossum,“Lapicque’s1907paper:fromfrogs spikingneuralnetworks,”in2020ACM/IEEE47thAnnualInternational
tointegrate-and-fire,”Biologicalcybernetics,vol.97,no.5,pp.337–339, Symposium on Computer Architecture (ISCA). IEEE, 2020, pp. 349–
2007. 362.
[43] I. Aliyev and T. Adegbija, “Fine-tuning surrogate gradient learning [62] I. Aliyev, K. Svoboda, and T. Adegbija, “Design space exploration of
for optimal hardware performance in spiking neural networks,” arXiv sparsity-awareapplication-specificspikingneuralnetworkaccelerators,”
preprintarXiv:2402.06211,2024. IEEEJournalonEmergingandSelectedTopicsinCircuitsandSystems,
2023.