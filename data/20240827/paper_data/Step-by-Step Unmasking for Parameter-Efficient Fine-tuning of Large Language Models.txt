Step-by-Step Unmasking for Parameter-Efficient
Fine-tuning of Large Language Models
AradhyeAgarwal∗ SuhasKamasettyRamesh∗ AyanSengupta∗ TanmoyChakraborty
IndianInstituteofTechnologyDelhi,India
Aradhye.Agarwal.cs520@cse.iitd.ac.in, suhaskr@gmail.com
eez228655@ee.iitd.ac.in, chak.tanmoy.iit@gmail.com
Abstract and generating natural language. To adapt these
modelsfordownstreamtasks,fine-tuningontask-
Fine-tuning large language models (LLMs)
specific datasets is essential in order for the mod-
on downstream tasks requires substan-
els to acquire specialized knowledge relevant to
tial computational resources. A class of
parameter-efficientfine-tuning(PEFT)aims particular tasks and domains. Larger pre-trained
to mitigate these computational challenges models, such as GPT-3 (Brown et al., 2020)
byselectivelyfine-tuningonlyasmallfrac- and Llama (Touvron et al., 2023), exhibit even
tion of the model parameters. Although more advanced language understanding and rea-
computationally efficient, these techniques
soningskills,whichenablethemtoleverageemer-
oftenfailtomatchtheperformanceoffully
gent capabilities (Radford et al., 2019) like in-
fine-tunedmodels,primarilyduetoinherent
context learning (ICL). This ability allows these
biases introduced during parameter selec-
models to quickly adapt to new tasks without
tion. TraditionalselectivePEFTtechniques
useafixedsetofparametersbasedonapre- needing gradient-based training. However, re-
definedbudget(aprocessalsoknownasun- cent research (Liu et al., 2022) indicates that, de-
masking), failing to capture parameter im- spite its practical advantages, ICL is often less
portance dynamically and often ending up
efficient than fine-tuning in terms of computa-
exceedingthebudget. WeintroduceID3,a
tional cost and downstream performance. Con-
novelselectivePEFTmethodthatcalculates
sequently, parameter-efficient fine-tuning has be-
parameter importance continually and dy-
comeincreasinglyimportantforeffectivelyadapt-
namically unmasks parameters by balanc-
ing exploration and exploitation in param- inglargepre-trainedmodelstospecifictasks.
eter selection. Our empirical study on 15
Parameter-efficient fine-tuning (PEFT) aims to
tasksspanningnaturallanguageunderstand-
increasethememoryandcomputationalefficiency
ing and generative tasks demonstrates the
of model fine-tuning by performing low-rank or
effectiveness of our method compared to
fixed-masking-basedPEFTtechniques. We sparseupdatesinsteadofdenseupdates,asistyp-
analyticallyshowthatID3reducesthenum- ical in the case of full fine-tuning (FFT). Addi-
ber of gradient updates by a factor of two, tive PEFT methods (Houlsby et al., 2019; Pfeif-
enhancingcomputationalefficiency. ID3 is fer et al., 2020) introduce additional trainable pa-
robust to random initialization of neurons
rameterstothefrozenpre-trainedmodel,whereas
and,therefore,canbeseamlesslyintegrated
reparametrization-based PEFT techniques (Hu
intoexistingadditiveandreparametrization-
et al., 2021; He et al., 2022) utilize low-rank rep-
based PEFT modules such as adapters and
LoRAfordynamicsparsification.1 resentations of existing model parameters to re-
duce the number of trainable parameters. Selec-
tive methods (Liao et al., 2023; Sung et al., 2021;
1 Introduction
Zaken et al., 2021), another class of PEFT tech-
Pre-trained large language models (Devlin et al., niques, use different heuristics to select the pa-
2018; Liu et al., 2019; Raffel et al., 2020; Brown rameters within the pre-trained models for fine-
et al., 2020; Touvron et al., 2023) have demon- tuning. The heuristic function assigns positive
strated remarkable capabilities in understanding real-valued importance to each parameter in the
model, while a suitable selection strategy deter-
*Equalcontribution
1Code is available at https://github.com/ mines which parameters to retain for fine-tuning.
Aradhye2002/selective-peft-toolkit For instance, Diff Pruning (Guo et al., 2020) uses
4202
guA
62
]LC.sc[
1v07441.8042:viXrachange in parameter magnitude to assess the pa- 90
rameter importance, whereas, Fish mask (Sung
et al., 2021) uses gradient-based Fisher impor- 88
tanceheuristicfunction. Amajorityoftheseselec-
86
tive PEFT techniques identify and fine-tune only
a static set of top-B parameters (aka ‘budget’) 84
from the entire parameter pool, with a fixed and
82
predefined budget B. Incorrect allocation of this
budgetcandetrimentallyimpactthemodel’sfine-
80
Full FT Fish BitFit
tuningperformanceduetothemisselectionofpa- R-Mask PaFi ID3
rameters, either by including non-essential ones 78
0.00 0.05 0.10 0.15 0.20
or excluding critical ones. The parameter selec- Fine-tuned Parameters (%)
tion strategies for these PEFT techniques can be
Figure 1: Comparison of different selective PEFT
broadly classified into two classes – static (static-
methods – Full fine-tuning (Full FT), Random
S) and repeated (repeat-S). These classes utilize
masking (R-Mask), Fish (Sung et al., 2021), Bit-
two extreme cases of exploration and exploita-
Fit (Zaken et al., 2021), PaFi (Liao et al., 2023)
tion of parameters with static-S as exploitation-
and ID3 on GLUE benchmark. Marker size de-
only(i.e.,reusesameparameters),whilerepeat-S
notesthenumberoftrainableparameters. Detailed
as exploration-only (i.e., always choose new pa-
resultsarereportedinTable1.
rameters). A majority of the existing selective
PEFT methods use static-S selection, and these
izeduntrainedparameters.
exploitation-only methods often fail to select the
We evaluate the effectiveness of various se-
optimal parameters for a given task and instead
lective PEFT methods on the GLUE bench-
reuse possibly redundant features. On the other
mark (Wang et al., 2018) comprising eight nat-
hand, repeat-S-based PEFT methods often over-
ural language understanding tasks. For a bud-
shoot the target budget and perform well only for
get of 103K, ID3 outperforms the other selective
very small budgets. Selective PEFT methods like
PEFTbaselinesbyawidemarginof1.2%withthe
Diff Pruning and Fish Mask often use masking
pre-trained DeBERTa-v3 (He et al., 2021b) back-
matrices but fail to utilize their sparsity, leading
bone. With only 0.17% of trainable parameters,
tocomputationalcostsakintofullfine-tuning.
ID3beatsthefullyfine-tunedDeBERTa-v3model
withamarginof0.3%acrossallGLUEtasks(c.f.
Toaddresstheseissues,weintroduceanovelse-
Figure1). WefurtherexploreID3withLoRA(Hu
lection strategy increment-S, which balances the
et al., 2021) on a pre-trained Llama-7b (Touvron
exploration and exploitation strategies adopted in
etal.,2023)backbonemodelonsixmathematical
repeat-S and static-S. We analytically show that
reasoningtasks. With94%sparsity, ID3 achieves
incremental parameter selection is computation-
0.5% better accuracy on mathematical reasoning
ally more efficient and also practically beneficial
tasks than LoRA, emphasizing the usefulness of
as it provides fine-grained control over the bud-
ID3 on untrained (randomly initialized) compo-
get, unlike existing methods. Moreover, we ex-
nents.
perimentallyshowthatdespiteperforminghalfthe
numberofgradientupdates,theperformancewith Ourmajorcontributionsarelistedbelow:
increment-S exceeds existing baselines. We also (1) We introduce a novel selective strategy,
proposeanewDynamicmagnituDeandgraDient- increment-S, for parameter-efficient fine-
basedheuristic(akaD3),whichcombinestheben- tuning, which enables incremental parameter
efits of magnitude and gradient-based parame- selection and dynamic assessment of parame-
ter importance heuristics. Our proposed method, terimportance.
whichwecallincrement-D3(akaID3),canbeeas- (2) Weproposeanewimportance-basedheuristic,
ily integrated into any neural module and spar- D3,thatcombinesthebenefitsofgradientand
sifyadditiveandreparameterizedmodulesofpre- magnitude-based parameter importance func-
trained models. Existing static-S PEFT tech- tions. Togetherwithincrement-S strategy,our
niques do not exhibit this property, as they fail to proposedselectivePEFTmethodID3 demon-
assess parameter importance for randomly initial- strates a strong performance on various nat-
)%(
erocS
EULGural language understanding and generation reparametrization-based techniques, selective
tasks, even with highly sparse parameter up- methods consider the importance of individual
dation. parameters instead of the entire component. In
(3) Ourmethodproducesaseriesofprogressively thiscontext,BitFit(Zakenetal.,2021)selectively
improvedmodelsacrossvariousbudgetlevels, trains the bias terms within each model unit. In
allowing users to balance budget and perfor- contrast,Diffpruning(Guoetal.,2020)evaluates
manceeffectively. the absolute parameter changes across successive
(4) Weprovideanopen-sourcetoolkitintegrating training phases, pruning those with the smallest
fourselectivePEFTtechniques,offeringcom- magnitude. The computation of the magnitude
prehensive support for selective methods that of change of parameters requires significant
isnotavailableinexistingtoolkits. computationalandstoragecosts,equivalenttofull
fine-tuning of the model. To alleviate these com-
2 RelatedWork putational burdens, Sung et al. (2021) proposed
Fish Mask that computes fixed sparse masks with
Thissectionhighlightstherepresentativeworksin
an empirical Fisher importance matrix. To avoid
three broad categories of PEFT strategies – addi-
thecomputationcostoftheparameterimportance,
tive,reparameterized andselective.
Liao et al. (2023) proposed PaFi, which assesses
Additive PEFT methods, such as
the significance based on the absolute magnitude
Adapters (Houlsby et al., 2019; Pfeiffer et al.,
of the parameters and prunes the unimportant
2020), add additional neural components to the
ones. Unlike earlier methods that modify the
pre-trained models. Due to their additive nature,
pre-trained model directly, He et al. (2022)
these methodologies usually offer flexibility in
proposed SparseAdapter, a novel approach that
multi-task fine-tuning setups, where the same
merges with existing adapter-based techniques to
pre-trained model is used with different task-
sparsify a fine-tuned adapter model, enhancing
specific adapters. The earliest adapter technique
theefficiencyofPEFT.
proposed by (Houlsby et al., 2019) utilized the Our proposed ID3 method distinguishes itself
additive component in feed-forward networks of
from current selective PEFT methods by progres-
self-attention (Vaswani et al., 2017). Subsequent
sively selecting the parameters throughout fine-
additive PEFT methods (He et al., 2021a; Li
tuning steps, thereby capturing the change in pa-
and Liang, 2021; Zhu et al., 2021) differ in
rameterimportanceduringtrainingprocess. Addi-
terms of placement of these additive components.
tionallythiswillallowustochoosemodelcheck-
The reparameterization-based PEFT techniques
pointswithincrementalbudgets,whichisnotpos-
such as LoRA (Hu et al., 2021) use a low-rank sibleincurrentmethods. Furthermore,ID3 lever-
approximation of the parameter update matrix
ages the magnitude and gradient of parameters,
∆W = BA to reduce the effective number
wherethelattercanbeefficientlycomputedusing
of trainable parameters. Meanwhile, LoRA
any standard automatic differentiation tool (Bay-
applies a uniform rank across all incremental
dinetal.,2018),thusavoidingextracomputational
parameters, assuming that all parameter matrices
delays.
are equally important. To address this limitation,
AdaLoRA (Zhang et al., 2023b) dynamically 3 Methodology
allocates the parameter budget among the ad-
ditional weight matrices with singular value Motivated by the key challenges of the exist-
decomposition of the ∆W and importance-aware ing budget-driven selective PEFT methodologies,
rank allocation. IncreLoRA (Zhang et al., 2023a) highlighted in Sections 1 and 2, we propose ID3,
proposed an incremental parameter allocation an iterative approach for calculating the parame-
method that computes the importance scores terimportanceandincrementallyselectingthetop
of each module and adaptively adds the most parametersforeachtrainingiteration. Figure2il-
importantcomponentstothetrainableparameters. lustrates the workings of ID3. In this work, we
Selective parameter-efficient fine-tuning strate- introduce the terms scalar parameter and tensor
gies generate a sparse mask M ∈ {0,1}|W| parameter, where we refer to individual entries
corresponding to each weight matrix W in in the weight matrices as scalar parameters and
the pre-trained model. Unlike additive and the whole weight matrix as the tensor parameter.each parameter given its value θi and the gradi-
ent, ∇ . Formally, we define the parameter im-
θi
portancefunction(alsoreferredtoastheheuristic
function):
|∇ |
H(θi) = θi , (1)
(|θi|+ϵ)exp
whereϵ ∈ (0,∞]andexp ∈ [−∞,∞]arehyper-
parameters to control the smoothing of the func-
tion and the effect of parameter magnitude on the
final importance, respectively. The following the-
orem provides the mathematical justification be-
hindtheheuristicfunction.
Definition 1. Given the output distribution
of y ∼ p (·|x), where p (y|x) = f(x,y;θ),
θ θ
for a given input x and a model parame-
Figure2: B (Budget) = 3, T (Total training steps)
ter θ, the Fisher information matrix I(θ) is
= 3 (Top): Static-S strategy where B number of
(cid:104)(cid:16) (cid:17)2(cid:12) (cid:105)
parameters are chosen initially and used in all fu- the variance E ∂ logf(x,y;θ) (cid:12)θ −
x,y ∂θ (cid:12)
ture training steps. (Middle): Repeat-S where (cid:104)(cid:16) (cid:17)(cid:12) (cid:105)2
B number of fresh parameters are chosen accord- E x,y ∂∂ θ logf(x,y;θ) (cid:12) (cid:12)θ .
ing to the heuristic at each training step. (Bot-
tom): Increment-S wherek (= B/T)parameters Fisher information measures the amount of
are chosen at each training step according to the information the random variable x carries about
heuristicandaddedtotheselectedpool. the unknown model parameter θ and is widely
usedtoassessthemodelparameterimportance.
For instance, a tensor parameter in a BERT (De-
(cid:112)
vlinetal.,2018)modelcanbethequerymatrixof Theorem1. Forϵ ≥ 1, I(θ)istheupperbound
an attention head. The query matrix has d n2 scalar ofE (cid:104) H(θ)(cid:105) .
x,y
parameters where d is the hidden dimension, and
Proof of Theorem 1. First, we show that
n is the number of attention heads. We also for- (cid:104)(cid:16) (cid:17)(cid:12) (cid:105)
E ∂ logf(x,y;θ) (cid:12)θ = 0.
mulate a common selective PEFT method as a x,y ∂θ (cid:12)
heuristicfunctioncombinedwithaselectionstrat-
(cid:104)(cid:16) ∂ (cid:17)(cid:12) (cid:105)
egy. We identify three common selection strate- E logf(x,y;θ) (cid:12)θ
gies–(1)static-S,wheretheinitialsetofselected x,y ∂θ (cid:12)
parameters according to the heuristic are reused (cid:90) (cid:90) ∂ f(x,y;θ)
= ∂θ f(x,y;θ)dx
throughout training, (2) repeat-S, where we use f(x;θ)
x y
theheuristicrepeatedlyateachtrainingsteptofind ∂ (cid:90) (cid:90) ∂
= f(x,y;θ)dx = ·1 = 0
a(potentially)newselectedset,and(3)increment- ∂θ ∂θ
x y
S where we accumulate the selected set over the
training iterations. These selection strategies are Therefore,
illustrated in Figure 2. Existing selective PEFT (cid:104)(cid:16) ∂ (cid:17)2(cid:12) (cid:105)
methods use static-S, and ID3 uses increment-S. I(θ) = E x,y
∂θ
logf(x,y;θ) (cid:12) (cid:12)θ
We provide results for repeat-S as baselines for
(cid:104) (cid:105) 1 (cid:104)(cid:12) ∂ (cid:12)(cid:105)
comparison. E H(θ) = E (cid:12) logf(x,y;θ)(cid:12)
x,y (|θ|+ϵ)exp x,y (cid:12)∂θ (cid:12)
3.1 DeterminingScalarImportance
UsingJensen’sinequality,weget,
Evaluating the scalar importance (i.e. impor-
tance of scalar parameters) of a neural network (cid:104)(cid:12) ∂ (cid:12)2(cid:105)
I(θ) = E (cid:12) logf(x,y;θ)(cid:12)
has always been a pivotal step in model prun- x,y (cid:12)∂θ (cid:12)
ing (Molchanov et al., 2019; Cheng et al., 2023). (cid:16) (cid:104)(cid:12) ∂ (cid:12)(cid:105)(cid:17)2
≥ E (cid:12) logf(x,y;θ)(cid:12)
For a given neural model, parameterized with θ, x,y (cid:12)∂θ (cid:12)
we calculate an importance function f : R2 → (cid:16) (cid:104) (cid:105)(cid:17)2
= E H(θ) ·(|θ|+ϵ)2·exp
x,y
[0,∞] that measures a real-valued importance for(cid:16) (cid:104) (cid:105)(cid:17)2
Hence, for ϵ ≥ 1, I(θ) ≥ E H(θ) . Pointers Values
x,y
Therefore, Theorem 1 justifies that parameters (0,2) 1
with maximum H(θi) have maximum Fisher im-
(1,1) 2
portance.
(3,1) 3
Algorithm1IncrementalParameterUpdation
Require: Unmasking scheduler {u }T , number of -1 1 -2 2 -1 1 1 2
t t=1
training steps T, trainable model θ , training -3 3 -4 4 -3 2 -4 4
(0)
dataset(X,Y),learningrateη -5 5 -6 6 -5 5 -6 6
t←0 -7 7 -8 8 -7 3 -8 8
Λ ←ϕ
0 Figure 3: Bottom left: A tensor parameter in the
whilet<T do
basepre-trainedmodel,Topleft: Tableofpointers
(x,y)∼(X,Y)minibatch
and values of the updated scalar parameters after
Computepredictedoutputyˆ=p (·|x)
Computelossl=L(y,yˆ)
θ(t)
back-propagation, Right: Updated model param-
Computegradient∇ =∇ l eterafterback-propagation.
θ(t) θ(t)
Compute parameter importance H for parame-
tersinθ \Λ usingEquation1 zeros, for parameters not in the unmask set Λ to
(t) t t
Findscalarparametersλ tusingEquation2 obtain∇˜ θt. Formally,
Λ ←Λ ∪λ
t+1 t t
tionU 3pdate parameter gradients ∇˜ θ(t) using Equa-
∇˜
=(cid:40) ∇
θ
ti, ifθ ti ∈Λ
t (3)
θi
t 0 otherwise
Perform parameter update θ ← θ +
(t+1) (t)
η∇˜ Finally, the parameters are updated with back-
θ(t)
t←t+1 propagationusingtheupdatedgradients∇˜ . Al-
θ
endwhile (t)
gorithm 1 formalizes the ID3 incremental param-
eterupdationalgorithm.
3.2 IncrementalParameterUpdation With the incremental parameter selection and
update,thetotalnumberofparameterupdatescan
Supposewewanttofine-tuneapre-trainedmodel
becalculatedas
parameterized by θ (0 denotes the fine-tuning
(0)
timestep),with|θ | = N onataskformaximum T−1 t
(0) (cid:88)(cid:88)
T number of steps. Suppose we fix the budget of U dynamic = u t
fine-tuning as B, i.e., we only fine-tune a maxi- t=0 i=0
mumofB numberofscalarparametersintheen-
Foruniformunmaskingscheduler,
tire model training. The factor B is called spar-
N
sityofthemodel. Wechooseasuitableunmasking T−1 t
(cid:88)(cid:88) B T +1
scheduler{u }T thatestimatesthenumberofpa- U = = B
t t=1 dynamic T 2
rameters to be updated in each iteration t. By de- t=0 i=0
fault, we use a uniform scheduler where u = B.
t T Forstatic-masking-basedPEFTtechniques,theto-
At the beginning of model fine-tuning, the un-
talnumberofparameterupdatesis
masked parameters Λ = ϕ. At each training it-
t
erationt,wemeasuretheimportanceforeachpa- T
(cid:88)
rameter in the set θ \Λ using Equation 1 U = B = T ·B
(t−1) t−1 static
anddeterminetheincrementalunmaskedparame- t=0
tersΛ suchthat
t
Hence, U = Ustatic (whenT >> 1).
dynamic 2
Therefore, the incremental selection can reduce
maxmin{H(θi)} s.t.|λ | = u (2)
λt
θi∈λt t t
theeffectivenumberofgradientupdatesbyafac-
torof2.
Finally, the set of unmasked parameters is up-
dated as Λ = Λ ∪ λ . During the for- 3.3 EfficientProcessingofSparseMasks
t t−1 t
ward pass, we compute the task-specific loss
Storing and loading the sparse masks requires ef-
(cid:16) (cid:17)
L y,p θ (y|x) . The gradients ∇ θ are set to ficient handling of the masked scalar parameters.
(t) (t)For storing the sparse weights, we store only the the pre-trained Llama-7b (Touvron et al., 2023)
weights of the unmasked scalar parameters and model. All the pre-trained model weights are ob-
their correspondingpointers. Sincethe maximum tainedfromHuggingface(Wolfetal.,2020).
dimension of any tensor does not typically ex-
4.3 ToolkitImplementation
ceed 2, we need to store at most two indices for
any given scalar parameter. These indices can be A significant contribution of our work is the im-
stored using a 32-bit unsigned integer. Each up- plementationoftheselective-peft-toolkit2. Weuse
dated model parameter can be stored using 64- PyTorch (Paszke et al., 2019) and the Hugging-
bit double floating point numbers. Therefore, we face Transformers library (Wolf et al., 2020) for
can reduce the space complexity of the masks to implementing the toolkit. We implement the fol-
O(2×32×B +64×B) = O(B). While load- lowing selective PEFT baselines in our toolkit:
ing, we can use these pointers (stored in the form (1)BitFit(Zakenetal.,2021)involvesfine-tuning
oftensors)toindexintothetensorparametersand only the bias terms in a pre-trained model; (2)
replace the pretrained ones with the stored ones PaFi(Liaoet al.,2023)selectsthe pre-trainedpa-
whichwerelearnduringselectivefine-tuning. Fig- rameters with the smallest magnitude and trains
ure 3 summarizes the process of handling sparse onlytheseparametersduringfine-tuning;(3)ID3.
masks. Thetoolkitallowsintegrationoftheseselective
PEFT methods into the original pre-trained mod-
4 ExperimentalSetup
els as well as into any additional neural modules
such as Adapters (Houlsby et al., 2019; Pfeiffer
4.1 DatasetsandTasks
etal.,2020)andLoRA(Huetal.,2021). Wealso
To evaluate the effectiveness of our proposed providemethodsforstoringandloadingthesparse
method, we conduct exhaustive experiments weights memory-efficiently, enabling end-to-end
across three distinct tasks: text classification, to- trainingandevaluationworkflows.
kenclassification,andtextgeneration. Hyperparameters. Batch size, learning rates
For text classification, we use eight tasks andotherPEFTmethod-specifichyper-parameters
from the GLUE benchmark (Wang et al., 2018): are provided in Section 8.2 and Table 7a and Ta-
CoLA, MRPC, RTE, STS-B, SST-2, MNLI- ble7bofAppendix. Allthemodelsaretrainedon
m/mm, QNLI, and QQP. In line with previous NvidiaA100andA6000GPUs.
studies (Liao et al., 2023; Sung et al., 2021; Za-
ken et al., 2021), we exclude the WNLI task due 5 Results
toitspoorperformancewithpre-trainedlanguage
models. On token classification, we experiment 5.1 TextClassification
with the named entity recognition (NER) task us-
We report the results on GLUE tasks in Table 1.
ingtheCoNLL-2003dataset(TjongKimSangand ID3 achieves an average score of 89.15% with a
DeMeulder,2003). Fortheseninetasks,wefine-
budget of 320K, surpassing the best-performing
tunethemodelusingthetrainingsplitsandevalu-
baseline (Fish) by over 1% and even outperform-
ateitsperformanceonthevalidationsplits.
ing the FFT baseline (88.86%). A similar com-
Weconsidersixgenerativearithmeticreasoning parison holds at smaller budget levels, with ID3
tasks for text generation: GSM8K, SVAMP, Mul-
outperformingotherselectivebaselinesby>1%.
tiArith, AddSub, AQuA, and SingleEq. We fine- We further evaluate the effectiveness of ID3
tune our models on the Math10K dataset, as cu-
with other adapters integrated with pre-trained
ratedbyHuetal.(2023),andevaluatethemonthe
languagemodels. Table2reportstheperformance
test splits of the datasets above. Detailed descrip-
of the DeBERTa-v3 model with rank 8 (indicated
tions of these datasets and tasks are provided in by r=8) LoRA adapter, with and without ID3.
Section8.1andTable6ofAppendix. With a budget of 320K (sparsity 76%), ID3 out-
4.2 Models performs full LoRA fine-tuning by a margin of
0.17%. Interestingly, LoRA sparsified with both
For NLU and NER tasks, we use the pre-trained
ID3 and PaFi beats the dense LoRA model on
encoder-only DeBERTa-v3-base (He et al.,
2021b)andRoBERTa-base(Liuetal.,2019)mod- 2https://github.com/Aradhye2002/
els as the backbone. For generative tasks, we use selective-peft-toolkitBudget Method MNLI-m MNLI-mm QQP QNLI SST-2 STS-B CoLA MRPC RTE Avg
184M Full-FT 90.340.18 90.510.19 92.110.28 94.240.10 96.330.11 91.040.48 71.430.72 89.951.07 83.751.81 88.86
R-Mask 85.180.43 85.870.04 87.090.61 89.312.04 93.610.53 84.053.91 61.143.51 78.594.42 60.536.68 80.60
Fish 88.140.18 88.360.16 87.500.06 92.290.24 95.220.27 91.680.31 69.160.76 90.52
0.51
84.72
0.75
86.95
103K PaFi 88.210.15 88.290.19 89.300.35 93.630.31 95.840.13 89.970.70 68.451.09 90.20
0.65
81.831.26 86.97
BitFit 88.640.10 88.390.18 88.940.26 93.260.22 95.380.13 89.801.12 69.081.07 89.541.35 80.872.60 86.43
ID3 89.380.07 89.640.10 89.970.01 93.670.17 95.640.23 92.10
0.13
70.620.44 91.01
0.14
87.12
2.17
88.13
R-Mask 88.30.39 88.330.17 89.070.47 92.30.54 95.30.23 88.260.66 66.190.98 86.110.75 74.732.53 85.40
320K
Fish 88.990.05 89.700.08 88.920.28 93.990.06 95.600.06 92.09
0.13
69.791.04 90.69
0.49
87.85
0.55
88.07
PaFi 89.610.10 89.730.15 90.350.22 93.930.11 96.140.07 90.370.24 68.911.16 90.52
0.62
81.592.01 87.68
ID3 89.730.07 89.910.09 90.420.20 94.060.10 95.950.07 92.07
0.22
72.07
0.99
91.26
0.28
87.73
1.45
89.15
Table1: Performance comparison of different selective PEFT methods on GLUE tasks with DeBERTa-
v3(Heetal.,2021b)pretrainedmodel. Foreachexperiment,wereportthemeanandstandarddeviation
obtainedacrosstopthreeoffourdifferentruns. WehighlightthebestPEFTbaselinewithinacomparable
budget in bold. We underline the tasks where the PEFT techniques outperform the FFT counterpart.
DeBERTa-v3hasonly103Kbiasterms; therefore, BitFitisapplicableonlywith103Kbudget. R-mask
refersthebaselinewitharandomstaticmask.
CoLA MRPC RTE CoLA MRPC RTE
0.91 0.91 0.84
0.69 0.84 0.69
0.90 0.90
0.68 0.82 0.68 0.82
0.89 0.89 0.67
0.66 0.88 0.80 0.67 0.88 0.80
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 2 0 2 2 0 2 2 0 2
Eps Eps Eps Exp Exp Exp
(a)PerformanceofID3withdifferentϵ (b)PerformanceofID3withdifferentexp
Figure4: PerformanceofID3 underdifferentϵandexpvalueswithDeBERTa-V3backbonemodel.
four of nine GLUE tasks, indicating the impor- 5.2 TokenClassification
tance of sparsification of adapters for more effi-
We present the CoNLL benchmark results in Ta-
cientandeffectivefine-tuning. Anempiricalstudy
ble 4. FFT, with 184M parameters, sets the base-
withadapters(Pfeifferetal.,2020)narratesasim-
line with an F1 score of 96.69% and a standard
ilar story as shown in Table 3. With a budget of
deviation of 0.18. Among PEFT methods, ID3
only8M(sparsity96%),ID3 canimprovetheper-
excels with 103K parameters, achieving a top F1
formanceofanadapter-integratedRoBERTa-base
score of 95.86% and a low standard deviation of
by a margin of 1.09%. SparseAdapter, another
0.07, indicating both high performance and con-
popularsparsificationtechniqueforadapters,falls
sistency. FishfollowswithanF1scoreof95.43%
shortby0.91%thanID3.
(SD=0.27) and PaFi scores 95.23% (SD=0.20).
BitFit scores 94.62% but has a higher standard
deviation of 0.84, suggesting greater variability.
We further evaluate ID3 with different ϵ and With a 320K parameter budget, ID3 improves to
expvaluesandreporttheperformanceacrossthree an F1 score of 96.17 (SD=0.14), surpassing other
different NLU tasks. Figure 4a highlights an methods. PaFi scores 96.04 (SD=0.29) and Fish
interesting trend where the best performance is scores 95.99 (SD=0.14). Notably, ID3 outper-
achieved typically with ϵ = 0.5. Low ϵ values forms Fish by 0.43% and PaFi by 0.63% at 103K
have less smoothing effect and, therefore, prevent parameters, demonstrating robustness. At 320K
parameters with low magnitude from being un-
parameters,ID3’sperformanceapproachesthatof
masked unfairly. The optimal selection remains theFFTmethod,provingittobeahighlyeffective
closeat0.5,whichhasmorebalancedeffectonall alternative.
theparameters. Aconsistenttrendisalsoobserved
5.3 TextGeneration
with exp (c.f. Figure 4b), where exp = −1 con-
sistently performs better than other values. This Wereporttheresultsforthemathematicalreason-
indicates that parameter magnitude has a positive ing tasks in Table 5. Llama-7B, fine-tuned with
dependenceontheparameterimportance. LoRA(r=32),achievesanaveragescoreof59.5%,
noitalerroC
swehttaM
ycaruccA ycaruccA
noitalerroC
swehttaM
ycaruccA ycaruccABudget Method MNLI-m MNLI-mm QQP QNLI SST-2 STS-B CoLA MRPC RTE Avg
1.33M LoRA(r=8) 90.470.23 90.460.12 91.950.12 93.760.36 95.570.21 91.860.29 69.731.42 89.711.32 85.320.86 88.76
320K PaFi+LoRA(r=8) 90.120.1 90.060.42 91.610.34 94.31
0.12
96.03
0.06
91.190.18 70.5
0.59
90.28
0.57
84.720.55 88.76
320K ID3+LoRA(r=8) 90.260.25 90.180.21 91.480.26 94.17
0.07
95.76
0.12
91.650.21 69.070.55 90.69
0.25
87.12
0.21
88.93
Table2: PerformanceofPaFiandID3 withLoRA+pretrainedDeBERTa-v3modelonGLUEtasks.
Budget Method STS-B CoLA MRPC RTE Average CoLA MRPC RTE
201M Pfeiffer 90.78 59.05 89.21 76.53 78.89 0.65 0.90 0.8
8M SparseAdapter+Pfeiffer 90.88 58.95 89.41 77.03 79.07 0.60 0.85 0.7
8M ID3+Pfeiffer 90.71 59.84 89.95 79.42 79.98 0.55 0.80
0.50 0.75 0.6
0.45 0.70 0.5
Table 3: Performance of ID3 compared with 10000 100000 200000 300000 10000 100000 200000 300000 10000 100000 200000 300000
Number Of Updated Parameters Number Of Updated Parameters Number Of Updated Parameters
SparseAdapter (He et al., 2022) on Pfeiffer
adapter (Pfeiffer et al., 2020) on pretrained Figure 5: Dev performance at different model
checkpointing with ID3. The red line highlights
RoBERTa-base(Liuetal.,2019)model.
the performance obtained with repeat-S parame-
Budget Full-FT Fish PaFi BitFit ID3 R-Mask terselection.
103K
-
95.430.27 95.230.2 94.620.84 95.860.07 85.427.03
13 82 40 MK
96.690.18
95.990.12 96.040.29 -
-
96.170.14 93.741.32
Arith (92.3%) and SVAMP (42.9%). ID3, along
with higher rank LoRA, consistently performs
Table 4: Performance of selective fine-tuning
well across different parameter budgets, specifi-
methods with DeBERTa-v3 model on NER task
callyexcellinginthe7Mbudgetcategorywiththe
with different budgets. As the DeBERTa model highestaveragescoreof59.1%. Notably,ID3 sur-
hastotal103Kbiasterms,BitFitisapplicableonly
passes the performance of the base LoRA in the
with103Kbudget.
same budget range, demonstrating its efficiency
andeffectiveness.
demonstratingstrongperformanceacrossalltasks,
particularly in MultiArith (95.5%) and SingleEq
6 Discussions
(81.7%). This serves as the baseline for compar-
ison with various PEFT methods. At a reduced Wefurtherstudythebehaviorsoffine-tunedmod-
budget of 3.5M parameters, LoRA (r=2) main- els under the incremental masking and answer
tains robust performance with an average score some of the questions relevant to selective fine-
of 58.1%, showing notable performance in Mul- tuning.
tiArith (96.7%) but a slight decline across other
tasks compared to the 201M budget. ID3 with Why do we need incremental parameter selec-
LoRA(r=32)achievesanaveragescoreof58.6%, tion? WeexploreavariantofID3 withrepeat-S
slightlyoutperformingLoRA(r=2)(whichhasan selection strategy instead of increment-S, which
equivalent budget), with notable scores including we call repeat-D3. Figure 5 highlights that for
80.7% for AddSub and 79.3% for SingleEq, indi- all the tasks – CoLA, MRPC and RTE, incre-
catingthatID3 improveperformancebysparsify- mental selection with < B number of updated
ing larger rank LoRA modules. PaFi with LoRA parameters can exhibit better validation perfor-
shows an average score of 57.0%, with its best mance than the repeat-S selection method where
performance in MultiArith (92.3%), but falls be- B number of parameters are unmasked and fine-
hindID3andLoRA(r=32)fullfine-tuninginmost tuned in each training step. As highlighted in
tasks. Section 1, a completely exploration-based selec-
IncreasingtheLoRArankto4(budget7M)im- tion strategy can fine-tune too many redundant
proves its average score to 58.5%, with signif- parameters, potentially leading to worse out-of-
icant gains in GSM8K (36.9%) and MultiArith distribution performance. To understand how the
(94.8%). ID3 with a 7M budget achieves the maximumbudgetimpactsthefine-tuningprocess,
highest average score in this category at 59.1%, we experiment with different budgets, as high-
showing strong performance in GSM8K (38.3%) lighted in Figure 6. For budgets as low as 80K
and SVAMP (47.2%), suggesting that ID3 effec- (sparsity 99.95%), repeat-S selection might work
tively utilizes the additional parameters. PaFi, better than increment-S (the selection strategy of
with an average score of 56.4%, shows consistent ID3) selection. This could be due to significantly
results but lags behind other methods in Multi- fewer trainable parameters B during the initial
S
noitalerroC
swehttaM
ycaruccA ycaruccABudget Method AddSub MultiArith SingleEq GSM8K AQuA SVAMP Avg.
201M LoRA(r=32) 81.3 95.5 81.7 34.1 17.7 46.7 59.5
3.5M LoRA(r=2) 78.2 96.7 76.6 35.3 16.9 44.9 58.1
3.5M PaFi+LoRA(r=32) 78.7 92.3 76.8 33.9 16.9 43.2 57.0
3.5M ID3+LoRA(r=32) 80.7 95.8 79.3 34.3 15.7 45.7 58.6
7M LoRA(r=4) 79.2 94.8 77.9 36.9 17.3 45.0 58.5
7M PaFi+LoRA(r=32) 75.7 92.3 76.2 34.9 16.5 42.9 56.4
7M ID3+LoRA(r=32) 79.5 94.8 79.7 38.3 15.0 47.2 59.1
Table5: ResultsonmathematicalreasoningtaskswithLlama-7BbackbonemodelwithLoRA.
CoLA MRPC RTE However, the reduction in tensor sparsity stabi-
0.700 0.90 0.85
0.675 lizes after a few training steps, indicating more 0.88 0.80
0.650
0.625 0.86 0.75 exploration from the same tensor parameters. A
0.600 0.84 0.70 similarbehaviorisalsoobservedwithrepeat-Spa-
0.575 0.82
80001060000 3 B20 u0 d00 get 640000 80001060000 3 B20 u0 d00 get 640000 80001060000 3 B20 u0 d00 get 640000 rameter selection. However, with this approach,
Increment-S Repeat-S
the tensor sparsity remains critically low, as this
Figure 6: Performance of ID3 under with incre- selection method exceeds the budget and poten-
mentalmaskingandrepeat-S maskingwithdiffer- tiallyfine-tunestheentiremodel.
entbudgets.
fine-tuningstage. However,increment-S selection Are all the tensor parameters equally impor-
performs substantially better than repeat-S selec- tant in selective fine-tuning? To answer this
tion for higher budgets. Interestingly, for higher question, we compute the sparsity probability for
budgets(sparsity<99.82%),theperformancefor
each tensor parameter P as
|Pj∩ΛT|
. Using this
repeat-S selectiondropssignificantly. Theseanal-
j |Pj|
probability distribution over all the tensor param-
ysesalsohighlighttheimportanceofdynamicpa-
eters,wecalculatethesparsityentropyofthefine-
rameterandbudgetselection. Traditionalselective
tuned model. A high entropy indicates uniform
PEFT methods use a fixed budget throughout the
sparsity probability across different parameters,
model fine-tuning, which may not be optimal for
indicating uniform parameter importance. Fig-
a given task. On the other hand, using incremen-
ure 9 suggests that for increment-S selection, ini-
talbudgetinglikeID3,wecansavetheintermedi-
tially, the entropy increases, indicating more ex-
ate checkpoints with only incremental parameters
plorationofimportantscalarparametersfromdif-
beingupdated. Therefore,bypreventingunneces-
ferent tensor parameters. However, after a few
sary parameter updation, ID3 can converge faster
training iterations, the model performs more ex-
andleadtobetterout-of-distributionperformance.
ploitation by selecting scalar parameters from the
Can we sparsify tensor parameters using ID3? same tensor parameters. On the other hand, a
ArelevantresearchobjectivewithselectivePEFT repeat-Sparameterselectionstrategyperformsex-
techniques is to study their ability to sparsify ten- ploration from the beginning, assigning impor-
sor parameters. Significant masking memory can tance to most of the tensor parameters. Fig-
beoptimizedifaselectivemethodunmasksscalar ure7highlightsthescalarparameterallocationfor
parametersfromafewselectivetensorparameters. different tensor parameters for the DeBERTa-v3
ForamodelwithM numberoftensorparameters model on the CoLA task. With both increment-
{Pi}M fine-tuned with T steps, we define ‘ten- S and repeat-S selection strategies, the initial pa-
i=1
sorsparsity’asthenumberofparametersPj such rameter allocation remains sparse. After training,
that ∄θk ∈ Pj ∩Λ . Figure 8 highlights the ten- the sparsity within the tensor parameter reduces.
T
sorsparsityforID3 withincrement-S andrepeat- However, it is interesting that increment-S selec-
S selection at different training iterations. For all tionintroducesmoretensorsparsitythanrepeat-S
threetasks–CoLA,MRPCandRTE,tensorspar- selection. These behaviors justify that increment-
sity remains close to one for ID3 at the begin- S selectionisessentialforassessingtensorparam-
ning. Asthetrainingcontinues,thetensorsparsity eter importance and sparse updation of model pa-
reduces as more scalar parameters are explored. rameters.
noitalerroC
swehttaM
ycaruccA ycaruccA0.05
0.00200
Key 0 0 0 0 0 0 0 0 0 0 0 0 Key 0 0 0 0 0 0 0 0 0 0 0 0
0.00175
0.04
Query 0 0 0 0 0 0 0 0 0 0 0 0 0.00150 Query 0 0 0 0 0 0 0 0 0 0 0 0
Value 0 0 0 0 0 0 0 0 0 0 0 0 0.00125 Value 0 0.01 0 0.01 0 0.01 0.01 0.01 0.01 0.01 0.02 0.01 0.03
0.00100
Attn Out 0 0 0 0 0 0 0 0 0 0 0 0 Attn Out 0.01 0.01 0 0 0 0 0.01 0.01 0.02 0.01 0.02 0.02 0.02
0.00075
FFN1 0 0 0 0 0 0 0 0 0 0 0.002 0 0.00050 FFN1 0 0.01 0.02 0.02 0.02 0.03 0.03 0.03 0.04 0.05 0.04 0.01
0.01
FFN2 0 0 0 0 0 0 0 0 0 0 0 0 0.00025 FFN2 0 0 0 0 0 0.01 0.01 0.01 0.01 0.01 0.01 0.04
L=1 L=2 L=3 L=4 L=5 L=6 L=7 L=8 L=9 L=10L=11L=12 0.00000 L=1 L=2 L=3 L=4 L=5 L=6 L=7 L=8 L=9 L=10L=11L=12 0.00
(a)Increment-Sstartingallocation (b)Increment-Sendingallocation
0.200 0.05
Key 0 0 0 0 0 0 0 0 0 0 0 0 Key 0 0 0 0 0 0 0 0.01 0.01 0 0 0
0.175
0.04
Query 0 0 0 0 0 0 0 0 0 0 0 0 0.150 Query 0 0 0 0 0 0 0 0.01 0 0 0 0
Value 0 0 0 0 0 0 0 0.001 0.001 0.003 0.004 0.025 0.125 Value 0.01 0 0 0.01 0.01 0.01 0.02 0.02 0.02 0.01 0.01 0.01 0.03
0.100
Attn Out 0 0 0 0 0 0 0 0 0 0 0.009 0.04 Attn Out 0.01 0.01 0.01 0.01 0.01 0.01 0.02 0.02 0.02 0.01 0.01 0.01 0.02
0.075
FFN1 0 0 0 0 0 0 0 0.001 0.001 0.004 0.008 0.022 0.050 FFN1 0.02 0.03 0.03 0.03 0.04 0.05 0.05 0.04 0.02 0.01 0.01 0
0.01
FFN2 0 0 0 0 0 0 0 0 0.001 0.002 0.008 0.2 0.025 FFN2 0.02 0.02 0.03 0.03 0.04 0.04 0.05 0.03 0.01 0.01 0 0.02
L=1 L=2 L=3 L=4 L=5 L=6 L=7 L=8 L=9 L=10L=11L=12 0.000 L=1 L=2 L=3 L=4 L=5 L=6 L=7 L=8 L=9 L=10L=11L=12 0.00
(c)Repeat-Sstartingallocation (d)Repeat-Sendingallocation
Figure 7: Allotted budget to different model parameters under increment-S and repeat-S masking for
CoLAtask.
CoLA MRPC RTE CoLA MRPC RTE
1.0 1.0 1.0 0.7
0.8 0.8 0.8 0.6 0.6 0.6
0.5 0.6 0.6 0.6 0.4 0.4 0.4
0.4 0.4 0.4 0.3
0.2 0.2 0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0 0.1
0 10 20 30 40 50 0 5 10 15 20 5 10 15 0 10 20 30 40 50 0 5 10 15 20 5 10 15
Training Iteration Training Iteration Training Iteration Training Iteration Training Iteration Training Iteration
Increment-S Repeat-S Increment-S Repeat-S
Figure 8: Analysis of tensor sparsity with Figure 9: Sparsity entropy with increment-S and
increment-S andrepeat-S selectionstrategies. repeat-S selectionstrategies.
7 Conclusion
In this paper, we introduced ID3, a novel
tivePEFTmethodsdoreducethenumberofgradi-
parameter-efficient fine-tuning (PEFT) technique ent updates (with ID3 achieving competitive per-
using incremental-masking-based parameter se-
formance in half as many updates), the current
lection to enhance the fine-tuning of large lan-
implementation does not fully leverage this ef-
guage models. ID3 dynamically evaluates and
ficiency due to limitations in low-level C++ li-
updatesparameterimportance,effectivelybalanc-
braries, which predominantly support dense up-
ing exploration and exploitation. Our extensive
dates. To overcome this, future work will aim to
evaluations showed that ID3 significantly out-
integrate our method directly into the PyTorch li-
performs traditional PEFT methods, with signif-
brary at a lower level, which could better realize
icantly less number of gradient updates. Ad-
the theoretical speedup discussed. Additionally,
ditionally, ID3 integrates seamlessly with other
an intriguing research direction is to explore the
PEFT methodologies, showcasing its versatility.
activationofparametersupdatedthroughselective
We provide an open-source toolkit with four se-
PEFT from a mechanistic perspective. Our cur-
lective PEFT techniques to support reproducibil-
rent work provides some insights into this area,
ity and further research. This study marks a
but a more detailed understanding could further
significant advancement in PEFT, improving per-
illuminate how selective fine-tuning affects large
formance while reducing computational overhead
pre-trainedlanguagemodelsandenhanceexplain-
andenablingbroaderscalabilityofLLMs.
abilityinthisfield.
Limitations and Future Scope. While selec-
ytisrapS
rosneT
yportnEReferences Proceedings of the ACL-PASCAL Workshop on
TextualEntailmentandParaphrasing,pages1–
Atilim Gunes Baydin, Barak A Pearlmutter,
9, Prague. Association for Computational Lin-
Alexey Andreyevich Radul, and Jeffrey Mark
guistics.
Siskind.2018. Automaticdifferentiationinma-
chine learning: a survey. Journal of machine
Demi Guo, Alexander M Rush, and Yoon Kim.
learningresearch,18(153):1–43.
2020. Parameter-efficienttransferlearningwith
diffpruning. arXivpreprintarXiv:2012.07463.
Tom Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared D Kaplan, Prafulla
Junxian He, Chunting Zhou, Xuezhe Ma, Tay-
Dhariwal, Arvind Neelakantan, Pranav Shyam,
lor Berg-Kirkpatrick, and Graham Neubig.
Girish Sastry, Amanda Askell, et al. 2020.
2021a. Towards a unified view of parameter-
Language models are few-shot learners. Ad-
efficient transfer learning. arXiv preprint
vances in neural information processing sys-
arXiv:2110.04366.
tems,33:1877–1901.
Pengcheng He, Jianfeng Gao, and Weizhu Chen.
Daniel Cer, Mona Diab, Eneko Agirre, Iñigo
2021b. Debertav3: Improving deberta us-
Lopez-Gazpio, and Lucia Specia. 2017.
ing electra-style pre-training with gradient-
SemEval-2017 task 1: Semantic textual sim-
disentangledembeddingsharing.
ilarity multilingual and crosslingual focused
evaluation. In Proceedings of the 11th Inter-
ShwaiHe,LiangDing,DaizeDong,MiaoZhang,
national Workshop on Semantic Evaluation
and Dacheng Tao. 2022. Sparseadapter: An
(SemEval-2017), pages 1–14, Vancouver,
easy approach for improving the parameter-
Canada. Association for Computational Lin-
efficiency of adapters. arXiv preprint
guistics.
arXiv:2210.04284.
Hongrong Cheng, Miao Zhang, and Javen Qin-
feng Shi. 2023. A survey on deep neural MohammadJavadHosseini,HannanehHajishirzi,
network pruning-taxonomy, comparison, anal- OrenEtzioni,andNateKushman.2014. Learn-
ysis, and recommendations. arXiv preprint ingtosolvearithmeticwordproblemswithverb
arXiv:2308.06767. categorization. InProceedingsofthe2014Con-
ference on Empirical Methods in Natural Lan-
Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
guageProcessing(EMNLP),pages523–533.
ian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton,
NeilHoulsby,AndreiGiurgiu,StanislawJastrzeb-
Reiichiro Nakano, et al. 2021. Training ver-
ski, Bruna Morrone, Quentin De Laroussilhe,
ifiers to solve math word problems. arXiv
Andrea Gesmundo, Mona Attariyan, and Syl-
preprintarXiv:2110.14168.
vain Gelly. 2019. Parameter-efficient transfer
learningfornlp. InInternationalconferenceon
Jacob Devlin, Ming-Wei Chang, Kenton Lee,
machinelearning,pages2790–2799.PMLR.
and Kristina Toutanova. 2018. Bert: Pre-
training of deep bidirectional transformers
EdwardJHu,YelongShen,PhillipWallis,Zeyuan
for language understanding. arXiv preprint
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,
arXiv:1810.04805.
andWeizhuChen.2021. Lora: Low-rankadap-
tationoflargelanguagemodels. arXivpreprint
William B. Dolan and Chris Brockett. 2005.
arXiv:2106.09685.
Automatically constructing a corpus of sen-
tential paraphrases. In Proceedings of the
Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu,
Third International Workshop on Paraphrasing
Ee-Peng Lim, Lidong Bing, Xing Xu, Sou-
(IWP2005).
janya Poria, and Roy Ka-Wei Lee. 2023. Llm-
Danilo Giampiccolo, Bernardo Magnini, Ido Da- adapters: An adapter family for parameter-
gan, and Bill Dolan. 2007. The third PASCAL efficient fine-tuning of large language models.
recognizing textual entailment challenge. In arXivpreprintarXiv:2304.01933.Rik Koncel-Kedziorski, Hannaneh Hajishirzi, library. In Advances in Neural Information
AshishSabharwal,OrenEtzioni,andSienaDu- Processing Systems 32, pages 8024–8035.
mas Ang. 2015. Parsing algebraic word prob- CurranAssociates,Inc.
lems into equations. Transactions of the Asso-
Arkil Patel, Satwik Bhattamishra, and Navin
ciation for Computational Linguistics, 3:585–
Goyal. 2021. Are nlp models really able to
597.
solve simple math word problems? arXiv
Xiang Lisa Li and Percy Liang. 2021. Prefix- preprintarXiv:2103.07191.
tuning: Optimizing continuous prompts for
Jonas Pfeiffer, Aishwarya Kamath, Andreas
generation. arXivpreprintarXiv:2101.00190.
Rücklé, Kyunghyun Cho, and Iryna Gurevych.
Baohao Liao, Yan Meng, and Christof Monz. 2020. Adapterfusion: Non-destructive task
2023. Parameter-efficient fine-tuning with- composition for transfer learning. arXiv
out introducing new latency. arXiv preprint preprintarXiv:2005.00247.
arXiv:2305.16742.
Alec Radford, Jeffrey Wu, Rewon Child, David
Luan, Dario Amodei, Ilya Sutskever, et al.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil
2019. Languagemodels are unsupervised mul-
Blunsom. 2017. Program induction by ratio-
titasklearners. OpenAIblog,1(8):9.
nale generation: Learning to solve and explain
algebraic word problems. In Proceedings of
Colin Raffel, Noam Shazeer, Adam Roberts,
the 55th Annual Meeting of the Association for
Katherine Lee, Sharan Narang, Michael
ComputationalLinguistics(Volume1: LongPa-
Matena, Yanqi Zhou, Wei Li, and Peter J Liu.
pers),pages158–167.
2020. Exploring the limits of transfer learning
with a unified text-to-text transformer. Journal
Haokun Liu, Derek Tam, Mohammed Muqeeth,
ofmachinelearningresearch,21(140):1–67.
JayMohta,TenghaoHuang,MohitBansal,and
Colin A Raffel. 2022. Few-shot parameter-
Pranav Rajpurkar, Jian Zhang, Konstantin Lopy-
efficient fine-tuning is better and cheaper than
rev,andPercyLiang.2016. SQuAD:100,000+
in-context learning. Advances in Neural Infor-
questions for machine comprehension of text.
mationProcessingSystems,35:1950–1965.
In Proceedings of the 2016 Conference on Em-
pirical Methods in Natural Language Process-
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei
ing, pages 2383–2392, Austin, Texas. Associa-
Du, Mandar Joshi, Danqi Chen, Omer Levy,
tionforComputationalLinguistics.
Mike Lewis, Luke Zettlemoyer, and Veselin
Stoyanov. 2019. Roberta: A robustly opti- Subhro Roy and Dan Roth. 2015. Solving gen-
mizedbertpretrainingapproach. arXivpreprint eral arithmetic word problems. In Proceedings
arXiv:1907.11692. of the 2015 Conference on Empirical Methods
in Natural Language Processing, pages 1743–
Pavlo Molchanov, Arun Mallya, Stephen Tyree,
1752.
Iuri Frosio, and Jan Kautz. 2019. Impor-
tanceestimationforneuralnetworkpruning. In Richard Socher, Alex Perelygin, Jean Wu, Jason
Proceedings of the IEEE/CVF conference on Chuang, Christopher D. Manning, Andrew Ng,
computervisionandpatternrecognition, pages and Christopher Potts. 2013. Recursive deep
11264–11272. models for semantic compositionality over a
sentimenttreebank. InProceedingsofthe2013
Adam Paszke, Sam Gross, Francisco Massa,
Conference on Empirical Methods in Natural
Adam Lerer, James Bradbury, Gregory
Language Processing, pages 1631–1642, Seat-
Chanan, Trevor Killeen, Zeming Lin, Natalia
tle,Washington,USA.AssociationforCompu-
Gimelshein, Luca Antiga, Alban Desmaison,
tationalLinguistics.
Andreas Kopf, Edward Yang, Zachary DeVito,
Martin Raison, Alykhan Tejani, Sasank Chil- Yi-Lin Sung, Varun Nair, and Colin A Raffel.
amkurthy, Benoit Steiner, Lu Fang, Junjie Bai, 2021. Training neural networks with fixed
and Soumith Chintala. 2019. Pytorch: An im- sparse masks. Advances in Neural Information
perative style, high-performance deep learning ProcessingSystems,34:24193–24205.Erik F. Tjong Kim Sang and Fien De Meul- Elad Ben Zaken, Shauli Ravfogel, and Yoav
der. 2003. Introduction to the CoNLL-2003 Goldberg. 2021. Bitfit: Simple parameter-
shared task: Language-independent named en- efficient fine-tuning for transformer-based
tity recognition. In Proceedings of the Seventh masked language-models. arXiv preprint
Conference on Natural Language Learning at arXiv:2106.10199.
HLT-NAACL2003,pages142–147.
Feiyu Zhang, Liangzhi Li, Junhao Chen,
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Zhouqiang Jiang, Bowen Wang, and Yiming
Xavier Martinet, Marie-Anne Lachaux, Timo- Qian. 2023a. Increlora: Incremental parameter
thée Lacroix, Baptiste Rozière, Naman Goyal, allocation method for parameter-efficient
EricHambro,FaisalAzhar,etal.2023. Llama: fine-tuning. arXivpreprintarXiv:2308.12043.
Openandefficientfoundationlanguagemodels.
Qingru Zhang, Minshuo Chen, Alexander
arXivpreprintarXiv:2302.13971.
Bukharin, Pengcheng He, Yu Cheng, Weizhu
Ashish Vaswani, Noam Shazeer, Niki Parmar, Chen, and Tuo Zhao. 2023b. Adaptive budget
JakobUszkoreit,LlionJones,AidanNGomez, allocation for parameter-efficient fine-tuning.
Łukasz Kaiser, and Illia Polosukhin. 2017. At- In The Eleventh International Conference on
tention is all you need. Advances in neural in- LearningRepresentations.
formationprocessingsystems,30.
Yaoming Zhu, Jiangtao Feng, Chengqi Zhao,
AlexWang,AmanpreetSingh,JulianMichael,Fe- Mingxuan Wang, and Lei Li. 2021. Counter-
lix Hill, Omer Levy, and Samuel R Bowman. interference adapter for multilingual machine
2018. Glue: Amulti-taskbenchmarkandanal- translation. arXivpreprintarXiv:2104.08154.
ysis platform for natural language understand-
8 Appendix
ing. arXivpreprintarXiv:1804.07461.
8.1 Datasetsandtasks
Alex Warstadt, Amanpreet Singh, and Samuel R
Bowman. 2019. Neural network acceptability We evaluate all the methods using the eight tasks
judgments. Transactions of the Association for fromtheGLUEbenchmarkasbelow.
ComputationalLinguistics,7:625–641.
1. RTE (Recognizing Textual Entailment) (Gi-
AdinaWilliams,NikitaNangia,andSamuelBow- ampiccolo et al., 2007) - Each example con-
man.2018. Abroad-coveragechallengecorpus sists of two sentences, and the task is to pre-
for sentence understanding through inference. dict whether the second sentence entails the
In Proceedings of the 2018 Conference of the firstsentence.
North American Chapter of the Association for
Computational Linguistics: Human Language 2. MRPC (Microsoft Research Paraphrase Cor-
Technologies, Volume 1 (Long Papers), pages pus) (Dolan and Brockett, 2005) - Predict
whether the given two sentences are seman-
1112–1122, New Orleans, Louisiana. Associa-
ticallyequivalent.
tionforComputationalLinguistics.
3. CoLA (Corpus of Linguistic Acceptability)
Thomas Wolf, Lysandre Debut, Victor Sanh,
(Warstadt et al., 2019) - The task is to pre-
JulienChaumond,ClementDelangue,Anthony
dict whether the given sentence is linguisti-
Moi, Pierric Cistac, Tim Rault, Rémi Louf,
callyacceptable.
MorganFuntowicz, JoeDavison, SamShleifer,
Patrick von Platen, Clara Ma, Yacine Jernite,
4. STS-B (Semantic Textual Similarity Bench-
Julien Plu, Canwen Xu, Teven Le Scao, Syl-
mark) (Ceretal.,2017)-Thetaskistopre-
vainGugger,MariamaDrame,QuentinLhoest,
dict how similar the given two sentences are
and Alexander M. Rush. 2020. Transformers:
onascaleof1to5.
State-of-the-artnaturallanguageprocessing. In
Proceedings of the 2020 Conference on Empir- 5. SST-2 (Stanford Sentiment Treebank)
ical Methods in Natural Language Processing: (Socher et al., 2013) - The task is to predict
System Demonstrations, pages 38–45, Online. whether the sentiment of a given movie
AssociationforComputationalLinguistics. reviewispositiveornegative.6. QNLI(Question-answeringNLI) (Rajpurkar Domain Dataset #train #validation #test
et al., 2016)- Each example consists of a RTE 2.5K 277 3K
question and a context. The task is to pre- MRPC 3.7K 408 1.7K
CoLA 8.5K 1K 1K
dict whether the given context contains the
STS-b 5.7K 1.5K 1.4K
answertothequestion. GLUE SST-2 67K 872 1.8K
QNLI 105K 5.5K 5.5K
QQP 364K 40K 390K
7. QQP (Quora Question Pairs) (Wang et al.,
m-10K 10K
2018) - The task is to determine whether a MNLI 393K
mm-10K 10K
given question pair is semantically equiva-
NER CoNLL2003 14K 3.2K 3.5K
lent.
Math10k 10K - -
GSM8K 8.8K - 1319
8. MNLI (Multi-Genre Natural Language In- SVAMP - - 1000
MultiArith - - 600
ference) (Williams et al., 2018) - Given a
MathReasoning AddSub - - 395
premise sentence and a hypothesis sentence, AQuA 100K - 254
thetaskistopredictwhetherthepremiseen- SingleEq - - 508
tailsthehypothesisorcontradictsthehypoth-
Table6: Datasetsanddatasplitsfordifferenttasks
esis or neither. This data-set has two vali-
usedinthepaper.
dation sets - matched (in-domain) and mis-
matched(cross-domain)data.
6. SingleEq (Koncel-Kedziorski et al., 2015)
- This dataset contains sentences express-
For token classification, we use the shared task
ingmathematicalrelationsthatformasingle
of CoNLL2003 (Tjong Kim Sang and De Meul-
equation.
der, 2003) that focuses on language-independent
named-entity recognition. The goal is to classify
The train, validation and test splits of all the
each token into four entities - persons, locations,
datasetsareshowninTable6.
organizations and miscellaneous entities that do
8.2 Hyperparameters
notbelongtothepreviousthreegroups.
We use six datasets from the Math Reasoning For NLU and NER tasks, we use batch size 16
benchmarkfortextgenerationtasks. across all models and PEFT methods. We choose
four learning rates: {1 × 10−4,3 × 10−4,5 ×
1. GSM8K (Cobbe et al., 2021) - This dataset 10−4,7×10−4}3 to fine-tune the models. These
contains diverse grade school math word learning rates were chosen without bias towards
problems. The task is to perform a sequence a particular method and considering the common
ofelementarycalculationstogetfinalanswer. patternofchoosinglearningratesinthe1×10−3
to 1 × 10−4. We average the best three out of
2. SVAMP (Patel et al., 2021) - This dataset these four runs for statistical reliability. We use
is developed by applying simple variations abatchsizeof4forgenerativetasksandfine-tune
to one-unkown arithmetic word problems of the models with a learning rate of 3 × 10−4 for
gradelevelupto4. 3 epochs. These and other PEFT method-specific
hyper-parametersareshowninTable7a.
3. MultiArith (Roy and Roth, 2015) - This
Metric used for evaluation, number of epochs,
dataset contains multi-step arithmetic word
numberofevalstepsandmaxsequencelengthare
problems that use the basic operations. eg.
showninTable7b.
additionfollowedbysubtraction,subtraction
followedbydivisionetc.
4. AddSub (Hosseini et al., 2014) - This cor-
pus contains arithmetic problems with addi-
tionandsubtraction.
5. AQuA (Lingetal.,2017)-Thisdatasetcon-
3Forfullfine-tuningtheselearning-ratesareveryhighand
tainsalgebraicwordproblemsalongwithan- themodeldiverges.HenceforFFTwechoose{5×10−6,7×
swerrationales. 10−6,1×10−5,3×10−5}.Category NLU NER GenerativeTasks
PEFTMethod Hyper-parameter Alltasks Conll2003 Alltasks
batchsize 16 16 4
1×10−4 1×10−4
3×10−4 3×10−4
Allmethods learningrate 3×10−4
5×10−4 5×10−4
7×10−4 7×10−4
seed {6,7,8,9} {6,7,8,9} 42
exp 2 2 0
ID3
ϵ 1 1 1
num_samples 1024 1024
Fish sample_type "label" "label" -
grad_type "square" "square"
lora_r 8 8
lora_alpha 8 16
query_proj query_proj
key_proj key_proj
LoRA -
value_proj value_proj
lora_modules
attention.output.dense up_proj
intermediate.dense down_proj
output.dense
(a)CommonandPEFTmethodspecifichyper-parameters
Benchmark Dataset Metric Epochs Eval_Steps MaxSeqLength
RTE Accuracy 30 100 256
MRPC Accuracy 30 100 256
CoLA MatthewsCorrelation 20 200 256
STS-B AvgofSpearmanandPearsonCorr. 15 200 256
GLUE
SST-2 Accuracy 7 500 256
QNLI Accuracy 7 1000 256
QQP Accuracy 3 4000 256
MNLI Accuracy 3 4000 256
NER CoNLL2003 F1 20 300 384
Math10K - 3 - 256
GSM8K Accuracy - 80 256
SVAMP Accuracy - 80 256
Generative
MultiArith Accuracy - 80 256
AddSub Accuracy - 80 256
AQuA Accuracy - 80 256
SingleEq Accuracy - 80 256
(b)Taskspecifichyper-parameters
Table7: Allthehyper-parametersusedinthepaper.