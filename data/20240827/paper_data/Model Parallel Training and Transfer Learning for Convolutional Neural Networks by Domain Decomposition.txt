Model Parallel Training and Transfer Learning
for Convolutional Neural Networks by Domain
Decomposition
AxelKlawonn,MartinLanser,andJanineWeber
AbstractDeepconvolutionalneuralnetworks(CNNs)havebeenshowntobevery
successfulinawiderangeofimageprocessingapplications.However,duetotheirin-
creasingnumberofmodelparametersandanincreasingavailabilityoflargeamounts
oftrainingdata,parallelizationstrategiestoefficientlytraincomplexCNNsarenec-
essary. In previous work by the authors, a novel model parallel CNN architecture
wasproposedwhichislooselyinspiredbydomaindecomposition.Inparticular,the
novelnetworkarchitectureisbasedonadecompositionoftheinputdataintosmaller
subimages.Foreachofthesesubimages,localCNNswithaproportionallysmaller
number of parameters are trained in parallel and the resulting local classifications
arethenaggregatedinasecondstepbyadensefeedforwardneuralnetwork(DNN).
Inthepresentwork,wecomparetheresultingCNN-DNNarchitecturetolesscostly
alternativestocombinethelocalclassificationsintoafinal,globaldecision.Addi-
tionally,weinvestigatetheperformanceoftheCNN-DNNtrainedasonecoherent
model as well as using a transfer learning strategy, where the parameters of the
pre-trained local CNNs are used as initial values for a subsequently trained global
coherentCNN-DNNmodel.
1 Introduction
Convolutional neural networks (CNNs) [8] have been shown to be tremendously
successfulinprocessingimagedataor,moregeneral,datawithagrid-likestructure.
However,withincreasingnumbersofmodelparametersandincreasingavailability
AxelKlawonn,MartinLanser,JanineWeber
DepartmentofMathematicsandComputerScience,UniversityofCologne,Weyertal86-90,
50931Ko¨ln,Germany,https://www.numerik.uni-koeln.de
CenterforDataandSimulationScience,UniversityofCologne,50923Ko¨ln,Germany,https:
//www.cds.uni-koeln.de
e-mail:\{axel.klawonn,martin.lanser,janine.weber\}@uni-koeln.de
1
4202
guA
62
]VC.sc[
1v24441.8042:viXra2 AxelKlawonn,MartinLanser,andJanineWeber
oflargeamountsoftrainingdata,parallelizationapproachesforatime-andmemory-
efficient training process have become increasingly important; see also [1] for an
overview.Ingeneral,mostparallelizationapproachescanbecategorizedintomodel
ordataparallelmethods[1].Indataparallelapproaches,differentcoresorprocessors
of a parallel machine obtain local copies of the underlying deep learning model
which are trained with local subsets of training data points. Usually, the locally
trainedmodelsarethenaggregatedonceoriterativelyafterafixednumberofepochs
toobtainafinal,globalmodel.Inmodelparallelapproaches,notthetrainingdatabut
theneuralnetworkmodelitselfisdistributedtodifferentcoresorprocessorsofaCPU
or,typically,aGPU.Dependingonthedecompositionofthenetworkarchitecture,
thetotalglobalmodelthenneedstobecomposedfromthelocallytrainednetwork
parameterseitheronce,attheendofthetraining,orfrequently,giventhatinneural
networks,onelayerusuallyneedstheoutputofthepreviouslayer.
Generallyspeaking,manymodelparalleltrainingapproachescanbeinterpreted
asdomaindecompositionmethods(DDMs)[13];see[6]forasurveyofexistingap-
proachesbasedonthecombinationofmachinelearningandDDMs.In[5],anovel
model parallel training strategy for CNNs applied to different image classification
problemshasbeenpresented.Thistrainingstrategyisbasedonadecompositionof
theinputimagesintosmallersubimagesandhence,proportionallysmallerCNNsop-
eratingexclusivelyonthesubimagesaretrainedinparallel.Inparticular,thetraining
ofthelocalCNNsdoesnotrequireanycommunicationbetweenthedifferentlocal
models. Subsequently, a dense feedforward neural network (DNN) is trained that
evaluatestheresultinglocalclassificationprobabilitydistributionsintoafinalglobal
decision.Duetothedivide-and-conquercharacteraswellastheimplementationof
a global coupling between the different local CNN models, the described method
canbelooselyinterpretedasadomaindecompositionapproach.
In this paper, we extend our previous work from [5] by several comparisons.
First,weprovidefurthercomparativeresultsoftheCNN-DNNmodelfrom[5]with
computationally less costly alternatives to combine the local CNN classifications
intoafinal,globaldecision.Second,wepresentclassificationaccuraciesfortraining
the CNN-DNN model from [5] as one cohesive model architecture. Finally, we
additionallyconsidertheideaoftransferlearningsuchthatthenetworkparameters
ofthelocallytrainedCNNmodelsareusedasinitialvaluesforasubsequentlytrained
globalcoherentCNN-DNNmodel.
2 Trainingstrategies
In this section, we briefly describe the parallel CNN-DNN model architecture as
introduced in [5] as well as its extended variants and modifications for transfer
learning which are considered in this paper for the first time. Let us note that
the idea of decomposing a CNN into smaller subnetworks within the context of
preconditioningandtransferlearninghasalsobeenconsideredin[2].However,theModelParallelTrainingandTransferLearningforCNNsbyDomainDecomposition 3
globally trained CNN model in [2] is different from our globally trained network
architecture.
2.1 ParallelCNN-DNNmodelarchitecture
Fig.1 VisualizationoftheCNN-DNNnetworkarchitecture.Left:Theoriginalimageisdecom-
posedinto 𝑁 = 4nonoverlappingsubimages.Middle:The 𝑁 = 4subimagesareusedasinput
datafor𝑁 independent,localCNNs.Right:TheprobabilityvaluesofthelocalCNNsareusedas
inputdataforaDNN.TheDNNistrainedtomakeafinalclassificationforthedecomposedimage
byweightingthelocalprobabilitydistributions.Figuretakenfrom[5,Fig.4].
Aspresentedin[5],weconsiderahybridCNN-DNNneuralnetworkarchitecture
whichnaturallysupportsamodelparalleltrainingstrategy.Asastartingpoint,we
assumethatwehaveaclassicCNNmodelthattakesasinputdataatwo-dimensional
pixel image with 𝐻 ×𝑊 pixels and outputs a probability distribution with respect
to 𝐾 ∈ N classes. In order to define our CNN-DNN model, we now decompose
theinputdatainformofimagesintoafinitenumberof 𝑁 ∈ Nsmallersubimages.
Notethatforcoloredinputimageswith3channelsof𝐻×𝑊 pixels,weexclusively
decompose the images in the first two dimensions, the height and the width, but
not in the third dimension. Hence, each image is decomposed into 𝑁 subimages
with height 𝐻 𝑖 and width 𝑊 𝑖, 𝑖 = 1,...,𝑁. Then, for each of these subimages,
we construct corresponding subnetworks, that is, local CNNs that only operate
on certain subimages of all input images. Let us note that, in this paper, due to
spacelimitations,weexclusivelyconsiderdecompositionsoftheinputimagesinto
rectangular subimages without overlap. We refer to this type of decomposition as
typeAdecomposition;referalsoto[5,Sect.3.1]formoregeneralandoverlapping
decompositions of the input images. Analogously, the described decomposition of4 AxelKlawonn,MartinLanser,andJanineWeber
theinputdatacanalsobegeneralizedtothree-dimensionaldata,thatis,voxeldata,for
example,informofcomputedtomography(CT)scans.Inthatcase,theconsidered
CNN model uses three-dimensional instead of two-dimensional convolutional and
pooling operations. The local CNN models are defined such that they always have
the same general structure as the original global CNN but differ in the number of
channelsofthefeaturemaps,thenumberofneuronswithinthefullyconnectedlayers,
as well as in the number of input nodes. All of the listed layers are proportionally
smallerthanfortherespectiveglobalCNN.Inparticular,eachlocalCNNistrained
with input data that correspond to a local part of the original pixel image but has
accesstoalltrainingdatapoints.Consequently,thedescribedapproachisamodel
paralleltrainingmethod.AsoutputdataforeachofthelocalCNNs,whichcanbe
trained completely in parallel and independently of each other, we obtain a set of
𝑁 local probability distributions with respect to the 𝐾 classes, where each of the
localprobabilitydistributionscorrespondstoalocaldecisionexclusivelybasedon
informationextractedfromthelocalsubimages.
Withtheaimofgeneratingafinal,globaldecisioninformofaglobalprobability
distributionwithrespecttothe𝐾-classclassificationproblem,wesubsequentlytrain
aDNNthataggregatesthelocalCNNdecisions.Moreprecisely,theDNNusesas
inputdataavectorcontainingthe𝐾∗𝑁localprobabilityvaluesofall𝑁localCNNs.
TheDNNmodelisthentrainedtomapthisinputvectortothecorrectclassification
labelsoftheoriginalinputimagescorrespondingtothe𝐾 classesoftheconsidered
imageclassificationproblem.InFig.1,weshowanexemplaryvisualizationofthe
describedCNN-DNNmodelarchitectureforaglobalCNNofVGG3type[11].The
definitionandtrainingofthelocalCNNsisbasedonthedecompositionoftheinput
imagesinto 𝑁 = 4subimagesandhence, 𝑁 = 4localCNNsaretrainedinparallel
forthiscase.Additionally,aDNNistrainedtoobtainthefinal,globalclassification.
Comparison with computationally less costly alternatives Besides evaluating
the training time and accuracy values of our presented CNN-DNN model, we ad-
ditionally compare its performance in terms of classification accuracy with two
computationallylessexpensivemethodstocombinethelocalclassificationsofthe
localCNNsintoafinalglobalclassification.Asafirstalternative,weconsiderthe
computation of an average probability distribution among the outputs of the local
CNNsandassigneachinputwiththelabelthatshowsthehighestaverageprobability.
InSection3,werefertothisvariantasaverageprobability(avg.prob.).Second,we
additionallyconsiderasimplemajorityvoting,thatis,weassigneachimagewiththe
labelthatmostofthelocalCNNsassigntheirrespectivesubimagesto.Letusnote
thatthisclassificationisnotnecessarilyuniquesincetwoormoreclassesmayexist
which share the majority of the votes. In such cases, we additionally consider the
probabilityvaluesfortherespectiveclassesandchoosetheclassamongthemajority
candidateswiththehighestassignedprobabilityvalue.InSection3,werefertothis
variantasmajorityvoting(maj.vot.).
Training the CNN-DNN as one model Even though the main objective in [5] is
to provide a network architecture that is well-suited for a model parallel trainingModelParallelTrainingandTransferLearningforCNNsbyDomainDecomposition 5
procedure,additionally,wecarefullyinvestigatetheclassificationaccuraciesofthe
proposedCNN-DNNmodeltoensurethattheenhancedparallelizationisnotofthe
costofdrasticallyreducedclassificationperformance.Hence,inSection3,wealways
comparetheaccuracyofourCNN-DNNmodelwithaglobalbenchmarkCNNwhich
has the same structure and architecture as the local CNNs but with proportionally
moreparametersandwhichoperatesontheentireimagesasinputdata.Additionally,
forthefirsttime,wealsocomparetheCNN-DNN,wherethelocalCNNsaretrained
in parallel as described above, with a CNN-DNN that is sequentially trained as
onecoherentmodel.ThatmeansthatweimplementtheCNN-DNNarchitectureas
shown in Fig. 1 as one model using the functional API of TensorFlow and train it
withinonesequentialtrainingloop.Fortheremainderofthispaper,werefertothis
approachascoherentCNN-DNN (CNN-DNN-coherent).
2.2 TransferLearning
To provide a broader performance test of our proposed network architecture, we
furtherusetheconceptoftransferlearningfortheCNN-DNNtrainedasonemodel.In
thiscase,wefirsttrainproportionallysmallerCNNsoperatingonseparatesubimages
asdescribedinSection2.1for150epochsandsubsequentlyusetheobtainednetwork
parametersasinitializationsfortherespectiveweightsandbiasvaluesofthecoherent
CNN-DNN model. The coherent CNN-DNN model with this initialization is then
further trained with respect to the global classification labels of the underlying
images. Regarding to the loose analogy of the CNN-DNN training approach to
DDMs,theconcreteimplementationoftransferlearningbasedonlocallypre-trained
smaller networks can also be interpreted as a preconditioning strategy within an
iterativesolveroroptimizationmethod,respectively;seealso[2]foracloselyrelated
approachforadifferentglobalneuralnetworkarchitecture.Inthefollowing,werefer
tothisapproachasCNN-DNNwithtransferlearning(CNN-DNN-transfer).
3 Experiments
Inthissection,wepresentsomeexperimentswithrespecttothedescribedapproaches
andcomparetheclassificationaccuraciesfordifferentimagerecognitionproblems.
Allexperimentshavebeencarriedoutonaworkstationwith8NVIDIATeslaV100
32GBGPUsusingtheTensorFlowlibrary.6 AxelKlawonn,MartinLanser,andJanineWeber
Fig. 2 Left: Exemplary images of the CIFAR-10 dataset [7]. Right: Exemplary images of the
TF-Flowersdataset[12].
3.1 Networkarchitecturesanddatasets
ToevaluatetheperformanceofthedescribedtrainingstrategiesfromSection2,we
considertwodifferentnetworkarchitecturesandthreedifferentimageclassification
datasets. First, we test our approach for a CNN with nine blocks of stacks of con-
volutionallayersandafixedkernelsizeof3×3pixels,incaseoftwo-dimensional
imagedata,or3×3×3voxels,forthree-dimensionalimagedata,respectively.We
refertothisnetworkarchitectureasVGG9fortheremainderofthispaperandrefer
to[11]formoreimplementationaldetailsofthisnetworkmodel.Second,weapply
all training strategies to a residual neural network (ResNet) [3] with 20 blocks of
convolutional layers where we additionally implement skip connections between
eachblockanditsthirdsubsequentblock;seealso[3]formoretechnicaldetails.We
refer to this network architecture as ResNet20. All networks are trained using the
Adam(Adaptivemoments)optimizer[4]andthecross-entropylossfunction.
We test both network models for the CIFAR-10 data [7], the TF-Flowers
dataset[12],andathree-dimensionaldatasetofchestCTscans[10].TheCIFAR-10
dataset[7]consistsof50000trainingand10000validationimagesof32×32pixels
whicharecategorizedin𝐾 = 10differentclasses;seealsoFig.2(left).Giventhat
theseimagesarerelativelysmall,weonlydecomposetheimagesinto𝑁 =4subim-
ages.TheTF-Flowersdataset[12]consistsof3670imageswhichwesplitinto80%
training and 20% validation data. All these images have 180×180 pixels and are
classifiedinto 𝐾 = 5differentclassesofflowers;cf.alsoFig.2(right).Asthelast
dataset,weconsiderthethree-dimensionalimagesetofchestCTscans[10]which
consistsofCTscanswithandwithoutsignsofCOVID-19relatedpneumonia,that
is, we have 𝐾 = 2. For an exemplary visualization of CT slices for one exemplary
datapoint, see Fig. 3. Each of these CT scans consists of 128×128×64 voxels
andhence,here,wetrainCNNmodelsusingthree-dimensionalfiltersandpooling
layers. For all datasets, the DNN model consists of four hidden layers; see [5] for
moredetails.ModelParallelTrainingandTransferLearningforCNNsbyDomainDecomposition 7
Fig.3 ExemplaryslicesforonechestCTscantakenfromtheMosMedDatadataset[10].
3.2 Results
InTable1,wecomparetheclassificationaccuraciesforthevalidationandtraining
data for the CNN-DNN approach for a VGG9 model with a majority voting and
anaverageprobabilitydistributiontocombinethelocalCNNclassificationsintoa
globaldecision.Aswecanobserve,foralltesteddatasets,theCNN-DNNapproach
resultsinhighervalidationaccuraciesthanboth,theaverageprobabilitydistribution
and the majority voting, for all tested decompositions. This shows that it is not a
trivial task to combine the local classifications obtained from the local CNNs into
afinal,globalclassificationandthatitseemstobehelpfultotrainasmallDNNto
makethisevaluationautomaticallyforus.
When observing the results in Table 2, two major observations can be made.
First, with respect to the CNN-DNN-coherent model, we see that for the VGG9
model, the coherent model trained in one sequential training loop results in lower
validation accuracies than the CNN-DNN model for all three considered datasets
and all tested decompositions. However, for the ResNet20 model, the quantitative
behavior is reversed, that is, the CNN-DNN-coherent networks result in higher
classificationaccuracieswithrespecttothevalidationdata.Apossibleexplanation
forthiscouldbeasfollows.TheCNN-DNN-coherentmodelwhichisimplemented
asoneconnectedmodelarchitecturemighthaveamorecomplexlossfunctionand
thus,losssurfacethanthelocallytrainedsmallerCNNsaswellastherelativelysmall
DNN.Hence,optimizingtherespectiveparametersoftheVGG9modelallatonce
might be more difficult than optimizing first the parameters of the local CNNs in
parallelandsubsequently,theparametersoftheDNN.However,whenconsidering
theResNet20model,theoptimizationoftheCNN-DNN-coherentmodelmightbe
easiergiventhattheintroductionofskipconnectionsusuallyresultsinsmootherloss
surfacesfordeepneuralnetworksandenhancedtrainingproperties;seealso[3,9].
This could explain that for the ResNet20 model, the CNN-DNN-coherent shows
an improved classification accuracy. A detailed investigation of the resulting loss
surfaces and their complexity for the tested models is a potential topic for future
research.
Second,whenconsideringthetransferlearningstrategy,weobservehigherclas-
sification accuracies for both, the VGG9 network model and the ResNet20 model
foralltesteddatasetscomparedtothetrainingstrategieswithouttransferlearning.
Hence,usingDDMformeansofpreconditioningandtransferlearningcanhelpto8 AxelKlawonn,MartinLanser,andJanineWeber
Table1 Classificationaccuraciesforthevalidationandtrainingdata(inbrackets)fortheCNN-
DNN approach for a VGG9 model and computationally less costly alternatives to combine the
classifications of the local CNNs. In particular, we show the obtained accuracy values for an
averageprobabilitydistribution(avg.prob.)andamajorityvoting(maj.vot.).
Decomp. avg.prob.maj.vot. CNN-DNN
CIFAR-10
typeA 0.6745 0.6237 0.7669
2×2,𝛿=0 (0.7081) (0.6546) (0.8071)
TF-Flowers
typeA 0.6162 0.5974 0.6938
2×2,𝛿=0 (0.6498) (0.6026) (0.7552)
typeA 0.7565 0.7022 0.8471
4×4,𝛿=0 (0.7745) (0.7238) (0.8593)
ChestCTscans
typeA 0.8038 0.7761 0.9143
2×2×1,𝛿=0 (0.8279) (0.7997) (0.9357)
typeA 0.8024 0.7453 0.8988
4×4×2,𝛿=0 (0.8409) (0.7999) (0.9493)
Table2 Classificationaccuraciesforthevalidationandtrainingdata(inbrackets)foraglobal
CNNbenchmarkmodel(VGG9orResNet20),theCNN-DNNapproachasintroducedin[5],the
CNN-DNN model trained as one coherent model (CNN-DNN-coherent), and a coherent CNN-
DNNmodeltrainedwithatransferlearningapproach(CNN-DNN-transfer).
Decomp. globalCNNCNN-DNNCNN-DNN-coherentCNN-DNN-transfer
CIFAR-10,VGG9
typeA 0.7585 0.7999 0.7515 0.8462
2×2,𝛿=0 (0.8487) (0.8663) (0.7902) (0.8889)
CIFAR-10,ResNet20
typeA 0.8622 0.8784 0.8998 0.9117
2×2,𝛿=0 (0.9343) (0.9467) (0.9558) (0.9664)
TF-Flowers,VGG9
typeA 0.7887 0.8154 0.7808 0.8378
2×2,𝛿=0 (0.9321) (0.8827) (0.8667) (0.8999)
typeA 0.7887 0.8589 0.7676 0.8608
4×4,𝛿=0 (0.9321) (0.8872) (0.7995) (0.8806)
TF-Flowers,ResNet20
typeA 0.8227 0.8475 0.8776 0.8997
2×2,𝛿=0 (0.9178) (0.9454) (0.9603) (0.9702)
typeA 0.8227 0.8068 0.8406 0.8654
4×4,𝛿=0 (0.9178) (0.8892) (0.9002) (0.9244)
ChestCTscans,VGG9
typeA 0.7667 0.9143 0.8889 0.9304
2×2×1,𝛿=0 (0.8214) (0.9357) (0.9097) (0.9577)
typeA 0.7667 0.8988 0.8774 0.9025
4×4×2,𝛿=0 (0.8214) (0.9493) (0.9305) (0.9488)
furtherincreasetheaccuracyofimageclassificationmodels;cf.also[2].Adetailed
investigationoftherequiredtrainingtimesofthetransferlearningstrategyforour
proposedmodelarchitectureisafurthertopicforfutureresearch.ModelParallelTrainingandTransferLearningforCNNsbyDomainDecomposition 9
References
1. T.Ben-NunandT.Hoefler. Demystifyingparallelanddistributeddeeplearning:Anin-depth
concurrencyanalysis. ACMComputingSurveys(CSUR),52(4):1–43,2019.
2. L.Gu,W.Zhang,J.Liu,andX.-C.Cai. Decompositionandcompositionofdeepconvolu-
tionalneuralnetworksandtrainingaccelerationviasub-networktransferlearning. Electronic
TransactionsonNumericalAnalysis,56:157–186,2022.
3. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages770–
778,2016.
4. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014.
5. A. Klawonn, M. Lanser, and J. Weber. A domain decomposition-based CNN-DNN archi-
tecture for model parallel training applied to image recognition problems. arXiv preprint
arXiv:2302.06564,2023.AcceptedforpublicationinSIAMJournalonScientificComputing.
6. A.Klawonn,M.Lanser,andJ.Weber.Machinelearninganddomaindecompositionmethods–a
survey. arXivpreprintarXiv:2312.14050,2023.
7. A.Krizhevsky,G.Hinton,etal. Learningmultiplelayersoffeaturesfromtinyimages,2009.
Technicalreport.
8. Y. LeCun. Generalization and network design strategies. Connectionism in perspective,
19:143–155,1989.
9. H.Li,Z.Xu,G.Taylor,C.Studer,andT.Goldstein. Visualizingthelosslandscapeofneural
nets. Advancesinneuralinformationprocessingsystems,31,2018.
10. S.Morozov,A.Andreychenko,N.Pavlov,A.Vladzymyrskyy,N.Ledikhova,V.Gombolevskiy,
I.Blokhin,P.Gelezhe,A.Gonchar,andV.Chernina. MosMedData:ChestCTScanswith
COVID-19RelatedFindingsDataset. medRxiv,2020.
11. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. arXivpreprintarXiv:1409.1556,2014.
12. TheTensorFlowTeam. Flowersdataset,Jan.2019.
13. A.ToselliandO.Widlund. DomainDecompositionMethods—AlgorithmsandTheory,vol-
ume34ofSpringerSeriesinComputationalMathematics. Springer-Verlag,Berlin,2005.