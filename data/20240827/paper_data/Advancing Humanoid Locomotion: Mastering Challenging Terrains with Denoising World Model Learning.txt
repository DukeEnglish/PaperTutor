Advancing Humanoid Locomotion: Mastering
Challenging Terrains with Denoising World Model
Learning
Xinyang Gu∗ Yen-Jen Wang∗ Xiang Zhu∗
RobotEra TECHNOLOGY CO., LTD. Tsinghua University Tsinghua University
∗Equal contribution. Project Co-lead. Shanghai Qi Zhi Institute Shanghai Qi Zhi Institute
Email: wangyenjen@berkeley.edu ∗Equal contribution. Project Co-lead.
∗Equal contribution. Project Co-lead.
Chengming Shi∗ Yanjiang Guo Yichen Liu Jianyu Chen†
Tsinghua University Tsinghua University Tsinghua University Tsinghua University
∗Equal contribution. Project Co-lead. Shanghai Qi Zhi Institute Shanghai Qi Zhi Institute
RobotEra TECHNOLOGY CO., LTD.
Email: jianyuchen@tsinghua.edu.cn
† Corresponding Author.
A B C
D E F G H I
Fig.1:Extensiveshowcaseoflocomotionskillsusingtheproposedframework.Displayedisasequenceillustratingahumanoid
robot skillfully executing various locomotion tasks in real world challenging environments.
Abstract—Humanoid robots, with their human-like skeletal to master real-world challenging terrains such as snowy and in-
structure, are especially suited for tasks in human-centric envi- clinedlandinthewild,upanddownstairs,andextremelyuneven
ronments. However, this structure is accompanied by additional terrains.Allscenariosrunthesamelearnedneuralnetworkwith
challenges in locomotion controller design, especially in complex zero-shot sim-to-real transfer, indicating the superior robustness
real-world environments. As a result, existing humanoid robots and generalization capability of the proposed method.
arelimitedtorelativelysimpleterrains,eitherwithmodel-based
control or model-free reinforcement learning. In this work, we I. INTRODUCTION
introduce Denoising World Model Learning (DWL), an end-to-
endreinforcementlearningframeworkforhumanoidlocomotion Modern environments are primarily designed for humans.
control, which demonstrates the world’s first humanoid robot Therefore, humanoid robots, with their human-like skeletal
4202
guA
62
]OR.sc[
1v27441.8042:viXrastructure, are especially suited for tasks in human-centric
environments and offer unique advantages over other types of
robots. Their mobility is crucial for completing diverse tasks
in the real world, underlining the necessity of their capacity
to walk on complex terrains.
Previously, model-based control techniques such as Zero
Moment Point (ZMP) and Model Predictive Control (MPC)
Closed Kinematic Chain
combinedwithWhole-BodyControl(WBC)havesignificantly Ankle Mechanism
advanced humanoid robots’ locomotion abilities, enabling
skills like walking, jumping, and even backflipping[2, 38, 5].
However, the success of these methods depends on accurately
modeling the environment’s dynamics, which can make it
difficult to handle complex interactions with the environment,
such as navigating challenging terrains.
Reinforcement learning (RL), on the other hand, relies less
on exact environmental modeling. Recent progress in model-
Height: 1.2 meters Height: 1.65 meters
free RL has shown great potential, particularly in develop- Weight: 38 kg Weight: 57kg
ing adaptive legged locomotion controllers [28]. This allows Actuated motors: 26 Actuated motors: 54
Total DOF: 32 (7 on each arm, Total DOF: 60 (6 on each leg,
robots to learn and adapt to a wide range of situations, often 6 on each leg,6onbase) 6onbase)
surpassing the capabilities of traditional model-based control
Fig.2:Illustrationofthehumanoidrobot’shardwarestructure
methods[22].
and the Closed Kinematic Chain Ankle Mechanism. This
However, ensuring robustness in humanoid robots, as op-
mechanism is notable for offering two degrees of freedom in
posed to quadrupedal[28] and bipedal[27] counterparts, in-
eachanklewhilereducingleginertia.Ourworksaretestedon
volvesaddressingseveraladditionalchallenges.Theseinclude
two distinct sizes of humanoid robots, XBot-S and XBot-L,
but are not limited to a higher center of gravity, instability
provided by Robot Era.
during leg swinging, greater leg inertia, extra weight from the
torso and arms, and their larger size overall. Therefore, to
ankle control, powered by RL and a closed kinematic
date, real-world applications of reinforcement learning (RL)
chain for enhanced stability and flexibility.
forcontrollinghumanoidrobots,asdemonstratedintherecent
study[26], have been limited to relatively simple terrains.
II. RELATEDWORKS
Inthiswork,weintroduceDenoisingWorldModelLearn-
ing (DWL) for controlling humanoid robots across varied a) Learning Robot Locomotion: Reinforcement learning
and complex terrains. To the best of our knowledge, DWL hasbecomemorepromisingtoenablerobotstoperformstable
enables the world’s first humanoid robot to master real-world locomotion[35,12,18].ComparedtopreviousRLeffortswith
challengingterrainswithend-to-endRLandzero-shotsim-to- quadrupedalrobots[28]andbipedalrobotslikeCassie[21,17],
real transfer. As shown in Fig.1, our humanoid robot is able our focus on humanoid robots presents a significantly more
to navigate stably through snowy inclined land in the wild, challenging setup. Our proposed method excels in automating
stairs,irregularsurfaces,etc.,andcanresistlargeexternaldis- state representation learning [19], mastering end-to-end learn-
turbances. All scenarios run the same learned neural network ing forboth predictionand adaptationand facilitating aseam-
policy, indicating its robustness and generalization. The key less zero-shot transfer to real-world scenarios by effectively
ingredient of DWL lies in establishing an effective represen- bridging the sim-to-real gap.
tation learning framework to denoise the factors enlarging the Furthermore, conventional approaches, often encompassing
sim-to-real gap. Furthermore, we are the first to enable active multi-stage training processes [1], detailed reward designs
2-DoF ankle control with a Closed Kinematic Chain Ankle [39], or behavior cloning [24], typically falter amidst the
Mechanism (shown in Fig.2) for humanoid robot locomotion dynamic and unpredictable real-world scenarios. On the other
learning.Unlikepreviousstudies[32]withonlyoneDoFankle hand, DWL instead integrates a world model within an
controlorpassiveanklecontrol[26],ourapproachenablesthe encoder-decoder framework, employing a masking loss to
robot to become extremely robust. The contributions of our predict the state from observations.
work are summarized as follows: b) Humanoid Robot Locomotion Control: The evolution
1) Demonstrate the world’s first humanoid robot master- of humanoid locomotion began with early concepts and basic
ing real-world challenging terrains with end-to-end RL models, exemplified by WABOT-1 in the 1970s[13]. Progress
through zero-shot sim-to-real transfer. in sensors and control algorithms enhanced humanoid robots’
2) Propose DWL, a novel RL framework to bridge the stability and adaptability. model-based control techniques[16]
sim-to-real gap and achieve robust generalizable perfor- like ZMP[34], MPC[33, 20], and WBC[30] have significantly
mance. improvedlocomotivecapabilities.Learning-basedapproaches,
3) Unveilacutting-edgehumanoidrobotwithactive2-Dof which are less reliant on precise dynamic models, offer betteradaptability and robustness. Despite this, real-world applica- raw sensory data into a latent space and reconstructing
tions of reinforcement learning for humanoid control, such as the robot’s full state from it.
[26, 8], have been successful but limited to simpler terrains. • A policy gradient method that facilitates iterative im-
provements of the controller, allowing optimizing com-
III. PROBLEMSETTING
plex objectives through environmental interaction.
A. Reinforcement Learning Background 1) Encoder-Decoder Architecture of DWL: In an ideal
Our approach utilizes a reinforcement learning problem world, a complete observed state, precise sensors, and a
setting, encapsulated in the tuple M = ⟨S,A,T,O,R,γ⟩. flawless simulator would eliminate the sim-to-real gap. Yet,
Here, S and A represent the state and action spaces, with the reality often presents us with noisy, incomplete sensor data,
transition dynamics T(s′|s,a), the reward function R(s,a), and the simulator is far from perfect. Consequently, the sim-
andthediscountfactorγ ∈[0,1].Orepresentstheobservation to-realgapcanbeconceptualizedasintroducingthefollowing
space. types of noises to the true state:
Ourframeworkdistinctlyadaptstobothsimulatedandreal- • Environmental noise: Real-world environments are com-
world environments. In the simulation, the agent is afforded plex and unpredictable, presenting challenges such as
complete visibility of state s∈S. On the other hand, the real navigatingonchallengingterrainsorunexpectedexternal
world is plagued by partial observability. The agent only has forces applied to the robot.
access to partially observations o∈O, which provide incom- • Dynamicsnoise:Accuratelysimulatingthetruedynamics
pleteinformationaboutthestateduetosensorylimitationsand of the physical world is unfeasible, leading to oversim-
environmental noise. The policy π(a|o ≤t) maps the historical plificationsinsimulations,likeapproximationsofground
observations to a distribution over actions. As a result, the friction or object deformability.
agent operates within a discrete-time Partially Observable • Sensory noise: Physical sensors inherently contain mea-
Markov Decision Process (POMDP), necessitating decision- surement noise, for example, IMU drift and inaccuracies
making based on sporadic and partial data. The primary goal in joint position readings.
is to optimize this policy π to maximize the expected total • Maskingnoise:Someinformationmaybeunobtainablein
return J =E[R t]=E[(cid:80) tγtr t]. realityduetotheabsenceofspecificsensorsontherobot,
such as linear velocity and contact force measurements.
B. Humanoid Robot Hardware
This partial observability can be regarded as adding
We use two different sizes of humanoid robots for our masking noise[6, 10, 11].
experiments,asillustratedinFig.2.XBot-Sweighs38kgand
To mitigate these noises, we have developed a framework
stands 1.2 meters tall. The robot is equipped with 26 actuated
that firstly simulates noisy observations within the simulation
motors: 7 in each arm and 6 in each leg. XBot-L weighs 57
and subsequently employs an encoder-decoder architecture to
kg and stands 1.65 meters tall. The robot is equipped with
denoisetheseobservationsandaccuratelyrecoverthetruestate
54 actuated motors. For the purposes of this study, we focus
and dynamics, as depicted in Fig. 3.
on leg control, keeping the arm motors stationary. Each leg
Additionally, to mimic the constraints of partial observabil-
is powered by 6 motors: the yaw and roll joint motors with
ity, we mask out some information that is not observable on
a peak torque of 100N·m, the pitch and knee joint motors
realrobots.Weemulatetheenvironment,dynamics,andsensor
with 250N·m, and 2 ankle motors each providing 36N·m of
noises utilizing domain randomization (DR) methods[37]. In
torque (50N·m in XBot-L). The ankle motors, situated near
thisapproach,weintroducerandomperturbationstotheactual
the knee, are operated via a Closed Kinematic Chain Ankle
state and dynamics, such as angular velocity and PD param-
Mechanism as Fig. 2. This design aims to reduce leg inertia
eters. This procedure aligns with an observation model[9]
while ensuring an adequate degree of freedom.
expressed as o ∼ P (o |s ). We elaborate on this in
t Noise t t
Section IV-D.
IV. METHODS
An encoder-decoder architecture is designed to denoise the
A. Denoising World Model Learning
observations. The recurrent encoder extracts latent state z
t
Utilizing RL, various skills can be learned in simulation, from the robot’s historical noisy sensor observations. This
but the transition to real-world robots faces significant chal- latentrepresentationisthecoreofstateestimation,providinga
lenges due to the sim-to-real gap, which is mainly caused by rich, condensed summary of the robot’s situational awareness.
inaccurate simulation of the robot hardware and the limited Subsequently,thedecoderendeavorstoreconstructtherobot’s
information provided by onboard sensors. To overcome this true state from this latent state. The formal expression of this
barrier, we introduce Denoising World Model Learning model is given by:
(DWL), which enables online adaptation and state estimation (cid:20)(cid:90) (cid:21)
through representation learning. DWL is characterized by two P(˜s )=E P (˜s |z )·P (z |o ) (1)
t o≤t Decoder t t Encoder t ≤t
primary features: z
• An encoder-decoder architecture for world model learn- where P(˜s t) represents the estimation of the real state distri-
ing, effectively embedding partially observed historical bution P(s ) at time t. The encoder captures the conditional
tdistribution P of these latent variables given the noisy 3) Formulating the DWL Loss Function: The DWL frame-
Encoder
historical observations o , and the decoder P recon- work consolidates its learning objectives through a composite
≤t Decoder
structs the state from the latent representation z . loss function that integrates the aspects of state reconstruction
t
It is imperative to recognize that the dimension of o and policy optimization. The total loss function is a weighted
≤t
is much larger than that of z , implying z an effective sum of the denoising loss Equation(2), the policy loss Equa-
t t
informationbottleneck[36].ThisallowsDWLtoprioritizethe tion(3),andthevaluelossEquation(4),formallyexpressedas:
salient aspects of sensory input. Furthermore, to enhance the
efficiency and robustness of state estimation, sparsity within L =L +λ L +λ L , (5)
DWL denoise π π v v
the latent representation is sought [4]. This is achieved by
where λ and λ are the weighting factors for the policy
introducing an L1 regularization term in the latent domain. π v
and value loss components, respectively. This approach en-
Moreover, since there is no need to generate new data from
ables DWL to fine-tune the learning process, ensuring precise
the latent space, and it is in a pure denoising process, a
stateestimationandinformeddecision-makingbasedonthese
deterministic loss could be adopted instead of a variational
estimates. Reconstructing the state from masking loss and
loss [15]. The denoising loss is thus expressed as follows:
domain randomization noise, DWL demonstrates robustness
andadaptabilityincomplexreal-worldscenarios,aselaborated
L =∥˜s −s ∥ +λ ∥z ∥ (2)
denoise t t 2 r t 1
in Section V.
In conclusion, the integration of masking noise and domain
where λ represents the regularization coefficient. And s is
r t
randomization noise cultivates a robust latent space for end-
the full state. By incorporating privileged information into the
to-end state representation learning. When allied with policy
state, such as the ground’s friction coefficients, the actuator’s
gradient loss, this strategy propels a comprehensive approach
torque values, and terrain height scans, enables the agent to
tostateestimationandpolicyoptimization.Thisrefinedsystem
effectivelyconductonlineadaptationandsystemidentification.
is adept at narrowing the sim-to-real gap, effectively translat-
2) Policy Learning in DWL: Within the DWL framework,
ing simulation-trained models to real-world applications.
an Asymmetric Actor-Critic architecture is employed, draw-
ing upon the concept of privilege learning as elucidated in
B. Reward Formulation
previous works[3] [25]. This architecture is instrumental in
Our reward function guides the robot to follow velocity
enhancing data utilization during the training phase, proving
commands, maintain a stable gait, and ensure gentle contact,
particularly beneficial in real-world scenarios where direct
therebyenablingrobustlocomotionacrosschallengingterrains
state information is inaccessible. The actor component of the
and above-ground obstacles.
model computes its loss via the Proximal Policy Optimization
1) Composition of Rewards: The reward function is struc-
(PPO)[29], which is articulated as:
turedintofourkeycomponents:(1)velocitytracking,(2)peri-
(cid:20) π(a |o ) odicreward,(3)feettrajectorytracking,and(4)regularization
L
π
=min
π
(at |o≤t )Aπb(o ≤t,a t),
terms.Ourapproach,drawinginspirationfrompreviousworks
b t ≤t
(cid:18) (cid:19) (cid:21) (3) [31,26,32],employsaperiodicrewardtofacilitatenaturalgait
π(a |o )
clip
π
b(at
t
|o≤ ≤t t),c 1,c
2
Aπb(o ≤t,a t) l ϕe (a er ,n win )g. :=Fu ert xh per (cid:0)m −o wre ·, ∥w ee ∥2i (cid:1)n ,tr wod hu ec ree a et rr ea pc rk ei sn eg ntl sos ts hede tfi ran ce kd ina gs
error and w the error tolerance strength. Detailed calculations
where π denotes the target policy to be optimized, π is
b
can be found in the appendix, Section VII-B.
the behavior policy employed for data sampling, and c ,c
1 2
A novel aspect of our reward design addresses the sparse
represents the PPO clipping range. In the context of DWL’s
nature of contact force feedback. Rather than relying exclu-
encoder-decoderstructure,theactorpolicyisdefinedasπ(a |
t
sively on contact force, our system focuses on foot velocity
P (z |o )).Ontheotherhand,thecriticcouldusestate
Encoder t ≤t
tracking. This is achieved by designing foot trajectories that
information for calculations of the value function. Thus, the
incorporatepredeterminedvelocitiesupongroundcontact,thus
critic loss is given by the following formula:
ensuring a consistent and robust reward signal at each step.
Such a strategy promotes gentle ground contacts, leading to
L v =∥R t−V(s t)∥ 2, (4) reduced impact forces and enhancing the effectiveness of the
sim-to-real transfer.
Inthisformulation,R denotesthecumulativereturnattime 2) QuinticPolynomialFootTrajectoryInterpolation: Inour
t
t, and V(s ) is the value function as determined by the critic approach, we focus on refining the locomotion of humanoid
t
atstates .Theintegrationofprivilegedinformationwithinthe robotsthroughthestrategicdesignoffoottrajectories.Quintic
t
state representation arms the learning agent with the capacity polynomial interpolation is utilized to determine these trajec-
to make informed decisions. This approach aligns seamlessly tories, a method that is particularly effective in meeting the
with the DWL framework. It obviates the need for design precise kinematic requirements of a humanoid robot’s gait
privilege information by employing a unified state definition cycle. This technique not only facilitates smoother motion
for both state estimation and value function assessment. but also ensures accurate foot placement, a crucial factorDenoising Loss
Training in simulation
Privileged Information
Simulation
St Critic S˜ t
1.Observation 5. Added Mass
2.Base Vel 6. PD Gains
3. Friction 7. Feet movement Critic Loss
4. Contact Mask 8. Height Map Rt Vt
Decoder
Masking &DR
Real World
𝑶𝒕−𝒌+𝟏,𝑶𝒕−𝒌+𝟐,…, 𝑶𝒕−𝟏,𝑶𝒕
Encoder
1. Clock Input 2. Command (GRU) zs t Actor 𝒂 𝒕
3. Joint Pos 4. Joint Vel
5. Angular Vel 6. Eular Angle
Policy Gradient Loss
Observation History
Zero-shot sim-to-real
Fig.3:IllustrationoftheDenoisingWorldModelLearningFramework.Thisdiagramdetailstheinformationflowfromsensory
input to action output in both simulated and real-world settings. Raw observations are generated by adding masking and DR
noisetoprivilegedobservations.Thisisthenencodedintoalatentstateanddecodedtoreconstructthetruestateviaadenoising
process.
for maintaining stability and efficiency in humanoid walking 6) Final foot velocity at the end of the swing phase,
patterns. f′(T)=v , where v is the final velocity.
swing swing
Quintic polynomial interpolation offers an advantageous To deduce the coefficients a , a numerical optimization
0...5
approach in robotic motion planning due to its ability to technique is employed. Once the coefficients are ascertained,
provide smooth trajectories and precise control over velocity they succinctly characterize the foot’s vertical trajectory (i.e.,
and acceleration. The general form of a quintic polynomial is the swing height). Quintic polynomial interpolation is instru-
given by: mentalinensuringsoftlandingswithinhumanoidroboticloco-
motion,offeringgranularcontrolovertherobot’sswingheight,
(cid:88)
f(t)= a tk (6) footacceleration,andvelocityprofiles.Oneoptimizationresult
k
and the corresponding trajectory plot are shown in Appendix
k≤5
Table IV and Fig. 7.
Let t denote the time variable, and a 0,a 1,...,a 5 be the This method facilitates the manipulation of higher-order
coefficients that need to be determined. We denote the swing derivatives to attenuate impact forces at footfall. By adjusting
time by T. In our periodic reward design, one leg being in the coefficients of the quintic polynomial, trajectories are
the swing phase implies the other is in the stance phase. One craftedthatnotonlyelevatethefoottosurmountobstaclesbut
swing phase and one stance phase together complete a full alsomaintainsmoothmovementsandmitigatetheimpactupon
gait cycle. The trajectory of the robot’s foot during the swing contact. These fluent transitions enable a gentler touchdown,
phase is defined by f(t), which is shaped through a series of enhancing the robot’s stability and advancing the creation of
kinematic constraints at critical moments in the robot’s gait. efficient, adaptable robots capable of safely traversing diverse
These constraints are: terrains.
1) Initial foot height at t=0, given by f(0)=h , where
0
C. Configuration of DWL training process
h is the initial height.
0
2) Initial foot velocity at t=0, determined by f′(0)=v , InourDWLframework,asillustratedinFig.3,weutilizea
0
with v being the initial velocity. Gated Recurrent Unit (GRU)[7] for the encoding process and
0
3) Initial foot acceleration, represented by f′′(0) = acc , a two-layer Multilayer Perceptron for both the decoding and
0
where acc is the initial acceleration. actor networks. Details of the training configuration can be
0
4) Reaching maximum foot height at the midpoint of the found in the appendix section VII-C.
swing phase, f(T/2) = h , where h is the target The robot’s base pose is denoted by Pb, and the pose of
max max
feet height. the feet is denoted by Pf. The pose, which includes both
5) Final foot height at the end of the swing phase, f(T)= position and orientation, is represented as a six-dimensional
h , with h as the final height. vector [x,y,z,α,β,γ]. Here, x,y,z specifies the position,
swing swingTABLEII:OverviewofDomainRandomization.Presentedare
and α,β,γ represents the orientation in Euler angles. The
the domain randomization terms and the associated parameter
policynetworkinputsincludeproprioceptivesensordataanda
ranges.Additiverandomizationincrementstheparameterbya
periodic clock signal, represented as (sin(t),cos(t)), in addi-
tion to command inputs defining the desired velocities P˙ . value within the specified range while scaling randomization
x,y,γ
adjusts it by a multiplicative factor from the same range.
These observations are detailed in Table I. The state includes
privileged observations, which are typically unavailable to
Parameter Unit Range Operator
standard proprioceptive sensors on physical robots. This state JointPosition rad [-0.3,0.3] additive
also integrates the current step’s reward, combining a reward JointVelocity rad/s [-1,1] additive
model with the world model, which is expected to enhance AngularVelocity rad/s [-0.1,0.1] additive
Orientation rad [-0.1,0.1] additive
the encoder’s ability to capture the environmental context in SystemDelay ms [0,10] -
the latent space. Friction - [0.2,2.0] -
Other important components of the state are the Periodic MotorOffset rad [-0.05,0.05] additive
MotorStrength % [90,110] scaling
Stance Mask I(t), a binary indicator of expected foot contact
Payload kg [-5,20] additive
patterns for a periodic gait, and the Cycle Time, essential for PDFactors % [80,120] scaling
computingfoottrajectoriesasoutlinedin(6).TheFeetMove-
ment,indicatingboththepositionPf andvelocityofthefeet
xyz
P˙f . Also, the height scan provides an approximate height on the inherent advantages of the DWL framework to refine a
xyz
maptofurtherenhancetheestimationofthestate.Pleasenote robust locomotion policy, and Hyper-parameters can be found
thattheinputtoourpolicyincludesonlyproprioceptivesensor in Appendix TABLE VIII. Remarkably, the resulting policy is
data and does not incorporate any LiDAR or depth camera ready for direct application to the physical robot without the
information. The height scans listed in Table I are privileged need for further adjustments, exemplifying a seamless zero-
observations employed by the Critic during training. shot transfer from simulation to real-world deployment.
TABLE I: Summary of Observation Space. The table catego- D. Domain Randomization
rizesthecomponentsoftheobservationspaceintoobservation
To bridge the gap between simulation and reality, our
and state. The table also details their dimensions.
methodology emphasizes extensive domain randomization of
crucial dynamics parameters. This addresses the main sources
Components Dims Observation State
ClockInput 2 ✓ ✓ of real-world variability: environment noise, dynamics noise,
Commands 3 ✓ ✓ and sensory noise.
JointPosition 12 ✓ ✓ Randomizationcoversenvironmentalelementssuchasfloor
JointVelocity 12 ✓ ✓
AngularVelocity 3 ✓ ✓ friction, orientation, and robot-specific aspects like mass and
Orientation 3 ✓ ✓ Center of Mass positioning. Variations in motor parameters,
LastActions 12 ✓ ✓ including PD controller settings, are introduced to acclimate
BaseLinearVelocity 3 ✓
Frictions 1 ✓ the policy to a range of motor behaviors.
PushForce&Torques 6 ✓ Furthermore, we incorporate system latencies and inject
CycleTime 1 ✓ randomdeviationsintherobot’sCenterofMass,equippingthe
PeriodicStanceMask 2 ✓
Feetmovement 12 ✓ policytohandleunforeseendisturbancesinrealenvironments.
FeetContact 2 ✓ This thorough randomization strategy is essential for ensuring
BodyMass 1 ✓ the policy’s resilience and flexibility in actual deployment
CurrentReward 1 ✓
Torques 12 ✓ scenarios. Further specifics are presented in Table II.
HeightScan 96 ✓
V. EXPERIMENTS
Each action a ∈ R12 determines the actuators’ target Inthissection,wemainlyfocusontheperformanceofchal-
t
positions, followed by a Proportional-Derivative controller to lengingsettingsinbothindoorandoutdoorenvironments.The
translate into joint torques. Our control policy functions at benchmark comparisons discussed below were all conducted
100Hz, surpassing the usual rates in RL-based locomotion usingthesmallerhumanoidrobot,whichstands1.2meterstall.
strategies (50Hz), thus providing finer granularity and en- Additionally,wedeployedouralgorithmonalargerhumanoid
hanced precision in the robot’s movements. The internal PD robot, measuring 1.65 meters in height, as detailed in Fig. 6.
controller operates at a higher frequency of 500Hz.
A. Benchmark Comparison
For our simulations, we use the Isaac Gym environment
[23]. However, its lack of support for the closed kinematic For the empirical assessment of our approach, we conduct
chain employed in our ankle control necessitates the addition a series of experiments using both the XBot-S and XBot-L,
of two virtual motors within the simulator. We then remap applyingthelearnedpolicyinazero-shottransfertoreal-world
the joint targets to the actual motors for deployment. The settings. This deployment encompasses a range of intricate
policy optimization employs the DWL loss function (refer to and challenging terrains, testing the limits of the locomotive
Equation5)withAdamoptimizer[14].Thismethodcapitalizes capabilities of our robot. To the best of our knowledge, thisA) bipedal and humanoid robots, many previous RL-based
locomotioncontrolshaveutilizedpassiveanklestrategies.
We conduct comparisons with a DWL variant employing
passive ankle control1 (with K = 0 and K = 10) to
p d
benchmark against this common approach.
Our experimental scenarios are diverse, including tasks
such as snowy ground, up and down stairs, and disturbance
rejection. During these tasks, the robot’s arms are maintained
inastationarypositiontoisolatetheassessmentoflocomotive
performance. This experimental setup provides a testbed to
evaluate the versatility and robustness of our locomotion con-
trol strategy in real-world conditions. The subsequent bench-
mark comparisons are conducted exclusively using XBot-S to
Plane Uneven Plane ensure consistency in our evaluation.
B) TABLE III: Real robot testing across various terrains. Bold
Force
values is our DWL with ankle control, DWL is DWL with
p
passive ankle, PPO control ankle as well.
Algorithm Slope Stair-up Stair-down Irregular
PPO 80% 20% 60% 20%
DWLp 80% 20% 100% 40%
DWL 100% 100% 100% 100%
Start pushing Single leg support Stand still
B. Indoor Experimental Validation
Fig. 4: Dynamic adaptation of the ankle control mechanism. A comprehensive suite of real-world trials is executed to
A) The top image demonstrates the humanoid robot’s ankle assess the robustness and adaptability of our algorithm in
control system actively maintaining balance on uneven ter- controlling the humanoid robot across a series of challenging
rain. The associated torque plot reveals the control system’s terrains.Ourindoorexperimentsemployedfourdistinctterrain
adjustments during steady locomotion. B) The bottom image types with different difficulties, detailed as follows:
shows the system’s resilience to external perturbations during • Slope Transit (Fig. 1F): A sloped platform with a
staticstanding,wherethe2-DoFanklecontrolplaysakeyrole gradient of 0.25 to test the robot’s capacity to adeptly
in maintaining stability. shift from planar to inclined locomotion, encompassing
both ascent and descent.
represents the world first humanoid robot to robustly navigate • StairDescent(Fig.1D):Taskedwithadownwardtraver-
such complex environments using end-to-end reinforcement sal, the robot encountered stairs, each spanning 20cm
learning. wide and 10cm high, commencing from the summit.
Our evaluation framework includes comparisons with two • Stair Ascent (Fig. 1B): Matching the descent staircase
baseline methodologies, providing a comprehensive perspec- in dimension, the robot faces the challenge of ascending
tive on the effectiveness and advancements offered by our the stairs with limited sensor observations.
approach. In summary, we run three kinds of algorithms: • Irregular Terrain (Fig. 1E): A custom-designed land-
• DWLBaseline(ours):Thisbaselineinvolvestheapplica- scapewithvariableelevationsupto10cm,simulatingthe
tionoftheDWLpolicywithactiveanklecontrol.Forthis unpredictability of challenging terrains.
configuration, we set the PD gains of the ankle joints to The results of these experiments, quantified in Table III,
K p = 20 and K d = 5. The network architecture details reveal significant insights. When comparing PPO with an-
of DWL can be found in Appendix Table VI. The total kle control to our DWL framework, our approach shows a
trainable parameters of the DWL actor is about 320,192. significant improvement in walking performance, achieving
• PPO with Ankle Control: Here, we eliminate the de- 100%successratesacrossvariousterrains.Thishighlightsthe
noising loss component while retaining the other aspects superior sim-to-real capabilities of our method and its robust
of our methodology. This setup aims to underscore the adaptability to diverse terrains. For relatively simple tasks
enhanced adaptability of our approach in contrast to like navigating slopes and descending stairs, both PPO and
traditional methods. The network architecture details of the passive variant of DWL (DWL ) demonstrate competent
p
PPO can be found in Appendix Table VII. The total
trainable parameters of the PPO actor is about 333,312. 1Wespecifythat“passive”anklecontrolinourcontextmeanstheankle’s
movement isn’t directly controlled by the policy but responds based on
• DWL without Ankle Control: Given the complexity of predefined physical damping properties, which is different from active RL
modeling closed kinematic chain ankle mechanisms in agentdecision-making.success rates. However, in challenging situations such as
walkingonirregularterrainorclimbingstairs,DWLdistinctly
outperforms,showcasingtheadaptabilityandrobustnessofour
method.
C. Outdoor Experiments
In addition to extensive indoor testing on various com-
plex terrains, we also conducted prolonged outdoor walking
tests across diverse and challenging landscapes. We assessed
walking performance on different surfaces and conditions,
including cemented ground, brick roads, soil, and snowy
terrain. Our algorithm allowed the robot to exhibit stable
walking across the aforementioned varied road conditions.
Particularly noteworthy is the walking evaluation on snow-
covered terrain Fig. 1A , which presents a highly challenging
task.Snow,withitsdeformablenature,posesdifficultiesasthe
robot’sfeetcansinkintoit,asituationchallengingtosimulate.
Furthermore,snowsurfacestendtobeslippery,makingrobots
prone to sliding. Our DWL algorithm, however, demonstrates
remarkablestabilityduringprolongedwalksonsnowyterrain,
affirming the robustness and adaptability of our algorithm to
diverse terrains.
D. Robustness Testing
Domain randomization is a common approach to achieving
a robust controller. However, its effectiveness is often limited
bythesim-to-realgapandtheimpracticalityofaccountingfor
every possible real-world scenario.
In our framework, the agent is designed to predict the true
Fig. 5: State estimation results of DWL-facilitated complex
state, thereby empowering the controller to rapidly adapt to
terrain traversing and adaptation. This sequence of images
a variety of situations. For example, if there is a tendency
visualizesthemodel’spredictionoffootcontact,basevelocity,
to fall, the controller can quickly recognize this and act to
and heightmap when the humanoid robot navigates through a
maintain balance. To assess the robustness of our controller,
slope and stairs. The results demonstrate DWL’s effectiveness
we conducted the following experiments:
in state estimation and online adaption.
1) Mass Displacement: As shown in Fig. 1G, the robot
carriedabagintowhichweprogressivelyaddedheavyobjects
while it was walking. Even with an additional weightof up to
robot, specifically focusing on its performance in terrain
15kg, over a third of the robot’s weight, it managed to sustain
traversing. Our results are shown in Fig. 5.
stable locomotion.
2) Heavy load: We executed two experiments, depicted 1) Terrain Height Scan Prediction and Gait Adaptation:
in Fig. 1H. It successfully walked with an additional load OurfindingshighlighttheDWL’sremarkableabilitytopredict
of up to 20kg, albeit with a slightly lowered height due to terrain height, an essential factor for terrain adaptability.
the added weight. Also, we attached the robot’s hand effector Utilizing only proprioceptive inputs, rather than relying on
to a loaded cart Fig. 1I, the robot was able to push a cart LiDAR or depth cameras, our system is capable of estimating
loaded with 60kg, demonstrating the controller’s adaptability a terrain’s rough profile. At first glance, this might seem im-
to handle significant loads. plausible,butourapproachcandiscernthegeneraltrendofthe
3) Push Recovery: During our experimentation, we sub- terrain,asexemplifiedinFig.5.DWL’sinternalmodeladeptly
jected the robot to external forces from multiple directions encodes environmental features with distinct separability, thus
while it was executing continuous standing commands. These facilitatingaccurateterrainrecognition.Therobotcanidentify
tests were carried out on both flat(Fig. 1C) and sloped ter- whether it is walking up a slope or descending stairs. While
rain(Fig. 4), with the robot successfully maintaining its stance precise shape prediction remains challenging, even a rough
in both conditions. estimate proves immensely useful. This capability crucially
affectstherobot’sgait,asobservedwhenittransitsfromslope
VI. RESULTANALYSIS
to stairs. Such gait modifications are imperative not just for
In this section, we delve into the empirical results obtained navigatingobstaclesbutalsoforensuringbalanceandstability
from the deployment of the DWL framework on a humanoid across diverse terrains.2) FootContactDetectionandItsImplications: Asdemon- A B C
strated in Fig. 5, foot contact patterns indicate a correlation
withthetypeofterrainencountered.Inhumanoidlocomotion,
especiallyduringsingle-legsupportphases,accuratedetection
offootcontactsisessentialforstability.TheDWLframework
aids in predicting these contact instances, thereby improving
theplanningoflegswingtrajectoriesandfacilitatingeffective
obstacle avoidance. The frequency and pattern of foot con-
tacts vary dynamically, guided by state estimation, leading to
crucial gait adjustments and adaptations for complex terrain
navigation. D E F
3) Velocity Estimation: Velocity prediction, particularly
linear velocity, is challenging to obtain directly from pro-
prioceptive sensors but is critical for successful locomotion.
DWL effectively estimates velocity states within a unified
framework,addressingchallengeslikeIMUangularyawdrift,
as shown in Appendix Fig. 10. This estimation enhances
command following and prevents yaw deviations. The con-
gruence between actual and estimated velocities observed in
our experiments significantly aids in the robot’s locomotion,
ensuring smoother and more predictable movements. Since
real states cannot be directly obtained in real-world settings, Fig.6:WedeployedtheDWLagentonalargehumanoidrobot
we validate our state prediction using a sim2sim transfer. We (1.65 meters tall, 57 kilograms) to evaluate its performance
testedourpolicyinMuJoCoandcomparedthestateestimation in various scenarios, including highly challenging deformable
with the ground truth. The predictions of linear velocity and conditions. The active control of the 2-DOF ankle joints
Euler yaw are displayed in Appendix Fig.12 and Fig. 11. proved particularly beneficial for maintaining balance while
The mean square error (MSE) of the forward velocity standing,recoveringfromexternaldisturbances,andtraversing
estimation in 60 seconds is 0.046 in the sim-to-sim scenario, through intricate and deformable terrain.
while the IMU drift was reduced by around 87% in real-
worldexperiments.Thesecomparisonsclearlydemonstratethe We achieved success in extensive real-world experiments in
accurate state estimation capabilities of the DWL algorithm. various complex environments such as snowy land, stairs,
4) Benefits and Importance of Ankle Control: By allowing deformable ground, and irregular surfaces. Demonstrating the
activated control of both the two freedoms of the ankle, the world’s first humanoid robot to master challenging terrains
robot is empowered to traverse complex terrains and resist with end-to-end RL and zero-shot sim-to-real transfer. In-
extra forces. The role of ankle control was pivotal, as it depth result analysis indicated the effectiveness of the state
allowstherobottopreservebalance,eveninsingle-legsupport estimation capability and the importance of the active 2-DoF
scenarios, generating human-like stability. Noticeably, ankle anklecontrol.Inthefuture,visualinformationwillbeaddedto
control becomes particularly pronounced when walking on enable more efficient navigation in challenging terrains while
irregular blocks and ascending stairs, as in Fig. 4, and also maintaining robustness.
on deformable ground as shown in Fig. 6. Without ankle
control,therobot’sfeetdeformedsignificantlyuponcontacting
the uneven ground, failing to recover. This was coupled
with inadequate ankle joint contact forces, raising the risk
of imbalance and falls during foot placements. In contrast,
our ankle control method navigates these terrains with ease
by adaptive contact forces at the ankle joints, as shown in the
torqueplotinFig.4,enablingtherobot’sadaptabilitytovaried
terrains.
VII. CONCLUSION
Inthiswork,weproposedDenoisingWorldModelLearning
(DWL)forcomplexhumanoidrobotlocomotionskilllearning.
The framework first masked out privileged information and
injected appropriate noise into the true state observed in
the simulation. It then designs an auto-encoder architecture
to denoise the observations and reconstruct the true state.REFERENCES volume 1, pages 11–24. Springer-Verlag, 1974.
[14] DiederikPKingmaandJimmyBa. Adam:Amethodfor
[1] Ananye Agarwal, Ashish Kumar, Jitendra Malik, and stochasticoptimization. arXivpreprintarXiv:1412.6980,
Deepak Pathak. Legged locomotion in challenging ter- 2014.
rains using egocentric vision. In Conference on Robot [15] Diederik P Kingma and Max Welling. Auto-encoding
Learning, pages 403–415. PMLR, 2023. variationalbayes. arXivpreprintarXiv:1312.6114,2013.
[2] Min Sung Ahn. Development and Real-Time [16] Scott Kuindersma, Robin Deits, Maurice Fallon, Andre´s
Optimization-basedControlofaFull-sizedHumanoidfor Valenzuela, Hongkai Dai, Frank Permenter, Twan
DynamicWalkingandRunning. UniversityofCalifornia, Koolen, Pat Marion, and Russ Tedrake. Optimization-
Los Angeles, 2023. based locomotion planning, estimation, and control de-
[3] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp sign for the atlas humanoid robot. Autonomous robots,
Kra¨henbu¨hl. Learning by cheating. In Conference on 40:429–455, 2016.
Robot Learning, pages 66–75. PMLR, 2020. [17] Ashish Kumar, Zhongyu Li, Jun Zeng, Deepak Pathak,
[4] Tianlong Chen, Zhenyu Zhang, Pengjun Wang, Santosh Koushil Sreenath, and Jitendra Malik. Adapting rapid
Balachandra, Haoyu Ma, Zehao Wang, and Zhangyang motor adaptation for bipedal robots. In 2022 IEEE/RSJ
Wang. Sparsity winning twice: Better robust gener- International Conference on Intelligent Robots and Sys-
alization from more efficient training. arXiv preprint tems (IROS), pages 1161–1168. IEEE, 2022.
arXiv:2202.09844, 2022. [18] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen,
[5] MatthewChignoli,DonghyunKim,ElijahStanger-Jones, VladlenKoltun,andMarcoHutter.Learningquadrupedal
and Sangbae Kim. The mit humanoid robot: Design, locomotion over challenging terrain. Science robotics, 5
motion planning, and control for acrobatic behaviors. (47):eabc5986, 2020.
In 2020 IEEE-RAS 20th International Conference on [19] Timothe´e Lesort, Natalia D´ıaz-Rodr´ıguez, Jean-Franois
Humanoid Robots (Humanoids), pages 1–8. IEEE, 2021. Goudou, and David Filliat. State representation learning
[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and for control: An overview. Neural Networks, 108:379–
Kristina Toutanova. Bert: Pre-training of deep bidirec- 392, 2018.
tional transformers for language understanding. arXiv [20] Junheng Li and Quan Nguyen. Multi-contact mpc for
preprint arXiv:1810.04805, 2018. dynamicloco-manipulationonhumanoidrobots. In2023
[7] Rahul Dey and Fathi M Salem. Gate-variants of gated American Control Conference (ACC), pages 1215–1220.
recurrent unit (gru) neural networks. In 2017 IEEE 60th IEEE, 2023.
internationalmidwestsymposiumoncircuitsandsystems [21] ZhongyuLi,XuxinCheng,XueBinPeng,PieterAbbeel,
(MWSCAS), pages 1597–1600. IEEE, 2017. Sergey Levine, Glen Berseth, and Koushil Sreenath.
[8] Xinyang Gu, Yen-Jen Wang, and Jianyu Chen. Reinforcement learning for robust parameterized loco-
Humanoid-gym: Reinforcement learning for humanoid motion control of bipedal robots. In 2021 IEEE Interna-
robot with zero-shot sim2real transfer. arXiv preprint tional Conference on Robotics and Automation (ICRA),
arXiv:2404.05695, 2024. pages 2811–2817. IEEE, 2021.
[9] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben [22] Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey
Villegas, David Ha, Honglak Lee, and James Davidson. Levine, Glen Berseth, and Koushil Sreenath. Robust and
Learning latent dynamics for planning from pixels. In versatile bipedal jumping control through multi-task re-
International conference on machine learning, pages inforcement learning. arXiv preprint arXiv:2302.09450,
2555–2565. PMLR, 2019. 2023.
[10] BoHan,JiangchaoYao,GangNiu,MingyuanZhou,Ivor [23] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong
Tsang, Ya Zhang, and Masashi Sugiyama. Masking: A Guo, Michelle Lu, Kier Storey, Miles Macklin, David
newperspectiveofnoisysupervision. Advancesinneural Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa,
information processing systems, 31, 2018. et al. Isaac gym: High performance gpu-based
[11] KaimingHe,XinleiChen,SainingXie,YanghaoLi,Piotr physics simulation for robot learning. arXiv preprint
Dolla´r, and Ross Girshick. Masked autoencoders are arXiv:2108.10470, 2021.
scalablevisionlearners. InProceedingsoftheIEEE/CVF [24] Josh Merel, Leonard Hasenclever, Alexandre Galashov,
conference on computer vision and pattern recognition, Arun Ahuja, Vu Pham, Greg Wayne, Yee Whye Teh,
pages 16000–16009, 2022. andNicolasHeess. Neuralprobabilisticmotorprimitives
[12] JeminHwangbo,JoonhoLee,AlexeyDosovitskiy,Dario for humanoid control. arXiv preprint arXiv:1811.11711,
Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco 2018.
Hutter. Learning agile and dynamic motor skills for [25] LerrelPinto,MarcinAndrychowicz,PeterWelinder,Wo-
legged robots. Science Robotics, 4(26):eaau5872, 2019. jciech Zaremba, and Pieter Abbeel. Asymmetric actor
[13] Ichiro Kato. Information-power machine with senses critic for image-based robot learning. arXiv preprint
and limbs (wabot 1). In First CISM-IFToMM Symp. arXiv:1710.06542, 2017.
on Theory and Practice of Robots and Manipulators, [26] Ilija Radosavovic, Tete Xiao, Bike Zhang, Trevor Dar-rell, Jitendra Malik, and Koushil Sreenath. Learning Automation (ICRA), pages 222–227. IEEE, 2014.
humanoid locomotion with transformers. arXiv preprint [39] Chuanyu Yang, Kai Yuan, Shuai Heng, Taku Komura,
arXiv:2303.03381, 2023. and Zhibin Li. Learning natural locomotion behaviors
[27] Jacob Reher, Wen-Loong Ma, and Aaron D Ames. Dy- for humanoid robots using human bias. IEEE Robotics
namicwalkingwithcomplianceonacassiebipedalrobot. and Automation Letters, 5(2):2610–2617, 2020.
In201918thEuropeanControlConference(ECC),pages
2589–2595. IEEE, 2019.
[28] Nikita Rudin, David Hoeller, Philipp Reist, and Marco
Hutter. Learning to walk in minutes using massively
parallel deep reinforcement learning. In Conference on
Robot Learning, pages 91–100. PMLR, 2022.
[29] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford,andOlegKlimov. Proximalpolicyoptimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.
[30] Luis Sentis and Oussama Khatib. A whole-body control
framework for humanoids operating in human environ-
ments. In Proceedings 2006 IEEE International Con-
ference on Robotics and Automation, 2006. ICRA 2006.,
pages 2641–2648. IEEE, 2006.
[31] Jonah Siekmann, Yesh Godse, Alan Fern, and Jonathan
Hurst. Sim-to-real learning of all common bipedal
gaits via periodic reward composition. In 2021 IEEE
International Conference on Robotics and Automation
(ICRA), pages 7309–7315. IEEE, 2021.
[32] Jonah Siekmann, Kevin Green, John Warila, Alan Fern,
and Jonathan Hurst. Blind bipedal stair traversal
via sim-to-real reinforcement learning. arXiv preprint
arXiv:2105.08328, 2021.
[33] Jean-Pierre Sleiman, Farbod Farshidian, Maria Vittoria
Minniti, and Marco Hutter. A unified mpc framework
for whole-body dynamic locomotion and manipulation.
IEEE Robotics andAutomation Letters, 6(3):4688–4695,
2021.
[34] Tomomichi Sugihara, Yoshihiko Nakamura, and Hi-
rochika Inoue. Real-time humanoid motion generation
through zmp manipulation based on inverted pendulum
control. InProceedings2002IEEEInternationalConfer-
enceonRoboticsandAutomation(Cat.No.02CH37292),
volume 2, pages 1404–1409. IEEE, 2002.
[35] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen,
Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent
Vanhoucke. Sim-to-real: Learning agile locomotion for
quadruped robots. arXiv preprint arXiv:1804.10332,
2018.
[36] Naftali Tishby, Fernando C Pereira, and William Bialek.
The information bottleneck method. arXiv preprint
physics/0004057, 2000.
[37] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider,
Wojciech Zaremba, and Pieter Abbeel. Domain ran-
domization for transferring deep neural networks from
simulation to the real world. In 2017 IEEE/RSJ in-
ternational conference on intelligent robots and systems
(IROS), pages 23–30. IEEE, 2017.
[38] Patrick M Wensing and David E Orin. Development
of high-span running long jumps for humanoids. In
2014 IEEE International Conference on Robotics andAPPENDIX As illustrated in Fig. 8, this indicator is set to 1 during the
planned contact phase and to 0 during the planned swing
A. Quintic Polynomial Foot Trajectory
phase, alternating for each leg throughout a locomotion cycle.
In Section.IV-B, we introduced the foot trajectory interpo-
The periodic rewards are formulated as follows:
lation method we designed. Through this approach, a smooth
foot trajectory can be generated as a reference, and a tracking rperiodic(t)=I (t)·F +I (t)·F (7)
reward is utilized to encourage the robot’s feet to follow the Force L L R R
generatedtrajectory.Table.IVpresentsthespecificpolynomial
parameters and optimization conditions for generating foot
rperiodic(t)=(1−I (t))·P˙f +(1−I (t))·P˙f (8)
trajectories. Figure.7 illustrates the curves of height, velocity, velocity L L R R
and acceleration for a generated foot trajectory. It can be In this context, the symbol F denotes force, while L and
observed that smoothness is maintained across the positional, R refer to the left and right foot, respectively. For clarity,
velocity, and acceleration aspects. we omit the scaling and clipping factors typically applied to
the forces and velocities, which are necessary due to their
TABLE IV: Quintic Polynomial Foot Trajectory Parameters differing magnitudes. For instance, forces, which can reach
f(t)=(cid:80) k≤5a ktk and Optimization Conditions magnitudes of hundreds, are scaled down by a factor of 400
and subsequently clipped to the range [0, 1] to maintain
TrajectoryParameters Optimizationconstraints
Coefficient Value Objection Value consistency in the reward function’s scale.
a5 9.6 h0 0.0
a4 12.0 hT 0.0
a3 -18.8 v0 0.1
a2 5.0 vT 0.0
a1 0.1 hmax 0.1
a0 0.0 T 0.5
Fig. 8: The stance mask for the left (L) and right (R) feet,
where 0 indicates the swing phase and 1 indicates the stance
phase is expected.
The reward function is summarized in Table V. It is im-
portanttonotethatthecommandsCMD areintentionally
z,γ,β
set to zero. This is because we do not control them; rather,
we aim to maintain their values at zero to ensure stable
and smooth walking. Therefore, the total reward at any time
Fig. 7: The foot trajectory, modeled through quintic polyno- step t is computed as the weighted sum of individual reward
(cid:80)
mial interpolation, is detailed in TABLE IV. It illustrates the components,expressedasr t = ir i·µ i,whereµ i represents
critical velocity and acceleration constraints, ensuring a tra- the weighting factor for each reward component r i.
jectory that facilitates seamless motion, harmonizing stability
C. Training Details
and gait efficiency in the robot’s movement dynamics.
SectionIV-Aprovidesadetailedexplanationofourdesigned
Denoising World Model Learning(DWL) method, and here in
B. Reward Function Table.VIII,weoutlinethehyperparametersforDWL.Table.VI
presents the specific network architecture we utilized. It in-
For periodic reward design, inspired by previous work
cludes the encoder and decoder for DWL, as well as the actor
[31, 32], our objective is to construct a reward function that
and critic.
leverages the distinct roles of foot forces and foot veloci-
ties. Specifically, the function aims to promote higher foot D. Additional Experimental Results
velocities during the swing phase and reasonable foot forces
We present further results and demonstrations of our exper-
during the stance phase of locomotion. We introduce a binary
iments, as illustrated in Fig. 9.
feet contact indicator, I(t), termed the periodic stance mask.TABLE V: In defining the reward function, we use a tracking
TABLE VIII: Hyperparameters of DWL.
error metric denoted by ϕ(e,w). This metric is expressed as
ϕ(e,w) := exp(−w·∥e∥2), where e represents the tracking
Parameter Value
error, and w is the associated weight. The target base height NumberofEnvironments 12288
is set to 0.7m. NumberTrainingEpochs 2
Batchsize 12288×24
EpisodeLength 2400steps
Reward Equation(ri) rewardscale(µi)
DiscountFactor 0.995
ALi nn g. .v ve elo loc cit iy tytr tra ac ck kin ing
g
ϕϕ (( PP ˙˙ αx bb y βz γ− −C CM MD Dx αy βz γ, ,5 7)
)
1 1. .0
0
G E c1nA trE opd yis Rco eu gn ut laf ra ic zt ao tr ionCoefficient 00 0.. 0 .9 805 5
Orientationtracking ϕ(P αb β,5) 1.0 c2 1.2
Baseheighttracking ϕ(P zb−0.7,10) 0.5 Learningrate 1e-5
PeriodicForce rperiodic(t) 1.0 regularizationcoefficientλr 0.002
Force
PeriodicVelocity rperiodic(t) 1.0 policycoefficientλπ 5
Footheighttracking
ϕv (e Plo zfci −ty
ft,5) 1.0
valuecoefficientλv 5
Footveltracking ϕ(P˙ zf −f˙ t,3) 0.5
DefaultJoint ϕ(θt−θ0,2) 0.2
EnergyCost |τ||θ˙| -0.0001
ActionSmoothness ∥at−2at−1+at−2∥2 -0.01
Feetmovements ∥P˙ zf∥2+∥P¨ zf∥2 -0.01
Largecontact CLIP(FL,R−400,0,100) -0.01
TABLE VI: DWL Network Architecture Details
Component Configuration
Encoder
RNN memory(0) GRU(47→256)
emb model(1) Linear(256→256)
emb model(2) ELU(alpha=1.0)
emb model(3) Linear(256→24)
Decoder
denoise net(0) Linear(24→64)
denoise net(1) ELU(alpha=1.0)
denoise net(2) Linear(64→184)
Actor
Fig. 9: Additional Experiment Setups and Results
policy net(0) Linear(24→48)
policy net(1) ELU(alpha=1.0)
policy net(2) Linear(48→12)
Critic
Critic Net(0) Linear(184→512)
Critic Net(1) ELU(alpha=1.0)
Critic Net(2) Linear(512→512)
Critic Net(3) ELU(alpha=1.0)
Critic Net(4) Linear(512→256)
Critic Net(5) ELU(alpha=1.0)
Critic Net(6) Linear(256→1)
TABLE VII: PPO Network Architecture Details
Component Configuration
Actor
RNN memory(0) GRU(47→256)
policy net(1) Linear(256→256)
policy net(2) ELU(alpha=1.0)
policy net(3) Linear(256→128)
policy net(4) ELU(alpha=1.0)
policy net(5) Linear(128→12)
Critic
Critic Net(0) Linear(184→512)
Critic Net(1) ELU(alpha=1.0)
Critic Net(2) Linear(512→512) Fig. 10: Even when the robot is stationary, the actual IMU
Critic Net(3) ELU(alpha=1.0)
readings exhibit the phenomenon of IMU drift. Conversely,
Critic Net(4) Linear(512→256)
Critic Net(5) ELU(alpha=1.0) the DWL algorithm is capable of predicting real IMU data
Critic Net(6) Linear(256→1) and mitigating the IMU drift by approximately 87%.Fig. 11: To emulate human command inputs, we varied the
commandinputevery1000steps(10seconds).IntheMuJoCo
simulation environment, the Euler yaw angle predicted by the
DWL algorithm is close to the ground truth with MSE 0.074.
Fig. 12: Comparison of Estimated and Actual Velocity Values
in MuJoCo: The forward velocity predicted by the DWL
algorithm closely approximates the ground truth with MSE
0.046.