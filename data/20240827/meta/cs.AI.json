[
    {
        "title": "Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning",
        "authors": "Xinyang GuYen-Jen WangXiang ZhuChengming ShiYanjiang GuoYichen LiuJianyu Chen",
        "links": "http://arxiv.org/abs/2408.14472v1",
        "entry_id": "http://arxiv.org/abs/2408.14472v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14472v1",
        "summary": "Humanoid robots, with their human-like skeletal structure, are especially\nsuited for tasks in human-centric environments. However, this structure is\naccompanied by additional challenges in locomotion controller design,\nespecially in complex real-world environments. As a result, existing humanoid\nrobots are limited to relatively simple terrains, either with model-based\ncontrol or model-free reinforcement learning. In this work, we introduce\nDenoising World Model Learning (DWL), an end-to-end reinforcement learning\nframework for humanoid locomotion control, which demonstrates the world's first\nhumanoid robot to master real-world challenging terrains such as snowy and\ninclined land in the wild, up and down stairs, and extremely uneven terrains.\nAll scenarios run the same learned neural network with zero-shot sim-to-real\ntransfer, indicating the superior robustness and generalization capability of\nthe proposed method.",
        "updated": "2024-08-26 17:59:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14472v1"
    },
    {
        "title": "K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences",
        "authors": "Zhikai LiXuewen LiuDongrong FuJianquan LiQingyi GuKurt KeutzerZhen Dong",
        "links": "http://arxiv.org/abs/2408.14468v1",
        "entry_id": "http://arxiv.org/abs/2408.14468v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14468v1",
        "summary": "The rapid advancement of visual generative models necessitates efficient and\nreliable evaluation methods. Arena platform, which gathers user votes on model\ncomparisons, can rank models with human preferences. However, traditional Arena\nmethods, while established, require an excessive number of comparisons for\nranking to converge and are vulnerable to preference noise in voting,\nsuggesting the need for better approaches tailored to contemporary evaluation\nchallenges. In this paper, we introduce K-Sort Arena, an efficient and reliable\nplatform based on a key insight: images and videos possess higher perceptual\nintuitiveness than texts, enabling rapid evaluation of multiple samples\nsimultaneously. Consequently, K-Sort Arena employs K-wise comparisons, allowing\nK models to engage in free-for-all competitions, which yield much richer\ninformation than pairwise comparisons. To enhance the robustness of the system,\nwe leverage probabilistic modeling and Bayesian updating techniques. We propose\nan exploration-exploitation-based matchmaking strategy to facilitate more\ninformative comparisons. In our experiments, K-Sort Arena exhibits 16.3x faster\nconvergence compared to the widely used ELO algorithm. To further validate the\nsuperiority and obtain a comprehensive leaderboard, we collect human feedback\nvia crowdsourced evaluations of numerous cutting-edge text-to-image and\ntext-to-video models. Thanks to its high efficiency, K-Sort Arena can\ncontinuously incorporate emerging models and update the leaderboard with\nminimal votes. Our project has undergone several months of internal testing and\nis now available at https://huggingface.co/spaces/ksort/K-Sort-Arena",
        "updated": "2024-08-26 17:58:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14468v1"
    },
    {
        "title": "Temporal Ensemble Logic",
        "authors": "Guo-Qiang Zhang",
        "links": "http://arxiv.org/abs/2408.14443v1",
        "entry_id": "http://arxiv.org/abs/2408.14443v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14443v1",
        "summary": "We introduce Temporal Ensemble Logic (TEL), a monadic, first-order modal\nlogic for linear-time temporal reasoning. TEL includes primitive temporal\nconstructs such as ``always up to $t$ time later'' ($\\Box_t$), ``sometimes\nbefore $t$ time in the future'' ($\\Diamond_t$), and ``$t$-time later''\n$\\varphi_t$. TEL has been motivated from the requirement for rigor and\nreproducibility for cohort specification and discovery in clinical and\npopulation health research, to fill a gap in formalizing temporal reasoning in\nbiomedicine. In this paper, we first introduce TEL in a general set up, with\ndiscrete and dense time as special cases. We then focus on the theoretical\ndevelopment of discrete TEL on the temporal domain of positive integers\n$\\mathbb{N}^+$, denoted as ${\\rm TEL}_{\\mathbb{N}^+}$. ${\\rm\nTEL}_{\\mathbb{N}^+}$ is strictly more expressive than the standard monadic\nsecond order logic, characterized by B\\\"{u}chi automata. We present its formal\nsemantics, a proof system, and provide a proof for the undecidability of the\nsatisfiability of ${\\rm TEL}_{\\mathbb{N}^+}$. We also discuss expressiveness\nand decidability fragments for ${\\rm TEL}_{\\mathbb{N}^+}$, followed by\nillustrative applications.",
        "updated": "2024-08-26 17:36:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14443v1"
    },
    {
        "title": "Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification",
        "authors": "Mahrukh AwanAsmar NadeemMuhammad Junaid AwanArmin MustafaSyed Sameed Husain",
        "links": "http://arxiv.org/abs/2408.14441v1",
        "entry_id": "http://arxiv.org/abs/2408.14441v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14441v1",
        "summary": "Exploiting both audio and visual modalities for video classification is a\nchallenging task, as the existing methods require large model architectures,\nleading to high computational complexity and resource requirements. Smaller\narchitectures, on the other hand, struggle to achieve optimal performance. In\nthis paper, we propose Attend-Fusion, an audio-visual (AV) fusion approach that\nintroduces a compact model architecture specifically designed to capture\nintricate audio-visual relationships in video data. Through extensive\nexperiments on the challenging YouTube-8M dataset, we demonstrate that\nAttend-Fusion achieves an F1 score of 75.64\\% with only 72M parameters, which\nis comparable to the performance of larger baseline models such as\nFully-Connected Late Fusion (75.96\\% F1 score, 341M parameters). Attend-Fusion\nachieves similar performance to the larger baseline model while reducing the\nmodel size by nearly 80\\%, highlighting its efficiency in terms of model\ncomplexity. Our work demonstrates that the Attend-Fusion model effectively\ncombines audio and visual information for video classification, achieving\ncompetitive performance with significantly reduced model size. This approach\nopens new possibilities for deploying high-performance video understanding\nsystems in resource-constrained environments across various applications.",
        "updated": "2024-08-26 17:33:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14441v1"
    },
    {
        "title": "Sparsity-Aware Hardware-Software Co-Design of Spiking Neural Networks: An Overview",
        "authors": "Ilkin AliyevKama SvobodaTosiron AdegbijaJean-Marc Fellous",
        "links": "http://arxiv.org/abs/2408.14437v1",
        "entry_id": "http://arxiv.org/abs/2408.14437v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14437v1",
        "summary": "Spiking Neural Networks (SNNs) are inspired by the sparse and event-driven\nnature of biological neural processing, and offer the potential for\nultra-low-power artificial intelligence. However, realizing their efficiency\nbenefits requires specialized hardware and a co-design approach that\neffectively leverages sparsity. We explore the hardware-software co-design of\nsparse SNNs, examining how sparsity representation, hardware architectures, and\ntraining techniques influence hardware efficiency. We analyze the impact of\nstatic and dynamic sparsity, discuss the implications of different neuron\nmodels and encoding schemes, and investigate the need for adaptability in\nhardware designs. Our work aims to illuminate the path towards embedded\nneuromorphic systems that fully exploit the computational advantages of sparse\nSNNs.",
        "updated": "2024-08-26 17:22:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14437v1"
    }
]