[
    {
        "title": "A Practitioner's Guide to Continual Multimodal Pretraining",
        "authors": "Karsten RothVishaal UdandaraoSebastian DziadzioAmeya PrabhuMehdi ChertiOriol VinyalsOlivier HénaffSamuel AlbanieMatthias BethgeZeynep Akata",
        "links": "http://arxiv.org/abs/2408.14471v1",
        "entry_id": "http://arxiv.org/abs/2408.14471v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14471v1",
        "summary": "Multimodal foundation models serve numerous applications at the intersection\nof vision and language. Still, despite being pretrained on extensive data, they\nbecome outdated over time. To keep models updated, research into continual\npretraining mainly explores scenarios with either (1) infrequent,\nindiscriminate updates on large-scale new data, or (2) frequent, sample-level\nupdates. However, practical model deployment often operates in the gap between\nthese two limit cases, as real-world applications often demand adaptation to\nspecific subdomains, tasks or concepts -- spread over the entire, varying life\ncycle of a model. In this work, we complement current perspectives on continual\npretraining through a research test bed as well as provide comprehensive\nguidance for effective continual model updates in such scenarios. We first\nintroduce FoMo-in-Flux, a continual multimodal pretraining benchmark with\nrealistic compute constraints and practical deployment requirements,\nconstructed over 63 datasets with diverse visual and semantic coverage. Using\nFoMo-in-Flux, we explore the complex landscape of practical continual\npretraining through multiple perspectives: (1) A data-centric investigation of\ndata mixtures and stream orderings that emulate real-world deployment\nsituations, (2) a method-centric investigation ranging from simple fine-tuning\nand traditional continual learning strategies to parameter-efficient updates\nand model merging, (3) meta learning rate schedules and mechanistic design\nchoices, and (4) the influence of model and compute scaling. Together, our\ninsights provide a practitioner's guide to continual multimodal pretraining for\nreal-world deployment. Our benchmark and code is here:\nhttps://github.com/ExplainableML/fomo_in_flux.",
        "updated": "2024-08-26 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14471v1"
    },
    {
        "title": "A domain decomposition-based autoregressive deep learning model for unsteady and nonlinear partial differential equations",
        "authors": "Sheel NidhanHaoliang JiangLalit GhuleClancy UmphreyRishikesh RanadeJay Pathak",
        "links": "http://arxiv.org/abs/2408.14461v1",
        "entry_id": "http://arxiv.org/abs/2408.14461v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14461v1",
        "summary": "In this paper, we propose a domain-decomposition-based deep learning (DL)\nframework, named transient-CoMLSim, for accurately modeling unsteady and\nnonlinear partial differential equations (PDEs). The framework consists of two\nkey components: (a) a convolutional neural network (CNN)-based autoencoder\narchitecture and (b) an autoregressive model composed of fully connected\nlayers. Unlike existing state-of-the-art methods that operate on the entire\ncomputational domain, our CNN-based autoencoder computes a lower-dimensional\nbasis for solution and condition fields represented on subdomains. Timestepping\nis performed entirely in the latent space, generating embeddings of the\nsolution variables from the time history of embeddings of solution and\ncondition variables. This approach not only reduces computational complexity\nbut also enhances scalability, making it well-suited for large-scale\nsimulations. Furthermore, to improve the stability of our rollouts, we employ a\ncurriculum learning (CL) approach during the training of the autoregressive\nmodel. The domain-decomposition strategy enables scaling to out-of-distribution\ndomain sizes while maintaining the accuracy of predictions -- a feature not\neasily integrated into popular DL-based approaches for physics simulations. We\nbenchmark our model against two widely-used DL architectures, Fourier Neural\nOperator (FNO) and U-Net, and demonstrate that our framework outperforms them\nin terms of accuracy, extrapolation to unseen timesteps, and stability for a\nwide range of use cases.",
        "updated": "2024-08-26 17:50:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14461v1"
    },
    {
        "title": "Reconstructing physiological signals from fMRI across the adult lifespan",
        "authors": "Shiyu WangZiyuan XuYamin LiMara MatherRoza G. BayrakCatie Chang",
        "links": "http://arxiv.org/abs/2408.14453v1",
        "entry_id": "http://arxiv.org/abs/2408.14453v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14453v1",
        "summary": "Interactions between the brain and body are of fundamental importance for\nhuman behavior and health. Functional magnetic resonance imaging (fMRI)\ncaptures whole-brain activity noninvasively, and modeling how fMRI signals\ninteract with physiological dynamics of the body can provide new insight into\nbrain function and offer potential biomarkers of disease. However,\nphysiological recordings are not always possible to acquire since they require\nextra equipment and setup, and even when they are, the recorded physiological\nsignals may contain substantial artifacts. To overcome this limitation, machine\nlearning models have been proposed to directly extract features of respiratory\nand cardiac activity from resting-state fMRI signals. To date, such work has\nbeen carried out only in healthy young adults and in a pediatric population,\nleaving open questions about the efficacy of these approaches on older adults.\nHere, we propose a novel framework that leverages Transformer-based\narchitectures for reconstructing two key physiological signals - low-frequency\nrespiratory volume (RV) and heart rate (HR) fluctuations - from fMRI data, and\ntest these models on a dataset of individuals aged 36-89 years old. Our\nframework outperforms previously proposed approaches (attaining median\ncorrelations between predicted and measured signals of r ~ .698 for RV and r ~\n.618 for HR), indicating the potential of leveraging attention mechanisms to\nmodel fMRI-physiological signal relationships. We also evaluate several model\ntraining and fine-tuning strategies, and find that incorporating young-adult\ndata during training improves the performance when predicting physiological\nsignals in the aging cohort. Overall, our approach successfully infers key\nphysiological variables directly from fMRI data from individuals across a wide\nrange of the adult lifespan.",
        "updated": "2024-08-26 17:48:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14453v1"
    },
    {
        "title": "Symmetry & Critical Points",
        "authors": "Yossi Arjevani",
        "links": "http://arxiv.org/abs/2408.14445v1",
        "entry_id": "http://arxiv.org/abs/2408.14445v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14445v1",
        "summary": "Critical points of an invariant function may or may not be symmetric. We\nprove, however, that if a symmetric critical point exists, those adjacent to it\nare generically symmetry breaking. This mathematical mechanism is shown to\ncarry important implications for our ability to efficiently minimize invariant\nnonconvex functions, in particular those associated with neural networks.",
        "updated": "2024-08-26 17:36:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14445v1"
    },
    {
        "title": "Model Parallel Training and Transfer Learning for Convolutional Neural Networks by Domain Decomposition",
        "authors": "Axel KlawonnMartin LanserJanine Weber",
        "links": "http://arxiv.org/abs/2408.14442v1",
        "entry_id": "http://arxiv.org/abs/2408.14442v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14442v1",
        "summary": "Deep convolutional neural networks (CNNs) have been shown to be very\nsuccessful in a wide range of image processing applications. However, due to\ntheir increasing number of model parameters and an increasing availability of\nlarge amounts of training data, parallelization strategies to efficiently train\ncomplex CNNs are necessary. In previous work by the authors, a novel model\nparallel CNN architecture was proposed which is loosely inspired by domain\ndecomposition. In particular, the novel network architecture is based on a\ndecomposition of the input data into smaller subimages. For each of these\nsubimages, local CNNs with a proportionally smaller number of parameters are\ntrained in parallel and the resulting local classifications are then aggregated\nin a second step by a dense feedforward neural network (DNN). In the present\nwork, we compare the resulting CNN-DNN architecture to less costly alternatives\nto combine the local classifications into a final, global decision.\nAdditionally, we investigate the performance of the CNN-DNN trained as one\ncoherent model as well as using a transfer learning strategy, where the\nparameters of the pre-trained local CNNs are used as initial values for a\nsubsequently trained global coherent CNN-DNN model.",
        "updated": "2024-08-26 17:35:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14442v1"
    }
]