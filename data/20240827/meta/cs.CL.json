[
    {
        "title": "A Practitioner's Guide to Continual Multimodal Pretraining",
        "authors": "Karsten RothVishaal UdandaraoSebastian DziadzioAmeya PrabhuMehdi ChertiOriol VinyalsOlivier HénaffSamuel AlbanieMatthias BethgeZeynep Akata",
        "links": "http://arxiv.org/abs/2408.14471v1",
        "entry_id": "http://arxiv.org/abs/2408.14471v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14471v1",
        "summary": "Multimodal foundation models serve numerous applications at the intersection\nof vision and language. Still, despite being pretrained on extensive data, they\nbecome outdated over time. To keep models updated, research into continual\npretraining mainly explores scenarios with either (1) infrequent,\nindiscriminate updates on large-scale new data, or (2) frequent, sample-level\nupdates. However, practical model deployment often operates in the gap between\nthese two limit cases, as real-world applications often demand adaptation to\nspecific subdomains, tasks or concepts -- spread over the entire, varying life\ncycle of a model. In this work, we complement current perspectives on continual\npretraining through a research test bed as well as provide comprehensive\nguidance for effective continual model updates in such scenarios. We first\nintroduce FoMo-in-Flux, a continual multimodal pretraining benchmark with\nrealistic compute constraints and practical deployment requirements,\nconstructed over 63 datasets with diverse visual and semantic coverage. Using\nFoMo-in-Flux, we explore the complex landscape of practical continual\npretraining through multiple perspectives: (1) A data-centric investigation of\ndata mixtures and stream orderings that emulate real-world deployment\nsituations, (2) a method-centric investigation ranging from simple fine-tuning\nand traditional continual learning strategies to parameter-efficient updates\nand model merging, (3) meta learning rate schedules and mechanistic design\nchoices, and (4) the influence of model and compute scaling. Together, our\ninsights provide a practitioner's guide to continual multimodal pretraining for\nreal-world deployment. Our benchmark and code is here:\nhttps://github.com/ExplainableML/fomo_in_flux.",
        "updated": "2024-08-26 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14471v1"
    },
    {
        "title": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models",
        "authors": "Aradhye AgarwalSuhas K RameshAyan SenguptaTanmoy Chakraborty",
        "links": "http://arxiv.org/abs/2408.14470v1",
        "entry_id": "http://arxiv.org/abs/2408.14470v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14470v1",
        "summary": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. A class of parameter-efficient fine-tuning\n(PEFT) aims to mitigate these computational challenges by selectively\nfine-tuning only a small fraction of the model parameters. Although\ncomputationally efficient, these techniques often fail to match the performance\nof fully fine-tuned models, primarily due to inherent biases introduced during\nparameter selection. Traditional selective PEFT techniques use a fixed set of\nparameters based on a predefined budget (a process also known as unmasking),\nfailing to capture parameter importance dynamically and often ending up\nexceeding the budget. We introduce $\\text{ID}^3$, a novel selective PEFT method\nthat calculates parameter importance continually and dynamically unmasks\nparameters by balancing exploration and exploitation in parameter selection.\nOur empirical study on 15 tasks spanning natural language understanding and\ngenerative tasks demonstrates the effectiveness of our method compared to\nfixed-masking-based PEFT techniques. We analytically show that $\\text{ID}^3$\nreduces the number of gradient updates by a factor of two, enhancing\ncomputational efficiency. $\\text{ID}^3$ is robust to random initialization of\nneurons and, therefore, can be seamlessly integrated into existing additive and\nreparametrization-based PEFT modules such as adapters and LoRA for dynamic\nsparsification.",
        "updated": "2024-08-26 17:58:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14470v1"
    },
    {
        "title": "Explicit Inductive Inference using Large Language Models",
        "authors": "Tianyang LiuTianyi LiLiang ChengMark Steedman",
        "links": "http://arxiv.org/abs/2408.14467v1",
        "entry_id": "http://arxiv.org/abs/2408.14467v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14467v1",
        "summary": "Large Language Models (LLMs) are reported to hold undesirable attestation\nbias on inference tasks: when asked to predict if a premise P entails a\nhypothesis H, instead of considering H's conditional truthfulness entailed by\nP, LLMs tend to use the out-of-context truth label of H as a fragile proxy. In\nthis paper, we propose a pipeline that exploits this bias to do explicit\ninductive inference. Our pipeline uses an LLM to transform a premise into a set\nof attested alternatives, and then aggregate answers of the derived new\nentailment inquiries to support the original inference prediction. On a\ndirectional predicate entailment benchmark, we demonstrate that by applying\nthis simple pipeline, we can improve the overall performance of LLMs on\ninference and substantially alleviate the impact of their attestation bias.",
        "updated": "2024-08-26 17:58:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14467v1"
    },
    {
        "title": "Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study",
        "authors": "Liuchang Xu Shuo ZhaoQingming LinLuyao ChenQianqian LuoSensen WuXinyue YeHailin FengZhenhong Du",
        "links": "http://arxiv.org/abs/2408.14438v1",
        "entry_id": "http://arxiv.org/abs/2408.14438v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14438v1",
        "summary": "The advent of large language models such as ChatGPT, Gemini, and others has\nunderscored the importance of evaluating their diverse capabilities, ranging\nfrom natural language understanding to code generation. However, their\nperformance on spatial tasks has not been comprehensively assessed. This study\naddresses this gap by introducing a novel multi-task spatial evaluation\ndataset, designed to systematically explore and compare the performance of\nseveral advanced models on spatial tasks. The dataset encompasses twelve\ndistinct task types, including spatial understanding and path planning, each\nwith verified, accurate answers. We evaluated multiple models, including\nOpenAI's gpt-3.5-turbo, gpt-4o, and ZhipuAI's glm-4, through a two-phase\ntesting approach. Initially, we conducted zero-shot testing, followed by\ncategorizing the dataset by difficulty and performing prompt tuning tests.\nResults indicate that gpt-4o achieved the highest overall accuracy in the first\nphase, with an average of 71.3%. Although moonshot-v1-8k slightly\nunderperformed overall, it surpassed gpt-4o in place name recognition tasks.\nThe study also highlights the impact of prompt strategies on model performance\nin specific tasks. For example, the Chain-of-Thought (COT) strategy increased\ngpt-4o's accuracy in path planning from 12.4% to 87.5%, while a one-shot\nstrategy enhanced moonshot-v1-8k's accuracy in mapping tasks from 10.1% to\n76.3%.",
        "updated": "2024-08-26 17:25:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14438v1"
    },
    {
        "title": "CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models",
        "authors": "Shubham BhartiShiyun ChengJihyun RhoMartina RaoXiaojin Zhu",
        "links": "http://arxiv.org/abs/2408.14419v1",
        "entry_id": "http://arxiv.org/abs/2408.14419v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14419v1",
        "summary": "We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large\nlanguage models. CHARTOM consists of specially designed data visualizing\ncharts. Given a chart, a language model needs to not only correctly comprehend\nthe chart (the FACT question) but also judge if the chart will be misleading to\na human reader (the MIND question). Both questions have significant societal\nbenefits. We detail the construction of the CHARTOM benchmark including its\ncalibration on human performance.",
        "updated": "2024-08-26 17:04:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14419v1"
    }
]