[
    {
        "title": "A Practitioner's Guide to Continual Multimodal Pretraining",
        "authors": "Karsten RothVishaal UdandaraoSebastian DziadzioAmeya PrabhuMehdi ChertiOriol VinyalsOlivier HénaffSamuel AlbanieMatthias BethgeZeynep Akata",
        "links": "http://arxiv.org/abs/2408.14471v1",
        "entry_id": "http://arxiv.org/abs/2408.14471v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14471v1",
        "summary": "Multimodal foundation models serve numerous applications at the intersection\nof vision and language. Still, despite being pretrained on extensive data, they\nbecome outdated over time. To keep models updated, research into continual\npretraining mainly explores scenarios with either (1) infrequent,\nindiscriminate updates on large-scale new data, or (2) frequent, sample-level\nupdates. However, practical model deployment often operates in the gap between\nthese two limit cases, as real-world applications often demand adaptation to\nspecific subdomains, tasks or concepts -- spread over the entire, varying life\ncycle of a model. In this work, we complement current perspectives on continual\npretraining through a research test bed as well as provide comprehensive\nguidance for effective continual model updates in such scenarios. We first\nintroduce FoMo-in-Flux, a continual multimodal pretraining benchmark with\nrealistic compute constraints and practical deployment requirements,\nconstructed over 63 datasets with diverse visual and semantic coverage. Using\nFoMo-in-Flux, we explore the complex landscape of practical continual\npretraining through multiple perspectives: (1) A data-centric investigation of\ndata mixtures and stream orderings that emulate real-world deployment\nsituations, (2) a method-centric investigation ranging from simple fine-tuning\nand traditional continual learning strategies to parameter-efficient updates\nand model merging, (3) meta learning rate schedules and mechanistic design\nchoices, and (4) the influence of model and compute scaling. Together, our\ninsights provide a practitioner's guide to continual multimodal pretraining for\nreal-world deployment. Our benchmark and code is here:\nhttps://github.com/ExplainableML/fomo_in_flux.",
        "updated": "2024-08-26 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14471v1"
    },
    {
        "title": "Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos",
        "authors": "Qirui ChenShangzhe DiWeidi Xie",
        "links": "http://arxiv.org/abs/2408.14469v1",
        "entry_id": "http://arxiv.org/abs/2408.14469v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14469v1",
        "summary": "This paper considers the problem of Multi-Hop Video Question Answering\n(MH-VidQA) in long-form egocentric videos. This task not only requires to\nanswer visual questions, but also to localize multiple relevant time intervals\nwithin the video as visual evidences. We develop an automated pipeline to\ncreate multi-hop question-answering pairs with associated temporal evidence,\nenabling to construct a large-scale dataset for instruction-tuning. To monitor\nthe progress of this new task, we further curate a high-quality benchmark,\nMultiHop-EgoQA, with careful manual verification and refinement. Experimental\nresults reveal that existing multi-modal systems exhibit inadequate multi-hop\ngrounding and reasoning abilities, resulting in unsatisfactory performance. We\nthen propose a novel architecture, termed as Grounding Scattered Evidence with\nLarge Language Model (GeLM), that enhances multi-modal large language models\n(MLLMs) by incorporating a grounding module to retrieve temporal evidence from\nvideos using flexible grounding tokens. Trained on our visual instruction data,\nGeLM demonstrates improved multi-hop grounding and reasoning capabilities,\nsetting a new baseline for this challenging task. Furthermore, when trained on\nthird-person view videos, the same architecture also achieves state-of-the-art\nperformance on the single-hop VidQA benchmark, ActivityNet-RTL, demonstrating\nits effectiveness.",
        "updated": "2024-08-26 17:58:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14469v1"
    },
    {
        "title": "Dense Center-Direction Regression for Object Counting and Localization with Point Supervision",
        "authors": "Domen TabernikJon MuhovičDanijel Skočaj",
        "links": "http://dx.doi.org/10.1016/j.patcog.2024.110540",
        "entry_id": "http://arxiv.org/abs/2408.14457v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14457v1",
        "summary": "Object counting and localization problems are commonly addressed with point\nsupervised learning, which allows the use of less labor-intensive point\nannotations. However, learning based on point annotations poses challenges due\nto the high imbalance between the sets of annotated and unannotated pixels,\nwhich is often treated with Gaussian smoothing of point annotations and focal\nloss. However, these approaches still focus on the pixels in the immediate\nvicinity of the point annotations and exploit the rest of the data only\nindirectly. In this work, we propose a novel approach termed CeDiRNet for\npoint-supervised learning that uses a dense regression of directions pointing\ntowards the nearest object centers, i.e. center-directions. This provides\ngreater support for each center point arising from many surrounding pixels\npointing towards the object center. We propose a formulation of\ncenter-directions that allows the problem to be split into the domain-specific\ndense regression of center-directions and the final localization task based on\na small, lightweight, and domain-agnostic localization network that can be\ntrained with synthetic data completely independent of the target domain. We\ndemonstrate the performance of the proposed method on six different datasets\nfor object counting and localization, and show that it outperforms the existing\nstate-of-the-art methods. The code is accessible on GitHub at\nhttps://github.com/vicoslab/CeDiRNet.git.",
        "updated": "2024-08-26 17:49:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14457v1"
    },
    {
        "title": "Center Direction Network for Grasping Point Localization on Cloths",
        "authors": "Domen TabernikJon MuhovičMatej UrbasDanijel Skočaj",
        "links": "http://arxiv.org/abs/2408.14456v1",
        "entry_id": "http://arxiv.org/abs/2408.14456v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14456v1",
        "summary": "Object grasping is a fundamental challenge in robotics and computer vision,\ncritical for advancing robotic manipulation capabilities. Deformable objects,\nlike fabrics and cloths, pose additional challenges due to their non-rigid\nnature. In this work, we introduce CeDiRNet-3DoF, a deep-learning model for\ngrasp point detection, with a particular focus on cloth objects. CeDiRNet-3DoF\nemploys center direction regression alongside a localization network, attaining\nfirst place in the perception task of ICRA 2023's Cloth Manipulation Challenge.\nRecognizing the lack of standardized benchmarks in the literature that hinder\neffective method comparison, we present the ViCoS Towel Dataset. This extensive\nbenchmark dataset comprises 8,000 real and 12,000 synthetic images, serving as\na robust resource for training and evaluating contemporary data-driven\ndeep-learning approaches. Extensive evaluation revealed CeDiRNet-3DoF's\nrobustness in real-world performance, outperforming state-of-the-art methods,\nincluding the latest transformer-based models. Our work bridges a crucial gap,\noffering a robust solution and benchmark for cloth grasping in computer vision\nand robotics. Code and dataset are available at:\nhttps://github.com/vicoslab/CeDiRNet-3DoF",
        "updated": "2024-08-26 17:49:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14456v1"
    },
    {
        "title": "Model Parallel Training and Transfer Learning for Convolutional Neural Networks by Domain Decomposition",
        "authors": "Axel KlawonnMartin LanserJanine Weber",
        "links": "http://arxiv.org/abs/2408.14442v1",
        "entry_id": "http://arxiv.org/abs/2408.14442v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14442v1",
        "summary": "Deep convolutional neural networks (CNNs) have been shown to be very\nsuccessful in a wide range of image processing applications. However, due to\ntheir increasing number of model parameters and an increasing availability of\nlarge amounts of training data, parallelization strategies to efficiently train\ncomplex CNNs are necessary. In previous work by the authors, a novel model\nparallel CNN architecture was proposed which is loosely inspired by domain\ndecomposition. In particular, the novel network architecture is based on a\ndecomposition of the input data into smaller subimages. For each of these\nsubimages, local CNNs with a proportionally smaller number of parameters are\ntrained in parallel and the resulting local classifications are then aggregated\nin a second step by a dense feedforward neural network (DNN). In the present\nwork, we compare the resulting CNN-DNN architecture to less costly alternatives\nto combine the local classifications into a final, global decision.\nAdditionally, we investigate the performance of the CNN-DNN trained as one\ncoherent model as well as using a transfer learning strategy, where the\nparameters of the pre-trained local CNNs are used as initial values for a\nsubsequently trained global coherent CNN-DNN model.",
        "updated": "2024-08-26 17:35:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14442v1"
    }
]