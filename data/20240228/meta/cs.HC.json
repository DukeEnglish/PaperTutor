[
    {
        "title": "Open Your Ears to Take a Look: A State-of-the-Art Report on the Integration of Sonification and Visualization",
        "authors": "Kajetan EngeElias ElmquistValentina CaiolaNiklas RönnbergAlexander RindMichael IberSara LenziFangfei LanRobert HöldrichWolfgang Aigner",
        "links": "http://arxiv.org/abs/2402.16558v1",
        "entry_id": "http://arxiv.org/abs/2402.16558v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16558v1",
        "summary": "The research communities studying visualization and sonification for data\ndisplay and analysis share exceptionally similar goals, essentially making data\nof any kind interpretable to humans. One community does so by using visual\nrepresentations of data, the other community does so by employing auditory\n(non-speech) representations of data. While the two communities have a lot in\ncommon, they developed mostly in parallel over the course of the last few\ndecades. With this STAR, we discuss a collection of work that bridges the\nborders of the two communities, hence a collection of work that aims to\nintegrate the two techniques to one form of audiovisual display, which we argue\nto be \"more than the sum of the two.\" We introduce and motivate a\nclassification system applicable to such audiovisual displays and categorize a\ncorpus of 57 academic publications that appeared between 2011 and 2023 in\ncategories such as reading level, dataset type, or evaluation system, to\nmention a few. The corpus also enables a meta-analysis of the field, including\nregularly occurring design patterns such as type of visualization and\nsonification techniques, or the use of visual and auditory channels, and the\nanalysis of a co-author network of the field which shows individual teams\nwithout much interconnection. The body of work covered in this STAR also\nrelates to three adjacent topics: audiovisual monitoring, accessibility, and\naudiovisual data art. These three topics are discussed individually in addition\nto the systematically conducted part of this research. The findings of this\nreport may be used by researchers from both fields to understand the potentials\nand challenges of such integrated designs, while inspiring them for future\ncollaboration with experts from the respective other field.",
        "updated": "2024-02-26 13:34:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16558v1"
    },
    {
        "title": "Improving behavior based authentication against adversarial attack using XAI",
        "authors": "Dong QinGeorge AmariucaiDaji QiaoYong Guan",
        "links": "http://arxiv.org/abs/2402.16430v1",
        "entry_id": "http://arxiv.org/abs/2402.16430v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16430v1",
        "summary": "In recent years, machine learning models, especially deep neural networks,\nhave been widely used for classification tasks in the security domain. However,\nthese models have been shown to be vulnerable to adversarial manipulation:\nsmall changes learned by an adversarial attack model, when applied to the\ninput, can cause significant changes in the output. Most research on\nadversarial attacks and corresponding defense methods focuses only on scenarios\nwhere adversarial samples are directly generated by the attack model. In this\nstudy, we explore a more practical scenario in behavior-based authentication,\nwhere adversarial samples are collected from the attacker. The generated\nadversarial samples from the model are replicated by attackers with a certain\nlevel of discrepancy. We propose an eXplainable AI (XAI) based defense strategy\nagainst adversarial attacks in such scenarios. A feature selector, trained with\nour method, can be used as a filter in front of the original authenticator. It\nfilters out features that are more vulnerable to adversarial attacks or\nirrelevant to authentication, while retaining features that are more robust.\nThrough comprehensive experiments, we demonstrate that our XAI based defense\nstrategy is effective against adversarial attacks and outperforms other defense\nstrategies, such as adversarial training and defensive distillation.",
        "updated": "2024-02-26 09:29:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16430v1"
    },
    {
        "title": "Human-AI Co-Creation of Worked Examples for Programming Classes",
        "authors": "Mohammad HassanyPeter BrusilovskyJiaze KeKamil AkhuseyinogluArun Balajiee Lekshmi Narayanan",
        "links": "http://arxiv.org/abs/2402.16235v1",
        "entry_id": "http://arxiv.org/abs/2402.16235v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16235v1",
        "summary": "Worked examples (solutions to typical programming problems presented as a\nsource code in a certain language and are used to explain the topics from a\nprogramming class) are among the most popular types of learning content in\nprogramming classes. Most approaches and tools for presenting these examples to\nstudents are based on line-by-line explanations of the example code. However,\ninstructors rarely have time to provide line-by-line explanations for a large\nnumber of examples typically used in a programming class. In this paper, we\nexplore and assess a human-AI collaboration approach to authoring worked\nexamples for Java programming. We introduce an authoring system for creating\nJava worked examples that generates a starting version of code explanations and\npresents it to the instructor to edit if necessary.We also present a study that\nassesses the quality of explanations created with this approach",
        "updated": "2024-02-26 01:44:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16235v1"
    },
    {
        "title": "MoodCapture: Depression Detection Using In-the-Wild Smartphone Images",
        "authors": "Subigya NepalArvind PillaiWeichen WangTess GriffinAmanda C. CollinsMichael HeinzDamien LekkasShayan MirjafariMatthew NemesureGeorge PriceNicholas C. JacobsonAndrew T. Campbell",
        "links": "http://dx.doi.org/10.1145/3613904.3642680",
        "entry_id": "http://arxiv.org/abs/2402.16182v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16182v1",
        "summary": "MoodCapture presents a novel approach that assesses depression based on\nimages automatically captured from the front-facing camera of smartphones as\npeople go about their daily lives. We collect over 125,000 photos in the wild\nfrom N=177 participants diagnosed with major depressive disorder for 90 days.\nImages are captured naturalistically while participants respond to the PHQ-8\ndepression survey question: \\textit{``I have felt down, depressed, or\nhopeless''}. Our analysis explores important image attributes, such as angle,\ndominant colors, location, objects, and lighting. We show that a random forest\ntrained with face landmarks can classify samples as depressed or non-depressed\nand predict raw PHQ-8 scores effectively. Our post-hoc analysis provides\nseveral insights through an ablation study, feature importance analysis, and\nbias assessment. Importantly, we evaluate user concerns about using MoodCapture\nto detect depression based on sharing photos, providing critical insights into\nprivacy concerns that inform the future design of in-the-wild image-based\nmental health assessment tools.",
        "updated": "2024-02-25 20:08:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16182v1"
    },
    {
        "title": "InstructEdit: Instruction-based Knowledge Editing for Large Language Models",
        "authors": "Bozhong TianSiyuan ChengXiaozhuan LiangNingyu ZhangYi HuKouying XueYanjie GouXi ChenHuajun Chen",
        "links": "http://arxiv.org/abs/2402.16123v1",
        "entry_id": "http://arxiv.org/abs/2402.16123v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16123v1",
        "summary": "Knowledge editing for large language models can offer an efficient solution\nto alter a model's behavior without negatively impacting the overall\nperformance. However, the current approach encounters issues with limited\ngeneralizability across tasks, necessitating one distinct editor for each task,\nwhich significantly hinders the broader applications. To address this, we take\nthe first step to analyze the multi-task generalization issue in knowledge\nediting. Specifically, we develop an instruction-based editing technique,\ntermed InstructEdit, which facilitates the editor's adaptation to various task\nperformances simultaneously using simple instructions. With only one unified\neditor for each LLM, we empirically demonstrate that InstructEdit can improve\nthe editor's control, leading to an average 14.86% increase in Reliability in\nmulti-task editing setting. Furthermore, experiments involving holdout unseen\ntask illustrate that InstructEdit consistently surpass previous strong\nbaselines. To further investigate the underlying mechanisms of\ninstruction-based knowledge editing, we analyze the principal components of the\nediting gradient directions, which unveils that instructions can help control\noptimization direction with stronger OOD generalization. Code and datasets will\nbe available in https://github.com/zjunlp/EasyEdit.",
        "updated": "2024-02-25 15:46:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16123v1"
    }
]