[
    {
        "title": "Enhancement of 3D Camera Synthetic Training Data with Noise Models",
        "authors": "Katarína OsvaldováLukáš GajdošechViktor KocurMartin Madaras",
        "links": "http://dx.doi.org/10.5281/zenodo.10694437",
        "entry_id": "http://arxiv.org/abs/2402.16514v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16514v1",
        "summary": "The goal of this paper is to assess the impact of noise in 3D camera-captured\ndata by modeling the noise of the imaging process and applying it on synthetic\ntraining data. We compiled a dataset of specifically constructed scenes to\nobtain a noise model. We specifically model lateral noise, affecting the\nposition of captured points in the image plane, and axial noise, affecting the\nposition along the axis perpendicular to the image plane. The estimated models\ncan be used to emulate noise in synthetic training data. The added benefit of\nadding artificial noise is evaluated in an experiment with rendered data for\nobject segmentation. We train a series of neural networks with varying levels\nof noise in the data and measure their ability to generalize on real data. The\nresults show that using too little or too much noise can hurt the networks'\nperformance indicating that obtaining a model of noise from real scanners is\nbeneficial for synthetic data generation.",
        "updated": "2024-02-26 11:50:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16514v1"
    },
    {
        "title": "Stochastic Conditional Diffusion Models for Semantic Image Synthesis",
        "authors": "Juyeon KoInho KongDogyun ParkHyunwoo J. Kim",
        "links": "http://arxiv.org/abs/2402.16506v2",
        "entry_id": "http://arxiv.org/abs/2402.16506v2",
        "pdf_url": "http://arxiv.org/pdf/2402.16506v2",
        "summary": "Semantic image synthesis (SIS) is a task to generate realistic images\ncorresponding to semantic maps (labels). It can be applied to diverse\nreal-world practices such as photo editing or content creation. However, in\nreal-world applications, SIS often encounters noisy user inputs. To address\nthis, we propose Stochastic Conditional Diffusion Model (SCDM), which is a\nrobust conditional diffusion model that features novel forward and generation\nprocesses tailored for SIS with noisy labels. It enhances robustness by\nstochastically perturbing the semantic label maps through Label Diffusion,\nwhich diffuses the labels with discrete diffusion. Through the diffusion of\nlabels, the noisy and clean semantic maps become similar as the timestep\nincreases, eventually becoming identical at $t=T$. This facilitates the\ngeneration of an image close to a clean image, enabling robust generation.\nFurthermore, we propose a class-wise noise schedule to differentially diffuse\nthe labels depending on the class. We demonstrate that the proposed method\ngenerates high-quality samples through extensive experiments and analyses on\nbenchmark datasets, including a novel experimental setup simulating human\nerrors during real-world applications.",
        "updated": "2024-02-27 04:46:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16506v2"
    },
    {
        "title": "Intelligent Known and Novel Aircraft Recognition -- A Shift from Classification to Similarity Learning for Combat Identification",
        "authors": "Ahmad SaeedHaasha Bin AtifUsman HabibMohsin Bilal",
        "links": "http://arxiv.org/abs/2402.16486v1",
        "entry_id": "http://arxiv.org/abs/2402.16486v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16486v1",
        "summary": "Precise aircraft recognition in low-resolution remote sensing imagery is a\nchallenging yet crucial task in aviation, especially combat identification.\nThis research addresses this problem with a novel, scalable, and AI-driven\nsolution. The primary hurdle in combat identification in remote sensing imagery\nis the accurate recognition of Novel/Unknown types of aircraft in addition to\nKnown types. Traditional methods, human expert-driven combat identification and\nimage classification, fall short in identifying Novel classes. Our methodology\nemploys similarity learning to discern features of a broad spectrum of military\nand civilian aircraft. It discerns both Known and Novel aircraft types,\nleveraging metric learning for the identification and supervised few-shot\nlearning for aircraft type classification. To counter the challenge of limited\nlow-resolution remote sensing data, we propose an end-to-end framework that\nadapts to the diverse and versatile process of military aircraft recognition by\ntraining a generalized embedder in fully supervised manner. Comparative\nanalysis with earlier aircraft image classification methods shows that our\napproach is effective for aircraft image classification (F1-score Aircraft Type\nof 0.861) and pioneering for quantifying the identification of Novel types\n(F1-score Bipartitioning of 0.936). The proposed methodology effectively\naddresses inherent challenges in remote sensing data, thereby setting new\nstandards in dataset quality. The research opens new avenues for domain experts\nand demonstrates unique capabilities in distinguishing various aircraft types,\ncontributing to a more robust, domain-adapted potential for real-time aircraft\nrecognition.",
        "updated": "2024-02-26 11:08:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16486v1"
    },
    {
        "title": "Edge Detectors Can Make Deep Convolutional Neural Networks More Robust",
        "authors": "Jin DingJie-Chao ZhaoYong-Zhi SunPing TanJia-Wei WangJi-En MaYou-Tong Fang",
        "links": "http://arxiv.org/abs/2402.16479v1",
        "entry_id": "http://arxiv.org/abs/2402.16479v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16479v1",
        "summary": "Deep convolutional neural networks (DCNN for short) are vulnerable to\nexamples with small perturbations. Improving DCNN's robustness is of great\nsignificance to the safety-critical applications, such as autonomous driving\nand industry automation. Inspired by the principal way that human eyes\nrecognize objects, i.e., largely relying on the shape features, this paper\nfirst employs the edge detectors as layer kernels and designs a binary edge\nfeature branch (BEFB for short) to learn the binary edge features, which can be\neasily integrated into any popular backbone. The four edge detectors can learn\nthe horizontal, vertical, positive diagonal, and negative diagonal edge\nfeatures, respectively, and the branch is stacked by multiple Sobel layers\n(using edge detectors as kernels) and one threshold layer. The binary edge\nfeatures learned by the branch, concatenated with the texture features learned\nby the backbone, are fed into the fully connected layers for classification. We\nintegrate the proposed branch into VGG16 and ResNet34, respectively, and\nconduct experiments on multiple datasets. Experimental results demonstrate the\nBEFB is lightweight and has no side effects on training. And the accuracy of\nthe BEFB integrated models is better than the original ones on all datasets\nwhen facing FGSM, PGD, and C\\&W attacks. Besides, BEFB integrated models\nequipped with the robustness enhancing techniques can achieve better\nclassification accuracy compared to the original models. The work in this paper\nfor the first time shows it is feasible to enhance the robustness of DCNNs\nthrough combining both shape-like features and texture features.",
        "updated": "2024-02-26 10:54:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16479v1"
    },
    {
        "title": "DCVSMNet: Double Cost Volume Stereo Matching Network",
        "authors": "Mahmoud TahmasebiSaif HuqKevin MeehanMarion McAfee",
        "links": "http://arxiv.org/abs/2402.16473v1",
        "entry_id": "http://arxiv.org/abs/2402.16473v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16473v1",
        "summary": "We introduce Double Cost Volume Stereo Matching Network(DCVSMNet) which is a\nnovel architecture characterised by by two small upper (group-wise) and lower\n(norm correlation) cost volumes. Each cost volume is processed separately, and\na coupling module is proposed to fuse the geometry information extracted from\nthe upper and lower cost volumes. DCVSMNet is a fast stereo matching network\nwith a 67 ms inference time and strong generalization ability which can produce\ncompetitive results compared to state-of-the-art methods. The results on\nseveral bench mark datasets show that DCVSMNet achieves better accuracy than\nmethods such as CGI-Stereo and BGNet at the cost of greater inference time.",
        "updated": "2024-02-26 10:42:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16473v1"
    }
]