[
    {
        "title": "Q-FOX Learning: Breaking Tradition in Reinforcement Learning",
        "authors": "Mahmood AlqaseerYossra H. AliTarik A. Rashid",
        "links": "http://arxiv.org/abs/2402.16562v1",
        "entry_id": "http://arxiv.org/abs/2402.16562v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16562v1",
        "summary": "Reinforcement learning (RL) is a subset of artificial intelligence (AI) where\nagents learn the best action by interacting with the environment, making it\nsuitable for tasks that do not require labeled data or direct supervision.\nHyperparameters (HP) tuning refers to choosing the best parameter that leads to\noptimal solutions in RL algorithms. Manual or random tuning of the HP may be a\ncrucial process because variations in this parameter lead to changes in the\noverall learning aspects and different rewards. In this paper, a novel and\nautomatic HP-tuning method called Q-FOX is proposed. This uses both the FOX\noptimizer, a new optimization method inspired by nature that mimics red foxes'\nhunting behavior, and the commonly used, easy-to-implement RL Q-learning\nalgorithm to solve the problem of HP tuning. Moreover, a new objective function\nis proposed which prioritizes the reward over the mean squared error (MSE) and\nlearning time (steps). Q-FOX has been evaluated on two OpenAI Gym environment\ncontrol tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewards\nthan HP tuning with other optimizers, such as PSO, GA, Bee, or randomly\nselected HP. The cumulative reward for the Cart Pole task was 32.08, and for\nthe Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has\nlimitations. It cannot be used directly in real-word problems before choosing\nthe HP in a simulation environment because its processes work iteratively,\nmaking it time-consuming. The results indicate that Q-FOX has played an\nessential role in HP tuning for RL algorithms to effectively solve different\ncontrol tasks.",
        "updated": "2024-02-26 13:39:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16562v1"
    },
    {
        "title": "Label Learning Method Based on Tensor Projection",
        "authors": "Jing LiQuanxue GaoQianqian WangCheng DengDeyan Xie",
        "links": "http://arxiv.org/abs/2402.16544v1",
        "entry_id": "http://arxiv.org/abs/2402.16544v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16544v1",
        "summary": "Multi-view clustering method based on anchor graph has been widely concerned\ndue to its high efficiency and effectiveness. In order to avoid\npost-processing, most of the existing anchor graph-based methods learn\nbipartite graphs with connected components. However, such methods have high\nrequirements on parameters, and in some cases it may not be possible to obtain\nbipartite graphs with clear connected components. To end this, we propose a\nlabel learning method based on tensor projection (LLMTP). Specifically, we\nproject anchor graph into the label space through an orthogonal projection\nmatrix to obtain cluster labels directly. Considering that the spatial\nstructure information of multi-view data may be ignored to a certain extent\nwhen projected in different views separately, we extend the matrix projection\ntransformation to tensor projection, so that the spatial structure information\nbetween views can be fully utilized. In addition, we introduce the tensor\nSchatten $p$-norm regularization to make the clustering label matrices of\ndifferent views as consistent as possible. Extensive experiments have proved\nthe effectiveness of the proposed method.",
        "updated": "2024-02-26 13:03:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16544v1"
    },
    {
        "title": "Model-based deep reinforcement learning for accelerated learning from flow simulations",
        "authors": "Andre WeinerJanis Geise",
        "links": "http://arxiv.org/abs/2402.16543v1",
        "entry_id": "http://arxiv.org/abs/2402.16543v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16543v1",
        "summary": "In recent years, deep reinforcement learning has emerged as a technique to\nsolve closed-loop flow control problems. Employing simulation-based\nenvironments in reinforcement learning enables a priori end-to-end optimization\nof the control system, provides a virtual testbed for safety-critical control\napplications, and allows to gain a deep understanding of the control\nmechanisms. While reinforcement learning has been applied successfully in a\nnumber of rather simple flow control benchmarks, a major bottleneck toward\nreal-world applications is the high computational cost and turnaround time of\nflow simulations. In this contribution, we demonstrate the benefits of\nmodel-based reinforcement learning for flow control applications. Specifically,\nwe optimize the policy by alternating between trajectories sampled from flow\nsimulations and trajectories sampled from an ensemble of environment models.\nThe model-based learning reduces the overall training time by up to $85\\%$ for\nthe fluidic pinball test case. Even larger savings are expected for more\ndemanding flow simulations.",
        "updated": "2024-02-26 13:01:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16543v1"
    },
    {
        "title": "Integrating Large Language Models with Graphical Session-Based Recommendation",
        "authors": "Naicheng GuoHongwei ChengQianqiao LiangLinxun ChenBing Han",
        "links": "http://arxiv.org/abs/2402.16539v1",
        "entry_id": "http://arxiv.org/abs/2402.16539v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16539v1",
        "summary": "With the rapid development of Large Language Models (LLMs), various\nexplorations have arisen to utilize LLMs capability of context understanding on\nrecommender systems. While pioneering strategies have primarily transformed\ntraditional recommendation tasks into challenges of natural language\ngeneration, there has been a relative scarcity of exploration in the domain of\nsession-based recommendation (SBR) due to its specificity. SBR has been\nprimarily dominated by Graph Neural Networks, which have achieved many\nsuccessful outcomes due to their ability to capture both the implicit and\nexplicit relationships between adjacent behaviors. The structural nature of\ngraphs contrasts with the essence of natural language, posing a significant\nadaptation gap for LLMs. In this paper, we introduce large language models with\ngraphical Session-Based recommendation, named LLMGR, an effective framework\nthat bridges the aforementioned gap by harmoniously integrating LLMs with Graph\nNeural Networks (GNNs) for SBR tasks. This integration seeks to leverage the\ncomplementary strengths of LLMs in natural language understanding and GNNs in\nrelational data processing, leading to a more powerful session-based\nrecommender system that can understand and recommend items within a session.\nMoreover, to endow the LLM with the capability to empower SBR tasks, we design\na series of prompts for both auxiliary and major instruction tuning tasks.\nThese prompts are crafted to assist the LLM in understanding graph-structured\ndata and align textual information with nodes, effectively translating nuanced\nuser interactions into a format that can be understood and utilized by LLM\narchitectures. Extensive experiments on three real-world datasets demonstrate\nthat LLMGR outperforms several competitive baselines, indicating its\neffectiveness in enhancing SBR tasks and its potential as a research direction\nfor future exploration.",
        "updated": "2024-02-26 12:55:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16539v1"
    },
    {
        "title": "Discovering Artificial Viscosity Models for Discontinuous Galerkin Approximation of Conservation Laws using Physics-Informed Machine Learning",
        "authors": "Matteo CaldanaPaola F. AntoniettiLuca Dede'",
        "links": "http://arxiv.org/abs/2402.16517v1",
        "entry_id": "http://arxiv.org/abs/2402.16517v1",
        "pdf_url": "http://arxiv.org/pdf/2402.16517v1",
        "summary": "Finite element-based high-order solvers of conservation laws offer large\naccuracy but face challenges near discontinuities due to the Gibbs phenomenon.\nArtificial viscosity is a popular and effective solution to this problem based\non physical insight. In this work, we present a physics-informed machine\nlearning algorithm to automate the discovery of artificial viscosity models in\na non-supervised paradigm. The algorithm is inspired by reinforcement learning\nand trains a neural network acting cell-by-cell (the viscosity model) by\nminimizing a loss defined as the difference with respect to a reference\nsolution thanks to automatic differentiation. This enables a dataset-free\ntraining procedure. We prove that the algorithm is effective by integrating it\ninto a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcase\nseveral numerical tests on scalar and vectorial problems, such as Burgers' and\nEuler's equations in one and two dimensions. Results demonstrate that the\nproposed approach trains a model that is able to outperform classical viscosity\nmodels. Moreover, we show that the learnt artificial viscosity model is able to\ngeneralize across different problems and parameters.",
        "updated": "2024-02-26 11:58:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.16517v1"
    }
]