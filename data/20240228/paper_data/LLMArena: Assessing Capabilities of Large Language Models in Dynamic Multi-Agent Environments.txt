LLMARENA: Assessing Capabilities of Large Language Models in
Dynamic Multi-Agent Environments
JunzheChen1∗,XumingHu2†,ShuodiLiu3,ShiyuHuang4,
Wei-WeiTu4,ZhaofengHe3,LijieWen1†
1TsinghuaUniversity,2TheHongKongUniversityofScienceandTechnology(Guangzhou),
3BeijingUniversityofPostsandTelecommunications,44ParadigmInc.
chenjz20@mails.tsinghua.edu.cn,
xuminghu97@gmail.com, wenlj@tsinghua.edu.cn
Abstract butalsodemonstrateremarkableadaptability,even
withouttrainingtailoredtospecifictasks(Weietal.,
Recentadvancementsinlargelanguagemodels
2023;Guoetal.,2023). Theypossesstheabilityto
(LLMs)haverevealedtheirpotentialforachiev-
analyze,strategize,andrespondeffectivelytoun-
ingautonomousagentspossessinghuman-level
familiarscenariosusingminimalguidingprompts.
intelligence. However, existing benchmarks
for evaluating LLM Agents either use static This has opened new avenues for researchers to
datasets, potentially leading to data leakage, considerLLMsasautonomousagents,providing
orfocusonlyonsingle-agentscenarios,over- automatedassistanceincomplexreal-worldtasks
lookingthecomplexitiesofmulti-agentinterac- suchassoftwaredevelopmentandknowledgeinte-
tions. Thereisalackofabenchmarkthateval-
gration(Qianetal.,2023;Panetal.,2024).
uates the diverse capabilities of LLM agents
inmulti-agent,dynamicenvironments. Tothis TobetterunderstandtheskillsneededbyLLMas
end, weintroduce LLMARENA, anoveland anagent,researchersarenowconcentratingoncre-
easilyextensibleframeworkforevaluatingthe atingandusingvariousscenariostoassessLLM’s
diversecapabilitiesofLLMinmulti-agentdy-
performanceinspecificsettings(Liuetal.,2023;
namic environments. LLMARENA encom-
Chenetal.,2023;Wuetal.,2023;Lietal.,2023a;
passessevendistinctgamingenvironments,em-
Yaoetal.,2022). Forinstance,AgentBenchintro-
ployingTrueSkill™scoringtoassesscrucial
duced eight distinct environments, such as oper-
abilitiesinLLMagents,includingspatialrea-
soning, strategic planning, numerical reason- ating systems and databases, to examine LLM’s
ing,riskassessment,communication,opponent proficiency in code generation and lateral think-
modeling, and team collaboration. We con- ing. Similarly, SmartPlay tests LLM in a gam-
ductanextensiveexperimentandhumanevalu- ing context as a solo agent to gauge its reason-
ationamongdifferentsizesandtypesofLLMs,
ing and strategic planning abilities. Nonetheless,
showingthatLLMsstillhaveasignificantjour-
these benchmarks have their constraints. Firstly,
neyaheadintheirdevelopmenttowardsbecom-
staticdatasetsusedinbenchmarkslikeAgentBench
ingfullyautonomousagents,especiallyinop-
ponentmodelingandteamcollaboration. We mightleadtoissuessuchasdataleakageandover-
hopeLLMARENA couldguidefutureresearch fitting, as LLMs could have already encountered
towardsenhancingthesecapabilitiesinLLMs, thisdataduringtheirpre-trainingphase(Zhouetal.,
ultimately leading to more sophisticated and 2023b). Secondly, evaluation systems that focus
practicalapplicationsindynamic,multi-agent
solely on a single agent also have evident limita-
settings. Thecodeanddatawillbeavailable.
tions, particularly in analyzing an agent’s perfor-
manceinintricateanddynamicallyinteractiveset-
1 Introduction
tings. Suchsingle-agentenvironmentstendtocon-
Recentlargelanguagemodels(LLMs)havegreatly centrateontheinteractionsbetweentheagentand
promoted the progress of the field of natural lan- itsenvironment,therebyoverlookingthecomplex
guageprocessing(NLP)duetotheirexcellentzero- dynamicsofinteractionsthatoccuramongmulti-
shotcapabilitiesinvariousdownstreamtasks(Ko- ple agents in a shared environment (Foster et al.,
jimaetal.,2022;Yangetal.,2023;Touvronetal., 2023). Tothisend,weproposed LLMARENA,a
2023a,b;OpenAI,2022,2023). LLMsnotonlyex- dynamicevaluationbenchmarkspeciallydesigned
celincomprehendingandgeneratingintricatetext for multi-agent interaction. As illustrated in Fig-
∗Workwasdoneduringtheinternat4ParadigmInc.. ure1, LLMARENA coverssevendifferenttypes
†Correspondingauthors. ofdynamic,multi-agentgameenvironments. For
4202
beF
62
]LC.sc[
1v99461.2042:viXraTicTacToe LLM Arena Bargain
LLM Arena System Prompt：You are playing tic-tac- LLM Arena System Prompt：You are participating in a game
of "Bargaining." Here are the game rules: ......
toe. Tic-tac-toe is ......
Observation Prompt: Rankings Rankings O plb as ye er rv _a 0t .i To hn e rP er o am rep 2t : h N ao tsw , .i .s .. .r .o Tu hn ed v a1 l: u Y eo fu o ra re
You play X. Qwen-72B GPT-3.5-Turbo GPT-4 GPT-4 each hat is 1, ......
The board status is GPT-3.5-Turbo LLaMA2 -70B Action Prompt: You should say something to
Qwen-72B Wins! LLaMA2 -70B GPT-3.5-Turbo bargain with your opponent first and then
TrueSkill rating changes from Yi-34B Qwen-72B output player_0 and your plan......
Y ou can only put the mark on
2 G6 P.6 T9
-3
t .o
5
-2 T7 u.0 rb1 o!
Loses! Qwe .n .-7 .2B Yi-3 .4 .B . Player 0 Player 0: 1 hat 2 balls 1 apple. I'd like 1 hat ....
[ A( c1 t,2 io) n,( 2 P, r1 o), m(2 p,3
t
) :, ( Y3 o,2 u) s,( h3 o,3 u) l] d.
output X and the
T 28ru .3e 1S tk oi l 2l 7r .a 8t 7in !g changes from O Acb ts ie or nv Pat ri oo mn pP tr :o Ym oup t s:
h
Y oo uu
l
da r ..e
..
.p .layer_1, ...... LLM Arena
position of the move, for example: "X: (1, 3)". Texas hold 'em
Player 1: 1 haDt a1 tbea llM 1 oapdpilfeic…a…tion
Player 0 X: (3,3) LLM Arena NSy os t Le imm i tP . r Ho em rep t a： reY to hu e a br ae s ip cl a ruy li en sg .T ..e ..x .as Hold'em …… Player 1
Observation Prompt: You play O. ...... LLM Arena Undercover
Action Prompt : You should output O and ...... Observation Prompt: Your observation now is :
The cards in your hands are [Clubs 2, Hearts 3] LLM Arena System Prompt：You are playing a game of the
…… O: (3,2) Player 1 T 8 Foh ]. le dY c ,o o Cum h c eam cn ku cn ahi nt o dy o c Csa e ar lod l, ns . .ea .. r .oe .f [ tS hp ea fd oe lls o 6 w, iC nl gu ab cs t K io, nC slu : bs sOU tbn agd se ee r :r Yvc oao utv i e o ar n r. e H P pre o lr ame y ea p rr t _e : 0 Nt .h o Ye w o g u ia s rm sth ee ce r r u g eil tve wis n: o g. r. . c d. l. u. ise s
Connect Four A ouct ti po un t P yr oo um r ap ct t: i oYo nu . F s oh ro eu xld am th pin lek : s 'Ate cp ti ob ny : s Ct ae lp l' .and " AS cn tio ow n" P. r.. o.. m.. pt: You should only output player_0
LLM Arena System Prompt：You are playing Check and Call and the clue, for example: "player_0: It's a fruit.".
Connect Four. Connect Four is ......
Player 0 Player 0: It's something that falls from the sky.
Observation Prompt: Observation Prompt: Your observation now is : ..... LLM Arena Player 0
You play X. Action Prompt: You should ...... Observation Prompt: You are player_1, ...... LLM Arena
The board status is Action Prompt: You should ......
Fold
…… Player 1 Player 1: It's somethiDnWga tothera dtM c oRoveedprsil ftaihccea egtmrioouennndt Player 1
Bid
Player 2: It's something that is white and cold.
Y coo lu u mca nn s o : [n 1ly , c 2h , o 3o , s 4e , 5o ,n 6e , o 7f ]f .ollowing LLM Arena S gy as mte em of P "r Fo irm stp -pt： ricY eo Su e a ar lee d p -a Br idti c Aip ua ct ti in og n .i "n H a e re …… Player 2
Action Prompt : are the game rules: ...... Hanabi
You should output X and the column of the
move you choose to put your mark, for O Yob us re vrv aa luti ao tn io P nr oo fm thp et: cY uo ru re a nr te a p ul ca ty ioer n_ i0 te. m is $3.75, LLM Arena HSy es rt ee m ar eP tr ho em bp at s： icY ro uu le a s r fe o rp Hla ay nin ibg i H ..a ..n ..ibi game.
example: "X: 1“. Your rewards will be equal to $(3.75 - your bid) ……
Action Prompt: You should think step by step and Observation Prompt: Your observation now is :
X: 3 output your action: For example: player_0: $1.08. The cards in another player's hands is :
{'position 1 : Yellow 1', 'position 0 : Yellow 5'} ...
Player 0 Action Promp0t: You should think step by step
Observation Prompt: You play O. ...... LLM Arena player_0: $2.50 choose one of the following actions: ......
Player 0
Action Prompt : You should output O and ......
Observation Prompt: You are player_1. ...... LLM Arena Player 0 Action: Reveal Rank 1 Cards for another player
O: 5 Action Prompt: YoNuo s hmooudlde .l. .d..i.d it wrong! Observation Prompt: Your observation ...... L L M A rena
Action Prompt : You should ......
…… Player 1 player_1: $8.50
…… Player 1 …… Action: Play Card at position 1 Player 1
Figure1: ExamplesofsevendifferentgameenvironmentsinLLMARENA.
example,theTexasHold’empokergameenviron- Weconductanextensiveexperimentandhuman
mentautomaticallygeneratesanewhandforeach evaluation among 14 different sizes and types of
game,andthecomplexityofeachround’sstatein- LLMs. Theresultsindicatedsignificantpotential
creasesexponentially. Thisdynamicnatureofthe forimprovementinLLM’steamworkandopponent
gameservestomitigatetheissueofdataleakage. modelingskillswithinmulti-agentenvironments.
In the Undercover environment, the LLM agents Basedontheseresults,wehope LLMARENA can
needtopointoutthe“undercover”duringthecom- inspire future research focused on enhancing the
municationprocesswithotheragents,whichmea- core capabilities of LLM in multi-agent settings
surestheLLMagents’communicationabilityand topromotethewidespreaduseofLLMagentsin
opponent modeling ability when facing different real-worldapplications.
agents. Basedontheseenvironments,wecancom-
2 BenchmarkDetail
prehensively evaluate LLM’s spatial comprehen-
sion,strategicplanning,numericalreasoning,risk In this section, we provide a detailed overview
assessment, communication, opponent modeling, of LLMArena’s composition, which includes its
andteamcollaborationcapabilitiesinthedynamic sevendistinctgameenvironments,evaluationmet-
multi-agentenvironment. rics,andthemethodologiesusedforassessment.
TomoreaccuratelyevaluateLLM’sperformance
2.1 BenchmarkOverview
inthesemulti-agentenvironments,weadoptedthe
TrueSkill™ (Herbrichetal.,2006)scoringsystem, Figure1illustratesthesevendistinctenvironments
whichcanbetterhelp LLMARENA evaluateother within LLMARENA. Within each specific envi-
metricsbesideswinrate,suchastherelativeskill ronment,LLMsmayneedtoutilizeauniquecom-
level between agents, to provide a more compre- binationofcapabilitiestoeffectivelytacklethese
hensiveassessmentunderdifferentscenariosand challenges. For instance, in the Undercover en-
opponentconfigurations. vironment, LLMs are required to possess a mul-tifaceted skill set, including opponent modeling, lacks interpretability (Wang et al., 2023a). LL-
communicatingeffectively,andteamcollaboration. MARENA addresses the gap by leveraging the
The absence of any one of these abilities could conceptofrewardfromreinforcementlearningto
resultinultimatefailureinthisenvironment. measure the quality of each action performed by
agents. Themethodallowsforaquantitativeand
2.2 BenchmarkConstruction
interpretableevaluationofLLMs’capabilities,of-
Toensureuser-friendlinessandrobustscalabilityin feringamorenuancedandunderstandableanalysis.
LLMARENA,wehavedevelopedtheenvironment
usingPettingZoo(Terryetal.,2021)asafounda- 2.4 EnvironmentSettings
tion. Thisapproachenablessubsequentresearchers Inthissection,wegiveacomprehensiveexplana-
toeffortlesslyintegratenewenvironmentsintothis tion of the diverse game environments included
framework. Bysimplycreatingenvironmentsthat intheLLMARENA benchmark,highlightingthe
adheretothePettingZoointerfacespecifications1,
specificchallengesthatlanguagemodelsencounter
theycanseamlesslyaddtheseto LLMARENA for whileengagingwiththeseenvironments.
evaluating the diverse capabilities of LLMs. Ev-
TicTacToe: TicTacToe2 isastrategygamewhere
eryenvironmentwithinLLMARENAofferssystem
twoLLMagentsalternateinplacingtheirmarkson
prompts that outline the game rules and prompt
a3×3grid. Victoryisachievedwhenoneagentsuc-
templatesdesignedtoguideLLMagentsingame-
cessfullyalignsthreeoftheirmarksinarow,either
play. Thesetemplatesincludeinformationonthe
horizontally, vertically, or diagonally. The game
current status of the game, historical data, and a
resultsinadrawifallcellsarefilledwithoutany
rangeofoptionalactions,facilitatingamorestruc-
playerachievingawinningalignment. Thisgame
turedandinformedgamingexperiencefortheLLM
examinesLLM’sstrategicplanningandspatialrea-
agents. AllthepromptsareshowninAppendixA.
soningcapabilities. WeadopttheTrueSkill™rat-
2.3 Metrics ingsastheevaluationmetrics.
In this section, we outline the evaluation metrics
ConnectFour As a more complex case of the
usedby LLMARENA,followedbyacomparative
boardgame,ConnectFour3 usesa6×7chessboard.
analysiswithmetricsutilizedinpriorresearch.
Here, two LLM agents alternately choose one of
TrueSkill™ TrueSkill™isascoringsystemde- thesevencolumnswithavailableemptycells. Each
veloped by Herbrich et al. (2006) that evaluates piece placed will descend to the lowest available
multipleagents’skilllevelsincompetitivegaming. spaceintheselectedcolumn,eitherrestingatthe
In contrast to static, opponent-independent met- bottomoratopapreviouslyplacedtoken. Anagent
rics employed in previous research, such as Win securesvictorybyaligningfouroftheirpiecesei-
Rate and Completion Rate (Wu et al., 2023; Xu ther horizontally, vertically, or diagonally. The
et al., 2023), TrueSkill™ offers a more nuanced gameconcludesinadrawifallcellsarefilledwith-
assessment. Itgoesbeyondmerelyaccountingfor outawinner. SimilartoTicTacToe,inConnectFour,
winsandlossesbyalsoconsideringthequalityof theLLMsprimarilyencounterchallengesinstrate-
the game and the skill disparities between play- gicreasoningandspatialreasoning. Weadoptthe
ers. WithintheTrueSkill™framework,defeating TrueSkill™ratingsastheevaluationmetrics.
higher-skilledopponentsyieldsmorepointscom-
TexasHold’em TexasHold’em4isawell-known
pared to victories over lower-skilled adversaries.
cardgame. In LLMARENA,eachgameisplayed
This approach stands in stark contrast to simple
by two LLM agents. At the beginning of each
winratecalculations,whichdonotaccountforthe
game, playersaredealttwoprivatecards, known
skillleveloftheopponent. Suchopponent-related
asholecards. Subsequently,fivecommunitycards
metricsprovideanaccurateevaluationofanagent’s
aredealtacrossthreestages-theflop,theturn,and
realcompetenciesinamulti-agentenvironment.
2https://pettingzoo.farama.org/
Reward Currently, a substantial amount of re-
environments/classic/tictactoe/
searchemploysexpertLLMstoassessthecapabil- 3https://pettingzoo.farama.org/
ities of other LLMs, yet this approach frequently environments/classic/connect_four/
4https://pettingzoo.farama.org/
1https://pettingzoo.farama.org/ environments/classic/texas_holdem_no_
content/environment_creation/ limit/theriver. Playersaimtoconstructthebestpossible bidswithoutknowledgeofeachother’sbids,and
five-card hand using a combination of their hole thehighestbidderwins. InLLMARENA,twoLLM
cards and the community cards. Throughout the agentsareparticipatingineachauction. Eachagent
game,therearefourbettingrounds. Ineachround, receivesarandominnervaluationoftheitem. The
playershavetheoptiontochoosefromthefollow- winningLLMintheauctionwillreceivetheirinner
ing actions: Fold, Check & Call, Raise Half Pot, valuationminustheactualbidasareward. Thisen-
RaiseFullPot,andAll-In. Thisgameenvironment vironmentevaluatestheLLM’snumericalreason-
demandsarangeofcapabilitiesfromLLMs,includ- ingandopponentmodelingabilities. Wemeasure
ingnumericalreasoning,opponentmodeling,and theLLM’sperformanceusingtheaveragereward
riskassessment. WeadopttheTrueSkill™ratings itachievesastheevaluationmetric.
astheevaluationmetrics.
Hanabi Hanabi is a cooperative card game
Undercover Undercoverisapartygame,where wherein two LLM agents can only view each
players are divided into two groups: undercover other’shandofcards. Theyuseinfotokensto“re-
andnon-undercover. Twogroupsofplayerseach vealcards”toeachother,“discardcards”toacquire
receive a pair of similar but distinct words. The moreinformationtokens,and“playcards”inacer-
gamehastwostagesperround: acommunication tain order to form fireworks. This requires team
stagewhereplayersdescribetheirwords,followed collaboration, strategic planning, and numerical
by a voting stage to identify an undercover. At reasoning skills from the LLMs. Upon success-
thistime,ifallundercoversareeliminated,thenon- fully setting up a firework, both LLM agents are
undercovers win, otherwise the next round will rewardedwithpointsequivalenttothesumofthe
proceed. In LLMARENA, 5 LLM agents play in card values in the firework. In LLMARENA, we
each game, which includes four non-undercover formthesametwoLLMsintoacooperationteam,
agents using GPT-3.5-Turbo and one undercover and we employ the average reward obtained by
LLMundergoingevaluation. Thegamelastsupto variousLLMteamsasanevaluationmetric.
two rounds, and if the undercovers remain unde-
3 Experiments
tected,theywin. Giventhesuperiorcapabilitiesof
GPT-4,employingitasthenon-undercoveragent 3.1 ExperimentsSetup
wouldrenderitnearlyimpossibleforothermodels
We conduct experiments with the closed-source
to win. Therefore, we choose GPT-3.5-Turbo as
GPT family LLM by directly calling its API. As
non-undercoveragents. ThisgametestsanLLM’s
fortheopen-sourceLLM,wedeployitlocallyand
proficiencyincommunication,opponentmodeling,
encapsulateitasanOpenAIAPIcall. Toensurere-
andteamcollaboration. Theevaluationmetricused
producibility,wesetthetemperatureofallLLMsto
hereisthewinningrateofeachLLMwhenplaying
0. InallenvironmentsexceptUndercover,wecon-
astheundercoveragainstGPT-3.5-Turbo.
ducted LLMARENArunsuntiltheTrueSkill™rat-
Bargain Bargain(Lewisetal.,2017)isasemi- ings between models reached convergence, with
cooperative game where two LLM agents are re- over50gamesplayedperenvironmentpermodel.
quiredtodeviseadistributionstrategyforapoolof For the Undercover environment, we played 100
items. Eachitemholdsadistinct,undisclosedvalue games of each LLM as an undercover agent in a
forbothagents. Thegameaimstoreachaconsen- gamewithfourGPT-3.5-turboasnon-undercover
susontheallocation. Whenanagreementonthe agents. Initially, all LLM agents started with a
allocationisachieved,theagentwiththemaximum TrueSkill™ scoreof25.0andavarianceof8.33.
totalvaluefromtheitemssecuredemergesvictori-
3.2 MainResult
ous. BargainrequiresLLMtohavetheabilityof
In Table 1, we present the relative normalized
numerical reasoning, communication, and oppo-
scoresfor14differentLLMsacross7environments
nentmodeling. Weadoptthe TrueSkill™ratings
astheevaluationmetrics.
within LLMARENA. Theoriginaldatabeforenor-
malizationcanbefoundinAppendixC.Thistable
Bid Afirst-pricesealed-bidauction5 isawidely allowsustodrawthefollowingconclusions.
usedauctionformatinwhichallparticipantsplace •Asthescaleofthemodelparametersincreases,
there is a noticeable improvement in the capabil-
5https://en.wikipedia.org/wiki/
First-price_sealed-bid_auction ities of LLMs. For instance, LLMs with aroundLLMs TicTacToe ConnectFour TexasHold’em Bid Bargain Undercover Hanabi Average
GPT-4 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00
GPT-3.5-Turbo 82.81 96.30 81.23 84.17 99.26 92.59 84.13 88.64
Qwen-72B-Chat 90.08 80.30 94.88 84.42 92.95 62.96 89.01 84.94
Llama-2-70B 82.21 95.90 98.42 94.22 88.63 85.19 80.35 89.27
Agentlm-70B 81.44 83.62 77.35 86.10 98.33 92.59 79.58 85.57
DeepSeek-67B-Chat 68.00 88.88 96.64 20.30 89.02 74.07 68.69 72.23
SUS-Chat-34B 79.75 96.30 84.38 66.17 74.53 88.89 86.33 82.34
Yi-34B-Chat 78.23 95.36 86.06 88.77 74.18 96.30 34.30 79.03
Qwen-14B-Chat 86.40 72.67 90.56 86.82 91.13 88.89 71.70 84.02
WizardLM-13B 79.14 83.35 74.26 35.28 91.28 48.15 54.23 66.53
AgentLM-13B 78.89 89.23 86.85 30.39 94.93 62.96 22.09 66.48
DeepSeek-7B-Chat 81.17 73.06 96.10 14.53 92.56 74.07 0.00 61.64
AgentLM-7B 68.29 90.35 85.14 0.04 83.53 66.67 98.20 70.32
Vicuna-7B 64.95 80.12 90.46 68.94 86.33 62.96 87.37 77.30
Average 80.10 87.53 88.74 61.44 89.76 78.31 68.28 79.16
Table1: Normalizedscoresof14differentLLMsin7environments.
70Bparametersexhibitanaverageperformanceof strategic planning, numerical reasoning, risk as-
82.87. LLMswithapproximately30Band10Bpa- sessment,communication,opponentmodeling,and
rametersaverage80.68and71.05inperformance, teamcollaboration. Wechosetoexperimentwith7
respectively. It’snoteworthythattheperformance LLMsthatdemonstratesuperiorperformance.
enhancementobservedwhenescalatingthemodel
sizefrom10Bto30Bparameters(+9.63)ismore SpatialComprehension Toinvestigatethespa-
pronouncedthantheimprovementseenfrom30B tialcomprehensioncapabilitiesofLLMs,wecon-
to70Bparameters(+2.19). ducted100self-playgameswith7differentLLMs
•Whileit’sgenerallyobservedthatLLMswith each, recording winning rates and the frequency
a larger scale of parameters tend to outperform ofhallucinations(instancesofillegalmoves). Si-
their counterparts with fewer parameters within multaneously,weeliminatedthepromptsregarding
the same series, exceptions can arise in specific the positioning of the pieces in the LLMs’ input
environments. Acaseinpointistheperformance andexecutedanothersetof100self-games,again
of Qwen-72B in the Undercover environment is tallying the winning rates and the occurrences of
surprisingly25.93pointslowerthanthatofQwen- hallucinations. The results are shown in Table 2,
14B,whichnotablydivergesfromthistrend. whichindicatesthatinbothscenarios,theabsence
of positional cues significantly escalates the like-
•TheperformanceofLLMsinboththeBidand
lihoodofLLMsexperiencinghallucinations,with
Hanabienvironmentssignificantlylagsbehindthe
anaverageincreaseof59.5%. Consequently,this
averageofall7environments,withdeficitsof17.72
resultsinasubstantialreductionintheirwinrates,
and10.88points,respectively. Smaller-scaleLLMs
whichonaveragedeclineby38.3%.
(∼10B)faredevenworse,scoring31.71and15.45
points below the average in these environments. Throughameticulouscasestudy,wediscovered
This underscores that tasks involving numerical thatLLMsdemonstratealimitedgraspoftheverti-
reasoning,opponentmodeling,andteamcollabora- caldimensionon2Dchessboards. Thislimitation
tionposesignificantchallengesforLLMs. likely arises because we transform the 2D chess-
boardintoa1DstringinputfortheLLMsvialine
•Thereremainsasignificantdisparitybetween
breaks. Graspingtheverticalconceptrequiresthe
GPT-4andothermodels. GPT-4hasattainedSOTA
LLM to correlate two tokens separated by a spe-
performanceacrossallevaluatedtasks,outperform-
cific distance in the one-dimensional input. Con-
ingothermodelsbyanaveragemarginof16.76.
sequently,eventhoughConnectFour’schessboard
4 Analysis islargeranditsstatespacemorecomplex,TicTac-
Toe’s 2D action space demands a higher level of
Inthissection,wewillconductathoroughanalysis spatial comprehension from the LLM. Hence, in
oftheexperimentalresults,evaluatingLLMsfrom theabsenceofpositionalprompts,theperformance
seven key perspectives: spatial comprehension, deteriorationinTicTacToeismorepronounced. ATicTacToe ConnectFour
LLMs
WinRate w/oHint ErrorRate w/oHint WinRate w/oHint ErrorRate w/oHint
GPT-4 69% 31%(↓38%) 0% 0% 31% 0%(↓31%) 25% 75%(↑50%)
GPT-3.5-Turbo 22% 17%(↓5%) 37% 38%(↑1%) 51% 42%(↓9%) 21% 24%(↑3%)
LLaMA2-70-Chat 56% 0%(↓56%) 0% 100%(↑100%) 56% 0%(↓56%) 0% 100%(↑100%)
Qwen-72B-Chat 56% 0%(↓56%) 0% 99%(↑99%) 61% 18%(↓43%) 0% 42%(↑42%)
Qwen-14B-Chat 56% 0%(↓56%) 0% 100%(↑100%) 54% 22%(↓32%) 3% 63%(↑60%)
DeepSeek-67B-Chat 55% 2%(↓53%) 4% 92%(↑88%) 64% 11%(↓53%) 0% 63%(↑63%)
DeepSeek-7B-Chat 52% 3%(↓49%) 7% 90%(↑83%) 36% 37%(↑1%) 4% 48%(↑44%)
Table2: ExperimentalresultsofsevenLLMsinTicTacToeandConnectFourenvironments,“WinRate”represents
thewinningrateofLLMswithpositionhintsin100games,“ErrorRate”representstherateofillegalmoves,“w/o
Hint”representstheresultswithoutpositionhint.
casestudycanbefoundinAppendixB.1 amoreadvantageousposition,asindicatedbythe
predominantlypositiverewardstheyreceiveonav-
Strategic Planning To delve into the strategic
erage. Conversely,LLMswithasmallerparameter
planningcapabilitiesofLLMswithinamulti-agent
scale tend to earn, on average, negative rewards.
environment,wecraftedaspecializedboardvalua-
Thissuggestsaninabilitytoaccuratelyassessthe
tionfunctiontailoredfortheConnectFourenviron-
current state of the chessboard and to formulate
mentasfollows:
effective strategies, ultimately leading the game
V(s) = 10×(My (s)−Oppo (s)) toprogressinadirectionthatfavorsitsopponent.
4 4
+5×(My (s)−Oppo (s)) (1) TofurtherexplorethegapbetweenLLM’sstrate-
3 3
gic planning capabilities and humans, we manu-
+2×(My (s)−Oppo (s)),
2 2
allyplayedfivegameswithsevenLLMseachand
where s represents the board status, My (s), humanplayersemergedvictoriousineverygame.
4
My (s),andMy (s)representthecountoffour, Simultaneously,weobservethatevenGPT-4rarely
3 2
three, and two consecutive pieces of the player prioritizes reward maximization (such as turning
on the chessboard, respectively. Conversely, a blind eye to winning positions). This tendency
Oppo (s),Oppo (s),andOppo (s)representthe couldstemfromthefactthatLLMspredominantly
4 3 2
similarformationsfortheopponent’spieces. We basetheirdecisionsontextualdata. Consequently,
definedtherewardasthechangeinboardvaluation the model must translate the game state into text
betweenthestateaftertheagent’smoves′ andthe descriptionsbeforemakingdecisions. Thisprocess
statefollowingthepreviousmoves: ofconversionmayresultininformationloss,affect-
ing the model’s strategic performance. An error
R(a,s,s′) = V(s′)−V(s), (2) analysiscanbefoundinAppendixB.1
where a represents the LLM agent’s action. We
Communication Toexplorethecommunication
conducted random battles between seven LLM
capabilities of LLM, we followed the settings
agentsuntiltherewardsconverged,andtheresults
in Section 2.4 and manually analyzed 50 game
areshowninFigure2. ItisevidentthatLLMswith
recordsforeachLLM,calculatingthepercentage
alargernumberofparametersconsistentlydemon-
of their in-game clues that accurately described
stratesuperiorstrategicplanningabilities. Eachde-
theirsecretword(“Description”inTable3). Asil-
cisionmadebytheseLLMspropelsthemtowards
lustratedinTable3,LLMsdemonstratedoutstand-
200 ing proficiency in this task. All LLMs achieved
182.72
175 a description success rate of 95.30% on average,
150 131.17 with four of them reaching a perfect 100% suc-
125 112.0 cess rate. This high level of performance can be
100
attributedtotheLLMsbeingtrainedonextensive
75 62.22
50 46.2 49.57 textualdatasets,whichequipthemwithanuanced
25 understandingofwordassociationsandcontextual
0 -2.45 meanings,allowingthemtomoreeasilydelineate
2 45 0 -35.96 the meanings of different words. However, we
Human GPT-4 GPT-3.5 LLaMA2-70B Qwen-72B Qwen-14B Deepseek-6 D7B eepseek-7b
also observed that receiving information poses a
LLMs
Figure 2: Average reward received by Human and 7 greaterchallengeforLLMscomparedtogivingin-
LLMspergame.
hctaM/sdraweRComparison of Scores
1.00 0.98
LLMs Description Guess Guess(strict)
0.75 0.67
0.56
GPT-4 100.00% 80.00% 20.00% 0.50
0.36
GPT-3.5-Turbo 100.00% 86.00% 6.00% 0.28 0.28
0.25
LLaMA2-70B-Chat 93.50% 52.00% 2.00%
Qwen-72B-Chat 100.00% 66.00% 10.00% 0.00
-0.16
Qwen-14B-Chat 87.65% 2.00% 0.00% 0.25
DeepSeek-67B-Chat 100.00% 74.00% 6.00%
0.50
DeepSeek-7B-Chat 85.91% 38.00% 0.00%
0.75
Table3: Threedifferentmetricsobtainedby7LLMsin GPT-4 GPT-3.5 LLaMA2-70B Qwen-72B Qwen-14 DB eepseek-6 D7 eB epseek-7B
theUndercoverenvironment.
Figure3: Scoresobtainedby7LLMsin100bidgames.
formation. Wheninterpretingmessagesfromother
seventypesofLLMsintheBidenvironment,and
LLMs, they often make cognitive errors, leading
countthedifferencebetweentheirbidsv′ andthe
tomisinterpretationsordistortionsoftheoriginal
Nashequilibrium v,andcalculatethenormalized
clues. This may be because their interpretations 2
v′−v/2
areoftenbasedontheirknowledgebaseratherthan scorescore = . Theresultsareshowninthe
v/2
relyingentirelyoninputinformation. Thiscanlead Figure3. Thedatapresentedinthefigureallowsus
them to misinterpret the intent or context of the todeducethatnearlyallLLMs,exceptDeepseek-
originalmessage,thustamperingwiththeoriginal 7B, tend to place bids that exceed the Nash equi-
clue,whichshowsthatLLMsstillhavelimitations librium, with their median bids being on average
inhandlingcomplex,multi-levelcommunications. 52% higher. Notably, Deepseek-67B exhibits an
AcasestudycanbefoundinAppendixB.2 extremebehavior: itoptsforbidsthatsignificantly
sacrificeitsprofits,ostensiblytoguaranteeawinin
OpponentModeling Moreover,theUndercover theauction. Consequently,itsoverallperformance
environmentdemandsthatLLMspossessnotonly inthisparticularenvironmentisnotablysubparas
communication skills but also the capability to illustratedinTable1.
modelopponents. Buildinguponthesettingsestab- Average Win Rate for Each Action by LLM
lishedinSection2.4,wefurthertaskedeachLLM 0.9 Fold
0.8 Check and Call
agentwithdeducingthesecretwordsofothersdur- Raise Half Pot
ing the voting phase. We recorded the instances 0.7 Raise Full Pot
All in
0.6
where each LLM accurately guessed the secret
0.5
wordofanyoneagent(“Guess”inTable3),aswell
0.4
astheoccasionswheretheysuccessfullyidentified
0.3
thesecretwordsofallotheragents(“Guess(strict)”
0.2
inTable3). TheresultsareshowninTable3,where
0.1
wecanfindthatinferringthesecretwordsofothers
0.0
throughtheirdescriptionsismorechallengingthan
Deepseek-7 DB eepseek-67B LLaMA2-70B Qwen-14B Qwen-72B GPT-3.5 GPT-4
describingoneself. Throughmanualanalysis,we
LLM Name
discoveredthatonlyGPT-4possessestheanalytical Figure4: Theaverageestimatedwinratecorresponding
toeachactionin100TexasHold’emgamesfor7LLMs.
andreasoningskillsnecessarytoidentifyitselfas
anundercoveragent. Subsequently,itadjuststhe RiskAssessment Toinvestigatetheriskassess-
cluesitprovidesinthefollowingroundformoreef- mentcapabilitiesofLLMsinanincompleteinfor-
fectivecamouflage. Thisadvancedabilitytomodel mation environment, we sampled 100 static card
opponentsalsoenablesGPT-4tomoreaccurately gamesfromthedynamicTexasHold’emenviron-
guessthesecretwordsofallotheragents,resulting ment. Ineachgame,theLLMsareequippedwith
insuperiorperformance. only two private cards and three public cards as
theirinformationbasis. WeuseMonteCarlosam-
NumericalReasoning Bidrepresentsatypical plingtoestimatethewinningrateofLLMineach
probleminincompleteinformationgames. Insuch game,thenrecordtheactionstakenbyeachLLM
scenarios,theBayesianNashequilibriumfortwo under these specific conditions. The specific ac-
rational agents is theoretically set at v, where v tions of each LLM, along with the average esti-
2
denotes each agent’s valuation of the item (Nis, matedwinningrateareshowninFigure4,where
2007). Westudythenumericalreasoningabilityof we can observe that LLMs tend to adopt a more
LLMsinthissituation. Weplay100self-gamesof conservative approach in their risk assessments.
serocS
etaR
niW
detamitsE
egarevAAdditionally,threeoftheLLMsconsistentlyopted 2022;Srivastavaetal.,2023)hasbecomeincreas-
for “Check and Call” strategies, and all models inglymainstream,focusingprimarilyontestingthe
exceptGPT-4refrainedfromselectingthemostag- world knowledge and reasoning capabilities in a
gressivecourseofaction,“Allin”. Thismaybebe- multi-task benchmark. In addition, some newly
causeLLMsreceivedcautionintreatinggambling- proposedbenchmarksandmethodsexaminelarge
relatedscenariosduringthepre-trainingandsecu- languagemodelsfrommultipledimensions,such
rity alignment phase. Simultaneously, we noted asadaptabilitytoprofessionalfields(Xiangetal.,
thatGPT-4exhibitsarobustriskassessmentcapa- 2023),real-worldapplication(Lietal.,2023b;Liu
bility. It tends to choose “All in” when the win- et al., 2024c,b,a), robustness (Zhu et al., 2023),
ningrateisexceptionallyhigh. Moreover,GPT-4 multi-modalcapabilities(Fuetal.,2023),etc. Re-
adaptsitsstrategyasthewinningrateofthehand cently, the trend of treating large language mod-
decreases, opting for relatively low-risk actions. els as agents has become increasingly evident in
Notably,whenthehand’swinningratefallsbelow academia,andaseriesofstudies(Liuetal.,2023;
40%,GPT-4isthesoleLLMthatselects“Fold”as Wuetal.,2023)aimedtoevaluatetheperformance
astrategicdecisiontomitigatelossesontime. of these models when acting as agents in games.
However, most of these studies are limited to a
TeamCollaboration AsoutlinedinSection2.4,
singleagent,thusfailingtofullycapturethechar-
withintheHanabienvironment,LLMsarerequired
acteristics of large language models in game or
toalternatebetween“discardingcards”and“reveal-
cooperativescenariosandgroupbehavior.
ingcards”tocooperatetoobtainmoreinformation,
andthenbuildfireworksthrough“playcards”. To LLM-basedAgents Reinforcementlearninghas
investigatetheefficacyofLLMsinexecutingteam been extensively employed in the training of au-
cooperation, we analyzed the frequency of each tonomousagents(Ribeiro,2002;Isbelletal.,2001;
typeofactionperformedbyLLMs. Asshownin Mnihetal.,2013). Withthesuperiorcapabilitiesof
Figure 5, although most LLMs frequently utilize LLMs,theutilizationofLLMsascognitiveentities
the“revealcards”actiontosharehandinformation andcontrollersforagentshasgarneredwidespread
withtheiropponent,theyseldomusethe“discard acclaim (Huang et al., 2022; Park et al., 2023;
cards” action, which would give the opponent a Sumers et al., 2023). Wei et al. (2022) initially
chanceto“revealcards”formoreinformation,thus introducedtheCoTtechnology,whichnotablyam-
rashlychoosing“playcards”whenthereisinsuffi- plifiesthereasoningandplanningabilitiesofLLM-
cientinformation,causingthegametofail. basedagents,consequentlyleadingaresearchsurge
1.0 dominatedbyreasoningstrategies(Yaoetal.,2023;
Play Card
0.9 Reveal Card Wang et al., 2023b; Zhou et al., 2023a). Further-
Discard Card
0.8 more,empiricalstudiesindicatethatcollaborative
0.7
andadversarialframeworks(Lietal.,2023a;Chen
0.6
0.5 etal.,2023;Chanetal.,2023)involvingmultiple
0.4 LLMs surpass the capabilities of singular agents
0.3 acrossvarioustasksandmayleadtotheemergence
0.2
ofsocialphenomena(Parketal.,2023).
0.1
0.0
Deepseek-7 DB eepseek-67BLLaMA2 Qwen-14B Qwen-72B GPT-3.5 GPT-4 6 Conclusion
Figure5: Theproportionofthreetypesofactionswhen Inourstudy,weintroduced LLMARENA,abench-
playingHanabiamong7LLMs. mark designed to assess the diverse abilities of
LLMagentsindynamic,multi-agentenvironments.
5 RelatedWorks
Through comprehensive analysis of seven game
LLM Evaluation With the rapid development environments that require necessary abilities for
of LLMs, the evaluation practices of traditional LLM agents, our findings reveal that LLMs ex-
NLP tasks (Hu et al., 2023; Bang et al., 2023; hibit weaknesses in spatial reasoning, opponent
Zhongetal.,2023;Qinetal.,2023;Huetal.,2022) modeling,andteamcollaboration. Enhancingthe
limittheirperformance. Instead,theuseofhuman- performance of LLM agents in dynamic multi-
writtenexamquestionsasanassessmentparadigm agent settings remains an unresolved challenge.
(Huetal.,2024;Hendrycksetal.,2021;Liangetal., We anticipate that future researchers will utilize
noitcA
hcaE
fo
noitroporPLLMARENA toconductevaluationsinabroader Dean Foster, Dylan J Foster, Noah Golowich, and
rangeofscenarios. Alexander Rakhlin. 2023. On the complexity of
multi-agent decision making: From learning in
Limitation games to partial monitoring. In The Thirty Sixth
AnnualConferenceonLearningTheory,pages2678–
Whileourresearchprimarilyfocusesonassessing 2792.PMLR.
the capabilities of LLM agents within dynamic
ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,
multi-agent environments, it is important to ac-
MengdanZhang,XuLin,JinruiYang,XiawuZheng,
knowledgetwolimitations. Firstly,inthejourney KeLi,XingSun,etal.2023. Mme:Acomprehensive
towards achieving autonomous agents, an LLM evaluationbenchmarkformultimodallargelanguage
agent’s ability extends beyond merely analyzing models. arXivpreprintarXiv:2306.13394.
textualinformation. Itnecessitatesengagingwitha
Jiaxian Guo, Junnan Li, Dongxu Li, Anthony
varietyofmodalinputs,includingvideoandaudio.
Meng Huat Tiong, Boyang Li, Dacheng Tao, and
ThecapabilitiesofLLMswithinsuchmulti-modal StevenHoi.2023. Fromimagestotextualprompts:
dynamiccontextsremainanuntappedareaofex- Zero-shot visual question answering with frozen
large language models. In Proceedings of the
ploration. Secondly, our study did not delve into
IEEE/CVFConferenceonComputerVisionandPat-
thepotentialofLLMsthatleverageexternaltools.
ternRecognition,pages10867–10877.
We anticipate and encourage further research in
bothofthesepromisingdimensionstobroadenour Dan Hendrycks, Collin Burns, Steven Basart, Andy
understandingandcapabilitiesofLLMagents. Zou,MantasMazeika,DawnSong,andJacobStein-
hardt.2021. Measuringmassivemultitasklanguage
EthicalConsiderations understanding. ProceedingsoftheInternationalCon-
ferenceonLearningRepresentations(ICLR).
TheethicallandscapesurroundingLLMagentsis
RalfHerbrich,TomMinka,andThoreGraepel.2006.
marked by significant challenges, particularly in
Trueskill™:abayesianskillratingsystem. Advances
therealmsofresponsibleusage,andthepotential
inneuralinformationprocessingsystems,19.
formisuse. ThedeploymentofautonomousLLM
agents in decision-making roles raises questions XumingHu, JunzheChen, XiaochuanLi, YufeiGuo,
aboutaccountabilityandtheneedforrobustframe- LijieWen,PhilipS.Yu,andZhijiangGuo.2024. Do
largelanguagemodelsknowaboutfacts? 12thIn-
workstopreventunethicalapplications. Thepoten-
ternationalConferenceonLearningRepresentations,
tialformisuse,suchasmanipulatingscenariosor
ICLR 2024, Messe Wien Exhibition and Congress
exploitingsystems, underscoresthenecessityfor Center,ViennaAustriaMay7th,2024toMay11th,
stringentguidelinesandmonitoring. 2024,2024.
XumingHu,ZhijiangGuo,JunzheChen,LijieWen,and
References PhilipS.Yu.2023. MR2: Abenchmarkformulti-
modalretrieval-augmentedrumordetectioninsocial
2007. AlgorithmicGameTheory. CambridgeUniver- media. InProceedingsofthe46thInternationalACM
sityPress. SIGIRConferenceonResearchandDevelopmentin
InformationRetrieval,SIGIR2023,Taipei,Taiwan,
YejinBang,SamuelCahyawijaya,NayeonLee,Wen- July23-27,2023,pages2901–2912.ACM.
liangDai,DanSu,BryanWilie,HolyLovenia,Ziwei
Ji,TiezhengYu,WillyChung,etal.2023. Amulti-
Xuming Hu, Zhijiang Guo, Guanyu Wu, Aiwei Liu,
task,multilingual,multimodalevaluationofchatgpt
LijieWen,andPhilipSYu.2022. Chef: Apilotchi-
onreasoning,hallucination,andinteractivity. arXiv
nesedatasetforevidence-basedfact-checking. arXiv
preprintarXiv:2302.04023.
preprintarXiv:2206.11863.
Chi-MinChan,WeizeChen,YushengSu,JianxuanYu,
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and
Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan
IgorMordatch.2022. Languagemodelsaszero-shot
Liu.2023. Chateval: Towardsbetterllm-basedeval-
planners: Extractingactionableknowledgeforem-
uators through multi-agent debate. arXiv preprint
bodiedagents. InInternationalConferenceonMa-
arXiv:2308.07201.
chineLearning,pages9118–9147.PMLR.
Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang,
Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Charles Isbell, Christian R Shelton, Michael Kearns,
Qin, Yaxi Lu, Ruobing Xie, et al. 2023. Agent- Satinder Singh, and Peter Stone. 2001. A social
verse: Facilitatingmulti-agentcollaborationandex- reinforcementlearningagent. InProceedingsofthe
ploringemergentbehaviorsinagents. arXivpreprint fifthinternationalconferenceonAutonomousagents,
arXiv:2308.10848. pages377–384.TakeshiKojima,ShixiangShaneGu,MachelReid,Yu- OpenAI. 2023. GPT-4 Technical Report. CoRR,
takaMatsuo,andYusukeIwasawa.2022. Largelan- abs/2303.08774.
guagemodelsarezero-shotreasoners. Advancesin
neural information processing systems, 35:22199– Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Ji-
22213. apuWang,andXindongWu.2024. Unifyinglarge
languagemodelsandknowledgegraphs: Aroadmap.
Mike Lewis, Denis Yarats, Yann N Dauphin, Devi IEEE Transactions on Knowledge and Data Engi-
Parikh, and Dhruv Batra. 2017. Deal or no deal? neering.
end-to-endlearningfornegotiationdialogues. arXiv
JoonSungPark,JosephO’Brien,CarrieJunCai,Mered-
preprintarXiv:1706.05125.
ithRingelMorris,PercyLiang,andMichaelSBern-
Guohao Li, Hasan Abed Al Kader Hammoud, Hani stein.2023. Generativeagents: Interactivesimulacra
Itani, Dmitrii Khizbullin, and Bernard Ghanem. ofhumanbehavior. InProceedingsofthe36thAn-
2023a. CAMEL:Communicativeagentsfor”mind” nual ACM Symposium on User Interface Software
exploration of large language model society. In andTechnology,pages1–22.
Thirty-seventh Conference on Neural Information
Chen Qian, Xin Cong, Cheng Yang, Weize Chen,
ProcessingSystems.
YushengSu,JuyuanXu,ZhiyuanLiu,andMaosong
Sun.2023. Communicativeagentsforsoftwarede-
Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song,
velopment. arXivpreprintarXiv:2307.07924.
HangyuLi,HaiyangYu,ZhoujunLi,FeiHuang,and
Yongbin Li. 2023b. API-bank: A comprehensive
ChengweiQin,AstonZhang,ZhuoshengZhang,Jiaao
benchmarkfortool-augmentedLLMs. InProceed-
Chen,MichihiroYasunaga,andDiyiYang.2023. Is
ingsofthe2023ConferenceonEmpiricalMethods
ChatGPTageneral-purposenaturallanguageprocess-
inNaturalLanguageProcessing,pages3102–3116,
ing task solver? In Proceedings of the 2023 Con-
Singapore.AssociationforComputationalLinguis-
ferenceonEmpiricalMethodsinNaturalLanguage
tics.
Processing,pages1339–1384,Singapore.Associa-
tionforComputationalLinguistics.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris
Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
CHCRRibeiro.2002. Reinforcementlearningagents.
Zhang,DeepakNarayanan,YuhuaiWu,AnanyaKu-
Artificialintelligencereview,17:223–250.
mar, et al. 2022. Holistic evaluation of language
models. arXivpreprintarXiv:2211.09110. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
AbuAwalMdShoeb,AbubakarAbid,AdamFisch,
Aiwei Liu, Leyi Pan, Xuming Hu, Shu’ang Li, Lijie
AdamRBrown,AdamSantoro,AdityaGupta,Adrià
Wen, IrwinKing, andPhilipS.Yu.2024a. Anun-
Garriga-Alonso, et al. 2023. Beyond the imitation
forgeablepubliclyverifiablewatermarkforlargelan-
game: Quantifying and extrapolating the capabili-
guage models. 12th International Conference on tiesoflanguagemodels. TransactionsonMachine
LearningRepresentations,ICLR2024,MesseWien
LearningResearch.
ExhibitionandCongressCenter,ViennaAustriaMay
7th,2024toMay11th,2024,2024. TheodoreRSumers,ShunyuYao,KarthikNarasimhan,
and Thomas L Griffiths. 2023. Cognitive ar-
Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, and chitectures for language agents. arXiv preprint
LijieWen.2024b. Asemanticinvariantrobustwater- arXiv:2309.02427.
markforlargelanguagemodels. 12thInternational
ConferenceonLearningRepresentations,ICLR2024, J Terry, Benjamin Black, Nathaniel Grammel, Mario
MesseWienExhibitionandCongressCenter,Vienna Jayakumar,AnanthHari,RyanSullivan,LuisSSan-
AustriaMay7th,2024toMay11th,2024,2024. tos,ClemensDieffendahl,CarolineHorsch,Rodrigo
Perez-Vicente, et al. 2021. Pettingzoo: Gym for
AiweiLiu,LeyiPan,YijianLu,JingjingLi,XumingHu, multi-agent reinforcement learning. Advances in
Xi Zhang, Lijie Wen, Irwin King, Hui Xiong, and NeuralInformationProcessingSystems,34:15032–
PhilipS.Yu.2024c. Asurveyoftextwatermarking 15043.
intheeraoflargelanguagemodels.
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
XiaoLiu,HaoYu,HanchenZhang,YifanXu,Xuanyu Martinet,Marie-AnneLachaux,TimothéeLacroix,
Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen BaptisteRozière,NamanGoyal,EricHambro,Faisal
Men,KejuanYang,etal.2023. Agentbench:Evaluat- Azhar,AurélienRodriguez,ArmandJoulin,Edouard
ingllmsasagents. arXivpreprintarXiv:2308.03688. Grave,andGuillaumeLample.2023a. Llama: Open
and efficient foundation language models. CoRR,
Volodymyr Mnih, Koray Kavukcuoglu, David Sil- abs/2302.13971.
ver, AlexGraves, IoannisAntonoglou, DaanWier-
stra, and Martin Riedmiller. 2013. Playing atari Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
with deep reinforcement learning. arXiv preprint bert, Amjad Almahairi, Yasmine Babaei, Nikolay
arXiv:1312.5602. Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale,DanBikel,LukasBlecher,CristianCanton-
OpenAI.2022. ChatGPT. Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,JudeFernandes,JeremyFu,WenyinFu,BrianFuller, Shunyu Yao, Howard Chen, John Yang, and Karthik
CynthiaGao,VedanujGoswami,NamanGoyal,An- Narasimhan.2022. Webshop: Towardsscalablereal-
thonyHartshorn,SagharHosseini,RuiHou,Hakan worldwebinteractionwithgroundedlanguageagents.
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa, AdvancesinNeuralInformationProcessingSystems,
IsabelKloumann,ArtemKorenev,PunitSinghKoura, 35:20744–20757.
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
anaLiskovich,YinghaiLu,YuningMao,XavierMar- Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
tinet,TodorMihaylov,PushkarMishra,IgorMoly- Thomas L. Griffiths, Yuan Cao, and Karthik R
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- Narasimhan. 2023. Tree of thoughts: Deliberate
stein,RashiRungta,KalyanSaladi,AlanSchelten, problem solving with large language models. In
Ruan Silva, Eric Michael Smith, Ranjan Subrama- Thirty-seventh Conference on Neural Information
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- ProcessingSystems.
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Dacheng Tao. 2023. Can chatgpt understand too?
Melanie Kambadur, Sharan Narang, Aurélien Ro-
acomparativestudyonchatgptandfine-tunedbert.
driguez,RobertStojnic,SergeyEdunov,andThomas
arXivpreprintarXiv:2302.10198.
Scialom. 2023b. Llama 2: Open foundation and
fine-tunedchatmodels. CoRR,abs/2307.09288.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,
Nathan Scales, Xuezhi Wang, Dale Schuurmans,
PeiyiWang,LeiLi,LiangChen,DaweiZhu,Binghuai
ClaireCui,OlivierBousquet,QuocVLe,andEdH.
Lin,YunboCao,QiLiu,TianyuLiu,andZhifangSui.
Chi.2023a. Least-to-mostpromptingenablescom-
2023a. Largelanguagemodelsarenotfairevaluators.
plex reasoning in large language models. In The
arXivpreprintarXiv:2305.17926.
EleventhInternationalConferenceonLearningRep-
XuezhiWang,JasonWei,DaleSchuurmans,QuocVLe,
resentations.
EdH.Chi,SharanNarang,AakankshaChowdhery,
KunZhou,YutaoZhu,ZhipengChen,WentongChen,
andDennyZhou.2023b. Self-consistencyimproves
Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong
chainofthoughtreasoninginlanguagemodels. In
Wen,andJiaweiHan.2023b. Don’tmakeyourllm
TheEleventhInternationalConferenceonLearning
an evaluation benchmark cheater. arXiv preprint
Representations.
arXiv:2311.01964.
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen
Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,
Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei
etal.2022. Chain-of-thoughtpromptingelicitsrea-
Ye, Neil Zhenqiang Gong, Yue Zhang, et al. 2023.
soninginlargelanguagemodels. AdvancesinNeural
Promptbench: Towardsevaluatingtherobustnessof
InformationProcessingSystems,35:24824–24837.
largelanguagemodelsonadversarialprompts. arXiv
preprintarXiv:2306.04528.
XiangWei,XingyuCui,NingCheng,XiaobinWang,
Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,
Yufeng Chen, Meishan Zhang, et al. 2023. Zero-
shotinformationextractionviachattingwithchatgpt.
arXivpreprintarXiv:2302.10205.
YueWu,XuanTang,TomMMitchell,andYuanzhiLi.
2023. Smartplay:Abenchmarkforllmsasintelligent
agents. arXivpreprintarXiv:2310.01557.
Tong Xiang, Liangzhi Li, Wangyue Li, Mingbai Bai,
LuWei,BowenWang,andNoaGarcia.2023. Care-
mi: chinese benchmark for misinformation evalu-
ation in maternity and infant care. arXiv preprint
arXiv:2307.01458.
Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren,
Zhen Dong, Kurt Keutzer, See Kiong Ng, and Ji-
ashi Feng. 2023. Magic: Benchmarking large lan-
guagemodelpoweredmulti-agentincognition,adapt-
ability,rationalityandcollaboration. arXivpreprint
arXiv:2311.08562.
JingfengYang, HongyeJin, RuixiangTang, Xiaotian
Han,QizhangFeng,HaomingJiang,BingYin,and
XiaHu.2023. Harnessingthepowerofllmsinprac-
tice: Asurveyonchatgptandbeyond. arXivpreprint
arXiv:2304.13712.A CompletePromptDesign
Inthissection, weprovidecompletepromptsforallsevengameenvironmentsinLLMArena. Asthe
agentintheLLMArenamustconsistentlyinteractwiththeenvironment,similartothereinforcement
learningsetting,variouspromptsarerequiredtoguidetheagentincomprehendingtherulesandtaking
action. Specifically,threetypesofpromptswork:
• SystemPrompt. SystempromptsarespecialmessagesusedtosteerthebehaviorofLLM.They
allowtoprescribetheagent’sstyleandtaskwithincertainbounds,makingitmorecustomizableand
adaptableforvarioususecases.
InLLMArena,westipulatetheroleoftheagentandtherulesofthegameinthesystemprompt.
• ObservationPrompt. ObservationPromptprovidesthenecessarystateinformationfortheagentto
interactwiththeenvironment,includingtheopponent’sactions,currentgamestatus,legalactions,
etc. Weuse{}toframethesechanginginformation.
• ActionPrompt. ActionPromptguidestheAgenttochoosefromlegalactionsandoutputregularized
actions. Inaddition,tostimulatetheagent’sreasoningability,insomeenvironments,wealsoadd
CoTPrompt.
A.1 TicTacToe
InTicTacToe,thecontentin{}canbeobtainedfromtheinteractionbetweentheagentandtheenvironment.
For example, {player_type} includes "X" and "O", {board_status} represents the current chessboard,
andwedescribetheentirechessboardintheformoftext,astheexampleinSystemPrompt. {available}
representsthecurrentlegalactionlistoftheagent.
ThedetailedprompttemplateofTicTacToeisasfollows.
SystemPrompt
YouareplayingTicTacToe.
TicTacToeisplayedonathree-by-threegridbytwoplayers,whoalternatelyplacethemarksX
andOinoneoftheninespacesinthegrid.
Theplayerwhosucceedsinplacingthreeoftheirmarksinahorizontal,vertical,ordiagonal
rowisthewinner.
Inthefollowingexample,thefirstplayer(X)winsthegameinsevensteps:
1. [Player1]: X:(1,3)
2. [Player2]: O:(1,1)
3. [Player1]: X:(3,1)4. [Player2]: O:(2,2)
5. [Player1]: X:(3,3)
6. [Player2]: O:(2,3)
7. [Player1]: X:(3,2)
X plays first. Players will specify the position of the stone and the moderator will plot the
boardstatus.
Ifapositionhasbeenmarked,futuremarkscannotbeputinthesameposition.
The players interact with the game by specifying the position of the stones (x, y), where x
indicatestherowandyindicatesthecolumn,so(1,1)isthetopleftcornerand(3,3)isthebottom
rightcorner."ObservationPrompt
Youplay{player_type}.
Theboardstatusis{board_status}.
Youcanonlyputthemarkon[{available}].
ActionPrompt
You should only output {player_type} and the position of the move, for example:
"{player_type}: (1,3)"
Thepositionyouputthemarkonmustbeempty.
Don’tsayanythingbesidesmarkposition.
A.2 ConnectFour
InConnectFour,theobservationpromptandactionpromptaresimilartoTicTacToe. Wealsousetextto
describethechessboard.
ThedetailedprompttemplateofTexasHold’emisasfollows.
SystemPrompt
YouareplayingConnectFour.
ConnectFourisplayedonasix-by-sevengridbytwoplayers,whoalternatelydroptheirmakers
XandOintooneofthesevencolumns. Eachmarkerwillfalltothelowestavailablespacewithin
theselectedcolumn.
Theplayerwhosucceedsinplacingfouroftheirmakersinahorizontal,vertical,ordiagonal
rowisthewinner. Xplaysfirst. Playersinteractwiththegamebyspecifyingthecolumnnumber
wheretheywanttodroptheirmarker. Ifacolumnisalreadyfull,playerscannotdropamarkerinto
thatcolumn.
Thecolumnsarenumberedfrom1to7,fromlefttoright. Playerscannotplaceatokenoutside
thisrange.
ObservationPrompt
Youplay{player_type}.
Theboardstatusis{board_status}.
Youcanonlychooseoneofthefollowingcolumns: [{available}].
ActionPrompt
You should only output {player_type} and the column of the move you choose to put your
mark,forexample: "{player_type}: 1"
Thecolumnyouputthemarkoncannotbefull.
Don’tsayanythingbesidesmarkposition.
A.3 TexasHold’em
In Texas Hold’em, the status information of the environment mainly includes three types: hand
cards({private}),communitycards({public}),andchips({chips}).
ThedetailedprompttemplateofTexasHold’emisasfollows.SystemPrompt
YouareplayingTexasHold’emNoLimit.
HerearethebasicrulesforTexasHold’emPoker:
Inthisgame,youarerequestedtoattend1gamewithyouropponent. Bothplayersstartwith
100chips,andtheplayerwiththemostchipsattheendofthegamewins. Ifyourchipsdropto0,
youlosethegame. Theobjectiveistoobtainmorechipsthanyouropponent.
TexasHold’emhandsarerankedfromhighesttolowestasfollows:
RoyalFlush: A,K,Q,J,10allofthesamesuit.
StraightFlush: Fiveconsecutivecardsofthesamesuit. Thehighertopcardwins.
FourofaKind: Fourcardsofthesamerank. Thehigherrankwins;ifthesame,comparethe
fifthcard.
FullHouse: Threecardsofonerankandtwocardsofanotherrank. Ahigherthree-cardrank
wins;ifthesame,comparethetwo-cardrank.
Flush: Five non-consecutive cards of the same suit. Compare the highest card, then the
second-highest,andsoon.
Straight: Fiveconsecutivecardsofdifferentsuits. Thehighertopcardwins.
ThreeofaKind: Threecardsofthesamerank. Higherrankwins.
TwoPair: Twocardsofonerankandtwocardsofanotherrank. Comparethehigherpairfirst,
thelowerpair,andthenthefifthcard.
OnePair: Twocardsofthesamerank. Comparethepairfirst,thenthehighestnon-pairedcard,
thenthesecondhighest,andsoon.
HighCard: Ifnohandcanbeformed,thehighestcardwins. Ifthehighestcardsarethesame,
comparethesecondhighest,andsoon.
Ifthehandsareofequalrank,thepotissplit.
ObservationPrompt
Yourobservationnowis:
Thecardsinyourhandsare[{private}]
Thecommunitycardsare[{public}]
Younowhave100-chipschips,thechipsthatyouhaveputinuntilnowis{chips}
Nowyoucanchooseoneofthefollowingactions:[{available}]
ActionPrompt
Youshouldthinkstepbystepandoutputyouraction. Forexample: ’Action: CheckandCall’
Youshouldexplainwhyyouchosetheaction
A.4 Undercover
InUndercover,unlikeotherenvironments,itinvolvesmorethantwoagents,andthegamehastwostages.
Therefore,wedesignedtwopromptsrespectivelyfortheGivingcluesstageandtheAccusationstage.
ThedetailedprompttemplateofUndercoverisasfollows.
SystemPrompt
YouareplayingagameoftheUndercover. Herearethegamerules:
InUndercover,playersaredividedintotwogroups: undercoversandnon-undercovers.
Each player receives a secret word, but the secret word given to the undercover is different
fromtherest.
Themainaimistoidentifytheundercoverbasedonthecluesprovidedbyallplayers.Becautiouswithyourclues;beingtoospecificmightrevealyouridentity.
Asanon-undercover,youneedtounmasktheundercoverwithoutrevealingyourownsecret
word.
Asanundercover,yourtaskistoseamlesslyintegratewithothersandavoiddetection.
Undercoverisplayedintwophases:
TheGivingCluePhase: Playersgivehintsabouttheirwordsoverseveralrounds.
The Accusation Phase: Players vote on who they think the undercover is. The undercover
shouldvotefornon-undercovers.
ObservationPrompt
• Givingcluesstage
Nowisthegivingcluesstage:
Youare{player_name}. Yoursecretwordis"{word}".
Thisroundtheotherplayerssay: {messages}
• Accusationstage
Nowistheaccusationstage:
Youare{player_name}. Yoursecretwordis{word}.
Thisroundtheotherplayerssay:{messages}
ActionPrompt
• Givingcluesstage
Now you give one clue (but never include the secret word). You cannot repeat what other
playershavesaid.
You should only output {player_name} and the clue, for example: "{player_name}: It’s a
fruit.".
• Accusationstage
Nowvotewhichoftheotherplayers(excludingyourself)isundercover.
Youshouldonlyoutput"vote"andtheundercovername,forexample: "vote: player_1.".
A.5 Bargain
Inbargain,bothpartiesneedtogiveaplanandbargain. Thequantityandpriceofgoodsownedbythe
agentarerepresentedby{item_nums}and{value}respectively,andtheplanproposedbytheagentis
representedby{oppo_plan}
ThedetailedprompttemplateofBargainisasfollows.
SystemPrompt
Youareparticipatinginagameof"Bargaining."Herearethegamerules:
(1)InformationandRoles
Thegameinvolvestwoplayers, PlayerAandPlayerB.Avarietyofitemsareavailablefor
bargaining,eachhavingadifferentvaluetobothplayers. However,thevalueofeachitemforthe
otherplayerisunknown.
Playersengageinaseriesofupto10bargainingroundstoreachaconsensus.Ifaconsensusisnotreachedwithinthese10rounds,thenegotiationisconsideredafailure,
anditresultsinastalematewherebothplayersleavewithoutanyrewards.
(2)Objectives
ThegoalforbothPlayerAandPlayerBistonegotiateandsharetheseitems.
Thechallengeisthatthesameitemhasdifferentvaluesforeachplayer,andtheydonotknow
theitem’svaluefortheircounterpart.
Eachplayeraimstomaximizethetotalvalueoftheitemstheyacquirethroughnegotiation.
ObservationPrompt
(Thegamebegins. Youstarttogiveaplan.)
Nowisround{round}:
Youare{player_name}.
Thereare{item_nums[0]}hats,{item_nums[1]}ballsand{item_nums[2]}apples.
Thevalueforeachhatis{value[0]},foreachballis{value[1]}andforeachappleis{value[2]}.
Thisroundyouropponentsays:"{bargaining}"
Hewillget{oppo_plan[0]}hats,{oppo_plan[1]}ballsand{oppo_plan[2]}apples.
Andyouwillget{item_nums[0]-oppo_plan[0]}hats,{item_nums[1]-oppo_plan[1]}balls
and{item_nums[2]-oppo_plan[2]}apples.
ActionPrompt
Doyouagreewithhisplan? Ifyouagree,Youshouldonlyoutput{player_name}and"Deal",
forexample: "{player_name}: Deal.".
Ifyoudonotagree,youshouldbargainwithyouropponentfirstandthenoutput{player_name}
andyourplaninthefollowingformat:
"{player_name}: xhatsyballszapples."x,y,zareArabicnumber.
Forexample: "I’dlike1hat,2balls,and0apples. {player_name}: 1hat2balls0apples".
Donottellyouropponentthevalueofeachitem.
A.6 Bid
In Bid, the status information is only the agent’s valuation of the product ({value}) and the income
({value}-bid). Theagentneedstomaximizetheincome.
ThedetailedprompttemplateofBidisasfollows.
SystemPrompt
Youareparticipatinginagameof"First-priceSealed-BidAuction."Herearethegamerules:
(1)InformationandRoles
Participantsactasbiddersintheauction.
Eachbidderhasanopportunitytosubmitonesealedbidfortheitembeingauctioned,without
knowingthebidsofotherparticipants.
(2)Objectives
Theprimaryobjectiveistowintheauctionbysubmittingthehighestbid.
Biddersmustbalancetheirdesiretowintheitemagainsttheriskofoverpaying.
Eachbidderaimstostrategicallydeterminetheirbidbasedontheirvaluationoftheitemand
assumptionsaboutotherbidders’valuations.
(3)Strategy
Thechallengeforbiddersisindecidinghowmuchtobid. Bidtoolow,andtheyrisklosingthe
auction;bidtoohigh,andtheyriskpayingmorethantheitem’svaluetothem(thewinner’scurse).Biddersmustassessnotonlytheirvaluationoftheitembutalsotrytoanticipatethebidsof
others.
Inthisgameofstrategyandvaluation,thekeyistomakeasmartbidthatbalancesthedesireto
winwiththeriskofoverpaying.
ObservationPrompt
Your valuation of the current auction item is ${value}, your bid must be lower than your
valuation.
Youwillgetrewardsequal${value}-yourbid
ActionPrompt
Youshouldthinkstepbystepandthenoutputyourbidinthefollowingformat:
"{player_name}: $x.xx"
Forexample: {player_name}: $1.08
A.7 Hanabi
In Hanabi, the status information of the environment is more complex and challenging than in other
environments. Forexample,theopponent’shand({other_hands}),currentfireworks({fireworks}),current
tokens({tokens}),knownhintedhands({first_card,second_card}),etc. Theagentneedstointegratethis
informationtomakereasonabledecisions.
ThedetailedprompttemplateofHanabiisasfollows.
SystemPrompt
YouareplayingaHanabigame.
TherearethebasicrulesforHanabiwithasmallstructure:
Hanabiisa2player-cooperativegamewhereyoushouldworktogethertoformfireworks. A
fireworkisasetofcardsofthesamecolor,orderedfrom1to5. Cardsinthegamehavebotha
color and a number; there are two colors and five numbers in total. Each player can only view
thecardsanotherplayerholds,butnottheirown. Playerscannotdirectlycommunicatewitheach
other,butmustinsteadremoveaninfotokenfromplaytogiveinformation. Playerscantellother
playerswhichofthecardsintheirhandisaspecificcolor,oraspecificnumber. Youmustpoint
outallthecardsofthiscolorornumber. Thereareinitially3infotokens,butplayerscandiscard
cardsintheirhandanddrawanewcardfromthedecktoreturnaninfotokenintoplay. Players
canalsoplayacardfromtheirhand: thecardmusteitherbeginanewfireworkorbeappended
toanexistingfirework. However,2fireworkscannothavethesamecolor,andasinglefirework
cannotrepeatnumbers. Iftheplayedcarddoesnotsatisfytheseconditions,alifetokenisplaced.
Thegameendswheneither3lifetokenshavebeenplaced,all2fireworkshavebeencompleted,or
allcardshavebeendrawnfromthedeck. Pointsareawardedbasedonthelargestcardvaluein
eachcreatedfirework.
Inthisgame,youandyouropponentareplayers. Eachpersonhastwohandcards,eachwith
twocolorsandfivepossiblenumbersfrom1to5. Playersshare3infotokensand1lifetoken.
ObservationPrompt
Yourobservationnowis:
Thecardsinanotherplayer’shandsare:{other_hands}
Thecurrentfireworksis:{fireworks}Thecurrentnumberoftokensis{tokens}
Theinformationofyourowncardsthatwasrevealedis:{first_card,second_card}
Thecurrentdecksizeis{deck_size}
Youropponent’sactionis{opponent_action}
Thecardthattheopponentlastplayedordiscardedis{last_played}
Nowyoucanchooseoneofthefollowingactions: {available}
ActionPrompt
Youshouldthinkstepbystepandoutputyouraction. Forexample: "RevealYellowCardsfor
anotherplayer"
Youwillrespondwithanaction,formattedas:
"Action: <action>"
whereyoureplace<action>withyouractualaction.
Youshouldexplainwhyyouchosetheaction.
B CaseStudy
Inthissection,wepresentadetailedcasestudyandconductanerroranalysiswithintheConnectFourand
Undercoverenvironments.
B.1 ConnectFour
AsdemonstratedinSection4,modelswithsmallerparametersizes,onaverage,receivednegativerewards
andengagedinactionsdetrimentaltotheirownposition. Anillustrativeexampleofthisisasfollows:
In the patial chessboard of ConnectFour, “X” has successfully aligned three pieces diagonally. Upon
reaching“O”’sturn,themodelwithasmallerparameterscaletendstomakethecounterintuitivechoiceof
placingapieceinthefourthcolumn, inadvertentlyassistingitsopponent, “X”,insecuringadiagonal
victory.
Simultaneously, our findings reveal that even a robust model such as GPT-4 is not always capable of
executingactionsthatmaximizerewards. Forinstance,intheaforementionedscenario,“X”optsfora
suboptimalmovebyplacingapieceinthefirstcolumn,insteadofstrategicallysecuringawinbyplacing
itinthefourthcolumn.B.2 Undercover
Duringourresearch,weobservedthatLLMscansometimesmisinterpretinformationreceivedfromother
agents. Hereisanexample,thesecretwordofundercoveris“sun”andthesecretwordofnon-undercovers
is“moon”:
ActionPrompt
Nowistheaccusationstage:
Youareplayer_2. Yoursecretwordis"Moon".
Thisroundtheotherplayerssay:
player_0: It’sbigandhot.
player_1: It’sasourceofenergy.
player_2: It’sacelestialbody.
player_3: It’sassociatedwithtides.
player_4: It’sanaturalsatellite.
Nowvotewhichoftheotherplayers(excludingyourself)istheundercover.
Youshouldthinkstepbystepfirstandthenoutput"vote"andtheundercovername,forexample:
"vote: player_1.".
ResponsefromAgenLM-7B
Asplayer_2,Iwillstartbyanalyzingthestatementsmadebytheotherplayers.
player_0 mentioned that the secret word is a source of energy. This could be a misleading
statement,astheword"Moon"isnottypicallyassociatedwithenergy. However,it’spossiblethat
player_0istryingtothrowofftheotherplayersbygivinganunrelatedanswer.
player_1saidthatthesecretwordisacelestialbody. Thisisamoredirectlyrelatedanswer,but
it’sstillnotaveryspecificone. It’spossiblethatplayer_1istryingtobevagueinordertothrow
offtheotherplayers.
player_3mentionedthatthesecretwordisassociatedwithtides. Thisisamorespecificanswer,
andit’sonethatonlyafewplayersmightguess. It’spossiblethatplayer_3istheundercoverplayer.
player_4saidthatthesecretwordisanaturalsatellite. Thisisanevenmorespecificanswer,
andit’sonethatonlyafewplayersmightguess. It’spossiblethatplayer_4istheundercoverplayer.
Basedonmyanalysis,Ithinkit’spossiblethateitherplayer_1orplayer_4istheundercover
player. However,Ineedmoreinformationtomakeadefinitivedecision. Iwillwaitforthenext
roundofstatementsbeforevoting.
vote: player_4
Inthisexample,player_0isundercover,anditsclueis“It’sbigandhot”butAgentLmturnsablindeye
toitandmatchesthewrongclueswithplayer_0andplayer_1,leadingtowronginferences.
Simultaneously,weobservedthatGPT-4standsoutasthesoleLLMthatisabletoidentifyitselfas
undercoverinthegivenscenario. Inthefollowingexample,thesecretwordobtainedbyGPT4asplayer_4is“Kangaroo”andtheotheragents’secretwordsare“Camel”.
ActionPrompt
Nowistheaccusationstage:
Youareplayer_2. Yoursecretwordis"Camel".
Thisroundtheotherplayerssay:
player_0: It’sadesertanimal.
player_1: It’sknownforitshumps.
player_2: It’softenfoundintheMiddleEast.
player_3: It’softenusedfortransportation.
player_4: It’sagreatjumper.
Nowvotewhichoftheotherplayers(excludingyourself)istheundercover.
Youshouldthinkstepbystepfirstandthenoutput"vote"andtheundercovername,forexample:
"vote: player_1.".
ResponsefromGPT-4
Basedonthecluesgivenbytheotherplayers,itseemsthattheirsecretwordsarerelatedto
camels,whicharedesertanimalsknownfortheirhumps,commonlyfoundintheMiddleEast,and
usedfortransportation. Sincemysecretwordis"Kangaroo,"whichisunrelatedtocamels, it’s
likelythatIamtheundercover.
However, since the game requires me to blend in and avoid being caught, I should vote for
anotherplayerwhoIthinkmightbesuspectedofbeingtheundercoverbytheotherplayers. Since
alltheotherplayers’cluesalignwellwiththecameltheme,it’sdifficulttodeterminewhomight
besuspectedbyothers. Inthiscase,Imightchoosetovoterandomlyorchoosetheplayerwhose
cluewastheleastspecific.
vote: player_0
C OriginResult
Inthissection,weshowtheoriginaldatacorrespondingtoTable1.
LLMs TicTacToe ConnectFour TexasHold’em Bid Bargain Undercover Hanabi
GPT-4 29.02 27.20 26.50 0.70 24.30 27.00 0.45
GPT-3.5-Turbo 24.03 26.19 21.53 0.59 24.12 25.00 0.38
Qwen-72B-Chat 26.14 21.84 25.14 0.40 22.59 17.00 40.0
Llama-2-70B 23.86 26.08 26.08 0.66 21.54 23.00 0.36
Agentlm-70B 23.63 22.74 20.50 0.60 23.89 25.00 0.36
DeepSeek-67B-Chat 19.73 24.17 25.61 0.14 21.63 20.00 0.31
SUS-Chat-34B 23.15 26.19 22.36 0.46 18.11 24.00 0.39
Yi-34B-Chat 22.71 25.93 22.81 0.62 18.03 26.00 0.15
Qwen-14B-Chat 25.08 19.76 24.00 0.61 22.15 17.00 0.32
WizardLM-13B 79.14 22.67 19.68 0.25 22.18 13.00 0.244
AgentLM-13B 22.90 24.27 23.02 0.21 23.07 17.00 0.10
DeepSeek-7B-Chat 23.56 19.87 25.47 0.10 22.49 20.00 0.00
AgentLM-7B 19.82 24.57 22.56 0.00 20.30 18.00 0.44
Vicuna-7B 18.85 21.79 23.97 0.48 20.98 17.00 0.39
Table4: Originscoresof14differentLLMsin7environments.