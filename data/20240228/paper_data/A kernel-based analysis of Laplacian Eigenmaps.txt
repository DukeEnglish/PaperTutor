A kernel-based analysis of Laplacian Eigenmaps
Martin Wahl∗
Abstract
Giveni.i.d.observationsuniformlydistributedonaclosedmanifold
Rp, we study the spectral properties of the associated empiri-
M ⊆
cal graph Laplacian based on a Gaussian kernel. Our main results
are non-asymptotic error bounds, showing that the eigenvalues and
eigenspacesof the empirical graphLaplacianare close to the eigenval-
ues and eigenspaces of the Laplace-Beltrami operator of . In our
M
analysis,we connect the empirical graphLaplacian to kernelprincipal
componentanalysis,andconsiderthe heatkernelof asreproducing
M
kernel feature map. This leads to novel points of view and allows to
leverage results for empirical covariance operators in infinite dimen-
sions.
Contents
1 Introduction 2
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Laplacian Eigenmaps and Diffusion Maps . . . . . . . . . . . 3
1.3 Laplace-Beltrami operator, heat semigroup and heat kernel . 5
1.4 Two main consequences . . . . . . . . . . . . . . . . . . . . . 6
1.5 Previous and new proof map . . . . . . . . . . . . . . . . . . 8
2 Heat kernel principal component analysis 9
2.1 The heat kernel as reproducing kernel . . . . . . . . . . . . . 10
2.2 Heat kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.3 Perturbation bounds for heat kernel PCA . . . . . . . . . . . 15
∗Universit¨at Bielefeld, Germany. E-mail: martin.wahl@math.uni-bielefeld.de
2010 Mathematics Subject Classification. 62H25, 60B20, 15A42, 47A55, 47D07, 35K08
Keywordsandphrases. empiricalgraphLaplacian,Laplace-Beltramioperator,heatsemi-
group, heat kernel, principal components analysis, empirical covariance operator, repro-
ducing kernelHilbert space, perturbation theory,concentration inequalities.
1
4202
beF
62
]TS.htam[
1v18461.2042:viXra3 The approximation error 22
3.1 Different graph Laplacians . . . . . . . . . . . . . . . . . . . . 22
3.2 Analysis of the approximation error . . . . . . . . . . . . . . 23
4 Additional proofs 27
4.1 Proof of Corollary 1 . . . . . . . . . . . . . . . . . . . . . . . 27
4.2 Proof of Corollary 2 . . . . . . . . . . . . . . . . . . . . . . . 29
A Appendix 33
A.1 Perturbation bounds . . . . . . . . . . . . . . . . . . . . . . . 33
A.2 Concentration inequalities . . . . . . . . . . . . . . . . . . . . 37
A.3 Eigenvalue estimates . . . . . . . . . . . . . . . . . . . . . . . 39
1 Introduction
1.1 Motivation
Laplacian Eigenmaps [3] and Diffusion Maps [15] are nonlinear dimen-
sionality reduction methods that use the eigenvalues and eigenvectors of
(un)normalized graph Laplacians. Both methods are applied when the
data is sampled from a low-dimensional manifold, embedded in a high-
dimensionalEuclideanspace. Fromamathematicalpointofview,thefunda-
mental problem is to understand these so-called empirical graph Laplacians
as spectral approximations of the underlying Laplace-Beltrami operator.
Spectral convergence properties of graph Laplacians have been studied
first by [4], based on pointwise convergence results [5, 19] and perturbation
theory for linear operators on Banach spaces [45, 11]. Since then, different
approaches have been proposed in order to study the spectral properties of
graph Laplacians (see e.g. [9, 40, 18, 10, 13] and the references therein). Be-
sides pointwise convergence results, an important role is played by Dirichlet
form convergence of the empirical graph Laplacian towards the Laplace-
Beltrami operator.
A different line of research studies principal component analysis (PCA)
in high dimensions [48, 29] or in infinite dimensions [23, 39], where the lat-
ter also includes functional and kernel PCA. Principal component analysis
is a standard dimensionality reduction method that uses the eigenvalues
and eigenspaces of the empirical covariance matrix or operator to find the
so-called principal components, capturing most of the variance of the (em-
pirical) distribution. Recently, there has been a lot of interest and activity
2in the(spectral) analysis of empirical covariance operators in infinitedimen-
sions (see, e.g., [42, 33, 32, 35, 28, 1] and the references therein). A related
problem studies approximations of integral operators [31, 37].
In this paper, we study empirical graph Laplacians through the lens of
kernel PCA. In particular, we connect the eigenvalue and eigenvector prob-
lem for the empirical graph Laplacian to a kernel PCAproblem, providing a
novel functional analytic framework. To achieve this, the semigroup frame-
workplaysanimportantrole. First, weinterprettheLaplace-Beltrami oper-
ator of as thegenerator of theheat semigroup of [16, 21]. Second, the
M M
heat semigroup is an integral operator on the space of all square-integrable
functionson ,withintegralkernelbeingtheheatkernelof . Restricting
M M
its domain to the reproducing kernel Hilbert space (RKHS) induced by the
heat kernel, the heat semigroup can be interpreted as the covariance opera-
tor of the random variable obtained by embedding the original observation
on the manifold using the heat kernel as reproducing kernel feature map.
This leads to novel points of view and allows us to use results for empirical
covariance operators in infinite dimensions. In particular, this paves the
way to apply Hilbert space perturbation theory in the analysis of empirical
graph Laplacians.
1.2 Laplacian Eigenmaps and Diffusion Maps
In this section, we introduce the Laplacian Eigenmaps and the Diffusion
Maps algorithm. Let X ,...,X be data points in Rp. Let , be the
1 n
h· ·i
standard inner product in Rp and let be the corresponding Euclidean
2
k·k
norm in Rp. Finally, let
1 x y 2
w (x,y) = exp k − k2 , x,y Rp, (1.1)
t (4πt)d/2 − 4t ∈
(cid:16) (cid:17)
be the Gaussian kernel with hyperparameters d N and t > 0. Then the
∈
(unnormalized) empirical graph Laplacian or point cloud Laplacian with
repsect to w is given by (see, e.g., [44])
t
1
L = (D W ) Rn×n
wt
t
wt
−
t
∈
with adjacency matrix W Rn×n defined by
t
∈
1
(W ) = w (X ,X )
t ij t i j
n
3and degree matrix D Rn×n being a diagonal matrix with
wt
∈
n
1
(D ) = w (X ,X ).
wt ii
n
t i j
j=1
X
Graph Laplacians have been intensively studied in spectral graph theory
(see, e.g., [14] or [44] and the references therein). A first key identity of the
unnormalized graph Laplacian is
n n
1
u,L u = w (X ,X )(u u )2, u Rn, (1.2)
h
wt
i 2n
t i j i
−
j
∈
i=1 j=1
XX
which can be seen as an empirical Dirichlet form (see, e.g., Proposition 1 in
[44]). In particular, L is symmetric and positive semi-definite. Let
wt
0 λ (L ) λ (L )
≤
1 wt
≤ ··· ≤
n wt
be the eigenvalues of L (in non-decreasing order), and let
wt
u (L ),...,u (L ) Rn
1 wt n wt
∈
be an orthonormal basis of corresponding eigenvectors. Since w (x,y) > 0
t
for all x,y Rn, it follows from Proposition 2 in [44] that 0 = λ (L ) <
∈
1 wt
λ (L ) and that u (L ) = n−1/2(1,...,1)⊤. The spectral characteristics
2 wt 1 wt
of L are often used in spectral clustering. Indeed, given a natural number
wt
1 j n, a reduced coordinate system is given by
≤ ≤
u (L )
1i wt
ϕ(j)(X i) =  . . .  Rj, 1 i n, (1.3)
∈ ≤ ≤
u (L )

ji wt

 
where u (L ) is the ith coordinate of the kth eigenvector. Often,
ki wt
one also introduces a weighting by multiplying the entries u (L ) by
ki wt
λ (L )m with m > 0. Moreover, often L is replaced by the ran-
k wt wt
dom walk graph Laplacian I D−1L , where I Rn×n is the iden-
n − wt wt n ∈
tity matrix, in which case (1.3) is also called (truncated) diffusion map and
ϕ(j)(X ) ϕ(j)(X ) is also called diffusion distance between data points
k
i1
−
i2 k2
X and X (see, e.g., [15] and [2]).
i1 i2
41.3 Laplace-Beltrami operator, heat semigroup and heat
kernel
Let be a submanifold of Rp of dimension d, equipped with the Rieman-
M
nian metric induced by the ambient space. We assume that is closed
M
(that is, compact and without boundary) and that the volume of is 1.
M
Let
H = ∆ (1.4)
W2(M)
− |
be the Laplace-Beltrami operator of (see Section 3.7 in [21], Chapter 5
M
in [16], or [38]), which we consider with domain being the Sobolev spaces
W2( ). The sign convention implies that H is a positive self-adjoint oper-
M
ator. Let
(e−tH) (1.5)
t>0
be the heat semigroup on L2( ) associated with H (see, e.g., Theorem
M
4.9 in [21] or Chapter 5 in [16]). For each t > 0, e−tH is a a positive self-
adjoint bounded operator on L2( ). Moreover, since is closed, e−tH is
M M
ultracontractive (see, e.g., Theorem 7.6 in [21]), implying the existence of
an integral kernel k satisfying
t
e−tHf(x)= k (x,y)f(y)dy
t
ZM
for all x , all t > 0 and all f L2( ), where dy denotes the volume
∈ M ∈ M
measureon (see,e.g.,Theorem5.2.1in[16]orTheorem7.13in[21]). The
M
kernel k is called heat kernel of . It is symmetric, bounded,non-negative
t
M
(infact,wehavek (x,y) > 0wheneverxandy belongtothesameconnected
t
componentof andk (x,y) = 0otherwise),andsatisfies k (x,y)dy = 1
M t M t
for all x . In particular, e−tH is additionally trace-class for every t > 0,
∈M R
implying that there exists an orthonormal basis φ ,φ ,... of L2( ) and
1 2
M
an non-decreasing sequence 0 = µ µ of non-negative real
1 2
≤ ≤ ··· → ∞
numbers such that (see, e.g., Lemma 7.2.1 in [16] or Theorem 10.13 in [21])
∞
e−tHφ = φ , e−µjt < . (1.6)
j j
∞
j=1
X
For each j N, φ is in the domain of H, implying that
j
∈
I e−tH
Hφ = lim − φ = µ φ ,
j j j j
t→0 t
5where I denotes the identity map. Moreover, the heat kernel admits the
expansion
∞
k (x,y) = e−µjtφ (x)φ (y) (1.7)
t j j
j=1
X
for all t > 0 and all x,y , which converges in L2( ) as well as
∈ M M×M
absolutely and uniformly in [a,b] for all real numbers b > a > 0.
×M×M
The multiplicity of the eigenvalue 0 is denoted by m, meaning that
0 = µ = = µ < µ .
1 m m+1
···
The number m corresponds to the number of connected components of .
M
Moreover, if ,..., arethe connected components of , then we can
1 m
M M M
choose φ = vol( )−1/21 as the first m eigenfunctions of H (see, e.g.,
j Mj Mj
Section 2.2 in [22]).
1.4 Two main consequences
The main goal of this paper is to study the behavior of the eigenvalues and
eigenvectors of L under the following manifold assumption.
wt
Assumption 1. Let X,X ,...,X be i.i.d. random variables that are uni-
1 n
formly distributed on , where is a closed submanifold of Rp with
M M
dim = d and vol(M) = 1 such that conditions (C1), (C3), (C4) and
M
(C5) from below hold.
In this section, we formulate two main consequences of the analysis pre-
sentedinSections 2and3. Wesaythatanevent holdswithhighprobability,
if it holds with probability at least 1 Cn−c, where c,C > 0 are constants
−
depending only on the manifold constants from (C1), (C3), (C4) and (C5).
The following result deals with eigenvalues.
Corollary 1. Grant Assumption 1. Then there are constants c,C > 0
depending only on the constants in (C1), (C3), (C4) and (C5) such that the
following holds. Let n 2 and let t (0,1] be such that logn c. Then
≥ ∈ ntd/2 ≤
logn
1 j n, λ (L ) µ C +µ tlog2(e)+(µ2 1)t .
∀ ≤ ≤ | j wt − j |≤ ntd/2+2 j t j ∨
r
(cid:16) (cid:17)
with high probability.
6For all j N let
∈
1
S φ = (φ (X ),...,φ (X ))⊤ Rn
n j j 1 j n
√n ∈
andletO(j)bethesetofallorthogonalj j matrices. Thenextresultdeals
×
with eigenspaces and provides an error bound for the reduced coordinate
system in (1.3).
Corollary 2. Grant Assumption 1. Then there are constants c,C > 0
depending only on the constants in (C1), (C3), (C4) and (C5) such that the
following holds. Let n 2, 1 j n and t (0,1] be such that µ > µ ,
j+1 j
≥ ≤ ≤ ∈
µ t 1 and logn c. Then
j+1 ≤ ntd/2 ≤
inf (S φ ,...,S φ ) (u (L ),...,u (L ))O
O∈O(j)k
n 1 n j
−
1 wt j wt k2
1 logn µ
C j + j+1 tlog2(e)+µ t .
≤ µ µ ntd/2+2 µ µ t j
p (cid:16)
j+1
−
jr j+1
−
j
(cid:17)
with high probability. Here, denotes the Hilbert-Schmidt norm.
2
k·k
The proofs of Corollaries 1 and 2 are based on classical (zero order)
perturbation bounds (i.e. the Weyl bound and the Davis-Kahan bound).
On the one hand, this leads to non-asymptotic perturbation bounds that
are valid under minimal assumptions and exhibit simple dependencies on
theinvolved quantities (d,j, µ ,spectralgap,etc). Forinstance, Corollary1
j
holdswithoutany spectralgapassumption. Ontheotherhand,thisleadsto
a stochastic error (i.e. the term involving n−1/2t−d/4−1 up to a logarithmic
term) that is suboptimal with respect to t.
Compared to recent results in the literature, our dependence on t is bet-
terthane.g.intheboundsobtainedin[18],butitisworsethaninthebounds
obtained in [13]. The latter provides a firstorder perturbation analysis, and
establishes the stochastic errors (up to logarithmic terms) of the asymptotic
order n−1/2t−d/4 and n−1/2t−d/4−1/2 for the eigenvalues and eigenfunctions,
respectively. However, this requires stronger assumptions, and in [13] only
a fixed number of leading eigenvalues (and eigenfunctions) are considered
under the (spectral gap) assumption that they all have multiplicity 1.
In a companion paper we will combine our approach (outlined in Sec-
tion 1.5) with higher order perturbation expansions obtained in [35, 28, 27].
We conjecture that this will lead to an improved stochastic error term of
the order n−1/2t−d/4 for both eigenvalues and eigenspaces (with an even
better rate in the case of eigenvalues), but also leads to more restrictive
7assumptions on d, j, µ , spectral gap, etc. In this paper, we formulate such
j
an improvement only for heat kernel PCA and for the leading eigenvalue
and the associated eigenspace (related to spectral clustering, see Theorem 3
below). This already requires more advanced techniques in the form of con-
centration inequalities (Lemmas 5 and 16), perturbation bounds (Lemmas
9 and 10), and eigenvalues computations (Lemma 17). Note that all these
techniques are not used in the proofs of Corollaries 1 and 2.
1.5 Previous and new proof map
AstandardrouteintheanalysisofthepointcloudLaplacianundertheman-
ifold assumption (see, e.g., [4, 19, 41, 18, 13]) is described by the following
error decomposition
I −e−tH on L2( ) (1) EL on L2( ) or ( )
t M
wt
M C M
(2)
L on ( )
wt
C M
Here, (1) corresponds to an approximation error and (2) corresponds to a
stochastic error. In the approximation error, the heat kernel k is replaced
t
by a Gaussian kernel w , leading to the operator
t
1
EL f(x)= (f(x) f(y))w (x,y)dy, f L2( ),x .
wt
t −
t
∈ M ∈ M
ZM
The approximation error tends to decrease when t decreases. It cannot be
controlled uniformly over L2( ), but only over proper classes of smooth
M
functions (see, e.g., Theorem 3.1 in [19], Theorem 5.1 in [13] and [41] for
pointwise error bounds and Theorem 3.2 in [13] and Section 3 in [18] for
Dirichlet form error bounds). In the stochastic error, the uniform distribu-
tion on is replaced by by the empirical distribution, leading to
M
n
11
L f(x)= (f(x) f(X ))w (x,X ), f ( ),x ,
wt
t n −
i t i
∈ C M ∈ M
i=1
X
where ( ) denotes the space of all continuous functions on . The
C M M
stochasticerrortendstoincreasewhennincreases. Again,forcertainproper
classes of functions, itcan becontrolled usingconcentration inequalities and
empirical process theory (see, e.g., [19] and [4]).
8The approach of this paper reverses the order of approximation error
and stochastic error and is described by the following error decomposition
I Σ I e−tH
t
− = − on
t
t t H
(1)
I −Σˆ t on (2) I n −K t (3) L (4) L
t
Ht
t
kt wt
Here, (1) corresponds to a stochastic error, (2) corresponds to the kernel
trick, (3) is another stochastic error and (4) is an approximation error.
The first main step will be to realize e−tH as the covariance operator Σ
t
of the embedded data point k (X, ) taking values in the reproducing kernel
t
·
Hilbert space (RKHS) induced by k (Section 2.2). The new stochastic
t t
H
error consists now in estimating Σ using the empirical covariance operator
t
Σˆ on based on the embedded data points k (X , ) , 1 i n.
t t i t
H · ∈ H ≤ ≤
This error can becontrolled in operator norm (which makes it easy to apply
perturbation bounds for eigenvalues and eigenspaces) using concentration
inequalities for random self-adjoint operators (Section 2.3). Together with
the kernel trick in (2), this allows to relate the spectral characteristics of
I−e−tH to those of In−Kt, where K Rn×n is the kernel matrix defined
t t t ∈
by (K ) = n−1k (X ,X ). Finally, the stochastic error in (3) and the
t ij t i j
approximation error in (4) can again be controlled in operator norm, us-
ing concentration inequalities and asymptotic expansions of the heat kernel
(Section 3.2).
Further basic notation. Given a bounded (resp. Hilbert-Schmidt) op-
erator A on a Hilbert space , we denote the operator norm (resp. the
H
Hilbert-Schmidt norm) of A by A (resp. A ). Given a trace class op-
∞ 2
k k k k
erator A on , we denote the trace of A by tr(A). For g,h , we denote
H ∈ H
by g h the rank-one operator on defined by (g h)x = h,x g, x .
⊗ H ⊗ h i ∈ H
Throughout the paper, c (0,1) and C > 1 denote constants that may
∈
change from line to line (by a numerical value). If no further dependencies
are mentioned, then these constants are absolute.
2 Heat kernel principal component analysis
Throughout Section 2, we suppose that Assumption 1 is satisfied.
92.1 The heat kernel as reproducing kernel
Therepresentation in (1.7) implies that k is a symmetricand positive semi-
t
definite kernel function in the sense of Definition 12.6 in [48]. Hence, k
t
induces the reproducing kernel Hilbert space (RKHS) (see, e.g., Corollary
12.26 in [48])
∞ ∞ ∞
= f = α φ : α2 < and eµjtα2 <
Ht j j j ∞ j ∞
n Xj=1 Xj=1 Xj=1 o
with the inner product
∞ ∞ ∞
f,g = eµjtα β , f = α φ ,g = β φ .
h
iHt j j j j j j
∈
Ht
j=1 j=1 j=1
X X X
In particular, an orthonormal basis of is given by
t
H
u = e−µjt/2φ , j N.
t,j j
∈
Alternatively, we have
= e−(t/2)HL2( )
t
H M
with
f,g = e(t/2)Hf,e(t/2)Hg
h
iHt
h
iL2(M)
for f,g . In particular, for all t > 0 and all x , we have
t
∈ H ∈ M
k (x, )
t t
· ∈ H
and the reproducing property
f(x)= f,k (x, )
h
t
·
iHt
holds. Setting f = k (y, ), we get
t
·
k (x,y) = k (y, ),k (x, ) (2.1)
t
h
t
·
t
·
iHt
for all t > 0 and all x,y . Hence, geometric computations for the
t
∈ H
functions k (x, ), x in the infinite-dimensional Hilbert space can
t t
· ∈ M H
be carried out using only evaluations of the kernel function.
102.2 Heat kernel PCA
Since k is a positive semi-definite kernel function with RKHS , we can
t t
H
consider the reproducing kernel feature map (see, e.g., [39, 42])
Φ : , x k (x, ).
t t t
M→ H 7→ ·
This map is similar to the (population) diffusion map (see, e.g., [15]) with
the difference that the range is instead of the larger space L2( ). This
t
H M
feature map yields the embedded random variables
k (X, ),k (X , ),...,k (X , ),
t t 1 t n
· · ·
taking values in the Hilbert space . The random variable k (X, ) has
t t
H ·
expectation 1 : R, x 1. Moreover, by (1.7), it has the Karhunen-
M
M→ 7→
Lo`eve (KL) representation
∞
k (X, ) = e−µjt/2φ (X)u (2.2)
t j t,j
·
j=1
X
with KL coefficients φ (X) satisfying Eφ (X)φ (X) = 0 for all j = k and
j j k
6
Eφ2(X) = 1 for all j N. By (2.2), we have
j ∈
∞
E k (X, ) 2 = e−µjt < ,
k
t
·
kHt
∞
j=1
X
meaning that k (X, ) is strongly square-integrable. Hence we can define the
t
·
(unnormalized) covariance operator
Σ = Ek (X, ) k (X, ),
t t t
· ⊗ ·
which isapositive self-adjoint trace class operator on (see, e.g., Theorem
t
H
7.2.5 in [23] or [27]). The following lemma collects some basic properties of
k (X, ).
t
·
Lemma 1. The random variable k (X, ) is strongly square-integrable with
t
·
ovariance operator Σ = Ek (X, ) k (X, ) satisfying
t t t
· ⊗ ·
∞
Σ = e−µjtu u = e−tH .
t t,j
⊗
t,j |Ht
j=1
X
In particular, Σ is a trace-class operator with
t
∞
tr(Σ ) = e−µjt.
t
j=1
X
11Proof. The spectral respresentation of Σ is a consequence of (2.2). For the
t
second equality it suffices to note that Σ tu
t,j
= e−µjtu
t,j
= e−tHu
t,j
for all
j N.
∈
Lemma 1 shows that the spectral characteristics of the heat semigroup
can be inferred from the covariance operator of k (X, ). In particular, the
t
·
eigenvalues of Σ are given by e−µ1t e−µ2t > 0 (we also often write
t
≥ ≥ ···
λ
t,j
= e−µjt) and corresponding eigenvectors are given by u t,1,u t,2,.... For
j N, let
∈
j
P = P , P =u u
t,≤j t,k t,k t,k t,k
⊗
k=1
X
be the orthogonal projection onto the eigenspace spanned by u ,...,u .
t,1 t,j
Let
n
1
Σˆ = k (X , ) k (X , )
t t i t i
n · ⊗ ·
i=1
X
be the empirical covariance operator, which is a positive self-adjoint finite-
rank operator on . In particular, there exists a sequence λˆ λˆ
t t,1 t,2
H ≥ ≥
0 of non-negative eigenvalues together with an orthonormal basis of
··· ≥
eigenvectors uˆ ,uˆ ,... in such that
t,1 t,2 t
H
∞
Σˆ = λˆ uˆ uˆ .
t t,k t,k t,k
⊗
k=1
X
For j N, let
∈
j
Pˆ = Pˆ , Pˆ =uˆ uˆ
t,≤j t,k t,k t,k t,k
⊗
k=1
X
be the orthogonal projection onto the eigenspace spanned by uˆ ,...,uˆ .
t,1 t,j
A related matrix is the kernel matrix K Rn×n defined by
t
∈
1 1
(K ) = k (X , ),k (X , ) = k (X ,X ).
t ij
nh
t i
·
t j
·
iHt
n
t i j
The so-called kernel trick states that Σˆ and K have the same non-zero
t t
eigenvalues and that the so called principal components (see, e.g., [30]) can
be computed from K . To formulate this, let
t
1
S : Rn, f (f(X ),...,f(X ))⊤
n t 1 n
H → 7→ √n
12be the (normalized) sampling operator. Then there exists an orthonormal
basis vˆ ,...,vˆ of Rn such that (see, e.g., [13])
t,1 t,n
n
K = λˆ vˆ vˆ⊤
n t,j t,j t,j
j=1
X
with principal components satisfying
λˆ1/2 vˆ = S uˆ Rn. (2.3)
t,j t,j n t,j ∈
The qualitative properties in Lemma 1 alone are not sufficient for a
statistical analysis of heat kernel PCA. For this we need some additional
quantitative properties of k . For better reference, we will formulate these
t
properties as conditions with explicit constants.
(C1) There exists a constant C > 0 such that
1
k (x,x) C t−d/2
t 1
≤
for all t (0,1] and all x .
∈ ∈ M
Condition (C1) is satisfied under Assumption 1, as follows from Theorem
7.9 in [21]. It has many consequences. For instance, (1.7) and (C1) yield
(after integrating over )
M
∞
je−µjt e−µkt C t−d/2
1
≤ ≤
k=1
X
and setting t = µ−1 with j > m leads to
j
µ c j2/d (2.4)
j 1
≥
forallj > m,withaconstantc > 0dependingonlyonC (seealsoExample
1 1
10.16 in [21] and Remark 3 in [12]).
(C2) There exists a constant C > 0 such that
2
j
φ2(x) C j
k ≤ 2
k=1
X
for all x and all j N.
∈ M ∈
13Condition (C2) says that the orthonormal basis φ ,φ ,... of L2( ) is
1 2
M
bounded on average. It holds under Assumption 1, as shown in [34]. It
is again closely related to (C1) and an upper bound for the eigenvalues. In-
deed, by Weyl’s formula there exists a constant C > 0 such that µ Cj2/d
j
≤
for all j N. Thus, (1.7) and (C1) yield for t = j−2/d that
∈
j ∞
e−C φ2(x) e−µktφ2(x) C t−d/2 = C j.
k ≤ k ≤ 1 1
k=1 k=1
X X
Finally, we state two simple consequences of (C1) and (C2).
Lemma 2. If (C1) holds, then
k (X, ) 2 C t−d/2 a.s.,
k
t
·
kHt
≤
1
where C is the constant in (C1).
1
Proof. By (2.1) we have
k (X, ) 2 = k (X, ),k (X, ) = k (X,X),
k
t
·
kHt
h
t
·
t
·
iHt t
Hence, the claim follows from inserting (C1).
Hence under (C1), k (X, ) is a bounded random variable, which is al-
t
·
ready sufficient to start the analysis of heat kernel PCA (see, e.g., [37, 7]).
The next lemma shows that k (X, ) has a more subtle probabilistic struc-
t
·
ture if additionally (C2) is fulfilled. This will allow us to apply more refined
concentration techniques.
Lemma 3. If (C1) and (C2) hold, then k (X, ) has the KL representation
t
·
∞
k (X, ) = e−µjt/2φ (X)u
t j t,j
·
j=1
X
such that for all j > m it holds that
j
µ cj2/d and φ2(X) Cj almost surely.
j ≥ k ≤
k=1
X
Proof. Follows from (2.2), (2.4) and (C2).
142.3 Perturbation bounds for heat kernel PCA
Inthissection,wederiveperturbationboundsfortheeigenvaluesλˆ andthe
t,j
spectral projectors Pˆ of the empirical covariance operator Σˆ , considered
t,≤j t
as spectral approximations of the eigenvalues λ
t,j
= e−µjt and the spectral
projectors P of the covariance operator Σ . We apply classical absolute
t,≤j t
perturbationbounds,basedon theperturbationboundsbyWeyl andDavis-
Kahan, and we will also show how to obtain more refined bounds, by using
relative perturbation bound from [35, 28, 27]. In the latter case we restrict
ourselves to the nullspace.
Lemma 4. Suppose that (C1) is satisfied. Then, with probability at least
1 Ct−d/2e−y, y > 0, we have
−
y y
Σˆ Σ C +
t t ∞
k − k ≤ ntd/2 ntd/2
r
(cid:16) (cid:17)
with constant C > 0 depending only on C .
1
Proof. Let ξ = k (X, ) k (X, ) Σ and ξ = k (X , ) k (X , ) Σ for
t t t i t i t i t
· ⊗ · − · ⊗ · −
1 i n. Then we have
≤ ≤
n
1
Σˆ Σ = ξ ,
t t i
− n
i=1
X
andourgoalistoapplytheoperatorBernsteininequalitystatedinAppendix
A.2. First,
ξ k (X, ) k (X, ) +E k (X, ) k (X, )
∞ t t ∞ t t ∞
k k ≤k · ⊗ · k k · ⊗ · k
= k (X, ) 2 +E k (X, ) 2
k
t
·
kHt
k
t
·
kHt
=k (X,X)+Ek (X,X)
t t
2C t−d/2
1
≤
almost surely,wherewesuccessively appliedthetriangleinequality, Jensen’s
inequality, the reproducing property in (2.1), and (C1). Similarly, we have
Eξ2 E(k (X, ) k (X, ))2
∞ t t ∞
k k ≤ k · ⊗ · k
= Ek (X,X)(k (X, ) k (X, ))
t t t ∞
k · ⊗ · k
C t−d/2 Σ C t−d/2
1 t ∞ 1
≤ k k ≤
and
tr(Eξ2)= tr(Ek (X,X)(k (X, ) k (X, )))
t t t
· ⊗ ·
15C t−d/2Etr(k (X, ) k (X, ))
1 t t
≤ · ⊗ ·
= C−d/2Ek (X,X) C2t−d.
1 t ≤ 1
An application of Lemma 15 yields
1 n nu2
P ξ u Ct−d/2exp c
n i ∞ ≥ ≤ − t−d/2+t−d/2u
(cid:16)(cid:13) Xi=1 (cid:13) (cid:17) (cid:16) (cid:17)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Ct−d/2exp( cntd/2min(u,u2))
≤ −
for all u > 0. Setting
y y
u= max , ,
ntd/2 ntd/2
r
(cid:16) (cid:17)
the claim follows.
Theorem 1. Suppose that (C1)is satisfied. Then there are constants c,C >
0 depending only on C such that the following holds. Suppose that logn c.
1 ntd/2 ≤
Then, with probability at least 1 Cn−c, we have
−
j N,
1 −λˆ
t,j
1 −e−µjt
C
logn
∀ ∈ t − t ≤ ntd/2+2
r
(cid:12) (cid:12)
(cid:12) (cid:12)
and
(cid:12) (cid:12)
1 λˆ logn
j N, − t,j µ C +µ2t
∀ ∈ t − j ≤ ntd/2+2 j
r
(cid:12) (cid:12)
(cid:12) (cid:12)
Proof. The first claim follows from Lemma 4 and Lemma 8. The second
(cid:12) (cid:12)
claim follows from the inequalities x2/2 1 e−x x 0, x 0.
− ≤ − − ≤ ≥
Theorem 2. Suppose that (C1)is satisfied. Then there are constants c,C >
0depending only onC suchthat the following holds. Letj N andt (0,1]
1
be such that µ > µ , µ t 1 and logn c. Then, w∈ ith probab∈ ility at
j+1 j j+1 ≤ ntd/2 ≤
least 1 Cn−c, we have
−
√j logn
Pˆ P C .
t,≤j t,≤j 2
k − k ≤ µ µ ntd/2+2
j+1
−
jr
Proof. Combining Lemma 4 with the Davis-Kahan bound in Lemma 8, we
get
√j logn
Pˆ P C
k t,≤j − t,≤j k2 ≤ e−µjt e−µj+1t ntd/2
− r
16with probability at least 1 Cn−c. Inserting
−
1 eµj+1t e
= ,
e−µjt e−µj+1t e(µj+1−µj)t 1 ≤ (µ
j+1
µ j)t
− − −
where we also used that µ t 1 and µ > µ , the claim follows.
j+1 j+1 j
≤
To obtain more refined perturbation bounds for the nullspace, we intro-
duce the relative complexity measure (see, e.g., Appendix A.1.1)
δ (Σˆ Σ )
≤m t t
−
P 1 P 1
= t,≤m +R 2 (Σˆ Σ ) t,≤m +R 2
1 e−µm+1t t,>m t − t 1 e−µm+1t t,>m ∞
(cid:13)(cid:16) − (cid:17) (cid:16) − (cid:17) (cid:13)
(cid:13) (cid:13)
with red(cid:13)uced outer resolvent (cid:13)
P
t,k
R = .
t,>m 1 e−µkt
k>m −
X
The following result extends Lemma 4.
Lemma5. Suppose that(C1)and(C2)aresatisfied. Letd 3andt (0,1]
≥ ∈
be such that µ t 1. Then, with probability at least 1 Ct−d/2+1e−y,
m+1
≤ −
y > 0, we have
y y
δ (Σˆ Σ ) C +
≤m t t ∞
k − k ≤ ntd/2+1 ntd/2
r
(cid:16) (cid:17)
with constant C > 0 depending only on C , C and m.
1 2
Proof. For 1 i n, set
≤ ≤
P 1 P 1
t,≤m 2 t,≤m 2
Z = +R X, Z = +R X
1 e−µm+1t t,>m i 1 e−µm+1t t,>m i
(cid:16) − (cid:17) (cid:16) − (cid:17)
and
ξ = Z Z E(Z Z), ξ = Z Z E(Z Z ).
i i i i i
⊗ − ⊗ ⊗ − ⊗
Then we have
n
1
δ (Σˆ Σ ) = ξ
≤m t t ∞ i
k − k n ∞
(cid:13) Xi=1 (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
17and our goal is to again apply the operator Bernstein inequality stated in
Appendix A.2. We start with bounding Z . By the KL representation
k
kHt
in (2.2), we have
1 m e−µkt
Z 2 = φ2(X)+ φ2(X). (2.5)
k kHt 1 e−µm+1t k 1 e−µkt k
− k=1 k>m −
X X
On the one hand, we have
1 m eµm+1t m
φ2(X) φ2(X)
1 e−µm+1t k ≤ eµm+1t 1 k
− k=1 − k=1
X X
eC m
2 t−1 Ct−d/2 (2.6)
≤ µ ≤
m+1
with a constant C > 0 depending only on C , C and m, and where we
1 2
applied µ t 1, (C2), (2.4) and d 3. To boundthe second term on the
m+1
≤ ≥
right-hand side of (2.5), introduce
e−µkt k
a = and B = φ2(X)
k 1 e−µkt k l
− l=1
X
for k > m. Then a ,a ,... is a non-increasing sequence with
m+1 m+2
a Ct−d/2
k
≤
k>m
X
by Lemma 17, and the B satisfy B C k for all k > m by (C2). Hence,
k k 2
≤
by partial summation, we get
M M−1
a (B B ) = a B a B + (a a )B
k k k−1 M M m+1 m k k+1 k
− − −
k=m+1 k=m+1
X X
M−1 M
Ca M +C (a a )k C a
M k k+1 k
≤ − ≤
k=m+1 k=m+1
X X
for all M > m. Letting M go to infinity, we get
e−µkt
φ2(X) = a (B B ) C a Ct−d/2. (2.7)
1 e−µkt k k k − k−1 ≤ k ≤
k>m − k>m k>m
X X X
Inserting (2.6) and (2.7) into (2.5), we arrive at
Z 2 Ct−d/2,
k
kHt
≤
18with a constant C > 0 dependingonly on C , C and m. Next, let us bound
1 2
E(Z Z) . By construction, we have
∞
k ⊗ k
P 1/2 P 1/2
E(Z Z)= t,≤m +R Σ t,≤m +R ,
⊗ 1 e−µm+1t t,>m t 1 e−µm+1t t,>m
(cid:16) − (cid:17) (cid:16) − (cid:17)
so that
1 eµm+1t e C
E(Z Z) = = ,
k ⊗ k∞ 1 e−µm+1t eµm+1t 1 ≤ µ m+1t ≤ t
− −
where we applied the spectral decomposition in Lemma 1, the condition
µ t 1 and (2.4).
m+1
≤
Equipped with the bounds for Z and E(Z Z) , we can now
k
kHt
k ⊗
k∞
proceed as in Lemma 2. In particular, we have
ξ Z 2 +E Z 2 Ct−d/2,
k
k∞
≤ k
kHt
k
kHt
≤
Eξ2 E Z (Z Z) Ct−d/2−1, (2.8)
k
k∞
≤ k k
kHt
⊗
k∞
≤
and
tr(Eξ2) = tr( Z (Z Z)) Ct−d/2E Z 2 C2t−d.
k
kHt
⊗ ≤ k
kHt
≤
An application of Lemma 15 yields
1 n nu2
P ξ u Ct−d/2+1exp c
n i ∞ ≥ ≤ − t−d/2−1+t−d/2u
(cid:16)(cid:13) Xi=1 (cid:13) (cid:17) (cid:16) (cid:17)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Ct−d/2+1exp( cnmin(td/2u,u2td/2+1))
≤ −
for all u > 0. Setting
y y
u= max , ,
ntd/2+1 ntd/2
r
(cid:16) (cid:17)
the claim follows.
Theorem 3. Suppose that (C1) and (C2) are satisfied. Let n 2, d 5
≥ ≥
and t (0,1] be such that µ t 1 and logn c. Then, with probability
∈ m+1 ≤ ntd/2+1 ≤
at least 1 Cn−c, we have
−
1 λˆ logn logn
t,j
− C +C
t ≤ nt2 ntd/2+1
r
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
19for all 1 j m and
≤ ≤
logn logn
Pˆ P C +C ,
t,≤m t,≤m 2
k − k ≤ ntd/2 ntd/2+1
r
with constants c,C > 0 depending only on C , C and m. In particular, if
1 2
additionally logn c holds, then, with probability at least 1 Cn−c, we
ntd/2+2 ≤ −
have
1 λˆ logn
t,j
− C ,
t ≤ ntd/2
r
(cid:12) (cid:12)
(cid:12) (cid:12)
for all 1 j m and (cid:12) (cid:12)
≤ ≤
logn
Pˆ P C .
k t,≤m − t,≤m k2 ≤ ntd/2
r
Extensionstoothereigenspaces canbeobtainedanalogously, butrequire
extensions of the perturbation bounds from [47] to the case of eigenvalues
with multiplicities.
Proof. By Lemma 5 we have
logn logn
δ (Σˆ Σ ) C + 1/4 (2.9)
≤m t t
− ≤ ntd/2+1 ntd/2 ≤
r
(cid:16) (cid:17)
with probability at least 1 Cn−c, provided that the constant c in the
−
assumptions of Theorem 3 is chosen suffficiently small. On this event,
Lemma 10 yields
1 λˆ 1
− t,j P (Σˆ Σ )P +µ δ (Σˆ Σ )2
t,≤m t t t,≤m 2 m+1 ≤m t t
t ≤ tk − k · −
(cid:12) (cid:12)
(cid:12) (cid:12)
for all 1 j m and
(cid:12) (cid:12)
≤ ≤
Pˆ P √2 P (Σˆ Σ )R +C δ (Σˆ Σ )2,
t,≤m t,≤m 2 t,≤m t t t,>m 2 ≤m t t
k − k ≤ k − k · −
where we also used that µ = 0 and that t−1(1 e−µm+1t) µ in the
1 m+1
− ≤
first inequality. We have
m m n
1 2
P (Σˆ Σ )P 2 = φ (X )φ (X ) δ
k t,≤m t − t t,≤m k2 n j i k i − jk
Xj=1 Xk=1(cid:16) Xi=1 (cid:17)
20and
m ∞ e−µkt 1 n 2
P (Σˆ Σ )R 2 = φ (X )φ (X ) ,
k t,≤m t − t t,>m k2 (1 e−µkt)2 n j i k i
Xj=1k= Xm+1 − (cid:16) Xi=1 (cid:17)
as can be seen from inserting the KL representation in (2.2). The sequence
e−µkt/(1 e−µkt)2 is non-increasing and satisfies
−
e−µkt ke−µkt
Ct−d/2 and Ct−d,
(1 e−µkt)2 ≤ (1 e−µkt)2 ≤
k>m − k>m −
X X
as follows from Lemma 17, (C1), (2.4), and the fact that d 5. Moreover,
≥
by (C2), we have
m N
N N, φ2(X)φ2(X) C2mN
∀ ∈ j k ≤ 2
(cid:12)Xj=1 Xk=1 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Hence, by Lemma 16, we obtain with probability at least 1 Cn−c,
−
m m n
1 2 logn
φ (X )φ (X ) δ C
j i k i jk
n − ≤ n
Xj=1 Xk=1(cid:16) Xi=1 (cid:17)
and
m ∞ e−µkt 1 n 2 logn log2n
φ (X )φ (X ) C +
(1 e−µkt)2 n j i k i ≤ ntd/2 n2td
Xj=1k= Xm+1 − (cid:16) Xi=1 (cid:17) (cid:16) (cid:17)
logn
C .
≤ ntd/2
We conclude that, with probability at least 1 Cn−c,
−
1 λˆ logn logn log2n
t,j
− C +C +C
t ≤ nt2 ntd/2+1 n2td
r
(cid:12) (cid:12)
(cid:12) (cid:12) logn logn
(cid:12) (cid:12) C +C ,
≤ nt2 ntd/2+1
r
for all 1 j m and
≤ ≤
logn logn
Pˆ P C +C .
k t,≤m − t,≤m k2 ≤ ntd/2 ntd/2+1
r
This completes the proof.
213 The approximation error
3.1 Different graph Laplacians
Recall that is a closed submanifold of Rp with dim = d and vol(M) =
M M
1, equipped with the Riemannian metric induced by the ambient space. Let
d be the intrinsic distance on (see, e.g., Definition 4.21 in [36]), where
M
M
we additionally set d (x,y) = + whenever x and y belong to different
M
∞
components of . Moreover, let x y be the Euclidean (extrinsic)
2
M k − k
distance between x,y . We consider the heat kernel k , the geodesic
t
∈ M
kernel g and the Gaussian kernel w , where the latter two are defined by
t t
1 d (x,y)2
M
g (x,y) = exp ,
t
(4πt)d/2 − 4t
(cid:16) (cid:17)
1 x y 2
w (x,y) = exp k − k2 ,
t
(4πt)d/2 − 4t
(cid:16) (cid:17)
for x,y . Based on these kernels, we introduce the following un-
∈ M
normalized graph Laplacians
D K
L = kt − t Rn×n,
kt
t ∈
D G
L = gt − t Rn×n,
gt
t ∈
D W
L = wt − t Rn×n,
wt
t ∈
where
1
(K ) = k (X ,X ),
t ij t i j
n
1
(G ) = g (X ,X ),
t ij t i j
n
1
(W ) = w (X ,X ),
t ij t i j
n
and the degree matrices D , D and D are diagonal matrices with
kt gt wt
n
1
(D ) = k (X ,X ),
kt ii
n
t i j
j=1
X
n
1
(D ) = g (X ,X ),
gt ii
n
t i j
j=1
X
22n
1
(D ) = w (X ,X ).
wt ii
n
t i j
j=1
X
3.2 Analysis of the approximation error
In order to relate the different graph Laplacians, we need relationships be-
tweentheextrinsicandtheintrinsicdistanceandbetweenthegeodesickernel
andtheheatkernel. Again, forbetterreference, weformulatetheserelation-
ships as conditions with explicit constants and refer to the literature where
these properties are proven.
(C3) (Extrinsic and intrinsic distance). Suppose that there are constants
c (0,1) and C > 1 such that
3 3
∈
0 d2 (x,y) x y 2 C d4 (x,y)
≤ M −k − k2 ≤ 3 M
for all x,y , d (x,y) < c and
M 3
∈ M
inf x y 2 c d2 (x,y)
x,y∈M,dM(x,y)>sk − k2 ≥ 3 M
for all 0 < s <c .
3
The first condition is standard in the manifold learning literature (see, e.g.,
[19, 4, 13]), and proven by the exponential map and Taylor expansions.
The second condition measures the bottleneckness of and the distance
M
between the different connected components of .
M
Lemma 6. Suppose that (C3) is satisfied. Let K N. Then there exist
∈
constants C > 0 and c (0,1) depending only on c ,C , d and K such that
3 3
∈
w (x,y) g (x,y) Cg (x,y)tlog2(e)+CtK
| t − t | ≤ t t
for all x,y and all t (0,c).
∈ M ∈
Proof. Let t (0,1] be such that
∈
4(K +d/2)
δ(t) := tlog(e) < c .
s c 3 t 3
Let x,y be arbitrary. If d (x,y) > δ(t), then (C3) yields x y 2 >
∈ M M k − k2
c δ(t)2 and thus
3
1 δ(t)2
0 w (x,y) g (x,y) w (x,y) exp c
t t t 3
≤ − ≤ ≤ (4πt)d/2 − 4t
(cid:16) (cid:17)
23Ct−d/2tK+d/2 = CtK. (3.1)
≤
On the other hand, if d (x,y) δ(t), then (C3) yields
M
≤
d4 (x,y)
0 w (x,y) g (x,y) g (x,y)exp C M g (x,y)
t t t 3 t
≤ − ≤ 4t −
g (x,y)(exp(cid:16) (Ctlog2(e)) (cid:17) 1)
≤ t t −
Cg (x,y)tlog2(e). (3.2)
≤ t t
The claim follows from combining (3.1) and (3.2).
(C4) (First order asymptotic expansion of the heat kernel). There are
smooth functions u and r on and constants c (0,1) and
t 4
M×M ∈
C > 0 such that
4
k (x,y) = g (x,y)(1+u(x,y)+r (x,y))
t t t
with
u(x,y) C d2 (x,y) and r (x,y) C t
4 M t 4
| | ≤ | | ≤
for all x,y with d (x,y) < c and all t (0,1].
M 4
∈ M ∈
If x and y belong to different components, then k (x,y) = g (x,y) = 0 and
t t
the above condition is trivially satisfied with u(x,y) = r (x,y) = 0. If x and
t
y belong to the same connected component, then (C4) is a consequence of
the parametrix construction. See, e.g., Equation (45) in [12] (which is an
improvement over Exercise 5 in [38]) and also [19] for an upper bound on u.
(C5) (Refined global heat kernel). There are constants c (0,1/4] and
5
∈
C > 0 such that
5
C d2 (x,y)
k (x,y) 5 exp c M
t 5
≤ td/2 − t
(cid:16) (cid:17)
for all x,y and all t (0,1].
∈ M ∈
Lemma 7. Suppose that (C4) and (C5) are satisfied. Let K N. Then
∈
there exist constants C > 0 and c (0,1) depending only on c ,c ,C ,C ,
4 5 4 5
∈
d and K such that
k (x,y) g (x,y) Ck (x,y)tlog(e)+CtK
| t − t | ≤ t t
for all x,y and all t (0,c).
∈ M ∈
24Proof. Let t (0,1] be such that
∈
(K +d/2)
δ(t) := tlog(e) < c .
s c 5 t 4
Let x,y be arbitrary. If d (x,y) > δ(t), then (C5) yields
M
∈ M
k (x,y) C t−d/2e−c5δ2(t)/t Ct−d/2tK+d/2 = CtK (3.3)
t 5
≤ ≤
and, since c < 1/4,
5
g (x,y) (4πt)−d/2e−δ2(t)/(4t) Ct−d/2tK+d/2 = CtK. (3.4)
t
≤ ≤
On the other hand, if d (x,y) δ(t), then (C4) yields
M
≤
k (x,y) = (1+u(x,y)+r (x,y))
t t
g (x,y)(1+C δ2(t)+C t)
t 4 4
≤
g (x,y)(1+Ctlog(e))
≤ t t
and similarly
k (x,y) g (x,y)(1 Ctlog(e)).
t ≥ t − t
Hence, if additionally Ctlog(e) 1/2 holds, then
t ≤
1 1
k (x,y) g (x,y) k (x,y)
1+Ctlog(e) t ≤ t ≤ 1 Ctlog(e) t
t − t
and, by elementary inequalities,
(1 Ctlog(e))k (x,y) g (x,y) (1+2Ctlog(e))k (x,y). (3.5)
− t t ≤ t ≤ t t
The claim follows from (3.3), (3.4) and (3.5).
Corollary 3. Suppose that (C3), (C4) and (C5) are satisfied. Let K
∈
N. Then there exist constants C > 0 and c (0,1) depending only on
∈
c ,c ,c ,C ,C ,C , d and K such that
3 4 5 3 4 5
k (x,y) w (x,y) Ck (x,y)tlog2(e)+CtK
| t − t | ≤ t t
for all x,y and all t (0,c).
∈ M ∈
Proof. Follows from Lemmas 6 and 7.
25We now state the main result of this section.
Theorem 4. Suppose that (C3), (C4) and (C5) are satisfied. Let K
∈
N. Then there exist constants C > 0 and c (0,1) depending only on
∈
c ,c ,c ,C ,C ,C , d and K such that
3 4 5 3 4 5
u,L u u,L u Ctlog2(e)( u,L u +tK u 2)
|h kt i−h wt i| ≤ t h kt i k k2
for all u Rn and all t (0,c).
∈ ∈
Proof. Let c,C > 0 be constants such that the conclusion of Corollary 3
holds with K +2. Let t (0,c) and let u Rn. Then
∈ ∈
u,(D K )u u,(D W )u
|h
kt
−
t
i−h
wt
−
t
i|
n n n n
1 1
= k (X ,X )(u u )2 w (X ,X )(u u )2
t i j i j t i j i j
2n − − 2n −
(cid:12) Xi=1 Xj=1 Xi=1 Xj=1 (cid:12)
(cid:12) n n (cid:12)
(cid:12)1 (cid:12)
k (X ,X ) w (X ,X )(u u )2
t i j t i j i j
≤ 2n | − | −
i=1 j=1
XX
n n n n
1 1
Ck (X ,X )tlog2(e)(u u )2+ CtK+2(u u )2,
≤ 2n t i j t i − j 2n i − j
i=1 j=1 i=1 j=1
XX XX
where we applied (1.2) (to both kernels w and k ) and Corollary 3. Hence,
t t
we arrive at
u,(D K )u u,(D W )u
|h
kt
−
t
i−h
wt
−
t
i|
tK+2 n n
Ctlog2(e) u,(D K )u +C (u2+u2)
≤ t h kt − t i n i j
i=1 j=1
XX
= Ctlog2(e) u,(D K )u +2CtK+2 u 2.
t h kt − t i k k2
Dividing through t, the claim follows.
Corollary 4. Under the assumptions of Theorem 4, we have
(L +tKI )−1/2(L L )(L +tKI )−1/2 Ctlog2(e)
k kt n wt − kt kt n k∞ ≤ t
for all t (0,c).
∈
264 Additional proofs
4.1 Proof of Corollary 1
We proceed as in theproofmapgiven in Section 1.5. Theclaim follows from
the following threesteps and the triangle inequality. Recall that wesay that
an event holds with high probability, if it holds with probability at least
1 Cn−c, where c,C > 0 are constants depending only on the constants
−
from (C1), (C3), (C4) and (C5). In the following, we often make use of the
factthatiftwo events holdwithhighprobability, theirunionalso holdswith
high probability.
Reduction to heat kernel PCA. We have
I K logn
1 j n, λ − n µ C +µ2t
∀ ≤ ≤ j t − j ≤ ntd/2+2 j
r
(cid:12) (cid:16) (cid:17) (cid:12) (cid:16) (cid:17)
(cid:12) (cid:12)
with high probability. (cid:12) (cid:12)
Proof. By Theorem 1, we have
1 λˆ logn µ2t
j N, − t,j µ C + j
j
∀ ∈ t − ≤ ntd/2+2 2
r
(cid:12) (cid:12)
(cid:12) (cid:12)
with high probability. Hen(cid:12)ce, the claim(cid:12)follow from the fact that the first n
eigenvalues of Σˆ coincide with those of K .
t t
Reduction to the heat graph Laplacian. We have
I K logn
1 j n, λ n − t λ (L ) C (4.1)
∀ ≤ ≤ j t − j kt ≤ ntd/2+2
r
(cid:12) (cid:16) (cid:17) (cid:12)
(cid:12) (cid:12)
with high probability. (cid:12) (cid:12)
Proof. By Weyl’s bound, we have
I K 1
λ n − t λ (L ) I D , (4.2)
j
t −
j kt
≤ tk
n
−
ktk∞
(cid:12) (cid:16) (cid:17) (cid:12)
(cid:12) (cid:12)
with diagonal matrix I D such that
(cid:12) n
−
kt (cid:12)
n
1
I D = max k (X ,X ) 1 .
k
n
−
ktk∞
1≤i≤n n
t i j
−
(cid:12) Xj=1 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
27Fix 1 i n. Then
≤ ≤
n n
1 k (X ,X ) 1 1
t i i
(k (X ,X ) 1) | − | + (k (X ,X ) 1) .
t i j t i j
n − ≤ n n −
(cid:12) Xj=1 (cid:12) (cid:12) j= X1,j6=i (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
From (C1) it follows that
k (X ,X ) C t−d/2
t i i 1
≤
almost surely and thus
k (X ,X ) 1 C t−d/2+1 1
t i i 1
| − | C .
n ≤ n ≤ ntd/2
Moreover for all j = i, we have
6
k (X ,X ) k (X ,X ) k (X ,X ) C t−d/2
t i j t i i t j j 1
≤ · ≤
q
almost surely and
E (k (X ,X ) 1)2 E k (X ,X )2 C t−d/2E k (X ,X ) = C t−d/2,
Xj t i j
− ≤
Xj t i j
≤
1 Xj t i j 1
where E denotes expectation with respect to X only. Hence, an applica-
Xj j
tion of Bernstein’s inequality (conditional on X ) yields
i
n
1
P (k (X ,X ) 1) u 2exp(cntd/2min(u,u2))
t i j
n − ≥ ≤
(cid:16)(cid:12) j= X1,j6=i (cid:12) (cid:17)
(cid:12) (cid:12)
(cid:12) (cid:12)
for all u > 0. Setting
logn logn logn
u = C max , C ,
· ntd/2 ntd/2 ≤ ntd/2
r r
(cid:16) (cid:17)
with C sufficiently large, we obtain
logn
I D C (4.3)
k
n
−
ktk∞
≤ ntd/2
r
with high probability, wherewealso applied theunion bound. Insertingthis
into (4.2), the claim follows.
28Reduction to the Gaussian graph Laplacian. We have
logn
1 j n, λ (L ) λ (L ) C +(µ2 1)tlog2(e) .
∀ ≤ ≤ | j kt − j wt | ≤ ntd/2+2 j ∨ t
r
(cid:16) (cid:17)
with high probability.
Proof. For K N let
∈
L˜ = L +tKI , L˜ = L +tKI .
kt kt n wt wt n
Then L˜ and L˜ are strictly positive and Corollary 4 yields
kt wt
L˜−1/2 (L˜ L˜ )L˜−1/2 Ctlog2(e)
k kt wt − kt kt k∞ ≤ t
for all t (0,c). An application of Lemma 14 yields
∈
λ (L ) λ (L ) = λ (L˜ ) λ (L˜ ) Cλ (L˜ )tlog2(e)
| j kt − j wt | | j kt − j wt |≤ j kt t
for all t (0,c) and all 1 j n. By the previous two steps this implies
∈ ≤ ≤
that
logn
λ (L ) λ (L ) C µ +µ2t+tK + tlog2(e)
| j kt − j wt | ≤ j j ntd/2+2 t
r
(cid:16) (cid:17)
logn
C µ tlog2(e)+(µ2 1)t+
≤ j t j ∨ ntd/2+2
r
(cid:16) (cid:17)
for all t (0,c) and all 1 j n.
∈ ≤ ≤
4.2 Proof of Corollary 2
We proceed as in the proof map given in Section 1.5. In particular, the
claim follows from the following steps and the triangle inequality.
Preliminaries. Recall that we write λ
t,k
= e−µkt for all k N. For all
∈
k N, we have
∈
S φ 2 = φ ,Σˆ φ λˆ φ 2
k n k k2 h k t k iHt ≤ t,k ·k k kHt
(1+ Σˆ
t
Σ
t
∞) eµkt,
≤ k − k ·
where we used Weyl’s bound and the fact that λ 1 for all k N in the
t,k
≤ ∈
last inequality. Hence, by Lemma 4 and the assumptions in Corollary 2, we
have for all 1 k j,
≤ ≤
logn
S φ 2 1+C eµkt Ceµkt Ce
k n k k2 ≤ ntd/2 ≤ ≤
r
(cid:16) (cid:17)
29with high probability. Consequently, we may assume in the following that
logn
+µ tlog2(e) c(µ µ ), (4.4)
ntd/2+2 j+1 t ≤ j+1 − j
r
for some sufficiently small constant c> 0 (to be determined in the following
steps), since otherwise the claim follows from the fact that the inequality
inf (S φ ,...,S φ ) (u (L ),...,u (L ))O
O∈O(j)k
n 1 n j
−
1 wt j wt k2
(S φ ,...,S φ ) + (u (L ),...,u (L ))
≤ k
n 1 n j k2
k
1 wt j wt k2
j
1/2
= S φ 2 + j C j
k n k k2 ≤
(cid:16)Xk=1 (cid:17) p p
holds with high probability. Let us also collect some perliminary eigenvalue
bounds. First, with high probability, we have
logn
λˆ λ Σˆ Σ e−µjt C
t,j t,j t t ∞
≥ −k − k ≥ − ntd/2
r
logn 1
e−1 C , (4.5)
≥ − ntd/2 ≥ 2e
r
provided that c> 0 from the assumptions is chosen small enough (such that
c 1/(2Ce)), meaning that λˆ λˆ > 0 with high probability.
t,1 t,j
≤ ≥ ··· ≥
Moreover, with high probability, we have
logn
1 λˆ 1 e−µkt+ Σˆ Σ 1 e−µkt+C c< 1
t,k t t ∞
| − | ≤ − k − k ≤ − ntd/2 ≤
r
for all 1 k j. Hence, with high probability, we have
≤ ≤
−1/2
λ 1 C 1 λ Ctµ
| t,k − | ≤ | − t,k | ≤ k
logn
λˆ−1/2 1 C 1 λˆ C tµ + (4.6)
| t,k − | ≤ | − t,k | ≤ k ntd/2
r
(cid:16) (cid:17)
for all 1 k j, where we also used that (1 x)−1/2 1 C x for all
≤ ≤ | − − | ≤ | |
x c < 1.
| | ≤
Reduction to heat kernel PCA. We have
inf (S φ ,...,S φ ) (vˆ ,...,vˆ )O
n 1 n j t,1 t,j 2
O∈O(j)k − k
30√j logn
C + jµ t
j
≤ µ µ ntd/2+2
(cid:16)
j+1
−
jr
p (cid:17)
with high probability.
j
Proof. If h ,...,h are elements in , then we consider (h ) =
1 j Ht k k=1
(h ,...,h ) as a linear map from Rj to , mapping α Rj to j α h .
1 j Ht ∈ k=1 k k
Now, on the one hand, we have φ = λ−1/2 u for all k N. On the other
k t,k t,k ∈ P
hand, (4.5) holds with high probability, implying that vˆ =
λˆ−1/2
S uˆ for
t,k t,k n t,k
all 1 k j by (2.3). We estimate with high probability
≤ ≤
inf (S φ ,...,S φ ) (vˆ ,...,vˆ )O
n 1 n j t,1 t,j 2
O∈O(j)k − k
= inf S
(λ−1/2
u
)j
S
(λˆ−1/2
uˆ
)j
O
O∈O(j)k n t,k t,k k=1− n t,k t,k k=1 k2
S inf
(λ−1/2
u
)j (λˆ−1/2
uˆ
)j
O
≤ k n k∞ ·O∈O(j)k t,k t,k k=1− t,k t,k k=1 k2
j j
C inf (u ) (uˆ ) O
≤ ·O∈O(j)k t,k k=1− t,k k=1 k2
+C
((λ−1/2
1)u
)j
+C
((λˆ−1/2
1)uˆ
)j
k t,k − t,k k=1k2 k t,k − t,k k=1k2
C P Pˆ
t,≤j t,≤j 2
≤ k − k
+C j max
λ−1/2
1 +C j max
λˆ−1/2
1,
1≤k≤j| t,k − | 1≤k≤j| t,k − |
p p
where we applied the fact that S 2 = λˆ C with high probability
n ∞ t,1
k k ≤
in the second inequality and Lemma 11 in the last inequality. Inserting
Theorem 2, (4.6), (4.4) and µ t 1 the claim follows.
j+1
≤
Reduction to the heat graph Laplacian. We have
inf (vˆ ,...,vˆ ) (u (L ),...,u (L ))O
O∈O(j)k
t,1 t,j
−
1 kt j kt k2
1 logn
P (1(I K )) P (L ) C .
≤ k ≤j t n − t − ≤j kt k2 ≤ µ µ ntd/2+2
j+1
−
jr
with high probability.
Proof. The first equality holds by Lemma 11. To see the second claim, first
note that
1 λ
t,j+1
1 λ
t,j
e−µjt e−µj+1t
− − = −
t − t t
31e(µj+1−µj)t 1 (µ
j+1
µ j)t µ
j+1
µ
j
= − − = − ,
teµj+1t ≥ te e
where we applied the assumption µ t 1 in the inequality. Combining
j+1
≤
this with Weyl’s bound and Lemma 4, we arrive at
I K I K 1 λˆ 1 λˆ
n t n t t,j+1 t,j
λ − λ − = − −
j+1 j
t − t t − t
1(cid:16) λ (cid:17) 1 λ(cid:16) 2 (cid:17)
− t,j+1 − t,j Σˆ Σ
t t ∞
≥ t − t − tk − k
µ µ logn
j+1 j
− C
≥ e − ntd/2+2
r
µ µ
j+1 j
− ,
≥ 2e
with high probability, provided that c > 0 in (4.4) is chosen small enough
(such that c 1/(2Ce)). Combining this with the Davis-Kahan bound in
≤
Corollary 5, we get
1 I D
P (1(I K )) P (L ) C j k n − ktk∞ (4.7)
k ≤j t n − t − ≤j kt k2 ≤ t µ µ
j+1 j
−
p
with high probability. Inserting (4.3), the claim follows.
Reduction to the Gaussian graph Laplacian. We have
µ
P (L ) P (L ) C j j+1 tlog2(e)
k ≤j kt − ≤j wt k2 ≤ µ µ t
j+1 j
−
p
with high probability.
Proof. For K N let L˜ = L +tKI and L˜ = L +tKI . Then L˜
and L˜ are st∈ rictly posk it tive ankt d Coroln lary 4 yiw et lds wt n kt
wt
L˜−1/2 (L˜ L˜ )L˜−1/2 Ctlog2(e)
k kt wt − kt kt k∞ ≤ t
for all t (0,c). In order to apply Corollary 5, it remains to upper bound
∈
λ (L˜ )
j+1 kt
.
λ (L˜ ) λ (L˜ )
j+1 kt
−
j kt
First, by Theorem 1, (4.1), (4.4) and (2.4), we have, with high probability,
λ (L˜ ) = λ (L )+tK
j+1 kt j+1 kt
321 e−µj+1t logn
− +C +tK
≤ t ntd/2+2
r
logn
µ +C +tK Cµ
j+1 j+1
≤ ntd/2+2 ≤
r
for all t (0,c). Second, by (4.3) and (4.4), we have, with high probability,
∈
λ (L˜ ) λ (L˜ ) = λ (L ) λ (L )
j+1 kt
−
j kt j+1 kt
−
j kt
I K I K 2
t t
λ − λ − I D
≥
j+1
t −
j
t − tk
n
−
ktk∞
(cid:16) (cid:17) (cid:16) (cid:17)
µ µ logn
j+1 j
− 2C
≥ 2e − ntd/2+2
r
µ µ
j+1 j
− ,
≥ 4e
provided that c > 0 in (4.4) is chosen small enough (such that c 1/(8Ce)).
≤
Applying Corollary 4 conclude that
P (L ) P (L ) = P (L˜ ) P (L˜ )
k
≤j kt
−
≤j wt k2
k
≤j kt
−
≤j wt k2
C j λ j+1(L˜ kt) L˜ −1/2 (L˜ L˜ )L˜ −1/2
≤ λ (L˜ ) λ (L˜ )k kt wt − kt kt k∞
j+1 kt
−
j kt
p µ
C j j+1 tlog2(e).
≤ µ µ t
j+1 j
−
p
for all t (0,c).
∈
A Appendix
A.1 Perturbation bounds
A.1.1 Positive self-adjoint operators
LetΣbeaself-adjointpositive operatoronaseparableHilbertspace . Let
H
λ λ > 0 be the sequence of positive eigenvalues and let u ,u ,...
1 2 1 2
≥ ≥ ···
be a sequence of corresponding eigenvectors. We assume that u ,u ,...
1 2
form an orthonormal basis of . For j N, let
H ∈
j
P = P , P = u u
≤j k k k k
⊗
k=1
X
bethe orthogonal projection onto the eigenspace spannedby u ,...,u . Let
1 j
Σˆ be another self-adjoint positive operator on . Let λˆ λˆ 0
1 2
H ≥ ≥ ··· ≥
33be the sequence of non-negative eigenvalues and let uˆ ,uˆ ,... bea sequence
1 2
of corresponding eigenvectors. We also assume that uˆ ,uˆ ,... form an or-
1 2
thonormal basis of . For j N, let
H ∈
j
Pˆ = Pˆ , Pˆ = uˆ uˆ .
≤j k k k k
⊗
k=1
X
The following lemma states some well-known absolute perturbation
bounds for eigenvalues and eigenvectors (see, e.g., [6]).
Lemma 8 (Absolute Weyl and Davis-Kahan bound). For all j N, we
∈
have
λˆ λ Σˆ Σ ,
j j ∞
| − |≤ k − k
Σˆ Σ
Pˆ P 32jk − k∞ .
≤j ≤j 2
k − k ≤ λ λ
j j+1
−
p
For the case of the leading eigenvalue and eigenspace, we formulate rel-
ative perturbation bounds derived in [35, 28, 47]. Let m N be the multi-
∈
plicity of λ such that
1
λ = = λ > λ .
1 m m+1
···
Moreover, let
P 1 P 1
δ (Σˆ Σ)= ≤m +R 2 (Σˆ Σ) ≤m +R 2
≤m >m >m
− λ 1 λ m+1 − λ 1 λ m+1 ∞
(cid:13)(cid:16) − (cid:17) (cid:16) − (cid:17) (cid:13)
(cid:13) (cid:13)
with (outer) redu(cid:13)ced resolvent (cid:13)
P
k
R = .
>m
λ λ
1 k
k>m −
X
Lemma 9. We have
Pˆ P 4√2√m δ (Σˆ Σ)
≤m ≤m 2 ≤m
k − k ≤ · −
and
Pˆ P √2 P (Σˆ Σ)R +20√2m δ (Σˆ Σ)2.
≤m ≤m 2 ≤m >m 2 ≤m
k − k ≤ k − k · −
Proof. See Proposition 1 in [26]. For the second boundcombine (2.4) in [26]
with the triangle inequality.
34Lemma 10. Assume that δ (Σˆ Σ) 1/4. Then
≤m
− ≤
λˆ λ (λ λ ) δ (Σˆ Σ).
j 1 1 m+1 ≤m
| − |≤ − · −
and
λˆ λ P EP +C(λ λ )m δ (Σˆ Σ)2
j 1 ≤m ≤m 2 1 m+1 ≤m
| − | ≤ k k − · −
for all 1 j m, with some absolute constant C > 0.
≤ ≤
Proof. Follows from proceeding as in Lemma 8 of [26] or [47]. See also
Theorem 3 in [28].
Lemma 11. Consider (u ,...,u ) and (uˆ ,...,uˆ ) as linear maps from Rj
1 j 1 j
to . Then we have
H
inf (u ,...,u ) (uˆ ,...,uˆ )O Pˆ P
1 j 1 j 2 ≤j ≤j 2
O∈O(j)k − k ≤k − k
with denoting the Hilbert-Schmidt norm.
2
k·k
Proof. See, e.g., Equation (2.6) and Proposition 2 in [46].
A.1.2 Symmetric matrices
We reformulate the results of Section A.1.1 in the case of matrices, since
our assumptions and notations for Laplacian matrices are slightly different
from those for covariance operators (for instance, the eigenvalues are listed
in reverse order). Let A Rn×n be a symmetric matrix. Let
∈
λ (A) λ (A) λ (A)
1 2 n
≤ ≤ ··· ≤
be the eigenvalues of A (in non-decreasing order), and let
u (A),u (A),...,u (A) Rn
1 2 n
∈
be corresponding eigenvectors. For 1 j n, let
≤ ≤
j
P (A) = P (A), P (A) = u (A)u (A)⊤,
≤j k k k k
k=1
X
be the orthogonal projection onto the eigenspace spanned by
u (A),...,u (A). For 1 j n with λ (A) > λ (A), let
1 j j+1 j
≤ ≤
P (A)
k
R (A) = .
>j
λ (A) λ (A)
k j
k>j −
X
35If A,B Rn×n are symmetric, then we define
∈
δ (B A)
≤j
−
P (A) 1 P (A) 1
≤j 2 ≤j 2
= +R (A) (B A) +R (A)
>j >j
λ j+1(A) λ j(A) − λ j+1(A) λ j(A) ∞
(cid:13)(cid:16) − (cid:17) (cid:16) − (cid:17) (cid:13)
for(cid:13)every 1 j n such that λ (A) > λ (A). (cid:13)
(cid:13) j+1 j (cid:13)
≤ ≤
Lemma 12 (Relative Davis-Kahan bound). Let A,B Rn×n be symmetric.
∈
Then we have
Pˆ (A) P (A) 32j δ (B A).
≤j ≤j ∞ ≤j
k − k ≤ · −
for every 1 j n such that λ (A) >pλ (A).
j+1 j
≤ ≤
Proof. See again Proposition 1 in [26].
Lemma 13. Let A,B Rn×n be symmetric and let 1 j n such that
∈ ≤ ≤
λ (A) > λ (A). Then
j+1 j
B A
∞
δ (B A) k − k
≤j
− ≤ λ (A) λ (A)
j+1 j
−
and, if A is additionally positive definite, then also
λ (A)
δ (B A) j+1 A−1/2(B A)A−1/2 .
≤j ∞
− ≤ λ (A) λ (A)k − k
j+1 j
−
Proof. Follows from simple properties of the operator norm.
Corollary 5. Let A,B Rn×n be symmetric and let 1 j n such that
∈ ≤ ≤
λ (A) > λ (A). Then
j+1 j
B A
Pˆ (B) P (A) 32j k − k∞
≤j ≤j ∞
k − k ≤ λ (A) λ (A)
j+1 j
−
p
and, if A is additionally positive definite, then also
λ (A)
Pˆ (B) P (A) 32j j+1 A−1/2(B A)A−1/2 .
≤j ≤j ∞ ∞
k − k ≤ λ (A) λ (A)k − k
j+1 j
−
p
Proof. Follows from inserting Lemma 13 into Lemma 12.
Lemma 14. Let A,B Rn×n be symmetric and let 1 j n. Then,
∈ ≤ ≤
λ (B) λ (A) B A
j j ∞
| − | ≤ k − k
and, if A is additionally strictly positive definite, then also
λ (B) λ (A) λ (A) A−1/2(B A)A−1/2 .
j j j ∞
| − | ≤ k − k
Proof. See Theorem 2.7 in [25].
36A.2 Concentration inequalities
The following lemma states an operator Bernstein inequality (see, e.g.,
Lemma 5 in [17] and also [43]).
Lemma 15. Let ξ ,...,ξ be a sequence of independently and identi-
1 n
cally distributed self-adjoint Hilbert-Schmidt operators on a separable Hilbert
space. Suppose that Eξ = 0 and that ξ R almost surely for some
1 1 op
k k ≤
constant R > 0. Moreover, suppose that there are constants V,D > 0 satis-
fying Eξ2 V and tr(Eξ2) VD. Then, for all u > 0,
k 1kop ≤ 1 ≤
1 n nu2
P ξ u 4Dexp .
i
n op ≥ ≤ − 2V +(2/3)uR
(cid:16)(cid:13) Xi=1 (cid:13) (cid:17) (cid:16) (cid:17)
(cid:13) (cid:13)
The next le(cid:13)mma sta(cid:13)tes an extension of Lemma A.1 in [24].
Lemma 16. Let (a ) be a non-increasing sequence of non-negative real
k
∞
numbers such that (a ) = a < . Let (Z ) be a sequence of
k k k1 k=1 k ∞ ik
real-valued centered random variables with k N and 1 i n. Suppose
P ∈ ≤ ≤
that (Z )n are i.i.d. for each k N and that there is a constant C > 0
ik i=1 ∈ 1
such that
N
N N, Z2 C N a.s.
∀ ∈ 1k ≤ 2
k=1
X
Then there is a constant C > 0 depending only on C such that, for every
2
t 1, we have
≥
∞ 1 n 2 t t2
a Z C (a ) + (ka )
k n ik ≤ k k k1 · n k k k1 n2
Xk=1 (cid:16) Xi=1 (cid:17) (cid:16) (cid:17)
with probability at least 1 e−t.
−
Proof. For p N Minkowski’s inequality yields
∈
∞ n ∞ n
1 2 1 2
a Z a Z (A.1)
k ik k ik
n Lp ≤ n L2p
(cid:13)Xk=1 (cid:16) Xi=1 (cid:17) (cid:13) Xk=1 (cid:13) Xi=1 (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
By
assumptio(cid:13)
n Z is centered
w(cid:13)
ith Z
(cid:13)√C
k.
H(cid:13)
ence, Bernstein’s
1k 1k 2
| | ≤
inequality yields
1 n 2zEZ2 z√C k
P Z 1k + 2 e−z
ik
n ≥ s n n ≤
(cid:16) Xi=1 (cid:17)
37for all z > 0 and all k N. Since the same deviation bound holds for
n−1 n Z , Theorem∈ 2.3 in [8] yields
− i=1 ik
P 1 n 2 8EZ2 p 4√C k 2p 1/p
p N, Z p! 1k +(2p)! 2
ik
∀ ∈ n L2p ≤ n n
(cid:13) Xi=1 (cid:13) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17) (cid:17)
(cid:13) (cid:13) (cid:13) (cid:13) C p EZ2 + p2 k .
≤ n 1k n2
(cid:16) (cid:17)
Inserting this into (A.1), we get
∞ 1 n 2 p ∞ p2 ∞
p N, a Z C E a Z2 + ka .
∀ ∈ k n ik Lp ≤ n k 1k n2 k
(cid:13)Xk=1 (cid:16) Xi=1 (cid:17) (cid:13) (cid:16) Xk=1 Xk=1 (cid:17)
(cid:13) (cid:13)
(cid:13) (cid:13)
Set B = k Z2. Then B Ck for all k N by assumption. Hence, for
k l=1 1l k ≤ ∈
all N N, we have
∈ P
N N
a Z2 = a (B B )
k 1k k k − k−1
k=1 k=1
X X
N−1
= a B a B + (a a )B
N N 1 0 k k−1 k
− −
k=1
X
N−1 N
C a N + (a a )k = C a
N k k−1 k
≤ −
(cid:16) Xk=1 (cid:17) Xk=1
almost surely. Letting N , this implies
→ ∞
∞
a Z2 C (a )
k 1k ≤ k k k1
k=1
X
almost surely. We arrive at
∞ 1 n 2 p p2
p N, a Z C (a ) + (ka ) .
∀ ∈ k n ik Lp ≤ nk k k1 n2k k k1
(cid:13)Xk=1 (cid:16) Xi=1 (cid:17) (cid:13) (cid:16) (cid:17)
(cid:13) (cid:13)
(cid:13) (cid:13)
The claim now follows from an application of Markov’s inequality with an
appropriate choice of p (see, e.g., Equation (3.2) in [20]).
38A.3 Eigenvalue estimates
Lemma 17. Let (µ )∞ be a sequence of real numbers such that µ
j j=m+1 j ≥
cj2/d for all j > m and some constants d 3 and c > 0. Then there is an
≥
absolute constant C > 0 depending only on c and d such that
e−µjt
Ct−d/2,
1 e−µjt ≤
j>m −
X
e−µjt
Ct−d/2,
(1 e−µjt)2 ≤
j>m −
X
je−µjt
Ct−d
(1 e−µjt)2 ≤
j>m −
X
for all t (0,1]. In the last inequality, we additionally assume that d 5.
∈ ≥
Proof. Since x x/(1 x) is increasing for x (0,1], we have
7→ − ∈
e−µjt e−cj2/dt
1 e−µjt ≤ 1 e−cj2/dt
j X>m − j X>m −
M
1
∞ e−cj2/dt
+
≤ ecj2/dt 1 1 e−cj2/dt
j= Xm+1 − j= XM+1 −
for M > m. Choose M =max j > m : cj2/dt 1 and M = m if the latter
{ ≤ }
set is empty. Then
M M
1 1
ecj2/dt 1 ≤ cj2/dt
j= Xm+1 − j= Xm+1
C M 1
dx
≤ t x2/d
Zm
Ct−1M1−d/2 Ct−1(t−d/2)1−d/2 = Ct−d/2
≤ ≤
and
∞ e−cj2/dt
1
∞
e−cx2/dt,
1 e−cj2/dt ≤ (1 e−1)
j= XM+1 − − j= XM+1
1 ∞
e−cx2/dtdx
≤ 1 e−1
−
ZM
1 ∞
= t−d/2 e−cx2/d dx Ct−d/2.
1 −e−1 ZMtd/2 ≤
The other bounds follow similarly.
39References
[1] F.Bach. Informationtheorywithkernelmethods. IEEETrans. Inform.
Theory, 69(2):752–775, 2023.
[2] A. S. Bandeira, A. Singer, and T. Strohmer. Mathematics of data
science. Book in preparation.
[3] M.BelkinandP.Niyogi. Laplacianeigenmapsfordimensionalityreduc-
tion and data representation. Neural Computation, 15(6):1373–1396,
2003.
[4] M.BelkinandP.Niyogi. Convergenceoflaplacianeigenmaps. InNeural
Information Processing Systems, 2006.
[5] M. Belkin and P. Niyogi. Towards a theoretical foundation
for Laplacian-based manifold methods. J. Comput. System Sci.,
74(8):1289–1308, 2008.
[6] R. Bhatia. Matrix analysis. Springer-Verlag, New York, 1997.
[7] G. Blanchard and J.-B. Fermanian. Nonasymptotic one- and two-
sample tests in high dimension with unknown covariance structure. In
Foundations of modern statistics, volume 425 of Springer Proc. Math.
Stat., pages 121–162. Springer, Cham, [2023] ©2023.
[8] S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities:
a nonasymptotic theory of independence. Oxford University Press, Ox-
ford, 2013.
[9] D. Burago, S. Ivanov, and Y. Kurylev. A graph discretization of the
Laplace-Beltrami operator. J. Spectr. Theory, 4(4):675–714, 2014.
[10] J. Calder and N. Garc´ıa Trillos. Improved spectral convergence rates
for graph Laplacians on ε-graphs and k-NN graphs. Appl. Comput.
Harmon. Anal., 60:123–175, 2022.
[11] F. Chatelin. Spectral approximation of linear operators. Society for
Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2011.
[12] I. Chavel. Eigenvalues in Riemannian geometry. Academic Press, Inc.,
Orlando, FL, 1984.
40[13] X. Cheng and N. Wu. Eigen-convergence of Gaussian kernelized graph
Laplacian by manifold heat interpolation. Appl. Comput. Harmon.
Anal., 61:132–190, 2022.
[14] F. R. K. Chung. Spectral graph theory. Published for the Conference
BoardoftheMathematical Sciences,Washington, DC;bytheAmerican
Mathematical Society, Providence, RI, 1997.
[15] R. R. Coifman and S. Lafon. Diffusion maps. Appl. Comput. Harmon.
Anal., 21(1):5–30, 2006.
[16] E. B. Davies. Heat kernels and spectral theory. Cambridge University
Press, Cambridge, 1990.
[17] L. H. Dicker, D. P. Foster, and D. Hsu. Kernel ridge vs. principal
component regression: minimax bounds and the qualification of regu-
larization operators. Electron. J. Stat., 11(1):1022–1047, 2017.
[18] N.Garc´ıaTrillos, M.Gerlach, M.Hein,andD.Slepˇcev. Errorestimates
for spectral convergence of the graph Laplacian on random geometric
graphs toward the Laplace-Beltrami operator. Found. Comput. Math.,
20(4):827–887, 2020.
[19] E. Gin´e and V. Koltchinskii. Empirical graph Laplacian approximation
of Laplace-Beltrami operators: large sample results. In High dimen-
sional probability, volume 51 of IMS Lecture Notes Monogr. Ser., pages
238–259. Inst. Math. Statist., Beachwood, OH, 2006.
[20] E. Gin´e, R. Lata l a, and J. Zinn. Exponential and moment inequalities
forU-statistics. InHighdimensional probability, II (Seattle, WA,1999),
volume 47 of Progr. Probab., pages 13–38. Birkh¨auser Boston, Boston,
MA, 2000.
[21] A.Grigor’yan. Heatkernelandanalysisonmanifolds. AmericanMathe-
maticalSociety, Providence,RI;InternationalPress,Boston,MA,2009.
[22] A. Grigor’yan. Estimates of heat kernels on Riemannian manifolds,
page140–225. LondonMathematical SocietyLectureNoteSeries.Cam-
bridge University Press, 1999.
[23] T. Hsing and R. Eubank. Theoretical foundations of functional data
analysis, with an introduction to linear operators. John Wiley & Sons,
Ltd., Chichester, 2015.
41[24] Laura Hucker and Martin Wahl. A note on the prediction error of
principal component regression in high dimensions. Theory Probab.
Math. Statist., 109:37–53, 2023.
[25] I. C. F. Ipsen. Relative perturbation results for matrix eigenvalues and
singular values. In Acta numerica, 1998, volume 7 of Acta Numer.,
pages 151–201. Cambridge Univ. Press, Cambridge, 1998.
[26] M. Jirak and M. Wahl. Quantitative limit theorems and
bootstrap for empirical spectral projectors. Available at
https://arxiv.org/abs/2208.12871.
[27] M. Jirak and M. Wahl. Perturbation bounds for eigenspaces under a
relative gap condition. Proc. Amer. Math. Soc., 148(2):479–494, 2020.
[28] M. Jirak and M. Wahl. Relative perturbation boundswith applications
to empirical covariance operators. Adv. Math., 412:Paper No. 108808,
59 pp, 2023.
[29] I. M. Johnstone and D. Paul. PCA in high dimensions: An orientation.
Proceedings of the IEEE, 106(8):1277–1292, 2018.
[30] I.Koch.Analysisofmultivariateandhigh-dimensional data. Cambridge
University Press, New York, 2014.
[31] V. Koltchinskii and E. Gin´e. Random matrix approximation of spectra
of integral operators. Bernoulli, 6(1):113–167, 2000.
[32] V. Koltchinskii, M. Lo¨ffler, and R. Nickl. Efficient estimation of linear
functionalsofprincipalcomponents. Ann. Statist.,48(1):464–490, 2020.
[33] V.KoltchinskiiandK.Lounici. Concentration inequalitiesandmoment
boundsforsamplecovarianceoperators.Bernoulli,23(1):110–133, 2017.
[34] S. Minakshisundaram and ˚A. Pleijel. Some properties of the eigen-
functions of the Laplace-operator on Riemannian manifolds. Canad. J.
Math., 1:242–256, 1949.
[35] M. Reiss and M. Wahl. Nonasymptotic upper bounds for the recon-
struction error of PCA. Ann. Statist., 48(2):1098–1123, 2020.
[36] J.W.RobbinandD.A.Salamon. Introduction to differential geometry.
Springer Spektrum, Wiesbaden, 2022.
42[37] L. Rosasco, M. Belkin, and E. De Vito. On learning with integral
operators. J. Mach. Learn. Res., 11:905–934, 2010.
[38] S. Rosenberg. The Laplacian on a Riemannian manifold. Cambridge
University Press, Cambridge, 1997.
[39] B. Scho¨lkopf, A. Smola, and K.-R. Mu¨ller. Nonlinear component anal-
ysis as a kernel eigenvalue problem. Neural Computation, 10(5):1299–
1319, 1998.
[40] Z. Shi. Convergence of laplacian spectra from random samples. Avail-
able at https://arxiv.org/abs/1507.00151.
[41] A. Singer. From graph to manifold Laplacian: the convergence rate.
Appl. Comput. Harmon. Anal., 21(1):128–134, 2006.
[42] I. Steinwart and A. Christmann. Support vector machines. Springer,
New York, 2008.
[43] J. A. Tropp. An Introduction to Matrix Concentration Inequalities.
2015.
[44] U. von Luxburg. A tutorial on spectral clustering. Stat. Comput.,
17(4):395–416, 2007.
[45] U. von Luxburg, M. Belkin, and O. Bousquet. Consistency of spectral
clustering. Ann. Statist., 36(2):555–586, 2008.
[46] V. Q. Vu and J. Lei. Minimax sparse principal subspace estimation in
high dimensions. Ann. Statist., 41(6):2905–2947, 2013.
[47] M. Wahl. On the perturbation series for eigenvalues and eigenprojec-
tions. Available at https://arxiv.org/abs/1910.08460.
[48] M. J. Wainwright. High-dimensional statistics. Cambridge University
Press, Cambridge, 2019.
43