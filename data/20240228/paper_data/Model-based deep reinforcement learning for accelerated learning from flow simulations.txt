Model-based deep reinforcement learning for
accelerated learning from flow simulations
Andre Weiner1*† and Janis Geise1†
1*Institute of Fluid Mechanics, TU Dresden, George-Ba¨hr-Str. 3c ,
Dresden, 01069, Saxony, Germany.
*Corresponding author(s). E-mail(s): andre.weiner@tu-dresden.de;
Contributing authors: j.geise@tu-braunschweig.de;
†These authors contributed equally to this work.
Abstract
Inrecentyears,deepreinforcementlearninghasemergedasatechniquetosolve
closed-loop flow control problems. Employing simulation-based environments in
reinforcementlearningenablesaprioriend-to-endoptimizationofthecontrolsys-
tem,providesavirtualtestbedforsafety-criticalcontrolapplications,andallows
to gain a deep understanding of the control mechanisms. While reinforcement
learning has been applied successfully in a number of rather simple flow control
benchmarks, a major bottleneck toward real-world applications is the high com-
putationalcostandturnaroundtimeofflowsimulations.Inthiscontribution,we
demonstrate the benefits of model-based reinforcement learning for flow control
applications.Specifically,weoptimizethepolicybyalternatingbetweentrajecto-
riessampledfromflowsimulationsandtrajectoriessampledfromanensembleof
environmentmodels.Themodel-basedlearningreducestheoveralltrainingtime
by up to 85% for the fluidic pinball test case. Even larger savings are expected
for more demanding flow simulations.
Keywords:closed-loopflowcontrol,modelensembleproximalpolicyoptimization,
fluidicpinball
1 Introduction
According to the 2022 IPCC report, smart control technologies are a key enabler in
mitigatingtheimpactofglobalwarmingandclimatechange[1].Controltasksinwhich
the manipulation of fluid flows can significantly reduce the carbon dioxide footprint
1
4202
beF
62
]nyd-ulf.scisyhp[
1v34561.2042:viXraare ubiquitous in society and industries, e.g., reducing aerodynamic forces acting on
vehicles [2], maximizing the coefficient of performance of heat pumps [3], or adjusting
the operating conditions of process engineering systems in response to the availability
of renewable energy resources [4], to name a few. To achieve optimal control in the
aforementionedexampleapplications,itisnecessarythatthecontrollercanrespondto
changesintheambientconditions.However,thedesignandimplementationofclosed-
loopactiveflowcontrol(AFC)systemsishighlycomplex.Asanexample,considerthe
flow past a truck. At highway speeds, such a flow is fully 3D and comprises turbulent
boundary layers, flow separation, reattachment, and large turbulent wakes [5, 6]. To
implementclosed-loopAFC,thiscomplexitymustbecapturedwithalimitedamount
ofsensorsplacedonthevehicle’ssurface.Similarly,theactuatorsmustbedesignedand
placed sensibly to control the dominant flow structures. Moreover, a suitable control
lawmustbederived,i.e.,amappingfromthesensorreadingtotheoptimalactuation.
Finally,thenonlinearflowdynamicscreateatightcouplingbetweensensors,actuators,
andcontrollaw.Hence,theAFCsystemshouldbedesignedandoptimizedasawhole.
Recently, reinforcement learning (RL), and in particular deep RL (DRL), started
toemergeasatechnologyinfluidmechanicswiththepotentialtotacklethecomplex-
ity of closed-loop AFC. Viquerat et al. [7] provide an extensive account of DRL-based
applications in fluid mechanics. DRL solves control problems through trial-and-error
interactionswithanenvironment,whereenvironment istheDRLtermforthesystem
to be controlled. The unique advantage of learning from simulation-based environ-
ments is the possibility of end-to-end optimization of the full control system, i.e.,
sensor locations, control law, and actuator positions [8–10]. However, a critical chal-
lenge of simulation-based RL is the high computational cost and turnaround time of
flow simulations. Even for relatively simple flow control benchmark problems, state-
of-the-art RL algorithms require O(100) episodes to converge [7], which is far from
practical for real-world AFC applications.
Toemphasizetheneedforsample-efficientlearning,consideragaintheapplication
of closed-loop AFC to vehicles. The current gold standard for accurate and fast simu-
lationsofvehicleaerodynamicsarehybridapproacheslikeimproveddelayeddetached
eddy simulations (IDDES) [11]. Computing converged mean force coefficients for the
DrivAer testcaseatRe=768000,requiredapproximatelytwodayson700CPUcores
in2016[11].ItisconceivablethatroughlyfivesequentialIDDEScanbeperformedper
day because it is not necessary to obtain converged statistics in every run. Moreover,
hardware and scalability have improved significantly over the past years. Assuming
that 10 simulations are performed in parallel per episode to generate more data, each
simulation being run on 1000 CPU cores, the computational cost for 100 episodes is
approximately 5M CPU hours, and the training lasts 20 days. Cloud providers charge
0.01-0.05 euros per CPU hour, which puts a price tag of 0.5M-2.5M euros on a single
trainingatasingleflowcondition.Additionalhyperparameteroptimization,sensor/ac-
tuator variants, and an extension to multiple flow conditions can easily increase the
cost by a factor of 10-100. This cost, combined with a turnaround time of 20 days, is
far from practical for the majority of potential users.
Several remedies exist to reduce the computational cost and turnaround time,
e.g., by exploiting invariances [12, 13] or by pre-training on simulations with very
2coarse meshes [14]. While effective, the applicability of the aforementioned techniques
strongly depends on the control problem, i.e., the problem must exhibit invariances,
and the mesh coarsening must not change the flow characteristics. A more general
approach to improve data efficiency is model-based DRL (MBDRL) [15]. The main
idea is to substitute the expensive simulation-based environment with one or more
surrogatemodels.Thesurrogatemodelsareoptimizedbasedondatacomingfromthe
high-fidelity environment. Additional data to optimize the control law can be created
with little effort by querying the optimized surrogate model. A plethora of MBDRL
algorithms exist that differ in the type of surrogate model and how the control law is
derived from the models. Moerland et al. [15] provide an extensive overview.
KeychallengesinMBDRLarei)theefficientcreationofaccuratesurrogatemodels
and ii) dealing with the presence of model error. Due to their flexibility, neural net-
works are a common choice for building data-driven surrogate models. However, the
models must be created on the fly based on data that is unavailable for prior tests,
so the optimization pipeline must be robust and efficient. Moreover, the models are
auto-regressive. Hence, long-term stability is a persistent challenge. If the prediction
accuracy is not sufficiently high, querying the models repeatedly leads to vanishing
or exploding solutions. Finally, even if the model creation workflow is robust and effi-
cient,thereisalwaysepistemicuncertaintyduetothealternatingiterativeupdatesof
environment models and control law. With each update of the control law, previously
unknown actuations and states will emerge, and consequently, the environment mod-
els will fail at some point. Estimating if the surrogate models are still reliable or if
they should be updated with high-fidelity data from the true environment is essential
to make MBDRL robust.
In this contribution, we present a modified version of the model ensemble trust
regionpolicyoptimization(METRPO)[16]algorithmanddemonstratethebenefitsof
MBDRL for fluid-mechanical applications. Specifically, we compare model-free (MF)
and model-based (MB) learning on the flow past a circular cylinder [17, 18] and
the fluidic pinball configuration [9, 19]. Moreover, we provide a complimentary code
repository with instructions to reproduce the numerical experiments and data anal-
yses presented hereafter at https://github.com/JanisGeise/MB DRL for accelerated
learning from CFD.
2 Theory
This section starts with a very short introduction to essential RL concepts and then
describes the MF algorithm employed to solve the flow control problems, namely
proximal policy optimization (PPO). Thereafter, we explain the creation of model
ensembles, which emulate the simulation outputs, and how these ensembles are
employed in an MBDRL variant of PPO.
2.1 Reinforcement learning
TherearetwomainentitiesinRL:theagentandtheenvironment.Theagentcontains
the control logic, while the environment represents the simulation or experiment that
shallbecontrolled.Theenvironmentatadiscretetimestepn Nischaracterizedbya
∈
3stateS ,where isthespaceofallpossiblestates.Formally,thereisadifference
n
∈S S
betweenthesystem’sstateandapartialobservationofthestate,sincethefullstateis
ofteninaccessible.Nonetheless, wefollowstandard practiceand referto partial obser-
vations and full states alike simply as states. In fluid mechanics, S could represent
n
instantaneouspressureorvelocityvaluesatoneormoresensorlocationsintheregion
of interest. The agent can manipulate the state via an action A , where is the
n
∈A A
space of all possible actions, upon which the environment transitions to a new state
S . Common actions (means of actuation) for flows are suction, blowing, rotation,
n+1
or heating. The control objective in RL is expressed by the reward R , where
n+1
∈R
is the space of all possible rewards, and is assessed based on the transition from S
n
R
to S . For example, if the control objective is drag reduction, the reward would be
n+1
definedsuchthatlargedragforcesyieldlowrewardswhilesmalldragforcesyieldhigh
rewards.ThecombinationofS ,A ,R ,andS iscalledanexperiencetuple[20].
n n n+1 n+1
TheaggregationofexperiencetuplesofN interactionsbetweenagentandenvironment
forms a so-called trajectory [20], τ = [(S ,A ,R ,S ),...,(S ,A ,R ,S )].
0 0 1 1 N 1 N 1 N N
− −
Trajectories form the basis for optimizing the policy π(A = aS = s) (the control
n n
|
law), which predicts the probability of taking some action a given that the current
state is s. Optimality in RL is defined in terms of cumulative rewards, the so-called
returnG = N γiR ,becausethesystem’sresponsetoanactionmaybedelayed.
n i=n+1 i
The discount factor γ (0,1] introduces a notion of urgency, i.e., the same reward
(cid:80) ∈
counts more if achieved early in a trajectory. Given the same policy π and the same
stateS ,thereturnG mightvarybetweentrajectories.Incompleteknowledgeofthe
n n
state is one common source of uncertainty. To consider uncertainty in the optimiza-
tion, RL ultimately aims to find the policy that maximizes the expected return v (s)
π
under policy π over all possible states s [20]:
∈S
v (s)=max E [G S =s], (1)
π∗
π
π n
|
n
vπ(s)
(cid:124) (cid:123)(cid:122) (cid:125)
where E [] denotes the expected value under policy π, e.g., the mean value for a
π
·
normally distributed random variable, and v (s) is the optimal state-value function.
π∗
Equation (1) forms the basis for all RL algorithms.
2.2 Policy learning
Expression(1)implicitlydefinestheoptimalpolicyπ (as),whichisthepolicyassoci-
∗
|
ated with the optimal value function v (s). However, additional algorithmic building
π∗
blocksarerequiredtoderiveaworkablepolicy.PPOisthemostcommonalgorithmto
solveproblem(1)forfluid-mechanicalsystems[7].PPOemploysdeepneuralnetworks
asansatzforthevaluefunctionv (s)andthepolicyπ(as).Hence,PPObelongstothe
π
|
group of actor-critic DRL algorithms. PPO is relatively straightforward to implement
and enables accelerated learning from multiple trajectories, i.e., multiple trajectories
may be sampled and processed in parallel rather than sequentially to produce new
training data T = [τ ,τ ,...τ ]. It is important to note that there are many PPO
1 2 K
details that vary across different implementations. Many of these details are not thor-
oughly described in the standard PPO reference article [21], but they are nonetheless
4of utmost importance. Andrychowicz et al. [22] provide guidelines based on compre-
hensive test results for a multitude of algorithmic variants. Our implementation is
based on the textbook by Morales [23].We outline only those elements needed for the
MBDRL version.
Basedonthetrajectories,thefreeparametersθ oftheparametrizedvaluenetwork
v
v (s) are optimized to predict the expected return over all trajectories. The value
θv
function’s subscript π has been dropped to simplify the notation. The loss function
includes some additional clipping to limit the change between two updates [23]:
K N 1
L = 1 − max Gk v (Sk) 2 , Gk Vk 2 , (2)
val KN n− θv n n− clip,n
k (cid:88)=1n (cid:88)=0 (cid:110)(cid:0) (cid:1) (cid:0) (cid:1) (cid:111)
Vk =vold(Sk)+clip v (Sk) vold(Sk), δ,δ ,
clip,n θv n θv n − θv n −
(cid:8) (cid:9)
where vold is the value network’s state before optimization, and the clip function is
θv
defined as:
x, for δ x δ
− ≤ ≤
clip(x, δ,δ)= δ, for x< δ . (3)

− − −
δ, for x>δ
Similar to the value function network, the policy network π (as) parametrizes a
 θπ
|
multivariate Beta distribution over possible actions. During exploration, actions are
sampled at random from this distribution, i.e., A π (S ). We do not consider a
n
∼
θπ n
potential cross-correlation between actions for exploration. To evaluate if a sampled
actionperformsbetterthanexpected,therewardiscomparedtotheexpectedreward,
i.e. [21, 23]:
δ =R (v (S ) γv (S )), (4)
n n
−
θv n
−
θv n+1
whereδ iscalledthe1-stepadvantageestimateattimestepn.Actionswithapositive
n
advantage estimate should be favored. Rather than looking one step ahead, one could
alsolooklstepsaheadtoconsiderdelayedeffects.Thegeneralizedadvantageestimate
Aˆ balances between short-term and long-term effects using the hyperparameter λ
∈
(0,1] [21, 23]:
N n
Aˆ = − (γλ)lδ . (5)
n n+l
l=0
(cid:88)
Finally,thefreeparametersofthepolicynetwork,θ ,areoptimizedtomakefavorable
π
actions (Aˆ > 0) more likely and poor actions (Aˆ 0) less likely. This concept is
n n
≤
combined with a clipping mechanism that restricts policy changes [23]:
L =
1 K N −1
min
π θπ(Ak n|S nk)
Aˆk,clip
π θπ(Ak n|S nk)
, ε,ε Aˆk
pol −KN
k (cid:88)=1n (cid:88)=0
(cid:40)π θo πld(Ak n|S nk) n (cid:40)π θo πld(Ak n|S nk) − (cid:41) n (cid:41)
K N 1
β −
E π (Sk) (6)
− KN θπ n
k=1n=0
(cid:88) (cid:88) (cid:2) (cid:3)
5where π /πold is the probability ratio between the current and old (before optimiza-
θπ θπ
tion) policy, ε is the clipping parameter, E(π) is the policy’s entropy [23], and β is
the entropy’s weighting factor. The additional entropy term avoids a premature stop
of the exploration [15].
One sequence of generating trajectories, updating the value network according to
(2), and updating the policy according to (6) is called an episode. Since the optimiza-
tion starts with randomly initialized value and policy networks, multiple episodes are
required to find the optimal policy.
2.3 Model learning
Thetypeofenvironmentmodelemployedhereisasimplefeed-forwardneuralnetwork
with weights θ that maps from the last d+1 states and a given action to the next
m
state and the received reward:
m :(S ,...,S ,S ,A ) (S ,R ). (7)
θm n −d n −1 n n
→
n+1 n+1
Tosimplifythenotation,weintroducethecurrentextendedstateSˆ ,whichcomprises
n
the last d+1 visited states:
Sˆ =[S ,...,S ,S ]. (8)
n n d n 1 n
− −
Arranging the current extended state and the current action into a feature vector,
x = [Sˆ ,A ], and the next state and the received reward into a label vector, y =
n n n n
[S ,R ],themodelweightsθ canbeoptimizedemployingameansquarederror
n+1 n+1 m
(MSE) loss of the form:
D
1 | |
L = (y m (x ))2, (9)
m
D
i
−
θm i
| | i
(cid:88)
where D is a set of feature-label-pairs constructed from high-fidelity trajectories, and
D is the overall number of feature-label-pairs. A single trajectory of length N yields
| |
D = N (d+1) such pairs. To obtain the model weights based on (9), we employ
| | −
state-of-the-art deep learning techniques, e.g., normalization of features and labels,
layer normalization, batch training, learning rate schedule, and early stopping. More
details about the model training are provided in appendix A.
2.4 Model-based proximal policy optimization
Givenatrainedmodeloftheform(7),itbecomesstraightforwardtosamplenew(fic-
titious) trajectories from the model by employing the model recursively. This process
isdescribedindetailinalgorithm1.TheinitialextendedstateSˆ isselectedfromthe
0
existing high-fidelity trajectories. Note that the action sampling makes the trajectory
sampling stochastic, so even when starting from the same Sˆ , executing algorithm 1
0
K times generates K different trajectories. For efficiency, the sampling of multiple
trajectories should be performed in parallel.
6Algorithm 1 Sampling of a trajectory from an environment model.
Input: Sˆ , π , m ▷ initial history, policy, env. model
0 θπ θm
Result: τ ▷ sampled model trajectory
τ [] ▷ initialize empty trajectory
Sˆ← Sˆ ▷ initialize current extended state
n 0
←
while i<N do
A π (S ) ▷ sample an action from the current policy
[Sn
∼
,Rθπ n
] m (Sˆ ,A ) ▷ predict the next state and the reward
n+1 n+1
←
θm n n
τ τ [[S ,A ,R ,S ]] ▷ append experience tuple to trajectory
n n n+1 n+1
Sˆ← [∪
S ,...,S ,S ] ▷ overwrite current extended state
n n d+1 n n+1
end w← hile −
In principle, MBDRL can work by sampling trajectories from a single model.
However, the policy changes with each episode, which leads to newly explored states
and actions. Naturally, the environment model’s prediction error increases with each
episode, and it becomes increasingly challenging to sample meaningful trajectories
fromthemodel.Therefore,itisvitaltomonitorthemodel’sconfidenceinaprediction
todecideatwhichpointnewhigh-fidelityepisodesshouldbegenerated.Therearetwo
pathways to include model uncertainty: one could train a fully probabilistic environ-
ment model, i.e., a model that predicts a distribution over the next possible states,
or one could train an ensemble of regular models [15]. Here, we follow the METRPO
algorithm [16] and train an ensemble of N simple environment models. Each mem-
m
ber in the ensemble M = [m ,m ,...,m ] has a different set of weights θ , which
1 2 Nm i
we drop from the index to simplify the notation. The ensemble approach introduces
a new hyperparameter, namely the number of models, but training the ensemble is
typically easier than training a fully probabilistic network. Moreover, it is straightfor-
ward to optimize the models in parallel. Each model is trained as described before,
i.e., employing loss function (9). Even though the training procedure stays the same,
the optimized models differ from one another for several reasons:
1. The dataset D is split randomly into N subsets, and each model is trained on a
m
different subset.
2. Each model has a different set of randomly initialized weights. Since the optimiza-
tion is nonlinear, the optimized weights likely differ, even if the same dataset is
used for training.
3. Thegradientdescentalgorithmupdatesthemodelweightsmultipletimesperepoch
(iteration) based on batch gradients. The mini-batches are chosen at random from
the training data (batch training).
There are several options to generate new trajectories from the model ensemble.
Themostobviousoneistorepeatalgorithm1oncepermodel.Incontrast,theoriginal
METRPO algorithm mixes the models within the trajectory sampling to improve
robustness [16]. The authors state that different models are likely to have different
model biases due to the training on a different subset of the data. When mixing the
models, the different biases can partially cancel out. The alternation between models
isachievedbysamplingadifferentmodelforeachstep from acategoricaldistribution
7P overN classes,whereeachclassihasanequalselectionprobabilityofp =1/N .
M m i m
Themodifiedsamplingstrategy,whichwealsoemployhere,isdepictedinalgorithm2.
Algorithm 2 Sampling of a trajectory from an ensemble of environment models.
Differences compared to algorithm 1 are marked in bold.
Input: Sˆ , π , M ▷ initial history, policy, model ensemble
0 θπ
Result: τ ▷ sampled model trajectory
τ [] ▷ initialize empty trajectory
Sˆ← Sˆ ▷ initialize current extended state
n 0
←
while i<N do
A π (S ) ▷ sample an action from the current policy
n
∼
θπ n
m P ▷ randomly select a model from the ensemble
n M
[S ∼ ,R ] m (Sˆ ,A ) ▷ predict the next state and the reward
n+1 n+1 n n n
←
τ τ [[S ,A ,R ,S ]] ▷ append experience tuple to trajectory
n n n+1 n+1
Sˆ← [∪
S ,...,S ,S ] ▷ overwrite current extended state
n n d+1 n n+1
end w← hile −
Besides the robust generation of trajectories, the ensemble allows to quantify pre-
diction uncertainty. More precisely, we would like to determine, at which point the
ensemble becomes unsuitable to generate new trajectories. Therefore, in each model-
based episode, K trajectories are generated from the ensemble employing algorithm
2, and N additional trajectories, one for each model, are generated employing algo-
m
rithm 1. The ensemble trajectories are used to update policy and value networks.
The model-specific trajectories are used to evaluate the policy loss (6) individually
for each model to obtain a scalar value expressing the model’s quality (not for gradi-
ent descent). Comparing the ith model’s loss values of the current episode, i.g., Lnew,
pol,i
with the loss of the same model obtained in the previous episode, i.e., Lold , allows
pol,i
assessing the quality of the policy update in the current episode. The loss trends for
all models in the ensemble are combined by evaluating:
Nm
N = H Lold Lnew , (10)
pos pol,i− pol,i
i=1
(cid:88) (cid:0) (cid:1)
whereH istheHeavisidestepfunction.Insimpleterms,N isthenumberofmodels
pos
in the ensemble for which the policy loss improves after an update of the policy.
Values of N /N close to zero indicate that the models’ generalization capabilities
pos m
are exhausted. Consequently, new high-fidelity data should be generated to update
the ensemble. We define the condition N N to switch between model-based
pos thr
≥
and simulation-based trajectory sampling. Values of N /N close to zero encourage
thr m
learning from the models at the risk of decreased robustness, whereas values close to
one ensure high model quality at the risk of decreased computational efficiency. It is
conceivablethatN /N 0.5mightbeasuitablecompromisebetweenefficiencyand
thr m
≈
control performance. The final model ensemble PPO (MEPPO) workflow is depicted
in algorithm 3.
8Algorithm 3 Model-based proximal policy optimization (MEPPO) algorithm.
Input: v , π , M ▷ initial value and policy networks, model ensemble
θv θπ
Result: θ ▷ optimal policy weights
π∗
e 0 ▷ initialize episode counter
←
while e<e do
max
if N <N or e<2 then ▷ sample high-fidelity data
pos thr
T GenerateCFDTrajectories(π ) ▷ run K simulations
←
θπ
M UpdateModelEnsemble(M,T) ▷ loss function (9)
←
N N ▷ all models trustworthy
pos m
←
else ▷ sample from model ensemble
T GenerateEnsembleTrajectories(π ,M) ▷ use algorithm 2 K times
←
θπ
N EvaluateModels(π ,M) ▷ algorithm 1, equation (10)
pos
←
θπ
end if
π UpdatePolicyNet(π ,v ,T) ▷ loss function (6)
θπ
←
θπ θv
v UpdateValueNet(v ,T) ▷ loss function (2)
θv
←
θv
end while
3 Results
WedemonstratetheMEPPOalgorithmontwoflowconfigurations,namelyarotating
cylinder in channel flow and the fluidic pinball. The numerical setups are described
in section 3.1. In section 3.2, we compare MF and MB trainings and investigate the
influenceofensemblesizeN andswitchingcriterionN .Section3.3presentsashort
m thr
discussion of the optimal policies found for each test case. The numerical simulations
are performed with the OpenFOAM-v2206 toolbox. The orchestration of simulations
and the DRL logic are implemented in the drlFoam package. Further implementation
details are available in the complementary code repository.
3.1 Flow control problems
3.1.1 Cylinder flow
The flow past a circular cylinder has become an established AFC benchmark. Orig-
inally, the setup was introduced by Sch¨afer et al. [24] as a benchmark for numerical
methods. Rabault et al. [17] were the first to adopt the setup for DRL-based AFC.
Since then, many variants with different means of actuation, sensor positions, and
Reynolds numbers have been investigated [7].
Thesetupusedinthepresentstudyisdepictedinfigure1.Theinletvelocityprofile
is parabolic [24], while the velocity vector at the upper and lower domain boundary
is zero. At the outlet boundary, the velocity gradient is set to zero. For the pressure,
a fixed reference value is applied at the outlet, while the gradient is set to zero on
all other boundaries. The Reynolds number based on the mean inlet velocity U , the
in
cylinder diameter d and the kinematic viscosity ν is Re=U d/ν =100.
in
The state (observation) used as input for control is formed by 12 pressure probes
placedinthecylinder’swake.Theflowisactuatedbyrotatingthecylinder.Thecontrol
aims to minimize the forces acting on the cylinder. Given the density-normalized
922d
wall
1.5d
ω
y d
wall
x
Fig. 1 AFCsetupoftheflowpastacylinder;basedon[17,18,24].
integral force vector F = [F ,F ]T, the corresponding force coefficients along each
x y
coordinate are:
2F 2F
x y
c = , c = , (11)
x U2A y U2A
in ref in ref
where the reference area is A = dz. Note that the setup is 2D, but 2D setups in
ref
OpenFOAM always have a single cell layer of depth z in the third direction. The
instantaneous reward at time step n is computed as [18]:
R =3 (c +0.1c ). (12)
n x,n y,n
− | |
Note that we do not use time or window-averaged coefficients but the instantaneous
values c . The magical constants in equation (12) simply scale the reward to a value
i,n
range near zero, which avoids additional tuning of PPO hyperparameters.
Given the convective time scale t = d/U , the control starts once the quasi-
conv in
steady state is reached, which is at 40t . The policy is queried once every 20
conv
numerical time steps, which corresponds to a control time step of ∆t = 0.1t .
c conv
Within the control interval, the angular velocity linearly transitions from ω to ω
n n+1
[18]. The value range of the normalized angular velocity, ω = ωd/U , is limited to
∗ in
ω [ 0.5,0.5]. Overall, N =400 experience tuples are generated within one trajec-
∗
∈ −
tory, which corresponds to a duration of 40t . The generation of a single trajectory
conv
takes approximately 4min on two MPI ranks.
3.1.2 Fluidic pinball
Noack et al. [25] introduced the fluidic pinball, which is a triangular configuration of
threerotatingcylinders.Theunforcedflowundergoesseveraldifferentvortexshedding
regimes, namely symmetric, asymmetric, and chaotic vortex shedding. The cylinder-
based Reynolds number of the present setup is Re=100, which places the flow in the
asymmetric vortex shedding regime.
The setup is depicted in figure 2. To reduce the initial transient simulation phase,
we apply a step function velocity profile at the inlet with ε = 0.01U . The same
in
10
telni
d6.1
teltuo
d1.422d
1.3d
ω
3
y
ω x
1
ω
2
6d
Fig. 2 AFCsetupofthefluidicpinball;basedon[25].
velocity vector is set on the lower and upper domain boundaries, i.e., 1.01U and
in
0.99U , respectively. The velocity gradient at the outlet boundary is set to zero.
in
Similar to the cylinder setup, the pressure is fixed at the outlet, and the pressure
gradient on all other boundaries is set to zero.
A total of 14 pressure sensors form the state. The positions depicted in figure 2
are loosely inspired by a preliminary optimal sensor placement study [9]. The precise
numericalcoordinatesofeachsensorcanbeinferredfromthesetupfilesinthecomple-
mentary code repository. The policy parametrizes a trivariate Beta distribution over
the angular velocities ω , i 1,2,3 . As the control objective, we aim to minimize
i
∈ { }
thecumulativeforcesactingonallthreecylinders.Giventhedensity-normalizedinte-
gralforceF =[F ,F ]T oftheithcylinder,thecorrespondingforcecoefficientsare
i x,i y,i
defined as:
2F 2F
x,i y,i
c = , c = , (13)
x,i U2A y,i U2A
in ref in ref
where the reference area is the projected area of all three cylinders in x-direction,
namely A =2.5dz. Definition (13) ensures that the sum over all cylinders recovers
ref
the correct total force coefficients:
3 3
c = c , c = c . (14)
x x,i y y,i
i=1 i=1
(cid:88) (cid:88)
Based on the cumulative force coefficients, the instantaneous reward is computed as:
R =1.5 (c +0.5c ). (15)
n x,n y,n
− | |
11
telni
−
Uin
ε
Uin
+ε
d
d5.1
d6
d6
teltuoTherewarddefinitionisverysimilartotheoneusedforthesinglecylinder.Notethat
there is no particular reason for this definition other than that it was employed in a
previousstudy[19].Alternatively,wecouldhavealsosummedupthemagnitudesofthe
individual force coefficients. However, the focus of this work is on the demonstration
of MBDRL. The magical constants in equation 15 have the same purpose as before,
namely normalizing the reward values.
Keeping the same definition of the convective time scale as before, the control
starts at the end of the initial transient regime, which is at 200t . The policy is
conv
queried every 50 numerical time steps, corresponding to a control interval of ∆t =
c
0.5t .Thetrajectoryextendsover100t suchthatN =200experiencetuplesare
conv conv
generated. The dimensionless angular velocities are limited to the range ω [ 5,5].
i∗
∈ −
Generating a single trajectory with eight MPI ranks requires approximately 40min.
The main parameters relevant to AFC are summarized in table 1 for both test cases.
Table 1 CharacteristicparametersofthetwoAFCsetups;ranks referstothenumber
ofMPIranks;CV isthenumberofcontrolvolumes,andTtr isthetimerequiredto
executeonesimulation(tosampleonetrajectory).
case Re tconv ∆tc/tconv ω∗ range N ranks CV Ttr
cylinder 100 0.1s 0.1 [−0.5,0.5] 400 2 5.3×103 ≈4min
pinball 100 1s 0.5 [−5,5] 200 8 5.3×104 ≈40min
3.2 Training performance
3.2.1 General remarks about the training
We compare the control performance and the computational cost between MF and
MB trainings. Each training is executed in isolation on a dedicated compute node
with 96 CPU cores. To discuss the schematics presented later on, it is necessary to
outline several implementation details. In each episode, exactly ten trajectories are
generated in parallel before updating the policy and value networks. The buffer size
and the parallel trajectory generation are the same for CFD and MB trajectories. We
performaveryroughcheckoftheforcecoefficientsintheMBtrajectoriesanddiscard
trajectoriesthatareobviouslyextremelyinaccurate.Specifically,weonlykeepanMB
trajectory for the cylinder flow if all coefficients fulfill the criteria 2.85 c 3.5
x
≤ ≤
and c 1.3. The analogous bounds for pinball trajectories are 3.5 c
y x,i
| | ≤ − ≤ ≤
3.5 and c 3.5. Note that these bounds are really generous and filter out only
y,i
| | ≤
extreme prediction errors. If one or more trajectories are discarded, the sampling is
repeated until ten valid trajectories are available. The training is stopped after 200
and 150 episodes for the cylinder and pinball cases, respectively. Finally, we note that
all parameter configurations for the cylinder flow are repeated with five different seed
values. All results presented in section 3.2.2 are averaged over these five runs. Due to
thecomputationalcostofthefluidicpinball,weexecuteonlyasingletrainingrunper
parameter configuration.
123.2.2 Cylinder flow
Figure 3 shows the episode-wise mean rewards received with different training con-
figurations. Within 200 episodes almost all configurations achieve a similar maximum
reward. The maximum reward in the MF training is slightly lower, but the reward
still keeps increasing moderately toward the end of the training. Surprisingly, all MB
trainings reach the maximum reward earlier than the MF training. We noticed that
the MB trajectories display less small-scale variance than the CFD trajectories. Pre-
sumably,thisindirectfiltering performedbytheenvironmentmodelsaffectsthepolicy
and value network updates positively. In general, The MB trainings display a higher
varianceinthemeanreward,especiallywithinthefirst50episodes,wherethechanges
intherewardarethestrongest.ThisbehaviorisrelatedtoindividualMBtrajectories
withpoorpredictionquality.Luckily,thesetrajectoriesseemtohavealimitedimpact
on the learning.
model-free N = 1 N = 5 N = 10
m m m
0.0
0.1
−
0.2
− N =3 N =5
thr thr
N =2 N =3
0.3 thr thr
−
0 100 200 0 100 200 0 100 200 0 100 200
e
Fig. 3 Cylinderflow:episode-wisemeanrewardR fordifferenttrainingconfigurations;theshaded
area encloses one standard deviation below and above the mean; mean and standard deviation are
computed over all trajectories of all seeds; for the MB training, the markers indicate CFD-based
trajectorysampling.
ThemarkersintheMBtraininginfigure3indicateCFDtrajectories.FortheMB
training with one model, we switch back to simulation-based sampling every fourth
episode, so 75% of all episodes use the environment model. The same ratio results
in the MEPPO training with N = 5 and N = 3. Interestingly, the automated
m thr
switching between CFD and model sampling leads to a relatively regular alternation.
The same number of models with a lowered threshold value of N = 3 reduces
thr
the amount of CFD episodes to 15%, notably without performance degradation. The
training with N = 10 and N = 5 also employs the model ensemble in 75% of
m thr
the episodes. Reducing the threshold value in the latter case to N = 3 reduces
thr
the number of CFD episodes to 6%. However, the optimization becomes significantly
less stable and does not reach optimal control performance. Note that the amount
of training data per model decreases as the number of models increases. Therefore,
13
Rincreasing the number of sampled trajectories per episode or allowing overlapping
training data could potentially stabilize trainings with a lowered threshold value. The
increased amount of training data when employing only a single model could explain
thequickconvergenceofthecorrespondingtrainingafterapproximately100episodes.
CFD sampling model update other
MB sampling PPO update
0.35
7.08%
9.15%
0.30 9.52%
8.94%
28.50% 11.93%
11.72%
0.25
9.75%
0.20
17.76%
0.15
11.29% 79.71%
76.41%
62.32% 75.84%
0.10
36.63%
67.21% 7.17%
0.05
49.86%
0.00
N =1 N =5 N =5 N =10 N =10 N =10
m m m m m m
N =3 N =2 N =6 N =5 N =3
thr thr thr thr thr
Fig. 4 Cylinder flow: composition of the total MB training time TMB normalized with the MF
trainingtimeTMF.
Assuming that the creation of environment models is reliable and much less costly
thanaCFDsimulation,themoreCFDepisodescanbereplacedwithMBepisodes,the
lower the overall training time. This trend is reflected by the normalized MB training
time depicted in figure 4. Remarkably, all configurations reduce the training time by
atleast65%,eventhoughthecomputationalcosttosimulatethecylinderflowisfairly
small.Tobetterunderstanddifferencesintheoveralltrainingtime,figure4alsoshows
the time spent on the most relevant parts of the training. For all configurations, the
simulation runs remain the most time-consuming element. The time spent updating
the networks for environment, policy, and value function remains fairly constant.
A noticeable increase can be observed in the time spent to sample trajectories
fromasingleenvironmentmodel.Thisincrease,aswellasothervariationsinthesam-
plingtime,arerelatedtotheremovalofobviouslycorruptedtrajectories,asexplained
in section 3.2.1. Figure 5 shows the number of discarded trajectories per training.
Clearly, the ensemble models lead to a significantly more robust trajectory sampling.
Moreover, ensembles with more models produce fewer invalid samples. It is notewor-
thy to mention that the majority of invalid trajectories result in the initial training
phase, e < 50, presumably because of the strong exploration and policy changes. In
14
T/
T
FM
BMour implementation, trajectories are sampled in parallel. However, the constraint of
executing the PPO update with exactly ten valid trajectories leads to repeated sam-
pling in the case of discarded trajectories. Of course, this extra time could be avoided
bysamplingmoretrajectoriesthanrequiredinparallelorbylooseningtheconstraint.
60
40
20
0
N =1 N =5 N =5 N =10 N =10 N =10
m m m m m m
N =3 N =2 N =6 N =5 N =3
thr thr thr thr thr
Fig. 5 Cylinder flow: number of discarded trajectories N d per training; the box plot shows the
outcomeoffiveindependentruns(seedvalues);theorangelineindicatesthemedianvalue,andthe
markersindicateextremeoutcomes.
We note that there are also small variations in the measured execution times that
arenotstraightforwardtoexplain.Onemeanstoreducethesevariationswouldbeexe-
cutingadditionaltrainingrunswithvaryingseeds.However,themaintrendsinterms
of control performance and training time are clearly visible and explainable. More-
over, there are nontrivial coupling effects between simulations, environment models,
and PPO updates. As discussed before, policy updates differ between MF and MB
trainings and also depend to some degree on ensemble size and switching criterion.
The policy employed in a simulation impacts the execution time, e.g., by subsequent
changes in the pressure-velocity-coupling iterations, or the linear solver iterations.
3.2.3 Fluidic pinball
Due to the computational cost of the pinball setup, we do not perform multiple train-
ing runs and investigate fewer hyperparameter configurations. Figure 6 shows the
episode-wise mean rewards of all trainings. The final reward is similar for all tested
configurations. The MF training reaches the maximum reward after approximately
60 episodes. Thereafter, the control performance drops slightly. The highest overall
reward is achieved by the MB training with N = 10. For the MB training with a
m
singlemodel,weswitcheveryfourthepisodetoCFDtrajectories.Alsotheautomated
switching criterion in the training with N = 10 and N = 5 leads to 25% CFD
m thr
episodes. The smaller ensemble with five models switches approximately every fifth
episode back to the high-fidelity samples.
Figure 7 shows the main contributions to the normalized training time. Even
though the relative amount of model trajectories is similar to the cylinder flow, the
15
N
dmodel-free N = 1 N = 5 N = 10
m m m
1.0
0.5
0.0
N = 2 N = 5
0.5 thr thr
−
1.0
−
0 50 100 0 50 100 0 50 100 0 50 100
e
Fig. 6 Fluidicpinball:episode-wisemeanrewardRfordifferenttrainingconfigurations;theshaded
area encloses one standard deviation below and above the mean; mean and standard deviation are
computedoveralltrajectoriesofseeds;fortheMBtraining,themarkersindicateCFD-basedtrajec-
torysampling.
runtimereductionisevenmoresignificantduetothehighercomputationalcostofper-
formingthepinballsimulation.Remarkably,allconfigurationsreducethetrainingtime
byatleast80%.TheremainingCFDepisodesdominatetheoveralltimeconsumption.
As for the cylinder test case, the time spent on sampling model trajectories increases
when using only a single model. Ten trajectories were discarded during the training
and had to be re-sampled. In the MEPPO runs, no invalid trajectories occurred. The
increased time spent on simulations in the training with ten models is a result of the
non-trivial interaction between policy learning and the numerical properties of the
simulation, as discussed before.
3.3 Analysis of the best policies
3.3.1 Cylinder flow
Inthissection,wecomparethefinalpoliciesoftheMFandMBtrainingsthatachieved
thehighestmeanreward.Notethattheachievedforcereductionandthecorresponding
control are by no means new [7]. However, we would like to point out a few subtle
differences in the final MF and MB policies. It is noteworthy that all MB policies
achievedasimilarforcereduction,eventhoughwepresentonlyresultsfortheMEPPO
training with ten models.
Figure 8 shows the optimal control and the corresponding force coefficients. Both
MF and MB policies reduce the drag force by approximately 7%. The second force
coefficient remains near zero after 20 convective time units. Both the MF and MB
policies perform a very similar actuation within the first five convective time units.
Thereafter, the MB policy rotates the cylinder slightly less and archives an almost
perfectly steady force balance. Small fluctuations remain in the angular velocity and
the side force when employing the MF policy.
16
RCFD sampling model update other
MB sampling PPO update
0.20
8.26%
0.15
0.10 95.19%
88.18%
94.49%
0.05
0.00
N =1 N =5 N =10
m m m
N =2 N =5
thr thr
Fig. 7 Fluidic pinball: composition of the total MB training time TMB normalized with the MF
trainingtimeTMF.
The cylinder rotation steadies and extends the wake. The pressure drop over the
cylinder decreases, as can be inferred visually in figure 9. The figure also shows the
significant suppression of vortex shedding. While small pressure fluctuations remain
whenemployingtheMFpolicy,theMBpoliciesleadstoanalmostperfectlysteadyflow
field.ItisconceivablethattheMFtrainingcouldachieveasimilarcontrolperformance
withmoretrainingepisodesandadditionalPPOhyperparametertuning.However,the
MB runs achieve the presented control performance with even less than 200 episodes.
3.3.2 Fluidic pinball
Several established control mechanisms exist for the fluidic pinball, e.g., boat tailing
and base bleeding [26]. The strategy learned by the policies in the present work is
referred to as boat tailing. Figure 10 compares the control and the resulting force
coefficients of the best-performing MF and MB policies. Both policies follow a similar
strategy, i.e., rotating cylinders 2 and 3 with maximum speed in opposite directions
while keeping the cylinder in the front nearly still. This actuation leads to boat tail-
ing and reduces the drag coefficient by approximately 87%. Of course, the reduction
strongly depends on the allowed maximum rate of rotation and would be smaller for
smaller rates of rotation.
While the drag reduction is fairly similar, the MF policy introduces a nonzero net
force in y-direction. This asymmetry is caused by a subtle difference in the absolute
values of ω and ω , i.e., the absolute value of ω is marginally larger. The same
2∗ 3∗ 2∗
small difference between ω and ω is present in the MB policy. However, the MB
2∗ 3∗
17
T/
T
FM
BMuncontrolled MB, N =10, N =5 MF
m thr
3.5
3.0
controlled
2.5 →
1
0
1
−0.5
0.0
0.5
−
30 40 50 60 70 80 90 100
t
∗
Fig. 8 Cylinder flow: angular velocity and force coefficients resulting from the best MF and MB
policies;thedimensionlesstimeist∗=t/tconv.
Fig.9 Temporalmeanandstandarddeviationofthepressurefieldswithandwithoutcontrol;both
meanandstandarddeviationarenormalizedwiththeminimum(blue)andmaximum(yellow)values
oftheuncontrolledcase;thecoordinatesarenormalizedwiththediameter.
18
ω ∗
xc
ycpolicy applies a small positive spin to the cylinder in the front, which balances the
sideforcestonetzero.Thereadermightbewonderingifthesamedragreductionand
net zero force in y-direction could be achieved by setting ω = 0 and ω = ω = 5.
1∗ 2∗
−
3∗
The short answer is yes. However, the latter control leads to relatively strong c
y
fluctuations whose amplitude decays only slowly. Approximately 150 convective time
units are necessary to reach the steady state with open-loop control, which is longer
thantheprescribedtrajectorylengthemployedinthetraining.Theasymmetriccontrol
performed by the MB policy achieves the same force reduction in about 20 convective
time units.
uncontrolled MB, N =10, N =5 MF
m thr
2 controlled
→ 1
0
0.5
0.0
0.5
− 5 ω 2∗
0 ω 1∗
ω
5 3∗
−
200 210 220 230 240 250
t
∗
Fig.10 Fluidicpinball:angularvelocitiesandsummedforcecoefficientsresultingfromthebestMF
andMBpolicies;thedimensionlesstimeist∗=t/tconv.
Finally, we want to shed some light on the local flow dynamics created by the
spinningcylinders.Figure11showsthevelocityfieldinthevicinityofthecylinders.As
mentionedbefore,theuncontrolledstateisasymmetric,ascanbeobservedbylooking
at the wakes of the two cylinders in the back. In the controlled flow, the rotation of
cylinderstwoandthreepushesfluidbetweentheminupstreamdirection.Thissuction
significantly reduces the extent of the wake and the cumulative pressure drop over
the cylinders. The fluid then passes through the gaps between the rear cylinders and
the front cylinder. For the MF policy, the fluxes in positive and negative y-direction
are fairly balanced. Instead, the MB policy hinders the flux between cylinders one
and two and diverts more fluid to the gap between cylinders one and three. This
small imbalance is enough to compensate for the different rotation speeds of the rear
cylinders.
4 Conclusion
DRL-basedAFCisapromisingapproachtoenablesmartcontroltechnology.Acurrent
limitation of simulation-based DRL is the high computational cost and turnaround
19
c
i,y
i
ω
c
∗i
i,x
i
P
PFig. 11 Comparison of instantaneous velocity fields with and without control; the velocity field is
normalizedwiththeinletvelocity.
timeassociatedwiththeoptimization.TheideaofMBDRListhecreationoflow-cost
environment models that partially replace the costly sampling of high-fidelity trajec-
tories from simulations. On two common benchmark AFC problems, we demonstrate
that the MEPPO adopted here can tremendously reduce the training cost and time
while achieving optimal control performance. The relative reduction of training time
increases with the cost of the CFD simulation. Therefore, we expect the MEPPO
algorithm, or variants thereof, to be a key enabler in performing DRL-based AFC on
realistic, 3D simulations at an industrial scale.
There are a number of promising options to reduce the training cost even further.
Creating accurate auto-regressive environment models on the fly posed a significant
challenge. Automating the model creation would be extremely beneficial in the appli-
cation of MBDRL to new problems. The more accurate the models can extrapolate,
the more savings are possible. More advanced recurrent network architectures or
transformers [27] might be capable of achieving higher accuracy than the simple,
fully connected networks employed here. However, such advanced networks are typi-
cally also harder to train. Alternatively, more straightforward approaches to creating
reduced-order models might be worthwhile to explore, too. For example, dynamic
mode decomposition with control [28] could be a drop-in replacement for the models
employed here.
We also noticed a strong dependency of the learning progress on the PPO hyper-
parameters. Due to the high training cost, automated tuning of these parameters
is infeasible for complex simulations. However, recent improvements in the PPO
algorithm, e.g., the PPO-CMA variant [29], reduce the number of hyperparameters,
20improve robustness, and accelerate learning. Such improvements would be extremely
beneficial both for MF and MB learning.
21Supplementary information.
Acknowledgements. The authors gratefully acknowledge the Deutsche
Forschungsgemeinschaft DFG (German Research Foundation) for funding this work
in the framework of the research unit FOR 2895 under the grant WE 6948/1-1. The
computational resources to conduct the numerical experiments were kindly provided
by Amazon Web Services. Finally, the authors gratefully acknowledge the organizing
committee of the 18th OpenFOAM Workshop for the support provided during the
preparation of this manuscript.
Declarations
The authors have no conflict of interest to declare.
Appendix A Environment models
We use a standard deep learning workflow to train the environment models. The
differentoptimizationtechniquesaredescribedindetailbyRaff[27].TableA1provides
anoverviewofthemostimportanthyperparameters.Sincetheamountoftrainingdata
is fairly small, we employ 8 CPU cores to train a single model. Note that the models
predict the individual force coefficients rather than the full reward. Consequently,
the output layer has 12+2 and 14+6 neurons for the cylinder and pinball test cases,
respectively. The training is stopped once the maximum number of epochs is reached,
the absolute loss value falls below a threshold value, or the relative change of the loss
averaged over 40 epochs becomes too small.
Table A1 Hyperparametersforthetrainingoftheenvironmentmodels.
parameter cylinder pinball
hiddenlayers 3 3
layernormalization LayerNorm LayerNorm
neuronsperhiddenlayer 100 100
timedelaysd 30 30
inputneurons 450 690
outputneurons 14 20
activationfunction leakyReLU leakyReLU
max.numberofepochs 2500 2500
batchsize 25 25
train/validationsplit 75/25% 75/25%
absolutestoppingloss 10−6 10−6
relativestoppingloss 10−7 10−7
lossfunction MSELoss MSELoss
optimizer AdamW AdamW
learningrateschedule ReduceLROnPlateau ReduceLROnPlateau
init./min.learningrate 10−2/10−4 10−2/10−4
22Appendix B PPO hyperparameters
MostPPOhyperparametersemployedherearestandardvaluessuggestedinthelitera-
ture[21,23].Incontrasttotheoriginalimplementation,weusetwoseparatenetworks
forpolicyandvaluefunction.Thetwonetworksareoptimizedsequentiallybytwodif-
ferentoptimizers.Sincetheoptimizationisinexpensive,weallocateonly5CPUcores.
As suggested in [23], the policy optimization is stopped if the difference between old
and new policy, measured in terms of KL divergence, exceeds a prescribed threshold.
However, we noticed that this criterion is rarely triggered.
Table B2 PPOhyperparameters.
parameter cylinder pinball
hiddenlayers 2 2
neuronsperhiddenlayer 64 512
inputneurons 12 14
outputneurons 2 6
activationfunction ReLU ReLU
discountfactorγ 0.99 0.99
advantagesmoothingλ 0.97 0.97
learningratepolicynet. 10−3 10−5
learningratevaluenet. 5×10−4 10−5
max.numberofepochs 100 100
batchsize full full
valueandpolicyclipping 0.1 0.1
optimizer AdamW AdamW
KLdivergence 0.2 0.2
entropyweightβ 0.01 0.01
References
[1] IPCC: Climate Change 2022: Impacts, Adaptation and Vulnerability. Summary
for Policymakers, pp. 3–33. Cambridge University Press, Cambridge, UK and
New York, USA (2022)
[2] Seifert, A., Shtendel, T., Dolgopyat, D.: From lab to full scale active flow control
drag reduction: How to bridge the gap? Journal of Wind Engineering and Indus-
trialAerodynamics147,262–272(2015)https://doi.org/10.1016/j.jweia.2015.09.
012
[3] Rohrer, T., Frison, L., Kaupenjohann, L., Scharf, K., Hergenr¨other, E.: Deep
reinforcement learning for heat pump control. In: Arai, K. (ed.) Intelligent
Computing, pp. 459–471. Springer, Cham (2023). https://doi.org/10.1007/
978-3-031-37717-4 29
23[4] Esche, E., Repke, J.-U.: Dynamic process operation under demand response – a
reviewofmethodsandtools.ChemieIngenieurTechnik92(12),1898–1909(2020)
https://doi.org/10.1002/cite.202000091
[5] Hucho, W., Sovran, G.: Aerodynamics of road vehicles. Annual Review of Fluid
Mechanics 25(1), 485–537 (1993) https://doi.org/10.1146/annurev.fl.25.010193.
002413
[6] Choi, H., Lee, J., Park, H.: Aerodynamics of heavy vehicles. Annual
Review of Fluid Mechanics 46(1), 441–468 (2014) https://doi.org/10.1146/
annurev-fluid-011212-140616
[7] Viquerat,J.,Meliga,P.,Larcher,A.,Hachem,E.:Areviewondeepreinforcement
learning for fluid mechanics: An update. Physics of Fluids 34(11), 111301 (2022)
https://doi.org/10.1063/5.0128446
[8] Paris, R., Beneddine, S., Dandois, J.: Robust flow control and optimal sensor
placementusingdeepreinforcementlearning.JournalofFluidMechanics913,25
(2021) https://doi.org/10.1017/jfm.2020.1170
[9] Krogmann, T.: Optimal sensor placement for active flow control with deep
reinforcement learning (2023). https://doi.org/10.5281/zenodo.7636959
[10] Paris, R., Beneddine, S., Dandois, J.: Reinforcement-learning-based actuator
selectionmethodforactiveflowcontrol.JournalofFluidMechanics955,8(2023)
https://doi.org/10.1017/jfm.2022.1043
[11] Ashton,N.,West,A.,Lardeau,S.,Revell,A.:Assessmentofransanddesmethods
for realistic automotive models. Computers and Fluids 128, 1–15 (2016) https:
//doi.org/10.1016/j.compfluid.2016.01.008
[12] Belus,V.,Rabault,J.,Viquerat,J.,Che,Z.,Hachem,E.,Reglade,U.:Exploiting
locality and translational invariance to design effective deep reinforcement learn-
ingcontrolofthe1-dimensionalunstablefallingliquidfilm.AIPAdvances9(12),
125014 (2019) https://doi.org/10.1063/1.5132378
[13] Vignon, C., Rabault, J., Vasanth, J., Alc´antara-A´vila, F., Mortensen, M.,
Vinuesa, R.: Effective control of two-dimensional Rayleigh–B´enard convection:
Invariant multi-agent reinforcement learning is all you need. Physics of Fluids
35(6), 065146 (2023) https://doi.org/10.1063/5.0153181
[14] Dixit, A., Elsheikh, A.H.: Robust optimal well control using an adaptive multi-
gridreinforcementlearningframework.MathematicalGeosciences55(3),345–375
(2023) https://doi.org/10.1007/s11004-022-10033-x
[15] Moerland,T.M.,Broekens,J.,Jonker,C.M.:Model-basedreinforcementlearning:
A survey. CoRR abs/2006.16712 (2020) 2006.16712
24[16] Kurutach,T.,Clavera,I.,Duan,Y.,Tamar,A.,Abbeel,P.:Model-ensembletrust-
region policy optimization. CoRR abs/1802.10592 (2018) 1802.10592
[17] Rabault, J., Kuchta, M., Jensen, A., R´eglade, U., Cerardi, N.: Artificial neural
networks trained through deep reinforcement learning discover control strategies
for active flow control. Journal of Fluid Mechanics 865, 281–302 (2019) https:
//doi.org/10.1017/jfm.2019.62
[18] Tokarev, M.,Palkin, E.,Mullyadzhanov, R.:Deep reinforcement learningcontrol
ofcylinderflowusingrotaryoscillationsatlowreynoldsnumber.Energies13(22),
5920 (2020) https://doi.org/10.3390/en13225920
[19] Holm, M.: Using reinforcement learning for active flow control. DUO Research
Archive (2020). http://hdl.handle.net/10852/79212
[20] Sutton, R.S., Barto, A.G.: Reinforcement Learning, Second Edition: An Intro-
duction. Adaptive Computation and Machine Learning series. MIT Press, ???
(2018). https://books.google.de/books?id=5s-MEAAAQBAJ
[21] Schulman,J.,Wolski,F.,Dhariwal,P.,Radford,A.,Klimov,O.:Proximalpolicy
optimization algorithms. CoRR abs/1707.06347 (2017) 1707.06347
[22] Andrychowicz,M.,Raichuk,A.,Stanczyk,P.M.,Orsini,M.,Girgin,S.,Marinier,
R.,Hussenot,L.,Geist,M.,Pietquin,O.,Michalski,M.,Gelly,S.,Bachem,O.F.:
What matters for on-policy deep actor-critic methods? a large-scale study. In:
ICLR (2021). https://openreview.net/pdf?id=nIAxjsniDzg
[23] Morales,M.:GrokkingDeepReinforcementLearning.ManningPublications,???
(2020). https://books.google.de/books?id=IpHJzAEACAAJ
[24] Sch¨afer, M., Turek, S., Durst, F., Krause, E., Rannacher, R.: In: Hirschel, E.H.
(ed.)BenchmarkComputationsofLaminarFlowAroundaCylinder,pp.547–566.
Vieweg+Teubner Verlag, Wiesbaden (1996)
[25] Noack, B.R., Stankiewicz, W., Morzyn´ski, M., Schmid, P.J.: Recursive dynamic
mode decomposition of transient and post-transient wake flows. Journal of Fluid
Mechanics 809, 843–872 (2016) https://doi.org/10.1017/jfm.2016.678
[26] Raibaudo,C.,Zhong,P.,Noack,B.R.,Martinuzzi,R.J.:Machinelearningstrate-
gies applied to the control of a fluidic pinball. Physics of Fluids 32(1), 015108
(2020) https://doi.org/10.1063/1.5127202
[27] Raff, E.: Inside Deep Learning: Math, Algorithms, Models. Manning, ??? (2022).
https://books.google.de/books?id=s8hhzgEACAAJ
[28] Proctor, J.L., Brunton, S.L., Kutz, J.N.: Dynamic mode decomposition with
control. SIAM Journal on Applied Dynamical Systems 15(1), 142–161 (2016)
25https://doi.org/10.1137/15M1013857
[29] H¨am¨al¨ainen, P., Babadi, A., Ma, X., Lehtinen, J.: Ppo-cma: Proximal policy
optimizationwithcovariancematrixadaptation.In:2020IEEE30thInternational
Workshop on Machine Learning for Signal Processing (MLSP), pp. 1–6 (2020).
https://doi.org/10.1109/MLSP49062.2020.9231618
26