Self Supervised Correlation-based Permutations for Multi-View Clustering
RanEisenberg*1 JonathanSvirsky*1 OfirLindenbaum1
Abstract informationfrommultipledatafacets(orviews)toobtain
amorecomprehensiveandaccurateunderstandingofthe
Fusinginformationfromdifferentmodalitiescan
underlyingdatastructures. Eachviewmaycapturedistinct
enhance data analysis tasks, including cluster-
aspectsorfacetsofthedata,andbyintegratingthem,we
ing. However, existing multi-view clustering
candiscoverhiddenpatternsandrelationshipsthatmightbe
(MVC)solutionsarelimitedtospecificdomains
obscuredinanysingleview[20,21]. Thisapproachholds
orrelyonasuboptimalandcomputationallyde-
immensepotentialinvariousapplications,frommultime-
manding two-stage procedure of representation
diaanalysis[22]andbioinformatics[23]tosocialnetwork
andclustering. Weproposeanend-to-enddeep
analysis[24]andgeophysics[25,26].
learning-basedMVCframeworkforgeneraldata
(image, tabular, etc.). Our approach involves ExistingMVCmethodscanbedividedintotraditional(non-
learning meaningful fused data representations deep)anddeeplearning-basedmethods. TraditionalMVC
withanovelpermutation-basedcanonicalcorrela- methods include: subspace methods [27, 28, 29], matrix
tionobjective. Concurrently,welearnclusteras- factorizationmethods[30,31,32],andgraphmethods[33,
signmentsbyidentifyingconsistentpseudo-labels 34,35]. Themaindrawbacksofthetraditionalmethodsare
acrossmultipleviews. Wedemonstratetheeffec- poorrepresentationability, highcomputationcomplexity,
tivenessofourmodelusingtenMVCbenchmark andoftenlimitedperformanceinreal-worlddata[36].
datasets. Theoretically,weshowthatourmodel
Recently,severaldeeplearning-basedMVCschemeshave
approximatesthesupervisedlineardiscrimination
demonstratedpromisingrepresentationandclusteringcapa-
analysis(LDA)representation. Additionally,we
bilities[37,38,39,40,41,42,43]. Mostofthesemethods
provideanerrorboundinducedbyfalse-pseudo
adoptatwo-stageapproach,wheretheyfirstlearnrepresen-
labelannotations.
tations, followed by clustering, as seen in works such as
[44,43,37,38,39,27,45,46,47]. However,suchatwo-
stageprocedurecanbecomputationallyexpensiveanddoes
1.Introduction
not directly update the model’s weights based on cluster
Clustering is a crucial technique in data-driven scientific assignments;therefore,itmayleadtosuboptimalresults.
discoverythathelpstocategorizesamplesintogroupsbased
Afewstudiespresentedanend-to-endschemeforMVrep-
onsemanticrelationships. Thisallowsforadeeperunder-
resentation learning and clustering [48, 49, 50, 51]). By
standingofcomplexdatasetsandisusedinvariousdomains
performingbothtaskssimultaneously,MVCcanimprove
suchasgeneexpressionanalysisinBioinformatics[1],ef-
thedataembeddingsbymakingthemmoresuitedforcluster
ficientcategorizingoflarge-scalemedicalimages[2],and
assignments. However,themulti-viewfusionprocessused
incolliderphysics[3]. Existingclusteringapproachescan
in these studies may only be adaptable to some types of
bebroadlycategorizedascentroid-based[4,5,6],density-
data,whichcanlimittheirgeneralizationcapabilitiesacross
based[7,8,9,10],distribution-based[11,12],hierarchical
awiderangeofdatasets.
[13,14]anddeeplearningmodel[15,16]. Multi-viewclus-
teringisanextensionoftheclusteringparadigmthatsimul- We have introduced a new approach called COrrelation-
taneouslyleveragesdiverseviewsofthesameobservations based PERmutations (COPER) that aims to address the
[17,18,19]. mainchallengesofmulti-viewclustering. Ourdeeplearn-
ing model combines clustering and representation tasks,
Themainideabehindmulti-viewclusteringistocombine
providing an end-to-end MVC framework for fusion and
*Equal contribution 1Faculty of Engineering, Bar-Ilan clustering. Thiseliminatestheneedforanadditionalstep.
University, Israel. Correspondence to: Ran Eisenberg Theapproachinvolveslearningdatarepresentationswitha
<eisenbr2@biu.ac.il>, Jonathan Svirsky <svirskj@biu.ac.il>, novelself-supervisiontask. Inthistask,inter-class(within
OfirLindenbaum<ofir.lindenbaum@biu.ac.il>.
class) pseudo-labels are permuted (PER) across different
views for canonical correlation (CO) analysis loss. The
Copyright2024bytheauthor(s).
1
4202
beF
62
]GL.sc[
1v38361.2042:viXraSelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
proposedframeworkmaximizesintra-class(betweenclass) mains,includingbiology[65],neuroscience[66],medicine
varianceandminimizesinter-classvariationintheshared [67],andengineering[68].
embeddedspace. Undermildassumptions,wedemonstrate
ThemaingoalofCCAistofindlinearcombinationsofvari-
thatourmodelapproximatesthesameprojectionthatwould
ablesfromeachview,aimingtomaximizetheircorrelation.
havebeenachievedbythe(supervised)lineardiscriminant
Formally,denotingtheobservationsasX(1) ∈RD1×N and
analysis(LDA)method[52].
X(2) ∈RD2×N,wherebothmodalitiesarecenteredanden-
Ourmaincontributionsaresummarizedasfollows: (i)De- compassN sampleswithD andD attributes,respectively.
1 2
velopadeeplearningmodelthatexploitsself-supervision CCAseeksforcanonicalvectorsa ∈ RD1 andb ∈ RD2
andaCCA-basedobjectiveforend-to-endMVC.(ii)Present suchthatu=aTX(1) andv =bTX(2). Theobjectiveis
amulti-viewpseudo-labelingprocedureforidentifyingcon- tomaximizecorrelationsbetweenthesecanonicalvariates,
sistent labels across views. (iii) Demonstrate empirically asrepresentedbythefollowingoptimization:
and theoretically that within cluster permutation can im-
aTX(1)(X(2))Tb
provetheusabilityofCCA-basedrepresentationsforMVC.
max ρ(u,v)= .
(cid:113) (cid:113)
(iv)Analyzetherelationbetweenthesolutionofournew a,b̸=0 aTX(X(1))Ta bTX(2)(X(2))Tb
permutation-basedCCAprocedureandthesolutionofLDA.
(v)Conductanextensiveexperimentalevaluationdemon- Thesecanonicalvectorscanbefoundbysolvingthegener-
stratingourproposedmodel’ssuperiorityoverthestate-of- alizedeigenpairproblem
the-artdeepMVCmodels.
C−1C C−1C a = λa
1 12 2 21
2.RelatedWork C 2−1C 21C 1−1C 12b = λb (1)
whereC ,C arewithinviewsamplecovariancematrices
Several existing MVC methods incorporate Deep Canon- 1 2
andC ,C arecross-viewsamplecovariancematrices.
ical correlation analysis (DCCA) during their represen- 12 21
tation learning phase. These methods obtain a useful Various extensions of CCA have been proposed to study
representation by transforming multiple views into max- non-linearrelationshipsbetweentheobservedmodalities.
imallycorrelatedembeddingsusingnonlineartransforma- Some kernel-based methods, such as Kernel CCA [57],
tions[53,54,27,47]. However,thesemethodsuseasubop- Non-parametricCCA[58],andMulti-viewDiffusionmaps
timaltwo-stageprocedure,wheretheclusteringschemeis [60,61],explorenon-linearconnectionswithinreproducing
appliedtotherepresentationlearnedthroughtheCCA-based Hilbertspaces. However,thesemethodsarelimitedbypre-
objective. defined kernels, have restricted interpolation capabilities,
anddonotscalewellwithlargedatasets.
Afewend-to-end,multi-viewDCCA-basedclusteringso-
lutionshavebeenproposed;theseinclude[48,50],which Toovercometheselimitations[62]introducesDeepCCA
updatetheirrepresentationstoimprovetheclusteringcapa- (DCCA),whichextendstraditionalCCAbyleveragingdeep
bilities. Our model also falls under the same category of neuralnetworkstolearnnon-linearmappingsbetweenthe
end-to-endrepresentationlearningandclustering. However, inputmodalitiesautomatically. Thisenablesmoreflexible
wehaveintroducedsomenewelementsthatmakeourap- and scalable modeling of complex relationships in large
proachdistinctfromexistingschemes. Theseincludeanew datasets. InSection4.3,wedescribehowweincorporatea
self-supervisedpermutationprocedurethatenhancestherep- DCCAobjectivetoembedthemulti-viewdata.
resentationandamulti-viewpseudo-labelselectionscheme.
Empiricalresultsdemonstratethatthesenewcomponents
3.2.Self-SupervisionforClustering
significantlyimproveclusteringcapabilities,presentedin
Section6. Self-supervisedlearningisatechniqueforlearningmeaning-
fuldatarepresentationswithoutlabeledobservations. The
mainideaistoleverageinformationpresentinunlabeled
3.Background
samplestocreateataskthatdoesnotrequireanymanual
3.1.CanonicalCorrelationAnalysis(CCA) annotations. Inclusteringtasks,self-supervisionimproves
datarepresentationlearningbyassigningpseudo-labelsto
CanonicalCorrelationAnalysis(CCA)[55,56]isawell-
unlabeleddatabasedonsemanticsimilaritiesbetweensam-
celebratedstatisticalframeworkformulti-view/modalrepre-
ples. These pseudo-labels are then used to improve data
sentationlearning. CCAcanhelpanalyzetheassociations
representationsthroughlearningtasks.
betweentwosetsofpairedhigh-dimensionalobservations.
Thisframeworkanditsnonlinear[57,58,59,60,61,62]or Insingle-viewdata,two-stageclusteringframeworkspro-
sparse[63,64]extensionshavebeenappliedinvariousdo- posedby[69,70,71]alternatebetweenclusteringandusing
the cluster assignments as pseudo-labels to revise image
2SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
representations. [72] have introduced a pseudo-labeling mumcorrelationobjective[54,74]. Specifically,wetrain
methodthatencouragestheformationofmoremeaningful view-specificautoencodersF(v)whichextractlatentrepre-
andcoherentclustersthatalignwiththesemanticcontent sentationsh(v) andreconstructxˆ(v),whereF(v)(x(v)) =
i i i
oftheimages. Theirframeworktreatssomepseudo-labels (h(v),xˆ(v)) for each sample x(v) in view v. The model
as reliable, synergizing the similarity and discrepancy of i i i
minimizes the mean squared error (MSE) by comparing
thesamples. Formulti-viewdatasets, clusteringandself- reconstructedsamplesxˆ(v)totheinputx(v):
supervisionframeworks[53,38,73]offerlosscomponents i i
thatenforceconsensusinthelatentmulti-viewrepresenta-
L(v) (Xv)=E ||x(v)−xˆ(v)||2.
tions.Weproposeanewmulti-viewpseudo-labelingscheme mse x(v)∈X(v) i i 2
i
thatusesthelabeledsamplestodefinewithin-clusterpermu-
tations,whichareusedtoenhancethelearnedembeddings. Weuseacorrelationlosstoforcetheembeddingtobecor-
related(seeSubsection3.1). Denotingthelatentrepresen-
4.TheProposedMethod tationsbyH(v) ∈Rdv×N,andH(w) ∈Rdw×N generated
bytheautoencodersF(v)andF(w)respectively.Thecovari-
4.1.ProblemSetup ancematrixbetweentheserepresentationscanbeexpressed
We are given a set of multi-view data X = {X(v) ∈ as C vw = N1 −1H(v)(H(w))T. Similarly, the covariance
Rdv×N}n v=v 1 with n v views and N samples. Each view matricesofH(v) andH(w) asC v = N1 −1H(v)(H(v))T
is defined by X(v) = [x(v),x(v),...,x(v)] and each x(v) andC = 1 H(w)(H(w))T. Anegativetraceexpresses
1 2 N i w N−1
is a d -dimensional instance. Our objective is to pre- thecorrelationloss:
v
dict cluster assignment y for each tuple of instances
i (cid:104) (cid:105)
(x( i1),x( i2),...,x( inv)),i = 1,...,N in X and the number L corr(H(v),H(w))=−Tr C− w1/2C wvC− v1C vwC− w1/2 .
ofclustersisK. (2)
4.2.HighLevelSolution
4.4.Multi-viewPseudo-labeling
Atthecoreofourproposedsolutionarethreecomponents:
A key component of our model is a clustering head that
(i)multi-viewrepresentationlearning,(ii)reliablepseudo-
predicts cluster assignments based on fused data embed-
labelprediction,and(iii)withinclustersamplepermutations.
dings. Thepredictionsoftheclusteringheadarethenused
Thesecomponentsarecomplementaryandaretrainedinan
fordefiningprototypesandreliablepseudo-labels, which
end-to-endfashionbasedonmulti-viewobservations.
willbeexploitedinthenextsteptoimprovetheembedding
Our representation learning component aims to create bycreatingwithinclusterpermutations(seeSection4.5.
aligneddataembeddingbyusingamaximumcorrelation
Theclusteringhead,denotedbyG,isoptimizedinaself-
objective. Thisembeddingcapturesthesharedinformation
supervisedmanneralongwiththepseudo-labelsintroduced
acrossmultipleviewsofthedata. Youcanfindmoredetails
in this subsection. The clustering head accepts a fusion
inSection4.3. Thesecondcomponentinvolvesfusingthe oflatentembeddingsrepresentedby 1 (cid:80) H(v),andpre-
embeddingsandpredictingpseudolabelsusingaclustering nv v
dictsclusterassignmentsrepresentedbyprobabilitymatrix,
head. Wethenpresentafiltrationproceduretoselectreli- G( 1 (cid:80) H(v))=P ∈RN×K.
ablerepresentativesineachcluster,whichwedescribein nv v
Section4.4. We now present the first part of our multi-view pseudo-
labelsscheme. Forclarityandsimplicity,wedemonstrate
Ourfinalcomponentusestheselectedrepresentatives,which
this for a single view such that the notation for view v is
introduces random permutation to the samples from the
omittedintherestofthissection.
sameclusteracrossviews. Youcanfindthedetailsofthis
permutation technique in Section 4.5. Through both the- For each cluster in a given view, we select the top Nmb
K
oreticalandempiricalanalysis,wehaveshownthatthese probabilitiesinP andusetheirindextogettheprototype
permutations can enhance cluster separation when using pseudo-labelssamplesh iforclusterk:
CCA-based objectives. For more details, please see Sec-
N
tions5.2and5.3.Anoverviewofourmodel,calledCOPER, T ={h |i∈argtopk(P , mb),
ispresentedinFig. 1. k i :,k K
∀i=1,2,...,N },
mb
4.3.DeepCanonicallyCorrelatedAutoencoders
whereP denotesthek-thcolumnofmatrixP,N isthe
:,k mb
To learn meaningful representations from the multi-view sizeofminibatchandargtopkreturns Nmb mostconfident
K
observations,weuseautoencodersequippedwithamaxi- samplesinP . Wenowcomputetheclustercentersh¯ as
:,k k
3SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
Figure1.OurproposeddeeplearningmodelCOPER.Illustrationusingtwomodalities,text(T)andimage(I),areproceededthrough
view-specificauto-encoders,creatinglatentembeddings(H(T)andH(I)).Theseembeddingsarealignedusingacorrelationlossand
thenfusedtoserveasinputfortheclusteringhead.TheclusteringheadestimatesaprobabilitymatrixP,whichisusedtoobtainmulti-view
pseudo-labels.Wethenusethisprobabilitymatrixalongwiththepseudo-labelstoupdatethemodelweightsthroughacross-entropy
loss.Finally,wegeneratewithin-clusterpermutationsbasedonthesepseudo-labels,whichhelpenhancetheseparationofclustersinthe
embeddingspace.
themeanofsamplesinT : acrossdifferentpairsofviews. Thisnovelself-supervision
k
taskenhancesintra-classscatterwhileattenuatinginter-class
h¯ k = NK (cid:88) h i. (3) variabilityofthelatentembeddings. Furtherdetails,analy-
mb h i∈Tk sis,andexamplesarediscussedinsection 5.1.
Next, we compute the cosine similarity between h¯ and First,wefurtherprocessthesamplesandtheirpseudo-labels,
k
everysampleinh i. N Kmb ofthemostsimilarsamplestoh¯ k X. Specifically, in case a sample is given more than one
areselectedandarefurtherfilteredbypassingthethreshold pseudo-label,werandomlyselectoneofthepseudo-labels
λ, makingthemreliablepseudo-labels. Thismayleadto inp˜(v),andtheprobabilitiesarechangedtoone-hotvector.
i
thepossibilitythatasamplewasassignedtomorethanone Next, we filter out all samples not assigned to the same
cluster. Assumingsamplex wasassignedtoseveralclus- pseudo-labelsacrossallviews. Theassignedpseudo-label
i
ters,wenormalizetheprobabilitieswithsoftmaxfunction: for sample i is denoted as yˆ. We now use these pseudo-
i
p˜ i = (cid:80)e kp ei pi,wherep i =0.5·(1+ ||h¯h¯ k|k |· ·|h |hi i||). Weassign wla ib le ll es nt ho ap ne cr efo or um rw rei pth rein sec nlu tas tt ie or nra an nd do cm lup se term rinu gta cti ao pn as b, iw lih tii ec sh
.
acorrespondingone-hotvectorrepresentationifthesample
Definition4.1. Arandompermutationforeachviewisde-
was assigned with a single label. Samples with different
fined for the vector of indices I¯ ⊆ 1,...,N with pseudo
pseudo-labelsacrossviewsarefilteredout,whichleadsto k
labelsforclusterkastheapplicationofthefollowingoper-
thesameindicesinallviews. Thesetofindicesthatwere
atorΠlI¯ ,whereΠl israndomlysampledfromthesetof
assignedwithreliablepseudo-labelsisnowdenotedasI. k k k
allpermutationmatricesofsizeN¯ ×N¯ .
k k
Wedenotethepairsofsamplesandtheirpseudo-labelsby
X ={(x ,p˜)} . ThemodelG isnowoptimizedwith HereI¯ aretheN¯ indicesofsamplesassignedwithpseudo-
p i i i∈I Γ k k
X={X }nv byminimizingcross-entropylossL where label k, and l denotes the permutation index as different
p v=1 ce
L isdefinedby: permutationsmaybeappliedseveraltimes. Onceweapply
ce
(cid:88) theserandompermutationstosamplesacrossthedifferent
L (X )=− p˜ log(p˜). (4)
ce p i i views(v =1,...,n ),wecreateanewartificialcorrespon-
v
i∈I dencebetweenviewsinthedataset.
4.5.Self-SupervisedCanonicalCorrelation-based We denote the new permuted multi-view data as X˜ =
Permutations {X˜ p(v)}n v=v 1. The corresponding latent embedding are de-
We now discuss our core training phase, where samples
notedasH˜(v)
. Allnewpermuteddataisnowusedtotrain
withthesamepseudo-labelsarepermutedwithinclusters themodelwithcross-entropylossasinEq.4andcorrelation
4SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
Figure2.Anillustrationforourmulti-viewpseudo-labelsscheme.Theclusterhead,denotedasG ,producesthematrixoflogitsvalues
Γ
P.Wechoosethetopthreesampleswiththehighestprobabilitiestocreatetheprototypepseudo-labelsforeachcluster.Inthisparticular
example,thesampleinthethirdindexwasassignedtotwoclusters.Wethenchoosethereliablepseudo-labelsasthetopthreeexamples
thathavethehighestcosinesimilaritybetweenthehiddenrepresentations,denotedash¯ .Thereliablepseudo-labelsarealsofilteredwith
k
agiventhreshold,λ.Inaddition,weuseSoftmaxtocreateprobabilities.
lossasinEq. 2. Chaudhuri et al. [76] presented an independent theoreti-
calanalysisthatmotivatesourpermutations. Theauthors
Overallourmodelisoptimizedusing:
showedthatCCA-basedobjectivescanimproveclustersep-
arationiftheviewsareuncorrelatedforsampleswithina
(cid:88)nv (cid:88)nv (cid:88)nv
L= [L(v) +L(v)]+ L (H(v),H(w)), givencluster. Ourwithin-clusterpermutationshelpobtain
mse ce corr
thisproperty,aswedemonstrateempiricallyinFigure4.
v=1 v=1w=v+1
whereweapplyalllosstermstoboththeoriginalandper- 5.1.Reminder: LinearDiscriminantAnalysis(LDA)
muted data in a similar way. This is done by selecting
Fisher’slineardiscriminantanalysis(LDA)aimstopreserve
random batches and applying stochastic gradient descent
variancewhileseekingtheoptimallineardiscriminantfunc-
(SGD).However,wehaveobservedthatwhenwefeedsam-
ples from
H˜(v) ,H˜(w)
to the correlation term, tuning its
tion[52]. Unlikeunsupervisedtechniquessuchasprincipal
componentanalysis(PCA)orcanonicalcorrelationanaly-
impactusingthehyperparameterβcanleadtobetterperfor-
sis(CCA),LDAisasupervisedmethodthatincorporates
mance. InAppendixsectionC.2,wepresentaprocedurefor
categoricalclasslabelinformationtoidentifymeaningful
tuningthehyperparametersofourmodel,andinAppendix
projections.Theapproachinvolvesmaximizinganobjective
sectionBwepresentthecomplexityanalysisofourmodel.
functionthattakesintoaccountthescatterpropertiesofeach
classandtheoverallscatter. Thisobjectivefunctioniscon-
5.Justification: COPERforClustering structedtobemaximizedthroughaprojectionthatenhances
intra-classscatteranddiminishesinter-classscatter.
COPERreliesonwithin-clusterrandompermutationtoen-
hancethelearnedmultiviewembedding. Thejustification ForadatasetX ∈RD×N andit’scovariancematrixC,we
forthistechniqueliesintwotheoreticalaspects. Thefirst denotetheinter-classcovariancematrixas:
waspresentedby[75],whichartificiallycreatedmulti-view
data by pairing examples with augmented samples from
the same class. The authors then demonstrate that apply-
1
(cid:88)K (cid:88)Nk
ingCCAtosuchdataisequivalenttolineardiscriminant C e = N (xk i −µ k)(xk i −µ k)T.
analysis(LDA)[52](describedin5.1). Here,weshowtheo- k=1i=1
reticallythatwithinclusterpermutationsofmulti-viewdata
leadtosimilarresults(seeSection5.2and5.3). Thisisalso
backedupbyempiricalevidencepresentedinFigure3. Where N are the samples from class k, xk is the i’th
k i
5SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
sample,andµ isthemean. Theinter-classcovarianceis: thelearnedrepresentations. Ifsuchlabelerrorsareinduced,
k
Proposition5.2breaksandthesolutionofCCAwithpermu-
K
C
=(cid:88)N
kµ µ T,
tationsisnolongerequivalenttothesolutionofLDA.
a N k k
To quantify this effect, we treat these induced errors as a
k=1
perturbation matrix. We substitute C−1C from Eq. 5
and C = C + C . The optimization for LDA can be e a
e a with A and denote D as the perturbation noise such that
formulatedas: Aˆ = A+D. Inadditionweusetoolsformperturbation
hTC h
max a . theory [79] to provide the following upper bound for the
h̸=0 hTC eh
approximatedLDAeigenvalues:
Whichcouldbesolvedusingageneralizedeigenproblem:
|λˆ −λ |≤||D|| ,i=1...n, (6)
i i 2
C ah=λC eh whereλ isthei’thLDAeigenvalueobtainedfromA;λˆ
i i
C e−1C ah=λh. (5) isthei’thLDAapproximatedeigenvalueobtainedfromAˆ;
andnisthetotalnumberofeigenvalues. InAppendixD.2,
Inthefollowingsubsection,weshowaconnectionbetween
we provide a complete derivation of this approximation.
thesolutionofCCAwithpermutationsinducedwithinthe
In subsection 4.5, we conduct acontrolled experiment to
clusterandthesolutionofLDA.
evaluatehowlabelnoiseinfluencesthisapproximation.This
impliesthatthemorethepseudo-labelsresembletheground
5.2.FromCOPERtoLDA
truth labels, the closer the representation is to LDA. We
presentedempiricalexperimentsinSection5.4tosupport
In[75]theauthorsconstructedanartificialmulti-viewset-
this.
ting by splitting samples into two views and pairing the
viewsbasedontheclasslabels. Basedonthisprocedure,
theycreatedanaugmentedmulti-viewdatasetinwhichthe 5.4.CaseStudyusingFashionMNIST
shared information across views is the class label. They
WeconductacontrolledexperimentusingF-MNIST[80]to
proved that applying CCA to such augmented data con-
corroborateourtheoreticalresultspresentedinthissection.
vergestothesolutionofLDA(describedinSection5.1).
First,wecreatetwocoupledviewsbyhorizontallysplitting
Weobservemulti-viewdataandusethefollowingassump- theimages. CCAissubsequentlyperformedonthemulti-
tiontoprovearelationbetweenourschemeandLDA: viewdataset,withdifferentversionsofwithinclustersample
permutations.
Assumption5.1. Fortwodifferentviewsv,w,theobserva-
tionsarecreatedbysomepushforwardfunction(withnoise) First, we only permute samples with the same label. We
ofsomelatentcommonparameterθthatissharedacrossthe show in Fig. 3 (a) that such supervised permutations im-
views.Specifically,Xv =fv(θ,ϵv)andϵvisviewspecific
proveclusterseparation,asevidencedbytheincreasedARI.
noise. Theparameterθcarriestheclusterinformation. Moreover, our experiments in panel (b) indicate that the
CCAsolutionbecomesmoresimilartotheLDAsolution.
Wenotethat assumptionsaboutcommon latentvariables
In panel (c), we have also confirmed our hypothesis that
werealsousedinpriorworkonCCA[77,78]. Underthis
thesepermutationsdecreasethemeaninter-classcorrelation
assumption,weshowthefollowing:
acrossviews(pleaserefertothesecondparagraphinSection
Proposition 5.2. CCA with all inter-cluster permutation 5formoredetails).Next,werepeatedtheevaluationsinpan-
convergestothesamerepresentationextractedbyLDA[52]. els(a)and(b)usingpermutationsbasedonpseudolabels.
Wehavealsoincludedtheresultsbasedonrandompermu-
AproofofthispropositionappearsinAppendix D.1,where tationsandtheoriginaldata(nopermutation)asbaselines.
weshowthath LDA =h CCA. Theprooffollowstheanal- Theresultsshowthatpseudo-label-basedpermutationsalso
ysisof[75]. Intuitively,thisresultindicatesthatthelabel improveclusterseparationandbringtherepresentationof
informationisleakedintoembeddinglearnedbyCCAonce CCAclosertothesolutionofLDA.
weapplywithinclusterpermutations. InSection5.4, we
Wehaveconductedanexperimenttomeasuretheerrorin-
conduct an experiment using Fashion MNIST to demon-
ducedbyfalselyannotatedpseudo-labels. Thisexperiment
stratehowwithinclusterrandompermutationcanenhance
complementsourLDAapproximationpresentedinsubsec-
theembeddinglearnedbyCCA.
tion 5.3. We first subsampled 20% of the data and then
graduallyintroducedtheremainingdatawitheitherrealor
5.3.LDAApproximation
noisy(false)labels. Theresults,asshowninFig. 4,demon-
Sinceourmodelreliesonpseudo-labels,thesecaninduce stratethatintroducingfalseannotationsincreasesthegap
errorsintherandomwithin-clusterpermutation,influencing betweentheeigenvalues. Ontheotherhand,addingmore
6SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
sampleswithcorrectannotationsleadstotheconvergence
ofthesolutiontothefullLDAsolution.
Figure4.WeconductedanexperimentonLDAapproximation
withinducedlabelnoise.Wegraduallyincreasedthenumber
of samples from F-MNIST and analyzed the effect on the
eigenvaluesofLDA.Wefoundthatnoisyannotationsincrease
theeigenvaluegapcomparedtothefullsolution.Incontrast,
theadditionofmorerealsamplesreducestheapproximation
error.
Figure3.Case study of premutation CCA using Fashion
MNIST. (a) Permuting more samples within a cluster im-
wereportthemeanovertenrunswhiletheyreportthebest
provesclusterseparationasmeasuredbytheAdjustedRand
result. We argue that our evaluation, which includes the
Index(ARI).Wecomparelabeledpermutation(supervised)to
standarddeviation,providesamoreinformativeindication
pseudo-label-based(unsupervised)andrandom.(b)Permuting
ofthecapabilitiesofeachmethod.
moresamplesalsopushestherepresentationobtainedbyCCA
towardsLDA,asindicatedbythegapbetweeneigenvalues.(c)
Themeaninter-classcorrelationbetweenviewsisattenuated 6.1.AblationStudy
aswepermutemoresamples.
Weperformedanablationtoassesstheimpactofdifferent
componentsofourmodel. Wecreatedfourvariationsofour
model,eachincludingdifferentsubsetsofthecomponentsof
6.RealWorldData
COPER.Thesevariationswere:(i)onlythemultipleautoen-
Weconductanextensiveexperimentwithtenpubliclyavail- coders(AE);(ii)COPERwithoutpseudo-labelpredictions;
ablemulti-viewdatasetsusedinrecentworks[51,48]. The (iii)COPERwithoutwithin-clusterpermutations;and(iv)
propertiesofthedatasetsarepresentedinTableA,anda COPER. For (i) and (ii), we utilized K-means to predict
complete description appears in Appendix A. The imple- clusterassignments,aswenolongerhadaclusteringhead.
mentationdetailsaredetailedinAppendixC.1 We conducted this ablation study using the METABRIC
dataset. In Table 2, we compare the clustering metrics
Weassesstheclusteringperformancewiththreecommonly
acrossthesevariationsofCOPER.Theresultsindicatethat
used metrics: Clustering Accuracy (ACC), The adjusted
thepseudo-labelprocedureslightlyimprovedassignment
Randindexscore(ARI),andNormalizedMutualInforma-
accuracyoverK-means. Additionally,thenewpermutation
tion(NMI).BothACCandARIarescoredbetween0to1,
schemeboostedperformancebymorethan10%.
wherehighervaluesindicatebetterclusteringperformance.
Weconductedeachexperiment10timesandreportedthe
6.2.ConclusionandLimitations
meanandstandarddeviationforeachmetric. Wecompare
ourmodeltoDSMVC[48]andCVCL[51]andseveraltwo- OurworkpresentsanewapproachcalledCOrrelation-based
stagebaselines,whereK-meanswasapplied: Rawsamples PERmutations(COPER),adeeplearningmodelformulti-
withoutanyfeaturestransformations;PCAtransformations; viewclustering(MVC).COPERintegratesclusteringand
linearCCAtransformations;Autoencoder(AE)transforma- representationtasksintoanend-to-endframework,elimi-
tions;DeepCCAwithautoencoders(DCCA-AE)[54,74] natingtheneedforaseparateclusteringstep. Themodel
transformations. employsauniqueself-supervisiontask,whereinter-class
pseudo-labelsarepermutedacrossviewsforcanonicalcor-
WepresenttheresultsintheTable6.Ourmodeloutperforms
relationanalysisloss,contributingtothemaximizationof
state of the art models in both accuracy and the adjusted
intra-classvarianceandminimizationofinter-classvariation
Randindexacrossalldatasets,wheretheaccuracyimprove-
inthesharedembeddedspace.
mentreachesupto14%. Wenotethattheresultsof[51,48]
aredifferentfromthevaluesreportedbytheauthorssince Wedemonstratethat,undermildassumptions,ourmodel
7SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
Table1.Clusteringresults.Ourmodel(COPER)iscomparedagainsttworecentdeepMVCmodelsandtwo-stageK-Meansclustering
methods.
Method/Dataset METABRIC Reuters Caltech101-20 VOC Caltech5V-7 RBGD MNIST-USPS CCV MSRVC1 Scene15
ACC
Raw 35.18±2.9 37.16±5.9 42.98±3.1 52.41±7.3 73.54±6.3 43.15±1.8 69.78±6.3 16.25±0.7 74.00±7.3 37.91±0.9
PCA 37.72±1.4 42.56±2.9 41.47±3.4 43.66±4.9 74.26±6.3 42.64±1.9 68.14±3.9 16.17±0.5 74.14±6.1 37.53±1.2
CCA 40.05±2.1 42.85±2.5 42.34±3.1 53.1±4.1 75.96±5.2 41.47±2.2 80.15±5 16.16±0.7 73.29±7.3 37.59±1.3
AE 38.92±2.5 43.35±4.0 40.39±2.3 36.66±2.69 54.48±3.2 32.16±1.6 33.83±3.6 15.58±0.6 56.76±3.8 33.59±2.3
DCCA-AE 45.39±2.8 43.18±2.8 48.18±3.4 46.53±2.7 58.37±3.9 32.59±1.3 92.19±2.4 18.28±0.9 56.76±3.8 33.70±2.0
DSMVC 40.60±3.8 46.37±4.4 39.33±2.4 57.82±5.0 79.24±9.5 39.77±3.6 70.06±10.3 17.90±1.2 60.71±15.2 34.30±2.9
CVCL 42.66±6.2 45.06±8.0 33.5±1.4 36.88±3.1 78.58±5.0 31.04±1.8 99.38±0.1 26.23±1.9 77.90±12.3 40.16±1.8
COPER 49.13±3.2 53.15±3.3 54.83±3.9 64.65±2.6 82.81±5.1 49.80±0.8 99.88±0.0 28.06±1.1 89.14±7.1 40.68±1.6
ARI
Raw 18.64±1.2 7.49±7.7 34.58±4 29.51±7.4 57.19±5.9 24.13±1.2 58.88±4.8 5.45±0.3 58.53±5.8 22.31±0.4
PCA 19.05±1.5 19.88±1.4 31.75±3.5 21.44±5.1 57.91±4.7 23.78±1.4 54.55±2.8 5.45±0.2 58.39±5.9 22.0±0.4
CCA 19.84±1.7 18.76±1.7 32.68±3.8 34.69±5.4 60.01±1.9 24.59±1.7 71.09±4.5 5.32±0.3 56.97±7.2 22.57±0.5
AE 19.71±3.0 10.36±4.3 31.27±4.1 18.86±2.7 30.73±4.8 15.26±1.6 13.93±2.0 3.56±0.3 33.43±4.8 17.00±1.7
DCCA-AE 21.70±2.8 7.63±4.0 37.19±3.2 32.77±3.4 34.63±4.1 15.00±2.0 84.04±4.4 5.0±0.4 33.43±4.8 17.12±1.6
DSMVC 18.24±2.4 23.41±5.0 31.21±2.4 50.78±4.6 69.06±9.4 23.74±2.6 56.87±13.4 5.97±0.5 42.63±19.0 18.85±2.5
CVCL 22.69±4.3 22.39±8.8 24.85±0.9 22.49±2.7 63.25±6.3 16.16±1.3 98.63±0.2 12.72±1.3 64.27±13.8 24.01±1.7
COPER 26.77±2.4 22.80±4.3 49.55±5.3 53.26±4.0 69.53±6.7 34.17±1.4 99.73±0.0 12.27±0.4 82.26±9.0 25.00±1.1
NMI
Raw 24.13±1.4 15.38±9.0 61.77±2.0 53.89±5.5 64.34±3.9 38.83±0.8 70.43±2.4 15.07±0.5 66.89±3.8 40.74±0.4
PCA 25.44±1.7 27.92±1.2 60.5±1.4 44.29±3.9 63.79±2.5 37.43±1.0 65.27±1.1 15.17±0.3 67.37±4.0 40.33±0.5
CCA 26.85±2.0 27.16±1.0 61.13±2.0 53.01±3.6 65.72±1.9 38.79±1.0 77.58±2.2 14.38±0.4 65.52±5.2 40.75±0.5
AE 27.34±2.6 18.61±3.2 50.15±2.4 30.46±2.0 38.97±5.2 28.85±1.4 22.83±2.5 10.68±1.0 48.31±4.0 33.18±2.6
DCCA-AE 30.95±2.8 23.04±3.1 54.46±2.4 47.29±4.0 43.38±3.9 27.54±3.5 85.32±3.3 16.39±1.0 48.31±4.0 33.65±2.0
DSMVC 25.51±1.9 29.48±4.8 60.72±1.4 65.13±3.2 75.08±6.8 37.60±2.5 67.80±11.5 16.70±0.9 57.77±13.0 36.55±3.3
CVCL 32.3±4.5 29.41±11.1 56.13±1.0 32.95±1.3 69.56±4.8 26.6±1.6 98.21±0.3 26.25±0.9 72.66±8.9 41.13±1.7
COPER 34.07±2.6 31.10±4.1 49.25±6.2 58.54±2.2 74.03±4.6 38.13±0.9 99.64±0.1 26.32±0.7 86.15±6.4 41.98±1.2
correspondence; this could be done by learning to match
Table2. AblationstudyontheMETABRICdataset.
kernelsasshownin[82].
Model ACC ARI NMI
AE 38.92±2.5 19.71±3.0 27.34±2.6
COPERw/opseudo-labels 45.39±2.8 21.70±2.8 30.95±2.8 6.3.BroaderImpact
COPERw/opermutations 45.82±3.2 22.41±3.3 22.41±31.3.1
COPER 49.13±2.9 26.77±2.4 34.07±2.6 Ourworkpresentsanewdeeplearning-basedsolutionfor
multi-viewclustering,alongwithatheoreticalfoundation
foritsperformance.Thisresearchcanpositivelyimpactvari-
approximatestheprojectionachievedbythesupervisedlin- ousdomainsbyprovidingresearchersandpractitionerswith
eardiscriminantanalysis(LDA)method. Ourcontributions aversatiledataanalysistoolforuseacrossheterogeneous
include the development of a self-supervised deep learn- datasets, thereby facilitating advancements in knowledge
ingmodelforMVC,amulti-viewpseudo-labelingproce- discoveryanddecision-makingprocesses.However,thepro-
dure,empiricalandtheoreticalinsightsintotheefficacyof posedmethodhasethicalimplicationsandpotentialsocietal
within-clusterpermutationforcanonicalcorrelationanaly- consequencesthatmustbeconsidered. Itiscrucialtopay
sis,ananalyticalexaminationoftherelationshipbetween attentiontobiasandfairnesstopreventtheamplificationof
our permutation-based CCA and LDA solutions, and an biasesacrossmodalities. Transparencyandexplainability
extensiveexperimentalevaluationshowingthatourmodel areessentialtoensureuserunderstandingandmitigatethe
canaccuratelyclusterdiversedatatypes. perceivedblack-boxnatureofdeeplearningmodels.
Itisimportanttoacknowledgethatourmodelhascertain
References
limitations. One of them is the potential requirement for
relativelylargebatchsizesduetotheDCCAloss. Thisisa
[1] ErickArmingol,AdamOfficer,OlivierHarismendy,
knownlimitationofthesetypesofobjectives[62]andcan
and Nathan E Lewis. Deciphering cell–cell interac-
beaddressedbyalternativeobjectivessuchassoftdecorre-
tionsandcommunicationfromgeneexpression. Na-
lation[81]. Anotherlimitationofourmodelisitssensitivity
tureReviewsGenetics,22(2):71–88,2021.
to datasets with many clusters. This is indicated by the
resultsobtainedonCaltech101-20andCCV,whichconsist [2] Turkay Kart, Wenjia Bai, Ben Glocker, and Daniel
of20clusters,showingasmallerrelativeimprovement. We Rueckert. Deepmcat: large-scaledeepclusteringfor
notethattheunweightedlosscombinationpresentedhere medical image categorization. In Deep Generative
issuboptimal. Evaluatingsmartermultitaskschemesisan Models,andDataAugmentation,Labelling,andIm-
interestingtopicforfuturework. Anotherdirectionforfu- perfections: First Workshop, DGM4MICCAI 2021,
ture work involves relaxing the requirement for bijective andFirstWorkshop,DALI2021,HeldinConjunction
8SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
with MICCAI 2021, Strasbourg, France, October 1, [14] GunnarECarlsson,FacundoMe´moli,etal. Charac-
2021,Proceedings1,pages259–267.Springer,2021. terization, stability and convergence of hierarchical
clustering methods. J. Mach. Learn. Res., 11(Apr):
[3] ViniciusMikuniandFlorenciaCanelli. Unsupervised
1425–1470,2010.
clustering for collider physics. Physical Review D,
103(9):092007,2021. [15] JonathanSvirskyandOfirLindenbaum. Interpretable
deep clustering. arXiv preprint arXiv:2306.04785,
[4] Daniel Boley, Maria Gini, Robert Gross, Eui-
2023.
Hong Sam Han, Kyle Hastings, George Karypis,
VipinKumar,BamshadMobasher,andJeromeMoore. [16] Amit Rozner, Barak Battash, Lior Wolf, and Ofir
Partitioning-basedclusteringforwebdocumentcate- Lindenbaum. Domain-generalizablemultiple-domain
gorization. DecisionSupportSystems,27(3):329–341, clustering. arXivpreprintarXiv:2301.13530,2023.
1999.
[17] XiaoshuaiSunandDachengTao. Asurveyofmulti-
[5] AnilKJain.Dataclustering:50yearsbeyondk-means. viewmachinelearning. Neurocomputing,128:22–45,
Patternrecognitionletters,31(8):651–666,2010. 2014.
[6] TVelmuruganandTSanthanam. Asurveyofpartition [18] Ankit Kumar, Hal Daume´ III, and Amol Saha. Co-
basedclusteringalgorithmsindatamining: Anexperi- regularizedmulti-viewspectralclustering. Proceed-
mentalapproach. InformationTechnologyJournal,10 ingsofthe28thinternationalconferenceonmachine
(3):478–484,2011. learning(ICML-11),pages521–528,2011.
[7] EshrefJanuzaj,Hans-PeterKriegel,andMartinPfei- [19] XiangLi,XiaojieGuo,andZhiZhang.Multiviewclus-
fle. Scalabledensity-baseddistributedclustering. In tering: Asurvey. In201824thInternationalConfer-
Knowledge Discovery in Databases: PKDD 2004: enceonPatternRecognition(ICPR),pages387–392.
8th European Conference on Principles and Prac- IEEE,2018.
ticeofKnowledgeDiscoveryinDatabases,Pisa,Italy,
[20] Xudong Huang, Yanan Gu, Kang Liu, and Xudong
September 20-24, 2004. Proceedings 8, pages 231–
Gao. Aself-trainingco-trainingalgorithmformulti-
244.Springer,2004.
viewspectralclustering. PatternRecognitionLetters,
33(13):1690–1700,2012.
[8] Hans-PeterKriegelandMartinPfeifle. Density-based
clustering of uncertain data. In Proceedings of the [21] ChangXu,DachengTao,andChaoXu. Asurveyon
eleventhACMSIGKDDinternationalconferenceon multi-viewlearning. arXivpreprintarXiv:1304.5634,
Knowledgediscoveryindatamining,pages672–677,
2013.
2005.
[22] ZhenguoLi,HongkeZhao,KeLu,andDachengTao.
[9] YixinChenandLiTu. Density-basedclusteringfor Multimodal clustering and content-based fusion for
real-time stream data. In Proceedings of the 13th multimediaanalysis. In2017IEEEInternationalCon-
ACMSIGKDDinternationalconferenceonKnowledge ferenceonMultimediaandExpo(ICME),pages1–6.
discoveryanddatamining,pages133–142,2007. IEEE,2017.
[10] LianDuan,LidaXu,FengGuo,JunLee,andBaopin [23] ZhongpingXu,DachengTao,ChaoXu,andJieYang.
Yan.Alocal-densitybasedspatialclusteringalgorithm Asurveyofmulti-viewrepresentationlearning. Neu-
withnoise. Informationsystems,32(7):978–986,2007. rocomputing,128:27–42,2014.
[11] Sarah P Preheim, Allison R Perrotta, Antonio M [24] XinCao,ChangYu,andJieboLuo. Diversifiedmulti-
Martin-Platero, Anika Gupta, and Eric J Alm. viewvideorecommendation. IEEETransactionson
Distribution-basedclustering: usingecologytorefine Multimedia,17(4):511–525,2015.
theoperationaltaxonomicunit. Appliedandenviron-
[25] Ofir Lindenbaum, Neta Rabin, Yuri Bregman, and
mentalmicrobiology,79(21):6593–6603,2013.
AmirAverbuch. Seismiceventdiscriminationusing
[12] BinJiang,JianPei,YufeiTao,andXueminLin. Clus- deepcca. IEEEGeoscienceandRemoteSensingLet-
teringuncertaindatabasedonprobabilitydistribution ters,17(11):1856–1860,2019.
similarity.IEEETransactionsonKnowledgeandData
[26] Ofir Lindenbaum, Yuri Bregman, Neta Rabin,
Engineering,25(4):751–763,2011.
and Amir Averbuch. Multiview kernels for low-
[13] FionnMurtagh. Asurveyofrecentadvancesinhier- dimensionalmodelingofseismicevents. IEEETrans-
archicalclusteringalgorithms. Thecomputerjournal, actions on Geoscience and Remote Sensing, 56(6):
26(4):354–359,1983. 3300–3310,2018.
9SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
[27] XiaochunCao,ChangqingZhang,HuazhuFu,SiLiu, [40] MingYin, WeitianHuang, andJunbinGao. Shared
and Hua Zhang. Diversity-induced multi-view sub- generativelatentrepresentationlearningformulti-view
spaceclustering. InCVPR,pages586–594,2015. clustering. InAAAI,pages6688–6695,2020.
[28] Shirui Luo, Changqing Zhang, Wei Zhang, and Xi- [41] JieWen,ZhengZhang,YongXu,BobZhang,Lunke
aochunCao. Consistentandspecificmulti-viewsub- Fei,andGuo-SenXie. CDIMC-net: Cognitivedeep
spaceclustering. InAAAI,2018. incompletemulti-viewclusteringnetwork. InIJCAI,
pages3230–3236,2020.
[29] RuihuangLi,ChangqingZhang,HuazhuFu,XiPeng,
[42] JieXu,YazhouRen,GuofengLi,LiliPan,CeZhu,and
TianyiZhou,andQinghuaHu. Reciprocalmulti-layer
Zenglin Xu. Deep embedded multi-view clustering
subspacelearningformulti-viewclustering. InICCV,
withcollaborativetraining. InformationSciences,573:
pages8172–8180,2019.
279–290,2021.
[30] HandongZhao,ZhengmingDing,andYunFu. Multi-
[43] Jie Xu, Yazhou Ren, Huayi Tang, Xiaorong Pu, Xi-
viewclusteringviadeepmatrixfactorization. InAAAI,
aofeng Zhu, Ming Zeng, and Lifang He. Multi-
pages2921–2927,2017.
VAE:Learningdisentangledview-commonandview-
peculiarvisualrepresentationsformulti-viewcluster-
[31] JieWen,ZhengZhang,YongXu,andZuofengZhong.
ing. InICCV,pages9234–9243,2021.
Incomplete multi-view clustering via graph regular-
izedmatrixfactorization. InECCVWorkshops,2018. [44] Yijie Lin, Yuanbiao Gou, Zitao Liu, Boyun Li,
JianchengLv, andXiPeng. COMPLETER:Incom-
[32] Zuyuan Yang, Naiyao Liang, Wei Yan, Zhenni Li,
pletemulti-viewclusteringviacontrastiveprediction.
andShengliXie. Uniformdistributionnon-negative
InCVPR,2021.
matrix factorization for multiview clustering. IEEE
TransactionsonCybernetics,pages3249–3262,2021. [45] ChangqingZhang,QinghuaHu,HuazhuFu,Pengfei
Zhu,andXiaochunCao. Latentmulti-viewsubspace
[33] FeipingNie,JingLi,andXuelongLi. Self-weighted clustering. InCVPR,pages4279–4287,2017.
multiviewclusteringwithmultiplegraphs. InIJCAI,
pages2564–2570,2017. [46] MariaBrbic´ andIvicaKopriva. Multi-viewlow-rank
sparsesubspaceclustering. PatternRecognition,73:
[34] KunZhan,ChangqingZhang,JunpengGuan,andJun- 247–258,2018.
shengWang. Graphlearningformultiviewclustering.
[47] Xiaoliang Tang, Xuan Tang, Wanli Wang, Li Fang,
IEEETransactionsonCybernetics,48(10):2887–2895,
andXianWei. Deepmulti-viewsparsesubspaceclus-
2017.
tering. InProceedingsofthe2018VIIInternational
[35] XiaofengZhu,ShichaoZhang,WeiHe,RongyaoHu, ConferenceonNetwork,CommunicationandComput-
CongLei,andPengfeiZhu. One-stepmulti-viewspec- ing,pages115–119,2018.
tralclustering. IEEETransactionsonKnowledgeand
[48] HuayiTangandYongLiu. Deepsafemulti-viewclus-
DataEngineering,31(10):2022–2034,2018.
tering: Reducing the risk of clustering performance
degradationcausedbyviewincrease. InProceedings
[36] JunGuoandJiahuiYe.Anchorsbringease:Anembar-
oftheIEEE/CVFConferenceonComputerVisionand
rassinglysimpleapproachtopartialmulti-viewclus-
PatternRecognition,pages202–211,2022.
tering. InAAAI,pages118–125,2019.
[49] DanielJ.Trosten,SigurdLøkse,RobertJenssen,and
[37] Mahdi Abavisani and Vishal M Patel. Deep multi-
MichaelKampffmeyer. Reconsideringrepresentation
modalsubspaceclusteringnetworks. IEEEJournal
alignmentformulti-viewclustering. InCVPR,pages
ofSelectedTopicsinSignalProcessing,12(6):1601–
1255–1265,2021.
1614,2018.
[50] ManshengChen,LingHuang,Chang-DongWang,and
[38] Humam Alwassel, Dhruv Mahajan, Bruno Korbar, DongHuang. Multi-viewclusteringinlatentembed-
Lorenzo Torresani, Bernard Ghanem, and Du Tran. dingspace. InAAAI,pages3513–3520,2020.
Self-supervisedlearningbycross-modalaudio-video
clustering. InNeurIPS,pages9758–9770,2019. [51] Jie Chen, Hua Mao, Wai Lok Woo, and Xi Peng.
Deepmultiviewclusteringbycontrastingclusteras-
[39] ZhaoyangLi,QianqianWang,ZhiqiangTao,Quanxue signments. InProceedingsoftheIEEE/CVFInterna-
Gao,andZhaohuaYang. Deepadversarialmulti-view tionalConferenceonComputerVision(ICCV),pages
clusteringnetwork. InIJCAI,pages2952–2958,2019. 16752–16761,October2023.
10SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
[52] RonaldAFisher. Theuseofmultiplemeasurements IEEE/CVFConferenceonComputerVisionandPat-
intaxonomicproblems. Annalsofeugenics,7(2):179– ternRecognition,pages19863–19872,2023.
188,1936.
[65] HaroldPimentel,ZhiyueHu,andHaiyanHuang. Bi-
[53] Bowen Xin, Shan Zeng, and Xiuying Wang. Self- clustering by sparse canonical correlation analysis.
supervised deep correlational multi-view clustering. QuantitativeBiology,6:56–67,2018.
In2021InternationalJointConferenceonNeuralNet-
works(IJCNN),pages1–8.IEEE,2021. [66] Fares Al-Shargie, Tong Boon Tang, and Masashi
Kiguchi. Assessmentofmentalstresseffectsonpre-
[54] SarathChandar,MiteshMKhapra,HugoLarochelle,
frontalcorticalactivitiesusingcanonicalcorrelation
andBalaramanRavindran. Correlationalneuralnet-
analysis:anfnirs-eegstudy.Biomedicalopticsexpress,
works. Neuralcomputation,28(2):257–285,2016.
8(5):2583–2598,2017.
[55] HotellingHarold. Relationsbetweentwosetsofvari-
[67] Yu Zhang, Guoxu Zhou, Jing Jin, Yangsong Zhang,
ables. Biometrika,28(3):321–377,1936.
XingyuWang,andAndrzejCichocki. Sparsebayesian
[56] Bruce Thompson. Canonical correlation analysis: multiwaycanonicalcorrelationanalysisforeegpattern
Usesandinterpretation. Number47.Sage,1984. recognition. Neurocomputing,225:103–110,2017.
[57] FrancisRBachandMichaelIJordan. Kernelindepen-
[68] Zhiwen Chen, Steven X Ding, Tao Peng, Chunhua
dentcomponentanalysis. Journalofmachinelearning
Yang, and Weihua Gui. Fault detection for non-
research,3(Jul):1–48,2002.
gaussian processes using generalized canonical cor-
relation analysis and randomized algorithms. IEEE
[58] Tomer Michaeli, Weiran Wang, and Karen Livescu.
Transactions on Industrial Electronics, 65(2):1559–
Nonparametriccanonicalcorrelationanalysis. InIn-
1567,2017.
ternational conference on machine learning, pages
1967–1976.PMLR,2016.
[69] Mathilde Caron, Piotr Bojanowski, Armand Joulin,
[59] Ofir Lindenbaum, Arie Yeredor, and Moshe Salhov. andMatthijsDouze. Deepclusteringforunsupervised
Learningcoupledembeddingusingmultiviewdiffu- learningofvisualfeatures. InProceedingsoftheEu-
sion maps. In Latent Variable Analysis and Signal ropeanconferenceoncomputervision(ECCV),pages
Separation: 12thInternationalConference,LVA/ICA 132–149,2018.
2015, Liberec, CzechRepublic, August25-28, 2015,
Proceedings12,pages127–134.Springer,2015. [70] Mathilde Caron, Ishan Misra, Julien Mairal, Priya
Goyal, Piotr Bojanowski, and Armand Joulin. Un-
[60] OfirLindenbaum,ArieYeredor,MosheSalhov,and supervisedlearningofvisualfeaturesbycontrasting
AmirAverbuch. Multi-viewdiffusionmaps. Informa- clusterassignments. Advancesinneuralinformation
tionFusion,55:127–149,2020. processingsystems,33:9912–9924,2020.
[61] MosheSalhov,OfirLindenbaum,YarivAizenbud,Avi
[71] ParaskeviNousiandAnastasiosTefas.Self-supervised
Silberschatz, Yoel Shkolnisky, and Amir Averbuch.
autoencodersforclusteringandclassification. Evolv-
Multi-viewkernelconsensusfordataanalysis.Applied
ingSystems,11(3):453–466,2020.
and Computational Harmonic Analysis, 49(1):208–
228,2020. [72] ChuangNiu,HongmingShan,andGeWang. Spice:
Semanticpseudo-labelingforimageclustering. IEEE
[62] GalenAndrew,RamanArora,JeffBilmes,andKaren
Transactions on Image Processing, 31:7264–7278,
Livescu.Deepcanonicalcorrelationanalysis.InICML,
2022.
pages1247–1255,2013.
[63] OfirLindenbaum,MosheSalhov,AmirAverbuch,and [73] ChaoLi,ChengDeng,NingLi,WeiLiu,XinboGao,
YuvalKluger.L0-sparsecanonicalcorrelationanalysis. andDachengTao. Self-supervisedadversarialhashing
InInternationalConferenceonLearningRepresenta- networksforcross-modalretrieval. InProceedingsof
tions,2021. theIEEEconferenceoncomputervisionandpattern
recognition,pages4242–4251,2018.
[64] Weiqing Yan, Yuanyang Zhang, Chenlei Lv, Chang
Tang, Guanghui Yue, Liang Liao, and Weisi Lin. [74] WeiranWang,RamanArora,KarenLivescu,andJeff
Gcfagg: Global and cross-view feature aggregation Bilmes. Ondeepmulti-viewrepresentationlearning.
for multi-view clustering. In Proceedings of the InICML,pages1083–1092,2015.
11SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
[75] OlcayKursun,EthemAlpaydin,andOlegVFavorov. [86] Mark Everingham, Luc Van Gool, Christopher KI
Canonicalcorrelationanalysisusingwithin-classcou- Williams,JohnWinn,andAndrewZisserman.Thepas-
pling. Pattern Recognition Letters, 32(2):134–144, calvisualobjectclasses(voc)challenge. International
2011. journalofcomputervision,88:303–338,2010.
[76] KamalikaChaudhuri,ShamMKakade,KarenLivescu, [87] DanielJTrosten,SigurdLokse,RobertJenssen,and
and Karthik Sridharan. Multi-view clustering via MichaelKampffmeyer. Reconsideringrepresentation
canonical correlation analysis. In Proceedings of alignmentformulti-viewclustering. InProceedings
the26thannualinternationalconferenceonmachine oftheIEEE/CVFconferenceoncomputervisionand
learning,pages129–136,2009. patternrecognition,pages1255–1265,2021.
[77] Adrian Benton, Huda Khayrallah, Biman Gujral, [88] LaurensVanderMaatenandGeoffreyHinton. Visu-
DeeAnnReisinger,ShengZhang,andRamanArora. alizingdatausingt-sne. Journalofmachinelearning
Deepgeneralizedcanonicalcorrelationanalysis.arXiv research,9(11),2008.
preprintarXiv:1702.02519,2017.
[89] DelbertDueckandBrendanJFrey.Non-metricaffinity
[78] QiLyuandXiaoFu. Nonlinearmultiviewanalysis: propagationforunsupervisedimagecategorization. In
Identifiabilityandneuralnetwork-assistedimplemen- 2007IEEE11thinternationalconferenceoncomputer
tation. IEEETransactionsonSignalProcessing,68: vision,pages1–8.IEEE,2007.
2697–2712,2020.
[90] ChenKong,DahuaLin,MohitBansal,RaquelUrtasun,
[79] GilbertWStewartandJi-guangSun. Matrixperturba- andSanjaFidler. Whatareyoutalkingabout? text-
tiontheory. (NoTitle),1990. to-image coreference. In Proceedings of the IEEE
conferenceoncomputervisionandpatternrecognition,
[80] Han Xiao, Kashif Rasul, and Roland Vollgraf.
pages3558–3565,2014.
Fashion-mnist: anovelimagedatasetforbenchmark-
ing machine learning algorithms. arXiv preprint [91] Runwu Zhou and Yi-Dong Shen. End-to-end
arXiv:1708.07747,2017. adversarial-attention network for multi-modal clus-
tering. InCVPR,pages14619–14628,2020.
[81] Xiaobin Chang, Tao Xiang, and Timothy M
Hospedales. Scalableandeffectivedeepccaviasoft [92] ArthurAsuncionandDavidNewman. Ucimachine
decorrelation. InProceedingsoftheIEEEConference learningrepository,2007.
onComputerVisionandPatternRecognition,pages
1488–1497,2018. [93] Yu-GangJiang,GuangnanYe,Shih-FuChang,Daniel
Ellis,andAlexanderCLoui. Consumervideounder-
[82] Tamir Baruch Yampolsky, Ronen Talmon, and Ofir standing: Abenchmarkdatabaseandanevaluationof
Lindenbaum. Domainandmodalityadaptationusing human and machine performance. In ICMR, pages
multi-kernelmatching. In202331stEuropeanSignal 1–8,2011.
ProcessingConference(EUSIPCO),pages1285–1289.
IEEE,2023. [94] WeiZhao,CaiXu,ZiyuGuan,andYingLiu. Multi-
view concept learning via deep matrix factorization.
[83] Christina Curtis, Sohrab P Shah, Suet-Feung Chin, IEEEtransactionsonneuralnetworksandlearning
GulisaTurashvili,OscarMRueda,MarkJDunning, systems,32(2):814–825,2020.
Doug Speed, Andy G Lynch, Shamith Samarajiwa,
YinyinYuan,etal. Thegenomicandtranscriptomic [95] LiFei-FeiandPietroPerona. Abayesianhierarchical
architectureof2,000breasttumoursrevealsnovelsub- modelforlearningnaturalscenecategories. In2005
groups. Nature,486(7403):346–352,2012. IEEEComputerSocietyConferenceonComputerVi-
sionandPatternRecognition(CVPR’05),volume2,
[84] Sarah-JaneDawson,OscarMRueda,SamuelAparicio,
pages524–531.IEEE,2005.
andCarlosCaldas. Anewgenome-drivenintegrated
classificationofbreastcanceranditsimplications.The [96] Peter J Rousseeuw. Silhouettes: a graphical aid to
EMBOjournal,32(5):617–628,2013. the interpretation and validation of cluster analysis.
Journal of computational and applied mathematics,
[85] MassihRAmini,NicolasUsunier,andCyrilGoutte.
20:53–65,1987.
Learning from multiple partially observed views-an
application to multilingual text categorization. Ad-
vancesinneuralinformationprocessingsystems,22,
2009.
12SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
A.Datasetsdescription
• METABRIC[83]: Consistsof1,440samplesfrombreastcancerpatientswhichareannotatedby8subtypesbasedon
InClust[84]. Weobservetowmodalities,namelytheRNAgeneexpressiondata,andCopyNumberAlteration(CNA)
data. Thedimensionsofthesemodalitiesare15,709and47,127respectively.
• Reuters[85]: Consistsof18,758documentsfrom6differentclasses. Documentsarerepresentedasabagofwords,
usingaTFIDF-basedweightingscheme. ThisdatasetisasubsetoftheReutersdatabase, comprisingtheEnglish
versionaswellastranslationsinfourdistinctlanguages: French, German, Spanish, andItalian. Eachlanguageis
treatedasadifferentview. Tofurtherreducetheinputdimensionswepreprocessthedatawithatruncatedversionof
SVD,turningallinputdimentionsto3,000.
• Caltech101-20[30]: Consistsof2,386imagesof20classes. ThisdatasetisasubsetofCaltech101. Eachviewis
anextracthandcraftedfeature,includingGaborfeature,WaveletMoments,CENTRISTfeature,HOGfeature,GIST
featureandLBPfeature. 1.
• VOC[86]: Consistsof9,963imageandtextpairsfrom20differentclasses. Followingtheconventionsby[87,88],
5,649instancesareselectedtoconstructatwo-viewdataset,wherethefirstandthesecondviewis512Gistfeatures
and399wordfrequencycountoftheinstancerespectively.
• Caltech-5V-7[89]: Consistsof1,400imagesof7classes. SameasCaltech101-20,thisdatasetisalsoasubsetof
Caltech101andiscomprisedfromthesameviewsapartfromtheGaborfeature. 1.
• RBGD[90]: Consistsof1,449samplesofindoorscenesimage-textof13classes. Wefollowtheversionprovided
in[87,91],whereimagefeaturesareextractedfromaResNet50modelpretrainedontheImageNetdatasetandtext
featuresfromadoc2vecmodelpretrainedontheWikipediadataset.
• MNIST-USPS[92]:Consistsof5,000digitsfrom10differentclasses(digits).MNIST,andUSPSarebothhandwritten
digitaldatasetsandaretreatedastwodifferentviews.
• CCV[93]: Consistsof6,773samplesofindoorscenesimage-textof20classes. Flowingtheconventionin[39]we
usethesubsetoftheoriginalCCVdata. Theviewscompriseofthreehand-craftedfeatures: STIPfeatureswith5,000
dimensionalBag-of-Words(BoWs)representation,SIFTfeaturesextractedeverytwosecondswith5,000dimensional
BoWsrepresentation,andMFCCfeatureswith4,000dimensionalBoWs.
• MSRCv1Consistsof210scenerecognitionimagesbelongingto7categories[94]. Eachimageisdescribedby5
differenttypesoffeatures.
• Scene15Consistsof4,485sceneimagesbelongingto15classes[95].
B.ComplexityAnalysis
Let N represent the number of samples in a mini batch and N the total number of samples in the dataset, L denote
mb
the maximum number of neurons in the model’s hidden layers across all views, and l indicates the dimension of the
low-dimensionalembeddingspace. Additionally,L′standsforthenumberofneuronsintheclusterhead,andthenumberof
clustersisdenotedasK. Forreliablelabels,wedenoteN(cid:101)mbasthenumberofsamplesinthepermutedminibatch.
(cid:24) (cid:25)
ThetimecomplexityoftrainingeachautoencoderisO(L·N · N ). WithM views,thetotalcomplexityforthe
mb Nmb
(cid:24) (cid:25)
pretrainingphaseisO(M ·L·N · N ). CalculatingtheCCAlossforeachpairofviewshasacomplexityofO(N2 ·
Nmb mb
(cid:24) (cid:25) (cid:24) (cid:25)
N )foreachpair. Sincethereare(cid:0)M(cid:1) possiblepairs,thetotalcomplexityfortheCCAlossisO(M2·N2 · N ).
Nmb 2 mb Nmb
1ThecreationofbothCaltech101-20andCaltech-5V-7isduetotheunbalanceclassesinCaltech-101.
13SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
Table3. Datasetsusedinourexperiments.
Dataset #Samples #Classes(K) #Views Dimensions Ref
METABRIC 1440 8 2 [15709,47127] [83]
Reuters 18758 6 5 [3000]×5 [85]
Caltech101-20 2386 20 6 [48,40,254,1984,512,928] [30]
VOC 5649 20 2 [512,399] [86]
Caltech-5V-7 1400 7 5 [40,254,1984,512,928] [89]
RBGD 1449 13 2 [2048,300] [90]
MNIST-USPS 5000 10 2 [784,784] [92]
CCV 6773 20 3 [4000,5000,5000] [93]
MSRVC1 210 7 5 [24,576,512,256,254] [94]
Scene15 4485 15 3 [20,59,40] [95]
(cid:24) (cid:25)
ThetimecomplexityfortrainingtheclusterheadisO(L′·N · N ). Computingreliablelabelsforeachviewand
mb Nmb
(cid:24) (cid:25)
eachclusteralsoinvolvesacomplexityofO(M ·N · N ·K). Therefore,thetotalcomplexityforthemulti-view
mb Nmb
(cid:24) (cid:25)
reliablelabelstuningphaseisO(M ·L′·N · N ·K).
mb Nmb
(cid:24) (cid:25)
ComputingCCAlossforreliablelabelsinvolvesacomplexityofO(N(cid:101)2 · N ). Theoveralltimecomplexityofour
mb N(cid:101)mb
multi-viewCCCmodelisthesumofthecomplexitiesoftheindividualphases. Therefore,thetotaltimecomplexityis:
(cid:24) (cid:25) (cid:24) (cid:25) (cid:24) (cid:25) (cid:24) (cid:25)
N N N N
O(M ·L·N mb·
N mb
+M2·N m2 b·
N mb
+M ·L′·N mb·
N mb
·K+N(cid:101) m2 b·
N(cid:101)mb
).
Wherethedominantfactoris:
(cid:24) (cid:25)
N
O(M2·N2 · ).
mb N
mb
C.Experiments
C.1.ImplementationDetails
WeimplementourmodelusingPytorch,andthecodeisavailableforpublicuse2. Allexperimentswereconductedusing
NvidiaA100GPUserverwithIntel(R)Xeon(R)Gold6338CPU@2.00GHz.
C.2.HyperparametersTuning
Totunetheparametersforeachdataset,weutilizetheSilhouetteCoefficient[96]asanunsupervisedmetricforcluster
separability. TheSilhouetteCoefficientiscalculatedusingeachsample’smeanintra-clusterdistanceandthemeannearest-
clusterdistance. Thedistancesarecalculatedonthefusionofthelearnedrepresentations 1 (cid:80) H(v). Wecalculatethe
nv v
SilhouetteCoefficientaftereachepochandpicktheconfigurationthatproducesthemaximalaverageSilhouetteCoefficient
valueacrossalimitedsetofoptions. WepresentthecorrelationbetweenSilhouetteCoefficientvalueandclusteringaccuracy
metricsinTableC.2ondatasetsVOCandMETABRIC.
2ThecodewillbereleasedatGithub.
14SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
Table4. Datasetsusedinourexperiments.
BatchSize ACC ARI NMI SilhouetteCoefficient
VOC
256 58.77 43.24 57.19 0.063
360 62.45 52.06 59.3 0.161
500 60.4 47.33 56.06 0.138
METABRIC
128 45.5 23.7 34.1 0.0396
256 53.9 27.8 37.7 0.0163
360 49.2 23.1 35.3 0.0569
500 46.4 27.9 38.9 0.1088
D.RelationBetweenCOPERandLDA
D.1.FromCOPERtoLDA
θ is treated as two different views θ1 and θ2 where each view is comprised of the same samples from θ but different,
inter-classpermutation.
TocreatetheviewsdifferentpermutationsΠlofθforl=0,1,2...,∞arestacked,wheretheorderofstackedpermutations
differentforthetwodifferentviews.
LemmaD.1. ApplyingCCAonθ1andθ2producesthesameprojectionasapplyingLDAonθwiththe(unknown)labels.
Sincebothθ1andθ2arecomprisedoutofthesamesamples,itfollows:
C =C =C (7)
θ θ1 θ2
And
C =C (8)
θ1θ2 θa
Wecanuseequationsin 1tofindtosolutionforCCA:
C−1C C−1C h =λ h (9)
θ θa θ θa CCA CCA CCA
Whilefromequation 5weknowthatthesolutionforLDAis:
C h =λ C h
a LDA LDA e LDA
Wecanfurtherplugequation 5.1andget:
C h =λ (C−C )h
a LDA LDA a LDA
λ (C−C )h =λ Ch −λ C h
LDA a LDA LDA LDA LDA a LDA
(1+λ )C h =λ Ch
LDA a LDA LDA LDA
λ
C h = LDA Ch
a LDA 1+λ LDA
LDA
λ
C−1C h = LDA h
a LDA 1+λ LDA
LDA
Next,ifwesubstituteC−1C a =Aand 1+λL λD LDA
A
=λ∗in D.1weget:
Ah =λ∗h (10)
LDA LDA
Inadditionifwesubstitutethisin 9weget:
C−1C C−1C h =AAh
θ θa θ θa CCA CCA
AAh =A(Ah )
CCA CCA
A(Ah )=A(λ∗h )
CCA LDA
A(λ∗h )=λ∗(Ah )
LDA LDA
λ∗(Ah )=λ∗(λ∗h )
LDA LDA
λ∗(λ∗h )=λ h
LDA CCA CCA
15SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
Sincethecanonicalcorrelationcoefficientsarethesquarerootoftheeigenvalueobtainedfromthegeneralizedeigenvalue
problemitfollowsthath =h
LDA CCA
D.2.LDAApproximation
First,tosimplifynotation,wedenotedhandhˆ theLDArepresentationandourrepresentationwhichisbasedonpseudo-
labelsthatarepotentiallyincorrect. IntheprevioussectionswesawthatCˆ−1 andCˆ areusedcomputehinequation
θ θa
5.1. Hence, toassessthisequivalence, wedrawattentiontopotentialerrorsinestimatingCˆ fromequation 7, where
θ
Cˆ =Cˆ +Cˆ .
θ θe θa
LetNˆ betheestimatedsamplesforeachclassk,andµˆ it’sestimatedmean. N¯ aretheindicesofsamplesfromclassk
k k k
notincludedinNˆ . N˜ aresamplesnotinclassk,whichareincorrectlyincludedinNˆ ,andN¨ aresamplesinNˆ which
k k k k k
arecorrectlyincluded,thecorrespondingclassforsamplesinN¨ areassumedtobeknownforthisanalysis.
k
ForCˆ = 1 (cid:80)K (xk −µˆ )(xk −µˆ )T,Incompleteinclusionofallinter-classsamplesmaycauseerror. Asthis
θe Nmb k=1 i k i k
dependsonthebatchsizeandthepseudo-labelsbatchsize. Wedenotethistypeoferroras:
K
1 (cid:88) (cid:88)
E1 =− (xk−µˆ )(xk−µˆ )T.
(cid:80)K |N¯ | i k i k
k=1 k k=1i∈N¯
k
Inaddition,falsepseudo-labelscausesamplesfromdifferentclassestobepermutedtogether. Wedenotethistypeoferror
as:
K
1 (cid:88) (cid:88) (cid:88)
E2 = (xk−µˆ )(xq−µˆ )T.
(cid:80)K |N˜ ||N¨ | i k j q
k=1 k k k=1i∈N¨ kj∈N˜
k
whereqisthetrueclassofsamplesj.
ForCˆ θa = NNˆ mk
b
(cid:80)K k=1µˆ k(µˆ k)T. Errorsinestimatingµˆ k,denotedby∆µ k =µ k−µˆ k canbefurtherpropagatedtothe
thirdtypeoferror,denotedas:
Nˆ
E3 =− k ∆µ (∆µ )T
N k k
mb
TogetherallthreetypesoferrorscanbeexpressedasE :=E1+E2+E3:
Takingtheseerrorsintoaccountwecannowtreatthemasaperturbations. Formally:
Cˆ =C +E3
θa θa
Cˆ =C +E1+E2
θe θe
Cˆ =C +E
θ θ
Sincethelatentdimensionissignificantlysmallerthantheinputdimension,itisplausibletoassumethatCˆ isinvertible. In
θ
addition,weexpressthefirstorderapproximationoftheinverseas:
Cˆ−1 =(C +E)−1 =C−1−C−1EC−1
θ θ θ θ θ
Thisisaccurateuptotermsoforder||E||2[79].
16SelfSupervisedCorrelation-basedPermutationsforMulti-ViewClustering
ThismeansthattheestimatedmatrixAfromEq. 10,Aˆ canbewrittenas:
Aˆ=Cˆ−1Cˆ
θ θa
Cˆ−1Cˆ =(C−1−C−1EC−1)(C +E3)
θ θa θ θ θ θa
(C−1−C−1EC−1)(C +E3)=(C−1−C−1EC−1)(C −C +E3)
θ θ θ θa θ θ θ θ θe
(C−1−C−1EC−1)(C −C +E3)=C−1C −C−1EC−1C −
θ θ θ θ θe θ θ θ θ θ
C−1C +C−1EC−1C +C−1E3−C−1EC−1E3
θ θe θ θ θe θ θ θ
C−1C −C−1EC−1C −C−1C +
θ θ θ θ θ θ θe
C−1EC−1C +C−1E3−C−1EC−1E3 =
θ θ θe θ θ θ
I−C−1EI−C−1C +C−1EC−1C +C−1E3−C−1EC−1E3
θ θ θe θ θ θe θ θ θ
AndAcanbeexpressedas:
A=C−1C
θ θa
C−1C =C−1(C −C )
θ θa θ θ θe
C−1(C −C )=C−1C −C−1C
θ θ θe θ θ θ θe
C−1C −C−1C =I−C−1C
θ θ θ θe θ θe
NowwecanestimatetheperturbationfromA,denotedasD:
D =A−Aˆ
A−Aˆ=(I−C−1C )−(I−C−1EI−C−1C +C−1EC−1C +C−1E3−C−1EC−1E3)
θ θe θ θ θe θ θ θe θ θ θ
(I−C−1C )−(I−C−1EI−C−1C +C−1EC−1C +C−1E3−C−1EC−1E3)=
θ θe θ θ θe θ θ θe θ θ θ
C−1E−C−1EC−1C −C−1E3+C−1EC−1E3
θ θ θ θe θ θ θ
Thisperturbationcanbeusedasboundfortheapproximatedeigenvaluesaccordingto[79]:
|λˆ −λ |≤||D|| ,i=1...n (11)
i i 2
17