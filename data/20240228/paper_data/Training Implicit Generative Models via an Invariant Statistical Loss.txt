Training Implicit Generative Models via an Invariant Statistical Loss
Jos´e Manuel de Frutos Pablo M. Olmos Manuel A. V´azquez Joaqu´ın M´ıguez
Universidad Carlos III Universidad Carlos III Universidad Carlos III Universidad Carlos III
jofrutos@ing.uc3m.es
Abstract 1 Introduction
Implicit generative models employ an m-dimensional
Implicit generative models have the capa-
latent random variable (r.v.) z to simulate ran-
bility to learn arbitrary complex data dis-
dom samples from a prescribed n-dimensional tar-
tributions. On the downside, training re-
get probability distribution. To be precise, the la-
quirestellingapartrealdatafromartificially-
tent variable undergoes a transformation through a
generated ones using adversarial discrimina- deterministic function g , which maps Rm (cid:55)→ Rn
θ
tors, leading to unstable training and mode-
using the parameter set θ. Given the model capa-
dropping issues. As reported by Zahee et
bility to generate samples with ease, various tech-
al. (2017), even in the one-dimensional
niques can be employed for contrasting two sam-
(1D) case, training a generative adversar-
ple collections: one originating from the genuine
ial network (GAN) is challenging and of-
data distribution and the other from the model dis-
ten suboptimal. In this work, we develop a
tribution. This approach essentially constitutes a
discriminator-free method for training one-
methodologyfortheapproximationofprobabilitydis-
dimensional (1D) generative implicit mod-
tributions via comparison. Generative adversarial
els and subsequently expand this method to
networks (GANs) [Goodfellow et al., 2014], f-GANs
accommodate multivariate cases. Our loss
[Nowozin et al., 2016], Wasserstein-GANs (WGANs)
function is a discrepancy measure between a
[Arjovsky et al., 2017], adversarial variational Bayes
suitably chosen transformation of the model
(AVB) [Mescheder et al., 2017], and maximum mean-
samplesandauniformdistribution; hence,it
miscrepancy (MMD) GANs [Li et al., 2017] are some
is invariant with respect to the true distri-
popular methods that fall within this framework.
bution of the data. We first formulate our
method for 1D random variables, providing Approximation of 1-dimensional (1D) parametric dis-
an effective solution for approximate repa- tributions is a seemingly naive problem for which the
rameterizationofarbitrarycomplexdistribu- above-mentioned models can perform below expecta-
tions. Then,weconsiderthetemporalsetting tions. In [Zaheer et al., 2017], the authors report that
(both univariate and multivariate), in which various types of GANs struggle to approximate rel-
wemodeltheconditionaldistributionofeach atively simple distributions from samples, emerging
sample given the history of the process. We with MMD-GAN as the most promising technique.
demonstrate through numerical simulations However, the latter implements a kernelized extension
that this new method yields promising re- of a moment-matching criterion defined over a repro-
sults, successfully learning true distributions ducingkernelHilbertspace, andconsequently, theob-
in a variety of scenarios and mitigating some jective function is expensive to compute.
ofthewell-knownproblemsthatstate-of-the-
In this work, we introduce a novel approach to train
art implicit methods present.
univariateimplicitmodelsthatreliesonafundamental
property of rank statistics. Let r < r < ··· < r be
1 2 k
a ranked (ordered) sequence of independent and iden-
tically distributed (i.i.d.) samples from the generative
Proceedings of the 27thInternational Conference on Artifi- model with probability density function (pdf) p˜, and
cial Intelligence and Statistics (AISTATS) 2024, Valencia, let y be a random sample from a pdf p. If p˜=p, then
Spain. PMLR: Volume 238. Copyright 2024 by the au- P(r ≤ y < r ) = 1 for every i = 1,...,K + 1,
thor(s). i−1 i K
with the convention that r = −∞ and r = ∞
0 K+1
4202
beF
62
]GL.sc[
1v53461.2042:viXraTraining Implicit Generative Models via an Invariant Statistical Loss
(a) ISLestimation (b) MMD-GANestimation (c) 7-DayElectricity-CISLForecast
Figure 1: In (a), we show the ISL estimation of the pdf of a 1D random variable with a true distribution that is a
mixture model with two components: a Gaussian N(−5,2) distribution and a Pareto(5,1) distribution. In (b), we show
theperformanceofMMD-GANin[Zaheer et al., 2017]forthesametarget. Intheleft-sideofbothfigures,wecomparethe
optimaltransformationfromaN(0,1)tothetargetdistributionversusthegeneratorlearnedbyISL(a)andMMD-GAN
(b). In (c), we show the temporal ISL-based 7-Day Forecast in the Electricity-C dataset [Trindade, 2015].
(see, e.g., [Rosenblatt, 1952] or [Elvira et al., 2016a] parametrized by a θ ∈ Rl. The aim is to use the
for a short explicit proof). This invariant property data y ,...,y to learn the parameters θ such that
1 N
holds for any continuously distributed data, i.e., for y˜ = g (z) is distributed according to the pdf of the
θ
any data with a pdf p. Consequently, even if p is un- data, p.
known,wecanleveragethisinvariancetoconstructan
objective(loss)function. Thisobjectivefunctionelim-
2.2 Implicit generative models
inatestheneedforadiscriminator,directlymeasuring
the discrepancy of the transformed samples with re-
Implicit generative models train a generator neural
spect to (w.r.t.) the uniform distribution. The com-
network (NN), parametrized by θ and represented as
putationalcostofevaluatingthislossincreaseslinearly
g , to transform samples z ∼ p into y˜ = g (z) with
withbothK andN,allowingforlow-complexitymini- θ 0 θ
pdf p˜such that, ideally, p≈p˜. Throughout the train-
batchupdates. Moreover, theproposedcriterionisin-
ing process, the emphasis is placed on computing the
variant across true data distributions, hence we refer
discrepancy, d, between p and p˜, which becomes the
to the resulting objective function as invariant statis-
loss function of an optimization problem. This dis-
tical loss (ISL). Because of this property, the ISL can
crepancy reaches zero if and only if p = p˜. In this
be exploited to learn multiple modes in mixture mod-
paper, we design a loss function d that measures the
els and different time steps when learning temporal
discrepancy between p and p˜, drawing upon the exis-
processes. Additionally, considering the marginal dis-
tence of a statistic that is uniform whenever the den-
tributions independently, it is straightforward to ex-
sities are equal. In particular, the proposed statistic
tend ISL to the multivariate case. In both scenar-
is invariant w.r.t. p and it is based on the comparison
ios, as illustrated in Figure 1, our approach enhances
between a set of i.i.d samples generated by our model
trainingrobustnessbyavoidingthemin-maxoptimiza-
and the real data.
tionproblemandoutperformsGANs,includingMMD-
GAN and WGAN, and state-of-the-art deep temporal
generative models.
3 Uniform rank statistics
2 Preliminaries
InSection3.1weconstructasimplerankstatisticthat
canbeprovedtobeuniformforanyi.i.d. randomsam-
Inthissectionweformallystatetheproblemofdensity
ple from a continuous distribution with pdf p. Then,
estimation, and introduce the main ideas that led to
in Section 3.2 we prove that one can use data from
the design of the proposed loss function.
anunknownpdfptoconstructrankstatisticsthatare
approximately uniform when p˜≈ p (and converge to
2.1 Problem statement
uniform r.v.’s when p˜→ p, where p˜ is the pdf of the
generative model).
Let y ,...,y be real i.i.d. samples from a probabil-
1 N
ity distribution with pdf p, let z be a random sample Finally, in Section 3.3 we prove that when the pro-
from a canonical distribution with pdf p (e.g., stan- posed rank statistics are uniformly distributed then
0
dard Gaussian) and let g : R (cid:55)→ R be a real map p=p˜almost everywhere.
θJos´e Manuel de Frutos, Pablo M. Olmos, Manuel A. V´azquez, Joaqu´ın M´ıguez
3.1 Discrete uniform rank statistics 3.3 A converse theorem
Let y˜ ,...,y˜ be a random sample from a univariate So far we have shown that if the pdf of the generative
1 K
real distribution with pdf p˜and let y be a single ran- model,p˜,isclosetothetargetpdf,p,thenthestatistic
domsampleindependentlydrawnfromanotherdistri- A is close to uniform. A natural question to ask is
K
bution with pdf p. We construct the set, whether A displaying a uniform distribution implies
K
(cid:110) (cid:111) that p˜ = p. In this section, we prove that if A K is
A := y˜∈{y˜ }K :y˜<y , 1
K k k=1 uniform, i.e., Q (n) = , for all n ∈ {0,...,K}
K K+1
and the statistic A := |A |, i.e., A is the number and all K, then p=p˜almost everywhere.
K K K
ofelementsofA . WenotethatA isarankstatistic
K K To this end, we recall a key result from
that can be alternatively constructed by first finding
[Elvira et al., 2021].
indices j ,...,j that sort the sample y˜ ,...,y˜ in
1 k 1 K
ascending order, Lemma 1. Let p and p˜ be two univariate pdf’s with
associated cfd’s F and F˜. If the rank statistic A
K
y˜ <y˜ <...<y˜ ,
j1 j2 jK constructed in Section 3 has a uniform distribution
1
and then setting on the set {0,...,K} then (F˜n,p) = , ∀n ∈
n+1
A =sup(cid:110) i∈{1,...,K}:y˜ <y(cid:111) . (1) {0,1,...,K}, where F˜n is the n-th power of F˜.
K ji
Also note that almost surely y ̸= y˜ for all i. The Proof. See [Elvira et al., 2021, Theorem 2].
i
statistic A is a discrete r.v. that takes values on
K
the set {0,...,K} and we denote its probability mass BasedonLemma1wecanprovetheresultanticipated
function (pmf) as Q K : {0,...,K}(cid:55)→[0,1]. This pmf at the beginning of this section.
satisfies the following fundamental result.
Theorem 3. Let p and p˜ be pdf’s of univariate real
Theorem 1. If p = p˜ then Q K(n) = K1
+1
∀n ∈ r.v.’s and let A
K
be the rank statistic constructed in
{0,...,K}, i.e., A K is a discrete uniform r.v. on the Section 3.1. If A K has a discrete uniform distribution
set {0,...,K}. on {0,...,K} for every K ∈ N, then p = p˜ almost
everywhere.
Proof. See [Elvira et al., 2021] for an explicit proof.
This is a basic result that appears under different Proof. Let Y be a r.v. with pdf p and cdf F. From
forms in the literature, e.g., in [Rosenblatt, 1952] or the inversion theorem (see, e.g., [Martino et al., 2018,
[Djuric and M´ıguez, 2010]. Theorem2.1])weobtainthatU =F(Y)isastandard
uniform r.v., i.e., U ∼U(0,1) and as consequence,
3.2 Approximately uniform statistics
1
E[Un]=(Fn,p)= , ∀n∈N.
For f : R(cid:55)→R a real function and p the pdf of an uni- n+1
variate real r.v. let us denote
On the other hand, since A is uniformly distributed
K
(cid:90) ∞ on {0,...,K}, Lemma 1 implies that the r.v. U˜ =
(f,p):= f(x)p(x)dx
F˜(Y) satisfies
−∞
and let B 1(R) := (cid:110) (f : R → R) : sup x∈R|f(x)| ≤ 1(cid:111) E[U˜n]=(F˜n,p)= 1 , ∀n∈N,
n+1
be the set of real functions bounded by 1.
hence,
Thequantitysup |(f,p)−(f,p˜)|canbedirectly
f∈B1(R) U˜ =F˜(Y)∼U(0,1) (2)
relatedtothetotalvariationdistancebetweenthedis-
tributions with pdf’s p and p˜. Hence, the following as well. However, the relationship in Eq. (2) implies
theorem states that if the generator output pdf, p˜, is that Y = F˜−1(U˜) has cdf F˜ (from the inversion the-
close to p then the rank statistic A K with pmf Q K(n) orem again), hence F = F˜, which implies that p = p˜
constructed in Section 3.1 is approximately uniform. almost everywhere.
Theorem 2. If sup |(f,p)−(f,p˜)|≤ϵ then,
f∈B1(R)
4 The invariant statistical loss
1 1
K+1
−ϵ≤Q K(n)≤
K+1
+ϵ, ∀n∈{0,...,K}. function
The proof of this Theorem can be found in the Ap- Taken together, Theorems 1, 4 and 3 imply that if we
pendix. train a generative model with output pdf p˜ to makeTraining Implicit Generative Models via an Invariant Statistical Loss
therandomstatisticsA uniformforsufficientlylarge k ∈ {0,...,K}, replacing sharp bin edges with func-
K
K, then we can expect that p˜≈ p, where p is the ac- tions that mirror bin values at k and smoothly decay
tual pdf of the available data that has been used for outsideaneighborhoodofk. Inourparticularcase,we
training. Thisistrueindependentlyoftheformofthe consider radial basis function (RBF) kernels {ψ }K
k k=0
pdf p and, in this sense, it enables us to construct loss centered at k ∈ {0,...,K} with length-scale ν2, i.e.,
functions which are themselves invariant w.r.t. p. In ψ (a) = exp(−(a−k)2/2ν2). Thus, the approximate
k
this section we introduce one such invariant loss and normalized histogram count at bin k is given by
then propose a suitable surrogate which is differen-
N
tiable w.r.t. the generative model parameters θ and, 1 (cid:88)
q[k]= ψ (a˜ (y )), (3)
therefore, can be used for training using standard op- N k K i
timization tools. i=1
for k = 0,...,K. The proposed ISL is now com-
4.1 The Invariant Statistic Loss puted as ℓ-norm distance between the uniform pmf
1 1 and the vector of empirical probabilities
K+1 K+1
The training data set consists of a set of N i.i.d. sam-
q=[q[0],q[1],...,q[K]]⊤, namely,
ples y ,...,y from the true data distribution p. For
1 N
each y , along with K i.i.d. samples from the gener- (cid:12)(cid:12) (cid:12)(cid:12)
n (cid:12)(cid:12) 1 (cid:12)(cid:12)
ative model y˜ = [y˜ 1,...,y˜ K]⊤, where y˜
i
= g θ(z i), we L ISL(θ,K,α,ν)=(cid:12) (cid:12)(cid:12) (cid:12)K+11 K+1−q(cid:12) (cid:12)(cid:12)
(cid:12)
. (4)
can obtain one sample of the r.v. A , that we denote ℓ
K
as a .
K,n
In Figure 2, we compare the surrogate ISL loss func-
The great advantage of our method is that we sim- tion and the theoretical counterpart for an exemplary
ply have to verify the discrepancy between the dis- case. The set of hyperparameters is listed in Table 1.
crete uniform distribution with K +1 outcomes and Note that evaluating (4) requires O(NK) operations,
the empirical distribution of the set of statistics but we can rely on mini-batch optimization to reduce
a K,1,a K,2,...,a K,N, i.e., their associated histogram. ittoO(MK),beingM themini-batchsize. Algorithm
On the downside, note that, by definition, the con- 1 summarizes the training method.
struction of the statistic A in Eq. (1) from samples
K
of the generator does not yield a function that is dif-
ferentiable w.r.t. its parameters θ.
Wenowintroduceanewlossfunction,coinedinvariant
statistical loss (ISL) that can be optimized w.r.t. θ
and mimics the construction of a histogram from the
statistics a ,a ,...,a . Forthis purpose, given
K,1 K,2 K,N
a real data point y, we can tally how many of the K
simulated samples in y˜ are less than the observation
y. Specifically, one computes
K K
(cid:88) (cid:88)
a˜ (y)= σ (y−y˜)= σ (y−g (z )),
K α i α θ i Figure 2: ISL surrogate loss function vs. ISL theoretical
i=1 i=1 loss during the training of N(4,2), K = 1000, N = 1000,
wherez isasamplefromaunivariatestandardGaus- log-scale.
i
sian distribution, i.e., z ∼ p ≡ N(0,1), σ(x) :=
i 0
1/(1+exp(−x))isthesigmoidfunctionandσ (x):=
α Table 1: ISL list of hyperparameters.
σ(αx). Weremarkthata˜ (y)isadifferentiable(w.r.t.
K
θ)approximationoftheactualstatisticA fortheob-
K Hyperparameter Value
servation y. Theparameterα enablesusto adjust the
Nº samples, K tunable
slopeofthesigmoidfunctiontobetterapproximatethe
Activation function σ(α·x)
(discrete) ‘counting’ in the construction of A (other
K {ψ}K RBF Kernels, length-scale=ν2
alternatives can certainly be chosen here). Sharper k=0
functionsofferbetterapproximationbutunstablegra-
dients. In our case, α ∈ [10,20] has been effective in
4.2 Progressively Training by Increasing K
practice.
Toconstructadifferentiablesurrogatehistogramfrom The parameter K dictates the number of simulated
a˜ (y ),...,a˜ (y ), we leverage a sequence of differ- samples per data point. As established in Section 3,
K 1 K N
entiable functions designed to mimic the bins around larger values of K ensure a better approximation ofJos´e Manuel de Frutos, Pablo M. Olmos, Manuel A. V´azquez, Joaqu´ın M´ıguez
Algorithm 1 ISL Algorithm p(Y[t]|Y[0],Y[1],··· ,Y[t−1]).
1: InputNeuralNetworkg θ;HyperparameterK,N; Giventhesequencey[0],y[1],...,y[t−1],ourgoalisto
Number of Epochs; Batch Size M; Training Data train an autorregressive conditional implicit generator
networkg (z,h[t])thatproducessampleswhosedistri-
{y }N . θ
i i=1 bution, referred to as p˜, is close to the unknown pdf
t
2: Output Trained Neural Network g . p when z is a sample drawn from a standard Gaus-
θ t
sian. The new input to our generator NN, h[t], is
3: For t=1,...,epochs do
an embedding of the sequence y[0],y[1],...,y[t − 1]
4: For iteration=1,...,N/M do provided by a suitable neural network. Specifically,
5: Select M samples from {y }N at random we consider a simple recurrent neural network (RNN)
j j=1
connectedtothegeneratorNNasillustratedinFigure
6: {z }K ∼N(0,1)
i i=1 3. During training, the newly-arrived observation at
(cid:16) (cid:17)
7: q= 1 (cid:80)M ψ (cid:80)K σ (y −g (z )) time t, y[t], is fed into the RNN to collapse the pro-
M j=1 k i=1 α j θ i
(cid:12)(cid:12) (cid:12)(cid:12) cess history into hidden state h[t]. The latter is then
8: ∇ loss←∇ (cid:12)(cid:12) 1 1 −q(cid:12)(cid:12)
θ θ(cid:12)(cid:12)K+1 K+1 (cid:12)(cid:12) exploited (along with input noise z) by generator NN
2
9: Backpropagation(g ,∇ loss) g (z,h[t])topredictthenextobservationy˜[t+1]. Dur-
θ θ θ
ingtest/validation,forecastingisperformedbyfeeding
10: return g
θ
y˜[t+1] back to the RNN.
y[t] y[t+1] y[t+2]
the true data distribution, albeit at the expense of an
h[t−1] h[t] h[t+1] h[t+2]
increased computational effort. RNN RNN RNN
We have found that the training procedure can be
made more efficient if it is performed in a sequence of h[t] h[t+1] h[t+2]
increasing values of K, say K(1) <K(2) <···<K(I),
where I is the total number of stages in the process g g g
z θ z θ z θ
andK(I) =K isthemaximumadmissiblevalueof
max
K –a hyperparameter to be chosen depending on the
y˜[t+1] y˜[t+2] y˜[t+3]
availablecomputingpower. Hence,atthei-thstagewe
trainthegenerativemodelusingtheISLL (θ,K(i)).
ISL Figure 3: Conditional implicit generative model for
During training, we periodically run a test (e.g., a
time-series prediction.
Pearson χ2 test against the discrete uniform distribu-
tion using the statistics a ,a ,...,a ). When
K,1 K,2 K,N Notice that in this new temporal setup, all the re-
thehypothesis“A isuniform”isacceptedweincrease
K sultsestablishedintheprevioussectionsremainvalid.
K(i) to K(i+1) for the (i+1)-th stage and keep train-
However,whilebeforewehadasinglestatisticA ,we
ing. Note that the generative model parameters ad- K
now have one at every time instant, A [t]. The latter
justedatthei-thstageserveasaninitialconditionfor K
is still constructed according to Eq. (1), and will fol-
training at the (i+1)-th stage. We have followed this
low a uniform distribution if p˜ is close enough to p .
methodology for all the experiments presented in this t t
This result is invariant w.r.t. t and p .
paper. t
We use the sequence of observations y =
[y[0],y[1],...,y[T]] to construct a sequence of
5 Invariant Statistical Loss for Time
statistics a [0],a [1],...,a [T], whose aggregated
Series Prediction K K K
empirical distribution should be approximately
uniform if p˜ ≈p for t=0,...,T.
t t
So far we have focused our discussion on 1D ran-
To construct the ISL, we apply a similar procedure to
dom variables, but our approach is amenable to be
obtain a differentiable histogram surrogate.
used on (scalar and multivariate) time series. Let
y[0],y[1],...,y[T] be a sample of a discrete-time ran- When observations are multivariate, i.e., we have a
dom process between time1 0 and time T, and collection of time series2, {y }N , we can essentially
i i=1
let Y[t] be the random variable of the process at apply the same procedure. Specifically, for every
time t, with unknown conditional distribution p t = time series, y i, we obtain a sequence of statistics
1Weassumewithnolossofgeneralitythattheoriginof 2Assumeforsimplicitythattheyareofthesamelength
the process is at time t=0. T, but this is not a constraint here.Training Implicit Generative Models via an Invariant Statistical Loss
a = [a [0],a [1],...,a [T]]. If p˜ ≈ p for [Salinas et al., 2020], and transformer architectures
i,K i,K i,K i,K t t
t=0,...,T, then again the agreggated empirical dis- [Zeng et al., 2023, Wu et al., 2021, Zhou et al., 2021,
(cid:83) (cid:83) (cid:83)
tribution of a a ... a , should be ap- Li et al., 2019a] with simpler network structures in
1,K 2,K T,K
proximately uniform. The ISL surrogate requires in both univariated and multivariated scenarios.
thisgeneralscenarioO(TNK)operationstoevaluate,
which can be reduced to O(WMK) if we use mini-
batches of M data points and we consider a sliding
7 Experiments
window of length W.
6 Related Work Ourexperimentsevaluateimplicitmodelstrainedwith
the ISL in Eq. (4) for density estimation. We show-
case its qualitative advantage over classical genera-
Among existing methods for training implicit
tive models for independent time settings, capturing
generative models, most of the works in the
multimodal and heavy-tailed distributions. ISL out-
literature consider the use of a discrimina-
performs vanilla-GAN, Wasserstein-GAN, and MMD-
tor/critic network that classifies whether data
GAN baselines. ISL also shows promise when com-
points are artificially generated or are real sam-
paredtomoremodernmethodssuchasdiffusionmod-
ples [Mohamed and Lakshminarayanan, 2016].
els. Inaddition,weexploreitspotentialasaGANreg-
From basic GANs [Goodfellow et al., 2014], to
ularizer. Ourexperimentalsettingsforthissectionare
WGANs [Arjovsky et al., 2017] and f-GANs
grounded in the study by [Zaheer et al., 2017]. Sub-
[Nowozin et al., 2016], this approach is known to
sequently, we assess ISL’s effectiveness in time series
suffer from mode collapse and training instabilities
forecasting using synthetic and real datasets.
[Arora and Zhang, 2017, Arora et al., 2017]. These
issues can be alleviated with techniques such as In the supplementary material, we extend the exper-
mini-batch discrimination [Salimans et al., 2016] and imental validation by comparing different NN hyper-
spectral normalization [Miyato et al., 2018]. parameters and several configurations of the proposed
experimental settings.
Among training methods for implicit models
that do not require a discriminator, Maximum
Mean Discrepancy (MMD) losses are a popu-
7.1 Learning 1-D distributions
lar choice [Gretton et al., 2012, Li et al., 2015,
Gouttes et al., 2021]. Incorporating MMD ensures
thatthedistancebetweentherealandgenerateddata We start considering the same experimental setup as
distributions is minimized in a Reproducing Kernel [Zaheer et al., 2017]. In particular, we aim at approx-
Hilbert Space, leading to more stable training and imating different target distributions using 1000 i.i.d.
nuanced density approximations. However, due to the samples.
reformulation in terms of kernels, the complexity of
The final three rows describe mixture models, each
evaluating the loss is large (scales quadratically with
with equal weighting among their components,
the amount of data), and approximations are needed.
Model consistsofamixturemodelthatcombinestwo
The literature on implicit temporal generative 1
normal distributions, N(5,2) and N(−1,1). Model
models is comparatively much shorter since the 2
is a mixture model that integrates three normal dis-
best-performing methods nowadays are based
tributions N(5,2), N(−1,1) and N(−10,3). Finally,
on prescribed conditional autoregressive models
Model combinesanormaldistributionN(−5,2)with
combined with sequential transformer architec- 3
a Pareto distribution Pareto(5,1).
tures, conditional normalizing flows, or difus-
sion models [Salinas et al., 2020, Li et al., 2019a, As generator NN, we use a 4-layer multilayer percep-
Moreno-Pino et al., 2023, Rasul et al., 2021b, tron (MLP) with 7,13,7 and 1 units at the corre-
Rasul et al., 2021a]. To our knowledge, due to the sponding layers. In the supplementary material we
inherent invariance, ISL is the first method to adapt compare the ISL performance with different architec-
implicit generative models to the temporal domain in tures. As activation function we use a exponential
a straightforward manner. As we demonstrate, ISL linear unit (ELU). We train each setting up to 104
achieves competitive performance when compared epochs with 10−2 learning rate using Adam. We com-
to prescribed autoregressive generative meth- pare ISL, GAN, WGAN, and MMD-GAN using KSD
ods such as Phrophet [Taylor and Letham, 2018], (Kolmogorov-Smirnov Distance), MAE (Mean Abso-
TRMF [Yu et al., 2016], Deep State Space Mod- luteError),andMSE(MeanSquaredError)errormet-
els (DSSM) [Rangapuram et al., 2018], DeepAR rics, defined as followsJos´e Manuel de Frutos, Pablo M. Olmos, Manuel A. V´azquez, Joaqu´ın M´ıguez
Table 2: Comparison of ISL results with vanilla GAN, WGAN, and MMD-GAN under the hyperparameters: Noise
∼N(0,1), K =10, epochs=1000, and N =1000.
max
ISL GAN WGAN MMD-GAN
Target KSD MAE MSE KSD MAE MSE KSD MAE MSE KSD MAE MSE
N(4,2) 8.5e−3 0.1005 0.0348 0.0154 0.4335 0.4015 0.0468 4.4644 55.1103 0.0238 4.2689 30.3614
U(−2,2) 0.0125 0.1341 0.0258 0.0265 0.1097 0.0180 0.0790 0.5659 0.4025 0.0545 0.5322 0.4130
Cauchy(1, 2) 6.3e−3 7.4957 8177 0.0823 8.9628 8207 0.0302 10.5681 8127 0.0263 9.6798 57975
Pareto(1, 1) 0.0822 4.7742 7843 0.0987 12.6397 114970 0.4919 7.6435 7062 0.5008 9.023 10674
Model 0.0169 0.2995 0.1437 0.0116 0.4246 0.4537 0.0315 1.8831 4.8943 0.0450 1.6433 4.6988
1
Model 0.0173 0.6660 0.7757 0.0536 0.6072 0.7506 0.0459 4.2390 28.9843 0.0232 8.7218 112
2
Model 0.1591 0.8791 1.5207 0.1846 0.4447 1.6053 0.2986 3.0376 13.1256 0.5608 3.2046 21.5244
3
Table 3: ISL vs Diffusion model (1D case). 7.2 GAN pre-training
ISL Diff. Our method can be used in combination with
Target KSD KSD
a GAN pre-trained generator. While GANs of-
N(4,2) 8.5e-3 0.0867
ten struggle with multimodal distributions, typi-
U(−2,2) 0.0125 0.2307
Cauchy(1, 2) 6.3e-3 0.0226 cally capturing only some modes [Arora et al., 2017,
Pareto(1, 1) 0.0822 0.0512 Arora and Zhang, 2017],ourapproachfullyrepresents
Model 1 0.0169 0.1507 thesupportofthetruedistribution. Ifnot, disparities
Model 0.0173 0.2217
2 wouldarisebetweenfictionalandrealdata. Especially
Model 0.1591 0.05475
3 with a large K, missed modes would cause fictional
samples to diverge from real data modes, biasing the
generated histogram. In this subsection, we now as-
sess the ability of ISL to correct a potentially biased
KSD=sup|F(x)−F˜(x)|,
GAN construction of the generator g (z).
x θ
(cid:90) ∞
Pre-training ISL with a GAN could potentially im-
MAE= |f(z)−f (z)|p (z)dz,
θ z prove results if practitioners notice that ISL is suf-
−∞
(cid:90) ∞ fering from a vanishing gradient problem due to poor
(cid:0) (cid:1)2
MSE= f(z)−f θ(z) p z(z)dz, parameter initialization. This strategy has only been
−∞
usedfortheexperimentsinTable4. Fortherestofthe
where we have denoted by F, F˜, and F the cdfs as- 1Ddensityapproximationsandtimeseriesforecasting
Z
sociated with the densities p, p˜, and p respectively. experiments, no pre-training was applied.
Z
Observe in Table 2 that ISL outperforms the rest We use the same parameters as before for the
of methods in most scenarios, especially in the case generator. For the GAN critics, following
of mixture models. Referring to [Zaheer et al., 2017, [Zaheer et al., 2017], we use a 4-layer MLP with
Theorem1],itassertsthatfora1Ddistribution,there 11, 29, 11, and 1 units. The ratio of updating critics
are at most two continuous transformations f such and generators are searched in {2:1,3:1,4:1,5:1}.
that if Z follows p and X follows p, then f(Z) = X. We train each setting for 104 epochs.
0
These transformations can be derived using the prob-
Upon training the GAN, we train the generator using
ability integral transform. In Figure 1 we compare
ISLwithK =10,1000epochs,andN =1000sam-
thegeneratorfunctiong (z)withthetruetransforma- max
θ ples. TheresultsaredisplayedinTable4andtheycan
tion function f(z) for ISL (a) and MMD-GAN (b) for
directly compare these results with the ones obtained
Model .
3 in [Zaheer et al., 2017]. The comparison w.r.t. Table
In Table 3 we compare ISL vs Diffusion model (1D 2showsthatpre-trainingwithaGANcanimprovethe
case). The latter is based on an MLP with three lay- performance of the ISL method.
ers of 100 units each, integrated with 100-dimensional
positional encodings for time steps. It has been op- 7.3 Time Series
timized for 20 epochs using MSE as a loss function,
and linearly spaced betas (0.0004 to 0.06) across 100 Experiments are performed using both synthetic and
denoising steps. For further information and details real-world datasets to demonstrate the enhanced fore-
aboutparameters,referto[Scotta and Messina, 2023]. casting capabilities of the proposed method.Training Implicit Generative Models via an Invariant Statistical Loss
Table4: ISLcombinedwithaGANpre-trainedgenerator: Left-ResultsafterapplyingISLmethod;Right-GANresults
before ISL method. Hyperparameters: Noise ∼N(0,1), K =10, epochs=1000, N =1000.
max
GAN+ISL GAN
Target KSD MAE MSE KSD MAE MSE
N(23,1) 7.4e−3 0.0881 0.0119 0.0170 0.1995 0.0549
U(22,24) 0.0287 0.1092 0.0327 0.4488 0.3942 0.2221
Cauchy(23, 1) 9.43e−3 6.9527 937.3329 0.1278 34.8575 21154
Pareto(23, 1) 0.0264 0.6328 0.8890 0.0474 131.0215 21418626
Model 5.63e−3 0.6328 0.8890 0.0160 0.4165 0.4584
1
Model 0.0161 0.4607 0.3350 0.0399 0.8484 0.9670
2
Model 0.0805 0.4507 0.3373 0.1378 0.7628 1.2044
3
7.3.1 Synthetic Time Series 7.3.2 Real World datasets
To begin, we conduct synthetic experiments to illus-
We evaluated our model using several real-world
trateourmethod’sproficiencyingraspingtheintrinsic
datasets. The ‘electricity-f’ dataset captures 15-
probability distribution of the time series. We delve
minute electricity consumption of 370 customers
into the scenario involving autoregressive models with
[Trindade, 2015]. The ‘electricity-c’ dataset aggre-
diverse parameters.
gates this data into hourly intervals.
Consider a simple autoregressive process AR(p) of
the form X =
(cid:80)p
ϕ X + ξ . In Table 5 we
Training employed 2012-2013 data, and validation
t i=1 i t−i t used 20 months starting from 15-April 2014. For this
consider different parameter settings for ϕ and the
i
purpose, we have used a 2-layer RNN layers with 3
noise variance, ξ. We display the forecasting perfor-
units each and a 2-layer MLP with 10 units each and
mance of both temporal ISL compared to DeepAR
a ReLU activation function. In Figure 1c we show
[Salinas et al., 2020] when the forecasting starts upon
the ISL 7-day forecast of one of the instances of the
observing τ samples of the process. As error met-
0
‘electricity-c’ dataset. In Tables 6, 1 we compare the
rics, we consider the Normal Deviation (ND), Root-
ISL performance with ARIMA [Wilson, 2016],
mean-square deviation (RMSE), and Quantile Loss
Phrophet [Taylor and Letham, 2018], TRMF
(QL ) metrics (For further details, please refer to
ρ=0.9
[Yu et al., 2016], a RNN based State Space Model,
[Moreno-Pino et al., 2023]).
DSSM [Rangapuram et al., 2018], a transformer-
In this scenario, we have used for both methods (ISL based model (ConvTrans) [Li et al., 2019b], and
and DeepAR) a two-layer RNN with 10 units each, DeepAR [Salinas et al., 2020]. As reported, with
utilizing a rectified linear unit (RELU) as activation a relatively simple NN structure, ISL outperforms
function. This network will be connected with an all baselines in all cases, except for ConvTrans for
MLP, consisting of two layers with 16 units each, ap- the 1-day forecast using the metric QL . Note,
ρ=0.5
plying RELU and identity activation functions. The however that ConvTrans is much more complex than
learning rates will be ascertained within the range of theproposedmodel, intermsoftheNNstructureand
{10−2,10−3,10−4}fortheAdamsoptimizer. Wetrain thenumberofparameters. Thisresultprovesthatthe
each model on 200 signals of 1000 elements each. inherent flexibility of the implicit model can compete
withmorecomplexprescribedmodels,usuallyapplied
in time-series forecasting. In Table 8, we make a
Table 5: Forecasting an Autoregressive Model AR(p),
comparison of ISL with different state-of-the-art
with noise ξ =N(0,0.01).
transformer architectures on various databases (see
[Zeng et al., 2023] for details). In this case, we have
Meths τ0 {ϕi}p
i
ND RMSE QLρ=0.9 used a 1-layer RNN with 5 units, followed by a Batch
20 .5,.3,.2 0.762±0.37 3.788±1.84 0.718±0.69 Normalization layer, and a 2-layer MLP with 10 units
.5,.2 0.104±0.03 0.551±0.14 0.174±0.06
ISL in each layer. The MLP uses a ReLU activation
50 .5,.3,.2 0.850±0.23 6.553±1.94 0.660±0.60
.5,.2 0.155±0.09 1.206±0.64 0.039±0.02 function in the first layer, an identity activation
20 .5,.3,.2 1.031±0.05 4.970±0.49 1.269±0.80 function in the last layer, and a 5% dropout rate in
DeepAR .5,.2 1.009±0.03 5.537±0.34 1.041±0.49 the first layer. We observe that with a very simple
50 .5,.3,.2 1.000±0.01 7.630±0.61 0.650±0.59
.5,.2 1.006±0.02 8.811±0.35 1.012±0.34 architecture, ISL is competitive and often capable of
surpassing transformer architectures for time series
prediction.Jos´e Manuel de Frutos, Pablo M. Olmos, Manuel A. V´azquez, Joaqu´ın M´ıguez
Table 6: Evaluation summary, using QL /QL Table 8: Univariate long sequence time-series forecasting
ρ=0.5 ρ=0.9
metrics, on electricity-c datasets, forecast window 1 and 7 results on four datasets.
days.
DB τ Autoformer Informer LogTrans ISL
MSE MAE MSE MAE MSE MAE MSE MAE
Method electricity-c electricity-c
1d 7d 96 0.071 0.206 0.193 0.377 0.283 0.468 0.113 0.239
ARIMA 0.154/0.102 0.283/0.109 ETTh1 192 0.114 0.262 0.217 0.395 0.234 0.409 0.168 0.294
336 0.107 0.258 0.202 0.381 0.386 0.546 0.208 0.333
Prophet 0.108/0.099 0.160/0.154
720 0.126 0.283 0.183 0.355 0.475 0.629 0.234 0.369
ETS 0.101/0.077 0.121/0.101 96 0.153 0.306 0.213 0.373 0.217 0.379 0.184 0.335
TRMF 0.084/− 0.087/− 192 0.204 0.351 0.227 0.387 0.281 0.429 0.177 0.333
ETTh2
DSSM 0.083/0.056 0.085/0.052 336 0.246 0.389 0.242 0.401 0.293 0.437 0.175 0.334
720 0.268 0.409 0.291 0.439 0.218 0.387 0.181 0.342
DeepAR 0.075/0.040 0.082/0.053
96 0.056 0.183 0.109 0.277 0.029 0.171 0.065 0.208
ConvTras 0.059/0.034 0.076/0.037 192 0.081 0.216 0.151 0.310 0.157 0.317 0.146 0.276
ETTm1
ISL 0.062/0.0189 0.063/0.021 336 0.076 0.218 0.427 0.591 0.289 0.459 0.107 0.242
720 0.110 0.267 0.438 0.586 0.430 0.579 0.092 0.224
96 0.241 0.387 0.591 0.615 0.279 0.441 0.193 0.298
192 0.273 0.403 1.183 0.912 1.950 1.048 0.198 0.300
Table 7: Evaluation summary, using QL ρ=0.5/QL ρ=0.9 Exch. 336 0.508 0.539 1.367 0.984 2.438 1.262 0.201 0.301
metrics, on electricity-f datasets, forecast window 1 day. 720 1.111 0.860 1.872 1.072 2.010 1.247 0.201 0.299
Table9: Multivariatelongsequencetime-seriesforecasting
Method electricity-f
1d results on four datasets.
TRMF 0.094/−
DeepAR 0.082/0.063
DB τ Autoformer Informer LogTrans ISL
ConvTrans 0.074/0.042
MSE MAE MSE MAE MSE MAE MSE MAE
ISL 0.050/0.036 96 0.449 0.459 0.865 0.713 0.878 0.740 0.384 0.459
192 0.500 0.482 1.008 0.792 1.037 0.824 0.380 0.511
ETTh1
336 0.521 0.496 1.107 0.809 1.238 0.932 0.334 0.392
720 0.514 0.512 1.181 0.865 1.135 0.852 0.747 0.667
96 0.358 0.397 3.755 1.525 2.116 1.197 0.305 0.435
7.3.3 Multivariate time series 192 0.456 0.452 5.602 1.931 4.315 1.635 0.449 0.525
ETTh2
336 0.482 0.486 4.721 1.835 1.124 1.604 0.410 0.511
Wehaveextendedourmodeltoincorporatemultivari- 720 0.515 0.511 3.647 1.625 3.188 1.540 0.400 0.499
96 0.672 0.571 0.543 0.510 0.600 0.546 0.408 0.508
ate scenarios. As described in Section 5, our method- 192 0.795 0.669 0.557 0.537 0.837 0.700 0.573 0.623
ETTm1
ology entails independently fitting the marginal dis- 336 1.212 0.871 0.754 0.655 1.124 0.832 0.702 0.670
720 1.166 0.823 0.908 0.724 1.153 0.820 0.768 0.686
tribution for each respective output and training the 96 0.255 0.339 0.365 0.453 0.768 0.642 0.276 0.443
average ISL. Comprehensive results of this approach, ETTm2 192 0.281 0.340 0.533 0.563 0.989 0.757 0.335 0.463
336 0.339 0.372 1.363 0.887 1.334 0.872 0.305 0.455
comparing ISL versus transformer techniques in mul- 720 0.433 0.432 3.379 1.338 3.048 1.328 0.427 0.546
tidimensional cases, are presented in Table 9. In this
case, we have use the same architecture as the one
described for the results of the Table 8. Several potential research paths exist. A straightfor-
ward direction is the use of random projections to ad-
As we observe, with an RNN-based structure and a
vanceISLcapabilitiesforgeneratingmultidimensional
simple MLP implicit model, ISL (≈25K parameters)
data, such as images [Deshpande et al., 2018]. Addi-
achieves forecasting accuracy better than many state-
tionally,asignificantresearchtrajectorywouldinvolve
of-the-artmodels,whicharebasedonmuchmorecom-
accuratelyestablishingtheorderofconvergenceofthe
plex structures with millions of parameters such as
method in relation with its parameters K and N.
transformers [Zeng et al., 2023].
Acknowledgments
8 Conclusion and Future Work
This work has been partially supported by the
We have introduced a novel criterion to train univari- the Office of Naval Research (award N00014-22-1-
ate implicit generator functions that simply requires 2647) and Spain’s Agencia Estatal de Investigaci´on
checkinguniformityonthestatisticdefinedbyEq. (1). (refs. PID2021-125159NB-I00 TYCHE and PID2021-
We designed a surrogate loss function to optimize the 123182OB-I00 EPiCENTER). Pablo M. Olmos also
neuralnetworkmodels. Inexperiments, weshowcased acknowledges the support by the Comunidad de
the method’s ability to capture complex 1D distri- Madrid under grants IND2022/TIC-23550 and ELLIS
butions, including multimodal cases and heavy tails, Unit Madrid.
distinguishing it from traditional generative models
that frequently face challenges such as mode collapse.
References
Additionally, our model demonstrated effectiveness in
handling univariate and multivariate time series data [Alexandrov et al., 2020] Alexandrov,A.,Benidis,K.,
with complex underlying density functions. Bohlke-Schneider, M., Flunkert, V., Gasthaus, J.,Training Implicit Generative Models via an Invariant Statistical Loss
Januschowski, T., Maddix, D. C., Rangapuram, [Elvira et al., 2016b] Elvira, V., M´ıguez, J., and
S., Salinas, D., Schulz, J., et al. (2020). Gluonts: Djuri´c, P. M. (2016b). Online adaptation of
Probabilistic and neural time series modeling in the number of particles of smc methods. In
python.TheJournalofMachineLearningResearch, 2016 IEEE International Conference on Acous-
21(1):4629–4634. tics, Speech and Signal Processing (ICASSP),pages
4378–4382. IEEE.
[Arjovsky et al., 2017] Arjovsky,M.,Chintala,S.,and
Bottou, L.(2017). Wassersteingenerativeadversar- [Elvira et al., 2021] Elvira,V.,Miguez,J.,andDjuri´c,
ial networks. In International conference on ma- P. M. (2021). On the performance of particle filters
chine learning, pages 214–223. PMLR. with adaptive number of particles. Statistics and
Computing, 31:1–18.
[Arora et al., 2017] Arora, S., Ge, R., Liang, Y., Ma,
T., and Zhang, Y. (2017). Generalization and equi-
[Goodfellow et al., 2014] Goodfellow, I., Pouget-
librium in generative adversarial nets (GANs). In
Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
Internationalconferenceonmachinelearning,pages
Ozair, S., Courville, A., and Bengio, Y. (2014).
224–232. PMLR.
Generative adversarial nets. Advances in neural
[Arora et al., 2018] Arora, S., Risteski, A., and information processing systems, 27.
Zhang, Y. (2018). Do GANs learn the distribution?
[Gouttes et al., 2021] Gouttes, A., Rasul, K., Koren,
some theory and empirics. In International Confer-
M., Stephan, J., and Naghibi, T. (2021). Proba-
ence on Learning Representations.
bilistictimeseriesforecastingwithimplicitquantile
[Arora and Zhang, 2017] Arora, S. and Zhang, Y. networks. arXiv preprint arXiv:2107.03743.
(2017). Do GANs actually learn the distri-
[Gretton et al., 2012] Gretton, A., Borgwardt, K. M.,
bution? an empirical study. arXiv preprint
Rasch, M. J., Sch¨olkopf, B., and Smola, A. (2012).
arXiv:1706.08224.
A kernel two-sample test. The Journal of Machine
[Billingsley, 1986] Billingsley, P. (1986). Probability Learning Research, 13(1):723–773.
and Measure. JohnWileyandSons, secondedition.
[Kingma and Ba, 2014] Kingma, D. P. and Ba, J.
[de Frutos, 2023] de Frutos, J. M. (2023). Training (2014). Adam: A method for stochastic optimiza-
implicit generative models via an invariant statisti- tion. arXiv preprint arXiv:1412.6980.
calloss(isl). https://github.com/josemanuel22/
ISL. [Li et al., 2017] Li, C.-L., Chang, W.-C., Cheng, Y.,
Yang, Y., and P´oczos, B. (2017). MMD GAN: To-
[Deshpande et al., 2018] Deshpande, I., Zhang, Z.,
wards deeper understanding of moment matching
and Schwing, A. G. (2018). Generative modeling
network. Advances in neural information processing
usingtheslicedwassersteindistance. InProceedings
systems, 30.
of the IEEE conference on computer vision and pat-
tern recognition, pages 3483–3491. [Li et al., 2019a] Li, S., Jin, X., Xuan, Y., Zhou, X.,
Chen, W., Wang, Y.-X., and Yan, X. (2019a). En-
[Devroye et al., 2017] Devroye, L., Gy¨orfi, L., Lugosi,
hancing the locality and breaking the memory bot-
G.,andWalk,H.(2017). Onthemeasureofvoronoi
tleneck of transformer on time series forecasting.
cells. Journal of Applied Probability, 54(2):394–408.
Advances in neural information processing systems,
[Djuric and M´ıguez, 2010] Djuric, P. M. and M´ıguez, 32.
J. (2010). Assessment of nonlinear dynamic models
[Li et al., 2019b] Li, S., Jin, X., Xuan, Y., Zhou, X.,
by kolmogorov–smirnov statistics. IEEE transac-
Chen, W., Wang, Y.-X., and Yan, X. (2019b). En-
tions on signal processing, 58(10):5069–5079.
hancing the Locality and Breaking the Memory Bot-
[Dziugaite et al., 2015] Dziugaite, G. K., Roy, D. M., tleneck of Transformer on Time Series Forecasting.
and Ghahramani, Z. (2015). Training generative Curran Associates Inc., Red Hook, NY, USA.
neuralnetworksviamaximummeandiscrepancyop-
timization. arXiv preprint arXiv:1505.03906. [Li et al., 2015] Li, Y., Swersky, K., and Zemel, R.
(2015). Generative moment matching networks. In
[Elvira et al., 2016a] Elvira, V., M´ıguez, J., and
Internationalconferenceonmachinelearning,pages
Djuri´c, P. M. (2016a). Adapting the number
1718–1727. PMLR.
of particles in sequential monte carlo methods
through an online scheme for convergence assess- [Martino et al., 2018] Martino, L., Luengo, D., and
ment. IEEE Transactions on Signal Processing, M´ıguez, J. (2018). Independent random sampling
65(7):1781–1794. methods. Springer.Jos´e Manuel de Frutos, Pablo M. Olmos, Manuel A. V´azquez, Joaqu´ın M´ıguez
[Mescheder et al., 2017] Mescheder, L., Nowozin, S., [Rosenblatt, 1952] Rosenblatt,M.(1952).Remarkson
and Geiger, A. (2017). Adversarial variational a multivariate transformation. The annals of math-
bayes: Unifyingvariationalautoencodersandgener- ematical statistics, 23(3):470–472.
ative adversarial networks. In International confer-
enceonmachinelearning,pages2391–2400.PMLR. [Salimans et al., 2016] Salimans, T., Goodfellow, I.,
Zaremba, W., Cheung, V., Radford, A., and Chen,
[Miyato et al., 2018] Miyato, T., Kataoka, T., X. (2016). Improved techniques for training GANs.
Koyama, M., and Yoshida, Y. (2018). Spectral Advances in neural information processing systems,
normalization for generative adversarial networks. 29.
arXiv preprint arXiv:1802.05957.
[Salinas et al., 2020] Salinas, D., Flunkert, V.,
[Mohamed and Lakshminarayanan, 2016] Mohamed, Gasthaus,J.,andJanuschowski,T.(2020). Deepar:
S. and Lakshminarayanan, B. (2016). Learning Probabilistic forecasting with autoregressive recur-
in implicit generative models. arXiv preprint rentnetworks. International Journal of Forecasting,
arXiv:1610.03483. 36(3):1181–1191.
[Moreno-Pino et al., 2023] Moreno-Pino, F., Olmos, [Santos et al., 2019] Santos, C. N. d., Mroueh, Y.,
P. M., and Art´es-Rodr´ıguez, A. (2023). Deep au- Padhi, I., and Dognin, P. (2019). Learning implicit
toregressivemodelswithspectralattention. Pattern generative models by matching perceptual features.
Recognition, 133:109014. InProceedingsoftheIEEE/CVFInternationalCon-
ference on Computer Vision, pages 4461–4470.
[Nowozin et al., 2016] Nowozin, S., Cseke, B., and
[Scotta and Messina, 2023] Scotta,S.andMessina,A.
Tomioka, R. (2016). f-GAN: Training generative
(2023). Understandingandcontextualisingdiffusion
neural samplers using variational divergence mini-
models. arXiv preprint arXiv:2302.01394.
mization. Advances in neural information process-
ing systems, 29.
[Sohier, cess] Sohier, D. (Year of Access).
30 years of european wind generation.
[Rangapuram et al., 2018] Rangapuram,S.S.,Seeger,
https://www.kaggle.com/datasets/sohier/
M. W., Gasthaus, J., Stella, L., Wang, Y., and
30-years-of-european-wind-generation.
Januschowski, T. (2018). Deep state space models
for time series forecasting. In Bengio, S., Wallach,
[Taylor and Letham, 2018] Taylor, S. J. and Letham,
H., Larochelle, H., Grauman, K., Cesa-Bianchi, N.,
B. (2018). Forecasting at scale. The American
and Garnett, R., editors, Advances in Neural In-
Statistician, 72(1):37–45.
formation Processing Systems, volume 31. Curran
Associates, Inc. [Trindade, 2015] Trindade, A. (2015). Elec-
tricityLoadDiagrams20112014. UCI
[Rasul et al., 2021a] Rasul, K., Seward, C., Schuster,
Machine Learning Repository. DOI:
I., and Vollgraf, R. (2021a). Autoregressive Denois-
https://doi.org/10.24432/C58C86.
ing Diffusion Models for Multivariate Probabilistic
Time Series Forecasting. In Meila, M. and Zhang, [Uppal et al., 2019] Uppal, A., Singh, S., and P´oczos,
T., editors, Proceedings of the 38th International B.(2019). Nonparametricdensityestimation&con-
Conference on Machine Learning, volume 139 of vergence rates for GANs under besov ipm losses.
Proceedings of Machine Learning Research, pages Advances in neural information processing systems,
8857–8868. PMLR. 32.
[Rasul et al., 2021b] Rasul, K., Sheikh, A.-S., Schus- [Wilson, 2016] Wilson, G. T. (2016). Time Series
ter, I., Bergmann, U., and Vollgraf, R. (2021b). Analysis: Forecasting and Control, 5th Edition , by
Multivariate Probabilistic Time Series Forecasting GeorgeE.P.Box,GwilymM.Jenkins,GregoryC.
via Conditioned Normalizing Flows. In Inter- Reinsel and Greta M. Ljung , 2015 . Published by
national Conference on Learning Representations John Wiley and Sons Inc. , Hoboken, N. Journal of
2021. Time Series Analysis, 37(5):709–711.
[Rodr´ıguez-Santana et al., 2022] Rodr´ıguez-Santana, [Wu et al., 2021] Wu, H., Xu, J., Wang, J., andLong,
S., Zaldivar, B., and Hernandez-Lobato, D. (2022). M. (2021). Autoformer: Decomposition transform-
Function-space inference with sparse implicit ers with auto-correlation for long-term series fore-
processes. In International Conference on Machine casting. Advances in Neural Information Processing
Learning, pages 18723–18740. PMLR. Systems, 34:22419–22430.Training Implicit Generative Models via an Invariant Statistical Loss
[Yu et al., 2016] Yu,H.-F.,Rao,N.,andDhillon,I.S.
(2016). Temporal regularized matrix factorization
for high-dimensional time series prediction. In Lee,
D.,Sugiyama,M.,Luxburg,U.,Guyon,I.,andGar-
nett, R., editors, Advances in Neural Information
Processing Systems, volume 29. Curran Associates,
Inc.
[Zaheer et al., 2017] Zaheer, M., Li, C.-l., P´oczos, B.,
and Salakhutdinov, R. (2017). Gan connoisseur:
Can gans learn simple 1D parametric distributions.
InProceedings of the 31st Conference on Neural In-
formation Processing Systems, pages 1–6.
[Zeng et al., 2023] Zeng,A.,Chen,M.,Zhang,L.,and
Xu, Q. (2023). Are transformers effective for time
seriesforecasting? InProceedings of the AAAI con-
ference on artificial intelligence, volume 37, pages
11121–11128.
[Zhou et al., 2021] Zhou, H., Zhang, S., Peng, J.,
Zhang, S., Li, J., Xiong, H., and Zhang, W. (2021).
Informer: Beyond efficient transformer for long se-
quence time-series forecasting. In Proceedings of
the AAAI conference on artificial intelligence, vol-
ume 35, pages 11106–11115.Jos´e Manuel de Frutos, Pablo M. Olmos, Manuel A. V´azquez, Joaqu´ın M´ıguez
Appendix
Contents
1 MISSING PROOFS 14
2 ADDITIONAL EXPERIMENTS 16
2.1 The Effect of Parameter K . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
max
2.2 Experiments with large hyperparameters K, N and number of epochs . . . . . . . . . . . . . . . 21
2.3 Evolution of the hyperparameter K . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.4 Mixture time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.5 More experiments with real world datasets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.5.1 Electricity-f time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.5.2 Electricity-c time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.5.3 Wind time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
3 Experimental Setup 28Training Implicit Generative Models via an Invariant Statistical Loss
1 MISSING PROOFS
For f :R(cid:55)→R a real function and p the pdf of an univariate real r.v. let us denote,
(cid:90) ∞
(f,p):= f(x)p(x)dx
−∞
(cid:110) (cid:111)
and let B (R):= (f :R→R):sup |f(x)|≤1 be the set of real functions bounded by 1.
1 x∈R
Let us also denote the indicator function as
(cid:40)
1, if x∈A,
I (x)=
A
0, if x̸∈A,
for A⊂R and x∈R.
Theorem 4. If sup |(f,p)−(f,p˜)|≤ϵ then,
f∈B1(R)
1 1
−ϵ≤Q (n)≤ +ϵ, ∀n∈{0,...,K}.
K+1 K K+1
Proof. Choose y ∈R. The univariative distribution function (cdf) asociated to the pdf p is
0
(cid:90) y0
(cid:0) (cid:1)
F(y )= p(y)dy = I ,p ,
0 (−∞,y0)
−∞
and for p˜we obtain the cdf
F˜(y
)=(cid:90) y0
p˜(y)dy =(cid:0) I ,p˜(cid:1) .
0 (−∞,y0)
−∞
Now, the rank statistic can be interpreted in terms of a binomial r.v. To be specific, choose some y ∈ R and
0
thenrunK Bernoullitrialswhere, forthei-thtrial, wedrawy˜ ∼p˜anddeclareasuccessif, andonlyif, y˜ <y .
i i 0
It is apparent that the probability of success for these trials is F˜(y ) and the probability of having exactly n
0
successes is
(cid:18) (cid:19)
K
h (y )= (F˜(y ))n(1−F˜(y ))K−n.
n 0 n 0 0
If we now let y ∼ p and integrate h (y ) with respect to p(y) we obtain the probability of the event A = n,
0 n 0 K
i.e.
Q (n)=(h ,p), ∀n∈{0,...,K}.
K n
Now, since h ∈B (R), the assumption in the statement of Theorem 2 yields
n 1
|(h ,p˜)−(h ,p)|≤ϵ,
n n
which, in turn, implies
(h ,p˜)−ϵ≤(h ,p)≤(h ,p˜)+ϵ.
n n nJos´e Manuel de Frutos, Pablo M. Olmos, Manuel A. V´azquez, Joaqu´ın M´ıguez
However, (h ,p)=Q (n) by construction. On the other hand, if we let y ∼p˜and integrate with respect to p˜
n K 0
1
Theorem 1 yields that (h ,p˜)= , hence we obtain the inequality
n K+1
1 1
−ϵ≤Q (n)≤ +ϵ,
K+1 K K+1
and conclude the proof.Training Implicit Generative Models via an Invariant Statistical Loss
2 ADDITIONAL EXPERIMENTS
In this section of the appendix, we expand upon the experiments conducted. In the first part, we will study how
the results of the ISL method for learning 1-D distributions change depending on the chosen hyperparameters.
Later, in a second part, we will examine how these evolve over time. Finally, we will extend the experiments
conducted with respect to time series, considering a mixture time series case. We will also provide additional
results for the ‘electricity-c’ and ‘electricity-f’ series, and lastly, we will include the results obtained for a new
time series.
2.1 The Effect of Parameter K
max
We will first analyze the effects of the evolution of the hyperparameter K for different target distributions.
max
For training, the number of epochs is 1000, the learning rate 10−2, and the weights are updated using Adam
optimizer. Also, the number of (ground-truth) observations is, in every case, N = 1000. First, we consider a
standardnormaldistribution,N(0,1),asthesourcefromwhichwesamplerandomnoisetogeneratethesynthetic
data. As generator, we use a 4-layer multilayer perceptron (MLP) with 7,13,7 and 1 units at the corresponding
layers. As activation function we use a exponential linear unit (ELU). We specify the target distribution and
K in each subcaption.
max
At the sight of the results (see Figure 1 and 2) we observe that a higher value of K generally leads to more
max
accurateresultscomparedtothetruedistribution. However,thisisnotalwaysthecasebecause,asdemonstrated
in the example of the Pareto distribution, the value of K achieved during training is lower (specifically 6) than
the maximum limit set for K. In such cases, K does not have an effect.
maxJos´e Manuel de Frutos, Pablo M. Olmos, Manuel A. V´azquez, Joaqu´ın M´ıguez
N(4,2),Kmax=2 N(4,2),Kmax=5 N(4,2),Kmax=10 N(4,2),Kmax=20
U(−2,2),Kmax=2 U(−2,2),Kmax=5 U(−2,2),Kmax=10 U(−2,2),Kmax=20
Cauchy(1,2),Kmax=2 Cauchy(1,2),Kmax=5 Cauchy(1,2),Kmax=10 Cauchy(1,2),Kmax=20
Pareto(1,1),Kmax=2 Pareto(1,1),Kmax=5 Pareto(1,1),Kmax=10 Pareto(1,1),Kmax=20
Figure 1: Results obtained for varying K for different target distributions. Global parameters: N = 1000,
max
Epochs=1000, LearningRate=10−2, and Initial Distribution=N(0,1).Training Implicit Generative Models via an Invariant Statistical Loss
Model1,Kmax=2 Model1,Kmax=5
Model1,Kmax=10 Model1,Kmax=20
Figure 2: Results obtained for varying K for Target Distributions = Model .
max 1
Global parameters: N =1000, Epochs=1000, LearningRate=10−2, and Initial Distribution=N(0,1).Jos´e Manuel de Frutos, Pablo M. Olmos, Manuel A. V´azquez, Joaqu´ın M´ıguez
As before, we are analyzing the effects of the evolution of the hyperparameter K for different distributions.
max
Inthiscase, weconsidertherandomnoiseoriginatingfromanuniformdistributionU(−1,1). Theothersettings
are identical to the N(0,1) case.
N(4,2),Kmax=2 N(4,2),Kmax=5 N(4,2),Kmax=10 N(4,2),Kmax=20
U(4,2),Kmax=2 U(4,2),Kmax=5 U(4,2),Kmax=10 U(4,2),Kmax=20
Cauchy(1,2),Kmax=2 Cauchy(1,2),Kmax=5 Cauchy(1,2),Kmax=10 Cauchy(1,2),Kmax=20
Pareto(1,1),Kmax=2 Pareto(1,1),Kmax=5 Pareto(1,1),Kmax=10 Pareto(1,1),Kmax=20
Figure 3: Results obtained for varying K for different target distributions. Global parameters: N = 1000,
max
Epochs=1000, Learning Rate=10−2, and Initial Distribution = U(−1,1).Training Implicit Generative Models via an Invariant Statistical Loss
Model1,Kmax=2 Model1,Kmax=20
Model1,Kmax=5 Model1,Kmax=10
Figure 4: Results obtained for varying K for Target Distributions = Model . Global parameters: N =1000,
max 1
Epochs=1000, Learning Rate=10−2, and Initial Distribution = U(0,1).Jos´e Manuel de Frutos, Pablo M. Olmos, Manuel A. V´azquez, Joaqu´ın M´ıguez
2.2 Experiments with large hyperparameters K, N and number of epochs
In this section, we investigate the capacity of ISL to learn complex distributions in scenarios where the hyper-
parameters K and N are set to large values. In each case, we specify the values of the hyperparameters. As we
can see, in these two examples with large values of K , N, and allowing for sufficient training time (number
max
of epochs is large), the method is capable of learning complex multimodal distributions.
(b) N =5000, K =50, epochs=10000, lr=10−2
(a) N =10000, K =10, epochs=10000, lr=10−2, max
max Target Distribution =
Target Distribution = Model . (cid:0) (cid:1)
3 MixtureModel N(−7,0.5),N(−1,1),N(4,1),N(10,2)
Figure 5: Complex Distribution Learning with High Values of Hyperparameters: K, N, and Epochs.
2.3 Evolution of the hyperparameter K
Here, we study the evolution of hyperparameter K during training. It is progressively adapted according to the
algorithm explained in Section 4.2 of the paper. In every experiment we use a learning rate of 10−2 and train
for 10000 epochs. The number of ground-truth observations is N =2,000. In this case, we haven’t imposed any
restrictions on K , and hence K can grow as required. In the subcaption, the target distribution is specified.
max
The initial distribution is a N(0,1) in all cases.
We notice that initially, as the number of epochs increases, the value of K grows very rapidly, but it eventually
levels off after reaching a certain point. The observed data (see Figure 6) leads us to infer that obtaining a
high K value requires a progressively more effort as compared to attaining a lower value, as evidenced by the
incremental increases over successive training epochs. This complexity seems to exhibit a logarithmic pattern.
Therefore, there is an advantage in initially setting smaller values of K and gradually increasing them.Training Implicit Generative Models via an Invariant Statistical Loss
(a) N(4,2) (c) Model
3
(b) Model (d) Pareto(1,2), N =2000
1
Figure 6: Evolution of the hyperparameter K as a function of the epochs. Figure top left: We compare the optimal
transformation from a N(0,1) to the target distribution versus the generator learned by ISL, Figure top right: The
resulting distribution, Figure at the bottom: The evolution of K as a function of epochs.Jos´e Manuel de Frutos, Pablo M. Olmos, Manuel A. V´azquez, Joaqu´ın M´ıguez
2.4 Mixture time series
We now consider the following composite time series, such that the underlying density function is multimodal.
For each time t, a sample is taken from functions,
y (t)=10cos(t−0.5)+ξ or, y (t)=10cos(t−0.5)+ξ,
1 2
based on a Bernoulli distribution with parameter 1/2. Noise, denoted by ξ, follows a standard Gaussian dis-
tribution, N(0,1). Our temporal variable, t, will take values in the interval (−4,4) where each point will be
equidistantly spaced by 0.1 units apart (when we represent it, we use the conventional notation t ≥ 0). The
neuralnetworkcomprisestwocomponents: anRNNwiththreelayersfeaturing16,16andfinally32units; anda
feedforward neural network with three layers containing 32, 64 units and 64 units. Activation functions include
ELU for the RNN layers and ELU for the MLP layers, except for the final layer, which employs an identity
activation function. The chosen learning rate is 10−3 in an Adam optimizer.
In the Figure 7, we display the predictions obtained for the neural network trained using ISL and DeepAR, with
a prediction window of 20 steps.
(a) ISLforecasting (b) DeepARforecasting
Figure 7: Mixture time series, forecasting 20-stepTraining Implicit Generative Models via an Invariant Statistical Loss
2.5 More experiments with real world datasets
In this section, we delve deeper into the experiments carried out with real world datasets. Previously, we
provided a brief overview of the results pertaining to ‘electric-f’ and ‘electric-c.’ Within this section, we will
present comparative graphs for these two datasets and introduce the results related to the wind dataset.
2.5.1 Electricity-f time series
We present comparative graphs of the results achieved by ISL using the ‘electricity-f’ dataset. The training
phase utilized data from 2012 to 2013, while validation encompassed a span of 20 months, commencing on April
15,2014. Forthispurpose,weemployedamodelarchitectureconsistingof2layers-deepRNNwith3unitseach,
as well as a 2-layer MLP with 10 units each and a ReLU activation function.
In the case of training with DeepAR, we utilized the GluonTS library [Alexandrov et al., 2020] with its default
settings, which entail 2 recurrent layers, each comprising 40 units. The model was trained over the course of 100
epochs. The results presented in Figure 8 relate to the prediction of the final day within the series for different
users, with the shaded area indicating the range of plus or minus one standard deviation from the predicted
value.
(a) user 5 (c) user 169
(b) user 8 (d) user 335
Figure8: 1-daypredictionof‘electricity-f’consumption. Blue: 1-daypredictionusingISL,Green: 1-daypredictionusing
DeepAR.Jos´e Manuel de Frutos, Pablo M. Olmos, Manuel A. V´azquez, Joaqu´ın M´ıguez
2.5.2 Electricity-c time series
WepresentcomparativegraphsoftheresultsachievedbyISLusingthe‘electricity-c’dataset. Weusedthesame
settings for both training and validation, as well as the same neural network as described for the ‘electricity-f’
dataset. Below, we present various prediction plots for different users, both for a 1-day (Figure 9) and a 7-day
forecast (Figure 10).
Ideal Target Ideal Target
Prediction Prediction
300
200
250
Wh) Wh)
Load
(k
200
Load
(k 150
100
150
50
0 10 20 30 0 10 20 30
t t
(a) user 5 (c) user 169
1600 Ideal Target Ideal Target
Prediction Prediction
400
1400
Wh) Wh) 300
Load
(k 1200
Load
(k
200
1000
100
800
0 10 20 30 0 10 20 30
t t
(b) user 8 (d) user 335
Figure 9: 1-day forecast for ‘electricity-c’ consumptionTraining Implicit Generative Models via an Invariant Statistical Loss
Ideal Target 250 Ideal Target
Prediction Prediction
350
200
300
Wh) 250 Wh) 150
Load
(k
200
Load
(k
100
150
100 50
0 50 100 150 200 0 50 100 150 200
t t
(a) user 5 (c) user 169
500
1800 Ideal Target Ideal Target
Prediction Prediction
1600
400
1400
Wh) 1200 Wh) 300
Load
(k
1000 Load
(k
200
800
600
100
400
0 50 100 150 200 0 50 100 150 200
t t
(b) user 8 (d) user 335
Figure 10: 7-day forecast for ‘electricity-c’ consumptionJos´e Manuel de Frutos, Pablo M. Olmos, Manuel A. V´azquez, Joaqu´ın M´ıguez
2.5.3 Wind time series
The wind [Sohier, cess] dataset contains daily estimates of the wind energy potential for 28 countries over the
period from 1986 to 2015, expressed as a percentage of a power plant’s maximum output. The training was
conducted using data from a 3-year period (1986-1989), and validation was performed on three months of data
from 1992. Below, we present the results obtained for 30-day and 180-day predictions for different time series.
For this purpose, we used a 3-layer RNN with 32 units in each layer and a 2-layer MLP with 64 units in each
layer, both with ReLU activation functions. The final layer employs an identity activation function.
(a) FR22 (c) FR51
(b) FR25 (d) AT31
Figure 11: 30 days forecast predictionTraining Implicit Generative Models via an Invariant Statistical Loss
(a) FR82
Figure 12: 180 days forecast prediction
Table 1: Evaluation summary, using QL /QL metrics, on wind datasets, forecast window 30 day
ρ=0.5 ρ=0.9
Method Wind
Prophet 0.305/0.275
TRMF 0.311/−
N-BEATS 0.302/0.283
DeepAR 0.286/0.116
ConvTras 0.287/0.111
ISL 0.204/0.245
3 Experimental Setup
Allexperimentswereconductedusingapersonalcomputerwiththefollowingspecifications: aMacBookProwith
macOSoperatingsystemversion13.2.1,anAppleM1ProCPU,and16GBofRAM.Thespecifichyperparameters
for each experiment are detailed in the respective sections where they are discussed. The repository with the
code can be found in at the following link https://github.com/josemanuel22/ISL.