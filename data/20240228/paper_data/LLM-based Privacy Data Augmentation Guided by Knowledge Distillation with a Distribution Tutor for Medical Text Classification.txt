LLM-based Privacy Data Augmentation Guided by Knowledge Distillation
with a Distribution Tutor for Medical Text Classification
YipingSong1,∗,JuhuaZhang2,*,ZhiliangTian2,†,
YuxinYang2,MinlieHuang3,DongshengLi2
1CollegeofScience,NationalUniversityofDefenseTechnology,Hunan,China
2CollegeofComputer,NationalUniversityofDefenseTechnology,Hunan,China
3TheCoAIGroup,DCST,BNRist,TsinghuaUniversity,Beijing100084,China
{songyiping, zhangjuhua23, tianzhiliang,
yangyuxin21a, dsli}@nudt.edu.cn, aihuang@tsinghua.edu.cn
Abstract Researchersfullyexploitlimitedpublicdataand
avoidaccessingprivatedatatoensuredatasecurity
As sufficient data are not always publically
(Fei-Feietal.,2006),whichachieveslearningon
accessibleformodeltraining, researchersex-
smalldataviameta-learning(Finnetal.,2017)or ploit limited data with advanced learning al-
gorithms or expand the dataset via data aug- active learning (Konyushkova et al., 2017). Fur-
mentation(DA).ConductingDAinprivatedo- ther, some researchers expand the public dataset
main requires private protection approaches withdataaugmentation(DA)(WeiandZou,2019);
(i.e.anonymizationandperturbation),butthose
anotherpromisingDAstrategyistosynthesizepri-
methodscannotprovideprotectionguarantees.
vate samples conditioned on privacy data while
Differentialprivacy(DP)learningmethodsthe-
ensuringthesensitiveinformationinprivatedata
oretically bound the protection but are not
arewell-protected(Yueetal.,2022).
skilledatgeneratingpseudotextsampleswith
large models. In this paper, we transfer DP- Synthesizingprivatetextrequirestocaptureorig-
based pseudo sample generation task to DP- inalprivatedatadistributionsbyaccessingthedata.
based generated samples discrimination task, Accessingthemshouldbeundertheguaranteeof
where we propose a DP-based DA method1 privacyprotectionsandthusweneedprivacypro-
withaLLMandaDP-baseddiscriminatorfor
tection on text generation. Researchers attempt
textclassificationonprivatedomains. Wecon-
anonymization methods to mask selected sensi-
structaknowledgedistillationmodelastheDP-
tive text span (Carlini et al., 2019b; Shi et al.,
baseddiscriminator: teachermodels,accessing
2021),randomlynoisetheinputtokens(Feyisetan
privatedata,teachesstudentshowtoselectpri-
vatesampleswithcalibratednoisetoachieve etal.,2020),orapplyregularizationtoavoidover-
DP.ToconstrainthedistributionofDA’sgener- fitting training data (Carlini et al., 2019b). How-
ation,weproposeaDP-basedtutorthatmodels ever,themasking,nosing,orregularizationmech-
thenoisedprivatedistributionandcontrolssam-
anism hardly ensures (almost) all personal infor-
ples’generationwithalowprivacycost. We
mationiswell-protected. Differentialprivacy(DP)
theoreticallyanalyzeourmodel’sprivacypro-
(Dworketal.,2006)providesprovableguarantees
tectionandempiricallyverifyourmodel.
againsttheidentificationofindividualinformation
1 Introduction in datasets. Deep generative models with DP en-
suretheexistenceofaspecificsample(withprivate
The ability of deep text classification models
information)cannotbedetected(Lietal.,2022).
mainlyderivesfromlarge-scaletrainingdataand
Noisy-SGD (Song et al., 2013; Abadi et al.,
recent large language models (LLMs) particu-
2016)isapracticalDPalgorithmfordeeplearning
larly benefit from big data. Many domains (e.g.
models,includingtextgenerationmodels(Kerrigan
medicine(Qingetal.,2019;Lietal.,2021))have
etal.,2020),whichaddscalibratednoiseonmodel
only limited amount of public data and large pri-
gradients to satisfy DP. However, the advantage
vatedata. Itisriskytoreleaseprivatedatatomodel
of NoisySGD is weaken as the model becomes
trainingsincemodelsprobablymemorizedetailsin
larger (Bassily et al., 2014; Yu et al., 2020) and
thosedataandunintentionallyoutputtheirsensitive
NoisySGDrequiresaper-examplegradientclipre-
information(Carlinietal.,2019a,2021).
sultinginnon-convergenceandsystemoverheads
*EqualContribution.
(Zhu et al., 2020; Bu et al., 2021). Another cate-
†CorrespondingAuthor.
gories of DP learning methods, PATE (Papernot
1Code is available at https://anonymous.4open.
science/r/DP_DA-1BF7 et al., 2017), acquires private information from
1
4202
beF
62
]LC.sc[
1v51561.2042:viXrateacher models learned on private data to a stu- 2 RelatedWork
dentandaddsnoisesonteachers’outputs. PATE’s
2.1 PrivacyProtectioninTextClassification
privacycostcomesfromteachers’outputsinstead
ofthewholemodel’sgradientsinNoisy-SGD(Li Initialprivacyprotectiontechniquespredominantly
et al., 2022). Hence, PATE’s required noise does focusedondataanonymization(Maedaetal.,2016;
notscalewithmodelsizeandPATEispromising Suzukietal.,2018),Forexample,de-identification
of working on large models (e.g. BERT (Devlin techniques(Garfinkeletal.,2015)suchasremov-
et al., 2019), Llama (Touvron et al., 2023), and ing,replacing,orencryptingsensitiveinformation
GPT-4 (Achiametal.,2023)). indatacanreducetheriskofprivacyleaks. Data
perturbationtechniques(JohnsonandShmatikov,
DP with PATE on text generation still suffers 2013)protectuserprivacybyincorporatingrandom
fromasequentialmultipletaskswithlargecandi- noiseintothedata. Nevertheless,straightforward
date space, which extremely (linearly) increases dataanonymizationmeasuresmaydifficulttoeffec-
the noise scale (Tian et al., 2022). We argue that tivelydealwithprivacyleakagechallenges(Rocher
wecantransferaDP-basedgenerationtasktoaDP- etal.,2019). Presently,federatedlearning(McMa-
baseddiscriminationtasktoavoidcomplexitiesin hanetal.,2017;Dengetal.,2022)andDP(Dwork
DP text generation models. Particularly, we em- etal.,2006)emergeasthetwoprincipalmethodolo-
ployLLMs’generationabilityP (x)togener- giesinthedomainofprivacyprotection. Federated
LLM
atepublicsamplesxandconstructaDP-baseddis- learningstrategiescanpreventprivacyleakscaused
criminator P (·|x) to select synthesized sam- by untrustworthy servers. DP aims to prevent at-
discri
plesxfitsforprivatedomainc,synthesizingprivate tackersfromextractingsensitiveinformationfrom
samplesviaP (x|·) = P (·|x)∗P (x). thetrainingdataset(Carlinietal.,2021),offeringa
syn discri LLM
quantifiableprivacyprotectionmechanism. With
Inthispaper,weproposeaDP-baseddataaug- its robust theoretical foundation and broad appli-
mentation(DA)paradigmwithaLLMandaDP- cability(Dworketal.,2014a),differentialprivacy
baseddiscriminatortogeneratesamplesforprivate is widely acknowledged as the standard practice
textclassification,wherethediscriminatorselects in the field of privacy protection. Independently
LLM-generatedsampleslikelytobelongtoprivate andconcurrentlywithourwork,(Wuetal.,2023)
domain as our synthesized samples. Specifically, and(Duanetal.,2023)studiedICLwithDPguar-
thediscriminatorachievesDPviaknowledgedistil- antees for text classification tasks. Our proposed
lation(KD),wheremultipleteachermodelsaccess methodpredominantlyembraces theprivacypro-
disjoint and unique private sets to learn private tectionprinciplesofdifferentialprivacy.
discriminators;astudentlearnsfromnoisedaggre-
2.2 DataAugmentation(DA)inText
gatedteacheroutputstoachieveDP.Tocontrolthe
Classification
distributioninDA’sgeneratedsamples,wepropose
aDP-baseddistributiontutorthatcapturesthedis- VariousNLPdataaugmentation(DA)techniques
tributionofprivatedata. InDP,queryingprivacyis have been developed, such as Back-Translation
expensive(studentqueryingteachercausesacer- (Kobayashi, 2018), EDA (Wei and Zou, 2019),
tain privacy cost), the tutor carries less sensitive and AEDA (Karimi et al., 2021). These meth-
information than teachers and helps the student ods primarily focus on modifying the original in-
withalowprivacycost(See§4). Wefurtherpro- put, which limits the diversity of the generated
vide theoretical analyses and empirical results to samples. In response, (Szegedy et al., 2016) ini-
verifyourmethods. tiallyexploreainterpolation-basedmethods(i.e.,
mixup) in computer vision. Subsequently, (Guo
Our contributions are as follows: (1) We con- et al., 2019) combine the mixup technique with
structaDP-basedDAwithLLMsthatsynthetizes CNNsandLSTMsfortextapplications.Thereare
(almost infinite) samples while bounding the pri- alsomanystudiesthatchoosedifferentstrategies
vacy leakage. (2) We propose a DP-based tutor toimprovethemixuptechnique(Chenetal.,2020;
forteacher-studentframeworkstoteachsomeless Zhangetal.,2020).
sensitivedatawithalowprivacycost. (3)Weexcel Moreover,someresearchersusepre-trainedlan-
strongDP-basedbaselinesontextclassificationin guage models (PLMs) for data for data augmen-
privatedomains. tation. (Kumaretal.,2020)provideastraightfor-
2wardandeffectivemethodforconditionalPLMby ofPATEarisesfromknowledgedistillationrather
prependingclasslabelstotextsequences. (Huetal., thanthegradientoftheentiremodel. Withthisad-
2019)utilizereinforcementlearningwithacondi- vantage,PATEhasenormouspotentialinadapting
tionallanguagemodelthatperformsbyappending tolargemodels. Moreover,PATEisdesignedfor
thecorrectlabeltotheinputsequenceduringtrain- classificationtasksandissuitableforourgoalof
ing. Further,anincreasingnumberofscholarshave trainingaDP-baseddiscriminator.
started to utilize adversarial learning methods to
3 Methods
generateaugmentedsamples,suchasBERT-Attack
(Lietal.,2020),G-DAUGC (Yangetal.,2020).
3.1 Overview
Toreducethenegativeimpactoflow-qualityaug-
Ourmodelconsistsoffourparts(Fig.1).
mentation samples on model performance, some
researchfocusonsampleselection. Forexample,
LLM-basedPublicGenerator (§3.3)synthetic
(Cao et al., 2021) propose UAST framework to
publictextualinputforthespecificlabelonthetext
quantify model uncertainty for selecting pseudo-
classificationtask.
labeledsamples. (Linetal.,2023)focusmoreon
the combination of sample selection and data en- DP-based Discriminator with Knowledge Dis-
hancementstrategies,andintroduceaself-training tillation (§3.4)learnstodiscriminatewhethera
selectionframeworktoselecthigh-qualitysamples sample is (likely) from public domain or private
from the data augmentation. Different from the domain,whichsatisfyDPprivacyguarantee. Itfil-
abovemethods,weaimtoachieveDAtosynthetic tersthesamplesfromthegenerator(§3.3)toobtain
privatedatawhileensuringtheprivateinformation newsamplesforprivacydomain.
fromtheprivatedataset.
LabelDistributionTutor (§3.5)leadsthegener-
atedpseudosamplestofollowthedistributionof
2.3 DPforDeepLearningModels
privatedataunderDPguarantee,whichalsofilter
SomeresearchersemployDPtoprotecttheprivacy
thegenerator§3.3’soutput.
ofempiricalriskminimizationclassifiers(Chaud-
hurietal.,2011)andSVM(Rubinsteinetal.,2009). PrivateDataAugmentation (§3.6)usesthegen-
Following (Song et al., 2013), NoisySGD intro- erator to obtain candidate samples and uses the
ducesnoiseintogradientstoachieveDPfordeep discriminator(§3.4)andthetutor(§3.5)tofilterthe
learningmodels, includingDP-SGD(Songetal., candidatestogetdataaugmentationsamples.
2013; Bassily et al., 2014; Bu et al., 2023) and
3.2 TaskDefinition
DP-Adam (Abadi et al., 2016; Kingma and Ba,
2014). The use of DP-SGD for large-scale pre- Givenaprivatetextclassificationcorporawithsen-
trainingofBERThasbeenshowntoachievecom- sitive information, our task aims to train a text
parable masked language modeling performance classifierwithacertainleveloftheoreticalprivacy
tonon-privateBERT(Aniletal.,2021),butwith guarantee,whichprotectsalltrainingsampleswith
aprivacybudgetis100orhigher. Recentstudies itslabel< x,c >frombeingdetected(protecting
havedemonstratedthatevenundermorestringent theexistenceofspecifictrainingsamplefrombe-
privacyconstraints,generativeanddiscriminative ing identified by any detectors via any detecting
language models can achieve high performance methods).
acrossvarioustasksbyappropriatelyselectinghy- Notethatourtaskallowsthemodelusingdata
perparameters and fine-tuning objectives aligned augmentation(DA)methodtogeneratepseudopri-
withthepre-trainingprocess(Lietal.,2022). Ad- vatesamplesfortraining. However,thedataaug-
ditionally, (Li et al., 2022) apply ghost clipping mentationmodelalsooughttoensuretheprivacy
topre-trainedlanguagemodelsusingNoisySGD, guaranteementionedabove.
reducingmemoryusage. (Heetal.,2022)leverage
3.3 LLM-basedPublicGenerator
group clipping with adaptive clipping thresholds,
privately fine-tuning GPT-3 with 1.75 trillion pa- We employ a LLM (i.e. GPT-3.5) to generate in-
rameters. PATE(Papernotetal.,2017)isanother put texts for each output label (on classification
typeofDPlearningalgorithm,transferringknowl- task). Even if the GPT-3.5 is trained on the pub-
edge from teacher models trained on private sets licdomain,wedesignaprompttexttoinducethe
with noises to a student model. The privacy cost LLMtoattempttogenerateprivatesamples,where
3LLM-based Public Generator
Label Distribution
Prompt template: Dataset
Please generate a LLM Public Data Dist.
medical transcription for
[LABEL] and ... 0.9 S wa itm h p Ple robability
0.9 Choosed Sample by Tutor
Input Text [LABEL] Train Predict
Public Data Data Transfer
DP-based Discriminator with Knowledge Distillation Output
Probability
Data 1 Teacher Model 1 Discriminate
[public sample, 0]
Merge +Noise 0.1 0.4
0.4 0.9
Tr Dai an ti ang Data 2 Teacher Model 2 Train Student Model 0.90.6 0.9 0.6 0.1
0.6 0.4
······ ······ 0.1
[private sample, 1]
Data M Teacher Model M
Downstream
Filtering Tasks
0.1 0.4
0.4 0.9 (Text classification)
0.6 0.6
Label Distribution Tutor 0.9 0.9 0.1
0.6 0.4
Private Data Dist. Noisy Data Dist. 0.1
+Noise
0.4
Private Data 0.9
0.6
0.9 0.9
0.6
Privacy Block pseudo private samples
Figure1: Overviewofourframework. Itmainlycontainsthreecomponents,LLM-basedPublicGenerator(grey
block)generatespublicdata. DP-basedDiscriminatorwithKnowledgeDistillation(pinkblock)discriminatespublic
dataandobtainsaprobabilitysimilartoprivatedata. TheLabelDistributionTutor(blueblock)selectsasubsetwith
thehighestprobabilitiesofsamplesmatchingthenoiselabeldistribution. Thegraydottedboxistheprivacyblock.
thepromptfollowsthistemplate: “Youareapro- TeacherModels. Wetrainmultipleteachermod-
fessional medical transcriber. Please generate a elsonmultipledisjointdatasets,wheretheprivate
medicaltranscriptionfor[LABEL]anddonotre- samplesactasthepositivesampleandthepublic
vealthepatient’sname. Thetextlengthofmedical samples(generatedby§3.3)asthenegativesample.
transcription is approximately 400 words and at The teachers learn to judge whether the samples
least 200 words.” In the above template, “[LA- fromprivatesetorpublicset. Allteachersfollow
BEL]” denotes the label c of classifications (i.e. the structure and the initial parameters of a pre-
medicalspecialty). Inthisway,LLMgeneratesthe trainedmodel.
inputtextxwiththelabelctocomposeacomplete
To satisfy DP, we carefully process the teach-
pseudosample< x,c >.
ers’ data and train teachers with two strategies.
First,theprivatetrainingdataneedtocontainonly
uniquesamples(privatesampleduplicatesshould
3.4 DP-basedDiscriminatorwithKnowledge
be removed). The reason is that DP prevents the
Distillation
unknowndetectorsfromidentificationoneachoc-
currence(Dworketal.,2006). Duplicatedsamples
WeproposeaDP-baseddiscriminatortocheckif
with N occurrences increase the private loss and
thepseudosamplesfitfortheprivatedistribution
thusdecreasetheboundofprivacyprotection(in
thus are capable of acting as private samples. In-
termsofε)(Dworketal.,2014a). Tomaintainthe
spired by (Papernot et al., 2017), we construct a
bound,thenoisescalehastoincreaseN times(Ker-
teacher-studentframeworkwithknowledgedistil-
riganetal.,2020)(orlog(N)timess.t. advanced
lation. This model consists of multiple teacher
DP(Lietal.,2022))whichextremelyharmsforthe
modelsandastudentmodel. Teachersareallowed
performance.
toaccesstheprivatedataandthestudentcanonly
accessthenoisedteachers’outputs. Second, we equally divide the shuffled private
4dataintoM disjointsetsforM teachersandtrain tocarrythelabeldistributionoftheprivatesamples,
each teacher on each set separately. In this way, as the label distribution is critical in generating
the existence of any specific sample affects only augmenteddata.
oneteacher’soutput,whichboundsthesensitivity The tutor follows a statistic way to collect the
oftheteachers’noisydistribution(Papernotetal., noised label distribution of all private samples
2017;Boenischetal.,2023)(Seedetailedanalyses P (C)withaverysmallprivacycost. Thetutor
label
in §4). The equally divide (in terms of the sam- firstobtainstheactuallabeldistributionbycount-
ple number) and shuffled assignment ensure the ingthetotalN trainingsamplesasthefirsttermof
balanceamongallteachersandthetrainingperfor- Eq.3. Then,itimportscalibratedGaussiannoise
mance. onthelabeldistributionasthesecondtermofEq.3
2. Theabovemechanismensuresthetutorsatisfy
StudentModel. Thestudentmodelisadiscrim-
DP(Analysesin§4).
inator cannot access raw private data and learn
fromteacherswiththosesteps: (1)Merging. For
agivensamplex,wemergeteachers’predictions
1
(cid:88)N
P m(Y|x),whereY = {0,1}indicateswhetherthe P label(C) ∝
N
P j(C)+N(0,σ t2 utor). (3)
sample x derives from private or public set. (2) j
Noising. According to DP theory (Dwork et al.,
3.6 PrivateDataAugmentation
2014b), we add calibrated noise on the teacher’s
Weproposeanoveldataaugmentation(DA)frame-
predictionstoobtainyˆasEq.1,denotingteachers’
worktogeneratesamplesforprivatedomainwhile
estimatedjudgmentonx.
protectingtheprivacy. Theframeworkinvolves: (1)
yˆ= f (Y|x) (1) apublicLLM-basedgeneratortoobtainaplentyof
tea_agg
candidatesamples(§3.4). (2)aDP-baseddiscrimi-
M
= argmax((cid:88) P (Y|x)+N(0,σ2 )) natortoselectLLM’sgeneratedsamplessimilarto
m KD
y∈Y privatesamples,and(3)atutortofiltergenerated
m
samplestoensurethelabeldistributionsfittingfor
, where the first term is the actual teacher aggre-
theprivatedata.
gatedoutputsandM indicatestheteachernumber;
Theideais(1)takingadvantageofstronggenera-
thesecondtermrepresentsthecalibratedGaussian
tionabilityofLLMstoobtainhighqualitydata. (2)
noise(Buetal.,2020)andσ controlsthede-
teacher accessing private data causes privacy cost but ac-
greeofaddingnoise(i.e. degreeofprivacyprotec-
cessingprivacythroughthestudent(i.e. discrimina-
tion). (3)Teaching. Weemploythenoisyoutputs
tor)andtutorsatisfyingDPwouldnotbringsinad-
Yˆ toactaspseudolabels,whichteachesthestudent
ditionalloss. Hence,wecan“infinitely”callLLMs,
howtodiscriminatetheprivacysamples. Foreach
student, and tutor to achieve DA while bounding
samplex,theteachingfollowsCross-Entropyloss
theprivacyprotection.
asEq.2.
4 PrivacyAnalysesofourMethod
L = CE(f (C|x),P (C|x)) (2) Lemma 4 Analytical Gaussian mechanism.
KD tea_agg student
(BalleandWang,2018)Foraqueryh : Xn → Yd
M
= −(cid:88) I(c = cˆ)logP (c |x) over a dataset D, the randomized algorithm out-
m student m
puttingh(D)+Z s.t. Z ∼ N(0,σ2I )satisfies
m d
(ε,δ(ε))-DPforallε ≥ 0andδ(ε) = Φ(∆−εσ)−
,whereIisaindicatorfunction. Theabovemecha- eεΦ(−∆ − εσ), where ∆ = max 2σ ∥h(D∆ )−
nismensuresthestudentsatisfiesDPas(Papernot 2σ ∆ D∼D′
h(D′)∥ isL2sensitivityofhandΦisCDFfunc-
2
etal.,2017)(Analysesin§4).
tionofN(0,1).
3.5 LabelDistributionTutor 2Mathematically,thenoisyaggregatedoutputofEq.3is
expectedtofollowprobabilitydistribution(rangefrom0to
We propose a tutor based on the above teacher-
1)asthemeanofnoisesis0.Butsomeoutputvaluesmaybe
student framework (§3.4), which avoids directly occasionallyoutof[0,1]. Ifso,wefollowTianetal.(2022)
accessthepureprivatesamplesbutaccessesonly tore-normalizetheout-of-boundvalueto0or1.Practically,
weobserved(inFig.2)beingout-of-boundisextremelyrare
a small amount of privacy information so as to
sinceN islarge(3k)andthefirsttermdominatesEq.3as
maintaintheboundofprotections. Thetutoraims (Tianetal.,2022).
53750trainingsamples 6000trainingsamples
Method
P R F1 Acc P R F1 Acc
Private 0.224 0.367 0.240 0.367 - - - -
Non-DP
DAw/Public 0.280 0.347 0.280 0.348 0.313 0.347 0.287 0.348
DP-SGD 0.130 0.310 0.170 0.314 - - - -
DP(ε = 4)
Ghost 0.143 0.289 0.166 0.291 - - - -
DAw/DP-SGD 0.290 0.350 0.277 0.352 0.283 0.350 0.280 0.352
DAw/DP(ε = 4) DAw/Ghost 0.303 0.347 0.297 0.344 0.283 0.350 0.297 0.350
Ours 0.340 0.373 0.337 0.372 0.353 0.377 0.340 0.376
Table1: Mainresultscomparingallthebaselinesontwosizeofdatasetsontextclassificationtasks.
Sensitivity Analysis of Knowledge Distillation According to composition theorem for (ε, δ)-DP
(KD). We denote the output distribution of m- (Dwork et al., 2006), when the KD algorithm
th teacher model is P (Y|x). The aggregation M satisfies (ε , δ )-DP and the tutor al-
m KD KD KD
function h(D) =
(cid:80)M
P (Y|x) is the summa- gorithm M satisfies (ε , δ )-DP, then
m=1 m tutor tutor tutor
tionoftheoutputprobabilityoveralltheteachers. the combination (i.e. our whole algorithm) M
Note that each teacher’s data are disjoint (§3.4) satisfies(ε +ε ,δ +δ )-DP.(Seede-
KD tutor KD tutor
and teachers have no duplicate samples, chang- ductioninApp.C).Insummary,addingthenoises
ing one sample only affects one teacher’s output. according to in Lemma 4 to KD and the tutor is
ForneighboringdatasetsD andD′ differingonly sufficienttopreserve(ε +ε ,δ +δ )-
KD tutor KD tutor
onesample,wedenotei-thteacherisaffectedby DPforthecompositionofKDandthetutorwhere
√
thedifferentsampleinD andD′. Theoutputdis- thesensitivityis 2.
tributions P (Y|x) and P′(Y|x) (from D and D′
i i
5 ExperimentalSettings
respectively)aredifferent. Thesensitivity∆ in
KD
Lemma4is(SeedeductionsinApp.A),
Datasets. Weevaluatethemethodson“Medical
Transcriptions"3 dataset,whichisadatasetinthe
∆ = ∥h(D)−h(D′)∥ (4) medicaldomainwithmedicaltranscriptionsamples
KD 2
√ from40variousmedicalspecialties. Itcontains5k
≤ ∥P (Y|x)−P′(Y|x)∥ ≤ 2.
i i 2 items. Medicaldataareextremelyhardtofinddue
to HIPAA privacy regulations (Act, 1996). This
SensitivityAnalysisofTutor. InEq.3,eachsam-
datasetwasscrapedfrommtsamples.com. Weper-
|C|
ple j’s distribution P (C) = {P (c )} is the
j j i i=1 formedbasictextprocessingonthedata,convert-
binary-valuedprobabilitydistribution,itsvalueon
ingalltexttolowercaseandremovingpunctuation.
thelabelis1whilethevalueonallothercategories
Subsequently, we randomly divided 75% of the
is0. P (c ) = I(c isx ’slabel),whereIisthein-
j i i j samplesfortrainingand25%fortesting.
dicatorfunction. Thefunctiong(D) =
(cid:80)N
P (C)
j j
isthedistributiononwholedatasets. Forneighbor- EvaluationMetrics. Metricsconsistsof: accu-
ing datasetD and D′, thedifferent samplel inD racy(Acc), precision(P),recall(R),andF1-score
andD′ affectstwoterms: P (C)ing(D)andP′(C) (F1,theharmonicmeanofPandR).
l l
in g(D′). The sensitivity ∆ in Lemma 4 is
tutor Comparing Methods. We used two non-DP
(SeedeductionsinApp.B),
methodsastheperformanceupperorlowerbound:
(1) Private directly trains on private data with-
∆ = ∥g(D)−g(D′)∥ (5) outprotections. (2)DA w/ Publictrainsondata
tutor 2
√ synthesizedbypublicGPT-3.5withoutaccessing
≤ ∥P (C)−P′(C)∥ ≤ 2.
l l 2 privatedata.
We use DP-based methods as: (1) DP-SGD
CompositionofKDandTutor. OurwholeDP
(Abadi et al., 2016) trains on private data based
learningalgorithmisactuallythecombinationof
KD algorithm (§3.4) and tutor algorithm (§3.5). 3www.kaggle.com/tboyle10/medicaltranscriptions
63750trainingsamples 6.2 AblationStudies
Method
P R F1 Acc
We conducted ablation studies to evaluate the ef-
Ours 0.340 0.373 0.337 0.372 fectivenessofourproposedcomponents. Table2
Ours−Multi-teacher 0.280 0.297 0.213 0.298
presentstheprecision,recall,F1-score,andaccu-
Ours−Gaussian 0.327 0.333 0.263 0.335
racyofOurs’variantsonthedownstreamtask. (1)
Ours−Tutor 0.340 0.340 0.313 0.340
Ours−Tutor+LabelDist. 0.347 0.377 0.340 0.377 Ours − Multi-teacher: onlyuseoneteacherto
train the discriminator instead of multiple teach-
Table2: Ablationstudies. “+"or“−"meansusingor ers, where its subpar performance is due to this
notusingthegivenstrategy. teachercompletelydeterminesthepredictionresult
whenthereisonlyoneteachermodel,andthenoise
willdirectlyaffectthepredictionresultandbring
on DP-SGD with noises on gradients. (2) Ghost a great negative impact. where its subpar perfor-
(Li et al., 2022) trains on private data with DP- mance is due to when there is only one teacher
Adam and “ghost clipping”. (3) DA w/ DP-SGD model,thisteachercompletelydeterminesthepre-
trains DP-SGD on private data to select pseudo dictionresult,andthenoisewilldirectlyaffectthe
DA samples. (4) DA w/ Ghost trains Ghost on predictionresultandbringagreatnegativeimpact.
privatedatatoselectpseudoDAsamples. (5)Ours (2)Ours−Laplace: usetheLaplacemechanismto
denotes our proposed method. Note that DA w/ addnoisetothediscriminatorinsteadoftheGaus-
DP-SGD and DA w/ Ghost also imitates the noisy sianmechanism. Theresultdemonstratesthatthe
labeldistributionforafaircomparisonwithOurs. performanceofusingtheGaussianmechanismin
(SeeimplementationdetailsinApp.D). ourframeworkisbetterthanusingLaplaciannoise.
(3)Ours − Tutor: discardthetutor(withthelabel
distribution)andsimplyselectsamplesuniformly
6 ExperimentalResults
consideringthelabel. Thepoorperformanceshows
the significance of the proposed tutor and using
6.1 MainResults
label distribution in DA. (4) Ours − Tutor +
Label Dist.: discardthetutorbutdirectlyimitate
Table 1 presents the overall performance and we
theprivatelabeldistributiontoverifytheeffective-
canobservethat: withoutdataaugmentation(DA),
nessoflearningtheprivatelabeldistribution,which
Private acts as performance up-bound since it
is confirmed by its excellent performance. Com-
fullyaccessestheprivatedatawithoutanyprivate
pared with Ours, it can learn a label distribution
protection. Thereareonly3750trainsampleswith-
closertorealdatawithoutanyprivacyprotection.
outDA.TheNoisy-SGDmethods(i.e. DP-SGDand
Ourtutorachievessimilarperformanceevenafter
Ghost)withDAexhibitsignificantincreasestothe
addingnoise.
samemethodswithoutDA,whichshowstheeffec-
tivenessofourdesignedDAframework. Notethat
6.3 Privacy-utilityTradeoff
we keep the sample number of DA and non-DA
methodssameforafaircomparison,andDAstill
worksbettersinceDAobtainshighqualitysamples
withalessprivacycost.
Ours outperforms other baseline methods in
a meaningful range of privacy protection (ε is
4), even Private. The reason for outperforming
Private, which acts as the up-bound, is that our
synthesizeddatahavehigherqualityanddiversity
thantheprivatedatasincetheyaresampledfrom
LLMs,whichimprovesthegeneralizationability
ofclassificationmodelsastrainingdataarenotsuf-
ficient. When the training samples selected from Figure2: Theprivate-utilitytradeoffinaccuracyacross
a fixed-size synthetic dataset increase from 3750 threeDAw/DPmethodsatvaryingε. Theverticalaxis
to6000,theperformanceofallmethodsgenerally representstheaccuracyondownstreamtextclassifica-
improves. tiontasks.
7Figure 3: The private-utility tradeoff in the discrimi-
nator’s accuracy across three DA w/ DP methods at
varyingε. Theverticalaxisrepresentstheaccuracyon Figure4: Analysisonteachernumber. Theverticalaxis
ourconstructedtestset. representsthepredictionaccuracyofthediscriminator
onourconstructedtestset.
InFig.2andFig.3,weshowtheprivate-utility
trade-off curve of three DA w/ DP methods and
noiseontheaggregatedteachers’performancebe-
theirdiscriminatorscoveringtherangeofmeaning-
comesnegligible. (2)Student’sperformancecor-
fulprotection(i.e. usuallyε ∈ [0.1,10](Triastcyn
relates positively with Teachers + Noise. (3)
andFaltings,2020)). InbothfiguresOursoutper-
Inmostcases, StudentoutperformsTeachers +
formsDA w/ DP-SGDandDA w / Ghostinmost
Noise. ThisisbecauseTeachers + Noisereduces
ranges of ε ∈ [0.1,10], showing that our method
Teachers’originalpredictionaccuracybydirectly
provides strong privacy protections while having
adding noise to the inference results. Student
excellentperformance. Asεincreases,privacypro-
trainswithnoisydata,allowingthemodeltoadapt
tection becomes weaker, leading to a gradual im-
tothenoise.
provementintheperformanceofvariousmethods.
In Fig. 3, the Accuracy of the discriminator im- 6.5 AnalysisOnTutor’sDistribution
provesasεincreases,whichalignswithourexpec-
Fig.5(SeeApp.E)illustratestwodistinctlabeldis-
tations. Butwhenε > 2,Oursexhibitsfluctuations
tributions: Private Dist represents the original
ondownstreamtasks. ThisisbecausetheDP-based
privatelabeldistribution; Tutor Distrepresents
discriminatorinOursalreadyachievesnear-perfect
thelabeldistributionafteraddingnoise(discussed
accuracy (97.6%), and the performance of Ours
in§3.5),whichprovidesstricterprivacyprotection
hasexceededup-bound(36.7%). Afterexceeding
by satisfying ε = 0.4. We observe a high con-
the up-bound, the final performance is probably
sistencybetweenTutor DistandPrivate Dist,
beyond the control of the discriminator (The dis-
whichindicatesthatTutor Disteffectivelyretains
criminatorjudgeswhetherthesyntheticdatameet
thecharacteristicsofPrivate Dist.
privacycharacteristics,buttheperformanceofse-
lectedsyntheticdatabyOursexceededtheoriginal
7 Conclusion
privacydata).
Inthispaper,weproposedaDP-basedDAmethod
6.4 AnalysisOnTeacherNumbers
for text classification in private domains, which
To analyze the impact of teacher number on dis- prompts a LLM to generate pseudo samples and
criminator,weconductedexperimentsacrossvary- uses a DP-based discriminator to examine the
ing teacher number with ε = 4 (in Fig. 4). LLM’s outputs. In this way, we transfer pseudo
Teachers denotes the aggregation of multiple text generation task, which is a challenging DP
teacher models; Teachers + Noise denotes to paradigm, to a discrimination task. We construct
add noise to the voting results after aggregating a DP-based discriminator via knowledge distilla-
multiple teacher models; Student denotes our fi- tion and construct a DP-based tutor to guide the
naldiscriminatormodel. Weconcludethat: (1)As samplegenerationwithalowprivacycost. Theo-
theteachernumberincreases,theperformanceof reticalanalysesillustratestheboundofprotections
Teachers + Noisegraduallyimproves. Whenthe ofourmodelsandourexperimentalresultsshows
teacher number reaches 20, the impact of adding ourmodel’seffectiveness.
88 Limitations intendtoimplementconstraintswithinourmodel
topreventthegenerationoftextsforillegalactivi-
Inourstudy,therewereseverallimitations.
ties,suchasintroducingfilterstoidentifyandflag
(1)Firstofall,weuseGPT-3.5insteadofGPT-
potentiallyharmfulorillegalcontent.
4 in our experiments and GPT-3.5 is not a latest
(3)Moreover,it’simperativetoexercisecaution
and SOTA GPT API, which seems to limited the
inutilizingourmodelandrefrainfromassuming
performanceofourmodel. Themainreasonisthat
itsinfallibility. Onepotentialunethicalapplication
our baselines (Abadi et al., 2016; Li et al., 2022)
involves gathering data from users who believe
are all based on GPT-3.5 for a fair comparison.
ourmodelguaranteescompleteprivacyprotection,
In addition, compared to GPT-3.5, GPT-4 costs
potentiallyoverlookingtheactualstrengthofpri-
too much to obtain API keys. According to the
vacy safeguards. This oversight could lead to ad-
officialwebsite,thefeeforGPT3.5is0.002$/1k
verse societal consequences. Therefore, we urge
Token, and the fee for GPT 4 is 0.06$/1k Token.
researchersintendingtoutilizethismodeltopriori-
Inouractualbaselinecomparison,about560,000
tizetheefficacyofprivacyprotection. Additionally,
piecesofdataweregeneratedusingGPT-3.5,and
measures should be taken to prevent researchers
the total cost was about 300$. And if GPT-4 is
fromcollectingdatafromuserswholackaproper
used, the same number of tokens will cost more
understandingofouralgorithm. Forexample,Clar-
than 9000$. Therefore, the use of GPT-3.5 can
ify privacy policies and rules for data usage, and
not only effectively reduce the cost significantly,
restrictaccesstocollecteddatatoauthorizedper-
but also there will not be much difference in the
sonnel only. We recommend that researchers en-
generationeffect. Atpresent,weuseGPT-3.5for
sureuserscontributingtheirdatacomprehendthe
method verification, and GPT-4 will be used for
risksassociatedwithourmodelfully.
effectverificationinthefuture.
(4)Ingeneral,ifthedatasetcontainsprivacy,it
(2) Secondly, our approach is based on LLM-
maybeleakedduringuse. Regardingthedatasets
generateddata,relativelydependentonthequality
utilizedinourexperiments,thedatasetgenerated
of the generated text. If the data are difficult to
byLLMdoesnotcontainthepersonallyidentifiable
generate, or the overall quality of the generated
informationoftherealuser. Incontrast,theMedi-
textispoor,thismaylimittheadvantagesofourap-
calTranscriptionsdatasetcontainssamplemedical
proach. Subsequently,wecanleverageapproaches
transcriptions for various medical specialties, al-
suchasprompttuning,bycarryingoutavarietyof
lowingustoconductexperimentstoassesstheeffi-
excellent prompt engineering during the training
cacyofprivacyprotectionmeasures. It’simportant
phase of the LLM, enhancing the quality of text
tonotethattheMedicalTranscriptionsdatasetwas
generatedbyLLM.
previouslymadeavailabletothepublic. Therefore,
ourresearchinthispaperdoesnotinvolvereleasing
9 EthicalConsiderations
anyadditionalpersonalinformationofusers.
Weplacesignificantimportanceonethicalconsid-
erationsandadhererigorouslytotheACLEthics
Policy. References
(1)Thisstudyintroducesanoveltextclassifica-
Martin Abadi, Andy Chu, Ian Goodfellow, H Bren-
tionmethod,utilizinganLLMtogeneratepseudo dan McMahan, Ilya Mironov, Kunal Talwar, and
samplesandaDP-baseddiscriminatortoevaluate LiZhang.2016. Deeplearningwithdifferentialpri-
vacy. InCCS,pages308–318.
them, without ethical concerns regarding motiva-
tion or algorithmic approach, as no private infor-
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama
mationisused. Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
(2)Nevertheless,it’scrucialtocontemplatesce- DiogoAlmeida,JankoAltenschmidt,SamAltman,
ShyamalAnadkat,etal.2023. Gpt-4technicalreport.
narios where individuals deliberately exploit our
arXivpreprintarXiv:2303.08774.
modelforillicitpurposes. Forinstance,someone
might use the text generation model, used in this AccountabilityAct.1996. Healthinsuranceportability
paperforgeneratingpseudodata,tofabricatefake andaccountabilityactof1996. Publiclaw,104:191.
textormisinformation. Thispotentialmisuseposes
RohanAnil,BadihGhazi,VineetGupta,RaviKumar,
anegativesocietalimpact,usingourmodeltogen-
andPasinManurangsi.2021. Large-scaledifferen-
erate false medical reports. Moving forward, we tiallyprivatebert. arXivpreprintarXiv:2108.01624.
9BorjaBalleandYu-XiangWang.2018. Improvingthe Jieren Deng, Chenghong Wang, Xianrui Meng, Yi-
gaussian mechanism for differential privacy: Ana- jue Wang, Ji Li, Sheng Lin, Shuo Han, Fei Miao,
lytical calibration and optimal denoising. In Inter- Sanguthevar Rajasekaran, and CaiwenDing. 2022.
national Conference on Machine Learning, pages Asecureandefficientfederatedlearningframework
394–403.PMLR. fornlp. arXivpreprintarXiv:2201.11934.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
2014. Privateempiricalriskminimization: Efficient KristinaToutanova.2019. Bert: Pre-trainingofdeep
algorithmsandtighterrorbounds. InProceedingsof bidirectionaltransformersforlanguageunderstand-
the2014IEEE55thAnnualSymposiumonFounda- ing. InNAACL,pages4171–4186.
tionsofComputerScience,pages464–473.
Jinshuo Dong, Aaron Roth, and Weijie J Su. 2019.
FranziskaBoenisch, ChristopherMühl, RoyRinberg, Gaussian differential privacy. arXiv preprint
Jannis Ihrig, and Adam Dziedzic. 2023. Individu- arXiv:1905.02383.
alizedpate: Differentiallyprivatemachinelearning
withindividualprivacyguarantees. Proceedingson HaonanDuan,AdamDziedzic,NicolasPapernot,and
PrivacyEnhancingTechnologies,1:158–176. FranziskaBoenisch.2023. Flocksofstochasticpar-
rots: Differentiallyprivatepromptlearningforlarge
ZhiqiBu,JinshuoDong,QiLong,andWeijieJSu.2020.
languagemodels. arXivpreprintarXiv:2305.15594.
Deeplearningwithgaussiandifferentialprivacy. In
Harvard Data Science Review, volume 2020. NIH CynthiaDwork, FrankMcSherry, KobbiNissim, and
PublicAccess. Adam Smith. 2006. Calibrating noise to sensitiv-
ity in private data analysis. In Theory of Cryptog-
ZhiqiBu,HuaWang,ZongyuDai,andQiLong.2023.
raphy: Third Theory of Cryptography Conference,
Ontheconvergenceandcalibrationofdeeplearning
TCC 2006, New York, NY, USA, March 4-7, 2006.
withdifferentialprivacy. TransactionsonMachine
Proceedings3,pages265–284.Springer.
LearningResearch.
CynthiaDwork,AaronRoth,etal.2014a. Thealgorith-
ZhiqiBu,HuaWang,QiLong,andWeijieJSu.2021.
micfoundationsofdifferentialprivacy. Foundations
Ontheconvergenceofdeeplearningwithdifferential
andTrends®inTheoreticalComputerScience,9(3–
privacy. InarXivpreprintarXiv:2106.07830.
4):211–407.
Pengfei Cao, Xinyu Zuo, Yubo Chen, Kang Liu, Jun
Cynthia Dwork, Aaron Roth, et al. 2014b. The algo-
Zhao, and Wei Bi. 2021. Uncertainty-aware self-
rithmicfoundationsofdifferentialprivacy. InTCS,
trainingforsemi-supervisedeventtemporalrelation
volume9,pages211–407.
extraction. In Proceedings of the 30th ACM Inter-
nationalConferenceonInformation&Knowledge
Li Fei-Fei, Robert Fergus, and Pietro Perona. 2006.
Management,pages2900–2904.
One-shotlearningofobjectcategories. IEEEtrans-
actionsonpatternanalysisandmachineintelligence,
NicholasCarlini,ChangLiu,ÚlfarErlingsson,Jernej
28(4):594–611.
Kos,andDawnSong.2019a. Thesecretsharer:Eval-
uatingandtestingunintendedmemorizationinneu-
OluwaseyiFeyisetan,BorjaBalle,ThomasDrake,and
ralnetworks. In28thUSENIXSecuritySymposium
TomDiethe.2020. Privacy-andutility-preservingtex-
(USENIXSecurity19),pages267–284.
tualanalysisviacalibratedmultivariateperturbations.
NicholasCarlini,ChangLiu,ÚlfarErlingsson,Jernej
InProceedingsofthe13thinternationalconference
Kos,andDawnSong.2019b. Thesecretsharer:Eval-
onwebsearchanddatamining,pages178–186.
uatingandtestingunintendedmemorizationinneural
ChelseaFinn,PieterAbbeel,andSergeyLevine.2017.
networks. InUSENIXSecurity,pages267–284.
Model-agnosticmeta-learningforfastadaptationof
Nicholas Carlini, Florian Tramer, Eric Wallace, deepnetworks. InICML,pages1126–1135.
Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee,AdamRoberts,TomBrown,DawnSong,Ulfar SimsonGarfinkeletal.2015. De-identificationofPer-
Erlingsson,etal.2021. Extractingtrainingdatafrom sonalInformation:. USDepartmentofCommerce,
large language models. In 30th USENIX Security NationalInstituteofStandardsandTechnology.
Symposium(USENIXSecurity21),pages2633–2650.
HongyuGuo,YongyiMao,andRichongZhang.2019.
KamalikaChaudhuri,ClaireMonteleoni,andAnandD Augmenting data with mixup for sentence clas-
Sarwate.2011. Differentiallyprivateempiricalrisk sification: An empirical study. arXiv preprint
minimization. Journal of Machine Learning Re- arXiv:1905.08941.
search,12(3).
JiyanHe,XuechenLi,DaYu,HuishuaiZhang,Janard-
JiaaoChen, ZhenghuiWang, RanTian, ZichaoYang, hanKulkarni,YinTatLee,ArtursBackurs,Nenghai
and Diyi Yang. 2020. Local additivity based data Yu, and Jiang Bian. 2022. Exploring the limits of
augmentationforsemi-supervisedner. arXivpreprint differentiallyprivatedeeplearningwithgroup-wise
arXiv:2010.01677. clipping. arXivpreprintarXiv:2212.01539.
10ZhitingHu,BowenTan,RussRSalakhutdinov,TomM Wakana Maeda, Yu Suzuki, and Satoshi Nakamura.
Mitchell, and Eric P Xing. 2019. Learning data 2016. Fasttextanonymizationusingk-anonyminity.
manipulationforaugmentationandweighting. Ad- InProceedingsofthe18thInternationalConference
vances in Neural Information Processing Systems, onInformationIntegrationandWeb-basedApplica-
32. tionsandServices,pages340–344.
Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Brendan McMahan, Eider Moore, Daniel Ramage,
2019. Clinicalbert: Modeling clinical notes and Seth Hampson, and Blaise Aguera y Arcas. 2017.
predicting hospital readmission. arXiv preprint Communication-efficientlearningofdeepnetworks
arXiv:1904.05342. fromdecentralizeddata. InArtificialintelligenceand
statistics,pages1273–1282.PMLR.
AaronJohnsonandVitalyShmatikov.2013. Privacy-
preserving data exploration in genome-wide asso- Nicolas Papernot, Martın Abadi, Ulfar Erlingsson,
ciation studies. In Proceedings of the 19th ACM Ian Goodfellow, and Kunal Talwar. 2017. Semi-
SIGKDDinternationalconferenceonKnowledgedis- supervisedknowledgetransferfordeeplearningfrom
coveryanddatamining,pages1079–1087. privatetrainingdata. InICLR.
AkbarKarimi,LeonardoRossi,andAndreaPrati.2021. Li Qing, Weng Linhong, and Ding Xuehai. 2019. A
Aeda: aneasierdataaugmentationtechniquefortext novelneuralnetwork-basedmethodformedicaltext
classification. arXivpreprintarXiv:2108.13230. classification. FutureInternet,11(12):255.
Gavin Kerrigan, Dylan Slack, and Jens Tuyls. 2020. LucRocher,JulienMHendrickx,andYves-Alexandre
Differentiallyprivatelanguagemodelsbenefitfrom De Montjoye. 2019. Estimating the success of re-
publicpre-training. InWorkshopinEMNLP,pages identificationsinincompletedatasetsusinggenera-
39–45. tivemodels. Naturecommunications,10(1):1–9.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A BenjaminIPRubinstein,PeterLBartlett,LingHuang,
methodforstochasticoptimization. arXivpreprint and Nina Taft. 2009. Learning in a large function
arXiv:1412.6980. space:Privacy-preservingmechanismsforsvmlearn-
ing. arXivpreprintarXiv:0911.5708.
Sosuke Kobayashi. 2018. Contextual augmentation:
Dataaugmentationbywordswithparadigmaticrela- Weiyan Shi, Aiqi Cui, Evan Li, Ruoxi Jia, and Zhou
tions. arXivpreprintarXiv:1805.06201. Yu.2021. Selectivedifferentialprivacyforlanguage
modeling. InarXivpreprintarXiv:2108.12944.
Ksenia Konyushkova, Raphael Sznitman, and Pascal
Fua.2017. Learningactivelearningfromdata. Ad- ShuangSong,KamalikaChaudhuri,andAnandDSar-
vancesinneuralinformationprocessingsystems,30. wate.2013. Stochasticgradientdescentwithdifferen-
tiallyprivateupdates. InGlobalSIP,pages245–248.
Varun Kumar, Ashutosh Choudhary, and Eunah Cho. IEEE.
2020. Data augmentation using pre-trained trans-
formermodels. arXivpreprintarXiv:2003.02245. YuSuzuki,KoichiroYoshino,andSatoshiNakamura.
2018. Ak-anonymizedtextgenerationmethod. In
Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Advances in Network-Based Information Systems:
Xue, and Xipeng Qiu. 2020. Bert-attack: Adver- The20thInternationalConferenceonNetwork-Based
sarialattackagainstbertusingbert. arXivpreprint InformationSystems(NBiS-2017),pages1018–1026.
arXiv:2004.09984. Springer.
XiangLi,MenglinCui,JingpengLi,RuibinBai,Zheng Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Lu,andUweAickelin.2021. Ahybridmedicaltext JonShlens,andZbigniewWojna.2016. Rethinking
classificationframework: Integratingattentiverule the inception architecture for computer vision. In
constructionandneuralnetwork. Neurocomputing, Proceedings of the IEEE conference on computer
443:345–355. visionandpatternrecognition,pages2818–2826.
XuechenLi,FlorianTramèr,PercyLiang,andTatsunori ZhiliangTian,YingxiuZhao,ZiyueHuang,Yu-Xiang
Hashimoto. 2022. Large language models can be Wang,NevinLZhang,andHeHe.2022. Seqpate:
strongdifferentiallyprivatelearners. InICLR. Differentiallyprivatetextgenerationviaknowledge
distillation. AdvancesinNeuralInformationProcess-
Xiaotian Lin, Nankai Lin, Yingwen Fu, Ziyu Yang, ingSystems,35:11117–11130.
and Shengyi Jiang. 2023. How to choose" good"
samplesfortextdataaugmentation. arXivpreprint HugoTouvron,ThibautLavril,GautierIzacard,Xavier
arXiv:2302.00894. Martinet,Marie-AnneLachaux,TimothéeLacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Ilya Loshchilov and Frank Hutter. 2017. Decou- Faisal Azhar, et al. 2023. Llama: Open and effi-
pled weight decay regularization. arXiv preprint cient foundation language models. arXiv preprint
arXiv:1711.05101. arXiv:2302.13971.
11Aleksei Triastcyn and Boi Faltings. 2020. Bayesian P′ (Y|x)| ∈ (0,1)foreachj. Hence,wehave,
ij
differential privacy for machine learning. In Inter-
national Conference on Machine Learning, pages (cid:18) |Y| (cid:19)1/2
9583–9592.PMLR. (cid:88) (P (Y|x)−P′ (Y|x))2 (7)
ij ij
j=1
JasonWeiandKaiZou.2019. Eda:Easydataaugmenta-
(cid:18) |Y| (cid:19)1/2
tiontechniquesforboostingperformanceontextclas- (cid:88)
≤ |P (Y|x)−P′ (Y|x)|
sificationtasks. arXivpreprintarXiv:1901.11196. ij ij
j=1
(cid:18) |Y| (cid:19)1/2
TongWu,AshwineePanda,JiachenTWang,andPra- (cid:88)
≤ |P (Y|x)+P′ (Y|x)|
teek Mittal. 2023. Privacy-preserving in-context ij ij
learningforlargelanguagemodels. arXive-prints, j=1
pagesarXiv–2305.
Weknow|a+b| = a+bwhena,b ∈ (0,1),so
wehave,
Yiben Yang, Chaitanya Malaviya, Jared Fernandez,
SwabhaSwayamdipta,RonanLeBras,Ji-PingWang,
(cid:18) |Y| (cid:19)1/2
ChandraBhagavatula,YejinChoi,andDougDowney. (cid:88)
|P (Y|x)+P′ (Y|x)| (8)
2020. Generative data augmentation for common- ij ij
sensereasoning. arXivpreprintarXiv:2004.11546. j=1
(cid:18) |Y| |Y| (cid:19)1/2
(cid:88) (cid:88)
Da Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. = P ij(Y|x)+ P i′ j(Y|x)
2020. Donotletprivacyoverbillutility:Gradientem- j=1 j=1
beddingperturbationforprivatelearning. InICLR.
(cid:18) (cid:19)1/2 √
= 1+1 ≤ 2,
XiangYue,HuseyinAInan,XuechenLi,GirishKumar,
Julia McAnallen, Hoda Shajari, Huan Sun, David
Insummary,theupperboundofthesensitivity
Levitan,andRobertSim.2022. Synthetictextgener-
is,
ationwithdifferentialprivacy:Asimpleandpractical
recipe. arXivpreprintarXiv:2210.14348.
∆ = ∥h(D)−h(D′)∥ (9)
KD 2
√
RongzhiZhang, YueYu, andChaoZhang.2020. Se- ≤ ∥P i(Y|x)−P i′(Y|x)∥ 2 = 2.
qmix: Augmentingactivesequencelabelingviase-
quencemixup. arXivpreprintarXiv:2010.02322.
B DeductionofSensitivity∆
tutor
Yuqing Zhu, Xiang Yu, Manmohan Chandraker, and WeobtaintheEquations(5)in§4ofthepaperbody
Yu-XiangWang.2020. Private-knn: Practicaldiffer-
sinceP (C)andP′(C)arethesamplel’sdistribu-
entialprivacyforcomputervision. InCVPR,pages l l
tion.
11854–11862.
∆ = ∥g(D)−g(D′)∥ (10)
tutor 2
A DeductionofSensitivity∆ KD ≤ ∥P (C)−P′(C)∥
l l 2
We obtain the Equations (4) in §4 of the paper
(cid:18) (cid:88)|C| (cid:19)1/2
= (P (C)−P′ (C))2
body since P (Y|x) and P′(Y|x) are the output lv lv
i i
v=1
distribution of i-th teacher model. where x is a
givensampleandY = {0,1}. Because l’s distribution is a binary probability
distribution with values 1 on its labels and 0 on
∆
KD
= ∥h(D)−h(D′)∥
2
(6) all other classes, (cid:80)| vC =| 1(P lv(C)−P l′ v(C))2 has a
maximumvaluewhensamplelbelongstodifferent
≤ ∥P (Y|x)−P′(Y|x)∥
i i 2 categoriesinneighboringdatasetsD andD′.
(cid:18) |Y| (cid:19)1/2
(cid:88)
= (P ij(Y|x)−P i′ j(Y|x))2 (cid:18) (cid:88)|C| (cid:19)1/2
(P (C)−P′ (C))2 (11)
j=1 lv lv
v=1
(cid:18) (cid:19)1/2 √
We know (P ij(Y|x) − P i′ j(Y|x))2 is smaller ≤ 1+1 = 2
than |P (Y|x) − P′ (Y|x)| since |P (Y|x) −
ij ij ij
12Figure5: Labeldistributions. Thehorizontalaxisenumeratesalldatalabels,whiletheverticalaxisrepresentsthe
frequencyofthelabels.
Insummary,theupperboundofthesensitivity D ImplementationDetails
is,
Weselect“gpt-3.5-turbo”withmaximum4,096to-
∆ = ∥g(D)−g(D′)∥ (12) kensconstructsynthetictext. IntheDP-basedDA.
tutor 2
√ We fine-tune teachers and students based on the
≤ ∥P l(C)−P l′(C)∥ 2 ≤ 2. publicpre-trainedRoBERTa-large4 (Devlinetal.,
2019),whichisrepresentativeandperformswell
C DetailedDeductionofComposition
inbinaryclassificationtasks. Wesetdefaultnum-
(KDandTutor)
ber of teacher models is 15 and use the AdamW
(Loshchilov and Hutter, 2017) optimizer with a
The KD algorithm M satisfies (ε , δ )-
KD KD KD learning rate of 2 × 10−5. The default target ε
DP and the tutor algorithm M satis-
tutor is 4 in §3.4 and 0.4 in §3.5, both δ is 10−6. We
fies (ε , δ )-DP. Since the output re-
tutor tutor
use the autoDP5 to obtain the standard deviation
sults of KD and Tutor are independent, for
σ oftheGaussianmechanism(Dongetal.,2019)
∀(D ×D ) ∈ (R ×R ),thereis
KD tutor KD tutor
is6. Inthedownstreamclassificationtask,weuse
ClinicalBERT(Huangetal.,2019)asthePLMwith
Pr[(M ),(M ) ∈ (D ×D )]
KD tutor KD tutor
batch size of 64 and the AdamW optimizer with
=Pr[(M ) ∈ D ]Pr[(M ) ∈ D ]
KD KD tutor tutor thelearningrateof10−6. ClinicalBERTisaBERT
≤([eεtutorPr′[(M ) ∈ D ]]∧1+δ )
tutor tutor tutor derivativespecificallyforclinicalmedicine,outper-
×Pr[(M ) ∈ D ] formsRoBERTa-largeinthemedicalfield. Allex-
KD KD
≤([eεtutorPr′[(M ) ∈ D ]]∧1) perimentswereperformedusingasingleNVIDIA
tutor tutor
RTX3090GPU.
×Pr[(M ) ∈ D ]
KD KD
+δ tutorPr[(M KD) ∈ D KD] E LabelDistributions
≤eεtutorPr′[(M ) ∈ D ]
tutor tutor Fig. 5 plots two distinct label distributions:
×Pr[(M ) ∈ D ]+δ
KD KD tutor Private Dist represents the original private la-
≤eεKD+tutorPr′[(M ) ∈ D ] bel distribution; Tutor Distrepresents thelabel
tutor tutor
×[Pr′[(M ) ∈ D ]+δ ]+δ distributionoftutor.
KD KD KD tutor
≤eεKD+tutorPr′[(M ) ∈ D ]
tutor tutor
×Pr′[(M ) ∈ D ]+δ +δ
KD KD KD tutor
=eεKD+tutor
′
×Pr[(M ),(M ) ∈ (D ×D )]
KD tutor KD tutor
+δ +δ . (13)
KD tutor
Therefore, it can be concluded from the above
4WehavetriedLLM(i.e.GPT-3.5)asdiscriminatorwith
derivationthatthecombinationMsatisfies(ε +
KD theaccuracyislessthan60%.
ε tutor,δ
KD
+δ tutor)-DP. 5https://github.com/yuxiangw/autodp
13