Q-FOX Learning: Breaking Tradition in Reinforcement Learning
Mahmood Alqaseer1* Yossra H. Ali2 Tarik A. Rashid3
1,2Department of Computer Science, University of Technology, Baghdad, Iraq
3Department of Computer Science and Engineering, University of Kurdistan Hewlêr, Erbil, Iraq
* Corresponding author’s Email: mwdali93@gmail.com
2 Yossra.H.Ali@uotechnology.edu.iq 3 tarik.ahmed@ukh.edu.krd
Abstract
Reinforcement learning (RL) is a subset of artificial intelligence (AI), where agents learn the best
action by interacting with the environment, making it suitable for tasks that do not require labeled
data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that
leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial
process because variations in this parameter lead to changes in the overall learning aspects, and
different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is
proposed. This uses both the FOX optimizer—a new optimization method inspired by nature that
mimics red foxes’ hunting behavior—and the commonly used, easy-to-implement RL Q-learning
algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed,
which prioritizes the reward over the mean squared error (MSE) and learning time (steps). Q-FOX
has been evaluated on two OpenAI Gym environment control tasks: Cart Pole and Frozen Lake. It
exposed greater cumulative rewards than HP tuning with other optimizers, such as PSO, GA, Bee,
or randomly selected HP. The cumulative reward for the Cart Pole task was 32.08, and for the
Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has limitations. It cannot be used
directly in real-word problems before choosing the HP in a simulation environment because its
processes work iteratively, making it time-consuming. The results indicate that Q-FOX has played
an essential role in HP tuning for RL algorithms to effectively solve different control tasks.
Keywords: Reinforcement Learning, Q-learning, Optimization, FOX Optimizer, Hyperparameters.
1. Introduction
RL is the study of decision-making. It involves selecting the effective actions to achieve the maximum
returns in a given state. This behavior is learned through interactions with the environment by observations
of its responses in trail-and-error style, similar to how children explore their surroundings and learn the
actions that lead to goal achievement. This approach is suitable when the problem has no labeled data [1].
The Q-learning is considered a standard algorithm in RL, which is model-free, off-policy, widely used, and
simple to implement to solve finite Markov decision processes (MDPs). RL algorithms solve problems,
such as gaming, controlling, and scheduling, by moving from one state to another based on an
approximating action-value function in the problem’s environment to select the optimal action that make a
correct decision in the current state[2]. The process of approximating the action-value function is repeated
several episodes to increase the reward and find the optimal policy [3]. A recently proposed metaheuristic
optimization algorithm is The FOX optimizer (FOX) that takes inspiration from nature by mimic how foxes’
hunt the prey [4]. In RL algorithms, the use of HP is required, which are parameters that are initialized
before the learning process that able to control and affect the algorithm’s learning process overall [5]. While
RL algorithms can yield reliable decisions and deliver maximum rewards, they require manual tuning of
the HP to achieve their optimum efficiency.The difficulty is in determining the most effective values for each HP, as even small adjustments might
influence the decision-making process in any RL system. Manually tuning HP is important for the
effectiveness of RL algorithms. However, it can be complex and time-intensive. Researchers are tasked
with adjusting these parameters precisely to make reliable decisions within the problem's environment,
making it a hot research area. Several research studies have suggested ways for HP tuning by manually or
randomly adjusting values for each parameter [6]. Nevertheless, these techniques may only apply to some
RL problems due to variations in environmental variables and conditions. This paper uses the FOX
optimizer algorithm for automatic HP tuning in the Q-learning algorithm to determine the optimal HP that
results in the desired decisions with the maximum reward and the lowest learning error (steps).
The paper's main contribution is the development of the Q-FOX method, a novel approach that utilizes the
FOX optimizer to tune HP automatically in the Q-learning algorithm. This technique solves the problems
associated with manual HP tuning in RL algorithms. It offers a more automated and potentially effective
solution that addresses the challenges of manual tuning. Furthermore, a new objective function has been
proposed because the FOX is a single objective optimizer. This function includes reward, error, and learning
time and prioritizes reward over error and learning time.
Q-FOX was evaluated through two control tasks in OpenAI Gym environments: Frozen Lake and Cart Pole
[7]. Frozen Lake presents a grid-world design where the agent must navigate slippery terrain to reach a goal
while avoiding hazards, requiring decision-making in a stochastic environment. On the other hand, a Cart
Pole balances a pole on a moving cart with precise control to maintain stability in a changing environment.
These tasks are benchmarks for evaluating RL algorithms, offering many challenges that test the robustness
of the proposed approach across different control situations. This paper’s experiments show that Q-FOX
has improved the overall Q-learning algorithm’s performance and increased the returns with minimum
learning error (steps) by automatically tuning the HP through the FOX optimizer instead of manually tuning
or using other optimizers.
This text is structured as follows: Section 2 presents the latest studies in the scope of the addressed problem.
Section 3 presents a methodology for the proposed Q-FOX and provides background information on FOX,
Q-learning, and OpenAI Gym control tasks. Sections 4 and 5 list and discuss the results. Finally, Section 6
presents the study's conclusion and suggestions for future work.
2. Related work
Recently, considerable researchers have created approaches for HP tuning in RL algorithms. This section
has reviewed the most noteworthy papers in this field. A recent work by Selvi and Vijayakumaran utilized
a FOX optimizer with Double Q-learning (DDQ) to detect emotions by analyzing EEG, ECG, and GSR
signals. FOX was responsible for balancing HP and local optima to achieve the highest level of accuracy.
According to this study, the FOX optimizer effectively identifies the five human emotions using the DDQ
algorithm when used with the FOX optimizer [8]. Zhu et al. designed a framework for reducing the burden
of HP tuning for computing state features by randomly using feature approximation and a factorial policy
to process several discrete actions. The proposed framework reduced the overhead of HP tuning in the
context of large-scale control tasks [9]. Also, Kiran and Ozyildirim developed a software package that uses
a scalable genetic algorithm to adjust and search for the best HP in deep RL. This package has been
experimented with in gym environments and compared to the Bayesian approach. The study's findings show
that this approach is highly effective at maximizing reward with fewer episodes [10]. A research team from
DeepMind conducted a study on selecting HP for offline RL and focused on ways to choose the best policy
from multiple trained policies with different HPs. Through extensive practical evaluation, they found offline
RL algorithms could be more robust to HP choices. Additionally, the technique used for approximating Q-
values can significantly affect HP tuning. Lastly, they concluded that HP tuning is reachable, even in
complex tasks involving observations of pixels and action spaces with high dimensions and long horizons
[11]. Ottoni et al. present a method for solving sequential ordering problems (SOPs) using RL, combined
with Scott-Knott methods to tune the HP of two RL algorithms, SARSA and Q-learning. RL application
has been tested using standards from the TSPLIB library. Thus, the mentioned method has indicated that
SARSA dominates Q-learning performance for this problem [12].Furthermore, Fernandez and Caarls usedthe Genetic Algorithm (GA) to tune the HP in both Q-learning and Sarsa algorithms. It was adopted to solve
the Pendulum problem and found that automatic tuning led to maximizing reward and fast learning speed
of the RL agent. However, this method requires multiple restarts of GA randomly to escape from local
minima. [13], [14]. This section emphasizes the progress made in HP tuning in recent years. The techniques
developed in this field have demonstrated notable enhancements in maximizing the efficiency of RL,
especially in HP tuning. Refer to Table 1 for a more detailed review.
Table 1: Methodologies and key findings in recent research of RL for HP tuning.
Authors Year Methodology Key Findings
Effective balancing of HP and local optima,
Selvi and
2023 FOX optimizer with DQN demonstrating the FOX's efficacy when
Vijayakumaran
applied with DDQ.
Random feature
Reduced overhead in HP tuning, particularly
Zhu et al. 2021 approximation and factorial
beneficial for large-scale control tasks.
policy
Superior effectiveness in maximizing reward
Kiran and
2021 Scalable genetic algorithm with fewer episodes compared to Bayesian
Ozyildirim
approaches
Non-robust nature of offline RL algorithms
Exploration of HP
Paine et al. 2020 to HP choices, emphasizing the impact of Q
selection for offline RL
value estimation methods on tuning
SARSA and Q-learning SARSA dominance over Q-learning for
Ottoni et al. 2019
with Scott-Knott methods SOPs when tuned with Scott-Knott methods
Fernandez and SARSA and Q-learning Maximizing reward and increasing the
2018
Caarls with genetic algorithm learning speed of the RL agent
3. Methodology
An automated HP tuning method for the Q-learning algorithm has been developed. The proposed method
uses FOX instead of manual or random tuning. Optimizing the HP to maximize reward with the fewest
episodes and steps can significantly enhance Q-learning performance. Automatically tuning the process for
the HP in the proposed method can save valuable time and resources while also boosting the efficiency and
effectiveness of the RL applications. The following subsections explain different algorithms, such as Q-
learning, FOX, and the proposed Q-FOX.
3.1. Q-learning algorithm
One of the most promising fields of AI is RL, which can learn directly from interactions between the agent
and the environment [15]. RL does not require training data or a target label; it only needs to take action in
the problem's environment by the agent and attempt to earn a reward [16]. The agent's learning process
mimics how humans play a specific video game. The agent always attempts to win; if it loses, it will avoid
the behavior that led to the loss in the next attempt [17]. In 1989, Watkins introduced a method considered
an early breakthrough in RL known as Q-learning, defined by the following equation [18]:
𝑄(𝑆 ,𝐴 )←𝑄(𝑆 ,𝐴 )+ 𝛼[𝑅 + 𝛾𝑚𝑎𝑥𝑄(𝑆 ,𝑎)−𝑄(𝑆 ,𝐴 )] (1)
𝑡 𝑡 𝑡 𝑡 𝑡+1 𝑡+1 𝑡 𝑡
𝑎
Where, 𝑄(𝑆 ,𝐴 ) is the action-value function, representing the cumulative reward expected if action 𝐴 is
𝑡 𝑡
taken in the state 𝑆. Action-value pairs are placed in a table called Q-table, which has dimensions of
[𝑛𝑜.𝑜𝑓 𝑠𝑡𝑎𝑡𝑒𝑠,𝑛𝑜.𝑜𝑓 𝑎𝑐𝑡𝑖𝑜𝑛𝑠]. 𝑆 and 𝐴 represent the current state and the current action, respectively.The agent will obtain the reward 𝑅 and move to another state 𝑆 by taking action 𝐴 in the current state 𝑆.
𝑖+1
The 𝛼 and 𝛾 are step size (alpha) and discount factor (gamma), respectively. The Q-learning is developed
to find the best course of actions that follow policy in a Markov decision process (MDP) by determining
the optimal function of action-value using the update rule Eq. (1). The agent interacts with the problem’s
environment to observe the reward and updates the action-value function to improve its policy estimates
over time. Agent interaction is done iteratively, allowing the algorithm to adjust its policy based on
observing the environment. After the learning stage, the agent should have the ability to select the best
action (optimal policy) in a given state using the estimated Q-table by the equation [19], [20]:
𝜋∗ = 𝑎𝑟𝑔𝑚𝑎𝑥 𝑄∗(𝑠,𝑎) (2)
𝑎
Where 𝜋∗ represents the optimal policy, and 𝑎𝑟𝑔𝑚𝑎𝑥 is a function that returns the action that maximizes
𝑎
the value of 𝑄(𝑎) based on learned policy. Below is a visual description of the Q-learning algorithm.
Observe current
state 𝑠
1- ε ε
Determine
exploration or
exploitation
Explotaion:
Exploration:
choose action with the
choose random action
best Q-value
Choose action 𝑎
Receive delayed
reward 𝑟
Update 𝑄−𝑡𝑎𝑏𝑙𝑒
Figure 1: Flowchart of Q-learning algorithm [21]
3.2. FOX
FOX is a new optimization algorithm inspired by red foxes' hunting behavior that aims to search for the
best solution from the entire population with the best fitness value [22]. FOX has many search agentsworking together in several iterations, each trying to find the best fitness value during the search. It involves
two major phases: exploration and exploitation. In exploration, the red FOX uses a random walk strategy
to locate and catch prey, utilizing its ability to detect ultrasound. During its search for prey, the red FOX
may listen for sounds indicating where its prey is. The red FOX enters the exploitation stage when it hears
the sound. In exploitation, the red FOX can hear ultrasound, so it takes time for the sound of its prey to
reach it. The speed of sound is 343 meters per second [23]. However, since this speed is always the same,
FOX uses a different method to measure it. The red FOX jumps when it thinks it can catch its prey based
on how long the sound takes to reach it. Thus, the red FOX's ability to catch prey depends on the time it
takes for the sound to reach them while jumping [24]:
𝐷𝑆𝐹 = 𝑆∗𝑇 (3)
𝑖 𝑖
The sound distance from the red FOX is represented by the variable 𝐷𝑆𝐹, As the same time the sound speed
in the medium is represented by the variable 𝑆 calculated by Eq. (4). Also, the variable 𝑇 represents an
arbitrary number in the range of 0 to 1. The variable 𝑖 represents the current iteration of the process [24].
𝑆 = 𝐵𝑒𝑠𝑡𝑃𝑜𝑠𝑖𝑡𝑖𝑜𝑛 /𝑇 (4)
𝑖 𝑖
The distance of a red FOX from its prey (𝐷𝐹𝑃) is defined as follows [25]:
𝐷𝐹𝑃 = 𝐷𝑆𝐹 ∗0.5 (5)
𝑖 𝑖
Once a red FOX has determined the distance between itself and its prey, it must move to jump and catch
the prey successfully. The red FOX must calculate the required jump height using the following formula to
perform this [26].
𝐽𝑢𝑚𝑝 = 0.5∗9.81∗𝑡2 (6)
𝑖
Where 9.81 represents the acceleration due to gravity and 𝑡, is time average taken by sound to travel [27].
The red FOX moves to another position in both the exploration and exploitation stages. The new position
is determined for exploitation based on one of the following two equations [28], [29]:
𝑋 = 𝐷𝐹𝑃 ∗𝐽𝑢𝑚𝑝 ∗𝑐 (7)
(𝑖+1) 𝑖 𝑖 1
𝑋 = 𝐷𝐹𝑃 ∗𝐽𝑢𝑚𝑝 ∗𝑐 (8)
(𝑖+1) 𝑖 𝑖 2
The values of 𝑐 and 𝑐 are 0.180 and 0.820, respectively. These values have been determined based on the
1 1
jump of a red FOX. The FOX's jumps are either directed towards the northeast or the opposite direction.
For exploration, the new position is determined using the following equation [30]:
𝑋 = 𝐵𝑒𝑠𝑡𝑋 ∗𝑟𝑎𝑛𝑑(1,𝑑𝑖𝑚𝑒𝑛𝑠𝑖𝑜𝑛)∗𝑚𝑖𝑛(𝑡𝑡)∗𝑎 (9)
(𝑖+1) 𝑖
The variable 𝑡𝑡 is the time average equal to the summation of the 𝑇 variable used in Eq. (3) and divided by
the dimension of the problem. 𝑡𝑡 is calculated by 2 ∗ [𝑖 –(1/𝑚𝑎𝑥(𝑖))] [31]. Recap: The FOX optimizer,
inspired by red FOX behavior in hunting, employs a static trade-off between the exploration and
exploitation stages, 50% for each. During exploration, the algorithm utilizes random walks, simulating the
FOX's prey detection ability. In the exploitation phase, the algorithm calculates the distance to prey, jump
height, and new positions. A visual representation of the FOX optimizer is shown in Figure 2.3.3. Automatic Model Tuning
The Q-learning algorithm has several HP, as listed in Table 2, that affect the agent's performance and
behavior on learned policy. From Eq. (1), the following HP can be noticed: step size (α), discount factor
(γ), and epsilon (ε). All of these are variables and must be initialized before the beginning of the learning
phase. The α parameter is crucial in updating the action-values function at each time step. It determines the
extent to which new information overrides old information, and its value is typically within the range [0, 1]
[32]. A lower α means that the algorithm will be less sensitive to new information and will take longer to
converge, while a higher α results in faster updates but may lead to instability. Therefore, choosing the
correct value for α is essential for the best performance in any learning algorithm [33], [34]. The γ parameter
affects future reward for an agent's decisions and its value range [0, 1] [35], [36]. A more significant value
indicates that the agent emphasizes future rewards, while a smaller value means that the agent gives less
importance to future rewards. This parameter controls the balance between immediate and future rewards,
affecting the agent's decision-making process [37], [38]. The ε parameter balances exploration and
exploitation when choosing actions. Its value ranges [0, 1] [39], with a higher value leading to more
exploitation and a lower value favoring more exploration. This parameter is essential for determining the
exploration and exploitation trade-off ratio, directly impacting the agent's choosing action process [40].
Start
Initialize population
Calculate fitness of search agents
If Yes
𝑟≥5
If
𝑝≥5
No No Yes
Search for new Find new X Find new X
X using Eq. (9) using Eq. (8) using Eq. (7)
Amend X if they are beyond the limitation
Calculate fitness and return the best fitness
End
Figure 2: Flowchart of FOX [4]Table 2: Summary of Q-learning HP used in this paper [41].
Parameter Purpose Range
𝛼 Extent to which new information overrides old information [0, 1]
𝛾 Importance of future rewards in the decision-making process [0, 1]
𝜀 Balance between exploration and exploitation [0, 1]
The development of a multi-objective fitness function was necessary to address the multiple objectives of
increasing reward and reducing error during the training process with minimum convergence time. This
function is needed to encapsulate the simultaneous optimization of these three objectives within a single
fitness function. Thus, this paper suggests the following fitness function:
𝑛
1
𝑓𝑖𝑡𝑛𝑒𝑠𝑠 = ∑[(2𝑅−𝑚𝑠𝑒)∗ ] (10)
𝑇
𝑖
This function is designed to give more weight to the average reward by doubling it and subtracting the mean
training error. This function is used on the last quarter of episodes from all episodes to ensure the stability
of the learning process, so 𝑖 = 𝑒𝑝𝑖𝑠𝑜𝑑𝑒𝑠 − 𝑒𝑝𝑖𝑠𝑜𝑑𝑒𝑠/4. For example, if the learning episodes 𝑛 = 1000,
start calculating the fitness function value from episode 750 to 𝑛. It should be noted that the value of 𝑇 in
this function represents the amount of time FOX takes to reach the best solution. The difficulties of
employing FOX with Q-learning to get the best HP, involved careful thinking of how to weigh the
importance of each objective and how to adjust their optimization within the constraints imposed by Table
2. This approach enabled the FOX to effectively seek solutions that optimally maximize the fitness function
Eq. (10), leading to maximizing the reward during the learning process.
3.4. Q-FOX
The Q-FOX algorithm represents a significant advancement in RL by incorporating FOX and Q-learning
in a novel approach. The algorithm demonstrates a high level of autonomy in problem-solving by
automating the tuning of HP without human intervention. Setting up the Q-FOX algorithm involves
determining the number of iterations 𝑖, agents 𝑔, and runs 𝑛, as illustrated in Figure 3. Once these
parameters are established, the FOX algorithm is employed, and its initial solution serves as the initial HP
for Q-learning. The algorithm then undergoes multiple learning iterations, during which reward, learning
error, and convergence time values are computed to derive a fitness function using Eq. (10). If suitable
convergence is not achieved, the solutions are updated using the FOX algorithm, repeated until convergence
or the completion of the specified number of iterations. This iterative approach serves to prevent Q-FOX
from getting stuck in local optima. In addition, it is carried out several times to select the most feasible
solution from each iteration, thereby improving the final result's robustness. The solution provides a
comprehensive and ready-to-use tool for real-world problem-solving, free from the limits of manual
parameter tuning, which represents the optimal HP after many runs have been performed. Q-FOX reflects
a significant advance in the RL because of its breadth and complexity.
3.5. Control tasks
Many standard control task problems are fundamental benchmarks for assessing RL algorithms'
performance. This investigation used two well-known control tasks—Frozen Lake and Cart Pole—taken
from OpenAI's gym environments [42]. In the Frozen Lake environment, an agent has to cross a grid world
over a frozen lake. This task introduces obstacles, creating a challenging environment for RL algorithms
[43], [44]. On the other hand, in the Cart Pole environment, an inverted pendulum on a moving cart needs
to be stabilized. This problem is common when evaluating the ability of RL algorithms to maintain stabilityin a continuous state space [45]. Q-FOX uses both of Frozen Lake and Cart Pole to guarantee a standardized
evaluation framework for evaluating the proposed method's performance [46]. The study advances
experiential understanding and makes relevant comparisons with previous research findings using other
methods and the proposed Q-FOX method in these control task environments.
Start
Initialize Q-FOX (𝑖, 𝑔,𝑛)
Run Q-FOX
Foreach 𝑔, run Q-
Update
learning and
solution
calculate fitness
No If Yes
convergence
No If Yes
𝑟𝑢𝑛𝑠≥𝑛
Best hyperparameters found
End
Figure 3: Flowchart of the proposed Q-FOX method
4. Results
In this paper, experiments were conducted to evaluate the performance of multiple optimization algorithms,
including PSO, Bee, GA, and RND, compared to the suggested Q-FOX. The following control task contexts
were utilized during the experiments: Cart Pole and the Frozen Lake. The experimental setup for the Q-
FOX methods comprised 50 iterations, each involving 30 agents with 25 runs. Performance metrics were
collected and analyzed, and the results are presented below after 1000 test runs. Q-FOX exhibited a
remarkable convergence speed in the tuning of HP. In Figure 4, the proposed method achieved a cumulative
reward of 0.95 within the 200 episodes. Q-FOX surpassed the performance of PSO (0.8818), RND (0.7409),
GA (0.6182), and Bee (0.2545). It is essential to note that this figure has been normalized to ensure astandard comparison between all reward scales. The results suggest that the Q-FOX method enhances the
learning efficiency of Q-learning in the Frozen Lake environment.
Figure 4: Comparison of cumulative reward achieved by Q-FOX in Frozen Lake environment.
Also, in Figure 5, Q-FOX showed a cumulative reward of 32.0773, outperforming PSO (29.9864), GA
(25.0273), Bee (22.6455), and RND (20.4636) on the Cart Pole environment. These results highlight the
versatility of the Q-FOX method in improving Q-learning efficiency across different control tasks. It is
essential to note that this figure also has been normalized to ensure a standard comparison between all
reward scales.
Figure 5: Comparison of cumulative reward achieved by Q-FOX in the Cart Pole environment.
Tables 3 and 4 present a wide-ranging breakdown of the results obtained from different optimization
methods applied to each environment, providing insights into the HP used and the resulting reward.Table 3: Frozen Lake control task - HP and reward.
Convergence
Method Alpha Gamma Epsilon Reward
(in seconds)
Q-FOX 0.7422 0.9692 0.0030 0.9500 156
PSO 0.9999 0.3757 0.0010 0.8818 210
RND 0.0921 0.6188 0.1332 0.7409 -
GA 0.3367 0.8328 0.1753 0.6182 234
Bee 0.9388 0.6396 0.9964 0.2545 197
Table 3 outlines the HP and corresponding reward for each optimization method in the Frozen Lake
environment. Q-FOX demonstrated optimal performance with an Alpha value of 0.7422, Gamma of 0.9692,
and Epsilon of 0.0030, leading to a cumulative reward of 0.9500. In comparison, PSO, RND, GA, and Bee
utilized different sets of HPs, resulting in rewards of 0.8818, 0.7409, 0.6182, and 0.2545, respectively.
Table 4: Cart Pole control task - HP and reward.
Convergence
Method Alpha Gamma Epsilon Reward
(in seconds)
Q-FOX 0.8287 0.9504 0.2590 32.0773 314
PSO 0.9990 0.6689 0.1096 29.9864 394
GA 0.8416 0.7069 0.3214 25.0273 428
Bee 0.9866 0.9940 0.7017 22.6455 367
RND 0.5582 0.3825 0.4235 20.4636 -
Table 4 outlines the HP and corresponding reward for each optimization method in the Cart Pole
environment. Q-FOX exhibited superior performance with tuned HP, including an Alpha value of 0.8287,
Gamma of 0.9504, and Epsilon of 0.2590, resulting in an outstanding cumulative reward of 32.0773.
Contrasting this, PSO, GA, Bee, and RND utilized different sets of HPs, leading to rewards of 29.9864,
25.0273, 22.6455, and 20.4636, respectively.
5. Discussion
The experimental results proved the success of the proposed Q-FOX method in tuning the HP in the Frozen
Lake and Cart Pole environments. The performance comparison against PSO, Bee, GA, and RND
consistently showed Q-FOX outperforming these methods regarding convergence speed to the optimal HP
and cumulative reward. In both environments, Q-FOX showcased remarkable convergence speeds,
achieving a cumulative reward of 0.95 within 200 episodes in the Frozen Lake environment and a
cumulative reward of 32.0773 in the Cart Pole environment. These results highlight the exceptional learning
efficiency of Q-FOX in enhancing the performance of selecting the best HP across different task
environments. The complete breakdown of HP and corresponding reward presented in Tables 3 and 4
further reinforces the superiority of Q-FOX. Optimal HP values for Alpha, Gamma, and Epsilon led to the
outstanding performances of Q-FOX, surpassing the results obtained by other optimization methods.
Notably, these HP show variability across distinct problems, indicating that each problem will have a uniqueHP after running Q-FOX. The challenge and limitation associated with the Q-FOX proposed method lies
in its direct application to real-world problems because it takes a long time and consumes considerable
resources through the optimization process when tuning the HP. Consequently, using Q-FOX in a simulated
environment representative of the real problem is advisable. This approach facilitates the acquisition of
optimal HP in the simulator, which can subsequently be directly applied to address the challenges presented
in the real-world context.
6. Conclusion
This paper introduces a novel HP tuning method called Q-FOX that automatically tunes HP in RL
algorithms, specifically Q-learning. Furthermore, the new objective function is proposed because the FOX
is a single-objective optimizer; thus, three objectives, reward, learning error, and learning time, were
included in the single fitness function. Through experiments conducted in the Frozen Lake and Cart Pole
control tasks, Q-FOX has proven more effective than traditional optimization methods used with Q-learning
regarding convergence speed and cumulative reward. This method eliminates the challenges of manual or
random tuning and provides a more efficient and effective alternative. However, it should be noted that
applying Q-FOX to real-world problems can be time-consuming and resource-intensive. Future Q-FOX
extensions will address resource utilization issues by testing it in more complex environments and
employing it with other RL algorithms.
References
[1] A. Alam, “A digital game based learning approach for effective curriculum transaction for teaching-
learning of artificial intelligence and machine learning,” in 2022 International Conference on
Sustainable Computing and Data Communication Systems (ICSCDS), IEEE, 2022, pp. 69–74.
[2] T. M. Moerland, J. Broekens, A. Plaat, and C. M. Jonker, “Model-based reinforcement learning: A
survey,” Foundations and Trends® in Machine Learning, vol. 16, no. 1, pp. 1–118, 2023.
[3] R. Carmona, M. Laurière, and Z. Tan, “Model-free mean-field reinforcement learning: mean-field
MDP and mean-field Q-learning,” The Annals of Applied Probability, vol. 33, no. 6B, pp. 5334–
5381, 2023.
[4] H. Mohammed and T. Rashid, “FOX: a FOX-inspired optimization algorithm,” Applied
Intelligence, vol. 53, no. 1, pp. 1030–1050, 2023.
[5] L. Yang and A. Shami, “On hyperparameter optimization of machine learning algorithms: Theory
and practice,” Neurocomputing, vol. 415, pp. 295–316, 2020.
[6] B. Bischl et al., “Hyperparameter optimization: Foundations, algorithms, best practices, and open
challenges,” Wiley Interdiscip Rev Data Min Knowl Discov, vol. 13, no. 2, p. e1484, 2023.
[7] S. Lang, M. Kuetgens, P. Reichardt, and T. Reggelin, “Modeling production scheduling problems
as reinforcement learning environments based on discrete-event simulation and openai gym,” IFAC-
PapersOnLine, vol. 54, no. 1, pp. 793–798, 2021.
[8] R. Selvi and C. Vijayakumaran, “An Efficient Multimodal Emotion Identification Using FOX
Optimized Double Deep Q-Learning,” Wirel Pers Commun, vol. 132, no. 4, pp. 2387–2406, 2023.
[9] L. Zhu, G. Takami, M. Kawahara, H. Kanokogi, and T. Matsubara, “Alleviating parameter-tuning
burden in reinforcement learning for large-scale process control,” Comput Chem Eng, vol. 158, p.
107658, 2022.
[10] M. Kiran and M. Ozyildirim, “Hyperparameter tuning for deep reinforcement learning
applications,” arXiv preprint arXiv:2201.11182, 2022.
[11] T. Le Paine et al., “Hyperparameter selection for offline reinforcement learning,” arXiv preprint
arXiv:2007.09055, 2020.
[12] A. L. C. Ottoni, E. G. Nepomuceno, M. S. de Oliveira, and D. C. R. de Oliveira, “Tuning of
reinforcement learning parameters applied to sop using the scott–knott method,” Soft comput, vol.
24, no. 6, pp. 4441–4453, 2020.[13] A. K. Hassan and S. N. Mohammed, “A novel facial emotion recognition scheme based on graph
mining,” Defence Technology, vol. 16, no. 5, pp. 1062–1072, 2020.
[14] F. C. Fernandez and W. Caarls, “Parameters tuning and optimization for reinforcement learning
algorithms using evolutionary computing,” in 2018 International Conference on Information
Systems and Computer Science (INCISCOS), IEEE, 2018, pp. 301–305.
[15] T. M. Moerland, J. Broekens, A. Plaat, and C. M. Jonker, “Model-based reinforcement learning: A
survey,” Foundations and Trends® in Machine Learning, vol. 16, no. 1, pp. 1–118, 2023.
[16] J. Fan, Z. Wang, Y. Xie, and Z. Yang, “A theoretical analysis of deep Q-learning,” in Learning for
dynamics and control, PMLR, 2020, pp. 486–489.
[17] L. Canese et al., “Multi-agent reinforcement learning: A review of challenges and applications,”
Applied Sciences, vol. 11, no. 11, p. 4948, 2021.
[18] M. Paniri, M. B. Dowlatshahi, and H. Nezamabadi-pour, “Ant-TD: Ant colony optimization plus
temporal difference reinforcement learning for multi-label feature selection,” Swarm Evol Comput,
vol. 64, p. 100892, 2021.
[19] S. Wei, Y. Bao, and H. Li, “Optimal policy for structure maintenance: A deep reinforcement learning
framework,” Structural Safety, vol. 83, p. 101906, 2020.
[20] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.
[21] F. Chang, T. Chen, W. Su, and Q. Alsafasfeh, “Charging control of an electric vehicle battery based
on reinforcement learning,” in 2019 10th international renewable energy congress (IREC), IEEE,
2019, pp. 1–63.
[22] L. M. Haile et al., “Hearing loss prevalence and years lived with disability, 1990–2019: findings
from the Global Burden of Disease Study 2019,” The Lancet, vol. 397, no. 10278, pp. 996–1009,
2021.
[23] M. Nilashi et al., “Factors influencing medical tourism adoption in Malaysia: A DEMATEL-Fuzzy
TOPSIS approach,” Comput Ind Eng, vol. 137, p. 106005, 2019.
[24] Y. Hussain Ali et al., “Optimization system based on convolutional neural network and internet of
medical things for early diagnosis of lung cancer,” Bioengineering, vol. 10, no. 3, p. 320, 2023.
[25] D. Połap and M. Woźniak, “Red fox optimization algorithm,” Expert Syst Appl, vol. 166, p. 114107,
2021.
[26] J. M. Kocarnik et al., “Cancer incidence, mortality, years of life lost, years lived with disability, and
disability-adjusted life years for 29 cancer groups from 2010 to 2019: a systematic analysis for the
global burden of disease study 2019,” JAMA Oncol, vol. 8, no. 3, pp. 420–444, 2022.
[27] M. A. Jumaah, A. I. Shihab, and A. A. Farhan, “Epileptic Seizures Detection Using DCT-II and
KNN Classifier in Long-Term EEG Signals,” Iraqi Journal of Science, pp. 2687–2694, 2020.
[28] A. K. Farhan, R. S. Ali, H. R. Yassein, N. M. G. Al-Saidi, and G. H. Abdul-Majeed, “A new
approach to generate multi S-boxes based on RNA computing,” Int. J. Innov. Comput. Inf. Control,
vol. 16, no. 1, pp. 331–348, 2020.
[29] A. Haque, A. Milstein, and L. Fei-Fei, “Illuminating the dark spaces of healthcare with ambient
intelligence,” Nature, vol. 585, no. 7824, pp. 193–202, 2020.
[30] P. Tan, H. Fu, and X. Ma, “Design, optimization, and nanotechnology of antimicrobial peptides:
From exploration to applications,” Nano Today, vol. 39, p. 101229, 2021.
[31] A. Zador et al., “Catalyzing next-generation artificial intelligence through neuroai,” Nat Commun,
vol. 14, no. 1, p. 1597, 2023.
[32] J. Irvin et al., “Chexpert: A large chest radiograph dataset with uncertainty labels and expert
comparison,” in Proceedings of the AAAI conference on artificial intelligence, 2019, pp. 590–597.
[33] M. S. Kadhm and A. K. A. Hassan, “Handwriting word recognition based on SVM classifier,”
International Journal of Advanced Computer Science & Applications, vol. 1, no. 6, pp. 64–68, 2015.
[34] D. Rolnick et al., “Tackling climate change with machine learning,” ACM Computing Surveys
(CSUR), vol. 55, no. 2, pp. 1–96, 2022.
[35] T. Yu and H. Zhu, “Hyper-parameter optimization: A review of algorithms and applications,” arXiv
preprint arXiv:2003.05689, 2020.[36] H. Q. Jaleel, J. J. Stephan, and S. A. Naji, “Textual Dataset Classification Using Supervised Machine
Learning Techniques,” Engineering and Technology Journal, vol. 40, no. 04, pp. 527–538, 2022.
[37] R. Khalid and N. Javaid, “A survey on hyperparameters optimization algorithms of forecasting
models in smart grid,” Sustain Cities Soc, vol. 61, p. 102275, 2020.
[38] N. Jameel and H. S Abdullah, “Intelligent feature selection methods: A survey,” Engineering and
Technology Journal, vol. 39, no. 1, pp. 175–183, 2021.
[39] D. Passos and P. Mishra, “A tutorial on automatic hyperparameter tuning of deep spectral modelling
for regression and classification tasks,” Chemometrics and Intelligent Laboratory Systems, vol. 223,
p. 104520, 2022.
[40] J. Waring, C. Lindvall, and R. Umeton, “Automated machine learning: Review of the state-of-the-
art and opportunities for healthcare,” Artif Intell Med, vol. 104, p. 101822, 2020.
[41] A. Alsharef, K. Aggarwal, Sonia, M. Kumar, and A. Mishra, “Review of ML and AutoML solutions
to forecast time-series data,” Archives of Computational Methods in Engineering, vol. 29, no. 7, pp.
5297–5311, 2022.
[42] N. Norori, Q. Hu, F. M. Aellen, F. D. Faraci, and A. Tzovara, “Addressing bias in big data and AI
for health care: A call for open science,” Patterns, vol. 2, no. 10, 2021.
[43] S. Padakandla, “A survey of reinforcement learning algorithms for dynamically varying
environments,” ACM Computing Surveys (CSUR), vol. 54, no. 6, pp. 1–25, 2021.
[44] M. Kodher, J. H. Saud, and H. S. Hassan, “Wheelchair movement based on convolution neural
network,” Engineering and Technology Journal, vol. 39, no. 6, pp. 1019–1030, 2021.
[45] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement learning for multiagent
systems: A review of challenges, solutions, and applications,” IEEE Trans Cybern, vol. 50, no. 9,
pp. 3826–3839, 2020.
[46] C. Zhang and Y. Lu, “Study on artificial intelligence: The state of the art and future prospects,” J
Ind Inf Integr, vol. 23, p. 100224, 2021.