DCVSMNet: Double Cost Volume Stereo Matching Network
Mahmoud Tahmasebi mahmoud.tahmasebi@research.atu.ie
Centre for Mathematical Modelling and Intelligent Systems for Health and Environment (MISHE),
Atlantic Technological University
Sligo, Ireland
Saif Huq shuq@ycp.edu
Department of Electrical, Computer Engineering, and Computer Science
York College of Pennsylvania
Pennsylvania, USA
Kevin Meehan kevin.meehan@atu.ie
Department of Computer Science
Atlantic Technological University
Donegal, Ireland
Marion McAfee Marion.McAfee@atu.ie
Centre for Mathematical Modelling and Intelligent Systems for Health and Environment (MISHE),
Atlantic Technological University
Sligo, Ireland
Abstract
We introduce Double Cost Volume Stereo Matching Network(DCVSMNet1) which is a novel ar-
chitecture characterised by by two small upper (group-wise) and lower (norm correlation) cost
volumes. Each cost volume is processed separately, and a coupling module is proposed to fuse
thegeometryinformationextractedfromtheupperandlowercostvolumes. DCVSMNetisafast
stereo matching network with a 67 ms inference time and strong generalization ability which can
produce competitive results compared to state-of-the-art methods. The results on several bench-
mark datasets show that DCVSMNet achieves better accuracy than methods such as CGI-Stereo
and BGNet at the cost of greater inference time.
1 Introduction
Stereomatchingnetworksattempttomimichumanvisionperceptionofdepth. Depthinformationis
fundamental for environment perception in robots, autonomous vehicle navigation, and augmented
realityapplications. Stereomatchingisavision-basedalgorithmwhichenablesformationofathree-
dimensional (3D) reconstruction of the environment from two rectified stereo images. The stereo
matching pipeline takes two rectified images as inputs to a feature extractor module. The extracted
convolutionalneuralnetwork(CNN)featuresareusedtoformacostvolumealongwiththedisparity
values. Thiscostvolumestoresfusedinformationofleftandrightfeaturestoencodelocalmatching
costs. This information is further processed by an aggregation block and regressed to estimate the
disparity map.
While stereo matching models can be roughly classified based on their speed and accuracy, there is
nocleardistinctionbetweenmodelswhichcanbeconsidered’fast’or’accurate’becausetheseterms
are highly dependent on the target application and available technology. For instance, applications
1. Thesourcecodeisavailableathttps://github.com/M2219/DCVSMNet.
1
4202
beF
62
]VC.sc[
1v37461.2042:viXraFigure 1: Comparison of DCVSMNet with state-of-the-art methods on SceneFlow dataset.
suchasautonomousvehiclesdemandstereomodelswithhighspeedtomimichuman-likeperception
of depth [1], on the other hand, many medical applications designed based on stereo endoscopy
and stereo microscopy require higher accuracy depth prediction to improve surgical precision and
patient safety with no need for a fast inference time [2]. To this end, research into deep learning
stereo matching models involves continuously testing and adopting various strategies to trade off
between the speed and accuracy for different target applications.
Considering the first block of the stereo matching pipeline which is feature extraction, designing
smaller feature extractors composed of fewer convolutional layers can lead to a fast inference time
at the cost of forming a poor cost volume and eventually less accurate prediction ability [3]. The
produced cost volume is usually four dimensional which requires expensive 3D convolutions for cost
aggregation. The computational effort and storage cost grows exponentially as the number of 3D
convolutional layers increase. Therefore, the size of the cost volume directly affects both the speed
andaccuracy. Thefactorthatcontrolsthesizeofcostvolumeisthenumberoffeaturesusedforthe
cost volume formation. Using the features from only a single scale of the feature extractor [1, 4, 5]
leadstoasmallercostvolumeandfasterinferencetimethanacostvolumeformedbyconcatenating
features from several scales. The latter results in storing richer matching information and produces
a more accurate disparity map but at the cost of higher computation effort [6, 7, 8, 9]. Modelling
approaches which focus on high accuracy tend to construct several cost volumes which are further
processed separately [10, 11] or are merged to form a big and well informed cost volume [6, 12, 13].
However, while such a large cost volume contains a great amount of useful matching information, it
isalsopronetostoringalargeamountofirrelevantinformation. Therefore,filteringbigcostvolumes
topreserverichermatchinginformationandsuppresslessimportantparametersisadirectiontaken
bysomeresearchers[14,15,16]. However,whilefilteringthecostvolumetoweakenirrelevantvalues
increases the disparity estimation accuracy, it does not reduce the computation because the size of
the cost volume is not changed after filtering. To alleviate this issue, one may consider designing
an algorithm to remove insignificant information and reduce the size of cost volume. For example,
MDCNet [17] and JDCNet [18] narrow the cost volume disparity range by comparing a pixel with
its surroundings using a predefined threshold to remove irrelevant parameters. In another work,
2SCV-Stereo [19] introduces a sparse cost volume that only stores the best K matching costs for
each pixel by using k-nearest neighbors. The downside of such algorithms is that, while they reduce
the size of the cost volume and increase the speed, some useful information will be lost together
with unimportant parameters. One approach to overcome the deficiencies of using small feature
extractors, cost volume filtering and cost volume dimension reduction, is to guide the aggregation
block with contextual information stored in the different scales of the feature extractor [20, 21] or
edge cues [22] and semantic information [23] extracted from the features. The guided aggregation
blockfusesthecontextualinformationwiththegeometryinformationextractedfromthecostvolume
whichimprovestheestimationaccuracy. Althoughsuchafusionleadstobetteraccuracy,thefusion
module which is usually made of convolutional layers or Gated Recurrent Unit (GRU) convolutions
[24] adds extra computation to the pipeline.
Thisresearchproposesanovelfaststereomatchingnetworkarchitecturewithhighlevelofaccuracy
and 67 ms inference time that we call DCVSMNet (see Fig.1). As shown in Fig.2, DCVSMNet is
comprised of two upper and lower small cost volumes to reduce computation burden compared to a
singlelargecostvolume. Theideaistostorerichmatchinginformationextractedfromfeaturesintwo
costvolumesbuiltusingtwodifferentmethods,group-wisecorrelationandcorrelationcostvolumes.
The cost volumes are aggregated separately and the obtained geometry information from the upper
and lower cost volume are fused by a coupling module at different scales to help the network learn
more accurate contextual information to achieve accurate disparity estimation. The summation of
the upper and lower branch outputs is regressed to generate the final disparity map. DCVSMNet
exhibits a competitive accuracy compared to other state-of-the-art methods with an inference time
less than or equal to 80 ms when tested on high end GPUs. Our network can generalize very well
to real-world datasets such as KITTI 2012 [25], KITTI 2015 [26], ETH3D [27] and Middlebury [28]
when only trained on the SceneFlow [29] dataset, and outperforms fast networks such as CGIStereo
[20], CoEx [16] and Fast-ACVNet [14].
Our main contributions are:
• We propose a double cost volume stereo matching pipeline which is capable of processing
two small cost volumes using two light 3D networks to achieve a high level of accuracy by
providingthenetworkwithrichermatchingcostinformationincomparisonwithasinglelarge
cost volume.
• Wedesignacouplingmoduletofusethegeometryinformationextractedfromtwodifferentcost
volumes, enabling the network to learn more complex geometry and contextual information.
• We design a fast and accurate stereo matching network which outperform state-of-the-art
methods (<80 ms) with a strong generalization ability.
2 Related work
DCVSMNet merges the left and right features to build two different cost volumes which can be
categorized as the multi cost volumes approach. Using multi-scale information is widely used in
computer vision tasks. For example, DeepLab [30] uses multi-scale features to improve semantic
segmentation. SpyNet [31] and PWC-Net [32] employ mutli-level information extracted from fea-
tures at different scales to compute optical flow. Using multiple cost volumes constructed from left
and right features in the stereo matching domain has also shown a promising improvement in the
disparityestimation. [3,33,34]proposeathree-stagedisparityrefinementbyconstructingthreecost
volumes. Thedisparityofeachstage(residualdisparity)isusedtowarpthecostvolumeatthenext
3Figure 2: DCVSMNetusesbothgroup-wiseandnormcorrelationcostvolumestostorerichmatching
cost information. Each volume is processed using a 3D hourglass network. The geometry
informationextractedfromtheupperandlowercostvolumeisfusedbyacouplingmodule
and the final disparity map is generated by regressing the summation of the upper and
lower branch outputs
4stage to progressively update the disparity map. In comparison our method, DCVSMNet, uses the
upperandlowercostvolumetopredictdisparityinasinglestagebyremovingthewarpingalgorithm
and taking advantage of a fusion module which results in outperforming these methods with better
accuracy and speed. SSPCV-Net [35] builds a cost volume based on semantic segmentation infor-
mation and three pyramidical cost volumes using features which are processed by a 3D-multi-scale
aggregation network which is highly computational. MASNet [36] also uses semantic segmentation
clues to refine the disparity estimation, however, the aggregation module includes three stacked 3D
hourglassnetworkswhichaddstothecomputationalburdenofthenetwork[37]. AANet[9]produces
multi-scale cost volumes from features. The cost volumes are aggregated by six stacked Adaptive
Aggregation Modules (AAModules). Each module consists of three Intra-Scale Aggregation (ISA)
modules to alleviate the well-known edge-fattening issue at disparity discontinuities, and a Cross-
Scale Aggregation (CSA) module which can extract information in textureless regions. However,
while the computational burden imposed by the ISA and CSA modules in AANet is at the level
of our coupling module, DCVSMNet gains much better accuracy with the same inference time (see
Fig. 1). ADCPNet [38] constructs a full range and a compact cost volume to predict the disparity
using a two-stage coarse-to-fine framework. ADCPNet sacrifices the accuracy over speed by aggres-
sively limiting the disparity range for forming a compact cost volume, which results in a faster but
less accurate network compared to DCVSMNet. The closest work to our approach is GWCNet [6].
GWCNet constructs a group-wise as well as a concatenation cost volume. Further, the two cost
volumes are merged to build a single combined cost volume which is processed by three 3D stacked
hourglassnetworks. MotivatedbyGWCNet, ourapproachispoweredbytwoparallellow-resolution
cost volumes built by the merged left and right features. The upper cost volume is a group-wise
cost volume with a half of the size of volume used in GWCNet and the lower cost volume is a norm
correlation cost volume which is aggressively compressed using a 3D convolution layer. Finally,
instead of merging the two cost volumes, they are separately processed using two light aggregation
networks which are connected by a coupling module for fusing the geometry information.
3 Method
AsshowninFig.2,DCVSMNettakestwostereopairimagesasinputandthecontextualinformation
is extracted using a ResNet-like network. The features are used to build two low-resolution cost
volumesaggregatedbytwoparallel3Dhourglassnetworkswhicharefusedusingacouplingmoduleto
extracted high-resolution geometry features. The summation of the upper and lower branch output
is regressed to estimate the final disparity map. In this section, we first introduce the architecture
of the coupling module (Sec.3.1). The feature extraction architecture and the construction of cost
volumes is described in section 3.2. Then, the architecture of the aggregation network is discussed
in section 3.3. Finally, we explain the disparity regression and the loss function used to train our
architecture in sections 3.4 and 3.5.
3.1 Coupling Module
Toextractaccurateandhigh-resolutiongeometryinformationfromtwolow-resolutioncostvolumes,
we propose a coupling module to fuse the information of the upper and lower branches extracted
from the decoder module of the aggregation networks.
The output features at each scale of the aggregation’s decoder module are used as the inputs of
the coupling module. Considering G
u
∈ RB×Cu×Du×Hu×Wu and G
l
∈ RB×Cl×Dl×Hl×Wl as two
geometry features extracted from the upper and lower branch subscribed by u and l, where B is the
batch size, D is the disparity range, C, H and W are the number of channels, feature height and
width. The coupling module takes G and G as inputs and fuse the information based on Eq.1,
u l
5where, f3×3 and f3×3 are convolution operations with the filter size 1×3×3 (see Fig.2). Eq.1
1 2
results in fused geometry features G with the same dimension as G and G .
fused l u
G =f3×3(f3×3(G )+G )+G (1)
fused 1 2 l u l
3.2 Feature Extraction and Cost Volumes
DCVSMNet adopts the PSMNet [39] feature extraction backbone with the half dilation settings,
ignoring its spatial pyramid pooling module. The output features are 1 resolution of the input
4
images. The last three scales of the feature extraction are concatenated to generate a 320-channel
feature map. Following GWCNet [6], to form a group-wise cost volume, the feature map is split to
N groups along the channel dimension. Considering the number of feature map channels as N ,
g c
gth feature group as fg and fg, the group-wise cost volume can be formed as Eq.2.
l r
N
C (d,x,y,g)= g⟨fg(x,y),fg(x−d,y)⟩ (2)
gwc N l r
c
⟨,⟩denotestheinnerproduct,disthedisparityindexand(x,y)representsthepixelcoordinate. The
dimension of the generated cost volume is [D /4,H/4,W/4,N ], where, D is the maximum
max g max
disparity. In addition, to form the norm correlation cost volume, the concatenated features are
passed through a convolution operation followed by a BatchNorm and leaky ReLU to aggressively
compressthechannelsfrom320to12. Then,thenormcostvolumeisconstructedusingEq.3,which
has the dimension of [D /4,H/4,W/4,1].
max
⟨f (:,x,y),f (:,x−d,y)⟩
C (:,d,x,y)= l r (3)
corr ||f (:,x,y)|| .||f (:,x,y)||
l 2 r 2
3.3 Cost Aggregation
To extract high-resolution geometry information, two UNet-like (3D hourglass [14]) networks are
used for aggregating the matching costs stored in the cost volumes. Each aggregation block consist
of an encoder and a decoder module. The encoder module is made of three down-sampling layers
and each layer includes a 3D convolution layer with kernel size 3×3×3 with stride 2 followed by
another 3D convolution layer with kernel size 3×3×3 and stride 1. The encoder module reduces
computation and using layers with stride 2 leads to increasing receptive field, which is a measure
of association of the output layer to the input region. The decoder module is made of three up-
sampling layers including 4×4×4 3D transposed convolution with stride 2 followed by 3×3×3
3D convolution with stride 1. To fuse geometry information extracted from two cost volumes, the
outputofeachup-samplinglayeroftheupperbranchisusedastheinputforcouplingmodulewhich
is alternatively employed after each up-sampling layer of the lower branch. Finally, the summation
oftheoutputsfromtheupperandlowerbranchisfedtoaregressionblocktocomputetheexpected
disparity map.
3.4 Disparity Regression
The aggregated cost volume is regularized by selecting top-k values at every pixel. To reduce the
computation, the model is designed to compute the disparity map d at 1 resolution of the input
0 4
images with k =2. Then, d is upsampled using weighted average of the ”superpixel” surrounding
0
each pixel to obtain the full resolution disparity map denoted as d [16].
1
63.5 Loss Function
DCVSMNet is trained end-to-end and supervised by the weighted loss function described in Eq.4,
where, d is the estimated disparity map at 1 resolution, d is the expected disparity map at full
0 4 1
resolution and d is the ground truth disparity.
gt
L=λ smooth (d −d )+λ smooth (d −d ) (4)
0 L1 0 gt 1 L1 1 gt
4 Experiment
In this section four datasets are introduced for evaluating DCVSMNet performance and studying
the generalization ability.
4.1 Datasets and Evaluation Metrics
SceneFlow [29] is a synthetic dataset including 35454 training image pairs and 4370 testing image
pairs with the resolution of 960×540. The performance evaluation on SceneFlow is measured by
End-Point Error (EPE) described in Eq.5 in which (x,y) is the pixel coordinate, d is the estimated
disparity, d is the ground truth disparity and N is the effective pixel number in one disparity
gt
image. Another metric that is used for evaluation on SceneFlow is disparity outlier (D1), which
is defined as the pixels with errors greater than max(3px,0.05d ). Because SceneFlow is a large
gt
dataset, it is widely used for pre-training stereo matching networks before fine-tuning on real-world
benchmarks.
(cid:80)
|d(x,y)−d (x,y)|
EPE = (x,y) gt (5)
N
KIITI includes two benchmarks KIITI 2012 [25] and KITTI 2015 [26]. KITTI 2012 contains 194
trainingstereoimagepairsand195testingimagespairs,andKITTI2015contains200trainingstereo
image pairs and 200 testing image pairs. KITTI datasets are a collection of real-world driving scene
and provide sparse ground-truth disparity measuted by LiDAR. For KIITI 2015, D1-all (percentage
of stereo disparity outliers in the reference frame), D1-fg (percentage of outliers averaged only
over foreground regions), and D1-bg (percentage of outliers averaged only over background regions)
metrics are used for evaluation. For KIITI 2012, Out-Noc (percentage of erroneous pixels in non-
occluded areas), Out-All (percentage of erroneous pixels in total), EPE-noc (end-point error in
non-occluded areas), EPE-all (end-point error in total) are used for evaluation.
ETH3D [27] contains 27 training and 20 testing grayscale image pairs with sparse ground-truth
disparity. ThedisparityrangeofETH3Dis0-64andthepercentageofpixelswitherrorslargerthan
1 pixel (bad 1.0) is used for performance evaluation on ETH3D dataset.
Middlebury 2014[28]isacollectionof15trainingandtestingindoorimagepairsatfull,half,and
quarter resolutions. the percentage of pixels with errors larger than 2 pixels (bad 2.0) is reported as
the metric for evaluation on this dataset. bad-σ error can be defined as Eq.6.
(cid:80)
|d(x,y)−d (x,y)|>σ
bad−σ = (x,y) gt ∗100% (6)
N
4.2 Implementation Details
DCVSMNet is implemented using PyTorch trained and evaluated on a single NVIDIA RTX 3090
GPU. The ADAM [40] method with β = 0.9 and β = 0.999 is used for optimization. The loss
1 2
7Table 1: AblationstudyonSceneFlowtestset,theBaselineistheDCVSMNetarchitecturewithout
the coupling module.
Coupling
Method EPE[px] D1[%] >1px[%] >2px[%] >3px[%] Time[ms]
Module
Baseline (cid:37) 0.72 2.60 7.98 4.35 3.18 65
DCVSMNet (cid:33) 0.60 2.11 6.62 3.60 2.62 67
function’sweightsareselectedasλ =0.3andλ =1.0. First, DCVSMNetistrainedonSceneFlow
0 1
dataset for 60 epochs and then fine-tuned for another 60 epochs. The learning rate initially is set
to 0.001 and decayed by a factor of 2 after epoch 20, 32, 40, 48 and 56. Then, the trained model
on SceneFlow is fine-tuned for 600 epochs on the mixed KITTI 2012 and KITTI 2015. For KITTI,
the learning rate initially set to 0.001 and decayed to 0.0001 at 300th epoch. Furthermore, the
generalization results on KITTI, ETH3D and Middlebury are obtained by the model trained only
on SceneFlow.
4.3 Ablation Study
To evaluate the effectiveness of merging geometry information, an ablation experiment is conducted
on SceneFlow dataset. To do so, we compare the performance of the baseline model with the full
model. Here,thebaselineisdefinedasthearchitecturewithoutthecouplingmoduleandtheoutput
of the baseline is directly generated by the summation of the upper and lower branch. As shown in
Table.1, the coupling module improves the performance of the baseline by reducing EPE from 0.72
to 0.60, which validates the efficiency of the proposed coupling module in improving the network
performance.
4.4 Comparisons with State-of-the-art
SceneFlow. Table.2 demonstrates the performance of DCVSMNet on the SceneFlow test set com-
pared to other state-of-the-art approaches. The methods are divided to two categories based on
whether the networks are designed primarily for accuracy or for speed. The results show that
DCVSMNet achieves remarkable accuracy (EPE = 0.60 px) on SceneFlow test set among the high
speed methods and outperforms some complex stereo matching networks such as PSMNet [39],
GwcNet [6], LEAStereo [7] and GANet [21].
KITTI 2012 and 2015. Table.3 demonstrates the official results on the KITTI 2012 and KITTI
2015 datasets. The results show that DCVSMNet outperforms other high speed methods in terms
of accuracy by a large margin, at the cost of greater runtime. However, our model still performs
better than JDCNet [18] which has an 80 ms inference time and some methods categorized as high
accuracy networks such as SegStereo [22] and SSPCVNet [35]. —Further Figs.3 and 4 show the
qualitative results for three scenes of KITTI 2012 and KITTI 2015 test set, which represents the
capability of DCVSMNet in recovering thin and smooth structures.
4.5 Generalization Performance
Table.4 shows our model generalization results on KITTI 2012 [25], KITTI 2015 [26], Middlebury
2014[28] and ETH3D [27] compared to other non-real-time and real-time methods. Among high
speedmethods,ourmodelachievessuperiorgeneralizationperformance. Furthermore,generalization
resultsonETH3DandMiddlbury2014denotethatourmethodnotonlygeneralizesbettercompared
8Target Method EPE[px] Time[ms]
CFNet [8] 0.97 180
LEAStereo [7] 0.78 300
GwcNet [6] 0.76 320
GANet [21] 0.84 360
PSMNet [39] 1.09 410
StereoNet [1] 1.10 15
ADCPNet [38] 1.48 20
BGNet [12] 1.17 25
Coex [16] 0.68 27
CGIStereo [20] 0.64 29
EBStereo [41] 0.63 29
MDCNet[17] 0.77 50
DeepPrunerFast[15] 0.97 62
AANet[9] 0.87 68
JDCNet[18] 0.87 80
DCVSMNet(ours) 0.60 67
Table 2: EvaluationonSceneFlowDataset. Themethodsarecategorizedbasedontheirdesignfocus
for accuracy or speed, we consider high speed methods to have an inference time ≤80ms.
KITTI2012 KITTI2015
Target Method 3-Noc 3-All 4-Noc 4-all EPEnoc EPEall D1-bg D1-fg D1-all Time[ms]
CFNet[8] 1.23 1.58 0.92 1.18 0.4 0.5 1.54 3.56 1.88 180
ACVNet[14] 1.13 1.47 0.86 1.12 0.4 0.5 1.37 3.07 1.65 200
LEAStereo[7] 1.13 1.45 0.83 1.08 0.5 0.5 1.40 2.91 1.65 300
EdgeStereo-V2[23] 1.46 1.83 1.07 1.34 0.4 0.5 1.84 3.30 2.08 320
CREStereo[15] 1.14 1.46 0.90 1.14 0.4 0.5 1.45 2.86 1.69 410
SegStereo[22] 1.68 2.03 1.25 1.52 0.5 0.6 1.88 4.07 2.25 600
SSPCVNet[35] 1.47 1.90 1.08 1.41 0.5 0.6 1.75 3.89 2.11 900
CSPN[42] 1.19 1.53 0.93 1.19 - - 1.51 2.88 1.74 1000
GANet[21] 1.19 1.60 0.91 1.23 0.4 0.5 1.48 3.46 1.81 1800
LaC+GANet[43] 1.05 1.42 0.80 1.09 0.4 0.5 1.44 2.83 1.67 1800
CGI-Stereo[20] 1.41 1.76 1.05 1.30 0.5 0.5 1.66 3.38 1.94 29*
CoEx[16] 1.55 1.93 1.15 1.42 0.5 0.5 1.79 3.82 2.13 33*
BGNet+[12] 1.62 2.03 1.16 1.48 0.5 0.6 1.81 4.09 2.19 35*
Fast-ACVNet+[14] 1.45 1.85 1.06 1.36 0.5 0.5 1.70 3.53 2.01 45*
DecNet[5] - - - - - - 2.07 3.87 2.37 50
MDCNet[17] 1.54 1.97 - - - - 1.76 - 2.08 50
DeepPrunerFast[15] - - - - - - 2.32 3.91 2.59 50*
HITNet[10] 1.41 1.89 1.14 1.53 0.4 0.5 1.74 3.20 1.98 54*
DispNetC[29] 4.11 4.65 2.77 3.20 0.9 1.0 2.21 6.16 4.43 60
AANet[9] 1.91 2.42 1.46 1.87 0.5 0.6 1.99 5.39 2.55 62
JDCNet[18] 1.64 2.11 - - - - 1.91 4.47 2.33 80
DCVSMNet(ours) 1.30 1.67 0.96 1.23 0.5 0.5 1.60 3.33 1.89 67
Table 3: Evaluation on KITTI Datasets. The methods are categorized based on their design focus
for accuracy or speed (we consider high speed to mean an inference time ≤ 80 ms). *
denotes the runtime is tested on our hardware (RTX 3090)
9
ycaruccA
deepS
ycaruccA
deepS(a) Left image (b) DCVSMNet
Figure 3: Qualitative results on KITTI 2012. Note how the model is able to recover fine details.
(a) Left image (b) DCVSMNet
Figure 4: Qualitative results on KITTI 2015. Note how the model is able to recover fine details.
10KITTI 2012 KITTI 2015 Middlebury ETH3D
Target Method
D1(%) D1(%) bad 2.0(%) bad 1.0(%)
PSMNet[39] 6.0 6.3 15.8 9.8
GANet[21] 10.1 11.7 20.3 14.1
DSMNet[44] 6.2 6.5 13.8 6.2
CFNet[8] 5.1 6.0 15.4 5.3
STTR[45] 8.7 6.7 15.5 17.2
FC-PSMNet[46] 5.3 5.8 15.1 9.3
Graft-PSMNet[47] 4.3 4.8 9.7 7.7
DeepPrunerFast [15] 7.6 7.6 38.7 36.8
BGNet[12] 12.5 11.7 24.7 22.6
CoEx[16] 7.6 7.2 14.5 9.0
CGI-Stereo[20] 6.0 5.8 13.5 6.3
DCVSMNet(ours) 5.3 5.7 9.0 4.1
Table 4: Generalization performance on KITTI, Middlebury and ETH3D. All models are trained
only on SceneFlow.
(a) Left image (b) DCVSMNet (c) Ground Truth
Figure 5: GeneralizationresultsofDCVSMNetonMiddlebury2014dataset. Ourmodelgeneralizes
well to real-world scenarios when trained only on the synthetic SceneFlow dataset
tocomplexmethods,butisalsofaster. QualitativeresultsaredemonstratedinFigs.5and6showing
how well our method generalizes on real-world datasets.
11
ycaruccA
deepS(a) Left image (b) DCVSMNet (c) Ground Truth
Figure 6: Generalization results of DCVSMNet on ETH3D dataset.
4.6 Computational Complexity Analysis
In the stereo matching pipeline, it is ideal to use cost volumes with few parameters while getting
an acceptable accuracy for disparity estimation. In our approach, in contrast to GWCNet [6] which
combines two large cost volumes and aggregates it using three hourglass networks, we construct
two separate cost volumes with fewer parameters and aggregate them using two parallel hourglass
networks which reduce the computation effort. Furthermore, we used the GWCNet [6] feature
extraction backbone with 3.32 million parameters and build the cost volumes from last three layers
of the feature extraction which have the same resolution. However, recent works such as ACVNet-
Fast[14]andCGI-Stereo[20]adoptedUNet-likefeatureextractionbackboneswithfewerparameters
(such as MobileNet-v2[48] feature extraction backbone with 2.69 Million parameters) and reported
promising results. Therefore, for the future work, we hope to increase the speed of our model by
designing a light-weight UNet-like feature extraction and constructing the cost volumes at different
resolutions to reduce the overall numbers of the network parameters and bring our network speed
into the real-time zone.
5 Conclusion
The reported results show excellent performance on four datasets; KITTI 2012, KITTI 2015, Mid-
dlebury2014,andETH3Dwithcompetitiveaccuracyandstronggeneralizationonreal-worldscenes.
DCVSMNet is capable of recovering fine structures and outperforms state-of-the-art methods such
as ACVNet-Fast [14] and CGI-Stereo [20] in terms of the accuracy. Further DCVSMNet exhibits
remarkable generalization results compared to other methods categorized based on both their speed
and accuracy. DCVSMNet owes its performance first to the two cost volumes built by two differ-
12ent methods, which allow for storing richer and more variant matching information; and second to
the coupling module that fuses the aggregated information from the upper and lower cost volumes
enabling the network to learn more complex geometry and contextual information to balance the
accuracy and speed. The DCVSMNet inference time of 67 ms is influenced by using two cost vol-
umes and consequently the need for two 3D aggregation networks for processing, which limits the
networkuseforapplicationswithhuman-likeperformancerequirement(40ms)andraiseschallenges
for implementation on edge devices with lower memory than high end GPUs. In future work, we
plan to overcome this limitation by replacing the complex feature extractor with a lighter network
to speed up the network, and prune the cost volumes by removing irrelevant information and pre-
serving important matching parameters to reduce the chance of sacrificing accuracy caused by the
lightweight feature extractor.
13References
[1] Sameh Khamis, Sean Fanello, Christoph Rhemann, Adarsh Kowdle, Julien Valentin, and
Shahram Izadi. Stereonet: Guided hierarchical refinement for real-time edge-aware depth
prediction. In Proceedings of the European Conference on Computer Vision (ECCV), pages
573–590, 2018.
[2] Kyoung Won Nam, Jeongyun Park, In Young Kim, and Kwang Gi Kim. Application of stereo-
imaging technology to medical field. Healthcare informatics research, 18(3):158–163, 2012.
[3] YanWang,ZihangLai,GaoHuang,BrianHWang,LaurensVanDerMaaten,MarkCampbell,
and Kilian Q Weinberger. Anytime stereo image depth estimation on mobile devices. In 2019
international conference on robotics and automation (ICRA), pages 5893–5900. IEEE, 2019.
[4] Cristhian A Aguilera, Cristhian Aguilera, Crist´obal A Navarro, and Angel D Sappa. Fast cnn
stereo depth estimation through embedded gpu devices. Sensors, 20(11):3249, 2020.
[5] Chengtang Yao, Yunde Jia, Huijun Di, Pengxiang Li, and Yuwei Wu. A decomposition model
for stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 6091–6100, 2021.
[6] Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, and Hongsheng Li. Group-wise cor-
relation stereo network. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 3273–3282, 2019.
[7] Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Yuchao Dai, Xiaojun Chang, Hongdong Li,
Tom Drummond, and Zongyuan Ge. Hierarchical neural architecture search for deep stereo
matching. Advances in Neural Information Processing Systems, 33:22158–22169, 2020.
[8] Zhelun Shen, Yuchao Dai, and Zhibo Rao. Cfnet: Cascade and fused cost volume for robust
stereomatching. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition, pages 13906–13915, 2021.
[9] Haofei Xu and Juyong Zhang. Aanet: Adaptive aggregation network for efficient stereo match-
ing. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages 1959–1968, 2020.
[10] Vladimir Tankovich, Christian Hane, Yinda Zhang, Adarsh Kowdle, Sean Fanello, and Sofien
Bouaziz. Hitnet: Hierarchicaliterativetilerefinementnetworkforreal-timestereomatching. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
14362–14372, 2021.
[11] JiankunLi, PeisenWang, PengfeiXiong, TaoCai, ZiweiYan, LeiYang, JiangyuLiu, Haoqiang
Fan, andShuaichengLiu. Practicalstereomatchingviacascadedrecurrentnetworkwithadap-
tive correlation. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pages 16263–16272, 2022.
[12] Bin Xu, Yuhua Xu, Xiaoli Yang, Wei Jia, and Yulan Guo. Bilateral grid learning for stereo
matching networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 12497–12506, 2021.
[13] Wei Chen, Xiaogang Jia, Mingfei Wu, and Zhengfa Liang. Multi-dimensional cooperative net-
work for stereo matching. IEEE Robotics and Automation Letters, 7(1):581–587, 2021.
[14] Gangwei Xu, Yun Wang, Junda Cheng, Jinhui Tang, and Xin Yang. Accurate and efficient
stereo matching via attention concatenation volume. arXiv preprint arXiv:2209.12699, 2022.
14[15] Shivam Duggal, Shenlong Wang, Wei-Chiu Ma, Rui Hu, and Raquel Urtasun. Deep-
pruner: Learning efficient stereo matching via differentiable patchmatch. In Proceedings of
the IEEE/CVF international conference on computer vision, pages 4384–4393, 2019.
[16] Antyanta Bangunharcana, Jae Won Cho, Seokju Lee, In So Kweon, Kyung-Soo Kim, and
Soohyun Kim. Correlate-and-excite: Real-time stereo matching via guided cost volume excita-
tion. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
pages 3542–3548. IEEE, 2021.
[17] Wei Chen, Xiaogang Jia, Mingfei Wu, and Zhengfa Liang. Multi-dimensional cooperative net-
work for stereo matching. IEEE Robotics and Automation Letters, 7(1):581–587, 2021.
[18] XiaogangJia,WeiChen,ZhengfaLiang,XinLuo,MingfeiWu,ChenLi,YulinHe,YusongTan,
and Libo Huang. A joint 2d-3d complementary network for stereo matching. Sensors, 21(4):
1430, 2021.
[19] Hengli Wang, Rui Fan, and Ming Liu. Scv-stereo: Learning stereo matching from a sparse cost
volume. In2021 IEEE International Conference on Image Processing (ICIP),pages3203–3207.
IEEE, 2021.
[20] Gangwei Xu, Huan Zhou, and Xin Yang. Cgi-stereo: Accurate and real-time stereo matching
via context and geometry interaction. arXiv preprint arXiv:2301.02789, 2023.
[21] FeihuZhang,VictorPrisacariu,RuigangYang,andPhilipHSTorr.Ga-net: Guidedaggregation
net for end-to-end stereo matching. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 185–194, 2019.
[22] Guorun Yang, Hengshuang Zhao, Jianping Shi, Zhidong Deng, and Jiaya Jia. Segstereo: Ex-
ploitingsemanticinformationfordisparityestimation.InProceedingsoftheEuropeanconference
on computer vision (ECCV), pages 636–651, 2018.
[23] Xiao Song, Xu Zhao, Liangji Fang, Hanwen Hu, and Yizhou Yu. Edgestereo: An effective
multi-task learning network for stereo matching and edge detection. International Journal of
Computer Vision, 128:910–930, 2020.
[24] KyunghyunCho,BartVanMerri¨enboer,CaglarGulcehre,DzmitryBahdanau,FethiBougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-
decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
[25] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving?
the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern
recognition, pages 3354–3361. IEEE, 2012.
[26] Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 3061–3070, 2015.
[27] Thomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler,
Marc Pollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution
images and multi-camera videos. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 3260–3269, 2017.
[28] Daniel Scharstein, Heiko Hirschmu¨ller, York Kitajima, Greg Krathwohl, Nera Neˇsi´c, Xi Wang,
and Porter Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In
Pattern Recognition: 36th German Conference, GCPR 2014, Mu¨nster, Germany, September
2-5, 2014, Proceedings 36, pages 31–42. Springer, 2014.
15[29] NikolausMayer,EddyIlg,PhilipHausser,PhilippFischer,DanielCremers,AlexeyDosovitskiy,
and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow,
and scene flow estimation. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 4040–4048, 2016.
[30] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and
fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):
834–848, 2017.
[31] Anurag Ranjan and Michael J Black. Optical flow estimation using a spatial pyramid network.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4161–
4170, 2017.
[32] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow
using pyramid, warping, and cost volume. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 8934–8943, 2018.
[33] Jia-Ren Chang, Pei-Chun Chang, and Yong-Sheng Chen. Attention-aware feature aggregation
for real-time stereo matching on edge devices. In Proceedings of the Asian Conference on
Computer Vision, 2020.
[34] Pier Luigi Dovesi, Matteo Poggi, Lorenzo Andraghetti, Miquel Mart´ı, Hedvig Kjellstr¨om,
Alessandro Pieropan, and Stefano Mattoccia. Real-time semantic stereo matching. In 2020
IEEE international conference on robotics and automation (ICRA), pages 10780–10787. IEEE,
2020.
[35] Zhenyao Wu, Xinyi Wu, Xiaoping Zhang, Song Wang, and Lili Ju. Semantic stereo matching
with pyramid cost volumes. In Proceedings of the IEEE/CVF international conference on
computer vision, pages 7484–7493, 2019.
[36] Jie Wang, Sunjie Zhang, Yongxiong Wang, and Zhengyu Zhu. Learning efficient multi-
task stereo matching network with richer feature information. Neurocomputing, 421:151–160,
2021. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2020.08.010. URL https:
//www.sciencedirect.com/science/article/pii/S0925231220312704.
[37] Aixin Chong, Hui Yin, Yanting Liu, Jin Wan, Zhihao Liu, and Ming Han. Multi-hierarchy
feature extraction and multi-step cost aggregation for stereo matching. Neurocomputing, 492:
601–611, 2022. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2021.12.052. URL
https://www.sciencedirect.com/science/article/pii/S0925231221018890.
[38] He Dai, Xuchong Zhang, Yongli Zhao, and Hongbin Sun. Adcpnet: Adaptive dispar-
ity candidates prediction network for efficient real-time stereo matching. arXiv preprint
arXiv:2011.09023, 2020.
[39] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching network. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages 5410–5418, 2018.
[40] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980, 2014.
[41] Weijie Bi, Ming Chen, Dongliu Wu, and Shenglian Lu. Ebstereo: edge-based loss function for
real-time stereo matching. The Visual Computer, pages 1–12, 2023.
16[42] Xinjing Cheng, Peng Wang, and Ruigang Yang. Learning depth with convolutional spatial
propagation network. IEEE transactions on pattern analysis and machine intelligence, 42(10):
2361–2379, 2019.
[43] Biyang Liu, Huimin Yu, and Yangqi Long. Local similarity pattern and cost self-reassembling
for deep stereo matching networks. In Proceedings of the AAAI Conference on Artificial Intel-
ligence, volume 36, pages 1647–1655, 2022.
[44] Feihu Zhang, Xiaojuan Qi, Ruigang Yang, Victor Prisacariu, Benjamin Wah, and Philip Torr.
Domain-invariant stereo matching networks. In Computer Vision–ECCV 2020: 16th Euro-
pean Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16, pages 420–439.
Springer, 2020.
[45] Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding, Francis X Creighton, Russell H
Taylor,andMathiasUnberath. Revisitingstereodepthestimationfromasequence-to-sequence
perspective with transformers. In Proceedings of the IEEE/CVF international conference on
computer vision, pages 6197–6206, 2021.
[46] Jiawei Zhang, Xiang Wang, Xiao Bai, Chen Wang, Lei Huang, Yimin Chen, Lin Gu, Jun
Zhou, Tatsuya Harada, and Edwin R Hancock. Revisiting domain generalized stereo matching
networks from a feature consistency perspective. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 13001–13011, 2022.
[47] BiyangLiu,HuiminYu,andGuodongQi. Graftnet: Towardsdomaingeneralizedstereomatch-
ing with a broad-spectrum and task-oriented feature. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages 13012–13021, 2022.
[48] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 4510–4520, 2018.
17