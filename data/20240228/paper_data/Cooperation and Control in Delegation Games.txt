Cooperation and Control in Delegation Games
Oliver Sourbut∗, Lewis Hammond∗, and Harriet Wood
University of Oxford
Abstract might not act according to the (human) principal’s objec-
tive, such as when an AV takes an undesirable route to a
Many settings of interest involving humans and ma-
destination. The second is that agents may fail to reach
chines – from virtual personal assistants to au-
tonomous vehicles – can naturally be modelled as a cooperative solution, even if they are acting in line with
principals(humans)delegatingtoagents(machines), theirprincipals’objectives,suchaswhenmultipleAVstake
which then interact with each other on their princi- the same route to their destinations and end up causing
pals’behalf. Werefertothesemulti-principal,multi- congestion. The first represents a control problem [37],
agent scenarios as delegation games. In such games, and the second represents a cooperation problem [13, 15].
there are two important failure modes: problems of
control(whereanagentfailstoactinlinetheirprinci-
pal’spreferences)andproblemsofcooperation(where
theagentsfailtoworkwelltogether). Inthispaperwe A B A B
formalise and analyse these problems, further break-
ing them down into issues of alignment (do the play- A 3, 3 6, 2 A 2, 3 3, 3
ers have similar preferences?) and capabilities (how
competent are the players at satisfying those pref- B 2, 6 0, 0 B 4, 6 3, 0
erences?). We show – theoretically and empirically –
(a) (b) (c)
howthesemeasuresdeterminetheprincipals’welfare,
howtheycanbeestimatedusinglimitedobservations,
Figure 1: (a) The payoffs of the agents in Example 1; (b)
and thus how they might be used to help us design
the payoffs of the principals; and (c) a graphical repre-
more aligned and cooperative AI systems.
sentation of the full delegation game, with vertical arrows
indicating control and horizontal arrows indicating coop-
1 Introduction eration.
With the continuing development of powerful and increas-
ing general AI systems, we are likely to see many more Controlandcooperationcaninturnbebrokendowninto
tasks delegated to autonomous machines, from writing problemsofalignment andofcapabilities [7,9,22]. Forex-
emails to driving us from place to place. Moreover, these ample,inthecontrolfailureabove,thefirstAVmightdrive
machines are increasingly likely to come into contact with undesirably by taking route A even though their passen-
eachotherwhenactingonbehalfoftheirhumanprincipals, ger prefers the scenic beachfront (an alignment problem),
whether they are virtual personal assistants attempting to or the second AV might undesirably take route B because
schedule a meeting or autonomous vehicles (AVs) using it is incapable of calculating the best route accurately (a
the same road network. We refer to these multi-principal, capabilities problem). Similarly, in the cooperation fail-
multi-agent scenarios as delegation games, an example of ure, the AVs might cause congestion because they cannot
which is as follows, and is shown in Figure 1. plan and communicate effectively enough (a capabilities
problem), or because their objectives are opposed to one
Example 1. Two AVs can choose between two differ-
another – e.g. they cannot both drive alone on route A –
ent routes on behalf of their passengers: A(utobahn) or
even if they have the ability to cooperate in general (an
B(eachfront). Theirobjectivefunctionsaredeterminedby
alignment problem).
the speed and comfort of the journey (which may not be
the same objective as their passenger). Each AV receives As one might expect, ensuring good outcomes for the
utility 6 or 2 for routes A or B, respectively, with an ad- principals requires overcoming all of these problems. This
ditional penalty of −3 or −2 if both AVs choose the same goal is made even more challenging because most research
route (due to the delays caused by congestion). The pas- considers each in isolation – such as cooperation between
sengers’ preferences are more idiosyncratic and as shown. agents with the same objective [16, 35, 44], or alignment
between a single principal and agent [25, 37, 42] – despite
Indelegationgamestherearetwoprimarywaysinwhich
this being an increasingly unrealistic assumption when it
things can go wrong. The first is that a (machine) agent
comes to the deployment of AI. In order to ensure positive
∗Equal contribution. Correspondence to oly@robots.ox.ac.uk outcomes,wecannotrelyonsolutionstoonlysome ofthese
andlewis.hammond@cs.ox.ac.uk. problems.
1
4202
beF
42
]TG.sc[
1v12851.2042:viXra1.1 Contributions work on inferring properties such as the price of anarchy
[12]. Other works attempt to quantify the capabilities of
In this work we provide the first systematic investigation
(reinforcement learning) agents by assessing their gener-
of these four failure modes and their interplay. More con-
alisation to different environments [10] or co-players [30].
cretely, we make the following three core contributions:
Whilesuchideasdonotformthecoreofourpresentwork,
measures for assessing each failure mode that satisfy a
we make use of them in our final set of experiments.
number of key desiderata (in Section 4); theoretical results
describing the relationships between these measures and
principal welfare (in Section 5); and experiments that val- 2 Background
idate these results and explore how the measures can be
inferred from limited observations (in Section 6). In doing In general, we use uppercase letters to denote sets, and
so, we formalise and substantiate the intuition that solv- lowercase letters to denote elements of sets or functions.
ingallfouroftheseproblemsis,ingeneral,bothnecessary We use boldface to denote tuples or sets thereof, typically
and sufficient for good outcomes in multi-agent settings, associating one element to each of an ordered collection of
whichinturnhasimportantimplicationsfortheprojectof players. Unless otherwise indicated, we use superscripts
building safe and beneficial AI systems. to indicate an agent 1 ≤ i ≤ n and subscripts j to index
the elements of a set; for example, player i’s jth (pure)
strategy is denoted by si. We also use theˆsymbol to
1.2 Related Work j
representprincipals; forexample, theith principal’sutility
function is written uˆi. A notation table can be found for
Giventhefoundationalnatureoftheproblemswestudyin
reference in Appendix A.
thiswork,thereisavastamountofrelevantpriorresearch;
due to space constraints we mention only a few exemplars
Definition 1. A(strategic-form)gamebetweennplayers
oneachtopic. Theprincipal-agent literature typicallycon- isatupleG=(S,u)whereS =× Si istheproductspace
siderssettingswithasingleprincipalandagent[28]. While i
of (pure) strategy sets Si and u contains a utility function
there exist multi-agent variants such as competing mecha-
ui : S → R, for each agent 1 ≤ i ≤ n. We write si ∈ Si
nism games [33, 54], to the best of our knowledge no pre-
ands∈S todenotepure strategiesandpure strategy
vious work investigates the general requirements for high
profiles, respectively. A mixed strategy for player i is
principal welfare. Our setting is also similar to that of
a distribution σi ∈ Σi over Si, and a mixed strategy
strategic delegation [38, 46], though we do not focus on profile is a tuple σ =(σ1,...,σn)∈Σ:=× Σi. We will
principals’ responses to each other’s choice of agents. i
sometimes refer to pure strategy profiles in strategic-form
The degree of alignment between two or more agents games as outcomes. We write s−i ∈S−i :=× Sj and
can be viewed as a measure of similarity between prefer- j̸=i
therefore s=(s−i,si), with analogous notation for mixed
ences. Such measures have been introduced in areas such
strategies. We also abuse notation by sometimes writing
asmathematicaleconomics[5],computationalsocialchoice ui(σ):=E (cid:2) ui(s)(cid:3) and u(σ):=(cid:0) u1(σ),...,un(σ)(cid:1) ∈Rn.
[1],andreinforcementlearning[20,40],thoughtheseworks σ
focus on either cooperation or alignment. In game theory, Formally, a solution concept can be represented as a
thereareseveralclassicalvaluesthatmeasurethedegree of function that maps from games G to subsets of the mixed
(and costs from) competition in a game, such as the price strategy profiles Σ in G. These concepts pick out cer-
of anarchy [26] or coco value [24]. Other important works tain strategy profiles based on assumptions about the
consider the robustness of these values under approximate (bounded)rationalityoftheindividualplayers. Thecanon-
equilibria[4,36]. Wetakeinspirationfromtheseideas,ex- ical solution concept is the Nash equilibrium.
tending them to settings in which the game we study is a
proxy for the game whose value we truly care about. Definition 2. Given some σ−i in a game G, σi is a best
There have been several proposals for how to formally response (BR) for player i if ui(σ) ≥ max σ˜iui(σ−i,σ˜i).
measure the capabilities of an agent. These include formal We write the set of best responses for player i as
definitions of, e.g. general intelligence [29], social intelli- BR(σ−i;G). A Nash equilibrium (NE) in a game G is
gence [23], and collective intelligence [52]. In this work, a mixed strategy profile σ such that σi ∈ BR(σ−i;G) for
we focus on how capabilities at the individual and collec- every player i. We denote the set of NEs in G by NE(G).
tive level can be distinguished and how they combine with
A social welfare function w : Rn → R in an n-player
alignment to impact welfare. Similarly, definitions of co-
game maps from payoff profiles u(s) to a single real num-
operation also abound (see [45] for an overview) though
ber, aggregating players’ payoffs into a measure of collec-
these are often informal and/or inconsistent [51], whereas
tive utility. We again abuse notation by writing w(s) :=
we require a mathematical formalisation appropriate for w(cid:0) u(s)(cid:1) and w(σ) := E (cid:2) w(s)(cid:3) . In the remainder of this
applications to AI systems. σ
paper, we assume use of the following social welfare func-
Finally, when it comes to estimating such properties
tion, though the concepts we introduce do not heavily de-
fromdata,thereisalargeliteratureontheproblemofpref-
pend on this choice.
erence elicitation/learning [17, 18], including in general-
sum games [11, 19, 55]. The latter setting is also studied Definition 3. Given a strategy profile s, the average
in empirical game theory [27, 48–50], including in recent utilitarian social welfareisgivenbyw(s)= 1 (cid:80) ui(s).
n i
23 Delegation Games
(D1) Alignment and capabilities – both individual and col-
lective–areall‘orthogonal’tooneanotherinthesense
In this work, we make the simplifying assumption that that they can be instantiated in arbitrary combina-
there is a one-to-one correspondence between principals tions.
andagents, andthateachprincipaldelegatesfullytotheir
corresponding agent (i.e. only agents can take actions). (D2) Two players are perfectly individually aligned (mis-
Our basic setting of interest can thus be characterised as aligned) if and only if they have identical (opposite)
follows. preferences. Two or more players are perfectly collec-
tively aligned if and only if they have identical prefer-
Definition 4. A (strategic-form) delegation game with
ences.
n principals and n agents is a tuple D = (S,u,uˆ), where
G := (S,u) is the game played by the agents, and Gˆ := (D3) If a set of agents are maximally capable, they achieve
(S,uˆ) is the game representing the principals’ payoffs as a maximal agent welfare. If they are also maximally
function of the agents’ pure strategies. individually aligned, then maximal principal welfare
We refer to G as the agent game and Gˆ as the principal is also achieved.2
game. For instance, G and Gˆ from Example 1 are shown
(D4) If a set of players are perfectly collectively aligned,
in Figures 1a and 1b respectively. When not referring to
then their maximal welfare is their ideal welfare.
principals or agents specifically, we refer to the players of
a delegation game. We denote the welfare in a delegation (D5) Individual measures are independent of any transfor-
game for the principals and agents as wˆ(s) and w(s), re- mations to the game that preserve individual pref-
spectively. erences, and collective measures are independent of
any transformations that preserve collective prefer-
Definition 5. Given a game G and a social welfare func-
ences(ascapturedbysomemeasureofsocialwelfare).
tion w, we define the maximal (expected) welfare
achievableunderwasw (G):=max w(σ). Wedefinethe
⋆ σ
4.1 Control
ideal welfareunderw asw (G):=w(u )whereu [i]=
+ + +
max ui(s). Similarly, we denote w (G) := min w(σ)
s • σ We begin by considering the control of a single agent by
and w (G) := w(u ) where u [i] = min ui(s). We ex-
− − − s a single principal. In essence, we wish to capture the de-
tend these definitions to delegation games D by defining
gree to which an agent is acting in line with its principal’s
w (D):=w (G) and wˆ (D):=w (Gˆ), for †∈{⋆,+,•,−}.
† † † † preferences. As we – and others [2, 7, 9, 22] – have noted,
When unambiguous, we omit the reference to D.
this can be decomposed into a question of: a) how similar
Note that the maximal and ideal welfare may not be the agent’s preferences are to the principal’s; and b) how
equivalent. Forinstance,inExample1wehavew =7but capable the agent is of pursuing its preferences.
⋆
w = 10. The former is the maximum achievable among
+
the available outcomes, while the latter is what would be 4.1.1 Alignment
achievable if all principals were somehow able to receive
How can we tell if principal i’s and agent i’s preferences
their maximal payoff simultaneously. We return to this
aresimilar? First,wemustbemorepreciseaboutwhatwe
distinction, represented graphically in Figure 2, later. In
meanbypreferences. Followingourassumptionthatagents
general,ourdependentvariablesofinterestwillbetheprin-
may play stochastically, we view preferences as orderings
cipals’welfareregretwˆ −wˆ(σ)andthedifferencewˆ −wˆ .
⋆ + ⋆ over distributions of outcomes σ ⪯ σ′ ⇔ u(σ) ≤ u(σ′).3
To compare the preferences of principal i
(⪯ˆi
) and agent i
w w w w
− • w(σ) ⋆ + (⪯i)wecanthereforecompareuˆi andui. Itiswell-known,
however, that the same preferences can be represented by
Figure 2: The range of social welfares in a game G. differentutilityfunctions. Inparticular,uandu′ represent
the same preferences if and only if one is a positive affine
transformation of the other [31].
4 Control and Cooperation Inordertomeaningfullymeasurethedifferencebetween
two utility functions, therefore, we must map each to a
canonicalelementoftheequivalenceclassesinducedbythe
In Section 1, we distinguished between alignment and ca-
preferences they represent. We define such a map using a
pabilities as contributors to the level of both control and
normalisation function ν : U → U, where U := R|S| rep-
cooperation. Our goal is to investigate how variations in
resents the space of utility functions in a game G. There
the alignment and capabilities of agents impact the wel-
aremanypossiblechoicesofnormalisationfunction,butin
fare of the principals. We therefore require ways to mea-
sure these concepts. Before doing so, we put forth a set of
2Weshallseelaterthatafurther‘fairness’criterionontheweight-
natural desiderata that we argue any any such measures ing of agents’ utilities is also required for the second part of this
should satisfy.1 desideratumtohold.
3Note that in strategic-form games, (mixed) strategy profiles are
1Formalisationsofthesedesiderataaregivenaspartofresultsin equivalent to distributions over the domain of players’ utility func-
Section5.1. tions,butingeneralthisneednotbethecase.
3essence they must consist of a constant shift c and a mul- 4.1.2 Capabilities
tiplicative factor m [43], which together define the affine
Oneobviouswayofcreatingaformalmeasureofanagent’s
relationship. Tosatisfyourdesiderata,weplaceadditional
capabilities is to consider the number of (distinct) strate-
requirements on m and c, as follows.
gies they have available to them. In the cases of bound-
edlyrationalagentsormulti-agentsettings,however,itcan
Definition 6. ForeachplayeriinG,wedefinetheirnor-
beneficialtorestrictone’sactionspace,eitherforcomputa-
malised utility function ν(ui)=ui as:
ν tional reasons [50], or by pre-committing to avoid tempta-
(cid:40)
0 if
m(cid:0) ui−c(ui)1(cid:1)
=0
tion[21],ortoforceone’sopponenttobackdown[34]. Al-
ui := ternatively, one could invoke a complexity-theoretic mea-
ν ui−c(ui)1 otherwise, sure of capabilities by considering the time and memory
m(ui−c(ui)1)
available to each agent, though in this work we aim to be
where c:U →R isan affine-equivariant shiftfunctionand agnostic to such constraints.
m:U →R is any strictly convex norm. Instead, inspired by the seminal work of [29], we view
an individual agent’s capabilities as the degree to which
For notational convenience, we sometimes write ci := it is able to achieve its objectives in a range of situations,
c(ui)1 and mi := m(ui −ci), with equivalent notation cˆi regardless of what those objectives are. In game-theoretic
and mˆi when applied to uˆi, resulting in uˆi ν. Then ui = parlance, we consider the rationality of the agent. We can
miui
ν
+ci.
naturally formalise this idea by defining an agent’s capa-
bilities as the degree of optimality of their responses to a
Lemma 1. For any u,u′ ∈ U, u ν = u′ ν if and only if given partial strategy profile σ−i.
⪯=⪯′.
Definition8. Givensomeσ−iinagameG,amixedstrat-
Givenuˆi andui,anaturalwaytomeasurethedegreeof egy σi is an ϵi-best response (ϵi-BR) for player i if:
ν ν
alignment between the ith principal and agent is to com-
pute some kind distance between uˆi and ui. To do so, ui(σ)≥minui(σ−i,σ˜i)
we use a norm of the difference
betwν
een uˆi
aν
nd ui, which
σ˜i
ν ν +(1−ϵi)(cid:0) maxui(σ−i,σ˜i)−minui(σ−i,σ˜i)(cid:1) .
givesrisetothefollowingpseudometricoverU. Inorderto
σ˜i σ˜i
contrast this principal-agent alignment measure with our
lateralignmentmeasureovernplayers,wesometimesrefer We write the set of such best responses for player i
to this as individual (as opposed to collective) alignment. as ϵi-BR(σ−i;G). An ϵ-Nash equilibrium (ϵ-NE)
in a game G is a mixed strategy profile σ such that
Definition 7. Given a delegation game D, the (individ- σi ∈ ϵi-BR(σ−i;G) for every every player i, where ϵ =
ual) alignment between agent i and their principal is (ϵ1,...,ϵn). We denote the set of ϵ-NEs in G by ϵ-NE(G).
given by IAi(D)=1− 1m(uˆi −ui), where m is the same
2 ν ν In essence, ϵi captures the fraction of their attainable
(strictly convex) norm used for normalisation.
utility that player i manages to achieve. Note that if
ϵi = 0 then ϵi-BR(σ−i;G) = BR(σ−i;G) and if ϵi = 1
The choice of norm and shift functions m and c deter-
then ϵi-BR(σ−i;G) = Σi. Similarly, when ϵ = 0 then
mine which differences between payoffs are emphasised by
ϵ-NE(G)=NE(G), and when ϵ=1 then ϵ-NE(G)=Σ.
the measure. One way to make this choice is by writing
m=∥·∥ ,wheredisadistributionoverS. Buthowshould
d Definition9. GivenadelegationgameD,theindividual
one choose d and ∥·∥? capabilities of agent i are ICi(D):=1−ϵi ∈[0,1] where
Beginning with d, one’s first intuition might be to con- ϵi is the smallest value such that agent i plays an ϵi-best
sider a distribution over the equilibria of the game. As- response in G.
suming agents act self-interestedly, there are certain out-
comes that are game-theoretically untenable; divergences Unlike other formulations of bounded rationality, such
between preferences over these outcomes could reasonably as a softmax strategy or randomisation with some fixed
be ignored. This intuition, however, conflicts with one of probability, Definition9–whichisanalogoustosatisficing
our primary desiderata (D1), which is to tease apart the [39,41]–isagnosticastotheprecisemechanism viawhich
differencebetweenalignmentandcapabilities–inthenext playersareirrational,andthusservesasageneral-purpose
subsection,weshowthatagents’individualcapabilitiesde- descriptor of a player’s (ir)rationality level.4
termine the equilibria of the game. Instead, we argue that
theoutcomeofagamedoesnot changetheextenttowhich 4.2 Cooperation
preferences(dis)agree,andsoingeneralassumethatdhas
full support. In order to achieve good outcomes for the principals, it is
not sufficient for the agents to coordinate with their prin-
Our primary requirements on ∥·∥ are that m is strictly
cipalsindividually, theymustalsocoordinatewithonean-
convex and is the same in Definitions 6 and 7. These re-
other. Indeed,itiseasytoshowthattheprincipals’welfare
strictionsarerequiredinordertosatisfyallofourdesider-
ata, but relaxations are possible if fewer requirements are 4Our choice of ϵi-best responses could also be weakened to, e.g.
needed (see Appendix E.2 for more discussion). ϵi-rationalisability[6,32].
4regret can be arbitrarily high in the only NE of a game, 4.2.2 Capabilities
despite perfect control of each agent by its principal.
One way to model collective capabilities is as ‘internal’ to
Lemma 2. There exists a (two-player, two-action) dele- thegameG. Underthisconceptualisation,weassumethat
gation game D such that for any x > 0, however small, theabilityoftheagentstocooperateiscapturedentirelyby
even if IA(D) = 1 and IC(D) = 1, we have only one NE their actions and payoffs in G. For example, if the agents
σ, and wˆ⋆−wˆ(σ) =1−x. were able to coordinate in G using a commonly observed
wˆ+−wˆ−
signal γ, then this would be modelled as them playing a
To achieve low principal welfare regret, we need to have
different game G′, in which each agent’s action consists of
a sufficiently high degree of cooperation, both in terms of:
a choice of action in G given their observation of γ.5
a) collective alignment (the extent to which agents have
This approach, however, conflates the agents’ collective
similar preferences); and b) collective capabilities (the ex-
alignment with their collective capabilities. In order to
tent to which agents can work together to overcome their
tease these concepts apart, we require a way to measure
differences in preferences).
the extent to which the agents can avoid welfare loss due
to their selfish incentives. Perhaps the best known for-
4.2.1 Alignment
malisation of this loss is the price of anarchy [26], which
Intuitively, it should be easier to achieve high welfare in a capturesthedifferenceinwelfarebetweenthebestpossible
game where the players have similar preferences over out- outcomeandtheworstpossibleNE.6 Inspiredbythisidea,
comes than one in which the players have very different we measure the collective capabilities of a group of agents
preferences. At the extremes of this spectrum we have as follows.
zero-sum games and common-interest games, respectively.
Definition 11. Let w := min w(σ). Given a
This intuition can be formalised by generalising Definition ϵ σ∈ϵ-NE(G)
delegation game D where the agents have individual capa-
7 to measure the degree of alignment between n utility
bilities ϵ, the collective capabilities of the agents are
functions, rather than two, as follows.
CC(D) := δ ∈ [0,1] if and only if the agents achieve
Definition 10. Given a delegation game D, the collec- welfare at least w +δ ·(w −w ), where recall that 0-
ϵ ⋆ 0
tive alignment between the agents is given by: NE(G)=NE(G).7
(cid:88) mi
CA(D)=1− ·m(µw−ui), Note that if ϵi ≥ ϵ˜i for every 1 ≤ i ≤ n, then we must
(cid:80) mj ν
i j have w ϵ ≤ w ϵ˜; a special case of this is that w 0 ≥ w ϵ.
In other words, the individual irrationality of the agents
where µw := (cid:80) (cid:80)iui m− ici is a proxy for the agents’ (nor-
can only lower the (worst-case) welfare loss. On the other
i
malised) welfare, recalling that mi :=m(ui−ci). hand, we can see that greater collective capabilities can
potentially compensate for this loss.
Intuitively, we consider the misalignment of each agent
As in the case of individual capabilities, we provide
from a hypothetical agent whose objective is precisely
a measure that is agnostic to the precise mechanism
to promote overall social welfare. This misalignment is
via which the agents cooperate, be it through commit-
weighted by mi, the idea being that the ‘stronger’ agent
ments, communications, norms, institutions, or more ex-
i’s preferences (and hence the larger mi is), the more their
oticschemes. Rather,wetakeasinputthefactthatagents
misalignmentwiththeoverallwelfareofthecollectivemat-
areabletoobtainacertainamountofwelfare,andusethis
ters. While it may not be immediately obvious why we
quantify how well they are cooperating. At one extreme,
use µw instead of ν(w), the former will allow us to derive
they do no better than w , at the other they get as close
tighterboundsandcaneasilybeshowntoinducethesame ϵ
to the maximal welfare w as their individual capabilities
ordering over mixed strategy profiles as w (and hence also ⋆
will allow.
ν(w)).
Lemma 3. For any σ,σ′ ∈ Σ, µw(σ) ≤ µw(σ′) if and
only if w(σ)≤w(σ′). 5 Theoretical Results
Unfortunately, collective alignment (even when paired
We begin our theoretical results by proving that the mea-
with perfect control) is insufficient for high principal wel-
sures defined in the preceding section satisfy our desider-
fare. The most trivial examples of this are equilibrium
ata, before using them to bound the principals’ welfare
selection problems, but we can easily construct a game
regret.
with a unique NE and arbitrarily high welfare regret, even
when we have perfect control and arbitrarily high collec-
5Thus, in a true prisoner’s dilemma, the ‘only thing to do’ (and
tive alignment. These issues motivate our fourth and final thereforetriviallythecooperativeaction)istodefect. Theideahere
measure. is that if the agents possessed better cooperative capabilities, they
wouldnotbefacedwithanactualprisoner’sdilemmatobeginwith.
Lemma 4. There exists a family of (two-player, k- 6Consideringtheworstcaseenablesustocapturethewelfareloss
action) delegation games D such that even if IAi(D) = 1, fromequilibriumselectionproblemsevenincommon-interestgames.
ICi(D) = 1 for each agent, and there is only one NE σ, 7As remarked in Footnote 4, the use of ϵ-NEs is not intrinsic to
our definition of (collective) capabilities and could be weakened to,
we have lim k→∞CA(D)=1 but lim k→∞ wˆ wˆ⋆ +− −wˆ wˆ(σ −) =1. e.g. ϵ-rationalisableoutcomes.
55.1 Desiderata Proposition 5 (D5). Given a delegation game D , let D
1 2
besuchthat⪯i=⪯i and⪯ˆi =⪯ˆi foreach1≤i≤n. Then
The fact that we define alignment as a feature of the un- 1 2 1 2
IA(D )=IA(D ) and IC(D )=IC(D ). Moreover, if D
derlying game, and capabilities are a feature of how the 1 2 1 2 2
is such that ⪯w=⪯w and the ui are affine-independent,
game is played means that capabilities and alignment are 1 2
then CA(D )=CA(D ) and CC(D )=CC(D ) as well.
naturally orthogonal. The potentially arbitrary difference 1 2 1 2
between the principals’ and agents’ utility functions is the
key to the other parts of the following result. 5.2 Bounding Welfare Regret
Proposition 1 (D1). Consider a delegation game D with The primary question we investigate in this work is how
measures IA(D), IC(D), CA(D), and CC(D). Holding the principals fare given the different levels of control and
fixedanythreeofthemeasures,thenforanyvaluev ∈[0,1] cooperation in the game played by the agents. We begin
(or v ∈[0,1]n for IA or IC), there is a game D′ such that by characterising the principals’ welfare in terms of the
the fourth measure takes value v (v) in D′ and the other agents’ utilities, and the alignment of each agent with its
measures retain their previous values. principal.
D2followschieflyfromclassicresultslinkingpreferences Proposition 6. Given a delegation game D, we have:
overmixedstrategyprofilestosetsofutilityfunctionsthat
are positive affine transformations of one another. wˆ −wˆ(σ)≤ 1 (cid:88) ri(cid:0) ui(sˆ )−ui(σ)(cid:1) + 4K mˆ⊤(1−IA),
⋆ n ⋆ n
Proposition 2 (D2). For any 1≤i≤n, IAi =1 (IAi = i
0) if and only if ⪯i= ⪯ˆi (⪯i= ⪰ˆi ). Similarly CA = 1 if where ri := mˆi, mˆ[i] = mˆi, K satisfies ∥u − u′∥ ≤
and only if ⪯i=⪯j for every i,j ∈N. mi ν ν ∞
K·m(u −u′) for any u,u′ ∈U, and wˆ(sˆ )=wˆ .
ν ν ⋆ ⋆
The first half of D3 is straightforward. The subtlety in
Using this result, we can bound the principals’ welfare
thesecondhalfisthat–perhapscounterintuitively,atfirst
regret in terms of both principal-agent alignment and the
– perfect capabilities (both individual and collective) and
agents’ welfare regret, which is in turn a function of the
perfectalignmentbetweentheprincipalsandtheiragentsis
agents’ capabilities. As we saw in Propositions 3 and 5,
not sufficientfortheprincipalstoachievemaximalwelfare
the ‘calibration’ between the principals and agents – as
(unlike the agents). Rather, the resulting solution will be
captured by the potentially differing ratios ri – remains
one (merely) on the Pareto frontier for the principals.
critical for ensuring we reach better outcomes in terms of
Inessence, thisisbecauseindividualalignmentdoesnot
principal welfare.
preserve welfare orderings ⪯w, only individual preference
orderings ⪯i. Recalling that mˆi and mi quantify the mag-
Theorem 1. Given a delegation game D, we have that:
nitudes of uˆi and ui respectively, we can see that, in gen-
eral, the aggregation over agents’ utilities (used to mea- 4K
wˆ −wˆ(σ)≤ mˆ⊤(1−IA)+r∗((w −w )
sure their success at cooperating) may not give the same ⋆ n 0 ϵ
weight to each party as the aggregation over principals’ + (1−CC)(w −w ))+R(σ),
⋆ 0
utilities (used to measure the value we care about). Al-
ternatively, the variation in magnitudes mi can be viewed where IC = 1 − ϵ, r∗ ∈ [min ri,max ri], R(σ) :=
i i
as capturing a notion of fairness (used to select a point on 1 (cid:80) (mˆi−r∗mi)(cid:0) ui(sˆ )−ui(σ)(cid:1) isaremainderaccount-
n i ν ⋆ ν
the Pareto frontier), which may not be the same as in Gˆ ing for collective misalignment and unequal ri, and K and
unless mˆi = r·mi for some r > 0. Further discussion on ri are defined as in Proposition 6. Note that when all ri
this point can be found in Appendix E.3. are equal or CA=1 then there is an r∗ with R(σ)=0.
Proposition 3 (D3). If IC = 1 and CC = 1 then any Beforecontinuing,wenotethatunlikeinthesingle-agent
strategy σ the agents play is such that w(σ) = w . If case, even small irrationalities can compound to dramati-
⋆
IA = 1 then σ is Pareto-optimal for the principals. If, cally lower individual payoffs (and thus welfare) in multi-
furthermore, mˆi = r ·mi for some r > 0 for all i, then agent settings, as formalised by the following lemma.
wˆ(σ)=wˆ .
⋆
Lemma5. Foranyϵ≻0, thereexistsagameGsuchthat
The proof of D4 follows naturally from Definition 10. w =w but for any x>0, however small, w −w <x.
0 + ϵ −
The final desideratum (D5) is simple in the case of indi-
vidual preferences (due to the form of our normalisation In many games of interest, however, the welfare of the
function). In the case of collective preferences (i.e. the or- players will be much more robust to small mistakes. For
dering ⪯w over mixed strategies induced by w), we make example, let us suppose that G is (ϵ,∆)-robust, in the
use of the fact that the relative magnitude of the agents’ sense that all ϵ-NEs are contained within a ball of radius
utility functions in both games must be the same (which ∆arounda(true)NE[4]. Thenitisrelativelystraightfor-
is closely related to the ‘fairness’ condition mˆi = r·mi in ward to show that:
Proposition 3).
2∆(cid:88)
w −w ≤ max|ui(s)−ui(s′)|.
Proposition 4 (D4). If CA=1 then w
⋆
=w +. 0 ϵ n
i
s,s′
6Indeed, there are a range of settings in which the price of
anarchy can be bounded under play that is not perfectly
rational [36]. While the bound we provide above is highly
general, assumingfurtherstructureinthegamemayallow
us to tighten it further.
Theorem1characterisestheprincipals’welfareregretin
terms of three of our four measures. Our next result char-
acterisesthegapbetweentheidealandmaximalwelfarein
terms of our fourth measure: collective alignment.
Proposition 7. GivenagameGwithcollectivealignment
CA, then w +−w
⋆
≤
K(cid:80) nimi
(1−CA), where K is defined
as in Proposition 6.
This bound can be applied to either the agent or princi-
pal game; we denote the collective alignment in the lat- Figure 3: We report mean principal welfare (in red) nor-
ter as CˆA. While it is possible characterise the differ- malised to [wˆ ,wˆ ], with wˆ and wˆ in green. The lower
− + • ⋆
ence between CˆA and CA in terms of IA, a tighter bound bounds on welfare, given by Theorem 1, and on wˆ (com-
⋆
on wˆ −wˆ(σ) can be obtained by considering the collec- pared to wˆ ), given by Proposition 7, are in orange and
+ +
tivealignmentbetweentheprincipalsandsimplysumming blue, respectively. Shaded areas show 90% confidence in-
the right-hand terms of the inequalities in Theorem 1 and tervals.
Proposition 7.
The preceding results show that individual/collective
alignment/capabilitiesarebothnecessaryand sufficientfor ϵ = 1−IC, and record the mean principal welfare over
goodoutcomesinmulti-agentsettings. Concretely,thene- these strategies.
cessity of CA/CC is shown in Lemmas 2 and 4. The ne- Asexpected,weseeapositiverelationshipforeachmea-
cessityofICfollowsfromthefactthatϵ-NE(G)=Σwhen sure. Given the ease of coordinating in relatively small
ϵ = 1 (which could lead to arbitrarily poor principal wel- games, alignment is more important in this example than
fare) and of IA, from the fact that one can construct G capabilities, as can be seen from the gentler slope of the
from Gˆ with uˆ =−u (so the best outcomes for the agents welfare curve as IA increases, and in how CˆA places an
aretheworstoutcomesfortheprincipals). Thesufficiency upperlimitonprincipalwelfareunderotherwiseidealcon-
ofthefourmeasuresisshowninPropositions3and4(and ditions.
more generally in Theorem 1 and Proposition 7). InAppendixC.1weincludefurtherdetailsandplotsfor
games ranging between 101 and 103 outcomes, with values
of the fixed measures ranging between 0 to 1. We find
6 Experiments
that the overall dependence of principal welfare upon each
measure is robust, though the tightness of the bounds is
While our primary contributions in this work are theo- reduced in larger games and for lower values of the fixed
retical, we support these results by: i) empirically vali- measures.
dating the bounds above; and ii) showing how the vari-
ous measures we introduce can be inferred from data.8 In
our experiments we define ν using c(u) = E [u(s)] and 6.2 Inference of Measures
s
m(u) = ∥u∥ . We also limit our attention to pure strate-
2 In the previous sections, we implicitly assumed full knowl-
gies, due to the absence of scalable methods for exhaus-
edge of the delegation game in order to define our four
tively finding mixed ϵ-NEs in large games.
measures. In the real world, this assumption will rarely
be valid, motivating the question of when and how we
6.1 Empirical Validation might infer the values of these measures given limited ob-
servations. Moreconcretely,weassumeaccesstoadataset
In order to visualise the results in the preceding section,
D containing (pure) strategy profiles and payoff vectors
we conduct a series of experiments in which we monitor
(s,u,uˆ)∼ d.
how the principals’ welfare changes based on the degree i.i.d.
Inferring alignment is relatively straightforward, as we
of control and cooperation in the delegation game. An
can simply approximate each ui and uˆi by using only the
example is shown in Figure 3, in which we change one
strategies D(S) ⊆ S contained in D, for which we know
measure, setting all others to 0.9. At each step we gen-
their values. Inferring capabilities is much more challeng-
erate 25 random delegation games (with approximately
ing, as they determine how agents play the game and
ten outcomes), compute the set of strategies s such that
(cid:2) (cid:3) therefore limit our observations. Fundamentally, measur-
w(s) ∈ w + CC · (w − w ),w + (w − w ) where
ϵ ⋆ 0 ϵ ⋆ 0 ing capabilities requires comparing the achieved outcomes
8Our code is available at https://github.com/lrhammond/ against better/worse alternatives, but if agents’ capabil-
delegation-games. ities are fixed we might never observe these other out-
7We focused on strategic-form games to make our the-
oretical contributions clearer and more general (as other
game models can typically be reduced to strategic-form).
A natural next step, however, would be to develop more
specificresultsinmorecomplex,structuredmodels,suchas
Markovgames,whichcouldformthebasisforapplications
to multi-agent reinforcement learning. Other important
extensionsincludegameswhere: a)thecorrespondencebe-
tween principals and agents is not one-to-one; and b) the
principals can also take actions. Finally, if the concepts
in this work are to more directly help us build safe and
beneficial AI systems, we must develop better methods for
inferring them from data.
Acknowledgements
Figure 4: We report the mean absolute error of estimates
of the four measures. The red, orange, blue, and green The authors wish to thank Bart Jaworksi for contribut-
linesrepresentgameswith10k outcomesfork ∈{1,2,3,4}, ing to an earlier version of this work, several anony-
respectively. Shaded areas show 90% confidence intervals. mous reviewers for their comments, and Jesse Clifton,
Joar Skalse, Sam Barnett, Vincent Conitzer, Charlie Grif-
fin, David Hyland, Michael Wooldridge, Ted Turocy, and
comes. Moreover, only observing the agents acting to-
Alessandro Abate for insightful discussions on these top-
gether(alone)leavesusunabletoasseshowwelltheywould
ics. Lewis Hammond acknowledges the support of an
perform alone (together), due to the orthogonality of indi-
EPSRC Doctoral Training Partnership studentship (Ref-
vidual and collective capabilities.
erence: 2218880). Oliver Sourbut acknowledges the Uni-
While there are many relaxations that might overcome
versity of Oxford’s Autonomous Intelligent Machines and
theseissues,perhapsthesimplestandweakestistoassume
SystemsCDTandtheGoodVenturesFoundationfortheir
that: a) all utilities are non-negative; and b) we receive
support.
observations of the agents acting both alone and together.
We can then estimate (upper bounds of) IC and CC as
follows. References
Proposition 8. Given a game G, if ui(s) for every i and
[1] Alcalde-Unzu, J. and Vorsatz, M. “Do we agree?
s∈S, and d has maximal support over the outcomes gen-
Measuring the cohesiveness of preferences”. Theory
erated when agents act together/alone (respectively), then:
and Decision 80.2 (2015), pp. 313–339.
min s∈D(S)w(s) [2] Armstrong, S. and Mindermann, S. “Occam’s Razor
CC≤ lim , and
|D|→∞max s∈D(S)w(s) Is Insufficient to Infer the Preferences of Irrational
Agents”. Proceedings of the 32nd International Con-
ui(s)
ICi ≤ lim min . ference on Neural Information Processing Systems.
|D|→∞s∈D(S)max s˜i∈D(Si)ui(s−i,s˜i)
2018, pp. 5603–5614.
In Figure 4 we evaluate the accuracy of these estimates [3] Arrow,K.J.,Barankin,E.W.,andBlackwell,D.“5.
usingsamplesgeneratedfrom100randomlygenerateddel- Admissible Points of Convex Sets”. Contributions to
egation games of various sizes. Consistent with our pre- theTheoryofGames(AM-28),VolumeII.Princeton
vious arguments, it is far easier to infer alignment than University Press, 1953, pp. 87–92.
capabilitiesinthesetupweconsider,thoughweleaveopen
[4] Awasthi, P., Balcan, M.-F., Blum, A., Sheffet,
the question of whether stronger assumptions and/or dif-
O., and Vempala, S. “On Nash-Equilibria of
ferent setups might allow us to gain improved estimates of
Approximation-Stable Games”. Algorithmic Game
the latter.
Theory.SpringerBerlinHeidelberg,2010,pp.78–89.
[5] Back, K. “Concepts of similarity for utility func-
7 Conclusion
tions”. Journal of Mathematical Economics 15.2
(1986), pp. 129–142.
In this work we formalised and studied problems of coop-
[6] Bernheim, B. D. “Rationalizable Strategic Behav-
eration and control in delegation games – a general model
ior”. Econometrica 52.4 (1984), p. 1007.
for interactions between multiple AI systems on behalf of
multiplehumans–breakingthemdownintoissuesofalign- [7] Bostrom, N. Superintelligence: Paths, Dangers,
ment and capabilities. We showed how these concepts are Strategies. Oxford University Press, 2014.
both necessary and sufficient for obtaining good outcomes
as well as how they can be inferred from data.
8[8] Che, Y.-K., Kim, J., Kojima, F., and Ryan, C. T. [23] Insa-Cabrera, J., Benacloch-Ayuso, J.-L., and
“”Near” Weighted Utilitarian Characterizations of Hern´andez-Orallo, J. “On Measuring Social Intelli-
Pareto Optima”. arXiv:2008.10819 (2020). gence: Experiments on Competition and Coopera-
tion”.Artificial General Intelligence.SpringerBerlin
[9] Christiano, P. Clarifying “AI alignment”. AI Align-
Heidelberg, 2012, pp. 126–135.
ment. 2018.
[24] Kalai, A. and Kalai, E. “Cooperation in Strategic
[10] Cobbe, K., Klimov, O., Hesse, C., Kim, T., and
Games Revisited”. The Quarterly Journal of Eco-
Schulman, J. “Quantifying Generalization in Rein-
nomics 128.2 (2013), pp. 917–966.
forcement Learning”. Proceedings of the 36th Inter-
national Conference on Machine Learning, ICML [25] Kenton, Z., Everitt, T., Weidinger, L., Gabriel, I.,
2019,9-15June2019,LongBeach,California,USA. Mikulik, V., and Irving, G. “Alignment of Language
Ed.byChaudhuri,K.andSalakhutdinov,R.Vol.97. Agents”. arXiv:2103.14659 (2021).
2019, pp. 1282–1289. [26] Koutsoupias, E. and Papadimitriou, C. “Worst-Case
[11] Conen, W. and Sandholm, T. “Preference elicitation Equilibria”. STACS 99. Springer Berlin Heidelberg,
in combinatorial auctions”. Proceedings of the 3rd 1999, pp. 404–413.
ACM conference on Electronic Commerce. 2001. [27] Kuleshov, V. and Schrijvers, O. “Inverse Game The-
[12] Cousins, C., Mishra, B., Areyan Viqueira, E., and ory: Learning Utilities In succinct Games”. Web
Greenwald, A. “Learning Properties in Simulation- and Internet Economics.SpringerBerlinHeidelberg,
BasedGames”.Proceedingsofthe2023International 2015, pp. 413–427.
Conference on Autonomous Agents and Multiagent [28] Laffont, J.-J. and Martimort, D. The Theory of In-
Systems. 2023, pp. 272–280. centives. Princeton University Press, 2002.
[13] Dafoe, A., Hughes, E., Bachrach, Y., Collins, T., [29] Legg, S. and Hutter, M. “Universal Intelligence: A
McKee, K. R., Leibo, J. Z., Larson, K., and Definition of Machine Intelligence”. Minds and Ma-
Graepel, T. “Open Problems in Cooperative AI”. chines 17.4 (2007), pp. 391–444.
arXiv:2012.08630 (2020).
[30] Leibo,J.Z.,Du´en˜ez-Guzm´an,E.A.,Vezhnevets,A.,
[14] Daniilidis, A. “Arrow-Barankin-Blackwell Theorems Agapiou, J. P., Sunehag, P., Koster, R., Matyas, J.,
and Related Results in Cone Duality: A Survey”. Beattie, C., Mordatch, I., and Graepel, T. “Scalable
Lecture Notes in Economics and Mathematical Sys- Evaluation of Multi-Agent Reinforcement Learning
tems.SpringerBerlinHeidelberg,2000,pp.119–131. with Melting Pot”. Proceedings of the 38th Interna-
tionalConferenceonMachineLearning,ICML2021,
[15] Doran, J. E., Franklin, S., Jennings, N. R., and Nor-
18-24July2021,VirtualEvent.Ed.byMeila,M.and
man,T.J.“Oncooperationinmulti-agentsystems”.
Zhang, T. Vol. 139. 2021, pp. 6187–6199.
The Knowledge Engineering Review 12.3 (1997),
pp. 309–314. [31] Mas-Colell, A., Whinston, M. D., and Green, J. R.
Microeconomic Theory. Oxford University Press,
[16] Du, Y., Leibo, J. Z., Islam, U., Willis, R., and Sune-
1995. 981 pp.
hag, P. “A Review of Cooperation in Multi-Agent
Learning”. arXiv:2312.05162 (2023). [32] Pearce, D. G. “Rationalizable Strategic Behavior
and the Problem of Perfection”. Econometrica 52.4
[17] Fischhoff, B. and Manski, C. F., eds. Elicitation of
(1984), p. 1029.
Preferences. Springer Netherlands, 2000.
[33] Peters, M. “Competing mechanisms”. Cana-
[18] Fu¨rnkranz, J. and Hu¨llermeier, E., eds. Preference
dian Journal of Economics/Revue canadienne
Learning. Springer Berlin Heidelberg, 2011.
d’´economique 47.2 (2014), pp. 373–397.
[19] Gal, Y., Pfeffer, A., Marzo, F., and Grosz, B. J.
[34] Rapoport, A. and Chammah, A. M. “The Game
“LearningSocialPreferencesinGames”.Proceedings
of Chicken”. American Behavioral Scientist 10.3
of the 19th National Conference on Artifical Intelli-
(1966), pp. 10–28.
gence. 2004, pp. 226–231.
[35] Rizk, Y., Awad, M., and Tunstel, E. W. “Coopera-
[20] Gleave, A., Dennis, M., Legg, S., Russell, S., and
tiveHeterogeneousMulti-RobotSystems:ASurvey”.
Leike, J. “Quantifying Differences in Reward Func-
ACM Computing Surveys 52.2 (2019), pp. 1–31.
tions”. 9th International Conference on Learning
[36] Roughgarden, T. “Intrinsic Robustness of the Price
Representations,ICLR2021,VirtualEvent,Austria,
ofAnarchy”.JournaloftheACM 62.5(2015),pp.1–
May 3-7, 2021. 2021.
42.
[21] Gul, F. and Pesendorfer, W. “Temptation and Self-
[37] Russell, S. Human Compatible. Penguin LCC US,
Control”. Econometrica 69.6 (2001), pp. 1403–1435.
2019.
[22] Hubinger,E.Clarifyinginneralignmentterminology.
[38] Sengul, M., Gimeno, J., and Dial, J. “Strategic
Alignment Forum. 2020.
Delegation: A Review, Theoretical Integration, and
Research Agenda”. Journal of Management 38.1
(2011), pp. 375–414.
9[39] Simon, H. A. “Rational choice and the structure of [55] Yu, L., Song, J., and Ermon, S. “Multi-Agent Ad-
the environment”. Psychological Review 63.2 (1956), versarial Inverse Reinforcement Learning”. Proceed-
pp. 129–138. ings of the 36th International Conference on Ma-
chine Learning, ICML 2019, 9-15 June 2019, Long
[40] Skalse, J., Farnik, L., Motwani, S. R., Jenner, E.,
Beach, California, USA. Ed. by Chaudhuri, K. and
Gleave, A., and Abate, A. “STARC: A General
Salakhutdinov, R. Vol. 97. 2019, pp. 7194–7201.
FrameworkForQuantifyingDifferencesBetweenRe-
ward Functions”. arXiv:2309.15257 (2023).
[41] Taylor,J.“Quantilizers:ASaferAlternativetoMax- A Notation
imizersforLimitedOptimization”.Proceedingsofthe
2016AAAI/ACMConferenceonAI,Ethics,andSo- See the opening paragraph of Section 2 for a summary of
ciety. 2016. the notational principles employed in this paper. Other
[42] Taylor,J.,Yudkowsky,E.,LaVictoire,P.,andCritch, general mathematical notation (aside from mere abbrevia-
A. “Alignment for Advanced Machine Learning Sys- tions) that we repeatedly make use of is as follows.
tems”. Ethics of Artificial Intelligence. Oxford Uni-
versity Press, 2020, pp. 342–382. Term Meaning Section
[43] Tewolde, E. “Game Transformations that preserve
G Game 2
Nash Equilibrium sets and/or Best Response sets”.
u Utility function 2
arXiv:2111.00076 (2021).
S Pure strategy space 2
[44] Torren˜o,A.,Onaindia,E.,Komenda,A.,andSˇtolba, s Pure strategy 2
M. “Cooperative Multi-Agent Planning: A Survey”. Σ Mixed strategy space 2
ACM Computing Surveys 50.6 (2017), pp. 1–32. σ Mixed strategy 2
[45] Tuomela, R. Cooperation. Springer Netherlands, D Delegation game 3
2000. w Social welfare function 3
w Ideal welfare 3
[46] Vickers,J.“DelegationandtheTheoryoftheFirm”. +
w Pessimal welfare 3
The Economic Journal 95 (1985), p. 138. −
w Maximal welfare 3
⋆
[47] von Neumann, J. and Morgenstern, O. Theory of w Minimal welfare 3
•
Games and Economic Behavior. Princeton Univer- ⪯ Preference ordering 4.1.1
sity Press, 1944. ν Normalisation function 4.1.1
[48] Walsh, W. E., Das, R., Tesauro, G., and Kephar, m (Strictly convex) norm 4.1.1
J. O. “Analyzing Complex Strategic Interactions in c (Affine-equivariant) shift function 4.1.1
Multi-Agent Systems”. Game Theoretic and Deci- µw Normalised welfare proxy 4.2.1
sion Theoretic Agents Workshop at AAAI. 2002. w ϵ Lowest ϵ-NE welfare 4.2.2
[49] Waugh, K., Ziebart, B. D., and Bagnell, J. A.
“Computational Rationalization: The Inverse Equi- B Proofs
librium Problem”. Proceedings of the 28th Interna-
tional Conference on International Conference on
We begin by providing proofs for all of the results stated
Machine Learning. 2011, pp. 1169–1176.
in the main body of the paper, as well as a small number
[50] Wellman, M. P. “Methods for Empirical Game- of auxiliary results.
TheoreticAnalysis”.ProceedingsoftheTwenty-First
AAAI Conference on Artificial Intelligence. 2006,
B.1 Desiderata
pp. 1552–1555.
[51] West, S. A., Griffin, A. S., and Gardner, A. “Social We begin by proving that the measures introduced in Def-
semantics: altruism, cooperation, mutualism, strong initions 7, 9, 10, and 11 satisfy the desiderata introduced
reciprocity and group selection”. Journal of Evolu- in Section 3. The original statements of the results can
tionary Biology 20.2 (2007), pp. 415–432. be found in 5.1. In the proofs below, we use the following
properties of the norm m:
[52] Woolley, A. W., Chabris, C. F., Pentland, A.,
Hashmi,N.,andMalone,T.W.“EvidenceforaCol- • Positive definiteness: if m(u)=0 then u=0,
lective Intelligence Factor in the Performance of Hu-
manGroups”.Science 330.6004(2010),pp.686–688. • Absolute homogeneity: m(k ·u) = |k|·m(u) for all
[53] Yaari, M. E. “Rawls, edgeworth, shapley, nash: The- k ∈R and u∈U,
ories of distributive justice re-examined”. Journal of • Strict convexity: m(cid:0) k·u+(1−k)·u′(cid:1) < k·m(u)+
Economic Theory 24.1 (1981), pp. 1–39.
(1−k)·m(u′) for all u̸=u′ ∈U and all k ∈(0,1).
[54] Yamashita, T. “Mechanism Games with Multiple
Principals and Three or More Agents”. Economet- Note that the second and third properties imply the trian-
rica 78.2 (2010), pp. 791–801. gle inequality: m(u+u′)≤m(u)+m(u′).
10Proposition 1 (D1). Consider a delegation game D with ui = −uˆi. Because ui ̸= 0 ̸= uˆi then by the absolute
ν ν ν ν
measures IA(D), IC(D), CA(D), and CC(D). Holding homogeneity of m and the fact that m(ui)=1:
ν
fixedanythreeofthemeasures,thenforanyvaluev ∈[0,1]
(or v ∈[0,1]n for IA or IC), there is a game D′ such that IAi =1−1 ·m(ui −uˆi)=1−1 ·m(2ui)=1−m(ui)=0,
the fourth measure takes value v (v) in D′ and the other 2 ν ν 2 ν ν
measures retain their previous values.
asrequired. Ontheotherhand,ifIAi =0thenthisimplies
Proof. The case for the capabilities measures IC and CC that m(ui −uˆi)=2. The triangle inequality implies that
ν ν
is straightforward as they define how the agents play the m(ui − uˆi) ≤ m(ui) + m(−uˆi). Because m is strictly
ν ν ν ν
game, not any particular features of the game itself. Thus convex, this inequality is an equality only when ui and
ν
wemayletD′ =D inthesecases,andsetICorCCtoany −uˆi arecollinear, i.e. ui =−λuˆi forsomeλ. Asm(ui)=
ν ν ν ν
v ∈[0,1]n or v ∈[0,1] respectively. m(−uˆi) = 1, this is possible only when λ = 1, and hence
ν
ForthecaseofIAweleavefixeduandvaryuˆ. Foreach ui =−uˆi. Thus, by 1, we have ⪯i=⪰ˆi as required.
ν ν
i,ifv[i]=1thenwesetuˆi =ui,andifv[i]=0thenweset
Finally, we consider the case of perfect collective align-
uˆi = −ui. Otherwise, assuming U is not one-dimensional, ment between two or more agents. First suppose that the
there are infinitely many choices of vectors u ∈ U such agents all have identical preferences ⪯. Then, by Lemma
that m(u ν −ui ν) = 2−2v[i]. It suffices to pick any such 1, this means that we have ui
ν
=uj
ν
for all i,j ∈N.
u and set uˆi =u. For the case of CA we vary both u and Letting u denote the common normalised utility, then
ν
uˆ, in order to preserve IA. As before, if v = 1 then we we have ui = miu +ci for each 1 ≤ i ≤ n. The welfare
ν
set each ui ν = u ν for some u ̸= 0. We then set each uˆi as representative utility is therefore given by:
above, such that m(uˆi −ui)=2−2IAi(D), ensuring that
ν ν
IA(D) = IA(D′). Similarly, if v = 0 then we set each ui (cid:80) miui u (cid:80) mi
µw := i ν = ν i =u .
such that ui ̸=0 but µw =0, which gives: (cid:80) mi (cid:80) mi ν
i i
CA(D):=1−(cid:88) mi ·m(µw−ui) Thus we have that:
(cid:80) mj ν
i j (cid:88) mi
=1−(cid:88) mi ·m(0−ui) CA:=1− (cid:80) mj ·m(µw−ui ν)
(cid:80) mj ν i j
i j (cid:88) mi
=1−(cid:88) mi ·1 =1− (cid:80) mj ·m(µw−u ν)
(cid:80) mj i j
i j (cid:88) mi
=1−1 =1− (cid:80) mj ·m(0)
=0. i j
=1,
Wethenonceagainadjusteachuˆisothatm(uˆi−ui)=2−
ν ν
2IAi(D),usingtheargumentfromprecedingcase. Finally, as required. Conversely, if we suppose that CA = 1 then
forthegeneralcaseofv ∈(0,1),thereareagainaninfinite we must have that m(µw −ui ν) = 0 for all 1 ≤ i ≤ n. By
numberofchoicesofeachui ̸=0suchthat(1−v)(cid:80) mj = the triangle inequality we have:
j
(cid:80) mi·m(µw−ui). Givensuchasettingofu, wemayset
i ν m(ui −uj)≤m(ui −µw)+m(µw−uj)=0,
uˆ as in the preceding cases. ν ν ν ν
Proposition 2 (D2). For any 1≤i≤n, IAi =1 (IAi = for any i,j ∈ N, and hence that ui ν = uj ν, by the positive
0) if and only if ⪯i= ⪯ˆi (⪯i= ⪰ˆi ). Similarly CA = 1 if definiteness of m. Thus, again by Lemma 1, we have that
alloftheagentshavethesamepreferences, concludingthe
and only if ⪯i=⪯j for every i,j ∈N.
proof.
Proof. We begin with the case of individual alignment by
considering the ith principal-agent pair, who have utility Proposition 3 (D3). If IC = 1 and CC = 1 then any
functionsuˆi andui,respectively. Recallthatwewriteci := strategy σ the agents play is such that w(σ) = w ⋆. If
c(ui) and mi :=m(ui−ci), and analogously for uˆi. IA = 1 then σ is Pareto-optimal for the principals. If,
Let us assume that
⪯i=⪯ˆi
. Then, by applying Lemma
furthermore, mˆi = r ·mi for some r > 0 for all i, then
wˆ(σ)=wˆ .
1 to the case of ui and uˆi we have that ⋆
1 1 Proof. RecallthatifIC=1−ϵandCC=δthentheagents
IAi =1− ·m(u −uˆi)=1− ·m(0)=1,
2 ν ν 2 onlyplaystrategiesσ suchthatw(σ)≥w ϵ+δ·(w ⋆−w 0).
If IC = 1 (and hence ϵ = 0) and CC = δ = 1 then clearly
as required. Conversely, if IAi = 1 then by the positive
w(σ)=w , as required.
⋆
definiteness of m we must have ui = uˆi and hence (by
ν ν Nowletussupposethateachagentismaximallyaligned
Lemma 1) that
⪯i=⪯ˆi
, as required. with its principal, i.e. IAi = 1 for every 1 ≤ i ≤ n. By
Next, we consider the case of maximal misalignment. In theconstructionintheproofofLemma1,thisimpliesthat
this case we have ⪯i= ⪰ˆi and thus (by Lemma 1) that ui =aiuˆi+bi whereai := mi. Assume,foracontradiction,
mˆi
11thatσ isnotPareto-optimalfortheprincipals. Thenthere Combining these two statements and rearranging results
exists some σ′ and some j ∈ N such that uˆj(σ′) > uˆj(σ) in:
and uˆi(σ′)≥uˆi(σ) for all 1≤i≤n. But then:
(cid:32) (cid:33)
(cid:88) mi mi
uj(σ′)=ajuˆj(σ′)+bj >ajuˆj(σ)+bj =uj(σ),
(cid:80)
m1
j
−a
(cid:80)
m2
j
ν(ui 1)=b.
i j 1 j 2
and similarly ui(σ′) ≥ ui(σ) for all 1 ≤ i ≤ n. Thus,
we have that w(σ′) = 1 (cid:80) ui(σ′) > 1 (cid:80) ui(σ) = w(σ),
n i n i Assumingthattheν(ui)arenotaffine-dependent,thenwe
contradicting the fact that w :=max w(σ). 1
Finally, let us assume that⋆ mˆi =r·mσ i for every 1≤i≤ must have (cid:80)m mi 1 j = a (cid:80)m mi 2 j for each 1 ≤ i ≤ n. Thus, we
j 1 j 2
n. Then ai = mi = 1 and so: have µw =µw, and therefore:
mˆi r 1 2
1 (cid:88)
wˆ(σ)= n uˆi(σ) CA(D ):=1−(cid:88) mi 1 ·m(cid:0) µw−ν(ui)(cid:1)
i 1 (cid:80) mj 1 1
=
1 (cid:88) r·(cid:0) ui(σ)−bi(cid:1) i j 1
=
n
r
(cid:88)i
ui(σ)− r (cid:88) bi
=1−(cid:88)
i
(cid:80)m jmi 2
j
2
·m(cid:0) µw
2
−ν(ui 2)(cid:1)
n n =:CA(D ),
i i 2
r (cid:88)
=rw(σ)− bi.
n which concludes the proof.
i
This implies that argmax wˆ(σ) = argmax w(σ), and
σ σ
hence that wˆ(σ) = wˆ , which concludes the final part of
⋆
the proof. B.2 Bounding Welfare Regret
Proposition 4 (D4). If CA=1 then w =w .
⋆ +
Next, we provide proofs of the bounds on welfare regret
Proof. Following the proof of Proposition 2, CA = 1 im-
from Section 5.2.
plies that ui = µw for every 1 ≤ i ≤ n, and thus that
ν
⪯i=⪯µ, where ⪯µ denotes the preference ordering over
mixed strategies induced by µw. By Lemma 3 we have
that ⪯µ=⪯w, where ⪯w denotes the preference ordering Proposition 6. Given a delegation game D, we have:
over mixed strategies induced by w, and hence, by tran-
sitivity, that ⪯i=⪯w for each 1 ≤ i ≤ n. Thus, we have wˆ −wˆ(σ)≤ 1 (cid:88) ri(cid:0) ui(sˆ )−ui(σ)(cid:1) + 4K mˆ⊤(1−IA),
argmax w(s)=argmax ui(s) and therefore: ⋆ n ⋆ n
s s
i
1 (cid:88) 1 (cid:88)
maxw(s)=max ui(s)= maxui(s)=w .
s s n i n i s + where ri := m mˆi i, mˆ[i] = mˆi, K satisfies ∥u ν − u′ ν∥ ∞ ≤
K·m(u −u′) for any u,u′ ∈U, and wˆ(sˆ )=wˆ .
By Lemma 7, w =max w(s), and so w =w . ν ν ⋆ ⋆
⋆ s ⋆ +
Proposition 5 (D5). Given a delegation game D , let D
1 2 Proof. Tobeginwith,letusconsidertheprincipal’swelfare
besuchthat⪯i=⪯i and⪯ˆi =⪯ˆi foreach1≤i≤n. Then
1 2 1 2 regret from a pure strategy profile s. Let sˆ ⋆ be such that
IA(D )=IA(D ) and IC(D )=IC(D ). Moreover, if D
1 2 1 2 2 wˆ(sˆ ) = wˆ , which we know must exist from Lemma 7.
⋆ ⋆
is such that ⪯w=⪯w and the ui are affine-independent,
1 2 Then we have:
then CA(D )=CA(D ) and CC(D )=CC(D ) as well.
1 2 1 2
Proof. First, notethatthecapabilitymeasuresaredefined wˆ −wˆ(s)
⋆
extrinsically to the game, and so do not vary as the game
=wˆ(sˆ )−wˆ(s)
varies. We therefore concern ourselves with the alignment ⋆
measures. =1 (cid:88) uˆi(sˆ )− 1 (cid:88) uˆi(s)
Inthecaseofindividualalignment,noticethatif⪯i=⪯i n ⋆ n
1 2 i i
then ν(ui)=ν(ui) by Lemma 1, and similarly for uˆi and (cid:32) (cid:33) (cid:32) (cid:33)
uˆi. Thu1 s we hav2 e that m(cid:0) ν(uˆi)−ν(ui)(cid:1) = m(cid:0) ν(uˆ1 i)− =1 (cid:88) mˆiuˆi(sˆ )+cˆi − 1 (cid:88) mˆiuˆi(s)+cˆi
ν(2 ui 2)(cid:1) , and hence IAi(D 1)=IA1 i(D 2). A1 pplying the s2 ame n i ν ⋆ n i ν
(cid:32) (cid:33)
argument to each i gives IA(D )=IA(D ).
1 2 1 (cid:88) (cid:88)
Forthecaseofcollectivealignment,thenbyLemma6we =
n
mˆiuˆi ν(sˆ ⋆)− mˆiuˆi ν(s) .
have w = aw +b for some a ∈ R and b ∈ R. Because i i
1 2 >0
we also have ν(ui)=ν(ui) for all 1≤i≤n, then:
1 2
BecausemisanormandU isfinite-dimensional,thenthere
(cid:80) ui −ci (cid:80) miν(ui) (cid:80) miν(ui)
µw 1 := (cid:80)i i1 mi 1 1 = i (cid:80) imi 1 1 = i (cid:80) imi 1 2 . i fs orso am nye ufi ,n uit ′e ∈K U.su Ac ph pt lyh ia nt g∥ tu hν is− bu ou′ ν∥ n∞ d t≤ o eK ac· hm uˆ( i νu aν n− du u′ ν i ν) ,
12and writing di :=m(uˆi −ui) we have: remainder can be seen to be intimately related to the dif-
ν ν
ferent ratios ri := mˆi, which determine when Pareto im-
(cid:88) mˆiuˆi ν(sˆ ⋆)−(cid:88) mˆiuˆi ν(s) provements are conm sii dered gains in agent welfare. When
i i all ri are identical, a choice of r∗ =ri causes R(σ) to van-
≤(cid:88) mˆi·(cid:0) ui(sˆ )+Kdi(cid:1) −(cid:88) mˆi·(cid:0) ui(s)−Kdi(cid:1) ish, which we refer to as the ‘perfectly calibrated’ case.
ν ⋆ ν
(We discuss several continuous bounds on this remain-
i i
(cid:88) (cid:88) (cid:88) der in Appendix B.3.) Now, from Definition 11 we have
= mˆiui(sˆ )− mˆiui(s)+2K mˆidi
ν ⋆ ν w(σ)≥w +CC·(w −w ), which implies that:
ϵ ⋆ 0
i i i
=(cid:88) mˆiui(sˆ ⋆)−ci −(cid:88) mˆiui(s)−ci +2K(cid:88)
mˆidi
mi mi w −w(σ)≤w −w −CC·(w −w )
i i i ⋆ ⋆ ϵ ⋆ 0
=(cid:88) ri(cid:0) ui(sˆ )−ui(s)(cid:1) +2K(cid:88) mˆidi, =(1−CC)·w ⋆−w ϵ+CC·w 0
⋆
=(1−CC)·w −w −(1−CC)·w +w
i i ⋆ ϵ 0 0
=(1−CC)(w −w )+(w −w ).
where ri := mˆi. Finally, note that for any σ, we have: ⋆ 0 0 ϵ
mi
w(σ):=E [w(s)]=E (cid:104)1 (cid:88) ui(s)(cid:105) = 1 (cid:88) E (cid:2) ui(s)(cid:3) , Substituting this inequality into the previous inequality
σ σ n n σ
and then into the bound from Proposition 6 gives:
i i
bythelinearityofexpectation. Applyingthistothebound
above,substitutingindi =2(1−IAi),andwritingthefinal 4K
wˆ −wˆ(σ)≤ mˆ⊤(1−IA)+r∗((w −w )
sum over players in vector notation gives: ⋆ n 0 ϵ
+ (1−CC)(w −w ))+R(σ).
(cid:32) (cid:33) ⋆ 0
wˆ −wˆ(σ)≤ 1 (cid:88) ri(cid:0) ui(sˆ )−ui(σ)(cid:1) +2K(cid:88) mˆidi
⋆ n ⋆
i i
= 1 (cid:88) ri(cid:0) ui(sˆ )−ui(σ)(cid:1) + 4K mˆ⊤(1−IA),
n ⋆ n
i
where mˆ[i]=mˆi, completing the proof.
Theorem 1. Given a delegation game D, we have that: Proposition 7. GivenagameGwithcollectivealignment
4K
CA, then w +−w
⋆
≤
K(cid:80) nimi
(1−CA), where K is defined
wˆ −wˆ(σ)≤ mˆ⊤(1−IA)+r∗((w −w ) as in Proposition 6.
⋆ n 0 ϵ
+ (1−CC)(w −w ))+R(σ),
⋆ 0
Proof. To begin, we first choose si ∈ argmax ui(s) for
where IC = 1 − ϵ, r∗ ∈ [min ri,max ri], R(σ) := + s
i i
1 (cid:80) (mˆi−r∗mi)(cid:0) ui(sˆ )−ui(σ)(cid:1) isaremainderaccount- each i. Second, because m is a norm and U is finite-
n i ν ⋆ ν dimensional, then there is some finite K such that ∥u−
ing for collective misalignment and unequal ri, and K and
u′∥ ≤ K ·m(u−u′) for any u,u′ ∈ U. Using these two
ri are defined as in Proposition 6. Note that when all ri ∞
pieces of information, we have:
are equal or CA=1 then there is an r∗ with R(σ)=0.
Proof. First, observing that by definition w ≥ w(sˆ ), we
⋆ ⋆ 1 (cid:88)
see that for any r∗ >0: w := ui(si )
+ n +
i
n1 (cid:88) ri(cid:0) ui(sˆ ⋆)−ui(σ)(cid:1) = 1 (cid:88)(cid:0) miui(si )+ci(cid:1)
i n ν +
i
= n1 (cid:88) r∗(cid:0) ui(sˆ ⋆)−ui(σ)(cid:1) + n1 (cid:88) (ri−r∗)(cid:0) ui(sˆ ⋆)−ui(σ)(cid:1) = 1 (cid:88)(cid:0) miµw(si )+ci(cid:1) + 1 (cid:88) mi(cid:0) ui(si )−µw(si )(cid:1)
i i n + n ν + +
i i
=r∗(w(sˆ ⋆)−w(σ))+ n1 (cid:88)
i
(mˆi−r∗mi)(cid:0) ui ν(sˆ ⋆)−ui ν(σ)(cid:1) ≤ n1 (cid:88)(cid:0) miµw(si +)+ci(cid:1) + n1 (cid:88) mi∥ui
ν
−µw∥
∞
i i
≤r∗(w ⋆−w(σ))+ n1 (cid:88) (mˆi−r∗mi)(cid:0) ui ν(sˆ ⋆)−ui ν(σ)(cid:1) ≤ 1 (cid:88)(cid:0) miµw(si )+ci(cid:1) + K ·(cid:88) mi·m(µw−ui).
i n + n ν
i i
=r∗(w −w(σ))+R(σ),
⋆
where R(σ) := 1 (cid:80) (mˆi −r∗mi)(cid:0) ui(sˆ )−ui(σ)(cid:1) . This Next, let us choose some s ∈argmax w(s). Then clearly
n i ν ⋆ ν ⋆ s
is a weighted combination of the agent welfare regret w(si ) ≤ w(s ) = w for any 1 ≤ i ≤ n, where the second
+ ⋆ ⋆
w − w(σ) with a ‘fairness remainder’ term R(σ). The equality follows from Lemma 7. Hence, by Lemma 3, we
⋆
13also have µw(si )≤µw(s ). Using this fact, we have: Lemma 2. There exists a (two-player, two-action) dele-
+ ⋆
gation game D such that for any x > 0, however small,
1 (cid:88)(cid:0) miµw(si )+ci(cid:1) even if IA(D) = 1 and IC(D) = 1, we have only one NE
n i + σ, and wˆ⋆−wˆ(σ) =1−x.
wˆ+−wˆ−
≤1 (cid:88)(cid:0) miµw(s )+ci(cid:1)
n ⋆ Proof. ConsiderthePrisoner’sDilemmaasspecifiedinFig-
i ure 5a, which we take to be the principal game Gˆ. Note
1 (cid:88) 1 (cid:88)
= nµw(s ⋆) mi+
n
ci that wˆ
+
= 1 and wˆ
−
= 0. If IA(D) = 1 then ⪯ˆi =⪯i by
i i Proposition 2, meaning that the NEs of G are the same
=1(cid:80) i(cid:0) ui(s ⋆)−ci(cid:1) (cid:88) mi+(cid:88)
ci
as those of Gˆ. Because IC(D) = 1 then we consider only
n (cid:80) mi these NEs, which in this case is simply s=(B,B), giving
i i i wˆ(s)= x. However,max wˆ(σ)=1−x,andthuswehave
= n1 (cid:88)(cid:0) ui(s ⋆)−ci(cid:1) + n1 (cid:88) ci wˆ wˆ⋆ +− −wˆ wˆ(σ −)2 =1−x. σ 2
i i
1 (cid:88)
= ui(s )
n ⋆ Lemma 3. For any σ,σ′ ∈ Σ, µw(σ) ≤ µw(σ′) if and
i
=w . only if w(σ)≤w(σ′).
⋆
Substituting this and using the fact that (cid:80) mi·m(µw − Proof. By Definition 10 we have that:
i
ui)=(1−CA)(cid:80) mi intoourourpreviousinequalitywe
haν ve: i (cid:80) ui−ci
w
+
≤w ⋆+
K(cid:80) nimi
(1−CA),
µw := (cid:80) 1i im (cid:88)i
(cid:80) ci
from which our result follows by subtracting w ⋆. = (cid:80) mi ui− (cid:80)i mi
i i i
B.3 Auxiliary Results n (cid:80) ci
= w− i .
(cid:80) mi (cid:80) mi
Lemma 1. For any u,u′ ∈ U, u = u′ if and only if i i
ν ν
⪯=⪯′.
The result follows by the linearity of expectation, taken
with respect to s. Indeed, setting a := n and b =
Proof. First, recall that, by Lemma 6, ⪯=⪯′ if and only (cid:80) mi
i
if there exist a ∈ R >0 and b ∈ R such that u = au′ +b. − (cid:80)(cid:80) i mci
i
thenbyLemma6wehavethat⪯µ=⪯w,where⪯µ
Using the fact that c is affine-equivariant we have: andi ⪯w denote the preference ordering over mixed strate-
gies induced by µw and w respectively.
u−c=au′+b−c(au′+b)
=au′+b−a·c(u′)−b
Lemma 4. There exists a family of (two-player, k-
=a·(u′−c′).
action) delegation games D such that even if IAi(D) = 1,
ICi(D) = 1 for each agent, and there is only one NE σ,
Next, by appealing to the absolute homogeneity of m and
the fact that a>0 we have: we have lim k→∞CA(D)=1 but lim k→∞ wˆ wˆ⋆ +− −wˆ wˆ(σ −) =1.
u−c
u = Proof. Consider the Traveller’s Dilemma as specified in
ν m Figure 5c, which we take to be the principal game Gˆ. As
a·(u′−c′)
= argued in the proof of Lemma 2, the fact that IA(D) = 1
(cid:0) (cid:1)
m a·(u′−c′) meansthatNE(G)=NE(Gˆ),andbecauseIC(D)=1then
a·(u′−c′) we need only consider the NEs of G, which in this case is
=
|a|·m(cid:0) u′−c′(cid:1) simply s = (E,E), and (as in the case of Lemma 2) leads
to a welfare regret of 1−2x. If the game has k actions for
u′−c′
= eachplayer, andweassumethatthepayoffsforeachagent
m′
arenormalisedsuchthatwˆ =1andwˆ =0thenwehave
=u′, + −
ν x= 1 . Thus, we have:
k+1
as required. Conversely, we see that:
wˆ −wˆ(σ) 1− 1
u−c u′−c′ lim ⋆ = lim k+1 =1.
u ν =u′ ν =⇒ m = m′ k→∞ wˆ +−wˆ − k→∞ 1−0
(cid:16)m(cid:17) (cid:16) m (cid:17)
=⇒ u= ·u′+ c− c′ . Now we consider the second limit. First note that
m′ m′
∥u1 − u2∥ = 3x and, as c is affine-equivariant, then
∞
Letting a′ := m > 0 and b′ := c− mc′ then we have u = ∥c1 − c2∥ = 3x too. Because m is a norm and U is
m′ m′ ∞
a′u′+b′ and hence ⪯=⪯′ (by Lemma 6), as required. finite-dimensional, then there is some finite L such that
14A B
A 1− x,1− x 0,1 A B C ··· Z
2 2
B 1,0 x,x A 1−x,1−x 1−3x,1 0,0 ··· 0,0
2 2
(a) B 1,1−3x 1−2x,1−2x 1−4x,1−x ··· 0,0
A B C C 0,0 1−x,1−4x 1−3x,1−3x ··· 0,0
A 1,1 x,1−ϵ2 0,0 . . . . . . . . . . . . ... . . .
B 1−ϵ1,x (1−ϵ1)x,(1−ϵ2)x x,0 Z 0,0 0,0 0,0 ··· x,x
C 0,0 0,x 0,0 (c)
(b)
Figure 5: (a) A Prisoner’s Dilemma leading to arbitrarily high welfare regret for the principals, despite perfect control
of each agent by its principal; (b) a game in which the welfare in the worst ϵ-NE is much lower than in the worst NE.;
and (c) a Traveller’s Dilemma leading to arbitrarily high welfare regret for the principals, despite perfect control of each
agent by its principal, and near-perfect collective alignment.
m(u−u′)≤L·∥u−u′∥ for any u,u′ ∈U, and therefore: erated when agents act together/alone (respectively), then:
∞
min w(s)
m1−m2 =m(u1−c1)−m(u2−c2)
CC≤ lim
s∈D(S)
, and
≤m(cid:0) (u1−c1)−(u2−c2)(cid:1)
|D|→∞max s∈D(S)w(s)
ui(s)
≤L·∥(u1−c1)−(u2−c2)∥ ICi ≤ lim min .
∞ |D|→∞s∈D(S)max s˜i∈D(Si)ui(s−i,s˜i)
≤L·∥u1−u2∥ +L·∥c1−c2∥
∞ ∞
Proof. Let us begin by considering the case in which the
=6Lx.
agents are observed acting together. Given their coopera-
tive capabilities CC we observe outcomes s such that:
Hence, let use denote u := lim u1 = lim u2 and
k→∞ k→∞
similarly for m and c. Then, assuming that m̸=0 (other- w +CC·(w −w )≤w(s)≤w +(w −w ).
ϵ ⋆ 0 ϵ ⋆ 0
wise the proof is complete), we compose limits to see that:
If d has maximal support over the set of such outcomes,
then we have:
u1−c1 u−c u2−c2
lim u1 = lim = = lim = lim u2.
k→∞ ν k→∞ m1 m k→∞ m2 k→∞ ν lim max w(s)=w ϵ+(w ⋆−w 0),
|D|→∞s∈D(S)
To conclude the proof, observe that by Lemma 1 we have lim min w(s)=w +CC·(w −w ).
ϵ ⋆ 0
lim ⪯1= lim ⪯2 and hence (by Proposition 2) |D|→∞s∈D(S)
k→∞ k→∞
that lim CA=1.
k→∞ If lim max w(s) = 0, then we must have
|D|→∞ s∈D(S)
ui(s) = 0 for every 1 ≤ i ≤ n and s ∈ S, in which
Lemma5. Foranyϵ≻0, thereexistsagameGsuchthat case it is meaningless to talk about the agents’ coopera-
w =w but for any x>0, however small, w −w <x. tive capabilities, so let us assume otherwise. Subtracting
0 + ϵ −
w (which, by assumption, is non-negative), cancelling the
ϵ
Proof. Consider the game specified in Figure 5b. Observe terms (w −w ) in these equalities, and composing limits
⋆ 0
that the only NE is given by (A,A), leading to welfare leads to:
min w(s)
w = w = 1. In contrast, note that (B,B) is an ϵ-NE, s∈D(S)
0 + CC≤ lim .
as neither player can deviate to obtain a greater fraction |D|→∞max s∈D(S)w(s)
than(1−ϵi)oftheutilityavailablewhenplayingtheirbest
Similarly, in the case where each agent is observed act-
response (x). Given this, we have: ing alone, then given their individual capabilities ICi we
observe outcomes s such that:
(cid:18) ϵ1+ϵ2(cid:19)
w ϵ−w − < 1− 2 x−0<x. ui •(s−i)+ICi·(cid:0) ui ⋆(s−i)−ui •(s−i)(cid:1) ≤ui(s)≤ui ⋆(s−i),
where we define:
ui(s−i):=minui(s−i,s˜i),
•
s˜i
Proposition 8. Given a game G, if ui(s) for every i and ui(s−i):=maxui(s−i,s˜i).
⋆
s∈S, and d has maximal support over the outcomes gen- s˜i
15If d has maximal support over the set of such outcomes, More generally, let us define di :=m(ui −µw).9 Incor-
w ν
including over all pure partial strategy profiles s−i, then poratingbothprinciples,wehaveaboundintermsofboth
we have: calibration and collective alignment.
ICi ≤ lim min
ui(s)−ui •(s−i) Proposition 9. If r∗ = (cid:80) (cid:80)im mˆi i, then:
|D|→∞s∈D(S)u ui ⋆( is (s− )i)−ui •(s−i)
R(σ)≤ 2K
(cid:88)i
(cid:12) (cid:12)mˆi−r∗mi(cid:12) (cid:12)di ,
≤ lim min n w
|D|→∞s∈D(S)ui ⋆(s−i) i
ui(s) where K is defined as in Proposition 6.
= lim min ,
|D|→∞s∈D(S)max s˜i∈D(Si)ui(s−i,s˜i) Proof. First, consider the term (mˆi −
r∗mi)(cid:0) ui(sˆ )−ui(σ)(cid:1) . The coefficient (mˆi − r∗mi)
ν ⋆ ν
which concludes the proof. is not necessarily positive, but irrespective of its sign, we
have:
Lemma 6. Assuming that a game G has at least three
outcomes, then for utility functions u and u′ with corre- (mˆi−r∗mi)(cid:0) ui(sˆ )−ui(σ)(cid:1)
ν ⋆ ν
s ifpo an nd din og np lyref ie fr te hn ec re eo er xd ie sr tin ags ∈⪯ Rand⪯ an′ dre bsp ∈ect Rive sly u, ch⪯= th⪯ at′ ≤(mˆi−r∗mi)(µ w(sˆ ⋆)−µ w(σ))+2K(cid:12) (cid:12)mˆi−r∗mi(cid:12) (cid:12)di w.
>0
u = au′ +b. Similarly, ⪯=⪰′ if and only if there exist Substituting this inequality into R(σ) gives:
a∈R and b∈R such that u=−au′+b.
>0 R(σ):= 1 (cid:88) (mˆi−r∗mi)(cid:0) ui(sˆ )−ui(σ)(cid:1)
Proof. See, e.g. Proposition 6.B.2 in [31], for a proof of n ν ⋆ ν
i
thefirsthalfofthislemma. Thesecondhalfcanbeproved 1 (cid:88)
≤ (mˆi−r∗mi)(µ (sˆ )−µ (σ))
analogously. n w ⋆ w
i
Lemma 7. Given a game G, there is a pure strategy s ⋆ + 2K (cid:88)(cid:12) (cid:12)mˆi−r∗mi(cid:12) (cid:12)di
such that w ⋆(G)=w(s ⋆). n w
i
Proof. Recall that w (G) := max w(σ). Let σ be such 1 (cid:88)
⋆ σ = (µ (sˆ )−µ (σ)) (mˆi−r∗mi)
that w(σ) = w (G), and consider the set S′ ⊆ S of pure n w ⋆ w
⋆
i
strategy profiles with positive support in σ. Clearly, for
any s,s˜ ∈ S we must have w(σ) = w(s˜). Otherwise, + 2K (cid:88)(cid:12) (cid:12)mˆi−r∗mi(cid:12) (cid:12)di
n w
let s ⋆ ∈ argmax s⋆∈S′. Then w(s ⋆) > w(σ), a contra- i
d wi (c σti )on =. (cid:80)But i σf (w s)( wσ () s)= =w w( (s˜ s)
)
ff oo rr aa nn yy ss ∈,s˜ S′∈
.
CS h, oot sh inen
g
= 2 nK (cid:88)(cid:12) (cid:12)mˆi−r∗mi(cid:12) (cid:12)di w,
s∈S′
i
one such s and denoting it s completes the proof.
⋆
where the last equality follows because:
B.3.1 Bounds on Welfare ‘Fairness’ Remainder (cid:88) (cid:88)
(cid:18)(cid:80) mˆi(cid:19)
(cid:88)
(mˆi−r∗mi)= mˆi− i mi =0,
(cid:80) mi
In Theorem 1 we remarked that R(σ) – a ‘fairness’ re- i i i i
mainder term accounting for collective misalignment and by our choice of r∗.
unequal ri – can be reduced to zero when either CA = 1
or all ri are equal, where recall that: Note that this bound is also equal to zero when either
CA=1orallriareequal. Variousotherrelatedboundson
R(σ):= 1 (cid:88) (mˆi−r∗mi)(cid:0) ui(sˆ )−ui(σ)(cid:1) . R(σ) are possible for different choices of r∗. If we have no
n ν ⋆ ν information about the collective alignment, we can either
i
adapt the preceding bound conservatively, or we can pro-
Here we provide more detail substantiating those claims. duce a similar bound with reference only to the ri terms.
First, making no assumptions about collective alignment, In the latter case, the choice of r∗ is freer, giving rise to a
we can see that when all ri are equal, we must have r∗ = familyofboundingplanes,whoseenvelopeproducesanon-
ri, and therefore R(σ) = 0. Alternatively, we can bound linear(piecewiseaffine)bound. SeeFigure6inSectionC.2
R(σ)byappealingtothecollectivealignmentoftheagents, for an example of this.
which, recall, is given in Definition 10 as:
(cid:88) mi C Additional Experiments
CA(D)=1− ·m(µw−ui),
(cid:80) mj ν
i j
In this section, we provide further details on the experi-
where µw := (cid:80) (cid:80)iui m− ici . Thus, when CA = 1, all ui
ν
are ments we ran, and include additional plots.10
equal to µw, andi so ui(sˆ )−ui(σ) is constant in i. By 9Note that we can express CA = 1− (cid:80) imidi w as a weighted
choosing r∗ = (cid:80) (cid:80)i im mˆi
i
soν th⋆ at largν er and smaller deviations av 1e 0r Oag ue rof ct oh de eset iserm as v.
ailable at
https://gi(cid:80) ti hm ubi
.com/lrhammond/
in ri cancel, we again have R(σ)=0. delegation-games.
16C.1 Empirical Validation
In this section, we provide further empirical validation of
our theoretical results across a range of values of the four
measures. As in the experiment described in Section 6.1,
we hold three of the measures fixed and vary a fourth,
recording how the average principal welfare changes as a
result. In the example in Section 6.1 we set the other
variables to 0.9, and inspected games with approximately (a) (b)
101 outcomes. Here, we set the variables to (the same)
values in0, 0.25, 0.5, 0.75, and1, respectively, andinspect
games with 101, 102, and 103 outcomes, in Figures 10, 11,
and 12, respectively.
For each such setting, we varied the fourth variable lin-
earlybetween0and1andgenerated10samplesperincre-
ment, as follows:
1. RandomlygenerateadelegationgameDsatisfyingthe
(c) (d)
givenmeasuresforindividualandcollectivealignment
(of the the principals). This is done by: Figure 6: 105 simulated games, plotted with principals’
welfare regret, agents’ welfare regret, and total agent mis-
• Sampling an initial vector from a multivariate
alignment. Our bound surface in blue, with individual
Gaussian N(0,1) for each principal 1 ≤ i ≤ n,
game outcomes as a green scatter. Collective alignment
and normalising it to get uˆi;
ν is permitted to vary, while welfare ratios ri are fixed per
• Sampling a magnitude mˆi and constant cˆi from plot as: (a) four players, all equal ri =1; (b) four players,
uniform distributions U(0.5,1.5) and U(−1,1), r1 = 1.4 with ri = 1 otherwise; four players, r1 = 2 with
respectively; ri = 1 otherwise; four players, r1 = 5 with ri = 1 other-
• Adjusting the angle to the vectors uˆi so as to wise.
satisfy the collective alignment measure;
• Sampling a utility function for each agent, using We abstract over the agents’ individual and collective ca-
thesameprocessasbefore,andadjustingthean- pabilities as separate inputs by considering only the agent
gle of the resulting vector ui so as to satisfy the welfare. We sample outcomes with full support as mixed
individual alignment measure. (correlated) strategies producing a given agent welfare re-
gret (so a given agent welfare regret does not here specify
2. Compute the pure NEs and ϵ-NEs.11
whatcapabilitiesormechanismgaverisetoit). Thisleaves
3. Find the pure strategy profiles s such that: the ‘welfare calibration ratios’ ri, which we hold fixed per
visualisation, the collective alignment CA which we per-
w +CC·(w −w )≤w(s)≤w +(w −w ). mittovaryinourgeneratingprocedure, andtheideal and
ϵ ⋆ 0 ϵ ⋆ 0
maximal welfareregret,whicharenotrepresentedinthese
4. Compute the mean principal welfare across all such visualisations.
strategies, recording this along with wˆ , wˆ , wˆ , and Figure 6 shows four plots of this kind, each taking a dif-
+ − ⋆
wˆ . ferentsetoffixedri ratios. Observethatwhentheseratios
•
are uneven, the principals are subject to additional ‘wel-
5. Compute and record the bounds from Theorem 1 and fare miscalibration’ risk, even when agent welfare regret
Proposition 7, which may vary slightly depending on and misalignment are zero, as seen in our bound surface
each mi and mˆi. and as exemplified by the experimental samples. This oc-
curs when a trade-off between agents’ utility registers as a
C.2 Alternative Visualisations gain according to the agents’ welfare, but the same trade-
off between corresponding principals is a bad one.
Another empirical perspective on our theoretical results is
Wealsoincludeavisualisationofthenormalisationpro-
given by 3-dimensional plots of the bounds from Section
cedure described in Definition 6, where recall that:
5.2. Given the number of separate terms in these bounds,
a full-dimensional plot is impossible, but there are a few (cid:40) 0 if m(cid:0) ui−c(ui)1(cid:1) =0
natural ways to combine the variables along three axes. ui ν := ui−c(ui)1 otherwise.
Theseplotsdisplaytheprincipals’welfareregret wˆ −wˆ(σ) m(ui−c(ui)1)
⋆
alongside agents’ welfare regret w −w(σ) and total agent
⋆ If we restrict U to three dimensions, with concrete choices
misalignment,measuredas(cid:80) m(uˆi−ui)=2(cid:80) (1−IAi).
i i of c and m, we can visualise the effect of standardisa-
11Unfortunately,atthetimeofwriting,therearenoscalablegame tion on a collection of random utility functions, shown
solversthatcancomputeallmixedϵ-NEsforarbitraryϵ. in Figure 7. Concretely, we use a uniformly-weighted
17expected value c(u) = 1(cid:80) u(s) and m = ℓ2. Inset
3 A B A B
graphics illustrate an alternative choice of shift function,
c(u)= 1(max u(s)−min u(s)). A 0, 0 1, −1 A -1, 0 0, 0
2 s s 3
B −1, 1 -1, -1 B 1, 1 0, -1
3
D Worked Example
Finally, we can compare the standardised utilities of
Here we show how to compute each of the four measures principals and agents, to find that m(uˆ1 −u1) = 4 and
ν ν 3
in the game from Example 1, which is shown as Figure 1 m(uˆ2 − u2) = 1. Thus, IA1 = 1 and IA2 = 5. This
ν ν 3 3 6
in the main body and repeated here for reference. matches our intuitions, as the first agent’s preferences vis-
ibly deviate from the first principal’s (inverting several
purepreferences),whilethesecondagent’sareaveryclose
A B A B
match for the second principal’s.
Next we consider the collective alignment of the agents
A 3, 3 6, 2 A 2, 3 3, 3
(the calculation for the principals proceeds in the same
B 2, 6 0, 0 B 4, 6 3, 0 way). We have already normalised the utilities. Be-
cause the magnitudes m1 and m2 are equal, the welfare-
(a) (b) (c) representative proxy utility µw is simply the mean of the
agents’ standardised utilities:
Figure 1: (a) The payoffs of the agents in Example 1; (b)
the payoffs of the principals; and (c) a graphical repre- A B
sentation of the full delegation game, with vertical arrows
A 0 1
indicating control and horizontal arrows indicating coop- 3
eration. B 1 -1
3
D.1 Alignment Usingµw weseethatbothm(µw−u1 ν)= 2
3
andm(µw−
u2) = 2. Again, because the magnitudes are equal, the
ν 3
First, how are the agents individually aligned with their weightedaverageofthesedistancesisalso 2, andsoCA=
3
principals? We must standardise the utility functions in 1. Thisagainmatchesourintuition,asnoneofthePareto-
3
order to make comparisons. (See Figure 7 for an represen- optimal outcomes (0,0), (3,−1), and (−1,3) comes close
tative visualisation of the procedure.) For ease of illustra- to the (impossible) ideal welfare outcome (3,3), but the
tion,12 we choose: situation is also clearly not entirely adversarial.
Using the quantities above we can also immediately
max u(s)−min u(s) consider the welfare ratios. As mˆ1 = 1, mˆ2 = 3, and
c(u)= s s and m=ℓ∞.
2 m1 =m2 =3thenwehaver1 =1andr2 = 1. Sincethese
3
are unequal, even had the agents been fully aligned, their
Eachagentandprincipalinthisexamplehasutilitiesrang- welfare optimum would not necessarily coincide with that
ing from 0 to 6 or 2 to 4, meaning that c(u) = 3 in each of the principals.
case. Subtracting this quantity gives:
D.2 Capabilities
A B A B
Since the capabilities are a feature of how the game is
A 0, 0 3, -1 A -1, 0 0, 0 played (rather than the game itself), they are not deter-
mined by the game described in Example 1.
B -1, 3 -3, -3 B 1, 3 0, -3 We note that the agent welfare regret is a function of
capabilities, and we can describe both the principal and
the agent welfare regret of each outcome by a simple com-
Thecentredagents’utilities alsocoincidentallyhavethe parison to the maximal welfare (8 for agents and 10 for
same norm, namely m1 =m2 =3, while the first principal principals):
hasnormmˆ1 =1andthesecondprincipalhasnormmˆ2 =
3.13 Dividing by these norms gives:
A B A B
12ℓ∞ is not a strictly convex norm, so while our upper bound re-
A 2 0 A 5 4
sultsaresatisfiedforthischoice,ouralignmentdesiderataarenotall
satisfied. Weuseℓ∞ forthepurposesofthisworkedexamplesimply
B 0 8 B 0 7
becauseitresultsinclearer,cleanernumbers.
13Wemightinterpretthisnormativelyasthefirstprincipal’sprefer-
encesovertheseoutcomesmatteringless,orastheirmarginalrelative
utilitybeing‘cheap’inthesettingwithtransferableutility. With observations of actual play by the agents we could
18(a) (b) (c)
Figure 7: (a) A random selection of utility functions u (each represented as a point) fill the space, and are naively
incomparable; (b) The affine-equivariant shift u−c(u) projects onto a lower-dimensional surface (here, a disc; inset, a
warpeddisc)whilepreservingpreferences; and(c)normalisingvia u−c(u) preservespreferenceswhiledistinguishingall
m(u−c(u))
possible preferences (here, a circle; inset, a warped circle).
estimate their capabilities (as in Section 6.2). Alterna- E.1 Variations on Alignment Metrics
tively, if we know the agents’ capabilities, we can describe
The alignment measure we introduce has the attractive
or draw from the distribution of possible outcomes.
property that a distance m(u −u′) of zero corresponds
Let us denote by pi the probability of agent i playing ν ν
exactly to identical preference orderings over mixed out-
strategyA. TheagentgameGhasasingledominantstrat-
comes,asdemonstratedintheproofofProposition2. This
egy equilibrium at (A,A), i.e. p1 = p2 = 1, which is the
property relies on the positive definiteness property of the
sole NE. As a result, we have w = 6, which gives agent
0 metric, which is guaranteed by any norm, including the
welfare regret of 2 (and ideal regret of 6) – the result ob- weightedℓ2normusedbyEPIC[20],providedthecoverage
tainedbyagentswithICi =1andCC=0. Ingeneral,the
distributionhassupportonallpossibleoutcomes. Without
agent welfare is given by 8p1+8p2−10p1p2, meaning that
this, the distance can be zero even for differing preference
if either ϵ1 ≤ 1 or ϵ2 ≤ 1 then the pure (A,A) strategy is
5 5 orderings (though preferences over outcomes with support
also the lowest-welfare ϵ-NE.
in the coverage distribution will still be in agreement).
As a concrete example, let us suppose that IC1 = 0.9 Beyond providing this guarantee by having support on
(so ϵ1 = 0.1), IC2 = 0.7 (so ϵ2 = 0.3), and CC = 0.5. allpossibleoutcomes,wemightwishtoguidethisnotionof
Thenthelowest-welfareϵ-NEisthepure(A,A)strategyas distance to give more weight to more ‘important’ or ‘plau-
above, and so w =6 (from Definition 11). But w =6 as sible’ outcomes. Indeed, if using weighted ℓ2 with a guide
ϵ 0
well, so with CC=0.5, the agents achieve welfare at least distribution∆whichispreciselythemixedstrategyprofile
6+0.5·(8−6)=7. I.e.,theagentwelfareregretis1. Solving σ selected by the players, the distance m(u −u′) has a
ν ν
for 8p1 +8p2 −10p1p2 ≥ 7 leads to the set of strategies close relationship with the correlation of players’ payoffs
played in G, which are characterised by 0 ≤ p1 ≤ 1 or under that strategy profile:
2
7 8 ≤p1 ≤1, with p2 = 18 0p p1 1− −7 4. m(u ν −u′ ν)=(cid:112) 2−2ρ σ(u,u′)
Finally, wecanassesswhatthismeansforprincipalwel-
where ρ is the Pearson correlation under the distribution
fare, which is given by 3+3p1 +7p2 −8p1p2. Comput- σ
arising from σ. This is appealing in some sense, but it
ing the maxima and minima of this function in the two
requires invoking a specific strategy profile σ which is in
regions of agent strategy space defined by the inequal-
general undetermined a priori, and it also fails to capture
ities above leads to wˆ(σ) ∈ [15,73] = [7.5,9.125] and
√ 2 8 the notion that players might agree or disagree on prefer-
wˆ(σ) ∈ [11, 3 (53 − 34)] ≈ [5.5,5.660]. As wˆ = 10
2 25 ⋆ ences over alternative, unrealised outcomes. If we expect
this leads to a principal welfare regret between 0.875 and
a particular solution concept to be played, we might limit
4.5.
the coverage distribution to support only those strategies
thatariseunderthatconcept, thoughtheweightingisstill
underdetermined.
E Further Discussion
E.2 Relaxing Our Desiderata
We conclude with some additional discussion of a more As hinted in our discussion of desiderata D1-D5, several
philosophical nature, while drawing connections to the relaxations are possible, which open up more permissive
mathematics underlying our measures and related results definitions of the measures we use to satisfy them. For
in the wider literature. example, recall:
19(a) (b) (c)
Figure 8: (a) ℓ1 norm, a non-strictly convex norm; (b) ℓ2 norm, a strictly convex norm; and (c) ℓ∞ norm, a non-strictly
convex norm – distance is represented as an interrupted line so that the length corresponds to the norm distance.
(D2) Two players are perfectly individually aligned (mis- If we use a general norm m instead of a strictly convex
aligned) if and only if they have identical (opposite) norm, opposite preferences are still guaranteed to corre-
preferences. Two or more players are perfectly collec- spond to maximal misalignment (the same proof suffices),
tively aligned if and only if they have identical prefer- buttheremaybeother utilityfunctionsbesidetheopposite
ences. preferences which nevertheless measure as maximally mis-
aligned. One such non-strictly convex norm is the max or
In Definition 6, we required c to be affine-equivariant, ℓ∞ norm, which corresponds to scaling normalised utility
and m, used for both normalisation and distance, to be functions to a fixed range. Consider utility functions over
a strictly convex norm. three values, represented as a vector u = (u ,u ,u ), and
0 1 2
Weakeninganyoftheserequirementsmeansthatatleast let us normalise to the range [−1,1] using the ℓ∞ norm.
one of our desiderata is not satisfied,14 but some relax- Now (−1,0,1) has opposite preferences to (1,0,−1), and
ations are nonetheless of interest for different purposes. It indeedtheirdistanceismax{2,0,2}=2,whichismaximal
is straightforward to see why positivity and absolute ho- ontheℓ∞-normalisedmanifold. Butthealternativeutility
mogeneity are necessary properties of the normalisation, (1,1,−1) also measures as maximally distant, despite not
to satisfy any of our desiderata, and why the centering exhibiting entirely opposite preferences!
function used to normalise must be equivariant to affine Figure 8 exhibits the difference between a strictly con-
transformations; without any of these properties, equiva- vexnormandsomenon-strictlyconvexnorms. Thedashed
lent preferences expressed by scaled or shifted utility func- contour is the unit circle of the respective norm. The left-
tions would map to different normalised points. It may be mostidentifiedpointisthe‘start’pointineachimage,and
lessclearthatpositivedefiniteness, thetriangleinequality, the colour-mapped distance and solid ‘maximal-distance
or strict convexity is required of the distance, or why the circle’ relates to that point, measured using the respective
distancemetricusedforcalculating(mis)alignmentshould norm. For ℓ1 and ℓ∞, non-strictly convex norms, the blue
be equivalent to the one used for normalisation. point, non-opposite, measures as equally distant from the
startpointasthe‘true’opposite-visually,thiscorresponds
to the ‘maximal distance circle’ coinciding with the ‘unit
E.2.1 The Enemy of my Enemy is my Friend: Op-
circle’ at multiple points. In contrast, for ℓ2, a strictly-
posite Preferences and Strict Convexity
convexnorm,thebluepoint,non-opposite,isstrictlycloser
thanthetrueopposite. Abluetrianglemarkswhereonthe
Ourdesiderataaskthatoppositepreferencescorrespondto
‘maximal-distance circle’ the blue point would have to be
maximal misalignment and vice versa, which makes ‘most
to match the maximal distance: it lies off the standard
misaligned’ an involution – i.e. ‘the enemy of my enemy
unit circle and is thus not the normalisation of any utility
is my friend’, interpreted as a statement about preferences
function.
(aside from any strategic considerations or coalition for-
If we relax things further, by permitting the distance
mation), holds true. Without strict convexity or match-
measure between two normalised utilities to differ from
ing norms for normalisation and distance, this part of our
that used to normalise,15 we still preserve the relation-
alignment desideratum is not in general satisfied.
ship between identical preferences and zero misalignment
14This is not to say that these requirements are necessary for the distance, but there will in general exist non-opposite util-
desiderata, merely that each is necessary given the others. For ex- ityfunctionsthatmeasureaseven more distantthantheir
ample if we ask for the normalisation and the distance to each be a
norm,thentheymustbethesamenormandstrictlyconvex,butthe 15In fact, the distance may be a general metric, and the normal-
desiderata can be satisfied by some functional forms not including isation need not be sub-additive, i.e. it need not satisfy a triangle
normsatall. inequality.
20E.3 Agent Welfare
HerewediscussacloseconnectionbetweenParetoconcepts
ofoptimaandimprovements,andwelfare-specificconcepts
of optima and improvements. This connection unlocks an
alternative perspective on our characterisation of coopera-
tion, and provides a more agnostic foundation than ‘agent
welfare’ while essentially retaining the same analysis.
Inthecaseoftheprincipals,whoseoutcomeswearemost
interestedinanalysing, itshouldnotbesurprisingthatwe
reachforanaggregateutility,i.e. asocialwelfarefunction,
withaverageutilitarianwelfareinparticularreceivingmost
ofourattentioninthispaper. Whenweimagineprincipals
(a)
being humans, this welfare bears some correspondence to
thecolloquialsenseof‘welfare’,andevenifwedonothave
preciseaccessto‘utility’nortothe‘right’waytoaggregate
it,thisoperationalisationofwelfareisagoodstartingplace
for analysis, though the finer points remain debateworthy.
For agents, on the other hand, especially when those
agents are artificial systems, the referents of ‘utility’ and
‘welfare’ in the colloquial and the philosophical sense may
beabsententirely, orattheveryleastinaccessibleandun-
known. We rescue a technical analysis of ‘agent utility’,
up to affine scaling, by appealing to well-known results on
the preferences embodied by the agents [31, 43, 47]. In
the case when agents are also moral patients (for example,
(b)
other humans), we may wish to analyse both utility and
Figure 9: (a) ℓ∞ distance with ℓ2 normalisation – distance welfare on the basis of some understanding of true wellbe-
is represented as an interrupted line so that the length ing. But when wellbeing is meaningless or unknown, and
correspondstothenormdistance;and(b)ℓ2 distancewith our utility functions thus represent embodied preferences
ℓ∞ normalisation. withnoguaranteeofcommensurability,itappearsdifficult
at first to sensibly produce meaningful aggregates. Must
we taboo ‘agent welfare’?
Recall that our purposes for agent welfare in particular
aretocaptureaspectsofcooperation: collectivealignment
opposites. Consider first a normalisation with ℓ∞ but a and capabilities. We consider cooperative success to con-
distance using ℓ2. Now take u=(−1,0,1 √) and notice that sist of mutual gains (perhaps compared with some non-
its opposite, (1,0,−1) is at ℓ2 distance 2 2, but (1,1,−1) cooperative baseline), i.e. Pareto improvements. Pareto
is ‘further’ away in ℓ2 (distance 3), despite having less in- improvements and Pareto optima are a much more agnos-
version of preference! Alternatively consider a normali- ticembodimentofcooperation,butaswehavealreadyseen
sation w √ith √ℓ2, and distance using √ℓ∞. Notice that sim- in Proposition 3, for positively-weighted welfares, welfare
ilarly, (cid:0) 2, 2,0(cid:1) has ℓ∞ distance 2 from its opposite, optima are Pareto optima. Various theorists have drawn
√ √ 2 2
(cid:0) 2, 2,0(cid:1) , while, for example, (−1,0,0) is more distant converseresultstoo: everyweak Paretooptimumistheop-
2 2
in ℓ∞. timumofsome(non-negative)welfare[53]. [3]anddescen-
dantworks–e.g. [14]and[8]–findothercloseconnections
Figure9exhibitstwocombinationsofinconsistentnorms betweenParetooptimaandutilitarianwelfareoptima. Our
for normalisation and for measuring the distance between initial alignment result (Proposition 3) thus characterises
normalisedpoints. Ineachcase,theleftmostblackpointis the Pareto frontiers of two fully-aligned disjoint sets of
the‘startingpoint’,anditstrueoppositeistheotherblack players as being identical – a statement that needs make
point. The blue point, non-opposite, is actually counted no mention of welfare. What the welfare weightings pro-
as ‘more misaligned’ under such combinations of inconsis- vide is an ‘orientation’ (either descriptive or prescriptive)
tent norms! Nevertheless, both of these relaxations are inthespaceofutilitytrade-offs,whichidentifiesparticular
able to preserve the other alignment desiderata, namely optima from the Pareto frontier.
thatidenticalpreferencescorrespondto0distanceandvice Bringingthesepointstogether,whatcanwedowhenwe
versa. Although our overall desiderata regarding opposite have a relatively clear way to analyse or estimate the wel-
preferences require these properties, our regret bounds are fare of principals, and we want to make richer claims than
still fulfilled by measures with somewhat weaker proper- mere Pareto concepts for them, but not for agents? We
ties, analogous to EPIC [20], or the EPIC-like distances of might attempt to ensure that agents’ utilities are scaled
[40]. equivalently to their principals’, which is to force all ‘wel-
21fare calibration’ ratios to ri = 1, but we need not be
this prescriptive. The aforementioned theorems on welfare
and Pareto concepts mean that for cooperative agent be-
havioursordispositions(construedasweight-agnosticmu-
tualgains,i.e. Paretoimprovements)thereissomereason-
able description in terms of welfare weighting with which
to quantify it.
We leave unspecified how specific trade-offs might en-
ter into agents’ collective behaviour or dispositions, but
providesomeexamples. Consider,forinstance, explicitin-
structions from their principals (whether or not they are
fair), or exchange rates between different goods, broadly
construed, or marginal production returns from given re-
sources, which can give rise to natural orientations in the
space of utility trade-offs (again, whether or not they are
fair). Our discussion in terms of ratios and the welfare re-
gret bounds then allow us to bring in concepts of welfare
as desired in order to make more specific statements like
those we prove in the main text.
22Figure 10: Each of these plots was generated from games with approximately 10 outcomes. The key for this figure is
identical to the related figure in the main body. Each row corresponds to setting (all of) the fixed variables to one of the
five values v ∈{0,0.25,0.5,0.75,1}, respectively.
23Figure 11: Each of these plots was generated from games with approximately 102 outcomes. The key for this figure is
identical to the related figure in the main body. Each row corresponds to setting (all of) the fixed variables to one of the
five values v ∈{0,0.25,0.5,0.75,1}, respectively.
24Figure 12: Each of these plots was generated from games with approximately 103 outcomes. The key for this figure is
identical to the related figure in the main body. Each row corresponds to setting (all of) the fixed variables to one of the
five values v ∈{0,0.25,0.5,0.75,1}, respectively.
25