Human-AI Co-Creation of Worked Examples for
Programming Classes
MohammadHassany1, PeterBrusilovsky1, JiazeKe2, KamilAkhuseyinoglu1 and
ArunBalajieeLekshmiNarayanan1
1UniversityofPittsburgh,Pittsburgh,PA,15260
2CarnegieMellonUniversity,Pittsburgh,PA,15213
Abstract
Workedexamples(solutionstotypicalprogrammingproblemspresentedasasourcecodeinacertain
languageandareusedtoexplainthetopicsfromaprogrammingclass)areamongthemostpopulartypes
oflearningcontentinprogrammingclasses.Mostapproachesandtoolsforpresentingtheseexamplesto
studentsarebasedonline-by-lineexplanationsoftheexamplecode.However,instructorsrarelyhave
timetoprovideline-by-lineexplanationsforalargenumberofexamplestypicallyusedinaprogramming
class. Inthispaper,weexploreandassessahuman-AIcollaborationapproachtoauthoringworked
examplesforJavaprogramming.WeintroduceanauthoringsystemforcreatingJavaworkedexamples
thatgeneratesastartingversionofcodeexplanationsandpresentsittotheinstructortoeditifnecessary.
Wealsopresentastudythatassessesthequalityofexplanationscreatedwiththisapproach.
Keywords
CodeExamples,AuthoringTool,Human-AICollaboration
1. Introduction
Program code examples play a crucial role in learning how to program [1]. Instructors use
examplesextensivelytodemonstratethesemanticsoftheprogramminglanguagebeingtaught
andtohighlightthefundamentalcodingpatterns. Programmingtextbooksalsopayalotof
attentiontoexamples,withaconsiderabletextbookspaceallocatedtoprogramexamplesand
associatedcomments[2,3]. Atypicalworkedexamplepresentsacodeforsolvingaspecific
programming problem and explains the role and function of code lines or code chunks. In
textbooks,theseexplanationsareusuallypresentedascommentsinthecodeorasexplanations
onthemargins. Whileinformative,thisapproachfocusedonpassivelearning,whichisknown
foritslowefficiency. Recognizingthisproblem,severalresearchteamsdevelopedlearningtools
thatofferedmoreinteractiveandengagingwaystolearnfromexamples[4,5,6,7,8].
Theexample-focusedlearningtoolsdemonstratedtheireffectivenessinclassroomstudies,but
theirusebyprogramminginstructorsisstilllimitedduetotheinsufficientnumberofworked
JointProceedingsoftheACMIUIWorkshops2024,March18-21,2024,Greenville,SouthCarolina,USA
$moh70@pitt.edu(M.Hassany);peterb@pitt.edu(P.Brusilovsky);jiazek@andrew.cmu.edu(J.Ke);
kaa108@pitt.edu(K.Akhuseyinoglu);arl122@pitt.edu(A.B.L.Narayanan)
(cid:128)https://github.com/mhassany-pitt/(M.Hassany);https://sites.pitt.edu/~peterb/(P.Brusilovsky)
(cid:26)0009-0004-8893-8454(M.Hassany);0000-0002-1902-1464(P.Brusilovsky);0009-0003-3122-2298(J.Ke);
0000-0002-7761-9755(K.Akhuseyinoglu);0000-0002-7735-5008(A.B.L.Narayanan)
©2024Copyrightforthispaperbyitsauthors.UsepermittedunderCreativeCommonsLicenseAttribution4.0International(CCBY4.0).
CEURWorkshopProceedings(CEUR-WS.org)
CWPrEooUrckR esehdoinpgshISttSpN:// c1e6u1r3-w-0s0.o7r3g
1
4202
beF
62
]CH.sc[
1v53261.2042:viXraMohammadHassanyetal.CEURWorkshopProceedings 1–12
examplesofferedbythesetools. Althoughtheauthorsofthesetoolsusuallyprovideagood
setofworkedexamplesthatcanbepresentedthroughtheirtools,manyinstructorspreferto
usetheirownfavoritecodeexamples. Theinstructorsareusuallyhappytobroadlysharethe
codeofexamplestheycreated(usuallyprovidingitonthecoursewebpage),buttheyrarely
havetimeorpatiencetoaugmentexampleswithexplanationsandaddtheirexamplestoan
example-focusedinteractivesystem. Indeed,producingasingleexplainedexamplecouldtake
30minutesormore,sinceitrequirestypinganexplanationforeachcodeline[4,8]orcreating
ascreencastinaspecificformat[5,7].
Thisissuehasbeenrecognizedbyseveralresearchteamsthathaveofferedseveralwaystoad-
dressthelackofcontent. Amongtheapproachesexploredarelearner-sourcing,thatis,engaging
studentsincreatingandreviewingexplanationsforinstructor-providedcode[9]andautomatic
extractionofinformationcontentfromavailablesources,suchaslecturerecordings[6]. Inthis
paper,wepresentanalternativeapproachtoaddressthelackofworkedexamplesbasedon
human-AIcollaboration. Withthisapproach,theinstructorprovidesthecodeofoneoftheir
favoriteexamplesalongwiththestatementoftheprogrammingproblemitissolving. TheAI
enginebasedonlargelanguagemodels(LLM)examinesthecodeandgeneratesexplanationsfor
eachcodeline. Theexplanationscouldbereviewedandeditedbytheinstructor. Tosupportand
explorethisauthoringapproach,wecreatedanauthoringsystem,whichradicallydecreasesthe
timetocreateanewinteractiveworkedexample. Theexamplescreatedbythesystemcouldbe
uploadedtoanexample-explorationsystemsuchasWebEx[4]orPCEX[8]orexportedina
reusableformat. Toassessthequalityoftheresultingexamples,weperformedauserstudyin
whichTAsandstudentscomparedcodeexplanationscreatedbyexpertsthroughatraditional
processwithexamplescreatedbyAItocontributetohuman-AIcollaborativeprocess.
Theremainderofthepaperisstructuredasfollowing. Westartbyreviewingrelatedwork,
introducetheexampleauthoringsystemthatimplementstheproposedcollaborativeapproach,
andexplainhowspecificdesigndecisionsweremadethroughseveralroundsofinternalevalua-
tion. Next,weexplainthedesignofouruserstudyandreviewitsresults. Weconcludewitha
summaryoftheworkandplansforfutureresearch.
2. Related Work
2.1. WorkedExamplesinProgramming
Codeexamplesareimportantpedagogicaltoolsforlearningprogramming. Notsurprisingly,
considerableeffortshavebeendevotedtothedevelopmentoflearningmaterialsandtoolsto
support students in studying code examples. For many years, the state-of-the-art approach
for presenting worked code examples in online tools was simply code text with comments
[1,10,11]. Morerecently,thisapproachhasbeenenhancedwithmultimediabyaddingaudio
narrationstoexplainthecode[12]orbyshowingvideofragmentsofcodescreencastswith
theinstructor’snarrationbeingheardwhilewatchingcodeinslidesoraneditorwindow[5,6].
Bothways,however,supportpassivelearning,whichistheleastefficientapproachfromthe
2MohammadHassanyetal.CEURWorkshopProceedings 1–12
prospectoftheICAPframework[13]1
Anattempttomakelearningfromprogramconstructionexamplesactivewasmadeinthe
WebExsystem, whichallowedstudentstointeractivelyexploreinstructor-providedline-by-
line comments for program examples via a web-based interface [4]. More recently, several
projects[6,7,8]augmentedexampleswithsimpleproblemsandotherconstructiveactivities
to elevate the example study process to the interactive and constructive levels of the ICAP
framework,knownasthemostpedagogicallyefficient.
AgoodexampleofamoderninteractivetoolforstudyingcodeexamplesisthePCEXsys-
tem [8]. PCEX (Program Construction EXamples) was created in the context of an NSF In-
frastructure project (https://cssplice.org) with a focus on broad reuse and has been used by
severaluniversitiesintheUSandEuropeinthecontextofJava,Python,andSQLcourses. PCEX
interface(Figure1)providesinteractiveaccesstotraditionallyorganizedworkedexamples,i.e.,
codelinesaugmentedwithinstructor’sexplanations. Separatingexplanations(Figure1-3)from
thecode(Figure1-2),allowsstudentstoselectivelystudyexplanationsforcodelinestheywant.
Explanationsareprovidedonseverallevelsofdetail,somoredetailscouldberequestedifthe
briefexplanationisnotsufficient(Figure1-3).
Sinceline-by-linemulti-levelexampleexplanationsofferedbyPCEXiscurrentlythemost
detailed approach for explaining worked examples, we selected the code example structure
implementedbyPCEXasthetargetmodelforourauthoringtoolpresentedinthispaper. The
toolproducescodeaugmentedwithline-by-lineexplanationsonseverallevelsofdetail. The
resultingexamplecouldbedirectlyuploadedtoPCEXorexportedinasystem-independent
formattobeuploadedtootherexampleexplorationsystemslikeWebEx[4].
2.2. UseofLLMsforCodeExplanations
SeveralresearchteamsexploredtheuseofLLMforcodeexplanationsusingGPT-3[14,15,16],
GPT-3.5[15,17,18],GPT-4[17],OpenAICodex[19,20,15],andGitHubCopilot[18].LLMswere
usedtogenerateexplanationsatdifferentlevelsofabstraction(line-by-line,step-by-step,and
high-levelsummary).Sarsaetal.[19]observedthatChatGPTcangeneratebetterexplanationsat
low-level(lines). ExplanationsandsummariesgeneratedbytheseLLMsweremostlyevaluated
byauthors[19],students[15,16],andtoolusers[18]. Sarsaetal.[19]reportedahighcorrect
ratioforgeneratedexplanationswithminormistakesthatcanberesolvedbytheinstructor
orteachingassistant. StudentsratedLLM-generatedexplanationsasbeinguseful,easier,and
moreaccuratethanlearner–sourcedexplanations[16].
Generatingcode-explanationsusingLLMsrequiresawell-formedprompt. Averboseprompt
willlimittheLLM’sabilitytoutilizeitsknowledge[20]. Iterativepromptsareproventoperform
well [14]. A temptation to allow non-expert form input prompts will not be any good, as
Zamfirescu-Pereiraandcolleagues[14]observednon-expertshavemisconceptionsaboutLLMs
andwillstruggletocomeupwithawell-formedprompt. ResearchersbelievethatLLMscan
bebeneficialinenvironmentswherehumansandAIcanworktogether,wherethehumancan
performtheexpertevaluationandtunetheresponsesgeneratedbytheAIwhiletheAIperforms
thetime-consumingmanualtasks[21].
1TheICAPframeworkdifferentiatesfourmodesofengagement,behavioriallyexhibitedbylearners:passive,active,
constructiveandinteractive.
3MohammadHassanyetal.CEURWorkshopProceedings 1–12
Figure1:StudyingacodeexampleinthePCEXsystem:1)titleandprogramdescription,2)program
sourcecodewithlinesannotatedwithexplanations,3)explanationsforthehighlightedline,4)linktoa
“challenge”-asmallproblemrelatedtotheexample.
3. The Feasibility Studies
To assess the feasibility of Human-AI co-creation of worked examples, we performed three
roundsofpreliminarystudies. Thepurposeofthesestudieswastodevelopanapproachfor
producingLLMcodeexplanationsofreasonablequality,comparetheexplanationsproducedby
LLMswiththeexplanationsproducedbyhumans,andassesswhethertheLLMexplanations
areconsideredsatisfactorybyinstructorsandstudents.
In the first study [22] guided by earlier work on LLM code explanations reviewed above,
weexploredarangeofpromptsandperformedanevaluationofthequalityofexplanations
generatedbythepromptstoselectthebest-performingpromptforthenextroundsofourwork.
Inthesecondstudy[23],weusedadatasetofexplanationsproducedbytwoexpertsand60
studentsforthesamefourJavacodeexampleswith33explainablelinestocompareChatGPT
explanationswithexplanationsproducedbyexpertsandstudentsusingseveralformalmetrics.
Tomakethiscomparison,wegeneratedChatGPTexplanationsusingourselectedpromptfor
the33explainablelinesfourtimes,usingtemperature0onceandtemperature1threetimes.
Tocalculateallcomparisonmetrics,wemergedalllineexplanationsgeneratedbyeachsource
(i.e, each expert, each student, and each round of ChatGPT generation) into a single source
document. Asthedatashows(Table1),theexplanationsproducedbyChatGPThavecomparable
length(measuredbythenumberoftokens)andlexicaldensitywiththeexplanationsproduced
byexperts,whiletheexplanationsproducedbystudentsweremorethantwiceasshortand
morelexicallydensethantheexplanationsproducedbytheothertwosources. Surprisingly
4MohammadHassanyetal.CEURWorkshopProceedings 1–12
(giventhelengthdifference)thereadabilityofexplanationsproducedbyexpertsisverysimilar
tothereadabilityofstudentexplanations,whileChatGPTexplanationsaremuchlessreadable.
ExpertexplanationsarealsomuchmoresimilarthanChatGPTexplanationstotheexplanations
producedbystudents(Table2). Thisdatacouldbepartiallyexplainedbytheconsiderablylarger
vocabularyusedbyChatGPTevenincomparisontoexperts.
Source N Vocabulary LexicalDensity #ofTokens GF FRE FK
Experts 2 209.0 0.48 690.0 8.46 78.45 6.18
ChatGPT* 4 238.0 0.49 769.5 11.09 69.64 7.83
Students 60 116.5 0.54 249.5 8.02 80.48 5.62
Table1
Medianlexicalandreadabilitymetricsfordifferentsourcesofexplanations(FRE=Flesch-ReadingEase,
FK=Flesch-Kincaid,GF=GunningFog).*referstothepromptselectedinthefirststudy.
Reference Source chrF METEOR USE BERTScore
Expert Student 0.33 0.144 0.33 0.63
ChatGPT Student 0.18 0.151 0.255 0.458
Expert ChatGPT 0.32 0.28 0.48 0.712
Table2
Assessinglexicalandsemanticalignment(largerisbetter)betweensourcesofexplanations.
Inthethirdstudy[22],weconductedacomparativeevaluationofexplanationsproducedby
expertsandChatGPTfromthepointofviewofhumanusers. Weusedtwotypesofhuman
users: authors(instructorsandTAs)whoareexpectedtouseChatGPT-generatedexplanations
asthestartingpointintheco-creationprocess,andstudentswhoarethetargetusersofthe
co-createdproduct. Explanationswerecomparedinpairs,eachexplanationinapairhastobe
judgedbycompleteness,andthebestexplanationinthepairhastobeselected. Apairincluded
anexpertandaChatGPTexplanation,andthejudgeswerenotawarewhichsourceproduced
eachexplanation. ThestudyresultsindicatedstrongpreferencesforChatGPTinbothgroupsof
judges(Table3). Ingeneral,ChatGPTexplanationswereratedasmorecompleteandjudged
tobebetterinthemajorityofcases. However,itwasnotaclearwin. Inasubstantialnumber
ofcases(15.05%forstudentsand27.41%forauthors),expertexplanationswereselectedasthe
bestoptioninapair.
Takentheresultsofthesetwostudiestogether,wecouldconcludethatproducingexplanations
forcodeexamplesisapromisingapplicationareaforHuman-AIco-creation. Ontheonehand,
the LLM-generated explanations are lagging behind expert explanations in several aspects.
ChatGPTexplanationshavehigherreadingdifficultythanexpertexplanations,andtheyare
further away from the students’ own explanations, as measured by most similarity metrics.
ThevocabularydatahintsthatChatGPTtendstouseterms,whichmightnotbeeasyforthe
studentstounderstand,whileexpertshaveexperienceinphrasingtheirexplanationscloserto
thestudents’activevocabulary. Ontheotherhand,theexplanationsproducedbyChatGPTwere
generallyratedhigherthantheexpertexplanationsbybothinstructorsandstudents. These
data hint that presenting ChatGPT explanations directly to students might not be a perfect
solution,buttheycanserveasanexcellentstartingpointforinstructorsinshapingtheirown
5MohammadHassanyetal.CEURWorkshopProceedings 1–12
Source Judgedby Notcomplete Complete Verycomplete “Thissourceisbetter”
ChatGPT Students 0.00% 13.33% 86.67% 51.11%
ChatGPT Authors 1.48% 32.59% 65.93% 58.15%
Experts Students 2.22% 55.56% 42.22% 16.05%*
Experts Authors 14.07% 57.78% 28.15% 27.41%*
Table3
AssessmentofexplanationsgeneratedbyChatGPTandexpertsbystudentsandauthors.Forconvenience,
wedonotcountthecasesinwhichtheexplanationsinapairwerejudgedequallygood.
explanations. Followingthat,wedecidedtostructuretheHuman-AIcollaborationincreating
workingexamplesasfollows. Instructorshavetheultimatecontroloverproducingexplanations.
Depending on the context (such as example complexity), they can either choose to explain
examplelinesthemselvesorrequestAI(LLM)helpinproducingexplanationsforspecificlines.
Inthelattercase,LLMgeneratestheinitiallineexplanationsleavingittotheinstructortoaccept
orrejectitand,ifaccepted,tofurtheredittheexplanationtexttosatisfaction. TheHuman-AI
co-creationinterfacepresentedinthenextsectionisbasedonthismodelofcollaboration.
4. The Human-AI Co-Creation Interface Design
Onthebasisofourfeasibilitystudies,wedevelopedaWorkedExampleAuthoringTool(WEAT).
WEATenablesinstructorstocreateworkedcodeexamplesforPCEXsystem,[8]throughthe
human-AIco-creationinterface. Inthisco-creationprocess,themaintaskofahumanauthor
istoprovidethecodeoftheexampleandthestatementoftheproblemthatthecodesolves.
ThemaintaskofChatGPTistogeneratethebulkofcodelineexplanationsonseverallevelsof
detail. Asanoption,ahumanauthorcouldeditandrefinethetextproducedbyChatGPTto
adaptittotheclassgoalsandtargetstudents. Asinanyproductivecollaboration,eachside
doeswhatitisbestsuitedtodo,leavingtheresttothepartner.
InthemainpartoftheWEATinterfacetheproblem(Figure2-1)andthecode(Figure2-2)
have to be provided by the instructor, while the explanations for each line (Figure 2-3) can
becreatedbytheinstructororgeneratedbyChatGPT.Thegeneratedexplanationscouldbe
furthereditedbytheinstructor. Whileweexpectthatco-creationofcodeexplanationswillbe
thepreferredwaytouseWEAT,thesystemsupportthewholerangeofoptionsfromusingAI
explanationswithouthumaneditingtocreatingthewholeexamplefromscratch,withoutthe
helpofAI.Authorswhowanttostartbycreatingexplanationsthemselvescouldsimplyselecta
codelinetoexplain(Figure2-2)andaddoneormoreexplanationfragmentstothisline(Figure
2-3). Theorderofthefragmentsisimportant: thefirstfragmentisdisplayedinPCEXwhen
thelineisclicked,whiletheremainingfragmentscanbeaccessedbyclickingthe“Additional
Details”button(Figure1-3).
TogenerateChatGPTexplanationsfortheprovidedexamplecodeandproblemdescription,
theauthorhastoclickthe“GenerateExplanations”buttontoopentheChatGPTdialog(Figure
3).Inthisdialog,theexplanationscouldbegeneratedbyclicking“Generate”buttonandaddedto
theexamplebyclicking“UseExplanations”button.Experiencedauthorshavetheopportunityto
6MohammadHassanyetal.CEURWorkshopProceedings 1–12
Figure 2: WEAT Authoring, 1) program title and description, 2) program source code (lines with
explanationsaremarkedwithabluequestionmarknexttothelinenumber),3)explanationsforthe
selectedline(thelinewithgraybackground-line6inthescreenshot).
tunethedefaultpromptbeforegeneratingexplanationsandreviewthegeneratedexplanations
beforeusingthem. Reviewingthegeneratedexplanationscanbedonelinebyline: selecting
oneoftheexplainedlines(markedby“?")inthecodebox(Figure3-3)willdisplayallgenerated
explanationsforthislineintheexplanationbox(Figure3-4). Theexplanationcouldbeaccepted
orrejectedbyclickingthecheckboxnexttothe“Includethisline”prompt.
Tosupportthereviewatthefinergrainlevel,WEATdividestheexplanationsintofragments
thatcanbeindependentlyacceptedorrejectedbyclickingthesmallgreencheckmarkiconnext
tothefragment(Figure3-4a). Theauthorcanalsoclickonthesmallgraythumb-upicon(Figure
3-4b)toprovidepositivefeedbackontheexplanationfragment. Oncethe“UseExplanations”
buttonisclicked,allacceptedexplanationfragmentsareaddedtothecorrespondingexample
linesandcanbefurthereditedinthemaininterface(Figure2).
5. Evaluation
ToassesshowwellWEATsupportsco-creationofworkedexamples,weengagedfiveinstructors
(A1-A5)teachingJavaofPythonclassesandaskedthemtocreateoneormoreworkedexamples
forPCEXfromrealexamplestheyuseintheirclasses. Toexplainthetooltotheinstructor,we
providedavideotutorialandintegratedtextualhelpintoWEAT.Theirinteractionsandusage
ofthetoolwererecordedthroughlogsandusedfortheanalysispresentedbelow.
Theinstructorsusedthetooltocreate12examplesintotal(Table4). TheChatGPTdialog
was used 21 times, and in 13 cases (A1=6, A2=2, A3=3, A4=1, and A5=1), instructors added
generatedexplanationstotheexamplebyclickingthe“UseExplanations”button. Asdiscovered
from an interview with instructors, in several cases they closed and reopened the ChatGPT
7MohammadHassanyetal.CEURWorkshopProceedings 1–12
Figure3: Human-AICollaborativeWorkedExampleAuthoring,1)“GenerateExplanations”button,
2)defaultprompt(authorcantunetheprompt-optional),3)programsourcepreview,4)generated
explanationsfortheselectedline.
dialogtoaccessthemaininterfaceblockedbythedialog. Analyzingtheinteractionlogs,we
observedthishasbeendoneatleast5times(3timeswiththeclose-reopenintervalof5seconds
and2with12secondsinterval)leavingonly16caseswhereexplanationshadachancetobe
examined. In total, 269 explanation fragments were generated for 119 lines of code with an
average of 2.26 fragments per line. In 13 cases where ChatGPT explanations were added to
theexamplebyinstructors,ChatGPTgenerated237explanationsfor99linesofcode(Table4).
Wefoundnocasesinwhichtheentiresetofexplanationsgeneratedforthelinewasexcluded
by the instructorsin its entirety, and amongthe 237 generatedfragments, only 24 (10. 12%)
237wereexcluded. Theinterviewrevealedthatinsomecasesthegeneratedfragmentswere
rejectednotbecausetheywereunsatisfactory,butbecausetheywereincorrect(Figure4). On
theotherhand,instructorsliked15(6.32%)explanations.
Afteraddingexplanationstotheexample,instructorstilldidn’tremovedtheexplanationsfor
anylineentirely,butremoved23(9.7%)ChatGPTgeneratedexplanationfragments. Instructor
A5reportedthatheremovedanumberoffragmentswhenmergingtwoormoreexplanation
fragmentstogether. Sincethetooldidnotprovidesupportformergingfragments,itdidsoby
copying the explanation from one fragment to the end of the other fragment and removing
the obsolete fragment. In only 10 cases, instructors attempted to create new explanations
fromscratch,butintheendtheseexplanationswereremoved. Inotherwords,allremaining
8MohammadHassanyetal.CEURWorkshopProceedings 1–12
A1 A2 A3 A4 A5 Total
ExamplesCreated 6 2 2 1 1 12
GeneratedExplanations 126 32 44 8 27 237
LinesofCodebeingExplainedbyChatGPT 55 12 21 2 9 99
ExplanationsExcluded 18 2 4 0 0 24
ExplanationsLiked 6 0 9 0 0 15
ExplanationsEdited 29 0 11 0 26 66
ExplanationsRemoved 8 0 15 0 0 23
Table4
AnalysisofChatGPTusedexplanations:Totalcountofgenerated,excluded,liked,andexplainedlines
ofcodeacross13instanceswheretheinstructoraddedexplanationstoexamples.
explanationfragmentswereoriginallygeneratedbyChatGPTwithsomeofthembeingedited
laterbytheinstructors. Apparently,theinstructorspreferredtoedittheexplanationfragments
ratherthancreatingthemfromscratch. Intotal,theinstructorsedited66(27.84%)ofChatGPT
generatedexplanationfragments,onaverage1.4times(stdev=0.55). Feedbackfrominstructors
indicatedthatmostoftheireditsinvolvedsummarizing,addingmissingdetails,orremoving
unnecessaryparts. Table4showsthatalmosthalfofthegeneratedfragmentswereusedwithout
beingtouched,savinganoticeableamountofinstructortime.
A1 A2 A3 A4 A5 Total
ChatGPTEditedExplanations 29 0 11 0 26 66
ChatGPTExplanationEdits 42 0 12 0 39 93
AverageLevenshteinRatioacrossall Average
FinalandOriginalChatGPTExplanations 0.435 1 0.833 1 0.412 0.736
Table5
ChatGPT-generatedexplanationsedits:NumberofeditsmadebyinstructorstoChatGPT-generated
explanations,alongwithameasureofsimilaritybetweentheoriginalandfinaleditedversion.
TheaverageLevenshteineditratioforChatGPT-generatedexplanations(editedandunedited)
is0.73(Table5),indicatingahighacceptancerateforgeneratedexplanations. Thisindicator,
however, is somewhat misleading since a portion of ChatGPT-generated explanations were
editedbecausethefirstversionofthetoolevaluatedinthestudydidn’tprovidedirectsupport
forreorderingandmergingtheexplanations, resultingincopy-pastingtheexplanations(as
reportedbyA5forwhomtheratiodroppedto0.412). ThetablealsopointsoutthatWEATwas
abletosupportdifferenteditingapproachespursuedbyinstructors. Someinstructorsspent
moretimereviewingthegeneratedexplanationsbeforeaddingthemtotheexample(A1),some
preferaddingthemtotheexamplesandthenevaluatingandeditingthem(A3), whilesome
usedthegeneratedexplanationswithoutchanges.
6. Conclusion
Inthispaper,weintroduceaworkedcodeexampleauthoringtoolWEATthatsupportshuman-
AIco-creationintheprocessofdevelopingsuchexamples. WEATsupportshumanauthors
9MohammadHassanyetal.CEURWorkshopProceedings 1–12
Figure4:AnincorrectexplanationfragmentgeneratedbyChatGPTandexcludedbytheauthor(line3).
byusingChatGPTforthegenerationofline-by-linecodeexplanationsandbyprovidingan
interfacetointegratethisfunctionalityintoabalancedauthoringprocess. Tothebestofour
knowledge,thisisthefirstattempttoproduceworkedexamplethroughhuman-AIcollaboration.
TodevelopWEAT,weperformedseveralroundsoffeasibilitystudies.Thesestudiessupported
theneedforahuman-AIco-creationinauthoringworkedexamples. Asthestudiesshowed,in
themajorityofcases,theexplanationsgeneratedbyChatGPTwithacarefullytunedprompt
werepositivelyevaluatedbyauthorsandstudents. However,inagoodfractionofcasesthey
wereinferiortotheexplanationsprovidedbyexperts. Thestudyalsorevealedthatonaverage
expertscancreateexplanationsthataremoreeasilyreadableandclosertotheexplanations
generated by the students themselves. With this data, we hypothesized that human-AI co-
creationcouldofferthe“bestofbothworlds”solutionwheregoodexplanationscouldbesimply
acceptedbyauthors,whileinferiororhard-to-understandexplanationscouldbeimproved.
AnevaluationofWEATsystemwithfivecourseinstructorssupportedtheseexpectations
and provided strong evidence in favor of co-creation. As the log analysis demonstrated, in
manycases,instructorschoosetoacceptgeneratedexplanationswithoutchanges,whichshould
have decreased the time and efforts required for example creation. Yet in other cases, the
instructorrejectedoreditedthegeneratedexplanationtoachievethedesiredquality. Insome
cases,explanationswererejectedbybeingsimplyincorrect,whichstressestheimportanceof
humanpresenceintheauthoringprocess. Theinterviewwithauthorsrevealedseveralcases
whereauthorsactedinefficientlyduetospecificinterfaceissues,suchasblockingthemainedit
windowbythegenerationdialogorthelackoftoolstomoveormergefragments. Nowweare
usingtheseobservationstodevelopanimprovedversionofWEAT.
Asthefirststeptowardsthisimportantgoal,ourworkhaslimitations. Mostimportantly,
the scale of our evaluation is relatively small. Since we targeted real instructors as users in
ourevaluationprocess,wewereabletorecruitonlyfivequalifiedsubjects. Additionally,since
thestudywasdoneatthebeginningofthesemesterwheninstructorswerebusysettingup
theirclasses,theycreatedonly12examplesusingthistool. Toobtainmorereliabledata,we
planalarger-scalesemester-longstudybyengaginginstructorstocreateavarietyofworked
examplesofvaryingdifficultyandusethemintheirclasses. Suchastudywillalsoenableusto
assessthequalityofexplanationsproducedthroughhuman-AIcollaborationandtheirvaluefor
studentsinintroductoryprogrammingclasses.
10MohammadHassanyetal.CEURWorkshopProceedings 1–12
References
[1] M.C.Linn,M.J.Clancy, Thecaseforcasestudiesofprogrammingproblems, Commun.
ACM35(1992)121–132.
[2] H.M.Deitel,P.J.Deitel,CHowtoProgram,2ndEdition,PrenticeHall,NewYork,1994.
[3] A.Kelley,I.Pohl,CbyDissection: TheEssentialsofCProgramming,Addison-Wesley,
NewYork,1995.
[4] P.Brusilovsky,M.V.Yudelson,I.-H.Hsiao, Problemsolvingexamplesasfirstclassobjects
in educational digital libraries: Three obstacles to overcome, Journal of Educational
MultimediaandHypermedia18(2009)267–288.
[5] R. Sharrock, E. Hamonic, M. Hiron, S. Carlier, Codecast: An innovative technology to
facilitate teaching and learning computer programming in a c language online course,
ProceedingsoftheFourth(2017)ACMConferenceonLearning@Scale(2017).
[6] K.Khandwala,P.J.Guo, Codemotion: expandingthedesignspaceoflearnerinteractions
with computer programming tutorial videos, Proceedings of the Fifth Annual ACM
ConferenceonLearningatScale(2018).
[7] J. Park, Y. H. Park, J. Kim, J. Cha, S. Kim, A. H. Oh, Elicast: embedding interactive
exercisesininstructionalprogrammingscreencasts, ProceedingsoftheFifthAnnualACM
ConferenceonLearningatScale(2018).
[8] R. Hosseini, K. Akhuseyinoglu, P. Brusilovsky, L. Malmi, K. Pollari-Malmi, C. Schunn,
T.Sirkiä, Improvingengagementinprogramconstructionexamplesforlearningpython
programming, InternationalJournalofArtificialIntelligenceinEducation30(2020)299–
336.
[9] I.-H. Hsiao, P. Brusilovsky, The role of community feedback in the student example
authoringprocess: anevaluationofannotex, BritishJournalofEducationalTechnology
42(2011)482–499.
[10] A. Davidovic, J. R. Warren, E. Trichina, Learning benefits of structural example-based
adaptivetutoringsystems, IEEETrans.Educ.46(2003)241–251.
[11] B. B. Morrison, L. E. Margulieux, B. Ericson, M. Guzdial, Subgoals help students solve
parsonsproblems, Proceedingsofthe47thACMTechnicalSymposiumonComputing
ScienceEducation(2016).
[12] B.Ericson,M.Guzdial,B.B.Morrison, Analysisofinteractivefeaturesdesignedtoenhance
learninginanebook, ProceedingsoftheeleventhannualInternationalConferenceon
InternationalComputingEducationResearch(2015).
[13] M.T.H.Chi,J.Adams,E.B.Bogusch,C.Bruchok,S.Kang,M.Lancaster,R.Levy,N.Li,
K.L.McEldoon, G.S.Stump, R.Wylie, D.Xu, D.L.Yaghmourian, Translatingtheicap
theoryofcognitiveengagementintopractice, CognitiveScience42(2018)1777–1832.
[14] J. Zamfirescu-Pereira, R. Y. Wong, B. Hartmann, Q. Yang, Why johnny can’t prompt:
Hownon-aiexpertstry(andfail)todesignllmprompts, in: Proceedingsofthe2023CHI
ConferenceonHumanFactorsinComputingSystems,CHI’23,AssociationforComputing
Machinery,NewYork,NY,USA,2023.
[15] S.MacNeil,A.Tran,A.Hellas,J.Kim,S.Sarsa,P.Denny,S.Bernstein,J.Leinonen, Ex-
periences from using code explanations generated by large language models in a web
softwaredevelopmente-book, in: Proceedingsofthe54thACMTechnicalSymposiumon
11MohammadHassanyetal.CEURWorkshopProceedings 1–12
ComputerScienceEducationV.1,SIGCSE2023,AssociationforComputingMachinery,
NewYork,NY,USA,2023,p.931–937.
[16] J.Leinonen,P.Denny,S.MacNeil,S.Sarsa,S.Bernstein,J.Kim,A.Tran,A.Hellas,Com-
paringcodeexplanationscreatedbystudentsandlargelanguagemodels,2023.
[17] J.Li,S.Tworkowski,Y.Wu,R.Mooney,Explainingcompetitive-levelprogrammingsolu-
tionsusingllms,2023.
[18] E. Chen, R. Huang, H.-S. Chen, Y.-H. Tseng, L.-Y. Li, Gptutor: A chatgpt-powered pro-
grammingtoolforcodeexplanation, in: N.Wang,G.Rebolledo-Mendez,V.Dimitrova,
N. Matsuda, O. C. Santos (Eds.), Artificial Intelligence in Education. Posters and Late
BreakingResults,WorkshopsandTutorials,IndustryandInnovationTracks,Practitioners,
DoctoralConsortiumandBlueSky,SpringerNatureSwitzerland,Cham,2023,pp.321–327.
[19] S.Sarsa,P.Denny,A.Hellas,J.Leinonen, Automaticgenerationofprogrammingexercises
andcodeexplanationsusinglargelanguagemodels, in: Proceedingsofthe2022ACMCon-
ferenceonInternationalComputingEducationResearch-Volume1,ICER’22,Association
forComputingMachinery,NewYork,NY,USA,2022,p.27–43.
[20] H. Tian, W. Lu, T. O. Li, X. Tang, S.-C. Cheung, J. Klein, T. F. Bissyandé, Is chatgpt the
ultimateprogrammingassistant–howfarisit?,2023.
[21] J.White,Q.Fu,S.Hays,M.Sandborn,C.Olea,H.Gilbert,A.Elnashar,J.Spencer-Smith,
D. C. Schmidt, A prompt pattern catalog to enhance prompt engineering with chatgpt,
2023.
[22] M. Hassany, P. Brusilovsky, J. Ke, K. Akhuseyinoglu, A. B. Lekshmi Narayanan, Au-
thoringWorkedExamplesforJavaProgrammingwithHuman-AICollaboration,Report
arXiv:2312.02105,arXiv,2023.URL:https://doi.org/10.48550/arXiv.2312.02105.
[23] A.-B.Lekshmi-Narayanan,P.Oli,J.Chapagain,M.Hassany,R.Banjade,P.Brusilovsky,
V.Rus, Explainingcodeexamplesinintroductoryprogrammingcourses: Llmvshumans,
in: WorkshoponAIforEducation-BridgingInnovationandResponsibilityatAAAI2024„
2024.
12