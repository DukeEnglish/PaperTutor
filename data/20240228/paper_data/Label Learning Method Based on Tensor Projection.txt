IEEETRANSACTIONS 1
Label Learning Method Based on Tensor
Projection
Jing Li, Quanxue Gao, Qianqian Wang, Cheng Deng, and Deyan Xie
Abstract—Multi-viewclusteringmethodbasedonanchorgraphhasbeenwidelyconcernedduetoitshighefficiencyandeffectiveness.
Inordertoavoidpost-processing,mostoftheexistinganchorgraph-basedmethodslearnbipartitegraphswithconnectedcomponents.
However,suchmethodshavehighrequirementsonparameters,andinsomecasesitmaynotbepossibletoobtainbipartitegraphswith
clearconnectedcomponents.Toendthis,weproposealabellearningmethodbasedontensorprojection(LLMTP).Specifically,we
projectanchorgraphintothelabelspacethroughanorthogonalprojectionmatrixtoobtainclusterlabelsdirectly.Consideringthatthe
spatialstructureinformationofmulti-viewdatamaybeignoredtoacertainextentwhenprojectedindifferentviewsseparately,weextend
thematrixprojectiontransformationtotensorprojection,sothatthespatialstructureinformationbetweenviewscanbefullyutilized.In
addition,weintroducethetensorSchattenp-normregularizationtomaketheclusteringlabelmatricesofdifferentviewsasconsistentas
possible.Extensiveexperimentshaveprovedtheeffectivenessoftheproposedmethod.
IndexTerms—Multi-Viewclustering,Anchorgraph,TensorSchattenp-norm,Tensorprojection.
✦
1 INTRODUCTION
AS one of the common techniques of data mining, and anchors (called anchor graph). The anchor graph is
clusteringcanbeusedtodiscovertheinternalstructure n×m. Generally speaking, m ≪ n, so the anchor graph-
and organization of data and divide them into different basedmulti-viewclusteringmethodscanhandlelarge-scale
meaningful clusters. With the wide application of various multi-view data well. In addition to being able to handle
sensorsandothertechnologies,thedescriptionofthesame large-scale data, anchor graph-based methods also inherit
objectismoreandmorepronetodiversificationandisomer- the advantages of graph-based methods represented by
ization.Forexample,aneventcanbedescribedbyatext,a spectralclustering,i.e.,theyarenotaffectedbythegeometric
picture,avoiceandavideo;Apicturecanbedescribedwith distributionofdata.Thismethodusuallyfirstexploresthe
differentfeatures,suchasGIST,CMT,HOG,etc.Asectionof geometrical structure of multi-view data by constructing
pathinformationinautomaticdrivingcanberepresentedby anchorgraph,andthenclustersontheanchorgraphusing
liDARpointclouddata,depthcameradata,andinfrareddata, existingclusteringtechnics.However,thesecondstepisstill
etc.Datalikethiscanbecalledmulti-viewdata.Multi-view time-consuming.
clustering(MVC)istheoperationofclusteringmulti-view To end this, another method using anchor graph clus-
data. tering is proposed. Theses methods use anchor graphs to
As an effective data mining method, multi-view clus- obtain bipartite graphs, and then learns bipartite graphs
teringhasbeenwidelyconcerned[1]–[7].Oneofthemost withK connectedcomponents(whereK isthenumberof
representativemethodsisthegraph-basedmulti-viewclus- classes),sothattheclusteringresultscanbeobtaineddirectly
tering method [8]–[13]. These method involve similarity withoutpost-processing.However,thesemethodsoflearning
graphsconstructandeigen-decomposeofLaplacianmatrices, bipartitegraphshavehighparameterrequirementsandmay
and the computational complexity is O(n2) and O(n3), notfindK connectedcomponentsinsomecases.
respectively,wherenrepresentsthenumberofsamples.With In order to avoid the above problems and avoid post-
theadventoftheeraofbigdata,dataacquisitionisbecoming processing,weconsidertheuseofprojectionmatrixtoproject
easier,resultinginthecontinuousexpansionofthescaleof theanchorgraphdirectlyintothelabelspace,i.e.,then×m
datasets.Sograph-basedmulti-viewclusteringmethodshave anchorgraphisregardedasfeaturematriceswithnsamples
beensomewhatdifficulttodealwithlarge-scalemulti-view andmfeaturedimensions,andthen×cclusterlabelmatrix
data.Onthisbasis,anchorgraph-basedmulti-viewclustering canbedirectlyobtainedaftertheprojectiontransformation.
methodshavebeenproposedandwidelyused. Generallyspeaking,theaboveprocessrequirestheprojection
The core idea of the anchor graph-based method is to transformationoftheanchorgraphoneachview,andthen
select m representative points (called anchors) from the n theclusteringlabelmatrixofeachviewisobtained,finally
samples,andthenlearntherelationshipbetweenthesamples these matrices are fused to obtain the clustering results of
multi-viewdata.However,projectivetransformationineach
viewseparatelymaycausethespatialstructureinformation
• Thisworkwassupportedinpartbyxxxxx.(Correspondingauthor:Q. betweendifferentviewsnotfullyutilized.
Gao.)
Toendthis,weproposeamulti-viewdatalabellearning
• J.Li,Q.Gao,Q.WangarewiththeStateKeylaboratoryofIntegrated
Services Networks, Xidian University, Xi’an 710071, China (e-mail: methodbasedontensorprojection(LLMTP).Inordertoget
qxgao@xidian.edu.cn). theclusterlabelsdirectlyfromtheanchorgraph,weconsider
ManuscriptreceivedXXXX;revisedXXXX;acceptedXXXX. projectingtheanchorgraphintothelabelspace,sothatthe
4202
beF
62
]GL.sc[
1v44561.2042:viXraIEEETRANSACTIONS 2
clusterresultscanbeobtaineddirectly.Consideringthatthe the utilization of self-supervised weighting. Moreover, it
projectionofeachviewseparatelywillcausethemodeltobe employs K connected components to symbolize clusters.
unabletomakegooduseofthecomplementaryinformation LCBG[16]takestheintra-viewandinter-viewspatiallow-
and spatial structure information between different views, rankstructuresofthelearnedbipartitegraphsintoaccount
weextendthematrixprojectiontransformationoftheanchor by minimizing tensor Schatten p-norm and nuclear norm.
graphtothetensorprojectiontransformationoftheanchor MSC-BG [17] leverages the Schatten p-norm to investigate
graphtensor,i.e.,projectthethird-ordertensordirectly.So thesynergisticinformationacrossdiverseviews,andderiv-
that the spatial structure information embedded between ingclustersthroughKconnectingcomponents.TBGL[18]
different views can be preserved to a large extent. Thus, employstheSchattenp-normtodelveintotheresemblances
better clustering performance can be obtained. Extensive among different views, while simultaneously integrating
experiments have proved the superiority of our proposed ℓ -normminimizationregularizationandconnectivitycon-
1,2
model. straintstoinvestigatethesimilaritieswithineachview.
Insummary,themaincontributionsofthispaperareas Butthesemethodsoflearningbipartitegraphshavehigh
follows: parameter requirements and may not find K connected
componentsinsomecases.
• It is proposed to project the anchor graph into the
label space to obtain the clustering label directly.
Meanwhile,thematrixprojectionisextendedtotensor 2.2 Multi-ViewClusteringMethodBasedonProjection
projection so that complementary information and
In the process of multi-view clustering, in most cases, we
spatial structure information between views can be
directlyprocesstheoriginaldataortheconstructedsimilarity
fullymined.
graph, but there may be redundant information, noise or
• We propose an algorithm to optimize the tensor outlierinformationintheoriginaldataorsimilaritygraph.
projectiontransformation,andverifytheconvergence
Theywilladverselyaffectthefinalclusteringperformance.
ofthisalgorithmfromtheexperimentalpointofview.
Multi-view clustering methods based on projection are
• We introduce the tensor Schatten p-norm to exploit mostlystudiedtosolvesuchproblems.
complementary information across different views,
Thegeneralpracticeofthiskindofmethodistoconstruct
facilitating the derivation of a common consensus
an orthogonal projection matrix to projective the original
labelmatrix.
data, and then get a relatively clean representation matrix
intheembeddedspace.Gaoetal.[19]proposeanewmulti-
2 RELATED WORKS view clustering framework that combines dimensionality
reduction,manifoldstructurelearningandfeatureselection,
2.1 AnchorGraph-BasedMulti-ViewClusteringMethods
which maps high-dimensional data to low-dimensional
Sincethecomputationalcomplexityofn×nsimilaritygraph spacestoreducedatacomplexityandreducetheeffectsof
is O(Vn2) during graph construction, and O(n3) during datanoiseandredundancy.Wangetal.[20]proposedarobust
eigendecompositionofLaplacematrix(whereV andnare self-weighted multi-view projection clustering (RSwMPC)
thenumberofviewsandsamples,respectively),itisdifficult basedonℓ -norm.Itcansimultaneouslyreducedimension,
2,1
forgraph-basedmulti-viewclusteringmethodstodealwith suppressnoiseandlearnlocalstructuregraph.Theresulting
large-scale multi-view data. By using m anchors to cover optimal graph can be directly used for clustering without
n sample point clouds, and constructing the relationship anyotherprocessing.Sangetal.[21]proposedaconsensus
between n samples and m anchors, the anchor graph is graph-basedauto-weightedmulti-viewprojectionclustering
obtained.Becausem≪n,themulti-viewclusteringmethod (CGAMPC).Itcansimultaneouslyreducedimension,save
basedonanchorgraphcandealwithlarge-scalemulti-view manifold structure and learn consensus structure graph.
dataeffectively. The information similarity graph is constructed on the
In general, anchor graph-based methods explore the projecteddatatoensuretheremovalofredundantandnoisy
geometryofmulti-viewdatabyconstructinganchorgraphs, information in the original similarity graph, and the ℓ -
2,1
andthenitrequiresadditionalclusteringtechniquessuchas normisusedtoselectadaptivediscriminantfeatures.Wang
K-meanstoclustertheobtainedanchorgraph.However,the etal.[22]proposedconsistencyanddiversitypreservingwith
additionalclusteringtechniquesistime-consuming. projectiondecompositionformulti-viewclustering(CDP2D).
To avoid post-processing, it is common to use anchor It automatically learns the shared projection matrix and
graphs to learn bipartite graphs with K connected compo- analyzesmulti-viewdatathroughprojectionmatrixdecom-
nents(whereKisthenumberofclusters).Therefore,post- position. Li et al. [23] propose a projection-based coupled
processing can be avoided and clustering results can be tensorlearningmethod(PCTL).Itconstructsanorthogonal
obtained directly from bipartite graphs with K connected projectionmatrixtoobtainthemainfeatureinformationof
components. The representative methods are as follows. the raw data of each view, and learns the representation
MVGL[14]gainsinsightsfromvarioussingleviewgraphs matrixinacleanembeddedspace.Moreover,tensorlearning
to construct a global graph. Instead of relying on post- is used to coupling projection matrix and representation
processingtechniques,itutilizesKconnectedcomponents matrix, mining higher-order information between views,
to extract clustering metrics. However, its scalability for and constructing more suitable and better representation
handlinglarge-scaledatamightbelimited.SFMC[15]intro- ofembeddedspace.
ducesaparameter-freeapproachtofusemulti-viewcluster Inspired by the above method, we consider whether
graphs, resulting in cohesive composite graphs through the n × m anchor graph can be directly regarded as anIEEETRANSACTIONS 3
featurematrixwithnsamplesandmdimensionalfeatures, withoutpost-processing.Whendealingwithmulti-viewdata
accordingto[24].Thenthefeaturematrixisdirectlyprojected wehavethefollowingformula:
intothelabelspacethroughprojectionchanges,andthefinal
clusteringlabelisobtaineddirectly. (cid:88)V (cid:13) (cid:13)2 (cid:88)V
min (cid:13)S(v)G(v)−H(v)(cid:13) +λ R(H(v))
(cid:13) (cid:13)
F (2)
v=1 v=1
3 NOTATIONS s.t. H(v) ⩾0,H(v)T H(v) =I,G(v)T G(v) =I,
We will cover t-product and the definition of the tensor
Schattenp-norminthissection. where, S(v) is the anchor graph from the v-th view, the
anchorsselectionmethodandtheanchorgraphconstruction
Definition 1 (t-product [25]).SupposeA ∈ Rn1×m×n3 and methodarethesameasthatof[18].G(v) istheorthogonal
B ∈ Rm×n2×n3, the t-product A ∗ B ∈ Rn1×n2×n3 is projectionmatrix.H(v) istheclusterlabelmatrix.Itcanbe
givenby
moreinterpretablebyconstrainingittobenon-negativeand
A∗B=ifft(bdiag(AB),[],3), orthogonal, i.e., each row of H(v) has only one non-zero
value,andthepositionofthevalueindicatestheclusterto
whereA=bdiag(A)anditdenotestheblockdiagonal which the corresponding element of the row belongs. The
matrix.TheblocksofAarefrontalslicesofA. purposeoftheregulartermwithcoefficientofλistomake
the clustering label matrices of different views tend to be
Definition 2. [26]GivenH∈Rn1×n2×n3,h=min(n 1,n 2),
consistent.
thetensorSchattenp-normofHisdefinedas
∥H∥
⃝Sp
=(cid:18) i(cid:80)n =3 1(cid:13) (cid:13) (cid:13)H(i)(cid:13) (cid:13) (cid:13)p ⃝Sp(cid:19) p1 =(cid:32) i(cid:80)n =3 1j(cid:80) =h 1σ j(cid:16) H(i)(cid:17)p(cid:33) p1 , M Projection
(1) K
where, 0 < p ⩽ 1, σ j(H(i) ) denotes the j-th singular
(i)
valueofH .
N
N
It should be pointed out that for 0 < p ⩽ 1 when p is
appropriately chosen, the Schatten p-norm provides quite
effective improvements for a tighter approximation of the 𝐇(𝟏)
𝐇(𝟐)
rankfunction[27],[28].
Also we introduce the notations used throughout this 𝐇(𝒗)
paper.Weuseboldcalligraphylettersfor3rd-ordertensors,
H ∈ Rn1×n2×n3, bold upper case letters for matrices, H, Anchor Graph Space Label Space
boldlowercaselettersforvectors,h,andlowercaseletters
suchash
ijk
fortheentriesofH.Moreover,thei-thfrontal
sliceofHisH(i) .HisthediscreteFouriertransform(DFT) Fig.1.Anchorgraphspacetolabelspace
of H along the third dimension, H = fft(H,[ ],3). Thus,
H=ifft(H,[],3).ThetraceandtransposeofmatrixHare ConsideringthattheprojectiontransformationinEq.(2)
expressedastr(H)andHT.TheF-normofHisdenotedby is carried out separately in each view, the complementary
∥H∥ . informationandspatialstructureinformationembeddedin
F
different views may not be fully utilized. To end this, we
consider extending the two-dimensional matrix projection
4 METHODOLOGY
into a third-order tensor projection. Eq. (2) is extended as
4.1 MotivationandObjectiveFunction follows:
Multi-view clustering method based on anchor graph can V
min∥S∗G−H∥2 +λ(cid:88) R(H(v))
dealwithlarge-scalemulti-viewdataefficiently.Bylearning F
(3)
a bipartite graph with K connected components using v=1
anchor graphs, the cluster labels can be obtained directly s.t. H⩾0,HT∗H=I,GT∗G =I
withoutanypost-processing.However,thismethodhashigh
In multi-view clustering, we should try our best to
requirementsonparameters,andinsomecasesitmaynot
make the H(v) of different views in (2) tend to be the
be able to obtain a clear bipartite graph with K connected
same. Inspired by the excellent performance of the tensor
components.
Schatten p-norm [17], [18], [26], [29], we fully explore the
Inspiredbytheexcellentperformanceoftheorthogonal
complementary information in label matrices of different
projection matrix in processing redundant information in
viewsbyintroducingtheregulartermofthetensorSchatten
therawdatamatrix,andreferringto[24],weconsiderthe
n×manchorgraphasanfeaturematrixcomposedofnm-
p-norm.
dimensionalfeaturevectorsandthen×cclusterlabelmatrix Thefinalobjectivefunctionisasfollows:
can be obtained directly by projecting the feature matrix
min∥S∗G−H∥2 +λ∥H∥p
(i.e.anchor graph) into the label space (as shown in Figure F ⃝Sp
(4)
1).Inthisway,theclusteringresultscanbeobtaineddirectly s.t. H⩾0,HT∗H=I,GT∗G =IIEEETRANSACTIONS 4
where0 < p ⩽ 1,λisthehyper-parameteroftheSchatten (8)isequivalentto:
p-norm term. The construction process of the tensor H is
max
tr(cid:16) (G(v) )TW(v) G(v)(cid:17) +2tr(cid:16) (G(v) )TW(v)(cid:17)
,
showninFigure2.Remark1brieflydescribestheroleofthe 1 2
tensorSchattenp-norm.
(G(v))TG(v)=I
(9)
Remark1(ExplanationofthetensorSchattenp-norm).For whereW(v) =βI−(S(v) )TS(v) andW(v) =(S(v) )TH(v)
,
the tensor H, as depicted in Fig 2, its k-th lateral slice 1 2 (v)
Θk representstherelationshipofnsampleswiththek-th where β is an arbitrary constant to ensure that W 1 is a
positivedefinitematrix.
clusteracrossdifferentviews.Multi-viewclusteringaims
Tosolve(9),weintroducethefollowingTheorem:
toharmonizethesample-clusterrelationshipsindifferent
views,makingH(1),··· ,H(v)
ascongruentaspossible.
Theorem1. [30]Forthemodel:
:,k :,k
However,theclusteringstructuresoftenvarysignificantly max tr(GTBG)+2tr(GTK) (10)
acrossviews.ApplyingthetensorSchattenp-normtoH GTG=I
ensuresthatΦk maintainsaspatiallylow-rankstructure, GissolvediterativelyandG∗ = UVT,whereU,V is
leveragingcomplementaryinformationacrossviewsand fromtheSVDdecomposition:UXVT =BG+K.
fosteringconsistencyintheclusteringlabels.
(v)
AccordingtoTheorem1,theG canbesolvediteratively
andthesolutionis:
Tensor 𝓗
V G∗(v) =U(v) (V(v) )T, (11)
K K
whereU(v) X(v) (V(v) )T =W(v) G(v) +W(v) .
1 2
•SolveHwithfixedQ,G,J.(5)becomes:
N 𝐓𝐞𝐧𝐬𝐨𝐫𝐂𝐨𝐧𝐬𝐭𝐫𝐮𝐜𝐭𝐢𝐨𝐧
N min ∥S∗G−H∥2 + µ(cid:13) (cid:13) (cid:13)H−Q+ Y 1(cid:13) (cid:13) (cid:13)2
𝐇(𝟏) 𝐇(𝟐)
𝐇(𝒗)
𝚯(𝒌)
HT∗H=I F
+
ρ2 (cid:13)
(cid:13)
(cid:13)(cid:13)
H−J +
Yµ 2(cid:13)
(cid:13)
(cid:13)(cid:13) 2F
,
(12)
𝐇(𝟏) 𝐇(𝟐) 𝐇(𝒗) 2(cid:13) ρ (cid:13) F
(12) is equivalent to the following in the frequency
domain:
Fig.2.Tensorconstruction
min
(cid:88)V (cid:13) (cid:13)S(v) G(v) −H(v)(cid:13) (cid:13)2
(cid:13) (cid:13)
(H(v))TH(v)=Iv=1 F
4 In.2 spirO edpti bm yiz Aa uti go mn ented Lagrange Multiplier (ALM), we +(cid:88)V µ 2(cid:13) (cid:13) (cid:13) (cid:13)H(v) −Q(v) + Y µ( 1v)(cid:13) (cid:13) (cid:13) (cid:13)2 (13)
introducetwoauxiliaryvariablesQandJ andletH=Q, v=1 (cid:13) (cid:13) F
H mo= delJ as, r thes ep fe oc lt loiv wel iy n, gw uh ne cr oe nsQ tra⩾ ine0 d. T ph roen bl, ew me
:
rewrite the +(cid:88)V ρ(cid:13) (cid:13) (cid:13)H(v) −J(v)
+
Y( 2v)(cid:13) (cid:13) (cid:13)2
,
2(cid:13) ρ (cid:13)
(cid:13) (cid:13)
min∥S∗G−H∥2 +λ∥J∥p v=1 F
F ⃝Sp
whereH=fft(H,[],3),andtheothersinthesameway.
+ µ(cid:13) (cid:13) (cid:13)H−Q+ Y 1(cid:13) (cid:13) (cid:13)2 + ρ(cid:13) (cid:13) (cid:13)H−J + Y 2(cid:13) (cid:13) (cid:13)2 (5) (13)canbereducedto:
2(cid:13) µ (cid:13) 2(cid:13) ρ (cid:13)
s.t.
Q⩾0,HT∗H=I,F
GT∗G =I
F
min
−2tr(cid:16) (H(v) )TS(v) G(v)(cid:17) −µtr(cid:16) (H(v) )TW( 3v)(cid:17)
(H(v))TH(v)=I
whereY 1,Y 2representLagrangemultipliersandµ,ρarethe −ρtr(cid:16) (H(v) )TW(v)(cid:17)
penaltyparameters.Theoptimizationprocesscantherefore 4
(14)
beseparatedintofoursteps:
•SolveG withfixedQ,H,J.(5)becomes: whereW( 3v) =Q(v) − Y µ( 1v) andW( 4v) =J(v) − Y ρ( 2v) .
(14)canbereducedto:
min ∥S∗G−H∥2,
GT∗G=I F (6) max tr(cid:16) (H(v) )TA(v)(cid:17) (15)
(6)isequivalenttothefollowinginthefrequencydomain: (H(v))TH(v)=I
min
(cid:88)V (cid:13) (cid:13)S(v) G(v) −H(v)(cid:13) (cid:13)2
, (7)
whereA(v) =2S(v) G(v) +µW( 3v) +ρW( 4v) .
(cid:13) (cid:13) Tosolve(15),weintroducethefollowingTheorem:
(G(v))TG(v)=Iv=1 F
Theorem 2.GivenGandP,whereG(G)T = IandPhas
whereG =fft(G,[],3),andtheothersinthesameway. thesingularvaluedecompositionP=ΛS(V)T,thenthe
(7)canbereducedto: optimalsolutionof
min
tr(cid:16) (G(v) )T(S(v) )TS(v) G(v)(cid:17)
max tr(GP) (16)
(G(v))TG(v)=I
(8)
G(G)T=I
−2tr(cid:16) (G(v) )T(S(v) )TH(v)(cid:17)
,
isG∗ =V[I,0](Λ)T.IEEETRANSACTIONS 5
Proof1.FromtheSVDP=ΛS(V)T andtogetherwith(10), Algorithm 1 Label Learning Method Based on Tensor
itisevidentthat Projection(LLMTP)
tr(GP)=tr(GΛS(V)T) Require: Datamatrices{X(v)}V ∈RN×dv;anchorsnum-
v=1
=tr(S(V)TGΛ)
bersm;clusternumberK.
Ensure: ClusterlabelsYofeachdatapoints.
=tr(SH) (17) 1: Initialize:µ=10−5,ρ=10−5,η =1.5,Y 1 =0,Y 2 =0,
(cid:88) (v)
= s iih ii, Q isidentitymatrix;
i 2: ComputegraphmatrixS(v) ofeachviews;
whereH = (V)TGΛ,s
ii
andh
ii
arethe(i,i)elements 3: whilenotco (n v)ditiondo
of S and H, respectively. It can be easily verified that 4: UpdateG bysolving(11);
(v)
H(H)T = I, where I is an identity matrix. Therefore 5: UpdateH bysolving(22);
−1⩽h
ii
⩽1ands
ii
⩾0,Thuswehave:
6:
UpdateQ(v)
bysolving(19);
tr(GP)=(cid:88) s iih ii ⩽(cid:88) s ii. (18) 7: UpdateJ byusing(24);
i i 8: Update Y 1, Y 2, µ and ρ: Y 1 = Y 1 + µ(H − Q),
Y
2
= Y
2
+ µ(H − J), µ = min(ηµ,1013), ρ =
TheequalityholdswhenHisanidentitymatrix.tr(GP) min(ηρ,1013);
reachesthemaximumwhenH=[I,0].
9: endwhile
AccordingtoTheorem2thesolutionof(15)is: 10: CalculatetheK clustersbyusing
H=(cid:80)V H(v)/V;
H(v) =Λ(v) (V(v) )T (19) 11: return v C= lu1 stering result (The position of the largest
(v) (v) (v) elementineachrowoftheindicatormatrixisthelabel
where Λ and V can be obtained by SVD A =
ofthecorrespondingsample).
Λ(v) X(V(v) )T
•SolveQwithfixedH,G,J.(5)becomes:
TABLE1
min =
µ(cid:13) (cid:13)
(cid:13)H−Q+
Y 1(cid:13) (cid:13) (cid:13)2
, (20)
Multi-viewdatasetsusedinourexperiments
Q⩾0 2(cid:13) µ (cid:13)
F
#Dataset #Samples #View #Class #Feature
(20)isobviouslyequivalentto:
MSRC 210 5 7 24,576,512,256,254
min µ(cid:13) (cid:13) (cid:13)(H+ Y 1)−Q(cid:13) (cid:13) (cid:13)2 (21) Han MdW nir si tt 4ten4 42 00 00 00 4 3 1 40 7 36 0, ,2 91 ,6 3, 047,6
Q⩾0 2(cid:13) µ (cid:13) Scene15 4485 3 15 1800,1180,1240
F Reuters 18758 5 6 21531,24892,34251,15506,11547
Accordingto[31],thesolutionof(21)is:
(cid:18) Y (cid:19)
Q= H+ 1 (22) 5 EXPERIMENTS
µ
+
Inthissection,wedemonstratetheperformanceofourpro-
•SolveJ withfixedH,G,Q.(5)becomes:
posedmethodthroughextensiveexperiments.Weevaluate
min=λ∥J∥p ⃝Sp + ρ 2(cid:13) (cid:13) (cid:13) (cid:13)H−J + Y ρ2(cid:13) (cid:13) (cid:13) (cid:13)2 , (23) 1th )e Ac Clu Cs ;te 2r )i Nng Mp I;er 3f )o Prm ura in tyc .e Tb hy e 3 higm he et rri tc hs eu vs ae ld uew ti hd eel by e, ti t. ee. r,
F the clustering results for all metrics mentioned above. To
Wecandeduce ensure reliability, we conducted 10 independent trials for
J∗ =argmin1 2(cid:13) (cid:13) (cid:13) (cid:13)H+ Y ρ2 −J(cid:13) (cid:13) (cid:13) (cid:13)2 + λ ρ∥J∥p ⃝Sp, (24) e Ea xc ph em rimet eh no td s, ure sc inor gd ti hng e Mthe SRm Ce ,a Hn aan nd dWva rr itia ten nc 4e ,o Mft nh ie str 4e ,s au nlt ds.
F
Scene15datasetswereconductedonalaptopequippedwith
whichhasaclosed-formsolutionasLemma1[26]: an Intel Core i5-8300H CPU and 16 GB RAM, utilizing
Lemma1.LetZ ∈Rn1×n2×n3 haveat-SVDZ =U∗S∗VT, Matlab R2018b. In contrast, the Reuters and NoisyMnist
thentheoptimalsolutionfor wereprocessedonastandardWindows10server,featuring
m Xin 21∥X −Z∥2 F +τ∥X∥p ⃝Sp . (25) d Gu Ba RlI An Mtel ,( wR) itX he Mon A( TR L) AG Bol Rd 26 02 23 00 a.CPUsat2.10GHzand128
Thedatasetsemployedinourexperimentsaredetailedin
isX∗ = Γ τ(Z) = U ∗ifft(P τ(Z))∗VT,whereP τ(Z)is
Table1.Wecompareourapproachagainstthefollowingstate-
anf-diagonal3rd-ordertensor,whosediagonalelements
of-the-art methods: CSMSC [32], GMC [33], ETLMSC [34],
canbefoundbyusingtheGSTalgorithmintroducedin
LMVSC[35],FMCNOF[31],SFMC[15],andFPMVS-CAG
[26].
[36].
Thesolutionof(24)is:
J∗ =Γ λ ρ(H+ Y ρ2). (26) T5 a.1 bleC 2lu as nt deri Tn ag blP ee 3rfo sr hm owanc the
e clustering performance of
Finally, the optimization procedure for Label Learning the proposed model on different datasets. The optimal
MethodBasedonTensorProjection(LLMTP)isoutlinedin performance is indicated by bold, and the sub-optimal
Algorithm1. performanceisindicatedbyunderline.Itcanbeseenfrom指标 ACC NMI Purity 指标 ACC NMI
0.1 0.98225 0.933267 0.98225 0.1 0.8 0.797155
0.2 0.98225 0.939078 0.98225 0.2 0.780952 0.803499
0.3 0.97025 0.907059 0.97025 0.3 0.847619 0.799247
0.4 0.9675 0.896753 0.9675 0.4 0.819048 0.781354
0.5 0.96525 0.901904 0.96525 0.5 0.904762 0.844011
0.6 0.96675 0.904645 0.96675 0.6 0.87619 0.829117
0.7 0.97 0.912215 0.97 0.7 0.961905 0.92943
0.8 0.9705 0.914459 0.9705 0.8 0.857143 0.762216
0.9 0.972 0.912557 0.972 0.9 0.985714 0.970882
1 0.956 0.882517 0.956 1 0.709524 0.655446
IEEETRANSACTIONS 6
the three tables that the method proposed in this paper is
1
superiortoothercomparisonmethods.Amongcomparison 0.9
methods, ETLMSC’s performance is relatively suboptimal. 0.8
0.7
It is worth mentioning that this method is a tensor-based 0.6
spectral clustering method, while others are non-tensor 0.5
0.4
methods. It may means that tensor-based methods have a 0.3
certaindegreeofperformanceimprovementcomparedwith 0.2
0.1
non-tensormethods. 0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
value of p ACC NMI Purity
(a)MSRC
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
value of p ACC NMI Purity
(b)Mnist4
(a) MSRC
Fig.5.ClusteringperformancewithdifferentponMSRCandMnist4
(a) MSRC
(b)Mnist4
Fig.3.ClusteringperformancewithdifferentanchorrateonMSRCand
Mnist4
(b) Mnist4
Fig.6.ClusteringperformancewithdifferentλonMSRCandMnist4
andthevalueofthecoefficientλoftheregulartermofthe
(a) MSRC (b) Mnist4
tensorSchattenp-norm.
The effect of anchor rate on clustering performance is
shown in Figure 3. We conducted experiments on a small
Fig. 4. Running time (sec.) with different anchor rate on MSRC and dataset,MSRC,andamedium-sizeddataset,Mnist4.They
Mnist4
obtained the best indicators at anchor rates of 0.7 and
0.4,respectively.Theexperimentalresultsshowthatlarger
anchor rate is not always better. In addition, according to
5.2 ParameterAnalysis
common sense, the larger the anchor rate, the more time
Thehyper-parametersinthemodelareanalyzedexperimen- and space it takes to construct the anchor graph. Figure 4
tally, including the anchor rate (affecting the constructed confirmsthisstatement.Itcanbefoundthatwiththeincrease
anchorgraph),thevalueofpinthetensorSchattenp-norm, of the anchor rate, the running time of the algorithm also
ecnamrofreP
gniretsulC
ecnamrofreP
gniretsulCIEEETRANSACTIONS 7
TABLE2
ClusteringperformanceonMSRC,HandWritten4,Mnist4andScene15
Datasets MSRC HandWritten4
Metrices ACC NMI Purity ACC NMI Purity
CSMSC 0.758±0.007 0.735±0.010 0.793±0.008 0.806±0.001 0.793±0.001 0.867±0.001
GMC 0.895±0.000 0.809±0.000 0.895±0.000 0.861±0.000 0.859±0.000 0.861±0.000
ETLMSC 0.962±0.000 0.937±0.000 0.962±0.000 0.938±0.001 0.893±0.001 0.938±0.001
LMVSC 0.814±0.000 0.717±0.000 0.814±0.000 0.904±0.000 0.831±0.000 0.904±0.000
FMCNOF 0.440±0.039 0.345±0.046 0.449±0.042 0.385±0.092 0.370±0.092 0.386±0.090
SFMC 0.810±0.000 0.721±0.000 0.810±0.000 0.853±0.000 0.871±0.000 0.873±0.000
FPMVS-CAG 0.786±0.000 0.686±0.000 0.786±0.000 0.744±0.000 0.753±0.000 0.744±0.000
Ours 0.986±0.000 0.971±0.000 0.986±0.000 0.963±0.000 0.937±0.000 0.963±0.000
Datasets Mnist4 Scene15
Metrices ACC NMI Purity ACC NMI Purity
CSMSC 0.641±0.000 0.601±0.010 0.728±0.008 0.334±0.008 0.313±0.005 0.378±0.003
GMC 0.920±0.000 0.807±0.000 0.920±0.000 0.140±0.000 0.058±0.000 0.146±0.000
ETLMSC 0.934±0.000 0.847±0.000 0.934±0.000 0.709±0.000 0.774±0.000 0.887±0.000
LMVSC 0.892±0.000 0.726±0.000 0.892±0.000 0.355±0.000 0.331±0.000 0.399±0.000
FMCNOF 0.697±0.119 0.490±0.102 0.711±0.096 0.218±0.033 0.166±0.022 0.221±0.029
SFMC 0.916±0.000 0.797±0.000 0.916±0.000 0.188±0.000 0.135±0.000 0.202±0.000
FPMVS-CAG 0.885±0.000 0.715±0.000 0.885±0.000 0.463±0.000 0.486±0.000 0.481±0.000
Ours 0.982±0.000 0.933±0.000 0.982±0.000 0.800±0.000 0.835±0.000 0.808±0.000
TABLE3 toasmallrangearound50.
ClusteringperformanceonReuters(”OM”meansoutofmemory,and”-”
meansthealgorithmranformorethanthreehours.)
5.3 ExperimentsofConvergence
Datasets Reuters Wetestedtheconvergenceofthemodel,asshowninFigure7.
SinceweintroducedtwoauxiliaryvariablesQandJ when
Metrices ACC NMI Purity
solving the model, we judged whether the model could
CSMSC OM OM OM
graduallyconvergeafterenoughiterationsbycalculatingthe
GMC - - -
difference between variables H and Q and the difference
ETLMSC OM OM OM
LMVSC 0.589±0.000 0.335±0.000 0.615±0.000 between H and J in Eq. (5). As shown in Figure 7, both
FMCNOF 0.343±0.000 0.125±0.000 0.358±0.000 differences approach 0 approximately within 50 iterations,
SFMC 0.602±0.000 0.354±0.000 0.604±0.000
so it can be judged that the model proposed in this paper
FPMVS-CAG 0.526±0.000 0.323±0.000 0.603±0.000
Ours 0.770±0.000 0.690±0.000 0.783±0.000 is convergent. At the same time, we also draw the change
curveofclusteringindexACCwiththenumberofiterations.
ItcanbefoundfromthefigurethatACCalsoconvergeswith
theconvergenceofthemodel.
increases in an approximate linear relationship. It is more
obviousontheMnist4.Italsoshowstheimportanceofthe
choiceofanchorrateontheotherhand.Ingeneral,forlarge 5.4 VisualizationofExperimentalResults
datasets,wetendtousesmalleranchorrates. Weconductedavisualizationexperimentonthelabelmatrix
Theinfluenceofthevaluepontheclusteringperformance H after final fusion according to Algorithm 1, as shown
is shown in Figure 5. We start from p = 0.1 and conduct in Figure 8. In the cluster label matrix H after fusion, the
experimentsatintervalsof0.1untilp=1.Itcanbeseenfrom positionwherethemaximumvalueofeachrowislocated
thefigurethatthebestclusteringperformanceisachieved can be regarded as the cluster to which the sample of
onMSRCandMnist4whenpis0.9and0.2respectively.It that row belongs. It can be found that MSRC, Mnist4 and
can also be shown from the experimental point of view HandWritten4areclearlydividedinto7clusters,4clusters
that, compared with the nuclear norm (where p = 1), and10clusters,respectively.
Schattenp-normminimizationcanensurethattherankofthe
tensorisclosertothetargetrank.Thus,thecomplementary
6 CONCLUSIONS
information between different views can be better mined,
andbetterclusteringperformancecanbeobtained. Inthispaper,weproposealabellearningmethodbasedon
Figure 6 shows the effect of different values of λ on tensorprojection(LLMTP).LLMTPprojectstheanchorgraph
clusteringperformance.Amongthem,whenλachieves51 spaceintothelabelspaceandthuswecangettheclustering
and 50, the best clustering results are obtained on MSRC results from the label tensor directly. Meanwhile, in order
andMnist4,respectively.Moreover,itcanbefoundfromthe to make full use of the complementary information and
figure that MSRC is more sensitive to the value of λ than spatial structure information between different views, we
Mnist4.Whenconductingexperimentsonothersmalland extendtheview-by-viewmatrixprojectionprocesstotensor
medium-sizeddatasets,thevalueofλisgenerallyadjusted projectionprocessingmulti-viewdatadirectly,andusetheIEEETRANSACTIONS 8
(a) MSRC
(a) MSRC
(b)Mnist4
(b) Mnist4
(c) HandWritten4
Fig.7.ConvergenceexperimentonMSRC,Mnist4andHandWritten4
(c) HandWritten4
tensorSchattenp-normtomaketheclusteringlabelmatrix
Fig.8.LabelvisualizationonMSRC,Mnist4andHandWritten4
of each view tend to be consistent. Extensive experiments
haveprovedtheeffectivenessoftheproposedmethod.
[5] Z. Hao, Z. Lu, G. Li, F. Nie, R. Wang, and X. Li, “Ensemble
REFERENCES clustering with attentional representation,” IEEE Trans. Knowl.
Data Eng., vol. 36, no. 2, pp. 581–593, 2024. [Online]. Available:
[1] Y. Yun, J. Li, Q. Gao, M. Yang, and X. Gao, “Low- https://doi.org/10.1109/TKDE.2023.3292573
rank discrete multi-view spectral clustering,” Neural Networks, [6] B. Yang, J. Wu, X. Zhang, X. Zheng, F. Nie, and B. Chen,
vol. 166, pp. 137–147, 2023. [Online]. Available: https: “Discretecorrentropy-basedmulti-viewanchor-graphclustering,”
//doi.org/10.1016/j.neunet.2023.06.038 Inf. Fusion, vol. 103, p. 102097, 2024. [Online]. Available:
[2] Z. Zhang, Q. Wang, Z. Tao, Q. Gao, and W. Feng, “Dropping https://doi.org/10.1016/j.inffus.2023.102097
pathways towards deep multi-view graph subspace clustering [7] Z.Wang,L.Li,X.Ning,W.Tan,Y.Liu,andH.Song,“Incomplete
networks,”inProceedingsofthe31stACMInternationalConference multi-viewclusteringviastructureexplorationandmissing-view
onMultimedia. ACM,2023,pp.3259–3267. inference,”Inf.Fusion,vol.103,p.102123,2024.[Online].Available:
[3] W. Zhao, Q. Gao, S. Mei, and M. Yang, “Contrastive self- https://doi.org/10.1016/j.inffus.2023.102123
representation learning for data clustering,” Neural Networks, [8] C.Tang,K.Sun,C.Tang,X.Zheng,X.Liu,J.Huang,andW.Zhang,
vol. 167, pp. 648–655, 2023. [Online]. Available: https: “Multi-viewsubspaceclusteringviaadaptivegraphlearningand
//doi.org/10.1016/j.neunet.2023.08.050 latefusionalignment,”NeuralNetworks,vol.165,pp.333–343,2023.
[4] H. Lu, H. Xu, Q. Wang, Q. Gao, M. Yang, and X. Gao, [Online].Available:https://doi.org/10.1016/j.neunet.2023.05.019
“Efficient multi-view -means for image clustering,” IEEE Trans. [9] M. Zhao, W. Yang, and F. Nie, “Auto-weighted orthogonal
Image Process., vol. 33, pp. 273–284, 2024. [Online]. Available: andnonnegativegraphreconstructionformulti-viewclustering,”
https://doi.org/10.1109/TIP.2023.3340609 Inf. Sci., vol. 632, pp. 324–339, 2023. [Online]. Available:IEEETRANSACTIONS 9
https://doi.org/10.1016/j.ins.2023.03.016
[10] Q. Xiao, S. Du, K. Zhang, J. Song, and Y. Huang, “Adaptive [31] B.Yang,X.Zhang,F.Nie,F.Wang,W.Yu,andR.Wang,“Fastmulti-
sparse graph learning for multi-view spectral clustering,” Appl. view clustering via nonnegative and orthogonal factorization,”
Intell.,vol.53,no.12,pp.14855–14875,2023.[Online].Available: IEEETransactionsonImageProcessing,vol.30,pp.2575–2586,2021.
https://doi.org/10.1007/s10489-022-04267-9
[11] Y. He and U. K. Yusof, “Self-weighted graph-based framework
formulti-viewclustering,”IEEEAccess,vol.11,pp.30197–30207,
2023.[Online].Available:https://doi.org/10.1109/ACCESS.2023.
3260971
[12] S. Mei, W. Zhao, Q. Gao, M. Yang, and X. Gao, “Joint feature
[32] S.Luo,C.Zhang,W.Zhang,andX.Cao,“Consistentandspecific
selection and optimal bipartite graph learning for subspace
multi-viewsubspaceclustering,”inThirty-secondAAAIconference
clustering,”NeuralNetworks,vol.164,pp.408–418,2023.[Online].
onartificialintelligence,2018.
Available:https://doi.org/10.1016/j.neunet.2023.04.044
[13] H. Wang, Q. Wang, Q. Miao, and X. Ma, “Joint learning of
datarecoveringandgraphcontrastivedenoisingforincomplete
multi-view clustering,” Inf. Fusion, vol. 104, p. 102155, 2024.
[Online].Available:https://doi.org/10.1016/j.inffus.2023.102155
[14] K. Zhan, C. Zhang, J. Guan, and J. Wang, “Graph learning for
multiview clustering,” IEEE Trans. Cybern., vol. 48, no. 10, pp. [33] H. Wang, Y. Yang, and B. Liu, “Gmc: Graph-based multi-view
2887–2895,2018. clustering,”IEEETransactionsonKnowledgeandDataEngineering,
[15] X.Li,H.Zhang,R.Wang,andF.Nie,“Multiviewclustering:A vol.32,no.6,pp.1116–1129,2019.
scalableandparameter-freebipartitegraphfusionmethod,”IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 44,
no.1,pp.330–344,2022.
[16] Q. Zhou, H. Yang, and Q. Gao, “Low-rank constraint bipartite
graphlearning,”Neurocomputing,vol.511,pp.426–436,2022.
[17] H.Yang,Q.Gao,W.Xia,M.Yang,andX.Gao,“Multiviewspectral
[34] J.Wu,Z.Lin,andH.Zha,“Essentialtensorlearningformulti-view
clusteringwithbipartitegraph,”IEEETrans.ImageProcess.,vol.31,
spectralclustering,”IEEETransactionsonImageProcessing,vol.28,
pp.3591–3605,2022.
no.12,pp.5910–5922,2019.
[18] W.Xia,Q.Gao,Q.Wang,X.Gao,C.Ding,andD.Tao,“Tensorized
bipartite graph learning for multi-view clustering,” IEEE Trans.
PatternAnal.Mach.Intell.,vol.45,no.4,pp.5187–5202,2023.
[19] Q.Gao,Z.Wan,Y.Liang,Q.Wang,Y.Liu,andL.Shao,“Multi-view
projectedclusteringwithgraphlearning,”NeuralNetworks,vol.126,
pp.335–346,2020.
[20] B. Wang, Y. Xiao, Z. Li, X. Wang, X. Chen, and D. Fang, [35] Z.Kang,W.Zhou,Z.Zhao,J.Shao,M.Han,andZ.Xu,“Large-scale
“Robust self-weighted multi-view projection clustering,” in multi-viewsubspaceclusteringinlineartime,”inProceedingsofthe
The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAIconferenceonArtificialIntelligence,2020,pp.4412–4419.
2020. AAAI Press, 2020, pp. 6110–6117. [Online]. Available:
https://doi.org/10.1609/aaai.v34i04.6075
[21] X. Sang, J. Lu, and H. Lu, “Consensus graph learning for auto-
weightedmulti-viewprojectionclustering,”Inf.Sci.,vol.609,pp.
816–837,2022.
[22] H. Wang, W. Zhang, and X. Ma, “Clustering of noised and
[36] S.Wang,X.Liu,X.Zhu,P.Zhang,Y.Zhang,F.Gao,andE.Zhu,
heterogeneousmulti-viewdatawithgraphlearningandprojection
decomposition,” Knowl. Based Syst., vol. 255, p. 109736, 2022. “Fastparameter-freemulti-viewsubspaceclusteringwithconsensus
anchorguidance,”IEEETransactionsonImageProcessing,vol.31,pp.
[Online].Available:https://doi.org/10.1016/j.knosys.2022.109736
556–568,2021.
[23] J.Li,X.Zhang,J.Wang,X.Wang,Z.Tan,andH.Sun,“Projection-
basedcoupledtensorlearningforrobustmulti-viewclustering,”
Inf.Sci.,vol.632,pp.664–677,2023.
[24] S.Wang,X.Liu,S.Liu,J.Jin,W.Tu,X.Zhu,andE.Zhu,“Alignthen
fusion:Generalizedlarge-scalemulti-viewclusteringwithanchor
matchingcorrespondences,”inAdvancesinNeuralInformationPro-
cessingSystems35,S.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,
K.Cho,andA.Oh,Eds.,2022.
[25] M.E.KilmerandC.D.Martin,“Factorizationstrategiesforthird-
ordertensors,”LinearAlgebraanditsApplications,vol.435,no.3,pp.
641–658,2011.
[26] Q.Gao,P.Zhang,W.Xia,D.Xie,X.Gao,andD.Tao,“Enhanced
tensor rpca and its application,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 43, no. 6, pp. 2133–2140,
2021.
[27] Z. Zha, X. Yuan, B. Wen, J. Zhou, J. Zhang, and C. Zhu, “A
benchmarkforsparsecoding:Whengroupsparsitymeetsrank
minimization,”IEEETransactionsonImageProcessing,vol.29,pp.
5094–5109,2020.
[28] Y.Xie,S.Gu,Y.Liu,W.Zuo,W.Zhang,andL.Zhang,“Weighted
schattenp-normminimizationforimagedenoisingandbackground
subtraction,”IEEETrans.ImageProcess.,vol.25,no.10,pp.4842–
4857,2016.
[29] J. Li, Q. Gao, Q. WANG, M. Yang, and W. Xia, “Orthogonal
non-negative tensor factorization based multi-view clustering,”
inThirty-seventhConferenceonNeuralInformationProcessingSystems,
2023.
[30] H.Xu,X.Zhang,W.Xia,Q.Gao,andX.Gao,“Low-ranktensor
constrainedco-regularizedmulti-viewspectralclustering,”Neural
Networks,vol.132,pp.245–252,2020.