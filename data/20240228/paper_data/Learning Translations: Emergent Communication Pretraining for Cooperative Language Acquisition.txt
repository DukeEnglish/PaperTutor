Learning Translations: Emergent Communication Pretraining
for Cooperative Language Acquisition
DylanCope∗ and PeterMcBurney
King’sCollegeLondon
Abstract and any permutation of a learned protocol is equally likely
toappearacrossdifferenttrainingruns(Bullardetal.,2021).
In Emergent Communication (EC) agents learn to
The result is that the learned conventions established within
communicate with one another, but the protocols
atrainingcommunitywillbeveryunlikelytoworkwithnew
that they develop are specialised to their training
agents,andbydefault,theECtrainedagentswillbeincapable
community. This observation led to research into
ofadapting.
Zero-Shot Coordination (ZSC) for learning com-
In response to this, many researchers have become inter-
munication strategies that are robust to agents not
estedindevisingmethodsinwhichagentslearncommunica-
encountered during training. However, ZSC typi-
tive strategies that can adapt to this Zero-Shot Coordination
cally assumes that no prior data is available about
(ZSC) setting (Li et al., 2023; Hu et al., 2021, 2020; Os-
theagentsthatwillbeencounteredinthezero-shot
senkopf,2020;CopeandSchoots,2020;Bullardetal.,2020).
setting. Inmanycases,thispresentsanunnecessar-
ZSC algorithms typically aim to successfully communicate
ily hard problem and rules out communication via
with an unknown agent on the first encounter, without any
preestablishedconventions. WeproposeanovelAI
priorinformation. Butinmanyreal-worldsettings,thisisan
challengecalledaCooperativeLanguageAcquisi-
unnecessarilychallengingassumption. Ifsomeoneisinjured
tion Problem (CLAP) in which the ZSC assump-
on a street in London, passing pedestrians can form an ad
tions are relaxed by allowing a ‘joiner’ agent to hocteamandaidthepatientbyspeakingtoeachotherinEn-
learnfromadatasetofinteractionsbetweenagents
glishtocoordinatearesponse. Indeed, languageisarguably
in a target community. We propose and compare
themostcriticalsetofconventionsthatsuchteamscandraw
twomethodsforsolvingCLAPs: ImitationLearn-
upontoefficientlyworktogether.
ing (IL), and Emergent Communication pretrain-
Thestudyofartificialagentsthatcanformadhocteamsis
ing and Translation Learning (ECTL), in which
knownasAdHocTeamwork(AHT)(Stoneetal.,2010).Sim-
an agent is trained in self-play with EC and then
ilarly to ZSC, most of these algorithms aim to make as few
learnsfromthedatatotranslatebetweentheemer-
assumptionsaspossibleabouttheplayersthatanagentmay
gentprotocolandthetargetcommunity’sprotocol.
formateamwith. Notably, SarrattandJhala(2015)applied
thisminimalistapproachtocommunication. Otherworkhas
1 Introduction relaxedthisbyassumingapriorknowncommunicationpro-
tocol(Barrettetal.,2014;Mirskyetal.,2020).
Creatingteamsofartificialagentsthatcancommunicateand
Inthiswork,wepresentanovelAIchallengethatwecall
cooperatehasbeenalong-standingareaofinterestinmulti-
aCooperativeLanguageAcquisitionProblem(CLAP).Here
agent systems research. Advances in multi-agent reinforce-
by ‘language acquisition’ we mean learning the syntax and
mentlearninghaveenabledresearchersinthefieldofEmer-
semantics of a preexisting communication system used by a
gent Communication (EC) to train such teams in ever more
community. This class of problems is positioned between
complexdomains(Wagneretal.,2003;Foersteretal.,2016;
the challenges of ZSC and AHT. In a CLAP, we are given
Sukhbaatar et al., 2016; Jaques et al., 2019; Lazaridou and
adatasetofcommunicationeventsbetweenspeakersandlis-
Baroni,2020). Theseagentsaretypicallytrainedbyallowing
teners in a target community as they solve a problem. Our
discretemessagestobeexchangedbetweenagents.Program-
goalistoconstructajoineragentthatcancommunicateand
mersdonotassignmeaningtothemessages,rather,meaning
cooperatewithagentsfromthiscommunity. Thisproblemis
emerges via the training process as communicative conven-
alsocloselyrelatedtoImitationLearning(IL),however,most
tions are developed in service of solving the task. As such,
work in IL is confined to the single agent setting (Hussein
the mapping between meanings and messages is arbitrary,
etal.,2017). Sotothebestofourknowledge,thisisthefirst
∗Correspondence: dylan.cope@kcl.ac.uk. DC is supported by attempttoposeanILproblemformulti-agentcommunication
theUKRICentreforDoctoralTraininginSafeandTrustedAI(EP- withinaformalcooperativemodel.
SRCProjectEP/S023356/1).Preprint(underreview). Alongside defining CLAP, we outline two baseline solu-
4202
beF
62
]GL.sc[
1v74261.2042:viXrationstothisproblem. Thefirstusesasimpleimitationlearn- ronmentactions e,andagentsarenotprogrammedtosend
Ai
ing method. The second is a novel algorithm called Emer- messageswithanyprescribedmeaning.Rather,thesemantics
gent Communication pretraining and Translation Learning emergethroughtraining.
(ECTL). We introduce two environments and train target For this paper, we will consider memoryless agents that
communities of agents that cooperate via a learned commu- communicatewithinacentralisedforwardpasscomputinga
nicationprotocol. Dataisthengatheredfromthesecommu- jointactionfortheenvironment(similartoGoldmanandZil-
nitiesandthenusedtotrainjoineragentswithECTLandIL. berstein2003).
We demonstrate that ECTL is more robust than IL to expert Policy Factorisation. We suppose that we can fac-
demonstrationsthatgiveanincompletepictureoftheunder- tor a memoryless communication agent’s policy π into an
i
lying problem and that ECTL is significantly more effective environment-level policy π ie : Ω
i
×Σns
→
Ae i, where n
s
thanILwhencommunicationsdataislimited.Finally,weap- is the number of agents that send messages to agent i, and
ply these methods to manually collected data and show that a communication policy πc : Ω c. Therefore, given a
i i → A(cid:81)i
ECTL can learn to communicate with a human to coopera- jointobservationo=(o ,...,o ) Ω ,thejointpolicy
1 N ∈ i i
tivelysolveatask. π(o) is computed by first producing the outgoing messages
Mi = πc(o ) from each sender agent i to each receiver
out i i
2 Background agentj C . AsetofincomingmessagesMi isthencon-
∈ i in
structedforeachagentibypassingthemessagesforithrough
2.1 DecentralisedPOMDPs
a communication channel function σ : c Σ. The final
ADecentralisedPartially-ObservableMarkovDecisionPro- environmentactionsarethengivenbyi a A =i π→e(o ,Mi ).
i i i in
cess (Dec-POMDP) is a formal model of a cooperative Self-play. Emergentcommunicationtrainingisoftendone
environment defined as a tuple = ( , ,T,r,Ω,O) inself-play,whereparametersaresharedbetweenagentsand
M S A
(Oliehoek and Amato, 2016), where is a set of states, centrally optimised (Hu et al., 2020). Communication can
(cid:81) S
and
A
= iAi is a product of individual agent action behardtolearnwithReinforcementLearning(RL)assend-
sets. A joint action a is a tuple of actions from each ing messages cannot provide reward unless listeners already
∈ A
agent that is used to compute the environment’s transition know how to use the information. This chicken-and-egg
dynamics, defined by a probability distribution over states problem can be solved with centralised training by allowing
T : [0,1]. Teamperformanceisdefinedbya gradientstobackpropagatethroughcommunicationchannels.
S ×A×S →
cooperativerewardfunctionr : overstatetransi- AGumbel-Softmax(Jangetal.,2017;Maddisonetal.,2017)
S ×A×S
tionsandjointactions. Ω= Ω i isasetofobservationsets, functionisapopularchoiceasitfacilitatesbackpropagation
(cid:81) { }
andO :S
→
iΩ iisanobservationfunction. duringtrainingandcanbediscretisedduringevaluation.
Eachagentifollowsapolicyπ i thatmapsanobservation However, as Lowe et al. (2019) have shown, measuring
sequence (or a single observation if i is memoryless) to a whether or not agents trained to communicate are doing so
distribution over its actions. A trajectory for an agent i is canbetrickyforcomplexenvironments.Toremedythis,they
a sequence of observation-action-reward tuples τ i i = introduced definitions of positive listening and positive sig-
(Ω i i R)∗. For a set of policies Π = π i ∈ , aT joint nalling. In short, to be positive listening/signalling; the lis-
traje× ctoA ry i× s τ = (Ω R)∗, and w{ e ca} n denote teningagentshouldchangeitsactionsaccordingtothemes-
∈ T ×A×
thedistributionofjointtrajectoriesforthissetofpoliciesact- sages it receives and the signalling agent should change its
ing in the environment as Π. In this work, we will only messagesdependingonwhatitobserves.
M|
consider finite-horizon Dec-POMDPs, so the lengths of tra-
jectories will always be bounded. The total reward for this 2.3 ImitationLearning
trajectoryisthesumofrewardsalongthesequence,denoted
Imitation Learning (IL) is a form of machine learning in
R(τ). Theexpectedsumofrewardsforasetofpolicieswill
whichexpertdemonstrationdataisusedtoconstructapolicy
bedenotedR(Π)=E [R(τ)].
τ∼M|Π forsolvingatask. Thedataistypicallyintheformofanob-
2.2 EmergentCommunication servationoandactiona, andtheimitationlearningproblem
is posed as supervised classification learning, e.g. optimis-
Emergent communication is the study of agents that learn
ing θ to minimise the difference between a and aˆ = π (o).
θ
(orevolve)tomake useofcommunicationchannelswithout
SeeHusseinetal.(2017)foramorecomprehensivereviewof
previously established semantics. In a typical set-up, each
thesemethods.
agent’s action set in the Dec-POMDP can be expressed as
= e c,where c isasetofcommunicativeactions,
A ani
d
Aei is× aA si
et of
envA iroi
nment actions. The communica-
3 CooperativeLanguageAcquisition
Ai
tiveactionscanfurtherbewrittenastheproductofone-way In this section, we introduce our definition of a Coopera-
communication channels from i to j C i using a discrete tive Language Acquisition Problem (CLAP). A CLAP can
∈
message alphabet Σ, i.e. c = Σ|Ci|. These are cheap-talk be formulated from the simplest case involving a preexist-
Ai
channels, meaning there is no cost to communication. This ing target community of two agents, denoted Π = A,B ,
{ }
variant of a Dec-POMDP is known as a Dec-POMDP-Com thatachievesomenon-trivialperformanceinaDec-POMDP-
(GoldmanandZilberstein,2004,2008;OliehoekandAmato, Com . Consider an interaction at time t in which A is
M
2016). The messages have no prior semantics as the transi- speaking and B is listening. While making the observation
tion function of the Dec-POMDP only depends on the envi- os, the speaker emits a message m . Then, while observing
t t(a)Jointagentsarchitecture(twoagents) (b)Illustrationofthe (c)Illustrationofthe
withcommunication. gridworldenvironment(Section5.1). drivingenvironment(Section5.1).
Figure1: (a)Agentsarchitecturediagramfortwoagents(topandbottom)withcommunication. (b)Illustrationofthegridworldtoyenvi-
ronmentwithtwoagents,depictedwithblueandgreencirclesona5x5grid. Thegreenandbluestarsindicatethelocationsofeachagents’
respectivegoals.Thecloudthoughtbubbledepictstheworldasthegreenagentobservesit.Note,eachagentdoesnotseetheirowngoal,and
theyareoff-by-onesquareintheirknowledgeofeachother’sgoal.(c)Illustrationofthedrivingenvironmentwithtwoagents.Thedarkcircle
inthecentreisthe‘pit’;theregioninwhichlargenegativepenaltiesaregivenwhenagentsenter.Thestarsindicategoallocations,whichcan
spawninoneofeightlocations(theunusedlocationsindicatedbythegreyed-outstars).Thecontinuousstatespaceisindicatedwiththegrid
axes. Theagentsarerepresentedbyarrowheadsindicatingtheircurrentpositionanddirection. Again,hereagentsdonotobservetheirown
goalsandthereforeneedtocommunicate,butunlikethegridworld,theydohaveperfectknowledgeoftheotheragent’sgoals.
orandm ,thereceivertakestheactiona intheenvironment. WhereΠ−k = Π π . Inotherwords,maximisetheteam
t t i \{ k }
Wearegivenaccessto andadatasetofsuchinteractions, rewardswhenthejoinerreplacesk.
denoted (os,m ,or,ar)M , and our task is to construct a
t t t t ∈ D Thiscanbedecomposedintothreerelatedproblems;afor-
joiner agent . In this paper, we focus on a specific case
J ward communication (signalling), a backward communica-
of the task in which we aim to replace a target agent in Π
tion(listening),andacting. Moreprecisely, needstolearn
(sothereisalwaysafixednumberofplayers),whichwecall J
tosendmessagesthatmaximisetheexpectedsumofrewards
CLAP-Replace. The agent should be able to take on the
J for listening agents and interpret messages to maximise the
role of a target agent (e.g. either A or B) and successfully
rewards of its trajectory. But the agent also needs to learn
communicate with the other, while also acting in the envi-
toutiliseinformation, bothcommunicatedandobserved, for
ronmenttomaximisethecooperativereward. Thejoinerwill
selectingactions.
be evaluated zero-shot, i.e. on the first joining event, so all
If we were allowed to interact with the target commu-
learningmustbedonebeforehand.
nityΠbeforeevaluation,wecouldapplystandardreinforce-
The emphasis of the dataset is on learning the communi-
ment learning tools. However, we are interested in settings
cationprotocolasthisisassumedtobethekeyaspectofthe
in which disrupting the community and jeopardising perfor-
community’s strategy that cannot be learned independently
mancetolearnisnotacceptable,so needstoperformwell
fromobservingtheirbehaviour. Asthecommunicationsym- J
onthefirstactualjoiningepisode.
bols have no prior semantics, any protocol could be equally
successfulifthesamemeaningswereassignedtoadifferent 3.2 DisentanglingEnvironment-Leveland
permutation of the symbols. On the other hand, there may
CommunicativeCompetencies
be many ‘environment-level’ behaviours that can be learned
without needing to know the specific strategies of a given Weassumethatteamsofagentscanachieveacertainlevelof
community. We will discuss disentangling these factors in performanceintheenvironmentbytwocategoriesofcompe-
Section3.2. tencies: (1) ‘environment-level’ skills, and (2) communica-
tionstrategiesthat relyonestablishedconventions. Thekey
3.1 ProblemDefinitions distinctionweaimforisthattheformershouldbelearnable
independently of observing a target community’s behaviour.
The cooperative language acquisition task is to construct a We start by defining that the team achieves an average to-
joiningagent thatlearnsfromobservingatargetcommu- talrewardofRwhenactingtogetherintheenvironmentand
J
nity Π. When replaces an agent in Π, we have a CLAP- communicating.
J
Replacetask: Measuringthecommunicativecompetency(1)isrelatively
Problem 1 (CLAP-Replace). Suppose there exists a Dec- easy: if communication is blocked and the team achieves a
POMDP-Com and a set of policies Π = π trained loweraveragetotalrewardR′ <R,andbyassumingthatthe
i
in . GivenaM targetpolicyπ Πtoreplace,{ ada} taset agentsareengagedinpositivelisteningandsignalling,wecan
k k
ofiM nteractions(os,m ,or,ar)in∈ whichπ iseitheraspeakD er attributethisdropinperformancetoalackofcommunication.
t t t t k
orlistener,thetaskistoconstructapolicyπ suchthat: We evaluate the influence of environment-level compe-
J
tencies (2) by similarly hampering them, and investigating
π argmaxR( π′ Π−k) (1) whether this leads to a drop in performance. We will as-
J
∈ π′ { }∪ sume that each agent’s observation can be partitioned into4.1 ImitationLearning(IL)
State Space
High Legend In the set-up for CLAP-Replace we are given a dataset
Reward (os,m ,or,ar) of speaker and listener observations,
Start t t t t ∈ Dk
messages, and actions taken by the receiver. The simplest
EExxppeerrtt
TTrraajjeeccttoorriieess baseline solution to this problem is to apply imitation learn-
ing separately to the signalling and listening problems. The
IL agent
Low Trajectories dataset is partitioned into two datasets: a signalling dataset
Reward ( (o (os
t
r, ,m mt)
)∈
,aDr)ks whe rre wk hi es reth kes ispe tha eke rr eca en id vea r.lis Tte hn ei sn eg dd aa tata ss ee tst
t t t ∈ Dk
Figure2:IllustrationoftheproblemofcompoundingerrorsforIm- are structured as input-label tuples to learn communicative
itationLearning. WhenILagentsexittheexpertstatedistribution and environmental-level policy factors (πc and πe) of the
il il
theyareunabletorecover. overallimitationpolicyπ il. Thesearelearnedwithacategor-
icalcross-entropy(CCE)lossbetweenpredictedandactual
labels.Wewilldenotethisimitatorjoiner .InFigure3,πc
‘private’ (or ‘local’) and shared (or ‘global’) components: and πe are composed of encoder and comJ mil unication/actioi nl
oi = (li,g ), where g is observed by both speakers and il
t t t t heads,withthesamearchitectureasinFigure1a.
listeners, but li is private to i. To assess the contribution
t
ofenvironment-levelskillswerestricttheenvironment-level 4.2 EmergentCommuncationPretrainingand
policy by removing private information, o′ t = (0,g t), and TranslationLearning(ECTL)
replacing each π with π′(o ,M ) = (πe(o′,M ),πc(o )).
i i t t i t t i t Awell-knownissuewithimitationlearningagentsisthatthey
We can then attribute a change R′′ < R to the agent being
can be brittle given the natural biases in the expert demon-
prevented from exercising environment-level skills. To un-
strationdata(Kumaretal.,2022). Thisproblemisillustrated
derstandthisintervention,considerthefollowingpoints:
in Figure 2. Especially in more complex domains, experts
1. Wehamperenvironment-levelskillsbyblockingprivate typicallystayin theregionsofstatespace inwhichtheyget
informationfromπe,butanagentmuststillbeallowed highrewards,whichmayonlybeasmallportionofthepos-
to receive messages as this may provide vital infor- sibilities. Ifanimitatormakesamistakewhenattemptingthe
mation for succeeding based on communication skills. taskitself,itmayenterintounseenterritory. Thuserrorscan
Thus,πcforthespeakermustnotbeintervenedwith. compoundastheagenthasnotlearnedwhattodoandmakes
more mistakes, leading to degraded team performance in a
2. Communicationmayrelyonsharedinformation, sore-
multi-agentcooperativesetting.
movingthecontextbyblockingthisinformationmayin-
Thisleadstotheideathatthejoinercouldexplorethestate
terferewithmeasuringenvironment-levelskills.
spaceof beforejoiningΠ,andtherebybecomeamorere-
3. Informationprivatetoarecipientcannothavebeenused M
liablecooperator. Ingeneral,pretrainingmayallowtheagent
bythespeakertocreatethemessages,soifadropinper-
tolearnenvironment-levelskillsthatcouldbetransferableto
formance is seen when it is removed from πe, this can
cooperatingwithanynewteam. Tothisend,weproposethe
only be due to a loss in the recipient’s capacity to use method Emergent Communication pretraining and Transla-
that private information to contribute to the team’s per- tionLearning(ECTL).ThefirststepofECTListo(pre)train
formance through its actions (as opposed to its contri- a set of agents Π′ from scratch in that we refer to as the
butionsthroughsendingmessages). Recall,weareonly ECtrainingcommunity,whereforM
eachagentinΠthereisa
obfuscatingthisinformationfromπe;πcstillseesit.
correspondingagentinΠ′.
4. Ifnodropfromblockingprivateinformationtoπeisob- The agents in Π′ are composed of three components, il-
served,itimpliesoneoftwocases: (a)allofanagent’s lustratedinFigure1a: anobservationencoder enc θ, acom-
contribution to the collective performance is the result munications head πc, and an action head πe, parameterised
θ θ
of following direct orders from a speaker. Or, (b) an byθ,andsharedamongstthepoliciesΠ′. Forbrevityandto
agent’s contribution is entirely contingent on global in- be consistent with the notation in previous sections we will
formation. omit the encoder from our notation, and write πc(o) instead
θ
of πc(enc (o)). This training process could use any viable
Notethatfollowing4b,theabsenceofadropdoesnotim- θ θ
multi-agent reinforcement learning algorithm, and which is
plythattheteamdoesnotpossessenvironment-levelcompe-
mostsuitabledependsonthepropertiesoftheunderlyingen-
tencies,butthepresenceofadropdoes.
vironmentandthenumberofagents. InSection5.2,wewill
outlinethespecificmethodsweusedforourexperiments.
4 MethodsforConstructingJoiners
AgentsintheECtrainingcommunitylearntocooperatevia
In this section, we introduce two methods for constructing an emergent communication protocol over their message al-
joiners for the CLAP-Replace task. The first naively ap- phabetΣ′.ThenextstepinECTLtowardsbuildingthejoiner
plies imitation learning, and the second pretrains agents us- agent for Π is to translate this protocol to the one used by
ingemergentcommunicationandthentranslatesthelearned the target community. For a CLAP-Replace task with target
communicationprotocoltothetargetcommunity’sprotocol. agentπ Π,weselecttheequivalentpretrainedπ′ Π′to
k ∈ k ∈
Eachmethodisdecomposedintoforwardandbackwardprob- useastartingpointfortranslationlearning. Wewillreferto
lems,posedassupervisedlearningtasks. π′ astheECpretrainedagent.
kECTL Forward ECTL Backward IL Forward IL Backward
Legend
Label
Pred
Input
Trainable
Figure3:TrainingarchitecturediagramsfortheImitationLearning(IL)andEmergentCommunicationpretrainingandTranslationLearning
(ECTL) methods. The forward and backward problems for each method are solved with supervised learning. Circles are variables and
trapezoidsarefunctions. Dottedlinesindicatethatgradientsareblockedfrombackpropagatingalongapath. FortheECTLdiagramsthe
encandπcfunctionsarelearnedduringtheemergentcommunicationpretrainingphase. Thevariablesos t,or t,m t,a tarethesender/receiver
agentobservations,thesender’smessage,andthereceiver’sactionfromthedatasetofinteractionscollectedfromthetargetcommunity.
NoModifications(R(Π)) R(Π)
NoCommunications ECTL
NoSharedObsforπe ECTL(BiasedData)
NoPrivateObsforπe IL
UntrainedAgents IL(BiasedData)
30 20 10 0 8 6 4 2 0
− M−eanTotalRew−ard − −MeanTot−alReward −
(a)Ablatedagentsinthetargetcommunity. (b)Meanrewardfordifferentteams.
Figure4: Performanceresultsfromcasesinwhich(a)theteamisformedofavarietyofagents,comparingECTLtoimitationlearning(IL)
withunbiasedandbiaseddatafromthegridworldenvironment. And(b)thetargetcommunityagentsablatedindifferentways. Allresults
arefrom500evaluationepisodesandtheerrorbarsshowmeanswithin95%confidenceintervals.
Onceagain, thetargetcommunitydata istransformed from the EC training community’s communication protocol
k
intoasignallingdataset(os,m ) sandD alisteningdataset to the target community’s protocol, and is parameterised by
((or,m ),m′) r. Nott ethat tt∈ heD Ek CTLsignallingdataset ϕ. Givenanobservationos Ω andmessagem Σfrom
isidt entit caltot th∈ eD sigk nallingdatasetusedforimitationlearn- s, we can compute thet m∈ essak ge m′ = Σ′ that t∈ the agent
ing,butthelisteningdatasetisdifferent. Insteadofthelabel
D π′k Π′wouldsendinthatsituation.Tt
hetranslationfunction
being an action, it is a message m′ from the EC pretrain- isk tr∈ ainedtopredictthedemonstratoragent’smessage:
t
ing message space Σ′. To construct these labels we use the
(cid:88)
speaker’sobservationfrom kandcomputethemessagethat ϕ∗ argmin CCE(m t,mˆ t) (2)
E i.eC . mpre ′ tt =rai πn θe c(d oa s tg ).entwouldD haveemittedthatsamesituation, whe∈ remˆϕ =τ( so (s t o,m s,t m)∈ ′D )ks , m′ =πc(os) (3)
t ϕ t t t θ t
From these data, we learn translation functions for sig-
nallingandforlistening. Aseparatetranslationfunctioncan A listening translation function τr : Ω Σ Σ′ maps
ψ k × →
belearnedforeachcommunicationchanneliftherearemul- from the target community’s communication protocol to the
tiplepossiblesendersand/orreceiversforthetargetagent,or ECtrainingcommunity’sprotocol,andisparameterisedbyψ.
two models (one for each CLAP sub-problem) can be used Note that, in general, τr and τs need not be inverses of one
byprovidingthesender/receiveridentifiersasinput. Forour another, as two agents can use arbitrarily different protocols
experiments, we will use the latter approach. However, for ineachdirection.
simplicity in the following descriptions of how these func- To train τr, we use the message that was received by
ψ
tionsaretrained,wewillassumeonesenderandreceiverfor the listener (the target agent) m , the listener’s observation
t
thetarget agentand omitagent identifiers. Additionally, the or, and the speaker’s observation or. The translation func-
t t
training architectures for the translation functions are illus-
tion takes the observation and the message and produces a
tratedbythetwoleftmostdiagramsofFigure3.
newmessageintheECtrainingcommunity’smessagespace:
Asignallingtranslationfunctionτs : Ω Σ′ Σmaps τr(or,m )=mˆ′ Σ′. Togettheground-truthlabelforthis
ϕ k × → ψ t t t ∈1.0 Ncollect/Pit
1000/False
1000 500/False
0.8 100/False
1000/True
500/True
0.6
100/True
800
0.4
IL
ECTL 0.2
600
R(Π)
0.0
10 25 50 100 250 500 1000 NoPit Pit NoPit Pit
DataCollectionEpisodes(log scale) ECTL IL
10
(a)ThemeanCLAP-ReplaceperformanceontheDrivingGame (b) Mean reward for each method normalised by the original
(NoPit)forECTLandIL,againstthenumberdatacollection teammeanrewards. ShowstheimpactofthepitonCLAPper-
episodes(N collect)fromthetargetcommunity. formance,highlightingIL’srelativefragility.
Figure5:ComparisonsbetweenILandECTLontheDrivingCommunicationenvironment.
prediction,wefindthemessagethattheECpretrainedagent space are not present in the demonstration data. To investi-
wouldhavesentinthetargetcommunityspeaker’ssituation. gate this problem, we introduced this ‘driving’ environment
Theresultisthefollowingoptimisationcriterion: in which the agent steers and accelerates a body in a con-
(cid:88) tinuous grid (Figure 1c). This environment has two settings
ψ∗ ∈argmin CCE(m′ t,mˆ′ t) (4) definedbythepresenceorabsenceofacircularregioninthe
ψ
((or t,mt),m′ t)∈D kr centreoftheworldwherealargepenaltyisappliedforevery
wheremˆ′ =τr(or,m ), m′ =πc(os) (5) agent within the region. The agents must navigate to one of
t ψ t t t θ t
eightfixedgoallocations,knownbytheirpartnerandselected
Finally,wecanputtogetherthesepiecesintoafinaltrans-
atrandomforeachepisode.
latorjoineragent composedofthefollowingcommuni-
ectl
J
cationandenvironment-levelpolicyfactors: 5.2 CreatingTargetCommunities
πc =τs πc and πe (o,m)=πe(o,τr(m)) (6) Target communities Π of three agents in the gridworld
ectl ϕ◦ θ ectl θ ψ
environment and two agents in the driving environment
Sowhenthejoineragentreceivesamessage,itusesthelisten-
were trained using Multi-Agent Proximal Policy Optimisa-
ing translation function to predict the message that it would
tion (MAPPO) (Yu et al., 2022). For the policy networks
have received from the equivalent agent in its EC training
we used the architecture in Figure 1a with parameter shar-
community. When speaking, the agent first computes the
ingbetweenagents. Thevaluenetworkusedaconcatenation
messagethatitwouldhavesentinitsoriginalmessagespace,
of all of the agents’ observations as input, meaning that we
and then passes that through the signalling translation func-
relyoncentralisationfortraining. Thevaluenetworkdidnot
tionbeforefinallysendingit.
share any parameters with the policy networks. A Gumbel-
Softmax (Jang et al., 2017; Maddison et al., 2017) commu-
5 Experiments
nicationchannelfunctionwasusedoncommunicationchan-
5.1 Environments nels. FurthertrainingdetailscanbefoundinAppendixA.
In order to empirically investigate IL and ECTL for solving
5.3 AblatingTargetCommunityAgents
CLAP-Replace tasks, we created two environments. Firstly,
asimplegridworldtoyenvironmentinwhichN agentscoop- Thegridworldenvironmentwasdesignedsuchthattheagents
eratebycommunicatinggoalinformation. Secondly,a‘driv- need to use a combination of communicative and individual
ing’communicationprobleminwhichagentsmustnavigatea skills in order to succeed. To demonstrate how these skills
continuousspacetoreachagoal,whilepotentiallyavoidinga contribute to the performance, we evaluate ablated versions
pitintheirway. Again,thegoallocationisknownbyanother oftheagentswiththeinterventionsdiscussedinSection3.2.
agentsotheyneedtocommunicatetosolvetheproblem. Figure4ashowstheresultsoftheseexperiments. Atthetop,
GoalCommunicationsGridworld.Eachagenthasagoal we show the original unmodified performance for reference
square in the grid that they need to reach that changes each (R(Π)). The next two bars, appearing roughly equal in size
episode. No agent observes its own goal unless it is within andvariance,correspondtoblockingcommunications(bar2)
one tile of it, but at all times it observes a ‘close guess’ (a
and‘global’/‘shared’informationfromπe(bar3).
location within one tile) of the goal of another of the agents
5.4 GridworldCLAP-Replace
inthegame. TheenvironmentisillustratedinFigure1b.
Goal Communications Driving Game. As discussed in We train ECTL and IL agents after collecting interaction
Section4.2,ILagentscanbebrittlewhenregionsofthestate datasets from N = 100 episodes of Π acting in the
collect
draweRedosipEnaeM
draweRnaeMdesilamroN7.5 1.0 7.5 1.0
5.0 0.8 5.0 0.8
150 2.5 2.5
0.6 0.6
0.0 0.0 100
0.4 0.4
2.5 2.5
− −
5.0 0.2 5.0 0.2 50
− −
7.5 7.5
− 5 0 5 − 5 0 5 0
− X-Coordinate − X-Coordinate ECTL Random IL
(a) (b) (c) (d)
Figure6: (a,b)Two-dimensionalhistogramsvisualisinghowmuchtimetheagentsspendinthedifferentregionsoftheenvironmentbefore
reachingtheirgoals. Leftandrightshowagentstrainedwith/withoutpitpenalties. Theradiusofthepitisindicatedwiththedashedcircle
outline. Wecanseethatwhenthepitispresentagentsavoidthatregion. Thegoallocationsarealsovisibleaswearenotplottingpositions
afterthegoalsarereached.(c)TheUIforthehumantoplaythegameasalistener.Thetop-leftofthescreenshowsmessagesfromtheother
player. (d)Themeanepisodelengthswhenweplayedthisgamewithdifferentagents. Thegoalisforustonavigatetoourgoallocationas
quicklyaspossible(oneofthegreystars)usingthemessages.WeseethatonlytheECTLagentwasabletoeffectivelycommunicate.
environment. See Appendix A for details on training hy- outperformsECTLwhenthepitisnotpresent.
perparameters and learning curves. Figure 4b shows the re-
sults from forming teams of different compositions in the 5.6 TranslatingtoanInterpretable
gridworld environment. We see that ECTL and IL solve the CommunicationProtocol
CLAP-Replacetaskbyachievingtheequivalentteamperfor-
After demonstrating that IL and ECTL could successfully
manceastheoriginalteam(R(Π)).
solve CLAPs for target communities of artificial agents, we
Bothmethodshaveroughlythesameperformance, which
investigatedwhetherornotthesamemethodscouldapplyto
is due to the fact that there are not opportunities for errors
data generated by humans. We developed an interactive UI
to compound. The imitator will have learned the experts’
throughwhichausercouldsimultaneouslycontroltwoagents
behaviour from any state as the agent starting positions and
inthedrivinggamewhileobservingtheentiregamestate(i.e.
goal positions are sampled from a uniform distribution over
both agent’s goals). The user observes the world through a
allgridtiles. Therefore,ifanimitatormakesamistakefrom
top-down view like in Figure 1c, with both agents, the goal
any given position, it will have seen during training how to
locations, and the pit visible. To control the first agent in
recoverinthenextposition.
the game, the user presses the ‘W’, ‘A’, and ‘D’ keys to ac-
So to investigate the effects of unseen states we bias the
celerate,turnanti-clockwise,andturnclockwiserespectively.
training data by only including cases where the target agent
Likewise,theusercontrolsthesecondagentwiththe‘UpAr-
was in one of the first two columns of the grid. The results
row’, ‘Left Arrow’, and ‘Right Arrow’ keys. Additionally,
are also shown in Figure 4b, and we see that while ECTL
to make it easier to control both agents at the same time the
maintainsitsperformance,ILsignificantlydegrades.
frame rate of the game was reduced to 2 frames per second.
This was then used to collect data from 70 episodes. Each
5.5 DrivingGameCLAP-Replace
rowofdatacomprisedtheactionsthattheusertookforeach
Moving on from the gridworld environment, we then per- agentandtheagent-specificobservations.Formally,eachrow
formedaseriesofexperimentsinthedrivinggame. First,we ofdatahadtheform(o1,o2,a1,a2),wheretisthetimestep
t t t t
trained four teams on the environment, with two agents per thatthedatawascollected,oiandaiaretheobservationsand
t t
training team. Two of the teams with the pit, and the other actionsforeachagenti.
twowithoutthepit. Figures6aand6bvisualisewhereinthe In its raw form, this data cannot be used to train CLAP
environment these agents spend their time (before reaching agents as it lacks messages. To remedy this we augmented
their goals), illustrating how the expert demonstrations may thedatabyconstructingmessagesontheprinciplethateach
notcoverthepitregionofstatespace. agent was following direct instructions from the other, so
In Figure 5a we see the effect of the number of data col- eachagenti’sactionai t becameagentj’smessagem t = ai t
lection episodeson the driving game CLAP-Replace perfor- to agent i. Now each row of the data can be transformed
mance without the pit. We find that the ECTL performance into two CLAP-Replace datasets rows, one dataset for each
starts notably better than IL, and can do well with very lit- agent that could be replaced: (o1 t,m t = a2 t,o2 t,a2 t) and
tle data. However, as more data is gathered, eventually IL (o2 t,m t =a1 t,o1 t,a1 t).
scalesbetterthanECTL.Yet,Figure5bcomplicatesthispic- Finally, we removed all rows where the user was
ture. Here we see the normalised reward comparisons be- not inputting one of the three movement actions (turn
tween ECTL and IL showing the effect of the pit on perfor- clockwise/anti-clockwiseoraccelerate)asitwashardtoco-
mance. ThepitdoesnothavemuchimpactonECTL,butthe herently press keys for both agents with exact simultaneity.
performanceofILdropsbyalargemargin,evenforthecase Furthermore, as each agent had the same observation space
with1000datacollectionepisodes–thesettinginwhichIL we could take each collected episode and construct data for
etanidrooC-Y etanidrooC-Y htgneledosipEanalternativeepisodewheretheagentroleswerereversed,re- pages1433–1434,Richland,SC.InternationalFoundation
sultinginafinaldatasetof140episodes. Formally,foreach forAutonomousAgentsandMultiagentSystems.
rowoftheoriginalrawcollecteddata(o1,o2,a1,a2),wecon-
t t t t Bullard, K., Kiela, D., Meier, F., Pineau, J., and Foerster, J.
structanotherrow(o2,o1,a2,a1).
t t t t (2021).Quasi-EquivalenceDiscoveryforZero-ShotEmer-
Withthisdata,ECTLandILagentscouldnowbetrainedin gentCommunication. arXiv:2103.08067[cs].
thesamemannerasdonefortheMAPPOtargetcommunities.
Bullard, K., Meier, F., Kiela, D., Pineau, J., and Foerster, J.
However,duetothelackofawell-definedtargetcommunity,
(2020). ExploringZero-ShotEmergentCommunicationin
theagentscouldnotbeevaluatedinthesameway.Soinstead,
EmbodiedMulti-AgentPopulations. arXiv:2010.15896.
wefocusedonspecificallyevaluatingthesignallingcapabili-
tiesoftheseagentswithanotherUIthatallowsausertoview Cope,D.andSchoots,N.(2020). LearningtoCommunicate
their agent, the possible goal locations, and a message from with Strangers via Channel Randomisation Methods. In
oneoftheartificialagents(Figure6c). Asthemessagespace TheEmergentCommunicationWorkshopatNeurIPS.
is the action space for the user, it is inherently interpretable Foerster, J., Assael, I. A., Freitas, N. d., and Whiteson, S.
and could be rendered to the user as one of the strings ‘Ac- (2016). LearningtoCommunicatewithDeepMulti-Agent
celerate’, ‘Turn Anti-Clockwise’, or ‘Turn Clockwise’. We Reinforcement Learning. In D. D. Lee and M. Sugiyama
then set up the UI to randomly switch the messenger agent and U. V. Luxburg and I. Guyon and R. Garnett, editor,
each episode from a pool of agents comprised of an ECTL Advances in Neural Information Processing Systems 29,
agent, an IL agent, and an agent that sends a random mes- pages2137–2145.CurranAssociates,Inc.
sage. Thustheuserwasalwaysunawareofwhichagentthey
Goldman, C. V. and Zilberstein, S. (2003). Optimizing in-
wereplayingwith. ThisUIrunsat10framespersecond,but
formationexchangeincooperativemulti-agentsystems. In
themessageonlyupdateseveryhalfsecond(every5frames).
Proceedings ofthe 2ndInternational Joint Conferenceon
This is to prevent it being too obvious which is the random
AutonomousAgentsandMultiagentSystems(AAMAS03),
agent as the messages would change much more frequently
AAMAS’03,pages137–144,NewYork,NY,USA.Asso-
thanfortheotheragents. Additionally,itgivestheusermore
ciationforComputingMachinery.
timetoreadandreacttothemessages.
Figure 6d shows the results of this experiment, measured Goldman, C. V. and Zilberstein, S. (2004). Decentralized
by the mean number of timesteps that it took for the user to control of cooperative systems: categorization and com-
reach the goal location while using the messages. It shows plexity analysis. Journal of Artificial Intelligence Re-
that only the ECTL agent was able to effectively communi- search,22(1):143–174.
catewiththeuser,withtheILagentconveyingthesamelack Goldman,C.V.andZilberstein,S.(2008). Communication-
ofinformationastherandompolicy. Based Decomposition Mechanisms for Decentralized
MDPs.JournalofArtificialIntelligenceResearch,32:169–
6 Conclusions 202. arXiv:1111.0065[cs].
Hu,H.,Lerer,A.,Cui,B.,Wu,D.,Pineda,L.,Brown,N.,and
In this paper, we have posed a new problem for multi-agent
Foerster, J. (2021). Off-Belief Learning. In the 38th In-
communication learning, namely the Cooperative Language
ternationalConferenceonMachineLearning,PMLR139.
Acquisition Problem (CLAP). Positioned relative to Zero-
arXiv: 2103.04000.
Shot Coordination and Ad Hoc Teamwork, the CLAP chal-
lengeistobuildanagentthatlearnsthecommunicationstrat- Hu,H.,Lerer,A.,Peysakhovich,A.,andFoerster,J.(2020).
egy of a target community via observational data. But can “Other-Play”forZero-ShotCoordination. InProceedings
also leverage general ‘environment-level skills’ that transfer
ofthe37thInternationalConferenceonMachineLearning,
toanyadhocteam. Wehaveproposedtwoapproachestothis pages4399–4410.PMLR. ISSN:2640-3498.
problem:ImitationLearning(IL)andEmergentCommunica- Hussein, A., Gaber, M.M., Elyan, E., andJayne, C.(2017).
tionpretrainingandTranslationLearning(ECTL). ImitationLearning: ASurveyofLearningMethods. ACM
WehaveshownthatECTLcanperformwellindatascarce ComputingSurveys,50(2):21:1–21:35.
scenarios, including learning to communicate with a human
Jang, E., Gu, S., and Poole, B. (2017). Categorical Repa-
user. Additionally, it can effectively compensate for expert rameterizationwithGumbel-Softmax. In5thInternational
demonstrationsthatonlycoveralimiteddistributionoverthe ConferenceonLearningRepresentations(ICLR17).
environmentstatespace. Ontheotherhand,whileILisbrit-
Jaques,N.,Lazaridou,A.,Hughes,E.,Gulcehre,C.,Ortega,
tle, it does have the potential to scale to large datasets. Fur-
P.,Strouse,D.,Leibo,J.Z.,andFreitas,N.D.(2019). So-
therworkshouldinvestigatecombiningthestrengthsofthese
cialInfluenceasIntrinsicMotivationforMulti-AgentDeep
methods.
ReinforcementLearning. InProceedingsofthe36thInter-
national Conference on Machine Learning, pages 3040–
References
3049.PMLR. ISSN:2640-3498.
Barrett, S., Agmon, N., Hazon, N., Kraus, S., and Stone, Kumar, A., Hong, J., Singh, A., and Levine, S. (2022).
P. (2014). Communicating with unknown teammates. In Should I Run Offline Reinforcement Learning or Behav-
Proceedings of the 2014 international conference on Au- ioralCloning? InProceedingsoftheInternationalConfer-
tonomous agents and multi-agent systems, AAMAS ’14, enceonLearningRepresentations(ICLR22).Lazaridou, A. and Baroni, M. (2020). Emergent A Appendix
Multi-Agent Communication in the Deep Learning Era.
A.1 EnvironmentDetails
arXiv:2006.02419[cs].
Goal Communications Gridworld. Each agent has a goal
Li, Y., Zhang, S., Sun, J., Du, Y., Wen, Y., Wang, X.,
squareinthegridthattheyneedtoreach. Atthestartofeach
and Pan, W. (2023). Cooperative Open-ended Learning
episode,thisgoalissampledfromauniformdistributionover
FrameworkforZero-ShotCoordination. InProceedingsof
thesquares.Therearefiveactions:moveup,down,right,left,
the 40th International Conference on Machine Learning,
orstayput. Agentsdonotphysicallyinteractonthegrid,and
pages20470–20484.PMLR. ISSN:2640-3498.
socanbeinthesamepositions. Foreachagent,iftheyhave
Lowe, R., Foerster, J., Boureau, Y.-L., Pineau, J., and not reached their goal then the team is deducted -1. When
Dauphin,Y.(2019).OnthePitfallsofMeasuringEmergent theyreachthegoal,arewardof+1isgiven,andiftheyhave
Communication. InThe18thInternationalConferenceon already reached it in the past then no penalty or reward is
AutonomousAgentsandMultiagentSystems(AAMAS). given. An episode ends once all agents have reached their
goalsoratimelimitisreached. Eachagent’sobservationis
Maddison,C.J.,Mnih,A.,andTeh,Y.W.(2017). TheCon-
theconcatenationofthefollowingvectors:
crete Distribution: A Continuous Relaxation of Discrete
Random Variables. In 5th International Conference on • A binary vector of nine numbers, indicating if the goal
Learning Representations (ICLR 17), Palais des Congre`s isinoneoftheninesquaresneartheagent.
Neptune,Toulon,France. • One-hot encodings of the (gˆ x,gˆ y) coordinates corre-
spondingtoa‘closeguess’ofanotheragent’sgoal. This
Mirsky,R.,Macke,W.,Wang,A.,Yedidsion,H.,andStone, close guess is sampled at the start of an episode from
P.(2020). APennyforYourThoughts:TheValueofCom- auniformdistributionoflocationswithinonetileaway
munication in Ad Hoc Teamwork. In Proceedings of the fromthetruegoal.
Twenty-Ninth International Joint Conference on Artificial • One-hotencodingsofeachofthe(p ,p )positioncoor-
x y
Intelligence, pages 254–260. International Joint Confer- dinatesforeachagentintheenvironment.
encesonArtificialIntelligenceOrganization.
Forallexperimentswekeepthegridsizefixedat(5 5)
Oliehoek, F. A. and Amato, C. (2016). A Concise Intro- andthemaximumnumberoftimestepsat10. ×
ductiontoDecentralizedPOMDPs. SpringerInternational GoalCommunicationsDrivingGame.Inthisgame,each
Publishing, Cham. Series Title: SpringerBriefs in Intelli- agentsteersandacceleratesabodyinacontinuousgrid(Fig-
gentSystems. ure1c). Theagentsmustnavigatetooneofeightfixedgoal
locations,knownbytheirpartnerandselectedatrandomfor
Ossenkopf, M. (2020). CoMaze: A cooperative game for
eachepisode.Eachagentobservesthepositions,previouspo-
zero-shot coordination. In The Emergent Communication
sitions,angles,andvelocitiesofalltheagents,representedas
WorkshopatNeurIPS.
vectorsR2. Eachagentobservesthegoallocationofanother
Sarratt, T. and Jhala, A. (2015). The Role of Models and agent,givingrisetotheneedtocommunicate.
CommunicationintheAdHocMultiagentTeamDecision There are four actions that an agent can take: (1) turn
Problem. InProceedingsoftheThirdAnnualConference clockwisebyθ radians,(2)turnanti-clockwisebyθ radians,
onAdvancesinCognitiveSystems. (3) accelerate by a units per second/second. The value θ is
fixed at π/8 and a at 1.0. Over each timestep, 0.2 seconds
Stone,P.,Kaminka,G.A.,Kraus,S.,andRosenschein,J.S.
of simulated time elapse, i.e. the game runs in ‘real-time’
(2010). AdHocAutonomousAgentTeams: Collaboration
at 5 frames per second. The team has a maximum of 200
without Pre-Coordination. In Proceedings of the Twenty-
timestepstocompletethetask, ortheepisodeendswhenall
FourthAAAIConferenceonArtificialIntelligence.
thegoalshavebeenreached. Therearetwoscenariosforthis
Sukhbaatar, S., Szlam, A., and Fergus, R. (2016). Learn- environmentdefinedbythepresenceorabsenceofacircular
ing multiagent communication with backpropagation. In regioncalledthe‘pit’inthecentreoftheworldwherealarge
Proceedingsofthe30thInternationalConferenceonNeu- penaltyisappliedforeveryagentwithintheregion.
ral Information Processing Systems, pages 2252–2260, Each agent’s reward at each time step is computed from
Barcelona,Spain.NeuralInformationProcessingSystems. fourcomponents:
Wagner, K., Reggia, J. A., Uriagereka, J., and Wilkinson, 1. c pit isthepenaltyfortheagentbeinginthepitlocation,
G.S.(2003).ProgressintheSimulationofEmergentCom- ifthepitscenarioisselected.
munication and Language. Adaptive Behavior, 11(1):37– 2. α∆d ,where∆d isthechangeinclosestdistance
min min
69. Publisher: SAGEPublicationsLtdSTM. tothegoalreachedduringtheepisode. αisahyperpa-
rameter set to 200. This term is zero if the distance to
Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A.,
thegoalischanging,butitisalsonotgiveniftheagent
and Wu, Y. (2022). The Surprising Effectiveness of PPO
isinthepit. However,the‘closestdistancetothegoal’
in Cooperative Multi-Agent Games. In 36th Conference
isstillupdatedwhileinthepit,sotheagentispenalised
onNeuralInformationProcessingSystems(NeurIPS2022)
directly and indirectly through the lost opportunity of
TrackonDatasetsandBenchmarks.,NewOrleans,USA.
beingawardedthebonus.(a) MAPPO hyperparameters for training teams on the (b)MAPPOhyperparametersfortrainingteamsonthedriving
gridworldproblem. problem.
Hyperparameter Value Hyperparameter Value
Messagedimension(Σ) 5 Messagedimension(Σ) 16
| | | |
Optimiser Adam Optimiser Adam
Learningrate 0.0005 Learningrate 0.0005
AdamBetas (0.9,0.999) AdamBetas (0.9,0.999)
Gumbel-SoftmaxTemperature 2.0(noannealing) Gumbel-SoftmaxTemperature 5.0(noannealing)
SGDMinibatchSize 2,048 SGDMinibatchSize 2,048
TrainingBatchSize 10,000 TrainingBatchSize 200,000
SGDNumIterations 5 SGDNumIterations 10
ValueFunctionClipParameter 10 ValueFunctionClipParameter 10
ValueFunctionLossCoef 0.25 ValueFunctionLossCoef 0.25
Lambda(GAE/PPO) 0.95 Lambda(GAE/PPO) 0.95
KLCoef(PPO) 0 KLCoef(PPO) 0
encHiddenLayerSizes [256,256] encHiddenLayerSizes [256,256]
πeHiddenLayerSizes [256] πeHiddenLayerSizes [256]
vf HiddenLayerSizes [256,256] vf HiddenLayerSizes [256,256]
Table2: Signalling/Listeningtraininghyperparameters(forbothIL create joiner/target community pairs. We trained joiner
andECTL).
agentsusingILandECTLforeachpossiblereplaceableagent
(2agentsindrivingand3agentsingridworld), andthenfor
Hyperparameter Value eachconfigurationweconducted3trialsandperformedeval-
uations. Each configuration runs the entire CLAP-Replace
Optimiser SGDwithmomentum
Learningrate 0.001 andtrainingpipelinefromscratch(i.e. independentdatacol-
lectionandrandomseeds).Eachtrainedjoineragentwasthen
Momentum 0.9
MinibatchSize 1024 evaluated for 500 episodes in the zero-shot CLAP-Replace
WeightDecay 1 10−5 setting. ForFigure5a,foreachsamplealongthex-axis(dif-
× ferentN values),thisentireprocesswasrun.Thusthere
Epochs 1500 collect
was9N samples 2communitypermutations 2pos-
collect
× ×
siblereplacementagents 3trials,totally432trainingruns.
×
3. c is a penalty given at each time step to encourage
time
completingthetaskquickly. Settoaconstantof0.1.
4. r is a bonus given for reaching the goal. It is set
goal
to500.0andonlyawardedonceatthetimestepthatthe
goalisreached.
After an agent has already reached its goal, it is always
awardedzerorewardunlessitisinthepit.
A.2 TrainingHyperparameters
All reinforcement learning was done using RLlib and Py-
torch. ThehyperparametersforMAPPOtrainingineachen-
vironmentcanbefoundinTables1aand1b.Notethekeydif-
ferencesbeingthetemperatureoftheGumbel-Softmaxcom-
munication channel, which was increased to improve learn-
ing,andthesizeofthetrainingbatchthatneededtodramati-
callyincrease. ILandECTLjustusePytorchfortrainingand
RLlibforevaluation. ForILbothcomponentsoftheπ are
im
feed-forward networks with hidden layer sizes [256]. Like-
wise,forECTL,bothtranslationfunctionshavehiddenlayer
sizes [256, 256]. The full hyperparameters for training with
thedifferentmethodsaregiveninTable2.
A.3 EvaluationMethodology
For evaluation on the CLAP-Replace tasks, we took each
trained community and used each permutation of teams toTrainingStatisticsfor111b500000
5 11
−
10
10 −
9
15
− 8
−20 7
6
25
−
5
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Timesteps 106 Timesteps 106
× ×
Figure7:Trainingcurvesforonetrainingrunonthegridworldenvironment.
TrainingStatisticsforb20cd00000
5 11
−
10
10 −
9
15
− 8
20 7
−
6
25
−
5
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Timesteps 106 Timesteps 106
× ×
Figure8:Trainingcurvesforonetrainingrunonthegridworldenvironment.
TrainingStatisticsfordc82500000
200
1400
1200 190
1000 180
800
170
600
400 160
200
150
0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Timesteps 108 Timesteps 108
× ×
Figure9:Trainingcurvesforonetrainingrunonthedrivingenvironment,inthe‘pit’scenario.
draweRnaeM
draweRnaeM
draweRnaeM
htgneLedosipEnaeM
htgneLedosipEnaeM
htgneLedosipEnaeMTrainingStatisticsforde02c00000
200
1400
1200 190
1000 180
800
600 170
400 160
200
150
0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Timesteps 108 Timesteps 108
× ×
Figure10:Trainingcurvesforonetrainingrunonthedrivingenvironment,inthe‘pit’scenario.
TrainingStatisticsfor3fba100000
1500 200
190
1250
180
1000
170
750
160
500
150
250
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Timesteps 108 Timesteps 108
× ×
Figure11:Trainingcurvesforonetrainingrunonthedrivingenvironment,inthe‘nopit’scenario.
TrainingStatisticsfor4a79d00000
200
2000 190
180
1500
170
1000
160
500 150
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Timesteps 108 Timesteps 108
× ×
Figure12:Trainingcurvesforonetrainingrunonthedrivingenvironment,inthe‘nopit’scenario.
draweRnaeM
draweRnaeM
draweRnaeM
htgneLedosipEnaeM
htgneLedosipEnaeM
htgneLedosipEnaeM