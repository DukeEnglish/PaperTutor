27thComputerVisionWinterWorkshop
TermeOlimia,Slovenia,February14–16,2024
Enhancement of 3D Camera Synthetic Training Data with Noise Models
Katar´ınaOsvaldova´1 Luka´sˇ Gajdosˇech1 ViktorKocur1 MartinMadaras1,2
1FacultyofMathematics,PhysicsandInformatics,ComeniusUniversityinBratislava
2SkeletexResearch,Slovakia
lukas.gajdosech@fmph.uniba.sk, viktor.kocur@fmph.uniba.sk, madaras@skeletex.xyz
Abstract. Thegoalofthispaperistoassesstheim- thetic data creation may involve the intentional ad-
pact of noise in 3D camera-captured data by mod- dition of artificial noise. Noise is, however, a com-
eling the noise of the imaging process and apply- plextopic. Countlessfactorsinfluenceitsbehaviour,
ing it on synthetic training data. We compiled a fromthetechnologyemployedbythedevice,design
dataset of specifically constructed scenes to obtain andquality,throughenvironmentalvariables,suchas
a noise model. We specifically model lateral noise, ambient light and temperature, to properties of the
affectingthepositionofcapturedpointsintheimage scene. The presence of some noise can be avoided,
plane, and axial noise, affecting the position along andinsomecases,thenoisecanbemodelled.
the axis perpendicular to the image plane. The esti- Noise has been the topic of numerous studies [1,
mated models can be used to emulate noise in syn- 5, 9, 17, 18]. Most of them, however, focus on in-
thetictrainingdata. Theaddedbenefitofaddingar- vestigationofonespecificdeviceorprinciple. Some
tificial noise is evaluated in an experiment with ren- works focus on theoretical models of noise. These
dered data for object segmentation. We train a se- modelsserveasaguideforinvestigationofthenoise
ries of neural networks with varying levels of noise of specific devices, as the parameters of the devices
inthedataandmeasuretheirabilitytogeneralizeon neededfortheemploymentofthemodelsareusually
real data. The results show that using too little or notpubliclyavailableandaresubjecttotradesecrets.
too much noise can hurt the networks’ performance Axialandlateralnoiseof3Dcameraswerechosen
indicating that obtaining a model of noise from real for a comprehensive investigation, as the theoreti-
scannersisbeneficialforsyntheticdatageneration. calmodelsdescribingtheirbehaviourrelyheavilyon
knowledge of publicly undisclosed parameters. We
have collected a dataset of several thousands scans
1.Introduction
from three different devices to fit probabilistic mod-
elsofnoisewithrespecttothedistanceoftheimaged
In the past, 3D cameras were rare and expensive.
objectsandanglesoftheirsurface.
Nowadays,aplethoraof3Dcamerasofvariousqual-
Wealsoperformedanexperimentwithasegmen-
ity and price are commercially available. As is the
tation neural network trained on synthetic data. We
case with any camera, range data captured by these
varied the amount of noise added to the generated
devicessufferfromthepresenceofnoise.
data. Evaluation on real scans shows that using too
The intersection of machine learning and com-
littleortoomuchnoisecanhurtthenetwork’sperfor-
putervisionhasemergedasadynamicfieldwithdi-
mance. The knowledge of noise parameters of real
verse applications. Notably, the synthesis of these
devices can thus be beneficial when employing syn-
domains has become increasingly prominent. Ma-
theticdatafortrainingdeepneuralnetworks.
chinelearningrequirestrainingdata,manualcreation
ofwhichisnotonlytime-consumingbutalsoexpen-
2.RelatedWork
sive. The advent of computer graphics has facili-
tated the cost-effective generation of synthetic data. Various approaches have been explored to en-
Nevertheless, synthetic data lacks the inherent noise hance the accuracy and efficiency of 3D scanning
presentinreal3Dcamera-captureddata,leadingtoa technologies, with a particular focus on training
domain gap. To bridge this gap, the process of syn- datasets that fuel machine learning models behind
4202
beF
62
]VC.sc[
1v41561.2042:viXratheprocessingpipelines. Understandingtheartifacts 2.3.SourcesofNoiseandErrorsin3DScanning
inherent in scanning technologies is crucial for gen-
Scanning devices in real life are prone to various
erating training data that accurately reflects the real
sources of noise and errors, related to the environ-
world’s variance. Different 3D scanning methods,
mentalconditionsandlimitationsofunderlyingtech-
such as structured light triangulation and time-of-
nology.
flight measurements, introduce unique artifacts that
Temporal noise in 3D scanning devices refers to
can impact data quality. Some common artifacts in-
variations in the captured data over time, introduc-
cludenoise,distortions,andsystematicerrors.
ing fluctuations or inconsistencies in the measure-
ments. Temporal noise is often correlated between
2.1.StructuredLightScanning consecutive scans and can arise from a range of
factors, including electronic instability, sensor char-
StructuredLight(SL)triangulationisbasedonthe
acteristics, or environmental conditions [17]. The
principles of two-view geometry. One camera is re-
amount of temporal noise can also be influenced
placed by a light source that projects a sequence of
by colour and material properties of the observed
patterns onto the scene. The patterns projected get
objects [9, 22, 24, 25] and the geometry of the
deformed by the geometric shapes of the objects in
scene[17].
thescene. Acamerasituatedatafixeddistancefrom
Thepresenceofadifferentsourceofsimilarradia-
the projector then captures the scene with the pro-
tioncaninterferewiththedevice’sabilitytocorrectly
jectedpattern[21]. Byanalysingthedistortionofthe
calculate the distance of the objects in the scene.
pattern, information about position of the objects in
Suchinterferencecanbecausedbyambientlight[8],
thescenecanbedetermined.
radiationfromotheractiveimagingdevices[2,3]or
Various patterns have been proposed [20]. For even radiation emitted from the device itself when
example, the Kinect v1 camera uses a fixed dot thescenecontainsreflectivesurfaces[20].
pattern [10]. Photoneo’s MotionCam-3D camera Systematic errors may also arise during 3D scan-
utilisesparallelstructuredlighttechnologywhichen- ning. This type of errors result in consistent differ-
ables the device to capture the scene depth at high ences between the scans and the actual scene geom-
resolutionandframe-rateatthesametime[16]. etry. For SL cameras, it is mainly caused by inad-
equate calibration, low resolution, and coarse value
quantisation [14]. In the case of ToF cameras, the
2.2.Time-of-FlightScanning
measurement is based on mixing of different opti-
Time-of-Flight (ToF) measurement technology is cal signals and approximation of their shapes. The
based on the principle of calculating the distance of mentionedapproximationisoneofthecontributions
anobjectinthescenebymeasuringthetimeittakes to the effect referred to as wiggling [20], periodic
foranemittedsignaltotraveltotheobjectandback. change of the systematic error with distance. Both
The distance is calculated from measurements of SL and ToF cameras may also suffer from temper-
phase difference [13]. The exact type of waves em- ature drift [22, 25]. Systematic error of devices can
ployedvariesbasedontheapplication. RADARand bemodelledwellwhenpreciseinformationaboutthe
LIDAR include ToF measurements [21]. The most sceneisknown[22].
commonapproachinToFcamerasisthecontinuous-
2.4.TrainingNNsusingSyntheticData
waveintensitymodulationIRLIDAR[12]. Thedis-
tanceiscalculatedfromtheobservedphasedelayof Inthecontextofmachinelearningandneuralnet-
theamplitudeenvelopeofthereflectedlight[22]. work training, the fusion of synthetic data genera-
tion, domain randomization and data augmentation
The range and accuracy of ToF devices are pri-
can be leveraged as powerful tools to avoid expen-
marily influenced by the wavelength and energy of
sivecreationofrealdatasets.
theemittedlight,necessitatingsafetyprecautions,in-
Awidelyrecognizedtoolforgeneratingsynthetic
cludingenergycappinginhumanenvironments[21].
data is for example NVIDIA replicator1. Synthetic
However, such devices often exhibit reduced preci-
data can further be enhanced by GANs [7], analyti-
sion outdoors due to sunlight interference, as sun-
light has higher power compared to the emitted sig- 1https://developer.nvidia.com/omniverse/
nal[13,22]. replicatorwhile the axial noise can be observed in the individ-
ual depth values themselves. An example of axial
noiseispresentedinFigure1b.
Multiple factors are known to influence axial
noise, from geometry of the scene to properties of
thematerialofthesurfacesinthescene[17,18,22].
For SL cameras, according to pin-hole camera
(a) (b)
model and the disparity-depth model, the standard
deviation of axial noise σ increases quadratically
Figure 1: (a) Cropped range image of a white pa- z
withincreasingdepthandcanbecalculatedas[17]:
per (blue rectangular area) positioned 1.25 m away
from the camera at a 20° angle captured by Kinect (cid:18) (cid:19)
m
v1. White pixels represent missing values. Lateral σ = z2σ , (1)
z ρ
fb
noise can be seen at the paper boundaries which are
straight in the real scene. (b) Cropped range image wherez referstodepth, σ tothestandarddeviation
ρ
ofaplanarwallcapturedbyKinectv2at90cmdis- ofnormaliseddisparityvalues,f tothefocallength,
tancewithnotableaxialnoise. b to the length of the baseline, and m to the param-
eterofinternaldisparitynormalisation. Inthispaper
calemulationofknownimagingerrors,artifactsand weestimatethenoiselevelsforbothaxialandlateral
noise [15], or domain randomization which intro- noisedirectlyfromtheobserveddatawithoutrelying
duces variability by altering key factors such as ob- onknowledgeofthecameraintrinsics.
ject properties, lighting conditions, and camera per-
3.2.CustomDataset
spectives[23].
In order to estimate the levels of lateral and axial
3.Estimating3DCameraNoiseParameters noise in various 3D scanning devices we collected a
custom dataset. The dataset consists of scenes with
Inthissectionwedescribetheprocessofestimat-
a large planar surface (white rectangular cardboard)
ingtheparametersoftwotypesofnoiseoccurringin
undervariousrotations.
real 3D scans and their dependence on the distances
Wecapturedthesceneusingthree3Dcameras:
ofobjectsaswellastheangleoftheimagedsurfaces.
In section 4 we perform an experiment showing that • Kinect v1 utilises IR SL projector combined
theestimatedparameterscanbeusedtoimprovethe with a monochrome CMOS sensor for depth
performanceofmodelstrainedonsyntheticdata. sensing,supplyingrangeimageswith640×480
resolution at 30 fps. Its default depth range is
3.1.LateralandAxialNoise
0.8m-4.0m,0.4m-3.0minnearmode.
Wespecificallyinvestigatetwotypesofnoise: lat-
• Kinect v2 employs a ToF camera for depth
eral and axial. These are the two most dominant
sensing. Compared to its predecessor, it has a
typesofnoisepresentinreal3Dscans.
wider field of view and offers depth measure-
ments with greater accuracy and wider depth
Lateral noise Lateral noise refers to error in the range,0.5m-4.5m. Theresolutionoftherange
reported position in the camera’s xy-plane. Even imagesis,however,slightlysmaller,512×424.
though lateral noise affects all measurements, it is
• MotionCam-3D camera by Photoneo is based
most visible at object boundaries, as illustrated in
on SL range sensing. Thanks to parallel struc-
Figure 1a. Existing research [18, 17] suggests the
tured light technology [16], the camera is able
distance of the object and its angle influences the
to capture dynamic scenes. Overall, the cam-
amountoflateralnoise.
era offers resolution up to 1680 × 1200. The
MotionCam-3Dcanrunintwodifferentmodes,
Axial noise Axial noise refers to noise orthogonal the static scanner mode where the resolution
to the imaging plane, parallel to the z-axis of the andscanningtimearehigher,anddynamiccam-
camera. The lateral noise presents itself by alter- era mode where the scanning time and the out-
ingthepositionsofdepthvaluesintherangeimage, putresolutionarelower.(a)stand (b)Kinectv1andv2 (c)MotionCam-3D
Figure4: Normalisedhistogramsoflateralerrorval-
Figure2: Physicalsetupforcapturingsurfaceatdif-
ues. Collected from 200 images by Kinect v1 and
ferentanglesanddistances.
Kinectv2,and100imagesbyMotionCam-3D.Each
histogramrepresentsascenecontainingthewhitepa-
perat0°angle. Thedistancesdifferforeachcamera,
being the shortest at which the paper was captured
completely;1mforKinectv1,0.75mforKinectv2,
(a)Kinectv1 (b)Kinectv2 (c)MotionCam-3D 0.5 m for MotionCam-3D. Each histogram contains
fittednormaldistribution(dashedline).
Figure 3: Range images captured by devices. The
scenecontainsawhitepaperat1mdistanceand30°
anglecapturedasportrayedinFigure2.
To mitigate the effects of thermal drift the de-
viceswerewarmedupbycapturingrangeimagesofa
blankwallin1-minuteintervalsfor60minutesprior
tocollectingthesamplesinthedataset.
To investigate the influence of surface distance
and angle on noise a set of range images contain-
ing a white planar paper at various positions was
captured. To minimise any distortion of the paper,
heavy-weight card stock was mounted on a rigid
stand,displayedinFigure2a. Thestandiscomprised
oftwoplasticboardsmountedtotwowoodenbeams Figure 5: Visualisation of the relationship between
attachedtoawideplasticpipewithaplug. Therub- the standard deviation of lateral noise, measured in
ber seal between the pipe and the plug was shaved mm, surface angle (left column), and distance (right
to allow smooth rotation while preserving the posi- column). Eachrowcontainsdatafromadifferentde-
tion when idle. The stand was constructed to have vice. The plots in each column and row share the x
the centre of rotation in the horizontal centre of the and y axes respectively. In the plots of the left col-
paper,withmarkingsnotingtherotationangle. umn,theunderlyinganglevaluesareallmultiplesof
With a mounted paper, this stand was positioned 10. A random shift of horizontal position between
at various distances from the cameras and was ro- frameswasaddedforlegibility.
tatedforthecaptureofvariousscenes. Foreachsuch
stationary scene 200 range images were captured by In order to estimate the effects of angle and dis-
each camera. To minimise the impact of temporal tance of planar surfaces on noise levels we segment
noise, for each set of range images capturing one the paper in the range images using manual annota-
scene,anaveragerangeimagewascomputedbyav- tioninconjunctionwiththeCannyedgedetection[4]
eraging captured depth values for individual pixels. andHoughtransformation[6].
To ensure all the cameras captured the same scene,
3.3.LateralNoiseEstimation
the entire process was repeated, as all the cameras
did not reasonably fit into the same space at once. To estimate the lateral noise levels we focus on
ThesetupsareportrayedinFigure2,whileexamples thepaperboundary. Wefirstestimatethepositionof
ofcapturedrangeimagesaredisplayedinFigure3. the boundary by fitting a line using orthogonal dis-tanceregressionontheedgepixels. Weperformthis
regression jointly for all scans with a given scene
setup. We then calculate the distances of the edge
pixelsfromtheestimatedboundaryline.
Examplehistogramsofthedistancesfromtheline
fitareshowninFigure4. TheKolmogorov-Smirnov
test rejected the normality of the distribution, prob-
ably due to the effects of quantization in pixel posi-
tions. However, we note that the error distributions
closelyresemblenormaldistributions.
AsseeninFigure5,theleveloflateralnoisedoes
notsignificantlychangewithsurfaceangle. Previous
researchindicateshyperbolicincreaseoflateralnoise
at angles greater than 60° for Kinect v1 [18]. Our
Figure 6: Visualisation of the relationship between
experimentsdidnotindicatesuchincrease,however,
standard deviation of axial noise, the surface angle
thankstolargenumberofinvalidpixels,wewerenot
(left column), and the distance (right column). Each
able to capture data for angles greater than 70°, and
row contains data from a different device. The plots
subsequently extract lateral noise. MotionCam-3D
ineachcolumnandrowsharexandyaxes.
exhibited similar inability to capture surfaces at ex-
treme angles. Contrastingly, Kinect v2 had no prob-
muchclosertothecentrethantheleftedge. Onmul-
lemwith80°angleandexhibitednoincreaseinnoise
tipleoccasions,therightedgewascapturedasaper-
withincreasingangle.
fectlyverticallineinall200imagescapturedforthe
Aslightdeclineinthestandarddeviationwithris-
scene, while the left edge was not. This can be ob-
inganglecanbeobserved. Wenotethatthisdecline
served in Figure 5 as some values are reported with
may not be caused by the change in angle directly,
standard deviation of 0. From our limited data, a
but as a result of presence of other noise causing a
correlation of lateral noise with the pixel’s position
great number of invalid pixels and thus preventing
seems likely. Further experimentation would be re-
lateralnoiseanalysis. Thistypeofnoiseincreasesby
quiredtofullyexplorethisrelationship.
rising distance, as surfaces with progressively lower
The results show that MotionCam-3D exhibits
angleswiththecameraviewareaffected. Hence,sur-
overalllowerlevelsoflateralnoisethanbothKinect
faces at greater angles are harder to measure from
cameraswithKinectv2achievinglowernoiselevels
greater distances, leaving fewer samples resulting in
ofthetwo.
lowerstandarddeviation.
3.4.AxialNoiseEstimation
Unlike in the case of the paper’s angle, the stan-
dard deviation of the errors is not constant through- To obtain the distributions of axial noise we first
out all distances, as seen in Figure 5. Notewor- performed low-pass filtering jointly on all scans of
thy is the elevated standard deviation at shorter dis- scenes with the same scanner distances and angles.
tances, between 50 cm and 1 m, for Kinect v2 and We then calculated the standard deviations of dif-
MotionCam-3D. Kinect v1 was not able to capture ferences of depth from the values obtained by fil-
thepaperatsuchshortdistancesatall. Thestandard tering. We have opted for this approach as despite
deviation of errors in millimetres increases linearly using heavy stock paper, the paper surface was not
with increasing distance, at different rate for each perfectly planar. We have also tested different types
camera, depending on the camera’s physical param- offilteringwhichledtosimilarresults.
eters[18]. Notethatthisisequivalenttothestandard Similar to lateral noise, the relationship between
deviationremainingconstantundervaryingdistances angleanddistanceonthestandarddeviationofnoise
whenmeasuredinpixelcoordinates. has been investigated. The results are visualised in
By aiming to capture the scenes simultaneously Figure 6. MotionCam-3D exhibits least axial noise,
withmultiplecameras,thepositionofthepaperwas followedbyKinectv2andKinectv1withthegreat-
not always perfectly centred for all cameras. As a estmagnitudeofnoise.
result, for the Kinect v2, the paper’s right edge was From the right column in Figure 6, the influence4.1.RealEvaluationDataset
To create our real data we manufactured five 3D
modelsoftheStanfordArmadillo. Theobjectswere
printed on J750 using the Vero family of materials.
Thisallowedus tocapture55real scansusing3dif-
ferent variants of the MotionCam-3D. The real data
contain samples from a close distance of around 70
cm, mid-range captures from around 100 cm, and
longer-range shots from 150 cm. This should model
thevarioususecasesofthe3Dscanningdevice,with
Figure 7: Fitted polynomial function of degree 2 for varying amounts of noise. Apart from the Armadil-
axial noise of MotionCam-3D, displayed as the sur- los, various cuboid-shaped objects were included in
face, with the measured values, displayed as points thescene,someofwhichhadaslightlyreflectivema-
colored by respective standard deviation of the sam- terial causing further noise. The real data was split
ple. intoavalidationsetwith20samples,atestsetof25
captures,and10sampleswereusedfortraining.
of surface distance on the standard deviation can be
4.2.TrainingData
clearly seen for both SL cameras, Kinect v1 and
To evaluate the benefit of adding axial and lateral
MotionCam-3D.ForKinectv2camera,thestandard
noise into synthetic data, we have rendered training
deviationdoesnotchangemuchwithincreasingdis-
data for the task of object segmentation using spe-
tancecomparedtotheothertwocameras. Theinflu-
cialized data generator [11], implemented to simu-
ence of surface angle can also be seen. In the case
late the MotionCam-3D and other Photoneo scan-
of Kinect v1, the values of standard deviation seem
ners. We are dealing with a simplified setting - seg-
tofluctuateunpredictablywithchangingangle. This
mentation of a singular object, the Armadillo fig-
may be caused by different sources of noise such as
urine.2 Due to a current limitation of the renderer,
systematicnoisearisingfromtheimagingprocess.
we were unable to account for the angle of the sur-
face,thustheamountofnoiseisonlyaffectedbydis-
3.5.NoiseModels
tance. Thissimplificationshouldnothindertheeval-
In previous subsections we have shown that stan- uation,asperouranalysisthesurfaceangledoesnot
dard deviations of both types of noise depend on greatlyaffectthestandarddeviationofthenoise,but
both the distance of objects to the scanners as well the amount of missing samples instead. Some sam-
as the angle of the imaged surface. To model the ples also contain cuboid-shaped walls of containers,
noisewe fitthe datashownin Figure5and Figure6 which served as boundaries for the physical simula-
withdegreetwopolynomialsusingtheordinaryleast tionofplacingtheArmadillosintothescene.
squares method. The resulting coefficients for both The dataset contains 180 synthetic samples. Ad-
lateral σ and axial noise σ are in Table 1. The re- ditionally, we have included 10 real samples, which
L z
sulting fit for the axial noise of MotionCam-3D is helped to avoid over fitting and permitted longer
showninFigure7. . training. The dataset was designed to empirically
evaluate the generalization of UNet-like CNN [19].
As different types of noise are abundant in the real
4. Enhancement of Synthetic Training Data
samples,anetworktrainedoncleanrendereddatais
withEmulatedNoise
oftenunabletogeneralize.
In this section we present an experiment that ver-
4.3.Training
ifies the importance of selecting an optimal level
A 4-channel input image with surface normals
of noise when generating synthetic training data for
and range image was used as an input to the U-Net
deepneuralnetworktraining. Weevaluatetheeffects
shaped CNN. We have performed purely stochastic
ofnoiseonasimplesegmentationtask. Wetrainthe
networksonsyntheticdataandevaluatethemonreal- 2http://graphics.stanford.edu/data/
worldscans. 3Dscanrep/Table1: Fittedstandarddeviationsoflateralnoise(σ -inpixels)andaxialnoise(σ -inmillimeters). Param-
L z
eterθ representsthesurfaceangleandz thedistancefromthecameracenter.
Kinectv1 σ L(z,θ)[px]= 0.94+4.51·10−5·z+6.20·10−4·θ
Kinectv2 σ L(z,θ)[px]= 0.736−6.20·10−4·z+5.35·10−3·θ+2.13·10−7·z2−1.40·10−6·z·θ−4.13·10−5·θ2
MotionCam-3D σ L(z,θ)[px]= 0.915−6.91·10−5·z+2.84·10−3·θ
Kinectv1 σ z(z,θ)[mm]= −0.422+6.89·10−4·z+2.24·10−2·θ+5.99·10−7·z2−2.70·10−6·z·θ−1.52·10−4·θ2
Kinectv2 σ z(z,θ)[mm]= 1.17+9.72·10−5·z−1.37·10−2·θ−6.35·10−9·z2+7.86·10−6·z·θ+1.17·10−4·θ2
MotionCam-3D σ z(z,θ)[mm]= 0.599−1.43·10−3·z−8.94·10−3·θ+8.84·10−7·z2+1.27·10−5·z·θ+2.75·10−5·θ2
(a)M =0 (b)M =1 (c)M =2 (d)M =3
n n n n
Figure8: Syntheticsamplefromourdatawithvaryingamountoflateralnoiseaddedtorangeimages.
training with batch size = 1, Adam optimizer with results. Wehypothesizethatbytheslightincreaseof
10−4 initial learning rate, and binary cross-entropy theσ arisingfromanalysis,othernoisetypesare
synth
as the loss function. The number of epochs was de- implicitly modeled, effectively making the network
termined by a training callback. It observed the IoU morerobusttonoiseuncapturedinthesyntheticpor-
on the real validation set and picked the best model, tion of the training data. This results indicates that
whichwasthenevaluatedonthetestset. addingslightlymorenoisethanestimatedallowsthe
network to be more robust while retaining good ac-
4.4.VaryingNoiseLevels
curacy.
To verify the effect of various levels of emulated
The results also show an interesting second peak
lateralandaxialnoisewetrainmultiplemodelseach
at M = 2. By qualitative evaluation, we have
n
usingadifferentlevelofaddednoise. Tocontrolthe
verified that the network performance was better for
noisewedefineanoisemultiplicator:
scans captured from larger distances. In such cases,
σ synth large amounts of interference noise is present, aris-
M = , (2)
n
σ ingfromthecharacterofstructuredlighttechnology
est
and the presence of ambient lightning. This case is
where σ denotes the estimated standard deviation
est
visualized in Figure 10, where the network trained
of noise as presented in subsection 3.5 and σ
synth
on data without noise wrongly segments the rough,
denotes the standard deviation of noise added to the
noisysurfaces.
training data. Note that the estimated standard de-
viations of noise depend on the surface angles and Ontheotherhand,wehavesamplescapturedfrom
distancesandthetypeofnoise(axial,lateral),butthe a close distance, where the captured surfaces are
ratioM isindependentofthesevariables. Thevalue smoother and objects have sharper boundaries. In
n
ofM = 0indicatesnonoiseaddedandM = 1in- these situations, networks trained with greater noise
n n
dicatesnoiseaddedaccordingtothelevelsestimated levels (M ≥ 1.5) have trouble with the segmenta-
n
intheprevioussection. Theeffectsofvariousvalues tion of fine details and manage to only detect larger
ofM ontheproducedsyntheticsamplesareshown blobs of the objects, see Figure 11. By a com-
n
inFigure8. bined quantitative and qualitative analysis we con-
clude from our experiment that the network trained
4.5.Results
on data with noise M = 1.25 delivers the most ro-
n
We evaluated models for varying values of M bustperformanceovervariouscases. Lastly,wenote
n
on the real testing data. Figure 9 shows the seg- that setting M = 1.75 failed to both segment fine
n
mentation IoU metric for the evaluated models. The details in close-shots and was not as robust as net-
networktrainedonsyntheticdatawithslightlymore workstrainedformoreextremenoise,deliveringthe
noise than estimated (M = 1.25) achieved the best weakestperformanceoverall.
n0.70
0.65
0.60
0.55
0.50
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 3.25 3.50 3.75 4.00
Mn
Figure 9: Performance of neural network for object segmentation trained over data with varying amounts of
noise. Multiplier M of the sigma from our analysis is shown on the horizontal axis with 0.25 interval. The
n
resulting average IoU on a test set of real captures is visualized by height of the bars. For clarity, the top 6
valuesaredepictedingreen,6worstareinredandtheremaining5middleresultsareinorange.
Our data3 and code4 used for noise analysis and
networktrainingispubliclyavailable.
5.Conclusion
(a) (b) (c)
In this paper we have presented an approach for
Figure 10: Qualitative evaluation on a real distant
modelingaxialandlateralnoiseofreal3Dscanning
capture. As per our analysis, with larger distance,
devices. Using our proposed methodology it is pos-
more noise is present, see normals in (a). In this sit-
sible to obtain a model of these types types of noise
uation, network trained on clean data without noise
with respect to imaged object distance and surface
wronglysegmentstheroughsurfaceasanobject,see
angles. Knowledge of the noise parameters can be
(c). Ontheotherhand,networktrainedondatawith
valuablewhenprocessingobtained3Dscans.
noise(M = 1.25)isresistanttothenoise(b).
n Wealsoshowthatemulatingnoisewhentraininga
deep learning segmentation model on synthetic data
is beneficial. Our experiment shows that the perfor-
mance of the segmentation network on real data is
bestwhentheemulatednoiseisslightlystrongerthan
estimatedfromtherealscans.
(a) (b) (c)
Infuture,othertypesofnoiseshouldbemodeled.
Figure11: Qualitativeevaluationonarealscenecap- Furthermore,thecombinedrangeimagewithsurface
tured in close distance. Image of surface normals normalsshouldbecomparedtootherdatarepresenta-
used as input to the networks is shown in (a). The tions. Weplantoexpandtheevaluationoftheeffects
masks produced by the trained network are shown of noise levels with extended data as well as decou-
for(b)M n = 1.25(c)M n = 2. Notetheinabilityof plingtheeffectsofdifferenttypesofemulatednoise.
thelatternetworktosegmentfinedetails. Lastly,theinteractionofaddednoisewithotherdata
augmentationtechniquesisworthinvestigating.
Albeit limited in scope, the experiment presented
in this section provides some insights into the ef- Acknowledgments: Theworkpresentedinthispaperwascar-
ried out in the framework of the TERAIS project, a Horizon-
fectofnoiseemulationduringsynthetictrainingdata
Widera-2021 program of the European Union under the Grant
generation on real-world performance of the trained
agreementnumber101079338.Researchresultwasobtainedus-
networks. Ourexperimentverifiestheimportanceof ingthecomputationalresourcesprocuredintheprojectNational
noise inclusion in synthetic training data. Addition- competence centre for high performance computing (project
code: 311070AKF2) funded by European Regional Develop-
ally,wecanobservethataddingtoomuchnoisemay
mentFund,EUStructuralFundsInformatizationofsociety,Op-
lead to poor models which are unable to detect fine
erational Program Integrated Infrastructure. We thank Michal
details in the scene structure. We thus conclude that Piovarcˇiforhishelpinpreparingprintingtraysforour3Dmod-
theabilitytomodelnoiseasitoccursinreal3Dcam- elsthatwereusedforrealdatasetscanning.
eras is an important aspect of synthetic training data 3https://doi.org/10.5281/zenodo.10581278
generation. 4https://doi.org/10.5281/zenodo.10581562
serutpaC
laeR
no
UoIReferences 26-28,2011.Proceedings,PartII7,pages199–208.
Springer,2011. 2
[1] A. Belhedi, A. Bartoli, S. Bourgeois, V. Gay-
[13] M. Hansard, S. Lee, O. Choi, and R. P. Horaud.
Bellile,K.Hamrouni,andP.Sayd. Noisemodelling
Time-of-flightcameras: principles,methodsandap-
in time-of-flight sensors with application to depth
plications. SpringerScience&Business,2012. 2
noise removal and uncertainty estimation in three-
[14] K. Khoshelham and S. O. Elberink. Accuracy and
dimensional measurement. IET Computer Vision,
resolution of kinect depth data for indoor mapping
9(6):967–977,2015. 1
applications. sensors,12(2):1437–1454,2012. 2
[2] K. Berger, K. Ruhl, Y. Schroeder, C. Bruemmer,
[15] V. Kocur, V. Hegrova´, M. Patocˇka, J. Neuman, and
A. Scholz, and M. A. Magnor. Markerless motion
A. Herout. Correction of afm data artifacts using a
captureusingmultiplecolor-depthsensors. InVMV,
cnntrainedwithsyntheticallygenerateddata. Ultra-
pages317–324,2011. 2
microscopy,246:113666,2023. 3
[3] A. Butler, S. Izadi, O. Hilliges, D. Molyneaux,
[16] T. Kovacovsky, M. Maly, and J. Zizka. Methods
S. Hodges, and D. Kim. Shake’n’sense: Reduc-
andapparatusforsuperpixelmodulation,U.S.Patent
ingstructuredlightinterferencewhenmultipledepth
US10965891B2,March2021. 2,3
camerasoverlap. Proc.HumanFactorsinComput-
[17] T. Mallick, P. P. Das, and A. K. Majumdar. Char-
ingSystems(ACMCHI).NY,USA.,14,2012. 2
acterizations of noise in kinect depth images: A
[4] J.Canny. Acomputationalapproachtoedgedetec- review. IEEE Sensors journal, 14(6):1731–1740,
tion.IEEETransactionsonpatternanalysisandma- 2014. 1,2,3
chineintelligence,PAMI-8(6):679–698,1986. 4
[18] C. V. Nguyen, S. Izadi, and D. Lovell. Modeling
[5] A. Chatterjee and V. M. Govindu. Noise in kinect sensor noise for improved 3d reconstruction
structured-lightstereodepthcameras:Modelingand and tracking. In 2nd International Conference on
itsapplications. arXiv:1505.01936,2015. 1 3D imaging, modeling, processing, visualization &
[6] R.O.DudaandP.E.Hart.Useofthehoughtransfor- transmission,pages524–530.IEEE,2012. 1,3,5
mationtodetectlinesandcurvesinpictures. Com- [19] O. Ronneberger, P. Fischer, and T. Brox. U-net:
municationsoftheACM,15(1):11–15,1972. 4 Convolutional networks for biomedical image seg-
mentation. InN.Navab,J.Hornegger,W.M.Wells,
[7] D.Duplevska,M.Ivanovs,J.Arents,andR.Kadikis.
and A. F. Frangi, editors, Medical Image Comput-
Sim2real image translation to improve a synthetic
ing and Computer-Assisted Intervention – MICCAI
dataset for a bin picking task. In 2022 IEEE 27th
2015, pages 234–241, Cham, 2015. Springer Inter-
InternationalConferenceonEmergingTechnologies
nationalPublishing. 6
andFactoryAutomation(ETFA),pages1–7,2022.2
[20] H. Sarbolandi, D. Lefloch, and A. Kolb. Kinect
[8] R.A.El-laithy,J.Huang,andM.Yeh. Studyonthe
rangesensing: Structured-lightversustime-of-flight
useofmicrosoftkinectforroboticsapplications. In
kinect. Computer vision and image understanding,
Proceedings of the 2012 IEEE/ION Position, Loca-
139:1–20,2015. 2
tionandNavigationSymposium, pages1280–1288.
[21] M.Sonka,V.Hlavac,andR.Boyle. Imageprocess-
IEEE,2012. 2
ing, analysis, and machine vision. Cengage Learn-
[9] D.FalieandV.Buzuloiu.Noisecharacteristicsof3d
ing,2014. 2
time-of-flight cameras. In 2007 International Sym-
[22] M. To¨lgyessy, M. Dekan, v. Chovanec, and P. Hu-
posiumonSignals,CircuitsandSystems,volume1,
binsky´. Evaluation of the azure kinect and its
pages1–4.IEEE,2007. 1,2
comparison to kinect v1 and kinect v2. Sensors,
[10] B. Freedman, A. Shpunt, and Y. Arieli. Distance- 21(2):413,2021. 2,3
varying illumination and imaging techniques for
[23] J. Tremblay, A. Prakash, D. Acuna, M. Brophy,
depth mapping, U.S. Patent US20100290698A1,
V. Jampani, C. Anil, T. To, E. Cameracci, S. Boo-
July2013. 2
choon, and S. Birchfield. Training deep networks
[11] L.Gajdosˇech,V.Kocur,M.Stuchl´ık,L.Hudec,and withsyntheticdata: Bridgingtherealitygapbydo-
M. Madaras. Towards deep learning-based 6d bin mainrandomization,2018. 3
pose estimation in 3d scan. In Proceedings of the [24] M. Vogt, A. Rips, and C. Emmelmann. Compari-
17th International Joint Conference on Computer son of ipad pro®’s lidar and truedepth capabilities
Vision,ImagingandComputerGraphicsTheoryand with an industrial 3d scanning solution. Technolo-
Applications - Volume 4: VISAPP, pages 545–552. gies,9(2):25,2021. 2
INSTICC,SciTePress,2022. 6
[25] O. Wasenmu¨ller and D. Stricker. Comparison of
[12] M. Gschwandtner, R. Kwitt, A. Uhl, and W. Pree. kinectv1andv2depthimagesintermsofaccuracy
Blensor: Blendersensorsimulationtoolbox. InAd- and precision. In ACCV 2016 International Work-
vancesinVisualComputing: 7thInternationalSym- shops, Taipei, Taiwan, November20-24, 2016, Part
posium,ISVC2011,LasVegas,NV,USA,September II13,pages34–45.Springer,2017. 2