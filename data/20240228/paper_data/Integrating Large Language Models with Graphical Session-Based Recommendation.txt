xxx
Integrating Large Language Models with Graphical Session-Based
Recommendation
NAICHENG,GUO,MYbank,AntGroup,China
HONGWEI,CHENG,MYbank,AntGroup,China
QIANQIAO,LIANG,MYbank,AntGroup,China
LINXUN,CHEN,MYbank,AntGroup,China
BING,HAN,MYbank,AntGroup,China
WiththerapiddevelopmentofLargeLanguageModels(LLMs),variousexplorationshavearisentoutilizeLLMscapability
ofcontextunderstandingonrecommendersystems.Whilepioneeringstrategieshaveprimarilytransformedtraditional
recommendationtasksintochallengesofnaturallanguagegeneration,therehasbeenarelativescarcityofexplorationinthe
domainofsession-basedrecommendation(SBR)duetoitsspeci￿city.SBRhasbeenprimarilydominatedbyGraphNeural
Networks,whichhaveachievedmanysuccessfuloutcomesduetotheirabilitytocaptureboththeimplicitandexplicit
relationshipsbetweenadjacentbehaviors.Thestructuralnatureofgraphscontrastswiththeessenceofnaturallanguage,
posingasigni￿cantadaptationgapforLLMs.Inthispaper,weintroducelargelanguagemodelswithgraphicalSession-Based
recommendation,namedLLMGR,ane￿ectiveframeworkthatbridgestheaforementionedgapbyharmoniouslyintegrating
LLMswithGraphNeuralNetworks(GNNs)forSBRtasks.Thisintegrationseekstoleveragethecomplementarystrengthsof
LLMsinnaturallanguageunderstandingandGNNsinrelationaldataprocessing,leadingtoamorepowerfulsession-based
recommendersystemthatcanunderstandandrecommenditemswithinasession.Moreover,toendowtheLLMwiththe
capabilitytoempowerSBRtasks,wedesignaseriesofpromptsforbothauxiliaryandmajorinstructiontuningtasks.These
promptsarecraftedtoassisttheLLMinunderstandinggraph-structureddataandaligntextualinformationwithnodes,
e￿ectivelytranslatingnuanceduserinteractionsintoaformatthatcanbeunderstoodandutilizedbyLLMarchitectures.
Extensiveexperimentsonthreereal-worlddatasetsdemonstratethatLLMGRoutperformsseveralcompetitivebaselines,
indicatingitse￿ectivenessinenhancingSBRtasksanditspotentialasaresearchdirectionforfutureexploration.
CCSConcepts:•Informationsystems Recommendersystems;•Datamining;•Computingmethodologies
! !
NeuralNetworks;
AdditionalKeyWordsandPhrases:behaviorsmodeling,session-basedrecommendation
ACMReferenceFormat:
NAICHENG,GUO,HONGWEI,CHENG,QIANQIAO,LIANG,LINXUN,CHEN,andBING,HAN.xxxx.IntegratingLarge
LanguageModelswithGraphicalSession-BasedRecommendation.Proc.ACMMeas.Anal.Comput.Syst.xx,x,Articlexxx
(xxxxx),23pages.https://doi.org/xx.xxxx/xxxxxxx.xxxxxxx
1 INTRODUCTION
Recommendersystemshavebecomeanintegralpartofvariousplatforms,recommendingrelevantresources
tousersbasedontheirpreferences.However,therearerealisticscenarioswhereonlylimitedinteractionsare
Authors’addresses:NAICHENG,GUO,guonaicheng.gnc@mybank.cn,MYbank,AntGroup,Beijing,Beijing,China;HONGWEI,CHENG,
MYbank,AntGroup,Shanghai,China;QIANQIAO,LIANG,MYbank,AntGroup,Shanghai,China;LINXUN,CHEN,MYbank,AntGroup,
Beijing,China;BING,HAN,MYbank,AntGroup,Shanghai,China.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthat
copiesarenotmadeordistributedforpro￿torcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthe￿rst
page.CopyrightsforcomponentsofthisworkownedbyothersthanACMmustbehonored.Abstractingwithcreditispermitted.Tocopy
otherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspeci￿cpermissionand/orafee.Requestpermissionsfrom
permissions@acm.org.
©xxxxAssociationforComputingMachinery.
2476-1249/xxxx/x-ARTxxx$15.00
https://doi.org/xx.xxxx/xxxxxxx.xxxxxxx
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.xxx:2 • Guo,etal.
availabletodetectuserintent,makingtraditionalrecommendersinadequate.Toaddressthis,researchersproposed
session-basedrecommendation(SBR),whichreliesonuserbehaviorsequencesforrecommendations.Traditional
SBRmethodsuseMarkovChains[10,28],ConvolutionNeuralNetworks(CNNs)[30],RecurrentNeuralNetworks
(RNNs)[11,19],andattentionmechanism[25],tocapturesequentialsignalsanddynamicuserinterests.Newly,
GraphNeuralNetworks(GNNs)havebeenincorporatedintoSBR,leveragingtheirabilitytomodelstructural
informationandcomplextransitionsbetweenitemsinasession.GNNshaverapidlyemergedasastate-of-the-art
(SOTA)approach[6,36,39]inSBRtasks,surpassingtraditionalmethodswiththeirimpressiveabilitytocapture
complexitemtransitionsandrelationshipswithinusersessions.TheGNN-basedmethodsexcelatdiscerningthe
intricatepatternsofuserbehavior,thankstotheirsophisticatedrepresentationofitemsasnodeswithinagraph
structure,allowingfortheaccuratemodelingofinteractionsanda￿nitiesbetweenitems.Bycapitalizingon
thisstructuralinformation,GNN-basedmethodshavedemonstratedsuperiorperformanceinpredictinguser
preferencesinSBRtaskswithlimitedinteractiondata.Despitesigni￿cantprogress,existingSBRalgorithms
primarilyrelyonuserinteractiondata,neglectingvaluabletextualinformationassociatedwithusersanditems.
Thisrestrictsthealgorithms’capabilitytocapturethenuancesandcontextofinteractions.
Recently,theemergenceoflargelanguagemodels(LLMs),whichexcelinassimilatingreal-worldknowledge
fromtheWebandachievingpro￿cientnaturallanguagegeneration,hastriggeredasigni￿cantrevolutionin
theresearchcommunity[43].Speci￿cally,therearealsoseveralattempts[5,12,22]thatadaptLLMszero/few-
shot ability for recommender systems, which struggle to provide accurate recommendations often due to a
lack of speci￿c recommendation task training. Moreover, increasing e￿orts have predominantly converted
the recommendation task into a natural language generation task to further ￿ne-tune LLMs using relevant
recommendationdata[2,3,16,40]toaddresstheabovelimits.ByintegratingLLMs,recommendersystemscan
bene￿tfromtherichtextualinformationassociatedwithusersanditems[38,41],tappingintothedescriptive
contenttobetterinferuserpreferencesandimprovetheaccuracyoftherecommendationsprovided.
InspiredbythesuccessofLLMsintraditionalrecommendationsystems,anaturalinquiryarises:Canexisting
LLMsbeappliedtothegraph-basedSBRtask?However,SBRapproachesheavilyrelyongraphstructures,and
cannotbedirectlyprocessedbyLLMs,missingoutonthebene￿tsofexistingLLM-basedrecommendation
methods.TobridgethisgapandleveragethecapabilitiesofLLMs,afundamentalproblemneedstobeaddressed
forsuccessfulitemrecommendations.
Challenge:Howtoexpressgraph-basedSBRtasksinnaturallanguage?Graph-basedSBRtasksare
inherentlymulti-faceted,encompassingboththerepresentationofcomplexiteminteractionsandthedynamic
natureofuserinterests.Incontrasttootherrecommendationscenarioswheretaskscanbereadilydescribed
aslanguagegenerationtasks,asshowninFig.1,SBRmethodsuseinherentlystructuredgraphdata,whichis
notnaturallyalignedwiththesequentialandinterpretiveprocessingofLLMs.Thestructuralnatureofgraphs
contrastswiththeessenceofnaturallanguage,posingasigni￿cantadaptationchallengeforLLMs.
Toovercomethesechallengesabove,weintegrateLargeLanguageModelswithGraph-BasedSession-Based
Recommendation, named LLMGR, which enhances session-based recommenders by ￿exibly adapting LLM.
LLMGRisdesignedtoenhanceSBRbyexploitingthevastknowledgeandsemanticunderstandingcapabilitiesof
LLMs,addressingthechallengesofcapturinguserpreferencesfromlimitedinteractiondata.Firstly,weengineer
aseriesofpromptsthatguidethemodelthroughbehaviorpatternmodelingandgraphcomprehensiontasks.
Thesepromptscombinetextualtemplateswithplaceholders,facilitatingthehybridencodingofbothlinguistic
andgraph-structuralinformationintoaformatamenabletoLLMprocessing.Secondly,wedesignatwo-stage
tuningprocess:anauxiliarystagefocusesonestablishingtext-nodeassociations,aswellasamainstagecaptures
behavioralpatternswithinsessiongraphs.Finally,toevaluateourapproach,weconductextensiveexperiments
onthreereal-worlddatasets,andourmethodachievesthebestperformancecomparedtoseveralcompetitive
baselines.Ourgoalisnotonlytoenhancethecurrentunderstandingofsession-basedrecommendationsbutalso
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.IntegratingLargeLanguageModelswithGraphicalSession-BasedRecommendation • xxx:3
EVH Wolfgang USA – Ivory
LLM
LLM Textual Emb. Look-up
Input：This user has watched PRS CE 24 - Grey
Black, Seagull Pro-G Guitar Stand, Blue Spark
Pop Filter, Seagull Pro-G Guitar Stand,EVH
Wolfgang USA -Ivory>. Please predict the
suitable item for user:
Fig.1. IllustrationofLLM-basedmethodforconventionalrecommendationscenarios.
toprovidepioneerattemptsforfutureresearchattheintersectionofcomplexgraphmodelingandadvanced
languagemodels.
Overall,ourmajorcontributionscanbesummarizedasfollows:
WedesignanLLM-basedframeworkthataimsataddressingtheshortcomingsofSBRine￿ectivelyutilizing
•
textualinformation.Toourknowledge,thisisthe￿rstframeworktotacklethisissueusingLLM.
We tailor an e￿ective two-stage instruction tuning strategy to boost LLM capability to decipher the
•
intricaterelationshipswithinsessiongraphs,understandgraphstructure,andcapturethebehaviorpattern
simultaneously.
ExtensiveexperimentalresultsindicatethatourproposedLLMGRnotonlysigni￿cantlyoutperformsthe
•
SOTAcomparisonmethodbutalsohasportabilitytobene￿ttheconventionalSBRmethods.
2 PRELIMINARIES
Inthissection,weintroducethenotationandformalizetheproblemweaimtoaddress.Additionally,weprovide
aconciseoverviewofSBRmethodstosetthestageforoursubsequentdiscussions.
2.1 ProblemFormulation
Asession-basedrecommendationtaskisconstructedonhistoricaluserbehaviorsessionsandmakespredictions
basedoncurrentusersessions.Inthistask,thereisanitemset ,where< = isthenumberofitemsand
V |V|
allitemsareunique.Eachsession( = E ,E ,...,E iscomposedofaseriesofuser’sinteractions,whereE
[ 1 2 =] 8
representsanitemclickedattime8 in( and=representssession’slength.SBRtaskistopredicttheitemthatthe
userismostlikelytoclickonnexttimeinagivensession(.Foreachgivensession( inthetrainingprocess,
thereisacorrespondinglabel~asthetarget.Inthetrainingprocess,foreachitemE inagivensession,our
8
2V
modellearnsthecorrespondingembeddingvectorE R3,where3 isthedimensionofnodeE.Ourframework
2 1 1
outputsaprobabilitydistributionyˆ overthegivensession(.
2.2 Graph-basedSBRMethods
MostrecentSOTASBRmethodshaveutilizedGNNstomodelthecoherenceofitemswithinasessiongraphdue
toGNNspossesspowerfulcapabilitiesforrepresentingstructureddataandcapturingimplicitdependencies.Fig.
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.xxx:4 • Guo,etal.
<Blue Spark Pop Filter>
! !!,# ! "∗
" & ! !!,$
!
!!,%
" $ " % " ' !
!!,&
<PRS CE 24 - <Seagull Pro-G <EVH Wolfgang
Grey Black> Guitar Stand> USA -Ivory> !
!!,'
#
%
Fig.2. TheoverallparadigmofGNN-basedSession-basedrecommendation.
2illustratestherepresentativeoverallparadigmofGNN-basedSBR.Graph-basedSBRmethodscanberoughly
dividedintothefollowingparts:sessiongraphconstruction,informationpropagation,andsessionoutput.
2.2.1 SessionGraphConstruction. Bycapturingsequentialpatternsamongadjacentitemsinauser’sbehavior
sequence,wetransformeachbehaviorsequence( = E ,E ,...,E intoacorrespondinggraph⌧ = E,4 ,where
1 2 8
[ ] ( )
E + represents the set of vertices and4 represents the set of directed edges. An illustration of a session
2
graphbuiltfromthesequence E ,E ,E ,E ,E isprovidedinFig.2,showcasingedgesthatconnecteachpairof
1 2 3 2 4
[ ]
consecutiveitems.
2.2.2 InformationPropagation. GNNscane￿ectivelycaptureandmodelcomplexrelationshipsanddependencies
presentingraphs.Existingworksaimtolearnaclassi￿erandthegraph-levelrepresentationtopredictthelabelof
thegraph.Givenacollectionofgraphs ⌧ ,⌧ ,...,⌧ ⌧ andthecorrespondinglabels E ,E ,...,E + .
(
1 2 =
)2 (
!1 !2 !=)2 !
GNNsusethestructureofthegraphandtheoriginalfeatureofeachnodetolearnitscorrespondingrepresentation.
Thelearningprocessistotakeanodeasthecenter,anditerativelyaggregatetheneighborhoodinformation
alongedges.Theinformationaggregationandupdateprocesscanbeformulatedasfollows:
tE(; +1 ) =5 aggregator xD(; ),D # E , (1)
( 2 ( ))
xE(; +1 ) =5 updater xE(; ),tE(; +1 ) , (2)
( )
wherex; representstheembeddingofnodeE after;-thlayeraggregatorand# E isneighborhoodofnodeE.The
E ( )
informationaggregationfunction5 aggregatestheinformationfromtheneighborhoodinformationand
aggregator
passesittothetargetE.Theupdatefunction5 calculatesthenewnodestatuesfromthesourceembedding
updater
x;
E
andtheaggregatedinformationt; E+1.After; stepsofinformationaggregation,the￿nalembeddinggathersthe
;-hopneighborhoodandthestructureinformation.Forthegraphclassi￿cationtask,readoutfunction 5
readout
generatesagraphlevelembeddingZbygatheringtheembeddingsofallnodesinthe￿nallayers
Z= 5
readout
xE(; ),E . (3)
({ 2V})
2.2.3 SBROutputLayer. Duetothelimitediterationofpropagation,GNNcannote￿ectivelycapturelong-range
dependency among items. Therefore, to obtain an e￿ective sequence representation and probability of user
preference,existingworksproposeseveralstrategiestointegratetheitemrepresentationsinthesequence.
3 METHODOLOGY
Inthissection,wepresenttheproposedLLM-basedSBRframeworkLLMGRindetail,whichenhancesSBRtask
byintegratinglanguagegraphstructurealignmentasillustratedinFig.3.
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.
GNN
Block
Sequence
model
Next
prediction
taskIntegratingLargeLanguageModelswithGraphicalSession-BasedRecommendation • xxx:5
! Updated
Frozen
! !
Output Layer Output Layer
LLM LoRA ! LLM LoRA
Text Emb. Text Emb. Text Emb. Graph Emb. Node Emb. Text Emb. Text Emb. Text Emb. Graph Emb. Node Emb.
Hybrid Encoding Hybrid Encoding
!
LLM Textual Emb. Look-up !$ LLM Textual Emb. Look-up Pre-trained !$ !
GNN !! !" !# GNN !! !" !#
<Text> <Graph ID> <Node ID list> <Text> <Graph ID> <Node ID list>
Contrastive Structure-Aware Prompt
Behavior Patterns Modeling Prompt
Node-Context Aligning Prompt
Instruction: Please recommend a suitable next item according Instruction: Please recommend a suitable next item according to the
toI nthsetr suecstsioionn: g Prlaepahse a tnedll imts ec owrhreicshp onnoddien gth neo tdeex t<ugal2 i>n formation Instruction: Please recommend a suitable next item according to
R< e1<s3S p5e o2a5 ng4u s>l el :Pro[-NGe GxtuItiteamr ]Stand >belongs to. tshees ssieosns igorna gprha apnhd a intds citosr rceosrpreosnpdoinngd innogd neosd<eG <2g>2 >< <V!1,3$5",2$5$4,>$",$#>.
Response: Response:
Fig.3. TheArchitectureoftheLLMGRFramework.Thele￿partdescribestheauxiliarytuningstage,thentherightpart
Instruction: Please recommend a suitable next item according to the
illustratesthemajorinstructiontuningstage.
session graph and its corresponding node<G2> < V!,$",$$,$",$#>
Response:
3.1 Overview
LLMGRconsistsoffouressentialphases:instructionpromptsconstruction,hybridencoding,LLMoutputlayer,
andtwo-stageinstructiontuning.Atthestageofmulti-taskinstructionpromptsconstruction,wecraftinstruction
promptsforbothauxiliaryandmajortasks.ThesepromptsaredesignedtoendowtheLLMwithanuanced
understandingofthesessiongraph’slatentuserpreferences.Themajortaskisdesignedtounderstandtheinherent
structureofthegraph,whiletheauxiliarytaskistobuildinterlinkagesbetweennodesandtheirassociated
textualinformation.TheseinstructionpromptsincorporatebothtextandIDcomponents,thelatterrepresentsa
nodefromthepre-trainedconventionalsession-basedrecommenderandcannotbedirectlyembedded,therefore
wedesignahybridencodinglayertoencodethem.Thislayeradeptlyconvertsthepromptsintoasequenceof
embeddings,transformingthemintoaformatthattheLLMcanprocesse￿ectively.Afterthat,recommendation
resultsaremadebycomputingthejointprobabilitydistributionbetweentheoutputofLLMandallthecandidate
itemsviaanitemlineartransformation.Inthetuningphase,duetothesigni￿cantdi￿erencebetweentheauxiliary
taskandmajortask,wedesignatuningstrategythatalternatelyupdatetheparameters.Tosumup,wepropose
aframeworkwherethestructuralstrengthsofgraph-basedmodelsaree￿ectivelycombinedwiththecontextual
understandingofLLMs,andyieldsession-basedrecommendersystemscapableofdeliveringhighlypersonalized
recommendations.
3.2 InstructionPromptConstruction
TosimultaneouslyenableLLMtocapturethehiddenpreferenceswithinsessiongraph,understandthegraph
structure,andalignthenodeswiththeircorrespondingtextualinformation,wedesignaseriesofpromptsto
assignlanguageandbehaviorpatternsfortuningLLMs.Inthepromptsdesigningstage,themaintaskistomodel
behaviorpatternmodel,followingbytwoauxiliarytaskstounderstandgraphstructureandalignnode-text.In
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.xxx:6 • Guo,etal.
theseprompts,weincorporatebothtextualinformation(template)andplaceholders.Theplaceholdersservethe
purposeofretrievingembeddingsatthehybridencodinglayer3.3.
3.2.1 BehaviorPa￿ernsModelingPrompt. Sinceourgoalistoenhancetheperformanceandgeneralizationability
oftheSBRtaskbyleveragingtheunderstandingcapabilityofLLM,behaviorpatternmodelinghasbeentreated
asthemajortuningtask.Weconstructpersonalizedrecommendationinstructionbasedonuser’scurrentsession.
Then,LLMsarepromptedbytheinstructionswithasessiongraphandcorrespondingnodelisttopredictthenext
itemthattheuserismostlikelytointeractwith.Thetemplateofinstructiontuningisshowninthefollowing:
Instruction: Please recommend a suitable next item
according to the session graph and its corresponding nodes
<session graph ID> <node ID list>.
Response:
Inthistemplate,<sessiongraphID>and<nodeIDlist>aretheplaceholderthatincorporatethegraphindex
anditsnode,whichisusedtoobtaintheirembeddinginthehybridencodinglayerdetailinsection3.3.Taking
Fig.2asanexample,<sessiongraphid>is⌧ andthe<nodelist>is E ,E ,E ,E ,E .Byrepresentingthegraph
2 1 2 3 2 4
[ ]
initsindexanditsnode,theassembledpromptisshownbelow:
Instruction: Please recommend a suitable next item
according to the session graph and its corresponding nodes
list<" ><$ ,$, $, $, $ >.
! " ! # ! $
Response:
Thispromptallowsforamorecomprehensiveunderstandingofthecomplicateditemtransitionpatternsand
userbehaviorpreferences,leadingtoanimprovedperformanceintheSBRtask.
3.2.2 Node-ContextAligningPrompt. ToalignnodesandthetextualcontextwithinLLMsmoree￿ectively,our
mainfocusistoexploreapromptthatcancollaboratewellwithLLMs.Bydesigningasuitableinstructionprompt
thatincorporatesboththenodeofthesessiongraphandthecorrespondingcontext,weaimtofacilitateseamless
integrationofnodeinformationintoLLMs.Wepresenttheinstructionsamplestoillustratethealignmenttuning
tasksinthefollowing.
Instruction: Please tell me which node the textual
information <textual information> belongs to.
Response:
The<textualinformation>referstothetitleordescriptionofthetargetnode.UsingthenodeE depictedin
4
Fig.2asanexample,theentirepromptisrevealedasfollows.
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.IntegratingLargeLanguageModelswithGraphicalSession-BasedRecommendation • xxx:7
Instruction:Please tell me which node the textual
information <Seagull Pro-G Guitar Stand> belongs to.
Response:
Forthisalignmentprompt,eachnodeE + wouldbeconsidered.Thispromptfacilitatesamorecomprehensive
2
comprehensionofnodes,resultinginanincreasedgeneralizationcapabilityintheSBRtask.
3.2.3 ContrastiveStructure-AwarePrompt. Toenhancetheunderstandingcapabilityoflargelanguagemodels
ongraphstructuralinformation,ourframeworkalsoemphasizesaligningthegraphstructurewiththenatural
languagespace.ThisalignmentaimstoenableLLMstoe￿ectivelycomprehendandinterpretthestructural
elementsofthegraph,maximizingtheirinherentunderstandingcapabilities.TheSBRmethodprimarilyfocuses
on directed graphs as input, therefore we place our emphasis on ￿nding succeeding nodes in the task. The
elaboratedcontrastivestructure-awarepromptisshownasfollows:
Instruction: Given a target node ID and a candidate node
ID list, select the succeeding nodes of the target node
<node ID> <candidate node ID list>.
Response:
Foreachsessiongraph⌧ ,thetargetnodeisgeneratedfromasubsetexcludingthetailnode.Theplaceholder
8
<candidatenodeIDlist>israndomlyarrangedusingsucceedingnodesandrandomlysamplednegativenodes,
whichaimstoimprovetherobustness.UsingthenodeE depictedinFig.2asthetargetnode,asampleinstance
3
isgivenasfollows:
Instruction: Given a target node ID and a candidate node
ID list, select the succeeding nodes of the target node
<"><",",",…," >
! " ! # $
Response:
ThisinstructionpromptstheLLMstodistinguishthepositiveitemfromthegivencandidate,basedonthe
graphstructuredatainformation.
3.3 HybridEncodingLayer
Thepromptforourframeworkconsistsoftext(e.g.templateand<textualinformation>),nodeID(e.g.<nodeID>,
<nodeIDlist>,),andgraphID(e.g.<sessiongraphID>).Thetextualelementsoftheprompt,suchastemplate,
title,anddescription,areprocessedbyusingtheLLM’stokenizerandwordembeddinglayertoconvertthem
intotokensandsubsequenttokenembeddings.DespitethefactthatLLMsarepowerfulinmodelingnatural
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.xxx:8 • Guo,etal.
languageandgeneratingreasonableresponses,theyareunabletocomprehendpureIDinformationduetothe
lackoftextualcontext.However,inSBRtask,theitemdependencieswithintheIDshavebeenproventobe
highlye￿ectiveincapturingbehavioralpatternsoveralongperiod.Formally,theIDembeddingsx R31 and
v 2
graphembeddingz R31 (aforementionedin2.2.2)areobtainedbydirectlyextractingfromapre-trainedSBR
i 2
model,e.g.,SRGNN[36],GCSAN[39],HCGR[6],whereΦ representtheirparameters.Sincethepre-trained
SBR
IDembeddingsusuallyhaveamuchlowerdimensionthanthewordembeddingsinLLM,weemployalinear
transformation 5 8= parameterized by Φ in to convert the dimension of ID embeddings from3 1 into the same
dimensionasthewordembeddings3 ,where3 alsomuchlargerthan3 .The￿nalinputsaretheconcatenation
2 2 1
ofthetextembeddingsandtheIDembeddingsintheiroriginalpositionsrepresentedasE.Thislayercombines
thestrengthsofbothtextandIDembeddingstoenhancetheunderstandingoftheinputdataforLLMs.
3.4 LLMOutputLayer
AftertheprompthasbeenencodedintoconsecutiveembeddingE,whichcanbedealtwithLLMLayer.Hence,
weintegratetheLLaMA-2[31]andtheLow-RankAdaptation(LoRA)[13]inourLLMlayerforinstruction
tuning.LoRAintroducestrainablerankdecompositionmatricesintopre-trainedmodellayers,notablyslashing
thenecessarytrainableparametersfordownstreamtasks.Then,theoutputoftheLLMlayercanbeformulated
asfollows:
O=6 E , (4)
ΦLLM+ΦLoRA( )
whereΦ representtheparametersofLLMandΦ andExistingLLM-basedrecommendationapproaches,
LLM LoRA
asdiscussedinpreviouswork[20],typicallytreattherecommendationtaskasanopen-domainnaturallanguage
generationtask,whichalignwiththenatureofLLMs.However,theseapproachesfacechallengesinaccurately
generatingrecommendationswithinthede￿nedscopeandarelimitedtogeneratingonerecommendationresult
atatime.Hence,weadoptaMultilayerPerceptrontorecommendtheprobabilityforeachelementintheitemset
+ ofeachgivensession(,whichcanbeformulatedas:
yˆ = 5 O , (5)
Φout( )
whereΦ istrainableparameters.Therecommendationresultsaremadebycomputingthejointprobability
out
distributionbetweentheoutputofLLMandallthecandidateitemsviaoutputlineartransformation5 .The
Φout
outputvectoryˆ R<,where< = + .Inthetuningstage,thecrossentropylosshasbeenutilizedforthelearning
2 | |
objectiveasfollows:
<
! = ~;>6 ~ˆ . (6)
8 8
  ( )
8=1
’
3.5 Two-stageInstructionTuningStrategy
Inthissubsection,wewillintroduceourtwo-stageinstructiontuningandrecommendationprocess,whichaim
toalignLLMwiththerecommendationtaske￿ectively.Duetothesigni￿cantdi￿erencebetweentheauxiliary
taskandmajortask,ourproposedLLMGRconsistsoftwostages:(1)auxiliaryinstructiontuningand(2)major
instructiontuning.Thistwo-stageoptimizationenablesLLMGRtounderstandgraphstructureandalignnodes
with their context in the former stage, then further capture behavior patterns among session graphs in the
latterstageunderbehaviorpatternsmodelingprompt.Speci￿cally,theauxiliarytuningstageaimstocreate
theassociationbetweentextandnodeIDbyleveragingnode-contextalignmentprompt3.2.2andcontrastive
structure-awareprompt3.2.3.Duringthisstage,wetunetheparameterofitemlineartransformationΦ inthe
in
hybridencodinglayer,LoRAmoduleΦ andoutputlineartransformationintheLLMoutputlayer,while
LoRA
freezingallothercomponents.Afterthat,theLLMhasalreadyacquiredthecapabilitytoconnectnodeIDswith
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.IntegratingLargeLanguageModelswithGraphicalSession-BasedRecommendation • xxx:9
textualcontext.However,inthemajortuningstage,wefocusoncapturingbehaviorpreferenceswithinthe
sessiongraphunderbehaviorpatternsmodelingprompt3.2.1,andonlyunfreezetheparametersΦ fromthe
SBR
pre-trainedSBRmethod.Tobespeci￿c,thelossfunctionofLLMGRisthesameasEquation6,wherewede￿ne
thetrainableparametersΦas:
Φ ,Φ ,Φ , auxiliaryinstructiontuningstage
Φ= { LoRA in out} (7)
Φ ,Φ , majorinstructiontuningstage
⇢ { SBR out}
4 EXPERIMENT
Inthissection,weprovideanoverviewoftheexperimentalsettings,includingthedatasets,thecomparison
methods, the evaluation metrics, and the parameter con￿gurations. We then proceed to conduct extensive
experimentsonthreecommonlyreal-worlddatasetstoanswerthefollowingresearchquestions:
RQ1:HowdoesLLMGRperforminSBRscenarioscomparedwiththeSOTAmethods?
•
RQ2:Whatisthee￿ectivenessandportabilityofourproposedLLMGR?
•
RQ3:HowdoeseachcomponentofLLMGRcontributetotheperformance?
•
RQ4:Howe￿ectiveisLLMGRintegratingLLMintoSBRscenariotoalleviatethedatasparsityproblem?
•
RQ5:CanLLMGRpresentreasonableexplanationtopredictuserpreferenceandgetbetterrecommendation
•
results?
4.1 ExperimentalSe￿ings
4.1.1 DatasetsDescription. Wepresentasummaryofthethreerepresentativereal-worlddatasetsusedinour
experimentsinTable1.Thesedatasetsarewell-knownAmazondatasets1,eachrepresentingadi￿erentcategory
ofproducts,namelyMusic(MusicalInstruments),Beauty(LuxuryBeauty),andPantry(PrimePantry).Toensure
data quality, we follow previous studies [1, 6, 29, 36] and remove inactive users and unpopular items with
fewerthan￿veinteractions.Inaddition,weemploytheleave-one-outstrategy,i.e.,foreachuser’sinteraction
sequence,thelastitemisusedasthetestdata,thesecondlastitemisusedasthevalidationdata,andtheremaining
itemsformthetrainingdata.
Table1. Datasetdescriptions.
Dataset Music Beauty Pantry
#Users 5,388 3,589 8,527
#Items 2,495 1,366 3,777
#Average Length 7.44 9.99 9.35
#Spaisity 99.70% 99.92% 99.75%
#Actions 40,136 32,732 79,789
4.1.2 ComparisonMethods. Toverifythee￿ectivenessofourmodel,wecompareourLLMGRwithseveral
baselinesasfollow:
FPMC[28]-aclassicalMarkov-chain-basedmethod,whichconsidersthelatestinteraction.
•
1http://jmcauley.ucsd.edu/data/amazon/
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.xxx:10 • Guo,etal.
CASER[30]-aCNN-basedmethodthatintegratesbothhorizontalandverticalconvolutionaloperations
•
tobettercapturethehigh-orderinteractionswithinbehaviorsequences.
GRU4Rec [11] - a representative RNN-based method, which stacks multiple GRU layers and adopts
•
ranking-basedlosstothemodelpreferencewithinsessions.
NARM[19]-ahybridencoderwithanattentionmechanismtomodelbehaviorpatterns.
•
STAMP[25]-anattentionmodeltocaptureuser’stemporalinterestsfromhistoricalclicksinasession
•
andreliesonself-attentionofthelastitemtorepresentusers’short-terminterests.
SRGNN [36] – a GNN-based model for the SBR task, which transforms the session data into a direct
•
unweightedgraphandutilizesgatedGNNtolearntherepresentationoftheitem-transitionsgraph.
GCSAN[39]–aGNN-basedmodelusesgatedGNNtoextractlocalcontextinformationandthenemploys
•
theself-attentionmechanismtoobtaintheglobalrepresentation.
NISER[9]–aGNN-basedmodelusingnormalizeditemandsession-graphrepresentationstoalleviate
•
popularitybias.
HCGR[6]–aGNN-basedSBRmethodenlightenedbythepowerfulrepresentationofnon-Euclidean
•
geometrywhichisprovedtobeabletoreducethedistortionofembeddingdataontopower-lawdistribution.
4.1.3 Evaluation Metrics. We employ three widely used[7, 8, 36] evaluation metrics, namely  8C'0C4   ,
( )
#⇡⇠⌧ # ,and" % " ,toassesstheperformanceoftheSBRmethods.
( ) ( )
ToevaluatetherankingabilityoftheSBRmethods,werandomlysample99negativeitemsforeachpositive
sample. Additionally, to provide a comprehensive evaluation, we select =5,10,20 to calculate the metrics
 8C'0C4@ ,#⇡⇠⌧@ ,and"''@ .Highervaluesforallthreemetricsindicatebetterperformance.
 8C'0C4@ :Ifoneormoreelementofthelabel~isshowninthepredictionresults~ˆ,wecallitahit.The
•
HitRateiscalculatedasfollows:
  ~B ~ˆB < 
 8C'0C4@ = B 2( \ , (8)
(
Õ   | |  
where ~ˆB = ,  denotestheindicatorfunctionand isanemptyset.AlargervalueofHitRatere￿ects
| | (⇤)
theaccuracyoftherecommendationresults.
#⇡⇠⌧@ :NormalizedDiscountedCumulativeGain #⇡⇠⌧ isaranking-basedmetric,thatfocuseson
• ( )
theorderofretrievalresultsandiscalculatedinthefollowingway:
1 2  ~ˆB ~B 1
#⇡⇠⌧@ = ( 2 )   , (9)
# log 8 1
: ’8=1 2( + )
where# isaconstanttodenotethemaximumvalueof#⇡⇠⌧@ given ~ˆB and  denotesanindicator
:
| | (⇤)
function.AlargeNDCGvaluere￿ectsahighertherankingpositionoftheexpecteditem.
"''@ MeanReciprocalRank(MRR)whentheritemisnotinthehigher position,thereciprocalisset
•
to0.Itisformallygivenby:
(
1 | | 1
"''@ = , (10)
( rank
| | ’8=1 8
whereA0=: denotesthepositionoftheitemin~ˆB.MRRisanormalizedrankingthatmeasurestheorderof
8
recommendationlist~B.AlargeMRRvaluere￿ectsahigherrankingpositionoftheexpecteditem.
4.1.4 ImplementationDetails. Toensureafairexperimentalcomparison,wemaintainasimilarhyperparameter
settingforallcomparisonbaselines.Speci￿cally,wesetthemini-batchsizeto1024,thedropoutrateto0.3,and
tunethelearningratefrom{1e-4,1e-3,1e-2,1e-1}.Theembeddingsizeissetto64,andthemaximumsequence
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.IntegratingLargeLanguageModelswithGraphicalSession-BasedRecommendation • xxx:11
Table2. Performanceillustrationofallcomparisonmethodsonalldatasets.
ComparisionMethods ProposedMethod
Datasets Metrics Improvement
FPMC CASER GRU4Rec NARM STAMP SRGNN GCSAN NISER HCGR LLMGR-M LLMGR
HitRate@5 0.2548 0.2556 0.2628 0.2782 0.2543 0.2610 0.2905 0.2806 0.3016 0.3047 0.3068 1.73%
NDCG@5 0.1794 0.1884 0.1867 0.2026 0.1882 0.1901 0.2218 0.2019 0.2226 0.2310 0.2330 4.68%
MRR@5 0.1547 0.1663 0.1618 0.1779 0.1665 0.1669 0.1992 0.1760 0.2001 0.2083 0.2094 4.65%
HitRate@10 0.3491 0.3536 0.3716 0.3860 0.3478 0.3476 0.3873 0.3870 0.3965 0.4052 0.4085 3.02%
Music NDCG@10 0.2097 0.2200 0.2218 0.2374 0.2182 0.2180 0.2530 0.2363 0.2631 0.2633 0.2719 3.35%
MRR@10 0.1671 0.1793 0.1762 0.1921 0.1788 0.1784 0.2120 0.1902 0.2115 0.2216 0.2218 4.62%
HitRate@20 0.4829 0.4883 0.5117 0.5267 0.4759 0.4731 0.5254 0.5290 0.5224 0.5485 0.5533 4.60%
NDCG@20 0.2434 0.2541 0.2571 0.2728 0.2505 0.2496 0.2877 0.2723 0.2948 0.3015 0.3051 3.51%
MRR@20 0.1763 0.1887 0.1858 0.2018 0.1876 0.1869 0.2214 0.2002 0.2312 0.2319 0.2355 1.89%
HitRate@5 0.4787 0.4840 0.4723 0.4957 0.4726 0.4712 0.5141 0.3918 0.5156 0.5372 0.5681 10.19%
NDCG@5 0.4173 0.4237 0.4048 0.4297 0.4247 0.4090 0.4618 0.2841 0.4644 0.4644 0.4924 6.03%
MRR@5 0.3970 0.4038 0.3826 0.4078 0.4088 0.3884 0.4445 0.2482 0.4461 0.4405 0.4673 4.77%
HitRate@10 0.5531 0.5528 0.5503 0.5662 0.5300 0.5419 0.5756 0.4634 0.5770 0.6342 0.6517 12.95%
Beauty NDCG@10 0.4413 0.4459 0.4298 0.4525 0.4431 0.4319 0.4818 0.3072 0.4830 0.4957 0.5192 7.50%
MRR@10 0.4069 0.4129 0.3927 0.4172 0.4163 0.3978 0.4528 0.2577 0.4528 0.4533 0.4783 5.64%
HitRate@20 0.6484 0.6509 0.6553 0.6662 0.6119 0.6392 0.6634 0.5807 0.6677 0.7490 0.7501 12.34%
NDCG@20 0.4653 0.4706 0.4562 0.4775 0.4636 0.4563 0.5038 0.3368 0.5041 0.5247 0.5441 7.93%
MRR@20 0.4135 0.4196 0.3999 0.4239 0.4219 0.4044 0.4587 0.2658 0.4604 0.4613 0.4852 5.37%
HitRate@5 0.2049 0.2151 0.2393 0.2314 0.2083 0.2196 0.2375 0.2335 0.2342 0.2753 0.2845 18.87%
NDCG@5 0.1340 0.1468 0.1623 0.1547 0.1399 0.1499 0.1656 0.1579 0.1592 0.2006 0.2078 25.44%
MRR@5 0.1108 0.1245 0.1370 0.1297 0.1175 0.1271 0.1421 0.1331 0.1338 0.1764 0.1825 28.46%
HitRate@10 0.3208 0.3225 0.3547 0.3477 0.3121 0.3221 0.3434 0.3455 0.3489 0.3813 0.3883 9.47%
Pantry NDCG@10 0.1712 0.1813 0.1994 0.1921 0.1732 0.1829 0.1996 0.1940 0.1942 0.2315 0.2413 20.88%
MRR@10 0.1260 0.1385 0.1522 0.1450 0.1311 0.1406 0.1559 0.1479 0.1490 0.1907 0.1963 25.89%
HitRate@20 0.4728 0.4650 0.4974 0.4930 0.4583 0.4665 0.4784 0.4996 0.4953 0.5215 0.5242 4.92%
NDCG@20 0.2095 0.2171 0.2352 0.2287 0.2100 0.2192 0.2336 0.2327 0.2335 0.2686 0.2754 17.09%
MRR@20 0.1365 0.1483 0.1619 0.1549 0.1411 0.1504 0.1652 0.1584 0.1585 0.1986 0.2056 24.44%
lengthissetto50forallmodelsonalldatasets.WeusetheAdamoptimizer[18]toupdatemodelparameters.For
thegraph-basedmethods,wetunethenumberofgraphaggregationlayersamong{1,2,3,4,5}.
OurLLMGRmodelisimplementedbasedonLLaMA2-7B2usingtheHuggingFace3libraryandaccelerated
trainingisperformedusingDeepSpeed4.Allexperimentsareconductedon2NvidiaTeslaA100GPUs.TheID
embeddingsinourLLMGRmodelaredirectlyextractedfromthepre-trainedGCSANmodelwithoutmodi￿cation,
asGCSANhasdemonstrateditse￿ectivenessandgeneralization.WeemploytheAdamWoptimizer[24]for
modeloptimization,tunethelearningrateamong{1e-3,1e-4,1e-5},andsetthebatchsizeto16.Additionally,
weutilizeacosineschedulertoadjustthelearningrateoversteps,andtheweightdecayissetto1e-2.Inthe
auxiliarytuningstage,wetrainfor1epoch,andinthemajortuningstage,wetrainfor3epochsoneachdataset.
Inthefollowingsection,weinvestigatetheimpactofkeyhyperparametersingreaterdepth.
4.2 OverallPerformance(forQ1)
ToanswerQ1,wedisplaytheHitRate,NDCG,andMAPmetricsinTable2.Thetop-performingvaluesforeach
metricareemphasizedinbold,whilethesecond-bestresultsareunderscored.Inthe￿nalcolumnofthistable,
wequantifytheperformanceenhancementofLLMGRincomparisontothemostcompetitivemethodsobserved.
TheresultsofLLMGRaswellasthebaselinesonthreedatasetsarepresentedinTable2.Afterobservation,we
candrawthefollowingmain￿ndings:
2https://huggingface.co/meta-llama/Llama-2-7b
3https://github.com/huggingface/transformers
4https://github.com/microsoft/DeepSpeed
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.xxx:12 • Guo,etal.
Deeplearningmethods(i.e.,CASER,GRU4Rec)showtheirstrengthinSBRtasks.Theirperformancesare
•
superiortotraditionalrecommendationmethods,e.g.FPMC,whichprovethatadoptingadeeplearning
technology in recommendation systems is a necessary manner since neural networks can model the
complexinteractionsamongitems.
Theattention-basedmodels,includingNARMandSTAMP,achievehigherperformancemetricscompared
•
tonon-attentionalternativeslikeGRU4RecandCASER.Thesuccessofattention-basedmethodsstemsfrom
theirabilitytodiscernshiftsinuserinterestswithinsessions,e￿ectivelypinpointingtheprimaryintent
byharnessingattentiontosynthesizingpersonalinterestsfromlong-termmemoriesorrecentshort-term
behaviors.
Thegraph-basedmethods,i.e.,SRGNN,GCSAN,NISERand,HCGRshowtheirsuperioritycomparedto
•
graph-freemethods,duetotheremarkablecapacityofgraphneuralnetworkstocapturecomplexinteraction
ofuserbehaviorsanddescribethecoherenceofitemsinasession.NISERpresentsunstableperformance,
especiallyonthePantrydataset,probablyduetoitsinabilitytocapturelong-termdependencies.Incontrast,
GCSANandHCGR,bydisseminatinglong-distanceinformationviaGNNsandmitigatingover-￿tting
issues,leadtheperformanceamongthebaselinemodels.
OurproposedLLMGRanditsvariantLLMGR-Msigni￿cantlyoutperformallthebaselinemethodsonall
•
threedatasetsthankstothepowerfulabilityofLLM.TofairlycomparewiththeconventionalSBRmethods,
wesetthevariantLLMGR-M,whichonlytrainsbythemajortuningtaskwithoutanytextualinformation.
TherelativeimprovementsofLLMGRinperformanceoverthemostcompetitivebaselinemethodsare
about8.68%onHR@20,10.71%onNDCG@20,and11.75%onMRR@20.Takingapartconsiderationfrom
the extra-textual information, LLMGR-M still outperforms most of the baseline, especially in ranking
metrics.Thesigni￿cantimprovementfullydemonstratesthee￿ectivenessofourframework.
4.3 PortabilityofLLMGR(forQ2)
OneofthekeyinnovationsofLLMGRisitsabilitytoenhancetraditionalSBRmodelsthroughthesemantic
richnessofLLMs.Asmentionedearlier,ourframeworkisportable,allowingittobeappliedtomostexisting
methodsandenhancetheirperformances.Asourmethodrequirestheuseofembeddings,wehaveonlyselected
deeplearning-basedmethodstoadapttoLLMGR.Tofurtherillustratethee￿ectivenessandportabilityofLLMGR,
weapplyourframeworktothecomparisonmethods.Theresultsbetweentheoriginalmethodsandtheremolded
versionsondi￿erentdatasetsareplottedinTable3,withthemethodsfollowedby‘-L’denotingtheirvariant
underourproposedLLMGR.However,ourframeworkisspeci￿callydesignedforgraph-basedSBRmethods.To
adaptourframeworktograph-freemethods,weconcealthe’<sessiongraphID>’placeholderfromthebehavior
patternsmodelingpromptduringthemajortuningtask.FromTable3,wecandrawthefollowingconclusions:
LLMGRworkswellwithvariousSBRmodels,notonlywiththeGCSAN.Asobserved,thevariantmethods
•
outperformtheiroriginalversionswithaverageimprovementsof8.58%,and17.09%onMusic,andBeauty,
respectively,whichdemonstratesthee￿ectivenessofLLMGR.
Theimprovementsontheoriginallylesse￿ectivemodelsareevenmoresigni￿cant,suchasSTAMPand
•
GRU4Rec.Besides,we￿ndthatsimplermodels(e.g.GRU4Rec,STAMP)canoutperformmostbaselineSBR
modelsunderLLMGR,whichindicatesthatLLMGRe￿ectivelyprovidesadditionalinformationtotheSBR
models.
UnderourLLMGRframework,thebaselinedemonstratessigni￿cantimprovementsacrossallmetrics,
•
especiallyatsmallerKvalues.Furthermore,rankingperformancemetricssuchasNDCGandMRRshow
considerablylargerimprovementscomparedtoHitRate.ThissuggeststhatourproposedLLMGRe￿ectively
capturesuserpreferences,resultinginmoreaccurateandreliablerecommendationoutcomes.
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.IntegratingLargeLanguageModelswithGraphicalSession-BasedRecommendation • xxx:13
Table3. PortabilityofLLMGRonMusicandBeautydatasets.
Dataset Method HitRate@5 NDCG@5 MRR@5 HitRate@10 NDCG@10 MRR@10 HitRate@20 NDCG@20 MRR@20
CASER 0.2556 0.1884 0.1663 0.3536 0.2200 0.1793 0.4883 0.2541 0.1887
CASER-L 0.2678 0.1986 0.1759 0.3558 0.2268 0.1874 0.4839 0.2590 0.1961
Improvement 4.79% 5.41% 5.74% 0.63% 3.09% 4.49% -0.91% 1.92% 3.93%
GRU4Rec 0.2628 0.1867 0.1618 0.3716 0.2218 0.1762 0.5117 0.2571 0.1858
GRU4Rec-L 0.2919 0.2211 0.1979 0.3920 0.2533 0.2111 0.5236 0.2864 0.2201
Improvement 11.09% 18.43% 22.31% 5.49% 14.21% 19.78% 2.32% 11.41% 18.43%
NARM 0.2782 0.2026 0.1779 0.3860 0.2374 0.1921 0.5267 0.2728 0.2018
NARM-L 0.2892 0.2146 0.1902 0.3846 0.2456 0.2030 0.5135 0.2781 0.2119
Improvement 3.94% 5.93% 6.93% -0.38% 3.45% 5.65% -2.50% 1.94% 5.00%
STAMP 0.2543 0.1882 0.1665 0.3478 0.2182 0.1788 0.4759 0.2505 0.1876
STAMP-L 0.3009 0.2300 0.2067 0.3862 0.2574 0.2179 0.5161 0.2902 0.2269
Improvement 18.32% 22.22% 24.13% 11.05% 17.98% 21.89% 8.46% 15.85% 20.94%
Music
SRGNN 0.2610 0.1901 0.1669 0.3476 0.2180 0.1784 0.4731 0.2496 0.1869
SRGNN-L 0.2699 0.2018 0.1795 0.3729 0.2349 0.1931 0.5009 0.2672 0.2019
Improvement 3.41% 6.17% 7.56% 7.26% 7.76% 8.25% 5.88% 7.08% 8.00%
GCSAN 0.2905 0.2218 0.1992 0.3873 0.2530 0.2120 0.5254 0.2877 0.2214
GCSAN-L 0.3068 0.2330 0.2094 0.4085 0.2719 0.2218 0.5533 0.3051 0.2355
Improvement 5.62% 5.08% 5.12% 5.46% 7.47% 4.62% 5.30% 6.08% 6.39%
NISER 0.2806 0.2019 0.1760 0.3870 0.2363 0.1902 0.5290 0.2723 0.2002
NISER-L 0.3048 0.2322 0.2084 0.4063 0.2649 0.2218 0.5380 0.2981 0.2309
Improvement 8.60% 14.98% 18.38% 4.99% 12.08% 16.61% 1.72% 9.50% 15.30%
HCGR 0.3016 0.2226 0.2001 0.3965 0.2631 0.2115 0.5224 0.2948 0.2312
HCGR-L 0.3171 0.2358 0.2128 0.4256 0.2836 0.2272 0.5534 0.3151 0.2479
Improvement 5.15% 5.90% 6.36% 7.33% 7.80% 7.41% 5.94% 6.90% 7.23%
CASER 0.4840 0.4237 0.4038 0.5528 0.4459 0.4129 0.6509 0.4706 0.4196
CASER-L 0.5467 0.4774 0.4546 0.6333 0.5053 0.4661 0.7297 0.5297 0.4727
Improvement 12.95% 12.66% 12.57% 14.57% 13.32% 12.86% 12.11% 12.56% 12.65%
GRU4Rec 0.4723 0.4048 0.3826 0.5503 0.4298 0.3927 0.6553 0.4562 0.3999
GRU4Rec-L 0.5497 0.4800 0.4570 0.6291 0.5055 0.4675 0.7295 0.5309 0.4745
Improvement 16.40% 18.56% 19.45% 14.33% 17.62% 19.04% 11.31% 16.37% 18.64%
NARM 0.4957 0.4297 0.4078 0.5662 0.4525 0.4172 0.6662 0.4775 0.4239
NARM-L 0.5542 0.4828 0.4592 0.6411 0.5105 0.4704 0.7434 0.5363 0.4775
Improvement 11.80% 12.34% 12.61% 13.24% 12.81% 12.76% 11.59% 12.31% 12.64%
STAMP 0.4726 0.4247 0.4088 0.5300 0.4431 0.4163 0.6119 0.4636 0.4219
STAMP-L 0.5375 0.4716 0.4498 0.6325 0.5024 0.4626 0.7345 0.5280 0.4695
Improvement 13.74% 11.04% 10.03% 19.35% 13.38% 11.12% 20.04% 13.88% 11.30%
Beauty
SRGNN 0.4712 0.4090 0.3884 0.5419 0.4319 0.3978 0.6392 0.4563 0.4044
SRGNN-L 0.5497 0.4800 0.4570 0.6291 0.5055 0.4675 0.7295 0.5309 0.4745
Improvement 16.68% 17.34% 17.65% 16.09% 17.06% 17.52% 14.12% 16.35% 17.31%
GCSAN 0.5141 0.4618 0.4445 0.5756 0.4818 0.4528 0.6634 0.5038 0.4587
GCSAN-L 0.5681 0.4924 0.4673 0.6517 0.5192 0.4783 0.7501 0.5441 0.4852
Improvement 10.51% 6.62% 5.14% 13.21% 7.78% 5.65% 13.06% 8.01% 5.78%
NISER 0.3918 0.2841 0.2482 0.4634 0.3072 0.2577 0.5807 0.3368 0.2658
NISER-L 0.5035 0.4339 0.4110 0.5896 0.4617 0.4225 0.6971 0.4889 0.4299
Improvement 28.52% 52.73% 65.59% 27.24% 50.28% 63.92% 20.06% 45.17% 61.76%
HCGR 0.5156 0.4644 0.4461 0.5770 0.4830 0.4528 0.6677 0.5041 0.4604
HCGR-L 0.5751 0.5016 0.4761 0.6568 0.5237 0.4864 0.7513 0.5593 0.4870
Improvement 11.54% 8.03% 6.73% 13.82% 8.42% 7.43% 12.53% 10.94% 5.77%
Theperformancesofallkindsofmodels,notonlygraph-basedmodels,improvesigni￿cantlyonthethree
•
datasets,whichdemonstratesthattheworldknowledgeandcontextcomprehensionabilitiesofLLMcould
enhanceitemunderstandingandusermodeling.Suchabilitiesinsession-basedrecommendationdataare
essentialforpredictingusers’nuancedbehavior,whilesuchinformationisjustignoredbythetraditional
SBRmodels.
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.xxx:14 • Guo,etal.
Table4. Theperformancewithdi￿erentinstructionpromptsonMusicandBeautydatasets.
Dataset Method HitRate@5 NDCG@5 MRR@5 HitRate@10 NDCG@10 MRR@10 HitRate@20 NDCG@20 MRR@20
LLMGR-M(SRGNN) 0.2676 0.2018 0.1801 0.3653 0.2334 0.1931 0.4952 0.2661 0.2020
LLMGR(SRGNN) 0.2699 0.2018 0.1795 0.3729 0.2349 0.1931 0.5009 0.2672 0.2019
Improvement 0.83% -0.01% -0.33% 2.08% 0.67% -0.01% 1.16% 0.41% -0.08%
LLMGR-M(GCSAN) 0.3018 0.2300 0.2063 0.3996 0.2615 0.2193 0.5254 0.2932 0.2279
Music LLMGR(GCSAN) 0.3047 0.2310 0.2083 0.4052 0.2633 0.2216 0.5485 0.3015 0.2319
Improvement 0.97% 0.45% 0.93% 1.39% 0.69% 1.05% 4.40% 2.82% 1.74%
LLMGR-M(NISER) 0.3036 0.2276 0.2025 0.4066 0.2607 0.2161 0.5340 0.2927 0.2248
LLMGR(NISER) 0.3048 0.2322 0.2084 0.4063 0.2649 0.2218 0.5380 0.2981 0.2309
Improvement 0.37% 2.03% 2.91% -0.09% 1.60% 2.66% 0.76% 1.83% 2.71%
LLMGR-M(SRGNN) 0.5102 0.4437 0.4217 0.5993 0.4725 0.4336 0.7155 0.5018 0.4416
LLMGR(SRGNN) 0.5497 0.4800 0.4570 0.6291 0.5055 0.4675 0.7295 0.5309 0.4745
Improvement 7.76% 8.19% 8.37% 4.97% 6.99% 7.82% 1.95% 5.81% 7.45%
LLMGR-M(GCSAN) 0.5372 0.4644 0.4405 0.6342 0.4957 0.4533 0.7490 0.5247 0.4613
Beauty LLMGR(GCSAN) 0.5681 0.4924 0.4673 0.6517 0.5192 0.4783 0.7501 0.5441 0.4852
Improvement 5.76% 6.02% 6.10% 2.77% 4.76% 5.52% 0.15% 3.70% 5.17%
LLMGR-M(NISER) 0.4865 0.4153 0.3918 0.5731 0.4431 0.4032 0.6879 0.4722 0.4112
LLMGR(NISER) 0.5035 0.4339 0.4110 0.5896 0.4617 0.4225 0.6971 0.4889 0.4299
Improvement 3.49% 4.48% 4.90% 2.87% 4.20% 4.79% 1.34% 3.53% 4.56%
4.4 AblationStudies(forQ3)
4.4.1 E￿ectofMulti-taskInstructionPrompt. OurproposedLLMGRconsistsoftwotuningstages.Theauxiliary
instructiontuningstageaimstoendowtheLLMtounderstandthestructureofthesessiongraphandestablish
text-nodeassociations.Themajortuningtaskfocusesoncapturingthebehavioralpatternswithinsessiongraphs
underLLM.ToanswerQ3,weconductablationexperimentswithasimpli￿edversionofLLMGR,e.g.,LLMGR-M.
Thisvariantremovesthenode-contextaligningpromptandcontrastivestructure-awareprompt(auxiliarytuning
stage),onlytuningundermajortuningtask.Furthermore,togainadeeperinsightintoourtailoredprompt,
weadapttheselectedcomparisonpre-trainedgraph-basedSBR(e.g.,SRGNN,GCSAN,NISER)toconductthis
experimentontheMusicandBeautydataset.TheexperimentalresultsareshowninTable4,wecandrawthe
followingobservations:
After taking apart the auxiliary tuning prompt from the LLMGR framework, which means removing
•
thetwoalignmenttasksforLLMunderthetuningstage.ThisoperationmaydisabletheLLMGRfrom
understandingthegraphstructureand realizetextofnodes.Asexpected, thismodi￿cationleadstoa
markeddeclineinperformanceacrossnearlyallmetrics,withamorepronounceddropobservedatlowerK
values.Moreover,thedegradationismoresubstantialinranking-basedmetricslikeNDCGandMRRthan
inHitRate.TheseobservationsindicatethatLLMGRisparticularlyadeptatdiscerninguserpreferences,
whichtranslatestoenhancedaccuracyanddependabilityintherecommendationsitgenerates.Besides,
theworldknowledgeandcontextcomprehensionabilitiesofLLMscouldenhanceitemunderstandingand
usermodeling.
BycomparingLLMGRtoLLMGR-Mamongallpre-trainedSBRmethods,itisobviousthatafterremoving
•
theauxiliarytuningstage,theaverageperformancedropsby2.04%,1.65%and1.42%intermsofHitRate@20,
NDCG@20,andMRR@20onMusicdataset,respectively,andthecorrespondingaveragedecreaseratesare
1.13%,4.16%and5.40%onBeauty.
4.4.2 E￿ectofTwo-stageTuningStrategy. Intheabovesection,wehavedemonstratedthee￿ectivenessofour
multi-taskinstructionprompt.Inthissection,wewillexaminethee￿ectsofourtwo-stagetuningstrategyonthe
LLMGR’se￿cacy.Thistwo-stageoptimizationprocess￿rstequipstheLLMGRwiththeabilitytodecodegraph
structuresandalignnodeswiththeirrespectivecontexts,thenfurthermodelbehavioralpatternswithinsession
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.IntegratingLargeLanguageModelswithGraphicalSession-BasedRecommendation • xxx:15
Fig.4. Theperformancewithdi￿erenttuningstrategiesonthreedatasets.
graphsusingbehaviorpatternmodelingprompts.Inthissubsection,wefurtherexplorethefollowingtuning
strategies:
Single-stage Tuning (LLMGR-O): Here, we employ only one tuning approach, directly tuning all the
•
trainableparameters,includingΦ ,Φ ,Φ andΦ .Thismeansthattheentirerangeofprompts
LoRA in out SBR
impliesthatallkindsofprompts(e.g.,behaviormodelingandgraphcomprehension)willbetrainedtogether.
Two-stageTuningWithoutFreeze(LLMGR-F):We￿rst￿ne-tuneLLMGRundertheauxiliarytaskandthen
•
proceedto￿ne-tunethemajortask.Itisimportanttonotethat,inthisvariant,noneoftheparameterswill
befrozen.
We compare these methods in terms of their overall performance, as shown in Fig. 4, and we can draw the
followingobservations:
LLMGRoutshinesbothLLMGR-OandLLMGR-Facrossallscenarioswithinthetwodatasets.Thesuccess
•
ofLLMGRcanbelinkedtoitsstrategyofsegregatingdistinctprompttypesduringseparatetuningstages,
whichmoree￿ectivelydiscernsthesharedcharacteristicsanduniqueaspectsofvarioustasks.Theobserved
improvementsmaybeascribedtothetwo-stagetuningstrategy’sabilitytomitigateinterferencebetween
semanticinformationandbehavioralpatternmodeling,therebyenhancingtheoverallrecommendation
performance.Furthermore,byallocatingdistinctpromptstodi￿erentstagesofthetuningprocess,LLMGR
ensuresafocusedoptimizationforeachtask.Duringtheinitialstage,themodelconcentratesongraspingthe
semanticsoftheitemsandtheuser’simmediatecontext.Thisformsasolidfoundationforthesubsequent
stage,whereLLMGRhonesinonintricatebehavioralpatterns,utilizingthegroundworklaidinthe￿rst
stagetoyieldamorenuancedunderstandingofuserinteractions.
Anin-depthexaminationoftheoutcomesdemonstratesthatourproposedLLMGRmethoduniformly
•
surpassestheothertuningstrategyvariantacrossbothdatasets.Thisconsistentedgeinperformance
highlightsthee￿cacyofourtwo-stagetuningstrategy.
4.5 E￿ectivenessAnalysis(forQ4)
4.5.1 AnalysisonSessionLengths. Thissectionaimstoanalyzetheperformanceofvariousrecommendation
modelsinhandlingsessionswithdi￿erentlengths,speci￿callyinthecontextoftheMusicandBeautydatasets.
Inordertoconductacomparativeanalysis,thesessionsareclassi￿edintotwodistinctcategoriesbasedontheir
length."Short"sessions,whichconsistofsevenitemsorfewer.Thechoiceofsevenasathresholdisstrategic—it
alignscloselywiththeaveragesessionlengththathasbeenobservedacrossalldatasetsexaminedinthestudy.
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.xxx:16 • Guo,etal.
(a) Music.
(b) Beauty.
Fig.5. Performancecomparisonondi￿erentsessionlengthsevaluatedinallmetricsontwodatasets.
Byde￿ning"short"inthisway,theresearchersarefocusingonsessionsthatareatorbelowtheaveragelength,
lookingtounderstandhowwelldi￿erentmodelscanhandlesessionsthatmaynotprovideasubstantialamount
ofuser-iteminteractiondata."Long"sessions,arede￿nedascontainingmorethansevenitems.Thesesessions
exceedtheaveragelengthandthuspresentadi￿erentkindofchallengeforrecommendationsystems—they
typicallyprovidemoreinteractiondatawithinasinglesession,whichcouldbebene￿cialformakingaccurate
recommendationsbutmightalsointroducemorecomplexityduetothevarietyofitemsanduserpreferences
re￿ectedinthelongersessiondata.Bysegmentingthesessionsinthismanner,thestudyintendstoshedlighton
theadaptabilityande￿ciencyofdi￿erentrecommendersystemsmodelswhenconfrontedwithvaryingamounts
ofinformation.Theperformanceofthemodelscanbein￿uencedbytheamountofdatatheyhavetowork
with—toolittledatamightnotprovideenoughinsightsforaccuraterecommendations,whiletoomuchdata
mightmakeitdi￿culttoidentifythemostrelevantitemsfortheuser.Theperformancemetricsforeachmodel
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.IntegratingLargeLanguageModelswithGraphicalSession-BasedRecommendation • xxx:17
(a) Music.
(b) Beauty.
Fig.6. Performancecomparisononcoldandwarmitemsevaluatedinallmetricsontwodatasets.
acrossbothsessionlengthsaredetailedinFig.5.BasedontheresultsshowninFig.5,wehaveobservationsas
follows:
OurproposedLLMGRanditsvariantsLLMGR-Mperformstablyontwodatasetswithdi￿erentsession
•
lengths.ItdemonstratesthesuperiorabilityoftheproposedmethodandtheadaptabilityofLLMinSBR
scenario.
ThesuperiorperformanceofLLMGRoverLLMGR-M,indicatesarobustnessinthecoredesignofLLMGR.
•
Thisrobustnessallowsittoconsistentlyoutperformnotjustitsvariant,butpotentiallyotherSBRmethods
aswell,acrossdi￿erentsessionlengths.ThefactthatLLMGRexcelsinbothlongandshortsessionsis
noteworthy,asitimpliesthatthemodelisnotmerelye￿ectiveinonescenariobutisgenuinelyversatile,
dealingwellwithbothrichandsparsedataenvironments.Theauxiliarytuningtaskmentionedsuggests
thatLLMGRisequippedwithamechanismthathelpscontextualizeeachnodewithinthegraphstructure
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.xxx:18 • Guo,etal.
ofthesessiondata.Byaligningthenodewithitscontextualinformation,LLMGRcanleverageadditional
signalsforrepresentationlearning.Thisprovidesamorenuancedandsemanticallyrichrepresentationof
userpreferences,whichisparticularlybene￿cialincapturingtheunderlyingpatternsinuserbehaviorthat
maynotbeimmediatelyapparentfrominteractiondataalone.
4.5.2 AnalysisonCold-startScenarios. Inthe￿eldofrecommendersystems,thecold-startproblemisindeeda
signi￿canthurdle.Itprimarilya￿ectsnewusersoritemsthathaverecentlybeenaddedtothesystem’sdatabase.
For new users, the system struggles to make personalized recommendations because it lacks data on their
preferences.Fornewitems,thesystemmaynothaveenoughinteractiondatatoestablishwhereortowhom
theseitemsmightappeal.Totestthee￿ectivenessofLLMGR,anexperimentwassetupwherethetestingset
wasdividedintowarmandcoldsubsets.The’warm’subsetactsasabenchmarkfortheperformanceofLLMGR
inidealconditions,whereuser-iteminteractionsareabundantandthesystemhashadampleopportunityto
learnfromtheseinteractions.AstrongperformanceinthewarmsubsetwouldindicatethatLLMGRisatleast
onparwithtraditionalrecommendationtechniquesinadata-richenvironment.The’cold’subsetisthetrue
testofLLMGR’sinnovation.SuccessinthissubsetwoulddemonstratethatLLMGRcane￿ectivelyalleviatethe
cold-startproblemandsuggestthatithasmechanismsinplacetoderivemeaningfulrecommendationsfrom
limitedinteractiondata.TheobservationsmadefromFigure6
Thesigni￿cantresultsobtainedfromthecoldscenario,whereeachitemhaslimitedinteractionsanditis
•
challengingtocapturebehavioralpreferences,furtherunderscoretherobustnessofourproposedLLMGR
whenhandlingextremelysparsedata.Additionally,italsoshowcasestheadvantagesoftheLLM-based
methodincoldscenarios,highlightinghowtraditionalSBRmethodslackthepro￿ciencytoe￿ectively
handlesuchscenarios.
Whencomparingtheperformanceinwarmandcoldscenarios,theperformancegainsarenotablymore
•
pronouncedinthecoldscenario.ThiscouldbeattributedtothecapabilityofLLMs,whichenhancesthe
performanceofLLMGR,enablingittoachievesatisfactoryresultswithlimiteddatafeatthattraditional
recommendationmethodsoftenstruggletoaccomplish.
Overall,theevaluationfromtheseparationoftestingdatainto’warm’and’cold’subsetsenablesanuanced
evaluationofLLMGR’sperformanceandhighlightsitspotentialstrengthsintacklingoneofthemostpersistent
issuesinrecommendersystems.
5 CASESTUDY(FORQ5)
5.1 EmebddingVisualization
Asdemonstratedinsection4.3,ourframeworkisportableandallowsittobeappliedtomostexistingmethods
andenhancetheirperformances.Tofurtherinvestigatethee￿ectsofpre-trainedSBRmethodsanditaftertuning
underourproposedLLMGR.Followingpreviouswork[15,33],weemployPrincipalComponentAnalysis(PCA)
tovisualizethenodeembeddings.“GCSAN”and"SRGNN"representitemembeddingfromthecorrespondingpre-
trainedSBRmethod,while“LLMGR”representstheitemembeddingunderourproposedframework.Asshown
inFig.7,8,thenodesofallpre-trainedmodelsareconcentratedinthemiddle,withpoordi￿erentiation.After
tuningunderLLMGR,therepresentationdistributionofthenodesbecomesmoreuniform,whichisbene￿cialfor
mitigatinginterferencebetweensemanticinformationandbehavioralpatternmodeling.
6 RELATEDWORKS
6.1 Session-basedRecommendation
Earlyapproachestosession-basedrecommendationareheavilyreliantonMarkovchains(MC).Forinstance,
FPMC[28]mergesmatrixfactorizationwithMCtolearnboththegeneralpreferencesandtheshort-terminterests
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.IntegratingLargeLanguageModelswithGraphicalSession-BasedRecommendation • xxx:19
(a) Music. (b) Beauty.
Fig.7. TheembeddingvisualizationofLLMGRandcomparisonmethod(GCSAN)onmusicandbeautydataset.
(a) Music. (b) Beauty.
Fig.8. TheembeddingvisualizationofLLMGRandcomparisonmethod(SRGNN)onmusicandbeautydataset.
ofusersforbasketrecommendation.Meanwhile,Fossil[10]integratessimilarity-basedmethodswithMCtoo￿er
personalizedsession-basedrecommendations.However,akeydrawbackofMC-basedmodelsisthattheyhave
beenstrugglingtocapturelong-rangedependencies,undertheassumptionthatfutureactionsareonlydependent
onthemostrecentpaststate.Inrecentyears,RecurrentNeuralNetworkshavebeenadoptedtomodeltemporal
dependencies.PioneeringtheRNN-basedsessionrecommendation,GRU4Rec[11]utilizestheGatedRecurrent
Unit(GRU)tocapturelong-termdependenciesacrosssessions,signi￿cantlyoutperformingMC-basedmethods
withanovelpairwiserankingloss.BuildingonthesuccessofGRU4Rec,MV-RNN[4]enhancesrecommendations
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.xxx:20 • Guo,etal.
byincorporatingmultimodaldata,andROM[44]employsaninteractiveself-attentionmechanismforrating
prediction. Nevertheless, RNNs inherently assumes a ￿xed sequential dependence between adjacent items,
resultinginpotentialinaccuraciesandnoiseinsessiondata,especiallyinscenarioslikemusicrecommendation.
ConvolutionalNeuralNetworkshavealsobeenleveragedtodiscernpatternsinsession-basedrecommendation.
CNN-basedmodels[30]applyconvolutional￿lterstocapturevaryingordersofuserbehavior.Morerecently,
attentionmechanism-basedmodels[32]haveshownexceptionalperformance.Forexample,ahybridencoder
withanattentionmechanismwasusedbyLietal.tomodelbothsequentialbehaviorandsessioninterests[19].
STAMP[23]introducesashort-termattentionprioritymodelthate￿ectivelycapturedboththelong-termsession
contextandtheuser’simmediateinterests.Advancedsession-basedrecommendationmodelsemployattention
mechanismstoidentifyuserbehaviorpatternsoverlongsequences.However,discerningboththeimplicitand
explicitrelationsbetweenadjacentbehaviorsremainschallenging.
GraphNeuralNetworks(GNNs)areadeptatuncoveringsuchrelationships[14,35]andcancapturecomplex
userbehaviorinteractions.SRGNN[36]andGCSAN[39]representtwosuchmethodsthatbuilddirectedgraphs
forsequencesandemployself-attentiontoenhancerepresentationlearning,respectively.Further,Wuetal.focuses
oncapturinguserhistorywithinsessionsusingadot-attentionmechanism[37],whileFGNN[27]proposes
a weighted attention layer for learning embeddings. Memory models have also been utilized to encompass
bothlong-termandshort-termbehaviors[26].WhileGNN-basedmethodshaveseensigni￿cantsuccess,they
predominantlyhavesigni￿cantchallengesinunderstandingthecontentofitems.
6.2 LargeLanguageModelsforRecommendation
As Large Language Models (LLMs) have driven impressive innovations in arti￿cial intelligence recently, re-
searchershavebeentryingtoleveragetheirstrongcapabilitiesinlanguagesemanticunderstandingonrecom-
mendersystems.At￿rst,researchersfocusonusingpromptingorin-contextlearning.ChatREC[5]proposethe
ideaofaninteractiverecommendsystembyutilizingChatGPTprompttoinjectuserinformationandinstruct
LLMstogeneraterecommendresults.Similartotheaboveconcept,Wangetal.[34]incorporateresearchintoa
3-steppromptingthatguidesGPT-3tocarrysub-tasksthatcapturetheuser’spreferencesandgeneratearanked
listrecommendation.Liuetal.[22]proposeageneralrecommendationpromptconstructionframework,relying
onlyonthepromptsthemselvestoconvertrecommendationtasksintonaturallanguagetasks.
However,sincetheunderlyingsemanticsofnaturallanguageandrecommendersystemsareincompatible,and
LLMshavenotbeenpre-trainedforrecommendersystems,researchershavemadee￿ortstoaddressthisproblem.
By￿ne-tuningLLMswithempiricalrecommendationdata,Zhangetal.[40]andBaoetal.[3]haveachieved
superior model performances. Based on comprehensive research, Kang et al.[17] discovers the importance
ofuserinteractiondataandproposestoformattheuserhistoricalinteractionsasprompts.Moreover,some
concurrentresearchworkshavealsoshedlightonbridgingthegapbetweennaturallanguageandrecommender
systems.BIGRec[2]integratescollaborativeinformationandutilizesatwo-stepgroundingframeworktogenerate
recommendationswithLLM.TransRec[21]employsmulti-facetidenti￿ers,combiningID,title,andattributesto
balanceitemdistinctivenessandsemantics.CoLLM[42]capturescollaborativeinformationthroughanexternal
traditionalmodelandmapsittotheinputtokenembeddingspaceofLLM,formingcollaborativeembeddingsfor
LLMusage[20].TheaboveworkshavemadegreatprogressinadaptingLLMstorecommendationsystems,but
haveproblemsinprovidingpersonalizedrecommendationsduetolackofsophisticatedtasktraining.
7 CONCLUSIONS
SBR systems play a crucial role in capturing users’ dynamic preferences by analyzing the session of their
interactions. The SOTA in this domain has been signi￿cantly advanced by the application of GNNs. These
graph-basedSBRmethodsexcelatcapturingtheintricatepatternsofuserbehaviorsandpreferences,whichis
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.IntegratingLargeLanguageModelswithGraphicalSession-BasedRecommendation • xxx:21
especiallycrucialinscenarioswithlimitedinteractiondata.Despitethesenotablestrides,existingSBRmethods
focusonuserinteractionswhileoftensideliningtherichtextualcontentthataccompaniesusersanditems.This
haslimitedtheirabilitytograspthefullcontextandnuancesbehinduserpreferences.WiththeadventofLLMs,
thereisnowanopportunitytotranscendtheselimitationsbyintegratingtheirsophisticatednaturallanguage
understandingcapabilitieswithSBRsystems.
Inthiswork,wehavepresentedLLMGR,anovelframeworkthatsynergizesLLMswithgraph-basedSBR
approaches.LLMGRisdesignedtoovercomethechallengesposedbytheintegrationofthesetwopowerful
technologies.Itaddressestwocentralchallenges:￿rst,theexpressionofgraph-basedSBRtasksinnaturallanguage,
andsecond,thealignmentoftextualinformationwithgraphnodestoe￿ectivelycaptureuserpreferences.Our
approach successfully bridges the gap between the graph-structured data of SBR tasks and the sequential
processingstrengthsofLLMs.Throughaninnovativetwo-stagetuningprocess,LLMGRprovidesameansto
associatetextwithgraphnodesandsubsequentlycapturethedynamicbehavioralpatternsofusers.Theapproach
notonlyenhancestheperformanceofsession-basedrecommendationsbutalsopioneersanewdirectionfor
futureresearchattheconvergenceofcomplexgraphmodelingandadvancedlanguageprocessingmodels.The
extensiveexperimentsconductedonthreereal-worlddatasetsa￿rmthee￿cacyofLLMGR,showcasingits
superiorityoverseveralcompetitivebaselines.Thisvalidatesthepotentialofourapproachtopushtheboundaries
ofSBRsystemsbyenrichingthemwiththedepthandbreadthofunderstandingthatLLMso￿er.Infuturework,
wewillexplorehowtoextendthecurrentapproachinmulti-domainrecommendations,sothatitcansupport
more￿exibleinteractionwithusers.
REFERENCES
[1] [n.d.].
[2] KeqinBao,JizhiZhang,WenjieWang,YangZhang,ZhengyiYang,YanchengLuo,FuliFeng,XiangnaanHe,andQiTian.2023.Abi-step
groundingparadigmforlargelanguagemodelsinrecommendationsystems.arXivpreprintarXiv:2308.08434(2023).
[3] KeqinBao,JizhiZhang,YangZhang,WenjieWang,FuliFeng,andXiangnanHe.2023. Tallrec:Ane￿ectiveande￿cienttuning
frameworktoalignlargelanguagemodelwithrecommendation.arXivpreprintarXiv:2305.00447(2023).
[4] QiangCui,ShuWu,QiangLiu,WenZhong,andLiangWang.2018.MV-RNN:Amulti-viewrecurrentneuralnetworkforsequential
recommendation.IEEETransactionsonKnowledgeandDataEngineering32,2(2018),317–331.
[5] YunfanGao,TaoSheng,YoulinXiang,YunXiong,HaofenWang,andJiaweiZhang.2023.Chat-rec:Towardsinteractiveandexplainable
llms-augmentedrecommendersystem.arXivpreprintarXiv:2303.14524(2023).
[6] N.Guo,X.Liu,S.Li,M.Ha,Q.Ma,B.Wang,Y.Zhao,L.Chen,andX.Guo.5555. HyperbolicContrastiveGraphRepresentation
LearningforSession-basedRecommendation. IEEETransactionsonKnowledgeamp;DataEngineering01(aug5555),1–15. https:
//doi.org/10.1109/TKDE.2023.3295063
[7] NaichengGuo,XiaoleiLiu,ShaoshuaiLi,QiongxuMa,KaixinGao,BingHan,LinZheng,ShengGuo,andXiaoboGuo.2023.Poincaré
HeterogeneousGraphNeuralNetworksforSequentialRecommendation. ACMTrans.Inf.Syst.41,3(2023),63:1–63:26. https:
//doi.org/10.1145/3568395
[8] XiaoboGuo,ShaoshuaiLi,NaichengGuo,JiangxiaCao,XiaoleiLiu,QiongxuMa,RunshengGan,andYunanZhao.2023.Disentangled
RepresentationsLearningforMulti-targetCross-domainRecommendation. ACMTrans.Inf.Syst.41,4(2023),85:1–85:27. https:
//doi.org/10.1145/3572835
[9] PriyankaGupta,DikshaGarg,PankajMalhotra,LovekeshVig,andGautamMShro￿.2019. NISER:normalizeditemandsession
representationswithgraphneuralnetworks.arXivpreprintarXiv:1909.04276(2019).
[10] RuiningHeandJulianMcAuley.2016.Fusingsimilaritymodelswithmarkovchainsforsparsesequentialrecommendation.In2016IEEE
16thInternationalConferenceonDataMining(ICDM).IEEE,191–200.
[11] BalázsHidasi,AlexandrosKaratzoglou,LinasBaltrunas,andDomonkosTikk.2015.Session-basedrecommendationswithrecurrent
neuralnetworks.arXivpreprintarXiv:1511.06939(2015).
[12] YupengHou,JunjieZhang,ZihanLin,HongyuLu,RuobingXie,JulianMcAuley,andWayneXinZhao.2023.Largelanguagemodelsare
zero-shotrankersforrecommendersystems.arXivpreprintarXiv:2305.08845(2023).
[13] EdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.2022. LoRA:
Low-RankAdaptationofLargeLanguageModels.InTheTenthInternationalConferenceonLearningRepresentations,ICLR2022,Virtual
Event,April25-29,2022.OpenReview.net. https://openreview.net/forum?id=nZeVKeeFYf9
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.xxx:22 • Guo,etal.
[14] JinHuang,WayneXinZhao,HongjianDou,Ji-RongWen,andEdwardYChang.2018.Improvingsequentialrecommendationwith
knowledge-enhancedmemorynetworks.InThe41stInternationalACMSIGIRConferenceonResearch&DevelopmentinInformation
Retrieval.505–514.
[15] JongwonJeong,JeongChoi,HyunsoukCho,andSeheeChung.2022.FPAdaMetric:False-Positive-AwareAdaptiveMetricLearning
forSession-BasedRecommendation.InThirty-SixthAAAIConferenceonArti￿cialIntelligence,AAAI2022,Thirty-FourthConferenceon
InnovativeApplicationsofArti￿cialIntelligence,IAAI2022,TheTwelvethSymposiumonEducationalAdvancesinArti￿cialIntelligence,
EAAI2022VirtualEvent,February22-March1,2022.AAAIPress,4039–4047. https://doi.org/10.1609/AAAI.V36I4.20321
[16] JianchaoJi,ZelongLi,ShuyuanXu,WenyueHua,YingqiangGe,JuntaoTan,andYongfengZhang.2023.Genrec:Largelanguagemodel
forgenerativerecommendation.arXive-prints(2023),arXiv–2307.
[17] Wang-ChengKang,JianmoNi,NikhilMehta,MaheswaranSathiamoorthy,LichanHong,EdChi,andDerekZhiyuanCheng.2023.Do
LLMsUnderstandUserPreferences?EvaluatingLLMsOnUserRatingPrediction.arXivpreprintarXiv:2305.06474(2023).
[18] DiederikP.KingmaandJimmyBa.2015. Adam:AMethodforStochasticOptimization.In3rdInternationalConferenceonLearning
Representations,ICLR2015,SanDiego,CA,USA,May7-9,2015,ConferenceTrackProceedings,YoshuaBengioandYannLeCun(Eds.).
http://arxiv.org/abs/1412.6980
[19] JingLi,PengjieRen,ZhuminChen,ZhaochunRen,TaoLian,andJunMa.2017.Neuralattentivesession-basedrecommendation.In
Proceedingsofthe2017ACMonConferenceonInformationandKnowledgeManagement.1419–1428.
[20] XinhangLi,ChongChen,XiangyuZhao,YongZhang,andChunxiaoXing.2023. E4SRec:AnElegantE￿ectiveE￿cientExtensible
SolutionofLargeLanguageModelsforSequentialRecommendation.arXivpreprintarXiv:2312.02443(2023).
[21] XinyuLin,WenjieWang,YongqiLi,FuliFeng,See-KiongNg,andTat-SengChua.2023.Amulti-facetparadigmtobridgelargelanguage
modelandrecommendation.arXivpreprintarXiv:2310.06491(2023).
[22] JunlingLiu,ChaoLiu,RenjieLv,KangZhou,andYanZhang.2023.Ischatgptagoodrecommender?apreliminarystudy.arXivpreprint
arXiv:2304.10149(2023).
[23] QiaoLiu,YifuZeng,RefuoeMokhosi,andHaibinZhang.2018.STAMP:Short-termattention/memoryprioritymodelforsession-based
recommendation.InProceedingsofthe24thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining.1831–1839.
[24] IlyaLoshchilovandFrankHutter.2019.DecoupledWeightDecayRegularization.In7thInternationalConferenceonLearningRepresenta-
tions,ICLR2019,NewOrleans,LA,USA,May6-9,2019.OpenReview.net. https://openreview.net/forum?id=Bkg6RiCqY7
[25] ChenMa,PengKang,andXueLiu.2019.Hierarchicalgatingnetworksforsequentialrecommendation.InProceedingsofthe25thACM
SIGKDDinternationalconferenceonknowledgediscovery&datamining.825–833.
[26] ChenMa,LihengMa,YingxueZhang,JianingSun,XueLiu,andMarkCoates.2020.Memoryaugmentedgraphneuralnetworksfor
sequentialrecommendation.InProceedingsoftheAAAIConferenceonArti￿cialIntelligence,Vol.34.5045–5052.
[27] RuihongQiu,JingjingLi,ZiHuang,andHongzhiYin.2019.Rethinkingtheitemorderinsession-basedrecommendationwithgraph
neuralnetworks.InProceedingsofthe28thACMInternationalConferenceonInformationandKnowledgeManagement.579–588.
[28] Ste￿enRendle,ChristophFreudenthaler,andLarsSchmidt-Thieme.2010. Factorizingpersonalizedmarkovchainsfornext-basket
recommendation.InProceedingsofthe19thinternationalconferenceonWorldwideweb.811–820.
[29] FeiSun,JunLiu,JianWu,ChanghuaPei,XiaoLin,WenwuOu,andPengJiang.2019.BERT4Rec:SequentialRecommendationwith
BidirectionalEncoderRepresentationsfromTransformer.InProceedingsofthe28thACMInternationalConferenceonInformation
andKnowledgeManagement(Beijing,China)(CIKM’19).AssociationforComputingMachinery,NewYork,NY,USA,1441–1450.
https://doi.org/10.1145/3357384.3357895
[30] JiaxiTangandKeWang.2018.PersonalizedTop-NSequentialRecommendationviaConvolutionalSequenceEmbedding.InProceedings
oftheEleventhACMInternationalConferenceonWebSearchandDataMining(MarinaDelRey,CA,USA)(WSDM’18).Associationfor
ComputingMachinery,NewYork,NY,USA,565–573. https://doi.org/10.1145/3159652.3159656
[31] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,SoumyaBatra,Prajjwal
Bhargava,ShrutiBhosale,etal.2023.Llama2:Openfoundationand￿ne-tunedchatmodels.arXivpreprintarXiv:2307.09288(2023).
[32] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,LukaszKaiser,andIlliaPolosukhin.2017.
Attentionisallyouneed.arXivpreprintarXiv:1706.03762(2017).
[33] LingxiaoWang,JingHuang,KevinHuang,ZiniuHu,GuangtaoWang,andQuanquanGu.2020.ImprovingNeuralLanguageGeneration
withSpectrumControl.In8thInternationalConferenceonLearningRepresentations,ICLR2020,AddisAbaba,Ethiopia,April26-30,2020.
OpenReview.net. https://openreview.net/forum?id=ByxY8CNtvr
[34] LeiWangandEe-PengLim.2023. Zero-ShotNext-ItemRecommendationusingLargePretrainedLanguageModels. arXivpreprint
arXiv:2304.03153(2023).
[35] LeWu,PeijieSun,YanjieFu,RichangHong,XitingWang,andMengWang.2019. Aneuralin￿uencedi￿usionmodelforsocial
recommendation.InProceedingsofthe42ndinternationalACMSIGIRconferenceonresearchanddevelopmentininformationretrieval.
235–244.
[36] ShuWu,YuyuanTang,YanqiaoZhu,LiangWang,XingXie,andTieniuTan.2019.Session-basedrecommendationwithgraphneural
networks.InProceedingsoftheAAAIConferenceonArti￿cialIntelligence,Vol.33.346–353.
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.IntegratingLargeLanguageModelswithGraphicalSession-BasedRecommendation • xxx:23
[37] ShuWu,MengqiZhang,XinJiang,XuKe,andLiangWang.2019.Personalizinggraphneuralnetworkswithattentionmechanismfor
session-basedrecommendation.arXivpreprintarXiv:1910.08887(2019).
[38] YueqiXie,PeilinZhou,andSunghunKim.2022.Decoupledsideinformationfusionforsequentialrecommendation.InProceedingsof
the45thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval.1611–1621.
[39] ChengfengXu,PengpengZhao,YanchiLiu,VictorSSheng,JiajieXu,FuzhenZhuang,JunhuaFang,andXiaofangZhou.2019.Graph
contextualizedself-attentionnetworkforsession-basedrecommendation..InIJCAI,Vol.19.3940–3946.
[40] JunjieZhang,RuobingXie,YupengHou,WayneXinZhao,LeyuLin,andJi-RongWen.2023.Recommendationasinstructionfollowing:
Alargelanguagemodelempoweredrecommendationapproach.arXivpreprintarXiv:2305.07001(2023).
[41] TingtingZhang,PengpengZhao,YanchiLiu,VictorSSheng,JiajieXu,DeqingWang,GuanfengLiu,andXiaofangZhou.2019.
Feature-leveldeeperself-attentionnetworkforsequentialrecommendation..InIJCAI.4320–4326.
[42] YangZhang,FuliFeng,JizhiZhang,KeqinBao,QifanWang,andXiangnanHe.2023.Collm:Integratingcollaborativeembeddingsinto
largelanguagemodelsforrecommendation.arXivpreprintarXiv:2310.19488(2023).
[43] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,YingqianMin,BeichenZhang,JunjieZhang,ZicanDong,
YifanDu,ChenYang,YushuoChen,ZhipengChen,JinhaoJiang,RuiyangRen,YifanLi,XinyuTang,ZikangLiu,PeiyuLiu,Jian-YunNie,
andJi-RongWen.2023.ASurveyofLargeLanguageModels.CoRRabs/2303.18223(2023). https://doi.org/10.48550/ARXIV.2303.18223
arXiv:2303.18223
[44] LinZheng,NaichengGuo,JinYu,andDazhiJiang.2020. Memoryreorganization:Asymmetricmemorynetworkforreorganizing
neighborsandtopicstocompleteratingprediction.IEEEAccess8(2020),81876–81886.
Proc.ACMMeas.Anal.Comput.Syst.,Vol.xx,No.x,Articlexxx.Publicationdate:xxxxx.