STABLE TRAINING OF NORMALIZING FLOWS FOR
HIGH-DIMENSIONAL VARIATIONAL INFERENCE
DANIEL ANDRADE
School of Informatics and Data Science, Hiroshima University
Abstract. Variational inference with normalizing flows (NFs) is an increas-
ingly popular alternative to MCMC methods. In particular, NFs based on
coupling layers (Real NVPs) are frequently used due to their good empirical
performance. Intheory,increasingthedepthofnormalizingflowsshouldlead
tomoreaccurateposteriorapproximations. However,inpractice,trainingdeep
normalizing flows for approximating high-dimensional posterior distributions
isofteninfeasibleduetothehighvarianceofthestochasticgradients. Inthis
work,weshowthatpreviousmethodsforstabilizingthevarianceofstochastic
gradient descent can be insufficient to achieve stable training of Real NVPs.
Asthesourceoftheproblem,weidentifythat,duringtraining,samplesoften
exhibit unusual high values. As a remedy, we propose a combination of two
methods: (1) soft-thresholdingofthescaleinRealNVPs, and(2)abijective
softlogtransformationofthesamples. Weevaluatetheseandotherpreviously
proposedmodificationonseveralchallengingtargetdistributions,includinga
high-dimensional horseshoe logistic regression model. Our experiments show
thatwithourmodifications,stabletrainingofRealNVPsforposteriorswith
several thousand dimensions is possible, allowing for more accurate marginal
likelihoodestimationviaimportancesampling. Moreover,weevaluateseveral
common training techniques and architecture choices and provide practical
advisefortrainingNFsforhigh-dimensionalvariationalinference.
1. Introduction
During the past decade, variational inference (VI) has been gaining increased
attention also in the statistics community [Blei et al., 2017]. Thanks to its com-
putational efficiency, VI can provide useful approximations to high-dimensional
posterior distributions that are computationally too expensive for MCMC meth-
ods. However, simple variational distributions, like the Gaussian distribution, can
lead to arbitrarily bad approximations [Zhang et al., 2018].
As a promising method for increasing the expressibility of the VI approxima-
tion, normalizing flows (NFs) have been proposed [Papamakarios et al., 2021]. In
particular, normalizing flows based on coupling layers (Real NVP) are known to
have good performance for variational inference [Dhaka et al., 2021, Vaitl et al.,
2022]. Real NVPs also have the advantage that both sampling and density estima-
tion is computationally efficient, making them the optimal choice for importance
sampling. Unfortunately,forhighdimensionalposteriordistributions,VIwithNFs
is known to suffer from unstable training, and therefore most previous works limit
E-mail address: andrade@hiroshima-u.ac.jp.
1
4202
beF
62
]LM.tats[
1v80461.2042:viXra2 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
their application to posteriors with less than hundred dimensions [Dhaka et al.,
2021, Vaitl et al., 2022, Liang et al., 2022].
As one source of the unstable training, we identify that Real NVPs with in-
creasing depth are prone to produce exponentially large sample values. This poses
a dilemma, since an increase in depth is also known to increase the expressibility
of the resulting posterior approximation [Koehler et al., 2021]. As a remedy, we
propose to (1) soft-threshold the scaling in Real NVPs, and (2) to pass all samples
through a bijective soft log transformation (named LOFT).1 We demonstrate that
withthesemodifications,theNFs’samplesthatareusedforgradientestimationcan
exhibit considerably lower variance, which consequently ensures good convergence
evenforhigh-dimensionaltargetdistributionswithfattails. Inparticular,forlogis-
tic regression with the horseshoe prior and more than 4000 dimensions we achieve
significantly sharper estimates of the marginal likelihood than previous NFs and
sequential Monte Carlo (SMC) [Dai et al., 2022]. Orthogonal to this work, we note
thatnormalizingflowsandSMCcanbeeffectivelycombinedtofurtherimprovethe
marginal likelihood estimate [Arbel et al., 2021, Matthews et al., 2022].
Furthermore, in this article, we also scrutinize and evaluate several other subtle
modifications in the architecture and training of Real NVPs, like path gradients
[Roeder et al., 2017] and annealing [Rezende and Mohamed, 2015]. Our analysis
shows that some of these modifications can have a significant impact on approx-
imation accuracy. We also empirically verify that for training NFs, the evidence
lower bound (ELBO) is well suited as training objective due to its high correlation
with the accuracy of the marginal likelihood estimate.
Behrmannetal.[2021]provethatRealNVPsarenotgloballyLipschitzcontinu-
ous,whichcanleadtonumericalinstabilityoftheinverse. Asaremedy,Behrmann
etal.[2021],Ardizzoneetal.[2019]proposetorestricttheoutputoutofthescalings.
However, their focus is on the inverse, in particular on the reconstruction of image
data. As we show in Section 7, their proposed restrictions can be sub-optimal for
training in variational inference. On the other hand, the works in [Salmona et al.,
2022, Hagemann and Neumayer, 2021] show that multi-modal target distributions
can lead to exploding Lipschitz constants, which can be mitigated by changing
the base distribution to a multivariate normal mixture model. However, training
withamorecomplexbasedistributioniscomputationallymoredemanding,andits
effectiveness is apparently highly dependent on how close the more complex base
distribution matches with the target distribution. The works in [Jaini et al., 2020,
Liang et al., 2022] show that a heavy-tailed target distribution cannot be modeled
with Real NVPs, unless the base distribution is also changed to a heavy tailed
distribution. In general, training with a heavy-tailed base distribution is more
challenging due to the increase in variance of the gradient estimates. We show
in our experiments, that our proposed modifications enable stable training with
a student-t distribution, leading to considerably better estimates of the marginal
likelihood.
Another line of work suggests the use of path gradients [Roeder et al., 2017,
Vaitletal.,2022]whichremovesthescoretermfromthegradientestimationofthe
Kullback-Leibler (KL)-divergence, and can lead to considerably lower variance of
the gradient estimates. Our experiments confirm the usefulness of path gradients
1Preliminaryresultsofourworkwerepresentedin[Andrade,2023].HIGH-DIMENSIONAL VARIATIONAL INFERENCE 3
also for high-dimensional posterior approximation. We note that in order to sta-
bilize training, control variates, as proposed in [Ranganath et al., 2014], could in
principle also be applied to NFs. However, in practice, due to the high number of
parameters of NFs, such control variates are not computationally feasible.
The remainder of this article is structured as follows. In Section 2, we provide
backgrounds on variational inference and Real NVPs. In Section 3, we describe
the problem of exponentially large sample values, followed by Section 4, where we
describe our proposed modifications. Moreover, in Section 5, we describe several
other possible modifications of Real NVPs. In Section 6, we describe all models
(target distributions) and methods that are used for marginal likelihood estima-
tion in our experiments. In Section 7, we report the main results for five different
models with 1000 or more dimensions, followed by Section 8, where we analyze
the impact of different modifications on training and marginal likelihood estima-
tion, and also compare to Hamiltonian Monte Carlo (HMC). Finally, in Section 9,
we summarize our results, and provide recommendations for the architecture and
training of NFs with VI.The code for reproducing allexperiments is available here
https://github.com/andrade-stats/normalizing-flows.
2. Background on Normalizing Flows and Real NVP
Variational inference with the reverse Kullback-Leibler (KL) divergence mini-
mizes
(cid:104) (cid:16)q (θ)(cid:17)(cid:105)
(1) KL(q ||p )=E log η ,
η ∗ qη p (θ)
∗
whereq denotesthevariationalapproximationparameterizedbythevectorη,and
η
p denotes the target density given by
∗
p(θ,D)
p (θ):= ,
∗ p(D)
where p(θ,D) is the density of the joint distribution of model parameters θ ∈ Rd
and data D, and p(D) is the marginal likelihood. We therefore have
(cid:104) (cid:16) q (θ) (cid:17)(cid:105)
KL(q ||p )=E log η +logp(D).
η ∗ qη p(θ,D)
Normalizing flows approximate the posterior as follows:
z∼q ,
0
f (z)∼q ,
η η
i.e. we first sample z from a base distribution q , and then use f (z) as a sample
0 η
from the approximate posterior q . Assuming that f : Rd → Rd is a smooth
η
bijective function, the density of the variational approximation is given by
q (f (z))=q (z)|det(J (z))|−1,
η η 0 f
where J (z) ∈ Rd×d is the Jacobian (matrix with all partial derivatives) of f
f η
evaluated at point z.
Using the law of the unconscious statistician, we get
(cid:104) (cid:16) q (θ) (cid:17)(cid:105) (cid:104) (cid:16) q (f (z)) (cid:17)(cid:105)
E log η =E log η η
qη p(θ,D) q0 p(f (z),D)
η4 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
which is called the negative ELBO (evidence lower bound) [Blei et al., 2017, Papa-
makarios et al., 2021].
Finally, for minimizing KL(q ||p ) with respect to the variational parameters η,
η ∗
we need to evaluate the gradient
∂ ∂ (cid:104) (cid:16) q (θ) (cid:17)(cid:105)
KL(q ||p )= E log η
∂η η ∗ ∂η qη p(θ,D)
(cid:104) ∂ (cid:16) q (f (z)) (cid:17)(cid:105)
=E log η η .
q0 ∂η p(f (z),D)
η
Using samples z ∼q for k =1,...,b, we can approximate the gradient using
k 0
b
(2)
∂
KL(q ||p )≈
1(cid:88)(cid:104) ∂ log(cid:16) q η(f η(z k)) (cid:17)(cid:105)
.
∂η η ∗ b ∂η p(f (z ),D)
η k
k=1
Alternatively, as originally proposed in [Roeder et al., 2017], the following gra-
dient estimate can have lower variance
(3)
1(cid:88)b (cid:104) ∂ log(cid:16) q η(f η(z j)) (cid:17)
−
∂
logq
(θ)(cid:12)
(cid:12)
(cid:105)
,
b
k=1
∂η p(f η(z j),D) ∂η η (cid:12) θ=fη(zj)
This gradient estimator removes the score term from Equation (2), and is also
referredtoaspathgradients[Vaitletal.,2022]. Themotivationandderivationcan
be found in the Appendix.
2.1. Real NVP. For training we require that f (z),f−1(θ), and det(J (z)) can
η η f
be evaluated efficiently.2 Real NVP (real-valued non-volume preserving) is an ar-
chitecture framework for f that has these required properties [Dinh et al., 2016].
Let S and S be a partition of the index set {1,2,3,...,d}, whereas |S | =
0 1 0
|S |= d.3 Define
1 2
(4) f :=f ◦f ...◦f ,
r r−1 1
where the functions f :Rd →Rd, for 1,2,...r are defined as follows. Let z(i+1) :=
i
f (z(i)), and define4
i
z(i+1) :=z(i),
A A
(5)
z(i+1) :=z(i)⊙exp(s (z(i)))+t (z(i)),
B B i A i A
where
A:=S , and
(i+1)mod2
(6)
B :=S ,
imod2
ands :Rd/2 →Rd/2 andt :Rd/2 →Rd/2 arearbitraryfunctionsthataremodeled
i i
withaneuralnetwork. Notethattheexponentialexpistakencomponent-wise,and
⊙denotestheHadamardproduct. Thevectorηcontainsallneuralnetworkweights
and bias terms. Note that z(1) is sampled from the base distribution q , which is
0
2Notethatwhenusingpathgradientsweneedtoevaluatetheinversefη−1(θ).
3For ease of representation here, we assume that d is even. The requirement that S0 and S1
areofthesamesizeisnotessential. Inourimplementation,weassignallevenindicestoS0 and
alloddindicestoS1.
4ThenotationzA denotestheset{zj|j∈A}.HIGH-DIMENSIONAL VARIATIONAL INFERENCE 5
often assumed to be a normal distribution. The Equations (5) are often referred to
asanaffinecouplinglayer[Dinhetal.,2016,Leeetal.,2021,Ishikawaetal.,2023].
Note that our definition of Real NVP as given in Equations (4), (5) and (6), is
the one often used in practice [Dinh et al., 2016, Huang et al., 2020, Koehler et al.,
2021]. Despite its restrictive form, i.e. no permutation between affine coupling
layers, and only alternation of the sets A and B, as described in Equation (6),
Koehler et al. [2021] proved that f#q is a universal approximator to any target
0
distribution on a compact set U ⊆Rd.5
Though, in theory, r ≥ 3 is sufficient for accurate approximation, in practice,
often a much larger number of coupling layers r is required [Koehler et al., 2021].
3. Exponentially Large Sample Values
For any dimension j, let us denote by u the maximum scaling value of the Real
NVP, i.e.
u:= max
(cid:0)
exp(s
(z(i)))(cid:1)
.
i∈{1,...r} i A j
Let us denote by m the maximum value that is sampled from q or output by any
0
t , i.e.
i
m:= max
{z(1),(cid:0)
t
(z(i))(cid:1)
}.
i∈{1,...r} j i A j
Then we have that
r
z(r+1) ≤m(cid:88) ui.
j
i=0
Inotherwords,themaximumvaluethatz(r+1) canattainisoforderO(mur). That
j
means deep Real NVP, with r being large, are prone to produce samples that are
large in magnitude.
InFigures10(a)and9(a),forr ∈{4,32,64},weshowthe(absolute)maximum
value of z(r+1) for each iteration during training. We observe that sometimes very
j
large values are sampled, leading to an unstable estimate of the ELBO estimate
that can cause high variance of the gradient estimates in Equations (2) and (3).
4. Stabilizing Real-NVP
The analysis from the previous section suggests two strategies for mitigating
large sample values:
• Placing an upper bound on the scaling factors exp(s (z(i))).
i A
• Performing a log-transformation of the samples.
5The notation f#q0 denotes the push-forward of the probability measure q0 by function f.
For universal approximation in distribution, r ≥3 and weak regularity conditions on the target
distributionarenecessary.6 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
Figure 1. Shows the asymmetric soft clamp function c(s) from
Equation (7) with α =2 and α =0.1.
neg pos
Upperandlowerboundsonthescalingfactors. Weproposetothresholdhighscaling
factorstoensurethats (z(i))≤α ,wherethethresholdα issettoafixedsmall
i A pos pos
value. Inordertoensurethestablecalculationoftheinversef−1 itisalsonecessary
to ensure that exp(s (z(i))) from Equation (5) does not get too close to zero. We
i A
therefore place an additional threshold that enforces s (z(i)) ≥ −α . Rather
i A neg
than hard thresholding, we found empirically that the following asymmetric soft
clamping improved training
(cid:40)
2 α arctan(s/α ) if s≥0,
(7) c(s)= pos pos
π α arctan(s/α ) if s<0.
neg neg
with α <α . Note that c(s) is differentiable everywhere. For our experiments
neg pos
we set α = 2 and α = 0.1.6 Note that this is a modification of the soft-
neg pos
clamping proposed in [Ardizzone et al., 2019], with the important difference that
high-values of s are suppressed more forcefully. The resulting function is shown in
Figure 1. With the proposed asymmetric soft clamping, Equation (5) changes to
(8) z(i+1) :=z(i)⊙exp(c(s (z(i))))+t (z(i)).
B B i A i A
Log-transformation of the samples. Since the maximum value of a sample z(r+1) is
of order O(mur), it is natural to consider some type of log-transformation g(z) at
the final layer. That means instead of (4), we use
(9) f :=g◦f ◦f ...◦f .
r r−1 1
Note that the function g must be a differentiable bijective function. In particular,
we propose the following log soft extension (LOFT) layer:

τ +log(z−τ +1) if θ ≥τ,

(10) g(z)= −τ −log(−z−τ +1) if θ ≤−τ,
z
else.
(cid:16) (cid:0) (cid:1) (cid:17)
=sign(z) log max(|z|−τ,0)+1 +min(|z|,τ) .
6Overall good performance in terms of ELBO, though manually tuning for each task might
improveresultsfurther.HIGH-DIMENSIONAL VARIATIONAL INFERENCE 7
Figure 2. Shows the LOFT function g(z) from Equation (10)
with τ =2.
The function is shown in Figure 2: within the range [−τ,τ] the layer performs an
identity mapping, and outside the range, the absolute value of the function grows
only logarithmically; τ is a fixed pre-specified parameter. For a vector z we apply
the LOFT function element-wise. Note that LOFT is a one-to-one function and
g−1(z)=sign(z)(cid:16) exp(cid:0) max(|z|−τ,0)(cid:1) −1+min(|z|,τ)(cid:17)
,
(cid:0) ∂ (cid:17) (cid:0) (cid:1)
log g(z) =−log max(|z|−τ,0)+1 .
∂z
Therefore, all necessary calculations can be expressed using only computationally
efficient elementary operations (without if-clauses). For all of our experiments we
set τ =100.
5. Details on Architecture and Alternative Choices
BaseDistributionandClipping. WhileweproposetousetheclippingfromEquation
(7), the works in [Ardizzone et al., 2019] propose to use symmetric clipping with
the arctan function, whereas [Jaini et al., 2020] suggests to use the tanh function.
For the base distribution q there are two common choices:
0
• independent identical standard Gaussian distribution for each dimension,
i.e.
(cid:81)d
N(0,1).
j=1
•
independentnon-identicalstudent-tdistribution(cid:81)d
StudentT(ν ),where
j=1 j
StudentT(ν ) denotes the standard student-t distribution with ν degrees
j j
of freedom.
An overview of some variations of Real NVP are given in Table 1.
Affine Transformation. All previous works transform the base distribution by an
affine transformation
a(z)=σ⊙z+µ,
with trainable parameters σ ∈ Rd and µ ∈ Rd. The resulting normalizing flow
+
therefore becomes
f :=f ◦f ...◦f ◦a.
r r−1 18 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
Table 1. Overview of previously proposed variations of Real
NVP.
Name Base Distribution Clipping Reference
standard Gaussian none [Dinhetal.,2016]
ATAF StudentT tanh [Jainietal.,2020,Liangetal.,2022]
SymClip Gaussian arctan [Ardizzoneetal.,2019]
WhenusingthemodificationsfromSection4,weperformtheaffinetransformation
at the end, i.e.
f :=a◦g◦f ◦f ...◦f ,
r r−1 1
which has the advantage of re-adjusting the scaling that might have been reduced
by LOFT and the asymmetric soft clamping of s .
i
6. Experiments
In this section, we first describe the different probabilistic models that we use
forevaluation(Section6.1),andthendescribethetrainingofthenormalizingflows
and other MCMC-based methods that are used for comparison (Section 6.2).
6.1. Models. For evaluation, we use four different target distributions, for which
the log marginal likelihood is known (Basic Models), and a challenging Horseshoe
Logistic Regression model with synthetic and real data.
6.1.1. Basic Models.
FunnelDistribution. Thefunneldistribution,asintroducedin[Neal,2003],isgiven
by
p (θ )=N(0,9), ∀j ∈2,...,d:p (θ |θ )=N(0,eθ1).
∗ 1 ∗ j 1
The target distribution is p (θ) with θ :=(θ ,θ ,...,θ )∈Rd.
∗ 1 2 d
Multivariate Student-T Distribution. The multivariate student-t distribution with
mean 0, degrees of freedom ν, and scale matrix Σ is given by
Γ((ν+d)/2) (cid:16) 1 (cid:17)−(ν+d)/2
p (θ)= 1+ θTΣ−1θ ,
∗ Γ(ν/2)(νπ)d/2|Σ|1/2 ν
where we set the scale matrix to Σ =0.8, for i̸=j, and Σ =1.
i,j i,i
Multivariate Gaussian Mixture.
k
(cid:88) 1
p (θ)= N(θ|µ ,I ),
∗ k j d
j=1
where I
d
denotes the identity matrix in Rd×d. Here, we set k =3, and µ
1
= √1 d6,
µ =−√1 6, and µ =0 (i.e. in all dimensions constant values √1 6, −√1 6, and 0,
2 d 3 d d
respectively).HIGH-DIMENSIONAL VARIATIONAL INFERENCE 9
Conjugate Linear Regression. We consider the following Bayesian linear regression
model
σ2 ∼Inv-Gamma(0.5,0.5)
β ∼N(0,σ2I )
d′−1
y i. ∼i.d N(xTβ,σ2) for i∈{1,...n}.
i i
The parameters are θ := (β,σ2), and the target distribution is the posterior
p(θ|y,X). Note that, different from before, θ ∈ Rd′ ⊗R . For the normalizing
+
flows, inordertoensurethepositivenessforσ2, weusethesoftplus-transformation
(as suggested, for example, in Kucukelbir et al. [2017]).
Duetotheconjugacyofthepriors,thelogmarginallikelihoodhasaclosedform
given by Chipman et al. [2001]:
Γ((1+n)/2) (cid:16) (cid:17)−(1+n)/2
logp(y|X)= 1+yTΣ−1y ,
Γ(1/2)πn/2|Σ|1/2
where Σ:=(I −X(XTX+I )−1XT)−1, and X ∈Rn×d′ contains all explanatory
n d
variables.
6.1.2. HorseshoeLogisticRegressionModel. Weconsiderthefollowinghigh-dimensional
logistic regression model with the Horseshoe prior [Carvalho et al., 2010]:
τ ∼C+(0,1)
for j ∈{1,...d′}:
λ ∼C+(0,1)
j
β ∼N(0,τ2λ2)
j j
µ∼C+(0,10)
y i. ∼i.d Bernoulli(σ(xTβ+µ)), for i∈{1,...n},
i i
where C+(0,s) is the half-Cauchy distribution with scale s, and σ is the sigmoid
function. Note that here the total number of parameters d is 2d′ +2. Again, to
ensure the positiveness for τ,λ, and µ, we use the softplus-transformation.
6.2. Description of Variational Inference and MCMC Methods. We com-
pare our proposed modifications of Real NVP (proposed) to three other variations,
namely standard, ATAF and SymClip, as described in Table 1. Furthermore, we
also compare to mean field Gaussian variational inference. We did not compare to
othernormalizingflowslikeresidualflowsorcontinuousflows[Papamakariosetal.,
2021], since, in preliminary experiments, they produced inferior results, or do not
allow for importance sampling to estimate the marginal likelihood.
For estimating the marginal likelihood, we also compare to a sequential monte
carlo(SMC)sampler. SMCisanextensionofannealedimportancesampling[Neal,
2001], but with improved performance for estimating the normalization constant
[Arbel et al., 2021]. Indeed, removing the resampling step in SMC corresponds to
annealed importance sampling [Dai et al., 2022].
For all variational inference (VI) methods (normalizing flows and mean field
Gaussian),weusedAdam[KingmaandBa,2015]withamini-batchofsize256and
path gradients (i.e. b=256, from Equation (3)) and a fixed learning rate of 10−4.
The number of training iterations is set to 60,000, except for the gene expression10 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
data experiment where we used 400,000 iterations. For SMC, we used a geometric
annealing scheme with 100,000 (intermediate) temperatures.
We also compared to an Hamiltonian Monte Carlo (HMC) sampler [Neal et al.,
2011] with NUTS [Hoffman et al., 2014] in terms of Wasserstein distance to the
true target distribution.
EstimationofELBOandMarginalLikelihood. LikemeanfieldVI,RealNVPshave
thebigadvantagethatsamplingandestimationofthedensityq isfast. Thisallows
η
us to use importance sampling (IS) to get an estimate of the marginal likelihood:
(cid:104)p(θ,D)(cid:105)
p(D)=E
qη q (θ)
η
≈
1 b (cid:88)eval p(θ k,D)
,
b q (θ )
eval η k
k=1
where θ ∼ q for k = 1,...,b . For the final evaluation of ELBO and the IS
k η eval
estimate, we use 20,000 samples (i.e. b = 20,000), and repeat each evaluation
eval
20 times to estimate the Monte Carlo error [Koehler et al., 2009].
7. Results
We evaluated all methods on the models described in 6.1 for different number
of dimensions d ranging from d = 10 to d = 4002. For d < 300, most methods
performed similarly, therefore, we focus here on the results for d ≥ 1000. The
results for d < 300 can be found in the Appendix in Tables 7, 8, and 11. We
show the results for four different variations of our proposed method using either a
student-t or Gaussian base distribution, and using either LOFT or not.
7.1. Basic Models. Figures 3 and 4 show the results for the four basic models,
where the true marginal likelihood is known. We see that all methods tend to un-
derestimatethetruemarginallikelihood. Inmostcases, theresultsofthemarginal
likelihood estimates based on SMC and the mean field Gaussian approximation
(for which average and standard deviation are shown on top of each graph) were
considerably worse than the estimates based on normalizing flows. For example,
for Funnel, the mean field Gaussian approximation and SMC had an error of more
than3,whereastheerrorforthenormalizingflowswasaround0.05orless. Overall
all normalizing flows performed similarly, except for the challenging multivariate
student-t model (Figure 3, (b)), where the proposed method performed slightly
better than other normalizing flows.
A higher ELBO tends to improve the estimate of the marginal likelihood. In
deed, as we show in Figure 5, optimizing the ELBO is strongly correlated with
betterestimatesofthemarginallikelihoodthatarefoundviaimportancesampling.
We note that, in theory, training normalizing flows with the forward KL [Jerfel
et al., 2021] should lead to better estimates of the marginal likelihood via impor-
tance sampling. However, in practice, we found that the ELBO, of the reverse KL,
is much more stable in training and, as suggested by Figure 5, highly correlated to
better estimates of the marginal likelihood.
7.2. Horseshoe Logistic Regression Model. Next, in Figure 6, we show the
resultsfortheHorseshoelogisticregressionmodelonsyntheticdatawithd=2002.
Finally, wealsoevaluateallmethodsonageneexpressiondatasetwith2000genes
from 62 tissue samples either belonging to tumor or normal colon tissue [AlonHIGH-DIMENSIONAL VARIATIONAL INFERENCE 11
(a) Funnel (b) Multivariate Student-T
Figure 3. Shows the ELBO and the log marginal likelihood esti-
mate using importance sampling (d = 1000). Red line shows true
log marginal likelihood.
etal.,1999];theestimatedmarginallikelihoodsoftheHorseshoeLogisticRegression
model (d = 4002) are shown in Figure 7. As before, mean field Gaussian VI and
SMC fail due to the high dimensionality. For lower dimensions, as shown in the
Appendix in Table 11, both methods perform better. Among all normalizing flows
the proposed method with a student-t as base distribution and LOFT, performs
best. Compared to ATAF, for the synthetics data, the proposed method improves
ELBO by around 30% and for the colon tissue data by around 20%, resulting in
considerably higher estimates of the marginal likelihood.
8. Analysis
Convergence and Large Sample Values. First, we take a closer look at the conver-
gencebehavioroftheproposedmethodfortheHorseshoelogisticregressionmodel.
Figure 8 confirms that the proposed method achieves faster convergence to a lower
optimum of the loss (= negative ELBO) than other normalizing flow variations.
Next, in Figures 9 and 10 we empirically confirm the observation from Section 3
that normalizing flows based on coupling layers tend to produce larger values with
increasing depth. However, comparing the left (a) and right-hand side (b) of the
Figures 9 and 10 we find that the proposed method successfully restrains large
sample values at the last layer.
ImportantDetailsofTrainingNormalizingFlows. Forthetrainingofallnormalizing
flows, we use double precision (opposite to training of ordinary neural networks,
where single precision is often considered sufficient). Furthermore, for training of12 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
(a) Multivariate Normal Mixture (b) Conjugate Linear Regression
Figure 4. Shows the ELBO and the log marginal likelihood es-
timate using importance sampling (d = 1000, for the Conjugate
Linear Regression model d=1001). Red line shows true log mar-
ginal likelihood.
all normalizing flows we used path gradients (as proposed in [Roeder et al., 2017,
Vaitl et al., 2022]), and do not employ annealing (opposite to the suggestion in
[Rezende and Mohamed, 2015]). Our analysis in the appendix shows that these
choices are crucial for stable training (see Tables 5 and 4).7
The works in [Dhaka et al., 2020] show that detecting convergence can be tricky
even for variational inference with a Gaussian approximation family. Indeed, we
found that training deep normalizing flows with the ELBO is similar to the mini-
batch training of ordinary deep learning: detection of convergence is difficult since
after a period of increase, the loss (negative ELBO) sometimes can drop again. As
a consequence, given a fixed budget of M iterations, we compared: (1) taking the
model after M iterations, and (2) taking the model with the lowest loss during the
iterations M/2 and M iterations. Note that at each iteration the loss is evaluate
usingonly256samples,whereasforthefinalevaluationtheELBOisestimatedwith
20,000 samples. The results in the Appendix (in Table 6) show that the ELBO of
the standard Real NVP can be considerably improved by taking the model with
the lowest loss, while for the proposed method the differences are negligible. Note
that in favor for the standard model, we used for all of our experiments the lowest
loss model.
7Moreoverwedonotusegradientclippingandl2-regularizationastheseshadonlydeteriorate
effects.HIGH-DIMENSIONAL VARIATIONAL INFERENCE 13
Figure 5. Plot of ELBO vs error in marginal likelihood of 13
differentvariationalinferencemethods/modelsforeachmodel;also
shows95%confidencelevelofpearsoncorrelationρestimatedwith
bootstrapping.
Importance of Depth of Normalizing Flows. We also evaluated the influence of the
number of coupling layers r, i.e. the depth of the normalizing flow. We compared
the ELBO and log marginal likelihoods results for r = 64 and r = 16, and found
thatfortheproposedmethodanincreaseofr consistentlyimprovesELBOandthe
logmarginallikelihoodestimate(seeAppendix, Figure11forthehorseshoemodel,
and 9, 7, 10, 8 for all basic models).
Comparison to HMC sampler. In the Appendix (in Table 3), we also compare the
samples from the normalizing flows to the ground truth, and evaluate in terms of
(sliced) Wasserstein distance WD(q ,p ). We find that the quality of the samples
η ∗
of the normalizing flows are comparable to the sample of a long run HMC.8
Details on Runtime. A rough estimate of runtimes for all methods are shown in
Table 2. We measured the runtime using NVIDIA RTX 6000 Ada with 48GB
memory, on a Linux server with 32 cores. For all variational inference methods
we used PyTorch [Paszke et al., 2017] and the normflows package [Stimper et al.,
2023]. All implementations, except the HMC from Numpyro [Phan et al., 2019],
were using the GPU, as such the runtime for HMC cannot be fairly compared to
the other results. All variations of normalizing flows had about the same runtime.
Note that sampling for normalizing flows is negligible (1∼2 minutes) compared to
training. Therefore, for repeated sampling, normalizing flows are computationally
more efficient than SMC and HMC.
8NotethatfortheMultivariateStudent-TmodeltheestimationerroroftheWassersteindis-
tanceisconsiderable,makingitdifficulttostateaclearwinner.14 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
Figure 6. Shows the ELBO and the log marginal likelihood esti-
mate using importance sampling for the Horseshoe logistic regres-
sion models on synthetic data with d=2002.
Table 2. Approximate runtimes (wall-clock) for train-
ing/sampling of all methods (Horseshoe Logistic Regression
model with d=2002, 20 repetitions).
Method Runtime in Minutes
meanfieldgaussian 10
proposed(student,withLOFT) 360
SMC 1510
HMC 33640
9. Conclusions and Recommendations
Our experiments showed that for high dimensional posteriors (more than 1000
dimensions), normalizing flows based on coupling layers provide better estimates
to the marginal likelihood than established methods like SMC. For the trainingHIGH-DIMENSIONAL VARIATIONAL INFERENCE 15
Figure 7. Shows the ELBO and the log marginal likelihood esti-
mate using importance sampling for the Horseshoe logistic regres-
sion models on colon data with d=4002.
objective of all normalizing flows, we used the reverse KL-divergence (ELBO), and
verifiedthatbetterestimatesoftheELBOleadtobetterestimatesofthemarginal
likelihood.
Furthermore, we found, theoretically and empirically, that normalizing flows
basedoncouplinglayerstendtoproducesampleswithhighabsolutevalues,whereas
theproblembecomesmorepronouncedwithincreasingdepth(=numberofcoupling
layers). As a remedy, we proposed a soft clipping of the conditional factors s
i
and a new log transformation layer named LOFT. Notably, these modification
still preserve the bijectiveness of the flow and allow for computationally efficient
calculationofallnecessaryderivatives. Inparticularforhighdimensionalposteriors
with heavy tails we found that our proposed modifications (soft clipping + LOFT)
lead to considerably improved estimates of the ELBO and the marginal likelihood.
Finally, we conducted extensive experiments with different modifications in ar-
chitecture and training, where we found that the following choices lead to good
estimates of ELBO/marginal likelihood:16 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
Figure 8. Showstheprogressoftheloss(=negativeELBO)over
the training iterations for the Horseshoe logistic regression model
with d=2002.
• Astudent-tdistributionasbasedistributionwithtrainabledegree’soffree-
dom for each dimension [Liang et al., 2022].
• A depth of 64 coupling layers (or possibly higher).9
• Gradient estimate without score term [Roeder et al., 2017, Vaitl et al.,
2022].
• No annealing of training objective (opposite to suggestion in [Rezende and
Mohamed, 2015]).
• Tracking and choosing the model with the smallest loss during training
rather than the last model.
Though our focus was on the estimation of the marginal likelihood, our evalua-
tion in terms of Wasserstein distance hints that normalizing flows with the above
modifications provide samples with similar quality as HMC. Therefore, combining
9Forourexperiments,weused64couplinglayers,andmorecouplinglayersarelikelytoachieve
anevenhigherELBO.However,thehighmemoryrequirementsduringback-propagationplacean
upperboundonthenumberofcouplinglayersthatcanbeusedinpractice.HIGH-DIMENSIONAL VARIATIONAL INFERENCE 17
(a) standard (b) proposed (student with LOFT)
Figure9. Showsthemaximumvalueofamini-batchsampleafter
the4-th,32-th,and64-thlayerfortheHorseshoelogisticregression
modelwithd=2002whenusing(a)thestandardnormalizingflow,
and (b) the proposed method.
theabovenormalizingflowswithMCMCas,forexamplediscussedin[Brofosetal.,
2022, Winter et al., 2023], is a promising direction for future work.
Funding
This work was supported by JSPS KAKENHI Grant Number 22K11934.
Appendix
Details of Neural Network. For all normalizing flows, we used for each function s
i
and t of Equation (5), a multi-layer perceptron with one hidden layer of size 100,
i
and ReLU as activation function. All network parameters were initialized with 0,
which means that, before training, output samples equal the distribution of the
base distribution.
Details of Synthetic Data. For the synthetic data, we generated 20 datasets with
dimensions d ∈ {10,100,1000} and n = 100. The regression data was generated
usingthelinearregressionmodeldescribedinSection7.2,Example1of[Tibshirani,
1996]. The logitistc regression data was generated as follows:
y i. ∼i.d Bernoulli(σ(xTβ +µ )), for i∈{1,...n},
i i 0 0
with β ∈ Rd, where the first, second, and fifth dimension of β are set to 3,1.5,
0 0
and 2, respectively; µ := 1, and x are generated from a multivariate normal
0
distribution with mean 0 and covariance matrix C, where C :=0.1|i−j|.
ij18 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
(a) standard (b) proposed (student with LOFT)
Figure 10. Shows the maximum value of a mini-batch sample
after the 4-th, 32-th, and 64-th layer for the Funnel model with
d = 1000 when using (a) the standard normalizing flow, and (b)
the proposed method.
Details of SMC sampler. For comparison, we use a sequential monte carlo (SMC)
sampler that is an extension of annealed importance sampling [Neal, 2001], but
withimprovedperformanceforestimatingthenormalizationconstant[Arbeletal.,
2021]. Indeed, removing the resampling step in SMC corresponds to annealed
importance sampling [Dai et al., 2022].
For SMC, we used the GPU implementation of [Arbel et al., 2021] available
at https://github.com/google-deepmind/annealed_flow_transport. As ini-
tial distribution q , we use a standard normal distribution. To bridge between q
0 0
and the target distribution p , we use a geometric annealing scheme with 100,000
∗
(intermediate) temperatures. The number of particles was set to 2000.10 At each
time step (temperature), one iteration of HMC with NUTS is performed. Resam-
plingateachtimestepisperformed,iftheeffectivesamplesizedropsbelow0.3N .
p
Comparison to HMC sampler. In Table 3, we also evaluate the samples from the
normalizing flows to the ground truth, in terms of (sliced) Wasserstein distance
WD(q ,p ), and compare to samples of a long run HMC. For the HMC we used
η ∗
NUTS [Hoffman et al., 2014] with 200,000×4 chains; discarding first half of each
chain as burn-in and thinning with interval 10.
10For Funnel, due to the long runtime, we used only 10,000 temperatures, and for d=1000,
wehadtoreducethenumberofparticlesto100.HIGH-DIMENSIONAL VARIATIONAL INFERENCE 19
Note that for the Conjugate Linear Regression Model (as described in 6.1.1) the
true posteriors are available as
logp(σ2|y,X)=Inv-Gamma(α ,β ),
post post
logp(β|y,X)=t (µ ,Σ ),
ν post post
whereα = 1+n,β = 1(∥y∥2+1−yTXU−1XTy),U =XTX+I;t denotes
post 2 post 2 2 ν
the multivariate student-t distribution with degrees of freedom ν = 1+n, mean
µ =U−1XTy, and scale matrix Σ = 2βU−1.
post post ν
Background on Path Gradients. Evaluating the gradient at some point η∗, and
using the chain-rule (from matrix calculus), we have
∂ (cid:12) (cid:16) ∂ (cid:17) ∂ ∂ (cid:12)
logq (f (z))(cid:12) = logq (θ) f (z)+ logq (θ)(cid:12)
∂η η η (cid:12) η=η∗ ∂θ η∗ ∂η η ∂η η (cid:12) θ=fη∗(z)
and
∂ ∂
logp(f (z),D)= logp(f (z)|D)
∂η η ∂η η
(cid:16) ∂ (cid:17) ∂
= logp(θ|D) f (z).
∂θ ∂η η
Putting this together, we get
∂ (cid:104) ∂ (cid:16) q (f (z)) (cid:17)(cid:105)
KL(q ||p )=E log η η
∂η η ∗ q0 ∂η p(f (z),D)
η
(cid:104)(cid:16) ∂ (cid:17) ∂ ∂ (cid:12) (cid:16) ∂ (cid:17) ∂ (cid:105)
=E logq (θ) f (z)+ logq (θ)(cid:12) − logp(θ|D) f (z)
q0 ∂θ η∗ ∂η η ∂η η (cid:12) θ=fη∗(z) ∂θ ∂η η
(cid:104)(cid:16) ∂ ∂ (cid:17) ∂ ∂ (cid:12) (cid:105)
=E logq (θ)− logp(θ|D) f (z)+ logq (θ)(cid:12) .
q0 ∂θ η∗ ∂θ ∂η η ∂η η (cid:12) θ=fη∗(z)
Assuming that q (θ)≈p(θ|D), we get
η∗
∂ (cid:104) ∂ (cid:12) (cid:105)
KL(q ||p )≈E logq (θ)(cid:12) ,
∂η η ∗ q0 ∂η η (cid:12) θ=fη∗(z)
whereas for samples z ∼q , k =1,...,b, the estimator is
k 0
1(cid:88)b (cid:104) ∂ (cid:12) (cid:105)
logq (θ)(cid:12) .
b
k=1
∂η η (cid:12) θ=fη∗(zk)
Note that the variance of this estimator is not 0. Therefore, the works in [Roeder
et al., 2017, Vaitl et al., 2022] propose to use instead the estimator
1(cid:88)b (cid:104) ∂ log(cid:16) q η(f η(z k)) (cid:17)
−
∂
logq
(θ)(cid:12)
(cid:12)
(cid:105)
,
b
k=1
∂η p(f η(z k),D) ∂η η (cid:12) θ=fη∗(zk)
which has variance 0 when q (θ)=p(θ|D).
η∗20 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
Table 3. Evaluation of all methods in terms of Wasserstein dis-
tance to true distribution (standard deviation in brackets) for dif-
ferentdimensionsd. ForestimationtheslicedWassersteindistance
is used; ”nan” means that a numerically stable estimate could not
be found; number of flows r =64.
Funnel
Method d=10 d=100 d=1000
meanfieldgaussian 7.96075(0.5571) 9.16279(1.95161) 8.81864(0.76501)
standard 3.55096(0.52902) 5.49619(0.75929) 7.05587(1.12632)
ATAF 6.16254(1.27847) 5.43344(1.35165) 7.08017(0.71492)
SymClip 5.464(0.79736) 6.39209(0.76325) 7.35277(0.88327)
proposed(gaussian,withLOFT) 5.26478(1.5853) 7.9078(5.04944) 7.17303(0.69758)
proposed(student,withLOFT) 5.10961(0.97158) 6.91015(1.97344) 7.28766(0.68416)
proposed(gaussian,noLOFT) 4.85281(1.25448) 6.04271(0.88001) 7.15134(0.75477)
proposed(student,noLOFT) 5.30764(1.43497) 6.48703(1.31521) 7.50812(1.26615)
HMC 3.80284(1.06943) 3.07891(1.14817) 6.23767(2.1133)
MultivariateStudent-T
Method d=10 d=100 d=1000
meanfieldgaussian 978.22757(2077.58198) 947.55754(1981.72515) 339.85119(319.99747)
standard 413.43534(377.51084) 643.26898(950.03763) 491.19396(410.11727)
ATAF 813.73265(1054.58138) 434.39064(487.75902) nan(nan)
SymClip 908.83123(1339.8407) 813.47494(1260.90776) 1675.82688(3798.4308)
proposed(gaussian,withLOFT) 421.94993(448.44594) 431.86027(502.31684) 525.45636(816.70971)
proposed(student,withLOFT) 1087.58516(2393.59627) 465.30794(626.16168) 581.85254(789.12662)
proposed(gaussian,noLOFT) 424.74731(537.22081) 368.34586(307.89202) 1073.10593(3099.12445)
proposed(student,noLOFT) 661.30146(1076.37543) 235.95782(153.88255) 630.76724(1291.59487)
HMC 1319.2934(1812.88423) 840.564(622.86178) 788.89202(1167.01651)
MultivariateGaussianMixture
Method d=10 d=100 d=1000
meanfieldgaussian 1.00168(0.00169) 0.16096(0.00062) 0.02499(0.00015)
standard 0.18057(0.00635) 0.02511(0.00172) 0.01405(0.00015)
ATAF 0.12186(0.00771) 0.04374(0.00215) 0.02392(0.00075)
SymClip 0.14645(0.00652) 0.04968(0.00214) 0.01622(0.00047)
proposed(gaussian,withLOFT) 0.03911(0.00337) 0.01673(0.00054) 0.02191(0.00065)
proposed(student,withLOFT) 0.20198(0.00806) 0.01652(0.00044) 0.01801(0.00054)
proposed(gaussian,noLOFT) 0.03906(0.00351) 0.01654(0.00033) 0.02195(0.00064)
proposed(student,noLOFT) 0.147(0.00912) 0.05994(0.00206) 0.01686(0.00048)
HMC 0.0257(0.00654) 0.01947(0.00155) 0.01597(0.00049)
ConjugateLinearRegression
Method d=11 d=101 d=1001
meanfieldgaussian 0.15449(0.00047) 0.31762(0.00033) 0.12625(0.00006)
standard 0.00787(0.00045) 0.00693(0.00024) 0.00972(0.00008)
ATAF 0.00648(0.00033) 0.00712(0.00025) 0.00545(0.00005)
SymClip 0.00603(0.00027) 0.00679(0.00015) 0.00591(0.00005)
proposed(gaussian,withLOFT) 0.00723(0.00042) 0.00658(0.00024) 0.00635(0.00004)
proposed(student,withLOFT) 0.00646(0.00034) 0.00686(0.0002) 0.00562(0.00004)
proposed(gaussian,noLOFT) 0.00694(0.00034) 0.00664(0.00021) 0.00636(0.00003)
proposed(student,noLOFT) 0.0066(0.0004) 0.0069(0.00022) 0.00561(0.00004)
HMC 0.00521(0.00036) 0.00595(0.00025) 0.00259(0.00002)HIGH-DIMENSIONAL VARIATIONAL INFERENCE 21
Table 4. Comparison of ELBO results with and without anneal-
ing for d = 1000 (for the Conjugate Linear Regression model
d = 1001). ”nan” means that a numerically stable estimate could
not be found
Funnel
Method withoutannealing withannealing
meanfieldgaussian -4.20619(0.00786) -4.25079(0.00797)
standard -0.13076(0.00262) -0.71427(0.00653)
ATAF -0.19676(0.00382) -11.87143(0.04698)
SymClip -0.19903(0.0034) -0.26312(0.00427)
proposed(gaussian,withLOFT) -0.16222(0.00316) -0.15195(0.00414)
proposed(student,withLOFT) -0.16584(0.0034) -0.20455(0.00309)
MultivariateStudent-T
Method withoutannealing withannealing
meanfieldgaussian -7.35684(0.00516) -7.56552(0.00736)
standard -1.07369(0.06991) nan(nan)
ATAF -1.51959(0.44861) nan(nan)
SymClip -0.33732(0.0055) -0.36437(0.00487)
proposed(gaussian,withLOFT) -0.23245(0.00392) -0.39716(0.00437)
proposed(student,withLOFT) -0.29187(0.00531) -0.52886(0.00652)
MultivariateGaussianMixture
Method withoutannealing withannealing
meanfieldgaussian -1.09121(0.00081) -1.091(0.00089)
standard -0.14578(0.0044) -1.16493(0.00271)
ATAF -0.19254(0.00561) -1.35144(0.01046)
SymClip -0.17354(0.00437) -0.1642(0.00308)
proposed(gaussian,withLOFT) -0.15943(0.00418) -0.1924(0.00507)
proposed(student,withLOFT) -0.20653(0.00458) -0.18603(0.00446)
ConjugateLinearRegression
Method withoutannealing withannealing
meanfieldgaussian -2348.47948(0.61318) -2339.86726(0.45202)
standard -327.26313(0.0318) nan(nan)
ATAF -328.46092(0.0203) nan(nan)
SymClip -327.61051(0.02208) -27497.59608(25.57452)
proposed(gaussian,withLOFT) -330.37689(0.02949) -334.25809(0.03358)
proposed(student,withLOFT) -330.41806(0.0309) -333.41908(0.04207)22 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
Table 5. Comparison of ELBO results with and without score
term for d = 1000 (for the Conjugate Linear Regression model
d=1001).
Funnel
Method withoutscoreterm withscoreterm
meanfieldgaussian -4.20619(0.00786) -4.21122(0.00646)
standard -0.13076(0.00262) -1.23624(0.00825)
ATAF -0.19676(0.00382) -1.34531(0.00832)
SymClip -0.19903(0.0034) -0.84768(0.00697)
proposed(gaussian,withLOFT) -0.16222(0.00316) -0.82072(0.0075)
proposed(student,withLOFT) -0.16584(0.0034) -0.93281(0.00817)
MultivariateStudent-T
Method withoutscoreterm withscoreterm
meanfieldgaussian -7.35684(0.00516) -7.63274(0.00774)
standard -1.07369(0.06991) -3.67112(0.0079)
ATAF -1.51959(0.44861) -3.71659(0.01131)
SymClip -0.33732(0.0055) -3.59259(0.01113)
proposed(gaussian,withLOFT) -0.23245(0.00392) -3.55175(0.00922)
proposed(student,withLOFT) -0.29187(0.00531) -3.59585(0.00792)
MultivariateGaussianMixture
Method withoutscoreterm withscoreterm
meanfieldgaussian -1.09121(0.00081) -1.09444(0.00111)
standard -0.14578(0.0044) -1.22924(0.00413)
ATAF -0.19254(0.00561) -1.27836(0.00491)
SymClip -0.17354(0.00437) -1.20088(0.00392)
proposed(gaussian,withLOFT) -0.15943(0.00418) -1.20163(0.00347)
proposed(student,withLOFT) -0.20653(0.00458) -1.24067(0.0039)
ConjugateLinearRegression
Method withoutscoreterm withscoreterm
meanfieldgaussian -2348.47948(0.61318) -2290.86055(0.53819)
standard -327.26313(0.0318) -336.70439(0.04761)
ATAF -328.46092(0.0203) -335.94146(0.03875)
SymClip -327.61051(0.02208) -335.4258(0.03106)
proposed(gaussian,withLOFT) -330.37689(0.02949) -336.43568(0.03155)
proposed(student,withLOFT) -330.41806(0.0309) -336.42227(0.03385)HIGH-DIMENSIONAL VARIATIONAL INFERENCE 23
Table 6. Comparison of ELBO results using either the model
withthelowestlossduringtraining,orthelastmodelford=1000
(for the Conjugate Linear Regression model d=1001).
Funnel
Method lowestlossmodel lastmodel
meanfieldgaussian -4.20619(0.00786) -4.20447(0.00711)
standard -0.13076(0.00262) -0.17381(0.00354)
ATAF -0.19676(0.00382) -0.2487(0.00584)
SymClip -0.19903(0.0034) -0.17057(0.004)
proposed(gaussian,withLOFT) -0.16222(0.00316) -0.17952(0.00274)
proposed(student,withLOFT) -0.16584(0.0034) -0.15643(0.00314)
MultivariateStudent-T
Method lowestlossmodel lastmodel
meanfieldgaussian -7.35684(0.00516) -7.32763(0.0068)
standard -1.07369(0.06991) -1.40608(0.03054)
ATAF -1.51959(0.44861) -5.34154(0.02333)
SymClip -0.33732(0.0055) -0.29835(0.00542)
proposed(gaussian,withLOFT) -0.23245(0.00392) -0.25133(0.00541)
proposed(student,withLOFT) -0.29187(0.00531) -0.27859(0.0046)
MultivariateGaussianMixture
Method lowestlossmodel lastmodel
meanfieldgaussian -1.09121(0.00081) -1.09118(0.00091)
standard -0.14578(0.0044) -0.23477(0.00475)
ATAF -0.19254(0.00561) -0.18509(0.00486)
SymClip -0.17354(0.00437) -0.15595(0.00344)
proposed(gaussian,withLOFT) -0.15943(0.00418) -0.1871(0.00403)
proposed(student,withLOFT) -0.20653(0.00458) -0.19927(0.00413)
ConjugateLinearRegression
Method lowestlossmodel lastmodel
meanfieldgaussian -2348.47948(0.61318) -2347.61854(0.47442)
standard -327.26313(0.0318) -329.39336(0.02498)
ATAF -328.46092(0.0203) -332.94404(0.03647)
SymClip -327.61051(0.02208) -331.89927(0.04517)
proposed(gaussian,withLOFT) -330.37689(0.02949) -331.51712(0.03626)
proposed(student,withLOFT) -330.41806(0.0309) -331.7316(0.03451)24 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
Table 7. Evaluation of all methods in terms of ELBO (standard
deviation in brackets) for d ∈ {10,100,1000}; number of flows
r =64.
Funnel
Method d=10 d=100 d=1000
meanfieldgaussian -1.86318(0.00707) -3.0504(0.00729) -4.20619(0.00786)
standard -0.01344(0.00117) -0.0566(0.00193) -0.13076(0.00262)
ATAF -0.05574(0.00697) -0.05184(0.00165) -0.19676(0.00382)
SymClip -0.02905(0.00114) -0.07004(0.00131) -0.19903(0.0034)
proposed(gaussian,withLOFT) -0.00896(0.00066) -0.04321(0.00187) -0.16222(0.00316)
proposed(student,withLOFT) -0.01244(0.00072) -0.0404(0.00153) -0.16584(0.0034)
proposed(gaussian,noLOFT) -0.01111(0.00084) -0.04291(0.00156) -0.16612(0.0029)
proposed(student,noLOFT) -0.01318(0.00082) -0.0402(0.00154) -0.16584(0.0034)
MultivariateStudent-T
Method d=10 d=100 d=1000
meanfieldgaussian -2.16601(0.00651) -4.5299(0.00767) -7.35684(0.00516)
standard -0.02437(0.00138) -0.08749(0.00483) -1.07369(0.06991)
ATAF -0.03101(0.00173) -0.10369(0.0026) -1.51959(0.44861)
SymClip -0.01674(0.00075) -0.05244(0.00131) -0.33732(0.0055)
proposed(gaussian,withLOFT) -0.01722(0.00052) -0.04715(0.00146) -0.23245(0.00392)
proposed(student,withLOFT) -0.01815(0.00102) -0.06186(0.00189) -0.29187(0.00531)
proposed(gaussian,noLOFT) -0.0164(0.00061) -0.05737(0.00199) -0.2555(0.00421)
proposed(student,noLOFT) -0.0158(0.00077) -0.05686(0.00217) -0.27904(0.00403)
MultivariateGaussianMixture
Method d=10 d=100 d=1000
meanfieldgaussian -1.09054(0.00072) -1.09085(0.00086) -1.09121(0.00081)
standard -0.01622(0.00128) -0.04753(0.00198) -0.14578(0.0044)
ATAF -0.01508(0.00118) -0.08112(0.00245) -0.19254(0.00561)
SymClip -0.01116(0.00095) -0.05408(0.0024) -0.17354(0.00437)
proposed(gaussian,withLOFT) -0.00966(0.00098) -0.06171(0.0025) -0.15943(0.00418)
proposed(student,withLOFT) -0.01534(0.00108) -0.06713(0.00258) -0.20653(0.00458)
proposed(gaussian,noLOFT) -0.00966(0.00098) -0.06171(0.0025) -0.15943(0.00418)
proposed(student,noLOFT) -0.01138(0.00105) -0.07312(0.00248) -0.19908(0.00292)
ConjugateLinearRegression
Method d=11 d=101 d=1001
meanfieldgaussian -275.82849(0.0274) -399.32425(0.05422) -2348.47948(0.61318)
standard -269.74157(0.00046) -347.86269(0.00181) -327.26313(0.0318)
ATAF -269.74116(0.00032) -347.86356(0.0016) -328.46092(0.0203)
SymClip -269.74059(0.00035) -347.87454(0.00245) -327.61051(0.02208)
proposed(gaussian,withLOFT) -269.74193(0.0005) -347.84601(0.00193) -330.37689(0.02949)
proposed(student,withLOFT) -269.74097(0.00039) -347.8542(0.00208) -330.41806(0.0309)
proposed(gaussian,noLOFT) -269.74193(0.0005) -347.84601(0.00193) -330.37689(0.02949)
proposed(student,noLOFT) -269.74097(0.00039) -347.8542(0.00208) -330.41806(0.0309)HIGH-DIMENSIONAL VARIATIONAL INFERENCE 25
Table 8. Estimated log marginal likelihood (standard deviation
in brackets) for d∈{10,100,1000}; number of flows r =64.
Funnel
Method d=10 d=100 d=1000
truevalue 0.0 0.0 0.0
meanfieldgaussian -0.9051(0.25602) -1.93821(0.67967) -3.11768(0.52003)
standard -0.00068(0.00282) -0.00615(0.00308) -0.02001(0.01194)
ATAF -0.00992(0.01665) 0.00199(0.01963) -0.02826(0.01323)
SymClip -0.00545(0.0103) -0.03279(0.00632) -0.06983(0.01259)
proposed(gaussian,withLOFT) -0.00184(0.00381) -0.01278(0.0038) -0.03778(0.02408)
proposed(student,withLOFT) -0.00267(0.00598) -0.01402(0.00323) -0.04245(0.00766)
proposed(gaussian,noLOFT) -0.00129(0.00292) -0.01385(0.00356) -0.04302(0.01002)
proposed(student,noLOFT) -0.00283(0.00468) -0.01393(0.00334) -0.04245(0.00766)
SMC -0.1429(0.06911) -1.76495(0.36939) -3.50737(0.46121)
MultivariateStudent-T
Method d=10 d=100 d=1000
truevalue 0.0 0.0 0.0
meanfieldgaussian -0.92496(0.16171) -2.87842(0.3654) -5.89942(0.22141)
standard -0.00642(0.00224) -0.01491(0.00444) -0.19315(0.04714)
ATAF -0.00416(0.01041) -0.0157(0.00605) -0.10957(0.09367)
SymClip -0.01059(0.00149) -0.00923(0.03558) -0.04795(0.01257)
proposed(gaussian,withLOFT) -0.01207(0.00144) -0.01696(0.00277) -0.03073(0.00651)
proposed(student,withLOFT) -0.01127(0.00197) -0.01844(0.00362) -0.0306(0.01334)
proposed(gaussian,noLOFT) -0.01031(0.00111) -0.01754(0.00449) -0.02604(0.01032)
proposed(student,noLOFT) -0.00979(0.00207) -0.01734(0.00476) -0.03379(0.01077)
SMC -0.01938(0.01633) -0.14798(0.09068) -0.98769(0.39748)
MultivariateGaussianMixture
Method d=10 d=100 d=1000
truevalue 0.0 0.0 0.0
meanfieldgaussian -0.93939(0.23324) -0.89837(0.29501) -0.90655(0.26212)
standard 0.00003(0.0012) -0.00063(0.00182) -0.00015(0.00529)
ATAF -0.00016(0.00092) 0.00111(0.00343) 0.00012(0.007)
SymClip 0.00004(0.00116) -0.00179(0.00194) -0.00073(0.00512)
proposed(gaussian,withLOFT) -0.0(0.00115) -0.00083(0.00262) 0.00007(0.00818)
proposed(student,withLOFT) -0.00032(0.00093) -0.00196(0.00337) 0.00229(0.01066)
proposed(gaussian,noLOFT) -0.0(0.00115) -0.00083(0.00262) 0.00007(0.00818)
proposed(student,noLOFT) -0.00012(0.00113) -0.00022(0.00293) -0.00135(0.00625)
SMC -0.00035(0.00177) -0.00019(0.00257) -0.00046(0.0023)
ConjugateLinearRegression
Method d=11 d=101 d=1001
truevalue -269.73904 -347.81384 -320.59267
meanfieldgaussian -271.18867(0.48871) -377.48859(1.99237) -2096.65892(15.65285)
standard -269.73934(0.00054) -347.81378(0.00127) -321.12109(0.27435)
ATAF -269.73912(0.00026) -347.81345(0.00242) -321.00036(0.58395)
SymClip -269.73882(0.00038) -347.81441(0.00228) -320.86489(0.57739)
proposed(gaussian,withLOFT) -269.73879(0.00038) -347.81429(0.00213) -321.45662(0.43597)
proposed(student,withLOFT) -269.73907(0.00041) -347.81414(0.00249) -320.99025(1.28594)
proposed(gaussian,noLOFT) -269.73879(0.00038) -347.81429(0.00213) -321.45662(0.43597)
proposed(student,noLOFT) -269.73907(0.00041) -347.81414(0.00249) -320.99025(1.28594)
SMC -281.18175(3.00131) -595.63322(22.78467) -3480.17078(123.3412)26 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
Table 9. Evaluation of all methods in terms of ELBO (standard
deviation in brackets) for d ∈ {10,100,1000}; number of flows
r =16.
Funnel
Method d=10 d=100 d=1000
meanfieldgaussian -1.86318(0.00707) -3.0504(0.00729) -4.20619(0.00786)
standard -0.02689(0.00183) -0.04959(0.00198) -0.25683(0.0073)
ATAF -0.01924(0.00171) -0.05021(0.0017) -0.2305(0.00641)
SymClip -0.42434(0.00289) -0.84732(0.00232) -1.03967(0.00336)
proposed(gaussian,withLOFT) -0.02236(0.00114) -0.10715(0.00125) -0.38719(0.00264)
proposed(student,withLOFT) -0.02472(0.00074) -0.14684(0.0019) -0.41061(0.00382)
proposed(gaussian,noLOFT) -0.02236(0.00114) -0.10715(0.00125) -0.38719(0.00264)
proposed(student,noLOFT) -0.02472(0.00074) -0.14684(0.0019) -0.41061(0.00382)
MultivariateStudent-T
Method d=10 d=100 d=1000
meanfieldgaussian -2.16601(0.00651) -4.5299(0.00767) -7.35684(0.00516)
standard -0.03773(0.00216) -0.16146(0.00566) -1.08358(0.00683)
ATAF -0.03397(0.00147) -0.19026(0.00388) -1.09653(0.00962)
SymClip -0.2069(0.00163) -0.52876(0.00305) -1.58265(0.00651)
proposed(gaussian,withLOFT) -0.07184(0.00096) -0.23981(0.00259) -0.812(0.00458)
proposed(student,withLOFT) -0.06782(0.00093) -0.2354(0.00199) -0.89013(0.0052)
proposed(gaussian,noLOFT) -0.07184(0.00096) -0.23981(0.00259) -0.812(0.00458)
proposed(student,noLOFT) -0.06782(0.00093) -0.2354(0.00199) -0.89013(0.0052)
MultivariateGaussianMixture
Method d=10 d=100 d=1000
meanfieldgaussian -1.09054(0.00072) -1.09085(0.00086) -1.09121(0.00081)
standard -0.03246(0.00167) -0.05054(0.00246) -0.22116(0.00461)
ATAF -0.01976(0.00173) -0.06112(0.00237) -0.19273(0.00392)
SymClip -0.02465(0.00143) -0.07069(0.00255) -0.17468(0.00528)
proposed(gaussian,withLOFT) -0.01937(0.00119) -0.08139(0.00244) -0.19073(0.00416)
proposed(student,withLOFT) -0.01625(0.00132) -0.06934(0.00283) -0.21368(0.00352)
proposed(gaussian,noLOFT) -0.01937(0.00119) -0.08139(0.00244) -0.19073(0.00416)
proposed(student,noLOFT) -0.01625(0.00132) -0.06934(0.00283) -0.21368(0.00352)
ConjugateLinearRegression
Method d=11 d=101 d=1001
meanfieldgaussian -275.82849(0.0274) -399.32425(0.05422) -2348.47948(0.61318)
standard -269.7403(0.00028) -347.84904(0.00169) -332.96779(0.04609)
ATAF -269.74006(0.00035) -347.86773(0.00228) -331.99212(0.03487)
SymClip -269.74247(0.00049) -347.9524(0.00331) -416.32816(0.09648)
proposed(gaussian,withLOFT) -269.7403(0.00037) -347.85343(0.00186) -331.47248(0.02527)
proposed(student,withLOFT) -269.74073(0.00047) -347.84652(0.00161) -331.62923(0.03325)
proposed(gaussian,noLOFT) -269.7403(0.00037) -347.85343(0.00186) -331.47248(0.02527)
proposed(student,noLOFT) -269.74073(0.00047) -347.84652(0.00161) -331.62923(0.03325)HIGH-DIMENSIONAL VARIATIONAL INFERENCE 27
Table 10. Estimatedlogmarginallikelihood(standarddeviation
in brackets) for d∈{10,100,1000}; number of flows r =16.
Funnel
Method d=10 d=100 d=1000
truevalue 0.0 0.0 0.0
meanfieldgaussian -0.9051(0.25602) -1.93821(0.67967) -3.11768(0.52003)
standard 0.00018(0.01105) -0.00295(0.00439) -0.03178(0.02555)
ATAF -0.00471(0.00219) -0.00002(0.00672) -0.01926(0.01717)
SymClip -0.23247(0.03862) -0.66797(0.05036) -0.86603(0.06121)
proposed(gaussian,withLOFT) -0.01075(0.00345) -0.0732(0.00722) -0.2795(0.01588)
proposed(student,withLOFT) -0.00391(0.02007) -0.07329(0.08636) -0.29113(0.02494)
proposed(gaussian,noLOFT) -0.01075(0.00345) -0.0732(0.00722) -0.2795(0.01588)
proposed(student,noLOFT) -0.00391(0.02007) -0.07329(0.08636) -0.29113(0.02494)
SMC -0.1429(0.06911) -1.76495(0.36939) -3.50737(0.46121)
MultivariateStudent-T
Method d=10 d=100 d=1000
truevalue 0.0 0.0 0.0
meanfieldgaussian -0.92496(0.16171) -2.87842(0.3654) -5.89942(0.22141)
standard -0.01292(0.00612) -0.01293(0.05005) -0.21087(0.06764)
ATAF -0.01524(0.00282) -0.00086(0.15296) -0.17079(0.07035)
SymClip -0.11578(0.02809) -0.27219(0.1134) -0.80163(0.14787)
proposed(gaussian,withLOFT) -0.04565(0.01678) -0.13769(0.021) -0.22575(0.04374)
proposed(student,withLOFT) -0.03505(0.02326) -0.13828(0.02616) -0.25835(0.08885)
proposed(gaussian,noLOFT) -0.04565(0.01678) -0.13769(0.021) -0.22575(0.04374)
proposed(student,noLOFT) -0.03505(0.02326) -0.13828(0.02616) -0.25835(0.08885)
SMC -0.01938(0.01633) -0.14798(0.09068) -0.98769(0.39748)
MultivariateGaussianMixture
Method d=10 d=100 d=1000
truevalue 0.0 0.0 0.0
meanfieldgaussian -0.93939(0.23324) -0.89837(0.29501) -0.90655(0.26212)
standard -0.00019(0.00186) -0.00066(0.00174) -0.00239(0.01083)
ATAF 0.00012(0.0013) 0.00009(0.00291) 0.00243(0.00582)
SymClip 0.00058(0.00226) -0.00059(0.00267) -0.00351(0.00885)
proposed(gaussian,withLOFT) -0.0001(0.00133) -0.00123(0.00284) 0.00088(0.01197)
proposed(student,withLOFT) -0.00012(0.0015) 0.00109(0.00353) -0.00217(0.00491)
proposed(gaussian,noLOFT) -0.0001(0.00133) -0.00123(0.00284) 0.00088(0.01197)
proposed(student,noLOFT) -0.00012(0.0015) 0.00109(0.00353) -0.00217(0.00491)
SMC -0.00035(0.00177) -0.00019(0.00257) -0.00046(0.0023)
ConjugateLinearRegression
Method d=11 d=101 d=1001
truevalue -269.73904 -347.81384 -320.59267
meanfieldgaussian -271.18867(0.48871) -377.48859(1.99237) -2096.65892(15.65285)
standard -269.73904(0.00041) -347.81354(0.00239) -321.87012(0.83609)
ATAF -269.73903(0.00027) -347.81391(0.00234) -321.86561(0.4688)
SymClip -269.73905(0.00056) -347.81312(0.00791) -377.06485(3.65637)
proposed(gaussian,withLOFT) -269.73924(0.00038) -347.81352(0.0013) -321.66793(0.96678)
proposed(student,withLOFT) -269.73885(0.00037) -347.81321(0.00196) -321.71383(0.46365)
proposed(gaussian,noLOFT) -269.73924(0.00038) -347.81352(0.0013) -321.66793(0.96678)
proposed(student,noLOFT) -269.73885(0.00037) -347.81321(0.00196) -321.71383(0.46365)
SMC -281.18175(3.00131) -595.63322(22.78467) -3480.17078(123.3412)28 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
Table 11. Evaluation of all methods in terms of ELBO and log
marginal likelihood estimates (standard deviation in brackets) for
d′ ∈{10,100,1000}.
HorseshoeLogisticRegressionModel(r=64)–SyntheticData–ELBO
Method d′=10 d′=100 d′=1000
meanfieldgaussian -50.16087(0.02565) -78.82118(0.03113) -206.80469(0.04076)
standard -43.58686(0.00193) -56.82198(1.10275) -170.82029(0.39773)
ATAF -43.58192(0.00146) -56.6599(0.01174) -150.64962(0.36023)
SymClip -43.6015(0.00149) -56.63336(0.01441) -154.68972(0.06192)
proposed(gaussian,withLOFT) -43.57562(0.00114) -56.16784(0.00936) -145.28784(0.06735)
proposed(student,withLOFT) -43.57637(0.00101) -56.10268(0.01114) -105.29181(0.07462)
proposed(gaussian,noLOFT) -43.59333(0.00204) -56.1108(0.00909) -156.21152(0.07304)
proposed(student,noLOFT) -43.57314(0.00116) -56.0003(0.01031) -123.24537(0.08508)
HorseshoeLogisticRegressionModel(r=64)–SyntheticData–LogMarginalLikelihoodEstimate
Method d′=10 d′=100 d′=1000
meanfieldgaussian -45.81527(0.54777) -68.21041(1.24219) -178.97184(2.58527)
standard -43.55468(0.00181) -54.70689(0.33386) -143.3784(1.98397)
ATAF -43.55316(0.00466) -54.73648(0.58386) -122.16614(2.35816)
SymClip -43.5537(0.01553) -54.90284(0.13389) -127.47038(2.08928)
proposed(gaussian,withLOFT) -43.55563(0.00156) -54.73608(0.21571) -116.7371(2.77658)
proposed(student,withLOFT) -43.55656(0.00151) -54.70299(0.16654) -81.45998(2.62084)
proposed(gaussian,noLOFT) -43.55392(0.00389) -54.72848(0.23261) -127.96218(2.46497)
proposed(student,noLOFT) -43.5535(0.00213) -54.69696(0.12413) -97.01509(2.01398)
SMC -72.15519(0.60622) -316.74614(1.23563) -2605.75083(0.82943)
HorseshoeLogisticRegressionModel(r=16)–SyntheticData–ELBO
Method d′=10 d′=100 d′=1000
meanfieldgaussian -50.16087(0.02565) -78.82118(0.03113) -206.80469(0.04076)
standard -43.59642(0.00235) -58.35805(0.02742) -172.75058(0.0779)
ATAF -43.60519(0.00209) -58.47138(0.10144) -151.93924(0.17945)
SymClip -43.71616(0.00359) -59.93733(0.01882) -171.2301(0.07379)
proposed(gaussian,withLOFT) -43.642(0.00259) -58.80727(0.01738) -166.96998(0.06232)
proposed(student,withLOFT) -43.61169(0.00202) -57.26398(0.01502) -129.22909(0.08087)
proposed(gaussian,noLOFT) -43.64326(0.00271) -58.78684(0.01471) -167.61975(0.05924)
proposed(student,noLOFT) -43.60367(0.00203) -57.25689(0.01509) -130.20364(0.07451)
HorseshoeLogisticRegressionModel(r=16)–SyntheticData–LogMarginalLikelihoodEstimate
Method d′=10 d′=100 d′=1000
meanfieldgaussian -45.81527(0.54777) -68.21041(1.24219) -178.97184(2.58527)
standard -43.55501(0.0045) -55.21554(0.26658) -145.56326(2.11553)
ATAF -43.55648(0.00472) -55.3366(0.18181) -122.46094(2.26795)
SymClip -43.57685(0.01127) -56.1934(0.27895) -144.30236(2.41617)
proposed(gaussian,withLOFT) -43.56231(0.01606) -55.68322(0.24981) -140.06385(1.82293)
proposed(student,withLOFT) -43.55693(0.00564) -54.97434(0.21206) -101.48784(2.69617)
proposed(gaussian,noLOFT) -43.56263(0.01568) -55.6475(0.36711) -140.41891(2.16415)
proposed(student,noLOFT) -43.55214(0.02036) -54.93189(0.27887) -101.52184(2.82329)
SMC -72.15519(0.60622) -316.74614(1.23563) -2605.75083(0.82943)HIGH-DIMENSIONAL VARIATIONAL INFERENCE 29
References
UriAlon,NaamaBarkai,DanielANotterman,KurtGish,SuzanneYbarra,Daniel
Mack, and Arnold J Levine. Broad patterns of gene expression revealed by
clustering analysis of tumor and normal colon tissues probed by oligonucleotide
arrays.ProceedingsoftheNationalAcademyofSciences,96(12):6745–6750,1999.
10
Daniel Andrade. Loft-stable training of normalizing flows for variational inference.
In The 6th Workshop on Tractable Probabilistic Modeling, 2023. 2
MichaelArbel,AlexMatthews,andArnaudDoucet.Annealedflowtransportmonte
carlo. InInternational Conference on Machine Learning, pages318–330.PMLR,
2021. 2, 9, 18
LyntonArdizzone,CarstenLu¨th,JakobKruse,CarstenRother,andUllrichK¨othe.
Guided image generation with conditional invertible neural networks. arXiv
preprint arXiv:1907.02392, 2019. 2, 6, 7, 8
Jens Behrmann, Paul Vicol, Kuan-Chieh Wang, Roger Grosse, and J¨orn-Henrik
Jacobsen. Understanding and mitigating exploding inverses in invertible neural
networks. In International Conference on Artificial Intelligence and Statistics,
pages 1792–1800. PMLR, 2021. 2
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A
review for statisticians. Journal of the American statistical Association, 112
(518):859–877, 2017. 1, 4
James Brofos, Marylou Gabri´e, Marcus A Brubaker, and Roy R Lederman. Adap-
tationoftheindependentmetropolis-hastingssamplerwithnormalizingflowpro-
posals. InInternationalConferenceonArtificialIntelligenceandStatistics,pages
5949–5986. PMLR, 2022. 17
CarlosMCarvalho,NicholasGPolson,andJamesGScott.Thehorseshoeestimator
for sparse signals. Biometrika, 97(2):465–480, 2010. 9
Hugh Chipman, Edward I George, Robert E McCulloch, Merlise Clyde, Dean P
Foster, and Robert A Stine. The practical implementation of bayesian model
selection. Lecture Notes-Monograph Series, pages 65–134, 2001. 9
ChenguangDai,JeremyHeng,PierreEJacob,andNickWhiteley. Aninvitationto
sequentialmontecarlosamplers. JournaloftheAmericanStatisticalAssociation,
117(539):1587–1600, 2022. 2, 9, 18
Akash Kumar Dhaka, Alejandro Catalina, Michael R Andersen, M˚ans Magnusson,
Jonathan Huggins, and Aki Vehtari. Robust, accurate stochastic optimization
for variational inference. Advances in Neural Information Processing Systems,
33:10961–10973, 2020. 12
Akash Kumar Dhaka, Alejandro Catalina, Manushi Welandawe, Michael R Ander-
sen, Jonathan Huggins, and Aki Vehtari. Challenges and opportunities in high
dimensional variational inference. Advances in Neural Information Processing
Systems, 34:7787–7798, 2021. 1, 2
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using
real nvp. arXiv preprint arXiv:1605.08803, 2016. 4, 5, 8
Paul Hagemann and Sebastian Neumayer. Stabilizing invertible neural networks
using mixture models. Inverse Problems, 37(8):085002, 2021. 2
Matthew D Hoffman, Andrew Gelman, et al. The no-u-turn sampler: adaptively
setting path lengths in hamiltonian monte carlo. J. Mach. Learn. Res., 15(1):
1593–1623, 2014. 10, 1830 HIGH-DIMENSIONAL VARIATIONAL INFERENCE
Chin-Wei Huang, Laurent Dinh, and Aaron Courville. Augmented normalizing
flows: Bridging the gap between generative flows and latent variable models.
arXiv preprint arXiv:2002.07101, 2020. 5
Isao Ishikawa, Takeshi Teshima, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and
Masashi Sugiyama. Universal approximation property of invertible neural net-
works. Journal of Machine Learning Research, 24(287):1–68, 2023. 5
PriyankJaini, IvanKobyzev, YaoliangYu, andMarcusBrubaker. Tailsoflipschitz
triangular flows. In International Conference on Machine Learning, pages 4673–
4681. PMLR, 2020. 2, 7, 8
Ghassen Jerfel, Serena Wang, Clara Fannjiang, Katherine A Heller, Yian Ma, and
Michael I Jordan. Variational refinement for importance sampling using the
forward kullback-leibler divergence. arXiv preprint arXiv:2106.15980, 2021. 10
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
In ICLR, 2015. 9
Elizabeth Koehler, Elizabeth Brown, and Sebastien J-PA Haneuse. On the assess-
mentofmontecarloerrorinsimulation-basedstatisticalanalyses. The American
Statistician, 63(2):155–162, 2009. 10
Frederic Koehler, Viraj Mehta, and Andrej Risteski. Representational aspects of
depth and conditioning in normalizing flows. In International Conference on
Machine Learning, pages 5628–5636. PMLR, 2021. 2, 5
Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M
Blei. Automaticdifferentiationvariationalinference. Journalofmachinelearning
research, 2017. 9
Holden Lee, Chirag Pabbaraju, Anish Prasad Sevekari, and Andrej Risteski. Uni-
versalapproximationusingwell-conditionednormalizingflows. AdvancesinNeu-
ral Information Processing Systems, 34:12700–12711, 2021. 5
Feynman Liang, Michael Mahoney, and Liam Hodgkinson. Fat–tailed variational
inference with anisotropic tail adaptive flows. In International Conference on
Machine Learning, pages 13257–13270. PMLR, 2022. 2, 8, 16
AlexMatthews,MichaelArbel,DaniloJimenezRezende,andArnaudDoucet. Con-
tinualrepeatedannealedflowtransportmontecarlo. InInternationalConference
on Machine Learning, pages 15196–15219. PMLR, 2022. 2
Radford M Neal. Annealed importance sampling. Statistics and computing, 11:
125–139, 2001. 9, 18
Radford M Neal. Slice sampling. The annals of statistics, 31(3):705–767, 2003. 8
Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov
chain monte carlo, 2(11):2, 2011. 10
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed,
and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and
inference. Journal of Machine Learning Research, 22(57):1–64, 2021. 1, 4, 9
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
Automatic differentiation in pytorch. 2017. 13
Du Phan, Neeraj Pradhan, and Martin Jankowiak. Composable effects for flex-
ible and accelerated probabilistic programming in numpyro. arXiv preprint
arXiv:1912.11554, 2019. 13
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference.
In Artificial intelligence and statistics, pages 814–822. PMLR, 2014. 3HIGH-DIMENSIONAL VARIATIONAL INFERENCE 31
DaniloRezendeandShakirMohamed. Variationalinferencewithnormalizingflows.
InInternational conference on machine learning,pages1530–1538.PMLR,2015.
2, 12, 16
GeoffreyRoeder,YuhuaiWu,andDavidKDuvenaud.Stickingthelanding: Simple,
lower-variance gradient estimators for variational inference. Advances in Neural
Information Processing Systems, 30, 2017. 2, 4, 12, 16, 19
Antoine Salmona, Valentin De Bortoli, Julie Delon, and Agn`es Desolneux. Can
push-forwardgenerativemodelsfitmultimodaldistributions? AdvancesinNeural
Information Processing Systems, 35:10766–10779, 2022. 2
VincentStimper,DavidLiu,AndrewCampbell,VincentBerenz,LukasRyll,Bern-
hard Sch¨olkopf, and Jos´e Miguel Hern´andez-Lobato. normflows: A PyTorch
Package for Normalizing Flows. arXiv preprint arXiv:2302.12014, 2023. 13
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the
Royal Statistical Society. Series B (Methodological), pages 267–288, 1996. 17
Lorenz Vaitl, Kim A Nicoli, Shinichi Nakajima, and Pan Kessel. Gradients should
stay on path: better estimators of the reverse-and forward kl divergence for
normalizingflows.MachineLearning: ScienceandTechnology,3(4):045006,2022.
1, 2, 4, 12, 16, 19
Steven Winter, Trevor Campbell, Lizhen Lin, Sanvesh Srivastava, and David B
Dunson.Machinelearningandthefutureofbayesiancomputation.arXivpreprint
arXiv:2304.11251, 2023. 17
ChengZhang,JudithBu¨tepage,HedvigKjellstr¨om,andStephanMandt. Advances
in variational inference. IEEE transactions on pattern analysis and machine
intelligence, 41(8):2008–2026, 2018. 1