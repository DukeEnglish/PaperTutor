Improving behavior based authentication against
adversarial attack using XAI
Dong Qin, George Amariucai*, Daji Qiao, Yong Guan
Iowa State University, Ames, IA, USA, {dqin, daji, guan}@iastate.edu
*Kansas State University, Manhattan, KS, USA, amariucai@ksu.edu
Abstract—In recent years, machine learning models, espe- set that follows a natural data distribution. However, the
cially deep neural networks, have been widely used for clas- manipulated data used in adversarial attacks can be out of
sification tasks in the security domain. However, these models
distribution(OOD)[6],leadingtounexpecteddetectionresults
have been shown to be vulnerable to adversarial manipulation:
in the model.
small changes learned by an adversarial attack model, when
applied to the input, can cause significant changes in the Giventhepotentialrisksofadversarialmanipulationattacks,
output. Most research on adversarial attacks and corresponding itiscrucialtounderstandhowmachinelearningcanbeapplied
defense methods focuses only on scenarios where adversarial in security areas and how to mitigate these risks. Researchers
samples are directly generated by the attack model. In this
and practitioners need to develop robust defenses and tech-
study, we explore a more practical scenario in behavior-based
niques to detect and counter adversarial attacks, ensuring the authentication,whereadversarialsamplesarecollectedfromthe
attacker. The generated adversarial samples from the model are reliability and effectiveness of machine learning models in
replicated by attackers with a certain level of discrepancy. We security applications.
propose an eXplainable AI (XAI) based defense strategy against Traditionaluserauthenticationmethods,suchaspasswords,
adversarial attacks in such scenarios. A feature selector, trained
fingerprints, and retina scans, typically require stable and
with our method, can be used as a filter in front of the original
consistent inputs from valid users. Any deviation from the
authenticator. It filters out features that are more vulnerable
to adversarial attacks or irrelevant to authentication, while typical input, such as a single wrong digit in a password,
retainingfeaturesthataremorerobust.Throughcomprehensive leadstocompleterejection.Incontrast,behavioralbiometrics,
experiments,wedemonstratethatourXAIbaseddefensestrategy including gait parameters, hand gestures, and mouse opera-
is effective against adversarial attacks and outperforms other
tion dynamics, exhibit more variability and inconsistency. A
defense strategies, such as adversarial training and defensive
user’s distinctive behavioral pattern may occasionally deviate
distillation.
from their usual pattern. Therefore, behavioral authentication
I. INTRODUCTION systems often need to collect a more substantial amount of
Machine learning models are rapidly gaining popularity in behavioral data to ensure acceptable accuracy.
variouscriticaldomainsofsociety,includingthefieldofsecu- The nature of behavioral biometric data gives rise to a trait
rity.Thesemodels,particularlydeepneuralnetworks(DNNs), wherein if a subset of the input data closely matches the
have been extensively applied in numerous scenarios, ranging behavioralbiometricauthenticator,thatspecificinputsampleis
from user recognition and intrusion detection to malware morelikelytoberecognizedasoriginatingfromthevaliduser.
detection [1], [2]. Unlike traditional user authenticators, which typically employ
However, it is important to note that there are potential an AND logic to validate all inputs, behavioral user authen-
risksassociatedwiththemisuseofthesemodels.Forinstance, ticators tend to utilize an OR logic for validation. However,
the 2024 Homeland Threat Assessment report of the US has this mechanism also renders behavioral authenticators more
highlightedthatadversariescanexploitartificialintelligenceto vulnerable to adversarial attacks, as attackers can manipulate
createmoreconvincingmisinformationandutilizethisemerg- only a portion of their input data to deceive the authenticator.
ing technology to develop evasive cyberattacks on critical To mitigate the vulnerability of behavioral biometric au-
infrastructure. This emphasizes the need for robust security thenticationtoadversarialattacks,weproposeanoveldefense
measures and continuous research to stay ahead of potential method called XAI (explainable AI)-based defense strategy.
threats in this rapidly evolving landscape. This strategy is designed to enhance the robustness of the
The adversarial use of deep learning models, known as authenticator against such attacks. XAI has gained significant
adversarial attacks, has been extensively researched in recent attention in recent years as it enables the interpretation of
years. These attacks involve manipulating input data, such model classification results and enhances the transparency of
as images, videos, or audio, to bypass deep learning-based the decision-making process [7], [8], [9]. Within the field of
authentication systems [3], [4]. The goal of such attacks is to XAI, feature attribution methods are widely used to explain
degrade the true positive rate of the detection model [5]. model decisions on an instance-by-instance basis [10], [11],
One challenge in defending against adversarial attacks is [12], [13]. These methods provide insights into the relative
that the detection models are typically trained on a training importance of features for a given input data in determining
4202
beF
62
]RC.sc[
1v03461.2042:viXrathe final classification. and conduct experiments to demonstrate the feasibility of
Building upon the concept of feature attribution methods, adversarial attacks in this scenario.
we have developed a feature selector that is trained using 2) We propose a defense strategy based on feature attribu-
the same training and testing set as the authenticator model. tion methods. By incorporating an adversarial attack-specific
Our feature selector incorporates a state-of-the-art feature loss term during training, our feature selector can effectively
attribution method, enhanced with our novel improvements, identify features that are more resilient to adversarial attacks
to identify the key features that define the characteristics of while still maintaining a good accuracy.
a valid user. Simultaneously, it eliminates redundant features 3)Wecompareourimprovedfeatureselector-baseddefense
that have no relevance to the final decision-making process. strategy with a basic feature selector-based defense strategy
Acrucialaspectofourfeatureselectorisitsabilitytoavoid as well as adversarial training and defensive distillation based
selecting the unique behavioral patterns of valid users that defense strategies using a mouse authentication dataset. The
exhibit significant fluctuations. This characteristic is advan- results clearly indicate that our proposed strategy outperforms
tageous from an attacker’s perspective, as it becomes easier allotherdefensestrategiesintermsofitseffectivenessagainst
to mimic such patterns. By excluding these irrelevant or adversarial attacks.
unstablefeatures,ourfeatureselectoractsasafilter,effectively
II. PROBLEMSTATEMENT
mitigating adversarial manipulations that specifically target
these features. A. Behavioral biometric authentication system
In this research, our focus is on the application of the Inabehavioralbiometricuserauthenticationscenario,when
XAI-based feature selector within the context of behavioral a person attempts to log into a system, the user’s behavioral
biometric-based user authentication. In this scenario, the ad- data,suchasmouseoperations,handgestures,andgaitparam-
versarial attacker possesses the capability to manipulate the eters, is collected by hardware devices like a mouse, camera,
input data in any manner using an adversarial attack model. or inertial measurement unit. This data is then processed and
It is worth noting that the generated adversarial sample does fedintoanauthenticatormodelthathasbeentrainedusingthe
notnecessarilyhavetoresembletheattacker’sinputbutrather valid user’s previous behavioral data.
needs to be recognized by the authenticator as a valid user’s Taking mouse-based user authentication as an example, as
input. illustratedinFig.2,whenauserattemptstologin,theirmouse
However, attackers have to be able to accomplish the movement trajectories during the login task is recorded as
generated adversarial sample in the physical world. There several movements in format of (position,time) sequence.
will be certain level of discrepancy between generated ideal Subsequently, mouse dynamics such as speed and/or accel-
adversarial sample and attack sample played by attacker in eration can be calculated from the raw input. The extracted
practice. Under this condition, we demonstrate that our XAI mousedynamicsarethenfedintoadeepneuralnetwork-based
based feature selector can select key features that follows authenticator.
the natural distribution and discard those out of distribution
B. Attack model for behavioral biometric authentication
featuresgeneratedbyadversarialattackmodelaswellasthose
redundant features. As shown in fig. 1, by using this feature In this paper, we focus on an attacking scenario where
selector as a filter in front of the original authenticator, the the authenticator is fully accessible to the attacker. However,
wholeclassificationsystembecomemorerobusttoadversarial in practical situations, attackers typically have limited access
attack. to the authenticator. Therefore, defense strategies that are
However,itiscrucialtoconsiderthatattackersmustpossess effective against attacks with unlimited access should also be
the capability to physically execute the generated adversarial applicable to attacks with limited access to the authenticator.
sample. In practice, there will inevitably be a certain level In the case of a behavioral biometric authenticator, the
of discrepancy between the ideal adversarial sample and the adversarial attack is deployed in a more practical manner.
actual attack sample created by the attacker. However, we In typical scenarios like image classification, the generated
demonstrate that our XAI-based feature selector is capable of adversarial samples obtained from the adversarial model are
selecting key features that adhere to the natural distribution directly fed into the classifier. However, for a behavioral
while disregarding out-of-distribution features generated by biometric authenticator, the input data is collected directly
the adversarial attack model, as well as redundant features. fromtheperson,suchasgaitparametersfromawalkingperson
Fig. 1 illustrates integrating our feature selector as a filter in or user-mouse interaction data from a PC/laptop user. As a
frontoftheoriginalauthenticator.Thisintegrationsignificantly result,thegeneratedadversarialattackdatahastobereplicated
enhances the overall robustness of the classification system by the attacker in practice in order to be effective. This adds
against adversarial attacks. additional complexity to the attack process.
The contributions of this research can be summarized as To simulate the discrepancy between the generated ideal
follows: adversarial attack data and the real data collected from the
1) We investigate the impact of adversarial attacks in the attacker, we incorporate Gaussian noise into the generated
context of behavioral biometric authentication, specifically data. The calculation for determining the noise level is pro-
focusing on mouse-based authentication scenarios. We design vided in Appendix A. This level of discrepancy is determinedFig. 1: Illustration of XAI augmented behavioral biometric authenticator. 3 mouse cursor movements (marked in blue) are
selectedaskeypartwhichismoreimportantthantherest9movements(markedinred)foruserauthentication.⃝1 Thefeature
selector takes an input x and returns a k-hot vector M which indicates whether each cognitive chunk, e.g. movement, will be
selected as explanation for classification. ⃝2 T = M ·X serves as a information bottleneck to get rid of the redundant and
unreliable features. ⃝3 The user authenticator takes T =M ·X for classification.
Fig. 2: A mouse behavior based user authentication system.
based on the scenario described in the research presented by achieves the lowest true positive rate is made from a subset
Mathew et al. [14], where the attacker can manually control of attack models. This subset consists solely of attack models
a cursor to follow the target cursor on the screen. Depending whose generated movement is guaranteed to be selected by
on the specific scenario, such as the absence of a targeting the feature selector.
cursororextensivepriorpractice,wecanadjustthenoiselevel Fig. 3 illustrates the training diagram of the adversarial
accordingly to achieve a more accurate simulation. attackmodel.Forsimplicityandpracticality,weonlyconsider
In this research, we investigate two attack scenarios: one the situation where attacker can only mimic one adversarial
where the attacker has partial access to the detection system, movement during login. But the attacking scenario 2 can also
and another where the attacker has complete access to the be indicative of a situation where an attacker is capable of
entire system. learning and replicating multiple movements during the login
Attacking Scenario 1) In the scenario of an adversar- process. As the attacker tries to learn additional movements,
ial attack where the authenticator is accessible, the attacker the chances of those learned movements been selected by our
strategically deploys an attack against the authenticator. In feature selector increase. In this attacking scenario, whether
order to achieve this, we train multiple attack models, each the attacker learns to mimic all movements or only the
designed to generate a distinct movement that will replace selected ones, it ensures that the adversarial movement will
thecorrespondingmovementoftheattacker’soriginalsample. be selected and influence the final decision-making process.
The ultimate goal is to minimize the true positive rate of In such case, our feature selector contributes by forcing the
the classifier. Subsequently, the attacker carefully selects the attacker to mimic those movements that are harder to mimic.
generated movement that results in the lowest true positive Consequently, the challenge to our feature selection based
rate. As a result, the focus of the attack is narrowed down to strategy changes to, whether focusing on several movements
effectivelymimickingthisspecificmovementduringthelogin that are both crucial for classification and harder to mimic
process, rather than targeting the entire sample as a whole. would improve the system overall performance compared to
Attacking Scenario 2) In addition to the scenario where consider all movements even if there are some movements
we assume the attacker has access to the user authenticator more vulnerable to adversarial attack.
model,wealsoexploreanattackingscenariowherethefeature Inourresearch,theobjectiveistodevelopadefensestrategy
selector is also accessible. In this particular scenario, the to protect against adversarial attacks. The performance of
attacker selectively chooses attack models that specifically this defense strategy is primarily evaluated based on two key
target the key movements identified by the feature selector. metrics. 1) True positive rate under adversarial attack, which
As a result, the attacker’s selection of the attack model that measures the effectiveness of the authentication system inFig. 3: In our experiment, each input sample includes 10 consecutive movements that follows certain pattern. 10 adversarial
attack models are trained to generate 10 movements correspondingly to maximize prediction loss. 1 out of 10 (feature selector
not accessible) or 1 out of 2/3/4/5 (feature selector accessible so attackers know which 2/3/4/5 movements will be selected)
models that achieves the lowest true positive rate is selected as the final attacking model.
withstanding such attacks. This metric ensures the system’s between classifier boundaries for most valid user’s samples.
robustness and ability to accurately identify and reject adver- On the contrary, corner cases like the top left sample point
sarial samples. 2) Accuracy of the defense strategy in the au- will no longer be recognized as valid user’s sample when
thenticator’stestingset,whichassessesitsoverallperformance we discard one feature, e.g. feature X. Because feature Y of
andreliabilityinreal-worldscenarios.Thismetricensuresthat the classifier has enough space for discrepancy in that area.
the defense strategy remains effective in practical everyday However, such sacrifice in true negative rate will either filter
usage, guaranteeing the system’s usefulness and reliability. out the feature manipulated by adversarial model or force
adversarial model to target on feature that has less tolerance
III. THEPROPOSEDXAIBASEDADVERSARIALATTACK for discrepancy.
DEFENSESTRATEGY
In the following subsections, we will explain in detail
The feature selector in our research primarily utilizes the how each part of our feature selector training diagram helps
feature attribution method proposed in [15] for training. improve the behavioral authenticator system.
However, in order to enhance the defense against adversarial
attacks, our objective is to ensure that the feature selector A. XAI model selects key features
not only filters out features with minimal contribution to
classification accuracy but also filters out features that are A feature attribution method is applied to a machine learn-
particularly susceptible to adversarial attacks in practical sce- ing classifier, generating a feature attribution vector for each
narios. By incorporating this additional capability into the input sample. This vector quantifies the contribution of each
feature selector, we aim to further strengthen the system’s feature to the classification decision. Subsequently, a feature
resistance against adversarial attacks and improve its overall selector, based on the feature attribution vector, is employed
robustness. to choose the top n features for each input sample that have
In our research, as shown in Fig. 4 we extend the training a greater impact on the classification outcome.
procedure proposed in the previous work [15] by introducing Intheuserauthenticationscenario,redundantinputfeatures
an additional loop. This new loop aims to minimize the are commonly observed. In our research, we utilized a dataset
prediction loss when our classifier is under adversarial attack, of mouse-based user authentications, where users mainly rely
where all the features have been replaced by an adversarial on a few specific movements for effective classification. By
sample generator. The purpose of this adversarial sample employing a well-trained XAI algorithm-based feature selec-
generatoristogenerateadversarialsampleswithrandomnoise tor, it is possible to identify the key features that significantly
on each data point. This randomness simulates the natural contribute to the classification process. By focusing on these
discrepancy between the ideal adversarial samples generated essentialfeatures,highclassificationaccuracycanbeachieved.
bytheadversarialsamplegeneratorandtherealinputsamples Therefore, to eliminate redundant features, we can train
collected from attackers who try to mimic the generated a feature selector using a state-of-the-art feature attribution
adversarial sample. algorithm [15] and employ it as a filter. Fig. 4 shows how
By including this loss term in the training loop, our feature this technique utilizes a generative model to handle common
selectorisencouragedtoselectfeaturesthataremoredifficult artifacts that may occur in conventional feature attribution
for attackers to mimic. In other words, the feature selector methods. Moreover, the feature selector is optimized by mini-
tendstoprioritizefeaturesthatrepresentvaliduser’sstableand mizingthepredictionlossassociatedwiththeselectedfeatures
consistent patterns and discard features that are more specific while maximizing the prediction loss associated with the
to corner cases. For example, in Fig. 5, the feature selector non-selected features. This optimization approach effectively
would likely to focus on feature X due to its smaller space reduces the risk of information leakage through masking.Fig. 4: Our improved feature selector training diagram.
In our research, the preprocessed input data consists of
sequences of (v ,v ). To simplify the process, the feature se-
x y
lectorwillgenerateavectorindicatingtheselectedmovements
rather than individual data points. We have conducted experi-
ments, as described in Sec.IV, to evaluate the performance of
the feature selector and provide accurate results.
B. Adversarial attack model achieves lower true positive rate
when manipulating more vulnerable features
Iftheclassifierisaccessible,itisindeedpossibletotrainan
Fig. 5: After we apply random Gaussian noise (purple bars)
adversarial attack model that generates a portion of the input
to each generated feature dimension of the adversary sample
to replace the attacker’s original input, resulting in a low true
(purple dots), more differentiable and more consistent feature
positive rate. However, in the case of a behavioral biometric
dimension (feature X) becomes more robust to the adversarial
data-based user authentication system, the authentication data
attack (part of the space between bars is out of classifier
is collected directly from a real person. Therefore, the adver-
boundaries (green lines) of the valid user).
sarial sample generated by the attack model would need to
be replicated by the attacker in the physical world. There will
alwaysbesomelevelofdistortionbetweentherealbehavioral
data performed by a human and the target behavioral data C. Choosing important but less vulnerable features
generated by the adversarial attack model. As a result, the
real sample played by a human may not be able to achieve as As shown in Fig. 5, applying random Gaussian noise to
low of a true positive rate as the generated target sample. the generated target features makes features that have a larger
Inordertofindthosevulnerablefeatureswhichtheclassifier space between classifier boundaries more vulnerable to the
boundaries in corresponding feature dimensions has larger adversarialattack.Incontrast,featuresthataremoreconsistent
tolerance for discrepancy and allows real world attackers to and stable, leading to a narrower feature space between
achieve lower true positive rate, we add Gaussian noise to the classifier boundaries, are preferred for authentication system.
adversary sample in the training of attack model. Therefore, Therefore, our feature selector training diagram consists of
by selecting one out of ten attack models that achieves the twoparts.Thefirstpartisthesameaspresentedin[15],which
lowest true positive rate, we can find the most vulnerable one allowsthefeatureselectortochoosefeaturesthatminimizethe
out of ten movements for the mouse authentication task that prediction loss in the training set comprising data from both
is easier for attacker to mimic. valid and invalid users. The second part involves a pre-trained
Fig.5providesanillustrationinatwo-dimensionalexample adversary sample generator that targets the authenticator. By
where the applied Gaussian noise improves the performance training the feature selector to minimize the prediction loss of
of the adversarial attack model by choosing to manipulate adversarial samples in the authenticator, our feature selector
the feature that has a wider feature space between classifier can avoid selecting features that are vulnerable to adversarial
boundaries.Inthisresearch,insteadofselectingasinglepoint, manipulation, such as feature y in Fig. 5. The trade-off be-
we select part of the entire velocity sequence, which we refer tweenpredictionlossintheregulartrainingsetandprediction
to as a movement for mouse behavior data. However, the loss in adversarial samples is controlled by the parameter
analysis for low-dimensional cases can also be applied to β. The training process of our improved feature selector is
higher-dimensional cases. summarized in Algorithm.1.Algorithm 1 Improved Feature Selector Training Algorithm Further details regarding the model structure and parameter
Input: X ∈ Rs×m, training set samples, where s and m are dataset size and settings can be found in Appendix. B.
featuresize.E∈Rs×m,referencebackgroundnoiseusedtofillingthenon-selected
features,whereEi(j),israndomlysampledfromfeaturej inthetrainingdataset. B. Mouse behavior based user authentication dataset
X i′,generatedidealadversarialsample.Ni(j)∼N(0,σi(j)2),randomGaussian
noiseusedtosimulatethedifferencebetweengeneratedidealadversarialfeaturevalue The dataset used to evaluate all three adversarial defense
andrealfeaturevaluecollectedfromattackers,whereσi(j)isdeterminedbyaverage
velocityvalueofcorrespondingmovementthatX′(j)belongsto.y∈Rs,ground methods is the mouse behavior authentication dataset pre-
truthlabelsfortrainingsetsamples.y∗ ∈ Rs,gi roundtruthlabelsforadversarial sented in [16]. In this dataset, 21 subjects were instructed
samples.P,behavioralauthenticator;
Output: Fne,se(·;θF),featureselectorthatreturnsmasktoselectne chunkseach to perform a predefined task consisting of 10 consecutive
consistofsefeaturesgivenaninputinstanceX movements in a specific pattern, as shown in Fig. 1. Each
Select: d,mini-batchsize;λ,learningrate;t,trainingsteps;α,controlparameterthat
balanceselectedandnon-selectedfeatures;β,controlparameterthatbalanceprediction movement has a defined starting and ending point.
lossandreconstructionloss;ne,numberofoutputexplanationunit;se,sizeofoutput The raw data is collected in the form of (x,y,t), where x
explanationunit;
for1,...,tdo andy representthecoordinatesofthemousecursor,andtrep-
Randomlysamplemini-batchofsized,X∈Rd×m
resents the time stamp. However, for classification purposes,
fori=1,...,ddo
Mi=F(Xi;θF) the velocity (v x,v y) is calculated from the raw data and used
X(cid:98)i=G1(Xi·Mi+Ei·(1−Mi);θG1) as input features. The decision to use velocity instead of raw
X˜ i=G2(Xi·(1−Mi)+Ei·Mi;θG2)
dataisbasedonitsbetteroverallperformanceinclassification
y(cid:98)i=P(X(cid:98)i)
y˜i=P(X˜ i) tasks.
yMX (cid:98)i∗i∗ i∗ == = PX F (Xi (′ X+ i∗i∗ ·N ; Mθi F i∗)
+Ei·(1−M i∗))
andTh ee dud ca ata tis oe nt c bo an cs ki gst rs ouo nf d2 ,1 as llub oj fec wts h, ov mary pi on sg sein ssa pg re o, fig ce in ed ne cr y,
l1=−(cid:80)d i=1y dilog(y(cid:98)i) in using a mouse. Each subject’s data comprises a minimum
l2=−(cid:80)d i=1y dilog(y˜i) of 66 trials. From these subjects, 18 are selected for training
l3=−(cid:80)d i=1yi∗ dlog(y(cid:98)i∗)
18 behavior authenticators, with one subject serving as the
endfor valid user and the remaining 17 as invalid users for training
θF =Adam(L=(1−α)l1−αl2+βl3,λ)
θG1 =Adam(L=l1,λ) and testing purposes. The last three subjects are designated as
θG2 =Adam(L=l2,λ) adversarialattackers,whereoneoutofeverytenmovementsin
endfor
theirsamplesisreplacedwithdatageneratedbyanadversarial
sample generator. Fig. 6 shows an example of the adversarial
IV. EXPERIMENTONMOUSEBASEDUSER attack sample.
AUTHENTICATIONDATASET
A. Evaluated adversarial attack defense strategies
In our experiment, we compare four different adversarial
attack defense strategies:
1)TheImprovedfeatureselectorbaseddefensestrategy,
as illustrated in Fig. 4 and Algorithm 1.
2) The Basic feature selector based defense strategy,
which is similar to the first strategy, but without including
an adversarial sample generator in the training loop.
For both feature selector based defense methods, we ex-
perimented with four different feature selectors. These feature Fig. 6: Example of authentication task played by an attacker:
selectorsselect2,3,4,or5movements,whilethenon-selected velocity sequence is transferred to position domain for better
features are replaced with randomly drawn values from the illustration, attacker’s sample with 1 movement (marked in
same feature in the entire training set. blue) replaced by generated adversarial movement with ap-
3) The Adversarial training based defense strategy, plied Gaussian noise.
which involves adding adversarial samples generated by an
adversarial sample generator to the training set. This new
C. Experiment Result
trainingsetisthenusedtoretrainanewbehaviorauthenticator.
The adversarial sample generator used in this strategy has the We tested two feature selector based defense strategies
same structure as the one employed by attackers to test these in two attacking scenarios. For each authenticator, based on
defense strategies. different strategies, we tested 4 feature selectors to select
4) The Defensive distillation based defense strategy, 2,3,4 and 5 movements as well as keeping all 10 movements.
which takes advantageous of the model distillation method. It Table. I summarizes the one-tailed paired t-test comparison
uses the output score of the regular model as the soft label to resultbetweenourstrategyandthreestrategiesbeencompared.
trainthedistilledmodel.Thedistilledmodelusedindefensive Table. II and Table. VI shows results for the performance
distillation has the same structure as the regular model but is of improved feature selector in the adversarial sample set and
less sensitive to small perturbation. regular testing set. The threshold for authenticators in bothtesting set and adversarial sample set are set to the default basedauthenticatorshowninTable.V,accuracyofourmethod
value as it is in the training set. Table. III and Table. VII decreasesby21.7%to5.0%whenselecting2to5movements.
shows corresponding results for basic feature selector. We However,thedecreaseinaccuracyinthetestingsetcompared
also tested adversarial training based strategy and defensive tothebasicfeatureselector-basedstrategy,showninTable.VII,
distillation based strategy for comparison. Table. IV and is relatively small. This suggests that our method strikes
Table. V summarizes the results. a balance between maintaining classification accuracy and
1) Performance under adversarial attack: As shown in improving defense against adversarial attacks.
Table. II, Table. IV and Table. V. Our method demonstrates a 3) ROC curves for valid user and adversarial attackers:
significant improvement in true positive rate against adversar- Toprovideaclearerillustration,wehaveplottedasetofROC
ial attacks compared to the adversarial training and defensive curves in Fig. 7, which combines data from valid users and
distillation based strategy. Only when both classifier and adversarial attackers. By examining these ROC curves and
feature selector are accessible, comparing to the adversarial referring to the results from previous tables, we can observe a
training based strategy, the improvement is insignificant when trend: the more movements we select, the higher accuracy we
selecting 3,4 or 5 movements for the significance level equal can achieve in the common testing scenario where there is no
to α=0.1. adversarial modification.
Additionally, comparing to the result shown in Table. III. When adversarial modification occurs, the performance
Our strategy outperforms the basic feature selector-based de- of the authenticator that takes all 10 movements as input
fense strategy under adversarial attacks in the scenario when shows a significant decrease. On the other hand, the feature
only classifier is accessible. When both classifier and feature selector-based authentication system demonstrates superior
selector are accessible, for selecting 3 or 4 movements, there performance,particularlywhentheselectedmovementsdonot
is no significant improvement from our strategy. include the modified movement. For instance, if the feature
The improvement from our method can be attributed to selector chooses 2 movements, the system achieves higher
the improved feature selector’s ability to avoid selecting accuracy. However, as the number of selected movements by
movements that are vulnerable to adversarial attacks. In other thefeatureselectorincreasesto5,theoverallaccuracyquickly
words,thefeatureselectordiscardsfeaturesthatcorrespondto drops due to the inclusion of the modified movement within
classifier boundaries with a large tolerance for discrepancies theselection.Basedonourobservationsfromallexperiments,
between generated ideal adversarial data and real data, even we have noticed a consistent trend: the more movements our
if these features may enhance classification accuracy to some feature selector selects, the higher the likelihood of including
extent. the modified movement in its selection.
There are also two trends we can see from the results. Nevertheless, even if the feature selector is accessible and
First, for attacking scenario 2 when both classifier and feature the modified movement falls within the selected movement
selector are accessible from attackers, the improvement from range, the overall performance remains superior compared to
our method becomes less comparing to attacking scenario 1. the scenario without a feature selector. This is because the
Second, when the number of selected movements increases, presence of a feature selector compels the attacker to mimic
the improvement from our method becomes less. a less vulnerable movement, which has a lower tolerance for
However, even in the worst-case scenario where the feature discrepanciesbetweengeneratedidealadversarialdataandreal
selectorisaccessibleanditselects5movementsforclassifica- data.Consequently,ourmethodenhancestherobustnessofthe
tion, our method still demonstrates a improvement in the true authentication system against adversarial attacks, regardless
positive rate for adversarial samples. Specifically, our method of the attacker’s ability to mimic multiple movements. The
improves the true positive rate by 16.3%, 45.6% and 140.7% performance upper bound of our method is actually depend
compared to the basic feature selector, adversarial training on the user’s stable and consistent mouse behavior pattern.
and defensive distillation respectively. But such improvement Although corner cases, such as the top left sample in Fig. 5,
is not evenly distributed in all 18 authenticators. On the can be learned by the classifier, they are disregarded by
contrary, it improve certain authenticators a lot while for the feature selector-classifier combination system due to its
otherauthenticators,theimprovementfromourmethodisless. vulnerable key feature pattern.
Morespecifically,themoreoverlapthereisbetweenimportant
featuresandinvulnerablefeaturesforaauthenticator,themore
V. CONCLUSION
robustness our method can bring to it against adversarial In this paper, our investigation focuses on the potential
attacks. This explains the relatively high p-value in some application of explainable AI (XAI) in defending against
scenarios. adversarial attacks in the context of behavioral biometric au-
2) Performance in normal testing set: Table.VI shows the thentication.WespecificallyexploreacategoryofXAIknown
classification accuracy in the testing set. When comparing as feature attribution methods, which are utilized to train
to the adversarial training-based authenticator shown in Ta- a feature selector responsible for selecting key features and
ble. IV, the classification accuracy of our strategy decreases eliminating redundant ones. In our study, we conduct tests on
by 19.5% to 2.4% when the feature selector selects 2 to 5 twotypesoffeatureselectors:thebasicfeatureselectorusedin
movements. When comparing to the defensive distillation- XAIresearch[15],andourimprovedversionthatincorporatesTABLE I: Statistical comparison for improved feature selector based defense strategy and two strategies been compared.
Improvedfeatureselectorbaseddefensestrategy
Ourdefensestrategy (showsintheleftsideoftheinequalitysign)
Defensestrategy Adversarialtrainingbased Defensivedistillation Basicfeatureselector
beencompared defensestrategy baseddefensestrategy baseddefensestrategy
Numberofmovements
beenselected 2 3 4 5 2 3 4 5 2 3 4 5
Accuracyin 0.702<0.876 0.751<0.876 0.809<0.876 0.851<0.876 0.702<0.896 0.751<0.896 0.809<0.896 0.851<0.896 0.702<0.717 0.751<0.764 0.809>0.805 0.851<0.852
testingset p=1.992e-07 p=2.372e-05 p=5.092e-04 p=0.106 p=1.260e-08 p=9.243e-07 p=1.749e-05 p=0.005 p=0.161 p=0.256 p=0.385 p=0.468
Classifier 0.724>0.195 0.674>0.195 0.658>0.195 0.463>0.195 0.724>0.118 0.674>0.118 0.658>0.118 0.463>0.118 0.724>0.644 0.674>0.614 0.658>0.569 0.463>0.388
Truepositive accessible p=1.266e-04 p=0.001 p=0.001 p=0.040 p=9.750e-07 p=5.431e-06 p=4.768e-06 p=0.005 p=0.090 p=0.086 p=0.070 p=0.094
rateunder Classifierand
adversarial feature 0.384>0.195 0.316>0.195 0.303>0.195 0.284>0.195 0.384>0.118 0.316>0.118 0.303>0.118 0.284>0.118 0.384>0.317 0.316<0.327 0.303>0.285 0.284>0.245
attack selector p=0.064 p=0.144 p=0.160 p=0.243 p=0.006 p=0.016 p=0.018 p=0.040 p=0.029 p=0.279 p=0.176 p=0.074
accessible
TABLE II: True positive rate of improved feature selector based defense strategy in adversarial sample set.
Truepositive
Numberof
rateunder
movements u1 u2 u3 u4 u5 u6 u7 u8 u9 u10 u11 u12 u13 u14 u15 u16 u17 u18 Mean
adversarial
beenselected
attack
10 0.068 0.035 0.13 0.035 0 0.276 0.085 0 0.004 0 0.004 0.087 0 0 0.034 0.013 0 0.087 0.048
2 0.817 0.667 0.492 0.987 1 0.874 0.908 1 0.511 0.667 0.969 1 0.719 0.307 0.015 0.962 0.143 0.996 0.724
Classifier
3 0.909 0.667 0.408 0.948 1 0.841 0.913 0 0.473 1 0.328 1 0.687 0.903 0.025 0.919 0.126 0.978 0.674
accessible
4 0.908 0.662 0.385 0.974 0.667 0.882 0.894 1 0.235 0.667 0.333 1 0.835 0.307 0.024 0.97 0.134 0.965 0.658
5 0.566 0 0.667 0.948 0.649 0.118 0.849 0 0.013 1 0.313 1 0.812 0.299 0.004 0.004 1 0.096 0.463
Classifierand 2 0.258 0.415 0.229 1 0.87 0.253 0.499 0.333 0.734 0 0 0.571 0.25 0 0.02 0.329 0.143 1 0.384
feature 3 0.272 0.088 0.144 0.758 0.341 0.212 0.585 0 0.004 0.996 0 0.556 0.123 0.157 0.034 0.299 0.126 1 0.316
selector 4 0.395 0.108 0.177 0.587 0.035 0.301 0.497 0.333 0 0.494 0 0.854 0.181 0 0.02 0.342 0.134 1 0.303
accessible 5 0.245 0.233 0.532 0.537 0.052 0.118 0.484 0 0.018 0.973 0 0.569 0.162 0 0.004 0.307 0.75 0.122 0.284
TABLE III: True positive rate of basic feature selector based defense strategy in adversarial sample set.
Truepositive
Numberof
rateunder
movements u1 u2 u3 u4 u5 u6 u7 u8 u9 u10 u11 u12 u13 u14 u15 u16 u17 u18 Mean
adversarial
beenselected
attack
10 0.068 0.035 0.13 0.035 0 0.276 0.085 0 0.004 0 0.004 0.087 0 0 0.034 0.013 0 0.087 0.048
2 0.817 0.325 0.492 0.987 1 0.874 0.908 0 0.424 0.667 0.969 1 0.719 0.299 0.015 0.962 0.134 0.996 0.644
Classifier
3 0.909 0.662 0.408 0.948 1 0.841 0.913 0 0 1 0.328 1 0.687 0.299 0.025 0.919 0.143 0.978 0.614
accessible
4 0.908 0.654 0.385 0.974 0.333 0.882 0.894 0.004 0.004 0.667 0.333 1 0.835 0.281 0.024 0.97 0.126 0.965 0.569
5 0.566 0.005 0.667 0.948 0 0.118 0.849 0 0.271 1 0.313 1 0.812 0 0.004 0.004 0.333 0.096 0.388
Classifierand 2 0.258 0.039 0.229 1 0.488 0.253 0.499 0 0.635 0 0 0.571 0.25 0 0.02 0.329 0.134 1 0.317
feature 3 0.272 0.135 0.144 0.758 0.607 0.212 0.585 0 0.022 0.996 0 0.556 0.123 0 0.034 0.299 0.143 1 0.327
selector 4 0.395 0.151 0.177 0.587 0 0.301 0.497 0.004 0.009 0.494 0 0.854 0.181 0 0.02 0.342 0.126 1 0.285
accessible 5 0.245 0.005 0.532 0.537 0 0.118 0.484 0 0.014 0.973 0 0.569 0.162 0 0.004 0.307 0.333 0.122 0.245
TABLE IV: Performance of adversarial training based defense strategy in testing set and adversarial attack sample set.
u1 u2 u3 u4 u5 u6 u7 u8 u9 u10 u11 u12 u13 u14 u15 u16 u17 u18 Mean
Accuracyin
0.882 0.863 0.788 0.911 0.917 0.894 0.974 0.966 0.873 0.788 0.833 0.882 0.821 0.923 0.833 0.833 0.981 0.807 0.876
testingset
Truepositive
rateunder
0 0.061 0 0.056 0.139 0 0.014 0.058 0 0.122 0.991 0.121 0.087 0 1 0 0.004 0.849 0.195
adversarial
attack
an adversarial sample generator within the training loop. We in the feature selector.
consider two attacking scenarios: one where the authenticator Indeed,whiletheremaybeaslightdecreaseinclassification
is fully accessible, and another where both the authenticator accuracyinthecommontestingsetduetothefeatureselector,
andfeatureselectorareaccessibletotheattacker.Additionally, this decrease can be limited to a small range if the valid
we compare our approach to other commonly used defense user exhibits several stable and consistent behavior patterns,
strategies against adversarial attacks, specifically adversarial and an appropriate number of features is selected. On the
training-baseddefensemethodanddefensivedistillation-based otherhand,theadditionoftheimprovedfeatureselectorleads
defense method. to a significant enhancement in the authentication system’s
resilienceagainstadversarialattacks.Therefore,thebenefitsof
Toevaluatetheaforementioneddefensemethods,weimple-
incorporatingthefeatureselectoroutweightheslightdecrease
mentedthemusingamousebehavior-baseduserauthentication
in classification accuracy in the common testing set.
dataset introduced in [16]. Through comprehensive testing,
several conclusions can be drawn. Firstly, incorporating a
VI. RELATEDWORK
feature selector in the authentication system significantly en-
hancesitssecurityagainstadversarialattacks.Secondly,when Various defense mechanisms have been extensively studied
an adversarial sample generator is included in the training to protect detection systems against adversarial attacks. One
loop of the feature selector, the system’s performance against such method, known as adversarial training [17][18][19], in-
adversarial attacks further improves. Thirdly, the number of volves augmenting the training dataset with adversarial sam-
features selected by the feature selector directly impacts the ples. This technique incorporates adversarial samples gener-
classification accuracy in common data sets and true positive atedbyoneormoreadversarialattackmodelsintothetraining
rate under adversarial attack. A balance between these factors set, followed by retraining the classifier. Although adversarial
canbeachievedbycontrollingthenumberofselectedfeatures training improves the classifier’s robustness in scenarios likeTABLE V: Performance of defensive distillation based defense strategy in testing set and adversarial attack sample set.
u1 u2 u3 u4 u5 u6 u7 u8 u9 u10 u11 u12 u13 u14 u15 u16 u17 u18 Mean
Accuracyin
0.871 0.931 0.831 0.953 0.926 0.821 0.931 0.929 0.826 0.857 0.894 0.902 0.784 0.918 0.933 0.898 1 0.929 0.896
testingset
Truepositive
rateunder
0 0 0 0.265 0 1 0 0.052 0.214 0.346 0 0.004 0 0 0.005 0 0.004 0.229 0.118
adversarial
attack
TABLE VI: Accuracy of improved feature selector based defense strategy in testing set.
Numberof
movements u1 u2 u3 u4 u5 u6 u7 u8 u9 u10 u11 u12 u13 u14 u15 u16 u17 u18 Mean
beenselected
10 0.923 0.911 0.894 0.947 0.962 0.965 0.953 0.918 0.833 0.863 0.91 0.903 0.841 0.922 0.934 0.882 0.966 0.913 0.913
2 0.613 0.655 0.615 0.641 0.685 0.732 0.621 0.804 0.681 0.653 0.809 0.647 0.549 0.918 0.733 0.729 0.878 0.679 0.702
3 0.661 0.828 0.708 0.781 0.685 0.857 0.721 0.893 0.71 0.674 0.851 0.647 0.49 0.837 0.717 0.797 0.918 0.749 0.751
4 0.71 0.862 0.677 0.828 0.815 0.911 0.855 0.857 0.826 0.714 0.872 0.784 0.608 0.898 0.8 0.831 0.898 0.816 0.809
5 0.855 0.897 0.677 0.859 0.852 0.946 0.972 0.839 0.812 0.735 0.915 0.843 0.667 0.898 0.867 0.881 0.918 0.886 0.851
TABLE VII: Accuracy of basic feature selector based defense strategy in testing set.
Numberof
movements u1 u2 u3 u4 u5 u6 u7 u8 u9 u10 u11 u12 u13 u14 u15 u16 u17 u18 Mean
beenselected
10 0.923 0.911 0.894 0.947 0.962 0.965 0.953 0.918 0.833 0.863 0.91 0.903 0.841 0.922 0.934 0.882 0.966 0.913 0.913
2 0.613 0.776 0.615 0.641 0.685 0.732 0.621 0.804 0.638 0.653 0.809 0.647 0.745 0.918 0.733 0.729 0.796 0.75 0.717
3 0.661 0.845 0.708 0.781 0.667 0.857 0.721 0.786 0.681 0.674 0.851 0.647 0.804 0.857 0.717 0.797 0.898 0.804 0.764
4 0.71 0.914 0.677 0.828 0.778 0.911 0.855 0.875 0.638 0.714 0.872 0.784 0.726 0.837 0.8 0.831 0.918 0.821 0.805
5 0.855 0.914 0.677 0.859 0.852 0.946 0.972 0.857 0.797 0.735 0.915 0.843 0.765 0.837 0.867 0.881 0.918 0.839 0.852
(a) Inputdata:validuserandattackerswithoutadver- (b) Inputdata:validuserandattackerswithadversarial
sarial modification. Feature selector selecting 2, 3, 4, modification. Feature selector selecting 2, 3, 4, 5
5movements. movements.
Fig. 7: Example of ROC curves for given input data from one valid user and three unknown attackers.
image recognition, its effectiveness may be limited in differ- mechanismsthatcaneffectivelycounterblack-boxattacksand
ent scenarios such as behavioral biometric authentication. In enhance the security of detection systems.
this specific scenario, the generated adversarial samples are
Thefeatureselectionmethodsmentionedin[23],[24],[25],
allowedtocloselyresemblethevaliduser’ssample,potentially
[26], [27] are the most similar approaches to our strategy.
leading to a significant reduction in the model’s true negative
These studies investigated the use of reduced feature sets for
rate if added to the training set. Thus, alternative defense
classificationasadefenseagainstadversarialattacks.However,
mechanisms need to be explored to ensure the security of
therearetwosignificantdifferencesbetweentheseapproaches
behavioral biometric authentication systems.
and our research.
Defensive distillation [20] is a defense mechanism that First, these defense methods are typically tested against
trains the classifier in two rounds using a modified version pure cyberattacks where the input data is directly sourced
of the distillation method [21]. This approach has the ad- from the adversarial attack model. However, in the case of
vantage of producing a smoother network and decreasing the behavioral biometric authentication, where the input data is
magnitude of gradients around input points. As a result, it collected from humans, there are limitations on the attacker’s
becomesmorechallengingforattackerstogenerateadversarial ability to accurately mimic the generated adversarial sample.
examples [20]. However, studies have revealed that while de- Unlike in scenarios involving image recognition, where the
fensive distillation proves effective against white-box attacks, modifiedadversarialsamplemustresembletheoriginalsample
it falls short in providing sufficient protection against black- from a human perspective, the modified attacker’s behavioral
box attacks that are transferred from other networks [22]. biometric sample does not necessarily need to be similar to
Therefore, further research is necessary to develop defense the original attacker’s sample in order to be recognized as alegitimate sample by the authenticator. [20] N. Papernot, P. Mcdaniel, X. Wu, S. Jha, and A. Swami, “Distillation
Second, it is important to note that the feature selection asadefensetoadversarialperturbationsagainstdeepneuralnetworks,”
in2016IEEESymposiumonSecurityandPrivacy(SP),2016.
methods discussed in [23], [24], [25], [26], [27] do not focus
[21] G.Hinton,O.Vinyals,andJ.Dean,“Distillingtheknowledgeinaneural
on instance-wise feature selection. In contrast, our XAI-based network,”ComputerScience,vol.14,no.7,pp.38–39,2015.
feature selector has the capability to achieve sample-specific [22] N.CarliniandD.Wagner,“Towardsevaluatingtherobustnessofneural
networks,”2016.
feature selection. As a result, our feature selector can ensure
[23] B. Biggio, G. Fumera, and F. Roli, “Evade hard multiple classifier
a higher level of classification accuracy. systems,”StudiesinComputationalIntelligence,2009.
[24] Battista,Biggio,Giorgio,Fumera,Fabio,andRoli,“Securityevaluation
REFERENCES of pattern classifiers under attack,” IEEE Transactions on Knowledge
andDataEngineering,2013.
[1] I. Vaccari, G. Chiola, M. Aiello, M. Mongelli, and E. Cambiaso, [25] B. Li and Y. Vorobeychik, “Feature cross-substitution in adversarial
“Mqttset, a new dataset for machine learning techniques on classification,”inNeuralInformationProcessingSystems,2014.
mqtt,” Sensors, vol. 20, no. 22, 2020. [Online]. Available: https: [26] Fei, Zhang, Patrick, P, K, Chan, Battista, Biggio, Daniel, and S, “Ad-
//www.mdpi.com/1424-8220/20/22/6578 versarial feature selection against evasion attacks.” IEEE Transactions
[2] M. Aiello, M. Mongelli, and G. Papaleo, “Dns tunneling detection onCybernetics,2016.
throughstatisticalfingerprintsofprotocolmessagesandmachinelearn- [27] Z. Yin, F. Wang, W. Liu, and S. Chawla, “Sparse feature attacks
ing,”InternationalJournalofCommunicationSystems,vol.28,no.14, in adversarial learning,” IEEE Transactions on Knowledge and Data
2015. Engineering,vol.30,no.6,pp.1164–1177,2018.
[3] W. Jin, Y. Li, H. Xu, Y. Wang, and J. Tang, “Adversarial attacks and
defensesongraphs:Areviewandempiricalstudy,”2020. APPENDIXA
[4] Y.PachecoandW.Sun,“Adversarialmachinelearning:Acomparative GAUSSIANNOISELEVELTOSIMULATEDIFFERENCE
studyoncontemporaryintrusiondetectiondatasets,”in7thInternational
BETWEENIDEALADVERSARIALSAMPLEANDREAL
ConferenceonInformationSystemsSecurityandPrivacy,2021.
[5] S.Sabour,Y.Cao,F.Faghri,andD.J.Fleet,“Thelimitationsofdeep SAMPLE
learninginadversarialsettings.”
Inordertosimulatethedifferencebetweenidealadversarial
[6] B.D.Mittelstadt,C.Russell,andS.Wachter,“Explainingexplanations
in AI,” CoRR, vol. abs/1811.01439, 2018. [Online]. Available: mouse behavior data and real data collected from attackers in
http://arxiv.org/abs/1811.01439 the physical world, we use the result from [14]. We assume
[7] Z. C. Lipton, “The mythos of model interpretability: In machine
the relative position of the manually controlled cursor in x or
learning,theconceptofinterpretabilityisbothimportantandslippery.”
Queue,vol.16,no.3,pp.31–57,2018. y coordinate regarding to the target cursor follows a random
[8] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, Gaussian distribution, so the expected distance between man-
A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering
ually controlled cursor and target cursor in x or y coordinate
thegameofgowithouthumanknowledge,”nature,vol.550,no.7676,
pp.354–359,2017. equals to
[9] M. Du, N. Liu, and X. Hu, “Techniques for interpretable machine
learning,”CommunicationsoftheACM,vol.63,no.1,pp.68–77,2019. (cid:90) ∞ 1 −(cid:18) x2 (cid:19) (cid:114) 2
[10] P. Dabkowski and Y. Gal, “Real time image saliency for black box E(d)= |x|√
2πσ
e 2σp2 dx= πσ p ≈0.8σ p (1)
classifiers,”Advancesinneuralinformationprocessingsystems,vol.30, −∞ p
2017.
Therefore, the average distance between the manually con-
[11] J.Chen,L.Song,M.Wainwright,andM.Jordan,“Learningtoexplain:
Aninformation-theoreticperspectiveonmodelinterpretation,”inInter- trolled cursor and the target cursor in x or y coordinate of the
nationalConferenceonMachineLearning. PMLR,2018,pp.883–892. manipulated movement with length 160 equals to
[12] J. Yoon, J. Jordon, and M. van der Schaar, “Invase: Instance-wise
variable selection using neural networks,” in International Conference (cid:80)160 σ
onLearningRepresentations,2018. d=0.8σ
p
=0.8 i 1= 61
0
pi (2)
[13] W. Fu, M. Wang, M. Du, N. Liu, S. Hao, and X. Hu, “Differentiated
explanation of deep neural networks with skewed distributions,” IEEE In each point of the velocity sequence there is an applied
TransactionsonPatternAnalysisandMachineIntelligence,2021.
Gaussianperturbationtothegeneratedtargetvelocityvalue,so
[14] J.Mathew,F.R.Sarlegna,P.-M.Bernier,andF.R.Danion,“Handedness
mattersformotorcontrolbutnotforprediction,”eneuro,vol.6,no.3, theaccumulatedpositionerrorfollowsaGaussiandistribution
2019. with σ equals to:
[15] D.Qin,G.Amariucai,D.Qiao,Y.Guan,andS.Fu,“AComprehensive i
and Reliable Feature Attribution Method: Double-sided Remove and σ2 = (cid:88) σ2 (3)
Reconstruct(DoRaR),”arXive-prints,p.arXiv:2310.17945,Oct.2023. pi vn
[16] S.Fu,D.Qin,G.Amariucai,D.Qiao,Y.Guan,andA.Smiley,“Artificial n=1
intelligence meets kinesthetic intelligence: Mouse-based user authen- where σ is sigma of the accumulated Gaussian error of i-
tication based on hybrid human-machine learning,” in Proceedings of pi
the2022ACMonAsiaConferenceonComputerandCommunications th point in position domain, and σ vn is sigma of the applied
Security,2022,pp.1034–1048. Gaussian perturbation of n-th point in velocity domain.
[17] I.J.Goodfellow,J.Shlens,andC.Szegedy,“Explainingandharnessing AssumingweapplyaconstantGaussianperturbationwhich
adversarial examples,” in 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, follows N(0,σ2) to each point of velocity sequence in both x
Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2015. and y coordinate, s.t.
[Online].Available:http://arxiv.org/abs/1412.6572 √
[18] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: A σ = i·σ (4)
simple and accurate method to fool deep neural networks,” in 2016
pi
IEEEConferenceonComputerVisionandPatternRecognition(CVPR), Therefore, we have
2016,pp.2574–2582.
[19] aC n. dSz Reg .e Fd ey r, gW us. ,Z “a Ir ne tm rib ga u, inI. gS pu rt osk pe ev rte ir e, sJ. oB fru nn ea u, raD l. nE er th wa on r, kI s. ,G ”o Co od mfe pll uo tw er, d=0.8(cid:80)1 i=60 1σ
pi =
0.8 σ∗(cid:88)160√
i≈6.78σ (5)
Science,2013. 160 160
i=1From [14], we have the ratio of the average cursor speed
to the average distance between target cursor and manually
controlled cursor,
v
d= (6)
8
Therefore,
σ =0.0184v (7)
thestandarddeviationofappliedGaussiannoisetothevelocity
sequence can be calculated from the average value of that
sequence.
APPENDIXB
MODELSTRUCTUREOFFEATURESELECTOR,GENERATIVE
MODEL,BEHAVIORAUTHENTICATORANDADVERSARIAL
SAMPLEGENERATOR
Inourexperiments,forthefeatureselector,ithas12Dcon-
volutionallayerwithkernelsize2×9and21Dconvolutional
layers with kernel size 7 and 5. These 3 CNN layers have 64,
96and1filtersrespectively.FirsttwoCNNlayersarefollowed
by Max pooling layer with pool size 2 and Batch norm layer.
Then there is a LSTM network with 2 recurrent layers. After
that, there are two fully connected layers with 400 and 10
output units. 10 output scores correspond to 10 movement
of 1 mouse based authentication task. Relu is chosen as the
active function. Learning rate for feature selector is set to
λ=5e−4. α is set to the ratio of selected movements to all
movementswhichis10inourexperiment.β issettothevalue
that guarantees a close accuracy (≤ 95%) to the accuracy
achieved by basic feature selector (without adversarial sample
generator in the training loop) in the validation set, under this
requirement,theparametervaluethatachievesthehighesttrue
positive rate in adversarial samples is selected.
According to the sample size of the mouse dataset, the
generative model consists of 2 fully connected linear layers
both with 3200 units. Learning rate for generative models are
set to λ=1e−4.
The behavior authenticator model in this research has 3
CNN layers each with a dropout layer in the front. First and
secondCNNlayerarefollowedbyabatchnormlayer.3CNN
layershave64,96,128kernelswithkernelsize(2,9),7and5.
Afterthat,thereisLSTMnetworkwith128inputsize,hidden
size and 2 layers. At the end, there are two fully connected
layerswith128and2outputunits.Reluischosenastheactive
functionforCNNandfullyconnectedlayers.Learningratefor
behavior authenticator is set to λ=1e−4.
The adversarial sample generator model consists of 2 fully
connectedlayerseachwith320outputunits,soitcangenerate
velocitysequenceofonemovementwithsize2×160.Learning
rate for adversarial sample generator is set to λ=1e−4.
In the defensive distillation method, the temperature T is
set to 10 during training.