Rotting Infinitely Many-armed Bandits beyond the Worst-case
Rotting: An Adaptive Approach
Jung-hunKim MilanVojnovic´ Se-YoungYun
SeoulNationalUniversity LondonSchoolofEconomics KAIST
junghunkim@snu.ac.kr m.vojnovic@lse.ac.uk yunseyoung@kaist.ac.kr
Abstract
Inthisstudy,weconsidertheinfinitelymanyarmedbanditproblemsinrottingenvironments,wherethemean
rewardofanarmmaydecreasewitheachpull,whileotherwise,itremainsunchanged.Weexploretwoscenarios
capturingproblem-dependentcharacteristicsregardingthedecayofrewards:oneinwhichthecumulativeamount
ofrottingisboundedbyV ,referredtoastheslow-rottingscenario,andtheotherinwhichthenumberofrotting
T
instancesisboundedbyS ,referredtoastheabrupt-rottingscenario.Toaddressthechallengeposedbyrotting
T
rewards,weintroduceanalgorithmthatutilizesUCBwithanadaptiveslidingwindow,designedtomanagethe
biasandvariancetrade-offarisingduetorottingrewards.Ourproposedalgorithmachievestightregretboundsfor
bothslowandabruptrottingscenarios.Lastly,wedemonstratetheperformanceofouralgorithmsusingsynthetic
datasets.
1 Introduction
Weconsidermulti-armedbanditproblems,whicharefundamentalsequentiallearningproblemswhereanagent
playsanaction,referredtoasan‘arm,’ateachtimeandreceivesacorrespondingreward. Thecoreoftheproblems
isindealingwiththeexploration-exploitationtrade-off. Thesignificanceofbanditproblemsextendsacrossdiverse
real-worldapplications,suchasrecommendationsystems(Lietal.,2010)andclinicaltrials(Villaretal.,2015). In
arecommendationsystem,anarmcouldrepresentanitem,andthegoalistomaximizetheclick-throughrateby
makingeffectiverecommendations.
Real-worldobservationsrevealthatthemeanrewardsassociatedwitharmsmaydecreaseoverrepeatedinteractions.
Forinstance,incontentrecommendationsystems,theclickratesforeachaction(item)maydiminishduetouser
boredomwithrepeatedexposuretothesamecontent.Anotherexampleisevidentinclinicaltrials,wheretheefficacy
ofamedicationcandeclineovertimeduetodrugtoleranceinducedbyrepeatedadministration. Thedeclinein
meanrewardsassociatedwithselectedarms,referredtoas(rested)rottingbandits,hasbeenstudiedbyLevine
etal.(2017);Seznecetal.(2019,2020). ThepreviousworkfocusesonfiniteK arms,inwhichSeznecetal.(2019)
√
proposedalgorithmsachievingO˜( KT)regret. Thissuggeststhatrottingbanditswithafinitenumberofarms
arenoharderthanthestationarycase. However,inreal-worldscenarioslikerecommendationsystems,wherethe
contentitemssuchasmoviesorarticlesarenumerous,thepriormethodsencounterlimitationsastheparameterK
1
4202
rpA
22
]GL.sc[
1v20241.4042:viXraTable1: Summarizedresultsofourregretanalysis.
Regretupperbounds Regretupperbounds Regretlowerbounds
Type
forβ ≥1 for0<β <1 forβ >0
Slowrotting(V T) O˜(cid:18) max{V Tβ+1 2Tβ β+ +1 2,Tβ+β 1}(cid:19) O˜(cid:16) max{V T1 3T2 3,√ T}(cid:17) Ω(cid:18) max{V Tβ+1 2Tβ β+ +1 2,Tβ+β 1}(cid:19)
Abruptrotting(S T)
O˜(cid:18) max(cid:26)
S
Tβ+1 1Tβ+β
1,V
T(cid:27)(cid:19) O˜(cid:0) max(cid:8)√
S TT,V
T(cid:9)(cid:1) Ω(cid:18) max(cid:26)
S
Tβ+1 1Tβ+β
1,V
T(cid:27)(cid:19)
increases,resultinginatrivialregret. Thisemphasizesthenecessityforstudyingrottingscenarioswithinfinitely
manyarms,particularlywhenthereisalackofinformationaboutthefeaturesofeachitem. Theconsiderationof
infinitelymanyarmsforrestedrottingbanditsfundamentallydistinguishestheseproblemsfromthosewithafinite
numberofarms,whichwewillexplainlater.
Thestudyofmulti-armedbanditproblemswithaninfinitenumberofarmshasbeenextensivelyconductedinthe
contextofstationaryrewarddistributionsBerryetal.(1997);Wangetal.(2009);BonaldandProutière(2013);
CarpentierandValko(2015);Bayatietal.(2020). Initially,thedistributionofthemeanrewardsforthearmswas
assumedtobeuniformovertheinterval[0,1]Berryetal.(1997);BonaldandProutière(2013). Thisassumption
wasexpandedtoincludeamuchwiderrangeofdistributionswithβ >0satisfyingP(µ(a)>µ∗−x)=Θ(xβ),
whereµ(a)representsthemeanrewardofarma,µ∗isthemeanrewardofthebest-performingarm(Wangetal.,
2009;CarpentierandValko,2015;Bayatietal.,2020). WhileKimetal.(2022)explorestheconceptofdiminishing
rewardsinthecontextofbanditswithinfinitelymanyarms,theirfocusislimitedtothebasiccasewheretheinitial
meanrewardsareuniformlydistributed,andthedecayrateofmeanrewardperplayisboundedbyamaximum
decayrateρ(=o(1)). Crucially,theirfocusisontheregretfortheworst-caseratherthanproblem-dependentcases
withrespecttothedecayrates. Wealsonotethatfeatureinformationforeacharmisnotrequiredformulti-armed
bandit problems with infinitely many arms, which differs from linear bandits Abbasi-Yadkori et al. (2011) or
continuum-armedbanditsAueretal.(2007);Kleinberg(2004),wherefeatureinformationforeacharm,eitherfor
theLipschitzorlinearstructure,isinvolved.
Inthiswork,weexplorerottingbanditswithinfinitelymanyarms,subjecttomoderateconstraintsontheinitial
meanrewarddistributionandtherateatwhichthemeanrewardofanarmdeclines.Weadopttheassumptionthatthe
initialmeanrewarddistributionsatisfiestheaforementionedconditionwithparameterβ >0. Ourexaminationof
thediminishing,or‘rotting,’rewardscoverstwocases: onewiththetotalamountofrottingboundedbyV ,andthe
T
otherwiththetotalnumberofrottinginstancesboundedbyS . Thisallowsustocapturetheproblem-dependency
T
regardingrottingrates. SimilarquantificationshavebeenstudiedinthecontextofnonstationaryfiniteK-armed
banditproblemsBesbesetal.(2014);Aueretal.(2019);Russacetal.(2019),inwhichtherewarddistribution
changesovertimeindependentlyoftheagent. Adheringtoestablishedterminologyfornonstationarybandits,we
refertotheenvironmentforaboundedtotalamountofrottingwithV astheslowrottingscenarioandtheonefora
T
boundedtotalnumberofrottinginstanceswithS astheabruptrottingscenario.
T
Hereweprovidewhy(rested)rottingforinfinitelymanyarmsarefundamentallydifferentfromthatforfinitearms.
Inthecaseoffinitearms,restedrottingisknowntobenoharderthanstationarycase(Seznecetal.,2019,2020).
2Thisresultcomesfromtheconfinementofmeanrewardsofoptimalarmsandselectedarmswithinconfidence
bounds(Lemma1inSeznecetal.(2019,2020)). However,inthecaseofinfinitearms,whichallowsforhavingan
infinitenumberofnear-optimalarms,therealwaysexistnear-optimalarmsoutsideofexploredarms. Therefore,the
meanrewardgapmaynotbeconfinedwithinconfidencebounds. Thismakesoursettingfundamentallydifferentfor
finitearmedbandits,introducingadditionalchallenges.
Fortheinfinitearmsettings,thereexistsanadditionalcostforexploringnewarmstofindnear-optimalarmswhile
eliminatingexploredsuboptimalarms. Ifthetotalrottingeffectonexploredarmsissignificant,thenthefrequency
atwhichnewnear-optimalarmsmustbesoughtincreasessubstantially,resultinginalargeregret. Thisiswhythe
restedrottingsignificantlyaffectstheexplorationcostregardingV orS inoursetting.
T T
Tosolveourproblem,weintroducealgorithmsthatemployanadaptiveslidingwindowmechanism,effectively
managingthetradeoffbetweenbiasandvariancestemmingfromrottingrewards. Notably,ourpaperisthefirstto
considertheslowandabruptrottingscenarios,capturingproblem-dependentcharacteristics,specificallyforinitial
meanrewarddistributionsthatsatisfytheconditionparameterizedwithβ,inthecontextofinfinitelymanyarmed
bandits.
SummaryofourContributions. Thekeycontributionsofthisstudyaresummarizedinthefollowingpoints.
RefertoTable1forasummaryofourregretbounds.
• To address the slow and abrupt rotting scenarios capturing the problem-dependency of rotting rates, we
proposeaUCB-basedalgorithmusinganadaptiveslidingwindowandathresholdparameter. Thisalgorithm
allowsforeffectivelymanagingthebiasandvariancetrade-offarisingfromrottingrewards.
• InthecontextofbothslowrottingwithV andabruptrottingwithS ,foranyβ >0,wepresentregretupper
T T
boundsachievedbyouralgorithmwithanappropriatelytunedthresholdparameter.
• Weestablishregretlowerboundsforbothslowrottingandabruptrottingscenarios.Theseregretlowerbounds
implytightnessofourupperboundsforthecasewhenβ ≥1. Intheothercase,when0<β <1,thereisa
gapbetweenourupperboundsandthecorrespondinglowerbounds,similartowhatcanbefoundinrelated
literature,whichisdiscussedinthepaper.
• Lastly, we demonstrate the performance of our algorithm through experiments using synthetic datasets,
validatingourtheoreticalresults.
2 Problem Statement
Weconsiderrottingbanditswithinfinitelymanyarmswherethemeanrewardofanarmmaydecreasewhenthe
agentpullsthearm. LetAbethesetofinfinitelymanyarmsandletµ (a)denotetheunknownmeanrewardofarm
t
a∈Aattimet. Ateachtimet,anagentpullsarmaπ ∈Aaccordingtopolicyπandobservesstochasticrewardr
t t
givenbyr =µ (aπ)+η ,whereη isanoisetermfollowinga1-sub-Gaussiandistribution. Tosimplify,weuse
t t t t t
a foraπ whenthereisnoconfusionaboutthepolicy. Weassumethatinitialmeanrewards{µ (a)} arei.i.d.
t t 1 a∈A
randomvariableson[0,1],whichiswidelyconsideredinthecontextofinfinitelymany-armedbandits(Bonaldand
3Proutière,2013;Berryetal.,1997;Wangetal.,2009;CarpentierandValko,2015;Bayatietal.,2020;Kimetal.,
2022).
AsinWangetal.(2009);CarpentierandValko(2015);Bayatietal.(2020),weconsider,toourbestknowledge,the
mostgeneralizeddistributionoftheinitialmeanrewardµ (a),foreverya∈A,satisfyingthefollowingcondition:
1
thereexistsaconstantβ >0suchthatforeverya∈Aandallx∈[0,1],
P(µ (a)>1−x)=P(∆ (a)<x)=Θ(xβ), (1)
1 1
where∆ (a)=1−µ (a)istheinitialsub-optimalitygap.
1 1
Figure1: P(∆ (a)<x)=xβ fordifferentvaluesofβ.
1
Todiscusstheeffectofβ onthedistributionof∆ (a)andtheprobabilityofsamplingagoodarm(havingsmall
1
∆ (a)),weconsiderthecasewhenP(∆ (a) < x) = xβ,whichisshowninFigure1forsomevaluesofβ. Itis
1 1
noteworthythattheuniformdistributionisaspecialcasewhenβ =1. Importantly,thelargerthevalueofβ,the
smallertheprobabilityofsamplingagoodarm.
TherottingofarmsisdefinedinasimilarmannertoKimetal.(2022);Levineetal.(2017);Seznecetal.(2019,
2020). Themeanrewardoftheplayedarma attimet>0undergoesthefollowingchange:
t
µ (a )=µ (a )−ϱ (a ),
t+1 t t t t t
whererottingrateϱ (a )≥0isarbitrarilydeterminedafterpullinga anditisunknowntotheagent. Themean
t t t
rewards of other arms remain unchanged as µ (a) = µ (a) with ρ (a) = 0 for a ∈ A/{a }, which are also
t+1 t t t
determinedafterpullinga . Wecanexpressµ (a)=µ
(a)−Pt−1ϱ
(a),allowingµ (a)totakenegativevalues.
t t 1 s=1 s t
Forsimplicity,weuseρ forρ (a )whenthereisnoconfusion. Weconsidertwodistinctmeasuresforquantifying
t t t
theamountofrotting;thetotalcumulativerottingoverT isboundedbyV
asPT−1ρ
≤ V ,andthenumber
T t=1 t T
ofrottinginstances(plusone)isboundedbyS
as1+PT−11(ρ
̸= 0) ≤ S (≤ T). WhilewehaveS ≤ T
T t=1 t T T
becausethenumberofrottinginstancesisatmostT −1,theupperboundforV maynotexistduetothelackof
T
constraintsonρ ’s. WewillshortlyintroduceanddiscussanassumptionforV .
t T
ThegoalofthisproblemistofindapolicythatminimizestheexpectedcumulativeregretoveratimehorizonofT
timesteps,which,foragivenpolicyπ,isdefinedasE[Rπ(T)]=E[PT
(1−µ (a ))].Theuseof1intheregret
t=1 t t
definitionfortheoptimalmeanrewardisjustifiedbecausethereexistsanarmwhosemeanrewardissufficiently
closeto1amongtheinfinitearmswithinitialmeanrewardsfollowingthedistributionspecifiedin(1).1
1 Foranyϵ>0,thereexistsa∈As.t.∆1(a)<ϵwithprobability1becauselimn→∞1−P(∆1(a)≥ϵ)n=1.
4Herewediscussanassumptionforthecumulativeamountofrotting(V ). InthecaseofV > T,theproblem
T T
becomestrivialasshowninthefollowingproposition.
Proposition2.1. InthecaseofV >T,asimplepolicythatsamplesanewarmeveryroundachievestheoptimal
T
regretofΘ(T).
Proof. TheproofisprovidedinAppendixA.1
Fromtheaboveproposition,whenV >T,theregretlowerboundofthisproblemisΩ(T),whichcanbeachieved
T
fromasimplepolicy. Therefore,weconsiderthefollowingassumptionfortheregionofnon-trivialproblems.
Assumption2.2. V ≤T.
T
Theaboveassumptionisnotstrong,asitfrequentlyarisesinreal-worldscenariosandismoregeneralthanthe
assumptionmadeinpriorwork,asdescribedinthefollowingremarks.
Remark 2.3. The assumption V ≤ T is satisfied when mean rewards consistently remain positive, ensuring
T
0≤µ (a )≤1forallt∈[T],becausethisconditionimpliesρ ≤1. Suchascenarioisfrequentlyencounteredin
t t t
real-worldapplications,whererewardisrepresentedbymetricslikeclickratesorratingsincontentrecommendation
systems.
Remark2.4. OurrottingscenariowithV ≤T ismoregeneralinscopethantheonewithamaximumrottingrate
T
constraintwhereρ ≤ρ=o(1)forallt∈[T−1],whichwasexploredinKimetal.(2022). Thisisbecauseforour
t
setting,ρ isnotnecessarilyboundedbyo(1),andforthemaximumrottingconstraintsettingwithρ ≤ρ=o(1),
t t
V ≤ T isalwayssatisfied. Additionally, ourworkdelvesintotherealmofageneralizedinitial meanreward
T
distributionwithβ >0,offeringabroaderperspectivethantheuniformdistributioncase(β =1)consideredin
Kimetal.(2022).
3 Algorithms and Regret Analysis
PreviousWork. Beforemovingontoouralgorithm,wereviewthepreviouslysuggestedalgorithm,UCB-TP
(Kimetal.,2022),whichisdesignedunderthecaseofmaximumrottingrateconstraintasρ ≤ρforallt>0. The
t
meanestimatorinUCB-TPconsiderstheworst-casescenariowiththemaximumrottingrateρasµ˜o(a)−ρn (a)
t t
whereµ˜o isanestimatorfortheinitialmeanrewardandn (a)isthenumberofpullingarmauntilt−1,which
t t
√
leadstoachieveO˜(max{ρ1/3T, T}). Theestimatorisnotappropriatetodealwithρ delicatelybecauseitaims
t
toattaintheworst-caseregretboundregardingrottingrates.
Therefore,weproposeusinganadaptiveslidingwindowfordelicatelycontrollingbiasandvariancetradeoffof
the mean reward estimator. This is why our algorithm can adapt to varying rotting rates ρ and achieve tight
t
regretboundswithrespecttoV orevenS . Furthermore,ouralgorithmaccommodatesthegeneralmeanreward
T T
distributionwithβ byemployingacarefullyoptimizedthresholdparameter.
Here we describe our proposed algorithm (Algorithm 1) in detail. We define µ (a) = Pt2 r 1(a =
b[t1,t2] t=t1 t t
a)/n (a)wheren (a) = Pt2 1(a = a)fort ≤ t . Thenforwindow-UCBindexofthealgorithm,
[t1,t2] [t1,t2] t=t1 t 1 2
5Figure2: Illustrationsfortheadaptiveslidingwindowusedbyouralgorithm: (left)theeffectoftheslidingwindow
lengthonthemeanrewardestimation,(right)slidingwindowcandidateswithdoublinglengths.
p
wedefineWUCB(a,t ,t ,T)=µ (a)+ 12log(T)/n (a).InAlgorithm1,wefirstselectanarbitrary
1 2 b[t1,t2] [t1,t2]
arma∈A′withoutpriorknowledgeregardingthearmsinA′,denotingthecorrespondingtimeast(a). Wedefine
T (a)asthesetofstartingtimesforslidingwindowsofdoublinglengths,definedasT (a) = {s ∈ [T] : t(a) ≤
t t
s≤t−1ands=t−2i−1forsomei∈N}. Thenthealgorithmpullsthearmconsecutivelyuntilthefollowing
thresholdconditionissatisfied:
min WUCB(a,s,t−1,T)<1−δ,
s∈Tt(a)
inwhichtheslidingwindowhavingminimizedwindow-UCBisutilizedforadaptingnonstationarity.Ifthethreshold
conditionholds,thenthealgorithmconsidersthearmtobeasub-optimal(bad)armandwithdrawsthearm. Thenit
samplesanewarmandrepeatsthisprocedure.
Algorithm1UCB-ThresholdwithAdaptiveSlidingWindow
Given: T,δ,A;Initialize: A′ ←A
Sampleanarma∈A′
Pullarmaandgetrewardr
1
t(a)←1
fort=2,...,T do
if min WUCB(a,s,t−1,T)<1−δthen
s∈Tt(a)
A′ ←A′/{a}
Selectanarma∈A′
Pullarmaandgetrewardr
t
t(a)←t
else
Pullarmaandgetrewardr
t
endif
endfor
UtilizingtheadaptiveslidingwindowhavingminimizedwindowUCBindexenhancesthealgorithm’sabilityto
6dynamicallyidentifypoorly-performingarmsacrossvaryingrottingrates. Thisadaptabilityisachievedbymanaging
thetradeoffbetweenbiasandvariance. TheconceptisdepictedinFigure2(left),whereanarmaisrottedseveral
times, and WUCB with a smaller window exhibits minimal bias with the arm’s most recent mean reward but
introduceshighervariance. Conversely,WUCBwithalargerwindowdisplaysincreasedbiasbutreducedvariance.
Inthisvisualrepresentation,thevalueofWUCBwithasmallwindowattainsaminimum,enablingthealgorithmto
comparethisvaluewith1−δtoidentifythesuboptimalarm. Moreover,asillustratedinFigure2(right),bytaking
intoaccounttheconstraintofs=t−2i−1forthesizeoftheadaptivewindows,wecanreducethecomputationtime
fordeterminingtheappropriatewindowfromO(T2)toO(T logT)acrossT timesteps,andreducetherequired
memoryfromO(t)toO(logt)foreachtimet.
SlowRottingwithV . HereweconsiderslowlyrottingrewarddistributionwithgivenV (≤T). Weanalyze
T T
theregretofAlgorithm1withtunedδ usingβ andV . Wedefineδ (β) = max{(V /T)1/(β+2),1/T1/(β+1)}
T V T
√
whenβ ≥1andδ (β)=max{(V /T)1/3,1/ T}when0<β <1. Thealgorithmwithδ (β)achievesaregret
V T V
boundinthefollowingtheorem.
Theorem3.1. ThepolicyπofAlgorithm1withδ =δ (β)achieves:
V
 O˜(max{V β+1 2Tβ β+ +1 2,Tβ+β 1}) for β ≥1,
E[Rπ(T)]= T
√
O˜(max{V T31 T2 3, T}) for 0<β <1.
Weobservethatwhenβ increasesabove1,theregretboundbecomesworsebecausethelikelihoodofsamplinga
goodarmdecreases. However,whenβ decreasesbelow1,theregretboundremainsthesameduetotheinability
toavoidacertainlevelofregretarisingfromestimatingthemeanreward. Furtherdiscussionwillbeprovided
√
later. Also,weobservethatwhenV =O(max{1/T1/(β+1),1/ T})wheretheproblembecomesnear-stationary,
T
√
the regret bound in Theorem 3.1 matches the previously known regret bound O˜(max{Tβ/(β+1), T}) for the
stationaryinfinitelymany-armedbanditsinWangetal.(2009);Bayatietal.(2020).
Proofsketch. ThefullproofisprovidedinAppendixA.2. Hereweoutlinethemainideasoftheproof. Themain
difficultiesintheproofcomefromanalyzingregretdelicatelyforvaryingρ overtimehorizonT withanadaptive
t
slidingwindowandforthegeneralizedinitialmeanrewarddistributionwithparameterβ,whichdonotappearin
Kimetal.(2022).
Weinitiallyseparatetheregretintotwocomponents: oneassociatedwithpullinginitiallygoodarmsandanother
withpullinginitiallybadarms. Thegoodarmsindicatearmsawhichsatisfyinitialmeanrewardµ (a)≥1−2δ
1
andotherwise,thearmsarerecognizedasbadarms. Thereasonwhytheseparationisrequiredisthatouradaptive
algorithm exhibits distinct behaviors depending on the category of arms. In short, good arms may be pulled
continuouslybyouralgorithmwhenrottingratesaresufficientlysmallbutbadarmsarenot. Thisseparationfor
regretisdenotedas
Rπ(T)=RG(T)+RB(T),
whereRG(T)isregretfromgoodarmsandRB(T)isregretfrombadarms.
7WefirstprovideaboundforE[RG(T)]. Foranalyzingregretfromgoodarms,weanalyzethecumulativeamount
ofrottingwhilepullingasampledgoodarmbeforewithdrawingthearmbythealgorithm. LetAG beasetof
T
good arms sampled until T, t (a) be the initial time step at which arm a is pulled, and t (a) be the final time
1 2
stepatwhichthearmispulledbythealgorithmsothatthethresholdconditionholdswhent = t (a)+1. For
2
simplicity,weuset andt fort (a)andt (a),whenthereisnoconfusion. Foranytimestepsn≤m,wedefine
1 2 1 2
V
(a)=Pm
ρ (a)andρ (a)=V (a)/n (a). Weshowthattheregretisdecomposedas
[n,m] t=n t [n,m] [n,m] [n,m]
X
Xt2 !
RG(T)= ∆ (a)n (a)+ V (a) , (2)
1 [t1,t2] [t1,t−1]
a∈AG t=t1+1
T
whichconsistsofregretfromtheinitialmeanrewardandthecumulativeamountofrottingforeacharm. Forthefirst
termofP ∆ (a)n (a)in(2),since∆ (a)=O(δ)fromthedefinitionofgoodarmsa∈AG,wehave
a∈AG
T
1 [t1,t2] 1 T
 
X
E ∆ 1(a)n [t1,t2](a)=O(δT). (3)
a∈AG
T
ThemaindifficultyliesindealingwiththesecondtermofP Pt2
V (a)in(2),whereweneedto
a∈AG
T
t=t1+1 [t1,t−1]
analyzetheamountofcumulativerottinguntilthearmiseliminatedfromtheadaptivethresholdcondition. Let
w(a)=⌈log(T)1/3/ρ (a)2/3⌉,whichdenotestheoptimizedwindowsizetocontrolvariance-biastradeoff.
[t1,t2−2]
WedefineanarmsetAG = {a ∈ AG;n (a) > w(a)}whichincludesarmspulledmorethanw(a)times.
T T [t1,t2−1]
Thenweanalyzethesecondtermin(2)byseparatingitas
X
Xt2
X
Xt2
X
Xt2
V (a)= V (a)+ V (a). (4)
[t1,t−1] [t1,t−1] [t1,t−1]
a∈AG
T
t=t1+1 a∈AG T/AG
T
t=t1+1 a∈AG
T
t=t1+1
Fromtherestrictednumberofpullsofw(a)timesforeacharma∈AG/AG
,byomittingdetailshere,weshowthat
T T
 
X
Xt2
V [t1,t−1](a)=O˜ V
T
+ X V [t1,t2−2](a)1 3n [t1,t2−2](a)32 , (5)
a∈AG/AGt=t1+1 a∈AG/AG
T T T T
wherethefirsttermcomesfromconsideringtheworstcaseofrotting.
G
RegardingthearmsinA ,whereeacharmisplayedsufficientlytohavetheoptimizedwindow,acarefulanalysis
T
oftheadaptivethresholdpolicyisrequiredtolimitthetotalvariationinrotting. Byexaminingtheestimationerrors
arisingfromvarianceandbiasduetothethresholdcondition,wecanestablishanupperboundforthecumulative
amountofrotting,givenby:
 
X
Xt2
V [t1,t−1](a)=O˜ Tδ+V
T
+ X V [t1,t2−2](a)1 3n [t1,t2−2](a)2 3 , (6)
a∈AGt=t1+1 a∈AG
T T
wherethefirsttermcomesfromthethresholdparameterfordeterminingwhentowithdrawarms. Therefore,from
δ =δ (β),V ≤T,andequations(2)-(6),usingHölder’sinequality,wehave
V T
 O˜(max{V β+1 2Tββ ++ 21 ,Tβ+β 1}) for β ≥1,
E[RG(T)]= T √ (7)
O˜(max{V T31 T2 3, T}) for 0<β <1.
8Next,weprovideaboundforE[RB(T)]. Weemployepisodicregretanalysis,defininganepisodeasthetimesteps
betweenconsecutivelysampleddistinctgoodarmsbythealgorithm. Byanalyzingbadarmswithineachepisode,we
canderiveanupperboundfortheoverallregretstemmingfrombadarms. Wedefinetheregretfrombadarmsover
√
mG episodesasRB . WefirstconsiderthecaseofV = ω(max{1/ T,1/T1/(β+1)}). Inthiscase,bysetting
mG T
mG =⌈2V /δ⌉,wecanshowthatRB(T)≤RB . ByanalyzingRB withtheepisodicanalysis,wecanshowthat
T mG mG
(cid:18) (cid:26) (cid:27)(cid:19)
E[RB(T)]≤E[R mB G]=O˜ max Tβ β+ +1 2V Tβ+1 2,T32V T1 3 . (8)
√
As in the similar manner, when V = O(max{1/ T,1/T1/(β+1)}), by setting mG = C for some constant
T 3
C >0,wecanshowthat
3
(cid:16) n √ o(cid:17)
E[RB(T)]≤E[R mB G]=O˜ max Tβ+β 1, T . (9)
From(8)and(9),wehave
 O˜(max{V β+1 2Tββ ++ 21 ,Tβ+β 1}) for β ≥1,
E[RB(T)]= T √ (10)
O˜(max{V T31 T32, T}) for 0<β <1.
Finally,using(7)and(10),wecanconcludetheprooffromE[Rπ(T)]=E[RG(T)]+E[RB(T)].
Remark 3.2. We compare our result in Theorem 3.1 with that in Kim et al. (2022) which, recall, is under the
maximum rotting constraint ρ ≤ ρ = o(1) for all t and uniform distribution of initial mean rewards (β = 1).
t
Underthemaximumrottingconstraint,wehaveV ≤ρT. Thenwithβ =1,wecanobservethattheregretbound
T
ofAlgorithm1istighterthanthatofUCB-TP(Kimetal.,2022)as
O˜(cid:16) maxn V 1 3T2 3,√ To(cid:17) ≤O˜(cid:16) maxn ρ1 3T,√ To(cid:17) , (11)
T
wheretherighttermistheregretboundofUCB-TP.Hereweprovideanexampletoclarifytheaboveinequality.
Weconsideracaseofρ =1/(tlog(T))foralltsothatρ=ρ =1/log(T)andV
=PT−1ρ
=O(1). Inthis
t √ 1 T t=1 t
case,theregretboundofAlgorithm1,O˜(max{V1/3T2/3, T})=O˜(T2/3),issublinearwhilethatofUCB-TP,
√ T
O˜(max{ρ1/3T, T}) = O˜(T/log(T)), is nearly linear. This is because UCB-TP is highly contingent on the
maximumrottingrateratherthanthecumulativerottingrates. Wewilldemonstratethisfurtherinournumerical
results.
Abrupt Rotting with S . Here we consider abruptly rotting reward distribution with given S (≤ T). We
T T
consider Algorithm 1 with δ newly tuned by S and β. We define δ (β) = (S /T)1/(β+1) for β ≥ 1 and
T S T
δ (β) = (S /T)1/2 for0 < β ≤ 1. Theninthefollowingtheorem,weanalyzetheregretofAlgorithm1with
S T
δ (β).
S
Theorem3.3. ThepolicyπofAlgorithm1withδ =δ (β)achieves:
S
E[Rπ(T)]= O˜(max{S Tβ+1 1Tβ+β 1,V T}) for β ≥1,
√
O˜(max{ S T,V }) for 0<β <1.
T T
9Asintheslowrottingcase,fortheabruptrottingscenario,weobservethatwhenβ increasesabove1,theregret
boundworsensasthelikelihoodofsamplingagoodarmdecreases. However,whenβ decreasesbelow1,theregret
boundremainsthesamebecausewecannotavoidacertainlevelofregretarisingfromestimatingthemeanreward.
Also,weobservethatforthecaseofS =1,wheretheproblembecomesstationary(implyingV =0),theregret
T T
√
boundinTheorem3.1matchesthepreviouslyknownregretboundofO˜(max{Tβ/(β+1), T})forthestationary
infinitelymany-armedbanditsWangetal.(2009);Bayatietal.(2020). Additionally,weobservethattheregret
boundislinearlyboundedbyV ,whichisattributedtothealgorithm’snecessitytopullarottedarmatleastonce
T
todetermineitsstatusasbad. Later,intheanalysisofregretlowerbounds,wewillestablishtheimpossibilityof
avoidingV regretintheworst-case.
T
Proofsketch. ThefullproofisprovidedinAppendixA.3. Hereweprovideaproofoutline. Wefollowtheproof
frameworkofTheorem3.1butthemaindifferenceliesincarefullydealingwithsubstantiallyrottedarms. For
theeaseofpresentation,weconsidereacharmthatexperiencesabruptrottingasifitwerenewlysampledbythe
algorithm,treatingthearmbeforeandafterabruptrottingasdistinctarms. Thedefinitionofagoodarmandabad
armisbasedonthemeanrewardatthetimewhenitisnewlysampled. Thenwedividetheregretintoregretfrom
goodandbadarmsasRπ(T)=RG(T)+RB(T). Fromthedefinitionofgoodarms,wecaneasilyshowthat
 O˜(Sβ+1 1Tβ+β
1) for β ≥1,
E[RG(T)]=O(δ (β)T)= T
S √
O˜( S T) for 0<β <1.
T
FordealingwithRB(T),wepartitiontheregretintotwoscenarios: onewherethebadarmisinitiallybadsampled
fromthedistributionof(1)andanotherwhereitbecomesbadafterrotting. Thiscanbeexpressedas
RB(T)=RB,1(T)+RB,2(T).
Thenfortheformerregret,RB,1(T),asintheproofofTheorem3.1,byusingtheepisodicanalysiswithmG =S ,
T
wecanshowthat
 O˜(Sβ+1 1Tβ+β
1) for β ≥1,
E[RB,1(T)]≤E[RB ]= T
mG √
O˜( S T) for 0<β <1.
T
Fortheregretfromrottedbadarms,RB,2(T),itiscriticaltoanalyzesignificantrottinginstancestoobtainatight
boundwithrespecttoS . Weanalyzethatwhenthereexistssignificantrotting,thenthealgorithmcandetectitasa
T
badarmandeliminateitbypullingitatonce. Fromthisanalysis,wecanshowthat
E[RB,2(T)]= O˜(max{S Tβ+β 1Tβ+1 1,V T}) for β ≥1,
√
O˜(max{ S T,V }) for 0<β <1.
T T
Putting all the results together with E[Rπ(T)] = E[RG(T)]+E[RB,1(T)]+E[RB,2(T)] and S ≤ T, we can
T
concludetheproof.
Remarkably,ourproposedmethodutilizinganadaptiveslidingwindowyieldsatightbound(lowerboundswillbe
presentedlater)notonlyforslowrottingbutalsoforabruptrottingscenarioscharacterizedbyalimitednumberof
10Figure3: Adaptiveslidingwindowforabruptrotting.
rottinginstances. Therationalebehindtheeffectivenessoftheadaptiveslidingwindowincontrollingthebiasand
variancetradeoffwithrespecttoabruptrottingisasfollows. Itcanbeobservedthattheadaptivethresholdcondition
ofmin WUCB(a,s,t−1,T)<1−δisequivalenttotheconditionofWUCB(a,s,t−1,T)<1−δfor
s∈Tt(a)
somessuchthatt (a)≤s≤t−1(ignoringthecomputationalreductiontrick). Thelatterexpressionrepresents
1
the threshold condition tested for every time step before t, encompassing the time step immediately following
an abrupt rotting event. Consequently, as illustrated in Figure 3, this adaptive threshold condition can identify
substantiallyrottedarmsbymitigatingbiasandvarianceusingthewindowstartingfromthetimestepfollowingthe
occurrenceofrotting.
In cases where abrupt rotting occasionally occurs with S = O(min{V(β+1)/(β+2)T1/(β+2),V2/3T1/3}), we
T T T
canobservethatAlgorithm1withδ (β)achievesatighterregretboundcomparedtotheonewithδ (β)from
S V
Theorems3.1and3.3asfollows. Forsimplicity,whenβ =1,wehave
O˜(cid:16) maxnp S TT,V To(cid:17) ≤O˜(cid:16) maxn V T1 3T2 3,√ To(cid:17) . (12)
Wewilldemonstratethislaterfromournumericalresults.
Rotting with V and S . In what follows, we study the case of rotting with given both V and S . Then
T T T T
Algorithm1withδ =min{δ (β),δ (β)}canachieveatighterregretboundasnotedinthefollowingcorollary,
V S
whichcanbeobtainedfromTheorems3.1and3.3.
Corollary3.4. LetR andR bedefinedas
V S
 1 β+1 β  1 β
max{V Tβ+2Tβ+2,Tβ+1} for β ≥1, max{S Tβ+1Tβ+1,V T} for β ≥1,
R V := √ andR S := √
max{V1/3T2/3, T} for 0<β <1 max{ S T,V } for 0<β <1.
T T T
The policy π of Algorithm 1 with δ = min{δ (β),δ (β)} achieves the following regret bound: E[Rπ(T)] =
V S
O˜(min{R ,R }).
V S
CasewithoutPriorKnowledgeofV ,S ,andβ. Herewestudythecasewhenthealgorithmdoesnothave
T T
priorinformationaboutthevaluesofV ,S ,andβ. Theseparametersplayacrucialroleindeterminingtheoptimal
T T
thresholdparameterδinAlgorithm1. Incaseswheretheseparametervaluesareunknown,anadditionalstepis
necessarytoestimatetheoptimalδ. Toaddressthis,weemploytheBandit-over-Bandit(BoB)approachCheung
11etal.(2019). ThisapproachinvolvesamasterandseveralbaseswithatimeblocksizeofH,inwhicheachbase
algorithmrepresentsAlgorithm1withacandidatevalueofδ,andtheroleofthemasteristofindthebestbase
havingnearoptimalδ. Furtherdetailsofthealgorithm(Algorithm2)canbefoundinAppendixA.4.
√
With a time block size of H (where H = ⌈ T⌉), the algorithm operates over ⌈T/H⌉ blocks. The regret is
composedoftwofactorsfromthemasterandbases. LetV representthecumulativeamountofrottinginthetime
H,i
stepswithinthei-thblock. ItispossibletoencounterV =V forsomei,potentiallyresultinginanarm’smean
H,i T
rewardhavingasignificantlylownegativevalue,leadingtosuboptimalbehaviorbythemaster. Toaddressthis,we
introducetheassumptionofequallydistributedcumulativerottingV ’s,statedasfollows:
H,i
Assumption3.5. V ≤H foralli∈[⌈T/H⌉]
H,i
Remark 3.6. As highlighted in Remark 2.3, this assumption is satisfied when mean rewards consistently stay
positive,guaranteeing0≤µ (a )≤1forallt∈[T],whichisfrequentlyencounteredinreal-worldapplications.
t t
Additionally,itisnoteworthythatAssumption3.5impliesAssumption2.2.
We require additional assumptions for the regret from the bases. Regret regarding V or S differs from each
T T
basebecausethebehaviorofeachbasepolicyvarieswithdifferentcandidatevaluesofδ,andtherottingratesare
arbitrarilydetermined. ToensurethattheregretboundconcerningV andS remainsguaranteedirrespectiveof
T T
thebaseschosen,weconsidereitherofthefollowingassumptions.
Assumption3.7. Rottingratesρ forallt>0aredeterminedfromanobliviousadversaryregardlessoftheactions.
t
Inotherwords,{ρ } aredeterminedarbitrarilybeforestartingthegameandeachρ isindependentofthe
t t∈[T] t
selectedarma .
t
Assumption3.8. LetΠbethesetofallfeasiblepolicies. ThenweconsiderthatV andS satisfiestheworst-case
T T
policyboundsregardingrottingratessuchthatmax P ρ (aπ)≤V and
π∈Π t∈[T−1] t t T
1+max P 1(ρ (aπ)̸=0)≤S ,respectively.
π∈Π t∈[T−1] t t T
WeprovidearegretboundofAlgorithm2underAssumption3.5andeitherofAssumption3.7orAssumption3.8in
thefollowing.
Theorem3.9. LetR′ andR′ bedefinedas
V S
 1 β+1 2β+1  1 β 2β+1
R′
:=V Tβ+2Tβ+2 +T2β+2 for β ≥1,
andR′
:=max{S Tβ+1Tβ+1 +T2β+2,V T} for β ≥1,
V V T1 3T2 3 +T3 4 for 0<β <1 S max{√ S TT +T43,V T} for 0<β <1.
√
Then,thepolicyπofAlgorithm2withH =⌈ T⌉achievesthefollowingregretbound:
E[Rπ(T)]=O˜(min{R′ ,R′ })
V S
Proof. TheproofisprovidedinAppendixA.5.
WecanobservethatthereistheadditionalregretcostofT(2β+1)/(2β+2)forβ ≥1orT3/4for0<β <1compared
toAlgorithm1. Thisadditionalcostoriginatesfromtheadditionalproceduretolearntheoptimalvalueofδ in
121e6 1e6
1.2
SSUCB Algorithm 1 with V
3 UCB-TP 1.0 Algorithm 1 with S
Algorithm 2 0.8
2 Algorithm 1 with V
0.6
0.4
1
0.2
0 0.0
0 1 2 3 4 5 0 1 2 3 4 5
T 1e6 T 1e6
Figure4: Performanceofalgorithmsdemonstrating(left)Eq.(11)and(right)Eq.(12).
Algorithm2,whichisnegligiblewhenV andS arelargeenough. Wealsonotethatthewell-knownblack-box
T T
frameworkproposedforaddressingnonstationarity(WeiandLuo,2021)isnotapplicabletothisproblembecause
theUCBofthechosenarminthiscontextdoesnotconsistentlysurpasstheoptimalmeanreward,violatingthe
necessaryassumptionfortheframework. Attainingtheoptimalregretboundunderaparameter-freealgorithm
remainsanunresolvedissue.
4 Regret Lower Bounds
Inthissection,weanalyzeregretlowerboundsofourproblemunderAssumption2.2toprovideguidanceonthe
tightnessofourachievedupperbounds. First,weprovidearegretlowerboundforslowrottingwithV inthe
T
following.
Theorem4.1. GivenV ≤T andβ >0,foranypolicyπ,therealwaysexistρ ≥0fort∈[T −1]suchthatthe
T t
regretofπsatisfies
(cid:18) (cid:26) (cid:27)(cid:19)
E[Rπ(T)]=Ω max V β+1 2Tββ ++ 21 ,Tβ+β 1 .
T
Proof. TheproofisprovidedinAppendixA.6.
Next,weprovidearegretlowerboundforabruptrottingwithS inthefollowing.
T
Theorem4.2. GivenS andβ >0,foranypolicyπ,therealwaysexistρ ≥0fort∈[T −1]withV ≤T such
T t T
thattheregretofπsatisfies
(cid:18) (cid:26) (cid:27)(cid:19)
E[Rπ(T)]=Ω max S Tβ+1 1Tβ+β 1,V
T
.
Proof. TheproofisprovidedinAppendixA.7.
WenotethatevenforabruptrottingwithS ,itisunavoidabletoincurΩ(V )regretintheworstcasebecausean
T T
armmayberottedV amountallatonceandanalgorithmpullsthisrottedarmatleastonce.
T
13
])T(
R[E
])T(
R[EDiscussiononOptimalRegret. AswesummarizeourresultsinTable1,Algorithm1achievesnear-optimal
regretonlywhenβ ≥ 1. Here,wediscussthediscrepanciesbetweenlowerandupperboundswhen0 < β < 1.
From(1),wecanobservethatasβ decreasesbelow1,theprobabilitytosamplegoodarmsmayincrease,which
appearstobebeneficialwithrespecttoregret. However,theregretupperboundsfor0<β <1inTheorems3.1
and3.3remainthesameasthecasewhenβ =1whiletheregretlowerboundsinTheorems4.1and 4.2decrease
asβdecreases,resultinginagapbetweentheregretupperandlowerbounds. Thephenomenonthattheregretupper
boundremainsthesamewhenβ decreaseshasalsobeenobservedinpreviousliteratureoninfinitelymany-armed
bandits(CarpentierandValko,2015;Bayatietal.,2020;Wangetal.,2009). AsmentionedinCarpentierandValko
(2015),althoughtherearelikelytobemanygoodarmswhenβ issmall,itisnotpossibletoavoidacertainamount
ofregretfromestimatingmeanrewardstodistinguisharmsundersub-Gaussianrewardnoise. Therefore,webelieve
thatourregretupperboundsarenear-optimalacrosstheentirerangeofβ,andachievingtighterregretlowerbounds
whenβ <1isleftforfutureresearch.
5 Experiments
Inthissection,weprovideempiricalvalidationofourtheoreticalresultsusingsyntheticdatasetsunderuniform
distributionforinitialmeanrewards(β =1).
Wefirstcomparetheperformanceofouralgorithmswithpreviouslyproposedonestodemonstrate(11). Wenote
thatUCB-TPKimetal.(2022)isthestate-of-the-artalgorithmfortherottingsettingandSSUCBBayatietal.(2020)
isknowntoachievenear-optimalregretinstationaryinfinitelymanyarmedbandits. Wesetρ = 1/(tlog(T))
t
foralltsothatρ = ρ = 1/log(T) = o(1)andV =
PT−1ρ
= O(1). InFigure4(left),wecanobservethat
1 T t=1 t
Algorithms1and2performbetterthanUCB-TPandSSUCB(andAlgorithm1outperformsAlgorithm2),which
isinagreementwiththeinsightsfromourregretanalysisforthecaseofβ =1. Inthiscase,theregretboundsof
Algorithms1and2areO˜(T2/3)andO˜(T3/4)fromTheorems3.1and 3.9,respectively,whicharetighterthanthe
regretboundofUCB-TP,O˜(T/log(T)1/3).
Nowwecomparetheperformanceofouralgorithmstodemonstrate(12). Lett(s)bethetimestepwhenthes-th
abruptrottingoccurs. Thenforthecaseofabruptlyrottingrewards,wesetS = T1/4 andρ = T1/2 forall
T t(s)
s ∈ [S ], which implies V = PST ρ = T3/4. We consider that the S abrupt rotting events are equally
T T s=1 t(s) T
distributedoverT. InFigure4(right),wecanobservethatAlgorithm1withδ (β)hasbetterperformancethan
S
Algorithm1withδ (β),whichobservationisconsistentwiththeinsightfromourregretanalysisforthecaseβ =1,
V
inwhichtheregretboundofAlgorithm1withδ (β),O˜(T3/4),istighterthanthatofonewithδ (β),O˜(T11/12).
S V
6 Conclusion
Weexplorethechallengesposedbyinfinitelymany-armedbanditproblemswithrottingrewards,examiningslow
rotting(V )andabruptrotting(S ),underthegeneralizedinitialmeanrewarddistributionwithβ >0. Toaddress
T T
thechallenges,weproposeanalgorithmincorporatinganadaptiveslidingwindow,whichachievestightregret
bounds.
14References
Abbasi-Yadkori,Y.,Pál,D.,andSzepesvári,C.(2011). Improvedalgorithmsforlinearstochasticbandits. Advances
inneuralinformationprocessingsystems,24.
Auer,P.,Cesa-Bianchi,N.,Freund,Y.,andSchapire,R.E.(2002). Thenonstochasticmultiarmedbanditproblem.
SIAMJournalonComputing,32(1):48–77.
Auer,P.,Gajane,P.,andOrtner,R.(2019). Adaptivelytrackingthebestbanditarmwithanunknownnumberof
distributionchanges. InConferenceonLearningTheory,pages138–158.PMLR.
Auer,P.,Ortner,R.,andSzepesvári,C.(2007). Improvedratesforthestochasticcontinuum-armedbanditproblem.
InInternationalConferenceonComputationalLearningTheory,pages454–468.Springer.
Bayati,M.,Hamidi,N.,Johari,R.,andKhosravi,K.(2020). Theunreasonableeffectivenessofgreedyalgorithmsin
multi-armedbanditwithmanyarms. arXivpreprintarXiv:2002.10121.
Berry,D.A.,Chen,R.W.,Zame,A.,Heath,D.C.,andShepp,L.A.(1997). Banditproblemswithinfinitelymany
arms. TheAnnalsofStatistics,25(5):2103–2116.
Besbes,O.,Gur,Y.,andZeevi,A.(2014). Stochasticmulti-armed-banditproblemwithnon-stationaryrewards.
Advancesinneuralinformationprocessingsystems,27.
Bonald,T.andProutière,A.(2013). Two-targetalgorithmsforinfinite-armedbanditswithbernoullirewards. In
Proceedingsofthe26thInternationalConferenceonNeuralInformationProcessingSystems-Volume2,NIPS’13,
page2184–2192,RedHook,NY,USA.CurranAssociatesInc.
Brown,D.G.(2011). HowIwastedtoolongfindingaconcentrationinequalityforsumsofgeometricvariables.
Foundathttps://cs.uwaterloo.ca/˜browndg/negbin.pdf,8(4).
Carpentier, A. and Valko, M. (2015). Simple regret for infinitely many armed bandits. In Proceedings of the
32ndInternationalConferenceonInternationalConferenceonMachineLearning-Volume37,ICML’15,page
1133–1141.JMLR.org.
Cheung,W.C.,Simchi-Levi,D.,andZhu,R.(2019). Learningtooptimizeundernon-stationarity. InThe22nd
InternationalConferenceonArtificialIntelligenceandStatistics,pages1079–1087.PMLR.
DylanJ.Foster,A.R.(2022). Statisticalreinforcementlearninganddecisionmaking: Coursenotes. MITLecture
notesforcourse9.S915Fall2022.
Kim,J.-h.,Vojnovic,M.,andYun,S.-Y.(2022). Rottinginfinitelymany-armedbandits. InInternationalConference
onMachineLearning,pages11229–11254.PMLR.
Kleinberg,R.(2004).Nearlytightboundsforthecontinuum-armedbanditproblem.AdvancesinNeuralInformation
ProcessingSystems,17.
Levine,N.,Crammer,K.,andMannor,S.(2017). Rottingbandits. Advancesinneuralinformationprocessing
systems,30.
15Li,L.,Chu,W.,Langford,J.,andSchapire,R.E.(2010). Acontextual-banditapproachtopersonalizednewsarticle
recommendation. InProceedingsofthe19thinternationalconferenceonWorldwideweb,pages661–670.
Rigollet,P.andHütter,J.-C.(2015). Highdimensionalstatistics. Lecturenotesforcourse18S997,813:814.
Russac,Y.,Vernade,C.,andCappé,O.(2019). Weightedlinearbanditsfornon-stationaryenvironments. arXiv
preprintarXiv:1909.09146.
Seznec,J.,Locatelli,A.,Carpentier,A.,Lazaric,A.,andValko,M.(2019). Rottingbanditsarenoharderthan
stochasticones. InThe22ndInternationalConferenceonArtificialIntelligenceandStatistics,pages2564–2572.
PMLR.
Seznec,J.,Menard,P.,Lazaric,A.,andValko,M.(2020). Asinglealgorithmforbothrestlessandrestedrotting
bandits. InChiappa,S.andCalandra,R.,editors,ProceedingsoftheTwentyThirdInternationalConferenceon
ArtificialIntelligenceandStatistics,volume108ofProceedingsofMachineLearningResearch,pages3784–3794.
PMLR.
Tsun,A.(2020). Probability&statisticswithapplicationstocomputing.
Villar,S.S.,Bowden,J.,andWason,J.(2015). Multi-armedbanditmodelsfortheoptimaldesignofclinicaltrials:
benefitsandchallenges. Statisticalscience: areviewjournaloftheInstituteofMathematicalStatistics,30(2):199.
Wang, Y., Audibert, J.-y., andMunos, R.(2009). Algorithmsforinfinitelymany-armedbandits. InKoller, D.,
Schuurmans, D., Bengio, Y., and Bottou, L., editors, Advances in Neural Information Processing Systems,
volume21.CurranAssociates,Inc.
Wei, C.-Y. and Luo, H. (2021). Non-stationary reinforcement learning without prior knowledge: An optimal
black-boxapproach. InConferenceonlearningtheory,pages4300–4354.PMLR.
A Appendix
A.1 ProofofProposition2.1
Recall ∆ (a) = 1 − µ (a). We first show that E[µ (a)] = Θ(1). For any randomly sampled a ∈ A, we
1 1 1
have E[µ (a)] ≥ yP(µ (a) ≥ y) = yP(∆ (a) < 1−y) for y ∈ [0,1]. With y = 1/2, we have E[µ (a)] ≥
1 1 1 1
(1/2)P(∆ (a) < (1/2)) = Θ(1) from constant β > 0 and (1). Then with E[µ (a)] ≤ 1, we can conclude
1 1
E[µ (a)] = Θ(1) (Especially when P(∆(a) < x) = xβ, we have E[∆ (a)] = R1 P(∆ (a) ≥ x)dx = 1−
1 1 0 1
R1 P(∆ (a)<x)dx=1−R1 xβdx=1− 1 ,whichimpliesE[µ (a)]=Θ(1)withconstantβ >0). Wethen
0 1 0 β+1 1
thinkofapolicyπ′thatrandomlysamplesanewarmandpullsitonlyonceeveryround. SinceE[µ (a)]=Θ(1)
1
foranyrandomlysampleda,wehaveE[Rπ′(T)]=Θ(T).
Nextweshowthatthepolicyπ′isoptimalfortheworstcaseofV >T. Wethinkofanypolicyπ′′exceptπ′. For
T
anypolicyπ′′,therealwaysexistsanarmasuchthatthepolicymustpullarmaatleasttwice. Lett′ andt′′ be
theroundswhenthepolicypullsarma. Ifweconsiderρ =V thensuchpolicyhasΩ(V )regretbound. Since
t′ T T
16V > T,anyalgorithmexceptπ′ hasΩ(T)intheworstcase. Thereforewecanconcludethatπ′ istheoptimal
T
algorithmforachievingtheoptimalregretofΘ(T).
A.2 ProofofTheorem3.1: RegretUpperBoundofAlgorithm1forSlowRottingwithV
T
Let∆ (a)=1−µ (a). Usingathresholdparameterδ,weclassifyanarmaasgoodif∆ (a)≤δ/2,near-good
t t 1
ifδ/2 < ∆ (a) ≤ 2δ, andotherwise, weclassifyaasabad arm. InA, leta¯ ,a¯ ,...,beasequenceofarms,
1 1 2
whichhavei.i.d. meanrewardswithuniformdistributionon[0,1]. Withoutlossofgenerality,weassumethatthe
policysamplesarms,whicharepulledatleastonce,accordingtothesequenceofa¯ ,a¯ ,...,.LetA bethesetof
1 2 T
sampledarmsoverthehorizonofT timesteps,whichsatisfies|A |≤T. LetAG beasetofgoodorneargood
T T
armsinA .
T
Letµ
(a)=Ps2
µ (a)/n (a)forthetimesteps0<s ≤s . WedefineeventE ={|µ (a)−
[s1,s2] t=s1 t [s1,s2] 1 2 1 b[s1,s2]
p
µ (a)|≤ 12log(T)/n (a)forall1≤s ≤s ≤T,a∈A }. ByfollowingtheproofofLemma35in
[s1,s2] [s1,s2] 1 2 T
DylanJ.Foster(2022),fromLemmaA.21wehave
s !
(cid:12) (cid:12) 12logT
P (cid:12)µ (a)−µ (a)(cid:12)≤
(cid:12)b[s1,s2] [s1,s2] (cid:12) n (a)
[s1,s2]
XT (cid:12) (cid:12)1 Xn (cid:12) (cid:12) p !
≤ P (cid:12) X (cid:12)≤ 12log(T)/n
(cid:12)n i(cid:12)
(cid:12) (cid:12)
n=1 i=1
2
≤ , (13)
T5
whereX = r −µ (a)andτ isthei-thtimethatthepolicypullsarmastartingfroms . Wenotethateven
i τi τi i 1
thoughX ’sseemtodependoneachotherfromτ ’s,eachvalueofX isindependentofeachother. Thenusing
i i i
unionboundfors ,s ,anda∈A ,wehaveP(Ec)≤2/T2.FromthecumulativeamountofrottingV ,wenote
1 2 T 1 T
that∆ (a)=O(V +1)foranyaandt,whichimpliesE[Rπ(T)|Ec]=o(T2)fromV ≤T. Forthecasewhere
t T 1 T
E doesnothold,theregretisE[Rπ(T)|Ec]P(Ec)=O(1),whichisnegligiblecomparedtotheregretwhenE
1 1 1 1
holds,whichweshowlater. Therefore,fortherestoftheproof,weassumethatE holds.
1
Forregretanalysis,wedivideRπ(T)intotwoparts,RG(T)andRB(T)correspondingtoregretofgoodornear-
goodarms,andbadarmsovertimeT,respectively,suchthatRπ(T)=RG(T)+RB(T). Wefirstprovideabound
ofRG(T)inthefollowinglemma.
LemmaA.1. UnderE andpolicyπ,wehave
1
(cid:16) (cid:17)
E[RG(T)]=O˜ Tδ+T2/3V1/3 .
T
Proof. Hereweconsiderarmsa∈AG. LetV (a)=Pm ρ (a)andρ (a)=Pm ρ (a)/n (a)for
T [n,m] l=n l [n,m] l=n l [n,m]
timestepsn ≤ m. Foreaseofpresentation,fortimestepsr > q,wedefineV (a) = n (a) = ρ (a) =
[r,q] [r,q] [r,q]
17Pq
x(t)=0forx(t)∈Rand1/0=∞. Then,foranyssuchthatn≤s≤m,underE wehave
t=r 1
q
µ (a) ≤ µ¯ (a)+ 12log(T)/n (a)
b[s,m] [s,m] [s,m]
m X−1 q
≤ µ (a)+ ρ 1(a =a)+ 12log(T)/n (a)
m l l [s,m]
l=s
m X−1 m X−1 q
= µ (a)− ρ 1(a =a)+ ρ 1(a =a)+ 12log(T)/n (a)
n l l l l [s,m]
l=n l=s
q
≤ µ (a)−V (a)+ρ (a)n (a)+ 12log(T)/n (a).
n [n,m−1] [s,m−1] [s,m] [s,m]
Therefore,fromµ (a)≤1weobtain
n
q
µ (a)+ 12log(T)/n (a)
b[s,m] [s,m]
q
≤1−V (a)+ρ (a)n (a)+2 12log(T)/n (a). (14)
[n,m−1] [s,m−1] [s,m] [s,m]
Lett (a)betheinitialtimewhenthearmaissampledandpulledandt (a)bethefinaltimewhenthepolicypulls
1 2
thearm. Forsimplicity,weuset andt insteadoft (a)andt (a),respectively,whenthereisnoconfusion. We
1 2 1 2
defineA0asasetofarmsa∈AG suchthatt (a)=t (a)anddefineA1asasetofarmsa∈AG suchthatt (a)=
T 2 1 T 2
t (a)+1.WealsodefineasetofarmsAG ={a∈AG/{A0∪A1}:n (a)>⌈(logT)1/3/ρ (a)2/3⌉}.
1 T T [t1,t2−1] [t1,t2−2]
Letw(a) = ⌈(logT)1/3/ρ (a)2/3⌉. Forsimplicity,weusew forw(a)whenthereisnoconfusion. Then
[t1,t2−2]
withthefactthatµ (a)=µ
(a)−Pt−1
ρ (a)=µ (a)−V (a)fort (a)≤t≤t (a),wehave
t t1 t=t1(a) t t1 [t1,t−1] 1 2
    
X
t X2(a)
X
t X2(a)
E[RG(T)]=E (1−µ t(a))=E ∆ 1(a)n [t1,t2](a)+ V [t1,t−1](a)
a∈AG
T
t=t1(a) a∈AG
T
t=t1(a)+1

X X
t X2(a)
≤E2Tδ+ ρ + V (a)
 t1(a) [t1,t−1]
a∈A1 a∈AG/{AG∪A0∪A1}t=t1(a)+1
T T
 
X
t1(a X)+w(a) t X2(a)
+  V [t1,t−1](a)+ V [t1,t−1](a) ,
a∈AG t=t1(a)+1 t=t1(a)+w(a)+1
T
(15)
wherethefirstinequalitycomesfrom∆ (a)≤2δforanya∈AG. Forthesecondtermintherighthandsideofthe
1 T
lastinequality(15),
X
ρ ≤V . (16)
t1(a) T
a∈A1
Forthethirdtermin(15),fromthefactthatn (a) = n (a) < w(a)foranya ∈
AG/AG
fromthe
[t1+1,t2] [t1,t2−1] T T
18G
definitionofA ,wehave
T
X
t X2(a)
V (a)
[t1,t−1]
a∈AG/{AG∪A0∪A1}t=t1(a)+1
T T
X
≤ n (a)V (a)+ρ
[t1+1,t2] [t1,t2−2] t2(a)−1
a∈AG/{AG∪A0∪A1}
T T
 
X
=OV + w(a)V (a)
 T [t1,t2−2] 
a∈AG/{AG∪A0∪A1}
T T
 
=O˜V + X n (a)2/3V (a)1/3.
 T [t1,t2−2] [t1,t2−2] 
a∈AG/{AG∪A0∪A1}
T T
(17)
G G
Now,wefocusonthefourthtermin(15). Fromt (a)+w(a)+1≤t (a)fora∈A fromthedefinitionofA
1 2 T T
and(17),wefirsthave
X
t1(a X)+w(a)
X
t1(a X)+w(a) Xt−1
V (a)= ρ
[t1,t−1] s
a∈AG t=t1(a)+1 a∈AG t=t1(a)+1 s=t1
T T
X
t1(a X)+w(a)t2X(a)−2
≤ ρ
s
a∈AG t=t1(a)+1 s=t1(a)
T
X
≤ w(a)V (a)
[t1,t2−2]
a∈AG
T
 
=O˜ X n (a)2/3V (a)1/3. (18)
 [t1,t2−2] [t1,t2−2] 
a∈AG
T
Now we focus on
P Pt2(a)
V (a) in (15). From the definition of t and the threshold
a∈AG
T
t=t1(a)+w(a)+1 [t1,t−1] 2
conditioninthealgorithmwith(14),foranyt ≤t≤t andanyt ≤s≤t−1s.t. s=t−2l−1forl∈Z+,we
1 2 1
have
q
1−V (a)+n (a)ρ (a)+2 12log(T)/n (a)≥1−δ. (19)
[t1,t−2] [s,t−1] [s,t−2] [s,t−1]
For t ≥ t +w(a)+1, there always exists t ≤ s(t) ≤ t−1 such that w(a)/2 ≤ n (a) ≤ w(a) and
1 1 [s(t),t−1]
s(t)=t−2l−1forl∈Z+. Thenfrom(19)withs=s(t),wehave
(cid:16) (cid:17)
V (a)=O˜ δ+ρ (a)/ρ (a)2/3+ρ (a)1/3 . (20)
[t1,t−2] [s(t),t−2] [t1,t2−2] [t1,t2−2]
Usingthefactsthatn (a) ≥ n (a)/2 ≥ w(a)/4andt−s(t) ≤ w(a)fromn (a) ≤ w(a),
[s(t),t−2] [s(t),t−1] [s(t),t−1]
19wecanobtainthat
t X2(a) t X2(a) Pt k− =2 t−w(a)ρ
k
ρ (a)≤
[s(t),t−2] n (a)
[s(t),t−2]
t=t1(a)+1+w(a) t=t1(a)+1+w(a)
≤
t2X(a)−2
w(a)ρ
t
n (a)
[s(t),t−2]
t=t1(a)+1
t2X(a)−2
≤4 ρ , (21)
t
t=t1(a)
where the second inequality is obtained from the fact that the number of times that ρ is duplicated for each
t
t∈[t (a)+1,t
(a)−2]intheexpressionPt2(a) Pt−2
ρ isatmostw(a). Thenwith(20)and
1 2 t=t1(a)+1+w(a) k=t−w(a) k
(21),usingthefactthat
t X2(a)
ρ (a)1/3 ≤n (a)ρ (a)1/3 =O(n (a)2/3V (a)1/3),
[t1,t2−2] [t1,t2−2] [t1,t2−2] [t1,t2−2] [t1,t2−2]
t1(a)+1+w(a)
wehave
X
t X2(a)
V (a)
[t1,t−1]
a∈AGt=t1(a)+1+w(a)
T
X
t X2(a)
≤ V (a)+ρ
[t1,t−2] t2(a)−1
a∈AGt=t1(a)+1+w(a)
T
 
=O˜δT +V + X
t X2(a)
ρ (a)/ρ (a)2/3+ X
t X2(a)
ρ (a)1/3
 T [s(t),t−2] [t1,t2−2] [t1,t2−2] 
a∈AGt=t1(a)+1+w(a) a∈AGt=t1(a)+1+w(a)
T T
 
=O˜δT +V + X
t X2(a)
ρ (a)/ρ (a)2/3+ X n (a)2/3V (a)1/3
 T [s(t),t−2] [t1,t2−2] [t1,t2−2] [t1,t2−2] 
a∈AGt=t1(a)+1+w(a) a∈AG
T T
 
=O˜δT +V + X
t2X(a)−2
ρ /ρ (a)2/3+ X n (a)2/3V (a)1/3
 T t [t1,t2−2] [t1,t2−2] [t1,t2−2] 
a∈AG t=t1(a) a∈AG
T T
 
=O˜δT +V + X n (a)2/3V (a)1/3. (22)
 T [t1,t2−2] [t1,t2−2] 
a∈AG
T
20Thenputtingtheresultsfrom(15),(17),(18),and(22)altogether,wehave
E[RG(T)]
  
X
t X2(a)
≤E ∆ 1(a)n [t1,t2](a)+ V [t1,t−1](a)
a∈AG
T
t=t1(a)+1
 
=O˜ Tδ+V T + X V [t1,t2−2](a)1/3n [t1,t2−2](a)2/3 
a∈AG/{A0∪A1}
T
(cid:16) (cid:17)
=O˜ Tδ+V1/3T2/3 , (23)
T
wherethelastequalitycomesfromHölder’sinequalityandV ≤T. Thisconcludestheproof.
T
Now, we provide a bound for RB(T). We note that the initially bad arms can be defined only when 2δ < 1.
Otherwisewhen2δ ≥1,wehaveR(T)=RG(T),whichcompletestheproof. Therefore,fortheregretfrombad
arms,weconsiderthecaseof2δ <1. WeadopttheepisodicapproachinKimetal.(2022)fortheremainingregret
analysis. Theepisodicapproachisreformulatedusingthecumulativeamountofrottinginsteadofthemaximum
rottingrate. Inthefollowing,wedefinesomenotation.
Givenapolicysamplingarmsinthesequenceorder,letmG bethenumberofsamplesofdistinctgoodarmsandmB
i
bethenumberofconsecutivesamplesofdistinctbadarmsbetweenthei−1-standi-thsampleofagoodarmamong
mG goodarms. Werefertotheperiodstartingfromsamplingthei−1-stgoodarmbeforesamplingthei-thgood
armasthei-thepisode. ObservethatmB,...,mB arei.i.d. randomvariableswithgeometricdistributionwith
1 mG
parameter2δ,givenafixedvalueofmG. Therefore,fornon-negativeintegerkwehaveP(mB =k)=(1−2δ)k2δ,
i
fori=1,...,mG. Definem˜ tobethenumberofepisodesfromthepolicyπoverthehorizonT,m˜G tobethe
T T
totalnumberofsamplesofagoodarmbythepolicyπoverthehorizonT suchthatm˜G =m˜ orm˜G =m˜ −1,
T T T
andm˜B tobethenumberofsamplesofabadarminthei-thepisodebythepolicyπoverthehorizonT.
i,T
Underapolicyπ,letRB betheregret(summationofmeanrewardgaps)contributedbypullingthej-thbadarmin
i,j
thei-thepisode. ThenletRB =PmG P RB ,whichistheregretfrominitiallybadarmsovertheperiod
mG i=1 j∈[mB] i,j
i
ofmG episodes.
Leta(i)beagoodarminthei-thepisodeanda(i,j)beaj-thbadarminthei-thepisode. WedefineV (a) =
T
PT ρ 1(a = a). Then excluding the last episode m˜ over T, we provide lower bounds of the total rotting
t=1 t t T
variationoverT fora(i),denotedbyV (a(i)),inthefollowinglemma.
T
LemmaA.2. UnderE ,givenm˜ ,foranyi∈[m˜G]/{m˜ }wehave
1 T T T
V (a(i))≥δ/2.
T
21Proof. SupposethatV (a(i))=δ/2,thenwehave
T
n q o
min µ (a(i))+ 12log(T)/n (a(i))
t1(a(i))≤s≤t2(a(i))
b[s,t2(a(i))] [s,t2(a(i))]
≥ min {µ (a(i))}
t1(a(i))≤s≤t2(a(i))
[s,t2(a(i))]
≥µ (a(i))
t2(a(i))
≥µ (a(i))−V (a(i))
1 T
≥1−δ,
where the first inequality is obtained from E , and the last inequality is from V (a(i)) = δ/2 and µ (a(i)) ≥
1 T 1
1−δ/2. Therefore,policyπ mustpullarma(i)untilitstotalrottingamountisgreaterthanδ/2,whichimplies
V (a(i))≥δ/2.
T
Inthefollowing,weconsidertwodifferentcaseswithrespecttoV ;largeandsmallV .
T T
√
Case1: WeconsiderV =ω(max{1/ T,1/T1/(β+1)})inthefollowing.
T
Inthiscase,wehaveδ =δ (β)=max{(V /T)1/(β+2),(V /T)1/3}.Here,wedefinethepolicyπaftertimeT
V T T
suchthatitpullsagoodarmuntilitstotalrottingvariationisequaltoorgreaterthanδ/2anddoesnotpullasampled
badarm. WenotethatdefininghowπworksafterT isonlyfortheprooftogetaregretboundovertimehorizon
T. Forthelastarma˜overthehorizonT,itpullsthearmuntilitstotalvariationbecomesmax{δ/2,V (a˜)}ifa˜is
T
agoodarm. Fori∈[mG],j ∈[mB]letVG andVB bethetotalrottingvariationofpullingthegoodarmini-th
i i i,j
episodeandj-thbadarmini-thepisodefromthepolicy,respectively. HerewedefineVG’sandVB’sasfollows:
i i,j
Ifa˜isagoodarm,
 
VG
=V T(a(i)) fori∈[m˜G
T
−1]
,VB
=V T(a(i,j)) fori∈[m˜G T],j ∈[m˜B i,T]
i i,j
max{δ/2,V (a(i))} fori∈[mG]/[m˜G −1] 0 fori∈[mG]/[m˜G],j ∈[mB].
T T T i
Otherwise,
 
VG
=V T(a(i)) fori∈[m˜G T]
,VB
=V T(a(i,j)) fori∈[m˜G T],j ∈[m˜B i,T]
i i,j
δ/2 fori∈[mG]/[m˜G] 0 fori∈[mG]/[m˜G −1],j ∈[mB]/[m˜B ].
T T i i,T
Fori∈[mG],j ∈[mB]letnB bethenumberofpullingthej-thbadarmini-thepisodefromthepolicy. Wedefine
i i,j
n (a)bethetotalamountofpullingarmaoverT. HerewedefinenB ’sasfollows:
T i,j

nB
=n T(a(i,j)) fori∈[m˜G T],j ∈[m˜B i,T]
i,j
0 fori∈[mG]/[m˜G],j ∈[mB].
T i
ThenweprovidemG suchthatRB(T)≤RB inthefollowinglemma.
mG
22LemmaA.3. UnderE ,whenmG =⌈2V /δ⌉wehave
1 T
RB(T)≤RB .
mG
Proof. FromLemmaA.2,wehave
X δ
VG ≥mG ≥V ,
i 2 T
i∈[mG]
whichimpliesthatRB(T)≤RB .
mG
FromtheresultofLemmaA.3,wesetmG =⌈2V /δ⌉. WeanalyzeRB forobtainingaboundforRB(T)inthe
T mG
following.
LemmaA.4. UnderE andpolicyπ,wehave
1
(cid:16) (cid:17)
E[RB ]=O˜ max{T(β+1)/(β+2)V1/(β+2),T2/3V1/3} .
mG T T
Proof. Leta(i,j)beasampledarmforj-thbadarminthei-thepisodeandm˜ bethenumberofepisodesfromthe
T
policyπoverthehorizonT. Supposethatthealgorithmsamplesarma(i,j)attimet (a(i,j)). Thenthealgorithm
1
p
stopspullingarma(i,j)attimet (a(i,j))+1ifµ (a)+ 12log(T)/n (a)<1−δforsome
2 b[s,t2(a(i,j))] [s,t2(a(i,j))]
ssuchthatt (a(i,j))≤s≤t (a(i,j))ands=t (a(i,j))+1−2l−1forl∈Z+. Forsimplicity,weuset and
1 2 2 1
t insteadoft (a(i,j))andt (a(i,j))whenthereisnoconfusion. Wefirstconsiderthecasewherethealgorithm
2 1 2
stopspullingarma(i,j)becausethethresholdconditionissatisfied. Fortheregretanalysis,weconsiderthatfor
t>t ,armaisvirtuallypulled. WenotethatunderE ,wehave
2 1
q q
µ (a(i,j))+ 12log(T)/n (a(i,j))≤µ (a(i,j))+2 12log(T)/n (a(i,j))
b[s,t2] [s,t2] [s,t2] [s,t2]
q
≤µ (a(i,j))+2 12log(T)/n (a(i,j)).
1 [s,t2]
Thenweassumethatt˜(≥t )isthesmallesttimethatthereexistst ≤s≤t˜ withs=t˜ +1−2l−1forl∈Z+
2 2 1 2 2
suchthatthefollowingthresholdconditionismet:
q
µ (a(i,j))+2 12log(T)/n (a(i,j))<1−δ. (24)
1 [s,t˜ 2]
Fromthedefinitionoft˜,weobservethatforgivent˜,thetimesteps=s′whichsatisfying(24)equalstot (i.e.
2 2 1
s′ =t ).Then,wecanobservethatn (a(i,j))=n (a(i,j))=⌈C log(T)/(∆ (a(i,j))−δ)2⌉forsome
1 [s′,t˜ 2] [t1,t˜ 2] 2 t1
constantC >0,whichsatisfies(24). Thenfromn (a(i,j))≤n (a(i,j)),foralli∈[m˜ ],j ∈[m˜B ]
2 [t1,t2] [t1,t˜ 2] T i,T
wehavenB =O˜(1/(∆ (a(i,j))−δ)2). ThenwiththefactsthatnB =0fori∈[mG]/[m˜G],j ∈[mB]/[m˜B ],
i,j 1 i,j T i i,T
wehave,foranyi∈[mG]andj ∈[mB],
i
nB =O˜(1/(∆ (a(i,j))−δ)2).
i,j t1
For2δ <x≤1,letb(x)=P(∆ (a)=x|aisabadarm). Thenwehave
1
b(x)=P(∆ (a)=x|∆ (a)>2δ)
1 1
=P(∆ (a)=x)/P(∆ (a)>2δ)
1 1
=P(∆ (a)=x)/(1−C(2δ)β).
1
23Wenotethat2δ <∆ (a(i,j))=∆ (a(i,j))≤1. SincenB =O˜(1/(∆ (a(i,j))−δ)2)=O˜(1/δ2),wehave
t1 1 i,j t1
 
t2( Xa(i,j)) t2(a X(i,j))−1 Xt
E[R iB ,j]=E ∆ t1(a(i,j))+ ρ s
t=t1(a(i,j)) t=t1(a(i,j))s=t1(a(i,j))
≤E[∆ (a(i,j))nB +VBnB ]
1 i,j i,j i,j
≤E[∆ (a(i,j))nB +VB(1/δ2)]
1 i,j i,j
(cid:18)Z 1 1 (cid:19)
=O˜ xb(x)dx+E[VB(1/δ2)] . (25)
(x−δ)2 i,j
2δ
Recallthatweconsider2δ <1forregretfrombadarms. WeadoptsometechniquesintroducedinAppendixDof
Bayatietal.(2020)todealwiththegeneralizedmeanrewarddistributionwithβ. LetK =(1−2δ)/δ,a = 2,
j jδ
R(j+1)δ
andp = b(t+δ)dt. Thenforobtainingaboundofthelastequalityin(25)wehave
j jδ
Z 1(cid:18) 1 (cid:19) Z 1−δ 1 δ
x b(x)dx= ( + )b(t+δ)dt
(x−δ)2 t t2
2δ δ
XK Z (j+1)δ 1 δ
= ( + )b(t+δ)dt
t t2
j=1 jδ
XK 2 Z (j+1)δ
≤ b(t+δ)dt
jδ
j=1 jδ
K
X
= a p (26)
j j
j=1
WenotethatPj p ≤C (jδ)β forallj ∈[K]forsomeconstantC >0. Thenforgettingaboundofthelast
i=1 i 0 0
equalityin(26),wehave
K K−1 j ! K
X X X X
a p = (a −a ) p +a p
j j j j+1 i K i
j=1 j=1 i=1 i=1
K−1
X
≤ (a −a )C (jδ)β +a C (Kδ)β
j j+1 0 K 0
j=1
K
X
=C δβa + C (jβ −(j−1)β)δβa
0 1 0 j
j=2
 
(cid:18) (cid:19) K (cid:18) (cid:19)
=O 1 δβ +X 1 (cid:0) (jδ)β −((j−1)δ)β(cid:1) 
δ jδ
j=2
 
K (cid:18) (cid:19)
=Oδβ−1+X 1 δβ−1 (cid:0) jβ −(j−1)β(cid:1) . (27)
j
j=2
Nowweanalyzetheterminthelastequalityin(27)accordingtothecriteriaforβ. Forβ =1,wecanobtain
 
K (cid:18) (cid:19)
Oδβ−1+X 1 δβ−1 (cid:0) jβ −(j−1)β(cid:1) =O˜(1). (28)
j
j=2
24Forβ >1,wehavejβ −(j−1)β ≤βjβ−1usingthemeanvaluetheorem. Therefore,weobtainthefollowing.
   
K (cid:18) (cid:19) K (cid:18) (cid:19)
Oδβ−1+X 1 δβ−1 (cid:0) jβ −(j−1)β(cid:1) =OX 1 δβ−1 jβ−1

j j
j=2 j=1
 
K
X
=O δβ−1jβ−2 
j=2
(cid:18) (cid:19)
=O δβ−1 1 (cid:0) (K+1)β−1−1(cid:1)
β−1
=O(1). (29)
Forβ <1,whenj >1wehavejβ−(j−1)β ≤β(j−1)β−1usingthemeanvaluetheorem. Therefore,weobtain
   
K (cid:18) (cid:19) K (cid:18) (cid:19)
Oδβ−1+X 1 δβ−1 (cid:0) jβ −(j−1)β(cid:1) =Oδβ−1+X 1 δβ−1 (j−1)β−1

j j
j=1 j=2
 
K
X
=Oδβ−1+ δβ−1(j−1)β−2

j=2
(cid:18) (cid:19)
=O δβ−1+δβ−1 1 (cid:0) (K+1)β−1−1(cid:1)
β−1
(cid:18) 1−((1−δ)/δ)β−1(cid:19)
=O δβ−1+δβ−1 =O(δβ−1). (30)
1−β
From(26),(27),(28),(29),and(30),wehave
Z 1(cid:18) 1 (cid:19)
x b(x)dx=O˜(max{1,δβ−1}).
(x−δ)2
2δ
Thenforanyi∈[mG],j ∈[mB],wehave
i
E[RB ]≤E(cid:2) ∆(a(i,j))nB +VBnB (cid:3)
i,j i,j i,j i,j
=O˜(cid:0) max{1,δβ−1}+E[VB]/δ2(cid:1)
. (31)
i,j
RecallthatRB =PmG P RB .Withδ =max{(V /T)1/(β+2),(V /T)1/3}andmG =⌈2V /δ⌉,from
mG i=1 j∈[mB] i,j T T T
i
thefactthatmB’sarei.i.d. randomvariableswithgeometricdistributionwithE[mB]=1/(2δ)β −1,wehave
i i
  
mG
X X
E[R mB G]=OE R iB ,j
i=1j∈[mB]
i
(cid:18) (cid:19)
1
=O˜ (V /δ) max{1,δβ−1}+V /δ2
T δβ T
(cid:16) (cid:17)
=O˜ max{T(β+1)/(β+2)V1/(β+2),T2/3V1/3} . (32)
T T
25FromRπ(T) = RG(T)+RB(T)andLemmas A.1,A.3,A.4,withδ = max{(V /T)1/(β+2),(V /T)1/3}we
T T
have
(cid:16) (cid:17)
E[Rπ(T)]=O˜ max{T(β+1)/(β+2)V1/(β+2),T2/3V1/3} . (33)
T T
√
Case 2: Now we consider V = O(max{1/ T,1/T1/(β+1)}) in the following. In this case, we have δ =
T
√
max{1/Tβ+1 1,1/ T}.ForgettingR mB G,herewedefinethepolicyπaftertimeT suchthatitpullsV
T
amountof
rottingvariationforagoodarmand0forabadarm. Wenotethatdefininghowπ worksafterT isonlyforthe
prooftogetaregretboundovertimehorizonT. Forthelastarma˜overthehorizonT,itpullsthearmuptoV
T
amountofrottingvariationifa˜isagoodarm. Fori ∈ [mG],j ∈ [mB]letVG andVB betheamountofrotting
i i i,j
variationfrompullingthegoodarmini-thepisodeandj-thbadarmini-thepisodefromthepolicy,respectively.
HerewedefineVG’sandVB’sasfollows:
i i,j
Ifa˜isagoodarm,
 
VG
=V T(a(i)) fori∈[m˜G
T
−1]
,VB
=V T(a(i,j)) fori∈[m˜G T],j ∈[m˜B i,T]
i i,j
V fori∈[mG]/[m˜G −1] 0 fori∈[mG]/[m˜G],j ∈[mB].
T T T i
Otherwise,
 
VG
=V T(a(i)) fori∈[m˜G T]
,VB
=V T(a(i,j)) fori∈[m˜G T],j ∈[m˜B i,T]
i i,j
V fori∈[mG]/[m˜G] 0 fori∈[mG]/[m˜G −1],j ∈[mB]/[m˜B ].
T T T i i,T
Fori∈[mG],j ∈[mB]letnB bethenumberofpullingthej-thbadarmini-thepisodefromthepolicy. Wedefine
i i,j
n (a)bethetotalamountofpullingarmaoverT. HerewedefinenB ’sasfollows:
T i,j

nB
=n T(a(i,j)) fori∈[m˜G T],j ∈[m˜B i,T]
i,j
0 fori∈[mG]/[m˜G],j ∈[mB].
T i
ThenweprovidemG suchthatRB(T)≤RB inthefollowinglemma.
mG
LemmaA.5. UnderE ,whenmG =C forsomelargeenoughconstantC >0,wehave
1 3 3
RB(T)≤RB .
mG
Proof. FromLemmaA.2,underE wecanfindthatVG ≥ min{δ/2,V }fori ∈ [mG]. ThenifmG = C with
1 i T 3 √
somelargeenoughconstantC >0,thenwithδ =Θ(max{1/T1/(β+1),1/T1/2})andV =O(max{1/T1/(β+1),1/ T}),
3 T
wehave
X
VG ≥C min{δ/2,V }>V ,
i 3 T T
i∈[mG]
whichimpliesRB(T)≤RB .
mG
26WeanalyzeRB forobtainingaboundforRB(T)inthefollowing.
mG
LemmaA.6. UnderE andpolicyπ,wehave
1
(cid:16) √ (cid:17)
E[RB ]=O˜ max{Tβ/(β+1), T} .
mG
Proof. From(31),foranyi∈[mG],j ∈[mB],wehave
i
E[RB ]≤E(cid:2) ∆(a(i,j))nB +VBnB (cid:3)
i,j i,j i,j i,j
=O˜(cid:0) max{1,δβ−1}+E[VB]/δ2(cid:1)
.
i,j
RecallthatRB =PmG P RB .Withδ =max{(1/T)1/(β+1),1/T1/2}andmG =C ,fromthefactthat
mG i=1 j∈[mB] i,j 3
i
mB’sarei.i.d. randomvariableswithgeometricdistributionwithE[mB]=1/(2δ)β −1,wehave
i i
  
mG
X X
E[R mB G]=OE R iB ,j
i=1j∈[mB]
i
(cid:18) (cid:19)
1
=O˜ max{1,δβ−1}+V /δ2
δβ T
(cid:16) √ (cid:17)
=O˜ max{Tβ/(β+1), T} .
√
1
FromLemmaA.1,withδ =max{1/Tβ+1,1/ T}wehave
(cid:16) √ (cid:17)
E[RG(T)]=O˜ max{Tβ/(β+1), T} .
√
FromRπ(T)=RG(T)+RB(T)andLemmasA.1,A.5,A.6withδ =max{1/Tβ+1 1,1/ T}wehave
(cid:16) √ (cid:17)
E[Rπ(T)]=O˜ max{Tβ/(β+1), T} . (34)
Conclusion: Overall,from(33)and(34),wehave
(cid:16) √ (cid:17)
E[Rπ(T)]=O˜ max{V1/(β+2)T(β+1)/(β+2),V1/3T2/3,Tβ/(β+1), T} .
T T
A.3 ProofofTheorem3.3: RegretUpperBoundofAlgorithm1forAbruptRottingwith
S
T
Usingthethresholdparameterδinthealgorithm,wedefineanarmaasagoodarmif∆ (a)≤δ/2,anear-good
t
armifδ/2 < ∆ (a) ≤ 2δ,andotherwise,aisabad armattimet. Foranalysis,weconsiderabruptchangeas
t
samplinganewarm. Inotherwords,ifasuddenchangeoccurstoanarmabypullingthearma,thenthearmis
consideredtobetwodifferentarms;beforeandafterthechange. Thetypeofabruptlyrottedarms(good,near-good,
orbad)afterthechangeisdeterminedbythecurrentvalueofrottedmeanreward. Withoutlossofgenerality,we
27assumethatthepolicysamplesarms,whicharepulledatleastonce,inthesequenceofa¯ ,a¯ ,...,.LetA bethe
1 2 T
setofsampledarms,whicharepulledatleastonce,overthehorizonofT timesteps,whichsatisfies|A | ≤ T.
T
WealsodefineA asasetofarmsthathavebeenrottedandpulledatleastonce,whichsatisfies|A |≤S . To
S S T
betterunderstandthedefinitions,weprovideanexample. Ifanarmasuffersabruptrottingatfirst,thenthearmais
consideredtobeadifferentarma′aftertherotting. Ifthearma′againsuffersabruptrotting,thenitisconsidered
tobea′′ aftertherotting. Ifarmsa,a′,a′′ arepulledatleastonce,then{a,a′,a′′}∈A and{a′,a′′}∈A but
T S
a∈/ A . Ifarma′′isnotpulledatleastoncebutaanda′arepulledatleastonce,then{a,a′}∈A anda′ ∈A
S T S
buta′′ ∈/ A .
S
Let µ (a) = Pt2 µ (a)1(a = a)/n (a). We define the event E = {|µ (a)−µ (a)| ≤
[t1,t2] t=t1 t t [t1,t2] 1 b[s1,s2] [s1,s2]
p
12log(T)/n (a)forall1 ≤ s ≤ s ≤ T,a ∈ A }. By following the proof of Lemma 35 in Dylan
[s1,s2] 1 2 T
J.Foster(2022),fromLemmaA.21wehave
s !
(cid:12) (cid:12) 12logT
P (cid:12)µ (a)−µ (a)(cid:12)≤
(cid:12)b[s1,s2] [s1,s2] (cid:12) n (a)
[s1,s2]
XT (cid:12) (cid:12)1 Xn (cid:12) (cid:12) p !
≤ P (cid:12) X (cid:12)≤ 12log(T)/n
(cid:12)n i(cid:12)
(cid:12) (cid:12)
n=1 i=1
2
≤ , (35)
T5
whereX = r −µ (a)andτ isthei-thtimethatthepolicypullsarmastartingfroms . Wenotethateven
i τi τi i 1
thoughX ’sseemtodependoneachotherfromτ ’s,eachvalueofX isindependentofeachother. Thenusing
i i i
unionboundfors ,s ,anda∈A ,wehave
1 2 T
P(Ec)≤2/T2.
1
Let t(s) be the time when s-th abrupt rotting occurs with ρ for s ∈ [S ]. Then we have ∆ (a) = O(1+
t(s) T t
PST ρ )=O(1+V )foranyaandt,whichimpliesE[Rπ(T)|Ec]=O(T +TV ). ForthecasethatE does
s=1 t(s) T 1 T 1
nothold,theregretisE[Rπ(T)|Ec]P(Ec)=O((1+V )/T),whichisnegligiblecomparingwiththeregretwhen
1 1 T
E holdstruewhichweshowlater. Therefore,intherestoftheproofweassumethatE holdstrue.
1 1
Recall that Rπ(T) = PT (1 − µ (a )). For regret analysis, we divide Rπ(T) into two parts, RG(T) and
t=1 t t
RB(T) corresponding to regret of good or near-good arms, and bad arms over time T, respectively, such that
Rπ(T)=RG(T)+RB(T). Recallthatweconsiderabruptchangeassamplinganewarminthisanalysis. Then,
from∆ (a)≤2δforanygoodornear-goodarmsaattimet,wecaneasilyobtainthat
t
E[RG(T)]=O(δT)=O(max{S1/(β+1)Tβ/(β+1),p
S T}). (36)
T T
NowweanalyzeRB(T). WedivideregretRB(T)intotworegretfrombadarmsinA /A ,denotedbyRB,1(T),
T S
andregretfrombadarmsinA ,denotedbyRB,2(T)suchthatRB(T) = RB,1(T)+RB,2(T). Wedenotebad
S
armsinA byAB. WefirstanalyzeRB,1(T)inthefollowing. Forregretanalysis,weadopttheepisodicapproach
S S
suggested in Kim et al. (2022). The main difference lies in analyzing our adaptive window UCB and a more
generalizedmean-rewarddistributionwithβ. Inthefollowing,weintroducesomenotation. Hereweonlyconsider
28armsinA /A sothatthefollowingnotationisdefinedwithoutconsidering(rotted)armsinA . Wenotethat
T S S
fromthedefinitionofA ,armsabeforehavingundergonerottingarecontainedinA /A . Hereweconsiderthe
T T S
caseof2δ (β)<1sinceotherwisewhen2δ (β)≥1,badarmsarenotdefinedinA /A .
S S T S
Givenapolicysamplingarmsinthesequenceorder,letmG bethenumberofsamplesofdistinctgoodarmsandmB
i
bethenumberofconsecutivesamplesofdistinctbadarmsbetweenthei−1-standi-thsampleofagoodarmamong
mG goodarms. Werefertotheperiodstartingfromsamplingthei−1-stgoodarmbeforesamplingthei-thgood
armasthei-thepisode. ObservethatmB,...,mB arei.i.d. randomvariableswithgeometricdistributionwith
1 mG
parameter2δ,givenafixedvalueofmG. Therefore,fornon-negativeintegerkwehaveP(mB =k)=(1−2δ)k2δ,
i
fori=1,...,mG.
Definem˜G tobethetotalnumberofsamplesofagoodarmbythepolicyπoverthehorizonT andm˜B tobethe
T i,T
numberofsamplesofabadarminthei-thepisodebythepolicyπoverthehorizonT. Fori∈[m˜G],j ∈[m˜B ],
T i,T
letn˜G bethenumberofpullsofthegoodarminthei-thepisodeandn˜B bethenumberofpullsofthej-thbadarm
i i,j
inthei-thepisodebythepolicyπoverthehorizonT. Leta˜bethelastsampledarmovertimehorizonT byπ.
With a slight abuse of notation, we use π for a modified strategy after T. Under a policy π, let RB be the
i,j
regret (summation of mean reward gaps) contributed by pulling the j-th bad arm in the i-th episode. Then let
RB = PmG P RB , which is the regret from initially bad arms over the period of mG episodes. For
mG i=1 j∈[mB] i,j
i
gettingRB ,herewedefinethepolicyπafterT suchthatitpullsT amountsforagoodarmandzeroforabadarm.
mG
AfterT wecanassumethattherearenoabruptchanges. Forthelastarma˜overthehorizonT,itpullsthearmupto
T amountsifa˜isagoodarmandn˜G <T. Fori∈[mG],j ∈[mB]letnG andnB bethenumberofpullingthe
m˜G i i i,j
T
goodarmini-thepisodeandj-thbadarmini-thepisodeunderπ,respectively. HerewedefinenG’sandnB ’sas
i i,j
follows:
Ifa˜isagoodarm,

n˜G
i
fori∈[m˜G
T
−1] 
n˜B fori∈[m˜G],j ∈[m˜B ]
nG = T fori=m˜G ,nB = i,j T i,T
i 0 fori∈[mT
G]/[m˜G]
i,j 0 fori∈[mG]/[m˜G T],j ∈[mB
i
]/[m˜B i,T].
T
Otherwise,

n˜G
i
fori∈[m˜G T] 
n˜B fori∈[m˜G],j ∈[m˜B ]
nG = T fori=m˜G +1 ,nB = i,j T i,T
i 0 fori∈[mT
G]/[m˜G +1]
i,j 0 fori∈[mG]/[m˜G
T
−1],j ∈[mB
i
]/[m˜B i,T].
T
UsingtheabovenotationandnewlydefinedπafterT,weshowthatifmG =S +1,thenRB(T)≤RB inthe
T mG
following.
LemmaA.7. UnderE ,whenmG =S wehave
1 T
RB,1(T)≤RB .
mG
29Proof. ThereareS −1numberofabruptchangesoverT. Weconsidertwocases;thereareS abruptchanges
T T
beforesamplingS -thgoodarmortherearenot. Fortheformercase,ifπsamplestheS -thgoodarmandthere
T T
areS −1numberofabruptchangesbeforesamplingthegoodarm,thenitcontinuestopullthegoodarmuntilT.
T
Thisisbecausewhenthealgorithmsamplesagoodarmaattimet′,fromE andthestationaryperiod,wehave
1
q
µ (a)+ 12log(T)/n (a)≥µ (a)≥1−δ.
b[t′,t] [t′,t] t′
Thisimpliesthatfromthethresholdcondition,thealgorithmdoesnotstoppullingthegoodarma. AfterT,from
thedefinitionofπforthecasewhena˜isagoodarm,nG =T. Therefore,thealgorithmpullsthegoodarmforT
m˜G
T
rounds.
Nowweconsiderthelattercase,suchthatπsamplestheS -thgoodarmbeforetheS −1-stabruptchangeover
T T
T. BeforesamplingtheS -thgoodarm,theremustexisttwoconsecutivegoodarmssuchthatthereisnoabrupt
T
changebetweenthetwosampledgoodarms. Thisisacontractionbecauseπmustpullthefirstgoodarmamongthe
twouptoT underE andS −1-stabruptchangemustoccurafterT.
1 T
Therefore,itisenoughtoconsidertheformercase. WhenmG =S ,wehave
T
X
nG ≥T,
i
i∈[mG]
whichimpliesRB,1(T)≤RB .
mG
Fromtheabovelemma,wesetmG =S . WeanalyzeRB togetaboundforRB,1(T)inthefollowinglemma.
T mG
LemmaA.8. UnderE andpolicyπ,wehave
1
E(cid:2) RB (cid:3) =O˜(cid:16) max{S1/(β+1)Tβ/(β+1),p S T}(cid:17) .
mG T T
Proof. Recall that we consider arms in A /A . Let a(i,j) be a sampled arm for j-th bad arm in the i-th
T S
episode and m˜ be the number of episodes from the policy π over the horizon T. Suppose that the algorithm
T
samples arm a(i,j) at time t (a(i,j)). Then the algorithm stops pulling arm a(i,j) at time t (a(i,j))+1 if
1 2
p
µ (a)+ 12log(T)/n (a) < 1−δ forsomessuchthatt (a(i,j)) ≤ s ≤ t (a(i,j))and
b[s,t2(a(i,j))] [s,t2(a(i,j))] 1 2
s=t (a(i,j))+1−2l−1forl∈Z+. Forsimplicity,weuset andt insteadoft (a(i,j))andt (a(i,j))when
2 1 2 1 2
thereisnoconfusion. Fortheregretanalysis,weconsiderthatfort>t ,armaisvirtuallypulled. WithE ,we
2 1
assumethatt˜(≥t )isthesmallesttimethatthereexistst ≤s≤t˜ withs=t˜ +1−2l−1forl∈Z+suchthat
2 2 1 2 2
thefollowingconditionismet:
q
µ (a(i,j))+2 12log(T)/n (a(i,j))<1−δ. (37)
t1 [s,t˜ 2]
Fromthedefinitionoft˜,weobservethatforgivent˜,thetimesteps=s′satisfying(37)equalstot (i.e. s′ =t ).
2 2 1 1
Then,wecanobservethatn (a(i,j))=n (a(i,j))=⌈C log(T)/(∆ (a(i,j))−δ)2⌉forsomeconstant
[s′,t˜ 2] [t1,t˜ 2] 2 t1
C > 0,whichsatisfies(37). Thenfromn (a(i,j)) ≤ n (a(i,j)),foralli ∈ [m˜ ],j ∈ [m˜B ]wehave
2 [t1,t2] [t1,t˜ 2] T i,T
nB =O˜(1/(∆ (a(i,j))−δ)2). Wenotethatthisboundforthenumberofpullinganarmholdsfornotonlythe
i,j t1
casewherethearmstopsbeingpulledfromthethresholdconditionbutalsothecasewherethearmstopsbeing
30pulledfrommeetinganabruptchange(recallthatabruptchangesareconsideredassamplinganewarm)orT. Then
withthefactsthatnB =0fori∈[mG]/[m˜ ],j ∈[mB]/[m˜B ],wehave,foranyi∈[mG]andj ∈[mB],
i,j T i i,T i
nB =O˜(1/(∆ (a(i,j))−δ)2).
i,j t1
For 2δ < x ≤ 1, let b(x) = P(∆ (a) = x|aisabadarm). Then we have P(∆ (a) = x|aisabadarm) =
t1 t1
P(∆ (a) = x|∆ (a) > 2δ) = P(∆ (a) = x)/P(∆ (a) > 2δ) = P(∆ (a) = x)/(1 − C(2δ)β) =
t1 t1 t1 1 t1
O(P(∆ (a)=x)),wherethelastequalitycomesfromsmallδwithS =o(T). Foranyi∈[mG],j ∈[mB],we
t1 T i
have
E[RB ]≤E(cid:2) ∆ (a(i,j))nB (cid:3)
i,j t1(a(i,j)) i,j
(cid:18)Z 1 1 (cid:19)
=O˜ xb(x)dx . (38)
(x−δ)2
2δ
Fromtheaboveresultsin(38),(26),(27),(28),(29),(30),forβ >0wehave
E[RB ]=O˜(max{1,δβ−1}).
i,j
Recall that RB = PmG P RB . With δ = max{(S /T)1/(β+1),(S /T)1/2} and mG = S , from
mG i=1 j∈[mB] i,j T T T
i
Lemma A.7andthefactthatmB’sarei.i.d. randomvariablesfollowinggeometricdistributionwithE[mB] =
i i
1/(2δ)β −1,wehave
  
mG
X X
E[R mB G]=OE R iB ,j
i=1j∈[mB]
i
(cid:18) (cid:19)
1
=O˜ S max{1,δβ−1}
Tδβ
=O˜(cid:16) max{S1/(β+1)Tβ/(β+1),p
S
T}(cid:17)
.
T T
(cid:16) √ (cid:17)
FromLemmaA.8,wehaveE[RB,1(T)]=E[RB ]=O˜ max{S1/(β+1)Tβ/(β+1), S T} .
mG T T
NowweanalyzeRB,2(T)inthefollowinglemma. Here,weconsiderarmsinAB,whichisallowedtohavenegative
S
meanrewards.
LemmaA.9. UnderE andpolicyπ,wehave
1
E(cid:2) RB,2(T)(cid:3) =O˜(max{S /δ,V }).
T T
Proof. Recall that we consider arms a ∈ AB so that ∆ (a) > 2δ from definition. Suppose that the arm a is
S t1
sampledandpulledforthefirsttimeattimet (a). Thenthealgorithmstopspullingarmaattimet (a)+1if
1 2
µ (a)+p 12log(T)/n (a) < 1−δ forsomessuchthats ≤ t (a)ands = t (a)+1−2l−1 for
b[s,t2(a)] [s,t2(a)] 2 2
l∈Z+. Forsimplicity,weuset andt insteadoft (a)andt (a)whenthereisnoconfusion. Forregretanalysis,
1 2 1 2
31weconsiderthatfort>t ,armaisvirtuallypulled. WithE ,weassumethatt˜(≥t )isthesmallesttimethat
2 1 2 2
thereexistst ≤s≤t˜ withs=t˜ +1−2l−1forl∈Z+suchthatthefollowingconditionismet:
1 2 2
q
µ (a)+2 12log(T)/n (a)<1−δ. (39)
t1 [s,t˜ 2]
Fromthedefinitionoft˜,weobservethatforgivent˜,thetimesteps,whichsatisfies(39),equalstot . Then,
2 2 1
wecanobservethatn (a)=max{⌈C log(T)/(∆ (a)−δ)2⌉,1}forsomeconstantC >0,whichsatisfies
[t1,t˜ 2] 2 t1 2
p
(39). Fromtheabove,foranya∈AB satifying∆ (a)≥ C log(T)+δ,wehaven (a)=1. Thisimplies
S t1 2 [t1,t˜ 2]
thatafterpullingthearmaonce,thearmiseliminatedandafterthat,thearmisnotpulledanymore. Therefore,for
p
anyarma′ whichwasrottedtoa,wehave∆ (a′) < C log(T)+δ. Thisisbecauseotherwisesuchthat
t1(a′) 2
p
∆ (a′)≥ C log(T)+δ,thearma′iseliminatedandacannotbepulledwhichmeansa∈/ AB,whichisa
t1(a′) 2 S
p
contradiction. Thenforanyarma∈AB,wehave∆ (a)≤ C log(T)+δ+ρ . Recallthatweconsider
S t1 2 t1(a)−1
abruptrottingofanarmassamplinganewarm. Lett(s)bethetimestepwhenthes-thabruptrottingoccurs. Then
wenotethatρ =ρ whenarmaisasampledarmfroms-thabruptrottingfors∈[S ].
t1(a)−1 t(s) T
Fromn (a) ≤ n (a),wehaven (a) = O˜(max{1/(∆ (a)−δ)2,1}). Wenotethatthisboundfor
[t1,t2] [t1,t˜ 2] [t1,t2] t1
numberofpullinganarmholdsfornotonlythecasewherethearmstopstobepulledfromthethresholdcondition,
butalsothecasewherethearmstopstobepulledfrommeetinganabruptchange(recallthatabruptchangesare
consideredassamplinganewarm)orT. Fromthedefinitionofbadarms,wehave∆ (a)≥2δ. Thentheregret
t1
fromarma, denotedbyR(a), isboundedasfollows: R(a) = ∆ (a)n (a) = O˜(max{∆ (a)/(∆ (a)−
t1 [t1,t2] t1 t1
δ)2,∆ (a)}).Sincex/(x−δ)2 ≤2/δforanyx≥2δ,wehaveR(a)=O˜(max{1/δ,∆ (a)}).Therefore,with
t1 t1
p
thefactthat∆ (a)≤ C log(T)+δ+ρ forthecorrespondings∈[S ]suchthatρ =ρ ,wehave
t1 2 t(s) T t1(a)−1 t(s)
  
X R(a)=O˜ max S T/δ, X ∆ t1(a) 
 
a∈AB a∈AB
S S
=O˜(max{S /δ,S
+XST
ρ })
T T t(s)
s=1
=O˜(max{S
/δ,XST
ρ })
T t(s)
s=1
=O˜(max{S /δ,V }),
T T
wherethesecondlastequalitycomesfromS /δ ≥S .
T T
Finally,fromRπ(T)=RG(T)+RB(T),(36),andLemmasA.8,A.9,wehave
E[Rπ(T)]=O˜(cid:16) max{S1/(β+1)Tβ/(β+1),p
S T,V
}(cid:17)
.
T T T
A.4 Pseudo-codeoftheAdaptiveAlgorithmforUnknownParameters
Theparametersofβ,V ,andS areusedtosettheoptimalthresholdparameterδinAlgorithm1. Therefore,when
T T
theparametersarenotgiven,theproceduretofindtheoptimalvalueδisrequired. WeadopttheBandit-over-Bandit
(BoB) approach in Cheung et al. (2019); Kim et al. (2022) by additionally considering adaptive window. In
32Algorithm2AdaptiveUCB-ThresholdwithAdaptiveSlidingWindow
Given: T,H,B,A,α,κ,C
Initialize: A′ ←A,w(δ′)←1forδ′ ∈B
fori=1,2,...,⌈T/H⌉do
t′ ←(i−1)H +1
Selectanarma∈A′
Pullarmaandgetrewardr
(i−1)H+1
p(δ′)←(1−α)Pw(δ′) +α1 forδ′ ∈B
w(k) B
k∈B
Selectδ ←δ′withprobabilityp(δ′)forδ′ ∈B
fort=(i−1)H +2,...,i·H ∧T do
if min WUCB(a,s,t−1,H)<1−δthen
s∈Tt(a)
A′ ←A′/{a}
Selectanarma∈A′
Pullarmaandgetrewardr
t
t′ ←t
else
Pullarmaandgetrewardr
t
endif
endfor
w(δ)←w(δ)exp(cid:18)
α
(cid:18)
1 +
Pi t· =H (i∧ −T 1√)Hrt (cid:19)(cid:19)
Bp(δ) 2 CHlog(H)+4 HlogT
endfor
Algorithm2,thealgorithmconsistsofamasterandseveralbasealgorithmswithB. Forthemaster,weuseEXP3
(Aueretal.,2002)tofindanearlybestbaseinB.EachbaserepresentsAlgorithm1withacandidatethresholdδ′ ∈B.
ThealgorithmdividesthetimehorizonintoseveralblocksoflengthH. Ateachblock,thealgorithmsamplesabase
inBfromtheEXP3strategyandrunsthebaseoverthetimestepsoftheblock. Usingthefeedbackfromtheblock,
thealgorithmupdatesEXP3andsamplesanewbaseforthenextblock. Byblocktimepasses,themasterislikely
p
tofindanoptimizedδinB. LetB =|B|. ThenforAlgorithm2,wesetα=min{1, BlogB/((e−1)⌈T/H⌉)}
andC >0tobealargeenoughconstant.
√
Wedefineδ† =max{(V /T)1/(β+2),(V /T)1/3,1/H1/(β+1),1/ H}and
V T T √
δ† =max{(S /T)1/(β+1),(S /T)1/2,1/H1/(β+1),1/ H}. Thentheoptimizedthresholdparameterisδ† =
S T T VS
min{δ†,δ†}. TheoptimizedthresholdparametercanbederivedfromthetheoreticalanalysisinAppendixA.5. The
S V √
targetofthemasteristofindtheparameter. Fromtheabove,wecanobservethat1/ H ≤δ† ≤1. Therefore,we
VS
√
setB ={1/2,...,1/2log 2 H}whichisthecandidatevaluesforunknownδ†.
A.5 ProofofTheorem3.9: RegretUpperBoundofAlgorithm2
Inthefollowing, wedealwiththecasesof(a)δ† ≤ δ† sothatδ† = δ† and(b)δ† > δ† sothatδ† = δ†,
V S VS V V S VS S
separately.
33A.5.1 Caseofδ† ≤δ†
V S
Letπ (δ′)forδ′ ∈Bdenotethebasepolicyfortimestepsbetween(i−1)H+1andi·H∧T inAlgorithm2using
i
1−δ′asathreshold. Denotebyaπi(δ′)thepulledarmattimesteptbypolicyπ (δ′).Then,forδ† ∈B,whichis
t i
setlaterforanear-optimalpolicy,wehave
 
T ⌈T/H⌉ i·H∧T
X X X
E[Rπ(T)]=E 1− µ t(aπ t)=E[R 1π(T)]+E[R 2π(T)]. (40)
t=1 i=1 t=(i−1)H+1
where
T ⌈T/H⌉ i·H∧T
Rπ(T)=X
1−
X X
µ
(aπi(δ†))
1 t t
t=1 i=1 t=(i−1)H+1
and
⌈T/H⌉ i·H∧T ⌈T/H⌉ i·H∧T
Rπ(T)= X X µ (aπi(δ†))− X X µ (aπ).
2 t t t t
i=1 t=(i−1)H+1 i=1 t=(i−1)H+1
NotethatRπ(T)accountsfortheregretcausedbythenear-optimalbasealgorithmπ (δ†)’sagainsttheoptimal
1 i
meanrewardandRπ(T)accountsfortheregretcausedbythemasteralgorithmbyselectingabasewithδ ∈ B
2
at every block against the base with δ†. In what follows, we provide upper bounds for each regret component.
We first provide an upper bound for E[Rπ(T)] by following the proof steps in Theorem 3.1. Then we provide
1
an upper bound for E[Rπ(T)]. We set H = ⌈T1/2⌉ and δ† to be a smallest value in B which is larger than
2
δ† =max{(V /T)1/(β+2),(V /T)1/3,1/H1/(β+1),1/H1/2}.
V T T
UpperBoundingE[Rπ(T)]. Werefertotheperiodstartingfromtimestep(i−1)H +1totimestepi·H ∧T as
1
thei-thblock. Foranyi∈⌈(T/H)−1⌉,policyπ (δ†)runsoverH timestepsindependenttootherblockssothat
i
eachblockhasthesameexpectedregretandthelastblockhasasmallerorequalexpectedregretthanotherblocks.
Therefore,wefocusonfindingaboundontheregretfromthefirstblockequaltoPH
1−µ
(aπ1(δ†)).
Wedefine
t=1 t t
anarmaasagoodarmif∆(a)≤δ†/2,anear-goodarmifδ†/2<∆(a)≤2δ†,andotherwise,aisabadarm. In
A,leta¯ ,a¯ ,...,beasequenceofarms,whichhavei.i.d. meanrewardsfollowing(1). Withoutlossofgenerality,
1 2
weassumethatthepolicysamplesarmsinthesequenceofa¯ ,a¯ ,...,.
1 2
Denote by A(i) the set of sampled arms in the i-th block, which satisfies |A(i)| ≤ H. Let µ (a) =
[t1,t2]
Pt2
µ (a)/n (a).WedefinetheeventE ={|µ (a)−µ
(a)|≤p
12log(H)/n (a)forall1≤
t=t1 t [t1,t2] 1 b[s1,s2] [s1,s2] [s1,s2]
s ≤s ≤H,a∈A(i)}. Asin(13),wehave
1 2
P(Ec)≤2/H2.
1
WedenotebyV thecumulativeamountofrottinginthetimestepsinthei-thblock.Fromthecumulativeamountof
H,i
rotting,wenotethat∆ (a)=O(V +1)foranyaandtini-thblock,whichimpliesE[Rπ(T)|Ec]=O(H2)from
t H,i 1
V ≤H underAssumption3.5. ForthecasewhereE doesnothold,theregretisE[Rπ(T)|Ec]P(Ec)=O(1),
H,i 1 1 1
whichisnegligiblecomparedtotheregretwhenE holds,whichweshowlater. ForthecasethatE doesnothold,
1 1
theregretisE[Rπ(H)|Ec]P(Ec)=O(1),whichisnegligiblecomparedwiththeregretwhenE holdstruewhich
1 1 1
weshowlater. Therefore,intherestoftheproofweassumethatE holdstrue.
1
34Inthefollowing,wefirstprovidearegretboundoverthefirstblock.
Forregretanalysis,wedivideRπ1(δ†)(H)intotwoparts,RG(H)andRB(H)correspondingtoregretofgoodor
near-goodarms,andbadarmsovertimeH,respectively,suchthatRπ1(δ†)(H)=RG(H)+RB(H). Wedenoteby
V thecumulativeamountofrottinginthetimestepsinthei-thblock. WefirstprovideaboundofRG(H)inthe
H,i
followinglemma.
LemmaA.10. UnderE andpolicyπ,wehave
1
(cid:16) (cid:17)
E[RG(H)]=O˜ Hδ†+H2/3V1/3 .
H,1
Proof. WecaneasilyprovethetheorembyfollowingtheproofstepsinLemmaA.1
Now,weprovidearegretboundforRB(H). Wenotethattheinitiallybadarmscanbedefinedonlywhen2δ† <1.
Otherwisewhen2δ† ≥1,wehaveR(T)=RG(T),whichcompletestheproof. Therefore,fortheregretfrombad
arms,weconsiderthecaseof2δ† <1. Fortheproof,weadopttheepisodicapproachinKimetal.(2022)forregret
analysis.
Given a policy sampling arms in the sequence order, let mG be the number of samples of distinct good arms
and mB be the number of consecutive samples of distinct bad arms between the i−1-st and i-th sample of a
i
good arm among mG good arms. We refer to the period starting from sampling the i−1-st good arm before
sampling the i-th good arm as the i-th episode. Observe that mB,...,mB are i.i.d. random variables with
1 mG
geometricdistributionwithparameter2δ,givenafixedvalueofmG. Therefore,fornon-negativeintegerkwehave
P(mB =k)=(1−2δ†)k2δ†,fori=1,...,mG. Definem˜ tobethenumberofepisodesfromthepolicyπover
i H
thehorizonH,m˜G tobethetotalnumberofsamplesofagoodarmbythepolicyπoverthehorizonH suchthat
H
m˜G =m˜ orm˜G =m˜ −1,andm˜B tobethenumberofsamplesofabadarminthei-thepisodebythepolicy
H H H i,H
π (δ†)overthehorizonH.
1
Underapolicyπ (δ†),letRB betheregret(summationofmeanrewardgaps)contributedbypullingthej-thbad
1 i,j
arminthei-thepisode. ThenletRB =PmG P RB ,whichistheregretfrominitiallybadarmsoverthe
mG i=1 j∈[mB] i,j
i
periodofmG episodes.
Forobtainingaregretbound,wefirstfocusonfindingarequirednumberofepisodes,mG,suchthatRB(T)≤RB .
mG
Thenweprovideregretboundsforeachbadarmandgoodarminanepisode. Lastly,weobtainaregretboundfor
E[RB(T)]usingtheepisodicregretbound.
Leta(i)beagoodarminthei-thepisodeanda(i,j)beaj-thbadarminthei-thepisode. WedefineV (a) =
H
PH ρ 1(a = a). Thenexcludingthelastepisodem˜ overH, weprovidelowerboundsofthetotalrotting
t=1 t t H
variationoverH fora(i),denotedbyV (a(i)),inthefollowinglemma.
H
LemmaA.11. UnderE ,givenm˜ ,foranyi∈[m˜G]/{m˜ }wehave
1 H H H
V (a(i))≥δ†/2.
H
Proof. WecaneasilyprovethetheorembyfollowingtheproofstepsinLemmaA.2
35We first consider the case where V = ω(max{T/H3/2,T/H(β+2)/(β+1)}). In this case, we have δ† =
T
Θ(max{(V /T)1/(β+2),(V /T)1/3}). Here, wedefinethepolicyπ aftertimeH suchthatitpullsagoodarm
T T
untilitstotalrottingvariationisequaltoorgreaterthanδ†/2anddoesnotpullasampledbadarm. Wenotethat
defininghowπworksafterH isonlyfortheprooftogetaregretboundovertimehorizonH. Forthelastarma˜
overthehorizonH,itpullsthearmuntilitstotalvariationbecomesmax{δ†/2,V (a˜)}ifa˜isagoodarm. For
H
i∈[mG],j ∈[mB]letVG andVB bethetotalrottingvariationofpullingthegoodarmini-thepisodeandj-th
i i i,j
badarmini-thepisodefromthepolicy,respectively. HerewedefineVG’sandVB’sasfollows:
i i,j
Ifa˜isagoodarm,
 
VG
=V H(a(i)) fori∈[m˜G
H
−1]
,VB
=V H(a(i,j)) fori∈[m˜G H],j ∈[m˜B i,H]
i i,j
max{δ†/2,V (a(i))} fori∈[mG]/[m˜G −1] 0 fori∈[mG]/[m˜G],j ∈[mB].
H H H i
Otherwise,
 
VG
=V H(a(i)) fori∈[m˜G H]
,VB
=V H(a(i,j)) fori∈[m˜G H],j ∈[m˜B i,H]
i i,j
δ†/2 fori∈[mG]/[m˜G] 0 fori∈[mG]/[m˜G −1],j ∈[mB]/[m˜B ].
H H i i,H
Fori ∈ [mG],j ∈ [mB]letnB bethenumberofpullingthegoodarmini-thepisodeandj-thbadarmini-th
i i,j
episodefromthepolicy,respectively. Wedefinen (a)bethetotalamountofpullingarmaoverH. Herewedefine
H
nB ’sasfollows:
i,j

nB
=n H(a(i,j)) fori∈[m˜G H],j ∈[m˜B i,H]
i,j
0 fori∈[mG]/[m˜G],j ∈[mB].
H i
ThenweprovidemG suchthatRB(H)≤RB inthefollowinglemma.
mG
LemmaA.12. UnderE ,whenmG =⌈2V /δ†⌉wehave
1 H,1
RB(H)≤RB .
mG
Proof. WecaneasilyshowthetheorembyfollowingtheproofstepsofLemmaA.3
FromtheresultofLemmaA.12,wesetmG =⌈2V /δ†⌉. Inthefollowing,weanlayzeRB forobtainingaregret
H,1 mG
boundforRB(H).
LemmaA.13. UnderE andpolicyπ,wehave
1
(cid:16) (cid:17)
E[RB ]=O˜ max{V (T/V )(β+1)/(β+2)+(T/V )β/(β+2),V (T/V )2/3+(T/V )1/3} .
mG H,1 T T H,1 T T
Proof. WecaneasilyprovethetheorembyfollowingproofstepsinLemmaA.4. From(31),foranyi ∈ [mG],
j ∈[mB],wehave
i
E[RB ]≤E(cid:2) ∆(a(i,j))nB +VBnB (cid:3)
i,j i,j i,j i,j
=O˜(cid:0) max{1,(δ†)β−1}+E[VB]/(δ†)2(cid:1)
.
i,j
36RecallthatRB = PmG P RB .Withδ† = max{(V /T)1/(β+2),(V /T)1/3}andmG = ⌈2V /δ†⌉,
mG i=1 j∈[mB] i,j T T H,1
i
fromthefactthatmB’sarei.i.d. randomvariableswithgeometricdistributionwithE[mB]=1/(2δ†)β−1,wehave
i i
  
mG
X X
E[R mB G]=OE R iB ,j
i=1j∈[mB]
i
(cid:18) (cid:19)
1
=O˜ (V /δ†+1) max{1,(δ†)β−1}+V /(δ†)2
H,1 (δ†)β H,1
(cid:18) (cid:26) (cid:27) (cid:26) (cid:27)(cid:19)
V V 1 1
=O˜ max H,1 , H,1 +max ,
(δ†)β+1 (δ†)2 (δ†)β δ†
(cid:16) (cid:17)
=O˜ max{V (T/V )(β+1)/(β+2)+(T/V )β/(β+2),V (T/V )2/3+(T/V )1/3} .
H,1 T T H,1 T T
FromRπ1(δ†)(H)=RG(H)+RB(H)andLemmasA.10,A.12,A.13,withδ† =max{(V T/T)1/(β+2),(V T/T)1/3}
wehave
E[Rπ1(δ†)(H)]
(cid:16) n
=O˜ max V (T/V )(β+1)/(β+2)+H(V /T)1/(β+2)+(T/V )β/(β+2),
H,1 T T T
o (cid:17)
V (T/V )2/3+H(V /T)1/3+(T/V )1/3 +H2/3V1/3 .
H,1 T T T H,1
Theaboveregretboundisforthefirstblock. Therefore,bysummingregretsfrom⌈T/H⌉numberofblocks,from
V =ω(max{T/H(β+2)/(β+1),T/H3/2})andH =⌈T1/2⌉,usingHölder’sinequalitywehaveshownthat
T
(cid:18) (cid:19)
T
E[Rπ(T)]=O˜ max{T(β+1)/(β+2)V1/(β+2),T2/3V1/3}+ max{(T/V )β/(β+2),(T/V )1/3}
1 T T H T T
(cid:16) (cid:17)
=O˜ max{T(β+1)/(β+2)V1/(β+2),T2/3V1/3}+max{T(2β+1)/(2β+2),T3/4} . (41)
T T
Now, we consider the case where V = O(max{T/H3/2,T/H(β+2)/(β+1)}). In this case, we have δ† =
T
√
Θ(max{1/ H,1/Hβ+1 1}). FromtheresultofLemmaA.12,bysettingmG = ⌈2V H,1/δ†⌉wehaveRB(H) ≤
RB .
mG
LemmaA.14. UnderE andpolicyπ,wehave
1
(cid:16) (cid:17)
E[RB ]=O˜ max{V (T/V )(β+1)/(β+2)+(T/V )β/(β+2),V (T/V )2/3+(T/V )1/3} .
mG H,1 T T H,1 T T
Proof. WecaneasilyprovethetheorembyfollowingproofstepsinLemmaA.4. From(31),foranyi ∈ [mG],
j ∈[mB],wehave
i
E[RB ]≤E(cid:2) ∆(a(i,j))nB +VBnB (cid:3)
i,j i,j i,j i,j
=O˜(cid:0) max{1,δβ−1}+E[VB]/δ2(cid:1)
.
i,j
37RecallthatRB =PmG P RB .Withδ† =max{1/H1/2,1/H1/(β+1)}andmG =⌈2V /δ†⌉,fromthe
mG i=1 j∈[mB] i,j H,1
i
factthatmB’sarei.i.d. randomvariableswithgeometricdistributionwithE[mB]=1/(2δ†)β −1,wehave
i i
  
mG
X X
E[R mB G]=OE R iB ,j
i=1j∈[mB]
i
(cid:18) (cid:19)
1
=O˜ (V /δ†+1) max{1,(δ†)β−1}+V /(δ†)2
H,1 (δ†)β H,1
(cid:18) (cid:26) (cid:27) (cid:26) (cid:27)(cid:19)
V V 1 1
=O˜ max H,1 , H,1 +max ,
(δ†)β+1 (δ†)2 (δ†)β δ†
(cid:16) (cid:17)
=O˜ V H +max{Hβ/(β+1),H1/2} .
H,1
FromRπ1(δ†)(H)=RG(H)+RB(H)andLemmasA.10,A.12,A.14,withδ† =Θ(max{1/H1/2,1/H1/(β+1)})
wehave
(cid:16) (cid:17)
E[Rπ1(δ†)(H)]=O˜ max{Hβ/(β+1),H1/2}+H2/3V1/3+V H .
H,1 H,1
Therefore,bysummingregretsfrom⌈T/H⌉numberofblocksandfromV =O(max{T/H3/2,T/H(β+2)/(β+1)}),
T
H =⌈T1/2⌉,andthefactthatlengthoftimestepsineachblockisboundedbyH,wehave
 
⌈T/H⌉ ⌈T/H⌉
E[R 1π(T)]=O˜  HT max{Hβ/(β+1),H1/2}+ X H2/3V H1/ ,i3+ X V H,iH
i=1 i=1
(cid:18) (cid:19)
T
=O˜ max{Hβ/(β+1),H1/2}+T2/3V1/3+V H
H T T
(cid:16) (cid:17)
=O˜ max{T/H1/(β+1),T/H1/2}
(cid:16) (cid:17)
=O˜ max{T(2β+1)/(2β+2),T3/4} , (42)
wherethesecondequalitycomesfromHölder’sinequality.
From(41)and(42),wehave
E[Rπ(T)]=O˜(max{T(β+1)/(β+2)V1/(β+2)+T(2β+1)/(2β+2),T2/3V1/3+T3/4}). (43)
1 T T
Upper Bounding E[Rπ(T)]. We observe that the EXP3 is run for ⌈T/H⌉ decision rounds and the number of
2
policies(i.e. π (δ′)forδ′ ∈B)isB. DenotethemaximumabsolutesumofrewardsofanyblockwithlengthH by
i
arandomvariableQ′. WefirstprovideaboundforQ′usingconcentrationinequalities. Foranyblocki,wehave
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) i·H∧T (cid:12) (cid:12) i·H∧T (cid:12) (cid:12) i·H∧T (cid:12)
(cid:12) (cid:12)
(cid:12)
X µ t(aπ t)+η t(cid:12) (cid:12) (cid:12)≤(cid:12) (cid:12)
(cid:12)
X µ t(aπ t)(cid:12) (cid:12) (cid:12)+(cid:12) (cid:12)
(cid:12)
X η t(cid:12) (cid:12) (cid:12). (44)
(cid:12)t=(i−1)H+1 (cid:12) (cid:12)t=(i−1)H+1 (cid:12) (cid:12)t=(i−1)H+1 (cid:12)
DenotebyT thesetoftimestepsinthei-thblock. WedefinetheeventE (i) = {|µ (a)−µ (a)| ≤
i 2 b[s1,s2] [s1,s2]
p T
14log(H)/n (a), foralls ,s ∈T ,s ≤s ,a∈A(i)}andE = E (i).FromLemmaA.21,
[s1,s2] 1 2 i 1 2 2 i∈[⌈T/H⌉] 2
38√
withH =⌈ T⌉wehave
X 2H3 2
P(Ec)≤ ≤ .
2 H6 T
i∈[⌈T/H⌉]
ByassumingthatE holdstrue,wecangetalowerboundforµ (aπ),whichmaybeanegativevaluefromrotting,
2 t t
forgettinganupperboundfor|Pi·H∧T µ (aπ)|. WecanobservethatPi·H∧T µ (aπ)≤H. Therefore
t=(i−1)H+1 t t t=(i−1)H+1 t t
theremainingpartistogetalowerboundforPi·H∧T µ (aπ). Fortheproofsimplicity,weconsiderthatwhen
t=(i−1)H+1 t t
anarmisrotted,thenthearmisconsideredasadifferentarmafterrotting. Forinstance,whenarmaisrottedat
times,thenarmaisconsideredasadifferentarma′afters. Therefore,eacharmcanbeconsideredtobestationary.
ThesetofarmsisdenotedbyL. WedenotebyL+thesetofarmshavingµ (a)≥0fora∈L. Wefirstfocuson
t
thearmsinL/L+.
Letδ denotethemaximumvalueinBsothatδ =1/2. WithE anda∈L/L+,weassumethatt˜(≥t )is
max max 2 2 2
thesmallesttimethatthereexistst ≤s≤t˜ withs=t˜ +1−2l−1forl∈Z+suchthatthefollowingcondition
1 2 2
ismet:
q q
µ (a)+ 12log(H)/n (a)+ 14log(H)/n (a)<1−δ . (45)
t1 [s,t˜ 2] [s,t˜ 2] max
Fromthedefinitionoft˜,weobservethatforgivent˜,thetimesteps,whichsatisfies(45),equalstot .Then,wecan
2 2 1
observethatn (a)=max{⌈C log(H)/(∆ (a)−δ )2⌉,1}forsomeconstantC >0,whichsatisfies(45).
[t1,t˜ 2] 2 t1 max 2
Fromn (a) ≤ n (a), wehaven (a) ≤ max{C log(H)/(∆ (a)−δ )2,1}forsomeconstant
[t1,t2] [t1,t˜ 2] [t1,t2] 3 t1 max
C > 0. Then the regret from arm a, denoted by R(a), is bounded as follows: R(a) = ∆ (a)n (a) ≤
3 t1 [t1,t2]
max{C log(H)∆ (a)/(∆ (a)−δ )2,∆ (a)}.Sincex/(x−δ )2 <1/(1−δ )2 =4foranyx>1,
3 t1 t1 max t1 max max
we have ∆ (a)/(∆ (a)−δ )2 ≤ 4. Then we have R(a) ≤ max{C log(H),∆ (a)} for some constant
t1 t1 max 4 t1
P
C >0. Thenfrom|L|≤H,wehave R(a)≤max{C Hlog(H),H +V }.
4 a∈L/{L+} 4 H,i
P P
Since R(a)≤H,wehave R(a)≤H +max{C Hlog(H),H +V }. ThereforefromR(a)=
a∈L+ a∈L 4 H,i
Pt2(a)
(1−µ (a)),wehave
t=t1(a) t
iH∧T
X
µ (a )≥−max{C Hlog(H),H +V },
t t 4 H,i
t=(i−1)H+1
whichimpliesthatfromV ≤H underAssumption3.5,forsomeC >0,wehave
H,i 5
(cid:12) (cid:12)
(cid:12) i·H∧T (cid:12)
(cid:12) (cid:12)
(cid:12)
X µ t(aπ t)(cid:12) (cid:12) (cid:12)≤max{C 4Hlog(H),H +V H,i}≤C 5Hlog(H).
(cid:12)t=(i−1)H+1 (cid:12)
Nextweprovideaboundfor|Pi·H∧T
η |.WedefinetheeventE
(i)={|Pi·H∧T
η
|≤2p
Hlog(T)}
t=(i−1)H+1 t 3 t=(i−1)H+1 t
T
andE = E (i). FromLemmaA.21,foranyi∈[⌈T/H⌉],wehave
3 i∈[⌈T/H⌉] 3
2
P(E (i)c)≤ .
3 T2
Then,underE ∩E ,with(44),wehave
2 3
p p
Q′ ≤max{C HlogHϱ,H}+2 Hlog(T)≤C HlogH +2 Hlog(T),
5 5
39√
which implies
1/2+Pi·H∧T
r /(C HlogH +4 HlogT) ∈ [0,1] or some large enough C > 0. With
t=(i−1)H t 5
therescalingandtranslationofrewardsinAlgorithm2,fromCorollary3.2. inAueretal.(2002)witheitherof
Assumption3.7or3.8,wehave
E[Rπ(T)|E ∩E
]=O˜(cid:16)
(C HlogH
+2p HlogT)p BT/H(cid:17) =O˜(cid:16)√ HBT(cid:17)
. (46)
2 2 3 5
RegardingtheutilizationoftheregretanalysisofCorollary3.2inAueretal.(2002),wenotethattherewardfor
eachbasecanbedefinedindependentlyofthemaster’saction. Specifically,therewardforeachbaseisdefinedby
therewardsobtainedwhenthecorrespondingbaseisselected,regardlessofthemaster’sactualaction.
NotethattheexpectedregretfromEXP3istriviallyboundedbyo(H2(T/H)) = o(TH)andB = O(log(T)).
Then,with(46),wehave
E[Rπ(T)]=E[Rπ(T)|E ∩E ]P(E ∩E )+E[Rπ(T)|Ec∪Ec]P(Ec∪Ec)
2 2 2 3 2 3 2 2 3 2 3
(cid:16)√ (cid:17)
=O˜ HT +o(TH)(4/T2)
(cid:16)√ (cid:17)
=O˜ HT . (47)
Finally,from(40),(43),and(47),withH =T1/2,wehave
(cid:18) (cid:26) (cid:27)(cid:19)
E[Rπ(T)]=O˜ max V β+1 2Tβ β+ +1 2 +T22 ββ ++ 21 ,V 1 3T2 3 +T3 4 ,
T T
whichconcludestheproof.
A.5.2 Caseofδ† >δ†
V S
Letπ (δ′)forδ′ ∈Bdenotethebasepolicyfortimestepsbetween(i−1)H+1andi·H∧T inAlgorithm2using
i
1−δ′asathreshold. Denotebyaπi(δ′)thepulledarmattimesteptbypolicyπ (δ′).Then,forδ† ∈B,whichis
t i
setlaterforanear-optimalpolicy,wehave
 
T ⌈T/H⌉ i·H∧T
X X X
E[Rπ(T)]=E 1− µ t(aπ t)=E[R 1π(T)]+E[R 2π(T)]. (48)
t=1 i=1 t=(i−1)H+1
where
T ⌈T/H⌉ i·H∧T
Rπ(T)=X
1−
X X
µ
(aπi(δ†))
1 t t
t=1 i=1 t=(i−1)H+1
and
⌈T/H⌉ i·H∧T ⌈T/H⌉ i·H∧T
Rπ(T)= X X µ (aπi(δ†))− X X µ (aπ).
2 t t t t
i=1 t=(i−1)H+1 i=1 t=(i−1)H+1
Note that Rπ(T) accounts for the regret caused by the near-optimal base algorithm π (δ†)’s against the op-
1 i
timal mean reward and Rπ(T) accounts for the regret caused by the master algorithm by selecting a base
2
with δ ∈ B at every block against the base with δ†. In what follows, we provide upper bounds for each re-
gret component. We first provide an upper bound for E[Rπ(T)] by following the proof steps in Theorem 3.3.
1
Then we provide an upper bound for E[Rπ(T)]. We set δ† to be a smallest value in B which is larger than
2
40δ† =max{(S /T)1/(β+1),1/H1/(β+1),(S /T)1/2,1/H1/2}suchthatwehave
S T T
δ† =Θ(max{(S /T)1/(β+1),1/H1/(β+1),(S /T)1/2,1/H1/2}).
T T
UpperBoundingE[Rπ(T)]. Werefertotheperiodstartingfromtimestep(i−1)H +1totimestepi·H ∧T as
1
thei-thblock. Foranyi∈⌈T/H −1⌉,policyπ (δ†)runsoverH timestepsindependenttootherblockssothat
i
eachblockhasthesameexpectedregretandthelastblockhasasmallerorequalexpectedregretthanotherblocks.
Therefore,wefocusonfindingaboundontheregretfromthefirstblockequaltoPH
1−µ
(aπ1(δ†)).
Wedefine
t=1 t t
anarmaasagoodarmif∆ (a)≤δ†/2,anear-goodarmifδ†/2<∆ (a)≤2δ†,andotherwise,aisabadarmat
t t
timet. InA,leta¯ ,a¯ ,...,beasequenceofarms,whichhavei.i.d. meanrewardsfollowing(1). Foranalysis,we
1 2
considerabruptchangeassamplinganewarm. Inotherwords,ifasuddenchangeoccurstoanarmabypulling
thearma,thenthearmisconsideredtobetwodifferentarms;beforeandafterthechange. Thetypeofabruptly
rottedarms(good,near-good,orbad)afterthechangeisdeterminedbytherottedmeanreward. Withoutlossof
generality,weassumethatthepolicysamplesarms,whicharepulledatleastonce,inthesequenceofa¯ ,a¯ ,...,.
1 2
Denote by A(i) the set of sampled arms, which are pulled at least once, in the i-th block, which satisfies
|A(i)| ≤ H. We also define A (i) as a set of arms that have been rotted and pulled at least once in the
S
i-th block, which satisfies |A (i)| ≤ S , where S is defined as the number of abrupt changes in the i-th
S i i
block. Let µ (a) =
Pt2
µ (a)/n (a). We define the event E = {|µ (a) − µ (a)| ≤
[t1,t2] t=t1 t [t1,t2] 1 b[s1,s2] [s1,s2]
p
12log(H)/n (a)forall1≤s ≤s ≤H,a∈A(i)}. FromLemmaA.21,asin(13),wehave
[s1,s2] 1 2
P(Ec)≤2/H2.
1
ForthecasethatE doesnothold,theregretisE[Rπ(H)|Ec]P(Ec)=O(1),whichisnegligiblecomparingwith
1 1 1
theregretwhenE holdstruewhichweshowlater. Therefore,intherestoftheproofweassumethatE holdstrue.
1 1
Inthefollowing,wefirstprovidearegretboundoverthefirstblock.
Forregretanalysis,wedivideRπ1(δ†)(H)intotwoparts,RG(H)andRB(H)correspondingtoregretofgoodor
1
near-goodarms,andbadarmsovertimeT,respectively,suchthatRπ(H) = RG(H)+RB(H). Wecaneasily
1
obtainthat
E[RG(H)]=O(δ†H), (49)
from∆(a)≤2δ†foranygoodornear-goodarmsa.
NowweanalyzeRB(H). WedivideregretRB(H)intotworegretfrombadarmsinA(1)/A (1), denotedby
S
RB,1(H),andregretfrombadarmsinA (1),denotedbyRB,2(H)suchthatRB(H)=RB,1(H)+RB,2(H). We
S
firstanalyzeRB,1(H)inthefollowing. WeconsiderarmsinA(1)/A (1). Fortheproof,weadopttheepisodic
S
approachinKimetal.(2022)forregretanalysis. Inthefollowing, weintroducesomenotation. Hereweonly
considerarmsinA(1)/A (1)sothatthefollowingnotationisdefinedwithoutconsidering(rotted)armsinA (1).
S S
Givenapolicysamplingarmsinthesequenceorder,letmG bethenumberofsamplesofdistinctgoodarmsand
mB bethenumberofconsecutivesamplesofdistinctbadarmsbetweenthei−1-standi-thsampleofagoodarm
i
amongmG goodarms. Werefertotheperiodstartingfromsamplingthei−1-stgoodarmbeforesamplingthei-th
41goodarmasthei-thepisode. ObservethatmB,...,mB ’sarei.i.d. randomvariableswithgeometricdistribution
1 mG
withparameter2δ†,conditionalonthevalueofmG. Therefore,P(mB =k)=(1−2δ†)k2δ†,fori=1,...,mG.
i
Definem˜G tobethetotalnumberofsamplesofagoodarmbythepolicyπ (δ†)overthehorizonH andm˜B
H 1 i,H
tobethenumberofselectionsofabadarminthei-thepisodebythepolicyπoverthehorizonH. Fori∈[m˜G],
H
j ∈[m˜B ],letn˜G bethenumberofpullsofthegoodarminthei-thepisodeandn˜B bethenumberofpullsofthe
i,H i i,j
j-thbadarminthei-thepisodebythepolicyπ (δ†)overthehorizonH. Leta˜bethelastsampledarmovertime
1
horizonH byπ (δ†).
1
Withaslightabuseofnotation, weuseπ (δ†)foramodifiedstrategyafterH. Underapolicyπ (δ†), letRB
1 1 i,j
betheregret(summationofmeanrewardgaps)contributedbypullingthej-thbadarminthei-thepisode. Then
letRB =PmG P RB ,whichistheregretfrominitiallybadarmsovertheperiodofmG episodes. For
mG i=1 j∈[mB] i,j
i
gettingRB ,herewedefinethepolicyπ (δ†)afterH suchthatitpullsH amountsforagoodarmandzerofora
mG 1
badarm. AfterH wecanassumethattherearenoabruptchanges. Forthelastarma˜overthehorizonH,itpulls
thearmuptoH amountsifa˜isagoodarmandn˜G <H. Fori∈[mG],j ∈[mB]letnG andnB bethenumber
m˜G i i i,j
H
ofpullingthegoodarmini-thepisodeandj-thbadarmini-thepisodeunderπ,respectively. HerewedefinenG’s
i
andnB ’sasfollows:
i,j
Ifa˜isagoodarm,

n˜G
i
fori∈[m˜G
H
−1] 
n˜B fori∈[m˜G],j ∈[m˜B ]
nG = H fori=m˜G ,nB = i,j H i,H
i 0 fori∈[mH
G]/[m˜G]
i,j 0 fori∈[mG]/[m˜G H],j ∈[mB
i
]/[m˜B i,H].
H
Otherwise,

n˜G
i
fori∈[m˜G H] 
n˜B fori∈[m˜G],j ∈[m˜B ]
nG = H fori=m˜G +1 ,nB = i,j H i,H
i 0 fori∈[mH
G]/[m˜G +1]
i,j 0 fori∈[mG]/[m˜G
H
−1],j ∈[mB
i
]/[m˜B i,H].
H
Withaslightabuseofnotation,wedefineS tobethenumberofabruptchangesini-thblock. Then,weshowthatif
i
mG =S ,thenRB,1(H)≤RB .
1 mG
LemmaA.15. UnderE ,whenmG =S wehave
1 1
RB,1(H)≤RB .
mG
Proof. ThereareatmostS −1numberofabruptchangesoverthefirstblockH. Weconsidertwocases;thereare
1
S −1abruptchangesbeforesamplingS -thgoodarmornot. Forthefirstcase,ifπ (δ†)samplestheS -thgood
1 1 1 1
armandthereareS −1numberofabruptchangesbeforesamplingthegoodarm,thenitcontinuestopullthegood
1
armforH roundsfromE andthedefinitionofπ (δ†)afterH.
1 1
Nowweconsiderthesecondcase. Ifπ (δ†)samplestheS -thgoodarmbeforeT andthereisatleastoneabrupt
1 1
changeaftersamplingthearm,thenbeforesamplingtheS -thgoodarm,theremustexisttwoconsecutivegood
1
42armssuchthatthereisnoabruptchangebetweensamplingthetwogoodarms. Thisisacontractionbecauseπ (δ†)
1
mustpullthefirstgoodarmuptoH underE andS −1-stabruptchangemustoccurafterH.
1 1
Therefore,consideringthefirstcase,whenmG =S +1,wehave
1
X
nG ≥H,
i
i∈[mG]
whichimpliesRB(H)≤RB .
mG
Fromtheabovelemma,wesetmG =S andanalyzeRB togetaboundforRB,1(H)inthefollowinglemma.
1 mG
LemmaA.16. UnderE andpolicyπ (δ†),wehave
1 1
E[RB ]=O˜(cid:0) S max{1/(δ†)β,1/δ†}(cid:1) .
mG 1
Proof. WecanshowthistheorembyfollowingtheproofstepsinLemmaA.8.
NowweanalyzeRB,2(H)inthefollowinglemma. WedenotebyV acumulativeamountofrottingratesinthe
H
firstblock.
LemmaA.17. UnderE andpolicyπ,wehave
1
E(cid:2) RB,2(H)(cid:3) =O˜ max{S
/δ†,XS1
ρ
}!
.
1 t(s)
s=1
Proof. WecanshowthistheorembyfollowingtheproofstepsinLemmaA.9.
FromLemmasA.15,A.16,A.17,wehave
E[RB(H)]=E[RB,1(H)]+E[RB,2(H)]=O˜(S
max{1/(δ†)β,1/δ†}+XS1
ρ ) (50)
1 t(s)
s=1
FromRπ(H)=RG(H)+RB(H),(49),and(50),wehave
1
E[Rπ1(δ†)]=O˜ Hδ†+S
max{1/(δ†)β,1/δ†}+XS1
ρ
!
.
mG 1 t(s)
s=1
Theaboveregretisforthefirstblock. Therefore,bysummingregretsover⌈T/H⌉numberofblocks,wehave
shownthat
E[Rπ(T)]=O˜(Tδ†+(T/H +S
)max{1/(δ†)β,1/δ†}+XST
ρ ). (51)
1 T t(s)
s=1
UpperboundingE[Rπ(T)]. ByfollowingtheproofstepsinTheorem3.9,wehave
2
43(cid:16)√ (cid:17)
E[Rπ(T)]=O˜ HT . (52)
2
Finally,from(48),(51),and(52),withH =T1/2and
δ† =Θ(max{(S /T)1/(β+1),1/H1/(β+1),(S /T)1/2,1/H1/2}),wehave
T T
E[Rπ(T)]=O˜ Tδ†+(T/H +S
)max{1/(δ†)β,1/δ†}+√
HT
+XST
ρ
!
T t(s)
s=1
=O˜ Tδ†+max{T/H,S
}max{1/(δ†)β,1/δ†}+√
HT
+XST
ρ
!
T t(s)
s=1
=O˜
2Tδ†+√
HT
+XST
ρ
!
t(s)
s=1
=O˜(cid:16) max{S1/(β+1)Tβ/(β+1)+T(2β+1)/(2β+2),p
S T +T3/4,V
}(cid:17)
,
T T T
whichconcludestheproof.
A.6 ProofofTheorem4.1: RegretLowerBoundforSlowlyRottingRewards
WefirstconsiderthecasewhenV = Θ(T). Recallthat∆ (a) = 1−µ (a). Thenforanyrandomlysampled
T 1 1
a ∈ A,wehaveE[µ (a)] ≥ yP(µ (a) ≥ y) = yP(∆ (a) < 1−y)fory ∈ [0,1]. Thenwithy = 1/2,wehave
1 1 1
E[µ (a)] ≥ (1/2)P(∆ (a) < (1/2)) = Θ(1)fromconstantβ > 0and(1). ThenwithE[µ (a)] ≤ 1, wehave
1 1 1
E[µ (a)] = Θ(1). Wethenthinkofapolicyπ′ thatrandomlysamplesanewarmandpullsitonceeveryround.
1
SinceE[µ (a)]=Θ(1)foranyrandomlysampleda,wehaveE[Rπ′(T)]=Θ(T).Next,wethinkofanypolicyπ′′
1
exceptπ′. Thenanypolicyπ′′mustpullanarmaatleasttwice. Lett′andt′′betheroundswhenthepolicypulls
arma. Ifweconsiderρ =V thensuchpolicyhasΩ(V )regretbound. SinceV =Θ(T),anyalgorithmhas
t′ T T T
Ω(T)intheworstcase. Thereforewecanconcludethatanyalgorithmincludingπ′hasaregretboundofΩ(T)in
theworstcase,whichconcludestheproofforV =Θ(T).
T
NowwethinkofthecasewhereV =o(T). Forthelowerbound,weadopttheproofmethodologyofTheorem1
T
inKimetal.(2022)bymakingnecessaryadjustmentstoaccommodateV andβ. Wefirstcategorizearmsaseither
T
badorgoodaccordingtotheirinitialmeanrewardvalues. Forthecategorization,weutilizetwothresholdsinthe
proofasfollows. Consider0<γ <c<1forγ,whichwillbespecified,andaconstantc. Thenthevalueof1−γ
representsathresholdvalueforidentifyinggoodarms,while1−cservesasthethresholdforidentifyingbadarms.
Werefertoarmsasatisfyingµ (a)≤1−cas‘bad’armsandarmsasatisfyingµ (a)>1−γas‘good’arms. We
1 1
alsoconsiderasequenceofarmsinAdenotedbya¯ ,a¯ ,.... Givenapolicyπ,withoutlossofgenerality,wecan
1 2
assumethatπselectsarmsaccordingtotheorderofa¯ ,a¯ ,.... Fortherottingrates,wedefineρ=V /(T −1).
1 2 T
Thenweconsiderρ =ρforallt∈[T
−1]sothatPT−1ρ
=V .
t t=1 t T
CaseofV T = O(1/T1/(β+1)): WhenV T = O(1/T1/(β+1)),thelowerboundoforderTβ+β 1 forthestationary
case,fromTheorem3inWangetal.(2009),istightenoughforthenon-stationarycase. FromTheorem3inWang
44etal.(2009),wehave
E[Rπ(T)]=Ω(Tβ+β
1). (53)
Wenotethateventhoughthemeanrewardsarerottinginoursetting,Theorem3inWangetal.(2009)remains
applicablewithoutrequiringanyalterationsintheproofsprovidingatightregretboundforthenear-stationarycase.
Forthesakeofcompleteness,weprovidetheproofofthetheoreminthefollowing. LetK denotethenumberof
1
badarmsathatsatisfyµ (a) ≤ 1−cbeforesamplingthefirstgoodarm,whichsatisfiesµ (a) > 1−γ,inthe
1 1
sequenceofarmsa¯ ,a¯ ,....Letµbetheinitialmeanrewardofthebestarmamongthesampledarmsbyπover
1 2
timehorizonT. Thenforsomeκ>0,wehave
Rπ(T)=Rπ(T)1(µ≤1−γ)+Rπ(T)1(µ>1−γ)
≥Tγ1(µ≤1−γ)+K c1(µ>1−γ)
1
≥Tγ1(µ≤1−γ)+κc1(µ>1−γ,K ≥κ). (54)
1
Bytakingexpectationsonthebothsidesin(54)andsettingκ=Tγ/c,wehave
E[Rπ(T)]≥TγP(µ≤1−γ)+κc(P(µ>1−γ)−P(K <κ))=cκP(K ≥κ).
1 1
We observe that K follows a geometric distribution with success probability P(µ (a) > 1−γ)/p(µ (a) ∈/
1 1 1
(1−c,1−γ])=γ ≤C γβ/(1+C γβ−C cβ)forsomeconstantsC ,C ,C >0from(1),inwhichthesuccess
1 2 3 1 2 3
probabilityistheprobabilityofsamplingagoodarmgiventhatthearmiseitheragoodorbadarm. Hereweseta
constant0<c<1satisfying1−C 3cβ >0. Thenbysettingγ =1/Tβ+1 1 withκ=Tβ+β 1/c,forsomeconstant
C >0wehave
(cid:18) β (cid:19)
E[Rπ(T)]≥cκ(1−γ)κ =Ω Tβ+β 1(1−Cγβ)Tβ+1/c =Ω(Tβ+β 1),
wherethelastequalityisobtainedfromlogx≥1−1/xforallx>0.
CaseofV = ω(1/T1/(β+1))andV = o(T): WhenV = ω(1/T1/(β+1)), however, thelowerboundofthe
T T T
stationarycaseisnottightenough. HereweprovidetheproofforthelowerboundofV1/(β+2)T(β+1)/(β+2) for
T
thecaseofV =ω(1/T1/(β+1)). LetK denotethenumberof“bad"armsathatsatisfyµ (a)≤1−cbefore
T m 1
samplingm-th“good"arm, whichsatisfiesµ (a) > 1−γ, inthesequenceofarmsa¯ ,a¯ ,....LetN bethe
1 1 2 T
numberofsampledgoodarmsasuchthatµ (a)>1−γ untilT.
1
WecandecomposeRπ(T)intotwopartsasfollows:
Rπ(T)=Rπ(T)1(N <m)+Rπ(T)1(N ≥m). (55)
T T
Wesetm = ⌈(1/2)T1/(β+2)V(β+1)/(β+2)⌉andγ = (V /T)1/(β+2) withV = o(T). Forthefirsttermin(55),
T T T
Rπ(T)1(N < m),weconsiderthefactthattheminimalregretisobtainedfromthesituationwherethereare
T
m−1armswhosemeanrewardsare1. Insuchacase,theoptimalpolicymustsamplethebestm−1armsuntil
theirmeanrewardsbecomebelowthethreshold1−γ (step1)andthensamplesthebestarmateachtimeforthe
remainingtimesteps(step2). Thenumberoftimeseacharmneedstobepulledforthebestm−1armsuntiltheir
45meanrewardfallsbelow1−γ isboundedfromabovebyγ/ϱ+1=γ((T −1)/V )+1. Therefore,theregret
T
fromstep2isR =Ω((T −mγ(T/V ))γ)=Ω(T(β+1)/(β+2)V1/(β+2))inwhichtheoptimalpolicypullsarms
T T
whichmeanrewardsarebelow1−γ fortheremainingtimeafterstep1. Therefore,wehave
Rπ(T)1(N <m)=Ω(R1(N <m))=Ω(T(β+1)/(β+2)V1/(β+2)1(N <m)). (56)
T T T T
Forgettingalowerboundofthesecondtermin(55),Rπ(T)1(N ≥m),weusetheminimumnumberofsampled
T
armsathatsatisfyµ (a)≤1−c.WhenN ≥mandK ≥κ,thepolicysamplesatleastκnumberofdistinct
1 T m
armsasatisfyingµ (a)≤1−cuntilT. Therefore,wehave
1
Rπ(T)1(N ≥m)≥cκ1(N ≥m,K ≥κ). (57)
T T m
√
Wehaveγ = Θ(γβ)from(1)withconstantβ > 0. Bysettingκ = m/γ −m− m/γ, withV = o(T)and
T
constantβ >0,wehave
κ=Θ(T(β+1)/(β+2)V1/(β+2)). (58)
T
Thenfrom(56),(57),and(58),wehave
E[Rπ(T)]=Ω(T(β+1)/(β+2)V1/(β+2)P(N <m)+T(β+1)/(β+2)V1/(β+2)P(N ≥m,K ≥κ))
T T T T m
≥Ω(T(β+1)/(β+2)V1/(β+2)P(K ≥κ)). (59)
T m
NextweprovidealowerboundforP(K ≥κ).ObservethatK followsanegativebinomialdistributionwith
m m
msuccessesandthesuccessprobabilityP(µ (a)>1−γ)/P(µ (a)∈/ (1−c,1−γ])=γ,inwhichthesuccess
1 1
probabilityistheprobabilityofsamplingagoodarmgiventhatthearmiseitheragoodorbadarm. Inthefollowing
lemma,weprovideaconcentrationinequalityforK .
m
LemmaA.18. Forany1/2+γ/m<α<1,
P(K ≥αm(1/γ)−m)≥1−exp(−(1/3)(1−1/α)2(αm−γ)).
m
Proof. LetX fori>0bei.i.d. Bernoullirandomvariableswithsuccessprobabilityγ.FromSection2inBrown
i
(2011),wehave
(cid:4) (cid:5) 
αm1
(cid:18) (cid:22) 1(cid:23) (cid:19) Xγ
P K ≤ αm −m =P X ≥m. (60)
m γ  i 
i=1
46From(60)andLemmaA.20,forany1/2+γ/m<α<1wehave
(cid:18) (cid:19) (cid:18) (cid:22) (cid:23) (cid:19)
1 1
P K ≤αm −m =P K ≤ αm −m
m γ m γ
(cid:4) (cid:5) 
αm1
γ
X
=P X ≥m
 i 
i=1
(cid:18) (1−1/α)2 (cid:22) 1(cid:23) (cid:19)
≤exp − αm γ
3 γ
(cid:18) (1−1/α)2 (cid:19)
≤exp − (αm−γ) ,
3
inwhichthefirstinequalitycomesfromLemmaA.20,whichconcludestheproof.
√
FromLemmaA.18withα=1−1/ mandlargeenoughT,wehave
1 √ (cid:18) 1 (cid:19)2!
P(K ≥κ)≥1−exp − (m− m−γ) √
m 3 m−1
1 √ (cid:18) 1 (cid:19)2!
≥1−exp − (m− m) √
6 m−1
√
(cid:18) (cid:19)
1 m
=1−exp − √
6 m−1
≥1−exp(−1/6). (61)
Therefore,from(59)and(61),wehave
E[Rπ(T)]=Ω(T(β+1)/(β+2)V1/(β+2)). (62)
T
Finally,from(53)and(62),weconcludethatforanypolicyπ,wehave
(cid:16) n o(cid:17)
E[Rπ(T)]=Ω max T(β+1)/(β+2)V1/(β+2),Tβ+β 1 .
T
A.7 ProofofTheorem4.2: RegretLowerBoundforAbruptlyRottingRewards
First,wedealwiththecasewhenS =1orS =Θ(T). WhenS =1(implyingV =0),fromthedefinition,
T T T T
√
the problem becomes stationary without rotting instances, which implies E[Rπ(T)] = Ω( T) from Theorem
3 in Wang et al. (2009). When S = Θ(T), we consider that rotting occurs for the first S −1 rounds with
T T
ρ = 1 for all t ∈ [S −1]. Then it is always beneficial to pull new arms every round until S −1 rounds
t T T
becausethemeanrewardsofrottedarmsarebelow0andthoseofnon-rottedarmsliein[0,1]. Thismeansthat
any ideal policy samples a new arm and pulls it every round until S −1. Then for any randomly sampled
T
a ∈ A, we have E[µ (a)] ≥ yP(µ (a) ≥ y) = yP(∆ (a) < 1−y) for y ∈ [0,1]. Then with y = 1/2, we
1 1 1
have E[µ (a)] ≥ (1/2)P(∆ (a) < (1/2)) = Θ(1) from constant β > 0 and (1). Then with E[µ (a)] ≤ 1,
1 1 1
47we have E[µ (a)] = Θ(1). Since E[µ (a)] = Θ(1) for any randomly sampled a ∈ A, any ideal policy has
1 1
E[Rπ(T)]≥PST
E[µ (a)]=Ω(S )=Ω(T),whichconcludestheproofforS =Θ(T).
i=1 1 T T
Now we consider the case of S = o(T) and S ≥ 2. We first provide a regret bound with respect to the
T T
cumulativerottingamountofV . Wefirstthinkofapolicyπthatrandomlysamplesanewarmandpullsitonce
T
every round. Then for any randomly sampled a ∈ A, we have E[µ (a)] = Θ(1). Then from constant β > 0,
1
E[Rπ(T)]=Ω(T). Thentherealwaysexistsρ ’ssatisfyingV =T,whichimpliesE[Rπ(T)]=Ω(V ). Nowwe
t T T
thinkofanynontrivialalgorithmwhichmustpullanarmaatleasttwice. Lett′ andt′′ betheroundswhenthe
policypullsarma(t′ < t′′). Ifweconsiderρ = V andρ = 0fort ∈ [T −1]/{t′}inwhichPT−1ρ = V
t′ T t t=1 t T
and1+PT−1ρ 1(ρ ̸=0)≤S ,thensuchpolicyhasΩ(V )regretboundbecauseitpullstherottedarmaby
t=1 t t T T
ρ attimet′′. Therefore,foranypolicyπ,therealwaysexistρ ’ssuchthat
t′ t
E[Rπ(T)]=Ω(V ). (63)
T
Next,fortheregretboundwithrespecttoS ,wefollowtheproofstepsinTheorem4.1. Wefirstcategorizearmsas
T
eitherbadorgoodaccordingtotheirinitialmeanrewardvalues. Forthecategorization,weutilizetwothresholdsin
theproofasfollows. Consider0<γ <c<1forγ,whichwillbespecified,andaconstantc. Thenthevalueof
1−γ representsathresholdvalueforidentifyinggoodarms,while1−cservesasthethresholdforidentifyingbad
arms. Werefertoarmsasatisfyingµ (a)≤1−cas‘bad’armsandarmsasatisfyingµ (a)>1−γ as‘good’
1 1
arms. WealsoconsiderasequenceofarmsinAdenotedbya¯ ,a¯ ,.... Givenapolicyπ,withoutlossofgenerality,
1 2
wecanassumethatπselectsarmsaccordingtotheorderofa¯ ,a¯ ,....
1 2
LetK denotethenumberofbadarmsathatsatisfyµ (a)≤1−cbeforesamplingm-thgoodarm,whichsatisfies
m 1
µ (a) > 1−γ, in the sequence of arms a¯ ,a¯ ,.... Let N be the number of sampled good arms a such that
1 1 2 T
µ (a)>1−γ untilT.
1
WecandecomposeRπ(T)intotwopartsasfollows:
Rπ(T)=Rπ(T)1(N <m)+Rπ(T)1(N ≥m). (64)
T T
Wesetm = S andγ = (S /T)1/(β+1) withS = o(T). Forgettingalowerboundforthefirsttermin(64),
T T T
Rπ(T)1(N < m),weconsiderthefactthattheminimalregretisobtainedfromthesituationwherethereare
T
m−1armswhosemeanrewardsare1. Insuchacase,theoptimalpolicymustsamplethebestm−1armsuntil
theirmeanrewardsbecomeequaltoorbelowthethresholdvalueof1−γ (step1)andthensamplesthebestarmat
eachtimefortheremainingtimesteps(step2).Instep1,whentheoptimalpolicypullsanoptimalarm,wecanthink
ofthecasewhenthemeanrewardofthearmisabruptlyrottedtothevalueof1−γ. Thisimpliesthattherequired
numberofroundsforstep1ism−1. Theregretfromstep2isR=Ω((T −m+1)γ)=Ω(S1/(β+1)Tβ/(β+1)),
T
inwhichtheoptimalpolicypullsarmswhichmeanrewardsarebeloworequalto1−γ fortheremainingtimeafter
step1. Therefore,wehave
Rπ(T)1(N <m)=Ω(R1(N <m))=Ω(S1/(β+1)Tβ/(β+1)1(N <m)). (65)
T T T T
Forgettingtheabove,wenotethattherealwaysexistsρ ’ssatisfyingV =O(γm)=o(T)forthefirststepand
t T
V =0forthesecondstep,whichimpliesV ≤T. Suchρ ’scanbeconsideredforthebelow. Forgettingalower
T T t
48boundofthesecondtermin(64),Rπ(T)1(N ≥m),weusetheminimumnumberofsampledarmsathatsatisfy
T
µ (a) ≤ 1−c.WhenN ≥ mandK ≥ κ,thepolicysamplesatleastκnumberofdistinctarmsasatisfying
1 T m
µ (a)≤1−cuntilT. Therefore,wehave
1
Rπ(T)1(N ≥m)≥cκ1(N ≥m,K ≥κ). (66)
T T m
Wesetγ =P(µ (a)>1−γ)/p(µ (a)∈/ (1−c,1−γ]). Thenwehaveγ =Θ(γβ)from(1)withconstantβ >0.
1 1
√
Bysettingκ=m/γ−m−m/(γ m+3),withS =o(T)andconstantβ >0,wehave
T
κ=Θ(S1/(β+1)Tβ/(β+1)). (67)
T
Thenfrom(65),(66),and(67),wehave
E[Rπ(T)]=Ω(S1/(β+1)Tβ/(β+1)P(N <m)+S1/(β+1)Tβ/(β+1)P(N ≥m,K ≥κ))
T T T T m
≥Ω(S1/(β+1)Tβ/(β+1)P(K ≥κ)). (68)
T m
NextweprovidealowerboundforP(K ≥κ).ObservethatK followsanegativebinomialdistributionwith
m m
msuccessesandthesuccessprobabilityP(µ (a)>1−γ)/P(µ (a)∈/ (1−c,1−γ])=γ,inwhichthesuccess
1 1
probabilityistheprobabilityofsamplingagoodarmgiventhatthearmiseitheragoodorbadarm. Werecall
LemmaA.18foraconcentrationinequalityforK inthefollowing.
m
LemmaA.19. Forany1/2+γ/m<α<1,
P(K ≥αm(1/γ)−m)≥1−exp(−(1/3)(1−1/α)2(αm−γ)).
m
√
FromLemmaA.19withα=1−1/ m+3andlargeenoughT,wehave
1 m
(cid:18)
1
(cid:19)2!
P(K ≥κ)≥1−exp − (m− √ −γ) √
m 3 m+3 m+3−1
1 m
(cid:18)
1
(cid:19)2!
≥1−exp − (m− √ ) √
6 m+3 m+3−1
√
(cid:18) (cid:19)
1 m m+3
=1−exp − √
6m+3 m+3−1
≥1−exp(−1/24), (69)
√ √
where the last inequality comes from m/(m+3) = (S )/(S +3) ≥ 1/4 and m+3/( m+3−1) ≥ 1.
T T
Therefore,from(68)and(69),wehave
E[Rπ(T)]=Ω(S1/(β+1)Tβ/(β+1)). (70)
T
Overallfrom(63)and(70),foranyπ,thereexistρ ’ssuchthatE[Rπ(T)]=Ω(max{S1/(β+1)Tβ/(β+1),V }).
t T T
49A.8 LemmasforConcentrationInequalities
Lemma A.20 (Theorem 6.2.35 in Tsun (2020)). Let X ,...,X be identical independent Bernoulli random
1 n
variables. Then,for0<ν <1,wehave
P
Xn
X
≥(1+ν)E" Xn
X
#! ≤exp(cid:18) −ν2E[Pn
i=1X
i](cid:19)
.
i i 3
i=1 i=1
LemmaA.21(Corollary1.7inRigolletandHütter(2015)). LetX ,...,X beindependentrandomvariableswith
1 n
σ-sub-Gaussiandistributions. Then,foranya=(a ,...,a )⊤ ∈Rnandt≥0,wehave
1 n
Xn ! (cid:18) t2 (cid:19) Xn ! (cid:18) t2 (cid:19)
P a X >t ≤exp − andP a X <−t ≤exp − .
i i 2σ2∥a∥2 i i 2σ2∥a∥2
i=1 2 i=1 2
50