A Multimodal Automated Interpretability Agent
TamarRottShaham1* SarahSchwettmann1*
FranklinWang1 AchyutaRajaram1 EvanHernandez1 JacobAndreas1 AntonioTorralba1
Abstract
This paper describes MAIA, a Multimodal
AutomatedInterpretabilityAgent. MAIAisasys-
temthatusesneuralmodelstoautomateneural
modelunderstandingtaskslikefeatureinterpre-
tation and failure mode discovery. It equips a
pre-trainedvision-languagemodelwithasetof
tools that support iterative experimentation on
subcomponentsofothermodelstoexplaintheir
behavior. Theseincludetoolscommonlyusedby
humaninterpretabilityresearchers: forsynthesiz-
ingandeditinginputs,computingmaximallyac-
tivatingexemplarsfromreal-worlddatasets,and
summarizinganddescribingexperimentalresults.
Interpretability experiments proposed by MAIA
composethesetoolstodescribeandexplainsys-
tembehavior. WeevaluateapplicationsofMAIA
tocomputervisionmodels. Wefirstcharacterize
MAIA’sabilitytodescribe(neuron-level)features
inlearnedrepresentationsofimages. Acrosssev-
eral trained models and a novel dataset of syn-
thetic vision neurons with paired ground-truth
descriptions, MAIA produces descriptions com-
Figure1.MAIAframework. MAIAautonomouslyconductsexper-
parable to those generated by expert human ex-
imentsonothersystemstoexplaintheirbehavior.
perimenters. We then show that MAIA can aid
intwoadditionalinterpretabilitytasks: reducing researchers—involvingexploratorydataanalysis,formula-
sensitivitytospuriousfeatures,andautomatically tionofhypotheses,andcontrolledexperimentation(Nushi
identifyinginputslikelytobemis-classified.† et al., 2018; Zhang et al., 2018). As a consequence, this
kindofunderstandingisslowandexpensivetoobtaineven
aboutthemostwidelyusedmodels.
1.Introduction
Recentworkonautomatedinterpretability(e.g.Hernan-
Understanding of a neural model can take many forms. dezetal.,2022;Billsetal.,2023;Schwettmannetal.,2023)
Given an image classifier, for example, we may wish to has begun to address some of these limitations by using
recognizewhenandhowitreliesonsensitivefeatureslike learnedmodelsthemselvestoassistwithmodelunderstand-
raceorgender,identifysystematicerrorsinitspredictions, ingtasks—forexample,byassigningnaturallanguagede-
orlearnhowtomodifythetrainingdataandmodelarchitec- scriptions to learned representations, which may then be
turetoimproveaccuracyandrobustness. Today,thiskind usedtosurfacefeaturesofconcern. Butcurrentmethods
ofunderstandingrequiressignificanteffortonthepartof areuseful almostexclusively as toolsfor hypothesisgen-
eration;theycharacterizemodelbehavioronalimitedset
*Equal contribution. 1 MIT CSAIL. Correspondence to
ofinputs,andareoftenlow-precision(Huangetal.,2023).
Tamar Rott Shaham: tamarott@mit.edu, Sarah Schwettmann:
Howcanwebuildtoolsthathelpusersunderstandmodels,
schwett@mit.edu
†Website:https://multimodal-interpretability. whilecombiningtheflexibilityofhumanexperimentation
csail.mit.edu/maia withthescalabilityofautomatedtechniques?
1
4202
rpA
22
]IA.sc[
1v49341.4042:viXraAMultimodalAutomatedInterpretabilityAgent
Figure2.MAIAinterpretationsoffeaturesinthewild.MAIAiterativelywritesprogramsthatcomposepretrainedmodulestoconduct
experimentsonsubcomponentsofothersystems.GeneratedcodeisexecutedwithaPythoninterpreterandtheoutputs(shownabove,
neuronactivationvaluesoverlaidinwhite,masksthresholdedat0.95percentileofactivations)arereturnedtoMAIA.Equippedwithan
APIcontainingcommoninterpretabilitytools,MAIAautonomouslydesignsexperimentsthatshowsophisticatedscientificreasoning.
2AMultimodalAutomatedInterpretabilityAgent
This paper introduces a prototype system we call the ronsuseexemplarsoftheirbehaviorasexplanation,either
Multimodal Automated Interpretability Agent (MAIA), by visualizing features they select for (Zeiler & Fergus,
whichcombinesapretrainedvision-languagemodelback- 2014;Girshicketal.,2014;Karpathyetal.,2015;Mahen-
bonewithanAPIcontainingtoolsdesignedforconducting dran&Vedaldi,2015;Olahetal.,2017)orautomatically
experimentsondeepnetworks. MAIAispromptedwithan categorizingmaximally-activatinginputsfromreal-world
explanationtask(e.g. “describethebehaviorofunit487in datasets(Bauetal.,2017;2020;Oikarinen&Weng,2022;
layer4ofCLIP”or“inwhichcontextsdoesthemodelfailto Dalvietal.,2019). Earlyapproachestotranslatingvisual
classifylabradors?”) anddesignsaninterpretabilityex- exemplarsintolanguagedescriptionsdrewlabelsfromfixed
perimentthatcomposesexperimentalmodulestoanswerthe vocabularies(Bauetal.,2017),orproduceddescriptionsin
query. MAIA’smodulardesign(Figure1)enablesflexible theformofprograms(Mu&Andreas,2021).
evaluationofarbitrarysystemsandstraightforwardincorpo-
rationofnewexperimentaltools. Section3describesthe
Automatedinterpretability. Laterworkonautomatedin-
currenttoolsinMAIA’sAPI,includingmodulesforsynthe-
terpretabilityproducedopen-endeddescriptionsoflearned
sizing and editing novel test images, which enable direct
featuresintheformofnaturallanguagetext,eithercurated
hypothesistestingduringtheinterpretationprocess.
fromhumanlabelers(Schwettmannetal.,2021)orgener-
WeevaluateMAIA’sabilitytoproducepredictiveexplana- ated directly by learned models (Hernandez et al., 2022;
tionsofvisionsystemcomponentsusingtheneurondescrip- Bills et al., 2023; Gandelsman et al., 2024). However,
tionparadigm(Bauetal.,2017;2020;Oikarinen&Weng, theselabelsareoftenunreliableascausaldescriptionsof
2022;Billsetal.,2023;Singhetal.,2023;Schwettmann model behavior without further experimentation (Huang
etal.,2023)whichappearsasasubroutineofmanyinter- etal.,2023). Schwettmannetal.(2023)introducedtheAu-
pretabilityprocedures. Weadditionallyintroduceanovel tomatedInterpretabilityAgentprotocolforexperimentation
datasetofsyntheticvisionneuronsbuiltfromanopen-set onblack-boxsystemsusingalanguagemodelagent,though
conceptdetectorwithground-truthselectivityspecifiedvia thisagentoperatedpurelyonlanguage-basedexplorationof
textguidance. InSection4,weshowthatMAIAdesriptions inputs,whichlimiteditsactionspace. MAIAsimilarlyper-
ofbothsyntheticneuronsandneuronsinthewildaremore formsiterativeexperimentationratherthanlabelingfeatures
predictive of behavior than baseline description methods, inasinglepass,buthasaccesstoalibraryofinterpretability
andinmanycasesonparwithhumanlabels. toolsaswellasbuilt-invisioncapabilities. MAIA’smodu-
lardesignalsosupportsexperimentsatdifferentlevelsof
MAIAalsoautomatesmodel-levelinterpretationtaskswhere
granularity, ranging from analysis of individual features
descriptionsoflearnedrepresentationsproduceactionable
to sweeps over entire networks, or identification of more
insightsaboutmodelbehavior. Weshowinaseriesofex-
complexnetworksubcomponents(Conmyetal.,2023).
perimentsthatMAIA’siterativeexperimentalapproachcan
beappliedtodownstreammodelauditingandeditingtasks
includingspuriousfeatureremovalandbiasidentificationin
Languagemodelagents. Modernlanguagemodelsare
atrainedclassifier. Bothapplicationsdemonstratetheadapt-
promising foundation models for interpreting other net-
abilityoftheMAIAframeworkacrossexperimentalsettings:
worksduetotheirstrongreasoningcapabilities(OpenAI,
novelend-usecasesaredescribedintheuserprompttothe
2023a). Thesecapabilitiescanbeexpandedbyusingthe
agent,whichcanthenuseitsAPItocomposeprogramsthat
LMasanagent,whereitispromptedwithahigh-levelgoal
conducttask-specificexperiments. Whiletheseapplications
and has the ability to call external tools such as calcula-
showpreliminaryevidencethatprocedureslikeMAIAwhich
tors,searchengines,orothermodelsinordertoachieveit
automatebothexperimentationanddescriptionhavehigh
(Schicketal.,2023;Qinetal.,2023). Whenadditionally
potentialutilitytotheinterpretabilityworkflow,wefindthat
promptedtoperformchain-of-thoughtstylereasoningbe-
MAIAstillrequireshumansteeringtoavoidcommonpitfalls
tweenactions, agenticLMsexcelatmulti-stepreasoning
includingconfirmationbiasanddrawingconclusionsfrom
tasks in complex environments (Yao et al., 2023). MAIA
smallsamplesizes. Fullyautomatingend-to-endinterpreta-
leveragesanagentarchitecturetogenerateandtesthypothe-
tionofothersystemswillnotonlyrequiremoreadvanced
sesaboutneuralnetworkstrainedonvisiontasks. While
tools,butagentswithmoreadvancedcapabilitiestoreason
ordinary LM agents are generally restricted to tools with
abouthowtousethem.
textualinterfaces,previousworkhassupportedinterfacing
withtheimagesthroughcodegeneration(Sur´ısetal.,2023;
2.Relatedwork Wuetal.,2023). Morerecently,largemultimodalLMslike
GPT-4Vhaveenabledtheuseofimage-basedtoolsdirectly
Interpretingdeepfeatures. Investigatingindividualneu-
(Zhengetal.,2024;Chenetal.,2023). MAIAfollowsthis
rons inside deep networks reveals a range of human-
designandis,toourknowledge,thefirstmultimodalagent
interpretablefeatures. Approachestodescribingtheseneu-
equippedwithtoolsforinterpretingdeepnetworks.
3AMultimodalAutomatedInterpretabilityAgent
3. MAIA Framework 3.2.ToolAPI
MAIAisanagentthatautonomouslyconductsexperiments TheToolsclassconsistsofasuiteoffunctionsenabling
onothersystemstoexplaintheirbehavior,bycomposingin- MAIA to write modular programs that test hypotheses
terpretabilitysubroutinesintoPythonprograms. Motivated about system behavior. MAIA tools are built from com-
bythepromiseofusinglanguage-onlymodelstocomplete moninterpretabilityproceduressuchascharacterizingneu-
one-shot visual reasoning tasks by calling external tools ron behavior using real-world images (Bau et al., 2017)
(Sur´ısetal.,2023;Gupta&Kembhavi,2023),andtheneed andperformingcausalinterventionsonimageinputs(Her-
toperformiterativeexperimentswithbothvisualandnu- nandez et al., 2022; Casper et al., 2022), which MAIA
mericresults,webuildMAIAfromapretrainedmultimodal then composes into more complex experiments (see Fig-
modelwiththeabilitytoprocessimagesdirectly. MAIAis ure 2). When programs written by MAIA are compiled
implementedwithaGPT-4Vvision-languagemodel(VLM) internallyasPythoncode,thesefunctionscanleveragecalls
backbone(OpenAI,2023b). Givenaninterpretabilityquery to other pretrained models to compute outputs. For ex-
(e.g. Which neurons in Layer 4 are selective for forested ample,tools.text2image(prompt_list)returnssyn-
backgrounds?), MAIA runs experiments that test specific theticimagesgeneratedbyatext-guideddiffusionmodel,
hypotheses(e.g. computingneuronoutputsonimageswith usingpromptswrittenbyMAIAtotestaneuron’sresponse
editedbackgrounds),observesexperimentaloutcomes,and tospecificvisualconcepts. Themodulardesignofthetool
updateshypothesesuntilitcananswertheuserquery. libraryenablesstraightforwardincorporationofnewtools
asinterpretabilitymethodsgrowinsophistication. Forthe
WeenabletheVLMtodesignandruninterpretabilityexper-
experimentsinthispaperweusethefollowingset:
imentsusingtheMAIAAPI,whichdefinestwoclasses: the
SystemclassandtheToolsclass,describedbelow. The Dataset exemplar generation. Previous studies have
APIisprovidedtotheVLMinitssystemprompt. Wein- shown that it is possible to characterize the prototypical
cludeacompleteAPIspecificationinAppendixA.Thefull behaviorofaneuronbyrecordingitsactivationvaluesover
input to the VLM is the API specification followed by a alargedatasetofimages(Bauetal.,2017;2020). Wegive
“userprompt”describingaparticularinterpretabilitytask, MAIAtheabilitytorunsuchanexperimentonthevalidation
suchasexplainingthebehaviorofanindividualneuronin- setofImageNet(Dengetal.,2009)andconstructthesetof
sideavisionmodelwithnaturallanguage(seeSection4). 15imagesthatmaximallyactivatethesystemitisinterpret-
Tocompletethetask,MAIAusescomponentsofitsAPIto ing. Interestingly,MAIAoftenchoosestobeginexperiments
writeaseriesofPythonprogramsthatrunexperimentson bycallingthistool(Figure2).Weanalyzetheimportanceof
thesystemitisinterpreting. MAIAoutputsfunctiondefini- thedataset_exemplarstoolinourablationstudy(4.3).
tionsasstrings,whichwethenexecuteinternallyusingthe
Image generation and editing tools. MAIA is equipped
Pythoninterpreter. ThePythonicimplementationenables
with a text2image(prompts) tool that synthesizes im-
flexibleincorporationofbuilt-infunctionsandexistingpack-
ages by calling Stable Diffusion v1.5 (Rombach et al.,
ages,e.g. theMAIAAPIusesthePyTorchlibrary(Paszke
2022a)ontextprompts. Generatinginputsenables MAIA
etal.,2019)toloadcommonpretrainedvisionmodels.
totestsystemsensitivitytofine-graineddifferencesinvi-
3.1.SystemAPI sual concepts, or test selectivity for the same visual con-
cept across contexts (e.g. the bowtie on a pet and on a
The System class inside the MAIA API instruments the child in Figure 2). We analyze the effect of using dif-
systemtobeinterpretedandmakessubcomponentsofthat
ferent text-to-image models in Section 4.3. In addition
system individually callable. For example, to probe sin-
to synthesizing new images, MAIA can also edit images
gle neurons inside ResNet-152 (He et al., 2016), MAIA using Instruct-Pix2Pix (Brooks et al., 2022) by calling
can use the System class to initialize a neuron object by edit_images(image, edit_instructions). Gener-
specifying its number and layer location, and the model
atingandeditingsyntheticimagesenableshypothesistests
thattheneuronbelongsto: system = System(unit_id, involvingimageslyingoutsidereal-worlddatadistributions,
layer_id, model_name). MAIAcanthendesignexper- e.g. theadditionofantelopehornstoahorse(Figure2,see
imentsthattesttheneuron’sactivationvalueondifferent Causalinterventiononimageinput).
imageinputsbyrunningsystem.neuron(image_list),
toreturnactivationvaluesandmaskedversionsoftheim- Imagedescriptionandsummarizationtools. Tolimit
agesinthelistthathighlightmaximallyactivatingregions confirmationbiasinMAIA’sinterpretationofexperimental
(See Figure 2 for examples). While existing approaches results, we use a multi-agent framework in which MAIA
to common interpretability tasks such as neuron labeling canaskanewinstanceofGPT-4Vwithnoknowledgeof
requiretrainingspecializedmodelsontask-specificdatasets experimentalhistorytodescribehighlightedimageregions
(Hernandezetal.,2022),the MAIA systemclasssupports inindividualimages,describe_images(image_list),
queryingarbitraryvisionsystemswithoutretraining. orsummarizewhattheyhaveincommonacrossagroupof
4AMultimodalAutomatedInterpretabilityAgent
Figure3.Generatingpredictiveexemplarsetsforevaluation.
images,summarize_images(image_list). Weobserve
that MAIA uses this tool in situations where previous hy- Figure4.PredictiveEvaluation. Theaverageactivationvalues
pothesesfailedorwhenobservingcomplexcombinations forMAIAdescriptionsoutperformMILANandarecomparableto
humandescriptionsforbothrealandsyntheticneurons.
ofvisualcontent.
Experiment log. MAIA can document the results of underdifferentobjectives: ResNet-152,aCNNforsuper-
each experiment (e.g. images, activations) using the visedimageclassification(Heetal.,2016),DINO(Caron
log_experiment tool, to make them accessible during et al., 2021), a Vision Transformer trained for unsuper-
subsequentexperiments. WepromptMAIAtofinishexperi- vised representation learning (Grill et al., 2020; Chen &
mentsbyloggingresults,andletitchoosewhattolog(e.g. He, 2021), and the CLIP visual encoder (Radford et al.,
datathatclearlysupportsorrefutesaparticularhypothesis). 2021),aResNet-50modeltrainedtoalignimage-textpairs.
Foreachmodel,weevaluatedescriptionsof100unitsran-
4.Evaluation domlysampledfromarangeoflayersthatcapturefeatures
atdifferentlevelsofgranularity(ResNet-152conv.1,res.1-4,
TheMAIAframeworkistask-agnosticandcanbeadapted DINOMLP1-11,CLIPres.1-4). Figure2showsexamples
tonewapplicationsbyspecifyinganinterpretabilitytaskin of MAIA experiments on neurons from all three models,
theuserprompttotheVLM.Beforetacklingmodel-level and final MAIA labels. We also evaluate a baseline non-
interpretabilityproblems(Section5),weevaluateMAIA’s interactiveapproachthatonlylabelsdatasetexemplarsof
performance on the black-box neuron description task, a eachneuron’sbehaviorusingtheMILANmodelfromHer-
widelystudiedinterpretabilitysubroutinethatservesavari- nandezetal.(2022). Finally,wecollecthumanannotations
etyofdownstreammodelauditingandeditingapplications ofarandomsubset(25%)ofneuronslabeledbyMAIAand
(Gandelsman et al., 2024; Yang et al., 2023; Hernandez MILAN, in an experimental setting where human experts
et al., 2022). For these experiments, the user prompt to writeprogramstoperforminteractiveanalysesofneurons
MAIA specifiesthetaskandoutputformat(alonger-form usingtheMAIAAPI.HumanexpertsreceivetheMAIAuser
[DESCRIPTION]ofneuronbehavior,followedbyashort prompt, write programs that run experiments on the neu-
[LABEL]),andMAIA’sSystemclassinstrumentsapartic- rons,andreturnneurondescriptionsinthesameformat.See
ular vision model (e.g. ResNet-152) and an individual AppendixC1fordetailsonthehumanlabelingexperiments.
unitindexedinsidethatmodel(e.g. Layer 4 Unit 122).
Weevaluatetheaccuracyofneurondescriptionsproduced
Taskspecificationsfortheseexperimentsmaybefoundin
Appendix B. We find MAIA correctly predicts behaviors
by MAIA, MILAN, and human experts by measuring how
well they predict neuron behavior on unseen test images.
ofindividualvisionneuronsinthreetrainedarchitectures
Similartoevaluationapproachesthatproducecontrastiveor
(Section4.1),andinasyntheticsettingwhereground-truth
counterfactualexemplarstorevealmodeldecisionbound-
neuronselectivitiesareknown(Section4.2). Wealsofind
aries(Gardneretal.,2020;Kaushiketal.,2020), weuse
descriptionsproducedbyMAIA’sinteractiveprocedureto
neurondescriptionstogeneratenewimagesthatshouldpos-
bemorepredictiveofneuronbehaviorthandescriptionsof
itivelyorneutrallyimpactneuronactivations(Figure3).For
afixedsetofdatasetexemplars,usingtheMILANbaseline
from Hernandez et al. (2022). In many cases, MAIA de-
agivenneuron,weprovideMAIA,MILAN,andhumande-
scriptionstoanewinstanceofGPT-4. Foreachdescription
scriptionsareonparwiththosebyhumanexpertsusingthe
(e.g. intricatemasks),GPT-4isinstructedtowriteprompts
MAIAAPI.InSection4.3,weperformablationstudiesto
togeneratesevenpositiveexemplarimages(e.g. AVenetian
testhowcomponentsoftheMAIAAPIdifferentiallyaffect
mask,Atribalmask,...) andsevenneutralexemplars(e.g.
descriptionquality.
Aredbus, Afieldofflowers,...), foratotalof42prompts
perneuron. Next, weuseanotherGPT-4instancetopair
4.1.Neuronsinvisionmodels
neuronlabelsproducedbyeachdescriptionmethodwiththe
WeuseMAIAtoproducenaturallanguagedescriptionsofa 7promptstheyaremostandleastlikelytoentail. Wethen
subsetofneuronsacrossthreevisionarchitecturestrained generatethecorrespondingimagesandmeasuretheiractiva-
5AMultimodalAutomatedInterpretabilityAgent
Figure5.Syntheticneuronimplementation. Segmentationof
inputimagesisperformedbyanopen-setconceptdetectorwith
textguidancespecifyingground-truthneuronselectivity.Synthetic
neurons return masked images and synthetic activation values
correspondingtotheprobabilityaconceptispresentintheimage.
tionvalueswiththetestedneuron. Usingthisprocedure,a
predictiveneuronlabelwillbepairedwithpositive/neutral
exemplars that maximally/minimally activate the neuron,
respectively. Thismethodprimarilydiscriminatesbetween
labelingprocedures: whetheritisinformativedependson
thelabelingmethodsthemselvesproducingrelevantexem-
plarprompts. Wereporttheaverageactivationvaluesfor
the3labelingmethodsacrossalltestedmodelsinFigure4.
MAIAoutperformsMILANacrossallmodelsandisoften
onparwithexpertpredictions. Weprovideabreakdown
ofresultsbylayerinTableA3.
4.2.Syntheticneurons
FollowingtheprocedureinSchwettmannetal.(2023)for
validating the performance of automated interpretability
methods on synthetic test systems mimicking real-world
behaviors, we construct a set of synthetic vision neurons
withknownground-truthselectivity. Wesimulateconcept
Figure6. MAIASyntheticneuroninterpretation.
detectionperformedbyneuronsinsidevisionmodelsusing
semanticsegmentation. Syntheticneuronsarebuiltusingan
realneuronsintheMAIAAPI,syntheticvisionneuronsac-
open-setconceptdetectorthatcombinesGroundedDINO
ceptimageinputandreturnamaskedimagehighlightingthe
(Liuetal.,2023)withSAM(Kirillovetal.,2023)toperform
concepttheyareselectivefor(ifpresent),andanactivation
text-guidedimagesegmentation. Theground-truthbehavior
value(correspondingtotheconfidenceofGroundedDINO
of each neuron is determined by a text description of the
inthepresenceoftheconcept). Datasetexemplarsforsyn-
concept(s)theneuronisselectivefor(Figure5). Tocapture
theticneuronsarecalculatedbycomputing15top-activating
real-worldbehaviors,wederiveneuronlabelsfromMILAN-
imagesperneuronfromtheCC3Mdataset(Sharmaetal.,
NOTATIONS,adatasetof60Khumanannotationsofneurons
2018). Figure5showsexamplesofeachtypeofneuron;the
acrossseventrainedvisionmodels(Hernandezetal.,2022).
fulllistof85syntheticneuronsisprovidedinAppendixC2.
Neuronsinthewilddisplayadiversityofbehaviors: some
Thesetofconceptsthatcanberepresentedbysyntheticneu-
respond to individual concepts, while others respond to
ronsislimitedtosimpleconceptsbythefidelityofopen-set
complexcombinationsofconcepts(Bauetal.,2017;Fong
conceptdetectionusingcurrenttext-guidedsegmentation
&Vedaldi,2018;Olahetal.,2020;Mu&Andreas,2021;
methods. Weverifythatallconceptsinthesyntheticneuron
Gurneeetal.,2023). Weconstructthreetypesofsynthetic
dataset can be segmented by Grounded DINO in combi-
neuronswithincreasinglevelsofcomplexity: monoseman-
nation with SAM, and provide further discussion of the
tic neurons that recognize single concepts (e.g. stripes),
limitationsofsyntheticneuronsinAppendixC2.
polysemanticneuronsselectiveforlogicaldisjunctionsof
concepts(e.g. trainsORinstruments),andconditionalneu- MAIAinterpretssyntheticneuronsusingthesameAPIand
ronsthatonlyrecognizeaconceptinpresenceofanother procedureusedtointerpretneuronsintrainedvisionmodels
concept(e.g. dog| ). Followingtheinstrumentationof (Section 4.1). In contrast to neurons in the wild, we can
leash
6AMultimodalAutomatedInterpretabilityAgent
Table1.2AFCtest.Neurondescriptionsvs.ground-truthlabels
MAIAvs.MILAN MAIAvs.Human Humanvs.MILAN
0.73±4e−4 0.53±1e−3 0.83±5e−4
evaluatedescriptionsofsyntheticneuronsdirectlyagainst
ground-truthneuronlabels. Wecollectcomparativeanno-
tationsofsyntheticneuronsfromMILAN,aswellasexpert
annotators (using the procedure from Section 4.1 where Figure7. Ablationstudy.MoredetailsinAppendixD.
humanexpertsmanuallylabelasubsetof25%ofneurons
usingtheMAIAAPI).WerecruithumanjudgesfromAma- 4.4.MAIAfailuremodes
zon Mechanical Turk to evaluate the agreement between
ConsistentwiththeresultinSection4.3thatMAIAperfor-
synthetic neuron descriptions and ground-truth labels in
manceimproveswithDALL-E3,weadditionallyobserve
pairwisetwo-alternativeforcedchoice(2AFC)tasks. For
thatSD-v1.5andInstructPix2Pixsometimesfailtofaithfully
eachtask,humanjudgesareshowntheground-truthneuron
generateandeditimagesaccordingtoMAIA’sinstructions.
label (e.g. tail) and descriptions produced by two label-
To mitigate some of these failures, we instruct MAIA to
ingprocedures(e.g. “fluffyandtexturedanimaltails”and
promptpositiveimage-edits(e.g. replacethebowtiewith
“circularobjectsandanimals”),andaskedtoselectwhich
a plain shirt) rather than negative edits (e.g. remove the
descriptionbettermatchestheground-truthlabel. Further
bowtie),butoccasionalfailuresstilloccur(seeAppendixE).
details are provided in Appendix C2. Table 1 shows the
Whileproprietaryversionsoftoolsmaybeofhigherquality,
resultsofthe2AFCstudy(theproportionoftrialsinwhich
theyalsointroduceprohibitiveratelimitsandcostsassoci-
procedureAwasfavoredoverB,and95%confidencein-
atedwithAPIaccess. Weareawarethatsimilarlimitations
tervals). Accordingtohumanjudges,MAIAlabelsbetter
applytotheGPT-4Vbackboneitself. TheMAIAsystemis
agreewithground-truthlabelswhencomparedto MI-
designedmodularlysothatopen-sourcealternativescanbe
LAN,andareevenslightlypreferredoverexpertlabels
usedastheirperformanceimproves.
onthesubsetofneuronstheydescribed(whilehumanlabels
arelargelypreferredoverMILANlabels). Wealsousethe 5.Applications
predictiveevaluationframeworkdescribedinSection4.1to
generatepositiveandneutralsetsofexemplarimagesforall MAIAisaflexiblesystemthatautomatesmodelunderstand-
syntheticneurons. Figure4shows MAIA descriptionsare ing tasks at different levels of granularity: from labeling
betterpredictorsofsyntheticneuronactivationsthanMILAN individualfeaturestodiagnosingmodel-levelfailuremodes.
descriptions,onparwithlabelsproducedbyhumanexperts. TodemonstratetheutilityofMAIAforproducingactionable
insightsforhumanusers(Vaughan&Wallach,2020),we
4.3.Toolablationstudy conductexperimentsthatapply MAIA totwomodel-level
tasks: (i)spuriousfeatureremovaland(ii)biasidentifica-
MAIA’s modular design enables straightforward addition
tioninadownstreamclassificationtask. InbothcasesMAIA
andremovaloftoolsfromitsAPI.Wetestthreedifferent
usestheAPIasdescribedinSection3.
settingstoquantifysensitivitytodifferenttools: (i)label-
ingneuronsusingonlythedataset_exemplarfunction 5.1.Removingspuriousfeatures
withouttheabilitytosynthesizeimages,(ii)relyingonlyon
Learned spurious features impose a challenge when ma-
generatedinputswithouttheoptiontocomputemaximally
chinelearningmodelsareappliedinreal-worldscenarios,
activatingdatasetexemplars,and(iii)replacingtheStable
where test distributions differ from training set statistics
Diffusiontext2imagebackbonewithDALL-E3. While
(Storkeyetal.,2009;Beeryetal.,2018;Bissotoetal.,2020;
thefirsttwosettingsdonotfullycompromiseperformance,
Xiao et al., 2020; Singla et al., 2021). We use MAIA to
neither ablated API achieves the same average accuracy
removelearnedspuriousfeaturesinsideaclassificationnet-
asthefull MAIA system(Figure7). Theseresultsempha-
work,findingthatwithnoaccesstounbiasedexamplesnor
size the combined utility of tools for experimenting with
groupingannotations,MAIAcanidentifyandremovesuch
real-worldandsyntheticinputs: MAIAperformsbestwhen
features, improving model robustness under distribution
initializingexperimentswithdatasetexemplarsandrunning
shiftbyawidemargin,withanaccuracyapproachingthat
additionaltestswithsyntheticimages. UsingDALL-Eas
offine-tuningonbalanceddata.
theimagegeneratorimprovesresults, suggestingthatthe
agent might be bounded by the performance of its tools, WerunexperimentsonResNet-18trainedontheSpawrious
rather than its ability to use them, and as the tools inside dataset(Lynchetal.,2023),asyntheticallygenerateddataset
MAIA’sAPIgrowinsophistication,sowillMAIA. involvingfourdogbreedswithdifferentbackgrounds.Inthe
7AMultimodalAutomatedInterpretabilityAgent
adaptabletothisexperiment: theoutputlogitcorresponding
toaspecificclassisinstrumentedusingthesystemclass,
and returns class probability for input images. MAIA is
providedwiththeclasslabelandinstructed(seeAppendix
G)tofindsettingsinwhichtheclassifierranksimagesre-
Figure8. Spawriousdatasetexamples.
latedtothatclasswithrelativelylowerprobabilityvalues,or
showsaclearpreferenceforasubsetoftheclass. Figure 9
Table2. Finallayerspuriousfeatureremovalresults.
presentsresultsforasubsetofImageNetclasses. Thissim-
Subset SelectionMethod #Units Balanced TestAcc.
pleparadigmsuggeststhatMAIA’sgenerationofsynthetic
All OriginalModel 512 ✗ 0.731
datacouldbewidelyusefulforidentifyingregionsofthe
All 50 ✗ 0.779 inputdistributionwhereamodelexhibitspoorperformance.
Random 22 ✗ 0.705±0.05
ℓ 1Top50
ℓ 1Top22 22 ✗ 0.757
Whilethisexploratoryexperimentsurfacesonlybroadfail-
MAIA 22 ✗ 0.837 urecategories,MAIAenablesotherexperimentstargetedat
All
ℓ 1Hyper.Tuning 147 ✓ 0.830 end-usecasesidentifyingspecificbiases.
ℓ 1Top22 22 ✓ 0.865
trainset,eachbreedisspuriouslycorrelatedwithacertain
backgroundtype,whileinthetestset,thebreed-background
pairingsarechanged(seeFigure8). WeuseMAIAtofinda
subsetoffinallayerneuronsthatrobustlypredictasingle
dogbreedindependentlyofspuriousfeatures(seeAppendix
F3). WhileothermethodslikeKirichenkoetal.(2023)re-
movespuriouscorrelationsbyretrainingthelastlayeron
balanced datasets, we only provide MAIA access to top-
activatingimagesfromtheunbalanced validationsetand
promptMAIAtorunexperimentstodeterminerobustness.
WethenusethefeaturesMAIAselectstotrainanunregular-
izedlogisticregressionmodelontheunbalanceddata.
Asademonstration,weselect50ofthemostinformative
neuronsusingℓ regularizationontheunbalanceddataset
1
andhaveMAIArunexperimentsoneachone. MAIAselects
22neuronsitdeemstoberobust. Traninganunregularized
model on this subset significantly improves accuracy, as
reported in Table 2. To show that the sparsity of MAIA’s
neuronselectionisnottheonlyreasonforitsperformance
improvements,webenchmarkMAIA’sperformanceagainst
ℓ regularizedfittingonbothunbalancedandbalancedver-
1
sionsofthedataset. Ontheunbalanceddataset,ℓ dropsin
1
performancewhensubsetsizereducesfrom50to22neu-
rons. Using a small balanced dataset to hyperparameter
tunetheℓ parameterandtrainthelogisticregressionmodel Figure9.MAIAbiasdetection. MAIAgeneratessyntheticinputs
1
tosurfacebiasesinResNet-152outputclasses. Insomecases,
onallneuronsachievesperformancecomparabletoMAIA’s
chosensubset,although MAIA didnothaveaccesstoany
MAIAdiscoversuniformbehaviorovertheinputs(e.g.flagpole).
balanced data. For a fair comparison, we test the perfor-
manceofanℓ 1modelwhichmatchesthesparsityofMAIA, 6.Conclusion
buttrainedonthebalanceddataset. SeeAppendixF2for
moredetails.
WeintroduceMAIA,anagentthatautomatesinterpretability
tasksincludingfeatureinterpretationandbiasdiscovery,by
composingpretrainedmodulestoconductexperimentson
5.2.Revealingbiases
othersystems. Whilehumansupervisionisneededtomaxi-
MAIAcanbeusedtoautomaticallysurfacemodel-levelbi- mizeitseffectivenessandcatchcommonmistakes,initial
ases. Specifically,weapplyMAIAtoinvestigatebiasesin experimentswith MAIA showpromise,andweanticipate
theoutputsofaCNN(ResNet-152)trainedonasupervised that interpretability agents will be increasingly useful as
ImageNet classification task. The MAIA system is easily theygrowinsophistication.
8AMultimodalAutomatedInterpretabilityAgent
7.Impactstatement datasetsandmodels? notsofast. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPattern
AsAIsystemstakeonhigher-stakesrolesandbecomemore
RecognitionWorkshops,pp.740–741,2020.
deeply integrated into research and society, scalable ap-
proaches to auditing for reliability will be vital. MAIA is Brooks,T.,Holynski,A.,andEfros,A.A. Instructpix2pix:
aprotoypeforatoolthatcanhelphumanusersensureAI Learning to follow image editing instructions. arXiv
systemsaretransparent,reliable,andequitable. preprintarXiv:2211.09800,2022.
WethinkMAIAaugments,butdoesnotreplace,humanover- Caron, M., Touvron, H., Misra, I., Je´gou, H., Mairal, J.,
sightofAIsystems. MAIAstillrequireshumansupervision Bojanowski, P., andJoulin, A. Emergingpropertiesin
tocatchmistakessuchasconfirmationbiasandimagegen- self-supervisedvisiontransformers.InProceedingsofthe
eration/editingfailures. Absenceofevidence(fromMAIA) IEEE/CVFinternationalconferenceoncomputervision,
isnotevidenceofabsence: thoughMAIA’stoolkitenables pp.9650–9660,2021.
causalinterventionsoninputsinordertoevaluatesystem
behavior,MAIA’sexplanationsdonotprovideformalverifi- Casper,S.,Hariharan,K.,andHadfield-Menell,D.Diagnos-
cationofsystemperformance. ticsfordeepneuralnetworkswithautomatedcopy/paste
attacks. arXivpreprintarXiv:2211.10024,2022.
8.Acknowlegements
Chen,L.,Zhang,Y.,Ren,S.,Zhao,H.,Cai,Z.,Wang,Y.,
Wang,P.,Liu,T.,andChang,B. Towardsend-to-endem-
We are grateful for the support of the MIT-IBM Watson
bodieddecisionmakingviamulti-modallargelanguage
AILab,theOpenPhilanthropyfoundation,HyundaiMotor
model: Explorationswithgpt4-visionandbeyond,2023.
Company, ARL grant W911NF-18-2-021, Intel, the Na-
tionalScienceFoundationundergrantCCF-2217064,the Chen,X.andHe,K. Exploringsimplesiameserepresen-
Zuckerman STEM Leadership Program, and the Viterbi tation learning. In Proceedings of the IEEE/CVF con-
Fellowship. The funders had no role in experimental de- ferenceoncomputervisionandpatternrecognition,pp.
signoranalysis,decisiontopublish,orpreparationofthe 15750–15758,2021.
manuscript. The authors have no competing interests to
report. Conmy,A.,Mavor-Parker,A.N.,Lynch,A.,Heimersheim,
S., andGarriga-Alonso, A. Towardsautomatedcircuit
discoveryformechanisticinterpretability. arXivpreprint
References
arXiv:2304.14997,2023.
Bau,D.,Zhou,B.,Khosla,A.,Oliva,A.,andTorralba,A.
Dalvi, F., Durrani, N., Sajjad, H., Belinkov, Y., Bau, A.,
Networkdissection: Quantifyinginterpretabilityofdeep
and Glass, J. What is one grain of sand in the desert?
visualrepresentations. InComputerVisionandPattern
analyzing individual neurons in deep nlp models. In
Recognition,2017.
ProceedingsoftheAAAIConferenceonArtificialIntelli-
Bau, D., Zhu, J.-Y., Strobelt, H., Lapedriza, A., gence,volume33,pp.6309–6317,2019.
Zhou, B., and Torralba, A. Understanding the
role of individual units in a deep neural network. Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei,
Proceedings of the National Academy of Sciences, L. Imagenet: Alarge-scalehierarchicalimagedatabase.
2020. ISSN 0027-8424. doi: 10.1073/pnas.
In2009IEEEconferenceoncomputervisionandpattern
1907375117. URL https://www.pnas.org/ recognition,pp.248–255.Ieee,2009.
content/early/2020/08/31/1907375117.
Fong,R.andVedaldi,A. Net2vec: Quantifyingandexplain-
Beery,S.,VanHorn,G.,andPerona,P. Recognitioninterra inghowconceptsareencodedbyfiltersindeepneural
incognita. InProceedingsoftheEuropeanconferenceon networks. In Proceedings of the IEEE conference on
computervision(ECCV),pp.456–473,2018. computervisionandpatternrecognition,pp.8730–8738,
2018.
Bills, S., Cammarata, N., Mossing, D., Tillman, H.,
Gao, L., Goh, G., Sutskever, I., Leike, J., Wu, Gandelsman,Y.,Efros,A.A.,andSteinhardt,J.Interpreting
J., and Saunders, W. Language models can clip’simagerepresentationviatext-baseddecomposition,
explain neurons in language models. https: 2024.
//openaipublic.blob.core.windows.net/
Gardner,M.,Artzi,Y.,Basmova,V.,Berant,J.,Bogin,B.,
neuron-explainer/paper/index.html,
Chen,S.,Dasigi,P.,Dua,D.,Elazar,Y.,Gottumukkala,
2023.
A.,Gupta,N.,Hajishirzi,H.,Ilharco,G.,Khashabi,D.,
Bissoto,A.,Valle,E.,andAvila,S. Debiasingskinlesion Lin,K.,Liu,J.,Liu,N.F.,Mulcaire,P.,Ning,Q.,Singh,
9AMultimodalAutomatedInterpretabilityAgent
S.,Smith,N.A.,Subramanian,S.,Tsarfaty,R.,Wallace, Kluyver,T.,Ragan-Kelley,B.,Pe´rez,F.,Granger,B.,Bus-
E., Zhang, A., and Zhou, B. Evaluating models’ local sonnier,M.,Frederic,J.,Kelley,K.,Hamrick,J.,Grout,J.,
decisionboundariesviacontrastsets,2020. Corlay,S.,Ivanov,P.,Avila,D.,Abdalla,S.,andWilling,
C. Jupyter notebooks – a publishing format for repro-
Girshick,R.,Donahue,J.,Darrell,T.,andMalik,J.Richfea- ducible computational workflows. In Loizides, F. and
turehierarchiesforaccurateobjectdetectionandsemantic Schmidt,B.(eds.),PositioningandPowerinAcademic
segmentation. InProceedingsoftheIEEEconferenceon Publishing: Players,AgentsandAgendas,pp.87–90.
computer vision and pattern recognition, pp. 580–587, IOSPress,2016.
2014.
Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li,
Grill,J.-B.,Strub,F.,Altche´,F.,Tallec,C.,Richemond,P.,
C., Yang, J., Su, H., Zhu, J., et al. Grounding dino:
Buchatskaya,E.,Doersch,C.,AvilaPires,B.,Guo,Z.,
Marryingdinowithgroundedpre-trainingforopen-set
GheshlaghiAzar,M.,etal. Bootstrapyourownlatent-a objectdetection. arXivpreprintarXiv:2303.05499,2023.
newapproachtoself-supervisedlearning. Advancesin
neuralinformationprocessingsystems,33:21271–21284, Liu,Z.,Luo,P.,Wang,X.,andTang,X. Deeplearningface
2020. attributesinthewild,2015.
Gupta,T.andKembhavi,A. Visualprogramming: Compo- Lynch, A., Dovonon, G. J.-S., Kaddour, J., and Silva, R.
sitionalvisualreasoningwithouttraining. InProceedings Spawrious: A benchmark for fine control of spurious
of the IEEE/CVF Conference on Computer Vision and correlationbiases,2023.
PatternRecognition,pp.14953–14962,2023.
Mahendran, A. and Vedaldi, A. Understanding deep im-
Gurnee, W., Nanda, N., Pauly, M., Harvey, K., Troit- age representations by inverting them. In Proceedings
skii, D., and Bertsimas, D. Finding neurons in a oftheIEEEconferenceoncomputervisionandpattern
haystack: Case studies with sparse probing. arXiv recognition,pp.5188–5196,2015.
preprintarXiv:2305.01610,2023.
Mu,J.andAndreas,J. Compositionalexplanationsofneu-
He,K.,Zhang,X.,Ren,S.,andSun,J. Deepresiduallearn- rons,2021.
ingforimagerecognition. InProceedingsoftheIEEE
conferenceoncomputervisionandpatternrecognition, Nushi,B.,Kamar,E.,andHorvitz,E. Towardsaccountable
pp.770–778,2016. ai: Hybridhuman-machineanalysesforcharacterizing
systemfailure. InProceedingsoftheAAAIConference
Hernandez,E.,Schwettmann,S.,Bau,D.,Bagashvili,T., onHumanComputationandCrowdsourcing,volume6,
Torralba,A.,andAndreas,J. Naturallanguagedescrip- pp.126–135,2018.
tionsofdeepvisualfeatures. InInternationalConference
onLearningRepresentations,2022. Oikarinen, T. and Weng, T.-W. Clip-dissect: Automatic
descriptionofneuronrepresentationsindeepvisionnet-
Huang,J.,Geiger,A.,D’Oosterlinck,K.,Wu,Z.,andPotts, works. arXivpreprintarXiv:2204.10965,2022.
C. Rigorouslyassessingnaturallanguageexplanationsof
neurons. arXivpreprintarXiv:2309.10312,2023. Olah,C.,Mordvintsev,A.,andSchubert,L. Featurevisual-
ization. Distill,2(11):e7,2017.
Karpathy, A., Johnson, J., and Fei-Fei, L. Visualizing
and understanding recurrent networks. arXiv preprint Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov,
arXiv:1506.02078,2015. M.,andCarter,S. Zoomin: Anintroductiontocircuits.
Distill,5(3):e00024–001,2020.
Kaushik, D., Hovy, E., and Lipton, Z. C. Learning the
differencethatmakesadifferencewithcounterfactually- OpenAI. Gpt-4technicalreport,2023a.
augmenteddata,2020.
OpenAI. Gpt-4v(ision) technical work and au-
Kirichenko,P.,Izmailov,P.,andWilson,A.G. Lastlayer thors. https://openai.com/contributions/
re-trainingissufficientforrobustnesstospuriouscorrela- gpt-4v,2023b. Accessed: [insertdateofaccess].
tions,2023.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Chanan,G.,Killeen,T.,Lin,Z.,Gimelshein,N.,Antiga,
Gustafson,L.,Xiao,T.,Whitehead,S.,Berg,A.C.,Lo, L.,etal. Pytorch: Animperativestyle,high-performance
W.-Y., Dolla´r, P., and Girshick, R. Segment anything. deep learning library. Advances in neural information
arXiv:2304.02643,2023. processingsystems,32,2019.
10AMultimodalAutomatedInterpretabilityAgent
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Singla, S., Nushi, B., Shah, S., Kamar, E., and Horvitz,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., E. Understandingfailuresofdeepnetworksviarobust
Weiss,R.,Dubourg,V.,Vanderplas,J.,Passos,A.,Cour- featureextraction. InProceedingsoftheIEEE/CVFCon-
napeau,D.,Brucher,M.,Perrot,M.,andDuchesnay,E. ferenceonComputerVisionandPatternRecognition,pp.
Scikit-learn: Machine learning in Python. Journal of 12853–12862,2021.
MachineLearningResearch,12:2825–2830,2011.
Storkey,A.etal. Whentrainingandtestsetsaredifferent:
Qin,Y.,Liang,S.,Ye,Y.,Zhu,K.,Yan,L.,Lu,Y.,Lin,Y., characterizinglearningtransfer. Datasetshiftinmachine
Cong,X.,Tang,X.,Qian,B.,Zhao,S.,Hong,L.,Tian, learning,30(3-28):6,2009.
R.,Xie,R.,Zhou,J.,Gerstein,M.,Li,D.,Liu,Z.,and
Sun,M. Toolllm: Facilitatinglargelanguagemodelsto Sur´ıs, D., Menon, S., and Vondrick, C. Vipergpt: Visual
master16000+real-worldapis,2023. inferenceviapythonexecutionforreasoning,2023.
Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G., Vaughan,J.W.andWallach,H. Ahuman-centeredagenda
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, for intelligible machine learning. Machines We Trust:
J.,Krueger,G.,andSutskever,I. Learningtransferable GettingAlongwithArtificialIntelligence,2020.
visualmodelsfromnaturallanguagesupervision,2021.
Wah,C.,Branson,S.,Welinder,P.,Perona,P.,andBelongie,
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and S. TheCaltech-UCSDBirds-200-2011Dataset. Caltech
Ommer, B. High-resolution image synthesis with la- VisionLab,Jul2011.
tentdiffusionmodels. InProceedingsoftheIEEE/CVF
Wu,C.,Yin,S.,Qi,W.,Wang,X.,Tang,Z.,andDuan,N.
ConferenceonComputerVisionandPatternRecognition
Visualchatgpt: Talking,drawingandeditingwithvisual
(CVPR),pp.10684–10695,June2022a.
foundationmodels,2023.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer,B. High-resolutionimagesynthesiswithlatent Xiao,K.,Engstrom,L.,Ilyas,A.,andMadry,A. Noiseor
diffusionmodels,2022b. signal: Theroleofimagebackgroundsinobjectrecogni-
tion. arXivpreprintarXiv:2006.09994,2020.
Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P.
Distributionallyrobustneuralnetworksforgroupshifts: Yang, Y., Panagopoulou, A., Zhou, S., Jin, D., Callison-
Ontheimportanceofregularizationforworst-casegener- Burch, C., andYatskar, M. Languageinabottle: Lan-
alization,2020. guagemodelguidedconceptbottlenecksforinterpretable
imageclassification,2023.
Schick,T.,Dwivedi-Yu,J.,Dess`ı,R.,Raileanu,R.,Lomeli,
M.,Zettlemoyer,L.,Cancedda,N.,andScialom,T. Tool- Yao,S.,Zhao,J.,Yu,D.,Du,N.,Shafran,I.,Narasimhan,
former: Languagemodels canteach themselvesto use K.,andCao,Y. React: Synergizingreasoningandacting
tools,2023. inlanguagemodels,2023.
Schwettmann, S., Hernandez, E., Bau, D., Klein, S., An- Zeiler,M.D.andFergus,R. Visualizingandunderstand-
dreas,J.,andTorralba,A. Towardavisualconceptvo- ingconvolutionalnetworks. InComputerVision–ECCV
cabulary for gan latent space. In Proceedings of the 2014: 13thEuropeanConference,Zurich,Switzerland,
IEEE/CVFInternationalConferenceonComputerVision, September6-12,2014,Proceedings,PartI13,pp.818–
pp.6804–6812,2021. 833.Springer,2014.
Schwettmann,S.,Shaham,T.R.,Materzynska,J.,Chowd-
Zhang, J., Wang, Y., Molino, P., Li, L., and Ebert, D. S.
hury, N., Li, S., Andreas, J., Bau, D., and Torralba, A.
Manifold: A model-agnostic framework for interpreta-
Find: Afunctiondescriptionbenchmarkforevaluating
tion and diagnosis of machine learning models. IEEE
interpretabilitymethods,2023.
transactionsonvisualizationandcomputergraphics,25
Sharma,P.,Ding,N.,Goodman,S.,andSoricut,R. Con- (1):364–373,2018.
ceptualcaptions: Acleaned,hypernymed,imagealt-text
Zheng,B.,Gou,B.,Kil,J.,Sun,H.,andSu,Y.Gpt-4v(ision)
datasetforautomaticimagecaptioning. InProceedings
isageneralistwebagent,ifgrounded,2024.
ofthe56thAnnualMeetingoftheAssociationforCompu-
tationalLinguistics(Volume1: LongPapers),pp.2556– Zou,X.,Yang,J.,Zhang,H.,Li,F.,Li,L.,Wang,J.,Wang,
2565,2018. L.,Gao,J.,andLee,Y.J.Segmenteverythingeverywhere
allatonce,2023.
Singh,C.,Hsu,A.R.,Antonello,R.,Jain,S.,Huth,A.G.,
Yu,B.,andGao,J. Explainingblackboxtextmodulesin
naturallanguagewithlanguagemodels,2023.
11AMultimodalAutomatedInterpretabilityAgent
Appendix
A. MAIA Library
ThefullMAIAAPIprovidedinthesystempromptisreproducedbelow.
import torch
from typing import List, Tuple
class System:
"""
A Python class containing the vision model and the specific neuron to interact with.
Attributes
----------
neuron_num : int
The unit number of the neuron.
layer : string
The name of the layer where the neuron is located.
model_name : string
The name of the vision model.
model : nn.Module
The loaded PyTorch model.
Methods
-------
load_model(model_name: str) -> nn.Module
Gets the model name and returns the vision model from PyTorch library.
neuron(image_list: List[torch.Tensor]) -> Tuple[List[int], List[str]]
returns the neuron activation for each image in the input image_list as well as the activation map
of the neuron over that image, that highlights the regions of the image where the activations
are higher (encoded into a Base64 string).
"""
def __init__(self, neuron_num: int, layer: str, model_name: str, device: str):
"""
Initializes a neuron object by specifying its number and layer location and the vision model that the neuron
belongs to.
Parameters
-------
neuron_num : int
The unit number of the neuron.
layer : str
The name of the layer where the neuron is located.
model_name : str
The name of the vision model that the neuron is part of.
device : str
The computational device ('cpu' or 'cuda').
"""
self.neuron_num = neuron_num
self.layer = layer
self.device = torch.device(f"cuda:{device}" if torch.cuda.is_available() else "cpu")
self.model = self.load_model(model_name)
def load_model(self, model_name: str) -> torch.nn.Module:
"""
Gets the model name and returns the vision model from pythorch library.
Parameters
----------
model_name : str
The name of the model to load.
Returns
-------
nn.Module
The loaded PyTorch vision model.
Examples
--------
>>> # load "resnet152"
>>> def run_experiment(model_name) -> nn.Module:
>>> model = load_model(model_name: str)
>>> return model
"""
return load_model(model_name)
def neuron(self, image_list: List[torch.Tensor]) -> Tuple[List[int], List[str]]:
"""
12AMultimodalAutomatedInterpretabilityAgent
The function returns the neuron's maximum activation value (in int format) for each of the images in the
list as well as the activation map of the neuron over each of the images that highlights the regions of
the image where the activations are higher (encoded into a Base64 string).
Parameters
----------
image_list : List[torch.Tensor]
The input image
Returns
-------
Tuple[List[int], List[str]]
For each image in image_list returns the activation value of the neuron on that image, and a masked
image,
with the region of the image that caused the high activation values highlighted (and the rest of the
image is darkened). Each image is encoded into a Base64 string.
Examples
--------
>>> # test the activation value of the neuron for the prompt "a dog standing on the grass"
>>> def run_experiment(system, tools) -> Tuple[int, str]:
>>> prompt = ["a dog standing on the grass"]
>>> image = tools.text2image(prompt)
>>> activation_list, activation_map_list = system.neuron(image)
>>> return activation_list, activation_map_list
>>> # test the activation value of the neuron for the prompt "a dog standing on the grass" and the neuron
activation value for the same image but with a lion instead of a dog
>>> def run_experiment(system, tools) -> Tuple[int, str]:
>>> prompt = ["a dog standing on the grass"]
>>> edits = ["replace the dog with a lion"]
>>> all_image, all_prompts = tools.edit_images(prompt, edits)
>>> activation_list, activation_map_list = system.neuron(all_images)
>>> return activation_list, activation_map_list
"""
return neuron(image_list)
class Tools:
"""
A Python class containing tools to interact with the neuron implemented in the system class,
in order to run experiments on it.
Attributes
----------
experiment_log: str
A log of all the experiments, including the code and the output from the neuron.
Methods
-------
dataset_exemplars(system: object) -> Tuple(List[int],List[str])
This experiment provides good coverage of the behavior observed on a very large dataset of images and
therefore represents the typical behavior of the neuron on real images.
This function characterizes the prototipycal behavior of the neuron by computing its activation on all
images in the ImageNet dataset and returning the 15 highest activation values and the images that
produced them.
The images are masked to highlight the specific regions that produce the maximal activation. The images are
overlaid with a semi-opaque mask, such that the maximally activating regions remain unmasked.
edit_images(prompt_list_org_image : List[str], editing_instructions_list : List[str]) -> Tuple[List[Image.Image
], List[str]]
This function enables loclized testing of specific hypotheses about how variations on the content of a
single image affect neuron activations.
Gets a list of input prompt and a list of corresponding editing instructions, then generate images according
to the input prompts and edits each image based on the instructions given in the prompt using a text-
based image editing model.
This function is very useful for testing the causality of the neuron in a controlled way, for example by
testing how the neuron activation is affected by changing one aspect of the image.
IMPORTANT: Do not use negative terminology such as "remove ...", try to use terminology like "replace ...
with ..." or "change the color of ... to ...".
text2image(prompt_list: str) -> Tuple[torcu.Tensor]
Gets a list of text prompt as an input and generates an image for each prompt in the list using a text to
image model.
The function returns a list of images.
summarize_images(self, image_list: List[str]) -> str:
This function is useful to summarize the mutual visual concept that appears in a set of images.
It gets a list of images at input and describes what is common to all of them, focusing specifically on
unmasked regions.
describe_images(synthetic_image_list: List[str], synthetic_image_title:List[str]) -> str
Provides impartial descriptions of images. Do not use this function on dataset exemplars.
13AMultimodalAutomatedInterpretabilityAgent
Gets a list of images and generat a textual description of the semantic content of the unmasked regions
within each of them.
The function is blind to the current hypotheses list and therefore provides an unbiased description of the
visual content.
log_experiment(activation_list: List[int], image_list: List[str], image_titles: List[str],
image_textual_information: Union[str, List[str]]) -> None
documents the current experiment results as an entry in the experiment log list. if self.
activation_threshold was updated by the dataset_exemplars function,
the experiment log will contains instruction to continue with experiments if activations are lower than
activation_threshold.
Results that are loged will be available for future experiment (unlogged results will be unavailable).
The function also update the attribure "result_list", such that each element in the result_list is a
dictionary of the format: {"<prompt>": {"activation": act, "image": image}}
so the list contains all the resilts that were logged so far.
"""
def __init__(self):
"""
Initializes the Tools object.
Parameters
----------
experiment_log: store all the experimental results
"""
self.experiment_log = []
self.results_list = []
def dataset_exemplars(self, system: object) -> Tuple(List[int],List[str])
"""
This method finds images from the ImageNet dataset that produce the highest activation values for a specific
neuron.
It returns both the activation values and the corresponding exemplar images that were used
to generate these activations (with the highly activating region highlighted and the rest of the image
darkened).
The neuron and layer are specified through a 'system' object.
This experiment is performed on real images and will provide a good approximation of the neuron behavior.
Parameters
----------
system : object
An object representing the specific neuron and layer within the neural network.
The 'system' object should have 'layer' and 'neuron_num' attributes, so the dataset_exemplars function
can return the exemplar activations and masked images for that specific neuron.
Returns
-------
tuple
A tuple containing two elements:
- The first element is a list of activation values for the specified neuron.
- The second element is a list of exemplar images (as Base64 encoded strings) corresponding to these
activations.
Example
-------
>>> def run_experiment(system, tools)
>>> activation_list, image_list = self.dataset_exemplars(system)
>>> return activation_list, image_list
"""
return dataset_exemplars(system)
def edit_images(self, prompt_list_org_image : List[str], editing_instructions_list : List[str]) -> Tuple[List[
Image.Image], List[str]]:
"""
This function enables localized testing of specific hypotheses about how variations in the content of a
single image affect neuron activations.
Gets a list of prompts to generate images, and a list of corresponding editing instructions as inputs. Then
generates images based on the image prompts and edits each image based on the instructions given in the
prompt using a text-based image editing model (so there is no need to generate the images outside of
this function).
This function is very useful for testing the causality of the neuron in a controlled way, for example by
testing how the neuron activation is affected by changing one aspect of the image.
IMPORTANT: for the editing instructions, do not use negative terminology such as "remove ...", try to use
terminology like "replace ... with ..." or "change the color of ... to"
The function returns a list of images, constructed in pairs of original images and their edited versions,
and a list of all the corresponding image prompts and editing prompts in the same order as the images.
Parameters
----------
prompt_list_org_image : List[str]
14AMultimodalAutomatedInterpretabilityAgent
A list of input prompts for image generation. These prompts are used to generate images which are to be
edited by the prompts in editing_instructions_list.
editing_instructions_list : List[str]
A list of instructions for how to edit the images in image_list. Should be the same length as
prompt_list_org_image.
Edits should be relatively simple and describe replacements to make in the image, not deletions.
Returns
-------
Tuple[List[Image.Image], List[str]]
A list of all images where each unedited image is followed by its edited version.
And a list of all the prompts corresponding to each image (e.g. the input prompt followed by the editing
instruction).
Examples
--------
>>> # test the activation value of the neuron for the prompt "a dog standing on the grass" and the neuron
activation value for the same image but with a cat instead of a dog
>>> def run_experiment(system, tools) -> Tuple[int, str]:
>>> prompt = ["a dog standing on the grass"]
>>> edits = ["replace the dog with a cat"]
>>> all_image, all_prompts = tools.edit_images(prompt, edits)
>>> activation_list, activation_map_list = system.neuron(all_images)
>>> return activation_list, activation_map_list
>>> # test the activation value of the neuron for the prompt "a dog standing on the grass" and the neuron
activation values for the same image but with a different action instead of "standing":
>>> def run_experiment(system, tools) -> Tuple[int, str]:
>>> prompts = ["a dog standing on the grass"]*3
>>> edits = ["make the dog sit","make the dog run","make the dog eat"]
>>> all_images, all_prompts = tools.edit_images(prompts, edits)
>>> activation_list, activation_map_list = system.neuron(all_images)
>>> return activation_list, activation_map_list
"""
return edit_images(image, edits)
def text2image(self, prompt_list: List[str]) -> List[Image.Image]:
"""Gets a list of text prompts as input, generates an image for each prompt in the list using a text to
image model.
The function returns a list of images.
Parameters
----------
prompt_list : List[str]
A list of text prompts for image generation.
Returns
-------
List[Image.Image]
A list of images, corresponding to each of the input prompts.
Examples
--------
>>> # test the activation value of the neuron for the prompt "a dog standing on the grass"
>>> def run_experiment(system, tools) -> Tuple[int, str]:
>>> prompt = ["a dog standing on the grass"]
>>> image = tools.text2image(prompt)
>>> activation_list, activation_map_list = system.neuron(image)
>>> return activation_list, activation_map_list
>>> # test the activation value of the neuron for the prompt "a fox and a rabbit watch a movie under a
starry night sky" "a fox and a bear watch a movie under a starry night sky" "a fox and a rabbit watch a
movie at sunrise"
>>> def run_experiment(system, tools) -> Tuple[int, str]:
>>> prompt_list = ["a fox and a rabbit watch a movie under a starry night sky", "a fox and a bear watch
a movie under a starry night sky","a fox and a rabbit watch a movie at sunrise"]
>>> images = tools.text2image(prompt_list)
>>> activation_list, activation_map_list = system.neuron(images)
>>> return activation_list, activation_map_list
"""
return text2image(prompt_list)
def summarize_images(self, image_list: List[str]) -> str:
"""
This function is useful to summarize the mutual visual concept that appears in a set of images.
It gets a list of images at input and describes what is common to all of them, focusing specifically on
unmasked regions.
Parameters
----------
15AMultimodalAutomatedInterpretabilityAgent
image_list : list
A list of images in Base64 encoded string format.
Returns
-------
str
A string with a descriptions of what is common to all the images.
Example
-------
>>> # tests dataset exemplars and return textual summarization of what is common for all the maximally
activating images
>>> def run_experiment(system, tools):
>>> activation_list, image_list = self.dataset_exemplars(system)
>>> prompt_list = []
>>> for i in range(len(activation_list)):
>>> prompt_list.append(f'dataset exemplar {i}') # for the dataset exemplars we don't have prompts,
therefore need to provide text titles
>>> summarization = tools.summarize_images(image_list)
>>> return summarization
"""
return summarize_images(image_list)
def describe_images(self, image_list: List[str], image_title:List[str]) -> str:
"""
Provides impartial description of the highlighted image regions within an image.
Generates textual descriptions for a list of images, focusing specifically on highlighted regions.
This function translates the visual content of the highlited region in the image to a text description.
The function operates independently of the current hypothesis list and thus offers an impartial description
of the visual content.
It iterates through a list of images, requesting a description for the
highlighted (unmasked) regions in each synthetic image. The final descriptions are concatenated
and returned as a single string, with each description associated with the corresponding
image title.
Parameters
----------
image_list : list
A list of images in Base64 encoded string format.
image_title : callable
A list of strings with the image titles that will be use to list the different images. Should be the
same length as image_list.
Returns
-------
str
A concatenated string of descriptions for each image, where each description
is associated with the image's title and focuses on the highlighted regions
in the image.
Example
-------
>>> def run_experiment(system, tools):
>>> prompt_list = ["a fox and a rabbit watch a movie under a starry night sky", "a fox and a bear watch
a movie under a starry night sky","a fox and a rabbit watch a movie at sunrise"]
>>> images = tools.text2image(prompt_list)
>>> activation_list, image_list = system.neuron(images)
>>> descriptions = tools.describe_images(image_list, prompt_list)
>>> return descriptions
"""
return describe_images(image_list, image_title)
def log_experiment(self, activation_list: List[int], image_list: List[str], image_titles: List[str],
image_textual_information: Union[str, List[str]]):
"""documents the current experiment results as an entry in the experiment log list. if self.
activation_threshold was updated by the dataset_exemplars function,
the experiment log will contain instruction to continue with experiments if activations are lower than
activation_threshold.
Results that are logged will be available for future experiments (unlogged results will be unavailable).
The function also updates the attribute "result_list", such that each element in the result_list is a
dictionary of the format: {"<prompt>": {"activation": act, "image": image}}
so the list contains all the results that were logged so far.
Parameters
----------
activation_list : List[int]
A list of the activation values that were achived for each of the images in "image_list".
image_list : List[str]
16AMultimodalAutomatedInterpretabilityAgent
A list of the images that were generated using the text2image model and were tested. Should be the same
length as activation_list.
image_titles : List[str]
A list of the text lables for the images. Should be the same length as activation_list.
image_textual_information: Union[str, List[str]]
A string or a list of strings with additional information to log such as the image summarization and/or
the image textual descriptions.
Returns
-------
None
Examples
--------
>>> # tests the activation value of the neuron for the prompts "a fox and a rabbit watch a movie under a
starry night sky", "a fox and a bear watch a movie under a starry night sky", "a fox and a rabbit watch
a movie at sunrise", describes the images and logs the results and the image descriptions
>>> def run_experiment(system, tools):
>>> prompt_list = ["a fox and a rabbit watch a movie under a starry night sky", "a fox and a bear watch
a movie under a starry night sky","a fox and a rabbit watch a movie at sunrise"]
>>> images = tools.text2image(prompt_list)
>>> activation_list, activation_map_list = system.neuron(images)
>>> descriptions = tools.describe_images(images, prompt_list)
>>> tools.log_experiment(activation_list, activation_map_list, prompt_list, descriptions)
>>> return
>>> # tests dataset exemplars, use umage summarizer and logs the results
>>> def run_experiment(system, tools):
>>> activation_list, image_list = self.dataset_exemplars(system)
>>> prompt_list = []
>>> for i in range(len(activation_list)):
>>> prompt_list.append(f'dataset_exemplars {i}') # for the dataset exemplars we don't have prompts,
therefore need to provide text titles
>>> summarization = tools.summarize_images(image_list)
>>> log_experiment(activation_list, activation_map_list, prompt_list, summarization)
>>> return
>>> # test the effect of changing a dog into a cat. Describes the images and logs the results.
>>> def run_experiment(system, tools) -> Tuple[int, str]:
>>> prompt = ["a dog standing on the grass"]
>>> edits = ["replace the dog with a cat"]
>>> all_images, all_prompts = tools.edit_images(prompt, edits)
>>> activation_list, activation_map_list = system.neuron(all_images)
>>> descriptions = tools.describe_images(activation_map_list, all_prompts)
>>> tools.log_experiment(activation_list, activation_map_list, all_prompts, descriptions)
>>> return
>>> # test the effect of changing the dog's action on the activation values. Describes the images and logs
the results.
>>> def run_experiment(system, prompt_list) -> Tuple[int, str]:
>>> prompts = ["a dog standing on the grass"]*3
>>> edits = ["make the dog sit","make the dog run","make the dog eat"]
>>> all_images, all_prompts = tools.edit_images(prompts, edits)
>>> activation_list, activation_map_list = system.neuron(all_images)
>>> descriptions = tools.describe_images(activation_map_list, all_prompts)
>>> tools.log_experiment(activation_list, activation_map_list, all_prompts, descriptions)
>>> return
"""
return log_experiment(activation_list, image_list, prompt_list, description)
17AMultimodalAutomatedInterpretabilityAgent
B. MAIA userprompt: neurondescription
Your overall task is to describe the visual concepts that maximally activate a neuron inside a deep network
for computer vision.
To do that you are provided with a library of Python functions to run experiments on the specific neuron
(inside the "System" class) given the functions provided in the "Tools" class. Make sure to use a variety of
tools from the library to maximize your experimentation power.
Some neurons might be selective for very specific concepts, a group of unrelated concepts, or a general
concept, so try to be creative in your experiment and try to test both general and specific concepts. If a
neuron is selective for multiple concepts, you should describe each of those concepts in your final
description.
At each experiment step, write Python code that will conduct your experiment on the tested neuron, using the
following format:
[CODE]:
```python
def run_experiment(system, tools)
# gets an object of the system class, an object of the tool class, and performs experiments on the neuron
with the tools
...
tools.log_experiment(...)
```
Finish each experiment by documenting it by calling the "log_experiment" function. Do not include any
additional implementation other than this function. Do not call "execute_command" after defining it. Include
only a single instance of experiment implementation at each step.
Each time you get the output of the neuron, try to summarize what the inputs that activate the neuron have in
common (where that description is not influenced by previous hypotheses). Then, write multiple hypotheses that
could explain the visual concept(s) that activate the neuron. Note that the neuron can be selective for more
than one concept.
For example, these hypotheses could list multiple concepts that the neuron is selective for (e.g. dogs OR cars
OR birds), provide different explanations for the same concept, describe the same concept at different levels
of abstraction, etc. Some of the concepts can be quite specific, test hypotheses that are both general and
very specific.
Then write a list of initial hypotheses about the neuron selectivity in the format:
[HYPOTHESIS LIST]:
Hypothesis_1: <hypothesis_1>
...
Hypothesis_n: <hypothesis_n>.
After each experiment, wait to observe the outputs of the neuron. Then your goal is to draw conclusions from
the data, update your list of hypotheses, and write additional experiments to test them. Test the effects of
both local and global differences in images using the different tools in the library.
If you are unsure about the results of the previous experiment you can also rerun it, or rerun a modified
version of it with additional tools. Use the following format:
[HYPOTHESIS LIST]: ## update your hypothesis list according to the image content and related activation values.
Only update your hypotheses if image activation values are higher than previous experiments.
[CODE]: ## conduct additional experiments using the provided python library to test *ALL* the hypotheses. Test
different and specific aspects of each hypothesis using all of the tools in the library. Write code to run the
experiment in the same format provided above. Include only a single instance of experiment implementation.
Continue running experiments until you prove or disprove all of your hypotheses. Only when you are confident
in your hypothesis after proving it in multiple experiments, output your final description of the neuron in
the following format:
[DESCRIPTION]: <final description> ## Your description should be selective (e.g. very specific: "dogs running
on the grass" and not just "dog") and complete (e.g. include all relevant aspects the neuron is selective for).
In cases where the neuron is selective for more than one concept, include in your description a list of all
the concepts separated by logical "OR".
[LABEL]: <final label drived from the hypothesis or hypotheses> ## a label for the neuron generated from the
hypothesis (or hypotheses) you are most confident in after running all experiments. They should be concise and
complete, for example, "grass surrounding animals", "curved rims of cylindrical objects", "text displayed on
computer screens", "the blue sky background behind a bridge", and "wheels on cars" are all appropriate. You
should capture the concept(s) the neuron is selective for. Only list multiple hypotheses if the neuron is
selective for multiple distinct concepts. List your hypotheses in the format:
[LABEL 1]: <label 1>
[LABEL 2]: <label 2>
C.Evaluationexperimentdetails
InTableA3weprovidefullevaluationresultsbylayer,aswellasthenumberofunitsevaluatedineachlayer. Unitswere
sampleduniformlyatrandom,forlargernumbersofunitsinlaterlayerswithmoreinterpretablefeatures.
18AMultimodalAutomatedInterpretabilityAgent
TableA3. Evaluationresultsbylayer
MILAN MAIA Human
Arch. Layer #Units + − + − + −
conv. 1 10 7.23 3.38 7.28 3.53 7.83 3.16
res. 1 15 0.82 0.73 0.78 0.69 0.46 0.64
ResNet-152 res. 2 20 0.98 0.92 1.02 0.90 0.83 0.95
res. 3 25 1.28 0.72 1.28 0.70 2.59 0.58
res. 4 30 5.41 2.04 7.10 1.74 7.89 1.99
Avg. 2.99 1.42 3.50 1.33 4.15 1.34
MLP1 5 1.10 0.94 1.19 0.74 0.63 0.34
MLP3 5 0.63 0.96 0.81 0.87 0.55 0.89
MLP5 20 0.85 1.01 1.33 0.97 0.84 0.84
DINO MLP7 20 1.42 0.77 1.67 0.82 2.58 0.54
MLP9 25 3.50 -1.15 6.31 -0.81 8.64 -1.06
MLP11 25 -1.56 -1.94 -1.41 -1.84 -0.61 -2.49
Avg. 1.03 -0.32 1.93 0.54 1.97 -0.23
res. 1 10 1.92 2.16 2.10 2.07 1.65 2.15
res. 2 20 2.54 2.46 2.78 2.39 2.22 2.81
CLIP-RN50 res. 3 30 2.24 1.70 2.27 1.70 2.41 1.96
res. 4 40 3.56 1.30 4.90 1.39 4.92 1.29
Avg. 2.79 1.74 3.41 1.75 3.29 1.89
C1.HumanexpertneurondescriptionusingtheMAIAtoollibrary
Werecruited8humaninterpretability
researcherstousetheMAIAAPItorun
experimentsonneuronsinordertode-
scribe their behavior. This data col-
lectioneffortwasapprovedbyMIT’s
CommitteeontheUseofHumansas
Experimental Subjects. Humans re-
ceivedtaskspecificationviatheMAIA
userprompt,wroteprogramsusingthe
functions inside the MAIA API, and
produced neuron descriptions in the
sameformatasMAIA. Allhumansub-
jects had knowledge of Python. Hu-
manslabeled25%oftheunitsineach
layerlabeledbyMAIA(onehumanla-
bel per neuron). Testing was admin-
FigureA10.Exampleinterfaceforhumansinterpretingneuronswiththesametoollibrary
isteredviaJupyterLab(Kluyveretal.,
usedbyMAIA.
2016),asdisplayedinFigureA10. Hu-
mansalsolabeled25%ofthesynthetic
neuronsusingthesameworkflow. Themediannumberofinteractionsperneuronforhumanswas7. However,formore
difficultneuronsthenumberofinteractionswereashighas39.
C2.Syntheticneurons
ToprovideagroundtruthagainstwhichtotestMAIA,weconstructedasetofsyntheticneuronsthatreflectthediverse
responseprofilesofneuronsinthewild. Weusedthreecategoriesofsyntheticneuronswithvaryinglevelsofcomplexity:
monosemanticneuronsthatrespondtosingleconcepts,polysemanticneuronsthatrespondtologicaldisjunctionsofconcepts,
and conditional neurons that respond to one concept conditional on the presence of another. The full set of synthetic
19AMultimodalAutomatedInterpretabilityAgent
neuronsacrossallcategoriesisdescribedinTableA4. Tocapturereal-worldneuronbehaviors,conceptsaredrawnfrom
MILANNOTATIONS,adatasetof60Khumanannotationsofprototypicalneuronbehaviors(Hernandezetal.,2022).
TableA4. Syntheticneurons.ConceptsaredrawnfromMILANNOTATIONS.
Monosemantic Polysemantic(AORB) Conditional(A| )
B
arch animal,door ball,hand
bird animal,ship beach,people
blue baby,dog bird,tree
boat bird,dog bridge,sky
brick blue,yellow building,sky
bridge bookshelf,building cup,handle
bug cup,road dog,leash
building dog,car fence,animal
button dog,horse fish,water
carwindow dog,instrument grass,dog
circle fire,fur horse,grass
dog firework,whisker instrument,hand
eyes hand,ear skyline,water
feathers necklace,flower sky,bird
flame people,building snow,road
frog people,wood suit,tie
grass red,purple tent,mountain
hair shoe,boat water,blue
hands sink,pool wheel,racecar
handle skirt,water
hat stairs,fruit
jeans temple,playground
jigsaw truck,train
legs window,wheel
light
neck
orange
paws
pencil
pizza
roof
shirt
shoes
sky
snake
spiral
stripes
sunglasses
tail
text
tires
tractor
vehicle
wing
yarn
Synthetic neurons are constructed using Grounded DINO (Liu et al., 2023) in combination with SAM (Kirillov et al.,
2023). Specifically,Grounded-DINOimplementsopen-vocabularyobjectdetectionbygeneratingimageboundingboxes
correspondingtoaninputtextprompt. TheseboundingboxesarethenfedintoSAMasasoftprompt,indicatingwhich
partoftheimagetosegment. Toensurethetextualrelevanceoftheboundingbox,wesetathresholdtofilteroutbounding
boxes that do not correspond to the input prompt, using similarity scores which are also returned as synthetic neuron
activationvalues. Weusethedefaultthresholdsof0.3forboundingboxaccuracyand0.25fortextsimilaritymatching,as
recommendedin(Liuetal.,2023). Afterthefinalsegmentationmapsaregenerated,per-objectmasksarecombinedand
dilatedtoresembleoutputsofneuronsinsidetrainedvisionmodels,instrumentedviaMAIA’sSystemclass.
20AMultimodalAutomatedInterpretabilityAgent
Wealsoimplementcompoundsyntheticneuronsthatmimicpolysemanticneuronsfoundinthewild(vialogicaldisjunction),
and neurons that respond to complex combinations of concepts (via logical conjunction). To implement polysemantic
neurons(e.g. selectiveforAORB),wecheckifatleastoneconceptispresentintheinputimage(ifbotharepresent,we
mergesegmentationmapsacrossconceptsandreturnthemeanofthetwoactivationvalues). Toimplementconditional
neurons(e.g. selectiveforA| ),wecheckifAispresent,andiftheconditionismet(Bispresent)wereturnthemaskand
B
activationvaluecorrespondingtoconceptA.
Thesetofconceptsthatcanberepresentedbysyntheticneuronsislimitedbythefidelityofopen-setconceptdetection
using current text-guided segmentation methods. We manually verify that all concepts in the synthetic neuron dataset
canbeconsistentlysegmentedbyGroundedDINOincombinationwithSAM.Therearesometypesofneuronbehavior,
however,thatcannotbecapturedusingcurrenttext-guidedsegmentationmethods. Someneuronsinsidetrainedvision
modelsimplementlow-levelprocedures(e.g. edge-detection),orhigherlevelperceptualsimilaritydetection(e.g. sensitivity
toradialwheel-and-spokepatternscommontoflowerpetalsandbicycletires)thatGroundedDINO+SAMcannotdetect.
Futureimplementationscouldexplorewhetheranend-to-endsinglemodelopen-vocabularysegmentationsystem,suchas
SegmentEverythingEverywhereAllatOnce(Zouetal.,2023),couldperformsegmentationforricherneuronlabels.
Evaluation of synthetic neuron labels using human judges. This data collection effort was approved by MIT’s
CommitteeontheUseofHumansasExperimentalSubjects. Tocontrolforquality,workerswererequiredtohaveaHIT
acceptancerateofatleast99%,bebasedintheU.S.,andhaveatleast10,000approvedHITs. Workerswerepaid0.10per
annotation. 10humanjudgesperformedeachcomparisontask.
D.Ablationstudies
Weusethesubsetof25%neuronslabeledbyhumanexpertstoperformtheablationstudies. Resultsofthepredictive
evaluation procedure described in Section 4 are shown below. Using DALL-E 3 improves performance over SD-v1.5.
TableA5. NumericaldatafortheablationsinFigure7.
ImageNet SD-v1.5 DALL-E3
ResNet-152 + 3.53 3.56 4.64
− 1.54 1.33 1.53
DINO-ViT + 1.48 1.98 1.88
− -0.37 -0.23 -0.27
CLIP-RN50 + 2.34 3.62 4.34
− 1.90 1.75 1.90
E.Failuremodes
E1.ToolFailures
MAIA is often constrained by the capibili-
ties of its tools. As shown in Figure A11,
the Instruct-Pix2Pix (Brooks et al., 2022)
andStableDiffusion(Rombachetal.,2022b)
modelssometimesfailtofollowtheprecise
instructionsinthecaption. Instruct-Pix2Pix
typicallyhastroublemakingchangeswhich
arerelativetoobjectswithintheimageand FigureA11.Examplewherethetoolsthat MAIA hasaccessto,failtocorrectly
alsofailstomakechangesthatareunusual generateanimagethewayMAIArequested.
(such as the example of replacing a person
21AMultimodalAutomatedInterpretabilityAgent
withavase). StableDiffusiontypicallyhasdifficultyassigningattributesinthecaptiontothecorrectpartsoftheimage.
TheseerrorsinimageeditingandgenerationsometimesconfuseMAIAandcauseittomakethewrongprediction.
E2.ConfirmationBias
Insomescenarios,whenMAIAgeneratesanimagethathasa
higheractivationthanthedatasetexemplars,itwillassumethat
theneuronbehavesaccordingtothatsingleexemplar.Insteadof
conductingadditionalexperimentstoseeiftheremaybeamore
generallabel,MAIAsometimesstopsexperimentingandoutputs
afinallabelthatisspecifictothatoneimage. Forinstance,in
FigureA12MAIAgeneratesoneunderwaterimagethatattains
ahigheractivationandoutputsanoverlyspecificdescription
withoutdoinganyadditionaltesting.
F.SpuriousFeatureRemovalExperiment
F1.DatasetSelection
We use the Spawrious dataset as it provides a more complex
classification task than simpler binary spurious classification
benchmarkslikeWaterbirds(Wahetal.,2011;Sagawaetal.,
2020)andCelebA(Liuetal.,2015;Sagawaetal.,2020). All
theimagesinthedatasetaregeneratedwithStableDiffusion
v1.4(Rombachetal.,2022b),whichisdistinctfromtheStable
Diffusionv1.5modelthatweequipMAIAwith. SeeLynchetal.
(2023)forfurtherspecificdetailsonthedatasetconstruction.
FigureA12.Exampleof MAIA havingconfirmationbiasto-
wardsasinglegeneratedexample,insteadofgeneratingfurther
F2.ExperimentDetails experimentstotestotherpossibilties.
Here, we describe the experiment details for each row from
Table 2. Note that for all the logistic regression models that
we train, we standardize the input features to have a zero
mean and variance of one. We use the 'saga' solver from
sklearn.linear_model.LogisticRegression for
theℓ regressionsandthe'lbfgs'solverfortheunregularizedregressions(Pedregosaetal.,2011).
1
All,OriginalModel,Unbalanced: WetrainaResNet-18model(Heetal.,2016)foroneepochontheO2O-Easydataset
fromSpawrioususingalearningrateof1e-4,aweightdecayof1e-4,andadropoutof0.1onthefinallayer. Weusea90-10
splittogetatrainingsetofsize22810andavalidationsetofsize2534.
ℓ Top50,All,Unbalanced: Wetunetheℓ regularizationparameteronthefullunbalancedvalidationsetsuchthatthere
1 1
are50neuronswithnon-zeroweigths,andweextractthecorrespondingneuronsindices. Wethendirectlyevaluatethe
performanceofthelogisticregressionmodelonthetestset.
ℓ 1Top50,Random,Unbalanced: TomatchMAIA’ssparsitylevel,weextract100setsof22randomneuronindicesand
performunregularizedlogisticregressionontheunbalancedvalidationset.
ℓ 1Top50,ℓ 1Top22,Unbalanced: Wealsouseℓ 1regressiontomatchMAIA’ssparsityinamoreprincipledmanner,tuning
theℓ parameteruntilthereareonly22neuronswithnon-zeroweights. Wethendirectlyevaluatetheperformanceofthe
1
regularizedlogisticregressionmodelonthetestset.
ℓ 1Top50,MAIA,Unbalanced: WerunMAIAoneachofthe50neuronsseparately,anditultimatelyselects22outofthe
50neurons. Wethenperformunregularizedlogisticregressionwiththisneuronsubsetontheunbalancedvalidationset. We
useamodifieduserpromptwhichweincludeinSectionF4.
Next,forthebalancedvalidationfine-tuningexperiments,wesampletenbalancedvalidationsetsofsize320andreportthe
meanperformancesofeachmethod. WhileKirichenkoetal.(2023)usesmultiplesubsampledbalancedvalidationsetsfor
22AMultimodalAutomatedInterpretabilityAgent
fine-tuningandthenaggregatesthemodelsforscoringonthetestset,weonlyallowthefollowingexperimentstoseea
singlebalancedvalidationsetsinceweseektocomparetheperformanceofMAIAtomethodswhichhaveaccesstoasmall
balancedfine-tuningdataset,ratherthanspuriousfeaturelabelsforeverydatasampleinalargevalidationset.
All,ℓ Tuning,Balanced: Weperformahyperparametersearchfortheℓ parameter,evaluatingeachhyperparametervalue
1 1
withfive50-50splitsofthebalancedvalidationdataset,trainingononehalfandevaluatingontheotherhalf. Wethentrain
thefinalmodelwiththebestℓ parameterontheentirebalancedvalidationdataset. Forthenumberofneurons,wereport
1
themediannumberofneuronswithnon-zeroweightsacrossthetentrials.
All, ℓ
1
Top 22, Balanced: We also investigate the performance when we match MAIA’s chosen neuron sparsity level
bytuningtheℓ parametertoonlyhave22neuronswithnon-zeroweights. Wetrainthemodeldirectlyonthebalanced
1
validationdataset.
F3.ExampleMAIANeuronRobustnessInteractions
InFigureA13,weshowexamplesofMAIA’sinteractionwithneuronsinthefinallayeroftheResnet-18modeltrainedon
Spawrious. Overall,wefindthatMAIAwilloftentestallfourdogbreedsacrossdifferentenvironments,andifitfindsthat
theexemplarsallhavethesamebackground,itwilltesttoseeifthebackgroundisaspuriousfeature.
FigureA13. TwodifferentMAIAinteractions,classifyingneuronsasselective(L)andspurious(R).
F4.MAIAPromptingDetails
TheuserprompttoMAIAspecifyingthespuriousneuronidentificationtaskissimilartothepromptspecifyingtheneuron
descriptiontask. Itisshowninfullbelow. WeonlyspecifytoMAIAthedomainofimagesthemodelwastrainedonand
thedefinitionsofspuriousandselectiveneuronssothatitcanoutputafinalclassificationusingexperimentsthatwillnot
generatesignificantlyout-of-distributionimages(suchasunrelateddogbreedsorclose-upposesnotpresentinthetraining
data). Wedonoteverspecifywhatthespuriouscorrelationsare.
23AMultimodalAutomatedInterpretabilityAgent
You are analyzing the prototypical behavior of individual neurons inside a deep neural network which
classifies the breed of a dog in a natural image as one of the following breeds that were in its training
dataset:
1. Labrador
2. Welsh Corgi
3. Bulldog
4. Dachshund
Your overall task is to classify the neuron as SELECTIVE (if it is selective for one and only one dog breed)
or SPURIOUS (if it is not). Conduct experiments until you meet the following criteria for SELECTIVE or
SPURIOUS.
SELECTIVE
Criteria:
1. The neuron only activates for one single breed of dog. It does not activate for any other breeds of dog,
even at lower magnitudes of activation. We consider a neuron to be activated if it returns a highlighted image
region with a red outline. Observe the presence or absence of a mask, and the magnitude of an activation
value, to determine whether a neuron is active.
2. The neuron is active for that single dog breed robustly across a variety of contexts. However, keep in mind
that the neuron has only seen images of full-body dogs in the following environments: beach, desert, muddy
park, jungle, and snow. Do not generate images that are very dissimilar to what the neural network has seen
before (eg. do not generate close-ups, do not generate humans). A selective neuron does not need to activate
for these dissimilar images.
SPURIOUS
Criteria:
1. The neuron activates for mutiple breeds of dogs. The amount the neuron activates for each breed of dog may
vary but the neuron is considered spurious if it activates enough for multiple breeds to return a highlighted
image region with a red outline.
2. The neuron may activate for a single breed of dog, but only does so in specific contexts. If the neuron's
activation is dependent on non-dog related concepts, it is considered spurious.
To complete your task, you are provided with a library of Python functions to run experiments on the specific
neuron (inside the "System" class) given the functions provided in the "Tools" class. Make sure to use a
variety of tools from the library to maximize your experimentation power.
Some neurons might be selective for very specific concepts, a group of unrelated concepts, or a general
concept, so try to be creative in your experiment and try to test both general and specific concepts. If a
neuron is selective for multiple concepts, you should describe each of those concepts in your final
description. At each experiment step, write Python code that will conduct your experiment on the tested neuron
, using the following format:
[CODE]:
```python
def execute_command(system, tools)
# gets an object of the system class, an object of the tool class, and performs experiments on the neuron
with the tools
...
tools.save_experiment_log(...)
```
Finish each experiment by documenting it by calling the "save_experiment_log" function. Do not include any
additional implementation other than this function. Do not call "execute_command" after defining it. Include
only a single instance of experiment implementation at each step. Each time you get the output of the neuron,
try to summarize what inputs that activate the neuron have in common (where that description is not influenced
by previous hypotheses), and make a hypothesis regarding whether the neuron is SELECTIVE (activates strongly
for only one dog breed across contexts) or SPURIOUS (activates for more than one dog breed or non-dog related
features). After each experiment, wait to observe the outputs of the neuron. Then your goal is to draw
conclusions from the data, update your hypothesis, and write additional experiments to test it. Test the
effects of concepts like different dog breeds or background features using the different tools in the library.
If you are unsure about the results of the previous experiment you can also rerun it, or rerun a modified
version of it with additional tools.
Use the following format:
[HYPOTHESIS]: ## update your hypothesis according to the image content and related activation values. Only
update your hypotheses if image activation values are higher than previous experiments.
[CODE]: ##Test different and specific aspects of your hypothesis using all of the tools in the library. Write
code to run the experiment in the same format provided above. Include only a single instance of experiment
implementation.
Continue running experiments until you meet one of the following stopping criteria.
SPURIOUS: If you find multiple pieces of evidence which show that the neuron does not only activate for one
breed or activates for non-dog related concepts you should deem the neuron to be spurious, even if you are not
entirely sure what the neuron activates for. Remember that we consider a neuron to have activation if it
returns a highlighted image region with a red outline. If you see this happen for a feature that is not dog
related or for multiple breeds you should deem the neuron to be spurious.
SELECTIVE: The neuron is selective for a single dog breed and activates strongly for that breed across
contexts. If you find any evidence suggesting that a neuron is spurious (such as an image outside one
particular breed showing activations), you should conduct more experiments to test your hypotheses. If you are
ever unsure about the result, you should lean towards outputting SPURIOUS. The neuron must be perfectly
selective across many variations of dog breeds and contexts to be considered to be selective. Once you have
met one of the stopping criteria, output your final classification of the neuron in the following format:
[REASONING]: <why the neuron is SELECTIVE or SPURIOUS, and if SELECTIVE, the breed it is SELECTIVE for>
[LABEL]: <SELECTIVE/SPURIOUS>
24AMultimodalAutomatedInterpretabilityAgent
G.Biasidentificationexperiment
TheuserprompttoMAIAspecifyingthebiasidentificationtaskissimilartothepromptspecifyingtheneurondescription
task. Weproduceitinfullbelow.
Your overall task is to describe the visual concepts that maximally activate a neuron inside a deep network
for computer vision.
You will get an access to a neuron inside a deep neural network that is sensitive to the presence of {
class_label} inside the input image. Your overall task is to reveal biases in the neuron's behavior. That is,
your task is to find specific contexts or settings in which the neuron tends to have relatively higher
activation values, but only covers partial asspect of the {class_label} class, or specific scenarios when the
neuron tends to have relatively lower activations although {class_label} is still present.
Try to look for different type of biases (e.g. gender, ethnicity, context-dependencies, breed-dependencies,
etc.).
To do that you are provided with a library of Python functions to run experiments on the specific neuron (
inside the "System" class) given the functions provided in the "Tools" class. Make sure to use a variety of
tools from the library to maximize your experimentation power. Some neurons might be selective for very
specific concepts, a group of unrelated concepts, or a general concept, so try to be creative in your
experiment and try to test both general and specific concepts. If a neuron is selective for multiple concepts,
you should describe each of those concepts in your final description. At each experiment step, write Python
code that will conduct your experiment on the neuron, using the following format:
[CODE]:
```python
def execute_command(system, tools)
# gets an object of the system class, an object of the tool class, and performs experiments on the neuron
with the tools
...
tools.save_experiment_log(...)
```
Finish each experiment by documenting it by calling the "save_experiment_log" function. Do not include any
additional implementation other than this function. Do not call "execute_command" after defining it. Include
only a single instance of experiment implementation at each step.
Each time you get the output of the neuron, try to summarize what inputs that activate the neuron have in
common (where that description is not influenced by previous hypotheses). Then, write multiple hypotheses that
could explain the biases of the neuron.
For example, these hypotheses could list multiple context that the neuron is less selective for.
Then write a list of initial hypotheses about the neuron biases in the format:
[HYPOTHESIS LIST]:
Hypothesis_1: <hypothesis_1>
...
Hypothesis_n: <hypothesis_n>.
After each experiment, wait to observe the outputs of the neuron. Then your goal is to draw conclusions from
the data, update your list of hypotheses, and write additional experiments to test them. Test the effects of
both local and global differences in images using the different tools in the library. If you are unsure about
the results of the previous experiment you can also rerun it, or rerun a modified version of it with
additional tools.
Use the following format:
[HYPOTHESIS LIST]: ## update your hypothesis list according to the image content and related activation values.
Only update your hypotheses if image activation values are higher than previous experiments.
[CODE]: ## conduct additional experiments using the provided python library to test *ALL* the hypotheses. Test
different and specific aspects of each hypothesis using all of the tools in the library. Write code to run
the experiment in the same format provided above. Include only a single instance of experiment implementation.
Continue running experiments until you prove or disprove all of your hypotheses. Only when you are confident
in your hypothesis after proving it in multiple experiments, output your final description of the neuron in
the following format:
[BIAS]: <final description of the neuron bias>
25