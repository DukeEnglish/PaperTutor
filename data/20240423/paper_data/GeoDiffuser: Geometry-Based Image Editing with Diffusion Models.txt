GeoDiffuser: Geometry-Based Image Editing with
Diffusion Models
Rahul Sajnani1,2, Jeroen Vanbaar2, Jie Min2, Kapil Katyal2, and Srinath
Sridhar1,2
1 Brown University, Providence, Rhode Island, USA
2 Amazon Robotics, North Reading, Massachusetts, USA
ivl.cs.brown.edu/projects/geodiffuser
2D Edit 3D Edit 3D Edit Removal Scale
Fig.1: WeintroduceGeoDiffuser,aunifiedmethodtoperformcommon2Dand3D
imageeditingtaskslikeobjecttranslation,3Drotation,objectremoval,andre-scaling
whilepreservingobjectstyleandinpaintingdisoccludedregions.Ourmethodisazero-
shot optimization-based method that builds on top of a pre-trained diffusion model.
We treat image editing as a geometric transformation of parts of the image and bake
thisdirectlyintoasharedattention-basededitoptimization.Inthisfigure,thetoprow
shows natural images and the bottom row shows the edit. vs.
Abstract. The success of image generative models has enabled us to
build methods that can edit images based on text or other user input.
However,thesemethodsarebespoke,imprecise,requireadditionalinfor-
mation, or are limited to only 2D image edits. We present GeoDiffuser,
azero-shotoptimization-basedmethodthatunifiescommon2Dand3D
image-based object editing capabilities into a single method. Our key
insightistoviewimageeditingoperationsasgeometrictransformations.
Weshowthatthesetransformationscanbedirectlyincorporatedintothe
attention layers in diffusion models to implicitly perform editing opera-
tions. Our training-free optimization method uses an objective function
that seeks to preserve object stylebutgenerate plausible images, for in-
stance with accurate lighting and shadows. It also inpaints disoccluded
partsoftheimagewheretheobjectwasoriginallylocated.Givenanat-
ural image and user input, we segment the foreground object [28] and
estimate a corresponding transform which is used by our optimization
approachforediting.Figure1showsthatGeoDiffusercanperformcom-
mon 2D and 3D edits like object translation, 3D rotation, and removal.
Wepresentquantitativeresults,includingaperceptualstudy,thatshows
how our approach is better than existing methods.
4202
rpA
22
]VC.sc[
1v30441.4042:viXra2 R. Sajnani et al.
1 Introduction
Image generative models have seen significant progress recently. The most ad-
vanced diffusion-based models can now generate high-quality images almost in-
distinguishablefromreality[46,48,50,61].Thesemodelsgenerateimageswiththe
desired content and detail by conditioning on text prompts, sometimes in com-
bination with additional information like segmentation masks [64]. They have
proliferated in use with many commercial products incorporating them [2–4].
Althoughrealisticimagegenerationisanimportantcapability,inmanycases,
we may also want to edit generated or existing natural images. While past work
relied on computer graphics techniques for image editing [11,27,29,65], recent
works have put generative models to use for this problem. In particular, gen-
erative models have been shown to enable text-based edits [21,38,57], object
stitching [15,53], object removal [48], and interactive edits using user-defined
points [39,42,51], 3D transforms [44] or flow [18]. However, these methods have
important limitations. Text-based editing methods are imprecise for edits re-
quiring spatial control. Object stitching and removal methods cannot easily be
extended to geometric edits. Finally, interactive point-/flow-based methods re-
quire additional input such as a text prompt or optical flow.
Inthispaper,wepresentGeoDiffuser,amethodthatunifiesvariousimage-
basedobjecteditingcapabilitiesintoasinglemethod.Wetaketheviewthatcom-
mon user-specified image editing operations can be cast as geometric trans-
formations of parts of the image. For instance, 2D object translation or 3D
object rotation can be represented as a bijective transformation of the fore-
ground object. However, naively applying this transformation on the image is
unlikely to produce plausible edits, for instance, due to mismatched lighting or
shadows. To overcome this problem, we use diffusion models, specifically the
general editing approach (see Figure 2) enabled by DDIM Inversion [37]. Our
key contribution is to bake in the geometric transformation directly within the
shared attention layers of a diffusion model to preserve style while enabling
a wide range of user-specified 2D and 3D edits. Additionally, GeoDiffuser is a
zero-shotoptimization-basedmethodthatoperateswithout the need for any
additional trainingandcansupportanydiffusionmodelwithattentionlayers.
Figure 1 shows common image edits performed by GeoDiffuser on natural
images. Without any hyperparameter tuning, our method can perform 2D edits
like object translation or removal, or 3D edits like 3D rotation and translation.
Givenanaturalimage,wefirstsegmenttheobjectofinterest[28],andoptionally,
extract a depth map [60] for 3D edits. For each type of edit, we first compute
a geometric transform based on user input and formulate an objective function
for optimization. Unlike approaches that first ‘lift’ an object from an image and
thenstitchthetransformedobjectbackintotheimage[27],weimplicitlyperform
these steps by applying the transform directly to the self- and cross-attention
layers. Since attention captures both local and global image interactions, our
results exhibit accurate lighting, shadows and reflection while inpainting the
disoccluded image regions. Moreover, our objective function incorporates terms
to preserve the original style of the transformed object.GeoDiffuser: Geometry-Based Image Editing with Diffusion Models 3
Weshowextensivequalitativeresultsthatdemonstratethatourmethodcan
performmultiple2Dand3Deditingoperationsusingasingleapproach.Toeval-
uate our method quantitatively, we provide experiments through a perceptual
study as well as metrics that measure how well the foreground and background
content is preserved during the edit. Results show that our method outperforms
existing methods quantitatively while being general enough to perform various
kinds of edits. To sum up, our main contributions are:
– Aunifiedimageeditingapproachthatformulatescommon2Dand3Dediting
operations as geometric transformations of parts of the image.
– GeoDiffuser, a zero-shot optimization-based approach that incorporates ge-
ometric transforms directly within the attention layers of diffusion models
enabling realistic edits while preserving object style.
– Qualitative results of numerous 2D and 3D object edits enabled by our
method without any hyperparameter tuning (see Figure 1).
2 Related Work
Image editing has been widely studied in computer vision and encapsulates a
rangeofoperations,suchasobjectremovalandaddition[7,53],styletransfer[19,
22,24,26], and 2D and 3D transforms [27]. One challenge with this problem is
to keep the edit consistent within the global context of the image. Traditional
methodssuchasPoissonimageediting[45]usegradientsofthecontexttoblend
edits with existing pixels, while inpainting methods uses boundary and context
to fill in pixels [59]. We limit our discussion below to generative model-based
and 3D-aware editing methods.
Text-Guided Image Editing: There are several works using generative im-
age models to edit images via changes to the text prompt. The preservation of
subject identity in different settings can be achieved by textual inversion along
with additional losses to finetune the generative model [49]. Null-text inversion
is an inversion approach where a null-text embedding is optimized to match an
inverted noise trajectory for a given input image along with attention reweight-
ing [37]. Instead of an inversion process, text prompt edits can also be achieved
byswap,orre-weightingofcross-attentionmapsderivedfromthevisualandtex-
tual representation [21]. Edits with text prompts can also be achieved by using
cross-attention from different prompts to manipulate self-attention maps [10].
Leveraging existing text-to-image models along with [9] gives the ability to gen-
eratepaireddataforfinetuningagenerativemodeltoachievetext-guidedediting
results. These methods mostly produce images with style changes or enhance-
ments,orobjectreplacement.[16]leveragepromptsandselfguidancetoperform
2D image edits of scaling and translation. However, it is difficult to guide the
diffusionmodeltoperformaspecific3Dgeometrictransformbasedonaprompt.
Weextendsomeoftheaboveapproachestobuildamethodtohandlegeometric
transforms without the need for additional training.
Non-Text-Guided Image Editing: Text-guided edits are mostly limited to
appearance and style changes. Non-text-guided edits on the other hand, can4 R. Sajnani et al.
achieveavarietyofedits.Point-basededitingapproachescanperformlocalimage
edits.[51]proposeamotionsupervisedlatentoptimizationbetweenthereference
and target edit, to guide the denoising to obtain the edit while preserving the
objectidentity.Stroke-basededitingcaneditlargerimageregions,orevenentire
images[35],byprojectingstrokesontotheimagemanifoldviadiffusion.Forthese
methods, edits such as translations are however not possible. ObjectStitch [53]
along with inpainting can achieve translation where the denoising diffusion is
applied to a target asked region, and guided by the embedding of the object
to stitch. However, object style preservation is difficult in this setting. Recent
methods[39,40]trytopreserveidentityandallowfortranslationswhilerequiring
notraining.However,thesearelimitedto2Dtranslationsandscaling.Anediting
approachwhichfirst‘lifts’theobjectfromabackgroundisproposedin[44].The
background is inpainted and a depth-to-image generative model is used, which
performs the denoising conditioned on an input depth. However, this approach
needs an additional text prompt while ours does not. Additionally, we support
various kinds of edits and not just 3D transforms. [18] is concurrent work that
uses flow-guidance for image editing. However, optical flow can be much harder
to obtain compared to depth [60]. We present a method that performs 2D and
3D edits using precise geometric transformations while preserving identity and
not requiring additional user input.
3D-AwareEditing: Somemethodshaveaddressedthe3Deditingproblem[27]
by ‘lifting’ objects into 3D and use 3D meshes and scene illumination to allow
for proper blending of the edited object with the existing image context. Other
methods use NeRF [13,20,58,62,63] or works [31,32] learn over large-scale
datasets [12], leverage geometry representations to perform edits but require
multi-view images that are difficult to obtain. Edits are also directly applied
to generative models, e.g., [43] propose a point-based edit along with motion
supervision to guide the neighboring pixels. The authors of [41] propose to rep-
resent foreground objects and background as neural feature fields, which can be
edited and composited for a final output. The method of [30] addresses limita-
tionsofpoint-basededitinginGANs,usingtemplatefeaturesratherthanpoints
for better tracking, and restricting search area around pixels to lines.
3 Background
Denoising Diffusion: We first briefly describe the concept of Denoising Dif-
fusion Probabilistic Models (DDPM) used successfully by diffusion models for
image generation [48]. Images can be considered as samples drawn from a data
distributionq(x).Denoisingdiffusionaimstolearnaparameterizedmodelp (x)
θ
that approximates q(x) and from which new samples can be drawn. The (for-
ward) diffusion process iteratively adds noise to an input image x , with t=0,
0
accordingtoeitherafixedorlearnedschedule,representedbyα witht∈[1,T].
t
At each timestep, the latent encoding is performed according to a Gaussian
distribution centered around the output of the previous timestep: q(x |x ) =
√ t t−1
N(x ; αx ,(1−α )I). The parameters vary over time such that p (x ) :=
t t−1 t θ TGeoDiffuser: Geometry-Based Image Editing with Diffusion Models 5
N(0,I). Using the reparameterization trick, the noised version of input x can
√ √ 0
directly be expressed as: x = α¯ x + 1−α¯ ϵ .
t t 0 t 0
The reverse process, where noise is gradually removed at each step, can be
expressed as the joint distribution p (x ) = p (x )(cid:81)T p(t)(x |x ). Under
θ 0:T θ T t=1 θ t−1 t
theassumptionoftrainablemeansandfixedvariances,aneuralnetworkϵˆ (x ,t)
θ t
can be trained with the objective of minimizing a variational lower bound to
estimate the source noise ϵ ∼N(ϵ;0,I) that determines x from x : L (ϵ ):=
0 t 0 γ θ
(cid:80)T γ E (cid:2) ∥ϵ −ϵˆ (x ,t)∥2(cid:3) . For more details see [34].
t=1 t x0∼q(x0),ϵt∼N(0,I) 0 θ t 2
Conditioning and Efficiency: This formulation can be extended to the con-
ditional case, i.e., p (x |y). The condition y could be images, (encoded) text,
θ 0:T
or something else. The computational bottleneck is the number of denoising
timestepsT,howeveranon-MarkovianvariantDenoisingDiffusionImplicitMod-
els (DDIM) was introduced to reduce the number of timesteps [52]. To further
reduce the computational burden, the diffusion process for images can be per-
formedinalowerdimensionallatent(feature)space,asproposedby[48].Aper-
ceptuallyoptimizedpretraineddecodertakesthelatentx ,andreconstructsthe
1
image x . In our work, we use a latent diffusion model together with Classifier-
0
Free Guidance (CFG) [23] for conditioning on text.
Attention: Attention was introduced as an alternative to recurrent networks
and large receptive fields in convolution-based neural networks, for capturing
local and global context [14,56]. The scaled dot-product self-attention mech-
anism adopted in transformers has found widespread application in computer
vision applications. The input is a tuple [Query (Q), Key (K), Value (V)], each
with learnable parameters via linear layers. An attention layer constructs an
(cid:16) (cid:17)
attention map AM(Q,K) := Softmax Q √KT and then computes attention as:
d
Attention(Q,K,V):=AM(Q,K)V. Here, d is the dimension of the embedding.
In addition to self-attention, the query can be derived from another input
source,e.g.,anothermodality,andusingthekeyandvaluesfromthefirstinput,
the cross-attention between the two inputs can be computed via Section 3
and Section 3. An example of cross-attention is the activation of a word in a
sentence with pixels in an image.
The correlation between semantics and pixels for image-text cross-attention
canbemodifiedinthedenoisingdiffusiongenerativeimagesettingtoadjustthe
appearance of a given generated image [21]. In addition, deriving masks from
cross-attention to guide self-attention [10] provides the ability to change the
appearance of objects while maintaining object identity.
General Editing Framework: Prior works leverage the learned capabilities
of diffusion models to perform edits to a given image, rather than a generated
one. A general framework (see Figure 2) that is followed in all these works
is to first perform an inversion [37,52] on the image. This inversion provides
us with a noise latent that sets a good starting point to regenerate the input
image as well as to edit it. Starting from the inverted noise latent, two parallel
diffusion processes generate the input image as well as the edited image. The
firstreferencediffusionprocessgeneratestheoriginalinputand,inourwork,
helps preserve un-edited regions of the image. An edit diffusion process runs6 R. Sajnani et al.
Fig.2: General image editing framework using diffusion models. (a) DDIM Inver-
sion:Theprocessofobtainingnoisetrajectory{rz ,rz ,......,rz }forthereferenceim-
0 1 t
age[52].(b) General Editing Framework:TheReferenceDiffusionProcessguides
the Edit Diffusion Process to achieve the desired edit. In GeoDiffuser, we perform
geometric 2Dand3Deditsbytransformingthesharedattentionlayersleadingtoplau-
sible edits that preserves object style, inpainting disoccluded background, and adding
details (e.g., the shadow cast by the car).
inparallelthatutilizestheattentionblocksfromthereferenceprocesstoperform
the desired edit. This shared attention is a key insight for our proposed work.
The editing framework is in sketched Figure 2 (b).
4 GeoDiffuser: Geometry-Based Image Editing
The goal of GeoDiffuser is to enable editing of segmented foreground objects
in either natural or generated images. We take the view that common editing
operations like 2D translation, 3D object rotation or object removal can be
expressed as geometric transformations of parts of the image. Naively applying
this transform to segmented foreground objects typically produces poor results
w.r.t.imagecontextanddoesnotfillinthedisoccludedbackground.Wepropose
to use diffusion models to realistically edit the image and preserve object style.
SupportedOperations: Inthispaper,wefocusongeometricedits toanimage
I specified by users through sliders that control transformations of foreground
objects. In particular, we unify three kinds of edit operations that previously
required separate bespoke methods: (1) 2D object edit operations deal with
realistically translating or scaling segmented objects within the image includ-
ing inpainting the background where the object was originally located. (2) 3D
object edit operations deal with realistically transforming objects based on
user-specified3Drotation,translationorscalingandinpaintinganydisoccluded
background as a result of the edit. Finally, (3) object removal refers to the
operation of removing the segmented object completely and inpainting the dis-
occluded background.GeoDiffuser: Geometry-Based Image Editing with Diffusion Models 7
In contrast with previous approaches, we formulate edits as an optimization
problem based on the shared attention and leverage a pre-trained text-to-image
Stable Diffusion model [48] to perform the edit. Notably, our method requires
no training and can use any diffusion model with attention. Given an image I,
an object mask M , a user-specified 2D or 3D transformation T, our goal is
obj
to edit the object in the image and inpaint any disoccluded regions introduced
by the edit. To compute T for 3D edits, we use a depth map D obtained from
DepthAnything[60]orsimplybysettingaconstantdepthof0.5m.Thisenables
us to edit in-the-wild natural images without any additional user input.
4.1 Edits via Shared Attention
Each edit operation begins by performing a DDIM inversion [52] on the given
image (Figure 2 (a)). Inverting the image provides us with the latent noise tra-
jectory that will guide the edit diffusion process. We then perform the reverse
diffusion process along with the geometry-aware attention sharing mechanism
as skecthed in Figure 2 (b). This attention sharing mechanism along with opti-
mizingfortheimagelatentsaswellastextembeddingsisthekeytoachievethe
desired geometric edit. Figure 3 (a) depicts the process for the shared attention
blocks from Figure 2 (b).
Image Inversion: For inversion, we use direct inversion [25] on the image I
withthenullprompt"".Thisinversionprovidesuswithlatents{rz ,rz ...rz }
t t−1 0
that preserve the image for the reference denoising process and guide the edit.
2D Edits: GeoDiffuser can perform 2D edits without requiring a depth map.
Through a user interface, we can obtain a transformation T corresponding to a
desired 2D translation or scaling. We define a 2D signal S : [0,1]2 → RC that
stores a per-pixel feature in the image. The signal S can represent the RGB
values or even the features of a deep network defined at each coordinate. Given
a per-pixel edit F defined on S, our shared attention mechanism uses F to
transform this signal for the desired edit. In our case, this signal is the Query
embedding of the attention layer.
3D Edits: 2Deditsarelimitedastheydonotleveragethegeometryofobjects.
We can extend 2D edits to 3D by additionally incorporating depth information
D monoculardepthestimators[8,60]orsimplyaconstantbillboarddepthmap.
Theuserspecifiesa3DrigidtransformationT whichcanthenbeusedtocompute
the per-pixel edit F as
F(S)[u]:=S[PTD[u]P−1u].
Here,P isthecameraintrinsicmatrixthatisusedtoprojectpointsintheimage
and u is the coordinate location of the signal. This edit field F captures the 3D
shape of the visible region of the object and provides an estimate of the desired
location of the object. Note that if the per-pixel edit field is known, e.g., from
optical flow, we do not need a depth map for guidance. However, optical flow is
much more challenging to obtain for a single image compared to depth maps.8 R. Sajnani et al.
Fig.3: (a) GeoDiffuser attention sharing mechanism that leverages the geometric
transformation F(·) transform the reference attention GY to guide the edit atten-
ref
tion layer. (b) Optimization Loss Functions that penalize the latents and text-
embeddings to perform the desired geometric edit.
Object Removal: Object removal introduces disocclusions to the background
where the object was originally located. We propose an additional loss (see Sec-
tion4.2)fortheoptimizationofthediffusionlatentstohandlesuchdisocclusions.
Disocclusions can also occur for 2D and 3D edits, and we consider such edits to
be composites of removal and placement operations. Our proposed formulation
for latent optimization thus extends to those edits as well.
Shared Attention: Akeyinsightofourworkisthatwecantransformobjects
bymerelyapplyingtheeditF tothequeryembeddingsofthereferenceattention
(Figure3(a)).LetrQ,rK,rV bethequeries,keys,andvalueswithinthediffusion
modelofthereferencedenoisingprocessandeQ,eK,eV bethequeries,keys,and
values of the corresponding attention block in the edit denoising process. The
reference attention guidance GY and edit attention guidance GY are then
ref edit
given by
GY :=Attention(F(rQ),rK,rV) (1)
ref
(cid:40)
Attention(eQ,rK,rV),if SelfAttention
GY := (2)
edit Attention(eQ,eK,rV),otherwise
The dot product of the edit query embeddings eQ with the reference key
embeddings rK in Eq. 2 provides us with correspondence between the edit and
referencedenoisingprocess.Thesecorrespondencespreservethebackgroundand
foreground features during the edit. To place the object at the desired location,
the edit and reference attention guidance should approximately be the same
(GY ≈GY )fortheforeground.Notethattheyneednotbeexactlythesame
ref editGeoDiffuser: Geometry-Based Image Editing with Diffusion Models 9
in the case of an ill-defined edit F. We then transform the output OY of the
edit
edit attention layer by
OY :=F(M )·GY +(1−F(M ))·GY , (3)
edit obj ref obj edit
where F(M ) refers to the foreground mask after applying the transformation
obj
F.Inotherwords,Eq.3aimtopreserveidentityfortheobjectintheeditatits
target location, while simultaneously preserve identity and consistency for the
remaining pixels (or background).
4.2 Optimization
GeoDiffuser is a zero-shot optimization-based method that operates without
the need for any additional training. We achieve this via optimization of
the latents for edit guidance. The shared attention guidance provides us with a
proxyofwheretheforegroundobjectmustbeplacedaftertheedit.Weformulate
an optimization procedure for the latents, in order to fill in the disocclusions
and penalize the deviation of the edit attention guidance from the reference
attention guidance. The loss functions used to penalize the diffusion latents in
the optimization (shown in Fig. 3 (b)) are explained in detail next.
BackgroundPreservationLoss: Performingsharedattentionguidancealong
with optimization could result in the un-edited regions of the image to also
be changed. We introduce a background preservation loss to prevent this. Let
the mask M represent the non-editable region of the image. We define the
ne
background preservation loss as
L :=mean(M ·||GY −Y || ). (4)
bg ne edit ref 1
Here, Y = Attention(Q ,K ,V ) is the attention block output for the refer-
ref ref ref ref
ence de-noising process. The reference attention preserves the style of the image
and constrains the optimization towards preserving the background.
Object Preservation Loss: Occasionally, the optimization changes the fore-
ground region of the image. This causes loss of detail in the foreground. To
prevent this, we penalize the deviation between the edit guidance and the refer-
ence guidance within the transformed foreground mask by
L :=mean(F(M )·||GY −GY || ). (5)
obj obj edit ref 1
Note, we don’t use this loss for object removal.
InpaintingLoss: Toinpaintthedisoccludedregionsoftheimage,wemaximize
thedifferencebetweentheeditguidanceattentionmapGA :=AM(eQ,rK)and
edit
thereferenceguidanceattentionmapA :=AM(rQ,rK).Letρ represent
ref obj→bg
the maximum normalized correlation score for each row in the foreground mask
oftheattentionmapGA toeachrowinthebackgroundmaskofthereference
edit
attention map A . We can similarly compute ρ that provides us with
ref obj→obj
the maximum foreground to foreground normalized correlation (see Figure 3
(b)). Our goal is to reduce ρ and increase ρ . We want to inject
obj→obj obj→bg
the disoccluded region with features from the background and ensure that the10 R. Sajnani et al.
diffusion process doesn’t in-paint the same features. We penalize for this using
(cid:16) (cid:17)
L :=mean e−dobj→bg(ln(ρ )−ln(ρ )) . (6)
remove obj→obj obj→bg
Here, d is the coordinate distance between the locations of the attention
obj→bg
map.Thelossweightedbycoordinatedistanceensuresthattheforegroundregion
inpaints the region using features within its vicinity. The negative log forces the
object to background correlation ρ to increase and also reduces object-
obj→bg
objectcorrelationforcingtheinpaintedregiontonotbefilledbythesameobject.
SmoothnessConstraint: Weadditionallypenalizetheeditattentionguidance
GY for smoothness by penalizing the absolute value of its gradients using L .
edit s
In our experiments, we found that the inpainting loss is hard to optimize
and changes every image differently. To combat this, we devise an adaptive
optimization scheme that increases the weight of the removal loss if the loss is
more than −1.8 and reduce the loss weight if the removal loss is lower than −6.
4.3 Implementation
The shared attention along with the loss functions defined above, enable per-
forming geometry image edits as a reverse diffusion process by optimizing the
latentsandtextembeddings.Tomaketheoptimizationfaster,weoptimizeevery
alternate step for the initial 32 diffusion steps. We set an initial learning rate of
1.5 and linearly decay it to 0. We share attention across all blocks within the
UNet till step 45. All our experiments are performed on an Nvidia A40 with a
run time of 25 seconds (for removal) up to 50 seconds (for 2D and 3D edits) on
image resolution of 512. We use [47] for projecting, splatting, and rendering in
our attention sharing mechanism.
5 Results & Experiments
Inthissection,wepresentvisualexamplesofoureditingresultsandquantitative
results of a perceptual study and other visual metrics of editing quality.
Dataset: To measure the efficacy of our method we collected a dataset of real
images from Adobe Stock images [1] to ensure we exclude generative AI images.
We collect 70 images corresponding to the prompts dog, car, cat, bear, mug,
lamp, boat, plane, living-room, peaceful scenery. We also test on real in-the-wild
imagesfrom[17]andgeneratedimagesfrom[44].Formanyimagesinourdataset,
weshowmultiple2Dand3Deditsdemonstratingthegeneraleditingcapabilities
of GeoDiffuser.
Baselines: Sincethereisnoextantmethodthatperformsalltypesofeditsthat
wesupport,wecompareeachedittypetoadifferentbaseline.Fortheobjectre-
moval,wecomparewithastate-of-the-artoff-the-shelf LaMaimagein-painting
model [54], dilating object masks to make LaMa work better. For the 3D edit
operations, we design a baseline based on Zero123-XL [32]. For this baseline,
wefirstuse[54]toin-painttheregionoftheremovedforegroundobjectfirst.WeGeoDiffuser: Geometry-Based Image Editing with Diffusion Models 11
then Zero123-XL to predict the novel view of the transformed object and com-
posite it to the in-painted background image using Laplacian pyramid blending.
We are unable to compare with recent/concurrent work such as DiffusionHan-
dles [44], Motion Guidance [18] and ObjectStitch [53] since no public code is
available. Moreover, our tests with [36] on real images produced poor results
and therefore we exclude them. For 2D edits, we compare with Dragon Diffu-
sion[39,40].Sincethismethodrequiresatextpromptwhileourmethoddoesnot,
wemanuallyaddedtextdescriptionstoourdataset.Wetestourmethodagainst
Zero123-XL for geometry editing and LaMa for inpainting with a perceptual
study. We additionally benchmark our method against Zero123-XL and Dragon
Diffusion using community accepted metrics of Mean Distance, Clip Similarity
Score, and Warp Error.
5.1 Quantitative Evaluation
Perceptual Study: To evaluate whether participants are satisfied with our
editing results compared to other work, we conducted a perceptual study. This
was setup as a forced choice questionnaire where participants had to select one
oftwooptionsascontainingthebesteditresult.Ofthetworandomlypresented
options, one was ours and the other was a baseline. In total we presented 70 im-
ages (30 for removal, 40 for other transforms) from our dataset. The questions
were divided in three categories: edit realism (ER), edit adherence (EA), and
removal edit realism (RRE). For removal, we generated results with LaMa [54],
andfortheremainingtwocategories,resultsweregeneratedwithLamafollowed
by Zero123-XL [32]. Each participant answered 12 ER questions, 12 EA ques-
tions and 6 RRE questions, for a total of 30 visual questions. In total 53 users
participated in the study for which they received no compensation. Please see
the supplementary document for more details.
Figure 4 shows the participant preference rate for each division of the study.
ForRRE,outofthe318choices,participantspreferredourmethodin94.06%of
thetime,whichshowsthatGeoDiffuserisbetterabletoinpaintthedisoccluded
background regions, especially removing shadows (see Figure 7). For ER, our
method was preferred 86.48% out of 636 cases. This demonstrates that GeoDif-
fuserpreservesobjectstylebetterthanothermethods,especiallyintransforming
shadows and reflections. Finally, for EA we included included 16 2D and 24 3D
edits. Our method was preferred 88.48% out of 636 cases. This demonstrates
that our method more faithfully performs the intended edit, even challenging
ones such as 3D rotation. Whereas the baseline is only capable of performing
edits from a more narrow range.
Metrics: In addition to the perceptual study, we also provide metrics to evalu-
ate and compare edit adherence as well as style preservation quantitatively. We
performed a total of 102 edits on our dataset comprising of 36 2D edits and 66
3D edits. Previously used metrics such as FID and Image Fidelity (IF) [39,51]
arenotsuitableforgeometriceditsbecausetherecouldbelargevisualdifference
(e.g.,largetranslation)andtheydonotmeasuredisocclusioninpaintingquality.
Therefore,weusethreeothermetricstobetterevaluatemethods:(1)TheMean12 R. Sajnani et al.
Warp Clip
MD↓ ↓ ↑
Error Similarity
2D&3DEdits
Zero123-XL+Lama[31,54] 19.628 0.148 0.950
GeoDiffuser(Ours) 6.420 0.0930 0.966
2DEdits
Zero123-XL+Lama[31,54] 20.29 0.134 0.929
DragonDiffusion[39] 40.38 0.160 0.965
GeoDiffuser(Ours) 5.65 0.098 0.963
Table 1: Our method adheres to the de-
sired edit having the least Mean Dis-
tance and Warp Error. While Drag- Fig.4: Results from perceptual study
onDiffusion performs slightly better on show that participants prefer our edits
CLIPsimilarity,thismetricdoesnotfully over [31] and [54] in a majority of the
measure small edits. cases.
Distance(MD)metriccomputesinterestpointsontheforegroundoftheimage
using SIFT [33] and finds correspondences between the input and edited image
using DiFT [55]. We then measure the distance between the correspondence es-
timated by DiFT and the edit specified by the user. This metric measures how
welleachapproachtransformstheforegroundobject.(2)theWarp Errormet-
ric forward warps the foreground region of the input image to the edited image
and compute the absolute difference between their pixels for the transformed
foreground. This metric measures how well each approach adheres to the edit.
(3) the CLIP Similarity metric computesthe CLIP image embedding [58] of
the input and edited image and measures the cosine similarity. A higher co-
sine similarity indicates better preservation of global context. A good approach
should preserve the global context of the image as well as adhere to the edit.
Table 1 shows quantitative comparison of our method with the baselines.
GeoDiffuseroutperformsbaselineswithanaverageMDof5.65for2Deditsand
6.42 for all edits. We also have the lowest warp loss of 0.098 for 2D edits and
0.093 for all edits. Dragon Diffusion does not perform well in these tasks since
their method fails to inpaint disocclusions or preserve the foreground. Zero123-
XL baseline performs better but since it is not trained on real-world images, it
does not preserve the foreground object well resulting in incorrect DiFT corre-
spondences. All method seem to preserve the context of the scene with a clip
score above 0.92. For 2D edits, Dragon Diffusion has the highest clip similarity
scoreof0.965however,theeditsareperformedonimageswithhigherresolution
compared to ours which might explain the difference.
Ablations: We perform a visual ablation of our design choices – please see the
supplementarymaterialformoredetailsandexperiments.Figures5and6shows
the importance of the attention sharing mechanism and adaptive optimization.
Wecanseeadegradationinstylepreservationoftheeditwhenwedon’tperform
geometric attention sharing till step 45. Without the adaptive optimization, we
need image specific tuning for loss weights which is not scalable.GeoDiffuser: Geometry-Based Image Editing with Diffusion Models 13
No Adaptive Adaptive
Input
Optimization Optimization
Fig.5: Ablation of adaptive optimization. Without adaptive optimization, the same
lossessuccessfullyinpaintsomeimageswhileothersfail(middlerow).Withouradap-
tive optimization, the same loss function works well for any image.
Input t=30 t=37 t=45
Fig.6: Ablation on the number of steps for which we use our shared attention mech-
anism. Increasing the steps t better preserves object style (translation edit shown).
Qualitative Results: We show more qualitative results of 2D and 3D edits
performed by GeoDiffuser in Figure 7. Notice how our method not only re-
moves/transforms objects but also the object’s reflection and shadows.
6 Conclusion
GeoDiffuser is a unified method to perform common 2D and 3D object edits on
images. Our approach is a zero-shot optimization-based method that uses diffu-
sion models to achieve these edits. The key insight is to formulate image editing
as a geometric transformation and incorporate it directly within the shared at-
tention layers in a diffusion model-based editing framework. Results show that
our single approach can handle a wide variety of image editing operations, and
produces better results compared to previous work.
Limitations&FutureWork: Whilewecanhandlebackgrounddisocclusions,
we cannot yet handle foreground object disocclusions resulting from large 3D
motions. Our method also occasionally generates artifacts due to downsampled
attention masks. We plan to address these limitations in future work.
Acknowledgements: Part of this work was done during Rahul’s internship at
Amazon. This work was additionally supported by NSF grant CNS #2038897,
ONR grant N00014-22-1-259, and an AWS Cloud Credits award.14 R. Sajnani et al.
Removal
2D Edits
3D Edits
Scaling
Fig.7: Qualitative results showing all variations of 2D and 3D edits performed by
GeoDiffuseronnaturalimages.Noticehowourmethodnotonlyremoves/transforms
objects but also the object’s reflection and shadows (car, couch, boat). For 3D edits,
ourmethodproducesplausibleresultsforrotationsashighas30◦.Forscaling,wecan
perform both uniform and non-uniform scaling operations.GeoDiffuser: Geometry-Based Image Editing with Diffusion Models 15
References
1. Adobe stock. http://web.archive.org/web/20080207010024/http://www.
808multimedia.com/winnt/kernel.htm
2. Amazon titan image generator, multimodal embeddings, and text models are
now available in amazon bedrock | aws news blog. https://aws.amazon.
com/blogs/aws/amazon-titan-image-generator-multimodal-embeddings-
and-text-models-are-now-available-in-amazon-bedrock/, (Accessed on
03/04/2024)
3. Dall·e 2. https://openai.com/dall-e-2, (Accessed on 03/04/2024)
4. Gemini-chattosuperchargeyourideas.https://gemini.google.com/,(Accessed
on 03/04/2024)
5. Qualtrics. https://www.qualtrics.com
6. Abid, A., Abdalla, A., Abid, A., Khan, D., Alfozan, A., Zou, J.: Gradio: Hassle-
freesharingandtestingofmlmodelsinthewild.arXivpreprintarXiv:1906.02569
(2019)
7. Avrahami,O.,Lischinski,D.,Fried,O.:Blendeddiffusionfortext-driveneditingof
naturalimages.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition. pp. 18208–18218 (2022)
8. Bhat,S.F.,Birkl,R.,Wofk,D.,Wonka,P.,Müller,M.:Zoedepth:Zero-shottransfer
by combining relative and metric depth. arXiv preprint arXiv:2302.12288 (2023)
9. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image
editing instructions. In: CVPR (2023)
10. Cao, M., Wang, X., Qi, Z., Shan, Y., Qie, X., Zheng, Y.: Masactrl: Tuning-free
mutual self-attention control for consistent image synthesis and editing (2023)
11. Chen, T., Zhu, Z., Shamir, A., Hu, S.M., Cohen-Or, D.: 3-sweep: Extracting ed-
itable objects from a single photo. ACM Transactions on graphics (TOG) 32(6),
1–10 (2013)
12. Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E.,
Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of
annotated3dobjects.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 13142–13153 (2023)
13. Dong, J., Wang, Y.X.: Vica-nerf: View-consistency-aware 3d editing of neural ra-
diance fields. Advances in Neural Information Processing Systems 36 (2024)
14. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby,
N.: An image is worth 16x16 words: Transformers for image recognition at scale.
In:9thInternationalConferenceonLearningRepresentations,ICLR2021,Virtual
Event, Austria, May 3-7, 2021. OpenReview.net (2021), https://openreview.
net/forum?id=YicbFdNTTy
15. Dwibedi,D.,Misra,I.,Hebert,M.:Cut,pasteandlearn:Surprisinglyeasysynthesis
for instance detection. In: Proceedings of the IEEE international conference on
computer vision. pp. 1301–1310 (2017)
16. Epstein,D.,Jabri,A.,Poole,B.,Efros,A.,Holynski,A.:Diffusionself-guidancefor
controllableimagegeneration.AdvancesinNeuralInformationProcessingSystems
36 (2024)
17. Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: The kitti
dataset.TheInternationalJournalofRoboticsResearch32(11),1231–1237(2013)
18. Geng, D., Owens, A.: Motion guidance: Diffusion-based image editing with differ-
entiable motion estimators. arXiv preprint arXiv:2401.18085 (2024)16 R. Sajnani et al.
19. Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,
Courville, A., Bengio, Y.: Generative adversarial nets. Advances in neural infor-
mation processing systems 27 (2014)
20. Haque,A.,Tancik,M.,Efros,A.A.,Holynski,A.,Kanazawa,A.:Instruct-nerf2nerf:
Editing 3d scenes with instructions. arXiv preprint arXiv:2303.12789 (2023)
21. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or,
D.: Prompt-to-prompt image editing with cross attention control. arXiv preprint
arXiv:2208.01626 (2022)
22. Hertz,A.,Voynov,A.,Fruchter,S.,Cohen-Or,D.:Stylealignedimagegeneration
via shared attention. arXiv preprint arXiv:2312.02133 (2023)
23. Ho, J., Salimans, T.: Classifier-free diffusion guidance (2022)
24. Jing, Y., Yang, Y., Feng, Z., Ye, J., Yu, Y., Song, M.: Neural style transfer: A
review. IEEE transactions on visualization and computer graphics 26(11), 3365–
3385 (2019)
25. Ju, X., Zeng, A., Bian, Y., Liu, S., Xu, Q.: Direct inversion: Boosting diffusion-
based editing with 3 lines of code. arXiv preprint arXiv:2310.01506 (2023)
26. Karras,T.,Laine,S.,Aila,T.:Astyle-basedgeneratorarchitectureforgenerative
adversarial networks. In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. pp. 4401–4410 (2019)
27. Kholgade,N.,Simon,T.,Efros,A.,Sheikh,Y.:3dobjectmanipulationinasingle
photograph using stock 3d models. ACM Transactions on graphics (TOG) 33(4),
1–12 (2014)
28. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint
arXiv:2304.02643 (2023)
29. Lalonde, J.F., Hoiem, D., Efros, A.A., Rother, C., Winn, J., Criminisi, A.: Photo
clip art. ACM transactions on graphics (TOG) 26(3), 3–es (2007)
30. Ling,P.,Chen,L.,Zhang,P.,Chen,H.,Jin,Y.:Freedrag:Pointtrackingisnotyou
needforinteractivepoint-basedimageediting.In:arXivpreprintarXiv:2307.04684
(2023)
31. Liu,M.,Shi,R.,Chen,L.,Zhang,Z.,Xu,C.,Wei,X.,Chen,H.,Zeng,C.,Gu,J.,
Su, H.: One-2-3-45++: Fast single image to 3d objects with consistent multi-view
generation and 3d diffusion. arXiv preprint arXiv:2311.07885 (2023)
32. Liu,R.,Wu,R.,VanHoorick,B.,Tokmakov,P.,Zakharov,S.,Vondrick,C.:Zero-
1-to-3:Zero-shotoneimageto3dobject.In:ProceedingsoftheIEEE/CVFInter-
national Conference on Computer Vision. pp. 9298–9309 (2023)
33. Lowe, G.: Sift-the scale invariant feature transform. Int. J 2(91-110), 2 (2004)
34. Luo, C.: Understanding diffusion models: A unified perspective (2022)
35. Meng,C.,He,Y.,Song,Y.,Song,J.,Wu,J.,Zhu,J.Y.,Ermon,S.:SDEdit:Guided
imagesynthesisandeditingwithstochasticdifferentialequations.In:International
ConferenceonLearningRepresentations(2022),https://openreview.net/forum?
id=aBsCjcPu_tE
36. Michel, O., Bhattad, A., VanderBilt, E., Krishna, R., Kembhavi, A., Gupta, T.:
Object 3dit: Language-guided 3d-aware image editing. Advances in Neural Infor-
mation Processing Systems 36 (2024)
37. Mokady,R.,Hertz,A.,Aberman,K.,Pritch,Y.,Cohen-Or,D.:Null-textinversion
for editing real images using guided diffusion models (2022)
38. Mokady, R., Hertz, A., Aberman, K., Pritch, Y., Cohen-Or, D.: Null-text inver-
sion for editing real images using guided diffusion models. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6038–
6047 (2023)GeoDiffuser: Geometry-Based Image Editing with Diffusion Models 17
39. Mou,C.,Wang,X.,Song,J.,Shan,Y.,Zhang,J.:Dragondiffusion:Enablingdrag-
style manipulation on diffusion models (2023)
40. Mou, C., Wang, X., Song, J., Shan, Y., Zhang, J.: Diffeditor: Boosting accuracy
and flexibility on diffusion-based image editing. arXiv preprint arXiv:2402.02583
(2024)
41. Niemeyer, M., Geiger, A.: Giraffe: Representing scenes as compositional genera-
tive neural feature fields. In: Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR) (2021)
42. Pan, X., Tewari, A., Leimkühler, T., Liu, L., Meka, A., Theobalt, C.: Drag your
gan: Interactive point-based manipulation on the generative image manifold. In:
ACM SIGGRAPH 2023 Conference Proceedings. pp. 1–11 (2023)
43. Pan, X., Tewari, A., Leimkühler, T., Liu, L., Meka, A., Theobalt, C.: Drag your
gan: Interactive point-based manipulation on the generative image manifold. In:
ACM SIGGRAPH 2023 Conference Proceedings (2023)
44. Pandey, K., Guerrero, P., Gadelha, M., Hold-Geoffroy, Y., Singh, K., Mitra, N.:
Diffusion handles: Enabling 3d edits for diffusion models by lifting activations to
3d (2023)
45. Pérez, P., Gangnet, M., Blake, A.: Poisson image editing. ACM Trans. Graph.
22(3), 313–318 (jul 2003). https://doi.org/10.1145/882262.882269, https:
//doi.org/10.1145/882262.882269
46. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
1(2), 3 (2022)
47. Ravi,N.,Reizenstein,J.,Novotny,D.,Gordon,T.,Lo,W.Y.,Johnson,J.,Gkioxari,
G.:Accelerating3ddeeplearningwithpytorch3d.arXivpreprintarXiv:2007.08501
(2020)
48. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR). pp. 10684–
10695 (June 2022)
49. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-
booth: Fine tuning text-to-image diffusion models for subject-driven generation
(2022)
50. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. Advances in Neural
Information Processing Systems 35, 36479–36494 (2022)
51. Shi, Y., Xue, C., Pan, J., Zhang, W., Tan, V.Y., Bai, S.: Dragdiffusion: Har-
nessing diffusion models for interactive point-based image editing. arXiv preprint
arXiv:2306.14435 (2023)
52. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.In:International
ConferenceonLearningRepresentations(2021),https://openreview.net/forum?
id=St1giarCHLP
53. Song, Y., Zhang, Z., Lin, Z., Cohen, S., Price, B., Zhang, J., Kim, S.Y., Aliaga,
D.: Objectstitch: Object compositing with diffusion model. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.18310–
18319 (2023)
54. Suvorov,R.,Logacheva,E.,Mashikhin,A.,Remizova,A.,Ashukha,A.,Silvestrov,
A., Kong, N., Goka, H., Park, K., Lempitsky, V.: Resolution-robust large mask
inpainting with fourier convolutions. In: Proceedings of the IEEE/CVF winter
conference on applications of computer vision. pp. 2149–2159 (2022)18 R. Sajnani et al.
55. Tang,L.,Jia,M.,Wang,Q.,Phoo,C.P.,Hariharan,B.:Emergentcorrespondence
from image diffusion. arXiv preprint arXiv:2306.03881 (2023)
56. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg,
U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.)
Advances in Neural Information Processing Systems. vol. 30. Curran Associates,
Inc. (2017), https://proceedings.neurips.cc/paper_files/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
57. Vinker, Y., Voynov, A., Cohen-Or, D., Shamir, A.: Concept decomposition for
visual exploration and inspiration. ACM Transactions on Graphics (TOG) 42(6),
1–13 (2023)
58. Wang, C., Chai, M., He, M., Chen, D., Liao, J.: Clip-nerf: Text-and-image driven
manipulation of neural radiance fields. In: Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition. pp. 3835–3844 (2022)
59. Yang,C.,Lu,X.,Lin,Z.,Shechtman,E.,Wang,O.,Li,H.:High-resolutionimage
inpainting using multi-scale neural patch synthesis. In: The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) (July 2017)
60. Yang, L., Kang, B., Huang, Z., Xu, X., Feng, J., Zhao, H.: Depth anything: Un-
leashing the power of large-scale unlabeled data. arXiv preprint arXiv:2401.10891
(2024)
61. Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O., Wang, T., Babu, A.,
Tang,B.,Karrer,B.,Sheynin,S.,etal.:Scalingautoregressivemulti-modalmodels:
Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591 (2023)
62. Yu, L., Xiang, W., Han, K.: Edit-diffnerf: Editing 3d neural radiance fields using
2d diffusion model. arXiv preprint arXiv:2306.09551 (2023)
63. Yuan,Y.J.,Sun,Y.T.,Lai,Y.K.,Ma,Y.,Jia,R.,Gao,L.:Nerf-editing:geometry
editingofneuralradiancefields.In:ProceedingsoftheIEEE/CVFConferenceon
Computer Vision and Pattern Recognition. pp. 18353–18364 (2022)
64. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
diffusion models. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 3836–3847 (2023)
65. Zheng, Y., Chen, X., Cheng, M.M., Zhou, K., Hu, S.M., Mitra, N.J.: Interactive
images:Cuboidproxiesforsmartimagemanipulation.ACMTrans.Graph.31(4),
99–1 (2012)GeoDiffuser (Supplement)
2D & 3D Edits
Removal
Fig.1:Wedisplaymorequalitativeresultsofourmethod.Eachexamplehastheinput
image in the left and the result of the edit in the right.20 R. Sajnani et al.
1 Qualitative Results
We present some more qualitative results in Figure 1. We also compare our
method against prior works in Figure 2 for 2D edits.
Input GeoDiffuser (Ours) Dragon Diffusion Zero123XL + LaMa
Fig.2: We perform the same edit using prior works and compare with out work. We
show 2D edits here as Dragon Diffusion can not perform 3D edits. Note that Dragon
Diffusion requires prompts along with the edit and our method does not.GeoDiffuser (Supplement) 21
2 Ablations
We also compute metrics over all the edits by changing the number of shared
attention steps and adaptive optimization in Table 1.
Warp Clip
MD↓ ↑
Error↓ Similarity
NumberofTimestepsforGeometricAttentionSharing
t=30 8.893 0.0965 0.875
t=37 6.994 0.0934 0.928
GeoDiffuser(t=45) 6.420 0.0930 0.966
AdaptiveOptimization
w/oAdaptiveOptimization 10.089 0.0931 0.967
GeoDiffuser(withAdaptiveOptimization) 6.420 0.0930 0.966
Table 1: Increasing the number of time steps for shared attention and adaptive opti-
mization both improve the mean distance, warp error, and clip similarity score.
3 Failure Cases
Figure 3 displays examples where our method does not perform well.
Fig.3: Eachexamplepresentstheinputimageatthetopfollowedbytheeditedimage
atthebottom.Asourgeometriceditsareperformedinalowerdimensionallatentspace,
we face aliasing and interpolation artefacts as shown in the yellow regions of the ship
(left). Occasionally our optimization results in sub-optimal solutions for foreground
(middle) and background dis-occlusions (right).
4 Perceptual Study
Our perceptual study was conducted using Qualtrics [5]. We first conducted a
pilot study having 2 images per division type with 3 users to ensure that all22 R. Sajnani et al.
questions are clear. These users did not participate in the final study. After get-
tingfeedbackfromthepilotstudyweconductedthefullstudy.Eachparticipant
completed the study within 10 minutes. They were allowed to click and enlarge
images for better inspection. We randomized the order of options presented in
the study to avoid biases.
5 Object Removal
We detail the object removal loss in Algorithm 1.
Algorithm 1 Object Removal Loss Algorithm
Require: rQ,rK,eQ,eK
Ensure: L :=RemovalLoss(rQ,rK,eQ,eK)
remove
if SelfAttentionBlock then
GA :=AM(eQ,rK) ▷ Shared Attention Map
edit
else if CrossAttentionBlock then
GA :=AM(eQ,eK) ▷ Shared Attention Map
edit
end if
GA :=AM(rQ,rK)
ref
ρ ,u :=torch_max(torch_bmm(GA ,GA )⊙M ,−1) ▷ Foreground to
obj→bg bg edit ref bg
background correlation
ρ ,_:=torch_max(torch_bmm(GA ,GA )⊙M ,−1) ▷ Foreground to
obj→obj edit ref obj
foreground correlation
d :=NormalizedCoordinateDistance(u ) ▷ Coordinate distance to the
obj→bg bg
background location having maximum correlation
L
remove
:=mean(cid:0) e−dobj→bg(ln(ρ obj→obj)−ln(ρ obj→bg))(cid:1)
6 User Interface
See Figures 4 and 5 that display the user interface used to perform edits using
GeoDiffuser. We develop this user interface using Gradio [6].GeoDiffuser (Supplement) 23
O!icial Implementation of Geometry Di!usion Editor: GeoDi!user
Editing Real Image
Foreground Image Image
Click Points to Select Object Mask
Click Points Mask Image
Height Width Show Background Image Tab
4480 6720
Transformed Image Depth Image
Transformed Mask Depth Image
tx 0.085 ty 0 rx 0 ry 0 sx 1 sy 1
Translation Translation Rotation slider Rotation slider Scale x Scale y
slider along slider along along x axis along y axis
x axis y axis
tz 0 Rrz otation slider along z axis 0 Ssz cale z 1
Translation slider along z axis
Check Transformed Image
Clear Transforms
Load exp directory
. ie/u si /_ Mo iu x/t 9puts/new_tr Load Experiment Experiment Type Save Directory Parent Path Dep dt eh
p
E thst _i am na yt to hr
ing
Get Depth
Mix ./ui_outputs/
S Sh ho ow w i n stp ita cin ht inin gg l olo ss ss w w ee igig hh tt ss Save Experiment View O pA td iova nn sced EdS it fh oino rgw M L G oo ve s eo s m m W ee e nt igr ty hts
Advanced Options g_scale 3 skip_steps 2 DDIM steps 50
Fig.4:GeoDiffuserUIGuidtanche ScaaletallowsusersStkipo Stepseditimagesinthddime stepswild.Weprovideoptions
for users to choose a mCrosso repnlaceocular de 0.9p
5
th mLateont rdeplaecel for geo 0.m
1
etriNcum fierst odptimi tsteipsng. Th 1e transformed
Cross replace Latent replace Num first optim steps
image represents the edit that the user wishes to perform. Here, the orange mask
displays the region thaSetlf repnlaceeeds to b0e.95 inpaOpitinmizte setepds . 0.65 learning rate 0.03
Self replace optimize steps learning rate
Push object depth farther away 0.1 Fast Optim Steps 0 splatting radius 1.3
from camera [0-1] Fast Optim Steps splatting radius
Push object depth farther away from
camera [0-1]
cam_focal_length 550 splatting tau 1
cam_focal_length splatting tau
splatting points per pixel 15
splatting points per pixel
Movement loss Background loss (self) 55 foreground preservation loss 15.5 loss movement_smoothness 30
background loss (self) (self) (self)
foreground preservation loss (self) loss movement_smoothness (self)
Background loss (cross) 45
background loss (cross) foreground preservation loss 8.34 loss movement_smoothness 15
(cross) (cross)
foreground preservation loss (cross) loss movement_smoothness (cross)
loss removal_scale (self) 1.6
loss removal_scale (self)
Hide Movement Panel
loss removal_scale (cross) 1.6
loss removal_scale (cross) Move Object
Edited Image
Edited Image
Input image in original aspect ratio Edited image in original aspect ratio
Download Input Image Download Image
Use via API ·Built with GradioO!icial Implementation of Geometry Di!usion Editor: GeoDi!user
Editing Real Image
Foreground Image Image
Click Points to Select Object Mask
Click Points Mask Image
Height Width Show Background Image Tab
4480 6720
Transformed Image Depth Image
Transformed Mask Depth Image
tx 0.085 ty 0 rx 0 ry 0 sx 1 sy 1
Translation Translation Rotation slider Rotation slider Scale x Scale y
slider along slider along along x axis along y axis
x axis y axis
tz 0 Rrz otation slider along z axis 0 Ssz cale z 1
Translation slider along z axis
24 R. Sajnani et al. Clear Transforms Check Transformed Image
Load exp directory
. ie/u si /_ Mo iu x/t 9puts/new_tr Load Experiment Experiment Type Save Directory Parent Path Dep dt eh
p
E thst _i am na yt to hr
ing
Get Depth
Mix ./ui_outputs/
S Sh ho ow w i n stp ita cin ht inin gg l olo ss ss w w ee igig hh tt ss Save Experiment View O pA td iova nn sced EdS it fh oino rgw M L G oo ve s eo s m m W ee e nt igr ty hts
Advanced Options g_scale 3 skip_steps 2 DDIM steps 50
Guidance Scale Skip Steps ddim steps
Cross replace 0.95 Latent replace 0.1 Num first optim steps 1
Cross replace Latent replace Num first optim steps
Self replace 0.95 Optimize steps 0.65 learning rate 0.03
Self replace optimize steps learning rate
Push object depth farther away 0.1 Fast Optim Steps 0 splatting radius 1.3
from camera [0-1] Fast Optim Steps splatting radius
Push object depth farther away from
camera [0-1]
cam_focal_length 550 splatting tau 1
cam_focal_length splatting tau
splatting points per pixel 15
splatting points per pixel
Movement loss Background loss (self) 55 foreground preservation loss 15.5 loss movement_smoothness 30
background loss (self) (self) (self)
foreground preservation loss (self) loss movement_smoothness (self)
Background loss (cross) 45
background loss (cross) foreground preservation loss 8.34 loss movement_smoothness 15
(cross) (cross)
foreground preservation loss (cross) loss movement_smoothness (cross)
loss removal_scale (self) 1.6
loss removal_scale (self)
Hide Movement Panel
loss removal_scale (cross) 1.6
loss removal_scale (cross) Move Object
Edited Image
Edited Image
Input image in original aspect ratio Edited image in original aspect ratio
Download Input Image Download Image
Use via API ·Built with Gradio
Fig.5: GeoDiffuser UI also provides options for varying parameters for editing. The
edited image in the bottom displays the image after the edit is complete.