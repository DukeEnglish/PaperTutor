AutoAD III: The Prequel – Back to the Pixels
TengdaHan1 MaxBain1 ArshaNagrani1† Gu¨lVarol1,2 WeidiXie1,3 AndrewZisserman1
1VisualGeometryGroup,UniversityofOxford 2LIGM,E´coledesPontsParisTech 3CMIC,ShanghaiJiaoTongUniversity
https://www.robots.ox.ac.uk/vgg/research/autoad/
Abstract
HowTo-AD
GeneratingAudioDescription(AD)formoviesisachalleng- (Ours)
ing task that requires fine-grained visual understanding and 106
an awareness of the characters and their names. Currently,
visual language models for AD generation are limited by a
MAD
lackofsuitabletrainingdata,andalsotheirevaluationisham-
peredbyusingperformancemeasuresnotspecializedtothe
AD domain. In this paper, we make three contributions: (i) 105 LSMDC CMD-AD
WeproposetwoapproachesforconstructingADdatasetswith (Ours)
alignedvideodata,andbuildtrainingandevaluationdatasets DiDeMo
usingthese. Thesedatasetswillbepubliclyreleased;(ii)We
developaQ-former-basedarchitecturewhichingestsrawvideo
102 103 104 105
Number of movies / videos
andgeneratesAD,usingfrozenpre-trainedvisualencodersand
Figure1.WeproposetwonewmovieAudioDescription(AD)datasets
largelanguagemodels;and(iii)Weprovidenewevaluationmet- withpixels–CMD-ADandHowTo-ADbytemporallyaligningor
ricstobenchmarkADqualitythatarewellmatchedtohuman textuallytransformingexistingpixelvideodatasets.Themarkersize
performance.Takentogether,weimprovethestateofthearton isproportionaltothetotalvideodurationsandgreycolorindicates
ADgeneration. datasetswithfeaturesinsteadofrawpixels.
crucialobjectiveofnamingthecharactersinthegeneratedtext
descriptions[21]. However,MADonlyprovidesframe-level
1.Introduction
CLIPfeatures(andonlyat5Hz)andthishaslimitedtheability
ofgenerativemodelstoprovidefine-grainedspatialdetails.Re-
Cinemaisamatterofwhat’sintheframeandwhat’sout.
centVisualLanguageModels(VLMs)[1,31–33,71,74]have
MartinScorsese
accessedthespatialfeaturemapoftheimage(orvideo)inorder
toobtainfullerdescriptionsoranswermoredetailedquestions.
Audiodescription(AD)isanaccessibilitytoolfortheblind Wemakethefollowingcontributions:First,weprovidetwo
and visually impaired that describes visual content which newdatasetsthatcanbeusedtotrainanADgenerationmodel
is essential for following video programs1. Automatically end-to-end. The datasets go beyond MAD [54] in that they
generatingADtextisachallengingtaskastheinformationmust providevideoasinput,ratherthanonlyaCLIPframefeature,
beaccurate,character-aware,story-aware,complementaryto i.e., they go back to the pixels. The first dataset, CMD-AD,
thesoundtrack,anddistilledintothegapsbetweenspeech.For isconstructedfromtwopubliclyavailablesources–theAD
TVbroadcastersintheUSandUK,providingADforacertain descriptionsforfilmsavailablefromAudioVault2andthemovie
percentageofvideocontenthasbecomealegalrequirement. clipsavailablefromCMD[4]. Thechallengeinthiscaseis
Withthecurrentpowerofvisual-to-textgenerativemodels, howtodeterminethetemporalalignmentofthesetwosources
generatingADautomaticallyisnowbecomingpossible[66], giventhatonehasonlyaudiowithAD,andtheother(CMD)
andtherehasbeenarecentflurryofinterestinthisgoal[20,21], isnon-contiguouswithtimingsunknownwithrespecttothe
kick-started by the availability of films and AD provided in originalmovies.Theseconddataset,HowTo-AD,isconstructed
theMADdataset[54]. Keyinnovationshaveincludedpartial from the large-scale HowTo100M video dataset [40] that
trainingofADgenerativemodelsusingavailablelarge-scale originallyconsistsofYouTubevideoswithnarratedinstructions.
datasets [20], and the introduction of a character bank to InspiredbytheuseofLanguageModels(LMs)torephrasethe
provide hints (as prompts) for the language model for the instructionsasvideocaptionsinHowtoCaption[53],weuse
LMstorepurposeHowTo100MasanADdatasetcontaining
1https://www.3playmedia.com/learn/popular-
topics/audio-description/ 2https://audiovault.net
1
4202
rpA
22
]VC.sc[
1v21441.4042:viXra
snoitpircsed
fo
rebmuNDataset withpixels #movies #AD totalduration thesamefilmhasADprovidedbyseveralhumanannotators.
MovieNet[24] ✓ 1100 – – Ontheseandtraditionalmetrics,weshowthatournewarchitec-
LSMDC[46] ✓ 202 118k 147h turetrainedonrawpixelsdirectlyachievesimpressiveresultsfor
MAD[54] ✗ 650 384k 1027h thetaskofMovieAD,outperformingpreviousworksonboth
CMD[4] ✓ 3605 – 1270h
thestandardMAD[54]evalset,andournewproposedtestset.
CMD[4]∩AudioVault-8K[20] ✓ 1803 – 647h
CMD-AD(ours) ✓ 1432 101k 477h
2.RelatedWork
HowTo-AD(ours) ✓ 180,034* 3.4M 23652h
Table 1. Statistics of Movie AD datasets. Only a small number Dense video captioning. With the availability of large-
of movie datasets with AD are available, and they have different scale data, the field has made significant progress in
limitations:MovieNetonlyprovideskeyframes,LSMDCisshortin captioning images [31, 32, 41, 71], and trimmed short
duration,MADonlyprovidesframe-levelvisualfeatures,andCMD video segments [35, 38, 48, 49]. Movie AD generation is
doesnothavecorrespondingADs.Weproposetwonewdatasetsfor
more related to the task of dense video captioning, where
ADgenerationtask: CMD-ADandHowTo-AD.*: strictlytheyare
the goal is to concurrently address temporal localization
longvideosratherthanmovies.
and to describe each identified interval in an untrimmed
videoswithanassociatedcharacterbank,andtextdescriptions video [30]. The approach to dense captioning has been
ofthevisualcontentthatalsonamesthepersonperformingthe exploredeitherintwostages[25,26,30,62,64]orasingle
actions.Whilethisdatasetisnotaground-truthADdataset,we stage[7,9,14,34,42,45,50,51,62,65,68,83],dependingon
showthatthepseudoground-truthannotationsareavaluable whetherlocalizationandcaptioningarejointlyaddressed.Stan-
sourceoftrainingdataforAD.Thestatisticsofthesetwonew dardevaluationbenchmarksfordensecaptioningconsistofweb
datasetsaregiveninTable1,andvisuallyillustratedinFigure1. videossuchasYouCook2[82],ActivityNetCaptions[30],and
Our second contribution is a new architecture for AD ViTT[23].Recently,Vid2Seq[68]repurposedthenarratedweb
generationthatdirectlyinputsavideoclipandcharacterbank videosYT-Temporal-1B[75],usingtranscribedspeechasthe
proposals, and outputs a character-aware description. The supervisionsource.Inasimilarspirit,HowToCaption[53]was
modelisbasedontheQ-formerarchitectureofBLIP-2[32]that collectedbytransformingthenarrationsofHowTo100M[40]
bridgesthevisualspacewiththelanguagespace,thengenerates intocaption-likedescriptionsusingLLMs,andLaVila[80]cap-
textualoutputswithalargelanguagemodel[72,77,84]. Our tionedlongvideostoenablelarge-scalevideo-textpretraining,
architecture is different from BLIP-2 [32] in that (i) it takes alsoleveragingLLMs.ThedistinctionbetweenADgeneration
multi-framemovieclipsasvisualinputs,and(ii)itincorporates and dense captioning lies in the former’s focus on character
characterbankinformationbothfromthefaceexemplarsand names, story relevance, and avoidance of interference with
thecharacternames. importantaudiocontent(e.g.characterspeech).
Ourthirdcontributionisonevaluation. Previousmethods Movie understanding datasets. For movie understanding,
useasmalltestsetofonly10moviesforevaluation.Weintro- current datasets facilitate a range of computer vision tasks
duceanevaluationdatasetof100movies,basedonouraligned includingmetadataclassification[43],VQA[58],andvisual
movieclipswithADfromAudioVault.Thishasfarmorediver- charactergrounding[47]. Thesetasksoftenrelyonauxiliary
sitythantheprevioustestsetsused,covering,e.g.sciencefiction, data such as movie plots [67], book adaptations [57], or
westerns,action,horror,cartoon,andromance.Aswellasintro- AD [54] for dense annotations. However, due to copyright
ducinganewtestset,wealsoadopttwonewevaluationmethods. constraints,manydatasetsarelimitedtoofferingvisualfeatures
ForAD,thegoldstandardevaluationistocomparethegener- (MAD[54])orsparsekeyframes(MovieNet[24]). CMD[4]
atedADwiththatprovidedbyhumans.Formodeldevelopment, circumventsthisbyprovidingurlstolicensedYouTubeclips.
however,anautomaticscalableevaluationisrequired.Previous AutoAD [20] improves on automatic AD collection, and
workshaveusedcaptioningmetricssuchasCIDEr[61]butthese provideslarge-scaleaudioADdata.
haveseverelimitationssincetheyessentiallymeasuren-gram ImprovementsinVLMsforimagesandvideos.Therecent
accuracy,andthesamesemanticADcanbepresentedinmulti- successofLLMs[12,59,60,78]andvisionencoders[15,44]
pleequivalentways.Todealwiththisproblem,[21]introduced hasledtoanexplosionofmultimodal(visionandlanguage)
aretrieval-basedassessment,evaluatinghowoftenwecanpick modelsthatcanjointlyunderstandbothvisionandtextdata.
outthecorrectADoutofmultipleneighboringADsbycompar- Thesemethodslargelyworkbymappingfrozenimageencoders
ingthemtothegeneratedADusingBertScore[79]semantictext (e.g. CLIP [44], EVA-CLIP [16]) to the textual embedding
similarity.Inthisworkweadopttwonewmeasures.Thefirst spaceoffrozenLLMs[59,60,78],forexampleFlamingo[1],
calledCRITIC,addressesoneessentialelementofADthatdis- whichdoessoviaaPerceiverresampler[27],orBLIP2[32],
tinguishesitfromstandardvideocaptioning–thatitmustname whichusesaQ-formertoachieveasimilarmapping. Video-
thecharactersinvolved.Thesecondmeasurefollowstherecent LLama[77]extendsthisideatotheaudiovisualdomain,byus-
trendinusingLLMstoassesstheveracityofcaptioning[8,11, ingthemultimodalImageBind [18]encoderinconjunctionwith
39,55,81]Asanexemplaroftheusefulnessofthesenewmea- videoandaudioQ-formers. WhileMV-GPT[48]finetunesa
sureswealsousethemtoassessinter-raterconsistencywhere nativevideobackbone[3]forthetaskofvideocaptioning,most
2worksadaptimageencoderstothecostlyvideodomainviatem- Split #movies #AD
porallysamplingafewframeswithlargestrides[10,63],orby
CMD-AD-Train 1332 93,952
representingeachframebyasingletoken[65,68,83].Giventhe CMD-AD-Eval 100 7,316
impressivegeneralisationcapabilitiesoftheseworksmadeofup total 1432 101,268
strongfrozencomponents[73],wealsoadoptasimilarapproach,
Table2.StatisticsoftheCMD-ADdataset.
leveragingVideo-LLama[77]andBLIP-2[32]modelsasour
soundtracksfromAudioVaultaudiofileshavebeenmodified
backbone,withthekeyadditionthatwealsointegratecharac-
andre-encodedtoaddtheAD,thereforetheaudiosignalsfrom
terinformation. MorerecentworkssuchasMiniGPT-4[84],
AudioVaultfilesandCMDmovieclipsarenotidentical;second,
MovieChat[55]andVideoBLIP[72]usestrongerinstruction-
AudioVaultaudiofilescoverthefullmovieduration(e.g.around
tunedLLMs,enablingfurtherzero-shotcapabilities.
90minutes),whilstaCMDclipcoversonly2minutes,andper-
Captioningevaluation.Humanevaluationisthegoldstandard
formingprecisealignmentovertheextentofthemoviehasthe
forjudgingcaptionquality,howeveritrequiresmultipleannota-
potentialformanyerroneousmatchesacrossthesearchspace;
torsforconsistency,isexpensiveandexceptionallyslow.Exist-
third, thesamemoviepublishedindifferentlocationsmight
ingautomaticmetrics,suchasBLEU,ROUGEandCIDEr[61],
havebeenrecordedatdifferentspeeds(e.g.NTSC29.97fpsvs.
allprimarilymeasuren-gramoverlap(howeverhavedifferent
PAL25fps3),introducinganotherunknownintothealignment.
weightingschemesbetweenn-grams,andacrossprecision/re-
We propose a two-stage alignment pipeline to overcome
call),anddonotcapturetheinherentsubjectivityofthetask,
thesechallengesandgetprecisetemporalalignmentbetween
wheredifferentphrasingisoftenequallyvalid. Othermetrics
hour-long AudioVault audio files and non-contiguous short
includeSPICE[2](addsactionandobjectrelationships),while
CMD movie clips from the same movie. To achieve this,
model-basedmetricsusingearlierlanguagemodelsorimage-
weusetwoquasi-independentmodalities: (i)thetranscribed
languagemodelsincludeBERT-Score[79],BERT-Score++[70]
spokentextfromthecharacters(nottheAD),and(ii)theraw
(fine-tunesBERTforimagecaptioning),LEIC[13]andNU-
audiosignalcontainingbothnon-speechsounds(music,sound
BIA[29](customtrainedmodelsforimagecaptionevaluation),
effects)andthespeech.
TIGEr [28], CLIPScore [22], and EMScore [52]. Given the
explosion of LLMs, however, recent works explore the use Stage1: Text-textalignment.Theaiminthisstageistofirst
of state-of-the-art LLMs, such as GPT-4, as a surrogate for roughly localize the CMD movie clip with the AudioVault
humans. BecausethesemodelsareoftentrainedwithRLHF, audiotoreducethesearchspace.Indetail,weuseWhisperX[5]
theyalreadyexhibitstronghumanalignment[6],andcanbe with the diarization module to separate the AD narration
usedtoassesstextqualitywell(LLM-as-a-judge).[11,81]show fromthecharacterspeech, andobtainmovie‘subtitles’with
thatusingstrongLLMsasjudges(suchasGPT-4)alignshighly timestamps for both AudioVault audio and CMD movie
withhumanpreferencesonarangeofstandardlanguage-based clips. ThesearedenotedasS ={(s ,t ),...,(s ,t )}and
AV 1 1 m m
tasks,suchasconversationalinstructionfollowing.CLAIR[8] S ={(s′,t′),...,(s′,t′)},whereeachs denotessubtitle
CMD 1 1 n n i
extendsthisideatoimagecaptioning,showingsimilarstrong strings and t denotes the temporal extent of this subtitle.
i
correlationstohumanpreferencesonvisual-languagedatasets Notethatn≪mbecauseCMDmovieclipsaremuchshorter
suchasMS-COCOandFlickr8K,whileVideoChatGPT[39] thantheentiremovie,alsothesubtitlesfromthetwosources
and MovieChat [55] use LLM-assisted evaluation for video are different because of arbitrary sentence partitioning by
taskssuchasvideoQAaswell. WhisperXorpossiblediarizationerrors.TolocalizetheCMD
clipontheAudioVaultmovietimeaxis,wecomputeasimple
3.NewDatasetsforPixelstoAD word-error-rate (WER) using a sliding window approach as
follows: we combine the CMD subtitles into a paragraph
Inthissection,wedescribeourtwonewdatasetsthatcontain P =[s′;...;s′],thencomputeWERwithAudioVaultsub-
CMD 1 n
raw video pixels mapped to AD annotation: CMD-AD titleswithinachunksizeofn.Formally,letP(i)=[s;...;s ]
(Sec. 3.1) which is based on CMD [4], and HowTo-AD AV i i+n
denote the AudioVault subtitle paragraph consisting of n
(Sec.3.2)basedonHowTo100M[40].
continuous subtitle entries starting from i-th subtitles. For a
particularCMDclip,theobjectiveoftext-textalignmentis
3.1.CMD-AD–PixelsfromAlignedCMD
(cid:110) (cid:111)
The AudioVault website provides human annotated Audio T =argmin WER(P ,P(i)) . (1)
tt-align CMD AV
Descriptions in the form of audio files with the spoken AD ti
added to the original movie soundtrack (no video). The Thetext-textalignmentisnotaccuratewhentheCMDmovie
CMD dataset [4] consists of short (about 2 minutes long) clipdoesnothavemanydialogues,e.g.inactionmovies. In
non-contiguous movie clips in the form of video files on practice,wefinditgivesreliableroughtimepointsformorethan
YouTube (around 10 clips per movie). Although there are 90%ofCMDclipsbyrandomlychecking10+moviesmanually.
about2000moviesoverlappingbetweenthesetwodatasources,
Stage2:Audio-audioalignment.Giventheroughalignment
temporallyaligningtheADwiththemovieclipsfromCMD
isanon-trivialtaskduetoseveralchallenges:First,themovie 3https://en.wikipedia.org/wiki/576i#PAL_speed-up
30.8<W′ <1.25 and empirically choose mean-square-error
MSE<100. Wefindthesetwoconditionsgiveverydecisive
boundariesforconfidentRANSACoutput. Forinstance,the
successfulRANSACfittingatFigure2hasanMSEof0.68,
whereasfailedfittingstypicallyhaveanMSE>500.
Summary. With this two-stage method, we obtain accurate
temporalalignmentbetweenAudioVaultaudioandCMDmovie
clips, thereforewecanmaptheAudioVaultADannotations
ontotheCMDtimeaxisto getvideo-textannotations. This
gives us the dataset CMD-AD (statistics are provided in
Table 2), consisting of 101k AD segments spanning 1,432
movies. Note that the total number of overlapping movies
Figure2.Audio-audioalignmentbetweentwosources.(left):For
betweenthetwodatasetsis1,803,whichmeansan80%success
eachsmallaudiosegmentonAudioVault,wefindthebest-matching
audiosegmentonCMDclip, andplottwotimestampsasscatters; rateofprecisealignment.Ahighersuccessratecanbeachieved
(right):FittingastraightlinewithRANSACwecangettheprecise by using a larger search window or an iterative alignment
mappingfunctionbetweentwosources. Theslopeofthefittedline pipeline,whichweleaveasfuturework.
0.959 < 1 indicates this CMD clip plays slightly faster than the Weuse1332moviesfortrainingand100moviesforeval-
correspondingAudioVaultchunk. uation,namingthesplitsCMD-AD-TrainandCMD-AD-Eval
sets,respectively.
(whichmaybenoisy)providedbyStage1,thisstageaimsto
verifythematch,andobtainaprecisetemporalalignmentby 3.2.HowTo-AD–PixelsfromHowTo100M
comparingaudiosignalsfromthetwosources. Theobjective
Ourseconddatasetisbasedonthelarge-scaleinstructionalvideo
istogetapreciselinearmappingforeachCMDmovieclip:
datasetHowTo100M[40],thatcontainsover1.2Mvideoswith
f:{T →T }=W·t +B, (2) ASRsubtitlesfromYouTube. Atfirstglance, theASRtran-
AV CMD AV
scriptsofthesevideosmaylookdrasticallydifferentfromthat
whereW isthespeedratebetweentheAudioVaultandCMD
ofADinmovies,sincethespokenwordsareprimarilyaimed
moviesourceswhichmightnotbe1.0duetodifferentmovie
toinstructthevieweronhowtocarryoutvariousdailytasks.
fps, and B is the time shift. The key idea here is that even
However, we can transform the instruction ASR into
though the individual CMD clips are matched locally, the
pseudo-ADintwosteps.Thefirststepistoadoptthecaptions
parametersW andBcanbeassumedtobeglobal(i.e.constant)
generatedfromHowToCaption[53],wheretheASRtranscripts
acrossthemovie.Hence,matchescanbeverifiedastheywilllie
havebeentransformedintoconciseanddescriptivecaptions
onalinespecifiedbyW andB,andthislinecanbeobtainedby
with large language models (LLMs). To improve caption
astandardrobustfittingmethod.HereweuseRANSAC[17].
temporalalignmentwiththecorrespondingvideotimestamps,
Toobtainpreciseaudioalignment,weperformalignment
the authors employ an off-the-shelf Temporal Alignment
onlow-levelaudiorepresentationmel-spectrogram. First,we
Network [19], while also discarding non-alignable subtitles
computemel-spectrogramforbothAudioVaultaudioandthe
(suchas“Hello,welcometomychannel!”). Thesecondstep
CMDmovieclip,denotedasMAVandMCMD.Weonlytake
addressesthekeydifferencebetweendescriptivecaptionsand
ashortAudioVaultaudiochunkbasedontheprevioustext-text
audiodescriptions, thatis, characternamesdonotappearin
alignmentresult.Second,wemaskoutmel-spectrogramregions
thecaptions. Forthistransformation, wedetectthesubjects
ofAudioDescriptionsbasedonthetimestampsobtainedfrom
of description sentences and uniformly replace them with a
WhisperX, as the AD signal only exists in AudioVault and
randomlychosencharactername,e.g.transforming‘amanis
notintheCMDmovieclip.Next,weperformslidingwindow
pouringwine’into‘Johnispouringwine’.Thiscompletesthe
matchingwithawindowsizew=1.6s. Foreach1.6-second
transformationfromHowTo100McaptionstotheHowTo-AD.
audio chunk on AudioVault starting from t to t , we find
1 2
Additionally,tomimichavingacharacterbankasexternal
the corresponding timestamps on CMD audio which has a
knowledgeasin[21,36,69],wealsoprovideeachinstructional
maximumcorrelation:
videowithapseudo-characterbankthatincludes: thechosen
(cid:110) (cid:16) (cid:17)(cid:111)
y ,y =argmax cor MAV ,MCMD . (3) characternameandthecharacterportraitfaceextractedfrom
1 2
ti,ti+w
[t1,t2] [ti,ti+w]
theinstructionalvideo,andafewfaceexemplarssampledfrom
These matches can be thought of as points on a scatter plot othervideostomimicoff-screencharacters. Anoverviewof
from (t ,y ) to (t ,y ) for a series of small windows from thepipelinewithanexampleisshowninFigure3.
1 1 2 2
AudioVault,asshowninFigure2.Finally,weuseaRANSAC Because of the noisy nature of YouTube videos and the
algorithmtofitalinethroughthesematchpointsoverallclips abundanceofdataintheHowTo100Mdataset,wefilteroutless
to obtain the mapping in Equation (2). Based on the ratio preferablevideosbythequalityofsubjectdetectioninHow-
between common movie fps, we filter RANSAC output by ToCaption,thefrequencyofnamesinASR,andthequalityof
4Hi, today we’re gonna demonstrate ...
[video intro]
Crop a frame with
[10, 18] John is pouring
face:
>>> wine into a glass [10, 18] A man is
John Find subjects: pouring wine into a glass
[20, 25] John takes a >>> A man, He
sniff of the wine [20, 25] He takes a sniff
of the wine
Sample a random
name: Construct character bank Rewrite descriptions Descriptions from HowToCaption
>>> John HowTo-AD
Figure3. HowTo-ADdataset. WeconverttheLLMrewrittenvideodescriptions(fromHowToCaption)tofitmovieaudiodescriptionsby(i)
uniformlyreplacingthesubjectsindescriptionswitharandomlysampledname,i.e.John,and(2)constructingacharacterbankbyproviding
aframewiththeinstructorandtherandomlysampledname.Thevideosampleisfromhttps://youtu.be/aRbQb19v2JI.
characterportraitfaces;detailsareintheAppendix.Asshown thenfinetunedonCMD-AD-Train.WepretrainonHowTo-AD
inTable1,theHowTo-ADdatasetendsupwithasubsetof180k for1epochandfinetuneonCMD-AD-Trainfor2epochs.We
YouTubevideosfromtheoriginalHowTo100Mdataset–which findfinetuningbeyond2epochsleadstooverfitting. Weuse
isabout20%ofthefullHowTo100M–and3.4Mtransformed abatchsizeof8ADsamples,anAdamWoptimizer[37]with
ADsegmentswithtimestampsfromHowToCaptiondataset. 3×10−5learningrateandacosinedecayschedule. Forboth
thepretrainingandfinetuningstages,thetrainingpipelinefits
4.ModelArchitecture inasingleA40GPUwith48GBGPUmemory.Moretraining
detailsareprovidedintheAppendix.
With pixel data available, we propose two visual captioning
modelsbasedonBLIP2[32]andLlama2[60]formovieAD 5.EvaluationMethods
generation. Specifically, we propose two new architectures
WeproposetwonewmethodsforevaluatingmovieADgener-
calledMovie-BLIP2andMovie-Llama2. Bothofthemtake
ation:CRITICforidentifyingcorrectcharacters,andanLLM-
8videoframes,resizedat224×224pixelsasinputs,thenuse
basedADevaluationforassessingholisticsemanticsofAD.
EVA-CLIP[56]toextractdensevisualfeatures.Next,weuse
aQ-formertoattendtospatial-temporalfeaturegridstoextract
CRITIC (Co-Referencing In Text for Identifying
visualdescriptorsrepresentedby32vectors.Bothmodelsalso
Characters). TheCRITICmetricassessestheaccuracyof
processesimageinputsfromcharacterfaceexemplars.Inthis
character naming in predicted AD against human-generated
case,theytakeasingleimageresizedat224×224pixels,and
reference AD. The metric is designed to be robust to (i)
thenusethesameEVA-CLIPtoextractvisualfeaturesinspatial
co-referencing complexities (ii) pronoun usage, and (iii)
grid, and the same Q-former to attend to this spatial feature
orthographic variation in character names. The objective is
gridandextract32vectorsasimagedescriptors.Thevideoand
tomeasurethequalityofcharacterreferenceinthegenerated
imagedescriptorsarepassedtotwoshallowprojectionheads
ADcomparedtotheground-truthAD.Forexample,themodel
respectively,toprojectthemonthelanguageembeddingspace.
mightgenerateADwiththetext‘Jack’orpronounslike‘he’,the
Finally, the projected visual outputs together with language
CRITICmetricaimstoevaluatetheaccuracyofthesereferences.
promptsarepassedtoalargelanguagemodel(OPTforMovie-
To achieve this, a co-referencing model4 is applied to
BLIP2andLlama2forMovie-Llama2)togeneratemovieAD
both predicted and reference AD passages. Specifically, let
intextform.AnoverviewofarchitectureisshowninFigure4.
C=“c ,c ,...,c .”denotethesetofofficialcharacternames
The Movie-BLIP2 architecture inherits from the original 1 2 n
fromthecastlistofamovie,combinedintoasinglesentence.
Image-basedBLIP2architecture,anditusesOPT[78]asthe
Foraspecificmovie,wegroupthepredictedandreferenceau-
languagemodel.TheMovie-Llama2architectureinheritsfrom
diodescriptionsintolongparagraphs,denotedasAD and
the image-based MiniGPT-4 [84] which connects BLIP2’s pred
AD respectively.Inordertoguidetheco-referencingmodel
visualembeddingwithLlama2languageembedding[60].Our ref
todetectcharacternames,bothAD andAD areprefixed
Movie-Llama2followsthesamesetupandusesLlama2asthe pred ref
withthecharacterlistsentenceC,asshowninFigure5(aandc).
languagemodel. Wetakepre-trainedcheckpointsfromopen-
Next,theco-referencingmodelisappliedtobothparagraphs
sourcedprojects[72]and[77]. Detailsofthesearchitectures
frompredictionandreference,yieldingsetsofidentitiesE
areintheAppendix.Followingpreviousworks[84],bydefault, pred
andE . EachidentityE includesreferencesandpronouns
allthevisualbackbone,languagemodel,andtheQ-formerare ref
linkedtoasingleentity.Weonlykeepidentitiescontainingex-
frozen,andweonlytraintheprojectionheads.
actlyonecharacternamefromC,ensuringdistinctassociation
Training details. By default, we adopt a two-stage training
strategy. ThemodelisfirstlypretrainedonHowTo-ADand 4https://github.com/shon-otmazgin/fastcoref
5❄ ViT ❄ ViT
❄ ViT
Q-Former Q-Former
Q-Former Prompt: Possible characters:
Jack Foley <Image> </Image>;
video-proj Karen Sisco <Image> </Image>. img-proj img-proj
<Video> </Video>
Please provide a detailed description of this movie clip.
❄ OPT / Llama2 Target: As Karen stares gloomily out of the window,
Jack approaches toying with a lighter.
Figure4.Architectureoverview.OurmodeltakesasinputmovieframesandmoviecharacterbankfromIMDbincludingfaceexemplarsand
characternames,andproducescharacter-awareaudiodescriptions. Theinputimages/videosarefirstfedtoafrozenvisualfeatureextractorto
obtainspatialorspatial-temporalvisualfeatures.ThenitusesasharedQ-formertoprocessthevisualinformationandprojectthemtothelanguage
embeddingspace,toleveragefrozenlargelanguagemodels(LLM)likeOPTandLlama2fortextgeneration.
whetherthenameiscorrect. TheCRITICscorehasarange
Jack Dawson, Rose Dewitt Bukater, Cal
Characters Co-referencing Identities for each AD:
Hockley, ... and Thomas Andrews.\n between0and1,with1beingperfect.
... - [Rose Dewitt Bukater, Rose]; [Jack
AD > Doors are open for her, as she meets Jack on Dawson, Jack, Mr. Dawson] LLM-AD-eval. WealsoadopttheLLMasajudge[11,81]
paragraph the on the grand staircase, next to the clock.\n - [Rose Dewitt Bukater, Rose]; [Jack
> He extends his hand to her and she takes it.\n Dawson, Jack, Mr. Dawson] procedure for AD quality assessment. Following previous
Reference (a) (b) works [39], we use a ‘gpt-3.5-turbo’ API from OpenAI and
Prediction Jack Dawson, Rose Dewitt Bukater, Cal 0.5 CRITIC score an open-sourced ‘llama-2-7b-chat’ model [60]. prompting
Characters Hockley, ... and Thomas Andrews.\n 0.5 the model to assess the matching quality between a pair of
... predicted AD and ground-truth AD by a score from 1 to 5,
> At the top of the stairs, Jack wearing his - [Jack Dawson, Jack, Mr. Dawson]
AD trousers held up by suspenders, stands and stares where5indicatesthebestmatchingand1indicatestheworst
paragraph at the wall clock.\n matching. To be complementary to the CRITIC metric, for
- [Jack Dawson, Jack, Mr. Dawson]
> He turns and smiles.\n
(c) (d) LLM-AD-evalweinstructtheLLMto(1)considerpronouns
Figure 5. Illustration of the CRITIC metric. The paragraphs as valid matches and ignore character names, and (2) focus
consistingcharacterlistandAD(a,c)arefedintoaco-referencing onhumanactions, objectsandinteractions. Thecustomized
model to get co-referencing identities (b,d). The CRITIC metric promptsareprovidedintheAppendix.
computes an IoU between the identities in the prediction vs. the
identitiesfromthereference. 6.Experiments
withindividualcharacters. Importantly,weremovepronouns
We outline the datasets (Sec. 6.1) and evaluation measures
like‘he’, ‘she’, and‘they’ineachco-referencingidentityto
(Sec. 6.2) employed in our experiments, and provide an
exclude ambiguous pronoun matching. Next, we map each
analysis on inter-rater agreement between AD annotation
sentencetoitscorrespondingsetofco-referencingidentities,
versions(Sec.6.3).Wereportquantitativeresults,ablatingthe
whichmaybeempty(whennonamesarerecognized),singular,
architectural design and the effect of HowToAD pretraining
ormultiple,asdepictedinFigure5(bandd).
(Sec.6.4),followedbyqualitativeresultsthankstoourpixel
TheCRITICmetricM isthencalculatedasanIoU,
CRITIC moviedata(Sec.6.5).
fori-thADreference(withvalidcharacteridentities):
6.1.Datasets
|E ∩E |
M CRITIC= |Epred ∪Eref
|
(4) AudioVault-8k is the dataset collected from https:
pred ref
//audiovault.net/by[21]thatcontainsfull-movieAD
where|E ∩E |isthecountofmatchingidentitiesbetween andsubtitlestranscribedfromuser-uploadedaudiodescription
pred ref
predictedandreferenceADs,and|E ∪E |isthetotalcount files covering 7800 movies. CMD (Condensed Movie
pred ref
of unique identities in both the prediction and the reference. Dataset) [4] contains movie clips collected from YouTube
The CRITIC score is averaged across all the AD references. for more than 3k movies. On average each movie has 10
Intuitively,ifthepredictedADincludesaname,theCRITIC non-contiguous clips and each clip spans for a few minutes.
scoreverifieswhetherthenamereferstothecorrectidentity; HowTo100M [40] contains 1M YouTube long videos with
if the prediction includes a pronoun like ‘he’, the CRITIC morethan100MASRsegments.Itistypicallyusedforvideo
scorefirstresolvestheidentitybyco-referencing,thenverifies pretraining.CMD-ADisthenewmovieADdatasetintroduced
6AudioVault #10435 AudioVault #16387 Method V-model L-model CIDEr R@1/5 CRITIC LLM-AD-eval†
[00:42:07.287, 00:42:09.369] [00:42:07.126, 00:42:09.428] AutoAD-II CLIP-B-32 GPT2 13.5 26.1 8.2 1.53/2.08
Audience members look at a All eyes turn to a red-haired boy Movie-BLIP2 Eva-CLIP OPT-2.7B 21.2 29.3 24.5 2.13/2.66
Movie-Llama2 Eva-CLIP LLama2-7B 21.7 30.0 25.2 2.05/2.85
boy in the crowd. in the audience.
Table4.ArchitectureexperimentsonCMD-AD-Eval.Wecompare
[01:02:05.836, 01:02:09.097] [01:02:05.955, 01:02:09.958]
the proposed two architectures with AutoAD-II. All of them take
Jamie grabs Lipton, hurls him to Jamie shoves the detective to the
the floor, and runs outside. ground and runs out of the house. character bank inputs. †: from ‘gpt-3.5-turbo’ / ‘llama-2-7b-chat’
respectively.Note,AutoAD-IIistrainedwithaveragedframefeatures
[01:06:17.747, 01:06:21.069] [01:06:17.670, 01:06:21.073]
tomimicitsoriginalsettingonthefeature-onlyMADdataset.
Lipton holds the lantern up, his As the detective watches, he
brow pinched in a frown. raises the lantern at arm's length.
Method pretrain CIDEr R@1/5 CRITIC LLM-AD-eval†
Figure 6. An example of inter-rater evaluation. Some movies Movie-BLIP2 ✗ 21.2 29.3 24.5 2.13/2.66
on AudioVault have multiple available ADs. Two versions of Movie-BLIP2 HowTo-AD 22.3 29.8 30.2 2.25/2.78
human-annotated ADs for the same visual scene are shown here. Movie-Llama2 ✗ 21.7 30.0 25.2 2.05/2.85
TheseADsarefilteredwithatIoUthresholdof0.9, andfromthe Movie-Llama2 HowToCaption‡ 20.8 29.4 25.6 2.07/2.85
movie‘DeadSilence’(tt0455760). Movie-Llama2 HowTo-AD 25.0 31.2 32.7 2.29/2.92
Table 5. Effect of HowTo-AD pretraining on CMD-AD-Eval.
tIoU #movies #ADpairs CIDEr R@1/5 CRITIC LLM-AD-eval† †:from‘gpt-3.5-turbo’/‘llama-2-7b-chat’,respectively. ‡usesthe
0.8 315 4447 61.5 71.2 42.0 2.56/3.04 same 180k-video subset as HowTo-AD, but without constructing
0.9 267 999 69.8 80.4 47.6 3.06/3.53 characterbanksorrewritingcaptions.
Table3. Inter-rateragreementonAudioVaultADannotations.
ADfromdifferentannotatorsdonotusuallysynchronize. Ahigher –CIDEr,Recall@1/5,CRITIC,andLLM-AD-eval.Note,these
tIoUthresholdfiltersoutfewerADpairsbuttheyaremorelikelyto evaluationsarecarriedoutdirectlyonthetextversionofthe
describetheexactsamevisualevent. †:LLM-AD-Evalscoresare AD,sonopixelsareinvolved.
computedfrom‘gpt-3.5-turbo’/‘llama-2-7b-chat’respectively. For In the AudioVault-8K dataset, there are 402 movies with
allthemetrics,ahighernumberindicatesbetterquality. R@1/5and more than one version of AD annotations. We conduct
CRITICareupperboundedat100andLLM-AD-evalisbetween1to5. inter-raterexperimentsonthissubsetofADannotations.Three
challengesemergewhenconductinginter-ratercomparisonsfor
in Section3.1, by aligning ADdata withCMD clips [4]. It
thesamevisualscene:(i)DifferentversionsofADannotations
contains101kADsformorethan1432movies.Wesplita100-
mightcorrespondtodifferentversionsofthesamemoviewhich
movieevaluationsetnamedCMD-AD-Evalandusetherestfor
donotnaturallysynchronizeasintroducedinSection3.1.We
training.HowTo-ADisthenewADdatasetintroducedinSec-
applytheaudio-audioalignmentpipelinein3.1tosynchronize
tion3.2,transformedfromHowTo100M[40].Itcontains180k
both AD annotations. (ii) The timing of providing AD is
YouTube videos with augmented descriptions and character
subjective and arbitrary within a short time interval [21], to
exemplars.WemainlyuseitforADgenerationpertaining.
obtaindifferentADsfortheexactsamevisualmoment,wehave
6.2.EvaluationMeasures tofilterthetimesegmentsoftwoADversionswithatemporal
Intersection-over-Union(tIoU).(iii)Forabout20%ofmovies,
In addition to the two new evaluation measures introduced
themultipleADversionsfromAudioVaultaresimplynarrating
inSection5,CRITICandLLM-AD-Eval,wealsomonitor
thesamescriptsagainwithminormodifications,whichdoes
Recall@k/N, CIDEr, and perplexity. Recall@k/N [21] is a
notreflectindependentinter-ratercomparisons. Wefilterout
retrievalmetricthatdistinguishesthepredictedtextamongaset
thosemoviesbycheckingtheexactsentence-matchingrate.
oftemporalneighbours.TheparameterskandN meanwithin
Theinter-raterevaluationisshowninTable3.Withahigher
atemporalwindowofN neighbouringreferenceADs,whether
tIoU threshold, we get fewer AD annotation pairs covering
the predicted AD can retrieve the corresponding reference
fewermovies,buttheADannotationpairsaremorelikelyto
ADattop-kposition.WeuseRecall@1/5onCMD-AD-Eval
describethesamemoviescene.Thenumberscanberegarded
and Recall@5/16 on MAD-Eval to compare with previous
ashuman-levelupperbound.NotethatwithalowertIoU(from
works. Weusetheofficialimplementationprovidedby[21].
0.9to0.8),allthemetricsdropsignificantly,highlightingthe
CIDEr[61]isapopulartextsimilaritymetricthatisbasedon
temporalsensitivityofADtasksandtheimportanceofprecise
wordmatchingrate.WeincludeRecall@k/NandCIDErhere
data alignment. A few pairs of human-annotated AD from
astheyhavebeenusedinrecentworkonAD[20,21]andwe
AudioVault are shown in Figure 6, where the left and right
alsocompareonthetestdatasetsofthoseworks.
panelsarefromtwoannotatorsforthesamevisualscene.
6.3.Inter-raterEvaluations 6.4.QuantitativeResults
ManyofthefilmsinAudiovaulthavemultipleADsavailable. ArchitectureComparisonsonAligned-CMD. InTable4,
Typically these are UK and US versions. In this section, wecomparetheproposedMovie-BLIP2andMovie-Llama2
we use the agreement between the human-provided AD architectureswithpreviousmethodsontheCMD-ADdataset.
versionstoassesstheusefulnessofthefourevaluationmetrics AllofthesemodelsaretrainedontheCMD-AD-Trainsetand
7Ground Truth: In the hall, Vientha overhears and looks up. Ground Truth: She starts to walk along the beam.
Prediction: Vayentha looks up at the ceiling. Prediction: She steps onto a ladder.
Ground Truth: A tear falls down the princess's cheek. Ground Truth: Other mounted officers watch Bourne steal the bike.
Prediction: She looks at him with tears in her eyes. Prediction: Bourne jumps off the motorcycle and runs.
Figure7. Qualitativeresults.AutoAD-IIIpredictionscorrectlyidentifythesemanticsofthescene,byreferringtothecharacters(‘Vayentha’,
‘Bourne’),theirrelations(‘looksathim’),actions(‘stepsonto’,‘jumpsoff’),emotions(‘tears’),objects(‘ladder’,‘motorcycle’).Thecomparison
withthegroundtruthfurtherhighlightsthelimitationsofthen-grambasedmetricssincethesamemeaningcanbeconveyedwithdifferentwordings.
CMD-AD-Eval MAD-Eval in this work, and MAD-Eval proposed in [20] (Table 6).
Method
CIDEr R@1/5 CRITIC LLM-AD-Eval† CIDEr R@5/16
Note that MAD-Eval is a 10-movie subset from LSMDC,
Video-BLIP2[72](noft) 4.8 22.0 0.0 1.40/1.89 05.0* 35.2*
wherewecangetshortmovieclipsforevaluation. However,
Video-Llama2[77](noft) 5.2 23.6 0.0 1.43/1.91 04.80 33.8*
we can not perform any training on MAD-Train since no
MM-Narrator-GPT4[76] - - - - 13.9* -
AutoAD-I[20] - - - - 14.3* 42.1* pixelsareavailable. TheproposedmethodMovie-BLIP2and
AutoAD-II[21] 13.5 26.1 8.2 1.53/2.08 19.2* 51.3*
Movie-Llama2performmuchbetterthanthepreviousmethods
Movie-BLIP2(ours) 22.3 29.8 30.2 2.25/2.78 22.8* 52.0*
Movie-Llama2(ours) 25.0 31.2 32.7 2.29/2.92 24.0* 52.8* including MM-Narrator with GPT4 as the language model.
WealsoevaluatevideocaptioningmodelslikeVideo-BLIP2
Table 6. Comparison with other methods on CMD-AD-Eval
and MAD-Eval. Additionally, we evaluate the out-of-the-box andVideo-Llama2butneitherofthemperformswellonAD,
Video-BLIP2andVideo-Llama2videocaptioningmodels(without highlightingthechallengesofMovieADtask.
anyADfinetuning)directlyonbothdatasets.†:from‘gpt-3.5-turbo’
6.5.QualitativeAnalysis
/ ‘llama-2-7b-chat’ respectively. *: these results are obtained on
MAD-EvalwithoutanytrainingonMAD-Train.
Figure 7 illustrates several random examples from the
CMD-AD-Evalset.Foreachsample,wedisplaythepredictions
evaluatedontheCMD-AD-Evalset.Weimplementandtrain
ofourMovie-Llama2model,aswellasthegroundtruthAD.
AutoAD-IIonCMD-AD-Trainbasedonthepubliccodebase.
Weobservethat,whiledifferentwordingthanthegroundtruth,
TheresultsshowthatbothMovie-BLIP2andMovie-Llama2
thesemanticsoftheADcontentremainlargelysimilarforOur
performmuchbetterthanAutoAD-IIarchitecture.Thestronger
method.Interestingly,inthefirstexample,theASRpipeline[5]
performanceisattributedtomultiplefactors–strongervisual
transcribed the name incorrectly as ‘Vientha’ but our model
backbone, stronger language model, and taking visual grid
fixedthenamethroughthecharacterbank.Moreexamplescan
feature as input instead of a single vector as in AutoAD-II.
befoundintheAppendix.
Movie-Llama2 has a much stronger language model than
Movie-BLIP2 (Llama2-7B vs OPT-2.7B), but it achieves a 7.Conclusion
similar performance wrt Movie-BLIP2. Note that all these
modelsarenotpretrainedonHowTo-ADyet. ThisworkadvancesautomaticADgenerationformoviesby:
(i)collectingADforpixeldatathroughaudio-audioalignment
EffectofHowTo-ADPretraining. TakingtheMovie-BLIP2 betweenfullmovies(withoutpixels)andpublicmoviesnippets,
and Movie-Llama2 settings from Table 4, we compare the andpseudo-labellinginstructionvideos;(ii)showingthatrecent
effectofHowTo-ADbypretrainingthesamearchitectureonthe video-languagearchitecturesprovideasignificantperformance
HowTo-ADdatasetthenfinetuningonCMD-AD-Trainset.We boost,bringingADgenerationsystemsclosertoreal-worldap-
alsopretrainwiththesamesubsetfromHowToCaptionwithout plications;and(iii)proposingnewevaluationmethodstailored
usingthecharacterbankorrewritingcaptions. Theresultsin for AD. One of the limitations that necessitates future work
Table5showthatlarge-scalepretrainingonourHowTo-AD isthecoherenceacrossADnarrationsthroughoutthemovie:
datasetsubstantiallybooststheperformanceonallfourmetrics ADshouldnotrepeatthesameinformation,orprovidestory-
forbothmodels.e.g.improvingCRITICfrom25.2to32.7and irrelevantdetails.Tothisend,externalknowledgesuchasplot
CIDErfrom21.7to25.0forMovie-Llama2.Butpretrainingon summariesmaybeutilizedtoincorporatestory-centricelements.
HowToCaptiondoesnothelpmuchonthefinetunedmovieAD Futuredirectionscouldalsoexploretheharmonybetweenthe
task,possiblybecauseofthedomaingapfromthedataandtask. narrationtoneandthemoviecontentforanengagingexperience.
Comparison with Other Methods. We compare with Acknowledgements.ThisresearchisfundedbyEPSRCPG
other methods on two datasets: CMD-AD-Eval introduced VisualAIEP/T028572/1,andANR-21-CE23-0003-01CorVis.
8References EVA: Exploring the limits of masked visual representation
learningatscale.InCVPR,2023.2
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
[17] M.A.FischlerandR.C.Bolles.Randomsampleconsensus:A
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
paradigmformodelfittingwithapplicationstoimageanalysisand
KatieMillican,MalcolmReynolds,etal. Flamingo: avisual
automatedcartography. Comm.ACM,24(6):381–395,1981.4
languagemodelforfew-shotlearning.InNeurIPS,2022.1,2
[18] RohitGirdhar,AlaaeldinEl-Nouby,ZhuangLiu,MannatSingh,
[2] PeterAnderson,BasuraFernando,MarkJohnson,andStephen
KalyanVasudevAlwala,ArmandJoulin,andIshanMisra.Image-
Gould.Spice:Semanticpropositionalimagecaptionevaluation.
Bind:Oneembeddingspacetobindthemall.InCVPR,2023.2
InProc.ECCV,pages382–398.Springer,2016.3
[19] Tengda Han, Weidi Xie, and Andrew Zisserman. Temporal
[3] AnuragArnab,MostafaDehghani,GeorgHeigold,ChenSun,
alignmentnetworksforlong-termvideo.InProc.CVPR,2022.4
Mario Lucˇic´, and Cordelia Schmid. ViViT: A Video Vision
[20] TengdaHan,MaxBain,ArshaNagrani,Gu¨lVarol,WeidiXie,
Transformer.InICCV,2021.2
andAndrewZisserman.AutoAD:Moviedescriptionincontext.
[4] Max Bain, Arsha Nagrani, Andrew Brown, and Andrew
InProc.CVPR,2023.1,2,7,8
Zisserman. Condensed movies: Story based retrieval with
[21] TengdaHan,MaxBain,ArshaNagrani,Gu¨lVarol,WeidiXie,
contextualembeddings.InProc.ACCV,2020.1,2,3,6,7
andAndrewZisserman. AutoADII:Thesequel–who,when,
[5] MaxBain,JaesungHuh,TengdaHan,andAndrewZisserman.
andwhatinmovieaudiodescription. InProc.ICCV,2023. 1,
Whisperx: Time-accurate speech transcription of long-form
2,4,6,7,8,14
audio.InINTERSPEECH,2023.3,8
[22] JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras,
[6] Se´bastien Bubeck, Varun Chandrasekaran, Ronen Eldan,
andYejinChoi.CLIPScore:Areference-freeevaluationmetric
JohannesGehrke,EricHorvitz,EceKamar,PeterLee,YinTat
forimagecaptioning.InEMNLP,2021.3
Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial
general intelligence: Early experiments with gpt-4. arXiv [23] GabrielHuang,BoPang,ZhenhaiZhu,ClaraRivera,andRadu
preprintarXiv:2303.12712,2023.3 Soricut. Multimodal pretraining for dense video captioning.
arXivpreprintarXiv:2011.11760,2020.2
[7] AmanChadha,GurneetArora,andNavpreetKaloty.iPerceive:
Applyingcommon-sensereasoningtomulti-modaldensevideo [24] QingqiuHuang,YuXiong,AnyiRao,JiazeWang,andDahua
captioningandvideoquestionanswering.InProc.WACV,2021.2 Lin. MovieNet:Aholisticdatasetformovieunderstanding. In
ECCV,2020.2
[8] DavidChan,SuzannePetryk,JosephEGonzalez,TrevorDarrell,
andJohnCanny.CLAIR:Evaluatingimagecaptionswithlarge [25] VladimirIashinandEsaRahtu.Abetteruseofaudio-visualcues:
languagemodels. arXivpreprintarXiv:2310.12971,2023.2,3 Densevideocaptioningwithbi-modaltransformer. InBMVC,
2020.2
[9] ShaoxiangChenandYu-GangJiang. Towardsbridgingevent
captionerandsentencelocalizerforweaklysuperviseddense [26] VladimirIashinandEsaRahtu. Multi-modaldensevideocap-
eventcaptioning.InProc.CVPR,2021.2 tioning.InCVPRWorkshopsonMultimodalLearning,2020.2
[10] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, [27] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,
SoravitChangpinyo,JialinWu,CarlosRiquelmeRuiz,Sebastian Andrew Zisserman, and Joao Carreira. Perceiver: General
Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up perceptionwithiterativeattention. InInternationalconference
a multilingual vision and language model. arXiv preprint onmachinelearning,pages4651–4664.PMLR,2021.2
arXiv:2305.18565,2023.3 [28] MingJiang,QiuyuanHuang,LeiZhang,XinWang,Pengchuan
[11] Cheng-Han Chiang and Hung-yi Lee. Can large language Zhang, Zhe Gan, Jana Diesner, and Jianfeng Gao. Tiger:
modelsbeanalternativetohumanevaluations? arXivpreprint Text-to-imagegroundingforimagecaptionevaluation. arXiv
arXiv:2305.01937,2023.2,3,6 preprintarXiv:1909.02050,2019.3
[12] AakankshaChowdhery,SharanNarang,JacobDevlin,Maarten [29] HassanKane,MuhammedYusufKocyigit,AliAbdalla,Pelkins
Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Ajanoh, and Mohamed Coulibali. Nubia: Neural based
HyungWonChung,CharlesSutton,SebastianGehrmann,etal. interchangeabilityassessorfortextgeneration. arXivpreprint
Palm:Scalinglanguagemodelingwithpathways. arXivpreprint arXiv:2004.14667,2020.3
arXiv:2204.02311,2022.2 [30] RanjayKrishna,KenjiHata,FredericRen,LiFei-Fei,andJuan
[13] Yin Cui, Guandao Yang, Andreas Veit, Xun Huang, and CarlosNiebles. Dense-captioningeventsinvideos. InProc.
Serge Belongie. Learning to evaluate image captioning. In ICCV,pages706–715,2017.2
Proceedings of the IEEE conference on computer vision and [31] JunnanLi,DongxuLi,CaimingXiong,andStevenC.H.Hoi.
patternrecognition,pages5804–5812,2018.3 BLIP: bootstrapping language-image pre-training for unified
[14] ChaoruiDeng,ShizheChen,DaChen,YuanHe,andQiWu. vision-languageunderstandingandgeneration. InICML,2022.
Sketch,ground,andrefine: Top-downdensevideocaptioning. 1,2
InCVPR,2021.2 [32] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.BLIP-2:
[15] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,Dirk Bootstrappinglanguage-imagepre-trainingwithfrozenimage
Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa encodersandlargelanguagemodels. arXiv:2301.12597,2023.
Dehghani, MatthiasMinderer, GeorgHeigold, SylvainGelly, 2,3,5
etal.Animageisworth16x16words:Transformersforimage [33] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng
recognitionatscale. arXivpreprintarXiv:2010.11929,2020.2 Liu, Ce Liu, and Lijuan Wang. LAVENDER: Unifying
[16] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, video-languageunderstandingasmaskedlanguagemodeling.In
XinggangWang,TiejunHuang,XinlongWang,andYueCao. CVPR,2023.1
9[34] YehaoLi,TingYao,YingweiPan,HongyangChao,andTao [51] Botian Shi, Lei Ji, Yaobo Liang, Nan Duan, Peng Chen,
Mei. Jointlylocalizinganddescribingeventsfordensevideo ZhendongNiu,andMingZhou.Denseprocedurecaptioningin
captioning.InProc.CVPR,2018.2 narratedinstructionalvideos.InAssociationforComputational
[35] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Linguistics,2019.2
Gan,ZichengLiu,YumaoLu,andLijuanWang. SwinBERT: [52] Yaya Shi, Xu Yang, Haiyang Xu, Chunfeng Yuan, Bing Li,
End-to-end transformers with sparse attention for video WeimingHu,andZheng-JunZha. Emscore:Evaluatingvideo
captioning.InProc.CVPR,2022.2 captioning via coarse-grained and fine-grained embedding
[36] KevinLin,FaisalAhmed,LinjieLi,Chung-ChingLin,Ehsan matching.InProc.CVPR,pages17929–17938,2022.3
Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, [53] Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian
ZichengLiu,YumaoLu,CeLiu,andLijuanWang. Mm-vid: Rupprecht,BerntSchiele,andHildeKuehne. HowToCaption:
Advancing video understanding with gpt-4v(ision). arXiv Prompting LLMs to transform video annotations at scale.
preprintarXiv:2310.19773,2023.4 arXiv:2310.04900,2023.1,2,4
[37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay [54] Mattia Soldan, Alejandro Pardo, Juan Leo´n Alca´zar, Fabian
regularization. arXivpreprintarXiv:1711.05101,2017.5 Caba,ChenZhao,SilvioGiancola,andBernardGhanem.MAD:
[38] HuaishaoLuo,LeiJi,BotianShi,HaoyangHuang,NanDuan, Ascalabledatasetforlanguagegroundinginvideosfrommovie
TianruiLi,XilinChen,andMingZhou. UniViLM:Aunified audiodescriptions.InProc.CVPR,2022.1,2
videoandlanguagepre-trainingmodelformultimodalunderstand- [55] EnxinSong,WenhaoChai,GuanhongWang,YuchengZhang,
ingandgeneration. arXivpreprintarXiv:2002.06353,2020.2 Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu,
Jenq-Neng Hwang, and Gaoang Wang. MovieChat: From
[39] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and
dense token to sparse memory for long video understanding.
FahadShahbazKhan. Video-chatgpt: Towardsdetailedvideo
arXiv:2307.16449,2023.2,3
understanding via large vision and language models. arXiv
[56] QuanSun,YuxinFang,LedellWu,XinlongWang,andYueCao.
preprintarXiv:2306.05424,2023.2,3,6
Eva-clip:Improvedtrainingtechniquesforclipatscale. arXiv
[40] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
preprintarXiv:2303.15389,2023.5
MakarandTapaswi,IvanLaptev,andJosefSivic. Howto100m:
[57] Makarand Tapaswi, Martin Bauml, and Rainer Stiefelhagen.
Learningatext-videoembeddingbywatchinghundredmillion
Book2movie: Aligningvideosceneswithbookchapters. In
narratedvideoclips. InProc.ICCV,pages2630–2640,2019.
Proc.CVPR,pages1827–1835,2015.2
1,2,3,4,6,7
[58] MakarandTapaswi,YukunZhu,RainerStiefelhagen,Antonio
[41] RonMokady,AmirHertz,andAmitHBermano.ClipCap:CLIP
Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Un-
prefixforimagecaptioning. arXivpreprintarXiv:2111.09734,
derstandingstoriesinmoviesthroughquestion-answering. In
2021.2
Proceedings of the IEEE conference on computer vision and
[42] JonghwanMun,LinjieYang,ZhouRen,NingXu,andBohyung
patternrecognition,pages4631–4640,2016.2
Han.Streamlineddensevideocaptioning.InProc.CVPR,2019.2
[59] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,
[43] AlejandroPardo,FabianCabaHeilbron,JuanLeo´nAlca´zar,Ali
Marie-Anne Lachaux, Timothe´e Lacroix, Baptiste Rozie`re,
Thabet,andBernardGhanem. Moviecuts: Anewdatasetand
NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,
benchmarkforcuttyperecognition. InEuropeanConference
Armand Joulin, Edouard Grave, and Guillaume Lample.
onComputerVision,pages668–685.Springer,2022.2
LLaMA: Open and efficient foundation language models.
[44] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,
arXiv:2302.13971,2023.2
GabrielGoh,SandhiniAgarwal,GirishSastry,AmandaAskell,
[60] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,Amjad
Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya
Sutskever. Learning transferable visual models from natural
Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:
languagesupervision.InProc.ICML,2021.2
Openfoundationandfine-tunedchatmodels. arXivpreprint
[45] TanzilaRahman,BichengXu,andLeonidSigal. Watch,listen
arXiv:2307.09288,2023.2,5,6
andtell:Multi-modalweaklysuperviseddenseeventcaptioning.
[61] RamakrishnaVedantam,CLawrenceZitnick,andDeviParikh.
InProc.ICCV,2019.2
Cider:Consensus-basedimagedescriptionevaluation. InProc.
[46] AnnaRohrbach, MarcusRohrbach, NiketTandon, andBernt CVPR,pages4566–4575,2015.2,3,7
Schiele.Adatasetformoviedescription.InProc.CVPR,2015.2 [62] JingwenWang,WenhaoJiang,LinMa,WeiLiu,andYongXu.
[47] AnnaRohrbach,AtousaTorabi,MarcusRohrbach,NiketTandon, Bidirectionalattentivefusionwithcontextgatingfordensevideo
ChristopherPal,HugoLarochelle,AaronCourville,andBernt captioning.InProc.CVPR,2018.2
Schiele.Moviedescription. IJCV,123(1):94–120,2017.2 [63] JianfengWang,ZhengyuanYang,XiaoweiHu,LinjieLi,Kevin
[48] PaulHongsuckSeo,ArshaNagrani,AnuragArnab,andCordelia Lin,ZheGan,ZichengLiu,CeLiu,andLijuanWang. Git: A
Schmid.End-to-endgenerativepretrainingformultimodalvideo generative image-to-text transformer for vision and language.
captioning.InProc.CVPR,pages17959–17968,2022.2 arXivpreprintarXiv:2205.14100,2022.3
[49] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu [64] Teng Wang, Huicheng Zheng, Mingjing Yu, Qian Tian, and
Soricut. Conceptualcaptions: Acleaned,hypernymed,image HaifengHu.Event-centrichierarchicalrepresentationfordense
alt-textdatasetforautomaticimagecaptioning. InAssociation videocaptioning. IEEETransactionsonCircuitsandSystems
forComputationalLinguistics,2018.2 forVideoTechnology,2020.2
[50] ZhiqiangShen,JianguoLi,ZhouSu,MinjunLi,YurongChen, [65] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran
Yu-GangJiang,andXiangyangXue.Weaklysuperviseddense Cheng,andPingLuo.End-to-enddensevideocaptioningwith
videocaptioning.InProc.CVPR,2017.2 paralleldecoding.InProc.ICCV,2021.2,3
10[66] Yujia Wang, Wei Liang, Haikun Huang, Yongqi Zhang, [82] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards
Dingzeyu Li, and Lap-Fai Yu. Toward automatic audio automaticlearningofproceduresfromwebinstructionalvideos.
description generation for accessible videos. In Proceedings InAAAI,2018.2
ofthe2021CHIConferenceonHumanFactorsinComputing [83] LuoweiZhou,YingboZhou,JasonJCorso,RichardSocher,and
Systems,pages1–12,2021.1 CaimingXiong.End-to-enddensevideocaptioningwithmasked
[67] YuXiong,QingqiuHuang,LingfengGuo,HangZhou,Bolei transformer.InProc.CVPR,2018.2,3
Zhou, and Dahua Lin. A graph-based framework to bridge [84] DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamed
moviesandsynopses.InProc.ICCV,pages4592–4601,2019.2 Elhoseiny. MiniGPT-4: Enhancing vision-language under-
[68] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine standingwithadvancedlargelanguagemodels. arXivpreprint
Miech,JordiPont-Tuset,IvanLaptev,JosefSivic,andCordelia arXiv:2304.10592,2023.2,3,5
Schmid. Vid2seq: Large-scale pretraining of a visual lan-
guage model for dense video captioning. arXiv preprint
arXiv:2302.14115,2023.2,3
[69] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,
Chung-ChingLin,ZichengLiu,andLijuanWang. Thedawn
ofLMMs:Preliminaryexplorationswithgpt-4v(ision). arXiv
preprintarXiv:2309.17421,9,2023.4
[70] Yanzhi Yi, Hangyu Deng, and Jinglu Hu. Improving image
captioningevaluationbyconsideringinterreferencesvariance.
InProceedingsofthe58thAnnualMeetingoftheAssociation
forComputationalLinguistics,pages985–994,2020.3
[71] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,Mojtaba
Seyedhosseini,andYonghuiWu.CoCa:Contrastivecaptioners
areimage-textfoundationmodels. TransactionsonMachine
LearningResearch,2022.1,2
[72] KeunwooPeterYu.VideoBLIP,2023.2,3,5,8,13
[73] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal.
Self-chainedimage-languagemodelforvideolocalizationand
questionanswering.InNeurIPS,2023.3
[74] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,
JaeSungPark,JizeCao,AliFarhadi,andYejinChoi.MERLOT:
Multimodalneuralscriptknowledgemodels.InNeurIPS,2021.1
[75] RowanZellers,JiasenLu,XimingLu,YoungjaeYu,Yanpeng
Zhao,MohammadrezaSalehi,AdityaKusupati,JackHessel,Ali
Farhadi,andYejinChoi.MERLOTreserve:Multimodalneural
scriptknowledgethroughvisionandlanguageandsound. In
CVPR,2022.2
[76] Chaoyi Zhang, Kevin Lin, Zhengyuan Yang, Jianfeng Wang,
Linjie Li, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.
Mm-narrator: Narrating long-form videos with multimodal
in-contextlearning. arXivpreprintarXiv:2311.17435,2023.8
[77] Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA:
An instruction-tuned audio-visual language model for video
understanding.InEMNLP2023Demo,2023.2,3,5,8,13
[78] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
MoyaChen,ShuohuiChen,ChristopherDewan,MonaDiab,
XianLi,XiVictoriaLin,etal.Opt:Openpre-trainedtransformer
languagemodels. arXivpreprintarXiv:2205.01068,2022.2,5
[79] TianyiZhang,VarshaKishore,FelixWu,KilianQWeinberger,
andYoavArtzi. BERTScore: Evaluatingtextgenerationwith
bert.InProc.ICLR,2020.2,3
[80] YueZhao,IshanMisra,PhilippKra¨henbu¨hl,andRohitGirdhar.
Learningvideorepresentationsfromlargelanguagemodels.In
CVPR,2023.2
[81] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,
ZhanghaoWu,YonghaoZhuang,ZiLin,ZhuohanLi,Dacheng
Li,EricXing,etal. Judgingllm-as-a-judgewithmt-benchand
chatbotarena. arXivpreprintarXiv:2306.05685,2023.2,3,6
11AutoAD III: The Prequel – Back to the Pixels
Appendix
(a)
#10
#9
#8
#7
#6
#5
#4
#3
#2
#1
00:45:00 01:00:00 01:15:00 01:30:00 01:45:00 02:00:00
AudioVault timestamps of the full movie "The Philadelphia Story"
(b)
FigureA.1.(a)MoreexamplesofRANSACforaudio-audioalignment.Fivesamplesarechosenfromfivedifferentmovies.Thefoursamples
ontheleftaresuccessfulRANSACresultsthathavelowMSEandreasonableslopes.Thesampleontherightisfilteredoutsincethefittedresult
fromRANSAChasahighMSEandtheslopedoesmatchourcriteria. Notethattheinliers(bluedots)arenotcontinuousbecausethosetime
segmentscorrespondtoADaudioandhavebeenmaskedoutbeforethealignmentprocess.(b)Audio-audioalignmentresultsofafullmovie.
WelocalizetenCMDclipsfromthecorrespondingfull-movieAudioVaultaudiofilebyvisualizingtheiralignedtimestampsfromtheRANSAC
algorithm.WecanclearlyseethatCMDdatasetcontainsmultiplediscontinuousmovieclips,buttheprecisetimestampsofeachclipareobtained
fromouralignmentalgorithm,whicharenotprovidedbyCMD.Theexamplemovieis‘ThePhiladelphiaStory’(1940, tt0032904).
A.DatasetDetails usethe spacy packageinPythontodetectuniquenamesin
theYouTubeASR.Wediscardvideoswithmorethan5unique
A.1.CMD-AD–PixelsfromAlignedCMD
names,becausewefindthosevideostypicallycorrespondto
NewsorSportsprogrammesandarenotsuitabletobetrans-
Following the main paper Figure 2, a few more examples
formedtomovieADstyle. (ii)weusethe spacy package
of RANSAC output are shown in Figure A.1-(a). Note that
to detect the subject of each sentence in the HowToCaption
we filter valid RANSAC output by two thresholds: (1) the
slope0.8<W′<1.25,and(2)MSE<100. Additionally,we dataset.Wediscardthosevideoswhichdonotcontainasubject
inanyofthecaptions,e.g.thecaptionslike‘cutcarrots’,‘chunk
visualize the audio-audio alignment results of a full movie
onion’,etc.Becauseourcaptiontransformingstageinthemain
exampleinFigureA.1-(b).
paperSection3.2requiresthedetectedsubjects.(iii)weextract
Thescriptforconductingaudio-audioalignmentbetween
afewvideoframesattheintroductionofthevideo–whichis
CMD movie clip and AudioVault audio chunk is shown in
inferredfromthefirstfewASRtimestamps–thenweusea
Algorithm1.
facedetector5 toverifywhetherafaceexistsintheseframes.
Wediscardvideosthatdonothaveframeswithfacesbecause
A.2.HowTo-AD–PixelsfromHowTo100M
ourcharacterbankrequiresfaceexemplars.
TheoriginalHowTo100MandHowToCaptiondatasetshave Acombinationofthesefiltersreducesthenumberofvideos
more than 1.2M videos. We apply a sequence of filtering
processes to get high-quality videos for our purpose. (i) we 5https://github.com/ageitgey/face_recognition
12
xedni
pilc
DMCVfvFZ-Kdt6Q 80142
80k 44.5%
70k
60k
50k
Alex Alex demonstrates the use of a thumb pump for a magic trick 40k 23 06 .6 30 %9 13 95 .5 72 %3
30k
TPJYaKMuOgU
20k
8134
1 00 kk 4.5% 24 .9 74 %1 23 .9 21 %8 12 .5 42 %6 11 .9 14 %7 11 .8 06 %9 01 .6 94 %7 01 .5 95 %2 06 .47 %4 01 .14 %7 01 .13 %3 01 .12 %9 0.6 03
%
0.5 09
%
0.1 04
%
0.07
%
s_5A lGn Fg Ne wla
f6k
Angela points to a doll with tangle hair Food and En Ht oer bt ba ii en si an Hg n od mC e C r aaa rf n st ds & HG oOa lt ir h dd ee ar yn s V ae nhi dc le Ts r Pea Ptd s ei rti a so n on d n s alA n Ci arm ea Sls a pn ord tsS t ayl ne d EAF di rt t us cn e aa ts ins od n H E ae n ntal e dt
C
rth oCai o mn pm um tme eun rt sn i aca nt di o En ls ectronics Y Fo iu F nt a ah nm Pi cl he i y l a oL n sif d o e pB hu ys i an ne ds s Religi Wo orn k World Tr Ra elv ae tl ionships
Figure A.3. Category distribution of HowTo-AD dataset. Similar
totheoriginalHowTo100Mdataset, theHowTo-ADdatasethasa
long-taileddistributionoveractioncategories,dominatedby‘Food
Vicki
Vicki continues to water the plant until it starts draining through andEntertaining’,‘HobbiesandCrafts’and‘HomeandGarden’.
FI5aR3ode8A
architecturecontains39transformerblockswithaconsistent
hidden dimension of 1408 channels and 16 heads. (ii) The
Q-formerisfromBLIP-2,whichcontains12transformerblocks
Kay Kay stirs the soup and adds spinach
with32learnablequeriesandcross-attendsvisualfeatureswitha
FigureA.2.MoresamplesfromHowTo-ADdataset.Fourexamples
dimensionof(1+162)×768.Foreachimageinput,itproduces
areshowninthisfigurewiththe‘characterbank’shownontheleft featureswithadimensionof32×768.(iii)ThevideoQ-Former
and the re-written video caption with randomly sampled character isfromVideo-Llama,whichalsocontains12transformerblocks
namesshownontheright.TheYouTubeIDsaredisplayedontopof with32learnablequeries.Itcross-attendsvideofeatureswitha
thecharacterbanks. dimensionof8×32×768andproduces32×768videofeatures.
(iv)Theprojectorisalinearlayerthatprojects32×768visual
featuresinto32×4096dimensiontofitthelanguagemodel.
from 1.2M to 180k, from which we build the HowTo-AD Wehavetwoprojectorsforvideoandimageinputsrespectively.
datasetasdescribedinthemainpaperSection3.2. (v) The language model is Llama2-7B, which contains 32
In Figure A.2, we show a few more examples from transformerblockswith4096hiddendimensions.
HowTo-AD dataset. We notice a few limitations of the
By default, we only train the parameters in the video
HowTo-AD:(1)inevitably,thevideocategoriesaredominated
Q-Formerdescribedin(iii)andtheprojectordescribedin(iv).
by‘FoodandEntertaining’,inotherwords,cookingactivities.
TherestofthecomponentsareinitializedfromVideo-Llama
SincethedatasetoriginatesfromHowTo100Mwhere50%of
pretrained weights. We also experimented with a setting
thecategoriesbelongstocookingactivities.Abreakdownofcat-
that only trains the linear projector (iv), it produces a lower
egorystatisticsisshowninFigureA.3.(2)ourcurrentpipeline
performance(CIDErdropsby1.5).
thattransformsHowToCaptionintoHowTo-ADassumesthe
videoonlyhasasinglecharacter.However,it’scommonthatin
instructionalvideos,twoorthreeinstructorsappearinthescene. Movie-BLIP2 isbasedontheVideo-BLIP[72]repository7.
Thesingle-characterassumptionmayintroducenoisyADinthe With an input of 8 movie frames, the architecture is mostly
HowTo-ADdataset,andweconsiderfixingthisinfuturework. the same as Video-Llama, except for two differences. First,
thefunctionsoftheQ-formerandvideoQ-Formeraremerged.
B.ImplementationDetails InMovie-BLIP2,theQ-formercross-attendstoallthevisual
featuresatoncewithadimensionof8×(1+162)×1408,then
B.1.Architectures
producesvideofeatures32×768.ThereisnovideoQ-Formerin
Movie-BLIP2.Second,thelanguagemodelisOPT-2.7B,which
Movie-Llama2 isbasedonthevisualbranchoftheVideo-
Llama [77] repository6. The input to the model is 8 movie contains32transformerblockswith2560hiddendimensions.
frames uniformly sampled from a given AD time segment. Similarly, we train the parameters in the Q-former and
The model contains five components, as detailed below. (i) theprojector. Therestofthecomponentsareinitializedfrom
The visual encoder is from EVA-CLIP which takes input Movie-BLIPpretrainedweights.Butonlytrainingtheprojector
from224×224×3pixelsanduses14×14patchsize. The producesasimilarperformance.
6https://github.com/DAMO-NLP-SG/Video-LLaMA 7https://github.com/yukw777/VideoBLIP
13
soediV
fo
rebmuNAudioVault version 1 AudioVault version 2
●Donny turns to glare at the back of ●DonnyDonnie turns to glare at
his mother's chair. the back of his mother's chair.
●He paces the room. ●He paces the room.
●Donnie marches purposefully out ●Donnie marches purposefully
Ground Truth: Depicted in it are hundreds of soldiers wearing armour locked in fierce battle.
of the room. out of the room. Prediction: The painting depicts a battle scene.
●He picks up the receiver and looks at ●He picks up thea receiver and
his New York calling card. looks at his New York calling card.
●He sits up and pulls the grouping
●He pulls the grouping of four chairs of four chairs closer and props his
closer and props his feet on them. feet on thementire seat unit closer
Ground Truth: Sid places him on a flat rock.
●The officer checks the form and sets to himself. Prediction: The lion paws at Sid.
●The officer checksglances at the
it aside.
form and sets it aside.
FigureA.4. Specialcasesofinter-rateranalysis: Twoversionsof
ADfromAudioVaultmightnotbeproducedindependently.Inthese
Ground Truth: The sky above is pale blue with a few wisps of white clouds.
twoexamples,annotatorseditafewwordsfromtheearlierversion,
Prediction: They ride camels.
toproduceanewversionofAD.Wediscard thesesampleswhen
conductinginter-rateranalysissincetheseexamplesdonotreflectreal
human-humanagreementonAD.
AudioVault annotations Movie-Llama2 predictions Metrics per sentence
Ground Truth: Garfield's tossed out of the bucket then lands back in it as it hits the roof
1.Holmes follows her around a corner. 1.Holmes follows her. CRITIC LLM-AD-Eval and rolls to the edge. Prediction: Garfield lands on the roof.
2.Mary is in her late thirties with a mop of 2.The carriage pulls up outside (0-1) (0-5)
curly red hair under her black veil. a narrow alleyway. 1.0 5
3 4. .D Ha isz e vd is, ih oe n l ii sf t bs l uu rp r eh dis , bh ue ta d h. e sees a man 3.H hee a dli e ts w o isn t et dh e a tg aro nu on dd d, h ai ns g le. 0.0 0 FigureA.6.Additionalqualitativeresults.Foreachmovieclip,three
scoop Mary up off the road and drag her 4.He falls to the ground. 1.0 4 framesareuniformlyextractedandvisualized. Themovieclipsare
towards the carriage. 0.33 4 fromInferno(2016, tt3062096),IceAge(2002, tt0268380),
Sex and the City 2 (2010, tt1261945) and Garfield (2004,
1.Marty's speedometer tops at a stop sign, 1.Marty's car speeds down the
he turns hard right. road. C (0R -1IT )IC L (0L -5M )-AD-Eval tt0356634).Forqualitativeresultsinvideoformat,pleasereferto
2.M hea ar dt y a' ss tr ha ed i ca ati ro zn o h oe mlm s ie nt f thal el s b o arv ne r d h oi os r . 2.T bah re n c aa nr d s p oe ue t d ths eth or to hu eg r h s it dh ee . 1.0 2 the mp4 fileinoursupplementarymaterial.
3.George rides through the square on a 3.Marty rides his bike out of 0.0 2
bicycle, and Marty bursts out of the cafe the diner. 0.5 4
after him.
annotationsforeachmovieanddiscardthosemovieswithmore
FigureA.5. ExamplesofCRITICandLLM-AD-Evalmetrics. We than one identical annotations. It ends up filtering out 18%
showsevenADannotation-predictionpairsfromtwomovies.Forboth from402movies,andtheremainingtrustworthyindependent
CRITICandLLM-AD-Evalmetrics, highervaluesindicatehigher ADannotationsareusedforinter-rateranalysis.
quality.FordetailsofmetricspleaserefertothemainpaperSection5.
C.2.AnalysisoftheEvaluationMetrics
AfewexamplesofADannotationsand(bothgoodandbad)
B.2.LLM-AD-Eval
model predictions are shown in Figure A.5, we show both
We use the gpt-3.5-turbo model from OpenAI to CRITICandLLM-AD-Evalmetricsforeachpair.Qualitatively,
constructtheLLM-AD-Evalmetric. Ourcustomizedprompt wecanfeelthatLLM-AD-Evalcorrelatestohumanjudgement
isshowninAlgorithm2. TheLLMisinstructedtoevaluate of semantic matching between two sentences. The CRITIC
thelevelofmatchbetweentwosentencesandreturnascore metric gives zeros when the predictions do not contain the
between0to5. charactersintheannotation.
C.3.BetterCharacterExemplarsforCMD-AD
C.AdditionalResults
In AutoAD-II [21], the character exemplars are constructed
C.1.DetailsofInter-raterAnalysisonAudioVault
fromthein-contextnearestneighboursofactors’IMDbprofile
As described in the main paper Section 6.3, there are 402 pictures.However,wequalitativelyfindthismethoddoesnot
movieswithmultipleversionsofADannotations. However, work well for old movies where the IMDb profile pictures
wefoundinsomecasestwoversionsofADannotationsarenot areoflowquality,orcartoonmovieswheretheIMDbprofile
producedindependently,asshowninFigureA.4–annotators picturesarethevoiceactorsinsteadofthecharacters.
slightlymodifyanolderversionofADtoproduceanewversion, Weinvestigatedanadditionalsourceofcharacterexemplars,
thereforethesesamplesdonotreflecttherealhuman-human whicharefromthe‘photo’pagesonIMDb,likehttps://
agreementonADannotations.Todiscardthesesamplesinour www.imdb.com/title/tt2294629/characters/
inter-rateranalysis,weapplyafiltertodetecttheidenticalAD nm0068338 or https://www.imdb.com/title/
14
semloH
kcolrehS
erutuF
eht
ot
kcaBMethod Setting CIDEr CRITIC
Movie-Llama2 newexemplars 21.7 25.2
Movie-Llama2 oldexemplars 20.1 23.3
TableA.1. EffectofnewcharacterexemplarsonCMD-AD-Eval.
BothexperimentsareconductedwithoutHowTo-ADpretraining.
tt0046912/characters/nm0001537. These photos
arethemovieframesuploadedbyusers,correspondingtospe-
cificcharacters.However,thissourcehastwomajorissues:first,
thephotosarebiasedtowardsfamouscharacters,andless-known
charactersusuallydonothavephotosuploaded;second,photos
underonecharactermaycontainothercharacters.Wefindonly
50%charactershavevalidphotopages,andweusefacedetec-
tor8toselectphotoswithonlyoneface;Forthosewithoutphoto
pages,wefallbacktotheoriginalAutoAD-IIstrategytofind
in-contextexemplars. Themainpaperusesthisnewstrategy
forcharacterexemplarsforalltheexperiments.Weablatethe
effectofthisupdateonthecharacterexemplarinTableA.1.
D.AdditionalQualitativeResults
MorequalitativeresultsareshowninFigureA.6.Additionally,
ontheprojectpagehttps://www.robots.ox.ac.uk/
vgg/research/autoad/,wedisplayafewvisualization
ofourmodelpredictionsasMP4files. TheADground-truth
andourmodelpredictionsareprovidedassubtitleswiththe
format‘[title]ground-truth∥prediction’. ThevoiceofADis
generated from OpenAI text-to-speech API9 and fused with
theoriginalmoviesoundtrack.
8https://github.com/ageitgey/face_recognition
9https://platform.openai.com/docs/guides/text-to-
speech
15Algorithm1:Pythonscriptforaudio-audioalignment.
import torch
import torchaudio
import numpy as np
from sklearn.linear_model import RANSACRegressor
from sklearn.metrics import mean_squared_error
def aa_match(cmd_wav, audiovault_wav, audiovault_anno):
"""
cmd_wav: audio path of CMD
audiovault_wav: audio path of AudioVault chunk,
audiovault_anno: AD annotations (with timestamps) of the AudioVault chunk
Return: mse, slope and intercept of RANSAC
"""
resolution_per_mel = 512/16000
cmd_waveform, cmd_sr = torchaudio.load(cmd_wav)
cmd_melspec = mel_spectrogram(cmd_waveform)
cmd_melspec = cmd_melspec / cmd_melspec.norm(dim=-2, keepdim=True)
av_waveform, av_sr = torchaudio.load(audiovault_wav)
av_melspec = mel_spectrogram(av_waveform)
av_melspec = av_melspec / av_melspec.norm(dim=-2, keepdim=True)
# mask melspec as zeros if AD appears. Only compute non-AD part
av_melspec_masked = torch.clone(av_melspec)
start_end_mel_idx = []
for _, row in audiovault_anno.iterrows():
start_mel_idx = int(row[’start’] / resolution_per_mel) # second to mel idx
end_mel_idx = int(row[’end’] / resolution_per_mel)
av_melspec_masked[...,start_mel_idx:end_mel_idx] = 0
start_end_mel_idx.append([start_mel_idx, end_mel_idx])
window_size = 50 # equal to 3.2 second
# use 1D conv to compute correlation in parallel
# use unfold() to get chunks by moving windows
melspec_av_all_cuts = av_melspec_masked.unfold(dimension=-1, size=window_size, step=window_size)
melspec_unfold = cmd_melspec.unfold(dimension=-1, size=window_size, step=1)
conv_output = torch.einsum(’bctw,bcaw->bta’, melspec_unfold, melspec_av_all_cuts)
ad_mask = torch.any(melspec_av_all_cuts.mean(1)==0, dim=-1)
conv_output = conv_output[0, :, ˜ad_mask[0].bool()] # only keep non-AD chunks
max_cor, max_pos = conv_output.max(0)
plot_outputs = torch.stack(
[torch.arange(0, av_melspec_masked.shape[-1]-window_size+1, window_size)[˜ad_mask[0].bool()],
max_pos, max_cor/window_size], -1).numpy()
# prepare scatters for RANSAC
peak_cor = np.max([i[-1] for i in plot_outputs])
samples_X = (plot_outputs[:, 1:2] + np.linspace(0, window_size, 5)[None,:]).flatten()
samples_y = (plot_outputs[:, 0:1] + np.linspace(0, window_size, 5)[None,:]).flatten()
samples_weight = plot_outputs[:, 2:3].repeat(5, axis=1).flatten()
ransac = RANSACRegressor(random_state=0, residual_threshold=50).fit(
samples_X[:,None], samples_y[:,None], samples_weight)
inlier_mask = ransac.inlier_mask_
outlier_mask = np.logical_not(inlier_mask)
slope = ransac.estimator_.coef_[0,0]
intercept = ransac.estimator_.intercept_[0]
# get mse
y_pred = ransac.predict(samples_X[inlier_mask][:,None])
mse = mean_squared_error(samples_y[inlier_mask][:,None], y_pred)
print(’MSE=’, mse)
print(’slope=’, slope)
print(’intercept=’, intercept)
return mse, slope, intercept
16Algorithm2:PythonscriptforLLM-AD-Eval,withOpenAIgpt-3.5-turboAPI.
import ast
import openai
def eval_each(text_gt, text_pred, client):
# Compute the LLM-AD-Eval score
completion = client.chat.completions.create(
model="gpt-3.5-turbo",
messages=[
{
"role": "system",
"content":
"You are an intelligent chatbot designed
for evaluating the quality of generative outputs for movie audio descriptions. "
"Your task is to compare the predicted audio descriptions with the correct audio
descriptions and determine its level of match, considering mainly the visual elements
like actions, objects and interactions. Here’s how you can accomplish the task:"
"------"
"##INSTRUCTIONS: "
"- Check if the predicted audio description covers
the main visual events from the movie, especially focusing on the verbs and nouns.\n"
"- Evaluate whether the predicted audio
description includes specific details rather than just generic points. It should
provide comprehensive information that is tied to specific elements of the video.\n"
"- Consider synonyms or
paraphrases as valid matches. Consider pronouns like ’he’ or ’she’ as valid matches
with character names. Consider different character names as valid matches. \n"
"- Provide a single evaluation score that reflects the level of match of the
prediction, considering the visual elements like actions, objects and interactions."
},
{
"role": "user",
"content":
"Please evaluate the following movie audio description pair:\n\n"
f"Correct Audio Description: {text_gt}\n"
f"Predicted Audio Description: {text_pred}\n\n"
"Provide your evaluation only as a matching score where the matching score
is an integer value between 0 and 5, with 5 indicating the highest level of match. "
"Please generate the response in the form of a Python dictionary string
with keys ’score’, where its value is the matching score in INTEGER, not STRING."
"DO NOT PROVIDE
ANY OTHER OUTPUT TEXT OR EXPLANATION. Only provide the Python dictionary string. "
"For example, your response should look like this: {’score’: }."
}
]
)
# Convert response to a Python dictionary.
response_message = completion.choices[0].message.content
response_dict = ast.literal_eval(response_message)
return response_dict
client = OpenAI()
eval_each(text_gt, text_pred, client)
17