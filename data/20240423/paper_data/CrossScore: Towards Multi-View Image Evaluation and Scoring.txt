CrossScore: Towards Multi-View Image
Evaluation and Scoring
Zirui Wang1 Wenjing Bian1 Omkar Parkhi2 Yuheng Ren2
Victor Adrian Prisacariu1
1Univeristy of Oxford, {ryan, wenjing, victor}@robot.ox.ac.uk
2Meta Reality Lab, {omkar, carlren}@meta.com
https://crossscore.active.vision
Query
GT
FR: SSIM
GR: FID
NR: NIQE
“White horse
standing on dry
grass, outdoor.”
MMR: CLIPScore CR: CrossScore
Fig.1: We propose a novel cross-reference (CR) image quality assessment (IQA)
scheme, which evaluates a query image using multiple unregistered reference images
thatarecapturedfromdifferentviewpoints.Thisapproachsetsanewresearchtrajec-
toryapartfromconventionalIQAschemessuchasfull-reference(FR),general-reference
(GR), no-reference (NR), and multi-modal-reference (MMR).
Abstract. We introduce a novel cross-reference image quality assess-
mentmethodthateffectivelyfillsthegapintheimageassessmentland-
scape,complementingthearrayofestablishedevaluationschemes–rang-
ing from full-reference metrics like SSIM [56], no-reference metrics such
asNIQE[30],togeneral-reference metricsincludingFID[17],andMulti-
modal-reference metrics,e.g.CLIPScore[16].Utilisinganeuralnetwork
withthecross-attentionmechanismandauniquedatacollectionpipeline
from NVS optimisation, our method enables accurate image quality as-
sessment without requiring ground truth references. By comparing a
query image against multiple views of the same scene, our method ad-
dressesthelimitationsofexistingmetricsinnovelviewsynthesis(NVS)
and similar tasks where direct reference images are unavailable. Exper-
imental results show that our method is closely correlated to the full-
reference metric SSIM, while not requiring ground truth references.
Keywords: Cross-Reference Image Assessment · Novel View Synthesis
4202
rpA
22
]VC.sc[
1v90441.4042:viXra2 Z. Wang et al.
1 Introduction
Accurate image quality evaluation is critical for enhancing the performance of
computer vision tasks, including image processing, image generation, and novel
view synthesis (NVS).
Image quality assessment (IQA) methods can be categorised based on the
typeofreferencing.Themostpopulargroup,full-reference (FR)metric,suchas
PSNR, SSIM [56], LPIPS [67], evaluates the differences between a query image
and a reference image in terms of pixels and perceptual quality. These metrics
areessentialfortaskssuchassuper-resolution,denoising,andcompression,and,
importantly, assume the availability of an oracle reference image.
Ground truth images, however, may not always be available, for instance,
in image generation tasks. Consequently, multiple attempts have been made
to alleviate the dependency on ground truth oracles. For instance, FID [17] is
a general-reference (GR) scheme that evaluates the discrepancy of data dis-
tribution between image sets. Alternatively, multi-modal-reference (MMR) ap-
proaches, such as CLIPScore [16], examine the image-text similarity, whereas
no-reference (NR) metrics, such as NIQE [30] and PIQE [52] evaluate single-
image statistics without referencing.
Althoughtheaforementionednon-ground-truthmetricsarewidelyemployed
in tasks like image compression, denoising, and generation, these methods gen-
erally rely on high-level statistics and global context, consequently lacking the
capacityfordetailedanalysis.ThisdeficiencyrenderstheminadequateforNVS,
which requires pixel-level assessment of novel view images with scene-specific
context.
The established approach to assess NVS performance involves selecting a
subsetoftestimagesfromanexistingcameratrajectory,whichcannotbeusedin
training,rendering imagesusing theircameraparameters, andcomputingpixel-
level FR scores by comparing these rendered images with the original captured
test images. While generally simple and effective, this subsampling approach
exhibitstwoprimaryissues:1)balancingthenumberofimagesbetweentraining
and evaluation can affect the statistical relevance of the assessment and the
effectiveness of the training; and 2) relying on FR metrics precludes the ability
to evaluate renderings using true novel trajectories, as ground truth images are
not available for true novel views.
These challenges motivate us to develop a novel IQA scheme, which eval-
uates the quality of a query image using multiple reference views, each ob-
serving the same content but from different viewpoints. The key intuition is
toleveragemulti-viewimagesasasubstituteforagroundtruthimage,enabling
a ‘perspective’-version FR evaluation. We term this process as cross-reference
(CR)evaluationandtheresultingscoreasCrossScore.Ourmethoddiffersfrom
prior works in two essential ways: 1) unlike FR-IQA, our approach eliminates
the need for aligned reference images; and 2) in contrast to GR-, NR-, and
MMR-IQA, our method offers detailed evaluation via multi-view reasoning.
Specifically,weproposetofindacross-referencefunctionthatpredictsafull-
reference metric, i.e. SSIM, of a distorted image, by comparing it with multipleCrossScore: Towards Multi-View Image Evaluation and Scoring 3
unregisteredreferenceimages.Weimplementthisfunctionwithaneuralnetwork
that employs the cross-attention mechanism [51].
Alongsidethemodelformulation,akeyadditionalchallengeliesingathering
training samples. Our solution involves rendering images throughout the NVS
optimisation process, covering a broad spectrum of distortion varieties and in-
tensities.Bycomparingtheseimagesagainsttheiroriginal,undistortedversions,
we obtain pixel-level SSIM scores that serve as a training objective. This self-
superviseddatacollectionapproachallowsustobuildarichdatasetandenables
per-pixel supervision for our network training.
In summary, our contribution is threefold. First, we unveil CR-IQA, a new
image evaluation regime tailored for multi-view scenarios. Second, we actualise
this concept through a neural network based on cross-attention mechanisms,
enabling detailed per-pixel evaluation in the absence of ground truth images.
Third, we develop a self-supervised data collection scheme that utilises existing
NVS algorithms to produce a wide variety of distorted images along with their
SSIM score maps, serving as our training samples. Our findings demonstrate
that the CrossScore aligns closely with the full-reference SSIM score, while
eliminating the need for ground truth reference images.
2 Related Work
Thissectionoffersanoverviewofimagequalityassessment(IQA)metrics,sorted
by reference image availability and nature, and reviews the current evaluation
framework for novel view synthesis (NVS).
2.1 Image Quality Assessment Metrics
Full-Reference Metrics Mean Squared Error (MSE), Peak Signal-to-Noise
Ratio (PSNR), and Structural Similarity Index Measure (SSIM) [56] are key
metrics in the FR-IQA group for comparing a query image with the ground
truthduetotheirsimplicityandaccuracy.Further,severalvariantsareproposed
to improve performance by evaluating at multi-scale [58], utilising handcrafted
features [63,66] and extend to specific applications, such as image stitching [43],
and high dynamic range (HDR) images [26]. Recently, deep neural networks
have advanced FR-IQA towards aligning assessments more closely with human
visual perception [13,67]. Overall, FR-IQA offers detailed evaluation at the cost
of requiring ground truth images.
Reduced-ReferenceMetrics RR-IQAmethodsaredesignedtoaddresssitua-
tionswhereonlypartialinformationabouttheoriginalreferenceimageisaccessi-
ble.Wangetal.[57,59]usesgeneralisedGaussiandensitymodelparametersthat
modelnaturalimagesinthewavelettransformdomainasRRfeatures.Reduced-
reference structural similarity index (RR-SSIM) [39] approximates FR-SSIM by
usingimagestatisticalpropertiesinthedivisivenormalisationtransformdomain.
Redi et al. [38] uses descriptors based on colour correlogram that describes the4 Z. Wang et al.
spatial correlation of colours as RR data. RR metrics are commonly used for
reducing transmission costs or accelerating processing, with a trade-off in infor-
mation.Sincethereferencefeaturesstillcomefromthegroundtruthimage,the
RR group shares the same limitation as FR-IQA in many applications.
No-Reference Metrics NR-IQA methods provide an alternative to evaluat-
ing image quality based on the input only when ground truth references are
unavailable. Classical approaches like DIVINE [31], BRISQUE [29], NIQE [30]
PIQE [52] are designed with handcrafted features and natural scene statistic
models to capture image distortions and estimate quality. Kang et al. [20] first
applied CNN to NR-IQA. TRIQ [64] applied a transformer encoder to features
extractedbyCNNtopredictimagequality.MUSIQ[22]addressedtheCNNsize
constraint with a patch-based transformer. NR metrics are primarily tailored to
measure distortions, including compression artefacts, noise, and blur. However,
their ability to provide a comprehensive analysis of image content is limited by
the lack of reference images, rendering them less suitable for multi-view scenar-
ios.
General-Reference Metrics To assess the quality of generated images, com-
monlyusedmetricsincludeInceptionScore(IS)[3,42],FID[17]andKID[7,62].
Thesemetricsevaluatetheoverallperformanceofgenerativemodelsratherthan
scoring individual images. For instance, FID measures the squared Fréchet dis-
tance between the distributions of the reference image set and the generated
image set. These metrics focus on global statistics, making them well-suited for
assessingimagegenerationmodels[14,18]whileinfeasiblefornovelviewsynthe-
sis tasks [27,28].
Multi-Modal-Reference Metrics Cross-modal models facilitate the assess-
ment of alignment between images and text, enabling the development of an
MMR-IQA scheme. CLIPScore [16] directly applies the CLIP model [37] to the
image captioning task by computing the adjusted cosine similarity between the
image and candidate text as a score. LIQE [68] employs a multi-task learning
model leveraging vision-language correspondences to estimate the quality score,
scene category and distortion type. CLIP-IQA [54] applies CLIP to IQA with
simple antonym prompts to access image qualities such as brightness and noisi-
ness. By associating semantics between text and vision, these metrics are com-
monlyusedintext-to-imagegenerationandediting[21,41]andimagecaptioning
tasks [32], yet they lack the capability for detailed evaluation.
2.2 Image Quality Assessment in NVS Systems
Common evaluation metrics for NVS tasks include Full-Reference (FR) metrics
such as PSNR, SSIM [56], and LPIPS [67], which produce detailed similarity
assessment between rendered and ground truth images. Enhancements to theCrossScore: Towards Multi-View Image Evaluation and Scoring 5
L
Φenc
Query Scross
Φcross
NVS
Φdec
NVS Iteration Φ
Scene Rendered Cross-Ref Cross-Ref
Sssim
Images Images Set Network
Data Engine Cross-Reference Image Evaluation
Fig.2:DataGenerationandTrainingPipeline.WeemployexistingNVSmodels
togeneratepairsofrenderedimagesandSSIMmapsfortrainingpurposes.AstheNVS
model iterates, rendered images at various optimisation stages are used as the query
imageforinputintoourmodel.Togetherwithasetofreferenceimagesfromthesame
scene, our model predicts a score map, supervised by the corresponding SSIM map.
More details see Secs. 3.1 and 3.2.
evaluation process have been proposed, including introducing explicit represen-
tation [2], simplifying evaluation through metric summarisation [4], incorporat-
ing additional robustness metrics [53], and benchmarking with more effective
camera coverage [11].
As NVS rapidly evolves to address more complex tasks, conventional FR-
style evaluations struggle, particularly with novel views lacking ground truth
camera data, as seen in tasks like joint camera parameter and NeRF optimisa-
tion [6,9,19,24,35,49,60]. Additionally, large-scale scenes and dramatic camera
movements, such as in city-scale [40,47,61] and egocentric setups [10,12,15,
34,44,46,50], render the subsample-then-compare strategy inadequate. These
issues underscore the need for a metric better suited to multi-view evaluation
while ground truth is not available.
3 Method
Our goal is to evaluate the quality of a query image I˜, using a set of reference
q
images I = {Ii|i = 1...N } that capture the same scene as the query image
r r ref
but from other viewpoints, where i denotes the ith image in a reference set with
N images. We refer to this reference set as the cross-reference (CR) set. From
ref6 Z. Wang et al.
the NVS application perspective, the query image I˜ is often a rendered image
q
with artefacts, and the reference set consists of the real captured images.
To achieve this goal, we propose a simple but effective strategy, by finding a
functionthatpredictsawell-establishedFRscore,e.g.SSIM,foraqueryimage.
Unlike the SSIM function, which takes the input of a pre-aligned ground truth
image, our new function takes multi-view images as input.
For a query image I˜ ∈ RH×W×3, the SSIM function compares it with its
q
ground truth image I in a sliding window fashion, and outputs a score map
q
S ∈RH×W:
ssim
f(I˜,I )(cid:55)→S , (1)
q q ssim
where f(·) denotes the SSIM function.1
The aim of our work is to predict a score map S ∈ RH×W, which is
cross
highly correlated with S , by comparing a query image with the CR set I ,
ssim r
instead of with its fully aligned ground truth image I . In other words, we seek
q
afunctiong(·)thatapproximatestheSSIMfunctionf(·),butmakinguseofthe
CR set I :
r
g(I˜,I )(cid:55)→S ≈S . (2)
q r cross ssim
The intuition here is to approximate a ‘perspective SSIM’ function by replacing
the ground image with a set of unregistered multi-view images.
We parameterise the cross-reference function g(·) with a neural network Φ.
We elaborate on the network design and training strategy in the subsequent
sections.
3.1 Network Design
As shown in Fig. 2, our network Φ consists of three parts, i) an image encoder
Φ ,whichextractsfeaturemapsfrominputimages;ii)across-referencemodule
enc
Φ , which associates a query image I˜ with images in a CR set I and
cross q ref
produces a latent score map; and iii) a score regression head Φ that decodes
dec
the latent score map to the final score map S .
cross
Image Encoder Φ We adapt a pre-trained DINOv2 [33] network as our
enc
imageencoderΦ ,whichtakesanimageI asinputandoutputsafeaturemap
enc
F = Φ (I). This image encoder is applied to all images including query and
enc
reference images, and produces feature maps F and Fi. We adopt the same
q r
patch-wise positional encoding scheme as DINOv2, with each small patch being
assigned a positional embedding. Since our cross-reference function takes a set
of unordered reference images, image-wise encoding is not applied.
1TherawSSIMscoremapsharesthesamedimensionsasaqueryimage,i.e.S ∈
ssim
RH×W×3. For simplicity, we follow a standard practice that averages the SSIM scores
across colour channels, yielding a single-channel score map S ∈RH×W.
ssimCrossScore: Towards Multi-View Image Evaluation and Scoring 7
Cross-Reference Module Φ We leverage a Transformer Decoder [51] in
cross
our cross-reference module Φ . Given a feature map of a query image F , the
cross q
cross-reference module Φ outputs a latent score map M = Φ (F ,F ),
cross cross q r
by comparing F with the set of reference feature maps F ={Fi|i=1...N }.
q r r ref
Specifically, this cross-reference is conducted by the cross-attention mechanism,
wherethefeaturemapofqueryimageF isthequery ofthecross-attention,and
q
the set of feature maps of the reference images F serve as key and value in the
r
cross attention.
Score Regression Head Φ With a latent score map predicted from the
dec
cross-reference module Φ , a small regression head Φ is applied to finally
cross dec
predict CrossScore S . We use a shallow Multi-layer Perceptron (MLP) to
cross
interpret a latent score map to a per-pixel score map. Since DINOv2 encodes
images by patches, a latent score vector contains the quality estimation for the
entire patch. In order to predict CrossScore in pixel level, we use the last layer
of the MLP to interpret each latent score estimation to a 196-dimension vector,
which is then reshaped to a 14×14 patch that corresponds to a small image
patch encoded by DINOv2.
3.2 Training Strategy
Self-supervised Training Data Collection We leverage existing NVS sys-
temsandabundantmulti-viewdatasetstogenerateSSIMmapsforourtraining.
Specifically, we select Neural Radiance Field (NeRF)-style NVS systems as our
data engine. Given a set of images, a NeRF recovers a neural representation
of a scene by iteratively reconstructing the given image set with photometric
losses. By rendering images with the camera parameters from the original cap-
tured image set at multiple training checkpoints, we generate a large number
of images that contain various types of artefacts at various levels. From which,
we compute SSIM maps S between rendered images and corresponding real
ssim
captured images, which serve as our training objectives.
Supervision This data collection scheme enables a self-supervised training
scheme for our network Φ. We consider each rendered image as a query im-
ageI˜,andwerandomlysamplerealcapturedimages(excludetherealimageof
q
the query) to form our reference set I . The SSIM map of the rendered image
r
S is then used to supervise our network to predict a S with an L loss:
ssim cross 1
L=|S −S |. (3)
ssim cross
4 Experiments
WestartthissectionbyoutliningourexperimentalsetupinSec.4.1,followedby
assessing the correlation between our score and SSIM through both qualitative8 Z. Wang et al.
and quantitative analyses in Sec. 4.2. We then demonstrate the application of
ourCR-IQAintwoscenarios:benchmarkingunseenNeRFalgorithms(Sec.4.3)
and evaluating images rendered from novel trajectories in NVS without ground
truth(Sec.4.4).Additionally,weexaminetheeffectivenessofourcross-reference
module via a visualisation of its attention maps in Sec. 4.5 and an ablation
study in Sec. 4.6. Lastly, Sec. 4.7 concludes our experiments with a discussion
on limitations and future research directions.
4.1 Experimental Setup
Dataset We utilise three datasets in our primary experiments. First, the Map-
free Relocalisation (MFR) [1] dataset, initially designed for camera parameter
estimation benchmarks, has been adapted for our data collection and network
training. This dataset features 460 outdoor videos of objects and buildings in a
resolution of 540x960. Second, we utilise the Mip360 [5] dataset, consisting of 9
videosthatcapture360-degreescansofdiversescenes,bothoutdoorandindoor.
The original resolution of the images is ∼4K. To facilitate DINOv2 [33] image
encoding,wedownscaleallimagesbyafactorof4.Third,werandomlyselect10
videos from the RealEstate10K (RE10K) [69] dataset, originally in 1920×1080
resolution, which are downscaled to 960×540. Training and evaluation: Our
network is solely trained on the MFR dataset, from which 348 and 14 videos
are randomly selected as training and evaluation split respectively. In addition
to evaluating on MFR evaluation split, we further assess the performance of our
method using Mip360 and RE10K datasets.
Metrics Toevaluatetheeffectivenessofourmethodinpredictingscoresclosely
aligned with SSIM values, we use correlation coefficients as our primary evalua-
tivemetric.ThePearsoncorrelationcoefficient[36]isutilisedacrossthemajority
ofouranalyses,complementedbySpearman’srankcorrelationcoefficient[45]for
studies involving new camera trajectories. These coefficients, ranging in [−1,1],
measurethestrengthofassociationbetweenCrossScoreandSSIM,withalarger
magnitude indicating a stronger correlation.
Baselines We choose five well-established IQA methods as baselines. Two
from FR-IQA family: SSIM [56] and PSNR, and three from NR-IQA family:
BRISQUE [29], NIQE [30], and PIQE [52].
Network Training Architecture: We adopt a pre-trained DINOv2-small net-
workastheimageencoderΦ ,whichencodesimageswithapatchsize14×14
enc
and produces features in 384 channels. The CLS token is ignored. The cross-
reference module Φ incorporates 2 transformer decoder layers with hidden
cross
dimension 384, and the decoder Φ is equipped with a 2-layer MLP. Pre-pro-
dec
cessing: During training, we randomly crop 518×518 images from both query
and reference images, whilst during inference, our model supports inputs at ar-
bitrary resolutions. Notably, raw SSIM maps may occasionally present valuesCrossScore: Towards Multi-View Image Evaluation and Scoring 9
Query Query Query
Ref 1 Ref 2 Ref 3 Ref 1 Ref 2 Ref 3 Ref 1 Ref 2 Ref 3
GT Ours SSIM GT Ours SSIM GT Ours SSIM
Fig.3: Qualitative results of CrossScore and SSIM on various datasets. We
present examples for test results on each dataset (from left to right: RE10K, MFR,
Mip360). We show our score maps have a strong correlation with SSIM score, demon-
strating the generalisation capability of our approach across diverse datasets. Score
colour coding: red represents the highest score, followed by orange, green, and blue,
indicating decreasing scores respectively.
below zero, and we found clamping raw SSIM maps to the range [0,1] leads to
a slightly more stable training process. For the cross-reference set selection, we
randomly choose N =5 real images from the same scene as the query image.
ref
Optimisation: We apply a constant learning rate of 5e-4 with an Adam-W [25]
optimiser, training on 2× NVIDIA A5000 24GB GPUs for 160,000 iterations in
60 hours, with a per-GPU batch size of 24.
Training Data Generation We optimise Gaussian-Splatting (GS) [23], Ner-
facto [48], and TensoRF [8] on the MFR [1] dataset for 15,000 iterations, saving
checkpoints every 1,000 iterations up to 10,000, and a final one at 15,000 it-
erations. Images rendered from these checkpoints are compared against ground
truths to produce SSIM maps. To reduce the cost of this process, we tempo-
rally subsample the MFR dataset by a factor of 8. The entire data processing
spannedapproximatelytwoweeks,utilising4×NVIDIAA5000GPUs.Thegen-
erated images and SSIM maps take about 1.5TB of storage. Our selection of
GS, Nerfacto, and TensoRF was based on their efficiency and output quality.
Each method employs a distinct NVS approach: GS models scenes with point
clouds, Nerfacto utilises voxel grids, and TensoRF decomposes a 3D scene to
planes,ensuringadiverseandhigh-qualityimagerenderingprocess.Asaresult,
this approach balances data generation cost while producing a wide variety of
distorted images and accurate corresponding SSIM maps.
4.2 Correlation with SSIM
WeevaluateCrossScorebycomparingittoSSIMusingPearsonCorrelation[36],
with results shown in Tab. 1 alongside other baselines. Our approach demon-10 Z. Wang et al.
Table1:CorrelationbetweenvariousmetricsandSSIMonvariousdatasets.
FR: full-reference. NR: no-reference. CR: cross-reference. We show CrossScore is
highly correlated with SSIM score on various datasets, while only being trained with
the MFR dataset.
FR NR CR
Datasets
SSIM [56]PSNR BRISQUE [29]NIQE [30]PIQE [52] Ours
RE10K 1.00 0.92 0.46 0.32 0.27 0.99
Mip360 1.00 0.91 0.19 0.61 0.69 0.95
MFR 1.00 0.92 0.23 -0.30 -0.11 0.83
Table2:EvaluatingFew-shotNeRFswithVariousMetrics.Weshowthatwhen
comparingtwofew-shotNeRFmodelsIBRNet[55]andPixelNeRF[65],CrossScoreis
consistentwithfull-referencemetricssuchasSSIMandPSNR.Inthiscase,allmetrics
shows that IBRNet performs better than PixelNeRF on MFR dataset.
Metric SSIM Ours PSNR
PixelNeRF 0.26 0.40 9.17
IBRNet 0.44 0.71 18.51
strates a strong correlation with the full-reference SSIM without using ground
truth.Moreover,trainedsolelyontheMFRdataset,ourmethodsuccessfullygen-
eralises to various settings, including indoor, outdoor, and 360-degree scanning
environments, highlighting its versatile applicability. Fig. 3 provides qualitative
results supporting our findings.
4.3 Application: Evaluating Few-shot NeRFs
Thisexperimentdemonstratestheapplicationof CrossScoreforevaluatingfew-
shot NeRF methods, specifically comparing IBRNet [55] and PixelNeRF [65]
usingofficialcheckpoints.Tab.2showsthatbothSSIM,PSNR,andCrossScore
suggest IBRNet performs better on the MFR dataset. Note that the aim here is
tohighlighttheabilityof CrossScoretodiscernperformancedifferencesbetween
methods rather than to benchmark them comprehensively.
4.4 Application: IQA on Images Rendered From a Novel Trajectory
Inthisexperiment,wedemonstratethatourcross-referencemethodenablestrue
novelviewrenderingevaluation.Specifically,givenanNVS-reconstructedscene,
we evaluated this scene in two distinct ways, as illustrated in Fig. 4. First, we
follow the conventional test split, which considers every 8th image as a test
image, and compute the SSIM score between the rendered image and ground
truth. Second, we evaluate true novel view renderings that are rendered fromCrossScore: Towards Multi-View Image Evaluation and Scoring 11
Legend
Real camera (train)
Real camera (subsampled test)
Novel camera (B-spline)
Fig.4: Illustration of two IQA approaches in NVS: 1) with subsampled test
views and 2) with true novel views. The first approach relies on full-reference
metrics that requires ground truth images, precluding test views in training (blue
circles enclosed in orange boxes). In contrast, our cross-reference approach bypasses
theneedforgroundtruthviews,allowingNVSevaluationfromtruenovelviews(green
circles) and enabling NVS modelling to utilise the entire captured image set.
Table 3: IQA on Images Renderings From Novel Trajectories. We evaluate
eachsequenceintwoways:1)computingSSIMonimagesrenderedfromthestandard
subsampled test split with ground truth images, and 2) computing CrossScore on
images rendered from a novel trajectory, with a cross-reference set randomly sam-
pled from training images. We show that our method can evaluate the quality of
Gaussian-Splattingfromanoveltrajectorywithoutrequiringalignedgroundtruthim-
ages. Our cross-reference style score is highly correlated with the full-reference SSIM
score, and ranking video quality using CrossScore is similar to ranking with SSIM.
Top: SSIM and CrossScore. Higher is better. Bottom: quality ranking using SSIM
and CrossScore respectively. Lower is better. ‘Corr’ denotes Pearson correlation for
scores and Spearman’s rank correlation for rankings.
Scene 426 34 10 135 238 284 103 441 345 311 175 244 82 4 Corr
SSIM 0.74 0.66 0.64 0.64 0.61 0.61 0.59 0.58 0.56 0.55 0.51 0.50 0.44 0.40
Score↑ 0.84
Ours 0.80 0.78 0.77 0.78 0.66 0.61 0.73 0.75 0.73 0.72 0.62 0.58 0.55 0.53
SSIM 0 1 2 3 4 5 6 7 8 9 10 11 12 13
Rank↓ 0.85
Ours 0 2 3 1 8 10 6 4 5 7 9 11 12 13
a novel trajectory2 with CrossScore without ground truth. Tab. 3 indicate a
closecorrelationbetweenCrossScoreevaluationsofnovelviewsandtraditional
SSIM scores. Additionally, the rankings of rendering quality for these scenes,
determined using both SSIM and CrossScore, are also closely aligned.
4.5 Visualising Attention Weights
To delve deeper into our cross-reference method, Fig. 5 visualises the attention
weights in the cross-attention layer for the central patch of a query image. This
illustration confirms that the cross-attention mechanism effectively focuses on
similar content from the cross-reference set, thereby providing insight into the
results of the ablation study in Sec. 4.6.
2Novel trajectories are generated by interpolating training poses with a B-spline
function (degree of 10), creating 20 novel poses per scene.12 Z. Wang et al.
Ref 1 Ref 1
Query Ref 2 Query Ref 2
Ours SSIM Ref 3 Ours SSIM Ref 3
Fig.5: Visualisation of attention weights from the cross-reference module
Φ .Top left:aqueryimagewitharegionofinterest(centreofimage)highlighted
cross
with a magenta box. Right column: We show 3 reference images from our cross-
referencesetwithattentionmapsoverlaid.Theattentionmapsillustratetheattention
thatispaidtopredictingimagequalityatthequeryregion.Redandbluedenotehigh
and low attention weights respectively. Note that we use N =5 but only 3 is shown
ref
duetospaceconstraint.Bottom:PredictedCrossScoremapandSSIMmap.Redand
blue denote high and low quality image regions respectively.
Ref 1 Ref 2
Query Ref 3 Ref 4 SSIM Ours: Ref ON Ours: Ref OFF
Fig.6: Ablation study: reference set enabled (on) and disabled (off). We
show that with reference images enabled, the score map predicted by our method
contains more details. When the reference images are disabled, the model tends to
assigneverythingahighscore.ThisisalsoevidencedbyquantitativeresultsinTab.4.
4.6 Ablation Study: Enable and Disable Reference Views
This experiment demonstrates that our cross-reference module effectively uses
the cross-reference set for quality prediction. When provided with reference im-CrossScore: Towards Multi-View Image Evaluation and Scoring 13
Table 4: Ablation study: reference set enabled (✓) and disabled (✗). Our
method performs closer to SSIM when reference images are provided. Note that when
referenceimagesaredisabled,thepredictedscores stillshow acertainlevelofcorrela-
tion,ascertainnoisepatternscanbeidentifiedfromlocalimagestatistics.Inthiscase
our method degrades to a no-reference-style image evaluation.
Scene 4 10 34 82 103 135 175 238 244 284 311 345 426 441 Avg Corr
SSIM 0.40 0.64 0.66 0.44 0.59 0.64 0.51 0.61 0.50 0.61 0.55 0.56 0.74 0.58 0.57 1.00
Ours✓ 0.46 0.72 0.72 0.48 0.64 0.75 0.56 0.61 0.66 0.58 0.65 0.72 0.82 0.71 0.65 0.83
Ours✗ 0.71 0.81 0.83 0.73 0.80 0.84 0.80 0.79 0.86 0.80 0.74 0.86 0.89 0.85 0.81 0.68
ages, the module offers detailed and accurate evaluations, as shown in Fig. 6,
in contrast to the high scores predicted across almost all regions when refer-
enceimagesaredisabled.Notethat,inthiscontext,wedisablereferenceimages
by setting all pixels in reference images to zero. Quantitative support for these
findings is presented in Tab. 4.
4.7 Limitations and Future Work
Query GT
We outline two future research directions: First, en-
hancing the sharpness of our score maps to match
theclarityoffull-referenceSSIM,possiblybyintegrat-
ing pixel-level positional encoding or super-resolution
Ours SSIM
methods to mitigate the blurring from patch-wise en-
coding of ViT models. Second, tackling the issue with
unconventional images, such as those from fish-eye
lensesthatleadtoinaccuratepredictions,asillustrated
in Fig. 7.
Fig.7:Evaluatingafish-
eye-style query image.
5 Conclusion
In summary, we introduce a novel Cross-Reference Image Quality Assessment
(CR-IQA) scheme, filling a critical gap in existing IQA schemes. By leveraging
a neural network with cross-attention mechanisms and a unique NVS-enabled
data collection pipeline, we demonstrate the feasibility of accurately evaluating
thequalityofanimagebycomparingitwithotherviewsofthesamescene.Our
experimental results indicate that our predictions closely align with ground-
truth-dependent metrics.
Acknowledgement
This research is supported by an ARIA research gift grant from Meta Reality
Lab. We gratefully thank Shangzhe Wu, Tengda Han, Zihang Lai for insightful
discussions, and Michael Hobley for proofreading.14 Z. Wang et al.
A Appendix
A.1 Additional Qualitative Results
We invite readers to check out a video with additional qualitative results on our
project page: https://crossscore.active.vision.
A.2 Additional Details for Table 1
We offer detailed results for Tab. 1 in Tables 5 to 7. For no-reference baselines,
assessments are conducted using Matlab with their default settings and feature
models.
Table 5: Correlation between various metrics and SSIM on the Map-Free Relocalisa-
tion (MFR) dataset.
MFR
FR NR CR
Scene
SSIMPSNR BRISQUENIQEPIQE Ours
s00004 0.40 15.88 19.40 3.00 34.10 0.46
s00010 0.64 19.16 20.82 3.21 28.39 0.72
s00034 0.66 21.91 25.23 2.66 29.74 0.72
s00082 0.44 16.35 23.47 2.68 34.54 0.48
s00103 0.59 16.43 30.50 3.16 47.39 0.64
s00135 0.64 20.12 20.90 2.88 42.21 0.75
s00175 0.51 17.32 24.83 3.24 31.79 0.56
s00238 0.61 16.74 27.15 2.74 34.17 0.61
s00244 0.50 18.03 25.57 3.24 42.06 0.66
s00284 0.61 19.46 25.79 2.78 30.60 0.58
s00311 0.55 17.82 24.08 3.21 26.75 0.65
s00345 0.56 18.82 19.71 2.83 41.05 0.72
s00426 0.74 22.10 24.41 2.66 32.16 0.82
s00441 0.58 20.36 26.03 2.51 39.36 0.71
Correlation 1.00 0.78 0.23 -0.30 -0.11 0.83CrossScore: Towards Multi-View Image Evaluation and Scoring 15
Table 6: Correlation between various metrics and SSIM on the Mip360 dataset.
Mip360
FR NR CR
Scene
SSIMPSNR BRISQUENIQEPIQE Ours
bicycle 0.85 26.66 22.30 2.67 33.41 0.82
bonsai 0.95 32.48 27.08 3.46 52.90 0.89
counter 0.92 29.82 25.61 2.78 50.57 0.87
flowers 0.72 25.31 26.53 2.57 31.46 0.64
garden 0.92 31.19 13.18 2.37 31.38 0.87
kitchen 0.95 32.48 30.73 3.03 43.82 0.85
room 0.94 33.42 33.43 2.93 53.95 0.91
stump 0.83 30.43 22.52 2.97 21.35 0.81
treehill 0.74 25.25 23.58 2.30 31.22 0.73
Correlation 1.00 0.91 0.19 0.61 0.69 0.95
Table 7: Correlation between various metrics and SSIM on the RealEstate10K
(RE10K) dataset.
RealEstate10K
FR NR CR
Scene
SSIMPSNR BRISQUENIQEPIQE Ours
00407b3f1bad1493 0.90 26.06 44.33 3.70 69.66 0.91
004ed278c2b168f1 0.73 20.13 53.48 4.39 54.82 0.77
0065a058603dfca4 0.88 22.98 49.01 4.18 75.67 0.90
00703cbf7531ef11 0.56 17.74 30.67 2.57 42.89 0.67
00761c6dcec91853 0.95 31.34 44.70 3.83 62.81 0.93
007ac6cef80a692c 0.90 22.76 33.68 3.25 70.71 0.91
0081cfd790d7ad74 0.02 10.45 NaN NaN NaN 0.23
009664cb1b8d351a 0.74 17.00 52.77 4.39 82.10 0.74
00a50bfbce75d465 0.86 23.86 38.82 3.22 65.02 0.88
00a9f110ad222aa4 0.81 22.34 32.61 2.25 49.61 0.82
00b52b21e0d54a42 0.89 22.64 43.64 3.70 72.21 0.90
00b9a7963f9bd9c6 0.37 14.60 34.06 3.30 67.52 0.38
00c8250efd605554 0.15 8.73 32.17 2.98 60.74 0.19
Correlation 1.00 0.92 0.46 0.32 0.27 0.9916 Z. Wang et al.
References
1. Arnold,E.,Wynn,J.,Vicente,S.,Garcia-Hernando,G.,Monszpart,Á.,Prisacariu,
V.A.,Turmukhambetov,D.,Brachmann,E.:Map-freevisualrelocalization:Metric
pose relative to a single image. In: ECCV (2022) 8, 9
2. Azzarelli,A.,Anantrasirichai,N.,Bull,D.R.:Towardsarobustframeworkfornerf
evaluation. arXiv preprint arXiv:2305.18079 (2023) 5
3. Barratt, S., Sharma, R.: A note on the inception score. arXiv preprint
arXiv:1801.01973 (2018) 4
4. Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srini-
vasan,P.P.:Mip-nerf:Amultiscalerepresentationforanti-aliasingneuralradiance
fields. In: ICCV (2021) 5
5. Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Mip-nerf
360: Unbounded anti-aliased neural radiance fields. In: CVPR (2022) 8
6. Bian,J.W.,Bian,W.,Prisacariu,V.A.,Torr,P.:Porf:Poseresidualfieldforaccu-
rate neural surface reconstruction. In: ICLR (2023) 5
7. Bińkowski,M.,Sutherland,D.J.,Arbel,M.,Gretton,A.:Demystifyingmmdgans.
In: ICLR (2018) 4
8. Chen,A., Xu,Z., Geiger, A.,Yu,J., Su,H.: Tensorf: Tensorial radiancefields. In:
ECCV (2022) 9
9. Chen, Y., Lee, G.H.: Dbarf: Deep bundle-adjusting generalizable neural radiance
fields. In: CVPR (2023) 5
10. Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E.,
Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: The epic-kitchens
dataset: Collection, challenges and baselines. TPAMI (2021) 5
11. DeLuigi,L.,Bolognini,D.,Domeniconi,F.,DeGregorio,D.,Poggi,M.,DiStefano,
L.: Scannerf: a scalable benchmark for neural radiance fields. In: WACV (2023) 5
12. Deng, N., He, Z., Ye, J., Duinkharjav, B., Chakravarthula, P., Yang, X., Sun, Q.:
Fov-nerf:Foveatedneural radiancefieldsforvirtualreality.IEEE Transactionson
Visualization and Computer Graphics (2022) 5
13. Ding, K., Ma, K., Wang, S., Simoncelli, E.P.: Image quality assessment: Unifying
structure and texture similarity. TPAMI (2020) 3
14. Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,
Courville, A., Bengio, Y.: Generative adversarial nets. In: NeurIPS (2014) 4
15. Grauman,K.,Westbury,A.,Byrne,E.,Chavis,Z.,Furnari,A.,Girdhar,R.,Ham-
burger, J., Jiang, H., Liu, M., Liu, X., et al.: Ego4d: Around the world in 3,000
hours of egocentric video. In: CVPR (2022) 5
16. Hessel,J.,Holtzman,A.,Forbes,M.,Bras,R.L.,Choi,Y.:Clipscore:Areference-
free evaluation metric for image captioning. In: Empirical Methods in Natural
Language Processing (EMNLP) (2021) 1, 2, 4
17. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
byatwotime-scaleupdateruleconvergetoalocalnashequilibrium.In:NeurIPS
(2017) 1, 2, 4
18. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS
(2020) 4
19. Jeong,Y.,Ahn,S.,Choy,C.,Anandkumar,A.,Cho,M.,Park,J.:Self-calibrating
neural radiance fields. In: ICCV (2021) 5
20. Kang, L., Ye, P., Li, Y., Doermann, D.: Convolutional neural networks for no-
reference image quality assessment. In: CVPR (2014) 4CrossScore: Towards Multi-View Image Evaluation and Scoring 17
21. Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani,
M.:Imagic:Text-basedrealimageeditingwithdiffusionmodels.In:CVPR(2023)
4
22. Ke, J., Wang, Q., Wang, Y., Milanfar, P., Yang, F.: Musiq: Multi-scale image
quality transformer. In: ICCV (2021) 4
23. Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for
real-time radiance field rendering. TOG (2023) 9
24. Lin, C.H., Ma, W.C., Torralba, A., Lucey, S.: Barf: Bundle-adjusting neural radi-
ance fields. In: ICCV (2021) 5
25. Loshchilov,I.,Hutter,F.:Decoupledweightdecayregularization.In:ICLR(2018)
9
26. Mantiuk, R., Kim, K.J., Rempel, A.G., Heidrich, W.: Hdr-vdp-2: A calibrated
visual metric for visibility and quality predictions in all luminance conditions.
TOG (2011) 3
27. Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi,
R.,Ng,R.,Kar,A.:Locallightfieldfusion:Practicalviewsynthesiswithprescrip-
tive sampling guidelines. TOG (2019) 4
28. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.:Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis.Commu-
nications of the ACM (2021) 4
29. Mittal, A., Moorthy, A.K., Bovik, A.C.: No-reference image quality assessment in
the spatial domain. IEEE Transactions on Image Processing (2012) 4, 8, 10
30. Mittal, A., Soundararajan, R., Bovik, A.C.: Making a “completely blind” image
quality analyzer. IEEE Signal Processing Letters (2012) 1, 2, 4, 8, 10
31. Moorthy, A.K., Bovik, A.C.: Blind image quality assessment: From natural scene
statistics to perceptual quality. IEEE Transactions on Image Processing (2011) 4
32. Nguyen, T., Gadre, S.Y., Ilharco, G., Oh, S., Schmidt, L.: Improving multimodal
datasets with image captioning. In: NeurIPS (2024) 4
33. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V.,
Fernandez,P.,Haziza,D.,Massa,F.,El-Nouby,A.,etal.:Dinov2:Learningrobust
visual features without supervision. TMLR (2023) 6, 8
34. Pan,X.,Charron,N.,Yang,Y.,Peters,S.,Whelan,T.,Kong,C.,Parkhi,O.,New-
combe, R., Ren, Y.C.: Aria digital twin: A new benchmark dataset for egocentric
3d machine perception. In: ICCV (2023) 5
35. Park, K., Henzler, P., Mildenhall, B., Barron, J.T., Martin-Brualla, R.: Camp:
Camera preconditioning for neural radiance fields. TOG (2023) 5
36. Pearson,K.:Mathematicalcontributionstothetheoryofevolution.iii.regression,
heredity,andpanmixia.PhilosophicalTransactionsoftheRoyalSocietyofLondon
(1896) 8, 9
37. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021) 4
38. Redi, J.A., Gastaldo, P., Heynderickx, I., Zunino, R.: Color distribution informa-
tionforthereduced-referenceassessmentofperceivedimagequality.IEEETrans-
actions on Circuits and Systems for Video Technology (2010) 3
39. Rehman, A., Wang, Z.: Reduced-reference image quality assessment by structural
similarity estimation. IEEE Transactions on Image Processing (2012) 3
40. Rematas,K.,Liu,A.,Srinivasan,P.P.,Barron,J.T.,Tagliasacchi,A.,Funkhouser,
T., Ferrari, V.: Urban radiance fields. In: CVPR (2022) 518 Z. Wang et al.
41. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. In: NeurIPS (2022)
4
42. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.:
Improved techniques for training gans. In: NeurIPS (2016) 4
43. Solh, M., AlRegib, G.: Miqm: A novel multi-view images quality measure. In:
International Workshop on Quality of Multimedia Experience (2009) 3
44. Somasundaram, K., Dong, J., Tang, H., Straub, J., Yan, M., Goesele, M., Engel,
J.J., De Nardi, R., Newcombe, R.: Project aria: A new tool for egocentric multi-
modal ai research. arXiv preprint arXiv:2308.13561 (2023) 5
45. Spearman,C.:Theproofandmeasurementofassociationbetweentwothings.The
American Journal of Psychology (1904) 8
46. Sun,J.,Qiu,J.,Zheng,C.,Tucker,J.,Yu,J.,Schwager,M.:Aria-nerf:Multimodal
egocentric view synthesis. arXiv preprint arXiv:2311.06455 (2023) 5
47. Tancik,M.,Casser,V.,Yan,X.,Pradhan,S.,Mildenhall,B.,Srinivasan,P.P.,Bar-
ron,J.T.,Kretzschmar,H.:Block-nerf:Scalablelargesceneneuralviewsynthesis.
In: CVPR (2022) 5
48. Tancik,M.,Weber,E.,Ng,E.,Li,R.,Yi,B.,Wang,T.,Kristoffersen,A.,Austin,J.,
Salahi,K.,Ahuja,A.,etal.:Nerfstudio:Amodularframeworkforneuralradiance
field development. In: SIGGRAPH (2023) 9
49. Truong,P.,Rakotosaona,M.J.,Manhardt,F.,Tombari,F.:Sparf:Neuralradiance
fields from sparse and noisy poses. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 4190–4200 (2023) 5
50. Tschernezki, V., Larlus, D., Vedaldi, A.: Neuraldiff: Segmenting 3d objects that
move in egocentric videos. In: 3DV (2021) 5
51. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
Ł., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017) 3, 7
52. Venkatanath, N., Praneeth, D., Bh, M.C., Channappayya, S.S., Medasani, S.S.:
Blindimagequalityevaluationusingperceptionbasedfeatures.In:NationalCon-
ference on Communications (2015) 2, 4, 8, 10
53. Wang,C.,Wang,A.,Li,J.,Yuille,A.,Xie,C.:Benchmarkingrobustnessinneural
radiance fields. arXiv preprint arXiv:2301.04075 (2023) 5
54. Wang, J., Chan, K.C., Loy, C.C.: Exploring clip for assessing the look and feel of
images. In: AAAI (2023) 4
55. Wang,Q.,Wang,Z.,Genova,K.,Srinivasan,P.P.,Zhou,H.,Barron,J.T.,Martin-
Brualla,R.,Snavely,N.,Funkhouser,T.:Ibrnet:Learningmulti-viewimage-based
rendering. In: CVPR (2021) 10
56. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:
fromerrorvisibilitytostructuralsimilarity.IEEETransactionsonImageProcess-
ing (2004) 1, 2, 3, 4, 8, 10
57. Wang, Z., Simoncelli, E.P.: Reduced-reference image quality assessment using a
wavelet-domain natural image statistic model. In: Human Vision and Electronic
Imaging X (2005) 3
58. Wang,Z.,Simoncelli,E.P.,Bovik,A.C.:Multiscalestructuralsimilarityforimage
quality assessment. In: Asilomar Conference on Signals, Systems & Computers
(2003) 3
59. Wang,Z.,Wu,G.,Sheikh,H.R.,Simoncelli,E.P.,Yang,E.H.,Bovik,A.C.:Quality-
aware images. IEEE Transactions on Image Processing (2006) 3CrossScore: Towards Multi-View Image Evaluation and Scoring 19
60. Wang,Z.,Wu,S.,Xie,W.,Chen,M.,Prisacariu,V.A.:NeRF−−:Neuralradiance
fields without known camera parameters. arXiv preprint arXiv:2102.07064 (2021)
5
61. Xiangli, Y., Xu, L., Pan, X., Zhao, N., Rao, A., Theobalt, C., Dai, B., Lin, D.:
Bungeenerf:Progressiveneuralradiancefieldforextrememulti-scalescenerender-
ing. In: ECCV (2022) 5
62. Xu, Q., Huang, G., Yuan, Y., Guo, C., Sun, Y., Wu, F., Weinberger, K.: An
empirical study on evaluation metrics of generative adversarial networks. arXiv
preprint arXiv:1806.07755 (2018) 4
63. Xue, W., Zhang, L., Mou, X., Bovik, A.C.: Gradient magnitude similarity devi-
ation: A highly efficient perceptual image quality index. IEEE Transactions on
Image Processing (2013) 3
64. You, J., Korhonen, J.: Transformer for image quality assessment. In: ICIP (2021)
4
65. Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelnerf: Neural radiance fields from
one or few images. In: CVPR (2021) 10
66. Zhang, L., Zhang, L., Mou, X., Zhang, D.: Fsim: A feature similarity index for
image quality assessment. IEEE Transactions on Image Processing (2011) 3
67. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: CVPR (2018) 2, 3, 4
68. Zhang, W., Zhai, G., Wei, Y., Yang, X., Ma, K.: Blind image quality assessment
via vision-language correspondence: A multitask learning perspective. In: CVPR
(2023) 4
69. Zhou,T.,Tucker,R.,Flynn,J.,Fyffe,G.,Snavely,N.:Stereomagnification.TOG
(2018) 8