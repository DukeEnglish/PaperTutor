PARAMANU-GANITA: Language Model with Mathematical Capabilities
MitodruNiyogi ArnabBhattacharya
GyanAIResearch Dept. ofComputerScience&Engineering,
AbuDhabi,UAE IndianInstituteofTechnologyKanpur,
mitodru@bharatgpts.com India
&GyanAIResearch
arnabb@cse.iitk.ac.in
Abstract vron et al., 2023b), PaLM (et al., 2022), Falcon
(Almazrouei et al., 2023), Code LlaMa (Rozière
Inthispaper,wepresentPARAMANU-GANITA,
a208millionparameternovelAutoRegressive etal.,2024),MPT1,etc.) havedemonstratedmulti-
(AR)decoderbasedlanguagemodelonmathe- dimensional abilities, such as in open-ended dia-
matics. Themodelispretrainedfromscratch logueorinstructionfollowing(Ouyangetal.,2022)
atcontextsizeof4096onourcuratedmixed capabilitiesandbeingtypicallygeneralistlanguage
mathematicalcorpus. Weevaluateourmodel
modelsbalancingtheperformanceacrosstheentire
onbothperplexitymetricandGSM8kmathe- distribution of natural language tasks. However,
maticalbenchmark. Paramanu-Ganitadespite
thesegeneralistmodelsarehumongousinsizeand
being 35 times smaller than 7B LLMs, out-
requires million dollars to train aside from high
performedgeneralistLLMssuchasLLaMa-1
7B by 28.4% points, LLaMa-2 7B by 27.6% engineeringinferencecostinvolved. Traditionally,
points, Falcon 7B by 32.6% points, PaLM tooptimizeperformancewithinspecificdomains
8B by 35.3% points, and math specialised suchasfinance(Wuetal.,2023),medicine(Sing-
LLMs such as Minerva 8B by 23.2% points,
haletal.,2023),etc.,thesemodelshavebeencon-
andLLEMMA-7Bby3.0%pointsinGSM8k
tinuedtrainedondomainspecificdata. However,
test accuracy metric respectively. Paramanu-
domainspecificcontinualpretrainingofLLMsare
Ganita also outperformed giant LLMs like
alsoveryexpensivetoouropinion. Foremploying
PaLM 62B by 6.4% points, Falcon 40B by
19.8% points, LLaMa-1 33B by 3.8% points adomain-specificLLM,lotofcomputationandin-
andVicuna13Bby11.8%pointsrespectively. ferencecostsareinvolvedalongwithhighrequire-
The large significant margin improvement in mentofGPUs. Forexample,toimprovethemathe-
performance of our math model over the ex- maticalreasoningcapabilitiesofLLMs,LLEMMA
istingLLMssignifiesthatreasoningcapabili-
7B (Azerbayev et al., 2024) was trained on 256
tiesoflanguagemodelarejustnotrestrictedto
A10040GBGPUsforroughly23,000A100train-
LLMswithhumongousnumberofparameters.
inghours,whichisyetveryexpensive. Insteadof
Paramanu-Ganitatook146hoursofA100train-
followingthedomainadaptationmethodofLLMs
ingwhereasmathspecialisedLLM,LLEMMA
7B,wastrainedfor23,000A100hoursoftrain- forbettermathematicalreasoning,wefocusedon
ing equivalent. Thus, our approach of pre- pretrainingfromscratchagenerativemathematical
trainingpowerfuldomainspecialisedlanguage language model only on our curated high quality
modelsfromscratchfordomainadaptationis
mathematical corpus. This avoids requiring im-
muchmorecost-effectivethanperformingcon-
mense compute power, high engineering maneu-
tinualtrainingofLLMsfordomainadaptation.
verandtechniquestoloadLLMsinmemory,and
Hence,weconcludethatforstrongmathemati-
mostly high cost of training, and non-specialised
calreasoningabilitiesoflanguagemodel,we
donotneedgiantLLMsandimmensecomput- tokenizerissueofexistingLLMs.
ingpowertoourend. Intheend,wewantto Followingourpreviousworkfordomainadapta-
pointoutthatwehaveonlytrainedParamanu- tion(NiyogiandBhattacharya,2024b),wecontin-
Ganitaonlyonapartofourentiremathematical uedourexplorationtoseewhetherwecandevelop
corpusandyettoexplorethefullpotentialof
strong reasoning mathematical language model
ourmodel.
from scratch and compares how well it performs
1 Introduction with respect to LLMs in mathematical reasoning
benchmarks. Wetrainedapowerfulmathematical
Pretrained Large Language Models (LLMs)
(LLaMa(Touvronetal.,2023a),LLaMa-2,(Tou- 1https://www.databricks.com/blog/mpt-7b
4202
rpA
22
]LC.sc[
1v59341.4042:viXralanguagemodelfromscratchwhichonlyrequired FLOPsUtilization(MFU)metricforpretrain-
146hoursofA100training. Yet,ourmathematical ing. Table 1 shows the validation perplexity
language model, Paramanu-Ganita outperformed andMFUmetricsofpretraining.
LLEMMA7BmathspecialisedmodelonGSM8K
• Wealsobenchmarkedourmathmodelonpop-
(Cobbeetal.,2021)benchmarkbysignificantmar-
ular math benchmark, i.e, GSM8k on CoT
ginof3percentagepointsdespitebeing35times
promptingandcomparedwiththegeneralist
smallerinsize. Onthememoryrequirements,the
LLMsandmathdomainspecialisedLLMs.
LLEMMA7Bcheckpointsizeis13.5GBwhereas
ourmodel,Paramanu-Ganitacheckpointsizeisless
• Ourmodel,Paramanu-Ganita208M,outper-
than1GB.ComparingwithLLEMMA7Btraining,
formed LLaMa-1 (33B, 13B, 7B), LLaMa-
we dropped the requirement of requiring 23,000
2 (7B, 13B), Falcon (40B, 7B) (Almazrouei
A100 hours of continual training to 146 hours of
et al., 2023), PaLM (62B, 8B), MPT (30B,
pretrainingourmathematicallanguagemodelfrom
7B), Vicuna 13B (Chiang et al., 2023), and
scratch.
math-specialised LLMs like Minerva 8B
OurmathmodelisbasedonParamanu(Niyogi
(Lewkowyczetal.,2022),LLEMMA-7Bon
and Bhattacharya, 2024a), released earlier by us.
GSM8kbenchmarkbylargesignificantmar-
We have trained an auto-regressive model from
gindespitebeingsmallerbymultipleorders
scratchatacontextsizeof4096onasingleNVidia
ofmagnitudeinsize.
A100-PCIE-40GB GPU. Our work is an attempt
tomakededicatedmathematicalspecializedmodel 2 Background
fromscratchratherthanperformingcontinualpre-
2.1 LanguageModeling
trainingofexistingLLMsfordomainadaptation.
Our models are much smaller in size by large or- This objective of the language modeling can for-
der of magnitude of LLMs, having only 208 mil- mallydescribedasmaximizingtheprobabilityofa
lionparameters. Hence, ourmodelsareveryfast sequenceoftokensw1,w2,...,w N
ininferencewithoutrequiringanyquantizationof
weights, andourmathmodelcanberunonCPU
n
(cid:89)
withoutneedofGPU. P(w ,w ,...,w ) = P(w |w ,w ,...,w )
1 2 n i 1 2 i−1
Ourmaincontributionsareasfollows: i=1
(1)
where p(w |w ,...w ) is the probability of
• We have curated an exclusive mathematical t 0 t−1
token w given the sequence of previous tokens
pretraining corpus with high quality mathe- t
w ,...,w .
maticaltextincludingtextbooks,lecturenotes, 0 t−1
Theperformanceofalanguagemodelisgener-
webcrawledmathematicaltext,mathematical
allybeingevaluatedusingthetotalcross-entropy
source code from various programming lan-
loss,i.e,thenegativelog-likelihoodoftheobserved
guages,mathematicalArXiVpapers,mathe-
dataunderthemodelunderconsideration, which
matical question answers pairs from forums
foragivendatasetisdefinedas:
like StackExchange, Reddit. We also devel-
oped a math domain specialised tokenizer
n
1 (cid:88)
fromscratch. AvgLoss = − log(P(w |w ,w ,...,w ))
i 1 2 i−1
N
i=1
• We developed first ever exclusive Auto Re- (2)
gressive decoder mathematical model of Lower the loss better is the model but just com-
208 million parameters only from scratch, puting the loss may be not intuitive. Therefore,
Paramanu-Ganita at context size of 4096 re- perplexityisametrictoevaluatetheperformance
spectively on a single GPU. We pretrained ofagivenlanguagemodelwhichistheexponent
only on a part of our curated mathematical oftheaverageloss.
corpusandareyettoexplorethefullpotential
2.2 ModelFlopsUtilization(MFU)
ofthecapabilitiesofourmodel.
ModelFlopsUitilization(MFU)(Korthikantietal.,
• We evaluated our mathematical pretrained 2022)estimateistheratiooftheobservedthrough-
modelsonvalidationperplexity,andonmodel put(tokens-per-second),relativetothetheoreticalmaximumthroughputofasystematpeakFLOPs. soningabilitiesthroughsupervisedfine-tuningand
Model flops utilization (MFU) estimate the num- PPOtraining(Schulmanetal.,2017). MAmmoTH
berofflopswedoperiteration. Itquantifieshow (Yueetal.,2023)integratesCoTandProgram-of-
efficientlytheGPUsareutilizedinmodeltraining. Thought (Chen et al., 2023) rationales to teach
LLMs how to utilize external tools (such as a
3 Data
Pythoninterpreter)forsolvingmathematicalprob-
lems. (Wang et al., 2023a) propose a constraint
We have curated high quality mathematical text
alignment loss for finetuning LLMs to improve
from mathematics text books, lecture notes, web
calibration.
suchasOpenWebMath(Pasteretal.,2023),blogs,
articles, AlgebraStack (Azerbayev et al., 2024),
5 Training
mathematicalquestionanswerspairsfromStack-
Exchange,andmathclassifiedArXivscientificpa- We have pretrained our math model, Paramanu-
pers. We templatised the mathematical question Ganita,fromscratchatacontextsizeof4096ona
answertuplesasCoT(Weietal.,2023)prompt. partofourcuratedcorpus. However,wehaveex-
Thefollowingtemplatewasusedtotemplatise cludedtrainingofourmathmodelonArXivmath
themathematicalquestionanswerspairssuchas papersaswebelievethattolearnbasicmathemat-
”Belowisaninstructionthatdescribesatask. Write ical concepts, and acquire mathematical logical
aresponsethatappropriatelycompletestherequest. reasoning,ArXivmathpapersarenotrequiredas
###Q:{question}###A:Let’sthinkstepbystep. theygenerallymeanttoservebeyondhighschool
{answer}” levelmathematics. Westartedwithsimplestrategy
We also included templatised training set of to use a part of our curated corpus which gener-
GSK8kinthepretrainingdataset. Therefore, our allycoversvariousmathematicalandlogicalcon-
combinedmathematicalcorpusisamixdatasetof cepts till secondary school education in general.
mathematical text, source code of programming Weperformedmix-trainingcombiningbothmath-
languages like TeX, Python, C, Matlab, etc., and ematical plain text, source code of programming
mathematicalquestionanswerstuplesinCoTtem- languages,andtemplatisedmathematicalquestion
platisedformat. answers pairs in the pretraining phase. For pre-
trainingParamanu-Ganita(4096contextsize),we
4 RelatedWork performed 95%-5% data split for pretraining, as
wewantedtousemostofthedatasetforpretrain-
(Weietal.,2023)booststhereasoningcapacityof
ing. We reported the validation perplexity of our
LLMs by supplementing the output with a series
pre-trainedmathematicallanguagemodelintable
of intermediate steps leading to the answer. Sev-
1. Wethenfine-tunedthemathmodelonthetem-
eral approaches have been suggested to enhance
platisedGSM8ktrainingdatasetfor2epochs.
thequalityofthesereasoningpaths. Forinstance,
However,wearealsoworkingontrainingmul-
Complexity-based CoT (Fu et al., 2023) picks
tiple pretrained models from scratch to check
examples with more steps as in-context demon-
whether different combinations of mathematical
strations,demonstratingthatpromptingwithaddi-
books,webcrawledmathematicaltext,ArXivmath
tionalreasoningstepsimprovesperformance. Self-
papers,sourcecodeofrelevantprogramminglan-
Consistency (Wang et al., 2023b) generates mul-
guages,andmathematicalquestionanswerspairs
tiple reasoning paths and selects the final answer
frompopularforumssuchasStackExchange,Red-
throughmajorityvoting. Anothersetoftechniques
dit improves the reasoning ability of our models
involves finetuning-based methods, which adapt
basedonpopularmathbenchmark,GSM8K.
open-sourcemodels(likeLLaMA)usinginsights
fromadvancedclosed-sourceLLMs(GPT-4,and
6 Evaluation
GPT-3.5-Turbo). (Magister et al., 2023) explore
the transfer of reasoning abilities through knowl- Weevaluatethemodel’sabilitytosolvemathemat-
edgedistillation. (Yuanetal.,2023)advocatefor icsproblemsusingchainofthoughtreasoning. Our
the use of rejection sampling finetuning (RFT) evaluations include GSM8k (Cobbe et al., 2021),
to enhance mathematical reasoning performance. the de-facto standard benchmarks for evaluating
WizardMath(Choietal.,2022)introducesarein- quantitativereasoninginlanguagemodels. Were-
forcedevol-instructmethodforstrengtheningrea- ported Pass@1 accuracy of Paramanu-Ganita asModel Perplexity MFU Model Parameters GSM8kPass@1
Paramanu-Ganita(4096) 4.34927 40.39193
LLaMa-1 33B 35.6
LLaMa-1 7B 11.0
Table 1: Perplexity and MFU metrics of Paramanu-
LLaMa-2 13B 28.7
Ganitapretrainedmodels.
LLaMa-2 7B 11.8
CodeLLaMa 7B 10.5
showninthetable2.
CodeLLaMa 34B 29.6
We used the following evaluation prompt for Falcon 40B 19.6
GSM8ktestsetforourmathmodel.
Falcon 7B 6.8
”Below is an instruction that describes a task. MPT 30B 15.2
Writearesponsethatappropriatelycompletesthe
MPT 7B 6.8
request. ###Q:{question}###A:Let’sthinkstep
GPT-J 6B 34.9
bystep. ”
Vicuna 13B 27.6
Table2alsoreportsthevariousLLMsandtheir
PaLM 8B 4.1
reportedscoresquotedfromtherespectivepublica-
PaLM 62B 33.0
tions. Paramanu-Ganita despite being 35 times
Minerva 8B 16.2
smaller than 7B LLMs, outperformed LLaMa-
Minerva 62B 52.4
1 7B by 28.4% points, LLaMa-2 7B by 27.6%
Minerva 540B 58.8
points, Falcon 7B by 32.6% points, PaLM 8B
LLEMMA 7B 36.4
by 35.3% points, Minerva 8B by 23.2% points,
LLEMMA 34B 51.5
and LLEMMA-7B by 3% points respectively.
Paramanu-Ganita 208M 39.4
Paramanu-Ganita also outperformed PaLM 62B
by6.4%pointsdespitebeingsmallerby305times,
Table 2: Evaluation of LLMs on GSM8k test set.
Falcon40Bby19.8%points(smallerby197times),
PaLM, LLaMa-1 (Touvron et al., 2023a), LLaMa-2
LLaMa-1 33B by 3.8% points (smaller by 162 (Touvron et al., 2023b), Falcon (Almazrouei et al.,
times) and Vicuna 13B by 11.8 % points respec- 2023), Code Llama (Rozière et al., 2024), MPT, Vi-
tivelydespitebeingsmallerby64timesinmodel cuna(Chiangetal.,2023),Minerva(Lewkowyczetal.,
2022)(Lewkowyczetal.,2022)scoresarequotedfrom
parameters. LLEMMA 34B, Minerva 62B, Min-
respectiveauthorpapers.
erva540BarethegiantLLMSthatperformedbet-
terthanParamanu-GanitaonGSM8kbenchmark.
However,aswehaveonlytrainedourmathmodel
points,mathspecialisedLLMssuchasMinerva8B
onapartofourentirecorpus,sowehopeitsnota
by23.2%points,andLLEMMA7Bby3%points
faircomparisontotestthefullpotentialofourmath
inaccuracymetricrespectively. Paramanu-Ganita
model,wealsodidnotperformDPOorPPOtrain-
alsooutperformedPaLM62Bby6.4%points,Fal-
ingtoimprovetheperformanceofourParamanu-
con40Bby19.8%points,LLaMa-133Bby3.8%
Ganita208Mcomparedtoothermathspecialised
pointsandVicuna13Bby11.8%pointsintestac-
LLMs.
curacymetricofGSM8krespectivelydespitejust
having208millionparametersonly. However,we
7 Conclusions
havenottrainedourmodelontheentiremathemat-
In this paper, we presented an exclusive mathe- ical corpus so we have not yet explored the full
maticalAutoRegressiveDecoderbasedlanguage potential of our model. We are currently work-
model, Paramanu-Ganita 208M, pretrained from ingonextensivestudytotrainmultiplepretrained
scratch on a part of our entire mathematical cor- mathematical language models from scratch and
pus for a context size of 4096. We evaluated our ofvarioussizes,somewhereinthesamerange,to
mathematicalmodelonvalidationperplexityand explorethedifferentcombinationsofmathematical
benchmarkedourmodelonpopularGSM8Kmath books,webcrawledmathematicaltext,ArXivmath
benchmark. We found that Paramanu-Ganita de- papers,sourcecodeofrelevantprogramminglan-
spite being 35 times smaller than 7B LLMs, out- guages,andmathematicalquestionanswerspairs
performedgeneralistLLMssuchasLLaMa-17B frompopularforumssuchasStackExchange,Red-
by 28.4% points, LLaMa-2 7B by 27.6% points, dit to judge the full potential of our models and
Falcon 7B by 32.6% points, PaLM 8B by 35.3% checkhowfarthereasoningabilityofourcurrentmodel can be improved based on popular math AitorLewkowycz,AndersAndreassen,DavidDohan,
benchmark,GSM8Kandwhetheritperformsbet- EthanDyer,HenrykMichalewski,VinayRamasesh,
Ambrose Slone, Cem Anil, Imanol Schlag, Theo
terthanthestate-of-the-artLLMonGSM8kbench-
Gutman-Solo,YuhuaiWu,BehnamNeyshabur,Guy
markdespitejustbeingof208millionparameters
Gur-Ari,andVedantMisra.2022. Solvingquantita-
insize. tivereasoningproblemswithlanguagemodels.
Lucie Charlotte Magister, Jonathan Mallinson, Jakub
References Adamek, Eric Malmi, and Aliaksei Severyn. 2023.
Teachingsmalllanguagemodelstoreason. InPro-
EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAl- ceedingsofthe61stAnnualMeetingoftheAssocia-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru, tionforComputationalLinguistics(Volume2: Short
MérouaneDebbah,ÉtienneGoffinet,DanielHesslow, Papers),pages1773–1781,Toronto,Canada.Associ-
JulienLaunay,QuentinMalartic,DanieleMazzotta, ationforComputationalLinguistics.
BadreddineNoune,BaptistePannier,andGuilherme
Penedo.2023. Thefalconseriesofopenlanguage MitodruNiyogiandArnabBhattacharya.2024a. Para-
models. manu: A family ofnovelefficient indic generative
foundationlanguagemodels.
ZhangirAzerbayev,HaileySchoelkopf,KeiranPaster,
Marco Dos Santos, Stephen McAleer, Albert Q. Mitodru Niyogi and Arnab Bhattacharya. 2024b.
Jiang,JiaDeng,StellaBiderman,andSeanWelleck. Paramanu-ayn: An efficient novel generative and
2024. Llemma: Anopenlanguagemodelformathe- instruction-tuned language model for indian legal
matics. casedocuments.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and LongOuyang,JeffWu,XuJiang,DiogoAlmeida,Car-
William W. Cohen. 2023. Program of thoughts rollL.Wainwright,PamelaMishkin,ChongZhang,
prompting: Disentanglingcomputationfromreason- SandhiniAgarwal,KatarinaSlama,AlexRay,John
ingfornumericalreasoningtasks. Schulman,JacobHilton,FraserKelton,LukeMiller,
Maddie Simens, Amanda Askell, Peter Welinder,
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
ZhanghaoWu,HaoZhang,LianminZheng,Siyuan Traininglanguagemodelstofollowinstructionswith
Zhuang,YonghaoZhuang,JosephE.Gonzalez,Ion humanfeedback.
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
sourcechatbotimpressinggpt-4with90%*chatgpt KeiranPaster,MarcoDosSantos,ZhangirAzerbayev,
quality. and Jimmy Ba. 2023. Openwebmath: An open
datasetofhigh-qualitymathematicalwebtext.
JasonIngyuChoi,SaarKuzi,NikhitaVedula,JieZhao,
GiuseppeCastellucci,MarcusCollins,ShervinMal- BaptisteRozière,JonasGehring,FabianGloeckle,Sten
masi,OlegRokhlenko,andEugeneAgichtein.2022. Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Wizardoftasks: Anovelconversationaldatasetfor JingyuLiu,RomainSauvestre,TalRemez,Jérémy
solving real-world tasks in conversational settings. Rapin,ArtyomKozhevnikov,IvanEvtimov,Joanna
InProceedingsofthe29thInternationalConference Bitton,ManishBhatt,CristianCantonFerrer,Aaron
on Computational Linguistics, pages 3514–3529, Grattafiori, Wenhan Xiong, Alexandre Défossez,
Gyeongju, Republic of Korea. International Com- JadeCopet,FaisalAzhar,HugoTouvron,LouisMar-
mitteeonComputationalLinguistics. tin,NicolasUsunier,ThomasScialom,andGabriel
Synnaeve.2024. Codellama: Openfoundationmod-
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, elsforcode.
MarkChen,HeewooJun,LukaszKaiser,Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro JohnSchulman,FilipWolski,PrafullaDhariwal,Alec
Nakano, Christopher Hesse, and John Schulman. Radford,andOlegKlimov.2017. Proximalpolicy
2021. Training verifiers to solve math word prob- optimizationalgorithms.
lems.
Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,
Aakanksha Chowdhery et al. 2022. PaLM: Scaling ElleryWulczyn,LeHou,KevinClark,StephenPfohl,
languagemodelingwithpathways. HeatherCole-Lewis,DarleneNeal,MikeSchaeker-
mann,AmyWang,MohamedAmin,SamiLachgar,
YaoFu,HaoPeng,AshishSabharwal,PeterClark,and PhilipMansfield, SushantPrakash, BradleyGreen,
TusharKhot.2023. Complexity-basedpromptingfor Ewa Dominowska, Blaise Aguera y Arcas, Nenad
multi-stepreasoning. Tomasev,YunLiu,ReneeWong,ChristopherSem-
turs,S.SaraMahdavi,JoelleBarral,DaleWebster,
Vijay Korthikanti, Jared Casper, Sangkug Lym, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi,
LawrenceMcAfee,MichaelAndersch,Mohammad AlanKarthikesalingam,andVivekNatarajan.2023.
Shoeybi,andBryanCatanzaro.2022. Reducingacti- Towards expert-level medical question answering
vationrecomputationinlargetransformermodels. withlargelanguagemodels.HugoTouvron,ThibautLavril,GautierIzacard,Xavier
Martinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,Faisal
Azhar,AurelienRodriguez,ArmandJoulin,Edouard
Grave,andGuillaumeLample.2023a. Llama: Open
andefficientfoundationlanguagemodels.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale,DanBikel,LukasBlecher,CristianCanton
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
CynthiaGao,VedanujGoswami,NamanGoyal,An-
thonyHartshorn,SagharHosseini,RuiHou,Hakan
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez,RobertStojnic,SergeyEdunov,andThomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tunedchatmodels.
PeiyiWang,LeiLi,LiangChen,FeifanSong,Binghuai
Lin,YunboCao,TianyuLiu,andZhifangSui.2023a.
Makinglargelanguagemodelsbetterreasonerswith
alignment.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc
Le,EdChi,SharanNarang,AakankshaChowdhery,
andDennyZhou.2023b. Self-consistencyimproves
chainofthoughtreasoninginlanguagemodels.
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Bosma,BrianIchter,FeiXia,EdChi,QuocLe,and
DennyZhou.2023. Chain-of-thoughtpromptingelic-
itsreasoninginlargelanguagemodels.
ShijieWu,OzanIrsoy,StevenLu,VadimDabravolski,
MarkDredze,SebastianGehrmann,PrabhanjanKam-
badur, David Rosenberg, and Gideon Mann. 2023.
Bloomberggpt: Alargelanguagemodelforfinance.
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting
Dong,KemingLu,ChuanqiTan,ChangZhou,and
JingrenZhou.2023. Scalingrelationshiponlearning
mathematicalreasoningwithlargelanguagemodels.
XiangYue,XingweiQu,GeZhang,YaoFu,Wenhao
Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023.
Mammoth: Buildingmathgeneralistmodelsthrough
hybridinstructiontuning.
Acknowledgements
Thefirstauthorwantstodedicatehisworktohis
belovedparents,RitaNiyogiandMalayNiyogifor
theiroutstandingsupportthroughouthisjourney.