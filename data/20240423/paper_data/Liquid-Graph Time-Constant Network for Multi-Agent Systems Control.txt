Liquid-Graph Time-Constant Network
for Multi-Agent Systems Control
Antonio Marino1, Claudio Pacchierotti2, Paolo Robuffo Giordano2
Abstract—Inthispaper,weproposetheLiquid-GraphTime- of the use of GNNs for distributed control are found in
constant (LGTC) network, a continuous graph neural network spacecoverage[8],multi-robotpathplanning[9],andmotion
(GNN) model for control of multi-agent systems based on the
planning [10], including obstacle-rich environments [11].
recent Liquid Time Constant (LTC) network. We analyse its
GNNs also enhance multi-agent perception [12] and enable
stability leveraging contraction analysis and propose a closed-
formmodelthatpreservesthemodelcontractionrateanddoes distributed active information acquisition [13], translating
not require solving an ODE at each iteration. Compared to multi-robot information gathering into graph representation
discrete models like Graph Gated Neural Networks (GGNNs), and formulating GNN-based decision-making.
the higher expressivity of the proposed model guarantees
remarkable performance while reducing the large amount Intherecentliterature,oneoftheobjectivesistomakethe
of communicated variables normally required by GNNs. We learning-basedmethodrobustandstable[14].Inthiscontext,
evaluate our model on a distributed multi-agent control case many works applied contraction analysis to demonstrate
study (flocking) taking into account variable communication
recurrent neural network stability [15], or directly closed-
rangeandscalabilityundernon-instantaneouscommunication.
loop stability in continuous learning [16] and adaptive con-
Index Terms—Distributed Control, Graph Neural Network, trol [17]. Recently, the stability analyses presented in [18],
Stability Analysis [19]showcasedtheconceptsofISS(Input-to-StateStability)
and incremental ISS (δISS) [20], [21] in the context of
I. INTRODUCTION LSTMs and GRUs, which are two of the most popular
Inrecentyears,theinterestinmulti-agentsystems(MAS) recurrent neural network models. Inspired by these last
has significantly increased [1]. The effective coordination results, we proposed [22] the conditions for δISS of the
of multiple agents necessitates an understanding of group recurrentversionofGNN,i.e.GatedGraphNeuralNetworks
dynamics. Particularly in multi-robot scenarios, generating (GGNN) [23].
individual robot motions relies not only on local sensing but In this study, we introduce a continuous graph ordinary
also on information about the group’s state, often obtained differential equation (ODE) network called Liquid-Graph
throughcommunicationwithalimitedsubsetofnearbyteam Time-Constant(LGTC)network,inspiredbyitssingle-agent
members [2]. As a result, communication becomes a crucial counterpart, the Liquid Time Constant (LTC) network [24].
element in achieving distributed solutions for MAS. TheLTCnetworkdemonstratesastrongrepresentationlearn-
Neural networks have become prominent in data-driven ing capability, empowering agents to extrapolate and make
control applications due to their outstanding approximation inferences even in unfamiliar scenarios. Additionally, its
capabilities [3]. In distributed control, neural networks offer closed-form version [25] eliminates the need to solve the
particular advantages by efficiently approximating complex initial value problem of the ODE, a requirement in other
distributed policies without the need for intricate optimiza- ODE networks [26]–[28]. In the existing literature, there are
tions and designs. The literature presents numerous data- few attempts to make graph ODEs for modelling dynamic
drivenapproaches,withafocusonthoserootedinreinforce- network systems [29], predicting traffic flow [30], or for
mentlearning[4],[5].However,theseapproachesoperateon sequential recommendations [31]. These models typically
local sensing data without inter-agent communication. On employ an auto-regressive network over a vector field mod-
the other hand, distributed machine learning employs com- elled by a Graph Neural Network (GNN), initializing the
munication to partition the learning process across multiple internal state with the previous layer’s output. In contrast,
machines contributing to group knowledge [6]. the proposed LGTC aims to introduce a novel model that
Leveraging communication, recent trends in data-driven learns a dynamic (or “liquid”) time constant, influenced by
distributed control involve employing Graph Neural Net- both input data and the internal state communicated over
works (GNNs) to encode distributed control and planning the agent graph. This dynamic time constant enables each
solutions. In this respect, Gama et al. [7] extend GNNs element of the state to capture specific dynamics, thereby
to flocking control for large teams. Additional examples enhancing the model’s predictive capabilities.
1A.MarinoiswithUnivRennes,CNRS,Inria,IRISA–Rennes,France. In addition, our contribution includes proposing an ap-
E-mail:antonio.marino@irisa.fr. proximatedclosed-formfortheLGTCsystem,enhancingthe
2C.PacchierottiandP.RobuffoGiordanoarewithCNRS,UnivRennes, computational efficiency and reducing the communication
Inria,IRISA–Rennes,France.E-mail:{claudio.pacchierotti,prg}@irisa.fr.
load. We proceed to evaluate the proposed models alongside
ThisworkwassupportedbytheANR-20-CHIA-0017project“MULTI-
SHARED”. GGNN and GraphODE in a flocking control example.
4202
rpA
22
]AM.sc[
1v28931.4042:viXraII. PRELIMINARIES exchangesintermediatequantitieswiththeotheragentsinthe
Let G = (V, E) be an undirected graph where V = network through the application of S, resulting in an overall
{v ,...,v } is the vertex set (representing the N agents distributed neural network. GNNs inherit some interesting
1 N
in the group) and E ⊆ V ×V is the edge set. Each edge properties from graph filters, such as permutational equiv-
e = (i, j) ∈ E is associated with a weight w ≥ 0 such ariance [33] and their local and distributed nature, showing
k ij
that w > 0 if the agent i and j can interact and w = 0 superior ability to process graph signals [9], [13], [34].
ij ij
otherwise.Asusual,wedenotewithN ={j ∈V|w >0}
i ij B. Gated Graph Neural Network
the set of neighbors of agent i. We also let A ∈ RN×N
Recurrent models of GNNs can solve time-dependent
be the adjacency matrix with entries given by the weights
problems. These models, similarly to recurrent neural net-
w . Defining the degree matrix D = diag(d ) with d =
(cid:80)ij i i works(RNNs),areknownasgraphrecurrentneuralnetworks
w ,theLaplacianmatrixofthegraphisL=D−A.
Tj∈ hN ei gri aj ph signal x ∈ RN, whose i-component x is (GRNNs). GRNNs utilize memory to learn patterns in data
i sequences,wherethedataisspatiallyencodedwithingraphs,
assigned to agent i, can be processed over the network by
regardless of the team size of the agents [35]. However,
the following linear combination rule applied by each agent
traditional GRNNs encounter challenges such as vanishing
(cid:88)
s x= s (x −x ), (1) gradients, which are also found in RNNs. Additionally, they
i ji i j
j∈Ni face difficulties in handling long sequences in space, where
where s is the i-th row of S. The signal manipulation can certain nodes or paths within the graph might be assigned
i
be operated by means of any graph shift operator S ⊆ more importance than others in long-range exchanges, caus-
S, e.g., Laplacian, adjacency matrix, weighted Laplacian , ing imbalances in the graph’s informational encoding.
which respects the sparsity pattern of the graph. Later, in Forgetting factors can be applied to mitigate this problem,
Sect. V, we will use the Laplacian as support matrix, as it is reducing the influence of past or new signal on the state. A
commonlyusedindistributedcontrol.However,theproposed Gated Graph Neural Network (GGNN) [23] is a recurrent
techniquesdonotassumetheuseofaspecificsupportmatrix. Graph Neural Network that uses a gating mechanism to
Performing k repeated applications of S on the same controlhowthepastinformationinfluencestheupdateofthe
signalrepresentstheaggregationofthek-hopneighbourhood GNN states. We can add a state and an input gate, qˆ,q˜∈
information. In analogy with traditional signal processing, Q ⊆ [0,1]N×F, that are multiplied via the Hadamard prod-
this property can be used to define a linear graph filter- uct◦bythestateandtheinputsofthenetwork,respectively.
ing [32] that processes the multi features signal x∈RN×G These two gates regulate how much the past information
with G features: and the input are used to update the network’s internal state.
GGNNs admit the following state-space representation [36],
K
H S(x)= k(cid:88) =0SkxH k (2)  q q˜ ˆ= =σ σ( (A A˜ ˆS( (x x) )+ +B B˜ ˆS( (u u) )+ +ˆ ˜b b)
) (4)
S S
where the weights H k ∈ RG×F define the output of the x+ =σ c(qˆ◦A S(x)+q˜◦B S(u)+b)
filter. Note that Sk = S(Sk−1), so that it can be com-
puted locally with repeated 1-hop communications between withσ(x)= 1+1
e−x
beingthelogisticfunction,andσ c(x)=
a node and its neighbors. Hence, the computation of H is ex−e−x being the hyperbolic tangent. Aˆ ,Bˆ are graph
S ex+e−x S S
distributed on each node. filters of the forgetting gate, A˜ ,B˜ are graph filters (2) of
S S
theinputgate,andA andB arethestategraphfilters(2).
A. Graph Neural Network S S
ˆb,˜b,b ∈ RN×F are respectively the biases of the gates and
Although H is simple to evaluate, it can only represent
S thestatebuiltas1 ⊗bwiththesamebiasforeveryagents.
N
a linear mapping between input and output filters. GNNs
We consider the system under the following assumption
increase the expressiveness of the linear graph filters by
means of pointwise nonlinearities ρ:RN×Fl−1 →RN×Fl−1 Assumption 1. The input u is unity-bounded: u ∈ U ⊆
following a filter bank. Letting H Sl be a bank of F l−1×F l [−1,1]N×G , i.e. ||u|| ∞ ≤1.
filters at layer l, the GNN layer is defined as
We identify the induced ∞-norm as ||·|| . We used the
∞
x
l
=ρ(H Sl(x l−1)), x
l−1
∈RN×Fl−1. (3) following notation for the filters in (4):
Starting by l = 0 with F 0, the signal tensor x ln ∈ RN×Fln S I,K ≜[I,S,...,SK]
is the output of a cascade of l GNN layers. This specific A ≜[A ,...,A ]T B ≜[B ,...,B ]T
n 0,K 0 K 0,K 0 K
(5)
type of GNN is commonly referred to as a convolution A˜ ≜[A˜ ,...,A˜ ]T Aˆ ≜[Aˆ ,...,Aˆ ]T
0,K 0 K 0,K 0 K
graph network because each layer utilizes a graph signal B˜ ≜[B˜ ,...,B˜ ]T Bˆ ≜[Bˆ ,...,Bˆ ]T
0,K 0 K 0,K 0 K
convolution (2). By the use of imitation learning or similar
whereK isthefilterslength.Then,inlightofassumption(1)
techniques,theGNNcanlearnadistributedpolicybyfinding
the optimal filter weights H to propagate information and knowing that ||x|| ∞ ≤1, each gate feature q i satisfies:
k
among the agents and generate the desired output. Notably, |qˆ|≤σ(||S || (||Aˆ || +||Bˆ || )+||bˆ|| )≜σ .
i 0,K ∞ 0,K ∞ 0,K ∞ ∞ qˆ
each agent employs an identical version of the network and (6)Assumption 2. Given any two support matrices ||S (t)|| We named the vector field in eq. (8) with F(x,u,S,t) :
1 ∞
a gn rad ph|| sS
,2
th( et) y||
a∞
re, b∀ ot un∈ dedR b+ ytha ess so ac mia ete ||d S¯|w |ith ;mt ow ro eod veif rf ,e tr he en yt X Gi× viU ng× tS he×R
in≥ d0
uc→ edR iN n× fiF nita end loi gts -nja oc rmobi da en fiw nii tt ih onD
x
µF = (X∂
∂
)F x|| =::.
∞ ∞
are lower bounded by ||S˜|| ∞. max i(x
ii
+ (cid:80)n j=1,j̸=1|x ij|) [37], the followings results are
known [15]:
Inourpreviouswork[22],wederivedthefollowingstability
condition
DefinitionIII.1. ThevectorfieldF isstronglyinfinitesimally
Theorem 1. Under Assumptions 1 and 2, a sufficient con-
dition for the system (4) to be δISS is A ≤1; where contractivng if:
δ
A ≜σ ||S¯ || ||A || + 1 ||S¯ ||2 ||Aˆ || ||A || µ ∞(D xF)<−c (9)
δ qˆ 0,K ∞ 0,K ∞ 4 0,K ∞ 0,K ∞ 0,K ∞
1
+ ||S¯ ||2 ||A˜ || ||B || . With c>0 known as contraction rate.
4 0,K ∞ 0,K ∞ 0,K ∞
(7)
Remark 1. In practice, Assumption 2 is met by limiting the Definition III.2. For a vector map F satisfying III.1 and
cardinality of #N < N for every agent i in the team. For given l , l the Lipschitz constants of F in the input u
i u S
normalized Laplacian, the assumption is met without any and S, any two solutions x and x with initial conditions
1 2
further restrictions on the graph topology. x (0), x (0) and inputs u (t), u (t) satisfy the δISS
1 2 1 2
relationship
III. LIQUID-GRAPHTIMECONSTANTNETWORK
In light of the recent findings on the continuous time ||x (t)−x (t)|| ≤e−ct||x (0)−x (0)||
1 2 ∞ 1 2 ∞
neural ODE, we want to propose a novel continuous-time
l
graph neural network: Liquid-Graph Time-Constant (LGTC) + cu(1−e−ct)sup τ∈[0,t]||u 1(τ)−u 2(τ)|| ∞
network. Inspired by the LTC network proposed in [24], the l
+ S(1−e−ct)sup ||S (τ)−S (τ)||
LGTC is described by the following ODE: c τ∈[0,t] 1 2 ∞
(10)
(cid:40)
f =ρ(Aˆ (x)+b )+ρ(Bˆ (u)+b )
x˙ =−(bS +f)◦xx −(cid:80)K k=1S SkxA k.u +f ◦σ c(B S(u)) (8) For the neural network 8, we provide the following theorem
where ρ(x) = ReLU(x). Like in GGNN system,
Aˆ S,Bˆ S,A S,B S aregraphfilters(2)andb x,b u,b∈RN×F are Theorem 2. Under Assumption 2, with x(0) ∈ X, sys-
biases of the model. This model disposes of an input-state- tem (8) is δISS if the following constraints are satisfied
dependentvaryingtime-constantallowingsingleelementsof
thehiddenstatetoidentifyspecializeddynamicalsystemsfor K
input features arriving at each time step. c≥0; b≥0; µ ∞((cid:88) AT k ⊗Sk)≥0 (11)
In the following results on the system, we used the vector k=1
operator X| that rearranges the elements of matrix X in a
:
vector. For the system in (8), the following lemma holds with c=||b|| +||A || ||S˜ ||
∞ 1,K ∞ 1,K ∞
. (12)
Lemma 1. If b i >0 and AT k ⊗Sk ≥0, for k =[1,...,K], +||b x|| ∞−||AˆT 0,K|| ∞||S¯ 0,K|| ∞
the state feature i at time t is bounded in the range [−1,1]
if x(0)∈X ⊆[−1,1]N×F . Proof. Let be σ = σ (B (u)) ⊆ [−1,1]N×F, the log-norm
u c S
of the jacobian µ (D F) has its superior in:
Proof. Defining the following Lypunov function ∞ x
1
V = 2x|T
:
x|
:
su xpµ ∞(D xF)=su xpµ ∞(−diag(b| :)−diag(f| :)
its time derivative along the system trajectories is −(cid:88)K
AT ⊗Sk+diag(σ |
−x|)diag(Df)(cid:88)K
Aˆ T ⊗Sk)
k u : : k
V˙ =−x|T : diag(b| :)x| :−x|T : diag(f| :)x| : k=1 k=0
−x|T :
(cid:88)K
AT k ⊗Skx| :+x|T : diag(f| :)σ c(B S(u))| : ≤−||b|| ∞+µ
∞(−(cid:88)K
AT k ⊗Sk)+sup−||f| :|| ∞
x
k=1 k=1
By imposing b i ≥ 0 and (cid:80)K k=1AT k ⊗ Sk ≥ 0 for k = +µ ∞(diag(σ u| :−x| :)diag(Df)(cid:88)K AˆT k ⊗Sk
[1,...,K], we have
k=0
V˙ <−x|T diag(f|)x| +x|T diag(f|)σ (B (u))|
: : : : : c S : where D f ∈ [0,1]NF is the derivative of f in x|. The last
x :
When x > 1, V˙ < 0, because the term inequality follows by µ (X)=||X|| when X >0 and by
i i ∞ ∞
x|T diag(f|)σ (B (u))| ≤1T diag(f|)1 and reminding that the fact that b| ≥0 and f| ≥0. Moreover, we can note that
: : c S : : : :
f >0. The proof is completed by noting that V˙ <0 when −||f||| ≤ −||ρ(Aˆ (x)+b )||| . Giving σ | ∈ [−1,1]NF
i i : ∞ S x : ∞ u :
x <−1 with a similar reasoning. and −µ (AT ⊗ Sk) ≤ 0, by lemma 1, x| ∈ [−1,1]NF.
i ∞ k :Therefore, we get the additional advantage of reducing the vanishing gradient
problem due to the ODE resolution. We modified this solver
K
µ (D F)≤−||b|| −||(cid:88) AT ⊗Sk|| − to handle our network as follows:
∞ x ∞ k ∞
k=1 x(t)+∆(−(cid:80)K Skx(t)A +f ◦σ (B (u))
K x(t+∆)= k=1 k c S .
||ρ(Aˆ (x+b ))|| + max 2µ (diag(d)(cid:88) AˆT ⊗Sk) 1+∆(b+f)
S x ∞ ∞ k (14)
d∈[−1,1]NF
k=0 With ∆ = T/n being a fixed step size for n evaluations
K
≤−||b|| −||(cid:88) AT ⊗Sk|| −||ρ(Aˆ (x+b ))|| + of(14),startingfromt=0untilreachingt=T.Thesupport
∞ k ∞ S x ∞
matrix S and the input u are fixed in the simulation interval
k=1
K K [0,T]. Each iteration of (14) requires to communicate again
2max(µ ∞(−(cid:88) AˆT k ⊗Sk),µ ∞((cid:88) AˆT k ⊗Sk)) the network state, e.g. x(t+∆) to evaluate x(t+2∆).
k=0 k=0 Therefore, solving the ODE nullifies the reduction in the
Tofurtherreducethepreviousinequality,wecanusethesub communicated variables for the high expressivity of the
multiplicitypropertyoftheinfinitenormandthefactthatthe model. For this reason, we provide in the next section a
last term in the previous inequality is ≤||(cid:80)K AˆT ⊗Sk|| closed-formsolutionforLGTCthatdoesnotrequireanODE
k=1 k ∞
to derive solver, thus significantly gaining in computation time and
µ (D F)≤−||b|| −||A || ||S˜ || communication load.
∞ x ∞ 1,K ∞ 1,K ∞
−||b x|| ∞+||AˆT 0,K|| ∞||S¯ 0,K|| ∞ =−c IV. CLOSED-FORMAPPROXIMATION
From the statement in the Theorem (2), the system is Byfixingtheinputandsupportmatrixbetweentheinterval
contractive under the defintion III.1 and δISS under the t = [0,T], the ODE solution should satisfy the following
definition III.2 with integral:
l u =(2||Bˆ 0,K|| ∞+(||Aˆ 0,K|| ∞||S¯ 0,K|| ∞+||Bˆ 0,K|| ∞||S¯ 0,K|| ∞ (cid:90) T
x(T)| =x(0)| + F(x(τ),S(0),u(0))dτ (15)
+||b || +||b || )||B || )||S¯ || : :
x ∞ u ∞ 0,K ∞ 0,K ∞ 0
(cid:32) (cid:33)
k+1 However, solving this integral is not straightforward due to
l = (((||Aˆ || ||S¯ || +||Bˆ || ||S¯ ||
S 2 0,K ∞ 0,K ∞ 0,K ∞ 0,K ∞ the non-linearity in F. We aim to find an approximate form
+||b || +||b || )||B || +2||Aˆ || of(15)thatkeepsthesamedynamicpropertiesoftheoriginal
x ∞ u ∞ 1,K−1 ∞ 1,K−1 ∞
system. Given the following quantities
+2||Bˆ || )||S¯ || −||S˜ || ||A || )
1,K−1 ∞ 1,K−1 ∞ 1,K−1 ∞ 1,K−1 ∞
(13) K K
f =−D fdiag(x(T))(cid:88) AˆT ⊗S(0)k+(cid:88) AT ⊗S(0)k
x k k
Weomittedthederivationofl andl whichcanbefoundby
u S k=0 k=1
computing sup u||D uF|| ∞,sup S||D sF|| ∞, respectively. f
x
=ρ(Aˆ S(x(T))+b x)
ThehigherexpressivityoftheLTCmodelhasbeenproved f =ρ(Bˆ (u(0))+b )+ρ(Aˆ (x(0))+b )
σ S u S x
in[24]viaprincipalcomponentanalysisontrajectorydepth.
we propose the following closed-form approximation
The results of this analysis showed that LTC performs at
worst like a discrete RNN model over short trajectories, but x(T)| : =e−(diag(b|:)+diag(fx|:)+f)T diag(σ(2f σ)| :)x(0)| : (16)
itsexpressivepowerdoesnotdecreaseonlongertrajectories. +diag(1−σ(2f )|)σ |.
σ : u :
We can derive the same conclusion for LGTC, which shares
Reminding that σ = σ (B (u(0))), one can easily demon-
u c S
with LTC the main components but over a network of
strate that x(T) ∈ X since its expression is a convex
agents. This higher expressivity allows us to reduce the
combination with coefficient σ(2f ) and 1 − σ(2f ). This
σ σ
communicated state variables. Therefore, we can select a
closed-form solution maintains the contraction rate as stated
subset of state features F′ << F and a subset of input
by the following theorem.
features G′ << G to communicate. Denoting by x F′ the
features to communicate and by x F−F′ the features to not Theorem 3. With t = [0,T], the state trajectory (16)
communicate, we can encode the reduced communication in is a closed-form approximation of the dynamic system (8)
the equations above by changing the filter bank in eq (2) as with the same contraction rate c under the constraints of
(cid:80)K k=0[I Nx F−F′,Skx F′]H
k
with I
N
the identity matrix of Theorem 2.
dimension N. With these reduced graph filters, Theorem 2
Proof. We start the proof by highlighting the relationship
still holds true.
between the superior of the induced log-norm and the
To compute the state of the system in eq. (8) at any time
superior of the time derivative of the induced norm of the
pointT,theneuralnetworkmustimplementandODEsolver,
matrix exponential
like Runga-Kutta (RK), that simulates the system starting
∂ (cid:12)
from a trajectory x(0), to x(T). When used in a prediction supµ (D F)= sup||exp(D Ft)|| (cid:12) (17)
framework, the neural layer (8) will forward the state x(T) x ∞ x ∂t x x ∞(cid:12) t=0+
to the next layers. Since the system is stiff, the authors Therefore, we can show that the equivalence between the
in [24] proposed an hybrid solver for LTC networks that has right-hand side of the previous relation and the inducednorm of the jacobian (D xx(T)) of (16). Let’s name F
ex
the
argumentoftheexponentialinthe(16).||D x(t)|| satisfies
x ∞
the following :
||D x(t)||| =||eFexdiag(σ(2f )|)+
x : ∞ σ :
K
diag(x|)eFex(diag(σ(2f )|)(cid:88) AˆT ⊗Skt+
: σ : k
k=0
K
diag(x|)eFex2diag(D |)(cid:88) AˆT ⊗Sk−
: σ : k
k=0
K
diag(σ )2diag(D |)(cid:88) AˆT ⊗Sk|| Fig. 1: Flocking control: a group of agents (yellow dots)
u σ : k ∞
move in order to reach the same velocity and to avoid
k=0
K collision. The leader (red dot) moves in order to reach the
≤||eFex|| ∞+||eFex(cid:88) AˆT k ⊗Skt|| ∞+ target (blue cross) and avoids the collision with the other
k=0 agents.
K
0.5(||eFex|| +1)||(cid:88) AˆT ⊗Sk||
∞ k ∞
k=0
K K they have approximately the same dynamic properties. We
≤||eFex|| ∞+||eFex(cid:88) AˆT
k
⊗Skt|| ∞+||(cid:88) AˆT
k
⊗Sk||
∞ demonstratethisgoodapproximationintheresultSectionV.
k=0 k=0
(18)
V. VALIDATIONEXAMPLE
where the last inequalities come from reasoning similar to
the proof in the theorem 2 and the Lipschitz constant of Weexperimentallyvalidatedtheneuralnetworksproposed
σ(2x)equalto0.5.Thefollowingtimederivativeprovesthe in a flocking control example. Despite its simplicity, this
equality (17) scenario has been previously studied [38] using different
approaches such as GNN, LSTM, allowing for a direct
∂∂ tsup||D xx(t)| :|| ∞(cid:12) (cid:12)
t=0+
=−||b|| ∞−||A 1,K|| ∞||S˜ 1,K|| ∞− comparison with the proposed model. We also explicitly
x compare GraphODE [29], GGNN, LGTC and CfGC, satis-
||b x|| ∞+||AˆT 0,K|| ∞||S¯ 0,K|| ∞ fyingtheconditionsinTheorem1,2and3respectively.The
(19)
stabilityconditionisimposedbythefollowingregularization
Moreover, one can easily demonstrate that the Lipschitz in addition to the training loss:
constant in uand S is upper bounded by l ,l respectively,
u S (cid:88)
taking into account that te−(F)t;F > 0 can be upper Π= Softplus(p i) (21)
bounded by 1. i
With p having only one element equal to δA −1 to satisfy
With a fixed T, the state trajectory (16) can serve as a i
theTheorem1andequalto[−c,−b,−µ ((cid:80)K AT⊗Sk)]
discrete-time system that in training and evaluation requires ∞ k=1 k
for Theorem 2 and 3. The Softplus is introduced to have
one iteration/communication to be evaluated, against the
a smooth ReLU function that can be regulated through its
n iteration required by the continuous system. However,
β to enforce a higher contraction rate and consequently fast
equation (16) can not be directly distributed over the agents
convergence. We used β =10.
duetothekronackerproducttermsappearinginf.Moreover,
In the following, we show a case study involving flocking
notably, the exponential term in recursion causes vanishing
control (Fig. 1) with a leader. In this problem, the agents
gradient problems in training. Therefore, we modified the
are initialized to follow random velocities while the goal
trajectory (16) to tackle these two issues. First, we multiply
the kronacker product terms by x/(x + ϵ), with / being is to have them all travelling at the same velocity while
the division element by element and a small ϵ. Second, we avoiding collisions with each other. Moreover, one of the
agents takes the leader role, conducting the team toward a
replace the exponential term with a sigmoidal nonlinearity
target unknown to the other agents. Flocking is a canonical
which decreases much more smoothly. The resulting closed-
problem in decentralized robotics [39], [40].
form graph time constant (CfGC) system is:
We considered N agents described by the position r(t)∈
 ff xσ == ρρ (( AB ˆˆ SS (( xu )) ++ bb xu ))+ρ(Aˆ S(x)+b x) R dyN n× am2 ia cn sdthevelocityv(t)∈RN×2withadoubleintegrator
f =−D fAˆ (x)+(cid:80)K SkxA /(x+ϵ)
x+ =(xx ◦σ(S
−(b+f
k +=1 f)t+π)k
−σ )◦σ(2f )+σ r(t+1)=r(t)+Tv(t); v(t+1)=v(t)+Tu(t);
x u σ u
(20)
This system still satisfies the theorem 3 for ϵ → 0 since with the discrete acceleration u(t)∈RN×2 taken as system
(cid:12)
σ(−Ft+π)(cid:12) ≈1.Themodelin(20)isdifferentfromthe input. Note that the agent dynamics is used for building the
t=0
onein(8),evenif,whentheyhavethesameweightmatrices, dataset and for simulation purposes, but it is not provided tothe learning algorithm. The flocking expert controller [38] condition imposed since it will constrain more parame-
for the follower agents is given by ters. For LGTC and GraphODE, we implement the hybrid
solver(14)andRK4,respectively.Theinputfeaturesarefirst
N
1 (cid:88) processed by a cascade of two fully connected layers of 128
u (t)= v (t)−∇ CA(r(t),r (t))| (22a)
f N i r j j=1...N nodes before feeding the graph neural network. A readout
i=1
of two layers with 128 nodes combines the F-features GNN
and for the leader it is
hidden state to get the bidimensional control u saturated to
the maximum admissible control. The input layers and the
u (t)=−W (r (t)−d(t))−∇ CA(r (t),r (t))|
l p l rl l j j=1...N
readout that encapsulate the graph neural network shape a
(22b)
where W is a gain, r ∈ R2 is the leader position, more realistic setting to test the stability of the graph neural
p l
networkthatisusuallyusedincombinationwithotherkinds
∇ CA(r(t),r (t))/∇ CA(r (t),r (t)) are the gradient of
r j rl l j
ofneuralmodels.Toreducethecommunicatedvariables,we
the collision avoidance potential with respect to the position
used F′ = 4 and G = 4 for a subset of the state and the
of the agents/leader r/r , evaluated at the position r(t)/r (t)
l l
input to communicate. Following the Remark 1, we used
and the position of every other agent r (t) at time t. The
j
normalized Laplacian as a support matrix.
i-element of ∇ CA for each robot i with respect to robot j
r
is given by [41]
Training
(cid:40) − rij − rij if||r ||2 ≤R
∇ riCA(r ij)= ||ri 0j||4 2 ||rij||2 2 otherij wi2
se
CA sepW are atc eo dll ie nc tt oed tha red ea sta us be st eb tsy or fec tro ar id ni in ng g,6 v0 alt ir da aje tic ot nor aie ns d, tf eu sr tth se er
t
(23) using the proportion 70%−10%−20%, respectively. Each
withr ij =r i−r j andR CA >0indicatingtheminimumac- trajectoryisgeneratedbyrandomlypositioningtheagentsin
ceptabledistancebetweenagents.Thispotentialfunctionisa a square such that their inter-distance is between 0.6 m and
non-negative,non-smoothfunctionthatgoestoinfinitywhen 1.0 m and that their initial velocities are picked at random
the distance reduces and grows when the distance exceeds from the interval [−2,2] m/s in each direction. The leader is
R CA, in order to avoid the team losing connectivity [41]. randomly selected among the agents and the target position
u f(t),u l(t) are a centralized controller since computing is randomly located within a square of length 20 m centered
them requires agent i to have instantaneous evaluation of atthelocationoftheleader.Regardlessofthetargetlocation,
N1 (cid:80)N i=1v i(t) and r j(t) of every other agent j in the team. the trajectories have a duration of 2.5 s and input saturation
R CA and W p are tunable parameters of the controllers. at 5 m/s2. Moreover, the 60 trajectories are recorded with a
random number of agents among N =[4,6,10,12,15]. We
Neural Network Architecture
fixed the communication range to R=4 m and the sensing
We assume that the agents form a communication graph to R = 1 m. We trained the models for 120 epochs
CA
when they are in a sphere of radius R between each others and executed the DAGGER algorithm [42] every 20 epochs.
andthatexchangesoccuratthesamplingtimeT =0.05s,so The algorithm evaluates the expert controller in (22) on the
that the action clock and the communication clock coincide. enrolled state trajectories applying the learned control and
The input features vector w ∈ R10 of the robot i for the adding them to the training set. Note that, thanks to the use
i
designed neural network is of DAGGER, we do not need a large dataset. We solve the
(cid:34) imitation learning problem using the ADAM algorithm [43]
w = v , (cid:88) r ij , (cid:88) r ij , with a learning rate 1e−3 and forgetting factors 0.9 and
i i
j∈NSi
||r ij||4
2 j∈NSi
||r (cid:35)ij||2
2
(24)
0 m.9 ea9 n9. sqT uh ae rel dos es rrf ou rn bc eti to wn eeu nse td hefo or uti pm utita ot fio thn el mea orn di en lg anis
d
t th he
e
{0 ,r −d},{[0,1],[1,0]} optimal control action.
2 l
Results
whereNS isthesetofthesensingagentswithinasphereof
i
radius R centred in the robot i. Moreover, the vector w In Fig. 2, we show a comparison between GraphODE,
CA i
containsthezerovector0 ∈R1×2andtheone-hotencoding GGNN,LGTCandCfGCcontrollersfortheflockingcontrol
2
[0,1], if the agent is a follower, while (r −d) and [1,0], if case. We evaluate the 4 controllers on 2 sets of experiments
l
the agent is a leader. We chose the one-hot encoding instead with 20 trajectories each. In the experiments, we varied
of the binary one because it allows differentiating the neural team size and communication range to test the robustness
network weights between the leader and the follower. Note of the controllers. Figure 2 reports the leader position error
that we assume all the information in the vector w to be evaluatedafterafixedtimeof2.5swithrespecttotheleader
i
locally available at the sampling/control time T. starting location, i.e. e /e with e ,e respectively being
f s f s
The core of the neural network for the flocking control is a the final and the initial square distance of the leader from
layer of GGNN, GraphODE, LGTC or CfGC with F = 50 the target. We also show the average flocking error in the
features in the hidden state and filter length K = 2. Note interval [0,2.5s] in logarithmic scale. We assume that the
that the choice of K affects the complexity of the stability communication happens within the sampling time T.Fig.2:FlockingandLeaderErrorforGGNN,LGCT,CfGCandGraphODE,varyingtheteamsizeNwithacommunication
range of 4m and a variable communication range with N =25.
In the first experiment, the controller scalability is evalu- preserves the dynamic properties of the system. We validate
atedforteamsizesN =[4,10,25,50]withacommunication theproposedmodelinaflockingcontrolcaseandcompareit
range at 4 m. Figure 2 shows empirically the higher predic- with δISS GGNN and the GraphODE. Results suggest that,
tion abilities of LGTC and CfGC compared to GGNN, as despite the reduced communicated vector, LGTC and CfGC
they are up to 40% for the flocking error and 10% for the have performances closer to the centralized expert and more
leader error closer to the expert controller. This is despite robust to parametric changes in a deployment scenario, such
LGTC and CfGC communicating fewer state and input as communication radius and team size. Future works will
variables as explained in the previous section. Moreover, the considerhowtofurtherreducethecommunicationload,still
performances of LGTC and CfGC are close to each other, high compared to model-based approaches, as well as an
confirming the good approximation of CfGC. Sometimes, experimental validation on quadrotor UAVs.
for example, for the flocking error with N = 10, we can
seeCfGCperformingbetterthanLGTCof1−2%.Thiscan
REFERENCES
be justified by the fact that CfGC is simpler to train, as it [1] A. Dorri, S. S. Kanhere, and R. Jurdak, “Multi-agent systems: A
is a discrete model, and so it reduces the vanishing gradient survey,”IeeeAccess,vol.6,pp.28573–28593,2018.
[2] J. Corte´s and M. Egerstedt, “Coordinated control of multi-robot
phenomena. In general, GraphODE is the one that performs
systems: A survey,” SICE Journal of Control, Measurement, and
the worst both for leader and flocking errors. This can be SystemIntegration,vol.10,no.6,pp.495–503,2017.
explained easily by no direct use of the input features, as [3] G.-B. Huang, L. Chen, C. K. Siew, et al., “Universal approximation
using incremental constructive feedforward networks with random
they are only used to initialize the internal state, resulting in
hiddennodes,”IEEETrans.NeuralNetworks,vol.17,no.4,pp.879–
an auto-regressive model. 892,2006.
In the second set of experiments, we evaluate the per- [4] S.Batra,Z.Huang,A.Petrenko,T.Kumar,A.Molchanov,andG.S.
Sukhatme, “Decentralized control of quadrotor swarms with end-to-
formances of the networks when the communication range
enddeepreinforcementlearning,”inConferenceonRobotLearning.
differs from the training range R = 4 m. For 2 m all the PMLR,2022,pp.576–586.
models show a drop in the performances with 0.18 m/s in [5] C. He, Y. Wan, Y. Gu, and F. L. Lewis, “Integral reinforcement
learning-based multi-robot minimum time-energy path planning sub-
the flocking error for GGNN,LGTC and CfGC, and 0.3 m/s
jecttocollisionavoidanceandunknownenvironmentaldisturbances,”
forGraphODE.Thisdropappearsduetothelowerabilityto IEEEControlSystemsLetters,vol.5,no.3,pp.983–988,2020.
broadcasttheinformationamongtheagentswhentheyspace [6] N. Majcherczyk, N. Srishankar, and C. Pinciroli, “Flow-fl: Data-
drivenfederatedlearningforspatio-temporalpredictionsinmulti-robot
out over 2 m. As expected, when the communication range
systems,” in 2021 IEEE International Conference on Robotics and
increases, the flocking errors decrease for all controllers, Automation(ICRA). IEEE,2021,pp.8836–8842.
since they can communicate with more agents at the same [7] F. Gama, E. Tolstaya, and A. Ribeiro, “Graph neural networks for
decentralized controllers,” in ICASSP 2021-2021 IEEE International
time. However, in this case, the leader error increases,
Conference on Acoustics, Speech and Signal Processing (ICASSP).
since the leader is often “encapsulated” by the other team IEEE,2021,pp.5260–5264.
agents and it is thus forced to follow them to not collide, [8] Q.Li,W.Lin,Z.Liu,andA.Prorok,“Message-awaregraphattention
networks for large-scale multi-robot path planning,” IEEE Robotics
causing a slower convergence to the target. Note that this
andAutomationLetters,vol.6,no.3,pp.5533–5540,2021.
behaviouralsoaffectstheexpertcontroller.Notably,alsofor [9] E. Tolstaya, J. Paulos, V. Kumar, and A. Ribeiro, “Multi-robot cov-
this experiment, LGTC and CfCG have better performances erage and exploration using spatial graph neural networks,” in 2021
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems
compared to the others and are similar to each other.
(IROS). IEEE,2021,pp.8944–8950.
[10] A. Khan, A. Ribeiro, V. Kumar, and A. G. Francis, “Graph neural
VI. CONCLUSIONS
networks for motion planning,” arXiv preprint arXiv:2006.06248,
In this work, we model a new continuous-time graph 2020.
[11] X. Ji, H. Li, Z. Pan, X. Gao, and C. Tu, “Decentralized, unlabeled
neural network, Liquid-Graph Time-constant (LGTC). Its
multi-agent navigation in obstacle-rich environments using graph
enhanced prediction capabilities allow us to reduce com- neural networks,” in 2021 IEEE/RSJ International Conference on
munication load, one of the main drawbacks of GNNs. IntelligentRobotsandSystems(IROS). IEEE,2021,pp.8936–8943.
[12] Y.Zhou,J.Xiao,Y.Zhou,andG.Loianno,“Multi-robotcollaborative
We analyze its dynamic properties leveraging contraction
perceptionwithgraphneuralnetworks,”IEEERoboticsandAutoma-
analysis and find a closed-form approximation (CfGC) that tionLetters,vol.7,no.2,pp.2289–2296,2022.[13] M. Tzes, N. Bousias, E. Chatzipantazis, and G. J. Pappas, “Graph [36] D. Lukovnikov, J. Lehmann, and A. Fischer, “Improving the long-
neuralnetworksformulti-robotactiveinformationacquisition,”arXiv range performance of gated graph neural networks,” arXiv preprint
preprintarXiv:2209.12091,2022. arXiv:2007.09668,2020.
[14] L. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, [37] T. Stro¨m, “On logarithmic norms,” SIAM Journal on Numerical
“Learning-based model predictive control: Toward safe learning in Analysis,vol.12,no.5,pp.741–753,1975.
control,” Annual Review of Control, Robotics, and Autonomous Sys- [38] E.Tolstaya,F.Gama,J.Paulos,G.Pappas,V.Kumar,andA.Ribeiro,
tems,vol.3,pp.269–296,2020. “Learningdecentralizedcontrollersforrobotswarmswithgraphneural
[15] A. Davydov, S. Jafarpour, and F. Bullo, “Non-euclidean contraction networks,”inConferenceonrobotlearning. PMLR,2020,pp.671–
theoryforrobustnonlinearstability,”IEEETransactionsonAutomatic 682.
Control,vol.67,no.12,pp.6667–6681,2022. [39] W. Yu, G. Chen, and M. Cao, “Distributed leader–follower flocking
[16] B. Song, J.-J. Slotine, and Q.-C. Pham, “Stability guarantees for control for multi-agent dynamical systems with time-varying veloci-
continuousrlcontrol,”arXivpreprintarXiv:2209.07324,2022. ties,”Systems&ControlLetters,vol.59,no.9,pp.543–552,2010.
[17] H.Tsukamoto,S.-J.Chung,andJ.-J.Slotine,“Learning-basedadaptive [40] L. E. Beaver and A. A. Malikopoulos, “An overview on optimal
control using contraction theory,” in 2021 60th IEEE Conference on flocking,”AnnualReviewsinControl,vol.51,pp.88–99,2021.
DecisionandControl(CDC). IEEE,2021,pp.2533–2538. [41] H. G. Tanner, A. Jadbabaie, and G. J. Pappas, “Stable flocking of
mobileagentsparti:dynamictopology,”in42ndIEEEInternational
[18] F. Bonassi, M. Farina, and R. Scattolini, “On the stability properties
Conference on Decision and Control (IEEE Cat. No. 03CH37475),
ofgatedrecurrentunitsneuralnetworks,”Systems&ControlLetters,
vol.2. IEEE,2003,pp.2016–2021.
vol.157,p.105049,2021.
[42] S.Ross,G.Gordon,andD.Bagnell,“Areductionofimitationlearning
[19] E. Terzi, F. Bonassi, M. Farina, and R. Scattolini, “Learning model
andstructuredpredictiontono-regretonlinelearning,”inProceedings
predictive control with long short-term memory networks,” Interna-
ofthefourteenthinternationalconferenceonartificialintelligenceand
tionalJournalofRobustandNonlinearControl,vol.31,no.18,pp.
statistics. JMLRWorkshopandConferenceProceedings,2011,pp.
8877–8896,2021.
627–635.
[20] F.Bayer,M.Bu¨rger,andF.Allgo¨wer,“Discrete-timeincrementaliss:
[43] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
Aframeworkforrobustnmpc,”in2013EuropeanControlConference
tion,”CoRR,vol.abs/1412.6980,2014.
(ECC). IEEE,2013,pp.2068–2073.
[21] W. D’Amico, A. La Bella, and M. Farina, “An incremental input-
to-state stability condition for a generic class of recurrent neural
networks,”arXivpreprintarXiv:2210.09721,2022.
[22] A.Marino,C.Pacchierotti,andP.R.Giordano,“Inputstatestability
of gated graph neural networks,” IEEE Transactions on Control of
NetworkSystems,pp.1–12,2024.
[23] L. Ruiz, F. Gama, and A. Ribeiro, “Gated graph recurrent neural
networks,”IEEETransactionsonSignalProcessing,vol.68,pp.6303–
6318,2020.
[24] R. Hasani, M. Lechner, A. Amini, D. Rus, and R. Grosu, “Liquid
time-constant networks,” in Proceedings of the AAAI Conference on
ArtificialIntelligence,vol.35,no.9,2021,pp.7657–7666.
[25] R. Hasani, M. Lechner, A. Amini, L. Liebenwein, A. Ray,
M. Tschaikowski, G. Teschl, and D. Rus, “Closed-form continuous-
time neural networks,” Nature Machine Intelligence, vol. 4, no. 11,
pp.992–1003,2022.
[26] E. Haber and L. Ruthotto, “Stable architectures for deep neural
networks,”Inverseproblems,vol.34,no.1,p.014004,2017.
[27] R.T.Chen,Y.Rubanova,J.Bettencourt,andD.K.Duvenaud,“Neu-
ral ordinary differential equations,” Advances in neural information
processingsystems,vol.31,2018.
[28] M.Lechner,R.Hasani,M.Zimmer,T.A.Henzinger,andR.Grosu,
“Designing worm-inspired neural networks for interpretable robotic
control,”in2019internationalconferenceonroboticsandautomation
(ICRA). IEEE,2019,pp.87–94.
[29] M. Poli, S. Massaroli, J. Park, A. Yamashita, H. Asama, and
J.Park,“Graphneuralordinarydifferentialequations,”arXivpreprint
arXiv:1911.07532,2019.
[30] Y. Qin, W. Ju, H. Wu, X. Luo, and M. Zhang, “Learning graph ode
for continuous-time sequential recommendation,” IEEE Transactions
onKnowledgeandDataEngineering,2024.
[31] Y.Su,B.Ren,andK.Zhang,“Graphoderecurrentneuralnetworksfor
traffic flow forecasting,” in 2022 IEEE 5th International Conference
on Electronics and Communication Engineering (ICECE). IEEE,
2022,pp.178–182.
[32] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Van-
dergheynst, “The emerging field of signal processing on graphs: Ex-
tendinghigh-dimensionaldataanalysistonetworksandotherirregular
domains,”IEEESignalProcessingMagazine,vol.30,no.3,pp.83–
98,2013.
[33] F.Gama,J.Bruna,andA.Ribeiro,“Stabilitypropertiesofgraphneural
networks,”IEEETransactionsonSignalProcessing,vol.68,pp.5680–
5695,2020.
[34] Q. Li, F. Gama, A. Ribeiro, and A. Prorok, “Graph neural networks
fordecentralizedpathplanning,”inProc.InternationalConferenceon
AutonomousAgentsandMultiagentSystems,2020,pp.1901–1903.
[35] F. Gu, H. Chang, W. Zhu, S. Sojoudi, and L. El Ghaoui, “Implicit
graph neural networks,” Advances in Neural Information Processing
Systems,vol.33,pp.11984–11995,2020.