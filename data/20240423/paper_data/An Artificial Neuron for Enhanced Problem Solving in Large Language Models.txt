AN ARTIFICIAL NEURON FOR ENHANCED PROBLEM SOLVING
IN LARGE LANGUAGE MODELS
SumedhRasal
GeorgiaInstituteofTechnology
Chicago,IL
srasal3@gatech.edu
ABSTRACT
Recent advancements in artificial intelligence have propelled the capabilities of Large Language
Models(LLMs), yet their ability to mimic nuancedhumanreasoningremainslimited. Thispaper
introducesa novelconceptualenhancementto LLMs, termed the "Artificial Neuron,"designed to
significantly bolster cognitive processing by integrating external memory systems. This enhance-
mentmimicsneurobiologicalprocesses,facilitatingadvancedreasoningandlearningthroughady-
namic feedback loop mechanism. We propose a unique framework wherein each LLM interac-
tion—specificallyinsolvingcomplexmathwordproblemsandcommonsensereasoningtasks—is
recordedandanalyzed. Incorrectresponsesarerefinedusinga higher-capacityLLMorhuman-in-
the-loopcorrections,andboththequeryandtheenhancedresponsearestoredinavectordatabase,
structuredmuchlikeneuronalsynapticconnections. This"ArtificialNeuron"thusservesasanex-
ternal memory aid, allowing the LLM to reference past interactions and apply learned reasoning
strategiestonewproblems.
OurexperimentalsetupinvolvestrainingwiththeGSM8Kdatasetforinitialmodelresponsegenera-
tion,followedbysystematicrefinementsthroughfeedbackloops. Subsequenttestingdemonstrated
a15%improvementinaccuracyandefficiency,underscoringthepotentialofexternalmemorysys-
tems to advance LLMs beyond current limitations. This approach not only enhances the LLM’s
problem-solving precision but also reduces computational redundancy, paving the way for more
sophisticatedapplicationsofartificialintelligenceincognitivetasks. Thispaperdetailsthemethod-
ology, implementation, and implicationsof the Artificial Neuronmodel, offeringa transformative
perspectiveonenhancingmachineintelligence.
1 Introduction
TheadventofartificialintelligencehasusheredinanerawhereLargeLanguageModels(LLMs)likeGPT-2through
GPT-4[1][2]havedemonstratedunprecedentedcapabilitiesinlanguageunderstandingandproblem-solving[3][4][5]
[6][7][8][9]. Thesemodelshaveprogressivelybridgedthegapbetweencomputationaltextprocessingandgenuine
linguistic comprehension,enhancingtheir utility across a spectrum of applications, from simple task automationto
complexproblemresolution[10][11]. Despitetheseadvancements,thequesttowardsachievingafullyautonomous
LLM,capableofexhibitinghuman-likereasoninganddecision-making,remainselusive[12].
Thispaperintroducestheconceptofthe"ArtificialNeuron"asastrategicenhancementforLLMs,aimingtoreplicate
aspects of human cognitive processes through the integration of an external memory module. Traditional LLMs,
whileproficientinlanguagetasks,cannotoftendrawonpastexperiencesoradapttheirresponsesbasedonpreviously
encounteredcontexts. The Artificial Neuron is designed to address these shortcomingsby enabling LLMs to store,
recall,andutilizepastinteractionsintheirdecision-makingprocesses,muchlikethesynapticactivitiesinthehuman
brain[13][14][15].
InexploringwhetherwecanendowLLMswithaformofartificialwisdom,thisstudyseekstodefineandoperational-
izewisdomwithintherealmofartificialintelligence. Wisdom,inthiscontext,extendsbeyondmeredataretrievalto
encompassthenuancedapplicationofknowledgeandexperienceinproblem-solving. Theflexibilityinherentin the
4202
rpA
22
]CH.sc[
1v22241.4042:viXraAnArtificialNeuronforEnhancedProblemSolvinginLargeLanguageModels
transformerarchitectureofLLMsallowsforlinguisticadaptability,yetitdoesnotinherentlyequipthesemodelswith
the depthof understandingnecessary to discernthe subtletiesof languageusage or to anticipatethe implicationsof
newlyacquiredinformation.
TheintroductionofanArtificialNeuronaimstorevolutionizethislandscapebyprovidingamechanismthroughwhich
LLMscan enhancetheir reasoningskills [16] [17] [18] [19] [20] [21] [22] [23] [24]. Thispaperwill detaila novel
approachtosupplementingLLMswithexternalmemory,thusenablingthemtolearnfromboththeirtrainingdataand
their interactiveexperiences. By employingpast encountersas templatesfor currentproblem-solving,the Artificial
NeuronseekstoimprovetheaccuracyandrelevanceofLLMresponses,ultimatelybridgingthegapbetweencurrent
capabilitiesandthepotentialfortrueartificialgeneralintelligence[25][26][27].
Ourproposedmethodologydeviatesfromtraditionaltrainingmethods,focusinginsteadonbuildinganexternalmem-
orythataidstheLLMinaddressingcomplexmathwordproblemsandcommonsensereasoningchallenges. Initially,
wecollectresponsesfromtheLLMusingdatasetscomprisedofmathproblemsandcommonsensequestions,docu-
mentingboththeanswersandtheLLM’srationaleinavectordatabase. Incorrectresponsesareidentifiedandrefined
either through further consultations with the GPT-4 model or via human intervention, ensuring that only corrected
answersarestored.
Uponcompletionoftheserevisions,theenrichedexternalmemoryisintegratedintotheLLM’soperationalframework.
WhentheLLMencountersanewproblem,itperformsasemanticsearchwithinthevectordatabasetoidentifythetop
threesimilarpastscenarios.Theserecords,detailingthestrategiespreviouslyemployed,guidetheLLMinformulating
itsapproachtothenewquestion,utilizingachain-of-thoughtpromptstylebasedonthemostrelevanttemplates.
Adoptinga"learningbycases"strategy,combinedwithchain-of-thoughtreasoning,weenhancethecapabilitiesofa
GPT-3.5-turbomodelbyequippingitwiththisstrategicexternalmemory. Theresultantoutputsfromthisaugmented
versionoftheGPT-3.5-turboarepromisingandsuggestsignificantimprovementsoveritspredecessors.Ourapproach
outlines:
• TheutilizationofavectordatabasetometiculouslyrecordactionsperformedbytheLLM.
• A mechanismto evaluateandamendtheserecords, ensuringthattheynotonlyreflectsuccessfuloutcomes
butalsoincorporatecorrectionswherenecessary.
• Autilityfunctionthatleveragestheseenrichedrecordstosetcontextuallyappropriateprompts,therebyele-
vatingtheLLM’sbaselineaccuracyandperformance.
ThisframeworknotonlyenhancestheaccuracyandefficiencyofLLMsbutalsobridgesthegaptowardsachievinga
morenuanced,context-awareartificialintelligence.
2 Methodology
Themethodologypresentedinthispaperrevolvesaroundtheinnovativeconceptofthe"ArtificialNeuron"forLarge
Language Models (LLMs), a structured external memory system designed to enhance the reasoning abilities and
contextualunderstandingofAIincomplexproblem-solvingscenarios.Theapproachisdelineatedintodistinctphases:
setup,interaction,errorcorrection,andperformanceenhancementthroughacontinuouslearningcycle.
Phase1: SetupandIntegrationoftheArtificialNeuron
Model Configuration: We begin by configuringthe LLM with an initial architecture that includes an interface for
integratingexternalmemorycomponents. ThissetupallowsthemodeltoaccessandinteractwiththeArtificialNeu-
ron,aspecializedvectordatabasedesignedtomimicsynapticconnectionsbystoringandretrievingproblem-solving
experiences.
ExternalMemoryInitialization: TheArtificialNeuronispopulatedinitiallywithabroadrangeofproblem-solving
scenariosdrawn fromselected datasets, includingGSM8K for math word problemsand other datasets for common
sense reasoning tasks. Each entry in this memory unit contains the problem statement, the LLM’s response, the
reasoningprocess,andmetadataaboutthetask.
Phase2: InteractiveLearningandMemoryUtilization
DataCollection:AstheLLMengageswithnewproblem-solvingtasks,eachinteraction—question,proposedsolution,
andreasoningpathway—islogged. ThiscontinuousdatacollectioniscriticalfordynamicallyupdatingtheArtificial
Neuron.
2AnArtificialNeuronforEnhancedProblemSolvinginLargeLanguageModels
Semantic Search Mechanism: When presented with a new problem, the LLM performs a semantic search within
theArtificialNeurontoretrievethemostrelevantpastinteractions.Thisprocessisfacilitatedbyadvancedalgorithms
thatanalyzethesimilaritybetweenthenewproblemandstoredentries,focusingonboththeproblem’snatureandthe
contextualnuancesofprevioussolutions.
Phase3: ErrorCorrectionandFeedbackIntegration
PerformanceAnalysis: ResponsesgeneratedbytheLLMareevaluatedforaccuracyagainstestablishedbenchmarks
orthroughexpertreview.ErrorsidentifiedintheLLM’sreasoningoroutputstriggeracorrectivefeedbackloop.
FeedbackMechanism: IncorrectresponsesarecorrectedeitherthroughfurtherprocessingbyamoreadvancedLLM
modelorviahumanintervention.Thecorrectedresponse,alongwithananalysisoftheerrorandadetailedexplanation
ofthecorrectreasoningprocess,isthenfedbackintotheArtificialNeuron.
MemoryUpdate: TheArtificialNeuronisupdatedwiththisnewinformation,enhancingitsdatabasewithboththe
correctedresponseandarevisedreasoningpathway. Thisupdatenotonlyrectifieserrorsbutalsoenrichesthefuture
problem-solvingcapacityoftheLLM.
Phase4: ContinuousImprovementandTesting
IterativeLearning: Theprocessofinteraction,errorcorrection,andmemoryenhancementiscyclic,withtheLLM
continuouslyrefiningitsproblem-solvingstrategiesbasedonaccumulatedexperiencestoredintheArtificialNeuron.
SystematicEvaluation: RegularlyscheduledevaluationsmeasuretheLLM’sperformanceimprovementsovertime.
These assessments help fine-tunethe semantic search algorithmsand the effectivenessof the feedbackmechanisms,
ensuringthattheArtificialNeuronremainsarobustanddynamicaidintheLLM’scognitiveprocessing.
ThismethodologynotonlyproposesapracticalframeworkforenhancingthecapabilitiesofLLMsthroughexternal
memorybutalsoillustratesapathwaytowardsmoresophisticatedAIsystemscapableoflearningandadaptingfrom
theirinteractions,muchlikehumancognitiveprocesses.
3 Experiments
3.1 EnhancingMathematicalReasoningwiththeArtificialNeuron
DatasetsUtilized
1. MAWPS: An extensive online repository of math word problems designed to serve as a unified platform for
evaluatingalgorithmicperformance. Thisdataset contains3,320problemssourcedfrom previousstudies, equipped
withtoolsthatallowforthecustomizationofdatasetsbyadjustinglexicalandtemplateoverlaps,aswellasfilteringout
ungrammaticalproblems. Thedynamic,onlinenatureofMAWPSencouragescontinuouscommunitycontributions,
makingitarichresourcefordiversemathematicalchallenges[28].
2. SVAMP:Achallengesettailoredforelementary-levelmathwordproblems,presentingsimplevariationsonarith-
meticproblemsencapsulatedinnaturallanguagenarratives. Theseproblemstestthemodel’sabilitytointerpretand
solvequestionsaboutunknownquantitiesbasedondescribedscenarios.
Training andTesting Procedure- For bothdatasets, half of the problemswere allocated for trainingthe Artificial
Neuron,whiletheotherhalfservedasatestsettoevaluatetheenhancementbroughtaboutbythisnovelapproach.
-Duringtraining,theArtificialNeuronwasexposedtovariousproblem-solvingscenarioswhereeachinteractionwas
logged,andincorrectresponsesweresubsequentlycorrectedandenrichedwithdetailedfeedback.
- In the testing phase, the LLM equippedwith the Artificial Neurontackled new, unseenproblemsfrom the test set.
Themodel’sresponseswerecomparedagainstacontrolgroup,whichconsistedofthesameLLMarchitecturewithout
theintegrationoftheArtificialNeuron.
Results - The experiments demonstrated a notable improvement of approximately 15% in solving accuracy over
the baselineGPT-3.5-turbomodel. Thisenhancementunderscoresthe efficacyofthe ArtificialNeuroninproviding
contextualguidanceandmemory-basedreasoningtotacklediverseandcomplexmathproblems.
3.2 ComplexSequentialQuestionAnswering(CSQA)
DatasetUtilized
3AnArtificialNeuronforEnhancedProblemSolvinginLargeLanguageModels
1. CSQADataset: Anewlyintroduceddatasetdesignedtosimulatereal-worldinteractionsinvolvingbothquestion
answering (QA) and dialogue systems. The dataset comprises approximately200,000dialogueswith a total of 1.6
millionturns.Thesedialoguesarebuiltaroundalarge-scaleknowledgegraph(KG)andrequireadvancedinferencing
toanswerquestionsthatofteninvolvelogical,quantitative,andcomparativereasoning.
TrainingandTestingProcedure-Similarly,halfoftheCSQAdatasetwasusedtotraintheArtificialNeuron,focus-
ing particularlyonenhancingthe model’sability to parsecomplexnaturallanguagequeries, manageconversational
context,andutilizeKGseffectively.
-Theremaininghalfofthedatasetwasusedtoevaluatethemodel’sperformanceinasequentialquestion-answering
setupthatmirrorspracticalconversationalscenariosencounteredbymodernchatbots.
Results-Preliminaryresultsindicatedthatexistingstate-of-the-artdialogueandQAmodelswereinadequateinhan-
dlingthecomplexityoftheCSQAdataset. However,theintroductionoftheArtificialNeuronsignificantlyimproved
theLLM’sabilitytoengageincoherentdialogueandanswerquestionsbasedoncomplexinferencing.
-Theexperimentrevealedanimprovementinthemodel’scapabilitytoclarifyambiguities,resolvecoreferences,and
retrieverelevantsubgraphsfromtheKGforaccurateresponses.
TheseexperimentsvalidatetheArtificialNeuron’spotentialtosubstantiallyenhanceLLMs’problem-solvingcapabil-
itiesacrossdifferentdomains. Byintegratingexperiencesandlearnedstrategiesintoanexternalmemory,theLLMs
exhibitedimprovedreasoning,adaptability,andaccuracyinhandlingcomplex,real-worldtasks. Thispromisingad-
vancementinvitesfurtherexplorationintoextendingthesemethodstobroaderAIapplications,potentiallypavingthe
wayformoresophisticatedcognitiveabilitiesinartificialintelligencesystems.
4 Limitations
WhiletheintroductionoftheArtificialNeuronrepresentsasignificantadvancementinthecapabilitiesofLargeLan-
guageModels(LLMs),thisapproachisnotwithoutitslimitations. TraditionalmethodsforenhancingLLMaccuracy
typically involve extensive retraining [29] on large datasets, which, while effective, can be prohibitively expensive
and time-consuming. Our methodologycircumventsthese costs by enablingLLMsto learn frominteractiveexperi-
encesstoredwithinanexternalmemoryframework,reducingtheneedforfrequentretraining. However,thissystem
introducesnewchallengesthatmustbeaddressed:
DependencyonManualInput: Thecurrentimplementationreliesheavilyonmanualinterventionsforerroridentifi-
cationandcorrection.EachinstancewheretheLLMfailstoprovidethecorrectanswerorreasoningmustbemanually
reviewedandcorrected,whichisnotonlylabor-intensivebutalsolimitsthescalabilityofthisapproach.
MemoryIntegration:WhileintegratingexternalmemorywithanLLMseemsstraightforward,optimizingthismem-
ory for efficient retrievaland ensuringit aligns seamlessly with the model’s operationalframeworkrequires further
development.TheprocessofstoringandretrievingrelevantexperiencesfromtheArtificialNeuronmustberefinedto
preventperformancebottlenecks.
QualityControl: EnsuringthequalityandrelevanceofthefeedbackstoredintheArtificialNeuroniscrucial. Incor-
rectormisleadingfeedbackcouldreinforcepoorreasoninghabitsintheLLM,potentiallydegradingitsperformance
overtime.
5 Conclusion
Thispaperpresentsapioneeringapproachtoimprovingthereasoningabilitiesanddecision-makingaccuracyofLarge
LanguageModelsthroughtheuseofanexternalmemorysystem,termedtheArtificialNeuron.ByallowingLLMsto
learnfromtheirpastinteractions,wefacilitateaformofexperientiallearningthatmimicshumancognitiveprocesses,
enablingthesemodelstoperformcomplexproblem-solvingtaskswithgreateraccuracy.
Futureiterationsofthismethodologyshouldfocusonautomatingthefeedbackmechanisms. Developingalgorithms
thatcanautomaticallyassesstheeffectivenessofanLLM’sresponsesandprovideaccuratecorrectionswillenhance
thescalabilityandefficiencyofthisapproach.-**IntegrationwithUserFeedback**:Implementingsystemsthatallow
LLMs to adjust their responses based on direct user feedback could providea more dynamiclearning environment.
Thiscouldinvolvereal-timeadjustmentstoresponsesbasedonuserinteractionsorsatisfactionratings.
WecouldexploretheintegrationoftheArtificialNeuronwithinamulti-agentframeworkthatcouldprovideinsights
into how shared external memories influence cooperative task-solving among multiple LLMs [30] [31] [32] [33]
4AnArtificialNeuronforEnhancedProblemSolvinginLargeLanguageModels
[34][35] [36] [37] [38][39]. ThiscouldleadtoadvancementsinhowLLMscollaborateandlearnfromeachother,
potentiallyopeningnewavenuesfordevelopingmoresophisticatedAIsystems.
Byaddressingtheselimitationsandexploringthesefuturedirections,wecancontinuetorefinetheArtificialNeuron
conceptandexpanditsapplicabilityacrossvariousdomains,movingclosertocreatingLLMsthatcanoperatewitha
levelofautonomyandreasoningcomplexitythatmirrorshumanintelligence.
References
[1] OpenAI. Gpt-4technicalreport.arxiv2303.08774. ViewinArticle,2:13,2023.
[2] SébastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKamar,PeterLee,
YinTatLee,YuanzhiLi,ScottLundberg,etal. Sparksofartificialgeneralintelligence: Earlyexperimentswith
gpt-4. arXivpreprintarXiv:2303.12712,2023.
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNee-
lakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners. Advances
inneuralinformationprocessingsystems,33:1877–1901,2020.
[4] JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,DaniYogatama,Maarten
Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint
arXiv:2206.07682,2022.
[5] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,Bap-
tisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openandefficientfoundationlanguage
models. arXivpreprintarXiv:2302.13971,2023.
[6] JacobDevlin, Ming-WeiChang, KentonLee, andKristina Toutanova. Bert: Pre-trainingofdeepbidirectional
transformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
[7] ShishirG Patil, TianjunZhang,Xin Wang, andJosephEGonzalez. Gorilla: Largelanguagemodelconnected
withmassiveapis. arXivpreprintarXiv:2305.15334,2023.
[8] ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao. React: Syner-
gizingreasoningandactinginlanguagemodels. arXivpreprintarXiv:2210.03629,2022.
[9] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel:
Communicative agents for" mind" exploration of large scale language model society. arXiv preprint
arXiv:2303.17760,2023.
[10] MarkChen, JerryTworek,HeewooJun, QimingYuan,HenriquePondedeOliveiraPinto, JaredKaplan, Harri
Edwards,YuriBurda,NicholasJoseph,GregBrockman,etal. Evaluatinglargelanguagemodelstrainedoncode.
arXivpreprintarXiv:2107.03374,2021.
[11] LongOuyang,JeffWu,XuJiang,DiogoAlmeida,CarrollLWainwright,PamelaMishkin,ChongZhang,Sand-
hini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human
feedback,2022. URLhttps://arxiv.org/abs/2203.02155,13,2022.
[12] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging
languagemodelsforcommonsensereasoning. arXivpreprintarXiv:1906.02361,2019.
[13] LawrenceESusskindandJasonCorburn. Usingsimulationstoteachnegotiation: Pedagogicaltheoryandprac-
tice. Teachingnegotiation:Ideasandinnovations,pages285–310,2000.
[14] Pei Zhou, Andrew Zhu, Jennifer Hu, Jay Pujara, Xiang Ren, Chris Callison-Burch, Yejin Choi, and Prithviraj
Ammanabrolu. I castdetectthoughts: Learningtoconverseandguidewith intentsandtheory-of-mindin dun-
geonsanddragons. InProceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics
(Volume1: LongPapers),pages11136–11155,2023.
[15] AndreaILuppi,PedroAMMediano,FernandoERosas,NeginHolland,TimDFryer,JohnTO’Brien,JamesB
Rowe,DavidKMenon,DanielBor,andEmmanuelAStamatakis. Asynergisticcoreforhumanbrainevolution
andcognition. NatureNeuroscience,25(6):771–782,2022.
[16] WangLing,DaniYogatama,ChrisDyer,andPhilBlunsom.Programinductionbyrationalegeneration:Learning
tosolveandexplainalgebraicwordproblems. arXivpreprintarXiv:1705.04146,2017.
[17] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,MatthiasPlappert,
Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv
preprintarXiv:2110.14168,2021.
5AnArtificialNeuronforEnhancedProblemSolvinginLargeLanguageModels
[18] SubhroRoyandDanRoth. Solvinggeneralarithmeticwordproblems. arXivpreprintarXiv:1608.01413,2016.
[19] Ting-RuiChiangandYun-NungChen.Semantically-alignedequationgenerationforsolvingandreasoningmath
wordproblems. arXivpreprintarXiv:1811.00720,2018.
[20] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint
arXiv:1905.13319,2019.
[21] XinyunChen,ChenLiang,AdamsWeiYu,DennyZhou,DawnSong,andQuocVLe. Neuralsymbolicreader:
Scalable integration of distributed and symbolic representations for reading comprehension. In International
ConferenceonLearningRepresentations,2019.
[22] AlonTalmor,JonathanHerzig,NicholasLourie,andJonathanBerant. Commonsenseqa: Aquestionanswering
challengetargetingcommonsenseknowledge. arXivpreprintarXiv:1811.00937,2018.
[23] MaartenSap,HannahRashkin,DerekChen,RonanLeBras,andYejinChoi.Socialiqa:Commonsensereasoning
aboutsocialinteractions. arXivpreprintarXiv:1904.09728,2019.
[24] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word
problems? arXivpreprintarXiv:2103.07191,2021.
[25] Cheng-HanChiangandHung-yiLee. Canlargelanguagemodelsbeanalternativetohumanevaluations? arXiv
preprintarXiv:2305.01937,2023.
[26] Sumedh Rasal and E. J. Hauer. Navigating complexity: Orchestrated problem solving with multi-agent llms,
2024.
[27] Chirag Agarwal, Sree Harsha Tanneru, and Himabindu Lakkaraju. Faithfulness vs. plausibility: On the
(un)reliabilityofexplanationsfromlargelanguagemodels. ArXiv,abs/2402.04614,2024.
[28] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,MatthiasPlappert,
Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to
solvemathwordproblems. arXivpreprintarXiv:2110.14168,2021.
[29] SumedhRasalandSanjayKumarBoddhu. Beyondsegmentation: Roadnetworkgenerationwithmulti-modal
llms. arXivpreprintarXiv:2310.09755,2023.
[30] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and
Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. arXiv
preprintarXiv:2305.19118,2023.
[31] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun.
Communicativeagentsforsoftwaredevelopment. arXivpreprintarXiv:2307.07924,2023.
[32] AnitaWilliamsWoolley,ChristopherFChabris,AlexPentland,NadaHashmi,andThomasWMalone.Evidence
foracollectiveintelligencefactorintheperformanceofhumangroups. science,330(6004):686–688,2010.
[33] AngelikiLazaridou,AnnaPotapenko,andOlivierTieleman.Multi-agentcommunicationmeetsnaturallanguage:
Synergiesbetweenfunctionalandstructurallanguagelearning,2020.
[34] LauraGraesser,KyunghyunCho,andDouweKiela. Emergentlinguisticphenomenainmulti-agentcommunica-
tiongames,2020.
[35] JasonLee,KyunghyunCho,JasonWeston,andDouweKiela. Emergenttranslationinmulti-agentcommunica-
tion,2018.
[36] NingWu, MingGong,LinjunShou,ShiningLiang,andDaxinJiang. Largelanguagemodelsarediverserole-
playersforsummarizationevaluation. arXivpreprintarXiv:2303.15078,2023.
[37] SumedhRasal. Llmharmony:Multi-agentcommunicationforproblemsolving,2024.
[38] Carlos Jose Xavier Cruz. Transformingcompetition into collaboration: The revolutionaryrole of multi-agent
systemsandlanguagemodelsinmodernorganizations,2024.
[39] EmanueleMusumeci, Michele Brienza, Vincenzo Suriani, Daniele Nardi, and DomenicoDaniele Bloisi. Llm
basedmulti-agentgenerationofsemi-structureddocumentsfromsemantictemplatesinthepublicadministration
domain. ArXiv,abs/2402.14871,2024.
6