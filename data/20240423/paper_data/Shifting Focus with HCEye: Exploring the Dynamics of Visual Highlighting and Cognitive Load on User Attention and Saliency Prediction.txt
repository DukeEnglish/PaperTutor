Shifting Focus with HCEye: Exploring the Dynamics of Visual
Highlighting and Cognitive Load on User Attention and
Saliency Prediction
ANWESHADAS,SaarlandUniversity,SaarlandInformaticsCampus,Germany
ZEKUNWU,SaarlandUniversity,SaarlandInformaticsCampus,Germany
IZAŠKRJANEC,SaarlandUniversity,LanguageScienceandTechnology,Germany
ANNAMARIAFEIT,SaarlandUniversity,SaarlandInformaticsCampus,Germany
Visualhighlightingcanguideuserattentionincomplexinterfaces.However,itseffectivenessunderlimited
attentionalcapacitiesisunderexplored.Thispaperexaminesthejointimpactofvisualhighlighting(permanent
anddynamic)anddual-task-inducedcognitiveloadongazebehaviour.Ouranalysis,usingeye-movement
datafrom27participantsviewing150uniquewebpagesrevealsthatwhileparticipants’abilitytoattendtoUI
elementsdecreaseswithincreasingcognitiveload,dynamicadaptations(i.e.,highlighting)remainattention-
grabbing.Thepresenceofthesefactorssignificantlyalterswhatpeopleattendtoandthuswhatissalient.
Accordingly,weshowthatstate-of-the-artsaliencymodelsincreasetheirperformancewhenaccounting
fordifferentcognitiveloads.Ourempiricalinsights,alongwithouropenlyavailabledataset,enhanceour
understandingofattentionalprocessesinUIsundervaryingcognitive(andperceptual)loadsandopenthe
doorfornewmodelsthatcanpredictuserattentionwhilemultitasking.
CCSConcepts:•Human-centeredcomputing→Userstudies;Usermodels;EmpiricalstudiesinHCI;
Web-basedinteraction.
AdditionalKeyWordsandPhrases:VisualAttention,CognitiveLoad,EyeTracking,ComputerVision,Saliency
Prediction
ACMReferenceFormat:
AnweshaDas,ZekunWu,IzaŠkrjanec,andAnnaMariaFeit.2024.ShiftingFocuswithHCEye:Exploringthe
DynamicsofVisualHighlightingandCognitiveLoadonUserAttentionandSaliencyPrediction.Proc.ACM
Hum.-Comput.Interact.8,ETRA,Article236(May2024),18pages.https://doi.org/10.1145/3655610
1 INTRODUCTION
Visualhighlightinghasbeenestablishedasatooltoguideusers’attentionincomplexuserinterfaces
(UI)bychangingvisualattributessuchascolour,hue,ormovement(e.g.[11,13,14,35]).Previous
researchhasfocusedondesigningvisualcuesthroughempiricalstudies[13,31,37,42,54].The
recentadvancementofdeep-learning-basedsaliencymodels,whichcanpredictfixationsofusers
basedonimagesofaUI[18,27],showpromisetoautomatethedesignandplacementofvisual
cues[33]inadaptiveinterfacesthatdirectauser’sattentiontoinformationrelevanttotheirtaskor
context[11,13].However,itisunclearwhethersuchimage-basedsaliencymodelscanpredictshifts
inuserattentionduetodynamicchangesintheUI.Moreover,weseeagapinunderstandingthe
Authors’addresses:AnweshaDas,adas@cs.uni-saarland.de,SaarlandUniversity,SaarlandInformaticsCampus,Saarbrücken,
Saarland,Germany,66123;ZekunWu,wuzekun@cs.uni-saarland.de,SaarlandUniversity,SaarlandInformaticsCampus,
Saarbrücken,Saarland,Germany,66123;IzaŠkrjanec,skrjanec@lst.uni-saarland.de,SaarlandUniversity,LanguageScience
andTechnology,Saarbrücken,Saarland,Germany,66123;AnnaMariaFeit,feit@cs.uni-saarland.de,SaarlandUniversity,
SaarlandInformaticsCampus,Saarbrücken,Saarland,Germany,66123.
ThisworkislicensedunderaCreativeCommonsAttribution-NonCommercial-ShareAlike4.0
InternationalLicense.
©2024Copyrightheldbytheowner/author(s).
ACM2573-0142/2024/5-ART236
https://doi.org/10.1145/3655610
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.
4202
rpA
22
]CH.sc[
1v23241.4042:viXra236:2 AnweshaDas,ZekunWu,IzaŠkrjanec,andAnnaMariaFeit
effectivenessofvisualhighlightinginrelationtotheuser’scognitivestate.Giventheprevalenceof
multitasking,thecognitiveloadinducedbyothertasksmayinfluencetheuser’sgazebehaviour[24]
andthustheefficacyofvisualhighlighting.
Inthispaper,weexplorethejointimpactofvisualhighlightingandcognitiveloadonusers’gaze
behaviourwhenviewingwebpages.Ourgoalistwofold:(1)toempiricallyunderstandhowthe
dynamicsofhighlightingandcognitiveloadaffectusers’viewingbehaviourofinterfacesandthe
effectivenessofvisualcues,and(2)toevaluatewhetherstate-of-the-artsaliencymodelscanpredict
usersviewingbehaviourofUIsundercognitiveloadandinthepresenceofvisualhighlighting.
Tothisend,wecollectanewdataset,HCEye,containinggazedatafrom27peoplelookingat
150differentwebsitescreenshots,displayedunderacombinationofdifferentconditions:withor
withouthighlighting,whichisshownpermanentlyorappearsdynamicallyafter3s;withoutany
secondarytaskorwhileperforminganon-visualsecondarytask(countingoutloud)thatinduced
eitherhighorlowcognitiveload.Giventhecomplexstudydesign,weconductathoroughstatistical
analysisofthecollecteddatausinggeneralizedlinearmixedmodels(GLMMs),examininghow
visualhighlightingandcognitiveloadjointlyimpactuserattention,disentanglingtheeffectdueto
confoundingfactors,suchasindividualdifferencesandUIdesigncomplexities,therebyaddressing
akeygapinexistingliterature.
Ourstudyreveals,thepresenceofvisualhighlightingonapreviouslyunnoticedregionofa
complexwebpagenotonlyincreaseditsnoticeabilitybutalsosignificantlyinfluencedparticipants’
gazebehaviourontheoverallwebpage.Participantsengagedlongerwiththehighlightedregion
than any other, even those that were ‘naturally’ salient like captivating images or titles and
thusexploredlessofthewebpage.Theirabilitytoexploreawidernumberofregionswasalso
impactedwhenundercognitiveload,duetolongerinformationprocessingtimes,alsoreducingthe
effectivenessofvisualhighlights.Yet,dynamichighlightingeffectivelycounteredthis,sustaining
thenoticeabilityofhighlightedregions.
Ourempiricalfindingsmotivatetheneedtodevelopcomputationalmodelsofvisualsaliency
thataretunedtowardsthecognitivestateofusersandthedynamicsoftheinterface.Therefore,we
implementastate-of-the-artsaliencymodel,SimpleNet[40]andshowthatbyadjustingtheinput
structureofthetrainingdataandtuningontheHCEyedatasetfromtherespectiveexperiment
condition,themodelsignificantlyincreasesthepredictiveperformanceofusers’fixationindifferent
highlightandcognitiveloads.
We hope that our dataset and findings will encourage future research to shift their focus to
developing predictive models that can account for dynamic content in UIs and the impact of
multitaskingwhichmightinducecognitiveload.ThesecouldpoweradaptiveUIsthatsupport
peopleinefficientlydistributingtheirattentionthroughdynamicvisualchanges.Insummary,this
papercontributes:
• HCEye,anovelgazedatasetof150uniquewebpagesviewedunderavarietyofconditions
coveringdifferenthighlightingtechniquesandcognitiveloads,overall1350stimuli.
• New empirical insights on the effects of visual highlighting and cognitive load on user
attention,disentangledfromotherconfoundingfactorsthroughstatisticalmodellingwith
GLMMs.
• Saliencymodelsthatpredictvisualattentiononwebpageswithdynamiccontentviewed
underdifferentcognitiveloads.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.ShiftingFocuswithHCEye 236:3
2 BACKGROUND
2.1 VisualCuesinUserInterfaces
Humanattentionalmechanismsprioritizespecificelementsfromincomingvisualstimulifordeeper
analysis,byeitherdirectingattentiontosalientregions(bottom-upattention)orbychoosingareas
thatareofinteresttotheviewerduetopriorknowledgeaboutastimulusorrelatedtask(top-down
attention)[44,55].Inbottom-upattention,theperipheralvisionplaysacrucialroleinguiding
attentionthroughbasiccueslikemovement,luminancechanges,andedgeidentification[52].In
contrast,top-downattentionstemsfromaperson’sgoalsorpriorexperiencewhichbiasesattention
towardsregionsaligningwiththeperson’sexpectations.Inpractice,humanattentiononastimulus
istheresultofbothmechanisms.WhilemostGUIlayoutsarealreadyoptimizedforuserexperience,
reflectingtop-downconsiderations,suchastargetsearch,morerecenteffortsinUIdesignaimto
guideattentiontoincomingrelevantinformationusingvisualcuesthatattractbottom-upattention
[8,11,13],likehighlightingAreasofInterest(AOIs)usingcolours(hue,intensity,andsaturation).
TheseprovedmoreeffectiveinmanipulatingthesaliencyofUIelementsthanothercues,suchas
changingshape,size,orposition[31,43],whilstalsoreducingreactiontimesfortargetidentification
insafetycriticalinterfaces[34,47].Weemploycolourhighlights[53]inourstudyandanalyze
howtheireffectivenessismodulatedbythedynamicsofthehighlightandthecognitiveloadofthe
viewer.
2.2 InfluenceofCognitiveLoadonVisualAttention
Oureyes,intricatelylinkedtothenervoussystem,provideinsightsintocognitiveprocessesand
internalstates[20].Whilstengaginginmentallydemandingtaskslikecomplexmathproblems,
observablevisualpatternsemerge—increasedblinking,frequentdownwardgazes,andreduced
fixationsindicatingheightenedcognitiveload[7,24,57].Undersuchheightenedloadnotonlydo
ourattentionalresourcessuffer,butitalsoaffectswhereandhowwedirectourvisualattention
[24].Asourfocusonataskintensifies,‘tunnelvision’mayoccur,impedingourabilitytoperceive
changesinperipheralvision—resemblingpeeringthroughatunnel.Thisalignswiththe‘general
interference’model,proposingthatinthisheightenedstateoffocus,theboundarybetweenattentive
andinattentivezonesbecomesunclear[41,61].Inthecaseofuserinterfaces,thisisespecially
relevantwhenmulti-tasking,asattentionaltunnelingcanreducesensitivitytovisualchanges[26]
andcauseustooverlookimportantinformation.Additionally,priorworkstudiedtheeffectof
cognitiveloadonvisualattentioninARandVRsettings[3,25,39,46,60].However,itsexploration
in the context ofthe effectiveness of visual cues,in particular while lookingat webpages, and
its impact on saliency prediction is limited. In this study, we, for the first time, explore these
phenomenatogethertodeterminehowtheyimpacttheeffectivenessofvisualcuesandtogether
modulatetheviewingbehaviour.
2.3 VisualSaliencyPrediction:DatasetsandModels
Visualsaliencypredictionhelpsforecastimageregionslikelytoattracthumanattention,treatingit
asabinarysegmentationtaskbyassigningasaliencyscoretoeachpixelbasedonitsattention-
grabbingpotential[30].Initially,bottom-upsaliencymodelsrelyingonlow-levelimagefeatures
wereused,butwithimprovingdeeplearningapproaches,data-drivensolutions[27,59]havecome
totheforefront.Meaningfulsaliencyprediction,thus,reliesonexpansiveeye-trackingdatasets
capturinghumangazepatternsandcomputationalmodelscapableoflearningfromthisdata.
While extensive research has explored salience in natural scenes [17, 19], gaze patterns in
UIcontexts,markedbyatop-leftfixationbiasandafocusontextoverimages,presentunique
challenges[18,27].Althoughexistingdatasets,FiWI[48]andUEyes[18]helpedoffertheabove
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.236:4 AnweshaDas,ZekunWu,IzaŠkrjanec,andAnnaMariaFeit
insightsintousergazebehaviouronanarrayofUIs,morecomprehensiveresearchonitremains
sparseespeciallynonethatconsidertheimpactofcognitiveload.Evenstate-of-the-artsaliency
modelsbasedonVGG-16[21,51]andResNet[15,29]trainedonlyondatasetswithstaticimagesof
naturalscenes,fallshortwhenhandlingdynamicUIinteractions;andsodovideo-basedsaliency
models[6,32],withcomplexarchitecturesdesignedforlongvideoswithhighframerates,which
appearexcessiveforpredictingdynamicbutsubtlechangesinUIs(e.g.,notificationsappearing).
Thisgapinexistingmodelsbecauseoftheabsenceofanydataonhowdynamicchangesinfluence
bottom-upattentionandtheeffectofcognitiveloademphasizestheneedfortheuseofsimpler
modelsinstudyingattentiondynamicsindynamicUIs.Thispaper,accompaniedbyournewdataset,
HCEye,whichprovideseye-trackingdatafromuserslookingatdynamichighlightsonUIs,under
thedemandsofvaryingcognitiveload,laysthefoundationforadvancedcomputationalmodelsto
predictuserattentioninreal-worldUIinteractions.
3 USERSTUDY
Thegoalofourempiricalstudyistounderstandhowusersattentionisinfluencedbydynamically
appearinghighlightsandtask-inducedcognitiveloads.Thus,ourstudymanipulatestwovariables:
the highlighting (‘Highlight’) of UI elements in the stimuli presented to participants, and the
cognitiveload(‘CognitiveLoad’)ofthesecondarytaskwhichparticipantsperformedwhilevisually
inspectingtheUIs.
3.1 Participants
Apilotstudywasconductedwith5volunteerstorefineourexperimentprocedure,interfaceand
materials.Forthemainstudy,werecruited30participantsthroughword-of-mouthreferralsand
universityadsbuthadtoexcludedatafrom3duetopoordataquality.Ourfinalparticipantgroup
included27individuals(𝑀
𝐴𝑔𝑒
=25.4±4.55;Range:20−37),consistingof12female,14male,and1
non-binaryparticipantsrepresenting13differentnationalities.Noneoftheparticipants,including
the17withvisualaids,hadahistoryofcolourblindnessorepilepsy.Beforeobtainingtheirwritten
consent,participantswereprovidedwithinformationaboutthestudy’sobjectives,theirrights,and
anypotentialrisks.Thestudytook45minutesonaverage,andparticipantswerepaid10EUR.
3.2 StimulusDesign
Weselectedwebpagesasasuitableproxyforuserinterfaces(UIs)tomaintainresearchconsistency,
given their shared characteristics with most other UIs.1 We chose two publicly available eye-
trackingdatasets,FiWI[48]andthemorerecentUEyes[18]fromwhichweselected150images,
excludingoverlysimpledesigns(basedontheFeatureCongestionFC)Score[45],asprovidedby
Oulasvirtaetal.[36]),designswithadarkbackground,explicitcontent,andnon-Englishtexts.All
imageswereresizedto2560×1440pixelsandpaddedwithwhiteborderstomaintaintheaspect
ratio.Oneachwebpageimage,weselectedanon-salientareatohighlight,basedonthefixation
mapprovidedbytherespectivedataset,i.e.aUIelementthatwasneverorseldomlookedatby
participants.Areaswerehighlightedbydrawingrectangularboxeswithayellowbackgroundand
aredborder,asshowninFigure1.
Basedonthese150imagesandselectedareas,wegeneratedthreestimulussetsusingdifferent
highlightingtechniques:(1)Absent:150imagesofwebpagesasintheoriginaldatasets,(2)Static:
thesameimagesasinAbsentbutoneareaispermanentlyhighlighted,(3)Dynamic:ananimated
1WebpagesexhibitsimilarcharacteristicsasmostUIs–complex,diverse,withnumerousattention-grabbingelements.This
choicehelpsmaintainconsistencywithrespecttovisualanglesetc.Theterms–interfaces,UIs,andwebpagesmaybeused
interchangeablythroughoutthispaper.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.ShiftingFocuswithHCEye 236:5
Fig.1. Wecontrolledthesizeandlocationofhighlightedareastobeequallydistributedacrossimages.Left:
Anexampleofasmall(11𝑐𝑚2)highlightinlocationQ2onalowclutter(𝐹𝐶 =5.05)UI.Right:Abig(33.3𝑐𝑚2)
highlightinQ4onacomplexUI(𝐹𝐶 =7.1).SeeFigure4andSupplementary(Supp.)Materialformoredetails.
gif whereanimagefromAbsentwasshownforthreesecondsafterwhichthesameimagefrom
Staticwasshownfortwoseconds,givingtheimpressionofadynamicallyappearinghighlighton
thewebpage.
3.3 StudyandTaskDesign
Participants’primarytaskwastolookat150images“naturallyas[they]wouldwhilesurfingthe
web”.Stimuliwerepresentedrandomlyoversixblocksof25imageseach.Eachimagewasdisplayed
for5𝑠,followedbya2𝑠 blankscreeninterval.Thestudyfollowedawithin-subjectdesignwith
twoindependentvariableswiththreelevelseach:HighlightingTechnique(HT)withAbsent,
Static,DynamicandCognitiveLoad(CL)withAbsent,Low,andHigh.HTcorrespondstothe
respectivesetofstimuli,asdescribedabove.Stimuliwererandomlydisplayedsothateachblock
containedstimulifromallHighlightingtechniques.CLdenotesthesecondarytaskparticipants
wereinstructedtoperform.Eitheritwasabsent,or,followingpriorwork[9,28],participantswere
askedtocountoutloudfrom0onwardinstepsof2,orbackwardsinstepsof7startingat800,
inducinglowandhighcognitiveloadcomparedtoAbsentconditionasconfirmedbyapost-study
questionnaireandastatisticalanalysisofthepupildilation(seeSupplementarymaterialfordetails).
Atthebeginningofeachblock,theinstructionforcountingwaspresentedonthescreenaswell
asverballybytheexperimenter.CognitiveLoadwasbalancedacrossparticipantsanditsorder
randomizedacrossblocks.ALatinsquaredesignwasusedtocreatebalancedlistsofimagessothat
eachparticipantviewedeachofthe150uniquewebpagesonceandexperiencedallnineconditions.
Accordingly,thedesignensuredthatallwebpageswerelookedatbyatleastthreeparticipants
underallHighlightingTechniquesHTandCognitiveLoadCLconditions.
3.4 ProcedureandApparatus
Afterfamiliarizingthemselveswiththeinterfacethroughtwoblocksofpracticetrials(viewing
stimulusimages,notpartofthemaintask,underHighandLowCL),participantsunderwenta
9-pointcalibrationontheTobiiProETMsoftware[1],immediatelyfollowedbya9-pointcalibration
verificationonourcustominterface.2 Aftersuccessfulcalibrationwithanaverageoffset< 0.5◦
(∼ 30px),theyproceededtothemaintasks,asdescribedabove,andfinishedwithapost-study
questionnaire(seesuppl.material).Participantswerecomfortablyseatedinastationarychairina
dimly-litroom,approximately65-75cmawayfroma25.5-inchmonitorwitharesolutionof2560×
1440pixels.RaweyegazewascapturedwithTobiiProFusion(250Hz),attachedatthebaseofthe
2BuiltusingHTML,JavaScript(JS),andFlaskv2.3.2;Pythonv3.8,tobii-research1.10.1
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.236:6 AnweshaDas,ZekunWu,IzaŠkrjanec,andAnnaMariaFeit
monitor,andsavedinCSVformatasprovidedbyTobiiSDK.Aftereachblock,participantstooka
shortbreakduringwhichthetwo-foldcalibrationwasrepeated.
3.5 DataProcessing
Foreachtimestamp,wesavegazepointsas(𝑥,𝑦)inscreencoordinatespace.Fordataprocessing,we
followedrecommendationssuggestedbyFeitetal.[12].Singleoutliersareidentifiedbycomparing
eachgazepointwithitsprecedingandsucceedingpoints.Ifthedeviationfromthesenearbypoints
exceeds1◦inx-or1.2◦iny-direction,thegazepointismarkedasnoiseandcorrectedbyreplacing
itwiththemedianofitsneighbours.Inter-samplevelocityisthencalculatedandutilizedtoidentify
potentialsaccades,allowingustoapplyaWeightedOn-OfffilterwithGaussiankernelsmoothing
[56]totherawgazedata.Thishelpsstabilizehigh-frequencydatawithoutintroducinganysaccade-
delaysorlatencies.Thesmoothedgazedataexhibitsahigherlevelofaccuracyandprecision3
compared to raw gaze data and is used to extract fixations. We implemented an algorithm for
fixationdetection,asproposedbyKumaretal.[22,23]usingPython.Inthiswaywecouldretain
99.3%ofthegazedata,excludingtrialswithdataloss>25%andgazetimeonscreen(GTS)<80%
(Hvelplund[16]define GTS=(cid:16)totalfixationduration(cid:17) ×100)asper-stimulusdataqualitymetrics.Theprocessed
totaltasktime
fixationdatawasthenusedtogeneratefixationmapsbysmoothingthemwitha2DGaussianfilter.
Weextractsalientregionsthatwerelookedatmost,bysegmentingtheseheatmapsandextracting
informationaboutthesalientregions’location,dimensions,andintensity.4
4 FINDINGS
4.1 DataAnalysis
Weusegeneralizedlinearmixed-effectsmodels(GLMM)toanalyzetheeye-trackingdatafromour
complexexperimentdesign,becauseitallowsustoaccountforrandomeffectsduetoindividual
variability,webpagedesigns,orlearningeffects,andisparticularlysuitedfornon-normaldata
and our repeated measures design ([5, 50]). We include a brief description of the eye-tracking
measures(responsevariables)intheGLMMsforeachpart,includingtheirobserveddistribution
(i.e.correspondingdistributionfamily).Weassumealoglinkbetweenpredictorsandresponse
variables.
ForSection4.2boththeHTandCLvariableswereencodedwiththethreelevelsusingHelmert
coding.ForHT,thefirstcomparisonisthatbetweenAbsenthighlightingandthemeanof Static
andDynamichighlighting.ThesecondcomparisonisbetweenStaticandDynamichighlighting.
ForCL,themodelfirstcomparesnoloadtothemeanof LowandHighload.Itssecondcomparison
isbetweenLowandHighload.AsidefromthemaineffectsofHTandCL,wealsoincludedtheir
interaction.WhenwenoticedsignificantinteractioneffectsfromtheGLMManalysis,Bonferroni-
correctedpost-hocpairwisecomparisonswereperformed.ForSections4.3and4.4weencodedHT
with2levelsStaticvs.Dynamicusingsumcoding;allowingustoreportthedirectcomparison
betweenthemeansofthesetwolevels.Inadditiontothesefixedeffects,eachregressionmodel
includedrandominterceptsbyparticipants,images,andblockindex,withrandomslopesforCL
andHTbyparticipant.WeconductedouranalysesinR[38]withlme4[2],whichyieldstvalues
fortheGammadistributionandzvaluesforother(PoissonorBinomial)distributions.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.ShiftingFocuswithHCEye 236:7
(c) The number of salient re-
(a) The total number of fixa- (b)Theaveragedurationoffix- gionsasgivenbyparticipants’
tionsontheimages. ationsontheimagesin𝑚𝑠. visualattention.
Fig.2. Differencesinviewingbehaviouracrosstheanalyzedconditions.Whitedotsandnumbersarethe
Mean,blackbarsthemedian.
4.2 ViewingBehaviourofWebpages
WefirstwantedtounderstandthejointimpactofHTandCLontheparticipant’sviewingbehaviour
ofthefullwebpage.Weanalyzedourdatabycomputingthefollowingmetrics(name,description,
distribution):
NumberofFixations NumberofFixationsinanAOIortheoverallimage Poisson
AverageFixationDuration AveragedurationofFixationsinanAOIorthewholeimage,in𝑚𝑠 Gamma
NumberofSalientRegions Overallnumberofregionsofinterest Poisson
Wefoundthatparticipantsmadefewerbutlongerfixationsinthepresenceofhighlightingand
withincreasingcognitiveload.Figure2givesanoverviewoftheresultsandspecificnumbers,
whichwediscussinthefollowing.
ThetotalnumberoffixationsacrosstheimageswaslargestinthecaseofAbsentHT,asconfirmed
bytheGLMManalysis(𝑧 =3.732,𝑝 <0.001).However,thetypeofhighlight(StaticHTvs.Dynamic
HT)didnotshowasignificanteffect(𝑧 =3.340,𝑝 =0.79).Thenumberoffixationswasalsoaffected
byCLwithGLMManalysisrevealingahighlysignificantdecreaseinfixationsfromAbsentCLto
thepresenceofany(𝑧 =3.732,𝑝 <0.001).FixationsfurtherdecreasedsignificantlyfromLowto
HighCL(𝑧 =3.577,𝑝 <0.001).
Asthenumberoffixationsreduced,theaveragedurationoffixationsacrosstheimagesincreased.
ParticipantsfixatedlongestunderHighCLcomparedtoLowandAbsentCL,wherethemedian
offixationdurationwas25𝑚𝑠 shorterinthelattercase.TheGLMMmodelshowsasignificant
difference between High CL and Low CL (𝑡 = −2.119,𝑝 = 0.034). However, with the Helmert
coding,nosignificanteffectisfoundwhencomparingAbsentCLvs.presenceofCL(Lowand
3Averageaccuracy=18.26𝑝𝑥(0.3◦)andaverageprecision=8.44𝑝𝑥(0.15◦)
4UsingOpenCV’sv4.7connectedComponentsWithStats
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.236:8 AnweshaDas,ZekunWu,IzaŠkrjanec,andAnnaMariaFeit
Fig. 4. The probability that a highlighted area is fixated across
different characteristics of the stimulus. We measure the visual
clutterofthestimulususingtheFeatureCongestionmetric,while
sizeisoperationalizedastheareacoveredbythehighlightincm
squared;andlocationiscategorizedbydividingthestimulusinto
Fig.3. ThepercentageoffixatedAOIs four quadrants: Quadrant 1 (Q1) represents the top-left, Q2 the
(highlightedareas)ineachcondition. top-right,Q3signifiesthebottom-left,andQ4thebottom-right.
High)(𝑡 =−0.716,𝑝 =0.47).Figure2indicatedanincreaseinthefixationdurationovertheimage
inthepresenceofavisualhighlightof10𝑚𝑠 comparedtoAbsentCL.However,thisdifferencewas
notsignificant(𝑡 =−1.672,𝑝 =0.095)andnosignificantinteractioneffectwasfoundbetweenCL
andHT.
Asaresultoftheobservationsaboveweseethatparticipantsexplorefewerpartsoftheimages
when under cognitive load and when a particular region of the image was highlighted. In the
AbsentCLcondition,thenumberofsalientregionsinanimage(extractedfromthecorresponding
heatmap asdescribed inSection 3.5)was the largest(𝑧 = 5.422,𝑝 < 0.0001). Comparing High
CL versus Low CL, participants looked at one region less, a statistically significant difference
(𝑧 =5.390,𝑝 <0.0001).Figure2alsoshowstheeffectofthepresenceofahighlightinthestimulus
imagewhichsignificantly(𝑧 =2.165,𝑝 =0.0303)reducedthenumberofsalientregionsfroman
imagewithoutanyhighlights(AbsentHT),comparedtoimageswithStaticHTandDynamicHT.
However,asignificantincreaseinthenumberofsalientregionsisalsoobservedbetweenStatic
HTandDynamicHT(𝑧 =−5.023,𝑝 <0.0001).
Insummary,weobservedthatthehighlightingtechnique(HT)affectedhowparticipantsviewed
thewebpage.Thepresenceofahighlightledtoamorefocusedgazebehaviour,resultinginfewer
andlongerfixations,comparedtotheAbsentHTcondition.Thetypeofhighlight(StaticorDynamic
HT)didnotmakeadifferenceinthenumberordurationoffixations.Nevertheless,thenumber
ofexploredregionswasmuchlargerintheDynamicHT,thantheStaticHT,whereparticipants
broadlyexploredthewebpagebeforethehighlightappearedafterthreeseconds.WhenunderHigh
CL,participantslookedatfewerregionsintheimage,makingfewerandlongerfixations.Inthe
caseof LowCL,theeffectwasnotasstrongandparticipantswereabletoexploreasimilarnumber
ofdifferentareasofthewebpage.Interestingly,nointeractioneffectwasfoundbetweenHTand
CL.
4.3 NoticeabilityofHighlights
Wenextwantedtounderstandwhatfactorsinfluencedparticipants’gazebehaviourindetecting
highlightedareasandhowtheyaffectedtheirbottom-upattention,basedonthefollowingmetrics
(name,description,distribution):
AOIhits(here,AOI=highlightedregion) Abinaryvalueindicatingwhetheranyfixationfellwithinaspec.AOI Binomial
TimetoFirstFixation Timefromhighlightonsettoinitialfixationinhighlightedregion Gamma
DistancefromLastFixation Distancefromlastfixationtofixationinhighlightedregion Gamma
4.3.1 HighlightandCognitiveLoadonAOI. Figure3summarizestheAOIhitsacrossconditions.
StaticHTtriggeredasubstantial55.2%riseinAOIhits,makingapreviouslyinconspicuousregion
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.ShiftingFocuswithHCEye 236:9
noticedbyparticipants.DynamicHTwasthemostattention-grabbingwith62.2%moreAOIhits
(𝑧 =−4.031,𝑝 <0.0001).ParticipantswerehinderedintheirabilitytonoticeanAOIwhenunder
HighCL.Therewasa13.1%decreaseinAOIhitscomparedtowhenunderLowCL,astatistically
significantdifference(𝑧 =4.675,𝑝 < 0.0001).However,thedifferenceinAOIhitsbetweenLowCL
andAbsentCLwasaminimal0.4%.WhileGLMManalysisconfirmedasignificanteffectinthe
presenceofanyCLcomparedtoAbsentCL,pairwisepost-hoctestscouldnotfindasignificant
differencebetweenAbsentHTandStaticHT(𝑧 =−0.93,𝑝 =1.00).Interestingly,whenthestimulus
imagehadeitherAbsentHTorStaticHT,participantsattendedto3.4%moreAOIswhilecounting
upinstepsof2comparedtofreelylookingatthewebpage(seeFigure3).Accordingly,intheStatic
HTcondition,thereisasignificantdifferencebetweenLowCLandHighCL(𝑧 =3.464,𝑝 =0.0016).
However,thisdifferenceisnotobservedintheDynamicHTcondition(𝑧 =0.987,𝑝 =0.97).
Tosummarize,ourfindingsindicatethatDynamicHTwasmosteffectiveinattractingpartici-
pants’attentiontootherwiseunnoticedareas.HighCLmarkedlyreducedthenumberofhitAOIsin
theStaticHTcondition.However,DynamicHTcounteractedthiseffectandparticipantsdetected
AOIsinbothCLconditionsequallywell.
4.3.2 StimulusCharacteristics. InSection3.2wedescribedourstimuluscuration,balancingvarious
characteristics:visualcomplexityoftheimage,highlightedregionsize,andlocation.Wewantedto
exploreifthesevariablesstronglyinfluencedtheprobabilityofparticipantsnoticingahighlighted
region.Fig.4suggeststhatnoneofthesevariablessignificantlyinfluencedthehighlightdetection
probability in our experimental setting. A GLMM model with these factors as the dependent
variables and a crossed-random effects was fit. The results (𝑝 > 0.1 for each) affirm that none
of themplayed aconsistently significantrolein thenoticeability ofa highlight, validatingthe
reliabilityofourexperimentaldesignandemphasizingtheprimaryroleofcontrolledfactorsin
explaininganyvariability.
4.3.3 DynamicHighlightandBottom-upAttention. InSection4.3.1,DynamicHTprovedtobe
moreeffectiveinattractingparticipants’attentiontowardsaUIregionthanStaticHT.Tobetter
understandthis,wecomparedDynamicvsStaticHTonhowquicklyandfromhowfarhighlighted
areaswerediscovered.
Figure5givesanoverviewofourfindings.RegionswithDynamicHTwerenotonlydiscovered
2.9timesquickerthanthosewithStaticHT,butDynamicHTalsoattractedparticipants’gazefrom
agreaterdistance.WefoundthattheTimetoFirstFixation(TtFF)wassignificantlyshorterfor
DynamiccomparedtoStaticHT(𝑡 =−19.780,𝑝 <0.0001),andtheDistancefromtheLastFixation
(DfLF)wassignificantlygreaterforDynamicHT(𝑡 =4.586,𝑝 <0.0001).
Participants’firstfixationonthehighlightedregionoccurredearliestunderAbsentCL,laterunder
LowCL,andonaverage120𝑚𝑠 laterunderHighCLcomparedtoAbsentCL.WhiletheTtFFinthe
presenceofCL(bothLow+High)didnotdiffersignificantlyfromAbsentCL(𝑡 =−0.777,𝑝 =0.437),
thepresenceof HighCLhadastatisticallysignificant(𝑡 =2.391,𝑝 =0.0168)effectonTfFF.Wealso
observedsignificantinteractioneffects(𝑡 =2.142,𝑝 =0.0322),andpost-hocanalysisrevealedthat
despitethepresenceof HighCL,therewerenostatisticallysignificantdifferencesintheTtFFwhen
theregionwasdynamicallyhighlighted;butwhentheregionhadStaticHTthereweresignificant
differencesintheTtFFbetweenLowvs.HighCL(𝑧 =−3.128,𝑝 =0.0053)andAbsentvs.HighCL
(𝑧 =−2.469,𝑝 =0.0407).
InFigure5weseethatparticipantswereabletotravelthelongestdistancefromtheirlastfixation
tothehighlightedregionwhennotunderanycognitiveload.Surprisingly,DfLFunderLowCLwas
amedianof20𝑝𝑥 lesserthanthatof HighCL.GLMManalysisrevealedthattheobserveddifference
inDfLFbetweenwhenparticipantswerenotunderanyCLcomparedtowhenitwaspresent(both
Low + High) was statistically significant (𝑡 = 2.950,𝑝 = 0.003). A significant interaction effect
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.236:10 AnweshaDas,ZekunWu,IzaŠkrjanec,andAnnaMariaFeit
Fig.5. Comparingnoticeablityofhighlightedregionsunderexperimentalconditionsandtheirfittedinterac-
tionplots.Row1comparesTimetoFirstFixationin𝑚𝑠,Row2theDistancefromtheLastFixation(previous
focusofattention)in𝑝𝑥.
(𝑡 = 2.336,𝑝 = 0.0194) can also be seen in Figure 5, and post-hoc analysis revealed significant
differencesbetweenAbsentvs.LowCLconditionsinthepresenceofStaticHT(𝑧 =3.342,𝑝 =0.002).
AswithTtFF,therewerenosignificantdifferencesacrossCLconditionsinthepresenceofDynamic
HT.
PuttingtheabovefindingstogetherwecanexplaintheeffectivenessofDynamicHTislargelydue
toittriggeringparticipants’bottom-upattentionmechanismsbymimicking‘movement’.Itledtoa
quicker,moreurgentdetection,showcasingattention-grabbingabilitiesevenoutsideparticipants’
fovealvision.TheabsenceofanysignificantdifferencesinTfFFandDfLFevenbetweenAbsentvs.
HighCLinregionswithDynamicHTcementsitsabilitytoremainnoticeable,copingwelleven
whenparticipantsareunderCL.
4.4 UnderstandingViewingBehaviourofHighlightedRegions
Finally,wewantedtoexplorewhetherahighlightedregionwaslookedatdifferentlyfromother
“naturally”salientregionsofawebpage,suchastitles,pictureswithfaces,buttons,etc.Therefore,
wecomparedtheFixationDuration(seeSection4.2)onahighlightedregiontothedurationon
theotherwisemostsalientregionofeachwebpage,acrossthethreeHTconditions.Todetermine
themostsalientregion,wecreatedfixationmaps,byaggregatingfixationsfromnineparticipants
acrossallCLconditionsandextractedsalientregions,asdescribedinSection3.5.UsingOpenCV,
wedeterminedthemostsalientregionastheonethatwasnotthehighlightedregionandwhich
hadthehighestintensityinthefixationmap,indicatingthatitwasfixatedonthelongest.
Figure6showsthat,while“naturally”salientregionswerelookedatlongerthanregionswith
nohighlight(AbsentHT),inthepresenceofavisualhighlight,thesehighlightedregions(AOIs
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.ShiftingFocuswithHCEye 236:11
withStaticandDynamicHT)werefixatedonlongerthanothersalientregions,nearlydoubling
FixationsDuration.WefitGLMMmodelsusingthesamemethoddescribedinSection4.1adding
sum-codedRegion(i.e.,comparedmeansofhighlightedvssalientregions)asanotherfixedeffect
alongwithHTandCL,withrandomslopesandinterceptsbyparticipants,images,andblockindex.
ThedifferenceinFixationDurationwasstatisticallysignificantlylonger(𝑡 =7.533,𝑝 <0.001)on
thehighlightedregionsacrossdifferentcognitiveloads,comparedtoothersalientregions.There
werenosignificantinteractioneffectsofHTorCLwithRegion.
Insummary,ourfindingssuggestthathighlightingcausesparticipantstoshifttheirattention
awayfromothersalientregionsofawebpage.AsillustratedinFigure7,attentiononregionswith
visualhighlightssignificantlyincreasedtoanextentthatparticipantsengagedlongerwiththe
highlightedareathananyotherareaonthewebpage.Asaresult,theyexploredfewerpartsofthe
webpage,aswealreadyobservedinSection4.2).
5 SALIENCYPREDICTION
Ourempiricalresultsshowedthatthepresenceofahighlightanditsdynamicssignificantlyaltered
the viewing behavior of webpages, not just for the highlighted region but also on the overall
image.Thecognitiveloadofparticipantsfurtherinfluencedtheirattentionontheimage.Sofar,
deep-learningbasedsaliencymodels,predictingtheusersfixationsfrompixel-leveldata,havenot
consideredthesemodulatingfactors.Ourgoalinthispartistoevaluatetheperformanceofexisting
modelsandcomparethemtoanimplementationbasedonourHCEyedataset.
Inselectingamodelforpredictingusersaliencyundervaryingexperimentalconditions,we
prioritized the balance between computational efficiency and predictive accuracy. Given that
HCEyeisthefirstsaliencydatasetthatexplicitlyconsidersHTandCL,weonlyhadlimiteddata
forfine-tuningexistingmodels.Thus,wechoseSimpleNet[40],characterizedbyitsarchitectural
simplicity,whichpreviouslydemonstratedproficiencyinidentifyingsalientfeatureswithindata-
richvisualizations[49].TheSimpleNetarchitectureincorporatesaProgressiveNeuralArchitecture
Search(PNAS)asitsencodercomponent,chosenforitshighefficacy.Themodel’simplementation
wascarriedoutusingPython3.9andthePyTorchv1.10.2framework.Thetrainingprocesswas
conductedonaworkstationequippedwithanInteli7-13700KFCPU,32GBofRAM,andanNVIDIA
GTX4080TiGPU.Forfine-tuningtheSimpleNetmodelasillustratedinthefollowingSections,the
inputimageswereresizedtoaresolutionof256×256pixels.Webegantrainingwithalearning
rateof1×10−4andscaleditdownbyafactorof0.1everyfiveepochs.Ourlossfunctioncombines
KL,CC,andNSSmetrics,asusedbypriorwork[40,49].OptimizationwasexecutedviatheADAM
algorithm.
Fig. 6. Fixation Duration on the High- Fig.7. ExampleFixationmapofparticipantsviewingaweb-
lighted Region (pink) compared to the pageintheAbsentHT(left)comparedtoStaticHTcondition.
most “naturally” Salient Region (blue) Thehighlightedareaattractsmoreattentionthananyother
acrossHTconditions. “naturally”salientregiononthewebpage.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.236:12 AnweshaDas,ZekunWu,IzaŠkrjanec,andAnnaMariaFeit
Inthefollowing,weexplaintheimplementationofoursaliencymodelswhicharebasedonthe
SimpleNetarchitecturepre-trainedontheSALICONdataset[17,49]andfine-tunedonsubsetsof
theHCEyedatasettoaccountforthedifferencesingazebehaviorduetohighlightingorcognitive
load.WecomparetheperformanceoftheseFine-tuned modelsagainsttwobaselines:(1)SALICON
denotestheSimpleNetmodelpre-trainedontheSALICONdataset[17,49]and(2)WEBdenotes
thatthemodelisfurtherfine-tunedonasubsetofourHCEyedatasetwhereCLandHTwereboth
absent,toadaptthemodeltothedomain(webpages).Thetwobaselinesallowtodisentanglethe
addedvaluethroughfine-tuningforthespecificdomainandforhighlightingandcognitiveload.
Wecomparethemodels’performanceonasetofwell-establishedmetrics[4]listedinTable1.A
higherscoreisbetterinallcasesexcepttheKLmetric.
5.1 SaliencyPredictionunderDifferentHighlightingTechniques
Wefirstexploredhowdynamicandstatichighlightingaffectsthepredictiveperformanceofmodels
trainedongazedatawithoutanyhighlighting(SALICON andWEB).Wedevelopedtwomodelsby
fine-tuningtheSALICON modelwithobservationsfromtheDynamicandStaticHT.Therefore,
wecreatedfixationmapsforeachHTconditionbyaggregatingthefixationpointsfromallusers
acrossallCLconditionsandsmoothingthemwitha2DGaussianfilter(𝑆𝐷 =35𝑝𝑥).Theresulting
dataset,HCEye_HT,comprises150stimuliforeachHTcondition.Foreachmodel,werandomly
splitthecorrespondingdataintoatrainingset(80%,120images)andatestingset(20%,30images).
Akeychallengeintrainingourownmodelwastocapturethetemporaldynamicsofthehighlight
intheDynamicHTcondition.Oursolutioninvolvedextractingapairofimagesfromeachdynamic
stimulus(withandwithoutthehighlight)andfeedingthemasstackedinputsintothenetwork.We
realizedthisthroughamodifiedinputlayerwithaspecializedchannelreductionconvolutionlayer.
Thisenablesthenetworktodiscernsalientfeaturedifferencesbetweentheimagepairsandthus
processthetemporaldynamics.MoredetailsareprovidedintheSupplementarymaterial.
Table 2 compares the performance of the two baseline models trained on data without any
highlightingtothefine-tunedmodelsintheStaticandDynamicconditions.FortheStaticHT
condition,thefine-tunedmodelachievesanotableimprovementintheCorrelationCoefficient(CC)
byapproximately28.97%,andintheNormalizedScanpathSaliency(NSS)byabout31.71%over
theSALICONbaseline.IntheDynamicHTcondition,enhancementsareparticularlysignificant
intheCC,withanincreaseof38.77%,andintheNSSmetric,wherethefine-tunedmodelshows
an impressive 81.23% improvement. We also evaluated our architecture for handling dynamic
highlights.Table3showsthesuperiorityofinputtingpairedimagescomparedtotrainingonlyon
Metric Description Range
AUC Measuresthemodel’seffectivenessasabi- [0,1]
naryclassifierforfixations
NSS Reflectstheaveragenormalizedsaliencyat [−∞,+∞] HT Model AUC CC NSS SIM KL
fixationpoints Static SALICON 0.819 0.466 1.094 0.468 1.126
SIM Assessesthematchbetweenthepredicted [0,1] WEB 0.835 0.532 1.269 0.505 0.889
saliencymapandtheactualfixationdistri- Fine-tuned 0.859 0.601 1.440 0.524 0.789
bution Dynamic SALICON 0.820 0.454 1.135 0.460 1.093
CC The linear relationship between the pre- [−1,1] WEB 0.863 0.512 1.468 0.491 0.910
dictedandactualfixationmaps Fine-tuned 0.900 0.630 2.057 0.540 0.749
KL Quantifiesthedifferencebetweenthepre- [0,+∞] Table2. ModelPerformanceunderdifferenthigh-
dictedsaliencydistributionandtheground
lights.
truth
Table1. Metricsforevaluatingsaliencymodels.See[4]
fordetails.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.ShiftingFocuswithHCEye 236:13
CL Model AUC CC NSS SIM KL
Low SALICON 0.875 0.356 1.229 0.325 1.689
Highlight ModelInput AUC CC NSS SIM KL
WEB 0.864 0.391 1.223 0.343 1.496
Highlighted 0.879 0.633 1.811 0.536 0.761
Fine-tuned 0.870 0.406 1.490 0.349 1.476
Dynamic Random 0.868 0.552 1.571 0.512 0.873
High SALICON 0.879 0.349 1.301 0.326 1.603
Pair 0.900 0.630 2.057 0.540 0.749
WEB 0.880 0.417 1.546 0.353 1.432
Table3. Comparisonofdifferentapproachesforin- Fine-tuned 0.887 0.440 1.411 0.367 1.411
puttingthedynamicstimulusduringfine-tuningof Table4. ModelPerformanceunderdifferentcog-
SimpleNet.Ourpairingapproachperformsbest. nitiveloads.
theimagewiththehighlight(highlightedinput)orrandomlychoosingtheimagewithorwithout
thehighlightduringthetrainingprocess(randominput).
Overall,ourfindingsindicate,thatagenericsaliencymodel(SALICON)cannotfullycapturethe
impactofa(dynamicallyappearing)highlightonauser’sattention,evenwhenfine-tunedonthe
imagedomain(WEB).Qualitativeexamplesaregiveninthesupplementarymaterial.
5.2 SaliencyPredictionunderCognitiveLoad
Wethenexploredtheimportanceofaccountingforthepresenceofcognitiveloadbyimplementing
individualmodelsfortheHighandLowCLconditions.Parallelingourapproachintheprevious
Section,wecreatedfixationsmapsforeachCLconditionbyaggregatingfixationpointsacross
usersinallHTconditions.Theresultingdataset,HCEye_CT,comprises150fixationmapsforeach
CLcondition.Wefine-tuneSimpleNetmodelsonthe150heatmapsofeachcondition,asbefore,
specializinginpredictingsaliencyasperthecognitivestateoftheuser.
Table 4 compares the performance of the two baseline models trained on data without any
secondary task (i.e. Absent CL) to the models fine-tuned on data from the Low and High CL
conditions.Thefine-tunedmodelsoutperformthebaselinesinmostmetrics.Inthecaseof LowCL
theNSSscorestandsoutwiththefine-tunedmodelshowinga21%higherperformancecompared
tobothbaselines.Interestingly,inothermetricsthedifferenceismuchsmallerandalsothetwo
baselinesperformquitesimilarly.TheHighCLconditionrevealsasimilartrend,wherethefine-
tunedmodeloutperformsbothbaselinesonmostofthemetrics,butonlybyasmallmargin.Inthis
case,theWEBmodelshowsaclearerperformanceimprovementcomparedtotheSALICON model.
ComparedtotheHTcondition,differencesinpredictiveperformancearenotaslargewhen
accountingforLoworHighCLinducedbysecondarytasks.Interestingly,both,theSALICON model
trainedonnaturalimagesandtheWEBmodelfine-tunedonwebpagesperformsimilarlywellin
predictingsaliencyinthiscondition.Thisvariationshowsthecomplexitiesofsaliencyprediction
across different scenarios: the presence of explicit visual variation such as visual highlighting
of inputs significantly influences model effectiveness, illustrating the challenges of accurately
capturinguserattentionshiftsdrivensolelybycognitiveloaddifferences.
6 DISCUSSIONANDCONCLUSION
Tothebestofourknowledge,thisisthefirststudytoexplorethejointimpactofbothdynamicand
staticvisualhighlighting,andcognitiveloadonusers’attentionwhenviewingcomplexinterfaces,
suchaswebpages.Intimeswheretechnologyconstantlycompetesforourattention,itisimportant
tounderstandwhichmechanismsaffectthedistributionofthislimitedresource.
Wecarefullydesignedafree-viewingtaskthatpresentedparticipantswithoverall150webpage
imagesindifferentconditions:intheiroriginaldesign,withapermanentlyhighlightedareaor
adynamicallyappearinghighlight.In4of6blocks,participantsperformedasecondarytaskof
countingoutloudwhichinducedeitherhighorlowcognitiveload,asconfirmedbyparticipants’
subjectivefeedback.Wecarefullydesignedthestimuliandperformedathoroughstatisticalanalysis
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.236:14 AnweshaDas,ZekunWu,IzaŠkrjanec,andAnnaMariaFeit
usingGLMMstoaccountforrandomeffects,e.g.duetoindividualdifferences,stimulusdesigns,or
learning.Thisenabledustoattributethechangesweobservedinvisualattentiontotheindependent
variablesinourexperiment:Highlightingtechniqueandcognitiveload.Inparticular,wemadethe
followingobservations:
Webpagesareexploredless,whenspecificcontentishighlighted,asindicatedbyfewerandlonger
fixationsovertheimage.However,theDynamichighlight,appearingonlyafterthreeseconds,
allowedparticipantstoattendasimilarnumberofimageregionsasfortheAbsentcondition.
Webpagesareexploredless,whenusersexperiencehighcognitiveload.Confirmingpriorwork[58],
participantsmadefewerandlongerfixationsunderHighCL,indicatingincreasedprocessingtime.
ThiswasnotthecaseunderLowCLwhereusersexploredasimilarnumberofregionscompared
totheAbsentcondition,despitemultitasking.
Dynamichighlightingattractsattentionefficientlyevenunderhighcognitiveload.Highlighted
areas were attended faster and from a larger distance from the periphery, in the Dynamic HT
condition.Inparticular,thiswasalsothecaseunderhighcognitiveload,whichintheStaticcase
ledtoaslowershiftofattention.
Highlightedregionsareengagedmorewiththananyothersalientregion,asindicatedbysignifi-
cantlymoreandlongerfixationsonthehighlightedregion,comparedtothemostlookedatregion
oftheoriginalwebpage.Thisindicates,thatparticipantsengagedwithhighlightedinformation
morethoroughly.
6.1 ShiftingtheFocusofSaliencyPredictiontoConsiderHighlightingandCognitive
Load
Ourempiricalfindingsmotivatedustoexplorewhetherstate-of-the-artsaliencymodelscanaccount
fortheimpactofhighlightingandcognitiveloadtousersattention.Weimplementedcondition-
specific saliency models based on a state-of-the-art architecture, SimpleNet, fine-tuned on our
dataset and achieved superior performance across all experimental conditions, compared to a
pre-trainedmodel.Inparticular,wepresentedthefirstimage-basedsaliencymodelthatcandeal
withtemporalchangesinaninterfaceduetodynamichighlighting.Weachievedthisbyfeeding
pairedimages(withandwithouthighlighting)asstackedinputsintothenetworkduringtraining.
Thisledtoasignificantimprovementinpredictiveperformance.
Importantly,ourresultshighlighttheneedforsaliencymodelstoexplicitlyincorporate(temporal)
highlightinformation,andpotentiallytailorthemtothespecificcognitivestateoftheviewer,to
achievemoreaccurateandrobustsaliencypredictions.Ourworkprovidesafoundationforfurther
research into saliency models that can deal with dynamic adaptations in interfaces, which are
increasinglycommon,inparticularonhead-worndevicessuchasaugmentedandvirtualreality
interfaces[10,28].TheHCEyedatasetcanalsoservefutureresearchtoinvestigatetheimpactof
CLandHTonotheraspectsofeyegaze,suchassaccadicbehaviorandcanbeusefulfordesigning
adaptiveordynamicinterfacesthatcandirectusers’attentiontorelevantinformation[11,33]
6.2 EthicalConsiderations,Limitations,andFutureWork
Whiledynamicadaptationssuchasvisualhighlightingoffersignificantpotentialtoenhanceuser
engagementandusabilitywithininterfaces,theycanalsobeusedtomanipulateusersattention
and behavior. Therefore, we do not allow commercial use of our data and call on the research
communitytopromotetheethicaluseofgazedatasetsandgazeresearch.Ontheotherhand,our
findings,andmoregenerallypredictivemodelsofvisualattention,couldservetohelpdesigners
employsuchfeaturesmoreresponsiblyandassesstheimpactonuserattentionalreadyatdesign
timetomaintaintheirusers’autonomyandtrust.Similarly,theycouldservetoautomaticallydetect
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.ShiftingFocuswithHCEye 236:15
deceptiveormanipulativedesignpracticesonwebpagesaspartofindependentauditingprocesses
ortodisplaywarningstousers.
Researchersutilizingthedatasetmustbemindfulofpotentialbiasesinthecollectedgazedata,
inparticulargiventhecomparativelysmallnumberofparticipants.Whileourparticipantpoolis
diverseintermsofgenderandnationality,thereislittlevarietyintheirageandeducationlevel.
We encourage future work to collect larger datasets with more users and more variety of UIs,
consideringalso3Dinterfaces,orothercognitiveaspectsthatmightaffectbottom-upattention.
Whilethesearecostlytoobtainforeyegaze,mouseinputhasbeenshownasaviableproxy[17]
forwhichourdatasetcouldserveasavalidationpoint.Followingthedataminimizationprinciple,
wedidnotcollectanyotherparticipant-relatedinformationandonlyreleaseanonymizedgaze
dataandnodemographicinformationtoprotectourparticipant’sprivacy.Thestudywasapproved
bytheethicalreviewboardoftheauthors’university.
6.3 TheHCEyeDataset
Wepublishanovelgazedatasetbasedon150uniquewebpageswhichwerepresentedeitherintheir
originaldesign,witharegionpermanentlyhighlightedorthehighlightappearingdynamically.
Eachstimuluswaslookedatunderthreecognitiveloadconditions(Absent,Low,andHigh).Asa
result,theHCEyedatasetconsistsofgazecoordinates,fixations,andsaliencymapsfor1350unique
stimuli:450stimuliviewedinthreecognitiveloadconditionseach,andcorrespondingly450viewed
inthreehighlightingconditions,withobservationsfromnineparticipantseach.TheHCEyedataset
isavailablefornon-commercialuseathttps://osf.io/x8p9b/,alongwiththeaccompanyingcode.
ACKNOWLEDGMENTS
ThisworkisfundedbyDFGgrant389792660aspartofTRR248–CPEC,seehttps://perspicuous-
computing.science
REFERENCES
[1] TobiiAB.[n.d.].TobiiProLab. https://www.tobii.com/
[2] DouglasBates,MartinMächler,BenBolker,andSteveWalker.2015.FittingLinearMixed-EffectsModelsUsinglme4.
JournalofStatisticalSoftware67,1(2015),1–48. https://doi.org/10.18637/jss.v067.i01
[3] JamesBaumeister,SeungYoubSsin,NevenAMElSayed,JillianDorrian,DavidPWebb,JamesAWalsh,TimothyM
Simon,AndrewIrlitti,RossTSmith,MarkKohler,etal.2017.Cognitivecostofusingaugmentedrealitydisplays.IEEE
transactionsonvisualizationandcomputergraphics23,11(2017),2378–2388.
[4] ZoyaBylinskii,TilkeJudd,AudeOliva,AntonioTorralba,andFrédoDurand.2018. Whatdodifferentevaluation
metricstellusaboutsaliencymodels? IEEEtransactionsonpatternanalysisandmachineintelligence41,3(2018),
740–757.
[5] LeenCatrysse,DavidGijbels,VincentDonche,SvenDeMaeyer,MarijeLesterhuis,andPietVandenBossche.2018.
Howarelearningstrategiesreflectedintheeyes?Combiningresultsfromself-reportsandeye-tracking.BritishJournal
ofEducationalPsychology88,1(2018),118–137.
[6] QinyaoChangandShipingZhu.2021.Temporal-spatialfeaturepyramidforvideosaliencydetection.arXivpreprint
arXiv:2105.04213(2021).
[7] SiyuanChenandJulienEpps.2023. AHigh-QualityLandmarkedInfraredEyeVideoDataset(IREye4Task):Eye
Behaviors,InsightsandBenchmarksforWearableMentalStateAnalysis.IEEETransactionsonAffectiveComputing
(2023).
[8] BjörnBDeKoning,HuibKTabbers,RemyMJPRikers,andFredPaas.2009.Towardsaframeworkforattentioncueing
ininstructionalanimations:Guidelinesforresearchanddesign.EducationalPsychologyReview21(2009),113–140.
[9] AndrewT.Duchowski,KrzysztofKrejtz,IzabelaKrejtz,CezaryBiele,AnnaNiedzielska,PeterKiefer,MartinRaubal,
andIoannisGiannopoulos.2018.TheIndexofPupillaryActivity:MeasuringCognitiveLoadVis-à-VisTaskDifficulty
withPupilOscillation.InProceedingsofthe2018CHIConferenceonHumanFactorsinComputingSystems(Montreal
QC,Canada)(CHI’18).AssociationforComputingMachinery,NewYork,NY,USA,1–13. https://doi.org/10.1145/
3173574.3173856
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.236:16 AnweshaDas,ZekunWu,IzaŠkrjanec,andAnnaMariaFeit
[10] JoãoMarceloEvangelistaBelo,MathiasN.Lystbæk,AnnaMariaFeit,KenPfeuffer,PeterKán,AnttiOulasvirta,and
KajGrønbæk.2022.AUIT–theAdaptiveUserInterfacesToolkitforDesigningXRApplications.InProceedingsofthe
35thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology(Bend,OR,USA)(UIST’22).Associationfor
ComputingMachinery,NewYork,NY,USA,Article48,16pages. https://doi.org/10.1145/3526113.3545651
[11] AnnaMariaFeit,LukasVordemann,SeonwookPark,CaterinaBerube,andOtmarHilliges.2020.DetectingRelevance
duringDecision-MakingfromEyeMovementsforUIAdaptation.InACMSymposiumonEyeTrackingResearchand
Applications(Stuttgart,Germany)(ETRA’20FullPapers).AssociationforComputingMachinery,NewYork,NY,USA,
Article10,11pages. https://doi.org/10.1145/3379155.3391321
[12] AnnaMariaFeit,ShaneWilliams,ArturoToledo,AnnParadiso,HarishKulkarni,ShaunKane,andMeredithRingel
Morris.2017.TowardEverydayGazeInput:AccuracyandPrecisionofEyeTrackingandImplicationsforDesign.In
Proceedingsofthe2017CHIConferenceonHumanFactorsinComputingSystems(Denver,Colorado,USA)(CHI’17).
AssociationforComputingMachinery,NewYork,NY,USA,1118–1130. https://doi.org/10.1145/3025453.3025599
[13] LeahFindlaterandKrzysztofZ.Gajos.2009. Designspaceandevaluationchallengesofadaptivegraphicaluser
interfaces.AIMagazine30(2009),68–73.Issue4. https://doi.org/10.1609/aimag.v30i4.2268
[14] DonaldL.Fisher,BruceG.Coury,TammyO.Tengs,andSusanA.Duffy.1989.MinimizingtheTimetoSearchVisual
Displays:TheRoleofHighlighting.HumanFactors31,2(1989),167–182. https://doi.org/10.1177/001872088903100206
arXiv:https://doi.org/10.1177/001872088903100206PMID:2744770.
[15] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresiduallearningforimagerecognition.In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.770–778.
[16] KristianTangsgaardHvelplund.2014. Eyetrackingandthetranslationprocess:Reflectionsontheanalysisand
interpretationofeye-trackingdata.(2014).
[17] MingJiang,ShengshengHuang,JuanyongDuan,andQiZhao.2015.Salicon:Saliencyincontext.InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition.1072–1080.
[18] YueJiang,LuisALeiva,HamedRezazadeganTavakoli,PaulRBHoussel,JuliaKylmälä,andAnttiOulasvirta.2023.
UEyes:UnderstandingVisualSaliencyacrossUserInterfaceTypes.InProceedingsofthe2023CHIConferenceonHuman
FactorsinComputingSystems.1–21.
[19] TilkeJudd,KristaEhinger,FrédoDurand,andAntonioTorralba.2009.Learningtopredictwherehumanslook.In2009
IEEE12thinternationalconferenceoncomputervision.IEEE,2106–2113.
[20] ArthurFKramer.2020.Physiologicalmetricsofmentalworkload:Areviewofrecentprogress.Multipletaskperformance
(2020),279–328.
[21] SrinivasSSKruthiventi,KumarAyush,andRVenkateshBabu.2017.Deepfix:Afullyconvolutionalneuralnetworkfor
predictinghumaneyefixations.IEEETransactionsonImageProcessing26,9(2017),4446–4456.
[22] ManuKumar.2007.GUIDesaccadedetectionandsmoothingalgorithm.TechnicalRep.StanfordCSTR3(2007),2007.
[23] ManuKumar,JeffKlingner,RohanPuranik,TerryWinograd,andAndreasPaepcke.2008.Improvingtheaccuracyof
gazeinputforinteraction.InProceedingsofthe2008symposiumonEyetrackingresearch&applications.65–68.
[24] NilliLavie.2005. Distractedandconfused?:Selectiveattentionunderload. Trendsincognitivesciences9,2(2005),
75–82.
[25] JieunLee,NahyunLee,JangkyuJu,JihwanChae,JiyoonPark,HoeSungRyu,andYangSeokCho.2023.VisualComplex-
ityofHead-UpDisplayinAutomobilesModulatesAttentionalTunneling.HumanFactors(2023),00187208231181496.
[26] Yi-ChingLee,JohnDLee,andLindaNgBoyle.2007. Visualattentionindriving:Theeffectsofcognitiveloadand
visualdisruption.HumanFactors49,4(2007),721–733.
[27] LuisALeiva,YunfeiXue,AvyaBansal,HamedRTavakoli,TuðçeKöroðlu,JingzhouDu,NirajRDayama,and
AnttiOulasvirta.2020.Understandingvisualsaliencyinmobileuserinterfaces.In22ndInternationalconferenceon
human-computerinteractionwithmobiledevicesandservices.1–12.
[28] DavidLindlbauer,AnnaMariaFeit,andOtmarHilliges.2019. Context-AwareOnlineAdaptationofMixedReality
Interfaces.InProceedingsofthe32ndAnnualACMSymposiumonUserInterfaceSoftwareandTechnology(NewOrleans,
LA,USA)(UIST’19).AssociationforComputingMachinery,NewYork,NY,USA,147–160. https://doi.org/10.1145/
3332165.3347945
[29] NianLiuandJunweiHan.2018.Adeepspatialcontextuallong-termrecurrentconvolutionalnetworkforsaliency
detection.IEEETransactionsonImageProcessing27,7(2018),3264–3274.
[30] TieLiu,ZejianYuan,JianSun,JingdongWang,NanningZheng,XiaoouTang,andHeung-YeungShum.2010.Learning
todetectasalientobject.IEEETransactionsonPatternanalysisandmachineintelligence33,2(2010),353–367.
[31] AristidesMairena,CarlGutwin,andAndyCockburn.2022.Whichemphasistechniquetouse?Perceptionofemphasis
techniqueswithvaryingdistractors,backgrounds,andvisualizationtypes. InformationVisualization21,2(2022),
95–129.
[32] KyleMinandJasonJCorso.2019. Tased-net:Temporally-aggregatingspatialencoder-decodernetworkforvideo
saliencydetection.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.2394–2403.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.ShiftingFocuswithHCEye 236:17
[33] PhilippMüller,SanderStaal,MihaiBâce,andAndreasBulling.2022.DesigningforNoticeability:Understandingthe
ImpactofVisualImportanceonDesktopNotifications.InProceedingsofthe2022CHIConferenceonHumanFactorsin
ComputingSystems(NewOrleans,LA,USA)(CHI’22).AssociationforComputingMachinery,NewYork,NY,USA,
Article472,13pages. https://doi.org/10.1145/3491102.3501954
[34] MaxNicosiaandPerOlaKristensson.2021.DesignPrinciplesforAI-AssistedAttentionAwareSystemsinHuman-in-
the-LoopSafetyCriticalApplications.InEngineeringArtificiallyIntelligentSystems:ASystemsEngineeringApproachto
RealizingSynergisticCapabilities.Springer,230–246.
[35] MorinOstkamp,GernotBauer,andChristianKray.2012.VisualHighlightingonPublicDisplays.InProceedingsof
the2012InternationalSymposiumonPervasiveDisplays(Porto,Portugal)(PerDis’12).AssociationforComputing
Machinery,NewYork,NY,USA,Article2,6pages. https://doi.org/10.1145/2307798.2307800
[36] AnttiOulasvirta,SamuliDePascale,JaninKoch,ThomasLangerak,JussiJokinen,KashyapTodi,MarkkuLaine,Manoj
Kristhombuge,YuxiZhu,AliakseiMiniukovich,etal.2018.AaltoInterfaceMetrics(AIM)AServiceandCodebasefor
ComputationalGUIEvaluation.InAdjunctProceedingsofthe31stAnnualACMSymposiumonUserInterfaceSoftware
andTechnology.16–19.
[37] GeerdPhilipsen.1994. Effectsofsixdifferenthighlightingmodesonvisualsearchperformanceinmenuoptions.
InternationalJournalofHuman–ComputerInteraction6,3(1994),319–335. https://doi.org/10.1080/10447319409526098
arXiv:https://doi.org/10.1080/10447319409526098
[38] RCoreTeam.2022.R:ALanguageandEnvironmentforStatisticalComputing.RFoundationforStatisticalComputing,
Vienna,Austria. https://www.R-project.org/
[39] EsaMRantanenandJosephHGoldberg.1999. Theeffectofmentalworkloadonthevisualfieldsizeandshape.
Ergonomics42,6(1999),816–834.
[40] NavyasriReddy,SamyakJain,PradeepYarlagadda,andVineetGandhi.[n.d.]. Tidyingdeepsaliencyprediction
architectures.In2020IEEE.InRSJInternationalConferenceonIntelligentRobotsandSystems(IROS).10241–10247.
[41] RyanVRinger,ZacharyThroneburg,AaronPJohnson,ArthurFKramer,andLesterCLoschky.2016.Impairingthe
usefulfieldofviewinnaturalscenes:Tunnelvisionversusgeneralinterference.JournalofVision16,2(2016),7–7.
[42] AnthonyC.Robinson.2011.HighlightinginGeovisualization.CartographyandGeographicInformationScience38,4
(2011),373–383. https://doi.org/10.1559/15230406384373arXiv:https://doi.org/10.1559/15230406384373
[43] AnthonyCRobinson.2011.Highlightingingeovisualization.CartographyandGeographicInformationScience38,4
(2011),373–383.
[44] RuthRosenholtz,JieHuang,andKristaAEhinger.2012.Rethinkingtheroleoftop-downattentioninvision:Effects
attributabletoalossyrepresentationinperipheralvision.Frontiersinpsychology3(2012),13.
[45] RuthRosenholtz,YuanzhenLi,JonathanMansfield,andZhenlanJin.2005.Featurecongestion:ameasureofdisplay
clutter.InProceedingsoftheSIGCHIconferenceonHumanfactorsincomputingsystems.761–770.
[46] JulieSaint-Lot,Jean-PaulImbert,andFrédéricDehais.2020. Redalert:acognitivecountermeasuretomitigate
attentionaltunneling.InProceedingsofthe2020CHIConferenceonHumanFactorsinComputingSystems.1–6.
[47] AngelaTSchriver,DanielGMorrow,ChristopherDWickens,andDonaldATalleur.2017.Expertisedifferencesin
attentionalstrategiesrelatedtopilotdecisionmaking.InDecisionmakinginaviation.Routledge,371–386.
[48] ChengyaoShenandQiZhao.2014. Webpagesaliency.InComputerVision–ECCV2014:13thEuropeanConference,
Zurich,Switzerland,September6-12,2014,Proceedings,PartVII13.Springer,33–46.
[49] SungbokShin,SunghyoChung,SanghyunHong,andNiklasElmqvist.2022. Ascannerdeeply:Predictinggaze
heatmapsonvisualizationsusingcrowdsourcedeyemovementdata.IEEETransactionsonVisualizationandComputer
Graphics29,1(2022),396–406.
[50] BrenoBSilva,DavidOrrego-Carmona,andAgnieszkaSzarkowska.2022.Usinglinearmixedmodelstoanalyzedata
fromeye-trackingresearchonsubtitling.TranslationSpaces11,1(2022),60–88.
[51] KarenSimonyanandAndrewZisserman.2014.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition.
arXivpreprintarXiv:1409.1556(2014).
[52] HansStrasburger,IngoRentschler,andMartinJüttner.2011. Peripheralvisionandpatternrecognition:Areview.
Journalofvision11,5(2011),13–13.
[53] HendrikStrobelt,DanielaOelke,BumChulKwon,TobiasSchreck,andHanspeterPfister.2015.Guidelinesforeffective
usageoftexthighlightingtechniques.IEEEtransactionsonvisualizationandcomputergraphics22,1(2015),489–498.
[54] HendrikStrobelt,DanielaOelke,BumChulKwon,TobiasSchreck,andHanspeterPfister.2016.GuidelinesforEffective
UsageofTextHighlightingTechniques.IEEETransactionsonVisualizationandComputerGraphics22,1(2016),489–498.
https://doi.org/10.1109/TVCG.2015.2467759
[55] AnneMTreismanandGarryGelade.1980.Afeature-integrationtheoryofattention.Cognitivepsychology12,1(1980),
97–136.
[56] OlegŠpakov.2012.ComparisonofEyeMovementFiltersUsedinHCI.InProceedingsoftheSymposiumonEyeTracking
ResearchandApplications(SantaBarbara,California)(ETRA’12).AssociationforComputingMachinery,NewYork,
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.236:18 AnweshaDas,ZekunWu,IzaŠkrjanec,andAnnaMariaFeit
NY,USA,281–284. https://doi.org/10.1145/2168556.2168616
[57] KerriWalterandPeterBex.2021.Cognitiveloadinfluencesoculomotorbehaviorinnaturalscenes.ScientificReports
11,1(2021),12405.
[58] KerriWalterandPeterBex.2022.Low-levelfactorsincreasegaze-guidanceundercognitiveload:Acomparisonof
image-salienceandsemantic-saliencemodels.Plosone17,11(2022),e0277691.
[59] ZhengWang,JinchangRen,DongZhang,MeijunSun,andJianminJiang.2018.Adeep-learningbasedfeaturehybrid
frameworkforspatiotemporalsaliencydetectioninsidevideos.Neurocomputing287(2018),68–83.
[60] ChristopherDWickensandAmyLAlexander.2009.Attentionaltunnelingandtaskmanagementinsyntheticvision
displays.Theinternationaljournalofaviationpsychology19,2(2009),182–199.
[61] RichardYoung.2012.Cognitivedistractionwhiledriving:Acriticalreviewofdefinitionsandprevalenceincrashes.
SAEInternationalJournalofPassengerCars-ElectronicandElectricalSystems5,2012-01-0967(2012),326–342.
ReceivedNovember2023;revisedJanuary2024;acceptedMarch2024
Proc.ACMHum.-Comput.Interact.,Vol.8,No.ETRA,Article236.Publicationdate:May2024.