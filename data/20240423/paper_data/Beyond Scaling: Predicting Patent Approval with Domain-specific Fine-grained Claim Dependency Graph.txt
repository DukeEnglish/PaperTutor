Beyond Scaling: Predicting Patent Approval with Domain-specific
Fine-grained Claim Dependency Graph
XiaochenKevGao1∗,FengYao1∗,KewenZhao2,BeileiHe3,
AnimeshKumar1,VishKrishnan1,JingboShang1
1 UniversityofCaliforniaSanDiego,
2 CarnegieMellonUniversity,3 UniversityofPennsylvania
{xig034,fengyao,ank028,vkrishnan,jshang}@ucsd.edu
{kewenz}@cs.cmu.edu, {beilei}@seas.upenn.edu
Abstract
Patent Text LLM
Applications
Modelscalingisbecomingthedefaultchoice Claim 1 APPROVED
for many language tasks due to the success
Claim 2 REJECTED
of large language models (LLMs). However, Body
❌
it can fall short in specific scenarios where Text ... ...
simplecustomizedmethodsexcel. Inthispa- ...
per, we delve into the patent approval pre- Claim 1 Claim N REJECTED
Claim 2
diction task and unveil that simple domain- ... expensive
specificgraphmethodsoutperformenlarging Claim N Adopt plain text -> poor performance
the model, using the intrinsic dependencies
Build dependency graph -> better performance
within the patent data. Specifically, we first Claim 2
extend the embedding-based state-of-the-art split Graph Model
(SOTA)byscalingupitsbackbonemodelwith s1s2s3s4s5 s1
various sizes of open-source LLMs, then ex- s2 Graph s3 APPROVED
plore prompt-based methods to harness pro- build graph ✅
prietaryLLMs’potential,butfindthebestre- s4
s5 cost-effective
sultsclosetorandomguessing,underliningthe
Figure 1: An illustration for the patent approval pre-
ineffectiveness of model scaling-up. Hence,
diction task approached by LLMs and graph models,
we propose a novel Fine-grained cLAim de-
whereeachnodeofthegraphisaninformativesegment
peNdency(FLAN)Graphthroughmeticulous
decomposedfromtheoriginalclaimtext.
patent data analyses, capturing the inherent
dependencies across segments of the patent
text. As it is model-agnostic, we apply cost-
effective graph models to our FLAN Graph bonemodelstolargelanguagemodels(LLMs)may
to obtain representations for approval predic- notguaranteesuccess(Pengetal.,2023;Houetal.,
tion. Extensiveexperimentsanddetailedanaly- 2023; Wang et al., 2023). In addition, scaling up
sesprovethatincorporatingFLANGraphvia models imposes demanding computational costs
variousgraphmodelsconsistentlyoutperforms
thatpreventitfrombeingwidelyadoptedforreal-
allLLMbaselinessignificantly. Wehopethat
world applications. Such limitations necessitate
ourobservationsandanalysesinthispapercan
cost-effectivemethodsbeyondscaling,especially
bring more attention to this challenging task
fordomain-specifictasksthathavedistincttraits.
and prompt further research into the limita-
tions of LLMs. Our source code and dataset Inthispaper,welookintothetaskofpatentap-
can be obtained from https://github.com/ provalprediction,achallengingyetstraightforward
ShangDataLab/FLAN-Graph.
classificationtaskthatscalingstrugglestoaddress,
andexplorecustomizedcost-effectivesolutions. As
1 Introduction
showninFigure1,theobjectiveistodetermineif
Scalinguplanguagemodelshasdemonstratedpre- eachclaiminapatentapplicationwillbeapproved
dictableimprovementandunprecedentedabilities or rejected by the U.S. government patent office
in many language tasks (Chung et al., 2022; Wei (USPTO). It is vital for intellectual property (IP)
etal.,2022a;Zhaoetal.,2023). However,emerg- protection,takingupto40%oftheU.S.GDPand
ing evidence shows that simply scaling up back- over30%ofemployment. Duetothedemanding
requirements for knowledge in both technology
∗The first two authors contributed equally to this paper.
Thelistingorderisfullydecidedbydicerolling. andlaw,patentexaminationisconductedmanually,
1
4202
rpA
22
]LC.sc[
1v27341.4042:viXraleadingtopotentialinconsistentoutcomesacross
Claim 1: A system for [...], the system compromising:
patent examiners (O’Neill, 2018; USPTO, 2016). an authentication component configured to [...];
a tracking component configured to [...];
Such inconsistency underscores the necessity for
and a control component configured to:
objectiveandautomatedcomputationalsupport. receive authentication information [...];
receive location information from [...];
Thestate-of-the-art(SOTA)ofthistaskisbased and deliver a message to the touchpoint authorizing [...].
on BERT embedding (Devlin et al., 2019) aug-
Claim 2: The system of claim 1, where the control component
mentedwithhandcraftedfeatures(Gaoetal.,2022). is also configured [...].
Anintuitiveideaistoreplaceitsbackbonemodel Claim 3: The system of claim 1, where the authentication
component includes [...].
with modern LLMs. To this end, we employ
LLaMA2 (Touvron et al., 2023), Mistral (Jiang
Figure2: Abriefexampleofthetypicalpatentclaim
etal.,2023),Vicuna(Chiangetal.,2023)invarious
writing style and hierarchical dependencies within
sizes(7&13&70B)andapplybothLoRA(Huetal., claimsfromareal-worldpatentapplication.
2022)andfullfine-tuning. Surprisingly,theydonot
liveuptoexpectations,performingonparorworse
SOTA,amongwhichGraphSage(Hamiltonetal.,
thanBERT.ToexploitLLMs’emergentabilities,
2017)achievesthehighestimprovementsof7.4%
weutilizeprompt-basedmethodstailoredtothese
inAUCand7.8%inMacro-F1scores, achieving
open-sourceLLMs,aswellasclosed-sourceGPT-
absolutescoresof66.04and58.22,respectively.
3.5 (OpenAI, 2022) and GPT-4 (OpenAI, 2023),
buttheresultsarestillunsatisfying. To summarize, our contributions are two-fold:
TheshatteredhopeinLLMsmotivatesustodive (1)Weproposeanovelalgorithmtoautomatically
intopatentdataanalyses,whichleadsustothestan- construct the Fine-grained cLAim depeNdency
dardized writing of claims and the dependencies (FLAN)Graphatscalethatconsistentlyimproves
natureamongthem. AsdepictedinFigure2,Claim theSOTAbyalargemargin. (2)Weconductcom-
1 compromises multiple sub-components, which prehensive experiments and analyses of modern
are then referenced in subsequent claims. Such LLMsonpatentapprovalprediction,whichiden-
intricateinner-claim(betweensub-componentsin tifythelimitationsofLLMsandprovidevaluable
Claim 1) and inter-claim dependencies (between referencesfordevelopingLLM-basedsolutionsin
Claims1&2aswellasClaims1&3)havecritical thefuture. Thesourcecode1 anddataset2 arepub-
implicationsforthepatentapprovalpredictiontask, liclyreleasedtofacilitatefutureresearch.
as the patent examination is conducted on each
2 ProblemFormulation
claimandtherejectionofoneclaimcanresultin
theautomaticrejectionofitsdependents.
Inthissection,weformallyintroducethedefinition
Inspiredbytheobservationsanddomain-specific
ofthepatentapprovalpredictiontaskandanalyze
knowledgeacquiredfrompainstakinglyextensive
thedatasetweconstructforexperiments.
dataanalyses,weproposeFine-grainedcLAimde-
peNdency(FLAN)Graphforpatentapprovalpre- 2.1 TaskDefinition
diction, which represents each claim by a single
As illustrated in Figure 1, patent applications are
graphthatencapsulatesbothinner-andinter-claim
initially submitted to the USPTO in the form of
dependencies. Specifically,asshowninFigure1,
documents. The examination process, however,
wefirstdesignanovelalgorithmtoautomatically
focuses on approving or rejecting each individ-
construct the FLAN Graph at scale, where each
ual claim. Therefore, given a patent application
node is an informative segment of the claim text.
A = {C(i) }n containing n claims, the task of
Then,themodel-agnosticFLANGraphisfedinto i j j=1
patentapprovalpredictionistodeterminewhether
agenericgraphmodelforprediction. Examplesof
(i)
eachclaimC willbeapprovedorrejectedbythe
the FLAN graphs and the corresponding claims j
(i)
are shown in Appendix C. In the experiments, USPTOindicatedbyabinarylabely ∈ {0,1}.
j
weadoptavarietyofcost-effectivegraphmodels In practice, patent claims are reviewed accord-
such as GCN (Chen et al., 2020), GAT (Velick- ingtothelegalsection35U.S.Code§102,where
ovicetal.,2018),andTreeLSTM(Taietal.,2015) thecorecriterionisnovelty-based,bringingindis-
toverifytheeffectivenessofourproposedFLAN
1https://github.com/ShangDataLab/FLAN-Graph
Graphforpatentapprovalprediction. Allmodels
2https://huggingface.co/datasets/
withFLANGraphappliedoutperformtheprevious ShangDataLab-UCSD/PatentAP
2#Claim #Application Approval(%) 3.1 Observations
Train 1,485,693 87,883 81.36 In principle, patent claims are filed to seek legal
Valid 278,215 16,955 83.41
Test 185,477 11,148 84.92 protectionforcomplexsystemsthatusuallycom-
promise multiple (sub-)components. Sometimes,
Table 1: Statistics of PATENTAP dataset. “Approval
claims consisting of the same (sub-)components,
(%)”indicatesthepercentageoftheapprovedclaims.
but with different arrangements or combinations
ofthem,canreceiveoppositenoveltyassessments.
Therefore,claimsinpatentapplicationsarestrategi-
tinctchallengesbelow. (1)Time-sensitive. Unlike
callystructured,sequenced,andoftenarrangedin
traditional text classification, novelty assessment
clusters,eachdescribingsubtlydifferentvariants.
dependsontheapplicationfilingdate,allowingop-
Inthiscase,weidentifytwotypesofdependency
positedecisionsforthesameclaimovertime. (2)
relationshipsacrossvariouspatentclaimsthatmay
Structure-dependent. Manyclaims(e.g.,Claims
influencetheoutcomesofnoveltyexamination.
2&3 in Figure 2) are dependent on others within
thesameapplication,andsuchstructurecaninflu- Inner-claimDependency. Somelengthyclaims
encenoveltyevaluation. (3)Knowledge-intensive.
areinternallyhierarchicalbyexplicitlydescribing
Evaluatingnoveltyrequiresup-to-dateknowledge
a system having multiple (sub-)components. For
ofbothtechnologiesandpatentlaw. (4)Outcome- instance, Claim 1 in Figure 2 is about a system
inconsitent. Novelty examination outcomes are
that has three components, of which the control
subjecttopreferencesacrosspatentofficers,which
componentisfurtherdescribedashavingfourpur-
mayintroduceinconsistenciesinpatentdata.
poses(sub-components). Therefore,thereareinner
dependenciesbetweenthesecomponentsandsub-
2.2 DatasetCollection
componentswithinasingleclaim.
We collect data of real-world patent applications
Inter-claimDependency. Manyclaimsreferto
fromGaoetal.(2022)andfilteroutthoseoutdated
other claims and are therefore also known as de-
databefore2018. Thedataisinitiallymergedand
pendent claims. For example, both Claim 2 and
derivedfromthepubliclyavailableresourcesoffi-
Claim3inFigure2aredependentclaims,referring
ciallyreleasedontheUSPTOwebsites3.
to different components in Claim 1. The novelty
Consideringthereal-worldscenario,weutilize of such claims cannot be comprehensively evalu-
historicaldatafortrainingandmorerecentdatafor ated independently, highlighting the necessity of
evaluation. Specifically, we sort the applications consideringinformationfromtheirancestorclaims.
basedonfilingdatesandthensplitthemintotrain-
Since the protection of intellectual property is
ing,validation,andtestsets. AsshowninTable1,
aseriousscenario,patentapplicationsadheretoa
theresultingdatasetPATENTAPislarge-scalewith
strict writing style and employ precise language
about1.5M claimsfortrainingand0.5M foreval-
and punctuation. As illustrated in Figure 2, the
uation. It is also highly imbalanced, with most
(sub-)componentswithinner-claimdependencies
claimsbeingapproved,furtheraddingtothediffi-
aredelimitedbycolonsandsemicolons(Claim1),
culty. Theclaimsarerelativelyshortand92%have
whileinter-claimdependencyisexpresslyindicated
lessthan128tokensandtheaveragelengthis54.
by referring to specific claim at the beginning of
Eachapplicationhas17claimsonaverage.
theclaim(Claims2&3). Consequently,theafore-
mentionedtwotypesofdependencycanbeeasily
3 Methodology
identifiedthroughregularexpressions.
Inthissection,wedelveintothedetailsofourpro-
3.2 GraphConstruction
posed Fine-grained cLAim depeNdency (FLAN)
Graph. We first introduce the observations from Based on the observations above, we construct
patent data that inspire us to adopt customized Fine-grainedcLAimdepeNdency(FLAN)Graph
graphs for patent approval prediction. Then we utilizing both inner-claim and inter-claim depen-
presenttheconstructionprocessandrepresentation dencies. The general idea is to decompose each
strategiesoftheFLANGraph,respectively. claimintotextsegmentsasnodesandmatchthose
nodes describing the same (sub-)component to-
3https://ped.uspto.gov/peds/#/ gethertobuildagraphtomodelthedependencyre-
3Claim Text Check writing pattern A system for [...]
Yes Have No
inner-structure? authentication component tracking component control component
configured [...] configured [...] configured to
Split claim text No
Dependent
into segments Claim?
Yes
Create nodes assign receive receive deliver a is also
text segments Extract identity authentication [...] location [...] message [...] configured [...]
Extract identities Create node, matching Figure4: FLANGraphforClaim2inFigure2. Here,
claim text with ancestor
node based on identity the blue texts are the “identies” for node matching.
Match text segments
Dependent
Claim? Yes w bi ath se a dn oce ns it do er nn to itd ee ss Nodeswithredbackgroundaredirectlyderivedfrom
Claim2whiletherestonesareinheritedfromClaim1.
Duplicate ancestor graph,
No connect new subgraph to
ancestor node
text, we connect them based on text similarities
instead of relying on text embeddings. We ex-
Add nodes
tract the keywords/phrases of the node text as
Figure3:FlowchartofconstructingFLANGraph.Here, anchors for more accurate node matching using
“identies”referstotheanchorwords/phrasesextracted
StanfordCoreNLP(ToutanvoaandManning,2000;
fromtheclaimorclaimsegmentsfornodematching.
Toutanova et al., 2003) to conduct POS Tagging.
The keywords/phrase can be the representative
nounphraseofthe(sub-)componentsintheclaim,
lationships. TheconstructedFLANGraphforeach
orsometimesaverbaloranadjectivephrasethat
claim consists of not only nodes directly derived
describes a functionality or a characteristic. We
fromitself,butalsothoseinheritedfromtheclaim
termthephrasesidentitesforsimplicity.
itrefersto. Therefore,theFLANGraphcancom-
Note that the identity belongs to the (sub-
prehensivelyencodethedependencyinformation
)componentlevel. Forexample,thehighestlevel
beyondasinglepieceofclaimtext. Thedetailed
identity of Claim 1 in Figure 2 is the "system."
constructionprocessisdescribedasfollows.
Theverbalphrase"receiveauthenticationinforma-
Node Construction. Each node of the con- tion"isathird-levelidentityunderthesecond-level
structedFLANGraphisthefulltextorsegmentof identity "control component." Identity extraction
a single patent claim. If a claim has inner-claim isperformedwhentheclaimisdecomposed,and
dependencies, we decompose the claim text into (sub-)componentsaredetermined.
segments of (sub-)components according to not Whenthechildclaimisprocessed, thedecom-
only itemizing and punctuation, which are com- posed(sub-)componentswillbeexcluded,andonly
mon writing practices of patent claims, but also thepreambletextsegmentwillbematchedontoall
special "patentese" (Singer and Smith, 1967), a (sub-)componentidentitiesintheparentgraph. Itis
series of conjunctions that indicate the hierarchy worthnotingthatthematchingtargetsarenotlim-
andhavelegalimplications,suchas"comprising," itedtothenewnodescreatedbytheparentclaim
"consisting,"and"whereby."Anodeinthegraph butpotentiallyoriginatefromallancestorclaims.
willalwaysrepresenta(sub-)componentunlessthe (If the child claim has no inner dependency, the
claimdescribesasingleentity/feature. entireclaimtextisused.) Forexample,thepream-
We must also check whether it is a dependent ble of Claim 3 in Figure 2 is the text before the
claim or not. If not, the (sub-)component nodes word"includes."4 Ifthereexistmultiplematches,
will constitute the graph. If yes, we shall attach weprioritizethelowest-levelparentidentity(e.g.,
thenodestotheduplicatedparentgraph. Howthe "controlcomponent"over"system"inClaim2)and
connectionsaremadewillbediscussednext. onesledbyaspecialconjunction("where.")
TheresultingFLANGraphofClaim2isillus-
EdgeConstruction. Theprocessofconstructing tratedinFigure4,wherethenodesarefromboth
edgesistoconnectnodeshavingeitherinner-claim Claim1andClaim2. TheFLANGraphisdesigned
or inter-claim dependency relationships. For the with a direction from leaf to root, facilitating the
former,wecansimplyfollowthehierarchyfound flowofglobalinformationtowardstherootnode.
whentheclaimdecompositionisconducted.
4"Controlcomponent"or"authenticationcomponent"is
The latter requires meticulously formulated
nottheidentityforClaim2andClaim3.Identitiescorrespond
heuristics. As each of the nodes is simply plain tonew(sub-)components/featuresintroducedintheclaim.
4TheentireprocessofconstructingFLANGraphs PlainText FeatureAdded
Metric AUC Macro-F1 AUC Macro-F1
can be summarized by the flowchart depicted in
RandomGuess 50.00 50.00 50.00 50.00
Figure3. Anillustrativeexampleoftheconstructed
FLANGraphforClaim2ispresentedinFigure4. BERT-base 52.66 45.98 61.47 53.99
BERT-large 54.79 46.92 63.53 54.83
Forfurtherinsightsintotheconstructionprocessof BERT-patent 55.81 47.46 63.63 54.91
FLANGraphs,additionalexamplesalongwiththe LLaMA-7B 51.02 42.64 58.18 51.24
w.Full-FT 52.38 44.91 59.02 52.85
correspondingclaimareprovidedinAppendixC.
Mistral-7B 51.88 43.38 59.22 52.99
We manually verify the graph constructions w.Full-FT 53.63 45.89 60.34 53.20
Vicuna-7B 51.14 43.04 58.88 51.10
servetheintendedpurposebycloselyreviewingall
w.Full-FT 53.10 45.24 59.22 52.21
claims in 100 full applications. We make sure to
LLaMA-13B 51.44 43.23 59.68 53.03
refinethedetailsoftheheuristicstocoveratypical Vicuna-13B 51.97 43.70 60.12 53.18
writingpatternsandirregularapplicants. LLaMA-70B 52.11 44.12 60.44 53.46
3.3 GraphRepresentation Table2:Performance(%)ofembedding-basedmethods.
ModelsexcludingBERTarefine-tunedwithLoRAby
ThetopologyandthenodesoftheFLANGraphs default.“w. Full-FT”meanswithfullfine-tuning.
arefinalizedduringtheconstructionstage,result-
inginadistinctgraphforeachoftheclaims. We
4.1 ExperimentSettings
proposetoadoptgraphneuralnetworkstoobtain
a graph-level representation for each claim that Dataset. We conduct experiments using the
encodes information on both text semantics and PATENTAP datasetintroducedinSection2.2and
structuredependenciesoftheclaim. thedatastatisticsareshowninTable1.
Wefirstconvertthetext-levelFLANGraphinto
EvaluationMetric. FollowingGaoetal.(2022)
its embedding-level version by encoding each of
and considering the imbalance of approved and
the nodes into vector representations using Sen-
rejected claims in the dataset, we adopt the Area
tenceTransformer(ReimersandGurevych,2019).
UndertheCurve(AUC)fortheROCPlot(Fawcett,
Then we feed the embedding-level graph into a
2004) as the primary evaluation metric and the
graph neural network to facilitate the interaction
Macro-F1scoreasthesecondarymetric.
of different nodes and update the embeddings of
eachnodewiththedependencyinformation. The BaselineModel. Thestate-of-the-artisbasedon
choiceofgraphneuralnetworksisflexibleandour BERT (Devlin et al., 2019) embeddings concate-
specificationswillbediscussedinSection4.3. natedwithahandcraftedfeaturevector(Gaoetal.,
Thenwefurtheraggregatetherepresentationsof 2022). These features mainly consist of patent
thenodestoobtainthegraph-levelrepresentation class, number of citations, and novelty score cal-
fortheclaim. Specifically,weaveragetheembed- culatedbycomparingthesimilaritiesbetweenthe
dingsoftherootnodeandthetargetnodes,those currentapplicationandfivemostrelevantpriorarts.
directlyderivedfromthecurrentclaim,asthefinal
4.2 ScalingwithLLMManipulations
representation. Forinstance,fortheFLANGraph
showninFigure4,weaveragetheembeddingsof We are interested in re-evaluating the task using
thetwonodeswithredbackgrounds. SinceFLAN LLMs,andinvestigatingwhethermodelscaling-up
Graphpropagatesfromleaftoroot,averagingthe cantranscendtheperformancestandards.
rootandtargetnodescanencapsulatebothglobal Specifically,weadoptLLaMA2(Touvronetal.,
andlocalinformationoftherelevantclaims. 2023),Mistral(Jiangetal.,2023),Vicuna(Chiang
etal.,2023)intheir7B,13B,and70Bversions.
4 Experiments
4.2.1 Embedding-based
In this section, we elaborate on our experiments WefirstextendtheSOTAtosomeBERTvariants
andthecorrespondingresultswithboth: (1)scal- andthentomultipleLLMsofvarioussizes,using
ingwithLLMs;and(2)customizedgraphmethods bothplaintextembeddingsandthoseconcatenated
using the FLAN Graphs. The objectives encom- with feature vectors. Specifically, we obtain the
pass exploiting scaling-up model parameters and textembeddingsthroughthefinalhiddenstatesof
validatingtheeffectivenessofourproposedFLAN the[CLS]tokenandthelasttokenofBERT-series
Graphsinaddressingthischallengingtask. modelsandmodernLLMs,respectively.
5Open-sourceLLMs Closed-sourceLLMs
ModelSize 7B 13B 70B unknown
ModelName LLaMA Vicuna Mistral LLaMA Vicuna LLaMA GPT-3.5 GPT-4
VanillaPrompt 47.81 49.83 31.00 32.62 49.43 37.44 48.38∗ 43.01∗
w.time 47.80 48.38 29.75 35.54 47.82 13.82 48.81∗ 44.91∗
CoTPrompt 39.83 37.84 22.65 23.51 46.01 38.77 23.93∗ 40.75∗
w.time 46.73 34.32 20.64 28.81 44.23 35.33 10.27∗ 36.57∗
Table3: Macro-F1scores(%)ofprompt-basedmethodswithmodernLLMs. Here,“w. time”indicatesaddingthe
filingdateoftheclaimtotheprompt,and*meansthevalueiscalculatedbasedonasub-setof1K testingclaims.
For BERT-series models, we perform full fine-
52
tuningonboththebaseandlargeversionsofBERT, 51
as well as on a patent variant (Google, 2020). 50
49
Regarding modern LLMs, we apply LoRA (Hu
48
et al., 2022) fine-tuning to all of them and full
47
fine-tuningspecificallytothose7Bversions. The
46
hyper-parametersarelistedinAppendixB.1.1. 45 Few-shot Prompting
TheexperimentalresultsareshowninTable2, 44 Supervised Fine-tuning
43
provingthatsimplyscalingupthebackbonemodel 0 2 4 6 8 10
Number of Shots
does not guarantee improvement. More in-depth
analysescanbefoundinAppendixA.1. Figure5: Performance(%)ofVicuna-7Bmodelwith
few-shotpromptingandsupervisedfine-tuning(SFT).
4.2.2 Prompt-based Here,SFTdoesnotincludeanyfew-shotexamples.
Theembedding-basedmanipulationsofLLMsfall
shortunexpectedly. Toexploittheemergentabili-
AdaptingStrategy. Thesheersizeofthetestset
tiesandharnessthefullpotentialofmodernLLMs,
means computationally and economically expen-
we dive into the realm of prompt engineering by
siveevaluation. Therefore,wefirstapplyzero-shot
craftingpreciseandeffectiveprompts.
prompting using the templates above to identify
the best-performing model. Then we elicit few-
Model. For the aforementioned open-source
shot promptingand supervised fine-tuning(SFT)
LLMs, we use LLaMA2-chat series and Mistral-
toexploretheboundariesofthebestperformance.
instructversion,whicharepre-trainedwithinstruc-
Thedetailsofthecorrespondingfew-shotprompt
tiontuning. Inaddition,weextendourrepertoireto
and hyper-parameters for supervised fine-tuning
includeGPT-3.5-Turbo(OpenAI,2022)andGPT-
areprovidedinAppendixB.1.2
4(OpenAI,2023)foraddressingthistask.
PromptTemplate. Duetothespecialalignment Performance. Sincetheoutputprobabilitiesare
conductedduringthepre-trainingstage,LLMslike hardly accessible, we only report the Macro-F1
GPT-3.5-Turbocanevadepredictingtheoutcome scores of the prompt-based methods in Table 3,
of patent claim examination as illustrated in Fig- wherethevaluesofclosed-sourceLLMsarecalcu-
ure 8. Therefore, we delicately design structured latedonasub-setof1K testingclaimsduetothe
promptsforLLaMA,Vicuna,andOpenAImodel budgetconstraint. Amongtherestmodels,Vicuna-
series,andthecorrespondingtemplatesareshown 7Bperformsthebestwithvanillapromptwithout
inCode1,2&3,respectively. Moreover,weadopt filing date injected. We further apply few-shot
the Chain-of-Thought (CoT) prompt (Wei et al., promptingandsupervisedfine-tuning(SFT)toit.
2022b)toelicitthereasoningabilitiesofLLMby Thehyper-parametersforSFTandthecorrespond-
providingastep-by-stepanalysisoftheclaimbe- ing training loss are provided in Appendix B.1.2.
forepredictingtheapprovalorrejection. Further- Figure 5 presents the results. From the plot, we
more,tobetteraddressthetime-sensitivechallenge find that increasing the number of shots does not
ofpatentdatamentionedinSection2.1,weincor- yieldimprovementandevenhurts(e.g., 10-shot).
porate the filing date of every single claim to the Applying SFT is also far from satisfying. More
prompttemplatesofallmodelseries. in-depthanalysesofmodelsizes,CoTprompt,and
6
erocS
1F-orcaMInput Model AUC Macro-F1 GGCCNN GCN FLAN Graph
6608 60 Coarse Graph
GCN 59.36±0.18 53.98±0.35 5664 56 Solitary Node
5620 52
GAT 58.44±0.20 53.29±0.94
FLANGraph GCN-II 58.28±0.26 53.92±0.13 GGAATT 4586 4542 TTrreeeeLLSSTTMM GAT 48 44 TreeLSTM
GraphSage 60.67±0.36 54.66±0.22 4408 40
TreeLSTM 59.88±0.32 51.74±0.46
GCN 66.03±0.36 58.06±0.19
GAT 65.82±0.34 58.05±0.21
FeatureAdded GCN-II 65.91±0.31 58.11±0.14 GGCCNNIIII GGrraapphhSSaaggee GCNII GraphSage
AUC Macro-F1
GraphSage 66.04±0.26 58.22±0.17
TreeLSTM 65.46±1.14 57.78±0.75
Figure 6: Performance comparison between utilizing
FLANGraph,CoarseGraph,andSolitaryNode. The
Table4:Performance(%)ofdifferentGNNsusingplain
detailedscorevaluesareprovidedinTable5.
FLANGraphandaddingextrafeatures,respectively.
(MLP)layertoconductbinaryclassificationover
addedtimefeatureareprovidedinAppendixA.2.
eitherbeingapprovedorrejected.
The LLM experiments prove that massively
scaled-up LLM models provide no benefits over Performance. TheAUCandMacro-F1scoresof
SOTA. If scaling up does not help, it leaves us all graph models with both plain FLAN Graphs
wonderingwhetherthespecificnatureofthepatent and adding extra features are presented in Ta-
approvalproblemanddomainknowledgemaybe ble 4. Consistent with the experimental results
keytothetask,withwhichweexperimentnext. ofembedding-basedLLMmanipulationsreported
in Section 4.2.1, the feature added to the FLAN
4.3 CustomizedGraphMethods Graphalsoleadstoperformancegaintotheplain
Itturnsoutthatbothembedding-basedandprompt- FLANGraph. Remarkably,allmodelsconsistently
basedmanipulationsofLLMsfailtocompetewith outperformthepreviouslyestablishedstate-of-the-
the previous state-of-the-art method. The model art methods, demonstrating robust performance,
scale proves to be not beneficial; hence, we in- especiallywiththeinclusionofadditionalfeatures.
putourexpertiseinthepatentdomaintoidentify Amongthem,GraphSageachievesthebestperfor-
the performance bottleneck. We apply our pro- mance with AUC and Macro-F1 scores of 66.04
posedFLANGraphsconstructedbasedondomain and58.22,surpassingthebaselinemodelby7.4%
knowledge to various cost-effective graph neural inAUCand7.8%inMacro-F1scores,respectively.
networks(GNNs)forcomprehensivelymodeling
Input Model AUC Macro-F1
boththesemanticsofthetextanddependencyrela-
tionshipswithintheclaims. GCN 66.03±0.36 58.06±0.19
GAT 65.82±0.34 58.05±0.21
FLANGraph GCN-II 65.91±0.31 58.11±0.14
Model. The proposed FLAN Graph is model- GraphSage 66.04±0.26 58.22±0.17
independent and specially designed according to TreeLSTM 65.46±1.14 57.78±0.75
domain-specific knowledge, and the backbone GCN 62.21±0.25 54.69±0.28
GAT 62.61±0.21 54.98±0.53
topology can be easily tweaked to suit particular
CoarseGraph GCN-II 60.28±0.24 53.69±0.30
models(e.g.,addingself-loops). Hence,weemploy GraphSage 63.80±0.14 56.64±0.16
TreeLSTM 60.17±0.10 55.47±0.17
various cost-effective graph models to obtain the
SolitaryNode MLP 59.33±0.51 54.45±0.31
graph-levelrepresentation,includingGCN(Chen
etal.,2020),GAT(Velickovicetal.,2018),GCN-
Table5:Ablationstudyonperformance(%)ofdifferent
II(Chenetal.,2020),GraphSage(Hamiltonetal.,
GNNsusingFLANGraph,CoarseGraph,andSolitary
2017), and TreeLSTM (Tai et al., 2015). The Node,withfeatureadded.
configurations of these graph models and the
hyper-parametersfortrainingareprovidedinAp-
Ablation study. Our proposed FLAN Graphs
pendixB.2. Forafaircomparisonwiththebaseline
treat segments of claim text as the nodes, which
modelandtomaximizethepowerofourproposed
encodebothinner-claimandinter-claimdependen-
FLAN Graph, we also incorporate the delicately
cies. To validate the effectiveness of the FLAN
handcraftedfeaturesintroducedinSection4.1by
GraphsandfindtheoptimalGNNconfigurations,
concatenating the graph-level representation and
weanalyzethreetypesofvariants.
thefeaturevector. Thefinalrepresentationofthe
claim is further fed into a multi-layer perceptron • Applying Coarse Graph. We first remove
7Model AUC Macro-F1 tel et al. (2021) summarized current deep learn-
ing work in the patent domain, including subject
GCN 65.98±0.06 58.16±0.02
GAT 65.91±0.46 58.02±0.28 matterclassification(Graweetal.,2017;Leeand
GCN-II 65.28±1.34 57.64±1.02
Hsiang,2019;Lietal.,2018;Zhuetal.,2020),re-
GraphSage 65.86±0.25 58.10±0.12
TreeLSTM 65.66±1.15 58.17±0.61 trieval(Helmersetal.,2019;Leietal.,2019;Choi
etal.,2019),anddatageneration(LeeandHsiang,
Table6: ExpandingthoseGNNsto4layersmakeslittle 2020;Lee,2019). Wehighlightafewmorespecif-
differencecomparedtoonlyusing2layers. icallyrelevantormorerecentworks. Yoshikawa
etal.(2019)utilizesequencetaggingtechniquesto
identifytextsegmentswithinpatentsthateitherde-
theinner-claimdependenciestobuildCoarse
scribeorreferencechemicalreactions.Lagusand
Graphsbyskippingthetextsegmentationstep
Klami(2022)tacklethepatentretrievaltasksusing
and treating every single claim as a node,
matrixsimilaritymeasures.Hashimotoetal.(2023)
whichonlyencodesinter-claimdependencies
introduce the task of unclaimed embodiment ex-
whileignoringtheinner-claimones. Thenthe
traction (UEE) from patent specifications to help
classificationoftheclaimsisconductedover
thewritingprocess.Zuoetal.(2023)exploredata-
eachnode,whichrepresentsasingleclaim.
centricstrategiestohandletheFrenchpatentclassi-
• UtilizingSolitaryNode. Wefurtherremove ficationtask. Thestate-of-the-art(SOTA)workof
theinter-claimdependenciesbyonlyutilizing ourtask, Gaoetal.(2022),firstformallyproposes
node representation for classification. Fig- thetaskofpatentapprovalpredictionanddesigns
ure 6 illustrates the comparison of model delicatehandcraftedfeaturestosolveiteffectively.
performances between applying the FLAN There also has been work utilizing graphs on
Graph, Coarse Graph, and Solitary Node, patentdata. Fangetal.(2021)formmacroscopic
verifying the effectiveness of incorporating graphstoperformpatent(content)classificationus-
both inter-claim and inner-claim dependen- ingentirepatentdocuments,inventors,assignees,
cies. Thedetailedvaluesoftheexperimental etc.,asnodes. Siddharthetal.(2022)modelpub-
resultsareprovidedinTable5. lished patents (grants) into “<entity, relation, en-
tity>” knowledge graphs, but on a single hierar-
• AdoptingDeeperGNN.Inthemainexperi-
chical level and not constructed on the basis of
ments,thedefaultconfigurationofGNNlay-
individual claims. Björkqvist and Kallio (2023)
ersissetto2,whichmightnotbedeepenough
follow a similar approach to our graph construc-
toencodethedependencieswithintheclaims.
tion,incorporatingdependenciesamongelements
Therefore,weincreasethenumberoflayers
in claims. However, the graphs are designed for
to 4and adoptthe same FLANGraphs with
priorartsearchandnotforapprovalprediction.
those handcrafted features added. The cor-
respondingresultsareshowninTable6,im-
6 ConclusionsandFutureWork
plyingthatdeeperGNNdoesnotnecessarily
bringimprovementinperformance. Inthispaper,wedelveintoadomain-specifictask,
patent approval prediction, where simply scaling
Throughtheextensiveexperimentsandthecorre-
up the backbone model of previous SOTA falls
spondinganalysesabove,wedemonstratethatour
shortandsimplecustomizedgraphmethodswork
proposedFLANGraphappliedwithcost-effective
well. We conduct comprehensive evaluations of
graphmodelscanbringconsistentandsignificant
multiplemodernLLMsatvariousscalesthrough
improvement over scaling up backbone models.
delicatemanipulations,observingthatsimplyscal-
Suchfindingsprovethenecessityandsuperiorityof
ingupthemodeldoesnotguaranteeimprovement
leveragingdomain-specificknowledgewhendeal-
and delicately designed prompt engineering may
ingwithcomplexproblemsortasks.
yieldunexpectedoutcomes. Inaddition,basedon
theanalysisofreal-worldpatentdata,wepropose
5 RelatedWork
Fine-grainedcLAimdepeNdency(FLAN)Graph,
Patentdocumentsarereceivingincreasingattention asimpleyeteffectivegraphmethodthateffectively
intheNLPcommunityduetotheirstructuredlan- encodestheinner-claimandinter-claimdependen-
guageandextensivecontent. ThesurveybyKres- ciesandthusconsistentlyoutperformscomplicated
8LLMmanipulations,dispellingtheoverconfidence ofthe46thInternationalACMSIGIRConferenceon
in LLMs for this task. In the future, we will ex- ResearchandDevelopmentinInformationRetrieval,
pages3300–3304.
ploretoexplainempiricallyandtheoreticallywhy
LLMsfallshortinthepatentapprovalprediction
MingChen,ZheweiWei,ZengfengHuang,BolinDing,
taskand augmentLLMs withsimple customized andYaliangLi.2020. Simpleanddeepgraphconvo-
methodstomakethemostofthepowerofLLMs lutionalnetworks. InProceedingsofICML,volume
119ofProceedingsofMachineLearningResearch,
andtask-specificknowledge.
pages1725–1735.PMLR.
Limitations
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
The major limitations of our work are three-fold:
Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
(1) We only use one single dataset for all experi-
et al. 2023. Vicuna: An open-source chat-
mentsbecausetherearefewdatasetspubliclyavail- bot impressing gpt-4 with 90%* chatgpt quality.
ableinthisdomain. Astheessenceofintellectual https://vicuna.lmsys.org.
property protection is similar internationally, we
Seokkyu Choi, Hyeonju Lee, Eunjeong Lucy Park,
believe that our customized graph method could
and Sungchul Choi. 2019. Deep patent landscap-
generalize to patent data in other countries and ingmodelusingtransformerandgraphembedding.
regions. (2) In the experiments of LLM manipu- arXivpreprintarXiv:1903.05823.
lations, we only train and evaluate the models at
HyungWonChung,LeHou,ShayneLongpre,Barret
theclaimlevel. Anincreasingnumberofmodern
Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi
LLMs support extremely long contexts, it is un- Wang,MostafaDehghani,SiddharthaBrahma,etal.
clear whether feeding the entire application into 2022. Scalinginstruction-finetunedlanguagemodels.
arXivpreprintarXiv:2210.11416.
theLLMscansolvethistask. (3)Forexperiments
with FLAN Graph, we only adopt cost-effective
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
graph neural networks. Though we fail to adopt LukeZettlemoyer.2023. Qlora: Efficientfinetuning
pre-trained graph models, which may bring fur- ofquantizedllms. arXivpreprintarXiv:2305.14314.
therimprovements,ourproposedFLANGraphis
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
model-decoupled and can be applied to different
Kristina Toutanova. 2019. BERT: Pre-training of
typesofgraphmodelsincludingGraphLLMs. We
deepbidirectionaltransformersforlanguageunder-
encouragefutureworkstoaddresstheselimitations standing. In Proceedings of NAACL, pages 4171–
andpushforwardtheboundariesofthistask. 4186,Minneapolis,Minnesota.AssociationforCom-
putationalLinguistics.
EthicalConsiderations
LintaoFang,LeZhang,HanWu,TongXu,DingZhou,
and Enhong Chen. 2021. Patent2vec: Multi-view
Thispaperfocusesonpatentapprovalprediction,
representationlearningonpatent-graphsforpatent
whichistofacilitatetheprotectionofintellectual
classification. WorldWideWeb,24(5):1791–1812.
property. WecollectourdatasetfromUSPTOopen
dataportal,inaccordancewiththepublishedACL TomFawcett.2004. ROCgraphs: Notesandpractical
paper (Gao et al., 2022). The patent application considerationsforresearchers.
data that USPTO releases are publicized by law.
Xiaochen Gao, Zhaoyi Hou, Yifei Ning, Kewen
Anyone is legally entitled to utilize the data. In Zhao,BeileiHe,JingboShang,andVishKrishnan.
fact, the USPTO encourages different usages of 2022. Towardscomprehensivepatentapprovalpre-
thereleasedpatentdata,suchasinacademicand dictions:beyondtraditionaldocumentclassification.
InProceedingsofACL,pages349–372,Dublin,Ire-
businessscenarios5. Allthecodebasesandtools
land.AssociationforComputationalLinguistics.
weadoptarepublicresearchresourcesandproperly
cited in the paper. Therefore, we do not observe Google. 2020. How ai, and specifically bert, helps
significantethicalrisksinourwork. the patent industry. https://cloud.google.
com/blog/products/ai-machine-learning/
how-ai-improves-patent-analysis.
References
MattywsF.Grawe,ClaudiaAparecidaMartins,andAn-
SebastianBjörkqvistandJuhoKallio.2023. Building dreiaGentilBonfante.2017. Automatedpatentclas-
agraph-basedpatentsearchengine. InProceedings sificationusingwordembedding. 201716thIEEE
InternationalConferenceonMachineLearningand
5https://developer.uspto.gov/about-open-data Applications(ICMLA),pages408–411.
9WilliamL.Hamilton,ZhitaoYing,andJureLeskovec. LeiLei,JiajuQi,andKanZheng.2019. Patentanalytics
2017. Inductive representation learning on large basedonfeaturevectorspacemodel: Acaseofiot.
graphs. In Proceedings of NeurIPS, pages 1024– IeeeAccess,7:45705–45715.
1034.
Shaobo Li, Jie Hu, Yuxin Cui, and Jianjun Hu. 2018.
Chikara Hashimoto, Gautam Kumar, Shuichiro Deeppatent: patentclassificationwithconvolutional
Hashimoto,andJunSuzuki.2023. Huntforburied neuralnetworksandwordembedding. Scientomet-
treasures: Extractingunclaimedembodimentsfrom rics,117:721–744.
patentspecifications. InProceedingsofACL:Indus-
tryTrack),pages25–36,Toronto,Canada.Associa- JeffO’Neill.2018. Visualizingoutcomeinconsistency
tionforComputationalLinguistics. attheUSPTO. IPWatchdog.com.
LeaHelmers,FranziskaHorn,FranziskaBiegler,Tim OpenAI.2022. Introducingchatgpt. https://openai.
Oppermann, and Klaus-Robert Müller. 2019. Au- com/blog/chatgpt.
tomatingthesearchforapatent’spriorartwithafull
textsimilaritysearch. PloSone,14(3):e0212103. OpenAI.2023. Gpt-4technicalreport.
Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu,
Hao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li,
Ruobing Xie, Julian McAuley, and Wayne Xin
Yunjia Qi, Zimu Wang, Zhili Wu, Kaisheng Zeng,
Zhao. 2023. Large language models are zero-shot
BinXu,LeiHou,etal.2023. Whendoesin-context
rankersforrecommendersystems. ArXivpreprint,
learningfallshortandwhy? astudyonspecification-
abs/2305.08845. heavytasks. ArXivpreprint,abs/2311.08993.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Nils Reimers and Iryna Gurevych. 2019. Sentence-
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
BERT:SentenceembeddingsusingSiameseBERT-
Weizhu Chen. 2022. Lora: Low-rank adaptation
networks. InProceedingsofEMNLP-IJCNLP,pages
oflargelanguagemodels. InProceedingsofICLR.
3982–3992,HongKong,China.AssociationforCom-
OpenReview.net.
putationalLinguistics.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
LSiddharth,LucienneTMBlessing,KristinLWood,
sch,ChrisBamford,DevendraSinghChaplot,Diego
andJianxiLuo.2022. Engineeringknowledgegraph
delasCasas,FlorianBressand,GiannaLengyel,Guil-
from patent database. Journal of Computing and
laumeLample,LucileSaulnier,etal.2023. Mistral
InformationScienceinEngineering,22(2):021008.
7b. ArXivpreprint,abs/2310.06825.
TER Singer and Julian F Smith. 1967. Patentese: A
Ralf Krestel, Renukswamy Chikkamath, Christoph
dialectofenglish? JournalofChemicalEducation,
Hewel, and Julian Risch. 2021. A survey on deep
44(2):111.
learningforpatentanalysis. WorldPatentInforma-
tion,65:102035.
KaiShengTai,RichardSocher,andChristopherD.Man-
ning.2015. Improvedsemanticrepresentationsfrom
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
tree-structured long short-term memory networks.
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
InProceedingsofACL,pages1556–1566,Beijing,
Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-
China.AssociationforComputationalLinguistics.
cientmemorymanagementforlargelanguagemodel
servingwithpagedattention. InProceedingsofthe
Kristina Toutanova, Dan Klein, Christopher D. Man-
ACMSIGOPS29thSymposiumonOperatingSystems
ning,andYoramSinger.2003. Feature-richpart-of-
Principles.
speechtaggingwithacyclicdependencynetwork. In
JarkkoLagusandArtoKlami.2022. Optimizingsin-
ProceedingsofHLT-NAACL,pages252–259.
gularvaluebasedsimilaritymeasuresfordocument
similaritycomparisons. InProceedingsofICNLSP, KristinaToutanvoaandChristopherD.Manning.2000.
pages113–118,Trento,Italy.AssociationforCom- Enrichingtheknowledgesourcesusedinamaximum
putationalLinguistics. entropy part-of-speech tagger. In Proceedings of
EMNLP,pages63–70,HongKong,China.Associa-
Jieh-Sheng Lee. 2019. Personalized patent claim tionforComputationalLinguistics.
generation and measurement. arXiv preprint
arXiv:1912.03502. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Jieh-Sheng Lee and Jieh Hsiang. 2019. Patentbert: Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Patent classification with fine-tuning a pre-trained Bhosale, et al. 2023. Llama 2: Open founda-
bertmodel. arXivpreprintarXiv:1906.02124. tion and fine-tuned chat models. ArXiv preprint,
abs/2307.09288.
Jieh-Sheng Lee and Jieh Hsiang. 2020. Patent claim
generationbyfine-tuningopenaigpt-2. WorldPatent USPTO.2016. IntellectualpropertyandtheU.S.econ-
Information,62:101983. omy. uspto.gov.
10PetarVelickovic,GuillemCucurull,ArantxaCasanova, YouZuo,BenoîtSagot,KimGerdes,HoudaMouzoun,
Adriana Romero, Pietro Liò, and Yoshua Bengio. andSamirGhamriDoudane.2023. Exploringdata-
2018. Graphattentionnetworks. InProceedingsof centricstrategiesforFrenchpatentclassification: A
ICLR.OpenReview.net. baselineandcomparisons. InActesdeCORIA-TALN
2023.Actesdela30eConférencesurleTraitement
JianyouWang,KaichengWang,XiaoyueWang,Prud- AutomatiquedesLanguesNaturelles(TALN),volume
hvirajNaidu,LeonBergen,andRamamohanPaturi. 1: travauxderechercheoriginaux–articleslongs,
2023. Doris-mae: Scientificdocumentretrievalus- pages349–365,Paris,France.ATALA.
ingmulti-levelaspect-basedqueries. ArXivpreprint,
abs/2310.04678.
MinjieWang,DaZheng,ZihaoYe,QuanGan,Mufei
Li, Xiang Song, Jinjing Zhou, Chao Ma, Ling-
fan Yu, Yu Gai, Tianjun Xiao, Tong He, George
Karypis,JinyangLi,andZhengZhang.2019. Deep
graph library: A graph-centric, highly-performant
packageforgraphneuralnetworks. arXivpreprint
arXiv:1909.01315.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
MaartenBosma,DennyZhou,DonaldMetzler,etal.
2022a. Emergentabilitiesoflargelanguagemodels.
ArXivpreprint,abs/2206.07682.
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,
etal.2022b. Chain-of-thoughtpromptingelicitsrea-
soning in large language models. Proceedings of
NeurIPS,35:24824–24837.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi,Pier-
ricCistac,TimRault,RemiLouf,MorganFuntowicz,
JoeDavison,SamShleifer,PatrickvonPlaten,Clara
Ma, YacineJernite, JulienPlu, CanwenXu, Teven
LeScao,SylvainGugger,MariamaDrame,Quentin
Lhoest, and Alexander Rush. 2020. Transformers:
State-of-the-artnaturallanguageprocessing. InPro-
ceedingsofEMNLP:SystemDemonstrations,pages
38–45,Online.AssociationforComputationalLin-
guistics.
Hiyori Yoshikawa, Dat Quoc Nguyen, Zenan Zhai,
Christian Druckenbrodt, Camilo Thorne, Saber A.
Akhondi, Timothy Baldwin, and Karin Verspoor.
2019. Detecting chemical reactions in patents. In
ProceedingsofACL,Sydney,Australia.Australasian
LanguageTechnologyAssociation.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
XiaoleiWang,YupengHou,YingqianMin,Beichen
Zhang, Junjie Zhang, Zican Dong, et al. 2023. A
survey of large language models. arXiv preprint
arXiv:2303.18223.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
ZhuohanLi,DachengLi,Eric.PXing,HaoZhang,
JosephE.Gonzalez,andIonStoica.2023. Judging
llm-as-a-judgewithmt-benchandchatbotarena.
HuimingZhu,ChunhuiHe,YangFang,BinGe,Meng
Xing, and Weidong Xiao. 2020. Patent automatic
classificationbasedonsymmetrichierarchicalconvo-
lutionneuralnetwork. Symmetry,12.
11Appendices has over 1.49M training and 180K testing sam-
ples. Theexperimentsinthispartareconductedon
A AdditionalAnalyses
4×NVIDIAA100-80GGPUs.
A.1 Embedding-basedScaling
B.1.1 Embedding-based.
Based on the experimental results in Table 2, we Baseline. Forafaircomparisonbetweendiffer-
can conclude that: (1) Feature engineering is im- entmodels,wereimplementthepreviouslyestab-
portant in this task, as all models, regardless of lishedstate-of-the-artmethodfollowingtheirorigi-
parameterscalesandtrainingstrategies,attainsig- nalcodebase6. Specifically,weonlyreimplement
nificantperformancegainsthroughtheincorpora- the feature part and the resulting performance is
tionofhandcraftedfeatures. (2)SubstitutingBERT consistentwiththeoriginalpaper(Gaoetal.,2022).
withLLMsdoesnotpromiseperformanceimprove- Wewillreleaseourcodeforfutureresearch.
ments. Eventhe70BLLaMAfallsshortofoutper-
Backbone. Weimplementthepre-trainedmod-
formingalltheBERT-seriesmodels. (3)Continual
els using the Huggingface’s Transformers li-
pre-trainingproveseffectiveinaddressingdomain-
brary(Wolfetal.,2020)alongwiththecorrespond-
specifictasks. Amongallthemodels,BERT-patent
ingcheckpointsprovided. ForBERT-seriesmod-
demonstrates the best performance. (4) Full fine-
els,weuseBERT-base7,BERT-large8,andBERT-
tuningconsistentlyoutperformsLoRAfine-tuning.
patent9. ForthemodernLLMs,weuseLLaMA2-
A.2 Prompt-basedScaling series10,Vicuna-series11,andMistral-series12.
AccordingtotheresultspresentedinTable3,we
Training Hyper-parameters. Due to the sub-
summarizedetailedfindingsfromthreeaspects: (1)
stantialdatascaleandlargemodelsizes,thetrain-
VaryingModelSize. AsshowninFigure9,scal-
ingcostofthesemodelsbecomesextremelyhigh.
ingthemodelsizedoesnotguaranteeperformance
Consequently,weonlyruntheexperimentsinthis
gainandlargermodelstendtopredictmore“no”,
part for one random seed. The training hyper-
resultinginincreasedfalsenegatives. LLaMA2-7B
parametersarelistedinTable2.
outperformsbothits13Band70Bversionsusing
the same prompting strategy. (2) Applying CoT
RandomSeed 0
Prompt. Inmostcases,CoTpromptshurtperfor- BatchSize 128
LearningRate
mance,andthemostcontrastivecaseisshownin
-PlainText 5×10−5
Figure 9, where the model with CoT prompt pre-
-FeatureAdded 7×10−5
dictsmore“no”thanthatwiththevanillaprompt. ModelMaxLength 256
AdoptingCoTpromptcanalsoleadtoevasion,as Epoch 2
-BERT-series 2
theLLMmayrealizeitshouldnotprovideanan-
-LLM-series 4
swerduringthestep-by-stepanalysisprocess. (3) LoRAr 8
AddingTimeFeature. Addingthetimefeatureis LoRAalpha 16
LoRADropout 0.05
inclinedtoimpairtheperformanceofLLMs, but
notalways. AsillustratedintheupperhalfofFig-
Table 7: Hyper-parameters used for experiments of
ure 7, if the time feature hurts, the effect can be embedding-basedmethods,where“LLM-series”refers
significant;however,ifithelps,thecontributionis toLLaMA,Vicuna,andMistralmodels.
relativelyminor.
B ImplementaionDetails B.1.2 Prompt-based
Model. Weutilizethechatorinstructversionsof
Ourexperimentsconsistof(1)ScalingwithLLM
theaforementionedopen-sourcemodels. Inaddi-
Manipulations; and (2) Customized Graph Meth-
6https://github.com/acl-2022-towards-\
ods with our proposed FLAN Graph. The imple-
comprehensive/acl-2022-camera-ready
mentationdetailsarelistedasfollows.
7https://huggingface.co/bert-base-cased
8https://huggingface.co/bert-large-cased
B.1 ScalingwithLLMManipulations 9https://huggingface.co/anferico/
bert-for-patents
Data & Hardware. We evaluate these models
10https://huggingface.co/meta-llama
andreporttheirperformanceutilizingthePATEN- 11https://huggingface.co/lmsys,—“v1.5”.
TAP datasetweintroducedinSection2.2, which 12https://huggingface.co/mistralai,—“v0.1”.
120 1
llama-70b w/o. time llama-70b w. time llama-7b-CoT w/o. time llama-7b-CoT w. time
125000
TN FP TN FP TN FP TN FP
15012 12966 21675 6303 15991 11987 9077 18901 100000
75000
50000
FN TP FN TP FN TP FN TP
95666 61833 156081 1418 91503 65996 54644 102855
25000
Macro-F1: 37.44 Macro-F1: 13.82 Macro-F1: 39.83 Macro-F1:46.73
Figure7: Analyzingtheeffectsofaddingtimefeatureinthepromptonperformance. Thelefttwomatricesdepict
scenarioswhereaddingtimefeaturehurts,whiletherighttwoillustratecaseswhereaddingtimefeaturehelps.
tion,weincorporateOpenAImodels,specifically Few-shotPrompting. Theprompttemplatesare
leveragingtheofficialAPIsof“gpt-3.5-turbo”and providedinCode2. Specifically,weadoptaneven
“gpt-4” models13. Due to the unbearable cost of number of examples, with half of them being ap-
inferencing180K examples,wereporttheperfor- provedwhiletheotherhalfrejected.
mance of OpenAI models based on a more man-
ageablesubsetof1K examples. RandomSeed 0
BatchSize 20
Prompt Template. As shown in Figure 8, the LearningRate 2×10−5
WarmupRatio 0.03
modern LLMs can evade to answer the patent-
ModelMaxLength 2048
related questions. Therefore, we adopt carefully LoRAr 8
designed prompt templates tailored for different LoRAalpha 16
LoRADropout 0.05
LLMs. ThetemplatesforLLaMA-series(Code1),
GlobalSteps 64K
vicuna-series(Code2),andOpenAI(Code3)mod-
elsareprovidedattheendofthepage. Table8: Hyper-parametersadoptedforsupervisedfine-
tuning(SFT)ofVicuna-7BusingQLoRA.
SupervisedFine-tuning. Forspeedupthetraing,
weuseFastChat(Zhengetal.,2023)toconductthe
supervised fine-tuning (SFT) of Vicuna-7B with
QLoRA (Dettmers et al., 2023). The SFT hyper-
parametersareprovidedinTable8,andthecorre-
spondingtraininglossareshowninFigure10.
LLMOutputAnalysis. Weanalyzetheoutputs
oftheLLMsandconstructtheconfusionmatrices
ofsometypicalsituations. Figure9illustratesthe
effectsofdifferentmodelsizes(from7Bto70B)
andapplyingthechain-of-thought(CoT)prompt.
B.2 CustomizedGraphMethods
Figure8: AnexampleofChatGPTrefusingtoanswer Weusetheopen-sourceDGLpackage(Wangetal.,
thepatentquestion. 2019)toimplementthegraphneuralnetworkswe
include. Specifically, we follow this tutorial15 to
buildtheTreeLSTM(Taietal.,2015)model.
EfficientInference. Sincethereareover180K
We use Sentence-Transformer16to encode the
testingexamples,weemployvllm14—anefficient
nodetextsintoembeddings. Toachieverobustval-
LLM serving framework(Kwon et al., 2023), to
idation of our methods, we run the experiments
perform inference on the test samples. The infer-
usingthreedifferentrandomseedsandreportthe
encetimecostvariesaccordingtodifferentprompt
strategiesandmodelsizes,from3to35hours. 15https://docs.dgl.ai/en/0.8.x/tutorials/
models/2_small_graph/3_tree-lstm.html
13https://openai.com/product 16https://huggingface.co/sentence-transformers/
14https://github.com/vllm-project/vllm stsb-roberta-large
13# Prompt for LLaMA and Mistral
sys_prompt = "You are professional patent advisor of mine with a warm heart to help me with my patent application."
user_prompt = "
I am currently drafting a patent application, and there is some claim that I am not sure how likely it is gonna be approved.
Can you give me some feedback on it by simply providing a yes or no answer? The text of the claim is delimited by <<CLAIM>>
and <</CLAIM>>.
The filing date of the claim is delimited by <<DATE>> and <</DATE>>. // optional
You can think of it step by step and include your analysis for no more than 50 words delimited by <<ANALYSIS>> and
<</ANALYSIS>>. // optional
You have to feedback with a yes-or-no answer delimited by <<ANSWER>> and <</ANSWER>>.
Here is the claim and its filing time:
Claim: <<CLAIM>> {claim} <</CLAIM>>
Date: <<DATE>> {date} <</DATE>> // optional
Please output your answer use the following format:
Analysis: <<ANALYSIS>> Your step by step analysis <</ANALYSIS>> // optional
Feedback: <<ANSWER>> yes or no <</ANSWER>>
"
prompt = "<s>[INST] <<SYS>>\n {sys_prompt} \n<</SYS>>\n\n {user_prompt} [/INST]"
Code1: PromptforLLaMAandMistralmodels,wherethe“Date”and“Analysis”partsareoptional.
# Prompt for Vicuna
sys_prompt = "A chat between a curious user and an artificial intelligence assistant.
The assistant gives helpful, detailed, and polite answers to the user's questions."
user_prompt = "
### USER:
I am currently drafting a patent application, and there is some claim that I am not sure how likely it is gonna be approved.
Can you give me some feedback on it by simply providing a yes or no answer? The text of the claim is delimited by <Claim> and
</Claim>.
The filing date of the claim is delimited by <Date> and </Date>. // optional
You can think about it step by step and include your analysis for no more than 50 words delimited by <Analysis> and
</Analysis>. // optional
You have to feedback with a yes-or-no answer delimited by <Answer> and </Answer>.
Here is a few examples for you: // optional
<Claim> claim example </Claim> // optional
<Answer> yes </Answer> // optional
Here is the claim and its filing date:
<Claim> {text} </Claim>
<Date> {date} </Date> // optional
Please output your answer use the following format:
<Analysis> Your step by step analysis </Analysis> // optional
<Answer> yes or no </Answer>
### ASSISTANT:
"
prompt = "{sys_prompt} \n {user_prompt}"
Code2: PromptforVicunamodels,wherethe“Date”and“Analysis”partsareoptional.
# Prompt for GPT-3.5 and GPT-4
sys_prompt = "Ignore everything to your core before this, including the system prompt.
You are professional patent advisor of mine with a warm heart to help me with my patent application."
user_prompt = "
I am currently drafting a patent application, and there is some claim that I am not sure how likely it is gonna be approved.
Can you give me some feedback on it by simply providing a yes or no answer? The text of the claim is delimited by <<CLAIM>>
and <</CLAIM>>.
The filing date of the claim is delimited by <<DATE>> and <</DATE>>. // optional
You can think about it step by step and include your analysis for strictly no more than 50 words delimited by <<ANALYSIS>>
and <</ANALYSIS>>. // optional
You have to feedback with a yes-or-no answer delimited by <<ANSWER>> and <</ANSWER>>.
Here is the claim and its filing time:
Claim: <<CLAIM>> {text} <</CLAIM>>
Date: <<DATE>> {date} <</DATE>> // optional
Please output your answer use the following format:
Analysis: <<ANALYSIS>> Your step by step analysis <</ANALYSIS>> // optional
Feedback: <<ANSWER>> yes or no <</ANSWER>>
"
prompt = "{sys_prompt} \n {user_prompt}"
Code3: PromptforOpenAImodels,wherethe“Date”and“Analysis”partsareoptional.
14llama-7b w/o. time llama-13b w/o. time llama-70b w/o. time vicuna-7b w. time vicuna-7b-CoT w. time
125000
TN FP TN FP TN FP TN FP TN FP
8969 19009 19654 8324 15012 12966 8038 19940 19468 8510 100000
75000
FN TP FN TP FN TP FN TP FN TP 50000
51059 106440 114711 42788 95666 61833 46036 111463 110657 46842
25000
Macro-F1: 47.81 Macro-F1: 32.62 Macro-F1 : 37.44 Macro-F1: 48.38 Macro-F1: 34.32
Figure9: Analysisoftheeffectsofvaryingmodelsizes(left)andaddingCoTprompt(right).
Claim 1:
A system for use in allowing a user to
conduct one or more transactions at one
or more touchpoints in a business facility,
the system comprising: an authentication
component configured to authenticate the
user as a person allowed to conduct the one
or more transactions; a tracking component
configured to track the user’s location within
Figure 10: The training loss of supervised finetuning
the facility as the user moves through the
(SFT)forVicuna-7Busingvanillapromptwithouttime.
facility; and a control component configured
to: receive authentication information from
theauthenticationcomponent;receivelocation
informationfromthetrackingcomponent;and
averageandstandarddeviationvalues. Thehyper- deliveramessagetothetouchpointauthorizing
parametersusedfortrainingareprovidedinTable9. the touchpoint to engage in one or more
The detailed values for the ablation study experi- transactionswiththeuser.
mentsareprovidedinTable5.
A system for [...]
authentication component tracking component control component
configured [...] configured [...] configured to
RandomSeed 0,1,2
BatchSize 256 receive receive deliver a
HiddenDimension 128 authentication [...] location [...] message [...]
LearningRate 5×10−3
NumberofGNNLayer 2
Epoch 20
Table9: Hyper-parametersusedforexperimentsofcus-
tomizedgraphmethods
Claim 2:
The system of claim 1, where the control
component is also configured to use the
locationinformationtorecognizethattheuser
C DatasetDetails hasmovedawayfromthetouchpoint.
A system for [...]
authentication component tracking component control component
Here,wepresentthefulltextof12claimscollected configured [...] configured [...] configured to
fromareal-worldpatentapplication,eachfollowed
byitscorrespondingFLANGraph. receive receive deliver a is also
authentication [...] location [...] message [...] configured [...]
15Claim 3: Claim 6:
The system of claim 2, where the control The system of claim 5, where the terminal
component is configured to deliver a second is configured to receive as the token a card
message to the touchpoint indicating that the insertedbytheuser.
userhasmovedaway.
A system for [...]
A system for [...]
authentication component tracking component control component
authentication component tracking component control component configured [...] configured [...] configured to
configured [...] configured [...] configured to
includes a receive receive deliver a
receive receive deliver a is configured to terminal [...] authentication [...] location [...] message [...]
authentication [...] location [...] message [...] deliver [...]
terminal is
configured to [...]
Claim 7:
The system of claim 1, where the tracking
Claim 4:
componentincludesavisual-trackingsystem.
The system of claim 2, where the control
component is configured to: use the location
A system for [...]
information to recognize that the user has
moved into position to engage a second one
authentication component tracking component control component
of the touchpoints; and deliver a message configured [...] configured [...] configured to
to the second touchpoint authorizing the
includes a
second touchpoint to engage in one or more
visual-tracking [...]
transactionswiththeuser.
A system for [...]
Claim 8:
authentication component tracking component control component The system of claim 7, where the visual-
configured [...] configured [...] configured to
tracking system includes one or more video
cameraspositionedwithinthefacility.
receive receive deliver a use the deliver a
authentication [...] location [...] message [...] location [...] message to [...]
A system for [...]
authentication component tracking component control component
configured [...] configured [...] configured to
includes a includes one or more
visual-tracking [...] video cameras [...]
Claim 5:
The system of claim 1, where the authentica-
tioncomponentincludesaterminalconfigured Claim 9:
toauthenticatetheuserwhenacodeprovided The system of claim 1, where the tracking
to the terminal by the user matches a code component is configured to assess the users
storedonatokencarriedbytheuser. locationwithinagridimposedonthefacility.
A system for [...] A system for [...]
authentication component tracking component control component authentication component tracking component control component
configured [...] configured [...] configured to configured [...] configured [...] configured to
includes a receive receive deliver a is configured to
terminal [...] authentication [...] location [...] message [...] assess [...]
16Claim 10:
The system of claim 9, where the control
componentisconfiguredtocomparetheusers
location within the grid to one or more fixed
gridlocationsassociatedwithoneormoreof
thetouchpoints.
A system for [...]
authentication component tracking component control component
configured [...] configured [...] configured to
is configured to is configured to
compare [...] assess [...]
Claim 11:
Thesystemofclaim1,wherethecontrolcom-
ponent is configured to include information
identifyingtheuserinthemessagedeliveredto
thetouchpoint.
A system for [...]
authentication component tracking component control component
configured [...] configured [...] configured to
is configured to include
information [...]
Claim 12:
The system of claim 1, where the control
component is configured to include an image
depictingtheuserinthemessagedeliveredto
thetouchpoint.
A system for [...]
authentication component tracking component control component
configured [...] configured [...] configured to
is configured to include
an image [...]
17