PRESERVING LINEAR INVARIANTS IN ENSEMBLE FILTERING
METHODS
MathieuLeProvost, JanGlaubitz,and YoussefMarzouk
LaboratoryforInformationandDecisionSystems
DepartmentofAeronauticsandAstronautics
MassachusettsInstituteofTechnology
Cambridge,MA,02139,USA
{mleprovo, glaubitz, ymarz}@mit.edu
ABSTRACT
Formulatingdynamicalmodelsforphysicalphenomenaisessentialforunderstandingtheinterplay
between the different mechanisms, predicting the evolution of physical states, and developing
effectivecontrolstrategies. However,adynamicalmodelaloneisofteninsufficienttoaddressthese
fundamental tasks, as it suffers from model errors and uncertainties. One common remedy is to
relyondataassimilation,wherethestateestimateisupdatedwithobservationsofthetruesystem.
Ensemblefilterssequentiallyassimilateobservationsbyupdatingasetofsamplesovertime. They
operate in two steps: a forecast step that propagates each sample through the dynamical model
andananalysisstepthatupdatesthesampleswithincomingobservations. Foraccurateandrobust
predictionsofdynamicalsystems,discretesolutionsmustpreservetheircriticalinvariants. While
modernnumericalsolverssatisfytheseinvariants,existinginvariant-preservinganalysisstepsare
limitedtoGaussiansettingsandareoftennotcompatiblewithclassicalregularizationtechniques
ofensemblefilters,e.g.,inflationandcovariancetapering. Thepresentworkfocusesonpreserving
linearinvariants,suchasmass,stoichiometricbalanceofchemicalspecies,andelectricalcharges.
Usingtoolsfrommeasuretransporttheory(Spantinietal.,2022,SIAMReview),weintroducea
genericclassofnonlinearensemblefiltersthatautomaticallypreservedesiredlinearinvariantsin
non-Gaussianfilteringproblems. ByspecializingthisframeworktotheGaussiansetting,werecover
aconstrainedformulationoftheKalmanfilter. Then,weshowhowtocombineexistingregularization
techniquesfortheensembleKalmanfilter(Evensen,1994,J.Geophys. Res.) withthepreservationof
thelinearinvariants. Finally,weassessthebenefitsofpreservinglinearinvariantsfortheensemble
Kalmanfilterandnonlinearensemblefilters.
Keywords linearinvariants·measuretransport·nonlinearfiltering·ensembleKalmanfilter
4202
rpA
22
]OC.tats[
1v82341.4042:viXraLeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
1 Introduction
Manyphysicalandbiologicalsystemsarewell-describedbydifferentialequationsrelatingthetimevariationofthe
statecomponentstoadvective,diffusive,interactive,and/orreactiveeffects. Ithaslongbeenrecognizedthatdynamical
systemspossessspecificproperties,usuallywrittenastheconservationofinvariantproperties,suchasmass,momentum,
Hamiltonian,energy,stoichiometricbalanceofchemicalspecies,andelectricalcharge.Toproducephysicallyadmissible
solutionsforthesedifferentialequations,advancednumericalsolversensurethatdiscretesolutionsmimicthecritical
invariantsoftheoriginalsystem[Haireretal.,2006].
Letusconsiderastateprocess{X } ∈Rn. WesaythatH: Rn →Rr isaninvariantforthestateprocess{X }
t t≥0 t t≥0
ifHisconservedovertime,i.e.,forX ∼π ,wehaveH(X )=H(X )forallt≥0.
0 X0 t 0
Unfortunately,thesenumericalmodelsareusuallynotsufficienttotracktheevolutionofcomplexdynamicalsystems
overtime. Thedynamicalmodelmightsufferfrommodelerrors,discretizationerrors,parameteruncertainties,and
unknownforcingterms[LawandStuart,2015,Carrassietal.,2018]. Toproduceaccuratestateestimates,itistherefore
essentialtocouplethesedynamicalmodelswithincomingdatafromphysicalsensors. Thefieldofdataassimilation,
originallydevelopedbythenumericalweatherpredictioncommunity,providesanelegantframeworktocombinethese
heterogeneoussourcesofinformation[Evensen,1994,Aschetal.,2016]. However, thesecomputationalmethods
haveno“intrinsic”knowledgeoftheinvariantsoftheunderlyingdynamicalsystemand,withoutpropertreatment,
canproducenon-physicalposteriorstateestimates,e.g.,aflowfieldwithnegativemassormassunbalance,anegative
chemicalconcentration,anon-zerodivergenceflowfieldinincompressiblefluidmechanics[Albersetal.,2019,Janjic´
etal.,2014]. Wereferreadersto[Janjic´etal.,2014]forfurthermotivationsonpreservinginvariantsindataassimilation
schemesforgeophysicalapplications. Thus,itiscriticaltoincorporateourlong-standingknowledgeofphysicsinthese
dataassimilationalgorithmstoproducephysicallyadmissiblestateestimates.
Iftheinvariantsoftheunderlyingdynamicalsystemareconstantoverthepriordistribution,weshowinAppendixA
thattheupdateofBayes’ruleguaranteesthattheinvariantsarealsoconstantovertheposteriordistribution. Inthis
setting,violationsintheinvariantsareentirelyduetothediscrete1treatmentoftheinferenceproblem. Inthiswork,we
areinterestedindiscreteinferencealgorithmsthatpreservethiscriticalpreservationpropertyofBayes’rule.
Here,wefocusonpreservinglinearinvariants,i.e.,invariantsoftheformH(X) = U⊤XwithU ∈ Rn×r. Linear
⊥ ⊥
invariants are ubiquitous in engineering and science. Examples include the conservation of mass, divergence-free
conditionsinincompressiblefluidmechanics[Kajishimaetal.,2016],forcebalanceinstatics[CraigJrandKurdila,
2006],conservationofelectricalintensitiesandcurrentsinKirchhoff’slaws,orstoichiometricbalanceofchemical
speciesinchemicalreactions.
Precisely,weaimtoconservelinearinvariantsinthecontextoffiltering. Thefilteringproblemisaclassicalsetting
ofdataassimilation,whereweseektosequentiallyassimilateobservationsinourstateestimate[Sanz-Alonsoetal.,
2023]. LetusdenotebyX ∈RnandY ∈Rdthestateandobservationvariablesattimet,respectively. Inthefiltering
t t
problem,weseektoestimatetheso-calledfilteringdistributionπ ,wherey⋆,...,y⋆aretherealizationsof
Xt|Y1:t=y⋆
1:t
1 t
theobservationvariablesuptotimet. Forgenericandhigh-dimensionalstate-spacemodels,thefilteringdistributionis
1Weusetheword“discrete”torefertoanyfiniterepresentationoftheinferenceproblem, e.g., finitesetofsamples, finite
computationalgrid,orevenrepresentationofoperators/functionsinaparametricspace.
2LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
eitherunavailableinclosedformorcomputationallychallenging. Ensemblefiltersareanimportantclassofalgorithms
forbuildingMonteCarloapproximationsofthefilteringdistributionbyupdatingasetofsamplesovertime[Asch
etal.,2016,Carrassietal.,2018]. Theyoperateintwosteps: theforecaststepthatpropagateseachsamplethrough
thedynamicalmodelandtheanalysisstepthatupdatesthesampleswithincomingobservations. Furtherdetailswill
beprovidedinSection3. Importantly, theanalysisstepdoesnotinvolvetimepropagationandcanbetreatedasa
staticinverseproblem[LeProvostetal.,2021]. Buildinguponthepreviousworks[Spantinietal.,2022,LeProvost
etal.,2021,2023,Ramgraberetal.,2023],weviewtheanalysisstepastheapplicationofatransformation(calledthe
analysismap)T
y⋆
t
thatmapsforecastsamples{(y(i),x(i))}fromthejointforecastdistributionπ
(Yt,Xt)|Y1:t−1=y⋆
1:t−1
tosamples{x( ai)}fromthefilteringdistributionπ Xt|Y1:t=y⋆
1:t
withx a(i) =T y⋆ t(y(i),x(i)). Forinstance,theensemble
Kalmanfilter(EnKF)[Evensen,1994]constructsanestimatorfortheaffinelinearanalysismapoftheKalmanfilterby
replacingthecovarianceswithempiricalcovariancescomputedfromthejointsamplesoftheobservationsandstates
{(x(i),y(i))}∼π [Spantinietal.,2022,LeProvostetal.,2022].
(Xt,Yt)|Y1:t−1=y⋆
1:t−1
OutsidetheGaussiansettingthatunderpinstheKalmanfilterandarecentextensiontot-distributions[LeProvost
et al., 2023], it is challenging to obtain analytical analysis maps for an arbitrary joint forecast distribution
π . Toaddresstheselimitations,Spantinietal.[2022]introducedaframeworkforestimatinganaly-
(Xt,Yt)|Y1:t−1=y⋆
1:t−1
sismapsinnon-Gaussiansettingsusingmeasuretransporttheory. Thisactivefieldofresearchaimsatcharacterizinga
targetdistributionπasthetransformationofasimplerreferencedistributionηbyamapS[Villanietal.,2009,Marzouk
etal.,2016,Peyréetal.,2019]. Precisely,Spantinietal.[2022]proposedagenericconstructionfortheanalysismap
T usingawell-chosentransportmapS thatpushesforwardthejointforecastdistributionπ to
y⋆ (Xt,Yt)|Y1:t−1=y⋆
1:t−1
the(n+d)-dimensionalstandardGaussianreferencedistribution. Theirmethodologyprovidesaprincipledgeneraliza-
tionofthelinearKalmanfiltertononlinearanalysismaps,producingconsistentinferencefornon-Gaussianfiltering
problems. Theresultingensemblefilteriscalledthestochasticmapfilter(SMF).
Asmentionedabove,modernnumericalsolversusedintheforecaststeptypicallypreserveinvariantsofthedynamical
system, such that violations of the invariants are due to flaws in the discrete approximation of the analysis step.
Motivatedbythislimitation,weintroduceaclassofanalysismapspreservinglinearinvariantsinnon-Gaussiansettings.
Thepreservationofinvariantsinensemblefilterscanbeunderstoodinastrongsense(whichthispaperconsiders)or
inaweaksense. Inthestrongsense,theanalysismapproducesposteriorsampleswithexactlythesameinvariants
asthepriorsamples. Thiscorrespondstoscenarioswheretheinvariantsareknownexactly. Intheweaksense,the
analysismapproducesposteriorsamplesthatdonotstrictlyrespecttheinvariantsofthepriorsamples. Thiscorresponds
toscenarioswherethevalueoftheseinvariantsareunknown,andweallowobservationstoupdateourbeliefinthe
invariantsinacontrolledmanner.
Inthiswork,wefocusonpreservinglinearinvariantswhichcorrespondstotheparticularcaseofenforcingthelinear
invariantsofthepriorsamples. TheproblemofenforcingconstraintshasbeenstudiedfortheKalmanfilterandthe
EnKFin[Simon,2010,Amoretal.,2018,Albersetal.,2019,GuptaandHauser,2007,Wuetal.,2019,Zhangetal.,
2020]. Differentstrategieshavebeenpursued: [Simon,2010,Albersetal.,2019]leveragedavariationalformulation
oftheKalmanfilterandtheEnKFtoenforcehardconstraints. [GuptaandHauser,2007]enforcedhardconstraints
intheKalmanfilterbyaugmentingtheobservationswithnoiselessversionoftheconstraints. [Prakashetal.,2010]
extendedthisobservationaugmentationapproachtoaccountforsoftconstraintsintheEnKF.[Janjic´ etal.,2014]used
3LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
aconstrainedoptimizationtoenforcelinearinvariantsandpositivityconstraintsintheEnKF.[Wuetal.,2019]enforced
softconstraintsintheEnKFbyareweightingoftheensemblemembers. [Zhangetal.,2020]introducedtheregularized
EnKFtoaccountforvarioussourcesofpriorknowledge,e.g.,hard/softconstraintsandsparsity.
Incontrast,thepresentworkreliesonafunctionalperspectiveandintroducesagenericframeworktoconstructLinear
invariant-PreservingAnalysisMaps(Lin-PAMs)inthestrongsenseforgeneral—potentiallynon-Gaussian—settings.
Precisely,considerrlinearinvariantsforthestatevariableXgivenbyH: Rn →Rr,x(cid:55)→U⊤x,whereU ∈Rr×n
⊥ ⊥
isasub-unitarymatrix. Weintroduceaclassof(potentiallynonlinear)analysismapsT(cid:101)y⋆ thatpreservelinearinvariants.
t
Thatis,forapairofsamples(y(i),x(i))fromπ withlinearinvariantsH(x(i))=C(i) ∈Rr,then
(Yt,Xt)|Y1:t−1=y⋆
1:t−1
H(T(cid:101)y⋆(y(i),x(i)))=C(i). ToconstructaLin-PAM,weoperateachangeofvariablesfortheanalysismapbetween
t
theambientspaceoftheobservationsandstatesandarotatedspacewherethelinearinvariantsaregivenbythefirst
rotated state coordinates. By performing the inference in this rotated space, one can preserve linear invariants by
omittingtheupdateofthesefirstrotatedstatecoordinatesandfinallyliftingtheresulttotheoriginalspace. Importantly,
weshowthatbyconstructionanempiricalestimatorT(cid:98)y⋆ ofaLin-PAMT(cid:101)y⋆ estimatedfromjointforecastsamples
t t
preserveslinearinvariants, independentlyofthequalityoftheempiricalestimatorT(cid:98)y⋆. Section6specializesthis
t
constructiontotheGaussiansettingandrecoversaprojectedformulationoftheKalmanfilterthatpreserveslinear
invariants[Amoretal.,2018]. Finally,weleveragethisformulationtoconstructalinearinvariants-preservingensemble
Kalmanfiltercompatiblewithexistingregularizationtechniques.
InSection7,weexaminedifferentscenariosregardingpreservinglinearinvariantsbytheKalmanfilterandtheensemble
Kalman filter. In particular, we demonstrate that the ensemble Kalman filter no longer preserves linear invariants
when combined with often-used regularization techniques such as covariance inflation, localization, or tapering
[Janjic´ etal.,2014,Aschetal.,2016]. Inpractice,weareofteninthelow-dataregime,i.e.,thenumberofsamples
M =O(100)issmallcomparedtothedimensionsofthestatesandobservations. Thus,theseregularizationtechniques
arecriticalforaddressingtheconsequencesofrankdeficiencyoftheempiricalKalmangain,suchasspuriouslong-range
correlations,samplingerrors,under-estimationofthestatistics[Aschetal.,2016]. Despitetheirdemonstratedvalue,
theseregularizationtechniquesareknowntoviolatelinearinvariantsoftheforecastdistribution[Janjic´ etal.,2014].
Here,weaddresstheseshortcomingsofexistingregularizedfilters,proposingnewlinearinvariant-preservingones.
Thecontributionsofthispaperare:
• Section5.2introducesaclassofanalysismapspreservinglinearinvariants(Lin-PAMs)inthestrongsense
innon-Gaussiansettings. WeshowthatbyconstructionanempiricalestimatorofaLin-PAMstillpreserves
linearinvariants,independentlyofthequalityoftheestimator.
• Section6specializesthisconstructiontotheGaussiancaseandrecoversaconstrainedformulationofthe
Kalmanfilter[Simon,2010].
• Section7clarifiesresultsregardingthepreservationoflinearinvariantsforthevanillaKalmanfilterandthe
vanillaensembleKalmanfilter.
• Section8.2demonstratesonasyntheticproblemwithanarbitrarynumberroflinearinvariantsthatpreserving
invariantsismostadvantageouswhentheensemblesizeM issmallandtheratiooflinearinvariantsoverthe
statedimensionr/nislarge.
4LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
• Sections8.3and8.4assessthebenefitsofpreservinglinearinvariantsforlinear/nonlinearensemblefilters
onalinearadvectionproblemandalow-dimensionalchaoticsystem,respectively. Wedemonstrateonthese
linear/nonlinearfilteringproblemsthatpreservinglinearinvariantsleadstolowertrackingerror. Finally,we
showthatthetrackingerroroflinearinvariantscanbesignificantfornoninvariant-preservingensemblefilters.
Theremainderofthepaperisorganizedasfollows. Section2presentsournotationconventions. Section3reviewsthe
filteringproblemanditsapproximationwithensemblemethods.Section4reviewstoolsfrommeasuretransporttoderive
theformulaoftheanalysismap. Section5constructstheclassoflinearinvariant-preservinganalysismaps(Lin-PAMs).
Section6specializestheresultsofSection5totheGaussiancase. Section7examinessettingsforwhichtheanalysis
mapofthevanillaKalmanfilterpreserveslinearinvariants. Numericalexperimentsfortheunconstrained/constrained
ensembleKalmanfiltersandstochasticmapfiltersarepresentedinSection8. ConcludingremarksfollowinSection
9. AppendixApresentstheresultonthepreservationofinvariantsinBayesianinferenceproblems. Detailsonthe
recursiveassimilationofconditionallyindependentobservationsarediscussedinAppendixB.Theparameterizationof
thestochasticmapfilterispresentedinAppendixC.Forthesakeofcompleteness,weprovidepseudo-codesforthe
constrainedstochasticmapfilterandtheconstrainedensembleKalmanfilter,seeAlgorithms1and2inAppendicesD
andE.Thecodetoreproduceourcomputationalexperimentsisopenlyavailableathttps://github.com/mleprovost/Paper-
Linear-Invariants-Ensemble-Filters.
2 Nomenclature
Intherestofthismanuscript,weusethefollowingconventions. Seriffontsrefertorandomvariables,e.g.,QonRn
orQonR. Lowercaseromanfontsrefertorealizationsofrandomvariables,e.g.,qonRnorqonR. π denotesthe
Q
probabilitydensityfunctionfortherandomvariableQ,andQ ∼ η meansthattherandomvariableQisdistributed
accordingtoη. Exceptifotherwisestated,weassumethattheprobabilitydensitieshavefullsupport. Themeanand
covariancematrixoftherandomvariableQaredenotedbyµ andΣ ,respectively. Thecross-covariancematrix
Q Q
oftherandomvariablesQandRisdenotedbyΣ . Empiricalquantitiesaredifferentiatedfromtheirasymptotic
Q,R
counterpartsusingcaretsaboveeachsymbol,e.g.,µ . AmatrixU ∈Rn×r isasub-unitarymatrixifitscolumnsare
(cid:98)Q
orthonormal,i.e.,U⊤U =I . Asub-unitarymatrixisorthonormalifr =nandUU⊤ =I .
r n
3 Backgroundonthefilteringproblem
Inthiswork,weconsideragenericstate-spacemodelgivenbythepairofadynamicalmodelandanobservationmodel
forthestateprocess{X } andtheobservationprocess{Y } . Thestateprocess{X } isfullydescribedbyan
t t≥0 t t>0 t t≥0
initialdistributionπ andadynamicalmodelthatpropagatesthestateforwardintime:
X0
X =f(X )+W , fort≥0, (1)
t+1 t t
where f : Rn → Rn is the forward operator and the process noise W ∈ Rn is independent of the state X .
t t
Unfortunately,wedonottypicallyhaveaccesstofullobservationsofthestate,insteadthestateprocess{X } is
t t≥
onlyobservedthroughanindirectandperturbedprocess{Y } ,wheretheobservationY attimetisgivenbythe
t t>0 t
observationmodel
Y =g(X )+E , fort>0, (2)
t t t
5LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
whereg: Rn →Rd istheobservationoperatorandtheobservationnoisevariableE isindependentofthestateX .
t t
Weassumethatnoobservationsarecollectedattimet=0.
Weareinterestedinthefilteringproblemwherewewanttocharacterizethefilteringdensityπ , which
Xt|Y1:t=y⋆
1:t
describestheprobabilityofaparticularstaterealizationattimetgivenalltherealizationsoftheobservationvariableY
uptothattime[Aschetal.,2016]. Thefilteringdensityπ cannotbecomputedinclosedformforgeneric
Xt|Y1:t=y⋆
1:t
state-spacemodelsandnon-Gaussianinitialstatedistributions. Thislimitationmotivatedthedevelopmentofempirical
approximationsofthefilteringdensity,presentedinthenextparagraph. Itisimportanttonotethatthroughtheform
of the dynamical and observation models, we are making particular assumptions on the conditional independence
structureofthejointdistributionofthestateandobservationprocessesπ [Carrassietal.,2018,Evensenetal.,
X0:t,Y1:t
2022]. Indeed,weassumethatthestateprocessfollowsaMarkovchainsuchthatthestateattimetisconditionally
independent of the state realizations at previous times given the state at time t−1, i.e., X ⊥⊥ X |X for any
t s t−1
s ≤ t−1. Thisimpliesπ = π . Similarly,weassumethattheobservationvariableY attimetis
Xt|X1:t−1 Xt|Xt−1 t
conditionallyindependentofthestaterealizationsatprevioustimesgiventhestateattimet,i.e.,π =π .
Yt|X1:t Yt|Xt
Undertheseconditionalindependenceassumptions,wecanfactorizethejointdistributionπ as
X0:T,Y1:T
T
(cid:89)
π = π π π . (3)
X0:T,Y1:T X0 Yt|Xt Xt|Xt−1
(cid:124)(cid:123)(cid:122)(cid:125) t=1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
initialdistribution observationmodeldynamicalmodel
Fromthisdecomposition,wederivearecursiverelationtopropagatethefilteringdistributionattimetfromthefiltering
distributionattimet−1:
(cid:90)
π ∝π π =π π π dX (4)
Xt|Y1:t Yt|Xt Xt|Y1:t−1 Yt|Xt Xt|Xt−1 Xt−1|Y1:t−1 t−1
Thisrecursiveupdateoperatesintwosteps. First,theforecastdistributionπ isobtainedbypropagatingthe
Xt|Y1:t−1
filteringdistributionπ throughthetransitionkernelπ ofthedynamicalmodel(1),i.e.,π =
Xt−1|Y1:t−1 Xt|Xt−1 Xt|Y1:t−1
(cid:82)
π π dX . ThislastequationisknownastheChapman-Kolmogorovequation[Aschetal.,2016,
Xt|Xt−1 Xt−1|Y1:t−1 t−1
Carrassietal.,2018]. Second, weapplyBayes’ruletoconditiontheforecastdistributionontherealizationofthe
observationattimet,resultinginthefilteringdistributionattimet.
Asmentionedearlier,solvinganalyticallythefilteringproblemisgenerallyinfeasibleoutsideofthelinearGaussian
setting [Asch et al., 2016]. Instead, we rely on a Monte Carlo approximation of the filtering density π by
Xt|Y1:t
propagating a set of M samples {x(i)} over time. It has been established that particle filters — without adequate
treatment—sufferfromthecurseofdimensionalityandrequireanexponentiallygrowingnumberofsampleswiththe
dimensionoftheproblem[Bengtssonetal.,2008,Snyderetal.,2008,Cotteretal.,2020]. Wedonotconsiderthisclass
ofmethodsinthisworkandfocusonalgorithmsassigningequalweightstoallthesamples. Wecallalgorithmsfalling
underthiscategoryensemblefilteringmethods[Carrassietal.,2018,Spantinietal.,2022]. Thesealgorithmsmimic
thetwo-steprecursiveupdateofthefilteringdistributionpresentedin(4)atthesamplelevel. Inthefirststep,called
theforecaststep,eachsampleispropagatedthroughthedynamicalmodel(1). TheresultingsamplesformaMonte
Carloapproximationoftheforecastdistributionπ . Inthesecondstep,calledtheanalysisstep,wecondition
Xt|Y1:t−1
theforecastsamplesontherealizationoftheobservationvariableattimet,denotedy⋆. Weusethestarsuperscriptto
t
stressthaty⋆istheobservationtoassimilate. Toconditionforecastsamplesonthisnewobservationrealization,we
t
leveragetheformalismofmeasuretransport[Marzouketal.,2016,Spantinietal.,2022]. Tothisend,werelyonthe
existenceofatransformationT y⋆,calledprior-to-posteriortransformationoranalysismap,thattransformsthejoint
t
6LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
forecastdistributionπ intothefilteringdistributionπ . UnderGaussianassumptions
(Yt,Xt)|Y1:t−1=y⋆
1:t−1
Xt|Y1:t=y⋆
1:t
onthejointforecastdistributionofthestatesandobservations,theanalysismapcorrespondstothecelebratedKalman
filterupdate[Kalman,1960,Spantinietal.,2022]. Inpractice,theanalysismapmustbeapproximatedfromsamples
{(y(i),x(i))}ofthejointforecastdistributionπ . Forinstance,theensembleKalmanfilter(EnKF)
(Yt,Xt)|Y1:t−1=y⋆
1:t−1
introducedbyEvensen[1994]reliesonaMonteCarloapproximationoftheKalmangainΣ Σ−1fromthejoint
Xt,Yt Yt
forecastsamples.
4 Backgroundonensembletransportmethods
Thegoalofthissectionistoreviewtriangulartransportmaps. Inparticular,werecallsomeoftheirappealingproperties
forconditionalinferenceandhowtheycanbeleveragedtoconstructanalysismaps[Spantinietal.,2022].
4.1 Overviewoftriangulartransportmethods
Westartbyrecallingsomeintroductoryelementsontriangulartransportmethods[Marzouketal.,2016]. Givena
targetdistributionwithdensityπ :Rm →R,itishelpfultodescribeitasthetransformationofasimplerreference
distributionwithdensityη :Rm →RbyamapS: Rm →Rm. AbijectiveanddifferentiablemapS thattransforms
the distribution π into η is called a transport map, and we say that S “pushes forward” π to η, denoted S π = η
♯
[Marzouk et al., 2016]. The formula for the push-forward distribution S π corresponds to the classical change of
♯
variablesinmultivariatecalculus:
S π(z)=π(S−1(z))det∇ S−1(z) (5)
♯ z
Theperspectiveoftransportmapsisappealingforsamplingpurposesasi.i.d.samples{z(i)}fromηgetmappedtoi.i.d.
samples{S(z(i))}fromπ[Marzouketal.,2016].BuildingsuchtransformationsSisthecoretopicofmeasuretransport
theory[Marzouketal.,2016],classicallyviewedthroughtheperspectiveofcostminimization[Villanietal.,2009,
Peyréetal.,2019]. Inthiswork,wetakeadifferentperspectiveandfocusontransformationswithappealingproperties
forBayesianinference. Specifically,weareinterestedintransportmapstailoredforsamplingfromtheconditionalsofa
jointdistribution. InthecontextoftheanalysisstepofthefilteringproblemdiscussedinSection3,wehavesamples
fromthejointforecastdistributionπ andseektogeneratesamplesfromthefilteringdistributionπ .
(Yt,Xt)|y⋆
1:t−1
Xt|y⋆
1:t
Amongthetransportmapspushingforwardπtoη,weconsidertheKnothe-Rosenblattrearrangement[Rosenblatt,1952]
definedastheuniquelowertriangularandstrictlyincreasingtransformationS: Rm →Rm tofacilitateconditional
sampling:
 
S1(z )
1
 
 S2(z 1,z 2) 
S(z)=S(z 1,z 2,··· ,z m)=  . .  , (6)
 . 
 
Sm(z ,z ,...,z )
1 2 m
where the strict monotonicity of Sk : Rk → R signifies that the univariate function ξ (cid:55)→ Sk(z ,ξ) is strictly
1:k−1
monotonicallyincreasingforallz ,z ,...,z .ThelowertriangularstructureoftheKnothe-Rosenblattrearrangement
1 2 k−1
presents both theoretical and computational benefits. If the reference density η can be factorized, i.e., η(z) =
(cid:81)m η (z ),thenMarzouketal.[2016]showedthattheunivariatefunctionξ (cid:55)→Sk(z ,ξ)resultingfromfixing
i=1 i i 1:k−1
thefirstk−1entriespushesforwardtheconditionaldistributionπ (ξ|z )tothekthcomponentofthe
Zk|Z1:k=1 1:k−1
7LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
referencedensity,i.e.,Sk(z ,·) π (ξ|z )=η . OncethemapShasbeendetermined,onecaneasily
1:k−1 ♯ Zk|Z1:k=1 1:k−1 k
representanyconditionalofthetargetdistribution. Inthenextsection,wewillshowhowtoleveragethispropertyto
constructtheanalysismap. Wenotethatmonotonelowertriangulartransformationsarealsocomputationallyattractive
asthedeterminantoftheirJacobianreducestotheproductofthepartialderivativeofeachmapcomponentwithrespect
toitslastentries. Moreover,theinversionoflowertriangulartransformationsreducestoasequenceofunivariateroot
findingproblems[Marzouketal.,2016]. Remarkably,theKnothe-RosenblattrearrangementisknownintheGaussian
case(seeRemark1below)andhasrecentlybeenidentifiedformultivariatet-distributionsbyLeProvostetal.[2023].
Remark1(TheGaussiancase). ConsiderX∼π =N (µ,Σ)andtheCholeskyfactorizationL⊤L=Σ−1. Then
X
S(x)=L(x−µ)istheKnothe-Rosenblattrearrangementthatpushesforwardπ toη =N (0 ,I ). Theproofuses
X n n
propertiesonlineartransformationsofGaussianvariables,andisomittedforbrevity.
Remark2. Forconvenience,wedropthetime-dependentsubscriptsfromthevariablesintherestofthispaper,since
theanalysisstepdoesnotinvolvetimepropagation.
4.2 Constructionoftheanalysismap
We now revisit the construction of the analysis map T that pushes forward the joint forecast distribution
y⋆
π to the filtering distribution π . In the rest of the paper, we omit the time depen-
(Yt,Xt)|Y1:t−1=y⋆
1:t−1
Xt|Y1:t=y⋆
1:t
dencesubscriptofthevariablesasoftenaspossible,astheanalysisstepinvolvesastaticBayesianinverseproblem
[Spantinietal.,2022,LeProvostetal.,2022]. Thejointrandomvariable(Y,X)willrefertothejointforecastrandom
variable(Y ,X )|Y =y⋆ .
t t 1:t−1 1:t−1
ConsidertheKnothe-RosenblattrearrangementS thatpushesforwardthejointdistributionoftheobservationsand
statesπ totheproductreferencedistributionη = η ⊗η withη : Rd → R andη : Rn → R. Fromitslower
Y,X Y X Y X
triangularstructure,S canbepartitionedintwoblocks:
 
SY(y)
S(y,x)= , (7)
SX(y,x)
where SY: Rd →− Rd and SX: Rd ×Rn →− Rn. [Baptista et al., 2020, Theorem 2.4] showed that if S is lower
triangularandpushesforwardπ toaproductreferencedistributionη =η ⊗η ,thenSX pushesforwardπ
Y,X Y X X|Y=y
toη ,i.e.,SX(y,·) π =η . WenotethatitsufficesforS tobelowerblocktriangularforthispropositionto
X ♯ X|Y=y X
hold,i.e.,SY andSX donotneedtobelowertriangular. Asthedistributionsη andη areuser-specified,onecanuse
Y X
standardGaussiandistributions.
OncethemapSX hasbeenlearned,onecaneasilygeneratesamplesfromπ fromη . Inpractice,thelearned
X|Y=y⋆ X
mapSX isimperfect,suchthaterrorsintheempiricalmapestimateS(cid:98)X willbepropagatedintheapproximationof
X
theconditionaldistributionviathepullbackdistributionS(cid:98) (y⋆,·)♯η X. Toreduceerrorpropagationintheconditional
distribution,Spantinietal.[2022]consideredacompositeanalysismapbuiltbypartialinversionofthemapSX. As
aconsequenceofthepushforwardrelation, wehavethattheconditionalmapx (cid:55)→ SX(y,x)isabijectiononRn
foranyy ∈ Rd. Let(y,x)beajointsamplefromπ . Forarealizationy⋆ oftheobservationvariableYthatwe
Y,X
wanttoconditionon,thereexistsauniqueelementx ∈RnsuchthatSX(y⋆,x )=SX(y,x). Thiselementx is
a a a
preciselytheposteriorupdateofthestatexgiventherealizationy⋆oftheobservationvariable. See[Spantinietal.,
2022,LeProvostetal.,2021]forfurtherdetails. Finally,thisyieldsthefollowinganalysismapT : Rd×Rn →Rn
y⋆
8LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
[Spantinietal.,2022]:
T (y,x)=SX(y⋆,·)−1◦SX(y,x), (8)
y⋆
wherethenotationSX(y⋆,·)−1 denotestheinversionofthemapx(cid:55)→SX(y⋆,x)forfixedy⋆ ∈Rd. Weconclude
thissectionbyconnectingtheKalmanfilterandtheSMFtotheanalysismapof(8)inRemarks3and4.
Remark3(ConnectionwiththeKalmanfilter). Let’sconsidertworandomvariablesX∈RnandY ∈Rdsuchthat
(Y,X)isjointlyGaussian,i.e.,
(cid:34) (cid:35) (cid:32)(cid:34) (cid:35) (cid:34) (cid:35)(cid:33)
Y µ Σ Σ⊤
∼N X , Y X,Y . (9)
X µ Σ Σ
Y X,Y X
(cid:16) (cid:17)
Thenfory ∈ Rd,π = N µ ,Σ withµ = µ +Σ Σ−1(y−µ )andΣ =
X|Y=y X|Y=y X|Y=y X|Y=y X X,Y Y Y X|Y=y
Σ −Σ Σ−1Σ⊤ .LetL⊤ L =Σ−1 betheCholeskyfactorizationofΣ−1 .ThenSX(y,x)=
X X,Y Y X,Y X|Y=y X|Y=y X|Y=y X|Y=y
L (x−µ ). Byapplying(8),wegetT (y,x)=x−Σ Σ−1(y−y⋆),recoveringtheanalysismap
X|Y=y X|Y=y y⋆ X,Y Y
oftheKalmanfilterasnotedbySpantinietal.[2022]. Thus,(8)correspondstoageneralizationoftheKalmanfilter
fornon-Gaussianjointdistributionoftheobservationsandstatesπ .
Y,X
Remark4(Thestochasticmapfilter). TheSMFintroducedin[Spantinietal.,2022]buildsanestimatorS(cid:98)X forSX
X
fromjointforecastsamplesofthestatesandobservations. TheestimatorS(cid:98) isbasedonaparsimoniousexpansionsof
radialbasisfunctionsestimatedbysolvingdecoupledandconvexoptimizationproblems. Thefilteringsamplesare
X
obtainedbyapplyingtheestimatedanalysismapof (8)fromS(cid:98) tothejointforecastsamples. Thus,thestochasticmap
filtercanbeviewedasanonlineargeneralizationoftheensembleKalmanfilter.
5 Linearinvariant-preservinganalysismaps(Lin-PAMs)
Inthissection,weproposeanewmethodologyforconstructinganalysismapsthatpreservelinearinvariantsofthe
state. ConsiderthelinearinvariantsH(x)=U⊤x. WewilldesignananalysismapT thatpushesforwardπ to
⊥ y⋆ Y,X
π whileensuringtheinvariantHispreserved,i.e.,H(T (y,x))=H(x)forall(y,x)∈Rd×Rn. Wecall
X|Y=y⋆ y⋆
ananalysismapverifyingthispropertyalinearinvariant-preservinganalysismap(Lin-PAM).Ourapproachworksfor
arbitrarynonlinearanalysismapandnon-Gaussianjointdistributionoftheobservationsandstatesπ . Tothisend,
Y,X
weassumethatthematrixU ∈Rn×r hasrankr. Furthermore,withoutlossofgenerality(seeRemark5below),we
⊥
assumethatU isasub-unitarymatrix,i.e.,U hasorthonormalcolumns.
⊥ ⊥
Remark5. ConsideralinearconstraintU⊤x=C ∈Rr,whereU⊤isnotnecessarilyasub-unitarymatrix. We
⊥ ⊥ ⊥
canthenuseathinQRfactorizationU = Q R , whereQ ∈ Rn×r issub-unitaryandR ∈ Rr×r islower
⊥ ⊥ ⊥ ⊥ ⊥
triangular[GolubandVanLoan,2013],torewritethelinearconstraintasQ⊤x=R−⊤C ∈Rr. Notably,Q⊤isa
⊥ ⊥ ⊥ ⊥
asub-unitarymatrix.
BasedontheorthonormalityofU ,wecanuniquelydecomposethestatex∈Rnas
⊥
x=U U⊤x+(I −U U⊤)x=U x ⊕U x . (10)
⊥ ⊥ ⊥ ⊥ ⊥ ⊥ ∥ ∥
where x = U⊤x ∈ Rr and x = U⊤x ∈ Rn−r. Furthermore, the matrix U is such that the columns of
⊥ ⊥ ∥ ∥ ∥
U =[U ,U ]∈Rn×nformanorthonormalbasisofRn. Inpractice,onecanuseaQRfactorizationofU tobuild
⊥ ∥ ⊥
U . Thus,wehavex=U[x ;x ],or,equivalently,[x ;x ]=U−1x=U⊤x.
∥ ⊥ ∥ ⊥ ∥
9LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
Equation(10)ispivotalinourconstructionofLin-PAMs. Itallowsustoreformulatetheproblemofbuildinganalysis
mapspreservinglinearinvariantsH(x)=U⊤xasbuildingananalysismappreservingthestatecomponentsinthe
⊥
spanofU . Therestofthissectionisorganizedasfollows. Section5.1formulatestheanalysismapof(8)inthe
⊥
rotatedspacegivenbythechangeofvariables(Y,X)(cid:55)→(Y,[U ,U ]⊤X). Section5.2leveragesthisformulationto
⊥ ∥
proposeaformulationforLin-PAMsexpressedintheoriginalspaceofthestatesandobservations.
5.1 Formulationoftheanalysismapintherotatedspace
In this section, we apply the change of coordinates (Y,X) (cid:55)→ (Y,[U ,U ]⊤X) = (Y,X ,X ) and express the
⊥ ∥ ⊥ ∥
analysis map of (8) in the rotated space. The distribution π is given by the pushforward of π by the linear
X⊥,X∥ X
transformationU,i.e.,π =U π . Usingthepushforwardformula(5),wehave
X⊥,X∥ ♯ X
π (x ,x )=π (U[x ;x ])det∇(U[x ;x ])=π (U U⊤x+U U⊤x)=π (x). (11)
X⊥,X∥ ⊥ ∥ X ⊥ ∥ ⊥ ∥ X ⊥ ⊥ ∥ ∥ X
Thus,wehavethefollowingfactorizationofπ :
Y,X
π (y,x)=π (y,x ,x )=π (y)π (x |y)π (x |y,x ) (12)
Y,X Y,X⊥,X∥ ⊥ ∥ Y X⊥|Y ⊥ X∥|Y,X⊥ ∥ ⊥
LetusnowconsidertheKnothe-RosenblattrearrangementS: Rd×Rr×Rn−r thatpushesforwardπ tothe
Y,X⊥,X∥
productofstandardGaussianreferencesη ⊗η ⊗η ,whereη ,η ,η isdefinedonRd,Rr,Rn−r,respectively.
Y X⊥ X∥ Y X⊥ X⊥
WedenoteitasS π =η ⊗η ⊗η . Fromitslowertriangularstructure,wecanpartitionS as
♯ Y,X⊥,X∥ Y X⊥ X∥
 
SY(y)
 
S(y,x ⊥,x ∥)=

SX⊥(y,x ⊥)  , (13)
 
SX∥(y,x ⊥,x ∥)
whereSY: Rd →Rd,SX⊥: Rd×Rr →Rr,andSX∥: Rd×Rr×Rn−r →Rn−r. Fromthelowerblockstructure
ofS and[Baptistaetal.,2020,Theorem2.4],wehavethefollowingrelationsbetweentheconditionalsofπ
Y,X⊥,X∥
andthemarginalsofη:
SY π =η ,
♯ Y Y
SX⊥ π =η , (14)
♯ X⊥|Y X⊥
SX∥♯π
X∥|Y,X⊥
=η
X∥
Lety⋆bearealizationoftheobservationvariableY. Usingtherelationsin(14),weproceedasfollowstosamplefrom
theconditionaldistributionπ ,orequivalentlyπ . Let(y,x ,x )beajointsamplefromπ .
X|Y=y⋆ X⊥,X∥|Y=y⋆ ⊥ ∥ Y,X⊥,X∥
Following the derivation of the analysis map in Section 4.2, we generate samples from π by seeking the
X⊥|Y=y⋆
solutionx ∈Rr of
⊥,a
SX⊥(y⋆,x )=SX⊥(y,x ). (15)
⊥,a ⊥
Formally,wethereforegettheanalysismapT⊥ :Rd×Rr →Rr thatpushesforwardπ toπ as
y⋆ Y,X⊥ X⊥|Y=y⋆
T⊥ (y,x )=SX⊥(y⋆,·)−1◦SX⊥(y,x ). (16)
y⋆ ⊥ ⊥
Thenwegeneratesamplesfromπ byseekingthesolutionx ∈Rn−r of
X∥|y⋆,x⊥,a ∥,a
SX∥(y⋆,x ⊥,a,x ∥,a)=SX∥(y,x ⊥,x ∥). (17)
10LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
Similarto(16),wewritetheanalysismapT∥ : Rd ×Rr ×Rn−r → Rn−r thatpushesforwardπ to
y⋆,x⊥,a Y,X∥,X⊥
π as
X∥|Y=y⋆,X⊥=x⊥,a
T∥ y⋆,x⊥,a(y,x ⊥,x ∥)=SX∥(y⋆,T⊥ y⋆(y,x ⊥),·)−1◦SX∥(y,x ⊥,x ∥). (18)
Thus,theanalysismapT : Rd×Rn →Rnformulatedintheoriginalspacethatpushesforwardπ toπ is
y⋆ Y,X X|Y=y⋆
(cid:34) (cid:35)
(cid:104) (cid:105) x
T y⋆(y,x)= U ⊥,U
∥
x⊥,a =U ⊥T⊥ y⋆(y,U⊤ ⊥x)+U ∥T∥ y⋆,x⊥,a(y,U⊤ ⊥x,U⊤ ∥x). (19)
∥,a
Notably, (19) provides an alternative formulation for the analysis map of (8) by building analysis maps T⊥ and
y⋆
T∥ inthespacesspannedbythecolumnsofU andU . Westressthat(19)and(8)arestrictlyequivalentas(19)
y⋆,x⊥,a ⊥ ∥
reliesonthefactorizationofπ intoπ π π insteadofπ π for(8). Interestingly,(19)proposesto
Y,X Y X⊥|Y X∥|X⊥,Y Y X|Y
performtheinferencebyfirstrotatingthestatealongthecolumnsofU,thenperformingtheinferenceforX followed
⊥
byX ,andfinallyliftingtheresulttotheoriginalspace. Theideaofperforminginferenceinadifferentcoordinate
∥
system has been previously exploited by [Le Provost et al., 2021, 2022]. These works used likelihood-informed
dimensionreductionoftheobservationsandstatestoperforminferenceintheidentifiedlow-informativesubspacewith
low-dimensionallinear/nonlinearanalysismaps.
5.2 Preservingthelinearinvariantsintherotatedspace
We stress that there is no reason for the analysis map of (19) to preserve the linear invariants H(x) = U⊤x in
⊥
generatingtheposteriorsamplesfromπ . Thestatedecomposition(10)suggeststhatpreservingH(x)corresponds
X|Y
togeneratingposteriorsamplesfromπ . Todoso,wemodifytheanalysismapof(19)byomittingthe
X|Y=y⋆,X⊥=x⊥
⊥ ⊥
updateofX ⊥. ThisisequivalenttoconstraintheanalysismapT(cid:101)
y⋆
tobetheidentity,i.e.,T(cid:101) y⋆(y,x ⊥)=x ⊥. Thus,
∥
weobtaintheconstrainedanalysismapT(cid:101)
y⋆
as
T(cid:101)∥ y⋆(y,x ⊥,x ∥)=SX∥(y⋆,x ⊥,·)−1◦SX∥(y,x ⊥,x ∥). (20)
We note that the second argument of SX∥−1 is x
⊥
in the constrained formulation instead of T⊥ y⋆(y,x ⊥) in (18).
Finally,theanalysismapT(cid:101)y⋆ formulatedintheoriginalspacepreservingtheinvariantH(x)=U⊤ ⊥xreads
T(cid:101)y⋆(y,x)=U ⊥T(cid:101)⊥ y⋆(y,U⊤ ⊥x)+U ∥T(cid:101)∥ y⋆(y,U⊤ ⊥x,U⊤ ∥x)
(21)
=U ⊥U⊤ ⊥x+U ∥T(cid:101)∥ y⋆(y,U⊤ ⊥x,U⊤ ∥x).
∥
(21) and (20) form the cornerstone of the Lin-PAM methodology. In practice, an empirical estimator T(cid:98)
y⋆
for the
∥
constrainedanalysismapT(cid:101)
y⋆
of(20)isbuiltateachassimilationcyclefromjointforecastsamples{y(i),x(i)}of
π
(Yt,Xt)|Y1:t−1=y⋆
1:t−1. Then, we obtain the resulting constrained estimator T(cid:98)y⋆ for the Lin-PAM T(cid:101)y⋆ of (21) by
∥ ∥
replacingT(cid:101)
y⋆
withT(cid:98) y⋆.Remark6showsthatanempiricalestimatorT(cid:98)y⋆ ofaLin-PAMstillpreserveslinearinvariants.
Remark7discussestheconstructionofLin-PAMof(21)whenthelinearinvariantsareconstantovertheprior.
WereferreaderstoFigure1foranoverviewoftheunconstrainedandconstrainedanalysismapspresentedinthis
section. InAppendixD,weprovideapseudo-codefortheconstrainedstochasticmapfilter(ConsSMF)thatperforms
the constrained inference of (21) from samples {(y(i),x(i))} of π . The ConsSMF transforms a set of forecast
Y,X
samplestofilteringsamplesbyassimilatingtherealizationy⋆oftheobservationvariablewhilepreservingthelinear
invariantsoftheforecastsamples. SeeAppendixCor[Spantinietal.,2022]formoredetailsontheestimationofthe
11LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
Knothe-Rosenblattrearrangement—underpinningthestochasticmapfiltersusedinthiswork—thatpushesforwardthe
jointforecastdistributionπ tothestandardGaussiandistribution.
(Yt,Xt)|Y1:t−1=y⋆
1:t−1
∥
Remark6(AnempiricalestimatorofaLin-PAMpreserveslinearinvariants). LetT(cid:98)
y⋆
beanempiricalestimator
∥
fortheconstrainedanalysismapT(cid:101)
y⋆
of (20),andT(cid:98)y⋆ betheresultingestimatoroftheLin-PAMT(cid:101)y⋆ of (21). In
∥
practice,theestimatorT(cid:98)
y⋆
isimperfectasweestimateitfromsamples{(y(i),x(i))}ofanapproximationofthejoint
forecastdistributionπ ,andtherealizationy⋆toassimilatecanoriginatefromanapproximation
(Yt,Xt)|Y1:t−1=y⋆
1:t−1
t
∥
oftheobservationdistributionπ Yt. IndependentlyofthesediscrepanciesandthequalityoftheestimatorT(cid:98)
y⋆
for
∥
T(cid:101) y⋆,anempiricalestimatorT(cid:98)y⋆ preserveslinearinvariants. Toprovethis,weintroducetheclassoftransformations
T ={T: Rd×Rn →Rn|T(y,x)=U U⊤x+U K(y,x),withK: Rd×Rn →Rn−r}. Bynotingthatany
LinP ⊥ ⊥ ∥
estimatorT(cid:98)y⋆ belongstoT LinP(withK(y,x)=T(cid:98)∥ y⋆(y,U⊤ ⊥x,U⊤ ∥x))andthatelementsofT LinPpreservethelinear
invariantsH(x)=U⊤x,weobtainthedesiredresult.
⊥
Remark7(Treatmentofconstantlinearinvariantsovertheprior). Assumethatthelinearinvariantsareconstantover
thepriorπ ,i.e.,U⊤x=C ∈Rr foranyrealizationx∈RnofX. Thus,thepriorπ isnotsupportedonRnbuton
X ⊥ X
theaffinespace{x ∈ Rn|U⊤x = C}. ThisremarkexplainshowtoadapttheconstructionofLin-PAMof (21)to
⊥
thissetting. Letx∈RnbearealizationofX. Fromthestatedecomposition(10),wehavex=U C+U x with
⊥ ∥ ∥
x ∈Rn−r. Thedistributionπ becomesapointmasscenteredatC,i.e.,π (x )=π (x )=δ(C−x ),
∥ U⊤ ⊥X U⊤ ⊥X ⊥ X⊥ ⊥ ⊥
whereδdenotestheDiracdeltadistribution. Thus,wecanomittherotatedstatevariableX fromtheanalysisand
⊥
operateonthereducedjointspace(Y,U⊤X)=(Y,X ). WedefinethereducedmapSX∥ pushingforwardπ
∥ ∥ reduced Y,X∥
toη asSX∥ : Rd×Rn−r →Rn−r, (y,x )(cid:55)→SX∥ (y,x ). Westressthatwedon’tneedtodefineareduced
X∥ reduced ⊥ reduced ∥
mapSX⊥ andthatSX∥ doesnotdependontherotatedstatecoordinatesx . Thereducedconstrainedanalysis
reduced reduced ⊥
mapT(cid:101)∥ y⋆,reduced: Rd×Rn−r →Rn−r reads
T(cid:101)∥ y⋆,reduced(y,x ∥)=SX red∥ uced(y⋆,·)−1◦SX red∥ uced(y,x ∥). (22)
Finally,thereducedconstrainedanalysismapT(cid:101)y⋆,reduced: Rd×Rn →Rnformulatedintheoriginalspacereads
T(cid:101)y⋆,reduced(y,x)=U ⊥C+U ∥T(cid:101)∥ y⋆,reduced(y,U⊤ ∥x). (23)
Thus,thestateupdateisconfinedtotheaffinespace{x∈Rn|U⊤x=C},asweonlyupdatethestatecoordinates
⊥
U⊤xspanningthissubspace.
∥
6 SpecializationtotheGaussiancase
Inthissection,wespecializetheconstrainedanalysismapT(cid:101)y⋆ of(21)fortheGaussiancase. Let(Y,X)beapairof
randomvariableswithY ∈RdandX∈RnthatarejointlyGaussian,i.e.,
(cid:34) (cid:35) (cid:32)(cid:34) (cid:35) (cid:34) (cid:35)(cid:33)
Y µ Σ Σ⊤
∼N X , Y X,Y . (24)
X µ Σ Σ
Y X,Y X
WedefinethechangeofcoordinatesU(cid:101) transforming(Y,X ⊥)into(Y,X ⊥,X ∥)as
 
(cid:34) (cid:35) I d 0 d×n
U(cid:101)⊤ = I d 0 d×n =  0 U⊤  . (25)
0 U⊤  d×r ⊥ 
d×n 0 U⊤
d×n−r ∥
12LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
T (y,x)
y⋆
T(cid:101)y⋆(y,x)
U T⊥ (y,x )
⊥ y⋆ ⊥
x
Projectonspan(U )
⊥
Projectonspan(U )
∥
U x
⊥ ⊥
∥
U ∥x
∥
U ∥T(cid:101) y⋆(y,x ⊥,x ∥)
RotationbyU
Figure1: SchematicoftheinferencewiththeunconstrainedanalysismapT of(19)andtheconstrainedanalysis
y⋆
mapT(cid:101)y⋆ of(21). TheupdateoftheunconstrainedmapT
y⋆
operatesasfollows: (i)constructtheanalysismapT y⋆,
(ii)applyT
y⋆
togettheposteriorupdateofthestate, i.e., x
a
= T y⋆(y,x). Incontrast, theconstrainedmapT(cid:101)y⋆
∥ ∥
operatesasfollows: (i)projectxontothecolumnsofU ⊥andU ∥,(ii)constructtheanalysismapT(cid:101) y⋆,(iii)applyT(cid:101)
y⋆
togettheposteriorupdateofthestateinthespanofU ,(iv)lifttheresulttotheoriginalspace.
∥
13LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
ByclosureofGaussiandistributionsunderlineartransformations,thedistributionfor(Y,X ,X )isalsoGaussian
⊥ ∥
withstatistics
     

Y

⊤(cid:34) Y(cid:35) µ Y
 
Σ Y Σ⊤ X⊥,Y Σ⊤ X∥,Y

X =U(cid:101) ∼N µ ,Σ Σ Σ⊤ . (26)
 ⊥  X  X⊥  X⊥,Y X⊥ X⊥,X∥
X µ Σ Σ Σ
∥ X∥ X∥,Y X⊥,X∥ X∥
For a generic m-dimensional Gaussian random variable Z ∼ N (µ ,Σ ), we denote by L ∈ Rm×m the lower
Z Z Z
Cholesky factor of its inverse covariance matrix, i.e., Σ−1 = L⊤L . To construct the Knothe-Rosenblatt rear-
Z Z Z
rangementS thatpushesforwardπ tothestandardGaussiandistribution,weintroducetheCholeskyfactor
Y,X⊥,X∥
L ∈Rd×d,L ∈Rr×r,L ∈R(n−r)×(n−r)oftheinversecovariancematrixΣ−1,Σ−1 ,Σ−1 ,
Y X⊥|Y X∥|Y,X⊥ Y X⊥|Y X∥|Y,X⊥
respectively. Theseareexplicitlygivenby
Σ−1 =L⊤L
Y Y Y
Σ−1 =L⊤ L (27)
X⊥|Y X⊥|Y X⊥|Y
Σ−1 =L⊤ L .
X∥|Y,X⊥ X∥|Y,X⊥ X∥|Y,X⊥
Using classical results on the conditional mean of Gaussian random variables, we obtain the Knothe-Rosenblatt
rearrangementSthatpushesforwardπ toη ⊗η ⊗η ,see[Spantinietal.,2022,LeProvostetal.,2023]as
Y,X⊥,X∥ Y X⊥ X∥
   
SY(y) L (y−µ )
Y Y
   (cid:16) (cid:17) 
S(y,x ⊥,x ∥)=

SX⊥(y,x ⊥)  =

L
X⊥|Y
x ⊥−µ
X⊥|Y
 . (28)
   
SX∥(y,x ⊥,x ∥) L X∥|Y,X⊥(x ∥−µ X∥|Y,X⊥)
Byspecializingtheformulas(16)and(18)tothetriangularmapS(y,x ,x )of(28),weobtain
⊥ ∥
T⊥ (y,x )=x −Σ Σ−1(y−y⋆),
y⋆ ⊥ ⊥ X⊥,Y Y
(29)
T∥ (y,x ,x )=x −Σ Σ−1(y−y⋆).
y⋆ ⊥ ∥ ∥ X∥,Y Y
NotethattheformulaforT⊥ andT∥ correspondstotheKalmanfilterupdateinthesubspacespannedbythecolumns
y⋆ y⋆
ofU andU ,respectively. Remarkably,despitetherecursiveupdateofthestatecomponents(firstupdatingx then
⊥ ∥ ⊥
x )inthegenericnonlinearcase,theanalysismapT∥ (y,x ,x )intheGaussiancasedoesnotdependonx . This
∥ y⋆ ⊥ ∥ ⊥
suggeststhatwecanfullydecoupletheupdateofthedifferentstatecomponentswiththeKalmanfilter, echoinga
similarconclusioninthecontextofKalmansmoothersbyRamgraberetal.[2023]. Theanalysismapintheoriginal
spaceT nowreads
y⋆
T (y,x)=U T⊥ (y,x )+U T∥ (y,x ,x )
y⋆ ⊥ y⋆ ⊥ ∥ y⋆ ⊥ ∥
=U (cid:0) x −Σ Σ−1(y−y⋆)(cid:1) +U (x −Σ Σ−1(y−y⋆))
⊥ ⊥ X⊥,Y Y ∥ ∥ X∥,Y Y
(30)
=U (U⊤x)+U (U⊤x)−U U⊤Σ Σ−1(y−y⋆)−U U⊤Σ Σ−1(y−y⋆)
⊥ ⊥ ∥ ∥ ⊥ ⊥ X,Y Y ∥ ∥ X,Y Y
=x−Σ Σ−1(y−y⋆),
X,Y Y
wherethelasttwoequationsrelyonx = U⊤xandU = [U ,U ]beingorthonormal. Asnotedintheprevious
⊥ ⊥ ⊥ ∥
section,rotatingthestatecomponents,performingtheinferenceinthesenewcoordinates,andliftingtheresulttothe
originalspaceisstrictlyequivalenttoperformingtheinferenceintheoriginalspace. Thus,itisnaturalthattheanalysis
mapof(19)intheGaussiancaserevertstotheKalmanfilterupdate. Similarly,weobtainthe(constrained)analysis
14LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
mapT(cid:101)y⋆ preservingtheinvariantH(x)=U⊤ ⊥xas
T(cid:101)y⋆(y,x)=U ⊥x ⊥+U ∥T∥ y⋆(y,x ⊥,x ∥)
=x−U U⊤Σ Σ−1(y−y⋆) (31)
∥ ∥ X,Y Y
=x−(I −U U⊤)Σ Σ−1(y−y⋆).
⊥ ⊥ X,Y Y
Weconcludethissectionwithseveralcommentsonthislastresult. First,theconstrainedanalysismapcorresponds
toaKalman-likeupdatewheretheKalmangainisprojectedontheorthonormalcomplementofU ,namelyonthe
⊥
spacespannedbythecolumnsofU . Interestingly,(31)establishesanequivalencebetweenan“embeddingapproach”
∥
anda“projectiveapproach”[Simon,2010]. Inthefirstcase,weperformaconstrainedinferenceintherotatedspace
before lifting back the result. In the latter case, we “naively” project the state’s update −Σ Σ−1(y −y⋆) onto
X,Y Y
thecolumnsofU toensurethattheinvariantisunchanged. Whilethereisnoreasonforthesetwoapproachesto
∥
coincideinthenon-Gaussiancasewitharbitraryinvariants,(31)statesthattheyareequivalentintheGaussiancasewith
linearinvariantsH. Inthecasewherethelinearinvariantsareconstantovertheprior—seeRemark7forthegeneral
treatment—wecanspecializethereducedtransformationsofRemark7toderiveareducedconstrainedanalysismap
T(cid:101)y⋆,reducedof(23)intheGaussiancase. Weshowthattheresultingmapisidenticaltothefullysupportedcase,since
theunconstrainedmapT∥ (y,x ,x )of(29)doesnotdependonx . Weomitdetailsforbrevity.
y⋆ ⊥ ∥ ⊥
7 PreservinglinearinvariantswithKalmanfilters
ItisoftenclaimedthatthevanillaKalmanfilteranditsMonteCarloapproximation,namelytheensembleKalmanfilter,
preservelinearinvariants. Thissectionaddressesthisclaim—anditsimportantnuances—intwoparticularscenarios. In
thefirstscenario,weassumethatthelinearinvariantsareconstantoverthepriordistributionπ ,i.e.,H(x)=C ∈Rr
X
foranyrealizationxofX. Inthesecondscenario,weassumethattheexpectedlinearinvariantsofthepriordistribution
are known, i.e., E [H(x)] = C ∈ Rr. This can model a scenario where the true invariant is known up to some
πX
uncertainty.
Importantly,theempiricalKalmangainoftenneedstoberegularizedwithcovariancetaperingorinflationtoalleviate
undesired sampling effects such as rank deficiency, spurious long-range correlations, and underestimation of the
statistics. However,suchmodificationsareknowntobreakthelinearpreservingpropertiesofthevanillaensemble
Kalmanfilter[Janjic´ etal.,2014]. Asaresult,wewillshowbelowthattheensembleKalmanfilteronlypreserves
linearinvariantswhennoregularizationtechniquesareappliedandwhenthelinearinvariantsareconstantovertheprior
distribution(thefirstscenario). Incontrast,theconstrainedKalmanfilterandtheconstrainedensembleKalmanfilter,
presentedintheprevioussection,intrinsicallypreservelinearinvariantsinbothoftheabovescenarios.
7.1 Scenario1: Thelinearinvariantsareconstantoverthepriordistribution
Inthisfirstscenario,weassumethatthelinearinvariantsareconstantoverthepriordistributionπ ,i.e.,H(x)=C ∈Rr
X
foranyrealizationxofX. ThenitisclearthatE [H(x)]=C. RecallingthattheanalysismapoftheKalmanfilter
πX
T isgivenin(30)asT (y,x)=x−Σ Σ−1(y−y⋆),whereΣ denotesthecross-covariancebetweenthe
KF KF X,Y Y X,Y
15LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
stateXandtheobservationY,wegetfor(y,x)∈Rd×Rn
H(T (y,x))=U⊤T (y,x),
KF ⊥ KF
=U⊤x−U⊤Σ Σ−1(y−y⋆),
⊥ ⊥ X,Y Y
(cid:20)(cid:90) (cid:21)
=U⊤x−U⊤ (x′−µ )(y′−µ )⊤dπ (y′,x′) Σ−1(y−y⋆),
⊥ ⊥ X Y Y,X Y
(cid:20)(cid:90) (cid:20)(cid:90) (cid:21) (cid:21)
=U⊤x− U⊤(x′−µ )π (x′) π (y′|x′)(y′−µ )⊤dy′ dx′ Σ−1(y−y⋆),
⊥ ⊥ X X Y|X Y Y
(cid:20)(cid:90) (cid:16) (cid:17)(cid:20)(cid:90) (cid:21) (cid:21)
=U⊤x− U⊤x′π (x′)−U⊤µ π (x′) π (y′|x′)(y′−µ )⊤dy′ dx′ Σ−1(y−y⋆),
⊥ ⊥ X ⊥ X X Y|X Y Y
(cid:20)(cid:90) (cid:20)(cid:90) (cid:21) (cid:21)
=U⊤x− Cπ (x′)−Cπ (x′) π (y′|x′)(y′−µ )⊤dy′ dx′ Σ−1(y−y⋆),
⊥ X X Y|X Y Y
=U⊤x−0 ,
⊥ r
=H(x),
(32)
where we use that for x′ ∈ Rn that is not in the support of π , i.e., π (x′) = 0, we have U⊤x′π (x′) =
X X ⊥ X
U⊤x′π (x′) = U⊤x′ × 0 = Cπ (x′). Hence, in this scenario, we have U⊤x′π (x′) = Cπ (x′) for any
⊥ X ⊥ X ⊥ X X
x′ ∈Rn. Thus,theinvariantispreservedbyT ifitisconstantoverπ . Wenotethatasimilarderivationholdsfor
KF X
theanalysismapoftheensembleKalmanfilter,wherethecovariancematricesΣ andΣ arereplacedbytheir
X,Y Y
empiricalcounterpartsestimatedfromsamples{(y(i),x(i))}ofthejointdistributionoftheobservationsandstates.
Finally,westressthatthisresultholdsforanyjointdistributionπ withfinitesecond-ordermoments,i.e.,Σ and
Y,X Y,X
Σ arefinite.
Y
7.2 Scenario2: Theexpectedvalueofthelinearinvariantsoverthepriorisknown
In this second scenario, we assume that the expected linear invariants of the prior distribution are known, i.e.,
E [H(x)] = C ∈ Rr is known. We investigate whether the Kalman filter preserves the expected linear invari-
πX
antsbycheckingifE [H(T (y,x))]=C issatisfied. Tothisend,notethat(30)implies
πY,X KF
(cid:104) (cid:105) (cid:104) (cid:105)
E [H(T (y,x))]=E U⊤x −E U⊤Σ Σ−1(y−y⋆)
πY,X KF πY,X ⊥ πY,X ⊥ X,Y Y
(33)
=C−U⊤Σ Σ−1(µ −y⋆).
⊥ X,Y Y Y
Thus,theKalmanfilterdoesnotnecessarilypreservetheexpectedlinearinvariantsofthepriorπ . Tobemoreprecise,
X
(33)impliesthattheKalmanfilterpreservestheexpectedlinearinvariantsofthepriorπ ifandonlyifU⊤b=0 with
X ⊥ r
“expectedstateupdate”b=Σ Σ−1(µ −y⋆)∈Rn. Wecaninterpretthisasblyinginthespanofthecolumnsof
X,Y Y Y
U and,therefore,notcontaininganyinformationaboutthelinearinvariants.
∥
Tosummarizeourexplorationofthesetwoscenarios:TheregularizedEnKFgenerallydoesnotpreservelinearinvariants
ofthepriordistribution. Incontrast,theconstrainedKalmanfilter(31)preservesthelinearinvariantsofanyrealization
ofthepriordistributionπ X. Inotherwords,thedistributionπ H(X)isunchangedbytheconstrainedanalysismapT(cid:101)y⋆.
16LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
8 Computationalexamples
ThissectionpresentstheresultsofournumericalexamplesonpreservinglinearinvariantswithensembleKalmanfilters
andstochasticmapfilters. Weconsiderthreeexamples: asyntheticlinearordinarydifferentialequation(ODE)withan
arbitrarynumberoflinearinvariants,thelinearadvectionequationasthe(arguablysimplest)prototypeofahyperbolic
conservationlaw,andanembeddingofthenonlinearLorenz-63modelinR4withalinearinvariant.
Forreproducibility,thecodeofournumericalexperimentsisavailableathttps://github.com/mleprovost/Paper-Linear-
Invariants-Ensemble-Filters.
8.1 Numericalsetup
Thissectiondiscussescommonconsiderationsinourdataassimilationexperiments. Toisolatetheperformanceofthe
filteringalgorithms,weperforma“twinexperiment”wherethesamestate-spacemodelisusedtogeneratetheground
truthandintheforecastandanalysisstepsoftheensemblefilters[Aschetal.,2016]. Thegroundtruthisgeneratedby
samplinganinitialstatex⋆fromtheinitialdistributionπ andevolvingitthroughthedynamicalmodel(1)overthe
0 X0
timeinterval[0,t ]. Discretizing[0,t ]usinganequidistantgridwithstepsize∆t ,ateachtimestep,wecollectthe
f f obs
truestatex⋆ andgenerateanoisyobservationy⋆ fromtheobservationmodel(2). Althoughwemightnotknowthe
t t
transitionkernelπ andthelikelihoodmodelπ ,weassumethatwecangeneratesamplesfromthem.
Xt|Xt−1 Yt|Xt−1
Furthermore,weassumethattheobservationoperatorgislinearandgivenbyg(x )=Gx withobservationmatrix
t t
G∈Rd×n. Inthefilteringsettingthatunderpinsthiswork,weseektoestimatethetruestate{x⋆}inasequentialway
t
giventhenoisyobservations{y⋆}ofthetrueprocessandtheinitialdistributionπ .
t X0
ToensurethattheprocessnoiseW inthedynamicalmodel(1)doesnotchangethelinearinvariantsH(x)=U⊤x,
t ⊥
weconsideralinearinvariant-preservingGaussianprocessnoisewithzeromeanandcovarianceσ2 I withσ >0.
Wt n Wt
Tosamplefromthisdistribution,weprojectthenoisesamplesfromN (cid:0) 0 ,σ2 I (cid:1) onthecolumnsofU spanning
n Wt n ∥
the(orthogonal)complementofspan(U ). Westressthatthenumericalschemesusedinthisworkpreservethelinear
⊥
invariantsofthedynamicalsystems. Thus,violationsoftheseinvariantscanbefullyattributedtoflawsinthefiltering
algorithms.
OurfirsttwoexperimentsconsistoflinearGaussianfilteringproblems,forwhichwecomparetworelatedensemble
filters: the stochastic ensemble Kalman filter [Evensen, 1994] without constraints on the linear invariants, called
unconstrainedEnKF(UnEnKF),andtheproposedstochasticensembleKalmanfilterthatpreserveslinearinvariantsby
buildingaMonteCarloapproximationoftheanalysismap(31),calledconstrainedEnKF(ConsEnKF).Forbothfilters,
weapplymultiplicativeinflationandcovariancetaperingtoregularizetheempiricalestimateoftheKalmangain,see
[Aschetal.,2016]formoredetails. Westressthatthesetechniquesarecriticalforsuccessfulstateestimationwith
ensembleKalmanfilterswithlimitedsamplesbutarealsoknowntobreaklinearinvariants[Janjic´ etal.,2014]. The
constrainedEnKFcombinesthepreviousregularizationtechniqueswithaprojectiononthespanofU topreserve
∥
linearinvariants. Algorithm2inAppendixEprovidesapseudo-codefortheconstrainedEnKF.
Forthefirsttwolinear-Gaussianfilteringexperiments,thefilteringdistributionisGaussian. Thus,nonlinearfilterssuch
asthestochasticmapfilter,offernoadvantageovertheensembleKalmanfilter. Inthethirdexperiment,weconsidera
non-Gaussianfilteringproblemwithalinearinvariant,forwhichwecompareunconstrained/constrainedstochasticmap
17LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
filterswiththeEnKF.WeprovideanoverviewoftheparameterizationofthestochasticmapfilterinAppendixC,but
referreadersto[Spantinietal.,2022]forfurtherdetails. Algorithm1inAppendixDprovidesapseudo-codeforthe
constrainedSMF.
Weassesstheperformanceofthedifferentensemblefilterswiththeroot-mean-square-error(RMSE)andthespreadof
√
theposteriorensemble. RecallthattheRMSEattimetisdefinedasRMSE =||x⋆−µ || / n,whereµ ∈Rn
t t (cid:98)t,a 2 (cid:98)t,a
isthefilteringensemblemeanattimet,andnisthedimensionofthestatevariable. Thespreadattimetisgiven
√
byspread
t
= tr(Σ(cid:98)t,a)/ n,whereΣ(cid:98)t,a ∈ Rn×n isthefilteringensemblecovarianceattimet. Thefiltersarerun
over2000assimilationcycles,andwediscardthefirst1000assimilationcyclessothatthestatisticsareapproximately
stationary. Afterthisspin-upphase,wereportthetime-averagedmetricsoftheensemblefilterswithoptimallytuned
multiplicativeinflationand/orcovariancetapering,achievingthelowestRMSEforagivenensemblesize.
8.2 Asyntheticlinearmodelwitharbitrarynumberoflinearinvariants
Inthisfirstexample,weconsiderasyntheticlinearODEmodelforwhichwecanfixanarbitrarynumberoflinear
invariants. ThisallowsustoinvestigatetheperformanceoftheproposedConsEnKF,comparedtotheexistingUnEnKF,
fordifferentnumbersofinvariantsrandensemblesizesM.
Considerthelineardynamicalmodel
dx
=A x,
dt r (34)
x(0)=x ,
0
wherex ∈Rn andthematrixA ∈Rn×n issemi-negativedefinite,i.e.,symmetricwithnon-positiveeigenvalues,
0 r
withspectraldecomposition
A =UD U−1, (35)
r Λr
where U ∈ Rn×n is an orthonormal matrix and D ∈ Rn×n denotes the diagonal matrix with diagonal entries
Λr
Λ ∈Rn. Thedynamicmodel(34)ismotivatedbyproduction-destructionsystemsinatmosphericchemistryforwhich
r
preservinglinearinvariantshasbeendemonstratedtobecrucialforaccurateandrobustnumericalsimulations[Nüßlein
etal.,2021,Huangetal.,2022,Izginetal.,2023,IzginandÖffner,2023].
Withoutlossofgenerality,weassumethatthevectorofeigenvaluesΛ isorganizedasfollows:
r
Λ =[0 ,−λ ,...,−λ ], withλ >0fork >r. (36)
r r r+1 n k
Weusetheunderscriptrtostressthat0isaneigenvalueofA withmultiplicityr. Wechoosenonnegativeeigenvalues
r
forA toproducealineardynamicalmodel(34)withstablesolutions[Huangetal.,2022]. Fromclassicalresultson
r
time-invariantlinearsystems,thesolutionx(t)of(34)isgivenby
x(t)=exp(A t)x . (37)
r 0
Using the eigendecomposition of A , we get x(t) = Uexp(D t)U⊤x . We denote by U ∈ Rn×r the first r
r Λr 0 ⊥
orthonormalcolumnsofU andbyU theremaining(n−r)orthonormalcolumns. Thus,wecanverifythatthelinear
∥
18LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
EnKF, r=1
ConsEnKF, r=1
0.100 EnKF, r=5
ConsEnKF, r=5
0.075 EnKF, r=10
ConsEnKF, r=10
EnKF, r=15
ConsEnKF, r=15
0.075
0.050
EnKF, M=20
ConsEnKF, M=20
EnKF, M=30
ConsEnKF, M=30
0.025
EnKF, M=50 0.050
ConsEnKF, M=50
EnKF, M=100
ConsEnKF, M=100
0.0 0.2 0.4 0.6 0.8 1.0 10 20 30 40 50 60 70 80 90 100
r/n M
Figure 2: Left panel: Evolution of the time-averaged RMSE for the unconstrained EnKF (solid lines) and the
constrainedEnKF(dashedlines)forvaryingratiosr/n ∈ [0,1]betweenthenumberoflinearinvariantsr andthe
problem dimension n with M = 20,30,50,100 samples. Right panel: Evolution of the time-averaged RMSE for
theunconstrainedEnKF(solidlines)andtheconstrainedEnKF(dashedlines)forvaryingensemblesizesM with
r =1,5,10,15. Bothfiltersuseoptimallytunedmultiplicativeinflationandcovariancetapering.
invariantsU⊤x =C ∈Rr arepreservedby(34):
⊥ 0 0
U⊤x(t)=U⊤Uexp(D t)U⊤x ,
⊥ ⊥ Λr 0
=U⊤U D U⊤x +U⊤U D U⊤x ,
⊥ ⊥ 1r ⊥ 0 ⊥ ∥ [exp(−λr+1t),...,exp(−λnt)] ∥ 0
(38)
=I C +0 ,
r 0 r
=C ,
0
wherewehaveusedthatU⊤U = I ,U⊤U = 0 ,andexp(D t) = D . We
⊥ ⊥ r ⊥ ∥ r×(n−r) Λr [1r,exp(−λr+1t),...,exp(−λnt)]
denoteby1 ∈Rr thevectorofonesoflengthr.
r
Fromthecontinuousdynamicalmodel(34),weconstructadiscretetimeforwardoperatorf forthedynamicalmodel
(1)byintegrationof(34)overatimestep∆t betweentwoassimilationcycles,i.e.,f(x )=exp(A ∆t )x . As
obs t r obs t
mentionedabove,thissettingallowsaparametricstudyoftheperformancemetricsovertheratiooflinearinvariants
r/n∈[0,1](withstatedimensionn)andtheensemblesizeM.
Weconsiderastateofdimensionn=20. Theeigenvalues−λ fork >rareindependentlydrawnfromauniform
k
distribution on [−5,0]. The time step size between two assimilation cycles is ∆t = 10−1. We use a Gaussian
obs
linearinvariant-preservingprocessnoisewithzeromeanandcovarianceσ2 I withσ =10−2. Weobserveevery
Wt n Wt
componentofthestate,i.e.,d = n = 20,corruptedbyanadditiveGaussianobservationnoisewithzeromeanand
covarianceσ2I withσ =10−1.
E d E
Figure2comparestheRMSEoftheunconstrainedEnKFandtheconstrainedEnKFfordifferentensemblesizesM and
ratiosr/nbetweenthenumberoflinearinvariantsrandtheproblemdimensionn. Preservingthelinearinvariants
consistentlyresultsinareductionoftheRMSE.Forr/n < 0.1,theimprovementsareataround5%,showinglittle
variationwiththeensemblesizeM. Asr/nincreases,weobservelargerRMSEimprovementforallensemblesizes
19
ESMR
egarevA
ESMR
egarevALeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
EnKF, r=1
ConsEnKF, r=1
EnKF, r=5
ConsEnKF, r=5
0.06 EnKF, r=10
0.06 ConsEnKF, r=10
EnKF, r=15
ConsEnKF, r=15
0.04
0.04
EnKF, M=20
ConsEnKF, M=20
EnKF, M=30
ConsEnKF, M=30
EnKF, M=50
0.02 ConsEnKF, M=50
EnKF, M=100
ConsEnKF, M=100 0.02
0.0 0.2 0.4 0.6 0.8 1.0 10 20 30 40 50 60 70 80 90 100
r/n M
Figure3: Leftpanel:Evolutionofthetime-averagedspreadfortheunconstrainedEnKF(solidlines)andtheconstrained
EnKF(dashedlines)forvaryingratiosr/n∈[0,1]withM =20,30,50,100samples. Rightpanel: Evolutionofthe
time-averagedspreadfortheunconstrainedEnKF(solidlines)andtheconstrainedEnKF(dashedlines)forvarying
ensemble sizes M with r = 1,5,10,15. Both filters use optimally tuned multiplicative inflation and covariance
tapering.
M. Specifically,forM =20andr =19,theRMSEoftheunconstrainedEnKFis7.7·10−2,whiletheRMSEofthe
constrainedEnKFis2.5·10−2,correspondingtoareductionoftheRMSEby67%. Moreover,forsmallensemblesizes
M <40,theRMSEimprovementsaresignificantandincreasewiththenumberoflinearinvariants. ForM =10and
r =10,theRMSEisreducedby36%. Forafixednumberofinvariantsr,theRMSEimprovementduetopreserving
linear invariants decreases with the ensemble size M. This is consistent with the empirical Kalman gain estimate
requiringlessregularizationastheensemblesizeM increases. Thus,theanalysismapoftheunconstrainedEnKFgets
closertotheanalysismapofthevanillaEnKF.FromSection7.1,thevanillaEnKFpreserveslinearinvariantsifallthe
forecastsampleshavethesameinvariants. WecanexpectthattheviolationsofthelinearinvariantsoftheUnEnKF
reduceinmagnitudeastheensemblesizeM increases,thusreducingthebenefitsofpreservinglinearinvariantsasthe
ensemblesizeincreases.
Figure2reportsonthespreadoftheunconstrainedEnKFandtheconstrainedEnKF.Wedonotobservesignificant
differencesinthespreadoftheunconstrainedEnKFandtheconstrainedEnKF.Weconcludethatpreservinglinear
invariantsisparticularlyadvantageousfortheRMSEwhentheensemblesizeM issmallandtheratiooflinearinvariants
r/nislarge. WeinterprettheseresultsasareductionofthevarianceoftheempiricalKalmangainbyconstraining
theimagespaceoftheestimatedKalmangaintothesubspacespannedbythecolumnsofU . Wereferreadersto
∥
[LeProvostetal.,2022]forfurtherdiscussionsonthebenefitsoflineardimensionreductionsfortheensembleKalman
filter.
20
daerpS
egarevA
daerpS
egarevALeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
8.3 Linearadvectiononperiodicdomain
Considertheone-dimensionallinearadvectionequationforafieldu(s,t)ontheperiodicdomainΩ=[0,1):
∂u(s,t)
+∇·(cu(s,t))=0, s∈Ω, t>0,
∂t (39)
u(s,0)=u (s), s∈Ω,
0
whereu : R →Ristheinitialfield,c=1istheadvectionvelocity. Notably,duetotheperiodicboundaryconditions,
0
(cid:82)
thetotalmassm(t) = u(s,t)dsofexactsolutionsu(s,t)of(39)isconstantintime,i.e.,m(t) = m(0)forallt,
Ω
andnumericalsolutionsshouldmimicthisbehavioronadiscretelevel. WediscretizethedomainΩ = [0,1)with
n=128gridnodes{s }. Thestatevectorx attimetisgivenbythepointwiseevaluationsofthecontinuousfield
k t
atthegridnodes{s },i.e.,x =u(s ,t)fork =1,...,n. Tosolve(39),weuseaspectralmethodforthespatial
j t,k k
discretizationandafourth-orderadaptivestrongstabilitypreserving(SSP)Runge–Kutta(RK)timeintegrationscheme
(SSPRK43), see [Brunton and Kutz, 2019, Rackauckas and Nie, 2017]. The distribution π used to generate the
X0
trueinitialconditionandtheinitialensemblememberisgivenbyann-dimensionalsmoothandperiodicdistribution,
denotedS (U ,C,α),thatisparameterizedbyasub-unitarymatrixU ∈Rn×r,avectorC ∈Rr,andasmoothing
n,r ⊥ ⊥
parameterα>0. Bydesign,arandomvariableX∼S (U ,C,α)satisfiesU⊤X=C. Togeneratesamplesfrom
n ⊥ ⊥
thedistributionS (U ,C,α),weproceedinthreesteps:
n ⊥
(i) Generatesamplesz ,z fromthe(n/2+1)-dimensionalstandardGaussiandistributionN (0,I);
Re Im
(ii) Computex˜ ∈Cn 2+1withcomponentsx˜
k
=(z Re,k+iz Im,k)exp(cid:0) −1 2kα(cid:1) ,k =1,...,n
2
+1;
(iii) Computex=U C+(I −U U⊤)F−1(x˜).
⊥ n ⊥ ⊥
Here, i ∈ C is the imaginary unit and F−1: Cn 2+1 → Rn denotes the inverse of the real fast Fourier transform,
implementedunderrfftintheFFTWlibrary[FrigoandJohnson,1998]. Figure4showssixsamplesxfromthis
distribution.
WeconsideraGaussianlinearinvariant-preservingprocessnoisewithzeromeanandcovarianceσ2 I withσ =
Wt n Wt
10−2. Thetimestepsizebetweentwoassimilationcyclesis∆t =2·10−1 (correspondingtohalftheconvective
obs
timet =L/c,whereListhelengthofthedomainΩ). Wehaveincomingpointobservationsateveryfourthgridpoint
c
(d=32)corruptedbyanadditivezero-meanGaussiannoisewithcovarianceσ2I withσ =10−1. ThetruemassC
E d E
isdrawnfromaGaussiandistributionwithmean1.0andstandarddeviation5·10−2.
Figure5reportsthetime-averagedevolutionoftheRMSEandthespreadofthelinearadvectionproblemusingthe
UnEnKF(blue)andtheConsEnKF(yellow)forvaryingensemblesizesM. Bothfiltersuseoptimallytunedinflation
andcovariancetapering. WenotethattheConsEnKFhasaslightlybetterRMSEthanitsunconstrainedversion. This
resultisconsistentwiththeresultsofSection8.2forsmallratior/n. Thespreadovertheensemblesizeisroughly
similar. Whiletheseglobalmetricscanbesomewhatdeceptiveintermsofimprovement,thetruebenefitofpreserving
themassisrevealedbyexaminingtheevolutionofthemassestimate,whichisreportedinFigure6.
WeobservefromFigure6thattheConsEnKFconservesmassuptomachineprecision. Atthesametime,theoptimally
tuned UnEnKF shows notable unphysical variations in the mass estimate (up to 20%) for M = 40 and does not
necessarilyconvergetothetrueinvariantasweassimilatemoreobservations.
21LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
1
0
−1
−2
0.0 0.5 1.0
s
Figure4: SixsamplesdrawnfromthedistributionS (U ,C,α)withn=128, r =1, U =1/n∈Rn,C =1∈
n,r ⊥ ⊥
R, andα=1. ThelinearconstraintforeachsampleisU⊤x=1.
⊥
EnKF
Constrained EnKF
0.10
0.04
0.08
0.03
0.06
EnKF
0.02 Constrained EnKF
20 40 60 80 100 20 40 60 80 100
M M
Figure5: Leftpanel: Time-averagedevolutionoftheRMSEofthelinearadvectionproblemusingtheUnEnKF(blue)
andtheConsEnKF(yellow)forvaryingensemblesizesM. Rightpanel: Medianevolutionofthespreadforvarying
ensemblesizesM. Bothfiltersuseoptimallytunedinflationandcovariancetapering.
8.4 AembeddedLorenz-63modelwithalinearinvariant
TheLorenz-63modelmodelisathree-dimensionalmodelfortheatmosphericconvection[Lorenz,1963],usedasa
classicalbenchmarkindataassimilation[Aschetal.,2016]. Thestatex = [x ,x ,x ]⊤ ∈ R3 isgovernedbythe
(cid:101) (cid:101)1 (cid:101)2 (cid:101)3
followingsetofODEs:
 
σ(x −x )
(cid:101)2 (cid:101)1
d dx (cid:101)
t
=F(cid:101)(x (cid:101),t)=  x (cid:101)1(ρ−x (cid:101)2)−x (cid:101)2  , (40)
x x −βx
(cid:101)1(cid:101)2 (cid:101)3
22
ESMR
egarevA
)s(x
daerpS
egarevALeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
1.4
1.2
1.0
Truth
EnKF
Constrained EnKF
0 100 200 300 400
t
Figure6: TimeevolutionofthelinearinvariantU⊤x forthetruestateprocess(green)andtheposteriormeanobtained
⊥ t
withtheUnEnKF(blue)andtheConsEnKF(dashedyellow)foranensemblesizeofM = 40. Faintedareasshow
the10%and90%quantilesoftheposteriorestimateoftheinvariant. Bothfiltersuseoptimallytunedinflationand
covariancetapering.
whereF(cid:101): R3×R →R3denotestheforwardoperatorof(40),andσ,β,ρarefixedparameters. Inoursimulation,we
useσ = 10,β = 8/3,ρ = 28. Forthesevalues,thesystemischaoticandbehaveslikeastrangeattractor[Lorenz,
1963]. We introduce an embedded version of the Lorenz-63 model with a linear invariant, called the embedded
Lorenz-63model. Forthislow-dimensionalproblem,covariancetamperingorlocalizationarenotusefultoregularize
theunconstrainedEnKF.Assumingthatthelinearinvariantisconstantovertheforecastdistributionπ ,Section
Xt|Y1:t−1
7provesthatinthiscase,theunconstrainedEnKFpreserveslinearinvariants. Thisexampleallowsustoinvestigatethe
influenceofanonlinearanalysismapandthepreservationoflinearinvariants. Wecomparetheconstrainedstochastic
mapfilter(ConsSMF)withtheunconstrainedensembleKalmanfilter(UnEnKF)andstochasticmapfilter(UnSMF).
Algorithm1inAppendixDprovidesapseudo-codefortheconstrainedSMF.
ToconstructtheembeddedLorenz-63model,weaugmentthestatex∈R3 oftheoriginalLorenz-63modelof(40)
(cid:101)
by adding a fourth component x with zero dynamics, i.e., dx /dt = 0, and performing a random rotation of the
(cid:101)4 (cid:101)4
embeddedcoordinates. Thisrandomrotationallowsustocreateadynamicalsystemwithanon-locallinearinvariant,
i.e.,theinvariantdependsonallthestatevariables. Wedenotetheaugmentedstatebyx ∈R4andtheaugmented
(cid:101)aug
forwardoperatorbyF(cid:101)aug: R4×R →R4,(x (cid:101)aug,t)(cid:55)→[F(cid:101)(x (cid:101),t),0]⊤. Observethatx
(cid:101)
(cid:55)→U(cid:101)⊤ ⊥x
(cid:101)
isalinearinvariantfor
theaugmentedstatex (cid:101)withU(cid:101)⊥ =[0,0,0,1]⊤. WethenapplyarandomorthogonalmatrixQ∈R4×4tox (cid:101)augtodefine
thenewstatex=Qx . Thus,thestatexisgovernedbytheODEsystem
(cid:101)aug
dx dQx
dt
= d(cid:101) taug =QF(cid:101)aug(Q−1x,t)=QF(cid:101)aug(Q⊤x,t), (41)
23
tx⟂⊤ULeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
wherethelastequalityisduetotheorthonormalityofQ. Onecanverifythatthelinearinvariantx (cid:55)→ U⊤xwith
⊥
U
⊥
=QU(cid:101)⊥ ∈R4ispreservedby(41):
dU⊤ ⊥x =U(cid:101)⊤ Q⊤dx
dt ⊥ dt
=U(cid:101)⊤ ⊥Q⊤QF(cid:101)aug(Q⊤x,t)
=U(cid:101)⊤ ⊥F(cid:101)aug(Q⊤x,t) (42)
=⟨[0,0,0,1],[F(cid:101)(Q⊤x,t),0]⟩
=0,
where⟨·,·⟩denotestheEuclidianscalarproductinRn. TherandomorthogonalrotationQ∈R4×4isconstructedby
extractingtheQfactoroftheQRfactorizationofa4×4matrixwhoseentriesaredrawnfromthestandardGaussian
distribution. Tosolve(41),weuseafourth-orderRKtimeintegrationscheme[RackauckasandNie,2017]. Thetime
stepsizebetweentwoassimilationcyclesis∆t =5·10−2. Weobserveeverycomponentofthestate,i.e.,d=n=4,
obs
corruptedbyadditiveGaussianobservationnoisewithzeromeanandcovarianceσ2I withσ =10−2. Theinitial
E d E
distributionπ isthestandardGaussianone. ThetrueinvariantC issetto1.
X0
Inthecaseofconditionallyindependentobservations,i.e.,ifthelikelihoodfactorizesasπ
=(cid:81)d
π ,itis
Y|X j=1 Yj|X
equivalenttoassimilatead-dimensionalobservationy⋆ ∈ Rd atonce,ortoassimilatetheobservationcomponents
y⋆ ∈R, j =1,...,d,indrecursiveupdates. WeprovideajustificationinAppendixB.Thus,theanalysisstepcanbe
j
equivalentlyperformedbycomputingdanalysismapswithn+1inputs—eachmapassociatedwithoneobservation,
or computing a single map with d+n inputs. In practice, the number of samples to estimate the analysis map is
smallcomparedtothedimensionsofthestatesandobservations. Thus,toreducethevarianceoftheresultinganalysis
map T , it is favorable to estimate d transport maps SX of n+1 inputs rather than a single map SX of d+n
y⋆
inputs. See[Spantinietal.,2022,HoutekamerandMitchell,2001]forfurtherdiscussions. Following[Spantinietal.,
2022],weapplytheunconstrained/constrainedstochasticmapfilterssequentially. Toallowforafaircomparison,the
unconstrainedEnKFisalsoappliedsequentiallyinthisexample.
Figure7reportsthetime-averagedevolutionoftheRMSEandthespreadoftheembeddedLorenz-63modelusing
theUnEnKF(blue),theUnSMF(yellow),andtheConsSMF(green)forvaryingensemblesizesM. Allfiltershave
optimallytunedinflation. ForsmallensemblesizeM <100,theunconstrainedEnKFhasalowerRMSEandspread
thanthestochasticmapfilters. ForlargerensemblesizeM >100,weobserveaconsistentimprovementintheRMSE
andspreadwiththenonlinearfilters. ForlargeensemblesizeM ≈500,theUnSMFandConsSMFperformsimilarly
intermsofRMSEandspread,correspondingtoareductionoftheRMSEby18%andthespreadby16%withrespect
totheEnKF.Theseresultsechotheconclusionsof[Spantinietal.,2022]onthebias-variancetradeoffofnonlinear
filterswithlimitedsamples. Interestingly,theConsSMFalwaysperformsbetterthantheUnSMF.ForM >80,the
ConsSMFachievesthelowestRMSEandspread. ForM ∈ [60,200],weobservethattheRMSEoftheConsSMF
rapidlydecreasesfrom8.0·10−1forM =60to5.3·10−1forM =200. Incontrast,theRMSEandspreadofUnSMF
haveaslowerdecaywiththeensemblesize. TheRMSEoftheConsSMFplateausatabout5.3·10−1forM >160,
whiletheunconstrainedSMFrequiresM =500samplestoachieveasimilarperformance.
WeobservefromFigure8thattheUnEnKFandtheConsSMFconservethelinearinvariant. However,theUnSMF
shows important variations in the linear invariant estimate (up to 200%) for M = 160. These results suggest that
24LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
EnKF 0.9 EnKF
0.8 SMF SMF
Constrained SMF Constrained SMF
0.8
0.7
0.6 0.7
100 200 300 400 500 100 200 300 400 500
M M
Figure7: Leftpanel: Time-averagedevolutionoftheRMSEoftheembeddedLorenz-63modelproblemusingthe
UnEnKF(blue),theUnSMF(yellow),andtheConsSMF(green)forvaryingensemblesizesM. Rightpanel: Median
evolutionofthespreadforvaryingensemblesizesM. Allfiltersuseoptimallytunedinflation.
4
3
2
1
0
Truth
−1 EnKF
SMF
Constrained SMF
−2
0 100 200 300 400
t
Figure8: TimeevolutionofthelinearinvariantU⊤x forthetruestateprocess(pink)andtheposteriormeanobtained
⊥ t
withtheUnEnKF(blue),theunconstrainedSMF(yellow),andtheconstrainedSMF(dashedgreen)foranensemble
sizeofM =160. Faintedareasshowthe10%and90%quantilesoftheposteriorestimateoftheinvariant. Allfilters
useoptimallytunedinflation.
itisbeneficialtocombinethepreservationoflinearinvariantswithnonlinearprior-to-posteriortransformationsfor
non-Gaussianfilteringproblems.
9 Conclusion
We introduced a class of analysis maps (Lin-PAMs) that preserve linear invariants of the forecast distribution in
non-Gaussianfilteringproblems. Linearinvariantsarefoundacrossaspectrumofproblemsinscienceandengineering,
25
ESMR
egarevA
x⟂⊤U
daerpS
egarevALeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
including divergence-free conditions in incompressible fluid mechanics, conservation of electrical intensities and
currentsinKirchhoff’slaws,andstoichiometricbalanceofchemicalspeciesinchemicalreactions. Toconstructthe
proposedLin-PAMs,weoperateintwosteps. First,werotatethestatecoordinatessuchthatthefirststatecomponents
align with the column space of U and the remaining state components span the orthogonal complement of U ,
⊥ ⊥
i.e.,span(U ). Inthisrotatedspace,thelinearinvariantsaregivenbythefirstrotatedstatecomponents,whilethe
∥
remainingstatecomponentsdonotinfluencethelinearinvariants. Thus,wecanperformthesameinferenceasinthe
originalspacebyfirstrotatingthestatevariables,thenperformingtheinferenceforthefirststatecomponentsfollowed
bytheremainingones,andfinallyliftingtheresulttotheoriginalspace.
Essentially,weareoperatingachangeofvariablesfortheanalysismapbetweentheoriginalandtherotatedspace.
Second,weconstructlinearinvariant-preservinganalysismaps(Lin-PAMs)byomittingtheupdateofthefirststate
componentscorrespondingtothelinearinvariants.
Thisformulationallowspreservinglinearinvariantsinnon-Gaussiansettings. Byspecializingthisconstructiontoa
jointlyGaussiandistributionfortheobservationsandstates,ourformulationrecoversaconstrainedformulationof
theanalysismapoftheKalmanfilterwheretheupdateisprojectedontheorthogonalcomplementofthecolumnsof
U . Then,weclarifiedexistingresultsonpreservinglinearinvariantsforthevanilla(unconstrained)Kalmanfilterand
⊥
ensembleKalmanfilter. Inparticular,weshowthatregularizationtechniquesfortheensembleKalmanfilter,such
ascovarianceinflation, localization, ortaperingcanviolatelinearinvariants. Wealsoshowhowtocombinethese
regularizationtechniquesfortheensembleKalmanfilterwiththepreservationofthelinearinvariants.
Weemphasizethatthetoolsdevelopedinthisstudyarenotlimitedtothefilteringsetting,astheanalysisstepessentially
solvesastaticBayesianinverseproblem[LeProvostetal.,2021,2022]. Infact,thetechniquespresentedinthiswork
arereadilyapplicabletootherensemble-basedmethodsusedtosolvestaticinverseproblemswhilepreservinglinear
invariants[Iglesiasetal.,2013,Zhangetal.,2020,Garbuno-Inigoetal.,2020].
Thisworkfocusedonthestrongpreservationoflinearinvariantsinnon-Gaussiansettings. Infuturework,wewill
exploreavariationalformulationofBayes’rule[Sanz-Alonsoetal.,2023]topreserveinvariantsintheweaksensefor
non-Gaussiansettings.
Morebroadly,preservinginvariantsoftheunderlyingdynamicalmodelsiscriticalforensuringthattherecentadvances
inBayesianstatisticsproduceposteriorestimatesrespectingourfundamentalknowledgeofphysics. Infuturework,we
plantoconstructanalysismapsandresultingensemblefiltersforimportantnon-linearinvariants,suchasHamiltonians
inmechanics[delCastillo,2018]orenergyandentropyinhyperbolicconservationlaws[LeVeque,1992]. Itwouldalso
beinterestingtoconsideraweakpreservationofnon-linearinvariantsinnon-Gaussiansettings. Infuturework,weplan
toexploitavariationalformulationofBayes’rule[TrillosandSanz-Alonso,2018,Sanz-Alonsoetal.,2023]totackle
thisproblem.
Data Accessibility: All the computational results are reproducible, and code is available at
https://github.com/mleprovost/Paper-Linear-Invariants-Ensemble-Filters.
Authors’ Contributions: MLP: Conceptualization, Writing - Original Draft, Review & Editing, Software. JG:
Conceptualization,Writing-Review&Editing. YM:Conceptualization,Writing-Review&Editing. Thefinalversion
ofthemanuscriptwasapprovedbyallauthors.
26LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
Funding: MLPandYMacknowledgesupportoftheNationalScienceFoundationunderGrantPHY-2028125. JGand
YMacknowledgesupportoftheUSDOD(ONRMURI)underGrantN00014-20-1-2595.
Acknowledgements: TheauthorswouldliketothankRicardoBaptista,JeffEldredge,ThomasIzgin,MatthewLevine,
andDanielSharpforinsightfuldiscussionsandconstructivefeedback.
Appendix
A AresultonthepreservationofinvariantsinBayesianinferenceproblems
Theorem1. Considerapriordistributionπ ,alikelihoodmodelπ ,andaninvariantH: Rn →Rr. Letusassume
X Y|X
thattheinvariantHisconstantoverthepriordistributionπ ,i.e.,H(x)=C ∈Rr foranyrealizationxofX. Then
X
theinvariantHisalsoconstantovertheposteriordistributionπ .
X|Y
Proof. Foradistributionπ,itssupportisdefinedassupp(π) = {x ∈ Rn|π(x) > 0}. IftheinvariantHisconstant
overthepriordistributionπ ,then
X
supp(π )⊆{x∈Rn|H(x)=C}. (43)
X
Foralikelihoodmodelπ ,wehavefromBayes’rulethatthesupportoftheposteriorπ = π ·π /π is
Y|X X|Y Y|X X Y
includedinthesupportofthepriorπ ,i.e.,
X
supp(π )⊆supp(π ). (44)
X|Y X
Indeed,amultiplicationbythenon-negativequantityπ (y|x)/π (y)≥0cannotincreasetheposteriorsupport
Y|X Y
supp(π )beyondthepriorsupportsupp(π ). Combining(43)and(44),weconcludethat
X|Y X
supp(π )⊆{x∈Rn|H(x)=C}. (45)
X|Y
B Recursiveassimilationofconditionallyindependentobservations
Inmanysettings,theobservationsY ∈Rd toassimilateareconditionallyindependent,i.e.,Y ⊥⊥Y |Xforj ̸=k,
j k
[Spantinietal.,2022,HoutekamerandMitchell,2001]. Thenthelikelihoodπ factorizesas
Y|X
d
(cid:89)
π = π . (46)
Y|X Yj|X
j=1
Inthiscase,wewillshowthattheassimilationofad-dimensionalobservationy⋆ ∈ Rd canbemadeindrecursive
stepsbyassimilatingonescalarobservationy⋆atatimeinthestate,i.e.,theposteriorfromassimilatingj components
j
y⋆ canbeusedasapriorforassimilatingthenextscalarobservationy⋆ . FromBayes’rule,wederivearecursive
1:j j+1
relationtoassimilatetheobservationY intheconditionaldistributionπ :
j+1 X|Y1:j
π π π π π π
π =
Y1:j+1|X X
=
Yj+1|X,Y1:j Y1:j|X X
=
Yj+1|X,Y1:jπ
. (47)
X|Y1:j+1 π π π π X|Y1:j
Y1:j+1 Yj+1|Y1:j Y1:j Yj+1|Y1:j
27LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
This factorization holds for an arbitrary likelihood π . In general, the “conditional” likelihood π is
Y|X Yj+1|X,Y1:j
intractable,andonecannoteasilyuse(47)toperformarecursiveupdateofthepriorwithoutfurtherassumptionsonthe
likelihoodπ . However,ifweassumethattheobservationsareconditionallyindependent,i.e.,Y ⊥⊥ Y |Xfor
Y|X j k
j ̸=k,(47)simplifiesto
π
π =
Yj+1|X
π , (48)
X|Y1:j+1 π X|Y1:j
Yj
andonecanassimilatetheobservationsinarecursivemanner.
C Detailsontheparameterizationofthestochasticmapfilter
ThissectionprovidesasummaryoftheparameterizationoftheKnothe-Rosenblattrearrangementusedintheuncon-
strained/constrainedstochasticmapfilters. See[Spantinietal.,2022]forfurtherdetails. LetX ∈ Rn bearandom
variablewithdistributionπ,andS: Rn →RnbetheKnothe-Rosenblattrearrangementthatpushesforwardπtothe
referencedistributionη. Weconsideralinearlyseparableparameterizationforthecomponents{Sk}ofS,i.e.,thekth
mapcomponentSk: Rk →Risparameterizedby
k−1
(cid:88)
Sk(x )= φ(j,k)(x )+φ(k,k)(x ), (49)
1:k j k
j=1
where φ(j,k): R → R,j = 1,...,k are univariate feature functions. Each off-diagonal feature φ(j,k), i.e., with
index j < k, is given by the sum of a linear term and p radial basis functions characterized by their centers ξ =
j
[ξ(1),...,ξ(p)]⊤ ∈Rpandscalewidthsσ =[σ(1),...,σ(p)]⊤ ∈Rp,i.e.,
j j j j j
p
φ(j,k)(x )=c x
+(cid:88)
c N
(cid:16) ξ(l),σ(l)(cid:17)
(x ), (50)
j j,0 j j,l j j j
l=1
(cid:16) (cid:17)
whereN ξ(l),σ(l) (x )denotestheevaluationoftheunivariateGaussiandistributionwithmeanξ(l)andstandard
j j j j
deviationσ(l)atx . Thecoefficients{c }areunknownandestimatedfromsamples. Thecentersξ areidentifiedas
j j j,l j
thepempiricalquantilesofthejthcomponentofthesamples. Thescalewidthsσ quantifyingtheempiricalspreadin
j
thejthcomponent,aredefinedasσ(l) = γ(ξ −ξ )/2withscalingfactorγ > 0,ξ = ξ andξ = ξ . We
j j+1 j−1 0 1 p+1 p
setγ = 2.0inournumericalexperiments. Forp = 0,theoff-diagonalfeatureφ(j,k) revertstoalinearfunction. To
enforcethemonotonocityconstraintonthemapcomponentSk,i.e.,x(cid:55)→Sk(x ,x)isstrictlyincreasingforall
1:k−1
x ∈Rk−1,thediagonalfeatureφ(k,k)isdecomposedasanon-negativesumofaconstantc andp+2strictly
1:k−1 k,0
increasingfunctions{ψ(l): R →R},i.e.,
p+2
(cid:88)
φ(k,k)(x )=c + c ψ(l)(x ), (51)
k k,0 k,l k
l=1
wherethep+2functions{ψ(l)}areparameterizedbyacenterparameterξ(l),awidthparameterσ(l),andgivenby
k k
ψ(1)(z)=
1(cid:18)(cid:16) z−ξ(1)(cid:17)(cid:16) 1−erf(cid:16) ∆(1)(cid:17)(cid:17) −σ(1)(cid:112) 2/πexp(cid:18) −(cid:16) ∆(1)(cid:17)2(cid:19)(cid:19)
,
2 k k k k
1(cid:16) (cid:16) (cid:17)(cid:17)
ψ(l)(z)= 1+erf ∆(l) ,forl=2,...p+1, (52)
2 k
ψ(p+2) =
1(cid:18)(cid:16) z−ξ(p+1)(cid:17)(cid:16) 1+erf(cid:16) ∆(p+1)(cid:17)(cid:17) +σ(p+1)(cid:112) 2/πexp(cid:18) −(cid:16) ∆(p+1)(cid:17)2(cid:19)(cid:19)
,
2 k k k k
28LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
√
where ∆(l)(x ) = (x −ξ(l))/ 2σ(l) for l = 1,...,p+2. The identification of the set of p+2 centers ξ =
k k k k k k
[ξ(0),...,ξ(p+1)]⊤ ∈Rp+2andscalewidthsσ =[σ(0),...,σ(p+1)]⊤ ∈Rp+2followthesameprocedureasforthe
k k k j j
off-diagonalfeaturesφ(j,k). ThestrictmonotonocityofSk isthenenforcedbythepositivityconstraints: c ≥0for
k,l
l =0,...,p+2. Forp=0,wesetφ(k,k) tobeastrictlyincreasingaffinefunction,i.e.,φ(k,k)(x )=c +c x
k k,0 k,1 k
withc >0. Theresultingparameterizationofthemapcomponentsconcentratesnonlinearfeaturesinthebulkofthe
k,1
jointforecastdistribution,andrevertstoalinearbehaviorinthetailsofthejointforecastforrobustness.
Inthiswork,weconsiderfurthersimplificationsoftheparameterization. Fork >2,theoff-diagonalfeaturesaregiven
byalineartermpluspradialbasisfunctions,whilethediagonalfeatureφ(k,k) isaffine. Fork = 1,2,thediagonal
featureφ(k,k)isgivenbyaconstantplusp+2increasingfunctions{ψ(l): R →R}givenby(52). Inourexperiments,
weusep = 2. Fork > 2,onecanexploitthisparameterizationandsolveinclosedformforthecoefficientsc by
l,j
solvingaleastsquareproblem[Spantinietal.,2022]. Fork =1,2,theoptimizationofthecoefficientsisperformed
withaprojectedNewton’smethod[Bertsekas,1982]. WenotethattheoptimizationofthefirstmapcomponentS1can
beskipped. Indeed,theanalysismap(8)onlydependsonthelowermapSX. TheuppermapSY isonlyanartifactof
constructionoftheanalysismap[Spantinietal.,2022,LeProvostetal.,2023]. Wereferreadersto[Spantinietal.,
2022]fordetailsontheestimationofthecoefficientsofthemapcomponentsfromsamples.
D Algorithmfortheanalysisstepoftheconstrainedstochasticmapfilter
Algorithm 1 presents pseudo-code for one analysis step of the constrained stochastic map filter (ConsSMF). This
algorithmtransformsasetofforecastsamplestofilteringsamplesbyassimilatingtherealizationy⋆ oftheobservation
variablewhilepreservinglinearinvariantsofthesamples.
Algorithm 1: ConsSMF(y⋆,π ,{xi}) assimilates the data y⋆ in the prior samples {x1,...,xM} while
Y|X=·
preservingthelinearinvariantsofthesamples.
Input: y⋆ ∈Rd,likelihoodmodelπ ,M samples{xi}fromπ
Y|X=· X
Output: M samples{x (i)}fromπ
a X|Y=y⋆
/* Generate likelihood samples {y(i)} */
1 for=1:M do
2
Drawsampley(i)fromπ
Y|X=x(i)
/* Project the forecast samples {x(i)} onto U and U : */
⊥ ∥
3 x ⊥(i) =U⊤ ⊥x(i), x ∥(i) =U⊤ ∥x(i)
4
EstimatethetransportmapSX∥ fromthetransformedjointsamples{(y(i),x ⊥(i),x ∥(i))}
/* Perform the constrained analysis in the transformed space by partial inversion of SX∥: */
5 x ∥,a(i) =T(cid:101)∥ y⋆(y(i),x ⊥(i),x ∥(i))=SX∥(y⋆,x ⊥(i),·)−1◦SX∥(y(i),x ⊥(i),x ∥(i))
/* Lift result to the original space: */
6 x( ai) =U ⊥x ⊥(i)+U ∥x ∥,a(i)
7 return{xi a}
29LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
E AlgorithmfortheanalysisstepoftheconstrainedensembleKalmanfilter
Algorithm2presentspseudo-codeforoneanalysisstepoftheconstrainedensembleKalmanfilter(ConsEnKF).This
algorithmtransformsasetofforecastsamplestofilteringsamplesbyassimilatingtherealizationy⋆ oftheobservation
variablewhilepreservinglinearinvariantsU⊤x=C ∈Rr ofthesamples.
⊥
Algorithm2:ConsEnKF(y⋆,G,π ,U ,{xi})assimilatesthedatay⋆inthepriorsamples{x1,...,xM}while
E ⊥
preservingthelinearinvariantsU⊤x=C ofthesamples.
⊥
Input: y⋆ ∈Rd,linearobservationoperatorG∈Rd×n,observationnoisedistributionπ =N (0 ,Σ ),
E d E
sub-unitarymatrixU ∈Rr×nforthelinearinvariantsU⊤x=C ∈Rr,M samples{xi}fromπ
⊥ ⊥ X
Output: M samples{x (i)}fromπ
a X|Y=y⋆
/* Generate observation noise samples {ϵ(i)} from N(0 ,Σ ) */
d E
1 : for=1:M do
2 Drawsampleϵ(i)fromN (0 d,Σ E)
/* Form the perturbation matrices for the state A ∈Rn×M and the observation noise
X
A ∈Rd×M: */
E
3 fori=1:M do
4 A X[:,i]←− √ M1 −1(cid:0) x(i)−µ (cid:98)X(cid:1)
5 A E[:,i]←− √ M1 −1(cid:0) ϵ(i)−µ (cid:98)E(cid:1)
/* Apply the Kalman gain based on the representers [Burgers et al., 1998]. Solve the linear
system for {b(i)}: */
6 fori=1:M do
7 ((GA X)(GA X)⊤+A EA⊤ E)b(i) =(Gx(i)+ϵ(i)−y⋆),
/* Build the posterior samples {x(i)}: */
a
8 fori=1:M do
9 x( ai) ←−x(i)−(I n−U ⊥U⊤ ⊥)A X(GA X)⊤b(i).
10 return{xi a}
References
D.J.Albers,P.-A.Blancquart,M.E.Levine,E.E.Seylabi,andA.Stuart. EnsembleKalmanMethodswithConstraints.
InverseProblems,35(9):095007,2019.
N.Amor,G.Rasool,andN.C.Bouaynaya.ConstrainedStateEstimation–AReview.arXivpreprintarXiv:1807.03463,
2018.
M.Asch,M.Bocquet,andM.Nodet. Dataassimilation: methods,algorithms,andapplications,volume11. SIAM,
2016.
R.Baptista,B.Hosseini,N.B.Kovachki,andY.Marzouk.ConditionalsamplingwithmonotoneGANs:fromgenerative
modelstolikelihood-freeinference. arXivpreprintarXiv:2006.06755,2020.
30LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
T.Bengtsson,P.Bickel,andB.Li. Curse-of-DimensionalityRevisited: CollapseoftheParticleFilterinVeryLarge
ScaleSystems. InProbabilityandStatistics: EssaysinHonorofDavidA.Freedman, volume2, pages316–335.
InstituteofMathematicalStatistics,2008.
D. P. Bertsekas. Projected Newton methods for optimization problems with simple constraints. SIAM Journal on
controlandOptimization,20(2):221–246,1982.
S.L.BruntonandJ.N.Kutz.Data-DrivenScienceandEngineering: MachineLearning,DynamicalSystems,andControl.
CambridgeUniversityPress,2019.
G.Burgers,P.J.vanLeeuwen,andG.Evensen. AnalysisSchemeintheEnsembleKalmanFilter. MonthlyWeather
Review,126(6),1998.
A.Carrassi,M.Bocquet,L.Bertino,andG.Evensen. Dataassimilationinthegeosciences: Anoverviewofmethods,
issues,andperspectives. WileyInterdisciplinaryReviews: ClimateChange,9(5):e535,2018.
C. Cotter, D. Crisan, D. Holm, W. Pan, and I. Shevchenko. Data assimilation for a quasi-geostrophic model with
circulation-preservingstochastictransportnoise. JournalofStatisticalPhysics,179(5-6):1186–1221,2020.
R.R.CraigJrandA.J.Kurdila. Fundamentalsofstructuraldynamics. JohnWiley&Sons,2006.
G.F.T.delCastillo. AnIntroductiontoHamiltonianMechanics. Springer,2018.
G.Evensen. SequentialDataAssimilationwithaNonlinearQuasi-GeostrophicModelUsingMonteCarloMethodsto
ForecastErrorStatistics. JournalofGeophysicalResearch: Oceans,99(C5):10143–10162,1994.
G.Evensen,F.C.Vossepoel,andP.J.VanLeeuwen. Dataassimilationfundamentals. SpringerNature,2022.
M.FrigoandS.G.Johnson. FFTW:AnAdaptiveSoftwareArchitecturefortheFFT. InProceedingsofthe1998
IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP’98 (Cat. No. 98CH36181),
volume3,pages1381–1384.IEEE,1998.
A.Garbuno-Inigo, F.Hoffmann, W.Li, andA.M.Stuart. InteractingLangevindiffusions: Gradientstructureand
ensembleKalmansampler. SIAMJournalonAppliedDynamicalSystems,19(1):412–441,2020.
G.H.GolubandC.F.VanLoan. Matrixcomputations. JHUpress,2013.
N. Gupta and R. Hauser. Kalman Filtering with Equality and Inequality State Constraints. arXiv preprint
arXiv:0709.2791,2007.
E.Hairer,M.Hochbruck,A.Iserles,andC.Lubich. Geometricnumericalintegration. OberwolfachReports,3(1):
805–882,2006.
P.L.HoutekamerandH.L.Mitchell. AsequentialensembleKalmanfilterforatmosphericdataassimilation. Monthly
WeatherReview,129(1):123–137,2001.
J.Huang, T.Izgin, S.Kopecz, A.Meister, andC.-W.Shu. Onthestabilityofstrong-stability-preservingmodified
PatankarRunge-Kuttaschemes. arXivpreprintarXiv:2205.01488,2022.
M.A.Iglesias,K.J.Law,andA.M.Stuart. EnsembleKalmanmethodsforinverseproblems. InverseProblems,29(4):
045001,2013.
T.IzginandP.Öffner. AstudyofthelocaldynamicsofmodifiedPatankarDeCandhigherordermodifiedPatankar–RK
methods. ESAIM:MathematicalModellingandNumericalAnalysis,57(4):2319–2348,2023.
31LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
T.Izgin,S.Kopecz,andA.Meister. AstabilityanalysisofmodifiedPatankar–Runge–Kuttamethodsforanonlinear
production–destructionsystem. PAMM,22(1):e202200083,2023.
T. Janjic´, D. McLaughlin, S. E. Cohn, and M. Verlaan. Conservation of mass and preservation of positivity with
ensemble-typeKalmanfilteralgorithms. MonthlyWeatherReview,142(2):755–773,2014.
T.Kajishima,K.Takizawa,andM.Tsubokura. ComputationalFluidDynamicsinPractice. CRCPress,2016.
R.E.Kalman. ANewApproachtoLinearFilteringandPredictionProblems. JournalofBasicEngineering,82(1):
35–45,031960.
K.J.LawandA.M.Stuart. DataAssimilation: AMathematicalIntroduction. TextsinAppliedMathematics,62,2015.
M.LeProvost,R.Baptista,Y.Marzouk,andJ.Eldredge. Alow-ranknonlinearensemblefilterforvortexmodelsof
aerodynamicflows. InAIAAScitech2021Forum,page1937,2021.
M.LeProvost,R.Baptista,Y.Marzouk,andJ.D.Eldredge. Alow-rankensembleKalmanfilterforellipticobservations.
ProceedingsoftheRoyalSocietyA,478(2266):20220182,2022.
M.LeProvost,R.Baptista,J.D.Eldredge,andY.Marzouk. Anadaptiveensemblefilterforheavy-taileddistributions:
tuning-freeinflationandlocalization. arXivpreprintarXiv:2310.08741,2023.
R.J.LeVeque. Numericalmethodsforconservationlaws,volume214. Springer,1992.
E.Lorenz. DeterministicNonperiodicFlow. JournalofAtmosphericSciences,20(2),1963.
Y.Marzouk,T.Moselhy,M.Parno,andA.Spantini. Samplingviameasuretransport: Anintroduction. Handbookof
UncertaintyQuantification,1:2,2016.
S.Nüßlein,H.Ranocha,andD.I.Ketcheson. Positivity-preservingadaptiveRunge–Kuttamethods. Communications
inAppliedMathematicsandComputationalScience,16(2):155–179,2021.
G. Peyré, M. Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and
Trends®inMachineLearning,11(5-6):355–607,2019.
J.Prakash,S.C.Patwardhan,andS.L.Shah. ConstrainednonlinearstateestimationusingensembleKalmanfilters.
Industrial&EngineeringChemistryResearch,49(5):2242–2253,2010.
C.RackauckasandQ.Nie. Differentialequations.jl–AperformantandFeature-richEcosystemforSolvingDifferential
EquationsinJulia. Journalofopenresearchsoftware,5(1):15–15,2017.
M.Ramgraber,R.Baptista,D.McLaughlin,andY.Marzouk. Ensembletransportsmoothing.PartI:Unifiedframework.
JournalofComputationalPhysics: X,17:100134,2023.
M. Rosenblatt. Remarks on a multivariate transformation. The Annals of Mathematical Statistics, 23(3):470–472,
1952.
D. Sanz-Alonso, A. Stuart, and A. Taeb. InverseProblemsandDataAssimilation. London Mathematical Society
StudentTexts.CambridgeUniversityPress,2023.
D.Simon. Kalmanfilteringwithstateconstraints: asurveyoflinearandnonlinearalgorithms. IETControlTheory&
Applications,4(8):1303–1318,2010.
C. Snyder, T. Bengtsson, P. Bickel, and J. Anderson. Obstacles to High-Dimensional Particle Filtering. Monthly
WeatherReview,136(12):4629–4640,2008.
32LeProvost,Glaubitz,andMarzouk/Preservinglinearinvariantsinensemblefilteringmethods
A.Spantini,R.Baptista,andY.Marzouk. Couplingtechniquesfornonlinearensemblefiltering. SIAMReview,64(4):
921–953,2022.
N.G.TrillosandD.Sanz-Alonso. Thebayesianupdate: variationalformulationsandgradientflows. arXivpreprint
arXiv:1705.07382,2018.
C.Villanietal. Optimaltransport: OldandNew,volume338. Springer,2009.
J.Wu,J.-X.Wang,andS.C.Shadden. AddingconstraintstoBayesianinverseproblems. InProceedingsoftheAAAI
ConferenceonArtificialIntelligence,volume33,pages1666–1673,2019.
X.-L.Zhang,C.Michelén-Ströfer,andH.Xiao. RegularizedensembleKalmanmethodsforinverseproblems. Journal
ofComputationalPhysics,416:109517,2020.
33