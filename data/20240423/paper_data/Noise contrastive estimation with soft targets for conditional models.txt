Noise contrastive estimation with soft targets for
conditional models
JohannesHugger VirginieUhlmann
EuropeanBioinformaticsInstitute EuropeanBioinformaticsInstitute
EuropeanMolecularBiologyLaboratory EuropeanMolecularBiologyLaboratory
jhugger@ebi.ac.uk uhlmann@ebi.ac.uk
Abstract
Softtargetscombinedwiththecross-entropylosshaveshowntoimprovegener-
alizationperformanceofdeepneuralnetworksonsupervisedclassificationtasks.
The standard cross-entropy loss however assumes data to be categorically dis-
tributed,whichmayoftennotbethecaseinpractice. Incontrast,InfoNCEdoes
notrelyonsuchanexplicitassumptionbutinsteadimplicitlyestimatesthetrue
conditional through negative sampling. Unfortunately, it cannot be combined
withsofttargetsinitsstandardformulation,hinderingitsuseincombinationwith
sophisticatedtrainingstrategies. Inthispaper,weaddressthislimitationbypropos-
ing a principled loss function that is compatible with probabilistic targets. Our
new soft target InfoNCE loss is conceptually simple, efficient to compute, and
canbederivedwithintheframeworkofnoisecontrastiveestimation. Usingatoy
example,wedemonstrateshortcomingsofthecategoricaldistributionassumption
ofcross-entropy,anddiscussimplicationsofsamplingfromsoftdistributions. We
observe that soft target InfoNCE performs on par with strong soft target cross-
entropybaselinesandoutperformshardtargetNLLandInfoNCElossesonpopular
benchmarks,includingImageNet. Finally,weprovideasimpleimplementationof
ourloss,gearedtowardssupervisedclassificationandfullycompatiblewithdeep
classificationmodeltrainedwithcross-entropy.
1 Introduction
Thecross-entropyloss,orlogloss,anditssofttargetvariantsareamongstthemostpopularobjective
functionsforconditionaldensityestimationinsupervisedclassificationproblemsHeetal.[2016],
Dosovitskiyetal.[2020]. Initsbasevariant,itassumesadegenerateone-hotdistributionp(l|x)=
δ(l,k)astheunderlyingconditionaldensityoverlabelslgivendatax,whichmightnotsufficeto
capturethenatureofcomplexorambiguouslyannotateddatasets. Thissimplifyingassumptioncan
haveanegativeeffectonparameterestimationaswellasmodelcalibrationpropertiesGuoetal.[2017],
Mülleretal.[2019],andadverselyaffectsthegeneralizationcapabilityofthemodel. Tomitigate
this,trainingwithprobabilistictargets,alsocalledsofttargets,hasshowntobebeneficialSzegedy
etal.[2016],Mülleretal.[2019]. Forinstance,labelsmoothinginjectsnoiseintolabels,leadingto
ahigherentropyinthetargetconditionaldistributionSzegedyetal.[2016]. Thiswasshowntobe
effectiveinawiderangeofproblemsand,whencombinedwithothersofttargettechniquesZhang
etal.[2017],Hanetal.[2022],leadstostate-of-the-artperformanceDosovitskiyetal.[2020],He
etal.[2022].
SeveralavenueshavebeenproposedtoimproveupontheloglossGutmannandHyvärinen[2010],
Linetal.[2017],includingtheInfoNCEobjectiveJozefowiczetal.[2016],MaandCollins[2018].
InfoNCEhasbecomethestandardlossfunctionforself-supervisedcontrastivelearningduetoits
informationtheoreticproperties,simplicityandexcellentempirically-observedperformanceChen
et al. [2020], He et al. [2020], Tian et al. [2020], Oord et al. [2018]. It has shown to produce
Preprint.Underreview.
4202
rpA
22
]GL.sc[
1v67041.4042:viXraversatile,high-qualityrepresentationsthatoptimizealowerboundontheirmutualinformation(MI)
contentTianetal.[2020],Oordetal.[2018]. Innaturallanguageprocessing,InfoNCEwasshown
tobeanefficientreplacementforthelogloss,withbettergeneralizationperformanceandon-par
parameterestimationpropertiesJozefowiczetal.[2016],MaandCollins[2018]. InfoNCEavoids
thesummationoveralllabelsthatisusuallyrequiredtocomputethenormalizationconstantand
insteadreliesonnegativesampling. Thiscanbeadvantageouswhenlabelspacesarelarge. Negative
samplinghasastrongconnectiontonoisecontrastiveestimationasitultimatelyestimatesthetrue
conditionalp(k|x)implicitlyinsteadofusinganexplicitmodelGutmannandHyvärinen[2010],Ma
andCollins[2018].
Despite its favourable theoretical properties as well as its empirical success in natural language
processingand,morebroadly,self-supervisedlearning,InfoNCEhasnotseenawideadoptionin
supervisedclassificationtasksinvolvingothertypesofdatasuchasimagesorgraphs. Onereasonfor
thismightbethedifficultyofidentifyingscenariosinwhichInfoNCEwouldbeafavourableloss
function.Anotherreasonmightbethat,duetoitsnoisecontrastivenature,combiningitwithpowerful
methodsleveragingsofttargetsSzegedyetal.[2016],Zhangetal.[2017],Hanetal.[2022],Yunetal.
[2019]isnotstraightforward. Thiscontrastswiththelogloss,wheresofttargetprobabilitiescanbe
explicitlyusedtomodelthetargetconditionaldensity. Itishoweverconceivablethatintegratingsuch
targetsandtheircorrespondingdistributionsmightboostgeneralizationperformance.
Inthiswork,wefirstshowthatInfoNCEisarelevantalternativetothelogloss,particularlyinthe
caseoflabeluncertainty. WedemonstratethroughatoyexamplethatInfoNCEindeedyieldsbetter
parameter estimates in this scenario. This motivates us to suggest approaches that combine soft
targetswithnoisecontrastiveestimation. OurmaincontributionisanInfoNCEgeneralizationthat
directlyintegratessofttargetprobabilitiesintothelossbycreatingweightedlinearcombinations
oflabelembeddings. Ourlossfunction,whichwedubsofttargetInfoNCE,canbederivedusing
thecontinuouscategoricaldistributionincombinationwithaBayesianargument. Wediscussthe
resultingattraction-repulsiondynamicsaswellashowthisprioraffectsgradients.Assecondapproach
wesuggesttosampletargetsfromadistributionwithhigherentropy,suchasthelabelsmoothing
distributionpϵ(y|x)=(1−ϵ)p(y|x)+ϵξ(y)inwhichϵ∈[0,1]determinestheweightingbetween
theunderlyingconditionalandthenoisedistribution. Furthermore,weinvestigatehowthisaffects
theMIandentropyinthelearnedrepresentationbyderivingavariationallowerbound.
Finally, we perform experiments on multiple classification benchmarks, including ImageNet, to
compareandexaminesofttargetInfoNCE’sproperties.Ourresultssuggestthatourlossiscompetitive
withsofttargetcross-entropyandoutperformshardtargetNLLandInfoNCE.Inanablationstudy
we give insights into the confidence calibration of the trained models and investigate how the
number of negative samples/batch size as well as the amount of label smoothing noise affects
model performance. Our results confirm the intuition that InfoNCE and soft target InfoNCE are
importantalternativestotheloglossanditssoftvariant. Ourimplementationisavailableathttps:
//github.com/uhlmanngroup/soft-target-InfoNCE.git.
2 Backgroundandrelatedwork
2.1 Logloss,softtargetsandthecontinuouscategoricaldistribution
In classification problems, the log loss, also known as negative log likelihood (NLL) or cross
entropy loss, is an objective to evaluate the quality of probabilistic models Hastie et al. [2009].
It is the standard loss function used to train deep supervised classification models Krizhevsky
etal.[2017],Heetal.[2016],Dosovitskiyetal.[2020]. Intuitively,givenadatasample,thelog
loss quantifies the discrepancy between the predicted class probability of the true label and the
probabilitygiventhroughatargetdistribution. Themostcommonlyusedtargetisthedegenerate
conditionaldistributionp(l|x)=δ(l,k),inducedbytheone-hotencodingoflabelsintoprobabilities.
The estimated probabilities q are usually computed as a softmax over class scores q(k|x,θ) :=
exp(z·w
)/(cid:80)K
exp(z·w ),wherekistheclasslabel,zdenotesthepenultimatelayeractivations
k l=1 l
(also called embedding) of sample x, and w corresponds to the weights of the classifier layer
k
responsibleforcomputingtherespectivescore. Theloglossfunctioncanthenbedefinedforsample
xwithlabelkas
lNLL(x,k):=−logq(k|x,θ). (1)
2Anumberofworkshaveaddresseddifferentdrawbacksofthelogloss,includingitsrobustnessto
noisylabelsZhangandSabuncu[2018],Sukhbaataretal.[2014]andpoormarginsElsayedetal.
[2018],Caoetal.[2019]. Inadditiontotheseshortcomings,neuralnetworkstrainedwith(1)tendto
bepoorlycalibrated,whichleadstooverconfidentpredictionsandtoanincreasedmisclassification
rateGuoetal.[2017]. Besidesscalinglogitswithatemperatureparameter, labelsmoothinghas
showntomitigatethisproblemMülleretal.[2019]. Labelsmoothinghasbeenintroducedasamodel
regularizationtechniqueinwhichtheoriginal"hard"one-hotencodingofalabelisreplacedbya
"soft"target,computedastheweightedaverageoftheoriginalhardtargetandanoisedistribution
overlabels,forinstancetheuniformdistributionξ ≡1/K Szegedyetal.[2016]. Formally,thelabel
smoothingdistributioncanbedefinedas
pϵ(l|x)=(1−ϵ)δ(l,k)+ϵξ(l), (2)
whereξdenotesanoisedistribution,ϵ∈[0,1],andwherethedeltadistributionδ(l,k)=p(l|x)isthe
originaltargetconditionaldistribution.Duringtraining,(1)isreplacedbythesofttargetcross-entropy
givenby
K
(cid:88)
lSoftTargetXent(x,k)=− pϵ(l|x)logq(l|x,θ). (3)
l=1
Bydistributingsmallprobabilitymassoverlabelsthatdonotcorrespondtothetrueclass,modelsare
incentivisedtoreducetheirpredictionconfidence,whichhasshowntoimproveperformanceDosovit-
skiyetal.[2020]. Besidesclassification,softtargetsarealsousedinknowledgedistillation,inwhich
ateachernetworkprovidesastudentnetworkitsestimatedprobabilitiesastrainingtargetsHinton
etal.[2015].
Theideaofdistributingprobabilitymassesovertwoormorelabelshasalsobeenusedincombination
withmixinginputsamplesinadataaugmentationtechniquecalledMixUpZhangetal.[2017]. In
MixUp data points, including their labels, are blend together into a new sample using a mixing
coefficient,resultinginanewsampleandacorrespondingsofttarget. Inthispaper,werefertotargets
thatdistributeprobabilitymassamongmultipleclassesassofttargetsregardlessoftheunderlying
sample.ThisincludesdataaugmentationtechniquessuchasMixUp,aswellasregularizationmethods
likelabelsmoothing.
Recently,Gordon-Rodriguezetal.[2020b]proposedalabelsmoothinglossbasedonthecontinuous
categoricaldistributionGordon-Rodriguezetal.[2020a]asanalternativeto(3)thatmodelsnon-
categorical data more faithfully. The continuous categorical distribution generalizes its discrete
analogandisdefinedontheclosedsimplexas
K
(cid:89)
α ,...,α ∼CC(λ) ⇐⇒ p(α ,...,α ;λ)∝ λαi, (4)
1 K 1 K i
i=1
with(cid:80)K
α =1.
i=1 i
2.2 InfoNCEforsupervisedclassification
InfoNCEhasbeendemonstratedtobeaversatilelossfunctionwithapplicationsacrossmanymachine
learning tasks and application domains Tian et al. [2020], He et al. [2020], Poole et al. [2019],
Damrichetal.[2022],Zimmermannetal.[2021]. Inthiswork,wefocuspredominantlyonitsdensity
estimationproperties. Inordertoestimatedensities,InfoNCEturnstheproblemintoasupervised
classificationtaskinwhichthepositionofasinglesamplecomingfromthedatadistributionpmustbe
identifiedinatupleT ofotherwisenoisesamplescomingfromadistributionηJozefowiczetal.[2016].
Thetheoreticalfootingforconditionaldensityestimationandclassificationhasbeenintroducedin
thecontextoflanguagemodellingMaandCollins[2018]. Concretely,MaandCollins[2018]shows
thatInfoNCE’soptimumθ∗aretheparametersofthetrueconditional,i.e. q(k|x,θ∗)=p(k|x). In
otherwords,supervisedInfoNCEfitsθbyminimizingthenegativeloglikelihoodof(x,k)beingat
apositionS withinT,givenby−logP(S|T). Withoutlossofgenerality,forS =0,theInfoNCE
losscanthenbedefinedas
exp(s(z,y )/τ)
lInfoNCE(T)=−log 0k0 , (5)
(cid:80)N
exp(s(z,y )/τ)
i=0 iki
3with(x,y ) ∼ p,y ,...,y ∼ η,andwhereτ denotesthetemperatureands(z,y;τ,η) =
0k0 1k1 NkN
zTy/τ −logη(y)isthescoringfunction. Further,y isthelearnablelabelembeddingofclassk ,
iki i
andidenotesthepositionof(x,k )withinT. Thelabelembeddingsareaparametrizedlookuptable
i
andaretheanalogstotheclassifierscoringweightsw ofaneuralnetworktrainedwiththelogloss.
k
SeveralvariantsofInfoNCEhavebeenintroduced,mostlyinthecontextofself-supervisedlearn-
ingChuangetal.[2020],Kalantidisetal.[2020],Chenetal.[2020],Lietal.[2023]. Thesupervised
variantproposedinKhoslaetal.[2020]isusedforpre-trainingandleverageslabelstocreatepositive
pairs. FurthervariantsdevelopedinChuangetal.[2020],Lietal.[2023]correctforfalsenegatives
comingfromthenoisedistributionbyre-weightingthesumovernegativesamples,whileKalantidis
etal.[2020]proposestosampleandcreateartificialhardnegativesfortraining.
InfoNCEisalsoabiasedestimatorofMIOordetal.[2018]. MaximizingtheMIbetweenlearned
representations that are based on related inputs (InfoMax principle Linsker [1988]) has yielded
empirical success in representation learning Hjelm et al. [2018], Henaff [2020], Bachman et al.
[2019],Chenetal.[2020],Heetal.[2020],Velicˇkovic´ etal.[2018]. AsMIisnotoriouslydifficult
toestimate,tractablevariationallowerboundsareusuallyoptimizedinpracticePooleetal.[2019].
InfoNCE provides such a lower bound: more specifically, optimizing InfoNCE is equivalent to
maximizingalowerboundofMIOordetal.[2018],Tianetal.[2020].
3 Results
3.1 InfoNCEimprovesparameterestimationovernegativelog-likelihoodinconditional
models
Precise parameter estimation is essential for conditional models to obtain good generalization
properties. Here we compare the estimation accuracy of InfoNCE against NLL in classification
settingswithdifferentdegreesoflabeluncertainty.
Initsstandardformulation, NLLmakestheassumptionthatthetargetconditionaldistributionis
degenerate,i.e. p(l|x)=δ(l,k)foradatasamplexwithclasslabelk. However,inpracticethetrue
conditionaldistributionisoftenmorecomplex,forinstanceduetouncertaintyorambiguityinthe
labelassignment. Asaconsequence,aninherentamountoferrorisintroducedintotheestimated
parameters. Several works have addressed this issue, for instance by introducing more entropy
intothereferencedistributionSzegedyetal.[2016]orthroughalternativelossfunctionsGutmann
andHyvärinen[2010],includingInfoNCE.InfoNCEdoesnotrequireanexplicitmodelastarget
distribution. Instead,parametersareestimatedinanimplicitmannerbycontrastinglabelscoming
fromthetruedistributionwithlabelscomingfromanoisedistribution.
Tostudythebenefitsofthisimplicitapproach,weconsidercasesinwhichtheunderlyingconditional
hasdifferentamountsofuncertainty(FigureSupp. 5). Specifically,wesampledatapointsxfroma
mixtureofGaussianswithK modes. Eachmodeθ ∈Rd correspondstoaclassinthesensethat
k
labelsforthedatapointsxaresampledfromtheconditionalmodelgivenby
exp(xTθ )
p(k|x,θ)= k . (6)
(cid:80)K exp(xTθ )
l=1 l
Concretely,wefirstsampleX ={x ,...,x }fromthemixturedistributionandthensubsample
1 M
X intoadatasetX ofsizeN > M withduplicates. Classlabelsareassignedbysamplingfrom
theconditionalmodelk ∼p(·|x,θ). Wethenestimatetheparametersθ usingNLLandInfoNCE.
k
Weincreasetheuncertaintybyincreasingthedegreeofalignmentbetweenmodesθ ,asdetailedin
k
SupplementarySectionA.1.
Theresults,visualizedinFigure1,indicatethatincreasedlevelsoflabeluncertaintyleadtoworse
parameterestimatesusingNLLwhencomparedtoInfoNCE.Asanticipated,parameterestimation
accuracydeterioratesasthemodealignmentdegreeincreasesforbothlossfunctions. However,the
error of InfoNCE’s estimates relative to those of NLL increases considerably more slowly. Our
experimentdemonstratestheadvantageofimplicitsamplingfromadatasetgeneratedbythetruedata
distributionasopposedtousinganexplicitmodeldistributionastarget.
Inordertoconnectthisresultwithdeepneuralnetworks,onecaninterpretthedataX asembeddings
(i.e. penultimatelayeractivations)andtheθ astheclassificationweightsproducingthelogitsof
k
4Figure 1: Parameter estimation quality of InfoNCE vs. negative log-likelihood in conditional
densitymodelswithdifferentdegreesofmodealignment. Theestimationerrorisreportedasthe
KL divergence. The alignment degree corresponds to the angle between modes. 0% alignment
correspondstoorthogonalmodesand80%correspondstoanangleof18◦.
thelastlayer. Fororthogonalmodes(0%alignment),X islinearlyseparableandbothobjectives
givecomparableestimates. However,fornon-orthogonalmodes(alignment>0%),InfoNCEmight
leadtobettergeneralization. Thismakesourobservationsparticularlyrelevantinfine-tuningsettings
whereonlythelastlayerisre-trained.
3.2 InfoNCEwithsofttargets
MotivatedbyInfoNCE’sfavourableparameterestimationpropertiesandtheempiricalsuccessof
training with soft targets, we sought to combine both. In this section, we propose a principled,
conceptuallysimplelossfunctionthatintegratessofttargetsdirectlyintoInfoNCE.
Derivation. ConsiderthecontinuouscategoricaldistributionsCC(p (·|x)),CC(η)inducedbythe
Y|X
trueconditionalandnoisedistribution,respectively.1 LetT =(α ,...,α )beatupleoflength
1 N+1
N+1,withα
≥0,(cid:80)K
α =1,containingasinglesampleα∼CC(p (·|x))andnoisesamples
i l=1 il Y|X
α˜ ∼ CC(η)otherwise. WeareinterestedinidentifyingthepositionS ofαgiventhetupleT, i.e.
P(S|T).Apriori,wehaveP(S =i)=1/(N+1)forallallowedpositionsi=1,...,N+1.Further,
weknowthatP(α |S = k) = p (α ;η)forallj ̸= k,andP(α |S = k) = p (α ;p (·|x)).
j CC j k CC k Y|X
Thus,
N (cid:89)+1 p CC(α k;p Y|X(·|x))N (cid:89)+1
P(T|S =k)=p (α ;p (·|x)) p (α ;η)= p (α ;η). (7)
CC k Y|X CC i p (α ;η) CC i
CC k
i=1 i=1
i̸=k
Fromthis,wecaninferthemarginal
1 N (cid:89)+1 N (cid:88)+1p CC(α k;p Y|X(·|x))
P(T)= p (α ;η) . (8)
N +1 CC i p (α ;η)
CC k
i=1 k=1
ByapplyingBayes’theoremincombinationwith(7)and(8),wecancomputetheposterior
pCC(αk;pY|X(·|x)) (cid:81)K
(cid:16) pY|X(i|x)(cid:17)αki
P(S =k|T)=
pCC(αk;η)
=
i=1 η(i)
. (9)
(cid:80)N+1 pCC(αl;pY|X(·|x)) (cid:80)N+1(cid:81)K (cid:16) p(Y|X(j|x)(cid:17)αlj
l=1 pCC(αl;η) l=1 j=1 η(j)
1To simplify notation we denote CC([p (1|x),...,p (K|x)]) as CC(p (·|x)) and
Y|X Y|X Y|X
CC([η(1),...,η(K)])asCC(η)
5Weestimatethedensityratiop (i|x)/η(i)usingtheparametricmodelexp(s(z,y ;τ,η))where
Y|X k
s(z,y ;τ,η)=zTy /τ −logη(y)andz =f(x;θ),whichcanforinstancebeaneuralnetwork. By
k k
insertingthisbackinto(9),wecanstatethemaximumlikelihoodestimationproblem
(cid:16) (cid:17)
exp
(cid:80)K
α s(z,y ;τ,η)
i=1 ki i
max . (10)
(cid:16) (cid:17)
θ (cid:80)N+1exp (cid:80)K α s(z,y ;τ,η)
l=1 j=1 lj j
Takingthenegativelogarithm,wecanrewrite (10)asoursofttargetInfoNCElossfunction
 exp(cid:16) (cid:80)K
α s(z,y
;τ,η)(cid:17) 
i=1 ki i
E −log
(cid:16)
(cid:17). (11)
α1,...,αk−α 1k ,α∼ kC +C 1( ,.p .Y .,α|X N)
+1∼CC(η)
(cid:80)N l=+ 11exp (cid:80)K j=1α ljs(z,y j;τ,η)
Sinceexp((cid:80)K
α s(z,y ;τ,η))convergestothedensityratiop(α ;p )/p(α ;η),wecansimply
i=1 ki i l Y|X l
useexp(zTy /τ)duringinferencetoestimateaproportionalscoreoftheprobabilityp (i|x).
i Y|X
Discussion. Toprovideanintuitionfor(11),werestateitasanegativetemperaturescaledenergy
cross-entropy
K
−(cid:88)
α log
exp(s(z,y i;τ,η))
, (12)
ki (cid:16) (cid:17)
(cid:80)N+1exp (cid:80)K
α s(z,y ;τ,η)
i=1 l=1 j=1 lj j
wherethefractioninsideofthelogarithmcanbeinterpretedasanenergyfunction2. Notably,(12)
isstructurallysimilartothesofttargetcross-entropy(3). Foreachclass,wenowhaveanattractive
pullactingonitsembeddingy andz weightedbyα . Forinstance, incaseoflabelsmoothing
i ki
withauniformnoisedistributionξ ≡1/K,wehaveα =1−(K−1/K)ϵforthetrueclassiand
ki
α =ϵ/K otherwise,whichresultsinadampenedattractionbetweenthedataembeddingandthe
kj
originalhardtarget.
Whiletheinferencemodelexp(zTy /τ)fromtheoriginalInfoNCEdoesnotchange,thegradient
i
dynamicsinducedbytherespectiveenergylandscapesaredifferent. Amoredetaileddiscussionof
thechangeinattraction-repulsiondynamicsaswellasaderivationforthegradientisprovidedinthe
SupplementarySectionA.2.
3.3 InfoNCEwithsoftdistributions
An alternative to integrating soft weights directly into the loss function is to sample from the
correspondingdistributioninstead. Inthissection,weexamineanInfoNCElossinwhichlabelsare
drawnfromthesoftdistribution
pϵ(k|x)=(1−ϵ)p(k|x)+ϵξ(k), (13)
instead of the data generated by the true conditional. A derivation of this loss is provided in the
SupplementarySectionA.3andfollowsdirectlyfromthestandardderivationofInfoNCE(e.g. see
Damrichetal.[2022]).
EffectofsoftdistributionsontheMIlowerbound. Here,wesketchthederivationanddiscuss
the implications of sampling from the soft distribution (13) on InfoNCE’s MI lower bound with
respecttotheunderlyingdatadistributionp. WeprovideacompleteproofinSupplementaryA.3.
Aswesawintheprevioussection,theoptimalcriticisproportionaltothedensityratiobetweenthe
softdataandthenoisedistribution
pϵ (y|z)
Y|Z
exp(s(z,y)/τ)∝ , (14)
p (y)
Y
2Notethatq(k|z) = exp(s(z,y ))/(cid:80)N+1exp((cid:80)K α s(z,y ))isnotnecessarilyaprobabilitysince
i l=1 j=1 lj j
(cid:80)K
q(k|x)doesnotsumto1ingeneral.
k=1
6withη ≡ξ ≡p . UsingtheargumentprovidedintheproofoftheMIlowerboundTianetal.[2020],
Y
Oordetal.[2018],wedirectlyobtain
(cid:34) pϵ (y|z)(cid:35)
lSDInfoNCE(T)≥logN − E log Y|Z . (15)
(y,z)∼pϵ p Y(y)
Theexpectedvaluecanbewrittenas
(cid:34) pϵ (y|z)(cid:35)
E log Y|Z =I(Z;Y)+H(Y|Z)−H (Y|Z), (16)
(y,z)∼pϵ p Y(y) ϵ
whereH(Y|Z)denotestheconditionalentropywithrespecttop and
Y,Z
(cid:104) (cid:105)
H (Y|Z)= E −logpϵ (y|z) . (17)
ϵ Y|Z
(y,z)∼pϵ
Y,Z
Insertingthisbackinto(15)yieldsfollowinginequality
I(Z;Y)+H(Y|Z)−H (Y|Z)≥logN −lSDInfoNCE(T). (18)
ϵ
For ϵ = 0, we have H (Y|Z) = H(Y|Z) and the above bound returns to its original form. In
0
contrast,forϵ=1,H (Y|Z)=H(Y)=I(Y;Z)+H(Y|Z),(18)becomestrivialduetothehigh
1
entropyinpϵ=1.
Thelefthandsideof(18)islargerorequaltozero(seeSupp. A.3). Thus,byminimizingInfoNCE
whensamplingfromasoftdistribution,thebaselineMIaswellastheentropyincrease. Intuitively,ϵ
regulatestheamountofentropyandMIdistilledintothelearnedrepresentations. Highvaluescan
increasepredictionuncertainty,whichmayhelpwithambiguousinstancesandmodelcalibration,
whilelowvaluesboostmodelconfidence,andincreaseMIbetweeninputandclassembeddings.
4 Experiments
WeshowthatsofttargetInfoNCEachievescompetitiveresultscomparedtosofttargetcross-entropy
andsurpassesNLL,InfoNCEandsoftdistributionInfoNCEinquantitativeevaluationonclassification
benchmarks,includingImageNet. Weprovideanablationstudyonthesensitivitytovaryingnumbers
ofnoisesamples,varyingamountsoflabelsmoothingnoiseandexaminecalibrationproperties,in
ordertoobtainfurtherinsightsintothemodelstrainedwithourloss.
Lossimplementation. Weprovideasimpleimplementationof(softtarget)InfoNCEthatdirectly
worksonclassscores(logits)and,thus,iscompatiblewithstandardclassificationmodels(Figure2).
InfoNCElossimplementationsusuallytakeasinputsembeddingvectors Heetal.[2020],Chenetal.
[2020],Khoslaetal.[2020],requiringadditionalcodeadaptationsoftheneuralnetworkinorderto
workinthesupervisedclassificationcase. Morespecfically,inordertocomputeclassscores(and
softtargetembeddings),classificationheadweightsneedtobeaccessed. Whilethisisinconvenient,
italsocreatesdiscrepanciesbetweentrainingandinferencecode. Instead,weleveragesofttargetsor
one-hotencodings,incaseofhardtargets,todirectlyworkonthelogits.
4.1 Classificationaccuracy
WeperformimageclassificationexperimentsonImageNetKrizhevskyetal.[2012],TinyImageNet
Krizhevskyetal.[2009]andCIFAR-100Krizhevskyetal.[2009],andnodeclassificationexperiments
ontheCellTypeGraphbenchmarkCerroneetal.[2022]. Forimageclassificationweuseavision
transformerbase(ViT-B/16)Dosovitskiyetal.[2020]witha3-layerMLPclassificationhead,pre-
trained as a masked autoencoder He et al. [2022] with image patches of size 16×16. For node
classification,weuseaDeeperGCNLietal.[2019,2020]andreportperformanceonthistaskasthe
averageovera5-foldcross-validationexperimentCerroneetal.[2022].
Fortheexperimentstobecomparable,weusethesametrainingrecipeandonlyadaptthe(base)
learningrateforoptimalperformanceoftherespectivelossfunction. Specifically,weusethesame
neural network architecture, fix the temperature to τ = 1.0, and use the dot-product as scoring
functions(z,y) = zTy. Forimageclassification,wefollowcommonpracticeforViTend-to-end
7class SoftTargetInfoNCE(nn.Module):
def __init__(self, noise_probs, T=1.0):
# noise_probs: (K,) noise probabilities over classes
# T: temperature
super().__init__()
self.K = noise_probs.shape[0]
self.log_noise = torch.log(noise_probs).unsqueeze(0)
self.T = T
def forward(self, logits, targets):
# logits: (N, K) class scores (not true logits!)
# targets: (N, K) (soft) target weights for each class
N = targets.shape[0]
targets = targets.repeat(N,1).unsqueeze(1).reshape(N, N, self.K)
logits = (logits / self.T) - self.log_noise
logits = (logits.unsqueeze(1) * targets).sum(-1)
logits -= (torch.max(logits, dim=1, keepdim=True)[0]).detach()
labels = torch.arange(N, dtype=torch.long).cuda()
return nn.CrossEntropyLoss()(logits, labels)
Figure2: SofttargetInfoNCEPyTorchcode. Forclarity,thisassumestrainingononeGPU.Inthe
distributedcase,thishastobeslightlyadaptedinordertogather(soft)targetsfromotherGPUssuch
thattheycanbeusedasadditionalnegatives. Formoredetailsseecoderepository.
fine-tuning,usingtherecipeprovidedinHeetal.[2022]. WetrainViTsfor100epochswithabatch
sizeof1024usinglabelsmoothingϵ=0.1,MixUpα =0.8andCutMixofα =1.0. Fornode
M C
classification,wetraina32-layerDeeperGCNwithabatchsizeof8andalearningrateof0.001.
Thehardtargetcross-entropyandInfoNCEbaselinesaretrainedfor500epochsduetooverfitting,
whilethesofttargetbaselinesaretrainedfor1000epochswithlabelsmoothingofϵ=0.1. Further
technicaldetailsareprovidedintheSupp. A.4.
Comparisontosofttargetcross-entropy. Table1showsacomparisonofsofttargetInfoNCE
toitscross-entropylossanalogacrossthefourbenchmarkdatasets. Whilesofttargetcross-entropy
givesslightlybetterresultsonImageNet(∆0.29%)andCellTypeGraph(∆0.55%),weobtainon-par
performanceonTinyImageNet(∆0.19%)andCIFAR-100(∆0.06%). Morebroadly,weobserve
similarconvergencebehaviourandperformanceforbothlossfunctionsasaresponsetolearning
rateswithinthesamerange. Forinstance,forimageclassification,wefoundthatbaselearningrates
between0.0002and0.0006leadtooptimalresultsinbothcases. Notably,forimageclassification,
wechosea3layerMLPasclassificationheadoveralinearlayersinceweobservedgainsintop-1
accuracyforbothlossfunctions. However,gainsforsofttargetInfoNCEwereconsistentlylarger
(∆1.72%TinyImageNet,∆0.7%CIFAR-100)comparedtosofttargetcross-entropy(TinyImageNet
∆0.6%,CIFAR-100∆0.36%)notonlyintop-1performancebutalsoacrossruns. Sinceperformance
seemstosaturatearoundthesameaccuracylevels,weconcludethatsofttargetInfoNCEiscompetitive
withsofttargetcross-entropy.
ComparisontoInfoNCEwithsoftdistributionsampling. Table1comparessofttargetInfoNCE
againstthemoresimplisticapproachofsamplingahardtargetfromthedistributionprovidedbythe
softtargets. IntegratingsofttargetsdirectlyintoInfoNCEconsistentlyresultsinbettertop-1accuracy
acrossallbenchmarks. However,samplingfromasoftdistributionresultsintheworstperforming
modelsonCIFAR-100andCellTypeGraphandisonlyon-parwiththehardtargetbaselines.
Comparisontohardtargetbaselines. WefinallycomparesofttargetInfoNCEtothestandard
cross-entropyloss(NLL)andInfoNCE,whichbothleveragehardtargets. Regularizationtechniques
suchaslabelsmoothingandMixUp,thus,cannotbeusedincombinationwiththesebaselines. While
NLLison-parwithsofttargetInfoNCEonCIFAR-100(∆0.04%)andCellTypeGraph(∆0.2%),
itisoutperformedonImageNet(∆1.19%)andTinyImageNet(∆1.23%). Thisalsoholdsforthe
InfoNCEbaseline,whichhasthesameperformancepatternastheNLLbaseline.
8Table 1: Top-1 classification accuracy for various datasets. We compare soft target InfoNCE to
differentbaselinelossfunctions.
ImageNet TinyImageNet CIFAR-100 CellTypeGraph
NLL 82.35 82.63 90.84 86.92
Softtargetcross-entropy 83.85 83.67 90.74 87.67
InfoNCE 82.52 82.72 90.75 86.80
SoftdistributionInfoNCE 82.96 82.77 89.82 86.62
SofttargetInfoNCE 83.54 83.86 90.80 87.12
(a) Top-1 accuracy as a function of noise sam-
ples/batchsizeforViT-B/16modelstrainedwith (b)Top-1accuracyasafunctionoflabelsmoothing
softtargetlossesincombinationwithlabelsmooth- noiseforViT-B/16modelstrainedwithsofttarget
ing,MixUpandCutMix. losses.
Figure3: Noisesample(left)andlabelsmoothing(right)ablationexperimentsonTinyImageNet.
4.2 Ablationexperiments
Inourablationexperiments,weexploredifferentaspectsofthesofttargetInfoNCElossusingthe
ViT-B/16modeldescribedintheprevioussection.
Numberofnoisesamples. SofttargetInfoNCEreliesonnoisesamplestocontrastthetruesoft
target with. While this can be separated from the batch size, it is more efficient to couple both
andleverageothersofttargetsasnoisesamples. Wegiveasofttargetcross-entropybaselinefor
comparison. Figure3visualizestheresultsofthisexperimentintermstheirtop-1performanceon
TinyImageNet. Asanticipated,smallerbatchsizeshaveastrongernegativeeffectonperformance
formodelstrainedwiththenoisecontrastivelosscomparedtothebaseline,duetofeweravailable
negative samples, resulting in a performance difference of ∆4.21% for the smallest batch size.
However, asthebatchsizeincreasesViTstrainedwithsofttargetInfoNCEareon-parwiththeir
cross-entropyanalog,andslightlyoutperformthemby∆0.19%.
Sensitivitytolabelsmoothing. Weinvestigatehowdifferentamountsoflabelsmoothingnoise
ϵaffectmodelstrainedwithsofttargetInfoNCE.Figure3visualizestheresultsintermsoftheir
top-1 accuracy on Tiny ImageNet. Notably, accuracy of soft target InfoNCE models varies less
(max. deviationof∆0.37%andstandarddeviationof0.12)comparedtothesofttargetcross-entropy
baseline(maxdeviationof∆0.96%andstandarddeviationof0.34).
Confidencecalibration. Labelsmoothingincombinationwithcross-entropyhasshowntoimprove
modelcalibrationMülleretal.[2019]. Here,weinvestigateconfidencecalibrationpropertiesofour
visiontransformermodelstrainedwithsofttargetInfoNCEandcomparetoitscross-entropyanalog.
SinceInfoNCEmodelsconvergetounnormalizeddensityfunctionsforwhichstandardcalibration
metricscannotbecomputed,wetreatthemodeloutputsaslogitsandnormalizethemusingasoftmax
function. Figure4showsreliabilitydiagramsofViTstrainedonTinyImageNetwithlabelsmoothing
ϵ=0.1andofthebestperformingViTsonCIFAR-100asdescribedintheprevioussection. Notably,
modelstrainedwithsofttargetInfoNCEwerebettercalibratedinthesetwocases. Specifically,we
9(b)BestperformingmodelsonCIFAR-100trained
(a)Softtargetlossestrainedwithlabelsmoothing withsofttargetlossesincombinationwithlabel
onTinyImageNet. smoothing,MixUPandCutMix.
Figure4: ReliabilitydiagramsofViT-B/16trainedonTinyImageNet(left)andCIFAR-100(right).
obtainedanexpectedcalibrationerror(ECE)of3.9%onTinyImageNetand2.9%onCIFAR-100
whichwaslowerthantherespectiveECEofthesofttargetcross-entropymodels(7%TinyImageNet,
15.8%CIFAR-100). WeadditionallycomparedthebestperformingTinyImageNetmodelsofthe
previoussectionforwhichweobtainedaslightlyworseECEof7.7%forthesofttargetInfoNCE
ViTcomparedtoanECEof6.3%forthesofttargetcross-entropyViT(FigureA.4).
5 Conclusionsandlimitations
WepresentedsofttargetInfoNCE,aprincipledgeneralizationofInfoNCEforconditionaldensity
estimationinsupervisedclassificationproblems. Wederivedourlossfromfirstprinciplesusinga
continuouscategoricaldistributionincombinationwithaBayesianargument. Theabilitytoleverage
softtargetscomingfromsophisticatedaugmentationandregularizationtechniquesallowsourloss
functionbeon-parwithsofttargetcross-entropyandtooutperformhardtargetNLLandInfoNCE
losses.
Aslimitationsweidentifiedthedropinperformancewhenasmallbatchsizeisusedincombination
withmini-batchandnoisesamplecoupling.However,weleaveituptofutureworktoinvestigateifthis
canbemitigatedwithseparatelysampledsoftnoisetargets.Further,weobservedaslightperformance
dropwithlinearclassificationheadswhencomparedtosofttargetcross-entropy. Nevertheless,deep
classification models are typically optimized to work well with cross-entropy, and we observed
competitiveperformancewhenemployinga3-layerMLPclassificationhead. Thissuggeststhatby
exploringdifferentneuralarchitecturesonemightfindmorefittingalternatives.
Inadditionalfutureworkwewanttoexploredifferentnoisedistributionsforsamplingsoftweights
aswellastestourlossinotherproblemsettingssuchasknowledgedistillation.
6 Acknowledgements
TheauthorsthankAnnaKreshuk(EMBL)formanyvaluableexchangesandinsightfuldiscussions.
References
PhilipBachman,RDevonHjelm,andWilliamBuchwalter. Learningrepresentationsbymaximizing
mutualinformationacrossviews. Advancesinneuralinformationprocessingsystems,32,2019.
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced
datasetswithlabel-distribution-awaremarginloss. Advancesinneuralinformationprocessing
systems,32,2019.
LorenzoCerrone,AthulVijayan,TejasvineeMody,KaySchneitz,andFredAHamprecht. Celltype-
graph: Anewgeometriccomputervisionbenchmark. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages20897–20907,2022.
10TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframeworkfor
contrastivelearningofvisualrepresentations. InInternationalconferenceonmachinelearning,
pages1597–1607.PMLR,2020.
Ching-YaoChuang,JoshuaRobinson,Yen-ChenLin,AntonioTorralba,andStefanieJegelka. De-
biasedcontrastivelearning. Advancesinneuralinformationprocessingsystems,33:8765–8775,
2020.
SebastianDamrich,JanNiklasBöhm,FredAHamprecht,andDmitryKobak. Contrastivelearning
unifiest-sneandumap. arXivpreprintarXiv:2206.01816,2022.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929,2020.
GamaleldinElsayed,DilipKrishnan,HosseinMobahi,KevinRegan,andSamyBengio.Largemargin
deepnetworksforclassification. Advancesinneuralinformationprocessingsystems,31,2018.
ElliottGordon-Rodriguez,GabrielLoaiza-Ganem,andJohnCunningham.Thecontinuouscategorical:
anovelsimplex-valuedexponentialfamily. InInternationalConferenceonMachineLearning,
pages3637–3647.PMLR,2020a.
Elliott Gordon-Rodriguez, Gabriel Loaiza-Ganem, Geoff Pleiss, and John Patrick Cunningham.
Usesandabusesofthecross-entropyloss: Casestudiesinmoderndeeplearning. arXivpreprint
arXiv:2011.05231,2020b.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. InInternationalconferenceonmachinelearning,pages1321–1330.PMLR,2017.
MichaelGutmannandAapoHyvärinen. Noise-contrastiveestimation: Anewestimationprinciple
forunnormalizedstatisticalmodels. InProceedingsofthethirteenthinternationalconferenceon
artificialintelligenceandstatistics,pages297–304.JMLRWorkshopandConferenceProceedings,
2010.
XiaotianHan,ZhimengJiang,NinghaoLiu,andXiaHu. G-mixup: Graphdataaugmentationfor
graphclassification. InInternationalConferenceonMachineLearning,pages8230–8248.PMLR,
2022.
TrevorHastie,RobertTibshirani,JeromeHFriedman,andJeromeHFriedman. Theelementsof
statisticallearning: datamining,inference,andprediction,volume2. Springer,2009.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages770–778,2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on
computervisionandpatternrecognition,pages9729–9738,2020.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
autoencodersarescalablevisionlearners.InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages16000–16009,2022.
OlivierHenaff. Data-efficientimagerecognitionwithcontrastivepredictivecoding. InInternational
conferenceonmachinelearning,pages4182–4192.PMLR,2020.
GeoffreyHinton,OriolVinyals,andJeffDean. Distillingtheknowledgeinaneuralnetwork. arXiv
preprintarXiv:1503.02531,2015.
RDevonHjelm,AlexFedorov,SamuelLavoie-Marchildon,KaranGrewal,PhilBachman,Adam
Trischler,andYoshuaBengio. Learningdeeprepresentationsbymutualinformationestimation
andmaximization. arXivpreprintarXiv:1808.06670,2018.
11RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploringthe
limitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016.
YannisKalantidis,MertBulentSariyildiz,NoePion,PhilippeWeinzaepfel,andDianeLarlus. Hard
negativemixingforcontrastivelearning. AdvancesinNeuralInformationProcessingSystems,33:
21798–21809,2020.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, CeLiu, andDilipKrishnan. Supervisedcontrastivelearning. AdvancesinNeural
InformationProcessingSystems,33:18661–18673,2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
UniversityofToronto,2009.
AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Imagenetclassificationwithdeepconvolu-
tionalneuralnetworks. Advancesinneuralinformationprocessingsystems,25,2012.
AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Imagenetclassificationwithdeepconvolu-
tionalneuralnetworks. CommunicationsoftheACM,60(6):84–90,2017.
GuohaoLi,MatthiasMüller,AliThabet,andBernardGhanem. Deepgcns: Cangcnsgoasdeepas
cnns? InTheIEEEInternationalConferenceonComputerVision(ICCV),2019.
GuohaoLi,ChenxinXiong,AliThabet,andBernardGhanem. Deepergcn: Allyouneedtotrain
deepergcns. arXivpreprintarXiv:2006.07739,2020.
HaochenLi,XinZhou,LuuAnhTuan,andChunyanMiao. Rethinkingnegativepairsincodesearch.
arXivpreprintarXiv:2310.08069,2023.
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense
objectdetection. InProceedingsoftheIEEEinternationalconferenceoncomputervision,pages
2980–2988,2017.
RalphLinsker. Self-organizationinaperceptualnetwork. Computer,21(3):105–117,1988.
ZhuangMaandMichaelCollins. Noisecontrastiveestimationandnegativesamplingforconditional
models: Consistencyandstatisticalefficiency. arXivpreprintarXiv:1809.01812,2018.
RafaelMüller,SimonKornblith,andGeoffreyEHinton. Whendoeslabelsmoothinghelp? Advances
inneuralinformationprocessingsystems,32,2019.
AaronvandenOord,YazheLi,andOriolVinyals. Representationlearningwithcontrastivepredictive
coding. arXivpreprintarXiv:1807.03748,2018.
BenPoole,SherjilOzair,AaronVanDenOord,AlexAlemi,andGeorgeTucker. Onvariational
boundsofmutualinformation. InInternationalConferenceonMachineLearning,pages5171–
5180.PMLR,2019.
SainbayarSukhbaatar,JoanBruna,ManoharPaluri,LubomirBourdev,andRobFergus. Training
convolutionalnetworkswithnoisylabels. arXivpreprintarXiv:1406.2080,2014.
ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonShlens,andZbigniewWojna. Rethinking
theinceptionarchitectureforcomputervision. InProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition,pages2818–2826,2016.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In European
conferenceoncomputervision,pages776–794.Springer,2020.
PetarVelicˇkovic´, WilliamFedus, WilliamLHamilton, PietroLiò, YoshuaBengio, andRDevon
Hjelm. Deepgraphinfomax. arXivpreprintarXiv:1809.10341,2018.
SangdooYun,DongyoonHan,SeongJoonOh,SanghyukChun,JunsukChoe,andYoungjoonYoo.
Cutmix: Regularizationstrategytotrainstrongclassifierswithlocalizablefeatures. InProceedings
oftheIEEE/CVFinternationalconferenceoncomputervision,pages6023–6032,2019.
12HongyiZhang,MoustaphaCisse,YannNDauphin,andDavidLopez-Paz. mixup: Beyondempirical
riskminimization. arXivpreprintarXiv:1710.09412,2017.
ZhiluZhangandMertSabuncu. Generalizedcrossentropylossfortrainingdeepneuralnetworks
withnoisylabels. Advancesinneuralinformationprocessingsystems,31,2018.
RolandSZimmermann,YashSharma,SteffenSchneider,MatthiasBethge,andWielandBrendel.
Contrastivelearninginvertsthedatageneratingprocess. InInternationalConferenceonMachine
Learning,pages12979–12990.PMLR,2021.
13Figure5: DifferentdegreesofmodealignmentsofdatasetsX visualizedusingTSNE.Thealignment
degreecorrespondstotheanglebetweenmodes. 0%alignmentcorrespondstoorthogonalmodesand
80%correspondstoanangleof18◦.
A Supplementarymaterial: Noisecontrastiveestimationwithsofttargetsfor
conditionalmodels
A.1 InfoNCEimprovesparameterestimationovernegativelog-likelihoodinconditional
models
InordertocompareparameterestimationqualityofInfoNCEandNLL,wecreateninedatasetssam-
pledfromdifferentGaussianmixturemodelswithdifferentdegreesofmodealignment. Specifically,
wesampleeachdatasetX fromaGMMp(x)=(cid:80)20 π N(x|10·θ ,I),withθ ∈R20,∥θ ∥=1
i=1 i i i i
andπ ≡1/20. Theamountofmodealignmentin%inaGMMcorrespondstothescalingfactor
i
appliedtoπ/2determiningtheanglesϕ ,...,ϕ ofthe20-sphericalcoordinatesystem. Concretely,
1 19
letsbeascalingfactordeterminedbythepercentageaccordingtos = 100/(100−perc.)thenϕ
iscomputedasϕ = π/(2s). Intuitively,wecaninterpretthisasfixingonemodeinanorthogonal
systemandcontracting/aligningtheremainingmodestothefixedone.
ForeachexperimentwecreateadatasetX composedof1600datapointssampledfromaGMM.X
isthenuniformlysubsampledintoX with|X|=32000,andweassignlabelstoeachx∈X based
ontheconditionalmodelp(k|x,θ)=exp(xTθ )/(cid:80)K exp(xTθ )describedinthemaintext.
k l=1 l
Forfittingtheparameters,wetrainfor500epochswithearlystopping,abatchsizeof1024anda
learningrateof0.001. WeevaluateparameterestimationqualityforeachGMMover10random
seeds.
Figure5visualizesdatasetsX withdifferentdegreesofmodealignment.
14A.2 InfoNCEwithsofttargets: discussion
Attraction-repulsionin
K
l=−(cid:88)
α log
exp(s(z,y i;τ,η))
, (19)
ki (cid:16) (cid:17)
(cid:80)N+1exp (cid:80)K
α s(z,y ;τ,η)
i=1 l=1 j=1 lj j
canbefurtherseparatedbyrestatingthelossfunctionasthetwosummands
K
−(cid:88)
α log
exp(s(z,y i))
, (20)
ki (cid:18) (cid:19)
i=1 exp(α kis(z,y k))+(cid:80)N l=+ 11exp α lis(z,y i)+(cid:80)K j=1α˜ ljs(z,y j)
l̸=k j̸=i
withα˜ =α −α ,and
lj lj ki
K K
(cid:88) (cid:88)
α α s(z,y ), (21)
ki li l
i=1 l=1
l̸=k
wherewehaveomittedthedependenceofthescoringfunctionsonτ andηforreadability. Similar
to(19),(20)predominantlycontributeswithanattractivepullbetweenthelabelembeddingy and
i
thedataembeddingzthatisregulatedbyα ,duetothenumeratorinthelogarithm. Wediscussthe
ki
denominatoraswellasthesecondterm(21)inthecontextoflabelsmoothingwithauniformnoise
distributionξ ≡1/K,i.e. α =1−(K−1)ϵ/K forthetrueclassi andα =ϵ/K forj ̸=i
ki0 0 kj 0
otherwise. Forthecompositionofnoiselabels,wedifferentiatebetweenthefollowingcases:
• Incaseofi = i andthel-thsamplehaslabeli ̸= i ,wehaveα = ϵ/K,α˜ = 1−ϵ,
0 l 0 li lil
andα˜ =0otherwise. Effectivelythisactsasrepulsionbetweenzandlabeli ofstrength
lj 0
ϵ/K aswellaszandlabeli ofstrength1−ϵ.
l
• Incaseofi = i andthel-thsamplehaslabeli = i ,wehaveα = 1−(K −1)ϵ/K,
0 l 0 li0
andα˜ =0,resultinginarepulsionbetweenzandi .
lj 0
• In case of i ̸= i and the l-th sample has label i ̸= i , we have α = ϵ−1 which
0 l 0 li0
results in an attractive force between z and label i . Further, in case of i = i then
0 l
α = 1−(K −1)ϵ/K, and α˜ = 0 otherwise. In the event of i ̸= i, we have α =
li lj l li
ϵ/K,α˜ = 1−ϵ,andα˜ = 0otherwise. Ultimately,thisactsasarepulsionbetweenz
lil lj
andthelabelsi,i withtherespectivestrength.
l
• Incaseofi ̸= i andthel-thsamplehaslabeli = i ,wehaveα = ϵ/K andα˜ = 0
0 l 0 li lj
otherwise,whichresultsinarepulsiveforcebetweenzandlabeli.
The second term (21) is a penalty term and acts as a repulsion between z and all classes while
penalzingthetrueclassi lessthanothersincaseofϵ<1/2. Insummary,softtargetschangethe
0
attractionrepulsiondynamicsbetweenanembeddeddatapointandalllabelembeddingsbyadjusting
theamountofpull-pushapplied.
Gradientanalysis. Here,wederiveandexaminethegradientresponsesofsofttargetInfoNCEwith
respecttodataembeddingsz. Weconsiderasscoringfunctionthecosinesimilarity(withtemperature
τ =1)s(z,y)=zTy/∥z∥∥y∥andfirstcomputethegradientwithrespecttozˆ=z/∥z∥.
 
K N+1 K
(cid:88) (cid:88) (cid:88)
∂ zˆl=− α kiy i− C l(z,y) α ljy j (22)
i=1 l=1 j=1
K (cid:32) N+1 (cid:33)
(cid:88) (cid:88)
= −α y + C (z,y)α y , (23)
ki i l lj j
i=1 l=1
(cid:16) (cid:17) (cid:16) (cid:17)
withC (z,y)=exp
(cid:80)K
α s(z,y
/(cid:80)N+1exp (cid:80)K
α s(z,y ) . Thus,fortheunnormal-
l j=1 lj j l=1 j=1 lj j
izedembeddingsz,wehave
I−zˆzˆT
∂ l= ∂ l (24)
z ∥z∥ zˆ
15For soft target embeddings, our loss function inherits gradient response properties directly from
theoriginalInfoNCE.Thus,forweaktargets3gradientmagnitudesapproachzero,whileforstrong
targets4gradientmagnitudesarelargerthanzero.
Fortheoriginalhardtargets,weobtaingradientmagnitudeslargerthanzerowhenthetargetisstrong,
i.e. whenzT ·y ≈ 0. Morespecifically, wewouldobtainthefollowinggradientresponsewith
j
respecttoy :
j
(cid:13)
(cid:13)
(cid:80)N+1α exp(cid:16) (cid:80)K
α s(z,y
)(cid:17)(cid:13)
(cid:13)
(cid:13) l=1 lj i=1 li i (cid:13)
α kj(cid:13)−1+
(cid:16)
(cid:17)(cid:13). (25)
(cid:13) (cid:80)K α exp (cid:80)K α s(z,y ) (cid:13)
(cid:13) i=1 kj i=1 li i (cid:13)
Asinthepreviousdiscussion, weconsiderthecaseoflabelsmoothingwithauniformnoisedis-
tribution. Inthiscase,forϵ ∈ (0,1/2)thesecondsuminthedenominatorissmallerthan1since
α /α ≤1,andlargerotherwise. Thus,inbothcasesthetermin(25)islargerthanzero.
lj kj
The analytic expression of soft target InfoNCE gradients is similar to those of soft target cross-
entropy,althoughwithdistinctcharacteristics: softtargetInfoNCEinvolvesasummationoverbatch
sampleembeddingsinsteadofclasses,andadditionally,thesoftlabelsdirectlyimpactthelogits. The
analyticalexpressionswederivedforthegradientmagnitudesforscenariosinvolvinghardpositive
andhardnegativepairsrevealthatinthepresenceofhardpositives,thegradientmagnitudeincreases
withthenumberandstrengthofthenegativepairs. Similarly,ininstancesinwhichhardnegativesare
given,thegradientmagnitudeincreasesproportionallywiththestrengthofthepositiveaswellasthe
numberandstrengthofthenegativepairs.
A.3 InfoNCEwithsoftdistributions
Derivation. LetS betherandomvariabledenotingthepositionofy withinT. Weareinterested
0
inrecoveringthepositionS giventheknowledgeofT,i.e. P(S|T). Apriori,wehaveP(S =i)=
1/(N +1)forallallowedpositionsi=0,...,N. Further,weknowthatP(y |S =0)=η(y )for
j j
allj ̸=0,andP(y |S =0)=pϵ(y ). Thus,
0 0
P(T|S =0)=
pϵ(y 0|x)(cid:89)N
η(y ). (26)
η(y ) i
0
i=0
Fromthis,wecaninfer
P(T)=
1 (cid:89)N
η(y
)(cid:88)N pϵ(y k|x)
. (27)
N +1 i η(y )
k
i=0 k=0
ByapplyingBayes’theoremincombinationwith(26)and(27),wecancomputetheposterior
pϵ(y0|x)
P(S =0|T)=
η(y0)
. (28)
(cid:80)N pϵ(yk|x)
k=0 η(yk)
Weestimatetheratiopϵ(y|x)/η(y)usingtheparameterizedmodel
exp(s(z,y)/τ), (29)
wherez =f(x;θ)isforinstanceaneuralnetwork. Inserting(29)into(28)andtakingthenegative
logarithmresultsinthesoftdistributionInfoNCElossfunction
exp(s(z,y )/τ)
lSDInfoNCE(T)=−log 0 . (30)
(cid:80)N
exp(s(z,y )/τ)
k=0 k
Notably,forη ≡ξ,theparameterizedmodel(29)convergestoafunctionproportionaltothedensity
ratio
p(y|x)
(1−ϵ) +ϵ. (31)
ξ(y)
Multiplyingbyξ(y)allowstorecoverthedensity(1−ϵ)p(y|x)+ϵξ(y).
3ThesearetargetswithzT·y≈1inthecaseoftruetargets,andzT·y≈−1inthecaseofnoiseornegative
targets.
4ThesearetargetswithzT ·y≈0.
16EffectofsoftdistributionsontheMIlowerbound: derivation. Herewederiveavariational
lower bound for soft distribution InfoNCE connecting MI and entropy properties of the learned
representations. To recapitulate, let p be the "data" distribution, p the negative sampling
Y,Z Y
distributionaswellasthenoisedistributionforlabelsmoothing,i.e. p ≡η ≡ξ. Further,wedefine
Y
theconditionalandjointsoftdatadistributionsas
pϵ (y|z)=(1−ϵ)p (y|z)+ϵp (y), (32)
Y|Z Y|Z Y
pϵ (z,y)=(1−ϵ)p (z,y)+ϵp (y)p (z). (33)
Z,Y Z,Y Y Z
Next,wederivethelowerbound
 pϵ Y|Z(y0|x) 
lSDInfoNCE(T)≥E −log pY(y0)  (34)
T (cid:80)N pϵ Y|Z(yk|x)
k=0 pY(yk)
=E(cid:34) log(cid:32)
1+
p Y(y 0) (cid:88)N pϵ Y|Z(y k|x)(cid:33)(cid:35)
(35)
T pϵ Y|Z(y 0|x)
k=1
p Y(y k)
(cid:34) (cid:32)
p (y )
(cid:34) pϵ (y|x)(cid:35)(cid:33)(cid:35)
≈E log 1+ Y 0 N E Y|Z (36)
T pϵ Y|Z(y 0|x) y∼pY p Y(y)
(cid:34) (cid:32) (cid:33)(cid:35)
p (y )
=E log 1+ Y 0 N (37)
T pϵ Y|Z(y 0|x)
(cid:34) pϵ (y|z)(cid:35)
≥logN − E log Y|Z , (38)
(z,y)∼pϵ
Z,Y
p Y(y)
whereN denotesthethenumberofnegativesamples.
Wecanrewritetheexpectationin(38)into
(cid:20) (1−ϵ)p (y|z)+ϵp (y)(cid:21)
E log Y|Z Y (39)
(z,y)∼pϵ
Z,Y
p Y(y)
(cid:20) p (y|z)(cid:21) (cid:20) (cid:18) p (y) (cid:19)(cid:21)
= E log Y|Z + E log 1−ϵ+ϵ Y (40)
(z,y)∼pϵ
Z,Y
p Y(y) (z,y)∼pϵ
Z,Y
p Y|Z(y|z)
(cid:20) p (y|z)(cid:21) (cid:20) p (y|z)(cid:21)
=(1−ϵ) E log Y|Z +ϵ E log Y|Z (41)
(z,y)∼pZ,Y p Y(y) yz ∼∼ pp YZ p Y(y)
(cid:20) (cid:18)(1−ϵ)p (y|z)+ϵp (y)(cid:19)(cid:21)
+ E log Y|Z Y (42)
(z,y)∼pϵ
Z,Y
p Y|Z(y|z)
(cid:18) (cid:20) (cid:18)(1−ϵ)p (y|z)+ϵp (y)(cid:19)(cid:21)(cid:19)
=(1−ϵ) I(Z;Y)+ E log Y|Z Y (43)
(z,y)∼pZ,Y p Y|Z(y|z)
(cid:32) (cid:20) p (y|z)(cid:21) (cid:20) (cid:18)(1−ϵ)p (y|z)+ϵp (y)(cid:19)(cid:21)(cid:33)
+ϵ E log Y|Z + E log Y|Z Y (44)
yz ∼∼ pp YZ p Y(y) yz ∼∼ pp YZ p Y|Z(y|z)
(cid:32) (cid:34) pϵ (y|z)(cid:35)(cid:33) (cid:34) pϵ (y|z)(cid:35)
=(1−ϵ) I(Z;Y)+ E log Y|Z +ϵ E log Y|Z (45)
(z,y)∼pZ,Y p Y|Z(y|z) yz ∼∼ pp YZ p Y(y)
(cid:104) (cid:105)
=(1−ϵ)(I(Z;Y)+H(Y|Z))+ E logpϵ (y|z) +ϵH(Y) (46)
Y|Z
(z,y)∼pϵ
Z,Y
(cid:104) (cid:105)
=(1−ϵ)(I(Z;Y)+H(Y|Z))+ E logpϵ (y|z) (47)
Y|Z
(z,y)∼pϵ
Z,Y
+ϵ(I(Z;Y)+H(Y|Z)) (48)
(cid:104) (cid:105)
=I(Z;Y)+H(Y|Z)+ E logpϵ (y|z) . (49)
Y|Z
(z,y)∼pϵ
Z,Y
17Table2: Top-5/classaverageclassificationaccuracyforvariousdatasets. Comparisonoffourbaseline
lossfunctionstosofttargetInfoNCE.
ImageNet TinyImageNet CIFAR-100 CellTypeGraph
NLL 94.95 93.84 98.37 80.57
Softtargetcross-entropy 96.46 94.91 98.40 81.41
InfoNCE 95.43 93.67 98.40 80.00
SoftdistributionInfoNCE 96.40 95.06 98.78 79.97
SofttargetInfoNCE 96.43 94.91 98.51 81.27
top-5acc. top-5acc. top-5acc. class-avg. acc.
Tosimplifynotationwedefine
(cid:104) (cid:105)
H (Y|Z):= E −logpϵ (y|z) . (50)
ϵ Y|Z
(z,y)∼pϵ
Z,Y
Inserting(49)backinto(38),weobtain
I(Z;Y)+H(Y|Z)−H (Y|Z)≥logN −lSDInfoNCE(T). (51)
ϵ
Nextweshowthatthel.h.s. of(51)islargerorequaltozero:
H (Y|Z)=H (Y,Z)−H(X) (52)
ϵ ϵ
≤H(Y)+H(X)−H(X)=H(Y). (53)
SinceH(Y)=I(Z;Y)+H(Y|Z),wehaveI(Z;Y)+H(Y|Z)−H (Y|Z)≥0.
ϵ
A.4 Experiments
A.4.1 Classificationaccuracy
Table2showstop-5classificationaccuracyforImageNet,TinyImageNet,CIFAR-100andclass-
averageaccuracyforCellTypeGraphforallbaselines.
Hereweprovidefurthertechnicaldetailsforourclassificationexperiments.
Imageclassification. Table3detailsthesettingsusedacrossallbaselines. Asclassificationhead
weuseda3-layerMLPwithhiddendimension4096. OnImageNet,weusedasbaselearningrate
of0.00055forInfoNCE-typelossfunctions,and0.0003forcross-entropy-typebaselines. OnTiny
ImageNet,weused0.00025forInfoNCE-typebaselines,and0.0003forcross-entropy-typebaselines.
On CIFAR-100, we used 0.0003 for InfoNCE, soft distribution InfoNCE and cross-entropy-type
baselines,and0.00055forsofttargetInfoNCE.Note,wedetermined(base)learningratesbasedona
searchovermultiplesamplesfromtheinterval[0.00005,0.003].
Node classification. Besides the parameter settings we described in the main experiments, all
DeeperGCNmodelsweretrainedwithadjacencydropout(dropoutprob. of0.1),addingofedges
(ratioof0.1),nodefeaturedropoutandmasking(prob. of0.1),randomnormalnoise(σof0.1)and
randombasistransforms.
A.4.2 Ablationexperiments
Confidencecalibration. Figure6showsthereliabilitydiagramofthebestperformingViT-B/16
modelsonTinyImageNet.
18Table3: ParametersettingsusedtotrainViT-B/16modelsacrossallbaselines.
parameter value
optimizer AdamW
weightdecay 0.05
layer-wiselrdecay 0.65
batchsize 1024
learningrateschedule cosinedecay
warmupepochs 5
trainingepochs 100
labelsmoothing 0.1
MixUp 0.8
CutMix 1.0
augmentations RandAug(9,0.5)
Figure6: ReliabilitydiagramofthebestperformingViT-B/16modelsonTinyImageNet,trained
withsofttargetlossfunctionsincombinationwithlabelsmoothing,MixUpandCutMix.
19