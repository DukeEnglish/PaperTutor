Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses
InheeLee ByungjunKim HanbyulJoo
For Teaser (light) SeoulNationalUniversity
{ininin0516, byungjun.kim, hbjoo}@snu.ac.kr
https://snuvclab.github.io/gtu/
Occluded partial observations
Novel View Synthesis Novel Pose Synthesis
Input: Monocular Video Output: Animatable Person 3D-GS
Figure1.Wepresentamethodtoreconstructdynamicscenesfromamonocularvideocapturingpartial2Dobservations.Asakeyadvantage,
ourmethodcanestimatetheunseenbodypartsbyleveragingapre-traineddiffusionmodel[42]viaSDSmethod[39].Thereconstructed
scenes can be rendered to any viewpoint and each human body can be transformed into any body posture controlled by SMPL [29]
parameters.
Abstract observations.Afterreconstruction,ourmethodiscapableof
notonlyrenderingthesceneinanynovelviewsatarbitrary
timeinstances,butalsoeditingthe3Dscenebyremoving
In this paper, we present a method to reconstruct the
individual humans or applying different motions for each
worldandmultipledynamichumansin3Dfromamonocu-
human.Throughvariousexperiments,wedemonstratethe
larvideoinput.Asakeyidea,werepresentboththeworld
qualityandefficiencyofourmethodsoveralternativeexist-
andmultiplehumansviatherecentlyemerging3DGaussian
ingapproaches.
Splatting(3D-GS)representation,enablingtoconveniently
and efficiently compose and render them together. In par-
ticular,weaddressthescenarioswithseverelylimitedand
1.Introduction
sparseobservationsin3Dhumanreconstruction,acommon
challengeencounteredintherealworld.Totacklethischal- The process of digitizing our world in 3D necessitates
lenge,weintroduceanovelapproachtooptimizethe3D-GS thereconstructionofnotonlystaticenvironmentalelements
representationinacanonicalspacebyfusingthesparsecues but also dynamic objects, particularly humans. Given the
inthecommonspace,whereweleverageapre-trained2D limitedavailabilityofmulti-viewcamerasetups,aground-
diffusionmodeltosynthesizeunseenviewswhilekeepingthe breaking leap can be achieved by developing a 4D recon-
consistencywiththeobserved2Dappearances.Wedemon- structionmethodcapableofrenderingthescenesatnovel
strateourmethodcanreconstructhigh-qualityanimatable viewsandarbitrarytimesjustusingamonocularvideoin-
3Dhumansinvariouschallengingexamples,inthepresence put.Reconstructingstaticcomponents(e.g.,buildings)from
ofocclusion,imagecrops,few-shot,andextremelysparse monocularvideobenefitsfromthewell-establishedmulti-
1
4202
rpA
22
]VC.sc[
1v01441.4042:viXraview geometry principles [13], where epipolar geometry incorporatingSDSlosswithadiffusionmodelandtextual
constraintsacrossdifferentviewsstillholdatdifferenttimes. inversion;and(3)introducingefficient4Dscenereconstruc-
Recentadvanceshavefurtherenhancedthequalityofthese tionandeditingpipeline.
reconstructionsbyleveragingimplicit3Drepresentations,as
demonstratedbyNeRF[33],NeuS[52],andGaussianSplat- 2.RelatedWork
ting(3D-GS)[20],resultinginmorerealisticrenderings.
Human Reconstruction from Monocular Video. There
However,thesameapproachisnotdirectlyapplicableto
has been a series of approaches [16,18,38,54,58,62,62]
dynamicallymovingcomponents,specificallyhumans.Early
to reconstruct 3D human avatars from a monocular video
workaddressesthisproblemwithinthecontextofgeneral
capturing moving humans. Mostly, they tackle the prob-
non-rigidstructure-from-motionapproaches[1,37].More
lem by finding the correspondences between each frame
recent breakthroughs leverage human prior models, such
andoptimizingtheminacommoncanonicalspace.Tofind
as SMPL [29,57], as a way to canonicalize the non-rigid
thecorrespondencesacrosstheframes,diversepriorknowl-
observations from multiple views of the monocular video
edge is leveraged such as parametric model [2,29], pixel-
andtransformbackintotheposedspaces[7,8,12,17,54].
alignedfeatures[35],oropticalflow[60].Afterthesuccess
Yet,theseapproachesoftenassumethescenarioswhere ofNeRF[33],recentmethods[16,38,54,62]useNeRFand
the camera focuses on the human subject, capturing their its variants to reconstruct a human by leveraging a para-
entirebodywhilethetargetpersonrevolvesaroundthecam- metric model SMPL [29]. HumanNeRF [54] and SelfRe-
era’s field of view. While this approach is suitable for in- con[16]improvethereconstructionqualitybycorrecting
tentionallydigitizingaspecificindividual,theyencounter theinaccuratecanonicalizationoriginatedbynon-rigidde-
substantialchallengesinin-the-wildvideoscenarios,where formation. Vid2Avatar [12] and Neuman [18] reconstruct
humansarecapturedinpartial,occluded,cropped,andsparse ahumanwithoutamaskbylearningabackgroundjointly.
observedconditions.SeetheexamplesshowninFig.1and InstantAvatar [17] reduces the required time of optimiza-
Fig.4.Moreover,reconstructingandrenderingmultiplein- tionfromafewhoursintoaminuteleveragingiNGP[34].
dividuals along with 3D backgrounds within the existing OccNeRF[56]proposesamethodthatcanreconstructpeo-
approachesisnon-trivial,mainlyduetothecomplexitiesof pleevenwithocclusion,usingsurface-basedrenderingand
integratingmultipleneuralradiancefieldmodels[36,63]. visibilityattention.However,unlikeourmethod,allofthe
In this paper, we present a method to reconstruct both aboveexceptOccNeRFassumethepersonisnotoccluded
thestaticworldandmultipledynamicallymovinghumans andmostofthebodyisshowninthevideo,whichisrarein
in3Dfromamonocularvideoinput,specificallyfocusing in-the-wildvideos.
onscenarioswithextremelylimitedandsparseobservations. Diffusion on 3D Tasks. After the recent breakthroughs
Toaddressthischallenge,werepresentboththeworldand of diffusion models on image generation task [14], sev-
multiplehumansinacommonGaussiansplatting3Drepre- eralmethodssuggestawaytousediffusionmodelon3D
sentation[20].Ourapproachallowstoefficientlycompose tasks[9,25,28,39,47,51,66].Forexample,RGBD2[25]
them for novel view rendering or scene editing. Notably, trainsanRGB-Ddiffusionmodeltocompletetheunobserved
to tackle the scenarios with extremely limited and sparse areaofaroomusingdiffusioninpaintingapproach[30]and
observations in 3D human reconstruction, we introduce a DiffuStereo[47]trainsdiffusion-stereonetworkforhigher
novelapproachtooptimizethe3DGSrepresentationina reconstructionqualityinsparsemulti-viewsettings.Inpar-
canonical space by fusing the sparse cues in the common ticular,theSDS[39]methodwhichleveragesapretrained
space.Asacoreidea,weleverageapre-trained2Ddiffusion text-to-imagediffusionmodel[45]hasbeenappliedwidely,
model,equippedwithTextureInversion[10],tosynthesize suchastext-to-3D[39,51,53]orsingleimage-to-3Dgen-
unseen views without losing the consistency with the ob- eration tasks [27,28,32]. However, the vanilla SDS loss
served2Dappearances[39,42].Viathoroughevaluations, is not 3D consistent itself and prone to artifacts like the
wedemonstratethatourapproachsuccessfullyreconstructs Januseffect.ToimprovetheSDSloss,manytechniquesare
high-qualityanimatable3Dhumanavatarsofdynamically proposedsuchasleveraging3Dprior[28],fine-tuningdiffu-
movingindividualsfromsparseandpartialobservations.The sion[53],givingbetterconditions[32]andusingadvanced
animatablenatureofour3Dhumanreconstructionoutputs optimizationschemes[5,6,6,27].SDSlossisalsoapplied
enables us to replay the observed motions of the humans tomakebetter3Davatarsrecently[15,26].However,there
in novel views and edit the postures of the humans with hasbeennotrialoncompletinganimperfectreconstruction
arbitrarymotionsaswell.Ourcontributionissummarized ofahumanwithadiffusion.
as:(1)representingbotha3Dworldandmultiplehumans CompositionalHuman-SceneReconstruction.Separating
inacommon3DGSrepresentationforefficientcomposing 4Dscenesintostaticbackgroundsanddynamicobjectsisa
andrendering;(2)reconstructingandcanonicalizinganimat- commonapproachinthe4Dreconstructionproblem.The
able3Dhumansfromsparseandpartial2Dobservationsby static-dynamicseparationcanbedonebypriorknowledge
2onthetargets[24,36],suchascarsandpedestriansaredy-
namic, or can be performed automatically by minimizing
a certain energy term [55]. Recent human reconstruction
methodsalsousecompositionalapproachestoreconstruct
ahumanfromvideos[12,18,48,63].Formonocularrecon-
struction,Neuman[18]andVid2avatar[12]usetwoseparate
implicitfunctionseachofwhichrepresentsbackgroundand
personrespectively. While theyallowthe modeltorecon-
structapersonregardlessoftheperson’smaskquality,these
Figure2.Methodoverview.Overviewofourpipeline.(Sec.4).
modelscannothandleocclusionandmultiplepeopleatonce.
Inamulti-viewsetting,Shuaietal.[48]tacklesmoreprac-
α-blendedrenderingasfollows:
ticalsituationswheremultiplepeopleinteractwithobjects.
Itmodelseachperson,object,andbackgroundwithaNeu- i−1
(cid:88) (cid:89)
ralBody[38]andNeRF[33]respectivelyandrendersthem C(p)= c iα i (1−α j), (3)
compositionallythroughST-NeRF[63]pipeline.However, i∈N j=1
itrequires8multi-viewvideosasinput,whichisunavailable α =g2D(p)·o , (4)
i i i
inin-the-wildsituations.
whereg2D istheweightafterthe2Dprojectionof3DGaus-
i
siang totheimageplane,andweusetheJacobianofthe
i
3.Preliminaries
affineapproximationoftheprojectivetransformation,fol-
lowingpreviousapproaches[20,68].Astheoutputof3D
In our method, we use 3D Gaussian Splatting [20] to
scenereconstruction,weobtaintheparametersof3DGaus-
represent 4D scenes, and use Score Distillation Sampling siansG ={G }M byoptimizingthemwithreconstruction
(SDS)asatooltoestimateunseenhumanbodyparts.Here, i i=1
losswhichiscalculatedfromtherenderingEq.(3).
weprovideanoverviewofthesepreliminaryconcepts.
3.2.ScoreDistillationSampling
3.1.3DGaussianSplatting Score Distillation Sampling (SDS) method [39] is an
approachthatleveragesthepriorknowledgeunderlyingtext-
3DGaussianSplatting(3D-GS)isanexplicit3Drepre-
to-image (T2I) diffusion models to generate 3D content.
sentation to model a radiance field of a static 3D scene
SDSoptimizesanydifferentiable3DrepresentationΘby
with a set of 3D Gaussians and their attributes [20]. A
aligningrenderedoutput{I}fromarbitraryviewstobeon
3D static scene can be modeled by a set of 3D Gaus-
thedistributionofdiffusionmodelϕ.Thiscanbeachieved
sians {G }M where the i-th Gaussian is represented by
i i=1 byminimizingtheresidualbetweennoiseϵ,whichperturbs
G = {µ ,q ,s ,c ,o }, where µ ∈ R3 is the Gaussian
i i i i i i zintoz ,andpredictednoiseϵ(z ;y,τ)wherezisalatent
center,s∈R3 andq ∈SO(3)arerespectivelythescaling τ τ
of{I}encodedbyVAEoflatentdiffusionmodel[43].By
factorandtherotationrepresentedinquaterniontodefine
omittinggradientthroughdiffusionmodelϕ,theSDSloss
thecovariancematrixΣ∈R3×3,c ∈R3isthecolor,and
i canbewrittenasfollows:
o ∈ R is opacity. For a 3D query location x ∈ R3, its
i (cid:20) (cid:21)
∂z ∂I
Gaussianweightg(x)isrepresentedas: ∇ L =E ω(τ)(ϵ (z ;y,τ)−ϵ) , (5)
Θ SDS t,ϵ ϕ τ ∂I ∂Θ
g(x)=e− 21(x−µ)TΣ−1(x−µ), (1) where z τ is a noised latent with time step τ and y is text
promptconditioningdiffusionmodelϕ.Theω(τ)denotes
where the symmetric 3D covariance matrix Σ ∈ R3×3 is theweightingfunctiondefinedbytheschedulerofthediffu-
sionmodel.
representedby
4.Method
Σ=RSSTRT. (2)
4.1.Overview
R=quat2rot(q)isarotationmatrixconvertedfromq,and Our model reconstructs dynamically moving multi-
S =diag(s)isadiagonalmatrixfromscalingfactors. humans and the static background jointly from a casually
3D-GSrasterizesthese3DGaussians{G }M bysorting captured monocular video. Our system takes, as input, T
i i=1
themindepthorderincameraspaceandprojectingthemto framesofimages{I } withcorrespondingcamerapa-
t t=1...T
the image plane. If N number of Gaussians are projected rameters{P } ,andoutputsthe3Dscenesintherep-
t t=1...T
on 2D location p ∈ R2, the pixel color C(p) is given by resentationof3DGaussianSplattingGBGand{Gh} ,
j j=1...N
34.2.InitializingandDensifyingGaussians
To represent the background and humans via 3D-GSs,
weinitializeeachrepresentationviatheavailablecues.The
backgroundGaussiansGBGareinitializedwithpointcloud
obtainedbyStructure-from-Motion(SfM)[46].Inthecases
of a fixed camera input where SfM cannot be applicable,
we assume that the background is a large 3D sphere with
background texture, centered on the mean position of hu-
mans.Werepresentaj-thhumanvia3D-GSinacanonical
space(denotedassubscriptc),Gh ,whichisinitializedby
j,c
Figure 3. Failure examples of optimizing 3D-GS naively. (a) theverticesofA-posedSMPLmeshV(β ,θ ),whereβ
j c j
showsthatnaivelyoptimizing3D-GSsuffersfromartifactsshaped
andθ aretheSMPLshapeandcanonicalposeparameters
c
likeahedgehoginunseenviewandinputview.(b)showsthatour
respectively,regressedusingamonocular3Dposeregres-
SDSlosseffectivelyremovestheartifactsobservedinbothinput
sor[40,49].Thecolorandopacityaresetingreyand0.9
andunseenviews.
respectively.WeassumetheSMPLshapeparameterβ from
j
whereGBGistorepresentthe3DbackgroundandGhisfor theposeregressorisfixedforeachhumanwhiletraining.
j
thej-thhuman. Tocapturethefinedetailsofthebackgroundandhuman,
Importantly, we represent the individual human in a wedensifytheinitialGaussiansadaptively[20]everyN
den
canonical space mapped to the rest pose (or A-pose) of iteration.TheGaussianstobedensifiedarechosenbasedon
SMPLmodel[29],whichcanbetransformedtoany“posed” theaccumulatedgradients∇µ ,bysummingthegradient
i
space parameterized by SMPL pose parameter θ ∈ R72. on the center of Gaussians µ computed in each iteration.
i
(cid:80)
Thus,theappearanceofthej-thhumanattimetcanberep- If the accumulated gradients ∇µ is bigger than a
Nden i
resentedasGh(θ )byinputtingthecorrespondingSMPL predefinedthreshold,wedensifytheGaussianG bycloning
j j,t i
parametersθ forj-thhumanattimet.Notethatwithin orsplittingit.
j,t
ourrepresentation,thepostureofeachindividualcanbecon-
4.3.CanonicalizingDynamicHumans
trolledindependentlyfromothersceneparts.Thisprovides
ustheflexibilitytoeditpeople’sbodymotionsusingvarious To fuse the cues of an individual with different poses
availablemotioncapturedata[31]. acrossdifferentframes,wemodeleachpersonwithasingle
Sincewebuildbothdynamichumansandbackgrounds canonicalmodelGh (forbrevity,wedropthepersonindex
j,c
inthesame3D-GSrepresentation,wecaneffectivelyren- j inthesubscript).Wealsomodelthedeformationfunction
derthewholescenebycompositing3D-GSrepresentations, Gh(t)=Fh(Gh,θ )whichtransformsGaussiansincanon-
d d c t
GAll ={GBG}∪{G jh} j=1...N,wherewecanusethesame ical into posed G dh(t) at time t, following the SMPL pose
renderingfunctionEq.(3)withoutanymodification.This parameterθ .OurdeformationfunctionFh :R3 →SE(3),
t d
showsthemajoradvantageoverthealternativeapproaches whichmapscanonicalspaceintoposedspace,isdefinedvia
such as NeRF-based representation [12,54], where com- theLinearBlendSkinning(LBS)basedontheSMPL[29].
positingmultiplehumansisnottrivial.Ourmethodismuch IttranslatesthecenterofGaussianx androtatesthecovari-
i
moreconvenientandefficient,showingmuchfasterrender- ancematrixΣ asfollows:
i
ingspeed(e.g.,40times)thanthecompetingapproachesas
demonstratedinourexperiments. N (cid:88)joint
x = w (x )(R x +T ), (6)
Notably,wemainlyfocusonreconstructingthe3Dhu- i,p k i,c k i,c k
mansfromsparsemonocularobservations,inthepresenceof k=1
severeocclusions,croppedviews,andfewshots,whichare Σ i,p =R weiΣ i,cR wT ei, (7)
commonlyobservableinthewild.Weaddressthischallenge
whereR andT arerotationandtranslationofkth joint
byfusingtheobservedcuesintothecanonicalspaces,for k k
ofSMPLskeletonwhichiscomputedfromθandβ .The
whichweintroducethetransformationbetweentheposed j
R isaderivationofLBSequaltotheweightedsumof
spacetothecanonicalspace,whileweleverage3D-GSrepre- wei
rotations{R }Njoint asfollows:
sentation(Sec.4.3).Asacorecontribution,wealsopresenta k k=1
solutiontoinclude2Ddiffusionpriorasawaytosynthesize
Njoint
themissingandunobservedpartoftargethumanwhilekeep- (cid:88)
R = w (x )R . (8)
ingtheconsistencytotheobservedparts(Sec.4.4),where wei k i,c k
k=1
wefurtherenhancethequalitybyincorporatingTextureIn-
versiontechnique[23]tobetterpreservethetargetidentity. Theskinningweightw (x)iscalculatedfromthealigned
k
TheFig.2showstheoverallpipelineofouroptimization. SMPL vertices defined in canonical space. Here we get
4w (x) by summing the skinning weight of the 30 nearest theconsistenthumanappearanceatthevirtualviewpoints.
k
verticeswithInverseDistanceWeight(IDW).Toaccelerate Toincorporatethisidea,weleveragetheTextualInversion
rendering speed, we pre-calculate and store the skinning (TI) by finding the text-embedding specific to our target
weight in a voxel grid, similar to SelfRecon [16]. In each human[10,44].Here,weuseCustomDiffusion[23]toin-
renderingtime,weobtaintheskinningweightbytrilinear vertobservationsofthetargetindividualintothetexttoken
interpolation on the weight grid instead of searching the <person-j>, where we collect the images of the target
nearestSMPLvertices. humanfrominputframesbycroppingandmaskingthetarget
persononly.TogetherwithTextualInversion,CustomDiffu-
4.4.Diffusion-GuidedReconstruction
sionfine-tunestheattentionandtextembeddinglayerofthe
diffusionϕwhichmakesaperson-specificdiffusionϕ .By
Weemploythe2Ddiffusionprior[42]inoptimizing3D- j
addingtheinvertedtext-tokentothetext-prompt,wecanget
GStorepresentahumanmodel,asakeyideatoovercome
diffusion-generatedimagesconsistentwiththeobservations.
sparseobservationsforthetargethumanininputvideos.The
Weperformthediffusionfine-tuningandTextualInversion
intuition behind our approach is that the quality of a 3D
per person separately and apply the subject-specific fine-
humanmodelcanbemeasuredbyassessingtherealismof
tunedversionofSDSforeachtargetperson.
therenderedimagesinnovelviews.Forexample,naively
optimizing3DGaussiansG fromsparseandoccludedviews Furthermore, we also utilize the OpenPose Control-
Net [64] to align the body pose of a person generated by
resultsinartifactsormissingparts,asshowninFig.3(a).
diffusionandourpersonGaussiansGh.Forthis,whenwe
Theuseofthediffusionmodel,particularlytheSDSloss[39],
compute the SDS loss, we project the 3D SMPL joints
canbebeneficialtoimprovingthequalityofthedesired3D
J (θ,β ) into the viewpoint, convert to OpenPose [4]
modelbyenforcingrealismintherenderedimagesatnovel smpl j
format,andquerythemintotheControlNet.Weadditionally
views.TheSDSlosscanbeconsideredasadditionalvirtual
addaview-augmentedlanguageprompt[39]forstableopti-
camerasguidedbythepre-trained2Ddiffusionmodel[42].
Foreachiteration,wemakerenderingRhofthetargethu- mization.Wesamplethenoisetime-stepτ fromU[0.2,0.98]
v
man’s3D-GSGhwithavirtualcameravwhichisrandomly for the first 2000 iterations and then we decay the maxi-
mumtime-stepfrom0.98to0.3for2000iterations.Alsoto
sampledfromaspherethatiscenteredoneachhumanand
enhancethefinedetailsofthebody,werandomlysample
viewing its body. To give more diversity to the rendering,
camerav whichzoomsintheface,lowerbody,andupper
wealsorandomlysamplethebodyposeθofthepersonand j
transformthe3D-GSGhintotheposedspace.Werandomly body.Refertooursupp.mat.formoredetails.
sampleθeitheramongobservedposesorthecanonicalA-
4.5.TrainingObjectives
pose(θ ),whichcanbewrittenas{θ } ∪{θ }.With
c t t∈[1...T] c
therenderedimageRhatviewpointv,wecomputetheSDS Foreveryiteration,werendertheimageR tcorresponding
v
loss[39],which isproportionaltothedifference between toframetandcalculateMSEloss,SSIMloss,andLPIPS
addednoiseϵandestimatednoiseϵ ϕbydiffusionmodelϕ: lossbycomparingitwiththegroundtruthimageI t:
∇L =E (cid:104) ω(τ)(ϵ (z ;y,τ)−ϵ) ∂z ∂R vh(cid:105) (9) L recon =λ rgbMSE(R t,I t)+λ ssimSSIM(R t,I t)
SDS t,ϵ ϕ τ ∂Rh ∂Gh +λ LPIPS(R ,I ). (10)
v lpips t t
, where z τ is a noised latent of rendering R vh, τ ∈ [0,1] ThenwecomputeSDSLossL SDS foreachperson’sGaus-
is a time-step of noise and y is conditions applied on the siansG jhwithperson-specificdiffusionϕ j:
diffusionmodel.TheSDSlossmitigatestheartifactsinour
3D-GShumanmodelbyenforcingtherenderedoutputRh (cid:88)N
v L = L (Gh,ϕ ). (11)
inanovelposetobeplausible. sds SDS j j
j=1
TextualInversiononSDS. Wefurtherimprovetheeffi- To avoid transparent artifacts, we additionally add hard-
cacyofourSDSmethodbyleveragingtheconceptofTexture surfaceregularizationonhumanrenderingfollowingLOL-
Inversion(TI)[10],asawaytomaketheSDSlosstosyn- NeRF[41]:
thesizethehumanappearancesimilartoourtargetidentity,
L =−log(exp−|α|+exp|α|)+const. (12)
rather than generating arbitrary appearance. Applying the hard
SDSlossonlywithtextprompts,suchas“Aphotoofaper- ,whereαistherenderedalphamapofpersonGaussianGh.
j
son”,mayeasilyconvergetoarandomhumanappearance
Ourfinaltrainingobjectiveisasfollows:
due to the diversity of diffusion prior model [42]. Ideally,
wewanttospecifythetargetindividualobservedfromour L =λ L +λ L +λ L . (13)
tot recon recon sds sds hard hard
inputimages,toencouragethediffusionmodeltosynthesize
5Input View (a) HumanNeRF (b) ST-NeuralBody (c) Ours (d) Ground Truth
Blue : Novel View1
Green : Novel View2
Red : Train View
Blue : Novel View1
Green : Novel View2
Red : Train View
Figure4.NovelviewsynthesizedresultsofPanopticDataset[19]
5.Experiments singlesideofthepeopleisvisibleanduseallother7views
asnovelGTviewsfortheevaluation.Weusetheprovided
Weperformrigorousquantitativeandqualitativeevalua-
pseudo-GTSMPLparameters.
tionstoshowthestrengthsofourmethod.Wefirsttestour
SingleHumanBenchmarksWealsoconductexperiments
methodonchallengingscenescapturingmultiplepeoplewith
on an existing single human benchmark ZJU-Mocap [38]
sparse2Dobservations,toshowthemajoradvantageofour
dataset,tochecktheperformanceofourmodelonthesingle
method.Wealsoapplyourmethodtoexistingsinglehuman
humanreconstructiontaskwithsufficientobservations.We
reconstructionbenchmarks.Additionally,byperformingab-
followtheevaluationpipelineusedinbaselines[17,54].
lationstudies,wedemonstratetheefficacyofeachmodule
within our pipeline. We also show the computational effi- 5.2.BaselineandEvaluationMetrics
ciencyofourmethodbycomparingtherenderingtimeto
We compare our model with three methods, Human-
theexistingmethod.
NeRF[54],InstantAvatar[17]andShuaietal.[48].Hu-
manNeRF[54]showsSOTAqualityonthemonocularhu-
5.1.Dataset
man reconstruction task. As HumanNeRF cannot handle
PanopticDataset[19]Thisdatasetcapturessociallyinter- multiplepeopleatonce,weoptimizeitseparatelyforeach
actingmultipleindividualsviaamulti-viewcamerasystem. personandmergetheminthefinalevaluation.Wegetaren-
Wesimulatethesparse2Dviewscenariobychoosingasin- deringofeachindividualseparatelyandaccumulatethemin
glecameraviewastheinputforthereconstructionandusing anαblendingmanner.Theorderofaccumulationisdeter-
otherviewsforGTviews.Weuseanultimatumsequence minedbythedistanceoftheSMPLpelvisfromthecenter
with6people(540framesinultimatum160422sequence), ofthecamera.BecauseHumanNeRFrequiresaforeground
andchooseanHDcameraviewinacommonviewangleas maskforprocessing,weuseGTmasksifavailable(Hi4D
the input video. We select other 7 HD cameras in diverse andZJU-Mocap),oruseanoff-the-shelfmethod[22].In-
novelviewpointsastheGTsfortheevaluations,asshown stantAvatar[17]isthemostefficientmethodtoreconstruct
inFig.4.Asapre-processing,wefitSMPLmodelsonthe ahumanfromamonocularvideowithhighquality.Weeval-
provided pseudo-GT 3D skeletons of the dataset using a uatetheInstantAvatarusingthesamepipelineappliedfor
pose-prior [3]. See the supp. mat. for more details of pro- HumanNeRFevaluation,asit’sonlycapableofmono-human
cessing. cases.Shuaietal.[48]isacompositionalmethodsimilarto
Hi4D[61]Thisdatasetcontainsmultipleindividualsatclose ours,whichoptimizesimplicitfunctionsofpeopleandback-
distances,whicharecapturedwithsynchronized8cameras. groundfromsparseviewinput[48].Itrepresentseachperson
Weconsiderchallengingtwosequencespair00-dance withNeuralBody[38]andbackgroundwithNeRF[33]and
andpair01-hug.SimilartoPanopticDB,wechoosethe rendersthemtogetherbyusingthecompositionalrendering
video from a camera (camera 76) as input where only a pipelineofST-NeRF[63].Differentfromtheoriginalpaper,
6
1weiV
levoN
2weiV
levoN
1weiV
levoN
2weiV
levoNMethods PSNR↑ SSIM↑ LPIPS*↓ Method PSNR↑ SSIM↑ LPIPS*↓
HumanNeRF[54] 19.59 0.6514 38.69 HumanNeRF[54] 30.23 0.9554 3.36
InstantAvatar[17] 15.03 0.4163 65.95 InstantAvatar [17] 28.55 0.9282 10.60
Shuaietal. [48] 15.79 0.8370 25.77
OurswoL 30.10 0.9529 4.68
SDS
Ours 23.60 0.8323 25.41 Ours 30.13 0.9535 4.50
Table 1. Quantitative results on Panoptic dataset. LPIPS* = Table 3. Quantitative results on 6 subjects in ZJU-Mocap
100×LPIPS.OurmethodshowsbetterperformanceinPSNRand dataset[38].LPIPS*=100×LPIPS.Ourmethodshowscompara-
LPIPSandcomparableresultsinSSIM. blequalitycomparedtoHumanNeRF[54]eveniftheobservation
issufficient.
pair00-dance pair01-hug
Method
PSNR↑ SSIM↑ PSNR↑ SSIM↑
HumanNeRF[54] 18.79 0.8552 21.40 0.8238
InstantAvatar[17] 18.60 0.8646 19.07 0.8254
Shuaietal.[48] 20.78 0.9165 19.72 0.9078
Ours 23.76 0.9328 25.14 0.9289
Table2.QuantitativeresultsonHi4Ddataset[61].Oursshow
betterperformanceonbothPSNRandSSIMinsituationswhen
peoplecloselyinteractlikehuggingordancingtogether
weoptimizeitusingonlyasingletrainview.
Wecomparethequalityofhumanrenderingbymasking
outthebackgroundofGTimages.Forevaluationmetric,we
usepeaksignal-to-noiseratio(PSNR),structuralsimilarity
(SSIM), and perceptual similarity (LPIPS) [65] following
Figure 5. Novel view synthesis output of Hi4D pair00-dance
thepriorwork[54].
sequence.WhileHumanNeRF[54]failstoreconstructaface,ours
synthesizesaplausiblefaceguidedbydiffusionmodel[42].(d)
5.3.ImplementationDetails
plotscamerapositionrelativetothefrontviewingfemalebody.As
Tostabilizeoptimization,wefirsttrainbackgroundGaus- shownhere,themajorityoftherenderedoutputshownherehas
sians GBG solely with background images where people beenneverobservedinthetrainview.
aremaskedout.WethenoptimizehumanGaussiansG
pi,c
withoutL forthefirst1000iterationsandthen,optimize showninthetable,ourmethodoutperformsthebaselinesin
SDS
humanGaussiansG cpi andbackgroundGaussiansG
bg
simul- PSNRandLPIPS.
taneouslytogetherwithL for10kiterations.Referto
SDS Hi4D dataset. We show the quantitative results in Tab. 2
oursupp.mat.formoredetails. andqualitativeexamplesinFig.5frompair00-dance
sequence.AsshowninTab.2,ourmethodoutperformsbase-
5.4.EvaluationsonMultiplePeopleReconstruction
linesinallmetrics.Interestingly,inthisdataset,somebody
Panopticdataset.Weshowthequalitativecomparisonbe- partsoftheindividualareneverobservedintheinputview
tweenourresultsandtheoutputsofbaselinesinFig.4.As due to the severe occlusions, e.g., the face of the female
shown,ourmethodreconstructstheappearancesofpeople personinFig.5,whereourmethod“hallucinates”realistic
eveninthepresenceofsevereocclusionsandimagecrop- humanfacewithoutanyobservations.
ping.Incontrast,bothHumanNeRF[54]andShuaietal.[48]
failtoreconstructdetailsshowingnoticeableartifactssince 5.5.EvaluationsonSinglePersonReconstruction
manybodypartsarenotvisibleintheinputview.Inpartic-
ular,HumanNeRFsuffersfromsevereocclusionsbecause We show the quantitative comparisons on ZJU-
itrequiresaccurateforegroundmaskscontainingwholehu- Mocap[38]dataset.AsshowninTab.3,ourmethodshows
manshapes,whichcannotbeobtainedduetotheoccluder comparableperformanceoverbaselineswhensufficient2D
infrontofthetargetindividual.Incontrast,ourmethoddoes observationsareavailable.Thisresultdemonstratesthatour
notsufferfromsuchissues. pipelinebasedon3D-GSwithSDSlossdoesnotnegatively
WealsoquantifytheoutputqualitiesinTab.1byrender- affectthesingle-personreconstructionscenarioswhileshow-
ing the reconstructed scenes into unseen novel views. As ingitsmajorstrengthsinthecaseofsparseobservations.
7RenderingSpeed(FPS↑)
Method VRAM↓
512×512 1024×1024
InstantAvatar[17] 18.03 7.09 4972MiB
Ours(15k) 361.01 277.01 1496MiB
Table5.Novelposerenderingspeed.Wecomparednovelpose
renderingspeedbetweenInstantAvatar[17]andourswithZJU-
Mocap[38]377subject.Itshowsthatourmethodconsistsofa15k
Gaussiansrendersfasteronbothlowandhighresolutions,with
lessVRAMconsumption.
5.7.RenderingEfficiency
Weshowthecomputationalefficiencyofourmethodby
comparing the rendering time of novel pose synthesis to
Figure6.Ablationstudies.From(a)showsthatusingonlyrecon- InstantAvatar[17],whichisknownasthemostefficientex-
structionlosssuffersfromartifacts(likehedgehogs)eveninshown istingmethodandalsothefastestamongourcompetitors.All
regions.(b)showsthattextualinversionisessentialtogenerate reportedtimesherearemeasuredwithasingleGeForceRTX
contextuallysimilarappearancesonunseenregions.
3090GPU.Bytakingadvantageof3D-GSrepresentations,
ourmethodachievesmorethanreal-timerenderingspeed
with300FPSon1K×1Kimages.AsshowninTab.5,our
w/TextualInversion w/LSDS w/Lrecon PSNR↑ SSIM↑ LPIPS*↓
methodsurpassesInstantAvatar[17]bothinrenderingspeed
✓ ✓ ✓ 26.03 0.9435 7.00 andmemoryconsumptionwhileshowingbetterrendering
✓ ✓ 23.43 0.9342 8.36
✓ 24.51 0.9323 10.54 qualityasshowninTab.3.
Table 4. Ablation Studies with lower-body occluded ZJU-
Mocap[38]377subject.LPIPS*=100×LPIPS.Asshown,SDS 6.Discussion
losswithtextualinversiontokenshowsthebiggestimprovement.
Inthispaper,wepresentamethodtoreconstructtheworld
anddynamicallymovinghumansin3Dfromamonocular
5.6.AblationStudies
videoinput,particularlyfocusingonsparseandlimitedobser-
vationscenarios.Werepresentboththeworldandmultiple
Weconductanablationstudytodemonstratetheimpor- humansvia3DGaussianSplattingrepresentation,enabling
tanceoftheproposedmodulesofourframework.Asaway ustoconvenientlyandefficientlycomposeandrenderthem
for the quantitative evaluations, we use ZJU-Mocap [38] together. We also introduce a novel approach to optimize
datasetandsimulateachallengingscenarioby(1)completely the3D-GSrepresentationinacanonicalspacebyfusingthe
occludingthelowerbody,and(2)usingafewframesonlyfor sparsecuesinthecommonspace,whereweleverageapre-
theinput(10%offramesfromtheZJU-Mocap377subject). trained2Ddiffusionmodeltosynthesizeunseenviewsby
Anexampleoftheartificiallyoccludedimagesisshownin keepingtheconsistencywiththeobserved2Dappearances.
Fig.6withtestresults. Viathoroughexperiments,wedemonstratethehighperfor-
manceandefficiencyofourmethodinvariouschallenging
OurFullmethodcansuccessfullyreconstructthewhole
examples.
partofthehumans.Interestingly,italsosynthesizesthecom-
pletelyunseenshortpantsandshoesofthetargetindividual, Ourapproach,however,stillhaslimitationssuchas:(1)
whiletheappearanceandcolorsaredifferentfromtheGT. SMPL fitting needs to be provided; (2) our method only
Asexpected,theoutputwithoutL failstoreconstruct considershumansasthedynamictarget,ignoringanimals,
SDS
theunseenlowerpart,andalsoshowspoorqualityonthe cars,orotherdynamicobjects;(3)thequalityofthesynthe-
upperbodyduetotheinsufficientimageframes.Theoutput sizedpartsarestilllimitedwithvisibleartifacts.Allthese
withoutTextureInversion(TI)reconstructstheunseenlower limitationscanbeexcitingfutureresearchdirections.
partsaswell,but,interestingly,theoutputappearanceisvery AcknowledgementsThisworkwassupportedbySamsung
differentfromtheGT.Thisresultdirectlydemonstratesthe ElectronicsC-Lab,NRFgrantfundedbytheKoreagovern-
importanceofourTextureInversionprocessinapplyingSDS ment(MSIT)(No.2022R1A2C2092724andNo.RS-2023-
loss,helpfulinpreservingtheidentityofthetargetindivid- 00218601),andIITPgrantfundedbytheKoreangovernment
ual.WealsoshowthequantificationresultsinTab.4,where (MSIT)(No.2021-0-01343).H.Jooisthecorrespondingau-
theimportanceofeachmoduleisalsoclearlydemonstrated. thor.
8References [17] TianjianJiang,XuChen,JieSong,andOtmarHilliges. In-
stantavatar: Learning avatars from monocular video in 60
[1] IjazAkhter,TomasSimon,SohaibKhan,IainMatthews,and
seconds. InCVPR,2023. 2,6,7,8,12
YaserSheikh. Bilinearspatiotemporalbasismodels. TOG,
[18] WeiJiang,KwangMooYi,GolnooshSamei,OncelTuzel,
2012. 2
andAnuragRanjan. Neuman:Neuralhumanradiancefield
[2] ThiemoAlldieck,MarcusMagnor,WeipengXu,Christian fromasinglevideo. InECCV,2022. 2,3
Theobalt,andGerardPons-Moll. Videobasedreconstruction [19] HanbyulJoo,TomasSimon,XulongLi,HaoLiu,LeiTan,
of3dpeoplemodels. InCVPR,2018. 2,12 LinGui,SeanBanerjee,TimothyScottGodisart,BartNabbe,
[3] FedericaBogo,AngjooKanazawa,ChristophLassner,Peter IainMatthews,TakeoKanade,ShoheiNobuhara,andYaser
Gehler,JavierRomero,andMichaelJ.Black. KeepitSMPL: Sheikh. Panopticstudio:Amassivelymultiviewsystemfor
Automaticestimationof3Dhumanposeandshapefroma socialinteractioncapture. TPAMI,2017. 6,12,13
singleimage. InECCV,2016. 6,13 [20] BernhardKerbl,GeorgiosKopanas,ThomasLeimku¨hler,and
[4] Z.Cao,G.HidalgoMartinez,T.Simon,S.Wei,andY.A. GeorgeDrettakis.3dgaussiansplattingforreal-timeradiance
Sheikh. Openpose:Realtimemulti-person2dposeestimation fieldrendering. InSIGGRAPH,2023. 2,3,4,12
usingpartaffinityfields. TPAMI,2019. 5 [21] Diederik P Kingma and Jimmy Ba. Adam: A method for
[5] EricR.Chan,KokiNagano,MatthewA.Chan,AlexanderW. stochasticoptimization. InICLR,2015. 12,14
Bergman,JeongJoonPark,AxelLevy,MiikaAittala,Shalini [22] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
DeMello,TeroKarras,andGordonWetzstein. Generative ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-
novel view synthesis with 3d-aware diffusion models. In head,AlexanderCBerg,Wan-YenLo,etal. Segmentany-
ICCV,2023. 2 thing. InICCV,2023. 6,13
[6] RuiChen,YongweiChen,NingxinJiao,andKuiJia. Fan- [23] NupurKumari,BingliangZhang,RichardZhang,EliShecht-
tasia3d: Disentangling geometry and appearance for high- man,andJun-YanZhu. Multi-conceptcustomizationoftext-
qualitytext-to-3dcontentcreation. InICCV,2023. 2 to-imagediffusion. InCVPR,2023. 4,5,13
[7] XuChen,TianjianJiang,JieSong,MaxRietmann,Andreas [24] AbhijitKundu,KyleGenova,XiaoqiYin,AlirezaFathi,Caro-
Geiger,MichaelJBlack,andOtmarHilliges. Fast-snarf:A linePantofaru,LeonidasGuibas,AndreaTagliasacchi,Frank
fastdeformerforarticulatedneuralfields. TPAMI,2023. 2 Dellaert,andThomasFunkhouser. PanopticNeuralFields:
ASemanticObject-AwareNeuralSceneRepresentation. In
[8] XuChen,YufengZheng,MichaelJBlack,OtmarHilliges,
CVPR,2022. 3
andAndreasGeiger. Snarf:Differentiableforwardskinning
foranimatingnon-rigidneuralimplicitshapes.InICCV,2021. [25] JiabaoLei,JiapengTang,andKuiJia. Rgbd2:Generative
2 scenesynthesisviaincrementalviewinpaintingusingrgbd
diffusionmodels. InCVPR,2023. 2
[9] CongyueDeng,ChiyuJiang,CharlesRQi,XinchenYan,Yin
[26] Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang,
Zhou,LeonidasGuibas,DragomirAnguelov,etal. Nerdi:
Yangyi Huang, Justus Thies, and Michael J Black. Tada!
Single-viewnerfsynthesiswithlanguage-guideddiffusionas
texttoanimatabledigitalavatars. In3DV,2024. 2
generalimagepriors. InCVPR,2023. 2
[27] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,
[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,
XiaohuiZeng,XunHuang,KarstenKreis,SanjaFidler,Ming-
AmitH.Bermano,GalChechik,andDanielCohen-Or. An
YuLiu,andTsung-YiLin. Magic3d:High-resolutiontext-to-
imageisworthoneword:Personalizingtext-to-imagegenera-
3dcontentcreation. InCVPR,2023. 2
tionusingtextualinversion. InICLR,2023. 2,5
[28] RuoshiLiu,RundiWu,BasileVanHoorick,PavelTokmakov,
[11] ShubhamGoel,GeorgiosPavlakos,JathushanRajasegaran,
SergeyZakharov,andCarlVondrick. Zero-1-to-3:Zero-shot
AngjooKanazawa*,andJitendraMalik*. Humansin4D:
oneimageto3dobject. InICCV,2023. 2
Reconstructingandtrackinghumanswithtransformers. In
[29] MatthewLoper,NaureenMahmood,JavierRomero,Gerard
ICCV,2023. 13
Pons-Moll,andMichaelJ.Black. Smpl:Askinnedmulti-
[12] ChenGuo,TianjianJiang,XuChen,JieSong,andOtmar
personlinearmodel. TOG,2015. 1,2,4
Hilliges. Vid2avatar:3davatarreconstructionfromvideosin
[30] AndreasLugmayr,MartinDanelljan,AndresRomero,Fisher
thewildviaself-supervisedscenedecomposition. InCVPR,
Yu,RaduTimofte,andLucVanGool. Repaint:Inpainting
2023. 2,3,4
using denoising diffusion probabilistic models. In CVPR,
[13] R. I. Hartley and A. Zisserman. Multiple View Geometry 2022. 2
in Computer Vision. Cambridge University Press, ISBN:
[31] NaureenMahmood,NimaGhorbani,NikolausF.Troje,Ger-
0521540518,secondedition,2004. 2
ardPons-Moll,andMichaelJ.Black. AMASS:Archiveof
[14] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu- motioncaptureassurfaceshapes. InICCV,2019. 4
sionprobabilisticmodels. InNeurIPS,2020. 2 [32] LukeMelas-Kyriazi,IroLaina,ChristianRupprecht,andAn-
[15] YangyiHuang,HongweiYi,YuliangXiu,TingtingLiao,Ji- dreaVedaldi.Realfusion:360degreconstructionofanyobject
axiangTang,DengCai,andJustusThies. TeCH:Text-guided fromasingleimage. InCVPR,2023. 2
ReconstructionofLifelikeClothedHumans. In3DV,2024. 2 [33] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
[16] BoyiJiang,YangHong,HujunBao,andJuyongZhang. Self- JonathanT.Barron,RaviRamamoorthi,andRenNg. Nerf:
recon:Selfreconstructionyourdigitalavatarfrommonocular Representingscenesasneuralradiancefieldsforviewsynthe-
video. InCVPR,2022. 2,5 sis. InECCV,2020. 2,3,6,12
9[34] ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexander [51] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
Keller. Instantneuralgraphicsprimitiveswithamultiresolu- andGregShakhnarovich. Scorejacobianchaining:Lifting
tionhashencoding. InSIGGRAPH,2022. 2 pretrained2ddiffusionmodelsfor3dgeneration. InCVPR,
[35] Natalia Neverova, David Novotny, Vasil Khalidov, Marc 2023. 2
Szafraniec,PatrickLabatut,andAndreaVedaldi. Continuous [52] PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,Taku
surfaceembeddings. InNeurIPS,2020. 2 Komura,andWenpingWang. Neus:Learningneuralimplicit
[36] JulianOst,FahimMannan,NilsThuerey,JulianKnodt,and surfacesbyvolumerenderingformulti-viewreconstruction.
FelixHeide. Neuralscenegraphsfordynamicscenes. In InNeurIPS,2021. 2
CVPR,2021. 2,3 [53] ZhengyiWang,ChengLu,YikaiWang,FanBao,Chongxuan
[37] HyunSooPark,TakaakiShiratori,IainMatthews,andYaser Li,HangSu,andJunZhu. Prolificdreamer:High-fidelityand
Sheikh. 3dreconstructionofamovingpointfromaseriesof diversetext-to-3dgenerationwithvariationalscoredistilla-
2dprojections. InECCV,2010. 2 tion. InNeurIPS,2023. 2
[38] SidaPeng,YuanqingZhang,YinghaoXu,QianqianWang, [54] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan,
QingShuai,HujunBao,andXiaoweiZhou. Neuralbody: JonathanT.Barron,andIraKemelmacher-Shlizerman. Hu-
Implicitneuralrepresentationswithstructuredlatentcodes mannerf:Free-viewpointrenderingofmovingpeoplefrom
fornovelviewsynthesisofdynamichumans. InCVPR,2021. monocularvideo. InCVPR,2022. 2,4,6,7,12
2,3,6,7,8,12 [55] TianhaoWu,FangchengZhong,AndreaTagliasacchi,For-
[39] BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall. resterCole,andCengizOztireli. Dˆ2nerf:Self-supervised
Dreamfusion:Text-to-3dusing2ddiffusion. InICLR,2023. decouplingofdynamicandstaticobjectsfromamonocular
1,2,3,5,12 video. InNeurIPS,2022. 3
[40] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo
[56] TiangeXiang,AdamSun,JiajunWu,EhsanAdeli,andLi
Kanazawa,andJitendraMalik.Trackingpeoplebypredicting
Fei-Fei. Renderinghumansfromobject-occludedmonocular
3Dappearance,location&pose. InCVPR,2022. 4
videos. InICCV,2023. 2
[41] DanielRebain,MarkMatthews,KwangMooYi,DmitryLa-
[57] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir,
gun,andAndreaTagliasacchi. Lolnerf:Learnfromonelook.
WilliamT.Freeman,RahulSukthankar,andCristianSmin-
InCVPR,2022. 5
chisescu. Ghum&ghuml:Generative3dhumanshapeand
[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
articulatedposemodels. InCVPR,2020. 2
Patrick Esser, and Bjo¨rn Ommer. High-resolution image
[58] WeipengXu,AvishekChatterjee,MichaelZollho¨fer,Helge
synthesiswithlatentdiffusionmodels. InCVPR,2022. 1,2,
Rhodin,DushyantMehta,Hans-PeterSeidel,andChristian
5,7,12,14
Theobalt. Monoperfcap:Humanperformancecapturefrom
[43] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
monocularvideo. InSIGGRAPH,2018. 2
Patrick Esser, and Bjo¨rn Ommer. High-resolution image
[59] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao.
synthesiswithlatentdiffusionmodels. InCVPR,2022. 3
ViTPose:Simplevisiontransformerbaselinesforhumanpose
[44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
estimation. InNeurIPS,2022. 13
MichaelRubinstein,andKfirAberman. Dreambooth:Fine
[60] GengshanYang,MinhVo,NataliaNeverova,DevaRamanan,
tuningtext-to-imagediffusionmodelsforsubject-drivengen-
AndreaVedaldi,andHanbyulJoo. Banmo:Buildinganimat-
eration. InCVPR,2023. 5
able3dneuralmodelsfrommanycasualvideos. InCVPR,
[45] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,
2022. 2
JayWhang,EmilyLDenton,KamyarGhasemipour,Raphael
[61] YifeiYin,ChenGuo,ManuelKaufmann,JuanZarate,Jie
GontijoLopes,BurcuKaragolAyan,TimSalimans,etal.Pho-
Song,andOtmarHilliges. Hi4d:4dinstancesegmentationof
torealistictext-to-imagediffusionmodelswithdeeplanguage
closehumaninteraction. InCVPR,2023. 6,7,12,13
understanding. InNeurIPS,2022. 2
[46] Johannes Lutz Scho¨nberger and Jan-Michael Frahm. [62] ZhengmingYu,WeiCheng,xianLiu,WayneWu,andKwan-
Structure-from-motionrevisited. InCVPR,2016. 4,12 YeeLin. MonoHuman:Animatablehumanneuralfieldfrom
monocularvideo. InCVPR,2023. 2
[47] Ruizhi Shao, Zerong Zheng, Hongwen Zhang, Jingxiang
Sun, and Yebin Liu. Diffustereo: High quality human re- [63] JiakaiZhang,XinhangLiu,XinyiYe,FuqiangZhao,Yanshun
constructionviadiffusion-basedstereousingsparsecameras. Zhang, Minye Wu, Yingliang Zhang, Lan Xu, and Jingyi
InECCV,2022. 2 Yu. Editable free-viewpoint video using a layered neural
[48] QingShuai,ChenGeng,QiFang,SidaPeng,WenhaoShen, representation. InSIGGRAPH,2021. 2,3,6
Xiaowei Zhou, and Hujun Bao. Novel view synthesis of [64] LvminZhang,AnyiRao,andManeeshAgrawala. Adding
humaninteractionsfromsparsemulti-viewvideos. InSIG- conditional control to text-to-image diffusion models. In
GRAPH,2022. 3,6,7,12 ICCV,2023. 5,12,13,14
[49] YuSun,QianBao,WuLiu,YiliFu,MichaelJBlack,andTao [65] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,
Mei. Monocular,one-stage,regressionofmultiple3dpeople. andOliverWang. Theunreasonableeffectivenessofdeep
InICCV,2021. 4 featuresasaperceptualmetric. InCVPR,2018. 7
[50] ZacharyTeedandJiaDeng. DROID-SLAM:DeepVisual [66] ZhizhuoZhouandShubhamTulsiani.Sparsefusion:Distilling
SLAM for Monocular, Stereo, and RGB-D Cameras. In view-conditioneddiffusionfor3dreconstruction. InCVPR,
NeurIPS,2021. 12,13 2023. 2
10[67] JosephZhuandPeiyeZhuang. Hifa:High-fidelitytext-to-3d
withadvanceddiffusionguidance. InICLR,2024. 12
[68] M. Zwicker, H. Pfister, J. van Baar, and M. Gross. Ewa
volumesplatting. InProceedingsVisualization,2001.VIS
’01.,2001. 3
11A.ImplementationDetails bodybeingtransparent.WedensifythehumanGaussiansin
[2000,2500,3000]iterationsfordetailedreconstructionand
A.1.BaselineImplementationDetails
pruneGaussianswhichareexceptionallylargeortransparent
HumanNeRF [54] does not support the simultaneous op- every500iterationsuntiltheendofoptimizationstoreduce
timizationofmultiplepeople,soweoptimizeeachperson artifacts.ThebackgroundGaussiansaredensifiedonlydur-
separatelyandmergethemintheevaluationstage.Following ing pre-optimization stage and keep the same number of
thedefaultHumanNeRFexperimentsettings,eachperson Gaussiansinthejointoptimizationstage.
isoptimizedfor400kiterationsusing4NVIDIARTX4090 OptimizationDetails.WeuseAdam[21]optimizerwith
GPUswhichtakesapproximately40hoursperperson.For differentlearningratesforeachcomponentof3DGaussians.
theZJU-Mocap[38]dataset,weutilizethepubliclyavailable ForthecenterofGaussianµ,wesetaninitiallearningrate
checkpointssharedbytheauthors. as1e−3 anddecayituntil2e−6 duringtraining.Weusea
Shuaietal.[48]representsthesceneasacompositionofa fixedlearningrate2.5e−3 forcolorc,5e−2 foropacityo,
backgroundmodelandhumanmodel,bothrepresentedbya 5e−3forscales,and1e−3forquaternionq.Wesettheloss
variantofNeRF[33,38].ForthePanopticdataset[19]and weight of SSIM loss λ = 0.2, MSE loss λ = 0.8,
ssim rgb
Hi4Ddataset[61],wemodelthebackgroundusingatime- LPIPSlossλ =0.1,andSDSlossλ =1.0.Forhard
lpips sds
conditioned NeRF defined on the surface of the cylinder surfaceregularizationloss,wesettheweightoflossλ
hard
fully covering the scene and the human model with Neu- relativetoreconstructionlossweightλ =0.1×λ
recon recon
ralBody [38]. We jointly optimize these models for 400k tokeepabalanceoflosses.Weuseafixedreconstructionloss
iterations using 2 NVIDIA RTX4090 GPUs which takes weightλ =1.0before1kiterationsandthenschedule
recon
approximately70hoursperscene.Theremainingsettings theweightafter1kiterationstobalancethereconstruction
arethesameastheoriginalpaper[48].Whenwerenderthe lossandSDSloss.
sceneforevaluation,wediscardthebackgroundandonly SDS loss details. We use a publicly available SD1.5 [42]
renderthehumanmodel. andOpenPoseControlNet[64]checkpointfortheSDSloss.
InstantAvatar[17]reconstructsasinglepersonfrommonoc- SimilartoothermethodsusingSDS[39],weuseahighCFG
ularvideoinput.Hence,weoptimizeitoneachpersonsepa- scaleof50togeneratedetailedtextureonunseenparts.We
ratelyandmergethemintheevaluationstagesameasHu- samplethenoisetimestepτ ofSDSlossfromU[0.5,0.98]
manNeRF [54]. We train the InstantAvatar for 50 epochs forthefirst2k iterationsandthensmoothlyannealitinto
usingasingleRTX3090,followingthedefaultoptionsused U[0.02,0.3]overfollowing2kiterationssimilartotheprior
tooptimizePeopleSnapShot[2]intheoriginalpaper. work[67].Wealsoscheduletheweightofreconstructionloss
λ withamaximumtimestepτ oneachiterationto
A.2.OursImplementationDetails recon max
balancethereconstructionlossandSDSlossasfollows:
Background pre-optimization. We first optimize back-
groundGaussiansGBGwithimagesthathumansaremasked λ =106×τ2 . (15)
recon max
out. The background Gaussians GBG are initialized with
point cloud obtained by SfM [46] or SLAM [50]. In the WeapplySDSlossfrom1kiterationofthejointoptimization.
caseofafixedcamera,weinitializeGaussiansGBGwitha Foreverysingleiterationofreconstructionloss,weapply
3Dspherewhoseradiusis30m,togetherwithbackground SDSlossonallhumanswhoappearedinthescene.
regularizationlosstopreventitfromoccludingthepeople WesamplerandomunseencamerasforSDSlossfrom
asfollows: thesurfaceofaspherewitharadiusof2.2,centeredonthe
human pelvis. The azimuth φ and elevation ϑ of cameras
N
LBG =λBG(cid:88) ||µBG−30||2 (14) are drawn from φ ∼ U[−π,π] and ϑ ∼ U[−0.3π,0.3π].
reg reg i
Additionally, we choose a view-augmented prompt [side,
i=0
front, back] based on the sampled azimuth φ and SMPL
, where µBG is the center of i-th background Gaussian. globalrotation.Fortheinitial3kiterationsofoptimization
i
Wescaletheworld’sunitdistancetobe1mbeforestarting with SDS loss, we mainly render the full body of posed
optimization.Thebackgroundisoptimizedfor30kiterations humanGaussiansGh(θ )andcanonicalhumanGaussians
j j,t
followingthedefault3D-GS[20]experimentsettings. Gh(θ )forSDSloss.Inthesubsequentiterations,wealso
j c
Human background joint optimization. After the pre- randomlysamplefromzoomed-inviewsofthehead,upper
optimizationofthebackground,weoptimizehumanGaus- body,andlowerbodytogetherwiththefullbodyoftheposed
siansGh andbackgroundGaussiansGBGtogether.
human,andthefullbodyofthecanonicalwithauniform
jj=1,...,N
For the first 1.5k iteration of joint optimization, we fix probabilityof0.2.Thistwo-stagerandomcamerasampling
the center of human Gaussians µ on the initial points facilitates the detailed reconstruction of unseen parts and
i
x and clamp the opacity o below 0.9 to avoid the head.
i,init i
12Aphotoof V*person
: Frozen
: Trainable
: w/ gradient
Text Transformer
: wo gradient
(detached)
⋯
Perturbed Image
Diffusion Model U-Net
Denoised Image
ControlNet Text Transformer
OpenPose Joints Aphotoof person
Figure8.OverallPipelineofTextualInversioninourmethod
Theorangepartiswhatweoptimizeduringtextualinversion.V*
indicates textual inversion token <person-j> which is train-
ing target. As shown here, we use CustomDiffusion [23] to-
getherwithControlNet[64]toobtainindividuals’inversiontoken
<person-j>andfine-tuneddiffusionmodelϕ .
j
isoccludedifit’sprojectedonthemasksofnearerpeople.
Figure7.Ablationstudyfortheclassifier-freeguidancescale. B.2.In-the-wildVideos
Wecroppedouttheblackblurryartifactsnearthefeetduetolackof
space.WecancheckthatalowCFGscale(a)generatesasmooth Inhandlingin-the-wildvideos,wecategorizetheminto
monotonic texture in unseen parts while a high CFG scale (b) two scenarios: static camera and moving camera. For the
synthesizesandenhanceswrinklesofclothingonbothseenand camera moving cases, we employ DROID-SLAM [50] to
unseenparts(lowerrow),butalsointroducesmoreartifacts.(upper estimatetheinitialcameraposeandGoeletal.[11]totrack
row) peoplewithregressingSMPLparameters.Subsequently,we
refinetheestimatedparametersbyminimizingthereprojec-
B.DatasetPreprocessing tionerrorbetweenestimated2Dbodyjoints[59].Incases
with a static camera, we skip the camera pose estimation
B.1.PanopticDataset[19]
step.
Wetrimthelastroundoftheultimatum160422sequence,
C.EffectofClassifier-FreeGuidanceScale
extracting540multi-viewimagesof6individualsbysub-
samplingevery4frames.Amongthe31HDcamerasinthe
To explore the impact of changing the classifier-free
PanopticDome,wespecificallychoosecameras0,3,5,8,
guidance(CFG)scale,weconductanablationstudyusing
22,24,and25forevaluation,whilecamera16servesasthe
Hi4D[61]pair00-dancesequence.Asillustratedinthe
input.Tosimulateachallengingscenario,weintentionally
lowerrowofFig.7,ahighCFGscalesynthesizesdetailed
picktheinputviewcamerathatexcludestheentranceofthe
unseenpartssuchasclothwrinklesanduniformnumbers,
PanopticDome[19]whereindividualsenteronebyone.
whilealowCFGscaleproducesasmooth,monotonictexture
ToacquiretheSMPLparametersθ andβ ofindividu-
t,j j withoutanywrinkles.Notably,ahighCFGscaleintroduces
als,weoptimizethembyminimizingthedistancebetween
moreartifactssuchasgreenstainswhichareamplifiedby
3DSMPLjointsandprovidedpseudogroundtruthCOCO
the light reflected from the floor shown in the upper row
3Djoints.Ouroptimizationprocessincorporatesposeprior,
of Fig. 7. This study shows the importance of selecting a
angleshaperegularization,and3Djointerror,asoutlined
proper CFG scale to reconstruct a detailed human avatar
in [3]. We leverage SMPL joints and SAM [22] to obtain
withminimalartifacts.
eachindividual’smaskintheinputframes.Initially,wear-
rangeindividualsbasedontheirdepthwhichiscalculated
D.DetailsofTextualInversion
asthedistancebetweenthepelvisofSMPLandthecamera
center. Starting with the individual closest to the camera, Toobtainanindividual’stext-token<person-j>and
we obtain a mask by querying the projected SMPL joints specifiedfine-tuneddiffusion,werunCustomDiffusionon
whichisnotoccludedintoSAM[22].Weassumethejoints eachindividual’sobservationswithmodificationsasshown
13
teNseR noitnettA VK
Q
teNseR noitnettA VK
Qan extreme lack of frontal train views. We further show
suchscenarioinFig.9(b),wheretrainingtheTIwithjust
asingleadditionalfrontalimagesubstantiallyimprovesthe
resemblance of the outputs, compared to Fig. 9 (a). This
demonstratestheuniqueadvantageofusingtextualinversion
forreconstruction,amethodthatisdifficulttoleverageusing
onlyreconstructionloss.
Figure9.AblationstudyofaddingadditionaldataduringTex-
tualInversion.TI*meansthetextualinversionusedinSDSlossis
trainedwithasingleadditionalimageofthefrontalview.Both(a)
and(b)areoptimizedwithtrainviewsandtheonlydifferenceisin
theTextualInversion.
inFig.8.WeuseOpenPoseControlNet[64]duringTextual
Inversion to avoid possible overfitting on observed body
poseandcamerapose.Toobtainanindividual’stext-token
<person-j>andspecifiedfine-tuneddiffusion,wefirst
randomlyperturbtheobservedimageandthenestimatethe
addednoiseoftheperturbedimage.ByminimizingtheMSE
lossbetweentheaddednoiseandtheestimatednoise,we
optimize the text-token and fine-tune the diffusion model.
Asweusethelatentdiffusionmodel[42]here,thetraining
objectiveisasfollows:
L =MSE(ϵ (z ;y,τ)−ϵ) (16)
textual ϕ τ
,wherez isaperturbedlatentcorrespondingtoperturbed
τ
imageinFig.8andϵistheaddednoise.Duringoptimization,
werandomlysampleτ fromτ ∼U[0,1].
Weoptimizetextualtokenandfine-tunediffusionusing
Adam[21]optimizerwithlearningrate5e−6andbatchsize
4for1000iterations.Tomitigatethesituationwherethetext
tokenlearnsthebackground,wemaskoutthebackground
andrandomlyfillitwithrandomcolor.Wedonotuseprior
preservationlossheretooverfitthetexttokenonobserved
images. The text-token <person-j> is queried only in
DiffusionU-NetandnotqueriedintheControlNetmodule
asshowninFig.8.
E.EnhancingIdentitywithAdditionalImages
By employing additional image sources for the target
identity,iftheyareknowninadvance,wecanenhancethe
identityofthepersonwithsparseobservations.Specifically,
trainingtheTextualInversion(TI)withanextrafaceimage
ofthetargetperson,assumingthisinformationisavailable
beforehand,enablesourmethodtoproduceresultsthatmore
closelyresemblethetargethuman,eveninscenarioswith
14Table6.Tableofnotations.
Symbol Description
Index
i Gaussianindex,i∈{1,...,N}in3DGaussianattributes
j Humanindex,inhumanGaussiansGhandSMPLparametersθ ,β
j j,t j
t Timeindex,t∈{1,...,T}inSMPLposeparameters,inputimages
k Jointindex,k ∈{1,...,N }inLBSskinning
joint
LearnableAttributesof3DGaussians
µ ∈R3 Centerofi-thGaussian
i
q ∈SO(3) CovarianceMatrix’sQuaternionComponentofi-thGaussian
i
s ∈R3 CovarianceMatrix’sScaleComponentofi-thGaussian
i
c ∈R3 Colorofi-thGaussian
i
o ∈R Opacityofi-thGaussian
i
G i-thGaussianconsistsof{µ ,q ,s ,c ,o }
i i i i i i
ParametersofDiffusionModel
ϕ/ϕ Diffusionmodel/Diffusionmodelfine-tunedonj-thperson
j
τ noisetime-stepofdiffusionmodelτ ∈[0,1]
z EncodedlatentofthequeriedRGBimagesondiffusionmodel
0
z Perturbedlatentwithnoisetime-stepτ ∈[0,1]
τ
ϵ Noiseaddedtothelatent
ϵ Noiseestimatedbydiffusionmodelϕ
ϕ
ParametersofHumanDeformation
θ ∈R72 SMPLposeparameterofj-thHumanintimet∈{1,...,T}
j,t
β ∈R10 SMPLshapeparameterofj-thHuman
j
θ ∈R72 Canonicalposeparametersharedforallhumans
c
RenderedandObservedImages
R /I Rendered/ObservedRGBimageintimet∈{1,...,T}
t t
Rh RenderedRGBimageofahumanwithcamerav
v
15