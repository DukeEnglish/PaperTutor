Rate Analysis of Coupled Distributed Stochastic Approximation for Misspecified
✩
Optimization
YaqunYanga,JinlongLeib,∗
aDepartmentofControlScienceandEngineering,TongjiUniversity,Shanghai,201804,China
bDepartmentofControlScienceandEngineering,TongjiUniversity,Shanghai,201804,China;ShanghaiInstituteofIntelligentScienceand
Technology,TongjiUniversity,Shanghai,200092,China
Abstract
Weconsiderannagentsdistributedoptimizationproblemwithimperfectinformationcharacterizedinaparamet-
ricsense,wheretheunknownparametercanbesolvedbyadistinctdistributedparameterlearningproblem. Though
eachagent onlyhas accessto itslocal parameterlearning andcomputationalproblem, theymean tocollaboratively
minimizetheaverageoftheirlocalcostfunctions. Toaddressthespecialoptimizationproblem,weproposeacoupled
distributedstochasticapproximationalgorithm, in whicheveryagentupdatesthecurrentbeliefs ofitsunknownpa-
rameteranddecisionvariablebystochasticapproximationmethod;thenaveragesthebeliefsanddecisionvariablesof
itsneighborsovernetworkinconsensusprotocol. Ourinterestliesintheconvergenceanalysisofthisalgorithm. We
quantitativelycharacterizethefactorsthataffectthealgorithmperformance,andprovethatthemean-squarederrorof
(cid:18) (cid:19)
thedecisionvariableisboundedbyO( n1 k)+O √ n(11
−ρw)
k11
.5
+O(cid:0) (1−1 ρw)2(cid:1) k1 2,wherekistheiterationcountand(1−ρ w)
isthespectralgapofthenetworkweightedadjacencymatrix.Itrevealsthatthenetworkconnectivitycharacterizedby
(1−ρ )onlyinfluencesthehighorderofconvergencerate,whilethedomainratestillactsthesameasthecentralized
w
algorithm.Inaddition,weanalyzethatthetransientiterationneededforreachingitsdominantrateO(1)isO( n ).
NumericalexperimentsarecarriedouttodemonstratethetheoreticalresultsbytakingdifferentCPUsan sk agents( ,1 w−ρ hw) i2
ch
ismoreapplicabletoreal-worlddistributedscenarios.
Keywords: DistributedCoupledOptimization,StochasticApproximation,Misspecification,ConvergenceRate
Analysis
1. Introduction
Inrecentyears,distributedoptimizationhasdrawnmuchresearchattentioninvariousfieldsincludingeconomic
dispatch[1,2],smartgrids[3,4,5],automaticcontrols[6,7,8]andmachinelearning[9,10]. Indistributedscenarios,
each agent only preserves its local information, while they can exchange information with others over a connected
network to cooperatively minimize the average of all agents’ cost functions [11, 12]. There are several approaches
for resolving distributed optimization problems such as (primary) consensus-based, duality-based, and constraint
exchangemethods,wheretheprimalapproachescharacterizedbygradient-basedalgorithmshaveattractedthemost
researchattentionduetotheirsatisfactoryperformanceandwell-scalablenature[13]. Thedistributeddualapproaches
based on Lagrange method regularly use dual decomposition like the alternating direction method of multipliers
(ADMM)[14]. Constraintexchangemethodisanotherprevalentschemewheretheinformationexchangedbyagents
amountstoconstraints[15].
✩ThepaperwassponsoredbytheNationalKeyResearchandDevelopmentProgramofChinaunderNo2022YFA1004701,theNationalNatural
ScienceFoundationofChinaunderNo.72271187andNo.62373283,andpartiallybyShanghaiMunicipalScienceandTechnologyMajorProject
No.2021SHZDZX0100,andNationalNaturalScienceFoundationofChina(GrantNo.62088101).
∗Correspondingauthor
Emailaddress:yangyaqun@tongji.edu.cn, leijinlong@tongji.edu.cn(JinlongLei)
4202
rpA
12
]CO.htam[
1v96631.4042:viXraHowever, among various formulations in distributed optimization, a crucial assumption is that we need precise
objective functions (or problem information), i.e., all parameters in the model are precisely known. Yet in many
economicandengineeringproblems,parametersofthefunctionsareunknownbutwemayhaveaccesstoobservations
thatcanaidinresolvingthismisspecification. Forexample,intheMarkowitzprofileproblem,itisroutinelyassumed
thattheexpectationorcovariancematricesassociatedwithacollectionofstocksareaccuratelyavailable,butinreality,
itneedsempiricalestimatesviapastdata[16].
This paper is devoted to proposing distributed algorithms for resolving optimization problems with parametric
misspecification, and quantitatively characterizing the influence of network properties, the heterogeneity of agents,
initialstates,etc. onthealgorithmperformance. Thisworkisprimarilycenteredaroundconductingacomprehensive
theoreticalanalysisofconvergence. Webeginbyinitiatingtheproblemformulation.
1.1. ProblemFormulation
Inthisarticle,weconsiderastaticmisspecifieddistributedoptimizationproblemdefinedasfollows:
1(cid:88)n
C (θ ): min f(x,θ )= f(x,θ ), (1)
x ∗ x∈Rp ∗ n i=1 i ∗
where f(x,θ ) ≜ E[f˜(x,θ,ξ)]isthelocalcostfunctiononlyaccessibleforagenti ∈ N ≜ {1,2,...,n}. Supposethat
i ∗ i i
foranyi ∈ N,ξ : Ω → Rd aremutuallyindependentrandomvariablesdefinedonaprobabilityspace(Ω ,F ,P ).
i x x x x
Here,θ ∈Rqdenotestheunknownparameter,whichisasolutiontoadistinctconvexproblem.
∗
1(cid:88)n
L : minh(θ)= h(θ), (2)
θ θ∈Rq n i=1 i
whereh(θ)≜E[h˜ (θ,ζ)]isthelocalparameterlearningfunctiononlyaccessibleforagenti∈N,andforanyi∈N,
i i i
ζ : Ω → Rm are mutually independent random variables defined on a probability space (Ω ,F ,P ). Problems in
i θ θ θ θ
theformeq.(1)andeq.(2)jointlyformulateanunknowncoupleddistributedoptimizationschemeconsistingofboth
computationalproblemandlearningproblem,wherethelearningproblemisindependentofthecomputationalone.
Wehavedepictedtheproblemsettinginfig.1.
(𝒙,𝜽)∈ℝ𝒑×𝒒
𝒇 (𝒙,𝜽)
𝒊
𝜽∈ℝ𝒒
𝒉 (𝜽)
𝒊
agent 𝑖
Figure 1: The problem setup: a connected network of communicating agents, where each agent preserving a local learning problem hi and
computationalproblem ficorrelatedwithhithroughtheunknownparameterθ,whiletheycooperatetosolvethedistributedcoupledoptimization
problem.
1.2. PriorWork
Wenowgiveareviewofpriorworkforresolvingoptimizationproblemswithunknownparameters.
2Robust optimization approach. Robust optimization considers the optimization problem where the parameter θ
is unavailable but one can have access to its uncertainty set, say U [17]. The key idea is to optimize against the
θ
worst-caserealizationwithinthisset,i.e.,
minmax f(x,θ).
x∈Rpθ∈U
θ
Robustoptimizationisshowntobeausefultechniqueintheresolutionofproblemsarisingfromcontrol,design,and
optimization [18]. However, it usually produces conservative solutions and sometimes is intractable when poor set
U ischosen(e.g. thesetisgivenbyunexplicitsystemsofnon-convexinequalities)[19].
θ
Stochasticoptimization. Unlikerobustoptimization,inastochasticoptimizationscenarioonemayobtainstatis-
tical or distributional information about the unknown parameter. For example, θ follows a probability distribution
D[20],theoptimalsolutionisgainedbyminimizingtheexpectationofcostfunctions,
minE [f(x,θ)].
θ∼D
x∈Rp
Stochasticoptimizationhasbeenwidelyinvestigatedintelecommunication,financeandmachinelearning[21].In
thescenarioofamulti-agentnetworkdealingwithlargedatasets,stochasticoptimizationhasbecomepopularsinceit
ischallengingtocalculatetheexactgradientwhilethestochasticgradientismucheasiertoobtain.Akeyshortcoming
inusingstochasticoptimizationmodelsforresolvingoptimizationproblemswithunknownparametersliesinthatit
needsthedistributionofθ,whichmightbeastringentrequirementwhentheavailabledataforestimatingislimitedor
noisy. Insuchcases,theresultingdistributionestimatesmaybeunreliableorbiased,leadingtosuboptimalsolutions
oreveninfeasiblesolutions[22]. Alternatively,supposethatθ canbelearntbyasuitablydefinedestimationproblem,
∗
thenitbringsaboutthefollowingapproach.
Data-driven learning approach. As data availability reaches hitherto unseen in recent years, we can use data-
drivenapproachestolessenoreveneliminatetheimpactofmodeluncertainty. Forexample,themodelparameterθ
canbeobtainedbysolvingasuitablydefinedlearningprobleml(θ)(seee.g.,[23]),
(cid:40) (cid:41)
min f(x,θ ):θ ∈argminl(θ) . (3)
∗ ∗
x∈Rp θ∈Rq
Computationalevidenceinportfoliomanagementandqueueingconfirmthatdata-drivensetssignificantlyoutperform
traditionalrobustoptimizationtechniques[19].
Anaturalquestioniswhetherthisproblemcouldbesolvedinasequentialmethod,i.e.,firstaccomplishestimating
θ with high accuracy and then solve the core computational optimization problem with the achieved estimation θˆ.
∗
However,theyhavesomedisadvantagesdiscussedin[23,24,25]:ontheonehand,thelarge-scaleparameterlearning
problemwillleadtolongtimewaitingforsolvingtheoriginalproblem. Ontheotherhand,thisschemeprovidesan
approximatesolutionθˆ,thenthecorrupterrormightpropagatesintothecomputationalproblem. Assuch,sequential
methods cannot provide asymptotically accurate convergence. Therefore, an alternative simultaneous approach is
designed(seee.g., [23,26]), whichuseobservationstogetanestimationθ ofunknownparametersθ∗ ateachtime
k
instantk;thenupdatetheupperoptimizationproblembytakingtheestimatedparameterθ as“true”parameter. This
k
simultaneousapproachcangenerateasequence{(x ,θ )}thatconvergestoaminimizerof f(x,θ )andl(θ)respectively
k k ∗
[23].
Suchdata-drivenlearningapproachesforunknownparameterhasgraduallyattractedresearchattentionrecently.
Forexample,theauthorsof[23]presentedacentralizedcoupledstochasticoptimizationschemetosolveproblem(3)
andshowedtheconvergencepropertiesinregimeswhenthefunctioniseitherstronglyconvexormerelyconvex.Then
[25]extendeditsmoothornonsmoothfunctions f andpresentedanaveraging-basedsubgradientapproach,butitis
stillacentralizedscheme. Inaddition,theauthorsof[24]dividedtheoptimizationproblemwithuncertaintyintotwo
paradigms: robustoptimizationandjointestimationoptimization,andtheyexploitedthesetwoproblemstructuresin
onlineconvexoptimizationandgaveregretanalysisunderdifferentconditions. Therecentwork[16]investigatedthe
misspecified conicconvex programs, anddeveloped acentralizedfirst-order inexactaugmented Lagrangianscheme
forcomputingtheoptimalsolutionwhilesimultaneouslylearningtheunknownparameters.Theaforementionedwork
[16,24,23,25]allinvestigatedcentralizedmethods,whiletherearesomeotherworkexploitdistributedapproaches.
Forexample,[27]consideredthedistributedstochasticoptimizationwithimperfectinformation,whileitonlyshowed
3thatthegeneratediteratesconvergealmostsurelytotheoptimalsolution.Thoughthework[28]presentedadistributed
problem with a composite structure consisting of an exact engineering part and an unknown personalized part, it
exhibitsaboundedregretundercertainconditions.
1.3. GapsandMotivation
Recallingtheproblemsetupinsection1.1,ourresearchfallsintodistributeddata-drivenstochasticoptimization
scenario. Takingintoaccounttheresearchthatismostpertinenttothispaper, themajorityofpreviousstudieshave
primarily concentrated on centralized inquiries (see e.g. [16, 24, 23, 25]), while the distributed schemes [27, 28]
mainlyinvestigatedtheasymptoticconvergence. Itremainsunknownhowtodesignanefficientdistributedalgorithm,
how does the network connectivity influence the algorithm performance, and whether the rate can reach the same
order as the centralized scheme? To be specific, this paper is motivated by the following gaps: (i) previous work
on unknown parameter learning problems focused on the centralized scheme, the distributed data-driven stochastic
approximationmethodislessstudied; (ii)thediscussionofconvergenceanalysisespeciallyhowfactorssuchasthe
numberofagents,thenetworkconnectivity,andtheheterogeneityofagentsinfluencetherateofalgorithmisrarely
studiedindetails;(iii)thegapbetweencentralizedanddistributedalgorithmunderimperfectinformationneedtobe
specified,orinotherwordscanwefindthetransienttimewhentherateofdistributedalgorithmsreachthesameorder
asthatofthecentralizedscheme?
1.4. OutlineandContributions
Toaddressthesegaps,weproposeadata-drivencoupleddistributedstochasticapproximationmethodtoresolve
thisspecialoptimizationproblemandgiveapreciseconvergencerateanalysisofouralgorithm. Themaincontribu-
tionsaresummarizedasfollows,andthecomparisonwithpreviousworksisshownintable1.
Table1:Workcomparationwithpreviousstudies
Paper Distributed ImperctectInformation Stochastic Rate FactorInfluence
(cid:16) (cid:17)
[24,23,25] (cid:37) (cid:33) (cid:37) O 1 (cid:37)
(cid:16)k(cid:17)
[29,30] (cid:33) (cid:37) (cid:33) O 1 (cid:33)
k
[28] (cid:33) (cid:33) (cid:37) \ (cid:33)
[27] (cid:33) (cid:33) (cid:33) \ (cid:37)
(cid:18) (cid:19)
[31] (cid:33) (cid:37) (cid:37) O √1 (cid:37)
(cid:16) k(cid:17)
OurWork (cid:33) (cid:33) (cid:33) O 1 (cid:33)
k
(1)Weproposeacoupleddistributedstochasticapproximationalgorithmthatgeneratesiterates{(xxx(k),θθθ(k))}for
the distributed stochastic optimization problem (1) with the unknown parameter learning prescribed by a separate
distributedstochasticoptimizationproblem(2). Ourmodelframeworkbuildsuponpreviousresearchinvolvingdeter-
ministicandstochasticgradientschemes. Thisisparticularlyrelevantforcertainstudieswherewaitingforparameter
learningtocompleteoveranextendedperiodisnotfeasible,orforreal-worldproblemsinwhichparameterlearning
andobjectiveoptimizationareintertwined.
(2) We characterize the convergence rate of the presented algorithm that combined the distributed consensus
protocol with stochastic gradient descent methods. On the one hand, we prove that the upper bound of expected
consensuserrorforeveryagentdecayatrateO(1);ontheotherhand,wealsoshowthattheupperboundedofexpected
k2
optimizationerrorisO(1). Wethengivethesublinearconvergencerateandquantitativelycharacterizesomefactors
k
affectingtheconvergencerate,suchasthenetworksize,spectralgapoftheweightedadjacencymatrix,heterogenous
of individual function, and initial values. We emphasize that the mean-squared error of the decision variable is
(cid:18) (cid:19)
bounded by O(1)+O √ 1 1 +O(cid:0) 1 (cid:1) 1, which indicates that the network connectivity characterized by
nk n(1−ρw) k1.5 (1−ρw)2 k2
(1−ρ ) only influences the high order of convergence rate, while the domain rate O(1) still acts the same as the
w k
centralizedalgorithm.
(3) We analyze the transient time K for the proposed algorithm, namely, the number of iterations before the
T
algorithmreachesitsdominantrate. Specially,weshowthatwhentheiteratek≥ K ,thedominantfactorinfluencing
T
4the convergence rate is related to stochastic gradient descent, while for small k < K , the main factor influencing
T
theconvergencerateoriginatesfromthedistributedaverageconsensusmethod. Finally,weshowthatthealgorithm
asymptoticallyachievesthesamenetwork-independentconvergencerateasthecentralizedscheme.
The paper is organized as follows. We present the algorithm and the related assumptions in section 2. In sec-
tion 3, the auxiliary results supporting the convergence rate analysis is proved. Our main results are in section 4.
Experimentalresultsareimplementedinsection5,whiletheconcludingremarksaregiveninsection6.
Notation. Allvectorsinthispaperarecolumnvectors. Thestructureofthecommunicationnetworkismodeled
byanundirectedweightedgraphG=(N,E,W)inwhichN ={1,2,...,n}representsthesetofvertices. E⊆N ×N
is the set of edges. W = [w ] ∈ Rn×n denotes the weighted adjacency matrix, w > 0 if and only if agent i and
ij n×n ij
agent jareconnected, w = w = 0otherwise. Eachagent(vertice)hasasetofneighborsN = {j|(i, j) ∈ E}. The
ij ji i
graphisconnectedmeansforeverypairofnodes(i, j)thereexistsapathofedgesthatgoesfromito j. ||·||denotes
L -normforvectorsandEuclideannormformatrices. Theoptimalsolutiondenoteas(x ,θ ).
2 ∗ ∗
2. AlgorithmandAssumptions
To solve this special optimization problem consisting of the computational problem eq. (1) and the learning
problem eq. (2), we will propose a Coupled Distributed Stochastic Approximation (CDSA) Algorithm and impose
someconditionsforrateanalysisinthissection.
2.1. AlgorithmSetUp
As mentioned previously, each agent i only knows its local core computational function f(x,θ) and parameter
i
learningfunctionh(θ),whiletheyareconnectedbyanetworkG=(N,E,W)inwhichagentsmaycommunicateand
i
exchangeinformationwiththeirneighborsN ={j|(i, j)∈E}.Ateachstepk≥0,everyagentiholdsanestimateofthe
i
decisionvariableandunknownparameter,denotedbyx(k)andθ(k),respectively.Supposethateveryagenthasaccess
i i
to a stochastic first-order oracle that can generate stochastic gradients g(x(k),θ(k),ξ(k)) ≜ ∇ f(x(k),θ(k),ξ(k))
i i i i x i i i i
and ϕ(θ(k),ζ(k)) ≜ ∇ h(θ(k),ζ(k)) respectively (where ξ,ζ,i = 1,2,...,n are independent random variables).
i i i θ i i i i i
Then, every agent updates its parameters through stochastic gradient descent method to obtain temporary estimates
(cid:101)x i(k)and(cid:101)θ i(k).Next,eachagentcommunicateswithitslocalneighborsandgatherstemporaryparametersinformation
over a static connected network to renew the iterates x(k+1) and θ(k+1) based on the consensus protocol. We
i i
summarizethepseudo-codeisinalgorithm1.
Algorithm1CoupledDistributedStochasticApproximation(CDSA)
Initialization: W =[w ] ;(x(0),θ(0)),∀i∈N
ij n×n i i
Evolution: fork=0,1,2,...;∀i∈N
Compute: stochasticgradientϕ (θ(k),ζ(k))andg(x(l),θ(k),ξ(k))
i i i i i i i
Choose: stepsizeα andγ (Tobeintroducedinsection3.3)
k k
Update accordingtothefollowingstochasticgradientdescentmethod.
(cid:101)x i(k)= x i(k)−α kg i(x i(k),θ i(k),ξ i(k))
(cid:101)θ(k)=θ(k)−γ ϕ (θ(k),ζ(k))
i i k i i i
Gather information(cid:101)x j(k),(cid:101)θ j(k) from its neighbors j ∈ N
i
and renew the iterates by the consensus protocol
below.
(cid:88)
x i(k+1)= w ij(cid:101)x j(k)
(cid:88)j∈N
i
θ(k+1)= w (cid:101)θ (k)
i ij j
j∈N
i
5WecanrewriteAlgorithm1inamorecompactformasfollows.
(cid:88)
x(k+1)= w (x (k)−α g (x(l),θ(k),ξ(k))), (4)
i ij j k j i i i
(cid:88)j∈N
i (cid:16) (cid:17)
θ(k+1)= w θ (k)−γ ϕ (θ(k),ζ(k)) . (5)
i ij j k j i i
j∈N
i
Define
xxx≜[x ,x ,··· ,x ]T ∈Rn×p,θθθ≜[θ ,θ ,··· ,θ ]T ∈Rn×q, (6)
1 2 n 1 2 n
ξ ≜[ξ ,ξ ,··· ,ξ ]T ∈Rn, ζ ≜[ζ ,ζ ,··· ,ζ ]T ∈Rn, (7)
1 2 n 1 2 n
g(xxx,θθθ,ξξξ)≜[g (x ,θ ,ξ ),g (x ,θ ,ξ ),··· ,g (x ,θ ,ξ )]T ∈Rn×p, (8)
1 1 1 1 2 2 2 2 n n n n
ϕ(θθθ,ζζζ)≜[ϕ (θ ,ζ ),ϕ (θ ,ζ ),··· ,ϕ (θ ,ζ )]T ∈Rn×q. (9)
i 1 1 i 2 2 i n n
Thenequation(4)and(5)canbereformulatedinthefollowingvectorformula.
xxx(k+1)=W(xxx(k)−α g(xxx(k),θθθ(k),ξξξ(k))), (10)
k
θθθ(k+1)=W(θθθ(k)−α ϕ(θθθ(k),ζζζ(k))). (11)
k
2.2. Assumptions
In this subsection, we will specify the conditions for rate analysis of the CDSA algorithm. We need to make
some assumptions about the properties of objective functions in both learning and computation metrics to get the
globaloptimalsolution. Besides,weimposesomeconstraintsonconditionalfirstandsecondmomentsof“stochastic
gradient”. Lastbutnotleast,weinheritthetypicalassumptionsaboutcommunicationnetworksasthatofdistributed
algorithms.
Assumption2.2.1(Functionproperties) (i)Foreveryθ ∈ Rq, f(x,θ),i = 1,··· ,nisstronglyconvexandLipschitz
i
smoothinxwithconstantsµ andL ,i.e.
x x
(∇ f(x′,θ)−∇ f(x,θ))T(x′−x)≥µ ||x′−x||2,∀x,x′ ∈Rp,
x i x i x
||∇ f(x′,θ)−∇ f(x,θ)||≤ L ||x′−x||,∀x,x′ ∈∈Rp.
x i x i x
(ii)Forevery x ∈ Rp, f(x,θ),i = 1,··· ,nisstronglyconvexandLipschitzsmoothinθwithconstantsµ and L
i θ θ
respectively,i.e.
(∇ f(x,θ′)−∇ f(x,θ))T(θ′−θ)≥µ ||θ′−θ||2,∀θ,θ′ ∈Rq,
x i x i θ
||∇ f(x,θ′)−∇ f(x,θ)||≤ L ||θ′−θ||,∀θ,θ′ ∈Rq.
x i θ θ
(iii)The learning metric h(θ) for every i ∈ {1,2,...,n} is strongly convex andLipschitz smooth with constants ν
i θ
andC ,i.e.
θ
(h(θ)−h(θ′))T(θ−θ′)≥ν ||θ−θ′||2,∀θ,θ′ ∈Rq,
θ
||∇h(θ)−∇h(θ′)||≤C ||θ−θ′||,∀θ,θ′ ∈Rq.
θ
Strongconvexityassumptionsindicatethatbothcomputationalproblemandlearningproblemhaveauniqueoptimal
solution x ∈ Rp and θ ∈ Rq [32]. The Lipschitz continuity of gradient functions ensure that the gradient doesn’t
∗ ∗
changearbitrarilyfastconcerningthecorrespondingparametervector. Itiswidelyusedintheconvergenceanalyses
of most gradient-based methods, without it, the gradient wouldn’t provide a good indicator for how far to move to
decrease the objective function[32]. These assumptions are satisfied for many machine learning problems, such as
logisticregression,linearregression,andsupportvectormachine(SVM).
Next,wedefineanewprobabilityspace(Z,F,P),whereZ≜Ω ×Ω ,F ≜F ×F andP≜P ×P . Weuse
x θ x θ x θ
F(k)todenotetheσ-algebrageneratedby{(x(0),θ(0)),(x(1),θ(1)),...,(x(k),θ(k))|i∈N}. Thengivethefollowing
i i i i i i
assumptions related to the stochastic gradient estimator, which assume that the stochastic gradient is an unbiased
estimatorofthetruegradient,andthevarianceofthestochasticgradientisrestricted.
6Assumption2.2.2(Conditionalfirstandsecondmoments) For all k ≥ 0 and i ∈ N,there exist σ > 0,σ >
x θ
0,M >0,M >0,suchthat
x θ
(a) E [g(x(k),θ(k),ξ(k))|F(k)]=∇ f(x(k),θ(k)), a.s.,
ξi(k) i i i i x i i i
(b) E [ϕ(θ(k),ζ(k))|F(k)]=∇h(θ(k)), a.s.,
ζi(k) i i i i i
(c) E [||g(x(k),θ(k),ξ(k))−∇ f(x(k),θ(k))||2|F(k)],
ξi(k) i i i i x i i i
≤σ2+M ||∇ f(x(k),θ(k))||2 a.s.,
x x x i i i
(d) E [||ϕ(θ(k),ζ(k))−∇h(θ(k))||2|F(k)]≤σ2+M ||∇h(θ(k))||2, a.s.,
ζi(k) i i i i i θ θ i i
Next, weimposethe connectivityconditiononthe graph, whichindicatesthataftermultiple roundsofcommu-
nication,informationcanbeexchangedbetweenanytwoagents. Thisinheritsthetypicalassumptionsonconsensus
protocols[33].
Assumption2.2.3(Graphandweightedmatrix) The graph G is static, undirected, and connected. The weighted
adjacencymatrixW isnonnegativeanddoublystochastic,i.e.,
W1=1,1TW =1T (12)
where1isthevectorofallones.
Next, we state two lemmas that partially explain the practicability of algorithm 1 based on the aforementioned
assumptions.
Lemma2.2.1 [34, Lemma 10] For any x ∈ Rp, define x+ = x−α∇f(x). Suppose that f is strongly convex with
constantµanditsgradientfunctionisLipschitzcontinuouswithconstantL. Ifα∈(0,2/L),wethenhave||x+−x ||≤
∗
λ||x−x ||,whereλ≜max(|1−αµ|,|1−αL|).
∗
Itcanbeobservedfromtheabovelemmathataslongaswechooseaproperstepsize(0<α<2/L),thedistanceto
optimizershrinksbyaratioλ<1ateachstepforstronglyconvexandsmoothfunctions. Whilethefollowinglemma
reveals that under distributed algorithm with linear iteration, the gap between the current iteration and consensus
optimalsolutionisdecreasedbyaratioρ <1comparedtothelastiteration.
w
Lemma2.2.2 [33,Theorem1]LetAssumption2.2.3hold,andρ denotethespecturalnormofmatrixW−111111T. Thus
w n
ρ <1. Defineωωω+ =Wωωωforanyωωω∈Rn×p. Wethenhave||ωωω+−111ω¯||≤ρ ||ωωω−111ω¯||,whereω¯ ≜ 1111Tωωω.
w w n
Theaforementionedlemmasshowthatboththegradientdescentmethodanddistributedlineariterationcanmove
thedecisionvariabletowardstheoptimalsolutionwithlineardecayingrates. Thus,ouralgorithmconsistingofboth
approachesmightfindtheoptimalsolutionefficiently. Wewillrigorouslyprovetheconvergencerateofalgorithm1
inthefollowingtwosections.
3. AuxiliaryResults
In this section, we will present some results to assist subsequent convergence rate analysis. We first give some
preliminaryboundwhichwillbeusedforlaterproof,thenpresentthesupportinglemmasconcerningrecursionsfor
expectedoptimizationerrorandexpectedconsensuserror,andfinally,weprovethatunderdiminishingstepsize,the
mean-squareddistancebetweenthecurrentiterateandtheoptimalsolutionisuniformlybounded.
3.1. PreliminaryBound
Forsimplicity,wedenote
1(cid:88)n 1(cid:88)n
x¯(k)≜ x(k), θ¯(k)≜ θ(k), (13)
n i=1 i n i=1 i
1(cid:88)n
g¯(xxx(k),θθθ(k),ξξξ(k))≜ g(x(k),θ(k),ξ(k)), (14)
n i=1 i i i i
∇¯ F(xxx(k),θθθ(k))≜
1(cid:88)n
∇ f (x(k),θ(k)). (15)
x n i=1 x i i i
7We will show in the following lemma that with Assumptions 2.2.1 and 2.2.2, the conditional squared distance
betweenthegradient∇¯ F(xxx(k),θθθ(k))anditsestimatecanbeboundedbylinearcombinationsofsquarederrors||xxx(k)−
x
111xT||2and||θθθ(k)−111θT||2. Forcompleteness,itsproofisgiveninappendixAppendix A.
∗ ∗
Lemma3.1.1 LetAssumption2.2.1and2.2.2hold. Thenforanyk≥0,
E[||g¯(xxx(k),θθθ(k),ξξξ(k))−∇¯ F(xxx(k),θθθ(k))||2|F(k)]
x
≤
3M xL2
x||xxx(k)−111xT||2+
3M xL θ2
||θθθ(k)−111θT||2+
M¯
, (16)
n2 ∗ n2 ∗ n
3M
(cid:80)n
||∇ f(x ,θ )||2
where M¯ = x i=1 x i ∗ ∗ +σ2. (17)
n x
Thefollowinglemmashowsthegapbetweenthegradientofobjectivefunctionattheconsensualpoints(x¯(k),θ¯(k)),
denotedby 1(cid:80)n ∇ f(x¯(k),θ¯(k)),andatcurrentiterates 1(cid:80)n ∇ f(x(k),θ(k))canalsobeboundedbylinearcom-
n i=1 x i n i=1 x i i i
binationsof||xxx(k)−111xT||2and||θθθ(k)−111θT||2. ThepreciseproofisinappendixAppendix B.
∗ ∗
Lemma3.1.2 LetAssumption2.2.1hold. Thenforanyk≥0,
L L
||∇ f(x¯(k),θ¯(k))−∇¯ F(xxx(k),θθθ(k))||≤ √x ||xxx(k)−111x¯(k)T||+ √θ ||θθθ(k)−111θ¯(k)T||. (18)
x x
n n
The above two lemmas, providing the related upper bounds of functions, are derived by virtue of the Lipschitz
smoothassumption. Theyareessentialforthesubsequentconvergenceanalysis.
3.2. SupportingLemmas
Inthissubsection,wepresentsomeresultsconcerningexpectedoptimizationerror E[||x¯(k)−x ||2]andexpected
∗
consensus error E[||xxx(k) −111x¯(k)T||2] for core computational problem, while the discussion of parameter learning
problemcanbefoundin[30]. Foreaseofpresentation,wedenote
U (k)≜E[||x¯(k)−x ||2],V (k)≜E[||xxx(k)−111x¯(k)T||2], (19)
1 ∗ 1
U (k)≜E[||θ¯(k)−θ ||2],V (k)≜E[||θθθ(k)−111θ¯(k)T||2]. (20)
2 ∗ 2
NextwewillboundU (k+1)andV (k+1)byerrortermsatiterationk. ThepreciseproofofLemma3.2.1isin
1 1
appendixAppendix C.
Lemma3.2.1 LetAssumption2.2.1∼2.2.3hold,underalgorithm1,
(AAA)Supposingstepsizeα ≤ 1,wehave
k Lx
α2L2 α2L2
U (k+1)≤(1−α µ )2U (k)+ k x V (k)+ k θV (k)
1 k x 1 n 1 n 2
2L L α2
+ x θ kE[||xxx(k)−111x¯(k)T||||θθθ(k)−111θ¯(k)T||]
n
2α L
+ √k x(1−α µ )E[||x¯(k)−x ||||xxx(k)−111x¯(k)T||]
k x ∗
n
2α L
+ √k θ(1−α µ )E[||x¯(k)−x ||||θθθ(k)−111θ¯(k)T||]
k x ∗
n
 
+α2 k3M nx 2L2 xE[||xxx(k)−111x ∗T||]+ 3M nx 2L θ2 E[||θθθ(k)−111θ ∗T||]+ M n¯ , (21)
8(BBB)Supposingstepsizeα ≤min{1, 1 },wehave
k Lx 3µx
U (k+1)≤(1−
3
α µ )U (k)+
6α kL2
xV (k)+
6α kL θ2
V (k)
1 2 k x 1 nµ 1 nµ 2
 x x 
+α2 k3M nx 2L2 xE[||xxx(k)−111x ∗T||2]+ 3M nx 2L θ2 E[||θθθ(k)−111θ ∗T||2]+ M n¯ . (22)
ResultBBBrestrictsthestepsizetosmalleronethanthatofResultAAA.ItthussimplifiesResultAAAeq.(21)byremoving
the cross term to facilitate the later analysis. We can revisit Inequality eq. (22) and reformulate it as U (k +1) ≤
1
(1− 3α µ )U (k)+error(α ),whereerror(α)meansanerrorfunctionthatisproportionaltoα. Weshouldmention
2 k x 1 k
that,sinceα >0andµ >0,expectedoptimizationerrorU (k)roughlyshrinksbyaratio(1− 3α µ )<1. Though
k x 1 2 k x
thereisanerrortermrelatedtoα ,whenwechoosediminishingstepsizepolicyandtheconsensuserrorsV (k),V (k)
k 1 2
aswellasE(||xxx(k)−111xT||2),E(||θθθ(k)−111θT||2)arebounded,theerrormaydecreaseto0,whichindicatestheconvergence
∗ ∗
ofU (k).
1
Wedefine
∇ F(xxx,θθθ)≜[∇ f (x ,θ ),∇ f (x ,θ ),··· ,∇ f (x ,θ )]T ∈Rn×p. (23)
x x 1 1 1 x 2 2 2 x n n n
In the next lemma, we will show the recursive formulation of expected consensus error V (k), which is critical for
1
convergenceanalysis. Forcompleteness,wegiveitsproofinappendixAppendix D.
Lemma3.2.2 LetAssumption2.2.1∼2.2.3hold,andconsideralgorithm1. Thenforanyk≥0,wehave
3+ρ2 (cid:32) 3 (cid:33) (cid:16)
V (k+1)≤ wV (k)+α2ρ2nσ2+3α2ρ2 +M L2E[||xxx(k)−111xT||2]
1 4 1 k w x k w 1−ρ2 x x ∗
w(cid:17)
+L2E[||θθθ(k)−111θT||2]+||∇ F(111xT,111θT)||2 . (24)
θ ∗ x ∗ ∗
TherecursionofexpectedconsensuserrorcanbereformulateasV (k+1)≤ 3+ρ2 wV (k)+error(α2ρ2). Itisworth
1 4 1 k w
mentioningthatV (k)canroughlyshrinkby
3+ρ2
w < 1sinceρ < 1. Notethattheextraerrortermintheconsensus
1 4 w
error is proportional to α2, compared to U (k) with an error term proportional to α . We might obtain a qualitative
k 1 k
conclusionthatexpectedconsensuserrordecreasefasterthanexpectedoptimizationerror. Wewillpresenttheprecise
proofinthenextpartthatconsensuserrordecreaseto0atanorderO(1)whileoptimizationerroratO(1).
k2 k
Remark1 Recalling recursion of U (k) in (22) and recursion of V (k) in (24), we could notice that the expected
1 1
consensuserrorismorerelatedtothenetworkconnectivityρ ,whichisnaturalbecause“consensus”isinducedfrom
w
thedistributedalgorithm, while“optimization”mainlycomesfromoriginaloptimizationmethodsuchasstochastic
gradientdescent.
3.3. UniformBound
Fromnowon,weconsiderstepsizepolicyasfollows
β β
α ≜ ,γ ≜ , ∀k, (25)
k µ (k+K) k µ (k+K)
x θ
wheretheβisapositiveconstant,and
K
≜(cid:38) max 3β(1+M x)L2 x,3β(1+M θ)L θ2 (cid:39)
(26)

µ2 µ2

x θ
with⌈·⌉denotingtheceilingfunction.
Next, We present a lemma that derives a uniform bound on the iterates {θθθ(k)},{xxx(k)} generated by algorithm 1.
Sucharesultishelpfulforboundingtheexpectedoptimizationerrorandexpectedconsensuserror.
9Lemma3.3.1 LetAssumption2.2.1∼2.2.3hold. Consideralgorithm1withstepsizepolicy(25). Wethenobtainfrom
[30,Lemma8]thatforallk≥0,
E[∥θ(k)−θ ∥2]≤Θˆ
≜max ∥θ(0)−θ ∥2,9∥∇h i(θ ∗)∥2
+
σ2
θ
 
. (27)
i ∗ i  i ∗ µ (1+M )L2
θ θ θ
Basedon(27),wecanobtainthefollowingresultwithΘˆ ≜(cid:80)n Θˆ ,
i=1 i
E[||xxx(k)−111xT||2]≤ Xˆ, where (28)
∗
(cid:40) 11L2Θˆ 11||∇ F(111xT,111θT)||2 7nσ2 (cid:41)
Xˆ ≜max ||xxx(0)−111xT||2, θ + x ∗ ∗ + x . (29)
∗ µ2 µ2 9(1+M )L2
x x x x
We will give the proof of (28) in appendix Appendix E. Lemma 3.3.1 indicates that although the problem we
consider is unconstrained, the gap between the iterates generated by algorithm CDSA and the optimal solution is
uniformly bounded. It is critical for the analysis of sublinear convergence rates of U (k) and V (k). Then based on
1 1
thislemma,wewillprovideuniformupperboundsfortheexpectedoptimizationerrorandexpectedconsensuserror.
Lemma3.3.2 LetAssumption2.2.1∼2.2.3hold.Consideralgorithm1withstepsizepolicy(25).WethenhaveU (k)≤
1
Xˆ,V (k)≤4Xˆ.
n 1
Proof Byrecalling(28)andusingCauchy-Schiwazinequality,weobtainthat
U 1(k)=E[||x¯(k)−x
∗||2]=E(cid:34)(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13)n1(cid:88)n
i=1x i(k)−
1 n(cid:88)n
i=1x
∗(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:35)
= 1 E(cid:20)(cid:13) (cid:13) (cid:13)(cid:88)n (x(k)−x )(cid:13) (cid:13) (cid:13)2(cid:21) ≤ 1 ×nE[||xxx(k)−111xT||2]≤ Xˆ ,
n2 (cid:13) i=1 i ∗ (cid:13) n2 ∗ n
V (k)=E[||xxx(k)−111x¯(k)T||2]=E[||xxx(k)−111xT +111xT −111x¯(k)T||2]
1 ∗ ∗
≤2E[||xxx(k)−111xT||2]+2E[||111(x −x¯(k)||2)]
∗ ∗
Xˆ
≤2Xˆ +2n× ≤4Xˆ.
n
□
4. MainResults
In this section, we will make full use of previous results and then give a precise convergence rate analysis of
algorithm 1. The elaboration will be divided into three parts. Firstly, we respectively establish the O(1) and O(1)
k k2
convergence rate of U (k) = E[||x¯(k) − x ||2] and V (k) = E[||xxx(k) −111x¯(k)T||2] based on two supporting lemmas
1 ∗ 1
in section 3.2. Secondly, we show that the convergence rate, measured by the mean-squared error of the decision
variables,isasfollows.
(cid:18) (cid:19)
(cid:16) (cid:17)
1(cid:88)n E[||x(k)−x ||2]≤ β2M¯ + O √ n(11 −ρw) + O (1−1 ρw)2 ,
n i=1 i ∗ (2β−1)nµ2(k+K) (k+K)1.5 (k+K)2
x
where the first term is only concerned with the stochastic gradient descent method which is network independent,
whilethehigher-orderdependson(1−ρ ). Finally,wecharacterizethetransienttimeneededforCDSAtoapproach
(cid:16) w (cid:17)
theasymptoticconvergencerateisO n .
(1−ρw)2
4.1. SublinearConvergence
WefirstprovethattheexpectedconsensuserrorV (k)=E[||xxx(k)−111x¯(k)T||2]decayswithrateV (k)=O(1).
1 1 k2
10Lemma4.1.1 LetAssumption2.2.1∼2.2.3hold. Consideralgorithm1withstepsize(25). RecallthedefinitionsofK.
Define
∇HHH(θ)≜[∇h (θ ),∇h (θ ),···∇h (θ )]∈Rn×q, (30)
1 1 2 2 n n
(cid:108) (cid:110) 16 (cid:111)(cid:109)
K ≜ max 2K, . (31)
1 1−ρ2
w
Wethenobtainfrom[30,Lemma10]thatforanyk≥ K −K,
1
Vˆ (cid:110) 8β2ρ2c′ (cid:111)
V (k)≤ 2 withVˆ ≜max K2Θˆ, w 4 , (32)
2 (k+K)2 2 1 µ2(1−ρ2)
(cid:32) (cid:33) θ w
3 (cid:16) (cid:17)
wherec′ ≜2 +M L2Θˆ +∥∇HHH(111θT)∥ +nσ2. (33)
4 1−ρ2 θ θ ∗ θ
w
Furthermore,weachievethat
(cid:40) (cid:41)
Vˆ 8β2ρ2c
V (k)≤ 1 withVˆ ≜max 4K2Xˆ, w 4 , (34)
1 (k+K)2 1 1 µ2(1−ρ2)
(cid:32) (cid:33) x w
3 (cid:16) (cid:17)
wherec ≜3 +M L2Xˆ +L2Θˆ +||∇ F(111xT,111θT)||2 +nσ2. (35)
4 1−ρ2 x x θ x ∗ ∗ x
w
Proof Wenowprove(34). FromLemma3.2.2and3.3.1itfollowsthat
3+ρ2
V (k+1)≤ wV (k)+α2ρ2c , ∀k≥0. (36)
1 4 1 k w 4
We use induction method to show that (34) holds for any k ≥ K −K. Recall from Lemma 3.3.2 that V (k) ≤ 4Xˆ.
1 1
Thenfork= K −K,V (k)≤ 4K 12Xˆ = 4K 12Xˆ ≤ Vˆ 1 bythedefinitionofVˆ in(34). Supposethat(34)holdsfork=k˜.
1 1 K2 (k+K)2 (k+K)2 1
1
Itsufficestoshowthat(34)holdsfork=k˜+1.
Notefrom(31)thatk˜+K ≥ 16 foranyk˜ ≥ K −K. Wethenhave
1−ρ2
w
1
(cid:32) k˜+K (cid:33)2 3+ρ2 2 1 3+ρ2
− w =1− + − w
k˜+K+1 4 k˜+K+1 (k˜+K+1)2 4
1−ρ2 2 1−ρ2
≥ w − ≥ w.
4 k˜+K 8
Dividebothsidesofaboveinequalityby β2ρ2 wc4. RecallingthedefinitionofVˆ in(34),wehave
µ2
x
1
β2ρ µ2
w
2c
4
 (cid:32) k˜+k˜+ KK +1(cid:33)2
−
3+ 4ρ2 w −1
≤
µ8 2(β 12ρ −2
w
ρc
4
2)
≤Vˆ 1. (37)
x x w
Thisimpliesthat
3+ρ2 Vˆ β2ρ2c 1 Vˆ
w 1 + w 4 ≤ 1 . (38)
4 (k˜+K)2 µ2 (k˜+K)2 (k˜+K+1)2
x
Thenbyusing(36)andthedefinitionofα in(25),wederivethatV (k˜+1) ≤ Vˆ 1 ,i.e.,(34)holdsfork =k˜+1.
k 1 (k˜+K+1)2
Thenthelemmaisproved. □
InlightofLemma4.1.1andotherauxiliaryresults,weestablishtheO(1)convergencerateofexpectedoptimiza-
k
tionerrorU (k)=E[||x¯(k)−x ||2]inthefollowinglemma.
1 ∗
11Lemma4.1.2 LetAssumption2.2.1∼2.2.3hold. Consideralgorithm1withstepsize(25),whereβ>2. Wethenhave
β2c (K +K)1.5β Xˆ
U (k)≤ 5 + 1
1 (1.5β−1)nµ2(k+K) (k+K)1.5β n
+ 3 (β 12 .( 51 β.5 −β 2x − )n1 µ)c 25 + (1.1 52 ββ −L2 x 2Vˆ )n1
µ2
+ (1.1 52 ββ −L θ2 2Vˆ )n2 µ2 · (k+1
K)2
x x x
foranyk≥ K −K,where
1
3M L2 3M L2
c ≜ x xXˆ + x θΘˆ +M¯, (39)
5 n n
Xˆ,K ,Vˆ ,Vˆ ,M¯ aredefinedby(29)(31)(32)(34)(17)respectively.
1 2 1
Proof Sinceα = β by(25),recallingthedefinitionofK andK in(26)and(31),wecanseethatα ≤ β ≤
k µx(k+K) 1 k µK1
β ≤ µx ≤min{ 1 , 1}. ThenLemma3.2.1(B)holds. Togetherwith3.3.1itfollowsthatforanyk≥ K −K,
2µK 6(1+Mx)L2
x
3µx Lx 1
U (k+1)≤(1− 3 α µ )U (k)+ 6α kL2 xV (k)+ 6α kL θ2 V (k)+ α2 kc 5 . (40)
1 2 k x 1 nµ 1 nµ 2 n
x x
Recallingthedefinitionofα = β ,wehave
k µx(k+K)
U (k+1)≤(1−
3β
)U (k)+
6βL2 xV 1(k)
+
6βL θ2V 2(k)
+
β2c
5 ·
1
. (41)
1 2(k+K) 1 nµ2(k+K) nµ2(k+K) nµ2 (k+K)2
x x x
Thus
(cid:89)k+K−1 3β
U (k)≤ (1− )U (K )
1 t=K1+K 2t 1

1

+(cid:88)k t=+ KK 1− +1 K(cid:89)k j=+ tK +− 11 (1− 3 2β j)6 nβ µL
2
x2 x · V 1(t t−K) + 6 nβ µL
2
xθ2 · V 2(t t−K) + β n2 µc
2
x5 · t1 2.
Recallfrom[30,lemma11]thatforany∀1< j<k, j∈Nand1<γ≤ j/2,(cid:81)k−1(1− γ)≤ jγ . Thenweachieve
t=j t kγ
(K +K)1.5β
U (k)≤ 1 U (K )
1 (k+K)1.5β 1 1
 
+(cid:88)k t=+ KK 1− +1
K
(( kt ++ K1) )1 1.5 .5β
β
6 nβ µL
2
x2 x · V 1(t t−K) + 6 nβ µL
2
xθ2 · V 2(t t−K) + β n2 µc
2
x5 · t1 2
=
(K 1+K)1.5β
U (K )+
6βL θ2 (cid:88)k+K−1 (t+1)1.5βV 2(t−K)
+
(k+K)1.5β 1 1 nµ2 x(k+K)1.5β t=K1+K t
6βL2
x
(cid:88)k+K−1 (t+1)1.5βV 1(t−K)
+
β2c
5
(cid:88)k+K−1 (t+1)1.5β
.
nµ2 x(k+K)1.5β t=K1+K t nµ2 x(k+K)1.5β t=K1+K t2
InlightofLemma4.1.1,wehaveV (k−K)≤ Vˆ 1 andV (k−K)≤ Vˆ 2 foranyk≥ K −K. Hence
1 k2 2 k2 1
U (k)≤
β2c
5
(cid:88)k+K−1 (t+1)1.5β
+
(K 1+K)1.5β
U (K )
1 nµ2 x(k+K)1.5β t=K1+K t2 (k+K)1.5β 1 1
+
6βL2 xVˆ
1
(cid:88)k+K−1 (t+1)1.5β
+
6βL θ2Vˆ
2
(cid:88)k+K−1 (t+1)1.5β
. (42)
nµ2 x(k+K)1.5β t=K1+K t3 nµ2 x(k+K)1.5β t=K1+K t3
Bytheproofin[30,lemma12],whenb>a≥ K ,wehave
1
(cid:88)b (t+1)1.5β b1.5β−1 3(1.5β−1)b1.5β−2 (cid:88)b (t+1)1.5β 2b1.5β−2
≤ + , ≤ . (43)
t=a t2 1.5β−1 1.5β−2 t=a t3 1.5β−2
12Thus
β2c 3β2(1.5β−1)c 1 (K +K)1.5β
U (k)≤ 5 + 5 · + 1 U (K )
1 (1.5β−1)nµ2(k+K) (1.5β−2)nµ2 (k+K)2 (k+K)1.5β 1 1
x x (44)
+ 12βL2 xVˆ 1 · 1 + 12βL θ2Vˆ 2 · 1 .
(1.5β−2)nµ2 (k+K)2 (1.5β−2)nµ2 (k+K)2
x x
RecallingLemma3.3.2yieldsthedesiredresult. □
4.2. RateEstimate
In this subsection, we will discuss the factors that affect the convergence rate of the algorithm, especially the
networksizen,thespectralgap(1−ρ
),thesummationofinitialoptimizationerrors(cid:80)n
||x(0)−x ||2andconsensus
errors
(cid:80)n
||θ(0)−θ ||2, and the
hetew
rogenous of computational functions and
learnii n= g1 fui nctions∗
characterized by
(cid:80)n
||∇
i f=1
(x
,i
θ )||2
an∗ d(cid:80)n
||∇h(θ )||2. Firstly,weboundtheconstantsappearinginLemmas4.1.1and4.1.2bythe
i=1 x i ∗ ∗ i=1 i ∗
aforementionedfactors. Wethenutilizethemtosimplifythesublinearrateoftheexpectedoptimizationerror,based
onwhich,wecanimprovetheconvergencerateandderivethemainresultforAlgorithm1.
Lemma4.2.1 DenoteA
≜(cid:80)n
||x(0)−x ||2,B
≜(cid:80)n
||∇ f(x ;θ )||2,
1 i=1 i ∗ 1 i=1 x i ∗ ∗
A ≜(cid:80)n ||θ(0)−θ ||2,andB ≜(cid:80)n ||∇h(θ )||2. Thentheordersofconstants’Xˆ (29),Θˆ (27),Vˆ (34),Vˆ (32),c
2 i=1 i ∗ 2 i=1 i ∗ 1 2 4
(35)andc (39)areasfollow.
5
Xˆ =O(A +A +B +B +n), Θˆ =O(A +B +n),
1 2 1 2 2 2
(cid:32) A +A +B +B +n(cid:33) (cid:32) A +B +n(cid:33)
Vˆ =O 1 2 1 2 , Vˆ =O 2 2 ,
1 (1−ρ )2 2 (1−ρ )2
(cid:32) A +A +Bw +B +n(cid:33) (cid:18)A +A w +B +B +n(cid:19)
c =O 1 2 1 2 , c =O 1 2 1 2 .
4 1−ρ 5 n
w
Proof TheupperboundofΘˆ andVˆ dealonlywithunknownparameterθ,whichcanbeinheriteddirectlyfrom[30,
2
lemma13]. AsforXˆ,recalling(29)wehave
11L2Θˆ 11||∇ F(111xT,111θT)||2 7nσ2
Xˆ ≤||xxx(0)−111xT||2+ θ + x ∗ ∗ + x
∗ µ2 µ2 9(1+M )L2 (45)
x x x x
=O(A +A +B +B +n).
1 2 1 2
Fromthedefinitionofc in(35),itfollowsthat
4
(cid:32) 3 (cid:33) (cid:32) A +A +B +B +n(cid:33)
c =3 +M (L2Xˆ +L2Θˆ +||∇ F(111xT,111θT)||2)+nσ2 =O 1 2 1 2 (46)
4 1−ρ2 x x θ x ∗ ∗ x 1−ρ
w w
(cid:16) (cid:17)
Notefrom(31)and(26)thatK =O 1 . Thenby(34),weobtain
1 1−ρw
(cid:40) 8β2ρ2c (cid:41) (cid:32) A +A +B +B +n(cid:33)
Vˆ =max 4K2Xˆ, w 4 =O 1 2 1 2 . (47)
1 1 µ2(1−ρ2) (1−ρ )2
x w w
Inlightofequation(39),wecanachieve
c =
3M xL2
xXˆ +
3M xL θ2
Θˆ +M¯
=O(cid:18)A 1+A 2+B 1+B 2+n(cid:19)
. (48)
5 n n x n
□
Thesimplificationoftheseconstantsmakesitconvenientforlateranalysis. Inlightofrelation(34),sinceVˆ isthe
1
onlyconstant, theconvergenceresultofexpectedconsensuserrorV (k)canbeeasilyobtained. Whiletheexpected
1
optimizationerrorU (k)needstobereformulatedmoreconcisely.
1
13Corollary4.1 LetAssumption2.2.1∼2.2.3hold. Consideralgorithm1withstepsizepolicy(25),whereβ > 2. Then
weobtainfrom[30,Corollary1]that
β2c′ 1 c′
U (k)≤ 5 · + 6 , ∀k≥ K −K,
2 (1.5β−1)nµ2 (k+K) (k+K)2 1
x
wherec′ ≜ 2MθL θ2Θˆ +2Mθ(cid:80)n i=1∥∇hi(θ∗)∥2+σ2,c′ =O(cid:16) A2+B2+n(cid:17) .Basedonwhich,wefurtherhavethatforanyk≥ K −K,
5 n n θ 6 n(1−ρw)2 1
β2c 1 c
U (k)≤ 5 · + 6 , (49)
1 (1.5β−1)nµ2 (k+K) (k+K)2
x
(cid:16) (cid:17)
wherec isdefinedin(39),andc =O A1+A2+B1+B2+n .
5 6 n(1−ρw)2
Proof InlightofLemma4.1.2andLemma4.2.1,wecanobtainthat
β2c (K +K)1.5β−2 Xˆ 1
U (k)≤ 5 + 1 ·
1 (1.5β−1)nµ2(k+K) (k+K)1.5β−2 n (k+K)2
+ 3 (β 12 .( 51 β.5 −x β 2− )n1 µ)c 25 + (1.1 52 ββ −L2 x 1Vˆ )n1
µ2
+ (1.1 52 ββ −L θ2 1Vˆ )n2 µ2 · (k+1
K)2
x x x
β2c 1 (cid:18)A +A +B +B +n(cid:19) 1
= 5 · +O 1 2 1 2
(1.5β−1)nµ2 (k+K) n (k+K)2
(cid:34) (cid:18)A +Ax +B +B +n(cid:19) (cid:32) A +A +B +B +n(cid:33) (cid:32) A +B +n(cid:33)(cid:35) 1
+ O 1 2 1 2 +O 1 2 1 2 +O 2 2
n2 n(1−ρ )2 n(1−ρ )2 (k+K)2
β2c 1 (cid:32) A +A +B +B +w n(cid:33) 1 w
≤ 5 · +O 1 2 1 2 .
(1.5β−1)nµ2 (k+K) n(1−ρ )2 (k+K)2
x w
□
Based on this corollary, together with Lemma 3.2.1, we further elaborate the convergence result of Algorithm
1. Especially, we give an upper bound of
1(cid:80)n E(cid:104)
∥x(k)−x
∥2(cid:105)
and formulate it in a way to make an intuitive
n i=1 i ∗
comparisonwiththecentralizedalgorithm.
Theorem4.1 LetAssumption2.2.1∼2.2.3hold. Consideralgorithm1withstepsizepolicy(25),whereβ > 2. Then
foranyk≥ K −K,wehave
1
1(cid:88)n β2M¯
E[||x(k)−x ||2]≤
n i=1 i ∗ (2β−1)nµ2(k+K)
(cid:32) A +A +B +B +n(cid:33) 1x (cid:32) A +A +B +B +n(cid:33) 1
+O 1 √2 1 2 +O 1 2 1 2 , (50)
n n(1−ρ w) (k+K)1.5 n(1−ρ w)2 (k+K)2
whereM¯ isdefinedin(17).
Proof Fork ≥ K −K,byrecallingLemma3.2.1(AAA)andthedefinitionofU (k),V (k)andU (k),V (k)in(19)and
1 1 1 2 2
14(20),wehave
U (k+1)≤(1−α µ )2U (k)+
α2 kL2
x V (k)+
α2 kL θ2
V (k)+
2L xL θα2
k
(cid:112)
V (k)V (k)
1 k x 1 n 1 n 2 n 1 2
2α L (cid:112) 2α L (cid:112)
+ √k x U (k)V (k)+ √k θ U (k)V (k)
1 1 1 2
n n
 
+α2 k3M nx 2L2 x(nU 1(k)+V 1(k))+ 3M nx 2L θ2 (nU 2(k)+V 2(k))+ M n¯ 
(cid:32) 3M L2(cid:33) 3M L2
=(1−2α µ )U (k)+α2 µ2+ x x U (k)+α2· x θU (k)
k x 1 k x n 1 k n 2
+
α2 kL2
x
(cid:32)
1+
3M x(cid:33)
V (k)+
α2 kL θ2 (cid:32)
1+
3M x(cid:33)
V (k)+
2L xL θα2
k
(cid:112)
V (k)V (k)
n n 1 n n 2 n 1 2
2α L (cid:112) 2α L (cid:112)
α2M¯
+ √k x U (k)V (k)+ √k θ U (k)V (k)+ k ,
n 1 1 n 1 2 n
wherethefirstinequalityfollowstheCauchy-Schwarzinequalityintheprobabilisticformandthefactthat
(cid:104) (cid:105)
E[||xxx(k)−111xT||2]=E ∥x(k)−111x¯T +111x¯T −111xT∥2
∗ ∗
(cid:104) (cid:105) (cid:104) (cid:105)
≤2E ∥x(k)−111x¯T∥2 +2E ∥111x¯T −111xT∥2 (51)
∗
(cid:104) (cid:105) (cid:104) (cid:105)
=2E ∥x(k)−111x¯T∥2 +2nE ∥x¯−x ∥2 =2V (k)+2nU (k).
∗ 1 1
Thus,duetoα = β ,wehave
k µx(k+K)
U
(k+1)≤(cid:32)
1−
2β (cid:33)
U (k)+
β2U 1(k) (cid:32)
1+
3M xL2 x(cid:33)
+
3M xL θ2β2U 2(k)
1 k+K 1 (k+K)2 nµ2 nµ2(k+K)2
x x
β2L2 (cid:32) 3M (cid:33) V (k) β2L2 (cid:32) 3M (cid:33) V (k)
+ x 1+ x 1 + θ 1+ x 2
nµ2 n (k+K)2 nµ2 n (k+K)2
x√ √ x √
2L L β2 V (k)V (k) 2βL U (k)V (k) 2βL U (k)V (k) β2M¯ 1
+ x θ 1 2 + √ x 1 1 + √ θ 1 2 +
nµ2 (k+K)2 nµ k+K nµ k+K nµ2 (k+K)2
x x x x
Denotebyc =1+ 3MxL2 x andc =1+ 3Mx . Theninlightof[30,Lemma11],weobtainthat
7 nµ2
x
8 n
(cid:32) (cid:33) (cid:32) (cid:32) (cid:33)(cid:33)(cid:34)
U
(k)≤(cid:89)k+K−1
1−
2β
U (K
)+(cid:88)k+K−1 (cid:89)k+K−1
1−
2β β2M¯
1 t=K1+K t 1 1 t=K1+K i=t+1 i nµ2 xt2
√
+
3M xL θ2β2U 2(t−K)
+
β2L2 xc 8V 1(t−K)
+
β2L θ2c 8V 2(t−K)
+
2L xL θβ2 V 1(t−K)V 2(t−K)
nµ2 t2 nµ2 t2 nµ2 t2 nµ2 t2
x √ x x √ x (cid:35)
β2c U (t−K) 2βL U (t−K)V (t−K) 2βL U (t−K)V (t−K)
+ 7 1 + √ x 1 1 + √ θ 1 2
t2 nµ t nµ t
≤
(K 1+K)2β
U (K
)+(cid:88)kx +K−1 (t+1)2β (cid:34) β2M¯
+
β2c 7Ux 1(t−K) (52)
(k+K)2β 1 1 t=K1+K (k+K)2β nµ2 xt2 √t2
+
β2L2 xc 8V 1(t−K)
+
β2L θ2c 8V 2(t−K)
+
2L xL θβ2 V 1(t−K)V 2(t−K)
nµ2 t2 nµ2 t2 nµ2 t2
+x 3M xL θ2β2U 2(t−K)x
+
2βL x√ U 1( √t−K)Vx 1(t−K)
+
2βL θ√ U 1( √t−K)V 2(t−K)(cid:35)
.
nµ2t2 nµ t nµ t
x x x
15AccordingtoCorollary4.1andLemma4.1.1,wehave
U (k)≤
(K 1+K)2β
U (K )+
1
·
β2M¯ (cid:88)k+K−1 (t+1)2β
1
+
β2( ck 7+K (cid:88))2β k+K1
−1
1 (t+1( )k 2β+ (cid:34)K)2β β2n cµ 52
x
·t= 1K1 ++K
c
6(cid:35)
t2
+
( nk µ3+
2
xM
(kK
x
+L)2 θ2β
Kβ2
)2βt (cid:88)=K1
k
t+ =+K
KK 1− +1
Kt (2
t+
t21)2( β1.

5 (β 1.− 5β1 β) −2n cµ
1′
52
x
)nµ2
θt
· 1
tt2
+ c t2′ 6 
+
β2L2 xc
8
(cid:88)k+K−1 (t+1)2β
·
Vˆ
1
nµ2 x(k+K)2β t=K1+K t2 t2
(cid:112)
+ β2L θ2c 8 (cid:88)k+K−1 (t+1)2β · Vˆ 2 + 2L xL θβ2 (cid:88)k+K−1 (t+1)2β Vˆ 1Vˆ 2
nµ2 x(k+K)2β t=K1+K t2 t (cid:115)2 nµ2 x(k+K)2β t=K1+ (cid:115)K t2 t2
+ √
2βL
x
(cid:88)k+K−1 (t+1)2β β2c
5 ·
1
+
c
6
Vˆ
1
nµ x(k+K)2β t=K1+K t (1.5β−1)nµ2
x
t t2 t2
(cid:115) (cid:115)
+ √
2βL
θ
(cid:88)k+K−1 (t+1)2β β2c
5 ·
1
+
c
6
Vˆ
2.
nµ x(k+K)2β t=K1+K t (1.5β−1)nµ2
x
t t2 t2
√ √ √
Since a+b≤ a+ b,wecanachieve
(cid:115) (cid:115) (cid:115) (cid:112)
β2c 1 c Vˆ c Vˆ 1 c Vˆ
5 · + 6 · 1 ≤β 5 1 · + 6 1, (53)
(1.5β−1)nµ2 t t2 t2 (1.5β−1)nµ2 t1.5 t2
x x
then
U 1(k)≤ β2M (¯
k
(cid:80) +k t=+ KKK )1− 2+1 βK nµ(t+
2
xt1 2)2β + (K 1+ (kK +)2 KβU )2β1(K 1) + 2β2√ (cid:112)c 5( (cid:112)L 1x .(cid:112) 5βVˆ −1+ (cid:113)1L ×θ n(cid:112) µV
2
xˆ
(
2 k) +(cid:80) tk K=+ KK )21− + β1 K (t+ t21 .5)2β
+ (k+1
K)2β
(1.5β β4 −c 5 1c 7
)nµ2
x
+ (1.53 βM −xβ 14 )L nθ2 2c µ′ 5
2 xµ2
θ
+ 2β(L x c 6V √ˆ 1 n+
µ
xL θ c′ 6Vˆ 2)(cid:88) tk =+ KK 1− +1
K
(t+ t31)2β
+ (k+1
K)2β
 β2c 6c 7+ 3M x nβ µ2
2
xL θ2c′ 6 + β2(L2 xVˆ 1 n+
µ2
xL θ2Vˆ 2)c 8 + 2L xL θ nβ µ2
2
x(cid:112) Vˆ 1Vˆ 2 (cid:88) tk =+ KK 1− +1
K
(t+ t41)2β .
Recall(43)andnotethat
(cid:88)b (t+1)2β ≤(cid:88)b 2(t+1)2β ≤(cid:90) b+1
2t2β−2.5dt≤
2(b+1)2β−1.5
,
(cid:88)t b=a (t+t2. 15
)2β
≤(cid:88)t b=a 2(t (t+ +1 1)2 )2.5
β
≤(cid:90)a+ b1
+1
2t2β−4dt≤
2(b2 +β 1− )2β1 −.5
3
, ∀a≥16.
(54)
t=a t4 t=a (t+1)4
a+1
2β−4
Thenbynoticingthatc =c =O(1)andusingLemma4.2.1,wehave
7 8
β2M¯ (cid:32) A +A +B +B +n(cid:33) 1 (cid:32) A +A +B +B +n(cid:33) 1
U (k)≤ +O 1 √2 1 2 +O 1 2 1 2
1
(cid:32)
A(2 +β A−1 +)n Bµ2
x
+(k B+K +)
n(cid:33)
1
n n( (cid:32)1 A−ρ +w)
A
+B( +k+ BK +)1 n.5
(cid:33)
1
n(1−ρ w)2 (k+K)2
+O 1 2 1 2 +O 1 2 1 2
n(1−ρ )2 (k+K)3 n(1−ρ )2β (k+K)2β
β2M¯ w (cid:32) A +A +B +B +n(cid:33) 1w (cid:32) A +A +B +B +n(cid:33) 1
= +O 1 √2 1 2 +O 1 2 1 2 .
(2β−1)nµ2 x(k+K) n n(1−ρ w) (k+K)1.5 n(1−ρ w)2 (k+K)2
16Byrecalling(51),wehave 1(cid:80)n E[||x(k)−x ||2]≤2U (k)+ 2V1(k).Thistogetherwith(34)andtheestimateofVˆ in
n i=1 i ∗ 1 n 1
Lemma4.2.1provetheresult. □
Inlightofrelation(50),byrecallingthedefinitionsofA ,A ,B ,B inLemma4.2.1,wecanseethattheconver-
1 2 1 2
gencerateisproportionaltoinitialerrorsforbothcomputationalproblem(cid:80)n ∥x(0)−x ∥2 andparameterlearning
problem(cid:80)n ∥θ(0)−θ ∥2. Itisworthnotingthattheheterogeneityofagents’i= i1 ndivi idualc∗ ostfunctions,measuredby
B = (cid:80)n |i |= ∇1 fi (x ;θ )|∗ |2, B = (cid:80)n ||∇h(θ )||2,alsoinfluencetheconvergencerateinasimilarway. Thoughθ ,x
ar1 erespi e= c1 tivex lyi th∗ eo∗ ptimal2 solutioi= n1 stomi in∗ 1(cid:80)n h(θ)andmin 1(cid:80)n f (x;θ∗), theyareusuallynottheopti∗ ma∗ l
θ n i=1 i x n i=1 i
solutiontoeachlocalfunctionh(θ), f(x,θ).Therefore,thebiggerthedifferencebetweenthelocalcosts,theslower
i i
theconvergencerateofthealgorithm.
Remark2 Herewegivesomecommentsregeradingtheinfluenceofthenetworksizenandthespectralgap(1−ρ )
w
ontheconvergencerate. SinceA ,A ,B andB areallO(n),wecansimplifytherelation(50)asfollow.
1 2 1 2
(cid:18) (cid:19)
(cid:16) (cid:17)
1(cid:88)n E[||x(k)−x ||2]≤ β2M¯ + O √ n(11 −ρw) + O (1−1 ρw)2 . (55)
n i=1 i ∗ (2β−1)nµ2(k+K) (k+K)1.5 (k+K)2
x
Itisnoticedthatthealgorithmconvergesfasterforbetternetworkconnectivity(i.e.,smallerρ ). Forexample,afully
w
connectedgraphisthemostefficientconnectiontopologysinceρ = 0. Incontrast,itholds1−ρ → 0asn → ∞
w w
for the cycle graph, which indicates that the algorithm will converge very slowly for large-scale cycle graphs. The
following table taken from [35, Chapter 4] characterizes the relation between network size n and the spectral gap.
Consideringpluggingtheorderconcerningnfromthetableintorelation(55),wemayobtainthequantitiveinfluence
ofthenetworksizeontheconvergencerate.
Table2:Relationbetweenthenetworksizenandthespectralgap1−ρw
NetworkTopology SpectralGap(1−ρw) NetworkTopology SpectralGap(1−ρw)
PathGraph O(1) 2D-meshGraph O(1)
n2 n
CycleGraph O(1) CompleteGraph 1
n2
ThereareotherfactorssuchasthestrongconvexityandLipschitzsmoothnessparameters,aswellasthevariance
ofthestochasticgradient,allofwhichcanalsoaffecttheconvergencerate. Wewillnotincludeaquantitativeanalysis
ofthesefactorssincethebigOconstantintheconvergencerateisalreadyquitecomplexandweoftenusetherelation
likeµ ≤ L forsimplicity. Whilesomeintuitivepropertycanbenaturallyobtainedfrom(55): thelargerconvexity
x x
and Lipschitz smoothness parameters can lead to the faster rate; the higher variance of stochastic gradient descent
leadstoalowerconvergenceratesincetermM¯ definedby(17)getsbigger.
4.3. TransientTime
Inthissubsection,wewillestablishthetransientiterationneededfortheCDSAalgorithmtoreachitsdominant
rate.
Firstly,werecalltheconvergenceratefrom[30,Theorem2]forthecentralizedstochasticgradientdescent,
(cid:32) (cid:33)
(cid:104) (cid:105) β2M¯ 1 1
E ∥x(k)−x ∥2 ≤ +O . (56)
∗ (2β−1)nµ2k n k2
Comparingitto(55),wemayconcludethatourdistributedalgorithmconvergestotheoptimalsolutionatacomparable
rate to the centralized algorithm, since they are both of the same order O(1). Besides, our work demonstrates that
k
thenetworkconnectivityρ doesnotinfluencethetermO(1),itonlyappearsinhigher-ordertermsO( 1 )andO(1).
w k k1.5 k2
Thoughourdistributedalgorithmasymptoticallyreachesthesameorderofconvergencerateasthatofthecentralized
algorithm, it’sunclearhowmanyiterationsittakestoreachthedominateorderO(1)sincetherearetwoextraerror
k
terms O( 1 ) and O(1) induced by averaging consensus. We refer to the number of iterations before distributed
k1.5 k2
stochasticapproximationmethodreachesitsdominantrateastransientiterations,i.e.,wheniterationkisrelatively
17small, the terms other than n and k still dominate the convergence rate[36, Section 2]. The next theorem state the
iterationsneededforAlgorithm1toreachitsdominantrate.
Theorem4.2 LetAssumption2.2.1∼2.2.3hold,andsetstepsizeas(25),whereβ>2. IttakesK =O(cid:0) n (cid:1) itera-
tioncountsforalgorithm1toreachtheasymptoticrateofconvergence,i.e. whenk ≥ K
,wehaT
ve
1(cid:80)n(1−ρ
Ew
[)2
||x(k)−
T n i=1 i
x ||2]≤ β2M¯ O(1).
∗ (2β−1)nµ2 xk
(cid:16) (cid:17)
Proof Recallingrelationineq.(55),weseethatforanyk≥O n ,
(1−ρw)2
β2M¯ 1 1
≥O(√ ) ,
(2β−1)nµ2(k+K) n(1−ρ ) (k+K)1.5
x (cid:32) w(cid:33)
β2M¯ 1 1
≥O .
(2β−1)nµ2(k+K) (1−ρ )2 (k+K)2
x w
□
5. Experiments
Inthissection,wewillprovidenumericalexamplestoverifyourtheoreticalfindings,andcarryoutexperiments
byBluefog1. ItisapythonlibrarythatcanbeconnectedtotheNVIDIACollectiveCommunicationsLibrary(NCCL)
formulti-GPUcomputingorMessagePassingInterface(MPI)libraryformulti-CPUcomputing[37],i.e.,eachagent
inourdistributedexperimentscenarioisCPU.
5.1. RidgeRegression
Considerthefollowingridge-distributedregressionproblemwithanunknownregularizationparameterθ ,
∗
C (θ
):min(cid:88)n
E
(cid:20)(cid:16) uTx−v(cid:17)2+θ ||x||2(cid:21)
,
x ∗ x∈Rp i=1 ui,vi i i ∗
whereθ canbeobtainedbythedistributedlearningproblembelow,
∗
(cid:88)n
L :θ =argmin (θ−α)2.
θ ∗ i=1 i
Specially,foragenti∈N ≜{1,··· ,n},itslocalobjectivefunctionsarespecifiedas
f(x;θ)=minE
(cid:20)(cid:16) uTx−v(cid:17)2+θ||x||2(cid:21)
,h(θ)=min(θ−α)2.
i
x
ui,vi i i i
θ
i
Here(u,v)aredatasamplecollectedbyeachagenti,whereu ∈ Rp arethesamplefeatures,whilev ∈ Rrepresent
i i i i
theobservedoutputs.
Parameter settings. Set p = 5 and suppose that for all i ∈ N, each component of u ∈ Rp is an independent
i
identical distribution in U(−0.5,0.5), and v
i
is drawn according to v
i
= uT
i
(cid:101)x
i
+ϵ i, where ϵ
i
is an gaussian random
v eaa sri ia lybl ce as lcp ue lc ai tfi ee dd thb ay tN th( e0, o0 p. t0 i1 m) a, la sn od lu(cid:101)x
ti
io= ns( a1 re3 θ5 4 =9 0) .0is 05a (npr +ed 1e )fi ,n ae nd dp xaram =et (cid:104)e (cid:80)r.
n
S Eet (α
ui
u= T)0 +.0 n1 θ× I(cid:105)i E. I (t uc ua Tn )b =e
∗ ∗ i=1 ui i i ∗ ui i i
11 2( 11
2
+θ ∗)−1 n1(cid:80)n i=1(cid:101)x i.
We compare the performance of Algorithm 1 under the path graph and complete graph topology with different
networksizen. Inlightoftheresultsintable2ofthepathgraphandcompletegraph,convergencerateestimationcan
bereformulated.
1(cid:88)n β2M¯ O(n3/2) O(n2)
Path: E[||x(k)−x ||2]≤ + + , (57)
n i=1 i ∗ (2β−1)nµ2(k+K) (k+K)1.5 (k+K)2
x √
1(cid:88)n β2M¯ O(1/ n) 1
Complete: E[||x(k)−x ||2]≤ + + . (58)
n i=1 i ∗ (2β−1)nµ2(k+K) (k+K)1.5 (k+K)2
x
1https://github.com/Bluefog-Lib/bluefog
18WerunAlgorithm1,wheretheinitialvaluesaresetas(x(0),θ(0))=(000 ,1)∀i,andtheweightedadjacencymatrix
i i 5
ofthecommunicationnetworkisbuiltaccordingtotheMetropolis-Hastingsrule[12]. Accordingto(25),wechoose
thestepsizesasα =γ = 20 foranyk≥0.
k k k+20
(a.1)n=10pathgraphtopology (b.1)n=10completegraphtopology
n=10 n=10
102 n=15 102 n=15
n=20 n=20
n=25 n=25
101 n=30 101 n=30
100 100
101 101
102 102
103 103
0 10000 20000 30000 40000 50000 60000 70000 0 10000 20000 30000 40000 50000 60000 70000
 N  N
(a.2)Theperformanceofpathgraph (b.2)Theperformanceofcompletegraph
Figure2:TheperformanceofCDSAbetweenpathgraphandcompletegraphtopology.Theresultsareaveragedover200MonteCarlosampling.
We demonstrate the empirical results in Fig. 2, where the empirical mean-squared error 1(cid:80)n E[||x(k)− x ||2]
n i=1 i ∗
is calculated by averaging through 200 sample paths. We can see from the Subfigure (a.2) that for the path graph,
whentheiteratekissmall,thelargernetworksizenwillleadtothehighermean-squarederror 1(cid:80)n E[||x(k)−x ||2].
n i=1 i ∗
However,withtheincreaseofk,weobserveaphasetransitionthatalargernetworksizenwillleadtoasmallermean-
squarederror 1(cid:80)n E[||x(k)−x ||2](namelyfasterconvergencerate).Thisphenomenonmatchesthetheoreticalresult
n i=1 i ∗
(57): when k is small, the main factor influencing the convergence rate is the second and third term concerning the
networksizenviathedistributedconsensusprotocol,whilewhenkislarge,thefirstterminheritedfromcentralized
stochasticgradientdescentdominatestheconvergencerate.
Comparedittotheempiricalperformanceofthecompletegraphshowninsubfigure(b.2),wecanfindthatfrom
thebeginningtotheend,alargernetworkngeneratessmallererrors,whichalsomatches(58).
19
n
]2
*x
)k(ix
[
1 n
1=i
n
]2
*x
)k(ix
[
1 n
1=i5.2. LogisticRegression
Wefurtherconsiderconvexbutnotstronglyconvexproblem,anduselogisticregressiontodemonstratethatour
algorithmcanalsoleadstoasymptoticconvergence.
Considerthebinaryclassificationvialogisticregressionwithunknownregularizationparameterθ ,
∗
C η(θ ∗): m
ηin(cid:88)n i=1(cid:88)m
j=i
1ln(cid:16) 1+e−ηTxijlij(cid:17)
+
θ
2∗||η||2,
whereθ canbeobtainedbyadistributedparameterlearningproblemasfollow,
∗
(cid:88)n
L : θ =argmin (θ−α)2.
θ ∗ i=1 i
Asforagenti,itsitsownlocalcomputationalproblemandparameterlearningproblemareasfollows.
f i(η;θ)=m
ηin(cid:88)m
j=i
1ln(cid:16) 1+e−ηTxijlij(cid:17)
+
2θ
n∗||η||2, h i(θ)=m θin(θ−α i)2.
In this scenario, we set α = 0.01×i and let each agent i ∈ N possess dataset D ≜ {(x ,l ) : j = 1,···m},
i i ij ij i
where x representsathree-dimensionalsamplefeaturewherethefirstdimensionis1andtheothertwodimension
ij
areselectedfromN((1,0)T,III)orN((0,1)T,III),whilel istherelatedsamplelabel1or−1respectively. Supposethat
ij
everyagentholdsanumberofpositivesamplesandnegativesampleswhichonlyaccessibletoitself.
100
Cycle Graph
Complete Graph
Path Graph
2D-mesh Graph
10 1
10 2
10 3
10 4
0 1000 2000 3000 4000 5000
 N
Figure3:TheperformanceofCDSAof25agentsunderfourtopologiesintable2forbinaryclassificationvialogisticregression.Theresultsare
averagedover200MonteCarlosampling.
WenowcomparetheempiricalperformanceofAlgorithm1underfourclassesofgraphtopologies, pathgraph,
cyclegraph,2D-meshgraph,andcompletegraph.Wesetn=25andrunAlgorithm1withinitialvalues(η(0),θ(0))=
i i
000 foralli∈N,wherethestepsizeandweightedadjacencymatrixaresetthesameasRidgeRegression.Theempirical
4
resultsareshowninfig.3,whichshowsthatthecompletegraphhasbestperformance,2D-meshgraphhasthesecond-
bestperformance,whilethepathgraphdisplaystheworstperformance. Theseempiricalfindingsmatchthatlistedin
table2,wherethe2D-meshgraphhasalargerspectralgapthanthepathgraphandcyclepath,henceleadstoalower
mean-squarederror.
20
n
]2
*x
)k(ix
[
1 n
1=i6. Conclusions
Inthiswork,weconsiderthedistributedoptimizationproblemmin 1(cid:80)n f(x;θ∗)withtheunknownparameter
θ∗
collaborativelysolvedbyadistributedparameterlearningproblemmx in
n
1i= (cid:80)1 ni
h(θ). Eachagentonlyhasaccess
θ n i=1 i
toitslocalcomputationalproblem f(x,θ)anditsparameterlearningproblemh(θ). Weproposeacoupleddistributed
i i
stochastic approximation algorithm for resolving this special distributed optimization, where agents can exchange
informationaboutdecisionvariablesxandlearningparameterθwithneighborsoveraconnectednetwork.Wequanti-
tativelycharacterizethefactorsthatinfluencetherateofconvergence,andvalidatesthatthealgorithmasymptotically
achievestheoptimalnetwork-independentconvergenceratecomparedtothecentralizedalgorithmscheme. Inaddi-
tion, we analyze the transient time K , and show that when the iterate k ≥ K , the dominate factor influencing the
T T
convergence rate is related to stochastic gradient descent, while for small k < K , the main factor influencing the
T
convergence rate originates from the distributed average consensus method. Future work will consider more gen-
eral problems under weakened assumptions. It is of interests to explore the accelerated algorithm to obtain a faster
convergencerate.
References
[1] G.Binetti,A.Davoudi,D.Naso,B.Turchiano,F.L.Lewis,Adistributedauction-basedalgorithmforthenonconvexeconomicdispatch
problem,IEEETransactionsonIndustrialInformatics10(2)(2014)1124–1132.doi:10.1109/TII.2013.2287807.
[2] P.Yi,Y.Hong,F.Liu,Initialization-freedistributedalgorithmsforoptimalresourceallocationwithfeasibilityconstraintsandapplicationto
economicdispatchofpowersystems,Automatica74(2016)259–269.doi:https://doi.org/10.1016/j.automatica.2016.08.007.
[3] A.Corte´s,S.Mart´Inez,Aprojection-baseddecompositionalgorithmfordistributedfastcomputationofcontrolinmicrogrids,SIAMJournal
onControlandOptimization56(2)(2018)583–609.doi:10.1137/15M103889X.
URLhttps://doi.org/10.1137/15M103889X
[4] S.Sahyoun, S.M.Djouadi, K.Tomsovic, S.Lenhart, OptimalDistributedControlforContinuumPowerSystems, pp.416–422. doi:
10.1137/1.9781611974072.57.
URLhttps://epubs.siam.org/doi/abs/10.1137/1.9781611974072.57
[5] L.-N.Liu,G.-H.Yang,Distributedoptimaleconomicenvironmentaldispatchformicrogridsovertime-varyingdirectedcommunicationgraph,
IEEETransactionsonNetworkScienceandEngineering8(2)(2021)1913–1924.doi:10.1109/TNSE.2021.3076526.
[6] V.Krishnan,S.Mart´ınez,Distributedcontrolforspatialself-organizationofmulti-agentswarms,SIAMJournalonControlandOptimization
56(5)(2018)3642–3667.doi:10.1137/16M1080926.
URLhttps://doi.org/10.1137/16M1080926
[7] T.Skibik,M.M.Nicotra,Analysisoftime-distributedmodelpredictivecontrolwhenusingaregularizedprimal–dualgradientoptimizer,
IEEEControlSystemsLetters7(2022)235–240.doi:10.1109/LCSYS.2022.3186631.
[8] Y.-L.YangTao, XuLei, Event-triggereddistributedoptimizationalgorithms, ActaAutomaticaSinica48(1)(2022)133–143. doi:10.
16383/j.aas.c200838.
[9] A.Nedic,Distributedgradientmethodsforconvexmachinelearningproblemsinnetworks:Distributedoptimization,IEEESignalProcessing
Magazine37(3)(2020)92–101.doi:10.1109/MSP.2020.2975210.
[10] S.A.Alghunaim, A.H.Sayed, Distributedcoupledmultiagentstochasticoptimization, IEEETransactionsonAutomaticControl65(1)
(2020)175–190.doi:10.1109/TAC.2019.2906495.
[11] B.Touri,B.Gharesifard,Aunifiedframeworkforcontinuous-timeunconstraineddistributedoptimization,SIAMJournalonControland
Optimization61(4)(2023)2004–2020.doi:10.1137/21M1442711.
URLhttps://doi.org/10.1137/21M1442711
[12] G.Notarstefano,I.Notarnicola,A.Camisa,Distributedoptimizationforsmartcyber-physicalnetworks,FoundationsandTrendsinSystems
andControl7(3)(2020)253–383.doi:10.1561/2600000020.
[13] X. Meng, Q. Liu, A consensus algorithm based on multi-agent system with state noise and gradient disturbance for distributed convex
optimization,Neurocomputing519(2023)148–157.doi:https://doi.org/10.1016/j.neucom.2022.11.051.
[14] N.S.Aybat,E.Y.Hamedani,Adistributedadmm-likemethodforresourcesharingovertime-varyingnetworks,SIAMJournalonOptimiza-
tion29(4)(2019)3036–3068.doi:10.1137/17M1151973.
URLhttps://doi.org/10.1137/17M1151973
[15] L.Carlone,V.Srivastava,F.Bullo,G.C.Calafiore,Distributedrandomconvexprogrammingviaconstraintsconsensus,SIAMJournalon
ControlandOptimization52(1)(2014)629–662.doi:10.1137/120885796.
URLhttps://doi.org/10.1137/120885796
[16] N.S.Aybat,H.Ahmadi,U.V.Shanbhag,Ontheanalysisofinexactaugmentedlagrangianschemesformisspecifiedconicconvexprograms,
IEEETransactionsonAutomaticControl67(8)(2021)3981–3996.
[17] D.Bertsimas,D.B.Brown,C.Caramanis,Theoryandapplicationsofrobustoptimization,SIAMreview53(3)(2011)464–501.
[18] A.Ben-Tal,L.ElGhaoui,A.Nemirovski,Robustoptimization,Princetonuniversitypress,2009.
[19] D.Bertsimas,V.Gupta,N.Kallus,Data-drivenrobustoptimization,MathematicalProgramming167(2018)235–292.
[20] C.Jie,L.Prashanth,M.Fu,S.Marcus,C.Szepesva´ri,Stochasticoptimizationinacumulativeprospecttheoryframework,IEEETransactions
onAutomaticControl63(9)(2018)2867–2882.
[21] A.Shapiro,D.Dentcheva,A.Ruszczynski,Lecturesonstochasticprogramming:modelingandtheory,SIAM,2021.
21[22] C.Wilson,V.V.Veeravalli,A.Nedic´,Adaptivesequentialstochasticoptimization,IEEETransactionsonAutomaticControl64(2)(2018)
496–509.
[23] H.Jiang, U.V.Shanbhag, Onthesolutionofstochasticoptimizationandvariationalproblemsinimperfectinformationregimes, SIAM
JournalonOptimization26(4)(2016)2394–2429.
[24] N.Ho-Nguyen,F.Kılınc¸-Karzan,Exploitingproblemstructureinoptimizationunderuncertaintyviaonlineconvexoptimization,Mathemat-
icalProgramming177(1-2)(2018)113–147.doi:10.1007/s10107-018-1262-8.
[25] H.Ahmadi,U.V.Shanbhag,Ontheresolutionofmisspecifiedconvexoptimizationandmonotonevariationalinequalityproblems,Compu-
tationalOptimizationandApplications77(1)(2020)125–161.
[26] N.Liu,L.Guo,Stochasticadaptivelinearquadraticdifferentialgames,arXivpreprintarXiv:2204.08869(2022).
[27] A.Kannan,A.Nedic´,U.V.Shanbhag,Distributedstochasticoptimizationunderimperfectinformation,in:201554thIEEEConferenceon
DecisionandControl(CDC),IEEE,2015,pp.400–405.
[28] I.Notarnicola,A.Simonetto,F.Farina,G.Notarstefano,Distributedpersonalizedgradienttrackingwithconvexparametricmodels,IEEE
TransactionsonAutomaticControl68(1)(2023)588–595.doi:10.1109/TAC.2022.3147007.
[29] J.Du,Y.Liu,Y.Zhi,H.Gao,Computationalconvergencerateanalysisofdistributedoptimizationalgorithm,in:InternationalConferenceon
Guidance,NavigationandControl,Springer,2022,pp.5288–5299.
[30] S.Pu,A.Olshevsky,I.C.Paschalidis,Asharpestimateonthetransienttimeofdistributedstochasticgradientdescent,IEEETransactionson
AutomaticControl67(11)(2021)5900–5915.
[31] S.Liang, L.Wang, G.Yin, Distributedquasi-monotonesubgradientalgorithmfornonsmoothconvexoptimizationoverdirectedgraphs,
Automatica101(2019)175–181.
[32] L.Bottou,F.E.Curtis,J.Nocedal,Optimizationmethodsforlarge-scalemachinelearning,SIAMReview60(2)(2018)223–311. doi:
10.1137/16M1080173.
URLhttps://doi.org/10.1137/16M1080173
[33] L.Xiao,S.Boyd,Fastlineariterationsfordistributedaveraging,Systems&ControlLetters53(1)(2004)65–78.
[34] G.Qu,N.Li,Harnessingsmoothnesstoacceleratedistributedoptimization,IEEETransactionsonControlofNetworkSystems5(3)(2017)
1245–1260.
[35] F.Bullo,LecturesonNetworkSystems,1.6Edition,KindleDirectPublishing,2022.
URLhttp://motion.me.ucsb.edu/book-lns
[36] B.Ying,K.Yuan,Y.Chen,H.Hu,P.Pan,W.Yin,Exponentialgraphisprovablyefficientfordecentralizeddeeptraining,AdvancesinNeural
InformationProcessingSystems34(2021)13975–13987.
[37] B.Ying,K.Yuan,H.Hu,Y.Chen,W.Yin,Bluefog: Makedecentralizedalgorithmspracticalforoptimizationanddeeplearning,arXiv
preprintarXiv:2111.04287(2021).
Appendix A. ProofofLemma3.1.1
Proof ByusingAssumption2.2.2,weobtainthat
E[||g¯(xxx(k),θθθ(k),ξξξ(k))−∇¯ F(xxx(k),θθθ(k))||2|F(k)]
x
=E(cid:34)(cid:13) (cid:13) (cid:13)1(cid:88)n
g(x(k),θ(k),ξ(k))−
1(cid:88)n
∇
f(x(k),θ(k))(cid:13) (cid:13) (cid:13)2 |F(k)(cid:35)
(cid:13) n i=1 i i i i n i=1 x i i i (cid:13)
1 (cid:88)n (cid:104) (cid:105)
= E ||g(x(k),θ(k),ξ(k))−∇ f(x(k),θ(k))||2|F(k)
n2 i=1 i i i i x i i i
≤
1 (cid:88)n (cid:16)
σ2+M ||∇
f(x(k),θ(k))||2(cid:17)
≤
σ2
x +
M
x(cid:80)n
i=1||∇ xf i(x i(k),θ i(k))||2
, (A.1)
n2 i=1 x x x i i i n n2
wherethesecondequalityusethefactthatξ,∀iareindependentrandomvariables. Byrecallingassumption2.2.1,we
i
achieve
||∇ f(x(k),θ(k))||2 =||∇ f(x(k),θ(k))−∇ f(x ,θ(k))
x i i i x i i i x i ∗ i
+∇ f(x ,θ(k))−∇ f(x ,θ )+∇ f(x ,θ )||2
x i ∗ i x i ∗ ∗ x i ∗ ∗
≤3||∇ f(x(k),θ(k))−∇ f(x ,θ(k))||2+3||∇ f(x ,θ(k))
x i i i x i ∗ i x i ∗ i
−∇ f(x ,θ )||2+3||∇ f(x ,θ )||2
x i ∗ ∗ x i ∗ ∗
≤3L2||x(k)−x ||2+3L2||θ(k)−θ ||2+3||∇ f(x ,θ )||2. (A.2)
x i ∗ θ i ∗ x i ∗ ∗
Combining(A.2)and(A.1)yieldstheresult(16). □
22Appendix B. ProofofLemma3.1.2
Proof Byrecallingthedefinitionofx¯(k),θ¯(k)and∇¯ F(xxx(k),θθθ(k))in(13)and(15),usingAssumption2.2.1,wehave
x
||∇ f(x¯(k),θ¯(k))−∇¯ F(xxx(k),θθθ(k))||
x x
1(cid:88)n 1(cid:88)n
=|| ∇ f(x¯(k),θ¯(k))− ∇ f(x(k),θ(k))||
n i=1 x i n i=1 x i i i
1(cid:88)n
≤ ||∇ f(x¯(k),θ¯(k))−∇ f(x(k),θ(k))||
n i=1 x i x i i i
1(cid:88)n
= ||∇ f(x¯(k),θ¯(k))−∇ f(x(k),θ¯(k))+∇ f(x(k),θ¯(k))−∇ f(x(k),θ(k))||
n i=1 x i x i i x i i x i i i
1(cid:88)n (cid:104) (cid:105)
≤ ||∇ f(x¯(k),θ¯(k))−∇ f(x(k),θ¯(k))||+||∇ f(x(k),θ¯(k))−∇ f(x(k),θ(k))||
n x i x i i x i i x i i i
i=1
1(cid:16) (cid:88)n (cid:88)n (cid:17)
≤ L ||x¯(k)−x(k)||+L ||θ¯(k)−θ(k)||
n x i=1 i θ i=1 i
L L
≤√x ||xxx(k)−111x¯(k)T||+ √θ ||θθθ(k)−111θ¯(k)T||,
n n
wherethelastrelationfollowsfromCauchy-Schwarzinequality. □
Appendix C. ProofofLemma3.2.1
Proof Accordingtothedefinitionsof x¯(k)andg¯(xxx(k),θθθ(k),ξξξ(k))in(13)and(14),togetherwith(cid:80)n w = 1form
i=1 ij
Assumption2.2.3,wehave
1(cid:88)n (cid:18)(cid:88)n (cid:16) (cid:17)(cid:19)
x¯(k+1)= w x (k)−α g (x (k),θ (k),ξ (k))
n i=1 j=1 ij j k j j j j
1(cid:88)n 1(cid:88)n
= x (k)−α · g (x (k),θ (k),ξ (k))= x¯(k)−α g¯(xxx(k),θθθ(k),ξξξ(k)). (C.1)
n j k n j j j j k
j=1 j=1
Thus,
||x¯(k+1)−x ||2 =||x¯(k)−α g¯(xxx(k),θθθ(k),ξξξ(k))−x ||2
∗ k ∗
=||x¯(k)−α ∇¯ F(xxx(k),θθθ(k))−x +α ∇¯ F(xxx(k),θθθ(k))−α g¯(xxx(k),θθθ(k),ξξξ(k))||2
k x ∗ k x k
=||x¯(k)−α ∇¯ F(xxx(k),θθθ(k))−x ||2+α2||∇¯ F(xxx(k),θθθ(k))−g¯(xxx(k),θθθ(k),ξξξ(k))||2
k x ∗ (cid:16)k x (cid:17)
+2α (x¯(k)−α ∇¯ F(xxx(k),θθθ(k))−x )T ∇¯ F(xxx(k),θθθ(k))−g¯(xxx(k),θθθ(k),ξξξ(k)) .
k k x ∗ x
InlightofAssumption2.2.2andLemma3.1.1,bytakingconditionalexpectationonbothsidesofaboveequation,we
have
E[|x¯(k+1)−x ||2|F(k)]|≤||x¯(k)−α ∇¯ F(xxx(k),θθθ(k))−x ||2
∗ k x ∗
 
+α2 k3M nx 2L2 x||xxx(k)−111x ∗T||2+ 3M nx 2L θ2 ||θθθ(k)−111θ ∗T||2+ M n¯ . (C.2)
Next,weboundthefirsttermontherightsideof(C.2).
||x¯(k)−α ∇¯ F(xxx(k),θθθ(k))−x ||2
k x ∗
=||x¯(k)−α ∇ f(x¯(k);θ¯(k))−x +α ∇ f(x¯(k),θ¯(k))−α ∇¯ F(xxx(k),θθθ(k))||2
k x ∗ k x k x
=||x¯(k)−α ∇ f(x¯(k),θ¯(k))−x ||2+α2||∇ f(x¯(k),θ¯(k))−∇¯ F(xxx(k),θθθ(k))||2
k x ∗ k x x
+2α (x¯(k)−α ∇ f(x¯(k),θ¯(k))−x )T(∇ f(x¯(k),θ¯(k))−∇¯ F(xxx(k),θθθ(k)))
k k x ∗ x x
≤||x¯(k)−α ∇ f(x¯(k),θ¯(k))−x ||2+α2||∇ f(x¯(k),θ¯(k))−∇¯ F(xxx(k),θθθ(k))||2
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)x(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)∗(cid:32)(cid:32)(cid:32) k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)x(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)x(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Term1 Term2
+2α ||x¯(k)−α ∇ f(x¯(k),θ¯(k))−x ||×||∇ f(x¯(k),θ¯(k))−∇¯ F(xxx(k),θθθ(k))||. (C.3)
(cid:32)(cid:32)(cid:32)(cid:32)k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)x(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)∗(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)x(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)x(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124) (cid:123)(cid:122) (cid:125)
Term3
AsforTerm1,byleveragingthefactthatα ≤ 1,Lemma2.2.1indicates
k L
||x¯(k)−α ∇ f(x¯(k),θ¯(k))−x ||2 ≤(1−α µ )2||x¯(k)−x ||2. (C.4)
k x ∗ k x ∗
23ByLemma3.1.2,Term2canbeboundedasfollow
(cid:32)
α L ||xxx(k)−111x¯(k)T|| α L
||θθθ(k)−111θ¯(k)T||(cid:33)2
α2||∇ f(x¯(k),θ¯(k))−∇¯ F(xxx(k),θθθ(k))||2 ≤ k x √ + k θ √
k x x n n
α2L2||xxx(k)−111x¯(k)T||2 α2L2||θθθ(k)−111θ¯(k)T||2 2L L α2
≤ k x + k θ + x θ k||xxx(k)−111x¯(k)T||×||θθθ(k)−111θ¯(k)T||. (C.5)
n n n
Finally,Term3canbeboundedbyinvokingthesametransformationapprochesusedinTerm1andTerm2:
(cid:32) (cid:33)
L L
Term3≤2α (1−α µ )||x¯(k)−x || √x ||xxx(k)−111x¯(k)T||√θ ||θθθ(k)−111θ¯(k)T||
k k x ∗
n n
2α L (1−α µ )||x¯(k)−x ||||xxx(k)−111x¯(k)T|| 2α L (1−α µ )||x¯(k)−x ||||θθθ(k)−111θ¯(k)T||
≤ k x k x √ ∗ + k θ k x √ ∗ . (C.6)
n n
Inlightofrelation(C.3)∼(C.6),takingfullexpectationonbothsideofrelation(C.2)yieldstheresult(AAA).Furthermore
byusingmeanvalueinequality2ab≤a2+b2,werearrange(21)andobtainthat
α2L2 α2L2 α2L2 α2L2
U (k+1)≤(1−α µ )2U (k)+ k x V (k)+ k θV (k)+ k x V (k)+ k θV (k)
1 k x 1 n 1 n 2 n 1 n 2
α2L2 1 α2L2 1
+(1−α µ )2c U (k)+ k x · V (k)+(1−α µ )2c U (k)+ k θ · V (k)
k x 1 1 n c 1 k x 2 1 n c 2
 1  2
+α2 k3M nx 2L2 xE[||xxx(k)−111x ∗T||]+ 3M nx 2L θ2 E[||θθθ(k)−111θ ∗T||]+ M n¯ 
1 α2L2 1 α2L2
≤(1+c +c )(1−α µ )2U (k)+(2+ ) k x V (k)+(2+ ) k θV (k)
1 2 k x 1 c n 1 c n 2
 1 2
+α2 k3M nx 2L2 xE[||xxx(k)−111x ∗T||]+ 3M nx 2L θ2 E[||θθθ(k)−111θ ∗T||]+ M n¯ , (C.7)
wherec ,c >0. Takec =c = 3α µ ,thenc +c = 3α µ . Noticingthatα ≤ 1 ,i.e. α µ ≤ 1,wehave
1 2 1 2 16 k x 1 2 8 k x k 3µx k x 3
13 1 3
(1+c +c )(1−α µ )2 =1− α µ + α2µ2+ α3µ3
1 2 k x 8 k x 4 k x 8 k x
(C.8)
13 1 3 1 3
≤1− α µ + α µ + × α µ =1− α µ ,
k x k x k x k x
8 12 8 9 2
and(2+ 1 )α ≤ 6,m=1,2. Plugtheminto(C.7)yeildstheresultBBB. □
cm k µx
Appendix D. ProofofLemma3.2.2
Proof Recallingthedefinitionofg(xxx,θθθ,ξξξ)in(8)andrelationx¯(k+1)= x¯(k)−α g¯(xxx(k),θθθ(k),ξξξ(k))ineq.(C.1),and
k
using(10),wehave
xxx(k+1)−111x¯(k+1)=W(xxx(k)−α g(xxx(k),θθθ(k),ξξξ(k)))−111(x¯(k)−α g¯(xxx(k),θθθ(k),ξξξ(k)))
k k
=(W−111111T
)(cid:2)
(xxx(k)−111x¯(k))−α
(g(xxx(k),θθθ(k),ξξξ(k))−111g¯(xxx(k),θθθ(k),ξξξ(k)))(cid:3)
. (D.1)
n k
ThusbyLemma2.2.2,weobtain
||xxx(k+1)−111x¯(k+1)||2
≤ρ2||(xxx(k)−111x¯(k))−α (g(xxx(k),θθθ(k),ξξξ(k))−111g¯(xxx(k),θθθ(k),ξξξ(k))||2
w k
=ρ2(cid:2) ||xxx(k)−111x¯(k)||2+α2||g(xxx(k),θθθ(k),ξξξ(k))−111g¯(xxx(k),θθθ(k),ξξξ(k))||2
w (cid:32)k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124) (cid:123)(cid:122) (cid:125)
Term4
−2α
(xxx(k)−111x¯(k))T(g(xxx(k),θθθ(k),ξξξ(k))−111g¯(xxx(k),θθθ(k),ξξξ(k))(cid:3)
(D.2)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124) (cid:123)(cid:122) (cid:125)
Term5
Inthefollowing,wewillseparatelyconsiderTerm4andTerm5. Notethat
∥I−11T/n∥≤1. (D.3)
24ThebyusingAssumptions2.2.2(a)and2.2.2(b),wederive
(cid:104) (cid:105)
E α2 ||g(xxx(k),θθθ(k),ξξξ(k))−111g¯(xxx(k),θθθ(k),ξξξ(k))||2|F(k)
k
(cid:104)
=α2E ||∇ F(xxx(k),θθθ(k))−111∇¯ F(xxx(k),θθθ(k))−∇ F(xxx(k),θθθ(k))
k x x x
(cid:105)
+111∇¯ F(xxx(k),θθθ(k))+g(xxx(k),θθθ(k)),ξξξ(k)−111g¯(xxx(k),θθθ(k),ξξξ(k))||2|F(k)
x
=α2||∇ F(xxx(k),θθθ(k))−111∇¯ F(xxx(k),θθθ(k))||2+α2E[||∇ F(xxx(k),θθθ(k))
k x x k x
−g((xxx(k),θθθ(k),ξξξ(k))−111(∇¯ F(xxx(k),θθθ(k))−g¯(xxx(k),θθθ(k)))||2|F(k)]
x
(D.3) (cid:104)
≤ α2 ||∇ F(xxx(k),θθθ(k))−111∇¯ F(xxx(k),θθθ(k))||2
k x x
(cid:105)
+E[||∇ F(xxx(k),θθθ(k))−g(xxx(k),θθθ(k),ξξξ(k))||2|F(k)]
x
≤α2||∇ F(xxx(k),θθθ(k))−111∇¯ F(xxx(k),θθθ(k))||2+α2nσ2+α2M ||∇ F(xxx(k),θθθ(k))||2. (D.4)
k x x k x k x x
RecallingAssumption2.2.2(a),weobtainthat
E(cid:2)
−2α
(xxx(k)−111x¯(k))T(g(xxx(k),θθθ(k),ξξξ(k))−111g¯(xxx(k),θθθ(k),ξξξ(k)))|F(k)(cid:3)
k
(cid:16) (cid:17)
=−2α (xxx(k)−111x¯(k))T ∇ F(xxx(k),θθθ(k))−111∇¯ F(xxx(k),θθθ(k)) , (D.5)
k x x
Inlightof(D.2),(D.4)and(D.5),wehave
1
E[||xxx(k+1)−111x¯(k+1)||2|F(k)]
ρ2
w
≤||xxx(k)−111x¯(k)||2+α2||∇ F(xxx(k),θθθ(k))−111∇¯ F(xxx(k),θθθ(k))||2
k x x
+α2nσ2+α2M ||∇ F(xxx(k),θθθ(k))||2
k x k x x
−2α (xxx(k)−111x¯(k))T(∇ F(xxx(k),θθθ(k))−111∇¯ F(xxx(k),θθθ(k)))
k x x
≤||xxx(k)−111x¯(k)||2+α2||∇ F(xxx(k),θθθ(k))−111∇¯ F(xxx(k),θθθ(k))||2
k x x
+α2nσ2+α2M ||∇ F(xxx(k),θθθ(k))||2
k x k x x
1
+c ||xxx(k)−111x¯(k)||2+ α2||∇ F(xxx(k),θθθ(k))−111∇¯ F(xxx(k),θθθ(k))||2
3 c k x x
3 (cid:32) (cid:33)
1
≤(1+c )||xxx(k)−111x¯(k)||2+α2nσ2+α2 1+M + ||∇ F(xxx(k),θθθ(k))||2, (D.6)
3 k x k x c x
3
wherec >0isarbitrary,andthelastinequalityalsousestheporpertyin(D.3).
3
Wethenconsidertheupperboundof||∇ F(xxx(k),θθθ(k))||2asfollow,
x
||∇ F(xxx(k),θθθ(k))||2 =||∇ F(xxx(k),θθθ(k))−∇ F(111xT,θθθ(k))
x x x ∗
+∇ F(111xT,θθθ(k))−∇ F(111xT,111θT)+∇ F(111xT,111θT)||2
x ∗ x ∗ ∗ x ∗ ∗
≤3||∇ F(xxx(k),θθθ(k))−∇ F(111xT,θθθ(k))||2
x x ∗
+3||∇ F(111xT,θθθ(k))−∇ F(111xT,111θT)||2+3||∇ F(111xT,111θT)||2
x ∗ x ∗ ∗ x ∗ ∗
≤3L2||xxx(k)−111x¯(k)||2+3L2||θθθ(k)−111θT||2+3||∇ F(111xT,111θT)||2. (D.7)
x θ ∗ x ∗ ∗
Letc = 1−ρ2 w. Combining(D.6)and(D.7),weobtainthat
3 2
1
E[||xxx(k+1)−111x¯(k+1)||2|F(k)]
ρ2
w (cid:32) (cid:33)
3−ρ2 3 (cid:16)
≤ w||xxx(k)−111x¯(k)||2+3α2 +M L2||xxx(k)−111x¯(k)||2
2 k 1−ρ2 x x
w (cid:17)
+L2||θθθ(k)−111θT||2+||∇ F(111xT,111θT)||2 +α2nσ2. (D.8)
θ ∗ x ∗ ∗ k x
Notethatρ2(3−ρ2 w) ≤ 3+ρ2 w byρ ∈ (0,1). Thenbytakingfullexpectationonbothsidesof(D.8)andmultiplyingρ2
w 2 4 w w
leadstotheresult(24). □
Appendix E. ProofofLemma3.3.1
Proof Foranyk≥0,inordertoboundE[||xxx(k)−111xT||2],wefirstlyconsiderboundingE[||x(k)−α g(x(k),θ(k),ξ(k))−
∗ i k i i i i
25x ||2]foralli∈N. ByusingAssumption2.2.1(i)andAssumption2.2.2(c),wehave
∗
E[||x(k)−α g(x(k),θ(k),ξ(k))−x ||2|F(k)]=||x(k)−x −α ∇ f(x(k),θ(k))||2
i k i i i i ∗ i ∗ k x i i i
(cid:104) (cid:105)
+α2E ||∇ f(x(k),θ(k))−g(x(k),θ(k),ξ(k))||2|F(k)
k x i i i i i i i
≤||x(k)−x ||2−2α ∇ f(x(k),θ(k))T(x(k)−x )
i ∗ k x i i i i ∗
+α2||∇ f(x(k),θ(k))||2+α2(σ2+M ||∇ f(x(k),θ(k))||2)
k x i i i k x x x i i i
≤||x(k)−x ||2−2α µ ||x(k)−x ||2+2α ||∇ f(x ,θ(k))||||x(k)−x ||
i ∗ k x i ∗ k x i ∗ i i ∗
+α2(1+M )||∇ f(x(k),θ(k))||2+α2σ2, (E.1)
k x x i i i k x
Considertheupperboundoftheterm||∇ f(x(k),θ(k))||2ontherightsideofaboveinequality. UsingAssumption
x i i i
2.2.1(i)and(ii),wehave
||∇ f(x(k),θ(k))||2 =||∇ f(x(k),θ(k))−∇ f(x ,θ(k))+∇ f(x ,θ(k))
x i i i x i i i x i ∗ i x i ∗ i
−∇ f(x ,θ )+∇ f(x ,θ )||
x i ∗ ∗ x i ∗ ∗
≤3L2||x(k)−x ||2+3L2||θ(k)−θ ||2+3||∇ f(x ,θ )||2. (E.2)
x i ∗ θ i ∗ x i ∗ ∗
Wecansimilarlyobtain||∇ f(x ,θ(k))||2 ≤2L2||θ(k)−θ ||2+2||∇ f(x ,θ )||2.Combining(E.2)and(E.1),itproduces
x i ∗ i θ i ∗ x i ∗ ∗
E[||x(k)−α g(x(k),θ(k),ξ(k))−x ||2|F(k)]≤||x(k)−x ||2−2α µ ||x(k)−x ||2
i k i i i i ∗ i ∗ k x i ∗
(cid:113)
+α2σ2+2α 2L2||θ(k)−θ ||2+2||∇ f(x ,θ )||2||x(k)−x ||
k x k θ i ∗ x i ∗ ∗ i ∗
+α2(1+M )(3L2||x(k)−x ||2+3L2||θ(k)−θ ||2+3||∇ f(x ,θ )||2)
k x x i ∗ θ i ∗ x i ∗ ∗
≤(1−2α µ +3α2(1+M )L2)||x(k)−x ||2
(cid:113)k x k x x i ∗
+2α 2L2||θ(k)−θ ||2+2||∇ f(x ,θ )||2||x(k)−x ||
k θ i ∗ x i ∗ ∗ i ∗
+α2[3(1+M )L2||θ(k)−θ ||2+3(1+M )||∇ f(x ,θ )||2+σ2]. (E.3)
k x θ i ∗ x x i ∗ ∗ x
Fromthedefinitionof K in(26),forallk ≥ 0,wehaveα ≤ µx . RecallthefactthatE[||θ(k)−θ ||2] ≤ Θˆ in
k 3(1+Mx)L2
x (cid:112)
i ∗ i
(27). Bytakingfullexpectationonbothsidesof(E.3)andusingE[||x(k)−x ||]≤ E[||x(k)−x ||2],wehave
i ∗ i ∗
E[||x(k)−α g(x(k),θ(k),ξ(k))−x ||2]≤(1−α µ )E[||x(k)−x ||2]
i k i i i i ∗ k x i ∗
(cid:113)
(cid:112)
+2α 2L2Θˆ +2||∇ f(x ,θ )||2 E[||x(k)−x ||2]
k θ i x i ∗ ∗ i ∗
 
+α kµ LxL 2θ2 Θˆ i+ Lµ 2x||∇ xf i(x ∗,θ ∗)||2+ 3(1µ +xσ M2 x )L2
x x (cid:20) x x
≤E[||x(k)−x ||2]−α µ E||x(k)−x ||2
i ∗ k x i ∗
(cid:113)
(cid:112)
−2 2L2Θˆ +2||∇ f(x ,θ )||2 E[||x(k)−x ||2]
θ i x i ∗ ∗ i ∗
− µ LxL 2θ2
Θˆ i+
Lµ
2x||∇ xf i(x ∗,θ ∗)||2+
3(1µ +xσ M2
x
)L2 (cid:21)
. (E.4)
x x x x
Next,weconsiderthefollowingset:
(cid:26) (cid:113) √
X ≜ q≥0:µ q−2 2L2Θˆ +2||∇ f(x ,θ )||2 q
i x θ i x i ∗ ∗
µ (cid:32) σ2 (cid:33) (cid:27)
− x 3L2Θˆ +3||∇ f(x ,θ )||2+ x ≤0 . (E.5)
3L2 θ i x i ∗ ∗ 1+M
x x
ItcanbeseenthatX isnon-emptyandcompact. IfE[||x(k)−x ||2] (cid:60) X,inlightof(E.4)weknownthatE[||x(k)−
i i ∗ i i
26α g(x(k),θ(k),ξ(k))−x ||2]≤E[||x(k)−x ||2]. WhileforE[||x(k)−x ||2]∈X,byusingα ≤ µx ,wederive
k i i i i ∗ i ∗ i ∗ i k 3(1+Mx)L2
x
E[||x(k)−α g(x(k),θ(k),ξ(k))−x ||2]
i k i i i i ∗
(cid:40) µ (cid:104) (cid:113) √
≤max q− x µ q−2 2L2Θˆ +2||∇ f(x ,θ )||2 q
q∈X
i
(cid:32)3(1+M x)L2
x
x θ i (cid:33)x (cid:41)i ∗ ∗
µ σ2 (cid:105)
− x 3L2Θˆ +3||∇ f(x ,θ )||2+ x ≜R. (E.6)
3L2 θ i x i ∗ ∗ 1+M i
x x
Basedonpreviousarguments,weconcludethatforallk>0,
(cid:110) (cid:111)
E[||x(k)−α g(x(k),θ(k),ξ(k))−x ||2]≤max E[||x(k)−x ||2],R . (E.7)
i k i i i i ∗ i ∗ i
InlightofW1=1,bynotingfrom(10)that
∥xxx(k+1)−1xT∥2 ≤∥W∥2∥xxx(k)−α g(xxx(k),θθθ(k),ξξξ(k))−1xT∥2
∗ k ∗
≤∥xxx(k)−α g(xxx(k),θθθ(k),ξξξ(k))−1xT∥2. (E.8)
k ∗
Thistogetherwith(E.7)produces
(cid:26) (cid:88)n (cid:27)
E[||xxx(k)−111xT||2]≤max E[||xxx(0)−111xT||2], R (E.9)
∗ ∗ i=1 i
Inthefollowing,wewillgiveanupperboundofR. FromthedefinitionofX in(E.5),weknowthattherightzero
i i
oftheupwardopeningparabolais
√ 1 (cid:34) (cid:113)
q = 2 2L2Θˆ +2||∇ f(x ,θ )||2
i 2µ θ i x i ∗ ∗
x
(cid:115)
(cid:32) (cid:33)(cid:35)
4µ2 σ2
+ 4(2L2Θˆ +2||∇ f(x ,θ )||2)+ x 3L2Θˆ +3||∇ f(x ,θ )||2+ x .
θ i x i ∗ ∗ 3L2 θ i x i ∗ ∗ 1+M
x x
Thenbyusingµ ≤ L ,weachieve
x x
(cid:34)
q ≤ 1 2×4(cid:0) 2L2Θˆ +2||∇ f(x ,θ )||2(cid:1)
i 4µ2 θ i x i ∗ ∗
x (cid:33)(cid:35)
(cid:16) 4µ2σ2
+2 8L2Θˆ +8||∇ f(x ,θ )||2+4L2Θˆ +4||∇ f(x ,θ )||2+ x x
θ i x i ∗ ∗ θ i x i ∗ ∗ 3L2(1+M )
x x
≤ 10L θ2Θˆ i + 10||∇ xf i(x ∗,θ ∗)||2 + 2σ2 x ≜q∗.
µ2 µ2 3(1+M )L2 i
x x x x
Thus,X =[0,q]⊂[0,q∗]. Hencefrom(E.6)itfollowsthat
i i i
µ (cid:34) (cid:113) √
R ≤q∗− x µ q−2 2L2Θˆ +2||∇ f(x ,θ )||2 q
i i 3(1+M )L2 x θ i x i ∗ ∗
− µ x (cid:32) 3L2x Θˆ x +3||∇ f(x ,θ )||2+ σ2 x (cid:33)(cid:35)(cid:12) (cid:12) (cid:12) (cid:12) √
3L2 x θ i x i ∗ ∗ 1+M x (cid:12) (cid:12) q= 2Lθ2Θˆi+2||∇xfi(x∗,θ∗)||2
µx
≤ 10L θ2Θˆ i + 10||∇ xf i(x ∗,θ ∗)||2 + 2σ2 x
µ2 µ2 3(1+M )L2
x (cid:34)x (cid:32) x x (cid:33)
µ µ σ2
+ x x 3L2Θˆ +3||∇ f(x ,θ )||2+ x
3(1+M )L2 3L2 θ i x i ∗ ∗ 1+M
x x x  x
+2L θ2Θˆ i+2|| µ∇ xf i(x ∗,θ ∗||2)
x
≤ 11L θ2Θˆ i + 11||∇ xf i(x ∗,θ ∗)||2 + 7σ2 x , (E.10)
µ2 µ2 9(1+M )L2
x x x x
wherethelastinequalityhasusedµ ≤ L .
x x
Combing(E.10)and(E.9),thelemmaholds. □
27