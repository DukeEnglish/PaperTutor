Poisoning Attacks on Federated Learning-based
Wireless Traffic Prediction
Zifan Zhang∗, Minghong Fang†, Jiayuan Huang∗, Yuchen Liu∗
∗North Carolina State University, USA, †University of Louisville, USA
Abstract—FederatedLearning(FL)offersadistributedframe- andefficientlyupgradeinfrastructure,resultingincostsavings
work to train a global control model across multiple base and enhanced network performance. Particularly, in the era of
stationswithoutcompromisingtheprivacyoftheirlocalnetwork
5G and beyond, where technologies like network slicing and
data. This makes it ideal for applications like wireless traffic
edge computing play crucial roles, WTP becomes essential
prediction (WTP), which plays a crucial role in optimizing
network resources, enabling proactive traffic flow management, for optimizing these advancements, which not only enhances
and enhancing the reliability of downstream communication- user experience but also facilitates the provision of innovative
aided applications, such as IoT devices, autonomous vehicles, services that demand high bandwidth and low latency. To
and industrial automation systems. Despite its promise, the
implementWTP,whilecentralizedmethodsexist[9],[10],FL-
securityaspectsofFL-baseddistributedwirelesssystems,partic-
based solution stands out by utilizing training data distributed
ularly in regression-based WTP problems, remain inadequately
investigated. In this paper, we introduce a novel fake traffic across diverse edge nodes. This approach enhances the gen-
injection(FTI)attack,designedtounderminetheFL-basedWTP eration of precise and timely predictions concerning network
system by injecting fabricated traffic distributions with minimal traffic [11]. Despite FL’s potential in accuracy, efficiency, and
knowledge. We further propose a defense mechanism, termed
privacy preservation, its integration into WTP is not devoid
global-local inconsistency detection (GLID), which strategically
of challenges. Notably, Byzantine attacks, particularly model removes abnormal model parameters that deviate beyond a
specificpercentilerangeestimatedthroughstatisticalmethodsin poisoning attacks, pose significant threats to the effectiveness
each dimension. Extensive experimental evaluations, performed and trustworthiness of FL-based WTP systems [12].
onreal-worldwirelesstrafficdatasets,demonstratethatbothour In a model poisoning attack, malicious network entities
attack and defense strategies significantly outperform existing
introduce adversarial modifications to the model parameters
baselines.
during training process of WTP. This tampering results in a
Index Terms—Poisoning attacks, wireless traffic prediction,
federated learning, injection attack. compromised global model when aggregated at the central
network controller, subsequently producing incorrect traffic
I. INTRODUCTION predictions. Such inaccuracies lead to the risk of network
Federated learning (FL) represents an evolving paradigm inefficiencies and even severe service disruptions, especially
in distributed machine learning techniques, allowing a unified in real-time applications like autonomous driving systems. In
model to be trained across numerous devices containing local more extreme scenarios, these attacks may serve as gateways
datasamples,allwithouttheneedtotransmitthesesamplesto to further malicious network intrusions, instigating broader
acentralserver.Thisinnovativeframeworkempowerstraining security and privacy concerns as illustrated in [13], [14].
on diverse datasets characterized by heterogeneous distribu- Thegraveimplicationsofmodelpoisoningattacksunderscore
tions, offering substantial advantages in the current landscape the pressing need for robust security measures to ensure the
ofbigdata.Inpracticalapplications,FLhasfoundwidespread integrity, reliability, and resilience of FL-based WTP systems
use in addressing real-world challenges, particularly in envi- against Byzantine failures, thereby safeguarding the overarch-
ronments dealing with sensitive or personal data, including ingnetworkinfrastructureandtheservicesreliantonit.While
the Internet of Things (IoT) [1], [2], edge computing [3], and most existing FL algorithms and their associated security
health informatics [4], [5]. strategies are typically assessed within the context of classifi-
In the realm of wireless networks, FL leverages its dis- cationproblems[15],[16],scantattentionhasbeenpaidtothe
tributed nature to facilitate multiple network services, includ- regressionproblems,asobservedinexaminedWTPscenarios,
ing wireless traffic prediction (WTP). With the exponential introducing distinct challenges related to data distribution,
growth in the number of connected devices and the ever- model complexity, and evaluation metrics. The distinction
increasing demand for data-intensive applications like stream- between data manipulation strategies in regression and clas-
ing,onlinegaming,andIoTservices,predictingwirelesstraffic sification problems, as well as their detection methodologies,
accurately becomes vital for ensuring network reliability and underscores the nuanced challenges in safeguarding machine
efficiency. By forecasting network load on a temporal basis, learning models against attacks. For instance, in regression-
serviceproviderscandynamicallyallocateresources,reducing based WTP problems, attackers typically target the model’s
the risk of congestion and ensuring a high Quality of Service continuous output by altering the distribution or magnitude of
(QoS) for users [6]–[8]. Furthermore, accurate traffic predic- inputtime-seriesdata,withthegoalofsteeringpredictionsina
tionsenableoperatorstostrategicallyplannetworkexpansions specific direction. This differs from classification tasks, where
4202
rpA
22
]IN.sc[
1v98341.4042:viXrathe manipulation revolves around modifying input features 2) Conversely, we propose an effective defense strategy
to induce misclassification without noticeably changing the designedagainstvariousmodelpoisoningattacks,which
input’s appearance to human observers. dynamically trims an adaptive number of model param-
To bridge this gap, we make the first attempt to introduce a eters by leveraging the percentile estimation technique.
novelattackcenteredoninjectingfakebasestation(BS)traffic 3) Lastly, we evaluate both the proposed poisoning attack
intowirelessnetworks.Existingmodelpoisoningattackshave and the defensive mechanism using real-world traffic
predominantly depended on additional access knowledge and datasets from Milan City, where the results demonstrate
directintrusionsonBSs[12],[15],[17].However,inpractical that the FTI attack indeed compromises FL-based WTP
cellular network systems, BSs have exhibited a commendable systems, and the proposed defensive strategy proves
level of resilience against attacks, making the extraction of notably more effective than other baseline approaches.
training data from them a challenging endeavor. In contrast,
the cost of deploying fake BSs that mimic their behaviors II. RELATEDWORKSANDPRELIMINARIES
is comparatively lower than the resources required for com-
A. FL-based WTP
promising authentic ones [18]. This assumption asserts that
these compromised BSs lack insight into the training data Consider a wireless traffic forecasting system that employs
and only have access to the initial and current global models, FL and incorporates a central server located in a macro-
aligningwiththepracticalsettingsstudiedin[18].Importantly, cell station along with n small-cell BSs (e.g., gNB). Ev-
other information, such as data aggregation rules and model ery BS i ∈ [n] possesses its own private training dataset
parameters from benign BSs, remains inaccessible to these u i = {u1 i,u2 i,...,uM i }, where M represents the total count
compromised BSs. Within the FL framework, the global of time intervals, and um denotes the traffic load on BS
i
model is aggregated based on the model parameters of BSs i during the m-th interval, with m ∈ [M]. To delineate
in each iterative round, encompassing both benign and fake a prediction model, we construct a series of input-output
BSs. Consequently, our threat model envisions a minimum- pairs {aj,bj}z . Here, each aj represents a historical sub-
i i j=1 i
knowledge scenario for an adversary. To this end, we propose set of traffic data that correlates to its associated output
FakeTrafficInjection(FTI),amethodologydesignedtocreate
bj ={um−1,...,um−r,um−ω1,...,um−ωs}.Theparameters
i i i i i
undetectable fake BSs with minimal prior knowledge, where r and s serve as sliding windows capturing immediate tem-
each fake BS employs both its initial model and current poraldependenciesandcyclicalpatterns,respectively.Further-
global information to determine the optimizing trajectory of more,ωencapsulatesinherentperiodicitieswithinthenetwork,
the FL process on WTP. These malicious participants aim potentiallydrivenbydiurnaluserpatternsorsystematicservice
to subtly align the global model towards an outcome that demands.Giventheimportanceofreal-timeresponsivenessin
undermines the integrity and reliability of the data learning wirelessnetworks,ourpredictionmodelisdesignedforaone-
process. Numerous numerical experiments are conducted to step-ahead forecast. To be specific, for the i-th BS, we seek
validate that our FTI demonstrates efficacy across various topredictthetrafficload˜bj basedonthehistoricaltrafficdata
i
state-of-the-art aggregation rules, outperforming other model aj and model parameter θ as˜bj =f(aj,θ), where f(·) is the
i i i
poisoning attacks in terms of vulnerability impacts. regression function.
On the contrary, we propose an innovative defensive strat- In a FL-based WTP system, the objective is to minimize
egy known as Global-Local Inconsistency Detection (GLID), prediction errors across n BSs. This can be formulated as
aimed at neutralizing the effects of model poisoning attacks the following optimization problem to determine the optimal
on WTP. This defense scheme involves strategically removing global model θ∗ in the central server:
abnormal model parameters that deviate beyond a specific
n z
1 (cid:88)(cid:88)
percentile range estimated through statistical methods in each θ∗ =argmin F(f(aj,θ),bj), (1)
dimension. Such an adaptive approach allows us to trim θ nz i i
i=1j=1
varying numbers of malicious model parameters instead of
a fixed quantity [19]. Next, a weighted mean mechanism is where F is the quadratic loss, i.e., F(f(aj,θ),bj) =
i i
(cid:12) (cid:12)2
employed to update the global model parameter, subsequently (cid:12)f(aj,θ)−bj(cid:12) . Eq. (1) can be resolved in a distributed
(cid:12) i i(cid:12)
disseminated back to each BS. Our extensive evaluations,
fashion based on FL with the following three steps in each
conducted on real-world datasets, demonstrate that the pro-
global training round t.
posed defensive mechanism substantially mitigates the impact
of model poisoning attacks on WTP, thereby showcasing a • Step I (Synchronization). The central server sends the
current global model θt to all BSs.
promisingavenueforsecuringFL-basedWTPsystemsagainst
Byzantine attacks. • StepII(Localmodeltraining).EachBSi∈[n]utilizes
itsprivatetime-seriestrainingdataalongwiththecurrent
The contribution of this work is summarized in three folds:
globalmodeltorefineitsownlocalmodel,thentransmits
1) We present a novel model poisoning attack, employing the updated local model θt back to the server.
i
fake BSs for traffic injection into FL-based WTP sys- • Step III (Local models aggregation).Thecentralserver
tems under a minimum-knowledge scenario. leverages the aggregation rule (AR) to merge the n re-ceived local models and subsequently updates the global isnotfeasiblebecauseitisbasedontheunrealisticassumption
model as follows: that an attacker can readily take control of authentic BSs. In
reality, it is highly challenging for an attacker to gain such
θt+1 =AR{θt,θt,...,θt}. (2)
1 2 n influence over existing, authentic BSs. In the MPAF attack,
ThecommonlyusedaggregationruleistheFedAvg[20], which has a simpler threat model, the model updates from
where the server simply averages the received n local fake clients are exaggerated by a factor such as 106. This
models from distributed BSs, i.e., AR{θt,θt,...,θt}= approach is impractical because the central server can easily
1 2 n
n
1 (cid:80) θt. identify these excessive updates as anomalies and discard
n i
i=1 them. By contrast, our proposed poisoning attack involves
B. Byzantine-robust Aggregation Rules carefully crafting model updates on fake BSs by addressing a
parametric optimization problem. This ensures that the server
In non-adversarial scenarios, the server aggregates the re-
isunabletodifferentiatethesefakeupdatesfrombenignones,
ceived local model updates by straightforwardly averaging
allowing the attacker to simultaneously breach the integrity of
them [20]. Nevertheless, recent research [21] has revealed
the system without detection.
that this averaging-based aggregation method is susceptible
to poisoning attacks, where a single malicious BS can ma-
nipulate the final aggregated outcome without constraints. Secure GLID Secure
Aggregation Aggregation
To counteract such potential threats, various Byzantine-robust
Server
aggregation rules have been suggested [19], [21]–[27]. For
Local Model Transmission
instance, in the Krum method [21], each client’s update is Global Model Transmission Bi-directional
Communication
scored based on the sum of Euclidean distances to other
clients’updates.Theglobalupdateisthenupdatedbyselecting
the update from the client (i.e., BS) with the minimum score.
InaMedianaggregationscheme[19],theservercalculatesthe
median value for each dimension using all the local model
Benign BS 1 Benign BS 2 Benign BS 3 Benign BS 4 Fake BS 5
updates. In the FLTrust [22], it is assumed that the server Data Privacy Protection FTI
possesses a validation dataset. The server maintains a model
Fig. 1: Framework of Security Protection in FL-based WTP.
derived from this dataset. To determine trust levels, the server
computes the cosine similarity between its model update and
the update of each BS. These scores are then used to weigh III. THREATMODELTOFL-BASEDWTPSYSTEMS
the contribution of each BS to the final aggregated model. In this section, we present a novel model poisoning attack,
employing fake BSs for traffic injection into FL-based WTP
C. Poisoning Attacks to FL-based Systems
systems under a minimum-knowledge scenario.
The decentralized nature of FL makes our considered prob-
lem susceptible to Byzantine attacks [12], [15], [16], [18], A. Attacker’s Goal
[28], [29], where attackers with control over malicious BSs
The attacker’s primary goal in compromising the integrity
can compromise the FL-based WTP system. Malicious BSs
of the FL-based WTP system is to degrade the final global
can corrupt their local training traffic data or alter their local
model’s performance. This degradation directly impacts the
models directly. For instance, in the Trim attack [15], the at-
accuracy of real-time traffic predictions, which is a critical
tackerintentionallymanipulatesthelocalmodelsonmalicious
aspect of network management and resource allocation. In
BSs to cause a significant deviation between the aggregated
practical cellular systems, inaccurate traffic predictions can
model after attack and the one before attack. In the Model
leadtonetworkcongestion,poorqualityofservice,andineffi-
Poisoning Attack based on Fake clients (MPAF) attack [18],
cient use of resources, thereby causing substantial operational
each malicious BS first multiplies the global model update
challenges for network providers. This disruption not only
synchronized from the central server by a negative scaling
affects service providers but also has a cascading effect on
factor and subsequently transmits these scaled model updates
end-userswhorelyonconsistentandreliablenetworkservices.
to the server. In the Random attack [15], every malicious BS
B. Attacker’s Capability
randomly generates a vector from a Gaussian distribution and
transmits it to the server. Recently, [12] introduced poisoning TheattackerachievesthisobjectivebyintroducingfakeBSs
attacksforFL-basedWTPsystems,wheretheattackercontrols into the targeted FL-based WTP system, as shown in Fig. 1.
some deployed BSs, each with its own local training data. These fake BSs, which could be simple network devices,
These malicious BSs fine-tune their local models using their mimic the traffic processing behaviors of benign BSs with
respective training data. Subsequently, the attacker scales the minimal effort and expense. Unlike the methods proposed
local model updates on malicious BSs by applying a scaling in [12] which involve compromising genuine BSs, the use of
factor and sends the scaled model updates to the server. fake BSs is far more feasible in real-world contexts. Creating
However, existing attacks suffer from the practical imple- fake BSs with open-source projects or emulators [18], [30]–
mentationlimitations.Forinstance,theattackdescribedin[12] [32] is a low-cost approach that can be executed without theneedforsophisticatedhackingskillsordeepaccesstothenet-
work infrastructure. This approach is particularly viable given
the heightened security measures in modern networks, which
make compromising genuine BSs increasingly challenging.
C. Attacker’s Knowledge
Optimal Points
The attacker’s minimal knowledge about the targeted FL-
based WTP system significantly increases the difficulty of
executing the attack. In many real-world systems, gaining
detailed insights into the central server’s aggregation rules or
acquiring information about benign BSs is highly challenging
Fig. 2: Optimal value of η over communication round of R in
due to stringent security protocols and encryption. Therefore,
Algorithm 1.
an attack strategy that requires limited knowledge is not only
more realistic but also more likely to get undetected. The Algorithm 1 Fake Traffic Injection (FTI)
fake BSs’ operation, which is limited to receiving the global Require: Current global model θt, base model θˆ, n benign
information and sending malicious updates, can be executed
BSs, m fake BSs, η
withbasictechnicalskills,furtherloweringthebarriertoentry
Ensure: Fake models θt,i∈[n+1,n+m]
forpotentialattackers.Thisaspectopensthedoortoabroader i
1: step←η
range of network adversaries, including those with limited
2: PreDist←−1
technical expertise or computing resources.
3: for r =1,2,...,R do
D. Fake Traffic Injection Attack
4: for each fake BS i do
The proposed Algorithm 1, referred to as the Fake Traffic 5: θt ←ηθˆ−(η−1)θt
i
Injection (FTI), outlines a Byzantine model poisoning attack 6: end for
strategy designed to manipulate the prediction accuracy of an 7: Dist←∥θt−θt∥
i 2
FL-basedWTPsystemundertheaforementionedassumptions. 8: if PreDist<Dist then
Central to the FTI attack is an iterative process where each 9: η ←η+ step
2
iteration involves a thorough examination of current global 10: else
modelθtandbasemodelθˆ.ForeachfakeBSi,amaliciouslo- 11: η ←η− step
2
calmodelθ it isconstructedbycombiningtheglobalmodelθt 12: end if
andabasemodelθˆinaweightedmanner(Line5).Following 13: step← step
2
the creation of θ it, it evaluates its divergence from the global 14: PreDist←Dist
model using the Euclidean norm (Line 7). The algorithm then 15: end for
checks for an increase in this distance relative to the prior 16: return θt,i∈[n+1,n+m]
i
measurement(Line8).Ifthedistancehasincreased,indicating
that the malicious local model θt from some BS is diverging In such cases, an attacker tends to choose a higher value for
i
further from the global model θt in the central server, the η to ensure the sustained effectiveness of the attack, as shown
value of η is adjusted upwards. Conversely, if no increase in in Fig. 2 with an initial η of 10. This holds true even after the
distanceisobserved,ηisadjusteddownwards.Theadjustment server consolidates the manipulated local updates from fake
of η is done in half-steps of its initial value (Lines 8-12). In BSs with legitimate updates from benign BSs.
other words, the value of η indicates the severity of poisoning
attacks, measuring their impact or intensity. IV. GLOBAL-LOCALINCONSISTENCYDETECTION
To this end, the algorithm involves guiding the global
The defense against model poisoning attacks on the FL-
model to align more closely with a predefined base model
basedWTPsystemreliesonanaggregationprotocoldesigned
in each round. Specifically, during the t-th round, fake BSs
to identify malicious BSs. This protocol, named the Global-
calculate the direction of local model updates, determined by
local Inconsistency Detection (GLID) method, is detailed in
the difference between current global model and base model,
Algorithm 2. In each global round t, GLID primarily scruti-
denoted as H = θˆ − θt. Moving towards this direction
nizes the anomalies present in each dimension of the model
indicates that the global model is becoming more similar parameters θt, aiding in the identification of any potentially
to the base model. A simple approach to acquire the local i
maliciousentities,wherei∈[1,n+m],andn+misthetotal
model of fake BS involves multiplying H by a scaling factor
number of BSs in the system. Such a robust and versatile
η. However, this direct method produces sub-optimal attack
nature allows the network to adapt to various operational
performance.SupposenisthenumberofbenignBSs,andthe
contexts without requiring intricate similarity assessments as
attacker wants to inject m fake BSs into the network system.
in other existing works, like FLTrust [22].
We propose a method for calculating θt for each fake BS
i Specifically, GLID approach enhances the detection of po-
i∈[n+1,n+m]:
tential malicious activities within the network by employing
θt =ηθˆ+(1−η)θt. (3) percentile-based trimming on each dimension of the model
iparameters. To establish an effective percentile pair for iden- Algorithm 2 Global-local Inconsistency Detection (GLID)
tifying abnormalities, four statistical methods can be adopted: Require: Local models θt,θt,...,θt , current global
1 2 n+m
StandardDeviation(SD),InterquartileRange(IQR),Z-scores, model θt, k
and One-class Support Vector Machine (One-class SVM). Ensure: Aggregated global model θt+1
Suppose the total count of dimensions of model parameter 1: for d=1,2,...,D do
is D, then for the default SD method, the percentile pair for 2: θ¯t ← 1 (cid:80)n+mθt
d n+m i=1 d,i
each dimension d can be calculated as follow: (cid:113)
3: σt ← 1 (cid:80)n+m(θt −θ¯t)2
percentile pairt d =(cid:0) g(cid:0) θ¯ dt −k·σ dt(cid:1) , g(cid:0) θ¯ dt +k·σ dt(cid:1)(cid:1) , (4) 4: ped rcentilen t d+ ←m (cid:0) gi (cid:0)= θ¯1 dt −d k,i ·σ dt(cid:1)d , g(cid:0) θ¯ dt +k·σ dt(cid:1)(cid:1)
where θ¯t is the mean of the d-th dimension across all models 5: Identify malicious BSs based on percentile pairs
inthet-td hglobaltraininground,σt isthestandarddeviationof 6: for each BS i do
thed-thdimension,andkisapredd efinedconstantdictatingthe 7: if θ dt ,i is benign then
sensitivityofoutlierdetection.g(·)istheinterpolationfunction 8: αt ← σ dt
d,i |θt −θ¯t|
basedonstandarddeviationboundtoestimatepercentilepairs, 9: else d,i d
shown as follows: 10: αt ←0
d,i
(cid:18) (cid:19)
P(x)−0.5 11: end if
g(x)= ×100, (5)
n+m 12: end for
where P(x) is the position of x in the sorted dataset. We
13: θ dt+1 ← (cid:80) (cid:80)n i=+ 1 nm +mαt d α,i t·θ dt ,i
i=1 d,i
use k = 3 for general purposes. Given that different tasks 14: end for
may require varied percentile bounds, a precise estimation 15: θt+1 ←(cid:2) θ 1t+1,θ 2t+1,...,θ Dt+1(cid:3)
method is crucial for generalizing our defense strategy. The 16: return θt+1
detailed percentile estimation methods can be found later in
where Q1t and Q3t are the first and third quartiles, and
this section. In the FL-based WTP system, model parameters d d
k adjusts sensitivity.
in the d-th dimension exceeding these percentile limits are IQR
flagged as malicious, and their weights αt are assigned as 0. • Z-scores: The Z-score method measures how many stan-
i dard deviations a point is from the mean. For each
Theotherbenignvaluesinthisdimensionareaggregatedusing
dimension d, the normal range bounds are:
a weighted average rule, where the weights αt are inversely
proportional to the absolute deviation of eachd v,i alue θt from lower boundt =g(cid:0) β¯t −k ·σt(cid:1) , (10)
d,i d,Z-score d Z d
the mean θ¯ dt, and normalized by the standard deviation σ dt. It upper boundt
d,Z-score
=g(cid:0) β¯ dt +k Z·σ dt(cid:1) , (11)
can be represented as follows:
where k is the number of standard deviations for the
σt Z
α dt ,i = (cid:12) (cid:12) (cid:12)θ dt ,i−d θ¯ dt(cid:12) (cid:12) (cid:12). (6) • n Oo nrm e-a Cl lara sn sg Se V. M: One-Class SVM constructs a decision
boundary for anomaly detection. The decision function
These weights of the d-th dimension are then normalized and
for each dimension d is:
map op dli ee ld θt to +1a ,g wgr he ig ca hte cae nac bh eB rS ep’s rel so ec na tl edm ao sde fl olθ loit win it no ta hegl vo ib ewal ft(β)=sign(cid:32) (cid:88)nSV
γ ·K(βt
,β)−ρ(cid:33)
, (12)
of each dimension:
d i SVi,d
i=1
θ dt+1 = (cid:80)n i (cid:80)=+ 1m n+α mdt , αi t·θ dt ,i. (7) w mh ue ltr ie plβ ieSt rV si ,,d Ka (r ·e ,·t )he issu tp hp eor kt ev rne ec lto fr us, ncγ ti ioa nre
,
t ah ne dL ρag ir san thg ee
i=1 d,i
offset. A point β is an outlier if ft(β)<0.
Subsequently, the server broadcasts this aggregated global d
Inessence,thisdefensemechanismisastrategicamalgama-
model parameter θt+1 back to all BSs for synchronization.
tionofdirectstatisticaltrimmingandaggregation,targetingthe
There are three additional percentile estimation strategies
preservation of the global model’s integrity against poisoning
listed below. Based on the upper and lower bound computed
attacks. By accurately isolating and excluding malicious BSs
below, we can get a final percentile estimation decision to
prior to model aggregation process, it significantly diminishes
detect abnormal values in each dimension.
the likelihood of adversarial disruption in the FL framework.
• Interquartile Range (IQR): The IQR method calculates Additionally, its capacity to accommodate various dimensions
the range between the first and third quartiles (25th and
and adapt to different inconsistency metrics and aggregation
75th percentiles) of the data, identifying outliers based
protocols considerably extends its applicability across a broad
on this range. For each dimension d, the outlier bounds
spectrum of distributed wireless network scenarios.
are:
V. EVALUATIONS
lower boundt =Q1t −k ·IQRt, (8)
d,IQR d IQR d In this section, we demonstrate the effectiveness of our FTI
upper boundt =Q3t +k ·IQRt, (9)
d,IQR d IQR d poisoningattackandtheGLIDdefensemechanism.Extensiveevaluation results are provided regarding the performance each dimension, thereby reducing the potential sway of
metrics in multiple dimensions. anomalous or malicious updates on the aggregate model.
• Krum[21]:EachBS’supdateisscoredbasedonthesum
A. Experiment Setup
of Euclidean distances to other BSs’ updates. The global
1) Datasets: We utilize the real-world datasets obtained update is then updated by selecting the update from the
from Telecom Italia [33] to evaluate our proposed methods. BS with the minimum score.
The wireless traffic data in Milan is segmented into 10,000 • FoolsGold [23]: It calculates a cosine similarity matrix
grid cells, with each cell served by a BS covering an area amongallBSsandadjuststheweightsforeachBSbased
of approximately 235 meters on each side. Milan Dataset on these similarities. The weighted gradients are then
containsthreesubsetdatasets,“Milan-Internet”,“Milan-SMS” aggregated to form a global model.
and “Milan-Calls”. These datasets capture different types • FABA [25]: It computes the Euclidean distance for each
of wireless usage patterns, and we are mainly focusing on BS’s model from the mean of all received models. By
“Milan-Internet”. Such comprehensive data collection enables identifying and excluding a specific percentage of the
an in-depth analysis of urban telecommunication behavior. most distant models, this process effectively filters out
2) Baseline Schemes: We evaluate various state-of-the-art potential outliers or malicious updates.
modelpoisoningattacksascomparisonpointstoourproposed • FLTrust[22]:Cosinesimilarityiscalculatedbetweenthe
FTI attack. Furthermore, we employ these baseline poisoning server’s current model and each BS’s model to generate
attacks to highlight the effectiveness of our defense strategy trustscores.ThesescoresarethenusedtoweightheBS’s
GLID. contribution to the final aggregated model.
• FLAIR [24]: Each BS calculates “flip-scores” derived
• Trim attack [15]: It processes each key within a model
from the changes in gradient directions and “suspicion-
dictionary, computing and utilizing the extremes in a
scores” based on historical behavior. These scores are
designated dimension to determine a directed dimension,
used to adjust the weights assigned to each BS’s contri-
wheremodelparametersareselectivelyzeroedorretained
butions to the global model.
to influence the model behavior.
3) Experimental Settings and Performance Metrics: In our
• History attack [18]: It iterates over model parameters,
experimentalsetup,werandomlyselected100BSstoevaluate
replacing current values with historically scaled ones,
the impact of poisoning attacks and the effectiveness of
effectively warping the model parameters using past data
defense mechanisms. By default, we report the results on
to misguide the aggregation process.
Milan-Internet dataset. Model training is configured with a
• Random attack [18]: It disrupts the model by replacing
learningrateof0.001andabatchsizeof64.Weinjecta20%
parameters with random and normally distributed values,
percentage of fake BSs to mimic benign ones in the system
scaled to maintain a semblance of legitimacy, thereby
for FTI attack and simulate a scenario where 20% of the BSs
injecting controlled chaos into the aggregation process.
arecompromisedforotherbaselineattacks.OurproposedFTI
• MPAF [18]: It calculates a directional vector derived
attack utilizes a parameter η = 10, and other attacks utilize
from the difference between initial and current param-
a scaling factor of 1000. For the Trim aggregation rule, we
eters. This vector is then used to adjust model values,
discard20%ofthemodelparametersfromallBSs.Inourpro-
intentionally diverging from the model’s original trajec-
posed GLID defense, we employ the standard deviation (SD)
tory to introduce an adversarial bias. Following these
method as the default percentile estimation method. Through-
calculations, the fake BSs are injected into the system.
outthemeasurementcampaign,weadoptMeanAbsoluteError
• Zheng attack [12]: It inverts the direction of model
(MAE)andMeanSquaredError(MSE)astheprimarymetrics
updates by incorporating the negative of previous global
forperformanceevaluation.MSEquantifiestheaverageofthe
updates.Thisinversionisrefinedthrougherrormaximiza-
squared discrepancies between estimated and actual values,
tion,generatingapoisonthatproveschallengingtodetect
while MAE calculates the average absolute differences across
due to its alignment with the model’s error landscape.
predictions,disregardingtheirdirectionalerrors.Thelargerthe
Besides,weconsiderseveralbaselinedefensivemechanisms
MAE and MSE, the better the effectiveness of the attack.
to demonstrate the effectiveness of our attack and defense.
B. Numerical Results
• Mean [20]: It calculates the arithmetic mean of updates
ineachdimension,assumingequaltrustworthinessamong 1) Performance of Proposed Methods: The FTI Attack,
all BSs. However, this method is susceptible to the in particular, exposes significant vulnerabilities in numerous
influence of extreme values. aggregationmethods.ItisobservedthatunderourFTIAttack,
• Median [20]: It identifies the median value in each both Mean and Krum Rules are completely compromised, as
dimension for each parameter across updates, which reflected by their MAE and MSE values reaching over 100.0
inherently discards extreme contributions to enhance the (values exceeding 100 are capped at 100). This result denotes
robustness against outliers. a total breakdown in their WTP functionality. The Median
• Trim [20]: It discards a specified percentage of the Rule further emphasizes the severity of FTI Attack, with both
highestandlowestupdatesbeforecomputingthemeanin its MAE and MSE escalating from modest baseline figures toTABLE I: Performance Metrics for Milan-Internet Dataset
Attack
AggregationRule Metric
NO Trim History Random MPAF Zheng FTI
MAE 0.211 100.0 100.0 100.0 100.0 0.698 100.0
Mean
MSE 0.086 100.0 100.0 100.0 100.0 0.294 100.0
MAE 0.211 0.213 0.211 0.212 0.211 0.217 100.0
Median
MSE 0.086 0.086 0.087 0.086 0.086 0.095 100.0
MAE 0.211 0.212 0.212 0.211 0.212 0.239 100.0
Trim
MSE 0.086 0.087 0.089 0.086 0.088 0.106 100.0
MAE 0.221 0.225 100.0 0.225 100.0 0.225 100.0
Krum
MSE 0.091 0.093 100.0 0.094 100.0 0.094 100.0
MAE 0.213 100.0 100.0 100.0 100.0 0.934 100.0
FoolsGold
MSE 0.095 100.0 100.0 100.0 100.0 0.607 100.0
MAE 0.219 100.0 100.0 100.0 100.0 0.623 100.0
FABA
MSE 0.089 100.0 100.0 100.0 100.0 0.249 100.0
MAE 0.242 0.234 100.0 0.240 100.0 3.182 100.0
FLTrust
MSE 0.094 0.092 100.0 0.094 100.0 1.208 100.0
MAE 0.216 0.228 100.0 100.0 100.0 0.250 100.0
FLAIR
MSE 0.094 0.088 100.0 100.0 100.0 0.096 100.0
MAE 0.211 0.211 0.212 0.211 0.211 0.212 72.383
GLID
MSE 0.086 0.087 0.086 0.086 0.087 0.086 27.528
100. This sharp contrast highlights FTI attack’s reliable per- model’s performance metrics. This impact is illustrated in
formance against other defenses, such as Trim Attack against Fig. 3, where the Median aggregation rule is employed as the
Medianrule,wheretheincreaseinMAEandMSEisrelatively baseline defense strategy. A notable observation is the corre-
minor at 0.234 and 0.092, respectively. Additionally, the Trim lation between increasing values of η and the corresponding
Rule, typically considered robust, exhibits a drastic increase rise in MAE and MSE. For example, at η = 1, the MAE
in MAE to over 100.0, a significant rise from its baseline and MSE are relatively low, recorded at 0.501 and 0.208,
without any attack (termed as NO in Table I) of 0.211. This respectively. However, increasing η to higher values, such as
surgeunderscoresTrimRule’svulnerabilitytotheFTIAttack, 10or20,resultsinadramaticsurgethatreachesthemaximum
markinganotabledeparturefromitstypicalresilience.Similar error rate. This increase suggests a significant compromise in
results can also be found in other aggregation rules under the model, surpassing the predefined threshold for effective
FTI attack, such as FoolsGold, FABA, FLTrust, and FLAIR. detection of the attack. The rationale behind this analysis
The FTI attack has the best overall performance against emphasizes the pivotal role of η in determining the strength
the given defenses. The Zheng attack, however, presents a of a poisoning attack. An increased initial η tends to degrade
distinct pattern of disruption. When subjected to this attack, model performance, deviating significantly from its expected
FLTrust,whichtypicallyexhibitslowererrormetrics,showsa operational state. Simultaneously, a higher η also raises the
significant compromise, evidenced by the dramatic increase riskoftheattack’sperturbationsbeingdetectedandeliminated
in its MAE to 3.182 and MSE to 1.208. Such a tailored during the defense process.
natureofZhengattackappearstotargetspecificvulnerabilities
within FLTrust, which are not as apparent in other scenarios, 100.00 MAE
MSE
such as Trim Attack, where the rise in MAE and MSE for 99.75
FLTrust is relatively modest. Regarding the MPAF Attack, 99.50
most aggregation rules in the table do not show a convincing 99.25
1.00
defense, except for a few like Median, Trim, and GLID.
0.75
Next, if we turn our attention to the defender’s stand-
0.50
point, the proposed GLID aggregation method demonstrates
0.25
consistent performance stability across various attacks. Both 0.00
1 3 5 10 20
its MAE and MSE values remain close to their baseline
levels. Even in the case of our FTI attack, GLID manages Fig. 3: Impact of Values of η.
to keep errors below 100, which is 72.383 and 27.528 for 3) Evaluation on Percentage of Fake BSs: The degree
MAE and MSE respectively. This stability is particularly of compromise in BSs significantly influences the model’s
noteworthy, especially when compared to other rules such performance, as evidenced in Table II. By adopting Median
as FLAIR, which exhibit a significant deviation from their aggregationasthedefensiveapproach,themodelfirstexhibits
non-attacked baselines under the same adversarial conditions. resilience at lower compromise levels, such as with only
GLID’sabilitytosustainitsperformanceinthefaceofdiverse 5%–10% fake BSs in the scenario. However, a noticeable
and severe attacks underscores its potential as a resilient decline in performance is observed as the percentage of
aggregation methodology. fake BSs increases to 20% or higher. This deterioration is
2) Evaluation on the Impact of η: The step size η in our evidentasMAEandMSEvaluesreach100.0inallcategories,
proposed FTI attack (see Algorithm 1) serves as a dynamic signaling a complete model failure. The underlying principle
scaling factor, and its initial value significantly influences the behind this trend suggests the model’s limited tolerance to
etaR
rorrEmalicious interference. More precisely, the network system patternofstableperformanceacrossvaryingparticipantsinthe
can withstand below 20% compromise without significant FL-based WTP system suggests that the total number of BS
performance degradation. However, beyond this threshold, does not substantially influence the effectiveness of the attack
the model’s integrity is severely undermined, resulting in a and defense strategies.
complete system breakdown. This observation highlights the TABLE IV: Impact of Different Percentile Pairs
critical importance of implementing robust security measures
Method
topreventexcessivecompromiseofBSs,ensuringthemodel’s Pair Metric Trim Hist. Rand. MPAF Zhe. FTI
reliability and effectiveness. [10,70] MAE 100.0 100.0 100.0 100.0 0.710 100.0
MSE 100.0 100.0 100.0 100.0 0.279 100.0
TABLE II: Impact of Percentages of Fake BSs MAE 0.215 0.214 0.218 0.217 0.216 100.0
[20,70]
MSE 0.083 0.085 0.084 0.082 0.086 100.0
Pct. Metric Trim Hist. RandA .ttac Mk PAF Zhe. FTI [30,70] M MA SEE 0 0. .2 01 98 0 0 0. .2 01 89 8 0 0. .2 02 80 9 0 0. .2 01 85 6 0 0. .2 01 87 8 7 22 7. .3 28 42 6
5% M MA SEE 0 0. .2 02 81 8 0 0. .2 01 85 9 0 0. .2 01 89 8 0 0. .2 01 85 8 0 0. .2 01 83 8 0 0. .2 02 89 9 [10,80] M MA SEE 1 10 00 0. .0 0 1 10 00 0. .0 0 1 10 00 0. .0 0 1 10 00 0. .0 0 0 0. .7 21 71 5 1 10 00 0. .0 0
10% M MA SEE 0 0. .2 02 80 7 0 0. .2 01 93 0 0 0. .2 01 88 8 0 0. .2 01 93 0 0 0. .2 01 94 6 0 0. .2 15 08 4 [20,80] M MA SEE 0 0. .2 01 87 5 0 0. .2 01 85 3 0 0. .2 01 88 4 0 0. .2 01 84 2 0 0. .2 01 86 6 7 22 7. .1 16 48 7
20% M MA SEE 0 0. .2 02 83 7 0 0. .2 01 98 6 0 0. .2 01 88 7 0 0. .2 01 96 2 0 0. .2 16 39 6 1 10 00 0. .0 0 [30,80] M MA SEE 0 0. .2 02 80 8 0 0. .2 01 88 9 0 0. .2 01 89 6 0 0. .2 01 86 8 0 0. .2 01 97 0 7 21 7. .2 09 28 2
30% M MA SEE 1 10 00 0. .0 0 1 10 00 0. .0 0 1 60 .10 4.0 1 1 10 00 0. .0 0 5 1. .9 19 50 4 1 10 00 0. .0 0 [10,90] M MA SEE 1 10 00 0. .0 0 1 10 00 0. .0 0 1 10 00 0. .0 0 1 10 00 0. .0 0 0 0. .7 21 72 4 1 10 00 0. .0 0
40% M MA SEE 1 10 00 0. .0 0 1 10 00 0. .0 0 1 10 00 0. .0 0 1 10 00 0. .0 0 1 10 00 0. .0 0 1 10 00 0. .0 0 [20,90] M MA SEE 0 0. .2 01 85 8 0 0. .2 01 87 6 0 0. .2 01 88 5 0 0. .2 01 86 9 0 0. .2 01 84 6 1 10 00 0. .0 0
MAE 0.217 0.218 0.219 0.216 0.215 100.0
[30,90]
4) Evaluations on Percentile Estimation Methods: The dy- MSE 0.086 0.088 0.089 0.085 0.088 100.0
namic trimming of an adaptive number of model parameters 6) Evaluations on the Percentile Range of GLID: Table IV
throughpercentileestimation,whichisadaptedinGLID,isef- presents an evaluation of performance across a variety of per-
fective for an effective defense strategy against various model centile pairs used in the proposed GLID method on different
poisoning attacks. In the comparative analysis of various attackmethods.Theconfigurationofthepercentilepairguides
estimationmethods,asshowninTableIII,StandardDeviation the GLID method in identifying and eliminating outliers. For
(SD) estimation emerges as the best technique, exhibiting example, specifying a percentile pair of [10, 70] means that
a marked consistency and robustness across a spectrum of valuesbelowthe10th percentileandabovethe70th percentile
estimation approaches. This is evidenced by the consistently are trimmed away, focusing the analysis on the data within
low MAE and MSE values for SD across these approaches, these bounds. It is observed that, when the percentile pair
at 0.219 and 0.087, respectively. In contrast, other methods is set at [10, 70], most methods, except for Zheng attack,
have varying degrees of inconsistency and vulnerability. For register a metric over 100.0, suggesting the models are fully
instance,One-classSVMexhibitspronouncedvariability,with attacked. Similarly, the percentile pair of [10, 90] yields a
MAE and MSE values reaching the maximal error level of valueover100forallmethodsexceptZhengattack.TheZheng
over 100.0 under Trim, History, and MPAF attacks. Such a attackconsistentlyrecordslowmetricsacrossallsettings,such
disparity in performance, particularly the stably lower error as 0.710, and 0.279 for the pair [10, 70], raising questions
rates of SD compared to the significant fluctuations in other about its attack efficacy. On the other hand, FTI shows varied
estimation methods, positions SD as a reliable and effective performance; it achieves over 100.0 for most percentile pairs
percentile estimation technique in GLID. like [10, 70] and [20, 90] but drops to 72.382 and 27.246 for
the pair [30, 70]. These results underscore the importance of
5) Evaluations on the Impact of BS Density: Given the
fine-tuningthepercentilepairparametersintheGLIDmethod.
percentageoffakeBSsat20%,Figs.4(a)-(d)compareMedian
Properparameterselectioncaneffectivelytrimoutlierswithout
and GLID rules with varying densities of BS in the network
significantly impacting overall network performance.
scenario. It is interesting to see that the total number of BSs
does not significantly impact the performance of any attack VI. CONCLUSION
and defense mechanisms, especially for our FTI and GLID.
Under Median aggregation, FTI consistently shows maximal In this study, we introduced a novel approach to perform
error (MAE and MSE at over 100.0) across different BS model poisoning attacks on WTP through fake traffic injec-
densities, indicating a failure of the defense. This consistent tion. Operating under the assumption that real-world BSs are
challenging to attack, we inject fake BS traffic distribution
TABLE III: Impact of Percentile Estimation Methods
with minimum knowledge that disseminates malicious model
Method Metric Attack parameters. Furthermore, we presented an innovative global-
NO Trim Hist. Rand. MPAF Zhe. FTI
MAE 0.219 0.219 0.219 0.218 0.219 0.219 72.38 local inconsistency detection mechanism, designed to safe-
SD
MSE 0.087 0.087 0.087 0.087 0.087 0.087 27.52 guard FL-based WTP systems. It employs an adaptive trim-
MAE 0.219 0.220 0.220 0.219 0.210 0.218 100.0
IQR MSE 0.087 0.087 0.087 0.087 0.087 0.088 100.0 ming strategy, relying on percentile estimations that preserve
MAE 0.219 0.219 0.219 0.219 0.220 1.047 100.0 accuratemodelparameterswhileeffectivelyremovingoutliers.
Z-scores
MSE 0.087 0.087 0.088 0.087 0.087 0.401 100.0
Extensive evaluations demonstrate the effectiveness of our
MAE 0.219 100.0 100.0 0.220 100.0 0.713 100.0
SVM
MSE 0.087 100.0 100.0 0.087 100.0 0.275 100.0 attack and defense, outperforming existing baselines.100.0 T Hr ii sm tory 100.0 T Hr ii sm tory
Random Random
99.8 M ZhP eA nF g 99.8 M ZhP eA nF g
FTI FTI
99.6 99.6
0.4 0.4
0.2 0.2
0.0 0.0
10 20 50 100 150 10 20 50 100 150
Total Number of BSs Total Number of BSs
(a)MedianARw.r.tMAE (b)MedianARw.r.tMSE
72.5 27.50
Trim Trim
72.0 H Rais nt dor oy m 27.25 H Rais nt dor oy m
71.5 M ZhP eA nF g 27.00 M ZhP eA nF g
71.0 FTI 26.75 FTI
70.5 26.50
0.4 0.4
0.2 0.2
0.0 0.0
10 20 50 100 150 10 20 50 100 150
Total Number of BSs Total Number of BSs
(c)GLIDARw.r.tMAE (d)GLIDARw.r.tMSE
Fig. 4: The impact of BS density on the performance of Median and GLID methods with respect to MAE and MSEs.
ACKNOWLEDGMENT [15] M.Fang,X.Cao,J.Jia,andN.Gong,“Localmodelpoisoningattacks
tobyzantine-robustfederatedlearning,”inUSENIXsecuritysymposium,
This research was supported by the National Science Foun-
2020.
dation through Award CNS–2312138. [16] V.ShejwalkarandA.Houmansadr,“Manipulatingthebyzantine:Opti-
mizingmodelpoisoningattacksanddefensesforfederatedlearning,”in
REFERENCES NDSS,2021.
[17] C.Xie,O.Koyejo,andI.Gupta,“Fallofempires:Breakingbyzantine-
[1] L.U.Khan,W.Saad,Z.Han,E.Hossain,andC.S.Hong,“Federated tolerantsgdbyinnerproductmanipulation,”inUAI,2020.
learning for internet of things: Recent advances, taxonomy, and open [18] X.CaoandN.Z.Gong,“MPAF:Modelpoisoningattackstofederated
challenges,”inIEEECommunicationsSurveys&Tutorials,2021. learningbasedonfakeclients,”inCVPRWorkshops,2022.
[2] D. C. Nguyen, M. Ding, P. N. Pathirana, A. Seneviratne, J. Li, and [19] D. Yin, Y. Chen, R. Kannan, and P. Bartlett, “Byzantine-robust dis-
H.V.Poor,“Federatedlearningforinternetofthings:Acomprehensive tributedlearning:Towardsoptimalstatisticalrates,”inICML,2018.
survey,”inIEEECommunicationsSurveys&Tutorials,2021. [20] B.McMahan,E.Moore,D.Ramage,S.Hampson,andB.A.yArcas,
[3] H.G.Abreha,M.Hayajneh,andM.A.Serhani,“Federatedlearningin “Communication-efficientlearningofdeepnetworksfromdecentralized
edgecomputing:asystematicsurvey,”inSensors,2022. data,”inAISTATS,2017.
[4] J. Xu, B. S. Glicksberg, C. Su, P. Walker, J. Bian, and F. Wang, [21] P. Blanchard, E. M. El Mhamdi, R. Guerraoui, and J. Stainer, “Ma-
“Federatedlearningforhealthcareinformatics,”inJournalofHealthcare chinelearningwithadversaries:Byzantinetolerantgradientdescent,”in
InformaticsResearch,2021. NeurIPS,2017.
[5] D. C. Nguyen, Q.-V. Pham, P. N. Pathirana, M. Ding, A. Seneviratne, [22] X. Cao, M. Fang, J. Liu, and N. Z. Gong, “Fltrust: Byzantine-robust
Z. Lin, O. Dobre, and W.-J. Hwang, “Federated learning for smart federatedlearningviatrustbootstrapping,”inNDSS,2021.
healthcare:Asurvey,”inACMComputingSurveys,2022. [23] C.Fung,C.J.Yoon,andI.Beschastnikh,“Mitigatingsybilsinfederated
[6] J.Wen,M.Sheng,J.Li,andK.Huang,“Assistingintelligentwireless learningpoisoning,”arXivpreprintarXiv:1808.04866,2018.
networks with traffic prediction: Exploring and exploiting predictive [24] A. Sharma, W. Chen, J. Zhao, Q. Qiu, S. Bagchi, and S. Chaterji,
causalityinwirelesstraffic,”inIEEECommunicationsMagazine,2020. “Flair: Defense against model poisoning attack in federated learning,”
[7] L.Nie,D.Jiang,S.Yu,andH.Song,“Networktrafficpredictionbased inASIACCS,2023.
ondeepbeliefnetworkinwirelessmeshbackbonenetworks,”inIEEE [25] Q. Xia, Z. Tao, and Q. Li, “Defending against byzantine attacks in
WirelessCommunicationsandNetworkingConference,2017. quantum federated learning,” in International Conference on Mobility,
[8] S. P. Sone, J. J. Lehtoma¨ki, and Z. Khan, “Wireless traffic usage SensingandNetworking,2021.
forecastingusingrealenterprisenetworkdata:Analysisandmethods,” [26] M.Fang,J.Liu,N.Z.Gong,andE.S.Bentley,“Aflguard:Byzantine-
inIEEEOpenJournaloftheCommunicationsSociety,2020. robustasynchronousfederatedlearning,”inACSAC,2022.
[9] C. Qiu, Y. Zhang, Z. Feng, P. Zhang, and S. Cui, “Spatio-temporal [27] Y. Xu, M. Yin, M. Fang, and N. Z. Gong, “Robust federated learning
wireless traffic prediction with recurrent neural network,” in IEEE mitigatesclient-sidetrainingdatadistributioninferenceattacks,”inThe
WirelessCommunicationsLetters,2018. WebConference,2024.
[10] Y.Xu,W.Xu,F.Yin,J.Lin,andS.Cui,“High-accuracywirelesstraffic [28] V.Tolpegin,S.Truex,M.E.Gursoy,andL.Liu,“Datapoisoningattacks
prediction: A gp-based machine learning approach,” in IEEE Global againstfederatedlearningsystems,”inESORICS,2020.
CommunicationsConference,2017. [29] M. Yin, Y. Xu, M. Fang, and N. Z. Gong, “Poisoning federated
[11] C. Zhang, S. Dang, B. Shihada, and M.-S. Alouini, “Dual attention- recommendersystemswithfakeusers,”inTheWebConference,2024.
basedfederatedlearningforwirelesstrafficprediction,”inINFOCOM, [30] “Android-x86runandroidonyourpc,”https://www.android-x86.org/.
2021. [31] “Noxplayer,theperfectandroidemulatortoplaymobilegamesonpc,”
[12] T.ZhengandB.Li,“Poisoningattacksondeeplearningbasedwireless https://www.bignox.com/.
trafficprediction,”inINFOCOM,2022. [32] “The world’s first cloud-based android gaming platform,”
[13] M. Joshi and T. H. Hadi, “A review of network traffic analysis and https://www.bluestacks.com/.
predictiontechniques,”arXivpreprintarXiv:1507.05722,2015. [33] Barlacchi, Gianni, M. D. Nadai, R. Larcher, A. Casella, C. Chitic,
[14] J.Fan,D.Mu,andY.Liu,“Researchonnetworktrafficpredictionmodel G. Torrisi, F. Antonelli, A. Vespignani, A. Pentland, and B. Lepri, “A
based on neural network,” in International Conference on Information multi-sourcedatasetofurbanlifeinthecityofmilanandtheprovince
SystemsandComputerAidedEducation,2019. oftrentino,”inScientificdata,2015.
eulaV
EAM
eulaV
EAM
eulaV
ESM
eulaV
ESM