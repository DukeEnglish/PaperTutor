A Survey on Self-Evolution of Large Language Models
ZhengweiTao12*,Ting-EnLin2,XiancaiChen1,HangyuLi2,YuchuanWu2,
YongbinLi2†, ZhiJin1†,FeiHuang2,DachengTao3,JingrenZhou2
1 KeyLabofHCST(PKU),MOE;SchoolofComputerScience,PekingUniversity
2AlibabaGroup 3NanyangTechnologicalUniversity
{tttzw, xiancaich}@stu.pku.edu.cn, zhijin@pku.edu.cn
{ting-en.lte, shengxiu.wyc, shuide.lyb, jingren.zhou}@alibaba-inc.com
dacheng.tao@ntu.edu.sg
Abstract mark a significant shift in language understand-
ing and generation. These models undergo three
Large language models (LLMs) have sig- stagesofdevelopmentasshowninFigure1: pre-
nificantly advanced in various fields and trainingonlargeanddiversecorporatogainagen-
intelligent agent applications. However,
eralunderstandingoflanguageandworldknowl-
current LLMs that learn from human or
edge (Devlin et al., 2018; Brown et al., 2020),
externalmodelsupervisionarecostlyandmay
followed by supervised fine-tuning to elicit the
faceperformanceceilingsastaskcomplexity
and diversity increase. To address this issue, abilitiesofdownstreamtasks(Raffeletal.,2020;
self-evolution approaches that enable LLM Chung et al., 2022). Finally, the human prefer-
to autonomously acquire, refine, and learn ence alignment training enables the LLMs to re-
from experiences generated by the model spond as human behaviors (Ouyang et al., 2022).
itself are rapidly growing. This new training
Suchsuccessivetrainingparadigmsachievesignif-
paradigm inspired by the human experiential
icant breakthroughs, enabling LLMs to perform
learning process offers the potential to scale
a wide range of tasks with remarkable zero-shot
LLMs towards superintelligence. In this
and in-context capabilities, such as question an-
work, we present a comprehensive survey
of self-evolution approaches in LLMs. We swering (Tan et al., 2023), mathematical reason-
first propose a conceptual framework for ing (Collins et al., 2023), code generation (Liu
self-evolutionandoutlinetheevolvingprocess etal.,2024b),andtask-solvingthatrequireinterac-
as iterative cycles composed of four phases:
tionwithenvironments(Liuetal.,2023b).
experienceacquisition,experiencerefinement,
Despitetheseadvancements,humansanticipate
updating, and evaluation. Second, we cate-
that the emerging generation of LLMs can be
gorize the evolution objectives of LLMs and
LLM-based agents; then, we summarize the tasked with assignments of greater complexity,
literature and provide taxonomy and insights such as scientific discovery (Miret and Krishnan,
foreachmodule. Lastly,wepinpointexisting 2024)andfutureeventsforecasting(Schoenegger
challenges and propose future directions to et al., 2024). However, current LLMs encounter
improveself-evolutionframeworks,equipping
challenges in these sophisticated tasks due to the
researcherswithcriticalinsightstofast-track
inherent difficulties in modeling, annotation, and
thedevelopmentofself-evolvingLLMs. Our
the evaluation associated with existing training
correspondingGitHubrepositoryisavailable
athttps://github.com/AlibabaResearch/DAMO- paradigms(Burns etal.,2023). Furthermore, the
ConvAI/tree/main/Awesome-Self-Evolution- recentlydevelopedLlama-3modelhasbeentrained
of-LLM. on an extensive corpus comprising 15 trillion to-
kens1. It’samonumentalvolumeofdata,suggest-
1 Introduction ing that significantly scaling model performance
byaddingmorereal-worlddatacouldposealimi-
With the rapid development of artificial intelli-
tation. Thishasattractedinterestinself-evolving
gence, large language models (LLMs) like GPT-
mechanisms for LLMs, akin to the natural evolu-
3.5 (Ouyang et al., 2022), GPT-4 (Achiam et al.,
tionofhumanintelligenceandillustratedbyAIde-
2023),Gemini(Teametal.,2023),LLaMA(Tou-
velopmentsingaming,suchasthetransitionfrom
vronetal.,2023a,b),andQwen(Baietal.,2023)
*WorkdonewhileinterningatAlibabaGroup. 1https://huggingface.co/meta-llama/Meta-Llama-3-70B-
†Correspondingauthors. Instruct
4202
rpA
22
]LC.sc[
1v78341.4042:viXra1.Experience
Acquisition
2.Experience
4.Evaluation
Refinement
3.Updating
2018 BERT 2019 T5 2022 InstructGPT 2023
(1) Pre-training (2) Supervised (3) Human Alignment (4) Self-Evolution
Fine-tuning
Figure1: TrainingparadigmsshiftofLLMs.
AlphaGo(Silveretal.,2016)toAlphaZero(Silver tonomously,akintohumanevolutioninresponse
etal.,2017). AlphaZero’sself-playmethod,requir- to changing environments and challenges. Self-
ingnolabeleddata,showcasesapathforwardfor evolvingLLMsarenotonlyabletotranscendthe
LLMs to surpass current limitations and achieve limitationsofcurrentstatic,data-boundmodelsbut
superhumanperformancewithoutintensivehuman also mark a shift toward more dynamic, robust,
supervision. and intelligent systems. This survey deepens un-
Drawing inspiration from the paradigm above, derstandingoftheemergingfieldofself-evolving
researchontheself-evolutionofLLMshasrapidly LLMs by providing a comprehensive overview
increasedatdifferentstagesofmodeldevelopment, through a structured conceptual framework. We
such as self-instruct (Wang et al., 2023b), self- tracethefield’sevolutionfromthepasttothelat-
play(Tuetal.,2024),self-improving(Huangetal., est cutting-edge methods and applications while
2022),andself-training(Gulcehreetal.,2023). No- examiningexistingchallengesandoutliningfuture
tably,DeepMind’sAMIEsystem(Tuetal.,2024) researchdirections,pavingthewayforsignificant
outperformsprimarycarephysiciansindiagnostic advancesindevelopingself-evolutionframeworks
accuracy,andMicrosoft’sWizardLM-22 exceeds andnext-generationmodels.
the performance of the initial version of GPT-4. The survey is organized as follows: We first
Bothmodelsaredevelopedusingself-evolutionary presenttheoverviewofself-evolution(§2),includ-
frameworkswithautonomouslearningcapabilities ing background and conceptual framework. We
andrepresentapotentialLLMtrainingparadigm summarizeexistingevolvingabilitiesanddomains
shift. However, the relationships between these ofcurrentmethods(§3). Then,weprovidein-depth
methodsremainunclear,lackingsystematicorgani- analysisanddiscussiononthelatestadvancements
zationandanalysis. indifferentphasesoftheself-evolutionprocess,in-
Therefore,wefirstcomprehensivelyinvestigate cludingexperienceacquisition(§4),experiencere-
the self-evolution processes in LLMs and estab- finement(§5),updating(§6),andevaluation(§7).
lishaconceptualframeworkfortheirdevelopment. Finally,weoutlineopenproblemsandprospective
Thisself-evolutionischaracterizedbyaniterative futuredirections(§8).
cycleinvolvingexperienceacquisition,experience
2 Overview
refinement,updating,andevaluation,asshownin
Figure2. Duringthecycle,anLLMinitiallygains
Inthissection,wewillfirstdiscussthebackground
experiencesthroughevolvingnewtasksandgener-
ofself-evolutionandthenintroducetheproposed
atingcorrespondingsolutions,subsequentlyrefin-
conceptualframework.
ingtheseexperiencestoobtainbettersupervision
signals. Afterupdatingthemodelin-weightorin- 2.1 Background
context,theLLMisevaluatedtomeasureprogress
Self-Evolution in Artificial Intelligence. Artifi-
andsetnewobjectives.
cial Intelligence represents an advanced form of
The concept of self-evolution in LLMs has
intelligentagent,equippedwithcognitivefaculties
sparked considerable excitement across various
andbehaviorsmirroringthoseofhumans. Theas-
research communities, promising a new era of
piration of AI developers lies in enabling AI to
models that can adapt, learn, and improve au-
harnessself-evolutionarycapabilities,paralleling
2https://wizardlm.github.io/WizardLM2/ theexperientiallearningprocessescharacteristicof1.Experience Acquisition
ENV
Task 𝓣𝒕 Solution 𝓨𝒕
Feedback
𝓕𝒕
Evolution Evolution
Evolution Objective 𝓔=(𝓐,𝓓)
𝓔𝟏 … 𝓔𝒕 𝓔𝒕"𝟏 Ability 𝓐 Direction 𝓓 𝓣𝒕,𝓨𝒕,𝓕𝒕
LLM Agent
ENV
2.Experience
4.Evaluation 𝑴𝒕"𝟏 3.Updating 𝓣%𝒕,𝓨%𝒕
Refinement
Figure2: Conceptualframeworkofself-evolution. Forthetthiteration: Etistheevolutionobjective;TtandYt
denotethetaskandsolution;Ftrepresentsfeedback;Mtisthecurrentmodel. Refinedexperiencesaremarkedas
T˜tandY˜t,leadingtotheevolvedmodelM˜. ENVistheenvironment. Thewholeself-evolutionstartsatE1.
humandevelopment. Theconceptofself-evolution purely be algorithmic or must involve emergent
inAIemergesfromthebroaderfieldsofmachine consciousness(Searle,1986).
learning and evolutionary algorithms (Bäck and
Schwefel,1993). Initiallyinfluencedbytheprinci- 2.2 ConceptualFramework
plesofnaturalevolution,suchasselection,muta- Intheconceptualframeworkofself-evolution,we
tion,andreproduction,researchershavedeveloped describe a dynamic, iterative process mirroring
algorithms that simulate these processes to opti- thehumanabilitytoacquireandrefineskillsand
mizesolutionstocomplexproblems. Thelandmark knowledge. Thisframeworkisencapsulatedwithin
paper by Holland (1992), which introduced the Figure2,emphasizingthecyclicalnatureoflearn-
genetic algorithm, marks a foundational moment ingandimprovement. Eachiterationoftheprocess
inthehistoryofAI’scapabilityforself-evolution. focusesonaspecificevolutiongoal,allowingthe
Subsequentdevelopmentsinneuralnetworksand modeltoengageinrelevanttasks,optimizeitsex-
deeplearninghavefurtheredthiscapability,allow- periences,updateitsarchitecture,andevaluateits
ingAIsystemstomodifytheirownarchitectures progressbeforemovingtothenextcycle.
andimproveperformancewithouthumaninterven-
tion(Liuetal.,2021). ExperienceAcquisition Atthetth iteration,the
modelidentifiesanevolutionobjectiveEt. Guided
Can Artificial Entities Evolve Themselves?
bythisobjective,themodelembarksonnewtasks
Philosophically,thequestionofwhetherartificial Tt, generating solutions Yt and receiving feed-
entities can self-evolve touches on issues of au- backFt fromtheenvironment,ENV. Thisstage
tonomy, consciousness, andagency. Whilesome
culminates in the acquisition of new experiences
philosophers argue that true self-evolution in AI (Tt,Yt,Ft).
wouldrequiresomeformofconsciousnessorself-
awareness, others maintain that mechanical self- ExperienceRefinement Afterexperienceacqui-
improvementthroughalgorithmsdoesnotconsti- sition, the model examines and refines these ex-
tutegenuineevolution(Chalmers,1997). Thisde- periences. Thisinvolvesdiscardingincorrectdata
bateoftenreferencestheworksofthinkerslikeDen- andenhancingimperfectones,resultinginrefined
nett (1993), who explore the cognitive processes outcomes(T˜t,Y˜t).
underhumanconsciousnessandcontrastthemwith
artificialsystems. Ultimately,thephilosophicalin- Updating Leveraging the refined experiences,
quiryintoAI’scapacityforself-evolutionremains the model undergoes an update process, integrat-
deeplyintertwinedwithinterpretationsofwhatit ing(T˜t,Y˜t)intoitsframework. Thisensuresthe
meansto’evolve’andwhethersuchprocessescan modelremainscurrentandoptimized.Evaluation The cycle concludes with an eval- Reasoning: LLMs can self-evolve to recognize
uation phase, where the model’s performance is statisticalpatterns,makinglogicalconnectionsand
assessed through an evaluation in external envi- deductionsbasedontheinformation. Theyevolve
ronment. The outcomes of this phase inform the toperformbetterreasoninginvolvingmethodically
objectiveEt+1,settingthestageforthesubsequent dissectingproblemsinalogicalsequence(Cuiand
iterationofself-evolution. Wang,2023).
The conceptual framework outlines the self- Math: LLMsenhancetheintricateabilitytosolve
evolutionofLLMs,akintohuman-likeacquisition, mathematicalproblemscoveringarithmetic,math
refinement, and autonomous learning processes. word, geometry, and automated theorem prov-
WeillustrateourtaxonomyinFigure3. ing(Ahnetal.,2024)towardsself-evolution.
Coding: MethodsimprovetheLLMcodingabil-
3 EvolutionObjectives
ities to generate more precise and robust pro-
grams(Singhetal.,2023;Zelikmanetal.,2023).
Evolutionobjectivesinself-evolvingLLMsserve
Furthermore,EvoCodeBench(Lietal.,2024a)pro-
aspredefinedgoalsthatautonomouslyguidetheir
videsanevolvingbenchmarkthatupdatesperiodi-
development and refinement. Much like humans
callytopreventdataleakage.
setpersonalobjectivesbasedonneedsanddesires,
Role-Play: Itinvolvesanagentunderstandingand
theseobjectivesarecrucialastheydeterminehow
actingoutaparticularrolewithinagivencontext.
the model iteratively self-updates. They enable
Thisiscrucialinscenarioswherethemodelmust
the LLM to autonomously learn from new data,
fitintoasocialstructureorfollowasetofbehaviors
optimize algorithms, and adapt to changing envi-
associatedwithaspecificidentityorfunction(Lu
ronments,effectively"feeling"itsneedsfromfeed-
etal.,2024a).
backorself-assessmentandsettingitsowngoalsto
Others: Apartfromtheabovefundamentalevolu-
enhancefunctionalitywithouthumanintervention.
tionobjectives,self-evolutioncanalsoachieveand
Wedefineanevolutionobjectiveascombining
awiderangeofNLPtasks(Stammeretal.,2023;
anevolvingabilityandanevolutiondirection. An
Koaetal.,2024;Gulcehreetal.,2023;Zhangetal.,
evolving ability stands for an innate and detailed
2024b,c).
skill. The evolution direction is the aspect of the
evolutionobjectiveaimingtoimprove. Weformu-
3.1.2 LLM-basedAgents
latetheevolutionobjectiveasfollows:
The abilities discussed here are characteristic of
advancedartificialagentsusedfortask-solvingor
Et = (At,Dt), (1)
simulations in digital or physical world. These
whereEt istheevolutionobjective,composedby capabilitiesmirrorhumancognitivefunctions,al-
evolvingabilitiesAt andevolutiondirectionsDt. lowingtheseagentstoperformcomplextasksand
Take"reasoningaccuracyimproving"asanexam- interacteffectivelyindynamicenvironments.
ple,"reasoning"istheevolvingabilityand"accu- Planning: Itinvolvestheabilitytostrategizeand
racyimproving"istheevolutiondirection. prepareforfutureactionsorgoals. Anagentwith
thisskillcananalyzethecurrentstate,predictthe
3.1 EvolvingAbilities outcomes of potential actions, and create a se-
In Table 1, we summarize and categorize the tar- quenceofstepstoachieveaspecificobjective(Qiao
geted evolving abilities in current self-evolution etal.,2024).
researchintotwogroups: LLMsandLLMAgents. Tool Use: This is the capacity to employ objects
orinstrumentsintheenvironmenttoperformtasks,
3.1.1 LLMs manipulatesurroundings,orsolveproblems(Zhu
Thesearefundamentalabilitiesunderlyingabroad etal.,2024).
spectrumofdownstreamtasks. EmbodiedControl: Itreferstoanagent’sability
Instruction Following: The capability to follow tomanageandcoordinateitsphysicalformwithin
instructionsisessentialforeffectivelyapplyinglan- an environment. This encompasses locomotion,
guage models. It allows these models to address dexterity, and the manipulation of objects (Bous-
specific user needs across different tasks and do- malisetal.,2023).
mains, aligning their responses within the given Communication: Itistheskilltoconveyinforma-
context(Xuetal.,2023a). tion and understand messages from other agentsStructured: Self-Align(Sunetal.,2024),Ditto(Luetal.,2024a),SOLID(Askarietal.,
2024)
Knowledge-
Based
Unstructured: UltraChat(Dingetal.,2023),SciGLM(Zhangetal.,2024b),EvIT(Tao
etal.,2024a),MEEL(Taoetal.,2024b)
Task(§4.1) Knowledge- Self-Instruct(Wangetal.,2023b;Honovichetal.,2022;Roziereetal.,2023),Ada-
Free Instruct(CuiandWang,2023),Evol-Instruct(Xuetal.,2024a),MetaMath(Yuetal.,
2023b),PromptBreeder(Fernandoetal.,2023),Backtranslation(Lietal.,2023b),
Kun(Zhengetal.,2024b)
Selective DIVERSE-EVOL(Wuetal.,2023),SOFT(Wangetal.,2024c),SelectiveReflection-
Tuning(Lietal.,2024b)V-STaR(Hosseinietal.,2024)
Rationale-Based: LMSI(Huangetal.,2022),STaR(Zelikmanetal.,2022),A3T(Yang
etal.,2024c)
Interactive: SelfEvolve(Jiangetal.,2023),LDB(Zhongetal.,2024a),ETO(Songetal.,
2024),A3T(Yangetal.,2024c),AutoAct(Qiaoetal.,2024),KnowAgent(Zhuetal.,2024)
Positive
Self-Play: Debates(Taubenfeldetal.,2024),Self-Talk(Ulmeretal.,2024),Ditto(Luetal.,
Experience 2024a),SOLID(Askarietal.,2024),SOTOPIA-π(Wangetal.,2024e)
Acquisition(§4)
Grounded: Self-Align(Sunetal.,2024),SALMON(Sunetal.,2023),Memory-
Solution Bank(Zhongetal.,2024b),TiM(Liuetal.,2023a),MoT(LiandQiu,2023),IML(Wang
(§4.2) etal.,2024a),TRAN(Yangetal.,2023b),MemGPT(Packeretal.,2023)
Contrastive: Self-Reward(Yuanetal.,2024),SPIN(Chenetal.,2024),GRATH(Chenand
Li,2024),Self-Contrast(Zhangetal.,2024c),ETO(Songetal.,2024),A3T(Yangetal.,
Negative 2024c),STE(Wangetal.,2024b),COTERRORSET(Tongetal.,2024)
Perturbative: RLCD(Yangetal.,2023a),DLMA(Liuetal.,2024a),Ditto(Luetal.,
2024a)
Model Self-Reward(Yuanetal.,2024),LSX(Stammeretal.,2023),DLMA(Liuetal.,2024a),
SIRLC(Pangetal.,2023),Self-Alignment(Zhangetal.,2024e),CAI(Baietal.,2022),
Feedback Self-Refine(Madaanetal.,2023)
(§4.3)
Environment SelfEvolve(Jiangetal.,2023),Self-Debugging(Chenetal.,2023c),Reflexion(Shinnetal.,
2023),CRITIC(Gouetal.,2023),RoboCat(Bousmalisetal.,2023),SinViG(Xuetal.,
2024b),SOTOPIA-π(Wangetal.,2024e)
Metric-Based: ReSTEM (Singhetal.,2023),AutoAct(Qiaoetal.,2024),Self-Talk(Ulmeretal.,2024),Self-
Instruct(Wangetal.,2023b)
Filtering
Metric-Free: Self-Consistency(Wangetal.,2022),LMSI(Huangetal.,2022),Self-Verification(Wengetal.,
2023),CodeT(Chenetal.,2022)
Experience
Refinement(§5)
Critique-Based: Self-Refine(Madaanetal.,2023),CAI(Baietal.,2022),RCI(Kimetal.,2023),SELF(Lu
etal.,2023),CRITIC(Gouetal.,2023),SelfEvolve(Jiangetal.,2023),ISR-LLM(Zhouetal.,2023b),Reflexion
Correcting (Shinnetal.,2024)
Critique-Free: STaR(Zelikmanetal.,2022),Self-Debugging(Chenetal.,2023c),IterRefinement(Chenetal.,
2023b),ClinicalSV(Geroetal.,2023)
Replay: RFT(Yuanetal.,2023),ReST(Gulcehreetal.,2023;Aksitovetal.,2023),AMIE(Tuetal.,2024),
SOTOPIA-π(Wangetal.,2024e),LLM2LLM(Leeetal.,2024),LTC(Wangetal.,2023a),A3T(Yangetal.,
2024c),SSR(Huangetal.,2024a),SDFT(Yangetal.,2024b)
In-Weight
Regularization: InstuctGPT(Ouyangetal.,2022),FuseLLM(Wanetal.,2024),ElasticReset(Noukhovitch
etal.,2024),WARM(Raméetal.,2024),AMA(Linetal.,2024)
Architecture: LoRA(Huetal.,2021),ConPET(Songetal.,2023),ModelSoups(Wortsmanetal.,2022),DAIR
Updating(§6) (Yuetal.,2023a),UltraFuser(Dingetal.,2024),EvoLLM(Akibaetal.,2024)
ExternalMemory: MoT(LiandQiu,2023),MemoryBank(Zhongetal.,2024b),TiM(Liuetal.,2023a),
IML(Wangetal.,2024a),TRAN(Yangetal.,2023b),MemGPT(Packeretal.,2023)UA2(Yangetal.,2024d),
In-Context ICE(Qianetal.,2024),AesopAgent(Wangetal.,2024d)
WorkingMemory: Reflexion(Shinnetal.,2023),IML(Wangetal.,2024a),EvolutionaryAgent(Lietal.,2024c),
Agent-Pro(Zhangetal.,2024d),ProAgent(Zhangetal.,2024a)
Quantitative LLM-as-a-Judge(Zhengetal.,2024a;Duboisetal.,2024),RewardScore(Ouyangetal.,2022)
Evaluation(§7)
Qualitative Yangetal.(2023b),LLMExplanation(Zhengetal.,2024a),ChatEval(Chanetal.,2023)
Figure3: Taxonomyofself-evolvinglargelanguagemodels.
sMLLfonoitulovE-fleSorhumans. Agentswithadvancedcommunication
Knowledge-Based
abilities can participate in dialogue, collaborate
withothers,andadjusttheirbehaviourbasedonthe 𝓔𝒕
communicationreceived(Ulmeretal.,2024).
𝓣𝒕
3.2 EvolutionDirections Generate
Examples of evolution directions include but are
notlimitedto:
ImprovingPerformance: Thegoalistocontinu- Knowledge-Free
ouslyenhancethemodel’sunderstandingandgen-
erationacrossvariouslanguagesandabilities. For
instance,amodelinitiallytrainedforquestionan- 𝓔𝒕 𝓣𝒕
sweringandchitchatcanautonomouslyextendits Generate
proficiencyanddevelopabilitieslikediagnosticdi-
alogue(Tuetal.,2024),socialskills(Wangetal.,
Selective
2024e),androle-playing(Luetal.,2024a).
AdaptationtoFeedback: Thisinvolvesimproving 𝓔𝒕
modelresponsesbasedonfeedbacktobetteralign
𝓣𝒕
with preferences or adapt to environments (Yang
Select
etal.,2023a;Sunetal.,2024).
𝕋
ExpansionofKnowledgeBase: Theaimistocon-
tinuouslyupdatethemodel’sknowledgebasewith
Figure4: Taskevolution. Et andTt aretheevolving
thelatestinformationandtrends. Forexample, a
objectiveandtaskoftthiteration.Tisthesetofalltasks
modelmightautomaticallyintegratenewscientific
to be selected. The first two are generative methods
researchintoitsresponses(Wuetal.,2024). thatdifferbasedontheirrespectiveuseofknowledge.
Safety, Ethic and Reducing Bias: The goal is Thethirdmethod,incontrast,employsadiscriminative
to identify and mitigate biases in the model’s re- approachtoselectwhattolearn.
sponses,ensuringfairnessandsafety. Oneeffective
strategyistoincorporateguidelines,suchasconsti-
tionintothreeparts: taskevolution,solutionevo-
tutionsorspecificrules,toidentifyinappropriate
lution,andobtainingfeedback. Intaskevolution,
orbiasedresponsesandcorrectedthroughmodel
LLMs curate and evolve new tasks aligning with
updates(Baietal.,2022;Luetal.,2024b).
evolutionobjectives. Forsolutionevolution,LLMs
developandimplementstrategiestocompletethese
4 ExperienceAcquisition
tasks. Finally,LLMsmayoptionallycollectfeed-
back from interacting with the environment for
Exploration and exploitation (Gupta et al., 2006)
furtherimprovements.
arefundamentalstrategiesforlearninginhumans
andLLMs. Amongthat,explorationinvolvesseek-
4.1 TaskEvolution
ing new experiences to achieve objectives and
To gain new experience, the model first evolves
is analogous to the initial phase of LLM self-
newtasksaccordingtotheevolutionobjectiveEt
evolution,knownasexperienceacquisition. This
inthecurrentiteration. Taskevolutionisthecrucial
process is crucial for self-evolution, enabling the
step in the engine that starts the entire evolution
modeltoautonomouslytacklecorechallengessuch
process. Formally,wedenotethetaskevolutionas:
asadaptingtonewtasks,overcomingknowledge
limitations, andenhancingsolutioneffectiveness.
Tt = fT(Et,Mt), (2)
Furthermore,experienceisaholisticconstruct,en-
compassingnotonlythetasksencountered(Dewey, where fT is the task evolution function. Et, Mt,
1938)butalsothesolutionsdevelopedtoaddress andTt denotetheevolutionobjective,themodel,
thesetasks(Schön,2017),andthefeedback(Boud and the evolved task at iteration t, respectively.
et al., 2013) received as a result of task perfor- Wesummarizeandcategorizeexistingstudieson
mance. the task evolution method fT into three groups:
Inspired by that, we divide experience acquisi- Knowledge-Based,Knowledge-Free,andSelective.METHOD ACQUISITION REFINEMENTfR UPDATINGfU OBJECTIVEE
TASKfT SOLUTIONfY FEEDBACKfF
LARGELANGUAGEMODELS
Self-Align(Sunetal.,2024) Context-Based Pos-G - Filtering In-W IF
SciGLM(Zhangetal.,2024b) Context-Based - - - In-W Other
EvIT(Taoetal.,2024a) Context-Based - - - In-W Reasoning
MEEL(Taoetal.,2024b) Context-Based - - - In-W Reasoning
UltraChat(Dingetal.,2023) Context-Based - - - In-W Role-Play
SOLID(Askarietal.,2024) Context-Based Pos-S - Filtering In-W Role-Play
Ditto(Luetal.,2024a) Context-Based Pos-S,Neg-P - - In-W Role-Play
MetaMath(Yuetal.,2023b) Context-Free Pos-R - - In-W Math
Self-Rewarding(Yuanetal.,2024) Context-Free - Model - In-W IF,Reasoning,Role-Play
Kun(Zhengetal.,2024b) Context-Free - - Filtering In-W IF,Reasoning
PromptBreeder(Fernandoetal.,2023) Context-Free - - - In-C Math,Reasoning
Ada-Instruct(CuiandWang,2023) Context-Free - - - In-W Math,Reasoning,Code
Backtranslation(Lietal.,2023b) Context-Free - - - In-W IF
DiverseEvol(Wuetal.,2023) Selective Pos-I - - In-W Code
Grath(ChenandLi,2024) Selective Neg-C Model - In-W Reasoning
RESTem(Singhetal.,2023) Selective - Model Filtering In-W Math,Code
SOFT(Wangetal.,2024c) Selective - - - In-W IF
LSX(Stammeretal.,2023) - Pos-R Model Correcting In-W Other
LMSI(Huangetal.,2022) - Pos-R - Filtering In-W Math
TRAN(Yangetal.,2023b) - Pos-G - - In-C Reasoning
MOT(LiandQiu,2023) - Pos-R,Pos-G - Filtering In-C Math,Reasoning
STaR(Zelikmanetal.,2022) - Pos-R,Neg-C Model Correct In-W Reasoning
COTERRORSET(Tongetal.,2024) - Pos-R,Neg-C - - In-W Math,Reasoning
Self-Debugging(Chenetal.,2023c) - Pos-I Env - In-C Code
SelfEvolve(Jiangetal.,2023) - Pos-I - - In-C Code
Reflexion(Shinnetal.,2024) - Pos-I,Pos-G - - In-C Code,Reasoning
V-STaR(Hosseinietal.,2024) - Neg-C Model Filter In-W Math,Code
Self-Contrast(Zhangetal.,2024e) - Neg-C Model - In-W Reasoning
SALMON(Sunetal.,2023) - Neg-C Model - In-W IF,Reasoning,Role-Play
SPIN(Chenetal.,2024) - Neg-C - - In-W IF,Reasoning,Role-Play
RLCD(Yangetal.,2023a) - Neg-P Model - In-W IF
DLMA(Liuetal.,2024a) - Neg-P Model - In-W IF
SELF(Luetal.,2023) - - Model Correct In-W IF,Math
LLMAGENTS
AutoAct(Qiaoetal.,2024) Context-Based Pos-I Env Filtering In-W Planning,Tool
KnowAgent(Zhuetal.,2024) Context-Based Pos-I,Pos-G Env Filtering In-W Embodied,Planning,Tool
RoboCat(Bousmalisetal.,2023) Context-Free Pos-I Env - In-W Embodied
STE(Wangetal.,2024b) Context-Free Pos-I,Neg-C Env Correct In-W Tool
IML(Wangetal.,2024a) - Pos-R,Pos-G - - In-C Reasoning
SinViGXuetal.(2024b) - Pos-I Env Filtering In-W Embodied
ETO(Songetal.,2024) - Pos-I,Neg-C Env Correct In-W Tool
A3T(Yangetal.,2024c) - Pos-I,Neg-C Env Correct In-W Tool
Debates(Taubenfeldetal.,2024) - Pos-S - - In-W Communication
SOTOPIA-π(Wangetal.,2024e) - Pos-S,Pos-G Env - In-W Communication
Self-Talk(Ulmeretal.,2024) - Pos-S,Pos-G Model Filtering In-W Communication
MemGPT(Packeretal.,2023) - Pos-G Env Filtering In-C Communication
MemoryBank(Zhongetal.,2024b) - Pos-G Env Filtering In-C Communication
ProAgent(Zhangetal.,2024a) - Pos-G Env - In-C Embodied
Agent-Pro(Zhangetal.,2024d) - Pos-G Env - In-C Planning
AesopAgent(Wangetal.,2024d) - Pos-G Env - In-C Planning
ICE(Qianetal.,2024) - Pos-G Env - In-C Planning
TiM(Liuetal.,2023a) - Pos-G - - In-C Communication
Werewolf(Xuetal.,2023b) - Pos-G - - In-C Planning
Table1: Overviewofself-evolutionmethods,detailingapproachesacrossevolutionarystages. Key: Pos(Positive),
Neg(Negative),R(Rationale-based),I(Interactive),S(Self-play),G(Grounded),C(Contrastive),P(Perturbative),
Env(Environment),In-W(In-Weight),In-C(In-Context),IF(Instruction-Following). Fortheevolutionobjectives,
AdaptationtoFeedbackisingreen,ExpansionofKnowledgeBaseisinblue,andSafety,EthicandReducingBias
isinbrown. ImprovingPerformanceisinthedefaultcolor,black.
Wedetaileachtypeinthefollowingpartsandshow Basedmethodsseekingtoevolvenewtasksofthe
theconceptsinFigure4. evolvingobjectiveassistedbyexternalinformation.
Knowledge-Based The objective Et may asso- Thefirstkindofknowledgeisstructured. Struc-
ciatewithexternalknowledgetoevolvewherethe turedknowledgeisdenseininformationandwell-
knowledgeisnotinherentlycomprisedinthecur- organized. Self-Align (Sun et al., 2024) curates
rent LLMs. Explicitly sourcing from knowledge topic-guidedtasksgeneratedcovering20scientific
enrichestherelevancebetweentasksandevolution topicssuchasscientificandlegalexpertise. Apart
objectives. Italsoensuresthevalidityofrelevant from topic knowledge, DITTO (Lu et al., 2024a)
facts in the tasks. We delve into the Knowledge- includescharacterknowledgefromWikidataandWikipedia. The knowledge comprises attributes, promptstoincreasethetaskdiversity.
profiles,andconcisecharacterdetailsforrole-play Third,derivingtasksfromplaintextisanother
conversations. SOLID(Askarietal.,2024)gener- way. Backtranslation(Lietal.,2023b)extractsself-
ates structured entity knowledge as conversation containedsegmentsinunlabelleddataandregards
starters. itastheanswerstotasks. Similarly, Kun(Zheng
The second group consists of tasks evolving et al., 2024b) presents a task self-evolving algo-
from an unstructured context. Unstructured con- rithm utilizing instruction harnessed from unla-
text is easy to obtain but is sparse in knowledge. belleddatatowardsback-translation.
UltraChat(Dingetal.,2023)gathersunstructured
Selective Insteadoftaskgeneration,wemaystart
knowledgeof20typesoftextmaterialsbasedon
with large-scale existing tasks. At each iteration,
30meta-conceptstoconstructconversationtasks.
LLMscanselecttasksthatexhibitthehighestrel-
SciGLM (Zhang et al., 2024b) derives questions
evance to the current evolving objective Et with-
fromthetextofdiversifiedsciencesubjects,which
outadditionalgeneration. Thisapproachobviates
coversrichscientificknowledge. EvIT(Taoetal.,
the intricate curation of new tasks, streamlining
2024a) derives event reasoning tasks based on
theevolutionprocess(Zhouetal.,2024;Lietal.,
large-scaleunstructuredeventsminedfromtheun-
2023a;Chenetal.,2023a).
supervised corpus. Similarly, MEEL (Tao et al.,
Asimpletaskselectingmethodistorandomly
2024b)evolvesmulti-modaleventsinbothimage
sample tasks from the task pool like REST (Gul-
and text to construct the tasks for MM event rea-
cehre et al., 2023), RESTem (Singh et al., 2023),
soning.
andGRATH(ChenandLi,2024)do. Ratherthan
Knowledge-Free Unlikepreviousmethodsthat random selection, DIVERSE-EVOL (Wu et al.,
require extensive human effort to gather external 2023)introducesadatasamplingtechniquewhere
knowledge, Knowledge-Free approaches operate the model selects new data points based on their
independentlyusingtheevolvingobjectEt andthe distinctiveness in the embedding space, ensur-
modelitself. Theseefficientmethodscangenerate ing diversity enhancement in the chosen subset.
more diversified tasks without additional knowl- SOFT (Wang et al., 2024c) then splits the initial
edgerestrictions. trainingset. Eachiterationselectsonechunkofthe
First,theLLMscanpromptthemselvestogener- splitsetastheevolvingtask.
atenewtasksaccordingtoEt. Self-Instruct(Wang Li et al. (2024b) propose Selective Reflection-
etal.,2023b;Honovichetal.,2022;Roziereetal., Tuningandselectasubsetoftasksviaanovelmet-
2023)isatypicalmethodologyofKnowledge-Free riccalculatingtowhatextenttheanswerisrelated
taskevolution. Thesemethodsself-generateava- tothequestion. V-STaR(Hosseinietal.,2024)se-
riety of new task instructions based on evolution lectsthecorrectsolutionsinthepreviousiteration
objectives. Ada-Instruct (Cui and Wang, 2023) and adds their task instructions to the task set of
furtherproposesanadaptivetaskinstructiongener- thenextiteration.
ationstrategythatfine-tunesopen-sourceLLMsto
4.2 SolutionEvolution
generatelengthyandcomplextaskinstructionsfor
codecompletionandmathematicalreasoning. Afterobtainingevolvedtasks,LLMssolvethetasks
Second,extendingandboostingoriginaltasksin- to acquire the corresponding solution. The most
creasesthequalityofinstructions. WizardLM(Xu commonstrategyistogeneratethesolutiondirectly
etal.,2023a)proposesEvol-Instructthatevolves accordingtothetaskformulation(Zelikmanetal.,
instruction following tasks with in-depth and in- 2022; Gulcehre et al., 2023; Singh et al., 2023;
breadth evolving and further expands it in code Zhengetal.,2024b;Yuanetal.,2024). However,
generation (Luo et al., 2024). MetaMath (Yu thisstraightforwardapproachmightreachsolutions
et al., 2023b) rewrites the question in multiple irrelevanttotheevolutionobjective,leadingtosub-
ways, includingrephrasing, self-verification, and optimalevolution(Hare,2019). Therefore,solution
FOBAR.ItevolvesanewMetaMathQAdatasetfor evolutionusesdifferentstrategiestosolvetasksand
fine-tuning LLMs to improve mathematical task- enhance LLM capabilities by ensuring that solu-
solving. Promptbreeder (Fernando et al., 2023) tions are not just generated but are also relevant
evolvesseedtasksviamutationprompts. Itfurther and informative. In this section, we comprehen-
evolvesmutationpromptsviathehypermutation sivelysurveythesestrategiesandillustratetheminPositive Negative
Thought-Based Self-Play Contrastive
𝓨𝒕
𝓔𝒕 𝓡 𝓔𝒕 𝓔𝒕 𝟏
𝓨𝒕
𝓨𝒕 𝓨𝒕 𝟐
…
𝓣𝒕 𝓣𝒕 𝓣𝒕
𝓨𝒕
𝒏
Interactive Grounded Perturbative
𝓔𝒕 𝓔𝒕 𝓣!𝒕,𝓨!𝒕 𝓔𝒕 Per Ptu err Ptb u ea rrt tbi uo a rn t bi o1 an ti o2 n 3 𝓨 𝟏𝒕
𝓨𝒕 𝓨𝒕 𝓨 𝟐𝒕
𝓣𝒕 𝓣𝒕 𝓣𝒕 …
𝓨𝒕
𝒏
Figure5: Solutionevolution. Et,Tt,andYtaretheevolvingobjective,task,andsolutionoftthiteration. Risthe
rationalthought.
Figure5. Wefirstformulatethesolution-evolution beledquestions. Similarly,STaR(Zelikmanetal.,
asfollows: 2022) generates rationale when solving the task.
If the answer is wrong, it further corrects the ra-
Yt = fY(Tt,Et,Mt), (3) tionale and the answer. Then, it uses the answer
andrationaleasexperiencestofine-tunethemodel.
wherefY isthemodel’sstrategytoapproachthe
Similarly,LSX(Stammeretal.,2023)proposesthe
evolutionobjective.
novelparadigmtogenerateanexplanationofthe
Wethencategorizethesemethodsintopositive
answer,incorporatinganiterativeloopbetweena
and negative according to the correctness of the
learnermoduleperformingabasetaskandacritic
solutions. Thepositivemethodsintroducevarious
module that assesses the quality of explanations
approaches to acquire correct and desirable solu-
givenbythelearner. Songetal.(2024);Yangetal.
tions. Onthecontrary,negativemethodselicitand
(2024c)obtainrationalesintheReAct(Yaoetal.,
collectundesiredsolutions,includingunfaithfulor
2022)stylewhensolvingthetasks. Therationales
mis-alignmodelbehaviors,whicharethenusedfor
are further engaged in training the agents in the
preferencealignment. Weelaborateonthedetails
followingstep.
ofeachtypeinthefollowingsections.
4.2.1 Positive Interactive Models can interact with the envi-
ronmenttoenhancetheevolutionprocess. These
Current studies explore diverse methods beyond
methodscanobtainenvironmentalfeedbackthatis
vanilla inference for positive solutions to obtain
valuableforguidingself-evolutiondirections.
correctsolutionsalignedwithevolutionobjectives.
SelfEvolveandLDB(Jiangetal.,2023;Zhong
We categorize the task-solving process into four
etal.,2024a)improvecodegenerationabilityvia
types: Rationale-Based,Interactive,Self-Play,and
self-evolution. They allow the model to generate
Grounded.
code and acquire feedback via running the code
Rationale-Based Themodelincorporatesratio- ontheinterpreter. Asanotherenvironment,Song
naleexplanationstowardsapproachingtheevolv- et al. (2024); Yang et al. (2024c) interact in em-
ingobjectivewhensolvingthetasksandcanself- bodiedscenariosandacquirefeedback. Theylearn
evolve by utilizing such rationales. These meth- totakeproperactionsbasedontheircurrentstate.
ods enable models to explicitly acknowledge the For agent abilities, AutoAct (Qiao et al., 2024)
evolutionobjectiveandcompletethistaskinthat introduces self-planning from scratch, focusing
direction(Weietal.,2022;Yaoetal.,2024;Besta on an intrinsic self-learning process. In this pro-
etal.,2024;Yaoetal.,2022). cess,agentsenhancetheirabilitiesthroughrecur-
Huang et al. (2022) proposes a method where siveplanningiterationswithenvironmentfeedback.
an LLM self-evolves using "high-confidence" FollowingAutoAct,(Zhuetal.,2024)furtheren-
rationale-augmented answers generated for unla- hancesagenttrainingbyintegratingself-evolutionand an external action knowledge base. This ap- Bank (Zhong et al., 2024b) and TiM (Liu et al.,
proach guides action generation and boosts plan- 2023a)answercurrentquestionsbyincorporating
ningabilitythroughenvironment-drivencorrective previousquestion-answerrecords. Ratherthanpre-
feedbackloops. vioussolutionhistories,MoT(LiandQiu,2023),
IML(Wangetal.,2024a),andTRAN(Yangetal.,
Self-Play It’sthesituationwhereamodellearns
2023b) incorporate induced rules from the histo-
toevolvebyplayingagainstcopiesofitself. Self-
ries to answer new questions. MemGPT (Packer
playisapowerfulevolvingmethodbecauseiten-
et al., 2023) combines these merits and retrieves
ablessystemstocommunicatewiththemselvesto
previousquestions,solutions,inducedevents,and
get feedback in a closed loop. It’s especially ef-
userportraitknowledge.
fectiveinenvironmentswherethemodelcansim-
ulate various sides of the roles, like multi-player 4.2.2 Negative
games(Silveretal.,2016,2017). Comparedwith
Inadditiontoacquiringpositivesolutions,recent
interactivemethods,self-playisaneffectivestrat-
research illustrates that LLMs can benefit from
egytoobtainfeedbackwithoutanenvironment.
negative ones for self-improvement (Yang et al.,
Taubenfeldetal.(2024)investigatethesystem-
2023b). Thisstrategyisanalogoustotrialander-
atic biases in simulations of debates by LLMs.
rorinhumanbehaviorwhenlearningskills. This
On the contrary to debating, Ulmer et al. (2024)
sectionsummarisestypicalmethodsofgainingneg-
engage LLMs in conversations following gener-
ativesolutionstoassistinself-evolution.
ated principles. Another kind of conversation
via role-playing. Lu et al. (2024a) proposes self- Contrastive Awidelyusedgroupofmethodsis
simulatedrole-playconversation. Theprocessin- to collect multiple solutions for a task and then
volvesinstructingtheLLMwithcharacterprofiles contrast the positive and negative ones to get im-
andaligningitsresponsestomaintainconsistency provements.
with the character’s knowledge and style. Simi- Self-Reward, SPIN (Yuan et al., 2024; Chen
larly,Askarietal.(2024)proposeSOLIDtogen- et al., 2024) updates the model by comparing
eratelarge-scaleintent-awarerole-playdialogues. the answers of high and low scores. Similarly,
This self-playing aspect harnesses the expansive GRATH (Chen and Li, 2024) generates both cor-
knowledgeofLLMstoconstructinformation-rich rect and incorrect answers. It then trains the
exchanges that streamline the dialog generation model by comparing these two answers. Self-
process. Wangetal.(2024e)introducesanovelap- Contrast (Zhang et al., 2024c) contrasts the dif-
proachwherebyeachLLMfollowsaroleandcom- ferencesandsummarizesthesediscrepanciesinto
municateswitheachothertoachievetheirgoals. a checklist that could be used to re-examine and
eliminatediscrepancies. InETO(Songetal.,2024),
Grounded Toreachtheevolvingobjectiveand
themodelinteractswiththeembodiedenvironment
reduceexplorationspace,modelscanbegrounded
to complete tasks and optimizes from the failure
on existing rules (Sun et al., 2024) and previous
solutions. A3T(Yangetal.,2024c)improvesETO
experiencesforfurtherexplicitguidancewhensolv-
by adding rationale after each action for solving
ingthetasks.
tasks. STE(Wangetal.,2024b)implementstrial
LLMscangeneratedesirablesolutionsmoreef-
and error where the model solves the tasks with
fectively by being grounded on pre-defined rules
unfamiliar tools. It learns by analyzing failed at-
and principles. For instance, Self-Align (Sun
temptstoimproveproblem-solvingstrategiesinfu-
et al., 2024) generated self-evolved questions
turetasks. Morerecently,COTERRORSET(Tong
withprinciple-drivenconstraintstoguidethetask-
etal.,2024)obtainsincorrectsolutionsgenerated
solvingprocess. SALMON(Sunetal.,2023)de-
by PALM-2 and proposes mistake tuning, which
sign a set of combined principles that requires
requiresthemodeltovoidmakingmistakes.
the model to follow when solving the task. Self-
Talk (Ulmer et al., 2024) ensures the LLMs gen- Perturbative ComparedtoContrastive,Pertur-
erate a workflow-aligned conversation based on bativemethodsseektoaddperturbationstoobtain
preset agent characters. They generate the work- negativesolutionsintentionally. Modelscanlater
flowinadvancebasedonGPT-4. learn to avoid generating these negative answers.
Besidespre-definedrules,groundingonprevious Addingperturbationstoobtainnegativesolutions
experiences can improve the solutions. Memory- ismorecontrollablethancontrastivemethods.4.3.1 Model
Model
Current studies demonstrate that LLMs can play
𝓔𝒕
wellasacritic(Zhengetal.,2024a). Inthecycle
𝓕𝒕
ofself-evolution,themodeljudgesitselftoacquire
𝓨𝒕 thefeedbackofthesolutions.
One type of feedback is a score that indicates
correctness. Self-Reward (Yuan et al., 2024),
Environment
LSXStammeretal.(2023),andDLMA(Liuetal.,
𝓔𝒕
2024a) rate their own solutions and output the
𝓕𝒕 scores via LLM-as-a-Judge prompting. Similar
𝓨𝒕 to that, SIRLC (Pang et al., 2023) utilizes self-
evaluationresultsofLLMastherewardforfurther
reinforcement learning. Self-Alignment (Zhang
Figure6:TypesofFeedback.EtandYtaretheevolving
etal.,2024e)leveragestheself-evaluationcapabil-
objectiveandtasksolutionsoftthiteration.
ityofanLLMtogenerateconfidencescoresonthe
factualaccuracyofitsoutputs.
Anothertypeprovidesatextualdescription,of-
Some methods add perturbation to generate
fering multi-dimensional information. To alter
harmful solutions (Yang et al., 2023a; Liu et al.,
the distribution of the responses via supervised
2024a). Givenatask,RLCD(Yangetal.,2023a)
learning, CAI (Bai et al., 2022) asks the model
curates both positive and negative instructions
tocritiqueitsresponseaccordingtoaprinciplein
and generates positive and negative solutions.
the constitution. In contrast to supervised learn-
DLMA(Liuetal.,2024a)gathersbothpositiveand
ing and reinforcement learning approaches, Self-
negative instructional prompts and subsequently
Refine(Madaanetal.,2023)allowsthemodelto
producescorrespondingpositiveandnegativesolu-
generatenaturallanguagefeedbackonitsownout-
tions.
putinafew-shotmanner.
Ratherthanharmfulperturbation,incorporating
negative context is another way. Ditto (Lu et al., 4.3.2 Environment
2024a)addsnegativepersonacharacterstogener-
Another form of feedback comes from the envi-
ateincorrectconversations. Themodelthenlearns
ronment,commonintaskswheresolutionscanbe
fromthenegativeconversationstoevolvepersona
directly evaluated. This feedback is precise and
dialogueability.
elaborate and can provide sufficient information
for model updating. They may be derived from
4.3 Feedback code interpreter (Jiang et al., 2023; Chen et al.,
2023c; Shinn et al., 2024), tool execution (Qiao
As humans learn skills, feedback plays a critical
et al., 2024; Gou et al., 2023), the embodied
role in demonstrating the correctness of the solu-
environment (Bousmalis et al., 2023; Xu et al.,
tions. This key information enables humans to
2024b; Zhou et al., 2023b), and other LLMs or
reflect and then update their skills. Akin to this
agents(Wangetal.,2024e;Taubenfeldetal.,2024;
process, LLMs should obtain feedback during or
Ulmeretal.,2024).
afterthetasksolutioninthecycleofself-evolution.
Forcodegeneration,Self-DebuggingChenetal.
Weformalizetheprocessasfollows:
(2023c)utilizesexecutionresultsontestcasesas
part of feedback while SelfEvolve (Jiang et al.,
Ft = fF(Tt,Yt,Et,Mt,ENV), (4)
2023) receives the error message from the inter-
preter. Similarly,Reflexion(Shinnetal.,2023)also
wherefF isthemethodtoacquirefeedback. obtainstherun-timefeedbackfromthecodeinter-
Inthispart,wesummarizetwotypesoffeedback. preter. Itthenfurtherreflectstogeneratethoughts.
Model feedback refers to gathering the critique Thisrun-timefeedbackcontainsthetrace-backin-
or score rated by the LLMs themselves. Besides, formation that can point out the key information
Environmentdenotesthefeedbackreceiveddirectly forimprovedcodegeneration.
fromtheexternalenvironment. Weillustratethese Recently, methods endow tool-using ability to
conceptsinFigure6. LLMsandagents. Executingtoolsleadingtofeed-backinreturn(Gouetal.,2023;Qiaoetal.,2024; F1-scoreandaccuracyasrewardsforsynthetictra-
Songetal.,2024;Yangetal.,2024c;Wangetal., jectoriesandcollectstrajectorieswithexactlycor-
2024b). rectanswersforfurthertraining. Self-Talk(Ulmer
RoboCat (Bousmalis et al., 2023) and Sin- et al., 2024) measures the number of completed
ViG(Xuetal.,2024b)actintheroboticembodied subgoalstofilterthegenerateddialogues,ensuring
environment. Thistypeoffeedbackispreciseand thatonlyhigh-qualitydataisusedfortraining. To
strongtoguideself-evolution. encouragediversityofthesourceinstructions,Self-
Communicationfeedbackiscommonandeffec- Instruct(Wangetal.,2023b)automaticallyfilters
tive in LLM-based multi-agent systems. Agents low-qualityorrepeatedinstructionsusingROUGE-
can correct and support each other, enabling co- Lsimilarityandheuristicsbeforeaddingthemto
evluation (Wang et al., 2024e; Taubenfeld et al., thetaskpool.
2024;Ulmeretal.,2024). The filtering criteria or metrics are crucial for
maintainingthequalityandreliabilityofthegen-
5 ExperienceRefinement
erated outputs, thereby ensuring the continuous
improvementofthemodel’scapability.
Afterexperienceacquisitionandbeforeupdatingin
self-evolution,LLMsmayimprovethequalityand 5.1.2 Metric-Free
reliability of their outputs through experience re-
Somemethodsseekfilteringstrategiesbeyondex-
finement. IthelpsLLMsadapttonewinformation
ternal metrics, making the process more flexible
andcontextswithoutrelyingonexternalresources,
and adaptable. Metric-Free filtering typically in-
leadingtomorereliableandeffectiveassistancein
volvessamplingoutputsandevaluatingthembased
dynamicenvironments. Thisprocessisformulated
on internal consistency measures or other model-
asfollows:
inherentcriteria (Huangetal., 2022;Weng etal.,
T˜t,Y˜t = fR(Tt,Yt,Ft,Et,Mt), (5) 2023; Chen et al., 2022). The filtering in Self-
Consistency (Wang et al., 2022) is based on the
wherefR isthemethodsofexperiencerefinement, consistencyofthefinalansweracrossmultiplegen-
T˜t,Y˜t aretherefinedtasksandsolutions. Weclas- eratedreasoningpaths,withhigheragreementindi-
catinghigherreliability. LMSI(Huangetal.,2022)
sifythemethodsintotwocategories: filteringand
utilizes CoT prompting plus self-consistency for
correcting.
generatinghigh-confidenceself-trainingdata.
5.1 Filtering Designinginternalconsistencymeasuresthatac-
curatelyreflectoutputqualitycanbechallenging.
Refinementinself-evolutioninvolvestwoprimary
Self-Verification (Weng et al., 2023) allows the
filteringstrategies: Metric-Based andMetric-Free.
modeltoselectthecandidateanswerwiththehigh-
Theformerusesexternalmetricstoassessandfilter
est interpretable verification score, calculated by
outputs,whilethelatterdoesnotrelyonthesemet-
assessing the consistency between the predicted
rics. Thisensuresthatonlythemostreliableand
andoriginalconditionvalues. Forthecodegenera-
high-qualitydataisutilizedforfurtherupdating.
tiontask,CodeT(Chenetal.,2022)considersboth
5.1.1 Metric-Based theconsistencyoftheoutputsagainstthegenerated
By relying on feedback and pre-defined criteria, test cases and the agreement of the outputs with
metric-basedfilteringimprovesthequalityofthe othercodesamples.
outputs(Singhetal.,2023;Qiaoetal.,2024;Ulmer Thesemethodsemphasizethelanguagemodel’s
etal.,2024;Wangetal.,2023b),ensuringthepro- ability to self-assess and filter its outputs based
gressiveenhancementofLLMcapabilitiesthrough on internal agreement, showcasing a significant
eachiterationofrefinement. step forward in self-evolution without the direct
Forexample,ReSTEM (Singhetal.,2023)incor- interventionofexternalmetrics.
poratesarewardfunctiontofilterthedatasetsam-
5.2 Correcting
pledfromthecurrentpolicy. Thefunctionprovides
binaryrewardsbasedonthecorrectnessofthegen- Recentadvancementsinself-evolutionhavehigh-
eratedsamplesratherthanalearnedrewardmodel lightedthesignificanceofiterativeself-correction,
trainedonhumanpreferencesinReST(Gulcehre which enables models to refine their experiences.
etal.,2023). AutoAct(Qiaoetal.,2024)leverages This section divides the methods employed intoFiltering Correcting
Critiques
𝓣𝒕,𝓨𝒕 𝓣$𝒕,𝓨$𝒕 𝓣𝒕,𝓨𝒕 (Why & How) 𝓣$𝒕,𝓨$𝒕
𝟏 𝟏 𝟏 𝟏 𝟏 𝟏 𝟏 𝟏
Metric- … Metrics … Critique- … …
Based (e.g. Acc) Based
𝓣 𝒏𝒕,𝓨 𝒏𝒕 𝓣$ 𝒎𝒕 ,𝓨$ 𝒎𝒕 𝓣 𝒏𝒕,𝓨 𝒏𝒕 𝓣$ 𝒏𝒕,𝓨$ 𝒏𝒕
𝓣 𝟏𝒕,𝓨 𝟏𝒕 … 𝓣 𝒏𝒕,𝓨 𝒏𝒕 𝓣$ 𝟏𝒕,𝓨$ 𝟏𝒕 𝓣 𝟏𝒕,𝓨 𝟏𝒕 𝓣$ 𝟏𝒕,𝓨$ 𝟏𝒕
Metric- … Critique- … …
Free Free
𝓣 𝟏𝒕,𝓨 𝟏𝒕 … 𝓣 𝒏𝒕,𝓨 𝒏𝒕 𝓣$ 𝒎𝒕 ,𝓨$ 𝒎𝒕 𝓣 𝒏𝒕,𝓨 𝒏𝒕 FeFa ec dt bu aa cl
k
𝓣$ 𝒏𝒕,𝓨$ 𝒏𝒕
Figure7: Experiencerefinement.
twocategories: Critique-Based andCritique-Free ISR-LLM(Zhouetal.,2023b)helpstheLLMplan-
correction. Critiques often serve as strong hints nerfindarevisedactionplanbyusingavalidator
thatincludetherationalebehindperceivederrors inaniterativeself-refinementprocess.
orsuboptimaloutputs,guidingthemodeltowards Theprimaryadvantageofthismethodliesinits
improvediterations. ability to process and react to detailed feedback,
potentiallyleadingtomoretargetedandnuanced
5.2.1 Critique-Based
corrections.
Thesemethodsrelyonadditionaljudgingprocesses
5.2.2 Critique-Free
to draw the critiques of the experiences. Then,
theexperiencesarerefinedbasedonthecritiques. Contrarytocritique-based,critique-freemethods
Byleveragingeitherself-generated(Madaanetal., correcttheexperiencesdirectlyleveragingobjec-
2023;Baietal.,2022;Shinnetal.,2023;Luetal., tiveinformation(Zelikmanetal.,2022;Chenetal.,
2023) or environment-interaction generated cri- 2023c,b;Geroetal.,2023). Thesemethodsoffer
tiques (Gou et al., 2023; Jiang et al., 2023; Zhou theadvantageofindependencefromnuancedfeed-
et al., 2023b), the model benefits from detailed backthatcritiquesprovide,allowingforcorrections
feedbackfornuancedcorrection. thatadherestrictlytofactualaccuracyorspecific
LLMs have demonstrated their ability to iden- guidelineswithoutthepotentialbiasintroducedby
tify errors in their outputs. Self-Refine (Madaan critiques.
etal.,2023)introducesaniterativeprocessinwhich One group of critique-free methods modifies
themodelrefinesitsinitialoutputsconditionedon the experiences on the signal of whether the task
actionableself-feedbackwithoutadditionaltrain- was correctly resolved. Self-Taught Reasoner
ing. Toevolvefromthecorrection,CAI(Baietal., (STaR) (Zelikman et al., 2022) proposes a tech-
2022)generatescritiquesandrevisionsofitsout- niquethatiterativelygeneratesrationalestoanswer
putsinthesupervisedlearningphase,significantly questions. Iftheanswersareincorrect,themodel
improving the initial model. Applied to an agent ispromptedagainwiththecorrectanswertogener-
automatingcomputertasks,RCI(Kimetal.,2023) ateamoreinformedrationale. Self-Debug(Chen
improvesitspreviousoutputsbasedonthecritique etal.,2023c)enablesthemodeltoperformdebug-
findingerrorsintheoutputs. gingstepsbyinvestigatingexecutionresultsfrom
Sinceweakermodelsmaystrugglesignificantly unittestsandexplainingthecodeonitsown.
withtheself-critiqueprocess,severalapproaches Differentfromdependingonthetask-solvingsig-
enablemodelstocorrecttheoutputsusingcritiques nal,otherinformationproducedduringthesolving
provided by external tools. CRITIC (Gou et al., process can be leveraged. IterRefinement (Chen
2023)allowsLLMstorevisetheoutputbasedon etal.,2023b)reliesonaseriesofrefinedprompts
thecritiquesobtainedduringinteractionwithtools that encourage the model to reconsider and im-
ingeneraldomains. SelfEvolve(Jiangetal.,2023) proveuponitspreviousoutputswithoutanydirect
promptsanLLMtorefinetheanswercodebased critique. For information extraction tasks, Clini-
ontheerrorinformationthrownbytheinterpreter. cal SV (Gero et al., 2023) grounds each elementinevidencefromtheinputandprunesinaccurate improvementandmixesgenerateddialogueswith
elementsusingsuppliedevidence. supervisedfine-tuningdatathroughinnerandouter
Thesecritique-freeapproachessimplifythecor- self-playloops. SOTOPIA-π (Wangetal.,2024e)
rectionmechanism,allowingforeasierimplemen- leveragesbehaviorcloningfromtheexpertmodel
tationandfasteradjustments. andself-generatedsocialinteractiontrajectoryto
reinforcepositivebehaviors.
6 Updating
Anotherisgenerativereplay, whichadoptsthe
Afterexperiencerefinement,weenterthecrucial self-generated synthesized data as knowledge to
updating phase that leverages the refined experi- mitigatecatastrophicforgetting. Forinstance,Self-
encestoimprovemodelperformance. Weformu- SynthesizedRehearsal(SSR)(Huangetal.,2024a)
lateupdatingasfollows: generatessynthetictraininginstancesforrehearsal,
Mt+1 = fU(T˜t,Y˜t,Et,Mt), (6) enablingthemodeltopreserveitsabilitywithout
relyingonrealdatafromprevioustrainingstages.
wherefU istheupdatingfunctions. Theseupdate Self-DistillationFine-Tuning(SDFT)(Yangetal.,
methods keep the model effective by adapting to 2024b)generatesadistilleddatasetfromthemodel
newexperiencesandcontinuouslyimprovingper- itself to bridge the distribution gap between task
formance in changing environments and during datasetsandtheLLM’soriginaldistributiontomit-
iterativetraining. igatecatastrophicforgetting.
Wedividetheseapproachesintoin-weightlearn-
ing,whichinvolvesupdatestomodelweights,and
in-contextlearning,whichinvolvesupdatestoex- 6.1.2 Regularization-based
ternalorworkingmemory.
Regularization-based methods constrain the
6.1 In-Weight
model’s updates to prevent significant deviations
ClassicaltrainingparadigmsinupdatingLLMsin
fromoriginalbehaviors,exemplifiedbyfunction-
weightencompasscontinuouspretraining(Brown
andweight-basedregularization. Function-based
etal.,2020;Roziereetal.,2023),supervisedfine-
regularization focuses on modifying the loss
tuning(Longpreetal.,2023),andpreferencealign-
function that a model optimizes during training
ment(Ouyangetal.,2022;Touvronetal.,2023a).
(Zhongetal.,2023;Pengetal.,2023). Forexam-
However, in the iterative training process of self-
ple, InstuctGPT (Ouyang et al., 2022) employs a
evolving,thecorechallengeliesinachievingover-
per-tokenKL-divergencepenaltyfromtheoutput
allimprovementandpreventingcatastrophicfor-
probabilitiesoftheinitialpolicymodelπ onthe
SFT
getting, which entails refining or acquiring new
updatedpolicymodelπ . FuseLLM(Wanetal.,
RL
capabilitieswhilepreservingoriginalskills. Solu-
2024) employs a technique akin to knowledge
tionstothischallengecanbecategorizedintothree
distillation (Hinton et al., 2015), leveraging the
mainstrategies: replay-based,regularization-based,
generated probability distributions from source
andmerging-basedmethods.
LLMstotransferthecollectiveknowledgetothe
targetLLM.
6.1.1 Replay-based
Replay-based methods reintroduce previous data Weight-basedregularization(Kirkpatricketal.,
to retain old knowledge. One is experience re- 2017) directly targets the model’s weights dur-
play, which mixes the original and new training ing training. Techniques such as Elastic Reset
data to update LLMs (Roziere et al., 2023; Yang (Noukhovitchetal.,2024)countersalignmentdrift
etal.,2024c;Zhengetal.,2023;Leeetal.,2024; inRLHFbyperiodicallyresettingtheonlinemodel
Wangetal.,2023a). Forexample,Rejectionsam- toanexponentiallymovingaverageofitsprevious
pling Fine-Tuning (RFT) (Yuan et al., 2023) and states. Furthermore,Raméetal.(2024)introduced
ReinforcedSelf-Training(ReST)(Gulcehreetal., WARM,whichcombinesmultiplerewardmodels
2023;Aksitovetal.,2023)methoditerativelyup- throughweightaveragingtoaddressrewardhack-
dateslargelanguagemodelsbymixingseedtrain- ingandmisalignment. Moreover,AMA(Linetal.,
ingdatawithfilterednewoutputsgeneratedbythe 2024) adaptively average model weights to opti-
modelitself. AMIE(Tuetal.,2024)utilizesaself- mize the trade-off between reward maximization
playsimulatedlearningenvironmentforiterative andforgettingmitigation.In-Weight In-Context
Replay Regularization External Memory
Replay
Buffer
𝓣"𝒊"𝟏
𝓣"𝒊"𝟏
Update
𝓣"𝒊"𝟏 𝑴𝒊"𝟏 External
𝑴𝒊"𝟏 Memory
𝓨"𝒊"𝟏 𝓨"𝒊"𝟏
𝓨"𝒊"𝟏 Retrieve
Architecture Working Memory
𝓣"𝒊"𝟏 Decomposition Merging 𝓣"𝒊"𝟏 Memory
Update
Stream
𝑴𝒊"𝟏
State,
𝓨"𝒊"𝟏 𝓨"𝒊"𝟏 Utilize Belief
Figure8: TheillustrationofUpdatingmethods,includingin-weightandin-contextupdating. ThetermsT˜t and
Y˜trepresentrefinedexperiences,eachcontainingtaskandcorrespondingsolutions,respectively. Mtdenotesthe
updatedmodel.
6.1.3 Architecture-based enablingfastadaptiveupdateswithoutexpensive
trainingcosts. Themethodscouldbedividedinto
Architecture-based methods explicitly utilize ex-
updatingexternalandworkingmemory.
tra parameters or models for updating, includ-
ingdecomposition-andmerging-basedapproaches.
ExternalMemory Thisapproachutilizesanex-
Decomposition-basedmethodsseparatelargeneu-
ternalmoduletocollect,update,andretrievepast
ral network parameters into general and task-
experiences and knowledge, enabling models to
specific components and only update the task-
access a rich pool of insights and achieve better
specific parameters to mitigate forgetting. LoRA
resultswithoutupdatingmodelparameters. Theex-
(Huetal.,2021;Dettmersetal.,2024)injecttrain-
ternalmemorymechanismiscommoninAIAgent
ablelow-rankmatricestosignificantlyreducethe
systems(Xuetal.,2023b;Qianetal.,2024;Wang
numberoftrainableparameterswhilemaintaining
et al., 2024d). This section provides a detailed
or improving model performance across various
overview of the latest methods for updating ex-
tasks. ThisparadigmislateradoptedbyGPT4tools
ternal memory, emphasizing the aspects of mem-
(Yang et al., 2024a), OpenAGI (Ge et al., 2024)
oryContentandUpdatingOperations,andsumma-
andDromedary(Sunetal.,2024). DynamicCon-
rizedinTable2.
PET (Song et al., 2023) combines pre-selection
Content: External memory mainly stores two
andpredictionwithtask-specificLoRAmodulesto
type of content: past experiences and reflected
preventforgetting,ensuringscalableandeffective
rationale, each serving distinct purposes. For in-
adaptationofLLMstonewtasks.
stance,pastexperienceprovidevaluablehistorical
Merging-basedmethods,ontheotherhand,in- context,servingasaguidingforcetowardachiev-
volve combining multiple models or layers to ingimprovedoutcomes. MoT(LiandQiu,2023)
achieve general improvements, including but not archivesfilteredquestion-answerpairstoconstruct
limitedtomergingmultiplegenericandspecialized abeneficialmemoryrepository. Additionally,the
modelweightsintoasinglemodel(Wortsmanetal., FIFOQueuemechanisminMemGPT(Packeretal.,
2022;Ilharcoetal.,2022;Yuetal.,2023a;Yadav 2023)maintainsarollinghistoryofmessages,en-
etal.,2024),throughmixture-of-expertapproach capsulatinginteractionsbetweenagentsandusers,
(Dingetal.,2024)orevenlayer-wisemergingand systemnotifications,andinputsandoutputsoffunc-
up-scalingsuchasEvoLLM(Akibaetal.,2024). tioncalls.
On the other hand, reflected rationales offer
6.2 In-Context
condensed explanations, such as rules, that sup-
Inadditiontodirectlyupdatingmodelparameters, portdecision-making. Forinstance,TRAN(Yang
anotherapproachistoleveragethein-contextcapa- etal.,2023b)archivesrulesinferredfromexperi-
bilitiesofLLMstolearnfromexperiences,thereby ences alongside information on mistakes to miti-METHOD CONTENT OPERATION
MoT(LiandQiu,2023) Experience Insert
TRAN(Yangetal.,2023b) Rationale Insert,Reflect
MemoryBank(Zhongetal.,2024b) Experience,Rationale Insert,Reflect,Forget
MemGPT(Packeretal.,2023) Experience Insert,Forget
TiM(Liuetal.,2023a) Rationale Insert
IML(Wangetal.,2024a) Rationale Insert,Reflect
ICE(Qianetal.,2024) Rationale Insert,Reflect
AesopAgent(Wangetal.,2024d) Experience,Rationale Insert,Reflect
Table2: Contentandoperationsofupdatingexternalmemory.
gatefutureerrors. Correspondingly,TiM(Liuetal., their environment by summarizing, refining, and
2023a) preserves inductive reasoning, defined as updatingknowledgebasedonpastexperiencedi-
textelucidatingtherelationshipsbetweenentities. rectlyinworkingmemory.
Moreover,IML(Wangetal.,2024a)andICE(Qian EvolutionaryAgent (Li et al., 2024c) aligns
etal.,2024)storecomprehensivenotesandrules agents with dynamically changing social norms
derivedfromaseriesoftrajectories,demonstrating through evolution and selection principles, lever-
thebroadspectrumofcontenttypesthatmemory aging environmental feedback for self-evolution.
systemscanaccommodate. Agent-Pro (Zhang et al., 2024d) employs policy-
MemoryBank(Zhongetal.,2024b)andAesopA- levelreflectionandoptimization,allowingagents
gent(Wangetal.,2024d)establishbothexperience to adapt their behavior and beliefs in interactive
andreflectionknowledgestores,whicharethein- scenarios based on past outcomes. Lastly, ProA-
tegrationofbothtwomemories. gent(Zhangetal.,2024a)enhancescooperationin
UpdatingOperation: Wecategorizetheopera- multi-agent systems by dynamically interpreting
tions to the memory into Insert, Reflect, and For- teammates’intentionsandadaptingbehavior.
get. The most common operation is insert, meth- Thesecollectiveworksdemonstratetheimpor-
ods insert text content into the memory for stor- tance of integrating past experiences and knowl-
age(LiandQiu,2023;Yangetal.,2023b;Zhong edge into the agents’ memory stream to refine
etal.,2024b;Packeretal.,2023;Liuetal.,2023a; theirstateorbeliefsforimprovedperformanceand
Wangetal.,2024a). Anotheroperationisreflection, adaptabilityacrossvarioustasksandenvironments.
whichistothinkandsummarizepreviousexperi-
7 Evaluation
ences to conceptualize rules and knowledge for
futureuse(Yangetal.,2023b;Zhongetal.,2024b; Muchlikethehumanlearningprocess,itisessen-
Wangetal.,2024a;Qianetal.,2024). Last,dueto tialtoascertainwhetherthepresentlevelofability
thelimitedstorageofmemory,forgettingcontent isadequateandmeetstheapplicationrequirements
iscrucialtokeepingmemoryefficientandthecon- throughevaluation. Furthermore,itisfromthese
tentvalid. MemGPT(Packeretal.,2023)adopts evaluationsthatonecanidentifythedirectionfor
the FIFO queue to forget the contents. Memory- futurelearning. However,howtoaccuratelyassess
Bank(Zhongetal.,2024b)establishesaforgetting theperformanceofanevolvedmodelandprovide
curveontheinserttimeofeachitem. directionsforfutureimprovementsisacrucialyet
underexploredresearcharea. Foragivenevolved
WorkingMemory Themethodsusepastexpe- modelMt,weconceptualizetheevaluationprocess
rience to evolve the capabilities of agents by up-
asfollows:
datinginternalmemorystreams,states,orbeliefs,
knownasworkingmemory,oftenintheformofver- Et+1,St+1 = fE(Mt,Et,ENV), (7)
balcues. Reflexion(Shinnetal.,2023)introduces
verbalreinforcementlearningfordecision-making where fE represents the evaluation function that
improvementwithoutconventionalmodelupdates. measurestheperformancescore(St+1)ofthecur-
Similarly,IML(Wangetal.,2024a)enablesLLM- rent model and provide evolving goal (Et+1) for
based agents to autonomously learn and adapt to the next iteration. Evaluation function fE can becategorized into quantitative and qualitative ap- Furthermore,theconceptofevolvingobjectives
proaches, each providing valuable insights into entails a potential hierarchical structure; for in-
modelperformanceandareasforimprovement. stance, UltraTool (Huang et al., 2024b) and T-
Eval(Chenetal.,2023d)categorizethecapability
7.1 QuantitativeEvaluation
of tool usage into various sub-dimensions. Ex-
Thismethodfocusesonprovidingmeasurablemet- ploring evolutionary objectives into manageable
ricstoreliablyassessLLMperformance,suchas sub-goalsandpursuingthemindividuallyemerges
automatic (Papineni et al., 2002; Lin, 2004) and asaviablestrategy.
humanevaluation. However,traditionalautomatic Overall,aclearandurgentneedexiststodevelop
metricsstruggletoaccuratelyevaluateincreasingly self-evolutionframeworksthateffectivelyaddress
complex tasks, and human assessment is not an diversifiedandhierarchicalobjectives.
idealoptionforautonomousself-evolution. Recent
8.2 LevelofAutonomy: FromLowtoHigh
trends use LLMs as human proxy for automatic
evaluators,offeringcost-effectiveandscalableso- Self-evolution in large models is emerging, yet
lutionsforevaluations. lacks clear definitions for its autonomous levels.
For example, reward model score has been Wecategorizeself-evolutionintothreetiers: low,
widelyusedtomeasuremodelortaskperformances medium,andhigh-levelautonomy
(Shinnetal.,2024)andselectthebestcheckpoint
Low-level In this level, the user predefined the
(Ouyang et al., 2022). LLM-as-a-judge (Zheng
evolvingobjectE anditremainsunchanged. The
etal.,2024a)usingLLMstoevaluateLLMs,em-
userneedstodesigntheevolvingpipeline,namely
ployingmethodslikepairwisecomparison,single
allmodulesf•,onitsown. Then,themodelcom-
answergrading,andreference-guidedgrading. It
pletes the self-evolution process based on the de-
showsthatLLMscancloselymatchhumanjudg-
signed framework. We denote this level of self-
ment,enablingefficientlarge-scaleevaluations.
evolutioninthefollowingformula:
7.2 QualitativeEvaluation
M˜ = EvolL(M,E,f•,ENV), (8)
Qualitative evaluation involves case studies and
analysistoderiveinsights,offeringevolvingguid-
whereMdenotesthemodeltobeevolved. M˜ isthe
anceforsubsequentiterations. Initiativessuchas
evolvingoutput. ENVistheenvironment. Mostof
LLM-as-a-judge(Zhengetal.,2024a)providethe
thecurrentworkslieatthislevel.
reasoningbehinditsassessments;ChatEval(Chan
etal.,2023)exploresthestrengthsandweaknesses
Medium-level Inthislevel,theuseronlysetsthe
ofmodeloutputsthroughdebatemechanisms. Fur-
evolving object E and keeps it unchanged. The
thermore, TRAN (Yang et al., 2023b) leverages userdoesn’tneedtodesignthespecificmodulesf•
past errors to formulate rules that enhance future
in the framework. The model can construct each
LLMperformances. Nonetheless,comparedwith modulef• independentlyforself-evolution. This
instance-levelcriticorreflection,qualitativeevalu-
leveldenotesasfollows:
ationatthetask-ormodel-levelstillneedscompre-
hensiveinvestigation. M˜ = EvolM(M,E,ENV), (9)
8 OpenProblems
High-level Inthefinallevel,themodeldiagnoses
8.1 Objectives: DiversityandHierarchy
its deficiency and constructs the self-evolution
Section3summarizesexistingevolutionobjectives methods to improve itself. This is the ultimate
andtheircoverage. Nonetheless,thesehighlighted purposeofself-evolution. Theusermodelsetsits
objectivescanonlysatisfyasmallfractionofthe ownevolvingobjectE accordingtotheevaluation
vast human needs. The extensive application of fE output. Theevolvingobjectivewouldchange
LLMs across various tasks and industries high- duringtheiteration. Besides,themodeldesignsthe
lights unresolved challenges in establishing self- specific modules f• in the framework. We repre-
evolutionframeworksforevolvingobjectivesthat sentthislevelas:
cancomprehensivelyaddressabroaderspectrum
ofreal-worldtasks(Eloundouetal.,2023). M˜ = EvolH(M,ENV), (10)Asdiscussedinpreviousopenproblem(§8.1), objective, but more experimental and theoretical
there are a large of unfulfilled objectives. How- supportisstillneeded. Thesefindingshighlighta
ever, most of the existing self-evolution frame- pressing need for more theoretical exploration in
worksareattheLow-levelwhichrequiresspecif- self-evolving LLMs. Addressing these concerns
ically designed modules (Yuan et al., 2024; Lu iscrucialforadvancingthefieldandensuringthat
etal.,2024a;Qiaoetal.,2024). Theseframeworks modelscaneffectivelylearnandimproveovertime.
are objective-dependent and rely on large human
8.4 Updating: Stability-PlasticityDilemma
efforts to develop. Exhausting all objectives are
not deployment-efficient which brings about the The stability-plasticity dilemma represents a cru-
urgent need to develop medium and high levels cial yet unresolved challenge that is essential for
self-evolution frameworks. At the medium level, iterativeself-evolution. Thisdilemmareflectsthe
itdoesn’trequireexperteffortstodesignspecific difficulty of balancing the need to retain previ-
modules. LLMs can self-evolve according to tar- ouslylearnedinformation(stability)whileadapting
getedobjectives. Thenatthehighlevel,LLMscan to new data or tasks (plasticity). Existing LLMs
investigatetheircurrentdeficienciesandevolvein either overlook this issue or adopt conventional
a targeted manner. In all, developing highly au- methods that may be ineffective. While training
tonomous self-evolution frameworks remains an models from scratch could mitigate the problem
openproblem. of catastrophic forgetting, it is highly inefficient,
particularly as model parameters increase expo-
8.3 ExperienceAcquisitionandRefinement:
nentiallyandautonomouslearningcapabilitiesad-
FromEmpiricaltoTheoretical
vance. Finding a balance between acquiring new
Supposewehaveaddressedtheprevioustwochal- skillsandpreservingexistingknowledgeiscrucial
lenges that we have developed promising self- forachievingeffectiveandefficientself-evolution,
evolution frameworks, the exploration of self- leadingtooverallimprovement.
evolutionLLMslackssolidtheoreticalgrounding.
8.5 Evaluation: SystematicandEvolving
ThisideapositsthatLLMscanself-improveorcor-
recttheiroutputs, withorwithoutfeedbackfrom To effectively assess LLMs, a dynamic, compre-
the environment. However, the mechanisms be- hensivebenchmarkiscrucial. Thisbecomeseven
hinditremainunclear. Studiesshowmixedresults: morepivotalasweprogresstowardsArtificialGen-
Huang et al. (2023) observed self-corrective be- eral Intelligence (AGI). Traditional static bench-
haviorinmodelswithover22billionparameters, marks risk obsolescence due to LLMs’ evolving
while Ganguli et al. (2023) finds LLMs struggle nature and potential access to test data through
to self-correct reasoning errors without external interactingwithenvironments,suchassearchen-
feedback. gines, undermining their reliability. A dynamic
Arelatedchallengeistheuseofself-generated benchmark,likeSotopia(Zhouetal.,2023a),pro-
dataforlearning. Criticsarguethisapproachcould posesasolutionbycreatinganLLM-basedenviron-
reduce linguistic diversity (Guo et al., 2023) and menttailoredforevaluatingthesocialintelligence
leadto"modelcollapse,"wheremodelsfailtocap- of LLMs, thereby avoiding the limitations posed
turecomplex, long-taileddatadistributions(Shu- bystaticbenchmarks.
mailovetal.,2023). Furthermore,Alemohammad
8.6 SafetyandSuperalignment
etal.(2023)revealthatgenerativemodelstrained
on their synthetic outputs progressively lose out- TheadvancementofLLMsopensthepossibilityfor
put quality and diversity. Fu et al. (2024) extend AIsystemstoachieveorevensurpassexpert-level
this by theoretically analyzing the impact of self- capabilities in both supportive and autonomous
consumingtrainingloopsonmodelperformance, decision-making. Forsafety,ensuringtheseLLMs
emphasizingtheimportanceofbalancingsynthetic alignwithhumanvaluesandpreferencesiscrucial,
andrealdatatomitigateerroraccumulation. particularlytomitigateinherentbiasesthatcanim-
Recentstudies(Yangetal.,2024c;Singhetal., pactareassuchaspoliticaldebates,ashighlighted
2023) also show that current methods struggle by Taubenfeld et al. (2024). OpenAI’s initiative,
to improve after more than three rounds of self- Superalignment(LeikeandSutskever,2023),aims
evolution. Onehypothesizedreasonisthattheself- toalignasuperintelligencebydevelopingscalable
criticofLLMhasnotco-evolvedwiththeevolving trainingmethods,validatingmodelsforalignment,and stress-testing the alignment process through RenatAksitov,SobhanMiryoosefi,ZonglinLi,Daliang
scalableoversight(Saundersetal.,2022),robust- Li, Sheila Babayan, Kavya Kopparapu, Zachary
Fisher,RuiqiGuo,SushantPrakash,PraneshSrini-
ness(Perezetal.,2022),automatedinterpretability
vasan, et al. 2023. Rest meets react: Self-
(Bills et al., 2023), and adversarial testing. Al-
improvement for multi-step reasoning llm agent.
thoughchallengesremain,Superalignmentmarks arXivpreprintarXiv:2312.10003.
aninitialattempttodevelopaself-evolvingLLM
SinaAlemohammad,JosueCasco-Rodriguez,Lorenzo
thatcloselyalignswithhumanethicsandvaluesin
Luzi, Ahmed Imtiaz Humayun, Hossein Babaei,
ascalableway. Daniel LeJeune, Ali Siahkoohi, and Richard Bara-
niuk. 2023. Self-consuming generative models go
9 Conclusion mad. In The Twelfth International Conference on
LearningRepresentations.
The evolution of LLMs towards self-evolution
Arian Askari, Roxana Petcu, Chuan Meng, Moham-
paradigmsrepresentsatransformativeshiftinartifi-
mad Aliannejadi, Amin Abolghasemi, Evangelos
cialintelligenceakintothehumanlearningprocess.
Kanoulas,andSuzanVerberne.2024. Self-seeding
Itispromisingtoovercomethelimitationsofcur- and multi-intent self-instructing llms for generat-
rentmodelsthatrelyheavilyonhumanannotation ingintent-awareinformation-seekingdialogs. arXiv
preprintarXiv:2402.11633.
andteachermodels. Thissurveypresentsacompre-
hensiveframeworkforunderstandinganddevelop-
Thomas Bäck and Hans-Paul Schwefel. 1993. An
ingself-evolvingLLMs,structuredarounditerative overviewofevolutionaryalgorithmsforparameter
cyclesofexperienceacquisition,refinement,updat- optimization. Evolutionarycomputation,1(1):1–23.
ing,andevaluation. Bydetailingadvancementsand
JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,
categorizing the evolution objectives within this XiaodongDeng,YangFan,WenbinGe,YuHan,Fei
framework, we offer a thorough overview of cur- Huang, et al. 2023. Qwen technical report. arXiv
preprintarXiv:2309.16609.
rentmethodsandhighlightthepotentialforLLMs
to adapt, learn, and improve autonomously. We Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
alsoidentifyexistingchallengesandproposedirec- Amanda Askell, Jackson Kernion, Andy Jones,
tionsforfutureresearch,aimingtoacceleratethe Anna Chen, Anna Goldie, Azalia Mirhoseini,
Cameron McKinnon, et al. 2022. Constitutional
progresstowardmoredynamic,intelligent,andeffi-
ai: Harmlessnessfromaifeedback. arXivpreprint
cientmodels. Thisworkdeepenstheunderstanding arXiv:2212.08073.
ofself-evolvingLLMs. Itpavesthewayforsignifi-
MaciejBesta,NilsBlach,AlesKubicek,RobertGersten-
cantadvancementsinAI,markingasteptowards
berger,MichalPodstawski,LukasGianinazzi,Joanna
achievingsuperintelligentsystemscapableofsur-
Gajda,TomaszLehmann,HubertNiewiadomski,Pi-
passinghumanperformanceincomplexreal-world otrNyczyk,etal.2024. Graphofthoughts: Solving
tasks. elaborateproblemswithlargelanguagemodels. In
Proceedings of the AAAI Conference on Artificial
Acknowledgements Intelligence,volume38,pages17682–17690.
Steven Bills, Nick Cammarata, Dan Mossing, Henk
This work was supported by Alibaba Group
Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever,
throughAlibabaResearchInternProgram.
Jan Leike, Jeff Wu, and William Saunders. 2023.
Languagemodelscanexplainneuronsinlanguage
models. URLhttps://openaipublic.blob.core.win-
References dows.net/neuron-explainer/paper/index.html.(Date
accessed: 14.05.2023).
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman, DavidBoud,RosemaryKeogh,andDavidWalker.2013.
DiogoAlmeida,JankoAltenschmidt,SamAltman, Reflection: Turningexperienceintolearning. Rout-
ShyamalAnadkat,etal.2023. Gpt-4technicalreport. ledge.
arXivpreprintarXiv:2303.08774.
KonstantinosBousmalis,GiuliaVezzani,DushyantRao,
Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui ColineManonDevin,AlexXLee,MariaBauzaVilla-
Zhang, and Wenpeng Yin. 2024. Large language longa,TodorDavchev,YuxiangZhou,AgrimGupta,
modelsformathematicalreasoning: Progressesand AkhilRaju,etal.2023. Robocat: Aself-improving
challenges. arXivpreprintarXiv:2402.00157. generalistagentforroboticmanipulation. Transac-
tionsonMachineLearningResearch.
TakuyaAkiba,MakotoShing,YujinTang,QiSun,and
DavidHa.2024. Evolutionaryoptimizationofmodel Tom Brown, Benjamin Mann, Nick Ryder, Melanie
mergingrecipes. arXivpreprintarXiv:2403.13187. Subbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda William Hart, et al. 2023. Evaluating language
Askell,etal.2020. Languagemodelsarefew-shot modelsformathematicsthroughinteractions. arXiv
learners. Advancesinneuralinformationprocessing preprintarXiv:2306.01694.
systems,33:1877–1901.
Wanyun Cui and Qianle Wang. 2023. Ada-instruct:
Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Adaptinginstructiongeneratorsforcomplexreason-
Bowen Baker, Leo Gao, Leopold Aschenbrenner, ing. arXivpreprintarXiv:2310.04484.
Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan
Leike, et al. 2023. Weak-to-strong generalization: DanielCDennett.1993. Consciousnessexplained. Pen-
Elicitingstrongcapabilitieswithweaksupervision. guinuk.
arXivpreprintarXiv:2312.09390.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
DavidJChalmers.1997. Theconsciousmind:Insearch LukeZettlemoyer.2024. Qlora: Efficientfinetuning
ofafundamentaltheory. OxfordPaperbacks. ofquantizedllms. AdvancesinNeuralInformation
ProcessingSystems,36.
Chi-MinChan,WeizeChen,YushengSu,JianxuanYu,
WeiXue,ShanghangZhang,JieFu,andZhiyuanLiu. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
2023. Chateval: Towardsbetterllm-basedevaluators KristinaToutanova.2018. Bert: Pre-trainingofdeep
throughmulti-agentdebate. InTheTwelfthInterna- bidirectionaltransformersforlanguageunderstand-
tionalConferenceonLearningRepresentations. ing. arXivpreprintarXiv:1810.04805.
BeiChen,FengjiZhang,AnhNguyen,DaoguangZan, JohnDewey.1938. Experienceandeducation: Kappa
ZeqiLin,Jian-GuangLou,andWeizhuChen.2022. deltapi. InternationalHonorSocietyinEducation.
Codet: Codegenerationwithgeneratedtests. arXiv
preprintarXiv:2207.10397. NingDing,YulinChen,GanquCui,XingtaiLv,Ruob-
ing Xie, Bowen Zhou, Zhiyuan Liu, and Maosong
LichangChen,ShiyangLi,JunYan,HaiWang,Kalpa
Sun.2024. Masteringtext,codeandmathsimultane-
Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srini-
ouslyviafusinghighlyspecializedlanguagemodels.
vasan,TianyiZhou,HengHuang,etal.2023a. Al-
arXivpreprintarXiv:2403.08281.
pagasus: Training a better alpaca with fewer data.
arXivpreprintarXiv:2307.08701.
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi
Zheng,ShengdingHu,ZhiyuanLiu,MaosongSun,
PinzhenChen,ZhichengGuo,BarryHaddow,andKen-
andBowenZhou.2023. Enhancingchat language
neth Heafield. 2023b. Iterative translation refine-
modelsbyscalinghigh-qualityinstructionalconver-
ment with large language models. arXiv preprint
sations. arXivpreprintarXiv:2305.14233.
arXiv:2306.03856.
YannDubois,ChenXuechenLi,RohanTaori,Tianyi
Weixin Chen and Bo Li. 2024. Grath: Gradual self-
Zhang,IshaanGulrajani,JimmyBa,CarlosGuestrin,
truthifyingforlargelanguagemodels. arXivpreprint
Percy S Liang, and Tatsunori B Hashimoto. 2024.
arXiv:2401.12292.
Alpacafarm: A simulation framework for methods
thatlearnfromhumanfeedback. AdvancesinNeural
Xinyun Chen, Maxwell Lin, Nathanael Schaerli, and
InformationProcessingSystems,36.
DennyZhou.2023c. Teachinglargelanguagemodels
toself-debug. InThe61stAnnualMeetingOfThe
TynaEloundou, SamManning, PamelaMishkin, and
AssociationForComputationalLinguistics.
DanielRock.2023. Gptsaregpts: Anearlylookat
Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun thelabormarketimpactpotentialoflargelanguage
Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo, models. arXivpreprintarXiv:2303.10130.
SongyangZhang,DahuaLin,KaiChen,etal.2023d.
Chrisantha Fernando, Dylan Banarse, Henryk
T-eval: Evaluatingthetoolutilizationcapabilitystep
Michalewski, Simon Osindero, and Tim Rock-
bystep. arXivpreprintarXiv:2312.14033.
täschel. 2023. Promptbreeder: Self-referential
ZixiangChen,YiheDeng,HuizhuoYuan,KaixuanJi, self-improvement via prompt evolution. arXiv
andQuanquanGu.2024. Self-playfine-tuningcon- preprintarXiv:2309.16797.
vertsweaklanguagemodelstostronglanguagemod-
els. arXivpreprintarXiv:2401.01335. Shi Fu, Sen Zhang, Yingjie Wang, Xinmei Tian, and
DachengTao.2024. Towardstheoreticalunderstand-
HyungWonChung,LeHou,ShayneLongpre,Barret ings of self-consuming generative models. arXiv
Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi preprintarXiv:2402.11778.
Wang,MostafaDehghani,SiddharthaBrahma,etal.
2022. Scalinginstruction-finetunedlanguagemodels. Deep Ganguli, Amanda Askell, Nicholas Schiefer,
arXivpreprintarXiv:2210.11416. ThomasILiao,Kamile˙Lukošiu¯te˙,AnnaChen,Anna
Goldie,AzaliaMirhoseini,CatherineOlsson,Danny
Katherine M Collins, Albert Q Jiang, Simon Frieder, Hernandez,etal.2023. Thecapacityformoralself-
Lionel Wong, Miri Zilka, Umang Bhatt, Thomas correctioninlargelanguagemodels. arXivpreprint
Lukasiewicz, Yuhuai Wu, Joshua B Tenenbaum, arXiv:2302.07459.Yingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan, JiaxinHuang,ShixiangShaneGu,LeHou,YuexinWu,
ShuyuanXu,ZelongLi,YongfengZhang,etal.2024. XuezhiWang,HongkunYu,andJiaweiHan.2022.
Openagi:Whenllmmeetsdomainexperts. Advances Large language models can self-improve. arXiv
inNeuralInformationProcessingSystems,36. preprintarXiv:2210.11610.
Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Jie Huang, Xinyun Chen, Swaroop Mishra,
Naumann, Michel Galley, Jianfeng Gao, and Hoi- Huaixiu Steven Zheng, Adams Wei Yu, Xiny-
fung Poon. 2023. Self-verification improves few- ingSong,andDennyZhou.2023. Largelanguage
shotclinicalinformationextraction. arXivpreprint models cannot self-correct reasoning yet. In The
arXiv:2306.00024. Twelfth International Conference on Learning
Representations.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong
Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
ShijueHuang,WanjunZhong,JianqiaoLu,QiZhu,Ji-
2023. Critic: Largelanguagemodelscanself-correct
ahuiGao,WeiwenLiu,YutaiHou,XingshanZeng,
with tool-interactive critiquing. arXiv preprint
YashengWang,LifengShang,etal.2024b. Planning,
arXiv:2305.11738.
creation,usage: Benchmarkingllmsforcomprehen-
Caglar Gulcehre, Tom Le Paine, Srivatsan Srini- sivetoolutilizationinreal-worldcomplexscenarios.
vasan,KseniaKonyushkova,LotteWeerts,Abhishek arXivpreprintarXiv:2401.17167.
Sharma, Aditya Siddhant, Alex Ahern, Miaosen
Wang, Chenjie Gu, et al. 2023. Reinforced self- GabrielIlharco,MarcoTulioRibeiro,MitchellWorts-
training(rest)forlanguagemodeling. arXivpreprint man,LudwigSchmidt,HannanehHajishirzi,andAli
arXiv:2308.08998. Farhadi.2022. Editingmodelswithtaskarithmetic.
InTheEleventhInternationalConferenceonLearn-
YanzhuGuo,GuokanShang,MichalisVazirgiannis,and ingRepresentations.
ChloéClavel.2023. Thecuriousdeclineoflinguistic
diversity:Traininglanguagemodelsonsynthetictext. ShuyangJiang,YuhaoWang,andYuWang.2023. Self-
arXivpreprintarXiv:2311.09807. evolve: A code evolution framework via large lan-
guagemodels. arXivpreprintarXiv:2306.02907.
Anil K Gupta, Ken G Smith, and Christina E Shal-
ley. 2006. The interplay between exploration and Geunwoo Kim, Pierre Baldi, and Stephen McAleer.
exploitation. Academy of management journal, 2023. Languagemodelscansolvecomputertasks.
49(4):693–706. arXivpreprintarXiv:2303.17491.
Joshua Hare. 2019. Dealing with sparse re-
JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,
wards in reinforcement learning. arXiv preprint
JoelVeness,GuillaumeDesjardins,AndreiARusu,
arXiv:1910.09281.
Kieran Milan, John Quan, Tiago Ramalho, Ag-
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. nieszka Grabska-Barwinska, et al. 2017. Over-
Distillingtheknowledgeinaneuralnetwork. arXiv coming catastrophic forgetting in neural networks.
preprintarXiv:1503.02531. Proceedings of the national academy of sciences,
114(13):3521–3526.
JohnHHolland.1992. Adaptationinnaturalandartifi-
cialsystems: anintroductoryanalysiswithapplica- KelvinJLKoa,YunshanMa,RitchieNg,andTat-Seng
tionstobiology,control,andartificialintelligence. Chua.2024. Learningtogenerateexplainablestock
MITpress. predictionsusingself-reflectivelargelanguagemod-
els. arXivpreprintarXiv:2402.03659.
OrHonovich,ThomasScialom,OmerLevy,andTimo
Schick. 2022. Unnatural instructions: Tuning lan- Nicholas Lee, Thanakul Wattanawong, Sehoon Kim,
guagemodelswith(almost)nohumanlabor. arXiv Karttikeya Mangalam, Sheng Shen, Gopala Anu-
preprintarXiv:2212.09689.
manchipali,MichaelWMahoney,KurtKeutzer,and
AmirGholami.2024. Llm2llm: Boostingllmswith
ArianHosseini,XingdiYuan,NikolayMalkin,Aaron
novel iterative data enhancement. arXiv preprint
Courville, Alessandro Sordoni, and Rishabh Agar-
arXiv:2403.15042.
wal.2024. V-star: Trainingverifiersforself-taught
reasoners. arXivpreprintarXiv:2402.06457.
JanLeikeandIlyaSutskever.2023. Introducingsuper-
Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, alignment. Accessed: 2024-04-01.
YuanzhiLi, SheanWang, LuWang, WeizhuChen,
etal.2021. Lora: Low-rankadaptationoflargelan- Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and
guagemodels. InInternationalConferenceonLearn- Zhi Jin. 2024a. Evocodebench: An evolving code
ingRepresentations. generationbenchmarkalignedwithreal-worldcode
repositories. arXivpreprintarXiv:2404.00599.
Jianheng Huang, Leyang Cui, Ante Wang, Chengyi
Yang,XintingLiao,LinfengSong,JunfengYao,and MingLi,LichangChen,JiuhaiChen,ShwaiHe,Jiuxi-
Jinsong Su. 2024a. Mitigating catastrophic forget- angGu,andTianyiZhou.2024b. Selectivereflection-
tinginlargelanguagemodelswithself-synthesized tuning: Student-selected data recycling for llm
rehearsal. arXivpreprintarXiv:2403.01244. instruction-tuning. arXivpreprintarXiv:2402.10110.MingLi,YongZhang,ZhitaoLi,JiuhaiChen,Lichang Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei
Chen,NingCheng,JianzongWang,TianyiZhou,and Wang,FeiMi,BaojunWang,WeichaoWang,Lifeng
JingXiao.2023a. Fromquantitytoquality:Boosting Shang,andQunLiu.2023. Self: Language-driven
llmperformancewithself-guideddataselectionfor self-evolution for large language model. arXiv
instructiontuning. arXivpreprintarXiv:2308.12032. preprintarXiv:2310.00533.
Shimin Li, Tianxiang Sun, and Xipeng Qiu. 2024c. KemingLu,BowenYu,ChangZhou,andJingrenZhou.
Agent alignment in evolving social norms. arXiv 2024a. Large language models are superpositions
preprintarXiv:2401.04620.
of all characters: Attaining arbitrary role-play via
self-alignment. arXivpreprintarXiv:2401.12474.
XianLi,PingYu,ChuntingZhou,TimoSchick,Luke
Zettlemoyer, Omer Levy, Jason Weston, and Mike
XinyuLu,BowenYu,YaojieLu,HongyuLin,Haiyang
Lewis.2023b. Self-alignmentwithinstructionback-
Yu, Le Sun, Xianpei Han, and Yongbin Li. 2024b.
translation. arXivpreprintarXiv:2308.06259.
Sofa: Shieldedon-the-flyalignmentviapriorityrule
XiaonanLiandXipengQiu.2023. Mot: Memory-of- following. arXivpreprintarXiv:2402.17358.
thoughtenableschatgpttoself-improve. InProceed-
ingsofthe2023ConferenceonEmpiricalMethods Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-
inNaturalLanguageProcessing,pages6354–6374. uboGeng,WenxiangHu,ChongyangTao,JingMa,
QingweiLin,andDaxinJiang.2024. Wizardcoder:
Chin-YewLin.2004. Rouge: Apackageforautomatic Empoweringcodelargelanguagemodelswithevol-
evaluation of summaries. In Text summarization instruct. InTheTwelfthInternationalConferenceon
branchesout,pages74–81. LearningRepresentations.
YongLin,HangyuLin,WeiXiong,ShizheDiao,Jian-
AmanMadaan, NiketTandon,PrakharGupta,Skyler
mengLiu,JipengZhang,RuiPan,HaoxiangWang,
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
WenbinHu,HanningZhang,HanzeDong,RenjiePi,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
HanZhao,NanJiang,HengJi,YuanYao,andTong
Shashank Gupta, Bodhisattwa Prasad Majumder,
Zhang.2024. Mitigatingthealignmenttaxofrlhf. In
Katherine Hermann, Sean Welleck, Amir Yazdan-
arXiv.
bakhsh, and Peter Clark. 2023. Self-refine: Itera-
tive refinement with self-feedback. arXiv preprint
Aiwei Liu, Haoping Bai, Zhiyun Lu, Xiang Kong,
arXiv:2303.17651.
Simon Wang, Jiulong Shan, Meng Cao, and Lijie
Wen.2024a. Directlargelanguagemodelalignment
SantiagoMiretandNMKrishnan.2024. Arellmsready
through self-rewarding contrastive prompt distilla-
forreal-worldmaterialsdiscovery? arXivpreprint
tion. arXivpreprintarXiv:2402.11907.
arXiv:2402.05200.
Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and
LingmingZhang.2024b. Isyourcodegeneratedby Michael Noukhovitch, Samuel Lavoie, Florian Strub,
chatgptreallycorrect? rigorousevaluationoflarge andAaronCCourville.2024. Languagemodelalign-
languagemodelsforcodegeneration. Advancesin mentwithelasticreset. AdvancesinNeuralInforma-
NeuralInformationProcessingSystems,36. tionProcessingSystems,36.
LeiLiu,XiaoyanYang,YueShen,BinbinHu,Zhiqiang LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
Zhang, Jinjie Gu, and Guannan Zhang. 2023a. CarrollWainwright,PamelaMishkin,ChongZhang,
Think-in-memory: Recallingandpost-thinkingen- SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
able llms with long-term memory. arXiv preprint 2022. Training languagemodelsto followinstruc-
arXiv:2311.08719. tionswithhumanfeedback. Advancesinneuralin-
formationprocessingsystems,35:27730–27744.
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xu-
anyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,
Charles Packer, Vivian Fang, Shishir G Patil, Kevin
Kaiwen Men, Kejuan Yang, et al. 2023b. Agent-
Lin,SarahWooders,andJosephEGonzalez.2023.
bench: Evaluating llms as agents. arXiv preprint
Memgpt: Towardsllmsasoperatingsystems. arXiv
arXiv:2308.03688.
preprintarXiv:2310.08560.
Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang,
GaryGYen,andKayChenTan.2021. Asurveyon Jing-ChengPang,PengyuanWang,KaiyuanLi,Xiong-
evolutionaryneuralarchitecturesearch. IEEEtrans- HuiChen,JiachengXu,ZongzhangZhang,andYang
actions on neural networks and learning systems, Yu.2023. Languagemodelself-improvementbyre-
34(2):550–570. inforcementlearningcontemplation. arXivpreprint
arXiv:2305.14483.
Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V KishorePapineni,SalimRoukos,ToddWard,andWei-
Le, Barret Zoph, Jason Wei, et al. 2023. The flan JingZhu.2002. Bleu: amethodforautomaticevalu-
collection: Designingdataandmethodsforeffective ationofmachinetranslation. InProceedingsofthe
instructiontuning. InInternationalConferenceon 40thannualmeetingoftheAssociationforComputa-
MachineLearning,pages22631–22648.PMLR. tionalLinguistics,pages311–318.Keqin Peng, Liang Ding, Qihuang Zhong, Yuanxin Noah Shinn, Beck Labash, and Ashwin Gopinath.
Ouyang,WengeRong,ZhangXiong,andDacheng 2023. Reflexion: an autonomous agent with dy-
Tao. 2023. Token-level self-evolution training for namic memory and self-reflection. arXiv preprint
sequence-to-sequence learning. In Proceedings of arXiv:2303.11366.
the61stAnnualMeetingoftheAssociationforCom-
putational Linguistics (Volume 2: Short Papers), Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao,
pages841–850. Yarin Gal, Nicolas Papernot, and Ross Anderson.
2023. The curse of recursion: Training on gen-
EthanPerez,SaffronHuang,FrancisSong,TrevorCai, erated data makes models forget. arXiv preprint
Roman Ring, John Aslanides, Amelia Glaese, Nat arXiv:2305.17493.
McAleese,andGeoffreyIrving.2022. Redteaming
languagemodelswithlanguagemodels. InProceed- David Silver, Aja Huang, Chris J Maddison, Arthur
ingsofthe2022ConferenceonEmpiricalMethods Guez,LaurentSifre,GeorgeVanDenDriessche,Ju-
inNaturalLanguageProcessing,pages3419–3448. lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam,MarcLanctot,etal.2016. Mastering
ChengQian,ShihaoLiang,YujiaQin,YiningYe,Xin
thegameofgowithdeepneuralnetworksandtree
Cong, Yankai Lin, Yesai Wu, Zhiyuan Liu, and
search. nature,529(7587):484–489.
MaosongSun.2024. Investigate-consolidate-exploit:
Ageneralstrategyforinter-taskagentself-evolution. DavidSilver,ThomasHubert,JulianSchrittwieser,Ioan-
arXivpreprintarXiv:2401.13996. nis Antonoglou, Matthew Lai, Arthur Guez, Marc
Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
ShuofeiQiao,NingyuZhang,RunnanFang,YujieLuo,
Graepel,etal.2017. Masteringchessandshogiby
WangchunshuZhou,YuchenEleanorJiang,Chengfei
self-playwithageneralreinforcementlearningalgo-
Lv, and Huajun Chen. 2024. Autoact: Automatic
rithm. arXivpreprintarXiv:1712.01815.
agentlearningfromscratchviaself-planning. arXiv
preprintarXiv:2401.05268.
AviSingh,JohnDCo-Reyes,RishabhAgarwal,Ankesh
Anand, Piyush Patil, Peter J Liu, James Harri-
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
son, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al.
Lee,SharanNarang,MichaelMatena,YanqiZhou,
2023. Beyond human data: Scaling self-training
Wei Li, and Peter J Liu. 2020. Exploring the lim-
for problem-solving with language models. arXiv
its of transfer learning with a unified text-to-text
preprintarXiv:2312.06585.
transformer. Journalofmachinelearningresearch,
21(140):1–67.
ChenyangSong,XuHan,ZheniZeng,KuaiLi,Chen
Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Chen, Zhiyuan Liu, Maosong Sun, and Tao Yang.
RobertDadashi,GeoffreyCideron,OlivierBachem, 2023. Conpet: Continual parameter-efficient tun-
and Johan Ferret. 2024. Warm: On the benefits ing for large language models. arXiv preprint
ofweightaveragedrewardmodels. arXivpreprint arXiv:2309.14763.
arXiv:2401.12187.
Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian
BaptisteRoziere,JonasGehring,FabianGloeckle,Sten Li, and Bill Yuchen Lin. 2024. Trial and error:
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Exploration-based trajectory optimization for llm
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. agents. arXivpreprintarXiv:2403.02502.
Codellama:Openfoundationmodelsforcode. arXiv
preprintarXiv:2308.12950. WolfgangStammer,FelixFriedrich,DavidSteinmann,
HikaruShindo,andKristianKersting.2023. Learn-
WilliamSaunders,CatherineYeh,JeffWu,StevenBills, ingbyself-explaining.
LongOuyang,JonathanWard,andJanLeike.2022.
Self-critiquingmodelsforassistinghumanevaluators. ZhiqingSun,YikangShen,HongxinZhang,Qinhong
arXivpreprintarXiv:2206.05802. Zhou,ZhenfangChen,DavidCox,YimingYang,and
Chuang Gan. 2023. Salmon: Self-alignment with
Philipp Schoenegger, Peter S Park, Ezra Karger, and
principle-followingrewardmodels. arXivpreprint
PhilipETetlock.2024. Ai-augmentedpredictions:
arXiv:2310.05910.
Llmassistantsimprovehumanforecastingaccuracy.
arXivpreprintarXiv:2402.07862.
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin
Zhang, Zhenfang Chen, David Cox, Yiming Yang,
Donald A Schön. 2017. The reflective practitioner:
and Chuang Gan. 2024. Principle-driven self-
Howprofessionalsthinkinaction. Routledge.
alignment of language models from scratch with
JohnRSearle.1986. Minds,brainsandscience. Har- minimal human supervision. Advances in Neural
varduniversitypress. InformationProcessingSystems,36.
Noah Shinn, Federico Cassano, Ashwin Gopinath, Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan
Karthik Narasimhan, and Shunyu Yao. 2024. Re- Hu, Yongrui Chen, and Guilin Qi. 2023. Evalu-
flexion: Languageagentswithverbalreinforcement ation of chatgpt as a question answering system
learning. AdvancesinNeuralInformationProcess- for answering complex questions. arXiv preprint
ingSystems,36. arXiv:2303.07992.Zhengwei Tao, Xiancai Chen, Zhi Jin, Xiaoying Bai, HaoyuWang,GuozhengMa,ZiqiaoMeng,ZeyuQin,
HaiyanZhao, andYiweiLou.2024a. Evit: Event- LiShen,ZhongZhang,BingzheWu,LiuLiu,Yatao
orientedinstructiontuningforeventreasoning. Bian, TingyangXu, etal.2024c. Step-on-feettun-
ing: Scalingself-alignmentofllmsviabootstrapping.
Zhengwei Tao, Zhi Jin, Junqiang Huang, Xiancai arXivpreprintarXiv:2402.07610.
Chen,XiaoyingBai,HaiyanZhao,YifanZhang,and
ChongyangTao.2024b. Meel: Multi-modalevent Jiuniu Wang, Zehua Du, Yuyuan Zhao, Bo Yuan,
evolutionlearning. Kexiang Wang, Jian Liang, Yaxi Zhao, Yihen Lu,
Gengliang Li, Junlong Gao, et al. 2024d. Aesopa-
AmirTaubenfeld,YanivDover,RoiReichart,andAriel gent: Agent-drivenevolutionarysystemonstory-to-
Goldstein. 2024. Systematic biases in llm simula- videoproduction. arXivpreprintarXiv:2403.07952.
tionsofdebates. arXivpreprintarXiv:2402.04049.
Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun
Gong,ChaoZhang,andYelongShen.2023a. Adapt-
Gemini Team, Rohan Anil, Sebastian Borgeaud,
ing llm agents through communication. arXiv
Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
preprintarXiv:2310.01444.
Radu Soricut, Johan Schalkwyk, Andrew M Dai,
Anja Hauth, et al. 2023. Gemini: a family of
RuiyiWang,HaofeiYu,WenxinZhang,ZhengyangQi,
highlycapablemultimodalmodels. arXivpreprint
Maarten Sap, Graham Neubig, Yonatan Bisk, and
arXiv:2312.11805.
HaoZhu.2024e. Sotopia-π: Interactivelearningof
sociallyintelligentlanguageagents. arXivpreprint
YongqiTong,DaweiLi,SizheWang,YujiaWang,Fei
arXiv:2403.08715.
Teng,andJingboShang.2024. Canllmslearnfrom
previousmistakes? investigatingllms’errorstoboost XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,
forreasoning. arXivpreprintarXiv:2403.20046. EdChi,SharanNarang,AakankshaChowdhery,and
DennyZhou.2022. Self-consistencyimproveschain
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- of thought reasoning in language models. arXiv
bert, Amjad Almahairi, Yasmine Babaei, Nikolay preprintarXiv:2203.11171.
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale, et al. 2023a. Llama 2: Open founda- YizhongWang,YeganehKordi,SwaroopMishra,Alisa
tion and fine-tuned chat models. arXiv preprint Liu,NoahASmith,DanielKhashabi,andHannaneh
arXiv:2307.09288. Hajishirzi.2023b. Self-instruct: Aligninglanguage
modelswithself-generatedinstructions. InThe61st
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- Annual Meeting Of The Association For Computa-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay tionalLinguistics.
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale, et al. 2023b. Llama 2: Open founda- JasonWei,XuezhiWang,DaleSchuurmans,Maarten
tion and fine-tuned chat models. arXiv preprint Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,
arXiv:2307.09288. etal.2022. Chain-of-thoughtpromptingelicitsrea-
soninginlargelanguagemodels. Advancesinneural
TaoTu,AnilPalepu,MikeSchaekermann,KhaledSaab, informationprocessingsystems,35:24824–24837.
Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna
YixuanWeng,MinjunZhu,FeiXia,BinLi,ShizhuHe,
Li, Mohamed Amin, Nenad Tomasev, et al. 2024.
Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao.
Towardsconversationaldiagnosticai. arXivpreprint
2023. Large language models are better reasoners
arXiv:2401.05654.
withself-verification. InFindingsoftheAssociation
forComputationalLinguistics: EMNLP2023,pages
DennisUlmer,ElmanMansimov,KaixiangLin,Justin
2550–2575.
Sun,XibinGao,andYiZhang.2024. Bootstrapping
llm-basedtask-orienteddialogueagentsviaself-talk.
MitchellWortsman,GabrielIlharco,SamirYaGadre,
arXivpreprintarXiv:2401.05033.
RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-
cos,HongseokNamkoong,AliFarhadi,YairCarmon,
FanqiWan,XintingHuang,DengCai,XiaojunQuan,
SimonKornblith,etal.2022. Modelsoups: averag-
WeiBi,andShumingShi.2024. Knowledgefusion
ingweightsofmultiplefine-tunedmodelsimproves
of large language models. In The Twelfth Interna-
accuracy without increasing inference time. In In-
tionalConferenceonLearningRepresentations.
ternationalconferenceonmachinelearning,pages
23965–23998.PMLR.
Bo Wang, Tianxiang Sun, Hang Yan, Siyin Wang,
Qingyuan Cheng, and Xipeng Qiu. 2024a. In- ShengguangWu,KemingLu,BenfengXu,JunyangLin,
memory learning: A declarative learning frame- QiSu,andChangZhou.2023. Self-evolveddiverse
work for large language models. arXiv preprint datasamplingforefficientinstructiontuning. arXiv
arXiv:2403.02757. preprintarXiv:2311.08182.
Boshi Wang, Hao Fang, Jason Eisner, Benjamin TongtongWu,LinhaoLuo,Yuan-FangLi,ShiruiPan,
VanDurme,andYuSu.2024b. Llmsintheimaginar- Thuy-TrangVu,andGholamrezaHaffari.2024. Con-
ium: Toollearningthroughsimulatedtrialanderror. tinuallearningforlargelanguagemodels: Asurvey.
arXivpreprintarXiv:2403.04746. arXivpreprintarXiv:2402.01364.Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
PuZhao,JiazhanFeng,ChongyangTao,andDaxin TomGriffiths,YuanCao,andKarthikNarasimhan.
Jiang. 2023a. Wizardlm: Empowering large lan- 2024. Treeofthoughts: Deliberateproblemsolving
guagemodelstofollowcomplexinstructions. arXiv with large language models. Advances in Neural
preprintarXiv:2304.12244. InformationProcessingSystems,36.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Shafran,KarthikNarasimhan,andYuanCao.2022.
Lin,andDaxinJiang.2024a. WizardLM:Empow- React: Synergizingreasoningandactinginlanguage
ering large pre-trained language models to follow models. arXivpreprintarXiv:2210.03629.
complexinstructions. InTheTwelfthInternational
ConferenceonLearningRepresentations. LeYu,BowenYu,HaiyangYu,FeiHuang,andYongbin
Li.2023a. Languagemodelsaresupermario: Ab-
Jie Xu, Hanbo Zhang, Xinghang Li, Huaping Liu, sorbingabilitiesfromhomologousmodelsasafree
XuguangLan,andTaoKong.2024b. Sinvig: Aself- lunch. arXivpreprintarXiv:2311.03099.
evolving interactive visual agent for human-robot
interaction. arXivpreprintarXiv:2402.11792. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,
Zhengying Liu, Yu Zhang, James T Kwok, Zhen-
Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xi- guo Li, Adrian Weller, and Weiyang Liu. 2023b.
aolong Wang, Weidong Liu, and Yang Liu. 2023b. Metamath: Bootstrapyourownmathematicalques-
Exploring large language models for communica- tions for large language models. arXiv preprint
tiongames: Anempiricalstudyonwerewolf. arXiv arXiv:2309.12284.
preprintarXiv:2309.04658.
WeizheYuan,RichardYuanzhePang,KyunghyunCho,
PrateekYadav,DerekTam,LeshemChoshen,ColinA Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.
Raffel,andMohitBansal.2024. Ties-merging: Re- 2024. Self-rewarding language models. arXiv
solving interference when merging models. Ad- preprintarXiv:2401.10020.
vances in Neural Information Processing Systems,
36. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting
Dong, ChuanqiTan, andChangZhou.2023. Scal-
KevinYang,DanKlein,AsliCelikyilmaz,NanyunPeng, ing relationship on learning mathematical reason-
and Yuandong Tian. 2023a. Rlcd: Reinforcement ing with large language models. arXiv preprint
learning from contrastive distillation for lm align- arXiv:2308.01825.
ment. In The Twelfth International Conference on
LearningRepresentations. Eric Zelikman, Eliana Lorch, Lester Mackey, and
Adam Tauman Kalai. 2023. Self-taught optimizer
RuiYang,LinSong,YanweiLi,SijieZhao,YixiaoGe, (stop): Recursivelyself-improvingcodegeneration.
XiuLi,andYingShan.2024a. Gpt4tools: Teaching arXivpreprintarXiv:2310.02304.
largelanguagemodeltousetoolsviaself-instruction.
AdvancesinNeuralInformationProcessingSystems, Eric Zelikman, Jesse Mu, Noah D Goodman, and
36. YuhuaiTonyWu.2022. Star: Self-taughtreasoner
bootstrappingreasoningwithreasoning. Advancesin
Zeyuan Yang, Peng Li, and Yang Liu. 2023b. Fail- NeuralInformationProcessingSystems(NeurIPS).
urespavetheway: Enhancinglargelanguagemodels
throughtuning-freeruleaccumulation. InProceed- Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang,
ingsofthe2023ConferenceonEmpiricalMethods Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei
inNaturalLanguageProcessing,pages1751–1777. Zhang,AnjiLiu,Song-ChunZhu,etal.2024a. Proa-
gent:buildingproactivecooperativeagentswithlarge
Zhaorui Yang, Qian Liu, Tianyu Pang, Han Wang, languagemodels. InProceedingsoftheAAAICon-
HaozheFeng,MinfengZhu,andWeiChen.2024b. ferenceonArtificialIntelligence,volume38,pages
Self-distillationbridgesdistributiongapinlanguage 17591–17599.
modelfine-tuning. arXivpreprintarXiv:2402.13669.
DanZhang,ZiniuHu,SiningZhoubian,ZhengxiaoDu,
Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei KaiyuYang,ZihanWang,YisongYue,YuxiaoDong,
Huang, and Yang Liu. 2024c. React meets ac- andJieTang.2024b. Sciglm: Trainingscientificlan-
tre: Autonomous annotations of agent trajecto- guagemodelswithself-reflectiveinstructionannota-
ries for contrastive self-training. arXiv preprint tionandtuning. arXivpreprintarXiv:2401.07950.
arXiv:2403.14589.
WenqiZhang,YongliangShen,LinjuanWu,Qiuying
Zonghan Yang, An Liu, Zijun Liu, Kaiming Liu, Peng,JunWang,YuetingZhuang,andWeimingLu.
Fangzhou Xiong, Yile Wang, Zeyuan Yang, 2024c. Self-contrast: Betterreflectionthroughincon-
Qingyuan Hu, Xinrui Chen, Zhenhe Zhang, et al. sistentsolvingperspectives.
2024d. Towards unified alignment between
agents, humans, and environment. arXiv preprint Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang,
arXiv:2402.07744. Yongliang Shen, GuiyangHou, Zeqi Tan, Peng Li,Yueting Zhuang, and Weiming Lu. 2024d. Agent- Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng,
pro: Learning to evolve via policy-level reflection Ningyu Zhang, Shiwei Lyu, Yue Shen, Lei Liang,
andoptimization. arXivpreprintarXiv:2402.17574. Jinjie Gu, and Huajun Chen. 2024. Knowa-
gent: Knowledge-augmentedplanningforllm-based
XiaoyingZhang,BaolinPeng,YeTian,JingyanZhou, agents. arXivpreprintarXiv:2403.03101.
Lifeng Jin, Linfeng Song, Haitao Mi, and Helen
Meng.2024e. Self-alignmentforfactuality: Mitigat-
inghallucinationsinllmsviaself-evaluation. arXiv
preprintarXiv:2402.09267.
Haoqi Zheng, Qihuang Zhong, Liang Ding, Zhiliang
Tian,XinNiu,ChangjianWang,DongshengLi,and
Dacheng Tao. 2023. Self-evolution learning for
mixup: Enhancedataaugmentationonfew-shottext
classificationtasks. InProceedingsofthe2023Con-
ferenceonEmpiricalMethodsinNaturalLanguage
Processing,pages8964–8974.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024a.
Judging llm-as-a-judge with mt-bench and chatbot
arena. AdvancesinNeuralInformationProcessing
Systems,36.
TianyuZheng,ShuyueGuo,XingweiQu,JiaweiGuo,
Weixu Zhang, Xinrun Du, Chenghua Lin, Wen-
hao Huang, Wenhu Chen, Jie Fu, et al. 2024b.
Kun: Answerpolishmentforchineseself-alignment
with instruction back-translation. arXiv preprint
arXiv:2401.06477.
Li Zhong, Zilong Wang, and Jingbo Shang. 2024a.
Ldb: A large language model debugger via verify-
ingruntimeexecutionstep-by-step. arXivpreprint
arXiv:2402.16906.
Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and
DachengTao.2023. Self-evolutionlearningfordis-
criminative language model pretraining. In Find-
ingsoftheAssociationforComputationalLinguis-
tics: ACL2023,pages4130–4145.
WanjunZhong,LianghongGuo,QiqiGao,HeYe,and
YanlinWang.2024b. Memorybank: Enhancinglarge
language models with long-term memory. In Pro-
ceedingsoftheAAAIConferenceonArtificialIntelli-
gence,volume38,pages19724–19731.
ChuntingZhou,PengfeiLiu,PuxinXu,SrinivasanIyer,
JiaoSun,YuningMao,XuezheMa,AviaEfrat,Ping
Yu,LiliYu,etal.2024. Lima: Lessismoreforalign-
ment. AdvancesinNeuralInformationProcessing
Systems,36.
XuhuiZhou,HaoZhu,LeenaMathur,RuohongZhang,
HaofeiYu,ZhengyangQi,Louis-PhilippeMorency,
Yonatan Bisk, Daniel Fried, Graham Neubig, et al.
2023a. Sotopia: Interactive evaluation for social
intelligenceinlanguageagents. InTheTwelfthInter-
nationalConferenceonLearningRepresentations.
ZhehuaZhou,JiayangSong,KunpengYao,ZhanShu,
and Lei Ma. 2023b. Isr-llm: Iterative self-refined
large language model for long-horizon sequential
taskplanning. arXivpreprintarXiv:2308.13724.