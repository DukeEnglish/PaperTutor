A mean curvature flow arising in adversarial training
Leon Bungert∗ Tim Laux† Kerrek Stinson ‡
April 23, 2024
Abstract
Weconnectadversarialtrainingforbinaryclassificationtoageometricevolutionequa-
tion for the decision boundary. Relying on a perspective that recasts adversarial training
as a regularization problem, we introduce a modified training scheme that constitutes a
minimizing movements scheme for a nonlocal perimeter functional. We prove that the
scheme is monotone and consistent as the adversarial budget vanishes and the perime-
ter localizes, and as a consequence we rigorously show that the scheme approximates a
weighted mean curvature flow. This highlights that the efficacy of adversarial training
maybeduetolocally minimizingthelengthofthedecision boundary. Inouranalysis, we
introduceavarietyoftoolsforworkingwiththesubdifferentialofasupremal-typenonlocal
total variation and its regularity properties.
Keywords: meancurvatureflow,adversarialtraining,adversarialmachinelearning,min-
imizing movements, monotone and consistent schemes
AMS subject classifications: 28A75, 35D40, 49J45, 53E10, 68T05
1 Introduction
Inthelastdecade,machinelearningalgorithmsandinparticulardeeplearninghaveexperienced
anunprecedentedsuccessstory. Suchmethods haveproventheircapabilities,interalia,forthe
difficulttasksofimageclassificationandgeneration. Mostrecently,theadventoflargelanguage
models is expected to have a strong impact on various aspects of society.
At the same time, the success of machine learning is accompanied by concerns about the
reliability and safety of its methods. Already more than ten years ago it was observed that
neuralnetworksforimageclassificationaresusceptibletoadversarialattacks[35],meaningthat
imperceptible or seemingly harmless perturbations of images can lead to severe misclassifica-
tions. As a consequence,the deploymentofsuchmethods in situationsthat affectthe integrity
and safety of humans, e.g., for self-driving cars or medical image classification, is risky.
To mitigate these risks, the scientific community has been developing different approaches
to robustify machine learning in the presence of potential adversaries. The most prominent of
these approaches in the context of classification tasks is adversarial training [23, 27], which is
∗Institute of Mathematics & Center for Artificial Intelligence and Data Science (CAIDAS), University of
Würzburg, Emil-Fischer-Str. 40,97074Würzburg, Germany. Email: leon.bungert@uni-wuerzburg.de
†Faculty of Mathematics, University of Regensburg, Universitässtraße 31, 93053 Regensburg, Germany.
Email: tim.laux@ur.de
‡HausdorffCenterforMathematics,UniversityofBonn,EndenicherAllee62,53115Bonn,Germany. Email:
kerrek.stinson@hcm.uni-bonn.de
1
4202
rpA
22
]PA.htam[
1v20441.4042:viXraa robust optimization problem of the form
inf E sup ℓ(u(x˜),y) , (1.1)
(x,y) µ
u ∈H ∼ "x˜ ∈B(x,ε) #
where we use the notation E [f(z)]:= f(z)dµ(z). The ingredients of adversarial training
z µ
∼
are readily explained: The probability measure µ ( ) models the distribution of given
R ∈P X ×Y
training pairs in the so-called feature space and the label space . Here is a metric
space, and a set, e.g., = 0,...,K 1 dX escribing K N classesY . In realisX tic situations,
Y Y { − } ∈
one uses an empirical distribution of the form µ = 1 M δ where (x ,y ) for
M i=1 (xi,yi) i i ∈ X ×Y
i =1,...,M. The optimization takes place in a so-called hypothesis class which is nothing
P H
buta classoffunctions from to ,e.g., linearfunctions,measurablefunctions, parametrized
neural networks, etc. We letX ℓ : Y R be a so-called loss function, which is often chosen
Y ×Y →
asapowerofanormorf-divergence. Finally,inessence,the optimalclassifierushouldsatisfy
u(x˜) y for µ-almost every (x,y) and all x˜ B(x,ε). Thereby, (1.1) enforces
≈ ∈ X × Y ∈
robustness of the classification in ε-balls around the data points, where ε > 0 is called the
adversarial budget.
Alreadyin[27]ithasbeenempiricallyobservedthat(1.1)indeedallowsonetocomputeneu-
ral networks that are significantly more robust than those trained with the standard approach
(corresponding to ε = 0 in (1.1)). However, the mathematical understanding of adversar-
ial training and related problems only began growing a few years ago: One line of research
connects (1.1) with (multimarginal) optimal transport or distributionally robust optimization
problems [21, 32, 33] and uses tools from these disciplines to analyse adversarialtraining. Ex-
istence of solutions to (1.1) in the binary classification case where = 0,1 , ℓ is the 0-1 loss
Y { }
ℓ(y˜,y)= y˜ y ,and isaclassofmeasurablefunctionswasprovedin[2,7]. In[2],theauthors
| − | H
considerclosedballsB(x,ε)in(1.1)andworkwithclassifierswhicharecharacteristicfunctions
of universally measurable sets in RN. In contrast, in [7] open balls are used and the classifiers
arecharacteristicfunctionsofBorelmeasurablesubsetsofagenericmetricmeasurespace. The
authors of [7] also proved that adversarialtraining for binary classificationis equivalent to the
following variational regularization problem:
inf E [1 (x) y ]+εPer (A). (1.2)
(x,y) µ A ε
A B( ) ∼ | − |
∈ X
Here,Per isa non-localanddata-dependentperimeterfunctionalthatregularizesthe decision
ε
boundary ∂A between the two classes. A similar decomposition into a “natural error” and
a “boundary error” was studied in [38] and used to derive the TRADES algorithm, which
essentially replaces the regularization parameter ε in (1.2) by 1 for λ > 0. Also for other
λ
notions of robustness which are weaker than adversarial robustness, geometric interpretations
similar to (1.2) exists, see [7, Section 4] or [6]. We also remark that generalizations of some
of the results in [2, 7] to the case of multi-class classification can be found in [19]. Finally, an
overview of recent mathematical developments in the field can be found in [20].
The perspective in (1.2) opens the door for the geometric analysis of adversarially robust
classifiers. As a first step in this direction, in [7] it was shown that maximal and minimal
minimizers of (1.2) possess one-sided regularity properties, and that for = RN there exists
X
a solution with a boundary that is locally the graph of a C1,1/3 function. Subsequently, it
was shown in [8] that (even for discontinuous densities with bounded variation) the nonlocal
perimeterPer Gamma-convergestoaweightedlocalperimeterand,asaconsequence,solutions
ε
of(1.2)convergetoperimeter-minimalsolutionsof(1.2)withε=0. In[22]itwasshownthatfor
sufficiently small ε>0 adversarially robust classifiers evolve (as parametrized by ε) according
2toageometricflow,whensmoothsolutionsstartingfromthe Bayesclassifierexist. Expansions
usedto derivethis resultshowthat, infinitesimally in ε, this flowis a weightedmeancurvature
flow,whichshowsthatadversarialtrainingisconnectedtodecreasingthelengthofthedecision
boundary.
The main contribution of the present paper is to make this connection with mean cur-
vature flow rigorous in a general setting and to move beyond the short time regime of [22]. To
achieve this, we will introduce a slight modification of the adversarial training problem (1.2).
Intuitively, the proposed iterative scheme prepares for attacks by an adversary with total ad-
versarial budget T >0and(instantaneous)adversarialbudgetε>0,allowingthe adversaryto
corrupt the data on scale ε and even to react to modified classifiers at most T/ε times. As we
willseeinSection 2.1below,theschemecanbeinterpretedasaminimizingmovementsscheme
for mean curvature flow, in the spirit of Almgren–Taylor–Wang[1]. To select unique solutions
we consider a strongly convex Chambolle-type scheme [11] and prove that it is monotone and
consistent with respect to a weightedmean curvature flow, thereby proving convergenceof the
scheme to smooth flows (Theorem 1).
Themainchallengeandthereasonwhyourresultsarenotjuststraightforwardextensions
of existing ones is that the adversarialbudget ε>0 in (1.2) acts both as a time step and as a
non-localityparameterfor the perimeter Per . Hence, inorder to proveconsistencywith mean
ε
curvatureflow,wehavetoperformacarefulanalysisoftheassociatedtotalvariationfunctional
and its subdifferential, showing that the latter is consistent with the 1-Laplace operator for a
suitable class of functions.
Wewouldliketoemphasizethatadversarialtrainingisnotthe onlymethodindatascience
connected to mean curvature flow. In particular, in the field of graph-based learning the so-
called Merriman–Bence–Osher (MBO) algorithm has been employed frequently for clustering
data sets or solving semi-supervised learning problems, see, e.g., [9, 28, 29, 36]. For rigorous
connections of such approaches to mean curvature flow we refer to [24, 25].
Organization of the paper. The rest of the paper is organized as follows. In the next
section we precisely introduce the proposed adversarial training scheme and state our main
result—convergence of the method to weighted mean curvature flow. In Section 3, we deduce
theneededpropertiesforthenonlocaltotalvariationand,inparticular,studyitssubdifferential.
Finally, in Section 4 we prove convergence of the adversarialtraining scheme by verifying that
it is monotone and consistent with respect to weighted mean curvature flow.
Notation. For the reader’s convenience, we collect notation used throughout the paper
here. Typically, Ω RN will be a bounded domain (i.e., a non-empty, open, and connected
set). Weuse N to⊂ denotetheN-dimensionalLebesguemeasureandB(Ω)todenotetheBorel
measurablesuL bsetsofΩ. Furthermore,we use forthe Euclideannormofa vectorin RN and
1 for the N N identity matrix. For a set A|·| RN, we let 1 be the characteristic function
A
taking the va× lue 1 on A and 0 otherwise. For a⊂ ny set Ω RN, we define the inner parallel set
⊂
of distance a>0 through
Ω := x Ω : dist(x,RN Ω)>a . (1.3)
a
{ ∈ \ }
Finally, for x RN and ε>0, we denote open balls by B(x,ε):= y RN : x y <ε .
∈ { ∈ | − | }
2 From adversarial training to mean curvature flow
Let Ω RN be a bounded domain, µ (Ω 0,1 ) be a probability measure, and ℓ(y¯,y)=
⊂ ∈P ×{ }
1 be the 0-1 loss function. We are interested in binary classifiers found via adversarial
y¯=y
6
3training (1.1), i.e., minimizers of the problem
inf E sup ℓ(1 (x˜),y) . (2.1)
(x,y) µ A
A ∈B(Ω) ∼ "x˜ ∈B(x,ε) ∩Ω #
We define the conditional distributions ̺ :=µ( i ) for i=0,1 and ̺:=̺ +̺ . We pose
i 0 1
·×{ }
the following assumption which we shall use in the whole paper, without further reference.
Assumption 1 (The densities). We assume that ̺ and ̺ (and hence also ̺) have densities
0 1
withrespecttotheN-dimensionalLebesguemeasureonΩwhicharecontinuouslydifferentiable
functions, i.e., ̺ C1(Ω). For notational convenience we shall identify ̺ and ̺ with their
i i
∈
densities, meaning fd̺ = f̺ dx. Furthermore, we assume that c < ̺ < c 1 in Ω for
(i) (i) ̺ −̺
some constant c >0.
̺
R R
In this situation it follows fromthe generalresults in [7] that problem(2.1) is equivalentto
inf 1 (x) y dµ(x,y)+εPer (A), (2.2)
A ε
A ∈B(Ω) ZZΩ ×{0,1 }| − |
where the generalized perimeter functional Per is defined as
ε
1
Per (A):= esssup 1 1 (x) d̺ (x)+ 1 (x) essinf 1 d̺ (x) .
ε A A 0 A A 1
ε " ZΩ B(x,ε) ∩Ω − ! ZΩ(cid:18) −B(x,ε) ∩Ω (cid:19) #
Notethat,inparticular,thesupremumin(2.1)canbereplacedbyessentialsupremaandinfima
in(2.2). Furthermore,itwasprovedin[7]thatminimizerstobothproblems(2.1)and(2.2)exist
and that the infimal values coincide. Studying the limit of the problem with small adversarial
budget, the first and third author showed in [8] that the perimeter functional Γ-converges as
ε 0 to a weighted but local perimeter. In the current setting with smooth densities this
→
local perimeter is given by ̺d N 1, where N 1 is the Hausdorff (surface) measure
∂∗A Ω H − H −
and ∂ A is the measure-theoreti∩c reduced boundary of A. Therefore, for small values of ε the
∗
R
problem (2.2) will effectively minimize the energy
1
1 (x) y dµ(x,y)+ ̺d N 1,
A −
ε ZZΩ ×{0,1 }| − | Z∂∗A ∩Ω H
which bears a strong resemblance to the Almgren–Taylor–Wang scheme introduced in [1] for
the study of mean curvature flow, with ε > 0 acting as the time step size. Consequently, the
minimization problem (2.2) should roughly be approximated by a mean curvature flow. As
remarked in the introduction, similar conclusions were drawn in [22] on short time horizons.
The natural initial condition for the mean curvature flow is any solution of the adversarial
training problem (2.2) with ε=0:
inf 1 (x) y dµ(x,y). (2.3)
A
A ∈B(Ω) ZZΩ ×{0,1 }| − |
Solutions are called Bayes classifiers and since we have
1 (x) y dµ(x,y)= 1 d̺ + 1 1 d̺ = 1 d(̺ ̺ )+̺ (Ω),
A A 0 A 1 A 1 0 1
| − | − − −
ZZΩ ×{0,1
}
ZΩ ZΩ ZΩ
problem (2.3) is solved by every set A which is the positive part of a Hahn decomposition of
the signed measure ̺ ̺ . For continuous densities ̺ ,̺ any set A which is sandwiched as
1 0 0 1
−
̺ >̺ A ̺ ̺ is a Bayes classifier.
1 0 1 0
{ }⊂ ⊂{ ≥ }
42.1 The minimizing movements scheme
Now we introduce an iterative adversarial training scheme starting from the Bayes classifier
which is a slight modification of (2.2) and has a rigorous connection to mean curvature flow.
Precisely, we replace (2.2) by a minimizing movements scheme in the spirit of [1, 26]:
A argmin 1 (x) y dµ(x,y),
0 A
 ∈ A ∈B(Ω)ZZΩ ×{0,1 }| − | (2.4)
A
k+1
∈a Ar ∈g Bm (Ωin )ZΩ|1
A
−1
Ak|dist(
·
ε,∂A k)
d̺+Per ε(A), k ≥0,
where in
this
special case we “overload” the distance function and define
dist(,∂A):=dist(,A1)+dist(,Ω A1),
· · · \
with A1 being the points in A with Lebesgue density 1, as the distance to the boundary of A
relative to Ω. The representative set A1 ensures that the distance function does not change
whenAismodifiedbyaLebesguenull-set,andwefurthernotethatthefunctioncoincideswith
the distance to ∂(A1) Ω when Ω is convex.
∩
We notethatthis departsfromthe originaladversarialtrainingproblemderivedin(2.2)by
the inclusion of a distance function. At a technical level, this is essentialto recoverthe correct
surface velocity for the boundary of the regularizedclassifier. Furthermore,one can show as in
[15, Theorem 5.6] that, if A is a smooth set and ε > 0 is small, the scheme (2.4) without the
0
distance function would stagnate, i.e., A = A for all k N. At the level of the application,
k 0
∈
we motivate this term in the following remark.
Remark 2.1 (The distance function). In the context of training a stable classifier the term
dist( ·,∂Ak) acts as an adaptive regularization parameter: For points far away from the decision
ε
boundary ∂A of the previous classifier, the perimeter regularization is unimportant and the
k
firsttermin(2.4)gets moreweight. Closetothe boundary,the oppositeholdstrue. Ifone just
performs two iterations of (2.4), the first A equals a Bayes classifier and the second one A a
0 1
solution to adversarial training, where the class labels are distributed according to the Bayes
classifier and weighted according to their distance to the respective other class. Computing
this distance function in practice can be done with several different methods, for instance
with the fast marching algorithm [34] or the heat flow [16] based on Varadhans formula [37].
In the high-dimensional settings that are characteristic for machine learning problems, such
methods are expensive which is why one resorts to so-called fast minimum norm attacks [31]
which computes an approximationof the radius of the smallestball arounda data point which
contains an adversarial attack. For binary classifiers as in (2.4) this is precisely the distance
function to the decision boundary.
As the solutions of (2.4) are not necessarily unique, we consider a selection procedure fol-
lowingChambolle’sapproachin[11]. To this end, let us introduce the signeddistance function
of a set A relative to Ω as
sdist(,A):=dist(,A1) dist(,Ω A1), (2.5)
· · − · \
whereasbeforeA1 denotesthepointsinAwithLebesguedensity1. Furthermore,weintroduce
the total variationof a measurablefunction u:Ω R which is naturallyassociatedwith Per :
ε
→
1
TV (u):= esssup u u(x) d̺ (x)+ u(x) essinf u d̺ (x) . (2.6)
ε 0 1
ε " ZΩ B(x,ε) ∩Ω − ! ZΩ(cid:18) −B(x,ε) ∩Ω (cid:19) #
5BydefinitionitholdsPer (A)=TV (1 )andfurthermoretheperimeterandthetotalvariation
ε ε A
are connected through a coarea formula, see [7] and Lemma 3.1 below. The central object of
study in this paper is the following modified adversarialtraining scheme
A argmin 1 (x) y dµ(x,y),
0 A

∈ A ∈B(Ω)ZZΩ ×{0,1 }| − |
w
∗
:= war ∈g Lm 2(Ωin )21
ε
ZΩ|w −sdist( ·,Ac k) |2 d̺+TV ε(w), k ≥0, (2.7)
A := w >0 , k 0.
We will
provek+ t1
hat
(2{
.7)∗
cons}
titutes
a≥
selection mechanism for (2.4); that is the sequence
of sets (A k) k N 0 found via (2.7) satisfies (2.4). We note that, in contrast to the work of
∈
Chambolle [11], who in our notation considered the scheme A := w 0 where w :=
k+1 ∗ ∗
{ ≤ }
har ag vem ti on w fl∈ipL2 t( hΩ e) 2 s1 ε ignΩ o|w
f
t− hesd sii gst n( e· d,A dk is) t|2 and cx e+ fuT nV ct( iw on) aa nn ddT piV ckis tht ehe sus pta en rld ea vr ed
l
it no st ta el av dar oi fa st uio bn l, evw ee
l
R
setoftheresultingminimizerw inorderfor(2.7)toselectasolutionof(2.4). Thisisnecessary
∗
since TV sees orientation, in the sense that TV ( u)=TV (u), in contrast to the standard
ε ε ε
− 6
total variation, for which TV( u)=TV(u).
−
The objective of this paper is to show that, as the adversarial budget vanishes, meaning
ε 0,thesequenceofsetsgivenby(2.7)convergetoatime-parametrizedcurvet A(t)which
→ 7→
is a solutionofa weightedmeancurvature flowequationwith the followingnormalvelocity (in
the direction ν )
A(t)
1
V(t)= div ̺ν =H log̺ ν on ∂A. (2.8)
−̺ A(t) A(t) −∇ · A(t)
(cid:0) (cid:1)
Here ν is (a smooth unit-length extension of) the inner unit normal to ∂A(t) and H :=
A(t) A(t)
divν denotes the mean curvature of ∂A(t). Note our orientation is such that H > 0 if
A(t) A
−
A is a ball. The convergence to this mean curvature flow is the content of Theorem 1. The
mathematical challenges arising in this problem are mostly consequences of the nonlocal TV
ε
in(2.7): First,astheTV functionalisneitherlocalnorsmooth,wewillneedtocarefullystudy
ε
its subdifferential and consistency with the 1-Laplace operator, i.e., the subdifferential of the
classicaltotalvariationfunctional. Beyondthis,wehavenotbeenabletoshowthatminimizers
w from (2.7) inherit the regularity of their data, e.g., Lip(w ) Lip(sdist(,Ac)) = 1, an
∗ ∗
≤ ·
extremely convenient property to have at hand. Circumnavigating this obstacle, we instead
provethatminimizersare“almost” Lipschitzbyexplicitlyconstructingsub-andsupersolutions
for conical data. Finally, in (2.7), the parameter ε (appearing in 1 and in Per ) effectively
2ε ε
behaves as the time-step in the discretization of a time interval (0,T) and as a non-locality
parameter. Consequently, the non-locality and time-step are of the same magnitude, and we
mustensurethatthisdoesnotpreventlocalizationoftheminimizingmovementsschemeinthe
limit as ε 0.
→
2.2 Main result
ThemainconsequenceofourresultsisthatiftheinitialBayesclassifierissmoothandcompactly
containedinΩ,thenatimeparametrizedversionofthescheme(A k) k N 0 givenin(2.7)converges
∈
to a solution of mean curvature flow with initial condition A . Precisely, we parametrize the
0
sets (A k) k ∈N 0 in (2.7) with a piecewise-constant curve t 7→A ε(t) defined by
A (t):=A for t [kε,(k+1)ε). (2.9)
ε k
∈
6With this at hand, we may state our result.
Theorem 1 (Main theorem). Let Ω RN be a bounded and convex domain. Suppose that
⊂
in (2.7) the Bayes classifier A Ω has C2-boundary and that t A(t) is a parameterized
0
⊂⊂ 7→
curve evolving by the weighted mean curvature flow with normal velocity (2.8) up to the first
singular time T , with initial condition A .
0
∗
Then as ε 0, the time parametrized curves t A (t) defined in (2.9), coming from the
ε
→ 7→
adversarial training scheme (2.7), converge in L ([0,T );L1(Ω; 0,1 )) and in the Hausdorff
∞loc
∗ { }
distance to the weighted mean curvature flow parametrized by t A(t).
7→
A couple of remarks on this theorem are in order.
Remark 2.2(SmoothBayesclassifiers). NotethatexistenceofBayesclassifierswithC2-bound-
ary is guaranteed, e.g., if the levelset ̺ =̺ is a C2-hypersurface in RN. This follows from
0 1
{ }
the implicit function theorem if ̺ ,̺ are C2-regular in a neighborhood of ̺ = ̺ and if
0 1 0 1
{ }
̺ ̺ =0 on ̺ =̺ .
1 0 0 1
∇ −∇ 6 { }
Remark 2.3(Convexity). ConvexityofthedomainΩisexclusivelyusedinLemma 4.7,acertain
comparisonprinciple for (2.7) when A is a ball. In particular,we believe that the assumption
0
could be avoided with some more work.
Remark 2.4 (Thefirstsingulartime). WealsoremarkthatthefirstsingulartimeinTheorem 1
could, for instance, be due to vanishing bubbles, pinch-off, or intersection with ∂Ω.
Remark 2.5 (Generalizedsolutionsofmeancurvatureflow). Theorem 1isadirectconsequence
ofTheorem 2furtherdownwhichstatesmonotonicity ofthescheme(2.7)andconsistency with
smooth sub- and superflows (see Definition 2 below). In [11, Theorem 4] for the Almgren–
Taylor–Wang scheme, sub- and superflows are used to define generalized flows that start from
moregenericinitialdatasolongasthe viscositysolutionisunique (see also[4,14, 30]formore
generalresultsofthatkind). Akeyelementforthistoworkisthattheschemeselectsasequence
of open (or closed) sets. However, since we do not have a proof for (Lipschitz) continuity of
w in (2.7) (see Corollary 4.10 and the discussion preceding it), the iterates w > 0 of our
∗ ∗
{ }
scheme are in general neither open nor closed. Alternatively, density estimates can be used to
construct open (or closed) selections as in [12], but in our case those are not available because
of the non-locality of TV . As a consequence, it is not clear how to use (2.7) to construct
ε
viscosity solutions of the weighted mean curvature flow.
Remark 2.6 (Boundary conditions). Herein, we do not address boundary conditions, but we
note that—following the numerical experiments in [17]—incorporation of Neumann boundary
conditions for the Almgren–Taylor–Wang scheme has only recently been rigorously addressed
in [18]. Their techniques appear highly PDE dependent, and it is not clear a similar approach
can be used in our nonlocal setting.
3 Properties of the total variation
First, we recall that the total variation admits a coarea formula with respect to the nonlocal
perimeter Per .
ε
Lemma 3.1 (Coarea formula [7]). For every u L1(Ω) it holds that
∈
TV (u)= Per ( u>t )dt, (3.1)
ε ε
R { }
Z
where both sides can take the value + .
∞
7We remark that the above lemma is stated in [7, Proposition 3.13] using the sets u t .
{ ≥ }
However,as noted in [8, Section 4.1] for sufficiently regular densities (in particular,continuous
densities) the statement holds for strict super-level sets, as well.
Next we study some basic properties of the subdifferential of the total variation, regarded
asaconvexfunctionalonL2(Ω)withthe standardinnerproduct. We firstrecordthe following
lemma which will be familiar to readers used to working with 1-homogeneous functionals.
Lemma 3.2. Let be a Banach space with dual pairing , : R, and let J :
∗
X h· ·i X ×X →
[0, ] be a proper functional with J(cu) = cJ(u) for all u domJ and c 0. Then the
X → ∞ ∈ ≥
subdifferential of J at u domJ, defined as
∈
∂J(u):= p : J(u)+ p,v u J(v) for all v , (3.2)
∗
{ ∈X h − i≤ ∈X}
has the characterization
∂J(u)= p ∗ : p,u =J(u), p,v J(v) for all v . (3.3)
{ ∈X h i h i≤ ∈X}
Remark 3.3. Elements p ∂J(u) are called subgradients of J at u.
∈
Proof. The inclusion “ ” in (3.3) is trivial. For the converse inclusion, we let p ∂J(u) and
⊃ ∈
choose v = 2u in (3.2), yielding J(u) + p,u J(2u) = 2J(u) and hence p,u J(u).
h i ≤ h i ≤
Choosingv =0andusingJ(0)=0yieldstheconverseinequalityJ(u) p,u . Hence,itholds
≤h i
p,u = J(u) which immediately also implies p,v J(v) for all v , using again (3.2).
h i h i ≤ ∈ X
This concludes the proof of “ ”.
⊂
It will be important to understand properties of the subdifferential of the total variation
TV ,regardedasanextended-valuedfunctionalonL2(Ω). Accordingto(3.2)itssubdifferential
ε
is given by
∂TV (u)= p L2(Ω) : TV (u)+ p(v u)dx TV (v) v L2(Ω) . (3.4)
ε ε ε
∈ − ≤ ∀ ∈
(cid:26) ZΩ (cid:27)
Using the characterization (3.3) of ∂TV (u) with v 1, we note that for p ∂TV (u) one
ε ε
≡ ± ∈
has pdx = 0. Characterizing the subdifferential in full generality beyond (3.3) is both not
Ω
necessary for our purposes and beyond the scope of this paper, for which it suffices to restrict
R
ourselves to suitably nice functions u and a smaller class of test functions than v L2(Ω).
∈
For this we start with a few informal considerations. Since TV (u) is positively homogeneous,
ε
according to (3.3) it suffices to find p such that pvdx TV (v) for all test functions v with
Ω ≤ ε
equalityforv =utocharacterizethe subdifferential. Ifweassumedthat uwassufficientlynice
R
such that esssup u and essinf were attained at unique points Γ (x) and γ (x),
B(x,ε) Ω B(x,ε) Ω ε ε
respectively, we could∩use a change of varia∩ bles to obtain
vd(Γ ) ̺ esssup vd̺ and vd(γ ) ̺ essinf vd̺
ε ♯ 0 0 ε ♯ 1 1
ZΩ ≤ ZΩB( ·,ε) ∩Ω ZΩ ≥ ZΩB( ·,ε) ∩Ω
with equality for v =u. Consequently and not being concerned about regularity,the function
(Γ ) ̺ ̺ ̺ (γ ) ̺
ε ♯ 0 0 1 ε ♯ 1
p:= − + −
ε ε
would be an element of ∂TV (u). For this to be rigorous, we would have to make sure that
ε
the maps Γ (x) := argmax u and γ (x) := argmin u are well-defined and the
ε B(x,ε) Ω ε B(x,ε) Ω
pushforwards(Γ ) ̺ and(γ ) ̺ ∩ havedensitiesinL2(Ω). Toward∩ sthisgoal,wefirstofallwork
ε ♯ 0 ε ♯ 1
with sufficiently regular functions u with non-vanishing gradients, and also with a restricted
class of test functions v for which we can prove the subdifferential inequality in (3.4) holds.
8Proposition 3.4. Let u C2(Ω) such that u c in Ω for a constant c > 0, and let Λ
max
∈ |∇ | ≥
denote the largest eigenvalue of the Hessian of u over Ω. If 0 < ε < c/Λ is small enough,
max
then
• for every x Ω the maps
ε
∈
Γ (x):=argmaxu and γ (x):= argminu
ε ε
B(x,ε) Ω B(x,ε) Ω
∩ ∩
are singletons;
• for every y Ω the densities of the pushforward measures (Γ ) ̺ and (γ ) ̺ with
2ε ε ♯ 0 ε ♯ 1
∈
respect to the Lebesgue measure N are given by
L
d(Γ ) ̺ d̺ u(y) u(y)
ε ♯ 0 0
(y)= y ε ∇ det y ε ∇ , (3.5a)
d N d N − u(y) ∇ − u(y)
L L (cid:18) |∇ |(cid:19)(cid:12) (cid:18) (cid:18) |∇ |(cid:19)(cid:19)(cid:12)
d(γ ) ̺ (y) d̺ u(y) (cid:12) u(y) (cid:12)
ε ♯ 1 1 (cid:12) (cid:12)
(y)= y+ε ∇ (cid:12)det y+ε ∇ (cid:12), (3.5b)
d N d N u(y) ∇ u(y)
L L (cid:18) |∇ |(cid:19)(cid:12) (cid:18) (cid:18) |∇ |(cid:19)(cid:19)(cid:12)
(cid:12) (cid:12)
where by Assumption 1 it holds dd̺ Ni =̺ i for i ∈(cid:12) (cid:12) {0,1 }. (cid:12) (cid:12)
L
• the function p L2(Ω ), defined by
2ε
∈
d (Γ ) ̺ ̺ ̺ (γ ) ̺
ε ♯ 0 0 1 ε ♯ 1
p:= − + − , (3.6)
d N ε ε
L (cid:20) (cid:21)
satisfies the inequality
TV (u)+ pϕdx TV (u+ϕ) (3.7)
ε ε
≤
ZΩ
for all ϕ L2(Ω) with ϕ=0 almost everywhere in Ω Ω .
2ε
∈ \
Proof. We willderivethe firsttwostatementsonly forΓ ; the onesforγ followfromreplacing
ε ε
u by u.
−
Step 1 (Optimality condition). First, we note that for any x Ω there exists a point
ε
∈
y argmax u since u is continuous. Second, by the Karush–Kuhn–Tuckeroptimality
∗ ∈ B(x,ε) Ω
∩
conditions (or direct verification) we get that y satisfies
∗
u(y∗) λ∗(y∗ x)=0, y∗ x ε, (3.8)
∇ − − | − |≤
foraLagrangemultiplierλ 0whichissuchthatλ (y x2 ε2)=0. Sincebyassumption
∗ ∗ ∗
≥ | − | −
u c>0onΩ,themaximumhastobe takenontheboundaryofB(x,ε), i.e., y x =ε.
∗
T|∇ he| re≥
fore, we obtain from (3.8) that the Lagrange multiplier is given by λ ∗
=|
|∇
y−
u ∗(y
x∗|
) | =
|∇u(y∗)
|.
| − |
ε
Step 2 (Unique maximum). Next we prove that the maximizer y is uniquely determined.
∗
For this, we define the Lagrangian
λ
L(y,λ):= u(y)+ y x2 ε2 for λ [0, )
− 2 | − | − ∈ ∞
(cid:16) (cid:17)
and observe that it holds
u(y ) c
∇2 yL(y∗,λ∗)= −∇2u(y∗)+λ∗1= −∇2u(y∗)+ |∇
ε
∗ |1
(cid:23)
−Λ max+
ε
1 ≻0
(cid:16) (cid:17)
9by our assumption on ε. We let M := c Λ and, supposing that y˜ ∂B(x,ε) is another
ε ε − max ∈
maximizer, we get using Taylor expansions and applying (3.8) that
u(y˜)=L(y˜,λ∗)
−
1
=L(y ,λ )+ L(y ,λ )(y˜ y )+ (y˜ y )T 2L(y ,λ )(y˜ y )+o y˜ y 2
∗ ∗ ∇y ∗ ∗ − ∗ 2 − ∗ ∇y ∗ ∗ − ∗ | − ∗ |
M (cid:16) (cid:17)
u(y∗)+ ε y˜ y∗ 2 ω(y˜ y∗ ) y˜ y∗ 2
≥− 2 | − | − | − | | − |
where ω is the modulus of continuity of 2L(y,λ ) in y, which is the same as the modulus of
∇y ∗
continuity of 2u (and thereby independent of ε). Using that u(y˜)=u(y ), we find
∗
∇
M
ε y˜ y 2 ω(y˜ y ) y˜ y 2. (3.9)
∗ ∗ ∗
2 | − | ≤ | − | | − |
We note that M is positive for ε small enough and even lim M = . Therefore, for
ε ε 0 ε
→ ∞
ε > 0 small enough, (3.9) becomes a contradiction unless y˜= y . Hence, we have shown that
∗
Γ (x)= y is a singleton.
ε ∗
{ }
Step 3 (Computation of the pushforward). Vice versa, solving(3.8) for x, we see that Γ is
ε
one-to-one with inverse
u(y )
Γ −ε1(y ∗)=y ∗ −ε ∇ u(y∗
)
, (3.10)
|∇ ∗ |
and in particular, this is a well-defined injective C1 function on Γ(Ω ). Therefore we can use
ε
the definition of the pushforward and the area formula to show for any continuous function
ϕ C(Ω) that
∈
ϕd(Γ ) ̺ = ϕ Γ d̺
ε ♯ 0 ε 0
◦
ZΓε(Ωε) ZΩε
= ϕ(y) det( Γ 1(y)) ̺ (Γ 1(y))dy
∇
−ε 0 −ε
ZΓε(Ωε)
(cid:12) (cid:12)
(cid:12) (cid:12) u(y) u(y)
= ϕ(y) det y ε ∇ ̺ y ε ∇ dy.
0
∇ − u(y) − u(y)
ZΓε(Ωε) (cid:12) (cid:18) (cid:18) |∇ |(cid:19)(cid:19)(cid:12) (cid:18) |∇ |(cid:19)
(cid:12) (cid:12)
(cid:12) (cid:12)
Restricting ourselves to arbitrary con(cid:12)tinuous functions ϕ with (cid:12)suppϕ Ω
2ε
Γ ε(Ω ε), we
⊂ ⊂
obtain the claimed identity (3.6) for the pushforwards. Note that Ω Γ (Ω ) follows from
2ε ε ε
⊂
(3.10).
Step 4 (Local subgradient). To obtain the last claim (3.7), let be the set of points,
N
which are not Lebesgue points for u+ϕ and hence N( ) = 0. In particular, also the sets
L N
:=Γ (Ω ) and :=γ (Ω ) have zero Lebesgue measure. Since Γ 1 and γ 1 are
N1 ε ε
∩N
N2 ε ε
∩N
−ε ε−
diffeomorphisms from Γ (Ω ) and γ (Ω ), respectively, to Ω , we have
ε ε ε ε ε
N( x Ω : Γ (x) or γ (x) )= N( x Ω : Γ (x) or γ (x) )
ε ε ε ε ε 1 ε 2
L { ∈ ∈N ∈N} L { ∈ ∈N ∈N }
N(Γ 1( ) γ 1( ))=0,
⊂L
−ε N1
∪
ε− N2
hence, for almost every x Ω the points Γ (x) and γ (x) are Lebesgue points of u + ϕ.
ε ε ε
∈
Necessarily, it follows that for such x Ω it holds
ε
∈
(u+ϕ) Γ (x) esssup(u+ϕ) and essinf(u+ϕ) (u+ϕ) γ (x).
ε ε
◦ ≤ B(x,ε) B(x,ε) ≤ ◦
10Hence, we obtain for any ϕ L2(Ω) with ϕ=0 almost everywhere on Ω Ω that
2ε
∈ \
ε pϕdx= ϕd(Γ ) ̺ ϕd̺ + ϕd̺ ϕd(γ ) ̺
ε ♯ 0 0 1 ε ♯ 1
− −
ZΩ ZΓε(Ωε) ZΩ ZΩ Zγε(Ωε)
= ϕ Γ d̺ ϕd̺ + ϕd̺ ϕ γ d̺
ε 0 0 1 ε 1
◦ − − ◦
ZΩε ZΩ ZΩ ZΩε
= (u+ϕ) Γ u Γ d̺ + u γ (u+ϕ) γ d̺
ε ε 0 ε ε 1
◦ − ◦ ◦ − ◦
ZΩε
(cid:16) (cid:17)
ZΩε
(cid:16) (cid:17)
ϕd̺ + ϕd̺
0 1
−
ZΩ ZΩ
esssup(u+ϕ) esssup u d̺ essinf u essinf (u+ϕ) d̺
0 1
≤ ZΩ B( ·,ε) ∩Ω − B( ·,ε) ∩Ω ! − ZΩ(cid:18)B( ·,ε) ∩Ω −B( ·,ε) ∩Ω (cid:19)
ϕd̺ + ϕd̺
0 1
−
ZΩ ZΩ
esssup(u+ϕ) esssup u d̺ + essinf u essinf (u+ϕ) d̺ .
0 1
− ZΩ \Ωε B( ·,ε) ∩Ω − B( ·,ε) ∩Ω ! ZΩ \Ωε(cid:18)B( ·,ε) ∩Ω −B( ·,ε) ∩Ω (cid:19)
The last two integrals vanish because ϕ=0 in Ω Ω . Hence
2ε
\
ε pϕdx= esssup(u+ϕ) (u+ϕ) d̺ + u+ϕ essinf (u+ϕ) d̺
0 1
ZΩ ZΩ B( ·,ε) ∩Ω − ! ZΩ(cid:18) −B( ·,ε) ∩Ω (cid:19)
esssup u u d̺ u essinf u d̺
0 1
− ZΩ B( ·,ε) ∩Ω − ! − ZΩ(cid:18) −B( ·,ε) ∩Ω (cid:19)
=εTV (u+ϕ) εTV (u).
ε ε
−
This shows (3.7) and concludes the proof.
Next we prove that the subgradient identified in the previous lemma is consistent with a
weighted 1-Laplacian operator which, neglecting boundary conditions, is the subgradient of a
local weighted total variation.
Proposition 3.5. Under the conditions of Proposition 3.4 it holds that
d (Γ ) ̺ ̺ ̺ (γ ) ̺ u
ε ♯ 0 0 1 ε ♯ 1
− + − = div ̺ ∇ +o (1) uniformly in Ω . (3.11)
d N ε ε − u ε →0 2ε
L (cid:20) (cid:21) (cid:18) |∇ |(cid:19)
Proof. We start by investigating the density of (Γ ) ̺ given in Proposition 3.4. Let us fix
ε ♯ 0
y Ω . Using a Taylor expansionof ̺ C1(Ω) and utilizing u(y) c, we have uniformly
2ε 0
∈ ∈ |∇ |≥
in Ω that
2ε
u(y) u(y)
̺ y ε ∇ =̺ (y) ε ̺ (y) ∇ +o(ε).
0 0 0
− u(y) − ∇ · u(y)
(cid:18) |∇ |(cid:19) |∇ |
Furthermore, using a Taylor expansion of the determinant and utilizing u C2(Ω) with
∈
u c, we get
|∇ |≥
u(y) u(y)
det y ε ∇ =1 εdiv ∇ +O(ε2).
∇ − u(y) − u(y)
(cid:18) (cid:18) |∇ |(cid:19)(cid:19) (cid:18)|∇ |(cid:19)
11In particular, we see that the determinant is non-negative if ε>0 is sufficiently small. Multi-
plying the two expressions and using the product rule yields
u(y) u(y)
̺ y ε ∇ det y ε ∇
0
− u(y) ∇ − u(y)
(cid:18) |∇ |(cid:19)(cid:12) (cid:18) (cid:18) |∇ |(cid:19)(cid:19)(cid:12)
(cid:12) u(y) (cid:12) u(y)
= ̺ 0(y) ε ̺ 0(y) (cid:12) (cid:12)∇ +o(ε) 1 εdiv(cid:12) (cid:12) ∇ +O(ε2)
− ∇ · u(y) − u(y)
(cid:18) |∇ | (cid:19)(cid:18) (cid:18)|∇ |(cid:19) (cid:19)
u(y) u(y)
=̺ (y) ε ̺ (y) ∇ ε̺ (y)div ∇ +o(ε)
0 0 0
− ∇ · u(y) − u(y)
|∇ | (cid:18)|∇ |(cid:19)
u(y)
=̺ (y) εdiv ̺ (y) ∇ +o(ε).
0 0
− u(y)
(cid:18) |∇ |(cid:19)
Using Proposition 3.4 we therefore obtain
d (Γ ) ̺ ̺ u(y)
ε ♯ 0 0
− = div ̺ (y) ∇ +o (1).
d N ε − 0 u(y) ε →0
L (cid:20) (cid:21) (cid:18) |∇ |(cid:19)
Repeatingthesameargumentsfor(γ ) ̺ andusing̺=̺ +̺ leadstothefinalconclusion.
ε ♯ 1 0 1
Next, we provethatthe totalvariationis a lowersemicontinuousfunctionalwith respectto
the weak L2 topology. In [7], it was already proved that it is weak- lower semicontinuous in
∗
L ,butthisisinsufficientforourpurposes,becausewewilluseadditivityofthesubdifferential
∞
(see the proof of Proposition 4.2).
Lemma 3.6. Let (u k) k N L2(Ω) be a sequence which converges weakly to u L2(Ω). Then
∈ ⊂ ∈
it holds that
TV (u) liminfTV (u ).
ε ε k
≤ k
→∞
Proof. It suffices to show that
1
TV0(u):= esssup u u(x) d̺ (x)
ε ε ZΩ B(x,ε) ∩Ω − ! 0
is weakly lower semi-continuous. As TV0 is convex, it suffices to prove lower semi-continuity
ε
with respect to strongly convergingsequences. In particular,we assume u u in L2(Ω), and
k
→
may further suppose that u (x) u(x) for almost every x Ω.
k
→ ∈
Let δ > 0. The strong convergence of u to u in particular implies for all y Ω and
k
∈
0<r <δ:
1 1
udz = lim u dz.
LN(B(y,r) ∩Ω) ZB(y,r) ∩Ω k →∞LN(B(y,r) ∩Ω) ZB(y,r) ∩Ω k
Taking Lebesgue points of u, for almost every y Ω we get
∈
1 1
u(y)= lim udz = lim lim u dz
r →0 LN(B(y,r) ∩Ω) ZB(y,r) ∩Ω r →0k →∞LN(B(y,r) ∩Ω) ZB(y,r) ∩Ω k
liminf esssup u .
k
≤ k →∞ B(y,δ) ∩Ω
12As δ >0 is arbitrary, we conclude that for almost every x Ω
∈
esssup u liminf esssup u .
k
B(x,ε) ∩Ω ≤ k →∞ B(x,ε) ∩Ω
As we have the pointwise convergence of u , we improve this to
k
esssup u u(x) liminf esssup u u (x) ,
k k
B(x,ε) ∩Ω − ≤ k →∞ B(x,ε) ∩Ω − !
from which Fatou’s lemma concludes the claimed lower semi-continuity.
Next we establish an important submodularity property of the total variation TV . In fact
ε
it follows from submodularity of the perimeter Per (proved in [7, Proposition 3.3]) and the
ε
coarea formula (see [13, Proposition 3.2]) but for self-containedness we elaborate on the proof.
Lemma 3.7. It holds for all u,v L2(Ω) that
∈
TV (u v)+TV (u v) TV (u)+TV (v).
ε ε ε ε
∨ ∧ ≤
Proof. The statement follows from the directly obtained inequalities
esssup(u v) esssup u esssup v ,
∨ ≤ ∨
B(,ε) Ω B(,ε) Ω B(,ε) Ω
· ∩ (cid:16) · ∩ (cid:17) (cid:16) · ∩ (cid:17)
esssup(u v) esssup u esssup v ,
∧ ≤ ∧
B(,ε) Ω B(,ε) Ω B(,ε) Ω
· ∩ (cid:16) · ∩ (cid:17) (cid:16) · ∩ (cid:17)
thereverseanaloguesfortheessinf,andtheelementaryidentitya b+a b=a+bfornumbers
a,b R. ∨ ∧
∈
4 Convergence of the adversarial training scheme
ForasetA B(Ω),letusdefinetheone-stepoperator S (A)oftheadversarialtrainingscheme
ε
∈
(2.7) via
1
S (A):= w >0 where w :=argmin u sdist(,Ac)2 d̺+TV (u). (4.1)
ε
{
ε∗
}
ε∗
u ∈L2(Ω) 2ε ZΩ| − · |
ε
Forconvenience,weassumethatw isaLebesguerepresentativewith w >0 =( w >0 )1,
ε∗
{
ε∗
} {
ε∗
}
where we recall the notation introduced in (2.5). In particular, with this representative con-
vention in place, one can verify by hand that
sdist( ·, {w ε∗ >0 }c)=dist( ·, {w ε∗ >0 }c) −dist( ·, {w ε∗ >0 }),
circumventingthe need for a well-chosenrepresentativein sdist (see (2.5)) for another applica-
tion of S .
ε
InProposition 4.2below,wewillfirstprovethattheoperatorS doesinfactselectasolution
ε
of(2.4);inotherwords,theconvexminimizationproblemarisingin(2.7)isconsistentwiththe
scheme (2.4). Second, to prove Theorem 1, we will connect the operator (4.1) to the limiting
equation by showing that the operator is monotone and consistent with respect to weighted
mean curvature flow in the following sense.
13Definition 1 (Monotonicity). The operator S defined in (4.1) is monotone if A A Ω
ε ′
⊂ ⊂
implies S (A) S (A).
ε ′ ε
⊂
While monotonicity is a property of the operator by itself, consistency, on the other hand,
directly connects the scheme to mean curvature flow.
Forconsistency,werelyonthenotionsofsub-andsuperflowstypicallyusedtoconstructbar-
riersolutionsformeancurvatureflow,seeforexample[3,Chapter9]. If[t ,t ] t A(t) Ω
0 1
∋ 7→ ⊂⊂
isasmoothcurveofsmoothsetswhichevolvewithnormalspeedV(t)= 1div ̺ν ,where
−̺ A(t)
ν istheinnernormalvectortotheboundaryofA(t),i.e.,asin(2.8),thenthesigneddistance
A(t) (cid:0) (cid:1)
function d(x,t):=sdist(x,Ac(t)) satisfies
1
∂ d(x,t)= div(̺(x) d(x,t)) (4.2)
t
̺(x) ∇
for any (x,t) with d(x,t) = 0. This is because ν = d(x,t). The PDE (4.2) forms the
A(t)
∇
basisofoursub- andsuperflowdefinitions forweightedmeancurvatureflow,adaptedfrom[14,
Definition 2.1]. Informally, a superflow is a smooth evolution of sets that moves strictly faster
thanmeancurvatureflow,whileasubflowmovesslower. Weemphasizethatourmeaningisthe
same as in other works,but the inequalities are reversedas the gradientof the signed distance
function sdist(x,Ac) points into the evolving set (more consistentwith BV-solutionconcepts).
Definition 2 (Sub- andsuperflows). LetA(t) Ω, t [t ,t ]. We saythat A(t)is a subflow
0 1
⊂⊂ ∈
of (4.2) if
• there exists a relatively open set B Ω [t ,t ] with ∂A(t) t B;
⊂ × 0 1 t0 t t1 ×{ }⊂
≤ ≤
• the function d(x,t) := sdist(x,Ac(t)) is continuouslySdifferentiable in time and twice
continuously differentiable in space in B, which we abbreviate as d C2,1(B);
∈ x,t
• there exists δ >0 such that
1
∂ d(x,t) div(̺(x) d(x,t))+δ (4.3)
t
≥ ̺(x) ∇
for any (x,t) B.
∈
We say that A(t) is a superflow whenever δ <0 and the reverse inequality holds in (4.3).
Definition 3 (Consistency). The operator S defined in (4.1) is consistent if
ε
• for every subflow [t ,t ] t A(t) in the sense of Definition 2 there exists ε > 0 such
0 1 0
∋ 7→
that S (A(t)) A(t+ε) for all 0<ε<ε and all t [t ,t ε];
ε 0 0 1
⊂ ∈ −
• for every superflow the same holds with the converse inclusion.
The interpretation of this definition is that for ε > 0 sufficiently small the scheme S (A)
ε
defined in (4.1) moves faster than a subflow and slower than a superflow starting at A. With
these definitions in hand, we may state the principle result of this paper.
Theorem 2 (Monotonicity and consistency). If Ω RN is a bounded and convex domain the
⊂
operatorS ismonotoneandconsistentwiththeweightedmeancurvatureflow(2.8)inthesense
ε
of Definitions 1 and 3, respectively.
Theorem 2willfollowdirectlyfollowfromPropositions 4.4and4.11below. Webrieflydefer
the proofs of the aforementioned selection principle and Theorem 2, as we can now directly
conclude Theorem 1.
14Proof of Theorem 1. Let A and t A(t) be as in the hypothesis of the theorem and recall
0
7→
t A (t) defined in (2.9). Then let t A (t) be any subflow with initial condition A
ε sub 0
7→ 7→ ⊂
A (0) and parameter δ >0. First, we will show that for any time t before the singular time
sub
of A , we have that
sub
A (t) A (ε t/ε ). (4.4)
ε sub
⊂ ⌊ ⌋
for all ε sufficiently small. Similarly, the converse set containment holds for a superflow A
sup
leading to
A (ε t/ε ) A (t) A (ε t/ε ) for all ε>0 sufficiently small. (4.5)
sup ε sub
⌊ ⌋ ⊂ ⊂ ⌊ ⌋
It turns out (4.4) is a simple consequence of Theorem 2. Let (A k) k N 0 be the sets in the
∈
definition of A in (2.9) coming from iteratively applying the scheme. By monotonicity and
ε
consistency we have
A =S (A ) S (A (0)) A (ε).
1 ε 0 ε sub sub
⊂ ⊂
Applying S to both sides once again, using monotonicity and consistency, we have
ε
A =S (A ) S (A (ε)) A (2ε).
2 ε 1 ε sub sub
⊂ ⊂
Iterating and recalling the definition of A in (2.9), we conclude (4.4) and hence also (4.5).
ε
Consequently, using (4.5) and letting T be the earliest singular time of A and A , we
sub sup
estimate for any s<T that
limsup sup N(A (t) A(t)) sup N(A (t) A(t))+ N(A (t) A(t)) , (4.6)
ε sub sup
L △ ≤ L △ L △
ε 0 t [0,s] t [0,s]
→ ∈ ∈ (cid:0) (cid:1)
where we have used continuity of the sub- and superflows to replace ε t/ε with t. Simi-
⌊ ⌋
larly, one can get an estimate for the Hausdorff distance d (A,B) := sup dist(x,B)
sup dist(x,A). Using (4.5) again we have
H x ∈A ∨
x B
∈
limsup sup d (A (t),A(t)) sup sup d(x,A(t)) sup dist(x,A (t))
H ε sup
≤ ∨
ε →0 t ∈[0,s] t ∈[0,s]y ∈Asub(t) x ∈A(t)
sup d (A (t),A(t))+d (A (t),A(t)). (4.7)
H sub H sup
≤
t [0,s]
∈
Itremainsto arguethat the righthandside of(4.6)and(4.7)canbe made arbitrarilysmallby
approximating t A(t) by sub- and superflows. Briefly, let d(x,t) = sdist(x,Ac (t)). Note
7→ sub
that if ∂ d= 1div(̺ d)+δ on ∂A (t), then as a curvature flow this may be written as
t ̺ ∇ sub
1
V (t)= div ̺ν δ =H log̺ ν δ on ∂A (t), (4.8)
sub −̺ Asub(t) − Asub(t) −∇ · Asub(t) − sub
(cid:0) (cid:1)
following the conventionof (2.8). As (4.8) is a perturbation of (2.8), one can show that if (2.8)
has a strong solutions up to time T , then for any T < T , there is δ > 0 sufficiently small
∗ T
∗
such that the flow (4.8) has a strong solution up to time T for all δ <δ . With this in mind,
T
| |
we see that the right-hand side of (4.6) and (4.7) can be made arbitrarily small by choosing
sub- and superflows satisfying the inequalities of Definition 2 with equality on the interface
and then sending δ 0 (in the definition of sub-/superflow one must replace δ by δ/2 to get
→
the neighborhood B); at the same time this will allow one to take T T , concluding the
→ ∗
theorem.
154.1 Well-posedness
We first show that the optimization problem in (4.1) has a unique solution, up to equality on
Lebesgue null-sets. This follows from the following more general statement.
Proposition 4.1. For any f L2(Ω) there exists a unique element w L2(Ω) such that
∈ ∈
1 1
w f 2 d̺+TV (w)= inf u f 2 d̺+TV (u).
ε ε
2ε ZΩ| − | u ∈L2(Ω)2ε ZΩ| − |
Furthermore, if f L (Ω) then w L (Ω) with w f .
∈
∞
∈
∞
k
kL∞(Ω)
≤k
kL∞(Ω)
Proof. We define the functional E :L2(Ω) [0, ] via
→ ∞
1
E(u):= u f 2 d̺+TV (u).
ε
2ε | − |
ZΩ
Thanks to Lemma 3.2 the functional u TV (u) is convex and hence E is strictly convex.
ε
7→
Thisimpliesuniqueness. Existenceoftheminimizerwisaconsequenceoflowersemi-continuity
ofthe functional(Lemma 3.6)andthedirectmethod. Forthe claimedL -boundwenote that
∞
we can replace w by the truncation wˆ := ( C) w C with C := f which satisfies
− ∨ ∧ k
kL∞(Ω)
E(wˆ) E(w) (as may be directly checked using Lemma 3.7) and therefore by uniqueness it
≤
holds that wˆ =w, and the bounds for w follow.
4.2 Selection property
Next we show that S selects a solution of (2.4). Our proof (lightly) deviates from that of [11,
ε
Proposition 2.2] for the standard TV functional due to the asymmetry of TV with respect to
ε
super- and sublevel sets. In particular, we have Per (A)=Per (Ac).
ε ε
6
Proposition 4.2 (Selection principle). Letting S be defined as in (4.1), it holds that
ε
dist(,∂A)
S (A) argmin 1 1 · d̺+Per (A).
ε E A ε
∈
E
∈B(Ω)ZΩ| − | ε
Proof. WemayofcourseassumeA= andA=Ω. Weusetheabbreviationd(x):=sdist(x,Ac)
6 ∅ 6
and recall that d(x) = dist(x,∂A) following the notation introduced after (2.4). As the L2-
| |
fidelity term and the TV functional in the minimization problem (4.1) are both convex and
ε
lower semi-continuous, we have that
w d
0
∈
ε∗
ε− ̺+∂TV ε(w ε∗).
We define p:= −wε∗ ε−d̺ ∈∂TV ε(w ε∗) and E
s
:= {w
ε∗
>s }.
Step 1 (Subdifferential of the super-level sets). We claim that for almost every s
| | ≤
diam(Ω)=:C,
p ∂TV (1 )=:∂Per (E ).
∈
ε Es ε s
Wefirstnotethatbytheε-coareaformulainLemma 3.1andthat kw ε∗ kL∞ ≤CbyProposition 4.1,
we have
C
TV (w )= Per (E )ds.
ε ε∗ ε s
Z−C
16Similarly, by the layer cake formula and Fubini’s theorem
C C
ZΩpw ε∗dx= ZΩp(x) Z−C1 Es(x)ds −C
!
dx= Z−CZΩp(x)1 Es(x)dxds,
where in the last equality we recall that pdx = 0 as noted immediately after (3.4). By
Ω
Lemma 3.2, we have pw dx=TV (w ), so that we may combine the abovedisplaysto find
Ω ε∗ ε ε∗ R
that
R C C
Per (E )ds= p(x)1 (x)dxds. (4.9)
ε s Es
Z−C Z−CZΩ
Once again by the characterizationof the subdifferential in Lemma 3.2, we have p1 dx
Ω Es ≤
Per (E ), so that (4.9) implies
ε s
R
Per (E )= p1 dx (4.10)
ε s Es
ZΩ
foralmosteverys with s C. Applying the characterization(3.3)ofthe subdifferentialgives
| |≤
the claim.
Step 2 (Subdifferential for w > 0 ). The claim of Step 1 can be improved to every
{
ε∗
}
s [ C,C): Fixing such an s and taking a sequence s s such that (4.10) holds for each
k
s ∈ , w− e have that 1 1 pointwise and thereby in L1↓ . We pass to the limit in (4.10) as
k Esk
→
Es
k using lower semi-continuity of Per , the L1 convergence, and that p ∂TV (w ) (for
→ ∞
ε
∈
ε ε∗
the last inequality below) to find
Per (E ) p1 dx Per (E ).
ε s
≤
Es
≤
ε s
ZΩ
Thusp ∂Per (E )foranys [ C,C), andinparticularp ∂Per (E )=∂Per ( w >0 ).
∈
ε s
∈ − ∈
ε 0 ε
{
ε∗
}
Step 3 (Conclusion). We apply the definition of subdifferential at E for any set E to find
0
that
Per (E )+ p(1 1 ) dx Per (E). (4.11)
ε 0 E
−
E0
≤
ε
ZΩ
Noting by definition of p= d −wε∗ ̺ that
ε
1 1 1
p(1 1 ) dx= d(1 1 ) d̺ w (1 1 )d̺ d(1 1 ) d̺,
E
−
E0
ε
E
−
E0
− ε
ε∗ E
−
E0
≥ ε
E
−
E0
ZΩ ZΩ ZΩ ZΩ
where we used that w (1 1 ) 0 almost everywhere, we can rearrange (4.11) as
ε∗ E
−
E0
≤
1 1
Per (E )+ ( d)1 d̺ Per (E)+ ( d)1 d̺.
ε 0
ε −
E0
≤
ε
ε −
E
ZΩ ZΩ
However,one can verify that Ω|1
E
−1
A
|dist( ε·,∂A)d̺= 1
ε
Ω( −d)1 Ed̺+ 1
ε
Ωd1 Ad̺, so that
the previous display is equivalently written
R R R
dist(,∂A) dist(,∂A)
Per (E )+ 1 1 · d̺ Per (E)+ 1 1 · d̺,
ε 0
|
E0
−
A
| ε ≤
ε
|
E
−
A
| ε
ZΩ ZΩ
concluding the proposition.
174.3 Monotonicity
For proving monotonicity we start with a simple comparison principle for solutions of the
optimization problem in (4.1).
Proposition 4.3 (Comparison principle I). For d,d L (Ω) with d d almost everywhere
′ ∞ ′
∈ ≤
in Ω assume that w,w satisfy
′
1 2
w() =argmin u d() d̺+TV (u).
′ ′ ε
u ∈L2(Ω) 2ε ZΩ
(cid:12)
−
(cid:12)
(cid:12) (cid:12)
Then it holds w w almost everywhere in Ω.(cid:12) (cid:12)
′
≤
Proof. Using optimality of w and w we have
′
1
w d2+ w′ d′ 2 d̺+TV ε(w)+TV ε(w′)
2ε | − | | − |
ZΩ (cid:16) (cid:17) (4.12)
1
2 2
w w′ d + w w′ d′ d̺+TV ε(w w′)+TV ε(w w′).
≤ 2ε | ∨ − | | ∧ − | ∨ ∧
ZΩ
(cid:16) (cid:17)
Using Lemma 3.7 to cancel the total variations we obtain from the above that
w d2+ w d 2 d̺ w w d2 + w w d 2 d̺.
′ ′ ′ ′ ′
| − | | − | ≤ | ∨ − | | ∧ − |
ZΩ
(cid:16) (cid:17)
ZΩ
(cid:16) (cid:17)
Expanding squares, canceling terms, and reordering this inequality, we reduce to
0 ((w (w w′))d+(w′ (w w′))d′) d̺
≤ − ∨ − ∧
ZΩ
= (w′ w)(d′ d) d̺.
ZΩ ∩{w′>w
}
− −
Using that ̺>c we infer thatΩ w >w d <d has zeroLebesgue measure. As in [11,
̺ ′ ′
∩{ }∩{ }
Lemma 2.1] one can argue that in fact w w holds almost everywhere.
′
≤
Withthiscomparisonprincipleathand,theproofofmonotonicityforTheorem 2isstraight-
forward.
Proposition 4.4 (Monotonicity). The operator S defined in (4.1) is monotone in the sense
ε
of Definition 1.
Proof. Let w and w denote the solutions of the problem in (4.1) for A and A, respectively.
′ ′
Since A A we have sdist(,(A)c) sdist(,Ac), and by Proposition 4.3, it follows that
′ ′
⊂ · ≤ ·
w w outside of a Lebesgue null-set . This immediately implies
′
≤ N
S (A)= w >0 =( w >0 )1 ( w>0 )1 = w>0 =S (A),
ε ′ ′ ′ ε
{ } { }\N ⊂ { }\N { }
where we have used that w() > 0 = ( w() > 0 )1 by choice of representative in (4.1) and
′ ′
{ } { }
( w() >0 )1 =( w() >0 )1 holds for any Lebesgue null-set .
′ ′
{ }\N { } N
184.4 Consistency
ThissubsectionisdevotedtotheproofofconsistencyforTheorem 2. Consistencyconnectsthe
numericalscheme directly to meancurvature flow,and assuch, will requirea delicate analysis.
Our approachis motivated by Chambolle and Novaga’sin [14] for an anisotropic but local TV
functional.
We briefly summarize the strategy of the proof. To show that the evolving set of a subflow
stays outside the adversarialscheme,we show that the subflow for mean curvature flow can be
modified to construct a subsolution for a static TV problem. To comparethe modified subso-
ε
lutiontow foundusing(4.1),wewillapplythe variationalcomparisonprincipleprovenbelow
ε∗
inProposition 4.6belowonatubular neighborhoodofthe interface. Forthis to work,wemust
know that the modified subsolution is greater than w on the boundary of the tubular neigh-
ε∗
borhood. This information comes from Lemma 4.7 below. In fact, this lemma can be used to
showthatupto anerrorO(√ε), the minimizerw isLipschitzcontinuous(seeCorollary 4.10),
ε∗
and related estimates will allow us to recover the boundary conditions.
We first note that global minimizers give rise to local minimizers, so long as the boundary
conditions are frozen on an ε-neighborhood.
Lemma 4.5 (Restricted minimizer). Let d L (Ω), and let w L (Ω) solve
∞ ∞
∈ ∈
1
w=argmin u d2 d̺+TV (u) : u L2(Ω) .
ε
2ε | − | ∈
(cid:26) ZΩ (cid:27)
Let Ω Ω be an open subset and recall the notation in (1.3). Then it also holds that
′
⊂
1
w =argmin
(cid:26)2ε
ZΩ′|u −d |2 d̺+TV ε(u;Ω′) : u ∈L2(Ω′), u=w in Ω′ \Ω′ε (cid:27),
where TV (u;Ω) is the total variation as defined in (2.6) with Ω replaced by Ω.
ε ′ ′
Proof. Let u L2(Ω) be a function such that u=w on Ω Ω . We extend u to a function in
∈
′ ′
\
′ε
L2(Ω) by setting u:=w on Ω Ω. Hence, using also the minimization property of w it holds
′
\
1 1
w d2 d̺+TV (w;Ω) u d2 d̺+TV (u;Ω)
ε ′ ε ′
2ε ZΩ′| − | − (cid:18)2ε ZΩ′| − |
(cid:19)
1 1
= w d2 d̺+TV (w;Ω) u d2 d̺+TV (u;Ω)
ε ′ ε ′
2ε | − | − 2ε | − |
ZΩ (cid:18) ZΩ (cid:19)
TV ε(u) TV ε(u;Ω′)+TV ε(w;Ω′) TV ε(w)
≤ − −
1 1
= esssup u u(x) d̺ (x)+ u(x) essinf u d̺ (x)
0 1
ε ZΩ \Ω′ (cid:18)B(x,ε) ∩Ω − (cid:19) ε ZΩ \Ω′ (cid:18) −B(x,ε) ∩Ω (cid:19)
1 1
esssup w w(x) d̺ (x) w(x) essinf w d̺ (x)
0 1
− ε ZΩ \Ω′ (cid:18)B(x,ε) ∩Ω − (cid:19) − ε ZΩ \Ω′ (cid:18) −B(x,ε) ∩Ω (cid:19)
Utilizing that u = w on Ω Ω one easily sees that all terms in the right hand side of this
\
′ε
inequality cancel which renders it equal to zero. Therefore, since u was arbitrary, w is a
minimizer as claimed.
Next, we establish a comparison principle for solutions of the minimization problem in
(4.1). Becausethe subdifferentialofTV fromLemma 3.2is nota differentialoperator,weuse
ε
variational instead of PDE techniques to prove this comparison principle.
19Proposition 4.6 (Comparison principle II). Let d L (Ω), let w L (Ω) solve
∞ ∞
∈ ∈
1
w=argmin u d2 d̺+TV (u) : u L2(Ω) ,
ε
2ε | − | ∈
(cid:26) ZΩ (cid:27)
and assume that v L (Ω) satisfies
∞ ′
∈
1 1
v d2 d̺+TV ε(v;Ω′) v w d2 d̺+TV ε(v w;Ω′)
2ε ZΩ′| − | ≤ 2ε ZΩ′| ∨ − | ∨
for an open subset Ω Ω. If v w almost everywhere on Ω Ω or if Ω = Ω, then v w
′
⊂ ≥
′
\
′ε ′
≥
holds almost everywhere in Ω.
′
Proof. LetusfirstabbreviatetheenergyE(u):= 1 u d2 d̺+TV (u;Ω)foru L2(Ω).
2ε Ω′| − | ε ′ ∈ ′
Lemma 4.5impliesthatw isaminimizerofE withfixeddataw onΩ Ω . Bythe assumption
R
′
\
′ε
thatv w onΩ Ω ,wegetthatv w=w onΩ Ω andhencev w isafeasiblecompetitor
≥
′
\
′ε
∧
′
\
′ε
∧
for w on Ω. In the case Ω =Ω it is trivial that v w is a competitor for w on Ω.
′ ′
∧
Assuming that v w =w on a set of positive measure in Ω and using the strict convexity
′
∧ 6
of E, we get
E(w)<E(v w).
∧
Furthermore, by assumption we get that
E(v) E(v w).
≤ ∨
Summing these two inequalities and using Lemma 3.7 to cancel the total variations we get
v d2+ w d2 d̺< v w d2+ v w d2 d̺
ZΩ′| − | | − | ZΩ′| ∨ − | | ∧ − |
= v d2+ w d2 d̺
ZΩ′| − | | − |
which is a contradiction. Hence, we have v w = w almost everywhere on Ω, proving the
′
∧
claim.
In the next lemma we derive a subsolution of the optimization problem in (4.1) where the
data is a cone which corresponds to controlling the action of the scheme (2.7) on a ball. Note
that,asopposedtothecaseofconstantdensitiesandalocaltotalvariation,wecannotcompute
theexplicitsolution. However,asubsolutionsufficesforourpurposes. Forconsistencywiththe
languagein Definition 2, a subsolution is minimal with respect to competitors that are greater
than it.
Lemma 4.7 (Subsolution for cone data). Let Ω RN a bounded and convex domain, x Ω,
0
and define d(x):= x x for x RN. Let furth⊂ ermore ∈
0
| − | ∈
1
w :=argmin u d2 d̺+TV (u).
ε
u ∈L2(Ω) 2ε ZΩ| − |
There exist constants C ,C 1 with 2C C >C and depending only on Lip(̺ ), Lip(̺ ),
1 2 2 1 2 0 1
≥ ≥
c , diam(Ω), and the dimension N, such that for ε > 0 sufficiently small it holds for almost
̺
every x Ω that
∈
C √ε if x x C √ε,
1 0 2
| − |≤
w(x) w(x):= C (C C )ε
≤ x x + 2 1 − 2 else.
0
| − | x x
0
| − |

20Remark 4.8. For a negative cone, i.e., d(x) = x x , it is not immediately obvious that
0
−| − |
w will be a supersolution, in the sense that w w. The reasonfor this is that TV ( u)=
ε
− ≥− − 6
TV (u). However, since all constants in the definition of w only depend on the Lipschitz con-
ε
stantsof̺ and̺ ,thedensitylowerboundc ,andthedimensionN,onecanjustexchangethe
0 1 ̺
rolesofthe densities inthe definitionofTV andreduceto the subsolutioncaseofLemma 4.7.
ε
Proof. Foralighternotationweassumewithoutlossofgeneralitythatx =0 Ω. Throughout
0
∈
the proof, we let C 1 and C 1 be the constants from the lemma statement, deferring
1 2
≥ ≥
their specific choice to the last step of the proof. The strategy is to construct p L2(Ω) that
∈
satisfies
(w d)̺+εp 0 in Ω, (4.13)
− ≥
pϕdx TV (w+ϕ) TV (w) for all ϕ L2(Ω), ϕ 0. (4.14)
ε ε
≤ − ∈ ≥
ZΩ
ThesetwopropertiesimmediatelyimplythatfortheenergyE(u):= 1 u d2 d̺+TV (u)
2ε Ω| − | ε
for u L2(Ω) it holds E(w) E(ϕ+w) for all non-negative test functions ϕ 0. Then
∈ ≤ R ≥
Proposition 4.6withthechoiceϕ:=w w w 0impliestheclaimthatw w. Therequired
∨ − ≥ ≤
function p is reminiscent of a subgradient of TV at w with the difference being that it only
ε
satisfies the subdifferential inequality (4.14) for non-negative test functions.
Step 1 (Construction of a “subgradient”). Since w is a radial function, constructing p is
not difficult. In the spirit of Proposition 3.4, we will define argmin and argmax operators
corresponding to the capped cone w on RN. We first note that w is radially non-decreasing.
Indeed, the derivative of the function f(r):=r+ C2(C1 r−C2)ε satisfies for r ≥C 2√ε:
C (C C )ε C (C C ) C C 2C C
2 1 2 2 1 2 1 2 2 1
f ′(r)=1
−
r−
2
≥1
−
C2− =1
−
C− = C− ≥0.
2 2 2
To construct p we begin by defining the argmax map
x
Γ (x):=σ (x) (4.15)
ε ε
| | x
| |
where the piecewise linear and increasing function σ is defined as
ε
t
t if 0 t<C 2√ε ε,
σ ε(t):=t+εmin
C √ε
ε,1 = 1 −√ε/C 2 ≤ −
(cid:26) 2 − (cid:27) t+ε if t C 2√ε ε,
≥ −
which in particular satisfies σ ε(0) = 0 so thatΓ
ε
is well defined at x = 0. Note that we can
assume ε<1 so that C √ε ε>0. It is important to note that here Γ is a “global” argmax
2 ε
−
map of w that does not see the geometry of the domain Ω, i.e., Γ (x) argmax w. Note
ε ∈ B(x,ε)
that Γ is invertible and by construction an ε-perturbation of the identity. More precisely, the
ε
function σ is invertible on [0, ) [0, ) with inverse τ :=σ 1. Hence, the inverse of Γ is
ε
∞ → ∞
ε ε− ε
given by
x
γ (x):=Γ 1(x)=τ (x)
ε −ε ε
| | x
| |
and thanks to the convexity of Ω it holds that γ (Ω) Ω. It is immediate from the piecewise
ε
⊂
definition of σ that
ε
s (1 √ε/C 2)s if 0 s<C 2√ε,
τ (s):=s εmin ,1 = − ≤ (4.16)
ε
− (cid:26)C 2√ε
(cid:27)
(s −ε if s ≥C 2√ε.
21Using this it is straightforwardto see that γ is an argmin map for w on RN.
ε
We can now define the functions
1
p (x):= ̺ (Γ 1(x)) det Γ 1(x) ̺ (x) ,
0
ε
0 −ε
∇
−ε
−
0
(cid:18) (cid:19)
1 (cid:12) (cid:12)
p (x):= ̺ (x) ̺ (γ(cid:12) 1(x)) det (cid:12) γ 1(x) 1 ,
1 ε 1 − 1 ε− ∇ ε− γε(Ω)
(cid:18) (cid:19)
(cid:12) (cid:12)
p(x):=p 0(x)+p 1(x). (cid:12) (cid:12)
Notethatthereasonwhywehavetointroducethecharacteristicfunction1 inthedefinition
γε(Ω)
of p is that the argmax map Γ =γ 1 exits Ω, i.e., γ 1(Ω) Ω.
1 ε ε− ε−
6⊂
Step 2 (Validity of the “subdifferential” inequality). Next we prove (4.14) by estimating the
L2-inner product of p and ϕ for i = 0,1, where slightly different arguments are required. We
i
startwith i=0. Using achangeofvariablesand Γ (x) argmax w for x Γ 1(Ω)we
find
ε ∈ B(x,ε) ∩Ω ∈ −ε
ε p ϕdx= ̺ (Γ 1) det Γ 1 ̺ ϕdx
0 0 −ε
∇
−ε
−
0
ZΩ ZΩ
(cid:0) (cid:12) (cid:12) (cid:1)
= ϕ Γ(cid:12) d̺ (cid:12)ϕd̺
ε 0 0
ZΓ− ε1(Ω) ◦ − ZΩ
= (w+ϕ) Γ d̺ w+ϕd̺ w Γ d̺ wd̺
ε 0 0 ε 0 0
ZΓ− ε1(Ω) ◦ − ZΩ −" ZΓ− ε1(Ω) ◦ − ZΩ #
esssup(w+ϕ) (w+ϕ) d̺ esssup w w d̺
0 0
≤ ZΩ B( ·,ε) ∩Ω − ! − ZΩ B( ·,ε) ∩Ω − !
+ esssup(w+ϕ)+ esssup wd̺ .
0
ZΩ \Γ− ε1(Ω)− B( ·,ε) ∩Ω B( ·,ε) ∩Ω
The last integral on the right-hand side is non-positive because ϕ 0. In the last inequal-
≥
ity above we used w Γ (x) = esssup w for x Γ 1(Ω), and that (ϕ+w) Γ
esssup (ϕ+w)◦ almε ost everywheB re(x i, nε) ∩ ΓΩ 1(Ω). Th∈ e re−ε asoning for the latter ine◦ quaε lit≤ y
B(,ε) Ω −ε
to hold i·s a∩nalogous to the one in the proof of Proposition 3.4, using that Γ is a Lipschitz
ε
isomorphism and hence preserves Lebesgue null-sets.
The casei=1 is treatedsimilarly,althoughnotentirelysymmetrically. Usingthe factthat
ϕ 0 and that by convexity γ (Ω) Ω we get
ε
≥ ⊂
ε p ϕdx= ̺ ̺ (γ 1) det γ 1 ϕdx
1 1
−
1 ε−
∇
ε−
ZΩ Zγε(Ω)
(cid:0) (cid:12) (cid:12)(cid:1)
(cid:12) (cid:12)
= ϕd̺ ϕ γ d̺
1 ε 1
Zγε(Ω) − Zγε−1(γε(Ω)) ◦
ϕd̺ ϕ γ d̺
1 ε 1
≤ − ◦
ZΩ ZΩ
= (w+ϕ)d̺ (w+ϕ) γ d̺ wd̺ w γ d̺
1 ε 1 1 ε 1
− ◦ − − ◦
ZΩ ZΩ (cid:20)ZΩ ZΩ (cid:21)
(w+ϕ) essinf (w+ϕ) d̺ w essinf w d̺ .
1 1
≤ ZΩ(cid:18) −B( ·,ε) ∩Ω (cid:19) − ZΩ(cid:18) −B( ·,ε) ∩Ω (cid:19)
Hereagain,weneedtoargueasbeforethatγ preservesLebesguenull-sets(asadiffeomorphism)
ε
for the validity of the last inequality. Adding the two inequalities we have just established and
dividing by ε>0 proves (4.14).
22Step 3 (Optimality conditions for supersolution). It remains to prove (4.13), i.e., the in-
equality (w d)̺+εp 0. The basic idea is that w d √ε by the choice of w, so that
− ≥ − ≈
the inequality will follow if we can show εp √ε (up to a constant multiple). Importantly,
≥ −
within this step, we let C > 0 be a constant (possibly changing from line to line) depending
̺
only on Lip(̺ ), c , diam(Ω), and the dimension N.
i ̺
For this we first compute det Γ 1(x) which appears in the definition of p . The Jacobian
∇
−ε 0
of Γ 1 is given by
−ε
x x τ (x) x x
∇Γ −ε1(x)=τ ε′( |x |)
x ⊗ x
+ ε x| | 1
− x ⊗ x
| | | | | | (cid:18) | | | |(cid:19)
τ (x) x x x
= ε x| | 1+ τ ε′( |x |)
τ
| (x|
)
−1
x ⊗ x
.
| | (cid:20) (cid:18) ε | | (cid:19)| | | |(cid:21)
The derivative of τ defined in (4.16) is given by τ (t)=1 √ε1 , so that
ε ε′ − C2 {t ≤C2√ε }
x 0 if x C √ε,
2
(cid:18)τ ε′( |x |)
τ
ε|
(
|x|
|)
−1 (cid:19)=
( xε ε if
| |x| |≤
≥C 2√ε.
| |−
Using this, we make a case distinctionbasedon x: For x <C √ε, using alsothe elemen-
2
| | | |
tary inequality (1+x)N 1+Nx for x 1, we get
≥ ≥−
τ (x) N √ε N √εN
det( ∇Γ −ε1(x))= ε x| | det(1)= 1
− C
≥1
− C
(4.17)
(cid:18) | | (cid:19) (cid:18) 2(cid:19) 2
since ε 1 C2. For x C √ε we can use a Taylor expansion of the determinant to get
≤ ≤ 2 | |≥ 2
N
ε ε x x
det( ∇Γ−ε1(x))= 1
− x
det 1+
x ε x ⊗ x
(cid:18) | |(cid:19) (cid:18) | |− | | | |(cid:19)
2
εN ε ε εN
1 1+ +O 1 , (4.18)
≥
(cid:18)
− |x
|(cid:19)
|x |−ε (cid:18)|x |−ε
(cid:19)
!!≥
(cid:18)
− |x
|(cid:19)
wherewenotethatforε>0sufficientlysmalltheterm ε dominatesthequadraticoneand,
x ε
consequently, both can be dropped. | |−
Using Lipschitz continuity of ̺ and the explicit formula for τ in (4.16) we also have
0 ε
x x x
̺ 0(Γ −ε1(x))=̺
0
τ ε( |x |)
x
≥̺ 0(x) −Lip(̺ 0) x
−
|x |−εmin C| √| ε,1
x
(cid:18) | |(cid:19) (cid:12) (cid:18) (cid:26) 2 (cid:27)(cid:19)| |(cid:12)
(cid:12) x (cid:12)
(cid:12) (cid:12)
=̺ 0(x) εC ̺min(cid:12) | | ,1 . (cid:12)(4.19)
− C √ε
(cid:26) 2 (cid:27)
Combining (4.17), (4.18) and (4.19) we obtain the following lower bound for p :
0
x εN
εp (x) ̺ (x) εC min | | ,1 1 ̺ (x)
0 0 ̺ 0
≥ − C √ε − max x ,C √ε −
(cid:18) (cid:26) 2 (cid:27)(cid:19)(cid:18) {| | 2 }(cid:19)
x √ε
√εC min | |,√ε + .
̺
≥− C max x ,C √ε
(cid:18) (cid:26) 2 (cid:27) {| | 2 }(cid:19)
Fromthisweobtaintwolowerbounds—agenericoneandanimprovedestimateawayfromthe
cone tip:
1
εp (x) √εC √ε+ √εC for all x Ω, (4.20)
0 ̺ ̺
≥− C ≥− ∈
(cid:18) 2(cid:19)
23√ε ε
εp (x) √εC √ε+ C if x C √ε, (4.21)
0 ̺ ̺ 2
≥− x ≥− x | |≥
(cid:18) | |(cid:19) | |
where in the last inequality we have absorbed diam(Ω) into C .
̺
We continue with proving a similar bound for p . For this we remember that γ 1(x) =
1 ε−
Γ (x)=σ (x) x . Analogous to before, we get that the Jacobian is
ε ε | | x
| |
σ (x) x x x
∇γ ε−1(x)= ε x| | 1+ σ ε′( |x |)
σ
| (x|
)
−1
x ⊗ x
,
| | (cid:20) (cid:18) ε | | (cid:19)| | | |(cid:21)
and we find σ (t)=1+ √ε 1 and compute
ε′ C2 −√ε {t ≤C2√ε −ε
}
0 if x C √ε ε,
x | |≤ 2 −
(cid:18)σ ε′( |x |)
σ
ε|
(
|x|
|)
−1 (cid:19)=

− x
ε
+ε
if |x |≥C 2√ε −ε.
| |
Making case distinctions, as before, and also using that (1+x)N 1+2Nx for sufficiently
≤
small x, it holds for x C √ε ε that
2
| |≤ −
ε N 2√εN
det( γ 1(x))= 1+ 1+ (4.22)
∇
ε−
C √ε ε ≤ C √ε
(cid:18) 2 − (cid:19) 2 −
whenever ε>0 is sufficiently small (depending on C ). Similarly, for x C √ε ε, we have
2 2
| |≥ −
2
2εN ε ε
det( ∇γ ε−1(x))
≤
(cid:18)1+
|x | (cid:19)
1
− |x |+ε
+O
(cid:18)|x |+ε (cid:19) !!
2εN
1+ (4.23)
≤ x
(cid:18) | | (cid:19)
for ε>0 sufficiently small. Using Lipschitz continuity of ̺ we have
1
x x x
̺ 1(γ ε−1(x))=̺
1
σ ε( |x |)
x
≤̺ 1(x)+Lip(̺ 1) x
−
|x |+εmin
C
√| ε| ε,1
x
(cid:18) | |(cid:19) (cid:12) (cid:18) (cid:26) 2 − (cid:27)(cid:19)| |(cid:12)
(cid:12) x (cid:12)
(cid:12) (cid:12)
=̺ 1(x)+εC ̺min(cid:12) | | ,1 . (4(cid:12).24)
C √ε ε
(cid:26) 2 − (cid:27)
Combining(4.22),(4.23)and(4.24)weobtainthefollowinglowerboundonp (x)forx γ (Ω):
1 ε
∈
x 2εN
εp (x) ̺ (x) ̺ (x)+εC min | | ,1 1+
1 1 1 ̺
≥ − C √ε ε max x ,C √ε ε
(cid:18) (cid:26) 2 − (cid:27)(cid:19)(cid:18) {| | 2 − }(cid:19)
x √ε
= √εC min | | ,√ε +
̺
− C √ε max x ,C √ε ε
(cid:18) (cid:26) 2 − (cid:27) {| | 2 − }(cid:19)
x ε
√εC min | | ,√ε
̺
− C √ε max x ,C √ε ε
(cid:26) 2 − (cid:27) {| | 2 − }
x √ε
√εC min | | ,√ε +
̺
≥− C √ε max x ,C √ε ε
(cid:18) (cid:26) 2 − (cid:27) {| | 2 − }(cid:19)
if we restrict √ε 1 which, in particular, means ε C √ε ε. Again we deduce two lower
≤ 2 ≤ 2 −
bounds, using also that for x Ω γ (Ω) we even have p (x)=0 by definition of p ,
ε 1 1
∈ \
εp (x) √εC x Ω, (4.25)
1 ̺
≥− ∈
24ε
εp (x) C x C √ε ε. (4.26)
1 ̺ 2
≥− x | |≥ −
| |
Adding the bounds (4.20), (4.21), (4.25) and (4.26) we obtain the cumulative lower bound
εp(x) √εC for all x Ω, (4.27)
̺
≥− ∈
ε
εp(x) C if x C √ε. (4.28)
̺ 2
≥− x | |≥
| |
Finally, we can now turn to proving (4.13), making a case distinction based on x. If
| |
0 x C √ε we can use the definition of w and the lower bound (4.27) to get
2
≤| |≤
(w(x) d(x))̺(x)+εp(x) (C √ε x)̺(x)+εp(x)
1
− ≥ −| |
√ε(C C C )̺(x) 0
1 2 ̺
≥ − − ≥
if we choose the gap between C and C sufficiently large (depending only on C ). In the case
1 2 ̺
x C √ε we can use the sharper lower bound (4.28) to obtain
2
| |≥
C (C C )ε ε
2 1 2
(w(x) d(x))̺(x)+εp(x) − ̺(x) C ̺(x)
̺
− ≥ x − x
| | | |
ε̺(x)
= (C (C C ) C ) 0
2 1 2 ̺
x − − ≥
| |
ifwechoosethegapbetweenC andC sufficientlylarge(againdependingonlyonC ). Hence,
1 2 ̺
we have proved (4.13) which concludes the proof.
The first corollary of Lemma 4.7 (in fact of Remark 4.8) is that it allows us to control the
evolution of a ball under the scheme (2.7).
Corollary 4.9 (Supersolution for balls). Under the conditions of Lemma 4.7 there exists a
constant C >0, depending only on Lip(̺ ), Lip(̺ ), c , diam(Ω), and the dimension N, such
0 1 ̺
that for any x Ω, 0<R<dist(x ,∂Ω), and ε>0 sufficiently small it holds that
0 0
∈
S (B(x ,R)) B(x ,R C√ε).
ε 0 0
⊃ −
Proof. WeapplyRemark 4.8tod(x):= x x andnotethatsdist(x,B(x ,R)c)=d(x)+R
0 0
−| − |
to infer that w in the definition of S (B(x ,R)) satisfies for almost all x Ω that
ε∗ ε 0
∈
w ε∗(x) ≥R −C√ε −|x −x
0
|.
Here, C >0 is a constant depending on C ,C in the definition of w in Lemma 4.7. Hence, we
1 2
obtain S (B(x ,R))= w >0 B(x ,R C√ε).
ε 0
{
ε∗
}⊃
0
−
Wehighlightthealmost-regularityofL2-minimizerswithLipschitzdataasanapplicationof
the cone Lemma 4.7. Our proof of consistency basically relies on the same argument, allowing
us to avoid Lipschitz regularity. We note that in the periodic setting with constant densities,
one can use a simple comparisonargument to show that minimizers of the functional in (4.29)
(just below) are Lipschitz regular;but the moment one destroystranslationalinvariance of the
problem,suchregularitybecomesmuchmorechallenging. Seeforinstance[18,Theorem3.1]for
the related TV problem with boundary conditions. Similar almost-Lipschitz regularity results
for solutions of nonlocal problems can be found, for instance, in [5, 10].
25Corollary 4.10 (Almost-Lipschitz regularity.). Let Ω RN be a bounded, open, and convex
⊂
set. Suppose that w L (Ω) satisfies
∞
∈
1
w :=argmin u f 2 d̺+TV (u) : u L2(Ω) , (4.29)
ε
2ε | − | ∈
(cid:26) ZΩ (cid:27)
for a 1-Lipschitz function f C(Ω). Then for almost all x,x Ω it holds that
0
∈ ∈
w(x) w(x ) x x +C√ε, (4.30)
0 0
| − |≤| − |
where C >0 depends only on ̺, diam(Ω), and the dimension N.
Proof. Fix x Ω. Let w be the minimizer
0 shift
∈
1
w :=argmin u(x) x x 2 d̺(x)+TV (u) : u L2(Ω) ,
shift 0 ε
2ε | −| − || ∈
(cid:26) ZΩ (cid:27)
andwbethefunctiondefinedinLemma 4.7. Asf is1-Lipschitz,wehavef() x +f(x ),
0 0
· ≤|·− |
andsobyProposition 4.3appliedtowandw =w +f(x )andLemma 4.7appliedtow ,
′ shift 0 shift
we have w w +f(x ) w+f(x ). Applying the same reasoning with supersolutions
shift 0 0
≤ ≤
(using Remark 4.8) and noting that w() x +C√ε gives that
0
· ≤|·− |
w(x) f(x ) x x +C√ε
0 0
| − |≤| − |
for any x Ω. Using this inequality twice (once with x=x ), applying the triangleinequality,
0
∈
and increasing C directly gives (4.30).
Finally, we conclude the proof of Theorem 2 by showing that the operator is consistent.
Proposition 4.11 (Consistency). The operator S defined in (4.1) is consistent in the sense
ε
of Definition 3.
Proof. The proof follows the strategy of [14, Proposition 4.1] with non-trivial modifications
since the scheme (4.1) involves the nonlocal total variation instead of the local one. We show
that Definition 3 is satisfied for subflows, with superflows being analogous.
Step 1 (Construction of an variational subsolution). We let [t ,t ] t A(t) be a subflow
0 1
∋ 7→
in the sense of Definition 2 contained in the neighborhood B. Recall that we define d(x,t) :=
sdist(x,Ac(t)).
For fixed t [t ,t ] and r >0 we define
0 1
∈
Ω := d(,t) <r
′
{| · | }
to be the tube of width r aroundthe boundary ∂A(t). Here we chooser sufficiently smallsuch
that Ω B (RN t ), so that d(x,t+τ) is in C2,1(Ω [ ε,ε]) for all small ε; note the
choiceo′ f⊂ r can∩ bema× d{ ei} ndependentoftdependingonx ly,τ on′ t× he− smoothsubflow. Letψ :R R
→
be smooth with ψ(s) s, ψ(s)=s for s in a neighborhoodof 0, and ψ (s) c>0, and define
′
≥ ≥
v (x):=ψ(d(x,t+ε)). By Definition 2, it holds that
ε
v (x) d(x,t) d(x,t+ε) d(x,t)
ε
− −
ε ≥ ε
ε d
= d(x,t+τ)dτ
− dτ
Z0
ε
= ∂ d(x,t+τ)dτ
t
−
Z0
26ε 1
div(̺(x) d(x,t+τ)) dτ +δ.
≥− ̺(x) ∇
Z0
Letting ω denote a modulus ofcontinuity of 1 div(̺(x) d(x,τ)) in τ (uniformly inx) onB,
̺(x) ∇
we have
v (x) d(x,t) 1
ε
− div(̺(x) d(x,t+ε))+δ ω(ε). (4.31)
ε ≥ ̺(x) ∇ −
Note furthermore that v (x)=ψ (d(x,t+ε)) d(x,t+ε). On one hand this implies
ε ′
∇ ∇
v c in Ω, (4.32)
ε ′
|∇ |≥
which will be useful later. On the other hand, we see that
v (x) d(x,t+ε)
ε
∇ = ∇ = d(x,t+ε).
v (x) d(x,t+ε) ∇
ε
|∇ | |∇ |
Using this and reordering (4.31) we get
v (x) d(x,t) v (x)
ε ε
− ̺(x) div ̺(x) ∇ ̺(x)(δ ω(ε)) 0. (4.33)
ε − v (x) − − ≥
(cid:18) |∇ ε |(cid:19)
Let ϕ L (Ω) be a non-negative test function with suppϕ Ω . Let us also define the
∈
∞ ′
⊂
′2ε
energy
1
E (u;Ω):= u d(,t)2 d̺+TV (u;Ω),
ε ′ ε ′
2ε ZΩ′| − · |
where TV (u;Ω) denotes the total variation (2.6) with Ω replaced by Ω. We let
ε ′ ′
d (Γ ) ̺ ̺ ̺ (γ ) ̺
ε ♯ 0 0 1 ε ♯ 1
V (y):= − + −
ε d N ε ε
L (cid:20) (cid:21)
be the density of the pushforward, where the right-hand side is defined as in Proposition 3.4
with u replaced by v , which is admissible due to (4.32). Multiplying (4.33) by ϕ, using its
ε
non-negativity, and integrating over Ω yields
′
v (x) d(x,t) v (x)
ε ε
E ε(v ε;Ω ′) E ε(v ε;Ω ′)+ − ̺(x) div ̺(x) ∇ ϕ(x)dx
≤ ZΩ′
(cid:18)
ε −
(cid:18)
|∇v ε(s)
|(cid:19)(cid:19)
(δ ω(ε)) ϕ(x)d̺(x)
− − ZΩ′
v (x) d(x,t)
ε
=E ε(v ε;Ω′)+ − ̺(x)+V ε(x) ϕ(x)dx
ZΩ′
(cid:18)
ε
(cid:19)
v (x)
ε
V (x)+div ̺(x) ∇ ϕ(x)dx
ε
− ZΩ′
(cid:18) (cid:18)
|∇v ε(s)
|(cid:19)(cid:19)
(δ ω(ε)) ϕ(x)d̺(x)
− − ZΩ′
E (v +ϕ;Ω)+(ω(ε)+o (1) δ) ϕ(x)d̺(x),
ε ε ′ ε 0
≤ → − ZΩ′
27where in the last step, we completed a square, used Propositions 3.4 and 3.5 on Ω together
′
with the gradient bound (4.32) for v and the fact that suppϕ Ω . Since δ > 0 we can
ε
⊂
′2ε
choose ε>0 sufficiently small such that the second term is non-positive which implies
E (v ;Ω) E (v +ϕ;Ω) (4.34)
ε ε ′ ε ε ′
≤
f or ε>0 sufficiently small.
Step 2 (Conclusion, assuming ordered boundary values). Supposing that
v w on Ω Ω , (4.35)
ε
≥
ε∗ ′
\
′2ε
the non-negative test function ϕ := v w v (where w solves the scheme (4.1)) can be
ε ε
∨
ε∗
−
ε ε∗
inserted into (4.34). Consequently, we have that E (v ) E (v w ), and Proposition 4.6
ε ε
≤
ε ε
∨
ε∗
implies that v w on Ω (one must technically deal with null-sets, but this may be done as
ε
≥
ε∗ ′
in the proof of Proposition 4.4), and hence, we find
S ε(A(t)) ∩Ω′ = {w ε∗ >0 }∩Ω′ ⊂{v ε >0 }∩Ω′ = {d( ·,t+ε)>0 }∩Ω′ =A(t+ε) ∩Ω′.
Similarly, we will see in the next step,
w >0 Ω d(,t)>0 Ω. (4.36)
{
ε∗
}\
′
⊂{ · }\
′
Further, outside of the set Ω, for sufficiently small ε (depending only on the smooth subflow),
′
we have A(t) Ω =A(t+ε) Ω. Putting these last two pieces together, we recover
′ ′
\ \
S (A(t)) Ω = w >0 Ω d(,t)>0 Ω =A(t+ε) Ω.
ε
\
′
{
ε∗
}\
′
⊂{ · }\
′
\
′
Uniting the subset relations for S (A(t)) concludes the proof. We now turn to the proof of
ε
(4.35) and (4.36).
Step 3 (Ordered boundary values). To prove (4.35), we have to pick a suitable function ψ
in the definition of v = ψ(d(,t+ε)). So far we have only used that ψ(s) s, ψ(s) = s in a
ε
· ≥
neighborhood of 0, and that ψ c>0.
′
≥
First, we argue that one can find ψ such that v w in the part of the 2ε-neighborhood
ε
≥
ε∗
of the boundary of Ω that lies inside of A(t): For all ε > 0 small enough and x ∂Ω with
′ ′
∈
d(x,t)=r, itholds thatd(x,t+ε) 7r sinced isuniformly continuousintime. Since dis also
≥ 8
uniformly continuous in space, we get d(x,t+ε) 3r for all x in (Ω Ω ) A(t) if ε > 0 is
≥ 4 ′ \ ′2ε ∩
sufficiently small. On the other hand, by Proposition 4.1 it holds that w d(,t)
ε∗
≤ k ·
kL∞(Ω)
≤
diamΩ. So if we choose ψ such that ψ(3r) diamΩ, we get
4 ≥
3r
v (x)=ψ(d(x,t+ε)) ψ diamΩ w (x)
ε
≥ 4 ≥ ≥
ε∗
(cid:18) (cid:19)
for all x (Ω Ω ) A(t).
∈
′
\
′2ε
∩
Next, we argue that also in the 2ε-neighborhoodof the exterior part of the boundary of Ω
′
one canfind anappropriateψ suchthat v w : The argumentfor this is more involvedthan
ε
≥
ε∗
fortheinnerpartsinceinprinciplew couldbearbitrarilyclosetozerooutsideofA(t)whereas
ε∗
the signed distance function d(,t+ε) in the definition of v might be substantially negative.
ε
·
If this happened, to obtain v w we could be forced to take ψ( 3r/4) = 0 breaking the
ε
≥
ε∗
−
constraint ψ c.
′
≥
Instead,fixapointx ∂Ω suchthatd(x,t)= r. Sincethedistancefunctionis1-Lipschitz
′
∈ −
d(,t) x r,
· ≤|·− |−
28and therefore Proposition 4.3 ensures that w w in Ω where
ε∗
≤
ε
1
w :=argmin u ( x r)2 d̺+TV (u).
ε ε
u ∈L2(Ω) 2ε ZΩ| − |·− |− |
However,byLemma 4.7,w w r,anditfollowsthatw r+C √εforall x x C √ε.
ε
≤ −
ε∗
≤−
1
|
′
− |≤
2
Noting that this reasoning can be uniformly applied at all points x ∂Ω with d(x,t) = r,
′
∈ −
we see that for sufficiently small ε>0,
3
w r on (Ω Ω ) A(t). (4.37)
ε∗
≤−4
′
\
′2ε
\
Restricting ψ to satisfy ψ(t) 3r for any t diamΩ we therefore get
≥− 4 ≥−
3r
v ε(x)=ψ(d(x,t+ε))
≥− 4
≥w ε∗(x)
for all x (Ω Ω ) A(t). Hence, we have shown (4.35). The same reasoning used to obtain
∈
′
\
′2ε
\
(4.37), but now at a point for which d(x,t) r, shows
≤−
w >0 (Ω A(t)) d(,t)>0 (Ω A(t))=
{
ε∗
}\
′
∪ ⊂{ · }\
′
∪ ∅
which directly gives (4.36), completing the proof.
Note we have proven that there exists an ε > 0 sufficiently small such that consistency
0
in Definition 3 is satisfied at a given time t for all ε < ε , but actually, our estimate for ε is
0 0
uniform in t [t ,t ].
0 1
∈
Acknowledgments
The authors would like to thank Antonin Chambolle for fruitful discussions around the con-
struction subsolutions for cone data which happened during the Oberwolfach workshop 2349
“VariationalMethods forEvolution”. Partsofthis workwere donewhen LB wasaffiliatedwith
the Technical University of Berlin, supported by Germany’s Excellence Strategy – The Berlin
Mathematics Research Center MATH+ (EXC-2046/1, project ID: 390685689). The authors
were affiliated with the Hausdorff Center for Mathematics during parts of this project and the
funding from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) un-
der Germany’s Excellence Strategy – EXC-2047/1– 390685813is greatly appreciated. KS was
also supported by the DFG project 211504053- SFB 1060.
References
[1] F.Almgren,J.E.Taylor,andL.Wang,“Curvature-drivenflows:Avariationalapproach,”
SIAM Journal on Control and Optimization, vol. 31, no. 2, pp. 387–438, 1993 (cit. on
pp. 3–5).
[2] P.Awasthi,N.Frank,andM.Mohri,“Ontheexistenceoftheadversarialbayesclassifier,”
in Advances in Neural Information Processing Systems 34, NeurIPS 2021, M. Ranzato,
A.Beygelzimer,Y.N.Dauphin, P.Liang,andJ.W.Vaughan,Eds.,2021,pp.2978–2990
(cit. on p. 2).
[3] G. Bellettini, Lecture Notes on Mean Curvature Flow, Barriers and Singular Perturba-
tions. Scuola Normale Superiore, 2013 (cit. on p. 14).
29[4] G. Bellettini and M. Novaga, “Minimal barriers for geometric evolutions,” Journal of
Differential Equations, vol. 139, no. 1, pp. 76–103,1997 (cit. on p. 7).
[5] L.Bungert,J.Calder,andT.Roith,“Uniformconvergenceratesfor lipschitz learningon
graphs,” IMA Journal of Numerical Analysis, vol.43, no. 4, pp. 2445–2495,2023(cit. on
p. 25).
[6] L. Bungert, N. García Trillos, M. Jacobs, D. McKenzie, Ð. Nikolić, and Q. Wang, It
begins with a boundary: A geometric view on probabilistically robust learning,2023.arXiv:
2305.18779[cs.LG](cit. on p. 2).
[7] L. Bungert, N. García Trillos, and R. Murray, “The geometry of adversarial training in
binary classification,” Information and Inference: A Journal of the IMA, vol. 12, no. 2,
pp. 921–968,Jun. 2023, issn: 2049-8772(cit. on pp. 2, 4, 6–8, 12, 13).
[8] L. Bungert and K. Stinson, “Gamma-convergence of a nonlocal perimeter arising in ad-
versarial machine learning,” Calculus of Variations and Partial Differential Equations,
2024, forthcoming (cit. on pp. 2, 4, 8).
[9] J. Calder, B. Cook, M. Thorpe, and D. Slepcev, “Poisson learning: Graph based semi-
supervised learning at very low label rates,” in International Conference on Machine
Learning, PMLR, 2020, pp. 1306–1316(cit. on p. 3).
[10] J.Calder,N. GarcíaTrillos,andM.Lewicka,“Lipschitzregularityofgraphlaplacianson
random data clouds,” SIAM Journal on Mathematical Analysis, vol. 54, no. 1, pp. 1169–
1222, 2022 (cit. on p. 25).
[11] A. Chambolle, “An algorithm for mean curvature motion,” Interfaces and Free Bound-
aries, vol. 6, no. 2, pp. 195–218,2004 (cit. on pp. 3, 5–7, 16, 18).
[12] A. Chambolle, D. De Gennaro, and M. Morini, “Minimizing movements for anisotropic
and inhomogeneous mean curvature flows,” Advances in Calculus of Variations, no. 0,
2023 (cit. on p. 7).
[13] A.Chambolle,A.Giacomini,andL.Lussardi,“Continuouslimits ofdiscreteperimeters,”
ESAIM: Mathematical Modelling and Numerical Analysis, vol. 44, no. 2, pp. 207–230,
2010 (cit. on p. 13).
[14] A. Chambolle and M. Novaga, “Approximation of the anisotropic mean curvature flow,”
Mathematical Models and Methods in Applied Sciences,vol.17,no.06,pp.833–844,2007
(cit. on pp. 7, 14, 19, 26).
[15] T. F. Chan and S. Esedoglu, “Aspects of total variation regularized L1 function approx-
imation,” SIAM Journal on Applied Mathematics, vol. 65, no. 5, pp. 1817–1837, 2005
(cit. on p. 5).
[16] K. Crane, C. Weischedel, and M. Wardetzky, “Geodesics in heat: A new approach to
computingdistancebasedonheatflow,” ACM Transactions on Graphics (TOG),vol.32,
no. 5, pp. 1–11,2013 (cit. on p. 5).
[17] T. Eto and Y. Giga, “On a minimizing movement scheme for mean curvature flow with
prescribedcontactangleinacurveddomainanditscomputation,” Annali di Matematica
Pura ed Applicata, pp. 1–27, Nov. 2023 (cit. on p. 7).
[18] T. Eto and Y. Giga, A convergence result for a minimizing movement scheme for mean
curvatureflowwithprescribedcontactangleinacurveddomain,2024.arXiv:2402.16180[math.AP]
(cit. on pp. 7, 25).
[19] N. Garcá Trillos, M. Jacobs, and J. Kim, On the existence of solutions to adversarial
training in multiclass classification, 2023. arXiv: 2305.00075[cs.LG] (cit. on p. 2).
30[20] N.GarcíaTrillosandM.Jacobs,“Ananalyticalandgeometricperspectiveonadversarial
robustness,” Notices of the American Mathematical Society, vol. 70, no. 08, 2023 (cit. on
p. 2).
[21] N. García Trillos, M. Jacobs, and J. Kim, “The multimarginal optimal transport for-
mulation of adversarialmulticlass classification,” Journal of Machine Learning Research,
vol. 24, no. 45, pp. 1–56,2023 (cit. on p. 2).
[22] N. García Trillos and R. Murray, “Adversarial classification: Necessary conditions and
geometric flows,” Journal of Machine Learning Research, vol. 23,no. 187,pp. 1–38,2022
(cit. on pp. 2–4).
[23] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial ex-
amples,” in3rd International Conference on Learning Representations, ICLR 2015, Con-
ference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2015 (cit. on p. 1).
[24] T.LauxandJ.Lelmi,Largedatalimitofthemboschemefordataclustering:Γ-convergence
of the thresholding energies, 2022. arXiv: 2112.06737[math.AP](cit. on p. 3).
[25] T. Laux and J. Lelmi, “Large data limit of the mbo scheme for data clustering: Conver-
genceofthedynamics,” Journalof Machine Learning Research,vol.24,no.344,pp.1–49,
2023 (cit. on p. 3).
[26] S. Luckhaus and T. Sturzenhecker, “Implicit time discretization for the mean curvature
flow equation,” Calculus of Variations and Partial Differential Equations, vol. 3, no. 2,
pp. 253–271,1995 (cit. on p. 5).
[27] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards deep learning
models resistant to adversarial attacks,” in 6th International Conference on Learning
Representations, ICLR 2018, Conference Track Proceedings, 2018 (cit. on pp. 1, 2).
[28] E. Merkurjev, A. L. Bertozzi, and F. Chung, “A semi-supervised heat kernel pagerank
mboalgorithmfordataclassification,” CommunicationsinMathematicalSciences,vol.16,
no. 5, pp. 1241–1265,2018 (cit. on p. 3).
[29] E.Merkurjev,T.Kostic,andA.L.Bertozzi,“Anmboschemeongraphsforclassification
and image processing,” SIAM Journal on Imaging Sciences, vol. 6, no. 4, pp. 1903–1930,
2013 (cit. on p. 3).
[30] M. Novaga and A. Chambolle, “Implicit time discretization of the mean curvature flow
withadiscontinuousforcingterm,” InterfacesandFreeBoundaries,vol.10,no.3,pp.283–
300, 2008 (cit. on p. 7).
[31] M. Pintor, F. Roli, W. Brendel, and B. Biggio, “Fast minimum-norm adversarialattacks
through adaptive norm constraints,” in Advances in Neural Information Processing Sys-
tems34: NeurIPS2021,M.Ranzato,A.Beygelzimer,Y.N.Dauphin,P.Liang,andJ.W.
Vaughan, Eds., 2021, pp. 20052–20062(cit. on p. 5).
[32] M. S. Pydi and V. Jog, “Adversarial risk via optimal transport and optimal couplings,”
in International Conference on Machine Learning, PMLR, 2020, pp. 7814–7823 (cit. on
p. 2).
[33] M. S. Pydi and V. Jog, “The many faces of adversarial risk,” Advances in Neural Infor-
mation Processing Systems, vol. 34, pp. 10000–10012,2021 (cit. on p. 2).
[34] J. A. Sethian, “A fast marching level set method for monotonically advancing fronts.,”
proceedings of the National Academy of Sciences, vol.93,no. 4,pp. 1591–1595,1996(cit.
on p. 5).
31[35] C.Szegedyet al.,“Intriguingpropertiesofneuralnetworks,” in2nd InternationalConfer-
ence on Learning Representations, ICLR 2014, Conference Track Proceedings, Y. Bengio
and Y. LeCun, Eds., 2014 (cit. on p. 1).
[36] Y. Van Gennip, N. Guillen, B. Osting, and A. L. Bertozzi, “Mean curvature, threshold
dynamics,andphasefieldtheoryonfinitegraphs,” Milan JournalofMathematics,vol.82,
pp. 3–65, 2014 (cit. on p. 3).
[37] S. R. S. Varadhan, “On the behavior of the fundamental solution of the heat equation
with variable coefficients,” Communications on Pure and Applied Mathematics, vol. 20,
no. 2, pp. 431–455,1967 (cit. on p. 5).
[38] H.Zhang,Y.Yu,J.Jiao,E.P.Xing,L.E.Ghaoui,andM.I.Jordan,“Theoreticallyprin-
cipledtrade-offbetweenrobustnessandaccuracy,” inProceedingsofthe36thInternational
ConferenceonMachineLearning,ICML2019,K.ChaudhuriandR.Salakhutdinov,Eds.,
ser.ProceedingsofMachineLearningResearch,vol.97,PMLR,2019,pp.7472–7482(cit.
on p. 2).
32