[
    {
        "title": "Preserving linear invariants in ensemble filtering methods",
        "authors": "Mathieu Le ProvostJan GlaubitzYoussef Marzouk",
        "links": "http://arxiv.org/abs/2404.14328v1",
        "entry_id": "http://arxiv.org/abs/2404.14328v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14328v1",
        "summary": "Formulating dynamical models for physical phenomena is essential for\nunderstanding the interplay between the different mechanisms and predicting the\nevolution of physical states. However, a dynamical model alone is often\ninsufficient to address these fundamental tasks, as it suffers from model\nerrors and uncertainties. One common remedy is to rely on data assimilation,\nwhere the state estimate is updated with observations of the true system.\nEnsemble filters sequentially assimilate observations by updating a set of\nsamples over time. They operate in two steps: a forecast step that propagates\neach sample through the dynamical model and an analysis step that updates the\nsamples with incoming observations. For accurate and robust predictions of\ndynamical systems, discrete solutions must preserve their critical invariants.\nWhile modern numerical solvers satisfy these invariants, existing\ninvariant-preserving analysis steps are limited to Gaussian settings and are\noften not compatible with classical regularization techniques of ensemble\nfilters, e.g., inflation and covariance tapering. The present work focuses on\npreserving linear invariants, such as mass, stoichiometric balance of chemical\nspecies, and electrical charges. Using tools from measure transport theory\n(Spantini et al., 2022, SIAM Review), we introduce a generic class of nonlinear\nensemble filters that automatically preserve desired linear invariants in\nnon-Gaussian filtering problems. By specializing this framework to the Gaussian\nsetting, we recover a constrained formulation of the Kalman filter. Then, we\nshow how to combine existing regularization techniques for the ensemble Kalman\nfilter (Evensen, 1994, J. Geophys. Res.) with the preservation of the linear\ninvariants. Finally, we assess the benefits of preserving linear invariants for\nthe ensemble Kalman filter and nonlinear ensemble filters.",
        "updated": "2024-04-22 16:39:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14328v1"
    },
    {
        "title": "A Bayesian Approach for Prioritising Driving Behaviour Investigations in Telematic Auto Insurance Policies",
        "authors": "Mark McLeodBernardo Perez-OrozcoNika LeeDavide Zilli",
        "links": "http://arxiv.org/abs/2404.14276v1",
        "entry_id": "http://arxiv.org/abs/2404.14276v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14276v1",
        "summary": "Automotive insurers increasingly have access to telematic information via\nblack-box recorders installed in the insured vehicle, and wish to identify\nundesirable behaviour which may signify increased risk or uninsured activities.\nHowever, identification of such behaviour with machine learning is non-trivial,\nand results are far from perfect, requiring human investigation to verify\nsuspected cases. An appropriately formed priority score, generated by automated\nanalysis of GPS data, allows underwriters to make more efficient use of their\ntime, improving detection of the behaviour under investigation.\n  An example of such behaviour is the use of a privately insured vehicle for\ncommercial purposes, such as delivering meals and parcels. We first make use of\ntrip GPS and accelerometer data, augmented by geospatial information, to train\nan imperfect classifier for delivery driving on a per-trip basis. We make use\nof a mixture of Beta-Binomial distributions to model the propensity of a\npolicyholder to undertake trips which result in a positive classification as\nbeing drawn from either a rare high-scoring or common low-scoring group, and\nlearn the parameters of this model using MCMC. This model provides us with a\nposterior probability that any policyholder will be a regular generator of\nautomated alerts given any number of trips and alerts. This posterior\nprobability is converted to a priority score, which was used to select the most\nvaluable candidates for manual investigation.\n  Testing over a 1-year period ranked policyholders by likelihood of commercial\ndriving activity on a weekly basis. The top 0.9% have been reviewed at least\nonce by the underwriters at the time of writing, and of those 99.4% have been\nconfirmed as correctly identified, showing the approach has achieved a\nsignificant improvement in efficiency of human resource allocation compared to\nmanual searching.",
        "updated": "2024-04-22 15:26:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14276v1"
    },
    {
        "title": "Rotting Infinitely Many-armed Bandits beyond the Worst-case Rotting: An Adaptive Approach",
        "authors": "Jung-hun KimMilan VojnovicSe-Young Yun",
        "links": "http://arxiv.org/abs/2404.14202v1",
        "entry_id": "http://arxiv.org/abs/2404.14202v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14202v1",
        "summary": "In this study, we consider the infinitely many armed bandit problems in\nrotting environments, where the mean reward of an arm may decrease with each\npull, while otherwise, it remains unchanged. We explore two scenarios capturing\nproblem-dependent characteristics regarding the decay of rewards: one in which\nthe cumulative amount of rotting is bounded by $V_T$, referred to as the\nslow-rotting scenario, and the other in which the number of rotting instances\nis bounded by $S_T$, referred to as the abrupt-rotting scenario. To address the\nchallenge posed by rotting rewards, we introduce an algorithm that utilizes UCB\nwith an adaptive sliding window, designed to manage the bias and variance\ntrade-off arising due to rotting rewards. Our proposed algorithm achieves tight\nregret bounds for both slow and abrupt rotting scenarios. Lastly, we\ndemonstrate the performance of our algorithms using synthetic datasets.",
        "updated": "2024-04-22 14:11:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14202v1"
    },
    {
        "title": "Prediction of flow and elastic stresses in a viscoelastic turbulent channel flow using convolutional neural networks",
        "authors": "Arivazhagan G. BalasubramanianRicardo VinuesaOuti Tammisola",
        "links": "http://arxiv.org/abs/2404.14121v1",
        "entry_id": "http://arxiv.org/abs/2404.14121v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14121v1",
        "summary": "Neural-network models have been employed to predict the instantaneous flow\nclose to the wall in a viscoelastic turbulent channel flow. The numerical\nsimulation data at the wall is utilized to predict the instantaneous velocity\nfluctuations and polymeric-stress fluctuations at three different wall-normal\npositions. Apart from predicting the velocity fluctuations well in a\nhibernating flow, the neural-network models are also shown to predict the\npolymeric shear stress and the trace of the polymeric stresses at a given\nwall-normal location with reasonably good accuracy. These non-intrusive sensing\nmodels can be integrated in an experimental setting to construct the\npolymeric-stress field in turbulent flows, which otherwise may not be directly\nquantifiable in experimental measurements.",
        "updated": "2024-04-22 12:19:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14121v1"
    },
    {
        "title": "Noise contrastive estimation with soft targets for conditional models",
        "authors": "Johannes HuggerVirginie Uhlmann",
        "links": "http://arxiv.org/abs/2404.14076v1",
        "entry_id": "http://arxiv.org/abs/2404.14076v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14076v1",
        "summary": "Soft targets combined with the cross-entropy loss have shown to improve\ngeneralization performance of deep neural networks on supervised classification\ntasks. The standard cross-entropy loss however assumes data to be categorically\ndistributed, which may often not be the case in practice. In contrast, InfoNCE\ndoes not rely on such an explicit assumption but instead implicitly estimates\nthe true conditional through negative sampling. Unfortunately, it cannot be\ncombined with soft targets in its standard formulation, hindering its use in\ncombination with sophisticated training strategies. In this paper, we address\nthis limitation by proposing a principled loss function that is compatible with\nprobabilistic targets. Our new soft target InfoNCE loss is conceptually simple,\nefficient to compute, and can be derived within the framework of noise\ncontrastive estimation. Using a toy example, we demonstrate shortcomings of the\ncategorical distribution assumption of cross-entropy, and discuss implications\nof sampling from soft distributions. We observe that soft target InfoNCE\nperforms on par with strong soft target cross-entropy baselines and outperforms\nhard target NLL and InfoNCE losses on popular benchmarks, including ImageNet.\nFinally, we provide a simple implementation of our loss, geared towards\nsupervised classification and fully compatible with deep classification model\ntrained with cross-entropy.",
        "updated": "2024-04-22 10:45:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14076v1"
    }
]