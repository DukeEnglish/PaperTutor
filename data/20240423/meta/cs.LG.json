[
    {
        "title": "SpaceByte: Towards Deleting Tokenization from Large Language Modeling",
        "authors": "Kevin Slagle",
        "links": "http://arxiv.org/abs/2404.14408v1",
        "entry_id": "http://arxiv.org/abs/2404.14408v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14408v1",
        "summary": "Tokenization is widely used in large language models because it significantly\nimproves performance. However, tokenization imposes several disadvantages, such\nas performance biases, increased adversarial vulnerability, decreased\ncharacter-level modeling performance, and increased modeling complexity. To\naddress these disadvantages without sacrificing performance, we propose\nSpaceByte, a novel byte-level decoder architecture that closes the performance\ngap between byte-level and subword autoregressive language modeling. SpaceByte\nconsists of a byte-level Transformer model, but with extra larger transformer\nblocks inserted in the middle of the layers. We find that performance is\nsignificantly improved by applying these larger blocks only after certain\nbytes, such as space characters, which typically denote word boundaries. Our\nexperiments show that for a fixed training and inference compute budget,\nSpaceByte outperforms other byte-level architectures and roughly matches the\nperformance of tokenized Transformer architectures.",
        "updated": "2024-04-22 17:59:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14408v1"
    },
    {
        "title": "A mean curvature flow arising in adversarial training",
        "authors": "Leon BungertTim LauxKerrek Stinson",
        "links": "http://arxiv.org/abs/2404.14402v1",
        "entry_id": "http://arxiv.org/abs/2404.14402v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14402v1",
        "summary": "We connect adversarial training for binary classification to a geometric\nevolution equation for the decision boundary. Relying on a perspective that\nrecasts adversarial training as a regularization problem, we introduce a\nmodified training scheme that constitutes a minimizing movements scheme for a\nnonlocal perimeter functional. We prove that the scheme is monotone and\nconsistent as the adversarial budget vanishes and the perimeter localizes, and\nas a consequence we rigorously show that the scheme approximates a weighted\nmean curvature flow. This highlights that the efficacy of adversarial training\nmay be due to locally minimizing the length of the decision boundary. In our\nanalysis, we introduce a variety of tools for working with the subdifferential\nof a supremal-type nonlocal total variation and its regularity properties.",
        "updated": "2024-04-22 17:58:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14402v1"
    },
    {
        "title": "RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?",
        "authors": "Adrian de WynterIshaan WattsNektar Ege AltıntoprakTua WongsangaroonsriMinghui ZhangNoura FarraLena BaurSamantha ClaudetPavel GajdusekCan GörenQilong GuAnna KaminskaTomasz KaminskiRuby KuoAkiko KyubaJongho LeeKartik MathurPetter MerokIvana MilovanovićNani PaananenVesa-Matti PaananenAnna PavlenkoBruno Pereira VidalLuciano StrikaYueh TsaoDavide TurcatoOleksandr VakhnoJudit VelcsovAnna VickersStéphanie VisserHerdyan WidarmantoAndrey ZaikinSi-Qing Chen",
        "links": "http://arxiv.org/abs/2404.14397v1",
        "entry_id": "http://arxiv.org/abs/2404.14397v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14397v1",
        "summary": "Large language models (LLMs) and small language models (SLMs) are being\nadopted at remarkable speed, although their safety still remains a serious\nconcern. With the advent of multilingual S/LLMs, the question now becomes a\nmatter of scale: can we expand multilingual safety evaluations of these models\nwith the same velocity at which they are deployed? To this end we introduce\nRTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and\noutputs in 28 languages. RTP-LX follows participatory design practices, and a\nportion of the corpus is especially designed to detect culturally-specific\ntoxic language. We evaluate seven S/LLMs on their ability to detect toxic\ncontent in a culturally-sensitive, multilingual scenario. We find that,\nalthough they typically score acceptably in terms of accuracy, they have low\nagreement with human judges when judging holistically the toxicity of a prompt,\nand have difficulty discerning harm in context-dependent scenarios,\nparticularly with subtle-yet-harmful content (e.g. microagressions, bias). We\nrelease of this dataset to contribute to further reduce harmful uses of these\nmodels and improve their safe deployment.",
        "updated": "2024-04-22 17:56:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14397v1"
    },
    {
        "title": "PARAMANU-GANITA: Language Model with Mathematical Capabilities",
        "authors": "Mitodru NiyogiArnab Bhattacharya",
        "links": "http://arxiv.org/abs/2404.14395v1",
        "entry_id": "http://arxiv.org/abs/2404.14395v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14395v1",
        "summary": "In this paper, we present Paramanu-Ganita, a 208 million parameter novel Auto\nRegressive (AR) decoder based language model on mathematics. The model is\npretrained from scratch at context size of 4096 on our curated mixed\nmathematical corpus. We evaluate our model on both perplexity metric and GSM8k\nmathematical benchmark. Paramanu-Ganita despite being 35 times smaller than 7B\nLLMs, outperformed generalist LLMs such as LLaMa-1 7B by 28.4% points, LLaMa-2\n7B by 27.6% points, Falcon 7B by 32.6% points, PaLM 8B by 35.3% points, and\nmath specialised LLMs such as Minerva 8B by 23.2% points, and LLEMMA-7B by 3.0%\npoints in GSM8k test accuracy metric respectively. Paramanu-Ganita also\noutperformed giant LLMs like PaLM 62B by 6.4% points, Falcon 40B by 19.8%\npoints, LLaMa-1 33B by 3.8% points and Vicuna 13B by 11.8% points respectively.\nThe large significant margin improvement in performance of our math model over\nthe existing LLMs signifies that reasoning capabilities of language model are\njust not restricted to LLMs with humongous number of parameters.\nParamanu-Ganita took 146 hours of A100 training whereas math specialised LLM,\nLLEMMA 7B, was trained for 23,000 A100 hours of training equivalent. Thus, our\napproach of pretraining powerful domain specialised language models from\nscratch for domain adaptation is much more cost-effective than performing\ncontinual training of LLMs for domain adaptation. Hence, we conclude that for\nstrong mathematical reasoning abilities of language model, we do not need giant\nLLMs and immense computing power to our end. In the end, we want to point out\nthat we have only trained Paramanu-Ganita only on a part of our entire\nmathematical corpus and yet to explore the full potential of our model.",
        "updated": "2024-04-22 17:55:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14395v1"
    },
    {
        "title": "Poisoning Attacks on Federated Learning-based Wireless Traffic Prediction",
        "authors": "Zifan ZhangMinghong FangJiayuan HuangYuchen Liu",
        "links": "http://arxiv.org/abs/2404.14389v1",
        "entry_id": "http://arxiv.org/abs/2404.14389v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14389v1",
        "summary": "Federated Learning (FL) offers a distributed framework to train a global\ncontrol model across multiple base stations without compromising the privacy of\ntheir local network data. This makes it ideal for applications like wireless\ntraffic prediction (WTP), which plays a crucial role in optimizing network\nresources, enabling proactive traffic flow management, and enhancing the\nreliability of downstream communication-aided applications, such as IoT\ndevices, autonomous vehicles, and industrial automation systems. Despite its\npromise, the security aspects of FL-based distributed wireless systems,\nparticularly in regression-based WTP problems, remain inadequately\ninvestigated. In this paper, we introduce a novel fake traffic injection (FTI)\nattack, designed to undermine the FL-based WTP system by injecting fabricated\ntraffic distributions with minimal knowledge. We further propose a defense\nmechanism, termed global-local inconsistency detection (GLID), which\nstrategically removes abnormal model parameters that deviate beyond a specific\npercentile range estimated through statistical methods in each dimension.\nExtensive experimental evaluations, performed on real-world wireless traffic\ndatasets, demonstrate that both our attack and defense strategies significantly\noutperform existing baselines.",
        "updated": "2024-04-22 17:50:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14389v1"
    }
]