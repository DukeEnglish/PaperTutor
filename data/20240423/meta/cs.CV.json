[
    {
        "title": "AutoAD III: The Prequel -- Back to the Pixels",
        "authors": "Tengda HanMax BainArsha NagraniGül VarolWeidi XieAndrew Zisserman",
        "links": "http://arxiv.org/abs/2404.14412v1",
        "entry_id": "http://arxiv.org/abs/2404.14412v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14412v1",
        "summary": "Generating Audio Description (AD) for movies is a challenging task that\nrequires fine-grained visual understanding and an awareness of the characters\nand their names. Currently, visual language models for AD generation are\nlimited by a lack of suitable training data, and also their evaluation is\nhampered by using performance measures not specialized to the AD domain. In\nthis paper, we make three contributions: (i) We propose two approaches for\nconstructing AD datasets with aligned video data, and build training and\nevaluation datasets using these. These datasets will be publicly released; (ii)\nWe develop a Q-former-based architecture which ingests raw video and generates\nAD, using frozen pre-trained visual encoders and large language models; and\n(iii) We provide new evaluation metrics to benchmark AD quality that are\nwell-matched to human performance. Taken together, we improve the state of the\nart on AD generation.",
        "updated": "2024-04-22 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14412v1"
    },
    {
        "title": "Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses",
        "authors": "Inhee LeeByungjun KimHanbyul Joo",
        "links": "http://arxiv.org/abs/2404.14410v1",
        "entry_id": "http://arxiv.org/abs/2404.14410v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14410v1",
        "summary": "In this paper, we present a method to reconstruct the world and multiple\ndynamic humans in 3D from a monocular video input. As a key idea, we represent\nboth the world and multiple humans via the recently emerging 3D Gaussian\nSplatting (3D-GS) representation, enabling to conveniently and efficiently\ncompose and render them together. In particular, we address the scenarios with\nseverely limited and sparse observations in 3D human reconstruction, a common\nchallenge encountered in the real world. To tackle this challenge, we introduce\na novel approach to optimize the 3D-GS representation in a canonical space by\nfusing the sparse cues in the common space, where we leverage a pre-trained 2D\ndiffusion model to synthesize unseen views while keeping the consistency with\nthe observed 2D appearances. We demonstrate our method can reconstruct\nhigh-quality animatable 3D humans in various challenging examples, in the\npresence of occlusion, image crops, few-shot, and extremely sparse\nobservations. After reconstruction, our method is capable of not only rendering\nthe scene in any novel views at arbitrary time instances, but also editing the\n3D scene by removing individual humans or applying different motions for each\nhuman. Through various experiments, we demonstrate the quality and efficiency\nof our methods over alternative existing approaches.",
        "updated": "2024-04-22 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14410v1"
    },
    {
        "title": "CrossScore: Towards Multi-View Image Evaluation and Scoring",
        "authors": "Zirui WangWenjing BianOmkar ParkhiYuheng RenVictor Adrian Prisacariu",
        "links": "http://arxiv.org/abs/2404.14409v1",
        "entry_id": "http://arxiv.org/abs/2404.14409v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14409v1",
        "summary": "We introduce a novel cross-reference image quality assessment method that\neffectively fills the gap in the image assessment landscape, complementing the\narray of established evaluation schemes -- ranging from full-reference metrics\nlike SSIM, no-reference metrics such as NIQE, to general-reference metrics\nincluding FID, and Multi-modal-reference metrics, e.g., CLIPScore. Utilising a\nneural network with the cross-attention mechanism and a unique data collection\npipeline from NVS optimisation, our method enables accurate image quality\nassessment without requiring ground truth references. By comparing a query\nimage against multiple views of the same scene, our method addresses the\nlimitations of existing metrics in novel view synthesis (NVS) and similar tasks\nwhere direct reference images are unavailable. Experimental results show that\nour method is closely correlated to the full-reference metric SSIM, while not\nrequiring ground truth references.",
        "updated": "2024-04-22 17:59:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14409v1"
    },
    {
        "title": "Hyp-OC: Hyperbolic One Class Classification for Face Anti-Spoofing",
        "authors": "Kartik NarayanVishal M. Patel",
        "links": "http://arxiv.org/abs/2404.14406v1",
        "entry_id": "http://arxiv.org/abs/2404.14406v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14406v1",
        "summary": "Face recognition technology has become an integral part of modern security\nsystems and user authentication processes. However, these systems are\nvulnerable to spoofing attacks and can easily be circumvented. Most prior\nresearch in face anti-spoofing (FAS) approaches it as a two-class\nclassification task where models are trained on real samples and known spoof\nattacks and tested for detection performance on unknown spoof attacks. However,\nin practice, FAS should be treated as a one-class classification task where,\nwhile training, one cannot assume any knowledge regarding the spoof samples a\npriori. In this paper, we reformulate the face anti-spoofing task from a\none-class perspective and propose a novel hyperbolic one-class classification\nframework. To train our network, we use a pseudo-negative class sampled from\nthe Gaussian distribution with a weighted running mean and propose two novel\nloss functions: (1) Hyp-PC: Hyperbolic Pairwise Confusion loss, and (2) Hyp-CE:\nHyperbolic Cross Entropy loss, which operate in the hyperbolic space.\nAdditionally, we employ Euclidean feature clipping and gradient clipping to\nstabilize the training in the hyperbolic space. To the best of our knowledge,\nthis is the first work extending hyperbolic embeddings for face anti-spoofing\nin a one-class manner. With extensive experiments on five benchmark datasets:\nRose-Youtu, MSU-MFSD, CASIA-MFSD, Idiap Replay-Attack, and OULU-NPU, we\ndemonstrate that our method significantly outperforms the state-of-the-art,\nachieving better spoof detection performance.",
        "updated": "2024-04-22 17:59:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14406v1"
    },
    {
        "title": "GeoDiffuser: Geometry-Based Image Editing with Diffusion Models",
        "authors": "Rahul SajnaniJeroen VanbaarJie MinKapil KatyalSrinath Sridhar",
        "links": "http://arxiv.org/abs/2404.14403v1",
        "entry_id": "http://arxiv.org/abs/2404.14403v1",
        "pdf_url": "http://arxiv.org/pdf/2404.14403v1",
        "summary": "The success of image generative models has enabled us to build methods that\ncan edit images based on text or other user input. However, these methods are\nbespoke, imprecise, require additional information, or are limited to only 2D\nimage edits. We present GeoDiffuser, a zero-shot optimization-based method that\nunifies common 2D and 3D image-based object editing capabilities into a single\nmethod. Our key insight is to view image editing operations as geometric\ntransformations. We show that these transformations can be directly\nincorporated into the attention layers in diffusion models to implicitly\nperform editing operations. Our training-free optimization method uses an\nobjective function that seeks to preserve object style but generate plausible\nimages, for instance with accurate lighting and shadows. It also inpaints\ndisoccluded parts of the image where the object was originally located. Given a\nnatural image and user input, we segment the foreground object using SAM and\nestimate a corresponding transform which is used by our optimization approach\nfor editing. GeoDiffuser can perform common 2D and 3D edits like object\ntranslation, 3D rotation, and removal. We present quantitative results,\nincluding a perceptual study, that shows how our approach is better than\nexisting methods. Visit https://ivl.cs.brown.edu/research/geodiffuser.html for\nmore information.",
        "updated": "2024-04-22 17:58:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.14403v1"
    }
]