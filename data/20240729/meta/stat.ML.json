[
    {
        "title": "Hybrid summary statistics: neural weak lensing inference beyond the power spectrum",
        "authors": "T. Lucas MakinenTom CharnockNatalia PorqueresAxel LapelAlan HeavensBenjamin D. Wandelt",
        "links": "http://arxiv.org/abs/2407.18909v1",
        "entry_id": "http://arxiv.org/abs/2407.18909v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18909v1",
        "summary": "In inference problems, we often have domain knowledge which allows us to\ndefine summary statistics that capture most of the information content in a\ndataset. In this paper, we present a hybrid approach, where such physics-based\nsummaries are augmented by a set of compressed neural summary statistics that\nare optimised to extract the extra information that is not captured by the\npredefined summaries. The resulting statistics are very powerful inputs to\nsimulation-based or implicit inference of model parameters. We apply this\ngeneralisation of Information Maximising Neural Networks (IMNNs) to parameter\nconstraints from tomographic weak gravitational lensing convergence maps to\nfind summary statistics that are explicitly optimised to complement angular\npower spectrum estimates. We study several dark matter simulation resolutions\nin low- and high-noise regimes. We show that i) the information-update\nformalism extracts at least $3\\times$ and up to $8\\times$ as much information\nas the angular power spectrum in all noise regimes, ii) the network summaries\nare highly complementary to existing 2-point summaries, and iii) our formalism\nallows for networks with smaller, physically-informed architectures to match\nmuch larger regression networks with far fewer simulations needed to obtain\nasymptotically optimal inference.",
        "updated": "2024-07-26 17:59:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18909v1"
    },
    {
        "title": "Learning Chaotic Systems and Long-Term Predictions with Neural Jump ODEs",
        "authors": "Florian KrachJosef Teichmann",
        "links": "http://arxiv.org/abs/2407.18808v1",
        "entry_id": "http://arxiv.org/abs/2407.18808v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18808v1",
        "summary": "The Path-dependent Neural Jump ODE (PD-NJ-ODE) is a model for online\nprediction of generic (possibly non-Markovian) stochastic processes with\nirregular (in time) and potentially incomplete (with respect to coordinates)\nobservations. It is a model for which convergence to the $L^2$-optimal\npredictor, which is given by the conditional expectation, is established\ntheoretically. Thereby, the training of the model is solely based on a dataset\nof realizations of the underlying stochastic process, without the need of\nknowledge of the law of the process. In the case where the underlying process\nis deterministic, the conditional expectation coincides with the process\nitself. Therefore, this framework can equivalently be used to learn the\ndynamics of ODE or PDE systems solely from realizations of the dynamical system\nwith different initial conditions. We showcase the potential of our method by\napplying it to the chaotic system of a double pendulum. When training the\nstandard PD-NJ-ODE method, we see that the prediction starts to diverge from\nthe true path after about half of the evaluation time. In this work we enhance\nthe model with two novel ideas, which independently of each other improve the\nperformance of our modelling setup. The resulting dynamics match the true\ndynamics of the chaotic system very closely. The same enhancements can be used\nto provably enable the PD-NJ-ODE to learn long-term predictions for general\nstochastic datasets, where the standard model fails. This is verified in\nseveral experiments.",
        "updated": "2024-07-26 15:18:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18808v1"
    },
    {
        "title": "Log-Concave Coupling for Sampling Neural Net Posteriors",
        "authors": "Curtis McDonaldAndrew R Barron",
        "links": "http://arxiv.org/abs/2407.18802v1",
        "entry_id": "http://arxiv.org/abs/2407.18802v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18802v1",
        "summary": "In this work, we present a sampling algorithm for single hidden layer neural\nnetworks. This algorithm is built upon a recursive series of Bayesian\nposteriors using a method we call Greedy Bayes. Sampling of the Bayesian\nposterior for neuron weight vectors $w$ of dimension $d$ is challenging because\nof its multimodality. Our algorithm to tackle this problem is based on a\ncoupling of the posterior density for $w$ with an auxiliary random variable\n$\\xi$.\n  The resulting reverse conditional $w|\\xi$ of neuron weights given auxiliary\nrandom variable is shown to be log concave. In the construction of the\nposterior distributions we provide some freedom in the choice of the prior. In\nparticular, for Gaussian priors on $w$ with suitably small variance, the\nresulting marginal density of the auxiliary variable $\\xi$ is proven to be\nstrictly log concave for all dimensions $d$. For a uniform prior on the unit\n$\\ell_1$ ball, evidence is given that the density of $\\xi$ is again strictly\nlog concave for sufficiently large $d$.\n  The score of the marginal density of the auxiliary random variable $\\xi$ is\ndetermined by an expectation over $w|\\xi$ and thus can be computed by various\nrapidly mixing Markov Chain Monte Carlo methods. Moreover, the computation of\nthe score of $\\xi$ permits methods of sampling $\\xi$ by a stochastic diffusion\n(Langevin dynamics) with drift function built from this score. With such\ndynamics, information-theoretic methods pioneered by Bakry and Emery show that\naccurate sampling of $\\xi$ is obtained rapidly when its density is indeed\nstrictly log-concave. After which, one more draw from $w|\\xi$, produces neuron\nweights $w$ whose marginal distribution is from the desired posterior.",
        "updated": "2024-07-26 15:05:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18802v1"
    },
    {
        "title": "Score matching through the roof: linear, nonlinear, and latent variables causal discovery",
        "authors": "Francesco MontagnaPhilipp M. FallerPatrick BloebaumElke KirschbaumFrancesco Locatello",
        "links": "http://arxiv.org/abs/2407.18755v1",
        "entry_id": "http://arxiv.org/abs/2407.18755v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18755v1",
        "summary": "Causal discovery from observational data holds great promise, but existing\nmethods rely on strong assumptions about the underlying causal structure, often\nrequiring full observability of all relevant variables. We tackle these\nchallenges by leveraging the score function $\\nabla \\log p(X)$ of observed\nvariables for causal discovery and propose the following contributions. First,\nwe generalize the existing results of identifiability with the score to\nadditive noise models with minimal requirements on the causal mechanisms.\nSecond, we establish conditions for inferring causal relations from the score\neven in the presence of hidden variables; this result is two-faced: we\ndemonstrate the score's potential as an alternative to conditional independence\ntests to infer the equivalence class of causal graphs with hidden variables,\nand we provide the necessary conditions for identifying direct causes in latent\nvariable models. Building on these insights, we propose a flexible algorithm\nfor causal discovery across linear, nonlinear, and latent variable models,\nwhich we empirically validate.",
        "updated": "2024-07-26 14:09:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18755v1"
    },
    {
        "title": "Finite Neural Networks as Mixtures of Gaussian Processes: From Provable Error Bounds to Prior Selection",
        "authors": "Steven AdamsPatanèMorteza LahijanianLuca Laurenti",
        "links": "http://arxiv.org/abs/2407.18707v1",
        "entry_id": "http://arxiv.org/abs/2407.18707v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18707v1",
        "summary": "Infinitely wide or deep neural networks (NNs) with independent and\nidentically distributed (i.i.d.) parameters have been shown to be equivalent to\nGaussian processes. Because of the favorable properties of Gaussian processes,\nthis equivalence is commonly employed to analyze neural networks and has led to\nvarious breakthroughs over the years. However, neural networks and Gaussian\nprocesses are equivalent only in the limit; in the finite case there are\ncurrently no methods available to approximate a trained neural network with a\nGaussian model with bounds on the approximation error. In this work, we present\nan algorithmic framework to approximate a neural network of finite width and\ndepth, and with not necessarily i.i.d. parameters, with a mixture of Gaussian\nprocesses with error bounds on the approximation error. In particular, we\nconsider the Wasserstein distance to quantify the closeness between\nprobabilistic models and, by relying on tools from optimal transport and\nGaussian processes, we iteratively approximate the output distribution of each\nlayer of the neural network as a mixture of Gaussian processes. Crucially, for\nany NN and $\\epsilon >0$ our approach is able to return a mixture of Gaussian\nprocesses that is $\\epsilon$-close to the NN at a finite set of input points.\nFurthermore, we rely on the differentiability of the resulting error bound to\nshow how our approach can be employed to tune the parameters of a NN to mimic\nthe functional behavior of a given Gaussian process, e.g., for prior selection\nin the context of Bayesian inference. We empirically investigate the\neffectiveness of our results on both regression and classification problems\nwith various neural network architectures. Our experiments highlight how our\nresults can represent an important step towards understanding neural network\npredictions and formally quantifying their uncertainty.",
        "updated": "2024-07-26 12:45:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18707v1"
    }
]