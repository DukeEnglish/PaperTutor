[
    {
        "title": "SOAP-RL: Sequential Option Advantage Propagation for Reinforcement Learning in POMDP Environments",
        "authors": "Shu IshidaJoão F. Henriques",
        "links": "http://arxiv.org/abs/2407.18913v1",
        "entry_id": "http://arxiv.org/abs/2407.18913v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18913v1",
        "summary": "This work compares ways of extending Reinforcement Learning algorithms to\nPartially Observed Markov Decision Processes (POMDPs) with options. One view of\noptions is as temporally extended action, which can be realized as a memory\nthat allows the agent to retain historical information beyond the policy's\ncontext window. While option assignment could be handled using heuristics and\nhand-crafted objectives, learning temporally consistent options and associated\nsub-policies without explicit supervision is a challenge. Two algorithms, PPOEM\nand SOAP, are proposed and studied in depth to address this problem. PPOEM\napplies the forward-backward algorithm (for Hidden Markov Models) to optimize\nthe expected returns for an option-augmented policy. However, this learning\napproach is unstable during on-policy rollouts. It is also unsuited for\nlearning causal policies without the knowledge of future trajectories, since\noption assignments are optimized for offline sequences where the entire episode\nis available. As an alternative approach, SOAP evaluates the policy gradient\nfor an optimal option assignment. It extends the concept of the generalized\nadvantage estimation (GAE) to propagate option advantages through time, which\nis an analytical equivalent to performing temporal back-propagation of option\npolicy gradients. This option policy is only conditional on the history of the\nagent, not future actions. Evaluated against competing baselines, SOAP\nexhibited the most robust performance, correctly discovering options for POMDP\ncorridor environments, as well as on standard benchmarks including Atari and\nMuJoCo, outperforming PPOEM, as well as LSTM and Option-Critic baselines. The\nopen-sourced code is available at https://github.com/shuishida/SoapRL.",
        "updated": "2024-07-26 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18913v1"
    },
    {
        "title": "A Scalable Quantum Non-local Neural Network for Image Classification",
        "authors": "Sparsh GuptaDebanjan KonarVaneet Aggarwal",
        "links": "http://arxiv.org/abs/2407.18906v1",
        "entry_id": "http://arxiv.org/abs/2407.18906v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18906v1",
        "summary": "Non-local operations play a crucial role in computer vision enabling the\ncapture of long-range dependencies through weighted sums of features across the\ninput, surpassing the constraints of traditional convolution operations that\nfocus solely on local neighborhoods. Non-local operations typically require\ncomputing pairwise relationships between all elements in a set, leading to\nquadratic complexity in terms of time and memory. Due to the high computational\nand memory demands, scaling non-local neural networks to large-scale problems\ncan be challenging. This article introduces a hybrid quantum-classical scalable\nnon-local neural network, referred to as Quantum Non-Local Neural Network\n(QNL-Net), to enhance pattern recognition. The proposed QNL-Net relies on\ninherent quantum parallelism to allow the simultaneous processing of a large\nnumber of input features enabling more efficient computations in\nquantum-enhanced feature space and involving pairwise relationships through\nquantum entanglement. We benchmark our proposed QNL-Net with other quantum\ncounterparts to binary classification with datasets MNIST and CIFAR-10. The\nsimulation findings showcase our QNL-Net achieves cutting-edge accuracy levels\nin binary image classification among quantum classifiers while utilizing fewer\nqubits.",
        "updated": "2024-07-26 17:58:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18906v1"
    },
    {
        "title": "Lessons from Learning to Spin \"Pens\"",
        "authors": "Jun WangYing YuanHaichuan CheHaozhi QiYi MaJitendra MalikXiaolong Wang",
        "links": "http://arxiv.org/abs/2407.18902v1",
        "entry_id": "http://arxiv.org/abs/2407.18902v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18902v1",
        "summary": "In-hand manipulation of pen-like objects is an important skill in our daily\nlives, as many tools such as hammers and screwdrivers are similarly shaped.\nHowever, current learning-based methods struggle with this task due to a lack\nof high-quality demonstrations and the significant gap between simulation and\nthe real world. In this work, we push the boundaries of learning-based in-hand\nmanipulation systems by demonstrating the capability to spin pen-like objects.\nWe first use reinforcement learning to train an oracle policy with privileged\ninformation and generate a high-fidelity trajectory dataset in simulation. This\nserves two purposes: 1) pre-training a sensorimotor policy in simulation; 2)\nconducting open-loop trajectory replay in the real world. We then fine-tune the\nsensorimotor policy using these real-world trajectories to adapt it to the real\nworld dynamics. With less than 50 trajectories, our policy learns to rotate\nmore than ten pen-like objects with different physical properties for multiple\nrevolutions. We present a comprehensive analysis of our design choices and\nshare the lessons learned during development.",
        "updated": "2024-07-26 17:56:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18902v1"
    },
    {
        "title": "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents",
        "authors": "Harsh TrivediTushar KhotMareike HartmannRuskin MankuVinty DongEdward LiShashank GuptaAshish SabharwalNiranjan Balasubramanian",
        "links": "http://arxiv.org/abs/2407.18901v1",
        "entry_id": "http://arxiv.org/abs/2407.18901v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18901v1",
        "summary": "Autonomous agents that address day-to-day digital tasks (e.g., ordering\ngroceries for a household), must not only operate multiple apps (e.g., notes,\nmessaging, shopping app) via APIs, but also generate rich code with complex\ncontrol flow in an iterative manner based on their interaction with the\nenvironment. However, existing benchmarks for tool use are inadequate, as they\nonly cover tasks that require a simple sequence of API calls.\n  To remedy this gap, we built $\\textbf{AppWorld Engine}$, a high-quality\nexecution environment (60K lines of code) of 9 day-to-day apps operable via 457\nAPIs and populated with realistic digital activities simulating the lives of\n~100 fictitious users. We then created $\\textbf{AppWorld Benchmark}$ (40K lines\nof code), a suite of 750 natural, diverse, and challenging autonomous agent\ntasks requiring rich and interactive code generation. It supports robust\nprogrammatic evaluation with state-based unit tests, allowing for different\nways of completing a task while also checking for unexpected changes, i.e.,\ncollateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our\n'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least\n16% fewer. This highlights the benchmark's difficulty and AppWorld's potential\nto push the frontiers of interactive coding agents. The project website is\navailable at https://appworld.dev/.",
        "updated": "2024-07-26 17:55:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18901v1"
    },
    {
        "title": "Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence",
        "authors": "Mengyao LyuTianxiang HaoXinhao XuHui ChenZijia LinJungong HanGuiguang Ding",
        "links": "http://arxiv.org/abs/2407.18899v1",
        "entry_id": "http://arxiv.org/abs/2407.18899v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18899v1",
        "summary": "Domain Adaptation (DA) facilitates knowledge transfer from a source domain to\na related target domain. This paper investigates a practical DA paradigm,\nnamely Source data-Free Active Domain Adaptation (SFADA), where source data\nbecomes inaccessible during adaptation, and a minimum amount of annotation\nbudget is available in the target domain. Without referencing the source data,\nnew challenges emerge in identifying the most informative target samples for\nlabeling, establishing cross-domain alignment during adaptation, and ensuring\ncontinuous performance improvements through the iterative query-and-adaptation\nprocess. In response, we present learn from the learnt (LFTL), a novel paradigm\nfor SFADA to leverage the learnt knowledge from the source pretrained model and\nactively iterated models without extra overhead. We propose Contrastive Active\nSampling to learn from the hypotheses of the preceding model, thereby querying\ntarget samples that are both informative to the current model and persistently\nchallenging throughout active learning. During adaptation, we learn from\nfeatures of actively selected anchors obtained from previous intermediate\nmodels, so that the Visual Persistence-guided Adaptation can facilitate feature\ndistribution alignment and active sample exploitation. Extensive experiments on\nthree widely-used benchmarks show that our LFTL achieves state-of-the-art\nperformance, superior computational efficiency and continuous improvements as\nthe annotation budget increases. Our code is available at\nhttps://github.com/lyumengyao/lftl.",
        "updated": "2024-07-26 17:51:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18899v1"
    }
]