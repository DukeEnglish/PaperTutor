[
    {
        "title": "Floating No More: Object-Ground Reconstruction from a Single Image",
        "authors": "Yunze ManYichen ShengJianming ZhangLiang-Yan GuiYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2407.18914v1",
        "entry_id": "http://arxiv.org/abs/2407.18914v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18914v1",
        "summary": "Recent advancements in 3D object reconstruction from single images have\nprimarily focused on improving the accuracy of object shapes. Yet, these\ntechniques often fail to accurately capture the inter-relation between the\nobject, ground, and camera. As a result, the reconstructed objects often appear\nfloating or tilted when placed on flat surfaces. This limitation significantly\naffects 3D-aware image editing applications like shadow rendering and object\npose manipulation. To address this issue, we introduce ORG (Object\nReconstruction with Ground), a novel task aimed at reconstructing 3D object\ngeometry in conjunction with the ground surface. Our method uses two compact\npixel-level representations to depict the relationship between camera, object,\nand ground. Experiments show that the proposed ORG model can effectively\nreconstruct object-ground geometry on unseen data, significantly enhancing the\nquality of shadow generation and pose manipulation compared to conventional\nsingle-image 3D reconstruction techniques.",
        "updated": "2024-07-26 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18914v1"
    },
    {
        "title": "HRP: Human Affordances for Robotic Pre-Training",
        "authors": "Mohan Kumar SriramaSudeep DasariShikhar BahlAbhinav Gupta",
        "links": "http://arxiv.org/abs/2407.18911v1",
        "entry_id": "http://arxiv.org/abs/2407.18911v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18911v1",
        "summary": "In order to *generalize* to various tasks in the wild, robotic agents will\nneed a suitable representation (i.e., vision network) that enables the robot to\npredict optimal actions given high dimensional vision inputs. However, learning\nsuch a representation requires an extreme amount of diverse training data,\nwhich is prohibitively expensive to collect on a real robot. How can we\novercome this problem? Instead of collecting more robot data, this paper\nproposes using internet-scale, human videos to extract \"affordances,\" both at\nthe environment and agent level, and distill them into a pre-trained\nrepresentation. We present a simple framework for pre-training representations\non hand, object, and contact \"affordance labels\" that highlight relevant\nobjects in images and how to interact with them. These affordances are\nautomatically extracted from human video data (with the help of off-the-shelf\ncomputer vision modules) and used to fine-tune existing representations. Our\napproach can efficiently fine-tune *any* existing representation, and results\nin models with stronger downstream robotic performance across the board. We\nexperimentally demonstrate (using 3000+ robot trials) that this affordance\npre-training scheme boosts performance by a minimum of 15% on 5 real-world\ntasks, which consider three diverse robot morphologies (including a dexterous\nhand). Unlike prior works in the space, these representations improve\nperformance across 3 different camera views. Quantitatively, we find that our\napproach leads to higher levels of generalization in out-of-distribution\nsettings. For code, weights, and data check: https://hrp-robot.github.io",
        "updated": "2024-07-26 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18911v1"
    },
    {
        "title": "Wolf: Captioning Everything with a World Summarization Framework",
        "authors": "Boyi LiLigeng ZhuRan TianShuhan TanYuxiao ChenYao LuYin CuiSushant VeerMax EhrlichJonah PhilionXinshuo WengFuzhao XueAndrew TaoMing-Yu LiuSanja FidlerBoris IvanovicTrevor DarrellJitendra MalikSong HanMarco Pavone",
        "links": "http://arxiv.org/abs/2407.18908v1",
        "entry_id": "http://arxiv.org/abs/2407.18908v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18908v1",
        "summary": "We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.",
        "updated": "2024-07-26 17:59:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18908v1"
    },
    {
        "title": "SHIC: Shape-Image Correspondences with no Keypoint Supervision",
        "authors": "Aleksandar ShtedritskiChristian RupprechtAndrea Vedaldi",
        "links": "http://arxiv.org/abs/2407.18907v1",
        "entry_id": "http://arxiv.org/abs/2407.18907v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18907v1",
        "summary": "Canonical surface mapping generalizes keypoint detection by assigning each\npixel of an object to a corresponding point in a 3D template. Popularised by\nDensePose for the analysis of humans, authors have since attempted to apply the\nconcept to more categories, but with limited success due to the high cost of\nmanual supervision. In this work, we introduce SHIC, a method to learn\ncanonical maps without manual supervision which achieves better results than\nsupervised methods for most categories. Our idea is to leverage foundation\ncomputer vision models such as DINO and Stable Diffusion that are open-ended\nand thus possess excellent priors over natural categories. SHIC reduces the\nproblem of estimating image-to-template correspondences to predicting\nimage-to-image correspondences using features from the foundation models. The\nreduction works by matching images of the object to non-photorealistic renders\nof the template, which emulates the process of collecting manual annotations\nfor this task. These correspondences are then used to supervise high-quality\ncanonical maps for any object of interest. We also show that image generators\ncan further improve the realism of the template views, which provide an\nadditional source of supervision for the model.",
        "updated": "2024-07-26 17:58:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18907v1"
    },
    {
        "title": "A Scalable Quantum Non-local Neural Network for Image Classification",
        "authors": "Sparsh GuptaDebanjan KonarVaneet Aggarwal",
        "links": "http://arxiv.org/abs/2407.18906v1",
        "entry_id": "http://arxiv.org/abs/2407.18906v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18906v1",
        "summary": "Non-local operations play a crucial role in computer vision enabling the\ncapture of long-range dependencies through weighted sums of features across the\ninput, surpassing the constraints of traditional convolution operations that\nfocus solely on local neighborhoods. Non-local operations typically require\ncomputing pairwise relationships between all elements in a set, leading to\nquadratic complexity in terms of time and memory. Due to the high computational\nand memory demands, scaling non-local neural networks to large-scale problems\ncan be challenging. This article introduces a hybrid quantum-classical scalable\nnon-local neural network, referred to as Quantum Non-Local Neural Network\n(QNL-Net), to enhance pattern recognition. The proposed QNL-Net relies on\ninherent quantum parallelism to allow the simultaneous processing of a large\nnumber of input features enabling more efficient computations in\nquantum-enhanced feature space and involving pairwise relationships through\nquantum entanglement. We benchmark our proposed QNL-Net with other quantum\ncounterparts to binary classification with datasets MNIST and CIFAR-10. The\nsimulation findings showcase our QNL-Net achieves cutting-edge accuracy levels\nin binary image classification among quantum classifiers while utilizing fewer\nqubits.",
        "updated": "2024-07-26 17:58:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18906v1"
    }
]