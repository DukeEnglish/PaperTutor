[
    {
        "title": "SOAP-RL: Sequential Option Advantage Propagation for Reinforcement Learning in POMDP Environments",
        "authors": "Shu IshidaJoão F. Henriques",
        "links": "http://arxiv.org/abs/2407.18913v1",
        "entry_id": "http://arxiv.org/abs/2407.18913v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18913v1",
        "summary": "This work compares ways of extending Reinforcement Learning algorithms to\nPartially Observed Markov Decision Processes (POMDPs) with options. One view of\noptions is as temporally extended action, which can be realized as a memory\nthat allows the agent to retain historical information beyond the policy's\ncontext window. While option assignment could be handled using heuristics and\nhand-crafted objectives, learning temporally consistent options and associated\nsub-policies without explicit supervision is a challenge. Two algorithms, PPOEM\nand SOAP, are proposed and studied in depth to address this problem. PPOEM\napplies the forward-backward algorithm (for Hidden Markov Models) to optimize\nthe expected returns for an option-augmented policy. However, this learning\napproach is unstable during on-policy rollouts. It is also unsuited for\nlearning causal policies without the knowledge of future trajectories, since\noption assignments are optimized for offline sequences where the entire episode\nis available. As an alternative approach, SOAP evaluates the policy gradient\nfor an optimal option assignment. It extends the concept of the generalized\nadvantage estimation (GAE) to propagate option advantages through time, which\nis an analytical equivalent to performing temporal back-propagation of option\npolicy gradients. This option policy is only conditional on the history of the\nagent, not future actions. Evaluated against competing baselines, SOAP\nexhibited the most robust performance, correctly discovering options for POMDP\ncorridor environments, as well as on standard benchmarks including Atari and\nMuJoCo, outperforming PPOEM, as well as LSTM and Option-Critic baselines. The\nopen-sourced code is available at https://github.com/shuishida/SoapRL.",
        "updated": "2024-07-26 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18913v1"
    },
    {
        "title": "Do We Really Need Graph Convolution During Training? Light Post-Training Graph-ODE for Efficient Recommendation",
        "authors": "Weizhi ZhangLiangwei YangZihe SongHenry Peng ZouKe XuHenry Peng ZouLiancheng FangPhilip S. Yu",
        "links": "http://arxiv.org/abs/2407.18910v1",
        "entry_id": "http://arxiv.org/abs/2407.18910v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18910v1",
        "summary": "The efficiency and scalability of graph convolution networks (GCNs) in\ntraining recommender systems (RecSys) have been persistent concerns, hindering\ntheir deployment in real-world applications. This paper presents a critical\nexamination of the necessity of graph convolutions during the training phase\nand introduces an innovative alternative: the Light Post-Training Graph\nOrdinary-Differential-Equation (LightGODE). Our investigation reveals that the\nbenefits of GCNs are more pronounced during testing rather than training.\nMotivated by this, LightGODE utilizes a novel post-training graph convolution\nmethod that bypasses the computation-intensive message passing of GCNs and\nemploys a non-parametric continuous graph ordinary-differential-equation (ODE)\nto dynamically model node representations. This approach drastically reduces\ntraining time while achieving fine-grained post-training graph convolution to\navoid the distortion of the original training embedding space, termed the\nembedding discrepancy issue. We validate our model across several real-world\ndatasets of different scales, demonstrating that LightGODE not only outperforms\nGCN-based models in terms of efficiency and effectiveness but also\nsignificantly mitigates the embedding discrepancy commonly associated with\ndeeper graph convolution layers. Our LightGODE challenges the prevailing\nparadigms in RecSys training and suggests re-evaluating the role of graph\nconvolutions, potentially guiding future developments of efficient large-scale\ngraph-based RecSys.",
        "updated": "2024-07-26 17:59:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18910v1"
    },
    {
        "title": "Hybrid summary statistics: neural weak lensing inference beyond the power spectrum",
        "authors": "T. Lucas MakinenTom CharnockNatalia PorqueresAxel LapelAlan HeavensBenjamin D. Wandelt",
        "links": "http://arxiv.org/abs/2407.18909v1",
        "entry_id": "http://arxiv.org/abs/2407.18909v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18909v1",
        "summary": "In inference problems, we often have domain knowledge which allows us to\ndefine summary statistics that capture most of the information content in a\ndataset. In this paper, we present a hybrid approach, where such physics-based\nsummaries are augmented by a set of compressed neural summary statistics that\nare optimised to extract the extra information that is not captured by the\npredefined summaries. The resulting statistics are very powerful inputs to\nsimulation-based or implicit inference of model parameters. We apply this\ngeneralisation of Information Maximising Neural Networks (IMNNs) to parameter\nconstraints from tomographic weak gravitational lensing convergence maps to\nfind summary statistics that are explicitly optimised to complement angular\npower spectrum estimates. We study several dark matter simulation resolutions\nin low- and high-noise regimes. We show that i) the information-update\nformalism extracts at least $3\\times$ and up to $8\\times$ as much information\nas the angular power spectrum in all noise regimes, ii) the network summaries\nare highly complementary to existing 2-point summaries, and iii) our formalism\nallows for networks with smaller, physically-informed architectures to match\nmuch larger regression networks with far fewer simulations needed to obtain\nasymptotically optimal inference.",
        "updated": "2024-07-26 17:59:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18909v1"
    },
    {
        "title": "Wolf: Captioning Everything with a World Summarization Framework",
        "authors": "Boyi LiLigeng ZhuRan TianShuhan TanYuxiao ChenYao LuYin CuiSushant VeerMax EhrlichJonah PhilionXinshuo WengFuzhao XueAndrew TaoMing-Yu LiuSanja FidlerBoris IvanovicTrevor DarrellJitendra MalikSong HanMarco Pavone",
        "links": "http://arxiv.org/abs/2407.18908v1",
        "entry_id": "http://arxiv.org/abs/2407.18908v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18908v1",
        "summary": "We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.",
        "updated": "2024-07-26 17:59:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18908v1"
    },
    {
        "title": "A Scalable Quantum Non-local Neural Network for Image Classification",
        "authors": "Sparsh GuptaDebanjan KonarVaneet Aggarwal",
        "links": "http://arxiv.org/abs/2407.18906v1",
        "entry_id": "http://arxiv.org/abs/2407.18906v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18906v1",
        "summary": "Non-local operations play a crucial role in computer vision enabling the\ncapture of long-range dependencies through weighted sums of features across the\ninput, surpassing the constraints of traditional convolution operations that\nfocus solely on local neighborhoods. Non-local operations typically require\ncomputing pairwise relationships between all elements in a set, leading to\nquadratic complexity in terms of time and memory. Due to the high computational\nand memory demands, scaling non-local neural networks to large-scale problems\ncan be challenging. This article introduces a hybrid quantum-classical scalable\nnon-local neural network, referred to as Quantum Non-Local Neural Network\n(QNL-Net), to enhance pattern recognition. The proposed QNL-Net relies on\ninherent quantum parallelism to allow the simultaneous processing of a large\nnumber of input features enabling more efficient computations in\nquantum-enhanced feature space and involving pairwise relationships through\nquantum entanglement. We benchmark our proposed QNL-Net with other quantum\ncounterparts to binary classification with datasets MNIST and CIFAR-10. The\nsimulation findings showcase our QNL-Net achieves cutting-edge accuracy levels\nin binary image classification among quantum classifiers while utilizing fewer\nqubits.",
        "updated": "2024-07-26 17:58:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18906v1"
    }
]