EMBEDDING AND CLUSTERING YOUR DATA CAN IMPROVE
CONTRASTIVE PRETRAINING
APREPRINT
LukeMerrick*
SnowflakeInc.
luke.merrick@snowflake.com
July29,2024
ABSTRACT
Recentstudiesoflarge-scalecontrastivepretraininginthetextembeddingdomainshowthatusing
single-sourceminibatches,ratherthanmixed-sourceminibatches,cansubstantiallyimproveoverall
modelaccuracy. Inthiswork,weexploreextendingtrainingdatastratificationbeyondsourcegranu-
laritybyleveragingapretrainedtextembeddingmodelandtheclassick-meansclusteringalgorithm
tofurthersplittrainingdataapartbythesemanticclusterswithineachsource. Experimentally,we
observeanotableincreaseinNDCG@10whenpretrainingaBERT-basedtextembeddingmodelon
query-passagepairsfromtheMSMARCOpassageretrievaldataset. Additionally,weconceptually
connect our clustering approach to both the Topic Aware Sampling (TAS) aspect of the TAS-B
methodologyandthenearest-neighbor-basedhard-negativeminingaspectoftheANCEmethodology
anddiscusshowthisunifiedviewmotivatesfuturelinesofresearchontheorganizationofcontrastive
pretrainingdata.
Keywords Contrastivelearning·Informationretrieval·Textembedding
1 Introduction
TheSnowflakeArcticEmbedreport[Merricketal.,2024]showedthatpretrainingwithstratifieddatasamplingthatfills
eachminibatchwithsamplesfromjustasinglesourcecanleadtobettermodelperformancethanthesimplerapproach
ofrandomlyshufflingalldatasourcestogetherintoasinglemixeddataset(seeFigure1,whichisreproducedfromthat
work). TheNomic-Embedreport[Nussbaumetal.,2024]alsocitesthissametrickaspartoftheirsuccessfulrecipefor
training,claimingitpreventsthemodelfrom“learningsource-specificshortcuts.”
ObservingtheempiricalretrievalqualityimprovementintheArcticEmbedreport’sablationstudy,twocomplimentary
researchobjectivesnaturallyarise:
1. Appliedobjective. Canweextendthesourcestratificationmethodtofurtherimprovelarge-scalecontrastive
pretraininglearningdynamics?
2. Theoreticalobjective.Canwedevelopadeepertheoreticalunderstandingofhowandwhysourcestratification
drivestheselearningimprovements?
Inpursuitofthesegoals,weconcoctandstudyasimpleextensionofsourcestratificationthatcreatesmoregranular
“semantic sub-sources” by clustering examples by their text embeddings. Empirically, our experiments using the
MSMARCOdataset[Bajajetal.,2018]showaclearimprovementfromthisextendedstratificationtechnique,while
theoretically,itmotivatesconceptualconnectiontopriorworksonTaskAwareSampling(TAS)[Hofstätteretal.,2021]
andnearest-neighbors-basedhardnegativemining[Xiongetal.,2020]. Thoughwefallshortofaconclusivetheoretical
explanation,weofferaninitialsynthesisofrelevanttheoreticalprinciplestoprovideacohesivepictureofimproved
∗https://lukemerrick.com
4202
luJ
62
]GL.sc[
1v78881.7042:viXraEmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
Figure 1: Figure 7 from [Merrick et al.,
2024],whichdepictstheNDCG@10score
ontheSciDocsdatasetduringtraining.This
experimentshowedthatstratifiedtraining
data(darkblueline)canleadtobetterqual-
ity than simply a large batch size (pur-
pleline), whileacombinationofbothap-
proaches(lightblueline)doesevenbetter
thaneachindividual. Italsoshowshowthe
stratifiedapproachimprovesthelong-term
trajectorymore,suggestinganelementof
curriculumlearningmaybeatplay.
contrastive pretraining at an intuitive level. Additionally, we believe our synthesized viewpoint offers insight into
promisingfuturelinesofrelatedresearch,whichwediscussatthecloseofthispaper.
2 Methodology
Ourmethodologyforextendingsourcestratificationtoamoregranularlevelisaratherstraightforwardapplicationof
clusteringandiscloselyrelatedtotheTopicAwareSampling(TAS)approachof[Hofstätteretal.,2021].
Ourclusteringreciperequirestwoinputobjects:
• Alarge-scalecontrastivepretrainingdatasetofN query-itempairs,i.e. D ={q ,p }N .
i i i=1
• Apretrainedtextembeddingmodelf mappingqueriesanditemstothesamevectorspace.
Usingtheseobjects,weapplythefollowingsteps:
• Usemodelf tocreateasetofN pairembeddings,eitherbyembeddingallthequeriesoralltheitems2,i.e.
Z ={f(q )}N orZ ={f(p )}N .
i i=1 i i=1
• Performk-meansclusteringtoassigneachembeddinginZ tooneofkclusters.
• ConstructdatasetsD ,D ,...,D bysplittingtheoriginalquery-itempairsofD intosub-datasetscorre-
1 2 k
spondingtotheclusterassignmentsoftheitems’embeddings.
• StratifyacrossD ,D ,...,D ,ratherthanD,whenconstructingsource-stratifiedminibatchesforlargescale
1 2 k
contrastivepretraining.
3 Experiments
Toassessthepracticalefficacyofourclusteringmethod,weadapttheMSMARCOpassagedataset[Bajajetal.,2018]
tothesettingoflarge-scalecontrastivepretrainingbydiscardingalllabelednegativeexamples,leavingonlyasetof
query-passagepairs. AlthoughMSMARCOcontainsover8millionpassages,thereareonlyabout0.5millionlabeled
query-passagepairsinthetrainingset,soourdatasetendsupbeingnotparticularly“largescale”.
AfterconvertingtheoriginalMSMARCOpassagedatasettoapretraining-compatibledataset,weperformembedding
andclusteringonqueriesandvectorsseparatelytocreatetwostratifiedtrainingdatasets. Wethenperformcontrastive
pretrainingontheoriginaldatasetandthetwostratifiedvariantsandevaluatethequalityoftheresultingmodels.
2Developingawaytocombinequeryanditemintoasingleembeddingvectorpresentsaninterestinglineforfutureworkaswell.
2EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
(a)Passageembeddingclusterstatistics. (b)Queryembeddingclusterstatistics.
Cluster Size Similarity Cluster Size Similarity
000 61,655 0.340 001 51,475 0.705
001 68,323 0.358 002 33,574 0.633
002 30,173 0.350 003 67,224 0.769
003 44,481 0.422 004 42,233 0.711
004 40,552 0.387 005 35,114 0.699
005 34,756 0.290 006 79,377 0.729
006 36,700 0.263 007 46,528 0.746
007 90,862 0.493 008 61,172 0.715
008 74,908 0.438 009 26,574 0.682
009 34,062 0.360 010 59,668 0.703
OverallDataset 0.305 OverallDataset 0.670
Table1: Clusterstatistics. Togivearoughapproximationofclusterdensity,wesample3,000pointsfromboththe
overalldatasetandeachclusterandcomputetheaveragepairwisecosinesimilaritywithineachcluster.
Figure2: Experimentaltraininglosscurves(rollingaverageof10stepsoverfadedoriginalvalues). Clusteringby
pseudo-sub-sourcesleadstosubstantiallyhigheraveragetrainingloss,aswellashighervariancestep-to-step.
3.1 ClusteringDetails
WebegintheclusteringprocessbyembeddingbothqueriesandpassagesusingtheArcticEmbedMmodel[Merrick
etal.,2024]. Wethenleveragethesphericalk-meansclusteringalgorithmimplementedintheFAISSlibrary[Douze
etal.,2024]withk =10clusters3toconstructourtwoclustereddatasets. ClusterstatisticsaregiveninTable1.
3.2 Training
Wetrainabase-sizedBERTmodel[Devlinetal.,2019]using[CLS]-token-basedembeddings. FromFigure2,we
seethatcomparedtotheun-clusteredbaseline,theclusteredtreatmentsleadtosubstantiallyhighertrainingloss(i.e.
increasednegativehardness). TrainingparametersaregiveninAppendixA.
3.3 Results
Weevaluateourtwopre-trainedmodelsonthefullMTEBRetrievalbenchmark[Muennighoffetal.,2023](which
includesthedevsplitofMSMARCOasoneofitsconstituentdatasets)andtabulatetheresultsinTable2. Weobserve
thatourk =10clusteringleadstoroughly2%improvementinNDCG@10scoreontheMSMARCOdevsplitboth
3Wearbitrarilyselectk = 10sinceitisaroundnumberintheballparkofthenumberofdifferentdatasourcesusedbythe
SnowflakeArcticEmbedandNomic-Embedprojects.
3EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
(a)RawNDCG@10scores. (b)NDCG@10scoresnormalizedtoshuffle.
Dataset Baseline DocCluster QueryCluster Dataset Baseline DocCluster QueryCluster
ArguAna 44.98% 45.72% 46.06% ArguAna 100.00% 101.65% 102.40%
CQADup. 30.53% 31.24% 31.60% CQADup. 100.00% 102.32% 103.50%
ClimateFEVER 20.35% 19.72% 19.37% ClimateFEVER 100.00% 96.90% 95.18%
DBPedia 31.49% 31.61% 32.41% DBPedia 100.00% 100.38% 102.92%
FEVER 66.02% 67.38% 64.72% FEVER 100.00% 102.06% 98.03%
FiQA2018 24.05% 25.15% 25.14% FiQA2018 100.00% 104.57% 104.53%
HotpotQA 50.79% 50.94% 50.45% HotpotQA 100.00% 100.30% 99.33%
MSMARCO 32.86% 33.58% 33.48% MSMARCO 100.00% 102.19% 101.89%
NFCorpus 28.30% 28.53% 28.62% NFCorpus 100.00% 100.81% 101.13%
NQ 37.04% 37.26% 38.30% NQ 100.00% 100.59% 103.40%
QuoraRetrieval 84.66% 84.78% 84.93% QuoraRetrieval 100.00% 100.14% 100.32%
SCIDOCS 14.05% 13.76% 13.79% SCIDOCS 100.00% 97.94% 98.15%
SciFact 57.70% 55.75% 56.74% SciFact 100.00% 96.62% 98.34%
TRECCOVID 48.40% 49.93% 47.19% TRECCOVID 100.00% 103.16% 97.50%
Touche2020 17.24% 17.20% 16.45% Touche2020 100.00% 99.77% 95.42%
Average 39.23% 39.50% 39.28% Average 100.00% 100.69% 100.13%
Table 2: Retrieval results on the MTEB Retrieval benchmark. Scores in NDCG@10. In-distribution dataset (MS-
MARCO)bolded.
whenclusteringbyqueryandbypassageembeddings,validatingourhypothesisthatfurtherstratificationwouldleadto
improvedperformance.
TheoverallMTEBRetrievalevaluationpaintsamorenuancedpicturethantheMSMARCOdevsplitscore,however.
Whilepassage-drivenclusteringisassociatedwithimprovementson11outofthe15datasets,itdemonstratessignificant
losses on ClimateFEVER, SCIDOCS, and SciFact. Interestingly, we see that the score degradation on these three
datasets,alongwiththestrongimprovementonFiQA,aremirroredwhentrainingwithclusteredstratificationdrivenby
queryembeddings,thoughperformanceelsewhereismoremixed. Afterexaminingsomeclusterexamplesmanually
(seeAppendixB,inparticular,passagecluster3),weconjecturethatclusteringledtosomeminibatchesmuchcloser
resemblingthedistributionoftheFiQAdatasetandthushelpingthemodellearntoperformwellinthatdomain.
Thoughtheseexperimentsonlyshowa0.7%increaseinMTEBRetrievalscoreusingthebetterofthetwovariations
considered, we contend that the utility of clustering may be more significant on larger datasets that can support a
largernumberofbigger-than-batch-sizeclusters. Althoughweleaveaformalablationstudyatthe100M+query-item
pairscaletofuturework,anecdotallywefindthatclusteringbypassageembeddingatthe100M+passagescaleinto
hundredsofclustersappearstobeassociatedwithscoreimprovementsashighasacouplepercentinlongpretraining
runs(e.g. ballpark48%NDCG@10onMTEBRetrievalinsteadoftheballpark47%publishedintheablationstudyof
[Merricketal.,2024]).
4 InSearchOfDeeperUnderstanding
Inthissectionweturnourattentionfromourappliedresearchobjectivetoourtheoreticalresearchobjective,connecting
ourempiricalobservationabovetotheconceptspresentedinseveralrelatedworks. Aftermakingtheseconnections,
wesynthesizetheideasfromthesedifferentsourcesintoaunifiedperspectiveonhowtomoreeffectivelyorganize
contrastivepretrainingdataintominibatches.
4.1 ConceptOne: TheClusteringHypothesisAndTopicalityAwareSampling
Topic Aware Sampling (TAS) – an approach consisting of clustering training data by query embedding and then
samplingeachtrainingminibatchfromasingleclustereach,wasoneofthekeydriversofsuccessforTAS-B[Hofstätter
etal.,2021],aholistictrainingrecipedesignedtoproduceaqualitytextembeddingmodelwithoutrequiringlarge-scale
computationalresources. Whileatfirst,thistechniquemaysoundidenticaltothemethodproposedbythispaper,the
twodiffersomewhat. TAScallsforadatasetcontainingmanylabelednegativeexamplesperquery,i.e. thekindused
inthefinetuningstageofworkslikeE5Wangetal.[2024]andArcticEmbedMerricketal.[2024],anditisthusnot
directlyapplicabletolarge-scalequery-itempairdatasetslikethoseusedinthepretrainingstepofthesemorerecent
textembeddingmodels.
4EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
However,itiscertainlyfairtoviewtheclusteringmethodpresentedinthispaperasawayofadaptingTAStothe
large-scalepretrainingsetting. Additionally,giventhecloserelationbetweenTASandthiswork,itisquiteplausible
that the theory that inspired the TAS methodology may also help explain our empirical findings in the large-scale
pretrainingsetting. Luckily,theTAS-Bauthorsprovideseveralnuggetsofwisdomintheirmotivatingremarksforusto
drawfrom:
1. Theybringupthevenerable“clusterhypothesis”[JardineandvanRijsbergen,1971,Voorhees,1985],which
statesthat“[i]tisintuitivelyplausiblethattheassociationsbetweendocumentsconveyinformationaboutthe
relevanceofdocumentstorequests”. Inotherwords,inreal-worlddatasets,itemsoftennaturallyfallinto
groupsofrelatedqueryrelevance.
2. Theymotivatethisconceptofnaturalgroupswithintrainingexamplesastheexistenceoftopics,andthey
arguethatquery-itempairscoveringcompletelydifferenttopicswillgenerallyserveasuninformativelyeasy
negativeexamplestooneanotherduringcontrastivetraining.
Givenhowneitheroftheseargumentsistiedtothesmall-batch,labeled-negativesettingstudiedintheTAS-Bpaper
[Hofstätteretal.,2021],wehappilydrawfromtheseinoursynthesis,whichwepresentbelowinSection4.3.
4.2 ConceptTwo: TheANCEPerspectiveOnHardNegativeMining
(cid:18) (cid:19)
exp(similarity(q,k ))
L =−log + (1)
infoNCE (cid:80) exp(similarity(q,k ))
i i
(cid:32) (cid:33)
(cid:88)
L =log exp(similarity(q,k ) −similarity(q,k ) (2)
infoNCE i +
i
L =smoothmax (similarity(q,k ))−similarity(q,k ) (3)
infoNCE i i +
L ≈max (similarity(q,k ))−similarity(q,k ) (4)
infoNCE i i +
ConsidertheubiquitousInfoNCEcontrastivelossfunction[vandenOordetal.,2019]giveninEquation(1).Rearranging
thetermsintoEquations(3)and(4),weseethatthislossapproximatesdiscountingaquery’smaximumquery-item
similarityacrossallitemsbyitssimilarityscoreassociatedwithitspositivelylabeleditem. Whenthesmoothmaximum
istunedtobeacloseapproximationtothemaximum4,allitemsineachminibatchalreadyscoredaslessrelevantto
thequerythanthepositively-labeleditemcontributeclosetonothingtothelossfunctionandthusdoclosetonothing
totrainthemodel. Seeingthis,wecanintuitivelygraspthemotivationforthepracticeofhardnegativemining: We
mustensureeachminibatchcontainsanamplenumberofinformative(i.e. difficult)negativeexamples,orelsetraining
progresswillslowtoastandstill.
Thoughseveralalgorithmsperformhard-negativemining,potentiallythemostrelevanttothisdiscussionisthatof
ApproximatenearestneighborNegativeContrastiveEstimation(ANCE)[Xiongetal.,2020],astheANCEauthors
offeramotivationthatisquitesimilartothemotivationgivenintheaboveparagraphs(thoughtheANCEauthors’
motivatingdispositionismorein-depthandmathematicallyrigorous). ThemethodofANCEisasfollows: Embedthe
entiretrainingdatasetusingacheckpointofthemodelintraining,constructanApproximateNearestNeighbors(ANN)
searchindexfromtheseembeddings,andthenconstructnewtrainingminibatchesbyaugmentingrandomlysampled
query-itempairswiththesetofnegativeitemsthattheANNindexretriesasmostrelevanttothequery(i.e. thehardest
negatives).
TheArcticEmbedreport[Merricketal.,2024]confirmedthatfine-tuningwithhardnegativeexamplesminedbytheir
embeddingswascriticaltoreachingSOTAperformanceontheMTEBRetrievalbenchmark[Muennighoffetal.,2023],
thoughthisworkusedonlyasimplified,non-iterativeversionofANCEthatleveragedasinglefixedtextembedding
modelcheckpointtoconstructtrainingbatchesonce. TheArcticEmbedreportalsofoundevidencethatimposingan
upperthresholdonnegativesimilarity,i.e. mining”hardbutnot-too-hard“negatives,workedbetterthanminingthe
hardestexamples.
Takentogether,theworksofANCEandArcticEmbedsuggestthattheidealcontrastivetrainingminibatchshouldbe
carefullycuratedvia“mined”examplesratherthanrandomlysampled,avoidingbothnon-activenegativeexamplesand
too-activenegativeexamplesandteachingthemodelsomethingusefulinallcontrastivepairings.
5EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
a
c b
c
a
b
(a)Guaranteedsimilarity. (b)Guaranteeddissimilarity.
Figure3: Thetriangleinequality|a−b|≤c≤a+bguaranteesthatforanypairofsimilarvectors,athirdvectorthat
issimilartooneofthemcannotbetoodissimilartotheother,whileathirdvectorthatisdissimilartoonecannotbetoo
similartotheother.
4.3 APossibleSynthesis: Topic-AlignedEmbeddingsAndATriangleInequalityThoughtExperiment
HavingdiscussedtherelevantconceptsofTAS-BandANCE,wenowattemptasynthesisoftheideasaboveintoan
explanationoftheefficacyofclusteringonembeddings. Letusconsiderthecaseofadatasetcontainingitemsthatfall
undervariousthemes,e.g. MSMARCO,whichcoversabroadarrayoftopicsincludinghealth,finance,andtechnology.
Letusfurtherconsidertraininganembeddingmodelonthisdatasettoproduceunit-normvectorsforvectorsimilarity
search. Letusfurther,foramoment,imagineastageoftraininginwhichthemodel’sembeddingsare(1)well-trained
enoughtoscoremostrandomlysamplednegativeitemssubstantiallylowerthanthelabeledpositiveitemformost
queries,and(2)havebecomegeometricallywellalignedwithmanyofthesemantictopicspresentinthedatasothat
same-topicitemsclustertightlyinembeddingspace.
Givenassumption(1),themotivatingprinciplesofhardnegativeminingfromSection4.2suggestthatcontinuingto
randomlysamplein-batchnegativesisapathtowardsalearningplateau. Furthermore,theclusterhypothesisdiscussed
inSection4.1givesusanintuitivewayofthinkingabouttheinefficiencyofrandomsampling–mostnegativeswillbe
drawnfromdifferenttopicsthanthepositiveitem,whichintuitivelyiseasiertotriviallydiscriminateasbeingoff-topic.
However,byassumption(2),wecangoastepfurtherandposeageometricargumentregardingthedifficultyofin-topic
vs. out-of-topicnegatives. Whenvectorsforsame-topicitemsareembeddedclosetooneanothergeometrically(e.g. the
dashedgreenanddottedyellowvectorsvisualizedinFigure3a),andqueryvectorsarealsoembeddedsimilarlytotheir
labeleditems(e.g. thesolidbluevectorfallingclosetothedashedgreenvectorinFigure3a),thetriangleinequality
|a−b|≤c≤a+bguaranteesthatotherin-topicitemswillallhavesomebareminimumlevelofsimilaritytothequery
asaresultoftheirsimilaritytooneanother,motivatingtheirinclusionasin-batchnegatives. Thetriangleinequality
alsoguaranteesthataslongasthequeryvectorfallsfarfromatleastoneoff-topicitem,alloff-topicitemswillexpress
somebareminimumlevelofdissimilarityfromthequerylikewiseduetothegeometricsimilarityofsame-cluster
embeddings,motivatingtheirexclusionasin-batchnegatives,asillustratedinFigure3b.
4.4 RealityCheck: DoesThisExplanation’sAssumptionsHoldUpInPractice?
Having considered the thought experiment above, let us ask ourselves how believable our two assumptions are in
thereal-worldpracticeoflarge-scalecontrastivepretraining. TheefficacyofANCE[Xiongetal.,2020]andother
hard-negativeminingworksofferconvincingsupportforassumption(1)tohold,andtheplateauingimprovementin
NDCG@10scorefortherunwithun-stratifieddatainFigure1agreeswiththistheoryaswell. Similarempirical
supportforassumption(2)isfoundbyexaminingrealclustered-by-embeddingsamplesinAppendixB,aswesee
thatourintuitivenotionsabouttopicsdoindeedmatchupwithgeometrically-closeclustersofembeddingvectors(for
example,wenoticea“quantitativequestion”topicinquerycluster5and“dollarvalueprice”topicinpassagecluster3
inAppendixB).
4This is often the case in practice, e.g. when we scale similarity scores by a high scaling constant (sometimes expressed
alternativelyasdividingthembyalowtemperatureparameter).
6EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
However,despitetheembedding-topicalignmentseeninAppendixB,theintra-clusteraverage-casesimilarityscores
giveninTable1(inparticularthoseinTable1a)suggestthattheclustersexperimentallystudiedinthisworkmaynot
betightlypackedenoughtomakethetriangleinequalityaconvincingboundonnegativehardnessforallin-cluster
negatives.Inotherwords,assumption(2)mayholdtoadegree,butnotsostronglythatthetriangleinequalityguarantees
thatallsame-clusterin-batchnegativeswillbeusefullyhardforthemodel.
Amoregranularclusteringapproach,e.g. the2,000clustersof[Hofstätteretal.,2021]ortheone-cluster-per-minibatch
casestudiedin[Choetal.,2024],maymakeforsubstantiallydenserclusters,butthiswouldnotexplaintheempirical
benefitobservedevenatthek =10clustersstudiedinthiswork. Thoughthisappearstobeadisagreementbetween
theoryandevidence,however,thedisagreementonlyinvalidatestheargumentforallin-clusternegativestobehard
negatives. Sinceeffectivetrainingdoesnotrequireeveryin-batchnegativetobeactiveforeveryquery,itstillappears
plausiblethattheprinciplesdiscussedinSection4.3applytotheexperimentathand.
5 LimitationsAndAlternatives
5.1 ANeedForCurriculumLearning
Wediscussedthatincreasingnegativesisacompellingfeatureforthephaseoftraininginwhichthemodelhasalready
learnedtogenerallyscore“easy”negativesasbeinglessrelevantthanlabeledpositives. However,thisisnotnecessarily
thecaseatthebeginningoftraining. Whileitisunclearthatusingclustereddataatthebeginningoftrainingwould
belessusefulthanun-clustereddata,neitherourtheoreticalargumentsnortheempiricalevidenceofFigure1(where
source-stratification does not improve performance in the first few hundred steps) suggest any benefits from this
approachuntilthemodelhasreachedabaselineleveloftraining.
Additionallyifatsomepointamodeliscapableofcorrectlyrankingthelabeleditemsformostqueriesabovethoseof
nearlyeveryotheriteminthedataset,evenifaclustercontainsalltheremaininghardnegatives,randomlysamplinga
relativelysmallminibatchfromarelativelylargeclusterstillrisksthehardnegativesnotendingupintheminibatch.
Thusalate-stagecurriculumadjustmentmaybenecessaryinsomecases,e.g. viashrinkingclustersize(possiblyto
sub-batch-sizedclusters)orswitchingovertofine-tuning-styletrainingwithhard-negativeminingoncethemodel
reachesthatpoint.
5.2 WhyNotJustHardNegativeMineForPretraining?
Section4.3isinsomewaysanargumentthatclusteringcanapproximatehardnegativemining,whichraisesthequestion
“whynotdirectlyhardnegativemineduringpretraining?” Theprimaryanswerhereisoneofefficiency: Ifwewant
morethanonehardnegativeperquery,hard-negativeminingwillrequiremorethanonenegativeitemperqueryinthe
minibatch. Thustotrainoneachquery,wemustpaynotjusttwicetheembeddingcost(onepositiveandonenegative
item),butseveraltimes(onepositiveandmanynegatives)theembeddingcostincurredbythestandardlarge-scale
pretrainingrecipe.
Thoughclusteringavoidsthiscost,otherpossiblemitigationsmayexistaswell. Forexample,onecouldtryincluding
in-batchpositivequeriesduringhardnegativemining–foreverynegativeitemmined,wewouldbringthecorresponding
positivequeryintotheminibatchaswelltoreturnthequery-to-itemratioto1. However,justlikeclustering,suchan
alternativewouldnotoutrightguaranteemeaningfullyhardnegativesinthebatchforeverypositive,soitisunclear
whetherthebenefitsofsuchanapproachwouldexceedthoseoftheclusteringapproach.
6 RelatedWork
Wehavealreadycoveredseveralrelatedworksaboveinsomedetail,includingtheArcticEmbed[Merricketal.,2024]
andNomicEmbed[Nussbaumetal.,2024]technicalreportswhichexaminedtheefficacyofthesourcestratification
approach,theTAS-B[Hofstätteretal.,2021]methodwhichappliedasimilarembed-and-clusterapproachtoconstructing
trainingexamples,andtheANCEmethod[Xiongetal.,2020]whichalsousesapretrainedtextembeddingmodelto
guidetheconstructionofminibatcheswithchallengingnegativeexamples.
Inadditiontotheseworkswhichwealreadydiscussedatsomelengthabove, therecentworkofChoetal.[2024]
alsoaddressesthechallengeofconstructingminibatchesforlarge-scalecontrastivetraining,thoughfromadifferent
perspective. Choetal.[2024]firstprovethattheonlysetofsize-Bminibatchesforwhichanoptimizedsolutionalso
optimizesthefull-batchlossoverallN
examplepairsisthecombinatoriallyexhaustivesetofall(cid:0)N(cid:1)
batches. Fromthis
B
perspective,theauthorsthenexaminehowonemightdotheirbestwithatractablysmallsetofminibatches,ultimately
attacking the simpler problem of partitioning N data points into N/B batches and arriving at a familiar-looking
7EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
embed-and-clusterapproach. However,themethodologicalsimilaritytothisworkissomewhatlimitedgiventhefact
thattheauthorsseektodirectlycreateminibatches(i.e. theirconceptof“cluster”and“minibatch”areoneandthe
same)insteadofthinkingintermsof“topics”or“sub-sources”aswehave. Additionally,insteadofdirectlyperforming
k-meansclusteringonembeddings,Choetal.[2024]applyaspectralclusteringalgorithmthatusesbothqueryand
itemembeddingsandismotivatedbyamathematicalargumentformaximizingalowerboundontotallossratherthan
anintuitiveargumentregardingnaturalsemanticclustersinthedata.
7 PathsForFutureWork
Inthiswork,wedevelopedadeepermentalmodelofwhythesourcestratificationtechniquemakessenseandprovided
aTAS-likealgorithmthatextendsthetechniqueanddrivesimprovementonareallarge-batchcontrastivepretraining
experiment. However,thecoreideabehindthispaper–thatlarge-scalecontrastivepretrainingworkloadscandobetter
throughcleverminibatchconstruction–extendswellbeyondwhatwehaveexploredinthesepages. Thusinplaceofa
typicalconclusionsection,weinsteadoptforadiscussionofseveraldirectionsthatfutureworkmaygofromhere.
7.1 TinyDenseClusters
Thispaper’sworkwasmotivatedbysourcestratificationandthusinheritedamindsetofsamplingmultipleminibatches
fromeachcluster. However,atlargeenoughpretrainingbatchsize,entiretopicsmayfitintoasfewasasinglebatch,
reversingthecluster-batchsizerelationship. TAS-B,forinstance,createdmuchsmallerclustersaveraging200queries
percluster5[Hofstätteretal.,2021]. Onecouldimagineargumentsforexactlyoneclusterperbatch(e.g. maximizing
thenumberofusefulnegativesforeachquery),aswellasformultipleclustersperbatch(someoff-topicnegatives
mightbehelpful,especiallyearlyintraining). Acomparisontothein-batchpositiveconceptsuggestedinSection5.2
wouldbeelucidatingaswell.
7.2 SmarterClustering
InaSection2footnote,wementionedthatleveragingqueryanditeminformationtogetherduringtheclusteringstep
mightyieldbetterresultsthanclusteringoneitherelementalone. Thespectralclusteringmethodof[Choetal.,2024]
leveragesinformationfrombothfieldsandprovesusefulinthissetting,thoughacomparisontoothernon-spectral
clusteringapproaches(suchassimplyconcatenatingqueryanditemembeddingvectors)maybeafruitfullineofinquiry
aswell.
7.3 DataEfficiencyAndFiltering
Innaturaldatasets,itisquiteplausiblethatmanyqueriesmayhavefewtonostronglyrelevantnegativeexamplesand
thuscannotteachthemodelasmuchasexamplesforwhichmoreinformativecontrastsexist. Additionally,itisquite
normaltohavesemanticallyredundantnegativeexamplesaswellwhichdemonstrateonlyasingleimportantcontrast
despiteincurringthecomputationalcostofmultipleembeddings. Webelievethatitmaybepossibletoimplementsome
datafilteringintothecluster-and-sampleapproachpresentedinthispaperandthatdoingsomightleadtosubstantial
efficiencyimprovementsinreal-worlddatasets.
Forexample, wesawthatseveralclustersinTable1weresmallerandlowerindensitythantheothers(e.g. query
clusters2and9). Whileretainingoutliertrainingexamplesmaystillhelpustraintheembeddingfunctiontoassociate
thequeryanditempairwithoneanother,itmaybebeneficialtoatleastgrouptheseoutliersintoseparatebatchesto
allowthenegativesofotherbatchesmoreinformative. Theclusteringapproachusedinthispapermaynaturallyfilter
someoutlierstotheirclusters,butunderstandingthedegreetowhichthishappens,thedegreetowhichthishelpsmodel
quality,andthedegreetowhichthiscanbeexplicitlyextendedrepresentanothercompellinglineforfuturework.
7.4 ClusteringBeyondText
Thoughthispaperfocuseditsexperimentationontextembeddingmodeltrainingforinformationretrieval,applyinga
similarembed-and-clusterapproachtovisionormultimodalsettingspresentsanaturalextension.
5TAS-Bcreated2,000clustersfrom400,000trainingqueries,aswellasabatchsizeof32.
8EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
7.5 AnEvolvingCurriculum
JustasANCE[Xiongetal.,2020]mandatesre-embeddingthedatasetandadjustingtheconstructionofminibatches
periodically,ananalogousre-embed,re-clusterandre-sampleapproachcouldbeappliedinthepretrainingphaseas
well. Effectivelyre-clusteringmorethanonceperepochwithoutexcessivelyover-samplingcertainexamplesalso
representsaninterestingpracticalproblemtoexplore. Additionally,theidealclustersizemaynotbeconstantover
training(e.g. Figure1suggeststhatsourcestratificationmattersmoreastrainingprogresses,soperhapsthegranularity
ofclusteringshouldprogressthroughouttraining).
8 Acknowledgements
WethankDanielCamposforhiscarefulreviewandfeedbackonmultipledraftsofthiswork.
References
LukeMerrick,DanmeiXu,GauravNuti,andDanielCampos. Arctic-embed: Scalable,efficient,andaccuratetext
embeddingmodels,2024. URLhttps://arxiv.org/abs/2405.05374.
ZachNussbaum,JohnX.Morris,BrandonDuderstadt,andAndriyMulyar. Nomicembed: Trainingareproducible
longcontexttextembedder,2024.
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew
McNamara,BhaskarMitra,TriNguyen,MirRosenberg,XiaSong,AlinaStoica,SaurabhTiwary,andTongWang.
MSMARCO:AHumanGeneratedMAchineReadingCOmprehensionDataset. arXivpreprintarXiv:1611.09268,
2018. doi:10.48550/arXiv.1611.09268.
SebastianHofstätter,Sheng-ChiehLin,Jheng-HongYang,JimmyLin,andAllanHanbury. Efficientlyteachingan
effectivedenseretrieverwithbalancedtopicawaresampling,2021. URLhttps://arxiv.org/abs/2104.06967.
LeeXiong,ChenyanXiong,YeLi,Kwok-FungTang,JialinLiu,PaulBennett,JunaidAhmed,andArnoldOverwijk.
Approximatenearestneighbornegativecontrastivelearningfordensetextretrieval,2020. URLhttps://arxiv.
org/abs/2007.00808.
MatthijsDouze,AlexandrGuzhva,ChengqiDeng,JeffJohnson,GergelySzilvasy,Pierre-EmmanuelMazaré,Maria
Lomeli,LucasHosseini,andHervéJégou. Thefaisslibrary. 2024.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformersforlanguageunderstanding,2019.
NiklasMuennighoff,NouamaneTazi,LoïcMagne,andNilsReimers. Mteb: Massivetextembeddingbenchmark,2023.
URLhttps://arxiv.org/abs/2210.07316.
LiangWang,NanYang,XiaolongHuang,BinxingJiao,LinjunYang,DaxinJiang,RanganMajumder,andFuruWei.
Textembeddingsbyweakly-supervisedcontrastivepre-training,2024.
NJardineandC.J.vanRijsbergen. TheUseofHierarchicClusteringinInformationRetrieval. InformationStorageand
Retrieval,7:217–240,1971.
Ellen M.Voorhees. The cluster hypothesisrevisited. In Proceedingsof the 8thAnnual InternationalACM SIGIR
Conference on Research and Development in Information Retrieval, SIGIR ’85, page 188–196, New York, NY,
USA, 1985. Association for Computing Machinery. ISBN 0897911598. doi:10.1145/253495.253524. URL
https://doi.org/10.1145/253495.253524.
AaronvandenOord,YazheLi,andOriolVinyals. Representationlearningwithcontrastivepredictivecoding,2019.
URLhttps://arxiv.org/abs/1807.03748.
JaewoongCho,KartikSreenivasan,KeonLee,KyunghooMun,SoheunYi,Jeong-GwanLee,AnnaLee,JyyongSohn,
DimitrisPapailiopoulos,andKangwookLee. Mini-batchoptimizationofcontrastiveloss. TransactionsonMachine
LearningResearch,2024. ISSN2835-8856. URLhttps://openreview.net/forum?id=Nux7OVXpJ9.
A TrainingParameters
• 3epochsatbatchsize4,096
• Learningratelinearwarmupfor50steps,thenlineardecayfrom4e-4to4e-5
9EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
• AdamWoptimizerusingPyTorchdefaultsforallparametersbesideslearningrate
• Gradientclippingto1.0
• InfoNCElosswithtemperature0.02
• Truncatedmaximumpassagelengthto256andmaximumquerylengthto32
B Appendix: ExamplesFromClusters
Anothermotivationforwhytheclusteringapproachmakesforbetterin-batchnegativesistosimplylookatthedata.
Examiningahandfulofrandommembersfromeachqueryorpassagecluster,weseethatindeedthereseemstobea
highersemanticrelevancebetweenothermembersofthesameclusterthanthereisbetweenmembersofoneclusterand
membersofanothercluster. Itisfairlyeasytothinkofqueriesforwhichhardnegativeswouldgenerallyallstemfrom
withinthesameclusterasthepositivetext.
B.1 QueryCluster1
• whenisaninternationalcalendar
• wasPeaceCorpsanexecutiveorder
• whyisprivateownershipanimportantsourceofeconomicprosperity?
• whyisbusinessresearchimportant
• whatisthestandarddeductionformarriedfilingjointly
• howlongtokeepfsadocuments
• whatisacomprehensivedeductibleamount?
• canyouuseyourpaypalmoneytobuythings
• whatiscompetencyandcliacompetencyassessment?
• differencebetweenabrokerdealerandanria
B.2 QueryCluster2
• drivingdistancebocaratontoatlanta
• doesamodemprovidewifi
• whendidcotognaopen?
• whatisthevalueofvintagerogersgasstove
• whatistheheightofmtsiinnorthbend?
• amexstolencardphonenumber
• temperatureinsydney,ausformarch
• averagemonthlytemperaturesst. augustinefl
• weatherinclayton
• howlargeisrussiainsqmiles
B.3 QueryCluster3
• whatisaghostaccount
• whatiskitana
• whatisaslug
• numbersinwordsform
• whatismagixfor
• whatisfiltration
• whatischaiwhatspecies
• whatdoesrfdmeaninan
• whatismarginalcost
• whoisjeremylondon
10EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
B.4 QueryCluster4
• canyoumakeyourlegslongernaturally
• howdoesphlevelsaffectthecolorofrosepetals?
• whatchemicalisfoundinappleseeds
• howlongafterpowerwashingcanistain
• howlongtocookaboiledegg
• whatfoodshavealgin
• whichpartofgoatmeatisbest
• doesthymeteagiveinsomnia
• howtobarbquebeefribs
• whattocleanlaminatefloorswith
B.5 QueryCluster5
• priceoflic
• howmuchisanewalternatorcost
• costcrushertreeremoval
• averagecostsofbridge
• gparequirementsforbaylor
• icelandaverageincome
• homeconstructioncostpersquarefoot
• whatshouldsugarcost
• costofhemerroid
• costofattendinguniversityofalabama
B.6 QueryCluster6
• whatislactobacilli
• whatispregnenolonesteal
• doallergiescausebleeding
• whatismsmvitaminsupplement
• whattypeofmedicineiscardiovascular
• canmridetectscartissue
• whichbodyparthasprimaryresponsibilityforeliminatingalcoholfromthebody
• whatdoestheopticdiskcause
• whatarethecharacteristicsofborderlinepersonalitydisorder
• iscodeineavasodilator
B.7 QueryCluster7
• ministrydefine
• whatismeaningofthesurnamedickinson
• whatismeantbytheterminternalenvironment
• whatisdefinitionofknots
• whatdoesthewordjessmean
• whatdoesthetermectopicmean
• whatarefelicitations?
• batteryequalizechargedefinition
• earthwormcropdefinition
• impulsivedefinition
11EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
B.8 QueryCluster8
• whereiscleveland
• whowrotewhenitcomestoyou,byjohnanderson
• whowasmelaniemartinez’scoachonthevoice
• usnavyrogerwilliambrown
• whichpresidentwasnicknamedslickwillie
• whosangfoolmeagain
• whenwasmarchofdimesfounded
• whowastedbundy’sfather
• aredaddylonglegsvenomous
• isflashfasterthansuperman
B.9 QueryCluster9
• whereischryslerbldglocated
• medianhomepriceinurbana,illinois
• whatisriverviewcounty
• populationgrowthcalifornia
• what’sthepropertytaxrateforalamedacounty
• whichcountyismiranda,ca
• whatcountyismountrainierin
• populationirvingtexas
• whereiswinstonsalemnc
• whatcountyishickorycreektexas
B.10 QueryCluster10
• themainfunctionofthecirculatorysystemisto__________.
• whatkindofmolluskisanoctopus
• whatarecriticallimitsinfishprocessing
• howdeepshoulddrainpipebe
• whatistheirondeficiencyproblemintreesthatrequiresironchelate?
• whyisargonunreactive
• whatisnonnativespecies
• whyisthenucleusimportantineukaryoticcells
• wheredoesbacterialive
• whatisinsulationmadefrom
B.11 PassageCluster1
• August3-6,2017. The2017ProFootballHallofFameEnshrinementWeekPoweredbyJohnsonControls
kicksoffwiththeannualHallofFameGame(Cardinalsvs. Cowboys)onThursday,Aug. 3.
• Hersultry,powerfulvoice,herincrediblelegs,hertime-testedbeautyandherunforgettablestoryallcontribute
toherlegendarystatus. TinaTurnerwasbornAnnaMaeBullockinNutbush,inHaywoodCounty,Tennessee,
toZelmaPriscilla(Currie)andFloydRichardBullock. Herfamilyweresharecroppers. Tinawasraisedinthe
segregatedSouth.
• Macy’sHeraldSquare,originallyknownastheR.H.MacyandCompanyStore,istheflagshipofMacy’s
departmentstores,locatedonHeraldSquareinManhattan,NewYorkCity.
12EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
• Buddy Fite was my guitar teacher and guitar hero during the late sixties. Buddy Fite is the greatest jazz
guitaristinmyopinion. Hisalbumscanstillbefo... BuddyFitewasmyguitarteacherandguitarheroduring
the late sixties. Buddy Fite is the greatest jazz guitarist in my opinion. His albums can still be found by
googlingorEbay. AmustforeveryJazzguitarplayer. Iwillbeputtingupmoreofhistunes. Thisisapure
loveofthemanandhisguitarplaying.
• From Wikipedia, the free encyclopedia. The Terracotta Army or the Terracotta Warriors and Horses is a
collectionofterracottasculpturesdepictingthearmiesofQinShiHuang,thefirstEmperorofChina.hefigures
includewarriors,chariotsandhorses. Estimatesfrom2007werethatthethreepitscontainingtheTerracotta
Armyheldmorethan8,000soldiers,130chariotswith520horsesand150cavalryhorses,themajorityof
whichremainedburiedinthepitsnearbyQinShiHuang’smausoleum.
B.12 PassageCluster2
• Theintertidalzone(sometimesreferredtoasthelittoralzone)istheareathatisexposedtotheairatlowtide
andunderwaterathightide(theareabetweenthelowandhightidelines).Thisareacanincludemanydifferent
types of habitats, including steep rocky cliffs, sandy beaches, or wetlands.he intertidal zone (sometimes
referredtoasthelittoralzone)istheareathatisexposedtotheairatlowtideandunderwaterathightide(the
areabetweenthelowandhightidelines).
• Asdescribedinthechapter’sContinuityandChangesection,whatmethoddidSigmundFreudusetoencourage
his patients to talk freely? Answer Selected Answer: Free association Correct Answer: Free association
Question6. Thispreviewhasintentionallyblurredsections.
• Herearesomewaystoclassifycoal(bygroupingitwithsimilartypesofrocks): Becauseslagformedfrom
lava-likemeltedrock,it’ssortoflikeanigneousrock–butbecausehumansmadethemelt,it’snotatrue
igneousrock. So,wemadeupanewclassificationforslag: Wecallitapseudo-igneousrock(pronounced
SUE-doeig-NEE-us).
• Theupperlimborupperextremityistheregioninananimalextendingfromthedeltoidregiontothehand,
includingthearm,axillaandshoulder.Contents.ostofthelargenumberofmusclesintheforearmaredivided
intothewrist,hand,andfingerextensorsonthedorsalside(backofhand)andthedittoflexorsinthesuperficial
layersontheventralside(sideofpalm). Thesemusclesareattachedtoeitherthelateralormedialepicondyle
ofthehumerus.
• Scientistsnowknowthatindependentassortmentofgenesoccursduringmeiosisineukaryotes. Meiosisisa
formofcelldivisionthatlowersthenumberofchromosomesinaparentcellbyhalftomakefourreproductive
cellscalledgametes.
B.13 PassageCluster3
• ThemedianpriceforahouseinthecoreOrlandomarket,whichincludesmostlyOrangeandSeminolecounties,
was$181,900inMaythatwasup10percentfromayearearlierand4percentfromamonthearlier,according
toareportreleasedMondaybyOrlandoRegionalRealtorsAssociation.
• Costofwisdomteethremoval-Extraction. Asof2017,ourcostrangefrom$200to$500pertoothforsurgical
wisdomteethremoval. Thisincludesthecostoflocalanesthesiaandfollowvisits. Onaverage,thepatientcan
expecttospendabout$1,400forfourwisdomteethremoval. Thecostdependsonthenatureofthesurgical
extraction.
• 1Onaverage,peoniescancostanywherefrom$2forapacketofseedstoasmuchas$3to$6ormoreforeach
stem. 2Apacketof500poppyflowerseedsretailsfor$4to$7. 3AJapaneseTreePeonythatisinfullbloom
canretailfor$27to$39fromlocalnurseries.4Peoniesareacommonflowerusedinvariousbouquets. For
futurebridesthatwanttousethisflowerinabouquet,theaveragepricecanfallbetween$2and$5perstem. 2
Forexample,thesiteMyFlowerBuyer.comsellsdifferenttypesofpeoniesforanywherefrom$350to$425as
awholesaleprice.
• Theaveragecosttoinstallgalvanizedoraluminumguttersisapproximately$4to$9perlinearfoot. Thereare
alsovinylgutterswhicharemucheasiertoinstall,andwhichrunatroughly$3to$5perlinearfoot. Therefore,
installingfrom125to200feetofgutterswillcost$1050-$2400. Theseprices,however,tendtoapplystrictly
totheDIYhomeowners.
13EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
B.14 PassageCluster4
• AlthoughtheageofsexualconsentinJapanis13yearsofage,prefecturelawusuallyoverridesfederallaw,
raisingtheageupto18. ThelegalageofconsentforsexinTokyoandNaganois13,not18liketherestofthe
country. Jersey. 16.
• 1ArrangementsunderPartXoftheBankruptcyActavoidingbankruptcy. 2Apersonwhoisinsolventmay
avoidbankruptcybyreachinganunderstandingwithcreditorsforthesatisfactionoftheirclaims. 3Thiscanbe
donebywayofapersonalinsolvencyagreementunderPartXoftheBankruptcyAct.eoplewhoaredeclared
bankruptarealsounabletoapplyforcertainjobsespeciallywhentheyareun-dischargedbankrupts. It’svery
difficult,forexample,toworkasacompanydirectorandobtainapositioninthefinancialservicessectorif
youaredeclaredbankrupt.
• Thisyear’sdeadlinetofileyourpersonalincometaxreturnismidnightonMay5fivedayslaterthanthe
usualApr. 30cutoff. YoucanthankthatnastyHeartbleedcomputerbugfortheextension, afteritforced
theCanadaRevenueAgencytoshutdownitsE-FilesystemearlierinApril.hisyear’sdeadlinetofileyour
personalincometaxreturnismidnightonMay5fivedayslaterthantheusualApr. 30cutoff. Youcanthank
thatnastyHeartbleedcomputerbugfortheextension,afteritforcedtheCanadaRevenueAgencytoshutdown
itsE-FilesystemearlierinApril.
• Real-timeprocessingcanhelpbanksdeliverablendedmultichannelexperience. Forexample, considera
customerwhohasanopeningbalanceof$250. Thatcustomertodaydeposits$750throughtheATM.Inanear
real-timeenvironment,thecustomermustwaituptoadayortwoforthebankprocessesthatdepositbefore
thefundsbecomeavailable. Inareal-timeprocessingsystem,that$750depositwouldbeclearedpromptly,
allowingthecustomertomakeatransactionusingthebanksdebitcard,whetheronlineorthroughasmart
phone.
• Theamountofincomethatisusedtocalculateanindividual’soracompany’sincometaxdue. Taxableincome
isgenerallydescribedasgrossincomeoradjustedgrossincomeminusanydeductions,exemptionsorother
adjustmentsthatareallowableinthattaxyear.
B.15 PassageCluster5
• 2. Beetjuice. Howitworks: Beetsareagoodsourceofpotassium–andagoodsourceoffolate,bothofwhich
areimportantinregulatingbloodpressure. What’smore,beetscontainnitrate,whichisconvertedintonitrites
onceingested. Nitritesrelaxsmoothmuscletissueandincreasebloodflow.
• Coconutoilisoneofthosepantryitemsthatcanhelpadogwithbadbreath. Itdoesntjustboostdigestive,
immunesystem,andmetabolicfunctionsitalsohelpstocombatcaninebadbreath. Putalovinteaspoonful
overyourdogsfoodeverysingleday,andyoullsoonsniffsweeterbreathplusdogslovethetaste;forthem,
coconutoilisasweettreat.
• Thereare40caloriesin110ChicletsservingofChewingGum(Sugared). Caloriebreakdown: 1%fat,99%
carbs,0%protein.
• Therefrigeratorisaverygoodstorageareaforflour,buttheuseofasealedcontainerisevenmoreimportant
to prevent the flour from absorbing moisture as well as odors and flavors from other foods stored in the
refrigerator. Thefreezerisusuallythebestlocationforlongtermstorage.
• Thehighest-proteinfruit,guavapacksmorethan4gramspercup,alongwith9gramsoffiberandonly112
calories. With600%ofyourDVofVitaminCpercuptheequivalentofmorethansevenmediumoranges! the
tropicalfruitshouldmerengueitswayintoyourshoppingcartASAP.
B.16 PassageCluster6
• 1 Place the meat in the oven and cook for 20 to 30 minutes at the high temperature. 2 Then lower the
temperaturetobetween275°Fand325°Fandroastuntildone(seedonenessguidelinesbelow). Don’tcover
the pan. 2 Ifyou are using a meat thermometer (analog or digital), insert the probe into thecenter of the
roast,beingcarefulnottohitbone. 3Placethemeatintheovenandcookfor20to30minutesatthehigh
temperature.
• WhatstheDifferenceBetweenPaleoandKeto? Intheend,themaindifferencebetweenPaleoandketoisone
ofemphasis. KetoemphasizesbeinginthestateofketosiswhereasPaleoemphasizesfoodquality. Inpractice,
mostfolksonaPaleodieteatamuchhigheramountofcarbohydratesthanthoseonaketogenicdiet.
• To mount your SKS you will need the Kalinka Optics. Warehouse Universal AK/SKS/SVD Side Plate,
Undrilledwhichcanbefoundinthe. Mounts&Ringssectionatwww.kalinkaoptics.com. Onceyouhave
14EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
attachedtheplateto. yourSKSyoucanuseanyAK,SKS/SVDorRussianSmallArmsmountincludingallof.
oursidemountsandthePOSP/PSOseriesofscope. MountingyourSKSbydrillingand. tappingaplateisthe
absoluteonlywaytomountanSKS,usingareceivercoveris.
• 2. UnderComputername,domain,andworkgroupsettingsclickonChangeSettings. ManageSettings. 3.
Under the tab Computer Name find the Change button and click it. Change Workgroup Name. 4.Under
Member Of change the Workgroup name. Change Workgroup Name. 5. Then click on OK, then when
promptedrebootyourdevice.hangingtheWorkgroupinWindows10. Followthestepsbelowtochangethe
workgroupinWindows10. 1. WiththerightmousebuttonclicktheStarticonandchooseSystem. Ifyou
haveatouchenableddevice,clickandholdthestartbutton,thentaptheSystembutton. Right-clickStartÂ»
System.
• LakeIsabella,CA-WeatherforecastfromTheweather.com. Weatherconditionswithupdatesontemperature,
humidity, wind speed, snow, pressure, etc. for Lake Isabella, California Today: Sunny intervals, with a
maximumtemperatureof52°andaminimumtemperatureof36°. Moderatewestwindwithmaximumgusts
of30mph.
B.17 PassageCluster7
• PhoneNumberofAllerganContactis+1(800)-347-4500,+1(714)-246-4500. Allerganisapharmaceuticals
company that was established in 1948. It is multi-specialty health care company that focuses on medical
dermatology,urology,neuroscience,ophthalmicpharmaceuticalsandeyecare.
• Ocean Springs High School. Ocean Springs High School is an IB-certified public high school in Ocean
Springs,Mississippi,UnitedStates. Theschoolservesstudentsingrades912andispartoftheOceanSprings
SchoolDistrict. Contents.
• ThetotaldrivingdistancefromFLLtoMIAis27milesor43kilometers. YourtripbeginsatFortLauderdale-
HollywoodInternationalAirportinFortLauderdale,Florida. ItendsatMiamiInternationalAirportinMiami,
Florida. Ifyouareplanningaroadtrip,youmightalsowanttocalculatethetotaldrivingtimefromFLLto
MIAsoyoucanseewhenyou’llarriveatyourdestination.
• Kalispell,Montana. Kalispellisacityin,andthecountyseatofFlatheadCounty,Montana,UnitedStates.
The2015CensusestimatesputKalispell’spopulationat22,052. TheKalispellMicropolitanStatisticalArea
hasapopulationof93,068anditisthelargestcityandcommercialcenterofnorthwestMontana. Thename
KalispellisaSalishwordmeaningflatlandabovethelake.
• TEXASCITY,TX77590. AmocoFederalCreditUnion’sroutingnumber(theleftmostnumberonthebottom
ofacheck)is313189391. Sometimes,bankshavemultipleroutingnumbersfordifferentbranchesoruses.
Pleasemakesurethisisthecorrectroutingnumberforyourbranch!
B.18 PassageCluster8
• Within eukaryotes, DNA replication is controlled within the context of the cell cycle. As the cell grows
anddivides,itprogressesthroughstagesinthecellcycle;DNAreplicationtakesplaceduringtheSphase
(synthesisphase).
• NaturalHabitat: Thenaturalhabitatofstarfishspansrightfromtheintertidalzone,i.e.,theseashorewhichis
exposedtotheairduringthelowtideandgoesunderwaterduringthehightide,totheabyssalzone,whichhas
adepthofroughlyabout4000-6000meters.
• Thereisanotherdefectintiresthatcancauseavibrationcalledloadedroadforcevariance. Thetiresandpass
avisualinspection,aradialandlateralrunouttest,beperfectlybalanced,butstillcauseavibration. Thisis
duetotheinternaldefectsinthetire.
• The lungs are a pair of spongy, air-filled organs located on either side of the chest (thorax). The trachea
(windpipe)conductsinhaledairintothelungsthroughitstubularbranches,calledbronchi. Thebronchithen
divideintosmallerandsmallerbranches(bronchioles),finallybecomingmicroscopic.
• Computedtomography(CTscanorCATscan)isanoninvasivediagnosticimagingprocedurethatusesa
combinationofX-raysandcomputertechnologytoproducehorizontal,oraxial,images(oftencalledslices)of
thebody.
15EmbeddingAndClusteringYourDataCanImproveContrastivePretraining APREPRINT
B.19 PassageCluster9
• Aldosteroneisahormonereleasedbytheadrenalglands.Ithelpsthebodyregulatebloodpressure.Aldosterone
increasesthereabsorptionofsodiumandwaterandthereleaseofpotassiuminthekidneys. Thisactionraises
bloodpressure.
• Stimulantsaredrugsthatcanincreasealertnessandawareness,usuallyforashorttimeonly. Moststimulants
havemoreside-effectsthanotherdrugs. Someareclassifiedasillegaldrugs,mostcancauseaddiction.Forthis
reason,mostlegalstimulantsareonlyavailableonprescription.Stimulantsactonthenerves: Stimulantscause
moreneurotransmitterstothesynapse(thisisthegapbetweendifferentnerves).omeareclassifiedasillegal
drugs, mostcancauseaddiction. Forthisreason, mostlegalstimulantsareonlyavailableonprescription.
Stimulantsactonthenerves: Stimulantscausemoreneurotransmitterstothesynapse(thisisthegapbetween
differentnerves).
• Superior vena cava syndrome (SVCS) occurs when a persons superior vena cava is partially blocked or
compressed. Thesuperiorvenacavaisamajorveininapersonsbody. Itcarriesbloodfromthehead,neck,
upperchest,andarmstotheheart. CancerisusuallythemaincauseofSVCS.
• Diarrhoeaisusuallyasymptomofgastroenteritis(abowelinfection),whichcanbecausedby: 1avirussuch
asnorovirusorrotavirus. 2bacteriasuchascampylobacter,Clostridiumdifficile(C.3parasitessuchasthe
Giardiaintestinalisparasitethatcausesgiardiasis.
• Regardlessoftheseverityoftheinjury,followthesestepstoimmediatelytreataburn: Flushtheburnedarea
withcoolrunningwaterforseveralminutes. Call911forasevereburn(seebelowtolearnifyourburnis
severe)Applyaburnointmentorsprayforpain. Takeibuprofenoracetaminophenforpainreliefifnecessary.
B.20 PassageCluster10
• impendingadjective[beforenoun]. usedtorefertoanevent,usuallysomethingunpleasantorunwanted,that
is going to happen soon: impending disaster/doom The player announced his impending retirement from
internationalfootball.
• MedicalDefinitionofhepatobiliary. : of,relatingto,situatedinornear,producedin,oraffectingtheliverand
bile,bileducts,andgallbladderhepatobiliarydiseasethehepatobiliarysystem.
• Unstintinglydefinition,tobefrugal;getalongonascantyallowance: Don’tstintonthefood. Theystintedfor
yearsinordertosavemoney. Seemore.
• Definitionofhunter. 11a: apersonwhohuntsgameb: adogusedortrainedforhuntingc: ahorseusedor
adaptedforuseinhuntingwithhounds;especially: afaststronghorsetrainedforcross-countryworkand
jumping. 22: onethatsearchesforsomething. 33: apocketwatchwithahingedprotectivecover.
• WhatisBaroqueMusic? Whatisbaroque,andwhenwastheBaroqueperiod? DerivedfromthePortuguese
barroco,oroddlyshapedpearl,thetermbaroquehasbeenwidelyusedsincethenineteenthcenturytodescribe
theperiodinWesternEuropeanartmusicfromabout1600to1750.
16