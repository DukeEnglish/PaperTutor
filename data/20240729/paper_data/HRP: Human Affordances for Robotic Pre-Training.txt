HRP: Human Affordances for Robotic Pre-Training
Mohan Kumar Srirama Sudeep Dasariψ Shikhar Bahlψ Abhinav Guptaψ
Carnegie Mellon University
Abstract—In order to generalize to various tasks in the wild,
Pretrained Visual Affordance
robotic agents will need a suitable representation (i.e., vision
Representation Fine-tuning
network) that enables the robot to predict optimal actions
given high dimensional vision inputs. However, learning such
a representation requires an extreme amount of diverse training DINO VC-1
data, which is prohibitively expensive to collect on a real robot. Contact
How can we overcome this problem? Instead of collecting more Points
robot data, this paper proposes using internet-scale, human
videos to extract “affordances,” both at the environment and
agent level, and distill them into a pre-trained representation. +
We present a simple framework for pre-training representations
on hand, object, and contact “affordance labels” that highlight CLIP ImageNet-MAE
relevant objects in images and how to interact with them. These
affordances are automatically extracted from human video data Hand Pose
(with the help of off-the-shelf computer vision modules) and
used to fine-tune existing representations. Our approach can Active
efficiently fine-tune any existing representation, and results in Object
modelswithstrongerdownstreamroboticperformanceacrossthe
… …
board. We experimentally demonstrate (using 3000+ robot trials)
that this affordance pre-training scheme boosts performance
by a minimum of 15% on 5 real-world tasks, which consider Fig. 1: Pre-trained representations offer a scalable solution to
three diverse robot morphologies (including a dexterous hand). the robotics data bottleneck [64, 70, 57], but existing methods
Unlike prior works in the space, these representations improve failtoreliablyimproveoversimplebaselineslikeImageNet[20,
performance across 3 different camera views. Quantitatively, we
7]. Thus, we present HRP, a method that mines affordances
find that our approach leads to higher levels of generalization in
(e.g.,contact,handpose,andobjectlabels)fromhumanvideos
out-of-distribution settings. For code, weights, and data check:
https://hrp-robot.github.io and uses them to improve self-supervised visual encoders.
Our best HRP representation consistently outperforms 6 SOTA
I. INTRODUCTION
baselinesby≥20%across5diversetasksand3cameraviews.
Atrulygeneralistroboticagentmustacquirediversemanipu-
lationskills(rangingfromblockstackingtopouring)thatwork
community.Afterpre-training,theserepresentationsareusedto
with novel objects and remain robust to realistic environmental
initialize downstream imitation learning [78] algorithms. This
disturbances (e.g., lighting changes, small camera shifts). Due
formula is extremely flexible, and can substantially reduce the
to the scale of this challenge, the field has trended towards
amount of robot data required for policy learning. However,
learning these agents directly from data [51, 67], particularly
the representations are often only effective when using specific
robot trajectories collected either by expert demonstrators or
camera views and robot setups. Furthermore, independent
autonomously by the agents themselves (via Reinforcement
evaluations [20, 7] recently showed that these representations
Learning [86]). Unfortunately, there are innumerable object-
cannot improve (on average) over the most obvious baseline –
s/environments, so roboticists cannot tractably collect enough
a self-supervised ImageNet representation [40, 21]!
real-world demonstration data and/or design a simulator that
This result is surprising since robot trajectories and human
captures all this diversity.
video sequences share so much common structure: both modal-
One promising solution for this “data challenge” is for the
ities contain an agent (e.g., human or robot) using their end-
robot to learn a suitable representation from Out-Of-Domain
effector(e.g.,humanhand,robotgripper)tomanipulateobjects
(OOD) data that can be transferred into the robotics domain.
intheirenvironment.Ideally,representationstrainedonthisdata
For example, prior work [64, 70, 57] trained self-supervised
wouldlearnusefulobjectattributes(e.g.,wheretograspamug),
image encoders on large scale datasets of human videos
and spatial relationships between the end-effector and target
(e.g., Ego4D [33]), using standard reconstruction objectives
objects.Wehypothesizethattraditionalself-supervisedlearning
and contrastive learning [65] objectives – e.g., Masked Auto-
objectives are unable to extract this information from human
Encoders[40](MAE)andTemporalContrastiveNetworks[79]
video data, and that explicitly predicting these object/spatial
(TCN) respectively – developed by the broader learning
features would result in a stronger robotic representation (i.e.,
ψ Denotesequaladvising. higher down-stream control performance). Our key insight is
4202
luJ
62
]OR.sc[
1v11981.7042:viXraActionable visual representations
Encoder
Contacts
ℒ
ct
+ + ℒ
hand
Hand pose
ℒ
obj
Frame
Frozen layer
Active object
Fig. 2: HRP fine-tunes a pre-trained encoder to predict three classes of human affordance labels via L2 regression. Specifically,
the network must predict future contact points, human hand poses, and the target object given an input frame from the
video stream. These affordance labels are mined autonomously from a human video dataset [33] using off-the-shelf vision
detectors [81]. HRP representations are then fine-tuned to solve downstream manipulation tasks via behavior cloning.
that abandoning self-supervision comes at minimal cost – the 4) We show that HRP representations generalize across dif-
necessary object and hand labels can be scalably mined using ferent imitation learning stacks – HRP improves diffusion
off-the-shelf vision pipelines. policy [11] performance by 20%!
This paper proposes Human affordances for Robotic Pre- 5) Our best representation, which increases performance by
training (HRP), a semi-supervised pipeline to learn effective 20% over State-of-the-Art (SOTA), will be fully open-
robotic representations from human video. HRP works in two sourced, along with all code and data.
stages: first, it extracts hand-object “affordance” information –
II. RELATEDWORK
i.e.,whichobjectsinthescenearegraspableandhowtherobot
Representation Learning in Robotics: End-to-end policy
should approach them – from human videos using off-the-shelf
learning offers a scalable formula for acquiring robotic repre-
tracking models [81, 72]. These affordances are then distilled
sentations: instead of hand-designing object detectors or image
into a pre-existing representation network (e.g., ImageNet
features, a visual encoder is directly optimized to solve a
MAE [40]), before the policy fine-tuning stage. This paradigm
downstream robotic task [51]. Numerous works applied this
allows us to inject useful information into the vision encoder,
idea to diverse tasks including bin-picking [45, 52, 67], in-
while preserving the flexibility of self-supervised pre-training –
the-wild grasping [35, 85], insertion [19, 51], pick-place [6],
i.e., all labels are automatically generated and the network can
and (non-manipulation tasks like) self-driving [5, 68, 10].
be easily slotted into downstream robotic policies/controllers
Furthermore, secondary learning objectives – e.g., dynamics
via fine-tuning. To summarize, we learn stronger robotic
modeling [36, 91], observation reconstruction [63], inverse
representations by predicting object interactions and hand
modeling [17], etc. – can be easily added to improve data
motion from human video dataset images (see Fig. 1).
efficiency.Whilethisparadigmcanbeeffective,learningpurely
Our investigations and experiments lead to the following
from robot data requires an expensive data collection effort
contributions:
(e.g., using an arm farm [52, 45], large-scale tele-operation [6],
1) We present a semi-supervised learning algorithm – HRP– ormulti-institutiondatacollection[18,12]),whichisinfeasible
that leverages off-the-shelf human affordance models to for (most) task settings.
learn effective robotic representations from human video. To increase data efficiency, prior work applied self-
The proposed pipeline strongly outperforms representa- supervisedrepresentationlearningalgorithmsonout-of-domain
tions learned purely via self-supervision. datasets (like Ego4D [33]), and then fine-tuned the resulting
2) ApplyingHRPto6pre-existingrepresentations(including representations to solve downstream tasks with a small
ImageNet[21,40],VC-1[57],andDINO[8])substantially amount of robot data – e.g., via behavior cloning on ≤ 50
boosts robot performance. This conclusion is backed by expert demonstrations [64, 57, 70], directly using them as
3000+ robot trials, and replicates across 3 camera views, a cost/distance function to infer robot actions [56, 89], or
3 distinct robotic setups, and 5 manipulation tasks! directly pre-training robot policies from extracted human
3) Our ablation study reveals that HRP’s three affordance actions. [82, 58, 47]. While this transfer learning paradigm
objectives (hand, object, and contact based loss terms) are can certainly be effective, it is unclear if these robotic
all critical for effective representation learning. representations [57, 64, 70] provide a substantial boost over
mroN
daeh-itluM
noitnetta
mroN PLMContact Hand Object vector f (I)∈Rd. This resulting “embedding vector” would
θ
ideally encode important scene details for robotic policy
learning – like the number and type of objects in a scene and
their relationship to the robot end-effector. In this paper, f is
θ
a transformer network (specifically ViT-B [96], with patch size
16 and d=768) parameterized with network weights θ. But
to be clear, all our methods are network architecture agnostic.
Self-Supervised Learning: The computer vision community
has broadly adopted self-supervised representation learning
algorithms that can pre-train network weights without using
any task-specific supervision. This can be accomplished using
a generative learning objective [22], which trains f alongside
θ
a decoder network D that reconstructs the original input image
input from the representation. Another common approach is
contrastive learning [65, 39], which optimizes f to maximize
Fig. 3: We extract 3 affordances – contact heatmaps, hand θ
the mutual information between the encoding and the input
poses and active object bounding boxes – from human videos.
image (i.e., place “similar” images closer in embedding space).
pre-existing vision baselines [20, 7], like ImageNet MAE [40] In practice, these methods can learn highly useful features
or DINO [8]. One potential issue is that roboticists often for downstream vision tasks [40, 39], but struggle in robotics
use the same exact pre-training methods from the vision settings [20, 7]. Our goal is to inject these features into an
community, but merely apply them to a different data mix existing self-supervised network, with an affordance-driven
(e.g., VC-1 [57] applies MAE [40] to Ego4D [33]). Thus, fine-tuning stage.
the resulting representations are never forced to key in
B. Extracting Affordance Labels from Human Data
on object/agent level information in the scene. This paper
proposes a simple formula for injecting this information into Before we can do any fine-tuning, we must first curate a
a vision encoder, using a mix of hand and object affordance suitable human affordance dataset D . Thankfully this task
H
losses, which empirically boost performance on robotic tasks can be done automatically using off-the-shelf vision modules,
by 25%. applied to a set of 150K human-object interaction videos
from Ego4D (originally sampled by Nair et al. [64]). These
Affordances from Humans: HRP is heavily inspired by are subsets of larger videos (around 1.2K) videos, which were
the affordance learning literature in computer vision [29, 28]. further broken down into shorter clips. Each clip contains a
These works use human data as a probe to learn environmental semantically meaningful action by the human. Each video
cues (i.e., affordances) that tell us how humans might interact clip V contains image frames V = {I ,...,I } that depict
1 T
with different objects. These include physical [23, 3, 34, 102, human hands performing tasks and moving around in the
37, 98, 61] and/or semantic [74, 76] scene properties, or scene. From these images, we obtain contact locations, future
forecast future poses [49, 71, 26, 43, 41, 88, 1, 50, 87, 33, 24, hand p-oses, and active object labels (examples in Fig. 3)
60, 30] Affordances can also be learned at object or part that capture various agent-centric properties (how to move
levels [103, 25, 31, 62, 53, 93]. Usually such approaches and interact) and environment centric properties (where to
leveragehumanvideodatasets[33,14,16,13]orusemanually interact) at multiple scales, i.e. contact-level and object-level.
annotated interaction data [54, 15, 81]. In addition to these The following sections detail how each of these labels were
cues, robotic affordances must consider how to move before generated.
and after interaction [2, 46]. A simple, scalable way to capture
this information is by detecting these cues from human hand Contact Locations: To extract contact locations for an image
posesinmonocularvideostreams[90,46,72,55],whichshow I (with no object contact), we find the frame I ;j >t where
t j
robots reaching for and manipulating diverse, target objects. contact with a given object will begin, using a hand-object
Ourmethodcombinesthesethreeapproachestocreateahuman interaction detection model [81]. Then, we use I to find
j
affordance dataset automatically from human video streams. the active object O and the hand mask M . The points
j j
The labels generated during this process are distilled into a intersecting M and O (acquired via skin segmentation) are
j j
representation and used to improve downstream robotics task our contact affordances (C ). To account for motion between
j
performance. I and I , we compute the homography matrix between the
t j
frames and project those points forward. This is done using
III. PRELIMINARIES
standard SIFT feature tracking [99]: C = H C . In other
t j,t j
A. Visual Representation Learning
words, the contact locations denote where in I the human
t
Our goal is to learn a visual encoder network f that takes will contact in the future. Note that there could be a different
θ
an input image I and processes it into a low-dimensional number of points for each contact scenario, which is non-idealPolicy Training
End-Effector Velocity +
HRP Representation Gripper Velocity
+ + 𝜋
policy
Scene Image o
t
Fig. 4: We present our policy training pipeline, which uses Behavior Cloning (BC) to train policy π, using optimal expert demonstrations.
Theimageobservation(o )isprocessedusingourHRPrepresentationsresultinginalatentvectorz.Thepolicyusesz topredictend-effector
t
velocity actions (delta ee-pose/gripper), which are directly executed on the robot during test-time.
for learning. Thus, we fit a Gaussian Mixture Model with aself-supervisednetwork,f ,usinganautomaticallygenerated
θ
k = 5 modes on C to make a uniform contact descriptor human affordance dataset, D (see above for definitions and
t H
– defined as the means c of the mixture model. For more dataset mining approach). HRP is illustrated in Fig. 2, and the
t
details on extraction, we refer to Appendix F. following sections describe its implementation in detail.
A. Training HRP
Future Hand Poses: This affordance label captures how
the human moves next (e.g., to complete a task or reach an The initial network f θ is fine-tuned using batches sampled
object), as the video V progresses. Given a current frame from the human dataset: (I t,c t,h t,b t) ∼ D H, where c t, h t,
I t, we detect the human hand’s 2d wrist position (h t+k) and b t are contact, hand, and object affordances corresponding
in a future frame I t+k, where usually k = 30 (empirically to image I t (see Sec. III-B for definitions). Some frames may
determined). This is done using the Frank Mocap [72] hand not include all 3 affordances, so we include 3 mask variables –
detector. To correctly account for the human’s motion, these m( tc),m( th),m( tb) –sothemissingvaluescanbeignoredduring
wrist points are back-projected (again using the camera training. We add 3 small affordance modules – p c, p h, p b – on
homography matrix) to I t to create the final “future wrist top of f θ that are trained to regress the respective affordances
label,” h
t
=H t+k,th t+k. for I t. This results in the following three loss functions:
Active Object Labels: In a similar manner to the contact L ct =||c t−p c(f θ(I t))|| 2 (1)
location extraction, we run a hand-object interaction detection
L =||h −p (f (I ))|| (2)
hand t h θ t 2
model[81]onV tofindtheimagewherecontactbeganI .The
c
samedetectorisusedtofindthefourboundingboxcoordinates L obj =||b t−p b(f θ(I t))|| 2 (3)
of the object that is being interacted with, which we refer to as
The full loss is:
the “active object.” These coordinates b are then projected to
c
everyotherframeI ,usingthehomographymatrix(seeabove).
t
This results in an active bounding box b for each image in V. L=m(c)λ L +m(h)λ L +m(b)λ L (4)
t t ct ct t hand hand t obj obj
IV. INTRODUCINGHRP Where the λs are hyper-parameters that control the relative
weight of each affordance loss. We empirically found λ =
Avarietyofvisualpre-trainingtaskshavebeenshowntohelp obj
0.05,λ = 0.005,λ = 0.5 to be optimal for downstream
with downstream robotic performance– ranging from simple ct hand
performance (see Appendix E).
ImageNet classification [80] to self-supervised learning on
human video [70, 64, 56, 57, 66]. Although these approaches B. Implementation Details
operate on human videos and simple image frames, they fail
Ouraffordancedataset(D )isatleastanorderofmagnitude
to explicitly model the rich hand-object contacts depicted. In H
smallerthanthepre-trainingimagedatasetinitiallyusedbythe
contrast,webelieveexplicitlymodelingtheaffordances[28]in
baseline representation (e.g., ImageNet has 1M frames v.s. our
this data could allow us to learn useful information about the
150K). To preserve the useful features learned from the larger
agent’sintents,goals,andactions.Indeed,pastworkhasshown
pre-training distribution, we keep most of the parameters in θ
that affordances can act as strong prior for manipulation [100,
fixedduringHRPfine-tuning.Specifically,weonlyfine-tunethe
44, 84, 95, 2, 9, 42, 4] in general. Moreover, this information
baselinenetwork’snormalizationlayersandleavetherestfixed,
can be represented in many different formats, such as physical
which has been shown to be an effective approach [27, 97].
attributes, geometric properties, interactions, object bounding
In the case of our ViT-B this amounts to fine-tuning only the
boxes, or motion forecasting. We observe that most tasks of
LayerNorm parameters γ and β:
interests humans perform are with their hands. We thus focus
x−µ
on training our model to predict hand-object interactions and
LayerNorm(x)= γ+β (5)
hand motion. σ
We present HRP, a simple and effective representation These parameters are fine-tuned to minimize L using
learningapproachthatinjectshand-objectinteractionpriorsinto standard back-propagation and the ADAM [48] optimizer.
mroN
daeh-itluM
noitnetta mroN PLMFranka xArm Dexterous Hand
Front Cam Front Cam Front Cam Start Start
Ego Cam Ego Cam Ego Cam End End
Toasting Pouring Stacking Pot on Stove Lift Cup
Fig. 5: Our experiments consider 5 unique manipulation tasks, ranging from classic block-stacking to a multi-stage toasting
scenario. These tasks are implemented on 3 unique robot setups, including a high Degree-of-Freedom dexterous hand (right).
The 3 camera views shown – front, ego, and side views (for xArm/dexterous hand) – are the same views ingested by the policy
during test-time. Note that 3 of the tasks consider 2 unique camera views in order to test for robustness!
V. EXPERIMENTALDETAILS 5) MVP [70] was trained by applying MAEs to a mix
Our contributions are validated using a simple empirical of in-the-wild datasets (100 DoH [81], Ego4D [33],
formula: first, HRP is applied to each baseline model (listed etc.). The authors showed strong performance on various
below). Then, (following standard practice [64, 57, 20]) the manipulation tasks. We used publicly available model
resulting representation is fine-tuned into a manipulation weights.
policy using behavior cloning. Details for each stage are 6) VC-1 [57] was trained in a similar fashion to MVP, but
provided below, and the HRP is illustrated in Fig. 2. used a larger dataset mix. It showed strong performance
on visual navigation tasks. We used publicly available
Baseline Representations: We chose 6 representative, SOTA model weights.
baselines from both the vision and robotics communities: Note that each baseline is parameterized with the same
1) ImageNet MAE was pre-trained by applying the Masked ViT-B encoder w/ patch size 16 (see Sec. III-B), to ensure
Auto-Encoders [40] (MAE) algorithm to the ImageNet- apples-to-apples comparisons.
1M dataset [21]. It achieved SOTA performance across
a suite of vision tasks, and is the first self-supervised Policy Learning: Each representation is evaluated on
representation to beat supervised pre-training. We use the downstream robotic manipulation tasks, by fine-tuning it
standard Masked Auto Encoder training scheme for this, into a policy (π) using Behavior Cloning [68, 77, 73]. Note
using hyperparmaeters from MAE [40]. that π must predict the expert action (a – robot motor
t
2) Ego4D MAE was trained by applying the MAE algo- command) given the observation (o – input image and
t
rithm to a set of 1M frames sampled from the Ego4D robot state): a ∼ π(·|o ). And π is learned using a set of
t t
dataset [33]. For consistency with prior work, we use the 50 expert demonstrations D = {τ ,...,τ }, where each
1 50
same1Mframe-setsampledbytheR3Mauthors[64].We demonstration τ = [(o ,a ),...,(o ,a )] is a trajectory
i 0 0 T T
use the standard Masked Auto Encoder training scheme of expert observation-action tuples. In our case, π is
for this, using hyperparmaeters from MAE [40]. parameterized by a small 2-layer MLP (p) placed atop the
3) CLIP [69] is a SOTA representation for internet data. It pre-trained encoder p(f(o )) that predicts a Gaussian Mixture
t
waslearnedbyapplyingcontrastivelearning[65]toalarge policy distribution w/ 5 modes. Both the policy network and
set natural language - image pairs crawled from internet visual encoder are optimized end-to-end (using ADAM [48]
captions. We used publicly available model weights. w/ lr =0.0001 for 50K steps) to maximize the log-likelihood
4) DINO [8] was trained using a self-distillation algorithm of expert actions: max log(π(a |p(f(o )))). During test
p,f t t
that encourages the network to learn local-to-global time actions are sampled from this distribution and executed
image correspondences. DINO’s emergent segmentation on the robot: a ∼π(·|p(f(o ))). This is a standard evaluation
t t
capabilities could be well suited for robotics, and it has formula that closely follows best practices from prior robotic
already shown SOTA performance in sim [7]. We used representation learning work [59, 20].
publicly available model weights.HRP v.s. Baselines (Front Cam) HRP v.s. Baselines (Ego Cam)
0.8 0.8
Ours Ours
0.7 0.7
Baseline Baseline
0.6 0.6
0.5 0.5
0.4 0.4
0.3 0.3
0.2 0.2
0.1 0.1
0.0 0.0
Ego4D ImageNet CLIP DINO MVP VC-1 Ego4D ImageNet CLIP DINO MVP VC-1
Initial Representation Initial Representation
Fig. 6: We apply HRP to 6 different baseline representations and plot how it affects performance on average across the toasting, pouring,
and stacking tasks. This evaluation procedure is repeated using two distinct cameras (shown in Fig. 5) in order to test if HRP representation
are robust to view shifts. We find that HRP representations consistently and substantially outperform their vanilla baselines, and that this
effect holds across both the front (left) and ego (right) cameras. In fact, our strongest representation – ImageNet + HRP– delivers SOTA
performance on both views!
Real World Tasks: We fine-tune policies for each rep- of meat or carrot from a plate and placing it within a
resentation on the 5 diverse tasks listed below, which are pot on a stove. During test time, novel “food” objects are
implemented on 3 unique robotic setups, including a dexterous used and the location is randomized. A trial is marked as
hand(illustratedinFig.5).50expertfine-tuningdemonstrations successful if the food is correctly placed in the pot. This
were collected for each task via expert tele-operation. Note task is implemented on a xArm and uses the side camera
that the stacking, pouring, and toasting tasks were evaluated view.
twice using different camera views to test robustness! • Hand Lift Cup This task requires a dexterous hand to
reach, grasp, and lift up a deformable red solo up. The
• Stacking: The stacking task requires the robot to pick hand’s high dimensional action space (R20) makes this
up the red block and place it on the green block. During
task especially challenging. A trial is marked successful
test time both blocks’ starting positions are randomized
if the cup is stably grasped and picked. This task is
to novel locations (not seen in training). A trial is marked
implemented on a custom dexterous hand using a side
as successful if the robot correctly picks and stacks the
camera view.
red block, and half successful if the red block is unstably
placed on the green block. This task is implemented on
VI. RESULTS
a Franka robot and uses both an Ego and Front camera Our experiments are designed to answer the following:
viewpoint. 1) Can HRP improve the performance of the pre-trained
• Pouring: The pouring task requires the robot to pick up baseline networks (listed above)? Does the effect hold
the cup and pour the material (5 candies) into the target across different camera views and/or new robots? (see
bowl. During test time we use novel cups and bowls Sec. VI-A)
and place each in new test locations. This task’s success 2) Our affordance labels are generated using off-the-shelf
metric is the fraction of candies successfully poured (e.g., visionmodules–doesdistillingtheiraffordanceoutputs
2/5 candies poured → 0.4 success). This task was also into a representation (via HRP) work better than sim-
implemented on the Franka using Ego and Front cameras. ply using those networks as encoders? (see Sec. VI-B)
• Toasting: The toasting task requires the robot to pick up 3) How does HRP compare against alternate forms of
a target object, place it in the toaster oven, and shut the supervision on the same human video dataset? (see
toaster. This is a challenging, multi-stage task. During Sec. VI-C)
test time the object type, and object/toaster positions are 4) How important are each of the three affordance losses for
both varied. A test trial is marked as successful if the HRP’s final performance? And is it really best to only
whole task is completed, and 0.5 successful if the robot fine-tune the LayerNorms and leave the other weights
only successfully places the object. This is the final task fixed? (see Sec. VI-D)
implemented on Franka w/ Ego and Front camera views. 5) Can HRP handle scenarios with OOD distractor objects
• Pot on Stove: The stove task requires picking up a piece during test time? (see Sec. VI-E)
ecnamrofreP ecnamrofrePFinetuning Setup Ablation Loss Function Ablations
0.8 0.8
LayerNorm Only
0.7 0.7
All Weights
0.6 0.6
0.5 0.5
0.4 0.4
0.3 0.3 Ours
Ablate Contact
0.2 0.2
Ablate Object
0.1 0.1
Ablate Hand
0.0 0.0
Ego4D ImageNet CLIP DINO MVP VC-1 Ego4D ImageNet VC-1
Initial Representation Initial Representation
Fig.7:ThischartappliesanablatedHRPmethod(fullfine-tuning)to Fig. 8: We drop each of the 3 losses in HRP, and compare the
the6baselinerepresentations,andcomparestheiraverageperformance ablated method’s average performance (across the toasting, pouring,
v.s. standard HRP representations on the toasting, pouring, and stacking tasks) against full HRP representations. Due to the number
stacking tasks (front cam). We find that LayerNorm only fine-tuning of ablations involved, this experiment is only run on the Ego4D,
is almost always superior. ImageNet, and VC-1 base models. We find that the object and
hand losses are critical for good performance, but the contact loss
only makes a significant impact on the Ego4D base model.
6) Can HRP representations work with different imita-
tion learning pipelines, like diffusion policy [11]? (see TeacherResNet HRPModels
Sec. VI-F) FrontCam 100DoH[81] w/Ego4D w/ImageNet w/CLIP
Note that all experiments were conducted on real robot Toasting 35%±15% 83%±9% 75%±10% 50%±11%
Pouring 34%±13% 60%±11% 48%±12% 39%±11%
hardware, and the models were all tested back-to-back (i.e.,
Stacking 0% 77%±10% 70%±11% 57%±11%
using proper A/B evaluation) using 50+ trials per model to
Average 35%±10% 73%±6% 64%±7% 48%±6%
guarantee statistical significance. Note that all of our figures
and tables report success rates (sometimes averaged across TABLE I: This table compares 3 representations trained w/ HRP
the toasting, stacking, and pouring tasks) alongside std. err. to against the teacher ResNet [81] that generated our human affordance
dataset (see Sec. III-B). We find that the ResNet teacher under-
quantify experimental uncertainty – i.e. success%±std. err..
performs even the worst HRP representation (fine-tuned from CLIP),
even after excluding the stacking task, which it failed on.
A. Improving Representations w/ HRP
To begin, we evaluate the 6 baseline representations representations perform very differently when the camera
(detailed in Sec. V) on the toasting, pouring, and stacking view (even slightly) changes. To address this issue, we
tasks using the front camera view. Then, we apply HRP to replicated the first experiment using a radically different ego
each of these baselines, and evaluate those 6 new models view, where the camera is placed over the robot’s shoulder
on the same tasks. Average success rates across all 3 (i.e., on its “head”). While perhaps a more realistic view,
tasks are presented in Fig. 6 (left), and the full table is in it is significantly more challenging due to the increased
the Appendix B. First, this experiment demonstrates that robot-object occlusion. Average success rates are presented in
ImageNet MAE is still highly competitive on real-world Fig. 6 (right), and a per-task breakdown is in Appendix C.
manipulation tasks when compared to other self-supervised Note that our findings replicate almost exactly from the
representations from the vision [33, 8], machine learning [69], front camera view. The ImageNet MAE representation is
and robotics communities [92, 57]. Second, we show that still competitive with the other baselines, and applying HRP
HRP uniformly boosts performance on downstream robotics uniformly improves the baseline performance. In addition,
tasks – i.e., baseline + HRP > baseline for every we find that HRP injects a higher level of robustness
baseline representation considered! Thus, we conclude that to camera view shifts, when compared to the baselines.
the affordance information injected by our method is highly For example, we find that ImageNet + HRP performs
useful for robot learning, and (for now) cannot be learned in a the same on the ego and front camera, even though the
purely self-supervised manner. ImageNet baseline clearly prefers the front cam. This
generaleffectholds(tovaryingdegrees)acrossallsixbaselines!
Second Camera View: A common critique is that robotic
ecnamrofreP ecnamrofrePEgo4D ImageNet CLIP
+ HRP + Semantic + HRP + Semantic + HRP + Semantic
Toasting 83%±9% 25%±13% 75%±10% 40%±14% 50%±11% 20%±13%
Pouring 60%±11% 30%±13.4% 48%±12% 26%±11% 39%±11% 22%±10%
Stacking 77%±10% 30%±11% 70%±11% 40%±12% 57%±11% 30%±13%
Average 73%±6% 28%±7% 64%±7% 35%±7% 48%±6% 24%±7%
TABLE II: We create Semantic representations by fine-tuning the Ego4D, ImageNet, and CLIP baselines using a classification loss,
instead of HRP’s affordance loss. Note that the exact same Ego4D clips (see Sec. III-B) are used during semantic fine-tuning, thanks to
object class labels generated automatically by Detic [101]. The sematic representations were evaluated (using the same BC pipeline) on the
Toasting, Pouring, and Stacking tasks, and compared against their HRP counterparts. Success rates (and standard error) are reported above.
We find that the affordance supervision provided by HRP is vastly superior to the semantic alternative.
Ego4D ImageNet
Initialization w/ HRP MAE Initialization
w/HRP Baseline w/HRP Baseline
PotonStove 50%±17% 40%±16% 60%±16% 40%±16% Ego4D 40%±15% 15%±11%
HandLiftCup 50%±17% 40%±16% 50%±17% 30%±15% ImageNet 40%±15% 40%±15%
TABLE III: We present results of Ego4D + HRP and ImageNet TABLEIV:ThistablecomparesEgo4D + HRPandImageNet +
+ HRP, as well as the respective baselines on the x-Arm (Pot on HRPrepresentationsagainsttheirrespectivebaselinesonastackingw/
Stove) and a dexterous hand task (Lift Cup). We see that HRP can distractors task. Here the robot must successfully complete the usual
even boost performance in multiple morphologies, including a high- stackingtask,whenextraneousobjects(anorangecarrot,andagreen
degree of freedom dexterous hand [83]. bowl)areaddedtothescene.WefindthatEgo4D + HRPimproved
over its baseline on this task, but ImageNet + HRP performed the
ScalingtoMoreRobots: Finally,weverifythatHRPrepresen- same as its baseline.
tations can provide benefits on other robotic hardware setups.
C. Comparing Against Alternate Forms of Supervision
Specifically, we compare Ego4D + HRP and ImageNet +
HRPversustherespectivebaselinesonthePotonStove(xARM) We now analyze if HRP’s losses are better suited for
andHandLiftCup(dexteroushand)tasks.Resultsarepresented robotics tasks than an alternate supervision scheme. To be
in Table III. Note that HRP representations provide consistent clear, the previous results already demonstrated that HRP +
and significant performance during policy learning on these Ego4D out-performed the Ego4D baseline by up to 20% (see
radically different robot setups, which both also use a unique Fig. 6; left), despite being sourced from the same image data.
side camera view. This gives us further confidence in HRP’s However, it could be that the additional fine-tuning step with
viewrobustnessanddemonstratesthattheserepresentationsare the 100K filtered interaction clips is responsible, and the
not tied to specific hardware setups, and can scale to complex specific affordance losses are not key. To test this, we ran
morphologies like dexterous hands. a modified version of HRP using a semantic classification loss,
instead of our affordance hand-object losses. The ground-truth
labels for each image were obtained using the Detic object
B. Distillation w/ HRP Improves Over Label Networks
detector [101]. We then similarly fine-tuned the ImageNet,
It is clear that applying HRP to self-supervised represen- Ego4D, and CLIP baseline representation using these labels,
tations results in a consistent boost. However, the hand, and compared them against the respective HRP models on the
object, and contact affordance labels for HRP themselves come toasting, pouring, and stacking tasks. The results are presented
from neural networks (see Sec. III-B) – specifically we use in Table II We find that the HRP models perform significantly
the ResNet-101 [38] detector from 100DoH [81] as a label better on every task. Thus, we conclude that HRP’s affordance
generatorforouractiveobjectandcontactaffordance.Thehand losses play an important role in boosting performance (i.e., it’s
affordance we use comes from FrankMocap [72], which uses not just data or extra fine-tuning).
100DoH[81]asabasemodel.Thus,doesdistillinglabelsfrom
D. What Design Decisions are Important?
this detector via HRP actually provide a benefit over simply
using the 100DoH model itself as a pre-trained representation? The following section ablates the key components of HRP
To test this question, we fine-tune policies on the toasting, to evaluate their relative importance. First, we apply HRP to
pouring, and stacking (front cam) tasks and compare them each of the 6 baseline representations again, but this time
against HRP applied to ImageNet, Ego4D, and (the weakest none of the weights are kept fixed (see Sec. IV-B). These
model) CLIP (see Table I). In all cases, our representation representations are fine-tuned on the toasting, stacking, and
handily beats the 100DoH policy. So while the affordance pouring tasks (front cam), and compared against the original
labels can dramatically boost policy learning (via HRP), the HRP representations in Fig. 7. Note that fine-tuning all the
source/teacher models are not at all competitive on robotics layers results in a substantial performance hit on average, and
tasks. this trend is consistent regardless of the base representation!Diffusion Policy Comparison when the representation is trained on less diverse data.
F. Evaluating w/ Diffusion Policy
0.8
Finally, we analyze if HRP representations offer improve-
ments when using a radically different imitation learning
0.6 framework,likediffusionpolicy[11].Specifically,weadoptthe
original U-Net action prediction head and environment setup
from Chi et. al. [11], but replace their ResNet visual encoder
0.4 (inspiredfromRoboMimic[59])withourHRP + ImageNet
ViT-B model. Then we compare this HRP enhanced diffusion
0.2 HRP + ImageNet policy implementation, against (diffusion agents which use)
ImageNet both the original ResNet encoder and the baseline ImageNet
ResNet ViT-B. Results for the (Franka) stacking, pouring, and toasting
0.0 tasksarepresentedinFig.9.WefindthatHRP + ImageNet
Average Toasting Pouring Stacking
significantly improves over both alternatives (76% for HRP
Task
v.s., 56% for Chi et. al.’s implementation [11]), despite using
Fig.9:ThisfiguretestsifHRPrepresentationscanboostperformance a radically different imitation learning objective/setup! Thus,
whenusingaradicallydifferentimitationlearningframework–namely
we conclude that HRP representations can boost performance
Diffusion Policy [11]. We evaluate diffusion policies (following the
across different setups.
U-Net + state action formula described by Chi et. al [11]) on the
toasting,pouring,andstackingtasksusing3differentvisualencoders:
thedefaultResNetencoderfromRoboMimic[59],theImageNet +
VII. DISCUSSIONANDFUTUREWORK
MAE baseline, and our HRP + ImageNet features. We find a clear In this paper, we investigate human affordances as a strong
improvement when using HRP weights, which suggests that HRP is
priorfortrainingvisualrepresentations.Thus,wepresentHRP,
applicable to different imitation learning frameworks!
a semi-supervised pipeline that extracts contact points, hand
poses, and activate objects from human videos, and uses these
Thus, we conclude fine-tuning only the layer norms when affordancesforfine-tuningrepresentations.HRPimprovesbase
applying HRP is the correct decision. model performance drastically, for five different, downstream
Next, we ablate each of the affordance losses in Eq. 4, behavior cloning tasks, across three robot morphologies and
by applying HRP three times: once with λ ct = 0, then with threecameraviews.Allcomponentsofourapproach,including
λ hand =0, and finally λ obj =0. This process is repeated using LayerNorm tuning, our three affordances, and our distillation
3 different base models; ImageNet, Ego4D, and VC-1. This process (from affordance labels to representations) are im-
creates 9 ablated models (3 losses x 3 initializations) that are portant for the model’s success. One key limitation of this
compared versus the full HRP models on the toasting, pouring, approach is that it has only been tested on imitation settings
and stacking tasks. The average results are presented in Fig. 8, in this paper. In the future, we hope to not only scale this
andthefull,per-taskbreakdownispresentedintheAppendixD. approach to many more tasks and robot morphologies, but also
We find that removing the object (Eq. 3) and hand (Eq. 2) incorporate HRP in other robot learning paradigms such as
losses uniformly results in significant performance degradation. reinforcement learning or model based control.
Meanwhile, the contact loss (Eq. 1) only provides a significant
boost for the Ego4D base model but does not affect the others. REFERENCES
Thus, we conclude that object and hand losses are critical for [1] Yazan Abu Farha, Alexander Richard, and Juergen
ourmethod,whilethecontactlossismoremarginal,mostlikely Gall. When will you do what?-anticipating temporal
due to the fact that the extraction of contacts is a relatively occurrences of activities. In CVPR, 2018. 3
noisy process. [2] Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain,
and Deepak Pathak. Affordances from human videos as
E. Novel Distractors During Test-Time
a versatile representation for robotics. 2023. 3, 4, 16
WeevaluatetheperformanceofHRPandbaselineapproaches [3] AayushBansal,BryanRussell,andAbhinavGupta.Marr
in OOD settings, by adding extraneous “distractor” objects (an revisited: 2d-3d alignment via surface normal prediction.
orange carrot and a light green bowl) in the stacking task. The In CVPR, 2016. 3
robot must successfully ignore the distractor and complete the [4] Homanga Bharadhwaj, Abhinav Gupta, Vikash Kumar,
task. Results are presented in Table IV. We found that both and Shubham Tulsiani. Towards generalizable zero-shot
ImageNet + HRP and ImageNet had the same level of manipulation via translating human interaction plans.
robustness to distractors. Meanwhile, Ego4D’s performance arXiv preprint arXiv:2312.00775, 2023. 4
dropped substantially, while Ego4D + HRP remained robust. [5] Mariusz Bojarski, Davide Del Testa, Daniel
Our hypothesis is that human data by itself does not contain Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
enough information to allow for OOD tasks. However, using Goyal, Lawrence D Jackel, Mathew Monfort, Urs
HRP allows for more focus on task-relevant features, even Muller, Jiakai Zhang, et al. End to end learning for
ecnamrofrePself-driving cars. arXiv preprint arXiv:1604.07316, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson,
2016. 2 Jonathan Yang, Joseph J. Lim, Joa˜o Silve´rio, Junhyek
[6] Anthony Brohan, Noah Brown, Justice Carbajal, Yev- Han, Kanishka Rao, Karl Pertsch, Karol Hausman,
gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg,
Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka,
Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Kevin Zhang, Keyvan Majd, Krishan Rana, Krishnan
Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto,
DmitryKalashnikov,YuhengKuang,IsabelLeal,Kuang- Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka,
HueiLee,SergeyLevine,YaoLu,UtsavMalla,Deeksha MaximilianDu,MichaelAhn,MingtongZhang,Mingyu
Manjunath, Igor Mordatch, Ofir Nachum, Carolina Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin
Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Kim,NaoakiKanazawa,NicklasHansen,NicolasHeess,
Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Nikhil J Joshi, Niko Suenderhauf, Norman Di Palo,
Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver
Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Kroemer, Pannag R Sanketi, Paul Wohlhart, Peng Xu,
Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Pierre Sermanet, Priya Sundaresan, Quan Vuong, Rafael
Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Rafailov, Ran Tian, Ria Doshi, Roberto Mart´ın-Mart´ın,
and Brianna Zitkovich. Rt-1: Robotics transformer Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Ju-
for real-world control at scale. In arXiv preprint lian, Samuel Bustamante, Sean Kirmani, Sergey Levine,
arXiv:2212.06817, 2022. 2 Sherry Moore, Shikhar Bahl, Shivin Dass, Shuran Song,
[7] Kaylee Burns, Zach Witzel, Jubayer Ibn Hamid, Tianhe Sichun Xu, Siddhant Haldar, Simeon Adebola, Simon
Yu, Chelsea Finn, and Karol Hausman. What makes Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker,
pre-trained visual representations successful for robust StephenTian,SudeepDasari,SuneelBelkhale,Takayuki
manipulation? ArXiv, 2023. 1, 3, 5 Osa, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao,
[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve´ Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao,
Je´gou, Julien Mairal, Piotr Bojanowski, and Armand Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent
Joulin. Emerging properties in self-supervised vision Vanhoucke,WeiZhan,WenxuanZhou,WolframBurgard,
transformers. CVPR, 2021. 2, 3, 5, 7 Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li,
[9] Matthew Chang, Aditya Prakash, and Saurabh Gupta. Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu,
Look ma, no hands! agent-environment factorization Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho,
of egocentric videos. Advances in Neural Information YoungwoonLee,YuchenCui,YuehhuaWu,YujinTang,
Processing Systems, 36, 2024. 4 Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo,
[10] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Zhuo Xu, and Zichen Jeff Cui. Open X-Embodiment:
Kra¨henbu¨hl. Learning by cheating. In CoRL, 2020. 2 Robotic learning datasets and RT-X models, 2024. 2
[11] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric [13] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,
Cousineau, Benjamin Burchfiel, and Shuran Song. Dif- Sanja Fidler, Antonino Furnari, Evangelos Kazakos,
fusion policy: Visuomotor policy learning via action Davide Moltisanti, Jonathan Munro, Toby Perrett, Will
diffusion. In Proceedings of Robotics: Science and Price,andMichaelWray. Scalingegocentricvision:The
Systems (RSS), 2023. 2, 7, 9 epic-kitchens dataset. In ECCV, 2018. 3
[12] Open X-Embodiment Collaboration, Abhishek Padalkar, [14] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,
Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Her- Sanja Fidler, Antonino Furnari, Evangelos Kazakos,
zog, Alex Irpan, Alexander Khazatsky, Anant Rai, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will
AnikaitSingh,AnthonyBrohan,AntoninRaffin,Ayzaan Price, and Michael Wray. Scaling egocentric vision:
Wahid,BenBurgess-Limerick,BeomjoonKim,Bernhard The epic-kitchens dataset. In European Conference on
Scho¨lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Computer Vision (ECCV), 2018. 3
Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, [15] Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma,
Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, AmlanKar,RichardHiggins,SanjaFidler,DavidFouhey,
Danny Driess, Deepak Pathak, Dhruv Shah, Dieter andDimaDamen. Epic-kitchensvisorbenchmark:Video
Bu¨chler, Dmitry Kalashnikov, Dorsa Sadigh, Edward segmentations and object relations. Advances in Neural
Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue Information Processing Systems, 35:13745–13758, 2022.
Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, 3
Giulio Schiavi, Hao Su, Hao-Shu Fang, Haochen Shi, [16] Pradipto Das, Chenliang Xu, Richard F Doell, and
Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Jason J Corso. A thousand frames in just a few words:
Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Lingual description of videos through latent topics and
Radosavovic, Isabel Leal, Jacky Liang, Jaehyung Kim, sparse object stitching. In Proceedings of the IEEE
Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey conference on computer vision and pattern recognition,
Bingham,JiajunWu,JialinWu,JianlanLuo,JiayuanGu, pages 2634–2641, 2013. 3[17] SudeepDasariandAbhinavGupta.Transformersforone- Ego4d: Around the world in 3,000 hours of egocentric
shot visual imitation. In Conference on Robot Learning, video. In Proceedings of the IEEE/CVF Conference
pages 2071–2084. PMLR, 2021. 2 on Computer Vision and Pattern Recognition, pages
[18] SudeepDasari,FrederikEbert,StephenTian,SurajNair, 18995–19012, 2022. 16
BernadetteBucher,KarlSchmeckpeper,SiddharthSingh, [33] Kristen Grauman, Andrew Westbury, Eugene Byrne,
Sergey Levine, and Chelsea Finn. Robonet: Large-scale Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jack-
multi-robot learning. arXiv preprint arXiv:1910.11215, son Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.
2019. 2 Ego4d: Around the world in 3,000 hours of egocentric
[19] Sudeep Dasari, Jianren Wang, Joyce Hong, Shikhar video. In Proceedings of the IEEE/CVF Conference
Bahl, Yixin Lin, Austin S Wang, Abitha Thankaraj, on Computer Vision and Pattern Recognition, pages
Karanbir Singh Chahal, Berk Calli, Saurabh Gupta, 18995–19012, 2022. 1, 2, 3, 5, 7
et al. Rb2: Robotic manipulation benchmarking with a [34] Abhinav Gupta, Scott Satkin, Alexei A Efros, and
twist. In Thirty-fifth Conference on Neural Information Martial Hebert. From 3d scene geometry to human
Processing Systems Datasets and Benchmarks Track workspace. In CVPR, 2011. 3
(Round 2), 2021. 2 [35] Abhinav Gupta, Adithyavairavan Murali,
[20] Sudeep Dasari, Mohan Kumar Srirama, Unnat Jain, and Dhiraj Prakashchand Gandhi, and Lerrel Pinto.
Abhinav Gupta. An unbiased look at datasets for visuo- Robot learning in homes: Improving generalization and
motor pre-training. In Conference on Robot Learning. reducing dataset bias. Advances in neural information
PMLR, 2023. 1, 3, 5 processing systems, 31, 2018. 2
[21] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, [36] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mo-
and Li Fei-Fei. Imagenet: A large-scale hierarchical hammad Norouzi. Dream to control: Learning behaviors
image database. In 2009 IEEE conference on computer by latent imagination. In International Conference on
vision and pattern recognition, pages 248–255. Ieee, Learning Representations, 2020. 2
2009. 1, 2, 5 [37] M Hassanin, S Khan, and M Tahtali. Visual affordance
[22] CarlDoersch.Tutorialonvariationalautoencoders.arXiv and function understanding: a survey. arxiv. arXiv
preprint arXiv:1606.05908, 2016. 3 preprint arXiv:1807.06775, 2018. 3
[23] D Eigen and R Fergus. Predicting depth, surface [38] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
normals and semantic labels with a common multi-scale Sun. Deep residual learning for image recognition.
convolutional architecture. corr, abs/1411.4734. arXiv CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/
preprint arXiv:1411.4734, 2014. 3 1512.03385. 8
[24] AntoninoFurnariandGiovanniMariaFarinella. Rolling- [39] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and
unrolling lstms for action anticipation from first-person Ross Girshick. Momentum contrast for unsupervised
video. TPAMI, 2020. 3 visual representation learning. In Proceedings of the
[25] Antonino Furnari, Sebastiano Battiato, Kristen Grau- IEEE/CVF conference on computer vision and pattern
man, and Giovanni Maria Farinella. Next-active-object recognition, pages 9729–9738, 2020. 3
prediction from egocentric videos. Journal of Visual [40] KaimingHe,XinleiChen,SainingXie,YanghaoLi,Piotr
Communication and Image Representation, 2017. 3 Dolla´r, and Ross Girshick. Masked autoencoders are
[26] Jiyang Gao, Zhenheng Yang, and Ram Nevatia. Red: scalablevisionlearners.InProceedingsoftheIEEE/CVF
Reinforced encoder-decoder networks for action antici- ConferenceonComputerVisionandPatternRecognition,
pation. arXiv preprint arXiv:1707.04818, 2017. 3 pages 16000–16009, 2022. 1, 2, 3, 5
[27] Angeliki Giannou, Shashank Rajput, and Dimitris Pa- [41] De-An Huang and Kris M Kitani. Action-reaction:
pailiopoulos. The expressive power of tuning only the Forecastingthedynamicsofhumaninteraction.InECCV,
normalization layers. arXiv preprint arXiv:2302.07937, 2014. 3
2023. 4 [42] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu
[28] James Jerome Gibson. The senses considered as Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable
perceptual systems, volume 2. 3, 4 3d value maps for robotic manipulation with language
[29] JJ Gibson. The ecological approach to visual perception. models. arXiv preprint arXiv:2307.05973, 2023. 4
Houghton Mifflin Comp, 1979. 3 [43] Ashesh Jain, Avi Singh, Hema S Koppula, Shane Soh,
[30] Rohit Girdhar and Kristen Grauman. Anticipative video and Ashutosh Saxena. Recurrent neural networks for
transformer. In ICCV, 2021. 3 driver activity anticipation via sensory-fusion architec-
[31] Mohit Goyal, Sahil Modi, Rishabh Goyal, and Saurabh ture. In ICRA, 2016. 3
Gupta. Human hands as probes for interactive object [44] Yuanchen Ju, Kaizhe Hu, Guowei Zhang, Gu Zhang,
understanding. In CVPR, 2022. 3 Mingrun Jiang, and Huazhe Xu. Robo-abc: Affor-
[32] Kristen Grauman, Andrew Westbury, Eugene Byrne, dance generalization beyond categories via semantic
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jack- correspondence for robot manipulation. arXiv preprint
son Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. arXiv:2401.07487, 2024. 4[45] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian arXiv:2303.18240, 2023. 1, 2, 3, 4, 5, 7, 16
Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, [58] Priyanka Mandikal and Kristen Grauman. Dexvip:
Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, Learning dexterous grasping with human hand pose
et al. Qt-opt: Scalable deep reinforcement learning priors from video. In Conference on Robot Learning,
for vision-based robotic manipulation. arXiv preprint pages 651–661. PMLR, 2022. 2
arXiv:1806.10293, 2018. 2 [59] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush
[46] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei,
and Jitendra Malik. End-to-end recovery of human Silvio Savarese, Yuke Zhu, and Roberto Mart´ın-Mart´ın.
shape and pose. CoRR, abs/1712.06584, 2017. URL What matters in learning from offline human demonstra-
http://arxiv.org/abs/1712.06584. 3 tions for robot manipulation. In Conference on Robot
[47] Aditya Kannan, Kenneth Shaw, Shikhar Bahl, Pragna Learning (CoRL), 2021. 5, 9, 16
Mannam, and Deepak Pathak. Deft: Dexterous fine- [60] Esteve Valls Mascaro, Hyemin Ahn, and Dongheui Lee.
tuning for real-world hand policies. CoRL, 2023. 2 Intention-conditionedlong-termhumanegocentricaction
[48] DiederikPKingmaandJimmyBa. Adam:Amethodfor forecasting@ ego4d challenge 2022. arXiv preprint
stochastic optimization. arXiv preprint arXiv:1412.6980, arXiv:2207.12080, 2022. 3
2014. 4, 5, 16 [61] Austin Myers, Ching L Teo, Cornelia Fermu¨ller, and
[49] Hema S Koppula and Ashutosh Saxena. Anticipating Yiannis Aloimonos. Affordance detection of tool parts
human activities using object affordances for reactive from geometric features. In ICRA), 2015. 3
robotic response. TPAMI, 2015. 3 [62] Tushar Nagarajan, Christoph Feichtenhofer, and Kristen
[50] Tian Lan, Tsung-Chuan Chen, and Silvio Savarese. A Grauman. Grounded human-object interaction hotspots
hierarchical representation for future action prediction. from video. In ICCV, 2019. 3
In ECCV, 2014. 3 [63] Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar
[51] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Bahl, Steven Lin, and Sergey Levine. Visual reinforce-
Abbeel. End-to-endtrainingofdeepvisuomotorpolicies. ment learning with imagined goals. Advances in neural
TheJournalofMachineLearningResearch,17(1):1334– information processing systems, 31, 2018. 2
1373, 2016. 1, 2 [64] SurajNair,AravindRajeswaran,VikashKumar,Chelsea
[52] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Finn, and Abhinav Gupta. R3m: A universal visual
Ibarz, and Deirdre Quillen. Learning hand-eye coor- representation for robot manipulation. arXiv preprint
dination for robotic grasping with deep learning and arXiv:2203.12601, 2022. 1, 2, 3, 4, 5
large-scale data collection. The International journal of [65] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Rep-
robotics research, 37(4-5):421–436, 2018. 2 resentation learning with contrastive predictive coding.
[53] Shaowei Liu, Subarna Tripathi, Somdeb Majumdar, and arXiv preprint arXiv:1807.03748, 2018. 1, 3, 5
Xiaolong Wang. Joint hand motion and interaction [66] Jyothish Pari, Nur Muhammad, Sridhar Pandian
hotspots prediction from egocentric videos. In CVPR, Arunachalam, Lerrel Pinto, et al. The surprising effec-
2022. 3, 16 tiveness of representation learning for visual imitation.
[54] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang arXiv preprint arXiv:2112.01511, 2021. 4
Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, [67] Lerrel Pinto and Abhinav Gupta. Supersizing self-
and Li Yi. Hoi4d: A 4d egocentric dataset for category- supervision: Learning to grasp from 50k tries and 700
level human-object interaction. In Proceedings of the robot hours. In 2016 IEEE international conference
IEEE/CVF Conference on Computer Vision and Pattern on robotics and automation (ICRA), pages 3406–3413.
Recognition, pages 21013–21022, 2022. 3 IEEE, 2016. 1, 2
[55] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris [68] DeanAPomerleau. Alvinn:Anautonomouslandvehicle
McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, in a neural network. Advances in neural information
Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee, processing systems, 1, 1988. 2, 5
et al. Mediapipe: A framework for building perception [69] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
pipelines. arXiv preprint arXiv:1906.08172, 2019. 3 Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
[56] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayara- Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
man, Osbert Bastani, Vikash Kumar, and Amy Zhang. Krueger, and Ilya Sutskever. Learning transferable
Vip: Towards universal visual reward and represen- visual models from natural language supervision. CoRR,
tation via value-implicit pre-training. arXiv preprint abs/2103.00020, 2021. URL https://arxiv.org/abs/2103.
arXiv:2210.00030, 2022. 2, 4 00020. 5, 7
[57] Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, [70] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter
Yecheng Jason Ma, Claire Chen, Sneha Silwal, Aryan Abbeel, Jitendra Malik, and Trevor Darrell. Real-world
Jain, Vincent-Pierre Berges, Pieter Abbeel, Jitendra robot learning with masked visual pre-training. CoRL,
Malik, et al. Where are we in the search for an artificial 2022. 1, 2, 4, 5
visual cortex for embodied intelligence? arXiv preprint [71] Nicholas Rhinehart and Kris M Kitani. Learning actionmaps of large environments via first-person vision. In learning: An introduction. MIT press, 2018. 1
CVPR, 2016. 3 [87] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu
[72] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. Frankmo- Lin,andHonglakLee. Decomposingmotionandcontent
cap:Amonocular3dwhole-bodyposeestimationsystem for natural video sequence prediction. arXiv preprint
via regression and integration. In Proceedings of the arXiv:1706.08033, 2017. 3
IEEE/CVFInternationalConferenceonComputerVision [88] Carl Vondrick, Deniz Oktay, Hamed Pirsiavash, and
(ICCV) Workshops, pages 1749–1759, October 2021. 2, Antonio Torralba. Predicting motivations of actions by
3, 4, 8 leveraging text. In CVPR, 2016. 3
[73] Ste´phane Ross, Geoffrey Gordon, and Drew Bagnell. A [89] Jianren Wang, Sudeep Dasari, Mohan Kumar Srirama,
reduction of imitation learning and structured prediction Shubham Tulsiani, and Abhinav Gupta. Manipulate
to no-regret online learning. In Proceedings of the four- by seeing: Creating manipulation controllers from pre-
teenth international conference on artificial intelligence trained representations. 2023. 2
and statistics, pages 627–635. JMLR Workshop and [90] JiayiWang,FranziskaMueller,FlorianBernard,Suzanne
Conference Proceedings, 2011. 5 Sorli, Oleksandr Sotnychenko, Neng Qian, Miguel A
[74] Anirban Roy and Sinisa Todorovic. A multi-scale cnn Otaduy, Dan Casas, and Christian Theobalt. Rgb2hands:
for affordance segmentation in rgb images. In ECCV, real-time tracking of 3d hand interactions from monoc-
2016. 3 ular rgb video. ACM Transactions on Graphics (TOG),
[75] Abraham Savitzky and Marcel JE Golay. Smoothing 39(6):1–16, 2020. 3
and differentiation of data by simplified least squares [91] William Whitney, Rajat Agarwal, Kyunghyun Cho, and
procedures. Analytical chemistry, 36(8), 1964. 16 Abhinav Gupta. Dynamics-aware embeddings. arXiv
[76] Johann Sawatzky, Abhilash Srikantha, and Juergen Gall. preprint arXiv:1908.09357, 2019. 2
Weaklysupervisedaffordancedetection. InCVPR,2017. [92] TeteXiao,IlijaRadosavovic,TrevorDarrell,andJitendra
3 Malik. Masked visual pre-training for motor control.
[77] Stefan Schaal. Is imitation learning the route to arXiv preprint arXiv:2203.06173, 2022. 7
humanoid robots? Trends in cognitive sciences, 3(6): [93] Yufei Ye, Xueting Li, Abhinav Gupta, Shalini De Mello,
233–242, 1999. 5 Stan Birchfield, Jiaming Song, Shubham Tulsiani, and
[78] Stefan Schaal et al. Learning from demonstration. Sifei Liu. Affordance diffusion: Synthesizing hand-
Advances in neural information processing systems, object interactions. In CVPR, 2023. 3
pages 1040–1046, 1997. 1 [94] Tianhe Yu, Deirdre Quillen, Zhanpeng He, R. Julian,
[79] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jas- Karol Hausman, Chelsea Finn, and S. Levine. Meta-
mine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, world: A benchmark and evaluation for multi-task and
and Google Brain. Time-contrastive networks: Self- meta reinforcement learning. In CoRL, 2019. 16
supervised learning from video. In 2018 IEEE interna- [95] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan
tional conference on robotics and automation (ICRA), Welker, Jonathan Chien, Maria Attarian, Travis Arm-
pages 1134–1141. IEEE, 2018. 1 strong, Ivan Krasin, Dan Duong, Vikas Sindhwani, and
[80] Rutav M Shah and Vikash Kumar. Rrl: Resnet as Johnny Lee. Transporter networks: Rearranging the
representation for reinforcement learning. In ICML, visual world for robotic manipulation. CoRL, 2020. 4
2021. 4 [96] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and
[81] Dandan Shan, Jiaqi Geng, Michelle Shu, and David LucasBeyer.Scalingvisiontransformers.InProceedings
Fouhey. Understanding human hands in contact at of the IEEE/CVF Conference on Computer Vision and
internet scale. In CVPR, 2020. 2, 3, 4, 5, 7, 8, 16 Pattern Recognition, pages 12104–12113, 2022. 3
[82] Kenneth Shaw, Shikhar Bahl, and Deepak Pathak. [97] Bingchen Zhao, Haoqin Tu, Chen Wei, Jieru Mei, and
Videodex: Learning dexterity from internet videos. In Cihang Xie. Tuning layernorm in attention: Towards
CoRL, 2022. 2 efficient multi-modal llm finetuning. arXiv preprint
[83] Kenneth Shaw, Ananye Agarwal, and Deepak Pathak. arXiv:2312.11420, 2023. 4
Leaphand:low-cost,efficient,andanthropomorphichand [98] Yibiao Zhao and Song-Chun Zhu. Scene parsing by
for robot learning. RSS, 2023. 8 integrating function, geometry and appearance models.
[84] MohitShridhar,LucasManuelli,andDieterFox. Cliport: In CVPR, 2013. 3
What and where pathways for robotic manipulation. In [99] Huiyu Zhou, Yuan Yuan, and Chunmei Shi. Object
CoRL, 2022. 4 tracking using sift features and mean shift. Computer
[85] Shuran Song, Andy Zeng, Johnny Lee, and Thomas vision and image understanding, 113(3):345–352, 2009.
Funkhouser. Graspinginthewild:Learning6dofclosed- 3
loop grasping from low-cost demonstrations. IEEE [100] Wenxuan Zhou, Bowen Jiang, Fan Yang, Chris Paxton,
Robotics and Automation Letters, 5(3):4978–4985, 2020. and David Held. Hacman: Learning hybrid actor-critic
2 maps for 6d non-prehensile manipulation. In 7th Annual
[86] Richard S Sutton and Andrew G Barto. Reinforcement Conference on Robot Learning, 2023. 4[101] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Phillip
Kra¨henbu¨hl,andIshanMisra.Detectingtwenty-thousand
classes using image-level supervision. arXiv preprint
arXiv:2201.02605, 2022. 8
[102] Yixin Zhu, Chenfanfu Jiang, Yibiao Zhao, Demetri
Terzopoulos, and Song-Chun Zhu. Inferring forces and
learning human utilities from videos. In CVPR, 2016. 3
[103] Yuke Zhu, Alireza Fathi, and Li Fei-Fei. Reasoning
about object affordances in a knowledge base represen-
tation. In Computer Vision–ECCV 2014: 13th European
Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part II 13, pages 408–424. Springer, 2014.
3APPENDIX We also find that HRP (Ours) consistently boosts the
performance across all three tasks for the ego camera.
A. Robot Controller Details
D. Ablation Breakdown
Franka: We use a 7-DOF Franka Emika Panda robot arm
with a parallel gripper, operating in delta end-effector action
space. We use a VR-based teleoperation system to collect TABLE VII: Fine-Tuning Ablation Breakdown
expert demos on Franka.
Initial Finetuning
xArm:Weusea6-DOFxArmrobotarmwithaparallelgripper, Representation Scheme Toasting Pouring Stacking Avg.(Real)
operating in absolute end-effector action space. We use an off- Ego4D AllWeights 0.92 0.51 0.77 0.73
the-shelf hand tracking system to collect expert demos on LayerNorm(Ours) 0.83 0.60 0.77 0.73
xArm. ImageNet AllWeights 0.82 0.34 0.63 0.60
LayerNorm(Ours) 0.75 0.48 0.70 0.64
Dexterous Hand: We use a 6-DOF xArm robot arm with
CLIP AllWeights 0.23 0.27 0.13 0.21
a custom dexterous hand, operating in absolute end-effector LayerNorm(Ours) 0.50 0.39 0.57 0.48
space. DINO AllWeights 0.57 0.39 0.40 0.45
For each task, the expert gets to practice for 30 to 60 mins LayerNorm(Ours) 0.67 0.57 0.50 0.58
before collecting the demonstrations. We collect 50 expert MVP AllWeights 0.45 0.39 0.47 0.43
LayerNorm(Ours) 0.73 0.44 0.63 0.60
demonstrations for each of the tasks.
VC1 AllWeights 0.52 0.41 0.47 0.47
LayerNorm(Ours) 0.83 0.34 0.53 0.57
B. Front Cam: Full Task Performance Breakdown
TABLE VIII: Loss Ablation Performance Breakdown
TABLE V: Front Cam Performance Breakdown
Initial Avg.
Initial Condition Toasting Pouring Stacking
Method Toasting Pouring Stacking Avg.(Real) Representation (Real)
Representation
Ego4D Baseline 0.58 0.36 0.60 0.51 Ego4D NoContact 0.65 0.34 0.5 0.50
Ours 0.83 0.60 0.77 0.73 NoObject 0.425 0.42 0.3 0.38
NoHand 0.625 0.48 0.4 0.50
ImageNet Baseline 0.53 0.45 0.47 0.48
Ours 0.9 0.66 0.75 0.77
Ours 0.75 0.48 0.70 0.64
Imagenet NoContact 0.625 0.64 0.7 0.66
CLIP Baseline 0.28 0.33 0.33 0.32
NoObject 0.525 0.52 0.55 0.53
Ours 0.50 0.39 0.57 0.48
NoHand 0.525 0.3 0.7 0.51
DINO Baseline 0.38 0.32 0.40 0.37 Ours 0.8 0.62 0.7 0.71
Ours 0.67 0.57 0.50 0.58
VC-1 NoContact 0.625 0.48 0.75 0.62
MVP Baseline 0.27 0.41 0.47 0.38 NoObject 0.225 0.38 0.65 0.42
Ours 0.73 0.44 0.63 0.60 NoHand 0.5 0.44 0.4 0.45
VC1 Baseline 0.52 0.33 0.57 0.47 Ours 0.525 0.44 0.8 0.59
Ours 0.83 0.34 0.53 0.57
Note: do not compare numbers between Table VIII and
We observe that HRP (Ours) consistently boosts the perfor- the other tables. The loss ablation experiments were run on
mance across all three tasks for the front cam. a separate day, so all numbers were re-ran on that day. This
was done to ensure a proper A/B comparison between the all
C. Ego Cam: Full Task Performance Breakdown methods in this table.
E. Loss Weighting Sweep
TABLE VI: Ego Cam Performance Breakdown
We swept through a range of weights for each of the losses
to narrow down on a particular set of loss weights for HRP
Initial
Method Toasting Pouring Stacking Avg.(Real)
Representation (presented in Table IX). These were based on relative orders
Ego4D Baseline 0.2 0.12 0.3 0.21 of magnitude of the ground truth labels in the dataset. We
Ours 0.2 0.22 0.45 0.29 empirically saw that increasing the loss weights by more than
ImageNet Baseline 0.3 0.3 0.45 0.35 0.5 negatively affected performance and led to collapse.
Ours 0.6 0.48 0.7 0.59
CLIP Baseline 0.2 0 0 0.07 TABLE IX: Wepresentthedifferentaffordancelossweightsweran
Ours 0.275 0.02 0 0.1 sweeps on.
DINO Baseline 0.35 0.32 0.3 0.32
Exp Loss Weights
Ours 0.45 0.7 0.55 0.57
MVP Baseline 0.175 0.32 0.45 0.32 HRP λ obj =0.05, λ ct =0.005, λ hand =0.5
Ours 0.3 0.4 0.65 0.45 Drop Contact Only λ =0.05, λ =0, λ =0.5
obj ct hand
VC1 Baseline 0.5 0.28 0.4 0.39 Drop Object Only λ obj =0, λ ct =0.005, λ hand =0.5
Ours 0.55 0.6 0.65 0.6 Drop Hand Only λ =0.05, λ =0.005, λ =0
obj ct handTABLE X: Sim Performance
F. Data Pipeline Description
Initial
To obtain human data, we first extract video clips from Method MetaWorld Avg Performance
Representation
Ego4D [32]. Our dataset contains approximately 1200 videos.
Each video is broken down semantically into smaller clips Ego4D Baseline 0.656
by human annotators (as a part of the Ego4D). Our clips are Ours 0.580
between 1 and 30 seconds. For a given clip, we pass every
ImageNet Baseline 0.556
frame through the 100 DOH model [81], which gives us hand
Ours 0.664
object contact information. These are {h ,h ,o ,o ,c ,c }. h
l r l r l r
are the hand bounding boxes, o are the object bounding boxes CLIP Baseline 0.444
(which are in contact with the hand). c are contact variable (i.e. Ours 0.408
fixed, portable, self or no contact). We only look at contacts DINO Baseline 0.660
with fixed and portable. r or l represents the left or right hand. Ours 0.664
Active object and hand trajectories used for our representations
MVP Baseline 0.592
are directly used. For contact points, it is assumed that at the
Ours 0.640
start of the clip there is no contact, from where we find the
frame of first contact t. Since per-frame predictions are noisy, VC1 Baseline 0.576
werunafilter[75]overthepredictions.Fromthecontactframe, Ours 0.648
we obtain the hand-bounding box h and object bounding o.
Contact points are computed in the intersection of h and o,
I. Evaluation
and the exterior of the hand. This exterior is obtained via skin
For each task, we run around 50 trials (per model), at
segmentation (similar to [2, 53]. These contacts can then be
various initial poses (for objects) and with different variations
projected to previous frames in the clips by the homography
in objects. In every task, about half the trials are from the
matrix H obtained via SIFT features.
t
training distribution and half are from the test. The differences
in objects include different colors, shapes, and even semantic
G. Behavior Cloning Hyper-Parameters differences: for example in the toasting task, plush toys were
tested instead of the vegetables used to train. Cups or bowls
Welistthehyper-paramatersthatweusedforpolicytraining weretested,insteadofmugsthatwereusedtotrainthepouring
usingbehavior-cloninginthissection.AsshowninFigure4,we task, etc.
pass an image through the learned HRP visual representation Lighting is not controlled between train and test. We did try
to obtain a 768-dimensional latent vector. This latent vector torunallbaselinesandmethodsascloselytogetheraspossible
is passed through a two-layer MLP with (512, 512) hidden to avoid any confounding factors: i.e. for every trial, we ran
layer dimensions. To the output of the MLP we apply RELU all the baselines and our method together. Across trials, we
activation along with dropout regularization with prob=0.2 to allowed for variation in lighting conditions.
estimate the mean (µ), the mixing parameters (ϕ), and the The results presented in the paper are the average of the
standard deviation (σ) of a Gaussian Mixture Model (GMM) successes, on a scale from 0 to 1. We present the criteria for
distribution with 5 modes. success in each task:
We choose GMM model based on prior work [59] that • Stacking: 1 if the robot correctly picks and stacks the
showed its crucial role in increasing BC performance. We red block, and 0.5 if the red block is unstably placed on
use ADAM optimizer [48] with the learning rate set to 1e-4, the green block.
l2 weight decay also set to 1e-4. We train policy for 50K • Pouring: The fraction of candies, out of 5, successfully
iterations. We also apply data augmentation (random crop and poured (e.g., 2/5 candies poured →0.4 success).
random blur) for the input images. We use the same set of • Toasting: 1 if the whole task is completed, and 0.5
hyper-parameters for both the real-world and the simulation successful if the robot only successfully places the object.
tasks. • Pot on Stove: 1 if the food is correctly placed in the pot.
• Hand Lift Cup 1 if the cup is stably grasped and picked.
Wealsocomputethestandarderrorforthesetrialsandshow
H. Simulation Results
that as our confidence in Tables 1-3, and as an error bar in
For simulation tasks, we choose 5 tasks from the Meta- Figures 5-7.
world [94] benchmark namely: BinPick, ButtonPress, Ham-
mering, Drawer Opening, and Assembly. This benchmark is
extensivelyusedbytherobotlearningcommunity.Weusedthe
same camera viewpoint, object sets, and expert demonstrations
asusedbypriorwork [57].Wereporttheaverageperformance
on all 5 tasks in table X.