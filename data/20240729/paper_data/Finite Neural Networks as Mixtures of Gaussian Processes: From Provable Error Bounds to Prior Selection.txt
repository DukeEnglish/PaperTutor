Finite Neural Networks as Mixtures of Gaussian Processes: From
Provable Error Bounds to Prior Selection
Steven Adams s.j.l.adams@tudelft.nl
Delft Center for Systems and Control
Technical Universiy of Delft
Delft, 2628 CD, The Netherlands
Andrea Patan`e apatane@tcd.ie
School of Computer Science and Statistics
Trinity College Dublin
Dublin 2, Ireland
Morteza Lahijanian morteza.lahijanian@colorado.edu
Department of Aerospace Engineering Sciences and Computer Science
University of Colorado Boulder
Boulder, CO 80303, USA
Luca Laurenti l.laurenti@tudelft.nl
Delft Center for Systems and Control
Technical Universiy of Delft
Delft, 2628 CD, The Netherlands
Abstract
Infinitely wide or deep neural networks (NNs) with independent and identically distributed (i.i.d.)
parameters have been shown to be equivalent to Gaussian processes. Because of the favorable
propertiesofGaussianprocesses,thisequivalenceiscommonlyemployedtoanalyzeneuralnetworks
and has led to various breakthroughs over the years. However, neural networks and Gaussian
processesareequivalentonlyinthelimit; inthefinitecasetherearecurrentlynomethodsavailable
toapproximateatrainedneuralnetworkwithaGaussianmodelwithboundsontheapproximation
error. In this work,wepresentanalgorithmicframeworkto approximatea neuralnetworkoffinite
width and depth, and with not necessarily i.i.d. parameters, with a mixture of Gaussian processes
witherrorboundsontheapproximationerror. Inparticular,weconsidertheWassersteindistanceto
quantifytheclosenessbetweenprobabilisticmodelsand,byrelyingontoolsfromoptimaltransport
and Gaussian processes, we iteratively approximate the output distribution of each layer of the
neural network as a mixture of Gaussian processes. Crucially, for any NN and ǫ>0 our approach
is able to return a mixture of Gaussian processes that is ǫ-close to the NN at a finite set of input
points. Furthermore, we rely on the differentiability of the resulting error bound to show how our
approach can be employed to tune the parameters of a NN to mimic the functional behavior of a
givenGaussianprocess,e.g.,forpriorselectioninthecontextofBayesianinference. Weempirically
investigatetheeffectivenessofourresultsonbothregressionandclassificationproblemswithvarious
neuralnetworkarchitectures. Ourexperimentshighlighthowourresultscanrepresentanimportant
step towardsunderstanding neuralnetwork predictions and formally quantifying their uncertainty.
Keywords: Neural Networks, Gaussian Processes, Bayesianinference, Wasserstein distance
©2024StevenAdams,AndreaPatan`e,MortezaLahijanian,LucaLaurenti.
License: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/.
4202
luJ
62
]GL.sc[
1v70781.7042:viXraAdams, Patan`e, Lahijanian and Laurenti
1 Introduction
Deep neuralnetworks have achieved state-of-the-art performancein awidevariety of tasks, ranging
fromimageclassification(Krizhevsky et al.,2012)toroboticsandreinforcementlearning(Mnih et al.,
2013). In parallel with these empirical successes, there has been a significant effort in trying to
understandthetheoretical properties ofneuralnetworks (Goodfellow et al.,2016)andtoguarantee
their robustness (Szegedy et al., 2013). In this context, an important area of research is that of
stochastic neural networks (SNNs), where some of the parameters of the neural network (weights
and biases) are not fixed, but follow a distribution. SNNs include many machine learning models
commonlyusedinpractice,suchasdropoutneuralnetworks(Gal and Ghahramani,2016),Bayesian
neuralnetworks (Neal, 2012), neuralnetworks with only a subsetof stochastic layers (Favaro et al.,
2023), and neural networks with randomized smoothing (Cohen et al., 2019). Among these, be-
cause of their convergence to Gaussian processes, particular theoretical attention has been given
to infinite neural networks with independent and identically distributed (i.i.d.) parameters (Neal,
2012; Lee et al., 2017).
Gaussian processes (GPs) are a class of stochastic processes that are widely used as non-
parametric machine learning models (Rasmussen, 2003). Because of their many favorable ana-
lytic properties (Adler and Taylor, 2009), the convergence of infinite SNNs to GPs has enabled
many breakthroughs in the understanding of neural networks, including their modeling capabil-
ities (Schoenholz et al., 2016), their learning dynamics (Jacot et al., 2018), and their adversarial
robustness (Bortolussi et al., 2024). Unfortunately, existing results to approximate a SNN with a
GP are either limited to untrained networks with i.i.d. parameters (Neal, 2012) or lack guaran-
tees of correctness (Khan et al., 2019). In fact, the input-output distribution of a SNN of finite
depth and width is generally non-Gaussian, even if the distribution over its parameters is Gaussian
(Lee et al., 2020). This leads to the main question of this work: Can we develop an algorithmic
framework to approximate a finite SNN (trained or untrained and not necessarily with i.i.d. param-
eters) with Gaussian models while providing formal guarantees of correctness (i.e., provable bounds
on the approximation error and that can be made arbitrarily small)?
In this paper, we propose an algorithmic framework to approximate the input-output distribu-
tion of an arbitrary SNN over a finite set of input points with a Gaussian Mixture Model (GMM),
that is, a mixture of M Gaussian distributions (McLachlan and Peel, 2000). Critically, the GMM
approximation resulting from our approach comes with error bounds on its distance (in terms of
the 2-Wasserstein distance1 (Villani et al., 2009)) to the input-output distribution of the SNN. An
illustrative example of our framework is shown in Figure 1a, where, given a SNN trained on a 1D
regression task (Figure 1a), our framework outputs a GMM approximation (Figure 1b) with error
bounds on its closeness to the SNN (Figure 1c). Our approach is based on iteratively approximat-
ing the output distribution of each layer of the SNN with a mixture of Gaussian distributions and
propagating this distribution through the next layer. In order to propagate a distribution through
a non-linear activation function, we first approximate it with a discrete distribution, which we call
a signature approximation.2 The resulting discrete distribution can then be propagated exactly
through a layer of the neuralnetwork (activation function and linear combination with weights and
biases), which in the case of jointly Gaussian weights and biases, leads to a new Gaussian mixture
distribution. To quantify the error between the SNN and the resulting GMM, we use techniques
1. Note that, as we will emphasize in Section 3, the choice of the 2-Wasserstein distance to quantify the distance
betweenaSNNandaGMMguaranteesthataboundonthe2-Wassersteindistancealsoimpliesaboundsinhow
distant are their mean and variance.
2. Adiscrete approximation of acontinuousdistribution is also called acodebook in thefield of constructivequan-
tization (Graf and Luschgy, 2007) or particle approximation in Bayesian statistics (Liu and Wang, 2016).
2Finite Neural Networks as Mixtures of Gaussian Processes
(a) SNN (b) GMM (c) Error
Figure1: Visualization of (a) aMonte Carloapproximation of aSNNtrained onsamples from a1D
mixture of sines with additive noise, and (b) the GMM approximation of the same SNN composed
of a mixture of size 10, as obtained with our approach. The SNN has one hidden layer with 64
neurons and is trained using VOGN (Khan et al., 2018). The red line and blue-shaded region in
(a) and (b) illustrate the point-wise mean and standard deviation at a uniform grid of 100 points in
the input domain. In (c), the dashed and solid lines represent, respectively, the empirical estimates
of the 2-Wasserstein distance between the GMM and SNN distributions at each input point, and
formal bounds of the same quantity, as obtained by the results derived in this paper.
from optimal transport, probability theory, and interval arithmetic. In particular, for each of the
approximation steps described above, we bound the error introduced in terms of the 2-Wasserstein
distance. Then, we combine these bounds using interval arithmetic to bound the 2-Wasserstein
distance between the SNN and the approximating GMM. Furthermore, by relying on the fact that
GMMs can approximate any distribution arbitrarily well (Delon and Desolneux, 2020), we prove
that by appropriately picking the parameters of the GMM, the bound on the approximation error
converges uniformly to zero by increasing the size of the GMM. Additionally, we show that the re-
sulting error boundis piecewise differentiable in the hyper-parameters of theneural network, which
allows one to optimize the behavior of a NN to mimic that of a given GMM via gradient-based
algorithms.
We empirically validate our framework on various SNN architectures, including fully connected
and convolutional layers, trained for both regression and classification tasks on various datasets
including MNIST, CIFAR-10, and a selection of UCI datasets. Our experiments confirm that our
approach can successfully approximate a SNN with a GMM with arbitrarily small error, albeit
with increasing computational costs when the network’s depth increases. Furthermore, perhaps
surprisingly, our results show that even a mixture with a relatively small number of components
generally suffices to empirically approximate a SNN accurately. To showcase the importance of
our results, we then consider two applications: (i) uncertainty quantification in SNNs, and (ii)
prior selection for SNNs. In the former case we show how the GMM approximation resulting
from our framework can be used to study and quantify the uncertainty of the SNN predictions in
classification tasks on the MNIST and CIFAR-10 datasets. In the latter case, we consider prior
selection for neural networks, which is arguably one of the most important problems in performing
Bayesian inference with neural networks (Fortuin, 2022), and show that our framework allows one
to precisely encode functional information in the prior of SNNs expressed as Gaussian processes
(GPs),therebyenhancingSNNs’posteriorperformanceandoutperformingstate-of-the-art methods
for prior selection of neural networks.
3Adams, Patan`e, Lahijanian and Laurenti
In summary, the main contributions of our paper are:
• We introduce a framework based on discrete approximations of continuous distributions to
approximateSNNsofarbitrarydepthandwidthasGMMs, withformalerrorboundsinterms
of the 2-Wasserstein distance.
• Weprovetheuniformconvergenceofourframeworkbyshowingthatforanyfinitesetofinput
points our approach can return a GMM of finite size such that the 2-Wasserstein distance
between this distribution and the joint input-output distribution of the SNN on these points
can be made arbitrarily small.
• We perform a large-scale empirical evaluation to demonstrate the efficacy of our algorithm in
approximating SNNs with GMMs and to show how our results could have a profound impact
invariousareas ofresearch forneuralnetworks, includinguncertaintyquantification andprior
selection.
1.1 Related Works
The convergence of infinitely wide SNNs with i.i.d. parameters to GPs was first studied by Neal
(2012) by relying on the central limit theorem. The corresponding GP kernel for the one hidden
layer case was then analytically derived by Williams (1996). These results were later generalized
to deep NNs (Hazan and Jaakkola, 2015; Lee et al., 2017; Matthews et al., 2018), convolutional
layers (Garriga-Alonso et al., 2019; Novak et al., 2018; Garriga-Alonso and van der Wilk, 2021),
and general non-linear activation functions (Hanin, 2023). However, these results only hold for
infinitely wide or deep neural networks with i.i.d. parameters; in practice, NNs have finite size and
depthandfortrainedNNstheirparametersaregenerallynoti.i.d.. Topartiallyaddresstheseissues,
recentworkshavestartedtoinvestigatehowtheseresultsapplyinthefinitecase(Dyer and Gur-Ari,
2019; Antognini, 2019; Yaida, 2020; Bracale et al., 2021; Klukowski, 2022; Balasubramanian et al.,
2024). In particular, Eldan et al. (2021) were the first to provide upper bounds on the rates at
which a single layer SNN with a specific parameter distribution converges to the infinite width GP
in terms of the Wasserstein distance. This result was later generalized to isotropic Gaussian weight
distributions (Cammarota et al., 2023), deep Random NN (Basteri and Trevisan, 2024), and to
other metrics, such as the total variation, sup norm, and Kolmogorov distances (Apollonio et al.,
2023; Bordino et al., 2023; Favaro et al., 2023). However, to the best of our knowledge, all existing
worksarelimitedtountrainedSNNswithi.i.d.weightsandbiases,whichmaybetakenaspriorsina
Bayesian setting (Bishop and Nasrabadi, 2006) or may represent the initialization of gradient flows
inanempiricalriskminimizationframework(Jacot et al.,2018). Incontrast, critically, inourpaper
we allow for correlated andnon-identical distributedweights andbiases, thus also includingtrained
posterior distributions of SNNs learned via Bayesian inference. In this context, Khan et al. (2019)
showed that for a subset of Gaussian Bayesian inference techniques, the approximate posterior
weight distributions are equivalent to the posterior distributions of GP regression models, implying
a linearization of the approximate SNN posterior in function space (Immer et al., 2021). Instead,
we approximate the SNN with guarantees in function space and our approach generalizes to any
approximate inference technique.
While the former set of works focuses on the convergence of SNNs to GPs, various works have
considered the complementary problem of findingthe distribution of the parameters of a SNN that
mimic a given GP (Flam-Shepherd et al., 2017, 2018; Tran et al., 2022; Matsubara et al., 2021).
This is motivated by the fact that GPs offer an excellent framework to encode functional prior
knowledge; consequently, this problem has attracted significant interest in encoding functional
4Finite Neural Networks as Mixtures of Gaussian Processes
prior knowledge into the prior of SNNs. In particular, closely related to our setting are the works
of Flam-Shepherd et al. (2017) and Tran et al. (2022), which optimize the parametrization of the
weight-space distribution of the SNN to minimize the KL divergence and 1-Wasserstein distance,
respectively, with a desired GP prior. They then utilize the optimized weight prior to perform
Bayesian inference, showing improved posterior performance. However, these methods lack formal
guarantees on the closeness of the optimized SNN and the GP that the error bounds we derive in
this paper provide.
2 Notation
For a vector x Rn, we denote with x the Euclidean norm on Rn, and use x(i) to denote the i-th
∈ k k
element of x. Similarly, for a matrix M Rn m we denote by M the spectral (matrix) norm
×
∈ k k
of M and use M(i,j) to denote the (i,j)-th element of M. Further, ¯1 Rn and ¯1 Rm n,
n m n ×
respectively, denote a vector and matrix of ones, and I Rn n is use∈ d to denote× the∈ identity
n ×
∈
matrix of size n. Given a linear transformation function (matrix) T Rm n, the post image of a
×
∈
region Rn under T is defined as Post( ,T) = Tx x . If Rn is finite, we denote
X ⊂ X { | ∈ X} X ⊂
by the cardinality of . Further, we define 1 : Rn 0,1 as the step function over region
|X| X X → { }
1 x ,
Rn, that is, 1 (x)= ∈ X
X ⊂ X (0 otherwise.
Given a measurable space ( , ( )) with ( ) being the σ algebra, we denote with ( ) the
X B X B X − P X
set of probability distributions on ( , ( )). In this paper, for a metric space , ( ) is assumed
X B X X B X
to be the Borel σ-algebra of . Considering two measurable spaces ( , ( )) and ( , ( )), a
X X B X Y B Y
probabilitydistributionp ( ), andameasurablemappingh : ,weuseh#ptodenotethe
∈ P X X → Y
push-forwardmeasureof p by h, i.e., the measure on such that , (h#p)( )) = p(h 1( )).
−
For N N, Π = π RN : N π(i) = 1 is the NY -simplex. A∀A dis∈ crY ete probabA ility distribuA tion
∈ N { ∈ 0 i=1 }
d ( ) is defined as d =≥ N π(i)δ , where δ is the Dirac delta function centered at location
∈ P X Pi=1 ci c
c and π Π . Lastly, the set of discrete probability distributions on with at most N
N
∈ X ∈ P X
locations is denoted as ( ) ( ).
N
D X ⊂ P X
3 Preliminaries
In this section, we give the necessary preliminary information on Gaussian models and on the
Wasserstein distance between probability distributions.
3.1 Gaussian Processes and Gaussian Mixture Models
A Gaussian process (GP) g is a stochastic process such that for any finite collection of input points
= x ,...,x the joint distribution of g( ) := g(x ),...,g(x ) follows a multivariate Guas-
1 D 1 D
X { } X { }
sian distribution with mean function m and covariance function k, i.e., g( ) (m( )),k( , ))
X ∼ N X X X
(Adler and Taylor, 2009). A Gaussian Mixture Model (GMM) with M N components, is a set
∈
of M GPs, also called components, averaged w.r.t. a probability vector π Π (Tresp, 2000).
M
∈
Therefore, the probability distribution of a GMM follows a Gaussian mixture distribution.
Definition 1 (Gaussian Mixture Distribution) A probability distribution q (Rd) is called
a Gaussian Mixture distribution of size M N if q = M π(i) (m ,Σ ), whe∈ reP π Π and
∈ i=1 N i i ∈ M
m Rd and Σ Rd d are the mean and covariance matrix of the i-th Gaussian distribution in
i i ×
∈ ∈ P
the mixture. The set of all Gaussian mixture distributions with M or less components is denoted
by (Rd) (Rd).
M
G ⊂ P
5Adams, Patan`e, Lahijanian and Laurenti
One of the key properties of GMMs, which motivates their use in this paper to approximate
the probability distribution induced by a neural network, is that for large enough M they can
approximate any continuous probability distribution arbitrarily well (Delon and Desolneux, 2020).
Furthermore, being a Gaussian mixture distribution a weighted sum of Gaussian distributions, it
inherits the favorable analytic properties of Gaussian distributions (Bishop and Nasrabadi, 2006).
3.2 Wasserstein Distance
To approximate SNNs to GMMs and vice-versa, and quantify the quality of the approximation, we
need a notion of distance between probability distributions. While various distance measures are
available in the literature, in this work we consider the Wasserstein distance (Gibbs and Su, 2002).
To define the Wasserstein distance, for ρ 1 we define the ρ Wasserstein space of distributions
≥ −
(Rd) as the set of probability distributions with finite moments of order ρ, i.e., any p (Rd)
ρ ρ
P ∈ P
is such that x ρdp(x) < . For p,q (Rd), the ρ-Wasserstein distance W between p and
Rdk
k ∞ ∈
Pρ ρ
q is defined as
R
1 1
ρ ρ
W (p,q) := inf E [ x y ρ] = inf x y ργ(dx,dy) , (1)
ρ (x,y) γ
γ Γ(p,q) ∼ k − k γ Γ(p,q) Rd Rdk − k
(cid:18) ∈ (cid:19) (cid:18) ∈ Z × (cid:19)
where Γ(p,q) (Rd Rd) represents the set of probability distributions with marginal distribu-
ρ
⊂ P ×
tionspandq. ItcanbeshownthatW isametric,whichisgivenbytheminimumcost,accordingto
ρ
theρ poweroftheEuclideannorm,requiredtotransformoneprobability distributionintoanother
−
(Villani et al.,2009). Furthermore,anotherattractivepropertyoftheρ-Wassersteindistance,which
distinguishes it from other divergence measures such as the KL divergence (Hershey and Olsen,
2007), is that closeness in the ρ-Wasserstein distance implies closeness in the first ρ moments. This
result is formalized in Lemma 2 below
Lemma 2 For p,q (Rn) and ρ N it holds that
2
∈ P ∈
E
x
p[ x ρ]ρ1 E
y
q[ y ρ]ρ1 W ρ(p,q) (2)
| ∼ k k − ∼ k k | ≤
Proof Let δ be the Dirac measure centered at zero, then, by the triangle inequality, it holds that
0
W (p,δ ) W (p,q)+W (q,δ )andW (q,δ ) W (q,p)+W (p,δ ). Additionally, usingthesym-
ρ 0 ρ ρ 0 ρ 0 ρ ρ 0
≤ ≤
metryaxiomofadistancemetric,wehavethatW (q,p) = W (p,q). Then,theseinequalitiescanbe
ρ ρ
combined to W (p,δ ) W (q,δ ) W (p,q). The proof is concluded by noticing that, as shown
ρ 0 ρ 0 ρ
| − | ≤
in Villani et al. (2009), it holds that that W ρ(p,δ 0) = E
x
p[ x ρ]ρ1 and W ρ(q,δ 0) = E
x
q[ x ρ]ρ1 .
∼ k k ∼ k k
As is common in the literature, in what follows, we will focus on the 2-Wasserstein distance.
However, it is important to note that since W W , themethods presented in this work naturally
1 2
≤
extend to 1-Wasserstein distance. Detailed comparisons andpotential improvements whenutilizing
the 1-Wasserstein distance are reported in the Appendix.
6Finite Neural Networks as Mixtures of Gaussian Processes
4 Problem Formulation
In this section, we first introduce the class of neural networks considered in this paper, and then
we formally state our problem.
4.1 Stochastic Neural Networks (SNNs)
For aninputvector x Rn0, weconsiderafullyconnected neuralnetwork of K hiddenlayers fw(x)
∈
defined iteratively as follows for k 0,...,K :
∈ { }
L (z ) if k = 0,
z = x, L (z )= W z +b , z = wk k fw(x) =z , (3)
0 wk k k k k k+1
(L wk(σ k(z k)) otherwise,
K+1
where, for n being the number of neurons of layer k, we have that σ :Rnk Rnk is the vector of
k k
→
piecewise continuous activation functions (one for each neuron), W
k
Rnk+1×nk and b
k
Rnk+1 are
∈ ∈
the matrix of weights and vector of bias parameters of the k th layer of the network. We denote
−
by w = W ,b the union of all parameters of the k-th layer and by w = w K the union of all
k { k k } { k }k=0
the neural network parameters, which we simply call weights or parameters of the neural network.
L (z ) is the final output of the network, possibly corresponding to the vector of logits in case
wK K
of classification problems.
In this work, we assume that the weights w, rather than being fixed, are distributed according
to a probability distribution p .3 For any x Rn0, placing a distribution over the weights leads to
w
∈
a distribution over the outputs of the NN. That is, fw(x) is a random variable and fw(x) x Rn0 is
{ } ∈
a stochastic process, which we call a stochastic neural network (SNN) to emphasize its randomness
(Yu et al., 2021). In particular, fw(x) follows a probability distribution p , which, as shown in
nn(x)
Adams et al. (2023), can be iteratively defined over the layers of the SNN. Specifically, as shown
in Eqn. (4) below, p is obtained by recursively propagating through the layers of the NN
nn(x)
architecture the distribution induced by the random weights at each layer. The propagation is
obtained by marginalization of the output distribution at each layer with the distribution of the
previous layer.
p = E [p ]
nn(x),1 z0 ∼δx Lw0(z0)
p = E [p ], k 1,...,K (4)
nn(x),k+1 zk∼pnn(x),k Lwk(σ(zk))
∈ { }
p = p ,
nn(x) nn(x),K+1
where p = L (z)#p and δ is the Delta Dirac function centered at x. For any finite
subset
Lwk( Rz)
n0 of
iw nk
put
poiw nk
ts, we
ux
se p to denote the joint distribution of the output of fw
nn( )
X ⊂ X
evaluated at the points in .
X
Inwhatfollows, becauseofits practicalimportanceandtheavailability ofclosed-formsolutions,
we will introduce our methodological framework under the assumption that p is a multivariate
w
Gaussian distribution that is layer- or neuron-wise correlated, i.e., p = (m ,Σ ) with Σ =
w w w w
N
diag(Σ ,...,Σ ) and in the case of neuron-wise correlation Σ = diag(Σ ,...,Σ ). We
w0 wK wk wk,1 wk,nk
stress that this does not imply that the output distribution of the SNN, p (x), is also Gaussian;
nn
in fact, in general, it is not. Furthermore, we should also already remark that, as we will explain
in Subsection 6.1 and illustrate in the experimental results, the methods we present in this paper
can be extended to more general (non-necessarily Gaussian) p ().
w 2
∈ P ·
3. Thisincludesarchitectureswithbothdeterministicandstochasticweights,wherethedistributionoverdetermin-
istic weights can be modeled as a Dirac delta function.
7Adams, Patan`e, Lahijanian and Laurenti
Remark 3 The above definition of SNNs encompasses several stochastic models of importance for
applications, including Bayesian Neural Networks (BNNs) trained with Gaussian Variational Infer-
ence methods such as, e.g., Bayes by Backprop (Blundell et al., 2015) or Noisy Adam (Zhang et al.,
2018)4, NNs with only a subset of stochastic layers (Tang and Salakhutdinov, 2013), Gaussian
Dropout NNs (Srivastava et al., 2014), and NNs with randomized smoothing and/or stochastic in-
puts (Cohen et al., 2019). The methods proposed in this paper apply to all of them. In particular,
in the case of BNNs, in Section 8, we will show how our approach can be used to investigate both
the prior and posterior behavior, in which case, depending on the context, p could represent either
w
the prior or the posterior distribution of the weights and biases.
4.2 Problem Statement
Givenanerrorthresholdǫ > 0andaSNN,themainproblemweconsider,asformalizedinProblem1
below,isthatoffindingaGMMthatisǫ closeaccordingtothe2-Wasserstein distancetotheSNN.
−
Problem 1 Let Rn0 be a finite set of input points, and ǫ > 0 be an error threshold. Then,
X ⊂
for a SNN with distribution p find a GMM with distribution q such that:
nn( ) nn( )
X X
W (p ,q ) ǫ. (5)
2 nn( ) nn( )
X X ≤
In Problem 1 we aim to approximate a SNN with a GMM, thus extending existing results (Neal,
2012; Matthews et al., 2018) that approximate an untrained SNN with i.i.d. parameters with a
GP under some limit (e.g., infinite width (Neal, 2012; Matthews et al., 2018) or infinitely many
convolutional filters (Garriga-Alonso et al., 2019)). In contrast, in Problem 1 we consider both
trained and untrained SNNs of finite width and depth and, crucially, we aim to provide formal
error bounds on the approximation error. Note that in Problem 1 we consider a set of input points
and compute the 2 Wasserstein distance of the joint distribution of a SNN and a GMM on these
−
points. Thus, also accounting for the correlations between these input points.
Remark 4 While in Problem 1 we seek for a Guassian approximation of a SNN, one could also
consider the complementary problem of finding the parameters of a SNN that best approximate
a given GMM. Such a problem also has wide application. In fact, a solution to this problem
would allow one to encode informative functional priors represented via Gaussian processes to
SNNs, addressing one of the main challenges inperforming Bayesian inference with neural networks
(Flam-Shepherd et al., 2017; Tran et al., 2022). As the W satisfies the triangle inequality and
2
because there exist closed-form expressions for an upper-bound on the Wasserstein distance between
Gaussian Mixture distributions (Delon and Desolneux, 2020), a solution to Problem 1 can be readily
used to address the above mentioned complementary problem. This will be detailed in Section 7.2
and empirically demonstrated in Section 8.3.
Remark 5 Note that in Problem 1 we consider a GMM of arbitrary size. This is because such a
model can approximate anycontinuous distribution arbitrarily well, and, consequently, guarantees of
convergence to satisfy Problem 1 can be obtained, as we will prove in Section 6. However, Problem 1
could be restricted to the single Gaussian case, in which case Problem 1 reduces to finding the GP
that best approximates a SNN. However, in this case, because p is not necessarily Gaussian,
nn( )
X
the resulting distance bound between the GP and the SNN may not always be made arbitrarily small.
4. ThemethodspresentedinthispapercanbeappliedtootherapproximateinferencemethodssuchasHMC(Neal,
2012), for which p w takes theform of a categorical distribution, as explained in Subsection 6.1.
8Finite Neural Networks as Mixtures of Gaussian Processes
a b c d e f g h
(cid:543) (cid:2026) (cid:543) (cid:2026)
= 0 =1 = 2
Compression e
(cid:1863) (cid:1863) (cid:1863)
Signature b f
Activation c g
Linear a d h
Output distribution SNN at layer k ( )
,
GMM approximate of the output dis (cid:1868)t(cid:3041)ri(cid:3041)bu(cid:3051)ti(cid:3038)on SNN at layer k (
,
)
Components GMM approximation (cid:1869)(cid:3041)(cid:3041) (cid:3051) (cid:3038)
Compressed GMM approximation ( ( ))
,
Signature of compressed GMM appr (cid:1844)o(cid:3014)xim (cid:1869)a(cid:3041)t(cid:3041)io(cid:3051)n (cid:3038)(
,
)
Pushforward of signature over activation ( # (cid:1856)(cid:3041)(cid:3041),(cid:3051))(cid:3038)
(cid:2026) (cid:1856)(cid:3041)(cid:3041) (cid:3051) (cid:3038)
Figure2: Illustrationoftheiterativeapproximationprocedureoftheoutputdistributionofasingle-
input single-output SNN with two hidden layers (K = 2) and one neuron per layer (n ,n = 1) at
1 2
a single input point by a Gaussian Mixture distribution.
4.2.1 Approach Outline
Our approach to solving Problem 1 is based on iteratively approximating the output distribution
of each layer of a SNN as a mixture of Gaussian distributions and quantifying the approximation
error introduced by each operation. As illustrated in Figure 2, following the definition of p (x) in
nn
Eqn (4), the first step is to perform a linear combination of the input point x with the parameters
of the first layer (step a). Because the SNN weights are assumed to be jointly Gaussian and jointly
Gaussian random variables are closed under linear combination, this linear operation leads to a
Gaussian distribution. Then, before propagating this distribution through an activation function,
asignatureoperation ontheresultingdistributionisperformed;that is, thecontinuous distribution
is approximated into a discrete distribution (step b). After the signature approximation operation,
9Adams, Patan`e, Lahijanian and Laurenti
the resulting discrete distribution is passed through the activation function (step c) and a linear
combination with the weights of the next layer is performed (step d). Under the assumption that
the weights are jointly Gaussian, this linear operation results into a Gaussian mixture distribution
of size equal to the support of the discrete distribution resulting from the signature operation.
To limit the computational burden of propagating a Gaussian mixture with a large number of
components, a compression operation is performed that compresses this distribution into another
mixture of Gaussian distributions with at most M components for a given M (step e). After this, a
signature operation is performed again on the resulting Gaussian mixture distribution (step f), and
theprocess is repeated until thelast layer. Consequently, to construct q , the Gaussian mixture
nn(x)
approximation of p , our approach iteratively performs four operations: linear transformation,
nn(x)
compression,signatureapproximation,andpropagationthroughanactivationfunction. Toquantify
the error in our approach, we derive formal error bounds in terms of the Wasserstein distance for
eachoftheseoperationsandshowhowtheresultingboundscanbecombinedviaintervalarithmetic
to an upper bound of W (p ,q ).
2 nn(x) nn(x)
In what follows, first, as it is one of the key elements of our approach, in Section 5 we introduce
the concept of signature of a probability distribution and derive error bounds on the 2-Wasserstein
distancebetweenaGaussianmixturedistributionanditssignatureapproximation. Then,inSection
6 we formalize the approach described in Figure 2 to approximate a SNN with a GMM and derive
bounds for the resulting approximation error. Furthermore, in Subsection 6.4 we prove that q ,
nn(x)
theGaussianmixtureapproximationresultingfromourapproach,convergesuniformlytop ,the
nn(x)
distribution of the SNN, that is, at the cost of increasing the supportof the discrete approximating
distributions and the number of components of q , the 1- and 2-Wasserstein distance between
nn(x)
q and p can be made arbitrarily small. Then, in Section 7 we present a detailed algorithm
nn(x) nn(x)
of our approach. Finally, we conclude our paper with Section 8, where an empirical analysis
illustrating the efficacy of our approach is performed.
5 Approximating Gaussian Mixture Distributions by Discrete Distributions
One of the key steps of our approach is to perform a signature operation on a Gaussian Mixture
distribution, that is, a Gaussian Mixture distribution is approximated with a discrete distribution,
called a signature (step b and f in Figure 2). In Subsection 5.1 we formally introduce the notion
of the signature of a probability distribution. Then, in Subsection 5.2 we show how for a Gaussian
Mixture distribution a signature can be efficiently computed with guarantees on the closeness of
the signature and the original Gaussian mixture distribution in the W -distance.
2
&(%)
&(")
&(!) &(#)
! % " #
$ ]]( $ ]]( $ ]]( $
! % " #
Figure 3: Signature of a continuous probability distribution. The signature is defined by locations
c 4 which imply Voronoi partition 4 . The continuous distribution is discretized w.r.t.
{ i }i=1 {Ri }i=1
the partition by storing all the probability mass π(i) covered by at c .
i i
R
10Finite Neural Networks as Mixtures of Gaussian Processes
5.1 Signatures: the Discretization of a Continuous Distribution
For a set of points = c N Rd called locations, we define ∆ : Rd as the function that
assigns any z Rd C to th{ e i c} loi= se1 s⊂ t point in , i.e., C → C
∈ C
∆ (z) = argmin z c .
C c k − k
∈C
The push-forward operation induced by ∆ is a mapping from (Rd) to (Rd) and defines the
N
C P D
signature of a probability distribution. That is, as formalized in Definition 6 below, a signature
induced by ∆ is an approximation of a continuous distribution with a discrete one of supportwith
C
a cardinality N.
Definition 6 (Signature of a Probability Distribution) The signature of a probability distri-
bution p (Rd) w.r.t. points = c N Rd is the discrete distribution ∆ #p = N π(i)δ
(Rd),∈ wP here π(i) = P [z C ]{ wi i} thi=1 ⊂ C i=1 ci ∈
N z p i
D ∼ ∈ R P
= z Rd : z c z c , j N ,j = i . (6)
i i j N
R { ∈ k − k ≤ k − k ∀ ∈ 6 }
The intuition behind a signature is illustrated in Figure 3: the signature of distribution p is a
discretization of p that assigns to each location c a probability given by the probability
i
∈ C
mass of according to p. Note that partition N , as defined in Eqn (6), is the Voronoi
Ri {Ri }i=1
partition of Rd w.r.t. the Euclidean distance. Hence, the signature of a probability distribution
can be interpreted as a discretization of the probability distribution w.r.t. a Voronoi partition of
its support. In the remainder, we refer to as the locations of the signature, as the signature
C |C|
size, N as the partition of a signature, and we call ∆ the signature operation. In the next
{Ri }i=1
subsection we show how we can efficiently bound W (p,∆ #p) in the case where p is a Gaussian
2
C
mixture distribution.
Remark 7 The approximation of a continuous distribution by a discrete distribution, also called
a codebook or particle approximation, is a well-known concept in the literature (Graf and Luschgy,
2007; Ambrogioni et al., 2018; Pages and Wilbertz, 2012). The notion of the signature of a prob-
ability distribution introduced here is unique in that the discrete approximation is fully defined by
the Voronoi partition of the support of the continuous distribution, thereby connecting the concept
of signatures to semi-discrete optimal transport (Peyr´e et al., 2019).
5.2 Wasserstein Bounds for the Signature Operation
The computation of W (p,∆ #p) requires solving a semi-discrete optimal transport problem,
2
C
where we need to find the optimal transport plan from each point z to a specific location c
i
∈ C
(Peyr´e et al., 2019). Luckily, as illustrated in the following proposition, which is a direct con-
sequence of Theorem 1 in (Ambrogioni et al., 2018), for the case of a signature of a probability
distribution, the resulting transport problem can be solved exactly.
Proposition 8 For a probability distribution p (Rn), and signature locations = c N
∈ P2 C { i }i=1 ⊂
Rn, we have that
N
W2(p,∆ #p)= π(i)E [ z c 2 z ].
2 C z ∼p k − i k | ∈Ri
i=1
X =:W2
2|Ri
| {z }
11Adams, Patan`e, Lahijanian and Laurenti
According to Proposition 8, transportingthe probability mass at each point z to a discrete location
c based on the Voronoi partition of Rd guarantees that the cost z c is the smallest and
i i
k − k
leads to the smallest possible transportation cost, i.e., the optimal transportation strategy for
the Wasserstein Distance. Proposition 8 is general and guarantees that for any distribution p
∈
(Rn), W (p,∆ #p) only depends on W , the partial 2 moment of p w.r.t. regions
P2 ,..., 2 . In C the following proposition, w2 |R ei show how closed− -form expressions for W2 caR ni b∈ e
{R1 RN } 2 i
derived for univariate Gaussian distributions. This result is then generalized in Coroll|Rary 10 to
general multivariate Gaussian distributions.
Proposition 9 For p = (0,1), signature locations = c N R and associated partition
N C { i }i=1 ⊂
n with = [l ,v ] R , for each i, it holds that
{Ri }i=1 Ri i i ⊆ ∪{−∞ ∞}
N
W2(∆ # (0,1), (0,1)) = ν +(µ c )2, (7)
2 C N N i i − i
i=1
X
with,
µ = φ(l ) φ(u ), (8)
i i i
−
ν = 1+l φ(l ) u φ(u ) [φ(l ) φ(u )]2 (9)
i i i i i i i
− − −
where φ is the pdf of a standard (univariate) Gaussian distribution, i.e., φ(x) = 1 exp( x2 ).
√2π − 2
In Proposition 9, µ and ν are the mean and variance of a standard univariate Gaussian distri-
i i
bution restricted on [l ,u ]. Note that some of the regions in the partition will be unbounded.
i i
However, as the standard Gaussian distribution exponentially decays to zero for large x, i.e.,
lim exp(x2) (x 0,1) = 0, it follows that the bound in Eqn. (7) is finite even if some of
x
the
r→ eg∞
ions in
N N|
are necessarily unbounded.
{Ri }i=1
Acorollary ofProposition9isCorollary10,whereweextendProposition9togeneralmultivari-
ate Gaussian distributions under the assumption that locations are such that sets N define
C {Ri }i=1
a grid in the transformed space induced by the basis of the covariance matrix that we denote as T.
As illustrated in Figure 4, it is always possible to satisfy this assumption by taking as the image
C
of an axis-aligned grid of points under transformation T 1, where T 1 can be computed via an
− −
eigendecomposition (Kasim,2020)or Cholesky decomposition (Davis et al.,2016)of the covariance
matrix.5
Corollary 10 (of Proposition 9) For (m,Σ) (Rn), let matrix T Rn n be such that
×
N ∈ G ∈
T = diag(λ) −1 2VT where diag(λ) = VTΣV is a diagonal matrix whose entries are the eigenvalues
of Σ, and V is the corresponding orthogonal (eigenvector) matrix. Further, let = c N Rn
C { i }i=1 ⊂
be a set of signature locations on a grid in the transformed space induced by T, i.e.,
Post( ,T) m = ¯1 ¯2 ... ¯n (10)
C −{ } C ×C × ×C
with ¯j = T(j,:)( m) R the set of signature locations in the transformed space for dimension
C { C− }⊂
j. Then, it holds that
n
W2(∆ # (m,Σ), (m,Σ)) = λ(j)W2(∆ # (0,1), (0,1)). (11)
2 C N N 2 C¯j N N
j=1
X
5. GiventheeigenvaluesvectorλandeigenvectormatrixV ofacovariancematrixΣ,wecantakeT−1=Vdiag(λ).
In the case of a degenerate multivariate Gaussian, where Σ is not full rank, we can take C as the image of an
axis-aligned grid ofpointsinaspaceofdimension rank(Σ),undertransformation T−1=(Vdiag(λ)21 )(:,1:rank(Σ)).
12Finite Neural Networks as Mixtures of Gaussian Processes
Remark 11 For a multivariate Gaussian p = (m,Σ), the set of N > 1 signature locations
N C
that minimize W (∆ #p,p) is generally non-unique, and finding any such set is computationally
2
C
intractable (Graf and Luschgy, 2007). However, for univariate Gaussians, an optimal signature
placement strategy exists as outlined in Pag`es and Printems (2003) and will be used in Subsection
7.1 to construct grid-constrained signature locations for multivariate Gaussians.
C
In the case of Gaussian mixture distributions, i.e., where p = M π¯(i) (m ,Σ ), we can simply
i=1 N i i
apply a signature operation to each of the Gaussian distributions in the mixture. In particular,
P
if we call = ,..., the resulting sets of locations, where are the signature locations
1 M i
C {C C } C
corresponding to (m ,Σ ), then we have that
i i
N
M M M
W2 π¯i (m ,Σ ), π¯(i)∆ # (m ,Σ ) π¯(i)W2( (m ,Σ ),∆ # (m ,Σ )). (12)
2 N i i Ci N i i
!
≤ 2 N i i Ci N i i
i=1 i=1 i=1
X X X
That is, for a mixture of Gaussian distributions, the resulting 2-Wasserstein distance from a sig-
nature can be bounded by a weighted sum of the 2-Wasserstein distance between each of the
Gaussian distributions in the mixture and their signatures.6 In what follows, with an abuse of
notation, for a Gaussian mixture distribution p = M π¯(i) (m ,Σ ) we will refer to its signature
i=1 N i i
as ∆ #p := M π¯(i)∆ # (m ,Σ ).7
C i=1 Ci N i i P
P
!"
Figure 4: Construction of the signature of a 2D Gaussian distribution (black dots) using the
signature of a standard Gaussian distribution (blue dots) as a template. In the space induced by
T, that is, the basis of the covariance matrix (orange arrows) of the Gaussian distribution, all
dimensions of the Gaussian distribution are independent. Hence, we can take the cross-product of
the signatures of each univariate Gaussian marginal in dimension j of the transformed space with
signature locations ¯j (green stripes) as a template. The signature in the original space is then
C
obtained by taking the post image of the template under T 1. The blue and black lines represent
−
the edges of the Voronoi partition associated with the signature locations in the transformed and
original space, respectively.
6. This trivial result is a special case of Proposition 24 in the Appendix on the Wasserstein distances between
mixturedistributions.
7. Note that an alternative approach would be to apply the same signature locations to all Gaussian distributions
in the mixture. However, this would require integrating Gaussian distributions with a non-diagonal covariance
over a hyper-rectangle, for which closed-form solutions such as those derived in Corollary 10 are not available,
and this should hence beperformed via numerical integration approaches.
13Adams, Patan`e, Lahijanian and Laurenti
6 Stochastic Neural Networks as Gaussian Mixture Models
In this section, we detail and formalize our approach as illustrated in Figure 2 to iteratively ap-
proximate a SNN with a GMM. We first consider the case where = x , i.e., we approximate
X { }
the distribution of a SNN over a single input point. The extension of our results for a finite set of
input points will be considered in Section 6.3. Last, in Section 6.4, we prove that q , i.e., the
nn( )
X
Gaussian mixtureapproximation resultingfrom ourapproach, converges uniformly to p . That
nn( )
X
is, the error between p and q can be made arbitrarily small by increasing the number of
nn( ) nn( )
X X
signature points and GMM components.
6.1 Gaussian Mixture Model Approximation of a Neural Network
As shown in Eqn.(4), p , the output distribution of a SNN at inputpoint x, can berepresented
nn(x)
as a composition of K stochastic operations, one for each layer. To find a Gaussian mixture
distribution q that approximates p , our approach is based on iteratively approximating
nn(x) nn(x)
each of these operations with a GMM, as illustrated in Figure 2. Our approach can then be
formalized as in Eqn (13) below for k 1,...,K :
∈{ }
q = E [p ] (Initialization and Linear operation) (13a)
nn(x),1 z0 ∼δx Lw0(z0)
d = ∆ #R (q ) (Compression and Signature operations) (13b)
nn(x),k Ck M nn(x),k
q = E [p ] (Activation and Linear operations) (13c)
nn(x),k+1 zk∼dnn(x),k Lwk(σ(zk))
|Ck|
= π¯(i) C¯ m +m ,C¯ Σ C¯T +Σ
N i Wk bk i Wk i bk
i=1
X (cid:0) (cid:1)
q = q (Output) (13d)
nn(x) nn(x),K+1
where
Ck ⊂
Rnk is the set of signature locations for layer k, C¯
i
= I
nk+1 ⊗
σ(cT
i
) with c
i ∈
Ck,
and m ,Σ ,m , and Σ are the means and the covariances of the weight matrix W and bias
Wk Wk bk bk k
vector b k. R
M
: (Rnk) M(Rnk) is the compression operation that compresses the Gaussian
G∞ → G
mixture distribution q into a Gaussian Mixture distribution with at most M components,
nn(x),k
thus limiting the complexity of the approximation. Details on how we perform operation R using
M
moment matching will be given in Section 7.1. For the rest of this section, R (q ) can be
M nn(x),k
simply considered as a mapping from the GMM q into a GMM with a maximum size of M.
nn(x),k
Eqn. (13) consists of the following steps: q is the distribution resulting from a linear
nn(x),1
combination of the inputx with the weights at the first layer, d is the result of a compression
nn(x),k
and signature operation of q , the output of the previous layer. The approximate output
nn(x),k
distribution q at each layer k is obtained by marginalizing p w.r.t. d . We
nn(x),k+1 Lwk(σ(zk)) nn(x),k
recallthatforanyfixedz ,p isGaussian,asitisthedistributionofalinearcombination of
k Lwk(σ(zk))
vector σ(z ) with matrix W and vector b , whose components are jointly Gaussian (and Gaussian
k k k
distributions are closed w.r.t. linear combination). Consequently, as for all k, d is a discrete
nn(x),k
distribution,q isaGaussianmixturedistributionwithasizeequaltothesupportofd .
nn(x),k+1 nn(x),k
Remark 12 The approach described in Eqn (13) leads to a Gaussian mixture distribution q
nn(x)
under the assumption that the weight distribution p is Gaussian. In the more general case,
w
where p is non-Gaussian, one can always recursively approximate p for each k by a dis-
w Lwk(z)
crete distribution using the signature operation. That is, p in Eqn. (13c) is replaced with
Lwk(z)
14Finite Neural Networks as Mixtures of Gaussian Processes
q
Lwk(z)
= L wk(z)#∆ Cwk#p wk, where
Cwk
⊂
R(nk+1+1) ×(nk+1) is the set of signature locations.
Applying this additional operation,8 Eqn (13) will lead to a discrete distribution for q .
nn(x)
6.2 Wasserstein Distance Guarantees
Since q is built as a composition of K iterations of the stochastic operations in Eqn (13), to
nn(x)
bound W (p ,q ) we need to quantify the error introduced by each of the steps in Eqn (13).
2 nn(x) nn(x)
A summary of how each of the operations occurring in Eqn (13) modifies the Wasserstein distance
between two distributionsis provided in Table 1(details aregiven in AppendixA.3). Thesebounds
are composed using interval arithmetic to bound W (p ,q ) in the following theorem.
2 nn(x) nn(x)
Theorem 13 Letp be the output distribution of a SNNwith K hidden layers forinput x Rn0
nn(x)
∈
and q be a Gaussian mixture distribution built according to Eqn. (13). For k 1,...,K
nn(x)
∈ { }
iteratively define Wˆ as
2
Wˆ = 0, (14a)
2,1
Wˆ = ˆ Wˆ + Wˆ +Wˆ , (14b)
2,k+1 2,w,k σ 2,k σ 2,R,k 2,∆,k
S L L
Wˆ = Wˆ , h i (14c)
2 2,K+1
where ˆ , Wˆ and Wˆ are such that
2,w,k 2,∆,k 2,R,k
S
E Wk∼pwk[ kW
k
k2]21
≤
Sˆ 2,w,k, (15)
W (σ#R (q ),σ#∆ #R (q )) Wˆ , (16)
2 M nn(x),k Ck M nn(x),k ≤ 2,∆,k
W (q ,R (q )) Wˆ . (17)
2 nn(x),k M nn(x),k 2,R,k
≤
Then, it holds that
W (p ,q ) Wˆ (p ,q ):= Wˆ . (18)
2 nn(x) nn(x) 2 nn(x) nn(x) 2
≤
The proof of Theorem 13 is reported in Appendix A.3 and is based on the triangular inequality
property of the 2-Wasserstein distance, which allows us to boundW (p ,q ) iteratively over
2 nn(x) nn(x)
the hidden layers. Note that Theorem 13 depends on quantities ˆ , Wˆ and Wˆ , which
2,w,k 2,∆,k 2,R,k
S
can be computed as follows. First, ˆ is a bound on the expected spectral norm of the weight
2,w,k
S
matrices of the k-th layer, which can be obtained according to the following Lemma.
Lemma 14 For distribution p (Rn m), we have that
W ×
∈P
1
E W ∼pW[ kW k2]1 2
≤
E W¯ ∼τM#pW[(W¯ (i,j))2] 2 + kM
k
(19)
(cid:16)Xi,j (cid:17)
where τ : Rn m Rn m is a translation defined by τ (W) =W +M and M = E [W], i.e.,
M ×
→
× M W ∼pW
τ #p is a zero mean distribution.
M W
The proof of Lemma 14 relies on the Minkowski inequality and the fact that the spectral norm
is upper bounded by the Frobenius norm, as explained in Appendix A.3. Furthermore, Wˆ
2,∆,k
can be computed as in Eqn. (12) using Corollaries 10 and 27. Lastly, Wˆ bounds the distance
2,R,k
between Gaussian mixture distributions, which can be efficiently bounded using the MW distance
2
introduced in Delon and Desolneux (2020) and formally defined below.
8. In Proposition 26 in Appendix A.3, it is shown how to formally account for the additional approximation error
introduced bythis additional signature operation.
15Adams, Patan`e, Lahijanian and Laurenti
Definition 15 (MW Distance) Let p = Np π(i) p (Rn) and q = Nq π(j) q (Rn)
2 i=1 p i ∈ GN1 j=1 q j ∈ GN1
be two Gaussian Mixture distributions. The MW -distance is defined as
2
P P
1 Np Nq 1
2 2
MW (p,q) := inf E [ x z 2] = min π(i,j)W2(p ,q ) .
2 (cid:18)γ ∈Γ(p,q) ∩G<∞(Rn ×Rn) (x,z) ∼γ k − k
(cid:19)
(cid:18)π ∈Γ(πp,πq)
i j
2 i j
(cid:19)
XX
(20)
The MW distance is a Wasserstein-type distance that restricts the set of possible couplings to
2
Gaussian mixtures. Consequently, it can be formulated as a finite discrete linear program with
N N optimization variablesandcoefficients beingtheWasserstein distancebetween themixture’s
p q
·
Gaussian components, which have closed-form expressions (Givens and Shortt, 1984).9
Operation Wasserstein Bounds Ref.
Linear
W 2(E
x
∼p[p Lw(x)],E
z
∼q[p Lw(z)])
≤
E
(W,b)
∼pw[ kW k2]1 2W 2(p,q) Prop. 25
Activation
W2(σ#p,σ#∆ #p) |C| 2 W2 Corol. 27
W2 (σ#p,σ#q)C ≤ i W=1L (pσ ,|R q)i 2 |Ri
2
≤
LPσ 2
Signature/
h ∆ ,R
Compression ∈ { C R }
W (p,h#q) W (p,q))+W (q,h#q) Prop. 28
2 2 2
≤
Table 1: Summary of the bounds on the Wasserstein distance between a distribution p (Rn)
2
∈ P
and q (Rn) obtained by pushing forward p through the most common SNN operation types.
2
∈ P
Here, and are, respectively, the global Lipschitz constant and the local Lipschitz constant
in regL ioσ n L ofσ |R thi e piecewise continuous activation function σ,10 while W2 is as defined in
Ri 2 i
Proposition 8. The propositions and their proofs are provided in Appendix A.|3R.
6.3 Approximation for Sets of Input Points
We now consider the case where = x ,...,x , that is, a finite set of input points, and extend
1 D
X { }
our approach to compute a Gaussian mixture approximation of p . We first note that p
nn( ) nn( )
X X
can be equivalently represented by extending Eqn. (4) as follows, where in the equation below
vec( ) = (xT,...,xT)T is the vectorization of
X 1 D X
p = E [p ],
nn( X),1 z0 ∼δ vec(X) LD w0(z0)
p = E [p ], k 1,...,K , (21)
nn( X),k+1 zk∼pnn(X),k LD wk(σ(zk)) ∈ { }
p = p ,
nn( ) nn( ),K+1
X X
whereLD
wk
:RD ·nk
→
RD ·nk isthestackingofDtimesL k,i.e.,LD wk((z 1T,...,z DT)T)= (L wk(z 1)T,...,
L (z )T)T,andp = LD (z)#p . Thatis,p iscomputedbystackingDtimestheneural
wk D LD w(z) wk wk nn( X)
9. Forp =N(m ,Σ )∈G(Rn)and p =N(m ,Σ )∈G(Rn),it holdsthatW2(p ,p )=km −m k2+trace(Σ +
1 1 1 2 2 2 2 1 2 1 2 1
1 1 1
Σ 2−2(Σ 12Σ 2Σ 12)2).
10. ForReLU,L
σ|Ri
=1forregionsR ioverlappingwiththepositivehalfplain,andzeroelse. Fortanh,L
σ|Ri
∈(0,1].
16Finite Neural Networks as Mixtures of Gaussian Processes
network for the inputs in . Following the same steps as in the previous section, p can then
nn( )
X X
be approximated by the Gaussian Mixture distribution q , for k 1,...,K defined as
nn( )
X ∈ { }
q = E [p ] (Initialization and Linear operation) (22a)
nn( X),1 z0 ∼δ vec(X) LD w0(z0)
d = ∆ #R (q ) (Compression and Signature operations) (22b)
nn( X),k Ck M nn( X),k
q = E [p ] (Activation and Linear operations) (22c)
nn( X),k+1 zk∼dnn(X),k LD wk(σ(zk))
q = q (Output) (22d)
nn( ) nn( ),K+1
X X
where
k
RD ·nk are the signature locations at layer k, and M N is the size of compressed
C ⊂ ∈
mixtures. A corollary of Theorem 13 is the following proposition that bounds the approximation
error of q .
nn( )
X
Proposition 16 Let p be the jointdistribution of a SNNwith K hidden layers overa finite set
nn( )
of input points = x DX Rn0 and q be a Gaussian mixture distribution built according to
Eqn. (22). ForX k { 1,i .} .i .= ,1 K⊂ iterativelyn dn( eX fi) ne Wˆ as in Eqn. (14) with ˆ , Wˆ and Wˆ
2 2,w,k 2,∆,k 2,R,k
∈ { } S
such that
D1 2E Wk∼pwk[ kW k k2]1 2
≤
Sˆ 2,w,k, (23)
W (σ#R (q ),σ#∆ #R (q )) Wˆ , (24)
2 M nn( X),k Ck M nn( X),k ≤ 2,∆,k
W (q ,R (q )) Wˆ . (25)
2 nn( ),k M nn( ),k 2,R,k
X X ≤
Then, it holds that
W (p ,q ) Wˆ (p ,q ) := Wˆ .
2 nn( ) nn( ) 2 nn( ) nn( ) 2
X X ≤ X X
In the above proposition, we can set ˆ as in the single-point case, scaled by the square root
2,w,k
S
of the number of input points. Similar to the single point-case, Wˆ can be computed as in
2,∆,k
Eqn. (12) using Corollaries 10 and 27, and Wˆ can be taken as the MW distance between
2,R,k 2
GMMs q and R (q ). Note that the effect of multiple input points on the error
nn( ),k M nn( ),k
introduced bX y Wˆ and Wˆ X depends on the specific choice of points in . For points in
2,∆,k 2,R,k
X X
where q is highly correlated, the error will be similar to that of a single-input case. However,
nn( ),k
X
if there is little correlation, the error bound will increase linearly with the number of points.
6.4 Convergence Analysis
In Theorem 13 and Proposition 16 we derived error bounds on the distance between q and
nn( )
X
p . In the following theorem, we prove that if one allows for the size of q to be arbitrarily
nn( ) nn(x)
X
large, then the error converges to zero.
Theorem 17 Let p be the output distribution of a SNN with K hidden layers for a finite set
nn( )
of inputs Rn0, q X be a Gaussian mixture distribution built according to Eqn. (22), and
nn( )
Wˆ be theX up⊂ per bound onXW (p ,q ) defined according to Eqn. (14). Then, for any ǫ > 0,
2 2 nn( ) nn( )
X X
there exist sets of signature locations ,..., of finite sizes, and a compression size M, such that
1 K
Wˆ ǫ. C C
2
≤
The proof of Theorem 17 is based on iteratively showing that for any k, Wˆ in Eqn. (14b)
2,k
can be made arbitrarily small. To do that, we extend the results in Graf and Luschgy (2007)
and show that for a GMM q its 2-Wasserstein distance from the signature approximation
nn( ),k
X
∆ #q can be made smaller than any ¯ǫ > 0 by selecting signature points for each
{C1,...,
C|C|}
nn( X),k Ci
17Adams, Patan`e, Lahijanian and Laurenti
elementiascenterpointsofagridpartitioningacompactset sothat z 2dq (z) ǫ¯/2.
R nn( ),k
X \X k k X ≤
Note that since for all k, q () it is always possible to find a compact so that the
nn(x),k ∈ P2 · R X
assumption is satisfied. Furthermore, thesignature points can always betaken to satisfy Eqn.10 in
Corollary 10, by taking, for each element, the grid to align with the basis of the covariance matrix.
Remark 18 While inthe proof of Theorem 17 we useuniform gridsforeach component of q ,
nn( ),k
in practice, adaptive, non-uniform gridding can guarantee closeness in W -distance with the sX ame
2
precision for smaller grid sizes.
7 Algorithmic Framework
In this section, the theoretical methodology to solve Problem 1 developed in the previous sections
is translated into an algorithmic framework. First, following Eqn. (22), we present an algorithm
to construct the Gaussian mixture approximation of a SNN with error bounds in terms of the 2-
Wasserstein distance, including details on the compression step. Then, we rely on the fact that the
error bound resulting from our approach is piecewise differentiable with respect to the parameters
of theSNN to address the complementary problem highlighted in Remark 4. This problem involves
deriving an algorithm to optimize the parameters of a SNN such that the SNN approximates a
given GMM.
Algorithm 1: Gaussian Mixture Approximation of a SNN
input : p
nn( )
output: q X ; Wˆ (p ,q )
nn( ) 2 nn( ) nn( )
X X X
begin
1 Initialize q as in Eqn. (22a) and Wˆ as in Eqn. (14a)
nn( ),1 2,1
X
for k 1,...,K do
∈ { }
2 Compress q using Algorithm 3 [Eqn. (22b)]
nn( ),k
3 Compute Wˆ X using the MW distance [Eqn. (25)]
2,R,k 2
4 Get signature d of R (q ) and Wˆ via Alg. 2 [Eqns. (22b) & (24)]
nn( ),k M nn( ),k 2,∆,k
X X
5 Construct q as in Eqn. (22c)
nn( ),k+1
6 Compute ˆ X using Lemma 14 [Eqn. (23)]
2,w,k
S
7 Compute Wˆ as in Eqn. (14c)
2,k+1
8 Set q = q and Wˆ (p ,q )= Wˆ [Eqns. (22d) & (14c)]
nn( ) nn( ),K+1 2 nn( ) nn( ) 2,K+1
X X X X
7.1 Construction of the GMM Approximation of a SNN
The procedure to build a Gaussian mixture approximation q of p , i.e. the output distri-
nn( ) nn( )
X X
bution of a SNN at a set of input points as in Eqn (22), is summarized in Algorithm 1. The
X
algorithm consists of a forward pass over the layers of the SNN. First, the Gaussian output dis-
tribution after the first linear layer is constructed (line 1). Then, for each layer k > 0, q is
nn( ),k
X
compressed to a mixture of size M (line 2); the signature of R (q ) is computed (line 4); and
M nn( )
X
thesignature is passedthrough theactivation and linear layer to obtain q (line5). Parallel
nn( ),k+1
X
to this, following Proposition 16, error bounds on the 2-Wasserstein distance are computed for the
compression operation (line 3), signature and activation operation (line 4), and linear operation
(line 6), which are then composed into a bound on W (p ,q ) that is propagated
2 nn( ),k+1 nn( ),k+1
X X
to the next layer (line 8).
18Finite Neural Networks as Mixtures of Gaussian Processes
Signature Operation The signature operation on q in line 4 of Algorithm 1 is performed
nn( ),k
X
according to the steps in Algorithm 2. That is, the signature of q is taken as the mixture
nn( ),k
X
of the signatures of the components of q (lines 1-7). For each Gaussian component of the
nn( ),k
X
mixture, to ensure an analytically exact solution for the Wasserstein distance resulting from the
signature, we position its signature locations on a grid that aligns with the covariance matrix’s
basis vectors, i.e., the locations satisfy the constraint as in Eqn. (10) of Corollary 10. In particular,
the grid of signatures is constructed so that the Wasserstein distance from the signature operation
is minimized. According to the following proposition, the problem of finding this optimal grid can
be solved in the transformed space, in which the Gaussian is independent over its dimensions.
Proposition 19 For (m,Σ) (Rn), let matrix T Rn ×n be such that T = diag(λ) −1 2VT
N ∈ P ∈
where diag(λ) = VTΣV is a diagonal matrix whose entries are the eigenvalues of Σ, and V is the
corresponding orthogonal (eigenvector) matrix. Then,
argmin W (∆ # (m,Σ), (m,Σ)) = ¯(N ),..., ¯(N ) ,
2 Post(¯1 ... ¯n,T−1)+ m N N {C 1∗ C n∗ }
¯1,...,¯n R, C × ×C { }
{Cn C¯j} =⊂
N
Pi=1|C |
where
¯(N) = argmin W (∆ # (0,1), (0,1)) (26)
2
C R, =N C N N
C∈ |C|
n
N ,...,N = argmin λ(j)W (∆ # (0,1), (0,1)). (27)
{
1∗ n∗
} {N Q1 n j,. =.. 1,N ,Nn j} =∈ NNn, Xj=1
2 C¯(Ni)
N N
Following Proposition 19, the optimal grid of signature locations of any Gaussian is defined by
the optimal signature locations for a univariate Gaussian (Eqn. (26)) and the optimal size of
each dimension of the grid (Eqn. (27)). While the optimization problem in Eqn. (26) is non-
convex, it can be solved up to any precision using the fixed-point approach proposed in Kieffer
(1982). In particular, we solve the problem once for a range of grid sizes N and store the optimal
locations in a lookup table that is called at runtime. To address the optimization problem in
Eqn. (27), we use that W (∆ # (0,1), (0,1)), with optimal locations ¯(N) as in Eqn. (26),
isstrictlydecreasingforin2 creaC¯ s( iN ng) NN sothatN evenforlargeN,thenumberoffeC
asiblenon-dominated
candidates N ,...,N is small.
1 n
{ }
Compression Operation Our approach to compress a GMM of size N into a GMM of size
M < N, i.e., operation R in Eqn.(13b), is described in Algorithm 3 andis based on theM-means
M
clustering using Lloyd’s algorithm (Lloyd, 1982) on the means of the mixture’s components (line
1). That is, each cluster is substituted by a Gaussian distribution with mean and covariance equal
to those of the mixture in the cluster (line 2).
Remark 20 For the compression ofdiscrete distributions, suchasinthe case ofBayesian inference
via dropout, we present a computationally more efficient alternative procedure to Algorithm 3 in
Section B of the Appendix. Note that in the dropout case, the support of the multivariate Bernoulli
distribution representing the dropout masks at each layer’s input grows exponentially with the layer
width, making compression of paramount importance in practice.
19Adams, Patan`e, Lahijanian and Laurenti
Algorithm 2: Signatures of Gaussian Mixtures
input : M π¯(i) (m ,Σ )
i=1 N i i
output: d = N π(j)δ ; Wˆ
P j=1 cj 2,∆
begin
P
Construct the signature of the components of the mixture using the (fixed and optimal)
signature of (0,1) with locations :
1d
N C
for i 1,...,M do
∈ { }
1 Compute eigenvalues vector λ and eigenvector matrix V of Σ
i i i
2 Find the optimal grid sizes N ,...,N for λ according to Eqn. (26)
{
1∗ n∗
}
i
3 Take grid ¯= ¯(N ) ... ¯(N ) , with ¯(N ) as in Eqn. (27)
1 n i
C C × ×C C
4 Transform the grid to the original space: i = Post(¯,S)+m i with S = V idiag(λ i)1 2
C C
5 Define d i = j|C =i | 1π(j)c j where c j
∈
Ci and π(j) = P z ∼N(mi,Σi)[z
∈
Rj]
6 Compute Wˆ P2,∆,i = | jC =| 1Lσ |Rjπ(j)Wˆ 2 |Ri with Wˆ 2 |Rj = n l=1λ i(l) (ν j(l) +(µ j(l) −c j(l) ))
(l) (l)
with µ as in Eqn. (8) and ν as in Eqn. (9) of Prop. 10
j P j P
7 Set d = M π¯(i)d and Wˆ2 = M π¯(i)W2
i=1 i 2,∆ i=1 2,∆,i
P P
Algorithm 3: Compress Gaussian Mixture
input : q¯= M¯ π¯(i) (m¯ ,Σ¯ )
i=1 N i i
output: q = M π(j) (m ,Σ )
Pj=1 N j j
begin
P
1 Collect the components of q¯in M clusters by applying M-means clustering to
m¯ ,...,m¯ using Lloyd’s Algorithm
{
1 M¯
}
for j 1,...,M do
∈ { }
Compress all components in the j-th cluster with indices in to component
j
I
(m ,Σ ) with weight π(j) by taking the weighted average of the means and
j j
N
covariances:
2 π(j) = π¯(i); m = 1 π¯(i)m¯ ;
i j j π(j) i j i
Σ = 1 ∈I π¯(i)[Σ¯ +(m¯ ∈I m )(m¯ m )T]
j Pπ(j) i j i Pi − j i − j
∈I
P
7.2 Construction of a SNN Approximation of a GMM
The algorithmic framework presented in the previous subsection finds the Gaussian Mixture dis-
tribution that best approximates the output distribution of a SNN and returns bounds on their
distance. In this subsection, we show how our results can be employed to solve the complementary
problem of finding the weight distribution of a SNN to best match a given GMM.
Let us consider a finite set of input points Rn0 and a SNN and GMM, whose distributions
X ⊂
at are respectively p and q . Furthermore, in this subsection, we assume that the
nn( ) gmm( )
X X X
weight distribution p of the SNN is parametrized by a set of parameters ψ. For instance, in the
w
casewherep isamultivariate Gaussiandistribution,thenψ parametrizes itsmeanandcovariance.
w
Note that, consequently, also p depends on ψ through Eqn. (21). To make the dependence
nn( )
X
explicit, in what follows, we will use a superscript to emphasize the dependency on ψ. Then, our
goal is to find ψ = argmin W (pψ ,q ). To do so, we rely on Corollary 21 below, which
∗ ψ 2 nn( ) gmm( )
uses the triangle inequality to
extendXPropositX
ion 16.
20Finite Neural Networks as Mixtures of Gaussian Processes
Corollary 21 (of Proposition 16) Let Rn0 be a finite set of input points. Then, for a SNN
X ⊂
ψ
and a GMM, whose distributions at are respectively p and q , it holds that
X nn( X) gmm( X)
W (pψ ,q ) Wˆ (pψ ,qψ )+MW (qψ ,q ), (28)
2 nn( X) gmm( X) ≤ 2 nn( X) nn( X) 2 nn( X) gmm( X)
where the GMM qψ and the bound Wˆ are obtained via Algorithm 1, and MW is as defined in
nn( ) 2 2
Xψ
Definition 15, with q also depending on ψ through Eqn. (22).
nn( )
X
Using, Corollary 21, we can approximate ψ as follows
∗
ψ argmin βWˆ (pψ ,qψ )+(1 β)MW (qψ ,q ) , (29)
∗ ≈
ψ
2 nn( X) nn( X) − 2 nn( X) gmm( X)
n o
where β [0,1] allows us to trade off between the gap between the bound Wˆ (pψ ,qψ ) and
∈ 2 nn( ) nn( )
W (pψ ,qψ )andthegapbetweentheboundMW (qψ ,q )andW (qψX ,q X ).
2 nn( ) nn( ) 2 nn( ) gmm( ) 2 nn( ) gmm( )
ForinstaX nce,inX thecasewhereqψ andq areGaussiaX ndistribX utions,MW (qψX ,q X )
nn( ) gmm( ) 2 nn( ) gmm( )
equals W (qψ ,q ), leadinX g us to chooX se β < 1. As discussed next, under the aX ssumptioX n
2 nn( ) gmm( ) 2
X X
that the mean and covariance of p are differentiable with respect to ψ, the objective in Eqn. (29)
w
is piecewise differentiable with respect to ψ. Consequently, the optimization problem can be ap-
proximately solved using gradient-based optimization techniques, such as Adam (Kingma and Ba,
2014).
Let us first consider the term Wˆ in the objective, which is iteratively defined in Eqn. (14)
2
over the layers of the network using the quantities ˆ , Wˆ , and Wˆ for each layer k.
2,w,k 2,∆,k 2,R,k
S
In Algorithm 1, we compute these quantities so that the gradients with respect to the mean and
covariance of p are (approximately) analytically tractable, as shown next:
w
1. ˆ is computed according to Lemma 14 (line 6 of Algorithm 1) as the sum of the variances
2,w,k
S
ofthek-thlayer’s weightsandthespectralnormofthemeanofthek-thlayer’s weightmatrix.
Hence, the gradient with respect to the variance is well defined,and the gradient with respect
to the mean exists if the largest singular value is unique; otherwise, we can use a sub-gradient
derived from the singular value decomposition (SVD) of the mean matrix (Rockafellar, 2015).
2. Wˆ is obtained according to Algorithm 2(line 4 of Algorithm 1) based on theeigendecom-
2,∆,k
position of the covariance matrix for each component of the GMM R #q (line 1 of
M nn( ),k
Algorithm2). Differentiability ofWˆ withrespecttothemeanandcovariancX eofp follows
2,∆,k w
from the piecewise differentiability of the eigenvalue decomposition of a (semi-)positive defi-
nite matrix (Magnus and Neudecker, 2019),11 provided that the covariances of R #q
M nn( ),k
X
are differentiable with respect to the mean and covariance of p . To establish the latter,
w
note that the compression operation R , as performed according to Algorithm 3, given the
M
clustering of the components of the mixture q , reduces to computing the weighted sum
nn( ),k
X
of the means and covariances of each cluster. Hence, the covariances of R #q are
M nn( ),k
X
piecewise differentiable with respect to the means and covariances of q . Here, q
nn( ),k nn( ),k
X X
is a linear combination of p and x if k = 1, and the support of the signature approxima-
wk−1
tion of R #q otherwise (see Eqn. (13c) for the closed-form expression of q
M nn( ),k 1 nn( ),k
X − X
11. Note that there are discontinuous changes in the eigenvalues for small perturbations if the covariance matrix is
(close to) degenerate, which occurs in our cases if the behavior of the SNN is strongly correlated for points in
X. Topreventthis,weuseastablederivationimplementation fordegeneratecovariance matricesasprovidedby
Kasim (2020) to computetheeigenvalue decomposition.
21Adams, Patan`e, Lahijanian and Laurenti
in the case = x ). As such, from the chain rule, it follows that for all k the means
X { }
and covariances of R #q , and consequently, Wˆ , are piecewise differentiable with
M nn( ),k 2,∆,k
X
respect to the means and covariances of p ,...,p .
wk−1 w0
3. Wˆ is taken as the MW distance between q and R #q (line 3 in Algorithm
2,R,k 2 nn( ),k M nn( ),k
1). Given the solution of the discrete optimal tranX sport problem in EX qn. (20), the MW dis-
2
tance between two Gaussian mixtures reduces to the sum of 2-Wasserstein distances between
Gaussiancomponents, whichhaveclosed-form expressions(Givens and Shortt,1984)thatare
differentiable with respect to the means and covariances of the components. In point 2, we
showed that the means and covariances of q and R #q are piecewise differen-
nn( ),k M nn( ),k
tiable w.r.t the mean and covariance of p ; theX refore, Wˆ is piX ecewise differentiable with
w 2,R,k
respect to p .
w
From the piecewise differentiability of ˆ , Wˆ , and Wˆ with respect to the mean and
2,w,k 2,∆,k 2,R,k
S
covariance of p for all layers k, it naturally follows that Wˆ (pψ ,qψ ) is piecewise differen-
w 2 nn( ) nn( )
tiable with respect to the mean and covariance of p using simple cXhain ruXles. For the second term
w
of the objective, MW (qψ ,q ), we can apply the same reasoning from point 3 to conclude
2 nn( ) gp( )
that it is piecewise differenXtiable X with respect to the means and covariances of the components
ψ
of the GMM q , which, according to point 2, are piecewise differentiable with respect to the
nn( )
X
mean and covariance of p . Therefore, we can conclude that the objective in Eqn. (29) is piecewise
w
differentiable with respect to the mean and covariances of the weight distribution of the SNN. Note
that, in practice, the gradients can be computed using automatic differentiation, as demonstrated
in Subsection 8.3 where we encode informative priors for SNNs via Gaussian processes by solving
the optimization problem in Eqn. (29).
Remark 22 While straightforward automatic differentiation shows to handle the discontinuity in
the gradients well in practice, as analyzed in Subsection 8.3, more advanced techniques for non-
smooth optimization can be employed to solve Eqn. (29) (Ma¨kel¨a, 2002; Burke et al., 2020).
8 Experimental Results
In this section we experimentally evaluate the performance of our framework in solving Problem 1
and then demonstrate its practical usefulness in two applications: uncertainty quantification and
prior tuning for SNNs. We will start with Section 8.1, where we consider various trained SNNs
and analyze the precision of the GMM approximation obtained with our framework. Section 8.2
focuses on analyzing the uncertainty of the predictive distribution of SNNs trained on classification
datasets utilizing our GMM approximations. Finally, in Section 8.3, we show how our method can
be applied to encode functional information in the priors of SNNs.12
Datasets We study SNNs trained on various regression and classification datasets. For regres-
sion tasks, we consider the NoisySines dataset, which contains samples from a 1D mixture of
sines with additive noise as illustrated in Figure 1, and the Kin8nm, Energy, Boston Housing,
and Concrete UCI datasets, which are commonly used as regression tasks to benchmark SNNs
(Hern´andez-Lobato and Adams, 2015). For classification tasks, we consider the MNIST, Fashion-
MNIST, and CIFAR-10 datasets.
12. Ourcode is available at https://github.com/sjladams/experiments-snns-as-mgps.
22Finite Neural Networks as Mixtures of Gaussian Processes
Networks & Training We consider networks composed of fully connected and convolutional
layers, among which the VGG style architecture (Simonyan and Zisserman, 2014). We denote by
[n n ] an architecture with n fully connected layers, each with n neurons. The composition of
l n l n
×
different layer types is denoted using the summation sign. For each SNN, we report the inference
techniquesusedtotrainitsstochasticlayersinbrackets,whiledeterministiclayersarenotedwithout
brackets. For instance, VGG-2 + [1x128] (VI) represents the VGG-2 network with deterministic
weights, stacked with a fully connected stochastic network with one hidden layer of 128 neurons
learned using Variational Inference (VI). For regression tasks, the networks use tanh activation
functions, whereas ReLU activations are employed for classification tasks. For VI inference, we use
VOGN (Khan et al., 2018), and for deterministic and dropout networks, we use Adam.
Metrics for Error Analysis To quantify theprecision of ourapproach forthedifferent settings,
we report the Wasserstein distance between the approximate distribution q and the true SNN
nn( )
X
distribution p relative to the 2-nd moment of the approximate distribution. Thatis, we report
nn( )
X
W (p ,q )
2 nn( ) nn( )
W 2(p nn( X),q nn( X)):= E z ∼qnn(X X)[ kz k2X ]1 2 ,
which we refer to as the relative 2-Wasserstein distance. We use W because, as guaranteed by
2
Lemma 2, it provides an upper bound on the relative difference between the second moment of
p and q , i.e.,
nn( ) nn( )
X X
E z ∼pnn(X)[ kz k2]21 −E z˜ ∼qnn(X)[ kz˜ k2]1 2
W (p ,q ).
(cid:12)
(cid:12)
E
z˜
∼qnn(X)[ kz˜ k2]21 (cid:12)
(cid:12)
≤ 2 nn( X) nn( X)
Notethatinourframework,q isaGMM,henceE [ z 2]canbecomputedanalytically.13
nn( X) z ∼qnn(X)
k k
In our experiments, we report both a formal bound of W (p ,q ) obtained using Theorem
2 nn( ) nn( )
X X
13andanempiricalapproximation. TocomputetheempiricalapproximationofW (p ,q ),
2 nn( ) nn( )
X X
we use 1e3 Monte Carlo (MC) samples from p and q and solve the resulting discrete
nn( ) nn( )
optimal transport problem to approximate W (p X ,q ). X
2 nn(x) nn(x)
8.1 Gaussian Mixture Approximations of SNNs
In this subsection, we experimentally evaluate the effectiveness of Algorithm 1 in constructing a
GMM that is ǫ-close to the output distribution of a given SNN. We firstdemonstrate that, perhaps
surprisingly, even a small number of discretization points for the signature operation and a large
reduction in the compression operation (i.e., M relatively small), often suffice for Algorithm 1 to
generate GMMs that accurately approximate SNNs both empirically and formally. Then, we pro-
vide empirical evidence that by increasing the signature and compression sizes, the approximation
error of the GMM can be made arbitrarily small and the formal error upper bound resulting from
Theorem 13 converges to 0.
Baseline Performance We start our analysis with Table 2, where we assess the performance of
Algorithm 1 in generating GMM approximations of SNN trained using both VI and dropout. The
results show that for the SNNs trained with VI, even with only 10 signature points, Algorithm 1 is
able to generate GMMs, whose empirical distance from the true BNN is always smaller than 10 2.
−
13. Wehavethat E z∼qnn(X)[kzk2]=kmk2+trace(Σ), where m and Σ are themean vectorand covariance matrix of
q nn(X), respectively. Closed-forms for m and Σ exist for q
nn(X)
∈G <∞(Rn).
23Adams, Patan`e, Lahijanian and Laurenti
Dataset Network Architecture Emp. Formal
NoisySines [1x64] (VI) 0.00109 0.33105
[1x128] (VI) 0.00130 0.13112
[2x64] (VI) 0.00355 1.63519
[2x128] (VI) 0.01135 2.48488
Kin8nm [1x128] (VI) 0.00012 0.04243
[2x128] (VI) 0.00018 0.46303
MNIST [1x128] (VI) 0.00058 0.05263
[2x128] (VI) 0.00127 0.44873
Fashion MNIST [1x128] (VI) 0.00017 0.06984
[2x128] (VI) 0.02787 1.45466
CIFAR-10 VGG-2 + [1x128] (VI) 0.00617 0.57670
VGG-2 + [2x128] (VI) 0.04901 2.35686
MNIST [1x64] (Drop) 0.27501 0.94371
[2x64] (Drop) 0.41874 1.37594
CIFAR-10 VGG-2 (Drop) + [1x32] (VI) 0.27492 179.9631
Table2: Theempiricalestimates andformalboundsontherelative 2-Wasserstein distancebetween
various SNNs and their Gaussian Mixture approximates obtained via Algorithm 1 using signature
approximations of size 10 and a compression size (M) of 5. For the dropout networks, instead of
Algorithm 3, we use the compression procedure as described in Appendix B. The reported values
are the average over 100 randomly selected points from the test sets.
This can be partly explained as for VI generally only a few neurons are active (Louizos et al.,
2017; Frankle and Carbin, 2018), so it is necessary to focus mostly on these on the signature
approximation to obtain accurate GMM approximations at the first K-th layers. Moreover, the
propagation of the GMM approximation through the last linear layer is performed in closed form
(i.e., by Eqn. (13c) for = x ) and this mitigates the influence of the approximation error of
X { }
the previous layers on the final approximation error. In contrast, for the dropout networks, the
empirical approximation error is, on average, one order of magnitude larger. This can be explained
because the input-output distribution of dropout networks is inherently highly multimodal. Thus,
as we will show in the next paragraph, to obtain tighter GMM approximations, it is necessary to
use M > 5 (the value used in Table 2) and allow for a larger size in the GMM approximation.
Note that the formal error bounds tend to become more conservative with increasing network
depth. This is related to the transformations in the (stochastic) linear layers, for which the (ex-
pected) norm of the weight matrices is used to bound its impact on the Wasserstein guarantees
(see the linear operator bound in Table 1). It is also interesting to note how the error bounds tend
to be more accurate, and consequently closer to the empirical estimates, for the dropout networks.
This is because for VI it is necessary to bound the expected matrix norm of each linear operation,
which has to be upper bounded using Lemma 14, thus introducing conservatism. Fortunately, as
we will illustrate in the following paragraph and as we have mathematically proven in the previous
sections, in all cases the error bounds can be made arbitrarily small by increasing signature size
and M.
Uniform Convergence We continue our analysis with Figure 5, where we conduct an extensive
analysis of how the formal error bound from Theorem 13 changes with increasing the signature
size for various SNN architectures trained on both regression and classification datasets. The plot
24Finite Neural Networks as Mixtures of Gaussian Processes
Figure 5: The formal bounds on the relative 2-Wasserstein distance between various SNNs and
their Gaussian Mixture approximations obtained via Algorithm 1 for a compression size (M) of 3
and different signature sizes. The plotted lines show the average of 100 random test points.
confirms that with increasing signature sizes, the approximation error, measured as W , tends
2
to decrease uniformly. In particular, we observe an inverse linear rate of convergence, which is
consistent with the theoretical asymptotic results established in Graf and Luschgy (2007) on the
convergence rates of the Wasserstein distance from signatures for increasing signature size.14
Compression Size: 1 3 No Compression
NoisySines [2x64] (VI) 1.97279 1.63519 1.08763
[2x128] (VI) 2.48109 2.48488 2.48398
Kin8nm [2x128] (VI) 0.48740 0.46303 0.45387
MNIST [2x128] (VI) 0.45036 0.44873 0.44873
Fashion MNIST [2x128] (VI) 1.45489 1.45466 1.45466
CIFAR-10 VGG-2 + [2x128] (VI) 2.36616 2.35686 2.35686
Compression Size: 10 100 10000
MNIST [1x64] (Drop) 0.94371 0.73682 0.3571
[2x64] (Drop) 1.37594 1.09053 0.89742
CIFAR-10 VGG-2 (Drop) + [1x32] (VI) 179.9631 180.0963 145.2984
Table 3: The formal boundson the relative 2-Wasserstein distance between various SNNs and their
Gaussian Mixture approximates obtained via Algorithm 1 for a signature size of 100 and different
compression sizes (M). The reported values are the average of 100 random test points.
In Table 3, we investigate the other approximation step: the compression operation, which
reduces the components of the approximation mixtures (step e in Figure 2). The results show
that while the networks trained solely with VI can be well approximated with single Gaussian
distributions, the precision of the approximation for the networks employing dropout strongly
improves with increasing compression size. This confirms that the distribution of these networks
is highly multi-modal and underscores the importance of allowing for multi-modal approximations,
such as Gaussian mixtures.
14. InAppendixC,weexperimentallyinvestigateinmoredetailtheuniformconvergenceofthe2-Wassersteindistance
resulting from the signature operation for Gaussian mixtures of different sizes.
25Adams, Patan`e, Lahijanian and Laurenti
(a) Kernel SNN (b) Mean SNN (c) Kernel GMM (d) Mean GMM
(I) [2x128] (VI) trained on MNIST
(a) Kernel SNN (b) Mean SNN (c) Kernel GMM (d) Mean GMM
(II) VGG-2 + [1x128] (VI) trained on CIFAR-10
(a) Kernel SNN (b) Mean SNN (c) Kernel GMM (a)(d) Mean GMM
(III) VGG-2 (Drop) + [1x128] (Drop) trained on CIFAR-10
Figure 6: Kernels and means for various SNNs obtained via Monte Carlo sampling (left two
columns) and of our GMM approximation (right two columns) for 30 input points on MNIST
in (I) and CIFAR-10 in (II)and (III).Instead of the fullkernel matrices, which have dimensionality
300 300, we store the trace of the sub-blocks in the output dimension, and show the 30 30
× ×
matrix that captures the covariance between the input points. The rows and columns are grouped
according to theclasses, wherethecolored regions on the y-axis in the mean plots mark the classes.
(I) shows the approximate kernels and the predictive means for a fully connected SNN trained via
VI, which gives 98% test accuracy. We see in the kernel that examples with the same class labels
are correlated. In (II), the VGG network with dropout is combined with a linear stochastic layer
on the more complex CIFAR-10 data set, where we obtain 65% accuracy. (III) shows VGG stacked
with a linear layer to which we both apply dropout and obtain 64% accuracy on CIFAR-10.
26Finite Neural Networks as Mixtures of Gaussian Processes
8.2 Posterior Performance Analysis of SNNs
The GMM approximations obtained by Algorithm 1 naturally allow us to visualize the mean and
covariance matrix (i.e., kernel) of the predictive distribution at a set of points. Theresulting kernel
offers the ability to reason about our confidence on the predictions and enhance our understanding
of SNN performances (Khan et al., 2019). The state-of-the-art for estimating such a kernel is to
rely on Monte Carlo sampling (Van Amersfoort et al., 2020), which, however, is generally time-
consuming due to the need to consider a large amount of samples for obtaining accurate estimates.
In Figure 6, we compare the mean and covariance of various SNNs (obtained via Monte Carlo
samplingusing1e4samples)andtherelative GMMapproximationonasetof30randomlycollected
input points for various neural network architectures on the MNIST and CIFAR-10 datasets. We
observe that the GMMs match the MC approximation of the true mean and kernel. However,
unlike the GMM approximations, the MC approximations lack formal guarantees of correctness,
and additionally, computing the MC approximations is generally two orders of magnitudes slower
than computing the GMM approximations. By analyzing Figure 6, it is possible to observe that
since the architectures allow for the training of highly accurate networks, each row in the posterior
mean reflects that the classes are correctly classified. For the networks trained using VI, the kernel
matrix clearly shows the correlations trained by the SNN. For the dropout network, the GMM
visualizes the limitations of dropout learning in capturing correlation among data points. This
finding aligns with the result in Gal and Ghahramani (2016), which shows that Bayesian inference
via dropout is equivalent to VI inference with an isotropic Gaussian over the weights per layer.
(a) Uninformative SNN Prior (b) GP Prior
(c) GP Induced SNN Prior (Ours) (d) GP Induced SNN Prior (Tran et al., 2022)
Figure7: Visualizationofthepriordistributionof(a)aSNNwithanisotropicGaussiandistribution
over its weights; (b)a zero mean GP withthe RBFkernel; (c-d) SNNswith aGaussian distribution
over its weight that is optimized to mimic the GP in (b) via, respectively, our framework and the
approach proposed in Tran et al. (2022). The SNN has 2 hidden layers, 128 neurons per layer, and
the tanh activation function, while the GP has zero mean and a RBF kernel with length-scale 0.5
and signal variance 1. The red lines, blue shaded region, and blue lines respectively indicate the
mean, 99.7% credible intervals, and a subset of the MC samples from the output distributions.
27Adams, Patan`e, Lahijanian and Laurenti
8.3 Functional priors for neural networks via GMM approximation
In this section, to highlight another application of our framework, we show how our results can
be used to encode functional priors in SNNs. We start from Figure 7a, where we plot functions
sampled from a two hidden layer SNN with an isotropic Gaussian distribution over its weights,
which is generally the default choice for priors in Bayesian learning (Fortuin, 2022). As it is
possible to observe in the figure, the sampled functions tend to form straight horizontal lines; this
is a well-known pathology of neural networks, which becomes more and more exacerbated with
deep architectures (Duvenaud et al., 2014), Instead, one may want to consider a prior model that
reflects certain characteristics that we expect from the underlying true function, e.g., smoothness
or oscillatory behavior. Such characteristics are commonly encoded via a Gaussian process with a
givenmeanandkernel(Tran et al.,2022),asshownforinstanceinFigure7b,whereweplotsamples
from a GP with zero mean and the RBF kernel (MacKay, 1996). We now demonstrate how our
framework can be used to encode an informative functional prior, represented as a GP, into a SNN.
In particular, in what follows, we assume centered Mean-Field Gaussian priors on the weights, i.e.,
p = (¯0,diag(ψ)), where ψ is a vector of possibly different parameters representing the variance
w
N
of each weight. Our objective is to optimize ψ to minimize the 2-Wasserstein distance between the
distributions of the GP and the SNN at a finite set of input points, which we do by following the
approach describedinSection 7.2. For thelow-dimensional1D regressionproblem, theinputpoints
contain uniform samples in the input domain. For the high-dimensional UCI problems, we use the
training sets augmented with noisy samples. We solve the optimization problem as described in
Section 7.2, setting β to 0.01, using mini-batch gradient descent with randomly sampled batches.
Length-Scale Network Uninformative GP Induced Prior GP Induced Prior
RBF Kernel Architecture Prior (Tran et al., 2022) (Ours)
1 [2x64] 0.97 0.30 0.25
[2x128] 0.46 0.30 0.25
0.75 [2x64] 1.09 0.39 0.32
[2x128] 0.56 0.39 0.31
0.5 [2x64] 1.23 0.53 0.47
[2x128] 0.69 0.53 0.43
Table 4: The empirical estimates of the relative 2-Wasserstein distance between the prior distribu-
tions of zero-mean GPs with the RBF kernel and SNNs with uninformative or GP-induced priors
over the weights, at a subset of 20 points from the test set. The GP-induced prior are either
obtained via Algorithm 1 with signatures of size 10 and compression size (M) of 1 as described in
Section 7.2, or via the method in Tran et al. (2022).
1D Regression Benchmark We start our analysis with the 1D example in Figure 7, where we
compare our framework with the state-of-the-art approach for prior tuning by Tran et al. (2022).
Figure 7c demonstrates the ability of our approach to encode the oscillating behavior of the GP
in the prior distribution of the SNN. In contrast, the SNN prior obtained using the method in
Tran et al. (2022) in Figure 7d is visually less accurate in capturing the correlation imposed by the
RBF kernel of the GP. The performance difference can be explained by the fact that Tran et al.
(2022)optimizethe1-Wasserstein distancebetweentheGPandSNN,whichrelates tothecloseness
inthefirstmoment(themean),astheyrelyonitsdualformulation. Incontrast, weoptimizeforthe
2-Wasserstein distance, which relates to the closeness in the second moment, including both mean
28Finite Neural Networks as Mixtures of Gaussian Processes
and correlations (see Lemma 2). In Table 4, we investigate the quantitative difference in relative
2-Wasserstein distancebetweenthepriordistributionsoftheGPandSNNinFigure7, amongother
network architectures andGP settings. Theresults show thatourmethod consistently outperforms
Tran et al. (2022) on all considered settings.
UCI Regression Benchmark We continue our analysis on several UCI regression datasets to
investigate whether SNNs with an informative prior lead to improved posterior performance com-
pared to uninformative priors. We evaluate the impact of an informative prior on the predictive
performance of SNNs using the root-mean-square error (RMSE) and negative log-likelihood (NLL)
metrics. The RMSE solely measures the accuracy of the mean predictions, whereas the NLL eval-
uates whether the predicted distribution matches the actual distribution of the outcomes, taking
into account the uncertainty of the predictions. Lower RMSE and NLL values indicate better
performance. Figure 8 shows that for all datasets, the GP outperforms the SNN with an unin-
formative prior both in terms of mean and uncertainty accuracy. By encoding the GP prior in
the SNN, we can greatly improve the predictive performance of the SNN, bringing it closer to the
performance of the GP. The remaining performance gap to the GP can be explained by the VI
error in approximating the posterior distribution.
Figure 8: Report of the predictive performance of a GP with zero-mean and RBF kernel, and a
[2 128] SNN with tanh activation function on several UCI regression datasets in terms of the
×
root-mean-squared error (RMSE) and negative log-likelihood (NLL). The dots and bars represent
the means and standard deviations over the 10 random test splits of the datasets, respectively.
The results for the GP are shown in blue, the results for the SNN with an uninformative isotropic
Gaussian prior on the weights are in yellow, and the results for the SNN with the GP-induced
informative priors obtained via our approach are in green.
29Adams, Patan`e, Lahijanian and Laurenti
9 Conclusion
We introduced a novel algorithmic framework to approximate the input-output distribution of a
SNN with a GMM, providing bounds on the approximation error in terms of the Wasserstein
distance. Our framework relies on techniques from quantization theory and optimal transport,
and is based on a novel approximation scheme for GMMs using signature approximations. We
performed a detailed theoretical analysis of the error introduced by our GMM approximations and
complemented the theoretical analysis with extensive empirical validation, showing the efficacy of
ourmethodsinbothregression andclassification tasks, andforvariousapplications, includingprior
selection and uncertainty quantification. We should stress that in this paper we focused on finding
GMM approximations of neural networks at a finite set of input points, thus also accounting for
the correlations among these points. While reasoning for a finite set of input points is standard in
stochastic approximations (Yaida, 2020; Basteri and Trevisan, 2024) and has many applications,
as also highlighted in the paper, for some other applications, such as adversarial robustness of
Bayesian neural networks (Wicker et al., 2020), one may require approximations that are valid for
compact sets of input points. We believe this is an interesting future research question.
References
Steven Adams, Andrea Patane, Morteza Lahijanian, and Luca Laurenti. Bnn-dp: robustness
certification of bayesian neural networks via dynamic programming. In International Conference
on Machine Learning, pages 133–151. PMLR, 2023.
Robert J Adler and Jonathan E Taylor. Random fields and geometry. Springer Science & Business
Media, 2009.
Luca Ambrogioni, Umut Guclu, and Marcel van Gerven. Wasserstein variational gradient de-
scent: From semi-discrete optimal transport to ensemble variational inference. arXiv preprint
arXiv:1811.02827, 2018.
Joseph M Antognini. Finite size corrections for neural network gaussian processes. arXiv preprint
arXiv:1908.10030, 2019.
NicolaApollonio, Daniela DeCanditiis,GiovanniFranzina, PaolaStolfi,andGiovanniLucaTorrisi.
Normal approximation of random gaussian neural networks. arXiv preprint arXiv:2307.04486,
2023.
KrishnakumarBalasubramanian, LarryGoldstein, Nathan Ross, and AdilSalim. Gaussian random
fieldapproximationviastein’smethodwithapplicationstowiderandomneuralnetworks. Applied
and Computational Harmonic Analysis, 72:101668, 2024.
Andrea Basteri and Dario Trevisan. Quantitative gaussian approximation of randomly initialized
deep neural networks. Machine Learning, pages 1–21, 2024.
Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, vol-
ume 4. Springer, 2006.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty
in neural network. In International conference on Machine Learning, pages 1613–1622. PMLR,
2015.
30Finite Neural Networks as Mixtures of Gaussian Processes
Alberto Bordino, Stefano Favaro, and Sandra Fortini. Non-asymptotic approximations of gaussian
neuralnetworks via second-order poincar´e inequalities. arXiv preprint arXiv:2304.04010, page 3,
2023.
Luca Bortolussi, Ginevra Carbone, Luca Laurenti, Andrea Patane, Guido Sanguinetti, and
Matthew Wicker. On the robustness of bayesian neural networks to adversarial attacks. IEEE
Transactions on Neural Networks and Learning Systems, 2024.
Daniele Bracale, Stefano Favaro, Sandra Fortini, Stefano Peluchetti, et al. Large-width functional
asymptotics for deep gaussian neural networks. In International Conference on Learning Repre-
sentations, 2021.
James V Burke, Frank E Curtis, Adrian S Lewis, Michael L Overton, and Lucas EA Simo˜es.
Gradient sampling methods for nonsmooth optimization. Numerical nonsmooth optimization:
State of the art algorithms, pages 201–225, 2020.
Valentina Cammarota, Domenico Marinucci, Michele Salvi, and Stefano Vigogna. A quantitative
functional central limit theorem for shallow neural networks. Modern Stochastics: Theory and
Applications, 11(1):85–108, 2023.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pages 1310–1320. PMLR, 2019.
Timothy A Davis, Sivasankaran Rajamanickam, and Wissam M Sid-Lakhdar. A survey of direct
methods for sparse linear systems. Acta Numerica, 25:383–566, 2016.
Julie Delon and Agnes Desolneux. A wasserstein-type distance in the space of gaussian mixture
models. SIAM Journal on Imaging Sciences, 13(2):936–970, 2020.
David Duvenaud, Oren Rippel, Ryan Adams, and Zoubin Ghahramani. Avoiding pathologies in
very deep networks. In Artificial Intelligence and Statistics, pages 202–210. PMLR, 2014.
Ethan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. arXiv
preprint arXiv:1909.11304, 2019.
Ronen Eldan, Dan Mikulincer, and Tselil Schramm. Non-asymptotic approximations of neural
networks by gaussian processes. In Conference on Learning Theory, pages 1754–1775. PMLR,
2021.
Stefano Favaro, Boris Hanin, Domenico Marinucci, Ivan Nourdin, and Giovanni Peccati. Quanti-
tative clts in deep neural networks. arXiv preprint arXiv:2307.06092, 2023.
Daniel Flam-Shepherd, James Requeima, and David Duvenaud. Mapping gaussian process priors
to bayesian neural networks. In NIPS Bayesian deep learning workshop, volume 3, 2017.
Daniel Flam-Shepherd, James Requeima, and David Duvenaud. Characterizing and warping the
function space of bayesian neural networks. In NeurIPS Workshop on Bayesian Deep Learning,
page 18, 2018.
Vincent Fortuin. Priors in bayesian deep learning: A review. International Statistical Review, 90
(3):563–591, 2022.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. In International Conference on Learning Representations, 2018.
31Adams, Patan`e, Lahijanian and Laurenti
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pages 1050–1059.
PMLR, 2016.
Adri`a Garriga-Alonso and Mark van der Wilk. Correlated weights in infinite limits of deep con-
volutional neural networks. In Uncertainty in Artificial Intelligence, pages 1998–2007. PMLR,
2021.
Adri`a Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional net-
works as shallow gaussian processes. In International Conference on Learning Representations,
2019.
Alison L Gibbs and Francis Edward Su. On choosing and bounding probability metrics. Interna-
tional statistical review, 70(3):419–435, 2002.
ClarkRGivensandRaeMichaelShortt. Aclassofwassersteinmetricsforprobabilitydistributions.
Michigan Mathematical Journal, 31(2):231–240, 1984.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Siegfried Graf and Harald Luschgy. Foundations of quantization for probability distributions.
Springer, 2007.
Boris Hanin. Random neuralnetworks inthe infinitewidthlimit as gaussian processes. The Annals
of Applied Probability, 33(6A):4798–4819, 2023.
TamirHazanandTommiJaakkola. Stepstowarddeepkernelmethodsfrominfiniteneuralnetworks.
arXiv preprint arXiv:1508.05133, 2015.
Jos´e Miguel Hern´andez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learn-
ing of bayesian neural networks. In International conference on machine learning, pages 1861–
1869. PMLR, 2015.
John R Hershey and Peder A Olsen. Approximating the kullback leibler divergence between gaus-
sian mixture models. In 2007 IEEE International Conference on Acoustics, Speech and Signal
Processing-ICASSP’07, volume 4, pages IV–317. IEEE, 2007.
Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar Ra¨tsch, and Khan Mohammad
Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In Inter-
national Conference on Machine Learning, pages 4563–4573. PMLR, 2021.
Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in neural information processing systems, 31, 2018.
Muhammad Firmansyah Kasim. Derivatives of partial eigendecomposition of a real symmetric
matrix for degenerate cases. arXiv preprint arXiv:2011.04366, 2020.
MohammadKhan,DidrikNielsen,VootTangkaratt, WuLin,YarinGal,andAkashSrivastava. Fast
andscalable bayesian deeplearningbyweight-perturbation inadam. InInternational Conference
on Machine Learning, pages 2611–2620. PMLR, 2018.
32Finite Neural Networks as Mixtures of Gaussian Processes
Mohammad Emtiyaz E Khan, Alexander Immer, Ehsan Abedi, and Maciej Korzepa. Approximate
inferenceturnsdeepnetworks intogaussian processes. Advances in neural information processing
systems, 32, 2019.
J Kieffer. Exponential rate of convergence for lloyd’s method i. IEEE Transactions on Information
Theory, 28(2):205–210, 1982.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
AdamKlukowski. Rate ofconvergence ofpolynomialnetworks togaussianprocesses. InConference
on Learning Theory, pages 701–722. PMLR, 2022.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. In Advances in Neural Information Processing Systems 25, pages
1097–1105. Curran Associates, Inc., 2012.
JaehoonLee, Yasaman Bahri, RomanNovak, SamuelSSchoenholz, Jeffrey Pennington,andJascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017.
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,
and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. Advances
in Neural Information Processing Systems, 33:15156–15172, 2020.
QiangLiuandDilinWang. Steinvariationalgradientdescent: Ageneralpurposebayesian inference
algorithm. Advances in neural information processing systems, 29, 2016.
Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):
129–137, 1982.
Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. Ad-
vances in neural information processing systems, 30, 2017.
David JC MacKay. Bayesian non-linear modeling for the prediction competition. In Maximum
Entropy and Bayesian Methods: Santa Barbara, California, USA, 1993, pages 221–234. Springer,
1996.
Jan R Magnus and Heinz Neudecker. Matrix differential calculus with applications in statistics and
econometrics. John Wiley & Sons, 2019.
Marko M¨akel¨a. Survey of bundle methods for nonsmooth optimization. Optimization methods and
software, 17(1):1–29, 2002.
Takuo Matsubara, Chris J Oates, and Franc¸ois-Xavier Briol. The ridgelet prior: A covariance
function approach to prior specification for bayesian neural networks. The Journal of Machine
Learning Research, 22(1):7045–7101, 2021.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin
Ghahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint
arXiv:1804.11271, 2018.
Geoffrey J McLachlan and David Peel. Finite mixture models, volume 299. John Wiley & Sons,
2000.
33Adams, Patan`e, Lahijanian and Laurenti
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012.
RomanNovak,LechaoXiao,YasamanBahri,JaehoonLee,GregYang,JiriHron,DanielAAbolafia,
Jeffrey Pennington, and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. In International Conference on Learning Representations, 2018.
Gilles Pag`es and Jacques Printems. Optimal quadratic quantization for numerics: the gaussian
case. Monte Carlo Methods Appl., 9(2):135–165, 2003.
GillesPagesandBenediktWilbertz. Optimaldelaunayandvoronoiquantizationschemesforpricing
american style options. In Numerical Methods in Finance: Bordeaux, June 2010, pages 171–213.
Springer, 2012.
Gabriel Peyr´e, Marco Cuturi, et al. Computational optimal transport: With applications to data
science. Foundations and Trends® in Machine Learning, 11(5-6):355–607, 2019.
Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer school on machine
learning, pages 63–71. Springer, 2003.
R.TyrrellRockafellar. ConvexAnalysis. PrincetonLandmarksinMathematicsandPhysics.Prince-
ton University Press, Princeton, NJ, revised edition edition, 2015. ISBN 978-0691015866.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. In International Conference on Learning Representations, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929–1958, 2014.
ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,IanGoodfellow,
and Rob Fergus. Intriguing properties of neuralnetworks. arXiv preprint arXiv:1312.6199, 2013.
Charlie Tang and Russ R Salakhutdinov. Learning stochastic feedforward neural networks. Ad-
vances in Neural Information Processing Systems, 26, 2013.
Ba-Hien Tran, Simone Rossi, Dimitrios Milios, and Maurizio Filippone. All you need is a good
functional prior for bayesian deep learning. Journal of Machine Learning Research, 23(74):1–56,
2022.
Volker Tresp. Mixtures of gaussian processes. Advances in neural information processing systems,
13, 2000.
JoostVan Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation usinga
single deep deterministic neuralnetwork. In International conference on machine learning, pages
9690–9700. PMLR, 2020.
34Finite Neural Networks as Mixtures of Gaussian Processes
C´edric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.
Matthew Wicker, Luca Laurenti, Andrea Patan`e, and Marta Kwiatkowska. Probabilistic safety for
Bayesian neural networks. Uncertainty in Artificial Intelligence (UAI), 2020.
Stefan Wilhelm et al. Moments calculation for the doubly truncated multivariate normal density.
arXiv preprint arXiv:1206.5387, 2012.
ChristopherWilliams. Computingwithinfinitenetworks.Advancesinneuralinformation processing
systems, 9, 1996.
Sho Yaida. Non-gaussian processes and neural networks at finite widths. In Mathematical and
Scientific Machine Learning, pages 165–192. PMLR, 2020.
Tianyuan Yu, Yongxin Yang, Da Li, Timothy Hospedales, and Tao Xiang. Simple and effective
stochastic neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 35, pages 3252–3260, 2021.
Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient
as variational inference. In International Conference on Machine Learning, pages 5852–5861.
PMLR, 2018.
35Adams, Patan`e, Lahijanian and Laurenti
Appendix A. Proofs
We present the proofs for all results discussed in the main text of this work. These proofs rely on
several less commonly reported properties of the Wasserstein distance, which we will first discuss
in Section A.1. After this, we present the proofs ordered per section.
A.1 Properties of the Wasserstein Distance
Lemma 23 (2-Wasserstein Distance for Independent Joint Distributions) Letp,q (Rn)
be joint distributions that are independent over each dimension, i.e., p = n p(i) and∈ qP =
i=1
n q(i), where p(i) and q(i) are the marginal distributions of p and q for the i-th dimension,
i=1 Q
respectively. Then, we have that:
Q
n
W2(p,q) = W2(p(i),q(i)).
2 2
i=1
X
Proof Due to p and q being independent over each dimension, we have:
W2(p,q) = inf E [ x y 2]
2 γ Γ(p,q) (x,y) ∼γ k − k
∈
n
= inf E [(x(i) y(i))2]
(x,y) γ
γ Γ(p,q) ∼ −
∈ i=1
X
n
= inf E [(x(i) y(i))2],
γ Γ(p,q)
(x(i),y(i)) ∼γ(i)
−
∈ i=1
X
where the set of couplings can be rewritten as
n n n
Γ(p,q)= Γ p(i), q(i) = Γ(p(i),q(i)),
!
i=1 i=1 i=1
Y Y Y
such that
n n
inf E [(x(i) y(i))2]= inf E [(x(i) y(i))2]
γ Γ(p,q)
(x(i),y(i)) ∼γ(i)
− γ(i) Γ(p(i),q(i))
(x(i),y(i)) ∼γ(i)
−
∈ Xi=1 Xi=1 ∈
n
= W2(p(i),q(i))
2
i=1
X
Proposition 24 (Upper Bound on the Wasserstein Distance between Mixtures Distri-
butions, aGeneralizationofProposition 4inDelon and Desolneux(2020))Let M π(i) p
i=1 p i ∈
(Rd) and N π(j) q (Rd) be two mixture distributions, and let Γ(π ,π ) be the subset of
P2 j=1 q j ∈ P2 p q P
simplex Π with marginals π and π . Then for any π¯ Γ(π ,π ), it holds that:
M N p q p q
×P ∈
M N M N
W2 π(i)p , π(j)q π¯(i,j)W2(p ,q ).
2 p i q j  ≤ 2 i j
i=1 j=1 i=1 j=1
X X XX
 
36Finite Neural Networks as Mixtures of Gaussian Processes
Proof Recall that according to the definition of the 2-Wasserstein distance:
M N
W2 π(i)p , π(j)q = inf E [ z z˜ 2],
2 Xi=1 p i Xj=1 q j  γ ∈Γ (cid:16)PM i=1πp(i)pi, PN j=1πq(j)qj(cid:17) (z,z˜) ∼γ k − k
 
where Γ M π(i) p , N π(j) q represents the set of all joint probability distributions with
i=1 p i j=1 q j
marginal(cid:16)dPistributionsP M i=1π p(i) p i(cid:17)and N j=1π q(j) q j. Consider the following subset of joint distri-
butions
P P
M N M N
Γ¯ := π¯(i,j)γ π¯ Γ(π ,π ),γ Γ(p ,q ) Γ π(i)p , π(j)q .
| ∈ p q ∈ i j ⊂  p i q j 
(cid:26)i=1 j=1 (cid:27) i=1 j=1
XX X X
 
It follows that:
inf E [ z z˜ 2] inf E [ z z˜ 2]
(z,z˜) γ (z,z˜) γ
γ ∈Γ (cid:16)PM i=1πp(i)pi, PN j=1πq(j)qj(cid:17) ∼ k − k ≤ γ ∈Γ¯ ∼ k − k
M N
= π¯(i,j) inf E [ z z˜ 2]
(z,z˜) γ
i=1 j=1
γ ∈Γ(pi,qj) ∼ k − k
XX
M N
= π¯(i,j)W2(p ,q ),
2 i j
i=1 j=1
XX
which completes the proof.
A.2 Proofs Section 5
Proposition 8: Optimal Transport Plan for Wasserstein Distance Between
Distributions and Their Signatures
Proof The proof follows directly from Theorem 1 in (Ambrogioni et al., 2018). According to this
theorem, for any ρ N, we have that
∈
N
Wρ(p,∆ #p)= π(i)E [ z c ρ z ].
ρ C z ∼p k − i k | ∈ Ri
i=1
X
Proposition 9: Wasserstein Distance from Signatures of Univariate Gaussians
Proof From Proposition 8, we have that
N
W2(∆ # (0,1), (0,1)) = Φ E [(z c )2 z [l ,u ]].
2 C N N i z ∼N(0,1) − i | ∈ i i
i=1
X
37Adams, Patan`e, Lahijanian and Laurenti
where Φ = P [z [l ,u ]]. Using the probability density function (pdf) of the Gaussian
i z (0,1) i i
∼N ∈
distribution and a simple change of variables, we can write conditional expectation terms as:
1
E [(z c )2 z [l ,u ]]= (z c )21 (z) (z 0,1)dz,
z ∼N(0,1)
−
i
| ∈
i i
Φ
i
−
i [li,ui]
N |
Z
1
= z21 (z) (z c ,1)dz
Φ
i
[li −ci,ui −ci]
N | −
i
Z
where (z c ,1) denotes the pdf of a Gaussian distribution with mean c and variance 1,
i i
N | − −
and 1 (z) is the indicator function which equals 1 if z [l,u] and 0 otherwise. Note that,
[l,u]
∈
1 1 (z) (z c ,1) is in fact the pdf of a random variable with a Gaussian distribution
Φi [li −ci,ui −ci] N | − i
( c ,1) conditioned on [l c ,u c ]. Hence, the above conditional expectation is equivalent to
i i i i i
N − − −
the second moment of a truncated Gaussian random variable on [l c ,u c ], for which closed-
i i i i
− −
form expressions exist (Wilhelm et al., 2012). Following Section 3 of Wilhelm et al. (2012) we can
write:
1 ν +(µ c )2
z21 (z) (z c ,1)dz = i i − i .
Φ
i
[li −ci,ui −ci]
N | −
i
Φ
i
Z
Thus, we finally obtain:
N N
Φ E [ z c 2 z [l ,u ]] = ν +(µ c )2.
i z (0,1) i i i i i i
∼N k − k | ∈ −
i=1 i=1
X X
Corollary 10 of Proposition 9: Wasserstein Distance from Signatures of
Multivariate Gaussians
Proof According to Proposition 8, we have:
N
W2(∆ # (¯0,Σ), (¯0,Σ)) = P [z ]E [ z c 2 z ].
2 C N N z ∼N(¯0,Σ) ∈ Ri z ∼N(¯0,Σ) k − i k | ∈ Ri
i=1
X
For all i 1,...,N , we can write
∈ { }
P [z ]E [ z c 2 z ]
z (¯0,Σ) i z (¯0,Σ) i i
∼N ∈ R ∼N k − k | ∈ R
1 (z˜) (z˜ ¯0,Σ)
=P z ∼N(¯0,Σ)[z
∈
Ri]
Z
kz˜ −c i k2 PR
z
∼i N(¯0N ,Σ)[z|
∈
Ri]dz˜
= z c 2 (z ¯0,Σ)dz.
i
k − k N |
Z i
R
By applying substitution q = Tz to the integral terms, we obtain:
z c 2 (z ¯0,Σ)dz = S(q Tc ) 2 (q ¯0,I)dq.
i i
k − k N | k − k N |
ZRi Z{Tz |z ∈Ri
}
Here,since isagridinthetransformedspaceinducedbyT, Tz z isanaxis-alignedhyper-
i
C { | ∈ R }
rectangle that can be defined by vectors l ,u Rn, i.e., Tz z = [l ,u ]. Furthermore,
i i i i i
∈ { | ∈ R }
because matrix V is orthogonal, we have:
n
Sq 2 = qT(diag(λ) −1 2VTVdiag(λ) −1 2) −1q = qTdiag(λ)q = λ(l)q(l)2 .
k k
l=1
X
38Finite Neural Networks as Mixtures of Gaussian Processes
Hence, we can write
n
S(q Tc ) 2 (q ¯0,I)dq = λ(l) (q(l) c¯(l) )2 (q ¯0,I)dq,
k − i k N | − i N |
Z[li,ui]
l=1
Z[li,ui]
X
where c¯ = Tc . Using that (q ¯0,I) = n (q(l) 0,1), we can further simplify the above
i i N | l=1N |
expression as follows
Q
n
λ(l) (q(l) c¯(l) )2 (q ¯0,I)dq
− i N |
l=1
Z[li,ui]
X
n n u(j)
= λ(l) i 1dq(j) (q(l) c(l) )2 (q(l) 0,1)dq(l)
Xl=1 (cid:16)j
Y=1Zl i(j) (cid:17)Z[l i(l),u( il)] − i N |
j=l
6
n n
= λ(l) P [q(j) [l(j) ,u(j) ]] (q(l) c(l) )2 (q(l) 0,1)dq(l)
Xl=1 (cid:16)j Y=1
q(j) ∼N(0,1) ∈ i i (cid:17)Z[l i(l),u( il)] − i N |
j=l
6
n n
= λ(l) P [q(j) [l(j) ,u(j) ]] E [(q(l) c(l) )2 q(l) [l(l) ,u(l) ]]
q(j) (0,1) ∈ i i q(l) (0,1) − i | ∈ i i
∼N ∼N
Xl=1 (cid:16)j Y=1 (cid:17)
n n
= P [q(j) [l(j) ,u(j) ]] λ(l)E [(q(l) c(l) )2 q(l) [l(l) ,u(l) ]]
q(j) (0,1) ∈ i i q(l) (0,1) − i | ∈ i i
∼N ∼N
(cid:16)j Y=1
=:P(j)
(cid:17)Xl=1
=:E(j)
i i
| {z } | {z }
Because of Eqn. (10), without loss of generality, we can set the indexing of the signature locations,
and hence the related regions, such that for all i N:
∈
c¯ = c¯ = (c¯(1) ,c¯(2) ,...,c¯(n) )T,
i i1 ·i2 ·... ·in i1 i2 in
where i 1,..., l , and c¯(l) l, l 1,...,n . Using straightforward algebra, we can now
l ∈ { |C |} il ∈ C ∀ ∈ { }
write
N n n 1 2 n n n
|C | |C | |C |
P(j) E(l)
= ...
P(j) E(l)
i i i1i2...in i1i2...in
· · · ·
Xi=1(cid:16)j Y=1 (cid:17)(cid:16)Xl=1 (cid:17) i X1=1i X2=1 i Xn=1(cid:16)j Y=1 (cid:17)(cid:16)Xl=1 (cid:17)
1 2 n n
|C | |C | |C |
=
P(1) P(2)
...
P(n) E(l)
i1i2...in i1i2...in i1i2...in i1i2...in
· · · · · · · ·
i X1=1 i X2=1 i Xn=1 Xl=1
n n k l
|C | |C |
=
P(k) P(l) E(l)
i1i2...in i1i2...in i1i2...in
· · · · · ·
Xl=1(cid:16)k Y=1i Xk=1 (cid:17)(cid:16)i Xl=1 (cid:17)
k=l
6
Note that, as a consequence of our indexing, |Ck | [l(k) ,u(k) ] = R, and hence
∪ik=1 ... ·ik·... ... ·ik·...
k k
|C | |C |
P(k) = P [q(k) [l(k) ,u(k) ]] = 1.
... ·ik·... q(k) ∼N(0,1) ∈ ... ·ik·... ... ·ik·...
i Xk=1 i Xk=1
39Adams, Patan`e, Lahijanian and Laurenti
Furthermore for l 1,...,n ,
∈{ }
l
|C |
P(l) E(l)
... ·il·... ... ·ik·...
i Xl=1
k
|C |
= λ(l) P [q(l) [l(l) ,u(l) ]]E [(q(l) c¯(l) )2 q(l) [l(l) ,u(l) ]]
q(l) ∼N(0,1) ∈ ... ·il·... ... ·il·... q(l) ∼N(0,1) − ... ·il·... | ∈ ... ·il·... ... ·il·...
i Xl=1
= λ(l)W2(∆ # (0,1), (0,1)).
2 ¯(l) N N
C
Combining the above results, we obtain:
n n k l
|C | |C |
W2(∆ # (¯0,Σ), (¯0,Σ)) = P(k) P(l) E(l)
2 C N N i1 ·i2... ·in i1 ·i2... ·in i1 ·i2... ·in
Xl=1(cid:16)k Y=1i Xk=1 (cid:17)(cid:16)i Xl=1 (cid:17)
k=l
6
n n
= 1) λ(l)W2(∆ # (0,1), (0,1))
2 ¯(l) N N
C
Xl=1(cid:16)k Y=1 (cid:16) (cid:17)
k=l
6
n
= λ(l)W2(∆ # (0,1), (0,1)).
2 ¯(l) N N
C
l=1
X
To conclude the proof, note that for non-zero mean, we can follow the same procedure but apply
substitution q = T(x m) and define c¯= T(c m).
− −
A.3 Proofs Section 6
Lemma 14: Bounds on the Spectral Norm of Random Matrices with Correlated
Elements
Proof We here generalize the result presented in the main text, and prove that for both ρ = 2
and ρ= 1:
1
E
W
∼pW[ kW kρ]ρ1
≤
E
W¯
∼p¯W[(W¯ (i,j))2] 2 + kM k. (30)
(cid:16)Xi,j (cid:17)
Using the triangle inequality for the spectral norm, we have
E [ W ] = E [ W¯ +M ] E [ W¯ ]+ M
W ∼pW k k W¯ ∼p¯W k k ≤ W¯ ∼p¯W k k k k
Using Minkowski’s inequality for the spectral norm, we get
E
W
∼pW[ kW k2]21 = E
W¯
∼p¯W[ kW¯ +M k2]21
≤
E W¯ ∼p¯W[ kW¯ k2]1 2 + kM
k
For any matrix W¯ Rn m, we have W¯ W¯ , where denotes the Frobenius defined by
×
W¯ 2 = (W¯
(i,j∈
))2. Therefore,
k k≤ k kF k·kF
k k i,j
F
P 1
E W¯ ∼p¯W[ kW¯ k2]21
≤
E W¯ ∼p¯W[ kW¯ k2 F]1 2
≤
E W¯ ∼p¯W[(W¯ (i,j))2] 2.
(cid:16)Xi,j (cid:17)
40Finite Neural Networks as Mixtures of Gaussian Processes
This concludes the proof for ρ= 2. For ρ= 1, we additionally apply Jensen’s inequality:
1 1
E [ W ] E [ W ] E (W¯ (i,j))2 2 E [(W¯ (i,j))2] 2.
W¯ ∼p¯W k k ≤ W¯ ∼p¯W k kF ≤ W¯ ∼p¯W ≤ W¯ ∼p¯W
h(cid:16)Xi,j (cid:17) i (cid:16)Xi,j (cid:17)
Results in Table 1: Wasserstein Distance under Push-Forward Operations in SNNs
Proposition 25 (Wasserstein Distance under Stochastic Linear Operations) Let
L : Rn Rm be the affine operation L (z) = Wz+b with parameters W Rn m and b Rm.
w w ×
→ ∈ ∈
Further, assume a distribution p (R(n+1) m) over w = (W,b) such that for any z Rn, the
w ×
∈ P ∈
random variable L (z) follows the probability distribution p = L (z)#p . Then, for ρ 1,2
w Lw(z) w w
∈ { }
and probability distributions p,q (Rn) it holds that
ρ
∈P
W ρ(E
z
∼p[p Lw(z)],E
z˜
∼q[p Lw(z˜)])
≤
E
(W,b)
∼pw[ kW kρ]ρ1W ρ(p,q). (31)
Proof Since the spectral (matrix) norm, denoted as , is consistent with the Euclidean vector-
k·k
norm, we have that Wz Wz˜ W z z˜ . Furthermore, by the monotonicity of the quadratic
k − k≤ k kk − k
function in the positive plane, it follows that Wz Wz˜ 2 W 2 z z˜ 2. Hence:
k − k ≤ k k k − k
Wρ(E [p ],E [p ]) = inf E [ Wz+b Wz˜+b ρ]
ρ z ∼p Lw(z) z˜ ∼q Lw(z˜) γ Γ(pw,p,q) ((W,b),z,z˜) ∼γ k − k
∈
= inf E [ Wz Wz˜ ρ]
((W,b),z,z˜) γ
γ Γ(pw,p,q) ∼ k − k
∈
inf E [ W ρ z z˜ ρ].
((W,b),z,z˜) γ
≤ γ Γ(pw,p,q) ∼ k k k − k
∈
Therefore, considering that p Γ(p,q) Γ(p ,p,q), we obtain:
w w
× ⊂
inf E [ W ρ z z˜ ρ] E [ W ρ] inf E [ z z˜ ρ]
γ Γ(pw,p,q)
((W,b),z,z˜) ∼γ
k k k − k ≤
(W,b) ∼pw
k k γ Γ(p,q)
(z,z˜) ∼γ
k − k
∈ ∈
= E [ W ρ ]Wρ(p,q).
W ∼pw k k k ρ
Proposition 26 (Wasserstein Distance under Approximated Stochastic Linear Opera-
tions) Let p (R(n+1) m) and q (R(n+1) m) be weight distributions, and denote p =
w
∈ P
× w
∈ P
× Lw(z)
L (z)#p and q = L (z)#q . Then, for probability distributions p,q (Rn) it holds that
w w Lw(z) w w
∈
Pρ
W ρ(E
z
∼p[p Lw(z)],E
z˜
∼q[q Lw(z˜)])
≤
E
W
∼pw[ kW kρ]ρ1W ρ(p,q)+E
z˜
∼q[ kz˜ kρ]ρ1W ρ(p W,q W)+W ρ(p b,q b).
Proof By the triangular inequality:
W (E [p ],E [q ]) W (E [p ],E [p ])+W (E [p ],E [q ]).
ρ z ∼p Lw(z) z˜ ∼q Lw(z˜)
≤
ρ z ∼p Lw(z) z˜ ∼q Lw(z˜) ρ z ∼q Lw(z) z˜ ∼q Lw(z˜)
(a) (b)
| {z } | {z }
41Adams, Patan`e, Lahijanian and Laurenti
For term (a), we derived upper bound E
W
∼pw[ kW kρ]ρ1W ρ(p,q) in Proposition 25. For term (b), we
can apply the Minkowski inequality to obtain:
W (E [p ],E [q ])
ρ z ∼q Lw(x) z˜ ∼q Lw(z˜)
=
γ
Γ(pin wf ,qw,q)E ((W,b),(W˜,˜b),z˜) ∼γ[ kWz˜ −W˜ z˜+b −˜b kρ]1 ρ
∈
≤ γ
Γ(pin wf ,qw,q)E
((W,b),(W˜,˜b),z˜)
∼γ[ k(W −W˜ )z˜ kρ]ρ1 +E
((W,b),(W˜,˜b),z˜)
∼γ[b −˜b kρ]ρ1
∈
= γb∈Γin (pf b,qb)E
(b,˜b)
∼γb[b −˜b kρ]ρ1 +
γ
∈Γ(pi Wnf ,qW,q)E
((W,b),(W˜,˜b),z˜)
∼γ[ k(W −W˜ )z˜ kρ]ρ1 .
(b.1) (b.2)
Here, p ,p and q| ,q denote{zthe marginal}s of|p and q w.r.t. b an{dzW, respectively. Not}e that
b W b W w w
term (b.1) is equivalent to W (p ,q ). Usingthetriangle inequality, we can upperboundterm (b.2)
ρ b b
as
inf E [ (W W˜ )z˜ ρ] inf E [ W W˜ ρ z˜ ρ]
γ Γ(pW,qW,q) ((W,b),(W˜,˜b),z˜) ∼γ k − k ≤ γ Γ(pW,qW,q) (W,W˜,˜b),z˜) ∼γ k − k k k
∈ ∈
E [ z˜ ρ] inf E [ W W˜ ρ]
≤ z˜ ∼q k k γ Γ(pW,qW) (W,W˜,z˜) ∼γ k − k
∈
E [ z˜ ρ]Wρ(p ,q ).
≤ z˜ ∼q k k ρ W W
Combining the above bounds, we obtain the inequality as in Eqn. (26).
Corollary 27 (of Proposition 8 on Lipschitz Transformations) For p (Rn) and signa-
2
∈ P
ture locations = c N Rn, let be the local Lipschitz constant of piecewise continuous
function σ : RnC R{ mi } ii n=1 re⊂ gion , tL heσ n|R fi or ρ N
i
→ R ∈
N
Wρ(σ#p,σ#∆ #p) ρ π(i)E [ z c ρ z ].
ρ C ≤ Lσ |Ri z ∼p k − i k | ∈ Ri
i=1
X
Proof According to Theorem 1 in (Ambrogioni et al., 2018) we have that
N
Wρ(σ#p,σ#∆ #p)= π(i)E [ σ(z) σ(c ) ρ z ].
ρ C z ∼p k − i k | ∈ Ri
i=1
X
Then, as by definition it hold that σ(z) σ(c ) z c , z,c , we have:
k − i k ≤ Lσ |Rik − i k ∀ i ∈ Ri
N N
π(i)E [ σ(z) σ(c ) ρ z ] ρ π(i)E [ z c ρ z ].
z ∼p k − i k | ∈ Ri ≤ Lσ |Ri z ∼p k − i k | ∈ Ri
i=1 i=1
X X
Note also that, for the global Lipschitz constant, i.e., = max , it follows that
Lσ Lσ i ∈{1,...,N }Lσ |Ri
N N
ρ π(i)E [ z c ρ z ] ρ π(i)E [ z c ρ z ] = ρWρ(p,q). (32)
Lσ |Ri z ∼p k − i k | ∈ Ri ≤ Lσ z ∼p k − i k | ∈ Ri Lσ ρ
i=1 i=1
X X
42Finite Neural Networks as Mixtures of Gaussian Processes
Proposition 28 (Triangle Inequality for Wasserstein Distance with Mappings) For ρ
∈
1,2 and probability distributions p,q (Rn) and mapping h :Rn Rn, it holds that
ρ
{ } ∈ P →
W (p,h#q) W (p,q)+W (q,h#q) (33)
ρ ρ ρ
≤
Proof Theprooffollows directly fromthefactthatW satisfies theaxioms ofadistance, including
ρ
the triangle inequality.
Theorem 13: Wasserstein Distance between the Distribution of a SNN and its
GMM Approximation Over a Point
Proof We prove Theorem 13 via induction and generalize the result to the ρ-Wasserstein distance
with ρ 1,2 . To do so, we first derive the base case of the induction. For k 1,...,K , we
∈ { } ∈ { }
have that:
W (p ,q )
ρ nn(x),k+1 nn(x),k+1
= W (E [p ],E [p ]) Eqns. (4) & (13)
ρ zk∼pnn(x),k Lwk(σ(zk)) z˜k∼RM(qnn(x),k) Lwk(σ(∆Ck(z˜k)))
≤
E Wk∼pwk[ kW kρ]ρ1W ρ(σ#p nn(x),k,σ#∆ Ck#R M(q nn(x),k)) Prop. 25
ˆ W (σ#p ,σ#∆ #R (q )) Eqn. (15)
≤ Sρ,w,k ρ nn(x),k Ck M nn(x),k
ˆ W (σ#p ,σ#R (q ))
ρ,w,k ρ nn(x),k M nn(x),k
≤ S
(cid:2) (I)
+W ρ|(σ#R M(q nn(x){,kz),σ#∆ Ck#R M}(q nn(x),k)) Prop. 28
(II) (cid:3)
| {z }
where, for (I),
W (σ#p ,σ#R (q ))
ρ nn(x),k M nn(x),k
W (p ,R (q )) Corol. 27
σ ρ nn(x),k M nn(x),k
≤ L
W (p ,q )+W (q ,R (q )) Prop. 28
σ ρ nn(x),k nn(x),k ρ nn(x),k M nn(x),k
≤ L
(cid:2)W (p ,q )+Wˆ , (cid:3) Eqn. (17)
σ ρ nn(x),k nn(x),k ρ,R,k
≤ L
h i
with L is the global Lipschitz constant of σ, and for (II),
σ
W (σ#R (q ),σ#∆ #R (q )) Wˆ Eqn. (16)
ρ M nn(x),k Ck M nn(x),k ≤ ρ,∆,k
Hence, if W (p ,q ) Wˆ , then
ρ nn(x),k nn(x),k ρ,k
≤
W (p ,q ) ˆ Wˆ + Wˆ +Wˆ := Wˆ
ρ nn(x),k+1 nn(x),k+1 ρ,w,k σ ρ,k σ ρ,R,k ρ,∆,k ρ,k+1
≤ S L L
h i
The assumption is naturally satisfied for k = 1, such that by induction, it holds that
W (p ,q ) Wˆ .
ρ nn(x) nn(x) ρ,K+1
≤
43Adams, Patan`e, Lahijanian and Laurenti
Proposition 16: Wasserstein Distance Between the Distribution of a SNN and its
GMM Approximation Over Set of Points
Proof We prove this Proposition via induction and generalize the result to the ρ-Wasserstein
distance with ρ 1,2 . We first derive the base case. For ρ 1,2 and k 1,...,K ,
∈ { } ∈ { } ∈ { }
W (p ,q )
ρ nn( ),k+1 nn( ),k+1
X X
= W (E [p ],E [p ]) Eqns. (21) & (22)
ρ zk∼pnn(X),k LD wk(σ(zk)) z˜k∼RM(qnn(X),k) LD wk(σ(∆Ck(z˜k)))
≤
E Wk∼pwk[ k¯1
D
⊗W kρ]ρ1W ρ(σ#p
nn(
X),k,σ#∆ Ck#R M(q
nn(
X),k)) Prop. 25,
where, following the same steps as in the proof of Theorem 13,
W (σ#p ,σ#∆ #R (q )) Wˆ + Wˆ +Wˆ .
ρ nn( X),k Ck M nn( X),k ≤ Lσ ρ,k Lσ ρ,R,k ρ,∆,k
Further, since by the properties of the spectral norm ¯1
D
W ρ = ¯1
D
ρ W ρ and ¯1
D
ρ = Dρ 2,
k ⊗ k k k k k k k
we have
E Wk∼pwk[ k¯1
D
⊗W kρ]ρ1 = (Dρ 2E Wk∼pwk[ kW kρ])ρ1 ,
ˆ Eqn. (23)
ρ,w,k
≤ S
Hence, if W (p ,q ) Wˆ , then
ρ nn( ),k nn( ),k ρ,k
X X ≤
W (p ,q ) ˆ Wˆ + Wˆ +Wˆ
ρ nn( ),k+1 nn( ),k+1 ρ,w,k σ ρ,k σ ρ,R,k ρ,∆,k
X X ≤ S L L
:= Wˆ h i
ρ,k+1
The assumption is naturally satisfied for k = 1, such that by induction, it holds that
W (p ,q ) Wˆ .
ρ nn( ) nn( ) ρ,K+1
X X ≤
Theorem 17: Uniform Convergence of the Wasserstein Distance from the GMM
approximation of a SNN
The proof of Theorem 17 relies on the fact that the ρ-Wasserstein distance resulting from the
signature operation performed on a Gaussian mixture distribution uniformly converges as the sig-
nature size increases, as shown in the following Lemma, which is an extension of Lemma 6.1
Graf and Luschgy (2007), and Corollary.
Lemma 29 (Uniform Convergence in Wasserstein Distance for of a Grid of Signatures)
Let p (Rn), then, for ǫ > 0 there exists signature locations = c ,...,c Rd that form
ρ 1 N
∈ P C { } ⊂
an n D hyper-cubic lattice in Rn, i.e., with corresponding regions satisfying Eqn. (10) for some
−
matrix T Rd d, such that
×
∈
Wρ(∆ #p,p) ǫ.
ρ C ≤
Proof Consider a hyper-cube Rd such that
A ⊂
ǫ
z ρdp(z) ,
Rn k k ≤ 2
Z \A
44Finite Neural Networks as Mixtures of Gaussian Processes
which is guaranteed to exist since p (Rn). Now consider a uniform partition of in N regions
ρ
∈ P A
,..., of diameter ǫ¯ 1 ǫ 1/ρ and for each region consider c the centroid of
{A1 AN } ≤ √n 2 Ak k ∈ Ak
. We let = c ,c ,...,c . Then, the following holds:
Ak C { 1 2 N } (cid:0) (cid:1)
Wρ(∆ #p,p)= inf E [ z˜ z ρ]
ρ C γ ∈Γ(∆C#p,p) (z˜,z) ∼γ k − k
N
E [ z c 1 ρ]
≤ z ∼p k − k Akk
k=1
X
N
= z ρdp(z)+ z c ρdp(z)
k
Rn k k k − k
Z \A Xk=1ZAk
N
ǫ
z ρdp(z)+ dp(z)
≤ Rn k k 2
Z \A Xk=1ZAk
ǫ.
≤
Inthelaststep,weusethatthesmallestballthatencloseshyper-cube hasradiusǫ¯√d= (ǫ/2)1/ρ,
k
A
so that every point in is (ǫ/2) close to c in terms of Euclidean distance for every k. Finally,
k k
A
observe that naturally satisfies constrain Eqn. (10) so that the Lemma is proven by the existence
C
of .
C
Corollary 30 (of Lemma 29 for Mixture Distributions) Let M π(i)p (Rd). Then,
i=1 i ∈ P2
for ǫ > 0, there exists a set of signature locations = ,..., Rd where every satisfies
1 M i
C {C C }P⊂ C
constraint Eqn. (10) for some matrix T Rd d, such that
i ×
∈
M M
W2 π(i)p , π(i)∆ #p ǫ. (34)
2 i Ci i
!
≤
i=1 i=1
X X
Proof According to Proposition 24:
M M M
W2 π(i)p , π(i)∆ #p π(i) W2(p ,∆ #p )
2 i Ci i
!
≤ 2 i Ci i
i=1 i=1 i=1
X X X (cid:0) (cid:1)
Consequently, if for all i 1,...,M we have that W2(p ,∆ #p ) ǫ/π(i), then we can conclude
∈ { } 2 i Ci i ≤
Eqn. (34). By Lemma 29, there always exists a that satisfies constraint Eqn. (10) such that
i
C
W2(p ,∆ #p ) ǫ/π(i), which concludes the proof.
2 i Ci i ≤
With the results of Lemma 29 and Corollary 30 established, we proceed to prove Theorem 17.
45Adams, Patan`e, Lahijanian and Laurenti
Proof We prove the Theorem for the ρ Wasserstein with ρ 1,2. If we do not compress the
− ∈
Gaussian Mixture approximates, i.e., R (q ) = q for every k, from Proposition 16:
M nn( ),k nn( ),k
X X
Wˆ Wˆ +Wˆ
ρ ρ,w,K σ ρ,K ρ,∆,K
≤ S L
h i
Wˆ +Wˆ + W
σ ρ,w,K ρ,w,K 1 σ ρ,K 1 ρ,∆,K 1 ρ,w,K ρ,∆,K
≤ L S S − L − − S
h i
...
≤
k K 1 k
K Wˆ +Wˆ + − kW
≤ Lσ Sρ,w,K −l Lσ ρ,0 ρ,∆,K −1 Lσ ρ,∆,k Sρ,w,K −l
Yl=0 h i Xk=0 Yl=0
K k
= kW ,
Lσ ρ,∆,k Sρ,w,K −l
k=0 l=0
X Y
Recall that we defined
Sρ,w,k
=
Dρ1E
Wk∼pwk[ kW
k
kρ]ρ1
(Eqn. (23)), so that, since it is assumed that
p
wk
∈
Pρ(Rnk×nnk−1), we have that
Sρ,w,k
∈
(0, ∞) for every k. Consequently, it is enough to
prove that for any ǫ
∆
> 0 there exists a signature location
k
Rnk of size N
k
such that for all
k 0,...,K it holds that Wˆ < ǫ . In fact, we can theC n c⊂ onclude by taking
ρ,∆,k ∆
∈ { }
ǫ
ǫ < .
∆ K k k
k=0Lσ l=0Sρ,w,K −l
Let Wˆ be computed as in Eqn. (12)Pusing CoQrollary 10 and Corollary 27. Then, according to
ρ,∆,k
Corollary 30, there exists a set of signature locations of finite size, such that Wˆ < ǫ for all
k ρ,∆,k ∆
C
k 0,...,k .
∈ { }
A.4 Proofs Section 7
Corollary 21: Wasserstein Distance between any GMM and a SNN
Proof Since W satisfies the triangle inequality, we have:
2
W (p ,q ) W (p ,q )+W (q ,q ),
2 nn( ) gmm( ) 2 nn( ) nn( ) 2 nn( ) gmm( )
X X ≤ X X X X
Given that MW is an upper bound for the 2-Wasserstein distance, we have:
2
W (q ,q ) MW (q ,q ).
2 nn( ) gmm( ) 2 nn( ) gmm( )
X X ≤ X X
Combining these two inequalities, we obtain:
W (p ,q ) W (p ,q )+MW (q ,q ),
2 nn( ) gmm( ) 2 nn( ) nn( ) 2 nn( ) gmm( )
X X ≤ X X X X
which completes the proof.
Proposition 19: Wasserstein-Optimal Grid for Signatures of Multivariate
Gaussians
Proof According to Corollary 10:
n
W 2(∆ Post( C¯1 ×... ×C¯n,T−1)+ {m }# N(m,Σ), N(m,Σ)) = λ(j)W2 2(∆ C¯j# N(0,1), N(0,1)).
j=1
X
46Finite Neural Networks as Mixtures of Gaussian Processes
This relation allows us to split the optimization problem of interest in n smaller optimization
problems as follows:
¯1a ,r ..g .,m ¯nin R,W 2(∆ Post( C¯1 ×... ×C¯n,T−1)+ {m }# N(m,Σ), N(m,Σ))
{Cn C¯j} =⊂
N
Pi=1|C |
n
= argmin λ(j)W2(∆ # (0,1), (0,1)),
¯1,...,¯n R,j=1
2 C¯j N N
{Cn C¯j} =⊂
N
X
Pi=1|C |
n
= argmin λ(j)W2(∆ # (0,1), (0,1)),
¯1,...,¯n R,
j=1
2 C¯j N N
¯j={C N∗, jC } 1⊂ ,...,n X
C j ∀ ∈{ }
n
= argmin W2(∆ # (0,1), (0,1)) ,
¯j R,¯j=N∗ 2 C¯j N N j=1
nC ⊂ C j o
where in step 2 we used the optimal grid-configuration N ,...,N as defined in Eqn.(27), and
{
1∗ n∗
}
in step 3, we use the independence of objective terms across dimensions.
Appendix B. Compression for Mixtures of Multivariate Bernoulli Distributions
We present an alternative to the procedure in Algorithm 3 to perform the compression operation
in the case of mixtures of multivariate Bernoulli distributions. We first show that mixtures of
Bernoullidistributionsnaturallyariseindropoutnetworksandthatefficientcompressionoperations
are required to formally analyze the output distribution of these networks. We then introduce an
alternative compression procedure that includes bounds on the resulting Wasserstein distance.
Let φ : Rn Rn with binary vector b 0,1 n return a vector with the elements in b
b
→ ∈ { }
masked, i.e., φ (z) = b z, where denotes the element-wise multiplication operation. For the
b
⊗ ⊗
dropout operation, we place multi-variate Bernoulli distribution over b that we denote as d . The
b
distribution d can be written:
b
d
b
= θ Pn l=1b(l) (1 θ)n −Pn l=1b(l) δ b,
−
b 0,1 n
∈X{ }
which is parameterized by the success probability, i.e., dropout rate, θ [0,1]. For a discrete input
distribution d = N π(i)c , which can represent a (set of) fixed in∈ put points of the SNN, the
in i=1 i
distribution at an intermediate layer of a dropout network, or the signature approximation at an
P
intermediate layer of a VI network, the output distribution of a dropout layer takes the form of a
mixture of multivariate Bernoulli distributions:
N
d
out
= E
z
∼din[φ(z)#d b] = π(i) θ Pn l=1b(l) (1 −θ)n −Pn l=1b(l) δ
b
⊗ci, (35)
i=1 b 0,1 n
X ∈X{ }
Note that d is a discrete distribution with a support of size N 2n, i.e., d (Rn). As
out out N 2n
· ∈ D ·
such,performingformalanalysisond quicklybecomescomputationallyintractableforreasonably
out
large n. Hence, we aim to construct a tractable discrete approximation d¯ (Rn) of size M
out M
∈ D
of d with error bounds. For this, we can employ Algorithm 3, which clusters the elements of
out
d into M clusters using M-means clustering and replaces each cluster with a single element using
out
47Adams, Patan`e, Lahijanian and Laurenti
moment matching. Thealgorithm then solves a discrete optimal transportproblem to compute the
resulting 2-Wasserstein distance. This procedure can be computationally demanding for mixtures
with a large number of components. Instead, we can use the structure of the mixture of Bernoulli
distributions to efficiently cluster the mixtures and obtain a closed-form upper bound on the 2-
Wasserstein distance, as we show below.
To compress d to a discrete distribution with M elements, we first collect the indices of the
out
log (M) neurons with the largest absolute weight value in set using a simple sorting operation.
2 I
Then, we take
d¯
out
= π(i)δ
ci
+ π(i) θ Pn l=1b(l) (1 −θ)n −Pn l=1b(l) δ
b ⊗ci
(36)
i c i b 0,1 n
X∈I X∈I ∈X{ }
where c = 1,...,N . Note that d¯ is equivalent to a mixture of multivariate Bernoulli
out
I { } \ I
distributionswithoutcomesmappedto¯0andc andwithprobabilityparameter1forthedimensions
i
in c and θ for the dimensions in . The following proposition provides a tight closed-form upper-
bouI nd on the 2-Wasserstein distanI ce between d and d¯ .
out out
Proposition 31 For multivariate Bernoulli distributions d
out N
2n(Rn) defined as in Eqn. (35)
and d¯ (Rn) as defined in Eqn. (36), we have that ∈ D ·
out M
∈ D
N
W (d ,d¯ ) (1 θ) c 2
2 out out i
≤ − k k
i c
X∈I
Proof According to Proposition 24:
W2(d ,d¯ )
2 out out
≤
π(i)W2
2
θ Pn l=1b(l) (1 −θ)n −Pn l=1b(l) δ
b
⊗ci,δ
ci
i c b 0,1 n
X∈I (cid:0) ∈X{ } (cid:1)
+ π(i)W2
2
θ Pn l=1b(l) (1 −θ)n −Pn l=1b(l) δ
b
⊗ci, θ Pn l=1b(l) (1 −θ)n −Pn l=1b(l) δ
b ⊗ci
i= b 0,1 n b 0,1 n
XI (cid:0) ∈X{ } ∈X{ } (cid:1)
Since the Wasserstein distance between a distribution and itself is zero, we obtain:
W2 2(d out,d¯ out)
≤
π(i)W2
2
θ Pn l=1b(l) (1 −θ)n −Pn l=1b(l) δ
b
⊗ci,δ
ci
i c b 0,1 n
X∈I (cid:0) ∈X{ } (cid:1)
Thetrivialoptimaltransportplanforeachiistomoveallprobabilitymassfrom
b 0,1
nθ
Pn l=1b(l)
(1
−
θ)n −Pn l=1b(l) δ
b ci
to c i. Therefore, for each i, we have that:
P
∈{ }
⊗
W2
2
θ Pn l=1b(l) (1 −θ)n −Pn l=1b(l) δ
b
⊗ci,δ
ci
= (1 −θ) kc
i k
b 0,1 n
(cid:0) ∈X{ } (cid:1)
which completes the proof.
48Finite Neural Networks as Mixtures of Gaussian Processes
(a) Empirical estimates and formal bounds on W2 (b) Empiricalestimates of W2 from grid-constrained
from grid-constrained signatures. and unconstrained signature locations.
Figure 9: The relative 2-Wasserstein distance between a 2D Gaussian mixture distribution with M
components and the signature with N grid-constrained locations obtained via Algorithm 2 (solid
anddottedlines),andthesignaturewithN unconstrainedoptimallocations(dash-dottedline). The
unconstrainedoptimallocationsareobtainedbyperformingN-Meansclusteringon1e3MCsamples
from the GMM. The solid lines represent the formal bounds on the relative 2-Wasserstein distance
provided by Algorithm 2, while the dotted and dash-dotted lines show empirical approximations.
Appendix C. Results on Signatures of Gaussian Mixture Distributions
In this section, we experimentally evaluate the effectiveness of Algorithm 2 in constructing signa-
tures on Gaussian mixtures. Specifically, we examine the conservatism of the formal bound on
the 2-Wasserstein distance resulting from the signature operation provided by Algorithm 2, and
analyze the restrictiveness of having the signature locations of each component of the mixture on
a grid as in Algorithm 2.
In Figure 9a, we analyze how the approximation error from thesignature of a Gaussian mixture
with M components, obtained according to Algorithm 2, changes with increasing signature size.
The plots show that as the signature size increases, the approximation error, measured as the
relative 2-Wasserstein distance W , decreases uniformly. Furthermore, in line with Corollary 10,
2
the formal upper bound on W is exact for Gaussians (M = 1). For Gaussian mixtures (M > 1),
2
theconservatism introducedbytheformalboundsgrows approximately linearly with increasingM.
This is because, in Algorithm 1, we bound the Wasserstein distance resulting from the signature
operation on the GMMs by the weighted sum of the 2-Wasserstein distance between each Gaussian
component in the mixture and their signatures (see line 7 in Algorithm 2 and Eqn. (12)).
Recall from Subsection 5.2 that to obtain a closed-form bound on the approximation error
in Algorithm 2, we place the signature locations of each component of the mixture on a grid
in the transformed space induced by the element’s covariance matrix, as illustrated in Figure 4.
Specifically, following Proposition 19, we choose the grids such that for a given signature size N,
the 2-Wasserstein distance from the signature operation is minimized. In Figure 9b, we estimate
the optimality gap if we could instead place the signatures freely to minimize the 2-Wasserstein
distance. Perhapssurprisingly,theplotsshowthatplacingthesignaturesfreelyonlyslightlyreduces
the 2-Wasserstein distance. Thus, although the optimal signature locations do not adhere to a grid
structure (in line with literature (Graf and Luschgy, 2007)), the optimal grid approximates the
optimal locations closely.
49