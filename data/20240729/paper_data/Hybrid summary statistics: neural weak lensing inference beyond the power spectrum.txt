Hybrid summary statistics: neural weak lensing inference beyond the power spectrum
T. Lucas Makinen∗ and Alan Heavens
Imperial Centre for Inference and Cosmology (ICIC) & Imperial Astrophysics, Imperial College London,
Blackett Laboratory, Prince Consort Road, London SW7 2AZ, United Kingdom
Natalia Porqueres
Department of Physics, University of Oxford, Denys Wilkinson Building,
Keble Road, Oxford OX1 3RH, United Kingdom
Tom Charnock
Axel Lapel
Sorbonne Universit´e, CNRS, UMR 7095, Institut d’Astrophysique de Paris,
98 bis boulevard Arago, 75014 Paris, France and
Sorbonne Universit´e, Universit´e Paris Diderot, Sorbonne Paris Cit´e, CNRS,
Laboratoire de Physique Nucl´eaire et de Hautes Energies (LPNHE). 4 place Jussieu, F-75252, Paris Cedex 5, France
Benjamin D. Wandelt
Sorbonne Universit´e, CNRS, UMR 7095, Institut d’Astrophysique de Paris,
98 bis boulevard Arago, 75014 Paris, France and
Center for Computational Astrophysics, Flatiron Institute, 162 5th Avenue, New York, NY 10010, USA
(Dated: July 29, 2024)
Ininferenceproblems,weoftenhavedomainknowledgewhichallowsustodefinesummarystatis-
tics that capture most of the information content in a dataset. In this paper, we present a hybrid
approach, where such physics-based summaries are augmented by a set of compressed neural sum-
mary statistics that are optimised to extract the extra information that is not captured by the
predefined summaries. The resulting statistics are very powerful inputs to simulation-based or im-
plicitinferenceofmodelparameters. WeapplythisgeneralisationofInformationMaximisingNeural
Networks (IMNNs) to parameter constraints from tomographic weak gravitational lensing conver-
gence maps to find summary statistics that are explicitly optimised to complement angular power
spectrum estimates. We study several dark matter simulation resolutions in low- and high-noise
regimes. We show that i) the information-update formalism extracts at least 3× and up to 8× as
much information as the angular power spectrum in all noise regimes, ii) the network summaries
arehighlycomplementarytoexisting2-pointsummaries,andiii)ourformalismallowsfornetworks
with smaller, physically-informed architectures to match much larger regression networks with far
fewer simulations needed to obtain asymptotically optimal inference.
Keywords: cosmology,statisticalmethods,weaklensing,large-scalestructure,machinelearning
I. INTRODUCTION (Dalal et al. 2023). However, two-point statistics do not
fully describe the rich non-Gaussian features present in
Weak gravitational lensing alters the trajectories of large-scale structure, where more cosmological informa-
distant photons as they pass through the large-scale tion might be found.
structure of visible and dark matter to our detectors.
Implicit inference (also known as simulation-based in-
These deflections alter the observed shapes of galaxies,
ference or likelihood-free inference) has made it possible
whose patterns can be used to trace the matter distri-
toutilisehigher-orderstatisticsderivedfromsimulations
bution in between, and are sensitive to parameters that
(seee.g. Cranmeretal.(2020)forareview),andcircum-
describetheexpansionhistoryandstructureformationof
vent the need for an explicit likelihood function, which
theUniverse. Theinferenceoftheseparametersfromcos-
can be challenging to compute via Bayesian Hierarchical
mologicalweaklensingsurveysisusuallyperformedusing
Models(Alsingetal.2016,Loureiroetal.2023,Porqueres
two-point statistics of the lensing images, such as shear
et al. 2021a,b, 2023, Sellentin et al. 2023). In weak grav-
correlation functions or power spectra. Recent analyses
itational lensing for example, even (incorrectly) assum-
includetheDarkEnergySurvey(Amonetal.2022,Secco
ingthattheunderlyingcosmologicaldensityisGaussian,
et al. 2022), the Kilo-Degree Survey (KiDS, Asgari et al.
thetwo-pointstatisticsthatdescribethisfieldcanthem-
2021,Lietal.2023)andtheHyperSuprime-Camsurvey
selveshavesignificantlynon-Gaussiansamplingdistribu-
tions (Alsing et al. 2016, Sellentin & Heavens 2017, von
Wietersheim-Kramsta et al. 2024). The question arises
∗ l.makinen21@imperial.ac.uk as to which additional statistics contain significant extra
4202
luJ
62
]OC.hp-ortsa[
1v90981.7042:viXra2
information about the parameters, beyond that which is neural summaries “hybrid” statistics since they combine
present in the two-point functions. Higher-order statis- new and existing functions of the data. We make our
tics are an obvious choice, but they suffer from a lack of constraint comparison within a completely simulation-
knowledge of their sampling distributions, and the very basedsetuptointerrogatetheinformationcontentofthe
large number of statistics make them cumbersome for respectivesummariesinasamplingdistribution-agnostic
implicit inference. However, advances in deep learning way.
have made it possible to learn highly-informative neural This paper is organised as follows: In Sections II and
compressions for massive simulations automatically, and III we present our general formalism for finding optimal
in some cases losslessly (Charnock et al. 2018, Makinen new summaries from simulated data given some exist-
et al. 2021, 2022). These compressions yield radically ing descriptive statistics, and describe how the strat-
smaller summary spaces, which are ideal for implicit in- egy can be implemented automatically with Information
ference, and which can be used for Bayesian posterior Maximising Neural Networks (IMNNs, Charnock et al.
estimationviaaccept-rejectordensityestimationstrate- 2018, Makinen et al. 2021). In Section IV, we describe
gies (Alsing et al. 2019). our mock weak lensing formalism and present the sim-
Thesenewadvanceshavemadesimulation-basedstud- ulation suite details. In Section V, we present our an-
iesforweaklensingapopularavenueofresearchinrecent gular C ℓ compression scheme as the existing statistic
years. This includes studies of peak counts (Kratochvil in the information-update formalism. We then present
et al. 2010, Lanzieri et al. 2023, Peel et al. 2017, Zu¨rcher our physics-inspired, lightweight neural network archi-
etal.2022),higher-orderstatistics(EuclidCollaboration tecture designed to find optimal additional summaries.
et al. 2023) such as wavelet scattering transformations In Section VIA, we make comparisons of information
(Chengetal.2024,2020),Fourier-spacenormalisingflows gain over the power spectrum as a function of resolution
(Dai & Seljak 2024), and field-based convolutional neu- and increased shape noise, and show that our optimisa-
ral networks (Fluri et al. 2019, 2018, Ribli et al. 2019, tion scheme captures physical features in realistic noise
Sharmaetal.2024). Theultimategoalistoobtainstatis- regimes.
ticsthatexhausttheinformationcontentoftheobserved
weaklensingfield. Thiscanbedoneexplicitly(assuming
alikelihoodforshearorconvergencevoxels)viafield-level II. IMPLICIT INFERENCE
sampling(Alsingetal.2016,Boruah&Rozo2023,Jasche
etal.2015,Leclercq2015,Loureiroetal.2023,Porqueres Thegoalofmostscienceexperimentsistoobtaindata
etal.2021a,b,2023,Ramanahetal.2019,Tsaprazietal. d with which to test models that describe the data gen-
2023). eration process. In cosmology, this often boils down to
Impicit inference approaches have matured enough for obtainingaposteriordistributionforsomemodelparam-
real-dataanalysis,beginningwithFlurietal.(2022),who eters θ: p(θ|d) ∝ p(d|θ)p(θ), which requires knowledge
analysedmap-levelKiDSdatawithanassumedGaussian ofthelikelihoodp(d|θ),andtheassumptionofasuitable
summarylikelihood,andmorerecentlyvonWietersheim- priorp(θ). Thisdata-generatingdistributionisoftentoo
Kramsta et al. (2024), who reproduced C constraints complicated to evaluate for inference, or too complex to
ℓ
with an implicit likelihood. Jeffrey et al. (2024) pre- write down analytically.
sented a Dark Energy Survey data analysis using a con-
volutional neural network compression of the full mass
A. Density Estimation
map and demonstrated marked improvement over exist-
ing power spectrum and peak count constraints on the
same dataset. Simulation-based inference circumvents the need for
a tractable likelihood p(d|θ), and instead seeks to pa-
This work seeks to demonstrate an improved optimi-
rameterise the underlying, implicit likelihood or poste-
sation strategy and add a new layer of interpretability
rior present in forward simulations of the data. Neural
to this growing body of literature. A common question
density estimators (NDEs; e.g. Bishop 1994) use neural
for deep learning and implicit inference practitioners is
networksthatgivesomeestimateq(θ,d;φ)ofthedesired
what features are being learned from the data by neural
posterior (or likelihood) by varying weights and biases
approaches. Makinenetal.(2022)showedexplicitlythat
(parameterised as φ) to minimize the loss
neuralnetworkstrainedonhalocataloguesidentifiedfea-
tures that could be linked to explicitly-understood cos-
N
mologicaldistributionssuchashalomassandcorrelation (cid:88)
U(φ)=− lnq(θ |d ;φ), (1)
i i
functions. Herewerespondtothisquestionbymodifying
i=1
our optimisation criterion such that a network only out-
puts statistics obtained from the data that work along- over batches of parameter-data samples drawn from the
side to an existing statistic, in this case the weak lensing joint distribution (θ ,d ) ↶ p(θ,d ). This loss is equiv-
i i i
angularpowerspectrum. Tobeexplicit,wetrainthenet- alent to minimising the Kullback-Leibler divergence be-
worktomaximisetheextraFisherinformationthatisnot tween the target distribution and q (Kullback & Leibler
already present in the power spectrum. We term these 1951). In this work we employ Masked Autoregressive3
Flows (MAF; Papamakarios et al. 2017) to model the tively small numbers of extra simulations for the
posterior distribution directly. We detail our implemen- new derivatives; e.g. the distribution p(θ,d) need
tationinSectionVC.Densityestimationalsomakespos- not be re-simulated.
teriorpredictiveandcoveragetestsfareasiertoperform.
We show examples of these tests in Appendix A. In the following section we will extend this approach
to find new (neural) data compressions that only in-
crease information about parameters above what is al-
B. Data Compression ready present in a set of existing statistics such as the
power spectrum.
Accept-reject and density estimation schemes become
more difficult to compare to a target, observed data vec-
tor d the larger the dimensionality dim(d), which III. HOW TO CHOOSE AN OPTIMAL NEW
obs
posits the need for data compression to some smaller SUMMARY
summary space x. We would like a function f : d (cid:55)→ x
that is ideally maximally informative about the parame- Consider some data d ∈ RN created from parame-
ters θ. Under certain conditions, f(d) can yield a suffi- tersθ thatcanbesummarisedinacompressedsummary
cient statistic, for which the dimension x is equal to the vector via a function h : d (cid:55)→ t with t ∈ Rnt where
number of parameters, e.g. dim(x) = dim(θ). Heavens n < N. We can estimate the covariance matrix of the
t
et al. (2000) introduced Massively Optimised Parame- summaries C , and the mean µ from simulations, along
t t
ter Estimation and Data (MOPED) compression, which with derivatives with respect to parameters of the mean
gives optimal score compression for cases where the like- µ . AssumingfornowthatthesummarieshaveaGaus-
,θi
lihoodandsamplingdistributionsareGaussian,andthis siansamplingdistribution(thisassumptionistemporary
was generalised to other forms of score compression by and is dropped in the inference phase), we can compute
Alsing&Wandelt(2018),Carron&Szapudi(2013),Hoff- the Fisher information of the observables via
mann&Onnela(2023). Neuralcompressionisapopular
scheme for learning mappings agnostic to sampling dis- [F ] =µT C−1µ (2)
t ij ,θi t ,θj
tributions, for which several optimisation schemes have
beenproposed. Regression-styleapproacheslearnacom- where we introduce the notation y ≡ ∂y/∂θ for par-
,θi i
pressionf(d;w)parameterisedby(neural)weightsw via tial derivatives with respect to parameters. The Fisher
a loss, for example quadratic, over parameter-data pairs informationmatrixheredescribeshowmuchinformation
over a prior, using variants of the mean square error h(d) contains about the model parameters, and is given
(MSE), or in some cases learning f and the neural pos- as the second moment of the score of the likelihood with
terior (via Eq. 1) simultaneously, dubbed Variational respect to h, assuming a parameter-independent, Gaus-
Mutual Information Maximisation (VMIM; Jeffrey et al. sian covariance of the statistic t.1 A large Fisher infor-
2020). Sharma et al. (2024) recently compared these mationforafunctionofthedataindicatesthatthemap-
losses paired with convolutional networks for a separate ping to t = h(d) is very informative about the model
weak lensing simulation suite. parameters used to generate the realisation of data d.
This work builds upon the Information Maximising Fisher forecasting for a given model is made possible by
NeuralNetwork(IMNN)approach(Charnocketal.2018, the information inequality and the Cram´er-Rao bound
Makinenetal.2021),whichprescribesaneuralcompres- (Cram´er 1946, Rao 1945), which states that the mini-
sionthatmaximisesthedeterminantoftheFishermatrix mum variance of the value of an estimator θ is given by
ofsummariesaroundalocal pointinparameterspace. A
compression is i) learned at a fiducial point from a set of ⟨(θ −⟨θ ⟩)2⟩≥(F−1) , (3)
i i ii
dedicatedfiducialandderivativesimulationsandthenii)
appliedtosimulationsoverapriorforposteriorconstruc- with no summation over i.
tion. This approach has numerous advantages, namely: We would next like to add new summary statistics to
increase the information over what is present in t. For
1. An asymptotically-optimal compression can be
clarity, we begin by adding a single summary x, before
learned from simulations around a single point in
generalising to multiple additional summaries. We do
parameter space.
this via some function f : d (cid:55)→ x. This new number has
2. Thecompressionautomaticallyandsimultaneously
gives Fisher posterior constraint forecasts.
3. Priors used for density estimation are decoupled
1 Note that the Gaussian assumption is used here only to de-
from the compression step and can be chosen after fine a compression; once the summaries are defined, SBI no
learning the compression. longer assumes Gaussianity. If the compressed summaries are
not Gaussian-distributed, the compression will be suboptimal
4. Adding additional parameters of interest to the but the downstream SBI analysis will implicitly determine and
compression learning scheme only requires rela- usetheirtruesamplingdistribution.4
+ pixel-level info
𝐱 𝐅
𝐭 𝐅
’
𝐝 = 𝜿
%
ℓ
large-scale info
{𝜽 ,𝜽±}
!"#
FIG. 1. Hybrid summary network schematic, illustrated for weak gravitational lensing. Noisy data (weak lensing κ with
b
shape noise) are passed in parallel to an existing summary function (tomographic C with optional MOPED compression) to
ℓ
producesummariest,andanetwork(CNN)tooutputanadditionalsetofsummariesx,describedinSectionVandillustrated
inFigure2. TotrainthenetworktheFisherinformationisfirstcalculatedfortandthenupdatedviaEquation11toyieldF,
for the loss Eq. 14.
varianceσ2andwhenconcatenatedtotheoldobservables
x
gives the mean vector Interpretation. The updated Fisher information in
Equation 7 clearly separates the information contribu-
µ=[µ t,⟨x⟩]T. (4) tion from the existing observables in the first Fisher
term and the new observables in the second term. An
For mean-subtracted quantities ∆t and ∆x, the covari- optimal, “complementary” new observable x adds a lot
ance vectors between old and new observables can be of information if it has highly correlated measurement
computed (e.g. from simulations) as [u] i = ⟨∆t i∆x⟩, error with the existing summaries t, but changes with
which yields the full covariance matrix respect to parameters in a way that is as distinguishable
as possible from how x and t change together.
(cid:18) (cid:19)
C u
C= t . (5)
uT σ2
x Multiple New Observables. The IUF can be nat-
urally extended to a vector of new summaries x. We
Notice here that the smaller the values of u, the less
promote v to a matrix
correlated x is with t. The full updated Fisher matrix is
then [V] =⟨x ⟩ −µT C−1u , (9)
ij j ,θi ,θi t j
F =µT C−1µ . (6) and the scalar s generalises to the matrix
ij ,θi ,θj
With some rearrangement, we obtain the fast Σ=C −UTC−1U (10)
x t
Information-Update Formula (IUF):
where [U] =[u ] and C is the covariance of network
ij j i x
1 outputs x. Altogether the updated Fisher matrix for a
F=F + vvT, (7)
t s vector of extended summaries is
with[v]
i
=⟨x⟩ ,θi−µT ,θiC−
t
1uands=σ x2−uTC−
t
1u. This F=F t+VΣ−1VT. (11)
calculationisonlyO(n2 +n d+d2)operations
params params
where d=dim(t), which is asymptotically d times faster
A. Finding a New Summary With a Neural
than Eq. 6 when d >> n . This formalism also
params Network
yieldsafastupdateforthedeterminantofthenewFisher
matrix:
WecanfindanewobservablexbyoptimisingtheIUF
(cid:18) (cid:19)
1 equation (7) with a neural network f : d (cid:55)→ x that op-
lndetF=lndetF +ln 1+ vTF−1v . (8)
t s t erates on the data. This formalism folds neatly into the
esion
)𝐝+
(NNC
)𝐝+
(
𝐶
ℓ
𝐶 ℓ5
IMNN formalism (Charnock et al. 2018, Makinen et al. combined Fisher information. The network can do no
2021, 2022). We illustrate this procedure for weak lens- worse at summarising the data than the existing sum-
ing data in a schematic in Figure 1. We can choose mary, since the loss only optimises the second term of
to optimise Equation 6 directly, but Equation 11 is less Eq. 11. WiththeupdatedFisherinformationwecanalso
computationally expensive for large covariance matrices computequasi-maximumlikelihoodestimates(MLE)for
and more than one additional summary. The ingredi- the parameters for a given mean-subtracted summary
ents needed to compute the components of the loss are a vector ∆=[∆t,∆x] (Alsing & Wandelt 2018):
suite of n simulations at a fiducial value of parameters
s
{d}ns | and a set of n seed-matched simulations θˆ=θ +F−1µ C−1∆T. (15)
at
pi e= r1 tuθ r= bθ efi dd
values of
eachd
parameter θ± = θ ±∆θ
fid ,θi
i i i
holding all other parameters fixed at their fiducial val- Wethenusethesehybridstatistics(astheyarefunctions
ues. Usingthisfinitedifferencegradient datasetthepar- of the data) as our highly-informative and extremely
tial derivatives of a data summary function Q(d) with compressed data set, ideal for simulation-based or im-
respect to parameters is plicit inference. The resulting compression is asymptoti-
cally optimalatthefiducialpointinparameterspace,but
(cid:18) ∂µˆ i(cid:19)
≈
1 (cid:88)nd Q(d+
i
)−Q(d−
i
)
. (12)
for smoothly-varying data manifolds results in a smooth
∂θ n θ+−θ− summaryspacethatcanbeexploitedforneuralposterior
α d α α
i=1 estimation away from the fiducial point as described in
For n summaries, this method requires n × Charnock et al. (2018), Makinen et al. (2021). A useful
params d
n × 2 simulations with n unique random seeds aspect of learning this local compression is that a prior
params d
alongside the n simulations at the fiducial point re- for posterior estimation can be specified after the com-
s
quired for the covariance. This is done for the mean of pression network is trained, unlike regression networks.
both existing and new summary statistics, consolidated
as y = [t,x]. The covariance of the (existing and new)
summaries is estimated from the data as well, using n IV. WEAK GRAVITATIONAL LENSING
s
simulations at θ :
fid
A. Formalism
Cˆ =
1
(cid:88)ns
(y −y¯) (y −y¯) , (13)
αβ n −1 i α i β The effect of weak lensing (WL) on a source field is
s
i=1
defined by its shear, γ, which captures the distortions
wherey¯ istheaverageoverthesimulationsatthefiducial in the shapes of observed galaxies. In the flat-sky limit
point. The full covariance can be broken down into (or in Fourier space, this observable can be related to the
estimated separately by) its components C ,C , and u convergence field κ observable, which describes variation
t x
accordingtoEq. 5. Notethatthiscovarianceisassumed in angular size:
to be independent of the parameters, which, whilst not
strictly true, is enforced by regularisation during the fit-
γ˜(ℓ)=
(ℓ 1+iℓ 2)2
κ˜(ℓ) (16)
ting of a network. If it does not hold, it simply makes ℓ2
the compression suboptimal elsewhere in the parameter
where ℓ = (ℓ ,ℓ ) is the complex wavevector. The
space. Crucially, both old and new summaries and their 1 2
convergence field can be connected to the underlying
statistics must be computed on the same (noisy) simula-
dark matter field by integrating the fractional overden-
tions to correctly distinguish between noise fluctuations
sity along the line-of-sight to give (Kilbinger 2015):
and newly-informative features of the data during opti-
misation.
3H2Ω (cid:90) rlim rdr
Optimising a neural function x=f(d;w) to maximise κ(ϑ)= 0 m g(r)δf(rϑ,r), (17)
2c2 a(r)
the determinant of F from Eq. 11 forces the new sum- 0
maries x to add complementary information to the ex-
where ϑ denotes the coordinate on the sky, r is the co-
isting summaries’ Fisher contributions. As described in
movingdistance,r isthegalaxysurvey’smaximumco-
Charnocketal.(2018)andLivetetal.(2021),theFisher lim
moving distance, δf is the dark matter overdensity field
information is invariant to nonsingular linear transfor-
at scale factor a, and, for a flat Universe
mations of the summaries. To remove this ambiguity, a
term penalising the network summary covariance C x is (cid:90) rlim r−r′
added. This gives the loss function g(r)= dr′n(r′) , (18)
r
r
1
Λ=−lndetF+λ 2trC x (14) is the integration of the redshift distribution n(r) in the
given comoving shell. In real-data analyses the data will
where λ is a regularising coefficient. This scalar loss be the cosmic shear, but here we restrict our analysis
function can be optimised via gradient descent to up- to noisy convergence maps. The forward model to gen-
date weights w for the network’s contributions to the erate κ consists of a cosmological parameter draw, θ,6
which is used to generate primordial fluctuations, δic. increasingly nonlinear scales described by the particle-
Here the initial conditions are a Gaussian random field mesh (PM) simulations. We compute the line-of-sight
governed by the Eisenstein & Hu (1999) cosmological integral in comoving units before binning the L dimen-
z
power spectrum P(k;θ), which includes baryonic acous- sion in redshift bins converted to comoving units via the
tic oscillations (BAO). The cosmic initial conditions are cosmology-dependent change of variable. For this anal-
then evolved forward via a specified non-linear gravity ysis we do not include lightcone effects. We choose our
model G(δic), which describes the growth of the large- four tomographic redshift bins to be Gaussian, centred
scale structure (LSS). The evolved dark matter field δf at z = [0.5,0.75,1.0,1.25] with width σ = 0.14, follow-
z
is then used to generate the convergence field. Using the ing Porqueres et al. (2021a). The resulting convergence
Born Approximation, we implement a discrete version of fields span a 3.58×3.58 deg2 field of view. Shape noise
Equation 17 using a summation over voxels to approxi- is added to the noise-free simulations before computing
mate the radial line-of-sight integrals: two-point or network statistics as described below. We
generate two distinct datasets to i) construct a locally
 
κb
mn
=
3H 202 cΩ
2m
(cid:88)N
δ mf
nj(cid:88)N (r
s
r−r j)
nb(r s)∆r
sr
j
a∆r
j,
o esp tt ii mm aa tl ioc no .mpression and ii) perform posterior density
j=0 s=j s j Compression Simulations. For a given resolution
(19) we generate two equally-sized datasets for training and
wherebindexesthetomographicbin,andm,nindexthe validation of our network compression. To calculate
spatialpixelsonthesky. Theindexj indicatesthevoxel network and two-point covariances described below we
along the line-of-sight at the comoving distance r . The simulate n = 1500 simulations at a fiducial cosmol-
j s
total number of voxels along the line-of-sight, N, is ob- ogy θ = (Ω ,S ) = (0.3,0.8). For finite-difference
fid m 8
tained from a ray tracer. ∆r is the length of the line derivatives we simulate n × 2 × n = 375 × 2 × 2
j d p
segment inside voxel j, and δf is the discretized dark seed-matched simulations at a perturbed parameter set
mnj
matteroverdensityfield. Thecomovingradialdistancer s θ±∆θ fid = θ fid±(0.0115,0.01). All other cosmological
is the distance to the source. Each tomographic bin has parameters were held fixed at Planck 2018 parameters
a source redshift distribution nb(z ). Once κb is com- (Planck Collaboration et al. 2020). The total number of
s mn
puted, the convergence field d = {κˆb } is obtained by simulations used for optimal compression is thus 4500.
mn
adding uncertainties equivalent to the shape noise (and Density Estimation Simulations. Because our
measurement error) in the shear field. This is captured compression from the convergence field data is learned
by zero-centred Gaussian white noise added pixel-wise locally, we are free to choose our prior guided by the
with variance compressionmethod’sFisherforecast. Wesimulate5000
simulations over a uniform prior in (Ω ,S ), whose
m 8
σ2 =σ2 N tomo , (20) width is chosen according to the strategy described in
n ϵn Ab Section VC.
gal pixel
where σ2 is the total galaxy intrinsic ellipticity disper-
ϵ
sion, n is the source galaxy density on the sky, and
gal
Ab istheangularsizeofthepixelineachtomographic V. FINDING HYBRID WEAK LENSING
pixel
bin. For Stage-IV weak lensing surveys like Euclid n STATISTICS
gal
will be ∼ 30 arcmin−2 and σ ≃ 0.3 (Euclid Collabora-
ϵ
tion 2022). For network training purposes we introduce The information-update formula is perfectly suited to
anamplitudescalingparameterσ′ =Aσ thatwereport improving weak lensing Ω −σ parameter constraints
n n m 8
in terms of effective source galaxy density. with neural summaries. We would like to know if more
cosmological parameter information can be extracted
from the convergence field beyond a simulation-based
B. Simulation Details tomographic angular C ℓ statistic analysis, and in what
resolution regimes. We present the MOPED scheme for
angular C compression and information-update neural
Weanalyseseveralsimulationsuitesatdifferentresolu- ℓ
network architecture.
tions to conduct our experiments. In all cases our phys-
ical box size is kept fixed at L = L = 250 Mpc h−1
x y
and L = 4000 Mpc h−1 in comoving units, in a pixel
z
grid of shape (N x,N y,N z) = (N,N,512), where we A. MOPED Angular C ℓ Compression
vary N to probe changing gravity solver scales. We
utilise pmwd particle mesh (PM) simulations (Li et al. Here our existing summaries are either binned angu-
2022) integrated for 63 timesteps to generate the non- lar C or MOPED-compressed vectors t (without the
ℓ
linear dark matter overdensity field for N = [64,128] optional Gram-Schmidt orthogonalisation employed in
resolution and 100 timesteps for N = 192 resolution. Heavens et al. (2000)). We outline our setup below and
This controls the particle spacings L/N which probe display a schematic of the architecture in Figure 1.7
We compute empirical auto- and cross-spectra C
ℓ
across the four noisy tomographic bins, resulting in 10
C vectors. To test scale-dependent information, we op- ℓ!=0
ℓ
tionally apply a maximum ℓ to each vector to mimic
cut ℓ!=1
existing survey analyses. To reduce the number of sim-
ulations needed to estimate the covariance matrix, we ℓ!=2
bineachspectrumintosixevenly-spacedℓbinsweighted
by C value. We can estimate the covariance of the C
ℓ ℓ
vector using the n fiducial simulations, and the finite
s
difference derivatives with respect to each parameter via
Eq. 12. Together, the Fisher matrix for these sum-
maries is computed with Eq. 2. With these ingredi-
entswecanthenperformtheMOPEDcompressionfrom
mean-subtractedvectorsevaluatedatafiducialsetofpa-
(cid:16) (cid:17)
rameters ∆ = Cˆ −⟨C ⟩ down to score summaries
ℓ ℓ fid
t:
t=θ +µ C−1∆T. (21)
fid ,θi t
whichcanthenberescaledbytheFishermatrixtoobtain
an MLE of the parameters (Alsing & Wandelt 2018):
θˆ =θ +F−1t. (22)
MOPED fid t
In practice, we replace t with θˆ as our existing
MOPED
MOPED statistics, which has covariance C = F−1.
t t
These compressed summaries are the default C -based
ℓ
summaries that we feed into the normalising flow pos-
terior estimation scheme (Section VC), as normalising
flows are not guaranteed to work well with large in-
puts e.g. the 60-dimensional binned C vector (Cran-
ℓ
mer et al. 2020). For network optimisation however, the
longer,binnedpowerspectrumvectorcanbeusedtofind
θˆ . Changesintheℓbinswithrespecttonoiseand
network
parametersincreasesthenumberofcross-correlations(u)
with network summaries and encourage improved infor-
mationextraction. Weexplorethisoptionfortrainingin
FIG. 2. We use a small convolutional neural network that
noisy settings in Section VIB. Both choices of statistics
exploitsthedatasymmetriestocompressκfieldsdowntoad-
fit neatly into the existing statistic formalism described
ditional summaries. Input data (here of shape (128,128,4))
in Section III.
are passed through a residual multipole kernel layer (shared
colourindicatessharedweights)andthensubsequentlypassed
to convolutional blocks with varying strides with small 2×2
B. Physically-Informed Neural Network kernels to capture fluctuations on different scales. All linear
Architecture layersarefollowedbyanonlinearactivationfunction. Dashed
arrowsindicatefeatureconcatenationatthesamespatialres-
olution. Thisdownsamplingcontinuesuntilthespatialresolu-
We design a 2D convolutional neural network with a
tionofthedatareaches8×8,afterwhichtheoutputtensoris
dedicatedstructuretoextractinformationintheconver-
mean-pooledalongthespatialaxesandpassedtothreedense
gencefielddatainawaythatweknowtheinformationis
layers. The output from the network is a pair of numbers.
likely to be distributed, and then learn a downsampling
compression of these relevant features. We present this
network layer-by-layer and display a schematic in Figure
where κ = |κb |+0.01, where κb is the minimum
2. o min min
convergence value for the tomographic bin b.
The inputs to the network are the log-transformation
of the convergence maps at the four specified tomo-
graphic bins, adapted from Joachimi et al. (2011), Seo Multipole Kernel Embedding. For convergence
et al. (2010), Simpson et al. (2015): maps we can target clustering information by learn-
ing convolution functions of the data with certain, en-
κb =κ ln(cid:2) 1+κb/κ (cid:3) (23) forced symmetries. Kodi Ramanah et al. (2020) demon-
o o
embedding
downsampling8
strated via neural emulation of dark matter simulations tinuedownsamplinginthistree-likefashionuntilthedata
that learned convolutional kernel weights tend to be dis- reach a spatial resolution of 8×8. We then mean-pool
tributed in spherically-symmetric ways. This motivates thefeaturesalongthespatialaxesandpasstheresulting
ordering kernel complexity by increasing breaking of ro- flattenedfilteraxistoafinallinearlayerthatoutputsthe
tationalsymmetry. Charnocketal.(2020)andDingetal. desired additional summaries. Every layer is followed by
(2024) explicitly encoded multipole expansion symme- anew,bijectivesmooth leakyactivationfunction,which
tries in CNN kernels to learn bias corrections in large we found empirically extracted information most consis-
scale structure modelling. Convolutional kernel weights tently across our experiments:
are shared for kernel pixels equidistant from the centre 
x, x≤−1
of a 3D or 2D kernel, associated to the spherical har- 
monic coefficients Yℓk(θ,ϕ). Here we make use of these smooth leaky(x)= −|x|3/3, −1≤x<1 (24)
multipole kernels
(Mm
PK) in a 2D setting for informa-
3x
x>1.
tion capture, embedding the convergence field using a
The intuition here is that unlike natural image data,
smaller number of neural weights. This choice of embed-
lensing shear maps are relatively smooth functions, so
dingisalsolikelytoimproveperformanceinthepresence
are best linked to smoother activation functions follow-
of (white) noise, as noise artefacts are not distributed
ing convolutions, in contrast to natural images with
with the same rotational symmetry as convergence clus-
sharp features like feature borders, for which typical
tering features.
disjoint activations like the leaky ReLU were developed
We first embed each log-transformed tomographic
(Xu et al. 2015).
bin into six filters corresponding to the ℓ = [0,1,2]
k
multipole moments for a 7×7 kernel per tomographic
Training Setup. To train the network we split our
bin, which for e.g. N = 128 corresponds to a 0.19 deg2
dataset into equally-sized validation and training sets,
receptive field. We illustrate a cartoon example of
withthesamen numberoffiducialandn seed-matched
s d
these symmetric kernels for a 3 × 3 kernel in Figure
derivative simulations. Every epoch a new noise realisa-
2. This output is then passed to a nonlinearity and
tion is added onto the noisefree convergence maps and
then to another multipole kernel for each input filter,
a random rotation is performed. These transformations
which are then summed along the filter axis at each
are seed-matched for each derivative simulation index.
multipole kernel to yield six output channels. We learn
We use the adam optimiser with a fixed learning rate
the residual from the first embedding layer l to the
of 0.0005 with gradient clipping at a value of 1.0, and
next, e.g. xl+1 =mpk layer(xl+ mpk layer(xl)). This
a weight decay penalty of 0.0005 added to the loss
choice of data embedding layer drastically reduces the
function. These two modifications to the optimisation
number of learnable weights and forces the network to
routine “smooths” the loss landscape and prevents the
learn physically-symmetric functions of the data in its
network from overfitting to the training data, respec-
first layer. The largest model considered here contains
tively. Training is halted when the validation loss stops
just 6,904 trainable parameters, which is 0.08% the
decreasingsignificantlyforapatiencenumberofepochs.
footprint of the ResNet18 employed e.g. by Lanzieri
et al. (2024), Sharma et al. (2024).
Noise Hardening. All networks are first trained at
a low noise level, A = 0.125, after which the noise
noise
Incept-StrideTreeNetwork. Theembeddeddataare level is increased in increments of ∆A = 0.05 for
noise
thenpassedtoaninception-stylenetwork(Szegedyetal. a minimum of 100 epochs subject to a patience setting
2016) with one important difference: instead of varying of 75 epochs at each setting. This can be thought of
kernel sizes, we keep the kernel shape fixed to 2×2 and as “domain-transfer” learning on-the-fly. Slowly increas-
vary the stride that each layer takes in parallel passes ing the noise allows physical features (e.g. convergence
over the data. The objective of this section of the net- patterns) to be embedded early in training, such that
workistodownsampletheembeddeddatabycombining the network outputs are already concentrated on the in-
informationfromdifferentscalessothattheonlyfeatures formative distribution of the data when the shape noise
on informative scales are strongly activated and pushed increases.
through the learned network to the output summaries
using the fewest independent kernel weights possible.
The data is passed to stride-2, stride-3, and stride-4 C. Neural Density Estimation
downsampling layers followed by a nonlinear activation
functionandasubsequentstride-1convolution. Theout- To measure the information capture in both MOPED
puts of the stride-3 block are padded periodically in the and network summaries we employ a neural poste-
spatial dimension and concatenated to the output of the rior estimation scheme to parameterise the amortised
stride-2 block, and then passed to another stride-1 con- summary-parameter posterior p(θ|y), where y(d) is ei-
volution. The stride-4 outputs are kept aside until the ther the MOPED summary or updated summary set
datahasbeenpassedtothenextinceptionblockandthe of MLE parameter estimates using Eq. 15. We em-
data has reached the same spatial resolution. We con- ployanidenticalensembleofmaskedautoregressiveflows9
NDE MOPED(C ℓ=1500)
𝑞 𝜃|𝜃$;𝜙 ≈𝑝 𝜃𝜃$) MOPED(C ℓ=6400)
net(field | C ))
ℓ
𝐭
𝜽 𝐝 𝜽%
𝐱
1.8
FIG.3. Cartoonofdensityestimationschemewithfixedcom-
pression(networkorMOPED).Parametersθaredrawnfrom 1.5
a prior and MLE estimates θˆ are produced from data d for
either (fixed) compression method using Eq. 15 and fed to 1.2
a MAF neural density estimator for the amortised posterior
distribution, trained under the loss in Eq. 1. 0.9
0.6
( foM rA eF acs h; A sels tin og
f
se ut ma ml. a2 r0 ie1 s9, usP inap ga tm ha eka Lr ti Uos -ILet
I
a cl o. d2 eb01 a7 se) 0.18 0.24 0.30 0.36 0.42 0.6 0.9 σ1.2 1.5 1.8
Ω 8
(Ho et al. 2024). We opt for networks with 50 hidden m
units and 12 transformations. We chose this high level
FIG.4. Usinginformation-updatenetworksummaries(green)
of complexity such that the posterior density parame-
drasticallyimprovesΩ −σ constraintsbeyondMOPEDC
terisations in all cases were sufficiently descriptive. A m 8 ℓ
summariesinalow-noisesetting. Wecomparetheposteriors
unique aspect of our network training scheme is that a
obtained from a KiDS-like survey truncation at ℓ = 1500
cut
jointparameter-datapriordistributioncanbechosenaf-
(blue)totheconstraintsfromallavailablemodesℓ =6400
cut
ter learning the network and MOPED compression, dis- atthegivenresolution(green). Thenetwork’sadditionalsum-
played as a cartoon in Figure 3. We generated 5000 maries(darkgreen)isabletoimproveinformationextraction
simulations for each of two wide uniform priors in the byafactorof5beyondtheℓ =6400andafactorof8above
cut
S 8 formalism: p(1)(Ω m,S 8) = U[[0.15,0.35]×[0.7,1.52]] ℓ cut =1500.
and p(2)(Ω ,S )=U[[0.15,0.35]×[0.5,1.0]]. We opt for
m 8
the smaller prior in cases where the 3σ Fisher posterior
estimate for the observable considered falls within the
support of p(2).
VI. RESULTS
A. Low-noise Regime
We first investigate the information extraction as a
function of dark matter simulation resolution with a
small amount of shape noise, and compare information
extraction to two-point statistics at different scales.
We construct particle mesh simulations with varying
numbers of pixels N = N ∈ [64,128,192]. Intuitively
x y
we expect more information beyond the two-point
statistic to be found at higher resolutions, which can
be interpreted as the descriptiveness of the underlying
gravity model. FIG. 5. Information-update network (bottom) makes sim-
ulations more distinguishable in summary space than C
ℓ
compression (top). Points in parameter-summary space are
Scale Cutoff. We first explored the effect of a
coloured by the opposite parameter’s value. The network
scale cutoff at resolution N = 128 for the C summaries
ℓ finds patterns that separate these summaries in a comple-
with a low noise setting. We construct MOPED sum-
mentaryfashionevenawayfromthefiducialpoint(Ω ,S )=
m 8
maries from C ℓs subject to a Stage-III survey-like cut at (0.3,0.8). Wedisplaya3Dviewofthisfour-dimensionalspace
ℓ cut = 1500, as well as summaries from all available ℓ in Figure 11.
modes at the given resolution. The highly-compressed
MOPED summaries give almost identical posteriors
simulator
compression
σ
810
L/N=3.91 Mpc h−1 L/N=1.95 Mpc h−1 L/N=1.30 Mpc h−1
1.0 1.0 1.0
0.5 0.5 0.5
0.2 0.4 0.2 0.4 0.2 0.4
Ω Ω Ω
m m m
MOPED(C ) network(field | MOPED(C ))
ℓ ℓ
FIG. 6. Computing additional complementary summaries from the convergence field improves parameter constraints (green)
over the two-point information (blue) as the field becomes more nonlinear in the low-noise regime. For N = 64 fields the
informationgainabovetheC constraintsismodest,butimprovesasmorenonlinearscalesareincludedatthelevelofthefield
ℓ
as resolution increases.
to using the full set of C values as the data vector.
ℓ resolution H(C ) H(net) ratio
The network is tasked with finding complementary ℓ
N = 64 6.9 7.4 2.9
summaries in the ℓ = ℓ case. We display the
cut max
constraints obtained on the same target simulation in low noise N = 128 6.9 7.7 5.0
Figure 4 and in Table I. The network extracts up to 5 N = 192 7.6 8.5 5.1
times more information than the two-point function in
high noise N = 128 5.3 6.0 4.5
a low-noise setting with all modes and 8.3 times more
information in high-noise settings, as measured by the N = 192 5.2 6.3 8.3
determinant of the Fisher matrix.
Summary Scatter. The information-update loss
TABLEI.SummaryofparameterShannoninformation(H=
scheme asks the network to use data features such 1lndetF) from MOPED and information-update networks
2
that output summaries complement existing C ℓ-based forlownoise(n
gal
=1900)andhighnoise(n
gal
=83)scenar-
summaries. We can interpret the statistics learned by ios. The ratios of Fisher determinants are shown in the last
the network by visualising a summary scatter over the column. For the noisy N = 192 case we optimise networks
suite of prior simulations. Figure 5 shows the network againstthebinnedC ℓvectorsasopposedtotheMOPEDsum-
and MOPED outputs versus true parameter, coloured maries.
by the opposite parameter’s value for summaries used
to generate the network and ℓ = 6400 posteriors in
max
Figure 4. Remarkably, even though the network is only
trained at the fiducial cosmology (dashed vertical line is then harnessed by the density estimation scheme to
in each plane), the information-update loss allows the provide tighter parameter constraints than MOPED.
network to find useful features with which to distinguish The complementary nature of the information from the
parameters in a smoother summary space (less scatter) network-updated statistics to the original statistics de-
thantheMOPEDcompression. Thisincreasedstructure creases away from the fiducial point in both dimensions,
σ
811
but does so smoothly, i.e. the information about the N=128 N=192
parameters coming from the four statistics is mixed
away from the fiducial point. 1.0 1.0
Resolution Dependence. We next compare con-
straintsasafunctionofPMsimulationresolution,which 0.5 0.5
effectively controls the nonlinearity of the dark matter
gravity solver. Here we wish to measure the parameter 0.25 0.50 0.25 0.50
informationgainthatusinganonlinearnetworktoprobe Ω Ω
m m
nonlinear scales adds to the power spectrum. We gen-
erate three suites of fiducial and finite-difference conver-
FIG. 7. Additional neural summaries (green) are robust to
gencemapstolearnthecompressionwithA noise =0.125. shape noise (n gal =120) at different resolutions. Constraints
Figure6showsconstraintsateachresolution. Morenon- fromC -onlysummaries(blue)sufferfromtheincreasednoise
ℓ
Gaussian information is extracted for the higher reso- due to shot-noise contributions to the high-ℓ bins.
lution simulations, indicated by the increase in network
constraining power over the C ℓs, since these simulations n gal=99 arcmin−2 n gal=37 arcmin−2
probesmaller,morenonlinearscalesaccessedbythenet-
work. We report our network and MOPED Fisher con-
1.0 1.0
straints in Table I. In this low-noise setting we observe
an information increase of a factor of 2.9 for N = 64
and a factors of 4-5 for N = [128,192] high-resolution
0.5 0.5
simulations, aligning with our intuition.
0.25 0.50 0.25 0.50
Ω Ω
m m
B. High-noise Regime
FIG. 8. Neural summaries (green) are robust to increased
The information-update formalism displays promising shapenoise,controlledbythegalaxydensityparametern .
gal
results in the presence of increased systematics such as Increased shot noise at small scales inhibits angular C ℓ con-
galaxyshapenoise. Herewestartwithanetworktrained straints (blue). Here we display an inference on the same
on the lowest noise setting and slowly increase the noise N = 192 resolution simulation subject to increased shape
noise. We optimise the network using the binned C vectors.
amplitude (equivalently decreasing the galaxy density). ℓ
The network is able to increase its relative performance
against the two-point statistic as we increase resolution
function in the noisiest (n =30) setting, compared to
(Fig. 7) and shape noise (Fig. 8). Figure 7 shows that gal
a 5.6 times improvement when optimising against the
with increased simulation resolution the network has
MOPED-compressed summaries. We visualise the joint
access to more nonlinear scales and can compensate for
covariance of learned and binned C summaries (Eq. 5)
the shot noise that dominates the C calculation at high ℓ
ℓ in Appendix B.
ℓvalues. Forafixedresolution(N =192),theadditional
summaries found by the network appear robust to noise;
keepingtheparameterconstraintsconsistentasincreased
shape noise pushes the C constraints towards the prior VII. DISCUSSION & CONCLUSIONS
ℓ
edge in Ω . This is especially promising for higher noise
m
cases for galaxy density n < 50 arcmin−2, as this In this paper, we present an implicit inference
gal
falls between the capabilities of Euclid (Euclid Collab- technique to extract neural summary statistics from
oration2022)andRoman(Spergeletal.2015)telescopes. field-level data, specifically weak lensing maps, that are
designed to match or increase automatically the Fisher
Optimising With the Binned Power Spec- informationaboutthecosmologicalparametersoveraset
trum. For our high-resolution simulations we also of pre-defined summaries, typically traditional two-point
explored optimising the information-update formalism statistics. Weapplythismethodtofindsummarystatis-
(Eq. 11) with respect to the full binned C vector. tics from tomographic convergence maps that explicitly
ℓ
This “stretches” the off-diagonals u of the full summary complementtheangularpowerspectrumestimates. This
covariance(Eq. 5), suchthattheIUFforcesthenetwork powerful hybrid mixture of physics-based and neural
to respond to explicit fluctuations in these C bins with network derived summary statistics is guaranteed to
ℓ
respect to noise. Here we posit that the network will improve the two-point parameter constraints and allows
be able to find summaries that complement fluctuations for networks with small physics-informed architecture
at different ℓ scales more efficiently. We find that to achieve similar results to larger regression networks.
indeed this choice of optimisation allows the network to We demonstrated that this approach extracts between a
extract 8.3 times more information than the two point factor 3 and 8 more information than the angular power
σ
σ
8
812
spectrum, as measured by the determinant of the Fisher alised to any dataset to identify the features from which
matrix. For weak lensing, the main gain is a substantial theinformationcapturedbylargeneuralnetworkscomes.
reduction in the credible region for Ω , with a smaller This technique might also reduce the need for large con-
m
improvement in the S error. volutional networks to learn the large-scale correlations
8
in larger dark matter and galaxy simulations (Lemos
et al. 2023b). In future work, we will apply this for-
What is Being Learned. Neural networks should
malism to find the summary that complements the in-
be thought of as a mapping to a manifold whose shape
formation from more than one pre-determined summary
is controlled by the network architecture. Here we
statistic,suchasangularpowerspectrumandpeakcount
restrict our manifold to be translationally-invariant
summaries. This has the potential to improve the cos-
(convolutional) and impose that it resides in the space
mology constraints from implicit likelihood analyses of
parameterised by weight-sharing multipole kernels.
weak lensing such as Fluri et al. (2022) and Jeffrey et al.
Furthermore, we optimise this lightweight architecture
(2024).
to find summaries that can only complement estimates
VIII. CODE AVAILABILITY
oftheangularpowerspectrum. Thiseffectivetruncation
of function space also contributes to the interpretability
of the network, as it guarantees that the function learnt The code for this analysis will be made available at
is not the power spectrum, or Gaussianly-compressed https://github.com/tlmakinen/hybridStatsWL. All
summaries of it. custom networks and simulation tools were written in
Jax (Bradbury et al. 2018) and flax (Heek et al. 2020)
and were run on a single NVIDIA v100 32Gb GPU. Pos-
Other studies have previously combined power spec-
terior density estimation was performed locally on a lap-
trum and network summaries in weak lensing analyses.
top CPU using the LtU-ILI code (Ho et al. 2024).
Jeffrey et al. (2024) for example feed in both sets of
independently-obtainedsummariesintoanNDEforpos-
terior estimation to obtain a ∼ 2× improvement in in-
IX. ACKNOWLEDGEMENTS
formation extraction in Ω − S . Here we show that
m 8
coordinatingthefield-levelnetworkoptimisationwithan
TLMacknowledgestheImperialCollegeLondonPres-
existing summary can give us even more efficient extrac-
ident’s Scholarship fund for support of this study, and
tion.
thanks Niall Jeffrey, Justin Alsing, David Spergel, and
Asymptotic Optimality. Unlike regression networks,
Maximilian von Wietersheim-Kramata for insightful dis-
which are trained over the entire prior distribution, our
cussions. NP is supported by the Beecroft Trust. BDW.
network learns the compression locally around a single
acknowledges support by the ANR BIG4 project, grant
point. This prevents the network from learning features
ANR-16-CE23-0002 of the French Agence Nationale de
of the convergence map that appear in unlikely points
la Recherche; and the Labex ILP (reference ANR-10-
in parameter space. Once the compression from data to
LABX-63) part of the Idex SUPER, and received fi-
network summaries is learned, simulations over a prior
nancial state aid managed by the Agence Nationale
p(θ,d) can be one-shot compressed before being fed to
de la Recherche, as part of the programme Investisse-
an NDE posterior.
ments d’avenir under the reference ANR-11-IDEX-0004-
02. TLM acknowledges helpful conversations facilitated
Thehybridsummaryformalismpresentedinthiswork by the Learning the Universe Collaboration. The Flat-
is not limited to weak lensing data, and it can be gener- iron Institute is supported by the Simons Foundation.
Alsing, J., Charnock, T., Feeney, S., & Wandelt, B. 2019, Boruah, S. S., & Rozo, E. 2023, Map-based cosmology in-
Monthly Notices of the Royal Astronomical Society, doi: 10. ference with weak lensing – information content and its de-
1093/mnras/stz1960 pendence on the parameter space. https://arxiv.org/abs/
Alsing,J.,Heavens,A.,&Jaffe,A.H.2016,MonthlyNotices 2307.00070
of the Royal Astronomical Society, 466, 3272, doi: 10.1093/ Bradbury,J.,Frostig,R.,Hawkins,P.,etal.2018,JAX:com-
mnras/stw3161 posabletransformationsofPython+NumPyprograms,0.3.13.
Alsing, J., & Wandelt, B. 2018, MNRAS, 476, L60, doi: 10. http://github.com/google/jax
1093/mnrasl/sly029 Carron, J., & Szapudi, I. 2013, Monthly Notices of the RAS,
Amon, A., Gruen, D., Troxel, M. A., et al. 2022, Phys. Rev. 434, 2961, doi: 10.1093/mnras/stt1215
D, 105, 023514, doi: 10.1103/PhysRevD.105.023514 Charnock, T., Lavaux, G., & Wandelt, B. D. 2018, Physical
Asgari, M., Lin, C.-A., Joachimi, B., et al. 2021, Astron- Review D, 97, doi: 10.1103/physrevd.97.083004
omy and Astrophysics, 645, A104, doi: 10.1051/0004-6361/ Charnock, T., Lavaux, G., Wandelt, B. D., et al. 2020,
202039070 Monthly Notices of the Royal Astronomical Society, 494,
Bishop, C. 1994 50–61, doi: 10.1093/mnras/staa68213
Cheng, S., Marques, G. A., Grando´n, D., et al. 2024, Cos- Kullback, S., & Leibler, R. A. 1951, The Annals of Mathe-
mological constraints from weak lensing scattering transform matical Statistics, 22, 79 , doi: 10.1214/aoms/1177729694
using HSC Y1 data. https://arxiv.org/abs/2404.16085 Lanzieri,D.,Lanusse,F.,Modi,C.,etal.2023,Astronomy&
Cheng, S., Ting, Y.-S., M´enard, B., & Bruna, J. 2020, Astrophysics, 679, A61, doi: 10.1051/0004-6361/202346888
Monthly Notices of the Royal Astronomical Society, 499, Lanzieri,D.,Zeghal,J.,Makinen,T.L.,etal.2024,Optimal
5902, doi: 10.1093/mnras/staa3165 NeuralSummarisationforFull-FieldWeakLensingCosmolog-
Cram´er,H.1946,Mathematicalmethodsofstatistics,byHar- ical Implicit Inference. https://arxiv.org/abs/2407.10877
ald Cramer, .. (The University Press) Leclercq,F.2015,Bayesianlarge-scalestructureinferenceand
Cranmer,K.,Brehmer,J.,&Louppe,G.2020,Proceedingsof cosmicwebanalysis,arXiv,doi:10.48550/ARXIV.1512.04985
theNationalAcademyofSciences,117,30055,doi:10.1073/ Lemos, P., Coogan, A., Hezaveh, Y., & Perreault-Levasseur,
pnas.1912789117 L.2023a,Sampling-BasedAccuracyTestingofPosteriorEsti-
Dai, B., & Seljak, U. 2024, Multiscale Flow for Robust and matorsforGeneralInference.https://arxiv.org/abs/2302.
Optimal Cosmological Analysis. https://arxiv.org/abs/ 03026
2306.04689 Lemos, P., Parker, L., Hahn, C., et al. 2023b, SimBIG:
Dalal, R., Li, X., Nicola, A., et al. 2023, Phys. Rev. D, 108, Field-level Simulation-Based Inference of Galaxy Clustering.
123519, doi: 10.1103/PhysRevD.108.123519 https://arxiv.org/abs/2310.15256
Ding,S.,Lavaux,G.,&Jasche,J.2024,PineTree: Agenera- Li, S.-S., Hoekstra, H., Kuijken, K., et al. 2023, Astron-
tive, fast, and differentiable halo model for wide-field galaxy omy and Astrophysics, 679, A133, doi: 10.1051/0004-6361/
surveys. https://arxiv.org/abs/2407.01391 202347236
Eisenstein,D.J.,&Hu,W.1999,TheAstrophysicalJournal, Li, Y., Lu, L., Modi, C., et al. 2022, pmwd: A Differen-
511, 5–15, doi: 10.1086/306640 tiable Cosmological Particle-Mesh N-body Library. https:
Euclid Collaboration. 2022, Astronomy and Astrophysics, //arxiv.org/abs/2211.09958
657, A91, doi: 10.1051/0004-6361/202141556 Livet, F., Charnock, T., Borgne, D. L., & de Lapparent, V.
Euclid Collaboration, Ajani, V., Baldi, M., et al. 2023, A & 2021, Catalog-free modeling of galaxy types in deep images:
A, 675, A120, doi: 10.1051/0004-6361/202346017 Massivedimensionalreductionwithneuralnetworks. https:
Fluri, J., Kacprzak, T., Lucchi, A., et al. 2019, Physical Re- //arxiv.org/abs/2102.01086
view D, 100, doi: 10.1103/physrevd.100.063514 Loureiro, A., Whiteaway, L., Sellentin, E., et al. 2023, The
Fluri,J.,Kacprzak,T.,Lucchi,A.,etal.2022,arXive-prints, OpenJournalofAstrophysics,6,doi:10.21105/astro.2210.
arXiv:2201.07771. https://arxiv.org/abs/2201.07771 13260
Fluri, J., Kacprzak, T., Refregier, A., et al. 2018, Physical Makinen, T. L., Charnock, T., Alsing, J., & Wandelt, B. D.
Review D, 98, doi: 10.1103/physrevd.98.123518 2021, Journal of Cosmology and Astroparticle Physics, 2021,
Heavens, A. F., Jimenez, R., & Lahav, O. 2000, Monthly 049, doi: 10.1088/1475-7516/2021/11/049
Notices of the Royal Astronomical Society, 317, 965–972, Makinen, T. L., Charnock, T., Lemos, P., et al. 2022, The
doi: 10.1046/j.1365-8711.2000.03692.x OpenJournalofAstrophysics,5,doi:10.21105/astro.2207.
Heek, J., Levskaya, A., Oliver, A., et al. 2020, Flax: A neu- 05202
ral network library and ecosystem for JAX, 0.5.2. http: Papamakarios, G., Pavlakou, T., & Murray, I. 2017, in
//github.com/google/flax Advances in Neural Information Processing Systems, ed.
Ho,M.,Bartlett,D.J.,Chartier,N.,etal.2024,LtU-ILI:An I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
All-in-One Framework for Implicit Inference in Astrophysics S. Vishwanathan, & R. Garnett, Vol. 30 (Curran Associates,
and Cosmology. https://arxiv.org/abs/2402.05137 Inc.).https://proceedings.neurips.cc/paper/2017/file/
Hoffmann, T., & Onnela, J.-P. 2023, Minimising the Ex- 6c1da886822c67822bcf3679d04369fa-Paper.pdf
pectedPosteriorEntropyYieldsOptimalSummaryStatistics. Peel, A., Lin, C.-A., Lanusse, F., et al. 2017, Astronomy &
https://arxiv.org/abs/2206.02340 Astrophysics, 599, A79, doi: 10.1051/0004-6361/201629928
Jasche, J., Leclercq, F., & Wandelt, B. 2015, Journal of Cos- Planck Collaboration, Aghanim, N., Akrami, Y., et al.
mology and Astroparticle Physics, 2015, 036, doi: 10.1088/ 2020, Astronomy and Astrophysics, 641, A6, doi: 10.1051/
1475-7516/2015/01/036 0004-6361/201833910
Jeffrey, N., Alsing, J., & Lanusse, F. 2020, Monthly Notices Porqueres, N., Heavens, A., Mortlock, D., & Lavaux, G.
of the Royal Astronomical Society, 501, 954, doi: 10.1093/ 2021a, Monthly Notices of the Royal Astronomical Society,
mnras/staa3594 502, 3035, doi: 10.1093/mnras/stab204
Jeffrey, N., Whiteway, L., Gatti, M., et al. 2024, Dark En- —. 2021b, Monthly Notices of the Royal Astronomical Soci-
ergy Survey Year 3 results: likelihood-free, simulation-based ety, 509, 3194–3202, doi: 10.1093/mnras/stab3234
wCDM inference with neural compression of weak-lensing Porqueres, N., Heavens, A., Mortlock, D., Lavaux, G., &
map statistics. https://arxiv.org/abs/2403.02314 Makinen, T. L. 2023, Field-level inference of cosmic shear
Joachimi, B., Taylor, A. N., & Kiessling, A. 2011, Monthly with intrinsic alignments and baryons. https://arxiv.org/
NoticesoftheRoyalAstronomicalSociety,418,145,doi:10. abs/2304.04785
1111/j.1365-2966.2011.19472.x Ramanah, D. K., Lavaux, G., Jasche, J., & Wandelt, B. D.
Kilbinger, M. 2015, Reports on Progress in Physics, 78, 2019, Astronomy and Astrophysics, 621, A69, doi: 10.1051/
086901, doi: 10.1088/0034-4885/78/8/086901 0004-6361/201834117
KodiRamanah,D.,Charnock,T.,Villaescusa-Navarro,F.,& Rao, C. R. 1945, Bulletin of the Calcutta Mathematical So-
Wandelt,B.D.2020,MonthlyNoticesoftheRoyalAstronom- ciety, 37, 81–89
ical Society, 495, 4227–4236, doi: 10.1093/mnras/staa1428 Ribli, D., Pataki, B. A., Zorrilla Matilla, J. M., et al. 2019,
Kratochvil, J. M., Haiman, Z., & May, M. 2010, Physical Monthly Notices of the Royal Astronomical Society, 490,
Review D, 81, doi: 10.1103/physrevd.81.043519 1843–1860, doi: 10.1093/mnras/stz2610Secco,L.F.,Samuroff,S.,Krause,E.,etal.2022,Phys.Rev. Appendix A: Posterior Coverage Tests
D, 105, 023515, doi: 10.1103/PhysRevD.105.023515
Sellentin, E., & Heavens, A. F. 2017, Monthly Notices of the
One of the distinct advantages of SBI neural density
Royal Astronomical Society, 473, 2355, doi: 10.1093/mnras/
estimation is the immediate availability of coverage
stx2491
tests. In this work we trained an estimator for the
Sellentin, E., Loureiro, A., Whiteway, L., et al. 2023, The
posterior distribution given some point-estimates for
Open Journal of Astrophysics, 6, 31, doi: 10.21105/astro.
2305.16134 the parameters via MOPED or the hybrid-summary
Seo, H.-J., Sato, M., Dodelson, S., Jain, B., & Takada, M. network: p(θ|θˆ). This density estimator is an amortised
2010, The Astrophysical Journal Letters, 729, L11, doi: 10. posterior, meaning the posterior density for any given
1088/2041-8205/729/1/L11 summaries θˆis immediately available without having to
Sharma,D.,Dai,B.,&Seljak,U.2024,Acomparativestudy
do MCMC sampling with a likelihood. We can then do
of cosmological constraints from weak lensing using Convo-
repeated mock data parameter inference over the prior
lutional Neural Networks. https://arxiv.org/abs/2403.
using this posterior density, and calculate how many
03490
true parameter vlaues from the credible intervals match
Simpson, F., Harnois-D´eraps, J., Heymans, C., et al. 2015,
the expected fraction, forming a posterior “coverage”
Monthly Notices of the Royal Astronomical Society, 456,
278–285, doi: 10.1093/mnras/stv2474 test. We display one such test in Figure 9 making use
Spergel, D., Gehrels, N., Baltay, C., et al. 2015, arXiv e- of the TARP coverage test framework presented in Lemos
prints, arXiv:1503.03757, doi: 10.48550/arXiv.1503.03757 et al. (2023a).
Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. 2016,
Inception-v4, Inception-ResNet and the Impact of Residual
Connections on Learning. https://arxiv.org/abs/1602. 1.00
07261 TARP
Tsaprazi, E., Jasche, J., Lavaux, G., & Leclercq, F. 2023,
arXive-prints,arXiv:2301.03581,doi:10.48550/arXiv.2301.
0.75
03581
von Wietersheim-Kramsta, M., Lin, K., Tessore, N., et al.
2024, KiDS-SBI: Simulation-Based Inference Analysis of
KiDS-1000 Cosmic Shear. https://arxiv.org/abs/2404. 0.50
15402
Xu, B., Wang, N., Chen, T., & Li, M. 2015, Empirical Eval-
uation of Rectified Activations in Convolutional Network.
https://arxiv.org/abs/1505.00853 0.25
Zu¨rcher, D., Fluri, J., Sgier, R., et al. 2022, Monthly Notices
of the Royal Astronomical Society, 511, 2075, doi: 10.1093/
mnras/stac078
0.00
0.0 0.5 1.0
Credibility Level
FIG. 9. Example coverage test result for low-noise inference
with N = 192 resolution (rightmost panel in Fig. 6) using
TARP (Lemos et al. 2023a). Using our amortised parameter-
summaryposteriorp(θ,θˆ),wecandorepeatedmockdatapa-
rameter inference over the prior, and measure which fraction
oftruevaluesfromtheappropriatecredibleintervalsmatches
the expected fraction. The blue line traces 100 “distances to
random points” (DRP), which is accelerated using the TARP
framework within LtU-ILI (Ho et al. 2024). The DRP line
(blue) traces the truth line (dashed), indicating a successful
test.
Appendix B: Learned Covariance Matrix
Visualisation
In Figure 10 we illustrate the full joint covariance,
(cid:18) (cid:19)
C u
C= t , (B1)
uT C
x
egarevoC
detcepxE14
×10−19 ×10−12
10 0.008
0.25
0.00 0.007
5
−0.25 0.006
−0.50
0.005
0
−0.75
0.004
−1.00
−5
0.003
−1.25
0.002
FIG. 10. Example joint C -network summary covariance visualisation (Eq. B1) for a network optimised against the binned
ℓ
power spectrum. We separate the 60×60 C covariance structure (upper left corner) from network summaries (lower right
ℓ
corner) with the set of intersecting white lines. The learned network summaries exhibit a non-zero correlation structure with
the ℓ bins, illustrated by the u off-diagonal matrix vectors on the bottom and right-hand edges. Here it is obvious that only
one of the two network summaries is highly correlated with the binned power spectrum modes.
of the binned tomographic C statistic and two learned work compressions in a low-noise setting. The informa-
ℓ
network summaries, clearly separated by the white in- tion update formalism tells the convolutional network
tersecting lines. We plot each component of this struc- duringoptimisationtomakeuseofthenonlinearinforma-
ture with a separate colourbar. The cross-correlation tion on the smaller (pixel-level) scales that it has access
row-matrices u indicate that the learned summaries ex- to in a way that is complementary to the power spec-
hibit non-trivial correlation structure with the binned ℓ- trum. Although optimised at a fiducial point, the map-
modes,whichcontributestoinformationcaptureaccord- ping learned is smooth as a function of data d(θ) away
ing to the hybrid statistics formalism. from the training point, resulting in more structure in
thefour-dimensionaljointdistributionspacep(θ,θˆ)than
MOPED, allowing the summaries (z-axis and colour) to
Appendix C: Summary Scatter respond more smoothly and rapidly as a function of pa-
rameters (x,y) = (Ω ,S ). This smoother joint distri-
m 8
Here we display a three-dimensional view of the four- butionsurfacecanthenbeharnessedbytheNDEscheme
dimensional joint distribution of compressed summaries to produce tighter posteriors in an amortised fashion.
and parameters p(θ,θˆ) obtained from MOPED and net-
C t u C x15
MOPED(C )
ℓ
1.0
0.5
1
1
0
0.0
0.2 0
0.6
0.4 0.4 0.5
Ω Ω 1.0
m 0.6 1.0 1.5 m 0.2 1.5 S 8 −0.5
0.5 S
8
2
network(field | C )
ℓ
1
0
0
−1
−1
−2
0.2
0.6 0
0.4 −2 0.4 0.5
Ω Ω 1.0
m 0.6 1.0 1.5 m 0.2 1.5 S 8
0.5 S
8
FIG. 11. Computing additional information from the data endows the network summaries (bottom row) with more structure
as a function of parameters (Ω ,S ) over a prior than MOPED two-point summaries (top row). Summaries for each method
m 8
Ωˆ and Sˆ are indicated by z-direction and colourbar, respectively. It is highly visible via the increased structure in joint
m 8
space that the network is able to capture information from the smaller scales it has access to. More scatter in z or colour at
a particular parameter value in the MOPED summaries indicates a less informative compression of the simulated convergence
data to from power spectrum summaries.