Granularity is crucial when applying differential privacy to text: An
investigation for neural machine translation
DoanNamLongVu1 and TimourIgamberdiev1 and IvanHabernal2
TrustworthyHumanLanguageTechnologies
1 DepartmentofComputerScience,TechnicalUniversityofDarmstadt
2 ResearchCenterTrustworthyDataScienceandSecurityoftheUniversityAllianceRuhr,
FacultyofComputerScience,RuhrUniversityBochum
timour.igamberdiev@tu-darmstadt.de
www.trusthlt.org
Abstract {
...
”de”: ”Kunde: ImmoHande-Hornig ”,
Applying differential privacy (DP) by means
”en”: ”Customer: Immo Hande−Hornig”,
oftheDP-SGDalgorithmtoprotectindividual ...
datapointsduringtrainingisbecomingincreas- ”de”: Agent: ... Ich bin ImmoHande-Hornig .
”en”: Agent: ... you are through to Immo Hande−Hornig.
inglypopularinNLP.However,thechoiceof
...
granularityatwhichDPisappliedisoftenne- }
glected. Forexample,neuralmachinetransla-
Figure1: Examplesofsentencesthatarenotindepen-
tion(NMT)typicallyoperatesonthesentence-
dentwithinadocument. Theindependenceisviolated
levelgranularity. FromtheperspectiveofDP,
viathe“ImmoHande-Hornig”sequence,breakingthe
thissetupassumesthateachsentencebelongs
DP guarantee of protecting each sentence during the
toasinglepersonandanytwosentencesinthe
trainingprocess.
trainingdatasetareindependent. Thisassump-
tion is however violated in many real-world
NMTdatasets,e.g. thoseincludingdialogues.
Oneparticulartaskthathasrecentlyraisedmany
ForproperapplicationofDPwethusmustshift
fromsentencestoentiredocuments. Inthispa- privacy concerns is neural machine translation
per,weinvestigateNMTatboththesentence (NMT).ApplyingDPatthesentencelevelforNMT
anddocumentlevels,analyzingtheprivacy/util- may break the independence assumption if more
itytrade-offforbothscenarios,andevaluating thanonesentenceisassociatedwithasingleindi-
therisksofnotusing theappropriateprivacy
vidual(Brownetal.,2022),asdepictedinFigure1.
granularityintermsofleakingpersonallyiden-
In such cases, scaling up the unit of privacy to
tifiableinformation(PII).Ourfindingsindicate
thedocumentlevelbygroupingrelatedsentences
thatthedocument-levelNMTsystemismore
resistanttomembershipinferenceattacks,em- overcomes the violated privacy protection which
phasizingthesignificanceofusingtheappro- ‘pretends’ all sentences are independent, i.e. the
priategranularitywhenworkingwithDP.1 statusquo.
Themainobjectiveofthispaperistocompare
1 Introduction
the use of DP for NMT systems and datasets at
Withincreasingconcernsabouttheprivacyofindi- thesentenceanddocumentlevels,focusingonthe
vidualsanddataleakagefromNLPsystems(Car- levelofprivacyprotectionoffered. First,weinves-
lini et al., 2021), a method that has gained pop- tigatethetrade-offbetweenprivacyandutilityfor
ularity in privacy-preserving NLP is Differential different levels of granularity (sentence vs. docu-
Privacy (DP) (Klymenko et al., 2022). However, ment)duringthetrainingprocessofanNMTsys-
theexactmannerinwhichDPisappliedtoatextual tem. Specifically, we examine how performance
datasethasnumerouspitfalls. Theunitofprivacy isaffectedwhenapplyingDPwithvaryinglevels
isoneamongthem,i.e.thegranularityatwhichwe of privacy guarantees and granularity. Secondly,
assumeanindividual‘datapoint’(e.g.sentences, weaimtoevaluatetherisksofnotusingaproper
documents,andsoforth)(Ponomarevaetal.,2022; privacygranularityduringthetrainingprocessof
IgamberdievandHabernal,2023),withanassump- anNMTsystemthroughdataextractionattackson
tion of independence among data points (Dwork thesesystems.
andRoth,2013). Our contributions are as follows. (1) We pro-
pose a novel approach to apply differential pri-
1Our code is available at https://github.com/
trusthlt/granularity-is-crucial-dp. vacy to NMT systems at the document level, uti-
4202
luJ
62
]LC.sc[
1v98781.7042:viXralizingtheDP-NMTframework(Igamberdievand exhibitadifferentbehaviorontrainingdatacom-
Habernal,2023)andthemLongT5model(Uthus paredtotestdata, withmodelparametersstoring
etal.,2023). Theevaluationresultsshowthatthe informationaboutspecifictrainingdataunit. Mem-
document-levelNMTsystemisextremelysensitive bership inference attacks (MIAs) aim to predict
totheprivacybudget(ε), whichcansignificantly whetherspecificexamplesaremembersofthetrain-
affectperformanceandleadtoadropinutility. We ing dataset (Hu et al., 2022). In general, an MIA
thereforesuggesttrainingthedocument-levelNMT isactuallyabinaryclassifier,whichisdesignedto
systemonalarger, non-sensitivedataset, suchas distinguishatargetmodel’sbehaviorofitstraining
WMT22 (Kocmi et al., 2022), to achieve a better membersfromthenon-members.
trade-offbetweenprivacyandutilityonthedown- The first MIA was proposed by Shokri et al.
stream dataset with DP. (2) We apply the loss- (2017), utilizing shadow datasets that have simi-
basedmembershipinferenceattack(MIA)(Yeom lardistributiontothe originaltraining data. Mul-
etal.,2018)todetectprivateinformationinNMT tipleshadowmodelsaretrainedonthesedatasets,
systems at both the sentence and document lev- whicharemeanttomimicthebehaviorofthetarget
els. Based on this MIA, we create an evaluation model. Theoutputpredictionsofthesemodelsare
schemaforpersonallyidentifiableinformation(PII) then used as input to a final binary classification
toestimatethepercentageofpotentialinformation model. This model detects whether a given data
leakage. Ourresultsshowthatthedocument-level pointbelongstothetargetmodelornot.
NMTsystemismorerobustagainsttheloss-based
MIAthanthesentence-levelNMTsystem,demon- 3 RelatedWork
stratingtheimportanceofusingthepropergranu-
3.1 PreviousworkonDP+NLP
laritywhenworkingwithDP.
Severalworksattemptedtopre-trainlanguagemod-
2 Background elswithDP-SGD (Aniletal.,2022;YinandHaber-
nal, 2022; Ponomareva et al., 2022). With a sig-
2.1 DifferentialPrivacy
nificantcomputationalburdenofpre-trainingwith
WerefertoAppendixAforadetailedexplanation DP-SGD,finetuningpre-trainedlanguagemodels
ofDifferentialPrivacyandDP-SGD. usingDP-SGDhasseenincreasedresearchoverthe
A key aspect of applying DP to text is that pastfewyears(Sengeetal.,2022;Lietal.,2022).
we cannot simply utilize group privacy (Dwork Themainobjectiveistoutilizeapre-trainedcheck-
andRoth,2013)toachievedocument-levelprivacy pointofamodelthatwascreatedbyusingapub-
fromsentence-levelprivacy. SinceDP-SGDlever- liclyavailablecorpusofdata,suchasWikipediaor
ages the Approximate DP definition (Dwork and C4(Raffeletal.,2020)andthenfine-tuneitonapri-
Roth, 2013), the δ value, i.e. the probability of vatedownstreamdatasetwithDP-SGD.Although
privacy leakage occurring, is not 0 when apply- fine-tuning is more efficient than pre-training, it
inggroupprivacy. Ifthenumberofdatapointsk stillrequiresalargeamountofdataontextgener-
in which two neighboring datasets differ is large ationtaskstoachievegoodperformance,e.g.lan-
enough,δwillexceed1duetoscalingwithafactor guage modeling (Li et al., 2022) or NMT (Igam-
ofke(k−1)ε. Moreover,usingrelaxedDPdefinition berdievetal.,2024). Theaforementionedrelated
suchasRenyiDP(Mironov,2017)toavoidinclud- works only concentrate on the protection of the
ing the δ value will scale ε to a very large value gradientatthesentencelevel. Incontrast,thecur-
thatispracticallyunmanageable,alsoresultingin rentworkisconcernedwiththeprotectionofthe
averyweakprivacyguarantee. gradientatthedocumentlevel.
2.2 Overviewofmembershipinference 3.2 Previousworkondocument-levelNMT
attacks(MIA)
Sentence-levelNMTisthemostcommonmethod
Since datasets used to train neural models often ofmachinetranslationbecauseitissimplertotrain
containconfidentialuserinformation,theymaybe andevaluatewithexistinglargedatasetsandevalu-
vulnerabletoprivacyrisks(Huetal.,2022). The ationmetrics(PostandJunczys-Dowmunt,2023).
trainedmodelsareoftenover-parameterized,mean- TheprimarylimitationisduetotheNMTmodel’s
ing they can memorize information about their memoryconsumptionforsequencelength,result-
trainingdataset(Mireshghallahetal.,2022). They ing in a larger memory footprint when increased.Secondly,populardatasets,suchasWMT(Fernan- et al., 2020) or mT5 (Xue et al., 2021) out of the
desetal.,2021),existonlyforsentence-levelma- box. However,thesemultilingualseq2seqmodels
chinetranslation,eventhoughtheywereoriginally arenotsuitableforourtask,astheyarepre-trained
createdasdocuments. Nonetheless,thisapproach onshortersentences.
hasitslimitations,asdemonstratedinthecurrent
mLongT5 model mLongT5 is a multilingual
study,whichistherelatedprivacyissuewithDP-
seq2seqmodelbasedonLongT5(Guoetal.,2022),
SGDatthesentencelevel. Moreover,fromtheper-
whichisaseq2seqmodelthatusesT5(Raffeletal.,
spective of machine translation, a sentence-level
2020) as its foundation, with a Transient Global
MT model is not suitable for translating lengthy
(TGlobal) Attention mechanism. This attention
documents without taking their context into ac-
mechanism is well-suited for long text tasks in
count (Wicks and Post, 2023; Wu et al., 2023).
termsofmemoryefficiency,andmLongT5lever-
Recentrelatedworksondocument-levelmachine
agesthemC4dataset(Xueetal.,2021)with4096
translation can be separated into two categories:
token long input sequences for pre-training. As
Encoder-DecoderModelsandDecoderOnlyMod-
mLongT5 checkpoints are designed for long text
els.
tasksandavailableforFlaxandJAX,weincorpo-
Encoder-Decoder Models Most of the re- ratemLongT5intotheDP-NMTframework.
cent works focus on the standard Transformer
4.2 Loss-basedMIA
model(Wuetal.,2023;Zhuochengetal.,2023a).
Previous work attacking an NMT system used
Typically, theyconcatenatemultiplesentencesto
the entire WMT dataset to create a sophisticated
form a document with the length up to 512 or
shadowMIAattack(Hisamotoetal.,2020). How-
maximum1024tokensfortraining. However,the
ever, in our work, the datasets on which we con-
naiveapproachgenerallysuffersfromalengthbias
duct experiments (see Section 5) have less than
problem,whichcausessignificantdegradationin
20,000 data points, which is significantly fewer
translationqualitywhendecodingdocumentsthat
than the millions of data points in WMT. This
aremuchshorterorlongerthanthemaximumse-
makesshadowMIAlesseffective. Also,interms
quence length during training (Zhuocheng et al.,
ofcomputationalcomplexityfortheshadowMIA,
2023a).
anattackermusttrainhundredsofshadowmodels
Decoder Only Models Unlike the vast major- toachievegoodperformance(Yeometal.,2018).
ity of the training/fine-tuning paradigm, recent Loss-based metrics (Yeom et al., 2018) are less
works (Hendy et al., 2023; Karpinska and Iyyer, computationallyintensivetoperform. Intuitively,
2023)suggestthatGenerativePre-Training(GPT) ifthelossofadatapointissmallerthanthetarget
models (Radfordetal.,2018)areabletoachieve model’sexpectedtrainingloss,therecordisclas-
verycompetitivetranslationqualityondocument- sified as a member; otherwise as a non-member.
level translation. As such, they use a few-shot Thetargetmodelistrainedbyminimizingthepre-
promptingtechniquetotranslateadocument,em- diction loss of its training members. Therefore,
ployingChatGPT2.Thepromptdisplaysexamples the prediction loss of a training record should be
ofeachtranslatedsentencepairfirstandinstructs smallerthanthatofatestrecord. TheattackExpM
loss
themodeltoconsiderthecontextwhentranslating, isdefinedasfollows:
asinadocument.
ExpM = 1(ℓ(θ(r s);r) τ), (1)
loss
4 Methods | ≤
where θ is the model, r is target output, s is the
4.1 Document-levelmachinetranslation input,ℓisthelossfunction(typicallycross-entropy
loss), τ is a threshold (average training loss) and
WeemploytheDP-NMTframeworkdevelopedby
1isaclassifierfunction,whichtakeseventAand
Igamberdiev et al. (2024) for privacy-preserving
returns1iftheeventAoccurs,0otherwise.
NMTwithDP-SGD. Theframeworkisbuiltontop
Loss-based MIA highlights that overfitting of
ofFlax(Heeketal.,2023)andJAX(Bradburyetal.,
targetMLmodelsistheprimaryfactorcontribut-
2018)forrapidDP-SGDtraining. Bydefault,the
ing to the success of MIAs. This attack strongly
frameworksupportsmodelssuchasmBART(Liu
exploitsthedifferentbehaviorsoftargetMLmod-
2https://openai.com/blog/chatgpt elsontheirtrainingversustestdata. Theattackerisassumedtohaveknowledgeofthedatapoints,but Dataset Level #Train #Val. #Test
itisuncertainwhethertheywereusedintraining.
Sentence 20,000 2,120 2,051
Infact,weareassumingaverypowerfuladversary BSD
Document 670 69 69
that already has knowledge of the data, which is
Sentence 13,380 2,488 2,109
whythisisastrongwhite-boxattackforinvestigat- MAIA
Document 355 71 70
ing data leakage. The original evaluation metric
schemefortheloss-basedMIAistheattackadvan-
Table1: Numberoftrainingexamplesforbothdatasets
tageorprivacyleakageresultingfromdifferences
insentence-levelanddocument-level
betweenthefalsepositiverate(FPR)andtrueposi-
tiverate(TPR)oftheattack: AdvM = TPR FPR
−
MAIA The Multilingual Artificial Intelligence
4.3 PIIexposure Agent Assistant (MAIA) corpus consists of gen-
uinebilingual(German-English)customersupport
Consideringprivateinformationasnamedentities
conversationsfromtheUnbabeldatabase(Farinha
that a model might overfit to during the training
etal.,2022). Tomaketheconversationspublicly
process,wepresentaevaluationschemetotestthe
available,thedatawasfirstanonymizedusingthe
effectivenessoftheMIAwithrespecttoPII.The
Unbabelproprietaryanonymizationtoolandthen
method works as follows: We carry out the loss-
manuallyverified.
based MIA, obtaining extracted training records
fromEqn.1. Wethenselectthetruepositivepre-
5.2 Datapreparation
dictions and calculate the number of PII that are
Sincebothdatasetsaredialoguedatasets,weneed
present. Forsentence-levelprivacy,duetocorrela-
to concatenate the utterances within a dialogue
tionofsentenceswithinadocument,morePIIleak-
intoasingledocumentfordocument-levelmachine
agewouldbeexpectedthanwithdocument-level
translation. First, we concatenate the speaker’s
privacy. Thisaimstoseehowwellthemodelopti-
name and the utterance into a single sentence.
mizesagainstentities. Weusethedefaultpipeline
setting of Presidio3 which consists of RegEx, Namely,<SPEAKER>: <UTTERANCE>. Then
weconcatenatealltheutteranceswithinadialogue
Spacy NER and BERT contextual awareness to
extractasetofPIIfromgivensentences.4 into a single document. This process results in
a smaller number of training examples than the
5 Experiments originalsentence-leveltrainingexamples. Table1
shows the number of training examples for both
5.1 Datasets datasetsatthesentencelevelanddocumentlevel
SeeFigures7and8inAppendixBforexamples
We aim to find a suitable dataset which mimics
ofbothdatasetpreparation.
the real private environment of processing sensi-
tiveinformation,butispubliclyavailable,bothfor
5.3 Experimentalsetup
reproducibilityandethicalreasons. Inaddition,a
datasetmustbeappropriateforbothsentence-level ThetrainingexperimentdirectlyonHuggingface’s
and document-level machine translation. There- mLongT5 checkpoint5 is denoted as θsen at the
fore,weselecttwodatasetsforourinvestigations, sentence level, and as θdoc at the document level.
BSDandMAIA,describedbelow. We denote training data at the sentence level
as sen and at the document level as doc ,
Dtrain Dtrain
BSD The Business Scene Dialogue corpus similarly for validation ( sen, doc) and test
Dval Dval
(BSD) (Rikters et al., 2019) is a collection of fic- data ( sen, doc). We refer to Table 3 in Ap-
test test
D D
tionalbusinessconversationsinvariousscenarios, pendixforthenotationanditsdescriptionusedin
with parallel data for Japanese and English. For thiswork.
ourexperiments,wecombinedtheoriginalcorpus,
Additional pre-training with WMT22 The
which consists of two translation direction into a
number of document-level training examples is
singleJapanese English(JA-EN)languagepair.
→ much smaller than at the sentence level (See Ta-
3https://microsoft.github.io/presidio/ ble1). Thus,themodelmayunderfitthetraining
4WenotethatPresidioisnotasthoroughashumanan-
notators,andmaymisssomePIIdataorreturnfalsepositives. 5https://huggingface.co/agemagician/
However,itisstillagoodapproximationofthenumberofPII. mlong-t5-tglobal-basedata during private training, resulting in poorer this, the DP-SGD hyperparameters, such as the
performance than normal training at the docu- gradient clip value C, are identical for both set-
ment level and private training at the sentence tings. Furthermore, we use more epochs to train
level. To improve the document-level model per- document-levelmodelinprivatesettingtoobtain
formance during private training, we fine-tuned decentperformance(seeAppendixC).
mLongT5 checkpoint without DP-SGD on the
5.5 Evaluation
WMT22 (Kocmi et al., 2022) dataset first before
fine-tuningonBSDorMAIAatthedocumentlevel. Performance WereportBLEU(Papinenietal.,
Afterfine-tuningonthedocument-levelWMT22 2002) for n-gram matching evaluation and
dataset, we fine-tune the model on the BSD and BERTScore(Zhangetal.,2020)forsemanticsimi-
MAIAdatasetsatthedocumentlevelforbothnor- larityevaluation. WerefertoAppendixDforthe
mal and private training. The model training ex- detailsofourmodificationtoBERTScoreforeval-
periments based on the document-level WMT22 uationonlongtexts.
augdoc
dataset are denoted as θ and with down-
zero−shot MIA It is difficult to know whether a sentence
streamdataasθaugdoc.
belongs to the training data of a document-level
modelorasentence-levelmodel. Foreachdataset
5.4 Hyperparameters
at sentence level, we consider the validation set
Theprimarydistinctionbetweentwolevelmodels and the test set as non-members and the training
in terms of hyperparameters is the maximum se- setasmembers. However,thetrainingsetishuge
quencelength. Forsentence-leveltraining,itisset
comparedtothevalidationsetandthetestset;itis
to64-128,whereasfordocument-leveltraining,it
recommendedtobalancethedatasetforMIAeval-
issetto1200-1500. Werefertothedetailsofour
uationtoavoidthebiasoftheattacker(Jayaraman
hyperparameterssearchinAppendixC.
andEvans,2019). Wesamplethetotalnumberof
ForadditionaltrainingwithWMT22,weusethe members from the training set to be equal to the
samehyperparametersasdocument-levelsettings. totalnumberofexamplesinthevalidationsetand
Topreparethedocument-levelWMT22dataset,we the test set for our experiments, similar to Yeom
concatenate the multiple sentences into a single et al. (2018); Jayaraman and Evans (2019). For-
document, as long as they reach 1200 tokens for mally, let α be the total number of sentences in
JapanesetoEnglishand1600tokensforGerman the validation set sen and the test set sen. By
toEnglish. Thosedocumentsarealignedwiththe leveragingSamplinD gv Wal ithoutReplacemenD tt te ost avoid
originalsentence-leveltrainingexamples. Were- duplicating instances, we have a sampled set of
fertoTable7inAppendixC.3forthenumberof sentences sen fromasentence-leveltraining
trainingexamplesinourexperimentwithWMT22. set sen :D (s sam ,p rle )d ,...,(s ,r ) sen , where
Dtrain i i α α ∼ Dtrain
s is the source and r is the corresponding tar-
i i
Privacy Hyperparameters We compare ε val- sampled
get sentence for i 1,...,α and =
ues6of ,990,90,10and1fortrainingonMAIA, ∈ { } |Dval |
∞ sen + sen (See Table 6a in Appendix E for
then ,400,40,10 and 1 for training on BSD. |Dval| |Dtest |
∞ theexactnumber).
Thosevaluesareappliedtobothsentence-leveland
document-leveltraining. Werefertothedetailsof PII InNMT,thecross-lingualPIIdetectionmight
ourprivacyguaranteeinAppendixH. be not comparable between languages of input s
Given that sentences must be concatenated to andtargetr. Hence,weonlyreportthePIIleakage
formadocument(seeTable1),thedocument-level estimationonthetargetlanguagebasedontheref-
datasets are necessarily smaller than the original erencer(SeeTable6binAppendixforthenumber
ones. Consequently, the σ noise introduced dur- of PII in sen of each dataset). We use the
Dsampled
ingdocument-leveltrainingwiththeDP-SGDal- percentageofPIIleakageestimationasthemetric,
gorithm is increased and a higher sampling rate sinceallPIIsareextractedfromthesampledtrain-
is employed, in order to match the exact same ε ingdata. Namely,thisistheratioofthenumberof
value among the two configurations. Apart from detectedPIIdatatothetotalnumberofPIIdatain
thesampledtrainingdata:
6Themaximumnumberofutterancesinadialoguewithin
eachdataset(99forMAIAand40forBSD)ismultipliedby PIIofTPr sen
theεvaluesof1and10. Finally,theresultingvaluesareε PII (r) = ∈ Dsampled (2)
equals990,90forMAIA,and400,40forBSD. %leakage TotalPIIofr sen
∈ Dsampled40
75
θdoc θaugdoc θaugdoc
zero-shot
50
20
25
0 0
1 10 40 400 1 10 99 990
∞ ∞
ε ε
(a)BSD (b)MAIA
Figure2:BLEUscoreson doc forthethreedocument-levelmodelfine-tuningconfigurations.Lowerεcorresponds
Dtest
tobetterprivacy.
30
θsen θaugdoc 60
20 θdoc θ za eu rg od -o shc ot
40
10
20
0 0
1 10 40 400 1 10 99 990
∞ ∞
ε ε
(a)BSD (b)MAIA
Figure3:BLEUscoreson sen forallfourmodelfine-tuningconfigurations.Lowerεcorrespondstobetterprivacy.
Dtest
Wesuspectaprivatetrainingmodeltohavealower Evaluation of sen Figure 3 shows the BLEU
test
D
PIIleakagepercentagethan50%. score of the three approaches on sen. On
test
D
MAIA, the BLEU score of θsen is superior
6 Results to θdoc and θaugdoc at any chosen value of ε, ex-
cept for ε = 1. Even at ε = 10, the BLEU score
6.1 Privacy/utilitytrade-off
of θsen is very high at 35 BLEU score, while the
WepresenttheBLEUscoresbelow,forBERTScore BLEUscoreofalldocument-levelmodelsarelow.
werefertoAppendixF.2. We refer to Table 8 in Appendix G for specific
translationexamples.
Evaluationon doc Figure2showstheBLEU
Dtest On BSD, the performance gap be-
score of the two approaches on doc. As ex-
Dtest tween θsen, θdoc and θaugdoc is less signif-
pected, we can observe the deterioration of the
icant, possibly due to the distantly related
BLEU score as the value of ε decreases on both
language pair. The difference in BLEU score
datasets. The additional training data from
between θsen and θaugdoc is about 10 for ε > 10.
WMT22 is beneficial for the translation qual-
Pre-training on WMT22 is beneficial for the
ity on both datasets. Without pre-training on
translation quality at the sentence level on the
WMT22, the BLEU score of θdoc is significantly
augdoc
BSDdataset,sincetheBLEUscoreofθ is
lower than θaugdoc, with in a significant drop in zero−shot
alreadyhigherthanθdoc atε = .
translation quality. Moreover, the BLEU score ∞
augdoc
of θ , which is fine-tuned on WMT22, is
zero−shot 6.2 Privacyriskevaluation
even higher than the fine-tuned θdoc at ε =
∞
on the BSD dataset. The results of θaugdoc are As in our assumption, we consider the adver-
consistentlybetterthanθdoc acrossallvaluesofε. sary knows the average loss of the model on the
Overall, these results indicate that privately fine- training data, more particularly, the average loss
tuning on the target task is slightly beneficial for of θsen on sen at ε = before performing
Dtrain ∞
thetranslationquality,withrespecttothedomain anyattackevaluation. Figure4showstheprivacy
translationqualityatthedocumentlevel. leakageforbothMAIAandBSDusingloss-based
UELB
UELB
UELB
UELBoptimizinglongersequencesplusthenoiseadded
θsen θaugdoc
0.2 θdoc θaugdoc tothegradientsaccordingtotheprivacybudget.
zero-shot
BSD OnBSD,theattackerhasahigheradvan-
tageatanylevelofε. Thisimpliesthatthetraining
0.0
datahasdistinctcharacteristicsthatmakeiteasier
1 10 40 400
∞
for the adversary to infer the membership of the
(a)BSD
trainingdatavs.testdatacomparedtoMAIA. At
ε = ,theprivacyleakageofθaugdoc isonly0.03
0.2 point∞ sbehindθsen. Theprivacyleakageonθdoc is
also lower than θsen; however, θaugdoc still has
a small leakage at ε = 400 and the leakage
0.0
ofθaugdoc convergestonearzeroatε = 10.
1 10 99 990
∞
ε
6.3 PIIdisclosure
(b)MAIA
Despite DP mitigating the privacy leakage to a
Figure4: Privacyleakageusingtheloss-basedMIAon largeextent,thetruepositivepredictionfromMIA
sen , senand sen ,forallfourmodelfine-tuning
Dsampled Dval Dtest afterapplyingDPstillplaysasignificantroleinthe
configurations.
privacy risk evaluation. This helps us determine
theextenttowhichthemodelcouldpotentially
1.0
revealPII(issueofoverfitting).
θsen θaugdoc
θdoc θaugdoc MAIA The PII leakage percentage
zero-shot
0.5 of θdoc and θaugdoc is 0 after applying dif-
ferential privacy. At ε = , the PII leakage
∞
percentage of those approaches is below 0.25,
0.0 while θsen is approximately 0.80 at ε = and
1 10 40 400 ∞
∞ downto0.40atε = 10.
(a)BSD
1.0 BSD OnBSD,surprisingly,thePIIleakageper-
centage of θaugdoc at ε = is slightly higher
∞
thanθsen. Botharearound0.75,whileθdoc isap-
0.5 proximately0.4. Themostinterestingobservation
is that the PII leakage percentage of θaugdoc de-
teriorates faster as ε increases compared to θsen,
0.0 thoughitishigheratε = . Thisisasignificant
1 10 99 990
ε ∞ signthattheθdoc andθaug∞ doc ismoreeffectivein
(b)MAIA
reducingprivacyrisksthanθsen.
Figure5: PIIleakagepercentageon sen . 7 Discussion
Dsampled
7.1 Trade-offforprivacygranularity
MIA. Overall on doc, the BLUE scores
test
D
of θaugdoc greatly vary across different val-
MAIA On MAIA, the privacy leakage via ues of ε in the private training setup on both
loss-based MIA on document-level training is datasets. Thisresultsinhighermeanandstandard
lowerthansentence-leveltraining. Empirically, deviationofBLEUscoresforθaugdoc atthelower
the leakage on θdoc and θaugdoc is 50% lower valuesofε,comparedtohighervaluesofε. This
thanθsen atε = . Furthermore,thevaluecontin- may be due to the private training process being
∞
uestodecreaseandeventuallydropsbelowzeroas unstable when optimizing the objective function
thevariableεdecreasesasFPRishigherthanTPR, withrespecttolongsequences, whichisthecase
resultsinineffectiveMIA.Thesameappliestothe ofθdoc innormaltraining.
othermodels,θdoc,θaugdoc,weobservenoprivacy For the results on sen, the translation qual-
test
D
leakageafterapplyingDPduetothedifficultyin ity of θsen is superior to θdoc and θaugdoc at any
egakaeLycavirP
egakaeLycavirP
egatnecrepegakaelIIP
egatnecrepegakaelIIPExample Epsilon θsen θdoc θaugdoc
✓ ✓ ✓
∞ 990 ✓
Agent:GoodMorning Dipl.-Ing.BastianHeuser 10 ✓ × ×
× ×
✓
∞ 990 ✓ × ×
× ×
Agent:Soyouwouldliketocancelthereorderofthelamp? 10
× × ×
✓
∞ 990 ✓ × ×
Customer:Thestandinglamporthehanginglamp? 10 ✓ × ×
× ×
✓ ✓ ✓
∞ 990 ✓
Agent:-Goto http://www.suessebier.de/ 10 ✓ × ×
× ×
✓ ✓ ✓
∞ 990 ✓
× ×
Agent:Doyouhavetheordernumberthatstarts 160.....? 10
× × ×
✓ ✓ ✓
∞ 990 ✓
× ×
Agent:Thankyou-sothe Ba¨rerGmbH (140x200cm), Samt in Nachtgrau? 10
× × ×
Table2: ExamplesofleakagefromMAIA sen usingtheMIAandPIIevaluationonsentence-levelanddocument-
Dtrain
levelmodels. Theutterancesarecollectedfromwithinadialogue. ThecolordepictstheselectedPIIbyPresidio.
✓denotesleakage. denotesnoleakage.
×
chosen value of ε > 1 on both datasets. With- informationcomparedtodocument-levelmodels.
outadditionaltrainingdatafromWMT22,thepri-
vate training process of θdoc again becomes un- 8 Conclusion
stableforlongsequences. Thisinstabilityresults
Wehaveexaminedtheprivacyleakageofsentence-
in a divergent loss in the training process and a
level vs. document-level approaches using the
significantdropintranslationquality,evenworse
loss-based membership inference attack on the
with the added noise from DP-SGD. As the base
mLongT5model. Theresultsshowthatasentence-
of θaugdoc, θaugdoc benefits θaugdoc, which en-
zero−shot levelmodelhasmorerisksofprivacyleakagethan
suresthetranslationqualitytobehigherthanzero
adocument-levelmodel. Specifically,asentence-
intermsofBLEUevenatε = 1onbothdatasets.
levelmodelismorelikelytooverfitcomparedtoa
The document-level training model’s results also
document-levelmodel,whichcanleadtomorecon-
varysignificantlyacrossdifferentvaluesofεinthe
fidentguessingofsentence-leveltraininginstances.
privatetrainingsetupsforbothdatasets.
Furthermore,regardingtheprivacy/utilitytrade-
The privacy/utility trade-off evaluation also
offinthedocument-levelmodel,optimizingtrans-
showsthatthelanguagepairofthedatasethasim-
formermodelsforlongtexts,especiallywithDP-
pactonthetranslationqualityoftheprivatetraining
SGD,isachallengingtaskthatrequiresmoredata
model. To close the gap between the translation
thanourdownstreamdataset. Wedemonstrateour
qualityofθsen andθaugdoc on ts ee sn t,trainingwith
solutiontothisproblemwithanaugmentedtrain-
D
moredatashouldbeconsidered.
ingtechnique,usingalargepublicdataset,inour
caseWMT22,toachieveanacceptableutility,then
7.2 PIIextractionanderroranalysis
fine-tuneonthedownstreamdatasettoachievethe
Table 2 shows an example of leakage over three bestprivacy/utilitytrade-off. Forfuturework,we
differentεvaluesacrossthreemodelsettings. Over- aimtodesignabetterMIAthattakesintoaccount
all,withouttrainingwithDP-SGD,weobservethe the correlation aspect of NLP datasets. Further-
modeltooverfittoutteranceswithPII,evenwhen more, despite computational limitations (see Ap-
usingθdoc andθaugdoc. BytrainingwithDP-SGD, pendix C.3), we hope to scale up experiments to
thereisnoleakagewiththedocument-levelmodels. larger datasets with longer texts in future work,
However,thisisnotthecaseforθsen,evengoing suchasEuroparl(Koehn,2005).
down to ε = 10, where the customer’s name and
websiteURLseemtobememorized. Thissuggests
that θsen might require a more stringent privacy
budget(lowerε)topreventoverfittingtosensitiveLimitations centerswithintheUARuhr(https://uaruhr.
de). Thanks to Erion C¸ano and Lena Held for
AlthoughscalingDPtothedocumentlevelinNLP
helpfulfeedbackonapreliminarydraft.
showspromise,thereareseveralnotablepitfallsin
ourworkthatneedtobeaddressedinfuturework:
References
1. The training data for the document-level sce-
narioisinsufficient. Martin Abadi, Andy Chu, Ian Goodfellow, H Bren-
dan McMahan, Ilya Mironov, Kunal Talwar, and
2. Loss-basedMIAdoesnotconsiderthecorrela-
LiZhang.2016. Deeplearningwithdifferentialpri-
tionbetweendataandthePIIevaluationschema
vacy. InProceedingsofthe2016ACMSIGSACcon-
isnotperfect. ferenceoncomputerandcommunicationssecurity,
pages308–318.
Forpoint1,sinceweareaimingtofindasensi-
tivedataset(e.g. multipleinstancesofPII),weare RohanAnil,BadihGhazi,VineetGupta,RaviKumar,
andPasinManurangsi.2022. Large-scaledifferen-
limited by the availability of such data for NMT.
tiallyprivateBERT. InFindingsoftheAssociation
We are also limited by the size of the document-
forComputationalLinguistics: EMNLP2022,pages
leveldatasetwhenconcatenatingsentences,which 6481–6491,AbuDhabi,UnitedArabEmirates.As-
is crucial for training a document-level model. sociationforComputationalLinguistics.
AsZhuochengetal.(2023b)suggest, weneedat
IzBeltagy,MatthewEPeters,andArmanCohan.2020.
leastfourmilliontraininginstancestooutperform
Longformer: Thelong-documenttransformer. arXiv
the sentence-level model, while in our case, we preprintarXiv:2004.05150.
onlyhavelessthan700traininginstancesfortrain-
James Bradbury, Roy Frostig, Peter Hawkins,
ingthedocument-levelmodel.
Matthew James Johnson, Chris Leary, Dou-
Regarding point 2, the loss-based MIA has a gal Maclaurin, George Necula, Adam Paszke,
significant limitation as it does not take into ac- Jake VanderPlas, Skye Wanderman-Milne,
countthecorrelationbetweendata. Wearguethat and Qiao Zhang. 2018. JAX: composable
transformations of Python+NumPy programs.
document-levelprivacyisstrongerthansentence-
http://github.com/google/jax.
level privacy when considering this correlation.
Therefore,abetterMIAmethodthatconsidersthe Hannah Brown, Katherine Lee, Fatemehsadat
Mireshghallah, Reza Shokri, and Florian Trame`r.
correlation between data is needed to prove this
2022. What does it mean for a language model
claim. Future work should investigate the corre-
to preserve privacy? In Proceedings of the 2022
lationbetweendataandhowitaffectstheprivacy ACM Conference on Fairness, Accountability, and
guarantee,suchasHumphriesetal.(2023),butfor Transparency,pages2280–2292.
NLPtasks. Inaddition,thePIIevaluationschema
Nicholas Carlini, Florian Tramer, Eric Wallace,
focuses more on risk assessment than strict eval- Matthew Jagielski, Ariel Herbert-Voss, Katherine
uation. Our PII evaluation relies on the model’s Lee,AdamRoberts,TomBrown,DawnSong,Ulfar
confidenceinidentifyingtruepositivepredictions Erlingsson,etal.2021. Extractingtrainingdatafrom
large language models. In 30th USENIX Security
from the MIA and the detection of PII is carried
Symposium(USENIXSecurity21),pages2633–2650.
outautomaticallywithPresidio,whichmayre-
sultinfalsepositives. Additionally,forthecaseof CynthiaDworkandAaronRoth.2013. TheAlgorithmic
Foundations of Differential Privacy. Foundations
NMT,weonlyconsiderPIIinthetargetlanguage,
andTrends®inTheoreticalComputerScience,9(3-
assumingtheattackerhasaccesstoboththesource
4):211–407.
andtargetinstances. Thisisrelevanttotheperfor-
manceofPIIdetectioninsourceinputswhichare AnaCFarinha, M. Amin Farajian, MariannaBuchic-
chio, Patrick Fernandes, Jose´ G. C. de Souza, He-
notEnglish.
lena Moniz, and Andre´ F. T. Martins. 2022. Find-
ingsoftheWMT2022sharedtaskonchattransla-
Acknowledgements
tion. InProceedingsoftheSeventhConferenceon
Machine Translation (WMT), pages 724–743, Abu
This project was supported by the PrivaLingo
Dhabi,UnitedArabEmirates(Hybrid).Association
research grant (Hessisches Ministerium des In- forComputationalLinguistics.
nern und fu¨r Sport). This work has also been
Patrick Fernandes, Kayo Yin, Graham Neubig, and
partly supported by the Research Center Trust-
Andre´F.T.Martins.2021. MeasuringandIncreasing
worthy Data Science and Security (https://
ContextUsageinContext-AwareMachineTransla-
rc-trust.ai), one of the Research Alliance tion. InProceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsand MarzenaKarpinskaandMohitIyyer.2023. Largelan-
the 11th International Joint Conference on Natu- guage models effectively leverage document-level
ralLanguageProcessing(Volume1: LongPapers), contextforliterarytranslation,butcriticalerrorsper-
pages6467–6478,Online.AssociationforComputa- sist. In Proceedings of the Eighth Conference on
tionalLinguistics. MachineTranslation,pages419–451,Singapore.As-
sociationforComputationalLinguistics.
MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOn-
tanon,JianmoNi,Yun-HsuanSung,andYinfeiYang. OleksandraKlymenko,StephenMeisenbacher,andFlo-
2022. LongT5: EfficientText-To-TextTransformer rian Matthes. 2022. Differential privacy in natural
for Long Sequences. In Findings of the Associa- languageprocessingthestorysofar. InProceedings
tion for Computational Linguistics: NAACL 2022, oftheFourthWorkshoponPrivacyinNaturalLan-
pages724–736,Seattle,UnitedStates.Association guageProcessing,pages1–11,Seattle,UnitedStates.
forComputationalLinguistics. AssociationforComputationalLinguistics.
Jonathan Heek, Anselm Levskaya, Avital Oliver, Tom Kocmi, Rachel Bawden, Ondˇrej Bojar, Anton
Marvin Ritter, Bertrand Rondepierre, Andreas Dvorkovich, Christian Federmann, Mark Fishel,
Steiner, and Marc van Zee. 2023. Flax: A Thamme Gowda, Yvette Graham, Roman Grund-
neural network library and ecosystem for JAX. kiewicz,BarryHaddow,RebeccaKnowles,Philipp
http://github.com/google/flax. Koehn,ChristofMonz,MakotoMorishita,Masaaki
Nagata,ToshiakiNakazawa,MichalNova´k,Martin
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Popel,andMajaPopovic´.2022. Findingsofthe2022
VikasRaunak,MohamedGabr,HitokazuMatsushita, conference on machine translation (WMT22). In
YoungJinKim,MohamedAfify,andHanyHassan ProceedingsoftheSeventhConferenceonMachine
Awadalla. 2023. How Good Are GPT Models at Translation(WMT),pages1–45,AbuDhabi,United
MachineTranslation? AComprehensiveEvaluation. ArabEmirates(Hybrid).AssociationforComputa-
ArXiv:2302.09210[cs]. tionalLinguistics.
Sorami Hisamoto, Matt Post, and Kevin Duh. 2020. PhilippKoehn.2005. Europarl: Aparallelcorpusfor
Membership inference attacks on sequence-to- statistical machine translation. In Proceedings of
sequencemodels: Ismydatainyourmachinetrans- MachineTranslationSummitX:Papers,pages79–86,
lationsystem? TransactionsoftheAssociationfor Phuket,Thailand.
ComputationalLinguistics,8:49–63.
TakuKudoandJohnRichardson.2018. SentencePiece:
HongshengHu,ZoranSalcic,LichaoSun,GillianDob- A simple and language independent subword tok-
bie, Philip S. Yu, and Xuyun Zhang. 2022. Mem- enizeranddetokenizerforneuraltextprocessing. In
bershipInferenceAttacksonMachineLearning: A Proceedings of the 2018 Conference on Empirical
Survey. ACMComputingSurveys,54(11s):1–37. Methods in Natural Language Processing: System
Demonstrations, pages 66–71, Brussels, Belgium.
T.Humphries,S.Oya,L.Tulloch,M.Rafuse,I.Gold-
AssociationforComputationalLinguistics.
berg,U.Hengartner,andF.Kerschbaum.2023. In-
vestigatingmembershipinferenceattacksunderdata XuechenLi,FlorianTramer,PercyLiang,andTatsunori
dependencies. In2023IEEE36thComputerSecurity Hashimoto. 2022. Large language models can be
FoundationsSymposium(CSF),pages473–488,Los strongdifferentiallyprivatelearners. InInternational
Alamitos,CA,USA.IEEEComputerSociety. ConferenceonLearningRepresentations.
Timour Igamberdiev and Ivan Habernal. 2023. DP- YinhanLiu,JiataoGu,NamanGoyal,XianLi,Sergey
BART for privatized text rewriting under local dif- Edunov, Marjan Ghazvininejad, Mike Lewis, and
ferentialprivacy. InFindingsoftheAssociationfor LukeZettlemoyer.2020. Multilingualdenoisingpre-
ComputationalLinguistics: ACL2023,pages13914– training for neural machine translation. Transac-
13934,Toronto,Canada.AssociationforComputa- tionsoftheAssociationforComputationalLinguis-
tionalLinguistics. tics,8:726–742.
TimourIgamberdiev,DoanNamLongVu,FelixKuen- YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
necke,ZhuoYu,JannikHolmer,andIvanHabernal. dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
2024. DP-NMT:Scalabledifferentiallyprivatema- Luke Zettlemoyer, and Veselin Stoyanov. 2019.
chine translation. In Proceedings of the 18th Con- Roberta: A robustly optimized bert pretraining ap-
ferenceoftheEuropeanChapteroftheAssociation proach. arXivpreprintarXiv:1907.11692.
forComputationalLinguistics: SystemDemonstra-
tions,pages94–105,St.Julians,Malta.Association FatemehsadatMireshghallah, ArchitUniyal, Tianhao
forComputationalLinguistics. Wang, David Evans, and Taylor Berg-Kirkpatrick.
2022. Anempiricalanalysisofmemorizationinfine-
BargavJayaramanandDavidEvans.2019. Evaluating tunedautoregressivelanguagemodels. InProceed-
differentiallyprivatemachinelearninginpractice. In ingsofthe2022ConferenceonEmpiricalMethods
28thUSENIXSecuritySymposium(USENIXSecurity inNaturalLanguageProcessing,pages1816–1826,
19), pages 1895–1912, Santa Clara, CA. USENIX AbuDhabi,UnitedArabEmirates.Associationfor
Association. ComputationalLinguistics.Ilya Mironov. 2017. Re´nyi Differential Privacy. In IEEE Symposium on Security and Privacy (SP),
2017IEEE30thComputerSecurityFoundationsSym- pages3–18. ISSN:2375-1207.
posium(CSF),pages263–275,SantaBarbara,CA.
IEEE. David Uthus, Santiago Ontanon, Joshua Ainslie, and
MandyGuo.2023. mLongT5: Amultilingualand
Nafise Sadat Moosavi, Andreas Ru¨ckle´, Dan Roth, efficienttext-to-texttransformerforlongersequences.
and Iryna Gurevych. 2021. Scigen: a dataset for InFindingsoftheAssociationforComputationalLin-
reasoning-aware text generation from scientific ta- guistics:EMNLP2023,pages9380–9386,Singapore.
bles. InThirty-fifthConferenceonNeuralInforma- AssociationforComputationalLinguistics.
tionProcessingSystemsDatasetsandBenchmarks
Track(Round2). RachelWicksandMattPost.2023. Identifyingcontext-
dependenttranslationsforevaluationsetproduction.
KishorePapineni,SalimRoukos,ToddWard,andWei- InProceedingsoftheEighthConferenceonMachine
JingZhu.2002. Bleu: amethodforautomaticevalu- Translation,pages452–467,Singapore.Association
ationofmachinetranslation. InProceedingsofthe forComputationalLinguistics.
40thAnnualMeetingoftheAssociationforCompu-
tational Linguistics, pages 311–318, Philadelphia, MinghaoWu,GeorgeFoster,LizhenQu,andGholam-
Pennsylvania,USA.AssociationforComputational rezaHaffari.2023. DocumentFlattening: Beyond
Linguistics. ConcatenatingContextforDocument-LevelNeural
Machine Translation. In Proceedings of the 17th
NataliaPonomareva,JasmijnBastings,andSergeiVas- ConferenceoftheEuropeanChapteroftheAssocia-
silvitskii. 2022. Training text-to-text transformers tionforComputationalLinguistics,pages448–462,
with privacy guarantees. In Findings of the Asso- Dubrovnik,Croatia.AssociationforComputational
ciation for Computational Linguistics: ACL 2022,
Linguistics.
pages2182–2193,Dublin,Ireland.Associationfor
ComputationalLinguistics. LintingXue,NoahConstant,AdamRoberts,MihirKale,
RamiAl-Rfou,AdityaSiddhant,AdityaBarua,and
Natalia Ponomareva, Hussein Hazimeh, Alex Ku-
ColinRaffel.2021. mT5: Amassivelymultilingual
rakin, Zheng Xu, Carson Denison, H Brendan
pre-trainedtext-to-texttransformer. InProceedings
McMahan, Sergei Vassilvitskii, Steve Chien, and
ofthe2021ConferenceoftheNorthAmericanChap-
AbhradeepGuhaThakurta.2023. Howtodp-fyml:
teroftheAssociationforComputationalLinguistics:
A practical guide to machine learning with differ-
HumanLanguageTechnologies,pages483–498,On-
entialprivacy. JournalofArtificialIntelligenceRe-
line.AssociationforComputationalLinguistics.
search,77:1113–1201.
SamuelYeom,IreneGiacomelli,MattFredrikson,and
MattPostandMarcinJunczys-Dowmunt.2023. Escap-
Somesh Jha. 2018. Privacy risk in machine learn-
ingthesentence-levelparadigminmachinetransla-
ing: Analyzingtheconnectiontooverfitting. In2018
tion. ArXiv:2304.12959[cs].
IEEE 31st Computer Security Foundations Sympo-
sium(CSF),pages268–282.
AlecRadford,KarthikNarasimhan,TimSalimans,Ilya
Sutskever, et al. 2018. Improving language under-
YingYinandIvanHabernal.2022. Privacy-preserving
standingbygenerativepre-training.
modelsforlegalnaturallanguageprocessing. InPro-
ceedingsoftheNaturalLegalLanguageProcessing
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Workshop2022,pages172–183,AbuDhabi,United
Zhou,WeiLi,andPeterJ.Liu.2020. Exploringthe ArabEmirates(Hybrid).AssociationforComputa-
limitsoftransferlearningwithaunifiedtext-to-text tionalLinguistics.
transformer. JournalofMachineLearningResearch,
TianyiZhang,VarshaKishore,FelixWu,KilianQWein-
21(140):1–67.
berger, andYoavArtzi.2020. BERTScore: Evalu-
Mat¯ıss Rikters, Ryokan Ri, Tong Li, and Toshiaki atingTextGenerationwithBERT. InInternational
Nakazawa.2019. Designingthebusinessconversa- ConferenceonLearningRepresentations.
tioncorpus. InProceedingsofthe6thWorkshopon
ZhangZhuocheng,ShuhaoGu,MinZhang,andYang
AsianTranslation,pages54–61,HongKong,China.
Feng.2023a. Addressingthelengthbiaschallengein
AssociationforComputationalLinguistics.
document-levelneuralmachinetranslation. InFind-
ManuelSenge,TimourIgamberdiev,andIvanHabernal. ingsoftheAssociationforComputationalLinguis-
2022. Onesizedoesnotfitall: Investigatingstrate- tics: EMNLP2023,pages11545–11556,Singapore.
gies for differentially-private learning across NLP AssociationforComputationalLinguistics.
tasks. In Proceedings of the 2022 Conference on
EmpiricalMethodsinNaturalLanguageProcessing, ZhangZhuocheng,ShuhaoGu,MinZhang,andYang
pages7340–7353,AbuDhabi,UAE. Feng.2023b. Scalinglawfordocumentneuralma-
chine translation. In Findings of the Association
Reza Shokri, Marco Stronati, Congzheng Song, and forComputationalLinguistics: EMNLP2023,pages
VitalyShmatikov.2017. MembershipInferenceAt- 8290–8303, Singapore. Association for Computa-
tacks Against Machine Learning Models. In 2017 tionalLinguistics.A DifferentialprivacyandDP-SGD
Algorithm 1: Differentially private SGD
(Outline)(Abadietal.,2016)
Differentialprivacy(DP)(DworkandRoth,2013)
isamathematicalframeworkthatensuresthatthe Input: Examples x 1,...,x N ,loss
output of an analysis on a dataset remains un- function L{ (θ) = N1 (cid:80) i} L(θ,x i).
changedwithinaspecificthresholdwhenanydata Parameters: learningrateη t,noise
pointisaddedorremovedfromthedataset. More scaleσ,groupsizeL,gradientnorm
formally,foraprivacybudgetε 0,δ [0,1],a boundC.
mechanism : Dn k is(ε,≥ δ)diff∈ erentially Initializeθ 0 randomly
privateifforM alldatas→ etsR D andD′ thatdifferinat fort [T]do
∈
TakearandomsampleL withsampling
mostoneinstance,andforallS Range( ): t
⊆ M probabilityL/N
Pr[ (D) S] exp(ε) Pr[ (D′) S]+δ
Computegradient
M ∈ ≤ · M ∈
(3)
Foreachi L ,compute
t
Inotherwords,amechanism is(ε,δ)differen-
g (x )
∈
(θ ,x )
tiallyprivateiftheprobabilityM
thatthemechanism
t i
←
∇θtL t i
Clipgradient
M
expr (e εt )ur tn imsa esre thsp eo pn rs oe bs
ab∈
iliS tyo on fd ta ht eas met ecD hais nia st mmost
g¯ t
←
g t(x
i)/max(cid:16)
1,
||gt( Cxi)||2(cid:17)
M Addnoise
returnsaresponses ∈ S ondatasetD′. g˜ 1 (cid:0)(cid:80) g (x )+ (0,σ2C2I)(cid:1)
Accordingtothedefinition,thesmallerthepri- t ← L i t i N
Descent
vacy budget ε, the greater privacy guarantees the
θ θ η g˜
M mechanism provides, due to its exponential Outpt u+ t1 :← θ T at n−dcot mt putetheoverallprivacy
nature, making the differing instance of D and
cost(ε,δ)usingaprivacy
D′ indistinguishable. This provides individuals
accountingmethod.
withplausibledeniability,asanattackercannotbe
certainwhetheraspecificinstancebelongstothe
datasetD ornot. However,choosingtheappropri-
ateprivacybudgetεiscrucialtoensuretheprivacy DuringeachSGDstept, (Abadietal.,2016)cal-
guarantee of the mechanism. If the privacy culatethegradient (θ,x )forarandomsubset
M ∇θ L i
budgetistoolarge,the mechanismwillnotpro- ofsamplesL viaPoissonSampling. Theℓ -norm
M t 2
videasatisfactoryprivacyguarantee. Ontheother of each gradient is then clipped, Gaussian noise
hand,iftheprivacybudgetistoolow,thenthenoise (0,σ2C2I) is added, and the average is taken
N
added to the query is very high; thus making the overallnoisygradientsforeachelementofL . A
t
mechanism impractical. Toachievethedesired step is then taken in the reverse direction of this
M
outcome, a compromise must be made between noisygradienttoupdateparametersθ . Thealgo-
t
utility and privacy in the DP application, this is rithm aims to prevent over-optimization towards
knownastheprivacy-utilitytrade-off (Dworkand individualdatapointsinthetrainingdataset.
Roth,2013). Therefore,selectingtheappropriate
privacy budget for mechanism is crucial. In B Datapreparation
M
addition, the privacy budget is not fixed and can
Figure7bshowsanexampleoftheMAIAdataset
be adjusted. The privacy budget is adjustable de-
withreplacedPIIdata.
pendingonthespecificusecase,data,andprivacy
preferences. Typically, to achieve the mecha-
M MAIA ThepreprocessingfortheMAIAdataset
nismthatsatisfiestheDPdefinition,wegenerally
is similar to the BSD dataset. Moreover, since
applynoisesampledfromtheGaussiandistribution
MAIA is a real-world dataset from Unbabel’s
tothe‘raw’output.
client,itisanonymizedbeforebeingreleased. To
In the case of deep learning, we can apply DP
make the dataset more realistic, we replace the
during the training process, in particular prior to
anonymized PII data with artificial PII data. We
the optimization step of a deep neural network,
useFaker7 togeneratefakePIIdataandreplace
acting as the data analyst, as in the case of DP-
thepre-anonymizedPIIdataintheMAIAdataset.
SGD(Abadietal.,2016). Thismethodispresented
In each dialogue, we keep the replaced PII data
in Algorithm 1, in which the empirical loss func-
tionL(θ)isminimizedwithanoisyvariantofSGD. 7https://faker.readthedocs.io/en/train 2000 train
2500 validation validation
test 1750 test
2000 1500
1250
1500
1000
1000 750
500
500
250
0 0
10 20 30 40 50 60 0 20 40 60 80 100 120 140
Numberoftokens Numberoftokens
(a)Sentence-levelBSDJapanese (b)Sentence-levelMAIAGerman
35 train train
validation 20 validation
30 test test
25 15
20
10
15
10
5
5
0 0
200 400 600 800 1000 0 200 400 600 800 1000 1200 1400 1600
Numberoftokens Numberoftokens
(c)Document-levelBSDJapanese (d)Document-levelMAIAGerman
Figure6: Distributionoftokenlengthinsentence-levelanddocument-levelforGermaninMAIAandJapanesein
BSDdatasets.
Notation Description 2018)thatcomeswithmLongT5. Thetokenizeris
sen Sentence-leveltrainingdata trainedonmC4(Raffeletal.,2020)withavocabu-
Dt sr ea nin
Sentence-levelvalidationdata larysizeof256,384. AsdepictedinFigure6and
Dv sea nl
Sentence-leveltestdata
Dt de os ct
Document-leveltrainingdata
Table4,bothdatasetshavealongtaildistribution
Dt dr oa cin
Document-levelvalidationdata oftokenlength.
Dv da ol
c Document-leveltestdata
D θst ee nst Finetunedmodelon sen
θdoc
FinetunedmodelonDt dr oa cin
θaugdoc FinetunedmodelonD dot cra ui mn ent-levelWMT22 Regardinghyperparametertuning,wedividethis
zero−shot
θaugdoc Finetunedmodelon doc withθaugdoc checkpoint intotwocases: (1)Normaltrainingand(2)private
Dtrain zero−shot
training. For both cases, we always set the maxi-
Table3: Notationusedinthiswork. mum sequence length to the longest sequence in
thetrainingdataset. Weconductedanexperiment
withtruncatedsequencesat512,256,and128to-
consistentacrossallutterances(e.g.,#NAME#is
kens on θdoc. The results of 512 and 256 token
alwaysreplacedbyoneartificialnamewithinadi-
sequencelengthsarebetterthansettingthemodel
alogue). Wealsouselocalizedfakedataforeach
tothelongestsequenceinthetrainingdata,when
language(e.g.#PRS ORG#isreplacedbyaGer-
using a high value of ε. However, the results at
mancompanyname).
smallεvaluesisindifferenttothatreportedinthis
workatthelongestsequenceofthetrainingdata.
C Hyperparameters
Wefirstconsidertheoptimalmaximumsequence
length. Fortokenizationofthetrainingdata,weuse Table5showsthefinalhyperparametersforeach
theSentencePiecetokenizer(KudoandRichardson, dataset.
stniopatadfo#
stniopatadfo#
stniopatadfo#
stniopatadfo#Train Validation Test
µ σ2 max µ σ2 max µ σ2 max
BSD Japanese 491 165 1007 484 155 843 495 150 1025
English 499 174 1090 486 163 870 495 158 1060
MAIA German 589 278 1606 466 440 1101 515 488 1200
English 555 262 1504 180 174 1034 242 230 1160
Table4: Maximumtokenlength,approximatemeanandstandardvariationofeachlanguageinthedocument-level
BSDandMAIAdatasets. WeusetheSentencePiecetokenizertotokenizethedata.
Dataset ϵ TrainingUnit Max.seq.length lr Epochs TotalBatchSize
MAIA Sentence-Level 128 1e 3 30 32
∞ −
Document-Level 1610 3e 3 25 2
∞ −
990,99,10,1 Sentence-Level 128 1e 2 30 1024
{ } −
990,99,10,1 Document-Level 1610 1e 2 100 256
{ } −
BSD Sentence-Level 64 1e 3 15 32
∞ −
Document-Level 1100 3e 3 30 2
∞ −
400,40,10,1 Sentence-Level 64 1e 2 15 1024
{ } −
400,40,10,1 Document-Level 1100 1e 2 100 512
{ } −
WMT22-JA-EN Document-Level 1200 1e 2 2 4
∞ −
WMT22-DE-EN Document-Level 1500 1e 2 2 4
∞ −
Table5: Finalresultsforhyperparametersearch.
Dataset #Member #Non-member Dataset #PII
BSD 4147 4147 BSD 1,286
MAIA 4597 4597 MAIA 1,156
(a)Numberofexamplesusedforsentence-levelloss-basedMIA. (b)NumberofPIIinDsen ofeachdataset
sampled
Table6: StatisticsofdatasetsusedforMIAevaluation.
Lang. Pair Sen.-level Doc.-level fivedifferentseedsforeachfinalselectedhyperpa-
rameter,foreachεvalue. Weusethesamenotation
JA-EN 33,875,119 851,525
for epochs when running DP-SGD training with
DE-EN 295,805,439 4,779,636
PoissonsamplingasAbadietal.(2016),being N.
L
Next,weutilizeverylargebatchsizesforbothof
Table 7: Number of training examples in WMT22
thesemethods,settingLtoalargevalueandbuild-
dataset.
ing up the resulting drawn batches with gradient
accumulation. Allprivatetrainingexperimentsare
C.1 Hyperparametertuningfornormal conductedusingoneH100GPU,duetothelimita-
training tionofduplicatingexamplesinlotswhensampling
In normal training, we only use one seed for hy- tomultipleGPUs. Similartonormaltraining,we
perparametertuning,butthreerunsforeachfinal keepthesamemaximumsequencelengthforboth
selectedhyperparametertogettheaverageperfor- datasetsinprivatetraining.
mance. We conduct experiments on two H100
C.3 Augmentedtrainingfordocument-level
GPUs.
models
C.2 Hyperparametertuningforprivate
Wethenfine-tunethemLongT5checkpointonthe
training
concatenateddocumentsfor2epochswithabatch
For the final results with DP-SGD, we run two sizeof16ontwoH100GPUs. Thehyperparameter
trialsforeachhyperparameterconfiguration,with searchspaceisthesameasfornormaltraining,as{
”de”: ... Kunde: Hallo, können sie mir sagen wann das bestellte Bett ca Versand wird? Agent: Guten Morgen Olav
Kusch Agent: vielen Dank, dass Sie Hethur Ullmann GmbH& Co. KG kontaktiert haben. Ich hoffe, dass es Ihnen gut geht. ...
”en”: ... Customer: Hello, Can you tell me when the ordered bed will be approximately shipped? Agent: Good Morning Olav
Kusch. Agent: Thank you for contacting Hethur Ullmann GmbH& Co. KG I hope you are well. ...
}
(a)Sentence-leveloriginaltrainingpairs
{
...
”de”: Kunde: Hallo, können sie mir sagen wann das bestellte Bett ca Versand wird?
”en”: Customer: Hello, Can you tell me when the ordered bed will be approximately shipped?
”de”: Agent: Guten Morgen Olav Kusch,
”en”: Agent: Good Morning Olav Kusch.
”de”: Agent: vielen Dank, dass Sie Hethur Ullmann GmbH& Co. KG kontaktiert haben. Ich hoffe, dass es Ihnen gut geht.
”en”: Agent: Thank you for contacting Hethur Ullmann GmbH& Co. KG I hope you are well.
...
}
(b)Sentence-levelwithartificialreplacedPIItrainingpairs
{
...
”de”: Hallo, können sie mir sagen wann das bestellte Bett ca Versand wird?
”en”: Hello, Can you tell me when the ordered bed will be approximately shipped?
”de”: Guten Morgen #NAME#,
”en”: Good Morning #NAME#.
”de”: vielen Dank, dass Sie #PRS_ORG# kontaktiert haben. Ich hoffe, dass es Ihnen gut geht.
”en”: Thank you for contacting #PRS_ORG# I hope you are well.
...
}
(c)Document-levelwithartificialreplacedPIItrainingpair(utteranceswithinadialogue)
Figure7: Differentbetweentrainingexamplesfordocument-levelvssentence-levelinMAIAdataset.
describedabove. ForJapanesetoEnglishtransla- the best correlation with human judgment on the
tion, we use the entire WMT dataset. However, WMT16MetricsSharedTask(Zhangetal.,2020).
forGermantoEnglishtranslation,weonlyusethe BERTScore uses pre-normalized vectors for co-
firstpartofthedataset. Thisisdonetoensurethat sine similarity, resulting in computed scores the
thedatasetsizeisequaltothatoftheJapaneseto range[ 1,1]. However,inpractice,theobserved
−
Englishdataset(851,525),duetotimeconstraints.8 BERTScore values are often limited to a nar-
row range (Moosavi et al., 2021; Zhang et al.,
D BERTScoreModification
2020). For instance, when using the default
large RoBERTa10 model, BERTScore typically
We also use BERTScore (Zhang et al., 2020) for
falls between 0.85 and 0.95. This is due to the
semantic similarity evaluation. BERTScore uses
learnedgeometryofcontextualembeddingswhich
RoBERTa embeddings (Liu et al., 2019) to com-
results in different scores from different embed-
putethe similarity betweenthecandidatetransla-
dings. Althoughthischaracteristicdoesnotaffect
tion and the reference translation. However, its
BERTScore’s ability to rank text generation sys-
embeddings are limited to 512 tokens, which is
tems,itdoesmaketheresultingscorelesscompre-
not enough for our task. Therefore, we modify
hensibletohumans. Toaddressthisissue, Zhang
BERTScoretouseLongformer9 embeddings(Belt-
etal.(2020)rescaleBERTScoreusingitsempirical
agyetal.,2020)insteadofRoBERTaembeddings.
lowerboundbasabaseline. Thecomputationofb
Longformer’sembeddingsareabletoencodelong
iscarriedoutusingCommonCrawl11 monolingual
sentencesupto4,096tokens. Anothermodification
datasets. Foreachlanguageandcontextualembed-
thatwemaketoBERTScoreinthisworkisrescal-
dingmodel,onemillioncandidate-referencepairs
ingthescorebaselinetomakeitmorereadable.
are created by grouping two random sentences.
The score is computed from the seventh layer
Due to the random pairing and corpus diversity,
output of Longformer’s embeddings, since it has
eachpairhasverylowlexicalandsemanticoverlap
8It takes 98 hours to finish one epoch on the entire (BLEU computed on these pairs is around zero).
document-levelWMT22. Wealsosplitthedatasetbyhalf
Tocomputethevalueofb, Zhangetal.(2020)take
andtrainedthemodeloneachpart.Theresultsarepoorcom-
paredtousingonlyasmallpartofthedata,despitetrainingof the average of BERTScore computed on the sen-
eachparttosimulatetrainingtheentiredatasetinoneepoch.
9https://huggingface.co/allenai/ 10https://huggingface.co/roberta-large
longformer-base-4096 11https://commoncrawl.org/...
{
”ja_speaker”: 土井さん
”ja_sentence”: 稲田さん、H社の高市様からお電話です。
”en_speaker”: Doi−san
”en_sentence”: Inada−san, you have a call from Mr. Takaichi of Company H.
}
{
”ja_speaker”: 稲田さん
”ja_sentence”: もしもし、稲田です。
”en_speaker”: Inada−san
”en_sentence”: Hello, this is Inada.
}
...
(a)Sentence-leveloriginaltrainingpairs
{
...
”ja”: 土井さん: 稲田さん、H社の高市様からお電話です。
”en”: Doi−san: Inada−san, you have a call from Mr. Takaichi of Company H.
”ja”: 稲田さん: もしもし、稲田です。
”en”: Inada−san: Hello, this is Inada.
...
}
(b)Sentence-levelmodifiedtrainingpairs
{
”ja”: ... 土井さん: 稲田さん、H社の高市様からお電話です。 稲田さん: もしもし、稲田です。...
”en”: ... Doi−san: Inada−san, you have a call from Mr. Takaichi of Company H. Inada−san: Hello, this is Inada. ...
}
(c)Document-leveltrainingpair(utteranceswithinadialogue)
Figure8: Differencebetweentrainingexamplesfordocument-levelvssentence-levelinBSDdataset.
1.0 1.0
θsen θsen
0.8
θdoc
0.8
θdoc
θaugdoc θaugdoc
0.6 θ za eu rg od -o shc ot 0.6 θ za eu rg od -o shc ot
0.4 0.4
0.2 0.2
0.0 0.0
1 10 40 400 1 10 99 990
∞ ∞
ε ε
(a)BSD (b)MAIA
Figure9: BERTScoreon sen forallfourmodelfine-tuningconfigurations. Lowerεcorrespondstobetterprivacy.
Dtest
tence. Afterthat,usingbaselineb,wegetalinearly E MIAexperimentdetails
rescaledBERTScore.
Table6ashowsthenumberofmembersandnon-
F b
Fˆ = BERT − membersforMIAevaluation.
BERT
1 b
−
The result Fˆ typically ranges between 0
BERT
F Privacy/utilitytrade-offevaluationwith
and 1, with anything below this range clipped to
BERTScore
0. As Zhang et al. (2020) note, this method does
not affect the ranking ability or human correla-
tionofBERTScore,asmeasuredbyPearson’sand Apart from using BLEU for evaluating the pri-
Kendall’scoefficients,buttoenhancethereadabil- vacy/utility trade-off (Section 5.5), we also use
ity of the score. Since the Longformer rescaled BERTScorefortheevaluationoftranslationqual-
BERTScore is not available, we compute it our- ityatthedocumentlevel,withrespecttosemantic
selves. similarity.
erocSTREB erocSTREB1.0 1.0
θdoc
0.8
θaugdoc
0.8
θaugdoc
zero-shot
0.6 0.6
0.4 0.4
θdoc
0.2 0.2 θaugdoc
θaugdoc
zero-shot
0.0 0.0
1 10 40 400 1 10 99 990
∞ ∞
ε ε
(a)BSD (b)MAIA
Figure10:BERTScoreon doc forthethreedocument-levelmodelfine-tuningconfigurations.Lowerεcorresponds
Dtest
tobetterprivacy.
F.1 Privacy/utilitytrade-offon doc noise during the training process with DP-SGD,
test
D
while the BERTScore of θaugdoc is equal to that
Figure 10 shows the BERTScore
augdoc
ofθ atapproximately0.38.
of θdoc and θaugdoc on doc. It is evident zero−shot
test
D
thattheBERTScoredecreasesasthevalueofεis
G Discussionontranslationquality
decreased for both datasets, similar to the BLEU
scoreresults. ComparedtoBLEUscores,themain Finally, Table 8 and Table 9 show that the
difference is that θaugdoc is more stable across document-leveltrainingmodelmainlyduplicates
different values of ε, in particular for the BSD the sentence until it reaches the maximum se-
dataset. Itisalsoworthnotingthatatε = 400on quence length. Overall, θaugdoc shows better per-
BSD,theBERTScoreofθdoc is0,whiletheBLEU formanceforprivatetrainingthanθdoc andisclose
scoreisstillaround2. OnMAIA,theBERTScore toθsen performancewithpost-processing.
ofθdoc andθaugdoc showslessvariationatε =
∞ H DPguaranteesinourexperiments
comparedtotheBLEUscore.
Toprovidealltheinformationneededtounderstand
F.2 Privacy/utilitytrade-offon sen
test our privacy guarantees, we follow the guidelines
D
Figure 9 shows the BERTScore of θsen, outlinedinPonomarevaetal.(2023).
θdoc and θaugdoc on sen. On MAIA, the
test
BERTScore of θsen sD lowly decreases as the 1. DP setting. We provide a central DP guar-
anteewheretheserviceprovideristrustedto
value of ε is decreased, for about 0.1 points per
correctlyimplementthemechanism.
decrease in ε from to 10. It is interesting that
∞
the BERTScore of θaugdoc varies a lot across
2. InstantiatingtheDPDefinition
differentvaluesofε,e.g.,0.088 0.079atε = 10
±
and0.134 0.081atε = 99. (a) Data accesses covered: Our DP guar-
±
On BSD, the BERTScores antees apply only to a single training
of θsen, θdoc and θaugdoc are very close to run. We don’t account for hyperparam-
one other, especially at ε = . From ε = 400, eter tuning in our guarantees. Public
∞
we no longer observe any meaningful trans- multilingualC4data(Raffeletal.,2020;
lation quality from θdoc. The BERTScores Xueetal.,2021)isusedforpre-training
of θsen and θaugdoc start to converge and become mLongT5.
equal at ε = 10, which demonstrates that the (b) Finalmechanismoutput: Onlythemodel
semantic content of θsen and θaugdoc equals predictions, such as the translated sen-
at the sentence level on the BSD dataset. As tences generated by the models trained
expected, the BERTScore of θsen at ε = 1 is withDP,arereleased. Themechanism’s
equal to zero due to the high amount of added output is technically the full sequence
erocSTREB erocSTREBModel ϵ SystemOutput
θsen Customer:IjustboughtabookwithmyGeislerConradiGmbH,itseemstobeonit.
∞
990 Customer:IboughtabookdirectlywithmyGeislerConradiGmbH,itseemstobeonit.
99 Customer:IboughtabookdirectlywithmyGeislerConradiGmbH,itseemstobeonit.
10 Customer:IboughttheGeislerConradiGmbH,abookitseemstobeon.
θdoc Customer:IboughtabookdirectlywithmyGeislerConradiGmbH,itseemstobeonit.
∞
Customer:IordereddirectlywithGeislerConradiGmbH,abook.Itseemstobeon..
n
\
Customer:IordereddirectlywithmyGeislerConradiGmbH,abook.Itseemstobeon..
n
\
990 Customer:IordereddirectlywithmyGeislerConradiGmbH,abook.Itseemstobeon..
n
\
Customer:IordereddirectlywithmyGeislerConradiGmbH,abook.Itseemstobeon..
n
\
Customer:IordereddirectlywithmyGeislerConradiGmbH,abook.Itseemstobeon..
θaugdoc Customer:IboughtabookdirectlywithmyGeislerConradiGmbHanditseemstobeonit.
∞
Customer:IboughtdirectlywithmyGeislerConradiGmbH,abookitseemstobeonit.
Customer:IboughtdirectlywithmyGeislerConradiGmbH,abookitseemstobeonit.
990 Customer:IboughtdirectlywithmyGeislerConradiGmbH,abookitseemstobeonit.
Customer:IboughtdirectlywithmyGeislerConradiGmbH,abookitseemstobeonit.
Customer:IboughtdirectlywithmyGeislerConradiGmbH,abookitseemstobeonit.
Customer:HavedirectlywithmyGeislerConradiGmbH,boughtabookitseemstobeonittoo.
Customer:HavedirectlywithmyGeislerConradiGmbH,boughtabookitseemstobeonittoo.
99 Customer:HavedirectlywithmyGeislerConradiGmbH,boughtabookitseemstobeonittoo.
Customer:HavedirectlywithmyGeislerConradiGmbH,boughtabookitseemstobeonittoo.
Customer:HavedirectlywithmyGeislerConradiGmbH,boughtabookitseemstobeonittoo.
Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonittoo.
Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonittoo.
10 Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonittoo.
Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonittoo.
Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonittoo.
Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonit.
Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonit.
1 Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonit.
Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonit.
Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonit.
Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonittoo.
Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonittoo.
θaugdoc - Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonittoo.
zero−shot
Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonittoo.
Customer:HavedirectlywithmyGeislerConradiGmbH,abookboughtitseemstobeonittoo.
Original Reference Customer:IboughtabookdirectlywithmyGeislerConradiGmbH,itseemstobeonittoo.
Source Kunde:HabedirektmitmeinemGeislerConradiGmbH,einbuchgekauftesscheintauchdraufzusein.
Table8: Translationsamplefrom sen ofMAIAdataset
Dtest
of privatized gradients, and the guaran- to128tokens,dependingonthedataset.
teealsoappliesatthislevel. Hence, all The document-level unit is the whole
checkpointsareprotectedandcanbere- conversation dialogue, which can be
leasedpublicly. composed of multiple sentences. Thus,
the maximum length of the document
(c) Unit of privacy. Since we are working
is not limited, and in our experiments,
in the NLP context, we consider sen-
the maximum length of the document
tencesanddocumentsastheunitofpri-
is up to 1,700 tokens. Token counting
vacy. Thesentence-levelunitisanutter-
isdoneaftertokenizationusingSenten-
anceinaconversation,typicallyasingle
cePiece (Kudo and Richardson, 2018).
sentence with a maximum length of 64Model ϵ SystemOutput
θsen Akiyama-san:Oki-san,canyoupreparesomedesignsforcupsandcardsbynextweek’smeeting?
∞
400 Ms.Murayama:Howcanyoupreparethedesignofyourcupandcardforthenextmeeting?
40 Mr.Yamamoto-san:Howcanyouprepareyourteamfortheupcomingmeeting?\nMichi
10 Mr.Miyamoto-san:I’mgoingtomakeacardforthemeetingtomorrow.
Mr.Maeda:Howmanycupsandcarddesignsdoyouneedtopreparefornextweek’smeeting?￿
θdoc
∞ Mr.Maeda:Howmanycardsdoyouneedtodesignforyourteam?
400 秋山さん,2020-08-202020-08-202020-08-202020-08-202020-08-20
θaugdoc Akiya-san:Bessho-san,canyouteamupwithyourteamtodraftsomecupandcarddesignsbynextweek’smeeting?
∞
Mr.Akiya:Mr.Bessho,canyoupreparesomecupandcarddesignideasforyourteambythemeetingnextweek?￿
400 Mr.Akiya:Mr.Bessho,canyoupreparesomecupandcarddesignideasforyourteambythemeetingnextweek?￿
Mr.Akiya:
Mr.Akiya:Mr.Bessho,canyoupreparesomecupandcarddesignideasforyourteambythemeetingnextweek?
40 Mr.Akiya:Mr.Bessho,canyoupreparesomecupandcarddesignideasforyourteambythemeetingnextweek?￿
Mr.Akiya:
Mr.Akiyama:Mr.Bessho,canyourteampreparesomecupandcarddesignideasbynextweek’smeeting?￿
10 Mr.Akiyama:Mr.Bessho,canyourteampreparesomecupandcarddesignideasbynextweek’smeeting?￿
Mr.Akiyama
Akiyama:Mr.Bessho,canyourteampreparesomecupandcarddesignideasbynextweek’smeeting?
1 Mr.Akiyama:Mr.Bessho,canyourteampreparesomecupandcarddesignideasbynextweek’smeeting?
Mr.Akiyama:Mr.Bess
Mr.Akiyama:Mr.Bessho,canyoupreparesomecupandcarddesignideasforyourteambynextweek’smeeting?￿
θaugdoc - Mr.Akiyama:Mr.Bessho,canyoupreparesomecupandcarddesignideasforyourteambynextweek’smeeting?￿
zero−shot
Mr
Reference Mr.Akiyama:Ms.Bessho,canyourteamprepareafewdesignideasforthecupandcardbynextweek’smeeting?
Source 秋山さん:別所さん、来週のミーティングまでにあなたのチームでカップとカードのデザイン案を幾つか準備してもらえますか？
Table9: Translationsamplefrom sen ofBSDdataset
Dtest
Wedemonstrateinourexperimentsthat
sentence-level privacy is weaker than
document-levelprivacy. However,group
privacycanbeusedtoachievedocument-
level privacy from sentence-level pri-
vacy.
(d) Adjacency definition for “neighboring”
datasets: Weusetheadd-or-removead-
jacencydefinition.
3. (a) Typeofaccountingused: RDP-basedac-
counting.
(b) Accounting assumptions: We correctly
usePoissonsampling.
(c) The formal DP statement: We
use various levels of ε val-
ues: 1,10,40,99,400,990. Our δ
issetto10−8.
(d) Transparency and verifiability: We are
going to open source our code based
on the open-source DP-NMT frame-
work(Igamberdievetal.,2024).