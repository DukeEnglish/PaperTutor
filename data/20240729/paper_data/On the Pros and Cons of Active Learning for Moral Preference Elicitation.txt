On the Pros and Cons of Active Learning for Moral
Preference Elicitation
VijayKeswani1,VincentConitzer∗2,3,HodaHeidari∗2,JanaSchaichBorg∗1,andWalter
Sinnott-Armstrong∗1
1DukeUniversity
2CarnegieMellonUniversity
3UniversityofOxford
Abstract
Computationalpreferenceelicitationmethodsaretoolsusedtolearnpeople’spreferences
quantitatively in a given context. Recent works on preference elicitation advocate for active
learningasanefficientmethodtoiterativelyconstructqueries(framedascomparisonsbetween
context-specific cases) that are likely to be most informative about an agent’s underlying pref-
erences. Inthiswork,wearguethattheuseofactivelearningformoralpreferenceelicitation
relies on certain assumptions about the underlying moral preferences, which can be violated
in practice. Specifically, we highlight the following common assumptions (a) preferences are
stable over time and not sensitive to the sequence of presented queries, (b) the appropriate
hypothesis class is chosen to model moral preferences, and (c) noise in the agent’s responses
is limited. While these assumptions can be appropriate for preference elicitation in certain
domains,priorresearchonmoralpsychologysuggeststheymaynotbevalidformoraljudg-
ments. Throughasyntheticsimulationofpreferencesthatviolatetheaboveassumptions,we
observethatactivelearningcanhavesimilarorworseperformancethanabasicrandomquery
selectionmethodincertainsettings. Yet,simulationresultsalsodemonstratethatactivelearn-
ingcanstillbeviableifthedegreeofinstabilityornoiseisrelativelysmallandwhentheagent’s
preferencescanbeapproximatelyrepresentedwiththehypothesisclassusedforlearning. Our
studyhighlightsthenuancesassociatedwitheffectivemoralpreferenceelicitationinpractice
andadvocatesforthecautioususeofactivelearningasamethodologytolearnmoralprefer-
ences.
1 Introduction
Ensuringproperdeploymentofartificialintelligence(AI)systemsinhigh-stakessocietaldomains
requires building trust in the decisions of these systems. To that end, recent work on ethical
andparticipatoryalgorithmicdevelopmentemphasizestheimportanceofencodingstakeholders’
valuesinthesesystems,especiallytheirmoraljudgments/preferencesoveractionsthatcancause
significantharmtoothersFefferetal.(2023). Incorporatingstakeholders’moralpreferencesallows
forthecreationoftoolswhosejudgmentsarenormativelyalignedwiththoseofthestakeholders
and helps counter various harms associated with the use of computational tools. To accomplish
thisgoal,however,onefirstneedstoaccuratelyelicitpeople’smoralpreferences.
*Theseauthorscontributedequallytothisworkandareorderedalphabetically.
1
4202
luJ
62
]CH.sc[
1v98881.7042:viXraFigure 1: Example of a pairwise comparison from the Boestler et al. (2024) study on kidney allo-
cationdecisions.
Studies on moral preference elicitation often present agents with pairs of context-specific cases
andaskthemtochoosetheonetheyprefer. Usingtheagent’sresponsesforasetofsuchpairwise
comparisons, one can try to learn a representation of their underlying preferences. To formalize
this setting, let X ⊆ Rd denote the space of all cases over which any agent has preferences, with
d ∈ Z + denotingthenumberoffeaturesdescribingeachcase. Followingthestandardpreference
elicitationliterature, supposethat anagent’s preferencesaredetermined bycomparing thevalue
ofanunderlyingutilityfunctionu : X → R acrosscasesFreedmanetal.(2020). Foranyinputpair
x,x′ ∈ X×X, the agent prefers x over x′ iff u(x) > u(x′). Let R : X×X → {0,1} denote their
responsefunction,with R(x,x′) = 1(u(x) > u(x′)),where1(·)istheindicatorfunction.
Multiple recent studies employ this framework for moral preference elicitation. For example,
Boestleretal.(2024)modellay-agent’smoralpreferencesinkidneyallocation. Theyprovidepar-
ticipantswithprofilesoftwopatientswhoneedkidneytransplantsandaskthemtodecidewhich
patient should receive the one available kidney. Each patient profile contains features like the
patient’s number of children, years of life they will gain from the transplant, etc. The choice
between the two patients can pose a moral dilemma when different features favor different pa-
tientsSinnott-Armstrongetal.(2021)(Figure1presentsapairwisecomparisonscenariofromthis
study). Another well-known example of this approach is the “Moral Machines” study, in which
participantsarepresentedwithsacrificialmoraldilemmasandaskedwhatanautonomousvehicle
should do in each case Awad et al. (2018); Noothigattu et al. (2018). In another study, Srivastava
et al. (2019) elicit fairness preferences by presenting participants with pairwise comparisons of al-
gorithmic predictions and backing out the notion of fairness that is most compatible with their
responses. Preference elicitation has similarly been part of the development pipeline of various
participatorycomputationalframeworksLeeetal.(2019);Kahngetal.(2019);Loreggiaetal.(2019);
Fefferetal.(2023).
Thegoalofpreferenceelicitationinthesesettingsistoaccuratelyandefficientlylearnarepresen-
tation of the agent’s underlying utility u(·) using their responses for a given set of N pairwise
comparisons, i.e., using {(x ,x′,R(x ,x′))}N . Here, accuracy refers to the ability to (a) recover
t t t t t=1
the utility function u and/or (b) offer an approximate representation of u that mimics decisions
madethroughuinalargenumberofcomparisons. Achievingaccuracyoftenrequirespresenting
an agent with numerous pairwise comparisons, which can be onerous and expensive. To reduce
the number of queries required to obtain a desired level of accuracy, active learning is frequently
2invokedasanalternativeapproach.
Active learning methods operate in the realm of scarce outcome-labeled data, where one has the
option to interactively query an oracle (the user/agent in this case) for labels, and the goal is to
learntherelationshipbetweenlabelsandrelevantfeaturesusingasfewqueriesaspossibleSettles
(2009). These methods can help improve the efficiency of preference elicitation as well. For pref-
erence elicitation, active learning techniques can suggest new pairwise comparisons that would
providethemaximalinformationabouttheagent’sutilityfunctiongiventheinformationgathered
sofarDragoneetal.(2018). Usingthisformofstructureddeterminationofthenextpairwisecom-
parison (based on the agent’s previous responses), the agent’s preferences can be inferred faster
than the setting where they are presented with random comparisons at each time step. For this
reason, multiple recent works consider active-learning-based preference elicitation. Yang et al.
(2021) use interactive elicitation to create recommendation systems. Srivastava et al. (2019) de-
velopactive-learning-basedsurveystoelicitfairnesspreferences. Johnstonetal. (2023)useactive
learning to learn preferences regarding healthcare resource allocation. These recent use cases of
active learning provide evidence of its ability to efficiently elicit people’s preferences. However,
theeffectivenessofactivelearningreliesoncertainassumptionsthatmaynotholdinthecase
ofmoralpreferences.
Moralpreferencescaptureaperson’snormativeviewsoveravailableactionsinmoraldilemmas—
thatis,whatistherightthingtodowhenthechosenactioncouldleadtosignificantharmtoothers,
but not (or not only) to the participant themself? A popular example is the trolley problem, where
theparticipantisaskedwhichhumanlivesshouldbeprioritized,passengersorpedestriansFoot
(1967). Similarly,inthekidneyallocationexampledescribedearlier,whenaskedtodecidewhich
of two patients on the kidney transplant list should get the kidney, a participant’s decisions are
based on patient features that they consider morally relevant. In these settings, when an agent
expresses a preference for one patient over another, their judgment can be characterized by the
underlying utility function they use to assign relevance scores to the available actions, choosing
the action with the highest assigned score. Note that, despite the use of utility functions, this
standardsetupdoesnotpresupposeanyutilitarianmoraltheorybecauseitcanmodelagentswho
base their decisions on non-utilitarian factors, such as past misbehaviors by patients. Modeling
theparticipant’spreferencesinmoraldecision-makingsettings(e.g.,bylearningtheirunderlying
utilityfunction)allowsforpredictingtheirmoraljudgmentswhenpresentedwithnewdilemmas
in the same setting. Therefore, these models can be useful in the development of ethical AI tools
Feffer et al. (2023).1 However, eliciting moral preferences can be challenging, and differ from the
processofelicitingotherkindsofpreferences.
Moralpreferencesconcernharmstoothers,anddifferfromself-interested,economic,ormaterial
preferences,wheretheagentchoosestheoptionwiththehighestsubjectiveutilitytoselfCapraro
and Perc (2021). Instead of being concerned only with the self, moral preferences are intended
to be impartial Vanberg (2008) and fair Bicchieri and Chavez (2010). Computational modeling
ofthesepreferencescan,therefore,helpdevelopdecision-aidtoolsthatincorporatestakeholders’
moral values, e.g., in applications like autonomous vehicles or biomedical situations. Unsurpris-
ingly, the standards of expected elicitation accuracy in these domains are quite high, since inac-
curate prediction of moral judgments can significantly harm the people using or affected by the
1Anoteonterminology:whatwecallmoralpreferencescanalsobedescribedasjudgments/orderingsoveravailable
actionsinmoraldilemmas. Thischaracterizationisdifferentthandecisiontheoryliterature,whichdefinespreference
orderingsoveroutcomesratherthanactionsArrowetal.(1996).Yet,weusethetermpreferencestobeconsistentwith
CSpreferenceelicitationliteratureonmodelingdecisionprocessesinpairwisecomparisonsettings.
3decision-aid tool. For these reasons, greater attention to elicitation performance is required in
moraldecision-makingsettings.
Yet, a crucial problem in eliciting moral preferences is that they can be unstable, i.e., the partic-
ipant’s choices for the first few presented moral dilemmas might appear inconsistent with each
other Crockett (2016). The participant can also be indecisive and provide “noisy” judgments
to moral dilemmas (e.g., there may be variability in their choices for similar scenarios), further
complicating the elicitation process Rehren and Sinnott-Armstrong (2022). Research on moral
psychology also lacks consensus on the structure of cognitive processes that incorporate moral
preferenceswithinourjudgmentsUgazioetal.(2022). Limitedunderstandingofmoraldecision-
making structures makes it difficult to model them computationally. All these properties taken
together make moral preference elicitation a complex task and call into question the validity of
activelearningasareliableelicitationmethodology.
Theuseofactivelearningforpreferenceelicitationoftenpresupposesthatthecontextinquestion
does not suffer from the above issues. Preference stability, limited variability in responses, and
availability of a hypothesis class that captures the underlying utility u are common assumptions
Dragone et al. (2018). An obvious question that then arises is whether active learning still leads to
efficientmoralpreferenceelicitationwhentheseassumptionsareviolated. Researchfrommoralpsychol-
ogy suggests that these assumptions may specifically not hold for moral preferences. Hence, the
efficacyofactivelearningformoralpreferenceelicitationneedsfurtherexamination.
1.1 OurContributions
In this paper, we investigate whether active learning can be effective for moral preference elici-
tation, based on simulations designed to replicate the above challenges with moral preferences.
Oursimulationstesttwopopularactivelearningparadigms,version-space-basedactivelearning
and Bayesian active learning (Section 2). Inspired by recent human subject research on proper-
ties of moral decision-making (Section 3), we consider the following challenges: (a) preference
instability, (b) model misspecification, and (c) noisy responses. In all settings, we compare
active-learning-basedapproachesagainstastandardapproachthatpresentsagentswithrandom
pairwisecomparisons. Weobservethefollowing:
Preference instability. Our simulations here evaluate elicitation performance when the agent’s
moral preference model stabilizes only after responding to a certain number of initial compar-
isons(Section4.1). Specificscenariosweconsiderinclude: (1)theagent,afterafewcomparisons,
simplifies their moral preference to reduce the decision-making effort, (2) the agent makes their
preference more complex to incorporate additional information, and (3) the agent changes their
preferenceentirelytoreflectsignificantupdatestotheirmoralvalues. Weobservethat,inallthree
cases, when the number of features is small, the Bayesian active learning approach recovers well
frominstabilityandachieveshigheraccuracythantherandomquerybaselinewithinasmallnum-
berofcomparisonsafterapreferencechange. However,incasesofdrasticpreferencechangesand
a large number of features, both active learning approaches have similar or worse performance
than the random query baseline due to their dependence on previous comparisons. The key take-
away here is that the accuracy and efficiency of active learning depend on the expected scale of preference
instability(ascapturedbythekindofpreferencechange)andthecomplexityofthedecision-makingcontext
(ascapturedbythenumberoffeatures).
4Modelmisspecification. Ourmodelmisspecificationsimulationevaluatespreferenceelicitation
performance when the agent’s moral decision-making model and the model class used by the
elicitation framework are different (Section 4.2). For instance, suppose the agent uses a shallow
decisiontreetoencodepreferences,butthepreferenceelicitationframeworkusestheclassoflin-
earmodels. Here,activelearningatbestconvergestothebesthypothesisinthelinearclassbuthas
arelativelyhighpredictiveerror–asobservedinoursimulations. Alongwithagentsthatusetree-
basedmodels,wesimulateotherscenariosofmodelmisspecification,suchasscenarioswherethe
agent uses feature interactions but the elicitation model doesn’t, and scenarios where the agent
and the elicitation model use different feature sets. When the extent of model misspecification is
large,weobservethatactivelearningapproachesandrandomquerybaselinehavesimilarperfor-
mance. Thekeytakeawayhereisthatappropriatemodelingoftheagent’smoraldecision-makingprocessis
necessaryforactivelearningtoimprovetheelicitationefficiencyoftheframework.
Noisy responses. We also consider the setting where the agent’s responses are stochastic and
simulatetwokindsofstochasticity: (a)responsenoise: whenstochasticityintheagent’sresponseto
apairwisecomparisondependsonthedifferencebetweenutilityassignedtoeachiteminthepair
(i.e.,highervariabilitywhenutilitiesareclose)and(b)preferencenoise: whentheagent’spreference
model is sampled from a certain distribution. For response noise, we observe that the Bayesian
active learning approach isstill more efficient than the random query baseline despite noise. For
preferencenoise,activelearningismoreefficientthanrandomquerybaselineonlywhennoiseis
small (e.g., when noise magnitude is small relative to the range of model parameter values). The
key takeaway here is that one needs to consider the source and impact of variability in agent responses to
assesstheeffectivenessofactive-learning-basedelicitation.
Overall, our simulations shed light on the performance of active learning for simulated moral
preference elicitation tasks. We find that active learning can improve elicitation efficiency in cer-
tain settings (e.g., small-scale noise) but also reduce elicitation efficiency in other settings (e.g.,
large-scale preference instability). Based on these results, we emphasize the need to understand
the nuances associated with the moral decision-making context in question before deploying ac-
tive learning-based elicitation frameworks. Additionally, our findings can inform future human-
subject studies aimed at understanding the extent to which these assumptions are violated in
commonmoralpreferenceelicitationtasks.
1.2 RelatedWork
Preferenceelicitationmethodsareemployedinmultipledomainstocreateuser-centeredservices,
e.g. to create recommendation systems Priyogi (2019), to understand consumer behaviour Ben-
Akivaetal.(2019),andforpatient-centereddecision-makinginhealthcareWeerninketal.(2014).
Researchonpreferenceelicitationsimilarlyspansmultipledisciplines,includingcomputerscience
Chen and Pu (2004), economics Beshears et al. (2008), and psychology Slovic (2020). Machine-
aidedelicitationhasfurtherimprovedlearningefficiencybyhelpingprocessavailableagentdata
and/or the choices they make in real and hypothetical scenarios Soekhai et al. (2019). As men-
tioned earlier, similar efforts have been made in moral domains, with several applications em-
ploying elicitation frameworks to model moral preferences Awad et al. (2018); Srivastava et al.
(2019); Loreggia et al. (2019); Balakrishnan et al. (2019); Sinnott-Armstrong et al. (2021); Johnston
et al. (2023). For a general survey of moral preference elicitation methods, we recommend Fef-
fer et al. (2023). In our work, we focus on methods that query an agent to choose between two
5givencasesandusetheirresponsestolearntheirpreferencesBen-Akivaetal.(2019). Whilepair-
wise comparisons are a popular elicitation technique, there are alternative approaches as well,
e.g., asking agents to report their preference strength Toubia et al. (2003), rank choices Ali and
Ronaldson (2012), participate in bidding processes Conen and Sandholm (2001), or describe the
motivationsfortheirchoicesLiscioetal.(2023,2024).
Active learning can be used to either learn the agent’s utility model or to successively present
them with better recommendations Houlsby et al. (2011); Dragone et al. (2018). For the former
settinglearningtheutilitymodel,HuangandLuo(2016)proposeactivelearningmethodstolearn
marketplace consumer preferences and Srivastava et al. (2019) elicit fairness preferences using
active-learning-basedsurveys. Forthelattersettingofgeneratingpersonalizedrecommendations,
Elahi et al. (2014) and Yang et al. (2021) discuss active learning strategies to streamline data col-
lectionforrecommendationsystems. Johnstonetal. (2023)useuncertainty-basedactivelearning
methodsproposedbyVayanosetal. (2020)tomodelhealthcareresourceallocationpreferencesof
survey participants. Our work focuses on learning the utility model since the eventual goal is to
usethelearnedutilityandpreferencesfordownstreamapplications.
Mostpreferenceelicitationstudiesfocusonpreferencesinvolvingself-benefits, e.g., tocreaterec-
ommendationsystemsorbetter-personalizedservices. Asmentionedearlier,moralpreferencesgo
beyond self-interest and explain people’s normative impartial judgments. For instance, Bicchieri
andChavez(2010)showtheinsufficiencyofmonetarypreferencesinexplainingpeople’sfairness
perceptions. Capraro and Rand (2018) discuss how social preference models can be incompati-
blewithpeople’schoicesofequitableactions. Otherexperimentalanalysesfrompsychology(see
Capraro and Perc (2021) for a review) provide further evidence of contrasts between moral and
materialpreferences.
Beyond our work, certain recent papers examine the limitations of active learning in different
contexts. Margatina and Aletras (2023) and Kottke et al. (2019) discuss the dependence of active
learning’s performance on common (but potentially unrealistic) assumptions, e.g. representative
trainingdataandequallabelingcostsacrosscases. Activelearningcanalsofailtooutperformran-
dom query baselines when faced with distribution shifts Snijders et al. (2023) or outliers Karam-
cheti et al. (2021). Data collected using active learning is implicitly tied to the learning model
and can lead to generalization issues Lowell et al. (2019). Our work adds to this line of research,
specificallyquestioningtheapplicabilityofactivelearningtomoralpreferenceelicitation.
2 Algorithms for Preference Elicitation
The basic structure of the elicitation procedure is described in Algorithm 1. At time-step t, the
agent is presented with a sampled comparison (x ,x′) and their response is recorded. Then, the
t t
algorithm finds the hypothesis h from class H which best fits the labeled comparisons recorded
t
tilltimet.
The sampling step of Algorithm 1 (Step 3) can be executed either by randomly sampling a pair
of input cases or by using active learning, whereby the chosen pair depends on the comparisons
presentedsofarandthehypothesisclass H. Wewilluse RANDOM-PE torefertotheinstanceof
Algorithm1thatusesrandomsampling. Whenusingactivelearningtosamplecomparisons,mul-
tiplemethodsfrompriorworkscanbeemployedandweoutlinetwopopularapproachesbelow.
LongermathematicaldescriptionsandusecasesofthesemethodsareprovidedinAppendixA.
6Algorithm1Onlinepreferenceelicitation
Input: Functionssample(·), R(·,·),andfit(·,·), N,classH
1: S ← ∅
2: fort ∈ {1,...,N}do
3: x t,x′
t
← sample(S){samplenewcomparison}
4: r t ← R(x t,x′ t){Getagent’sresponse}
5: S ← S∪{(x t,x′ t,r t)}
6: h t ← fit(S,H){LearnhypothesisusingdatasetS}
7: returnh N
Version-Space-based Active Learning. Given a kernel-SVM decision boundary learned using
available labeled data, the informativeness of any new query can be approximated using the
distance of the query from the decision boundary and this heuristic can be used to generate an
informative next query Tong and Koller (2001). To implement this approach, we learn an SVM
hypothesis f that best fits ((x ,x′))t to labels (r )t and find a comparison that is closest to f’s
i i i=1 i i=1
decisionboundary. WewillcallthisapproachACTIVE-VS-PE.
BayesianActiveLearning. TheBayesianActiveLearningwithDisagreement(BALD)algorithm
representspreferencesusingaGaussianprocesswithaspecifiedkernelandchoosesthenextquery
tobetheonethatmaximizesthemutualinformationbetweenmodelpredictionsandmodelpos-
terior Houlsbyet al. (2011). Implementation of thisapproach for Algorithm1 requireslearning a
representation of the posterior corresponding to the labeled dataset ((x ,x′,r ))t and then find-
i i i i=1
ing the pairwise comparison with high mutual information. We will call this approach ACTIVE-
BAYES-PE.Notethattheuseoflearnedposteriorislimitedtothesamplingstepandcanbeinde-
pendentofthelearningstep.
The final step of Algorithm 1 (Step 6) uses a pre-specified function fit(·,·) to learn a hypothesis
h from H that “best” simulates responses (r )t using comparisons ((x ,x′))t . For instance,
t i i=1 i i i=1
if H is the class of linear functions, then fit(·,·) can implement an SVM, logistic regression, or
any other linear classification training procedure (with appropriate regularization). Alternately,
torankcasesbasedontheagent’sresponses(withHdenotingthesetofallrankings),thepopular
Bradley-Terry approach can be implemented within fit(·,·) Bradley (1984). The choice of H here
depends on prior beliefs about the agent’s preference model. However, a mismatch between the
agent’spreferencemodelandHcanimpacttheeffectivenessoftheframework(seeSection4.2).
3 Challenges to Modeling Moral Preferences
Inspiredbypriorresearchfrommoralpsychology,wehighlightthreeobstaclestocomputationally
modelinganagent’smoralpreferences. Theseobstaclesare(a)changeinpreferenceaftermaking
a certain number of decisions, (b) the agent’s model not being included in H, and (c) noise in
theagent’sresponses. Wedescribethesechallengeshere, specificallyfocusingonpriorempirical
evidenceforthemfromhumansubjectresearchinthepairwisecomparisonsetting.
Preferenceinstability. Empiricalstudiesinpsychologyprovideextensiveevidencethatagent’s
preferencesinunfamiliarcontextsaredevelopedastheymakedecisionsinthosecontextsHoeffler
andAriely(1999);ArielyandZakay(2001);Warrenetal.(2011);Dharetal.(1999). Inthesesettings,
7the first few choices made by an agent can be unstable (i.e., their preferences can change after
makingsomedecisions)andmaynotreflecttheireventualpreferencesforfuturedecisions. Moral
preferencescanhavesimilarinstabilityandcanbeshapedbyanagent’songoingexperiencewith
the decision-making context Crockett (2016); Rehren and Sinnott-Armstrong (2022); Helzer et al.
(2017);Curryetal.(2019).
In the context of pairwise comparisons, data from Boestler et al. (2024) provides evidence of this
phenomenoninthekidneyallocationsetting. Intheirstudy, participantsareaskedtoparticipate
in10sessions(oneperday)andpresentedwith60pairwisecomparisonsineachsession. Session-
specific analysis shows that, for many participants, there is significant variation in their weight
distribution over the patient features across different sessions. In other words, for many partici-
pants, their underlying utility functions change from session to session. This kind of preference
changecansignificantlyimpairtheabilitytocomputationallymodelmoralpreferences.
Note that we consider the instability of moral preferences over available actions and not prefer-
ences over moral values (e.g., one’s value preference could be to prioritize equality in resource
allocationoverefficiency). Moralvaluesdoinformmoraljudgmentsandthepreferencesanagent
hasoveravailableactions. Butpriorworkhasarguedthatwhilevaluesaregenerallystable,agents
can still be unstable in applying those values to make moral judgments – this is referred to as the
“value-action gap” Gould et al. (2023). In our setting, since we only observe the agent’s moral
judgments, we mainly focus on the challenge posed by the observed instability of preferences
expressedthroughthesejudgments.
Modelmisspecification. Anotherchallengeinthecomputationalmodelingofmoralpreferences
is model misspecification, i.e., making incorrect/misrepresentative assumptions regarding the
structure of the agent’s decision-making process. A popular modeling assumption is the additive
independencemodel,whereweassumetheutilitytheagentassignstoanyinputcanberepresented
asasumoftheutilitiesassignedtoindividualinputfeaturesChenandPu(2004)(e.g.,linearutil-
ity satisfies this assumption). Another common modeling assumption is the complete information
assumption, i.e., all the information explicitly used by the agent to make their decision is available
totheelicitationframework. Assumptionsofthiskindarecommoninactivelearning-basedelici-
tationastheyreducethecomplexityofquerygenerationYangetal.(2021);Johnstonetal.(2023).
TheyalsoaffectthechoiceofHinAlgorithm1,e.g.,ifweassumeadditiveindependenceandcom-
pleteinformation, thensetting H tobethelinearclasscanhelplearnexplainablerepresentations
oftheagent’spreferences.
However, in many situations, these assumptions do not reflect the agent’s decision-making pro-
cess Pine et al. (2009); Gonzalez Sepulveda et al. (2021). Cognitive processes underlying moral
decision-making are not clearly understood Ugazio et al. (2022) and can be more complex than
a linear combination of available features Hofmann et al. (2008). Both empirical and theoretical
analyses of moral judgments highlight this complexity. Cohen and Ahn (2016) fit multiple kinds
of linear and nonlinear models over people’s responses in moral dilemmas and find that mod-
els from the exponential function family often provide the best fit. Kagan (1988) theoretically
questionsboththeadditiveandindependenceassumption(inanarticleappropriatelytitled“The
Additive Fallacy”), explaining through multiple contrastive examples that (a) moral status of an
act cannot always be determined by the sum of weights of individual features, and (b) weight
assigned to each feature can depend on the weight assigned to other features (i.e., feature inter-
actions). Assuch,non-linearityanddependenceacrossvariousfeaturescanbeexpectedinmoral
decision-makingprocesses.
8Noisy responses. Stochasticity in agent’s choices, (specifically, changes in their responses to
similar scenarios at different times) has been noted in various domains Marley and Regenwet-
ter (2016); Becker et al. (1963). The same is true for moral decision-making domains, where re-
sponsevariabilitycanbearesultofongoingdeliberation,increaseddecision“difficulty”,and/or
increased complexity of the decision context Sivill (2019). Boestler et al. (2024) provide concrete
evidence of this phenomenon. In their kidney allocation study, participants take part in multi-
ple sessions and six pairwise comparisons are repeated in each session. Participants’ responses
to the repeated comparisons provide insight into response variability, quantified by the fraction
of times a participant’s choice to a repeated scenario differed from their majority choice for this
scenario. Boestler et al. (2024) observe significant response variability for certain repeated com-
parisons (in the range of 10-18%). Additionally, the results of Boestler et al. (2024) suggest that
responsevariabilityislargerwhenthepairwisecomparisonisperceivedasbeingmore“difficult”
bytheparticipant,implyingamplifiedstochasticityfordifficultmoraldilemmas.
All of these properties pose significant obstacles to the computational modeling of moral pref-
erences. As we see in the following sections, the impact of these challenges can potentially be
amplifiedbytheuseofactivelearning.
4 Testing the Efficacy of Active Learning
Withtheabovechallengesinmind,wenextcomparetheperformanceofactivelearningforpref-
erenceelicitationagainsttherandombaselineoversimulationsofthesechallenges.
Simulation setup. We primarily simulate agents that use linear utility functions, i.e., u(x) =
w⊤x, forany x ∈ X, givenweights w ∈ Rd. Theassumptionoflinearutilityisquiteprevalentin
thepreferenceelicitationliterature(e.g.,Noothigattuetal. (2018),McElfreshetal. (2021),Johnston
et al. (2023)). In Section 4.2, we will also question this assumption and simulate agents that use
tree-based models and linear models with feature interactions. To simulate an agent with linear
utility, we sample weights w from the uniform distribution Unif([−1,1]d). We run Algorithm 1
for each simulated agent, presenting them with N pairwise comparisons (ranging from 5 to 50).
H is set to be the class of linear SVM classifiers over feature differences (with fit(·) performing
SVM training). Hence, each h will contain the learned SVM weights, say wˆ . We evaluate per-
t ht
formanceusingtwometrics: (i)accuracy-foraheld-outcollectionof1000comparisons,measure
the fraction of comparisons for which the response using weights wˆ matches that of the agent
hN
-and(ii)normalizeddistance-measurethe L -distancebetween wˆ and w afternormalization.
2 hN
For each setup, we report the mean and standard deviation of these metrics across 50 simulated
agents. Inthemainbodyofthepaper,wewillprimarilydiscusstheaccuracymetric. Resultswith
respect to distance are similar but deferred to Appendix C. The number of features d is varied
from{3,...,15}andeachfeaturehasrange{1,...,10}(unlessspecifiedotherwise). ACTIVE-VS-
PE and ACTIVE-BAYES-PE will use a linear kernel function κ. Other implementation details are
presentedinAppendixB.
4.1 PreferenceInstability
The first challenge we discuss in Section 3 is preference instability, i.e., the agent’s underlying
preferences can change after making some decisions. Since the next query suggested by active
learningdependsontheagent’sresponsestocomparisonspresentedsofar,wesimulatescenarios
9Figure2: Performanceforpreference-changescenariosfromSection4.1. ACTIVE-BAYES-PE often
performs better than RANDOM-PE post-t
change
when d=5. However, in many cases (e.g., d=10,
t change=20,30),bothactivelearningalgorithmshavesimilarorworseperformancethanRANDOM-
PE.
where an agent’s preferences undergo changes to assess the impact of preference instability on
activelearningalgorithms.
We assume that the agent’s utility function is linear. Suppose that the agent changes their pref-
erences once, at timestep t ∈ [N]. Let wpre ∈ Rd denote the agent’s weight vector for all
change
timestepst < t andwpost ∈ Rd denotetheagent’sweightvectorforalltimestepst ≥ t .
change change
Wesimulatethefollowingkindsofpreferencechanges.
• Downscale-ordered. Agentchangestheirpreferenceutilityfunctiontoonlyusethefeature
towhichtheyassignedthehighestweightpreviously. Forthisagent,wesamplepre-change
preference wpre ∼ Unif([−1,1]d) and set I = argmax |wpre|. Then, for post-change prefer-
i i
ence,wpost = wpre andwpost = 0foralli ∈ [d]\{I}.
I I i
• Downscale-random. Agentchangestheirutilityfunctiontoagainuseonlyonefeature,but
the feature is randomly selected. For this agent, we sample pre-change preference wpre ∼
Unif([−1,1]d) and set I is chosen randomly from set [d]. Then, for post-change preference,
wpost = wpre andwpost = 0foralli ∈ [d]\{I}.
I I i
• Upscale-ordered: Agent changes preference utility function from using just one feature to
allfeatures,withfeaturesin wpost havinglowerrelativeweightthanthenon-zeroweightin
10wpre. For this agent, sample wpost ∼ Unif([−1,1]d) and I = argmax |wpost|. Then, wpre =
i i I
wpost andwpre = 0foralli∈[d]\{I}.
I i
• Random-switch. Agentchangestoarandomnewpreferenceafter t . Here,wesample
change
bothweightsvectorswpre,wpost ∼ Unif[−1,1]d,independently.
Downscale-ordered and Downscale-random model the settings where an agent changes their
preference to reduce decision-making effort Shah and Oppenheimer (2008). In certain cases, the
agent can choose only to use the feature that was most important to them pre-t , which is
change
modeled by Downscale-ordered. Upscale-ordered is a symmetric scenario where the agent in-
steadincorporatesadditionalfeaturesintheirpreference. Finally,Random-switchmodelsagents
whomakemoredrasticchangestotheirpreference,e.g. followinganentirelydifferentsetofmoral
norms. AppendixC.1modelsmultipleotherscenariosaswell,e.g.,agentdownscaling/upscaling
to or from random features (instead of highest weighted feature) and downscaling/upscaling to
or from more than one feature. For all scenarios, we compare the accuracy achieved by ACTIVE-
VS-PEandACTIVE-BAYES-PEvs.RANDOM-PE,varyingthenumberoffeatures,numberofcom-
parisons,andt .
change
Results. The results of our simulations are presented in Figure 2. As expected, ACTIVE-VS-PE
and ACTIVE-BAYES-PE always achieve higher accuracy than RANDOM-PE prior to t change. Post-
t performanceshowshowwelleachalgorithmrecoversfrompreferencechange.
change
Let us first look at the Downscale-ordered setting (plots on the top-left side of Figure 2). In this
case, when the preference change occurs early (i.e., t
change
= 10), ACTIVE-BAYES-PE recovers
quite fast from the preference change: the accuracy of ACTIVE-BAYES-PE becomes higher than
that of RANDOM-PE within 10 timesteps (on average) post-t
change
when d = 5. In comparison,
ACTIVE-VS-PE takes longer to recover and exceeds RANDOM-PE in accuracy. For larger t change,
both active learning approaches seem to recover slower and incompletely. When d = 10 and
t
change
is20or30,wefurtherobservethat ACTIVE-BAYES-PE and ACTIVE-VS-PE havesimilaror
even lower accuracy than RANDOM-PE for all timesteps post-t change. This also implies reduced
efficiencyofactivelearninginsettingswithhighfeaturecomplexity;foranydesiredlevelofaccu-
racy,activelearningapproachestakeasimilarorlargernumberofcomparisonsthantherandom
query baseline to achieve that accuracy level. In the case of Downscale-random setting, the per-
formanceofactivelearningalgorithms,relativeto RANDOM-PE,followssimilarpatterns–when
d = 5 and t
change
is small, ACTIVE-BAYES-PE recovers well compared to other algorithms, but
this recovery is much slower for d = 10. The drop in accuracy around timestep t is also
change
larger in magnitude for Downscale-random compared to Downscale-ordered; this is expected
sincethereisrelativelymoreconsistencybetweenpre-changeandpost-changepreferencesinthe
Downscale-orderedsetting.
Similar trends are observed for the Upscale-Ordered and Random-switch plots in Figure 2. Ac-
tive learning approaches have the worst recovery in the Random-switch setting where, due to
the drastic change in the agent’s preferences, both ACTIVE-BAYES-PE and ACTIVE-VS-PE have
similar or worse performance than the RANDOM-PE post t
change
when d = 10. On the positive
side,whend = 5, ACTIVE-BAYES-PE doesachievehigheraccuracythan RANDOM-PE within20
timestepspost-t onaverage.
change
Overall,Bayesianactivelearningapproachescanefficientlyelicitpreferenceswhilehandlingpref-
erencechangeswhenthenumberoffeaturesdissmall. However,theseapproachesfailtoprovide
similarlyimprovedperformanceasfeaturecomplexityandpreferencechangetimestepincreases.
11These results highlight the importance of knowing the nature and scale of preference instability
beforedeployingactivelearning. Whileactivelearningwilleventuallyrecoverafteralargernum-
ber of timesteps beyond 50, we see that in the timesteps following t , it can perform even
change
worse than the random baseline due to its dependence on the agent’s previous responses. Con-
sideringthatactivelearningisusuallyemployedwhenonehastobeeconomicalwiththenumber
ofpresentedcomparisons(duetotimeand/orcostconstraints),notbeingabletorelyonacertain
numberofinitialresponsescansignificantlyaffecttheaccuracyofthelearnedpreferencesandfail
toimprove,orevenharm,theefficiencyoftheframework.
4.2 Modelmisspecification
The second challenge we discussed in Section 3 is model misspecification, specifically question-
ing the additive independence and complete information assumptions for the agent’s moral decision-
making process. In this section, we evaluate active learning when additive independence and
complete information assumptions are not satisfied. Setting H to be the linear class, we simulate
thefollowingscenarios.
• Agentusestree-basedutility. Wesimulateagentsthatuseshallowbinarydecisiontreesto
assignutility. Tree-basedmodelsreflectdecisionsmadeusingif-thenrules; e.g., inanorgan
allocation setting, an agent might assign a higher utility to a patient if their age is >50 but
can be indifferent to the exact age number. To maintain parity between capacity of a tree
model and models in H, we simulate agents with tree models of depth ⌊logd⌋, where d is
thenumberoffeatures. Wesimulatethisscenariowithbinaryandnon-binaryfeatures.
• Agent uses second-order interaction terms. Even with a linear utility model, the agent’s
utilityfunctioncoulduseinteractionsbetweendifferentfeatures. Interactiontermsaccount
for scenarios where the importance an agent might assign to any feature is correlated with
thevalueofanotherfeature. Forexample,intheorganallocationsetting,anagentmightas-
signahigherweighttoapatient’snumberofdependentsifthepatientisyoung,implyingan
interactionbetweentheageandnumberofdependentsvariables. Wesimulatethisscenario
by measuring performance across a varying numberof features d and a varying number of
second-orderinteractions.
• Missing features. Finally, we consider the scenario where the agent uses information un-
available to the elicitation framework. We simulate this scenario by allowing the agent to
use a larger feature set than that available for elicitation. Our simulations assess perfor-
manceacrossavaryingnumberoftotalandmissingfeatures.
Results. The results for these simulations are presented in Figure 3. When the agent uses tree-
based preference, ACTIVE-BAYES-PE has marginally better accuracy than the RANDOM-PE after
30comparisonswhendislarge. Forsmalld,bothactivelearningapproachestendtohavesimilar
orworseaccuracythantherandombaseline. Theimpactofmodelmisspecificationalsodepends
ontheinputdomain–overallaccuracyislowerfornon-binaryfeatures.
When the agent uses interaction terms, Figure 3 shows that accuracy decreases as the number
of interaction terms increases. However, when the number of interaction terms is much smaller
than d, ACTIVE-BAYES-PE and ACTIVE-VS-PE can achieve higher accuracy than RANDOM-PE
after 30 comparisons. Finally, in the case of missing features, the larger the number of missing
features (relative to d), the lower the accuracy, and the smaller the gap between ACTIVE-BAYES-
PE, ACTIVE-VS-PE, and RANDOM-PE after 30 comparisons. Missing information reduces the
12Figure 3: Performance for model misspecfication scenarios from Section 4.2. Active learning is
moreeffectivewhentheextentofmodelmisspecificationissmallinscale.
capacityoftheframeworktocapturetheagent’sdecision-makingprocess,leadingtoanaccuracy
drop.
Forthesescenarios,weseethatthelargerthescaleofdisparitybetweentheagent’sutilityfunction
and H, the worse the performance of active learning as compared to the random query baseline.
ActivelearningmightstillconvergetothebesthypothesisinH (seeaccuracyvs.timestepresults
in Appendix C.2); however, the above results show that disparity between functions in H and
theagent’sutilityaffectsactivelearning’sabilityingeneratinginformativequeriesandleadstoa
reductioninaccuracyoflearnedpreferences.
4.3 NoisyResponses
The final challenge we highlighted in Section 3 is stochasticity or variability in agent’s responses
tomoraldilemmas. Twowaysinwhichthisstochasticityhasbeenmodeledinpriorliteratureare
(a) response noise: noise that arises and affects the agent’s response after the agent has computed
utility for the presented cases, and (b) preference noise: noise that arises due to variability in the
13Figure4: PerformanceforthenoisemodelsfromSection4.3. ACTIVE-BAYES-PEperformsbetter
than the random query baseline even with response noise. However, it fails to provide a similar
improvementinmostscenariosofpreferencenoise.
agent’s underlying utility function Bhatia and Loomes (2017); Marley and Regenwetter (2016).
Suppose the agent uses linear utility, i.e., u(x) = w⊤x, for some w∈Rd. Then, the above noise
modelscansimulatedasfollows.
• Response noise model. This model induces noise ε ∼ N(0,σ2) after utility is computed.
Assuming an additive noise model, the impact of this noise on the agent’s response R can
beinterpretedaschangingitto R(x,x′) = 1[u(x)−u(x′)+ε > 0].Oursimulationsevaluate
performanceforvaryingσ ∈ R .
• Preference noise model. This model assumes noise in the utility generation process itself.
Wesimulatethissettingasfollows: Supposethatwheneverpresentedwithapairwisecom-
parison, the agent first samples w ∼ N(w⋆,σ2I/d), and then uses the sampled w to com-
puteutilities. Here,w⋆ ∈ Rdrepresentssummaryfeatureweightsassignedbytheagentand
σ ∈ R isthenoiseparametervariedinoursimulations.
Results. The results for this simulation are presented in Figure 4. As expected, increasing σ
leadstoadecreaseinaccuracyofallalgorithms. However,inthecaseofresponsenoise,ACTIVE-
BAYES-PEhashigheraccuracythantheRANDOM-PEbaselineevenforhighvaluesofσ. Accuracy
vs number of comparisons plot for σ = 2 further shows that ACTIVE-BAYES-PE starts achieving
higher accuracy than RANDOM-PE with as few as 20 comparisons. Performance of ACTIVE-VS-
PE,ontheotherhand,isrelativelybetterthanRANDOM-PEforsmallσvaluesbutbecomessimilar
tothatofRANDOM-PEforlargeσ. Hence,inthiscase,activelearning(especially,ACTIVE-BAYES-
PE)canberelativelymoreaccurateatpreferenceelicitationdespitenoise.
In the preference noise setting, both active learning approaches have similar performance as the
RANDOM baseline for almost all non-zero σ values. Variation with respect to σ and number of
comparisons shows that noise in preference weights significantly affects the ability of all algo-
rithmstolearntheunderlyingpreferenceswhenσ>1. Hence,hereactivelearningfailstoprovide
anyperformanceimprovementincomparisontotherandomquerygenerationbaseline.
145 Discussion, Limitations, and Future Work
Throughthepresentedsimulations,wehighlighthowpotentialissuesassociatedwithmoralpref-
erences, such as preference instability, response variability, or modeling errors, can impact the
efficacyofactive-learning-basedpreferenceelicitation. Inallsimulatedscenarios,wecomparethe
performance of active learning-based preference elicitation against the baseline method of using
random queries at each time step. Overall, there are positive scenarios where active learning still
performsbetterthantherandomquerybaseline–e.g.,whennoiseaffectsutilitybutnottheunder-
lyingpreferences,orinthecaseofsmall-scalepreferenceinstabilityininitialiterations. Then,there
are neutral scenarios where the simulated challenge impacts the efficiency of all algorithms simi-
larly and the performance of active learning and the random baseline are comparable – e.g., for
large-scale modeling errors or when the agent’s underlying preferences are noisy. In these cases,
using active learning does not provide any added benefit but it also does not cause any harm to
the elicitation framework. Finally, there are negative scenarios, where using active learning is less
effective than the random baseline – e.g., when the number of features is large and the agent’s
preference changes after they have responded to a large number of comparisons. Here, since ac-
tive learning uses the agent’s previous responses to construct the next query, it takes longer to
recoverfrompreferencechanges.
Differentreal-worldchallengeshavedifferenteffectsonactivelearningforpreferenceelicitation.
Deployingtheseframeworkswithoutpriorunderstandingoftheagent’sdecision-makingforthe
givencontextcanleadtoinaccuraterepresentationsoftheirpreferences. Whileusingasmallnum-
ber of queries will almost always provide only an approximate representation of the underlying
preferences,oursimulationscallattentiontothesourcesofinaccuracythatwereunappreciatedin
previousworksandcouldleadtoincorrectinterpretationsofresultsifnotconsideredinpractice.
In the paragraphs below, we highlight other characteristics of our assessment as well as future
workonthistopic.
Algorithmicsolutions. Oneresponsetothechallengeswesimulateisthatmanyofthemcanbe
addressed algorithmically if they are known in advance. If an agent’s preferences are known to
beunstableforinitialcomparisons,thenonecan,say,modifytheelicitationapproachtodisregard
a certain number of initial comparisons or assign sample weights to each case that are inversely
proportional to the duration since the case was observed by the agent. This way, active learning
canconstructqueriesthatareprimarilybasedonthemostrecentagentresponses. Toaccountfor
feature interactions, the models in H can allow interactions by default and use regularization to
ruleoutscenarioswhereinteractionsarenotused. Priorworkonactivelearningmethodsthatare
robust to noise or distribution shifts can be potentially adapted to make elicitation more resilient
tonoiseormodelingerrorsAngluinandLaird(1988);Zhaoetal.(2021). Insimulations,Bayesian
approaches often appear more robust to certain challenges, e.g., small-scale instability. Hence,
one approach is to use ACTIVE-BAYES-PE with an expanded hypothesis class H (e.g., combining
linearandtreeclasses)tocounterissuesofmodelmisspecification. Themainchallengehereiscre-
atinganefficientquery-selectionalgorithmoveranexpanded H whilebeingrobusttoinstability
and noise, and can be explored as part of future work. Most of these modifications, however, re-
quirepriorknowledgeofthenatureofthechallengeassociatedwiththeagent’sdecision-making
process. Indeed, the primary goal of our analysis is to highlight that certain assumptions made
whenusingactivelearningincorrectlyruleoutthesechallenges. Knowingthattheseassumptions
mightbeviolatedcanhelppractitionersdevelopmodificationsthatmightbebettersuitedforthe
15given context. Also, some active learning algorithms may be generally more robust to violated
assumptionsthanothers.
Sensitivityofmoralpreferences. Asdiscussed,moralpreferencescanbedifferentfromgeneric
preferences for self-benefit and, hence, assuming moral preferences to have a similar structure
as other preferences will hurt the accuracy of the elicitation framework. Based on prior insights
fromtheliteratureonmoralpreferences,ourworkdiscussesspecificmechanismsviawhichthese
inaccuracies can occur. With the highlighted challenges and considering the emergent nature of
moral psychology research, the task of eliciting moral preferences can be tricky. Nevertheless,
buildingelicitationmethodsspecificallyformoralpreferencesisaworthwhiledirectionforfuture
research,giventheirroleincreatingethicalAItools. Atthesametime,moralpreferenceelicitation
is just one (albeit complex) part of ethical AI development. Mechanisms to incorporate learned
moral preferences within AI systems involve additional work and should be similarly subjected
totechnicalanalysesoffeasibilityundervariousreal-worldchallenges.
On utility functions. Our framework employs utility functions to model people’s preferences
over actions in moral dilemmas, as is standard practice in this literature. Despite the overlap in
namingconventions,itisimportanttoclarifythatmodelingmoralpreferencesusingutilityfunc-
tions does not presuppose a reliance on utilitarian or consequentialist moral theories (as long as
consequentialism isn’t used generically to cover all possible theories Portmore (2022)). The justi-
ficationspeoplehaveforconsideringfeaturesthatcontributetotheirutilityfunctiondonothave
to draw on consequentialist principles, and the features people consider may not impact future
consequences directly, such as when people think patients’ past criminal behavior is important
fordeterminingwhoshouldreceiveanavailablekidney. Adherencetomanydifferentmoralthe-
ories (including non-consequentialist theories) can be modeled using utility functions, and our
analysis aims to call attention to challenges that can arise when using active learning to obtain
accurate representations of various utility functions. Nevertheless, future work is needed to as-
sess the effectiveness and challenges of using active learning to predict moral judgments under
othermodelingframeworksorconditions,e.g.,whenusingexplicitmoralconstraintsBlack(2020),
harm-basedutilitiesBeckersetal.(2022),ormodifiedutility-basedframeworksthatexplicitlyac-
countfordeontologicalvaluesLazar(2017).
Onnon-moralpreferences. Issuesofinstability,noise,ormodelmisspecificationcanarisewith
non-moralpreferencesaswell. Yet,wefocusonmoralpreferencesbecausethespecificchallenges
we simulate are inspired by the literature on moral philosophy and psychology. AI applications
thatwouldrelyonmoralpreferenceelicitationofteninvolvehighstakesanderrorsinpreference
elicitationcancauseundueharmtousersandimpactedindividuals(e.g.,inautonomousvehicles
andkidneyallocationsettings),requiringhighlevelsofelicitationaccuracyandreliability.
Otheranalyses/baselines. Futureassessmentsofactivelearningcanalsosimulateviolationsof
multiple assumptions; e.g., the presence of both preference instability and model errors. These
combinationscanbereflectiveofmorecomplexdecision-makingsettings. Additionally,inappli-
cations where data from past agents is available, other baselines (beyond simple random query
baseline)canbeconsidered. Forinstance,onecouldcreateelicitationusingacuratedsetofqueries
thatwere informativeof thepreferencesof pastagents. All orrandom subsets ofthis curatedset
canbeusedtoelicitpreferences. Evaluationofactivelearningagainstsuchbaselinescanprovide
insightintowhetheritisbetterthanmethodsthatusepriorinformation.
16Limitations of our analysis. Our simulations demonstrate the need for improved modeling of
human moral preferences and developing active learning approaches that are more robust to the
simulated challenges. Along with this direction for future work, additional analyses can be con-
ducted to further discover other failure points of quantitative preference elicitation frameworks.
Note that all of our analysis simulates agents with linear or tree-based utility functions. Human
moralpreferencescanbemorecomplexandanalyzingactivelearningperformancethroughreal-
worlddatacanprovidemorerobustresults. Inparticular,thiswillrequirehuman-subjectstudies
whereparticipantsrespondtocomparisonsgeneratedusingactivelearningandrandomcompar-
isons. Asexpected,collectingthisdatawillbeexpensiveandtime-consuming. Inthatregard,our
simulationprovidesastartingpointonthekindofdatathatcanbegatheredusingactivelearning
andraiseschallengesthatneedtobeaccountedforwhenanalyzingthisdata.
6 Conclusion
The results of our simulations highlight the challenges associated with extracting accurate repre-
sentations of agents’ moral preferences while using as few queries as possible. In cases of large-
scale instability or noise in agent preferences or responses, active learning has similar or worse
performance than the random baseline. The assumptions made by the elicitation framework re-
garding the agent’s moral preferences also impact the effectiveness of active learning. The use
of active learning for moral preference elicitation therefore requires careful evaluation of mod-
ellingassumptionsandthescaleofexpectedvariabilityinagentpreferencesandresponsesforthe
relevant context. If large-scale instability, noise, and/or violation of modeling assumptions are
expected, then appropriate alternatives or modifications to active learning should be considered
tocountersuchissues.
Acknowledgements
WearegratefulforfinancialsupportfromOpenAI&Duke.
17References
S. Ali and S. Ronaldson. Ordinal preference elicitation methods in health economics and health
services research: using discrete choice experiments and ranking methods. British medical bul-
letin,103(1):21–44,2012.
D.AngluinandP.Laird. Learningfromnoisyexamples. Machinelearning,2:343–370,1988.
D.ArielyandD.Zakay. Atimelyaccountoftheroleofdurationindecisionmaking. Actapsycho-
logica,108(2):187–207,2001.
K. J. Arrow, E. Colombatto, M. Perlman, and C. Schmidt. The Rational Foundation of Economic
Behaviour. MacmillanPress,1996.
E. Awad, S. Dsouza, R. Kim, J. Schulz, J. Henrich, A. Shariff, J.-F. Bonnefon, and I. Rahwan. The
moralmachineexperiment. Nature,563(7729):59–64,2018.
A.Balakrishnan,D.Bouneffouf,N.Mattei,andF.Rossi.Usingmulti-armedbanditstolearnethical
prioritiesforonlineaisystems. IBMJournalofResearchandDevelopment,63(4/5):1–1,2019.
G. M. Becker, M. H. DeGroot, and J. Marschak. Stochastic models of choice behavior. Behavioral
science,8(1):41–55,1963.
S.Beckers,H.Chockler,andJ.Halpern. Acausalanalysisofharm. AdvancesinNeuralInformation
ProcessingSystems,35:2365–2376,2022.
M.Ben-Akiva,D.McFadden,andK.Train.Foundationsofstatedpreferenceelicitation: Consumer
behaviorandchoice-basedconjointanalysis. FoundationsandTrendsinEconometrics,2019.
J. Beshears, J. J. Choi, D. Laibson, and B. C. Madrian. How are preferences revealed? Journal of
publiceconomics,2008.
S. Bhatia and G. Loomes. Noisy preferences in risky choice: A cautionary note. Psychological
review,124(5):678,2017.
C.BicchieriandA.Chavez. Behavingasexpected: Publicinformationandfairnessnorms. Journal
ofBehavioralDecisionMaking,23(2):161–178,2010.
D.Black. Absoluteprohibitionsunderrisk. Philosopher’sImprint,20(20),2020.
K. Boestler, V. Keswani, J. Schaich Borg, H. Heidari, V. Conitzer, and W. Sinnott-Armstrong. On
thestabilityofmoralpreferences: Aproblemwithcomputationalelicitationmethods. Toappear
inAIES2024,2024.
R.A.Bradley. 14pairedcomparisons: Somebasicproceduresandexamples. Handbookofstatistics,
4:299–326,1984.
V.CapraroandM.Perc.Mathematicalfoundationsofmoralpreferences.JournaloftheRoyalSociety
interface,18(175):20200880,2021.
V.CapraroandD.G.Rand. Dotherightthing: Experimentalevidencethatpreferencesformoral
behavior,ratherthanequityorefficiencyperse,drivehumanprosociality.JudgmentandDecision
Making,13(1):99–111,2018.
18L.ChenandP.Pu. Surveyofpreferenceelicitationmethods. Technicalreport,2004.
D.J.CohenandM.Ahn. Asubjectiveutilitariantheoryofmoraljudgment. JournalofExperimental
Psychology: General,145(10):1359,2016.
W. Conen and T. Sandholm. Preference elicitation in combinatorial auctions. InProceedings of the
3rdACMConferenceonElectronicCommerce,pages256–259,2001.
M. J. Crockett. Computational modeling of moral decisions. In The social psychology of morality,
pages71–90.Routledge,2016.
O. S. Curry, M. J. Chesters, and C. J. Van Lissa. Mapping morality with a compass: Testing the
theory of ‘morality-as-cooperation’with a new questionnaire. Journal of Research in Personality,
78:106–124,2019.
R.Dhar,S.M.Nowlis,andS.J.Sherman. Comparisoneffectsonpreferenceconstruction. Journal
ofconsumerresearch,26(3),1999.
P.Dragone,S.Teso,andA.Passerini. Constructivepreferenceelicitation. FrontiersinRoboticsand
AI,4:71,2018.
M. Elahi, F. Ricci, and N. Rubens. Active learning strategies for rating elicitation in collaborative
filtering: a system-wide perspective. ACM Transactions on Intelligent Systems and Technology
(TIST),5(1):1–33,2014.
M.Feffer,M.Skirpan,Z.Lipton,andH.Heidari.Frompreferenceelicitationtoparticipatoryml: A
criticalsurvey&guidelinesforfutureresearch. InProceedingsofthe2023AAAI/ACMConference
onAI,Ethics,andSociety,2023.
P.Foot. Theproblemofabortionandthedoctrineofdoubleeffect,volume5. Oxford,1967.
R.Freedman,J.S.Borg,W.Sinnott-Armstrong,J.P.Dickerson,andV.Conitzer. Adaptingakidney
exchangealgorithmtoalignwithhumanvalues. ArtificialIntelligence,283:103261,2020.
J. M. Gonzalez Sepulveda, F. R. Johnson, and D. A. Marshall. Incomplete information and irrel-
evant attributes in stated-preference values for health interventions. Health Economics, 30(11):
2637–2648,2021.
R.K.Gould,T.M.Soares,P.Arias-Are´valo,M.Cantu´-Fernandez,D.Baker,H.N.Eyster,R.Kwon,
L. Prox, J. Rode, A. Suarez, et al. The role of value (s) in theories of human behavior. Current
OpinioninEnvironmentalSustainability,64:101355,2023.
E. G. Helzer, W. Fleeson, R. M. Furr, P. Meindl, and M. Barranti. Once a utilitarian, consistently
a utilitarian? examining principledness in moral judgment via the robustness of individual
differences. Journalofpersonality,85(4):505–517,2017.
S. Hoeffler and D. Ariely. Constructing stable preferences: A look into dimensions of experience
andtheirimpactonpreferencestability. Journalofconsumerpsychology,8(2):113–139,1999.
E. Hofmann, E. Hoelzl, and E. Kirchler. A comparison of models describing the impact of moral
decisionmakingoninvestmentdecisions. JournalofBusinessEthics,82:171–187,2008.
N.Houlsby,F.Husza´r,Z.Ghahramani,andM.Lengyel. Bayesianactivelearningforclassification
andpreferencelearning. arXivpreprintarXiv:1112.5745,2011.
19D.HuangandL.Luo. Consumerpreferenceelicitationofcomplexproductsusingfuzzysupport
vectormachineactivelearning. MarketingScience,35(3):445–464,2016.
C. M. Johnston, P. Vossler, S. Blessenohl, and P. Vayanos. Deploying a robust active preference
elicitation algorithm on mturk: Experiment design, interface, and evaluation for covid-19 pa-
tient prioritization. In Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms,
Mechanisms,andOptimization,pages1–10,2023.
S.Kagan. Theadditivefallacy. Ethics,99(1):5–31,1988.
A. Kahng, M. K. Lee, R. Noothigattu, A. Procaccia, and C.-A. Psomas. Statistical foundations
of virtual democracy. In International Conference on Machine Learning, pages 3173–3182. PMLR,
2019.
S. Karamcheti, R. Krishna, L. Fei-Fei, and C. D. Manning. Mind your outliers! investigating the
negative impact of outliers on active learning for visual question answering. In Proceedings of
the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International
JointConferenceonNaturalLanguageProcessing,2021.
D. Kottke, J. Schellinger, D. Huseljic, and B. Sick. Limitations of assessing active learning perfor-
manceatruntime. arXivpreprintarXiv:1901.10338,2019.
J. Kremer, K. Steenstrup Pedersen, and C. Igel. Active learning with support vector machines.
WileyInterdisciplinaryReviews: DataMiningandKnowledgeDiscovery,4(4):313–326,2014.
S.Lazar. Deontologicaldecisiontheoryandagent-centeredoptions. Ethics,127(3):579–609,2017.
M.K.Lee,D.Kusbit,A.Kahng,J.T.Kim,X.Yuan,A.Chan,D.See,R.Noothigattu,S.Lee,A.Pso-
mas, et al. Webuildai: Participatory framework for algorithmic governance. Proceedings of the
ACMonhuman-computerinteraction,3(CSCW):1–35,2019.
E. Liscio, R. Lera-Leri, F. Bistaffa, R. I. Dobbe, C. M. Jonker, M. Lopez-Sanchez, J. A. Rodriguez-
Aguilar,andP.K.Murukannaiah. Inferringvaluesviahybridintelligence. InHHAI2023: Aug-
mentingHumanIntellect,pages373–378.IOSPress,2023.
E. Liscio, L. C. Siebert, C. M. Jonker, and P. K. Murukannaiah. Value preferences estimation and
disambiguationinhybridparticipatorysystems. arXivpreprintarXiv:2402.16751,2024.
A. Loreggia, N. Mattei, F. Rossi, and K. B. Venable. Metric learning for value alignment. In
AISafety@IJCAI,2019.
D. Lowell, Z. C. Lipton, and B. C. Wallace. Practical obstacles to deploying active learning. In
Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9th
InternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages21–30,2019.
K. Margatina and N. Aletras. On the limitations of simulating active learning. arXiv preprint
arXiv:2305.13342,2023.
A. Marley and M. Regenwetter. Choice, preference, and utility: Probabilistic and deterministic
representations. Newhandbookofmathematicalpsychology,1:374–453,2016.
D. McElfresh, L. Chan, K. Doyle, W. Sinnott-Armstrong, V. Conitzer, J. S. Borg, and J. Dickerson.
Indecision modeling. In Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence,
2021.
20R. Noothigattu, S. Gaikwad, E. Awad, S. Dsouza, I. Rahwan, P. Ravikumar, and A. Procaccia.
A voting-based system for ethical decision making. In Proceedings of the AAAI Conference on
ArtificialIntelligence,2018.
A.Pine,B.Seymour,J.P.Roiser,P.Bossaerts,K.J.Friston,H.V.Curran,andR.J.Dolan. Encoding
of marginal utility across time in the human brain. Journal of Neuroscience, 29(30):9575–9581,
2009.
D.W.Portmore. Consequentializing. StanfordEncyclopediaofPhilosophy,2022.
B.Priyogi. Preferenceelicitationstrategyforconversationalrecommendersystem. InProceedings
ofthetwelfthACMinternationalconferenceonwebsearchanddatamining,pages824–825,2019.
P. Rehren and W. Sinnott-Armstrong. How stable are moral judgments? Review of Philosophy and
Psychology,2022.
B.Settles. Activelearningliteraturesurvey. 2009.
A. K. Shah and D. M. Oppenheimer. Heuristics made easy: an effort-reduction framework. Psy-
chologicalbulletin,134(2):207,2008.
W.Sinnott-Armstrong,J.A.Skorburg,etal. Howaicanaidbioethics. JournalofPracticalEthics,9
(1),2021.
T.Sivill. Ethicalandstatisticalconsiderationsinmodelsofmoraljudgments. FrontiersinRobotics
andAI,6:449751,2019.
P.Slovic. Theconstructionofpreference. InShapingentrepreneurshipresearch.Routledge,2020.
A. Snijders, D. Kiela, and K. Margatina. Investigating multi-source active learning for natural
languageinference. InProceedingsofthe17thConferenceoftheEuropeanChapteroftheAssociation
forComputationalLinguistics,pages2179–2201,2023.
V. Soekhai, C. Whichello, B. Levitan, J. Veldwijk, C. A. Pinto, B. Donkers, I. Huys, E. van Over-
beeke,J.Juhaeri,andE.W.deBekker-Grob. Methodsforexploringandelicitingpatientprefer-
encesinthemedicalproductlifecycle: aliteraturereview. Drugdiscoverytoday,24(7):1324–1331,
2019.
M.Srivastava,H.Heidari,andA.Krause. Mathematicalnotionsvs.humanperceptionoffairness:
Adescriptiveapproachtofairnessformachinelearning. InProceedingsofthe25thACMSIGKDD
internationalconferenceonknowledgediscovery&datamining,pages2459–2468,2019.
S.TongandD.Koller. Supportvectormachineactivelearningwithapplicationstotextclassifica-
tion. Journalofmachinelearningresearch,2(Nov):45–66,2001.
O.Toubia,D.I.Simester,J.R.Hauser,andE.Dahan. Fastpolyhedraladaptiveconjointestimation.
MarketingScience,22(3),2003.
G. Ugazio, M. Grueschow, R. Polania, C. Lamm, P. Tobler, and C. Ruff. Neuro-computational
foundationsofmoralpreferences. SocialCognitiveandAffectiveNeuroscience,17(3):253–265,2022.
V.J.Vanberg. Ontheeconomicsofmoralpreferences. AmericanjournalofEconomicsandSociology,
67(4):605–628,2008.
21P. Vayanos, Y. Ye, D. McElfresh, J. Dickerson, and E. Rice. Robust active preference elicitation.
arXivpreprint2003.01899,2020.
C. Warren, A. P. McGraw, and L. Van Boven. Values and preferences: defining preference con-
struction. WileyInterdisciplinaryReviews: CognitiveScience,2(2):193–205,2011.
M. G. Weernink, S. I. Janus, J. A. Van Til, D. W. Raisch, J. G. Van Manen, and M. J. IJzerman. A
systematic review to identify the use of preference elicitation methods in healthcare decision
making. Pharmaceuticalmedicine,28:175–185,2014.
H. Yang, S. Sanner, G. Wu, and J. P. Zhou. Bayesian preference elicitation with keyphrase-item
coembeddings for interactive recommendation. In Proceedings of the 29th ACM Conference on
UserModeling,AdaptationandPersonalization,2021.
E. Zhao, A. Liu, A. Anandkumar, and Y. Yue. Active learning under label shift. In International
ConferenceonArtificialIntelligenceandStatistics.PMLR,2021.
22A Details of Active Learning Algorithms
Inthissection,weprovideadditionaldetailsofthetwoactivelearningalgorithmsthatweevaluate
inthemainbody.
Version Space-based Active Learning – ACTIVE-VS-PE. The first approach relies on kernel
SVM-basedclassification. GivenanSVMdecisionboundarylearnedusingavailablelabeledcom-
parisonsforanagent,theinformativenessofanynewquerycanbequantifiedusingthedistance
ofthequeryfromthedecisionboundaryandthisheuristiccanbeusedtogenerateaninformative
nextqueryfortheagent. Foranygiveninputx ∈ Rd,anSVMclassifiercomputes
f (x) = ⟨w,ϕ(x)⟩,
w
whereϕ : Rd → F isamappingtoakernel-inducedspaceF foragivenkernelκ : Rd×Rd → R ,
such that κ(x,x′) = ⟨ϕ(x),ϕ(x′)⟩, w ∈ F is the weight vector, and the classifier decision is 1 if
f (x) > 0 and −1 otherwise2. Suppose we have a labelled training set S = {(x ,y )} , where
w i i i
x ∈ Rd andy ∈ {1,−1}. TheversionspaceforSdenotesallvectorsw ∈ F thatfitS,i.e.,
i i
V(S) := {w | ||w|| = 1,y· f (x) > 0forall(x,y) ∈ S}.
w
The goal of active learning here is to select an element to add to S which significantly reduces
the size of
V(S).
To that end, Tong and Koller (2001) propose sampling a query that is closest to
thedecisionboundary,i.e,samplinganelementxˆ := argmin |⟨w,ϕ(x)⟩| = argmin |f (x)|.3 To
u u w
implement thisapproach inAlgorithm 1, ateach timestep t, weare given thedataset ((x ,x′))t
i i i=1
tolabels(r )t . Hence,samplingusingtheaboveSVM-basedapproachwouldrequirefirstlearn-
i i=1
ing an SVM hypothesis f that best fits ((x ,x′))t to labels (r )t and then finding the pairwise
i i i=1 i i=1
comparisonargmin (x,x′)|f((x,x′))|.
BayesianActiveLearning–ACTIVE-BAYES-PE. Anotherpopularactivelearning-basedprefer-
ence elicitation approach uses a Bayesian framework for sampling based on information learned
from previous agent responses. The Bayesian Active Learning with Disagreement (BALD) al-
gorithm by Houlsby et al. (2011) models individual preferences using a Gaussian process with
a specified kernel. Based on the Gaussian Process classification literature, suppose each point
x ∈ Rd can be characterized by a latent value, f(x), such that f follows a Gaussian distribution.
That is, f ∼ GP(µ(·),κ(·,·)), where µ denotes the mean function and κ is a pre-defined kernel.
Wecanmodelthelabely ∈ {0,1}atanypointxasfollowingtheBernoulli(Φ(f(x)))distribution,
Φ
where is the Gaussian CDF function. In this setup, suppose the following queries have been
made so far: S : {(x ,y )}t . Then, the BALD approach suggests selecting the next query to be
i i i=1
theonethatmaximizesmutualinformationorthedecreaseinexpectedposteriorentropy,i.e.,
argmaxH(y | x,S)−E [H(y | x, f)].
f∼P[f|S]
x
2Sincewearedealingwithpairwisecomparisons,wewillnotincludeanybiastermintheSVMfunctionalform.
3AlternativeSVM-basedsamplingheuristicsthatcanhavebettertheoreticalandempiricalreal-worldperformance
havealsobeenproposedinotherworksKremeretal.(2014). Wetestedtheseheuristicsinoursimulationsettingand
theyhavesimilarperformanceastheonedescribedabove.
23Houlsby et al. (2011) show that, in case of Gaussian process prior for the function f, we can
approximatethequantity H(y | x,S)−E [H(y | x, f)],foranyqueryx,as
f∼P[f|S]
(cid:18) (cid:19)
   exp − µ2 x,S
µ 2(σ2 +C2)
I(x) := hΦ (cid:113) x,S −C (cid:113) x,S . (1)
σ2 +1 σ2 +C2
x,S x,S
√
Hereh(p) := −plogp−(1− p)log(1− p),C = πln2/2,andµ ,σ aretheposteriorpredic-
x,S x,S
tivemeananddeviationof f(x). Hence,intheactivelearningsetup,thegoalineveryiterationis
tofindthequery xthatmaximizes I(x).
ImplementationofthisapproachforthepairwisecomparisonsettingofAlgorithm1firstrequires
learningarepresentationoftheposteriorof f(·)correspondingtothelabeleddataset((x ,x′))t ,
i i i=1
(r )t andthencomputinganewpairwisecomparison(x,x′)thatmaximizes I((x,x′)).
i i=1
Use-cases of the above algorithms. The two active learning approaches we consider represent
twodifferentparadigmsforquerysampling. ACTIVE-VS-PEisanon-probabilisticdiscriminative
sampling method while ACTIVE-BAYES-PE represents a probabilistic entropy-based approach.
Dependingontheapplication,onemightbefavoredovertheother. TheBayesiancanbefavorable
when the information gain function I(·) is submodular – allowing for near-optimal optimization
using greedy approaches. Another advantage of this approach is that it is independent of model
choices or optimization methods. The version-space approach, on the other hand, is easier to
implementandperformsbetterforcertainapplicationsliketextclassification. However,thereare
theoretical similarities between the two approaches and they can be equivalent in some settings
aswellHoulsbyetal.(2011).
0.15 0.15 1.00 1.00
0.10 0.10 0.95 0.95 0.90 0.90
0.05 0.05 0.85 0.85
0 0. .0 00
5
A AC CT TI IV VE E- -VB SAY -PE ES :- P 3E 0: q3 u0
e
q riu ee sries
0 0. .0 00
5
A AC CT TI IV VE E- -VB SAY -PE ES :- P 5E 0: q5 u0
e
q riu ee sries
000 ... 778 050
A
A
RAC
C
NT TI
I
DV
V
OE E-
-
MVB -SA PY
-
EPE ES-PE
000 ... 778 050
A
A
RAC
C
NT TI
I
DV
V
OE E-
-
MVB -SA PY
-
EPE ES-PE
4 6 8 10 12 14 4 6 8 10 12 14 10 20 30 40 50 10 20 30 40 50
Number of features Number of features Number of comparisons (d=5) Number of comparisons (d=10)
Figure 5: Performance of ACTIVE-VS-PE, ACTIVE-BAYES-PE and RANDOM-PE in an “idealized
setting”(i.e.,noassumptionviolations).
B Additional Implementation Details
Implementation details of the presented simulations that were excluded from the main body are
presentedhere.
Other details of ACTIVE-BAYES-PE and ACTIVE-VS-PE implementation. As mentioned ear-
lier, both ACTIVE-BAYES-PE and ACTIVE-VS-PE use linear kernels. For each method, at every
time-stept,wesample1000newcomparisons T := {(xˆ,xˆ′)} andcomputetheinformativenessof
i i i
eachcomparisonasdefinedbythemethod.
24
ffid ycaruccA
modnaR
&
LA
w/b
ycaruccAFor ACTIVE-VS-PE,wefirstlearnanSVMhypothesis(usingthelabelledcomparisons),saywith
coefficients wˆ , and measure the absolute value of dot product between feature differences for
t
each comparison in T and wˆ , i.e., we compute {|(xˆ −xˆ′)⊤wˆ |} . The chosen comparison is the
t i i t i
onewiththesmallestabsolutedotproductvalue.
For ACTIVE-BAYES-PE, when using a linear kernel, we first employ Bayesian ARD regression
(over the labelled comparisons) with priors for the regularization and precision parameters set
to
Γ(1,1).
With the kernel parameters obtained from this regression, we compute the mean and
variance of the latent value for each pairwise comparison difference in T. For each (xˆ,xˆ′) ∈ T,
we then compute I(xˆ −xˆ′) (where I(·) is the mutual information function and defined in the
descriptionofthebayesianactivelearningapproach),andchoosethecomparisonthatmaximizes
thisvalue. WealsotestedACTIVE-BAYES-PEwiththestandardRBFkernelandithadasimilaror
worseperformancethanthelinearkernelapproach.
Model fitting details. The hypothesis class H is set to be the class of linear SVM functions and
the fit(·,·) function executes the standard linear-SVM training procedure (using python sklearn
library) with L -regularization. Specifically, we set these functions to operate over feature differ-
2
ences, i.e., for any given comparison (x,x′), the input to an SVM classifier h in the feature-wise
t
difference (x−x′). Using feature differences is a standard approach in pairwise comparison set-
ting in practice Freedman et al. (2020). Correspondingly, the vector wˆ for each h represents
ht t
weightsassignedtoindividualfeaturedifferences.
Implementing the tree-based utility model. Finally, to simulate an agent with a tree-based
model of depth d′, we essentially create a binary tree of depth d′, choosing random features and
featurevaluesforpartitioningateachnode.
C Additional results
In this section, we provide additional results that had to be omitted from the main body due to
spaceconstraints.
First, Figure 5 denotes the performance of both active learning algorithms in an “ideal” setting,
i.e., when preferences are stable and do not suffer from any kind of noise and when both the
underlyingpreferencesandthehypothesisclassHarelinearfunctionsovertheavailablefeatures
(without interactions). In this case, we clearly see the advantage of using active learning. Both
ACTIVE-BAYES-PE and ACTIVE-VS-PE outperformthe RANDOM-PE baselineinallcases.
C.1 PreferenceInstability
Performance with respect to normalized distance metric. The results with respect to Normal-
ized L2-distance for scenarios Downscale-ordered, Upscale-ordered, and Random-switch are
presentedinFigure6. Overall,thetrendsaresimilarastheonesforaccuracymetric.
Additionalpreferencechangescenarios. BeyondtheonespresentedinSection4.1,wesimulate
thefollowingotherkindsofpreferencechangesaswell.
• Downscale-random: Agentchangepreferencetousejustonerandomfeatureaftertimestep
t .
change
25ACTIVE-BAYES-PE ACTIVE-VS-PE RANDOM-PE
Downscale-ordered Upscale-ordered Random-switch
tchange=10; d=5 tchange=10; d=10 tchange=10; d=5 tchange=10; d=10 tchange=10; d=5 tchange=10; d=10
0.8 1.5 1.0 1.5 2.5
0.6 1.0 00 .. 68 1.0 12 .. 50 12 .. 50
00 .. 24 0.5 00 .. 24 0.5 01 .. 50 01 .. 50
0.0 0.0 0.0 0.0 0.0 0.0
10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50
tchange=20; d=5 tchange=20; d=10 tchange=20; d=5 tchange=20; d=10 tchange=20; d=5 tchange=20; d=10
0.8 1.25 0.8 1.25 2.5 2.5
0000 .... 0246 0001 .... 2570 5050 0000 .... 0246 0001 .... 2570 5050 00112 ..... 05050 00112 ..... 05050
10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50
tchange=30; d=5 tchange=30; d=10 tchange=30; d=5 tchange=30; d=10 tchange=30; d=5 tchange=30; d=10
0.8 1.25 0.8 1.25 2.5 2.5
0.6 1.00 0.6 1.00 2.0 2.0
0.4 0.75 0.4 0.75 1.5 1.5
0.50 0.50 1.0 1.0
0.2 0.25 0.2 0.25 0.5 0.5
0.0 0.00 0.0 0.00 0.0 0.0
10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50
#comparisons N #comparisons N #comparisons N #comparisons N #comparisons N #comparisons N
Figure6: NormalizedL2-distancevsnumberofcomparisonsforACTIVE-VS-PE,ACTIVE-BAYES-
PE and RANDOM-PE algorithms. Eachplotrepresentsadifferentconfigurationofthepreference
changescenariopresentedinSection4.1.
• Upscale-random: Agentchangespreferencefromusingjustonefeaturetousingallfeatures
aftertimestept .
change
• Downscale-ordered-2: Agentsimplifiespreferencetousethetop-twohighestweightedfea-
turesaftertimestept .
change
• Downscale-ordered-4: Agentsimplifiespreferencetousethetop-fourhighestweightedfea-
turesaftertimestept .
change
• Upscale-ordered-2: Agentchangespreferencefromusingonlytwofeaturestousingallfea-
tures after timestep t , with all features in wpost having lower relative weight than the
change
non-zeroweightsinwpre.
• Upscale-ordered-4: Agent changes preference from using only four features to using all
features after timestep t , with all features in wpost having lower relative weight than
change
thenon-zeroweightsinwpre.
TheresultsforthesepreferencechangescenariosarepresentedinFigure7. Thescenariosconsid-
eredherealsoreflectdifferentscalesofpreferencechange. Forinstance, preferencechangeinthe
caseofDownscale-ordered-4isrelativelylowerscalethanthatinDownscale-ordered-2.
This preference change scale is also reflected in the results. The accuracy drop post-t is
change
smaller in the case of Downscale-ordered-4 and Upscale-ordered-4, compared to Downscale-
ordered-2andUpscale-ordered-2. Correspondingly,inthesesettings,theimpactonactivelearn-
ing algorithms is also relatively smaller. For all settings in Downscale-ordered-4 and Upscale-
ordered-4, one can see that both ACTIVE-VS-PE and ACTIVE-BAYES-PE recover well from pref-
erencechangeandhavebetteraccuracyandefficiencythanthe RANDOM-PE baselinechange.
As the scale of preference change increases, active learning can be seen to be less effective. For
example,thedifferenceinaccuracybetweentheactivelearningapproachesandtherandombase-
lineissmallerinthecaseofDownscale-ordered-2andUpscale-ordered-2. Nevertheless,weonce
26
ecnatsiD-2L
ecnatsiD-2L
ecnatsiD-2LFigure 7: Accuracy vs number of comparisons for additional preference change scenarios dis-
cussedinAppendixC.1.
againobservethatwhenthenumberoffeaturesissmalland/orpreferencechangehappensearly,
activelearning(especially ACTIVE-BAYES-PE)canstillachievehighaccuracyfasterthantheran-
domquerybaselinepost-t .
change
C.2 ModelMisspecification
For model misspecification scenarios, here we present results for accuracy vs number of com-
parisons. We set the number of features d to be 4, 8, or 16. In all cases, the extent of model
misspecificationisquantifiedby⌊log(d)⌋. TheresultsforthissettingarepresentedinFigure8.
C.3 NoisyResponses
Thissectionpresentsadditionalresultsforthesimulationswheretheagent’sresponsesarenoisy
orstochastic.
27ACTIVE-BAYES-PE ACTIVE-VS-PE RANDOM-PE
Agent uses tree-based preference model, but is the linear class (tree depth= log(d) )
d=4 d=8 d=16
1.0 1.0 1.0
0.9 0.9 0.9
0.8 0.8 0.8
0.7 0.7 0.7
0.6 0.6 0.6
0.5 0.5 0.5
10 20 30 40 50 10 20 30 40 50 10 20 30 40 50
Agent uses second-order preferences but assumes no interactions (#interaction terms= log(d) )
d=4 d=8 d=16
1.0 1.0 1.0
0.9 0.9 0.9
0.8 0.8 0.8
0.7 0.7 0.7
0.6 0.6 0.6
0.5 0.5 0.5
10 20 30 40 50 10 20 30 40 50 10 20 30 40 50
Missing features in the preference elicitation framework (#missing features= log(d) )
d=4 d=8 d=16
1.0 1.0 1.0
0.9 0.9 0.9
0.8 0.8 0.8
0.7 0.7 0.7
0.6 0.6 0.6
0.5 0.5 0.5
10 20 30 40 50 10 20 30 40 50 10 20 30 40 50
Figure 8: Accuracy vs number of comparisons for model misspecification scenarios presented in
Section4.2.
28Performance with respect to L2-distance. Figure 9 presents the equivalent of Figure 4 for nor-
malized distance comparison. Here, the trends are similar to the accuracy plots. ACTIVE-BAYES-
PE hasthebestperformanceinthecaseofresponsenoisebutfailstoprovidesimilarlyimproved
performanceinallsettingsofpreferencenoise.
ACTIVE-BAYES-PE ACTIVE-VS-PE RANDOM-PE
Response Noise Preference Noise
d=5; N=30 d=5; =2 d=5; N=30 d=5; =2
0.8 11 .. 02 05 2.0 2.0
0.6 0.75 1.5 1.5
0.4 0.50 1.0 1.0
0.2 0.25 0.5 0.5
0.0 0.00 0.0 0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 10 20 30 40 50 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 10 20 30 40 50
Noise parameter #comparisons N Noise parameter #comparisons N
d=10; N=30 d=10; =2 d=10; N=30 d=10; =2
1.25 2.0
0.6 1.5 1.00
1.5
0.4 0.75 1.0
1.0
0.50
0.2 0.25 0.5 0.5
0.0 0.00 0.0 0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 10 20 30 40 50 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 10 20 30 40 50
Noise parameter #comparisons N Noise parameter #comparisons N
Figure 9: L2-distance comparison of ACTIVE-VS-PE, ACTIVE-BAYES-PE and RANDOM-PE algo-
rithmsfordifferentkindsofnoisemodelspresentedinSection4.3.
Performancewithrespecttotime-variantnoise. Inadditiontoconstantnoise, wesimulatethe
setting where the noise decreases with time. This model can also be considered a combination
ofthenoisyresponsesandpreferenceinstabilitysimulationssincethevariationintheagent’sprefer-
ence/utilitycomputationmodelistime-dependent.
√
In this case, for the Response noise model, ε ∼ N(0,σˆ2), where σˆ = σ/ t, where t is the
time-step/number of comparisons made so far. Similarly, for the Preference noise model, w ∼
√
N(w⋆,σˆ2I/d), where σˆ = σ/ t. This time we vary σ from 1 to 10, to capture a larger range of
noiseparameters.
The results for these two noise models with time-variant noise parameters are presented in Fig-
ure 10. Considering the relatively smaller impact of noise in this setting, the active learning al-
gorithmshavemuch-improvedperformanceincomparisontotherandomquerybaselineforthe
responsenoisemodel. Forthepreferencenoisemodel,however,improvedperformanceofactive
learningisagainonlyobservedwhenthenoisemagnitudeisrelativelysmall.
29
ecnatsiD-2L
ecnatsiD-2LACTIVE-BAYES-PE ACTIVE-VS-PE RANDOM-PE
Response Noise (time variant) Preference Noise (time variant)
d=5; N=30 d=5; =5 d=5; N=30 d=5; =5
1.0 1.0 1.0
0.9 0.9 0.8
0.9 0.8 0.8 0.7
0.8 0.7 0.7 0.6
0.6 0.5
0.6
0.7
0 2 4 6 8 10 10 20 30 40 50 0 2 4 6 8 10 10 20 30 40 50
Noise parameter #comparisons N Noise parameter #comparisons N
d=10; N=30 d=10; =5 d=10; N=30 d=10; =5
1.0 1.0 1.0
0.9 0.9 0.8
0.9 0.8 0.8 0.7
0.8 0.7 0.7 0.6
0.6 0.5
0.6
0.7
0 2 4 6 8 10 10 20 30 40 50 0 2 4 6 8 10 10 20 30 40 50
Noise parameter #comparisons N Noise parameter #comparisons N
Figure10: AccuracyofACTIVE-VS-PE,ACTIVE-BAYES-PEandRANDOM-PEalgorithmsfortime-
variantnoisemodels.
30
ycaruccA
ycaruccA