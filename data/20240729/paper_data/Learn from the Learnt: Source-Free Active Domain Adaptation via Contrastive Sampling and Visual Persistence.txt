Learn from the Learnt: Source-Free Active
Domain Adaptation via Contrastive Sampling and
Visual Persistence
Mengyao Lyu1,2⋆, Tianxiang Hao1,2⋆, Xinhao Xu1,2, Hui Chen1,2(cid:12), Zijia Lin1,
Jungong Han1,2, and Guiguang Ding1,2(cid:12)
1 Tsinghua University, Beijing, China
2 BNRist, Beijing, China
mengyao.lyu@outlook.com, beyondhtx@gmail.com, {xxh22@mails,
huichen@mail}.tsinghua.edu.cn, linzijia07@tsinghua.org.cn,
jungonghan77@gmail.com, dinggg@tsinghua.edu.cn
Abstract. DomainAdaptation(DA)facilitatesknowledgetransferfrom
a source domain to a related target domain. This paper investigates a
practicalDAparadigm,namelySourcedata-FreeActiveDomainAdapta-
tion(SFADA),wheresourcedatabecomesinaccessibleduringadaptation,
and a minimum amount of annotation budget is available in the target
domain. Without referencing the source data, new challenges emerge in
identifying the most informative target samples for labeling, establish-
ing cross-domain alignment during adaptation, and ensuring continuous
performance improvements through the iterative query-and-adaptation
process. In response, we present learn from the learnt (LFTL), a novel
paradigm for SFADA to leverage the learnt knowledge from the source
pretrained model and actively iterated models without extra overhead.
We propose Contrastive Active Sampling to learn from the hypotheses
of the preceding model, thereby querying target samples that are both
informativetothecurrentmodelandpersistentlychallengingthroughout
active learning. During adaptation, we learn from features of actively
selected anchors obtained from previous intermediate models, so that
the Visual Persistence-guided Adaptation can facilitate feature distribu-
tion alignment and active sample exploitation. Extensive experiments
on three widely-used benchmarks show that our LFTL achieves state-of-
the-art performance, superior computational efficiency and continuous
improvements as the annotation budget increases. Our code is available
at https://github.com/lyumengyao/lftl.
Keywords: Transfer learning · Domain adaptation · Active learning
1 Introduction
Deep neural networks have thrived in computer vision but struggle when real-
world data deviates from the ideal independent and identical distribution of
⋆ Equal Contribution (cid:12) Corresponding Author
4202
luJ
62
]VC.sc[
1v99881.7042:viXra2 M. Lyu et al.
Unlabeled Data
n n
iam
Source Model
iam
Model
SFUDA-𝑆𝐹𝐷𝐴2
oD
ecru
oS
Labeled Data Source-Free
oD
tegraT
Active Selections Oracles
)%
( ycaruccA
S 𝑠F 𝑜U 𝑢D 𝑟𝑐A 𝑒- -𝑆 𝑜𝐻 𝑛𝑙𝑂 𝑦𝑇++
A SA FDD AAA D-- A𝐿𝑆𝐷 𝐴 -𝐿𝐷𝑀 𝐹𝐴𝐴 𝑇𝐺 𝐿
𝑅𝑒𝑠𝑁𝑒𝑡
Source samples Target samples Target samples with Target samples with Decision
with labels withoutlabels active annotations model judgements Boundary Annotation Budget (%)
Fig.1:(L)Sourcedata-FreeActiveDomainAdaptation(SFADA)paradigm.Notethat
theratiooflabeled/unlabeledtargetsismuchlower(≤1%or≤5%inourexperiments)
than in the illustration. (R) Performance comparison among DA SOTAs of different
settings on Office-31.
training data, causing performance drops in well-trained models. This is where
domain adaptation (DA) comes into play, which enables knowledge transfer from
a source domain to a target domain of different data distribution. Most DA
studies assume concurrent access to data from both domains to leverage the
inter-domainrelationship.Nonetheless,therelianceonlabeledsourcedataduring
adaptationcouldimpedeitswidespreadusageinreal-worldscenarios,considering
stringent data protection regulations and resource deficiency in storage and
computation. Therefore, source-free unsupervised DA (SFUDA) [16,17,25,28,40,
59,65,68] advocates for encapsulating knowledge within a source-trained model.
They typically employ parameter sharing and hypothesis-driven learning for
model adaptation. Yet, the absence of direct supervision from any domain can
intensifytheill-posednatureofunsupervisedDA[50],leadingtotheperformance
bottlenecks despite considerable efforts made.
In fact, the presence of the target sample pool during adaptation implies
that minimal annotation effort is possible, yielding substantial performance
gains.HencewestudyamorepracticalDAsetting,dubbedsource-freeactiveDA
(SFADA).AsshowninFig1(L),theadaptationprocessisfreedfromsourcedata,
andmeantimeaminimumamountofannotationbudgetisavailableforiteratively
querying labels in the target domain. Limited yet definitive supervisory signals
intrinsically alleviate the ill-posedness phenomenon, and additionally, SFADA
offers an appealing trade-off between labeling costs and adaptation performance
regarding accuracy and efficiency.
Taking both targetannotation availabilityand source datainaccessibility into
consideration, SFADA presents new challenges. During query selection, general
active learning criteria [3,9,19,31,47,48,64] tend to fail under the domain shift
phenomenon as observed in both previous Active DA (ADA) studies [8,39,41,52,
53,60]andourexperiments4.2.Ontopofthat,ADAtypicallyemployssourcedata
asareferencetoidentifydistinctivetargetsamples,whichviolatesoursource-free
setting and thus is not compatible here either. Therefore, SFADA necessitates a
newactivelearningstrategytopinpointthemostinnovativeandinformativetarget
samples utilizing solely a source-trained model. Another challenge comes at the
transfer stage. Without the source data, manifold alignment, typically employed
via minimizing distribution divergence to boost performance, is impeded. Yet theTitle Suppressed Due to Excessive Length 3
labelingcostshaveincreasedtheexpectationsforperformanceenhancement.This
promptsustoaskhow to exploit the newly acquired knowledge while consolidating
the learnt domain-invariant information during adaptation? And following these,
how to guarantee continuous performance improvement during the iterative query-
and-adaptation process?
To fill in this gap, this paper formulates a novel SFADA paradigm to learn
from the learnt (LFTL), as illustrated in Fig. 2. We employ the hypothesis and
visual representations of target samples obtained from the learnt source model
and actively iterated models to ascertain what to supplement and where to
adapt in the target domain. We first conduct active learning from the learnt
model. Although it is impossible to directly identify target samples the most
divergent from the source, the hypothesis of learnt model can be utilized as an
indication of how well the target domain knowledge is understood. Furthermore,
as the model progressively adapts towards the target domain, the hypothesis of
the current model becomes increasingly important to query newer labels. This
has not been addressed by the one-round query of MHPL [57]. Inspired from
contrastive decoding [6,11,23,27,63], we propose Contrastive Active Sampling
(CAS) to leverage the hypothesis of the model from the previous active round. It
emphasizes samples that are both informative to the current model and persis-
tently challenging throughout the iterative process, while deprioritizing samples
that are redundant or deliver knowledge already acquired during active selection.
Upon the contrastively decoded hypothesis, we take the difference between the
best-versus-second best (BvSB) guesses [19], integrate a class-balancing factor,
so that temporally-stagnant, sample-uncertain and class-minor targets can be
effectively queried. Secondly, we perform transfer learning from the learnt visual
representations. Since the alignment of cross-domain distributions cannot be
directly achieved, we establish Visual Persistence-guided Adaptation (VPA) to
maintain feature representations of active target samples throughout the whole
process, where the understanding initially derived from the source domain and
subsequently obtained from previous active rounds are effectively conserved via
momentum updates. Then our learning objective encourages unlabeled samples
tobecentralizedaroundthesememory-preservingrepresentatives,approximating
a source-similar distribution while exploiting target-specific knowledge obtained
from queried labels.
Gaining insights from the intermediate results already computed during
iteration makes our method simple in architecture, effective in active learning
and knowledge transfer, superior in terms of both accuracy and efficiency, and
flexible in the trade-off between annotation plus time budget and adaptation
accuracy. Take the performance on the VisDA-C [38] benchmark for example,
the entire query-and-adaptation process obtains 87.4% with merely 1% labeling
budget and 780 training iterations (0.3h). In contrast, SHOT++ operating under
the SFUDA setting requires 21.6K iterations (5.8h) to achieve 87.3%. Even when
factoring in the estimated annotation time (1.83h), our method is significantly
more time-saving. Compared with LADA [53], the ADA state-of-the-art (SOTA),
we achieve comparable performance without accessing the source data, while4 M. Lyu et al.
benefiting from a 25% deduction in active sampling time and an approximately
17-fold increase in adaptation speed. Furthermore, it exceeds prior SFADA work
MHPL [57] by a clear margin of 1.5%. Our main contributions are:
– a novel SFADA paradigm to learn from the learnt source model and actively
iteratedmodels,whichfreesitfromspecializedarchitectureandsophisticated
learning schemes with superior accuracy and efficiency;
– a CAS strategy to learn from previous model hypothesis, thereby prioritizing
targets that are confusing for the current model, less transferable in class
membership and consistently challenging in previous active rounds;
– a VPA design to learn from previous feature representations, so that intrinsic
distribution alignment and active sample exploitation can be achieved;
– SOTA adaptation accuracy, superior computational efficiency, and continual
improvements extensively validated on varied-scaled benchmarks.
2 Related Work
Domain Adaptation (DA) is a specialized case of transfer learning that
enablesknowledgegeneralizationfromasourcedomaintoarelatedtargetdomain.
Varying in the assumption of the label-set relationship between source and target
domains, namely semantic shift, DA settings can be mainly divided into close-set
DA, open-set DA [35,46], partial DA [4,26] and universal DA [45,66]. According
to the assumption of target data accessibility, DA settings fall into unsupervised
DA (UDA) [10,61], semi-supervised DA (SSDA) [13,44], weakly-supervised DA
(WSDA) [54], zero-shot DA [37,58], one-shot DA [30], few-shot DA [12,34,62] as
well as active DA (ADA) [8,39,41,52,53,60].
Among the subfields, the one most related to our setting is ADA, where
the goal is to query the most informative target samples for annotation to best
benefit classification on the target domain. Saha et al. [43] either infer pseudo
labels on target samples or query them for oracle annotation, which is based
on inter-domain similarity between source and target domains. Su et al. [52]
trainadomaindiscriminativemodelfordomainalignmentandtargetuncertainty
estimation. While most of them utilize or are orthogonal to off-the-shelf active
learning mechanisms, Fu et al. [8] argue that constructing a committee [33],
i.e., ensemble to measure target sample uncertainty based on consensus between
multiple predictions is more robust under domain shift. CLUE [39] is a k-means
clustering-weighted uncertainty-based active learning strategy, and it optimizes
with cross-domain minmax entropy [44]. Xie et al. [60] apply the margin loss to
the source domain training to exploit hard source samples and a less domain-
biased decision boundary, and they further supplement margin sampling with
expected error reduction consistent with the training objective. LADA [53] bases
its sampling criterion on the predictive purity of a local structure, and exploits
both data sources during adaptation. Despite previous efforts, ADA methods
typically necessitate the simultaneous presence of data from both the source and
target domains to exploit the relationship between domains. However, when thisTitle Suppressed Due to Excessive Length 5
assumption fails in practical scenarios, we must tackle the challenges associated
with the unavailability of source data.
On the other hand, SFUDA posits a transfer setting where neither the
source data nor the target annotations are available. Kundu et al. [22] adopt
a two-stage paradigm for universal DA, where the procurement stage trains a
source model in consideration of future semantic shifts and domain gaps, so that
thedeploymentstageiscapableofoperatingwithoutsourcedata.Liangetal.[28]
harness information maximization [15,50] to align different domains, and utilize
self-supervision and curriculum learning techniques via pseudo label clustering.
Xia et al. [59] introduce adversarial training to distinguish between source-
similar and -dissimilar target samples for cross-domain alignment, facilitated
with self-supervision and clustering to improve performance. Based on prediction
confidence, DaC [68] also divides the target samples based on source similarity,
employing a global structure for source-like targets while implementing a local
structure for the distinctive samples. Without definitive supervision signal from
either domain, SFUDA makes the alignment more challenging and exacerbates
the ill-posedness of UDA. Consequently, it frequently demands intricate design
and increased computations to attain desirable outcomes.
Given the accessibility of target samples, this paper focuses on the more
practicalSFADAparadigm,leveragingminimalresourcesforbetterperformance.
Prior work MHPL [57] integrates three active learning strategies to sample
targets that are uncertain, diverse and different from the source domain, and
then utilizes a neighbor focal loss to emphasize them in adaptation. However,
the dependency on the source dissimilarity metric confines it to a one-round
sampling method, failing to guarantee sustained performance improvement in
real-world applications. SALAD [21] samples target with a binary weighted
function between an entropy score and the expected model change. However
the adaptation procedure incorporates an additional Guided Attention Transfer
Network, leading to undesirable computational overhead and, as evidenced by
the results, inefficient annotation utilization. In contrast, our LFTL offers a
straightforward, broadly applicable, and growth-oriented solution that can make
the best from previous knowledge and a limited budget to guarantee accuracy,
efficiency,andaflexibletrade-offbetweenannotationresourcesandperformance.
3 Learn from the Learnt
Inthissection,wepresentLFTLforSFADA.AsillustratedinFig.2,italternates
between Contrastive Active Sampling (CAS) and Visual Persistence-guided
Adaptation (VPA).
3.1 Overview
Given a labeled source domain S with restricted access and an unlabeled target
domain T, our goal is to derive a model that can minimize the risk on the
target domain through knowledge transfer with a small annotation budget. At6 M. Lyu et al.
VisualPersistenceAdaptation
Temporally ensembled Newly updated at
at previous rounds 𝑟𝑡ℎround
𝒇෨(𝒯(𝑟))
𝑙
reprev si es nu ta al tions ℒ𝑣𝑝𝑎(𝒯𝑙𝑟,𝒯𝑢(𝑟)) ℒ𝑒𝑛𝑡(𝒯𝑢(𝑟))
LearntKnowledge active feature with VP decision boundary
unlabeled target feature update direction
hypothesis
ℳ𝑠 Initialize ℳ 𝑡(0) … ℳ 𝑡(r−1) ℳ 𝑡(𝑟) contrast
𝒑(r−1) 𝒑(r)
×𝛼
𝑢𝑐𝑚
𝒑෥(𝑟) +𝑢𝑐𝑡
Oracles
ContrastiveActiveSampling 𝑢
Labeled Target Data 𝒯𝑙(𝑟) Unlabeled Target Data 𝒯𝑢(𝑟)
𝑟=𝑟+1 Top-btarget candidates
Fig.2: The proposed LFTL framework for SFADA. Contrastive Active Sampling
emphasizes freshly acquired knowledge in the posterior distribution so that novel
samples are more likely to be queried for annotations. Then during adaptation the
persistence vaultretains previous domain-invariantknowledge to facilitatealignment in
the target domain via L and L . As the iterative process continues, it yields more
vpa ent
informative targets and an improved target model.
the source end, image and label pairs S = {(x ,y )} from the source domain
s s
are first utilized to train a model M : x (cid:55)→ y , which consists of a feature
s s s
extractor f(·) and a classifier g(·) , i.e., M (x ) = g (f (x )). After severing
s s s s s s s
communication with the source domain data, we adapt M , where the prior
s
knowledge is maintained, to the target domain with the assistance of a budget
of B labeled instances. The initial target dataset has a total of n(0) unlabeled
tu
samples T u(0) ={xi tu}i i= =n 1( t0 u) and a shared label space Y T.
At the beginning of the R-round query-and-adaptation alternation (r =0),
the target model M(r), without any architectural adaptation, is initialized from
t
M as M(0). We actively query labels for b = B/R most informative target
s t
instances with the criterion of CAS (Sec. 3.2), which initializes the labeled subset
of the target pool, denoted as T l(1) = {xj tl}j j= =n 1( t1 l) , and thus T u(1) = T u(0)\T l(1).
Then the parameters of the target model can be optimized using the newly
acquired target data splits, and updated to M(1) via the proposed VPA strategy
t
(Sec. 3.3). Likewise, in subsequent rounds (r ≥ 1), target data splits T(r) ←
l
T(r−1)∪{xj} and T(r) =T(r−1)\T(r), as well as the task model M(r) are
l tl n(r) u u l t
tl
updated, continuously querying novel knowledge and optimizing towards the
target domain through the synergistic interaction of sampling and adaptation.Title Suppressed Due to Excessive Length 7
3.2 Contrastive Active Sampling
In each active learning round, the aim is to find the most informative instances
from the target domain, especially those diverging from the source, to benefit the
adaptation of the model. Without directly referencing the source data, previous
source-free approaches [7,25,28,59] often rely on the hypothesis transfer to
distinguish between source-like and source-dissimilar samples. However, when
appliedtotheSFADAscenario,suchsolution[57]isconstrainedtoasingleround
of active sampling and model updating, which restricts the potential applications
of this setting.
Considering the iterative nature of the task context, we tackle this chal-
lenge from the active perspective. Just as the source knowledge is encapsulated
within the initial model M , the target knowledge learned in previous rounds
s
is manifested in the hypotheses of the preceding model M(r−1). Compared to
t
the hypotheses of the preceding model, samples that obtain higher prediction
confidence from the newly updated model reflect the fresh insight just acquired.
The larger the gap in predictive confidence, the better the current model has
grasped the sample, and thus the less informative for the subsequent phase of
sample selection. As illustrated in Fig 2, we first contrastively highlight the less
informative samples as follows:
(cid:40)
logp(r) if r =0,
p˜(r)(·|xi )= (1)
tu logp(r)+α(logp(r)−logp(r−1)) if r >0,
weightedbyα,wherepisderivedfromthesoftmaxofthemodellogitsδ(g(f(xi ))).
tu
We omit the superscript of round (r) for simplicity in the following text.
Based on p˜, various active learning criteria can be employed. Without
loss of generality, here we focus on the most confused classes. Specifically,
we take the difference between the best-versus-second best (BvSB) guesses as
the uncertainty indicator, defined as u (xi ) = p˜(yi|xi )−p˜(yi|xi ), where
cm tu a tu b tu
yi =argmax p˜(y|xi ), and yi =argmax p˜(y|xi ). Then the unlabeled
a y∈Y tu b y∈Y\y ai tu
target sample pool is ranked by:
(cid:88)ntu
r(xi )= I(u (xk )<u (xi )), (2)
tu cm tu cm tu
k=1
where I is the indicator function. A smaller u score indicates that the assessed
cm
target sample xi furnishes novel insights that diverge from the source domain
tu
and, in contrast to prior iterations, has not been grasped. Conversely, when the
current stronger model, aided by a marginal increment of active labels, assigns
moreprobabilitypreferencetooneclassforanunlabeledsample,CASemphasizes
it via a larger u score, lowering the priority of those familiar candidates. As
cm
the query-and-adaptation alternation proceeds, CAS continuously harvests fresh
information for transfer by leveraging readily available intermediate results,
without additional extra computational burden or memory usage.
Thecontrastivelydecodedmarginevaluateseachsampleonanindividualbasis.
However, the batch-mode active learning could be susceptible to outliers, since8 M. Lyu et al.
the queried samples are not selected based on the underlying natural density
distribution [1,49]. Particularly in DA tasks, different classes show different
sensitivity to the domain shift. Some of them are more robust to certain shifts,
i.e., their learnt features present more domain-invariant patterns, whereas others
arelikelytosufferasteepperformancedrop[18].Thesemotivateustoincorporate
a broader, semantic view of the target domain into our active criterion, where
classes with higher transferability is estimated via the class memberships of
reliable model hypotheses:
(cid:80)ntu I(yi =c)·I(r(xi )>(n −κ))
u (c|T )= i=1 a tu tu . (3)
ct u max [(cid:80)ntu I(yi =c)·I(r(xi )>(n −κ))]
c i=1 a tu tu
Eq. 3 measures the frequency of unlabeled samples being inferred as class c
among κ highest contrastive margins, wherein κ controls the range of statistics.
Thus the classes with higher u scores can be downweighted in favor of less
ct
transferable classes.
In this way, we drive a hybrid active sampling criterion that favors target
samples with local uncertainty, global intransferability and temporal stagnancy:
ui =ui +λu (yi), (4)
cm ct a
where λ controls the importance of class-level cross-domain transferability.
3.3 Visual Persistence-guided Adaptation
During each iterative round, the top-b most informative samples are queried for
labels towards the adaption to the target domain. They offer reliable signals
but also pose a challenge of making the best use of them. We first warm up the
adaptation process via empirical risk minimization on the small amount of target
instances with active labels:
L =−E q(x )T log[p(·|x )], (5)
ce xtl∼Tl tl tl
where q(x ) is the ground truth 1-hot vector for the labeled instance x .
tl tl
To further minimize the risk on the target domain, we are motivated to
bridge the distance between actively queried data and unlabeled data. Instead
of seeking cluster centroids in the target pool, we directly take active samples
as representative anchors, and encourage unlabeled data to form concentrated
clusters around the nearest anchor on the visual embedding space via soft
similarity minimization:
L =−E dT log(d ),where d =δ[D(f (x ),f(T ))]. (6)
ac xtu∼Tu tu tu tu e tu l
In Eq. 6, the feature extractor of the adapted model outputs the visual repre-
sentation of x as f (x ). f(T ) is a n ×d matrix where each row represents a
tu t tu l tl
d−dimensional feature of a labeled anchor xi . D denotes a distance metric. For
tl
simplicity and without loss of generality, we adopt cosine similarity to measureTitle Suppressed Due to Excessive Length 9
the relationship between different instances, and a normalization function is
appliedtotransformthedistancesto[0,1].Indecreasingtheentropyofdistances,
each unlabeled sample is encouraged to get close to its nearest labeled anchors
and meantime keep other clusters at a considerable distance. Consequently, as
shown in Fig. 2, features are aggregated in a class-wise manner, which makes it
easier for classifier decision boundaries to be established.
However, in the context of the SFADA framework, as the labeled target
subset T and the model M iterate for R rounds, the incremental domain-
l t
specific information obtained from the actively queried samples may intensify
the cross-domain incompatibility [5,51,55]. Furthermore, there is a potential
risk of domain-invariant knowledge being forgotten during the process. Without
accessing the source domain data, previous source-free methods achieve domain
alignment via freezed hypotheses [25,28,59] or buffered embeddings [7] from
the source classifier. In our multi-round iterative process, we introduce a visual
persistence vault to guide the adaptation. It preserves judgements on the active
anchors, which have been derived from source and intermediate models, via
exponential moving average:
˜f(x )←γf(x )+(1−γ)˜f(x ), (7)
tl tl tl
wheref(x )isderivedfromtheadaptedfeatureextractorf ,andγ isthemomen-
tl t
tumparametercontrollingtheweightingdecreaseofpreviousobservations,which
is empirically fixed as 0.9. In replacing the most recent feature representation of
active anchors f(T ) in Eq. 6 with the temporally ensembled ˜f(T ), we obtain the
l l
visual persistence-guided adaption loss L . The learnt domain-invariant knowl-
vpa
edgemaintainedbythepersistencevaulteffectivelysupportsthealignmentinthe
target domain, and meantime the target-specific information is well exploited.
Additionally, the entropy minimization loss [16,28,57] is introduced to ap-
proach the ideal adaptation performance and foster discriminative features in
the learnt manifold:
L =−E p(·|x )T log[p(·|x )]. (8)
ent xtu∼Tu tu tu
Overall, the combination of supervised cross-entropy, visual persistence guid-
ance and entropy minimization losses yields:
L=L +β L +β L . (9)
ce 1 vpa 2 ent
We emphasize that no explicit supervision (e.g., pseudo labels) is enforced for
unlabeled samples during the optimization, so that the error accumulation
problem is greatly alleviated in the source data-free and target label-scarce
setting.
4 Experiments
4.1 Setup
Datasets. We conduct experiments on three widely-used DA benchmarks:
a) VisDA-C [38] aims to solve the simulation-to-reality shift, in which the10 M. Lyu et al.
Table 1: Results on the large-scale VisDA-C dataset (ResNet101) in terms of classi-
fication accuracy (%). SF represents source inavailability, and AS shows percentage
of active annotations (%). Best results are highlighted in bold, and the best in each
section are underlined.
Method SF AS plane bcycl bus car horse knife mcycl person plant sktbrd train truck Per-class
SFDA[20] ✓ - 86.9 81.7 84.6 63.9 93.1 91.4 86.6 71.9 84.5 58.2 74.5 42.7 76.7
A2Net[59] ✓ - 94.0 87.8 85.6 66.8 93.7 95.1 85.8 81.2 91.6 88.2 86.5 56.0 84.3
SHOT[28] ✓ - 95.8 88.2 87.2 73.7 95.2 96.4 87.9 84.5 92.5 89.3 85.7 49.1 85.5
SHOT++[28] ✓ - 97.7 88.4 90.2 86.3 97.9 98.6 92.9 84.1 97.1 92.2 93.6 28.8 87.3
CPGA[40] ✓ - 94.8 83.6 79.7 65.1 92.5 94.7 90.1 82.4 88.8 88.0 88.9 60.1 84.1
DaC[68] ✓ - 96.6 86.8 86.4 78.4 96.4 96.2 93.6 83.8 96.8 95.1 89.6 50.0 87.3
SF(DA)2[17] ✓ - 96.8 89.3 82.9 81.4 96.8 95.7 90.4 81.3 95.5 93.7 88.5 64.7 88.1
AADA[52] (cid:37) 1 83.5 64.0 67.2 80.5 87.8 61.4 88.5 79.1 87.9 78.5 84.7 32.6 74.6
TQS[8] (cid:37) 1 87.3 77.5 77.1 67.0 90.5 58.4 81.1 82.0 91.1 73.9 68.7 57.6 76.0
CLUE[39] (cid:37) 1 77.0 57.8 73.4 76.5 76.9 68.7 87.0 75.5 85.1 66.5 76.1 47.3 72.3
SDM-AG[60] (cid:37) 1 84.0 72.3 77.8 82.0 92.2 81.5 83.1 71.4 85.2 74.2 80.2 35.1 76.6
LADA[53] (cid:37) 1 97.8 82.3 92.0 86.8 98.0 94.9 94.7 88.8 95.6 95.0 93.9 57.0 89.7
AADA[52] (cid:37) 5 92.3 78.5 87.4 87.4 91.8 90.8 91.2 86.2 92.6 90.3 90.1 61.5 86.7
TQS[8] (cid:37) 5 89.6 85.8 82.9 78.5 96.8 82.8 90.2 81.6 93.9 85.2 87.0 69.6 85.3
CLUE[39] (cid:37) 5 92.8 81.7 83.0 84.0 93.4 89.1 91.4 89.1 94.2 88.0 85.3 62.0 86.2
SDM-AG[60] (cid:37) 5 93.3 85.1 83.7 86.7 94.2 90.2 92.8 85.5 90.2 81.1 80.8 56.2 85.0
LADA[53] (cid:37) 5 98.6 87.4 92.9 91.3 98.6 97.0 96.0 91.8 97.4 97.2 95.2 61.6 92.1
LFTL ✓ 1 95.9 84.6 84.6 77.1 95.4 93.6 91.4 87.1 93.2 90.4 87.8 67.6 87.4
MHPL[57] ✓ 5 - - - - - - - - - - - - 91.3
LFTL ✓ 5 98.0 92.5 88.7 89.1 98.0 97.2 94.3 93.5 98.0 96.5 92.6 75.6 92.8
source domain contains 152K synthetic images with varying angles and lightning
conditions, and the target domain is composed of 55K images cropped from
the Microsoft COCO dataset [29], each with the same 12 categories. b) Office-
Home [56] is a medium-sized dataset consisting of 4 domains: Artistic (Ar), Clip
Art (Cl), Product (Pr) and Real-World (Rw) images, each with the same 65
classes. c) Office-31 [42] is small-sized, which consists of 31 classes in 3 different
domains: Amazon (A), DSLR (D) and webcam (W).
Active Setting. Taking the dataset scales into consideration, we budget 1%
and 5% of VisDA-C for annotation. For the medium- and small-scaled office
datasets, 5% and 10% data is actively sampled. In each round, 0.1% annotation
increment is made. In all experiments, the average performance of 3 repeated
trials is reported.
Implementation. FollowingSHOT[28],A2Net[59],CPGA[40]andMHPL[57],
our main results are reported on ResNet101 [14] for VisDA-C and ResNet50
for Office, on top of which is a bottleneck layer that compresses the 2048 high
dimensional features into 256 units, followed by a task-dependent FC layer. In
the source domain, we randomly split source data into 90% training and 10%
validation to obtain a well-trained model. Considering that the number of active
selections is quite small, we harness Mixup [67] regularization to exploit the
potential of data during target adaptation. Component verification and more
training details can be found in supplementary.Title Suppressed Due to Excessive Length 11
Table 2: Results on the Office-Home dataset (ResNet50) in terms of classification
accuracy (%). SF represents source inavailability, and AS shows percentage of active
annotations (%). Best results are highlighted in bold, and the best in each section are
underlined.
Method SFASAr→ClAr→PrAr→RwCl→ArCl→PrCl→RwPr→ArPr→ClPr→RwRw→ArRw→ClRw→Pr Avg
SFDA[20] ✓ - 48.4 73.4 76.9 64.3 69.8 71.7 62.7 45.3 76.6 69.8 50.5 79.0 65.7
A2Net[59] ✓ - 58.4 79.0 82.4 67.5 79.3 78.9 68.0 56.2 82.9 74.1 60.5 85.0 72.8
SHOT[28] ✓ - 57.7 79.1 81.5 67.6 77.9 77.8 68.1 55.8 82.0 72.8 59.7 84.4 72.0
SHOT++[28] ✓ - 57.9 79.7 82.5 68.5 79.6 79.3 68.5 57.0 83.0 73.7 60.7 84.9 73.0
CPGA[40] ✓ - 59.3 78.1 79.8 65.4 75.5 76.4 65.7 58.0 81.0 72.0 64.4 83.3 71.6
DaC[68] ✓ - 59.1 79.5 81.2 69.3 78.9 79.2 67.4 56.4 82.4 74.0 61.4 84.4 72.8
AADA[52] (cid:37) 5 56.6 78.1 79.0 58.5 73.7 71.0 60.1 53.1 77.0 70.6 57.0 84.5 68.3
TQS[8] (cid:37) 5 58.6 81.1 81.5 61.1 76.1 73.3 61.2 54.7 79.7 73.4 58.9 86.1 70.5
CLUE[39] (cid:37) 5 50.7 76.1 78.1 62.6 75.2 71.0 64.0 52.3 79.7 72.7 56.9 83.8 68.6
SDM-AG[60] (cid:37) 5 61.2 82.2 82.7 66.1 77.9 76.1 66.1 58.4 81.0 76.0 62.5 87.0 73.1
LADA[53] (cid:37) 5 71.2 87.4 84.6 72.1 87.0 83.6 71.5 71.6 85.3 79.3 75.5 90.4 80.0
AADA[52] (cid:37) 10 65.8 84.5 82.2 64.1 80.6 76.1 67.6 62.6 80.1 73.7 66.1 88.6 74.3
TQS[8] (cid:37) 10 68.0 87.7 85.7 67.0 83.0 78.7 69.3 64.5 83.9 77.8 68.9 90.6 77.1
CLUE[39] (cid:37) 10 62.1 79.1 80.3 64.1 77.1 76.7 65.4 63.9 84.2 74.2 68.6 84.7 73.4
SDM-AG[60] (cid:37) 10 68.5 87.6 86.4 69.5 84.8 81.2 71.0 66.2 84.8 79.3 69.9 90.8 78.3
LADA[53] (cid:37) 10 77.2 91.9 88.1 76.9 91.1 86.8 76.6 78.1 88.3 82.0 79.0 93.8 84.2
MHPL[57] ✓ 5 69.0 85.7 86.4 72.6 87.4 84.2 73.3 67.4 86.4 80.1 69.6 89.8 79.3
LFTL ✓ 5 66.9 86.6 85.5 73.1 86.3 84.5 72.2 65.7 85.9 79.2 69.0 90.2 78.8
LFTL ✓ 10 76.6 92.2 89.7 78.9 93.0 89.2 78.6 77.1 90.0 83.4 77.8 94.6 85.1
Table 3: Results on the Office-31 dataset
(ResNet50) in terms of classification accuracy
(%). SF represents source inavailability, and
AS shows percentage of active annotations (%).
Best results are highlighted in bold, and the
best in each section are underlined.
Method SFASA→DA→WD→AD→WW→AW→DAvg
SFDA[20] ✓ - 92.2 91.1 71.0 98.2 71.2 99.5 87.2
A2Net[59] ✓ - 94.5 94.0 76.7 99.2 76.1 100.0 90.1
SHOT[28] ✓ - 93.9 90.1 75.3 98.7 75.0 99.9 88.8
SHOT++[28]✓ - 94.3 90.4 76.2 98.7 75.8 99.9 89.2
CPGA[40] ✓ - 94.4 94.1 76.0 98.4 76.6 99.8 89.9
SF(DA)2[17] ✓ - 95.8 92.1 75.7 99.0 76.8 99.8 89.9
AADA[52] (cid:37) 5 89.2 87.3 78.2 99.5 78.7 100.0 88.8
TQS[8] (cid:37) 5 92.8 92.2 80.6 100.0 80.4 100.0 91.1
CLUE[39] (cid:37) 5 90.7 94.3 78.7 99.1 76.1 99.8 89.8
SDM-AG[60] (cid:37) 5 93.5 94.8 81.9 100.0 81.9 100.0 92.0
LADA[53] (cid:37) 5 96.3 97.7 83.1 99.6 85.0 99.7 93.6 Fig.3: Comparison on the overall
AADA[52] (cid:37) 10 93.5 93.1 83.2 99.7 84.2 100.0 92.3 time consumption (training, active
TQS[8] (cid:37) 10 96.4 96.4 86.4 100.0 87.1 100.0 94.4 sampling and annotation time) and
CLUE[39] (cid:37) 10 96.2 94.7 84.4 99.4 81.0 100.0 92.6
SDM-AG[60] (cid:37) 10 95.9 96.4 86.1 100.0 86.5 100.0 94.2 adaptation accuracy with SFUDA
LADA[53] (cid:37) 10 97.8 99.1 87.3 99.9 87.6 99.7 95.2 (SHOT) and ADA (AADA, TQS,
MHPL[57] ✓ 5 97.8 96.7 82.5 99.3 82.6 100.0 93.2
LFTL ✓ 5 98.0 98.5 82.6 99.9 82.2 100.0 93.5 CLUE, SDM-AG, LADA) methods
LFTL ✓ 10 98.9 99.4 87.8 100.0 86.3 100.095.4 on the VisDA-C dataset.
4.2 Comparison with SOTAs
We consider the following competitors: SFUDA methods including SFDA [20],
A2Net [59], SHOT [28], SHOT++ [28], CPGA [40], DaC [68] and SF(DA)2 [17],
ADA methods including AADA [52], TQS [8], CLUE [39], SDM-AG [60] and
LADA [53], and SFADA prior work MHPL [57] For fair inter- and intra-setting12 M. Lyu et al.
100
Table4:Comparisononcomplexityand
actual query time with ADA methods.
90
Notations are explained in text.
QueryMethod Complexity Time(s/cycle)
80
AADA[52] O(NC+NlogN) 79.16
TQS[8] O(NCM+NlogN) 127.51
CLUE[39] O(tNBD) 196.92 70 Method
SDM-AG[60] O(NCD+NlogN) 127.44 Random
LADA[53] O(ND+NlogN) 97.6 AADA
LFTL O(NC+NlogN) 73.96 60 TQS
CLUE
SDM-AG
Table 5: ResultsofLADAminussource 50 LADA
loss (SF-LADA) and our LFTL plus a MHPL
LFTL
source cross-entropy loss (S-LFTL).
0% 2% 5% 10% 20%
Method SF1%VisDA-C5%Office-Home5%Office-31 Annotation Budget (%)
LADA (cid:37) 89.7 80.0 93.6
S-LFTL (cid:37) 87.6 80.1 94.1 Fig.4:ResultsofSFADA(MHPLandLFTL)
SF-LADA ✓ 82.6 72.4 89.0 and ADA (others) methods on VisDA-C
LFTL ✓ 87.4 78.8 93.5 (ResNet50) as the annotation budget grows.
comparison, consistent experimental conditions provided by SHOT are utilized
for ADA methods based on the official codes of TQS [8], CLUE [39] and SDM-
AG [60], CLUE [53] as well as the re-implementation of AADA [52] provided
by CLUE [39]. Results not reported by MHPL and not reproducible due to
unavailability of source code are indicated by a dash.
WesummarizetheadaptionaccuraciesonVisDA-C,Office-HomeandOffice-31
inTab.1,2and3,respectively.Onthelarge-scaledatasetVisDA-C,givenmerely
1% label annotations, LFTL is already capable of outperforming most of ADA
methods. When the budget increased to 5%, it exceeded the SFADA SOTA by
1.5%andalsooutdidthesource-accessibleADASOTAunderthesameconditions
by0.7%.Onthemedium-sizedOffice-Homedataset,LFTLelevatestheadaptation
performance to a level of 78.8% with 5% annotation costs, which is comparable
with SFUDA methods. We note a continuous enhancement in performance as
the budget is raised to 10%, culminating in the highest 85.1% accuracy. Similar
trend can also be observed in the small-sized Office-31 dataset. When 5% target
samples are labeled, our results surpass MHPL in average performance and in
four out of five subtasks, while being on par with the source-accessible LADA.
The results show that, given a minimum amount of annotations, noticeable
improvements can be achieved over SFUDA with increased flexibility. Meantime,
despite the inavailability of the source domain, learning from what the model
already learnt can offset the absence of the source data via our proposed CAS
strategy and VPA approach.
4.3 Efficiency Analysis
Wecomparewithopen-sourceSFUDAandADAmethodsintermsofactualtime
consumption, including model training, active sampling and human annotation,
)%(
ycaruccATitle Suppressed Due to Excessive Length 13
as presented in Fig. 3. We take the adaptation on the VisDA-C dataset for
example, and provide 1% budget, i.e., 550 images of the target domain for active
learning-based methods. Note that here we report the consumption of the whole
sample-and-adaptation process, instead of the per-round time. All experiments
were conducted on 1 NVIDIA GeForce RTX 3090 GPU, with 80 CPU cores and
256 GB memory.
Considering the training time for adaptation, SFUDA SOTAs often
require sophisticated schemes (clustering, two-stage training, etc.) to adapt to a
completely unlabeled domain, which necessitate more computational resource.
For example, SHOT and SHOT++ takes 3.7h and 5.8h to converge respectively,
andCPGAusesmorethanfivedaystoattainthereportedperformance,whichwe
omit in Fig. 3 to prevent it from dominating the figure. On the other hand, when
equippedwithbothsourceandtargetdata,ADAmethodsareexpectedtodirectly
align different domains, as well as identifying novel samples, which adds to the
training burden. For example, AADA, CLUE and TQS consume more than 6h
mainly because of the adversarial training. TQS additionally trains five classifiers
to construct a voting committee for active learning, culminating in an overall
duration of 8.5h. LADA employs local neighbor search for more reliable training
guidance,whichtakes5.9h.SDM-AGimprovesontheoptimizationcomplexityyet
still requires 5.1h. In contrast, drawing upon queried informative data, our LFTL
leveragestheiterativenatureofactivelearninginsteadofadditionalcomputations.
The proposed VPA retrospects on the knowledge gained from the source and
intermediate models of previous rounds, which is temporarily ensembled in the
VP vault for readily use. As the comparison shows, it significantly outperforms
competitors with only 0.3h for adaptation while yieding superior accuracy.
As for the active sampling efficiency, we summarize the theoretical com-
plexityandactualtrainingdurationforactivemethodsinTab.4,whereC andN
to denote the number of classes and unlabeled instances respectively. AADA and
ourLFTLoperatewithO(NC+NlogN)complexity,whicharethemostefficient
approaches.Theclustering-basedCLUEworksatO(tNBD),inwhichD denotes
theembeddingdimension(512),B isthebudget,andtisthenumberofclustering
iterations(300).Theranking-basedTQSandSDM-AGuses O(NCM+NlogN)
and O(NCD +NlogN) respectively, in which M represents the number of
members in the classifier committee (5 is adopted). The local neighbor search
in LADA leads to a complexity of O(ND+NlogN). The feature dimension
D of SDM-AG and LADA is 256. Contrary to previous active criteria designed
for sampling in the target domain, CAS contrasts the hypotheses generated
by models from successive rounds to identify target samples that deviate from
the source domain and persistently challenging for the recognition task. The
comparison shows that the proposed CAS is effective, scalable and efficient.
Regarding the human annotation time, given that image-level labels take
1 second per class [2,36], 1% target domain subset of VisDA-C would cost
550×12≈1.83h. As Fig. 3 shows, compared to non-active methods, although
factoring in the annotation time introduces additional burden for ADA methods,
we still retain the top place.14 M. Lyu et al.
Collectively, the comparative analysis of accuracy and efficiency reveals that
our LFTL framework, through the synergy of CAS and VPA strategies, can be
freed from source-accessibility, specialized architectures or complex optimization
techniques while still delivering SOTA performance.
4.4 Promises of Continual Performance Growing
In addition to the experiments presented in main results under varying bud-
get constraints, in this section, we explore the trade-off between the cost and
adaptation performance with increased budget and larger strides. The budget is
increased to 2%, 5%, 10% and 20% successively, with the corresponding results
summarized in Figure 4. We observe that the proposed LFTL framework per-
forms notably better than competitors in terms of both accuracy and robustness.
The performance superiority is consistent across different budget scenarios, from
tight to generous, indicating its effectiveness in leveraging data resources. On
the contrary, other methods either gradually deviate from the source domain
throughout the process, failing to acquire domain-variant novel knowledge, or
they solely support one-round active sampling as in MHPL. We emphasize that
the promise of continual performance growth holds practical value in real-world
applications, as it facilitates the effective feedback loop between data reflow and
model iteration.
4.5 Discussion of Source Availability
Considering the performance margin of SFADA over ADA, we take LADA, the
ADA SOTA, as an example for comparison to explore the role of source data in
adaptation. We first ablate the source domain loss from LADA, denoted as SF-
LADA to compare with LFTL. Given that the source data becomes unavailable,
each epoch now iterates over the labeled target dataset instead of the labeled
source dataset, the same as our LFTL. The averaged results in Tab. 5 show that,
without specifically designed hypothesis transfer strategy, removing the source
data from ADA methods will cause task models to forget the previously learnt
knowledge in the source domain during adaptation. And the newly acquired
information from the target domain is too limited to counteract it, resulting in
noticeable accuracy gaps. This further validates the necessity of learning from
the learnt under the source-free condition.
Then we add a simple cross-entropy loss for the source data to LFTL, namely
S-LFTL, while keeping the number of iterations unchanged. As shown in Tab. 5,
S-LFTL improves over LFTL, demonstrating that direct source supervision
could still provide more useful information than hypothesis transfer and memory
recollection.ThesourcedataprovesespeciallyeffectiveforsmallerOfficedatasets,
where the improvements are noticeable while the additional amount of additional
time is affordable. For the large dataset, although the performance of S-LFTL
doesnotmatchthatofLADAwithinjust6%ofitsadaptationtime,extendingthe
trainingiterationstoaccessallavailabledatawouldguaranteefurtherperformance
improvements.Title Suppressed Due to Excessive Length 15
Table 6: Comparison between the proposed CAS and active learning baselines in
terms of classification accuracy (%). Experiments are performed on the large-scale
VisDA-C dataset (ResNet101) with 1% budget in total for 10 cycles. The best results
are highlighted in bold.
Method 1 2 3 4 5 6 7 8 9 10
Random 67.0 70.6 73.7 76.6 78.7 79.9 81.1 81.8 82.2 82.7
Ent-max 62.2 67.9 73.7 77.6 79.9 80.5 80.2 80.2 80.0 80.5
Ent-min 60.4 66.4 64.5 64.2 63.8 64.0 64.1 63.5 64.6 63.9
Kcenter-greedy 67.5 71.3 74.5 77.4 78.9 80.6 81.4 82.3 82.7 83.5
Kmeans 60.5 67.3 72.9 75.7 77.5 78.7 79.7 80.4 80.9 81.2
Least-confidence 63.2 69.9 74.5 78.5 80.2 80.9 80.7 81.1 81.2 81.9
Bayesian 66.5 70.4 73.9 76.7 78.7 80.2 81.4 82.1 82.8 83.4
BvSB 65.7 72.2 77.0 79.8 81.4 82.5 83.4 84.0 84.6 85.1
LFTL 67.6 74.9 79.1 81.5 83.6 84.3 85.6 86.2 86.9 87.4
4.6 Comparison with Active Learning Baselines
To validate the proposed active query strategy, we compare LFTL with active
learning baselines. We choose Random, uncertainty-based EntMax [48], Least
Confidence [24], BvSB [24], Bayesian [9], distribution-based K-means [64] and
K-center greedy [47]. Experiments are performed on the large-scale VisDA-C
dataset with 1% annotation budget in total for 10 rounds. The results for each
round are listed in Tab. 6. It manifests that the proposed sampling strategy
consistently outperforms its substitutes. As the iteration proceeds, information
queried by other active learning methods become redundant, failing to fulfill the
requirements of the DA task. In comparison, the CAS and class-level transfer-
ability estimation we propose efficiently prioritize samples with local uncertainty,
global intransferability and temporal stagnancy along the sample-and-adaptation
progress, bringing continuous growth to the DA task.
5 Conclusion
Inthispaper,weinvestigatethechallengingsource-freeactivedomainadaptation
(SFADA) setting, where source data becomes inaccessible during adaptation, and
a minimum amount of annotation budget is available for the target domain. We
present a shared framework for both active sampling and domain adaptation
to learn from the hypotheses and feature representations of the learnt source
model and actively iterated models. The proposed CAS criterion effectively
prioritizessamplesthatarebothinformativetothecurrentmodelandpersistently
challenging throughout the iterative process, and their visual understandings of
actively queried samples are temporally preserved and exploited via the VPA
strategy during adaptation.
Extensive experiments on three DA benchmarks have shown that, in com-
parison with ADA and SFUDA, SFADA provides a worthy trade-off between
annotation costs and model performance in both accuracy and efficiency. In com-
parison with previous SFADA competitors, our LFTL framework also presents
superior performance and exhibits more flexibility than the one-round method.16 M. Lyu et al.
Acknowledgment
ThisworkwassupportedbyNationalScienceandTechnologyMajor(2021ZD0114703-
2), National Natural Science Foundation of China (Nos. 61925107, 62271281,
62021002).
References
1. Baum, E.B., Lang, K.: Query learning can work poorly when a human oracle is
used. In: International Joint Conference on Neural Networks. vol. 8, p. 8. Beijing
China (1992)
2. Bearman, A., Russakovsky, O., Ferrari, V., Fei-Fei, L.: What’s the point: Semantic
segmentation with point supervision. In: Proceedings of the European Conference
on Computer Vision (ECCV). pp. 549–565. Springer (2016)
3. Beluch, W.H., Genewein, T., Nürnberger, A., Köhler, J.M.: The power of ensem-
bles for active learning in image classification. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 9368–9377 (2018)
4. Cao, Z., Ma, L., Long, M., Wang, J.: Partial adversarial domain adaptation. In:
ProceedingsoftheEuropeanConferenceonComputerVision(ECCV).pp.135–150
(2018)
5. Chang, W.G., You, T., Seo, S., Kwak, S., Han, B.: Domain-specific batch normal-
ization for unsupervised domain adaptation. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 7354–7362 (2019)
6. Chuang, Y.S., Xie, Y., Luo, H., Kim, Y., Glass, J., He, P.: Dola: Decoding by
contrasting layers improves factuality in large language models. arXiv preprint
arXiv:2309.03883 (2023)
7. Ding, N., Xu, Y., Tang, Y., Xu, C., Wang, Y., Tao, D.: Source-free domain adapta-
tion via distribution estimation. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). pp. 7212–7222 (June 2022)
8. Fu,B.,Cao,Z.,Wang,J.,Long,M.:Transferablequeryselectionforactivedomain
adaptation.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
Pattern Recognition. pp. 7272–7281 (2021)
9. Gal,Y.,Islam,R.,Ghahramani,Z.:Deepbayesianactivelearningwithimagedata.
In: International Conference on International Conference on Machine Learning. pp.
1183–1192. PMLR (2017)
10. Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F.,
Marchand,M.,Lempitsky,V.:Domain-adversarialtrainingofneuralnetworks.The
Journal of Machine Learning Research 17(1), 2096–2030 (2016)
11. Gera, A., Friedman, R., Arviv, O., Gunasekara, C., Sznajder, B., Slonim, N.,
Shnarch, E.: The benefits of bad advice: Autocontrastive decoding across model
layers. In: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Proceedings of the 61st
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). pp. 10406–10420. Association for Computational Linguistics, Toronto,
Canada (Jul 2023)
12. Hao, T., Ding, X., Feng, J., Yang, Y., Chen, H., Ding, G.: Quantized prompt
for efficient generalization of vision-language models. In: European Conference on
Computer Vision (ECCV). Springer (2024)
13. He, G., Liu, X., Fan, F., You, J.: Classification-aware semi-supervised domain
adaptation.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
Pattern Recognition Workshops. pp. 964–965 (2020)Title Suppressed Due to Excessive Length 17
14. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 770–778 (2016)
15. Hu, W., Miyato, T., Tokui, S., Matsumoto, E., Sugiyama, M.: Learning discrete
representations via information maximizing self-augmented training. In: Interna-
tionalConferenceonInternationalConferenceonMachineLearning.pp.1558–1567.
PMLR (2017)
16. Huang, J., Guan, D., Xiao, A., Lu, S.: Model adaptation: Historical contrastive
learning for unsupervised domain adaptation without source data. Advances in
Neural Information Processing Systems 34 (2021)
17. Hwang,U.,Lee,J.,Shin,J.,Yoon,S.:SF(DA)$^2$:Source-freedomainadaptation
through the lens of data augmentation. In: The Twelfth International Conference
on Learning Representations (2024)
18. Jin, Y., Wang, X., Long, M., Wang, J.: Minimum class confusion for versatile
domain adaptation. In: Proceedings of the European Conference on Computer
Vision (ECCV). pp. 464–480. Springer (2020)
19. Joshi, A.J., Porikli, F., Papanikolopoulos, N.: Multi-class active learning for image
classification. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 2372–2379. IEEE (2009)
20. Kim,Y.,Cho,D.,Han,K.,Panda,P.,Hong,S.:Domainadaptationwithoutsource
data. IEEE Transactions on Artificial Intelligence 2(6), 508–518 (2021)
21. Kothandaraman, D., Shekhar, S., Sancheti, A., Ghuhan, M., Shukla, T., Manocha,
D.: Salad: Source-free active label-agnostic domain adaptation for classification,
segmentation and detection. In: Proceedings of the IEEE/CVF Winter Conference
on Applications of Computer Vision. pp. 382–391 (2023)
22. Kundu,J.N.,Venkat,N.,Babu,R.V.,etal.:Universalsource-freedomainadaptation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 4544–4553 (2020)
23. Leng, S., Zhang, H., Chen, G., Li, X., Lu, S., Miao, C., Bing, L.: Mitigating object
hallucinations in large vision-language models through visual contrastive decoding.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) (2024)
24. Lewis, D.D., Gale, W.A.: A sequential algorithm for training text classifiers. In:
Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on
Research and Development in Information Retrieval. pp. 3–12. Springer (1994)
25. Li, R., Jiao, Q., Cao, W., Wong, H.S., Wu, S.: Model adaptation: Unsupervised do-
mainadaptationwithoutsourcedata.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 9641–9650 (2020)
26. Li, S., Liu, C.H., Lin, Q., Wen, Q., Su, L., Huang, G., Ding, Z.: Deep residual
correction network for partial domain adaptation. IEEE Transactions on Pattern
Analysis and Machine Intelligence 43(7), 2329–2344 (2020)
27. Li,X.L.,Holtzman,A.,Fried,D.,Liang,P.,Eisner,J.,Hashimoto,T.,Zettlemoyer,
L., Lewis, M.: Contrastive decoding: Open-ended text generation as optimization.
In: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Proceedings of the 61st Annual
MeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers).
pp. 12286–12312. Association for Computational Linguistics, Toronto, Canada (Jul
2023)
28. Liang, J., Hu, D., Wang, Y., He, R., Feng, J.: Source data-absent unsupervised
domain adaptation through hypothesis transfer and labeling transfer. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence (2021)18 M. Lyu et al.
29. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: Proceedings of the
European Conference on Computer Vision (ECCV). pp. 740–755. Springer (2014)
30. Luo, Y., Liu, P., Guan, T., Yu, J., Yang, Y.: Adversarial style mining for one-
shot unsupervised domain adaptation. Advances in Neural Information Processing
Systems 33, 20612–20623 (2020)
31. Lyu,M.,Zhou,J.,Chen,H.,Huang,Y.,Yu,D.,Li,Y.,Guo,Y.,Guo,Y.,Xiang,L.,
Ding, G.: Box-level active detection. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 23766–23775 (2023)
32. Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine
learning research 9(11) (2008)
33. Melville,P.,Mooney,R.J.:Diverseensemblesforactivelearning.In:Proceedingsof
the Twenty-First International Conference on Machine Learning. p. 74. ICML ’04,
Association for Computing Machinery, New York, NY, USA (2004)
34. Motiian, S., Jones, Q., Iranmanesh, S., Doretto, G.: Few-shot adversarial domain
adaptation. Advances in Neural Information Processing Systems 30 (2017)
35. Panareda Busto, P., Gall, J.: Open set domain adaptation. In: IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV). pp. 754–763 (2017)
36. Papadopoulos, D.P., Clarke, A.D., Keller, F., Ferrari, V.: Training object class
detectors from eye tracking data. In: Proceedings of the European Conference on
Computer Vision (ECCV). pp. 361–376. Springer (2014)
37. Peng, K.C., Wu, Z., Ernst, J.: Zero-shot deep domain adaptation. In: Proceedings
of the European Conference on Computer Vision (ECCV). pp. 764–781 (2018)
38. Peng, X., Usman, B., Kaushik, N., Wang, D., Hoffman, J., Saenko, K.: Visda: A
synthetic-to-real benchmark for visual domain adaptation. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops.
pp. 2021–2026 (2018)
39. Prabhu,V.,Chandrasekaran,A.,Saenko,K.,Hoffman,J.:Activedomainadaptation
via clustering uncertainty-weighted embeddings. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision. pp. 8505–8514 (2021)
40. Qiu, Z., Zhang, Y., Lin, H., Niu, S., Liu, Y., Du, Q., Tan, M.: Source-free domain
adaptation via avatar prototype generation and adaptation. In: International Joint
Conference on Artificial Intelligence (2021)
41. Rai, P., Saha, A., Daumé III, H., Venkatasubramanian, S.: Domain adaptation
meets active learning. In: Proceedings of the NAACL HLT 2010 Workshop on
Active Learning for Natural Language Processing. pp. 27–32 (2010)
42. Saenko, K., Kulis, B., Fritz, M., Darrell, T.: Adapting visual category models to
new domains. In: Proceedings of the European Conference on Computer Vision
(ECCV). pp. 213–226. Springer (2010)
43. Saha, A., Rai, P., Daumé, H., Venkatasubramanian, S., DuVall, S.L.: Active super-
vised domain adaptation. In: Joint European Conference on Machine Learning and
Knowledge Discovery in Databases. pp. 97–112. Springer (2011)
44. Saito, K., Kim, D., Sclaroff, S., Darrell, T., Saenko, K.: Semi-supervised domain
adaptation via minimax entropy. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 8050–8058 (2019)
45. Saito,K.,Kim,D.,Sclaroff,S.,Saenko,K.:Universaldomainadaptationthroughself
supervision. Advances in Neural Information Processing Systems 33, 16282–16292
(2020)
46. Saito, K., Yamamoto, S., Ushiku, Y., Harada, T.: Open set domain adaptation by
backpropagation. In: Proceedings of the European Conference on Computer Vision
(ECCV). pp. 153–168 (2018)Title Suppressed Due to Excessive Length 19
47. Sener,O.,Savarese,S.:Activelearningforconvolutionalneuralnetworks:Acore-set
approach. In: International Conference on Learning Representations (2018)
48. Settles, B.: Active learning literature survey (2009)
49. Settles, B.: Active learning literature survey. Computer Sciences Technical Re-
port 1648, University of Wisconsin–Madison (2009)
50. Shi,Y.,Sha,F.:Information-theoreticallearningofdiscriminativeclustersforunsu-
pervised domain adaptation. In: Proceedings of the 29th International Conference
on International Conference on Machine Learning. pp. 1275–1282 (2012)
51. Shu,R.,Bui,H.H.,Narui,H.,Ermon,S.:Adirt-tapproachtounsuperviseddomain
adaptation. In: Proc. 6th International Conference on Learning Representations
(2018)
52. Su,J.C.,Tsai,Y.H.,Sohn,K.,Liu,B.,Maji,S.,Chandraker,M.:Activeadversarial
domain adaptation. In: Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision. pp. 739–748 (2020)
53. Sun, T., Lu, C., Ling, H.: Local context-aware active domain adaptation. In: 2023
IEEE/CVFInternationalConferenceonComputerVision(ICCV).pp.18588–18597
(2023)
54. Tan, S., Jiao, J., Zheng, W.S.: Weakly supervised open-set domain adaptation
by dual-domain collaboration. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 5394–5403 (2019)
55. Tzeng, E., Hoffman, J., Saenko, K., Darrell, T.: Adversarial discriminative domain
adaptation.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
Pattern Recognition. pp. 7167–7176 (2017)
56. Venkateswara, H., Eusebio, J., Chakraborty, S., Panchanathan, S.: Deep hashing
network for unsupervised domain adaptation. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 5018–5027 (2017)
57. Wang, F., Han, Z., Zhang, Z., He, R., Yin, Y.: Mhpl: Minimum happy points
learningforactivesourcefreedomainadaptation.In:ProceedingsoftheIEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 20008–20018 (2023)
58. Wang, J., Jiang, J.: Conditional coupled generative adversarial networks for zero-
shotdomainadaptation.In:ProceedingsoftheIEEE/CVFInternationalConference
on Computer Vision. pp. 3375–3384 (2019)
59. Xia, H., Zhao, H., Ding, Z.: Adaptive adversarial network for source-free domain
adaptation. In: Proceedings of the IEEE/CVF International Conference on Com-
puter Vision. pp. 9010–9019 (2021)
60. Xie, M., Li, Y., Wang, Y., Luo, Z., Gan, Z., Sun, Z., Chi, M., Wang, C., Wang, P.:
Learning distinctive margin toward active domain adaptation. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
pp. 7993–8002 (2022)
61. Xiong, Y., Chen, H., Lin, Z., Zhao, S., Ding, G.: Confidence-based visual dispersal
for few-shot unsupervised domain adaptation. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision. pp. 11621–11631 (2023)
62. Xu, X., Zhou, X., Venkatesan, R., Swaminathan, G., Majumder, O.: d-sne: Do-
main adaptation using stochastic neighborhood embedding. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
2497–2506 (2019)
63. Xu, X., Chen, H., Lin, Z., Han, J., Gong, L., Wang, G., Bao, Y., Ding, G.: Tad: A
plug-and-play task-aware decoding method to better adapt llms on downstream
tasks. In: Proceedings of the Thirty-Third International Joint Conference on Ar-
tificial Intelligence, IJCAI 2024, Jeju, South Korea, 3-9 August 2024. ijcai.org
(2024)20 M. Lyu et al.
64. Yan,D.,Huang,L.,Jordan,M.I.:Fastapproximatespectralclustering.In:Proceed-
ings of the 15th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining. pp. 907–916 (2009)
65. Yang, B., Yeh, H.W., Harada, T., Yuen, P.C.: Model-induced generalization error
bound for information-theoretic representation learning in source-data-free unsu-
pervised domain adaptation. IEEE Transactions on Image Processing 31, 419–432
(2021)
66. You, K., Long, M., Cao, Z., Wang, J., Jordan, M.I.: Universal domain adaptation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 2720–2729 (2019)
67. Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk
minimization. In: International Conference on Learning Representations (2018)
68. Zhang, Z., Chen, W., Cheng, H., Li, Z., Li, S., Lin, L., Li, G.: Divide and contrast:
Source-free domain adaptation via adaptive contrastive learning. In: Oh, A.H.,
Agarwal,A.,Belgrave,D.,Cho,K.(eds.)AdvancesinNeuralInformationProcessing
Systems (2022)Title Suppressed Due to Excessive Length 1
Learn from the Learnt: Source-Free Active
Domain Adaptation via Contrastive Sampling and
Visual Persistence
Supplementary Material
Round 0 Round 1 Round 2 Round 3 Round 4
Round 5 Round 6 Round 7 Round 8 Round 9
plane bcycl bus car horse knife mcycl person plant sktbrd train truck
Fig.5: t-SNEvisualizationofunlabeledtargetsamples(coloredbyclasses)andactively
queried samples (marked by cross) on VisDA-C with 0.1% budget per round.
A Qualitative Analysis
A.1 t-SNE Visualization
To visualize the task model’s comprehension of the target data and the selection
of informative samples during the iterative process of active learning and domain
adaptation,wepresentthet-SNE[32]plotsfortheVisDA-CdatasetinFig.5.For
the sake of visual clarity, only a subset of unlabelled data is randomly sampled
and depicted in each round.
We first observe that, as the volume of actively selected samples increases,
the classification boundaries are progressively revealed, indicating that the task
model’s understanding of the target data distribution has become more compre-
hensive. As our CAS is motivated, under the domain shift problem, unlabeled
samples we identify as challenging to transfer are proximate to decision bound-
aries in the target domain, where our active selection often happens. At the
same time, our sampling process prioritizes minority classes (e.g., knife), which
is contributed to our class-level transferability estimation.
A.2 Queried Target Samples
We further validate the efficacy of proposed CAS via visual inspection of the
queriedtargetsamples.Fig.6showstop-5activelyqueriedsamplesofeachactive2 M. Lyu et al.
Cycle 0 Cycle 1 Cycle 2 Cycle 3 Cycle 4 Cycle 5 Cycle 6 Cycle 7 Cycle 8 Cycle 9
person plant train bcycl mcycl truck truck person person car
bcycl truck bus sktbrd truck mcycl mcycl car person bus
car knife train sktbrd car bus truck person person truck
person person car person person train truck horse mcycl knife
person car train knife truck bcycl bus car sktbrd person
Fig.6: Top-5 CAS samples of 10 rounds on VisDA-C.
learningroundonVisDA-C.Basedontherecognitionresultsfromthesource-only
model, person (19.2%), truck (6.1%) and knife (4.9%) are challenging classes. We
can observe that more iconic and representative samples are selected in order
to learn authentic features that differ from those of the generated images. On
the other hand, for more transferable classes, e.g., car (80.0%) and motorcycle,
samples that are truncated, occluded, distant and blurry are queried at first.
Those patterns are not present in the source domain but are ubiquitous in real
world scenarios, which provide rich information for the adaptation.
We have also noted that in the later stages of iteration, our CAS not only
selects samples that are prone to confusion but is also able to identify instances
with inaccurate labels. For example, the image where human hands holding
apples is labeled as person, or the case where the foreground is a person but the
label corresponds to a blurry truck in the background.
B Component Verification
B.1 Ablation Study
Take the combination of vannila BvSB [24] sampling strategy and labeled target
supervision via CE loss as a baseline, we present the ablation study of the
propose LFTL framework in Tab. 7. On VisDA-C and Office-31 datasets with
differing budget constraints, consistent improvements can be observed with each
component, which validate our motivations. Given the same annotation budget,
our CAS strategy prioritizes target samples that remain unrecognizable to theTitle Suppressed Due to Excessive Length 3
Table 7: Ablation study with 1%-labeled VisDA-C and 5%, 10%-labeled Office-31.
u u L L 1%-VisDA-C 5%-O3110%-O31
cm ct ac vpa
85.1 89.2 93.1
✓ 86.7 91.4 94.8
✓ ✓ 86.9 91.8 95.1
✓ ✓ ✓ 87.0 92.0 95.3
✓ ✓ ✓ 87.4 93.5 95.4
Table 8: Results of applying CAS to SFUDA SOTAs on VisDA-C.
AS CAS SHOT SHOT++ SF(DA)2 MHPL LFTL
0 85.5 87.3 88.1 - -
(cid:37)
1 ✓ 86.4 87.5 88.9 - 87.4
5 ✓ 91.9 91.8 92.3 91.3 92.8
current model and have not been captured in preceding active learning rounds,
and meantime it factors out samples with knowledge previously acquired when
thehypothesesexhibitincreasedconfidence.Inadditiontothecontrastivemargin
u , the class-level transferability u enhances our sampling criterion with a
cm ct
global semantic perspective, which promotes the selection of tail and challenging
classes. The actively selected target samples then play the role of anchors to
guide the optimization during the adaptation procedure. Without explicit error-
pronepseudo-labelingortime-consumingclustering,theL efficientlyencourages
ac
densityaroundactiveanchors.WhenreplacedwithfeatureswithVPdeliveredvia
L , the representations of active anchors are features temporally ensembled
VPA
from the source model to the current, which efficiently promotes a source-like
feature distribution in the target domain.
B.2 CAS for SFUDA SOTAs
This section further verifies CAS by applying it to SFUDA methods to facilitate
their adaptation. Here we take SOTAs SHOT and SF(DA)2 for example. Results
in Tab. 8 show that CAS can be simply plugged into source-free methods to
query informative target samples for a performance boost. Meantime, a better
approachistosimultaneouslyconsidertheriskofforgettingthedomain-invariant
knowledge during transfer and thereby incorporating our VPA strategy to learn
from the learnt.
B.3 Parameter Sensitivity Analysis
To validate the effectiveness and generalization ability of the proposed method,
westudythesensitivityofLFTLtoαinEq.1,κinEq.3,andβ ,β inEq.10on
1 2
the VisDA-C dataset. We experiment around the optimal values of parameters,
performthreetrialswithasetofseedsandaveragetheresults.InFig.7,weobserve4 M. Lyu et al.
88.0 88.0 88.0 1 88.0 2
87.5 87.5 87.5 87.5
87.0 87.0 87.0 87.0
86.5 86.5 86.5 86.5
86.0 86.0 86.0 86.0
0.01 0.02 0.03 0.04 0.05 300 400 500 600 700 8 9 10 11 12 0.8 0.9 1.0 1.1 1.2
Fig.7: ParametersensitivityanalysisofαinEq.1,κinEq.3,andβ ,β inEq.10on
1 2
VisDA-C.
similar bell-shaped curves on all experiments, indicating consistent performance
gains benefit from our proposed methods and demonstrating robustness to
parameter choices.
C Additional Implementation Details
During the sample-and-adaptation interplay, the SGD optimizer with a momen-
tum of 0.9 and a weight decay of 1e−3 is applied. The learning rate is set as
1e−3 for VisDA-C and 1e−2 for Office, scheduled by η =η (1+10p)−0.75 where
0
p increases from 0 to 1 during optimization. We set batch size as 64. For all
datasets, the CAS coefficient α=0.03, β ,β in model adaptation are set to 10.0
1 2
and 0.9 to balance the magnitude of each loss terms. We set κ = 500 for the
large-scale VisDA-C and 100 for smaller Office datasets.
)%(
ycaruccA VisDA-C