A Scalable Quantum Non-local Neural Network for Image Classification
SparshGupta1,DebanjanKonar2,VaneetAggarwal2
1FranklinW.OlinCollegeofEngineering,Needham,MA02492USA
2PurdueUniversity,WestLafayette,IN47907USA
sgupta1@olin.edu,dkonar@purdue.edu,vaneet@purdue.edu
Abstract theselimitations,non-localneuralnetworkswereintroduced
tocapturelong-rangedependenciesindata,anextensionof
Non-localoperationsplayacrucialroleincomputervision
theself-attentionmechanismusedinTransformerarchitec-
enabling the capture of long-range dependencies through
tures(Vaswanietal.2017).Initiallyproposedforcomputer
weightedsumsoffeaturesacrosstheinput,surpassingthecon-
visionapplicationsby(Wangetal.2018),non-localneural straintsoftraditionalconvolutionoperationsthatfocussolely
onlocalneighborhoods.Non-localoperationstypicallyrequire networkscaptureglobalcontexteffectivelyandhaveshown
computingpairwiserelationshipsbetweenallelementsinaset, significantimprovementsincomputervisiontasksthatbene-
leadingtoquadraticcomplexityintermsoftimeandmemory. fitfrommodelinglong-rangedependencies.
Duetothehighcomputationalandmemorydemands,scaling Paralleltotheseadvancements,quantummachinelearning
non-localneuralnetworkstolarge-scaleproblemscanbechal- (QML)(Biamonteetal.2017)hasemergedasarevolutionary
lenging.Thisarticleintroducesahybridquantum-classical technologybuiltupontheprinciplesofquantummechanics,
scalable non-local neural network, referred to as Quantum
whichdescribesthebehaviorandnatureofatomsatthesmall-
Non-LocalNeuralNetwork(QNL-Net),toenhancepattern
estfundamentallevelandappliesthemtomachinelearning.
recognition.TheproposedQNL-Netreliesoninherentquan-
Inclassicalcomputing,informationonlyexistsinbits,which
tumparallelismtoallowthesimultaneousprocessingofalarge
are 0 or 1. In contrast, quantum computing introduces the
numberofinputfeaturesenablingmoreefficientcomputations
inquantum-enhancedfeaturespaceandinvolvingpairwise conceptofqubitsthatcanhavequantumstates|0⟩and|1⟩
relationshipsthroughquantumentanglement.Webenchmark simultaneously,takingadvantageoftheconceptofsuperpo-
ourproposedQNL-Netwithotherquantumcounterpartsto sitionandintroducingfeaturessuchasquantumparallelism
binaryclassificationwithdatasetsMNISTandCIFAR-10.The (Nielsen and Chuang 2001). Theoretically, this allows for
simulationfindingsshowcaseourQNL-Netachievescutting- speedingupcomputationsanddevisingalgorithmsthatcan
edge accuracy levels in binary image classification among solvecomplexchallengesmoreefficientlycomparedtoclas-
quantumclassifierswhileutilizingfewerqubits.
sicalcomputingwherecomputationmightbeexpensiveand
inefficient, and potentially revolutionize fields like pattern
Code—https://anonymous.4open.science/r/QNL-Net/
recognition and image classification, optimization (Abbas
etal.2023),cryptography(BernsteinandLange2017),etc.
Introduction QMLalsointroducesalgorithmsforclassificationproblems
suchasQSVM,QuantumKernelmethods,VariationalQuan-
Computervisionhasbecomeacornerstoneofartificialintel-
tumClassifiers(VQC),etc.VQCisaparticularlyinteresting
ligence,consistingofawidearrayofapplicationssuchasau-
approachwithinQMLbecauseitletsuscombineclassical
tonomousdriving(Baoetal.2023),medicalimaging(Lietal.
andquantumcomputingtouseaquantumcircuitasthecore
2023),healthcare(Zhouetal.2022),etc.Anessentialtaskin
algorithmconsistingofquantumgatesthatcanbeparameter-
thisdomainisimageclassification,wherethegoalistoassign
ized(Benedettietal.2019;Peddireddy,Bansal,andAggar-
alabeltoanimagebasedonitsvisualsignificance.Thistask
wal2023).Theseparameterscanbeoptimizedusingclassi-
alsobuildsuponmorecomplexapplicationssuchasimage
calmethods,enablingthemodeltobetrainedwithahybrid
segmentation(Minaeeetal.2021),objectdetection(Amjoud
quantum-classicalapproach,whichweutilizeinourwork.
andAmrouch2023;Fuetal.2023),andsceneunderstand-
Researchers have still been trying to understand whether
ing(Naseer,Khan,andPorikli2018).Imageclassification
QML offers a significant advantage to classical machine
approaches have been significantly advanced by Convolu-
learning theoretically and practically, and it has been an
tional Neural Networks (CNNs) (O’shea and Nash 2015),
active field of research for long (Biamonte et al. 2017). A
whichachievestate-of-the-artperformanceondatasetslike
sub-domaininthisfield,QuantumNeuralNetworks(QNNs),
ImageNet(Krizhevsky,Sutskever,andHinton2012).How-
hasbeendevelopedintensivelytodeterminewhetherthese
ever,CNNsarelimitedbytheirlocalreceptivefields,which
are capable of outperforming classical neural networks. It
restrictsthemfromcapturingbroadercontextualinformation
wasalsorecentlyexploredin(Abbasetal.2021)byperform-
andlong-rangedependencieswithinanimage.Toovercome
ing simulations on actual quantum hardware and proving
4202
luJ
62
]VC.sc[
1v60981.7042:viXrathatQMLdoes,infact,offerseveraladvantages.However,a 2020)in2019,whichemployedquantumconvolution(quan-
fewchallengesexistduetoinsufficientcapabilitiesforfault- volutional) layers. These layers transformed classical data
tolerance,still-evolvingquantumerrorcorrectiontechniques, usingrandomquantumcircuitstoextractfeatures,akintothe
andquantumscalability,whichpresentsanavenueforrevo- featureextractionprocessinclassicalCNNs.Theirresults
lutionaryresearchinthisfieldandtransitioningtoanewera demonstrated superior accuracy and training performance
ofquantumcomputingfromthepresentnoisyintermediate- comparedtoclassicalCNNs.QMLmodelshavealsoshown
scalequantum(NISQ)era(Preskill2018;Gujju,Matsuo,and efficacyinbinaryclassificationtasksfornoisydatasetsand
Raymond2024).Thepresentchallengeslimitthepractical images(Schetakisetal.2022).Additionally,recentworkby
applications of QML algorithms. Still, despite that, devel- (Cherratetal.2024)developedanapproachforloadingma-
opingQMLresearchanddevisingnewquantumalgorithms tricesasquantumstatesandintroducingtrainablequantum
offersanoutlettotestmethodologiesasaproof-of-concept orthogonallayersadaptabletodifferentquantumcomputer
onasmallerscaleanddeployapplicationsinthefuturewhile capabilities.Thismethodyieldedpromisingresultsonsuper-
weapproachscalablequantumprocessors. conductingquantumcomputers.
Inthiscontext,ourpaperproposesaQuantumNon-localNeu- Severalworksalsoexploredtheapplicationofquantumneu-
ralNetwork(QNL-Net),whichcombinestheprinciplesof ralnetworksforbinaryclassificationtasksincomputervision,
quantumcomputingwiththenon-localneuralnetworkmech- providingbenchmarksforourresults.QTN-VQC(Qi,Yang,
anism.Byleveragingquantumentanglement,theQNL-Net andChen2023)builtaframeworkwithquantumcircuitsfor
can establish non-local correlations between qubits, mim- tensor-train networks integrated with variational quantum
ickingthebehaviorofclassicalnon-localoperationswhile circuitsforanefficienttrainingpipeline.Anotherworkin-
exploitingtheadvantagesofquantummechanicstoenhance troducedhierarchicalquantumclassifiers(Grantetal.2018),
machinelearningperformanceandcapabilities.Thispaper whichutilizedseveralexpressivecircuitstoclassifyhighly
aimstoimprovepatternrecognitionandbinaryclassification entangled quantum states and demonstrated robustness to
tasksincomputervisionbycapturinglong-rangedependen- noise. A scalable approach for quantum neural networks
ciesmoreeffectively.Themaincontributionsofthispaper (SQNNs)forclassificationwasdiscussedin(Wu,Tao,and
are: Li2022),whereauthorsproposedastrategytousemultiple
small-scalequantumdevicestoextractlocalfeaturesandper-
1. Weintroduceascalableandcustomizablequantumim-
formpredictionoverthesecollectedfeatures.(Jiang,Xiong,
plementationofthenon-localneuralnetworkmechanism
and Shi 2021) presented the QuantumFlow model, which
that utilizes only four qubits to achieve state-of-the-art
representeddataasunitarymatricestoachievequantumad-
performance on binary classification tasks in computer
vantage, reducing the cost complexity of unitary matrices-
vision.
based neural computation. These recent advancements in
2. Wealsoutilizeclassicalmachinelearningtechniquesfor
QMLmodelsdemonstratedrobustnessinimageclassifica-
dimensionalityreductionoffeaturesbeforethesearepro-
tion,makingthemsuitableforhandlinghigher-dimensional
cessedbytheQNL-Netframework,thereforeleveraging
datamoreeffectivelythantheirclassicalcounterparts.
theadvantagesofhybridquantum-classicalmachinelearn-
ingmodels.
Non-localNeuralNetworks
3. WecomprehensivelyevaluatetheQNL-Net’sperformance
Traditional convolution operations in convolutional neural
onbenchmarkdatasetssuchasMNISTandCIFAR-10,
networks(CNNs),apopularchoiceforcomputervisionmod-
showcasingitsrobustnesstonoiseanditspotentialadvan-
els,processalocalneighborhood,andapplyingtheseopera-
tagesovertraditionalquantumbinaryclassificationmod-
tionsrepeatedlytocapturelong-rangedependenciescauses
els. Our hybrid classical-quantum models, particularly
incremental growth in the receptive field. This has several
theCNN-QNL-Net,achievednear-perfectbinaryclassi-
drawbacksassociatedwithcomputationalinefficiencyand
fication accuracies on the MNIST dataset (99.96% test
difficulties in optimization. Non-local neural networks ad-
accuracy)andoutperformedbenchmarkquantumclassi-
dresstheselimitationsbyintroducingnon-localoperations
fierssuchasQTN-VQC(98.6%with12qubits)(Qi,Yang,
thatcomputetheresponseatapositionasaweightedsum
andChen2023)andHybridTTN-MERA(99.87%with8
ofthefeaturesatallpositionsintheinput.Thesearesimple
qubits)(Grantetal.2018)usingsignificantlyfewerqubits,
operationsthatarehighlyefficientandgenericincapturing
i.e.,4qubits.
long-rangedependencies,whichisofutmostimportancein
computervision(Wangetal.2018).
RelatedWork
Agenericnon-localoperationforaninputsignal’s(image,
Imageclassificationusingquantummachinelearning(QML) sequence, video) feature map x ∈ RN×C, where N is the
becameanareaofsignificantinterestduetoitspotentialad- number of positions (i.e. pixels) and C is the number of
vantagesoverclassicalmethods.Onenotableapproachwas channels,canbedefinedas:
QuantumConvolutionalNeuralNetworks(QCNNs),which
utilizedquantumcircuitstoimplementconvolutionalopera- 1 (cid:88)
y = f(x ,x )g(x ), (1)
tions,focusingonquantumphaserecognitionandquantum i C(x) i j j
∀j
errorcorrectionoptimizationtechniques(Cong,Choi,and
Lukin2019).AnothersignificantdevelopmentwastheQuan- whereiistheindexofanoutputposition(inspace/time/s-
volutionalNeuralNetwork,introducedby(Hendersonetal. pacetime)whoseresponseistobecomputed,j enumeratesover all possible positions, and y is the output signal. f is
apairwisefunctionthatcomputesascalarrepresentingthe
relationship(suchassimilarity)betweeniandallj.g isa
unaryfunctionthatcomputesarepresentationoftheinput
signalatj andnormalizesitbyafactorC(x).
Intermsofthefunctions,gissimplyconsideredasalinear
embeddingsuchthatg(x )=W x ,whereW isalearned
i g j g
weight matrix. There are several choices for the pairwise
functionf suchas:
1. Gaussian:
f(x i,x j)=exT ixj, C(x)=(cid:88) f(x i,x j);
∀j
Figure1:TheQuantumNon-localNeuralNetwork(QNL-
2. EmbeddedGaussian:
Net)mechanism’sfour-qubitcircuitcomposedofthreeparts:
f(x ,x )=eθ(xi)Tϕ(xj), C(x)=(cid:88) f(x ,x ); (i) Encoder: uses Qiskit’s ZFeatureMap (Kanazawa et al.
i j i j
2023)toencodeclassicaldataintoquantumstates.(ii)Vari-
∀j
ational Quantum Circuit (VQC): the classically trainable
whereθ(x i) = W θx i andϕ(x j) = W ϕx j arelinearem- quantumcircuit.(iii)Measurement:thecircuitismeasured
beddings.Thisalsorelatestoself-attention(asexploredin at qubit 0 in the Pauli-Z basis. The encoder and the VQC
(Vaswanietal.2017)),whichnon-localnetworksarejust ansatzhaveadepthofrandDrespectively.
anextensionofinthecomputervisiondomain,ormore
specifically,agenericspaceorspacetimedomain.Thisis
duetothefactthatforanyi, C(1 x)f(x i,x j)becomesthe andcircuitsthatcanreplicatethefunctionalityofclassical
softmaxcomputationalongthedimensionj. non-locallayers,allowingthenetworktoanalyzecomplex
3. DotProduct: datastructuresmoreefficiently.Toencodetheclassicaldata
X ∈RdintothequantumspacewhereX =[y ,y ,··· ,y ],
f(x ,x )=θ(x )Tϕ(x ), C(x)=N; 0 1 k
i j i j weutilizeQiskit’sZFeatureMap(Kanazawaetal.2023)(as
whereN isthenumberofpositionsinx. onecanseeinFig.1)whichtakesadvantageofthequantum-
enhancedfeaturespace,providingaquantumadvantageto
4. Concatenation:
classificationproblems(Havlícˇeketal.2019).Thefeature
f(x ,x )=ReLU(wT[θ(x )T,ϕ(x )]), C(x)=N; mapΦactsasΦ:X →|ψ ⟩,wherethefeaturespace|ψ ⟩
i j f i j Φ Φ
isann-qubitHilbertspace,suchthat
Anon-localblockinspacetimeencapsulatesthenon-local
operationineq.(1)elegantlyandcanbedefinedas: |ψ Φ⟩=(U Φ(y k)H⊗n)r|0n⟩, 0≤k ≤N −1, (3)
z =W y +x (2) whereristhedepthorthenumberofrepetitionsofthefeature
i i i i
map,H⊗nisalayerofHadamardgatesactingonallnqubits,
wherey iisthenon-localoperationandresidualconnection andN denotesthenumberofinputvectorsinadatasample.
‘+x i’ allows integrating the non-local block into any pre- U Φistheencodingansatzusedinthefeaturemapcomposed
trained model without disruptions in their initial behavior ofsingle-qubitphase-shift(P )gatesforrotationaboutthe
λ
(Heetal.2016). Z-axis,
n
QuantumNon-localNeuralNetwork (cid:79)
U (X)= P . (4)
Inthiswork,weintroducetheQuantumNon-LocalNeural
Φ 2∗yk
k=1
Network(QNL-Net),whichutilizestrainablequantumcir-
cuitstoimplementnon-localoperations,effectivelycapturing Weutilizevariationalquantumcircuitsinthismechanism,
andprocessinglong-rangedependenciesininputdata.The whichcanbetrainedclassicallytooptimizetrainableparam-
QNL-Netmoduleintegrateswithclassicaldimensionalityre- etersusingapredefinedcostfunction(Benedettietal.2019).
ductiontechniquestofunctionasahybridquantum-classical Thishybridquantum-classicalapproachleveragestheexpres-
classifier.Inthissection,wefirstdelveintothedesignandim- sivepowerofquantumcircuitswhilemakinguseofclassical
plementationoftheQNL-Netmodule.Next,wediscussthe optimizationtechniquesforefficientparametertuning.We
integrationofclassicaldimensionalityreductiontechniques implementthreedifferentansatzeswithfiveparameterized
withtheQNL-Nettocreateahybridclassifier,highlighting rotation gates in each, enabling fine-grained control over
theCNN-QNL-NetandPCA-QNL-Netmodels.Finally,we thequantumstate.ThesethreeansatzeachhavethreeCX
coverthepost-QNL-Netclassicalcomputation. gateswithdistinctconfigurations,introducingentanglement-
TheQNL-Netmechanismtranslatesclassicalnon-localoper- induced correlations between the qubits and ensuring that
ationsintoquantumcircuits,enablingthenetworktoexploit thequantumstatecapturesthenon-localdependenciesinthe
theparallelismandentanglementpropertiesofquantumcom- data.Thequantumcircuitsfortheansatzesarepresentedin
puting. This translation involves designing quantum gates Fig.2.Figure2:Thethreeansatzesusedasthevariationalquantumcircuits(VQCs)intheQNL-Netmechanism.R ,R ,R rotation
x y z
gatesrepresentsingle-qubitrotationsalongthex,y,z-axesrespectivelywithtrainableparameters.Thespecificstrategiesfor
performingentanglementusingCXgatesare:cyclicpattern(Ansatz-0),reverselinearchain(Ansatz-1),andamixedpattern
(Ansatz-2).
We initially apply rotation gates to each qubit in all the intothestate|1⟩withprobability|β|2(NielsenandChuang
ansatzestoobtainthefollowingquantumstate. 2001).Ingeneral,theexpectationofanyobservableOˆ fora
state|ψ⟩canbedenotedas
|ψ ⟩=R (x ) R (θ ) R (ϕ ) R (g ) |ψ ⟩ (5)
1 z 0 q0 y 0 q1 y 0 q2 x 0 q3 Φ
Thevariationintheansatzescomesfromdifferententangle-
⟨Oˆ⟩=⟨ψ|Oˆ|ψ⟩=(cid:88)
m ip i (12)
mentstrategiesdescribedbelow.Theconfigurationsensure i
thatallqubitsareentangledwitheachotherinordertocap- where m are the possible measurement values, i.e., the
i
ture the non-local dependencies throughout the circuit. In eigenvaluesweightedbytheirrespectiveprobabilitiesp =
i
Ansatz-0,theCXgatesformacyclicpattern,creatingaloop |α|2 −|β|2. Therefore, to measure an observable O on n-
ofentanglementamongthequbits.Ansatz-1’sCNOTgates qubits,wecanalsorepresentitasasumoftensorproducts
formareverselinearchain,creatingabackwardsequential ofPaulioperators,suchthat,
entanglement.Ansatz-2formsanon-linearanduniquemixed
pattern entanglement strategy using the CNOT gates. The (cid:88)K
O = α P , α ∈R, (13)
resultantquantumstatesfortherespectiveansatzareasfol- k k k
lows. k=1
|ψ[0]⟩=CX CX CX |ψ ⟩, (6) whereP k ∈ {I,X,Y,Z}⊗n denotesaPauliobservable.In
2 q1q2 q2q3 q3q0 1 ourmechanism,wemeasurethecircuitinthePauli-Z com-
putationalbasisatonlyonequbit,q ,whereastherestofthe
0
|ψ 2[1]⟩=CX q3q2CX q2q1CX q1q0|ψ 1⟩, (7) qubitsaremeasuredusinganIdentity(I)operation.So,we
candenotethemeasurementofq foraninputstateof|ψ ⟩
0 Φ
|ψ[2]⟩=CX CX CX |ψ ⟩. (8) as⟨Z x⟩ Φ,wherexisthedataembeddingonqubitq 0,and
2 q1q3 q3q2 q2q0 1
⟨Z ⟩ =⟨ψ |U†(θ )Z U (θ )|ψ ⟩, (14)
Then,weaddourfinalrotationgateonqubit0ineachansatz, x Φ Φ x x 1 x x Φ
andobtainthefollowingquantumstate. whereθ isaparameteronthequantumansatzU andZ
x x 1
representsPauli-Zmeasurementonthefirstqubit.Ithasbeen
|ψ[a]⟩=R (x ) |ψ[a]⟩, (9)
3 z 1 q0 2 shown in recent years that the integration of classical and
wherearepresentsthedesiredansatzanda=0,1,or2. quantummechanismsformachinelearningisverypromis-
ing as these hybrid models leverage the strengths of both
This sequence of gates and entanglements in the ansatzes
paradigms. Classicalmodels performbest withfeature ex-
constitutesonelayeroftherespectiveansatzandcanalsobe
tractionanddimensionalityreductionproblems,whilequan-
representedbytheunitaryoperator,suchthat,
tummodelsoffersuperiorcapabilitiesbyusingquantumme-
U (θ)=|ψ[a]⟩. (10) chanicalphenomenasuchasentanglementandsuperposition,
a 3
enabling the extraction of exponentially more information
Now,eachlayercanbeappliedforadepthofDtoenhance out of each chunk of data (Biamonte et al. 2017; Huang
theexpressivenessofthemodel,andtherefore,weobtainthe etal.2022).Therefore,thishybridapproachcanleadtomore
finalstateofthecircuit, effectivemachinelearningsystems,particularlyfortasksin-
|ψ ⟩=(U (θ))D|ψ ⟩, (11) volvingpatternrecognitioninlargeandcomplexdatasets.
final a Φ
Inthiswork,weemploytwohybridclassical-quantumnon-
whereaisthedesiredansatza=0,1,or2.Accordingtothe localneuralnetwork(QNL-Net)models:CNN-QNL-Netand
Bornrule,measuringanyquantumstateinthePauli-Z basis PCA-QNL-Net,asseeninFig.3.Theprimarypurposefor
(σ )eithercollapsesintothestate|0⟩withprobability|α|2or usingtheseclassicalmodelsisfordimensionalityandfeature
zFigure3:ThepipelinefortheHybridClassical-QNL-NetModelsusedinthiswork:(a)CNN-QNL-Net:Twoconvolutional
layerswithReLUactivationandmaxpoolingextractfeaturesfrominputimages,followedbyadropoutlayerandflatteningof
thefeatures,andthentwofullyconnected(FC)layerstopreparethedatafortheQNL-Netmodule.(b)PCA-QNL-Net:PCA
reducestheinputdatadimensionalityto4components,whicharethenprocessedbyafullyconnected(FC)layerbeforebeing
fedintotheQNL-Netmodule.(c)QNL-Net:Thequantumpartofthepipelinewhichperformscomputationonthefourfeatures
obtainedfromtheclassicalmodelsaccordingtothemechanismandoutputsasinglemeasurementobtainedfromqubit0.(d)
Post-QNL-NetClassicalComputation:Afullyconnectedlayerisusedtofine-tunethequantumoutput,andthenthisoutputis
transformedandconcatenatedasdesiredforbinaryclassification.
reduction, transforming input data into a feature vector of andCIFAR-10(Krizhevsky,Hintonetal.2009)(sampleim-
size4×1,suchthateachvalueofthevectorcanbeencoded agesareshownintheAppendixinFig.5).ForMNIST,we
intoonequbitoftheQNL-Netlayertocaptureandanalyze focusonbinaryclassificationusingthedigits0and1,com-
long-rangedependenciesandintricatepatternsamongthese prising12,665trainingsamplesand2,115testingsamples.
features. ForCIFAR-10,weperformbinaryclassificationusingclasses
AftermeasuringtheoutputfromtheQNL-Net,weperform 2(birds)and8(ships).Beforefeedingtheimagesintothe
further classical computation by adding a fully connected models,wenormalizethemusingtheglobalmeanandstan-
layerconsistingofasinglelearnableparameter,whichhelps darddeviationofeachdataset,scalingpixelvaluesfrom[0,
fine-tuneandoptimizethequantumoutputandimprovethe 255]to[0,1].
model’sperformance.Thelineartransformationperformed
bythislayerisasfollows.
SimulationSettings
x=W Q+b , (15)
4 4
whereQ ∈ R1 isthesingleQNL-Netoutput,W ∈ R1×1 TheexperimentsareconductedonaMacBookProwithan
istheweightmatrix,andb
∈R1isthebiasvecto4
r.Finally,
M2Maxchipand64GBRAM.TheQNL-Netnetworkisim-
4 plementedbyutilizingtheEstimatorQNNmoduleofQiskit
forperformingbinaryclassification,wecomputethecomple-
MachineLearning0.7.2andQiskit1.1.0,whichfacilitates
mentoftheprobabilitiesandconcatenatethemtogetherto
theencodingofclassicaldataintoquantumdataandenables
obtainasinglevectorcompatibleforlosscomputationand
thetrainingoftheansatz.Theclassicalnodeisconstructed,
backpropagation,
andgradientoptimizationisperformedusingPyTorch2.3.0,
yˆ=cat(x,1−x), (16) whichconnectsseamlesslywiththeEstimatorQNNmodule.
whereyˆ∈R2istheresultantvectorcontainingthepredicted Themodelsaretrainedfor100epochswithabatchsizeof
1usingthenegativelog-likelihood(NLL)lossfunctionfor
probabilitiesforbothclasses.
convergence.TheAdamoptimizer(KingmaandBa2014)is
configuredwithdifferentlearningrates,whichvarybetween
QuantumSimulation
0.0001to0.0004dependinguponthemodelandtheansatz
Datasets
usedaslistedinAppendixTable1.TheExponentialLRsched-
Inourexperiments,weutilizetwowidelyusedimageprocess- uler(LiandArora2019)usesaγ decayrateof0.9.Therest
ingdatasetsforimageclassification:MNIST(Deng2012) oftheparametersaresettodefault.SimulationResults
The experiments were conducted using different combina-
tionsoffeaturemaprepetitionsr=1,2,or3andthenumber
ofansatzrepetitionsD=1,2or3.Theaccuraciesreported
areaveragedforallrunsforeachspecificansatzandmodel
configuration,asshowninAppendixTable1.
TheresultsontheMNISTdatasetforclasses0and1indi-
cate that the CNN-QNL-Net model performs slightly bet-
terthanthePCA-QNL-Netmodel,achievinganear-perfect
averageclassificationtestaccuracyof99.96%whereasthe
PCA-QNL-Netachievedatestaccuracyof99.59%.Ansatz-
0 and Ansatz-2 generally yield better results compared to
Ansatz-1forthisdataset,asevidentinAppendixTable1.For
theCIFAR-10dataset,thehybrid-QNL-Netmodelsperform
comparativelyworsethantheMNISTduetotheintroduction
of three color channels (i.e., RGB) compared to MNIST’s
grayscaleimages.However,theCNN-QNL-Netwasstillable
toobtainanaveragetestaccuracyof93.98%.Ansatz-1per-
formsbetteronthetestingdatasetforCIFAR-10compared
totheotheransatzes.Onbothdatasets,theCNN-QNL-Net
significantlyoutperformsthePCA-QNL-Netduetoitsability
Figure 4: Training loss convergence and accuracy plots
toefficientlyextractfeaturesfromthedatasetbeforefeeding
forCNN-QNL-NetandPCA-QNL-Netmodelsusingthree
themtotheQNL-Netmodule.
ansatzes with one feature map repetition (r = 1) and one
Increasingthedepthofboththefeaturemapandtheansatzes
(i.e.,randD,respectively)generallyimprovedclassification
ansatzrepetition(D =1).(a)and(b)showlossconvergence
duringtrainingfortheMNISTdataset(classes0and1)and
accuraciesduetotheincreasedexpressivenessofthecircuit.
theCIFAR-10dataset(classes2and8),respectively.(c)and
However,italsoresultedinlongertrainingtimescomparedto
(d)displaytrainingaccuracyontherespectivedatasets.
usingfewerrepetitions,whichstillobtainedreasonablygood
resultswithintheboundsreportedinAppendixTable1.Fur-
thermore,PCA-QNL-Netmodelsrequiredhigherlearning
theCIFAR-10trainingdata.TheCNN-QNL-Netmodelalso
ratestoobtainconvergencethantheCNN-QNL-Netmodels.
consistentlyachieveshighertrainingaccuracyfasterthanthe
ThePCA-QNL-Netdemonstratedfastertrainingcompared
PCA-QNL-NetmodelinFig.4d,reinforcingitseffectiveness
totheCNN-QNL-Net,asCNNsaddanoverheadfortraining
in learning from more complex data. Overall, our experi-
parameters,utilizingatotalof34,282classicalparameters
mentalresultsconfirmthatintegratingquantumcircuitswith
on MNIST and 41,314 classical parameters on CIFAR-10.
classicalneuralnetworkarchitecturessignificantlyenhances
The PCA-QNL-Net, however, optimizes only 22 classical
model performance, setting a new benchmark in quantum
parametersfromthelinearlayers,offeringanadvantagein
machinelearning.
termsofclassicaltrainingefficiency.
Table1presentsthecompleteresultsforalldifferentansatz
Discussion
andmodelsrunonthedatasets.Further,Table2compares
theresultsfromourmodelstothebenchmarkquantumbi- ThenoveltyoftheQNL-Netarchitectureliesinitsefficient
naryclassifiersdiscussedintheRelatedWork.Ourhybrid utilization of fewer qubits, a critical consideration in the
classical-quantummodels,particularlytheCNN-QNL-Net, NISQera.However,itssignificanceextendsbeyondthatto
outperformthebenchmarkmodelsacrossbothdatasets.This takeintoaccountthefundamentalprinciplesofquantumme-
superiorperformanceunderscorestheeffectivenessofcom- chanics,particularlyinitstreatmentofrotationsaroundaxes
biningclassicalconvolutionalnetworkswithquantumneural andentanglement.TThechoiceofrotationgatesintheQNL-
networklayers,enablingmorerobustandaccurateclassifi- Netansatzesistiedtothefundamentalideabehindnon-local
cation. For the MNIST dataset, Figure 4a shows that the neuralnetworks,whichaimtocaptureintricatespatialdepen-
PCA-QNL-Netmodelachievesalowerfinallosscompared dencieswithinthedata.Inthequantumparadigm,rotation
totheCNN-QNL-Netmodelacrossallansatzconfigurations, gatesachievethisbytranslatingquantumstatesarounddif-
suggesting that the PCA-QNL-Net model fits the training ferentaxes,therebyimplementingspatialtransformations.
datamorecloselybytheendofthetrainingperiod.Despite Now,forsimplification,considertherawdataxandthethree
this,theCNN-QNL-Netmodelreacheshighertrainingaccu- embeddings(θ,ϕ,g)inthenon-localneuralnetarchitecture
racymorequicklythanthePCA-QNL-Netmodel,asshown asakintothefourfeaturespassedontotheQNL-Netcircuit.
inFigure4c,indicatingbettergeneralization.Incontrast,on Innon-localneuralnets,rawdataisnotembedded,butinthis
the CIFAR-10 dataset, the CNN-QNL-Net model demon- work,weutilizerotationaroundthez-axisprimarilybecause
stratesamorerapiddecreaseintraininglossandachievesa itallowsustomaintainatrainableparameteronqubit0af-
lowerfinallossvaluecomparedtothePCA-QNL-Netmodel terthefeaturemapencodingwhichensurestheconsistency
(Fig.4b).Thissuggestsfasterconvergenceandabetterfitto andcompletenessofquantumtheorywhilenotaffectingtheTable1:ThistablesummarizestheperformancemetricsofvariousconfigurationsoftheQNL-Netmodelonbinaryclassification
tasksacrossdatasets:MNISTdigits0and1andCIFAR-10classes2(bird)and8(ship).Theconfigurationsvarybytheansatz
type,themodelused(CNN-QNL-NetorPCA-QNL-Net),andthelearningrate.Thereportedmetricsincludetheaveragetraining
accuracyandtestaccuracyacrossdifferentvariationsofthenumberofrepetitionsofthefeaturemap(r)andthenumberof
repetitionsoftheansatz(D).Numbersfollowing±representthestandarddeviationacrossconfigurations.
Dataset Ansatz Model LearningRate AverageTrainAccuracy(%) AverageTestAccuracy(%)
0 CNN-QNL-Net 1×10−4 99.97±0.02 99.96±0.03
1 CNN-QNL-Net 1×10−4 99.96±0.02 99.95±0.02
MNIST 2 CNN-QNL-Net 1×10−4 99.96±0.03 99.95±0.04
(0,1) 0 PCA-QNL-Net 1.5×10−4 99.65±0.17 99.54±0.16
1 PCA-QNL-Net 1.5×10−4 99.24±0.19 99.18±0.34
2 PCA-QNL-Net 1.5×10−4 99.67±0.23 99.59±0.21
0 CNN-QNL-Net 3×10−4 94.20±0.77 93.54±0.66
1 CNN-QNL-Net 3×10−4 94.13±0.45 93.98±0.37
CIFAR-10 2 CNN-QNL-Net 3×10−4 94.21±0.32 93.76±0.14
(2,8) 0 PCA-QNL-Net 4×10−4 81.94±1.51 81.16±1.09
1 PCA-QNL-Net 4×10−4 81.79±0.34 80.95±0.35
2 PCA-QNL-Net 4×10−4 81.67±0.73 80.86±0.74
Table2:ThistablecomparestheperformanceofourQNL- several limitations were identified. The current implemen-
Net model with other benchmark quantum classifiers on tation is restricted by the reliance on classical computing
binary classification tasks using the MNIST dataset. The methods that might be computationally inefficient as the
CNN-QNL-Netmodeldemonstratesbetterresultsusingsig- datasets become much more extensive and the models are
nificantlyfewerqubits. morecomplex.Still,itisatrade-offwemustconsideraswe
utilizelesserquantumresources.Multi-classclassification
Model Classes Qubits TestAccuracy posed a particular challenge as it performed poorly, likely
QTN-VQC(Qi,Yang,andChen2023) 0,1 12 98.6
HybridTTN-MERA(Grantetal.2018) 0,1 8 99.87±0.02 due to the small circuit size with Pauli-Z measurement at
CNN-QNL-Net[ours] 0,1 4 99.96±0.03
onlyonequbit,whichlimitsthemodel’scapabilityindistin-
SQNN(Wu,Tao,andLi2022) 3,6 64 97.47
QF-hNet-BN(Jiang,Xiong,andShi2021) 3,6 12 98.27 guishingbetweenmultipleclasses.Additionally,themodel’s
CNN-QNL-Net[ours] 3,6 4 99.94±0.02
performancecouldbefurthervalidatedbytestingonmore
diverseandlargerdatasetsandexploringtheeffectsofdiffer-
entquantumencodingsandvariationalcircuitdesigns.
measurementinitially.TheR gatesaroundqubits1and2 Nevertheless,theimplicationsofthisresearchextendtovari-
y
forembeddingsθandϕ,respectively,iscrucial.Thesegates ouspracticalapplications.Infieldslikeimageclassification,
inducephaseshiftsinthestatesofthecorrespondingqubits, medical imaging, and real-time video analysis, efficiently
playing a vital role in generating interference effects and capturinglong-rangedependenciesusingquantum-enhanced
complexquantumstates.Additionally,theyfacilitatecaptur- modelscanleadtosignificantadvancementsinaccuracyand
ingrelativephaseinformation,whichispivotalindiscerning performance.Furthermore,thescalablenatureofQNL-Net
subtlepatternswithinthedata.Thelinearembeddinggtrans- suggests that as quantum hardware evolves, these models
latestoqubit3,whereanR gatemodifiestheprobability couldbedeployedinreal-worldscenarios,providingacom-
x
amplitudesofstates|0⟩and|1⟩,fine-tuningthedatarepre- petitiveedgeovertraditionalclassicalapproaches.
sentationwithinthequantumcircuit.
AkeyadvantageofQNL-Netisleveragingquantumentan- Conclusion
glement.Whileclassicalnon-localblockstypicallyinvolve
matrixmultiplicationswithintheembeddingsandelement- ThispaperintroducedtheQuantumNon-localNeuralNet-
wisesummationwiththerawdata,theQNL-Netutilizesthe works (QNL-Net) mechanism as a novel hybrid classical-
intrinsic significance of CNOT entanglements to replicate quantumapproachforimageclassification.Throughexperi-
variabledependenciespresentinclassicalnon-localmecha- mentsonMNISTandCIFAR-10datasets,QNL-Netmodels
nisms.Thiscreatesahighlyinterconnectedsystembetween demonstratedcompetitiveperformanceinbinaryclassifica-
all qubits, effectively extracting intricate probabilities and tiontasks,usingfewerqubitscomparedtotraditionalquan-
facilitatingtheexplorationofcomplexdatastructuresina tumclassifiers.Theuseoffundamentalquantumtechniques
quantumframework.Aswecaninferfromtheresults,itdoes likeentanglementandrotationgatesprovedeffectiveincap-
notmattersignificantlywhatthespecificpatternforentangle- turingintricatespatialdependenciescriticalforimageanaly-
mentisindifferentansatz,rathertheideathatallqubitsen- sis.
compassingdifferentrotationsareentangledtogether.Then, However,QNL-Netexhibitslimitationsinmulti-classclassi-
further applying another R gate on qubit 0, which is al- ficationandefficiencywithlarger,complexdatasetsduetore-
z
readyinahighlyentangledstate,introducesadditionalphase lianceonclassicalpreprocessingmethods.Thesechallenges
modulationwithintheentangledsystem.Thismodulationis underscore the need to explore future work on innovative
crucialforfine-tuningthequantumstatebeforemeasurement, QNL-Netvariantsandoptimizationtechniques.Additionally,
therebyinfluencingtheoutcomeprobabilities. exploringtheintegrationofmoreefficientquantumencoding
Despitethepromising resultsobtainedfromthese models, strategiesmightalsoenhanceperformance.In conclusion, QNL-Net promises advancements in accu-
racyandefficiencyforimageclassificationtasks,withpoten-
tialtransformativeimpactsinfieldsrequiringrobustpattern
recognition, such as medical imaging and real-time video
analysis.Itsabilitytooperatewithreducedcomputational
resourcescomparedtoclassicalandexistingquantummeth-
odspositionsQNL-Netasascalablesolutionforquantum-
enhancedmachinelearningapplications,layingtheground-
workforbroaderuseinpracticalapplications.References Henderson,M.;Shakya,S.;Pradhan,S.;andCook,T.2020.
Quanvolutionalneuralnetworks:poweringimagerecognition
Abbas, A.; Ambainis, A.; Augustino, B.; Bärtschi, A.;
withquantumcircuits. QuantumMachineIntelligence,2(1):
Buhrman,H.;Coffrin,C.;Cortiana,G.;Dunjko,V.;Egger,
2.
D.J.;Elmegreen,B.G.;etal.2023. Quantumoptimization:
Potential,challenges,andthepathforward. arXivpreprint Huang, H.-Y.; Broughton, M.; Cotler, J.; Chen, S.; Li, J.;
arXiv:2312.02279. Mohseni,M.;Neven,H.;Babbush,R.;Kueng,R.;Preskill,J.;
etal.2022.Quantumadvantageinlearningfromexperiments.
Abbas,A.;Sutter,D.;Zoufal,C.;Lucchi,A.;Figalli,A.;and
Science,376(6598):1182–1186.
Woerner,S.2021. Thepowerofquantumneuralnetworks.
Jiang,W.;Xiong,J.;andShi,Y.2021. Aco-designframe-
NatureComputationalScience,1(6):403–409.
workofneuralnetworksandquantumcircuitstowardsquan-
Amjoud, A.B.; andAmrouch, M.2023. Objectdetection
tumadvantage. Naturecommunications,12(1):579.
usingdeeplearning,CNNsandvisiontransformers:Areview.
Jolliffe, I. T.; and Cadima, J. 2016. Principal component
IEEEAccess,11:35479–35516.
analysis:areviewandrecentdevelopments. Philosophical
Bao,F.;Wang,X.;Sureshbabu,S.H.;Sreekumar,G.;Yang, transactionsoftheroyalsocietyA:Mathematical,Physical
L.;Aggarwal,V.;Boddeti,V.N.;andJacob,Z.2023. Heat- andEngineeringSciences,374(2065):20150202.
assisteddetectionandranging. Nature,619(7971):743–748.
Kanazawa, N.; Egger, D. J.; Ben-Haim, Y.; Zhang, H.;
Benedetti,M.;Lloyd,E.;Sack,S.;andFiorentini,M.2019. Shanks,W.E.;Aleksandrowicz,G.;andWood,C.J.2023.
Parameterizedquantumcircuitsasmachinelearningmodels. Qiskitexperiments:Apythonpackagetocharacterizeandcal-
QuantumScienceandTechnology,4(4):043001. ibratequantumcomputers. JournalofOpenSourceSoftware,
Bernstein,D.J.;andLange,T.2017. Post-quantumcryptog- 8(84):5329.
raphy. Nature,549(7671):188–194. Kingma,D.P.;andBa,J.2014. Adam:Amethodforstochas-
ticoptimization. arXivpreprintarXiv:1412.6980.
Biamonte,J.;Wittek,P.;Pancotti,N.;Rebentrost,P.;Wiebe,
N.;andLloyd,S.2017. Quantummachinelearning. Nature, Kocsis,P.;Súkeník,P.;Brasó,G.;Nießner,M.;Leal-Taixé,
549(7671):195–202. L.; and Elezi, I. 2022. The unreasonable effectiveness of
fully-connected layers for low-data regimes. Advances in
Brunton,S.L.;andKutz,J.N.2022. Data-drivenscience
NeuralInformationProcessingSystems,35:1896–1908.
andengineering:Machinelearning,dynamicalsystems,and
Krizhevsky, A.; Hinton, G.; et al. 2009. Learn-
control. CambridgeUniversityPress.
ing multiple layers of features from tiny images.
Cherrat, E. A.; Kerenidis, I.; Mathur, N.; Landman, J.;
https://www.cs.toronto.edu/kriz/cifar.html.
Strahm,M.;andLi,Y.Y.2024.Quantumvisiontransformers.
Krizhevsky,A.;Sutskever,I.;andHinton,G.E.2012. Ima-
Quantum,8:1265.
genetclassificationwithdeepconvolutionalneuralnetworks.
Cong,I.;Choi,S.;andLukin,M.D.2019. Quantumconvolu- Advancesinneuralinformationprocessingsystems,25.
tionalneuralnetworks. NaturePhysics,15(12):1273–1278.
Li, M.; Jiang, Y.; Zhang, Y.; and Zhu, H. 2023. Medical
Deng, L. 2012. The mnist database of handwritten digit imageanalysisusingdeeplearningalgorithms. Frontiersin
imagesformachinelearningresearch[bestoftheweb].IEEE PublicHealth,11:1273253.
signalprocessingmagazine,29(6):141–142.
Li, Z.; and Arora, S. 2019. An exponential learning rate
Fu,X.;Zhou,F.;Peddireddy,D.;Kang,Z.;Jun,M.B.-G.; schedulefordeeplearning. arXivpreprintarXiv:1910.07454.
andAggarwal,V.2023. Anfiniteelementanalysissurrogate Minaee,S.;Boykov,Y.;Porikli,F.;Plaza,A.;Kehtarnavaz,
modelwithboundaryorientedgraphembeddingapproach N.; and Terzopoulos, D. 2021. Image segmentation using
forrapiddesign. JournalofComputationalDesignandEngi- deeplearning:Asurvey. IEEEtransactionsonpatternanaly-
neering,10(3):1026–1046. sisandmachineintelligence,44(7):3523–3542.
Grant,E.;Benedetti,M.;Cao,S.;Hallam,A.;Lockhart,J.; Mitarai,K.;Negoro,M.;Kitagawa,M.;andFujii,K.2018.
Stojevic,V.;Green,A.G.;andSeverini,S.2018.Hierarchical Quantumcircuitlearning. PhysicalReviewA,98(3):032309.
quantumclassifiers. npjQuantumInformation,4(1):65. Naseer, M.; Khan, S.; and Porikli, F. 2018. Indoor scene
Gujju, Y.; Matsuo, A.; and Raymond, R. 2024. Quantum understanding in 2.5/3d for autonomous agents: A survey.
machine learning on near-term quantum devices: Current IEEEaccess,7:1859–1887.
state of supervised and unsupervised techniques for real- Nielsen, M. A.; and Chuang, I. L. 2001. Quantum com-
worldapplications. PhysicalReviewApplied,21(6):067001. putationandquantuminformation,volume2. Cambridge
Havlícˇek,V.;Córcoles,A.D.;Temme,K.;Harrow,A.W.; universitypressCambridge.
Kandala, A.; Chow, J. M.; and Gambetta, J. M. 2019. Su- O’shea,K.;andNash,R.2015. Anintroductiontoconvolu-
pervised learning with quantum-enhanced feature spaces. tionalneuralnetworks. arXivpreprintarXiv:1511.08458.
Nature,567(7747):209–212. Peddireddy,D.;Bansal,V.;andAggarwal,V.2023. Classical
He,K.;Zhang,X.;Ren,S.;andSun,J.2016. Deepresidual simulation of variational quantum classifiers using tensor
learningforimagerecognition. InProceedingsoftheIEEE rings. AppliedSoftComputing,141:110308.
conferenceoncomputervisionandpatternrecognition,770– Preskill,J.2018. QuantumcomputingintheNISQeraand
778. beyond. Quantum,2:79.Qi, J.; Yang, C.-H.; and Chen, P.-Y. 2023. Qtn-vqc: An
end-to-endlearningframeworkforquantumneuralnetworks.
PhysicaScripta,99(1):015111.
Schetakis,N.;Aghamalyan,D.;Griffin,P.;andBoguslavsky,
M.2022. ReviewofsomeexistingQMLframeworksand
novel hybrid classical–quantum neural networks realising
binaryclassificationforthenoisydatasets. Scientificreports,
12(1):11927.
Schuld, M.; Bergholm, V.; Gogolin, C.; Izaac, J.; and Kil-
loran, N. 2019. Evaluating analytic gradients on quantum
hardware. PhysicalReviewA,99(3):032331.
Srivastava,N.;Hinton,G.;Krizhevsky,A.;Sutskever,I.;and
Salakhutdinov,R.2014. Dropout:asimplewaytoprevent
neural networks from overfitting. The journal of machine
learningresearch,15(1):1929–1958.
Vaswani,A.;Shazeer,N.;Parmar,N.;Uszkoreit,J.;Jones,L.;
Gomez,A.N.;Kaiser,Ł.;andPolosukhin,I.2017. Attention
isallyouneed. Advancesinneuralinformationprocessing
systems,30.
Wang,X.;Girshick,R.;Gupta,A.;andHe,K.2018. Non-
localneuralnetworks.InProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition,7794–7803.
Wu,J.;Tao,Z.;andLi,Q.2022. wpScalablequantumneu-
ralnetworksforclassification. In2022IEEEInternational
ConferenceonQuantumComputingandEngineering(QCE),
38–48.IEEE.
Zhou,G.;Aggarwal,V.;Yin,M.;andYu,D.2022. Acom-
putervisionapproachforestimatingliftingloadcontributors
toinjuryrisk. IEEETransactionsonHuman-MachineSys-
tems,52(2):207–219.Supplementary Material: Technical Appendix
BackgroundonQuantumComputing (cid:20) cos(θ) −isin(θ)(cid:21)
R (θ)= 2 2 , (20)
x −isin(θ) cos(θ)
Quantumcomputingleveragesthefundamentalsofquantum 2 2
mechanics, such as superposition and entanglement, to in-
troducenewpropertiestocomputing.Itusestheconceptof (cid:20) cos(θ) −sin(θ)(cid:21)
R (θ)= 2 2 , (21)
qubitsthathavethecomputationalbasisstates|0⟩and|1⟩, y sin(θ) cos(θ)
2 2
whichcanalsoberepresentedas
(cid:20) 1(cid:21) (cid:20) 0(cid:21) 1 0 0 0
|0⟩= 0 ,|1⟩= 1 . (17) R (θ)=(cid:20) e−iθ 2 0 (cid:21) , CX =0 1 0 0 . (22)
z 0 eiθ 2 0 0 0 1
Aqubitisalinearcombinationofthesebasisstates,whichis 0 0 1 0
thegeneralprincipleofsuperposition,andcanberepresented
asavectorinatwo-dimensionalcomplexHilbertspace,such CNN-QNL-Net
that, WeusetheConvolutionalNeuralNetwork(CNN)architec-
tureincombinationwithourQNL-NetModulebecauseCNN
|ψ⟩=α|0⟩+β|1⟩, (18)
isadeptatcapturingspatialdependenciesandidentifyinglo-
whereα,β ∈Carethecomplexcoefficientsofthequantum calpatternswithincompleximagedatathroughconvolutional
states|0⟩and|1⟩respectively.Theprobabilitiesofthequbit andpoolinglayers,whichreducetheinputdimensionality
beinginstate|0⟩or|1⟩aregivenbythemagnitudesquaredof whileretainingessentialfeatures(O’sheaandNash2015).
thesecoefficients|α|2and|β|2.Theseprobabilityamplitudes IntheproposedCNN-QNL-Netarchitecture,westartwith
satisfythenormalizationcondition|α|2+|β|2 =1. two convolutional layers, each with an activation function
Entanglement is another quantum phenomenon where the andmaxpooling,foraninputimagetensorX ∈RW×H×C,
statesoftwoormorequbitsbecomeinterconnected,andthe whereW isthewidth,Histheheight,andCisthenumberof
stateofonequbitaffectstheotherentangledqubits.Thisalso channels(i.e.1forgrayscaleimagesand3forRGBimages)
demonstratesthatthestatescannotbefactoredintoaproduct oftheinputimage.Ingeneral,mathematically,aconvolution
ofindividualqubitstatesastheyarestronglycorrelated(i.e., operation‘∗’foraninputimageI andafilterK tooutputa
|ψ ⟩≠ |ψ ⟩⊗|ψ ⟩forstatesAandB). featuremapF lookslike,
AB A B
Thefirststeptoanyquantumcomputationisencodingclas-
F[i,j]=(I∗K) ; (23)
sicaldataintoquantumstates.Toachievethis,severalfun- [i,j]
damentalencodingtechniquesareused:(i)Basisencoding
mapsclassicalbits0and1tostates|0⟩and|1⟩directly,and M−1N−1C−1
(cid:88) (cid:88) (cid:88)
therefore each classical bit string is encoded as the corre- F[i,j]= I [i+m,j+n,c]·K [m,n,c], (24)
sponding quantum state; (ii) Amplitude encoding uses the m=0 n=0 c=0
amplitudesofaquantumstatetorepresentclassicaldatasuch wherei,jarepositionsintheoutputfeaturemapF,m,nare
thatthesumofthesquaredamplitudesofthequantumstate positionsinthefilterK forchannelc,andM,N,andC are
isnormalizedto1fortheclassicaldata;(iii)Phaseencoding
thewidth,height,andnumberofchannelsofthefilterrespec-
mapsclassicalinformationtothephasesofaquantumstate
tively.Thefirstconvolutionallayerappliesa2Dconvolution
andisusedintheproposedQNL-Netmechanism. operationwithK filters(orkernels)ofsize5×5resulting
1
Quantum computations are performed primarily by ma- inK outputchannels,andisdefinedas,
1
nipulatingquantumstatesthroughunitarytransformations,
achievedusingquantumgates.Hadamard(H)gateisused C
(cid:88)
toattainanequalsuperpositionofthetwobasisstates.The Y [k] = X [c]∗W [k]+b [k], k =1,...,K 1, (25)
Hgatemapsthebasisstate|0⟩to |0⟩ √+|1⟩ andthebasisstate c=1
2
|1⟩ to |0⟩ √−|1⟩. Rotation gates (R x,R y,R z) rotate the state whereW [k]isthek-thfilterweightandb [k]isthebiasterm.
2 WeapplytheactivationfunctionReLU,whichsimplyelim-
of a qubit around a specified axis on the Bloch sphere. A
inatesthenegativevaluesinaninputvectorandisdefined
Phase(P)gateshiftsthephaseofaqubitbyaspecifiedan-
asReLU(x)=max(0,x),oneachfilterelement-wisesuch
gle ϕ such that applying P(ϕ) to |ψ⟩ in eq.(18) results in
that,
P(ϕ)|ψ⟩ = α|0⟩+βeiϕ|1⟩.ACNOT(CXgate)isatwo-
qubitgatethatflipsthestateofthesecondqubit(target)only A =ReLU(Y ), A∈RW1×H1×K1, (26)
[k] [k]
ifthefirstqubit(control)is|1⟩.Thefollowingarethematrix
whereW andH arethewidthandheightafterconvolution.
representationsoftherelevantgatesutilizedinthiswork: 1 1
Then, we apply a max pooling operation with a pool size
(cid:20) (cid:21) (cid:20) (cid:21)
1 1 1 1 0 of 2 × 2 on the Convolution + ReLU layer to obtain the
H = √ , P(ϕ)= , (19)
2 1 −1 0 eiϕ pooled tensor P, which reduces the spatial dimensions ofeachchannelbyselectingthemaximumvaluewithineach image.BeforeapplyingtheSVD,theinputdataiscentered
pool,suchthat, foreachfeature,suchthat,
P
[k]
=MaxPool(A [k]), P ∈RW2×H2×K1, (27) 1 (cid:88)N
µ= X , (34)
N i
whereW 2 = W 21 andH 2 = H 21.ThisConvolution+ReLU+ i=1
MaxPoollayercombinationisrepeatedagainonP forfurther
featurereductionwithK filtersofsize5×5,andweobtain, X¯ =X−1 µT, (35)
2 N
(cid:88)K1 whereµisthecalculatedmeanvectorofthedata,1
N
isan
Z = P ∗W +b , k =1,...,K ; (28) N-dimensionalvectorofones,andX¯ isthecenteredinput
[k] [c] [k] [k] 2
c=1 data matrix. We perform SVD on this centered matrix to
decomposeitintoseveralcomponentmatriceswithvarious
B =ReLU(Z ), B ∈RW3×H3×K2, (29) interestingproperties(BruntonandKutz2022),
[k] [k]
X¯ =UΣWT, (36)
whereW andH arethewidthandheightafterthesecond
3 3
convolution.Again,weapplythemaxpoolingoperationwith whereU ∈RN×N isamatrixwitheachofitscolumnsbeing
apoolsizeof2×2onthepreviouslayer, alength-N orthogonalunitvectorortheleftsingularvector
ofX,Σ∈RN×P isadiagonalmatrixcomposedofsingular
Q
[k]
=MaxPool(B [k]), Q∈RW4×H4×K2, (30)
values of X, and W ∈ RP×P is a matrix with each of its
whereW 4 = W 23 andH 4 = H 21.WeapplyaDropoutlayer c sio nl gum uln arsb ve ei cn tg ora ol fen Xgt .h W-P eo pr rt oho jeg co tn ta hl eu cn eit nv tee rc et dor do ar tath mer ai tg rih xt
(Srivastavaetal.2014)totheresultantpooledtensorQto
onto the principal components by selecting the desired L
preventoverfittingwhichsetsanyelementoftheinputto0
numberofcolumns(orprincipalcomponents,i.e.,4inthis
duringtrainingwithprobabilityp,suchthat,
case)ofW,suchthat,
D =Dropout(Q,p), p=0.5. (31)
Z =X¯W , (37)
L
Then, we flatten the output D to obtain a 1-dimensional whereZ ∈ RN×L isthedesiredreducedformofthedata.
vectorF ofsize(W ·H ·K )×1.Weapplyafully-connected
4 4 2 Thismatrixisstandardizedtohavezeromeanandunitvari-
(FC)layertothisflattenedvectorF withReLUactivation,
ance,
whichresultsin,
Z−µ
H =ReLU(W F +b ), H ∈R128×1, (32) Z¯ = z, (38)
1 1 1 1 σ
z
whereW 1istheweightmatrixandb 1isthebiasvector.FC whereµ
z
andσ
z
arethemeanandstandarddeviationofZ
layerssimplyapplyalineartransformationtoaninputvector, respectively.Z¯isareducedvectorofsizeL×1(i.e.4×1)
essentialfordimensionalityreduction,aggregatingscattered andisthenpassedtoafullyconnectedlayer,suchthat,
patternsacrossthefeatures,andoptimizingparameters(Koc-
sis et al. 2022). Further, applying a non-linear activation H 3 =W 3Z¯+b 3, (39)
function(e.g.,ReLU)toalineartransformationenablesrep-
where W is the weight matrix and b is the bias vector.
3 3
resentingnon-linearrelationshipswithinthedata.Finally,to Finally,H ∈R4×1canbefeddirectlytotheQNL-Netfor
3
obtain an output vector H with size 4×1, which can be
2 furtherprocessing.
passedtotheQNL-Netmodule,weapplyourlastFClayer
totheoutputofH 1suchthat, LossConvergenceAnalysis
H =W H +b , H ∈R4×1, (33) In the QNL-Net framework, a hybrid gradient backpropa-
2 2 1 2 2
gationapproachisusedtotrainourmodeleffectively.This
whereW istheweightmatrixandb isthebiasvector.
2 2 approachcomprisesoptimizingboththeclassicalparameters
inneuralnetsandthesetoftrainableparameters,whichare
PCA-QNL-Net
theanglesofquantumgatesinVQCs.Thishybridtraining
PrincipalComponentAnalysisisanotherlineardimensional- approachfirstappliesaforwardpasstooptimizeparameters
ityreductiontechniquethatissuitableforlinearlyseparable andconvergethelossfunction.Ourmodelusesthenegative
datasetsusedinthisstudy.ItusesSingularValueDecompo- log-likelihood(NLL)lossfunctionforthebinaryclassifica-
sition(SVD)ofthedatatoprojectittoalowerdimensional tionproblem.TheNLLlossmeasuresthevariationbetween
space.PCAprovestobecomputationallyefficientandeasyto the true labels y and the classical predicted probabilities
computecomparedtoatechniquelikeCNN.Itdoesprovide yˆ = [yˆ 0,yˆ 1] = p(y|x;θ) obtained from the measurement
some disadvantages by losing some patterns and informa- ofthehybridclassical-QNL-Netmodel,andisdefinedfor
tion in the data when reducing its dimensionality (Jolliffe binaryclassificationas:
andCadima2016).InthePCA-QNL-Netarchitecture,our
n
inputdatamatrixisX ∈RN×P,whereN isthenumberof L(θ,ϕ)=−(cid:88) (y logyˆ +(1−y )logyˆ ), (40)
i i1 i i0
samplesinthedatasetandP isthetotalnumberofpixelsper
i=1where n is the number of samples in the dataset, yˆ is the Datasets
0
predictedprobabilityforclass0,yˆ 1isthepredictedprobabil- MNIST (Deng 2012) is a handwritten digit recognition
ityforclass1,θarethequantumparameters,andϕarethe
dataset used for many machine learning and computer vi-
classicalparameters. siontasks.EachimageinMNISTisagrayscale28x28-pixel
Tooptimizebothclassicalandquantumparameters,wecom- representationofhandwrittendigitsrangingfrom0to9.The
putethegradientsofthelosswithrespecttothepredicted MNISTdatasetcontains60,000trainingsamplesusedtotrain
probabilities, modelsand10,000testingsamplesusedtoevaluatemodel
∂L =− y i , ∂L =−1−y i. (41) performance.Thesesamplesarehandwrittenbyvariousin-
∂yˆ yˆ ∂yˆ yˆ dividuals, covering a lot of variations and styles, ideal for
i1 i1 i0 i0
machinelearning.
Then,wecomputethegradientofthepredictedprobabilities
CIFAR-10(Krizhevsky,Hintonetal.2009)isanotherwidely-
withrespecttothequantumparametersusingtheparameter-
used benchmark dataset in the field of computer vision. It
shiftrule(Mitaraietal.2018;Schuldetal.2019)toobtain,
presentsacollectionof32x32sizeRGBimagesdistributed
∂yˆ p(y |x;θ + π)−p(y |x;θ − π)
i1 = i j 2 i j 2 . (42) acrosstenclasses,includingimagesofobjectssuchasair-
∂θ j 2 planes,cars,birds,cats,etc.Thedatasetcontainsatotalof
Now, we can derive the gradient of the loss function with 50,000 training samples (5000 training samples per class)
respecttothequantumparametersfromeq.(41)&(42),such and10,000testingsamples(1000testingsamplesperclass).
that, Itsdiversesetofclasses,coupledwithvariationsinlighting,
∂L =(cid:88)(cid:18) ∂L ∂yˆ
i1 +
∂L ∂yˆ i0(cid:19)
. (43)
a fon rg ele v, ala un ad tinp gos the ew ri ot bh uin stnim esa sg ae ns d, m gea nk ee rs ali it zaa tis ou nit ca ab ple abd ila it ta ys oe ft
∂θ ∂yˆ ∂θ ∂yˆ ∂θ
j i i1 j i0 j imageclassificationmodels.
For the classical parameters, we calculate the gradients of
thepredictedprobabilitiesusingstandardbackpropagation
techniques,
(cid:18) (cid:19)
∂L =(cid:88) ∂L ∂yˆ i1 + ∂L ∂yˆ i0 ∂h i, (44)
∂ϕ ∂yˆ ∂h ∂yˆ ∂h ∂ϕ
i1 i i0 i
i
wherehdenotestheoutputfromtheclassicalmodel.These
Figure5:Sampleimagesfromthedatasetsusedinthiswork:
derivedfirst-orderobjectivefunctionsareoptimizedusingthe
(a)MNIST(Deng2012)classes0and1;and(b)CIFAR-10
stochasticgradientdescentmethod,Adam(KingmaandBa
(Krizhevsky,Hintonetal.2009)classes2and8,i.e.,birdand
2014).Thefirst-ordermomentm andthesecond-ordermo-
t shiprespectively.
mentforthequantumgradient(fromeq.(43))areestimated
usingthefollowingequations,
∂L
m =β m +(1−β ) , (45)
t 1 t−1 1 ∂θ
j
(cid:18) ∂L(cid:19)2
v =β v +(1−β ) , (46)
t 2 t−1 2 ∂θ
j
wheretistheiteration/time-stepandconstantsβ &β are
1 2
the exponential decay rate. These moments are corrected
forinitializationbiasandweobtainbias-correctedmoments,
suchthat,
m v
mˆ = t , vˆ = t . (47)
t 1−βt t 1−βt
1 2
Then,theparametersareupdatedaccordingly,
mˆ
θ ←θ −η√ t , (48)
j j
vˆ +ϵ
t
whereηisthelearningrateandϵisanaddedsmallconstant
fornumericalstability.Theoptimizationprocessissimilarto
theaboveforclassicalparametersϕfortheclassicalgradient
ineq.(44).
WealsoutilizetheExponentialLRscheduler(LiandArora
2019) to adjust the learning rate η after every epoch t for
fasterconvergencetoobtain,
η =η·γt, (49)
new
whereη istheupdatedlearningrateandγ isthedecay
new
rate.