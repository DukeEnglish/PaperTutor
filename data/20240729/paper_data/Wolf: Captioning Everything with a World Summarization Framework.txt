Wolf: Captioning Everything
with a World Summarization Framework
BoyiLi1,2 LigengZhu1,3 RanTian1,2 ShuhanTan1,4 YuxiaoChen1 YaoLu1
YinCui1 SushantVeer1 MaxEhrlich1 JonahPhilion1,5 XinshuoWeng1 FuzhaoXue1
AndrewTao1 Ming-YuLiu1 SanjaFidler1,5 BorisIvanovic1
TrevorDarrell2 JitendraMalik2 SongHan1,3 MarcoPavone1,6
1NVIDIA 2UCBerkeley 3MIT 4UTAustin 5UniversityofToronto 6StanfordUniversity
Abstract
WeproposeWolf, aWOrLdsummarizationFrameworkforaccuratevideocap-
tioning. Wolf is an automated captioning framework that adopts a mixture-of-
expertsapproach,leveragingcomplementarystrengthsofVisionLanguageModels
(VLMs). Byutilizingbothimageandvideomodels,ourframeworkcapturesdiffer-
entlevelsofinformationandsummarizesthemefficiently. Ourapproachcanbe
appliedtoenhancevideounderstanding,auto-labeling,andcaptioning. Toevaluate
captionquality,weintroduceCapScore,anLLM-basedmetrictoassessthesimi-
larityandqualityofgeneratedcaptionscomparedtothegroundtruthcaptions. We
furtherbuildfourhuman-annotateddatasetsinthreedomains: autonomousdriving,
generalscenes,androbotics,tofacilitatecomprehensivecomparisons. Weshow
thatWolfachievessuperiorcaptioningperformancecomparedtostate-of-the-art
approachesfromtheresearchcommunity(VILA1.5,CogAgent)andcommercial
solutions(Gemini-Pro-1.5,GPT-4V).Forinstance,incomparisonwithGPT-4V,
WolfimprovesCapScorebothquality-wiseby55.6%andsimilarity-wiseby77.4%
onchallengingdrivingvideos. Finally,weestablishabenchmarkforvideocap-
tioningandintroducealeaderboard,aimingtoaccelerateadvancementsinvideo
understanding,captioning,anddataalignment.
1 Introduction
Videocaptioningiscrucialasitfacilitatescontentunderstandingandretrievalbyprovidingaccurate,
searchabledescriptions. Italsoprovidespairwisedataforeffectivetrainingoffoundationmodelsfor
taskslikevideogeneration,suchasSora[4]andRunaway[34]. However,generatingdescriptive,
accurate,anddetailedvideocaptionsremainsachallengingresearchproblemforseveralreasons:
firstly, high-quality labeled data are scarce. Video captions from the internet can be faulty and
misaligned and human annotation is prohibitively expensive for large datasets. Secondly, video
captioningisinherentlymorechallengingthanimagecaptioningduetotheadditionalcomplexity
of temporal correlation and camera motion. Existing captioning models [13, 43] struggle with
temporalreasoningandfailtoachieveaccuratesceneunderstanding. Thirdly,thereisnoestablished
benchmarktomeasurecaptioningprogress. ExistingvideoQAbenchmarks[23]areoftenlimitedto
shortanswers,makingitdifficulttomeasurehallucinationsindetailedlongcaptions. Fourthly,the
correctnessandcompletenessofthecaptionsarecrucialforsafety-criticaltasks. IntheeraofLLMs,
textdescriptionsofscenariosusedbyembodiedagentsforplanningandcontrolbecomeincreasingly
common[24,25,15,11]. Consequently,afalseorincompletedescriptionofthescenariomayleadto
thedecision-makingmoduleoverlookingacriticalobjectaftertrainingonsuchcaptiondata,resulting
insafetyrisks. Forinstance,missingthepresenceofahumaninthevicinityofavegetable-chopping
manipulatorcanleadtoaninjury.
4202
luJ
62
]GL.sc[
1v80981.7042:viXraTohandlethesechallenges,weintroduceWOrLdsummarizationFramework(Wolf),anovelsumma-
rizationcaptioningframework,alongwithacaptioningmetricCapScore,andtheWolfcaptioning
benchmarkwithcorrespondingdatasets. Unlikepreviousworksthatutilizeasinglemodeltogenerate
captions, we propose to use multiple models to collaborate [14], producing much more accurate
captions. Byleveragingmultiplemodels,wecanprovidemorefine-graineddetailswhilereducing
hallucinations. We show that Wolf achieves superior captioning performance compared to state-
of-the-artapproachesfromtheresearchcommunity(VILA[20],CogAgent[13])andcommercial
solutions(Gemini-Pro-1.5[38],GPT-4V[27]). Insummary,wehavethreemaincontributions:
1. WedesignthefirstworldsummarizationframeworkWolfforvideocaptioningandintroduce
anLLM-basedmetricCapScoreforevaluatingthequalityofcaptions. Theresultsshow
thatourmethodimprovesCapScorebyalargemargin.
2. We introduce Wolf benchmark and four human-annotated benchmark datasets. These
datasetsincludeautonomousdriving,generalscenesfromPexels,androboticsvideos,along
withhuman-annotatedcaptions,referredtoastheWolfDataset.
3. Thecode,dataandleaderboardwillbeopen-sourcedandmaintainedontheWolfwebpage1.
ContinuouseffortsandimprovementswillbemadetorefinetheWolfDataset,codebase,
andCapScore. WehopethatWolfwillraiseawarenessaboutthequalityofvideocaptioning,
setastandardforthefield,andboostcommunitydevelopment.
2 RelatedWorks
ImageCaptioning. Visuallanguagemodels(VLMs)haveshownrapidadvancements,achieving
leadingperformanceinimagecaptioningtasks,largelyduetothesuccessoflargelanguagemodels.
CLIP[30]pioneeredthisfieldbytrainingasharedfeaturespaceforvisionandlanguagemodalities
onimage-captionpairs. BuildingonCLIP,BLIP[17]andBLIP-2[16]improvedperformanceby
aligningthepre-trainedencoderwithlargelanguagemodels. Followingthedirection,LLaVA[21]
andInstructBLIP[9]demonstratedthatjointlytrainingondiversedatasetsasaninstruction-following
taskleadstostronggeneralizationacrossvarioustasks. VILA[20]highlightedtheimportanceof
pre-trainingwithdiversedata,andthereforesignificantlyscaledupthepre-trainingdataset. Kosmos-
2[29]andPaLI-X[6]furtherintroducedpseudo-labelingboundingboxesfromopen-vocabulary
objectdetectorstoscaleupthesizeofpre-trainingdataset.
Video Captioning. As image-based VLMs are not specifically trained with video data, they are
limitedindescribingdetailspresentinthevideodata. Toimprovevideocaptioning,PLLaVa[41]
builds on top of LLaVa and introduced a parameter-free pooling strategy to enhance the caption
quality. Video-llava[18]achievesstate-of-the-artperformanceonseveralbenchmarksbyconducting
jointtrainingonimagesandvideos,therebylearningaunifiedvisualrepresentation. Additionally,
Video-LLama[43]incorporatesbothvideoandaudiointoLLMsbyintroducingtwoQ-formersto
extractfeatures. Vid2seq[42]conductslarge-scalepre-trainingwithnarratedvideosfordensevideo
captioning. Meanwhile,MV-GPT[35]employsanautomatedspeechrecognition(ASR)modelto
provideadditionallabelingforthevideos.
LLM-basedSummarization. Recentlymanyworkshavefoundthatitisefficienttosummarize
usefulinformationusingLLMs.Forexample,LLaDA[15]canprovideuserswithhelpfulinstructions
basedontheuserrequestandcorrespondingtrafficrulesinthedesiredlocation. OpenAIteamfinds
re-captioning[3]viaLLMscanbeveryhelpful.
3 Wolf: CaptioningEverythingwithaWorldSummarizationFramework
WeproposeWolf,whichisanautomatedcaptioningsummarizationframeworkthatadoptsamixture
ofexpertsapproachtogeneratelong,accurate,anddetailedcaptionsforvideos. Figure1provides
anoverviewofourframework. Inthispaper,weuseCogAgent[13],GPT-4V[24]togenerating
image-levelcaptions,anduseVILA-1.5[20],Gemini-Pro-1.5[38]togeneratevideocaptions.
Chain-of-thoughtSummarizationinImage-levelModels. Asimage-levelmodels(image-based
VLMs)havebeenpre-trainedwithalargeramountofdatathanvideo-levelmodels(video-based
1https://wolfv0.github.io
2Figure1: OverviewofproposedWolfframework. Wolfutilizesbothimage-levelandvideo-level
modelstogeneratediverseanddetailedcaptions,whicharethensummarizedforcross-checking.
VLMs),wefirstuseimage-basedVLMstogeneratecaptions. WedesignaChain-of-thoughtprogram
toobtainvideocaptionsfromimage-levelmodels. AsillustratedinFigure1,wefirstsplitthevideo
intosequentialimages,samplingtwokey-frameseverysecond. WestartbyfeedingImage1intothe
Image-levelModeltoobtainCaption1,wherewerequirethemodeltogeneratedetailedscene-level
informationandobjectlocations. Giventhetemporalcorrelationbetweenkeyframesinavideo,
wethenfeedbothCaption1andImage2intothemodeltogenerateCaption2. Byrepeatingthis
procedure,wegeneratecaptionsforallsampledframes. Finally,weuseGPT-4tosummarizethe
informationfromallcaptionswiththeprompt“Summarizeallthecaptionstodescribethevideowith
accuratetemporalinformation”.
LLM-basedVideoSummarization. Besidesobtainingthecaptionsfromimage-levelmodels,we
then summarize all captions into one. We use the prompt “Please summarize on the visual and
narrativeelementsofthevideoindetailfromdescriptionsfromImageModels(Image-levelCaption
andMotionCaption)anddescriptionsfromVideoModels(Video-levelCaption)”. Optionally,we
canalsoaddtheannotatedcaptiontothesummarization. Basedonthissimplescheme,Wolfcan
capturearichvarietyofdetailsofthevideoandreducehallucinations(inFigure2). Weassumethis
isbecausethemodelcancomparethecaptionsandreduceredundantandhallucinatedinformation.
Afterobtaining thedescriptions fromthe image-levelandvideo-level models, wenextapply the
prompt“Pleasedescribethevisualandnarrativeelementsofthevideoindetail,particularlythe
motionbehavior”.
4 WolfBenchmark: BenchmarkingVideoCaptioning
To showcase the effectiveness of Wolf, we constructed four distinct datasets. These include two
autonomous driving video captioning datasets based on the open-sourced NuScenes [5] dataset
(CreativeCommonsAttribution-NonCommercial-ShareAlike4.0InternationalPublicLicense),a
general daily video captioning dataset from Pexels 2, and a robot manipulation video captioning
datasetfromanopen-sourcerobotlearningdataset[28]. Thesebenchmarkdatasetsaretailoredto
assessthecaptionmodel’sscenecomprehensionanditsbehaviorunderstandingcapabilities,both
of which are vital for auto-labeling in embodied AI tasks. All captions were generated using a
combination of ground truth information, rule-based heuristics, human labeling, and GPT-based
rewriting.
4.1 BenchmarkDatasetCuration
4.1.1 AutonomousDrivingDataset
High-qualitycaptionsofdrivingvideosarecrucialnotonlyfortrainingvideogenerationmodels
butalsofortrainingVLMstointerpretthedynamictrafficenvironment. TheNuScenesdatasetis
2https://www.pexels.com/
3t = 1 [s]. Key features: t = 4[s]. Key features: t = 8[s]. Key features:
t = 0 [s]. Key features: lane. person, beach, ocean, person, ocean, smile, person, ocean, white
long hair, sunglass. sunglass,phone. shirt, dark shorts.
Caption:Thevideoopenswithapersonstandingonarockybeach,holdinga
smartphone.Theyaredressedinawhiteshirtanddarkshorts,withlong
hairandsunglasses.Inthebackground,thereareotherbeachgoersandthe
ocean.Thepersonisseentakingaselfie,withthesmartphone'sscreen
visibleinsomeframes.Thelightingsuggestsitislateafternoonorearly
t = 2 [s]. Key features: lane, traffic cones, construction zone. evening,withthesunlowonthehorizon,castingawarmglowonthescene.
Theperson'sactionsarecasualandrelaxed,astheyposeandinteractwith
thecamera.Theocean is calm, and theskyis clear, indicatinggood
weather.
t = 5[s]. Key features: lane, traffic cones, construction zone, lane
change.
Caption:Thefootagecapturesacarnavigatingthroughanurban
area where traffic cones are placed in its path. The cones t = 4[s]. Key features: t = 6[s]. Key features: t = 8[s]. Key features:
indicateaconstructionzoneaheadandblockthevehicle's robot arm, bottle,table. push the bottle. the bottle falls onto table.
lane.Thecarmaintainsasteadyspeedandsmoothlyswitches Caption:Thevideoshowsayellowindustrialroboticarmpositionedovera
lanestopassbyeachtrafficconeontherightside. tablewithagreencover.Theroboticarmisattemptingtopushagreen
bottlefromtheleftsidetotherightside.However,duringtheprocess,the
bottlefallsontothetable.
Figure2:WolfDatasetexamples.Wedisplaythevideosandcorrespondinghuman-annotatedcaptions
ofautonomousdriving(Left),Pexels(Top-Right),andRobotlearningvideodataset(Bottom-Right),
totaling25.7hoursfornow,andthedatasetsizewillberegularlyupdatedandexpanded.
alarge-scalecollectionofdrivingvideosdesignedtoaccelerateautonomousdrivingresearch. It
features1,000annotatedscenesfromBostonandSingapore. Eachsceneconsistsofa20-second
drivingvideoclipthatprovidesanego-centricviewfromtheegovehicle. Wespliteachsceneinto
5-secondsegmentsandprovidethecorrespondingcaptions. Ourcaptionsemphasizethehigh-level
drivingbehavioroftheegovehicletostresstestthesceneunderstandingabilityandthebehavior
understanding ability of the captioning model. Our dataset contains 500 intensely interactive
video-captionpairs(≈0.7hours)inwhichtheegovehicleisinvolvedinintenseinteractionswith
itssurroundingtrafficagents(suchasnavigatingaroundconstructionzonesandovertakingstatic
obstacles)and4785normaldrivingscenevideo-captionpairs(≈6hours). Ourcaptiongeneration
processconsistsofthreesteps: i)agent-levelmotionannotation,ii)ego-centricinteractionannotation,
andiii)GPT-rewriting.
Agent-levelmotionannotation.TheNuScenesdatasetprovidesfullannotationofthetrafficelements
ineachscene,includingthe3-Dboundingboxandcategoriesoftrafficelements,andsemanticmap
information. Similar to [39], we leverage this ground-truth information and the lane-topology
information[26]toannotateboththespeedandangularmotioncharacteristicsoftheegovehicle
andothertrafficparticipantswithinavideoclip. Specifically,wecategorizeagentactionsinto11
typessuchasStopping,Accelerating,Decelerating,LaneChanges,Turns,andmore,basedontheir
observedmovementsandbehaviors.
Ego-centric interaction annotation. We are
alsointerestedintheegovehicle’sinteraction
withtheothertrafficparticipants(e.g.,crossing
pedestrians,blockingtrafficcones,etc.) shown
inthevideoclip. Toefficientlycaptionthein-
Figure3: Illustrationofhomotopytypesofdiffer- teraction,weleveragetwotypesofcategorical
entrelativemotionsbetweenapairofvehicles. modestodescribethelane-relationshipbetween
atrafficparticipantandtheegovehicle(agent-egolanemode)andtherelativemotionbetweena
trafficparticipantandtheegovehicle(homotopy)[7]. Agent-egolanemodeatatimesteptencodes
thetopologyrelationshipbetweentheego’scurrentlaneandthetrafficagent’slane,including: LEFT,
RIGHT,AHEAD,BEHIND,andNOTON,whereNOTON describesthatthetrafficagentisnotonany
derivablelanesinthescene(e.g.,aparkedvehicleinaparkinglot). Tocomputetheagent-egolane
modeforeachtrafficagent,wefollow[7]tofirstidentifythelaneonwhicheachagentislocatedand
thenleveragethelanetopologymaptoannotatetheagent-egolanemode. Weprojecttheagent’s
centertothelanepolylineanduseitsrelativepositioninthelocalFrenetframetodetermineitslane
4association. Homotopiesdescribetherelativemotionbetweenapairofagentsshowninthevideo,
including: [S,CW,CCW](static,clockwise,counterclockwise),asshowninFigure3.
GPT-rewriting. Combiningagent-egolanemode,homotopy,agentgroundtruthstateinformation,
and scene context information (e.g., ego is located near intersection) together, we can leverage
heuristicstoannotatetheinteractionshowninthevideoclip. Forexample,inavideoclip,astatic
object’sagent-egolanemodechangesfromAHEAD,toLEFT,toBEHIND,andtheegovehicle’sfirst
performsRIGHT-LANE-CHANGE,KEEP-LANE,thenLEFT-LANE-CHANGE,indicatingtheego
vehicleovertakesthatobjectfromtheegovehicle’sleftside. Weidentified6interactioncategories
fromtheNuScenesdataset: 1)bypassblockingtrafficconestonavigatearoundconstructionzone;2)
yieldtocrossingpedestrians;3)yieldtoincomingvehicles;4)overtaketrafficagentsviastraddlingthe
lanedividers;5)overtaketrafficagentvialane-change;6)othernon-intensiveinteractions. Withboth
agent-levelmotionannotationandego-centricinteractionannotation,weuseGPT3.5tosummarize
eachcliptobuildthefinalcaption.
4.1.2 RobotManipulationDataset
Inadditiontothedrivingenvironment,wecollect100robotmanipulationvideos(eachhasalength
rangingfrom5secondsto1minute)from[28]thatdemonstratecomplexrobotmanipulations(e.g.,
pickandplace,push,ect.)invariousenvironments,includingkitchen,office,lab,andopenworld.We
manuallycaptioneachvideo. Thecaptionsfocusonthedescriptionofthesceneandtheinteraction
betweentherobotandtheobjects(seetheexampleinFigure2).
4.1.3 PexelsDataset
Toevaluatecaptionmodelsingeneraldailyenvironments,wefurthercollecthighquality(360pto
1080p)videosfromPexels3. Itconsistsof473high-qualityvideossourcedglobally,whereeach
videohasalengthvaryingbetween10secondsand2minutesandthecontentincludes15popular
categories (details in Appendix A.2). This diversity not only adds depth to our dataset but also
providesawiderangeofscenariosandcontextsforouranalysis.
4.2 EvaluationMetricandLeaderboard
4.2.1 CapScore: EvaluatingCaptionswithLLMs
Videocaptioninghasbeenanill-posedproblemsincethereisnometrictoevaluatethequalityof
captionsandthealignmentbetweenthevideoandthecaption. InspiredbyBERTScore[44]and
CLIPScore [12], we introduce CapScore (Captioning Score), a quantitative metric to use LLMs
(GPT-4)toevaluatethesimilaritybetweenpredictedandhuman-annotated(ground-truth)captions.
Assumewehave6captions,wefeedallthecaptionsintoGPT-4andaddtheprompt“Canyougivea
score(twodecimalplaces)from0to1forcaptions1,2,3,4and5,indicatingwhichoneiscloserto
thegroundtruthcaption(metric1)andwhichcontainsfewerhallucinationsandlessmisalignment
(metric2)? Pleaseoutputonlythescoresofeachmetricseparatedonlybyasemicolon. Foreach
metric,pleaseoutputonlythescoresofcaptions1,2,3,4and5separatedbycommas,inorder—no
textintheoutput. ”. WeaskGPT-4tooutputtwoscores: captionsimilarityandcaptionquality.
CaptionSimilarity. Captionsimilarityisbasedonhowwelleachcaptionalignswiththeground
truth description on a scale from 0 to 1, considering the key criteria mentioned. GPT-4 lists the
requirementsthataffectthescore: thismetricmeasureshowsimilareachcaptionistotheground
truthcaption. Theevaluationfocusesonthecontentandcontextdescribedinthecaptions,assessing
whethertheycapturethemainthemesanddetailsofthegroundtruth.
Caption Quality. Caption quality evaluates whether the caption contains reduced hallucination
andmistakescomparedtothegroundtruthcaptionsonascalefrom0to1. GPT-4liststhecriteria
thataffectthescore: thismetricevaluatestheaccuracyandrelevanceofeachcaption,identifying
anyextraneousorincorrectdetails(hallucinations). Captionswithfewerhallucinationsandbetter
alignmentreceivehigherscores.
3https://www.pexels.com/
5t=0[s] Wolf!:The video presents a series of scenes depicting city streets
bustling with various vehicles such as cars, trucks, motorcycles, and a
person riding a bicycle. The scenes are often divided into panels, each
showcasing a different vehicle in motion, including a yellow taxi, a blue
car, a red car, a white truck, and a silver car. The streets are marked with
t=1[s] white lines and are surrounded by green trees and buildings,
contributing to an urban landscape setting. The video captures the
typical flow of traffic through a four-way intersection, with vehicles
moving in different directions. The vehicles are shown in various
positions in the lanes, implying movement through the area. The
t=2[s] sequence likely shows vehicles accelerating and decelerating as
they approach and leave the intersection in response to traffic signals or
the flow of other vehicles. The video is a wide-angle shot of the street
intersection, split into three sections, providing a panoramic view. The
left section shows the road the viewer is on, the center focuses on the
t=3[s]
intersection itself, and the right section provides a view of the crossing
traffic. The lighting suggests a sunny day, with a bright spot from the sun
on the left section and more even lighting on the right. The video
captures the typical flow of traffic at an intersection, with vehicles
approaching, some waiting their turn while others proceed
t=4[s]
through the intersection when it's clear. The cars driving on
the right side of the road suggest the location might be a country where
right-hand drive is the standard. The absence of sound enhances the
focus on the visual information and allows the viewer to interpret the
t=5[s] scene without auditory cues. Overall, the video provides a snapshot of
everyday life at a busy intersection, highlighting the complexities of
navigating a traffic intersection. The wide-angle perspective and focus
on movement create a sense of dynamism.
Figure 4: Wolf example for driving videos that focus on interactive operations. Wolf captions
discussesthemotionbehaviorindetailsandservesasagoodreferenceforautonomousdriving.
4.2.2 BenchmarkingVideoCaptioning
Asfarasweknow,nostandardevaluationbenchmarkshavebeenestablishedforvideounderstanding
andcaptioning. Toacceleratetheadvancementofthisfield,wehavedevelopedthefirstleaderboard
forvideocaptioning. AsLLMevaluationhasbecomeincreasinglypopular[8],werealizedthelack
ofastandardplatformtoevaluateVLM’sperformanceonvideounderstanding. Weassumethisis
duetothedifficultyofcollectingground-truthcaptionsthataccuratelyalignwithvideos. Please
checkourinitialversionofCaptioningLeaderboard.
5 Experiments
5.1 ExperimentalSetup
DataSetup. WeusefoursetsofdatatoevaluatethevalidityofWolf: 1)500NuscencesInteractive
Videos;2)4,785NuscencesNormalVideos;3)473generalvideosand4)100roboticsvideos. We
extract2framespersecondforautonomousdrivingvideos. Forroboticsvideos,weextract1frame
persecond. Forshortvideosthatsamplelessframes,wewillincreasefpstocapturemoredetails.
ComparisonSetup. WeuseourproposedCapScoretoevaluatethesimilaritybetweenpredicted
andgroundtruthcaptions. CogAgentandGPT-4Vareimage-levelmethods,soweuploadsequential
framesintothemodeltoobtaintheoutput. VILA-1.5andGemini-Pro1.5arevideo-basedmethods,
so we directly feed a video into the model. As for the prompt for each captioning model, we
use“elaborateonthevisualandnarrativeelementsofthevideoindetail,particularlythemotion
behavior". Wecomparewithfourstate-of-the-artimage-levelandvideo-levelcaptioningmethod
CogAgent[13],GPT-4V[1],VILA-1.5[20]andGemini-Pro-1.5[38]. AsforCogAgent,wefeed
themiddleframeofthevideointothemodeltoobtainthecaptions. AsforGPT-4V,weuniformly
sample16framesfromavideoandfeedthesequentialimagesintothemodeltoobtaincaptions. As
forVILA-1.5andGemini-Pro-1.5,wefeedthevideointothemodeltoobtainthecaptions.
5.2 QualitativeResults
ToillustrateenhancedcaptioningabilitybyWolf,weshowthequalitativeresultsinFigure4(please
checkdetailsinAppendix). WenoticedthatalthoughGPT-4Visgoodatrecognizingthescenes,
capturingtemporalinformationinavideoisnotideal. Gemini-Pro-1.5cancapturevideoinformation
such as “waiting their turn while others proceed through the intersection when it’s clear”, but it
6CaptionSimilarity↑ CaptionQuality(eg.reducedhallucination)↑
Method
Nuscenes Pexels Robotics Nuscenes Pexels Robotics
CogAgent[13] 0.18 0.68 0.38 0.24 0.72 0.43
GPT-4V[1] 0.31 0.72 0.34 0.36 0.75 0.35
VILA-1.5[20] 0.21 0.85 0.62 0.25 0.86 0.67
Gemini-Pro-1.5[38] 0.42 0.87 0.63 0.45 0.87 0.67
Wolf 0.55 0.88 0.72 0.56 0.89 0.75
Table1: Comparisonon500highlyinteractive(difficultyandchallenging)Nuscenesvideos,473
Pexelsvideosand100roboticsvideos. Thebestandsecondresultsarehighlightedwithboldand
underline. OurWolfexhibitsbetterperformancethanbothopen-andclosed-sourcemodels.
Method CaptionSimilarity↑ CaptionQuality(eg.reducedhallucination)↑
CogAgent[13] 0.27 0.30
VILA-1.5[20] 0.35 0.39
Wolf(basedonVILA-1.5) 0.56 0.60
Table2: Comparisonon4,785normalNuscenesvideos. ThequalityofWolfisconsistentlybetter.
failstodescribethedetailedmotions. Incomparisontothesetwostate-of-the-artapproaches,we
observedthatWolfnotonlycapturesthemotiondescribedinGemini-Pro-1.5butalsosuccessfully
captionsthemotionbehaviorof“vehiclesmovingindifferentdirections”and“vehiclesaccelerating
anddeceleratingastheyapproachandleavetheintersectioninresponsetotrafficsignalsortheflow
ofothervehicles”.
5.3 QuantitativeResults
WecompareWolfwithvariousstate-of-the-art VILA-1.5 After Finetuning VILA-1.5 with Wolf Captions
0.40
captioningmodelsanddisplaytheresultson4
datasetsinTable1and 2. Inthedefaultsetting,
0.35
WolfusesCogAgent,GPT-4V,VILA-1.5,and
Gemini-Pro-1.5asVideo-levelmodels. Dueto
0.30
therunningcost,weuseWolf(basedonVILA-
1.5)ontheNuscenesNormaldataset,whichonly
0.25
usesCogAgentandVILA-1.5. Wenoticethat
existingimage-levelmodelsfailtocapturethe
0.20
temporalinformationindetail.Video-levelmod- Captioning Similarity Caption Quality
elsperformbetter,whileWolfcanachievethe
bestresultscomparedtoallstate-of-the-artcap- Figure5: ComparisonbetweenVILA-1.5andfine-
tioningmodels. tunedVILA-1.5withWolfprovidedcaptions. on
500highlyinteractiveNuscenesvideos.
5.4 FinetuningVideoCaptioningModels
TofurtherverifytheeffectivenessofWolf,wefinetuneVILA-1.5basedonWolf’scaptionson4,785
normalNuscenesvideosandevaluateiton500highlyinteractiveNuscenesvideos,whichhavemuch
moredifficultcaptionsandcomplexscenarios. WefollowtheoriginalVILA’strainingsetupand
launchsupervised-finetuningwithWolfgeneratedvideo-captionpairsforoneepoch. Thetraining
isperformedon8xA100GPUswithbatchsize8. Wesetthelearningrateto10−4 withwarmup
strategy. Noweightdecayisapplied.
WedemonstratetheresultsinFigure5,correspondingtoTable1. Weobservethatfinetuningwith
Wolfbooststhemodelperformanceto71.4%oncaptionsimilarityand48.0%oncaptionquality,
whichoutperformsGPT-4VandapproachesGemini-Pro-1.5. ThissuggeststhatWolfcaptionscanbe
easilyappliedtopushVLMs’performancetoahigherlevel.
5.5 AblationStudyonVideo-levelModelSelection
Tofurtherevaluatehowvariousvideo-levelmodelsaffecttheperformance,weconductanablation
studyonthecomponentsofthemodelsinTable3. Wefirstcomparethecaptionfromthemiddle
7Method CaptionSimilarity↑ CaptionQuality(eg.reducedhallucination)↑
CogAgent 0.18 0.24
WolfCogAgentpart(chain-of-thought) 0.26 0.32
Wolf(basedonVILA-1.5) 0.35 0.37
Wolf(basedonVILA-1.5+Gemini-Pro-1.5) 0.48 0.49
Wolf(basedonVILA-1.5+Gemini-Pro-1.5+GPT-4V) 0.55 0.56
Table3: Ablationstudyon500highlyinteractiveNuscenesvideos.
Figure6: CaptionSimilarity/QualityevaluatedbyGPT-4undervaryingcaptionlength.
frameofCogAgentwithWolfCogAgentCaptionbasedonthechain-of-thoughtapproach. Thechain-
of-thoughtprocedurecouldlargelyimprovethevideounderstandingqualityfromanimage-level
modelsuchasCogAgent. ThenwecompareWolfwithvariouscombinationsofvideocaptions. We
noticethatWolfconsistenslyshowsbetterCapScoreasitincorporatesadditionalvideocaptions.
5.6 AblationStudyonTokenEfficiency
Itiswell-knownthattheLLMsfinetunedwithRLHFfavorlongerresponse[37], aphenomenon
referredtoasverbosityissue. Tobetterassesstheefficiencyofthecaptions,weperformedadditional
evaluationusingtheCapScorejudge. Specifically,weseparateeachcaptionresultintosentences,
thenincrementallyusemoresentencestoformshortenedcaptions,startingfromonlyusingthefirst
sentence,tothewholeoriginalcaption. TheseshortenedcaptionsarescoredviaCapScore,andwe
plotthescoreagainstthenumberoftokensused. WeshowtheresultsinFigure6.
Fromtheresult,weobservethatforthebetterperformingmodels(Wolf,Gemini-Pro-1.5andGPT-4V)
the similarity scores grow with token length when caption lengths are short, but quickly plateau
or even drop as the caption lengths get too long. The caption quality score demonstrates quite
diversepatternsfromdifferentmodels. GPT-4Vmaintainsarelativelyconsistentqualityscorewhile
Gemini-Pro-1.5andWolfdisplaybetterqualitywhenthecaptionlengthisshort.
6 DiscussionandFutureWorks
LimitationsandOptimization. Wolfisstillsignificantlymorecost-effectiveforautolabelingand
captioning than procuring human labels. However, there is an efficiency concern when using an
ensemblemethodlikeours. ThismustbehandledwithgreatcaretoensurethatGPUresourcesare
usedeffectivelytomitigateanythroughputdegradationcomparedtousingsinglemodels,eventhough
Wolfoffersasignificantimprovementincaptionquality. ModernGPUsarebasedonamassively
parallelpipeline,andourgoalistosaturatethispipelinewithmeaningfulwork. Weconsiderthree
primaryareasforoptimizationtomakeWolfaunifiedandefficientframework: Low-HangingFruit,
BatchedInference,andModelQuantization. Forexample,wereducethesizeofthemodelweights
for model quantization. Several recent works [19, 10, 22] have noted that LLMs and VLMs can
producehighlyaccurateresultsevenwhentheirweightsarequantizedtolowbitdepths. Therefore,
wequantizeallconstituentmodelsusedinWolfto4bitstofurtherimproveefficiency. Thishas
twobenefits. First,itreducesthebandwidthrequiredforcomputation. Thesealgorithmsworkby
packingtwo4-bitnumbersintoasingle8-bittype,sowhenmovingdataontheGPU,onlyhalfthe
numberofbitsneedtobemoved. SinceallcurrentlyreleasedGPUssupportnativeinstructionson
8-bitfloatingpointnumbers,thetwo4-bitnumbersareextractedandexpandedbyeachkernel. In
8otherwords,twocomputationscanbeperformedforeverymoveoperation. Next-generationGPUs
willnativelysupport4-bitdatatypes,andweexpectfurtherefficiencyimprovementsfromhaving
dedicated4-bitmultiplyandaddinstructions. Second,itsynergizeswithbatchedinferencesincethe
modelweights,whicharetraditionally16-bit,nowonlyrequireonequarteroftheGPUmemorythey
wouldordinarilyuse. ThisallowsustofitlargerbatchsizesoneachGPUandprocessmorevideosin
parallel. Pleasecheckourappendixformoreexplanations.
SafetyConsiderations. Asanensembleofcaptioners,Wolfmitigatesthepossibilityofmissingout
oncrucialinformationinthecaptionsandrectifyinganyhallucinationsthatdonotagreewiththe
outputofmostmodels,whichisafundamentalpillarfordevelopingsafeautonomoussystems,as
specifiedinthefunctionalsafetystandardISO26262[33]. BeyondthebenefitsofWolf,thereare
stillvariousopenquestionspertainingtosafetyofVLMcaptionersindeploymentwhichweaimto
exploremoreinfuture: (i)Weneedanapproachtoalignthecaptionswiththetaskathand;e.g.,ina
drivingscenario,adetaileddescriptionofthefoliagearoundtheroad,evenifcorrect,isirrelevant
andcanpotentiallyactasdistractorforthedecisionmaker. (ii)Complementarytothefirstpoint,we
needamethodtomeasurehowwellacaptionalignswiththetaskathandanddevelopanadvanced
versionofCapScore. (iii)Finally,weneedanapproachtoquantifytheconfidencewehaveinthe
captionsbyleveragingtechniquesfromlearningtheory,suchasconformalprediction[36]. Most
priorworkinthisdirectionassumesanMCQ-styledoutputsorthosewhereauniquecorrectanswer
exists[32,31],buttheseapproachesdonottranslatetofree-formtextdescriptions.
7 Conclusion
Inthiswork,weproposeWolf,acaptioningframeworktoannotateanyvideoautomaticallywith
significantdataalignmentimprovement. Wefindoutthatadoptingamixtureofcaptioningmodels
andsummarizationcanlargelyboostthequalityofthecaptions. Thisenablesobtaininglong,detailed,
andaccuratevideocaptioning. SinceWolfenablesunderstandingvideosfromvariousaspects,itcan
beefficientlyusedtounderstandchallengingvideossuchasmultiviewdrivingvideos. Beyondthat,
wesetupaleaderboardtoboostthedevelopmentofvideocaptioning,whichpreservesaguarantee
fordataalignment. Wewillalsosetupathoroughlibrarythatcontainsdifferenttypesofvideoswith
high-qualitycaptions,regionalinformationsuchas2Dor3Dboundingboxesanddepth,andmultiple
objectmotions.
Acknowledgements
WethankJimFan,YukeZhu,andJanKautzfortheirinsightfulfeedbackandthehelpfuldiscussions.
9References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport. arXiv
preprintarXiv:2303.08774,2023.
[2] JasonAnsel,EdwardYang,HoraceHe,NataliaGimelshein,AnimeshJain,MichaelVoznesensky,Bin
Bao,PeterBell,DavidBerard,EvgeniBurovski,GeetaChauhan,AnjaliChourdia,WillConstable,Alban
Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh,
SherlockHuang,KshiteejKalambarkar,LaurentKirsch,MichaelLazos,MarioLezcano,YanboLiang,
JasonLiang, YinghaiLu, CKLuk, BertMaher, YunjiePan, ChristianPuhrsch, MatthiasReso, Mark
Saroufim,MarcosYukioSiraichi,HelenSuk,MichaelSuo,PhilTillet,EikanWang,XiaodongWang,
WilliamWen,ShuntingZhang,XuZhao,KerenZhou,RichardZou,AjitMathews,GregoryChanan,Peng
Wu,andSoumithChintala. PyTorch2: FasterMachineLearningThroughDynamicPythonBytecode
TransformationandGraphCompilation. In29thACMInternationalConferenceonArchitecturalSupport
forProgrammingLanguagesandOperatingSystems,Volume2(ASPLOS’24).ACM,April2024.
[3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang
Zhuang,JoyceLee,YufeiGuo,etal. Improvingimagegenerationwithbettercaptions. ComputerScience.
https://cdn.openai.com/papers/dall-e-3.pdf,2(3):8,2023.
[4] TimBrooks,BillPeebles,ConnorHolmes,WillDePue,YufeiGuo,LiJing,DavidSchnurr,JoeTaylor,
TroyLuhman,EricLuhman,ClarenceNg,RickyWang,andAdityaRamesh. Videogenerationmodelsas
worldsimulators. 2024.
[5] HolgerCaesar,VarunBankiti,AlexH.Lang,SourabhVora,VeniceErinLiong,QiangXu,AnushKrishnan,
YuPan,GiancarloBaldan,andOscarBeijbom. nuscenes:Amultimodaldatasetforautonomousdriving.
arXivpreprintarXiv:1903.11027,2019.
[6] XiChen,JosipDjolonga,PiotrPadlewski,BasilMustafa,SoravitChangpinyo,JialinWu,CarlosRiquelme
Ruiz,SebastianGoodman,XiaoWang,YiTay,etal. Pali-x: Onscalingupamultilingualvisionand
languagemodel. arXivpreprintarXiv:2305.18565,2023.
[7] YuxiaoChen, SanderTonkens, andMarcoPavone. Categoricaltraffictransformer: Interpretableand
diversebehaviorpredictionwithtokenizedlatent. arXivpreprintarXiv:2311.18307,2023.
[8] Wei-LinChiang,LianminZheng,YingSheng,AnastasiosNikolasAngelopoulos,TianleLi,DachengLi,
HaoZhang,BanghuaZhu,MichaelJordan,JosephEGonzalez,etal. Chatbotarena:Anopenplatformfor
evaluatingllmsbyhumanpreference. arXivpreprintarXiv:2403.04132,2024.
[9] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,BoyangAl-
bertLi,PascaleFung,andStevenC.H.Hoi.Instructblip:Towardsgeneral-purposevision-languagemodels
withinstructiontuning. ArXiv,abs/2305.06500,2023.
[10] TimDettmers,ArtidoroPagnoni,AriHoltzman,andLukeZettlemoyer. Qlora: Efficientfinetuningof
quantizedllms. AdvancesinNeuralInformationProcessingSystems,36,2024.
[11] YanDing,XiaohanZhang,ChrisPaxton,andShiqiZhang. Taskandmotionplanningwithlargelanguage
modelsforobjectrearrangement. In2023IEEE/RSJInternationalConferenceonIntelligentRobotsand
Systems(IROS),pages2086–2092.IEEE,2023.
[12] JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras,andYejinChoi. Clipscore:Areference-free
evaluationmetricforimagecaptioning. arXivpreprintarXiv:2104.08718,2021.
[13] WenyiHong,WeihanWang,QingsongLv,JiazhengXu,WenmengYu,JunhuiJi,YanWang,ZihanWang,
YuxiaoDong,MingDing,andJieTang. Cogagent:Avisuallanguagemodelforguiagents,2024.
[14] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,ChrisBamford,
DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,etal. Mixtralof
experts. arXivpreprintarXiv:2401.04088,2024.
[15] BoyiLi,YueWang,JiagengMao,BorisIvanovic,SushantVeer,KarenLeung,andMarcoPavone. Driving
everywherewithlargelanguagemodelpolicyadaptation. 2024.
[16] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.Blip-2:Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. arXivpreprintarXiv:2301.12597,2023.
[17] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip:Bootstrappinglanguage-imagepre-training
for unified vision-language understanding and generation. In International Conference on Machine
Learning,pages12888–12900.PMLR,2022.
[18] BinLin,YangYe,BinZhu,JiaxiCui,MunanNing,PengJin,andLiYuan. Video-llava:Learningunited
visualrepresentationbyalignmentbeforeprojection,2023.
[19] JiLin,JiamingTang,HaotianTang,ShangYang,XingyuDang,andSongHan. Awq:Activation-aware
weightquantizationforllmcompressionandacceleration. arXivpreprintarXiv:2306.00978,2023.
10[20] JiLin,HongxuYin,WeiPing,YaoLu,PavloMolchanov,AndrewTao,HuiziMao,JanKautz,Mohammad
Shoeybi,andSongHan. Vila:Onpre-trainingforvisuallanguagemodels,2023.
[21] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. 2023.
[22] ShumingMa,HongyuWang,LingxiaoMa,LeiWang,WenhuiWang,ShaohanHuang,LiDong,Ruiping
Wang,JilongXue,andFuruWei. Theeraof1-bitllms:Alllargelanguagemodelsarein1.58bits. arXiv
preprintarXiv:2402.17764,2024.
[23] MuhammadMaaz,HanoonaAbdulRasheed,SalmanH.Khan,andFahadShahbazKhan. Video-chatgpt:
Towardsdetailedvideounderstandingvialargevisionandlanguagemodels. arXiv,abs/2306.05424,2023.
[24] JiagengMao,YuxiQian,HangZhao,andYueWang.Gpt-driver:Learningtodrivewithgpt.arXivpreprint
arXiv:2310.01415,2023.
[25] JiagengMao,JunjieYe,YuxiQian,MarcoPavone,andYueWang. Alanguageagentforautonomous
driving. arXivpreprintarXiv:2311.10813,2023.
[26] AlexanderNaumann,FelixHertlein,DanielGrimm,MaximilianZipfl,SteffenThoma,AchimRettinger,
LavdimHalilaj,JuergenLuettin,StefanSchmid,andHolgerCaesar. Lanelet2fornuscenes: Enabling
spatialsemanticrelationshipsanddiversemap-basedanchorpaths. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR)Workshops,pages3247–3256,June
2023.
[27] OpenAI. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023.
[28] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander
Khazatsky,AnantRai,AnikaitSingh,AnthonyBrohan,etal. Openx-embodiment: Roboticlearning
datasetsandrt-xmodels. arXivpreprintarXiv:2310.08864,2023.
[29] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,ShaohanHuang,ShumingMa,andFuruWei.Kosmos-2:
Groundingmultimodallargelanguagemodelstotheworld. arXivpreprintarXiv:2306.14824,2023.
[30] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learning
transferable visual models from natural language supervision. arXiv: Computer Vision and Pattern
Recognition,2021.
[31] AllenZRen,JadenClark,AnushriDixit,MashaItkina,AnirudhaMajumdar,andDorsaSadigh. Explore
untilconfident:Efficientexplorationforembodiedquestionanswering. arXivpreprintarXiv:2403.15941,
2024.
[32] AllenZRen,AnushriDixit,AlexandraBodrova,SumeetSingh,StephenTu,NoahBrown,PengXu,Leila
Takayama,FeiXia,JakeVarley,etal. Robotsthataskforhelp:Uncertaintyalignmentforlargelanguage
modelplanners. arXivpreprintarXiv:2307.01928,2023.
[33] ROHM. ISO26262:Functionalsafetystandardformodernroadvehicles.
[34] Runway.Gen-3alpha.https://runwayml.com/ai-tools/gen-3-alpha/,2024.Accessedon[Insert
Date].
[35] PaulHongsuckSeo,ArshaNagrani,AnuragArnab,andCordeliaSchmid.End-to-endgenerativepretraining
formultimodalvideocaptioning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages17959–17968,2022.
[36] GlennShaferandVladimirVovk. Atutorialonconformalprediction. JournalofMachineLearning
Research,9(3),2008.
[37] PrasannSinghal,TanyaGoyal,JiachengXu,andGregDurrett. Alongwaytogo: Investigatinglength
correlationsinrlhf. arXivpreprintarXiv:2310.03716,2023.
[38] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[39] XiaoyuTian,JunruGu,BailinLi,YichengLiu,ChenxuHu,YangWang,KunZhan,PengJia,Xianpeng
Lang,andHangZhao. Drivevlm: Theconvergenceofautonomousdrivingandlargevision-language
models. arXivpreprintarXiv:2402.12289,2024.
[40] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,Pierric
Cistac,TimRault,RémiLouf,MorganFuntowicz,etal. Huggingface’stransformers: State-of-the-art
naturallanguageprocessing. arXivpreprintarXiv:1910.03771,2019.
[41] LinXu,YilinZhao,DaquanZhou,ZhijieLin,SeeKiongNg,andJiashiFeng. Pllava:Parameter-freellava
extensionfromimagestovideosforvideodensecaptioning. arXivpreprintarXiv:2404.16994,2024.
11[42] AntoineYang,ArshaNagrani,PaulHongsuckSeo,AntoineMiech,JordiPont-Tuset,IvanLaptev,Josef
Sivic,andCordeliaSchmid. Vid2seq:Large-scalepretrainingofavisuallanguagemodelfordensevideo
captioning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages10714–10726,2023.
[43] HangZhang,XinLi,andLidongBing. Video-llama:Aninstruction-tunedaudio-visuallanguagemodel
forvideounderstanding. arXivpreprintarXiv:2306.02858,2023.
[44] TianyiZhang,VarshaKishore,FelixWu,KilianQWeinberger,andYoavArtzi. Bertscore:Evaluatingtext
generationwithbert. InInternationalConferenceonLearningRepresentations,2019.
12A Appendix
A.1 Contributions
WewouldliketolistWolfContributions:
1) Framework and Evaluation Metric. We designed a novel world summarization framework,
Wolf,forvideocaptioningandintroducedanLLM-basedmetric,CapScore,toevaluatethequality
ofcaptions. TheresultsshowthatourmethodsignificantlyimprovesCapScore.
2)DatasetsandBenchmark. WeintroducetheWolfbenchmark(leaderboard)andfourhuman-
annotated benchmark datasets. These datasets include autonomous driving, general scenes from
Pexels,roboticsvideos,andhuman-annotatedcaptions,collectivelyreferredtoastheWolfDataset.
3)IntendedUses. WebelieveWolfcanserveasoneofthebestpractices(auto-labelingtool)for
creatingandcuratingpaireddatasetsandbenchmarks.
4)Hosting,licensing,andmaintenanceplan. Thecode,data,andleaderboardwillbeopen-sourced
andmaintainedontheWolfwebpage4. ContinuouseffortswillbemadetorefinetheWolfDataset,
Wolfcodebase,andCapScore. WehopethatWolfwillraiseawarenessaboutthequalityofvideo
captioning,setastandardforthefield,andboostcommunitydevelopment.
A.2 PexelDatasetCategories
Wecategorizevideosfrompexelintothefollowingtypes: Travel&Events,Sports,Education,Pets
&Animals,People&Blogs,Nonprofits&Activism,News&Politics,Music,Science&Technology,
Comedy,Entertainment,Film&Animation,Gaming,Robotics,HowtoStyles.
EvaluationProcedures.Wehaveprovidedevaluationproceduresin“Sec4.2.1CapScore:Evaluating
CaptionswithLLMs”ofthemainpaper. SinceCapScorecomparescaptionsfromdifferentmethods
simultaneously,itcanguaranteethequalityoftheevaluation. Inourpaper,weranalltheexperiments
threetimesandtooktheaveragescoreinthereportedtablesandfiguresofthepaper.
A.3 QualitativeComparisononInteractiveNuscenesDrivingVideos
WedisplaythedetailsofFigure4ofthepaper(Wolfexamplefordrivingvideosthatfocusoninterac-
tiveoperations)inFigure7. Thecorrespondingvideo(5cca256ed9ac40a1ac2f91fe485a4d5c.mp4)
canbeviewedhere.
A.4 WolfEfficiencyOptimization
Weconsiderthreeprimaryareas:Low-HangingFruit,BatchedInference,andModelQuantization
asoptimizationswhichmakeWolfaunifiedandefficientframework.Usingtheoptimizationsdetailed
inthissectionwewereabletoincreasethespeedofCogVLMbyafactorofapproximately10x
(450s/videoto41s/video),VILAthroughputwassimilarlyimprovedtoonlyabout3spervideo.
Low-HangingFruit. Theseareprimarilysystemsconcernsandworkaroundsforsimplistically
writtenAPIs. Forexample,theoff-the-shelfCogVLM[13]andVILA[20]supportingcodeisheavily
basedonloadingPILimagestopresenttoahuggingfacepipeline[40]. Inthenaivepipeline,videos
wouldneedtobedecodedandthenconvertedtoPILimagesbeforeinputtotherespectivepipelines,
whichinturnconvertthemtoGPUPyTorch[2]tensors. Thisisextremelyinefficient. Instead,we
canleveragethehardwarevideodecoderpresentinmodernGPUstodecodethevideosdirectlyto
GPUtensorsandrewritethepreprocessingpipelinestooperateonthesetensorsdirectly. Thishasthe
additionalbenefitofshiftingpreprocessingtransformworkfromCPUtoGPU.
BatchedInference. SimplifyingWolfintothesimplestterms,weareessentiallyperformingrepeated
neuralnetworkinference. Surprisingly,mostVLMsupportingcodeisdesignedtoruninferenceon
onlyasingleexampleatatime.However,justasinotherdeep-learningproblems,therefundamentally
noreasonwhywecannotprocessesmultiplevideosatasingletimeinbatches. Thisstepiscrucialto
maximizingtheuseofGPUresources. Processingasingleexamplemayonlyuseaslittleas25%of
amoderndatacenterGPUwhichwouldeitherincreasethetimetoprocessadatasetorthenumberof
4https://wolfv0.github.io
13GPUsrequiredtocompleteataskinafixedtimebudget. Wecanreimplementmoreofthesupporting
codetoenableprocessingbatchesofasmanyvideosaswillfitinGPUmemoryatasingletime
yieldingalinearspeedupinprocessing. Forexample,ifwecanfitbatchesof4inGPUmemorywe
observeaspeedupof4xoverprocessingsingleexamples.
ModelQuantization. Thefinaloptimizationweconsideristoreducethesizeofthemodelweights.
Severalrecentworks[19,10,22]havenotedthatLLMsandVLMscanproducehighlyaccurate
resultsevenwhentheirweightsarequantizedtolowbit-depths.Therefore,wequantizeallconstituent
modelsusedinWolfto4-bitstofurtherimproveefficiency. Thishastwobenefits. First,itreduces
thebandwidthrequiredforcomputation. Thesealgorithmsworkbypackingtwo4-bitnumbersintoa
single8-bittype,sowhenmovingdataontheGPUonlyhalfthenumberofbitsneedtobemoved.
SinceallcurrentlyreleasedGPUssupportnativeinstructionson8-bitfloatingpointnumbers,thetwo
4-bitnumbersareextractedandexpandedbyeachkernel. Inotherwords,twocomputationscanbe
performedforeverymoveoperation. NextgenerationGPUswillnativelysupport4-bitdatatypesand
weexpectfurtherefficiencyimprovementsfromhavingdedicated4-bitmultiplyandaddinstructions.
Next,itsynergizeswithbatchedinferencesincethemodelweights,whicharetraditionally16-bit,
nowonlyrequireonequarteroftheGPUmemorytheywouldordinarilyuse. Thisallowsustofit
largerbatchsizesoneachGPUandprocessmorevideosinparallel.
A.5 UpdatedResultsandDocumentation
WewillregularlyupdateWolfresultsanddocumentationonourwebpage. Pleasecheckourupdated
webpageformoreresultsandinformation.
14t=0[s]
t=2[s]
t=5[s]
CogAgent:The picture shows a three-panel scene of a city street. In the first panel, a white truck is driving on the road, and
there are several cars and motorcycles on the street. In the second panel, the white truck is still driving on the road, and
there are several cars and motorcycles on the street. In the third panel, the white truck is driving on the road, and there are
several cars and motorcycles on the street.
VILA-1.5:The video presents a sequence of images showing a street view with a pedestrian crossing. The images feature a
white truck parked on the left side of the frame, a pedestrian crossing the street, and a pedestrian crossing sign visible on
the right side. The sky is clear with few clouds, and the buildings in the background have a modern architectural style. The
street is lined with trees and there are multiple cars parked on the right side of the street. The pedestrian is wearing dark
clothing and is seen walking from one side of the street to the other.
GPT-4:It appears that you‘ve provided a series of still images which seem to be sequential frames from a video that
captures a scene at a traffic intersection. The visual elements include the following: a road with multiple lanes, white lane
markings, and a variety of vehicles, including cars and a truck, navigating the intersection. The vehicles are captured in
various positions in the lanes, implying movement through the area. There appears to be a tree-lined median or central
island, and trees and buildings can be seen in the background, contributing to an urban landscape setting.The motion
behavior depicted across these frames suggests the typical flow of traffic through an intersection. Vehicles are moving in a
coordinated manner, adhering to lane directions and traffic signals that are implied rather than visible in the images. The
sequence likely shows vehicles accelerating and decelerating as they approach and leave the intersection in response to
traffic signals or the flow of other vehicles. The spatial positioning of the vehicles changes progressively, implying a real-
time capture of their movements.If the sequence was put together as a continuous video, you would see vehicles entering
the frame from one side and exiting on the other, with varying speeds and some vehicles possibly turning, illustrating the
dynamic and fluid nature of traffic movement within a regulated urban environment.
Gemini-Pro-1.5:The video is a wide-angle shot of a street intersection. The image is split into three sections, providing a
panoramic view. Here‘s a breakdown:Visual Elements:Perspective:The three-part image creates a wide-angle
perspective, simulating the view a driver might have at the intersection. The left section shows the road the (assumed)
viewer is on, the center focuses on the intersection itself, and the right section provides a view of the crossing traffic.Color
Palette:The video appears to be in color, but the colors are somewhat muted, giving it a realistic but slightly desaturated
feel. Lighting:The lighting suggests a sunny day. The left section has a bright spot from the sun, possibly late afternoon,
while the right section is more evenly lit.Static Elements:Elements like trees, buildings, lampposts, and road markings
provide context and a sense of place. Dynamic Elements:Vehicles, including cars and a truck, move through the
intersection, creating a dynamic scene.Narrative Elements/Motion Behavior:Traffic Flow:The video captures the typical
flow of traffic at an intersection. Vehicles approach, some wait their turn while others proceed through the intersection
when it’s clear.Right-Hand Drive:The cars driving on the right side of the road suggest the location might be a country
where right-hand drive is the standard.Implied Viewer:While there is no visible camera or person, the perspective of the
shot implies the viewer is positioned within a vehicle approaching the intersection.No Sound:The absence of sound
enhances the focus on the visual information and allows the viewer to interpret the scene without auditory cues.Overall
Impression:The video provides a snapshot of everyday life at a busy intersection. The wide-angle perspective and focus on
movement create a sense of dynamism and highlight the complexities of navigating a traffic intersection.
Wolf:The video presents a series of scenes depicting city streets bustling with various vehicles such as cars, trucks,
motorcycles, and a person riding a bicycle. The scenes are often divided into panels, each showcasing a different vehicle in
motion, including a yellow taxi, a blue car, a red car, a white truck, and a silver car. The streets are marked with white lines
and are surrounded by green trees and buildings, contributing to an urban landscape setting. The video captures the
typical flow of traffic through a four-way intersection, with vehicles moving in different directions. The vehicles are shown
in various positions in the lanes, implying movement through the area. The sequence likely shows vehicles accelerating
and decelerating as they approach and leave the intersection in response to traffic signals or the flow of other vehicles.
The video is a wide-angle shot of the street intersection, split into three sections, providing a panoramic view. The left
section shows the road the viewer is on, the center focuses on the intersection itself, and the right section provides a view
of the crossing traffic. The lighting suggests a sunny day, with a bright spot from the sun on the left section and more even
lighting on the right. The video captures the typical flow of traffic at an intersection, with vehicles approaching, some
waiting their turn while others proceed through the intersection when it's clear. The cars driving on the right side of the
road suggest the location might be a country where right-hand drive is the standard. The absence of sound enhances the
focus on the visual information and allows the viewer to interpret the scene without auditory cues. Overall, the video
provides a snapshot of everyday life at a busy intersection, highlighting the complexities of navigating a traffic intersection.
The wide-angle perspective and focus on movement create a sense of dynamism.
Figure7: ComparisonofCogAgent,VILA-1.5,GPT-4,Gemini-Pro-1.5,andWolfonInteractive
NuscenesDrivingVideos,CorrespondingtoFigure4ofthePaper.
15