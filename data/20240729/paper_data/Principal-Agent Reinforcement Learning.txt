Principal-Agent Reinforcement Learning
DimaIvanov∗ PaulDütting† InbalTalgam-Cohen‡ TonghanWang§ DavidC.Parkes¶
Abstract
Contractsaretheeconomicframeworkwhichallowsaprincipaltodelegateatask
toanagent—despitemisalignedinterests,andevenwithoutdirectlyobservingthe
agent’sactions. Inmanymodernreinforcementlearningsettings,self-interested
agentslearntoperformamulti-stagetaskdelegatedtothembyaprincipal. We
explore the significant potential of utilizing contracts to incentivize the agents.
We model the delegated task as an MDP, and study a stochastic game between
theprincipalandagentwheretheprincipallearnswhatcontractstouse,andthe
agentlearnsanMDPpolicyinresponse. Wepresentalearning-basedalgorithm
foroptimizingtheprincipal’scontracts,whichprovablyconvergestothesubgame-
perfectequilibriumoftheprincipal-agentgame. AdeepRLimplementationallows
ustoapplyourmethodtoverylargeMDPswithunknowntransitiondynamics. We
extendourapproachtomultipleagents,anddemonstrateitsrelevancetoresolving
acanonicalsequentialsocialdilemmawithminimalinterventiontoagentrewards.
1 Introduction
Asomewhatimplicityetfundamentalassumptioninboththesingle-agent[75]andmulti-agent[47]
RLliteratureisthatthesameentitythatlearnsandexecutestheactionpolicyinanMDP(Markov
DecisionProcess)istheentitythatfullyenjoysthebenefitsfromitsexecution. However,inmany
real-life scenarios, this basic assumption is violated. For example, drivers exploring new routes
benefittheirnavigationapp[3];usersconsumingandratingonlinecontentbenefitthewebsite[84];
andstudentstakinganonlinecoursefulfillthegoalsofthecourseinstructor[89,40]. Inallofthese
applications,theprincipalbenefitingfromtheagent’sactionscannotdirectlycontrolthem,butisable
toshapetheincentivesbydesigninge.g.agradingschemeorabadgesystem.
Thetheoreticalframeworkthatappliestosuchscenariosisknownineconomicsasprincipal-agent
theory(e.g.,[32,26,66,71]). Intheaforementionedapplications,itdistinguishesbetweentheagents,
whodirectlyinteractwiththeMDPwhileincurringcostsfortheiractions,andtheprincipal,who
receivesrewardsfromtheinteraction. Oneofthemostimportanttoolsineconomicsforshaping
incentivesiscontracts. Acontractdefinespaymentsfromtheprincipaltotheagent(monetaryor
other)basedontheobservableoutcomes(i.e.,rewards)oftheactionschosenbytheagent(s). The
mappingfromactionstorewardsisoften(butnotnecessarily)modeledasbeingstochastic,inwhich
case it may be impossible to infer the chosen actions from the observed outcomes. A standard
assumptionineconomics,knownaslimitedliability,isthatthepaymentsmustbenon-negative. The
agent(s),inturn,chooseanactionpolicythatmaximizestheirutility,givenbypaymentsminuscosts.
Wearethuslookingatastochasticgame,inwhichprincipalandagent(s)bestrespondtoeachother.
ThecelebratedtheoryofcontractswasawardedtheNobelPrizeinEconomicsin2016. Whileithas
seenasurgeofinterestfromthecomputerscienceandlearningcommunitiesrecently(e.g.,[30,19,
∗IsraelInstituteofTechnology,Haifa,Israel.Email:divanov.ml@gmail.com
†Google,Zurich,Switzerland.Email:duetting@google.com
‡IsraelInstituteofTechnology,Haifa,Israel.Email:italgam@cs.technion.ac.il
§HarvardUniversity,Cambridge,USA.Email:twang1@g.harvard.edu
¶HarvardUniversity,Cambridge,USA.Email:parkes@eecs.harvard.edu
Preprint.Underreview.
4202
luJ
52
]TG.sc[
1v47081.7042:viXra20,91,79])ithasreceivedrelativelylittleattentionfromtheRLperspective. Inthisworkwebridge
thisgapbyprovidingtheory-inspired,practicalalgorithmsforsolvingRLproblemswithmisaligned
incentives(withstandardRLpipelines,achievingscalewithdeepRL).
Whyisprincipal-agentRLchallenging? Principal-agentRLischallengingforseveralreasons:
(1)Complexityofthesetting. MDPsareinherentlycombinatorialduetotheirtemporalnature,even
moresowhenmultiplepartiesareinvolved. Contractsarecontinuous,makingthesetofallpossible
contracts(evenforsingle-periodproblems)infinite.(2)Misalignedpreferencesandstrategicbehavior.
Principalandagenthavemisalignedpreferences,eachlearningtomaximizetheirownutilityinthe
presenceoftheother. Privateinformation(suchashiddenactions)furthercomplicatestheproblem.
(3)Approximationandlearning. Inpractice,learninginherentlycomeswithapproximationerrors.
Evenslighterrorsmayhavedevastatingeffectsduetodiscontinuityoftheprincipal’sutilityfunction.
1.1 OurContribution
Weformulateandstudyaprincipal-agentgameinwhichtheagentlearnsapolicyforanMDPon
behalfoftheprincipal,andtheprincipallearnstoguidetheagentviaaseriesofcontracts. After
definingthissetupformally(Section2),wefirstdiscussthepurelyeconomicsetting(Section3)with
fullaccesstotheMDPmodelandnolearningrequired. Wefocusonthestandardsolutionconcept
forextensive-formgames,namelysubgame-perfectequilibrium(SPE)—seeSection3.1.
Akeyobservationisthatfixingoneplayer’spolicydefinesastandardMDPforanotherplayer. Based
onthis,weformulateasimplemeta-algorithm(Algorithm1)thatfindsSPEinafinite-horizongame
inatmostT +1iterations,whereT isthehorizonoftheMDP(Theorem3.3). Themeta-algorithm
iterativelyoptimizestheprincipal’sandagent’spoliciesintheirrespectiveMDPs,butdoesnotspecify
theoptimizationsteps. Wealsogivethemeta-algorithmacleanmathematicalinterpretationasan
iterativeapplicationofacontractionoperatortotheprincipal’sQ-function(Theorem3.4).
Next,weturntothestandardmodel-freeRLsettingwheretheMDPisablackbox,andthepolicies
arelearnedbysamplingstochastictransitionsandrewardsthroughinteractingwiththeMDP.We
instantiatethemeta-algorithmbysolvingbothprincipal’sandagent’sMDPswith(deep)Q-learning
and apply it in a two-phase setup. First, we train the policies assuming the principal has access
totheagent’soptimizationproblem(ofmaximizingitsQ-functionestimategivenacontractina
state). Suchanaccessisastandardassumptionineconomics,anddoesnottrivializetheproblem.
Then,werelaxtheassumptionandvalidatethelearnedprincipal’spolicyagainstblack-boxagents
trainedfromscratch,mimickingitsexecutionintherealworld. Alternatively,itispossibletolift
thisassumptioncompletelybyapplyinganylearningalgorithmfortheone-shotcontractingproblem,
suchasthedeep-learningapproachofWangetal.[79]. Throughthissetup,weverifyempirically
thatourmethodapproximatesSPEwelldespiteearlyterminationandapproximationerrors.
InSection5,weextendourapproachtomulti-agentRLandsequentialsocialdilemmas(SSDs)[42],
ageneralizationofprisoner’s-dilemma-stylesingle-shotgamestomultipletimeperiodsandcomplex
statespaces. AcommonapproachtoSSDsisthroughshapingtheagents’rewards,withafocuson
cooperationandsocialwelfaremaximization. However,theextenttowhichtherewardsaremodified
istypicallyignored,anddespitethevastliteratureonthetopic,thereisnogeneralprocedurefor
findingaminimalinterventionintoagents’rewardsthatdrivescooperation. Weaddressthisgapusing
ourdevelopedprincipal-agentmachinery. WeempiricallyvalidateourapproachonaprominentSSD
knownastheCoinGame[23]. Wecomparetoanalternative,simplerapproachwithhand-coded
paymentschemesinspiredbyareward-redistributionmethodofChristoffersenetal.[8],andobserve
thatwiththesameamountofsubsidy,thisless-targetedinterventionachievesasignificantlylower
welfarelevel.
1.2 RelatedWork
Ourworkfitsintothewiderliteratureonautomatedmechanismdesign[10],inparticularapproaches
basedondeeplearning[18,79]alsoknownasdifferentiableeconomics[21]. Mostcloselyrelated
fromthislineofwork,Wangetal.[79]considerstatelessone-shotcontractingproblemsandprovide
anetworkarchitectureforcapturingthediscontinuitiesintheprincipal’sutilityfunction. Wediffer
fromthisworkinourfocusonsequentialcontractingproblemsandtheentailinguniquechallenges.
2Thereisanumberofalgorithmicworksthatfocusonrepeatedprincipal-agentinteractionsonMDPs,
includingworkonenvironmentdesignandpolicyteaching[87,89,85,3]. Ourapproachdiffersfrom
theseearlierworksinseveralways,includingthatweactivelysearchforthebest(unconstrained)
equilibrium in the game between the principal and the agent through reinforcement learning. A
closely related line of work, including [24], is concerned with learning Stackelberg equilibria in
generalleader-followergames,includinggamesonMDPs. OurworkdiffersinitsfocusonSPE,
which is the more standard equilibrium concept in dynamic contract design problems. Several
workshavestudiedrepeatedcontractdesignproblemsfromano-regretonline-learningperspective
[30, 91, 29, 70]. However, these works are typically limited to stateless and/or non-sequential
interactions. AprominentexceptionisacontemporaneousstudybyWuetal.[82]thatintroducesa
modelofprincipal-agentMDPsnearlyidenticaltoours,barringanimportantnotationaldistinction
ofencodingoutcomesasnextstates. However,theirandourstudiespursueorthogonalalgorithmic
developments: whereastheytreatcontractpoliciesasarmsofabanditandminimizeregret,werely
ondeepRLtoscaletolargeMDPsandmultipleagents.
StartingwiththeworkofLeiboetal.[42],thereisahugeliteratureonSSDs. Mostcloselyrelatedin
thisdirectionistheworkbyChristoffersenetal.[8]onmulti-agentRLandapplicationstoSSDs.
This work pursues an approach in which one of the agents (rather than the principal) proposes a
contract(anoutcome-contingent,zero-sumrewardredistributionscheme),andtheotheragentscan
eitheracceptorveto. TheyconsiderseveralSSDsandshowhowhand-craftedcontractspacesstrikea
balancebetweengeneralityandtractability,andcanbeaneffectivetoolinmitigatingsocialdilemmas.
Animportantdistinctionofourworkisthatwedistinguishbetweenprincipalandagent(s),andinsist
on the standard limited liability requirement from economics. Furthermore, in our approach the
principallearnstheconditionsforpayments,allowingittoutilizecontractsintheirfullgenerality.
Also,ourmethodhasaninterpretationaslearningk-implementation[52].
WeprovideadditionaldetailsontherelatedliteratureinAppendixA.
2 ProblemSetup
Inthissection,wefirstintroducetheclassic(limitedliability)contractdesignmodelofHolmström
[32]andGrossmanandHart[26],andthenproposeitsextensiontoMDPs.Wedeferfurtherextension
tomulti-agentMDPstoSection5.1.
2.1 StaticHidden-ActionPrincipal-AgentProblem
Inaprincipal-agentproblem,theprincipalwantstheagenttoperformatask. Theagenthasachoice
betweenseveralactionsa ∈ Awithdifferentcostsc(a), interpretedaseffortlevels. Eachaction
stochasticallymapstooutcomeso ∈ OaccordingtoadistributionO(a),withhighereffortlevels
morelikelytoresultingoodoutcomes,asmeasuredbytheassociatedprincipal’srewardrp(o). By
default,arationalagentwouldchoosethecost-minimizingaction. Toincentivizetheagenttoinvest
aneffort,theprincipalmayofferacontractbpriortotheactionchoice. Crucially,theprincipalmay
beunable(orunwilling)todirectlymonitortheagent’saction,sothecontractualpaymentsb(o)≥0
aredefinedperoutcomeo ∈ O. Theprincipalseeksanoptimalcontract: apaymentschemethat
maximizestheprincipal’sutility,max E [rp(o)−b(o)],giventhattheagentbest-responds
b o∼O(a)
withmax E [b(o)]−c(a).
a o∼O(a)
If costs and outcome distributions are known, the optimal contract can be precisely found using
LinearProgramming(LP):foreachactiona ∈ A,findthecontractthatimplementsit(makesthe
agentatleastindifferentbetweenthisandotheractions)throughaminimalexpectedpayment,and
thenchoosethebestactiontoimplement. OtherwiseoriftheLPsareinfeasible,anapproximation
canbeobtainedbytraininganeuralnetworkonasampleofpastinteractionswiththeagent[79].
2.2 Hidden-ActionPrincipal-AgentMDPs
InordertoextendcontractdesigntoMDPs,weassumeaprincipal-agentproblemineachstateof
theMDP,andlettheoutcomesadditionallydefineits(stochastic)transitioningtothenextstate. A
hidden-actionprincipal-agentMDPisatupleM=(S,s ,A,B,O,O,R,Rp,T,γ). Asusualin
0
MDPs, S is a set of states, s ∈ S is the initial state, and A is a set of n agent’s actions a ∈ A.
0
Additionally,Oisasetofmoutcomeso ∈ O,B ⊂ Rm isasetofprincipal’sactions(contracts)
≥0
3Figure1: Exampleofaprincipal-agentMDPwiththreestatesS ={s ,s ,s }.Ineachstate,the
0 L R
agentcantakeoneoftwoactions:noisy-left(a ),whichiscostlyandleadstooutcomesLandRwith
L
probabilities0.9and0.1,andnoisy-right(a ),whichisfreeandhastherolesofLandRreversed.
R
The principal’s rewards in any state s ∈ S for outcomes L,R are rp(s,L) = 14,rp(s,R) = 0,
9
resp.,whilethoseoftheagentfortheactionsarer(s,a ) = −4,r(s,a ) = 0. Foranalysis,see
L 5 R
AppendixB.1.
b∈B,andb(o)istheo-thcoordinateofacontract(payment). Ineachstate,theoutcomeissampled
basedontheagent’sactionfromadistributionO(s,a),whereO :S×A→∆(O)istheoutcome
function. Then,theagent’srewardisdeterminedbytherewardfunctionR(s,a,b,o)=r(s,a)+b(o)
forsomer(s,a). Likewise,theprincipal’srewardisdeterminedbyRp(s,b,o)=rp(s,o)−b(o)for
somerp(s,o);notethattheagent’sactionisprivateandthusRp doesnotexplicitlydependonit.
Basedontheoutcome,theMDPtransitionstothenextstates′ ∼T(s,o),whereT :S×O→∆(S)
isthetransitionfunction. Finally,γ ∈[0,1]isthediscountfactor. Foranexampleofaprincipal-agent
MDP,seeFigure1. Inthemaintext,wefocusonMDPswithafinitetimehorizonT. Weassume
w.l.o.g.thateachstateisuniquelyassociatedwithatimestep(seeAppendixB.4).
Weanalyzetheprincipal-agentMDPasastochasticgameG (canbeseenasextensive-formwhen
thehorizonisfinite),wheretwoplayersmaximizetheirlong-termpayoffs. Thegameprogresses
as follows. At each timestep t, the principal observes the state s of the MDP and constructs a
t
contractb ∈ B accordingtoitspolicyρ : S → ∆(B). Then,theagentobservesthepair(s ,b )
t t t
and chooses an action a ∈ A according to its policy π : S ×B → ∆(A). After this, the MDP
t
transitions,andtheinteractionrepeats. Bothplayersmaximizetheirvaluefunctions: theprincipal’s
valuefunction,Vρ(π),ofapolicyρgiventheagent’spolicyπisdefinedinastatesbyVρ(s|π)=
E[(cid:80) γtRp(s ,b ,o )|s =s];likewise,theagent’svaluefunctionVπ(ρ)isdefinedinastatesand
givent acontrat ctbt byt Vπ(0 s,b|ρ)=E[(cid:80) γtR(s ,a ,b ,o )|s =s,b =b]. Players’utilitiesin
t t t t t 0 0
thegamearetheirvaluesintheinitialstate. Additionally,definetheplayers’Q-valuefunctionsQρ(π)
andQπ(ρ)byQρ(s,b|π)=Vρ(s|b =b,π)andQπ((s,b),a|ρ)=Vπ(s,b|a =a,ρ).
0 0
Aspecialcaseoftheprincipal-agentMDPtrivializeshiddenactionsbymakingtheoutcomefunction
deterministicandbijective;theresultingobserved-actionmodelissimilartoBen-Poratetal.[3]. For
anexplicitcomparisonofthetwomodelswitheachotherandwithstandardMDPs,seeAppendixB.
3 PurelyEconomicSetting
Inthissection,wedefineoursolutionconceptforprincipal-agentstochasticgames(Section3.1)and
introduceameta-algorithmthatfindsthissolution(Section3.2). WeassumefullaccesstotheMDP
model(includingtransitionandrewardfunctions)andaddressthelearningsettinginthenextsection.
3.1 Subgame-PerfectEquilibrium(SPE)
InwhatfollowsletG beaprincipal-agentstochasticgame. LetasubgameofG instates∈S bea
gameG′definedbyreplacings withs,andS withasubsetofstatesthatcanbereachedfroms.
0
Observation3.1. Fixingoneplayer’spolicyinG definesa(standard)MDPforanotherplayer. In
particular,aprincipal’spolicydefinestheagent’sMDPbymodifyingtherewardfunctionthrough
contracts;likewise,anagent’spolicydefinestheprincipal’sMDPbymodifyingthetransitionand
4Algorithm1Meta-algorithmforfindingSPE
1: Initializetheprincipal’spolicyρarbitrarily ▷e.g.,∀s∈S :ρ(s)=0
2: whileρnotconvergeddo ▷Inner-outeroptimizationloop
3: Solvetheagent’sMDP:findπ :=π∗(ρ) ▷Inneroptimizationlevel
4: Solvetheprincipal’sMDP:findρ:=ρ∗(π) ▷Outeroptimizationlevel
5: return(ρ,π)
rewardfunctionsthroughtheagent’sresponsestocontracts. ForexactformulationsoftheseMDPs,
seeAppendixB.2. TheoptimalpoliciesinbothMDPscanbeassumedw.l.o.g.tobedeterministic(in
single-agentsetting),andcanbefoundwithanysuitabledynamicprogrammingorRLalgorithm[75].
Agent’sperspective. Intheagent’sMDPdefinedbyρ,refertotheoptimalpolicyasbest-responding
to ρ (where ties are broken in favor of the principal, as is standard in contract design). Define
a function π∗ that maps a principal’s policy ρ to the best-responding policy π∗(ρ). Denote the
actionprescribedbyπ∗(ρ)instatesgivencontractbbyπ∗(s,b | ρ) ≡ π∗(ρ)(s,b). Notethatthe
best-respondingpolicyisdefinedinsforanyb∈Bandisnotlimitedtotheprincipal’sactionρ(s).
Principal’sperspective.Similarly,intheprincipal’sMDPdefinedbyπ,refertotheoptimalpolicyas
subgame-perfectagainstπ. Defineafunctionρ∗thatmapsanagent’spolicyπtothesubgame-perfect
policyρ∗(π). Denotethecontractprescribedbyρ∗(π)instatesbyρ∗(s | π) ≡ ρ∗(π)(s). Inall
states,thispolicysatisfies:ρ∗(s|π)∈argmax Q∗(s,b|π),whereQ∗(s,b|π)≡Qρ∗(π)(s,b|π)
b
–thatis,ineachsubgame,theprincipaltakestheoptimalaction.
Definition3.2. Asubgame-perfectequilibrium(SPE)ofG isapairofpolicies(ρ,π),wherethe
principal’spolicyρ≡ρ∗(π)issubgame-perfectagainstanagentthatbest-respondswithπ ≡π∗(ρ).
SPEisastandardsolutionconceptforextensive-formgames[55]. Italwaysexistsandisessentially
unique(seeLemmaB.11forcompleteness). Comparedtonon-subgame-perfectsolutionslikethe
well–studiedStackelbergequilibrium,SPEcanloseutilityfortheprincipal. However,itdisallows
threatsthatarenon-credible,i.e.,requireplayingasuboptimalcontractinasubgame.Wedemonstrate
thedifferenceonourexampleinAppendixB.1. Furthermore,GerstgrasserandParkes[24]show
thatlearningaStackelbergequilibriumnecessitatestheprincipaltogothroughlongepisodeswith
observationsofthelearningdynamicsofthefollowers,andwithonlysparserewards(seealsoBrero
etal.[5]). SPE,incontrast,naturallyfitsRL,asbothplayers’policiessolvetherespectiveMDPs.
3.2 Meta-AlgorithmforFindingSPE
Algorithm1presentsageneralpipelineforfindingSPEinaprincipal-agentstochasticgame. Itcanbe
seenasaninner-outer(bilevel)optimizationloop,withagentandprincipaloptimizationrespectively
constitutingtheinnerandouterlevels. Werefertothisasameta-algorithm,aswedonotyetspecify
howtoperformtheoptimization(inLines3and4). Superficially,thisapproachresemblestheuseof
bileveloptimizationforlearningoptimalrewardshaping[72,33,7,78,6,49]. Thecrucialdifference
of that setting is that the two levels optimize the same downstream task rather than distinct and
possiblyconflictingobjectivesofprincipalandagent.
Itiswell-knownthatSPEofanextensive-formgamecanbefoundwithbackwardinduction(e.g.,see
Section5.6ofOsborne[55]). Theorem3.3statesthattheproposedmeta-algorithmalsofindsSPE.
Theproof,providedinAppendixB.4,essentiallyshowsthatitperformsbackwardinductionimplicitly.
Thatis,eachiterationofthemeta-algorithm,theplayers’policiesreachSPEinanexpandingsetof
subgames,startingfromterminalstatesandendingwiththeinitialstate. Theproofdoesnotrelyon
thespecificsofourmodelandappliestoanygamewhereplayersmovesequentially,andtheagent
observesandcanrespondtoanyprincipal’sactioninastate.
Theorem3.3. Givenaprincipal-agentstochasticgameGwithafinitehorizonT,themeta-algorithm
findsSPEinatmostT +1iterations.
Themeta-algorithmhasseveraluniqueadvantages. First,aswediscussinSection4,bothinnerand
outeroptimizationtaskscanbeinstantiatedwithQ-learning. Thisremovestheneedtoknowthe
modeloftheMDPandallowshandlingoflarge-scaleMDPsbyutilizingdeeplearning. Second,it
canalsobeseenasiterativelyapplyingacontractionoperator,whichweformulateasatheorem:
5Table1: Correctnessofthemeta-algorithmindifferentscenarios
finitehorizonT infinitehorizon
hiddenaction findsSPEinT +1iterations(Theorem3.3) maydiverge(AppendixB.6)
observedaction findsSPEthatisalsoStackelbergin1iteration(AppendixB.7)
Theorem3.4. Givenaprincipal-agentfinite-horizonstochasticgameG,eachiterationofthemeta-
algorithmappliestotheprincipal’sQ-functionanoperatorthatisacontractioninthesup-norm.
TheproofisprovidedinAppendixB.5.Thispropertyimpliesthateachiterationofthemeta-algorithm
monotonically improves the principal’s policy in terms of its Q-function converging. This has a
practicaladvantage: ifmeta-algorithmisterminatedearly,thepoliciesstillpartiallyconvergetoSPE.
Asanindependentobservation,Theorem3.3complementsthetheoreticalresultsofGerstgrasser
andParkes[24]. Specifically,theirTheorem2presentsanexamplewhereRLfailstoconvergetoa
Stackelbergequilibriumiftheagent‘immediately’bestresponds. ThisprocedureisourAlgorithm1,
with agent optimization solved by an oracle and principal optimization performed with RL. Our
Theorem3.3complementstheirnegativeresultbyshowingthatsuchaprocedureconvergestoSPE.
Finally,whilewefocusonfinite-horizonhidden-actionMDPsinthemaintext,wealsoanalyzethe
otherscenariosintheAppendix. OurfindingsaresummarizedinTable1.
4 LearningSetting
In this section, we develop an RL approach to principal-agent MDPs by solving both inner and
outeroptimizationtasksofthemeta-algorithmwithQ-learning. Thesetaskscorrespondtofinding
optimal policies in the respective agent’s and principal’s MDPs defined in Observation 3.1. We
operateinastandardmodel-freeRLsetting,wherelearningisperformedthroughinteractionswitha
black-boxMDP.Forbothprincipalandagent,weintroducemodifiedQ-functions,whichweformally
deriveasfixedpointsofcontractionoperatorsinAppendixB.3. Wedetaildeepimplementationsin
AppendixD.Ourapproachconsistsofthefollowingtwo-phasesetup:
1. Training: Principal’spolicyistrained‘forfree’insimulatedinteractionswithagentand
MDP.Principalhasaccesstothelearningagentandessentiallycontrolsit.
2. Execution/Validation: Trainedprincipal’spolicycanbeexecutedorvalidatedagainsta
black-box(possiblylearning)agent.
Thisisincontrastwiththeonlinesetupwheretheprincipalinteractswithanactualblack-boxagent
duringlearning,incurringlossesfrompaymentsintheprocess[31,92]. Ontheotherhand,thissetup
isonestepaheadoftheMARLliteratureadjacenttoourapplicationinSection5,wheretheanalysis
istypicallylimitedtothetrainingphase.
Agent’sperspective. Considertheagent’sMDPdefinedbyprincipal’spolicyρ. Thebest-responding
agent’sQ-functioninastates,Q∗((s,b),a|ρ),dependsontheobservedcontractb–particularlyon
theexpectedpayment. ThiseffectcanbeisolatedbyapplyingBellmanoptimalityoperator:
Q∗((s,b),a|ρ)=E [b(o)]+Q∗ (s,a|ρ), (1)
o∼O(s,a)
whereQ∗ (s,a|ρ)=[r(s,a)+γEmax Q∗((s′,ρ(s′)),a′ |ρ)]isthetruncatedoptimalQ-function,
a′
whichrepresentsagent’sexpectedlong-termutilitybarringtheimmediatepayment. Ourapproachto
trainingtheagent(solvinginneroptimization)istolearnthetruncatedQ-functionandcomputethe
Q-functionthrough(1). Thisway,theQ-functionisdefinedforanyb∈Bins,underanassumption
that the principal plays according to ρ in future (e.g., ρ(s′) in the next state). From the agent’s
perspective,thisisjustifiedinSPE,whereρisoptimalfortheprincipalinallfuturesubgames. Note
that computing the expected payment requires the outcome distribution – if unknown, it can be
approximatedasaprobabilisticclassifier(moreonthisinAppendixD.1).
Principal’sperspective. Considertheprincipal’sMDPdefinedbyabest-respondingagentπ∗(ρ)for
anarbitraryρ. Thebasicideaistodividetheprincipal’slearningproblemintotwoparts: 1)learnthe
6agent’spolicythattheprincipalwantstoimplement(recommendstotheagent),and2)computethe
optimalcontractsthatimplementit(theminimalimplementation)usingLinearProgramming(LP).
Essentially,thisextendstheclassicLPapproachfromstaticcontractdesigndescribedinSection2.1.
Toapproachthefirstsubproblem,weneedananalogueoftheprincipal’sQ-functionthatisafunction
ofanagent’saction. Tothisend,wedefinethecontractualQ-functionq∗(π∗(ρ)):S×A→Rby
q∗(s,ap |π∗(ρ))= max Q∗(s,b|π∗(ρ)), (2)
{b|π∗(s,b|ρ)=ap}
whichcanbeinterpretedinsasthemaximalprincipal’sQ-valuethatcanbeachievedbyimplementing
ap ∈ A. To compute the optimal contract argmax Q∗(s,b | ρ) using q∗(π∗(ρ)), we can select
b
theoptimalactiontoimplementasargmax q∗(s,ap | π∗(ρ)), andthenfindthecorresponding
ap
contractassolutiontotheconditionalmaximizationin(2). Thisconditionalmaximizationisthe
secondsubproblemdefinedabove. WesolveitasLP(fordetails,seeAppendixB.8):
maxE [−b(o)] s.t.
o∼O(s,ap)
b∈B (3)
∀a∈A:E [b(o)]+Q∗ (s,ap |ρ)≥E [b(o)]+Q∗ (s,a|ρ).
o∼O(s,ap) o∼O(s,a)
∗
SolvingthisLPrequiresaccesstotheagent’struncatedQ-functionQ . Althoughthisrequirement
isinlinewiththetrainingphaseofoursetup,itcanbealleviated,e.g.,byapproximatingoptimal
contractswithdeeplearning[79]. Wedonotexplorethisdirection,soastonotconflatethedistinct
learningproblemsoriginatingfromourMDPformulationandtheagentbeingblack-box.
Practical concerns. The above instantiation of the meta-algorithm assumes that Q-learning is
rununtilconvergencetopreciselysolvebothinnerandouteroptimizationtasks. Inpractice,one
has to terminate early and approximate; in case of using deep RL, function approximation also
contributestotheerror. Inourmodel,evenasmallerrorcanhaveadevastatingeffectbecausethe
principal’sQ-functionisdiscontinuous: amisestimationoftheoptimalcontract(howeverslight)may
changetheagent’saction,resultinginaworseoutcome(seealso[79]). Weempiricallyvalidatethe
robustnessofourimplementationtothiseffect: InAppendixD.1,weapplyittosolvetoytreeMDPs
(generalizationsofFigure1). Furthermore,ourmulti-agentexperimentsinSection5.3arevalidated
inacomplexandhighlycombinatorialsequentialsocialdilemma. Wereportadditionalmulti-agent
experimentsinAppendixD.2,whereweapplyaformofnudgingtheagentstodesirablebehaviour
throughextrapayments,whichhelpscounteractthedegradingeffectofapproximationerrors.
5 ExtensiontoMulti-AgentRL
Inthissection,weexploreanextensiontomultipleagents. WestatetheformalmodelinSection5.1.
Weintroducesequentialsocialdilemmas(SSDs)andtheCoinGame(inSection5.2). Wepresent
experimentalresultsinSection5.3.
5.1 ProblemSetup
Bothourprincipal-agentmodelandourtheoryforthemeta-algorithmcanbeextendedtomulti-agent
MDPs. First, we formulate an analogous principal-multi-agent MDP, where a principal offers a
contracttoeachagent,andpaymentsaredeterminedbythejointactionofallagents. Wetreatjoint
actionsasoutcomesandomithiddenactions. Then,thetheoryextendsbyviewingallagentsasa
centralizedsuper-agentthatselectsanequilibriumjointpolicy(inthemulti-agentMDPdefinedbythe
principal). Finally,weaddresstheissueofmultipleequilibriabyimposinganadditionalconstraint
onincentive-compatibilityofcontracts,makingourapproachmorerobusttodeviations. Seealso
AppendixC,whereweillustratethemulti-agentmodelonPrisoner’sDilemma.
A principal-multi-agent MDP is a tuple M = (S,s ,N,(A ) ,B,T,(R ) ,Rp,γ). The
N 0 i i∈N i i∈N
notationisasbefore,withtheintroductionofasetofkagents,N,andthecorrespondingchanges:A
i
(cid:81)
istheactionsetofagenti∈N withn elements;A isthejointactionsetwithm= n elements,
i N i i
definedasaCartesianproductofsetsA ;B ⊂Rm isasetofcontractstheprincipalmayoffertoan
i ≥0
agent;b denotesacontractofferedtoagenti,andb (a)denotesapaymenttoideterminedbyjoint
i i
actiona∈A ;T :S×A →∆(S)isthetransitionfunction;R :S×A ×B →Risthereward
N N i N
functionofagentidefinedbyR (s,a,b )=r (s,a)+b (a)forsomer ;Rp :S×A ×Bk →R
i i i i i N
istheprincipal’srewardfunctiondefinedbyRp(s,a,b)=rp(s,a)−(cid:80)
b (a). Inourapplication,
i i
7theprincipal’sobjectiveistomaximizeagents’socialwelfarethroughminimalpayments,sowe
defineitsrewardbyrp(s,a) = 1 (cid:80) r (s,a),where0 < α < 1isahyperparameterthatensures
α i i
thatpaymentminimizationisasecondarycriterionanddoesnothurtsocialwelfare(weuseα=0.1).
Additionally,ρ : S → Bk istheprincipal’spolicy,andπ : S×B → ∆(A )isanagent’spolicy.
i i
BecauseB growsexponentiallywiththenumberofagentsk,inourimplementation,theprincipal
givesanactionrecommendationtoeachagent,andthepaymentsaredeterminedafteragentsact.
AnalogouslytoObservation3.1,afixedprincipal’spolicyρdefinesamulti-agentMDPbychanging
theagents’rewardfunctions.Importantly,thisMDPcanitselfbeanalyzedasaMarkovgamebetween
theagents[47]. Inthisgame,weuseabasicsolutionconceptcalledMarkovPerfectEquilibrium
(MPE)[50],definedasatupleofagents’policiesπ∗(ρ)suchthatthefollowingholds:
∀s,b ,i,π :Vπ i∗(ρ) (s,b |π∗ (ρ),ρ)≥Vπi(s,b |π∗ (ρ),ρ). (4)
i i i i −i i i −i
Here,π∗ (ρ)denotesequilibriumpoliciesofagentsotherthani,andVπi(· | π ,ρ)isthevalue
−i i −i
functionofagentiplayingπ giventhatotheragentsplayπ andprincipalplaysρ. InMPE,no
i −i
agenthasabeneficial,unilateraldeviationinanystate.
CallMPEπ∗(ρ)abest-respondingjointpolicy;incasetherearemultiple,assumethatagentsbreak
tiesinfavoroftheprincipal. Thisassumptionallowsagentstofreelycoordinatethejointaction,
similarlytotheequilibriumoracleofGerstgrasserandParkes[24]. Theprincipal’ssubgame-perfect
policyisdefinedbyρ∗(s|π)∈argmax Q∗(s,b|π),andanSPEisdefinedasapairofpolicies
b
(ρ,π∗(ρ))thatarerespectivelysubgame-perfectandbest-respondingagainsteachother.
Withthis,ourtheoryinSection3canbeextendedtothemulti-agentmodel. Particularly,convergence
proofsofAlgorithm1applywiththeswapofnotationandthenewdefinitionofbest-respondingpolicy
π∗(ρ). However,implementingthistheoryisproblematicbecauseofhowstrongthetie-breaking
assumption is. In practice, there is no reason to assume that decentralized learning agents will
convergetoanyspecificequilibrium. Forexample,eveninsimplegamessuchasIteratedPrisoner’s
Dilemma,RLagentstypicallyfailtoconvergetocooperativeTit-for-Tatequilibrium[23,81].
To provide additional robustness, we specify the principal’s policy ρ in SPE by requiring that it
implementsMPEπ∗(ρ)indominantstrategies. Specifically,ρmustadditionallysatisfy:
∀s,b ,i,π ,π : Vπ i∗(ρ) (s,b |π ,ρ)≥Vπi(s,b |π ,ρ). (5)
i i −i i i −i i i −i
Thisway, anagentprefersπ∗(ρ)regardlessofotherplayers’policies. Werefertocontractsthat
i
makeastrategyprofiledominantasIncentive-Compatible(IC),andtotheICcontractsthatminimize
paymentsasaminimalimplementation. Intheseterms,theprincipal’sobjectiveistolearnasocial
welfaremaximizingstrategyprofileanditsminimalimplementation. Thissolutionconceptisinspired
bythek-implementationofMondererandTennenholtz[52].
5.2 ASequentialSocialDilemma: TheCoinGame
Inthissection,weaugmentamulti-agentMDPknownastheCoinGame[23]withaprincipaland
conductaseriesofexperiments. Theseexperimentscomplementourtheoreticalresultsbyempirically
demonstratingtheconvergenceofouralgorithmtoSPEinacomplexmulti-agentsetting. Onthe
otherhand,wefindaminimalimplementationofastrategyprofilethatmaximizessocialwelfareina
complexSSD,whichisanovelresultofindependentinterest,asdiscussedintheIntroduction.
Environment. TheCoinGameisastandardbenchmarkinMulti-AgentRLthatmodelsanSSD
withtwoself-interestedplayersthat,iftrainedindependently,failtoengageinmutuallybeneficial
cooperation. Thisenvironmentishighlycombinatorialandcomplexduetoalargestatespaceand
theinherentnon-stationarityofsimultaneouslyactingandlearningagents. Eachplayerisassigneda
color,redorblue,andcollectscoinsthatspawnrandomlyonagrid. Playersearn+1forcollectinga
coinoftheircolorand+0.2forothercoins. 6 Ourexperimentsarecarriedoutona7×7gridwith
eachepisodelastingfor50timesteps;resultsonasmallergridareprovidedinAppendixD.2.
Experimentalprocedure. GiventhecomplexityoftheCoinGame,comparingwithexactsolutions
is infeasible. Instead, we implement the two-phase approach described in Section 4. First, we
parameterize principal’s and agent’s Q-functions as deep Q-networks, θ and ϕ, and train them
6Weusetherllibcodebutremovethepenaltyaplayerincursifitscoinispickedupbytheotherplayer.
8(a)Socialwelfare (b)Proportionofsocialwelfarepaid
(c)Accuracy (d)ConvergencetoSPE? (e)Incentive-Compatiblecontracts?
Figure2: LearningcurvesintheCoinGame. SeeSection5.3forplotexplanations. Shadedregions
representstandarderrorsinthetopplotsandmin-maxrangesinthebottomplots.
centrallyusingVDN[74]andparametersharing. Then,thetrainedprincipal’spolicyisvalidatedby
trainingfromscratchnew,black-boxagents. Fordetailsandpseudocode,seeAppendixD.2.
Forbaselines,wecompareagainstaheuristicthatdistributesaconstantproportionofsocialwelfare.
Forafaircomparison,thisproportionissettobeexactlyequaltotheproportionthatourmethodends
uppayingafterthevalidationphase. Thisheuristicisatthecoreofapproachesthatimprovesocial
welfarethroughcontractualagreementsbetweenagents(Christoffersenetal.[8],seeAppendixA).
We also include a selfish baseline with self-interested agents in the absence of contracts, and an
optimalbaselinewhereagentsarefullycooperativeanddirectlymaximizesocialwelfare. Theseare
instancesoftheconstantproportionbaselinewiththeproportionsetto0and1,respectively.
5.3 ExperimentalResults
TheresultsarepresentedinFigure2. Thesocialwelfaremetric(Fig.2a)showsagapbetweenthe
performances of selfish and optimal baselines, confirming the presence of a conflict of interests.
During training, our algorithm finds a joint policy that matches the optimal performance, and is
implementedwithanaveragepaymentofjustabove30%ofsocialwelfare,substantiallyreducing
theinterventionintotheagents’rewardscomparedtotheoptimalbaseline(Fig.2b).
Afterthevalidationphase,thesocialwelfareandtheproportionpaidtoagentscloselymatchthe
correspondingmetricsintraining. Furthermore,theagentsfollowtheprincipal’srecommendationsin
around80%to90%ofstatesinanaverageepisode(Fig.2c). Theseresultssuggestthattheprincipal
closely approximated the SPE, as agents deviate only rarely and in states where it does not hurt
socialwelfare. GiventhechallengesofconvergenceofindependentRLagentstomutuallybeneficial
equilibria,wefindthissuccessquitesurprising,andattributeittotheICpropertyoftheprincipal.
Fromtheperspectiveofanagent,therecouldbeotheroptimalpoliciesagainstdifferentopponents,
butfollowingtheprincipal’srecommendationsisrobustagainstanyopponent.
Wealsoseethattheconstantproportionbaselineismuchlesseffectivethanouralgorithmwhen
giventhesameamountofbudget. Theheuristicschemeoverpaysinsomestateswhileunderpaying
inothers—incentivizingagentstoselfishlydeviatefromawelfare-optimizingpolicy.
Theseresultssuggestthealgorithm’sconvergencetoSPEandtheICpropertyofcontracts. Tofurther
verifythis,wecollectadditionalmetricsthroughoutthevalidationphase. Considertheperspective
oftheblueagent(Blue). Atagiveniteration,wefixtheredagent’s(Red’s)policy,estimateBlue’s
utilities(averagereturns)underitspolicyandtherecommendedpolicy,andcomparetheirratio. In
thisscenario, ifRedfollowsarecommendedpolicy, thenautilityratioexceeding1wouldmean
thatthereisabetterpolicyforBluethantherecommendedone,indicatingaviolationoftheSPE
condition(4)ins . WereportthisratioinFigure2d. Althoughagentsoccasionallydiscoverslightly
0
9moreprofitablepolicies,theaverageutilityratiohoversaround1,indicatinganapproximateSPE.In
thesamescenario,ifinstead,Redactsaccordingtoitsownpolicy,thenautilityratioexceeding1for
BluewouldindicateaviolationoftheICconditions(5)ins . WereportthisratioinFigure2e. It
0
behavessimilarly,inthattheaverageratioagainhoversaround1. Weconcludethattheprincipalis
findingagoodapproximationtoaminimalimplementationthatmaximizessocialwelfare.
6 Conclusion
Inthiswork,wetakeanautomateddesignapproachtodelegatedsingle-andmulti-agentRLproblems.
Inadditiontoprovidingaformaldefinitionofsuchproblems,wegiveasimplealgorithmicblueprint
for solving these games through contracts, and show convergence to SPE. We offer a deep RL
implementation, and empirically validate the guarantees of our algorithms. We also explore an
applicationofthecontract-drivenapproachtosequentialsocialdilemmas,showinghowtheycan
be an effective tool for maximizing social welfare with minimal intervention. Our research, and
particularlytheapplicationtoSSDs,opensthedoortomanyexcitingfollow-upquestions. Whilethe
CoinGamepresentsachallengingsetup,itwouldbeinterestingtofurtherscaleouralgorithmsto
evenmorecomplexenvironments. Partially-observablesettingscouldbeofparticularinterestdueto
thepotentialinformationasymmetrybetweentheprincipalandtheagents. Additionally,allowingthe
principaltorandomizecontractscouldenhanceitsabilitytocoordinateagents. Overall,wehopethis
studywillmakethefieldofcontractdesignmoreaccessibletotheRLcommunity,aswellasallow
contractdesigntoscaletopreviouslyinfeasibleproblems.
References
[1] N. Ananthakrishnan, S. Bates, M. Jordan, and N. Haghtalab. Delegating data collection in
decentralized machine learning. In International Conference on Artificial Intelligence and
Statistics,pages478–486.PMLR,2024.
[2] T. Baumann, T. Graepel, and J. Shawe-Taylor. Adaptive mechanism design: Learning to
promotecooperation. In2020InternationalJointConferenceonNeuralNetworks(IJCNN),
pages1–7.IEEE,2020.
[3] O.Ben-Porat,Y.Mansour,M.Moshkovitz,andB.Taitler. Principal-agentrewardshapingin
mdps. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages
9502–9510,2024.
[4] L.Biewald. Experimenttrackingwithweightsandbiases,2020. URLhttps://www.wandb.
com/. Softwareavailablefromwandb.com.
[5] G.Brero,A.Eden,D.Chakrabarti,M.Gerstgrasser,V.Li,andD.C.Parkes. Learningstackel-
bergequilibriaandapplicationstoeconomicdesigngames. arXivpreprintarXiv:2210.03852,
2022.
[6] S.Chakraborty,A.S.Bedi,A.Koppel,D.Manocha,H.Wang,M.Wang,andF.Huang. Parl: A
unifiedframeworkforpolicyalignmentinreinforcementlearning. InTheTwelfthInternational
ConferenceonLearningRepresentations,2023.
[7] S.Chen,D.Yang,J.Li,S.Wang,Z.Yang,andZ.Wang. AdaptivemodeldesignforMarkov
decisionprocess. InInternationalConferenceonMachineLearning,pages3679–3700.PMLR,
2022.
[8] P. J. Christoffersen, A. A. Haupt, and D. Hadfield-Menell. Get it in writing: Formal con-
tractsmitigatesocialdilemmasinmulti-agentRL. InProceedingsofthe2023International
ConferenceonAutonomousAgentsandMultiagentSystems,pages448–456,2023.
[9] V.ConitzerandN.Garera. Learningalgorithmsforonlineprincipal-agentproblems(andselling
goodsonline). InProceedingsofthe23rdinternationalconferenceonMachinelearning,pages
209–216,2006.
[10] V. Conitzer and T. Sandholm. Complexity of mechanism design. In Proceedings of the
EighteenthconferenceonUncertaintyinartificialintelligence,pages103–110,2002.
10[11] V.ConitzerandT.Sandholm. Automatedmechanismdesign: Complexityresultsstemming
fromthesingle-agentsetting. InProceedingsofthe5thinternationalconferenceonElectronic
commerce,pages17–24,2003.
[12] V.ConitzerandT.Sandholm. Self-interestedautomatedmechanismdesignandimplications
foroptimalcombinatorialauctions. InProceedingsofthe5thACMConferenceonElectronic
Commerce,pages132–141,2004.
[13] C.Daskalakis,C.Tzamos,andM.Zampetakis. AconversetoBanach’sfixedpointtheoremand
itsCLS-completeness. InProceedingsofthe50thAnnualACMSIGACTSymposiumonTheory
ofComputing,pages44–50,2018.
[14] Z. Duan, J. Tang, Y. Yin, Z. Feng, X. Yan, M. Zaheer, and X. Deng. A context-integrated
transformer-basedneuralnetworkforauctiondesign. InInternationalConferenceonMachine
Learning.PMLR,2022.
[15] Z.Duan,H.Sun,Y.Chen,andX.Deng. AscalableneuralnetworkforDSICaffinemaximizer
auctiondesign. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
URLhttps://openreview.net/forum?id=cNb5hkTfGC.
[16] I.Durugkar,E.Liebman,andP.Stone. Balancingindividualpreferencesandsharedobjectives
inmultiagentreinforcementlearning. GoodSystems-PublishedResearch,2020.
[17] P.Dütting,F.Fischer,P.Jirapinyo,J.K.Lai,B.Lubin,andD.C.Parkes. Paymentrulesthrough
discriminant-basedclassifiers. ACMTransactionsonEconomicsandComputation,3(1),2015.
[18] P. Dütting, Z. Feng, H. Narasimhan, D. Parkes, and S. S. Ravindranath. Optimal auctions
throughdeeplearning. InInternationalConferenceonMachineLearning,pages1706–1715.
PMLR,2019.
[19] P. Dütting, T. Roughgarden, and I. Talgam-Cohen. Simple versus optimal contracts. In
Proceedingsofthe2019ACMConferenceonEconomicsandComputation,EC2019,Phoenix,
AZ,USA,June24-28,2019,pages369–387,2019.
[20] P.Dütting,T.Exra,M.Feldman,andT.Kesselheim. Multi-agentcontracts. InACMSTOC
2023,pages1311–1324,2023.
[21] P.Dütting,Z.Feng,H.Narasimhan,D.C.Parkes,andS.S.Ravindranath. Optimalauctions
throughdeeplearning: Advancesindifferentiableeconomics. JournaloftheACM,71(1):1–53,
2024.
[22] T. Eccles, E. Hughes, J. Kramár, S. Wheelwright, and J. Z. Leibo. Learning reciprocity in
complexsequentialsocialdilemmas. arXivpreprintarXiv:1903.08082,2019.
[23] J.Foerster,R.Y.Chen,M.Al-Shedivat,S.Whiteson,P.Abbeel,andI.Mordatch. Learning
withopponent-learningawareness. InProceedingsofthe17thInternationalConferenceon
AutonomousAgentsandMultiAgentSystems,pages122–130,2018.
[24] M.GerstgrasserandD.C.Parkes. Oracles&followers: Stackelbergequilibriaindeepmulti-
agentreinforcementlearning. InInternationalConferenceonMachineLearning,pages11213–
11236.PMLR,2023.
[25] N. Golowich, H. Narasimhan, and D. C. Parkes. Deep learning for multi-facility location
mechanismdesign. InIJCAI,pages261–267,2018.
[26] S.J.GrossmanandO.D.Hart. Ananalysisoftheprincipal-agentproblem. Econometrica,51
(1):7–45,1983.
[27] J.K.Gupta, M.Egorov, andM.Kochenderfer. Cooperativemulti-agentcontrolusingdeep
reinforcement learning. In International conference on autonomous agents and multiagent
systems,pages66–83.Springer,2017.
11[28] B.Guresti,A.Vanlioglu,andN.K.Ure. Iq-flow: Mechanismdesignforinducingcooperative
behaviortoself-interestedagentsinsequentialsocialdilemmas. InProceedingsofthe2023
InternationalConferenceonAutonomousAgentsandMultiagentSystems,pages2143–2151,
2023.
[29] G.Guruganesh,Y.Kolumbus,J.Schneider,I.Talgam-Cohen,E.-V.Vlatakis-Gkaragkounis,J.R.
Wang,andS.M.Weinberg.Contractingwithalearningagent.arXivpreprintarXiv:2401.16198,
2024.
[30] C.Ho,A.Slivkins,andJ.W.Vaughan. Adaptivecontractdesignforcrowdsourcingmarkets:
Bandit algorithms for repeated principal-agent problems. J. Artif. Intell. Res., 55:317–359,
2016.
[31] C.-J.Ho,A.Slivkins,andJ.W.Vaughan. Adaptivecontractdesignforcrowdsourcingmarkets:
Banditalgorithmsforrepeatedprincipal-agentproblems. InProceedingsofthefifteenthACM
conferenceonEconomicsandcomputation,pages359–376,2014.
[32] B.Holmström. Moralhazardandobservability. TheBellJournalofEconomics, 10:74–91,
1979.
[33] Y.Hu,W.Wang,H.Jia,Y.Wang,Y.Chen,J.Hao,F.Wu,andC.Fan.Learningtoutilizeshaping
rewards: A new approach of reward shaping. Advances in Neural Information Processing
Systems,33:15931–15941,2020.
[34] E. Hughes, J. Z. Leibo, M. Phillips, K. Tuyls, E. Dueñez-Guzman, A. García Castañeda,
I.Dunning, T.Zhu, K.McKee, R.Koster, etal. Inequityaversionimprovescooperationin
intertemporalsocialdilemmas. Advancesinneuralinformationprocessingsystems,31,2018.
[35] D.Ivanov,V.Egorov,andA.Shpilman. Balancingrationalandother-regardingpreferencesin
cooperative-competitiveenvironments. InProceedingsofthe20thInternationalConferenceon
AutonomousAgentsandMultiAgentSystems,pages1536–1538,2021.
[36] D.Ivanov, I.Safiulin, I.Filippov, andK.Balabaeva. Optimal-erauctionsthroughattention.
AdvancesinNeuralInformationProcessingSystems,35:34734–34747,2022.
[37] D.Ivanov,I.Zisman,andK.Chernyshev. Mediatedmulti-agentreinforcementlearning. In
Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent
Systems,pages49–57,2023.
[38] N. Jaques, A. Lazaridou, E. Hughes, C. Gulcehre, P. Ortega, D. Strouse, J. Z. Leibo, and
N. De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement
learning. InInternationalconferenceonmachinelearning,pages3040–3049.PMLR,2019.
[39] J.JiangandZ.Lu. Learningfairnessinmulti-agentsystems. AdvancesinNeuralInformation
ProcessingSystems,32,2019.
[40] J.M.KleinbergandM.Raghavan.Howdoclassifiersinduceagentstoinvesteffortstrategically?
ACMTrans.EconomicsandComput.,8(4):19:1–19:23,2020.
[41] S.Lahaie. Akernel-basediterativecombinatorialauction. InProceedingsoftheAAAIConfer-
enceonArtificialIntelligence,volume25,pages695–700,2011.
[42] J.Z.Leibo,V.Zambaldi,M.Lanctot,J.Marecki,andT.Graepel. Multi-agentreinforcement
learninginsequentialsocialdilemmas. InProceedingsofthe16thConferenceonAutonomous
AgentsandMultiAgentSystems,pages464–473,2017.
[43] W.D.Li,N.Immorlica,andB.Lucier. Contractdesignforafforestationprograms. InWINE
2021,pages113–130,2021.
[44] E.Liang,R.Liaw,R.Nishihara,P.Moritz,R.Fox,K.Goldberg,J.Gonzalez,M.Jordan,and
I.Stoica. Rllib: Abstractionsfordistributedreinforcementlearning. InInternationalconference
onmachinelearning,pages3053–3062.PMLR,2018.
[45] A.Likhodedov,T.Sandholm,etal. Approximatingrevenue-maximizingcombinatorialauctions.
InAAAI,volume5,pages267–274,2005.
12[46] Y.Lin,W.Li,H.Zha,andB.Wang. Informationdesigninmulti-agentreinforcementlearning.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[47] M. L. Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machinelearningproceedings1994,pages157–163.Elsevier,1994.
[48] X.Liu,C.Yu,Z.Zhang,Z.Zheng,Y.Rong,H.Lv,D.Huo,Y.Wang,D.Chen,J.Xu,etal.
Neuralauction: End-to-endlearningofauctionmechanismsfore-commerceadvertising. In
Proceedingsofthe27thACMSIGKDDConferenceonKnowledgeDiscovery&DataMining,
pages3354–3364,2021.
[49] S.Lu. Bileveloptimizationwithcoupleddecision-dependentdistributions. InInternational
ConferenceonMachineLearning,pages22758–22789.PMLR,2023.
[50] E.MaskinandJ.Tirole.Markovperfectequilibrium:I.observableactions.JournalofEconomic
Theory,100(2):191–219,2001.
[51] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,
M.Riedmiller,A.K.Fidjeland,G.Ostrovski,etal. Human-levelcontrolthroughdeeprein-
forcementlearning. Nature,518(7540):529–533,2015.
[52] D.MondererandM.Tennenholtz.k-implementation.InProceedingsofthe4thACMconference
onElectronicCommerce,pages19–28,2003.
[53] H.Narasimhan,S.B.Agarwal,andD.C.Parkes. Automatedmechanismdesignwithoutmoney
viamachinelearning. InProceedingsofthe25thInternationalJointConferenceonArtificial
Intelligence,2016.
[54] C. Oesterheld, J. Treutlein, R. B. Grosse, V. Conitzer, and J. N. Foerster. Similarity-based
cooperative equilibrium. In Thirty-seventh Conference on Neural Information Processing
Systems,2023.
[55] M.J.Osborne. Anintroductiontogametheory,volume3. OxforduniversitypressNewYork,
2004.
[56] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N.Gimelshein,L.Antiga,etal. Pytorch: Animperativestyle,high-performancedeeplearning
library. Advancesinneuralinformationprocessingsystems,32,2019.
[57] A.PeysakhovichandA.Lerer. Consequentialistconditionalcooperationinsocialdilemmas
withimperfectinformation. InInternationalConferenceonLearningRepresentations,2018.
[58] A.PeysakhovichandA.Lerer. Prosociallearningagentssolvegeneralizedstaghuntsbetter
thanselfishones. InProceedingsofthe17thInternationalConferenceonAutonomousAgents
andMultiAgentSystems,pages2043–2044.InternationalFoundationforAutonomousAgents
andMultiagentSystems,2018.
[59] T. Phan, F. Sommer, P. Altmann, F. Ritz, L. Belzner, and C. Linnhoff-Popien. Emergent
cooperationfrommutualacknowledgmentexchange. InProceedingsofthe21stInternational
ConferenceonAutonomousAgentsandMultiagentSystems,pages1047–1055,2022.
[60] M.L.Puterman. Markovdecisionprocesses: discretestochasticdynamicprogramming. John
Wiley&Sons,2014.
[61] A.Raffin,A.Hill,A.Gleave,A.Kanervisto,M.Ernestus,andN.Dormann. Stable-baselines3:
Reliablereinforcementlearningimplementations. JournalofMachineLearningResearch,22
(268):1–8,2021. URLhttp://jmlr.org/papers/v22/20-1364.html.
[62] J.Rahme,S.Jelassi,J.Bruna,andS.M.Weinberg. Apermutation-equivariantneuralnetwork
architectureforauctiondesign. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume35,pages5664–5672,2021.
[63] S.S.Ravindranath,Z.Feng,S.Li,J.Ma,S.D.Kominers,andD.C.Parkes. Deeplearningfor
two-sidedmatching. arXivpreprintarXiv:2107.03427,2021.
13[64] S.S.Ravindranath,Y.Jiang,andD.C.Parkes. Datamarketdesignthroughdeeplearning. In
Thirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
[65] P.RennerandK.Schmedders. Discrete-timedynamicprincipal–agentmodels: Contraction
mappingtheoremandcomputationaltreatment. QuantitativeEconomics,11(4):1215–1251,
2020.
[66] W.P.Rogerson. Repeatedmoralhazard. Econometrica,53:69–76,1985.
[67] E.Saig,I.Talgam-Cohen,andN.Rosenfeld. Delegatedclassification. InConferenceonNeural
InformationProcessingSystemsNeurIPS,2023.https://doi.org/10.48550/arXiv.2306.
11475.
[68] T. Sandholm and A. Likhodedov. Automated design of revenue-maximizing combinatorial
auctions. OperationsResearch,63(5):1000–1025,2015.
[69] T.Schaul,J.Quan,I.Antonoglou,andD.Silver. Prioritizedexperiencereplay. arXivpreprint
arXiv:1511.05952,2015.
[70] A. Scheid, D. Tiapkin, E. Boursier, A. Capitaine, E. M. E. Mhamdi, É. Moulines, M. I.
Jordan,andA.Durmus. Incentivizedlearninginprincipal-agentbanditgames. arXivpreprint
arXiv:2403.03811,2024.
[71] S. E. Spear and S. Srivastava. On repeated moral hazard with discounting. The Review of
EconomicStudies,54:599–617,1987.
[72] B.Stadie,L.Zhang,andJ.Ba. Learningintrinsicrewardsasabi-leveloptimizationproblem.
InConferenceonUncertaintyinArtificialIntelligence,pages111–120.PMLR,2020.
[73] X.Sun,D.Crapis,M.Stephenson,andJ.Passerat-Palmbach. Cooperativeaiviadecentralized
commitmentdevices. InMulti-AgentSecurityWorkshop@NeurIPS’23,2023.
[74] P.Sunehag,G.Lever,A.Gruslys,W.M.Czarnecki,V.Zambaldi,M.Jaderberg,M.Lanctot,
N.Sonnerat,J.Z.Leibo,K.Tuyls,etal. Value-decompositionnetworksforcooperativemulti-
agentlearningbasedonteamreward. InProceedingsofthe17thInternationalConferenceon
AutonomousAgentsandMultiAgentSystems,pages2085–2087,2018.
[75] R.S.SuttonandA.G.Barto. Reinforcementlearning: Anintroduction. MITpress,2018.
[76] M. Towers, J. K. Terry, A. Kwiatkowski, J. U. Balis, G. d. Cola, T. Deleu, M. Goulão,
A.Kallinteris,A.KG,M.Krimmel,R.Perez-Vicente,A.Pierré,S.Schulhoff,J.J.Tai,A.T.J.
Shen, and O. G. Younis. Gymnasium, Mar. 2023. URL https://zenodo.org/record/
8127025.
[77] J.X.Wang,E.Hughes,C.Fernando,W.M.Czarnecki,E.A.Duéñez-Guzmán,andJ.Z.Leibo.
Evolvingintrinsicmotivationsforaltruisticbehavior. InProceedingsofthe18thInternational
Conference on Autonomous Agents and MultiAgent Systems, pages 683–692. International
FoundationforAutonomousAgentsandMultiagentSystems,2019.
[78] L.Wang,Z.Wang,andQ.Gong. Bi-leveloptimizationmethodforautomaticrewardshaping
ofreinforcementlearning. InInternationalConferenceonArtificialNeuralNetworks,pages
382–393.Springer,2022.
[79] T.Wang,P.Dütting,D.Ivanov,I.Talgam-Cohen,andD.C.Parkes. Deepcontractdesignvia
discontinuousnetworks. InNeurIPS,2023. forthcoming.
[80] T.Willi,A.H.Letcher,J.Treutlein,andJ.Foerster. COLA:Consistentlearningwithopponent-
learningawareness. InInternationalConferenceonMachineLearning,pages23804–23831.
PMLR,2022.
[81] R.Willis,Y.Du,J.Z.Leibo,andM.Luck. Resolvingsocialdilemmaswithminimalreward
transfer. arXivpreprintarXiv:2310.12928,2023.
[82] J.Wu,S.Chen,M.Wang,H.Wang,andH.Xu. Contractualreinforcementlearning: Pulling
armswithinvisiblehands. arXivpreprintarXiv:2407.01458,2024.
14[83] J.Yang,A.Li,M.Farajtabar,P.Sunehag,E.Hughes,andH.Zha. Learningtoincentivizeother
learningagents. AdvancesinNeuralInformationProcessingSystems,33:15208–15219,2020.
[84] G. Yu and C. Ho. Environment design for biased decision makers. In Proceedings of the
Thirty-FirstInternationalJointConferenceonArtificialIntelligence,IJCAI,pages592–598,
2022.
[85] G.YuandC.-J.Ho. Environmentdesignforbiaseddecisionmakers. InIJCAI2022,2022.
[86] B.H.Zhang,G.Farina,I.Anagnostides,F.Cacciamani,S.M.McAleer,A.A.Haupt,A.Celli,
N.Gatti,V.Conitzer,andT.Sandholm. Steeringno-regretlearnerstoadesiredequilibrium.
arXivpreprintarXiv:2306.05221,2023.
[87] H.ZhangandD.C.Parkes. Value-basedpolicyteachingwithactiveindirectelicitation. In
AAAI,volume8,pages208–214,2008.
[88] H.Zhang,Y.Chen,andD.Parkes. Ageneralapproachtoenvironmentdesignwithoneagent.
In Proceedings of the 21st International Joint Conference on Artificial Intelligence, pages
2002–2008,2009.
[89] H.Zhang,D.C.Parkes,andY.Chen. Policyteachingthroughrewardfunctionlearning. In
Proceedingsofthe10thACMconferenceonElectroniccommerce,pages295–304,2009.
[90] S.Zhao,C.Lu,R.B.Grosse,andJ.N.Foerster. Proximallearningwithopponent-learning
awareness. AdvancesinNeuralInformationProcessingSystems,35,2022.
[91] B.Zhu,S.Bates,Z.Yang,Y.Wang,J.Jiao,andM.I.Jordan. Thesamplecomplexityofonline
contractdesign. InProceedingsofthe24thACMConferenceonEconomicsandComputation,
EC2023,London,UnitedKingdom,July9-12,2023,page1188,2023.
[92] B.Zhu,S.Bates,Z.Yang,Y.Wang,J.Jiao,andM.I.Jordan. Thesamplecomplexityofonline
contractdesign. InProceedingsofthe24thACMConferenceonEconomicsandComputation,
pages1188–1188,2023.
[93] M. Zimmer, C. Glanois, U. Siddique, and P. Weng. Learning fair policies in decentralized
cooperative multi-agent reinforcement learning. In International Conference on Machine
Learning,pages12967–12978.PMLR,2021.
15Appendix
Table of Contents
A RelatedWork 16
A.1 AutomatedMechanismDesign . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 AlgorithmicContractDesign . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 Multi-AgentRL(MARL) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B ProofsandDerivations(Sections3and4) 18
B.1 StackelbergvsSubgame-PerfectEquilibrium(ExampleinFigure1,Revisited) . 18
B.2 Principal’sandAgent’sMDPs(Observation3.1) . . . . . . . . . . . . . . . . . 19
B.3 ContractionOperatorsandtheirFixedPoints . . . . . . . . . . . . . . . . . . . 20
B.4 Meta-AlgorithmfindsSPE(ProofofTheorem3.3) . . . . . . . . . . . . . . . . 23
B.5 Meta-AlgorithmappliesContraction(ProofofTheorem3.4) . . . . . . . . . . . 24
B.6 Meta-AlgorithmmaydivergeinInfinite-HorizonMDPs . . . . . . . . . . . . . 25
B.7 Meta-AlgorithmintheObserved-ActionModel . . . . . . . . . . . . . . . . . . 26
B.8 DerivingtheLinearProgram(Section4) . . . . . . . . . . . . . . . . . . . . . 28
C Principal-Multi-AgentExample:Prisoner’sDilemma 28
D Experiments 29
D.1 ExperimentsinTreeMDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
D.2 ExperimentsintheCoinGame . . . . . . . . . . . . . . . . . . . . . . . . . . 31
D.3 Compute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
A RelatedWork
A.1 AutomatedMechanismDesign
Ourstudyconcernsfindinganoptimalwaytoinfluencethebehaviorofoneormultipleagentsin
anenvironmentthroughoptimization,itcanthusbeattributedtotheautomatedmechanismdesign
literature. ThefieldwaspioneeredbyConitzerandSandholm[10,11,12]; theearlyapproaches
mostlyconcernedoptimalauctionsandreliedonclassicoptimizationandmachinelearningalgorithms
[45,41,68,17,53]. ThefieldhasreceivedasurgeofinterestwiththeintroductionofRegretNet[18]
–adeeplearningbasedapproachtoapproximatelyincentive-compatibleoptimalauctiondesign. This
inspiredmultipleotheralgorithmsforauctiondesign[62,14,36,15],aswellasapplicationsofdeep
learningtoothereconomicareassuchasmulti-facilitylocation[25],two-sidedmatchingmarkets
[63],E-commerceadvertising[48],anddatamarkets[64].
Notably, a deep learning approach to contract design has been recently proposed by Wang et al.
[79]asaviablealternativetolinearprogramminginproblemswithahighnumberofactionsand
outcomes. Since our investigation focuses on scenarios where the primary source of complexity
comesfromlargestatespacesratherthanactionoroutcomespaces,wedonotusetheirapproximation
technique. Attheheartoftheirapproachisanovelneuralnetworkarchitecturespecificallydesigned
toapproximatediscontinuousfunctions. Giventhattheprincipal’sQ-function,Qρ(s,b|π),inour
settingisdiscontinuouswithrespecttob,thisarchitectureholdspotentialtobringfurtherscalability
andpracticalitytoourapproach;weleavethisdirectionasfuturework.
A.2 AlgorithmicContractDesign
Abodyofworkstudiesrepeatedprincipal-agentinteractionsingameseitherstatelessorwithstates
unobservedbytheprincipal(suchasagenttypes). Dependingonthemodel,learninganoptimal
16paymentschemecanbeformalizedasabanditproblemwitharmsrepresentingdiscretizedcontracts
[9,31,92]orasaconstraineddynamicprogrammingproblem[65]. Scheidetal.[70]extendthe
banditformulationtoalinearcontextualsettingandproposealearningalgorithmthatisnear-optimal
in terms of principal’s regret. Zhang et al. [86] formulate a so-called steering problem where a
mediatorcanpayno-regretlearnersthroughoutrepeatedinteractionsandwishestoincentivizesome
desirable predetermined equilibrium while satisfying budget constraints. Similarly, Guruganesh
et al. [29] study a repeated principal-agent interaction in a canonical contract setting, with the
agentapplyingno-regretlearningoverthecourseofinteractions. Lietal.[43]studycontractsfor
afforestationwithanunderlyingMarkovchain;theydonotextendtoMDPsanddonotapplylearning.
Thereisalsoworkonpolicyteaching,whichcanbeseenastheearliestexamplesofcontractdesign
inMDPs. Zhangetal.[89]studyaproblemofimplementingaspecificpolicythroughcontractsand
solveitwithlinearprogramming. ZhangandParkes[87]additionallyaimtofindthepolicyitself,
whichtheyshowtobeNP-hardandsolvethroughmixedintegerprogramming. Contemperaneously
withthepresentwork,Ben-Poratetal.[3]extendtheseresultsbyofferingpolynomialapproximation
algorithmsfortwospecialinstancesofMDPs. These,aswellasourwork,canbeseenasinstances
ofamoregeneralenvironmentdesignproblem[88]. Crucially,theseworksfocusonMDPsofup
toahundredstates. ByemployingdeepRL,weextendtomuchlargerMDPs. Ourapproachalso
generalizestohidden-actionandmulti-agentMDPs.
MondererandTennenholtz[52]proposek-implementation,whichcanbeseenascontractdesign
appliedtonormal-formgames.Specifically,theprincipalwantstoimplement(incentivize)somedesir-
ableoutcomeandcanpayforthejointactionsoftheagents. Thegoalistofindthek-implementation
(we call it a minimal implementation), i.e., such payment scheme that the desirable outcome is
dominant-strategyincentivecompatibleforallagents,whiletherealizedpaymentkforthisoutcome
isminimal. Ourmulti-agentproblemsetupcanbeseenaslearningaminimalimplementationofa
socialwelfaremaximizingstrategyprofileinaMarkovgame.
Relatedtoalgorithmiccontractdesignisaproblemofdelegatinglearningtasksinthecontextof
incentive-awaremachinelearning[1,67]. Thesestudiesconcernaprincipalproperlyincentivizing
agent(s)throughcontractstocollectdataortrainanMLmodelinaone-shotinteraction.
A.3 Multi-AgentRL(MARL)
Inourapplications,wefocusongeneral-sumMarkovgameswherenaivelytrainedagentsfailto
engageinmutuallybeneficialcooperation–colloquiallyknownas“SequentialSocialDilemmas”
or SSDs [42]. The solution concepts can be divided into two broad categories. The majority of
studiestakeapurelycomputationalperspective,arbitrarilymodifyingtheagents’rewardfunctions
[57, 58, 34, 38, 77, 22, 39, 16, 83, 35, 93, 59] or training procedures [27, 23, 80, 90] in order to
maximize the aggregate reward. Alternative solutions view the problem as aligning the players’
incentivesbymodifyingtherulesofthegametoinducebetterequilibria. Examplesincludeenabling
agentstodelegatetheirdecisionmakingtoamediator[37],allowingagentstorevieweachothers’
policiespriortodecisionmaking[54], andaddingacheap-talkcommunicationchannelbetween
agents[46]. Ourworkshouldbeattributedtothesecondcategoryaswemodeltheprincipal-agents
interactionasagame.Whiletheprincipaleffectivelymodifiesagents’rewardfunctions,thepayments
arecostly. Thequestionisthenhowtomaximizesocialwelfarethroughminimalintervention,which
isanopenresearchquestion.
Theworksonadaptivemechanismdesign[2,28]canbeseenasprecursorsofcontractdesignfor
SSDs. These consider augmenting the game with a principal-like planning agent that learns to
distributeadditionalrewardsandpenalties,themagnitudeofwhichiseitherlimitedheuristicallyor
handcoded. Importantly,theplanningagentisnotconsideredaplayer,andthustheequilibriaarenot
analyzed.
Mostrelevanttous,Christoffersenetal.[8]consideracontractingaugmentationofSSDs. Before
anepisodebegins,oneoftheagentsproposesazero-sumrewardredistributionschemethattriggers
according to predetermined conditions. Then, the other agents vote on accepting it, depending
onwhichtheepisodeproceedswiththeoriginalormodifiedrewards. Becausetheconditionsare
handcoded based on domain knowledge, the contract design problem reduces to finding a one-
dimensionalparameterfromadiscretizedintervalthatoptimizestheproposalagent’swelfare,andby
thesymmetryofcontracts,thesocialwelfare. Besidesthetechnicalitythattheprincipalinoursetting
17Table2: ComparisonofstandardandPrincipal-AgentMDPs
MDP Principal-AgentMDP
observedaction hiddenaction
States S S S
Agent’sactions(nelements) A A A
Outcomes(melements) – – O
Principal’sactions — B ⊂Rn B ⊂Rm
≥0 ≥0
MDPtransitioning s′ ∼T(s,a) s′ ∼T(s,a) o∼O(s,a),s′ ∼T(s,o)
Agent’sreward r(s,a) r(s,a)+b(a) r(s,a)+b(o)
Principal’sreward — rp(s,a)−b(a) rp(s,o)−b(o)
Agent’spolicy π(s) π(s,b) π(s,b)
Principal’spolicy — ρ(s) ρ(s)
Agent’svalue Vπ(s) Vπ(s,b|ρ) Vπ(s,b|ρ)
Principal’svalue — Vρ(s|π) Vρ(s|π)
isexternaltotheenvironment,acrucialdistinctionisthatweemploycontractsinfullgenerality,
allowing the conditions for payments to emerge from learning. We empirically verify that this
mayresultinaperformancegap. ToadaptthisapproachtotheCoinGameandrelaxthedomain
knowledgeassumption,we1)duplicatepartsofrewardsratherthanredistributethem,whichcanalso
beinterpretedaspaymentsbyanexternalprincipal,and2)alloweachagenttoimmediatelysharethe
duplicatedpartofitsrewardwiththeotheragent,ratherthanaconstantvalueeverytimeahandcoded
conditionismet. Inexperiments,werefertothisasa‘constantproportionbaseline’.
Thisworkonlycoversfullyobservableenvironments,butourmethodcouldpotentiallybeextended
topartialobservability, limitingtheinformationavailabletotheprincipalandtheagentstolocal
observations. Inthisregard,ourmethodmaybeconsideredashavingdecentralizedexecution. While
thepresenceofaprincipalasathirdpartymaybeconsideredacentralizedelement,eventhiscould
bealleviatedthroughtheuseofcryptography[73].
B ProofsandDerivations(Sections3and4)
In this appendix, we provide formal proofs and derivations for the results in Sections 3 and 4.
Appendix B.1 provides additional intuition on the differences between the solution concepts of
StackelbergandSubgame-PerfectEquilibria. AppendixB.2supplementsObservation3.1anddefines
theprincipal’sandagent’sMDPs. AppendixB.3definescontractionoperatorsusefulforsucceeding
proofsandconnectstheseoperatorstothemodifiedQ-functionsdefinedinSection4. Appendices
B.4andB.5presenttherespectiveproofsofTheorems3.3and3.4. Whilethetheoryinthemain
textfocusesonthefinite-horizonhidden-actionscenario,wealsodiscusstheinfinite-horizonand
observed-action scenarios in appendices B.6 and B.7, respectively. Finally, the linear program
formulatedinSection4isderivedanddescribedinmoredetailinAppendixB.8.
Table2summarizesthedifferencesbetweenstandardMDPsandPrincipal-AgentMDPswithand
withouthiddenactions.
B.1 StackelbergvsSubgame-PerfectEquilibrium(ExampleinFigure1,Revisited)
First, consider the SPE notion: At the left subgame, the principal incentivizes the agent to take
noisy-leftbychoosingacontractthatpays1foroutcomeLand0otherwise. Thisway,bothactions
yield the same value for the agent, 0.9 · 1 + 0.1 · 0 − 0.8 = 0.1 · 1 + 0.9 · 0 = 0.1, and the
agentchoosesa bytie-breakinginfavouroftheprincipal. Sotheprincipal’svalueinstates is
L L
0.9(rp(s ,L)−1)=0.9(14 −1)=0.5. Bythesamelogic,theprincipaloffersthesamecontract
L 9
ins anditsvalueequals0.5(andtheagent’svalueequals0.1). Then,ins ,theagentisindifferent
R 0
between transitioning to s and s , so the principal has to offer the same contract again. The
L R
principal’svalueins (giventhattheagentchoosesa )is0.5+0.9·0.5+0.1·0.5=1,andthe
0 L
agent’svalueis0.1+0.9·0.1+0.1·0.1=0.2. Theseareestimatedbyconsideringutilitiesins
L
ands withoutdiscounting(usingγ =1). Notethatinthisanalysis,wefoundSPEusingbackward
R
induction: wefirstanalyzedtheterminalstates,thentherootstate.
18ComparethiswithStackelberg: Ifnon-crediblethreatswereallowed,theprincipalcouldthreatento
actsuboptimallyintherightsubgamebyalwayspayingtheagent0. Bothprincipalandagentthen
values at0. Ins ,thecontractisthesameasinSPE.Knowingthis,theagentvaluess overs
R L L R
(0.1>0),whichwoulddriveittochoosethenoisy-leftactionattherootstateeveniftheprincipal
paysonly1−0.1=0.9foroutcomeL. Bypayingless,theprincipal’sutility(valueins )wouldbe
0
highercomparedtoSPE,(14 −0.9+0.5)·0.9=1.04>1.
9
This illustrates how Stackelberg equilibrium may produce more utility for the principal, but the
surpluscomesatthecostoftheinabilitytoengageinmutuallybeneficialcontractualagreements
in certain subgames, even if these subgames are reached by pure chance and despite the agent’s
best efforts. On the one hand, Stackelberg requires more commitment power from the principal.
WhereasinSPEtheagentcanbecertainthattheprincipalstickstoitspolicyinfuturestatesbecause
itisoptimalinanysubgame,inStackelberg,theprincipalhastopreemptivelycommittoinefficient
contractsandignorepotentialbeneficialdeviations. Ontheotherhand,Stackelbergisnotrobustto
mistakes: iftheagentmistakenlychoosesthewrongaction,itmightbepunishedbytheprincipal,
losingutilityforbothplayers. Thisisespeciallyconcerninginthecontextoflearning,wheremistakes
canhappenduetoapproximationerrors,orevenduetotheagentexploring(inonlinesetups). SPEis
henceamorepracticalsolutionconcept.
B.2 Principal’sandAgent’sMDPs(Observation3.1)
A (standard) MDP is a tuple (S,S ,A,R,T,γ), where S is a set of states, S ∈ S is a set of
0 0
possibleinitialstates,Aisasetofactions,T : S×A → ∆(S)isastochastictransitionfunction,
R:S×A×S →P(R)isastochasticrewardfunction(R(s,a,s′)isadistributionandmaydepend
onthenextstates′),γ isan(optional)discountingfactor. Assumefinitehorizon(asinSection2).
Considerahidden-actionprincipal-agentMDPM=(S,s ,A,B,O,O,R,Rp,T,γ)asdefinedin
0
Section2. First,considertheagent’sperspective. Lettheprincipal’spolicybesomeρ. Thisdefines
astandardMDP(Sa,Sa,A,Ra,Ta,γ)thatwecalltheagent’sMDP,where: Thesetofstatesis
0
Sa =S×B(infiniteunlessBisdiscretized). ThesetofinitialstatesisS ={s }×B. Thesetof
0 0
actionsisA.
ThetransitionfunctionTa :Sa×A→∆(Sa)definesdistributionsovernextstate-contractpairs
(s′,ρ(s′)),wheres′ ∼T(s,o∼O(s,a)). Notethatthenextstates′issampledfromadistribution
thatisafunctionofstate-actionpairs(s,a)marginalizedoveroutcomes,andthenextcontractρ(s′)
isgivenbytheprincipal’spolicy. Informally,theagentexpectstheprincipaltosticktoitspolicy
inanyfuturestate. Atthesametime,sincethestatespaceSa isdefinedoverthesetofcontracts
B,theagentmayadaptitspolicytoanyimmediatecontractinthecurrentstate,andthusitspolicy
π :Sa →∆(A)isdefinedbyπ(s,b)foranyb∈B.
Therewardfunctionisabitcumbersometoformalizebecauseitshouldnotdependonoutcomes,
whilebothO andT canbestochastic. Specifically,thenewrewardfunctionhastobestochastic
w.r.t. outcomes: Ra((s,b),a,s′) = [r(s,a)+b(o) | o ∼ P(o | s,a,s′)], where the conditional
distributionofoutcomesisgivenbyP(o|s,a,s′)= P(O(s,a)=o)P(T(s,o)=s′) .
(cid:80) o∗P(O(s,a)=o∗)P(T(s,o∗)=s′)
Next,considertheprincipal’sperspective. Lettheagent’spolicybesomeπ. Thisdefinesastandard
MDP(S,{s },B,Rp,Tp,γ)thatwecalltheprincipal’sMDP,where: thesetofstatesisS;theset
0
ofinitialstatesis{s };thesetofactionsisB(infiniteunlessdiscretized);thetransitionfunctionis
0
definedbyTp(s,b)=T(s,o∼O(s,a∼π(s,b)));therewardfunctionisdefinedsimilarlytothe
agent’sMDPbyRp(s,b,s′)=[r(s,o)−b(o)|o∼P(o|s,a,s′)].
Thus,theoptimizationtaskoftheprincipal(agent)givenafixedpolicyoftheagent(principal)canbe
castasastandardsingle-agentMDP.Inbothplayers’MDPs,theotherplayer’spresenceisimplicit,
embeddedintotransitionandrewardfunctions. NotethatourdefinitionofstandardMDPallowsfor
stochasticrewardfunctionsthatdependonthenextstate,aswellasasetofinitialstatesthatisnot
asingleton. ThesamegeneralizationscanbemadeinourPrincipal-AgentMDPdefinition,andin
particular,theabovederivationswouldstillhold,butwedecidedtoslightlyspecifyourmodelfor
brevity.
19B.3 ContractionOperatorsandtheirFixedPoints
Our meta-algorithm iteratively solves a sequence of principal’s and agent’s MDPs as defined in
AppendixB.2. BothtaskscanbeperformedwithQ-learningandinterpretedasfindingfixedpoints
oftheBellmanoptimalityoperatorintherespectiveMDPs. Furthermore,ourlearningapproachin
Section4makesuseofmodifiedQ-functions,whicharealsofixedpointsofcontractionoperators.
Forconvenience,wedefineallfouroperatorsbelow. Wherenecessary,weprovetheoperatorsbeing
contractionsanddefinetheirfixedpoints.
B.3.1 Optimalityoperatorsintheagent’sMDP
Hereweassumesomefixedprincipal’spolicyρthatdefinesanagent’sMDP.
Bellmanoptimalityoperator. Considertheagent’soptimizationtask(line3ofthemeta-algorithm).
SolvingitimpliesfindingafixedpointofanoperatorS ,whichistheBellmanoptimalityoperatorin
ρ
theagent’sMDPdefinedbyaprincipal’spolicyρ.
DefinitionB.1. Givenanagent’sMDPdefinedbyaprincipal’spolicyρ,theBellmanoptimality
operatorS isdefinedby
ρ
(S Q)((s,b),a)=E
(cid:104)(cid:0) R(s,a,b,o)+γmaxQ((s′,ρ(s′)),a′)(cid:1)(cid:105)
, (6)
ρ o∼O(s,a),s′∼T(s,o)
a′
whereR(s,a,b,o)=r(s,a)+b(o),QisanelementofavectorspaceQ ={S×B×A→R},
SBA
andsubscriptρdenotesconditioningontheprincipal’spolicyρ.
ThisoperatorisacontractionandadmitsauniquefixedpointQ∗(ρ)thatsatisfies:
V∗(s,b|ρ)=maxQ∗((s,b),a|ρ), (7)
a
(cid:104) (cid:105)
Q∗((s,b),a|ρ)=E R(s,a,b,o)+γmaxQ∗((s′,ρ(s′)),a′ |ρ) . (8)
a′
Thepolicycorrespondingtothefixedpointiscalledthebest-respondingpolicyπ∗(ρ):
∀s∈S,b∈B :π∗(s,b|ρ)=argmaxQ∗((s,b),a|ρ),
a
wheretiesarebrokeninfavouroftheprincipal.
∗
Truncated Bellman optimality operator. Here we show that the truncated Q-function Q (ρ)
definedinthemaintext(1)isafixedpointofacontractionoperatorandthuscanbefoundwith
Q-learning.
Definition B.2. Given an agent’s MDP defined by principal’s policy ρ, the truncated Bellman
optimalityoperatorS isdefinedby
ρ
(S Q)(s,a)=r(s,a)+γE max(cid:2)E ρ(s′)(o′)+Q(s′,a′)(cid:3) , (9)
ρ o∼O(s,a),s′∼T(s,o) o′∼O(s′,a′)
a′
whereQisanelementofavectorspaceQ ={S×A→R}.
SA
LemmaB.3. OperatorS isacontractioninthesup-norm.7
ρ
Proof. LetQ ,Q ∈Q ,γ ∈[0,1). TheoperatorS isacontractioninthesup-normifitsatisfies
1 2 SA ρ
∥S Q −S Q ∥ ≤γ∥Q −Q ∥ . Thisinequalityholdsbecause:
ρ 1 ρ 2 ∞ 1 2 ∞
7InlemmasB.3andB.6,weshowforacasethatincludesfinite-andinfinite-horizonMDPs,butrequires
γ <1.Forγ =1andfinite-horizonMDPs,per-timestepoperatorscanbeshowntobecontractions,similarto
theBellmanoperatorinchapter4.3ofPuterman[60].
20(cid:12)
∥S Q −S Q ∥ =max(cid:12)γE (cid:2) max(E ρ(s′)(o′)+Q (s′,a′))−
ρ 1 ρ 2 ∞ (cid:12) o,s′ o′ 1
s,a a′
(cid:12)
max(E ρ(s′)(o′)+Q (s′,a′))(cid:3) +r(s,a)−r(s,a)(cid:12)≤
o′ 2 (cid:12)
a′
(cid:12) (cid:12)
max(cid:12)γE maxE (cid:2) ρ(s′)(o′)+Q (s′,a′)−ρ(s′)(o′)−Q (s′,a′)(cid:3)(cid:12)=
(cid:12) o,s′ o′ 1 2 (cid:12)
s,a a′
(cid:12) (cid:12)
max(cid:12)γE max(cid:2) Q (s′,a′)−Q (s′,a′)(cid:3)(cid:12)≤
(cid:12) o,s′ 1 2 (cid:12)
s,a a′
maxγmax(cid:12) (cid:12)Q 1(s′,a′)−Q 2(s′,a′)(cid:12) (cid:12)=γ∥Q 1−Q 2∥ ∞.
s,a s′,a′
BecauseS isacontractionasshowninLemmaB.3,bytheBanachtheorem,itadmitsauniquefixed
ρ
∗ ∗ ∗
pointQ s.t. ∀s,a : Q (s,a) = (S Q )(s,a). Wenowshowthatthisfixedpointisthetruncated
ρ ρ ρ ρ
Q-function. DefineQ ((s,b),a)=E b(o)+Q∗ (s,a). Noticethatthefixedpointsatisfies:
ρ o∼O(s,a) ρ
∗ ∗ (Eq.9)
∀s∈S,a∈A: Q (s,a)=(S Q )(s,a) =
ρ ρ ρ
r(s,a)+γE max(cid:2)E ρ(s′)(o′)+Q∗ (s′,a′)(cid:3) =
o,s′ o′ ρ
a′
r(s,a)+γE maxQ ((s′,ρ(s′)),a′).
o,s′ ρ
a′
Atthesametime,bydefinition:
∀s∈S,a∈A: Q∗ (s,a)=Q ((s,b),a)−E b(o).
ρ ρ o∼O(s,a)
Combiningtheabovetwoequationsandswappingterms:
∀s∈S,a∈A: Q ((s,b),a)=r(s,a)+E [b(o)+γmaxQ ((s′,ρ(s′)),a′)].
ρ o,s′ ρ
a′
Notice that the last equation shows that Q is the fixed point of the Bellman optimality operator
ρ
S (6),i.e.,Q = Q∗(ρ),asitsatisfiestheoptimalityequations(8). ItfollowsthatQ∗((s,b),a |
ρ ρ
ρ) = E b(o)+Q∗ (s,a), andthusQ∗ satisfiesthedefinitionofthetruncatedQ-function(1), i.e.,
o ρ ρ
∗ ∗
Q = Q (ρ). ThetruncatedQ-functionisthenafixedpointofacontractionoperatorandcanbe
ρ
foundwithQ-learning. Itcanalsobeusedtocomputethebest-respondingpolicy: π∗(s,b | ρ) =
argmax [E b(o)+Q∗ (s,a|ρ)].
a o
B.3.2 Optimalityoperatorsintheprincipal’sMDP
Hereweassumesomefixedbest-respondingagent’spolicyπ∗(ρ)thatdefinesaprincipal’sMDP.
Whilewecouldinsteadassumeanarbitrarypolicyπ,weareonlyinterestedinsolvingtheprincipal’s
MDPastheouterlevelofthemeta-algorithm,whichalwaysfollowstheinnerlevelthatoutputsan
agent’spolicyπ∗(ρ)best-respondingtosomeρ.
Bellman optimality operator. Consider the principal’s optimization level (line 4 of the meta-
algorithm). SolvingitimpliesfindingafixedpointofanoperatorB ,whichistheBellmanoptimality
ρ
operatorintheprincipal’sMDPdefinedbytheagent’spolicyπ∗(ρ)isbest-respondingtosomeρ.
DefinitionB.4. Givenaprincipal’sMDPdefinedbytheagent’spolicyπ∗(ρ)best-respondingto
someρ,theBellmanoptimalityoperatorB isdefinedby
ρ
(B Q)(s,b)=E (cid:104)(cid:0) Rp(s,b,o)+γmaxQ(s′,b′)(cid:1) |a=π∗(s,b|ρ)(cid:105) , (10)
ρ o∼O(s,a),s′∼T(s,o)
b′
whereRp(s,b,o)=rp(s,o)−b(o),QisanelementofavectorspaceQ ={S×B →R},and
SB
subscriptρdenotesconditioningontheagent’sbest-respondingpolicyπ∗(ρ).
21ThisoperatorisacontractionandadmitsauniquefixedpointQ∗(π∗(ρ))thatsatisfiesoptimality
equations:
V∗(s|π∗(ρ))=maxQ∗(s,b|π∗(ρ)), (11)
b
Q∗(s,b|π∗(ρ))=E(cid:104)(cid:0) Rp(s,b,o)+γmaxQ∗(s′,b′ |π∗(ρ))(cid:1) |a=π∗(s,b|ρ)(cid:105) . (12)
b′
Thepolicycorrespondingtothefixedpointiscalledthesubgame-perfectpolicyρ∗(π∗(ρ)):
∀s∈S :ρ∗(s|π∗(ρ))=argmaxQ∗(s,b|π∗(ρ)).
b
Contractual Bellman optimality operator. Here we show that the contractual Q-function
q∗(π∗(ρ))definedinthemaintext(2)isafixedpointofacontractionoperatorandthuscanbefound
withQ-learning.
DefinitionB.5. Givenaprincipal’sMDPdefinedbytheagent’spolicyπ∗(ρ)best-respondingto
someρ,thecontractualBellmanoptimalityoperatorH isdefinedby
ρ
(cid:104) (cid:105)
(H q)(s,ap)= max E Rp(s,b,o)+γmaxq(s′,a′) , (13)
ρ o∼O(s,ap),s′∼T(s,o)
{b|π∗(s,b|ρ)=ap} a′
whereqisanelementofavectorspaceQ ={S×A→R},andap ∈Adenotestheprincipal’s
SA
recommendedaction.
LemmaB.6. OperatorH isacontractioninthesup-norm.
ρ
Proof. Letq ,q ∈Q ,γ ∈[0,1). TheoperatorH isacontractioninthesup-normifitsatisfies
1 2 SA ρ
∥H q −H q ∥ ≤γ∥q −q ∥ . Thisinequalityholdsbecause:
ρ 1 ρ 2 ∞ 1 2 ∞
(cid:12)
∥H q −H q ∥ =max(cid:12) max E(cid:2) Rp(s,b,o)+γmaxq (s′,a′)(cid:3) −
ρ 1 ρ 2 ∞ (cid:12) 1
s,a {b∈B|π∗(s,b|ρ)=a} a′
(cid:12)
max E(cid:2) Rp(s,b,o)+γmaxq (s′,a′)(cid:3)(cid:12)=
2 (cid:12)
{b∈B|π∗(s,b|ρ)=a} a′
(cid:12) (cid:12)
maxγ(cid:12)E[maxq (s′,a′)−maxq (s′,a′)](cid:12)≤
(cid:12) 1 2 (cid:12)
s,a a′ a′
(cid:12) (cid:12)
maxγE(cid:12)maxq (s′,a′)−maxq (s′,a′)(cid:12)≤
(cid:12) 1 2 (cid:12)
s,a a′ a′
maxγEmax|q (s′,a′)−q (s′,a′)|≤
1 2
s,a a′
maxγmax|q (s′,a′)−q (s′,a′)|=γ∥q −q ∥ .
1 2 1 2 ∞
s,a s′,a′
BecauseH isacontractionasshowninLemmaB.6,bytheBanachtheorem,itadmitsauniquefixed
ρ
pointq∗s.t. ∀s,ap :q∗(s,ap)=(H q∗)(s,ap). Wenowshowthatthisfixedpointisthecontractual
ρ ρ ρ ρ
Q-function. Noticethatthefixedpointsatisfies:
∀s∈S : maxq∗(s,ap)=max(H q∗)(s,ap)(Eq =.13)
ρ ρ ρ
ap ap
max(H q∗)(s,π∗(s,b|ρ))=max(H (H q∗))(s,π∗(s,b|ρ))=···=
ρ ρ ρ ρ ρ (14)
b b
(cid:88)(cid:104) (cid:105)
maxE γtRp(s ,b ,o )|s =s,b =b,π∗(ρ) =maxQ∗(s,b|π∗(ρ)),
t t t 0 0
b b
t
22and:
∀s∈S,ap ∈A: q∗(s,ap)=(H q∗)(s,ap)(Eq =.13)
ρ ρ ρ
max
E(cid:104) Rp(s,b,o)+γmaxq∗(s′,a′)(cid:105)(Eq =.14)
ρ
{b|π∗(s,b|ρ)=ap} a′
(15)
max E(cid:104) Rp(s,b,o)+γmaxQ∗(s′,b′ |π∗(ρ))(cid:105)(Eq =.12)
{b|π∗(s,b|ρ)=ap} b′
max Q∗(s,b|π∗(ρ)).
{b|π∗(s,b|ρ)=ap}
Thus,q∗satisfiesthedefinitionofthecontractualQ-function(2),i.e.,q∗ =q∗(ρ). Thecontractual
ρ ρ
Q-functionisthenafixedpointofacontractionoperatorandcanbefoundwithQ-learning. The
contractualQ-functioncanalsobeusedtocomputethesubgame-perfectprincipal’spolicyasρ∗(s)=
argmax (H q∗)(s,π∗(s,b|ρ))=argmax Q∗(s,b|π∗(ρ)). Weaddresscomputingtheargmax
b ρ ρ b
inAppendixB.8usingLinearProgramming.
B.4 Meta-AlgorithmfindsSPE(ProofofTheorem3.3)
ObservationB.7. Aprincipal-agentstochasticgameG isinSPEifandonlyifeverysubgameofG
isinSPE.
ObservationB.7willbeusefulfortheproofsinthissection.
LetS ⊆S denotethesetofallpossiblestatesattimestept. Forexample,S ={s }. StatesinS
t 0 0 T
areterminalbydefinition. WeassumethatsetsS aredisjoint. Thisiswithoutlossofgenerality,as
t
wecanalwaysredefinethestatespaceofafinite-horizonMDPsuchthattheassumptionholds;e.g.,
byconcatenatingthetimesteptoastateassnew =(s ,t). Inotherwords,anyfinite-horizonMDP
t t
canberepresentedasadirectedacyclicgraph.
ThenextlemmaisaprecursortoprovingtheconvergenceofAlgorithm1andconcernsitssingle
iteration. GivenapairofpoliciesthatformanSPEinallsubgamesbuttheoriginalgame,itstates
thatperformingoneadditionaliterationofthealgorithm(updatetheagent,thentheprincipal)yields
anSPEinthegame. Thisisbecausetheagentobservesthecontractofferedinastateandadapts
its action, in accordance with our definition of the agent’s MDP in Appendix B.2. In particular,
theagent’sbest-respondingpolicyisdefinedasπ∗(s,b | ρ)foranypair(s,b),soins ,theagent
0
best-respondswithπ∗(s ,b|ρ)foranybregardlessoftheprincipal’spolicyρ(s ).
0 0
Lemma B.8. Given a finite-horizon principal-agent stochastic game G and a principal’s policy
ρ, if (ρ,π∗(ρ)) is an SPE in all subgames with a possible exception of G (i.e., all subgames in
s∈S\{s }),then(ρ∗(π∗(ρ)),π∗(ρ))isanSPEinG.
0
Proof. Ins ,theagentobservestheprincipal’saction. Consequentlyandbecausetheagentbest-
0
respondstoρ, italsobest-respondstoany{ρ′ | ∀s ∈ S \{s } : ρ′(s) = ρ(s)}regardlessofthe
0
offeredcontractρ′(s ),includingthesubgame-perfectρ∗(π∗(ρ))(thatonlydiffersfromρins ,as
0 0
subgamesinotherstatesareinSPEalready). Thus,(ρ∗(π∗(ρ)),π∗(ρ))isanSPEinG,aswellasin
allsubgamesofG (byObservationB.7).
CorollaryB.9. GivenG withasinglesubgame(S ={s }),(ρ∗(π∗(ρ)),π∗(ρ))isanSPEinG for
0
anyρ.
ThiscorollaryconcernsMDPswithasinglestate,whichcouldalsobeinterpretedasstateless. This
coversstaticcontractdesignproblems,aswellassubgamesinterminalstatesinourmodel.
By using Lemma B.8, we can now show that each iteration of the algorithm expands the set of
subgames that are in SPE, as long as some subgames satisfy the conditions of the lemma for an
arbitrarilyinitializedρ. Thisisalwaysthecaseforfinite-horizonMDPs,asthesubgamesinterminal
stateshaveasinglesubgame(itself),andthusCorollaryB.9applies. Thisreasoningisusedtoprove
Theorem3.3.
ProofofTheorem3.3. Denotethepolicyinitializedatline1asρ .Denotethebest-respondingpolicy
0
toρ asπ ≡π∗(ρ )andthesubgame-perfectpolicyagainstπ asρ ≡ρ∗(π ). Likewise,π and
0 1 0 1 1 1 i
23ρ respectivelydenotethebest-respondingpolicytoρ andthesubgame-perfectpolicyagainstπ ,
i i−1 i
whereiisaniterationofthealgorithm.
By Corollary B.9, (ρ ,π ) forms an SPE in all subgames in terminal states, including s ∈ S .
1 1 T
ApplyingLemmaB.8,aswellasourassumptionthatsetsS aredisjoint,(ρ ,π )isanSPEinall
t 2 2
subgamesinS ∪S . Byinduction,(ρ ,π )isanSPEinallsubgamesins∈S ∪S ···∪
T T−1 i i T T−1
S . Thus,(ρ ,π )isanSPEinallsubgamesins∈S ∪···∪S =S,andthusinthe
T−i+1 T+1 T+1 T 0
gameG (byObservationB.7).
B.5 Meta-AlgorithmappliesContraction(ProofofTheorem3.4)
Consider the principal’s optimization task (line 4 of the meta-algorithm). As we discuss in Ap-
pendixB.3,solvingthistaskcanbeinterpretedasfindingthefixedpointofacontractionoperatorB
ρ
definedin(10). Bydefinitionofcontraction,thisfixedpointcanbefoundbyiterativelyapplyingthe
operatoruntilconvergence,whichwedenoteasH∗ =B (B (B ...Q(s,b))).
ρ ρ ρ
ObservationB.10. H∗isacompositionoflinearoperatorsandthusisalinearoperator.
For H∗ to be a contraction, its fixed point has to be unique. Since its iterative application (the
meta-algorithm,Algorithm1)convergestoanSPE,wenextprovetheuniquenessofQ-functionsin
SPE.
LemmaB.11. Givenafinite-horizonprincipal-agentstochasticgameG,theprincipal’sQ-function
isequalinallSPEforanystate-action;thesameholdsfortheagent’sQ-function.
Proof. Consideraprincipal’spolicyρthatformsSPEwithanybest-respondingagent’spolicyπ∗(ρ).
Anyπ∗(ρ)solvestheagent’sMDPandthusallsuchpoliciesdefineauniqueagent’sQ-function
(whichisthefixedpointoftheBellmanoptimalityoperator). Furthermore,bytheassumptionthat
theagentbreakstiesinfavoroftheprincipal,allbest-respondingpoliciesalsouniquelydefinethe
principal’sQ-function(inotherwords,anypolicythatsolvestheagent’sMDPbutdoesnotmaximize
theprincipal’sQ-functionwhenbreakingtiesinsomestateisnotabest-respondingpolicybythe
assumedtie-breaking). Thus,foranypairofSPEwithnon-equalQ-functions,theprincipal’spolicies
mustalsodiffer.
Considertwoprincipal’spolicies, ρ andρ , thatformSPEwithanyrespectivebest-responding
x y
agents’policies,π∗(ρ )andπ∗(ρ ). Bytheaboveargument,thechoiceofπ∗(ρ )andπ∗(ρ )is
x y x y
inconsequential,sowecanassumethosetobeunique(e.g. byaddinglexicographictie-breakingif
theprincipal-favoredtie-breakingdoesnotbreakallties).
Forρ andρ todiffer, theremustbeastates ∈ S suchthat1)allsubgamesinstates“after”t,
x y t
i.e., {S } , areinuniqueSPEgivenbysomeρandπ∗(ρ)(e.g., thisholdsinaterminalstate)
t′ t′>t
and 2) the subgame in s has two contracts, b = ρ (s) and b = ρ (s), that both maximize the
x x y y
principal’sutility, i.e., Q∗(s,b | π∗(ρ )) = Q∗(s,b | π∗(ρ )). Denotetheagent’sactionsins
x x y y
asa = π∗(s,b | ρ)anda = π∗(s,b | ρ). Wenowshowthatthechoicebetweenb andb is
x x y y x y
inconsequentialasbothcontractsalsoyieldthesameutilityfortheagent.
Assume the agent prefers b to b , i.e., Q∗((s,b ),a | ρ) > Q∗((s,b ),a | ρ). First, use
x y x x y y
the observed-action notation. Applying the Bellman optimality operator and using the defi-
nition of R, we have E[r(s,a ) + γmax Q∗((s′,ρ(s′)),a′ | ρ)] + b (a ) > E[r(s,a ) +
x a′ x x y
γmax Q∗((s′,ρ(s′)),a′ | ρ)] + b (a ). Observe that the principal may simply decrease the
a′ y y
paymentb (a )bythedifferenceoftheagent’sQ-values(sothattheinequalitybecomesequality),
x x
increasingtheprincipal’sutility. So,theassumptionthattheagentprefersb tob meansthatneither
x y
contractmaximizestheprincipal’sutilityinthesubgame,leadingtoacontradiction.
The same can be shown in the hidden-action model. The agent preferring b to b
x y
would mean E[r(s,a ) + b (o) + γmax Q∗((s′,ρ(s′)),a′ | ρ)] > E[r(s,a ) + b (o) +
x x a′ y y
γmax Q∗((s′,ρ(s′)),a′ |ρ)],andtheprincipalwouldbeabletoadjustb inordertodecreasethe
a′ x
expectedpaymentE[b (o)]relativelytoE[b (o)],e.g.,bydecreasingeachnon-zeropaymentb (o)
x y x
byaconstant. Again,thisleadstoacontradiction.
Wethushaveshownthatthechoicebetweenb andb insisinconsequentialforthevaluefunctions
x y
ofbothprincipalandagentins: V∗(s | π∗(ρ )) = Q∗(s,b | π∗(ρ )) = Q∗(s,b | π∗(ρ )) =
x x x y y
V∗(s | π∗(ρ ))andV∗(s,b | ρ) = Q∗((s,b ),a | ρ) = Q∗((s,b ),a | ρ) = V∗(s,b | ρ). By
y x x x y y y
Bellmanoptimalityequations((7)and(8)fortheagent,(11)and(12)fortheprincipal),itisalso
24inconsequentialfortheplayers’Q-functionsinallstates“before”t,i.e.,{S } . Forstates“after”
t′ t′<t
t,thechoicealsohasnoeffectbytheMDPbeingfinite-horizon(andourw.l.o.g. assumptionabout
theuniquenessofstates). Thisholdsforanysuchb andb inanys. Thus,foranySPE(ρ,π∗(ρ)),
x y
eachplayer’sQ-functionisidenticalinallSPEforanystate-action.
ProofofTheorem3.4. By Observation B.10, H∗ is a linear operator. Moreover, as Algorithm 1
convergestoSPEbyTheorem3.3andthepayoffsinSPEareuniquebyLemmaB.11,theiterative
applicationofH∗ convergestoauniquefixedpoint. ByusingaconverseoftheBanachtheorem
(Theorem1inDaskalakisetal.[13]), thisoperatorisacontractionunderanynormthatformsa
completeandpropermetricspace. Thisincludesthesup-norm.
B.6 Meta-AlgorithmmaydivergeinInfinite-HorizonMDPs
Here we present an example of a hidden-action infinite-horizon principal-agent MDP where the
meta-algorithmdivergesbygettingstuckinacycle. Tosolvetheprincipal’sandagent’soptimization
tasks,wespecificallydevelopedexactsolversforprincipal’sandagent’sMDPs.
TheMDPconsistsoftwostates,s ands . Ineachstate,theagenthastwoactions,a anda ,which
1 2 1 2
determineprobabilitiesofsamplingoneoftwooutcomes,o ando . Whenagentchoosesa inany
1 2 1
states,outcomesaresampledwithrespectiveprobabilitiesO(o |s,a )=0.9andO(o |s,a )=
1 1 2 1
0.1. Viceversa,choosinga inanystatessamplesanoutcomewithprobabilitiesO(o |s,a )=0.1
2 1 2
andO(o |s,a )=0.9. Aftersamplinganoutcomeo ,theMDPdeterministicallytransitionstos
2 2 i i
(e.g.,ifo issampled,theMDPtransitionstos regardlessoftheoldstateandtheagent’saction).
1 1
ChoosinganactionthatismorelikelytochangethestateoftheMDP(so,a ins anda ins )
2 1 1 2
requireseffortfromtheagent,respectivelycostingc(s ,a )=1andc(s ,a )=2. Theotheraction
1 2 2 1
isfreefortheagent: c(s ,a )=0andc(s ,a )=0. Otherthingsequal,theprincipalprefersthe
1 1 2 2
agenttoinvestaneffort: itonlyenjoysarewardwhenevertheMDPtransitionstoadifferentstate,
equaltorp(s ,o )=rp(s ,o )=1.5. Thediscountfactorissettoγ =0.9.
1 2 2 1
Wenowdescribeseveraliterationsofthemeta-algorithm,showingthatitoscillatesbetweentwopairs
ofplayers’policies. Wereporttheagent’struncatedQ-function(1)andtheprincipal’scontractual
Q-function(2)ateachiterationofthealgorithm(roundedtothreedecimals). Wealsoverifythat
theseQ-functionsareindeedfixedpointsofrespectiveoperatorsandthussolvetherespectiveMDPs,
butonlydosoin(s ,a )asderivationsinotherstate-actionpairsareidentical.
1 2
Initialization
Initializetheprincipalwithapolicyρ thatdoesnotofferanypayments,i.e.,ρ (s ) = ρ (s ) =
0 0 1 0 2
(0,0),wherewedenoteacontractbyatupleb=(b(o ),b(o )).
1 2
Iteration1: agent
The agent’s best-responding policy π simply minimizes costs by never investing an effort:
1
π (s ,ρ (s ))=a ,π (s ,ρ (s ))=a . ThiscorrespondstothefollowingtruncatedQ-function:
1 1 0 1 1 1 2 0 2 2
Qπ1(s
,a
)=Qπ1(s
,a
)=0,Qπ1(s
,a )=−c(s ,a
)=−1andQπ1(s
,a )=−c(s ,a )=
1 1 2 2 1 2 1 2 2 1 2 1
−2.
ToverifythatthisQ-functionisafixedpointofthetruncatedBellmanoptimalityoperator(9),observe
thattheoptimalityequationsholdinallstate-actionpairs. Forexample,in(s ,a )wehave:
1 2
−1=Qπ1(s ,a )=−c(s ,a )+γE [ρ(s′)(o′)+maxQπ1(s′,a′)]=
1 2 1 2 o,s′,o′
a′
−1+0.9[0.1·0+0.9·0]=−1.
In theabsence of contracts, the agent’s truncatedQ-function is equalto its Q-functionunder the
principal’spolicy: Qπ1((s,ρ 0(s)),a)=Qπ1(s,a).
Iteration1: principal
Theprincipal’ssubgame-perfectpolicyρ attemptstoincentivizeeffortins andoffersthefollowing
1 1
contracts: ρ (s ) = (0,1.25), ρ (s ) = (0,0). This corresponds to the following contractual Q-
1 1 1 2
function: qρ1(s 1,a 1)=1.991,qρ1(s 1,a 2)=2.048,qρ1(s 2,a 1)=1.391,andqρ1(s 2,a 2)=2.023.
25ToverifythatthisQ-functionisafixedpointofthecontractualBellmanoptimalityoperator(13),
observethattheoptimalityequationsholdinallstate-actionpairs. Forexample,in(s ,a )wehave:
1 2
2.048=qρ1(s 1,a 2)=E o,s′[−ρ 1(s 1)(o)+rp(s 1,o)+γmaxqρ1(s′,a′)]=
a′
0.1(0+0+0.9·2.048)+0.9(−1.25+1.5+0.9·2.023)=2.048.
Incentivizing a in s requires offering a contract ρ(s ) = (2.5,0), which is not worth it for the
1 2 2
principal,asevidencedbyqρ1(s 2,a 1)<qρ1(s 2,a 2).
Iteration2: agent
The principal ρ underestimated the payment required to incentivize effort, and the agent’s best-
1
respondingpolicyπ stillneverinvestsaneffort: π (s ,ρ (s )) = a ,π (s ,ρ (s )) = a . This
2 2 1 1 1 1 2 2 1 2 2
correspondstothefollowingtruncatedQ-function:
Qπ2(s
,a ) =
0.723,Qπ2(s
,a ) = −0.598,
1 1 1 2
Qπ2(s
,a
)=−1.277,andQπ2(s
,a )=0.402.
2 1 2 2
ToverifythatthisQ-functionisafixedpointofthetruncatedBellmanoptimalityoperator(9),observe
thattheoptimalityequationsholdinallstate-actionpairs. Forexample,in(s ,a )wehave:
1 2
−0.598=Qπ2(s ,a )=−c(s ,a )+γE [ρ(s′)(o′)+maxQπ1(s′,a′)]=
1 2 1 2 o,s′,o′
a′
−1+0.9[0.1(0.9(0+0.723)+0.1(1.25+0.402))+
0.9(0.1(0+0.723)+0.9(0+0.402))]=−0.598.
Giventhecontractsfromtheprincipal’spolicyρ ,wecancomputetheagent’sQ-valuesusing(1):
1
Qπ2((s 1,ρ 1(s 1)),a 1) = 0.848, Qπ2((s 1,ρ 1(s 1)),a 2) = 0.527, Qπ2((s 2,ρ 1(s 2)),a 1) = −1.277,
andQπ2((s 2,ρ 1(s 2)),a 2)=0.402. Observethatindeed,theagentstillprefersnottoinvestaneffort
ins 1,asevidencedbyQπ2((s 1,ρ 1(s 1)),a 1)>Qπ2((s 1,ρ 1(s 1)),a 2).
Iteration2: principal
Theprincipalgivesuponincentivizingeffortandonceagainoffersnocontracts: ρ (s ) = (0,0),
2 1
ρ 2(s 2) = (0,0). This correspondsto thefollowingcontractual Q-function: qρ2(s 1,a 1) = 1.661,
qρ2(s 1,a 2)=1.503,qρ2(s 2,a 1)=1.422,andqρ2(s 2,a 2)=1.839.
ToverifythatthisQ-functionisafixedpointofthecontractualBellmanoptimalityoperator(13),
observethattheoptimalityequationsholdinallstate-actionpairs. Forexample,toincentivizea in
2
s ,theprincipalhastoofferacontractρ(s )=(0,1.652),whichgivesus:
1 1
1.503=qρ2(s 1,a 2)=E o,s′[−ρ 2(s 2)(o)+rp(s 1,o)+γmaxqρ2(s′,a′)]=
a′
0.1(0+0+0.9·1.661)+0.9(−1.652+1.5+0.9·1.839)=1.503,
Incentivizinga ins requiresofferingacontractρ(s )=(2.098,0),whichisstillnotworthitfor
1 2 2
theprincipal,asevidencedbyqρ2(s 2,a 1)<qρ2(s 2,a 2).
Subsequentiterations
Becausetheprincipal’ssubgame-perfectpolicyρ repeatsthepolicyρ fromapreviousiteration,the
2 0
meta-algorithmisnowstuckinacyclewhereiterations1and2repeatinfinitely. Inotherwords,the
meta-algorithmdiverges.
B.7 Meta-AlgorithmintheObserved-ActionModel
Inthespecialcasewheretheagent’sactionsareobserved(definedinSection2.2andsummarized
inTable2),themeta-algorithmcanbeshowntofindSPEinasingle(ratherthanT +1)iteration,
asweshowinTheoremB.12. Thispropertyisbasedontheobservationthattheagentisindifferent
between an MDP without a principal and an MDP augmented with a subgame-perfect principal;
similarobservationhasbeenmadeincontemporaneouswork[3]. Consequently,evaluatingminimal
implementationonlyrequiresaccesstotheagent’soptimalQ-functionintheabsenceoftheprincipal.
26Is is also easy to show that SPE coincides with Stackelberg equilibrium in the observed-action
scenario(LemmaB.14),andconsequentlythemeta-algorithmfindsStackelbergequilibrium.
TheoremB.12. Givenaprincipal-agentstochasticgame,G,withobservedactionsandeitherfinite
orinfinitehorizon,iftheprincipal’spolicyisinitializedtoofferzero-vectorsascontractsinallstates,
themeta-algorithmfindsSPEinoneiteration.
ProofofTheoremB.12. Use the same notations of ρ and π as in the proof of Theorem 3.3 in
i i
AppendixB.4.
Afterinitializingρ (thatalwaysoffers0)andfindingthebest-respondingagentπ ,considerthe
0 1
optimalpaymentsfoundattheouteroptimizationlevelofAlgorithm1. Givenastates∈S,denote
theagent’sactionas
a∗ =argmaxQ∗((s,0),a|ρ ).
0
a
Fornow,letcontractsinstatesotherthansremain0;wewillomittheconditioningofQ∗onρ for
0
brevity. Theoptimalcontractins,denotedasb∗,reimbursestheagentfortakingasuboptimalaction,
payingexactly
b∗(s,ap)=Q∗((s,0),a∗)−Q∗((s,0),ap)
iftheagentselectsap,andpays0otherwise. Notethatb∗ =0ifap =a∗. Thiscontractmakesthe
agentindifferentbetweenapanda∗because
Q∗((s,b∗),ap)=Q∗((s,0),ap)+b∗(s,ap)=Q∗((s,0),a∗),
changingtheagent’sactioninstoap(accordingtotie-breaking). However,theagent’svaluefunction
remainsunchanged:
V∗(s,b∗)=Q∗((s,b∗),ap)=Q∗((s,0),a∗)=V∗(s,0).
Thus,theBellmanoptimalityequations(7)and(8)stillholdinallstatesafterreplacing0withb∗in
s,andπ stillbest-respondstotheupdatedprincipal. Thisreplacementofzero-vectorsforoptimal
1
contractsb∗ isperformedinallstatesduringtheupdateoftheprincipalattheouteroptimization
level, yieldingaprincipal’spolicyρ subgame-perfectagainstπ . Atthesametime, π remains
1 1 1
best-respondingagainstρ . Thus,(ρ ,π )isanSPE.
1 1 1
RemarkB.13. UnlikeourgeneralTheorem3.3,theaboveproofreliesonthespecificsofourmodel
suchastheprincipal’sactionsetandtheplayers’rewardfunctions.
LemmaB.14. Givenaprincipal-agentstochasticgame, G, withobservedactions, anySPEisa
Stackelbergequilibrium.
Proof. ConsideranSPE.Asdiscussedintheaboveproof,thecontractsinSPEexactlyreimbursethe
agentforchoosingsuboptimalactions,andtheagent’svaluefunctionwhenofferedsuchacontractin
somestatesremainsthesameasintheabsenceofthecontract. Additionally,noticethattheprincipal
mayneverdecreasetheagent’svalueinanystatebelowthevalueitgetsintheabsenceofcontracts
(sincepaymentsarenon-negative). Thus, theprincipalmaynotdeviatefromSPEbydecreasing
theagent’svalueinanyofthefuturestatesthroughsuboptimalcontractsinordertoincentivizea
suboptimalactionapwhilepayinglessins.
Ontheotherhand,considertheprincipaltryingtopaylessinsomestatesbyincreasingtheagent’s
value in some future states. If transition function is determenistic, then in order to decrease the
payment in s by some v < b∗(ap) while still incentivizing ap, the principal must, for example,
increasethepaymentins′ =T(s,O(s,ap))byv/γ –whichisinconsequentialfortheprincipal’s
valueins(inourmodel, thediscountfactoristhesamefortheprincipalandtheagent). Incase
ofastochastictransitionfunction,theincreaseofpaymentsinfuturestatesrequiredtobalancethe
decreaseofthepaymentinsbyvmayevendecreasetheprincipal’svalueinscomparedtoSPE.
Thus,theprincipalmaynotdeviatefromanSPE(committonon-crediblethreats)toincreaseits
valueintheinitialstate,andthereforeanySPEisaStackelbergequilibrium.
27B.8 DerivingtheLinearProgram(Section4)
InSection4,wedescribeanRLapproachtosolvingtheprincipal’sMDPthatinvolvestwointerde-
pendenttasksof1)learningapolicythattheprincipalwantstoimplementand2)computingthe
contractsthatdosooptimally. TheformerissolvedbylearningthecontractualQ-function(2),which
wederiveasafixedpointofacontractionoperatorH (13)inAppendixB.3. Thelatter(whichis
ρ
requiredasasubroutinetoapplyH )wesolveusingLinearProgramming,akintohowthestatic
ρ
principal-agentproblemsaretypicallysolved(asmentionedinSection2.1).
Givenastates ∈ S andanactiontorecommendap ∈ A,rewritetheright-handsideofH asthe
ρ
followingconstrainedoptimizationproblem:
(cid:104) (cid:105)
maxE rp(s,o)−b(o)+γmaxq(s′,a′) s.t.
o∼O(s,ap),s′∼T(s,o)
b∈B a′ (16)
∀a∈A:Q∗((s,b),ap |ρ)≥Q∗((s,b),a|ρ),
wheretheconstraintsexplicitlyrequiretherecommendedactionap tobeatleastas‘good’forthe
agentasanyotheractiona. Notethatrp(s,o)andq(s′,a′)areconstantswithrespecttobandcan
beomittedfromtheobjective. Toseethattheconstraintsarelinear,applytheBellmanoptimality
operatortotheagent’sQ-function,andrewriteconstraintsthroughthetruncatedQ-functionusing(1):
maxE [−b(o)] s.t.
o∼O(s,ap)
b∈B (17)
∀a∈A:E [b(o)]+Q∗ (s,ap |ρ)≥E [b(o)]+Q∗ (s,a|ρ).
o∼O(s,ap) o∼O(s,a)
Becauseboththeobjectiveandtheconditionsarelinear,theproblem(17)isanLP.
AsdiscussedinSection4,ourapproachtosolvingtheagent’soptimizationproblemistolearnthe
∗
truncatedQ-functionQ (ρ)andtransformitintotheQ-functionbyaddingtheexpectedpayment.
Notethatthisrepresentationoftheagent’sQ-functionisusedintheconstraintsoftheaboveLP.The
requirementoftheprincipalhavingaccesstotheagent’s(truncated)Q-functioncanbeseenasa
limitationoftheoutlinedapproach. Apotentialremedyistoinsteadparameterizetheprincipal’s
Q-functionQ∗(π)withadiscontinuousneuralnetworkabletoefficientlyapproximatetheQ-function
andthesolutionstoLPswithoutrequiringaccesstotheagent’sprivateinformation[79]. Ofcourse,
onecouldalsodirectlylearnQ∗(π)withsimplerapproaches,e.g.,bydiscretizingthecontractspace
oremployingdeepRLmethodsforcontinuousactionspaces(suchasDeepDeterministicPolicy
GradientorSoftActor-Critic).
SolvingtheLPalsorequiresaccesstotheoutcomefunctionO;wediscussinAppendixD.1how
thiscanbecircumventedbyparameterizingtheoutcomefunctionwithanadditionalneuralnetwork,
trainedasaprobabilisticoutcomeclassifier.
In the special case of the observed-action model, the LP has a simple solution if the agent best-
respondstoaspecificprincipalthatalwaysoffersazero-vector0asacontract. Inthissolution,the
principalexactlyreimbursestheagentforchoosinga(suboptimal)recommendedactionap,andpays
0ifagentchoosesanyotheractiona̸=ap. Similarobservationhasbeenmadeincontemporaneous
work,seeendofSection3.1inWuetal.[82].
C Principal-Multi-AgentExample: Prisoner’sDilemma
Belowweillustratethemulti-agentmodelfromSection5.1ontheexampleofasimplematrixgame.
Considerastandardone-stepPrisoner’sDilemmawiththepayoffmatrixasinTable3a. Here,the
onlyequilibriumismutualdefection(DD),despitecooperation(CC)beingmutuallybeneficial. How
shouldabenevolentprincipalchangethematrixthroughpaymentstoincentivizecooperation?
OneansweristhatCCshouldbecomeanequilibriumthroughminimalpaymentsinCC.Inoneof
the paymentschemes that achieve this, the principalpays a unit ofutility to both playersfor the
28Table3: Prisoner’sDilemmaasaprincipal-multi-agentgame. ‘Def’denotes‘Defect’and‘Coop’
denotes‘Cooperate’. Bluehighlightsprincipal’schangesofagents’payoffs.
(a)nopayments (b)arbitrarypaymentsinSPE (c)ICpaymentsinSPE
Def Coop Def Coop Def Coop
Def 2,2 4,0 Def 2,2 4,0 Def 2,2 4,2
Coop 0,4 3,3 Coop 0,4 4,4 Coop 2,4 4,4
CC outcome, resulting in the payoff matrix as in Table 3b.8 However, note that DD remains an
equilibriumfortheagents. Infact,thenewpayoffmatrixisalsoasocialdilemmaknownastheStag
Hunt. Inthecontextof(decentralized)learning,thereisnoreasontoexpecttheagentstoconverge
toCCinsteadofDD,andconvergencetosuboptimalequilibriahasbeenobservedempiricallyin
generalizedStagHuntgames[58].
Asamorerobustapproach,theprincipalcanmakeactionCdominant. Thisisstrongerthanjust
makingCCanequilibriumbecauseeachagentwillpreferactionCregardlessoftheactionsofothers.
Toachievethis,inadditiontopayingaunitofutilitytobothagentsinCC,theprincipalpaystwounits
ofutilitytothecooperatingagentinCDandDC,resultinginthepayoffmatrixasinTable3c. Thisis
thekindofsolutionweimplementinourapplicationtoSSDs,whereweformulatetheprincipal’s
objectiveaslearningasocialwelfaremaximizingstrategyprofileanditsminimalimplementation.
Importantly,inthecontextofourprincipal-agentmodel,theminimalimplementationisconsistent
withSPEinthattheprincipalminimizespaymentswhenagentsbest-respondbyfollowingrecom-
mendations(CCinourexample). SotheICpropertyisanadditionalconstraint, whichspecifies
anotherwiseambiguousobjectiveoffindingtheprincipal’spolicyinSPE,andwhichonlyrequires
additionalpaymentsinequilibrium.
Notethatinourexample,thepaymenttoeachagentforthesameactiondependsontheotheragent’s
action(e.g.,therowagentreceives+2inCDand+1inCC).Thisdependenceonotheragentsis
evenmorepronouncedinstochasticgames,wherepaymentsofaminimalimplementationcondition
onthepoliciesofothers–notonlyontheirimmediateactionsinthecurrentstatebutalsoontheir
actionsinallpossiblefuturestates.9 Forthisreason,learningthepreciseminimalimplementation
is generally impractical: consider a neural network used for contract estimation for some agent
explicitlyconditioningontheparametersofneuralnetworksofallotheragents,orontheiractionsin
allstatesoftheMDP.
As a tractable alternative, we use a simple approximation that performs well in our experiments.
Specifically, wetraintheprincipalassumingthateachagentstickstooneoftwopolicies: either
alwaysfollowtheprincipal’srecommendationsandgetpaid,orignorethemandactindependentlyas
ifthereisnoprincipal. Inequilibrium,bothpoliciesmaximizetheagent’swelfare,whichjustifiesthe
assumption. Forimplementationdetailsofthisapproximation,seeAppendixD.2.
D Experiments
D.1 ExperimentsinTreeMDPs
Inthissection,weempiricallytesttheconvergenceofAlgorithm1towardsSPEinhidden-action
MDPs. Weexperimentwithasmallvariationonthealgorithm,whichexpeditesconvergence: the
agentandprincipalpoliciesareupdatedsimultaneouslyinsteadofiteratively. Thiscanbeseenas
terminatinginnerandoutertasksearly,afteroneupdate,leadingtoapproximatelyoptimalpolicies.
Environment. Intheseexperiments,wesimulateamulti-stageprojectinwhichtheprincipaloffers
contractsatintermediatestages,influencingtheagent’schoiceofeffort. Specifically,wemodelan
MDPthatisacompletebinarytree,wheretheagent’sdecisionsateachstatearebinary,representing
8Unlikethesingle-agentmodel,theoptimalpaymentschemehereisambiguous.Anypaymentschemethat
givesaunitofutilitytobothagentsinCCandnoutilitytoadefectingagentinDCandCDformsanSPEwhen
coupledwithbest-respondingagents:CCbecomesanequilibrium,thepaymentsinwhichareminimized.
9Herewerefertothedependenceofvaluefunctionsonπ intheICconstraints(5).
−i
29Algorithm2AdeepQ-learningimplementationofAlgorithm1inthesingle-agentsetting
Require: principal’sQ-networkθ,agent’sQ-networkϕ,targetnetworksθ′andϕ′,replaybufferRB
1: InitializebufferRBwithrandomtransitions,networksθandϕwithrandomparameters
2: fornumber of updatesdo ▷Trainingloop
3: fornumber of interactions per updatedo ▷InteractwiththeMDP
4: Selectanactiontorecommend,ap,withqθ(s,a)viaϵ-greedy
5: Sampleo∼O(s,a),ra =r(s,a),rp =rp(s,o),transitiontheMDPtos′ ∼T(s,o)
6: Addthetransition(s,ap,ra,rp,o,d,s′)tothebufferRB
7: Ifd=1,resettheMDP ▷disabinary‘done’variableindicatingtermination
8: Sampleamini-batchoftransitionsmb∼RB
9: for(s,ap,ra,rp,o,d,s′)∈mbdo ▷Estimatetargetvariablestoupdateθandϕ
10: ap′ =argmax qθ(s′,a′) ▷SelectthenextactionforQ-learningupdates
a′
11: Findoptimalcontractsb∗(s,ap)andb∗(s′,ap′)bysolvingLP(17)
12:
yp(s,ap)=rp−b∗(s,ap,o)+γ(1−d)qθ′(s′,ap′)
13: ya(s,ap)=ra+γ(1−d)(E
o′∼O(s′,ap′)b∗(s′,ap′,o′)+Qϕ′
(s′,ap′))
14:
MinimizeL(θ)=(cid:80) (cid:0) qθ(s,ap)−yp(s,ap)(cid:1)2
▷UpdateθasDQN
mb
15:
MinimizeL(ϕ)=(cid:80)
(cid:16)
Qϕ
(s,ap)−ya(s,ap)(cid:17)2
▷UpdateϕasDQN
mb
highorloweffort,andcorrespondtogoodorbadoutcomeswithdifferentprobabilities. Thisisan
intentionallysimpleenvironment,allowingustocomparewithprecisegroundtruth.
TheMDPisrepresentedbyacompletebinarydecisiontreewithdepth10,resultingin1023states.
Ineachstate,theagentmaytaketwoactions,a anda ,whichmayresultintwooutcomes,o and
0 1 0
o . Theactiona resultsintheoutcomeo withprobability0.9inallstates;likewise,a resultsino
1 0 0 1 1
withprobability0.9. Thetreetransitionstotheleftsubtreeafteroutcomeo andtotherightsubtree
0
afteroutcomeo .
1
The action a yields no cost for the agent, i.e., ∀s : r(s,a ) = 0; and the outcome o yields no
0 0 0
reward for the principal, i.e. ∀s : rp(s,o ) = 0. Conversely, the action a is always costly and
0 1
the outcome o is always rewarding, with values randomly sampled. Specifically, let U denote
1
one-dimensionaluniformdistribution, U = U[0,1]andU = U[0,2]. Then, theagent’sreward
1 2
isgeneratedasr(s,a ) = −(u ∼ U[0,1−(v ∼ U )]),andtheprincipal’srewardisgeneratedas
1 1
rp(s,o )=(u∼U[0,2−(v ∼U )])Notethattheprincipal’srewardisonaveragehigherthanthe
1 2
agent’scost. Usingthisrewardfunctionsamplingmethod,theprincipal’spolicyρ∗ inSPEoffers
non-trivialcontractsthatincentivizea inabout60%ofstates.
1
Experimental procedure. We generate three instances of the Tree MDP, each with randomly
sampled reward functions, and used five trials of our algorithm on each Tree MDP. We also use
backwardinductiontofindtheexactpoliciesinSPE(π∗,ρ∗)andthecorrespondingoptimalutilities,
whichweadoptasthegroundtruth.Bycomparingwithgroundtruthdirectly,ourtwo-phaseprocedure
fromSection4becomessomewhatredundant,sowedeferittoourmulti-agentexperimentsina
significantlymorecomplexMDP.
Implementation details. We parameterize the principal’s and the agent’s Q-functions as Deep
Q-Networks[51]respectivelydenotedbyθandϕ. Theinputtobothnetworksisastates,andboth
networksapproximateQ-valuesforallactionsa∈A. Specifically,theprincipal’snetworkestimates
thecontractualoptimalQ-valuesqθ(s,a),representingitspayoffswhenoptimallyincentivizingthe
agenttotakeactionainstates;andtheagent’snetworkestimatesthetruncatedoptimalQ-values
ϕ
Q (s,a),representingitspayoffsminustheexpectedimmediatepayment.
Algorithm2describesourDeepQ-learning-basedimplementationofAlgorithm1forthesingle-agent
MDPs. Theoverallpipelineisstandard,withafewnotabledetails.
First,unliketheiterativeconvergenceoftheprincipalandtheagentinAlgorithm1,thetwopolicies
aretrainedsimultaneously.
30(a)Principal’sutility (b)Agent’sutility (c)AccuracyofPrincipalAction
Figure3: ResultsinTreeMDPs. SolidlinesarelearningcurvesofDQNstrainedwithAlgorithm2.
Dashedlinesrepresent‘optimal’utilitiesinSPEobtainedwithdynamicprogramming. Different
colorsrepresentthreedistinctinstancesofthetreeenvironment. Foreach,weusefivetrialsofthe
algorithm(shadedregionsrepresentstandarderrors).
Second, the outcome function O is assumed to be known. If not, it can be parameterized as an
additional neural network ξ and trained as a probabilistic classifier with sampled outcomes used
as ground truth; the resulting loss function would be L(ξ) = (cid:80) CE[O (s,ap),o], where CE
mb ξ
denotescross-entropy,mbdenotesamini-batch,andO (s,ap)denotesthepredictedprobabilitiesof
ξ
outcomesasafunctionofstate-action. Weimplementedthisinoursingle-agentexperimentswithO
constantacrossstatesandfoundnodifferencefromhavingaccesstoO,althoughthismightchange
inmorecomplexscenarios.
Third,whenupdatingtheagent’snetworkϕ,thetargetvariableyϕ′(s,ap)isestimatedbasedonthe
contractinthenextstates′ ratherthanthecurrentstates. Sincepaymentisapartofreward,the
targetvariableiseffectivelyestimatedasamixtureofpartsofrewardsinsands′. Thisisrequiredso
thatthetruncatedratherthanthestandardQ-functionislearned.
Hyperparameters. Theneuralnetworkshave2hiddenfullyconnectedlayers, eachconsisting
of256neuronsandfollowedbyaReLUactivation. Thenetworksaretrainedfor20000iterations,
each iteration including 8 environment interactions and a single gradient update on a mini-batch
with128transitions,sampledfromthereplaybuffer. Every100iterations,thetargetnetworksare
updatedbycopyingtheparametersoftheonlinenetworks. Thelearningrateisinitializedat0.001
andisexponentiallyannealedto0.0001throughoutthetraining. Similarly,theexplorationrateϵis
initializedat1andislinearlyannealedto0throughoutthetraining. Rewardsarenotdiscounted,i.e.,
γ =1.
Results. The results arepresented in Figure 3. From Figure 3a, we observe that our algorithm
attainsaprincipal’sutilityofjust2%belowtheoptimalutilityinSPE.Ontheotherhand,Figure3b
showsthattheagent’sutilitycanexhibitsimilar(inabsoluteterms)deviationsfromSPEineither
direction. The’accuracy’metricinFigure3cindicatesthattheprincipalrecommendstheaction
prescribedbytheoptimalpolicyinapproximately90%ofthestates.
The small underperformance in the principal’s utility and the biases in the agent’s utility can be
partiallyattributedtothe10%ofthenon-optimalprincipal’sactions. Thataround90%ofcorrect
actionsamounttoaround98%oftheprincipal’sutilitysuggestserrorslikelyoccurinrarelyvisited
statesorstateswhereactionsresultinsimilarpayoffs. effectivenessinapproximatingtheSPE.The
minordiscrepanciesinutility,coupledwiththelearningdynamics,underscorethecomplexinterplay
betweentheprincipalandtheagentastheyadapttoeachotherthroughouttraining.
D.2 ExperimentsintheCoinGame
Implementationdetails. Algorithm3describesourDeepQ-learning-basedimplementationof
Algorithm1forthemulti-agentMDPswithself-interestedagents,oftenreferredtoas“sequential
socialdilemmas”. Theexperimentalprocedureconsistsoftwophases: trainingandvalidation.
Inthetrainingphase, foreachagenti, theprincipal’sobjectiveistofindapolicytorecommend
bylearningthecontractualQ-function,qθ,aswellasitsminimalimplementationbylearningthe
i
agent’sQ-functionintheabsenceoftheprincipal,Qϕ. InSection5.1,weadditionallyrequirethe
i
31(a)Socialwelfare (b)Socialwelfare,nudging (c)Proportionofsocialwelfarepaid
(d)Accuracy (e)ConvergencetoSPE? (f)Incentive-Compatiblecontracts?
Figure 4: Learning curves in the Coin Game with a 3×3 grid and each episode lasting for 20
timesteps. ThestructureisthesameasinFigure2,withtheadditionofthetopmiddleplot. The
definitionoftheplotsisasfollows: a)totalreturnofthetwoagents(withoutpayments);b)same,
butouralgorithmandconstantbaselineadditionallypay10%ofsocialwelfaretotheagents;c)a
ratioofthetotalpaymentbytheprincipaltowhattheagentswouldeffectivelybepaidifdirectly
maximizingsocialwelfare;d)theproportionoftheprincipal’srecommendationsfollowedbythe
validation agents; r) a ratio between the utilities of an agent’s policy at a given iteration and the
recommendedpolicy,withtheopponentusingtherecommendedpolicy;f)sameratio,butwiththe
opponent’spolicyatagiveniteration. Eachexperimentisrepeated5times,andeachmeasurementis
averagedover80episodes.
minimalimplementationtobeindominantstrategies. Learningsuchanimplementationrequires
conditioningQϕontheinter-agentpoliciesπ ,whichisintractable. Asatractableapproximation,
i −i
given that rational agents will only deviate from recommendations if their expected payoffs are
notcompromised,wesimplifyeachagent’sstrategyspacetotwoprimarystrategies: cooperation
(followingtheprincipal’srecommendations)orrationaldeviation(defaultoptimalpolicydisregarding
contracts,whichgivesthesameutility). Weencapsulatethisbehaviorinabinaryvariablef ∈{0,1},
i
indicatingwhetheragentifollowstherecommendationap. Then,Qϕisconditionedonthejointf
i −i
variableoftheotheragents,indicatingboththeimmediateoutcomeandtheagents’futurebehavior.
Theefficacyofthissimplificationisvalidatedthroughourexperimentalresults.
Duringthetrainingphase,werandomlysamplef foreachagentatthebeginningofanepisode. For
i
theepisode’sremainder,eachagenteitherfollowstherecommendationsgivenbyqθ (iff =1)or
i i
actsselfishlyaccordingtoQϕ (iff =0). Theexperiencegeneratedthiswayisthenusedtotrain
i i
bothθandϕ,enhancingexplorationandcoveringthewholespaceoff.
Giventheprincipalobtainedinthetrainingphase,thesubsequentvalidationphaseindependently
trainsselfishagentsparameterizedbyψ fromscratchinthemodifiedenvironment. Specifically,each
i
agentobservesanactionrecommendationwhencomputingQ-values,Qψi((s,ap),a ),andispaid
i i i
bytheprincipaliftherecommendationisfollowed. Whenestimatingpayments,f simplyindicates
i
whetheragentifollowedtherecommendation. Otherthanthesechanges,weusethestandardDQN
trainingprocedure. Wealsodonotassumetheprincipal’saccesstotheagents’privateinformation
likeQ-valuesorparameters.
Hyperparameters. Theneuralnetworksconsistof1hiddenconvolutionallayerfollowedby2
hiddenfullyconnectedlayersandanoutputlayer. Theconvolutionshave4×4kernelsandtransform
theinitial4-channelstatesinto32channels. Thefullyconnectedlayersconsistof64neurons. All
hiddenlayersarefollowedbyReLUactivations. Thenetworksaretrainedfor1000000iterations,
eachiterationincludingasingleenvironmentinteractionandasinglegradientupdateonamini-batch
with128transitions,sampledfromthereplaybuffer. Every100iterations,thetargetnetworksare
32updatedbycopyingtheparametersoftheonlinenetworks. Thelearningrateisinitializedat0.0005
andisexponentiallyannealedto0.0001throughoutthetraining. Similarly,theexplorationrateϵis
initializedat0.4andislinearlyannealedto0throughoutthetraining. Thediscountingfactorissetat
γ =0.99. Formoreefficienttraining,weuseprioritizedreplaybuffers[69]withamaximumsizeof
100000,α=0.4,β =0,andϵ=10−7.
Additionalresults. InFigure4,wereportresultsintheCoinGamewitha3×3gridandeach
episodelastingfor20timesteps. Thisisasimplerenvironmentthantheoneinthemaintext,asthe
statespaceandthehorizonaresmaller.
Duringtraining,ouralgorithmfindsanapproximatelyoptimaljointpolicy(Fig. 4a)andestimates
that about 30% of social welfare is sufficient for an IC implementation (Fig. 4c). Interestingly
and contrary to our previous results, the validation phase does not confirm this: we observe that
thevalidationagentsfallshortoftheoptimalperformance,aswellasthatouralgorithmdoesnot
outperformtheconstantproportionbaseline(Fig. 4a). Ontheonehand,wedonotfindevidencethat
itdoesnotconvergetoSPE,asinFigure4e,theutilityratiohoversaround1. Ontheotherhand,a
testonincentivecompatibility(Fig. 4f)revealsthatthevalidationagentsconsistentlyfindpolicies
thatperform5%to10%betteragainsttheopponentDQNs,meaningthattheprincipalfailstolearn
ICcontracts. Formoreinformationonthesetests,seethediscussioninSection5.3Weconjecture
thatthisnegativeresultisduetousingtheapproximationofICthroughf variablesduringtraining,
i
asdescribedintheimplementationdetails.
As an ad-hoc remedy, we attempt to artificially increase the principal’s payments by 10% of the
socialwelfare;weincreasetheproportionoftheconstantbaselineaccordingly. Thisisaformof
nudgingintendedtoremedytheperformance-degradingeffectsofapproximationerrorsdiscussedin
Section4. TheeffectofthismodificationisillustratedinFigure4b: theperformanceofouralgorithm
reachesthatoftheoptimalbaseline,whereastheconstantbaselinestillfallsshort. Furthermore,the
ICpropertyappearsfixedastheutilityratiodecreasestoaround1(Fig. 4f). Theaccuracymetricalso
improves,raisingthefrequencyofagentsfollowingtheprincipal’srecommendationfromaround
80%toaround90%.
Overall,webelievetheseresultstobepositive: ouralgorithmfallsshortsomewhatpredictablygiven
thepracticalapproximationofIC(andthetie-breakingassumption),andstillmanagestooutperform
theconstantbaselinewhilepayingthesame.Asasidenote,theseresultsalsoshowcasetheusefulness
ofourtwo-stepexperimentalprocedure,asopposedtotheusualperformancecomparisonduringthe
trainingphase,whichdoesnotrevealthehiddenissues.
D.3 Compute
AllexperimentswererunonadesktopPCwith16GBRAM,11thGenIntel(R)Corei5-11600KF
@3.90GHzprocessor,andNVIDIAGeForceRTX2060GPU.Thesingle-agentexperimentswere
runusingonlyCPU,andthemulti-agentexperimentswererunusingGPUtostoreandtrainneural
networks and CPU for everything else. Each run (one algorithm, one random trial) takes thirty
minutestoanhour. Theopen-sourcecodepackagesweusearePyTorch(BSD-stylelicense)[56],
RLlib(Apachelicense)[44],Stable-Baselines3(MITlicense)[61],Gymnasium(MITlicense)[76],
andW&B(MITlicense)[4].
33Algorithm3AdeepQ-learningimplementationofAlgorithm1inthemulti-agentsetting
TRAININGPHASE
Require: principal’sQ-networkθ,agents’Q-networkϕ,targetnetworksθ′,ϕ′,replaybufferRB
1: InitializebufferRBwithrandomtransitions,networksθandϕwithrandomparameters
2: Sampleabinaryvectorf =(f i) i∈N ▷f iindicatesififollowsrecommendationsthisepisode
3: fornumber of updatesdo ▷Trainingloop
4: fornumber of interactions per updatedo ▷InteractwiththeMDP
5: fori∈N :f i =1do: ▷Theprincipalactsfortheseagents
6: Selectarecommendedactionap i withq iθ(s,ap i)viaϵ-greedy,seta i =ap i
7: fori∈N :f i =0do: ▷Theseagentsactselfishlyignoringpayments
8: Selectanagent’sactiona iwithQϕ i((s,f −i),a i)viaϵ-greedy
9: fori∈N do: ▷Getrewardsfromtheenvironment
10: r i =r i(s,a)
11: TransitiontheMDPtos′ ∼T(s,a)
12: Addthetransition(s,a,f,r,s′)tothebufferRB
13: Ifd=1,resettheMDPandresamplef
14: Sampleamini-batchoftransitionsmb∼RB
15: for(s,a,f,r,s′)∈mbdo ▷Estimatetargetvariablestoupdateθandϕ
16: fori∈N do
17: b∗ i(s,a i)=max aQϕ i((s,1),a)−Qϕ i((s,1),a i) ▷Asifawererecommended
18: y ip(s,a i)=r i−αb∗ i(s,a i)+γmax a′ iq iθ′(s′,a′ i) ▷wesetα=0.1
19: y ia(s,a i)=r i+γmax a′ Qϕ′((s′,f −i),a′ i)
20: MinimizeL(θ)=(cid:80) mb(cid:0)(cid:80) iq iθ(i s,a i)−(cid:80) iy ip(s,a i)(cid:1)2 ▷UpdateθasVDN
(cid:16) (cid:17)2
21: MinimizeL(ϕ)=(cid:80) mb(cid:80)
i
Qϕ i((s,f −i),a i)−y ia(s,a i) ▷UpdateϕasDQN
VALIDATIONPHASE
Require: validationagents’Q-networks(ψ ) ,targetnetworks(ψ′),replaybufferRB
i i∈N i
22: InitializebufferRBwithrandomtransitions,networksψ iwithrandomparameters
23: fornumber of updatesdo ▷Trainingloop
24: fornumber of interactions per updatedo ▷InteractwiththeMDP
25: fori∈N do ▷Agentsactselfishly
26: ap =argmax qθ(s,a) ▷Recommendedactionbyθ
i a i
27: Selectanagent’sactiona iwithQψ ii((s,ap i),a i)viaepsilon-greedy
28: Setf i =1ifa i =ap i elsef i =0 ▷f iindicatesififollowedrecommendationins
29: fori∈N do: ▷Gettotalrewards
30: b∗ i(s,ap i)=max aQϕ i((s,f −i),a)−Qϕ i((s,f −i),ap i)
31: R i =r i(s,a)+f ib∗ i(s,ap i) ▷Agentiisonlypaidiff i =1
32: TransitiontheMDPtos′ ∼T(s,a)
33: Addthetransition(s,a,R,s′)tothebufferRB
34: Ifd=1,resettheMDP
35: Sampleamini-batchoftransitionsmb∼RB
36: fori∈N do ▷IndependentlyupdateeachQ-network
37: for(s,a i,R i,s′)∈mbdo ▷Estimatetargetvariablestoupdateψ i
38: ap′ =argmax qθ(s′,a′)
i a′ i
39: y i(s,a i)=R i+γmax a′ iQψ ii′ ((s′,ap i′),a′ i)
(cid:16) (cid:17)2
40: MinimizeL(ψ i)=(cid:80)
mb
Qψ ii((s,ap i),a i)−y i(s,a i) ▷Updateψ iasDQN
34