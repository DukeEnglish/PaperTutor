Do We Really Need Graph Convolution During Training?
Light Post-Training Graph-ODE for Efficient Recommendation
WeizhiZhang LiangweiYang ZiheSong
wzhan42@uic.edu lyang84@uic.edu zsong29@uic.edu
UniversityofIllinoisChicago UniversityofIllinoisChicago UniversityofIllinoisChicago
Chicago,USA Chicago,USA Chicago,USA
HenryPengZou KeXu PhilipS.Yu
pzou3@uic.edu LianchengFang psyu@uic.edu
UniversityofIllinoisChicago kxu25@uic.edu UniversityofIllinoisatChicago
Chicago,USA lfang87@uic.edu Chicago,USA
UniversityofIllinoisatChicago
Chicago,USA
ABSTRACT KEYWORDS
Theefficiencyandscalabilityofgraphconvolutionnetworks(GCNs) GraphRecommendation,EfficientRecommendation,GraphConvo-
intrainingrecommendersystems(RecSys)havebeenpersistentcon- lutionNetwork,GraphOrdinary-Differential-Equation
cerns,hinderingtheirdeploymentinreal-worldapplications.This
ACMReferenceFormat:
paperpresentsacriticalexaminationofthenecessityofgraphcon-
WeizhiZhang,LiangweiYang,ZiheSong,HenryPengZou,KeXu,Liancheng
volutionsduringthetrainingphaseandintroducesaninnovative
Fang,andPhilipS.Yu.2024.DoWeReallyNeedGraphConvolutionDuring
alternative:theLightPost-TrainingGraphOrdinary-Differential-
Training?LightPost-TrainingGraph-ODEforEfficientRecommendation.
Equation(LightGODE).Ourinvestigationrevealsthatthebenefits
InProceedingsofthe33rdACMInternationalConferenceonInformationand
ofGCNsaremorepronouncedduringtestingratherthantraining. KnowledgeManagement(CIKM’24),October21–25,2024,Boise,ID,USA.ACM,
Motivatedbythis,LightGODEutilizesanovelpost-traininggraph NewYork,NY,USA,11pages.https://doi.org/10.1145/3627673.3679773
convolutionmethodthatbypassesthecomputation-intensivemes-
sagepassingofGCNsandemploysanon-parametriccontinuous
1 INTRODUCTION
graphordinary-differential-equation(ODE)todynamicallymodel
noderepresentations.Thisapproachdrasticallyreducestraining Recommendersystems(RecSys)aresignificantintegralpartsof
timewhileachievingfine-grainedpost-traininggraphconvolution manyonlineplatformsandwebapplications,helpingusersnavigate
toavoidthedistortionoftheoriginaltrainingembeddingspace, vastamountsofinformationbyprovidingpersonalizeditemrecom-
termedtheembeddingdiscrepancyissue.Wevalidateourmodel mendations.Thesesystemsareessentialacrossvariousdomains
acrossseveralreal-worlddatasetsofdifferentscales,demonstrat- suchasdigitalretailing[13,33],socialnetworkingplatforms[5,14],
ingthatLightGODEnotonlyoutperformsGCN-basedmodelsin andvideo-sharingservices[38],wheretheyfilterandtailorcontent
terms of efficiency and effectiveness but also significantly miti- toalignwithindividualuserpreferences.Amongthetechniques
gatestheembeddingdiscrepancycommonlyassociatedwithdeeper [18,26,29,31]usedinRecSys,collaborativefiltering(CF)[18]isno-
graphconvolutionlayers.OurLightGODEchallengestheprevailing tablyeffective,anditpredictsuserpreferencesbasedonhistorical
paradigmsinRecSystrainingandsuggestsre-evaluatingtherole user-iteminteractions.Essentially,thosehistoricalinteractionscan
ofgraphconvolutions,potentiallyguidingfuturedevelopmentsof berepresentedasauser-itembipartitegraph.Inspiredbythesupe-
efficientlarge-scalegraph-basedRecSys. riorabilityofgraphconvolutionnetworks(GCNs)[16,22,39,42]in
modelingongraph-structureddata,alargenumberofGCN-based
recommendationmodels[8,36,37,47]haveemergedrecently.They
CCSCONCEPTS
sharethecommonideaoflearningthenoderepresentationviaac-
•Computingmethodologies→Datamining;•Collaborative
quiringneighborhoodinformationinthebipartitegraphlayerby
filteringandgraphrecommendation;
layer,thuscapturingthemulti-hopconnectivityofusers/items[40].
Despitetheinspiringprogressmadeingraph-basedrecommen-
dation,theseapproachesareinherentlychallengedbytheissues
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
ofefficiencyandscalability.Theyareintrinsicallyraisedbythe
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation computation-intensemessage-passingofgraphconvolutioninthe
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe existingtrainingparadigmofgraph-basedrecommendation.Such
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
problemsarefurtherexaggeratedinthereal-worldapplicationof
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. large-scalegraphsasthetime/computationcomplexitywillgrow
CIKM’24,October21–25,2024,Boise,ID,USA exponentiallywiththenumberofusersanditems.Recentstudies
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
showthatsimpleMLPsastheinitializationofgraphmodel[7,46]
ACMISBN979-8-4007-0436-9/24/10...$15.00
https://doi.org/10.1145/3627673.3679773 ortrainedwithcontrastivelearning[10],knowledgedistillation
4202
luJ
62
]GL.sc[
1v01981.7042:viXraCIKM’24,October21–25,2024,Boise,ID,USA WeizhiZhang,LiangweiYang,ZiheSong,HenryPengZou,KeXu,LianchengFang,andPhilipS.Yu
[48]demonstratecompetitiveperformancecomparedwithGCN First,itcharacterizesthecontinuousdynamicsofuser/itemrepre-
modelsaslongastheyshareanequivalentweightspace.Consider- sentationswithinthebipartitegraph,makingthetraditionalgraph
ingthatonecantriviallyderiveacounterpartlightgraphmodel[8] convolution a specific discretization of seamless layer-wise em-
giventhematrixfactorization(MF)[25]weight,wenaturallyraisea beddingtransformation.Additionally,itenablespreciseandfine-
meaningfulandsignificantquestion:Dowereallyneedcomputation- grainedgraphconvolutiontoachievetheoptimaltrade-offwith
intensegraphconvolutionduringtrainingforrecommendation? thecontinuous-timevaluetocapturehigh-orderinformationwhile
Toaddresstheinquiry,wefirstconductedapreliminaryexperi- balancingtheembeddingdiscrepancy.Tofosterfutureresearch
menttoinvestigatetheroleofgraphconvolution.Theresultsreveal anddevelopmentofLightGODE,wehavereleaseditopen-source,
thatgraphconvolutionhasamorepivotalroleintestingrather availableathttps://github.com/DavidZWZ/LightGODE.Here,we
thanintraining.Notably,theMFmodeliscapableofmatchingthe summarizeourcontributionsasfollows:
performanceoftheGCNwhenasimilarlightweightgraphconvo- • Tothebestofourknowledge,wearethefirsttochallengethe
lution[8]isimplementedaftertraining.Touncovertheunderlying
long-standing authority in the graph-based recommendation
reasonsfromatrainingviewpoint,weexaminedthesupervision
- the necessity of graph convolution, and we empirically and
alignmentforcewhentrainingwithMFandLightGCNmodels,find-
analyticallyrevealitsdecisiveroleintestingratherthantraining.
ingthatthealignmentproperty[32,34]ofpositiveuser-itempairs • Wedevelopedanovelpost-traininggraphconvolutionframe-
isapproximateintwodistincttrainingparadigms.Thisprompted
workforextremelyefficienttraininganddevisedanone-parametric
ustofurtherexplorethetrainingprocessesoftheMFandGCN
GCNwithself-loop,alleviatingtheembeddingdiscrepancyissue.
models,leadingustoconcludethatGCN-basedtrainingessentially • Originally,weproposedacontinuousgraphordinary-differential-
actsasadegree-weightedformofMFtraining.Intuitively,byfol-
equation(LightGODE),whichallowsdynamicmodelingofnode
lowingthepairwisealignmentforcefromadepth-firstsearch(DFS)
representationsandachievingoptimaltrade-offsofhigh-order
perspective,MFtrainingresultsineffectsakintoGCNtrainingthat
informationandembeddingdiscrepancy.
adoptsinformationaggregationbasedonbreadth-firstsearch(BFS). • Weconductextensiveexperimentsonthreereal-worlddatasets
Giventhetimedemandoftheseprocesses,wesuggestthatgraph
totesttheeffectivenessofLightGODE,demonstratingthehighest
convolutionmaynotbenecessaryduringtraining.However,the
recommendationperformancewiththelowesttrainingtime.
currentgraphconvolutionmethodissuboptimal,asweempirically
findthattheincreasingnumberoflayerssignificantlyenlargesthe 2 INVESTIGATIONOFTHEGRAPH
differencebetweenembeddingsbeforeandafterconvolution,de-
CONVOLUTIONFORRECOMMENDATION
notedastheEmbeddingDiscrepancy.AssumingtheMFmodelis
well-trained,anypost-trainingoperationsshouldnotsignificantly Inthissection,weinitiallyinvestigatethenecessityofgraphcon-
altertheoriginalembeddingspace,whereastheexistingconvolu- volutionforrecommendationandexaminethekeyreasonsbehind
tionstrategywithhighembeddingdiscrepancymaypotentially theunexpectedlysuperiorperformanceoftheMFmodelenhanced
offsetthebenefitsofhigher-orderinformation.Moreover,theex- bypost-traininggraphconvolution.Subsequently,wepinpointthe
istingcoarse-grainedgraphconvolutionapproachesfailtofindan trade-offsindesigningpost-traininggraphconvolutionbyidenti-
optimalconvolutiondepthduetoitsdiscretecharacteristics.These fyingtheembeddingdiscrepancyissueswhenconstructingdeeper
motivateustoseekamorefine-grainedmethodtointegratehigher- graphconvolutionlayers.
orderuser-iteminteractionswhileavoidingcomputation-intense
messagepassingduringtraining.
In this paper, we introduce Light Post-Training Graph-ODE 0.08 MF-init LightGCN-init 0.150 MF-init LightGCN-init
MF-conv LightGCN-conv MF-conv LightGCN-conv
( aL ni dgh et ffiGO ciD enE t), la an rgo ev -e sl cg ar la ep Rh- eb ca Ss ye sd .m Spe eth co ifid cd ae lls yig ,n we edf fio rr sfi tn pe r- og pr oai sn ee ad 0.06 95.87%100% 93.76%100% 0.125 97.65%100% 94.88%100%
novelPost-TrainingGraphConvolution(PTGC)paradigmthatsig- 0.0473.67% 59.07% 75.78% 67.51%
00 .. 01 70 5076.03%
62.10% 72.97% 67.51%
nificantlyimprovestrainingefficiencybyskippingthemosttime-
0.050
consumingoperations,includingadjacencymatrixnormalization 0.02
0.025
andlayer-by-layergraphconvolutions,makingthetrainingpro-
0.00 0.000
cessasefficientasfortraditionalMFmodels.Totackletheissue Beauty Toys-and-Games Beauty Toys-and-Games
Datasets Datasets
of embedding discrepancy, we develop a non-parametric graph
convolutionthatincorporatestheself-loopduringtheinformation Figure1:Preliminarystudyontheroleofgraphconvolu-
update.Thisstraightforwardoperationwillprioritizethepreceding tionforrecommendationintrainingandtestingstages.The
layers,therebyimplicitlyassigninggreaterimportancetoshallow MFmodelwithgraphconvolutionaftertraining(MF-conv)
layers, particularly the initial embeddings during graph convo- achievescompetitiveresultswiththeLightGCN-conv.
lution,whichaidsinminimizingthevariationbetweentheem-
beddingspacesbeforeandaftergraphconvolution.Thus,ithelps
2.1 TheRoleandNecessityofGraph
to reduce the distribution discrepancy between the embedding
ConvolutionduringTraining
space.Buildingonthisfoundation,weproposeacontinuousgraph
ordinary-differential-equationderivedfromthediscreteparameter- Toinvestigatethenecessityofgraphconvolutionforgraph-based
freegraphconvolution.Thecontinuityoffersseveraladvantages. RecSys,weconductpreliminaryexperimentsontheAmazon-Beauty
(denotedasBeauty)andtheAmazon-Toys-and-Games(denotedas
02@GCDN 02@llaceRDoWeReallyNeedGraphConvolutionDuringTraining?
LightPost-TrainingGraph-ODEforEfficientRecommendation CIKM’24,October21–25,2024,Boise,ID,USA
Toys-and-Games)datasetstounderstandtheimpactofgraphconvo- BFS perspective of alignment in GCN DFS perspective of alignment in MF
lutioninthetraining/testingstagesofrecommendation.Specifically,
wedesignfourvariationsofthemodelwiththesameamountof
embeddingparameters,includingMF-init,whichinvolvestraining
withtraditionalmatrixfactorizationandtestingwithitsfactorized
IDembeddings;MF-conv,whichintegratesthe2-hopsLightGCN
convolutionafterMFtraining;LightGCN-init,whichtestsonlywith
initialembeddingsfromaLightGCNmodel;andLightGCN-conv, 1-hop graph convolution to The alignment force applied to
the user and item nodes the positive user-item pairs
whichfullyimplementsa2-layerLightGCNmodelarchitecture.
As illustrated in Figure 1, we establish the LightGCN model Figure2:AcomparisonofalignmentforceinGCN-basedand
(LightGCN-conv) as the benchmark by setting its performance MF-basedmodelsfromBFSandDFS,respectively.
as100%.Tooursurprise,MF-convconsistentlyoutperformsboth
MF-initandLight-convacrossthetwodatasets,achievinganim-
pressiveaverageofover95%oftheperformancemetricscompared Intuitively,inanillustrativescenariowhereasingle-layergraph
to LightGCN. Thisclearly highlights thesubstantial benefits of convolutionnetworkisemployedforgatheringinformation,as
integratingpost-traininggraphconvolutionwithMFinitialization, depictedintheleftsectionofFigure2,oneobservesthesubgraph
whichsignificantlyreducescomputationalcostsbycircumventing comprisingpositivepairs𝑈 1 and𝑉 1 alongsidetheirneighboring
theintricategraphconvolutionprocess.Furthermore,theseresults nodes.Whenthealignmentforceactsupon𝑈 1and𝑉 1,therepresen-
underscorethattheimprovedperformanceofgraph-basedRecSys tationsoftheirrespectiveadjacentnodes,suchas𝑉 2and𝑈 2,also
primarilyarisesfromthegraphconvolutionaftertraining,which moveclosertogether.Conversely,intherightportionofFigure2,
promptsathoroughreconsiderationofthenecessityofthegraph analternativeapproachisshowcasedwhere,insteadofemploying
convolutionduringthetrainingphase.Meanwhile,weproposea
BFSforneighborhoodaggregation,directconnectionsbetween𝑈
1
newpointofviewtounderstandtheunderlyingreasonsbehindthe andnodes𝑉 2 and𝑈 2 areestablishedviaDFSpathsamongvari-
unexpectedexceptionalperformanceoftheMF-convmodeleven oususer-itempairs,achievingacomparableoutcomeintermsof
trainedwithoutgraphconvolution. alignedrepresentationlearning.Thisconclusionfurtherweakens
thenecessityoftime-intensivegraphconvolutionintraining.
2.2 TheAlignmentForce:ADFSPerspective
2.3 Trade-offinDesigningGraphConvolution
Recommendationlossestypicallyaimtoidentifythepotentialposi-
tiveinteractionsviaapplyingasupervisedalignmentforcetoposi-
tiveuser-itempairsduringtraining.Inthiscontext,weempirically Beauty Toys-and-Games
evaluatedthealignmentproperty[32,34](theaveragedistance 0.058 0.10 0.056 0.10
betweennormalizedpositiveembeddings)offourmodelversions 0.056 0.08 0.054 0.08
0.054 0.06 0.052 0.06
inSection2.1acrosstheBeautyandToys-and-Gamesdatasets. 0.052 0.04 0.050 0.04
0.050 0.02 0.048 0.02
Table1:Thealignmentpropertyofpositivepairsintraining. 0.048 1 2 3 4 5 6 7 8 910 0.00 0.046 1 2 3 4 5 6 7 8 910 0.00
Number of Layers Number of Layers
Beauty Toys-and-Games Figure3:Studyofthetrade-offofembeddingdiscrepancy
Training
andhigh-orderinformationonBeautyandToys-and-Games.
Initial Conv. Initial Conv.
MF 0.7952 0.6631 0.8100 0.7033
Intheprecedingsections,wehighlightedthenotableefficacyof
LightGCN 0.8270 0.6594 0.7761 0.6503
theMF-convmodelandanalyzedthealignmentpropertiestopin-
pointkeycontributorstoitsapproximateperformanceofLightGCN-
InTable1,theinitialIDembeddingsforbothMFandLightGCN conv.However,theMF-convstillslightlylagsbehindtheLightGCN-
exhibit approximate alignment values in Beauty and Toys-and- conv,anditisnotyetclearhowtodesignamoreeffectivenon-
Games,suggestingcomparabletrainingeffectswithandwithout parametricgraphconvolutioninthepost-trainingstage.Thehint
lightweightgraphconvolution.Similarly,theembeddingsofpost- intheexperiments(Figure1)suggeststhatincorporatingthemulti-
trainingconvolutionshowcloselymatchedvalues,complyingwith hopconnectivityinformationduringthetestingphaseprovesben-
theexperimentalfindingsoftheircomparableperformancelevels eficial.Then,itwouldbevaluabletoinvestigatewhetherhigher-
inFigure1.Theseobservationspromptustoexplorewhetherthe order connectivity continues to be advantageous after training
alignmentforcesexertedonuser-itempairs,withandwithoutthe withtheMFmodel.Inaddition,ifamodelisoptimallytrained
lightgraphconvolution,areeffectivelyequivalent. to fit the user-item interactions, one would expect the training
Analytically,whenagraphmodelisoptimizedbythesameobjec- embeddingdistributiontobeidealfortesting.Consequently,any
tiveastheMFmodel,thealignmentforceappliedonsurrounding post-trainingoperationsshouldminimallyimpacttheoriginalem-
neighborsofpositivepairswithgraphconvolutionisthedegree- beddingspace.Weareparticularlyinterestedinexploringhowthe
weightedversionofthatalignmentdirectlyforcedontwoclusters model’sperformancecorrelateswiththedifferencesbetweeninitial
ofnodes.TheassumptionandproofarelistedintheAppendixC. andconvolutionembeddings,termedasEmbeddingDiscrepancy.
02@GCDN
ycnapercsiD
gniddebmE
02@GCDN
ycnapercsiD
gniddebmECIKM’24,October21–25,2024,Boise,ID,USA WeizhiZhang,LiangweiYang,ZiheSong,HenryPengZou,KeXu,LianchengFang,andPhilipS.Yu
WeutilizetheaverageEuclideandistance,widelyacknowledged TheuniformitylossL𝑢𝑛𝑖𝑓𝑜𝑟𝑚 = (L 𝑢𝑈 𝑛𝑖𝑓𝑜𝑟𝑚+L 𝑢𝑉 𝑛𝑖𝑓𝑜𝑟𝑚)/2,and
for numerical shifts [6], to quantify distribution shift across all theuser-sideuniformityisgivenby:
users/itemsintheembeddingspaceduringgraphconvolution.
InFigure3,weempiricallyincreasethelayernumberofpost- L 𝑢𝑈 𝑛𝑖𝑓𝑜𝑟𝑚 =log |B1 𝑢|2 ∑︁ ∑︁ 𝑒−2∥u𝑖−u𝑖′∥, (2)
traininggraphconvolution,andtheperformancepeaksatatwo- u𝑖∈B𝑢u𝑖′∈B𝑢
hopconvolution.Surprisingly,incorporatingmorecomplex,higher- whereB𝑢 istheuserbatchandu𝑖′ isrestofusersinbatch.The
orderinformationmostlyleadstoaperformancedecline.Addition- itemsideuniformityL𝑈
followsthesameformatandfinal
𝑢𝑛𝑖𝑓𝑜𝑟𝑚
ally,thediscrepancybetweeninitialandconvolutionembeddings
lossbecomesL=L𝑎𝑙𝑖𝑔𝑛+𝛾L𝑢𝑛𝑖𝑓𝑜𝑟𝑚 adjustedbyweight𝛾.
enlargeswithmorelayers,indicatingthattheexistinggraphcon-
volutionstrategycandisruptthefoundationaltraining,potentially
3.2 DiscreteGCNwithSelf-Loop
causing over-smoothing [28] as layers increase (Figure 3). This
Empiricalevidencein[8,37]andSection2suggeststhatoptimal
suggeststhatwhileadditionalconvolutionlayersintroducemore
performanceistypicallyachievedwhenthegraphmodelisconfig-
high-orderinformation,theyalsoriskperturbingwell-trainedem-
uredwithtwoorthreelayers.However,abruptlydiscontinuingthe
beddings.Thiscouldexplainthecounterexampleasincreasingthe
convolutionprocessathigher-orderlayersisinappropriatesince
numberofconvolutionlayersinitiallypromotestheperformance
neithertheprecedingshallowlayersaredistinctivelytreatednorthe
andthencontinuallybringsnegativeeffects-thecurrentstrategy
subsequenthigh-orderlayersarenoticed.Suchanapproachlacksa
findsabalanceofconfiguringtwolayers.
seamlesstransitionfromlowertohigher-ordergraphconvolutions,
Toenhanceperformance,itiscrucialtoincorporatehigher-order
potentiallyoverlookingnuanceddifferencesinstructuralinforma-
informationwhilemaintaininganembeddingdistributionclose
tionembeddedinshallowanddeepgraphrelationships.Thiscalls
tothatoftheoriginalMFmodelbyaddingmorelayers.Thisne-
forreconsideringthegraphconvolutionprocessacrossdifferent
cessitatesamorenuancedgraphconvolutionapproachthatdeli-
layerdepthstobettercapturethecomplexityanddynamicsofthe
catelyconstructslayerstomaintainatrade-offbetweenhigh-order
graphdatainrecommendationcontexts.
structureinformationandtheembeddingdiscrepancyissue.Ina
Onestraightforwardsolutionistointegratetheself-loop(SL)
comprehensiveviewofefficiencyandeffectiveness,wedesigna
intothegraphconvolutionprocess.Thissimpleoperationhigh-
morefine-grainedapproachtobalancetheconvolutiondepthand
lights the importance of the node representations of preceding
embeddingdiscrepancy,asintroducedinthefollowingsection.
layersineachmessage-passingprocess,contributingtoagradual
transitiontohigher-orderconnectivity.Supposeweobserveapair
3 LIGHTPOST-TRAININGGRAPH-ODEFOR
ofinteractedusersanditemswithcorrespondinginitialinputID
EFFICIENTRECOMMENDATION embeddingsu 𝑖0andv0 𝑗,andwedesigntheparameter-freegraphcon-
Inthissection,weproposethepost-traininggraphconvolution volutionbasedonthesmoothedneighborhoodaggregationprocess
framework,includingthepre-traininguser/itemembeddingsforex- asin[8].ThegraphconvolutionwithSLisfinalizedas:
wt or fe ehm
di
ee
g
vl hy
i-
soe effi
r
adc nei
r
oen nint -pfg
o
ar
r
ra
m
ap mh
at
er
i
toe rc
n
ico am
gn
rm
d
ape
t
hn hd
e
ca ort nii so vkn olo. uT
f
to
e
iomb na
b
wl ea dn itdc he
in
st egh lfe
d
-li
i
on
sc
ote
r
peg .pr Ba
a
at
n
sio
c
eyn
d,
u𝑘
𝑖
=u𝑘 𝑖−1+ 𝑗∑︁
∈𝑁𝑖
√︁ |𝑁
𝑖|1
√︁ |𝑁
𝑗|v𝑘 𝑗−1,
(3)
o pn ost th -te raf io nr im ngul gat ri ao pn h,w coe np vr oo lp uo tis oe nth ba at seL dig oh ntG oO rdD inE a- rya -dc io ffn et rin enu to iu als
-
v𝑘
𝑗
=v𝑘 𝑗−1+ 𝑖∑︁
∈𝑁𝑗
√︁ |𝑁
𝑗1
|√︁ |𝑁
𝑖|u𝑘 𝑖−1,
equationsaimingtoachievetheoptimaltrade-off.Finally,adetailed
timecomplexityanalysisandcomparisonwithotherstrongGCN whereu𝑘 𝑖−1andv𝑘 𝑗−1areembeddingsofuseru𝑖anditemv𝑗atlayer
baselinesaredemonstrated. of𝑘−1,respectively.Thenormalizationemploystheaveragedegree
√ 1√ totemperthemagnitudeofpopularnodesaftergraph
|𝑁𝑖| |𝑁𝑗|
3.1 Pre-trainingUser/ItemEmbedding convolution.Afterward,thecollaborativefilteringfinalembedding
Here,weoutlineouroveralltrainingpipelinetowardtheextremely isobtainedbysynthesizingthelayer-wiserepresentations:
efficientgraph-basedrecommendation.Sincethegraphconvolution 𝐾 𝐾
provedunnecessaryinthetrainingstageasSection2,weabandoned u 𝑖(𝐾) =∑︁ u𝑘 𝑖; v( 𝑗𝐾) =∑︁ v𝑘 𝑗. (4)
thegraphconvolution-relatedoperationsandfocusedsolelyon 𝑘=0 𝑘=0
trainingtherandomlyinitializedIDembeddings,asshowninthe
3.3 ContinuousGraph-ODE
trainingsectionofFigure4.Regardingthelosscomputation,we
directlyoptimizethealignmentanduniformitypropertiesasin Motivatedby[2,30,41]derivingcontinuousdifferentialequations
[32]toreachoptimalstatusforMFembeddingtrainingasanideal fromdiffusionprocesstomodelthedynamicsinthegraph,we
foundationforthesubsequentgraphconvolutionphase.Specifically, aimtodesignacontinuousversionofourdiscretenon-parametric
thealignmentlossminimizesthedistancebetweenthenormalized graphconvolutionwithself-loops.
embeddingsofthepositivepairs(u𝑖,v𝑗)withinbatchB: Formally, given h0 as the initial embedding of the users and
items,wecanrewritethelayer-wiseinformationupdateEquation3
L𝑎𝑙𝑖𝑔𝑛 = 1 ∑︁ (cid:13) (cid:13)u𝑖 −v𝑗(cid:13) (cid:13)2. (1) intermsofthematrixoperations:
|B| (u𝑖,v𝑗)∈B h𝑘 =Ah𝑘−1, (5)DoWeReallyNeedGraphConvolutionDuringTraining?
LightPost-TrainingGraph-ODEforEfficientRecommendation CIKM’24,October21–25,2024,Boise,ID,USA
Post-Training Graph Convolution
Training of Graph-based Models
Traditional GCN GCN with Self-loop LightGODE Why Self-loop?
Graph Construction Graph Encoder ED
Traditional GCN
Trainin Ing p P ur tocess Loss Computation SuD mis mcr ae tt ie on R Le as yc ea rsle C Ino tn et gin rau to iou ns Lower Discrepancy
0 1 2 3 k
... 0 1 2 3 k 0 1 2 3 k 2.8 t NDCW Ghy Cont Oin ptu imo au l s T? rade-off
k=0 k=1 k=2 Discrete t=0 t=2.8
Time k
LightGODE with Post-Training User Node Continuous
Graph Convolution Item Node Self-loop Time t 0 1 2 3 k
Figure4:ThetrainingpipelineoftraditionalGCN-basedrecommendationandourproposedLightGODEwithpost-traininggraph
convolution(PTGC)framework,whereweskipthetime-consumingconvolution-relatedoperationstospeedupthetraining.
InthePTGCstage,theself-loopprioritizestheshallowlayersbyweighingmoreonprecedinglayerrepresentations,thus
mitigatingthedistributiondiscrepancyproblem.Basedonthedesignofdiscretenon-parametricGCN,wederiveLightGODE,a
continuousODEfunctionthatimplementsfine-grainedgraphconvolutiontoachievetheoptimaltrade-offintheGCNdesign.
whereh𝑘 isthenodeembeddingsof𝑘-thlayer,aggregatingtheir computing2|E|non-zeroelementsoftheoriginaladjacencyma-
neighborhoodinformationandfusingwithitsownrepresentation trix.Onthecontrary,LightGODEalleviatestheneedforgraph
ofthepreviouslayerviaself-loop.ThematrixA=A¯ +IandA¯ is constructionandadjacencymatrixnormalizationintraining.
thenormalizedadjacencymatrix. • Inthegraphconvolutionstage,LightGCNandGraphAUboth
Consequently,theendresultfora𝐾-layerdiscretegraphconvo- performlinearmessage-passingthroughthegraph’sedgesin
lutionnetworkh(𝐾)canberepresentedas: eachlayer,whichincursacomputationalcostof2|E|𝐾𝑑.Whereas
𝐾 𝐾 LightGODEdoesnotinvolvegraphconvolutionintraining,sig-
h(𝐾)=∑︁ h𝑘 =∑︁ A𝑘 h0. (6) nificantlyfacilitatinglarge-scalegraphrecommendations.
𝑘=0 𝑘=0 • Regardingthelosscomputation,LightGCNadoptstheBPRloss
foroptimization,leadingtoacomputationaldemandof𝑂(2𝐵𝑑).
ThissumfromEquation6canbeseenasaRiemannsumextend-
ingfromlayer0tolayer𝐾 → ∞,transitioningtoacontinuous GraphAU,ontheotherhand,usesalignmentanduniformityloss
calculationsbetweenusersanditemsinthebatch,resultingin
ODEfunction(proofprovidedintheAppendixB):
atimecomplexityinthebatchof𝑂(2𝐾𝐵𝑑+2𝐵2𝑑).LightGODE,
dh(𝑡)
d𝑡
=lnAh(𝑡)+(A−lnA)h0, (7) f ho ac su asi tn img eon cl oy mo pn let xh ite ya pli eg rn bm ae tcn ht olo fs 𝑂s (a 𝐵t 𝑑th +e 2i 𝐵n 2it 𝑑i )a .l Ie tm shb oe ud ld din bg e,
whichsimplifiesunderafirst-orderTaylorexpansionapproxima- notedthatalltheexperimentsareimplementedonGPU-based
tionwherelnA=A−I=A¯,leadingto:
parallelcomputation,whichminimizestherelativeimportanceof
dh(𝑡) batchsize𝐵inmodelcomparisons.Furthermore,theBPRloss’s
d𝑡
=A¯h(𝑡)+h0. (8)
relianceonnegativesamplingforeachuser-itempairinevery
batchthroughallepochsmakesLightGCNlessefficientthan
Thegeneralformofthiscontinuousgraphconvolutionnetwork
LightGODEinhandlinglarge-scalegraphs.
isobtainedbyintegrationfromtheinitialconditionas:
∫ 𝑡
h(𝑡)=ℎ 0+ [A¯h(𝑠)+h0]d(𝑠). (9) Table 2: Time complexity comparison of LightGCN,
0 GraphAU,andLightGODEduringtraining.
Notethatthefinalintegrationformcouldbesolvedanalytically
usingtheintegrationfactor.However,consideringthatcomputing
Stages LightGCN GraphAU LightGODE
the exponential of the matrix in the analytical solution is time-
consuming,weresorttothesimpleandfastEulersolver[3]to Adjency 𝑂(2|E|) 𝑂(2|E|) -
approximatetheODEsolution. Matrix
Graph 𝑂(2|E|𝐾𝑑) 𝑂(2|E|𝐾𝑑) -
3.4 TimeComplexityAnalysis Convolution
Inthissubsection,weanalyzethecomputationcomplexityofLight- Loss 𝑂(2𝐵𝑑) 𝑂(2𝐾𝐵𝑑+2𝐵2𝑑) 𝑂(𝐵𝑑+2𝐵2𝑑)
GODEandcompareitwithtwoprominentGCNbenchmarkmeth- Computation
ods,LightGCN[8]andGraphAU[46].Wefirstdefinethenumber
ofedgesintheuser-itembipartitegraphas|E|.Then,let𝐾 rep-
4 EXPERIMENTS
resentthenumberofgraphconvolutionlayersand𝑑 thesizeof
embeddings.Onthisbasis,wecanderivethefollowingfacts: 4.1 Datasets
• Inthegraphconstructionprocess,bothLightGCNandGraphAU Weexperimentonthreepublicreal-worlddatasets:Gowalla,Amazon-
requirenormalizationoftheadjacencymatrix.Thisstepinvolves Beauty(Beauty),andAmazon-Toys-and-Games(Toys-and-Games),CIKM’24,October21–25,2024,Boise,ID,USA WeizhiZhang,LiangweiYang,ZiheSong,HenryPengZou,KeXu,LianchengFang,andPhilipS.Yu
Table3:PerformancecomparisononthreebenchmarkdatasetsintermsofNDCGandRecall.
Gowalla Beauty Toys-and-Games
Method
N@20 R@20 N@50 R@50 N@20 R@20 N@50 R@50 N@20 R@20 N@50 R@50
BiasMF 0.0406 0.0700 0.0507 0.1122 0.0428 0.0904 0.053 0.1404 0.0413 0.0826 0.0503 0.1271
NeuMF 0.0487 0.0952 0.0637 0.1597 0.0343 0.0746 0.043 0.1173 0.0301 0.0632 0.0375 0.0994
NGCF 0.0501 0.0923 0.0644 0.1535 0.0438 0.0943 0.0559 0.1537 0.0379 0.0827 0.0486 0.1356
DGCF 0.0553 0.0967 0.0692 0.1556 0.0516 0.1081 0.0624 0.1610 0.0485 0.1007 0.0589 0.1515
SimpleX 0.0451 0.0876 0.0611 0.1555 0.0502 0.1104 0.0623 0.1697 0.0521 0.1092 0.0632 0.1640
LightGCN 0.0683 0.1224 0.0860 0.1974 0.0581 0.1189 0.0709 0.1816 0.0555 0.1131 0.0669 0.1696
ODE-CF 0.0680 0.1220 0.0854 0.1960 0.0537 0.1158 0.0661 0.1760 0.0516 0.1075 0.0633 0.1656
DirectAU 0.0768 0.1437 0.0978 0.2319 0.0555 0.1149 0.0673 0.1725 0.0571 0.1184 0.0677 0.1714
GraphAU 0.0811 0.1461 0.1017 0.2346 0.0662 0.1398 0.0782 0.2116 0.0622 0.1324 0.0725 0.1952
LightGODE 0.0929 0.1678 0.1150 0.2628 0.0714 0.1452 0.0852 0.2130 0.0673 0.1371 0.0794 0.1983
varyinginscalesanddomains.TheGowalla1isalocation-based • Mostofthegraph-basedrecommendersystemsconsistentlyout-
socialnetworkingdatasetobtainedfromusers’checking-in.Beauty performthetraditionalMFmodels.Thissuggeststheimportance
andToys-and-Gamesarecrawledfromreal-worlddatainAmazon2 ofgraphconvolutionforcapturingthemulti-hopinformation.
accordingtotheproductcategory.Wefollowthe5-coresettingin Thoughleveragingcontrastivelearningloss,SimpleXperforms
[32,50]byremovingtheusers/itemswithnodedegreeslessthan poorlyinthecontextofsparsedatasetGowalla,whereasLight-
fivetoensurethedataqualityfortesting.Wesplitalldatasetsinto GCNandODE-CFaremorerobustacrossalldatasets.
training(80%),validation(10%),andtesting(10%),andthestatisti-
calinformationofthethreedatasetsafterfilteringissummarized 4.3 AblationStudy
inTable4.Moredetailsaboutimplementations,evaluations,and
AblationstudiesonLightGODEareconductedtovalidatethera-
baselinecanbefoundinAppendixA.
tionalityandeffectivenessofourdesignchoices.FromTable(5),it
isevidentthatthefullversionoftheLightGODEmodelachieves
Table4:Thestatisticsofthedatasets. thebestscoresacrossallmetricsanddatasets,showcasingtheeffi-
cacyofthecontinuousODEfunction.Furthermore,usingonlyour
parameter-freegraphconvolutionwithself-loop(w/oODE)still
Dataset #Users #Items #Interactions Sparsity
resultsinhigherNDCGandRecallandlowerembeddingdiscrep-
Gowalla 64,116 164,533 2,018,421 99.9809%
ancycomparedtotraditionallightweightgraphconvolution(w/o
Beauty 22,364 12,102 198,502 99.9267%
SL),indicatingmoreconsistentembeddings.Themodelwithout
Toys-and-Games 19,413 11,925 167,597 99.9276%
post-traininggraphconvolution(w/oConv)exhibitsthelowest
performance.Therefore,eachcomponentwithinourLightGODE
contributessignificantlytothefinalrecommendationperformance.
4.2 OverallPerformanceComparison
Inthiscomprehensiveexperiment,wecomparetheperformance
Table5:Abalationstudyondifferentcomponents.Theem-
ofseveralstate-of-the-artrecommendationalgorithmsonthree
beddingdiscrepancy(ED)istheEuclideandistancebetween
diverse datasets: Gowalla, Beauty, and Toys-and-Games, using
initialandconvolutionembeddings;thelowerthebetter.
NDCG20,NDCG@50,Recall@20,andRecall@50.Here,wehigh-
lightthemainobservationsasfollows:
• Noticeably,LightGODEachievesthehighestscoresinNDCG Dataset Metrics Light- w/o w/o w/o
GODE ODE SL Conv
andRecallacrossalldatasets,demonstratingitseffectivenessin
differentrecommendationtasks.Itshouldhighlightedthatinthe NDCG 0.0929 0.0833 0.0801 0.0768
Gowalla
large-scaledatasetGowalla(with64,116usersand164,533items), Recall 0.1678 0.1537 0.1481 0.1437
LightGODEsurpassesalltheotherbaselinemethodsbylarge ED 0.0066 0.0158 0.0282 -
marginswithmorethan10%improvementoverthestrongest NDCG 0.0714 0.0700 0.0686 0.0555
Beauty
baseline,emphasizingitspotentialtobedeployedinthelarge- Recall 0.1452 0.1450 0.1428 0.1149
scalegraphsinreal-worldapplications. ED 0.0022 0.0049 0.0082 -
• Amongall,DirectAUandGraphAUemergeasthemostcompeti-
Toys-and- NDCG 0.0673 0.0644 0.0641 0.0571
tivebaselinesinallthreedatasets,demonstratingtheeffective- Games Recall 0.1371 0.1343 0.1337 0.1184
nessofthealignmentanduniformity[34]inoptimization. ED 0.0011 0.0045 0.0067 -
1https://snap.stanford.edu/data/loc-gowalla.html
2https://jmcauley.ucsd.edu/data/amazon/links.htmlDoWeReallyNeedGraphConvolutionDuringTraining?
LightPost-TrainingGraph-ODEforEfficientRecommendation CIKM’24,October21–25,2024,Boise,ID,USA
4.4 EfficiencyAnalysis Table6:TrainingtimecomparisonofGCN-basedmodelson
Gowalla,Beauty,andToys-and-Gamesdatasets.Itincludes
theaveragetrainingtimeperepoch,thenumberofepochs,
Performance vs Efficiency andthetotaltrainingtime.Forabbreviation,wedenotesec-
0.10
LightGODE ondsass,minutesasm,andhoursash.
0.09
GraphAU Dataset Method Time/Epoch #Epochs TotalTime
0.08 DirectAU
NGCF 1175.79s 84 27.44h
0.07 ODECF DGCF 6720.88s 43 80.28h
LightGCN
LightGCN 608.69s 105 17.75h
0.06 DGCF Gowalla ODECF 679.25s 79 14.91h
0.05 NeuMF NGCF GraphAU 597.11s 91 15.09h
SimpleX LightGODE 58.46s 61 0.99h
BiasMF
0.04 NGCF 10.79s 48 8.63m
DGCF 204.95s 72 245.94m
102 103 104
LightGCN 6.76s 83 9.35m
Average Trainig Time per Epoch (Seconds) Beauty
ODECF 8.34s 108 15.01m
GraphAU 8.72s 41 5.96m
Figure5:Trade-offbetweentheperformanceandtheeffi- LightGODE 3.16s 68 3.58m
ciencyontheGowalladataset.Theleftupperdirectionindi-
NGCF 11.19s 57 10.63m
catesstrongerperformanceandmoreefficienttraining.
DGCF 116.69s 61 118.64m
Toys-and LightGCN 5.01s 119 9.94m
4.4.1 Trade-offbetweenthePerformanceandtheefficiency. -Games ODECF 6.60s 147 16.17m
Figure 5 illustrates the overall comparison of performance and GraphAU 7.09s 41 4.84m
efficiencyonthelargeGowalladataset.LightGODEmarkedlyout- LightGODE 2.65s 76 3.36m
performsallbenchmarkswhilemaintaininghighefficiency,un-
derscoringitspotentialforeffective,large-scalerecommendation
systems.EarlyworksthatleverageGCNencoders,suchasNGCF Gowalla Beauty Toys-and-Games
0.07
andDGCF,fallbehindinaveragetrainingtimesperepochand
0.06
0.08
NDCG.MoreadvancedGCN-basedapproaches,includingLight- 0.06
GCN,ODECF,andGraphAU,showsubstantialimprovementsin 0.07 0.05
0.05
NDCGscoresyetarestillmuchslowerthansimplerMFmodelsin 0.06 0.04
speed.Conversely,BiasMF,NeuMF,andSimpleXdirectlyutilize 0.04
0.05 0.03
user-iteminteractions,achievingnotablylowtrainingtimesbut NGCF 0.03 NGCF NGCF
DGCF DGCF DGCF
exhibitingpoorrankingscores.OnlyDirectAUmanagesabalanced 0.04 LightGCN LightGCN 0.02 LightGCN
ODECF 0.02 ODECF ODECF
trade-offbutstilllagsbehindLihgtGODEregardingNDCG. 0.03 GraphAU GraphAU GraphAU
LightGODE LightGODE 0.01 LightGODE
0.01
4.4.2 TrainingTimeComparison. Todelvedeeperintotheeffi- 0 20 40 0 20 40 0 20 40
Epochs Epochs Epochs
ciencyandscalabilityanalysisintermsofthetraining,weprovide
acomprehensivetrainingtimecomparisonfeaturingtheaverage
Figure6:Performancecurveinthefirst40epochs.
timeperepoch,thenumberofrequiredepochs,andthetotaltrain-
ingcostshowninTable6.OnthelargedatasetGowalla,NGCFand
DGCFconsumelongertimesperepochfortraining,takingtensof
performancesofGraphAUandLightGODEensureconvergenceat
hoursintotaltoreachtheoptimalstatus.LightGCNandODECF
earlytrainingstages.Ourmethodrequiresfewerepochstocon-
demonstrateshorterepochdurationbutdemandagreaternumber
vergeandconsistentlyresultsinhighrecommendationscores.
ofepochstocompletetraining.AlthoughGraphAUexhibitsthe
fastesttrainingspeedperepochamongthebaselinemethodsin 4.5 ComparisonwithGODE
theBeautyandToys-and-Gamesdataset,itisalmostasslowas
ToevaluatetheeffectivenessofourcontinuousODEfunctionand
LightGCNandODECF,especiallyonthelargeGowalladataset.
self-loopoperationstailoredforpost-traininggraphconvolution,
LightGODEsignificantlyreducestheoveralltrainingtimetoless
wecompareLightGODEwithpost-traininggraphconvolutionand
thanonehouronGowalla.Theseobservationshighlighttheeffi-
GODEwithpre-traininggraphconvolution.LightGODEconsis-
ciencyandscalabilityofLightGODEtowardsindustrialRecSys.
tentlyemergesasthesuperiorperformeracrossallmetricsinall
4.4.3 PerformanceCurveandConvergenceSpeed. InFigure threedatasets,especiallyinthelarge-scaledatasetGowalla.This
6,wepresentthetrainingcurvesofperformanceagainstepochson demonstratesthatourinnovativedesignforpost-traininggraph
thetreedatasets.Overall,NGCFandDGCFexhibitlow-performance convolutionnotonlyallowsoursimplemodeltoexceedtheperfor-
peaks,whileLightGCNandODECFconvergeslowly.Byenforcing manceofGCNmodelstrainedwithtraditionalgraphconvolution
alignmentanduniformityintherepresentationhyperspace,the butalsosignificantlyspeedsupthetrainingstrategy.
GCDN
GCDN GCDN GCDNCIKM’24,October21–25,2024,Boise,ID,USA WeizhiZhang,LiangweiYang,ZiheSong,HenryPengZou,KeXu,LianchengFang,andPhilipS.Yu
0.12 connectivityintheuser-itembipartitegraph.Anotherearlywork,
GODE 0.20 GODE
0.10 LightGODE LightGODE PinSAGE[47],utilizesrandom-walktosamplesubgraphsandscales
0.08 0.15 upRecSysindustriallevel.DGCF[37]disentangleslatentintentions
0.06 0.10 ofusersanddiversifiesitemrecommendations,thusyieldingbetter
0.04 performanceandinterpretability.LightGCN[8]isalightweight
0.05
0.02
frameworkthatomitslineartransformationsandnonlinearactiva-
0.00 Gowalla Beauty Toy Gs- aa mn ed- s 0.00 Gowalla Beauty Toy Gs- aa mn ed- s t ri eo cn os min mG enC dN atl ia oy ne .r Ts oa fn ud rtd hr ea rst si ic mal ply lifi ym tp hr eo gv re as pt hhe tre affi inc ii ne gnc py roo cf eg ssr ,a Uph
l-
Figure7:PerformDaatnac ReaCtioomparisonofLigDhattGa ORaDtiEoandGODE. traGCN[24]adjuststherelativeimportanceofnodestoaggregate
the embeddings by weights and directly approximates the con-
4.6 HyperparameterAnalysis vergedstateofmessagepassing,whichacceleratesLightGCNby
morethanmultipletimes.GraphAU[46]identifiestheinefficiency
4.6.1 ImpactoftheTime𝑡. Weevaluatehowthecontinuous
ofDirectAU[32]ongraph-basedrecommendationsandproposes
time𝑡 affectstheperformanceofLightGODE.AsobservedinFig-
high-orderrepresentationalignmentforthelinearscaleofcompu-
ure8,itsuggeststhatanappropriatetime𝑡 isgenerallyassociated
tationforadditionallayers.ODECF[43]condensesmultipleGCN
withthescaleofthedatasets.InGowalla,performancepeakedat
layersintoonecontinuouslayer,whichiscapableofleveraging
around3asalargertime𝑡enablesthemodeltolong-distanceneigh-
fastODEsolversandimprovingbothperformanceandefficiency.
borhoodaggregations.WhereasinBeautyandToys-and-Games,
Allpreviousworkaimstoimprovethetrainingefficiencyfrom
𝑡 ischosenat1.8and0.8,indicatingasmallerreceptivefieldto
thegraphconvolutionprocess,whereasweinnovativelychallenge
achievetheoptimalconvolutiondepth.
thenecessityoftime-intensivegraphconvolutionandproposethe
extremelyefficientpost-traininggraphconvolutionframework.
Gowalla Beauty Toys-and-Games
0.168 0.138
0.167 0.145 0.137 5.2 GraphOrdinaryDifferentialEquation
0.166 0.144 0.136
0.165 0.143 0.135 Neuralordinary-differential-equations(NODE)[4]proposeanew
0.164 0.142 0.134
0.163 0.141 0.133 paradigmthatmodelscontinuousdynamicsthroughthederiva-
0.162 0.50.81.01.21.51.82.02.22.53.03.55.0 0.14 0.50.81.01.21.51.82.02.22.53.03.55.0 0.132 0.50.81.01.21.51.82.02.22.53.03.55.0 tiveoftheneuralnetwork’shiddenstate.Motivatedbythis,graph
t t t
ordinary-differential-equations(GDE)[27]combinestheconcepts
Figure8:Impactthetime𝑡 onRecall.
withGCNanddirectlytreatstheGCNlayerasacontinuousvector
field.Derivedfromthediffusionprocess,continuousgraphmodel
4.6.2 Impactoftheuniformityweight𝛾. Anotherhyperpa- CGNN[41]characterizesthedynamicsofnoderepresentations
rameteristheweightoftheuniformityloss𝛾.Fromthecurvesin usingacontinuousmessage-passinglayer.Concurrently,inthe
continuoustimedata,ODEwithagraphencoder[11,12,21]arede-
Figure9,asmalleruniformityweight(0.5)achievesthehighestre-
velopedformodelinginteractingdynamics.Incomparison,instead
callontheBeautyandToys-and-Gamesdatasets.Incontrast,larger
valuesof𝛾 aredetrimentaltotherecommendationperformance ofusingadeepneuralnetworktoparametrizetheODEderiva-
tive,wederivethecontinuousgraphODEbasedonthediscrete
inGowalladatasets.Asforlarge-scaledatasets,theuseranditem
non-parametricgraphconvolutionforefficientrecommendation.
representationsshouldbemoreevenlydistributedsoastomake
theuser/itemembeddingsmorerepresentativefordistinction.
6 CONCLUSION
In this study, we critically challenge the conventional reliance
Gowalla Beauty Toys-and-Games
0.18 0.16 0.16
0.16 0.14 0.14 ongraphconvolutioninthetrainingofgraphRecSysbydemon-
0.14 0.12 0.12 stratingthattheirprimarybenefitsarerealizedduringthetest-
0.12
0.1 0.1 0.1 ingphase.WeproposetheLightPost-TrainingGraph-ODE(Light-
0.08 0.08 0.08
GODE),whichinnovativelyskipstraditionalresource-heavycon-
0.060.20.5 1 2 5 10 15 20 0.060.20.5 1 2 5 10 15 20 0.060.20.5 1 2 5 10 15 20
volutionprocesses,anddeviseanovelcontinuousgraphordinary-
Figure9:Impactoftheuniformityweight𝛾 onRecall. differentialequationmodeltomitigatetheembeddingdiscrepancy
foroptimalconvolutiondepth.Ourempiricalevaluationsacross
three real-world datasets, especially on the large-scale dataset
5 RELATEDWORK Gowalla,showthatLightGODEsignificantlyoutperformstradi-
5.1 GraphConvolutionNetworkforRecSys tionalCFmodelsinbothrecommendationperformanceandcompu-
tationalefficiency.Thisworknotonlyquestionsexistingtraining
Collaborativefiltering(CF)iswidelyusedtoproviderecommen-
paradigmsbutalsopinpointspotentiallynewresearchdirections
dationsbasedonuser-iteminteractions.Recentdevelopmentsin
forefficientandlarge-scalegraphRecSys.
graphconvolutionnetworks(GCNs)havereformedCFfromcon-
ventionalmatrixfactorizationCFtoGraph-basedCF,incorporating ACKNOWLEDGMENTS
socialnetworks[5,20,45],knowledgegraph[1,19,35],anduser-
ThisworkissupportedinpartbyNSFundergrantsIII-2106758,
iteminteractions[8,24,36,37,44,47,49].Oneoftheearlyattempts
andPOSE-2346158
isNGCF[36],whichincorporatestheimportanceofhigh-order
02@GCDN 02@llaceRDoWeReallyNeedGraphConvolutionDuringTraining?
LightPost-TrainingGraph-ODEforEfficientRecommendation CIKM’24,October21–25,2024,Boise,ID,USA
A EXPERIMENTALSETUP Differentiatingthis,wehave:
A.1 Baselines dh d( 𝑡𝑡) =A𝑡+1h0.
(12)
• BiasMF[17]isamatrixfactorizationtechniquethatintegrates
biasvectorsforbothusersanditemsforenhancedprediction.
GiventhechallengesincomputingA𝑡+1fornon-integervalues
• NeuMF[9]leveragesdeepneuralnetworkstomodelthecom- oft,itisreformulatedintoanODEusingthesecondderivative:
•
p Nl Gex Ca Fn [d 36n ]o in n- tl ri on de uar cei sn Gte Cra Nct mio on ds eb lse wtw ite he cn ou lls ae br os ra an tid veit fie lm tes r.
ing
d2 dh 𝑡( 2𝑡) =lnAA𝑡+1h0=lnAdh d( 𝑡𝑡)
(13)
toexploittherichuser-iteminteractiongraphstructure.
TheODEintegratesto:
• DGCF [37] utilizes a disentangled representation learning
approachtoexploitdistinctfactorsofuser-iteminteractions. dh(𝑡)
=lnAh(𝑡)+𝑋 𝑐𝑜𝑛𝑠𝑡, (14)
• SimpleX[23]proposeanovelcosinecontrastivelossfunction d𝑡
tobeintegratedwithsimplecollaborativefilteringmodels. Applying𝑡 =0toEquation12andEquation14gives:
•
•
L m Oi Deg n Eh d Ct aG FtiC o [4nN 3v ][ i8 pa] ro es b si em v nip a tstl ii anfi ge ns et uhth re ae c loG OmC DpN El -ea bxr ac n sh eo di nte - mlc i ot nu der eae lr tf o ho p ar e ta r ca ar t ne ioc so n km . ip- dh d( 𝑡𝑡)(cid:12) (cid:12) (cid:12) (cid:12)𝑡=0=Ah0=lnAh0+𝑋 𝑐𝑜𝑛𝑠𝑡. (15)
Therefore,
multipleGCNlayerstoreachthefinalrepresentation.
• DirectAU[32]explorestodirectlyoptimizethealignmentand 𝑋 𝑐𝑜𝑛𝑠𝑡 =(A−lnA)h0, (16)
uniformityoflatentrepresentationsincollaborativefiltering. resultingintheODEformulationforthegraphconvolutionas:
• GraphAU[46]extendthealignmentlosslayer-wiseandtailor dh(𝑡)
forgraphencodersforefficientgraphrecommendation. d𝑡
=lnAh(𝑡)+(A−lnA)h0. (17)
C ANALYSISONTHEALIGNMENTFORCE
A.2 EvaluationMetrics
DefinitionC.1(PerfectAlignment). Apairofobserveduser-item
For evaluating performance metrics, we use NDCG@K and Re-
call@Ktoensureafaircomparisonamongallbaselinemethods
pairisperfectlyalignedif𝑒
𝑢
=𝑒
𝑣
and(𝑢,𝑣)∼𝑃
𝑝𝑜𝑠
inthetop-Krecommendationtasks.Inallexperiments,Kissetto Tosimplifythederivationprocess,weconsiderthegivenuser-
20bydefaultunlessspecified.Weemploythefull-rankingstrat- itempair(𝑢,𝑣)perfectlyaligned,andthenumberofusersisless
egy[51]forallexperiments,meaningthatallcandidateitemsnot thanthenumberofitems.Thelowerboundofthealignmentforce
previouslyinteractedbytheuserwillberankedduringtesting. fortheMFmodelisheldas:
∑︁ ∑︁
A.3 ImplementationDetails
L𝑎𝑙𝑖𝑔𝑛−𝑚𝑓 =∥𝑒 𝑢0−𝑒 𝑣0∥2+ ∥𝑒 𝑖0−𝑒 𝑢0∥2+ ∥𝑒 𝑣0−𝑒0 𝑗∥2
𝑖∈𝑁𝑢 𝑗∈𝑁𝑣
OurimplementationofLightGODEandallbaselinemodelsare ∑︁ ∑︁
carriedoutusingRecBole[52].Forthebaselinetraining,wemeticu-
≥ ∥𝑒 𝑖0−𝑒 𝑢0∥2+ ∥𝑒 𝑣0−𝑒0 𝑗∥2
louslysearchtheirhyperparametersforvariousdatasetsfollowing
𝑖∈𝑁𝑢 𝑗∈𝑁𝑣
(cid:13) (cid:13)2
respectiveoriginalpaperstoensureafaircomparison.Thebatch (cid:13) (cid:13)
sizeandtheembeddingsizearestandardizedat256and64,respec- ≥(cid:13) (cid:13)∑︁ (𝑒 𝑖0−𝑒 𝑢0)+ ∑︁ (𝑒 𝑣0−𝑒0 𝑗)(cid:13) (cid:13)
(cid:13) (cid:13)
tively.AllmodelsemploytheAdamoptimizer[15],withalearning (cid:13)𝑖∈𝑁𝑢 𝑗∈𝑁𝑣 (cid:13)
ratesetat1e-3.Topreventoverfitting,weutilizeanearlystopping (cid:13) (cid:13)2
(cid:13) (cid:13)
mechanismthatterminatestrainingifthereisaconsistentdecline ≥(cid:13) (cid:13)∑︁ 𝑒 𝑖0− ∑︁ 𝑒0 𝑗(cid:13)
(cid:13)
,
intheperformancemetricNGCG@20over10epochs.Specifically, (cid:13) (cid:13)
forourmethodLightGODE,wetunetheuniformityweight𝛾within
(cid:13)𝑖∈𝑁𝑢 𝑗∈𝑁𝑣 (cid:13)
(18)
theset[0.2,0.5,1,2,5,10,15,20]andsearchtime𝑡 intherange where𝑒 𝑢0 and𝑒 𝑣0 representstheinitialembeddingofuser𝑢 and
of[0.5,0.8,1.0,1.2,1.5,1.8,2.0,2.2,2.5,3.0,3.5,5.0]foroptimal item𝑣,with𝑖and𝑗 beingtheirneighboringnodes.Assumingthe
performance.Tomaintainimpartialityinourefficiencyevaluations,
GCNmodelemployslightconvolutionasin[8]forneighborhood
eachmodelistrainedindependentlyusingasingleGPU.
aggregation,thealignmentforcefora1-layergraphconvolution
fortheuser-itempair(𝑢,𝑣)canbedescribedas:
B DERIVATIONOFCONTINUOUSODE
OnecanviewfinalrepresentationsinEquation6asaRiemannsum:
L𝑎𝑙𝑖𝑔𝑛−𝑔𝑐𝑛 =(cid:13) (cid:13)𝑒 𝑢1−𝑒 𝑣1(cid:13) (cid:13)2
h(𝐾) =𝐾 𝑘∑︁ =+ 11 A(𝑘−1)Δ𝑡ℎ 0Δ𝑡, (10) =(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)𝑖∑︁
∈𝑁𝑢
√︁ |𝑁 𝑢𝑒 |𝑖0 √︁ |𝑁 𝑖| − 𝑗∑︁
∈𝑁𝑣
√︁ |𝑁 𝑢𝑒 |0 𝑗 √︁ |𝑁 𝑖|(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 . (19)
withΔ𝑡 = 𝐾𝑡+ +1 1.Inthisdiscretesetup,𝑡 = 𝐾 andthusΔ𝑡 = 1for Therefore,comparingEquation18andEquation19,thealignment
thediscretegraphconvolutionnetworks.Nowlet𝐾 → ∞,the force applied on surrounding neighbors of positive pairs using
equationtransitionstoitscontinuousform:. graphconvolutionistheweightedversionofthedirectalignment
∫ 𝑡+1 forceappliedontwoclusteringsofnodes.
h(𝑡)= Ash0d𝑠. (11)
0CIKM’24,October21–25,2024,Boise,ID,USA WeizhiZhang,LiangweiYang,ZiheSong,HenryPengZou,KeXu,LianchengFang,andPhilipS.Yu
REFERENCES
[26] MichaelJPazzaniandDanielBillsus.2007. Content-basedrecommendation
[1] YixinCao,XiangWang,XiangnanHe,ZikunHu,andTat-SengChua.2019. systems. InTheadaptiveweb:methodsandstrategiesofwebpersonalization.
Unifyingknowledgegraphlearningandrecommendation:Towardsabetter Springer,325–341.
understandingofuserpreferences.InTheworldwidewebconference.151–161. [27] MichaelPoli,StefanoMassaroli,JunyoungPark,AtsushiYamashita,Hajime
[2] BenChamberlain,JamesRowbottom,MariaIGorinova,MichaelBronstein,Stefan Asama,andJinkyooPark.2021.GraphNeuralOrdinaryDifferentialEquations.
Webb,andEmanueleRossi.2021.Grand:Graphneuraldiffusion.InInternational arXiv. https://doi.org/10.48550/arXiv.1911.07532arXiv:1911.07532[cs,stat].
ConferenceonMachineLearning.PMLR,1407–1418. [28] TKonstantinRusch,MichaelMBronstein,andSiddharthaMishra.2023.Asurvey
[3] RickyT.Q.Chen,YuliaRubanova,JesseBettencourt,andDavidDuvenaud. onoversmoothingingraphneuralnetworks. arXivpreprintarXiv:2303.10993
2018.NeuralOrdinaryDifferentialEquations.AdvancesinNeuralInformation (2023).
ProcessingSystems(2018). [29] PoonamBThorat,RajeshwariMGoudar,andSunitaBarve.2015.Surveyoncol-
[4] RickyT.Q.Chen,YuliaRubanova,JesseBettencourt,andDavidDuvenaud. laborativefiltering,content-basedfilteringandhybridrecommendationsystem.
2018.NeuralOrdinaryDifferentialEquations.AdvancesinNeuralInformation InternationalJournalofComputerApplications110,4(2015),31–36.
ProcessingSystems(2018),6571–6583. [30] MatthewThorpe,TanMinhNguyen,HeidiXia,ThomasStrohmer,Andrea
[5] WenqiFan,YaoMa,QingLi,YuanHe,EricZhao,JiliangTang,andDaweiYin. Bertozzi,StanleyOsher,andBaoWang.2022. GRAND++:Graphneuraldif-
2019.Graphneuralnetworksforsocialrecommendation.InTheworldwideweb fusionwithasourceterm.InInternationalConferenceonLearningRepresentation
conference.417–426. (ICLR).
[6] IgorGoldenbergandGeoffreyIWebb.2019. Surveyofdistancemeasuresfor [31] AaronVandenOord,SanderDieleman,andBenjaminSchrauwen.2013.Deep
quantifyingconceptdriftandshiftinnumericdata.KnowledgeandInformation content-basedmusicrecommendation.Advancesinneuralinformationprocessing
Systems60,2(2019),591–615. systems26(2013).
[7] XiaotianHan,TongZhao,YozenLiu,XiaHu,andNeilShah.2023. MLPInit: [32] ChenyangWang,YuanqingYu,WeizhiMa,MinZhang,ChongChen,YiqunLiu,
EmbarrassinglySimpleGNNTrainingAccelerationwithMLPInitialization.In andShaopingMa.2022.Towardsrepresentationalignmentanduniformityin
TheEleventhInternationalConferenceonLearningRepresentations. collaborativefiltering.InProceedingsofthe28thACMSIGKDDconferenceon
[8] XiangnanHe,KuanDeng,XiangWang,YanLi,YongdongZhang,andMeng knowledgediscoveryanddatamining.1816–1825.
Wang.2020.Lightgcn:Simplifyingandpoweringgraphconvolutionnetworkfor [33] JianlingWang,RaphaelLouca,DianeHu,CaitlinCellier,JamesCaverlee,and
recommendation.InProceedingsofthe43rdInternationalACMSIGIRconference LiangjieHong.2020.TimetoShopforValentine’sDay:ShoppingOccasionsand
onresearchanddevelopmentinInformationRetrieval.639–648. SequentialRecommendationinE-commerce.InProceedingsofthe13thInterna-
[9] XiangnanHe,LiziLiao,HanwangZhang,LiqiangNie,XiaHu,andTat-Seng tionalConferenceonWebSearchandDataMining.645–653.
Chua.2017.Neuralcollaborativefiltering.InProceedingsofthe26thinternational [34] TongzhouWangandPhillipIsola.2020.Understandingcontrastiverepresentation
conferenceonworldwideweb.173–182. learningthroughalignmentanduniformityonthehypersphere.InInternational
[10] YangHu,HaoxuanYou,ZhecanWang,ZhichengWang,ErjinZhou,andYue conferenceonmachinelearning.PMLR,9929–9939.
Gao.2021.Graph-mlp:Nodeclassificationwithoutmessagepassingingraph. [35] XiangWang,XiangnanHe,YixinCao,MengLiu,andTat-SengChua.2019.Kgat:
arXivpreprintarXiv:2106.04051(2021). Knowledgegraphattentionnetworkforrecommendation.InProceedingsofthe
[11] ZijieHuang,YizhouSun,andWeiWang.2020. Learningcontinuoussystem 25thACMSIGKDDinternationalconferenceonknowledgediscovery&datamining.
dynamicsfromirregularly-sampledpartialobservations. AdvancesinNeural 950–958.
InformationProcessingSystems33(2020),16177–16187. [36] XiangWang,XiangnanHe,MengWang,FuliFeng,andTat-SengChua.2019.
[12] ZijieHuang,YizhouSun,andWeiWang.2021.Coupledgraphodeforlearning Neuralgraphcollaborativefiltering.InProceedingsofthe42ndinternationalACM
interactingsystemdynamics.InProceedingsofthe27thACMSIGKDDconference SIGIRconferenceonResearchanddevelopmentinInformationRetrieval.165–174.
onknowledgediscovery&datamining.705–715. [37] XiangWang,HongyeJin,AnZhang,XiangnanHe,TongXu,andTat-Seng
[13] HyunwooHwangbo,YangSokKim,andKyungJinCha.2018.Recommendation Chua.2020.Disentangledgraphcollaborativefiltering.InProceedingsofthe43rd
systemdevelopmentforfashionretaile-commerce.ElectronicCommerceResearch internationalACMSIGIRconferenceonresearchanddevelopmentininformation
andApplications28(2018),94–101. retrieval.1001–1010.
[14] MohsenJamaliandMartinEster.2010.Amatrixfactorizationtechniquewith [38] WeiWei,ChaoHuang,LianghaoXia,andChuxuZhang.2023. Multi-Modal
trustpropagationforrecommendationinsocialnetworks.InProceedingsofthe Self-SupervisedLearningforRecommendation.InProceedingsoftheACMWeb
fourthACMconferenceonRecommendersystems.135–142. Conference2023.790–800.
[15] DiederikPKingmaandJimmyBa.2014.Adam:Amethodforstochasticopti- [39] FelixWu,AmauriSouza,TianyiZhang,ChristopherFifty,TaoYu,andKilian
mization.arXivpreprintarXiv:1412.6980(2014). Weinberger.2019.Simplifyinggraphconvolutionalnetworks.InInternational
[16] ThomasNKipfandMaxWelling.2016. Semi-SupervisedClassificationwith conferenceonmachinelearning.PMLR,6861–6871.
GraphConvolutionalNetworks.InInternationalConferenceonLearningRepre- [40] ShiwenWu,FeiSun,WentaoZhang,XuXie,andBinCui.2022.Graphneural
sentations. networksinrecommendersystems:asurvey.Comput.Surveys55,5(2022),1–37.
[17] YehudaKoren,RobertBell,andChrisVolinsky.2009.Matrixfactorizationtech- [41] Louis-PascalXhonneux,MengQu,andJianTang.2020.Continuousgraphneural
niquesforrecommendersystems.Computer42,8(2009),30–37. networks.InInternationalconferenceonmachinelearning.PMLR,10432–10441.
[18] YehudaKoren,SteffenRendle,andRobertBell.2021.Advancesincollaborative [42] KeyuluXu,WeihuaHu,JureLeskovec,andStefanieJegelka.2018.HowPowerful
filtering.Recommendersystemshandbook(2021),91–142. areGraphNeuralNetworks?.InInternationalConferenceonLearningRepresenta-
[19] XiaolongLiu,LiangweiYang,ZhiweiLiu,MingdaiYang,ChenWang,HaoPeng, tions.
andPhilipSYu.2024. KnowledgeGraphContext-EnhancedDiversifiedRec- [43] KeXu,YuanjieZhu,WeizhiZhang,andSYuPhilip.2023.GraphNeuralOrdinary
ommendation.InProceedingsofthe17thACMInternationalConferenceonWeb DifferentialEquations-basedmethodforCollaborativeFiltering.In2023IEEE
SearchandDataMining.462–471. InternationalConferenceonDataMining(ICDM).IEEE,1445–1450.
[20] ZhiweiLiu,LiangweiYang,ZiweiFan,HaoPeng,andPhilipSYu.2022.Feder- [44] ChenxiaoYang,QitianWu,JiahuaWang,andJunchiYan.2023.GraphNeural
atedsocialrecommendationwithgraphneuralnetwork.ACMTransactionson NetworksareInherentlyGoodGeneralizers:InsightsbyBridgingGNNsand
IntelligentSystemsandTechnology(TIST)13,4(2022),1–24. MLPs.InTheEleventhInternationalConferenceonLearningRepresentations.
[21] XiaoLuo,JingyangYuan,ZijieHuang,HuiyuJiang,YifangQin,WeiJu,Ming [45] LiangweiYang,ZhiweiLiu,YingtongDou,JingMa,andPhilipSYu.2021.Con-
Zhang,andYizhouSun.2023. Hope:High-ordergraphodeformodelingin- sisrec:Enhancinggnnforsocialrecommendationviaconsistentneighboraggre-
teractingdynamics.InInternationalConferenceonMachineLearning.PMLR, gation.InProceedingsofthe44thinternationalACMSIGIRconferenceonResearch
23124–23139. anddevelopmentininformationretrieval.2141–2145.
[22] JingMa,LiangweiYang,QiongFeng,WeizhiZhang,andPhilipSYu.2023.Graph- [46] LiangweiYang,ZhiweiLiu,ChenWang,MingdaiYang,XiaolongLiu,JingMa,and
basedVillageLevelPovertyIdentification.InProceedingsoftheACMWebCon- PhilipSYu.2023.Graph-basedalignmentanduniformityforrecommendation.
ference2023.4115–4119. InProceedingsofthe32ndACMInternationalConferenceonInformationand
[23] KelongMao,JiemingZhu,JinpengWang,QuanyuDai,ZhenhuaDong,XiXiao, KnowledgeManagement.4395–4399.
andXiuqiangHe.2021.SimpleX:Asimpleandstrongbaselineforcollaborative [47] RexYing,RuiningHe,KaifengChen,PongEksombatchai,WilliamLHamilton,
filtering.InProceedingsofthe30thACMInternationalConferenceonInformation andJureLeskovec.2018. Graphconvolutionalneuralnetworksforweb-scale
&KnowledgeManagement.1243–1252. recommendersystems.InProceedingsofthe24thACMSIGKDDinternational
[24] KelongMao,JiemingZhu,XiXiao,BiaoLu,ZhaoweiWang,andXiuqiangHe. conferenceonknowledgediscovery&datamining.974–983.
2021.UltraGCN:ultrasimplificationofgraphconvolutionalnetworksforrecom- [48] ShichangZhang,YozenLiu,YizhouSun,andNeilShah.2021.Graph-lessNeural
mendation.InProceedingsofthe30thACMinternationalconferenceoninformation Networks:TeachingOldMLPsNewTricksViaDistillation.InInternational
&knowledgemanagement.1253–1262. ConferenceonLearningRepresentations.
[25] AndriyMnihandRussRSalakhutdinov.2007.Probabilisticmatrixfactorization. [49] WeizhiZhang,LiangweiYang,YuweiCao,KeXu,YuanjieZhu,andSYuPhilip.
Advancesinneuralinformationprocessingsystems20(2007). 2023.Dual-TeacherKnowledgeDistillationforStrictCold-StartRecommendation.
In2023IEEEInternationalConferenceonBigData(BigData).IEEE,483–492.DoWeReallyNeedGraphConvolutionDuringTraining?
LightPost-TrainingGraph-ODEforEfficientRecommendation CIKM’24,October21–25,2024,Boise,ID,USA
[50] WeizhiZhang,LiangweiYang,ZiheSong,HenryPengZou,KeXu,Yuanjie Information&KnowledgeManagement.2329–2332.
Zhu,andPhilipSYu.2024.MixedSupervisedGraphContrastiveLearningfor [52] WayneXinZhao,YupengHou,XingyuPan,ChenYang,ZeyuZhang,ZihanLin,
Recommendation.arXivpreprintarXiv:2404.15954(2024). JingsenZhang,ShuqingBian,JiakaiTang,WenqiSun,YushuoChen,LanlingXu,
[51] WayneXinZhao,JunhuaChen,PengfeiWang,QiGu,andJi-RongWen.2020. GaoweiZhang,ZhenTian,ChangxinTian,ShanleiMu,XinyanFan,XuChen,and
Revisitingalternativeexperimentalsettingsforevaluatingtop-nitemrecommen- Ji-RongWen.2022.RecBole2.0:TowardsaMoreUp-to-DateRecommendation
dationalgorithms.InProceedingsofthe29thACMInternationalConferenceon Library.InCIKM.