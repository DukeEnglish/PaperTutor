Log-Concave Coupling for
Sampling Neural Net Posteriors
Curtis McDonald Andrew R. Barron
Department of Statistics and Data Science Department of Statistics and Data Science
Yale University Yale University
New Haven, CT, USA New Haven, CT, USA
Email: curtis.mcdonald@yale.edu Email: andrew.barron@yale.edu
Abstract—In this work, we present a sampling algorithm for estimates, are more robust to model inaccuracy by balancing
singlehiddenlayerneuralnetworks.Thisalgorithmisbuiltupon a mixture of models in their posterior means, and are more
a recursive series of Bayesian posteriors using a method we call amenable to predictive risk bounds via information theoretic
Greedy Bayes. Sampling of the Bayesian posterior for neuron
analysis.
weight vectors w of dimension d is challenging because of its
multimodality. Our algorithm to tackle this problem is based The computational barrier to implementing an effective
on a coupling of the posterior density for w with an auxiliary Bayesian model is computing the resulting posterior means.
random variable ξ. Suchmeansareusuallycomputedviatheempiricalaverageof
Theresultingreverse conditionalw|ξ of neuronweightsgiven
a Markov Chain Monte Carlo (MCMC) sampling algorithm.
auxiliary random variable is shown to be log concave. In the
In order to be sampled efficiently, one requires a guarantee
construction of the posterior distributions we provide some
freedom in the choice of the prior. In particular, for Gaussian of rapid mixing of an MCMC algorithm for the posterior
priors on w with suitably small variance, theresultingmarginal distribution in a polynomial number of iterations dependent
density of the auxiliary variable ξ is proven to be strictly log onthedimensionoftheparametersdandnumberofobserved
concave for all dimensionsd. Fora uniformprioron theunitℓ 1 data points n.
ball, evidence is given that the density of ξ is again strictly log
For continuous parameter values, algorithms where rapid
concave for sufficiently large d.
mixing of MCMC methods is established are focused on
The score of the marginal density of the auxiliary random
variableξisdeterminedbyanexpectationoverw|ξandthuscan probabilitydistributionswithalogconcaveprobabilitydensity
be computed by various rapidly mixing Markov Chain Monte functioneitherunrestrictedoverRd orrestrictedoveraconvex
Carlo methods. Moreover, the computation of the score of ξ set.Insuchasituation,commondevicesforestablishingrapid
permitsmethodsofsamplingξbyastochasticdiffusion(Langevin
mixingsuchas log-Sobolevinequalities,conductancebounds,
dynamics) with drift function built from this score. With such
and spectral conditions follow nicely. This often results in an
dynamics, information-theoretic methods pioneered by Bakry
andEmeryshowthataccuratesamplingofξ isobtainedrapidly exponential decay in the relative entropy D t D 0e −ct at
≤
when its density is indeed strictly log-concave. After which, one some rate c along the Markov process.
moredrawfromw|ξ,producesneuronweightswwhosemarginal However, in the realm of modern machine learning with
distribution is from the desired posterior.1
sufficientlycomplexmodelsandinherentnon-linearity,there-
sultingposteriorforaBayesianmodelwilloftenexhibitmulti-
I. INTRODUCTION
modality and a non-concave landscape for the log likelihood.
Bayesian methods for parameterized models have long
Thus,itisnotguaranteedtobeefficientlysampledbyexisting
been prized by statisticians for various reasons. Maximum
MCMC methods.
Likelihood Estimation (MLE) provides only a single point
Therefore,one is left with the difficultyof how to compute
estimate among all the models of a given class, while a
the posterior means necessary to follow a Bayesian approach
Bayesianposteriorprovidesafulldistributionoverallpossible
for modern machine learning algorithms. In this paper, we
model parameters. As such, a posterior mean is a mixing of
study a class of posterior distributions for single hidden layer
many different models in a class, compared to a single point neuralnetworkswithK neuronsandneuronweightsw Rd,
k
estimate, and can have muchricher estimationpropertiesthan ∈
for k 1, ,K . The posterior distributions p(w) on neu-
any single model. Furthermore, MLE requires optimization ∈{ ··· }
ron weights we study are not themselves log concave. How-
of what can be a potentially multimodal surface, whereas
ever, by coupling with a specifically chosen auxiliary random
Bayesian posterior sampling can potentially overcome that
variableξ withpredefinedforwardconditionaldensityp(ξ w),
challenge. Bayesian methods have smooth transition from the |
we can construct a joint density p(w,ξ)=p(w)p(ξ w). The
prior distribution to the posterior compared to single point |
joint density can also be expressed via the resulting marginal
densityp(ξ)onξ andreverseconditionaldensityp(wξ),with
1Thisresearch waspresented attheInternational SymposiumonInforma- |
p(w,ξ)=p(wξ)p(ξ).
tion Theory (ISIT). Athens, Greece, July 11, 2024. The material was also |
presented inthe2024ShannonLecture. The key insight of this work is that with properly chosen
4202
luJ
62
]LM.tats[
1v20881.7042:viXraauxiliary random variable ξ, the reverse conditional density order α has more favorable risk properties, whereas smaller
p(wξ) can be shown to be log concave. The authors explore order α gives easier proof of efficient sampling methods. In
|
the question of the log concavity of the marginal density separate study, reasonable risk control occurs as long as α is
1
p(ξ) and relate this matter to a comparison of conditional at least of order n −2.
variances of linear combinationsof w given ξ that arise from We considertwodifferentpriorsforp (w) andcorrespond-
0
the posterior to those under the broader prior. When p(ξ) is ing assumptions on the data matrix X:
strictly log concave an efficient draw of ξ p(ξ) can be
1) Assume x 1 for all data matrix entries, and use
∼ i,j
made,andaresultingdraww p(wξ)canbemadethereafter | | ≤
a uniform prior for weights w over the set of ℓ norm
∼ | 1
resulting in a draw from the original posterior density for w
less than 1, C = w: w 1 .
1
using only log concave sampling methods. 2) With control of th{ e lark gesk t ei≤ gen} valueof XTX, we use
Thesedensitiescanbeusedtoconstructarecursiveseriesof a normal prior p =N(0,σ2I) with variance σ2.
0 0 0
posterior meansbased on the residuals of the previousfit in a
We will denote the prior as p (w) and specialize to each
method the authorscall the Greedy Bayes estimator. With the 0
specific case when necessary.
expectationsdefinedinthismethodexpressedvialogconcave
The density (2) is by itself not generally log concave.
densities,theycanbesampledinloworderpolynomialnumber
Indeed,checkingthe Hessian of logp(w) it is a linear combi-
of iterations. Using the information-theoretic techniques of
nationof rank 1 matriceswith positiveand negativemultiples
([1], [2]) these estimators have beneficial predictive risk
plus a contribution from the prior,
bounds, which will be detailed in future work.
n
II. MODELPARAMETERS AND AUXILIARY RANDOM 2logp(w)=α r ψ (x w)x xT + 2logp (w), (3)
VARIABLE DISTRIBUTION ∇
i=1
i ′′ i · i i ∇ 0
X
Let d be the dimension of the input covariates. We have wherethex areinterpretedascolumnvectorswithouterprod-
i
in pai 1rs , of ,i nnpu .t Td ha eta y ix gi iv∈ enR xd i w isit dh efire ns ep don bs ye sv oa mlu ee fs uny ci tif oo nr iu nct thx eix uT i ni. foT rh me cp ar si eor oc ront 2ri lb ou gt pion (wis )e =ither 1∇ I2 inlo tg hp e0 G(w au) s= sia0 n
f(∈ x{ )w· h· i· chis} notknowntousandwe wishto estimate from ∇ 0 −σ02
i case.Neithercaseisguaranteedtooverpowerthecontributions
our observations.
from the rank one matrices, so the overall expression could
Say we have a neuron activation function ψ which is havebothnegativeorpositiveeigenvaluesatdifferentwinputs
continuous in its first derivative and has bounded second
and is not a negative definite matrix. Therefore, we introduce
derivative ψ ′′(u) c for all u R. This includes, for
an auxiliary random variable as a tool to overcome this non-
| | ≤ ∈
example, the tanh function and the squared ReLU.
concavity.
Define a neuronweightvectorw Rd. Supposeat present,
We define the n dimensional random variable ξ by,
we have some existing fit fˆ for e∈ ach data observation y .
i i
These fits could come from some common fitting function fˆ ξ
i
=(αcr
i
)21 x iw+Z i, Z
i
N(0,1), (4)
applied to each point x , fˆ = fˆ(x ), but are not required | | ∼
i i i
with Z an independent normal random variable . With w
to. For some mixture weight β (0,1), we want to create an i
∈ p(w) this couplingdefinesthe forwardconditionaldensityfo∼r
updatedfitforeachdatapointbydown-weightingtheprevious
p(ξ w). Combiningthese two densities gives the joint density
fit and incorporating in some small amount of a new neuron, p(w|,ξ)=p(w)p(ξ w) as proportional to,
|
where x w
df eˆ nine ow te=
s
t( h1 e− inβ ne)f rˆ i p+ roβ duψ c( t.x Wi · hw e) n,
this
parame( t1 e)
r
p 0(w)exp α n r iψ(x i·w)− 21 n (ξ i−(αc|r i|)21 x i·w)2 !. (5)
i i=1 i=1
· X X
w is found by a least squares optimization, then this is the
By expanding the quadratic form this can be expressed as,
relaxed greedy neural net approach of Barron [3]–[6] and
Jones [7], and has connections to projection pursuit [7] and n αcr
boosting algorithms [8]. As an important example, we could p 0(w)exp αr iψ(x i w) | i |(x i w)2
· − 2 ·
consider the previous fit fˆ as a K 1 wide neural net i=1
nfˆ i eu= ron. K k=− 11c kψ(x i · w k) ai nd we wis− h to add in one new exp n [ −(cid:0)X 21 ξ i2+(αc |r i |)1 2ξ ix i ·w] . (cid:1)
DefiP ne r i =y i (1 β)fˆ i as the residualsof our previous (cid:0)Xi=1 (cid:1)
− −
fit, and given a prior density p (w) we define the Greedy For notational convenience define,
0
Bayes posterior for some scaling parameter α (0,1) as,
n
∈ αcr
g(w)= αr ψ(x w) | i |(x w)2. (6)
n i i i
− 2
p(w) exp α r ψ(x w) p (w). (2) i=1
i i 0 X
∝ · !
Xi=1 The joint density for p(w,ξ) can be written in two ways,
Thisdensityprioritizesweightswwhichhavehighinnerprod- theforwardexpressionp(w)p(ξ w) andthereverseexpression
|
uct under the activation function with the residuals. Constant p(ξ)p(wξ) using the inducedmarginaldensity p(ξ) for ξ and
|the reverse conditional density p(wξ)for wξ. The resulting facilitated by MCMC samples from the log concave density
| |
conditional density p(wξ) is, p(wξ).
| |
The Hessian of logp(ξ) is the negativeidentity matrix plus
n
p(w |ξ) ∝exp g(w)+ Xi=1(αc |r
i
|)1 2ξ ix
i
·w !p 0(w). (7) the conditio 2na lol gco pv (ξa )ria =nce Im +at Cri ox vo [(f α( cα Rc |R
)|
21) X1 2X ww ξ,
]. (14)
∇ − | | |
The resulting marginal on p(ξ) is,
In order for this to be a negative definite matrix, we need
p(ξ) ∝e −1 2Pn i=1ξ i2 eg(w)+Pn i=1(αc |ri|)1 2ξixi·wp 0(w)dw. (8) 1th .e Tl ha ir sge isst ee qi ug ie vn av lea nlu te toof thC eo sv t[ a( tα emc
|
eR
n|
t)1 2 thX atw
f|
oξ r] ato nyb ue nl ie ts vs et ch ta on
r
Z
III. THE LOGCONCAVITY OF DENSITIESp(wξ)ANDp(ξ) a,the scalarrandomvariablez =aT(αcR)Xw hasvariance
| less than 1, Var(z ξ) 1 ξ Rn. W| e t| hen study the log
A. Reverse Conditional Density p(wξ) | ≤ ∀ ∈
| concavity of this density under the two different assumptions
The exponent of p(wξ) is composed of three parts (we
ignore a constant here a|s we only study the density up to on the data matrix and prior.
proportionality), 1) Gaussian Prior and Data Matrix Eigenvalues:
Let XTX have largest eigenvalue λ . In this section, we
n max
1 will prove the following:
logp(w|ξ)=g(w)+ (αc|r i|)2ξ ix i·w+logp 0(w)+K ξ. (9)
Xi=1 a) Using a Gaussian prior with small variance σ 02
≤
The Hessian is then, 1 results in p(ξ) being log concave.
αc r ∞λmax
b) Thkerkeexistlargervariancesσ2 > 1 thatresult
2logp(wξ)= 2g(w)+ 2logp (w) (10) 0 αc r ∞λmax
∇ | ∇ ∇ 0 in p(ξ) being log concave. k k
n
=α |r
i
|(sign(r i)ψ ′′(x
i
·w) −c)x ixT
i
+ ∇2logp 0(w). Lemma 1. The conditional covariance matrix of the density
i=1 p(wξ) under the Gaussian prior is dominated by the covari-
X (11) |
ance matrix of the prior,
B isy aa ss usu mmp ot fio nn eg| aψ ti′ v′( eu) m| u≤ ltipc lef sor ofan ray nkinp ou nt eu m, as to rict eh se pa lb uo sv ae Cov[w |ξ] (cid:22)σ 02I. (15)
negativedefinite prior contribution,so it is a negativedefinite Equivalently, for any direction v the variance of z =v w is
·
expression. Thus the log density of p(wξ) is a concave less than σ2 v 2,
| 0k k
function for any conditioning value ξ.
Var(v wξ) σ2 v 2. (16)
NotefortheGaussianpriorcase(wherewisunbounded)the · | ≤ 0k k
presence of the Hessian from the prior makes p(wξ) strictly Proof. The log density for p(wξ) is the Gaussian prior log
log concave, as will be needed for rapid mixing of| Langevin densityplusalineartermandthe| concavefunctiong(w).From
dynamics in this case, whereas for the uniform prior on the theresultsofCaffarelli[9]andChewiandPooladian[10],we
compact C, the log concavity need not be strict as sampling have that over the whole of Rd, for two densities p(w)
methods mix rapidly for log concave densities on compact e V(w) and q(w)=e V(w) G(w) where V,G strictly conve∝ x
− − −
sets. functions, there exists as transport map from p to q that is
a contraction. Restricting to one dimensional directions z =
B. Marginal Density p(ξ)
v w, the one dimension density for z when w is drawn from
The log density of the marginal p(ξ) has a quadratic term ·
p(wξ) is more log concave than when w is drawn from the
in ξ and a term which represents the cumulant generating |
function of w under the density p˜(w) eg(w)p (w). That prior.As such, the transportmap for scalar randomvariablez
0
is, logp(ξ) is given by, ∝ is a contraction. Therefore for any direction v the variance of
z =v w is less when w is drawn from p(wξ) than when w
logp(ξ)=−1 n ξ2+log ePn i=1(αc|ri|)21 ξixi·wp˜(w)dw+K. is draw· n from the prior. |
2 i
Xi=1 Z (12) Lemma 2. Using a Gaussian prior with variance σ 02
≤
1 , the density p(ξ) is log concave.
Denote R as the diagonalmatrix of absolute valuesof the αc r ∞λmax
k k
residuals. T| h| e score is then a linear term in ξ and the condi- Proof. For any unit vector a Rd, by Lemma 1 we have,
∈
tional expectation under the reverse conditional distribution,
aTCov[(αcR)1
2Xwξ]a
1 | | |
∇logp(ξ)= −ξ+E[(αc |R |)2Xw |ξ]. (13) σ2aT(αcR)21 XXT(αcR)21
a
≤ 0 | | | |
Important to the implementation of MCMC samplers of σ2(αc r λ ) a 2
p(ξ) is that we are able to compute it’s score. Fortunately, ≤ a0 2.k k∞ max k k
the score function has the desired property that it is defined ≤k k
by an expected value over the previouslydefined log concave Thisresultsinexpression(14)beingnegativedefiniteandthus
distribution for wξ. The computation of this expectation is p(ξ) is log concave.
|While Lemma (1) is true, bounding Cov[wξ] by the prior sothe termin theexpectationof(20)isnegativesemidefinite
|
variance alone is a simple but loose bound. By using more for all input w values.
involvedanalysis,wecanshowthattherearehighervariances The term in the expectation is zero only at those w values
σ2 > 1 that result in the log concavity of p(ξ) as where 1 Λ 2 = C(w). At all other w values, the term is
w0
ell.
α kr k∞cλmax strictlyσ n02 ega−
tive definite. As this set is not a probability one
event, the expectation must be some finite amount below the
Lemma 3. Let X have singular value decomposition X =
0matrix.Thus,we canincreasethe priorvarianceσ2 to some
UΛVT with λ = max λ2. Denote the diagonal 0
matrix
ofresidum aa lsx
asR,
absi o∈l{u1 t, ed }vali
ue residualsas R, and
amountabovethevalue
αc r
1
∞λmax
andstillmaintainnegative
| | definiteness for these valukesk.
define the diagonal matrix S(w) with entries,
2) Bounded Data Entries and Uniform Prior over ℓ Ball:
[S(w)] =ψ (x w). (17) 1
i,i ′′ i In the case of the uniform prior over the ℓ ball, we would
1
Define the matrices A Rn d,B,C(w) Rd d, liketogiveacontractionresultsimilartoLemma1.However,
× ×
∈ ∈ for a log concave distribution restricted to a convex set,
A=(αcR)1 2U, B =UT(αcR)U, (18) the one dimensional marginals are more complicated as the
| | | |
C(w)=αUTRS(w)U. (19) geometryoftheconvexsetcanimpacttheHessianofthescalar
distributions.Therefore,theauthorsleavetheequivalentresult
Then we have upper bound on the Hessian of p(ξ) as
for the uniform prior as a conjecture to be proven in future
1 work.
∇2logp(ξ)(cid:22)AE −B−1+( Λ−2−C(w)+B)−1 ξ AT.
σ2
(cid:20) 0 (cid:12) (cid:21) Conjecture 1. The covariance matrix of the density p(wξ)
(cid:12) (20) |
(cid:12) under the uniform prior over the ℓ ball is dominated by the
(cid:12) 1
If σ2 = 1 then (20) is a negative definite matrix covariance matrix of the prior,
0 αc r ∞λmax
and we havek k2logp(ξ) 0. The expression is continuous
∇ ≺ Cov[wξ] Cov (w). (27)
in σ2 thus there exists values σ2 > 1 that achieve | (cid:22) Uni(C)
0 0 αc r ∞λmax
negative definiteness as well. k k Equivalently, for any direction v the variance of z =v w is
·
less under w drawn from p(wξ) than w drawn uniformly
Proof. Usingintegrationbyparts,anequivalentexpressionfor |
the covariance of wξ is, Var(v wξ) Var (v w). (28)
| · | ≤ Uni(C) ·
Cov[wξ]=σ2I +σ4 E[ 2g(w)ξ]+Cov( g(w)ξ) . Lemma 4. If conjecture 1 holds, if the dimension satisfies
| 0 0 ∇ | ∇ |
(21) d>αcn r andif x 1foralldataentriesthenp(ξ)
(cid:0) (cid:1) k k∞ | i,j |≤
is strictly log concave.
Sincewξ islogconcavewehavea Brascamp-Liebinequality
|
[11] upper bounding this covariance term, Proof. Considerthecovarianceofwdrawnuniformlyfromthe
ℓ ball. When drawn uniformly, Var(w )= d 1
Cov[ g(w)ξ] (22) 1 j (d+1)2(d+2) ≤ d2
∇ |
1
and Cov Uni(C)[w j1,w j2] = 0 for j
1
6= j 2. This follows from
E[( 2g(w))( I 2g(w)) 1( 2g(w))ξ] (23) properties of the Dirichlet distribution. For any unit vector a,
(cid:22) −∇ σ2 −∇ − −∇ |
=E[ −∇2g(w)
−
σ10
02I+
σ1
04(
σ1
02I −∇2g(w)) −1 |ξ]. (24)
αca αT c|R
|
n21 XCov Uni(C)[w]XT |R |1 2a
≤
α d2c aT |R |21 XXT |R |21 a
1 1
Combining (24) with (21) gives the upper bound, = d2 a ia j |r i |2 |r j |2x i ·x j.
i,j=1
1 X
Cov[w |ξ] (cid:22)E[( σ2I −∇2g(w)) −1 |ξ]. (25) Note that x
i
·x
j
≤d ∀i,j due to bounded data assumption.
0
n n
αc αc
This upper bound can then be input to equation (14) and via 1 1 1 1
matrix algebra expressed as equation (20). d2 a ia j |r i |2 |r j |2x i ·x j ≤ d a ia j |r i |2 |r j |2
i,j=1 i,j=1
Theonlypartoftheexpectationin(20)changinginwisthe X n X
matrixC(w).Notethatsinceψhasboundedsecondderivative, = α dc ( |r
i
|21 a i)2
≤
α dc kr
k1
≤
αcn k dr k∞ <1
C(w) αcUT RU αc r I (26) Xi=1
(cid:22) | | (cid:22) k k∞ Equation (14) then shows p(ξ) is strictly log concave.
Withpriorvarianceσ2 = 1 theterm 1 Λ 2 C(w)
representsa
positives0 emiα dker fikn∞ itc eλm max atrixforanσ y02 ch− oic−
eof w.
Remark 1. After the advance publication of this work [12],
we developedanalternativeprooffor the log-concavityof the
As such, the inverse of a matrix plus a positive semi definite
marginaldensitywhichwepresentedatthe2024International
matrix is dominated by the inverse of the standalone matrix,
SymposiumonInformationTheory(ISIT)[13].Theproofuses
σ1 02Λ −2 −C(w) (cid:23)0 =
⇒
( σ1 02Λ −2 −C(w)+B) −1 (cid:22)B −1, a toH aö cl hd ie er vein le oq gua cl oi nty car ve iq tyu .ir Uin sg ina
g
r aa ntio αo of f20 1(α ,cn tk hdr isk∞ r) e2 qu< ire1
s
√ndimension 20(c r )2n < d, while the conjectured result Modern methods of [18], [19], [20] continue to push the
only requires c
k
r
k∞
√n<d. This is a weaker result, but the polynomial mixing times bounds for log concave densities
k k∞
authors where able to achieve a proof while the covariance over convex sets. These methods take a sampling algorithm
domination condition (27) presented here remains a conjec- in the unconstrained case, e.g. Langevin diffusion or Hamil-
ture. tonian Monte Carlo, and produceversionsthat can be applied
The Hölder argument goes as follows. Define g ξ(w) as the over a constrained convex set. These methods vary in their
log density of p(wξ), and define g˜ (w) as the log density
ξ dependence on different properties of the set in question,
shifted by it’s mea|n under the prior. Define Γ (α) as the
ξ encapsulated in properties of a so called barrier function φ,
cumulant generating function of g˜ (w) under the prior at a
ξ
givenαlevel.Thenforanydirectionv,byaHölderinequality whichthe authorswillnotgointodetailabouthere.However,
with parameter ℓ we have upper bound: these algorithms essentially obtain mixing time bounds of
order O(d3).
Var[v·w|ξ]≤(E
p0[(v·w)2ℓ])1
ℓexp
ℓ−ℓ
1Γ ξ(
ℓ−ℓ
1α)−Γ ξ(α) Our sampling problem for p(ξ) represents a log concave
(cid:26) (2(cid:27) 9) densityoverthefullRn space,asdoesp(wξ)intheGaussian
|
prior case. The results of Bakry, Emery, et al [21], [22] study
The first term depends on the moments of the uniform
when a continuous time stochastic diffusion has exponential
prior over the ℓ ball, which are well understood, and the
secondtermdepe1
ndsonthegrowthofthecumulantgenerating
decayinit’srelativeentropyD(P
t
kP) ≤D(P
0
kP)e −2ct.This
result relies on the fact that the derivative of the relative
functioninahighprobabilityregionofξvalues.Studyingboth
entropyisminushalftheexpectednormsquaredofthediffer-
termsseparatelyandoptimizingoverthechoiceofℓyieldsthe
ence in scores, known as the relative Fisher information. The
stated result. This proof method will be presented completely
establishment of a log-Sobolev inequality shows the relative
in future work.
Fisherinformationislowerboundbyamultipleoftherelative
C. Connections with Reverse Diffusion entropy, which establishes exponential decay. For Langevin
The authors initially came up with this coupling while diffusion,theSDEwiththescoreasthedriftandwithconstant
studying score based diffusion [14], [15] as a sampling dispersion, the Bakry Emery condition, a sufficient condition
method. Consider ξ = X˜w with w drawn from p(w) and foralogSobolevinequality,reducestoaconditiononthestrict
0
then following the SDE dξ = ξ dτ +√2dB converging concavityofthe loglikelihood.Ifthedensityisc stronglylog
τ τ τ
tostandardnormalforlargeτ.Th− isalsowouldinduceforward concave, then the relative entropy decays at rate e −c 2t.
conditional p(ξ ξ =X˜w) N(e τX˜w,(1 e 2τ)I). The
τ 0 − −
idea of score
bas|
ed
diffusion∼
is if one can
com−
putethe scores
V. GREEDY BAYES FORNEURALNETWORKS
of the induced marginals logp(ξ τ) one can implement a We now construct a series of recursive posterior means
∇
reverse SDE that takes samples from a standard normal to defined by densities of the form (2). For each index i
samples from X˜w,w p(w). As discussed above, the scores 1, ,n initialize fits fˆ (x) = 0 and residuals r = y∈ .
of the marginals p(ξ∼ τ) can be computed via expectations S{ et· β·· (} 0,1) as our updi a,0 te weight and α (0,1i ),0 as oui r
over the reverse condition distributions p(ξ 0 ξ τ). The authors samplin∈ g scaling. ∈
noticed for values of
1e− e−2τ
2τ
≥
αc the
re|
verse conditional Then,picksome orderK forourgreedyfit. For allindexes
which defines this expec−tation is log concave, and thus these k 1, ,K , recursively define a posterior for index i
scoresasexpectationscouldbecomputedviaMCMCaverages usi∈ ng{ the· p·· revio} us residuals of indexes j 1, ,i 1
as discussed above. However, we have to be able to sample ∈{ ··· − }
the marginal p(ξ ) at time τ to initialize the reverse process, i 1
τ −
andthisseemsonlyfeasibleifthisdensityisitselflogconcave p i,k(w) exp α r j,k 1ψ(x j w) p 0(w). (30)
∝  − · 
or near a log concavedensity at the given τ value. Therefore, j=1
X
if one can show p(ξ ) and p(ξ ξ ) are both log concave,  
τ 0 | τ Update the fit by the posterior mean of this distribution and
we can remove the apparatus of the reverse SDE altogether
define a new set of residuals
and simply get draws from p(ξ ) and p(ξ ξ ) to sample our
τ 0 τ
|
original distribution. This intuition leads to the coupling we fˆ (x)=(1 β)fˆ (x)+βE [ψ(x w)] (31)
define.
i,k
r =y − (1
i,k
β−
)f1
ˆ (x ).
pi,k
· (32)
i,k i i,k i
− −
IV. MCMCSAMPLING FORLOG CONCAVE TARGET
DISTRIBUTIONS At level k = 1, for any index i, fˆ i,0(x) = 0 thus fˆ i,1 is the
posterior mean,
In the uniformpriorcase, the samplingproblemfor p(wξ)
|
representsalogconcavedensityoveraconstrainedconvexset fˆ (x)=βE [ψ(x w)].
C.Thefirstpolynomialtimeboundsforlogconcavesampling
i,1 pi,1
·
over convex sets come from [16] of order O˜(d10) (note O˜ This mean can be computed and maintained by storing some
ignores logn factors). Over the years, the polynomial time L number of samples from p i,1(w). Then for any desired x
boundforHit-and-RunandBallWalkalgorithmswasreduced value, fˆ (x) at this value can be computed as the empirical
i,1
to yield mixing time bounds of order O˜(d4) in [17]. mean of the stored L weights.Movingtolevelsk >1thepreviousestimatesfˆ i,k 1(x)can [14] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and
beevaluatedbyempiricalaveragesofstoredpreviou−ssamples B. Poole, “Score-based generative modeling through stochastic differ-
ential equations,” arXivpreprintarXiv:2011.13456, 2020.
from p (w) densities for j <i,s<k and so on.
j,s [15] B. Tzen and M. Raginsky, “Theoretical guarantees for sampling and
Given a new data point x, we define the K order Greedy inferenceingenerativemodelswithlatentdiffusions,”inConferenceon
Bayes estimator as LearningTheory. PMLR,2019,pp.3084–3114.
[16] D. Applegate and R. Kannan, “Sampling and integration of near log-
1 n concave functions,” in Proceedings of the twenty-third annual ACM
fˆ (x)= fˆ (x). (33) symposiumonTheoryofcomputing, 1991,pp.156–163.
K i,K
n [17] L.LovászandS.Vempala,“Thegeometryoflogconcavefunctionsand
i=1
X samplingalgorithms,”RandomStructures&Algorithms,vol.30,no.3,
This amounts to a mixture of nK conditional means, where pp.307–358, 2007.
each fit fˆ is only a function of data (x ,y ) for j [18] V. Srinivasan, A. Wibisono, and A. Wilson, “Fast sampling from
i,K j j ∈ constrained spaces using the Metropolis-adjusted mirror Langevin al-
1, ,i . All the conditional means here are of the form
gorithm,”arXivpreprintarXiv:2312.08823, 2023.
{ ··· }
(2) which can be expressed via the coupling in terms of log [19] Y. Kook and S. S. Vempala, “Gaussian cooling and Dikin walks:
concave densities and thus sampled efficiently via MCMC the interior-point method for logconcave sampling,” arXiv preprint
arXiv:2307.12943, 2023.
methods.
[20] Y. Kook, Y.-T. Lee, R. Shen, and S. Vempala, “Sampling with Rie-
mannian Hamiltonian Monte Carlo in a constrained space,” Advances
VI. FUTURE WORK inNeural Information Processing Systems, vol. 35, pp. 31684–31696,
2022.
Inthiswork,theauthorsdefinetheGreedyBayesprocedure
[21] D. Bakry and M. Emery, “Diffusions hypercontractives,” Seminaire
andstudyconditionsonthepriorandscalingparameterαthat de probabilites de Strasbourg, vol. 19, pp. 177–206, 1985. [Online].
give rise to provably efficient sampling. Ongoing work will Available: http://eudml.org/doc/113511
[22] D.Bakry,I.Gentil, andM.Ledoux,Analysis andgeometry ofMarkov
analyze the risk properties of this procedure. Current work
diffusion operators. Springer, 2014,vol.103.
indicates that the Greedy Bayes procedurecan be paired with
certain priors yielding both efficient sampling as studied here
and information-theoreticdetermination of the risk.
REFERENCES
[1] A. R. Barron, “Information-theoretic characterization of Bayes per-
formance and the choice of priors in parametric and nonparametric
problems,”inBayesianStatistics. OxfordUniv.Press,1998,vol.6.
[2] Y.YangandA.R.Barron,“Anasymptoticpropertyofmodelselection
criteria,” IEEETransactions onInformation Theory,vol.44,no.1,pp.
95–116,1998.
[3] A. Barron, “Universal approximation bounds for superposition of a
sigmoid function,” IEEE Transaction on Information Theory, vol. 39,
no.3,pp.930–945,1993.
[4] A. R. Barron, A. Cohen, W. Dahmen, and R. A. DeVore,
“Approximation and learning by greedy algorithms,” The Annals of
Statistics, vol. 36, no. 1, pp. 64 – 94, 2008. [Online]. Available:
https://doi.org/10.1214/009053607000000631
[5] C.Huang,G.Cheang,andA.Barron,“Riskofpenalized leastsquares,
greedyselectionandl1-penalizationforflexiblefunctionlibraries,”2008.
[6] J.M.KlusowskiandA.R.Barron,“Riskboundsforhigh-dimensional
ridgefunctioncombinations includingneuralnetworks,”arXivpreprint
arXiv:1607.01434, 2016.
[7] L. K. Jones, “A simple lemma on greedy approximation in Hilbert
spaceandconvergenceratesforprojectionpursuitregressionandneural
network training,” Ann. Statist., vol. 20, no. 1, pp. 608–613, 1992.
[Online]. Available: https://doi.org/10.1214/aos/1176348546
[8] J. H. Friedman, “Greedy function approximation: a gradient boosting
machine.” The Annals of Statistics, vol. 29, no. 5, pp. 1189 – 1232,
2001.[Online]. Available: https://doi.org/10.1214/aos/1013203451
[9] L.A.Caffarelli, “Monotonicity propertiesofoptimaltransportationand
the FKG and related inequalities,” Communications in Mathematical
Physics,vol.214,pp.547–563,2000.
[10] S. Chewi and A. A. Pooladian, “An entropic generalization of Caf-
farelli’scontractiontheoremviacovarianceinequalities,”Reports.Math-
ematical, vol.361,pp.1471–1482, 2023.
[11] S.G.Bobkov andM.Ledoux, “From Brunn-Minkowski toBrascamp-
LiebandtologarithmicSobolevinequalities,”GeometricandFunctional
Analysis,vol.10,pp.1028–1052, 2000.
[12] C. Mcdonald and A. R. Barron, “Log-concave coupling for sampling
neuralnetposteriors,”inInternational SymposiumonInformationThe-
ory(ISIT),Athens,Greece, July2024.
[13] A. R. Barron, “Information theory and high-dimensional Bayesian
computation,” Shannon Lecture at 2024 International Symposium on
InformationTheory(ISIT),Athens,Greece,July2024.[Online].Avail-
able: http://www.stat.yale.edu/~arb4/presentations/ShannonLecture.pdf