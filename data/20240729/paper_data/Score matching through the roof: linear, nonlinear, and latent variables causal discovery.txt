Score matching through the roof: linear, nonlinear,
and latent variables causal discovery
FrancescoMontagna∗ PhilippM.Faller∗
MaLGa,UniversityofGenoa KarlsruheInstituteofTechnology,Amazon
PatrickBlöbaum ElkeKirschbaum FrancescoLocatello
Amazon Amazon InstituteofScienceandTechnologyAustria(ISTA)
Abstract
Causaldiscoveryfromobservationaldataholdsgreatpromise,butexistingmethods
relyonstrongassumptionsabouttheunderlyingcausalstructure,oftenrequiring
fullobservabilityofallrelevantvariables. Wetacklethesechallengesbyleveraging
the score function ∇logp(X) of observed variables for causal discovery and
proposethefollowingcontributions. First,wegeneralizetheexistingresultsof
identifiabilitywiththescoretoadditivenoisemodelswithminimalrequirements
onthecausalmechanisms. Second,weestablishconditionsforinferringcausal
relations from the score even in the presence of hidden variables; this result is
two-faced: wedemonstratethescore’spotentialasanalternativetoconditional
independence tests to infer the equivalence class of causal graphs with hidden
variables,andweprovidethenecessaryconditionsforidentifyingdirectcausesin
latentvariablemodels. Buildingontheseinsights,weproposeaflexiblealgorithm
forcausaldiscoveryacrosslinear,nonlinear,andlatentvariablemodels,whichwe
empiricallyvalidate.
1 Introduction
Theinferenceofcausaleffectsfromobservationsholdsthepotentialforgreatimpactarguablyinany
domainofscience,whereitiscrucialtobeabletoanswerinterventionalandcounterfactualqueries
fromobservationaldata[1,2,3]. Existingcausaldiscoverymethodscanbecategorizedbasedon
theinformationtheycanextractfromthedata[4], andtheassumptionstheyrelyon. Traditional
causaldiscoverymethods(e.g. PC,GES[5,6])aregeneralintheirapplicabilitybutlimitedtothe
inferenceofanequivalenceclass. Additionalassumptionsonthestructuralequationsgenerating
effectsfromthecauseare,infact,imposedtoensuretheidentifiabilityofacausalorder[7,8,9,10].
Asaconsequence,existingmethodsforcausaldiscoveryrequirespecializedandoftenuntestable
assumptions,preventingtheirapplicationtoreal-worldscenarios.
Further,themajorityofexistingapproachesarehinderedbytheassumptionthatallrelevantcauses
ofthemeasureddataareobserved,whichisnecessarytointerpretassociationsinthedataascausal
relationships. Despitetheconvenienceofthishypothesis,itisoftennotmetinpractice,andthesolu-
tionsrelaxingthisrequirementfacesubstantiallimitations. TheFCIalgorithm[11]canonlyreturnan
equivalenceclassfromthedata.Appealingtoadditionalrestrictionsensurestheidentifiabilityofsome
directcausaleffectsinthepresenceoflatentvariables: RCD[12]reliesonthelinearnon-Gaussian
additivenoisemodel,whereasCAM-UV[13]requiresnonlinearadditivemechanisms. Nevertheless,
thestrictconditionsonthestructuralequationsholdbacktheirapplicabilitytomoregeneralsettings.
∗Sharedfirstco-author
Preprint.Underreview.
4202
luJ
62
]LM.tats[
1v55781.7042:viXraOurpapertacklesthesechallengesandcanbeputinthecontextofarecentlineofacademicresearch
thatderivesaconnectionbetweenthescorefunction∇logp(X)andthecausalgraphunderlying
the data-generating process [14, 15, 16, 17, 18, 19]. The use of the score for causal discovery is
practicallyappealing,asityieldsadvantagesintermsofscalabilitytohighdimensionalgraphs[16]
andguaranteesoffinitesamplecomplexitybounds[20]. Insteadofimposingassumptionsthatensure
strong,thoughoftenimpractical,theoreticalguarantees,weorganicallydemonstratedifferentlevelsof
identifiabilitybasedonthestrengthofthemodelinghypotheses,alwaysrelyingonthescorefunction
toencodeallthecausalinformationinthedata. StartingfromresultsofSpantinietal.[21]andLin
[22],weshowhowconstraintsontheJacobianofthescore∇2logp(X)canbeusedasanalternative
toconditionalindependencetestingtoidentifytheMarkovequivalenceclassofcausalmodelswith
hiddenvariables. Further,weprovethatthescorefunctionidentifiesthecausaldirectionofadditive
noisemodels,withminimalassumptionsonthecausalmechanisms.Thisextendsthepreviousfindings
ofMontagnaetal.[17],limitedbytheassumptionofnonlinearityofthecausaleffects,andGhoshal
andHonorio[14],limitedtolinearmechanisms. Ontheseresults,webuildthemaincontributions
ofourwork,enablingtheidentificationofdirectcausaleffectsinhiddenvariablesmodels.
Ourmaincontributionsareasfollows: (i)Wepresentthenecessaryconditionsfortheidentifiability
of direct causal effects and the presence of hidden variables with the score in the case of latent
variablesmodels. (ii)WeproposeAdaScore(AdaptiveScore-basedcausaldiscovery), aflexible
algorithmforcausaldiscoverybasedonscorematchingestimationof∇logp(X)[23]. Basedonthe
user’sbeliefabouttheplausibilityofseveralmodelingassumptionsonthedata,AdaScorecanoutput
aMarkovequivalenceclass,adirectedacyclicgraph,oramixedgraph,accountingforthepresence
ofunobservedvariables. Tothebestofourknowledge,thebroadclassofcausalmodelshandledby
ourmethodisunmatchedbyotherapproachesintheliterature.
2 Modeldefinitionandrelatedworks
Inthissection,weintroducetheformalismofstructuralcausalmodels(SCMs),separatelyforthethe
caseswithandwithouthiddenvariables.
2.1 Causalmodelwithobservedvariables
LetX beasetofrandomvariablesinRdefinedaccordingtothesetofstructuralequations
X :=f (X ,N ), ∀i=1,...,k. (1)
i i PAG i
i
N ∈ Raremutuallyindependentrandomvariableswithstrictlypositivedensity,knownasnoise
i
or error terms. The function f is the causal mechanism mapping the set of direct causes X
i PAG
i
of X and the noise term N , to X ’s value. A structural causal model (SCM) is defined as the
i i i
tuple (X,N,F,P ), where F = (f )k is the set of causal mechanisms, and P is the joint
N i i=1 N
distributionrelativetothedensityp overthenoisetermsN ∈Rk. WedefinethecausalgraphG
N
asadirectedacyclicgraph(DAG)withnodesX ={X ,...,X },andthesetofedgesdefinedas
1 k
{X → X : X ∈ X },suchthatPAG aretheindicesoftheparentnodesofX inthegraph
j i j PAG i i
i
G. (Intheremainderofthepaper,weadoptthefollowingnotation: givenasetofrandomvariables
Y ={Y ,...,Y }andasetofindicesZ ⊂N,thenY ={Y |i∈Z,Y ∈Y}.)
1 n Z i i
Underthismodel,theprobabilitydensityofX satisfiestheMarkovfactorization(e.g. Petersetal.
[1]Proposition6.31):
k
(cid:89)
p(x)= p(x |x ), (2)
i PAG
i
i=1
whereweadopttheconventionoflowercaselettersreferringtorealizedrandomvariables,andusep
todenotethedensityofdifferentrandomobjects,whenthedistinctionisclearfromtheargument.
ThisfactorizationisequivalenttotheglobalMarkovcondition(e.g. Petersetal.[1]Proposition6.22)
thatdemandsthatforall{X ,X }∈X,X ⊆X\{X ,X },then
i j Z i j
X dX |X =⇒ X X |X , i G j Z i j Z
where(· ·|·)denotesprobabilisticconditionalindependenceofX ,X givenX ,and(· d ·|·) i j Z G
is the notation for d-separation, a criterion of conditional independence defined on the graph G
2
=|
=| =|
=|(Definition 5 of the appendix). As it is commonly done, we assume that the reverse direction
X X |X =⇒ X dX |X hold, andwesaythatthedensitypisfaithfultothegraphG i j Z i G j Z
[2,24](hencethefaithfulnessassumption). TogetherwiththeglobalMarkovcondition,faithfulness
impliesanequivalencebetweentheprobabilisticandgraphicalnotionsofconditionalindependence:
X X |X ⇐⇒X dX |X . (3) i j Z i G j Z
Ingeneral,severalDAGsmayentailthesamesetofd-separations: graphssharingsuchcommon
structureformaMarkovequivalenceclass(seeDefinition6intheappendix).
Theabovemodelassumesthattherearen’tanyunobservedcausesofvariablesinX,otherthanthe
noisetermsinN. Asweareinterestedindistributionswithpotentialhiddenvariables,wewillnow
generalizeourmodeltorepresentdata-generatingprocessesthatmayinvolvelatentcauses.
Definitions on graphs. As graphs play a central role in our work, Appendix A.1 provides a
detailedoverviewofthefundamentalnotationanddefinitionsthatwerelyonintheremainderof
thepaper. Forthenextsection,weadvisethereadertobecomfortablewiththenotionsofancestors
(Definition2)andinducingpaths(Definition3)inDAGs.
Closelyrelatedworks. Severalmethodsforthecausaldiscoveryoffullyobservablemodelsusing
thescorehavebeenrecentlyproposed. GhoshalandHonorio[14]demonstratestheidentifiabilityof
thelinearnon-Gaussianmodelfromthescore,anditiscomplementedbyRollandetal.[15],which
showstheconnectionbetweenscorematchingestimationof∇logp(X)andtheinferenceofcausal
graphs underlying nonlinear additive noise models with Gaussian noise terms, also allowing for
samplecomplexitybounds[20]. Montagnaetal.[17]providesidentifiabilityresultsinthenonlinear
setting,withoutposinganyrestrictiononthedistributionofthenoiseterms. Montagnaetal.[16]
is the first to show that the Jacobian of the score provides information equivalent to conditional
independencetestinginthecontextofcausaldiscovery,limitedtothecaseofadditivenoisemodels.
Allofthesestudiesmakespecializedassumptionstofindtheoreticalguaranteesofidentifiability,
whereas our paper provides a unifying view of causal discovery with the score function, which
generalizesandexpandstheexistingresults.
2.2 Causalmodelwithunobservedvariables
Underthemodel(1),weconsiderthecasewherethesetofvariablesX ispartitionedintothedisjoint
subsetsofobservedrandomvariablesV ={V ,...,V }andunobserved(orlatent)randomvariables
1 d
U ={U ,...,U }. Weassumethatthefollowingsetofstructuralequationsissatisfied:
1 p
V :=f (V ,Ui,N ), ∀i=1,...,d, (4)
i i PAG i
i
whereUi standsforthesetofunobservedparentsofV ,andV = {V |k ∈ PAG,V ∈ V}are
i PAG k i k
i
theobserveddirectcausesofV . Someofthecausalrelationsandtheconditionalindependencies
i
impliedbythesetofequations(4)canbesummarizedinagraphobtainedasamarginalizationofthe
DAGG ontotheobservablenodesV.
Definition1(Marginalgraph,Zhang[25]). LetX =V∪˙U andG beaDAGoverX. Thefollowing
constructiongivesthemarginalgraphMG,withnodesV andedgesfoundasfollows:
V
• pairofnodesV ,V areadjacentinthegraphMG ifandonlyifthereisaninducingpath
i j V
betweenthemrelativetoU inG;
• foreachpairofadjacentnodesV ,V inMG,orienttheedgeasV →V ifV isanancestor
i j V i j i
ofV inG,elseorientitasV ↔V .
j i j
WedefinethemapG (cid:55)→MG asthemarginalizationoftheDAGG ontoV,theobservablenodes.
V
Thegraphresultingfromtheaboveconstructionisamaximalancestralgraph(MAG,Definition4),
hencewewilloftenrefertoitasthemarginalMAGofG. Intuitively,adirectededgedenotesthe
presenceofanancestorshiprelation,whereasbidirectededgesrepresentdependenciesthatcannotbe
removedbyconditioningonanyofthevariablesinthegraph.
3
=| =|
=| =|In the case of DAGs, d-separation encodes the probabilistic conditional independence relations
betweenthevariablesofX inthegraphG,asexplicitbyEquation(3). Suchnotionofgraphicalsepa-
rationhasanaturalgeneralizationtomaximalancestralgraphs,knownasm-separation(Definition5
oftheappendix). Zhang[25]showsthatm-separationandd-separationareinfactequivalent(see
Lemma1oftheappendix),suchthatgivenV ⊂V and{V ,V }⊂V,thefollowingholds:
Z i j
V dV |V \{V ,V } ⇐⇒ V m V |V \{V ,V }, (5) i G j Z i j i MG j Z i j
V
where(· m ·|·)denotesm-separationrelativetothegraphMG. JustlikewithDAGs,MAGs
MG V
V
that imply the same set of conditional independencies define an equivalence class. Usually, the
commonstructureofthesegraphsisrepresentedbypartialancestralgraphs(PAGs,Definition7of
theappendix). WeuseP todenotethePAGrelativetoMG.
MG V
V
Problem definition. In this work, our goal is to provide theoretical guarantees for the
identifiabilityoftheMarkovequivalenceclassofthemarginalgraphMG anditsdirectcausal
V
effectswiththescore,wherevariablesV aredefinedaccordingtoEquation(4).
i
Withoutfurtherassumptionsonthedata-generatingprocess,wecanidentifythegraphMG onlyup
V
toitspartialancestralgraph,asdiscussedinthenextsection.
Closelyrelatedworks. Causaldiscoverywithlatentvariableshavebeenfirststudiedinthecontext
ofconstraint-basedapproacheswiththeFCIalgorithm[11],whichshowstheidentifiabilityofthe
equivalence class of a marginalized graph via conditional independence testing. The RCD and
CAM-UV [12, 13] approaches instead demonstrate the inferrability of directed causal edges via
regression and residuals independence testing. Both methods rely on strong assumptions on the
causalmechanisms: theirtheoreticalguaranteesapplytomodelswheretheeffectsaregeneratedbya
linear(RCD)ornonlinear(CAM-UV)additivecontributionofeachcause. Ourworkdemonstrates
thatusingthescorefunctionforcausaldiscoveryunifiesandgeneralizestheseresults,presenting
analternativetoconditionalindependencetestingforconstraint-basedmethods,andbeingagnostic
abouttheclassofcausalmechanismsoftheobservedvariables,undertheweakerrequirementof
additivityofthenoiseterms.
3 Theoryforascore-basedtestofseparation
Inthissection,weshowthatforV ⊆X generatedaccordingtoEquation(4)theHessianmatrixof
logp(V)identifiestheequivalenceclassofthemarginalMAGMG. Ithasalreadybeenproventhat
V
cross-partialderivativesofthelog-likelihoodareinformativeaboutasetofconditionalindependence
relationshipsbetweenrandomvariables: Spantinietal.[21](Lemma4.1)showsthat,givenV ⊆X
Z
suchthat{V ,V }⊆V ,then
i j Z
∂2
logp(V )=0 ⇐⇒ V V |V \{V ,V }. (6) ∂V ∂V Z i j Z i j
i j
Equation (3) resulting from faithfulness and the directed global Markov property immediately
impliesthatthisexpressioncanbeusedasatestofconditionalindependencetoidentifytheMarkov
equivalenceclassofthegraphMG,ascommonlydoneinconstraint-basedcausaldiscovery(for
V
reference,seee.g.Section3inGlymouretal.[4]).ThisresultgeneralizesLemma1ofMontagnaetal.
[16],whereitisusedtodefineconstraintstoinferedgesinthecausalstructurewithoutlatentvariables.
Proposition1(Adapted2from[21]). LetV beasetofrandomvariableswithstrictlypositivedensity
generatedaccordingtomodel(4). ForeachsetV ⊆V ofnodesinMG suchthat{V ,V }⊆V ,
Z V i j Z
2IntheirLemma4.1Spantinietal.[21]providestheconnectionbetweenvanishingcross-partialderivatives
ofthelog-likelihoodandconditionalindependenceofrandomvariables.Notethatthisresultdoesnotdependon
theassumptionofagenerativemodel,thusholdingbeyondthesetofstructuralequations(4).Ourresultadapts
theirfindingtothecasewhenobservationsaregeneratedaccordingtoafullyobservablecausalmodel.
4
=|
=| =|
=|thefollowingholdsforeachsupportedvaluev :
Z
∂2
logp(v )=0 ⇐⇒ V m V |V \{V ,V }. ∂V ∂V Z i MG j Z i j
i j V
TheresultofProposition1presentsanalternativetoconditionalindependencetestinginconstraint-
basedapproachestocausaldiscovery,showingthattheequivalenceclassofthegraphMG canbe
V
identifiedusingthecrosspartialderivativesofthelog-likelihoodasatestofconditionalindependence
betweenvariables,muchinthespiritoftheFastCausalInferencealgorithm[11]. Identifyingthe
Markovequivalenceclassisthemostwecanhopetoachievewithoutfurtherhypotheses. Aswewill
seeinthenextsection,thescorefunctioncanalsohelpleverageadditionalrestrictiveassumptionson
thecausalmechanismsofEquation(4)toidentifydirectcausaleffects.
4 Atheoryofidentifiabilityfromthescore
Inthissection,weshowthat,underadditionalassumptionsonthedata-generatingprocess,wecan
identifythedirectcausalrelationsthatarenotinfluencedbyunobservedvariables,aswellasthe
presenceofunobservedactivepaths(Definition5)betweennodesinthemarginalizedgraphMG.
V
Asapreliminarystepbeforedivingintocausaldiscoverywithlatentvariables,weshowhowthe
propertiesofthescorefunctionidentifyedgesindirectedacyclicgraphs,thatisintheabsenceof
latent variables (when U = ∅ and G = MG). The goal of the next section is two-sided: first, it
V
introducesthefundamentalideasconnectingthescorefunctiontocausaldiscoverythatalsoapplyto
hiddenvariablemodels,second,itextendstheexistingtheoryofcausaldiscoverywithscorematching
toadditivenoisemodelswithbothlinearandnonlinearmechanisms.
4.1 Warmup: identifiabilitywithoutlatentconfounders
Inthissection,wesummariseandextendthetheoreticalfindingspresentedinMontagnaetal.[17],
wheretheauthorsshowhowtoderiveconstraintsonthescorefunctionthatidentifythecausalorderof
theDAGG whereallthevariablesinthesetX areobserved. Definethestructuralrelationsof(1)as:
X :=h (X )+N ,i=1,...,k, (7)
i i PAG i
i
withthreetimescontinuouslydifferentiablemechanismsh ,noisetermscenteredatzero,andstrictly
i
positivedensityp . GiventheMarkovfactorizationofEquation(2),thecomponentsofthescore
X
function∇logp(x)are:
(cid:88)
∂ logp(x)=∂ logp(x |x )+ ∂ logp(x |x )
Xi Xi i PAG
i
Xi j PAG
j
j∈CHG
i (8)
(cid:88)
=∂ logp(n )− ∂ h (x )∂ logp(n ),
Ni i Xi j PAG
j
Nj j
j∈CHG
i
whereCHG denotesthesetofchildrenofnodeX . WeobservethatifanodeX isasink,i.e. a
i i s
nodesatisfyingCHG =∅,thenthesummationoverthechildrenvanishes,implyingthat:
s
∂ logp(x)=∂ logp(n ). (9)
Xs Ns s
Thekeypointisthatthescorecomponentofasinknodeisafunctionofitsstructuralequationnoise
term,suchthatonecouldlearnaconsistentestimatorof∂ logp fromasetofobservationsofthe
Xs X
noisetermN . Giventhat,ingeneral,onehasaccesstoX samplesratherthanobservationsofthe
s
noiserandomvariables,authorsinMontagnaetal.[17]showthatN ofasinknodecanbeconsistently
s
estimatedfromi.i.d. realizationsofX. ForeachnodeX ,...,X ,wedefinethequantity:
1 k
R :=X −E[X |X ], (10)
i i i \Xi
whereX aretherandomvariablesinthesetX\{X }. E[X |X ]istheoptimalleastsquares
\Xi i i \Xi
predictorofX fromalltheremainingnodesinthegraph, andR istheregressionresidual. For
i i
asinknodeX ,theresidualsatisfies:
s
R =N , (11)
s s
5
=|which can be seen by rewriting E[X |X ] = h (X ) + E[N |X ,X ] =
s \Xs s PAG
s
s DEG
s
NDG
s
h (X )+E[N ],whereX andX denotesthedescendantsandnon-descendantsofX ,
s PAG s DEG NDG s
s s s
respectively. Equations(9)and(11)togetherimplythatthescore∂ logp(N )isafunctionofR ,
Ns s s
suchthatitispossibletofindaconsistentapproximatorofthescoreofasinkfromobservationsofR .
s
Proposition 2 (Generalization of Lemma 1 in Montagna et al. [17]). Let X be a set of random
variables,generatedbyarestrictedadditivenoisemodel(Definition9)withstructuralequations(7),
andletX ∈X. Considerr inthesupportofR . Then:
j j j
(cid:104)(cid:0) (cid:2) (cid:3) (cid:1)2(cid:105)
X isasink⇐⇒E E ∂ logp(X)|R =r −∂ logp(X) =0. (12)
j Xj j j Xj
Our result generalizes Lemma 1 in Montagna et al. [17], as they assume X generated by an
identifiableadditivenoisemodelwithnonlinearmechanisms. Instead,weremovethenonlinearity
assumptionandmaketheweakerhypothesisofarestrictedadditivenoisemodel,whichisprovably
identifiable[9],intheformalsensedefinedintheappendix(Definition8). Thisresultdoesn’tcome
asasurprise,giventhepreviousfindingsofGhoshalandHonorio[14]showingthatthescoreinfers
linearnon-Gaussianadditivenoisemodels: Proposition2providesaunifyingandgeneraltheory
fortheidentifiabilityofmodelswithpotentiallymixedlinearandnonlinearmechanisms.
Based on these insights, Montagna et al. [17] propose the NoGAM algorithm to exploit the con-
dition in (12) for identifying the causal order of the graph: being E[∂ logp(X)|R ] the opti-
Xi i
mal least squares estimator of the score of node X from R , a sink node is characterized as the
i i
argmin E[E[∂ logp(X)|R ]−∂ logp(X)]2, where in practice the residuals R , the score
i Xi i Xi i
componentsandtheleastsquaresestimatorsarereplacedbytheirempiricalcounterparts. Aftera
sinknodeisidentified,itisremovedfromthegraphandassignedapositionintheorder,andthe
procedureisiterativelyrepeateduptothesourcenodes. Beingthescoreestimatedbyscorematching
techniques[23],weusuallymakereferencetoscorematching-basedcausaldiscovery.
Inthenextsection,weshowhowwecangeneralizetheseresultstoidentifydirectcausaleffects
betweenapairofvariablesinthemarginalMAGMG whenU ̸=∅
V
4.2 Identifiabilityinthepresenceoflatentconfounders
Wenowintroducethelastofourmaintheoreticalresults,thatis: givenapairofnodesV ,V that
i j
areadjacentinthegraphMG withU ̸= ∅,wecanusethescorefunctiontoidentifythepresence
V
ofadirectcausaleffectbetweenV andV ,orthatofanactivepaththatisinfluencedbyunobserved
i j
variables.GiventhatthecausalmodelofEquation(4)ensuresidentifiabilityonlyuptotheequivalence
class,weneedadditionalrestrictiveassumptions. Inparticular,weenforceanadditivenoisemodel
withrespecttoboththeobservedandunobservednoisevariables. Thiscorrespondstoanadditive
noisemodelontheobservedvariableswiththenoisetermsrecenteredbythelatentcausaleffects.
Assumption 1 (SCM assumptions). The set of structural equations of the observable variables
specifiedin(4)isnowdefinedas:
V :=f (V )+g (Ui)+N ,∀i=1,...,d, (13)
i i PAG i i
i
assumingthemechanismsf itobeofclassC3(R|V PAG i| ),andmutuallyindependentnoisetermswith
strictlypositivedensityfunction. TheN ’sareassumedtobenon-Gaussianwhenf islinearinsome
i i
ofitsarguments.
Crucially,ourhypothesisisweakerthanthoserequiredbytwostate-of-the-artapproaches,CAM-UV
[13]andRCD[12]: CAM-UVassumesaCausalAdditiveModel(CAM)withstructuralequations
with nonlinear mechanisms in the form V := (cid:80) f (V )+(cid:80) g (Ui)+N , and RCD
i k∈PAG ik k Ui ik k i
i k
requiresanadditivenoisemodelwithlineareffectsofboththelatentandobservedcauses. Thus,
ourmodelencompassesandextendsthenonlinearandlinearsettingsofCAM-UVandRCD,such
thatthetheorydevelopedintheremainderofthesectionisvalidforabroaderclassofcausalmodels.
Ourfirststepisrewritingthestructuralrelationsin(13)as:
V :=f (V )+N˜ ,
i i PAG i
i (14)
N˜ :=g (Ui)+N ,∀i=1,...,d,
i i i
6whichprovidesanadditivenoisemodelintheformof(7). Next,wedefinethefollowingregression
residualsforanynodeV inthegraphMG:
k V
R (V ):=V −E[V |V ], (15)
k Z k k Z\{k}
whereV denotesthesetofrandomvariablesV \{V }.
Z\{k} Z k
Giventhesedefinitions,wearereadytoshowhowdirectededges,andthepresenceofunobserved
variablescanbeidentifiedfromthescoreoflinearandnonlinearadditivenoisemodels.
4.2.1 Identifiabilityofdirectededges
Consider V ,V adjacent nodes in the PAG P : we want to investigate when a direct causal
i j MG
V
effect V ∈ V can be identified from the score. We make the following observations: for
i PAG
j
V =V ∪{V }andV GUj,byEquation(15)itfollows Z PAG j PAG d
j j
R (V )=N˜ −E[N˜ ], (16)
j Z j j
whereweuseV GUjtowriteE[N˜ |V ]=E[N˜ ].Moreover,wenotethatV isasinknode PAG d j Z\{j} j j
j
relativetoMG ,themarginalizationofGontoV . Inanalogytothecasewithoutlatentvariables,we
VZ Z
canshowthat∂ logp(V )isafunctionofN˜ ,theerrortermintheadditivenoisemodelofEquation
Vj Z j
(14),suchthatthescoreofV canbeconsistentlypredictedfromobservationsoftheresidualR (V ).
j j Z
Proposition3. LetXbegeneratedbyarestrictedadditivenoisemodelwithstructuralequations(7),
andcausalgraphG. ConsiderV ,V adjacentinMG,marginalizationofG. Further,assumethat
i j V
thescorecomponent∂ logp(V )isnotconstantforuncountablevaluesofV .
Vj Z Z
(i) LetV =V ∪{V ,V },andr ∈RinthesupportofR (V ). Then:
Z PAG i j j j Z
j
V dUj ∧V ∈V ⇐⇒E[∂ logp(V )−E[∂ logp(V )|R (V )=r ]]2 =0. PAG
j
G i PAG
j
Vj Z Vj Z j Z j
(ii) LetV ⊆V,suchthat{V ,V }⊆V . Then:
Z i j Z
V ̸ dUj ∨V ̸∈V ⇐⇒E[∂ logp(V )−E[∂ logp(V )|R (V )=r ]]2 ̸=0. PAG
j
G i PAG
j
Vj Z Vj Z j Z j
Intuitively, the proposition has two essential implications. Part (i) provides the condition for the
identifiabilityofthepotentialdirectcausaleffectbetweenapairV ,V ,thatis,whentheassociation
i j
betweenV anditsobservedparentsisnotinfluencedbyactivepathsthatinvolvelatentvariables.
j
Thisconditionisnecessary: givenanactivepathsuchthatV ̸ dUj,thescorecouldnotidentify PAG G
j
adirectcausaleffectV →V ,whichisthecontentofthesecondpartoftheproposition.
i j
Wehaveestablishedtheoreticalguaranteesofidentifiabilityforlinearandnonlinearadditivenoise
models,eveninthepresenceofhiddenvariables: wefindthatthescorefunctionisameansforthe
identifiabilityofalldirectparentalrelationsthatarenotinfluencedbyunobservedvariables;allthe
remainingarrowheadsoftheedgesinthegraphMG areidentifiednobetterthanintheequivalence
V
class. Based on these insights, we propose AdaScore, a score matching-based algorithm for the
inferenceofMarkovequivalenceclasses,directcausaleffects,andthepresenceoflatentvariables.
4.3 Ascore-basedalgorithmforcausaldiscovery
Buildingonourtheory,weproposeAdaScore,ageneralizationofNoGAMtolinearandnonlinear
additive noise models with latent variables. The main strength of our approach is its adaptivity
withrespecttostructuralassumptions: basedontheuser’sbeliefabouttheplausibilityofseveral
modelingassumptionsonthedata,AdaScorecanoutputanequivalenceclass(usingthecondition
ofProposition1insteadofconditionalindependencetestinginanFCI-likealgorithm),adirected
acyclicgraph(asinNoGAM),oramixedgraph,accountingforthepresenceofunobservedvariables.
Wenowdescribetheversionofouralgorithmwhoseoutputisamixedgraph,wherewerelyonscore
matching estimation of the score and its Jacobian (Appendix C.2). At an intuitive level, we find
unorientededgesusingProposition1,i.e. checkingfordependenciesintheformofnon-zeroentries
7
=|
=|
=|
=|
=|intheJacobianofthescoreviahypothesistestingonthemean,andfindtheedges’directionsviathe
conditionofProposition3,i.e.byestimatingresidualsofeachnodeX andcheckingwhethertheycan
i
correctlypredictthei-thscoreentry(thevanishingmeansquarederrorsareverifiedbyhypothesistest
ofzeromean). Itwouldbetemptingtosimplyfindtheskeleton(i.e. thegraphicalrepresentationof
theconstraintsofanequivalenceclass)firstviathewell-knownadjacencysearchoftheFCIalgorithm
andtheniteratethroughallneighborhoodsofallnodestoorientedgesusingProposition3. This
wouldbeprohibitivelyexpensive,asfindingtheskeletoniswell-knowntohavesuper-exponential
computationalcomplexity[11]. Instead,weproposeanalternativesolution: exploitingthefactthat
somenodesmaynotbeinfluencedbylatentvariables,wefirstuseProposition2tofindsinknodes
thatarenotaffectedbylatents(usinghypothesistestingtofindvanishingmeansquarederrorinthe
scorepredictionsfromtheresiduals),inthespiritoftheNoGAMalgorithm. Ifthereissuchasink,
wesearchallitsadjacentnodesviaProposition1(plusanoptionalpruningstepforbetteraccuracy,
AppendixC.2),andorienttheinferrededgestowardsthesink. Else,ifnosinkcanbefound,wepick
anodeinthegraphandfinditsneighborsbyProposition1,orientingitsedgesusingtheconditionin
Proposition3(scoreestimationbyresidualsunderlatenteffects). Thisway,wegetanalgorithmthat
ispolynomialinthebestcase(AppendixC.3). DetailsonAdaScoreareprovidedinAppendixC,
whileapseudo-codesummaryisprovidedintheAlgorithm1box.
Algorithm1Simplifiedpseudo-codeofAdaScore
whilenodesremaindo
if Proposition3findsasinkwithallparentsobserved then
addedgesfromadjacentnodestosink
else
picksomeremainingnodeV ∈V
i
pruneneighbourhoodofV usingProposition1
i
orientedgesadjacenttoV usingProposition3
i
ifV hasoutgoingdirectededgetosomeV ∈V then
i j
continuewith V
j
else
removeV formremainingnodes
i
pruneremainingbidirectededgesusingProposition1
5 Experiments
We use the causally3 Python library [26] to generate synthetic data with known ground truths,
createdasErdös-Rényisparseanddensegraphs,respectivelywithprobabilityofedgebetweenpair
ofnodesequals0.3and0.5. Wesamplethedataaccordingtolinearandnonlinearmechanismswith
additivenoise, wherethenonlinearfunctionsareparametrizedbyaneuralnetworkwithrandom
weights,acommonapproachintheliterature[18,26,27,28,29]. Noisetermsaresampledfroma
uniformdistributioninthe[−2,2]range. Hiddencausaleffectsareobtainedbyrandomlypicking
two nodes and dropping the corresponding column from the data matrix. See Appendix D.1 for
furtherdetailsonthedatageneration. Asmetric,weconsiderthestructuralHammingdistance(SHD)
[30, 31], a simple count of the number of incorrect edges, where missing and wrongly directed
edgescountasoneerror. WefixthelevelofthehypothesistestsofAdaScoreto0.05,whichisa
commonchoiceintheabsenceofpriorknowledge. WecompareAdaScoretoNoGAM,CAM-UV,
RCD,andDirectLiNGAM,whoseassumptionsaredetailedinTable1. Inthemainmanuscript,we
commentontheresultsondatasetsof1000observationsfromdensegraphs,withandwithoutlatent
variables. AdditionalexperimentsincludingthoseonsparsenetworksarepresentedinAppendixE.
Oursyntheticdataarestandardizedbytheirempiricalvariancetoremoveshortcutsinthedata[18,32].
Discussion. OurexperimentalresultsonmodelswithoutlatentvariablesofFigure1ashowthatwhen
causalrelationsarelinear,AdaScorecanrecoverthecausalgraphwithaccuracythatiscomparable
withalltheotherbenchmarks,withtheexceptionofDirectLiNGAM.OnnonlineardataAdaScore
presents better performance than CAM-UV, RCD, and DirectLiNGAM while being comparable
to NoGAM in accuracy. This is in line with our expectations: in the absence of finite sample
3https://causally.readthedocs.io/en/latest/
8adascore camuv nogam rcd lingam
linear nonlinear
35 35
30 30
25 25
20 20
15 15
10 10
5 5
0 0
3 5 7 9 3 5 7 9
number of nodes number of nodes
(a)Fullyobservablemodel
linear nonlinear
35
30
30
25
25
20 20
15 15
10 10
5 5
0 0
3 5 7 9 3 5 7 9
number of nodes number of nodes
(b)Latentvariablesmodel
Figure1:Empiricalresultsondensegraphswithdifferentnumbersofnodes,onfullyobservable(nohidden
variables) and latent variable models. We report the SHD accuracy (the lower, the better). We note that
DirectLiNGAMissurprisinglyrobusttodifferentstructuralassumptions,andAdaScoreisgenerallycomparable
orbetter(asinnonlinearobservabledata)thantheotherbenchmarks.
Table1:Experimentscausaldiscoveryalgorithms.Thecontentofthecellsdenoteswhetherthemethodsupports
(✓)ornot(✗)theconditionspecifiedinthecorrespondingrow.
CAM-UV RCD NoGAM DirectLiNGAM AdaScore
Linearadditivenoisemodel ✗ ✓ ✗ ✓ ✓
Nonlinearadditivenoisemodel ✗ ✗ ✓ ✗ ✓
NonlinearCAM ✓ ✗ ✓ ✗ ✓
Latentvariableseffects ✓ ✓ ✗ ✗ ✓
Output Mixed Mixed DAG DAG Mixed
errorsandinthefullyobservablesetting,NoGAMandAdaScoreareindeedthesamealgorithms.
Wheninferringunderlatentcausaleffects,Figure1b,ourmethodperformscomparablytoCAM-
UVandRCDongraphsuptosevennodeswhileslightlydegradingonninenodes. Additionally,
AdaScoreoutperformsNoGAMinthissetting,aswewouldexpectaccordingtoourtheory. Overall,
weobservethatourmethodisrobusttoavarietyofstructuralassumptions,withaccuracythatis
oftencomparableandsometimesbetterthancompetitors(asinnonlinearobservablesettings). We
remarkthatalthoughAdaScoredoesnotclearlyoutperformtheotherbaselines,itsbroadtheoretical
guaranteesofidentifiabilityarenotmatchedbyanyavailablemethodintheliterature;thismakesit
anappealingoptionforinferenceinrealisticscenariosthatarehardtoinvestigatewithsyntheticdata,
wherethestructuralassumptionsofthecausalmodelunderlyingtheobservationsareunknown.
6 Conclusion
Theexistingliteratureoncausaldiscoveryshowsaconnectionbetweenscorematchingandstructure
learninginthecontextofnonlinearANMs: inthispaper,(i)weformalizeandextendtheseresults
tolinearSCMs,and(ii)weshowthatthescoreretainsinformationonthecausalstructureeveninthe
9
dhs
dhs
dhs
dhspresenceofunobservedvariables. Additionally,whilepreviousworksposittheaccentonfindingthe
causalorderthroughthescore,westudyitspotentialtoidentifytheMarkovequivalenceclasswitha
constraint-basedstrategythatdoesnotexplicitlyrequiretestsofconditionalindependence,aswellas
toidentifydirectcausaleffects.OurtheoreticalinsightsresultinAdaScore:unlikeexistingapproaches
fortheestimationofcausaldirections,ouralgorithmprovidestheoreticalguaranteesforabroadclass
ofidentifiablemodels,namelylinearandnonlinear,withadditivenoise,inthepresenceoflatent
variables. EventhoughAdaScoredoesnotclearlyoutperformtheexistingbaselinesonoursynthetic
benchmark,itsadaptivitytodifferentstructuralhypothesesisasteptowardscausaldiscoverythatis
lessreliantonpriorassumptions,whichareoftenuntestableandthushinderingreliableinferencein
real-worldproblems. Whilewedonottouchonthetaskofcausalrepresentationlearning[33],where
causalvariablesarelearnedfromdata,webelievethisisapromisingresearchdirectioninrelation
toourworkduetothespecificinterplaybetweenscore-matchingestimationandgenerativemodels.
References
[1] JonasPeters,DominikJanzing,andBernhardSchölkopf. Elementsofcausalinference: founda-
tionsandlearningalgorithms. TheMITPress,2017.
[2] JudeaPearl. Causality. Cambridgeuniversitypress,2009.
[3] PeterSpirtes. Introductiontocausalinference. JournalofMachineLearningResearch,11(54):
1643–1662,2010. URLhttp://jmlr.org/papers/v11/spirtes10a.html.
[4] ClarkGlymour,KunZhang,andPeterSpirtes. Reviewofcausaldiscoverymethodsbasedon
graphicalmodels. FrontiersinGenetics,10,2019. ISSN1664-8021. doi: 10.3389/fgene.2019.
00524. URLhttps://www.frontiersin.org/articles/10.3389/fgene.2019.00524.
[5] P.Spirtes,C.Glymour,andR.Scheines. Causation,Prediction,andSearch. MITpress,2nd
edition,2000.
[6] DavidMaxwellChickering. Optimalstructureidentificationwithgreedysearch. J.Mach.Learn.
Res.,3(null):507–554,mar2003. ISSN1532-4435. doi: 10.1162/153244303321897717. URL
https://doi.org/10.1162/153244303321897717.
[7] ShoheiShimizu,PatrikO.Hoyer,AapoHyvärinen,andAnttiKerminen. Alinearnon-gaussian
acyclic model for causal discovery. J. Mach. Learn. Res., 7:2003–2030, dec 2006. ISSN
1532-4435.
[8] PatrikHoyer,DominikJanzing,JorisMMooij,JonasPeters,andBernhardSchölkopf. Non-
linearcausaldiscoverywithadditivenoisemodels. InD.Koller,D.Schuurmans,Y.Bengio,
andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems,volume21.Cur-
ranAssociates,Inc.,2008. URLhttps://proceedings.neurips.cc/paper/2008/file/
f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf.
[9] JonasPeters,JorisM.Mooij,DominikJanzing,andBernhardSchölkopf. Causaldiscovery
withcontinuousadditivenoisemodels. J.Mach.Learn.Res.,15(1):2009–2053,jan2014. ISSN
1532-4435.
[10] KunZhangandAapoHyvärinen. Ontheidentifiabilityofthepost-nonlinearcausalmodel. In
ProceedingsoftheTwenty-FifthConferenceonUncertaintyinArtificialIntelligence,UAI’09,
page647–655,Arlington,Virginia,USA,2009.AUAIPress. ISBN9780974903958.
[11] PeterSpirtes.Ananytimealgorithmforcausalinference.InThomasS.RichardsonandTommiS.
Jaakkola,editors,ProceedingsoftheEighthInternationalWorkshoponArtificialIntelligence
and Statistics, volume R3 of Proceedings of Machine Learning Research, pages 278–285.
PMLR,04–07Jan2001. URLhttps://proceedings.mlr.press/r3/spirtes01a.html.
ReissuedbyPMLRon31March2021.
[12] Takashi Nicholas Maeda and Shohei Shimizu. Rcd: Repetitive causal discovery of linear
non-gaussianacyclicmodelswithlatentconfounders. InSilviaChiappaandRobertoCalandra,
editors,ProceedingsoftheTwentyThirdInternationalConferenceonArtificialIntelligenceand
Statistics,volume108ofProceedingsofMachineLearningResearch,pages735–745.PMLR,
26–28Aug2020. URLhttps://proceedings.mlr.press/v108/maeda20a.html.
10[13] TakashiNicholasMaedaandShoheiShimizu.Causaladditivemodelswithunobservedvariables.
InUncertaintyinArtificialIntelligence,pages97–106.PMLR,2021.
[14] AsishGhoshalandJeanHonorio. Learninglinearstructuralequationmodelsinpolynomial
timeandsamplecomplexity. InAmosStorkeyandFernandoPerez-Cruz,editors,Proceedings
oftheTwenty-FirstInternationalConferenceonArtificialIntelligenceandStatistics,volume84
ofProceedingsofMachineLearningResearch,pages1466–1475.PMLR,09–11Apr2018.
URLhttps://proceedings.mlr.press/v84/ghoshal18a.html.
[15] PaulRolland,VolkanCevher,MatthäusKleindessner,ChrisRussell,DominikJanzing,Bernhard
Schölkopf,andFrancescoLocatello. Scorematchingenablescausaldiscoveryofnonlinear
additivenoisemodels. InKamalikaChaudhuri,StefanieJegelka,LeSong,CsabaSzepesvari,
Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on
MachineLearning,volume162ofProceedingsofMachineLearningResearch,pages18741–
18753.PMLR,17–23Jul2022.
[16] FrancescoMontagna,NicolettaNoceti,LorenzoRosasco,KunZhang,andFrancescoLocatello.
Scalablecausaldiscoverywithscorematching. In2ndConferenceonCausalLearningand
Reasoning,2023. URLhttps://openreview.net/forum?id=6VvoDjLBPQV.
[17] FrancescoMontagna,NicolettaNoceti,LorenzoRosasco,KunZhang,andFrancescoLocatello.
Causaldiscoverywithscorematchingonadditivemodelswitharbitrarynoise.In2ndConference
on Causal Learning and Reasoning, 2023. URL https://openreview.net/forum?id=
rVO0Bx90deu.
[18] FrancescoMontagna,NicolettaNoceti,LorenzoRosasco,andFrancescoLocatello. Shortcuts
forcausaldiscoveryofnonlinearmodelsbyscorematching,2023.
[19] PedroSanchez,XiaoLiu,AlisonQO’Neil,andSotiriosA.Tsaftaris. Diffusionmodelsfor
causaldiscoveryviatopologicalordering.InTheEleventhInternationalConferenceonLearning
Representations,2023. URLhttps://openreview.net/forum?id=Idusfje4-Wq.
[20] ZhenyuZhu,FrancescoLocatello,andVolkanCevher. Samplecomplexityboundsforscore-
matching: Causaldiscoveryandgenerativemodeling. AdvancesinNeuralInformationProcess-
ingSystems,36,2024.
[21] Alessio Spantini, Daniele Bigoni, and Youssef Marzouk. Inference via low-dimensional
couplings,2018.
[22] Juan Lin. Factorizing multivariate function classes. In M. Jordan, M. Kearns, and
S. Solla, editors, Advances in Neural Information Processing Systems, volume 10. MIT
Press, 1997. URL https://proceedings.neurips.cc/paper_files/paper/1997/
file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf.
[23] AapoHyvärinen. Estimationofnon-normalizedstatisticalmodelsbyscorematching. J.Mach.
Learn. Res., 6:695–709, 2005. URL https://api.semanticscholar.org/CorpusID:
1152227.
[24] CarolineUhler,G.Raskutti,PeterBühlmann,andB.Yu. Geometryofthefaithfulnessassump-
tionincausalinference. TheAnnalsofStatistics,41,072012. doi: 10.1214/12-AOS1080.
[25] JijiZhang. Causalreasoningwithancestralgraphs. JournalofMachineLearningResearch,9
(7),2008.
[26] FrancescoMontagna, AtalantiMastakouri, EliasEulig, NicolettaNoceti, LorenzoRosasco,
Dominik Janzing, Bryon Aragam, and Francesco Locatello. Assumption violations
in causal discovery and the robustness of score matching. In A. Oh, T. Neumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural
Information Processing Systems, volume 36, pages 47339–47378. Curran Associates,
Inc.,2023. URLhttps://proceedings.neurips.cc/paper_files/paper/2023/file/
93ed74938a54a73b5e4c52bbaf42ca8e-Paper-Conference.pdf.
11[27] PhillipLippe,TacoCohen,andEfstratiosGavves. Efficientneuralcausaldiscoverywithout
acyclicityconstraints. InInternationalConferenceonLearningRepresentations,2022. URL
https://openreview.net/forum?id=eYciPrLuUhG.
[28] NanRosemaryKe,SilviaChiappa,JaneXWang,JorgBornschein,AnirudhGoyal,MelanieRey,
TheophaneWeber,MatthewBotvinick,MichaelCurtisMozer,andDaniloJimenezRezende.
Learningtoinducecausalstructure. InInternationalConferenceonLearningRepresentations,
2023. URLhttps://openreview.net/forum?id=hp_RwhKDJ5.
[29] Philippe Brouillard, Sébastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and
AlexandreDrouin. Differentiablecausaldiscoveryfrominterventionaldata. InProceedingsof
the34thInternationalConferenceonNeuralInformationProcessingSystems,NIPS’20,Red
Hook,NY,USA,2020.CurranAssociatesInc. ISBN9781713829546.
[30] IoannisTsamardinos,LauraEBrown,andConstantinFAliferis. Themax-minhill-climbing
bayesiannetworkstructurelearningalgorithm. Machinelearning,65:31–78,2006.
[31] SofiaTriantafillouandIoannisTsamardinos. Score-basedvsconstraint-basedcausallearningin
thepresenceofconfounders. InCfa@uai,pages59–67,2016.
[32] AlexanderG.Reisach,ChristofSeiler,andSebastianWeichwald. Bewareofthesimulateddag!
causaldiscoverybenchmarksmaybeeasytogame. InNeuralInformationProcessingSystems,
2021. URLhttps://api.semanticscholar.org/CorpusID:239998404.
[33] BernhardScholkopf,FrancescoLocatello,StefanBauer,NanKe,NalKalchbrenner,Anirudh
Goyal,andY.Bengio. Towardcausalrepresentationlearning. ProceedingsoftheIEEE,PP:
1–23,022021. doi: 10.1109/JPROC.2021.3058954.
[34] Peter Spirtes and Thomas Richardson. A polynomial time algorithm for determining dag
equivalenceinthepresenceoflatentvariablesandselectionbias. InProceedingsofthe6th
InternationalWorkshoponArtificialIntelligenceandStatistics,pages489–500.Citeseer,1996.
[35] YingzhenLiandRichardETurner. Gradientestimatorsforimplicitmodels. arXivpreprint
arXiv:1705.07107,2017.
[36] PeterBühlmann,JonasPeters,andJanErnest. CAM:Causaladditivemodels,high-dimensional
order search and penalized regression. The Annals of Statistics, 42(6), dec 2014. URL
https://doi.org/10.1214%2F14-aos1260.
12A Usefulresults
Inthissection,weprovideacollectionofresultsanddefinitionsrelevanttothetheoryofthispaper.
A.1 Definitionsovergraphs
LetX =X ,...,X asetofrandomvariables. AgraphG =(X,E)consistsoffinitelymanynodes
1 d
orverticesX andedgesE. Wenowprovideadditionaldefinitions,separatelyfordirectedacyclic
andmixedgraphs.
Directedacyclicgraph. Inadirectedgraph,nodescanbeconnectedbyadirectededge(→),and
betweeneachpairofnodesthereisatmostonedirectededge. WesaythatX isaparentofX if
1 j
X →X ∈E,inwhichcasewealsosaythatX isachildofX . Twonodesareadjacentifthey
i j j i
areconnectedbyanedge. Threenodesarecalledav-structureifonenodeisachildoftheother
two,e.g. asX →X ←X isacollider. ApathinG isasequenceofatleasttwodistinctvertices
i k j
X ,...,X suchthatthereisanedgebetweenX andX . IfX → X foreverynode
i1 im ik ik+1 ik ik+1
inthepath,wespeakofadirectedpath,andcallX anancestorofX ,X adescendantof
ik ik+1 ik+1
X . GiventhesetDEG ofdescendantsofanodeX ,wedefinethesetofnon-descendantsofX as
ik i i i
NDG =X\(DEG∪{X }). Anodewithoutparentsiscalledasourcenode. Anodewithoutchildren
i i i
iscalledasinknode. Adirectedacyclicgraphisadirectedgraphwithnocycles.
Mixedgraph. Inamixedgraphnodescanbeconnectedbyadirectededge(→)orabidirected
edge(↔),andbetweeneachpairofnodesthereisatmostonedirectededge. Twoverticesaresaid
tobeadjacentinagraphifthereisanedge(ofanykind)betweenthem. Thedefinitionsofparent,
child,ancestor,descendant,pathprovidedfordirectedacyclicgraphalsoapplyinthecaseofmixed
graphs. Additionally,X isaspouseofX (andvice-versa)ifX ↔X ∈E. Analmostdirected
i j i j
cycleoccurswhenX ↔X ∈E andX isanancestorofX inG.
i j i j
Foreaseofreferencefromthemaintext,weseparatelyprovidethedefinitionofinducingpathsand
ancestorsindirectedacyclicgraphs.
Definition2(Ancestor). ConsideraDAGG withsetofnodesX,andX ,X elementsofX. We
i j
say that X is an ancestor of X if there is a directed path from X to X in the graph, as in
i j i j
X →...→X .
i j
Definition3(Inducingpath). ConsideraDAGG withsetofnodesX,andY,Z disjointsubsetssuch
thatX =Y∪˙Z. WesaythatthereisaninducingpathrelativetoZ betweenthenodesY ,Y ifevery
i j
nodeonthepaththatisnotinZ∪{Y ,Y }isacollideronthepath(i.e. foreachY ∈Y onthepath
i j k
thesequenceY ...→Y ←...Y appears)andeverycollideronthepathisanancestorofY orY .
i k j i j
Onenaturalwaytoencodeinducingpathsandancestralrelationshipsbetweenvariablesisrepresented
bymaximalancestralgraphs.
Definition4(MAG). Amaximalancestralgraph(MAG)isamixedgraphsuchthat:
1. therearenodirectedcyclesandnoalmostdirectedcycles;
2. therearenoinducingpathsbetweentwonon-adjacentnodes.
Next,wedefineconditionalindependenceinthecontextofgraphs.
Definition5(m-separation). LetMbeamixedgraphwithnodesX.ApathπinMbetweenX ,X
i j
elementsofX isactivew.r.t. Z ⊆X\{X ,X }if:
i j
1. everynon-collideronπisnotinZ
2. everycollideronπisanancestorsofanodeinZ.
X andX aresaidtobem-separatedbyZ ifthereisnoactivepathbetweenX andX relativetoZ.
i j i j
TwodisjointsetsofvariablesW andY arem-separatedbyZ ifeveryvariableinW ism-separated
fromeveryvariableinY byZ.
Ifm-separationisappliedtoDAGs,itiscalledd-separation.
13Thesetofdirectedacyclicgraphsthatsatisfythesamesetofconditionalindependenciesforman
equivalenceclass,knownastheMarkovequivalenceclass.
Definition6(MarkovequivalenceclassofaDAG). LetG beaDAGwithnodesX. Wedenotewith
[G]theMarkovequivalenceclassofG. ADAGG˜withnodesX isin[G]ifthefollowingconditions
aresatisfiedforeachpairX ,X ofdistinctnodesinX:
i j
• thereisanedgebetweenX ,X inG ifandonlyifthereisanedgebetweenX ,X inG˜;
i j i j
• letZ ⊆X\{X ,X }. ThenX dX |Z ⇐⇒ X dX |Z; i j i G j i G˜ j
• letπbeapathbetweenX andX . X isacolliderinthepathπinG ifandonlyifitisa
i j k
colliderinthepathπinG˜.
In summary, graphs in the same equivalence class share the edges up to direction, the set of d-
separations,andthesetofcolliders.
Just as for DAGs, there may be several MAGs that imply the same conditional independence
statements. DenotetheMarkov-equivalenceclassofaMAGMwith[M]: thisisrepresentedbya
partialmixedgraph,theclassofgraphsthatcancontainfourkindsofedges: →,↔,◦−−◦and◦→,
andhencethreekindsofendmarksforedges: arrowhead(>),tail(−)andcircle(◦).
Definition7(PAG,Definition3ofZhang[25]). Let[M]betheMarkovequivalenceclassofan
arbitraryMAGM. Thepartialancestralgraph(PAG)for[M],P ,isapartialmixedgraphsuch
M
that:
• P hasthesameadjacenciesasM(andanymemberof[M])does;
M
• AmarkofarrowheadisinP ifandonlyifitissharedbyallMAGsin[M];and
M
• AmarkoftailisinP ifandonlyifitissharedbyallMAGsin[M].
M
Intuitively,aPAGrepresentsanequivalenceclassofMAGsbydisplayingallcommonedgemarks
sharedbyallmembersoftheclassanddisplayingcirclesforthosemarksthatarenotincommon.
A.2 Equivalencebetweenm-separationandd-separation
Inthissection,weprovideaproofforequation(5),statingtheequivalencebetweenm-separationand
d-separationinaformalsense.
Lemma1(AdaptedfromZhang[25]). LetG beaDAGwithnodesX = V ∪U, withV andU
disjointsets,andMG themarginalizationofGontoV.Forany{V ,V }∈V andV ⊆V\{V ,V },
V i j Z i j
thefollowingequivalenceholds:
V dV |V ⇐⇒ V m V |V . i G j Z i MG j Z
V
Proof. TheimplicationV dV |V =⇒ V m V |V isadirectconsequenceofLemma18 i G j Z i MG j Z
V
fromSpirtesandRichardson[34],wherewesetS =∅,sincewedonotconsiderselectionbias. The
implicationV dV |V ⇐= V m V |V followsfromLemma17bySpirtesandRichardson i G j Z i MG j Z
V
[34], againwithS = ∅. Note, thatintheirterminology“d-separationinMAGs”iswhatwecall
m-separation.
A.3 Additivenoisemodelidentifiability
Westudytheidentifiabilityoftheadditivenoisemodel,reportingresultsfromPetersetal.[9]. We
startwithaformaldefinitionofidentifiabilityinthecontextofcausaldiscovery.
Definition8(Identifiablecausalmodel). Let(X,N,F,p )beanSCMwithunderlyinggraphGand
N
p jointdensityfunctionofthevariablesofX. Wesaythatthemodelisidentifiablefromobserva-
X
tionaldataifthedistributionp cannotbegeneratedbyastructuralcausalmodelwithgraphG˜̸=G.
X
First,weconsiderthecaseofmodelsoftworandomvariables
X :=f(X )+N, X N. (17) 2 1 1
14
=|
=|
=|
=|
=|
=|
=|
=|
=|Condition1(Condition19ofPetersetal.[9]). Consideranadditivenoisemodelwithstructural
equations(17). Thetriple(f,p ,p )doesnotsolvethefollowingdifferentialequationforallpairs
X1 N
x ,x withf′(x )ν′′(x −f(x ))̸=0:
1 2 2 2 1
(cid:18) f′′ ν′′′f′(cid:19) ν′′′ν′f′′f′ ν′(f′′)2
ξ′′′ =ξ′′ − + − −2ν′′f′′f′+ν′f′′′, (18)
f′ ν′′ ν′′ f′
Here, ξ := logp , ν := logp , thelogarithmsofthestrictlypositivedensities. Thearguments
X1 N
x −f(x ),x ,andx ofν,ξandf respectively,havebeenremovedtoimprovereadability.
2 1 1 1
Next,weshowthatastructuralcausalmodelsatisfyingCondition1isidentifiable,asinDefinition8
Theorem1(Theorem20ofPetersetal.[9]). Letp thejointdistributionofapairofrandom
X1,X2
variablesgeneratedaccordingtothemodelofequation(17)thatsatisfiesCondition1,withgraphG.
Then,G isidentifiablefromthejointdistribution.
Finally,weshowanimportantfact,holdingforidentifiablebivariatemodels,whichisthatthescore
∂ logp(x ,x )isnonlinearinx .
∂X1 1 2 1
Lemma2(Sufficientvariabilityofthescore). Letp thejointdistributionofapairofrandom
X1,X2
variablesgeneratedaccordingtoastructuralcausalmodelthatsatisfiesCondition1,withgraphG.
Then:
∂
(ξ′(x )−f′(x )ν′(x −f(x )))̸=0,
∂X 1 1 2 1
1
forallpairs(x ,x ).
1 2
Proof. Bycontradiction,assumethatthereexists(x ,x )suchthat ∂ (ξ′(x )−f′(x )ν′(x −
1 2 ∂X1 1 1 2
f(x )))=0. Then:
1
 
∂2 π(x ,x )
∂ ∂X2 1 2
 1 =0,
∂X 1 ∂X∂ 1∂2 X2π(x 1,x 2)
whereπ(x ,x ) = logp(x ,x ). Byexplicitlycomputingallthepartialderivativesoftheabove
1 2 1 2
equation,weobtainthatequation18issatisfied,whichviolatesCondition1.
Theseresultsguaranteeingtheidentifiabilityofthebivariateadditivenoisemodelcanbegeneralized
tothemultivariablecase,withasetofrandomvariablesX ={X ,...,X }thatsatisfy:
1 k
X :=f (X )+N ,i=1,...,k, (19)
i i PAG i
i
whereG istheresultingcausalgraphdirectedandacyclic. Theintuitionisthat,ratherthanstudying
themultivariatemodelasawhole,weneedtoensurethatCondition1issatisfiedforeachpairof
nodes,addingrestrictionsontheirmarginalconditionaldistribution.
Definition9(Definition27ofPetersetal.[9]). Consideranadditivenoisemodelwithstructural
equations(19). WecallthisSCMarestrictedadditivenoisemodelifforallX ∈X,X ∈X ,
j i PAG
j
andallsetsX ⊆ X,S ⊂ N,withX \{X } ⊆ X ⊆ XG \{X ,X },thereisavaluex
S PAG
j
i S NDj i j S
withp(x )>0,suchthatthetriplet
S
(f (x ,·),p ,p )
j PAG j\{i} Xi|XS=xS Nj
satisfies Condition 1. Here, f (x ,·) denotes the mechanism function x (cid:55)→ f (x ).
j PAG\{i} i j PAG
j j
Additionally, we require the noise variables to have positive densities and the functions f to be
j
continuousandthreetimescontinuouslydifferentiable.
Then,forarestrictedadditivenoisemodel,wecanidentifythegraphfromthedistribution.
Theorem 2 (Theorem 28 of Peters et al. [9]). Let X be generated by a restricted additive noise
modelwithgraphG,andassumethatthecausalmechanismsf arenotconstantinanyoftheinput
j
arguments,i.e. forX ∈X ,thereexistx ̸=x′ suchthatf (x ,x )̸=f (x ,x′).
i PAG i i j PAG\{i} i j PAG\{i} i
j j j
Then,G isidentifiable.
15A.4 Otherauxiliaryresults
Westateseveralresultsthatholdforapairofrandomvariablesthatarenotconnectedbyanactivepath
thatincludesunobservedvariables(activepathsareintroducedinDefinition5). Fortheremainderof
thesection,letV,U beapairofdisjointsetsofrandomvariables,X =V ∪U generatedaccording
tothestructuralcausalmodeldefinedbythesetofequations(1),G theassociatedcausalgraph,and
MG themarginalizationontoV.
V
Thefirststatementprovidesunderwhichconditiontheunobservedparentsoftwovariablesinthe
marginalMAGaremutuallyindependentrandomvectors.
Lemma3. LetV ∈ V, andZ ⊂ NsuchthatV = V ∪{V }. AssumeV dUj. Then j Z PAG j PAG G
j j
Uj d GUZk foreachindexZ k ̸=j.
Proof. TheassumptionV dUj impliesthatthereisnoactivepathinGbetweennodesinV PAG G PAG
j j
andnodesinUj. GiventhatforeachZ
k
∈ Z,Z
k
̸= Z,nodesinUZk aredirectcausesofatleast
onenodeinV PAG,anyactivepathbetweennodesinUZk andnodesinUj wouldalsobeanactive
j
pathbetweenV PAG andUj,whichisacontradiction. HenceUj d GUZk.
j
Thepreviouslemmasallowprovingthefollowingresult,whichwillbefundamentaltodemonstrate
thetheoryofProposition3.
Lemma4. LetV ∈V,andZ ⊂NsuchthatV =V ∪{V }. AssumeV dUj. W.l.o.g., j Z PAG j PAG G
j j
letthej-thelementofV beV =V . DenoteasUZ thesetofunobservedparentsofnodesinV ,
Z Zj j Z
andUZ\{j} theunobservedparentsofnodesinV := V \V . Then,thefollowingholdsfor
Z\{j} Z j
eachv ,uZ values:
Z
logp(v )=logp(v |v )+logQ(v ),
Z j PAG Z
j
where
|Z|
(cid:88) (cid:89)
Q(v Z)= p(uZ\{j}) p(v Zk|v Z1,...,v Zk−1,uZk).
uZ\{j} k̸=j
Proof. Bythelawoftotalprobabilityandthechainrule,wecanwritep(v )as:
Z
(cid:88)
p(v )= p(v |u)p(u)
Z Z
u
(20)
(cid:88)
= p(u)p(v |u,v )p(v |u).
Zj Z\{j} Z\{j}
u
wBy ecL ae nm fm aca to3 r, izU eZ pj (u)=UZ pk (cid:0), uk Z̸= j(cid:1)j p, (cid:0)w uh Ze \r {e j}U (cid:1) .Z Pk ld ue gn go inte gs thu eno fab cs te or rv ie zd atp ioa nre in nts eqo uf at th ioe nn (o 2d 0e )V wZ ek fi. nT dhen,
p(v
)=(cid:88) p(cid:0) uZj(cid:1) p(cid:16) uZ\{j}(cid:17)
p(v |u,v )p(v |u)
Z Zj Z\{j} Z\{j}
u
=(cid:88) p(cid:0) uZj(cid:1) p(cid:16) uZ\{j}(cid:17) p(v |uZj,v )p(v |u),
Zj PAG
Zj
Z\{j}
u
wherethelatterequationcomesfromtheglobalMarkovpropertyonthegraphG. Further,byassump-
tionofV PAG
j
d GUj,weknowthatUZj V Zk,k ̸=j,suchthatp(v Z\{j}|u)=p(v Z\{j}|uZ\{j}).
Then:
p(v )=(cid:88) p(cid:0) uZj(cid:1) p(cid:16) uZ\{j}(cid:17) p(v |uZj,v )p(v |uZ\{j})
Z Zj PAG
Zj
Z\{j}
u
=(cid:88) p(cid:0) uZj(cid:1) p(v |uZj,v ) (cid:88) p(cid:16) uZ\{j}(cid:17) p(v |uZ\{j})
Zj PAG
Zj
Z\{j}
uZj uZ\{j}
(cid:88) (cid:16) (cid:17)
=p(v |v ) p uZ\{j} p(v |uZ\{j}),
Zj PAG
Zj
Z\{j}
uZ\{j}
whichprovestheclaim.
16
=|
=|
=|
=|
=|
=|
=|
=|Intuitively,Lemma4showsthatgivenanodeV withoutchildrenandbidirectededgesinamarginal-
j
izedgraphMG ,thekernelofnodeV intheMarkovfactorizationofp(v )isequaltothekernelof
VZ j Z
thesamenodeintheMarkovfactorizationofp(x)ofequation(2),relativetothegraphwithoutlatent
confoundersG.
B Proofsoftheoreticalresults
B.1 ProofofProposition1
ProofofProposition1. Observethat
∂2
logp(v )=0 ⇐⇒ V dV |V \{V ,V } ⇐⇒ V m V |V \{V ,V }, ∂V ∂V Z i G j Z i j i MG j Z i j
i j V
wherethefirstequivalenceholdsbyacombinationofthefaithfulnessassumptionwiththeglobal
Markovproperty,asexplicitinequation(3),andthesecondduetoLemma1. Then,theclaimis
proven.
B.2 ProofofProposition2
Proof. The forward direction is immediate from equation (9) and R = N , when X is a sink
j j j
(equation(11)). Thus,wefocusonthebackwarddirection. Given
(cid:104)(cid:0) (cid:2) (cid:3) (cid:1)2(cid:105)
E E ∂ logp(X)|R =r −∂ logp(X) =0,
Xj j j Xj
wewanttoshowthatX hasnochildren,whichweprovebycontradiction.
j
Letusintroduceafunctionq :R→Rsuchthat:
(cid:2) (cid:3)
E ∂ logp(X)|R =r =q(r ),
Xj j j j
ands :R|X| →R,
j
s (x)=∂ logp(x).
j Xj
Themeansquarederrorequaltozeroimpliesthats (X)isaconstant,onceR isobserved. Formally,
j j
undertheassumptionofp(x)>0foreachx∈Rk,thisimpliesthat
p(s (x)̸=q(R )|R =r )=0,∀x∈Rk.
j j j j
Bycontradiction,weassumethatX isnotaleaf,andwanttoshowthats (X)isnotconstantinX,
j j
givenR fixed. LetX suchthatX ∈X . Beingthestructuralcausalmodelidentifiable,there
j i j PAG
i
isnomodelwithdistributionp whosegraphhasabackwardedgeX → X : thus,theMarkov
X i j
factorizationofequation(2)isuniqueandimplies:
(cid:88)
∂ logp(X)=∂ logp(N )− ∂ h (X )∂N logp(N ).
Xj Nj j Xj k PAk k k
k∈CHG
j
Wenotethat,bydefinitionofresidualinequation(10),R =r fixesthefollowingdistance:
j j
R =N −E[N |X ].
j j j \Xj
Hence,conditioningonR doesn’trestrictthesupportofX: givenR = r ,foranyx (value
j j j \Xj
ofthevectorofelementsinX \{X }),∃n withp(n >0)(bythehypothesisofstrictlypositive
j j j
densitiesofthenoiseterms)thatsatisfies
r =n −E[N |x ].
j j j \Xj
Next, weconditiononalltheparentsofX , exceptforX , toreduceourproblemtothesimpler
i j
bivariate case. Let S ⊂ N and X ⊆ X such that X \{X } ⊆ X ⊆ X \{X ,X },
S PAG j S NDG i j
i i
and consider x such that p(x > 0). Let X = x hold under X = x . We define
S S PAG PAG S S
i i
X := X |(X = x ), and similarly X := X|(X = x ). Being the SCM a restricted
j|xs j S S |xs S S
17
=| =|additive noise model, by Definition 9, the triplet (g ,p ,p ) satisfies Condition 1, where
i Xj|xs Ni
g (x )=h (x ,x ). ConsiderX =x ,andthepairofvalues(x ,x∗)suchthatx ̸=x∗
i j i PAG i\{Xj} j i i j j j j
and
ν′′ (x −g (x ))g′(x )̸=0,
Ni i i j i j
ν′′ (x −g (x∗))g′(x∗)̸=0,
Ni i i j i j
whereweresorttotheusualnotationν :=logp . ByLemma2,(x ,x )and(x ,x∗)satisfy:
Ni Ni i j i j
∂ (ξ′(x )−ν′ (x −g (x ))g′(x ))̸=0,
Xj j Ni i i j i j
∂ (ξ′(x∗)−ν′ (x −g (x∗))g′(x∗))̸=0,
Xj j Ni i i j i j
whereξ :=logp . Thus,wecanfixx andx∗(whicharearbitrarilychosen)suchthat
Xj|xs j j
∂ (ξ′(x )−ν′ (x −g (x ))g′(x ))−∂ (ξ′(x∗)−ν′ (x −g (x∗))g′(x∗))̸=0. (21)
Xj j Ni i i j i j Xj j Ni i i j i j
FixingX =xandX =x∗,wherethetwovaluesdifferonlyintheirj-thcomponent,we
|xS,xj |xS,x∗
j
findthefollowingdifference:
s (x)−s (x∗)=∂ (ξ′(x )−ν′ (x −g (x ))g′(x ))−∂ (ξ′(x∗)−ν′ (x −g (x∗))g′(x∗)),
j j Xj j Ni i i j i j Xj j Ni i i j i j
whichisdifferentfrom0byequation(21). Thiscontradictsthefactthatthescores isconstantonce
j
R isfixed,whichprovesourclaim.
j
B.3 ProofofProposition3
Inthisproof,weuseseveralideasfromthedemonstrationofProposition2. Wedemonstratethe
forwardandthebackwardpartsofthetwostatementsseparately.
Proofofpart(i),forwarddirection. GivenV = V ∪{V ,V }andr ∈ RintheimageofR ,
Z PAG i j j j
j
wewanttoshow:
V dUj ∧V ∈V =⇒E[∂ logp(V )−E[∂ logp(V )|R (V )=r ]]2 =0. PAG
j
G i PAG
j
Vj Z Vj Z j Z j
ByLemma4,thescoreofV is
j
∂ logp(V )=∂ logp(V |V )+∂ logQ(V )
Vj Z Vj j PAG
j
Vj Z
=logp(N˜ ),
j
for some Q map acting on V . The latter equality holds because all variables in V are non-
Z Z
descendantsofV ,suchthat∂ Q(V )=0. Further,byequation(16)weknowthat
j Vj Z
R (V )=N˜ +c,
j Z j
wherec=−E[N˜ ]isaconstant. ItfollowsthattheleastsquareestimatorofthescoreofV from
j j
R (V )satisfiesthefollowingequation:
j Z
E[∂ logp(V )|R (V )]=E[∂ logp(N˜ )|N˜ ]=∂ logp(N˜ ),
Vj Z j Z Vj j j Vj j
wherethefirstequalityholdsbecauseE[·|N˜ ]=E[·|N˜ +c]. Then,wefind
j j
E[∂ logp(V )−E[∂ logp(V )|R (V )=r ]]2 =E[∂ logp(N˜ )−∂ logp(N˜ )]2 =0,
Vj Z Vj Z j Z j Vj j Vj j
whichisexactlyourclaim.
Proofofpart(i),backwarddirection. GivenV =V ∪{V ,V },r ∈RintheimageofR ,and
Z PAG i j j j
j
E[∂ logp(V )−E[∂ logp(V )|R (V )=r ]]2 =0, (22)
Vj Z Vj Z j Z j
wewanttoshowthatV dUj ∧V ∈ V ,meaningthatthereisadirectcausaleffectthat PAG G i PAG
j j
is not biased by unobserved variables. We provide the proof by contradiction, in analogy to the
demonstrationofthebackwarddirectionofProposition2.
18
=|
=|Letusintroduces
j
:R|VZ| →R,
s (v )=∂ logp(V ).
j Z Vj Z
Themeansquarederrorequaltozeroimpliesthats (V )isconstantinV ,onceR isobserved.
j Z Z j
Bycontradiction,weassumethatV ̸ dUj ∨V ̸∈V ,andwanttoshowthats (V )isnot PAG G i PAG j Z
j j
constantinV ,givenR fixed. Inthisregard,wemakethefollowingobservation: bydefinitionof
Z j
residualinequation(15),R (V )=r fixesthefollowingdistance:
i Z i
R (V )=N˜ −E[N˜ |V ].
j Z j j Z\{j}
Hence,conditioningonR (V )doesn’trestrictthesupportofV : givenR (V ) = r ,∃n˜ with
j Z Z j Z j j
p(n˜ )>0(byassumptionofstrictlypositivedensitiesp andp ),thatsatisfies
j Nj X
r =n˜ −E[N˜ |v ],
j j j Z\{j}
forallv . Hence,therandomvariableV |R (V )=r hasstrictlypositivedensityonallpoints
Z\{j} Z j Z j
v wherep (v )>0. Now,considerv andv∗,takenfromthesetofuncountablevaluessuchthat
Z VZ Z Z Z
thescores functionisnotaconstant,meaningthats (v )̸=s (v∗),whereV issampledgiven
j j Z j Z Z
R (V )=r . Giventhatdifferentv andv∗ areselectedfromanuncountablesubsetofthesupport,
j Z j Z Z
weconcludethatthescores |(R (V )=r )=∂ logp(V |R (V )=r )isnotaconstantforat
j j Z j Vj Z j Z j
leastanuncountablesetofpoints,whichcontradictsequation(22).
Proofofpart(ii),forwarddirection. GiventhatV isconnectedtoV inthemarginalMAGandthat
i j
V ̸ dUj ∨V ̸∈ V , we want to show that for each V ⊆ V with {V ,V } ⊆ V , the PAG G i PAG Z i j Z
j j
followingholds:
E[∂ logp(V )−E[∂ logp(V )|R (V )=r ]]2 ̸=0. (23)
Vj Z Vj Z j Z j
Letusintroduceh:R→Rsuchthat:
E[∂ logp(V )|R (V )=r ]=h(r ),
Vj Z j Z j j
andfurtherdefine:
s (V )=∂ logp(V ).
j Z Vj Z
Havingthemeansquarederrorinequation(23)equalszeroimpliesthats (V )isaconstant,once
j Z
R (V )isobserved. Thus,thegoaloftheproofistoshowthattherearevaluesofV suchthatthe
j Z Z
scoreisnotaconstantonceR isfixed. Bydefinitionofresidualinequation(15),R (V )=r fixes
j j Z j
thefollowingdistance:
R (V )=N˜ −E[N˜ |V ].
j Z j j Z\{j}
Hence,conditioningonR (V )doesn’trestrictthesupportofV : givenR (V ) = r ,∃n˜ with
j Z Z j Z j j
p(n˜ )>0(byassumptionofpositivedensityofthenoiseN onthesupportR),thatsatisfies
j j
r =n˜ −E[N˜ |v ],
j j j Z\{j}
forallv . Hence,therandomvariableV |R (V )=r hasstrictlypositivedensityonallpoints
Z\{j} Z j Z j
v wherep (v )>0. Now,considerv andv∗,takenfromthesetofuncountablevaluessuchthat
Z VZ Z Z Z
thescores functionisnotaconstant,meaningthats (v )̸=s (v∗),whereV issampledgiven
j j Z j Z Z
R (V )=r . Giventhatdifferentv andv∗ areselectedfromanuncountablesubsetofthesupport,
j Z j Z Z
weconcludethatthescores |(R (V )=r )=∂ logp(V |R (V )=r )isnotaconstantforat
j j Z j Vj Z j Z j
leastanuncountablesetofpoints,suchthattheclaimfollows.
Proofofpart(ii),backwarddirection. Given that E[∂ logp(V ) − E[∂ logp(V )|R (V ) =
Vj Z Vj Z j Z
r ]]2 ̸= 0forallV ⊆ V suchthat{V ,V } ∈ V , andgivenV andV adjacentinthemarginal
j Z i j Z i j
MAG,wewanttoshowthat
V ̸ dUj ∨V ̸∈V . PAG G i PAG
j j
Theprovecomeseasilybycontradiction: saythatV dUj ∧V ∈V . Then,bytheforward PAG G i PAG
j j
directionofpart(i)ofProposition3,weknowthatV =V ∪{V }satisfiesE[∂ logp(V )−
Z PAG
j
j Vj Z
E[∂ logp(V )|R (V )=r ]]2 =0,leadingtoacontradiction.
Vj Z j Z j
19
=|
=|
=|
=|C Algorithm
C.1 Detaileddescriptionofouralgorithm
InProposition1wehaveseenthatscorematchingcandetectm-separationsandthereforetheskeleton
ofthePAGdescribingthedata. IfoneiswillingtomaketheassumptionsrequiredforProposition3
it could be desirable to use this to orient edges, since the interpretation of PAG edges might be
cumbersomeforpeoplenotfamiliarwithancestralmodels. Therefore,onecouldsimplyfindthe
skeleton of the PAG using the fast adjacency search [5] and then orient the edges by applying
Proposition3oneverysubsetoftheneighbourhoodofeverynode. Thiswouldyieldaverycostly
algorithm. ButifwemaketheassumptionsrequiredtoorientedgeswithProposition3wecandoa
bitbetter. InAlgorithm2wepresentanalgorithmthatstillhasthesameworstcaseruntimebutruns
polynomiallyinthebestcase. Themainintuitionisthatweiterativelyremoveirrelevantnodesinthe
spiritoftheoriginalSCOREalgorithm[15]. Tothisend,wefirstcheckiftheisanyunconfounded
sinkifweconsiderthesetofallremainingvariables. Ifthereisone,wecanorientitsparentsand
ignoreitafterwards. Ifthereisnosuchset,weneedtofallbacktotheprocedureproposedabove,i.e.
weneedtochecktheconditionofProposition3onallsubsetsoftheneighbourhoodofanode,until
wefindnonodewithadirectoutgoingedge. InProposition4weshowthatthiswaywedonotfail
orientedgeorfailtoremoveanyadjacency. Inthefollowingdiscussion,wewillusethenotation
δ (X ):=E[∂ logp(V )−E[∂ logp(V )|R (V )=r ]]2,
i Z Vj Z Vj Z j Z j
forthesecondresidualfromProposition3andalso
∂2
δ (X ):= logp(v )
i,j Z ∂V ∂V Z
i j
forthecross-partialderivative,whereX ,X ∈V andZ ⊆V.
i j
Proposition4(Correctnessofalgorithm). LetX =V∪˙U begeneratedbytheSCMinEquation(4)
withnon-constantscoresforuncountablymanyvalues. LetG bethecausalDAGofX andG be
X V
themarginalMAGofG . ThenAlgorithm2outputsadirectededgefromX ∈ V toX ∈ V iff
X i j
thereisadirectedgeinG betweenthemandnounobservedbackdoorpathw.r.t. U. Further,the
X
outputofAlgorithm2hasthesameskeletonasG .
V
Proof. We proof the statement by induction over the steps of the algorithm. Let S be the set of
remainingnodesinanarbitrarystepofthealgorithm.OurinductionhypothesisisthatforX ,X ∈S
i j
andX ∈B wehave
k i
1. X isanunconfoundedsinkw.r.t. tosomesetS′ ⊆S iffX isanunconfoundedsinkw.r.t.
i i
someS′′ ⊆V
2. ifthereisnoS′ ⊆V \{X ,X }suchthatX X |S′thenX ∈B i j i j j i
Clearly,thisholdsintheinitialstepasS =V.
Suppose we find δ (X ) = 0 for X ∈ S. If X has at least one adjacent node in MG, by
i S i i V
Proposition3,weknowthatX doesnothaveanychildrenandisalsonotconnectedtoanyother
i
node in S via a hidden mediator or unobserved confounder. This means, all nodes that are not
separablefromX mustbedirectparentsofX ,whicharebyourinductionhypothesis2)thenodes
i i
inB . SinceX doesnothavechildren,italsosufficestocheckX X |S\{X ,X }forX ∈B i i i j i j j i
(insteadofconditioningonallsubsetsofB ). Sowecanalreadyaddthesedirectedgestotheoutput.
i
If,ontheotherhand,X hasnoadjacentnodesinMG,wehaveX X |S\{X ,X }forX ∈B , i V i j i j j i
soinbothcasesweaddthecorrectsetofparents. SinceX isnotanancestorofanyofthenodesin
i
S\{X },X cannotbeahiddenmediatororhiddenconfounderbetweennodesinS\{X }and
i i i
conditioningonX cannotblockanopenpath. Thus,theinductionhypothesisstillholdsinthenext
i
step.
SupposenowthereisnounconfoundedsinkandweexploreX . Byourinductionhypothesis2),B
i i
containstheparentsofX andbyProposition3itsufficestoonlylookatsubsetsofB toorientdirect
i i
edges. Andalsoduetotheinductionhypothesis2)B containsallnodesthatarenotseparablefrom
i
X . SobyaddingbidirectededgestoallnodesinB canonlyaddtoomanyedgesbutnotmisssome.
i i
20
=|
=|
=|Algorithm2AdaScoreAlgorithm
procedureADASCORE(p,X 1,...,X d)
S ←{X ,...,X } ▷Remainingnodes
1 d
E ←{} ▷Edges
forX ∈S do
i
B ←{X ,...,X } ▷Neighbourhoods
i 1 d
whileS ̸=∅do ▷Whilenodesremain
if ∃X ∈S :δ (X )=0then ▷Ifthereisanunconfoundedsink
i i S
S ←S\{X }
i
E ←E∪{X →X :δ (X )̸=0} ▷AddedgeslikeDAS
j i i,j S
else
forX ∈S do
i
forX ∈B do ▷Pruneneighbourhoods
j i
ifδ (X )=0then
i,j S
B ←B \{X }
i i j
B ←B \{X }
j j i
forX ∈B do ▷OrientedgesinB
j i i
m =min δ (X )
i S′⊆Bi i S′∪{Xi}
m =min δ (X ))
j S′⊆Bj j S′∪{Xj}
ifm =0∧m ̸=0then
i j
E ←E∪{X →X }
j i
elseifm ̸=0∧m =0then
i j
E ←E∪{X →X }
i j
else
E ←E∪{X ↔X }
i j
if∃X ∈B :(X →X )∈E then
j i i j
continuewith X
j
else ▷X hasnounconfoundedoutgoingedge
i
S ←S\{X } ▷RemoveX
i i
break
forX ↔X ∈E do ▷Prunebidirectededges
i j
ifmin δ (X )=0∨min δ (X )=0then
S′⊆Adj(Xi) i,j S′∪{Xi} S′⊆Adj(Xj) i,j S′∪{Xi}
E ←E\{X ↔X }
i j
returnE
NowitremainstoshowthattheinductionhypothesisholdsifwesetS toS\{X }. For1)weneed
i
toshowthatX cannotbeahiddenmediatororhiddenconfounderw.r.t. S\{X }(sinceignoring
i i
X won’tchangewhetherthereisadirectedgeornot). SupposeX isonaunobservedcausalpath
i i
X →···→Um →X withX ,X ∈S\{X }andUm ∈X\(S\{X }). Thispathmusthave
k l k l i i
beenaunobservedcausalpathbefore,unlessX =Um. ButthenthereisadirectedgeX →X .
i i l
WewouldnotremoveX fromS ifthisedgewasunconfounded,sotheremustahiddenconfounder
i
betweenX andX . Butinthiscase,Proposition3wouldn’tallowustodirecttheedgeanyway,since
i l
V ̸ dU . SupposethereisconfoundingpathX ←···→Um →X withX ,X ∈S\{X } PAl G l k l k l i
andUm ∈X\(S\{X }). IfX ̸=UmthepathwasalreadybeenaconfoundingpathwithoutX
i i i
beingunobserved. Soagain,theremustbeaconfounderbetweenX andX ,asotherwisewewould
i l
notremoveX . Andanalogouslytobefore,wecouldnothaveorientedtheedgeevenwithX ∈S
i i
sinceV ̸ dU . For2)weonlyhavetoseethatwejustremovenodesfromB ifwefoundan PAl G l i
independence.
For |S| < 2, the algorithm enters the final pruning stage. From the discussion above it is clear,
that we already have the correct result, up to potentially too many bidirected edges. In the final
stepwecertainlyremovealltheseedgesX ↔X ,aswecheckm-separationforallsubsetsofthe
i j
neighbourhoodsAdj(X )andAdj(X ),whicharesupersetsofthetrueneighbourhoods.
i j
21
=|
=|C.2 FinitesampleversionofAdaScore
Alltheoreticalresultsinthepaperhaveassumedthatweknowthedensityofourdata. Obviously,in
practisewehavetodealwithafinitesampleinstead. Especially,inProposition1andProposition3
wederivedcriteriathatcomparerandomvariableswithzero. Clearly,thisconditionisnevermetin
practise. Therefore,weneedfindwaystoreasonablysetthresholdsfortheserandomquantities.
Firstnote,thatweusetheSteingradientestimator[35]toestimatethescorefunction. Thismeans
especiallythatforanodeV wegetavector
i
(cid:18) (cid:19)
∂
( logp(v)) , (24)
∂V l
i l=1,...,m
i.e. anestimateofthescoreforeveryoneofthemsamples. Analogously,wegetam×d×dtensor
fortheestimatesof ∂2 logp(v).
∂Vi∂Vj
InProposition1weshowedthat
∂2
logp(v )=0 ⇐⇒ X m V |V \{V ,V }. ∂V ∂V Z i MG j Z i j
i j V
In the finite sample version, we use a one sample t-test on the vector of estimated cross-partial
derivativeswiththenull-hypothesisthatthemeansiszero. Duetothecentrallimittheorem, the
samplemeanfollowsapproximatelyaGaussiandistribution,regardlessofthetruedistributionofthe
observations.
ForProposition3weneedtodosomeadditionalsteps. Recall,thattherelevantquantityinPropo-
sition3isthemeansquarederrorofaregression, whichisalwayspositive. Therefore, atestfor
meanzeroishighlylikelytorejectinanycase. Wedecidedtoemployatwo-sampletestinasimilar
(butdifferent)mannerasMontagnaetal.[17]. Astest,weusedtheMann-WhitneyU-test. Note,
thatAlgorithm2employsProposition3intwodifferentways: first,todecidewhetherthereisan
unconfoundedsinkandsecond,toorientedgesincasethereisnounconfoundedsink. Wepicka
differentsampleassecondsampleoftheMann-WhitneyU-test.
Analogouslytobefore,thisisavectorwithmentries,oneforeverysample.
Note,thatinthecasewherewewanttocheckifthereisanunconfoundedsink,wedonotmakeany
mistakebyrejectingtoofewhypotheses,i.e. ifwemisssomeunconfoundedsinks(instead,weonly
loseefficiency,aswedothecostlyiterationoverallpossiblesetsofparents). Therefore,forthistest
wechoseaasecondsamplethatyieldsa“conservative”testresult.
AscandidatesinkforsetS ⊆V,wepickthenodeX =min mean(δ (X )). Infact,wewantto
i i i S
knowwhetherthemeanofδ issignificantlylowerthanallothermeans. Butweempiricallyobserved
i
thatchoosingtheconcatenatedδsofallnodesassecondsamplemakesthetestrejectwithveryhigh
probability,whichwouldleadouralgorithmtofalselyassumetheexistenceofanunconfoudnedsink.
Instead,wethenpickassecond“referencenode”X =min mean(δ (X )). Wethendothetwo
j j̸=i j Z
sampletestbetweenδ (X )andδ (X ). Theintuitionisthatthetestwillrejectthehypothesisof
i Z j Z
identicalmeans,ifX isanunconfoundedsinkbutX isnot.
i j
In the case where we use Proposition 3 to orient edges, we only need to decide whether an not
previsoulydirectededgeX −X needstobeorientedoneway,theotherway,ornotatall. Instead,
i j
heretheissueliesinthefactthatweneedtoiterateoverpossiblesetsofparentsofthenodes. Let
B be the set of nodes that have not been m-separated from X by any test so far. We pick the
i i
subsetZ =min mean(δZ′),i.e. thesetwiththelowestmeanerror. Wethenconductthetest
i Z′⊆Bi i
withδ (X )andδ (X ). Ifthereisadirectededgebetweenthem,oneoftheresidualswillbe
i Zi j Zj
significantlylowerthantheother.
JustlikeMontagnaetal.[17]weuseacross-validationschemetogeneratetheresiduals,inorderto
preventoverfitting. Wesplitthedatasetintoseveralequallysized,disjointsubsamples. Forevery
residualwefittheregressiononallsubsamplesthatdon’tcontaintherespectivetarget.
Also,justlikeintheNoGAMalgorithmMontagnaetal.[17]weaddapruningstepforthedirected
edgestotheend. Theideaistouseafeatureselectionmethodtoremoveinsignificantedges. Justlike
Montagnaetal.[17],weusetheCAM-basedpruningstepproposedbyBühlmannetal.[36],which
fitsageneralisedadditiveregressionmodelfromtheparentstoachildandtestwhetheroneofthe
22
=|additivecomponentsissignificantlynon-zero. Allparentsforwhichthetestrejectsthishypothesis
areremoved.
C.3 Complexity
Proposition5. ComplexityLetnbethenumberofsamplesanddthenumberofobservablenodes.
Algorithm2runsin
Ω(cid:0) (d2−d)·(r(n,d)+s(n,d))(cid:1)
and
O(cid:0) d2·2d(r(n,d)+s(n,d))(cid:1)
,
wherer(n,d)isthetimerequiredtosolvearegressionproblemands(n,d)isthetimeforcalculating
thescore. Withe.g. kernel-ridgeregressionandtheStein-estimator,bothruninO(n3).
Proof. Algorithm2runsitsmainloopdtimes. Itfirstchecksfortheexistenceofanunconfounded
sink, which involves solving 2d regression problems (including cross-validation prediction) and
calculatingthescore,addingupto(2d2−d)regressionsanddscoreevaluations. Intheworstcase,
we detect no unconfounded sink and iterate through all subsets of the neighbourhood of a node
(whichisintheworstcaseofsized−1)andforallothernodesintheneighbourhoodwesolve2d
regressionproblemsandevaluatethescore. Foreachsubsetwecalculatetworegressionfunctions,
thescoreandcalculatetheentriesintheHessianofthelog-density,i.e. d·2dregressions,d·2d−1
scoresandadditionally2d−1 Hessians. Ifweareunlucky,thisnodehasadirectedoutgoingedge
andwecontinuewiththisnode(withthesamesizeofnodes). Thiscanhappend−1times. Sowe
get(d2−d)·2dregressionsand(d2−d)·2d−1scoresandHessians. Inthefinalpruningstepwe
calculateforeverybidirectededge(ofwhichtherecanbe(d2−d)/2)aHessianforallsubsetsofthe
neighbourhoods,whichcanagainbe2d−1subsets. UsingthepruningprocedurefromCAMforthe
directededgeswealsospendatmostO(nd3)steps.
Inthebestcase,wealwaysfindanunconfoundedsink. ThenouralgorithmreducestoNoGAM.
D Experimentaldetails
Inthissection,wepresentthedetailsofourexperimentsintermsofsyntheticdatagenerationand
algorithmshyperparameters.
D.1 Syntheticdatageneration
Inthiswork,werelyonsyntheticdatatobenchmarkAdaScore’sfinitesamplesperformance. For
eachdataset,wefirstsamplethegroundtruthgraphandthengeneratetheobservationsaccordingto
thecausalgraph.
Erdös-Renyigraphs. ThegroundtruthgraphsaregeneratedaccordingtotheErdös-Renyimodel.
Itallowsspecifyingthenumberofnodesandtheprobabilityofconnectingeachpairofnodes). InER
graphs,apairofnodeshasthesameprobabilityofbeingconnected.
Nonlinearcausalmechanisms. Nonlinearcausalmechanismsareparametrizedbyaneuralnetwork
withrandomweights. Wecreateafullyconnectedneuralnetworkwithonehiddenlayerwith10
units,ParametricReLUactivationfunction,followedbyonenormalizinglayerbeforethefinalfully
connectedlayer.TheweightsoftheneuralnetworkaresampledfromastandardGaussiandistribution.
Thisstrategyforsyntheticdatagenerationiscommonlyadoptedintheliterature[26,18,28,29,27].
Linearcausalmechanisms. Forthelinearmechanisms,wedefineasimplelinearregressionmodel
predictingtheeffectsfromtheircausesandnoiseterms,weightedbyrandomlysampledcoefficients.
CoefficientsaregeneratedassamplesfromaUniformdistributionsupportedintherange[−3,−0.5]∪
[0.5,3]. WeavoidtoosmallcoefficientstoavoidclosetounfaithfuldatasetsUhleretal.[24].
23Noisetermsdistribution. ThenoisetermsaresampledfromaUniformdistributionsupported
between−2and2.
Finally,weremarkthatwestandardizethedatabytheirempiricaldata. Thisisknowntoremove
shortcutsthatallowfindingacorrectcausalordersortingvariablesbytheirmarginalvariance,asin
varsortability,describedinReisachetal.[32],orsortingvariablesbythemagnitudeoftheirscore
|∂ logp(X)|,aphenomenonknownasscoresortabilityanalyzedbyMontagnaetal.[18].
Xi
D.2 AdaScorehyperparameters
ForAdaScore,wesettheαlevelfortherequiredhypothesistestingat0.05. FortheCAM-pruning
step,thelevelisinsteadsetat0.001,thedefaultvalueofthedodidscoverPythonimplementationof
themethod,andcommonlyfoundinallpapersusingCAM-pruningforedgeselection[15,16,17,36].
Fortheremainingparameters. Theregressionhyperparametersfortheestimationoftheresidualsare
foundviacross-validationduringinference: tuningisdoneminimizingthegeneralizationerroron
theestimatedresiduals,withoutusingtheperformanceonthecausalgraphgroundtruth. Finally,for
thescorematchingestimation,theregularizationcoefficientsaresetto0.001.
D.3 Computerresources
All experiments have been run on an AWS EC2 instance of type p3.2xlarge. These machines
containIntelXeonE5-2686-v4processorswith2.3GHzand8virtualcoresaswellas61GBRAM.
Allexperimentscanberunwithinaday.
E AdditionalExperiments
Inthissection,weprovideadditionalexperimentalresults. Allsyntheticdatahasbeengeneratedas
describedinAppendixD.1.
E.1 Non-additivemechanisms
InFigure1wehavedemonstratedtheperformanceofourproposedmethodondatageneratedby
linearSCMsandnon-linearSCMswithadditivenoise. ButProposition1alsoholdsforanyfaithful
distributiongeneratedbyanacyclicmodel.Thus,weemployedasmechanismaneuralnetwork-based
approachsimilartothenon-linearmechanismdescribedinAppendixD.Insteadofaddingthenoise
term,wefeeditasadditionalinputintotheneuralnetwork. Resultsinthissettingarereportedin
Figure2. AsneitherAdaScorenoranyofthebaselinealgorithmshastheoreticalguaranteesforthe
orientationofedgesinthisscenario,wereporttheF -score(popularinclassificationproblems)w.r.t.
1
totheexistenceofanedge,regardlessoforientation. OurexperimentsshowthatAdaScorecan,in
general,correctlyrecoverthegraph’sskeletoninallthescenarios,withanF scoremedianbetween
1
1and∼0.75,respectivelyforsmallandlargenumbersofnodes.
E.2 Sparsegraphs
Inthissection,wepresenttheexperimentsonsparseErdös-Renyigraphswhereeachpairofnodes
is connected by an edge with probability 0.3. The results are illustrated in Figure 3. For sparse
graphs,recoveryresultsaresimilartothedensecase,withAdaScoregenerallyprovidingcomparable
performancetotheothermethods.
E.3 Increasingnumberofsamples
Inthefollowingseriesofplotswedemonstratethescalingbehaviourofourmethodw.r.t. tothe
numberofsamples. Figure5showsresultswithedgeprobability0.5andFigure4with0.3. All
graphscontainsevenobservablenodes. AsbeforeweobservethatAdaScoreperformscomparablyto
othermethods. E.g. inFigures4aand5bwecanseethatthemedianerrorAdaScoreimproveswith
additionalsamplesandinallplotsweseethatnootheralgorithmseemstogainanadvantageover
AdaScorewithincreasingsamplesize.
24adascore camuv nogam rcd lingam
sparse dense
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
3 5 7 9 3 5 7 9
number of nodes number of nodes
(a)Fullyobservablemodel
sparse dense
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
3 5 7 9 3 5 7 9
number of nodes number of nodes
(b)Latentvariablesmodel
Figure2: Empiricalresultsfornon-additivecausalmechanismsonsparsegraphswithdifferentnumbersof
nodes,onfullyobservable(nohiddenvariables)andlatentvariablemodels.WereporttheF scorew.r.t.the
1
existenceofedges(thehigher,thebetter).
adascore camuv nogam rcd lingam
linear nonlinear
25
16
14 20
12
10 15
8
10
6
4 5
2
0 0
3 5 7 9 3 5 7 9
number of nodes number of nodes
(a)Fullyobservablemodel
linear nonlinear
25 25
20 20
15 15
10 10
5 5
0 0
3 5 7 9 3 5 7 9
number of nodes number of nodes
(b)Latentvariablesmodel
Figure3:Empiricalresultsonsparsegraphswithdifferentnumbersofnodes,onfullyobservable(nohidden
variables)andlatentvariablemodels.WereporttheSHDaccuracy(thelower,thebetter).
25
1f_noteleks
1f_noteleks
dhs
dhs
1f_noteleks
1f_noteleks
dhs
dhsadascore camuv nogam rcd lingam
linear nonlinear
14 20.0
12 17.5
15.0
10
12.5 8
10.0
6
7.5
4 5.0
2 2.5
0 0.0
500 1000 1500 2000 500 1000 1500 2000
number of samples number of samples
(a)Fullyobservablemodel
linear nonlinear
20.0 20.0
17.5 17.5
15.0 15.0
12.5 12.5
10.0 10.0
7.5 7.5
5.0 5.0
2.5 2.5
0.0 0.0
500 1000 1500 2000 500 1000 1500 2000
number of samples number of samples
(b)Latentvariablesmodel
Figure4: Empiricalresultsonsparsegraphswithdifferentnumbersofsamplesandsevennodes, onfully
observable(nohiddenvariables)andlatentvariablemodels.WereporttheSHDaccuracy(thelower,thebetter).
adascore camuv nogam rcd lingam
nonlinear linear
20.0 17.5
17.5 15.0
15.0 12.5
12.5 10.0
10.0
7.5
7.5
5.0 5.0
2.5 2.5
0.0 0.0
500 1000 1500 2000 500 1000 1500 2000
number of samples number of samples
(a)Fullyobservablemodel
nonlinear linear
20.0
20.0
17.5 17.5
15.0 15.0
12.5 12.5
10.0 10.0
7.5 7.5
5.0 5.0
2.5 2.5
0.0 0.0
500 1000 1500 2000 500 1000 1500 2000
number of samples number of samples
(b)Latentvariablesmodel
Figure 5: Empirical results on dense graphs with different numbers of samples and seven nodes, on fully
observable(nohiddenvariables)andlatentvariablemodels.WereporttheSHDaccuracy(thelower,thebetter).
26
dhs
dhs
dhs
dhs
dhs
dhs
dhs
dhsE.4 Limitations
Inthissection,weremarkthelimitationsofourempiricalstudy.Itiswellknownthatcausaldiscovery
lacksmeaningful,multivariatebenchmarkdatasetswithknowngroundtruth. Forthisreason,itis
commontorelyonsyntheticallygenerateddatasets.Webelievethatresultsonsyntheticgraphsshould
betakenwithcare,asthereisnostrongreasontobelievethattheyshouldmirrorthebenchmarked
algorithms’ behaviors in real-world settings, where often there is no prior knowledge about the
structuralcausalmodelunderlyingavailableobservations.
27