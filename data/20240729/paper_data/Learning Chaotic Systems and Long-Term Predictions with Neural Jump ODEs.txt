Preprint
LEARNING CHAOTIC SYSTEMS AND LONG-TERM PRE-
DICTIONS WITH NEURAL JUMP ODES
FlorianKrach JosefTeichmann
DepartmentofMathematics DepartmentofMathematics
ETHZurich ETHZurich
Zurich,Switzerland Zurich,Switzerland
florian.krach@math.ethz.ch josef.teichmann@math.ethz.ch
ABSTRACT
ThePath-dependentNeuralJumpODE(PD-NJ-ODE)isamodelforonlinepre-
dictionofgeneric(possiblynon-Markovian)stochasticprocesseswithirregular
(intime)andpotentiallyincomplete(withrespecttocoordinates)observations. It
isamodelforwhichconvergencetotheL2-optimalpredictor,whichisgivenby
theconditionalexpectation,isestablishedtheoretically. Thereby,thetrainingof
themodelissolelybasedonadatasetofrealizationsoftheunderlyingstochastic
process,withouttheneedofknowledgeofthelawoftheprocess. Inthecasewhere
theunderlyingprocessisdeterministic,theconditionalexpectationcoincideswith
theprocessitself. Therefore,thisframeworkcanequivalentlybeusedtolearnthe
dynamicsofODEorPDEsystemssolelyfromrealizationsofthedynamicalsys-
temwithdifferentinitialconditions. Weshowcasethepotentialofourmethodby
applyingittothechaoticsystemofadoublependulum. Whentrainingthestandard
PD-NJ-ODEmethod,weseethatthepredictionstartstodivergefromthetruepath
afterabouthalfoftheevaluationtime. Inthisworkweenhancethemodelwithtwo
novelideas,whichindependentlyofeachotherimprovetheperformanceofour
modellingsetup. Theresultingdynamicsmatchthetruedynamicsofthechaotic
systemveryclosely. Thesameenhancementscanbeusedtoprovablyenablethe
PD-NJ-ODEtolearnlong-termpredictionsforgeneralstochasticdatasets,where
thestandardmodelfails. Thisisverifiedinseveralexperiments.
1 INTRODUCTION
The Path-dependent Neural Jump ODE (PD-NJ-ODE) (Krach et al., 2022) is a model for online
predictionofgeneric(possiblynon-Markovian)stochasticprocesseswithirregularandpotentially
incompleteobservations. ItisthefirstmodelforwhichconvergencetotheL2-optimalpredictor,
whichisgivenbytheconditionalexpectation,isestablishedtheoretically. Thereby,thetrainingofthe
modelissolelybasedonadatasetofrealizationsoftheunderlyingstochasticprocess,withoutthe
needofknowledgeofthelawoftheprocess. Thisresultwasfurthergeneralizedinthefollow-up
workAnderssonetal.(2024). Inparticular,let(X ) beastochasticprocesstakingvaluesin
t t∈[0,T]
Rd,lett ∈[0,T]berandomobservationstimesfor1≤i≤n,wherencanbearandomvariable
i
describingthetotalnumberofobservations(i.e., thenumberofobservationscanbedifferentfor
each realization) and let M ∈ {0,1}d be the corresponding random observation masks, telling
i
whichcoordinatesX areobserved(ifM =1)ateachobservationtimet . Theσ-algebraofthe
ti,j ij i
currentlyavailableinformationatanytimet∈[0,T]isdefinedas
A :=σ(X ,t ,M |t ≤t, j ∈{1≤l≤d|M =1}),
t ti,j i ti i ti,l
whereσ(·)denotesthegeneratedσ-algebra. Then,Anderssonetal.(2024,Theorem4.3)statesthat
theoutputYθ mmi ,n Nm ofthePD-NJ-ODEmodel(whereθarethetrainableparametersofthemodel,m
isthesizeoftheusedneuralnetworksandN isthenumberoftrainingpaths,i.e.,realizationsofX)
m
convergestotheL2-optimalpredictionXˆ =(E[X |A ]) asmtendsto∞. Thisconvergence
t t t∈[0,T]
holdsunderweakassumptionsonX andtheobservationframework(t ,M ,n), whichbasically
i i
requiresomeintegrabilitypropertiesandcontinuousdifferentiabilityoft(cid:55)→Xˆ .
t
1
4202
luJ
62
]LM.tats[
1v80881.7042:viXraPreprint
In Krach et al. (2022); Andersson et al. (2024), the focus lies on optimal prediction of generic
stochasticprocesses,asforexampleprocessesdefinedviaanstochasticdifferentialequation,given
thecurrentlyavailableinformation. Inparticular,thismeansthatthemodelneverpredictsfurtherthan
untilthenextobservationtime,sincethentheavailableinformationchanges. Ifthenextobservation
timeisdeterministically(orwithveryhighprobability)smallerthanr,thenitisunlikelythatthe
modellearnstopredictwellfort>r,withoutgettingthenewinformationasinput,whenitbecomes
availableatthenextobservationtime. Inthiswork,wefocusonaprovabletrainingstrategy,that
makessuchlong-termpredictionspossible.
Thisisofparticularimportanceinthecaseofadeterministic(giventheinitialcondition)underlying
process,asin(chaotic)ODEorPDEsystems. Importantly,inthissettingtheconditionalexpectation
coincideswiththeprocessitself. Inparticular,ifX isobserved,i.e.,ifX isA -measurablefor
0 0 t
anyt∈[0,T],thenXˆ =E[X |A ]=X . Therefore,Anderssonetal.(2024,Theorem4.3)implies
t t t t
thatthePD-NJ-ODEframeworkcanequivalentlybeusedtolearnthedynamicsofODEorPDE
systemssolelyfromrealizationsofthedynamicalsystemwithdifferentinitialconditions. Thisresult
wasalreadystatedinKrachetal.(2022,AppendixB.3)andwasusedintheexperimentsinKrach
etal.(2022,AppendixC.3). Eventhoughthetheoreticalresultsarepromising,itcanbeseeninthe
empiricalresultsofKrachetal.(2022,AppendixC.3)(andinFigure1left)thatthePD-NJ-ODEhas
problemstopredictachaoticsystemwelloverlongertimeperiods,whenthepredictionisonlybased
ontheinitialvalue. Inparticular,thepredictionstartstodivergefromthetruepathafterabouthalf
oftheevaluationtime. Theproblemisthatduringthetraining,themodelneverneedstopredictso
farahead(sinceitgetsintermediateobservationsasinput). Hence,italsodoesnotlearntodothis
well. Inthiswork,weanalysethePD-NJ-ODEmodelfromtheperspectiveoflearninglong-term
predictionsofstochasticordeterministic(differential)systemsandintroducetwonovelideas,which
enhancethetrainingofthemodelsignificantlyandindependentlyofeachotherinthiscontext.
1.1 RELATEDWORK
ThisworkisbasedonthesequenceofpapersintroducingNJ-ODE(Herreraetal.,2021),extending
it to a path-dependent setting with incomplete observations (Krach et al., 2022) and further to
noisyobservationswithdependencebetweentheobservationframeworkandtheunderlyingprocess
(Andersson et al., 2024). The focus of this paper lies on long-term predictions (i.e., multiple
observationtimesahead),withaspecialemphasisonfullyobserved(chaotic)deterministicsystems.
Theframeworkof(Krachetal.,2022)alsoallowsforpartiallyobservedchaoticsystems,whichare
notdeterministic. Suchcasesresemblestochasticprocesses,wheretheoptimalprediction,givenby
theconditionalexpectation,islearnt. Hence,theycanbetreatedwiththeprovidedresultforgeneral
stochasticprocesses.
Navone&Ceccatto(1995)wereoneofthefirsttouseneuralnetworkstolearnchaoticdynamics
andseveralotherworksfollowedusingRNNs,reservoircomputingandneuralODEs(Vlachasetal.,
2018;Pathaketal.,2018;Vlachasetal.,2020;Brenneretal.,2022;Chenetal.,2018;Raissi,2018).
Churchill&Xiu(2022)proposeamemory-basedresidualdeepneuralnetwork(DNN)architecture
tolearnchaoticsystemsfromfullyorpartiallyobserveddataandapplyittothechaoticLorenz63
and 96 systems. Hess et al. (2023) use piece-wise linear RNNs together with teacher forcing to
effectivelylearnchaoticdynamicsandprovideanextensiveoverviewofrelatedworkandnumerical
comparisontomanystate-of-the-artmodels. Ourapproach,usingneuralODEs,isparticularlyrelated
toChenetal.(2018)andRubanovaetal.(2019). However, incontrasttoallthesemethods, our
approachcomeswiththeoreticallearningguaranteeseveninthemostgeneralcaseofirregularlyand
incompletelyobservedpath-dependentstochasticprocesses.
2 MAIN RESULTS
IntheresultsofKrachetal.(2022, AppendixC.3)weseethattheempiricalperformanceofthe
PD-NJ-ODEappliedtochaoticsystemscouldbeimproved,especiallyforlongpredictionhorizons,
eventhoughthetheoreticalresultssuggestthatthemodelshouldlearntopredictchaoticsystems
correctlyatanytime.Thisisrelatedtotheinductivebiaswhentrainingthemodelwithafiniteamount
oftrainingsamples(seeAnderssonetal.(2024,AppendixB)formoredetailsontheinductivebias
ofthePD-NJ-ODE).Inparticular,evenifthedistributionoftheobservationtimesissuchthatitis
(theoretically)possibletohaveverylongperiodswithoutobservations,theprobabilityofexperiencing
2Preprint
this necessarily becomes smaller the larger the period is. Hence, the respective training samples
wherethishappensarescarceandconsequentlytheempiricalresultsofthemodelfallshortofthe
theoreticalexpectations.
Therefore,wesuggesttwoenhancementsofthePD-NJ-ODEmodelforlearninglong-termpredictions
in deterministic (differential) as well as stochastic systems. In Section 2.1.1 we prove that in
the deterministic case, the model only taking the initial value as input (and potentially some of
the following ones), converges to the same limiting process as the standard model, since all the
observationsarestillusedinthelossfunction. Thisshouldimprovetheinductivebiasofthetraining,
sincethemodelisnowforcedtopredictfurtherintothefuture.InSection2.1.3weshowthatthesame
trainingenhancementalsoleadstoaccuratelong-termpredictionsinstochasticsystems. Moreover,
inSection2.2wediscussthatusingoutputfeedback (whichisknowntostabilizethetrainingof
dynamicalsystems)inthePD-NJ-ODEmodelframework,stillyieldsthesametheoreticalresults.
2.1 LONG-TERMPREDICTIONSWITHPD-NJ-ODE
Inourcontext,long-termpredictionsalwaysrefertopredictionswithinthetimehorizon[0,T],where
forany0≤s≤t≤T,theinformationavailableuptotimesisusedtopredicttheprocessattime
t. Thisisageneralizationofthestandardframework,wherewehaves=t,i.e.,wherepredictions
are based on all available information up to the prediction time. Importantly, we make no claim
fort > T. Toextendourresultsfort > T,additionalassumptionsonthetime-invarianceofthe
underlyingsystemwouldbenecessary,whichwedonotrequirehere.
We start by discussing the special case of deterministic (chaotic) systems in Section 2.1.1, then
proposeatrainingprocedureinSection2.1.2basedonthoseinsights,andfinallyshowthatthesame
methodalsoappliesinthegeneralcaseofstochasticsystemsinSection2.1.3.
2.1.1 THESPECIALCASEOFDETERMINISTICSYSTEMS
As described in Section 1, the PD-NJ-ODE is a model that can be used to predict a stochastic
processX givenitspreviousdiscreteandpossiblyincompleteobservationssummarizedinA forany
t
t∈[0,T]. Inparticularthismodeldirectlyuseseverynewobservationasinputwhentheobservation
becomesavailableandpredictsforalltimesafterwardsbasedadditionallyonthisnewobservation.
Inthesettingofstochasticprocessesthisbehaviourmakesperfectsense,sinceeverynewpieceof
informationchanges(improves)thefollowingforecasts. However,inthesettingofdeterministic
(differential)systems,whicharefullydeterminedbytheirinitialvalue,usingnewobservationsas
input for the PD-NJ-ODE model is (in principle) not needed, since they do not provide any new
informationaboutX. Inparticular,wehave
Xˆ =E[X |A ]=X =E[X |A ] (1)
t t t t t 0
if σ(X ) ⊆ A . This allows us to formulate the following corollary of Andersson et al. (2024,
0 0
Theorem4.3).
Corollary2.1. UnderthesameassumptionsasinAnderssonetal.(2024,Theorem4.3)withthe
additionalassumptionthatX isdeterministicgivenitsinitialvalueX 0,wedenotebyY˜θ mmi ,n Nm the
outputofthePD-NJ-ODEmodel,whereonlythefullyobservedinitialvalueX isusedasinputto
0
themodel(inthetraining). Then,thesameconvergenceresultholdsforY˜θ mmi ,n Nm asforYθ mmi ,n Nm in
Anderssonetal.(2024,Theorem4.3). Inparticular,Y˜θ mmi ,n Nm convergestoXˆ asm→∞.
Remark2.2. WeemphasizethatallavailableobservationsofX arestillusedinthelossfunctionto
trainthemodel,theyareonlynotusedasinputtothemodel. Therefore,westillhaveconvergencein
themetricsd forall1≤k ≤K.
k
ProofofCorollary2.1. FirstnotethatX beingfullyobservedimpliesthatσ(X )=A . Revisiting
0 0 0
the proof of Andersson et al. (2024, Theorem 4.3), it is easy to see that the L2-optimal σ(X )-
0
measurableprediction(E[X |A ]) ofX istheuniqueminimizer(uptoindistinguishability)
t 0 t∈[0,T]
ofthelossfunctionamongstallσ(X )-measurableprocesses. Moreover,itfollowsasbeforethat
0
the PD-NJ-ODE model can approximate (E[X |A ]) arbitrarily well. Therefore, training
t 0 t∈[0,T]
the PD-NJ-ODE model, which only takes X as input, with the same training framework yields
0
3Preprint
convergenceofY˜θ mmi ,n Nm to(E[X t|A 0]) t∈[0,T]. Finally,Equation(1)impliesthatY˜θ mmi ,n Nm actually
convergestoXˆ =(E[X |A ]) .
t t t∈[0,T]
Clearly,foranyPD-NJ-ODEmodel,takingX andsomeofthefollowingobservationsasinput,the
0
sameconvergenceresultholds,sincetheresultholdsforthetwoextremecasesofmodelstakingall
ornoneofthefollowingobservationsasinput.
WhileCorollary2.1mightseemtobeatrivialextensionoftheoriginalresult,itspracticalimportance
islargeinthecontextoflearningdeterministic(differential)systems. Asoutlinedinthebeginningof
Section2,themodelwhichonlytakesX asinputisforcedtolearntopredictwellovertheentire
0
timeperiod. Hence,weeffectivelyimprovetheinductivebiasofthemodelwithoutchangingthe
theoreticalguarantees.
2.1.2 SUGGESTEDTRAININGPROCEDURE
We note that using the observations as input is not only disadvantageous but also has a positive
effectontheinductivebias. Inparticular,every(full)observationthatisusedasinputforthemodel
basicallyamountstousingthisobservationasnewinitialvalueforthesystem,hence,increasing
theamountofinitialvaluesusedtotrainthePD-NJ-ODEmodel. Thisisparticularlyusefulinthe
beginningofthetraining. Therefore,weintroduceaprobabilityp ∈ [0,1]andusei.i.d.Bernoulli
randomvariablesI ∼Ber(p),whichdecidewhetheranobservationisusedasinputtothemodel
k
duringtraining. Bydecreasingtheprobabilitypthroughoutthetrainingwecanthereforefirstuse
theobservationsasadditionalinitialvaluesandthenfocusthetrainingmoreandmoreonpredicting
welloveralongtimeperiod. Sincethereexistsonesolutionwhichisoptimalforallp∈[0,1],this
procedureadditionallyencouragesthemodeltolearnit. Theeffectivenessofthisprocedurecanbe
seeninSection3. Nevertheless,wenotethattheoretically,choosinganyfixedp∈(0,1)leadstothe
sameoptimalsolution,asproveninthefollowingsection.
2.1.3 GENERALSTOCHASTICSYSTEMS
Similarly as in the case of a deterministic (chaotic) X, also in the stochastic case, we might be
interestedinlearningtopredictmultipletimestepsahead.Inthestandardframework,thePD-NJ-ODE
modelonlylearnstopredictuntilthenextobservationtime,sinceitconvergestoE[X |A ],whichis
t t
theoptimalpredictionofX givenallinformationavailableuptot,i.e.,allinformationgatheredat
t
observationtimesbeforeoratt. However,thetrainingproceduresuggestedinSection2.1.2allowsto
generalisethis,suchthatthePD-NJ-ODEmodellearnstocorrectlypredict
Xˆ :=E[X |A ], (2)
t,s t s∧t
forany0≤s,t≤T,whichisshowninthefollowingtworesults.
Corollary2.3. Letp ∈ (0,1)andI ∼ Ber(p)bei.i.d.randomvariablesfork ∈ N, whichare
k
independent of X and the observation framework n,t ,M . Under the same assumptions as in
i i
Anderssonetal.(2024,Theorem4.3),withAreplacedbyA˜ definedbelow,wedenotebyY˜θ mmi ,n
Nm
theoutputofthePD-NJ-ODEmodel,whereI determineswhetherthek-thobservationisusedas
k
inputtothePD-NJ-ODEmodelduringtraining. Inparticular,themodelonlyusestheinformation
availableintheσ-algebra
A˜ :=σ(X ,t ,M |I =1,t ≤t, j ∈{1≤l≤d|M =1}).
t ti,j i ti i i ti,l
WedenotethecorrespondingfiltrationbyA˜. ThenY˜θ mmi ,n
Nm
isA˜-adaptedandconvergestotheunique
(uptoindistinguishability)A˜-adaptedminimizert(cid:55)→E[X |A˜ ]ofthelossfunction.
t t
Proof. AdaptednessofthemodeloutputY˜θ mmi ,n Nm toA˜ followsfromtheusedinputandthemodel
architecture. TheremainderofthestatementfollowsequivalentlyasintheproofofAnderssonetal.
(2024,Theorem4.3).
In the following proposition we show that for all s ∈ [0,T], (E[X |A˜ ]) coincides with
t t 0≤t≤T
(Xˆ ) conditioned on the event B := {∀k ≤ n : I = 1 }, which has positive
t,s 0≤t≤T s k {tk≤s}
probability. Hence,thePD-NJ-ODEmodellearnstopredict(Xˆ ) onB .
t,s 0≤t≤T s
4Preprint
Proposition2.4. Foralls,t∈[0,T]wehaveP(B )>0andP-a.s.
s
1 E[X |A˜ ]=1 Xˆ , and
Bs t t Bs t,s
(3)
1 Y˜θ mmi ,n Nm =1 Y˜θ mmi ,n Nm(X˜≤t∧s),
Bs t Bs t
(cid:18) (cid:19)
hence, 1 Y˜θ mmi ,n Nm(X˜≤t∧s) converges (as in Andersson et al., 2024, Theorem 4.3) to
Bs t
0≤t≤T
1 (Xˆ ) asm→∞.
Bs t,s 0≤t≤T
Proof. Fixs∈[0,T]. B canbewrittenasthedisjointunionB =∪ ∪ {n=m,τ(s)=
s s m≥1 0≤k≤m
t ,∀j ≤m:I =1 },hence,independenceofI tonandt implies
k j {tk≤s} k i
m
(cid:88) (cid:88)
P(B )= P(n=m,τ(s)=t )pk(1−p)m−k >0,
s k
m≥1k=0
whereτ(s)isthelastobservationtimebeforeorattimes,whichshowsthefirstpartoftheclaim.Next,
wenotethatonB wehaveA˜ =A ,sinceallobservationsbeforeandnoobservationsaftersare
s t t∧s
“used”. BythesamereasoningwehaveonB
thatY˜θ mmi ,n Nm(X˜≤t∧s)=Y˜θ mmi ,n Nm(X˜≤t)=Y˜θ mmi ,n
Nm,
s t t t
hence(3)follows. Finally,weshowtheconvergenceinthemetricsd forany1≤k ≤K. With(3)
k
wehave
d k(cid:18) 1 BsY˜ ·θ mmi ,n Nm(X˜≤·∧s),1 BsXˆ ·,s(cid:19) =d k(cid:16) 1 BsY˜θ mmi ,n Nm,1 BsE[X t|A˜ t](cid:17)
(cid:16) (cid:17)
≤d
k
Y˜θ mmi ,n Nm,E[X t|A˜ t] −m −→ −−∞ →0,
wheretheconvergencefollowsfromCorollary2.3.
Remark2.5. Therearemanyequivalentoptionstochoosetheobservationsthatareusedasinputto
themodel. Selectingthemviai.i.d.Bernoullirandomvariablesisonepossibilitythatweusedue
toitssimplicity. However,thesameresultscanbederivedwithanyothermethodofchoosingthe
observationsasinputs,aslongastheprobabilityofarbitrarilylongperiodswithoutnewinputsis
positive,i.e.,P(B )>0foralls∈[0,T](wheretheI aredefinedthroughthechosenmethod).
s k
Oneexplicitalternativemethodistouseexponentiallydistributedrandomvariablestodetermine
thetimewithinwhichnoobservationsareusedasinput. Inparticular,assumingthatthecurrent
observationatt isusedasinputandthate ∼Exp(λ)forsomeλ>0,thenextobservationthatis
i i
usedasinputisatthefirstobservationtimet suchthatt −t ≥e . Thissub-samplingprocedure
k k i i
hastheadvantagethattheprobabilityofnotusinganyobservationasinputduringacertainperiod
onlydependsonthelengthoftheperiodbutnotontheamountofobservationsduringthisperiod(as
isthecaseforthei.i.d.Bernoullirandomvariables).
2.2 OUTPUTFEEDBACKINTHEPD-NJ-ODEMODEL
Usingtheoutputofadiscretedynamicalsystemattimetasadditionalinputtothesystematthe
followingtimet+1isdenotedasoutputfeedbackintheliteratureofreservoircomputingandknown
tostabilizethetrainingofsuchdynamicalsystems(Reinhart,2011). Inlinewiththis,wepropose
to use output feedback in the PD-NJ-ODE framework and remark that this does not change the
theoreticalguaranteesofthemodel. Indeed,themodelcanalwaysjustignorethisadditionalinput,
hence,thesameresultshold. However,theinductivebiaswhentrainingthemodelwiththisadditional
inputisbetterasweseeinSection3.
3 EXPERIMENTS
The code with all experiments is available at https://github.com/FlorianKrach/
PD-NJODE. For the experiments on synthetic stochastic datasets, we use the evaluation metric
ofKrachetal.(2022,Section8). Onallsyntheticdatasets,weuseapreviouslyunseenandindepen-
denttestsettoevaluatethemodels.
5Preprint
Table1: ComparisonofMSEsontestsetofthedifferentmodels.
model N N-OF N-IS N-OF-IS N-IIS N-OF-IIS N-OF-IIS-large
MSE 2.492 2.024 0.719 0.641 0.474 0.468 0.181
InSection3.1weshowthattheenhancedtrainingframeworktogetherwithoutputfeedbackenables
the PD-NJ-ODE to predict (deterministic) chaotic systems with great accuracy over a long time
horizon. Theenhancedtrainingframeworkalsoleadstobetterlong-termpredictionsforstochastic
datasets,asshownon3examplesinSection3.2.
3.1 LONG-TERMPREDICTIONOFCHAOTICSYSTEMS
WeshowcasethepotentialofourenhancedPD-NJ-ODEmodelfordeterministic(differential)systems
byapplyingittothechaoticsystemofadoublependulum,thatwasalreadydescribedandusedin
Krachetal.(2022,AppendixB.3&C.3). ThischaoticsystemcanbedescribedbyanODEin4
variables(thetwoanglesα ofthependulumsandtheirtwogeneralizedmomentap ). Bychoosing
i i
theinitialvalueofα =α randomlyaroundπweintroducesmalldeviationsintheinitialconditions
1 2
of this chaotic system, which lead to highly diverse paths. For more details on the setup of the
experimentseeAppendixA.
WeusethesamesettingasinKrachetal.(2022,AppendixC.3)andcomparethestandardPD-NJ-
ODEmodel(labelled“N”)toi)thePD-NJ-ODEwithoutputfeedback(N-OF),ii)thePD-NJ-ODE
withinputskipping(N-IS),iii)thePD-NJ-ODEwithoutputfeedbackandinputskipping(N-OF-IS),
iv) the PD-NJ-ODE with increasing input skipping (N-IIS) and v) the PD-NJ-ODE with output
feedbackandincreasinginputskipping(N-OF-IIS).Inparticular,N-ISreferstothemodelwhere
noneoftheobservationsafterX areuseasinputandN-IISreferstotheprocedureofSection2.1.2,
0
wherewedefinep(E)=max(0,1− E ),whereE denotesthecurrenttrainingepoch. Allofthese
100
modelsusethesamearchitectureandaretrainedfor200epochs. Moreover,weadditionallytrainthe
N-OF-IISagainwiththesamearchitecture,howeverwitha5timeslargerdatasethavinga2.5times
largerobservationprobabilityandwith300epochs(N-OF-IIS-large).
Weevaluatethetrainedmodelsonthetestset,bycomputingtheMSEbetweentheirpredictionsand
thetruepathsonafineequidistantgrid(thesameasusedforsamplingtheODEpaths). Theresults
aregiveninTable1. Inparticular,weseethatoutputfeedbackandinputskippingindependentlyof
eachotherimprovetheresults,wheretheimpactofinputskippingislargerthantheoneofoutput
feedback. Moreover,weseeaclearincreaseinperformancewhenswitchingfrominputskipping
toincreasinginputskipping(withandwithoutoutputfeedback). Inparticular,thisshowsthatthe
modelbenefitsfromtheadditional“initialvalues”usedinthebeginningofthetraining. Overall,the
performanceincreasesbymorethanafactor5fromNtoN-OF-IISandbymorethanafactor13
fromNtoN-OF-IIS-large.
InFigure1weshowthecomparisonofNandN-OF-IIS-largeontwosamplesofthetestset. While
thestandardPD-NJ-ODEmodelstartstodivergefromthetruepathafterabouthalfoftheevaluation
time,theenhancedPD-NJ-ODEmodelnearlyperfectlypredictsthepathovertheentireperiod.
3.2 LONG-TERMPREDICTIONSINSTOCHASTICSYSTEMS
Weuse3differentgeometricBrownianmotion(Black–Scholes)datasetwithsimilarspecificsasin
Herreraetal.(2021). Twoofthedatasetshaveconstantdriftandareidenticalexceptthattheyeither
useanobservationprobabilityof10%(BS-Base)or40%(BS-HighFreq). The3rddatasetusesa
time-dependentdriftandanobservationprobabilityof10%andisotherwiseidenticaltotheother
datasets(BS-TimeDep).
IntheBS-Basedataset,eachofthe100pointsofthesamplinggridisrandomlychosenasobservation
timewithprobability10%. Hence,theprobabilityofnothavinganobservationfor100consecutive
steps is smaller than 0.01%. Therefore, it is very unlikely that the model will learn to correctly
predict for such a long time (without intermediate observations), when trained with the standard
trainingframework. FortheBS-HighFreqdataset,thisprobabilityisfurtherreducedtobelow10−22,
6Preprint
2.5 2.5
0.0 0.0
2.5 2.5
10 10
5 5
0 0
10 10
true path true path
our model our model
5 5 true conditional expectation true conditional expectation
observed observed
0 0
0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5
t t
2.5 2.5
0.0 0.0
2.5 2.5
0 0
5 5
0 0
10 10
0 true path 0 true path
our model our model
true conditional expectation true conditional expectation
10 observed 10 observed
0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5
t t
Figure1: Left: testsamplesofaDoublePendulumwithstandardtrainingframework(N).Right: the
sametestsamplesoftheDoublePendulumwiththeenhancedtrainingframeworkandlargerdataset
(N-OF-IIS-large). Theconditionalexpectationcoincideswiththeprocess,sinceitisdeterministic.
Table2: Comparisonoftheminimalevaluationmetricofthestandard(N)andtheenhanced(N-OF-
IIS)PD-NJ-ODEmodelondifferentdatasets.
BS-Base(×10−3) BS-HighFreq(×10−3) BS-TimeDep(×10−2)
N 3.59 2506.29 3.52
N-OF-IIS 0.58 0.37 0.23
makingitevenmoreunlikelythatthestandardmodelwilllearntocorrectlypredictoverlongterms.
ThedifficultyoftheBS-TimeDepdatasetisthatthedynamicchangeswithtime(asinthechaotic
systemdataset). Thismakesitmoredifficultforthestandardmodeltolearn, whenobservations
arenotfarenoughapart. Theenhancedtrainingframework(Section2.1.3)shouldallowthemodel
tocircumventthesechallenges,asshowntheoretically. WecomparethestandardPD-NJ-ODE(N)
with the PD-NJ-ODE with output feedback and increasing input skipping (N-OF-IIS), where an
observationisusedasinputtothemodelwithprobabilityp(E)=max(0,1− E ),decreasingwith
100
thetrainingepochE duringthe200epochsoftraining.
We evaluate and compare both models on the test sets of the 3 datasets and see in Table 2 that
the enhanced training framework leads to large improvements in terms of the evaluation metric.
Moreover, this improvement is also well visible in Figure 2. For BS-Base we see the (slightly)
degradingperformanceofthestandardmodelNapproachingthetimehorizon,whichisnotprevalent
fortheenhancedmodelN-OF-IIS.OntheBS-HighFreqdatasetthestandardmodelNperformsmuch
worse,divergingfromthetrueconditionalexpectationalreadyafterashorttime,whiletheenhanced
model predicts nearly perfectly. This was expected, since the model N is much less exposed to
predictingoverlongertimeintervalsduringthetraining. Finally,incontrasttoN-OF-IIS,thestandard
modeldoesnotlearnthecorrectdynamicinthelongrunontheBS-TimeDepdataset. Comparingthe
7
1
2
1p
2p
1
2
1p
2p
1
2
1p
2p
1
2
1p
2pPreprint
true path true path
7 our model 7 our model
true conditional expectation true conditional expectation
observed observed
6 6
5 5
4 4
3 3
2 2
1 1
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
t t
9 9
true path true path
our model our model
8 8
true conditional expectation true conditional expectation
observed observed
7 7
6 6
5 5
4 4
3 3
2 2
1 1
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
t t
true path true path
7 our model 7 our model
true conditional expectation true conditional expectation
observed observed
6 6
5 5
4 4
3 3
2 2
1 1
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
t t
Figure2:Comparisonofthestandard(N;left)andenhanced(N-OF-IIS;right)modelonatestsample
oftheBS-Base(top),BS-HighFrequ(middle)andBS-TimeDep(bottom)dataset.
8Preprint
resultsofN-OF-IISontheBS-BaseandBS-HighFreqdataset,itmightseemssurprisingatfirstthat
themodelperformsbetteronthelatterdataset,wherethemodelNperformsmuchworse. However,
thiscanbeexplainedbythemuchlargernumberofobservedsamplesavailableinBS-HighFreqthat
themodelcanmakeuseofwiththeenhancedtrainingprocedure.
4 CONCLUSION
WhileithasbeenknownbeforethatthePD-NJ-ODEmodelcanbeusedtolearn(chaotic)deter-
ministicsystems,givenforexamplethroughODEsorPDEs,alimitingfactorfortheuseinpractice
was the degrading prediction accuracy for increasing prediction time. In this work we proposed
twoenhancementsofthePD-NJ-ODEmodelasaremedyforthisproblem. Simultaneously,these
enhancementsalsoenablelong-termpredictionswiththePD-NJ-ODEmodelinthecaseofgeneric
stochasticdatasets. Inparticular,convergenceofthemodeloutputtoamuchmoregeneralconditional
expectationprocess(witharbitrarysub-information)isguaranteedbythesuggestednewtraining
procedure. Sincetherearenoknowndrawbacks,theuseofthisnewtrainingprocedureisalways
recommended.
ACKNOWLEDGEMENT
TheauthorsthankJakobHeissformanydeepandinsightfuldiscussionsaboutthetopicstreatedin
thisworkandrelatedtopics.
REFERENCES
WilliamAndersson,JakobHeiss,FlorianKrach,andJosefTeichmann. Extendingpath-dependent
NJ-ODEstonoisyobservationsandadependentobservationframework. TransactionsonMachine
Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?
id=0T2OTVCCC1.
ManuelBrenner,FlorianHess,JonasMMikhaeil,LeonardFBereska,ZahraMonfared,Po-Chen
Kuo, and Daniel Durstewitz. Tractable dendritic rnns for reconstructing nonlinear dynamical
systems. InInternationalConferenceonMachineLearning,pp.2292–2320.Pmlr,2022.
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary
differentialequations. NeurIPS,2018.
VictorChurchillandDongbinXiu. Deeplearningofchaoticsystemsfrompartially-observeddata.
JournalofMachineLearningforModelingandComputing,3(3):97–119,2022. ISSN2689-3967.
CalypsoHerrera,FlorianKrach,andJosefTeichmann. Neuraljumpordinarydifferentialequations:
Consistent continuous-time prediction and filtering. In International Conference on Learning
Representations,2021.
FlorianHess,ZahraMonfared,ManuelBrenner,andDanielDurstewitz. Generalizedteacherforcing
forlearningchaoticdynamics. InAndreasKrause,EmmaBrunskill,KyunghyunCho,Barbara
Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International
ConferenceonMachineLearning,volume202ofProceedingsofMachineLearningResearch,pp.
13017–13049.PMLR,23–29Jul2023. URLhttps://proceedings.mlr.press/v202/
hess23a.html.
Florian Krach, Marc Nu¨bel, and Josef Teichmann. Optimal estimation of generic dynamics by
path-dependentneuraljumpODEs. arXivpreprintarXiv:2206.14284,2022.
H.D. Navone and H.A. Ceccatto. Learning chaotic dynamics by neural networks. Chaos,
Solitons & Fractals, 6:383–387, 1995. ISSN 0960-0779. doi: https://doi.org/10.
1016/0960-0779(95)80045-I. URL https://www.sciencedirect.com/science/
article/pii/096007799580045I. ComplexSystemsinComputationalPhysics.
9Preprint
JaideepPathak,BrianHunt,MichelleGirvan,ZhixinLu,andEdwardOtt. Model-freepredictionof
largespatiotemporallychaoticsystemsfromdata: Areservoircomputingapproach. Phys.Rev.
Lett., 120:024102, Jan2018. doi: 10.1103/PhysRevLett.120.024102. URL https://link.
aps.org/doi/10.1103/PhysRevLett.120.024102.
MaziarRaissi. Deephiddenphysicsmodels: Deeplearningofnonlinearpartialdifferentialequa-
tions. JournalofMachineLearningResearch,19(25):1–24,2018. URLhttp://jmlr.org/
papers/v19/18-046.html.
Rene´ FelixReinhart. Reservoircomputingwithoutputfeedback. DoctoralThesis,2011.
YuliaRubanova,RickyT.Q.Chen,andDavidKDuvenaud. Latentordinarydifferentialequations
forirregularly-sampledtimeseries. NeurIPS,2019.
Alex Svirin. Double pendulum, 2009. URL https://math24.net/double-pendulum.
html. Accessed: 2022-09-23.
PantelisR.Vlachas,WonminByeon,ZhongY.Wan,ThemistoklisP.Sapsis,andPetrosKoumout-
sakos. Data-driven forecasting of high-dimensional chaotic systems with long short-term
memory networks. Proceedings of the Royal Society A: Mathematical, Physical and Engi-
neering Sciences, 474(2213):20170844, 2018. doi: 10.1098/rspa.2017.0844. URL https:
//royalsocietypublishing.org/doi/abs/10.1098/rspa.2017.0844.
P.R.Vlachas,J.Pathak,B.R.Hunt,T.P.Sapsis,M.Girvan,E.Ott,andP.Koumoutsakos. Backprop-
agationalgorithmsandreservoircomputinginrecurrentneuralnetworksfortheforecastingof
complexspatiotemporaldynamics. NeuralNetworks,126:191–217,2020. ISSN0893-6080. doi:
https://doi.org/10.1016/j.neunet.2020.02.016. URLhttps://www.sciencedirect.com/
science/article/pii/S0893608020300708.
A EXPERIMENTAL DETAILS
A.1 DOUBLEPENDULUM
Dataset. Weexplainthechaoticsystemofadoublependulum,depictedinFigure3,followingSvirin
(2009);Krachetal.(2022). Thedynamicalsystemisdeterminedcompletelybya4-dimensional
Figure3: Aschematicrepresentationofadoublependulum. PicturecopiedfromSvirin(2009).
statevector(α ,α ,p ,p ),where(α ,α )determinethecurrentpositionofbothpendulumsand
1 2 1 2 1 2
10Preprint
(p ,p )aretheso-calledgeneralizedmomenta,whicharerelatedtothevelocitiesofbothpendulums.
1 2
Thisstatevectorsatisfiesthedifferentialsystem
p l −p l cos(α −α )
α′ = 1 2 2 1 1 2 ,
1 l2l A
1 2 0
p (m +m )l −p m l cos(α −α )
α′ = 2 1 2 1 1 2 2 1 2 ,
2 m l l2A
2 1 2 0
p′ =−(m +m )gl sin(α )−A +A ,
1 1 2 1 1 1 2
p′ =−m gl sin(α )+A −A ,
2 2 2 2 1 2
where
A =[m +m sin2(α −α )],
0 1 2 1 2
p p sin(α −α )
A = 1 2 1 2 ,
1 l l A
1 2 0
[p2m l2−2p p m l l cos(α −α )+p2(m +m )l2]sin(2(α −α ))
A = 1 2 2 1 2 2 1 2 1 2 2 1 2 1 1 2 ,
2 2l2l2A2
1 2 0
andgisthegravitationalaccelerationconstant.
Wechoosem = m = l = l = 1andonlyconsiderpositionswherethedoublependulumis
1 2 1 2
straight, i.e., bothpendulumshavethesameangleα := α = α , andthegeneralizedmomenta
1 2
p ,p are0,asinitialconditionsX . Hence,theinitialpointsaresampledbyrandomlysampling
1 2 0
αfromsomedistributionon[0,2π]. Weuseα ∼ N(π,0.22),i.e.,normallydistributedaroundto
highestpointthePendulumcanreach. Foreachinitialpoint,wesampleapathfromtheODEusing
theRunge–Kuttamethodoforder4(RK4)withstepsize0.025onthetimeinterval[0,2.5],which
leadsto101timepoints. Eachtimepointisindependentlychosenasobservationwithprobability0.1.
Overall,wesample20K pathsoutofwhich20%areusedasvalidationset. ForN-OF-ISS-large,a
datasetwith100K samplesandobservationprobability0.25isused.
Architecture. WeusethePD-NJ-ODE,withthefollowingarchitecture. Thelatentdimensionis
d = 400 and all 3 neural networks (encoder, neural ODE and readout network) have the same
H
structureof1hiddenlayerswithtanhactivationfunctionand200nodes. Empirically,themodel
performedbestwhenusingtherecurrentjumpnetwork,butnosignaturetermsasinput.
Training. Allmodelsaretrainedfor200epochs,exceptforN-OF-ISS-large,whichistrainedfor
300epochs. Earlystoppingisperformedbasedonthelossonthevalidationset.
A.2 GEOMETRICBROWNIANMOTIONDATASETS
ThedatasetsarethesameastheBlack–ScholesdatasetsinHerreraetal.(2021,Section6.1and6.2).
Dataset. ThegeometricBrownianmotionisdefinedbytheSDE
dX =µX dt+σX dW ,
t t t t
whereW isaBrownianmotion. Foralldatasets,weuseσ =0.3,X =1,andsample20′000paths
0
usingtheEuler-Maruyamamethodwithtimestep∆t=0.01onthetimeinterval[0,T]. ForBS-Base
andBS-HighFreqwechoosedriftµ=2,whileweusethetimedependentdriftµ(t)=sin(2πt)+1
fortheBS-TimeDepdataset. Eachtimepointisindependentlychosenasobservationwithprobability
0.1forthedatasetsBS-BaseandBS-TimeDep,andwithprobability0.4forBS-HighFreq(leadingto
shorterintervalsbetweenanytwoobservations).
Architecture. WeusethePD-NJ-ODE,withthefollowingarchitecture. Thelatentdimensionis
d = 100 and all 3 neural networks (encoder, neural ODE and readout network) have the same
H
structureof1hiddenlayerswithtanhactivationfunctionand100nodes. Themodelusesarecurrent
jumpnetworkandthesignaturetermsoflevel3asinput.
Training. Allmodelsaretrainedfor200epochs. Earlystoppingisperformedbasedonthelosson
thevalidationset.
11