Lessons from Learning to Spin “Pens”
JunWang*,1 YingYuan*,2 HaichuanChe*,1 HaozhiQi*,3
YiMa3 JitendraMalik3 XiaolongWang1
1UCSanDiego 2CarnegieMellonUniversity 3UCBerkeley
https://penspin.github.io/
Continuous Spinning
Diverse Objects
Figure1:Toprow:Continuousrotationofapen-likeobjectinhand.Bottomrows:Ourpolicycangeneralize
toadiversesetofpen-likeobjectswithdifferentphysicalproperties,usingonlyproprioceptionasfeedback.
Morevideosareavailableonourprojectwebsite.
Abstract: In-hand manipulation of pen-like objects is an important skill in our
dailylives,asmanytoolssuchashammersandscrewdriversaresimilarlyshaped.
However,currentlearning-basedmethodsstrugglewiththistaskduetoalackof
high-quality demonstrations and the significant gap between simulation and the
real world. In this work, we push the boundaries of learning-based in-hand ma-
nipulation systems by demonstrating the capability to spin pen-like objects. We
firstusereinforcementlearningtotrainanoraclepolicywithprivilegedinforma-
tionandgenerateahigh-fidelitytrajectorydatasetinsimulation. Thisservestwo
purposes:1)pre-trainingasensorimotorpolicyinsimulation;2)conductingopen-
looptrajectoryreplayintherealworld. Wethenfine-tunethesensorimotorpolicy
usingthesereal-worldtrajectoriestoadaptittotherealworlddynamics.Withless
than50trajectories,ourpolicylearnstorotatemorethantenpen-likeobjectswith
differentphysicalpropertiesformultiplerevolutions.Wepresentacomprehensive
analysisofourdesignchoicesandsharethelessonslearnedduringdevelopment.
Keywords: DexterousIn-HandManipulation,PenSpinning,Sim-to-Real
1 Introduction
Dexterousin-handmanipulationisafoundationalskillforvariousdownstreammanipulationtasks.
For example, one often needs to reorient a tool in hand before using it. Despite decades of active
researchinthisarea[1,2,3,4],in-handmanipulationremainsasignificantchallenge. Manipulating
∗EqualContribution.
4202
luJ
62
]OR.sc[
1v20981.7042:viXrapen-like objects, in particular, is considered one of the most challenging and crucial tasks [5, 6].
Thiscapabilityishighlypractical, asmanytools, suchashammersandscrewdrivers, havesimilar
shapes. Moreover, spinning pen-like objects requires dynamic balancing and sophisticated finger
coordination,makingitanidealtestbedforadvancingdexterousmanipulationsystems.
Pen spinning has been studied from several perspectives. Classic robotics works demonstrate ro-
tating wooden blocks with open-loop force control [1]. With high-speed cameras and advanced
hardware, agile pen spinning can also be achieved [6]. However, these methods rely on accurate
objectmodelsandcannotgeneralizetounseenobjects. Ontheotherhand,learning-basedmethods
holdthepromiseofbeinggeneralizablewithlarge-scaledata.Theyhaveindeedachievedsignificant
progresseitherwithimitation learning[7,8,9]orsim-to-real[3, 10,11,12]. However, they have
onlydemonstratedmanipulationofregularsphericalorcuboid-shapedobjects,andnonecanextend
thecapabilitytopen-likeobjects. Weattributethistotworeasons: Forimitationlearning, current
teleoperationsystemsfailatcollectingcomplexanddynamicdemonstrationssuchaspenspinning;
forsim-to-real,bridgingthegapfordynamictasksbecomessubstantiallydifficult.
In this work, we push the boundaries of learning-based in-hand manipulation systems by demon-
strating their capability to spin pen-like objects. Similar to previous approaches [10, 11, 13], we
firstlearnanoraclepolicywithprivilegedinformationusingreinforcementlearninginsimulation.
However, when attempting to distill it into a sensorimotor policy, we find the sim-to-real gap too
large. Whilethisgapgenerallyexistsinpreviousin-handmanipulationtasks[11,13],theextreme
difficultyofspinningpen-likeobjectsexposesthegapevenfurther.Fine-tuningthepolicywithreal-
worldtrajectoriescanbeonewaytomitigatethisgap,butitischallengingtocollectdemonstrations
viateleoperationforthisdynamictask.Inspiredbyrecentanalysisonopen-loopcontrollers[14,15],
we instead collect a high-fidelity trajectory dataset in simulation and use it as an open-loop con-
troller on the real robot. The successful trajectories in the real world serve as our high-quality
demonstrations. We then bridge the sim-to-real gap by fine-tuning our sensorimotor policy with
thesereal-worldtrajectories. Withsimulationpre-training,oursensorimotorpolicyhasthemotion
priorfromdiversedataandcanadapttoreal-worldphysicswithfewerthan50trajectories.
We conduct comprehensive experiments in both simulation and the real world. In simulation, we
identifythekeyfactorsthatenabletheoraclepolicytolearnthechallengingpen-spinningtaskand
generaterealistictrajectories. Wethenevaluatedifferentmethodsofobtainingadeployablepolicy
intherealworld. Wealsoconductablationexperimentsshowingtheimportanceofpre-trainingin
simulation. Wedemonstratethatourpolicycanadapttoreal-worldphysicswithfewerthan50real-
world trajectories. To the best of our knowledge, this is the first learning-based system to achieve
continuousspinningofpen-likeobjectsintherealworld.
2 RelatedWork
Classicin-handmanipulation. In-handmanipulationhasbeenstudiedfordecades[2,16]. Clas-
sicalmethodsrelyonanaccuratemodelandanalyticallyplanasequenceofmotionstocontrolthe
object. Forexample,HanandTrinkle[17]manipulateobjectsusingsliding,rolling,andfingergait-
ing motions, while Bai and Liu [18] studies the collaboration of fingers and the palm. Open-loop
manipulationalsoshowssurprisingrobustnessanddexterousbehavior[14,15]. SielerandBrock
[19] uses linearized feedback-control for in-hand manipulation with a soft hand. State-of-the-art
systemsinthiscategoryincludefullSO(3)reorientationusingacompliance-enabledhand[20]and
an accurate pose tracker [21]. However, most methods cannot manipulate pen-like objects due to
their complex and dynamic nature. Extrinsic dexterity [22] can also be used to achieve dynamic
manipulation,butaprecisemodelisnecessary. Incontrast,ourmethoduseshumanpriorstobuild
asimulatorenvironmentbutdoesnotrelyonanaccuratemodelduringdeployment.
Learning-baseddexterousmanipulation. Learning-basedmethodsmakefewerassumptionsand
holdthepromiseofbeingmoregeneralizableasweacquiremoredata.Recently,significantprogress
has been made in this field [3, 4]. The advancement mainly comes from two sources: 1) low-
cost and accessible teleoperation systems [7, 8, 23, 24, 25, 26, 27, 28] combined with imitation
learning [29, 30]; and 2) reinforcement learning in simulation [31, 32] combined with sim-to-
2(A) Oracle Policy Training with RL Rollout
 (B) Pre-training in Sim
in Sim
Oracle
 Student

Policy Policy
Sim Dataset
(C) Open-loop Replay (D) Real-World Fine-tuning
Copy
Human-in-the-loop
selection Student

Real Dataset
Policy
Success Traj.
On Real Robot
Figure 2: Anoverviewofourapproach. Wefirsttrainanoraclepolicyinsimulationusingreinforcement
learning.Thispolicyprovideshigh-qualitytrajectoryandactiondatasets.Weusethisdatasettotrainastudent
policyandasanopen-loopcontrollerintherealworldtocollectsuccessfulreal-worldtrajectories.Finally,we
fine-tunethestudentpolicyusingthisreal-worlddataset.
real[10,11,12,32]. However,bothmethodshavelimitations. Currentteleoperationcannotsupport
agileanddynamictaskssuchasspinningpens,duetothenon-negligiblecommunicationlatencyand
retargetingerrors. Ontheotherhand,sim-to-realapproachesdemonstrategreatgeneralizationand
robustnessbytrainingpoliciesinrandomizedenvironments. Theyshowsuccessinmultiplefields
such as in-hand manipulation [13, 33, 34, 35, 36, 37, 38, 39], grasping [40, 41, 42], long-horizon
tasks[43],andbimanualdexterity[44,45].However,thegapbetweensimulationandrealityisquite
large, and some results are only limited to simulation [46, 47, 48]. Our paper distinguishes itself
from all previous work by leveraging the advantages of both fields. We use reinforcement learn-
inginsimulationtoobtainhigh-qualitydemonstrationsandusereal-worldtrajectoriestobridgethe
sim-to-realgap.
Ourworkisalsorelatedtoseveralrecentworksoncombiningreal-worldandsimulationdata.Torne
etal.[49]andWangetal.[50]augmentreal-worldhumandemonstrationsbycreatingasimulated
environment,showingthisishelpfulforpolicyrobustness. Jiangetal.[51]demonstratesthatsim-
to-realpoliciescanadapttoreal-worldcomplexdynamicswithonlyafewhumandemonstrations.
Ourapproachalsoadaptspoliciestrainedinsimulationtotherealworldusingdemonstrationsand
utilizessimulationdatatomakethepolicymoregeneralizableandrobust. However,sinceourtask
is more challenging, it is difficult to collect human demonstrations or provide human feedback.
Therefore,weneedtogeneratehigh-fidelitytrajectoriesbylearningapolicyinsimulation.
Pen spinning. The specific problem of pen spinning has also been studied extensively due to its
challenging nature and practical implications in the real world. Fearing [1] shows an open-loop
forcecontrolstrategycanachieverobustfingergaitingformanipulatingalongwoodenblock. Ishi-
haraetal.[6]andNakataniandYamakawa[5]demonstratehigh-speedpenspinningusingahigh-
speedrobothandandcamera. Inthemachinelearningcommunity,CharlesworthandMontana[52]
demonstratepromisingresultswithRLandtrajectoryoptimization. Maetal.[53]usesalanguage
modelforrewarddesign. However,theresultsarelimitedtosimulation. Bringingsimulationresults
totherealworldisasubstantiallyhardertask. Thereareworksthatinvolvelearningtomanipulate
longobjectsusingreal-worldreinforcementlearning[54]oraugmentedwithimitation[55], butit
can only do less than half a circle and no finger gaiting. In contrast, we achieve continuous pen
spinningusingalearning-basedapproachandcommerciallyavailablehardware.
3 LearningtoSpinPens
AnoverviewofourmethodisshowninFigure2. Ourmethodconsistsofthreesteps. First,wetrain
an oracle policy with privileged information to generate realistic trajectories in simulation. With
thesetrajectories,wepre-trainasensorimotorpolicyinsimulation. Wethenusethesetrajectoriesas
3
...
...Figure 3: Visualization of canonical grasp. Inspired by how humans spin pens, we design six canonical
initialposesusedtoresettheepisode. Theseposesarekeyframeswheretheindex,thumb,andmiddlefingers
breakandre-establishcontact.
anopen-loopcontrollertogeneratedemonstrationsintherealworld,whichisusedtofine-tunethe
sensorimotorpolicytoadaptittothereal-worlddynamics.
3.1 OraclePolicyTraining
Obtaining high-quality data for pen spinning is itself a challenging task due to the dynamic and
complex movements involved. The current teleoperation system is not suitable due to the non-
negligible latency and imperfect retargeting error between the human hand and the robot hand.
Alternatively, previous work shows that reinforcement learning can synthesize complex behaviors
insimulation[52,53].Thesemethodsachievefastanddynamicbehaviorbutmayviolatereal-world
physics and hardware constraints. In contrast, we design our approach to generate high-quality
trajectories that are realistic enough for use as an open-loop controller in the real world. This is
achievedbyproperlydesigningtheinputspace,rewardfunction,andinitialstatedistributions.
Observations. Theobservationo oftheoraclepolicyf isacombinationofthefollowingquanti-
t
ties: jointpositionsq , previousjointpositiontargeta , binarytactilesignalsc , fingertipposi-
t t−1 t
tionsp ,thepen’scurrentposeandangularvelocityw ,andapointcloudofthepenatthecurrent
t t
state∈R100×3.Toobtainfine-grainedtactileresponses,weaugmentthesensorarrangementin[12]
toincludefivebinarysensorsoneachfingertip(seeFigure6). Thepointcloudisobtainedbytrans-
formingpointsontheoriginalmeshbasedonthecurrentground-truthobjectpose. Weencodethe
pointcloudusingPointNet[56]asin[33,57,58]. Westackthreehistoricalstatesofjointpositions
andtargetsasinputs.Wealsoincludephysicalpropertiessuchasmass,centerofmass,coefficientof
friction,andobjectsizeintheinput[11]. Thedimensionsoftheinputsaredetailedintheappendix.
Actions. Ateachstep,theactionprovidedbythepolicynetworkf(o )isarelativetargetposition.
t
Thepositioncommanda = ηf(o )+a ,whereη istheactionscale,issenttotherobotandit
t t t−1
willbeconvertedtotorqueviaalow-levelPDcontroller.
Reward. Thegoalofthepolicyistocontinuouslyrotatethepenaroundthez-axis. Ourrewardis
definedasacombinationofrotationrewardandafewenergypenaltyterms.Therewardandpenalty
termsfollow[11,12]. However,stablegaitsdonotemergesolelyfromthis. Motivatedby[52],we
propose another reward r , a penalty regarding the height difference between the highest and the
z
lowestpointsonthepen,encouragingtherobothandtokeepthepenhorizontalduringrotation.
Insummary,ourrewardfunctionis(tomittedforsimplicity):r =r +λ r +λ r ,where
rot z z energy energy
r rewardsthepen’srotationvelocityandr penalizestheobject’slinearvelocity,deviation
rot energy
frominitialjointpositions,mechanicalwork,andtorqueapplied(seeappendixfordetails).
Initialstatedesign. Ourtaskfundamentallydiffersfrompreviousworkwheretheobjectisplaced
onthepalm[12,13],atable[10],orfingertipbygravity[11],wherethereisnaturalsupportinthose
cases. Therefore, using randomly sampled poses does not provide meaningful exploration in our
case. Wefindthataproperdesignoftheinitialstatedistributioniscriticalforpolicytraining. De-
signinginitialstatesforpenrotationisnon-trivialbecausetheinitialgraspshouldbestableenough
to facilitate learning subsequent steps of motion. Moreover, exploration can be slow if we repeat-
edly use the same initial state upon reset. Thus, inspired by human behavior, we manually design
multiple patterns of grasping that may occur in the cycle of pen rotation (visualized in Figure 3),
andthenaddnoisetogenerateandfilterforasetofstableinitialstates.
4Episode Reward Episode Length (s) Episode Reward Episode Length (s)
120 20 120 20
90 15 90 15
60 10 60 10
30 5 30 5
0 0 0 0
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Agent Steps (M) Agent Steps (M) Agent Steps (M) Agent Steps (M)
Ours Single Canonical Pose Ours No Tactile No Point Cloud No Privileged Object Info
Figure4: Learningcurvesforourpolicyanddifferentbaselines. Left: Usingawell-designedinitialdis-
tributioniscritical. Ourmethodsamplestheinitialstatesfromsixproposedcanonicalstateswithnoise,while
SingleCanonicalPoseonlysamplesnearonecanonicalgrasp.Thishasunstabletrainingperformanceandthe
fingergaitingdoesnotemerge(alsoseeFigure5C).Right: Thenecessityofusingvisuotactileinformation
andprivilegedinformationduringoraclepolicytraining.Wetraineachpolicywith3seeds.
Policy optimization. We use proximal policy optimization (PPO) [59] to train the oracle policy.
Giventhestateinformation,weuseaMulti-LayerPerceptron(MLP)forboththepolicyandvalue
networks. Weapplydomainrandomizationtoperceptioninputs,physicalparameters,objectprop-
erties,etc. Anepisodeterminateswhenresetconditionsaremetortheagentreachesthemaximum
numberofstepsT. Wepruneunnecessaryexplorationswhenthepenfallsbelowaheightthreshold.
3.2 SensorimotorPolicyPre-training
Theoraclepolicymentionedabovecanlearnsmoothanddynamicbehaviorduringsimulationtrain-
ing.However,itcannotbedeployedbecauseitrequiresprivilegedinformationasinput,whichisnot
accessible in the real world. Previous works typically distill the oracle policy into a sensorimotor
policyusingDAgger[60]. However,wefindthisapproachdoesnotworkwellforourpen-spinning
task. We experimented with either proprioception [11] or adding visuotactile feedback [13, 33].
While the policy with visuotactile feedback can learn reasonable behavior in simulation, the mis-
match between simulation and reality is too large for these two modalities. On the other hand,
proprioceptivefeedbackisthemostsimilarandreliablesensingmethodbetweensimulationandthe
realworld, buttheproprioceptivepolicycannotconvergeeveninsimulationandalwaysdropsthe
objectinthefirstfewsteps.
Forthisreason,weproposeanalternativeapproach:werollouttheoraclepolicyf insimulation,in
contrasttopreviousworkusingDAggerandrollingoutthesensorimotorpolicy[11,33],andcollect
a dataset of proprioception and actions (s ,a ). This dataset is used to pre-train a proprioceptive
t t
policyinsimulation. Thegoalofthisstepistoexposethesensorimotorpolicytodiversetraining
data. Although training with such data cannot enable direct transfer to the real world due to inac-
curatedynamics,itcanprovideamotionprior,allowingthepolicytobeefficientlyfine-tunedwith
real-worldtrajectories.
Following[11],ourproprioceptivepolicytakes30stepsofjointpositionsq andpreviousjoint
t−29:t
targetsa asinput. Weuseatemporaltransformersimilartotheoneusedin[33]tomodel
t−30:t−1
sequentialfeaturesandanMLPforthepolicynetwork. Suchpre-trainingallowsourproprioceptive
policytoexperienceawiderrangeofcircumstances,preventingoverfittingtospecifictrajectories.
3.3 Fine-tuningSensorimotorPolicywithOracleReplay
Due to the large sim-to-real gap of our task, we choose to use real-world trajectories to fine-tune
thepre-trainedsensorimotorpolicytoadapttoreal-worlddynamics. However,obtainingreal-world
trajectoriesischallenging. Ourkeyobservationisthatalthoughtheoraclepolicycannotbedirectly
distilledandzero-shottransferredtotherealworld, itdoesprovidemotionsequencesthataredif-
ficult to generate using teleoperation. Inspired by recent work that highlights the effectiveness of
open-loop controllers for in-hand manipulation [14, 15], we use the trajectories generated by the
oraclepolicyasanopen-loopcontrollerintherealworld.
5(A) Our Approach. Stable Fingergaiting Emerges.
(B) Without z-reward. Objects are tilted at certain configurations.
Tilted Object


 Tilted Object



Unstable in Real Unstable in Real
(C) Single Initial Pose. No Fingergaiting.
Figure 5: Importanceofr andinitialstatedesign. (a)Ourpolicyspinsthepeninasmoothandstable
z
manner,withthepenmostlyhorizontal. (b)Policiestrainedwithoutther tendtomakethepenmoretilted
z
duringrotation. Thisbehaviorisunstableandcannotbeusedasanopen-loopcontrollerintherealworld. (c)
Initializingwithasinglecanonicalstatelacksexplorationandcannotlearnfingergaiting.
Specifically,aftertrainingtheoraclepolicyf,wetestitinthesimulationenvironmentwithdifferent
initialposes.Weselect15trajectoriesfromdifferentinitialposesthatlastlongerthan800timesteps.
Werecordtheseactionsandreplaythemontherealrobotwiththreetrainingobjects(Figure7). For
each replay, we randomly select one of the 15 trajectories. If this open-loop controller can rotate
objectsmorethan2πinthistrial,westorethistrajectoryinthedataset. Werepeatthisprocessuntil
wecollect15trajectoriesperobject(45trajectoriesintotal).
Using the learned policy to generate such trajectories has two benefits: First, it naturally provides
smoothness driven by our reward definition; Second, compared to alternative approaches such as
learningfromhumanvideos,itprovidestrajectorydatawithactions.Weusethisdatasettofine-tune
our proprioceptive policy π to make it adapt to real-world dynamics. Because the proprioceptive
policyhasalreadybeenpre-trainedindiversesimulationenvironments,itcanadapttotherealworld
withfewerthan50trajectories.
4 Experiments
In this section, we compare our approach for pen spinning to several baselines in both simulation
andtherealworld. Specifically,westudy1)thecriticaldesignchoicesinobtaininganoraclepolicy
thatcanbereplayedintherealworld;2)varioustechniquesforsim-to-realdeployment.
4.1 ExperimentSetup
Hardwaresetup. WeusetheAllegroHandforourhardwareexperiments.
The Allegro Hand has four fingers, each with 4 degrees of freedom. Our
neuralnetworkoutputsthejointpositiontargetat20Hz, whichissenttoa
low-levelPDcontrolleroperatingat333Hz.
Simulation setup. We use Isaac Gym [32] for our simulation training. To
obtainadditionaltactilefeedbackfororaclepolicytraining,wesimulate20
tactilesensorsaroundthefingertips,with5oneachfingertip. Wegatherthe Figure 6: Touch sen-
contact signal from each sensor and binarize the measurement based on a sor(blue)arrangement.
6Training Objects Unseen Objects
Object A Object B Object C Object D Object E Object F Object G Object H Object I Object J
Figure7: Training/TestSplitofObjects.Weusethreetrainingobjectstocollectreal-worldtrajectories.We
evaluateourpolicyandbaselinesonbothtrainingobjectsandunseenobjects.
TrainingObjects UnseenObjects
Object ObjectA ObjectB ObjectC ObjectD ObjectE ObjectF ObjectG ObjectH ObjectI ObjectJ
Metric RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑
Replay 2.80 37.62 3.37 54.29 2.65 29.52 3.83 78.21 3.44 67.09 2.47 51.49 2.93 44.35 3.53 41.51 2.65 30.99 2.56 34.38
P.Distill N.A. N.A.
V.Distill 1.85 17.65 1.57 0.00 1.70 8.33 1.57 0.00 1.57 0.00 1.57 0.00 1.57 0.00 1.57 0.00 1.57 0.00 1.57 0.00
Ours 3.43 54.93 3.38 70.00 3.62 57.55 4.10 80.65 3.50 68.18 2.71 53.33 4.47 78.02 4.63 75.79 3.64 46.60 3.49 60.47
Table1: Comparisonwithdifferentdeployablesystems. OracleReplayachievesreasonableperformance
butisstillinferiortoours. Distillationtoproprioceptivepolicy(P.Distill)failstoconvergeevenduringsimu-
lationtraining.Distillationtovisionpolicy(V.Distill)suffersfromasignificantsim-to-realgap.Manyentries
arerecordedas1.57forVisionDistillationduetoaconsistentfailuremode: thethumbandindexfingercan
rotatetheobjectby90degrees,butthentheobjectdrops.Ourmethodachievesthebestperformance.
pre-defined threshold [12, 13]. In simulation, the control frequency is 20Hz and the simulation
frequencyis200Hz.
Objectdataset. Insimulation,weonlyusecylindricalobjectswithrandomizedphysicalproperties.
Duringreal-worldbehaviorcloningtrainingandtesting,weuse3objectstocollectdemonstrations
andfortraining,and7differentobjectsforevaluation.
Evaluationmetrics. Inoursimulationexperiments,weevaluatetheCumulativeRotationReward
andDuration(seconds)[11,12,13].Intherealworld,wemeasuretheradiansofrotation(RR.)over
thez-axisandthesuccessrate(Suc.). Wedefinesuccessastherateatwhichthepolicycanrotate
targetobjectsatleast180degrees,whichtypicallycorrespondstothepolicycompletingonecircle
offingergaiting,whereeachfingercompletesabreakandre-establishescontact.
4.2 OraclePolicyTraining
Thegoaloftheoraclepolicyistogeneraterealistictrajectoriesthatcanbeusedbothforpre-training
the student policy and serving as an open-loop controller in the real world. We compare several
criticalfactorsinachievingthis,specifically: 1)withoutawell-designedinitialposedistribution;2)
withoutprivilegedinformation;3)withoutr .
z
Q1: Howdoestheinitialstatedistributionhelppolicytraining? Westudytheeffectofawell-
designed initial state distribution. The results are shown in Figure 4 left. Single Canonical Pose
samplesstatesaroundonecanonicalhandpose,asusedin[11,33]. Incontrast,ourmethoddefines
multiplecanonicalhandposesinspiredbyhowhumansspinpensandachievesbetterperformance
compared to using a single canonical pose. We also emphasize that although the curve for single
canonical init does increase over time, the finger gaiting cannot emerge, and this policy cannot
escapefromthelocalminima. WevisualizethebehaviorinFigure5(c)andfindthefingerdoesnot
breakcontactwiththeobjectandfailstoachievemorethanonerevolution.
Q2: How does privileged information help policy training? We study the importance of privi-
legedinformationinFigure4right.Unlike[11],theoraclepolicycannotbetrainedonlywithsimple
objectpropertiessuchasobjectposition. Wefindthatwithouttactilefeedbackorapointcloud,the
policydoesnotachievegoodenoughperformance. Theshapeofthepenisimportantasthepolicy
needs to know whento liftthe fingersto spinthe pen. Privileged informationsuch asthe object’s
physicalpropertiesandfingerpositionsisalsocritical,withoutwhichthepolicydoesnotconverge.
Q3: Howdoesz-rewardhelppolicytraining? Westudytheeffectofz-rewardr ,showninFig-
z
ure5(b). Althoughthetrajectorieslooksimilartoourapproachatfirstglance,theobjectgetstilted
7TrainingObjects UnseenObjects
Object ObjectA ObjectB ObjectC ObjectD ObjectE ObjectF ObjectG ObjectH ObjectI ObjectJ
Metric RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑
OnlyPretrain 1.89 15.15 2.44 44.87 1.70 8.11 1.74 6.86 2.13 29.35 1.98 21.05 2.06 19.77 2.11 21.59 2.68 54.08 2.14 22.22
NoPretrain 2.62 53.66 2.34 36.84 2.29 30.00 1.92 16.53 1.88 19.61 1.90 16.42 2.09 23.86 2.15 24.72 2.92 63.22 2.41 33.70
Ours 3.43 54.93 3.38 70.00 3.62 57.55 4.10 80.65 3.50 68.18 2.71 53.33 4.47 78.02 4.63 75.79 3.64 46.60 3.49 60.47
Table2: Theeffectofpre-trainingandfine-tuningforourmethod. Weshowbothcomponentsarecritical
forourmethod.Withoutpre-training,thepolicytendstooverfittothelimitedamountofreal-worldtrajectories.
Withonlypre-training,thepolicydoesnotworkwellbecauseofthelargesim-to-realgap.
atcertainconfigurationsinthethirdandsixthsub-figure. Suchconfigurationcanbarelysucceedin
real-worldreplay. Incontrast,policiestrainedwiththez-rewardrotatethepenmorestably,keeping
thepenapproximatelyhorizontal,whichfacilitatesdatacollectioninreal-worldreplay.
4.3 SensorimotorPolicyTraining
Althoughtheoraclepolicyachievesgreatperformanceinsimulation,itcannotbedirectlydeployed
in the real world. To address this issue, we use it as an open-loop controller to collect real-world
trajectories.Wealsopre-trainaproprioceptivepolicyinsimulationandfine-tuneitusingthisdataset.
Wecompareourmethodwithseveralalternativesintherealworld.TheresultsareshowninTable1.
Q4: Is oracle replay a good enough controller? We design our oracle policy so that it achieves
decent performance in the real world (Oracle Replay). However, it still performs worse than our
method. On Training Objects A/B/C, our method achieves 15%-30% better performance in terms
of success rate. On Unseen Objects D/E/F, which are considered out-of-distribution, our method
achievesa10%increaseintheradiusrotated,despitehavingasimilarsuccessrate.Ourmethodalso
achieves 15%-30% success rate improvements on objects I/J/K. This result demonstrates that our
methodgenerally achievesa longerradiusrotated comparedtothe oraclereplay becauseitis also
pre-trainedinsimulationwithmorediversedata.
Q5: Doesdistillationworkforpenspinning? Previousapproachesdemonstratepromisingresults
by distilling the oracle policy into the sensorimotor policy [11, 33, 13] using DAgger. However,
thisapproachdoesnotworkforourdynamicandcontact-richtask(Figure1). First, wetrytouse
segmenteddepth[33]ortwoendpointsofthepen,andthevisuotactilepolicycanachievereasonable
performanceinsimulation.However,thesim-to-realgapissignificantlylargercomparedtoprevious
works. Inourreal-worlddeployment, theobjectsoscillatealot, makingtheimagedistributionfar
removedfromthetrainingone. Secondly,proprioceptivefeedbackdoesnothavethisproblem,but
usingproprioceptionalonedoesnotachievegoodperformanceinsimulation.
Q6: Howdopre-trainingandfine-tuningcontributetothefinalperformance? Ourapproachis
firstpre-trainedinsimulationandthenfine-tunedusingreal-worlddata. Westudythecontribution
of each part in Table 2. With only pre-training, the policy has limited effectiveness in the real
world. ItrarelycompletesfingergaitingonObjectsCandD,andthesuccessrateisalsolowforthe
remainingobjects. Thisismainlybecausethephysicsgapbetweensimulationandrealitybecomes
moresignificantinourtask. Withonlybehaviorcloning,theapproachalsodoesnotperformwell.
Ithasa50%lowerrotationradiusonTrainingObjects. Onout-of-distributionobjects,thesuccess
ratedropstolessthan20%, indicatingthatitsgeneralizationcapabilityislimited. Thisisbecause
weonlyhave15trajectoriesforeachobject,andthepolicytendstooverfittothatdata.
Q7: Cansimulationpre-trainingbereplacedbymoredemonstrations? Wealsostudywhether
increasing the number of real-world demonstrations can substitute for the advantages gained from
pre-traininginsimulation. TheresultsareshowninTable3. Wefindthatalthoughtheperformance
of the No Pretraining baseline can be improved with more demonstrations, it gradually saturates
whenincreasingthenumberofdemonstrationsfrom45to75. Inaddition,themajorimprovements
come from the training objects (A/B/C), while the performance on unseen objects (D/E/F) is still
farworsecomparedtoourmethods. Thisindicatesthatsolelyrelyingonreal-worldtrajectoriesis
likelytooverfittocertainobjects.
8TrainingObjects UnseenObjects
#Demo ObjectA ObjectB ObjectC ObjectD ObjectE ObjectF
RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑ RR.↑ Suc.↑
15 1.80 14.29 1.82 15.79 1.57 0.00 1.75 11.11 1.84 13.04 1.57 0.00
NoPretrain 45 2.62 53.66 2.34 36.84 2.29 30.00 1.92 16.53 1.88 19.61 1.90 16.42
75 2.93 76.67 2.78 40.00 2.57 43.33 2.36 26.67 2.09 23.33 1.96 15.00
Ours 45 3.43 54.93 3.38 70.00 3.62 57.55 4.10 80.65 3.50 68.18 2.71 53.33
Table 3: We study whether having more demonstrations could substitute for simulation pre-training.
WefindthatalthoughtheNoPretrainbaselineimprovesasweincreasethenumberofdemonstrationsfrom15
to75,itstillperformsworsethanourmethod,especiallyonunseenobjects. Thisindicatesthattrainingwith
muchmorediversedatainsimulationisbeneficialandcanalsoavoidoverfittingtocertainobjects.
4.4 QualitativeExperiments
In addition to the objects we present in the quantitative study, we also try more different objects
forourpolicyandtrytopushthelimitsonobjectsthataresignificantlyout-of-distribution. Some
examplesareshowninourFigure1. Wefindourmodelcanrotateobjectsformultiplerevolutions
withsmoothfingergaiting. Videosareshownonourprojectwebsite.
5 ConclusionandLessons
In this paper, we present the first learning-based approach for spinning pen-like objects. Through
ourextensiveexperiments,wesharethelessonswelearnedasfollows:
• Simulation training requires extensive design for exploration, such as the proper design of
initialdistributionstoaidexplorationandusingprivilegedinformationtofacilitatepolicylearning.
• Sim-to-Realdoesnotdirectlyworkforsuchcontact-richandhighlydynamictasks. Evenwhen
isolating touch and vision, the pure physics sim-to-real gap remains significant and cannot be
bridgedbyextensivedomainrandomizationalone.
• Simulationisstillusefulforexploringskills. Thedynamicskillofspinningpenswitharobotic
handisnearlyimpossibletoachievewithhumanteleoperationandimitationlearningalone. Re-
inforcementlearninginsimulationiscriticalforexploringfeasiblemotion.
• Onlyafewreal-worldtrajectoriesareneededforfine-tuning.Althoughaproprioceptivepolicy
learnedpurelyinsimulationdoesnotworkdirectlyintherealworld,itcanbefine-tunedtoadapt
toreal-worldphysicsusingonlyafewsuccessfultrajectories.
Limitations.Wehaveidentifiedseveralkeybottlenecksofusingvisionandtouchduringsim-to-real
forthisdynamictask. However, wearenotstatingtheyshouldnotbeused. Humansdonotseem
toneedvisiontospinapen, buttouchfeedbackseemsimportant. Infuturework, wewillexplore
whetherusingthemcanhelpfurtherimproveperformance. Currently,thesystemisonlycapableof
rotatingalongz-axis,itisalsoapromisingdirectiontoextendittogeneralmulti-axisrotation.
Acknowledgments
Xiaolong Wang’s lab is supported, in part, by Amazon Research Award, Intel Rising Star Faculty
Award,andQualcommInnovationFellowship. HaozhiQiandJitendraMalikaresupportedinpart
byONRMURIN0001421-1-2801. HaozhiQiandYiMaaresupportedinpartbyONRN00014-
22-1-2102.
References
[1] R.Fearing. Implementingaforcestrategyforobjectre-orientation. InICRA,1986.
[2] D.Rus. In-handdexterousmanipulationofpiecewise-smooth3-dobjects. IJRR,1999.
[3] OpenAI, M. Andrychowicz, B. Baker, M. Chociej, R. Jo´zefowicz, B. McGrew, J. Pachocki,
A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder,
L.Weng,andW.Zaremba. Learningdexterousin-handmanipulation. IJRR,2019.
9[4] OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron,
A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welin-
der, L.Weng, Q.Yuan, W.Zaremba, andL.Zhang. Solvingrubik’scubewitharobothand.
arXiv:1910.07113,2019.
[5] S. Nakatani and Y. Yamakawa. Dynamic manipulation like normal-type pen spinning by a
high-speed robot hand and a high-speed vision system. In International Conference on Ad-
vancedIntelligentMechatronics,2023.
[6] T. Ishihara, A. Namiki, M. Ishikawa, and M. Shimojo. Dynamic pen spinning using a high-
speedmultifingeredhandwithhigh-speedtactilesensor. InHumanoids,2006.
[7] S. P. Arunachalam, S. Silwal, B. Evans, and L. Pinto. Dexterous imitation made easy: A
learning-basedframeworkforefficientdexterousmanipulation. InICRA,2023.
[8] C.Wang,H.Shi,W.Wang,R.Zhang,L.Fei-Fei,andC.K.Liu.Dexcap:Scalableandportable
mocapdatacollectionsystemfordexterousmanipulation. InRSS,2024.
[9] S. Haldar, J. Pari, A. Rai, and L. Pinto. Teach a robot to fish: Versatile imitation from one
minuteofdemonstrations. InRSS,2023.
[10] T.Chen,M.Tippur,S.Wu,V.Kumar,E.Adelson,andP.Agrawal. Visualdexterity: In-hand
reorientationofnovelandcomplexobjectshapes. ScienceRobotics,2023.
[11] H. Qi, A. Kumar, R. Calandra, Y. Ma, and J. Malik. In-hand object rotation via rapid motor
adaptation. InCoRL,2022.
[12] Z.-H.Yin,B.Huang,Y.Qin,Q.Chen,andX.Wang.Rotatingwithoutseeing:Towardsin-hand
dexteritythroughtouch. InRSS,2023.
[13] Y. Yuan, H. Che, Y. Qin, B. Huang, Z.-H. Yin, K.-W. Lee, Y. Wu, S.-C. Lim, and X. Wang.
Robotsynesthesia: In-handmanipulationwithvisuotactilesensing. InICRA,2024.
[14] A.Bhatt,A.Sieler,S.Puhlmann,andO.Brock. Surprisinglyrobustin-handmanipulation: An
empiricalstudy. InRSS,2021.
[15] S.Patidar,A.Sieler,andO.Brock. In-handcubereconfiguration: Simplified. InIROS,2023.
[16] A.M.Okamura,N.Smaby,andM.R.Cutkosky. AnOverviewofDexterousManipulation. In
ICRA,2000.
[17] L.HanandJ.C.Trinkle. Dextrousmanipulationbyrollingandfingergaiting. InICRA,1998.
[18] Y.BaiandC.K.Liu. Dexterousmanipulationusingbothpalmandfingers. InICRA,2014.
[19] A.SielerandO.Brock. Dexteroussofthandslinearizefeedback-controlforin-handmanipu-
lation. InIROS,2023.
[20] A.S.Morgan,K.Hang,B.Wen,K.Bekris,andA.M.Dollar. Complexin-handmanipulation
viacompliance-enabledfingergaitingandmulti-modalplanning. RA-L,2022.
[21] B.Wen,C.Mitash,B.Ren,andK.E.Bekris. Se(3)-tracknet: Data-driven6dposetrackingby
calibratingimageresidualsinsyntheticdomains. InIROS,2020.
[22] N. C. Dafle, A. Rodriguez, R. Paolini, B. Tang, S. S. Srinivasa, M. Erdmann, M. T. Mason,
I. Lundberg, H. Staab, and T. Fuhlbrigge. Extrinsic dexterity: In-hand manipulation with
externalforces. InICRA,2014.
[23] S. P. Arunachalam, I. Gu¨zey, S. Chintala, and L. Pinto. Holo-dex: Teaching dexterity with
immersivemixedreality. InICRA,2023.
10[24] Y.Qin,H.Su,andX.Wang.Fromonehandtomultiplehands:Imitationlearningfordexterous
manipulationfromsingle-camerateleoperation. RA-L,2022.
[25] Y.Qin,W.Yang,B.Huang,K.VanWyk,H.Su,X.Wang,Y.-W.Chao,andD.Fox.Anyteleop:
Ageneralvision-baseddexterousrobotarm-handteleoperationsystem. InRSS,2023.
[26] Z. Si, K. L. Zhang, Z. Temel, and O. Kroemer. Tilde: Teleoperation for dexterous in-hand
manipulationlearningwithadeltahand. arXiv:2405.18804,2024.
[27] A.Iyer,Z.Peng,Y.Dai,I.Guzey,S.Haldar,S.Chintala,andL.Pinto. Openteach:Aversatile
teleoperationsystemforroboticmanipulation. arXiv:2403.07870,2024.
[28] K. Shaw, A. Agarwal, and D. Pathak. Leap hand: Low-cost, efficient, and anthropomorphic
handforrobotlearning. RSS,2023.
[29] Y.Ze,G.Zhang,K.Zhang,C.Hu,M.Wang,andH.Xu. 3ddiffusionpolicy. InRSS,2024.
[30] T.Lin,Y.Zhang,Q.Li,H.Qi,B.Yi,S.Levine,andJ.Malik. Learningvisuotactileskillswith
twomultifingeredhands. arXiv:2404.16823,2024.
[31] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In
IROS,2012.
[32] V.Makoviychuk,L.Wawrzyniak,Y.Guo,M.Lu,K.Storey,M.Macklin,D.Hoeller,N.Rudin,
A.Allshire,A.Handa,andG.State. Isaacgym: Highperformancegpu-basedphysicssimula-
tionforrobotlearning. InNeurIPSDatasetsandBenchmarks,2021.
[33] H.Qi,B.Yi,S.Suresh,M.Lambeta,Y.Ma,R.Calandra,andJ.Malik. Generalin-handobject
rotationwithvisionandtouch. InCoRL,2023.
[34] G.Khandate,S.Shang,E.T.Chang,T.L.Saidi,Y.Liu,S.M.Dennis,J.Adams,andM.Cio-
carlie. Sampling-basedexplorationforreinforcementlearningofdexterousmanipulation. In
RSS,2023.
[35] G.Khandate,T.L.Saidi,S.Shang,E.T.Chang,Y.Liu,S.Dennis,J.Adams,andM.Ciocarlie.
R×R:Rapidexplorationforreinforcementlearningviasampling-basedresetdistributionsand
imitationpre-training. arXiv:2401.15484,2024.
[36] J. Pitz, L. Ro¨stel, L. Sievers, and B. Ba¨uml. Dextrous tactile in-hand manipulation using a
modularreinforcementlearningarchitecture. InICRA,2023.
[37] M. Yang, C. Lu, A. Church, Y. Lin, C. Ford, H. Li, E. Psomopoulou, D. A. Barton, and
N. F. Lepora. Anyrotate: Gravity-invariant in-hand object rotation with sim-to-real touch.
arXiv:2405.07391,2024.
[38] Y. Toshimitsu, B. Forrai, B. G. Cangan, U. Steger, M. Knecht, S. Weirich, and R. K.
Katzschmann. Gettingtheballrolling: Learningadexterouspolicyforabiomimetictendon-
drivenhandwithrollingcontactjoints. InHumanoids,2023.
[39] A. Handa, A. Allshire, V. Makoviychuk, A. Petrenko, R. Singh, J. Liu, D. Makoviichuk,
K.VanWyk,A.Zhurkevich,B.Sundaralingam,Y.Narang,J.-F.Lafleche,D.Fox,andG.State.
Dextreme: Transferofagilein-handmanipulationfromsimulationtoreality. InICRA,2023.
[40] W.Wan,H.Geng,Y.Liu,Z.Shan,Y.Yang,L.Yi,andH.Wang. Unidexgrasp++: Improving
dexterous grasping policy learning via geometry-aware curriculum and iterative generalist-
specialistlearning. InICCV,2023.
[41] J.Chen,Y.Chen,J.Zhang,andH.Wang. Task-orienteddexterousgraspsynthesisviadiffer-
entiablegraspwrenchboundaryestimator. arXiv:2309.13586,2023.
11[42] Z.Q.Chen,K.VanWyk,Y.-W.Chao,W.Yang,A.Mousavian,A.Gupta,andD.Fox.Learning
robustreal-worlddexterousgraspingpoliciesviaimplicitshapeaugmentation. InCoRL,2022.
[43] Y.Chen,C.Wang,L.Fei-Fei,andC.K.Liu.Sequentialdexterity:Chainingdexterouspolicies
forlong-horizonmanipulation. InCoRL,2023.
[44] B.Huang,Y.Chen,T.Wang,Y.Qin,Y.Yang,N.Atanasov,andX.Wang. Dynamichandover:
Throwandcatchwithbimanualhands. InCoRL,2023.
[45] T. Lin, Z.-H. Yin, H. Qi, P. Abbeel, and J. Malik. Twisting lids off with two hands.
arXiv:2403.02338,2024.
[46] H. Xu, Y. Luo, S. Wang, T. Darrell, and R. Calandra. Towards learning to play piano with
dexteroushandsandtouch. InIROS,2022.
[47] K.Zakka,P.Wu,L.Smith,N.Gileadi,T.Howell,X.B.Peng,S.Singh,Y.Tassa,P.Florence,
A. Zeng, et al. Robopianist: Dexterous piano playing with deep reinforcement learning. In
CoRL,2023.
[48] Y. Chen, T. Wu, S. Wang, X. Feng, J. Jiang, Z. Lu, S. McAleer, H. Dong, S.-C. Zhu, and
Y.Yang. Towardshuman-levelbimanualdexterousmanipulationwithreinforcementlearning.
InNeurIPS,2022.
[49] M. Torne, A. Simeonov, Z. Li, A. Chan, T. Chen, A. Gupta, and P. Agrawal. Recon-
ciling reality through simulation: A real-to-sim-to-real approach for robust manipulation.
arXiv:2403.03949,2024.
[50] J.Wang,Y.Qin,K.Kuang,Y.Korkmaz,A.Gurumoorthy,H.Su,andX.Wang. Cyberdemo:
Augmentingsimulatedhumandemonstrationforreal-worlddexterousmanipulation.InCVPR,
2024.
[51] Y. Jiang, C. Wang, R. Zhang, J. Wu, and L. Fei-Fei. Transic: Sim-to-real policy transfer by
learningfromonlinecorrection. arXiv:2405.10315,2024.
[52] H. J. Charlesworth and G. Montana. Solving challenging dexterous manipulation tasks with
trajectoryoptimisationandreinforcementlearning. InICML,2021.
[53] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y. Zhu, L. Fan, and
A. Anandkumar. Eureka: Human-level reward design via coding large language models. In
ICLR,2024.
[54] V.Kumar,E.Todorov,andS.Levine. Optimalcontrolwithlearnedlocalmodels: Application
todexterousmanipulation. InICRA,2016.
[55] V. Kumar, A. Gupta, E. Todorov, and S. Levine. Learning dexterous manipulation policies
fromexperienceandimitation. arXiv:1611.05095,2016.
[56] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d
classificationandsegmentation. InCVPR,2017.
[57] Y. Qin, B. Huang, Z.-H. Yin, H. Su, and X. Wang. Dexpoint: Generalizable point cloud
reinforcementlearningforsim-to-realdexterousmanipulation. InCoRL,2022.
[58] C.Bao,H.Xu,Y.Qin,andX.Wang. Dexart: Benchmarkinggeneralizabledexterousmanipu-
lationwitharticulatedobjects. InCVPR,2023.
[59] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov.Proximalpolicyoptimization
algorithms. arxiv:1707.06347,2017.
[60] S.Ross,G.Gordon,andD.Bagnell.Areductionofimitationlearningandstructuredprediction
tono-regretonlinelearning. InAISTATS,2011.
12Hyper-parameters Values ObsType Dimension Hyper-parameters Values
λ rot 1.0 q t R3×16 #environments 48
λ -1.0 a t−1 R3×16 #steps 512
λz -0.3 c t R32 #minibatches 4096
λvel -0.1 p t R4×3 #epochs 2000
diff w R7
t learningrate 1e-3
λ ang -0.3 PointCloud R100×3
λ -0.1
torque Table6: Hyper-parametersfor
λ work -1.0 Table5: Dimensionsofthein- training the student policy in
putsoftheoraclepolicy. thesimulation.
Table4: Hyper-parametersfor
therewardfunction.
A ImplementationDetails
A.1 TrainingHyper-parameters
Our reward function is a combination of r ,r and r . The energy reward consists of
rot z energy
r ,r ,r ,r , and r . Here, r penalizes the pen’s linear velocity, r discourages
vel diff ang torq work vel diff
thehand’sposefromdeviatingmuchfromitsinitialpose,r penalizesthepen’sangularvelocity
ang
aboveapre-definedthresholdtoencouragestablerotation,r penalizeslargetorques,andr
torq work
penalizestheworkofthecontroller. Wefollowthesamedefinitionofrewardin[33]. Wecombine
theaboverewardswithweightslistedinTable4.
We detail the dimensions of the inputs of our oracle policy in Table 5. We train our oracle policy
withPPO,andthetraininghyper-parametersareshowninTable7. Specifically,wetrainwith8192
parallelenvironments.Eachenvironmentgathers#stepsdatatotrainineachepochofPPO.Thedata
issplitinto#minibatchesandoptimizedwithPPOloss. γandλareusedforcomputinggeneralized
advantageestimate(GAE)returns. WeusetheAdamoptimizertotrainPPOandadoptthegradient
cliptostabilizetraining. Wetrain500millionagentstepsintotal,whichtakeslessthanonedayon
asingleGPU.WetrainourstudentpolicywithBehaviorCloning,andthetraininghyper-parameters
areshowninTable6. Wecollectapproximately50Mstepsofdataintotal.
Hyper-parameters Values
#environments 8192
#steps 12
#minibatches 16384
#AgentSteps 500000000
γ 0.99
λ 0.95
learningrate 5e-3
cliprange 0.2
entropycoefficient 0.0
klthreshold 0.02
maxgradientnorm 1.0
Table7: Hyper-parametersfortrainingtheoraclepolicy.
13