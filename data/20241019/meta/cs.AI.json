[
    {
        "title": "How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs",
        "authors": "Guhao FengKai YangYuntian GuXinyue AiShengjie LuoJiacheng SunDi HeZhenguo LiLiwei Wang",
        "links": "http://arxiv.org/abs/2410.13857v1",
        "entry_id": "http://arxiv.org/abs/2410.13857v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13857v1",
        "summary": "Despite the remarkable success of Transformer-based Large Language Models\n(LLMs) across various domains, understanding and enhancing their mathematical\ncapabilities remains a significant challenge. In this paper, we conduct a\nrigorous theoretical analysis of LLMs' mathematical abilities, with a specific\nfocus on their arithmetic performances. We identify numerical precision as a\nkey factor that influences their effectiveness in mathematical tasks. Our\nresults show that Transformers operating with low numerical precision fail to\naddress arithmetic tasks, such as iterated addition and integer multiplication,\nunless the model size grows super-polynomially with respect to the input\nlength. In contrast, Transformers with standard numerical precision can\nefficiently handle these tasks with significantly smaller model sizes. We\nfurther support our theoretical findings through empirical experiments that\nexplore the impact of varying numerical precision on arithmetic tasks,\nproviding valuable insights for improving the mathematical reasoning\ncapabilities of LLMs.",
        "updated": "2024-10-17 17:59:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13857v1"
    },
    {
        "title": "Can MLLMs Understand the Deep Implication Behind Chinese Images?",
        "authors": "Chenhao ZhangXi FengYuelin BaiXinrun DuJinchang HouKaixin DengGuangzeng HanQinrui LiBingli WangJiaheng LiuXingwei QuYifei ZhangQixuan ZhaoYiming LiangZiqiang LiuFeiteng FangMin YangWenhao HuangChenghua LinGe ZhangShiwen Ni",
        "links": "http://arxiv.org/abs/2410.13854v1",
        "entry_id": "http://arxiv.org/abs/2410.13854v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13854v1",
        "summary": "As the capabilities of Multimodal Large Language Models (MLLMs) continue to\nimprove, the need for higher-order capability evaluation of MLLMs is\nincreasing. However, there is a lack of work evaluating MLLM for higher-order\nperception and understanding of Chinese visual content. To fill the gap, we\nintroduce the **C**hinese **I**mage **I**mplication understanding\n**Bench**mark, **CII-Bench**, which aims to assess the higher-order perception\nand understanding capabilities of MLLMs for Chinese images. CII-Bench stands\nout in several ways compared to existing benchmarks. Firstly, to ensure the\nauthenticity of the Chinese context, images in CII-Bench are sourced from the\nChinese Internet and manually reviewed, with corresponding answers also\nmanually crafted. Additionally, CII-Bench incorporates images that represent\nChinese traditional culture, such as famous Chinese traditional paintings,\nwhich can deeply reflect the model's understanding of Chinese traditional\nculture. Through extensive experiments on CII-Bench across multiple MLLMs, we\nhave made significant findings. Initially, a substantial gap is observed\nbetween the performance of MLLMs and humans on CII-Bench. The highest accuracy\nof MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an\nimpressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional\nculture images, suggesting limitations in their ability to understand\nhigh-level semantics and lack a deep knowledge base of Chinese traditional\nculture. Finally, it is observed that most models exhibit enhanced accuracy\nwhen image emotion hints are incorporated into the prompts. We believe that\nCII-Bench will enable MLLMs to gain a better understanding of Chinese semantics\nand Chinese-specific images, advancing the journey towards expert artificial\ngeneral intelligence (AGI). Our project is publicly available at\nhttps://cii-bench.github.io/.",
        "updated": "2024-10-17 17:59:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13854v1"
    },
    {
        "title": "Retrospective Learning from Interactions",
        "authors": "Zizhao ChenMustafa Omer GulYiwei ChenGloria GengAnne WuYoav Artzi",
        "links": "http://arxiv.org/abs/2410.13852v1",
        "entry_id": "http://arxiv.org/abs/2410.13852v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13852v1",
        "summary": "Multi-turn interactions between large language models (LLMs) and users\nnaturally include implicit feedback signals. If an LLM responds in an\nunexpected way to an instruction, the user is likely to signal it by rephrasing\nthe request, expressing frustration, or pivoting to an alternative task. Such\nsignals are task-independent and occupy a relatively constrained subspace of\nlanguage, allowing the LLM to identify them even if it fails on the actual\ntask. This creates an avenue for continually learning from interactions without\nadditional annotations. We introduce ReSpect, a method to learn from such\nsignals in past interactions via retrospection. We deploy ReSpect in a new\nmultimodal interaction scenario, where humans instruct an LLM to solve an\nabstract reasoning task with a combinatorial solution space. Through thousands\nof interactions with humans, we show how ReSpect gradually improves task\ncompletion rate from 31% to 82%, all without any external annotation.",
        "updated": "2024-10-17 17:59:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13852v1"
    },
    {
        "title": "Influence Functions for Scalable Data Attribution in Diffusion Models",
        "authors": "Bruno MlodozeniecRuna EschenhagenJuhan BaeAlexander ImmerDavid KruegerRichard Turner",
        "links": "http://arxiv.org/abs/2410.13850v1",
        "entry_id": "http://arxiv.org/abs/2410.13850v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13850v1",
        "summary": "Diffusion models have led to significant advancements in generative\nmodelling. Yet their widespread adoption poses challenges regarding data\nattribution and interpretability. In this paper, we aim to help address such\nchallenges in diffusion models by developing an \\textit{influence functions}\nframework. Influence function-based data attribution methods approximate how a\nmodel's output would have changed if some training data were removed. In\nsupervised learning, this is usually used for predicting how the loss on a\nparticular example would change. For diffusion models, we focus on predicting\nthe change in the probability of generating a particular example via several\nproxy measurements. We show how to formulate influence functions for such\nquantities and how previously proposed methods can be interpreted as particular\ndesign choices in our framework. To ensure scalability of the Hessian\ncomputations in influence functions, we systematically develop K-FAC\napproximations based on generalised Gauss-Newton matrices specifically tailored\nto diffusion models. We recast previously proposed methods as specific design\nchoices in our framework and show that our recommended method outperforms\nprevious data attribution approaches on common evaluations, such as the Linear\nData-modelling Score (LDS) or retraining without top influences, without the\nneed for method-specific hyperparameter tuning.",
        "updated": "2024-10-17 17:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13850v1"
    },
    {
        "title": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
        "authors": "Chengyue WuXiaokang ChenZhiyu WuYiyang MaXingchao LiuZizheng PanWen LiuZhenda XieXingkai YuChong RuanPing Luo",
        "links": "http://arxiv.org/abs/2410.13848v1",
        "entry_id": "http://arxiv.org/abs/2410.13848v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13848v1",
        "summary": "In this paper, we introduce Janus, an autoregressive framework that unifies\nmultimodal understanding and generation. Prior research often relies on a\nsingle visual encoder for both tasks, such as Chameleon. However, due to the\ndiffering levels of information granularity required by multimodal\nunderstanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple\nvisual encoding into separate pathways, while still leveraging a single,\nunified transformer architecture for processing. The decoupling not only\nalleviates the conflict between the visual encoder's roles in understanding and\ngeneration, but also enhances the framework's flexibility. For instance, both\nthe multimodal understanding and generation components can independently select\ntheir most suitable encoding methods. Experiments show that Janus surpasses\nprevious unified model and matches or exceeds the performance of task-specific\nmodels. The simplicity, high flexibility, and effectiveness of Janus make it a\nstrong candidate for next-generation unified multimodal models.",
        "updated": "2024-10-17 17:58:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13848v1"
    }
]