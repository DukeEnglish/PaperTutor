Preprint
UNIDRIVE: TOWARDS UNIVERSAL DRIVING
PERCEPTION ACROSS CAMERA CONFIGURATIONS
YeLi1∗ WenzhaoZheng2† XiaonanHuang1 KurtKeutzer2
1UniversityofMichigan,AnnArbor 2UniversityofCalifornia,Berkeley
https://wzzheng.net/UniDrive
ABSTRACT
Vision-centricautonomousdrivinghasdemonstratedexcellentperformancewith
economical sensors. As the fundamental step, 3D perception aims to infer 3D
information from 2D images based on 3D-2D projection. This makes driving
perceptionmodelssusceptibletosensorconfiguration(e.g.,cameraintrinsicsand
extrinsics)variations. However,generalizingacrosscameraconfigurationsisim-
portantfordeployingautonomousdrivingmodelsondifferentcarmodels. Inthis
paper, we present UniDrive, a novel framework for vision-centric autonomous
drivingtoachieveuniversalperceptionacrosscameraconfigurations. Wedeploy
a set of unified virtual cameras and propose a ground-aware projection method
to effectively transform the original images into these unified virtual views. We
further propose a virtual configuration optimization method by minimizing the
expectedprojectionerrorbetweenoriginalcamerasandvirtualcameras. Thepro-
posedvirtualcameraprojectioncanbeappliedtoexisting3Dperceptionmethods
asaplug-and-playmoduletomitigatethechallengesposedbycameraparameter
variability,resultinginmoreadaptableandreliabledrivingperceptionmodels. To
evaluatetheeffectivenessofourframework,wecollectadatasetonCarlabydriv-
ingthesamerouteswhileonlymodifyingthecameraconfigurations. Experimen-
talresultsdemonstratethatourmethodtrainedononespecificcameraconfigura-
tioncangeneralizetovaryingconfigurationswithminorperformancedegradation.
Codeandbenchmark: https://github.com/ywyeli/UniDrive.
1 INTRODUCTION
Vision-centric autonomous driving has gained significant traction (Wang et al., 2023a) due to its
abilitytodeliverhigh-performanceperceptionusingeconomicalsensorslikecameras. Atthecore
of this approach lies 3D perception (Liu et al., 2022), which reconstructs 3D spatial information
from 2D images via 2D-3D lift transform (Philion & Fidler, 2020). This transform is critical for
enabling vehicles to understand their environment, detect objects, and navigate safely. Previous
works(Huangetal.,2021;Xieetal.,2022;Readingetal.,2021;Lietal.,2022;Zhou&Kra¨henbu¨hl,
2022; Zeng et al., 2024; Lu et al., 2022; Huang & Huang, 2022; Liu et al., 2023a) have achieved
remarkable 3D perception ability by utilizing Bird’s Eye View (BEV) representations to process
2D-3D lift. Recently, many vision-based 3D occupancy prediction methods (Huang et al., 2023;
Wei et al., 2023; Huang et al., 2024a;b; Zhao et al., 2024) further improved the understanding of
dynamicandcluttereddrivingscenes,pushingtheboundariesoftheresearchdomain. Asaresult,
vision-basedsystemshavebecomeoneoftheprimarysolutionsforscalableautonomousdriving.
Despitetheexcitingdevelopmentofthestate-of-the-artvision-basedautonomousdriving(Liuetal.,
2024;Zongetal.,2023;Zhengetal.,2024b;a;Huetal.,2023;Jiangetal.,2023a),acriticallimi-
tationstillremains: thesensitivityofthesemodelstovariationsincameraconfigurations,including
intrinsicsandextrinsics.Autonomousdrivingmodelstypicallyrelyonwell-calibratedsensorsetups,
and even slight deviations in camera parameters across different vehicles or platforms can signifi-
cantlydegradeperformance(Wangetal.,2023b). AsillustratedinFigure1,thislackofrobustness
to sensor variability makes it challenging to transfer perception models between different vehicle
∗WorkdonewhilevisitingUCBerkeley.
†Correspondingauthor.
1
4202
tcO
71
]VC.sc[
1v46831.0142:viXraPreprint
(a)Deployonsamecameraconfigurations:Succeed! (b)Deployondifferentcameraconfigurations:Fail!
Figure1: Comparisonofdeployingperceptionmodelsonthesameanddistinctconfigurations.
platforms without extensive retraining or manual adjustment. This variation necessitates training
separatemodelsforeachvehicle,whichconsumesasignificantamountofcomputationalresources.
Thus,achievinggeneralizationacrosscameraconfigurationsisessentialforthepracticaldeployment
ofvision-centricautonomousdriving.
Inthispaper, weaddresstwokeyquestionssurroundinggeneralizabledrivingperception: 1)How
canweconstructaunifiedframeworkthatenablesperceptionmodelstogeneralizeacrossdifferent
multi-cameraparameters? 2)Howcanwefurtheroptimizethegeneralizationofperceptionmodels
toensurerobustperformanceacrossvaryingmulti-cameraconfigurations?
To achieve this, we introduce UniDrive, a novel framework designed to address the challenge of
generalizingperceptionmodelsacrossmulti-cameraconfigurations. Thisframeworkdeploysaset
ofunifiedvirtualcameraspacesandleveragesaground-awareprojectionmethodtotransformorigi-
nalcameraimagesintotheseunifiedvirtualviews. Additionally,weproposeavirtualconfiguration
optimization strategy that minimizes the expected projection error between the original and vir-
tual cameras, enabling consistent 3D perception across diverse setups. Our framework serves as
aplug-and-playmoduleforexisting3Dperceptionmethods,improvingtheirrobustnesstocamera
parameter variability. We validate our framework in CARLA by training and testing models on
differentcameraconfigurations,demonstratingthatourapproachsignificantlyreducesperformance
degradation while maintaining adaptability across diverse sensor setups. To summarize, we make
thefollowingkeycontributionsinthispaper:
• Tothebestofourknowledge,UniDrivepresentsthefirstcomprehensiveframeworkdesignedto
generalizevision-centric3Dperceptionmodelsacrossdiversecameraconfigurations.
• Weintroduceanovelstrategythattransformsimagesintoaunifiedvirtualcameraspace,enhanc-
ingrobustnesstocameraparametervariations.
• Weproposeavirtualconfigurationoptimizationstrategythatminimizesprojectionerror,improv-
ingmodelgeneralizationwithminimalperformancedegradation.
• We contribute a systematic data generation platform along with a 160,000 frames multi-camera
dataset,andbenchmarkevaluatingperceptionmodelsacrossvaryingcameraconfigurations.
2 RELATED WORK
Vision-based 3D Detection. The development of camera-only 3D perception has gained great
momentum recently. Early works such as FCOS3D (Wang et al., 2021), which extended the 2D
FCOS detector (Tian et al., 2020) by adding 3D object regression branches, paved the way for
improvements in depth estimation via probabilistic modeling (Wang et al., 2022a; Chen et al.,
2022a). Later methods like DETR3D (Wang et al., 2022c), PETR (Liu et al., 2022), and Graph-
DETR3D(Chenetal.,2022b)appliedtransformer-basedarchitectureswithlearnableobjectqueries
2Preprint
in 3D space, drawing from the foundations of DETR (Zhu et al., 2021; Wang et al., 2022b), by-
passingthelimitationsofperspective-baseddetection. Recentworksutilizebird’s-eyeview(BEV)
for better 3D understanding. BEVDet (Huang et al., 2021) and M²BEV (Xie et al., 2022) effec-
tively extended the Lift-Splat-Shoot (LSS) framework (Philion & Fidler, 2020) for 3D object de-
tection. CaDDN (Reading et al., 2021) introduced explicit depth supervision in the BEV trans-
formation to improve depth estimation. In addition, BEVFormer (Li et al., 2022), CVT (Zhou &
Kra¨henbu¨hl, 2022), and Ego3RT (Lu et al., 2022) explored multi-head attention mechanisms for
viewtransformation,demonstratingfurtherimprovementsinconsistency. Tofurtherenhanceaccu-
racy, BEVDet4D (Huang & Huang, 2022), BEVFormer (Li et al., 2022), and PETRv2 (Liu et al.,
2023a)leveragedtemporalcuesinmulti-cameraobjectdetection,showingsignificantimprovements
oversingle-framemethods.
Cross Domain Perception. The cross-camera configurations problem proposed in this paper lies
in the area of cross-domain perception. Domain generalization or adaptation is to enhance model
performance on varying domains without re-training. For 2D perception, numerous cross-domain
methods,suchasfeaturedistributionalignmentandpseudo-labeling(Muandetetal.,2013;Lietal.,
2018; Dou et al., 2019; Facil et al., 2019; Chen et al., 2018; Xu et al., 2020; He & Zhang, 2020;
Zhaoetal.,2020),haveprimarilyaddresseddomainshiftscausedbyenvironmentalfactorslikerain
or low light. Recent 3D driving perception works (Hao et al., 2024; Peng et al., 2023) focus on
transferingthemodelstrainedoncleanenvironmentorperfectsensorsituationstocorruptedsensor
and noisy environments, leading to several benchmarks and methods. Cross camera configuration
isarelativelynewtopicinthisarea. Whilesomeworks(Wangetal.,2023b)findthatthemodel’s
overfitting to camera parameters can lead to degrade performance because the models learn the
fixed observation perspectives, the driving perception across camera parameters has seldom been
systematicallyinvestigated.
Sensor Configuration. Sensor configurations has been proven important in the design of percep-
tionsystems. performance(Joshi&Boyd,2008;Xuetal.,2022). Despitebeingrelativelynewin
autonomous driving research (Liu et al., 2019), sensor placement has gained significant attention.
Forinstance,Huetal.(2022)werethefirsttoexploremulti-LiDARsetupsforimproving3Dobject
detection,andLietal.(2024)studiedhowcombiningLiDARandcamerasimpactsmulti-modalde-
tectionsystems. Severalotherstudies(Jinetal.,2022;Kimetal.,2023;Caietal.,2023;Jiangetal.,
2023b) focused on the strategic positioning of roadside LiDAR sensors for vehicle-to-everything
(V2X) communication, shifting away from in-vehicle sensor placements. Although many efforts
have aimed to refine sensor configurations for better performance, the challenge of adapting per-
ception models to different sensor setups has been largely overlooked. Our research is the first to
explorethegeneralizationofdrivingperceptionmodelsacrossdiversecameraconfigurations.
3 UNIDRIVE
3.1 PROBLEMFORMULATION
In real-world multi-camera driving systems, perception models are typically trained on a specific
camera configuration with fixed intrinsic and extrinsic parameters. However, the performance of
thesemodelsoftendeteriorateswhenappliedtonewcameraconfigurations,wherethecamerasmay
havedifferentplacements,orientations,orintrinsicproperties.
PerceptionAcrossMulti-cameraConfigurations. GivenasetofcamerasC ={C ,C ,...,C },
1 2 J
eachcharacterizedbyitsintrinsicmatrixKCj ∈R3×3andextrinsicmatrixECj ∈R4×4,wherej ∈
{1,2,...,J}andJ isthenumberofcameras. Theimagescapturedbythesecamerasaredenoted
as ICj ∈ RHCj×WCj×3, where HCj and WCj are the height and width of the image ICj. When
deployingthemodeltrainedon{C ,C ,...,C }toanewsetofcameras{C′,C′,...,C′ }with
1 2 J 1 2 J′
differentcameranumbersandintrinsicandextrinsicparameters,themodelmaynolongereffectively
understandthe3Dsceneduetothedifferencesbetweenthetrainingandtestingconfigurations.
UniversalMulti-cameraRepresentation. Toaddressthetransferabilityoflearnedmodelsacross
camera configurations, we attempt to design a universal representation, which transforms images
from different camera configurations to a unified space before input to the deep learning network.
Toachievethis,weproposeaVirtualCameraProjectionapproach,whichre-projectstheviewsICj
fromtheoriginalcamerasC ={C ,C ,...,C }intoaunifiedsetofvirtualcameraconfigurations
1 2 J
3Preprint
Plug-and-Play Virtual Camera Projection Module Optimization
OptimizingVirtual
Mapto Ground-aware Configurations True 3D
Original View Depth Assumption Virtual 3D Bounding Box
Bounding Box
Project Train
a) Vision-based b) Virtual Camera c) Vision-based Iterative
Driving System Projection Perception Model Optimization
Camera Cross-configuration
Parameters Deploy
CARLA
e) Multi-camera f) VisionData d) Minimizing
Configuration Synthesis Projection Error
Figure2: OverviewofUniDriveframework. Wetransformtheinputimagesintoaunifiedvirtual
cameraspacetoachieveuniversaldrivingperception. Toestimatethedepthofpixelsinthevirtual
view for projection, we propose a ground-aware depth assumption strategy. To obtain the most
effective virtual camera space for multiple real camera configurations, we propose a data-driven
CMA-ES (Hansen, 2016) based optimization strategy. To evaluate the efficacy of our framework,
weproposeanautomaticdatagenerationplatforminCARLA(Dosovitskiyetal.,2017).
V = {V ,V ,...,V }, where K is the number of virtual cameras. The image is represented as
1 2 K
IVk ∈RHVk×WVk×3,whereHVk andWVK aretheimagesizes,andk ∈{1,2,...,K}indexesthe
virtualcameraviews. WedenoteKVk andEVk astheintrinsicandextrinsicmatricesforthevirtual
cameraV . Thisvirtualconfigurationservesasastandardizedcoordinatesystemforbothtraining
k
andinference,allowingthemodeltooperateconsistentlyacrossdifferentphysicalcamerasetups.
3.2 VIRTUALCAMERAPROJECTION
In this subsection, we explain the Virtual Camera Projection method to project points from mul-
tiple camera views onto virtual camera views using a combination of ground and cylindrical sur-
face assumptions, as shown in Figure 2. The goal is to learn a transformation function T
V←C
that maps the images from the original cameras C = {C ,C ,...,C } to the virtual cameras
1 2 J
V ={V ,V ,...,V }withminimumerrors.
1 2 K
Ground-aware Assumption. For each pixel at coordinates (uVk,vVk) in the virtual view, its 3D
coordinatesinthevirtualcameraframe(XVk,YVk,ZVk)arecalculatedbasedonthepixel’sposition
c c c
intheimageandthedepthassumptions.Letthecameraheightbeh ,thefocallengthsofthecamera
c
befVk andfVk, andtheprincipalpoint(imagecenter)be(cVk,cVk). Wefirstprojectallpixelsto
x y x y
thegroundplanetocomputetheinitialassumptionof3Dcoordinatesinvirtualcameraframeas,
XˆVk =
(uVk −cV xk)·h
c , YˆVk =h , ZˆVk =
h
c . (1)
c (vVk −cV yk)/f yVk c c c (vVk −cV yk)/f yVk
The Euclidean distance to optical center is computed as Dˆ cVk = (cid:13) (cid:13)(cid:0) Xˆ cVk,Yˆ cVk,Zˆ cVk(cid:1)(cid:13) (cid:13). Then we
comparethedistanceDˆ cVk withthresholdD 0,ifDˆ cVk <D 0,thepointsconnectedtocorresponding
pixelsintheimagesareassumedontheground,(XVk,YVk,ZVk) = (XˆVk,YˆVk,ZˆVk). IfDˆVk ≥
c c c c c c c
D , we assume that the points lie on a cylindrical-like surface at a fixed distance D from the
0 0
camera’sopticalcenter. Inthiscase,the3Dcoordinatesarecomputedas:
XVk = (uVk −cV xk)·D 0 , YVk = (vVk −cV yk)·D 0 , ZVk = f yVk ·D 0 , (2)
c dVk c dVk c dVk
wheredVk =(cid:13) (cid:13)(cid:0) uVk −cVk,vVk −cVk,fVk(cid:1)(cid:13) (cid:13).
x y y
Point-wise Projection. Once the 3D coordinates (XVk,YVk,ZVk) in the virtual camera frame
c c c
arecalculated,wetransformthepointintotheworldcoordinatesystemwithextrinsicmatrixEVk,
4Preprint
p w =EVk ·pV ck,wherepV ck =(X cVk,Y cVk,Z cVk,1)⊤isthehomogeneouscoordinateofthepointin
thevirtualcamera’sframe,andp ∈ R4 isthe3Dpointintheworldcoordinatesystem. Next,we
w
transformthepointfromtheworldcoordinatesystemintotheoriginalcamera’scoordinatesystem
usingtheinverseoftheoriginalcamera’sextrinsicmatrixpC cj =ECj−1 ·p w.Finally,weprojectthe
point back onto the original camera’s 2D image plane using its intrinsic matrix, (uCj,vCj,1)⊤ =
KCj ·pC cj = KCj ·ECj−1 ·p w. This provides the pixel coordinates (uCj,vCj) in the original
view that correspond to the pixel (uVk,vVk) in the virtual view. We denote P Vk←Cj(Dˆ cVk) as the
projection transform matrix from (uCj,vCj) in the i-th original view to (uVk,vVk) based on the
EuclideandistancetovirtualcameraopticalcenterDˆVk.
c
Image-level Transformation. The
point-wise projection is extended to Algorithm1VirtualCameraProjection
the entire image view. For each
pixel (uVk,vVk) in the k-th virtual
1: Input:{C j,KCj,ECj,ICj}J j=1,{V k,KVk,EVk}K
k=1
2: Output:{IVk}K
view, we compute the corresponding k=1
3: fork=1,2,...,Kdo
pixel (uCj,vCj) in the i-th original 4: for(uVk,vVk)inIVk do
view based on the projection matrix 5: Compute(XˆVk,YˆVk,ZˆVk),DˆVk usingequation1
P Vk←Cj(Dˆ cVk). The entire image ICj 6: ifDˆ cVk <D 0c thenc c c
ofthei-thoriginalviewiswarpedinto 7: (XVk,YVk,ZVk)←(XˆVk,YˆVk,ZˆVk)
the virtual view IVk←Cj as follows, 8: else c c c c c c
IVk←Cj = T(ICj,P Vk←Cj(Dˆ cVk)), 9: Compute(X cVk,Y cVk,Z cVk)usingequation2
where T(I,P) represents the warping 10: endif
functionappliedtotheimageICj using 11: p w ←EVk ·pV ck,pV ck =(X cVk,Y cVk,Z cVk)
t bh ae sedpr oo nje tc ht eio Dn ˆVm k.atrix P Vk←Cj(Dˆ cVk) 1 12 3:
:
p (uC c Cj j← ,vCE jC )j ←−1 K·p Cjw
·pC cj
c 14: IVk←Cj(uVk,vVk)←ICj(uCj,vCj)
Blending MultipleViews. Since each 15: endfor
pixelinasinglevirtualviewmayhave 16: endfor
correspondingpixelsfromvariousorig- 17: IVk ← W1 (cid:80)J j=1w j·IVk←Cj usingequation3
inalview,aftertransformingeachorigi-
nalviewintothevirtualview,wemerge
allthetransformedimagesIVk←Cj toformthefinaloutputimageIVk. Thisblendingisperformed
bycomputingaweightedsumofalltheprojectedviews:
J J
IVk = W1 (cid:88) w
j
·IVk←Cj = W1 (cid:88) w
j
·T(ICj,P Vk←Cj(Dˆ cVk)), (3)
j=1 i=1
where W =
(cid:80)J
w is the total weight, and w is the blending weight for the j-th original
j=1 j j
view. The weights can be based on factors such as the angular distance between the original and
virtual views, or the proximity of the cameras. We presented the detailed computation process in
Algorithm1.
3.3 VIRTUALPROJECTIONERROR
ToevaluatetheaccuracyoftheVirtualCameraProjectionmethodinthecontextofa3Dobjectde-
tectiontask,weproposeaweightedprojectionerrormetricbasedonangulardiscrepanciesbetween
the virtual and original camera views. This method accounts for both angular deviations and the
distancefromthecamera’sopticalcentertoprovideamorerobusterrorevaluation.
AngleComputation.Givenadrivingscenarioof3Dboundingboxinformation,foreach3Dbound-
ingboxb ={(x ,y ,z )⊤}8 ,wefirstprojectitscornerpointsontotheoriginalcamera
n n,m n,m n,m m=1
C
j
as the pixel (uC n,j m,v nC ,j m), using the intrinsic matrix KCj and extrinsic matrix ECj. Then, we
usetheinverseofthewarpingprocessP Vk←Cj tofindthecorrespondingpixel(uV nk ,m,v nV ,k m)inthe
virtualcameraviewV kforeachcornerpoint. Wecomputethepitchangleθ nV ,k mandyawangleϕV nk
,m
relativetothevirtualcamera’sopticalcenter:
vVk −cVk uVk −cVk
θVk =arctan( n,m y ),ϕVk =arctan( n,m x ). (4)
n,m f yVk n,m f xVk
5Preprint
Algorithm2VirtualCameraConfigurationOptimization
1: Initialize: t←0,m(0),σ(0),C(0),N t,M t,∀t∈{0,1,2,...,T}
2: fort=0,1,2,...,T do
3: fori=1toN tdo
4: Sampleu(t) ∼N(m(t),(σ(t))2C(t))fromδ-densitygird-levelcandidates
i
5:
CalculateE(u(t))
i
6: endfor
7: Updatem(t+1)basedonthetopM tbestsolutionsuˆ( it)viaequation7
8: Updateσ(t+1)andC(t+1)viaequation8,equation9,equation10,andequation11
9: endfor
Next, for the same corner points, we directly project to the virtual view using KVk and EVk as
(uVk′ ,vVk′ ). ThenthepitchangleθVk′ andyawangleϕVk′ are
n,m n,m n,m n,m
vVk′ −cVk uVk′ −cVk
θVk′ =arctan( n,m y ),ϕVk′ =arctan( n,m x ). (5)
n,m f yVk n,m f xVk
AngleErrorCalculation.Foreachcornerpoint,wecomputetheangularerrorbetweentheoriginal
camera projection and the corresponding point in the virtual camera. The absolute errors in pitch
(cid:12) (cid:12) (cid:12) (cid:12)
and yaw are ∆θVk = (cid:12)θVk −θVk′ (cid:12), ∆ϕVk = (cid:12)ϕVk −ϕVk′ (cid:12). We use the distance DVk of
n,m n,m n,m n,m n,m n,m n,m
eachcornerpointfromtheoriginalcamera’sopticalcenterasaweight. Thedistanceiscomputed
as DVk = (cid:13) (cid:13)(cid:0) xVk ,yVk ,zVk (cid:1)(cid:13) (cid:13). The weighted error for each corner point is then calculated as
n,m n,m n,m n,m
E nV ,k m = D nV ,k m ·(∆θ nV ,k m +∆ϕV nk ,m). The overall error for a 3D bounding box b n is obtained by
summingtheweightederrorsofitseightcornerpointsEVk = (cid:80)8 EVk .Wesumtheprojection
errorsacrossall3Dboundingboxesb ∈Btocomputetb hn etotalpm ro= j1 ectn io,m nerror
n
N N 8
(cid:88) (cid:88) (cid:88)
E = EVk = EVk . (6)
bn n,m
n=1 n=1m=1
3.4 OPTIMIZINGVIRTUALCAMERACONFIGURATIONS
Given a set of multi-camera systems, we aim to design a unified virtual camera configuration that
minimizesthereprojectionerroracrossalloriginalcameraconfigurations. Toachievethis,weadopt
theheuristicoptimizationbasedontheCovarianceMatrixAdaptationEvolutionStrategy(CMA-ES)
(Hansen,2016)tofindanoptimizedsetofvirtualcameraconfigurations.
Objective Function. Given multiple driving perception systems with varying multi-camera conf-
girations indexed by s, the total error across all systems is expressed as E = (cid:80)S E(s)(u),
total s=1
whereu = {V k,KVk,EVk}K
k=1
includesboththeintrinsicandextrinsiccameraparametersofvir-
tualmulti-cameraframework, K isthetotalquantityofvirtualcamerasandS isthetotalquantity
of multi-camera driving systems that share the same perception model. We aim to minimize this
errorbysamplingandupdatingthevirtualcameraparametersiterativelythroughaCMA-ESbased
optimizationmethod.
OptimizationMethod. OurOptimizationstrategybeginsbydefiningamultivariatenormaldistri-
butionN(m(t),(σ(t))2C(t)),wherem(t)representsthemeanvector,σ(t)denotesthestepsize,and
C(t) isthecovariancematrixatiterationt. TheconfigurationspaceU isdiscretizedwithadensity
δ, and N candidate configurations u(t) ∼ N(m(t),(σ(t))2C(t)) are sampled at each iteration t.
t i
Initializationbeginswiththeinitialmeanm(0),stepsizeσ(0),andcovariancematrixC(0) =I. The
updatedmeanvectorm(t+1) iscalculatedinthesubsequentiterationtoserveasthenewcenterfor
thesearchdistributionconcerningthevirtualcameraconfiguration. Theprocesscanbemathemati-
callyexpressedas:
m(t+1)
=(cid:88)Mt
w uˆ(t), E(uˆ(t))≥E(uˆ(t))≥···≥E(uˆ(t)), (7)
i i 1 2 Mt
i=1
6Preprint
(a)4×95◦ (b)5×75◦ (c)6×80◦a (d)6×80◦b
(e)6×70◦ (f)6×60◦ (g)8×50◦ (h)5×70◦+110◦
Figure3: Visualizedmulti-cameraconfigurations. Weillustratethemulti-viewcameraconfigu-
rationsusedinourstudy. Thereconfigurationsareinspiredbypracticalapplicationsintheindustry.
whereM isthenumberoftopsolutionsselectedtoupdatem(t+1),andw areweightsdetermined
t i
bysolutionperformance. Theevolutionpathp(t+1),whichtracksthedirectionofsuccessfulopti-
C
mizationsteps,isupdatedas:
(cid:115)
p(t+1) =(1−c )·p(t)+(cid:112) 1−(1−c )2· 1 · m(t+1)−m(t) , (8)
C C C C (cid:80)Mt w2 σ(t)
i=1 i
wherec isthelearningrateforupdatingthecovariancematrix. ThecovariancematrixC,which
C
definesthedistribution’sshapeforcameraconfigurations,isadjustedateachiterationasfollows:
C(t+1) =(1−c )C(t)+c
p(t+1)p(t+1)T
. (9)
C C C C
Similarly, the evolution path for the step size, p , is updated, and the global step size σ is then
σ
adjustedtobalanceexplorationandexploitation:
(cid:115)
(cid:112) 1 m(t+1)−m(t)
p(t+1) =(1−c )p(t)+ 1−(1−c )2· · , (10)
σ σ σ σ (cid:80)Mt w2 σ(t)
i=1 i
(cid:32) (cid:32) (cid:33)(cid:33)
c ∥p(t+1)∥
σ(t+1) =σ(t)exp σ σ −1 , (11)
d E∥N(0,I)∥
σ
wherec isthelearningrateforupdatingp ,andd isanormalizationfactorcontrollingtheadjust-
σ σ σ
mentrateoftheglobalstepsize. WepresentedthedetailedoptimizationprocessinAlgorithm2.
4 EXPERIMENTS
4.1 BENCHMARKSETUPS
Data Generation. We generate multi-view image data and 3D objects ground truth in CARLA
simulator(Dosovitskiyetal.,2017).WeusethemapsofTowns1-6tocollectdata.Weincorporate6
classesfor3Dobjectdetection,includingCar,Bus,Truck,Motorcycle,Bicycle,andPedestrian.The
datasetconsistsof500scenes(20,000frames)foreachcameraconfiguration. Wesplit250scenes
fortrainingand250scenesforvalidation.OurdatasetisorganizedastheformatofnuScenes(Caesar
etal.,2020)andcompatibletothenuscenes-devkitpythonpackageforconvenientprocessing.
Camera Configurations. We adopt several commonly used camera configurations in automotive
practice with various camera quantities, placements and field of views. These configurations are
represented in Figure 3. We set all camera resolutions to 1600×900 as nuScenes. Our camera
configurations include camera numbers from 4 to 8. For the field of view (FOV) for cameras,
weconductstudymainlyon6cameraswithFOV=60, 70, 80. Fortheplacement, wedesigntwo
7Preprint
Table1: QuantitativeresultsofBEVFusion-Cfor3Ddetectionacrosscameraconfigurations.
The detector is trained on the blue configurations and tested on all configurations directly. We
reportthemAP(↑)andclass-levelAP(↑)scoresinpercentage(%).
ConfigurationsmAP Car Bus TruckPed. MotorBic. ConfigurationsmAP Car Bus TruckPed. MotorBic.
5×70◦+110◦ 63.9 62.4 58.0 66.5 54.7 68.7 72.9 6×60 69.3 68.7 67.4 66.3 62.2 78.4 72.8
4×95◦ 4.9 4.6 5.1 3.8 3.9 3.1 4.2 4×95◦ 3.8 4.1 4.3 3.2 4.1 3.3 3.6
5×75◦ 7.2 9.5 4.8 6.2 5.1 9.0 8.3 5×75◦ 3.4 3.7 2.5 3.1 2.7 4.3 4.2
6×80◦a 8.5 11.7 8.8 8.4 6.1 8.2 7.7 6×80◦a 0.6 1.7 0.4 0.5 0.1 0.4 0.6
6×80◦b 6.9 10.0 7.2 7.8 5.2 6.1 5.6 6×80◦b 0.4 1.4 0.0 0.7 0.0 0.2 0.1
6×70◦ 67.5 65.2 61.2 69.3 57.9 79.5 72.1 6×70◦ 8.1 9.6 4.3 6.8 7.1 11.0 10.0
6×60◦ 9.2 12.4 7.0 8.0 6.5 11.9 9.4 5×70◦+110◦ 4.6 4.9 3.0 3.5 3.4 5.4 7.4
8×50◦ 0.5 0.6 0.1 0.9 0.2 0.3 0.6 8×50◦ 17.3 18.5 9.9 14.1 16.7 21.2 23.4
6×80◦a 66.7 65.4 66.2 63.7 55.8 75.9 72.9 6×80◦b 69.1 66.0 65.1 72.1 58.3 78.6 74.2
4×95◦ 3.8 4.3 5.0 3.6 3.2 2.8 3.9 4×95◦ 3.5 3.9 4.1 3.3 3.2 2.6 3.7
5×75◦ 30.4 31.2 23.4 27.8 28.6 36.9 34.2 5×75◦ 29.6 30.3 22.6 27.1 27.9 36.3 33.2
5×70◦+110◦ 9.2 10.5 6.5 8.6 7.1 8.8 13.3 6×80◦a 63.2 65.5 67.0 66.4 46.7 68.2 65.1
6×80◦b 63.3 65.4 63.8 70.9 46.3 68.1 65.4 6×60◦ 1.7 2.9 0.5 1.1 0.7 2.3 2.6
6×70◦ 16.4 18.0 9.4 13.3 14.7 22.2 20.6 6×70◦ 16.1 17.7 8.3 12.3 14.6 23.7 19.9
6×60◦ 1.8 3.3 0.8 1.5 0.6 2.3 2.4 5×70◦+110◦ 8.9 10.3 5.6 7.5 7.1 9.6 13.4
8×50◦ 0.4 0.5 0.0 0.8 0.1 0.1 0.6 8×50◦ 0.2 0.4 0.0 0.9 0.3 0.3 0.4
differenttypesofplacementasshowninFigure3(c),(d).Wealsoincludetheoriginalconfigurations
ofnuScenes(Caesaretal.,2020)datasetwithfive70◦camerasanda110◦camera.
Deteciton Method. Due to the extensive computation resource needed to benchmark the multi-
camera configurations, we only compare our method with the camera variant of BEVFusion (Liu
et al., 2023b) (abbreviated as BEVFusion-C). BEVFusion is one of the most state-of-the-art ap-
proaches in many leaderboards, such as nuScenes (Caesar et al., 2020) and Waymo (Sun et al.,
2020). Thus,webelieveourcomparisonswithBEVFusionarerepresentativeandmeaningful.
4.2 COMPARATIVESTUDY
Weconductcomparativestudiestoevaluatetheperformanceofcameraperceptionacrossconfigu-
rations. Throughouranalysis,weareabletodemonstratetheeffectivenessofUniDriveframework.
Effectiveness of UniDrive. In Table 1 and 2, we present the 3D object detection results of
BEVFusion-C (Liu et al., 2023b) and UniDrive. The models are trained on one configuration
and tested on other varying camera configurations. Table 1 demonstrates that the performance of
BEVFusion-C degrades a lot when deployed on cross-camera configuration tasks, nearly unus-
able on other configurations. As shown in Table 2, we train the models using our plug-and-play
UniDriveframework. ThedetectionperformancesignificantlyimprovescomparedtoBEVFusion-
C(Liuetal.,2023b). Ourmethodonlyexperienceslittleperformancedegradationoncross-camera
configurationtasks. WepresentmoreresultsinFigure4,whichcomprehensivelyshowstheeffec-
tivenessofourframework.
OptimizationviaUniDrive. TodemonstratetheimportanceofoptimizationinUniDrive,wecom-
paretheperceptionperformancebetweenoptimizedvirtualcameraconfigurationsandintuitiveone
inFigure4. Theintuitivevirtualcameraconfigurationplacesallcamerasinthecenterofthevehicle
roof. AsshowninFigure4(b),althoughtheintuitivesetup(withoutoptimizing)alsosignificantly
improved cross-camera configuration perception performance compared to BEVFusion-C (in Fig-
ure 4 (a)), it exhibited a clear preference for certain configurations while performing poorly on
others. Incontrast,theoptimizedvirtualcameraparameters(inFigure4(c))demonstratedgreater
adaptability,showingrelativelyconsistentperformanceacrossvariousconfigurations.Thisiscrucial
fortheconcurrentdevelopmentofmultiplemulti-cameraperceptionsystemsinautonomousdriving.
8Preprint
Table2: QuantitativeresultsofUniDrivefor3Ddetectionacrosscameraconfigurations. The
detectoris trainedon the blue configurations andtested onallconfigurations directly. We report
themAP(↑)andclass-levelAP(↑)scoresinpercentage(%).
ConfigurationsmAP Car Bus TruckPed. MotorBic. ConfigurationsmAP Car Bus TruckPed. MotorBic.
5×70◦+110◦ 68.8 67.5 64.8 71.9 59.1 73.6 75.9 6×60◦ 64.6 63.4 58.2 59.7 59.2 76.7 70.0
4×95◦ 60.1 59.1 57.2 58.4 59.2 68.6 67.8 4×95◦ 58.9 57.7 54.1 56.9 53.1 65.2 67.4
5×75◦ 66.7 64.5 65.9 67.8 59.6 72.3 70.1 5×75◦ 62.2 59.5 60.6 65.8 53.6 70.3 63.1
6×80◦a 69.4 68.4 68.1 67.5 57.8 78.2 76.1 6×80◦a 64.4 65.1 64.9 65.5 53.3 70.2 67.1
6×80◦b 65.8 64.7 62.0 63.9 55.8 76.7 71.7 6×80◦b 65.7 63.2 63.1 63.9 55.8 76.7 71.7
6×70◦ 68.4 66.8 64.3 69.8 57.6 79.1 72.8 6×70◦ 65.0 62.9 60.8 65.4 55.1 73.0 72.8
6×60◦ 63.1 60.6 57.4 58.8 59.2 73.0 69.6 5×70◦+110◦ 63.6 61.2 61.4 64.8 55.2 71.0 68.1
8×50◦ 58.9 57.1 55.4 56.1 51.1 69.7 64.1 8×50◦ 63.8 60.2 58.3 62.6 56.8 74.2 70.5
6×80◦a 69.4 69.0 67.7 66.6 58.6 78.4 76.1 6×80◦b 63.1 63.2 61.9 59.8 53.4 71.1 69.4
4×95◦ 55.9 56.4 58.4 52.7 48.3 59.5 60.1 4×95◦ 53.6 51.2 48.0 49.7 51.0 61.5 60.1
5×75◦ 65.2 63.6 64.8 66.8 57.0 70.9 68.3 5×75◦ 62.7 62.1 60.6 62.2 56.3 69.9 67.0
5×70◦+110◦ 63.7 61.3 58.9 60.5 59.9 72.8 68.8 6×80◦a 64.5 62.6 60.3 62.4 58.6 71.4 71.6
6×80◦b 66.2 65.2 61.1 65.1 56.1 76.3 73.2 6×60◦ 62.6 60.3 59.1 62.6 53.4 70.2 70.2
6×70◦ 68.9 67.9 63.5 70.7 58.3 79.5 73.8 6×70◦ 62.5 59.4 55.6 62.9 52.6 73.4 70.8
6×60◦ 59.6 57.2 54.0 55.7 57.0 67.5 66.1 5×70◦+110◦ 57.9 56.1 52.5 53.7 56.9 64.3 63.7
8×50◦ 61.2 60.3 58.1 59.9 54.5 68.2 65.9 8×50◦ 58.4 60.3 57.0 54.3 53.9 64.4 60.2
4.3 ABLATIONSTUDY
Inthissection,weanalyzetheinterplaybetweenourproposedvirtualprojectionstrategyandpercep-
tionperformancetoaddressthesequestions: 1)What’stheimpactofcameraextrinsicandintrinsic
forcross-configurationperception? 2)HowUniDriveworkstowardstheseparametersseparately?
CameraIntrinsics. Changesincameraintrinsicsposethegreatestchallengeforcross-camerapa-
rameter perception. In Figure 4 (a), BEVFusion-C almost entirely fails when tasked on distinct
camera intrinsics with the detection accuracy mostly under 20%. For instance, BEVFusion only
gets 1.8% when deploying models trained on 6×80◦a to 6×60◦. In contrast, in Figure 5, our
UniDrive framework demonstrates substantial robustness, with performance dropping by at most
9.8%underthelargestintrinsicdifferences,whichhighlightstheeffectivenessofourapproach.
CameraHeight. Thevariationintheverticalpositionofcamerascansignificantlyimpactpercep-
tionperformance,ascamerasatvaryingheightscaptureimageswithdistinctgeometricfeatures.We
performexperimentsspecificallyforvaryingcameraheightsat1.6meters, 1.4meters, 1.8meters,
and 2.5 meters. We train the model on 1.6 meters and test on other configuraitons. As shown in
Fig.5b. BEVFusion-Cexperiencesasubstantialperformancedropformorethan10%,whenfaced
withvaryingcameraheights. Incontrast,UniDrivesignificantlyimprovesperformanceacrossdif-
ferentcameraheights,demonstratingenhancedrobustnesswithonly3.0%performancedecreasing.
Camera Placement. Changing the camera’s horizontal position and orientation on presents a
relatively smaller challenge for cross-camera parameter perception. As shown in Figure 4 (a),
BEVFusion-C experiences a performance drop of 5.9% when deploying the model trained on the
6×80◦bconfigurationtothe6×80◦aconfiguration. Nonetheless,ourUniDriveframeworkfurther
enhances cross-camera parameter perception performance. In Figure 4 (c), we train the model on
the6×80circaconfigurationandtestonotherconfigurations, UniDriveonlyexperiencesa4.6%
whendeployingthemodeltrainedonthe6×80◦bconfigurationtothe6×80◦aconfiguration.
4.4 ANALYSIS
In this section, we further investigate some useful insight points found in the benchmark experi-
ments: 1) What’s the impact of inconsistency in multi-camera intrinsics for perception? 2) How
UniDriveworkstowardsthisinconsistency?
9Preprint
test test test
4x95 4.9 3.5 3.8 2.5 0.8 0.4 60.4 4x95 55.3 49.2 51.7 54.3 62.1 58.8 60.8 4x95 60.1 53.6 55.9 57.1 58.9 61.2 63.2
5x75 7.2 29.6 30.4 24.1 9.4 64.9 3.8 5x75 54.2 58.1 61.2 59.2 58.3 64.3 57.7 5x75 58.9 62.7 65.2 64.5 62.2 65.4 60.1
6x60 9.2 1.7 1.8 18.3 69.3 8.7 0.2 6x60 57.1 53.3 57.9 62.3 63.0 62.1 55.2 6x60 63.1 62.6 59.6 64.3 64.6 64.3 59.2
6x70 67.5 16.1 16.4 69.1 62.6 28.3 0.9 6x70 59.3 56.3 64.3 67.2 61.2 65.9 59.4 6x70 68.4 62.5 68.9 66.1 65.0 62.1 60.3
6x80a 8.5 63.2 66.7 14.5 0.6 24.6 4.6 6x80a 60.1 62.9 65.6 63.4 59.4 61.2 58.2 6x80a 69.4 64.5 69.4 67.2 64.4 58.9 62.3
6x80b 6.9 69.1 63.3 17.0 0.4 22.1 5.1 6x80b 63.2 63.1 64.4 66.2 61.0 60.4 59.9 6x80b 65.8 63.1 66.2 67.9 65.7 59.6 60.9
5x70+110 63.9 8.9 9.2 61.3 4.6 13.2 0.8 5x70+110 63.3 53.2 62.0 66.9 58.1 62.8 53.2 5x70+110 68.8 57.9 63.7 65.4 63.6 63.5 57.2
(a)BEVFusion-C(Liuetal.,2023b) (b)UniDrive(w/ooptimizing) (c)UniDrive(w/optimizing)
Figure4:PerformanceevaluationsofBEVFusion-CandUniDriveon3Dobjectdetectionacross
cameraconfigurations. WereportthemAP(↑)scoresinpercentage(%).
BEVFusion-C UniDrive BEVFusion-C UniDrive BEVFusion-C UniDrive
75 75
100
80 70 70
60 65 65
40
60 60
20
0 55 55
6x80a 6x75 6x70 6x60 6x1.6 6x1.4 6x1.8 6x2.5 6x80a 6x80b 6x80c
(a)Cameraintrinsic (b)Cameraheight (c)Cameraplacement
Figure 5: Ablation Study of BEVFusion-C and UniDrive on 3D object detection across camera
configurations. WereportthemAP(↑)scoresinpercentage(%).
DegradationwithInconsistentIntrinsics. Inourexperiments,weobservedthatformulti-camera
systems,modelsperformbetterwhencameraintrinsicsareconsistentcomparedtowhentheyvary.
However, duetodesignaestheticsandotherconstraints, manyautonomousdrivingcompaniesuse
multiplecameraswithdifferentintrinsicparameterstoachieve360-degreeperception. Forinstance,
thenuScenes(Caesaretal.,2020)usesfive70◦ camerasandone110◦ camera. AsshowninFig1,
BEVFusion-Cperformsalotbetterin6×80◦aand6×60◦comparedtoconfiguration5×70◦+110◦.
Thus,inconsistencyincameraintrinsicscanpotentiallyhinderperceptionimprovement.
ImprovementviaUniDrive. Ourframeworksignificantlyenhancestheperceptionperformanceof
multi-camera systems with varying intrinsics by leveraging a virtual camera system with consis-
tent intrinsics. For training and testing on the same configurations, as demonstrated in Figure 2,
UniDrvie achieves 68.8% accuracy in 5×70◦ +110◦ configuration, which surpasses 4.9% than
BEVFusion-C(63.9%). Fortestingacrosscameraconfigurations,UniDriveexperienceslittleaccu-
racyreductiononlyinraresituations. ThisdemonstratesthatUniDrivehassubstantialpotentialto
pushadvancementsindrivingperceptiontechnology.
5 CONCLUSION
In this paper, we introduce the UniDrive framework, a robust solution for enhancing the gener-
alization of vision-centric autonomous driving models across varying camera configurations. By
leveragingaunifiedsetofvirtualcamerasandaground-awareprojectionmethod,ourapproachef-
fectively mitigates the challenges posed by camera intrinsics and extrinsics. The proposed virtual
configurationoptimizationensuresminimalprojectionerror,enablingadaptableandreliableperfor-
mance across diverse sensor setups. Extensive experiments in CARLA validate the effectiveness
ofUniDrive,demonstratingstronggeneralizationcapabilitieswithminimalperformanceloss. Our
framework not only serves as a plug-and-play module for existing 3D perception models but also
pavesthewayformoreversatileandscalableautonomousdrivingsolutions.
Limitation. Thecameraconfigurationsanalyzedinthispapercannotcoverallreal-worldsetups,
morecomprehensiveexperimentsmayberequired. Inaddition,ourresearcharefullyconductedon
simulationdata,asreal-worldexperimentsaretime-consumingandneedextensiveresource.
10
7.66 4.96
8.44
5.96
4.61
9.86
8.1
6.95 7.66
4.96
2.46
1.96
9.36
2.76
2.65
4.66 7.66
4.96
3.36
2.66
4.26 8.46Preprint
REFERENCES
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush
Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for
autonomousdriving. InCVPR,pp.11621–11631,2020.
XinyuCai,WentaoJiang,RunshengXu,WenquanZhao,JiaqiMa,SiLiu,andYikangLi.Analyzing
infrastructure lidar placement with realistic lidar simulation library. In ICRA, pp. 5581–5587,
2023.
HanshengChen,PichaoWang,FanWang,WeiTian,LuXiong,andHaoLi. Epro-pnp: Generalized
end-to-endprobabilisticperspective-n-pointsformonocularobjectposeestimation. InCVPR,pp.
2781–2790,2022a.
YuhuaChen,WenLi,ChristosSakaridis,DengxinDai,andLucVanGool. Domainadaptivefaster
r-cnnforobjectdetectioninthewild. InCVPR,pp.3339–3348,2018.
Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinhong Jiang, and Feng Zhao. Graph-
detr3d: rethinking overlapping regions for multi-view 3d object detection. In ACM MM, pp.
5999–6008,2022b.
AlexeyDosovitskiy,GermanRos,FelipeCodevilla,AntonioLopez,andVladlenKoltun. CARLA:
Anopenurbandrivingsimulator. InCoRL,pp.1–16,2017.
QiDou,DanielCoelhodeCastro,KonstantinosKamnitsas,andBenGlocker. Domaingeneraliza-
tionviamodel-agnosticlearningofsemanticfeatures. NeurIPS,32,2019.
JoseMFacil,BenjaminUmmenhofer,HuizhongZhou,LuisMontesano,ThomasBrox,andJavier
Civera. Cam-convs: Camera-awaremulti-scaleconvolutionsforsingle-viewdepth. InCVPR,pp.
11826–11835,2019.
NikolausHansen. Thecmaevolutionstrategy: Atutorial. arXivpreprintarXiv:1604.00772,2016.
Xiaoshuai Hao, Mengchuan Wei, Yifan Yang, Haimei Zhao, Hui Zhang, Yi Zhou, Qiang Wang,
WeimingLi,LingdongKong,andJingZhang. Isyourhdmapconstructorreliableundersensor
corruptions? arXivpreprintarXiv:2406.12214,2024.
ZhenweiHeandLeiZhang. Domainadaptiveobjectdetectionviaasymmetrictri-wayfaster-rcnn.
InECCV,pp.309–324,2020.
Hanjiang Hu, Zuxin Liu, Sharad Chitlangia, Akhil Agnihotri, and Ding Zhao. Investigating the
impactofmulti-lidarplacementonobjectdetectionforautonomousdriving. InCVPR,pp.2550–
2559,2022.
Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du,
TianweiLin,WenhaiWang,etal. Planning-orientedautonomousdriving. InCVPR,pp.17853–
17862,2023.
JunjieHuangandGuanHuang. Bevdet4d: Exploittemporalcuesinmulti-camera3dobjectdetec-
tion. arXivpreprintarXiv:2203.17054,2022.
JunjieHuang,GuanHuang,ZhengZhu,YunYe,andDalongDu. Bevdet: High-performancemulti-
camera3dobjectdetectioninbird-eye-view. arXivpreprintarXiv:2112.11790,2021.
Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, and Jiwen Lu. Tri-perspective view
forvision-based3dsemanticoccupancyprediction. InCVPR,pp.9223–9232,2023.
YuanhuiHuang,WenzhaoZheng,BoruiZhang,JieZhou,andJiwenLu. Selfocc: Self-supervised
vision-based3doccupancyprediction. InCVPR,pp.19946–19956,2024a.
Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, and Jiwen Lu. Gaussianformer:
Scene as gaussians for vision-based 3d semantic occupancy prediction. arXiv preprint
arXiv:2405.17429,2024b.
11Preprint
BoJiang,ShaoyuChen,QingXu,BenchengLiao,JiajieChen,HelongZhou,QianZhang,Wenyu
Liu, ChangHuang, andXinggangWang. Vad: Vectorizedscenerepresentationforefficientau-
tonomousdriving. arXivpreprintarXiv:2303.12077,2023a.
WentaoJiang,HaoXiang,XinyuCai,RunshengXu,JiaqiMa,YikangLi,GimHeeLee,andSiLiu.
Optimizingtheplacementofroadsidelidarsforautonomousdriving. InICCV,pp.18381–18390,
2023b.
Shaojie Jin, Ying Gao, Fei Hui, Xiangmo Zhao, Cheng Wei, Tao Ma, and Weihao Gan. A novel
informationtheory-basedmetricforevaluatingroadsidelidarplacement. IEEESensorsJournal,
22(21):21009–21023,2022.
Siddharth Joshi and Stephen Boyd. Sensor selection via convex optimization. IEEE Transactions
onSignalProcessing,57(2):451–462,2008.
Tae-Hyeong Kim, Gi-Hwan Jo, Hyeong-Seok Yun, Kyung-Su Yun, and Tae-Hyoung Park. Place-
ment method of multiple lidars for roadside infrastructure in urban environments. Sensors, 23
(21):8808,2023.
HaoliangLi,SinnoJialinPan,ShiqiWang,andAlexCKot. Domaingeneralizationwithadversarial
featurelearning. InCVPR,pp.5400–5409,2018.
YeLi,HanjiangHu,ZuxinLiu,XiaohaoXu,XiaonanHuang,andDingZhao. Influenceofcamera-
lidar configuration on 3d object detection for autonomous driving. In ICRA, pp. 9018–9025,
2024.
Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng
Dai. Bevformer: Learning bird’s-eye-view representation from multi-camera images via spa-
tiotemporaltransformers. InECCV,pp.1–18,2022.
FengLiu,TengtengHuang,QianjingZhang,HaotianYao,ChiZhang,FangWan,QixiangYe,and
Yanzhao Zhou. Ray denoising: Depth-aware hard negative sampling for multi-view 3d object
detection. InECCV,2024.
YingfeiLiu,TiancaiWang,XiangyuZhang,andJianSun. Petr: Positionembeddingtransformation
formulti-view3dobjectdetection. InECCV,pp.531–548,2022.
YingfeiLiu,JunjieYan,FanJia,ShuailinLi,AqiGao,TiancaiWang,andXiangyuZhang. Petrv2:
A unified framework for 3d perception from multi-camera images. In ICCV, pp. 3262–3272,
2023a.
Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L. Rus, and Song
Han. Bevfusion: Multi-task multi-sensor fusion with unified bird’s-eye view representation. In
ICRA,pp.2774–2781,2023b.
ZuxinLiu,MansurArief,andDingZhao.Whereshouldweplacelidarsontheautonomousvehicle?-
anoptimaldesignapproach. InICRA,pp.2793–2799,2019.
JiachenLu,ZheyuanZhou,XiatianZhu,HangXu,andLiZhang. Learningego3drepresentation
asraytracing. InECCV,pp.129–144,2022.
KrikamolMuandet,DavidBalduzzi,andBernhardScho¨lkopf. Domaingeneralizationviainvariant
featurerepresentation. InICML,pp.10–18,2013.
Xidong Peng, Runnan Chen, Feng Qiao, Lingdong Kong, Youquan Liu, T Wang, X Zhu, and
Y Ma. Learning to adapt sam for segmenting cross-domain point clouds. arXiv preprint
arXiv:2310.08820,2023.
JonahPhilionandSanjaFidler. Lift, splat, shoot: Encodingimagesfromarbitrarycamerarigsby
implicitlyunprojectingto3d. InECCV,2020.
Cody Reading, Ali Harakeh, Julia Chae, and Steven L Waslander. Categorical depth distribution
networkformonocular3dobjectdetection. InCVPR,pp.8555–8564,2021.
12Preprint
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui,
James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan
Ngiam,HangZhao,AlekseiTimofeev,ScottEttinger,MaximKrivokon,AmyGao,AdityaJoshi,
YuZhang,JonathonShlens,ZhifengChen,andDragomirAnguelov. Scalabilityinperceptionfor
autonomousdriving: Waymoopendataset. InCVPR,pp.2446–2454,2020.
Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: A simple and strong anchor-free object
detector. TPAMI,44(4):1922–1933,2020.
Shihao Wang, Yingfei Liu, Tiancai Wang, Ying Li, and Xiangyu Zhang. Exploring object-centric
temporalmodelingforefficientmulti-view3dobjectdetection. InICCV,pp.3621–3631,2023a.
ShuoWang,XinhaiZhao,Hai-MingXu,ZehuiChen,DamengYu,JiahaoChang,ZhenYang,and
FengZhao. Towardsdomaingeneralizationformulti-view3dobjectdetectioninbird-eye-view.
pp.13333–13342,2023b.
Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin. Fcos3d: Fully convolutional one-stage
monocular3dobjectdetection. InICCV,pp.913–922,2021.
TaiWang,ZHUXinge,JiangmiaoPang,andDahuaLin. Probabilisticandgeometricdepth: Detect-
ingobjectsinperspective. InCoRL,pp.1475–1485,2022a.
Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for
transformer-based detector. In Proceedings of the AAAI conference on artificial intelligence,
2022b.
Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, and Justin
Solomon. Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In CoRL,
pp.180–191,2022c.
YiWei,LinqingZhao,WenzhaoZheng,ZhengZhu,JieZhou,andJiwenLu. Surroundocc: Multi-
camera3doccupancypredictionforautonomousdriving. InICCV,pp.21729–21740,2023.
EXie,ZYu,DZhou,JPhilion,AAnandkumar,SFidler,PLuo,andJMAlvarez. M2bev: Multi-
camera joint 3d detection and segmentation with unified birds-eye view representation. arXiv
preprintarXiv:2204.05088,2022.
Chang-DongXu,Xing-RanZhao,XinJin,andXiu-ShenWei. Exploringcategoricalregularization
fordomainadaptiveobjectdetection. InCVPR,pp.11724–11733,2020.
Xiaohao Xu, Zihao Du, Huaxin Zhang, Ruichao Zhang, Zihan Hong, Qin Huang, and Bin Han.
Optimizationofforcemyographysensorplacementforarmmovementrecognition. InIROS,pp.
9845–9850,2022.
ShuaiZeng,WenzhaoZheng,JiwenLu,andHaibinYan. Hardness-awarescenesynthesisforsemi-
supervised3dobjectdetection. TMM,2024.
GanlongZhao, GuanbinLi, RuijiaXu, andLiangLin. Collaborativetrainingbetweenregionpro-
posallocalizationandclassificationfordomainadaptiveobjectdetection. InECCV,pp.86–102,
2020.
Linqing Zhao, Xiuwei Xu, Ziwei Wang, Yunpeng Zhang, Borui Zhang, Wenzhao Zheng, Dalong
Du, Jie Zhou, and Jiwen Lu. Lowrankocc: Tensor decomposition and low-rank recovery for
vision-based3dsemanticoccupancyprediction. InCVPR,pp.9806–9815,2024.
WenzhaoZheng, WeiliangChen, YuanhuiHuang, BoruiZhang, YueqiDuan, andJiwenLu. Occ-
world: Learninga3doccupancyworldmodelforautonomousdriving. InECCV,2024a.
Wenzhao Zheng, Ruiqi Song, Xianda Guo, and Long Chen. Genad: Generative end-to-end au-
tonomousdriving. InECCV,2024b.
Brady Zhou and Philipp Kra¨henbu¨hl. Cross-view transformers for real-time map-view semantic
segmentation. InCVPR,pp.13760–13769,2022.
13Preprint
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:
Deformabletransformersforend-to-endobjectdetection. InICLR,2021.
ZhuofanZong,DongzhiJiang,GuangluSong,ZeyueXue,JingyongSu,HongshengLi,andYuLiu.
Temporalenhancedtrainingofmulti-view3dobjectdetectorviahistoricalobjectprediction. In
ICCV,pp.3781–3790,2023.
14