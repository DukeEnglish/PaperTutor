INFLUENCE FUNCTIONS FOR SCALABLE DATA ATTRI-
BUTION IN DIFFUSION MODELS
BrunoMlodozeniec,1,2 RunaEschenhagen1 JuhanBae3,4
∗
AlexanderImmer2,5 DavidKrueger6 RichardTurner1,7
1DepartmentofEngineering,UniversityofCambridge,UK
2MaxPlanckInstituteforIntelligentSystems,Tu¨bingen,Germany
3DepartmentofComputerScience,UniversityofToronto,Canada
4VectorInstitute,Toronto,Canada
5DepartmentofComputerScience,ETHZurich,Switzerland
6MILA–QuebecAIInstitute,Montreal,Canada
7TheAlanTuringInstitute,London,UK
ABSTRACT
Diffusionmodelshaveledtosignificantadvancementsingenerativemodelling.
Yet their widespread adoption poses challenges regarding data attribution and
interpretability.his paper, we aim to help address such challenges in diffusion
modelsbydevelopinganinfluencefunctionsframework. Influencefunction-based
dataattributionmethodsapproximatehowamodel’soutputwouldhavechanged
ifsometrainingdatawereremoved. Insupervisedlearning,thisisusuallyused
forpredictinghowthelossonaparticularexamplewouldchange. Fordiffusion
models, we focus on predicting the change in the probability of generating a
particularexampleviaseveralproxymeasurements. Weshowhowtoformulate
influencefunctionsforsuchquantitiesandhowpreviouslyproposedmethodscanbe
interpretedasparticulardesignchoicesinourframework. Toensurescalabilityof
theHessiancomputationsininfluencefunctions,wesystematicallydevelopK-FAC
approximationsbasedongeneralisedGauss-Newtonmatricesspecificallytailored
todiffusionmodels. Werecastpreviouslyproposedmethodsasspecificdesign
choicesinourframework,andshowthatourrecommendedmethodoutperforms
previousdataattributionapproachesoncommonevaluations,suchastheLinear
Data-modellingScore(LDS)orretrainingwithouttopinfluences,withouttheneed
formethod-specifichyperparametertuning.
1 INTRODUCTION
Generativemodellingforcontinuousdatamodalities—likeimages,video,andaudio—hasadvanced
rapidlypropelledbyimprovementsindiffusion-basedapproaches. Manycompaniesnowoffereasy
accesstoAI-generatedbespokeimagecontent. However,theuseofthesemodelsforcommercial
purposescreatesaneedforunderstandinghowthetrainingdatainfluencestheiroutputs. Incases
wherethemodel’soutputsareundesirable,itisusefultobeabletoidentify,andpossiblyremove,the
trainingdatainstancesresponsibleforthoseoutputs. Furthermore,ascopyrightedworksoftenmake
upasignificantpartofthetrainingcorporaofthesemodels(Schuhmannetal.,2022),concernsabout
theextenttowhichindividualcopyrightowners’worksinfluencethegeneratedsamplesarise. Some
alreadycharacterisewhatthesecompaniesofferas“copyrightinfringementasaservice”(Saveri&
Butterick,2023a),whichhascausedaflurryofhigh-profilelawsuitsSaveri&Butterick(2023a;b).
Thismotivatesexploringtoolsfordataattributionthatmightbeabletoquantifyhoweachgroupof
trainingdatapointsinfluencesthemodels’outputs. Influencefunctions(Koh&Liang,2017;Bae
etal.,2022)offerpreciselysuchatool. Byapproximatingtheanswertothequestion,“Ifthemodel
wastrainedwithsomeofthedataexcluded,whatwoulditsoutputbe?”,theycanhelpfindingdata
∗Correspondenceto:bkm28@cam.ac.uk
1
4202
tcO
71
]GL.sc[
1v05831.0142:viXraGenerated Top Most Negative
sample influences neutral influences
... ...
Figure1:MostinfluentialtrainingdatapointsasidentifiedbyK-FACInfluenceFunctionsforsamples
generatedbyadenoisingdiffusionprobabilisticmodeltrainedonCIFAR-10. Thetopinfluences
arethosewhoseomissionfromthetrainingsetispredictedtomostincreasethelossofthegenerated
sample. Negativeinfluencesarethosepredictedtomostdecreasetheloss,andthemostneutralare
thosethatshouldchangethelosstheleast.
pointsmostresponsibleforalowlossonanexample,orahighprobabilityofgeneratingaparticular
example. However,theyhaveyettobescalablyadaptedtothegeneraldiffusionmodellingsetting.
Influencefunctionsworkbylocallyapproximatinghowthelosslandscapewouldchangeifsomeof
thetrainingdatapointsweredown-weightedinthetrainingloss(illustratedinFigure5).Consequently,
this enables prediction for how the (local) optimum of the training loss would change, and how
that change in the parameters would affect a measurement of interest (e.g., loss on a particular
example). Byextrapolatingthisprediction,onecanestimatewhatwouldhappenifthedatapoints
were fully removed from the training set. However, to locally approximate the shape of the loss
landscape, influence functions require computing and inverting the Hessian of the training loss,
whichiscomputationallyexpensive. Onecommonapproximationofthetrainingloss’sHessianisthe
generalisedGauss-Newtonmatrix(GGN,Schraudolph,2002;Martens,2020). TheGGNhasnotbeen
clearlyformulatedforthediffusionmodellingobjectivebeforeandcannotbeuniquelydetermined
basedonitsgeneraldefinition. Moreover,tocomputeandstoreaGGNforlargeneuralnetworks
furtherapproximationsarenecessary. WeproposeusingKronecker-FactoredApproximateCurvature
(K-FAC, Heskes, 2000; Martens & Grosse, 2015) to approximate the GGN. It is not commonly
knownhowtoapplyittoneuralnetworkarchitecturesusedindiffusionmodels;forexample,Kwon
etal.(2023)resorttoalternativeHessianapproximationmethodsbecause“[K-FAC]mightnotbe
applicabletogeneraldeepneuralnetworkmodelsasithighlydependsonthemodelarchitecture”.
However, based on recent work, it is indeed clear that it can be applied to architectures used in
diffusion models (Grosse & Martens, 2016; Eschenhagen et al., 2023), which typically combine
linearlayers,convolutions,andattention(Hoetal.,2020).
Inthiswork,wedescribeascalableapproachtoinfluencefunction-basedapproximationsfordata
attributionindiffusionmodels,usingaK-FACapproximationofGGNsasHessianapproximations.
Wearticulateadesignspacebasedoninfluencefunctions,unifypreviousmethodsfordataattribution
indiffusionmodels(Georgievetal.,2023;Zhengetal.,2024)throughourframework,andarguefor
thedesignchoicesthatdistinguishourmethodfrompreviousones. Oneimportantdesignchoiceis
theGGNusedastheHessianapproximation. WeformulatedifferentGGNmatricesforthediffusion
modellingobjectiveanddiscusstheirimplicitassumptions. Weempiricallyablatevariationsofthe
GGNandotherdesignchoicesinourframeworkandshowthatourproposedmethodoutperforms
theexistingdataattributionmethodsfordiffusionmodelsasmeasuredbycommondataattribution
metricsliketheLinearData-modellingScore(Parketal.,2023)orretrainingwithouttopinfluences.
Finally,wealsodiscussinterestingempiricalobservationsthatchallengeourcurrentunderstanding
ofinfluencefunctionsinthecontextofdiffusionmodels.
22 BACKGROUND
Thissectionintroducesthegeneralconceptsofdiffusionmodels,influencefunctions,andtheGGN.
2.1 DIFFUSIONMODELS
Diffusionmodelsareaclassofprobabilisticgenerativemodelsthatfitamodelp (x)parameterised
θ
byparametersθ ∈Rdparam toapproximateatrainingdatadistributionq(x),withtheprimaryaimbeing
tosamplenewdatax∼p (·)(Sohl-Dicksteinetal.,2015;Hoetal.,2020;Turneretal.,2024). This
θ
isusuallydonebyaugmentingtheoriginaldataxwithT fidelitylevelsasx(0:T) =[x(0),...,x(T)]
with an augmentation distribution q(x(0:T)) that satisfies the following criteria: 1) the highest
fidelity x(0) equals the original training data q(x(0)) = q(x), 2) the lowest fidelity x(T) has a
distributionthatiseasytosamplefrom,and3)predictingalowerfidelitylevelfromtheleveldirectly
above it is simple to model and learn. To achieve the above goals, q is typically taken to be a
first-orderGaussianauto-regressive(diffusion)process: q(x(t)|x(0:t 1))=N(x(t)|λ x(t 1),(1−
− t −
λ )2I), with hyperparameters λ set so that the law of x(T) approximately matches a standard
t t
GaussiandistributionN(0,I).Inthatcase,thereverseconditionalsq(x(t 1)|x(t:T))=q(x(t 1)|x(t))
− −
are first-order Markov, and if the number of fidelity levels T is high enough, they can be well
approximatedbyadiagonalGaussian,allowingthemtobemodelledwithaparametricmodelwitha
simplelikelihoodfunction,hencesatisfying(3)(Turneretal.,2024). Themarginalsq(x(t)|x(0))=
N x(t)| t λ x(0), 1− t λ2 I alsohaveasimpleGaussianform,allowingforthe
t′=1 t′ t′=1 t′
aug(cid:16)mente(cid:16)dsamplest(cid:17)obesa(cid:16)mpledas: (cid:17) (cid:17)
(cid:81) (cid:81)
t t 1/2
x(t) = λ x(0)+ 1− λ2 ϵ(t), withϵ(t) ∼N(0,I). (1)
t′=1
t
t′=1
t′
Diffusionmodels(cid:89) aretrainedtoappr(cid:16) oxima(cid:89) tethereve(cid:17) rseconditionalsp (x(t 1)|x(t))≈q(x(t 1)|x(t))
θ − −
bymaximisinglog-probabilitiesofsamplesx(t 1)conditionedonx(t),foralltimestepst=1,...,T.
−
Wecannotethatq(x(t 1)|x(t),x(0))hasaGaussiandistributionwithmeangivenby:
−
1 1−λ2 x(t)− t λ x(0)
µ (x(t),ϵ(t))= x(t)− t ϵ(t) , withϵ(t)=def t′=1 t′
t −1 |t,0 λ t(cid:32) 1− t t′=1λ2
t′
1/2 (cid:33) (cid:0) (1− (cid:81)t t′=1λ2 t′)1/2 (cid:1)
asinEquation(1). Inotherwords(cid:0),the(cid:81)meanisa(cid:1)mixtureofthesamplex(t) andth(cid:81)enoiseϵ(t) that
wasappliedtox(0)toproduceit. Hence,wecanchoosetoanalogouslyparameterisep (x(t 1)|x(t))
θ −
asN x(t 1)|µ x(t),ϵt(x(t)) ,σ2I . Thatway,themodelϵ(t)(x(t))simplypredictsthenoise
− t 1t,0 θ t θ
− |
ϵ(t) that was added to the data to produce x(t). The variances σ2 are usually chosen as hyper-
(cid:0) (cid:0) (cid:1) (cid:1) t
parameters (Ho et al., 2020). With that parameterisation, the negative expected log-likelihood
E −logp(x(t 1)|x(t)) ,uptoscaleandshiftindependentofθorx(0),canbewritten
asq( (x Ht − o1 e,x t( at) l.| ,x( 20 0)) 20;Turneret− al.,2024):1
(cid:2) (cid:3)
ϵ(t) ∼N(0,I)
2
ℓ t(θ,x(0))=E
ϵ(t),x(t)
ϵ(t)−ϵt
θ
x(t)
t t 1/2
x(t) = λ x(0)+ 1− λ2 ϵ(t)
(cid:20)(cid:13)
(cid:13) (cid:13)
(cid:16) (cid:17)(cid:13)
(cid:13) (cid:13)
(cid:21) (cid:89)t′=1 t
(cid:16)
(cid:89)t′=1 t′
(cid:17) (2)
Thisleadstoatraininglossℓforthediffusionmodelϵt(x(t))thatisasumofper-diffusiontimestep
θ
traininglosses:2
ℓ(θ,x)=E [ℓ (θ,x)] t˜∼Uniform([T]).
t˜ t˜
TheparametersarethenoptimisedtominimisethelossaveragedoveratrainingdatasetD={x }N :
n n=1
N
1
θ⋆(D)=argminL (θ) L (θ)=def ℓ(θ,x ). (3)
θ D D N n
n=1
(cid:88)
Otherinterpretationsoftheaboveprocedureexistintheliterature(Song&Ermon,2020;Songetal.,
2021b;a;Kingmaetal.,2023).
1Notethatthetworandomvariablesx(t),ϵ(t)aredeterministicfunctionsofone-another.
2Equivalently,aweightedsumofper-timestepnegativelog-likelihoods logp (x(t−1) x(t)).
θ
− |
32.2 INFLUENCEFUNCTIONS
The aim of influence functions is to answer questions of the sort “how would my model behave
wereittrainedonthetrainingdatasetwithsomedatapointsremoved”. Todoso,theyapproximate
thechangeintheoptimalmodelparametersinEquation(3)whensometrainingexamples(x ) ,
j j
I ={i ,...,i }⊆[N],areremovedfromthedatasetD. Toarriveatatractableapproximation∈,Iit
1 M
isusefultoconsideracontinuousrelaxationofthisquestion: howwouldtheoptimumchangewere
thetrainingexamples(x ) down-weightedbyε∈Rinthetrainingloss:
j j
∈I
N
1
r (ε)=argmin ℓ(θ,x )−ε ℓ(θ,x ) (4)
−I θ N n j
n=1 j
(cid:88) (cid:88)∈I
Thefunctionr : R → Rdparam (well-definediftheoptimumisunique)istheresponsefunction.
Setting ε to 1⁄ N−rIecovers the minimum of the original objective in Equation (3) with examples
(x ,...,x )removed.
i1 iM
Undersuitableassumptions(seeAppendixA),bytheImplicitFunctionTheorem(Krantz&Parks,
2003),theresponsefunctioniscontinuousanddifferentiableatε = 0. Influencefunctionscanbe
defined as a linear approximation to theresponse function r by a first-order Taylor expansion
aroundε=0: −I
dr (ε)
r (ε)=r (0) + −I ′ ε +o(ε)
−I =θ− ⋆(I D) + dε ∇′ 2 θ⋆L(cid:12) (cid:12) (cid:12)ε D′= (0 θ⋆) −1 ∇ θ⋆ℓ(θ⋆,x j)ε +o(ε), (5)
j
(cid:88)∈I(cid:0) (cid:1)
asε → 0. SeeAppendixAforaformalderivationandconditions. Theoptimalparameterswith
examples(x i)
i
removedcanbeapproximatedbysettingεto1⁄ Nanddroppingtheo(ε)terms.
∈I
Usually,wearenotdirectlyinterestedinthechangeinparametersinresponsetoremovingsome
data,butratherthechangeinsomemeasurementfunctionm(θ⋆(D),x)ataparticulartestinputx
′ ′
(e.g. per-exampletestloss). Wecanfurthermakeafirst-orderTaylorapproximationtom(·,x)at
′
⊺
θ⋆(D)—m(θ,x) = m(θ⋆,x)+∇ m(θ⋆,x)(θ−θ⋆)+o(∥θ−θ⋆∥ )—andcombineitwith
′ ′ θ⋆ ′ 2
Equation(5)togetasimplelinearestimateofthechangeinthemeasurementfunction:
m(r (ε),x ′)=m(θ⋆,x ′)+ ∇⊺ θ⋆m(θ⋆,x ′) ∇2 θ⋆L (θ⋆) −1 ∇ θ⋆ℓ(θ⋆,x j)ε+o(ε).
(6)
−I D
j
(cid:88)∈I (cid:0) (cid:1)
2.2.1 GENERALISEDGAUSS-NEWTONMATRIX
Computing the influence function approximation in Equation (5) requires inverting the Hessian
∇2 θL (θ)∈Rdparam×dparam. Inthecontextofneuralnetworks,theHessianitselfisgenerallycompu-
tationDallyintractableandapproximationsarenecessary. AcommonHessianapproximationisthe
generalisedGauss-Newtonmatrix(GGN).WewillfirstintroducetheGGNinanabstractsetting
ofapproximatingtheHessianforageneraltraininglossL(θ)=E [ρ(θ,z)],tomakeitclearhow
z
differentvariantscanbearrivedatfordiffusionmodelsinthenextsection.
Ingeneral,ifwehaveafunctionρ(θ,z)oftheformh ◦f (θ),withh aconvexfunction,theGGN
z z z
foranexpectationE [ρ(θ,z)]isdefinedas
z
GGN(θ)=E ∇⊺ f (θ) ∇2 h (f (θ)) ∇ f (θ) ,
z θ z fz(θ) z z θ z
(cid:104) (cid:16) (cid:17) (cid:105)
where ∇ f (θ) is the Jacobian of f . Whenever f is (locally) linear, the GGN is equal to the
θ z z z
HessianE [∇2ρ(θ,z)]. Therefore,wecanconsidertheGGNasanapproximationtotheHessianin
z θ
whichwe“linearise”thefunctionf . Notethatanydecompositionofρ(θ,z)resultsinavalidGGN
z
aslongash isconvex(Martens,2020).3Wegivetwoexamplesbelow.
z
3h istypicallyrequiredtobeconvextoguaranteetheresultingGGNisapositivesemi-definite(PSD)matrix.
z
Avalidnon-PSDapproximationtotheHessiancanbeformedwithanon-convexh aswell;allthearguments
z
abouttheexactnessoftheGGNapproximationforalinearf wouldstillapply. However,thePSDproperty
z
helpswithnumericalstabilityofthematrixinversion,andguaranteesthattheGGNwillbeinvertibleifasmall
dampingtermisaddedtothediagonal.
4Option1. Atypicalchoicewouldbeforf tobetheneuralnetworkfunctiononatrainingdatapoint
z
z, and for h to be the loss function (e.g. ℓ -loss), with the expectation E being taken over the
z 2 z
empirical(training)datadistribution;wecalltheGGNforthissplitGGNmodel. TheGGNwiththis
splitisexactforlinearneuralnetworks(orwhenthemodelhaszeroresidualsonthetrainingdata)
(Martens,2020).
f :=mappingfromparameterstomodeloutput
z →GGNmodel(θ) (7)
h :=lossfunction(e.g. ℓ -loss)
z 2
Option2. Alternatively,adifferentGGNcanbedefinedbyusingatrivialsplitofthelossρ(θ,z)
intotheidentitymaph :=idandthelossf :=ρ(·,z),andagaintakingtheexpectationoverthe
z z
empiricaldatadistribution. Withthissplit,theresultingGGNis
f z :=ρ(·,z) →GGNloss(θ)=E ∇ ρ(θ,z)∇⊺ ρ(θ,z) . (8)
h :=id z θ θ
z
(cid:2) (cid:3)
ThisisalsocalledtheempiricalFisher(Kunstneretal.,2019). NotethatGGNlossisonlyequalto
theHessianunderthearguablymorestringentconditionthatρ(·,z)—thecompositionofthemodel
and the loss function — is linear. This is in contrast to GGNmodel, for which only the mapping
from the parameters to the model output needs to be (locally) linear. Hence, we might prefer to
useGGNmodelforHessianapproximationwheneverwehaveanonlinearloss,whichisthecasefor
diffusionmodels.
3 SCALABLE INFLUENCE FUNCTIONS FOR DIFFUSION MODELS
Inthissection,wediscusshowweadaptinfluencefunctionstothediffusionmodellingsettingin
ascalablemanner. Wealsorecastdataattributionmethodsfordiffusionmodelsproposedinprior
work(Georgievetal.,2023;Zhengetal.,2024)astheresultofparticulardesigndecisionsinour
framework,andargueforourownchoicesthatdistinguishourmethodfromthepreviousones.
3.1 APPROXIMATINGTHEHESSIAN
Indiffusionmodels,wewanttocomputetheHessianofthelossoftheform
L (θ)=E [ℓ(θ,x )]=E E E ∥ϵ(t˜)−ϵt˜ (x(t˜))∥2 ,
D
xn n xn t˜ x(t˜),ϵ(t˜) θ
(cid:104) (cid:104) (cid:104) (cid:105)(cid:105)(cid:105)
whereE [·]= 1 N · istheexpectationovertheempiricaldatadistribution. 4Wewillibehow
xn N n=1
toformulatedifferentGGNapproximationsforthissetting.
(cid:0) (cid:80) (cid:1)
3.1.1 GGNFORDIFFUSIONMODELS
Option 1. To arrive at a GGN approximation, as discussed in Section 2.2.1, we can partition
thefunctionθ (cid:55)→ ∥ϵ(t) −ϵt(x(t))∥2 intothemodeloutputθ (cid:55)→ ϵt(x(t))andtheℓ -lossfunction
θ θ 2
∥ϵ(t)−·∥2. ThisresultsintheGGN:
f
z
:=ϵt θ˜ (x(t˜))
→GGNmodel(θ)=E E E ∇⊺ ϵt˜ x(t˜) (2I)∇ ϵt˜ x(t˜) ,
h :=∥ϵ(t˜)−·∥2 D xn t˜ x(t˜),ϵ(t˜) θ θ θ θ
z (cid:104) (cid:104) (cid:104) (cid:16) (cid:17) (cid:16) (cid:17)(cid:105)(cid:105)(cid:105)
(9)
whereI istheidentitymatrix. Thiscorrespondto“linearising”theneuralnetworkϵt. Fordiffusion
θ
models,thedimensionalityoftheoutputofϵt˜istypicallyverylarge(e.g. 32×32×3forCIFAR),so
θ
computingtheJacobians∇ ϵt explicitlyisstillintractable. However,wecanexpressGGNmodelas
θ θ
D
F (θ)=E E E E g (θ)g (θ)⊺ , ϵ ∼N ϵt˜ x(t˜) ,I (10)
D
xn t˜ xn(t˜) ϵmod n n mod θ n
(cid:104) (cid:104) (cid:2) (cid:2) (cid:3)(cid:3)(cid:105)(cid:105) (cid:16) (cid:16) (cid:17) (cid:17)
4Generally,E mightalsosubsumetheexpectationoverdataaugmentationsappliedtothetrainingdata
xn
points(seeAppendixG.8fordetailsonhowthisishandled).
5whereg n(θ)=∇ θ∥ϵ mod−ϵt θ˜(x( nt˜))∥2 ∈Rdparam;seeAppendixBforthederivation.Thisformulation
lendsitselftoaMonteCarloapproximation,sincewecannowcomputegradientsusingauxiliary
targetsϵ sampledfromthemodel’soutputdistribution,asshowninEquation(10). F canbe
mod
interpretedasakindofFisherinformationmatrix(Amari,1998;Martens,2020),butitiDsnotthe
Fisherforthemarginalmodeldistributionp (x).
θ
Option2. AnalogouslytoEquation(8),wecanalsoconsiderthetrivialdecompositionofℓ(·,x)into
theidentitymapandtheloss,effectively“linearising”ℓ(·,x). TheresultingGGNis:
f :=ℓ(·,x )
z n →GGNloss(θ)=E [∇ ℓ(θ,x )∇⊺ ℓ(θ,x )], (11)
h
z
:=id
D
xn θ n θ n
where ℓ(θ,x) is the diffusion training loss defined in Equation (2). This Hessian approximation
GGNlossturnsouttobeequivalenttotheonesconsideredinthepreviousworksondataattribution
for difDfusion models (Georgiev et al., 2023; Zheng et al., 2024; Kwon et al., 2023). In contrast,
in this work, we opt for GGNmodel in Equation (9), or equivalently F , since it is arguably a
better-motivatedapproximationDoftheHessianthanGGNloss(c.f. SectionD 2.2.1).
D
InZhengetal.(2024),theauthorsexploredsubstitutingdifferent(theoreticallyincorrect)training
lossfunctionsintotheinfluencefunctionapproximation. Inparticular,theyfoundthatreplacingthe
loss∥ϵ(t)−ϵt(x(t))∥2withthesquarenormloss∥ϵt(x(t))∥2(effectivelyreplacingthe“targets”ϵ(t)
θ θ
with0)gavethebestresults. Notethatthetargetsϵ(t)donotappearintheexpressionforGGNmodel
inEquation(9).5 Hence,inourmethodsubstitutingdifferenttargetswouldnotaffecttheHeDssian
approximation. InZhengetal.(2024),replacingthetargetsonlymakesadifferencetotheHessian
approximationbecausetheyuseGGNloss(anempiricalFisher)toapproximatetheHessian.
D
3.1.2 K-FACFORDIFFUSIONMODELS
WhileF (θ)andGGNloss donotrequirecomputingfullJacobiansortheHessianoftheneural
networkDmodel,theyinvoDlvetakingouterproductsofgradientsofsizeRdparam,whichisstillintractable.
Kronecker-FactoredApproximateCurvature(Heskes,2000;Martens&Grosse,2015,K-FAC)isa
commonscalableapproximationoftheGGNtoovercomethisproblem. ItapproximatestheGGN
withablock-diagonalmatrix,whereeachblockcorrespondstooneneuralnetworklayerandconsists
ofaKroneckerproductoftwomatrices. DuetoconvenientpropertiesoftheKroneckerproduct,this
makestheinversionandmultiplicationwithvectorsneededinEquation(6)efficientenoughtoscale
tolargenetworks. K-FACisdefinedforlinearlayers,includinglinearlayerswithweightsharinglike
convolutions(Grosse&Martens,2016). Thiscoversmostlayertypesinthearchitecturestypically
usedfordiffusionmodels. Whenweightsharingisused,therearetwovariants–K-FAC-expandand
K-FAC-reduce(Eschenhagenetal.,2023). Forourrecommendedmethod,wechoosetoapproximate
theHessianwithaK-FACapproximationofF ,akintoGrosseetal.(2023).
D
Fortheparametersθ oflayerl,theGGNF inEquation(10)isapproximatedby
l
D
N N
F (θ )≈ 1 E E a(l)a(l)⊺ ⊗ E E b(l)b(l)⊺ , (12)
D l N2 t˜ xn(t˜),ϵ(t˜) n n t˜ x( nt˜),ϵ(t˜),ϵ m(t˜ o) d n n
n (cid:88)=1 (cid:104) (cid:104) (cid:105)(cid:105) n (cid:88)=1 (cid:104) (cid:104) (cid:105)(cid:105)
with a( nl) ∈ Rdl in being the inputs to the lth layer for data point x( nt˜) and b( nl) ∈ Rdl out being the
gradientoftheℓ -lossw.r.t. theoutputofthelthlayer,and⊗denotingtheKroneckerproduct.6 The
2
approximationtriviallybecomesanequalityforasingledatapointandalsofordeeplinearnetworks
withℓ -loss(Bernacchiaetal.,2018;Eschenhagenetal.,2023). Weapproximatetheexpectationsin
2
Equation(12)withMonteCarlosamplesanduseK-FAC-expandwheneverweightsharingisused
sincetheproblemformulationofdiffusionmodelscorrespondstotheexpandsettinginEschenhagen
etal.(2023);inthecaseofconvolutionallayersthiscorrespondstoGrosse&Martens(2016). Lastly,
toensuretheHessianapproximationiswell-conditionedandinvertible,wefollowstandardpractice
andaddadampingtermconsistingofasmallscalardampingfactortimestheidentitymatrix. We
ablatethesedesignchoicesinSection4(Figures4,7and9).
5ThisisbecausetheHessianofanℓ -lossw.r.t.themodeloutputisamultipleoftheidentitymatrix.
2
6Forthesakeofasimplerpresentationthisdoesnottakepotentialweightsharingintoaccount.
63.2 GRADIENTCOMPRESSIONANDQUERYBATCHING
Inpractice,werecommendcomputinginfluencefunctionestimatesinEquation(6)byfirstcomputing
andstoringtheapproximateHessianinverse,andtheniterativelycomputingthepreconditionedinner
products∇⊺ θ⋆m(θ⋆,x) ∇2 θ⋆L (θ⋆) −1 ∇ θ⋆ℓ(θ⋆,x j)fordifferenttrainingdatapointsx j. Following
Grosseetal.(2023),weuseqDuerybatchingtoavoidrecomputingthegradients∇ ℓ(θ⋆,x )when
θ⋆ j
(cid:0) (cid:1)
attributingmultiplesamplesx. Wealsousegradientcompression;wefoundthatcompressionby
quantisationworksmuchbetterfordiffusionmodelscomparedtotheSVD-basedcompressionused
byGrosseetal.(2023)(seeAppendixC),likelyduetothefactthatgradients∇ ℓ(θ,x )arenot
θ n
low-rankinthissetting.
3.3 WHATTOMEASURE
For diffusion models, arguably the most natural question to ask might be, for a given sample x
generatedfromthemodel,howdidthetrainingsamplesinfluencetheprobabilityofgeneratinga
samplex? Forexample,inthecontextofcopyrightinfringement,wemightwanttoaskifremoving
certaincopyrightedworkswouldsubstantiallyreducetheprobabilityofgeneratingx. Withinfluence
functions,thesequestionscouldbeinterpretedassettingthemeasurementfunctionm(θ,x)tobethe
(marginal)log-probabilityofgeneratingxfromthediffusionmodel: logp (x).
θ
Computingthemarginallog-probabilityintroducessomechallenges.Diffusionmodelshaveoriginally
beendesignedwiththegoaloftractablesampling,andnotlog-likelihoodevaluation. Hoetal.(2020);
Sohl-Dicksteinetal.(2015)onlyintroducealower-boundonthemarginallog-probability. Song
etal.(2021b)showthatexactlog-likelihoodevaluationispossible,butitonlymakessenseinsettings
where the training data distribution has a density (e.g. uniformly dequantised data), and it only
correspondstothemarginallog-likelihoodofthemodelwhensamplingdeterministically(Songetal.,
2021a).7Also,takinggradientsofthatmeasurement,asrequiredforinfluencefunctions,isnon-trivial.
Hence,inmostcases,wemightneedaproxymeasurementforthemarginalprobability. Weconsider
acoupleofproxiesinthiswork:
1. Loss.Approximatelogp (x)withthediffusionlossℓ(θ,x)inEquation(2)onthatparticular
θ
example. This corresponds to the ELBO with reweighted per-timestep loss terms (see
Figure18).
2. Probability of sampling trajectory. If the entire sampling trajectory x(0:T) that gen-
erated sample x is available, consider the probability of that trajectory p (x(0:T)) =
θ
p(xT) T p (x(t 1)|x(t)).
t=1 θ −
3. ELBO.Approximatelogp (x)withanEvidenceLower-Bound(Hoetal.,2020,eq.(5)).
(cid:81) θ
Initially,wemightexpectELBOtobethebestmotivatedproxy,asitistheonlyonewithaclear
linktothemarginallog-probability. Probabilityofsamplingtrajectorymightalsoappearsensible,
butitdoesn’ttakeintoaccountthefactthattherearemultipletrajectoriesx(0:T)thatallleadtothe
samefinalsamplex(0),andithasthedisadvantageofnotbeingreparameterisationinvariant.8We
empiricallyinvestigatethesedifferentproxiesinSection4.
4 EXPERIMENTS
EvaluatingDataAttribution. Toevaluatetheproposeddataattributionmethods,weprimarilyfocus
ontwometrics: LinearDataModellingScore(LDS)andretrainingwithouttopinfluences. LDS
measureshowwellagivenattributionmethodcanpredicttherelativemagnitudeinthechangeina
measurementasthemodelisretrainedon(random)subsetsofthetrainingdata. Foranattribution
methoda(D,D ,x)thatapproximateshowameasurementm(θ⋆(D),x)wouldchangeifamodel
′
was trained on an altered dataset D , LDS measures the Spearman rank correlation between the
′
predicted change in output and actual change in output after retraining on different subsampled
7Unlessthetrainedmodelsatisfiesveryspecific“consistency”constraints(Songetal.,2021b,Theorem2).
8Wecanrescalethelatentvariablesx(t)withoutaffectingthemarginaldistributionp (x),butchangingthe
θ
probabilitydensityofanyparticulartrajectory.
7datasets:
M M
spearman a(D,D˜ ,x) ; m(θ⋆(D˜ ),x) ,
i i
i=1 i=1
(cid:20)(cid:16) (cid:17) (cid:16) (cid:17) (cid:21)
whereD˜ areindependentlysubsampledversionsoftheoriginaldatasetD,eachcontaining50%of
i
thepointssampledwithoutreplacement. However,arealityofdeeplearningisthat,dependingonthe
randomseedusedforinitialisationandsettingtheorderinwhichthedataispresentedintraining,
trainingonafixeddatasetcanproducedifferentmodelswithfunctionallydifferentbehaviour. Hence,
foranygivendatasetD ,differentmeasurementscouldbeobtaineddependingontherandomseed
′
used. Tomitigatetheissue,Parketal.(2023)proposetouseanensembleaveragemeasurementafter
retrainingasthe“oracle”target:
M 1 K M
LDS=spearman a(D,D˜ ,x) ; m(θ˜⋆(D˜ ),x) , (13)
i i=1 K k=1 k i i=1
(cid:20)(cid:16) (cid:17) (cid:16) (cid:88) (cid:17) (cid:21)
whereθ˜ k⋆(D ′)∈Rdparam aretheparametersresultingfromtrainingonD ′withaparticularseedk.
Retraining without top influences, on the other hand, evaluates the ability of the data attribution
methodtosurfacethemostinfluentialdatapoints–namely,thosethatwouldmostnegativelyaffect
themeasurementm(θ⋆(D ),x)underretrainingfromscratchonadatasetD withthesedatapoints
′ ′
removed. Foreachmethod,weremoveafixedpercentageofthemostinfluentialdatapointsfrom
DtocreatethenewdatasetD ,andreportthechangeinthemeasurementm(θ⋆(D ),x)relativeto
′ ′
m(θ⋆(D),x)(measurementbythemodeltrainedonthefulldatasetD).
Inallexperiments,welookatmeasurementsonsamplesgeneratedbythemodeltrainedonD. We
primarilyfocusonDenoisingDiffusionProbabilisticModels(DDPM)(Hoetal.,2020)throughout.
BaselinesWecompareinfluencefunctionswithK-FACandGGNmodel(MC-Fisher;Equation(10))
astheHessianapproximation(K-FACInfluence)toTRAKasfoDrmulatedfordiffusionmodelsin
Georgievetal.(2023);Zhengetal.(2024). Inourframework,theirmethodcanbeterselydescribed
as using GGNloss (Empirical Fisher) in Equation (11) as a Hessian approximation instead of
GGNmodel(MCD-Fisher)inEquation(10),andcomputingtheHessian-preconditionedinnerproducts
usingDrandomprojections(Dasgupta&Gupta,2003)ratherthanK-FAC.Wealsocomparetothe
ad-hocchangestothemeasurement/traininglossintheinfluencefunctionapproximation(D-TRAK)
thatwereshownbyZhengetal.(2024)togiveimprovedperformanceonLDSbenchmarks. Notethat,
thechangesinD-TRAKweredirectlyoptimisedforimprovementsinLDSscoresinthediffusion
modellingsetting,andlackanytheoreticalmotivation. Hence,adirectcomparisonforthechanges
proposedinthiswork(K-FACInfluence)isTRAK;theinsightsfromD-TRAKareorthogonaltoour
work. Thesearetheonlypriorworksmotivatedbypredictingthechangeinamodel’smeasurements
afterretrainingthathavebeenappliedtothegeneraldiffusionmodellingsettingthatweareaware
of. We also compare to na¨ıvely using cosine similarity between the CLIP (Radford et al., 2021)
embeddings of the training datapoints and the generated sample as a proxy for influence on the
generatedsamples. Lastly,wereportLDSresultsfortheoraclemethodof“ExactRetraining”,where
weactuallyretrainingasinglemodeltopredictthechangesinmeasurements.
LDS.TheLDSresultsattributingthelossandELBOmeasurementsareshowninFigures2aand2b.
K-FACInfluenceoutperformsTRAKinallsettings. K-FACInfluenceusingthelossmeasurement
alsooutperformsthebenchmark-tunedchangesinD-TRAKinallsettingsaswell. InFigures2a
and2b,wereporttheresultsforboththebestdampingvaluesfromasweep(seeAppendixD),as
wellasfor“default”valuesfollowingrecommendationsinpreviouswork(seeAppendixG.4). TRAK
andD-TRAKappeartobesignificantlymoresensitivetotuningthedampingfactorthanK-FAC
Influence. Theyoftendon’tperformatallifthedampingfactoristoosmall,andtakeanoticeable
performancehitifthedampingfactorisnottunedtotheproblemormethod(seeFigures8and10in
AppendixD).However,inmostapplications,tuningthedampingfactorwouldbeinfeasible,asit
requiresretrainingthemodelmanytimesovertoconstructanLDSbenchmark,sothisisasignificant
limitation. Incontrast,forK-FACInfluence,wefindthatgenerallyanysufficientlysmallvalueworks
reasonablywellifenoughsamplesaretakenforestimatingthelossandmeasurementgradients(see
Figures7and9).
8BetterLDSresultscansometimesbeobtainedwhenlookingatvalidationexamples(Zhengetal.,2024),
butdiffusionmodelsareusedprimarilyforsampling,soattributinggeneratedsamplesisofprimarypractical
interest.
8CIFAR-2 CIFAR-10
5.3% 0.7 2.7% 0.8 CLIPCosineSimilarity
± ±
10.3% 0.8 (1.3%) 8.9% 0.8 (5.2%) TRAK
± ±
21.5% 0.9 (21.5%) 18.3% 0.7 (14.4%) K-FAC Influence
± ±
20.9% 0.9 (0.1%) 15.4% 0.8 (0.9%) D-TRAK
± ±
50.3% 0.2 41.2% 0.3 ExactRetraining
± ±
0 20 40 60 80 100 0 20 40 60 80 100
Rank Correlation % (LDS) Rank Correlation % (LDS)
(a)LDSresultsonthelossmeasurement.
CIFAR-2 CIFAR-10
8.0% 0.7 3.6% 0.7 CLIPCosineSimilarity
± ±
5.3% 0.8 (0.9%) 3.6% 0.6 (2.1%) TRAK
± ±
10.9% 0.8 (10.9%) 5.8% 0.8 (5.8%) K-FAC Influence
± ±
18.5% 0.9 (0.9%) 10.5% 0.7 (0.6%) D-TRAK
± ±
23.0% 0.8 (23.0%) 14.7% 0.7 (11.8%) K-FAC Influence(m. loss)
± ±
43.7% 0.7 16.4% 0.3 ExactRetraining
± ±
0 20 40 60 80 100 0 20 40 60 80 100
Rank Correlation % (LDS) Rank Correlation % (LDS)
(b)LDSresultsontheELBOmeasurement.
Figure2: LinearData-modellingScore(LDS)fordifferentdataattributionmethods. Methodsthat
substituteinincorrectmeasurementfunctionsintotheapproximationareseparatedandplottedwith
. WeplotresultsforboththebestHessian-approximationdampingvaluewith anda“default”
dampingvaluewith (whereapplicable). Whereapplicable,thenumericalresultsarereportedin
blackforthebestdampingvalue,andforthe“default”dampingvaluein(gray). “(m.loss)”implies
thattheappropriatemeasurementfunctionwassubstitutedwiththelossℓ(θ,x)measurementfunction
intheapproximation. Resultsfortheexactretrainingmethod(oracle),areshownwith . Standard
errorintheLDSscoreestimateisindicatedwith‘ ’,wherethemeanistakenoverdifferentgenerated
±
samplesxonwhichthechangeinmeasurementisbeingestimated.
Retrainingwithouttopinfluences. ThecounterfactualretrainingresultsareshowninFigure3for
CIFAR-2,CIFAR-10,with2%and10%ofthedataremoved. Inthisevaluation,influencefunctions
withK-FACconsistentlypickmoreinfluentialtrainingexamples(i.e.thosewhichleadtoahigher
lossreduction)thanthebaselines.
HessianApproximationAblation. InFigure4,weexploretheimpactoftheHessianapproximation
designchoicesdiscussedinSection3.1. WeuseK-FACtoapproximatetheGGNinallcases,with
eitherthe“expand”orthe“reduce”variant(Section3.1.2). Wefindthatthebetter-motivated“MC-
Fisher”estimatorGGNmodelinEquation(9)doesindeedperformbetterthanthe“empiricalFisher”
inEquation(11)usedinTRAKandD-TRAK.Secondly,wefindthatK-FACexpandsignificantly
outperformsK-FACreduce,whichstandsincontrasttotheresultsinthesecond-orderoptimisation
settingwherethetwoareonparwithoneanother(Eschenhagenetal.,2023). Therearemultiple
differencesfromoursettingtotheonefromthepreviousoptimisationresults: weuseasquareloss
instead of a cross entropy loss, a full dataset estimate, a different architecture, and evaluate the
approximationinadifferentapplication. Notably,theexpandvariantisthebetterjustifiedonesince
the diffusion modelling problem corresponds to the expand setting in Eschenhagen et al. (2023).
Hence,ourresultsallseemtoimplythatabetterHessianapproximationdirectlyresultsinbetter
downstreamdataattributionperformance. However,wedonotdirectlyevaluatetheapproximation
qualityoftheestimatesandalsodonotsweepoverthedampingvalueforallvariants.
4.1 POTENTIALCHALLENGESTOUSEOFINFLUENCEFUNCTIONSFORDIFFUSIONMODELS
OnepeculiarityintheLDSresults,similartothefindingsinZhengetal.(2024),isthatsubstituting
thelossmeasurementfortheELBOmeasurementwhenpredictingchangesinELBOactuallyworks
9CIFAR-2 — 2% removed CIFAR-2 — 10% removed
0.0024 0.0009 0.0027 0.0009 Random
± ±
0.0029 0.0009 0.0037 0.0009 TRAK
± ±
0.0032 0.001 0.0042 0.0008 D-TRAK
± ±
0.0034 0.0009 0.0043 0.0008 K-FACInfluence
± ±
0.0000 0.0025 0.0050 0.0075 0.0100 0.00000.00250.00500.00750.0100
CIFAR-10 — 2% removed CIFAR-10 — 10% removed
2.3e 05 0.0003 0.00015 0.0003 Random
− ± ±
0.00041 0.0003 0.00069 0.0002 TRAK
± ±
0.00067 0.0002 0.001 0.0002 D-TRAK
± ±
0.0008 0.0004 0.0013 0.0005 K-FACInfluence
± ±
0.000 0.001 0.002 0.003 0.004 0.005 0.000 0.002 0.004 0.006
Measurement Change Measurement Change
Figure3: Changesinmeasurementsundercounterfactualretrainingwithouttopinfluencesforthe
lossmeasurement. Thestandarderrorintheestimateofthemeanisindicatedwitherrorbarsand
reportedafter‘±’,wheretheaverageisoverdifferentgeneratedsamplesforwhichtopinfluencesare
beingidentified.
6.05% 0.7 GGNloss (Empirical)
6.47%± 0.7 GGNmodel (MC-Fisher) reduce
±
17.85% 0.9 GGNloss (Empirical) o
± expand
21.46% 0.9 GGNmodel (MC-Fisher)
±
o
50.32% 0.2 ExactRetraining
±
0 10 20 30 40 50 60 70
Rank Correlation % (LDS)
Figure 4: Ablation over the different Hessian approximation variants introduced in Section 3.1.
WeablatetwoversionsoftheGGN:the“MC”FisherinEquation(9)andthe“Empirical”Fisher
in Equation (11), as well as two settings for the K-FAC approximation: “expand” and “reduce”
(Eschenhagenetal.,2023).
betterthanusingthecorrectmeasurement(seeFigure2b“K-FACInfluence(measurementloss)”).9 To
tryandbetterunderstandthepropertiesofinfluencefunctionsforpredictingthenumericalchangein
behaviourafterretraining,inthissectionweperformmultipleablationsandreportdifferentinteresting
phenomenaresultingfromthesethatgivesomeinsightintothechallengesofusinginfluencefunctions
inthissetting.
AsillustratedinFigure18,gradientsoftheELBOandtraininglossmeasurements,uptoaconstant
scaling,consistofthesameper-diffusion-timesteplosstermgradients∇ ℓ (θ,x),butwithadifferent
θ t
weighting. Totryandbreak-downwhyapproximatingthechangeinELBOwiththetrainingloss
measurementgiveshigherLDSscores,wefirstlookatpredictingthechangeintheper-diffusion-
timesteplossesℓ whilesubstitutingdifferentper-diffusion-timesteplossesintotheK-FACinfluence
t
approximation. TheresultsareshowninFigure11,leadingtothefollowingobservation:
Observation1 Higher-timesteplossesℓ (θ,x)actasbetterproxiesforlower-timesteplosses.
t
Morespecifically,changesinlossesℓ caningeneralbewellapproximatedbysubstitutingmeasure-
t
mentsℓ intotheinfluenceapproximationwitht >t. Insomecases,usingtheincorrecttimestep
t′ ′
t >tevenresultsinsignificantlybetterLDSscoresthanthecorrecttimestept =t.
′ ′
9Notethat,unlikeZhengetal.(2024),weonlychangethemeasurementfunctionforaproxyintheinfluence
functionapproximation,keepingtheHessianapproximationandtraininglossgradientinEquation(6)thesame.
10BasedonObservation1,itisclearthatinfluencefunction-basedapproximationshavelimitations
when being applied to predict the numerical change in loss measurements. We observe another
patterninhowtheycanfail:
Observation2 Influencefunctionspredictbothpositiveandnegativeinfluenceonloss,but,in
practice,removingdatapointspredominantlyincreasesloss.
WeshowinFigures15and16thatinfluencefunctionstendtooverestimatehowoftenremovalof
a group data points will lead to improvements in loss on a generated sample (both for aggregate
diffusiontraininglossinSection2.1,andtheper-diffusion-timesteplossinEquation(2)).
Lastly, although ELBO is perhaps the measurement with the most direct link to the marginal
probabilityofsamplingaparticularexample,wefindithassomepeculiarproperties. Thebelow
observationparticularlyputsitsusefulnessasameasurementfunctionintoquestion:
Observation3 ELBOisclosetoconstantongeneratedsamples,irrespectiveofwhichexamples
wereremovedfromthetrainingdata.
AsillustratedinFigure17,ELBOmeasurementisclosetoconstantforanygivensamplegenerated
from the model, no matter which 50% subset of the training data is removed. In particular, it is
extremelyrarethatifonesampleismorelikelytobegeneratedthananothersamplebyonemodel
(asmeasuredbyELBO),itwillbelesslikelytobegeneratedthananotherbyamodeltrainedona
differentrandomsubsetofthedata. ThisimpliesthatthestandardDDPMELBOmightbeapoor
proxyfortherealquestionwe intendtoask: “howlikely isagivensampletobegeneratedbya
model?”. Itwouldappearimprobablethatthetrueprobabilitiesofgeneratinganygivensampleare
close-tounaffectedbyresamplingwhatdatasubsetthemodelwastrainedon.
5 DISCUSSION
Inthiswork,weextendedtheinfluencefunctionsapproachtothediffusionmodellingsetting,and
showeddifferentwaysinwhichtheGGNHessianapproximationcanbeformulatedinthissetting.
Ourproposedmethodwithrecommendeddesignchoicesimprovesperformancecopmaredtoexisting
techniquesacrossvariousdataattributionevaluationmetrics. Nonetheless,experimentally,weare
metwithtwocontrastingfindings: ontheonehand,influencefunctionsinthediffusionmodelling
setting appear to be able to identify important influences. The surfaced influential examples do
significantlyimpactthetraininglosswhenretrainingthemodelwithoutthem(Figure3),andthey
appearperceptuallyveryrelevanttothegeneratedsamples. Ontheotherhand, theyfallshortof
accurately predicting the numerical changes in measurements after retraining. Thes appears to
beespeciallythecaseformeasurementfunctionswewouldarguearemostrelevantintheimage
generativemodellingsetting–proxiesformarginalprobabilityofsamplingaparticularexample.
Thisappearstobebothduetothelimitationsoftheinfluencefunctionsapproximation,butalsodue
totheshortcomingsoftheconsideredproxymeasurements(Section4.1).
Despitetheseshortcomings,influencefunctionscanstilloffervaluableinsights: theycanserveasa
usefulexploratorytoolforunderstandingmodelbehaviourinadiffusionmodellingcontext,andcan
helpguidedatacuration,identifyingexamplesmostresponsibleforcertainbehaviours. Tomakethem
usefulinsettingswherenumericalaccuracyinthepredictedbehaviourafterretrainingisrequired,
such as copyright infringement, we believe more work is required into 1) finding better proxies
for marginal probability than ELBO and probability of sampling trajectory , and 2) even further
improvingtheinfluencefunctionapproximation.
ACKNOWLEDGMENTS
WethankJihaoAndreasLinforusefuldiscussionsoncompression,andhelpwithimplementationof
quantisationandSVDcompression. WewouldalsoliketothankKristianGeorgievforsharingwith
usthediffusionmodelweightsusedforanalysisinGeorgievetal.(2023),andFelixDangelforhelp
andfeedbackonimplementingextensionstothecurvlinopspackageusedfortheexperimentsin
thispaper. RichardE.TurnerissupportedbyGoogle,Amazon,ARM,Improbable,EPSRCgrant
EP/T005386/1,andtheEPSRCProbabilisticAIHub(ProbAI,EP/Y028783/1).
11REFERENCES
Shun-IchiAmari. Naturalgradientworksefficientlyinlearning. Neuralcomputation,10(2),1998.
JuhanBae,NathanNg,AlstonLo,MarzyehGhassemi,andRogerGrosse. IfInfluenceFunctionsare
theAnswer,ThenWhatistheQuestion?,September2022.
JuhanBae,WuLin,JonathanLorraine,andRogerGrosse. Trainingdataattributionviaapproximate
unrolleddifferentiation,2024. URLhttps://arxiv.org/abs/2405.12186.
AlbertoBernacchia,MateLengyel,andGuillaumeHennequin. Exactnaturalgradientindeeplinear
networksanditsapplicationtothenonlinearcase. InNeurIPS,2018.
SanjoyDasguptaandAnupamGupta.AnelementaryproofofatheoremofJohnsonandLindenstrauss.
RandomStructures&Algorithms,22(1):60–65,January2003. ISSN1042-9832,1098-2418. doi:
10.1002/rsa.10073.
Runa Eschenhagen, Alexander Immer, Richard E. Turner, Frank Schneider, and Philipp Hennig.
Kronecker-FactoredApproximateCurvatureformodernneuralnetworkarchitectures. InNeurIPS,
2023.
Kristian Georgiev, Joshua Vendrow, Hadi Salman, Sung Min Park, and Aleksander Madry. The
Journey,NottheDestination: HowDataGuidesDiffusionModels,December2023.
RogerGrosseandJamesMartens. AKronecker-factoredapproximateFishermatrixforconvolution
layers. InICML,2016.
RogerGrosse, JuhanBae, CemAnil, NelsonElhage, AlexTamkin, AmirhosseinTajdini, Benoit
Steiner,DustinLi,EsinDurmus,EthanPerez,EvanHubinger,Kamile˙Lukosˇiu¯te˙,KarinaNguyen,
Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Samuel R. Bowman. Studying Large
LanguageModelGeneralizationwithInfluenceFunctions,August2023.
TomHeskes. On“natural”learningandpruninginmultilayeredperceptrons. NeuralComputation,
12(4),2000.
JonathanHo,AjayJain,andPieterAbbeel. DenoisingDiffusionProbabilisticModels. InAdvances
inNeuralInformationProcessingSystems,volume33,pp.6840–6851.CurranAssociates,Inc.,
2020.
DiederikP.Kingma,TimSalimans,BenPoole,andJonathanHo. VariationalDiffusionModels,April
2023.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference
on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1885–
1894.PMLR,06–11Aug2017.URLhttps://proceedings.mlr.press/v70/koh17a.
html.
StevenG.KrantzandHaroldR.Parks. TheImplicitFunctionTheorem. Birkha¨user,Boston,MA,
2003. ISBN978-1-4612-6593-1978-1-4612-0059-8. doi: 10.1007/978-1-4612-0059-8.
Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Technical
report, University of Toronto, 2009. URL http://www.cs.utoronto.ca/˜kriz/
learning-features-2009-TR.pdf.
FrederikKunstner,LukasBalles,andPhilippHennig. LimitationsoftheempiricalFisherapproxima-
tionfornaturalgradientdescent. InNeurIPS,2019.
YongchanKwon,EricWu,KevinWu,andJamesZou. DataInf: EfficientlyEstimatingDataInfluence
inLoRA-tunedLLMsandDiffusionModels. InTheTwelfthInternationalConferenceonLearning
Representations,October2023.
JamesMartens. Newinsightsandperspectivesonthenaturalgradientmethod. JMLR,21(146),2020.
12JamesMartensandRogerGrosse. OptimizingneuralnetworkswithKronecker-factoredapproximate
curvature. InICML,2015.
SungMinPark,KristianGeorgiev,AndrewIlyas,GuillaumeLeclerc,andAleksanderMadry. TRAK:
AttributingModelBehavioratScale,April2023.
AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh, GabrielGoh, SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever.
Learning transferable visual models from natural language supervision, 2021. URL https:
//arxiv.org/abs/2103.00020.
Joseph Saveri and Matthew Butterick. Image generator litigation. https://
imagegeneratorlitigation.com/,2023a. Accessed: 2024-07-06.
JosephSaveriandMatthewButterick. Languagemodellitigation. https://llmlitigation.
com/,2023b. Accessed: 2024-07-06.
Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.
Neuralcomputation,14(7),2002.
ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,Mehdi
Cherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,PatrickSchramowski,
SrivatsaKundurthy,KatherineCrowson,LudwigSchmidt,RobertKaczmarczyk,andJeniaJitsev.
Laion-5b: Anopenlarge-scaledatasetfortrainingnextgenerationimage-textmodels,2022. URL
https://arxiv.org/abs/2210.08402.
JaschaSohl-Dickstein,EricA.Weiss,NiruMaheswaranathan,andSuryaGanguli.DeepUnsupervised
LearningusingNonequilibriumThermodynamics,November2015.
JiamingSong,ChenlinMeng,andStefanoErmon. DenoisingDiffusionImplicitModels,October
2022.
YangSongandStefanoErmon. Generativemodelingbyestimatinggradientsofthedatadistribution,
2020. URLhttps://arxiv.org/abs/1907.05600.
Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum Likelihood Training of
Score-BasedDiffusionModels,October2021a.
YangSong,JaschaSohl-Dickstein,DiederikP.Kingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-BasedGenerativeModelingthroughStochasticDifferentialEquations,February
2021b.
RichardE.Turner,Cristiana-DianaDiaconu,StratisMarkou,AliaksandraShysheya,AndrewY.K.
Foong, andBrunoMlodozeniec. Denoisingdiffusionprobabilisticmodelsinsixsimplesteps,
2024.
Xiaosen Zheng, Tianyu Pang, Chao Du, Jing Jiang, and Min Lin. Intriguing Properties of Data
AttributiononDiffusionModels,March2024.
13A DERIVATION OF INFLUENCE FUNCTIONS
Inthissection,westatetheimplicitfunctiontheorem(AppendixA.1). Then,inAppendixA.2,we
introducethedetailsofhowitcanbeappliedinthecontextofalossfunctionL(ε,θ)parameterised
byacontinuoushyperparameterε(whichis,e.g.,controllinghowdown-weightedthelosstermson
someexamplesare,asinSection2.2).
LinearExtrapolation
Minimum
N1 N n=1‘(xn,θ)
−
N1‘(xj,θ)
P
(ε,θ)
L
N1 N n=1‘(xn,θ)
−
21 N‘(xj,θ)
N1 P
N1 N n=1‘(xn,θ)
21 N P
ε
−4 −3 −2 −1 0 1 2 3 4 0
θ
Figure5:Illustrationoftheinfluencefunctionapproximationfora1-dimensionalparameterspaceθ ∈
R. InfluencefuncitonsconsidertheextendedlosslandscapeL(ε,θ)=def 1 N ℓ(x ,θ)−εℓ(x ,θ),
N n=1 n j
wherethelossℓ(x ,θ)forsomedatapointx (alternatively,groupofdatapoints)isdown-weighted
j j
byε. Bylinearlyextrapolatinghowtheoptimalsetofparametersθwoul(cid:80)dchangearoundε=0( ),
wecanpredictedhowtheoptimalparameterswouldchangewhenthetermℓ(x ,θ)isfullyremoved
j
fromtheloss( ).
A.1 IMPLICITFUNCTIONTHEOREM
Theorem1(ImplicitFunctionTheorem(Krantz&Parks,2003)) LetF : Rn ×Rm → Rm be
a continuously differentiable function, and let Rn × Rm have coordinates (x,y). Fix a point
(a,b) = (a ,...,a ,b ,...,b ) with F(a,b) = 0, where 0 ∈ Rm is the zero vector. If the
1 n 1 m
Jacobianmatrix∇ F(a,b)∈Rm mofy(cid:55)→F(a,y)
y ×
∂F
[∇ F(a,b)] = i (a,b),
y ij ∂y
j
is invertible, then there exists an open set U ⊂ Rn containing a such that there exists a unique
function g : U → Rm such that g(a) = b, and F(x,g(x)) = 0 for all x ∈ U. Moreover, g is
continuouslydifferentiable.
Remark1(Derivativeoftheimplicitfunction) DenotingtheJacobianmatrixofx(cid:55)→F(x,y)as:
∂F
[∇ F(x,y)] = i (x,y),
x ij ∂x
j
thederivative ∂g :U →Rm nofg :U →RminTheorem1canbewrittenas:
∂x ×
∂g(x)
∂x
=−[∇ yF(x,g(x))]−1∇ xF(x,g(x)). (14)
Thiscanreadilybeseenbynotingthat,forx∈U:
dF(x,g(x))
F(x,g(x))=0 ∀x ∈U ⇒ =0.
′ ′ ′
dx
Hence,sincegisdifferentiable,wecanapplythechainruleofdifferentiationtoget:
dF(x,g(x)) ∂g(x)
0= =∇ F(x,g(x))+∇ F(x,g(x)) .
dx x y ∂x
RearranginggivesequationEquation(14).
14A.2 APPLYINGTHEIMPLICITFUNCTIONTHEOREMTOQUANTIFYTHECHANGEINTHE
OPTIMUMOFALOSS
Consider a loss function L : Rn ×Rm → R that depends on some hyperparameter ε ∈ Rn (in
Section2.2,thiswasthescalarbywhichcertainlosstermsweredown-weighted)andsomeparameters
θ ∈Rm. AttheminimumofthelossfunctionL(ε,θ),thederivativewithrespecttotheparameters
θwillbezero. Hence,assumingthatthelossfunctionistwicecontinuouslydifferentiable(hence
∂L iscontinuouslydifferentiable),andassumingthatforsomeε ∈Rnwehaveasetofparameters
∂ε ′
θ⋆ suchthat ∂ (ε,θ⋆) = 0andtheHessian ∂2 (ε,θ⋆)isinvertible, wecanapplytheimplicit
∂Lε ′ ∂θL2 ′
functiontheoremtothederivativeofthelossfunction ∂ :Rn×Rm →Rm,togettheexistenceofa
∂Lε
continuouslydifferentiablefunctiongsuchthat ∂ (ε,g(ε))=0forεinsomeneighbourhoodofε.
∂Lε ′
Nowg(ε)mightnotnecessarilybeaminimumofθ (cid:55)→ L(ε,θ). However,bymakingthefurther
assumption that L is strictly convex we can ensure that whenever ∂ (ε,θ) = 0, θ is a unique
∂Lθ
minimum,andsog(ε)representsthechangeintheminimumaswevaryε. Thisissummarisedinthe
lemmabelow:
Lemma1 LetL:Rn×Rm →Rbeatwicecontinuouslydifferentiablefunction,withcoordinates
denoted by (ε,θ) ∈ Rn ×Rm, such that θ (cid:55)→ L(ε,θ) is strictly convex ∀ε ∈ Rn. Fix a point
(ε,θ⋆) such that ∂ (ε,θ⋆) = 0. Then, by the Implicit Function Theorem applied to ∂ , there
ex′ istsanopensetU∂Lθ ⊂R′
ncontainingθ⋆suchthatthereexistsauniquefunctiong :U
→∂RLθ
msuch
that g(ε) = θ⋆, and g(ε) is the unique minimum of θ (cid:55)→ L(ε,θ) for all ε ∈ U. Moreover, g is
′
continuouslydifferentiablewithderivative:
∂g(ε) ∂2L −1 ∂2L
=− (ε,g(ε)) (ε,g(ε)) (15)
∂ε ∂θ2 ∂ε∂θ
(cid:20) (cid:21)
Remark2 ForalossfunctionL:R×RmoftheformL(ε,θ)=L (θ)+εL (θ)(suchasthatin
1 2
Equation(4)), ∂2 (ε,g(ε))intheequationabovesimplifiesto:
∂ε∂Lθ
∂2L ∂L
(ε,g(ε))= 2 (g(ε)) (16)
∂ε∂θ ∂θ
TheabovelemmaandremarkgivetheresultinEquation(5). Namely,insection2.2:
L2
1 N 1 M eq.(16) ∂2L 1 M ∂
L(ε,θ)= ℓ(θ,x )− ℓ(θ,x )ε =⇒ =− ℓ(θ,x )
N i (cid:122) M (cid:125)(cid:124) ij(cid:123) ∂ε∂θ M ∂θ ij
i=1 j=1 j=1
(cid:88) (cid:88) (cid:88)
L1
(cid:124) (cid:123)(cid:122) (cid:125) eq.(15) ∂g(ε) ∂2L −1 1 M ∂
=⇒ = (ε,g(ε)) ℓ(θ,x )
∂ε ∂θ2 M ∂θ ij
(cid:20) (cid:21) j=1
(cid:88)
B DERIVATION OF THE FISHER “GGN” FORMULATION FOR DIFFUSION
MODELS
AsdiscussedinSection2.2.1partitioningthefunctionθ (cid:55)→∥ϵ(t)−ϵt(x(t))∥2intothemodeloutput
θ
θ (cid:55)→ϵt(x(t))andtheℓ lossfunctionisanaturalchoiceandresultsin
θ 2
GGNmodel(θ)
D
N
= 1 E E ∇⊺ ϵt˜ x(t˜) ∇2 ϵ(t˜)−ϵt˜ x(t˜) 2 ∇ ϵt˜ x(t˜)
N t˜ x(t˜),ϵ(t˜) θ θ ϵt˜(x(t˜)) θ θ θ
n (cid:88)=1 (cid:20) (cid:20) (cid:16) (cid:17) θ (cid:13) (cid:16) (cid:17)(cid:13) (cid:16) (cid:17)(cid:21)(cid:21)
(cid:13) (cid:13)
= 2 N E E ∇⊺ ϵt˜ x(t˜) I∇ ϵt˜ x(cid:13) (t˜) . (cid:13) (17)
N t˜ x(t˜),ϵ(t˜) θ θ θ θ
n (cid:88)=1 (cid:104) (cid:104) (cid:16) (cid:17) (cid:16) (cid:17)(cid:105)(cid:105)
15Notethatweused
1 2
∇2 ϵ(t˜)−ϵt˜ x(t˜) =I.
2 ϵt˜(x(t˜)) θ
θ
(cid:13) (cid:16) (cid:17)(cid:13)
WecansubstituteI with (cid:13) (cid:13)
(cid:13) (cid:13)
1
I =E − ∇2 logp ϵ |ϵt˜ x(t˜) , p ϵ |ϵt˜ x(t˜) =N ϵ |ϵt˜ x(t˜) ,I ,
ϵmod 2 ϵt˜(x(t˜)) mod θ mod θ mod θ
(cid:20) θ (cid:16) (cid:16) (cid:17)(cid:17)(cid:21) (cid:16) (cid:16) (cid:17)(cid:17) (cid:16) (cid:16) (cid:17) (cid:17)
wherethemeanoftheGaussianischosentobethemodeloutputϵt˜ x(t˜) . Furthermore,byusing
θ
the“score”trick: (cid:16) (cid:17)
E ∇2 logp ϵ |ϵt˜ x(t˜)
ϵmod ϵt˜(x(t˜)) mod θ
θ
=−E(cid:104) ∇ l(cid:16) ogp ϵ (cid:16) |ϵt˜(cid:17) x(cid:17) ((cid:105) t˜) ∇⊺ logp ϵ |ϵt˜ x(t˜)
ϵmod ϵt θ˜(x(t˜)) mod θ ϵt θ˜(x(t˜)) mod θ
(cid:104) (cid:16) (cid:16) (cid:17)(cid:17) (cid:16) (cid:16) (cid:17)(cid:17)(cid:105)
=−E 1 ∇ ϵ −ϵt˜ x(t˜) 2 1 ∇⊺ ϵ −ϵt˜ x(t˜) 2 ,
ϵmod (cid:20)2 ϵt θ˜(x(t˜))
(cid:13)
mod θ
(cid:16) (cid:17)(cid:13)
2 ϵt θ˜(x(t˜))
(cid:13)
mod θ
(cid:16) (cid:17)(cid:13) (cid:21)
wecanrewrite: (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
∇⊺ ϵt˜ x(t˜) ∇ ϵt˜ x(t˜)
θ θ θ θ
=−2(cid:16) ∇⊺ ϵt˜(cid:17) x(t˜) (cid:16) E (cid:17) ∇2 logp ϵ |ϵt˜ x(t˜) ∇ ϵt˜ x(t˜)
θ θ ϵmod ϵt˜(x(t˜)) mod θ θ θ
θ
(cid:16) (cid:17) (cid:104)(cid:16) (cid:16) (cid:16) (cid:17)(cid:17)(cid:17)(cid:105) (cid:16) (cid:17)
= 1 E ∇⊺ ϵt˜ x(t˜) ∇ ϵ −ϵt˜ x(t˜) 2 ∇⊺ ϵ −ϵt˜ x(t˜) 2 ∇ ϵt˜ x(t˜)
2 ϵmod
(cid:20)
θ θ
(cid:16) (cid:17)
ϵt θ˜(x(t˜))
(cid:13)
mod θ
(cid:16) (cid:17)(cid:13)
ϵt θ˜(x(t˜))
(cid:13)
mod θ
(cid:16) (cid:17)(cid:13)
θ θ
(cid:16) (cid:17)(cid:21)
= 1 E ∇ ϵ −ϵt˜ x(t˜) (cid:13) (cid:13)2 ∇⊺ ϵ −ϵt˜ (cid:13) (cid:13)x(t˜) 2 , (cid:13) (cid:13) (cid:13) (cid:13)
2 ϵmod θ mod θ θ mod θ
(cid:20) (cid:13) (cid:16) (cid:17)(cid:13) (cid:13) (cid:16) (cid:17)(cid:13) (cid:21)
wherethelasteq(cid:13)ualityfollowsbyt(cid:13)hecha(cid:13)inruleofdifferenti(cid:13)ation. Wecanthusrewritetheexpression
(cid:13) (cid:13) (cid:13) (cid:13)
fortheGGNinEquation(17)as
GGNmodel(θ)
D
N
= 1 E E ∇ g (θ)∇ g (θ)⊺ g(θ)=def ϵ −ϵt˜ x(t˜) 2 .
N t˜ x(t˜),ϵ(t˜),ϵmod θ n θ n mod θ
n (cid:88)=1 (cid:104) (cid:2) (cid:3)(cid:105) (cid:13) (cid:13) (cid:16) (cid:17)(cid:13) (cid:13)
(cid:13) (cid:13)
C GRADIENT COMPRESSION ABLATION
InFigure6,weablatedifferentcompressionmethodsbycomputingthepertrainingdatapointinflu-
encescoreswithcompressedquery(measurement)gradients,andlookingatthePearsoncorrelation
andtherankcorrelationtothescorescomputewiththeuncompressedgradients. Wehopetosee
a correlation of close to 100%, in which case the results for our method would be unaffected by
compression. Wefindthatusingquantisationforcompressionresultsinalmostnochangetothe
orderingovertrainingdatapoints,evenwhenquantisingdownto8bits. ThisisincontrasttotheSVD
compressionschemeusedinGrosseetal.(2023). Thisislikelybecausetheper-examplegradients
naturallyhavealow-rank(Kronecker)structureintheclassification,regression,orautoregressive
languagemodellingsettings,suchasthatinGrosseetal.(2023). Ontheotherhand,thediffusion
traininglossandothermeasurementfunctionsconsideredinthisworkdonothavethislow-rank
structure. Thisisbecausecomputingthemrequiresmultipleforwardpasses;forexample,forthe
diffusiontraininglossweneedtoaveragethemean-squarederrorlossinEquation(2)overmultiple
noisesamplesϵ(t)andmultiplediffusiontimesteps. Weuse8bitquantisationwithquerygradient
batching(Grosseetal.,2023)forallKFACexperimentsthroughoutthiswork.
D DAMPING LDS ABLATIONS
WereportanablationovertheLDSscoreswithGGNapproximatedwithdifferentdampingfactors
for TRAK/D-TRAK and K-FAC influence in Figures 7 to 10. The reported damping factors for
TRAKarenormalisedbythedatasetsizesothattheycorrespondtotheequivalentdampingfactors
forourmethodwhenviewingTRAKasanaltenrativeapproximationtotheGGN(seeSection3.1).
16int8 int16
1.00
Rank100 Rank150
Rank50
0.95 Rank20
Rank10
0.90
Rank5
0.85
Rank2
0.80
Rank1
0.75
0.70
0% 10% 20% 30% 40% 50% 60% 70%
Compression ratio
Figure6: Comparisonofgradientcompressionmethodsfortheinfluencefunctionapproximation.
CIFAR-2 - K-FAC Influence
Loss 100 samples
0.20 Loss 250 samples
Loss 1000 samples
0.15 Loss 2500 samples
ELBO 250 samples
ELBO 1000 samples
0.10
ELBO 2500 samples
0.05
0.00
10−9 10−7 10−5 10−3 10−1 101
Damping factor
Figure7: EffectofdampingontheLDSscoresforK-FACinfluenceonCIFAR-2. Inthisplot,
K-FACGGNapproximationwasalwayscomputedwith1000samples,andthenumberofsamples
usedforcomputingaMonteCarloestimateofthetrainingloss/measurementgradientisindicatedon
thelegend.
E EMPIRICAL ABLATIONS FOR CHALLENGES TO USE OF INFLUENCE
FUNCTIONS FOR DIFFUSION MODELS
Inthissection,wedescribetheresultsfortheobservationsdiscussedinSection4.1.
Observation 1 is based on Figures 11 and 12. Figure 11 shows the LDS scores on CIFAR-2
whenattributingper-timestepdiffusionlossesℓ (seeEquation(2))usinginfluencefunctions,whilst
t
varyingwhat(possiblywrong)per-timestepdiffusionlossℓ isusedasameasurementfunctionin
t′
theinfluencefunctionapproximation(Equation(6)). Figure12isacounter-equivalenttoFigure16
whereinsteadofusinginfluencefunctionstoapproximatethechangeinmeasurement,weactually
retrainamodelontherandomlysubsampledsubsetofdataandcomputethemeasurement.
AnaturalquestiontoaskwithregardstoObservation1is: doesthiseffectgoawayinsettingswhere
theinfluencefunctionapproximationshouldmoreexact? Notethat,barthenon-convexityofthe
17
serocs
desserpmocnu
ot
noitalerroc
knaR
)SDL(
noitalerroc
knaRCIFAR-2 - TRAK Influence
Target Measure Train.Loss
0.20
Loss Loss Loss
Loss Sq.Norm Sq.Norm (D-TRAK)
0.15 ELBO ELBO* Loss
ELBO Sq.Norm Sq.Norm (D-TRAK)
0.10
0.05
0.00
10−9 10−7 10−5 10−3 10−1 101 103 105
Damping factor
Figure 8: Effect of damping on the LDS scores for TRAK (random projection) based influence
onCIFAR-2. 250sampleswereusedforMonteCarloestiamtionofallquantities(GGNandthe
trainingloss/measurementgradients). Inthelegend: Targetindicateswhatmeasurementwe’re
tryingtopredictthechangeinafterretraining,Measureindicateswhatmeasurementfunctionwas
substitutedintotheinfluencefunctionapproximation,andTrain.Lossindicateswhatfunction
wassubstitutedforthetraininglossinthecomputationoftheGGNandgradientofthetrainingloss
intheinfluencefunctionapproximation.
CIFAR-10 - K-FAC Influence
Loss 250samples
0.15 ELBO*250samples
0.10
0.05
0.00
10−9 10−7 10−5 10−3 10−1 101
Damping factor
Figure 9: Effect of damping on the LDS scores for K-FAC based influence on CIFAR-10. 100
sampleswereusedforcomputingtheK-FACGGNapproximation,and250forcomputingaMonte
Carloestimateofthetrainingloss/measurementgradients. ×indicatesaNaNresult(thecomputation
wasnotsufficientlynumericallystablewiththatdampingfactor).
traininglossfunctionL ,theinfluencefunctionapproximationinEquation(6)isalinearisation
oftheactualchangeintDhemeasurementfortheoptimumofthetraininglossfunctionswithsome
examples down-weighted by ε around ε = 0. Hence, we might expect the approximation to be
moreexactwheninsteadoffullyremovingsomedatapointsfromthedataset(settingε=1/N),we
insteaddown-weighttheircontributiontothetraininglossbyasmallernon-zerofactor. Toinvestigate
whetherthisisthecase,werepeattheLDSanalysisinFigures11and12,butwithε=1/2N;inother
words,thetraininglosstermscorrespondingtothe“removed”examplesaresimplydown-weighted
by a factor of 1/2 in the retrained models. The results are shown in Figures 13 and 14. Perhaps
somewhatsurprisingly,acontrastingeffectcanbeobserved,whereusingper-timestepdiffusionlosses
forlargertimesyieldsahigherabsoluterankcorrelation,butwiththeopposingsign. Thenegative
correlationbetweenmeasurementℓ ,ℓ fort̸=t canalsobeobservedforthetruemeasurements
t t′ ′
intheretrainedmodelsinFigure14. Wealsoobservethatinthissetting,influencefunctionsfail
completelytopredictchangesinℓ withthecorrectmeasurementfunctionfort≤200.
t
Observation1Figure15showsthechangesinlossesafterretrainingthemodelonhalfthedata
removedagainstthepredictedchangesinlossesusingK-FACInfluencefortwodatasets: CIFAR-2
18
)SDL(
noitalerroc
knaR
)SDL(
noitalerroc
knaRCIFAR-10 - TRAK Influence
Target Measure Train.Loss
0.15
Loss Loss Loss
Loss Sq.Norm Sq.Norm (D-TRAK)
Loss Sq.Norm Loss
0.10 ELBO* Loss Loss
ELBO* ELBO* Loss
ELBO* Sq.Norm Sq.Norm (D-TRAK)
0.05
0.00
10−10 10−8 10−6 10−4 10−2 100
Damping factor
Figure10: EffectofdampingontheLDSscoresforTRAK(randomprojection)basedinfluence
onCIFAR-10. 250sampleswereusedforMonteCarloestiamtionofallquantities(GGNandthe
trainingloss/measurementgradients). Inthelegend: Targetindicateswhatmeasurementwe’re
tryingtopredictthechangeinafterretraining,Measureindicateswhatmeasurementfunctionwas
substitutedintotheinfluencefunctionapproximation,andTrain.Lossindicateswhatfunction
wassubstitutedforthetraininglossinthecomputationoftheGGNandgradientofthetrainingloss
intheinfluencefunctionapproximation.
Influence measurement
Square
‘1 ‘10 ‘20 ‘50 ‘80 ‘100 ‘200 ‘500 ‘800 ‘999 norm
‘1 14.4 12.6 10.4 9.6 9.1 8.5 7.8 13.1 5.0 4.7 -12.8
‘10 4.8 10.5 12.0 11.5 10.5 9.8 10.0 14.7 3.0 2.7 -4.7 40
‘20 2.1 6.8 9.6 12.0 12.0 11.8 12.3 15.6 2.7 2.6 -1.7
‘50 0.4 2.0 3.7 9.4 13.9 15.4 16.8 15.2 2.4 2.3 2.0 20
‘80 0.2 1.0 1.5 6.7 12.7 15.5 19.7 15.1 2.7 2.4 2.5
0
‘100 0.3 0.7 0.9 5.5 11.5 14.8 21.1 15.2 2.9 2.6 2.6
‘200 0.9 0.6 0.4 2.7 7.1 10.4 22.7 17.7 3.5 3.0 1.0 20
−
‘500 2.1 1.5 1.7 2.1 3.1 4.1 12.1 43.8 5.1 4.2 -1.6
‘800 5.0 2.4 1.4 0.7 0.4 0.5 2.8 22.8 56.4 50.2 -3.0 40
−
‘999 4.2 0.7 0.3 0.0 0.5 0.5 1.6 10.3 17.5 17.2 -0.0
Figure 11: Rank correlation (LDS scores) between influence function estimates with different
measurement functions and different true measurements CIFAR-2. The plot shows how well
differentper-timestepdiffusionlossesℓ workasmeasurementfunctionsintheinfluencefunction
t
approximation,whentryingtoapproximatechangesintheactualmeasurementswhenretraininga
model.
andCIFAR-10. Inbothcases,foravastmajorityofretrainedmodels,thelossmeasurementona
sampleincreasesafterretraining. Ontheotherhand,theinfluencefunctionspredictroughlyevenly
that the loss will increase and decrease. This trend is amplified if we instead look at influence
predictedforper-timestepdiffusionlossesℓ (Equation(2))forearliertimestepst, whichcanbe
t
seeninFigure16. OnCIFAR-2,actualchangesinℓ ,ℓ 0,ℓ 00measurementsareactuallyalways
1 5 1
positive, which the influence functions approximation completely misses. For all plots, K-FAC
Influencewasranwithadampingfactorof10 8and250samplesforallgradientcomputations.
−
Observation3Lastly,theobservationsthattheELBOmeasurementsremainessentiallyconstantfor
modelstrainedondifferentsubsetsofdataisbasedonFigure17. There,weplotthevaluesofthe
ELBOmeasurementfordifferentpairsofmodelstrainedondifferentsubsetsofdata,wherewefind
nearperfectcorrelation. TheonlypairsofmodelsthatexhibitanELBOmeasurementcorrelation
19
)SDL(
noitalerroc
knaR
tnemerusaem
eurT
)%(
noitalerroc
knaRInfluence measurement
‘1 ‘10 ‘20 ‘50 ‘80 ‘100 ‘200 ‘500 ‘800 ‘999
‘1 32.3 20.3 13.9 7.9 6.4 6.1 6.1 7.1 4.0 0.6
60
‘10 19.9 30.3 28.2 19.4 15.5 14.1 12.2 10.1 3.6 -0.0
‘20 13.8 28.6 32.4 27.4 22.5 20.5 16.6 11.4 3.0 0.1 40
‘50 8.0 20.4 28.3 36.3 34.9 33.1 26.2 13.4 2.5 1.3 20
‘80 6.7 16.7 23.8 35.8 39.1 39.0 33.3 14.7 2.6 1.6
0
‘100 6.4 15.5 21.9 34.3 39.5 40.6 37.1 15.6 2.7 1.7
‘200 6.9 14.2 18.9 28.8 35.8 39.3 48.6 23.6 2.9 1.6 −20
‘500 9.4 13.6 15.1 17.2 18.5 19.3 27.8 74.9 7.2 2.8 40
−
‘800 4.3 4.0 3.3 2.9 2.9 3.0 3.0 6.5 51.2 14.7
60
‘999 -0.1 -0.4 -0.2 0.7 1.1 1.2 1.1 1.7 10.4 7.8 −
Figure12: Rankcorrelationbetweentruemeasurementsforlossesatdifferentdiffusiontimestepson
CIFAR-2.
Influence measurement
Square
‘1 ‘10 ‘20 ‘50 ‘80 ‘100 ‘200 ‘500 ‘800 ‘999 norm
‘1 -2.9 -1.0 -0.6 -0.5 -1.1 -1.3 -3.0 -8.3 -3.1 -2.6 0.1 40
‘10 -3.6 -3.8 -3.5 -3.0 -2.5 -2.9 -7.7 -23.4 -9.5 -8.1 2.7 30
‘20 -2.6 -3.4 -3.6 -3.0 -2.8 -3.3 -9.4 -28.8 -11.1 -9.6 2.4
20
‘50 -1.9 -2.8 -3.1 -2.9 -3.3 -4.0 -10.7 -33.5 -11.4 -9.8 1.3
10
‘80 -1.8 -2.4 -2.6 -2.4 -2.9 -3.7 -10.6 -34.7 -10.9 -9.2 1.0
0
‘100 -1.9 -2.2 -2.4 -2.3 -2.8 -3.5 -10.4 -35.2 -10.8 -9.1 1.2
10
‘200 -2.0 -1.7 -2.0 -2.0 -2.2 -2.8 -7.9 -34.8 -10.1 -8.6 1.3 −
‘500 0.6 0.1 0.2 0.3 1.0 1.7 6.1 15.7 -0.9 -0.9 -0.3 −20
‘800 3.6 2.5 1.9 1.4 0.6 0.3 1.5 16.8 43.9 38.3 -2.5 −30
‘999 6.5 5.8 4.0 3.2 1.9 1.4 1.6 7.2 10.1 8.6 -11.3 40
−
Figure 13: Rank correlation (LDS scores) between influence function estimates with different
measurementfunctionsanddifferenttruemeasurementsCIFAR-2,butwiththeretrainedmodels
trainedonthefulldatasetwitharandomsubsetofexampleshavingadown-weightedcontribution
toatraininglossbyafactorof×0.5.
oflessthat0.99aretheCIFAR-2modeltrainedonthefulldatasetcomparedtoanymodeltrained
ona50%subset,whichislikelyduetothefactthatthe50%subsetmodelsaretrainedforhalfas
manygradientiterations,andsomayhavenotfullyconvergedyet. ForCIFAR-10,wherewetrain
for5×asmanytrainingstepsduetoalargerdatasetsize,weobservenear-perfectcorrelationinthe
ELBOmeasurementsacrossallmodels.EachELBOmeasurementwascomputedwithaMonte-Carlo
estimateusing5000samples.
F LDS RESULTS FOR PROBABILITY OF SAMPLING TRAJECTORY
Theresultsforthe“logprobabilityofsamplingtrajectory”measurementsareshowninFigure19.The
probabilityofsamplingtrajectoryappearstobeameasurementwithaparticularlylowcorrelation
across different models trained with the same data, but different random seeds. This is perhaps
unsurprising,sincethemeasurementcomprisesthelog-densitiesofparticularvaluesof1000latent
variables.
20
tnemerusaem
eurT
tnemerusaem
eurT
)%(
noitalerroc
knaR
)%(
noitalerroc
knaRInfluence measurement
‘1 ‘10 ‘20 ‘50 ‘80 ‘100 ‘200 ‘500 ‘800 ‘999
‘1 6.1 4.7 3.8 3.7 3.8 3.9 3.8 -0.7 -1.2 -0.1
‘10 4.8 11.2 12.7 13.1 13.1 13.2 12.4 -6.0 -4.5 -0.9 20
‘20 4.3 13.4 16.6 18.2 18.2 18.3 17.0 -9.1 -5.8 -0.9
‘50 4.4 14.7 19.4 23.4 24.2 24.4 22.6 -13.1 -6.7 -1.3 10
‘80 4.6 14.9 19.8 24.6 25.9 26.3 24.8 -14.4 -6.8 -1.5
0
‘100 4.7 14.9 19.9 24.9 26.4 26.9 25.9 -14.6 -6.9 -1.5
‘200 4.5 14.0 18.5 23.2 25.1 26.0 28.1 -12.8 -6.9 -1.5 10 −
‘500 -1.0 -7.2 -10.3 -14.0 -15.0 -15.2 -13.2 29.9 -1.7 0.5
‘800 -1.6 -4.9 -6.1 -6.6 -6.6 -6.7 -6.6 -1.7 19.3 5.2 −20
‘999 -0.2 -0.7 -0.7 -1.1 -1.3 -1.3 -1.1 0.4 3.8 -2.0
Figure14: Rankcorrelationbetweentruemeasurementsforlossesatdifferentdiffusiontimestepson
CIFAR-2,butwiththeretrainedmodelstrainedonthefulldatasetwitharandomsubsetofexamples
havingadown-weightedcontributiontoatraininglossbyafactorof×0.5.
×10−2 CIFAR-2 ×10−3 CIFAR-10
4
1.2 103
3
1.0
0.8 2
102
0.6 1
0.4 0 101
0.2
1
−
0.0
2 100
−
0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.6 0.4 0.2 0.0 0.2 0.4
− − − − − −
Estimated change in ‘(x,θ) Estimated change in ‘(x,θ)
Figure15: ChangeindiffusionlossℓinSection2.1whenretrainingwithrandomsubsetsof50%of
thetrainingdataremoved,aspredictedbyK-FACinfluence(x-axis),againsttheactualchangeinthe
measurement(y-axis). Resultsareplottedformeasurementsℓ(x,θ)for50samplesxgeneratedfrom
thediffusionmodeltrainedonallofthedata. Thescattercolorindicatesthesamplexforwhichthe
changeinmeasurementisplotted. Thefigureshowsthatinfluencefunctionstendtooverestimate
howoftenthelosswilldecreasewhensometrainingsamplesareremoved;inreality,ithappensquite
rarely.
G EXPERIMENTAL DETAILS
Inthissection,wedescribetheimplementationdetailsforthemethodsandbaselines,aswellasthe
evaluationsreportedinSection4.
G.1 DATASETS
Wefocusonthefollowingdatasetinthispaper:
21
)θ,x(‘
ni
egnahc
lautcA
tnemerusaem
eurT
)%(
noitalerroc
knaR
)nib
ni
selpmas
fo
#(
ycneuqerF×10−2 ×10−2
5
5
4
4
3
3
2 2
1 1
1 0 1 0.6 0.4 0.2 0.0 0.2
− − − −
Estimated change in ‘ Estimated change in ‘
1 50
×10−2 ×10−3
4
5
3
4
3 2
2 1
1 0
0.2 0.0 0.2 0.04 0.02 0.00 0.02 0.04
− − −
Estimated change in ‘ Estimated change in ‘
100 500
100 101 102 103
Frequency (# of samples in bin)
Figure16: Changeinper-diffusion-timesteplossesℓ whenretrainingwithrandomsubsetsof50%
t
ofthetrainingdataremoved,aspredictedbyK-FACinfluence(x-axis),againsttheactualchangein
themeasurement(y-axis). ResultsareplottedfortheCIFAR-2dataset,formeasurementsℓ (x,θ)
t
for50samplesxgeneratedfromthediffusionmodeltrainedonallofthedata. Thescattercolor
indicatesthesamplexforwhichthechangeinmeasurementisplotted. Thefigureshowsthat: 1)
influencefunctionspredictthatthelossesℓ willincreaseordecreaseroughlyequallyfrequently
t
whensomesamplesareremoved,but,inreality,thelossesalmostalwaysincrease;2)forsufficiently
largetime-steps(ℓ 00),thispatternseemstosubside. Lossesℓ inthe 200−500rangeseemtowork
5 t
wellforpredictingchangesinotherlossesFigure11.
CIFAR-10CIFAR-10isadatasetofsmallRGBimagesofsize32×32Krizhevsky(2009). We
use50000images(thetrainsplit)fortraining.
CIFAR-2ForCIFAR-2, wefollowZhengetal.(2024)andcreateasubsetofCIFAR-10with
5000examplesofimagesonlycorrespondingtoclassescarandhorse. 2500examplesofclass
carand2500examplesofclasshorsearerandomlysubsampledwithoutreplacementfromamong
allCIFAR-10imagesofthatclass.
G.2 MODELS
ForallCIFARdatasets,wetrainaregularDenoisingDiffusionProbabilisticModelusingastandard
U-NetarchitectureasdescribedforCIFAR-10inHoetal.(2020). ThisU-Netarchitecturecontains
22
‘
ni
egnahc
lautcA
‘
ni
egnahc
lautcA
1
001
‘
ni
egnahc
lautcA
‘
ni
egnahc
lautcA
05
005r –Pearsoncorrelation 50% 50% 50% Full
coefficient
ρ–Spearmanrank subsample3 subsample2 subsample1 dataset
correlationcoefficient
r=0.668 r=1.000 r=1.000 r=1.000 r=1.000
ρ=0.649 ρ=1.000 ρ=1.000 ρ=1.000 ρ=1.000
r=0.668 r=0.999 r=1.000 r=1.000 r=1.000
ρ=0.649 ρ=0.999 ρ=1.000 ρ=1.000 ρ=0.999
r=0.669 r=0.999 r=0.999 r=1.000 r=1.000
ρ=0.648 ρ=0.999 ρ=0.999 ρ=1.000 ρ=0.999
r=0.668 r=0.999 r=0.999 r=0.999 r=1.000
ρ=0.651 ρ=0.999 ρ=0.999 ρ=0.999 ρ=0.999
CIFAR-10
Full 50% 50% 50% →
dataset subsample1 subsample2 subsample3 CIFAR-2
←
Figure17: CorrelationoftheELBO(x,θ)measurementsondifferentdatapointsx(samplesgen-
eratedfromthemodeltrainedonfulldata),formodelstrainedondifferentsubsetsofdata. Each
subplotplotsELBO(x,θ)measurementsfor200generatedsamplesx,asmeasuredbytwomodels
trained from scratch on different subsets of data, with the x-label and the y-label identifying the
respectivesplitofdatausedfortraining(eitherfulldataset,orrandomlysubsampled50%-subset).
EachsubplotshowsthePearsoncorrelationcoefficient(r)andtheSpearmanrankcorrelation(ρ)
fortheELBO(x,θ)measurementsasmeasuredbythetwomodelstrainedondifferentsubsetsof
data. Thetwopartsofthefigureshowresultsfortwodifferentdatasets: CIFAR-2ontheleft,and
CIFAR-10ontheright.
ELBO weighting
Training loss weighting
101
100
0 200 400 600 800 1000
Diffusion timestep
Figure18: ThediffusionlossanddiffusionELBOasformulatedin(Hoetal.,2020)(ignoringthe
reconstructiontermthataccountsforthequantisationofimagesbacktopixelspace)areequalupto
theweightingoftheindividualper-diffusion-timesteplosstermsandaconstantindependentofthe
parameters. Thisplotillustratestherelativesdifferenceintheweightingforper-diffusion-timestep
lossesappliedintheELBOvs. inthetrainingloss.
bothconvolutionalandattentionlayers. WeusethesamenoisescheduleasdescribedfortheCIFAR
datasetinHoetal.(2020).
23
%05
%05
%05
%05
1elpmasbus
2elpmasbus
3elpmasbus
4elpmasbus
%05
%05
%05
%05
1elpmasbus
2elpmasbus
3elpmasbus
4elpmasbusCIFAR-2 CIFAR-10
7.1% 0.7 3.4% 0.7 CLIPCosineSimilarity
± ±
3.1% 0.8 0.5% 0.8 K-FAC Influence
± ±
9.8% 0.8 (1.7%) 3.1% 0.7 (0.5%) D-TRAK
± ±
12.3% 0.8 (12.3%) 2.3% 0.8 (2.3%) K-FAC Influence(m. loss)
± ±
25.1% 0.3 3.2% 0.1 ExactRetraining
± ±
0 20 40 60 80 100 0 20 40 60 80 100
Rank Correlation % (LDS) Rank Correlation % (LDS)
Figure19: LinearData-modellingScore(LDS)fortheprobabilityofsamplingtrajectory. The
plot follows the same format as that of Figures 2a and 2b. Overall, probability of the sampling
trajectoryappearstobeadifficultproxyforthemarginalprobabilityofsamplingagivenexample,
giventhatitsuffersfromthesameissuesastheELBOonCIFAR-2(it’sbetterapproximatedbythe
wrongmeasurementfunction),andthereisextremelylittlecorrelationinthemeasurementacrossthe
retrainedmodelsonlargerdatasets(CIFAR-10).
SamplingWefollowthestandardDDPMsamplingprocedurewithafull1000timestepstocreatethe
generatedsamplesasdescribedbyHoetal.(2020). DDPMsamplingusuallygivesbettersamples(in
termsofvisualfidelity)thanDenoisingDiffusionImplicitModels(DDIM)samplingSongetal.(2022)
whenalargenumberofsamplingstepsisused. AsdescribedinSection2.1,whenparameterising
theconditionalsp (x(t 1)|x(t))withneuralnetworksasN x(t 1)|µ x(t),ϵt(x(t)) ,σ2I we
θ − − t 1t,0 θ t
haveachoiceinhowtosetthevariancehyperparameters{σ2}T . T− he| σ2hyperparametersdonot
(cid:0) t t=1 t(cid:0) (cid:1) (cid:1)
appearinthetrainingloss;however,theydomakeadifferencewhensampling. Weusethe“small”
variancevariantfromHoetal.(2020,§3.2),i.e. weset:
σ t2 = 1 1−
−
t t t− ′=1 1λ λt′ (1−λ t)
(cid:81)t′=1 t′
(cid:81)
G.3 DETAILSONDATAATTRIBUTIONMETHODS
TRAK For TRAK baselines, we adapt the implementation of Park et al. (2023); Georgiev et al.
(2023)tothediffusionmodellingsetting. WhenrunningTRAK,thereareseveralsettingstheauthors
recommendtoconsider:1)theprojectiondimensiond fortherandomprojections,2)thedamping
proj
factorλ,and3)thenumericalprecisionusedforstoringtheprojectedgradients. For(1),weusea
relativelylargeprojectiondimensionof32768asdoneinmostexperimentsinZhengetal.(2024).
Wefoundthattheprojectiondimensionaffectedthebestobtainableresultssignificantly,andsowe
couldn’t get away with a smaller one. We also found that using the default float16 precision
intheTRAKcodebasefor(3)resultsinsignificantlydegradedresults(seeFigure20, andsowe
recommendusingfloat32precisionforthesemethodsfordiffusionmodels. Inallexperiments,
weusefloat32throughout. Forthedampingfactor, wereportthesweepsoverLDSscoresin
Figures8and10,andusethebestresultineachbenchmark,asthesemethodsfaildrasticallyifthe
dampingfactoristoosmall. Thedampingfactorreportedintheplotsisnormalisedbythedataset
sizeN,tomatchthedefinitionoftheGGN,andtomakeitcomparablewiththedampingreportedfor
otherinfluencefunctionsmethodsintroducedinthispaper. Fornon-LDSexperiments,weusethe
bestdampingvaluefromthecorrespondingLDSbenchmark.
CLIPcosinesimilarityOneofthedataattributionbaselinesusedfortheLDSexperimentsisCLIP
cosinesimilarity(Radfordetal.,2021).Forthisbaseline,wecomputetheCLIPembeddings(Radford
et al., 2021) of the generated sample and training datapoints, and consider the cosine similarity
betweenthetwoasthe“influence”ofthattrainingdatapointonthatparticulartargetsample. See
(Parketal.,2023)fordetailsofhowthisinfluenceisaggregatedfortheLDSbenchmark. Ofcourse,
thiscomputationdoesnotinanywaydependonthediffusionmodelorthemeasurementfunction
used,soitisaprettyna¨ıvemethodforestimatinginfluence.
K-FACWebuildonthehttps://github.com/f-dangel/curvlinopspackageforour
implementation of K-FAC for diffusion models. Except for the ablation in Figure 4, we use the
24K-FACexpandvariantthroughout. WecomputeK-FACforPyTorchnn.Conv2dandnn.Linear
modules(includinginattention),ignoringtheparametersinthenormalisationlayers.
CompressionforallK-FACinfluencefunctionsresults,weuseint8quantisationforthequery
gradients.
Monte Carlocomputation of gradients andthe GGN for influence functions Computing the
per-exampletraininglossℓ(θ,x )inSection2.1,thegradientsofwhicharenecessaryforcomputing
n
the influence function approximation (Equation (6)), includes multiple nested expectations over
diffusion timestep t˜and noise added to the data ϵ(t). This is also the case for the GGNmodel in
Equation (9) and for the gradients ∇ ℓ(θ,x ) in the computation of GGNloss in EquatiDon (11),
θ n
aswellasforthecomputationofthemeasurementfunctions. UnlessspecifiDedotherwise,weuse
the same number of samples for a Monte Carlo estimation of the expectations for all quantities
considered. Forexample,ifweuseK samples,thatmeansthatforthecomputationofthegradientof
theper-example-loss∇ ℓ(θ,x )we’llsampletuplesof(t˜,ϵ(t˜),x(t˜))independentlyK timestoform
θ n
aMonteCarloestimate. ForGGNmodel,weexplicitlyiterateoveralltrainingdatapoints,anddraw
K samplesof t˜,ϵ(t˜),x(t˜) foreacD hdatapoint. ForGGNloss,weexplicitlyiterateoveralltraining
n
D
datapoints,and(cid:16)drawK sa(cid:17)mplesof t˜,ϵ(t˜),x(t˜) tocomputethegradients∇ ℓ(θ,x )beforetaking
n θ n
anouterproduct. Notethat,forGG(cid:16)Nloss,beca(cid:17)usewe’reaveragingoverthesamplesbeforetaking
theouterproductofthegradients,theDestimatoroftheGGNisnolongerunbiased. Similarly,K
samplesarealsousedforcomputingthegradientsofthemeasurementfunction.
ForallCIFARexperiments,weuse250samplesthroughoutforallmethods(includingallgradient
andGGNcomputationsforK-FACInfluence,TRAK,D-TRAK),unlessexplicitlyindicatedinthe
captionotherwise.
CIFAR-2 - TRAK Influence
0.15
Target Measure Train.Loss
0.10 Loss Loss Loss
Loss Sq.Norm Sq.Norm (D-TRAK)
ELBO ELBO* Loss
0.05 ELBO Sq.Norm Sq.Norm (D-TRAK)
0.00
10−9 10−7 10−5 10−3 10−1 101 103 105
Damping factor
Figure20: LDSscoresonforTRAK(randomprojection)basedinfluenceonCIFAR-2whenusing
half-precision (float16) for influence computations. Compare with Figure 8. NaN results are
indicatedwith×.
G.4 DAMPING
Forallinfluencefunction-likemethods(includingTRAKandD-TRAK),weusedampingtoimprove
thenumericalstabilityoftheHessianinversion. Namely,foranymethodthatcomputestheinverseof
theapproximationtotheHessianH ≈∇2 θL =∇2 θ1/N ℓ(theta,x n),weaddadampingfactorλ
tothediagonalbeforeinversion: D
(H +λI)
1,(cid:80)
−
whereI isad ×d identitymatrix. Thisisparticularlyimportantformethodswherethe
param param
Hessianapproximationisatahighriskofbeinglow-rank(forexample,whenusingtheempirical
GGNinEquation(11),whichisthedefaultsettingforTRAKandD-TRAK).ForTRAK/D-TRAK,
the approximate Hessian inverse is computed in a smaller projected space, and so we add λ to
the diagonal directly in that projected space, as done in Zheng et al. (2024)). In other words, if
25
)SDL(
noitalerroc
knaRP ∈Rdproj×dparam istheprojectionmatrix(see(Parketal.,2023)fordetails),thendampedHessian-
inversepreconditionedvectorinnerproductsbetweentwovectorsv ,v ∈Rdparam (e.g. thegradients
1 2
inEquation(6))wouldbecomputedas:
⊺
(Pv 1) (H +λI)−1Pv ,.
whereH ≈P∇2 θL P⊺ ∈Rdproj×dproj isanapproximationtotheHessianintheprojectedspace.
D
Forthe“default”valuesusedfordampingforTRAK,D-TRAKandK-FACInfluence,weprimarily
followrecommendationsfrompriorwork. ForK-FACInfluence,thedefaultisasmalldampingvalue
10 8throughoutaddedfornumericalstabilityofinversion,asdoneinpriorwork(Baeetal.,2024).
−
ForTRAK-basedmethods, Parketal.(2023)recommendusingnodamping. Hence, weusethe
lowestnumericallystablevalueof 10 9asthedefaultvaluethroughout.
−
NotethatalldampingvaluesreportedinthispaperarereportedasifbeingaddedtotheGGNforthe
Hessianofthelossnormalisedbydatasetsize. ThisdiffersfromthedampingfactorintheTRAK
implementation(https://github.com/MadryLab/trak),whichisaddedtotheGGNfor
theHessianofanunnormalisedloss( ℓ(θ,x )). Hence,thedampingvaluesreportedin(Zheng
n n
etal.,2024)arelargerbyafactorofN (thedatasetsize)thantheequivalentdampingvaluesreported
inthispaper. (cid:80)
G.5 LDSBENCHMARKS
For all LDS benchmarks Park et al. (2023), we sample 100 sub-sampled datasets (M := 100 in
Equation(13)),andwetrain5modelswithdifferentrandomseeds(K :=5inEquation(13)),each
with50%oftheexamplesinthefulldataset,foratotalof500retrainedmodelsforeachbenchmark.
WecomputetheLDSscoresfor200samplesgeneratedbythemodeltrainedonthefulldataset.
MonteCarlosamplingofmeasurementsForallcomputationsofthe“true”measurementfunctions
fortheretrainedmodelsintheLDSbenchmarksweuse5000samplestoestimatethemeasurement.
G.6 RETRAININGWITHOUTTOPINFLUENCES
Fortheretrainingwithouttopinfluencesexperiments(Figure3),wepick5samplesgeneratedby
themodeltrainedonthefulldataset,and,foreach,trainamodelwithafixedpercentageofmost
influentialexamplesforthatsampleremovedfromthetrainingdataset,usingthesameprocedureas
trainingonthefulldataset(withthesamenumberoftrainingsteps). Wethenreportthechangeinthe
measurementonthesampleforwhichtopinfluenceswereremoved.
MonteCarlosamplingofmeasurementsAgain,forallcomputationsofthe“true”measurement
functionsfortheoriginalandtheretrainedmodelsusedforcalculatingthedifferenceinlossafter
retrainingweuse5000samplestoestimatethemeasurement.
G.7 TRAININGDETAILS
ForCIFAR-10andCIFAR-2weagainfollowthetrainingprocedureoutlinedinHoetal.(2020),
withtheonlydifferencebeingashortenednumberoftrainingiterations. ForCIFAR-10,wetrain
for160000steps(comparedto800000inHoetal.(2020))forthefullmodel,and80000stepsfor
thesubsampleddatasets(410epochsineachcase). OnCIFAR-2,wetrainfor32000stepsforthe
modeltrainedonthefulldataset,and16000stepsforthesubsampleddatasets(800epochs). We
trainforsignificantlylongerthanZhengetal.(2024),aswenoticedthemodelstrainedusingtheir
procedureweresomewhatsignificantlyundertrained(someper-diffusion-timesteptraininglosses
ℓ (θ,x)havenotconverged). Wealsouseacosinelearning-rateschedulefortheCIFAR-2models.
t
G.8 HANDLINGOFDATAAUGMENTATIONS
InthepresentationinSection2,weignoreforthesakeofclearpresentationtherealitythatinmost
diffusionmodellingapplicationswealsoapplydataaugmentationstothedata. Forexample,the
traininglossL inEquation(3)inpracticeoftentakestheform:
D
N
1
L = E [ℓ(θ,x˜ )],
D N x˜n n
n=1
(cid:88)
26wherex˜ isthedatapointx afterapplyinga(random)dataaugmentationtoit. Thisneedstobe
n n
takenintoaccount1)whendefiningtheGGN,astheexpectationoverthedataaugmentationsE
caneitherbeconsideredaspartoftheouterexpectationE ,oraspartofthelossρ(seeSection2.2.x˜ 1n ),
z
2)whencomputingtheper-exampletrainlossgradientsforinfluencefunctions,3)whencomputing
thelossmeasurementfunction.
WhencomputingGGNmodel inEquation(9),wetreatdataaugmentationsasbeingpartoftheout
“empiricaldatadistributDion”. Inotherwords,wewouldsimplyreplacetheexpectationE inthe
definitionoftheGGNwithanestedexpectationE E :
xn
xn x˜n
GGNmodel(θ)=E E E E ∇⊺ ϵt˜ x(t˜) (2I)∇ ϵt˜ x(t˜) .
xn x˜n t˜ x(t˜),ϵ(t˜) θ θ θ θ
D
(cid:104) (cid:104) (cid:104) (cid:104) (cid:16) (cid:17) (cid:16) (cid:17)(cid:105)(cid:105)(cid:105)(cid:105)
withx(t˜) nowbeingsampledfromthediffusionprocessq(x(t˜)|x˜ )conditionedontheaugmented
n
samplex˜ . Thetermschangingfromtheoriginalequationareindicatedinyellow. The“Fisher”
n
expressionamenabletoMCsamplingtakestheform:
F (θ)=E E E E E g (θ)g (θ)⊺ , ϵ ∼N ϵt˜ x(t˜) ,I ,
D
xn x˜n t˜ xn(t˜),ϵ(t˜) ϵmod n n mod θ n
where,again,g
(θ)(cid:104) =∇(cid:104) ∥ϵ(cid:104) −ϵt˜(x(t˜))∥2(cid:2)
.
(cid:3)(cid:105)(cid:105)(cid:105) (cid:16) (cid:16) (cid:17) (cid:17)
n θ mod θ n
WhencomputingGGNlossinEquation(11),however,wetreattheexpectationoverdaeaaugmenta-
tionsasbeingpartofthDelossρ,inordertobemorecompatiblewiththeimplementationsofTRAK
(Parketal.,2023)inpriorworksthatrelyonanempiricalGGN(Zhengetal.,2024;Georgievetal.,
2023).10Hence,theGGNinEquation(11)takestheform:
GGNloss(θ)=E ∇ (E [ℓ(θ,x˜ )])∇⊺ (E [ℓ(θ,x˜ )])
xn θ x˜n n θ x˜n n 
D

ℓ˜(θ,xn)

 
=E ∇ ℓ˜(θ,x˜ )∇⊺ ℓ˜(θ,x˜ ) (cid:124), (cid:123)(cid:122) (cid:125)
xn θ n θ n
(cid:104) (cid:105)
whereℓ˜istheper-examplelossinexpectationoverdata-augmentations. ThisishowtheHessian
approximation is computed both when we’re using K-FAC with GGNmodel in presence of data
augmentations,orwhenwe’reusingrandomprojections(TRAKandD-TDRAK).
WhencomputingthetraininglossgradientininfluencefunctionapproximationinequationEqua-
tion(5),weagainsimplyreplacetheper-exampletraininglossℓ(θ⋆,x )withtheper-exampletraining
j
lossaveragedoverdataaugmentationsℓ˜(θ⋆,x ),sothatthetraininglossL canstillbewrittenasa
j
finitesumofper-examplelossesasrequiredforthederivationofinfluenceDfunctions.
ForthemeasurementfunctionminEquation(6),weassumeweareinterestedinthelogprobability
of(orlosson)aparticularqueryexampleintheparticularvariationinwhichithasappeared,sowe
donottakedataaugmentationsintoaccountinthemeasurementfunction.
Lastly, since computing the training loss gradients for the influence function approximation for
diffusion models usually requires drawing MC samples anyways (e.g. averaging per-diffusion
timesteplossesoverthediffusiontimest˜andnoisesamplesϵ(t)),wesimplyreportthetotalnumber
of MC samples per data point, where data augmentations, diffusion time t˜, etc. are all drawn
independentlyforeachsample.
10Theimplementationsofthesemethodsstorethe(randomlyprojected)per-exampletraininglossgradients
foreachexamplebeforecomputingtheHessianapproximation.Hence,unlessdataaugmentationisconsidered
tobepartoftheper-exampletrainingloss,thenumberofgradientstobestoredwouldbeincreasedbythe
numberofdataaugmentationsamplestaken.
27