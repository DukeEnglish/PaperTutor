1
R : Robotic Augmented Reality for Machine
AMPA
Programming and Automation
Fatih Dogangun1, Serdar Bahar1, Yigit Yildirim1, Bora Toprak Temir1, Emre Ugur1, and Mustafa Doga Dogan2,1
Abstract—Asroboticscontinuetoentervarioussectorsbeyond
traditional industrial applications, the need for intuitive robot
training and interaction systems becomes increasingly more
important. This paper introduces Robotic Augmented Reality
for Machine Programming (RAMPA), a system that utilizes the
capabilities of state-of-the-art and commercially available AR
headsets, e.g., Meta Quest 3, to facilitate the application of Pro-
gramming from Demonstration (PfD) approaches on industrial
robotic arms, such as Universal Robots UR10. Our approach
enables in-situ data recording, visualization, and fine-tuning of
skill demonstrations directly within the user’s physical environ-
ment.RAMPAaddressescriticalchallengesofPfD,suchassafety
concerns,programmingbarriers,andtheinefficiencyofcollecting
demonstrationsontheactualhardware.Theperformanceofour
system is evaluated against the traditional method of kinesthetic
controlinteachingthreedifferentroboticmanipulationtasksand
analyzed with quantitative metrics, measuring task performance
and completion time, trajectory smoothness, system usability,
user experience, and task load using standardized surveys. Our
findingsindicateasubstantialadvancementinhowrobotictasks
are taught and refined, promising improvements in operational
safety, efficiency, and user engagement in robotic programming.
Fig.1. With RAMPA,userscandemonstratetrajectoriesandsimulaterobot
IndexTerms—augmentedreality,extendedreality,mixedreal- movementsinsitu.
ity, programming from demonstration
integrating such approaches into robotic programming often
I. INTRODUCTION
encounterssignificantchallenges,includingthelabor-intensive
The advent of sophisticated robotic applications across var- andpotentiallyhazardousprocessesinvolvedindatagathering
iousindustriesnecessitatesadvancedprogrammingtechniques and model testing in the real world [3].
that can accommodate complex, dynamic environments. The Inresponsetothesechallenges,weintroduceasophisticated
conventional approach to robot programming typically in- AR-based framework that transforms robotic programming
volves segmented processes where users must alternate be- through direct, interactive modeling and visualization. Our
tween physical and digital realms. Traditional approaches system, Robotic Augmented Reality for Machine Program-
rely heavily on offline simulation and manual coding, which ming and Automation (RAMPA), leverages the immersive and
can be time-consuming and often impractical for real-time interactive capabilities of state-of-the-art XR headsets (e.g.,
applications[1].Asthesemethodslackthecapabilitytoadapt Meta Quest 3) to create a cohesive and safe user experience
quickly to new environments or to intuitively incorporate for robot programming and automation of tasks in situ.
human expertise, the complexity and time involved in pro- By integrating in-situ simulation, real-time spatial under-
gramming increases, which also heightens the risk of errors standing, and direct manipulation capabilities, RAMPA offers
and accidents. a dual advantage: it streamlines the programming process
TheprocedureknownasProgrammingfromDemonstration while enhancing the physical safety, flexibility of robotic
(PfD), a.k.a. Learning from Demonstration (LfD), is a widely operations, and efficiency of PfD procedures and, therefore,
adopted procedure that enables robots to learn skill policies reduces the barriers to effective human-robot collaboration.
bywatchinganexpert[2].Recently,MachineLearning(ML)- The ability to produce and adjust demonstration trajectories
based PfD frameworks have become instrumental in enabling in real-time and in situ facilitates the task of the operator
robotstomodelandexecuteintricateskillsautonomously.Yet, while interacting directly with the learned models establishes
a deeper understanding of these models.
1Authors are with the Department of Computer Engineering, Bogazici
University, Istanbul, Tu¨rkiye. 2Mustafa Doga Dogan is also with Adobe Our system facilitates a seamless, intuitive interface for:
Research, Basel, Switzerland. This work has been supported by the IN- a) Data Recording: Operators use XR to register real
VERSE project (no. 101136067), funded by the EU. Corresponding author:
and/orvirtualobjectsanddemonstrateskills,capturingthedata
fatih.dogangun@std.bogazici.edu.tr
Codeisavailableathttps://github.com/dogadogan/rampa directly in the user’s environment. Compared to traditional
4202
tcO
71
]OR.sc[
1v21431.0142:viXra2
Fig. 2. RAMPA allows users to (a) collect trajectory data by drawing the desired path with a pinch. (b) The virtual robot then simulates the trajectory,
allowinguserstoobservehowtheactualrobotwouldinteractwithitsenvironment,giventhetrajectory.(c)Afterthesimulation,theusermaydemonstrate
thetrajectoryontheactualrobot.(d)RAMPAalsofacilitatesthecollectionoftrajectorydatatotrainMLmodels.Aftertraining,userscanconditionthemodel
interactivelyandgenerateasampletrajectoryfromit,asshowninFig.5.
methods,suchasthekinestheticcontrol,RAMPAsimplifiesthe • Thereleaseofthesystemasopen-sourcefortherobotics
collectionofdemonstrationtrajectories,anintegralcomponent community, compatible with cutting-edge mixed reality
of PfD procedures, offering a more efficient experience for headsets (e.g., Quest 3) and robotic arms.
robotic programmers. • Comprehensive evaluation and analysis of the system in
b) Data Playback and Adjustment: The system allows teaching different robot manipulation tasks using quanti-
operators to visualize and modify the movements of a virtual tative metrics measuring task performance and comple-
robot before these movements are executed on the actual tion time, trajectory smoothness, system usability, user
robotic arm. This step ensures that potential errors can be experience, and task load.
corrected in a safe, virtual space. Moreover, the ability to • Facilitation of interactive and situated programming of
adjust any portion of demonstration trajectories increases the PfDmodelssuitableforcommonlyusedmovementprim-
operator’s overall performance. itives in robotics.
c) ModelTrainingandValidation: Utilizingthecollected
data, popular PfD models, such as Probabilistic Movement II. RELATEDWORK
Primitives (ProMPs) [4], can be trained and validated within
A. MovementPrimitivesinProgrammingfromDemonstration
the AR environment. Operators can observe the generated
Enabling robots to perform intricate real-world tasks has
trajectories in real-time, ensuring the robot’s behavior aligns
been the driving force of robotics research. Functioning in
with desired outcomes.
the real world, outside the controlled laboratory conditions,
To the best of our knowledge, we propose the first end-to-
requires sophisticated policies. To overcome the difficulty of
endXR-drivenPfDframeworktoenablethedemonstrationof
developing such policies, movement primitives (MP), which
robotictrajectories,training,andtestingMLmodelaugmented
provide effective abstractions that facilitate motor control [8],
by visualization of the robot motion and trajectory adjustment
is a modular formalism widely employed to encode and
in situ. Importantly, our comprehensive evaluation comprises
reproduce complex movements. PfD enables learning MP by
the experiment consisting of three distinct robotic tasks, 20
observing an expert [2]. Following the PfD paradigm, [8]
participants including both experienced and novice users, and
proposed the Dynamical Movement Primitives (DMPs) model
an extensive analysis using quantitative metrics, incorporating
to learn MPs by leveraging human expertise. The original
standard surveys, such as System Usability Scale (SUS) [5],
DMP model could encode only a single trajectory, while
User Experience Questionnaire (UEQ) [6], and NASA Task
different demonstrations may contain important variabilities
Load Index (NASA-TLX) [7].
that the modeling framework must capture [9], [10]. In order
This paper details the development and implementation of
to capture the variety in demonstrations, [11] proposed to
our system, evaluates its performance against the traditional
modelmovementtrajectoriesusingprobabilisticmethodssuch
method of kinesthetic control, and demonstrates its effec-
as GMM and GMR, and [12] combined the DMP and GMM
tiveness through empirical studies. Our findings indicate a
based approaches. These methods allowed the use of multiple
significant advancement in how robotic tasks are taught and
demonstrations to model the skill trajectories, enhancing the
refined,promisingnotableimprovementsinoperationalsafety,
representationalpowerandthegeneralizationcapacityofMPs
efficiency, and user engagement in robotic programming.
[13].
In summary, we make the following contributions:
Latest advances in ML have significantly influenced PfD
• AnovelAR-basedsystemthatenablesuserstocreateand approaches, enabling the synthesis of more flexible and gen-
fine-tuneroboticapplicationsthatmimichumanbehavior, eralizable MP-based controllers to perform tasks with higher
allowing safe, in-situ simulation and execution. efficiency and adaptability. Recent studies, such as [14], [15],3
[16],usedeepneuralnetworkstomodelmovementtrajectories real-time, which improves the accuracy of robot actions in
in the form of MPs. Although they are successful in encoding human-robot collaborative operations. Similarly, Quintero et
andreproducingmorecomplexmotortrajectories,theyarenot al. [24] introduces a method that allows users to interact
practical for real-time applications, specifically for human-in- with robot trajectories with ease by using an AR interface
the-loop interactive systems, as they need a training phase to visualize the motion of the robot. Other studies [25], [26]
to model demonstration trajectories. Due to the real-time showthatvisualizingrobotmovementimprovestaskefficiency
requirement, a widely used approach, ProMPs, which models and enhances user understanding of robot intentions, hence
a distribution of trajectories as a probability distribution over facilitating human-robot interactions.
a set of basis functions, improving the generalization capa- AR interfaces have also been used for robot training meth-
bility to novel via points [17], is used as the MP modeling ods such as PfD and teleoperation, as they offer interactive
framework in this study. and intuitive ways to teach robots new skills. Holo-Dex [27]
Despite these techniques allowing for efficient learning provides users with a broad AR-based framework for robot
and generalization of movements, the application of these learning with teleoperation. Alongside, Luebbers et al. [28]
methodsgenerallyrequirestime-consumingproceduresindata offers in-situ visualizations of robot trajectories with their
collection, model training, and applying the trained model constraints, and Diehl et al. [29] proposes a system for visu-
back to real-world settings [3]. Furthermore, operators could alization of learned skills for verification and error detection,
find it challenging to grasp and interpret the behavior of therefore providing an intuitive platform for users to convey
ML models, posing challenges to the explainability of AI proper demonstrations to the robot. However, these studies
in robotics. In the RAMPA system, integrating AR into the require kinesthetically demonstrated trajectories. Lotsaris et
ML training and visualization process allows users to define al. [30] introduces an AR system to demonstrate robot tra-
both real and virtual objects and demonstrate tasks within an jectories by placing the virtual gripper in the target position.
AR environment, eliminating manual spatial data recording. Thiswork,however,doesnotallowtheusertodemonstratethe
Moreover, the RAMPA system enhances the comprehensibility precise path. By bridging the gap left by AR-based interfaces
of ML models by offering a user-friendly and interactive for PfD, we propose a comprehensive, end-to-end system
platform for ML training and fine-tuning. that allows users to demonstrate robotic trajectory, visualize
the robot motion, modify the trajectory, train, and test the
ML model in situ, hence making robotic programming more
B. AR-Oriented Interfaces
accessible and practical.
Asroboticsystemshaveincreasinglybeenemployedinvar-
ious domains, the necessity for efficient methods and intuitive III. RAMPA:ANEND-TO-ENDSYSTEMFORROBOTICAR
interfaces for robot programming has grown to assist users
The RAMPA system incorporates ROS server, Unity game
ranging from novice to technical expert. Different types of
engine, and Quest 3 XR headset to provide an intuitive plat-
robot programming frameworks, such as visual programming
formforinteractivedemonstrationcollection,MLtraining,and
methods [18]combinedwithspeech-basedprogramming [19]
visualization. In this section, features and the implementation
have been proposed for users to control and instruct robotic
of the proposed system are discussed.
systems and aim to allow users to communicate instructions
and feedback to robots with ease and high precision, which
improvesthefunctionalityandusabilityofroboticsystemsand A. System Overview
enablesefficienthuman-robotinteraction.Recently,foundation RAMPA features key functionalities to facilitate safe and
modelshavebeendeployedinreal-worldroboticapplications; cohesive trajectory capturing with hand mimicry, fine-tuning
nevertheless, they are still inadequate for tasks requiring of the demonstrated trajectories, use of this recorded data to
human involvement [20]. trainMLmodelsandsafeexecutionoftherecordedtrajectories
Unlike the traditional methods, XR/AR technologies, with inthevirtualenvironment,orthetrajectoriessampledfromthe
their real-time and contextual feedback directly within the trained model on the real robot.
user’s field of view, make it possible to have more natural 1) Simulated Robot and Auto-calibration: A simulated
and efficient interactions between human users and robots. As robotispresenttomodeltheactualrobot’sbehavior,allowing
a subset of XR technologies, they enable the superimposition foraccuratetestingandvalidationoftrajectoryplans.Anauto-
of computer graphics onto the physical world [21] and have calibration feature is included, which uses the controller from
been increasingly utilized to enhance human-robot interaction the headset and a tool that can hold the controller for the
(HRI),bridgingthegapbetweenvirtualandrealenvironments. virtual robot to be aligned with the actual robot, as shown in
Recent surveys [22], [3] demonstrate the potential of AR to Fig.3.Thesimulationenvironmentensuresthatanytrajectory
transform robot programming, providing a detailed taxonomy can be tested thoroughly for safety before being deployed on
of AR applications in robotics, including beneficial aspects of the real robot, also making sure the actual robot will replicate
AR for robot control, programming, and visualization. both the location and the motion of the simulated one.
AR visualization of the robot’s motion has been used to 2) Trajectory Playback and Adjustment: Trajectory play-
simplify the process of planning, observing, and modifying back and adjustment are implemented to allow the user to
the robot’s trajectory for users. For instance, GhostAR [23] fine-tunetherecordedtrajectory.Theusercangobackwardor
enables users to visualize and adjust robot trajectories in forward, play the rest of the re-winded trajectory, and pause4
Fig. 5. Users can save and view the trajectories they have recorded. Users
canthentrainanMLmodel,conditionitusingvirtualcubes,andgeneratea
newtrajectory.
which enhances the adaptability of the system and flexibility
of the user during the demonstration collection process. The
training set can then be used to train a model. To test the
model, the user can add way-points to condition the trained
Fig.3. Whentheauto-transformoptionisselected,thevirtualrobotteleports model using virtual objects, as demonstrated in the right-hand
basedonanoffsetrelativetotheleftcontroller.Whenthecontrollerisattached
side of Fig. 5. This process facilitates the tasks of training,
totherobot’sbase,thevirtualrobotalignswiththeactualrobot.
conditioning, and sampling trajectories efficiently in an AR
environment.
whendesired(Fig.4).Iftheuserdecidestodiscardaportionof 5) Execution on the Real Robot: The execution of the
the trajectory, they can redraw the trajectory from the current recorded trajectories and the sampled trajectories from the
way-point. This interactive approach enables precise control trained model can be done using the virtual user interface.
of the robot’s movements and ensures that the final trajectory This capability actualizes the transfer of trained tasks from
is both accurate and safe for the desired task. simulation to real-world execution.
3) Real-TimeHandFollowing: Theusermaypreferhaving 6) Collision Detection: Incorporating scene understanding
the simulated robot follow the position and orientation of and collision detection, the simulated robot can perceive
their hand, as shown in Fig. 1. This enables the user to plan and interpret its environment. This capability ensures that
moreeffectivelyandrefinetrajectoriesinasafeandcontrolled the robot identifies potential obstacles and issues warnings
environment,avoidingpotentialsafetyissuesorcollisions.The when a possibility of collusion occurs. This integration of
immediate feedback allows previewing how the robot will sceneunderstanding,collisiondetection,andreal-worldtesting
navigate through the real objects. ensures the robot operates safely and efficiently, providing
4) ML Training and Preview: The recorded trajectories confidence in its deployment for various tasks.
can be either discarded or added to a training set, which
can then be viewed or deleted, as shown in Fig. 5. The B. Implementation
RAMPA allows the user to add demonstrated trajectory to the
The implementation can be divided into four parts: In the
training set after the simulation of the robot motion and/or
gameengine,whereUnityisused,themainfunctionalitiesare
physical robot execution. Moreover, the user can modify
implemented using Meta Quest API packages. The ROS part
the recorded trajectory post-simulation and/or post-execution,
manages the interactions between the application and the IK
service and the models used for training, along with handling
the communication between the application and the actual
robot. The actual robot is used to record the demonstrated
trajectories and execute the skills effectively upon request.
Finally, the XR headset, specifically Quest 3, deploys and
operates Unity application where the scene detection features
are utilized.
1) Unity: The simulated robot is integrated into Unity,
using the URDF Importer package of Unity, where the simu-
lated robot replicates the kinematics, dynamics, and physical
properties according to the Unified Robot Description Format
(URDF) file of the robot. The robot also behaves like a
physical object and can collide with object meshes obtained
from Scene API of Meta Quest packages. A menu is also
implemented, enabling the user to draw and adjust trajecto-
ries, switch between different trajectory capture modes, and
perform the previously mentioned ML tasks.
Fig.4. Userscangobackandforthinthetrajectoryandthencanredrawit Detecting and recording the hand movements are done by
fromthecurrentway-pointusingtheUImenuprovided. the use of Meta Quest’s hand gesture recognition and the5
message types that the Unity client complies with.
WhentheUnityclientsendsarequest,includingthecurrent
stateoftherobotandtargetpositions,theROSserverresponds
totherequestwithatrajectoryconsistingofjointstates,using
the solution from the IK solver. Recorded trajectories can be
saved and ML models can be trained using these trajectories
upon service requests to ROS client.
3) Actual Robot: For the execution of the trajectory on the
real robot, a TCP connection is set up between the server
and the actual robot. By publishing states from the recorded
trajectory in the virtual environment, the actual robot follows
the published states.
4) XR Headset: The Unity application is deployed on the
headset for complete operation with AR experience. For the
co-locationofthesimulatedrobotwiththeactualrobot,theleft
controller’s position and rotation are used with relevant offset
values, as illustrated in Fig. 3. For collision detection, Quest
3’s Space Setup feature is utilized, where the current room is
Fig.6. TechnicalworkflowofRAMPA.Thestateofthehand,controller,and scanned with the headset, creating scene anchors, which will
room environment is retrieved from the tracking system of the XR headset.
then be used in the application as physical meshes.
TheUnityapplicationalsorunsontheheadset.
IV. EVALUATION
related APIs. The user can choose whether the positions in
We conducted a comprehensive evaluation using quantita-
the trajectory are recorded with a fixed orientation or if the
tive metrics to assess the competence and performance of the
orientation of the user’s hand is also captured. The user can
RAMPAsystemandcompareitwithKinestheticControl(KC).
also choose whether the simulated robot demonstrates the
These assessments include measuring the time required for
trajectory after the hand-drawn trajectory is completed or if
taskcompletion,evaluatingtheperformanceoftrainedMP,the
therobotdemonstratesitinrealtimeasthetrajectoryisbeing
smoothness of demonstrated trajectories, and user feedback
drawn.
concerning usability, user experience, and task load through
If the user opts for real-time hand-following by the sim-
questionnaires.
ulated robot, the IK solver continuously computes the next
state of the trajectory, while the ROS client simultaneously
A. Participants
sends this state information to Unity for execution as the
user continues to draw the trajectory as shown in Fig. 1. A total of 20 participants (18 male and 2 female, ages
Otherwise, the whole data of the trajectory is sent to the ROS 20-36, M=23.5, SD=3.5) were recruited for the user study.
client to calculate the necessary joint angles for execution. Participants include 10 people with experience in robotics,
To facilitate data playback, the number of joint states is where 7 of them additionally had experience in AR (experts),
kept equal to the number of target points in the trajectory and 10 people with no prior experience in AR and robotics
recorded. After necessary joint states are received from the (novices). Permission for the user study is taken from the
ROS client, the trajectory is ready to be executed with user InstitutionalReviewBoardforResearchwithHumanSubjects
input. If the simulated robot collides with any object from of Bogazici University (No:2024/22).
the surroundings, a warning is triggered to the user. With
trajectory data being stored, the user then can playback and B. Experimental Setup
adjust the trajectory by redrawing from the current waypoint,
We have conducted an experiment including three robotic
using the menu illustrated on the right-hand side of Fig. 4.
tasks; both demonstrated using RAMPA system (RAMPA con-
Trajectory points and orientations can be saved, viewed -
dition) and the physical robot in Kinesthetic Control (KC)
whichcorrespondstoyellowtrajectoriesinFig.5,andbeused
condition. Participants were equipped with the Quest 3 XR
to train an ML model in the ROS client upon user request.
headset. We used the Universal Robot UR10, and the experi-
The trained model can also be conditioned with position and
ment was conducted in a robotics laboratory on campus.
orientation, using virtual cubes, as demonstrated in the right-
hand side of Fig. 5.
C. Procedure
2) ROS Client: The ROS server remains on standby to
accommodate the requests from the Unity application. Upon We arranged the user study so that half of the participants
initialization, the application establishes a TCP connection firstusedthe RAMPA systemandthenmovedtherobotinKC
usingtheROS-TCP-ConnectorpackagefromUnity.Withinthe mode, while other participants performed these tasks in the
server, multiple ROS services are utilized to manage various opposite order.
types of operations initiated by the Unity application. These Before participants started performing tasks, we offered an
services operate within a single process and require unique overview and explanation of the procedure and experiment.6
TABLEI
THECOMPARISONOFTASKCOMPLETIONTIMES
KC RAMPA
Experts 27.6s±5.2 7.9s±1.3
Task1 Novices 33.3s±10.0 7.5s±1.4
Experts 69.5s±15.6 17.7s±3.4
Task2 Novices 92.1s±18.9 19.2s±4.8
Experts 63.4s±5.3 46.8s±7.0
Task3 Novices 60.7s±13.0 40.9s±7.0
D. Results & Discussion
Fig.7. ComparisonofTaskCompletionTimesofTheRAMPAandKinesthetic
Control. The box shows the interquartile range (IQR), including the middle a) Task Completion Time: We measured the task com-
50% of the data between the first quartile (Q1) and the third quartile (Q3). pletion time to evaluate the efficiency of the RAMPA system
Thewhiskersextendfromtheedgesoftheboxto1.5IQRoftheQ1andQ3,
comparedtoKC.Ouranalysis,aspresentedinFig.7,revealed
andtherestofthedatapointsareconsideredoutliers,shownasredpoints.
that the average task completion time is shorter while partic-
ipants utilize the RAMPA system. A paired t-test confirmed
We demonstrated to the user how to use both RAMPA system significantlyshortercompletiontimesintheRAMPAcondition
andthephysicalrobot.Afterparticipantsbecamefamiliarwith across all tasks: Task 1 (t = 11.83, p <0.001), Task 2 (t =
using the RAMPA system and moving the real robot in KC 13.93, p <0.001), Task 3 (t = 7.55, p <0.001).
mode, we asked them to perform three main tasks: IntheKCcondition,TableIshowsthatthetaskcompletion
timedifferencebetweenexpertandnoviceuserswas22.6%for
a) Warm-up Exercise: Users were asked to move and
Task 1 and 34.5% for Task 2, indicating a notable difference
control the physical robot. To familiarize participants with
duetotheincreasedcomplexityofthedemonstrationforTask
the RAMPA system, they were asked to place a 3D printed
2 as expected. On the other hand, when utilizing the RAMPA
controller attachment to ensure the real and virtual robots are
system, the task completion time difference was remarkably
correctlycalibratedposition-wiseanddrawasimpletrajectory.
reduced. Our analysis thus suggests that RAMPA allows users
b) Basketball Task (Task 1): Participants were requested
to demonstrate robot trajectories efficiently and lessens the
to draw a trajectory where when the simulated robot mimics
impact of prior experience on task performance.
the route drawn, it can pass a ball that it holds through a
b) Trained Movement Primitives: To compare the tra-
basketball hoop. The robot initially holds a ball and releases
jectory generation performances of the models learned from
it at the end of the route. We also asked them to perform this
RAMPA and KC modes, a ground truth trajectory is required
task by manipulating the real robot with their hands.
for the new environment where these models are tested. For
c) Chain-Hook Task (Task 2): Inspired by crane opera-
this, we initially trained a contextual ProMP model using the
tions for logistics in the industry, a chain hanging from the
trajectories demonstrated during Task 3, using the heights of
ceiling and an object on the table with an attached hook were
the objects as the model’s parameter. We considered the tra-
prepared.Userswereaskedtodrawatrajectoryfortherobotto
jectorysampledfromthismodelastheground-truthtrajectory.
perform a sequence of actions: moving the chain towards the
Then, we computed the Mean Squared Error (MSE) between
object, connecting the chain with the hook, lifting the object
the ground-truth trajectory and the generated trajectory from
from the table, and subsequently placing the object back on
the trained and conditioned ProMP by the user during task 3.
thetable.Theywerealsoaskedtodothesametaskbymoving
The result shows that the mean of MSE values is 0.0017 (SD
the real robot manually with their hands.
= 0.0024) for the KC condition and 0.0014 (SD = 0.0011) for
d) Obstacle Avoidance Task (Task 3): Users were re- RAMPA condition, and indicates that the ProMP model shows
quested to train and test a model for an obstacle avoidance comparable performance when trained on trajectories derived
task. They were asked to draw trajectories by avoiding the using either RAMPA system or kinesthetic control.
totalof3objectswithchangingsizesandtrainthemodelusing c) Trajectories Smoothness: We conducted an analysis
thesetrajectories.Then,weplacedanobjectofadifferentsize using the following metrics: jerk, deviation, and variation to
from the previous objects and asked participants to test their analyze the smoothness of trajectories. As shown in Table II,
model by specifying it via a point above the object. trajectories recorded using the RAMPA system show superior
We recorded the completion time for each task. In tasks 1 or comparable smoothness across the metrics. Our analysis
and 2, the duration for demonstrating a single robotic trajec- indicates that RAMPA facilitates the demonstration of robotic
torywasrecorded.Intask3,wemeasuredthetimerequiredto trajectoriesbyincreasedefficiencywhilemaintainingthecon-
demonstratethreedistinctrobottrajectories.Intask3,wealso sistency and quality of demonstrations.
saved the demonstrated trajectories for subsequent analysis, d) SurveyResults: ThesurveyconsistsofSystemUsabil-
allowing for the comparison of their respective smoothness ityScale(SUS)[5],theshorterversionoftheUserExperience
andthesimilarityamongtrainedMPs.Uponcompletionofthe Questionnaire(UEQ)[6],theNASATaskLoadIndex(NASA-
tasks, the participants were also asked to complete a survey. TLX) [7], and additional questions to compare the different7
TABLEII We additionally asked the participants to rate the flexibility
THECOMPARISONOFSMOOTHNESSPARAMETERSOFNORMALIZED of the RAMPA system and their learning curve on a scale of
DEMONSTRATIONTRAJECTORIES.
0-5.Theresultsindicatethattheaveragescoreofflexibilityis
4.15(SD=0.73),andtheaveragelearningcurvescoreis4.15
KC RAMPA
(SD = 0.65). High scores imply that users grasped RAMPA
Jerk 0.032±0.008 0.003±0.001
quickly, and emphasize its user-friendly and intuitive design.
Deviation 0.077±0.007 0.080±0.012
Variation 0.037±0.005 0.036±0.010
To evaluate the perceived safety of the RAMPA system,
participants were asked to express which method they found
safer. Reported responses were overwhelmingly positive to-
aspects of RAMPA system and KC. We utilized the SUS, wardsthe RAMPA system,with95%ofparticipants(19outof
Fig. 8, to evaluate the usability of the RAMPA system, and 20) indicating that using RAMPA is safer than KC. This result
we obtained a score of 82.75, which shows that the RAMPA suggests that RAMPA addresses safety concerns in human-
system is within A grade (i.e., between 90% and 95%) [31]. robot interactions, especially for novice users.
We employed a subset of questions from the SUS to We employed the shorter version of the UEQ to further
compare the RAMPA and KC conditions, and the results are evaluatetheuser’sexperienceofusingthe RAMPA system.As
generally favorable towards the RAMPA system. As shown in shown in Fig. 9, the RAMPA system has positive evaluations
Table III, the RAMPA system has superior scores across posi- across both pragmatic and hedonic qualities. Particularly,
tive statements such as Q1, Q3, Q5, and Q7, which suggests RAMPAreceivedascoreof2.09onpragmaticqualityand1.85
that the RAMPA system provides an easy-to-use framework on hedonic quality scales, which puts the RAMPA system in
fordemonstratingrobotictrajectories.Inaddition,thereported the range of the top 10% results [6].
scores for Q4, Q5, and Q6 suggest that manually moving the
robot might be perceived as more demanding and complex
comparedtousingthe RAMPA system.Thesefindingssupport
that the RAMPA enhances the user experience and satisfaction
whilesimplifyingtherobotictrajectorydemonstrationprocess.
TABLEIII
SUSRESULTSCOMPARISONOFKINESTHETICCONTROLANDRAMPA.
Question KC RAMPA
IthinkthatIwouldliketouse
2.95±1.16 4.3±0.56
thissystemfrequently. Fig.9. UEQResultsofTheRAMPA.
Ifoundthesystemunnecessarily
1.9±0.89 1.45±0.50
complex. The analysis of the NASA-TLX survey with a paired t-
Ithoughtthesystemwaseasytouse. 3.4±1.16 4.25±0.70 testrevealsthatparticipantsusing RAMPA weremoresatisfied
IthinkthatIwouldneedthesupport with their task performance (t = 2.08, p = 0.044) and felt
ofatechnicalpersontobeabletouse 2.75±1.30 2.2±0.98
less frustration (t = -2.518, p = 0.016) while indicating that
thissystem.
Iwouldimaginethatmostpeoplewould
3.5±1.02 4.1±0.77
learntousethissystemveryquickly.
Ifoundthesystemverycumbersome
3.15±1.49 1.6±0.73
touse.
Ifeltveryconfidentusingthesystem. 3.5±0.92 4.15±0.79
Ineededtolearnalotofthingsbefore
1.75±0.89 1.5±0.59
Icouldgetgoingwiththissystem.
Fig.10. ComparisonofNASA-TLXResultsofTheRAMPAandKinesthetic
Fig.8. SUSResultsofTheRAMPA. Control.Thebox-and-whiskerplotisexplainedinFig.78
RAMPA requires less physical demand (t = -6.92, p <0.001) [4] A. Paraschos, C. Daniel, J. R. Peters, and G. Neumann, “Probabilistic
and effort (t = -5.04, p <0.001 (Fig. 10). The increased movementprimitives,”NIPS,vol.26,2013.
[5] J.Brooke,“Sus:Aquickanddirtyusabilityscale,”UsabilityEvaluation
performancesatisfactionwithreducedphysicaldemand,effort,
inIndustry,1996.
and frustration suggests that RAMPA provides an intuitive [6] M. Schrepp, A. Hinderks, and J. Thomaschewski, “Design and evalu-
and user-friendly interface to perform tasks effectively. These ation of a short version of the user experience questionnaire (ueq-s),”
IJIMAI,vol.4,p.103,012017.
findings show that RAMPA enhances not only task efficiency
[7] S.Hart,“Developmentofnasa-tlx(taskloadindex):Resultsofempirical
but also user experience and comfort. andtheoreticalresearch,”Humanmentalworkload/Elsevier,1988.
At the end of the survey, participants were asked which [8] S.Schaal,J.Peters,J.Nakanishi,andA.Ijspeert,“Learningmovement
approach they would prefer to use for demonstrating robotic primitives,” in Eleventh International Symposium Robotics Research,
2005,pp.561–572.
trajectory. The reported answers show a significant preference
[9] M. Saveriano, F. J. Abu-Dakka, A. Kramberger, and L. Peternel,
towards RAMPA with 95% (19 out 20 participants), which, “Dynamic movement primitives in robotics: A tutorial survey,” IJRR,
considered with previous evaluation results, indicates that vol.42,no.13,pp.1133–1184,2023.
[10] E. Ugur and H. Girgin, “Compliant parametric dynamic movement
RAMPA strongly facilitates the efficacy and accessibility of
primitives,”Robotica,vol.38,no.3,pp.457–474,2020.
the robot programming process. [11] E.Gribovskaya,S.M.Khansari-Zadeh,andA.Billard,“Learningnon-
linearmultivariatedynamicsofmotioninroboticmanipulators,”IJRR,
vol.30,no.1,pp.80–117,2011.
V. CONCLUSION
[12] H. Girgin and E. Ugur, “Associative skill memory models,” in 2018
We introduce an AR-based framework, RAMPA, that pro- IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS). IEEE,2018,pp.6043–6048.
vides a comprehensive and intuitive platform for demonstrat-
[13] X.YinandQ.Chen,“Learningnonlineardynamicalsystemformove-
ing robotic skill trajectories, integrating trajectory visualiza- ment primitives,” in 2014 IEEE International Conference on Systems,
tions and modifications, real-time hand mimicry, and training Man,andCybernetics(SMC). IEEE,2014,pp.3761–3766.
[14] R. Pe´rez-Dattari and J. Kober, “Stable motion primitives via imitation
and testing ML models. These functionalities of RAMPA
andcontrastivelearning,”IEEETransactionsonRobotics,2023.
simplify the demonstration collection, an integral part of [15] D. Blessing, O. Celik, X. Jia, M. Reuss, M. Li, R. Lioutikov, and
the PfD procedures, and enhance the human-robot interac- G.Neumann,“Informationmaximizingcurriculum:Acurriculum-based
approachforlearningversatileskills,”NeurIPS,vol.36,2024.
tion by leveraging AR. Importantly, we present quantitative
[16] Y. Yildirim and E. Ugur, “Conditional neural expert processes for
evaluations of RAMPA, supporting our initial hypothesis of learningmovementprimitivesfromdemonstration,”IEEERoboticsand
increased effectiveness without sacrificing user experience or AutomationLetters,2024.
[17] A.Paraschos,C.Daniel,J.Peters,andG.Neumann,“Usingprobabilistic
task precision. Thus, RAMPA demonstrates the potential of
movementprimitivesinrobotics,”AutonomousRobots,vol.42,pp.529–
leading to more accessible and efficient robotic programming 551,2018.
for both novices and experts. [18] C.Paxton,A.Hundt,F.Jonathan,K.Guerin,andG.D.Hager,“Costar:
Instructingcollaborativerobotswithbehaviortreesandvision,”inICRA,
Currently, RAMPA allows registering virtual objects while
2017,pp.564–571.
demonstrating trajectories. However, this study does not pro- [19] S. Alexandrova, M. Cakmak, K. Hsiao, and L. Takayama, “Robot
vide any evaluations of this functionality. The integration of programming by demonstration with interactive action visualizations.”
inRobotics:scienceandsystems,2014,pp.1–9.
this capability offers a more interactive and rich environment
[20] K.Kawaharazuka,T.Matsushima,A.Gambardella,J.Guo,C.Paxton,
where users can use real and/or virtual objects to create their
and A. Zeng, “Real-world robot applications of foundation models: A
own scenarios for collecting demonstrations. Also, visualiza- review,”arXivpreprintarXiv:2402.05741,2024.
tion of model-specific parameters - means and variances in [21] S.A.Green,M.Billinghurst,X.Chen,andJ.G.Chase,“Human-robot
collaboration: A literature review and augmented reality approach in
the case of ProMPs - provides an enhanced introspection tool
design,”Intl.journalofadvancedroboticsystems,vol.5,p.1,2008.
that can be used to ensure the robot’s behavior aligns with [22] M. Walker, T. Phung, T. Chakraborti, T. Williams, and D. Szafir,
the desired outcome. Even though we use the UR10 robotic “Virtual,augmented, andmixed realityforhuman-robot interaction:A
survey and virtual design element taxonomy,” ACM Transactions on
platform and ProMPs for convenience, we designed RAMPA
Human-RobotInteraction,vol.12,no.4,pp.1–39,2023.
to be platform and model-agnostic. Therefore, the system can [23] Y. Cao, T. Wang, X. Qian, P. S. Rao, M. Wadhawan, K. Huo, and
be (1) adapted to other robots as long as their URDF files are K. Ramani, “Ghostar: A time-space editor for embodied authoring of
human-robot collaborative task withaugmented reality,” in Symposium
provided and (2) extended by integrating other ML models.
onUserInterfaceSoftwareandTechnology,2019,pp.521–534.
[24] C. P. Quintero, S. Li, M. K. Pan, W. P. Chan, H. M. Van der Loos,
VI. ACKNOWLEDGEMENT and E. Croft, “Robot programming through augmented trajectories in
augmentedreality,”inIROS. IEEE,2018,pp.1838–1844.
WewouldliketothankEmreBatuhanGocandMuhammet [25] M. Walker, H. Hedayati, J. Lee, and D. Szafir, “Communicating robot
Batuhan Ilhan for their contributions to the development of motion intent with augmented reality,” in ACM/IEEE Intl. Conf. HRI,
2018,pp.316–324.
the underlying software of the RAMPA system.
[26] E.Rosen,D.Whitney,E.Phillips,G.Chien,J.Tompkin,G.Konidaris,
andS.Tellex,“Communicatingrobotarmmotionintentthroughmixed
REFERENCES realityhead-mounteddisplays,”inRoboticsresearch:The18thinterna-
tionalsymposiumISRR. Springer,2020,pp.301–316.
[1] H.Ravichandar,A.S.Polydoros,S.Chernova,andA.Billard,“Recent [27] S. P. Arunachalam, I. Gu¨zey, S. Chintala, and L. Pinto, “Holo-dex:
advances in robot learning from demonstration,” Annual review of Teaching dexterity with immersive mixed reality,” in ICRA, 2023, pp.
control,robotics,andautonomoussystems,vol.3,pp.297–330,2020. 5962–5969.
[2] B.D.Argall,S.Chernova,M.Veloso,andB.Browning,“Asurveyof [28] M. B. Luebbers, C. Brooks, C. L. Mueller, D. Szafir, and B. Hayes,
robotlearningfromdemonstration,”Roboticsandautonomoussystems, “Arc-lfd: Using augmented reality for interactive long-term robot skill
vol.57,no.5,pp.469–483,2009. maintenance via constrained learning from demonstration,” in ICRA,
[3] R. Suzuki, A. Karim, T. Xia, H. Hedayati, and N. Marquardt, “Aug- 2021,pp.3794–3800.
mented reality and robotics: A survey and taxonomy for enhanced [29] M. Diehl, A. Plopski, H. Kato, and K. Ramirez-Amaro, “Augmented
human-robotinteractionandroboticinterfaces,”inCHI,2022,pp.1–33. realityinterfacetoverifyrobotlearning,”inRO-MAN,2020,p.378.9
[30] K.Lotsaris,C.Gkournelos,N.Fousekis,N.Kousi,andS.Makris,“Ar
basedrobotprogrammingusingteachingbydemonstrationtechniques,”
ProcediaCIRP,vol.97,pp.459–463,012021.
[31] J. Sauro and J. R. Lewis, Quantifying the user experience: Practical
statistics for user research, 2nd ed. Cambridge, MA: Morgan Kauf-
mann,2016.