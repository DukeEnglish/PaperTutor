FLUID: SCALING AUTOREGRESSIVE TEXT-TO-IMAGE
GENERATIVE MODELS WITH CONTINUOUS TOKENS
LijieFan1,* TianhongLi2,† SiyangQin1,† YuanzhenLi1 ChenSun1
MichaelRubinstein1 DeqingSun1 KaimingHe2 YonglongTian1,*
1GoogleDeepMind 2MIT *equalcontribution,projectlead † equalcontribution
Figure1: SamplesfromourFluid10.5Bautoregressivemodelwithcontinuoustokens.
ABSTRACT
Scalingupautoregressivemodelsinvisionhasnotprovenasbeneficialasinlarge
languagemodels. Inthiswork,weinvestigatethisscalingprobleminthecontext
oftext-to-imagegeneration,focusingontwocriticalfactors: whethermodelsuse
discrete or continuous tokens, and whether tokens are generated in a random or
fixed raster order using BERT- or GPT-like transformer architectures. Our em-
piricalresultsshowthat,whileallmodelsscaleeffectivelyintermsofvalidation
loss,theirevaluationperformance—measuredbyFID,GenEvalscore,andvisual
quality—follows different trends. Models based on continuous tokens achieve
significantly better visual quality than those using discrete tokens. Furthermore,
the generation order and attention mechanisms significantly affect the GenEval
score: random-ordermodelsachievenotablybetterGenEvalscorescomparedto
raster-order models. Inspired by these findings, we train Fluid, a random-order
autoregressive model on continuous tokens. Fluid 10.5B model achieves a new
state-of-the-artzero-shotFIDof6.16onMS-COCO30K,and0.69overallscore
ontheGenEvalbenchmark. Wehopeourfindingsandresultswillencouragefu-
tureeffortstofurtherbridgethescalinggapbetweenvisionandlanguagemodels.
4202
tcO
71
]VC.sc[
1v36831.0142:viXraFluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
1 INTRODUCTION
Scalinglawsunderpintheunprecedentedsuccessoflargelanguagemodels(LLMs).Empirically,in-
creasingthenumberofparametersinautoregressivemodelsconsistentlyleadstosignificantperfor-
manceimprovementsandtheemergenceofnewcapabilitiesinnaturallanguageprocessing(NLP)
tasks(Devlinetal.,2018;Radfordetal.,2018;Brownetal.,2020;Kaplanetal.,2020;Weietal.,
2022). This empirical relationship has inspired numerous efforts to scale up language models, re-
sultinginthedevelopmentofmanyhighlycapablemodels(Bubecketal.,2023;Teametal.,2023;
Achiametal.,2023).
Encouraged by this success, many attempts have been made to adopt and scale up autoregressive
modelsincomputervision,particularlyforgenerativetasksliketext-to-imagegeneration(Yuetal.,
2021;2023;Baietal.,2024;El-Noubyetal.,2024). However,theperformanceandvisualquality
ofcontentgeneratedbythesemodelsoftenfallshortcomparedtoothergenerativemodels,suchas
diffusionmodels(Hoetal.,2020;Sahariaetal.,2022;Rombachetal.,2022a;Esseretal.,2024),
leavingitunclearwhethersimilarscalinglawsapplytothevisiondomain.
Weproposeseveralhypothesesfortheperformancegap.First,thevectorquantization(VQ)(vanden
Oordetal.,2017)step,whichisrequiredformostvisualautoregressivemodels,mayintroducesig-
nificant information loss, ultimately limiting model performance. Second, unlike the inherently
sequentialnatureoflanguage,generatingvisualcontentmightbenefitmorefromadifferentautore-
gressive prediction order. Third, there is often a confusion between two levels of generalizability
whenevaluatingscalinglawsinvisionmodels: (a)generalizationtonewdatausingthesamemetric
asthetrainingloss(commonlyreferredtoasvalidationloss),and(b)generalizationtoanewmetric
or problem different from the training objective, such as FID (Heusel et al., 2017), the GenEval
benchmark(Ghoshetal.,2024),orvisualquality. Wehypothesizethatpower-lawscaling(Kaplan
etal.,2020)appliesto(a)forautoregressivemodelsonvisiondata,butnotnecessarilyto(b).
Toinvestigatethesehypotheses,weconductacomprehensiveempiricalstudyonthescalingbehav-
iorofautoregressivemodelsinthecontextoftext-to-imagegeneration. Specifically,weexploretwo
key factors: whether the model operates on continuous or discrete tokens, and whether tokens are
generated in a random or fixed raster order. To this end, we utilize the Diffusion Loss (Li et al.,
2024)tomakeautoregressivemodelscompatiblewithcontinuoustokens. WegeneralizeBERT-like
visionmodelMaskGIT(Changetal.,2022)asrandom-orderautoregression,asitconceptuallypre-
dicts output tokens in a randomized order while retaining the autoregressive nature of “predicting
next tokens based on known ones”. We analyze the behavior of four autoregressive variants, each
employingdifferentcombinationsofthesetwofactors. Wescaletheirparametersfrom150Mto3B
andevaluatetheirperformanceusingthreemetrics: validationloss, FID(Heuseletal.,2017), and
GenEvalscore(Ghoshetal.,2024). Wealsoinspectthevisualqualityofthegeneratedimages.
OurexperimentsindicatethatVQ-basedmodels,regardlessofwhethertheyusearandomorfixed
rasterorder,exhibitaslower improvementinFIDscoreswhenscalingupmodelsizecomparedto
models operating on continuous tokens. VQ models also produce images of lower visual quality,
likelyduetoinformationlossintroducedbyvectorquantization.
Furthermore,thetokengenerationorderandtheassociatedattentionmechanismprimarilyinfluence
theglobalstructureofthegeneratedimage. IntheGenEvalbenchmark,random-ordermodelswith
bidirectional attention significantly outperform raster-order models with causal attention, particu-
larly when generating multiple objects. Random-order models can readjust the global structure at
everypredictionstep,whereasraster-ordermodelscannot. Thissuggeststhatthetokengeneration
orderplaysacrucialroleinachievingbettertext-to-imagealignment.
Our experiments also demonstrate that validation loss scales as a power-law with model size, no
matterwhetherthemodeloperateswithcontinuousordiscretetokens. Thisimpliesscalablebehav-
ioratthelevelof(a)generalization—generalizingtonewdatausingthesamemetricasthetraining
loss—which aligns with observations in language models (Kaplan et al., 2020). However, as for
generalizingtoadifferentmetric,suchasFIDorGenEvalscore,althoughperformanceconsistently
improveswithbettervalidationloss,thetrendmaynotfollowastrictpower-law.
Building on these findings, we scale the Fluid model, i.e., random-order model with continuous
tokens,upto10.5BparametersandtrainitusingtheWebLIdataset(Chenetal.,2022).Theresulting
Fluid10.5Bmodelachievesazero-shotFIDof6.16onMS-COCOandaGenEvaloverallscoreof
2Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
0.69,comparingfavorablywithleadingtext-to-imagegenerativemodelssuchasDALL-E3(Betker
et al., 2023) and Stable Diffusion 3 (Esser et al., 2024). We hope that our empirical findings and
positive results could shed light on the scaling behavior for text-to-image generation models and
furtherinnovationinthisfrontier.
2 RELATED WORK
Text-to-imagediffusionmodels. Thedominantapproachesarebasedondiffusionmodels. Dall-
E 2 (Ramesh et al., 2022), Imagen (Ho et al., 2022), and Stable Diffusion (Rombach et al.,
2022a) revolutionized text-to-image generation. Most recently, SD v3 (Esser et al., 2024) and
Imagen3 (Baldridge et al., 2024) can generate realistic images that are hard to tell from real im-
ages.However,generatingsamplesisusuallycomputationallyexpensiveduetothemultipleforward
passes.
Autoregressive(AR)models. Whilebeingthedefactomodelforlanguagemodeling,ARmodels
are lagging behind diffusion models for text-to-image generation. AR models (Yu et al., 2022;
Changetal.,2023;Yuetal.,2023)areoftenusedtogetherwithdiscretetokenizers(vandenOord
etal.,2017;Esseretal.,2021), whichoftenlimitthemodelingcapability. Forexample, Parti(Yu
et al., 2022) scales up the model to 20B, which obtains a slightly lower/better FID score of 7.23
onMS-COCOthan7.27bythe3.4Bdiffusion-basedImagen. Herewefindthat,withacontinuous
tokenizer,oursmall369MmodelcanalreadyachievethesameFIDscoreasPartiwith20Bmodel.
Recently Li et al. (2024) challenges the conventional wisdom and replaces the discrete tokenizer
with a continuous tokenizer via a diffusion loss. They introduced masked autoregressive (MAR)
modelobtainsstrongresultsforclass-conditioninggenerationonImageNet.However,scalingMAR
modelsfortext-to-imagegenerationisunexplored. Hereweempiricallystudythescalingbehavior
forMARandreportseveralfindingsimportantfortheresearchcommunity.
Scalinglanguagemodels. Kaplanetal.(2020)empiricallyobservedthat,forlanguagemodel,the
validationlossscalesasapower-lawwithmodelsize,datasetsize,andtheamountofcomputeused
fortraining. Hoffmannetal.(2022)discoveredthatcontemporaryLLMsareunder-trained,andthat
for compute-optimal training, the model size and the number of training tokens should be scaled
equally. Under the same compute budget while using 4x more more data, their 70B Chinchilla
outperforms much larger models, such as the 530B Megatron-Turing NLG (Smith et al., 2022).
Wei et al. (2022) found that larger models have emergent abilities that are not present in smaller
models. Theseobservationshaveinspiredsignificanteffortstoscaleuplanguagemodelstotrillions
ofparameters(Achiametal.,2023;Teametal.,2023;Dubeyetal.,2024).
Scalingvisionmodels. Similarscalinglawhasbeenobscureforcomputervision. Forrecognition
models (Tan, 2019; Zhai et al., 2022; He et al., 2022; Dehghani et al., 2023), scaling often comes
withdiminishingreturns.Forexample,Dehghanietal.(2023)scaleViTupfrom3Bto22Bbutonly
observed0.25%accuracyincreaseinImageNetlinearprobing. Thescalingofgenerativemodelsis
more promising - DiT (Peebles & Xie, 2023) shows consistent improvement in generation quality
whenscalingupcomputeandmodelsize(despiteonlyupto600M).Thefollow-upSora(OpenAI,
2024) further shows the potential to scale up for video generation. In this paper, we perform a
comprehensiveempiricalstudyofARmodelsfortext-to-imagegeneration.
3 PRELIMINARY: AUTOREGRESSIVE IMAGE GENERATION
Given a sequence of tokens {x1,x2,...,xn} where the superscript 1 ≤ i ≤ n specifies an order,
autoregressivemodels(Gregoretal.,2014;vandenOordetal.,2016b;a;Parmaretal.,2018;Chen
etal.,2018;2020)formulatethegenerationproblemas“nexttokenprediction”:
n
(cid:89)
p(x1,...,xn)= p(xi|x1,...,xi−1). (1)
i=1
Following the chain rule, the network is trained to model p(xi | x1,...,xi−1) and generate tokens
iteratively. While all autoregressive models share this fundamental approach, differences in their
3Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
known/predicted to predict at this step unknown
[s] 1 2 3 4 5 1 [m] [m] 4 [m] 6
1 2 3 4 5 1 4 6
loss loss loss loss loss loss loss loss loss
(a) Raster Order, Causal Attention (b) Random Order, Bidirectional Attention
Figure 2: Autoregressive models with different orders. (a) A raster-order autoregressive model
predictsonenexttokenbasedontheknownones,implementedusingaGPT-liketransformerwith
causal attention. (b) A random-order autoregressive model predicts one or multiple tokens simul-
taneously given a random order, implemented using a BERT-like transformer with bidirectional
attention.
designcanaffecttheperformance. Twokeydesignchoicesaretherepresentationofx,i.e.,discrete
orcontinuous,andthegenerationorder,whichweelaboratebelow.
Discrete vs. continuous tokens. The goal of an autoregressive model is to estimate
p(xi | x1,...,xi−1). Traditionally, this is done by transforming the image into a set of discrete
tokens with a finite vocabulary and then estimating a categorical distribution over the vocabulary.
The training objective is to minimize the cross-entropy loss, and sampling can be efficiently per-
formedusingcategoricalsampling. Mostautoregressiveimagegenerationmodelsrelyonthisform
oftokendiscretization(Esseretal.,2021;Changetal.,2022;Tianetal.,2024;Yuetal.,2022).
However,suchdiscretizationoftenleadstoasignificantlossofinformationfromtheimage(Figure
4). Recentwork(Lietal.,2024)hasshownthepossibilityofapplyingasmalldiffusionmodelto
approximatethedistributionofeachimagetokeninacontinuousfashion. Thisapproacheliminates
the need for vector quantization, and allows modeling images with continuous tokenizers which
yield much better reconstruction visual quality. In this paper, we explore the scaling behavior of
autoregressiveimagemodelsonbothdiscreteandcontinuoustokens.
RasterOrder+GPTvs. RandomOrder+BERT. Inautoregressiveimagegeneration,thereare
twoprimarygenerationorders: rasterandrandom. AsillustratedinFigure2,rasterordergenerates
tokenssequentiallyfromlefttoright, toptobottom. Thisfixed-ordergenerationiswell-suitedfor
aGPT-liketransformerarchitecture,whichpredictsthenexttokeninacausalmanner. Incontrast,
randomorderallowsmultipletokenstobegeneratedineachstep. Theselectionofthesetokenscan
eitherbecompletelyrandomorbasedonasamplingmechanismthatprioritizestokenswithhigher
predictedconfidencescores(Changetal.(2023);Lietal.(2024)).
Each generation order has its pros and cons. Raster order models with GPT-like transformer sup-
port fast inference via key-value (kv) caching. However, this causal structure can also introduce
performance degradation. On the other hand, random order generation is usually achieved with a
BERT-likebidirectionalattentionmechanism.Whilethisapproachpreventstheusageofkv-caching,
itenablesthemodeltodecodemultipletokensateachautoregressivestep,allowingglobalediting.
Despitetheirindividualstrengths,itremainsunclearintheliteraturewhichgenerationorderscales
better for text-to-image generation tasks. In this work, we compare the performance and scaling
behaviorsofraster-orderandrandom-orderautoregressivemodels.
4 IMPLEMENTATION
Theoverallframeworkofourtext-to-imagemodeltrainingisstraightforward. Animagetokenizer
first converts the original image into tokens. These tokens are then partially masked, and a trans-
former is trained to reconstruct the masked tokens conditioned on the text. Below, we provide a
detaileddescriptionofeachcomponentofourframework(showninFigure3).
4Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
“Mona Lisa” Transformer Blocks
Mask
T5 Text
Encoder Self Attention
Frozen Cross Attention
… Image
Trainable Text Tokenizer
Aligner
Output Head
Figure3: Ourtext-to-imagegenerationframework. Apre-trainedimagetokenizerconvertsthe
image into either discrete or continuous tokens. The text is embedded using a pre-trained T5 en-
coder,followedbyatrainabletextaligner. Thetransformerthentakescross-attentionfromthetext
embeddingstopredictthemissingtokens(onlyrandomordermodelisshownhere).
Image Tokenizer. We use a pre-trained Original Discrete Continuous
image tokenizer to encode 256×256 im-
ages into a token space. Such a tok-
enizer can be either discrete or continu-
ous, facilitating different training objec-
tives of the autoregressive model. In our
experiments, the discrete tokenizer is an
VQGAN model (Esser et al., 2021) pre-
PSNR:26.6 PSNR:31.5
trainedontheWebLIdataset(Chenetal.,
2022). We follow Muse (Chang et al., Figure 4: Reconstruction quality of the tokenizers.
2023) to encode each image into 16×16 Imageresolutionis256x256. Thediscretetokenizeris
discrete tokens with a vocabulary size of significantlyworsethanthecontinuoustokenizer.
8192. For the continuous tokenizer, we
adopt a widely-used one from Stable Diffusion (Rombach et al., 2022b), which encodes the im-
ageinto32×32continuoustokens,eachcontaining4channels. Tobeconsistentinsequencelength
withthediscretetokenizer,wefurthergroupeach2×2patchofcontinuoustokensintoasingleto-
ken,resultinginafinalsequencelengthof256,witheachtokencontaining16channels. Asshown
in Figure 4, the continuous tokenizer can achieve notably higher reconstruction quality than the
discreteone.
TextEncoder. Therawtext(maximumlengthof128)istokenizedbySentencePiece(Kudo,2018),
andembeddedthroughapre-trainedT5-XXLencoder(Raffeletal.,2020),whichhas4.7Bparam-
etersandisfrozenduringtraining. Tofurtheralignthetextembeddingsforimagegeneration, we
addasmalltextalignerconsistingofsixtrainabletransformerblocksontopoftheT5embeddings,
toextractthefinaltextrepresentation.
Transformer. After encoding the original image into a sequence of tokens, we use a standard
decoder-only transformer model (Vaswani et al., 2017) for autoregressive generation. Each block
consists of three consecutive layers – self-attention, cross-attention, and MLP. The self-attention
and MLP layers are only applied to visual tokens, while the cross attention layer takes visual and
textual tokens as queries and keys, respectively. As shown in Figure 2, for raster-order models,
thetransformerpredictsthenexttokenbasedonprevioustokensusingcausalattentionfortheself-
attentionblock,similartoGPT.Inrandom-ordermodels,unknowntokensaremaskedbyalearnable
token, and the transformer predicts these masked tokens using bidirectional attention, similar to
BERT.
Outputhead. Fordiscretetokens,wefollowthecommonpracticewithautoregressivemodels. The
outputs are transformed into categorical distributions by softmax following a linear layer, whose
weights are reused from the input embedding layer. For continuous tokens, we apply a six layer
light-weight MLP as the diffusion head (Li et al., 2024) to model the per-token distribution. The
embeddingdimensionofthisheadisthesameasthebackbonetransformer. Theper-tokendiffusion
processfollows(Nichol&Dhariwal,2021;Lietal.,2024). Thenoiseschedulehasacosineshape,
with1000stepsattrainingtime;atinferencetime,itisresampledto100steps.
5Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
5 EXPERIMENTS
Dataset. WeuseasubsetoftheWebLI(WebLanguageImage)dataset(Chenetal.,2022)asour
trainingset,whichconsistsofimage-textpairsfromthewebwithhighscoresforbothimagequality
andalt-textrelevance. Bydefault,theimagesarecenter-croppedandresizedto256×256.
Training. Unless otherwise specified, we use the AdamW optimizer (β = 0.9,β = 0.95)
1 2
(Loshchilov & Hutter, 2019) with a weight decay of 0.02 to train each model for 1M steps with
abatchsizeof2048. Thisisequivalenttoapproximately3epochsonourdataset. Forcontinuous
tokens,weemployaconstantlearningrateschedulewitha65K-steplinearwarmupandamaximum
learningrateof1×10−4;fordiscretetokens,weuseacosinelearningratescheduleaswefinditto
bebetter. Fortrainingtherandom-ordermodels,werandomlysamplethemaskingratiofrom[0,1]
followingacosineschedule,similartoMaskGIT(Changetal.,2022),tomaskeachimage. Forall
models,exponentialmovingaverageoftheweightsaregatheredbyadecayrateof0.9999andthen
usedforevaluation.
Inference. We follow the practices established by Imagen (Saharia et al., 2022), Muse (Chang
et al., 2023), and Parti (Yu et al., 2022) to generate images from text prompts without rejection
sampling. Forrandom-ordermodels,weuse64stepsforgenerationwithacosineschedule(Chang
etal.,2022). Tofurtherenhancegenerationperformance,weapplytemperatureandclassifier-free
guidance,asiscommonlypracticed.
Evaluation. We evaluate the scaling behavior of different autoregressive model variants both
quantitatively and qualitatively. Quantitatively, we evaluate the validation loss on 30K images
from the MS-COCO 2014 training set, as well as two widely-adopted metrics: zero-shot Frechet
Inception Distance (FID) on MS-COCO, and the GenEval score (Ghosh et al., 2024). Inference
hyper-parameters, such as temperature and classifier-free guidance, are optimized for each evalu-
ation metric. FID is computed over 30K randomly selected image-text pairs from the MS-COCO
2014trainingset, providingametricthatevaluatesboththefidelityanddiversityofgeneratedim-
ages. TheGenEvalbenchmark,ontheotherhand,measuresthemodel’sabilitytogenerateimages
thataccuratelyreflectthegivenprompt. Forqualitativeevaluation,wegenerateimagesfromseveral
promptsusingeachmodelandcomparethevisualqualityofthegeneratedimages.
5.1 SCALINGBEHAVIORS
Inthissection,weexplorehowtwokeydesignchoicesinautoregressiveimagegenerativemodels—
tokenrepresentationandgenerationorder—affectperformanceandscalingbehavior. Weconstruct
models with different combinations of these two design choices, resulting in four distinct variants
of autoregressive image generation models. We also explore the generalizability of these models
acrossdifferentdataandevaluationmetrics. Ourexperimentsrevealseveralintriguingproperties.
Validationlossesconsistentlyscalewithmodelsize. InFigure5,weexaminethescalingbehav-
iorofthefourautoregressivevariantsintermsofvalidationloss. Weobservealinearrelationship
betweenvalidationlossandmodelsizeinthelogspace,asweincreasesmodelsizefrom150mil-
lionto3billionparameters. Thisalignswiththepower-lawfindinginHenighanetal.(2020). This
demonstratesthattheimprovementsintraininglossresultingfromincreasedmodelsizegeneralize
welltovalidationlossondatadifferentfromthetrainingdata.
Random-order models with continuous tokens scale the best in evaluation scores. In Figure
6,weanalyzethescalingbehaviorofthefourautoregressivevariantsintermsofFIDandGenEval
overall scores. We find that the improvements observed in validation loss do not always translate
linearlytobetterevaluationmetrics,implyingthatthereisnostrictpower-lawrelationshipbetween
these metrics and model size. For example, raster-order models with discrete tokens (blue line)
reach a plateau in both FID and GenEval scores around 1B parameters. Among the four variants,
random-ordermodelswithcontinuoustokens(i.e.,Fluid)showconsistentimprovementsinevalua-
tionmetricsupto3Bparameters, achievingthebestoverallperformance. Therefore, wefocuson
furtherinvestigatingthescalingbehaviorofthismodel.
6Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
Raster Order, Discrete Random Order, Discrete Raster Order, Continuous Random Order, Continuous
6.8 6.65 0.255 0.306
6.7 6.60 0.250
0.302
6.6 6.55 0.245
0.298
6.5 6.50 0.240
6.4 6.45 0.235 0.294
0.1 0.3 1.0 3.0 0.1 0.3 1.0 3.0 0.1 0.3 1.0 3.0 0.1 0.3 1.0 3.0
Parameters (B) Parameters (B) Parameters (B) Parameters (B)
Figure5: Validationlossscalesasapower-lawwithmodelsize. Thevalidationlossisevaluated
on30KimagesrandomlysampledfromtheMS-COCO2014trainingset. Thexandyaxesarein
log-scale. Thechangeinyisrelativelysmallforeachplot,makingthelog-scalealikelinear-scale.
Raster Order, Discrete Random Order, Discrete Raster Order, Continuous Random Order, Continuous
0.70
8.5
0.65
8.0
0.60
7.5
0.55
7.0 0.50
0.45
6.5
0.1 0.3 1.0 3.0 0.1 0.3 1.0 3.0
Parameters (B) Parameters (B)
Figure 6: Random-order models using continuous tokens (orange) achieve the best perfor-
manceonevaluationmetrics. FID(lowerisbetter)isevaluatedon30Kimagesrandomlysampled
from the MS-COCO 2014 training set, while the GenEval overall score (higher is better) is as-
sessed using the 553 prompts provided by the official benchmark, with four images generated for
eachprompt. Amongallmodels,random-ordermodelsoncontinuoustokensconsistentlyshowan
improvement in evaluation metrics as model size increases and achieve the best FID and GenEval
scores.
Random-ordermodelswithcontinuoustokensscalewithtrainingcomputes. InFigure7,we
plot validation loss, FID, and GenEval scores as functions of total training steps and compute for
different model sizes of Fluid. We observe consistent improvements in both validation loss and
evaluation performance with increased training steps and compute. However, the benefits from
additionaltrainingstepssaturatearound1Msteps,indicatingthattrainingsmallermodelsformore
steps is less compute-efficient compared to training larger models for fewer steps. This behavior
aligns with observations in language models, highlighting the potential for scaling up model sizes
withsufficienttraining.
Strong correlation between validation loss and evaluation metrics. In Figure 8, we plot FID
andGenEvalscoresagainstvalidation lossfordifferentmodelsizesofFluidandobserve astrong
correlation. Toquantifythis,wefitthedatapointsusinglinearregression. ThePearsoncorrelation
coefficientsforFIDandGenEvalscoresare0.917and-0.931,respectively,indicatinganearlylinear
relationship between validation loss and these evaluation metrics across model sizes ranging from
150Mto3B1.Encouragedbythispositivetrend, wetrainedamodelwith10.5Bparametersanda
batchsizeof4096for1Msteps,achievingstate-of-the-arttext-to-imagegenerationperformance,as
discussedinthenextsection.
1SincebothFIDandGenEvalscoreshavelower/upperbounds,thislinearrelationshipcannotholdindefi-
nitely.Futureworkshouldexplorethelimitsofthiscorrelation.
7
ssoL
noitadilaV
DIF
ssoL
noitadilaV
ssoL
noitadilaV
)llarevo(
lavEneG
ssoL
noitadilaVFluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
0.2B 0.4B 0.7B 1.1B 3.1B
10
0.315 0.70
0.65
0.310 9
0.60
0.305 8
0.55
0.300
7 0.50
0.295 0.45
6
250 500 750 1000 250 500 750 1000 250 500 750 1000
Training Steps (K) Training Steps (K) Training Steps (K)
10
0.315 0.70
0.65
0.310 9
0.60
0.305 8
0.55
0.300
7 0.50
0.295 0.45
6
1020 1021 1022 1021 1022 1021 1022
Training FLOPs Training FLOPs Training FLOPs
Figure7:Validationlossesandevaluationperformancescalewithincreasingtrainingstepsand
computes. Weuserandom-ordermodelswithcontinuoustokens. Resultsforotherautoregressive
variantsareincludedintheappendix. ThetrainingcomputeiscomputedasmodelGFLOPs×batch
size×training steps×3, where the factor of 3 accounts for the backward pass being approximately
twiceascompute-intensiveastheforwardpass.
0.2B 0.4B 0.7B 1.1B 3.1B
10.0 0.75
9.5
0.70
9.0
8.5 r = 0.917 0.65 r = -0.931
8.0 0.60
7.5
0.55
7.0
6.5 0.50
6.0 0.45
5.5
0.295 0.300 0.305 0.295 0.300 0.305
Validation Loss Validation Loss
Figure 8: Validation loss and evaluation metrics are highly correlated. We use random-order
models with continuous tokens. The Pearson correlation coefficients for FID and GenEval scores
are0.917and-0.931,respectively. Wealsoobservethatthelinearcorrelationslightlyweakensand
becomeslesspronouncedforthe3.1Bmodel.
Continuoustokensandlargemodelsarecrucialforvisualquality. InFigure9,wecomparethe
visualqualityofimagesgeneratedbythefourautoregressivevariants. Thevisualqualityofmodels
using discrete tokens is significantly worse than that of models using continuous tokens, e.g., the
eyes of the corgi is asymmetric for discrete token based models and scaling up can not solve this
problem. This limitation is largely because of the discrete tokenizer, which introduces substantial
informationloss. Forinstance,evenwith3Bparameters,thediscretetokenmodelscannotgenerate
8
ssoL
noitadilaV
ssoL
noitadilaV
DIF
DIF
DIF
)llarevo(
lavEneG
)llarevo(
lavEneG
)llarevo(
lavEneGRandom Order, FClounidt:inSucoaulisn.g (FAluutidor)e g r e s s iRvaesTteerx tO-trod-eimr. aCgoenGtiennueorautsiv. e M o d Realsnwdoitmh COordnteirn.u DoiusscrTeotkee. n s Raster Order. Discrete.
0.2B 0.4B 0.7B 1.1B 3.1B 0.2B 0.4B 0.7B 1.1B 3.1B
An angry duck doing heavy weightlifting at the gym. Mona Lisa in winter
A corgi. graffiti of a panda with snow goggles snowboarding on a street wall.
A group of three teddy bears in suit in an office celebrating the A photo of a smiling person with snow goggles on
birthday of their friend. There is a pizza cake on the desk. holding a snowboard
Figure 9: Visual quality and image-text alignment improves with increasing model size. Best
•
viewedzoomed-in. Fluid achievesthehighestvisualqualityandbestimage-textalignment.
9Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
Table 1: System-level comparison. Fluid achieves leading results on both MS-COCO zero-shot
FID-30KandGenEvalbenchmark(Ghoshetal.,2024). †:CM3Leonresultisreportedwithoutretrieval.
MS-COCO GenEval
#params FID-30K↓ SingleObj. TwoObj. Counting Colors Position ColorAttri. Overall
diffusionmodel
LDM 1.4B 12.64 0.92 0.29 0.23 0.70 0.02 0.05 0.37
DALL-E2 4.2B 10.39 0.94 0.66 0.49 0.77 0.10 0.19 0.52
DALL-E3 - - 0.96 0.87 0.47 0.83 0.43 0.45 0.67
Imagen 3B 7.27 - - - - - - -
SD3 8B - 0.98 0.84 0.66 0.74 0.40 0.43 0.68
Transfusion 7.3B 6.78 - - - - - - 0.63
RAPHAEL 3B 6.61 - - - - - - -
autoregressivemodel
CM3Leon† 7B 10.82 - - - - - - -
Show-o 1.3B 9.24 0.95 0.52 0.49 0.82 0.11 0.28 0.53
Muse 3B 7.88 - - - - - - -
Parti 20B 7.23 - - - - - - -
Fluid(ourwork) 369M 7.23 0.96 0.64 0.53 0.78 0.33 0.46 0.62
665M 6.84 0.96 0.73 0.51 0.77 0.42 0.51 0.65
1.1B 6.59 0.96 0.77 0.61 0.78 0.34 0.53 0.67
3.1B 6.41 0.98 0.83 0.60 0.82 0.41 0.53 0.70
10.5B 6.16 0.96 0.83 0.63 0.80 0.39 0.51 0.69
an accurate Mona Lisa due to poor reconstruction quality of the tokenizer (Figure 4). In contrast,
modelswithcontinuoustokensproducemuchhigher-qualityimages.
Additionally, larger models show consistent improvements in both visual quality and image-text
alignment. For example, a random-order model with 0.2B parameters struggles to generate “an
angry duck doing heavy weightlifting at the gym”, while the same model with 3B parameters can
generate the corresponding images successfully. This demonstrates that modeling continuous to-
kensandincreasingmodelsizearecrucialforachievinghighvisualqualityinautoregressiveimage
generationmodels.
5.2 BENCHMARKINGWITHPREVIOUSSYSTEMS
In this section, we compare our Fluid, i.e., continuous random-order autoregressive model, with
leading text-to-image generation systems in Table 1 (Rombach et al., 2022b; Ramesh et al., 2022;
Betkeretal.,2023;Sahariaetal.,2022;Esseretal.,2024;Zhouetal.,2024;Xueetal.,2024;Yu
et al., 2023; Xie et al., 2024; Chang et al., 2023; Yu et al., 2022). The Fluid smallest model, with
369Mparameters,achievesazero-shotFIDof7.23onMS-COCOandaGenEvaloverallscoreof
0.62,matchingtheperformanceofmanystate-of-the-artmodelswithseveralbillionparameters(e.g.,
Parti with 20B parameters only achieves 7.23). The Fluid largest model, with 10.5B parameters,
furtherimprovesthezero-shotFIDonMS-COCOto6.16andincreasestheGenEvaloverallscore
to0.692,withaspeedof1.571secondsperimageperTPU(evaluatedon32TPUv5withabatchsize
of2048). Detailedmodelconfigurationsandgenerationspeedresultsareincludedintheappendix.
Wehopethesestrongresultsandpromisingscalingbehaviorprovidevaluableinsightsandsupport
forthescalabilityofautoregressivemodelsinvisualgenerativemodeling.
6 DISCUSSION
In this paper, we present an empirical study on the scaling behavior of autoregressive models for
text-to-image generation. We investigate two critical design factors: random order versus raster
order, and discrete tokens versus continuous tokens. Our results show that random-order models
withcontinuoustokensachievethebestperformanceandscalingbehavioracrossvariousevaluation
metrics and in terms of visual quality. Building on these findings, we scale up the random-order
modelwithcontinuoustokens,namelyFluid,to10.5Bparameters,andachievesstate-of-the-arttext-
to-image generation performance. We hope that our findings and promising results could provide
valuableinsightsintothescalingbehaviorofautoregressivemodelsforimagegenerationandhelp
bridgethegapbetweenthescalingperformanceofvisionmodelsandlanguagemodels.
2WeobservethattheGenEvalscoresplateauforthe10.5BFluidcomparedtothe3.1BFluid;however,it
continuestoshowconsistentimprovementsinvisualqualityandFID.
10Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
Reproducibility Statement. To aid reproducibility, we have provided the implementation details
of our framework in Section 4, training hyper-parameters in Section 5, and model configurations
in the Appendix. For the diffusion loss used for continuous tokens, we have strictly followed the
open-sourcedcodeofLietal.(2024).
Acknowledgements. We would like to express our gratitude to Amy Shen and Alex Rizkowsky
fortheirassistanceinsecuringcomputationalresources,andtoYashKatariya,IvyZheng,andRoy
FrostigfortheirvaluableinsightsonJAX-relatedquestions.WealsothankMatanCohenforhissup-
portwithprecomputedT5embeddings. OurappreciationextendstoHanZhang, JasonBaldridge,
Dilip Krishnan, David Salesin and VisCam team for their helpful discussions and valuable feed-
back.
REFERENCES
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technical
report. arXivpreprintarXiv:2303.08774,2023.
YutongBai,XinyangGeng,KarttikeyaMangalam,AmirBar,AlanLYuille,TrevorDarrell,Jitendra
Malik,andAlexeiAEfros.Sequentialmodelingenablesscalablelearningforlargevisionmodels.
InCVPR,2024.
Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan,
YichangChen,SanderDieleman,YuqingDu,ZachEaton-Rosen,etal. Imagen3. arXivpreprint
arXiv:2408.07009,2024.
JamesBetker,GabrielGoh,LiJing,TimBrooks,JianfengWang,LinjieLi,LongOuyang,Juntang
Zhuang,JoyceLee,YufeiGuo,etal. Improvingimagegenerationwithbettercaptions. OpenAI,
2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,
JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,
BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,
andDarioAmodei. Languagemodelsarefew-shotlearners. InNeurIPS,2020.
Se´bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-
mar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general
intelligence: Earlyexperimentswithgpt-4. arXivpreprintarXiv:2303.12712,2023.
HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamTFreeman. MaskGIT:Maskedgenera-
tiveimageTransformer. InCVPR,2022.
Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan
Yang,KevinMurphy,WilliamTFreeman,MichaelRubinstein,YuanzhenLi,andDilipKrishnan.
Muse: Text-to-imagegenerationviamaskedgenerativeTransformers. InICML,2023.
MarkChen,AlecRadford,RewonChild,JeffreyWu,HeewooJun,DavidLuan,andIlyaSutskever.
Generativepretrainingfrompixels. InICML,2020.
XiChen, NikhilMishra, MostafaRohaninejad, andPieterAbbeel. PixelSNAIL:Animprovedau-
toregressivegenerativemodel. InICML,2018.
XiChen,XiaoWang,SoravitChangpinyo,AJPiergiovanni,PiotrPadlewski,DanielSalz,Sebastian
Goodman,AdamGrycner,BasilMustafa,LucasBeyer,etal. Pali: Ajointly-scaledmultilingual
language-imagemodel. arXivpreprintarXiv:2209.06794,2022.
MostafaDehghani,JosipDjolonga,BasilMustafa,PiotrPadlewski,JonathanHeek,JustinGilmer,
Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling
visiontransformersto22billionparameters. InICML,2023.
11Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In
NeurIPS,2021.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels.
arXivpreprintarXiv:2407.21783,2024.
Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev,
Vaishaal Shankar, Joshua M Susskind, and Armand Joulin. Scalable pre-training of large au-
toregressiveimagemodels. arXivpreprintarXiv:2401.08541,2024.
PatrickEsser,RobinRombach,andBjornOmmer. TamingTransformersforhigh-resolutionimage
synthesis. InCVPR,2021.
PatrickEsser, SumithKulal, AndreasBlattmann, RahimEntezari, JonasMu¨ller, HarrySaini, Yam
Levi,DominikLorenz,AxelSauer,FredericBoesel,etal. Scalingrectifiedflowtransformersfor
high-resolutionimagesynthesis. InICML,2024.
DhrubaGhosh,HannanehHajishirzi,andLudwigSchmidt. Geneval:Anobject-focusedframework
forevaluatingtext-to-imagealignment. InNeurIPS,2024.
KarolGregor,IvoDanihelka,AndriyMnih,CharlesBlundell,andDaanWierstra. Deepautoregres-
sivenetworks. InICML,2014.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dolla´r, and Ross Girshick. Masked
autoencodersarescalablevisionlearners. InCVPR,2022.
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo
Jun,TomBBrown,PrafullaDhariwal,ScottGray,etal.Scalinglawsforautoregressivegenerative
modeling. arXivpreprintarXiv:2010.14701,2020.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. InNeurIPS,
2017.
JonathanHoandTimSalimans. Classifier-freediffusionguidance. arXiv:2207.12598,2022.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InNeurIPS,
2020.
JonathanHo,WilliamChan,ChitwanSaharia,JayWhang,RuiqiGao,AlexeyGritsenko,DiederikP
Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition
videogenerationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.Train-
ingcompute-optimallargelanguagemodels. arXivpreprintarXiv:2203.15556,2022.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXivpreprintarXiv:2001.08361,2020.
TKudo. Sentencepiece:Asimpleandlanguageindependentsubwordtokenizeranddetokenizerfor
neuraltextprocessing. arXivpreprintarXiv:1808.06226,2018.
Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image
generationwithoutvectorquantization. arXivpreprintarXiv:2406.11838,2024.
IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InICLR,2019.
AlexanderQuinnNicholandPrafullaDhariwal. Improveddenoisingdiffusionprobabilisticmodels.
InICML,2021.
12Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
OpenAI. Sora: Creatingvideofromtext. https://openai.com/sora,2024.
NikiParmar,AshishVaswani,JakobUszkoreit,LukaszKaiser,NoamShazeer,AlexanderKu,and
DustinTran. ImageTransformer. InICML,2018.
WilliamPeeblesandSainingXie. ScalablediffusionmodelswithTransformers. InICCV,2023.
AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever. Improvinglanguageunder-
standingbygenerativepre-training. TechnicalReport,2018.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. JMLR,2020.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditionalimagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,2022.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,2022a.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,2022b.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistic
text-to-imagediffusionmodelswithdeeplanguageunderstanding. NeurIPS,2022.
ShadenSmith, MostofaPatwary, BrandonNorick, PatrickLeGresley, SamyamRajbhandari, Jared
Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deep-
speedandmegatrontotrainmegatron-turingnlg530b,alarge-scalegenerativelanguagemodel.
arXivpreprintarXiv:2201.11990,2022.
Mingxing Tan. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv
preprintarXiv:1905.11946,2019.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
KeyuTian,YiJiang,ZehuanYuan,BingyuePeng,andLiweiWang.Visualautoregressivemodeling:
Scalableimagegenerationvianext-scaleprediction. arXivpreprintarXiv:2404.02905,2024.
Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, and Koray
Kavukcuoglu. ConditionalimagegenerationwithPixelCNNdecoders. InNeurIPS,2016a.
AaronvandenOord,NalKalchbrenner,andKorayKavukcuoglu. Pixelrecurrentneuralnetworks.
InICML,2016b.
AaronvandenOord,OriolVinyals,andKorayKavukcuoglu. Neuraldiscreterepresentationlearn-
ing. arXivpreprintarXiv:1711.00937,2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InNeurIPS,2017.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-
gatama,MaartenBosma,DennyZhou,DonaldMetzler,etal.Emergentabilitiesoflargelanguage
models. arXivpreprintarXiv:2206.07682,2022.
Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin,
YuchaoGu,ZhijieChen,ZhenhengYang,andMikeZhengShou.Show-o:Onesingletransformer
tounifymultimodalunderstandingandgeneration. arXivpreprintarXiv:2408.12528,2024.
Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo.
Raphael: Text-to-imagegenerationvialargemixtureofdiffusionpaths. NeurIPS,2024.
13Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
JiahuiYu,XinLi,JingYuKoh,HanZhang,RuomingPang,JamesQin,AlexanderKu,Yuanzhong
Xu,JasonBaldridge,andYonghuiWu. Vector-quantizedimagemodelingwithimprovedvqgan.
arXivpreprintarXiv:2110.04627,2021.
JiahuiYu,YuanzhongXu,JingYuKoh,ThangLuong,GunjanBaid,ZiruiWang,VijayVasudevan,
Alexander Ku, et al. Scaling autoregressive models for content-rich text-to-image generation.
arXivpreprintarXiv:2206.10789,2022.
LiliYu,BowenShi,RamakanthPasunuru,BenjaminMuller,OlgaGolovneva,TianluWang,Arun
Babu,BinhTang,BrianKarrer,ShellySheynin,etal.Scalingautoregressivemulti-modalmodels:
Pretrainingandinstructiontuning. arXivpreprintarXiv:2309.02591,2023.
XiaohuaZhai,AlexanderKolesnikov,NeilHoulsby,andLucasBeyer. Scalingvisiontransformers.
InCVPR,2022.
ChuntingZhou,LiliYu,ArunBabu,KushalTirumala,MichihiroYasunaga,LeonidShamis,Jacob
Kahn, XuezheMa, LukeZettlemoyer, andOmerLevy. Transfusion: Predictthenexttokenand
diffuseimageswithonemulti-modalmodel. arXivpreprintarXiv:2408.11039,2024.
14Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
0.2B 0.3B 0.6B 1.0B 3.0B
9.5 9.5
6.9
9.0 9.0
6.8
8.5 8.5
6.7
8.0 8.0
6.6
7.5 7.5
6.5
7.0 7.0
6.4
6.5 6.5
1020 1021 1022 1020 1021 1022 6.4 6.6 6.8
Training FLOPs Training FLOPs Validation Loss
Figure 10: Validation loss and FID w.r.t. training FLOPs for raster-order models with discrete
tokens.
0.2B 0.3B 0.6B 1.0B 3.0B
9.5 9.5
6.80
6.75 9.0 9.0
6.70
8.5 8.5
6.65
8.0 8.0
6.60
6.55 7.5 7.5
6.50 7.0 7.0
6.45
6.5 6.5
1020 1021 1022 1020 1021 1022 6.5 6.6 6.7
Training FLOPs Training FLOPs Validation Loss
Figure 11: Validation loss and FID w.r.t. training FLOPs for random-order models with discrete
tokens.
0.2B 0.4B 0.7B 1.1B 3.1B
9.5 9.5
0.265
0.260 9.0 9.0
0.255 8.5 8.5
0.250 8.0 8.0
0.245 7.5 7.5
0.240
7.0 7.0
0.235
6.5 6.5
1020 1021 1022 1021 1022 0.24 0.25 0.26
Training FLOPs Training FLOPs Validation Loss
Figure 12: Validation loss and FID w.r.t. training FLOPs for raster-order models with continuous
tokens.
Scalingbehaviorw.r.ttrainingFLOPs. InFigure10–12,wepresenttherelationshipbetween
validation loss and FID with respect to training FLOPs for the other three autoregressive variants.
Asshown,allthreevariantsexhibitconsistentscalingbehaviorinvalidationloss,buttheFIDgains
starttoleveloffforthe3Braster-ordermodelthatusesdiscretetokens. Thishintsthatsimplyusing
a GPT-like language model for images in a straightforward way may not scale well. To improve
scalingforvisualdata,furtheradaptationssuchascontinuoustokensandrandom-ordergeneration
areneeded.
Fine-grainedGenEvalscores. InFigure13,wepresenttheperformanceofthefourautoregres-
sive variants across all metrics in the GenEval benchmark. As shown, all models perform well in
single-objectscenarios. Theperformanceonothermetricsalsoconsistentlyimprovesasmodelsize
15
ssoL
noitadilaV
ssoL
noitadilaV
ssoL
noitadilaV
DIF
DIF
DIF DIF
DIF
DIFFluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
Raster Order, Discrete Random Order, Discrete Raster Order, Continuous Random Order, Continuous
0.98
0.8
0.96
0.7
0.94
0.6 0.92
0.5
0.90
0.88 0.4
0.1 0.3 1.0 3.0 0.1 0.3 1.0 3.0
Parameters (B) Parameters (B)
0.60 0.82
0.55 0.80
0.50 0.78
0.45 0.76
0.40 0.74
0.35 0.72
0.70
0.30
0.68
0.25
0.1 0.3 1.0 3.0 0.1 0.3 1.0 3.0
Parameters (B) Parameters (B)
0.55
0.40
0.50
0.35
0.45
0.30
0.40
0.25 0.35
0.20 0.30
0.25
0.15
0.1 0.3 1.0 3.0 0.1 0.3 1.0 3.0
Parameters (B) Parameters (B)
Figure 13: Validation loss and FID w.r.t. training FLOPs for raster-order models with discrete
tokens.
increases. Additionally,weobservethatrandom-ordermodelssignificantlyoutperformraster-order
modelsinmetricsrelatedtocountingandposition,bothofwhichrequireabetterglobalgeneration
structure—anareawhererandom-ordermodelshaveanadvantage.
Model Configurations. In Table 2, we Table 2: Model configurations of our random-order
provide the detailed configurations of our modelsoncontinuoustokens.
models across different sizes. The MLP #Params #Blocks #Channels #Heads Speed(sec/img)
ratioisfixedat4forallmodels. Thetext
166M 12 768 12 0.047
aligner consistently consists of 6 trans- 369M 16 1024 16 0.078
former blocks, with the same channel 665M 20 1280 16 0.110
size as the image transformer. The Dif- 1.1B 24 1536 16 0.180
fLoss MLP also contains 6 MLP layers, 3.1B 32 2304 24 0.483
with channels matching those of the im- 10.5B 34 4096 64 1.571
age transformer. The generation speed is
evaluatedon32TPUv5withabatchsizeof2048, andwereportthetimeneededtogenerateone
imageperTPU.
16
tcejbO
elgniS
lavEneG
gnitnuoC
lavEneG
noitisoP
lavEneG
sroloC
lavEneG
noitubirttA
roloC
lavEneG
stcejbO
owT
lavEneGFluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
A photo of a cat playing chess. A bird made of crystal A pair of old boots covered in Photo of a bear catching salmon.
mud.
High quality, a close up photo of A vintage typewriter with paper A white horse reading a book, A window with raindrops trickling
a human hand spewing out like a waterfall. fairytale. down, overlooking a blurry city.
A photo of a Shiba Inu dog with a A close-up photo of a bright red Hot air balloons and flowers, an astronaut rides a pig through
backpack riding a bike. It is rose, petals scattered with some collage art, photorealism, muted in the forest. next to a river, with
wearing sunglasses and a beach water droplets, crystal clear. colors, 3D shading beautiful clouds in the sky
hat. eldritch, mixed media, vaporous
An otherworldly forest of giant A close-up photo of a baby sloth A still life of a vase overflowing A tranquil scene of a Japanese
glowing mushrooms under a holding a treasure chest. A warm, with vibrant flowers, painted in garden with a koi pond, painted
vibrant night sky filled with distant golden light emanates from within bold colors and textured in delicate brushstrokes and a
planets and stars, creating a the chest, casting a soft glow on the brushstrokes, reminiscent of van harmonious blend of warm and
dreamlike, cosmic landscape sloth's fur and the surrounding Gogh's iconic style. cool colors.
rainforest foliage.
Figure14: Additionalimagesgeneratedfromour10.5BFluidmodel.
17Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
A golden hourglass, half-filled A space explorer discovering an A gorgeous mountain landscape A medieval knight standing on a cliff
with flowing silver sand, placed alien jungle planet under a purple at sunset. Masterful painting by overlooking a vast battlefield.
on a rich velvet cloth. sky. Rembrandt
An image of a chrome sphere A post-apocalyptic city overtaken A cloud dragon flying over A moonlit beach with glowing
reflecting a vibrant city skyline at by vines, with a robot tending a mountains, its body swirling with seashells and two moons
sunset. rooftop garden. the wind reflected on the water.
An enchanted garden where
A cozy cabin in the middle of a A dark forest under a full moon, A mountain village built into the
every plant glows softly, and
snowy forest, surrounded by tall with twisted, gnarled trees, cliffs of a canyon, where bridges
creatures made of light and
trees with lights glowing through shadows lurking behind every connect houses carved into rock,
shadow flit between the trees,
the windows, a northern lights branch, and a lone figure holding and waterfalls flow down into the
with a waterfall flowing in the
display visible in the sky. a glowing lantern. valley below.
background
An ancient, overgrown temple A steampunk airship soaring over A pristine white teapot with A sleek modern bicycle, leaning
hidden deep within a jungle, with a desert landscape, with delicate blue floral designs, against a cobblestone alley, with
vines crawling up its stone walls, mechanical wings and gears steaming with hot tea, sitting on a shiny chrome details and a deep
and golden light filtering through turning, casting shadows on the round wooden table. red paint job.
the thick canopy above. sand dunes below
Figure15: Additionalimagesgeneratedfromour10.5BFluidmodel.
183.1B Fluid: ScalingAutoregress1iv0e.T5eBxt-to-imageGenerativeMode l s 3w.i1thBContinuousTokens 10.5B
a clock on a desk, cartoon style Mona Lisa wearing headphone
an egg and a bird made of wheat bread a photo of a bear next to a STOP sign
Cute small dog sitting in a movie theater eating dark high contrast render of a psychedelic tree of
popcorn watching a movie life illuminating dust in a mystical cave
Astronaut riding a horse in a forest, low poly Metal statue of a rabbit detective standing under a
street light near a brick lined street on a rainy night.
Bokeh
Figure 16: Additional qualitative comparisons between images generated from 3.1B and 10.5B
Fluidmodel.
193.1B Fluid: ScalingAutoregress1iv0e.T5eBxt-to-imageGenerativeMode l s 3w.i1thBContinuousTokens 10.5B
Downtown New York City at sunrise. detailed ink wash medieval duck that lives in a village, 4k digital art
A photo of a confused racoon in computer an eggplant and an avocado, stuffed toys next to each
programming class other
A building made up of flowers, in the middle of the A boat made up of crystals, in the middle of the empty
empty sand, pink and purple, dreamy, foggy, sand, cream and orange, dreamy, foggy, photograph
photograph
Temple in ruins, epic, forest, stairs, columns, cinematic, A futuristic street train a rainy street at night in an old
detailed, atmospheric, epic, concept art, matte European city. Painting by David Friedrich, Claude
painting, background, mist, photo-realistic, concept Monet and John Tenniel
art, volumetric light, cinematic epic, 8k
Figure 17: Additional qualitative comparisons between images generated from 3.1B and 10.5B
Fluidmodel.
20Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
AblationstudyonTextAligner. Table3presentsapilotstudyon Table3:Ablationonthenum-
theeffectofthetrainabletextaligner. Wetrainedthesmallermodel beroflayersinthetextaligner.
with277Mparametersusingrandomordercontinuoustokenswitha #layers FID
T5-XLtextencoderfor500ksteps. TheFIDscoreshowsconsistent 0 9.38
improvementsasmorelayersareaddedtothetextaligner.Wechoose 3 8.61
touse6layersinourFluidmodelstobalanceperformanceandeffi- 6 8.42
ciency.
Visualization Details of Figure 1. To generate Figure 1, we first used our 10.5B random-order
model with continuous tokens to generate 256×256 images conditioned on the text prompts. We
thenappliedanin-housesuper-resolutionmodeltoupscaletheimagesto1024×1024forimproved
visual quality (only for Figure 1). The prompts used, from left to right and top to bottom, are:
“Close up photo of a knight”, “A baseball bat”, “An origami bird made of paper is perched on a
branchofanevergreentree”,“Awiseoldmushroomwearingspectaclesandreadingabookunder
atree”,“photoofaneaglewithagoldencrownrestinguponitshead”,“Abeautifulcastlebesidea
waterfallinthewoodsbyJosefThoma,mattepainting,trendingonartstationHQ”,“Aphotorealistic
image of a beautiful mountain range reflected perfectly on the still surface of a crystal-clear lake,
surroundedbylushgreenmeadowsandvibrantwildflowers”,“Oilpaintingofavibrantlandscapeof
rollinghillscoveredinwildflowers,withaquaintfarmhousenestledinthedistanceunderabright
blue sky”, “Mona Lisa on a cake”, “A grumpy-looking lemon wearing a raincoat and holding an
umbrella, standing in the rain”, “A hyperrealistic close-up of an eye, with the iris reflecting a vast
anddetailedlandscape,completewithmountains,rivers,andforests”,“AsectionoftheGreatWall
inthemountains,detailedcharcoalsketch”,“ApresentwithablueribbonunderaChristmastree”.
AdditionalQualitativeResults. InFigure14and15weshowadditionalimagesgeneratedfrom
our10.5BFluidmodel. Wealsoshowmorequalitativecomparisonsbetweentheimagesgenerated
by the 3.1B and 10.5B Fluidmodel in Figure 16 and 17. While the 3.1B model performs well in
mostcases, the10.5Bmodeldemonstratesbetterabilityingeneratingtextandfinerimagedetails,
andcreatingimagesthatbetteralignwiththecorrespondingtexts.
FailureCases. Figure18illustratesfailurecasesforrasterordergenerationusingcontinuousto-
kens. The model occasionally generates gray tokens in the lower part of the images, and once it
begins, itkeepsgeneratinggraytokensandrarelyrecovers. Thisresultsinincompletecontentsin
thegeneratedimages. Wesuspectthisissueisbecausethelearnedpositionalembeddingforraster
order generation struggles to capture the discontinuity between the end token of one line and the
starttokenofthenext. Thiscouldpotentiallybeaddressedbyusing2Dpositionalembeddings.
A photo of a cat A photo of an apple A photo of a dog A photo of a bottle
A photo of a bird A photo of a bear A photo of a sheep a photo of a yellow bicycle
and a red motorcycle
Figure18: Failurecasesforrasterordergenerationwithcontinuoustokens.
21Fluid: ScalingAutoregressiveText-to-imageGenerativeModelswithContinuousTokens
Figure19showsafailurecaseforrandomordergenerationwithcontinuoustokens,i.e.Fluid,where
themodelproducesabnormalbrightspots. Thisrarelyhappens,andthisissuecanbeaddressedby
increasingthediffusionsteps,e.g.,from100to200.
Diffusion Steps 100 Diffusion Steps 200 Diffusion Steps 100 Diffusion Steps 200
a photo of an orange tennis racket and a a photo of a train below an airplane
yellow sports ball
a photo of a suitcase left of a banana a photo of a couch below a vase
Figure19: Failurecasesforrandomordergenerationwithcontinuoustokens. Inveryrarecases,an
abnormalbrightspotscanovershadowothertokens. Increasingthediffusionstepssolvesthisissue.
CFG and Temperature. To enable CFG (Ho & Salimans, 2022), we randomly replace the text
conditionwithadummyvacanttextstringfor10%ofthesamples. Duringinference,themodelis
runwiththegiventextconditionandthevacanttext,yieldingtwooutputsforeachtoken.
For discrete tokens, the two outputs are conditional logit ℓ and unconditional logit ℓ . Then the
c u
finallogitℓ withaguidancescaleofω isℓ = (1+ω)·ℓ −ω·ℓ . Afterwards,thefinallogitis
g g c u
dividedbythetemperatureτ,whichcontrolsthediversityofthesamples.
Forcontinuoustokens,theyareconditionalvectorz andunconditionalvectorz forthediffusion
c u
loss head. The predicted noise ϵ is then extrapolated as: ϵ = ϵ (x |t,z ) + ω · (ϵ (x |t,z ) −
θ t u θ t c
ϵ (x |t,z )),whereωistheguidancescale. Tocontrolsamplediversityviatemperatureτ, Dhari-
θ t u
wal&Nichol(2021)suggeststoeitherdivideϵ byτ orscalethenoisewithτ. WefollowMAR(Li
θ
etal.,2024)toadoptthelatteroption.
Table4: Optimalguidancescaleωandtemperatureτ forFID.
Modelvariants ω τ
randomorder,continuoustoken 5 0.975
rasterorder,continuoustoken 4.5 0.975
randomorder,discretetoken 1.6 1.05
rasterorder,discretetoken 2.5 0.95
Weconductasweepovertheguidancescaleωandtemperatureτ todeterminetheoptimalcombina-
tionforeachmodelvariant. Thissweepisperformedformodelswith160Mand360Mparameters,
and we find that the optimal parameters remain consistent across these two scales. Therefore, we
applythesameparameters,asshowninTable4,formodelswithsizesupto3Bparameters.
22