Discrete distributions are learnable from metastable samples
Abhijith Jayakumar,∗ Andrey Y. Lokhov, Sidhant Misra, and Marc Vuffray
Theoretical Division, Los Alamos National Laboratory, Los Alamos, NM 87545, USA
Markovchainsamplersdesignedtosamplefrommulti-variabledistributionsoftenundesirablyget
stuckinspecificregionsoftheirstatespace. Thiscausessuchsamplerstoapproximatelysamplefrom
ametastabledistributionwhichisusuallyquitedifferentfromthedesired,stationarydistributionof
thechain.Weshowthatsingle-variableconditionalsofmetastabledistributionsofreversibleMarkov
chain samplers that satisfy a strong metastability condition are on average very close to those of
the true distribution. This holds even when the metastable distribution is far away from the true
model in terms of global metrics like Kullback–Leibler divergence or total variation distance. This
property allows us to learn the true model using a conditional likelihood based estimator, even
when the samples come from a metastable distribution concentrated in a small region of the state
space. Explicit examples of such metastable states can be constructed from regions that effectively
bottleneck the probability flow and cause poor mixing of the Markov chain. For specific cases of
binary pairwise undirected graphical models, we extend our results to further rigorously show that
datacomingfrommetastablestatescanbeusedtolearntheparametersoftheenergyfunctionand
recover the structure of the model.
I. INTRODUCTION Likelihood approach explicitly attempts to minimize the
KL divergence. Due to the large KL divergence be-
Markov chains are by far the most popular tool used tween the metastable and stationary distributions, the
to study systems described by many-variable probability estimator will never converge to the true distribution
distributions. It is well known that such samplers can even if a very large amount of data is collected from the
mixveryslowlytotheequilibriumdistributionandthisis metastable state.
oftenassociatedwiththeexistenceofso-calledmetastable However for the purposes of learning, it is not al-
states where the chain can get stuck for long periods of ways necessary or useful to minimize such global met-
time [6, 19, 26, 32]. Slow mixing in such systems is not rics. Forexample,considertheproblemoflearningundi-
merely an algorithmic inconvenience but is a pervasive rected graphical models. Efficient algorithms that are
property observed in natural systems from molecular bi- used to learn such models like Pseudo-Likelihood (PL)
ology to quantum field theory [8, 11, 22, 44]. Many re- [34,42,51],InteractionScreening[48,49]andSparsitron
centworksthatgiverigorousguaranteesonlearningsuch [27], work by learning the single variable conditionals of
distributions work under the assumption that indepen- thisdistribution(i.e.,theconditionaldistributionsofone
dent and identically distributed (i.i.d) samples are avail- variable where other variables are fixed). These works
able from the ground truth distribution [7, 10, 47, 50]. exploit the fact that there is a bijective mapping be-
This assumption is well justified in contexts where data tween positive distributions over discrete variables and
is collected from independent agents in a real-world set- their single variable conditionals. It might seem then
ting. However, it is less likely to hold in cases where that samples from the true distribution are necessary to
the source of the data is a natural dynamical system or even approximately learn the conditionals. In this work,
Markov chain sampling algorithms, due to slow mixing we will establish that this is not the case. That is, there
often exhibited by such systems [3, 31, 36]. These ob- exist other distributions that can be globally different
servations raise the following questions: is it possible to fromthe truedistributionbut haveconditionals thatare
learn something useful about the equilibrium/stationary on average very close to the ground-truth single variable
distributionofaMarkovchaingivensamplesdrawnfrom conditionals. We will explicitly show that metastable
such a metastable state of the chain? distributions of reversible Markov chains that have the
Prima facie, this task looks hopeless. A chain exhibit- true model as its equilibrium distribution satisfy such a
ing metastability often samples from a restricted part of property.
a state space. This in turn implies that metastable dis-
tributions will be quite different from the stationary dis-
tribution when their difference is measured using global A. Prior works and theoretical motivations
metrics like Kullback–Leibler (KL) divergence or total
variation (TV) distance. This can have severe conse-
The main theoretical motivation for studying learn-
quences for learning using certain algorithms that at-
ing from metastable distributions comes from the ob-
tempt to minimize global metrics between the data dis-
servation that many instances of multi-variable discrete
tribution and a hypothesis. For instance, the Maximum
distributions are believed to be hard to sample from
[3, 46]. This necessarily leads to poor mixing of Markov
chain samplers, which is usually observed in the low-
∗ abhijithj@lanl.gov temperature region, when the variables in the model in-
4202
tcO
71
]LM.tats[
1v00831.0142:viXra2
teractstrongly[32,37]. Thispromptsustolookforlearn- give learning guarantees. This leads to the discovery of
ing algorithms that can correctly learn the energy func- elegantconnectionsbetweenseveralideasfromstatistical
tionoftheundirectedgraphicalmodeleveninsuchcases. physics and learning theory.
The theory of learning undirected graphical models We begin in Section II by defining two notions of
with discrete variables in the setting where i.i.d samples metastability by relaxing respectively the stationarity
are given from the true distribution is well developed and detailed balance conditions that are obeyed by the
[4, 20, 27, 34, 48, 49, 51]. Recent works have established equilibrium distribution of a reversible Markov chain
that methods that learn the parameters of the model by (P). The more general definition of metastability corre-
minimizing metrics based on single variable condition- sponds to distributions that only change by η in the to-
als achieve efficient sample and computational complex- talvariationdistance(TV)whenupdatedbytheMarkov
ity scaling. Some works have also demonstrated efficient chain. It follows that when η = 0 this condition is sat-
learning in the setting where samples are given as time isfied only by the equilibrium distribution. This defi-
seriesfromaMarkovchainsampler[5,13,15,16]. Notice nition of metastability captures the intuitive notion of a
thatthissettingismarkedlydifferentfromlearningfrom metastabledistributionasonethatdoesnotchangemuch
metastable distributions. In our case, we do not observe with time. We then define a second notion of metasta-
any dynamical information from the Markov chain. In- bilitywhichwecancallη-strong metastability. Theseare
stead, weassumethatthesampleswehavearei.i.dfrom defined as distributions that violate the detailed balance
a metastable distribution of the Markov chain conditionofP byη.Heretheviolationisagainmeasured
AnalternativeperspectiveonMarkovchainsandlocal inTV.Justlikehowtheexactdetailedbalancecondition
learningmethodshasbeenexploredinsomerecentworks ofadistributionwithrespecttoP impliesthatitisasta-
in the context of learning energy based models with con- tionary state of P, the strong notion of metastabilty can
tinuous variables [29, 41]. These works show that such be trivially shown to imply the notion of metastability.
models that do not bottleneck the probability flow are To foreshadow the learning results, in the context of bi-
efficientlylearnablebyalocallearningmethodknownas nary graphical models, we will later show that samples
scorematching. Inourwork, inthediscretevariableset- from a strongly metastable distribution will allow us to
ting, weshowthatthepresenceoflargebottlenecksdoes learn a graphical model for the equilibrium distribution.
not impede learning. After introducing these two definitions of metastabil-
Metastability in the context multi-variable discrete ity, we briefly explore the connections between these two
systems has been well explored in the statistical physics definitions. The obvious question that we explore is
literature[19,21,45]duetotheconnectionbetweensuch whethertheη−strongmetastabilityimpliesη′−metasta-
distributions and the emergence of different thermody- bility, with either η′ = η or with a small difference be-
namic phases in models of interacting systems. A rigor- tweenthem. Weanswerthisquestioninthenegativeand
ousexaminationofthisphenomenonhasalsobeenunder- demonstrate significant separations between these two
taken by some authors [32, 46] who show specific exam- notionsofmetastability. Weshowthat,intheworstcase,
plesofregionswithinwhichstochasticdynamicscanmix the ratio between these two quantities can even scale as
well but it struggles to escape the said region effectively. the size of the state space. These general separations
We will use the Curie-Weiss model [32] to numerically are constructed using the well-known mapping between
demonstrate learnability from Glauber dynamics stuck reversible Markov chains and electrical networks. This
in a metastable state. also leads to an interpretation of metastability in terms
of current flows which might be of independent interest.
The question remains whether there are interesting or
relevant cases of strong metastable distributions. We
B. Extended summary of results
show two explicit constructions of strongly metastable
distributions that show these states do exist and that
First, we informally state the primary result of our
theyalignwiththeintuitivenotionofmetastablesystem
work:
as one that is stuck in a region of state space, or as a
Let µ(σ) be a distribution over a set of discrete variables
system trapped in local minima of free energy [1, 40]
and let P be a Markov chain that has µ as its equi-
librium distribution; there exists a class of metastable • Construction 1: Strong metastable states for re-
distributions of P such that given i.i.d samples from versible Markov chains. For general reversible
these distributions the stationary distribution µ can be Markovchainsweconstructη−stronglymetastable
learned to near optimality using the Pseudo-Likelihood statesbyusingthenotionofconductance fromthe-
method. Moreover, these metastable distributions closely ory of Markov chains. We show that any subset
match the intuitive picture of a distribution “stuck” in a in the state space supports a η-strong metastable
region of state space. state with η proportional to the conductance of
the subset. From the known results that connect
The rest of the paper is dedicated to defining conductance and mixing time of chains, this shows
metastabledistributions,establishingthesedefinitionsby that there exists η−strongly metastable state with
examples, and then using the notion of metastability to η being exponentially small in the number of vari-3
y
g
re
n
e
e
e
r
f
𝜇 𝜎 ≠𝜈(𝜎)
𝔼 𝜇 𝜎 𝜎 −𝜈 𝜎 𝜎 ≈0
𝜈(𝜎) 𝑢 \𝑢 𝑢 \𝑢
𝜎~𝜈
𝜇(𝜎)
configurations
FIG. 1: An informal representation of our result given by Theorem 1. Samples coming from a metastable
distribution of reversible Markov chain samplers are far from the full measure in global metrics. Suprisingly, at the
same time we show that single-variable conditionals in metastable distributions are on average close to those of the
true distribution. In Curie-Weiss model, we will use an an explicit construction to demonstrate that such metastable
states correspond to the local minima of the free energy, which agrees with an intuitive statistical physics picture of
metastability.
ables in the system. Moreover, the explicit con- als of the true distribution µ and the probability of a
structionswehavecorrespondtodistributionsthat single-spinflipintheMarkovchainP. Thisisamildcon-
have mixed well inside a set but have no support dition as reversible Markov chain samplers like Glauber
outside of it. Essentially being stuck in such a set. dynamics or Metropolis-Hastings samplers satisfy this
condition with the ratio going as Θ(1/n), which is suffi-
• Construction2: StrongmetastablestatesforCurie- cientlylargetoshownon-triviallearningguarantees. We
Weiss model. We also demonstrate strongly call this ratio ω .
P
metastable distributions in the Curie-Weiss model.
From these natural assumptions, we rigorously estab-
In this case, we show that such distributions are
lish that strongly metastable states are almost as good
centeredattheminimaofthefree-energyfunction.
as the equilibrium distribution for learning the energy
We numerically demonstrate that these construc-
function. Below we informally state the main result that
tions develop O( 1 )−strongly metastable states,
n2 facilitates this.
wherenisthenumberofspinsinthemodels. This
Theorem 1: Closeness of conditionals The single
exampleclarifiestheconnectionbetweenthestrong
variable conditionals of an η− strongly metastable of P
metastability and the statistical physics picture of
differs from those of the equilibrium distribution µ in
thermaldynamicsbeingstuckattheminimaofthe
average TV by only O(η/ω ). Where the average in this
free energy. P
case is taken over the conditioning variables using the
After establishing the notion of strong metastability metastable distribution.
we move on to showing the connection between strongly This result is schematically represented in Figure 1.
metastable distributions and learning of binary graphi- It is crucial as the PL method works by minimizing the
cal models. The key connection we show is that strongly averageKLdivergencebetweensinglevariablecondition-
metastable states have on average almost the same sin- als. NowusingthewellknownboundsconnectingTVto
gle variable conditionals as the equilibrium distribution the KL divergence, we argue that the PL loss function
µ; which is the true graphical model we are ultimately computed from these samples will also be be close to op-
interested in learning. timum.
First, we assume a natural conditions on the ground Now notice from our constructions that η for strong
truth equilibrium distribution µ and the Markov chain metastablestatesisadecayingfunctionofthenumberof
P, which we state informally below variables for slow-mixing systems. This implies that for
Condition 1: Bounded spin-flip We assume that there largesystemsthatexhibitslowmixing,thisbiasincurred
is a non-zero ratio between the single variable condition- in the PL loss function is dominated by the statistical4
errors that scale as M−1/2. conditioncanbeshowntoimplystationaryi.e. Foradis-
In the final part of our paper, we apply the results tributionµandachainP thefollowingimplicationholds
derivedsofartotheproblemoflearningbinarygraphical true,
models.
P(i|j)µ(j)=P(j|i)µ(i), ∀i,j ∈S =⇒ µP =µ (2)
We use tools from statistical learning theory to show
that the parameters of the energy function of pairwise Nowifwetaketheviewthatthistimereversibilitycon-
binary (Ising) exponential family models can be recon- dition must only be mildly violated for a metastable dis-
structed from samples from metastable states. We show tributionthenastrongernotionofmetastabilityemerges.
that the coupling and magnetic field parameters of such
Definition 2 (StrongMetastability). Adistributionν is
models can be learned using eO(βd)log(n) samples for
η-strongly metastable with respect to a Markov chain P
annspinsystemwithonlyabiasthatscaleswith η/ω .
P iff,
Henceinthemostinterestingcases(i.e. likeinConstruc-
tion 1 for a slow mixing chain) this bias is exponentially
1 (cid:88)
smallinthenumberofspinsandiscompletelydominated |P(i|j)ν(j)−P(j|i)ν(i)| ≤ η (3)
by the statistical error in the learning process. 2
i,j∈S
Finally, we provide a numerical demonstration of our
If we set η to zero in the definition of strong metasta-
results. We use the canonical example of a physical sys-
bility, then we get the detailed balance condition,
tem that exhibits metastability, the Curie-Weiss ferro-
P(i|j)ν(j)=P(j|i)ν(i).Asforeshadowedbytheirnames,
magnet, to demonstrate numerically that the model pa-
itcanbeeasilydeducedfromtheirdefinitionsthatstrong
rameters can be faithfully learned from samples coming
metastability implies metastability with the same η.
from a demonstrably poorly mixed Glauber dynamics
Inthefollowingsectionswewillshowexplicitconstric-
chain.
tions of strongly metastable distributions that applies to
generalreversiblechains. Wewillthenusethisdefinition
of strong metastability to show how the equilibrium dis-
II. DEFINITIONS OF METASTABILITY
tribution can be learned given samples drawn from such
astate. Wealsodiscussmorecloselytherelationshipbe-
Ametastabledistributionmustbeastatethatbehaves tweenthesetwonotionsofmetastabilityinAppendix B.
similarly to the stationary state under a time update. Specifically in Proposition 2, we show that it is possible
From this, one possible definition of a metastable dis- to have large separations between these two measures
tribution could be a distribution that does not change of metastability, implying that the strong metastability
under time updates. conditionisindeedtherelevantconditiontolookforfrom
the context of learning.
Definition 1 (Metastability). A distribution ν is η-
metastable with respect to a Markov chain P iff,
A. Existence of strongly metastable distributions
|ν−νP| ≤η (1) for reversible Markov chains
TV
This definition most naturally captures the idea of a From the definition of strong metastability, it is not
metastabledistribution. Throughoutthiswork,wethink clear whether metastable distributions seen while sam-
of η as a decaying function of |S|. Later we will show pling from challenging many-variable problems satisfy
explicit constructions where being smaller than some in- this condition. We will show that strong metastability
verse polynomial function of the size of the state space is closely tied to regions in the state space that bottle-
(O(|S|−k). In the language of spins, this implies an ex- neck the probability flow. This will further imply that
ponentiallysmallquantityinthenumberofspins. Itcan any slow-mixing reversible Markov chain must necessar-
be shown that if the Markov chain is started from such ily have strongly metastable distributions.
adistributionthenthatchainwillmixpoorlytothesta- To this end let us first define conductance, which cap-
tionary distribution. tures the ease with which probability can flow out of a
certain region in phase space.
Proposition 1. Starting from η− metastable distribu-
tion it takes at least |µ−ν|TV−ϵ number of steps to get ϵ Definition 3 (Conductance). Given a Markov chain P
η on a state space S with stationary state µ. The conduc-
close the equilibrium distribution µ in TV.
tance of any set A⊆S,
Thissimplefactisaconsequenceofthedata-processing (cid:80) P(i|j)µ(j)
inequality. The proof is given in AppendixA. Γ(A):= j∈A (cid:80),i∈Ac (4)
µ(j)
However, almost all Markov chain samplers used in j∈A
practice have a stationary state that satisfies time re- From this, the conductance of the chain is defined as,
versibility,whichiscapturedbythedetailedbalancecon-
Γ:= min Γ(A) (5)
dition. ForreversibleMarkovchains,thedetailedbalance
A:µ(A)<1/25
Conductance is a well-studied property of Markov model [28, 32]. This model is defined by the following
chains and form the basis of many classic results in ran- energyfunctiononnbinaryspinvariables(σ ∈{1,−1})
i
domized algorithms [2, 9, 14, 25]. The usefulness of this parameterized by a coupling constant J and a magnetic
propertycomesfromtheCheegerbound,whichconnects filed h,
it to the spectral gap of the chain and in turn to the
mixing time.
n n
J (cid:88) (cid:88) (cid:88)
E(σ)= σ σ −h σ . (9)
Lemma 1 (Cheeger bound [25]). Let λ be the sec- n i j i
2
i=1j=i+1 i
ond largest eigenvalue of a reversible, irreducible Markov
chain P with conductance Γ. Then, This model is widely studied in statistical physics
where it serves as a canonical model for understanding
Γ2
symmetry-breaking phase transitions.
2Γ≥1−λ ≥ (6)
2 2 The CW model model has permutation invariance i.e.
the probability of observing a certain configuration of
Now let 1=λ >λ ≥...λ >−1, be the eigenval-
1 2 |S| spins in this model only depends on their total sum. For
ues of a reversible, ergodic, irreducible chain. It is well
this reason, the the essential statistical properties of this
knownthatthemixingtimeofsuchachainisdetermined
system are controlled by the free energy of the model,
bytheinversespectralgap, τ =Θ(1/(1−λ ))[2,33].
mix 2
Nowletuslookattheimplicationsofthesestatements −J
Ψ(m)= m2+hm−S(m). (10)
for a reversible Markov chain that is attempting to sam- 2
ple from a family of graphical models on n-interacting Here S(m) = 1 log(cid:0) n (cid:1) is the entropy term. Now if
discretevariables,eachtakingavaluefrom{1,...,Q}. In n n(1+m)
2 (cid:80)
this case S =[Q]n. Furthermore, assume that the chain we define the magnetization (m(σ)= iσ i/n) of a spin
is only allowed to make moves that change at most one configuration, then the probability of observing a config-
variable at a time. This is in line with Glauber dynam-
urationwithmagnetizationmisproportionaltoe−Ψ(m).
icsortheMetropolis-Hastingsalgorithm. Slowmixingin The minima of this free energy function also correspond
such a chain implies that the mixing time of the chain to regions in the state space from which Glauber dy-
scales exponentially with n. From the Cheeger bound namics struggles to escape from [19]. We will use these
this implies that there exists a set with an exponentially known connections between metastability and the min-
smallconductance. Wewillnowshowthatsuchasetwill ima of the free energy to construct strongly metastable
necessarily support a strongly metastable distribution. states around these minima.
For A⊂S define the following distribution, Inspired by the statistical physics picture, we look for
candidatemetastablestatesbyanalysingΨ(m). Wemo-
(cid:40)
µ(σ), σ ∈A, tivate this method of construction of metastable states
µ A(σ):= 0µ ,(A)
σ ∈Ac.
(7) further in Appendix C matching the conditionals of can-
didate metastable states with that of the true distribu-
tion. Therewefindthatthemetastablestatestheclosely
Observe that the detailed balance condition is only vi-
match the conditionals of the true distribution are cen-
olatedattheboundaryofthisset. Fromthisobservation
tred around the minima of the free-energy. Now, define
wecandirectlycomputethestrongmetastabilityofsuch
m to be the positive value where the gradient of the
a state, 0
free-energy vanishes,
1 (cid:88) |P(σ|σ′)µ (σ′)−P(σ′|σ)µ (σ)|= −Jm 0+h−S′(m 0)=0, m 0 ≥0. (11)
2 A A
σ,σ′∈S We construct candidate metastable states by using a
(cid:88) |P(σ′|σ)µ (σ)|=Γ(A). (8) K-order Taylor approximation of the free energy around
A
this minimum point. Notice that the first order terms
σ∈A,σ′∈Ac
vanishes and the constant term can be ignored as we are
From the Cheeger, bound this implies that for distribu- working with energies.
tion µ that is the stationary distribution of reversible
Markov chain P there exists η-strongly metastable dis-
tributions with η exponentially small in the number of
Φ(K)(m)=(cid:88)K
Ψ(k)(m
)(m−m 0)k
(12)
spins. 0 k!
k=2
Here Ψ(k) is the k−th derivative of the free energy.
B. Strongly metastable distributions in the Thismetastablefree-energydefinesametastablestateas
Curie-Weiss model follows,
e−nΦ(K)(m(σ))
We can also construct explicit examples of strongly ν(K)(σ)∝ . (13)
(cid:0) n (cid:1)
metastable states with small η in the Curie-Weiss (CW) n(1+m(σ))
26
Notice that these metastable states are also permuta- We will later see that ω will control the error in
P
tion invariant. The violation in detailed balance con- the learning procedure and metastable states with small
dition used in (3) is difficult to compute for general η/ω gives us good learning guarantees.
P
distributions. However, for permutation invariant dis- Now we state one of the main results of the paper, the
tributions this quantity can le efficiently computed by proof of this result is given in AppendixD
first mapping the problem entirely onto the magnetiza-
tion space. We give the details of this mapping in the Theorem1(Conditionalsofstrongmetastabledistribu-
AppendixC. Now by numerically evaluating the detailed tions). Consider a system of n discrete variables where
balance violation we conclude that the ν(4) distribution each of them can take values Q different values i.e.
is O( 1 ) strongly metastable for an n−spin Curie-Weiss S =[Q]n. Let ν be an η−strongly metastable distribution
n2
model for J > 1 and 0 < h < 0.05. We also find ofreversibleMarkovchainP withastationarystateµ. If
thattruncatingthefree-energyexactlyaroundm witha this chain satisfies Condition 1, then the single variable
0
well chosen width leads to e−O(n)− strongly metastable conditionals of ν are close to that of µ in the following
states, akin to the construction in Subsection IIA. Nu- sense,
merical results establishing this and further details can
be found in Appendix C. (cid:88)n (cid:88) (cid:12) (cid:12) η
ν(σ)(cid:12)ν(.|σ )−µ(.|σ )(cid:12) ≤ (15)
(cid:12) \u \u (cid:12) TV ω P
u=1σ∈S
III. LEARNING FROM METASTABLE
This is an important result that says that a strong
DISTRIBUTIONS
metastable distribution has on average single variable
conditionals that are very close to that of the equilib-
Now that we have established that interesting cases rium distribution. This result will be the basis for show-
of strongly metastable distributions exist, we will show ing that the parameters of the true distribution (µ) can
that the energy function corresponding to the true dis- be learned given samples from (ν). This due to the fact
tribution can be learned given samples from a strongly that methods like PL and Interaction Screening that ef-
metastable distribution. The key result that we will ex- ficiently reconstruct undirected graphical models rely on
ploit to learn from metastable distributions is the fact minimizing average metrics over the single variable con-
that strong metastability, when combined with the time ditionals.
reversibility of the chain, implies that the single variable Theorem 1 should contrasted with the explicit con-
conditionals of the metastable distribution are close to structions in Equation (7) that gave strongly metastable
those the stationary state of the Markov chain. Build- distributions that are considerably far from the equilib-
ing on this, we then use prior information on the true rium in terms of global TV. According to that construc-
model and stochastic convex optimization techniques to tion,theTVofthemetastabledistributionfromtheequi-
showthatlearningisfrommetastabledistributionsiscan librium distribution is, |µ −µ| =1−µ(A), which is
A TV
be successfully performed using the Psuedo-likelihood generally not an exponentially small quantity even if the
method. Markov chain mixes slowly. On the other hand, the dis-
To this end, assume a technical condition on the tran- tance in Theorem 1 for ν = µ is Γ(A)/ω which can
A P
sition probability of a reversible Markov chain. beexponentially smallif A isa set thatcauses slowmix-
ing in the Markov chain. A demonstration of this can
Condition 1 (Bounded spin-flip probability). For a re- also been for the CW model. Theorem 1 implies that
versiblechainP, letP(σ |σ)betheprobabilityofvari- themetastabledistributionsupportedononeofthemin-
u→q
able at position u being changed to q ∈[Q] when the sys- imaoffreeenergyfunctionhasconditionalsthatarevery
tem is in the configuration σ. Then we assume that the close to the true distribution. For instance, the average
ratio of this to the single variable conditionals is lower distance between the quartic state ν(4) and µ scales as
bounded by quantity that depends on P, O(1/n)forannspinCWmodel. Thisistrueevenifthis
metastable state is supported around one of the minima
P(σ |σ) where the true model has effectively no mass. We study
u→q ≥ω >0, ∀σ ∈[Q]n−1, q ∈[Q], u∈[n].
µ(q|σ ) P \u such a extreme case numerically in Section IV
\u
(14) These also tell us that trying to learn these distribu-
tions by minimizing global metrics will give us markedly
The bound in the above condition is satisfied by the different answers. For instance, the Maximum likeli-
two of the most commonly used Markov chain samplers, hood estimation minimizes the KL divergence between
Glauber dynamics and the Metropolis-Hastings sampler thedatadistributionandaproposedparametrichypoth-
[35]. For Glauber dynamics, by definition of the algo- esis. The KL divergence between µ and µ is given by
A
rithm ω = 1. For Metropolis-Hasting Markov chain, −log(µ(A)), is very far from being the optimal. We nu-
P n
ω will also be proportional to 1/n, with the ratio be- mericallyshowthedifferencebetweenMLandPLforthe
p
tween them only depending on the inverse temperature CW model in FIG.3a and 3b. So, for the purpose of re-
of the chain. coveringthetruemodelfromametastablestate,wehave7
to rely on minimizing the distance between conditionals model,
and not global metrics.
1
Theorem 1 can be modified to give closeness in terms
µ(σ |σ )= . (19)
of other metrics like the average KL divergence between u \u 1+exp(−2E u(σ;θ∗))
conditionals. This is a simple consequence of the re-
In the learning task considered in most of the prior
versePinkerinequality(Lemma4.1in[18]),whichupper
works [48], we are given i.i.d samples from µ and we at-
bounds the KL divergence between two distributions in
tempt to reconstruct the θ∗ parameters from them. An
terms of the TV . Given distributions p and q, we have,
D (p||q) ≤ 2 |p−q| . Using this relation di- important feature of the sample complexity of this prob-
KL miniq(i) TV lem is its fundamental dependence on the strength of
rectly on Theorem 1 we get,
the local interactions i.e. (cid:80) |θ∗|. This feature is ob-
k∈K u
Corollary 1. For distributions µ and ν as defined in served both in the sample complexity of algorithms that
Theorem1,foreveryu∈[n]wehavethefollowingbound, solve this problem and also in the known information-
theoretic lower bound [43]. An unbounded interaction
(cid:16) (cid:17) 2η
E D ν(.|σ )||µ(.|σ ) ≤ . strengthcancausethesamplecomplexityofthelearning
σ∼ν KL \u \u ω P min σµ(σ u|σ \u) tasktoblowup. Thisisobviousastheparametersinthe
(16) energycannotbereconstructedingeneralifweonlyhave
access to the minimum energy states. To mitigate this
This metric is specifically important to bound, as we
intrinsicpathology,wefollowpriorliteratureandassume
will show later that PL estimator precisely minimizes
a finiteness condition on the interaction strengths.
the average KL divergence between conditionals . The
lower bound on the conditional is a natural quantity Condition 2 (Finiteinteractionstrength). Defineθ∗ :=
u
that controls the error in these types of learning results {θ∗|k ∈ K }. Then the local interaction strengths are
k u
[27,34,42,43]. Forinstance,intheSparsitronalgorithm bounded as follows,
[27], the authors use the term δ−unbiased conditionals
to refer to the existence of such a lower bound. ||θ∗|| = (cid:88) |θ∗|≤γ <∞, ∀ u∈V (20)
u 1 k
k∈Ku
A. Learning binary graphical models from Crucially this condition gives size independent upper
metastable states and lower bounds on the conditional probabilities which
ultimately control the sample complexity of learning al-
The result in Theorem 1 and the ensuing implications gorithms like PL,
can be applied to general distribution over binary vari-
ablesindependentofthegraphicalmodelframework. Go- 1 1
≤µ(σ |σ )≤ . (21)
ing forward we will build these results specifically for 1+exp(2γ) u \u 1+exp(−2γ)
undirected graphical models over binary spins.
We define the ground-truth graphical model on a set In the learning task we are interested in this paper,
of n Ising spins, σ ∈ {1,−1},u ∈ V, where V = [n] is we assume that the samples come from an η− strongly
u
the vertex set of this model. In the most general form, metastabledistribution,ν,ofreversibleMarkovchainP.
such a model can be equivalently expressed as a Gibbs Moreover, this Markov chain has µ as its unique station-
distribution [30], ary state. As the Markov chain is reversible this implies
that the detailed balance equations are satisfied,
exp(cid:0)(cid:80) θ∗σ (cid:1)
µ(σ)= k∈K k k . (17) P(σ′|σ)µ(σ)=P(σ|σ)µ(σ′). (22)
Z
Here K ⊆ 2V is the set of interactions present in
(cid:81)
the model and σ
k
:= u∈kσ u. For example, the CW 1. Pseudo-Likelihood method
model fits into this form with K being the set of all
tuples of size less than three. The function in the ex-
Going forward we will focus on PL and explore the
ponent, E(σ;θ∗) := (cid:80) θ∗σ is known as the energy
k∈K k k consequences of Theorem 1 on this method.
function of this model. Additionally, it is useful to de- Given samples σ(1),...,σ(M) drawn from the
fine a local energy function E (σ) for every u ∈ V. Let
u metastable distribution ν. The PL estimate of the
K ={k|k ∈K,u∈k}, then the local energy function is
u parameters in the energy function can be written as the
defined as,
solution to the following ℓ constrained optimization
1
(cid:88) problem of minimizing a specific loss function[42],
E (σ;θ∗)= θ∗σ (18)
u k k
k∈Ku
M
Theimportanceofthelocalenergycomesfromthefact θˆ = argmin 1 (cid:88) L(θ ,σ). (23)
that it determines the single variable conditionals of the u ||θ u||1≤γ M t=1 u8
(cid:88)
L(θ ,σ):=log(1+exp(−2 θ σ )). (24) wouldthenimplythatthesmallchangesinthelossvalues
u k k
translate to small changes in the estimated parameters.
k∈Ku
1
Theℓ constraintreflectsthepriorinformationwehave
1
aboutthestrengthoftheoptimalparameters. Butwedo
not assume assume that the γ here is the most optimal B. Parameter learning guarantees
γ we can choose in Condition 2.
Now in the M → ∞ limit, one can show that this
Inthissection, weexploitthepropertiesofsinglevari-
minimizestheaverageKLdivergencebetweenthecondi-
able conditionals of strongly metastable states to prove
tionals of ν and the single variable conditionals of the
guaranteesonparameterandstructurerecoveryformod-
(cid:80)
parametrized distribution, p(σ;θ) ∝ exp( θ σ ).
k∈K k k elswithuptopairwiseinteractionsi.e. Isingmodelswith
To see this connection, notice that the single variable
magnetic field terms. The most general energy function
conditionalsoftheparametricdistributiontaketheform
for this class of models can be written as,
p(σ |σ ;θ) = 1 . Then the above
u \u 1+exp(−2(cid:80) k∈Kuθkσ k)
mentioned average KL divergence between conditionals
(cid:88) (cid:88)
takes the form, E(σ)= θ i∗ jσ iσ j + θ i∗σ i (26)
i<j i
E D (ν(.|σ )||p(.|σ ;θ))= (25)
Thelearningtaskhereistoestimatetheparametersθ∗
σ∼ν KL \u \u from data. In this case the PL estimator can be written
(cid:32) (cid:33) as,
(cid:88)
ν(σ ) E log
ν(σ u|σ \u)
σ
\u σu∼ν(.|σ \u) p(σ u|σ \u;θ)
\u θˆ = argmin 1 (cid:88)M log(1+exp(−2σ(t)((cid:88) θ σ(t)+θ ))).
We can see that up to an unimportant constant, this u M u uj j u
is precisely the loss function in (23) as M →∞. ||θ u||1≤γ t=1 j̸=u
(27)
Now using the results developed so far, we can bound
Belowwestatethelearningguaranteeontheerrorbe-
the value of this average divergence when the hypothesis
tween the pairwise parameters learned in this way and
matches the true model, i.e. θ = θ∗. In this case, from
the true parameters of the energy function.
Corollary 1 and (21), we can see that the the average
(cid:108) (cid:109)
conditional KL divergence between the metastable state Theorem 2. For M = 28e8γγ4 log(8n) , with probabil-
and the true distribution upper bounded by
4(1+e2γ)η. ε4 δ
ωP ity greater than 1−δ the following guarantee holds for all
In practically interesting models, γ is usually a constant
u∈[n] ,
independent of n. In the previous sections, we showed
constructions of strongly metastable distributions where (cid:115)
(1+γ)η
η scales as inverse of the mixing time.For such distri- ||θ∗ −θˆ || ≤ ε+4e2γ . (28)
\u \u ∞ ω
butions associated with slow mixing Markov chains this P
upper bound decays exponentially fast with the number
The proof of this result is provided in the Appendix.
of spins in the model.
Thistheoremcanbeseenasthemetastableversionofthe
Remember that the KL divergence is always non-
learning guarantee for PL proved in [34]. Just like in the
negative, hence this quantity also upper bounds the dif-
settingwherewehavesamplesfromthetruedistribution,
ference in the PL loss value between θ∗ and the true
we see that at sample complexity is exponential in the γ
optimum θˆ. From this we can conclude that there is
parameter [34]. But unlike these settings, there is an
a family of strongly metastable distributions for which
unavoidable bias in the error that does not go to zero
the the Pseudo-likelihood estimator asymptotically ap-
as the number of samples increases. We can see from
proachesthetruemodelparameterinthelimitofinfinite
the constructions of strongly metastable states discussed
learningsamplesdrawnfromthemetastabledistribution.
before, this bias decays with the size of the system.
This is an extremely interesting observation, as this tells
For Glauber dynamics remember that ω = 1/n. So
P
us that the true model can be nearly reconstructed from any metastable state with η =o(1) will give sufficiently
“ bad ” data using the PL estimator. n
√ strongerrorguaranteeswithabiasthatdecayswithsys-
In practice, for finite M, there will be an O(1/ M)
tem size.
statistical error in the estimation of the loss function.
Animportantproblemassociatedwithlearninggraph-
Hence, if M is small enough this statistical error will
ical models is structure learning, which refers to the re-
swamptheO(η/ω )biasintroducedbyhavingbaddata.
P construction of the underlying graph structure of the
However, this result by itself does not guarantee that
the estimated parameters from (23) will closely match
thatofthetruedistribution. Togivesuchlearningguar-
antees we have to show that the loss function has suf- 1 See Figure 2 of Reference [39] for a visual explanation of this
ficiently large curvature near the optimal point. This point9
model. To this end define the edge set E = {(u,v) ∈ We sample from this model using the Markov chain
[n]×[n]|θ ̸= 0}. Then this edge set can be recovered method known as Glauber dynamics [17] and then at-
u,v
from the learned couplings with high probability using a tempttolearntheparametersJ andhusingthepseudo-
simple thresholding method. likelihood method [34], which is known to be one of the
efficient methods that can solve this learning problem.
Theorem 3 (Structure learning). Consider an Ising
Wechoosethisasourpreliminarymodelfortworeasons.
model with |θ∗ | > α for every (u,v) ∈ E Let θˆ be es- Firstly,intherightparameterrange,thismodeldevelops
uv
(cid:108) (cid:109)
timated using M = 224e8γγ4 log(8n) samples from a two metastable distributions. By tuning the magnetic
α4 δ
field h we can also change the relative size (in terms of
η−strongly metastable state by solving the optimization
(cid:113) probability)ofthesetwostates. Secondly,becauseofthe
problem in (27). Then if α > 16e2γ (1+γ)η, the esti- all-to-all,uniformcouplings,wecanrunGlauberdynam-
ωP
mated edge set, ics on very large system sizes. This is because energies
ofnewconfigurationscanbecomputedextremelyfastby
Eˆ ={(u,v)∈[n]×[n] | max(|θˆ |,|θˆ |)>α/2} (29) exploiting this struclure.
uv vu
will match E with probability greater 1−δ
In this experiment, we choose the system parameters
The assumed η dependent lower bound on α is nec- suchthatthedistributionisbimodalbutheavilylopsided
essary to prevent the inherent bias in the learning from with most of the probability in states that have negative
(cid:80)
swamping very small non-zero couplings in the model. magnetization ( iσ i). FromFIG.2b,weseethatifi.i.d
Finally we will show how the magnetic field can be samples were produced the chance of observing a sample
(cid:80)
provablerecoveredford−sparsemodels. Weachievethis with positive magnetization ( iσ i > 0) will essentially
by doing a second round of optimization after structure be zero. But if we use Glauber dynamics to sample from
learning. This is done by estimating the second order thismodelandstarttheMarkovchainfromallonestate
terms first as in Theorem 2, reconstructing the structure (σ i = 1∀i) then the dynamics will get stuck in the pos-
and then optimizing the PL loss for a second round with itive magnetization part of the distribution. This is a
onlythemagneticfieldtermsasthevariable. Asfaraswe well-studied property of CW models that can be rigor-
know, this results give the first guarantees for recovering ously established [32]. We corroborate this by plotting
magnetic fields even in the η =0 setting. theempiricalprobabilitiesproducedbyanexactsampler
and Glauber dynamics in FIG. 2b. This shows that the
Theorem 4 (Recovering magnetic fields). Suppose that Glauber dynamic is stuck around the minima with pos-
we are given the edge set E and estimates for all the itive magnetization. At this point, we assume that the
second order parameters in the model such that ||θ∗ − distribution of the Markov chain is close to a metastable
\u
θˆ || ≤ ε. Further assume that the underlying graph distribution with positive magnetization. Notice that by
\u ∞
has max degree of d and that |θ∗| ≤ h . Then given any global metric (TV, KL-Divergence, etc.) this distri-
u max
samples from an η−strongly metastable state we can es- butionisveryfarfromtheGibbsdistributionofthetrue
timate the magnetic fields for each u as follows; CWmodel. ThisimpliesthatatechniquelikeMaximum
Likelihood Estimation (MLE) will not succeed in recov-
M ering the right model parameters (J and h) as it empir-
θˆ = argmin 1 (cid:88) log(1+exp(−2σ (⟨ˆθ ,σ ⟩+θ )). ically minimizes the KL-divergence between the samples
u M u Eu Eu u
|θu|≤hmax t=1 and the parametric model we are trying to learn.
(30)
For M
=(cid:108) 27h4 maxe4hmax log(4)(cid:109)
we can guarantee with
ε4 δ However, we find that the original model parameters
h
probability at least 1−δ that the error in this estimate canberecoveredfromthesesamplesusingPLestimator.
√
will be bounded as, |θˆ u−θ u∗|≤ε h+4 dεh maxeγ+hmax+ The results of learning using PL are shown in FIG. 2a,
(cid:113)
4 ηhmaxehmax which shows that J and h can be learned from samples
ωP from the metastable distributions with remarkable accu-
racy.
Thethreetheoremsinthissectiontogetherimplythat
every parameter in a d− sparse Ising model can be re-
coveredwithasmallbiaswithsamplecomplexityscaling Given these samples, the correct values of J,h cannot
as O(log(n)). be predicted by the maximum likelihood method. We
demonstrate this in Figure 3a by plotting the negative
log-likelihoodinthe(J,h)space. Weseethattheminima
IV. NUMERICAL RESULTS ofthisfunctionliesfarawayfromthetruemodel. Figure
3b show the PL loss function. We see from these plots
To numerically validate our results we examine the that that the PL loss function has it’s minima close to
learningproblemfortheCWmodelintroducedinSection the true model, while maximum likelihood predicts the
IIB. wrong sign for the magnetic field in the model.10
(a) (b)
FIG. 2: (a) Error in learning the Curie-Wiess model on 5000 spins. Samples here are produced by Glauber dynamics
“stuck” at the positive minima of the free energy. True parameters here are J =1.2,h=0.04. (b) The true
distribution is highly biased towards towards negative magnetization as seen by free energy curve. There is a
metastable distribution with positive magnetization that is highly suppressed in terms of probability. The empirical
distributions of samples (M =4×109) drawn by an exact sampler and Glauber dynamics is overlaid on top of the
free energy. This shows that the Markov chain is effectively stuck around the positive minima.
(a) Maximum likelihood loss (b) Psuedo-liklihood loss
FIG. 3: Comparison of the loss function landscape for the CW model with true parameters J =1.2,h=0.04. These
are plotted with M =232 samples produced by Glauber dynamics “stuck” at the positive minima of the free energy.
(a) Negative log-likelihood computed from this data clearly has it’s minimum far from the true model. The sign of
the magnetization is opposite of the true model. This is expected as maximum likelihood tries to match the
sufficient statistics of the data to the model. (b) PL loss function has the minima close to the true model.
V. CONCLUSION forGlauberdynamicsMarkovchain. Thisisbecausethe
underlying graph of transitions for Glauber dynamics is
the n-bit hypercube, which has a diameter of n. This
Our work opens up several interesting directions for would then imply non-trivial learning guarantees for all
exploration. For instance, it is natural ask if there are metastable states. The connections between these no-
large separations in the two measures of metastability tions and electrical networks used in the proof of Propo-
defined in this work. In AppendixB, we show that for a sition 2 might be useful for these explorations.
given P, the largest separation between these measures
is at least as big as the diameter of the underlying graph The learning guarantees for the Ising case presented
oftransitions. Weconjecturethatthisisnottheoptimal here can be generalized to more general models as done
separation between there two measures of metastability. in prior works on learning graphical models [27, 48]. No-
If this separation is indeed tight then that implies only ticethatwhilewehavefocusedonthePseudo-Likelihood
anO(n)differencebetweenthetwomeasuresofmeasures method, the same arguments presented here would be11
applicable to any method that exploits conditionals for used in the optimization). At least in graphical model
learning. In fact, we believe that similar ideas will even learning, this demonstrates the interplay between algo-
carry over to the case of continuous alphabets. rithms and prior which allow learning to succeed even in
Viewing the results of this work from a higher level, thepresenceofthe“wrong”data. Theconditionalbased
we have established that generalization is possible in the methods used here are not limited to graphical models,
contextoflearningdiscretedistributionsevenifthedata they can also be used for instance to learn energy based
isconstrainedtocomefromsomespecificregionsofstate models with a neural net parametrization of the energy
space. This generalization is made possible by the use of [24]. The exploration of the ideas presented here in the
methodsthatthelearntheconditionalsandbypriorsim- context of more general energy-based models is sure to
posedduringlearning(intermsofthelow-degreeformof shed light on the observed generalization properties of
the energy assumed during learning and the constraints such models.
[1] Chapter3-precursorsofmaterialsscience. InTheCom- (1991), 1–17.
ingofMaterialsScience,R.W.Cahn,Ed.,vol.5ofPerg- [15] Gaitonde,J.,Moitra,A.,andMossel,E.Efficiently
amon Materials Series. Pergamon, 2001, pp. 57–156. learning markov random fields from dynamics. arXiv
[2] Aldous,D.,andFill,J. Reversiblemarkovchainsand preprint arXiv:2409.05284 (2024).
random walks on graphs. [16] Gaitonde, J., and Mossel, E. A unified approach
[3] Barahona,F.Onthecomputationalcomplexityofising to learning ising models: Beyond independence and
spin glass models. Journal of Physics A: Mathematical boundedwidth.arXivpreprintarXiv:2311.09197 (2023).
and General 15, 10 (1982), 3241. [17] Glauber, R. J. Time-dependent statistics of the ising
[4] Bresler, G. Efficiently learning Ising models on ar- model. Journalofmathematicalphysics4,2(1963),294–
bitrary graphs. In Proceedings of the forty-seventh an- 307.
nual ACM symposium on Theory of computing (2015), [18] Go¨tze, F., Sambale, H., and Sinulis, A. Higher or-
pp. 771–782. derconcentrationforfunctionsofweaklydependentran-
[5] Bresler, G., Gamarnik, D., and Shah, D. Learn- domvariables. ElectronicJournalofProbability24,none
ing graphical models from the glauber dynamics. IEEE (2019), 1 – 19.
Transactions on Information Theory 64,6(2017),4072– [19] Griffiths, R. B., Weng, C.-Y., and Langer, J. S.
4080. Relaxation times for metastable states in the mean-field
[6] Cassandro, M., Galves, A., Olivieri, E., and model of a ferromagnet. Physical Review 149, 1 (1966),
Vares, M.E. Metastablebehaviorofstochasticdynam- 301.
ics: a pathwise approach. Journal of statistical physics [20] Hamilton, L., Koehler, F., and Moitra, A. In-
35 (1984), 603–634. formation theoretic properties of markov random fields,
[7] Cule, M., Samworth, R., and Stewart, M. Max- and their algorithmic applications. In Advances in Neu-
imum likelihood estimation of a multi-dimensional log- ral Information Processing Systems 30, I. Guyon, U. V.
concave density. Journal of the Royal Statistical Society Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
Series B: Statistical Methodology 72, 5 (2010), 545–607. wanathan, and R. Garnett, Eds. 2017, pp. 2463–2472.
[8] DelDebbio, L., Manca, G.M., andVicari, E. Crit- [21] Hanggi, P. Escape from a metastable state. Journal of
ical slowing down of topological modes. Physics Letters Statistical Physics 42 (1986), 105–148.
B 594, 3-4 (2004), 315–323. [22] Hodgman, S., Dall, R., Byron, L., Baldwin, K.,
[9] Diaconis, P., and Stroock, D. Geometric bounds Buckman, S., and Truscott, A. Metastable helium:
for eigenvalues of markov chains. The annals of applied A new determination of the longest atomic excited-state
probability (1991), 36–61. lifetime. Physical review letters 103, 5 (2009), 053002.
[10] Diakonikolas, I., Kane, D. M., and Stewart, A. [23] Horn, R. A., and Johnson, C. R. Matrix analysis.
Learningmultivariatelog-concavedistributions. InCon- Cambridge university press, 2012.
ferenceonLearningTheory (2017),PMLR,pp.711–727. [24] J, A., Lokhov, A., Misra, S., and Vuffray, M.
[11] Dinner,A.R.,andKarplus,M.Ametastablestatein Learning of discrete graphical models with neural net-
foldingsimulationsofaproteinmodel. Nature structural works. Advances in Neural Information Processing Sys-
biology 5, 3 (1998), 236–241. tems 33 (2020), 5610–5620.
[12] Doyle, P. G., and Snell, J. L. Random walks and [25] Jerrum, M., and Sinclair, A. Conductance and the
electric networks, vol. 22. American Mathematical Soc., rapidmixingpropertyformarkovchains: theapproxima-
1984. tion of permanent resolved. In Proceedings of the twen-
[13] Dutt, A., Lokhov, A., Vuffray, M. D., and Misra, tieth annual ACM symposium on Theory of computing
S. Exponential reduction in sample complexity with (1988), pp. 235–244.
learning of ising model dynamics. In International Con- [26] Kirkpatrick, T., and Wolynes, P. Stable and
ference on Machine Learning (2021), PMLR, pp. 2914– metastable states in mean-field potts and structural
2925. glasses. Physical Review B 36, 16 (1987), 8552.
[14] Dyer, M., Frieze, A., and Kannan, R. A random [27] Klivans, A., and Meka, R. Learninggraphicalmodels
polynomial-timealgorithmforapproximatingthevolume using multiplicative weights. In 2017 IEEE 58th An-
of convex bodies. Journal of the ACM (JACM) 38, 1 nual Symposium on Foundations of Computer Science12
(FOCS) (Oct 2017), pp. 343–354. [42] Ravikumar, P., Wainwright, M. J., Lafferty,
[28] Kochman´ski, M., Paszkiewicz, T., and Wolski, S. J. D., et al. High-dimensional Ising model selection
Curie–weissmagnet—asimplemodelofphasetransition. using l -regularized logistic regression. The Annals of
1
European Journal of Physics 34, 6 (2013), 1555. Statistics 38, 3 (2010), 1287–1319.
[29] Koehler, F., Heckett, A., and Risteski, A. Sta- [43] Santhanam, N. P., and Wainwright, M. J.
tistical efficiency of score matching: The view from Information-theoretic limits of selecting binary graphi-
isoperimetry. arXiv preprint arXiv:2210.00726 (2022). cal models in high dimensions. IEEE Transactions on
[30] Koller, D., and Friedman, N. Probabilistic graphical Information Theory 58, 7 (2012), 4117–4134.
models: principles and techniques. MIT press, 2009. [44] Schaefer, S., Sommer, R., Virotta, F., Collabo-
[31] Krauth,W. Statisticalmechanics: algorithmsandcom- ration, A., et al. Critical slowing down and error
putations, vol. 13. OUP Oxford, 2006. analysis in lattice qcd simulations. Nuclear Physics B
[32] Levin, D. A., Luczak, M. J., and Peres, Y. Glauber 845, 1 (2011), 93–119.
dynamics for the mean-field ising model: cut-off, critical [45] Sewell, G. L. Stability, equilibrium and metastability
power law, and metastability. Probability Theory and in statistical mechanics. Physics Reports 57, 5 (1980),
Related Fields 146 (2010), 223–265. 307–342.
[33] Levin,D.A.,andPeres,Y.Markovchainsandmixing [46] Sly, A., and Sun, N. The computational hardness of
times, vol. 107. American Mathematical Soc., 2017. countingintwo-spinmodelsond-regulargraphs. In2012
[34] Lokhov, A. Y., Vuffray, M., Misra, S., and IEEE 53rd Annual Symposium on Foundations of Com-
Chertkov, M. Optimal structure and parameter learn- puter Science (2012), IEEE, pp. 361–369.
ing of Ising models. Science advances 4, 3 (2018), [47] Verleysen,M.,etal.Learninghigh-dimensionaldata.
e1700791. Nato Science Series Sub Series III Computer And Sys-
[35] MacKay, D. J., Mac Kay, D. J., et al. Information tems Sciences 186 (2003), 141–162.
theory, inference and learning algorithms. Cambridge [48] Vuffray, M., Misra, S., and Lokhov, A. Efficient
university press, 2003. learningofdiscretegraphicalmodels.AdvancesinNeural
[36] Martinelli, F. Relaxation times of markov chains in InformationProcessingSystems33 (2020),13575–13585.
statisticalmechanicsandcombinatorialstructures.Prob- [49] Vuffray, M., Misra, S., Lokhov, A., and
ability on discrete structures (2004), 175–262. Chertkov, M. Interaction screening: Efficient and
[37] Mossel, E., and Sly, A. Exact thresholds for ising– sample-optimal learning of Ising models. In Advances in
gibbs samplers on general graphs. NeuralInformationProcessingSystems(2016),pp.2595–
[38] Nash-Williams, C. S. J. Random walk and electric 2603.
currentsinnetworks. InMathematicalProceedingsofthe [50] Wang, Z., and Scott, D. W. Nonparametric density
Cambridge Philosophical Society (1959), vol. 55, Cam- estimation for high-dimensional data—algorithms and
bridge University Press, pp. 181–194. applications. Wiley Interdisciplinary Reviews: Compu-
[39] Negahban, S. N., Ravikumar, P., Wainwright, tational Statistics 11, 4 (2019), e1461.
M. J., and Yu, B. A unified framework for high- [51] Wu, S., Sanghavi, S., and Dimakis, A. G. Sparse
dimensionalanalysisofm-estimatorswithdecomposable logistic regression learns all discrete pairwise graphical
regularizers. Statistical Science 27, 4 (2012), 538–557. models. In Advances in Neural Information Processing
[40] Perepezko, J. Phase transformation. In Encyclopedia Systems (2019), pp. 8071–8081.
of Condensed Matter Physics, F. Bassani, G. L. Liedl, [52] Zhou, X. On the fenchel duality between strong con-
andP.Wyder,Eds.Elsevier,Oxford,2005,pp.247–258. vexity and lipschitz continuous gradient. arXiv preprint
[41] Qin,Y.,andRisteski,A. Fitlikeyousample: Sample- arXiv:1803.06573 (2018).
efficient generalized score matching from fast mixing
markovchains. arXiv preprint arXiv:2306.09332 (2023).13
Appendix A: Proof of Proposition 1
For any Markov chain, P and any vector, v the following data-processing inequality holds,
(cid:88) (cid:88) (cid:88)(cid:88)
||vP|| = | P(i|j)v(j)|≤ P(i|j)|v(j)|=||v|| (A1)
1 1
i j i j
Nowgiveanη metastabledistributionν,usingtheabovedata-processinginequalityrepeatedlyonv =νP−ν gives
us,
|νPk+1−νPk|≤η (A2)
NowsupposethatafterT stepsstartingfromν theMarkovchaingetsϵcloseinTVtotheequilibriumdistribution
µ. From this,
|ν−µ| ≤|ν−νP| +|νP −µ| , (A3)
TV TV TV
≤|ν−νP| +|νP2−νP| ...|νPT −νPT−1| +|νPT −µ| , (A4)
TV TV TV TV
≤ηT +|νPT −µ| . (A5)
TV
This gives |νPT −µ| ≥|ν−µ| −ηT.
TV TV
Now |νPT −µ| ≤ϵ, gives us T ≥ |ν−µ|TV−ϵ.
TV η
Appendix B: Separations between metastability and strong metastabililty
Thetwodefinitionsofmetastabilityraises aninterestingquestion: For a reversible Markov chain does η− metasta-
bility imply η− strong metastability? We can construct a simple counter-example showing that is not true. For an
even integer L, let S = [L] and P be the Markov chain corresponding to the random walk on a cycle graph on L
elements. Take ν to be the following ‘tent’ distribution,
(cid:40)
1 iL, i≤L/2,
ν(i)= (B1)
Z −iL+L2, i>L/2,
The Z here ensures normalization. Now by direct computation, we can verify that this state is 2L metastable
Z
while being L2 strongly metastable. This shows that there can be Θ(|S|) factor difference between the two measures
2Z
metastability. ThisconstructioncanbegeneralizedtogeneralreversibleMarkovchainsusingbymappingthemtoan
electrical network and analyzing the current flows.
Proposition 2. Let G = (S,E) be a graph defined on the state space with the edge set E = {(i,j)|P(i|j) ̸=
0}. Let diam(G) be the diameter of this graph. Then the chain P has a η- metastable distribution that is
Ω(diam(G)η)−strongly metastable.
We conjecture that this construction does not give the optimal separation between metastable distributions and
strong metastable distributions. The reason is that the diameter is a topological property of the chain and it can be
changed drastically by adding a few well-chosen very small but non-zero transitions to the chain.
Whiletherelationshipbetweenthesetwonotionsofmetastabilityisaninterestingquestion,itisnotthemainfocus
of this paper. We leave further exploration of this for future work.
a. Proof of Proposition 2
Mapping the problem to an electric network: ToprovethisstatementwefirstmaptheMarkovchaintoanelectric
network using the well-known mapping between these two problems[12, 38]. The conductance 2 between two state
i,j will be given by,
c =µ(i)P(j|i)=µ(j)P(i|j). (B2)
ij
2 NottobemistakenfortheconductanceoftheMarkovchainthat
inverseoftheelectricalResistance.
isdefinedincontextofprobabilityflows. Thisconductanceisthe14
We also associate a voltage with each node,
ν(i)
V(i)= . (B3)
µ(i)
Let E be the edges with non-zero conductance, E ={(i,j)∈S×S| c ̸=0}.
ij
We also associate a current with every edge according to Ohm’s law,
I(i,j)=(V(i)−V(j))c ∀(i,j)∈E (B4)
ij
Note that this current is antisymmetric, that is, I(i,j)=−I(i,j).
Now for two nodes s and t in the graph, pick an η− metastable distribution such that ν(I −P) = η(⃗e −⃗e ). In
2 s t
the last part of the proof we will show that such a state always exists for some η >0. We leave the choice of s and t
free for now, but later we will fix it to get optimal bounds. We can show that this metastable distribution maps to
an electrical network that has current η/2 injected at s and extracted at t. Consider the following relations,
(cid:88) (cid:88) (cid:88) (cid:88)
ν(i)− ν(j)P(i|j)= P(j|i)ν(i)−ν(j)P(i|j)= c (V(i)−V(j))= I(i,j) (B5)
ij
j j j j:(i,j)∈E
The above expression is just the total current flowing into i. From the metastability condition on ν we see that
total current flow is conserved at all nodes except s and t. At these nodes current of η/2 is injected/extracted.
Now the strong metastability of the state can be shown to be measured by the total absolute value current flowing
in the system,
(cid:88) (cid:88) (cid:88)
|P(j|i)ν(i)−ν(j)P(i|j)|= c |V(i)−V(j)|= |I(i,j)| (B6)
ij
i,j i,j i,j∈E
Toboundthisquantityweconsiderspheresofincreasingsizecentredaroundthesourceandconsiderthecurrentflow
through their boundaries. We define these spheres using the graph distance d (i,j) which we take to the shortest
E
path length that connects node i and j in the undirected graph G(S,E).
∂B(s,r)={(i,j)∈E | d (s,i)=r, d (s,j)=r+1}. (Boundary sets) (B7)
E E
Notice that the boundary sets are directed. They do not contain both (i,j) and (j,i) edges.
s and t nodes have been unspecified as of now. We take these to the maximally separated nodes in the graph
G(S,E), that is d (s,t)=diam(G) Using this observation in (B6),
E
(cid:12) (cid:12)
diam(G)−1 diam(G)−1(cid:12) (cid:12)
(cid:88) (cid:88) (cid:88) (cid:88) (cid:12) (cid:88) (cid:12)
|P(j|i)ν(i)−ν(j)P(i|j)|≥ |I(i,j)|≥ (cid:12) I(i,j)(cid:12) (B8)
(cid:12) (cid:12)
i,j r=0 (i,j)∈∂B(s,r) r=0 (cid:12)(i,j)∈∂B(s,r) (cid:12)
Now intuitively we expect that the total current flow through every boundary set must be the same due to current
conservation law in (B5). We show this rigorously below. From the conservation law for any 0 < r < diam(G)
(avoiding the source and sink)
(cid:88) (cid:88)
0= I(i,j) (B9)
i∈B(s,r)j:(i,j)∈E
In the above equation the edges connecting nodes inside B(s,r) will not contribute as both I(i,j) and I(j,i) will
existinthesumwhichcancelduetoantisymmetry. Theremainingedgeseitherliein∂B(s,r)or∂B(s,r−1). Hence,
(cid:88) (cid:88)
0= I(i,j)+ I(i,j) (B10)
i,j∈∂B(s,r) (j,i)∈∂B(s,r−1)
(cid:80) (cid:80)
Now the total flow out of the source I(i,j) = I(s,j) = η/2. Using this relation iteratively
(i,j)∈∂B(s,0) (s,j)∈E
in (B10) we get that the current flow through every boundary is the same,15
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:88) (cid:12) (cid:12) (cid:88) (cid:12) (cid:12) (cid:88) (cid:12)
(cid:12) I(i,j)(cid:12)=(cid:12) I(i,j)(cid:12)=...=(cid:12) I(i,j)(cid:12)=η/2 (B11)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)(i,j)∈∂B(s,0) (cid:12) (cid:12)(i,j)∈∂B(s,1) (cid:12) (cid:12)(i,j)∈∂B(s,diam(G)−1) (cid:12)
Using these relation in (B12), we get,
diam(G)−1
(cid:88) (cid:88) (cid:88)
|P(j|i)ν(i)−ν(j)P(i|j)|≥ |I(i,j)|≥diam(G)η/2 (B12)
i,j r=0 (i,j)∈∂B(s,r)
Existence of s-t η− metastable distribution The question remains whether there exists a distribution such that,
ν(I−P)= η(⃗e −⃗e ). We will now show that such a distribution must always exist with some non-zero η.
2 s t
First,observethatifP isanirreduciblechainthentheequilibriumdistributionµistheuniquezeroleft-eigenvectorof
I−P. Moreover, in this case the by Perron–Frobenius thoerem (refer Chapter 8, [23]) µ is a strictly positive vector.
For some η′ >0, consider the system of equations ν′(I−P)= η′ (⃗e −⃗e ). Since there are no-additional constraints
2 s t
on ν′, there is always a solution to this set of equations.
For any ν′ satisfying this ν′+α∗µ any is also a solution to the above linear system. Now we can take α to be a
large enough value such that we hit a positive solution. Thus for any η′, there always exists a positive vector ν′ >0
such that ν′(I−P)= η′ (⃗e −⃗e ). Now given such an η′ and ν′ we can define,
2 s t
(cid:88)
ν(i)=ν′(i)/ ν′(i) (B13)
i
(cid:88)
η =η′/ ν′(i) (B14)
i
Weseethatν isaprobabilitydistributionwhichwillbeanη−metastabledistribution,satisfyingν(I−P)= η(⃗e −⃗e ).
2 s t
Since η′ >0 and ν′ is positive, we have η >0.
Appendix C: Metastable distributions of Curie-Weiss model
Consider the order parameter magnetization per spin, m(σ)= (cid:80) iσi.
n
The Gibbs distribution associated with the Curie-Weiss model can be expressed as,
(cid:88) exp(−nΨ(m))
µ(σ)= µ(σ|m) (C1)
Z
Ψ
m∈{−1,−1+2/n,...,1}
Whereµ(σ|m)istheprobabilityofobservingtheconfigurationσ givenafixedmagnetization. ForCW-typemodels
with permutation symmetry, the probability of observing a certain configuration only depends on the magnetization
and hence this is purely an entropic quantity, µ(σ|m) = δ 1 . The free-energy function, Ψ(m) fixes the
m(σ),m( n )
n(1+m)
2
distribution over the order parameters values. By direct inspection of the CW model Hamiltonian we can see that,
J
Ψ(m)=− m2+hm−S(m). (C2)
2
HereS(m)= 1 log(cid:0) n (cid:1) istheentropyterm. Thisfreeenergyisawell-studiedquantityinstatisticalphysicsand
n n(1+m)
2
definesthecanonicalmodelforthestudyofphasetransitions[28]. Itisalsowell-establishedthattheGlauberdynamics
for this model gets stuck at metastable distributions which are defined by minima of this free energy [32]. Now we
will show that the minima of the free energy also corresponds to distributions that have the same single-variable
conditionals as µ.
To this end, let us explore the following question; Which other permutation invariant distributions will have the
same single variable conditional as µ?
The relevance of this question comes from the fact that learning algorithms like PLE learn the single variable
conditionals, the existence of such a distribution ν that matches µ approximately in single variable conditionals
implies that µ can be learned even if the parameters come from ν.16
From the permutation invariance, we define ν using the following general form,
(cid:88) exp(−nΦ(m))
ν(σ)= µ(σ|m) (C3)
Z
Φ
m
Now use the following relations, µ(σ|m)= µ(σ,m) =µ(σ)δ(m(σ),m)exp(nΨ(m))Z ,
µ(m) Ψ
Z
ν(σ)=µ(σ) Ψe−n(Φ(m)−Ψ(m)) δ . (C4)
Z m(σ),m
Φ
Now remember that σ is just σ with the i−th spin flipped. The single variable conditionals of any distribution
¬i
is simply ν(σ |σ ) = 1 . Hence these conditionals are fixed completely by the ratio of probabilities in the
i \i 1+ν(σ¬i)
ν(σ)
denominator. Let us now compute this ratio for ν. Without loss of generality we assume that σ =1,
i
ν(σ) µ(σ)
= e−n(Φ(m)−Φ(m−2/n)−(Ψ(m)−Ψ(m−2/n)) δ . (C5)
ν(σ ) µ(σ ) m(σ),m
¬i ¬i
Now for the conditionals of these two distributions to approximately match the term in the exponent has to be
close to zero. Enforcing this condition will give us the form of Φ which is the only unknown in the RHS.
To make the calculations easier, define the following finite difference operator of a function,
∆ f(m)=f(m)−f(m−2/n) (C6)
n
Then,
2
∆ m= , (C7)
n n
4m 4
∆ m2 = − , (C8)
n n n2
mk−1
∆ mk =O( ). (C9)
n n
Now we make the assume the following quadratic ansatz for Φ(m), for some a>0 and m ∈[−1,1],
0
−a(m−m )2
Φ(m)= 0
2
We have to determine a and m such that the single variable conditionals match as much as possible.
0
−2a(m−m ) 2a
∆ Φ(m)= 0 + . (C10)
n n n2
Now for Ψ, we expand the free energy up to second order around the (as of yet unknown) point m ,
0
J (m−m )2
Ψ(m)=− m2+hm−S(m )−(m−m )S′(m )− 0 S′′(m )+O((m−m )3) (C11)
2 0 0 0 2 0 0
This gives,
2m 2 2 2 2S′′(m )(m−m ) 2S′′(m )
∆ Ψ(m)≈−J( − )+h −S′(m ) − 0 0 + 0 . (C12)
n n n2 n 0 n n n2
Now equating terms of order 1 from (C10) and (C12) gives us the following relation for finding a,
n2
a=−J −S′′(m ) (C13)
0
Substituting this back in the previous expression and matching terms of 1/n gives us the following expression for
m
0
−Jm +h−S′(m )=0. (C14)
0 017
Now this above expression is precisely the first-order stationarity condition for the free energy of CW model. And
from a>0 we find that Φ′′(m )=J −S′′(m )>0 which tells us that m lies at the minimum of the free-energy.
0 0 0
Thiscalculationestablishesthefollowingimportantobservation: MinimaoftheCWfreeenergydefinesdistributions
whose single variable conditionals match that of the CW distribution
Notice also that Φ(m) defined as in (C) is just the second order Taylor expansion of the free energy Ψ at m . Now
0
starting from this observation we can define a class of potentially interesting metastable states by expanding further
around the positive minima of Ψ.
Φ(K)(m)=(cid:88)K
Ψ(k)(m
)(m−m 0)k
(C15)
0 k!
k=2
e−nΦ(K)(m(σ))
ν(K)(σ)= . (C16)
(cid:0) n (cid:1)
n(1+m)
2
1. Numerical evaluation of strong metastability violation in Curie-Weiss models
For distributions which are permuation invariant,i.e. where ν(σ) is only a function of m(σ), the quantity in (3) can
be computed efficiently my mapping the problem on to the magnetization space first.
First let us fix the Markov chain as Glauber dynamics defined by the Curie-Weiss model in (10). This gives us,
1
Pr(σ |σ)= µ(−σ |σ ) (C17)
¬i n i \i
Now using this the violation in the detailed balance condition as defined in (3) can be rewritten as,
n
1 (cid:88) 1 (cid:88)(cid:88)
|P(σ′|σ)ν(σ)−P(σ|σ′)ν(σ′)|= |µ(−σ |σ )ν(σ)−µ(σ |σ )ν(σ )| (C18)
2 2n i \i i \i ¬i
σ,σ′ i=1 σ
n
1 (cid:88)(cid:88)
= |(1−µ(σ |σ ))ν(σ)−µ(σ |σ )ν(σ )| (C19)
2n i \i i \i ¬i
i=1 σ
n
1 (cid:88)(cid:88)
= |ν(σ)−µ(σ |σ )(ν(σ )+ν(σ))| (C20)
2n i \i ¬i
i=1 σ
n
1 (cid:88)(cid:88)
= ν(σ )|ν(σ |σ )−µ(σ |σ )| (C21)
2n \i i \i i \i
i=1 σ
Where ν(σ )=ν(σ )+ν(σ) is the marginal distribution of all spins except σ
\i ¬i i
(cid:80)
Nowsince|ν(σ |σ )−µ(σ |σ )|=|ν(−σ |σ )−µ(−σ |σ )|. Wecandeducethat ν(σ )|ν(σ |σ )−µ(σ |σ )|=
i \i i \i i \i i \i σ \i i \i i \i
(cid:80)
ν(σ)|ν(σ |σ )−µ(σ |σ )|. This gives us the final expression,
σ i \i i \i
n
1 (cid:88) 1 (cid:88)(cid:88)
|P(σ′|σ)ν(σ)−P(σ|σ′)ν(σ′)|= ν(σ)|ν(σ |σ )−µ(σ |σ )| (C22)
2 2n i \i i \i
σ,σ′ i=1 σ
FortheCurie-Weissmodel,wecaneasilyseethatthesinglevariableconditionalshavethefollowingexpressionthat
depends only on σ and the magnetization of the state,
i
1 J
µ(σ |σ )= (1−tanh(σ (Jm(σ)−h)+ )≡f(σ ,m(σ)). (C23)
i \i 2 i n i
Now we assume the following, permutation invariant form for ν,18
e−n(Φ(m(σ))+S(m(σ))) e−nH(m(σ))
ν(σ)= ≡ (C24)
Z Z
Φ Φ
From this we can get the conditional as,
1 1
ν(σ |σ )= = ≡g(σ ,m(σ)) (C25)
i \i 1+ ν(σ ¬i) 1+exp(n(H(m(σ)−H(m(σ ¬i))) i
ν(σ)
We can use these expressions for conditionals in (C22) and write this completely in the magnetization space. Since
the number of states with a fixed per spin magnetization m is simply given by exp(nS(m)). Now for a given i, a 1+m
2
fraction of these will have σ =1. Using these we can write,
i
1 (cid:88)(cid:88)
ν(σ)|ν(σ |σ )−µ(σ |σ )| (C26)
2n i \i i \i
i=1 σ
n (cid:18) (cid:19)
1 (cid:88) (cid:88) 1+m 1−m
= e−nΦ(m) |g(1,m)−f(1,m)|+ |g(−1,m)−f(−1,m)| , (C27)
2Z n 2 2
Φ
i=1m∈{−1,−1+2,...,1}
n
1 (cid:88)
= e−nΦ(m)|g(1,m)−f(1,m)| (C28)
2Z
Φ
m∈{−1,−1+2,...,1}
n
Where in the last line we have use the normalization condition on the conditionals, g(1,m)+g(−1,m)=f(1,m)+
f(−1,m) = 1. Using this reformulation we can compute the strong metastability of the state in essentially O(n)
(cid:80)
time. The normalization constant can also be computed efficiently using the formula, Z = exp(−nH(m(σ))) =
(cid:80) ϕ σ
exp(−nΦ(m)).
m
The details of numerical evaluation of three different families metastable distributions for the CW model is shown
in FIG.4. First we look at the quartic (K = 4) and quadratic (K = 2) approximations to the CW free energy as
defined in (C16). We perform the expansion around the positive minimum of the CW free energy (m >0). As this
0
corresponds to the mode of the distribution with suppressed probability when h > 0 (refer FIG.2b). For the third
case, we truncate the free energy around m with a certain width,
0
(cid:40)
Φ(m)=
Ψ(m), if |m−m 0|< 4m√0
a (C29)
∞, otherwise
This truncation corresponds to the type of metastable states defined in Section IIA. We see numerically that these
metastable states have an η value that is exponentially small in the system size.
Appendix D: Proof of Theorem 1
Proof. The η−strong metastability condition gives us,
1 (cid:88)
|P(σ′|σ)ν(σ)−P(σ|σ′)ν(σ′)|≤η (D1)
2
σ,σ′∈S
Now let us introduce the notation σ to represent σ with the i-th variable replaced by the alphabet α.
i→α
Now if we consider only one variable transitions of the chain we get,
n
1 (cid:88)(cid:88) (cid:88)
|P(σ |σ)ν(σ)−P(σ|σ )ν(σ )|≤η (D2)
2 u→α u→α u→α
σ∈Su=1α∈[Q],α̸=σi
Since P is a reversible Markov chain, the following relations can be derived from the detailed balance conditions,
P(σ |σ) µ(σ ) µ(α|σ )
u→α = u→α = \u . (D3)
P(σ|σ ) µ(σ) µ(σ |σ )
u→α u \u19
(a) (b) (c)
FIG. 4: Strongly metastable states in the CW model. Here we plot the violation in detailed balance condition as
defined in (3) computed by projecting to the magnetization space as in (C28). (a) Fourth-order approximation to
the free energy at m (b) Second-order approximation (c) Truncated free energy as defined in (C29)
0
After plugging these into (D2) and a bit of algebra gives us,
1 (cid:88)(cid:88)n (cid:88) P(σ |σ)(cid:12) (cid:12)
u→α (cid:12)µ(α|σ )ν(σ)−µ(σ |σ )ν(σ )(cid:12)≤η. (D4)
2 µ(α|σ ) (cid:12) \u u \u u→α (cid:12)
σ∈Su=1α∈[Q],α̸=σi \u
Now we can take the marginal probability ν(σ ) out of the expression with in the summations in the RHS. Then
\u
we can use the definition of conditionals given by: ν(σ i|σ \i)= νν (σ(σ) ), ν(α|σ \i)= ν( νσ (σu→α )), to rewrite it as follows,
\i \i
1 (cid:88)(cid:88)n (cid:88) P(σ |σ) (cid:12) (cid:12)
u→α ν(σ )(cid:12)µ(α|σ )ν(σ |σ )−µ(σ |σ )ν(α|σ )(cid:12)≤η. (D5)
2 µ(α|σ ) \u (cid:12) \u u \u u \u \u (cid:12)
σ∈Su=1α∈[Q],α̸=σi \u
Now applying Condition 1 we can write this as,
1 (cid:88)(cid:88)n (cid:88) (cid:12) (cid:12) η
ν(σ ) (cid:12)µ(α|σ )ν(σ |σ )−µ(σ |σ )ν(α|σ )(cid:12)≤ . (D6)
2 \u (cid:12) \u u \u u \u \u (cid:12) ω
P
σ∈Su=1 α∈[Q],α̸=σi
(cid:12) (cid:12)
Now we assert that the expression (cid:80) (cid:12)µ(α|σ )ν(σ |σ )−µ(σ |σ )ν(α|σ )(cid:12) is lower bounded simply
α∈[Q],α̸=σi(cid:12) \u u \u u \u \u (cid:12)
by |ν(σ |σ )−µ(σ |σ ))|.
u \u u \u
To show this this consider two vectors a,b∈RQ, such that they sum to one, (cid:80) a =(cid:80) b =1. It is simple to see
i i i i
that,
(cid:88) (cid:88) (cid:88) (cid:88)
|a −b |=|a b − a b |=| a b −a b |≤ |a b −a b | (D7)
k k k j j k k j j k k j j k
j∈[n] j∈[n] j∈[Q],j̸=k j∈[Q],j̸=k
Since the conditionals sum to one, as a consequence of the above inequality we can see that,
(cid:12) (cid:12)
(cid:80) (cid:12)µ(α|σ )ν(σ |σ )−µ(σ |σ )ν(α|σ )(cid:12)≥|ν(σ |σ )−µ(σ |σ ))|. Putting this into (D6), we get,
α∈[Q],α̸=σi(cid:12) \u u \u u \u \u (cid:12) u \u u \u20
1 (cid:88)n (cid:88) (cid:12) (cid:12) η
ν(σ )(cid:12)ν(σ |σ )−µ(σ |σ ))(cid:12)≤ . (D8)
2 \u (cid:12) u \u u \u (cid:12) ω
P
u=1σ∈S
Now by splitting the inner sum over σ as one going over σ and another over σ , we can easily deduce that,
u \u
(cid:88)n (cid:88) (cid:12) (cid:12) η
ν(σ)(cid:12)ν(.|σ )−µ(.|σ ))(cid:12) ≤ . (D9)
(cid:12) \u \u (cid:12) TV ω P
u=1σ∈S
Appendix E: Proofs of Learning guarantees
For Ising models, the exact PL estimator and the one computed from samples have the following form
(cid:88)
L(θ )= E L(θ ,σ)= E log(1+exp(−2( θ σ σ +θ ))) (E1)
u u u,k u k u
σ∼ν σ∼ν
k̸=u
M M
L (θ )= 1 (cid:88) L(θ ,σ(t))= 1 (cid:88) log(1+exp(−2σ(t)((cid:88) θ σ(t)+θ ))) (E2)
M u M u M u u,k k u
t=l t=l k̸=u
θˆ := argminL (θ ). (E3)
u M u
||θ u||1≤γ
A key quantity we will use through out this proof is the following measure of curvature of the loss around θ∗
u
δL (∆,θ∗):=L (θˆ )−(L (θ∗)+⟨∆,∇L (θ∗)⟩) (E4)
M u M u M u M u
a. Proof of Theorem 2
Proof. Let,
∆:=θˆ −θ∗, (E5)
u u
∆ :=θˆ −θ∗ (E6)
\u \u \u
L (θˆ )−L (θ∗)=δL (∆,θ∗)+⟨∆,∇L (θ∗)⟩, (E7)
M u M u M u M u
≥δL (∆,θ∗)−|⟨∆,∇L (θ∗)⟩|, (E8)
M u M u
≥δL (∆,θ∗)−||∆|| ||∇L (θ∗)|| (E9)
M u 1 M u ∞
Now to avoid a contradiction with the definition of θˆ as the minimizer of L , the quantity on the left must be
M
negative, i.e. δL (∆,θ∗) ≤ ||∆|| ||∇L (θ∗)|| . Due to the ℓ constraint in the problem we have ||∆|| ≤ 2γ. This
M u 1 M u ∞ 1 1
gives us,
δL (∆,θ∗)≤2γ||∇L (θ∗)|| . (E10)
M u M u ∞
Now we have two technical lemmas that give relevant bounds that can turn the above inequality in to a learning
guarantee.21
From Lemma 3, when M ≥ 8 log(8n2) the following statement holds with probability 1− δ ,
ε2 δ 2n
a
4η
||∇L (θ∗)|| ≤ +ε (E11)
M u ∞ ω a
P
From Lemma 6, when M ≥ 2γ4 log(2n), the following statement holds with probability 1− δ ,
ε2 δ 2n
b
e−4γ 8e−2γη
δL (∆,θ∗)≥ ||∆ ||2 − −ε (E12)
M u 2 \u ∞ ω b
P
Using these bound in (E10) gives us the following bound with probability 1−δ/n,
16ηe2γ (cid:18) 4η (cid:19)
||∆ ||2 − −2e4γε ≤ 2e4γγ +ε . (E13)
\u ∞ ω b ω a
P P
Now make the following choices; choose ε and ε such that 2e4γγε = 2e4γε = ε2/2, and choose M =
a b a b
(cid:108) (cid:109) (cid:16) (cid:17)
27e8γγ4 log(8n2) ≥ max 2γ4 log(2n), 8 log(8n2) . Plugging these choices in the expression above gives us the
ε4 δ ε2 δ ε2 δ
c b a
following bound with probability 1−δ/n.
16ηe2γ 8e4γγη
||∆ ||2 ≤ + +ε2, (E14)
\u ∞ ω ω
P P
16η(1+γ)e4γ
≤ +ε2. (E15)
ω
P
(E16)
This implies that with the same probability,
(cid:115) (cid:115)
16η(1+γ)e4γ (1+γ)η
||θ∗ −θˆ || ≤ ε2+ ≤ ε+4e2γ (E17)
\u \u ∞ ω ω
P P
Now a union bound over each u∈[n] gives us the desired bound in the theorem statement.
1. Technical lemmas for PL estimator without sparsity
a. Gradient concentration
Lemma 2. For an η-strongly metastable ||∇L(θ∗)|| ≤ 4η
u ∞ ωP
Proof.
∂L(θ∗) −2σ σ
u = E u k , (E18)
∂θ uk σ∼ν 1+exp(2σ u((cid:80) k′̸=uθ u∗ ,k′σ k′ +θ u∗))
=−2 E µ(−σ |σ )σ σ , (E19)
u \u u k
σ∼ν
=−2 E (cid:0) µ(−σ |σ )−ν(−σ |σ )(cid:1) σ σ . (E20)
u \u u \u k u
σ∼ν
In the last line, we have used the identity E ν(−σ |σ )σ σ =0 ∀ k ̸=u.
u \u u k
σ∼ν
Now we can bound this gradient directly from Theorem 1,
(cid:12) (cid:12) (cid:12) (cid:12)∂ ∂L θ( uθ ,∗ u k)(cid:12) (cid:12) (cid:12) (cid:12)≤2 σE
∼ν
|(cid:0) µ(−σ u|σ \u)−ν(−σ u|σ \u)(cid:1) | (E21)
4η
≤ . (E22)
ω
P22
Similarly one can also show that,
(cid:12) (cid:12) (cid:12)∂L(θ∗ u)(cid:12) (cid:12)
(cid:12)≤
4η
. (E23)
(cid:12) ∂θ (cid:12) ω
u P
Lemma3. Foraη−stronglymetastabledistributionandε ,δ >0. GivenM ≥ 8 log(2n)guaranteeswithprobability
a a ε2
a
δa
at least 1−δ that,
a
4η
||∇L (θ∗)|| ≤ +ε (E24)
M u ∞ ω a
P
Proof. By direct calculation as done in the proof of Lemma 2,
∂L(θ∗)
u =−2 E µ(−σ |σ )σ σ . (E25)
∂θ u,k σ∼ν u \u u k
Now the variable in the expectation can be easily bounded,
−1≤σ σ µ(−σ |σ )≤1. (E26)
u k u \u
This implies from Hoeffding’s inequality that,
Pr(cid:18)(cid:12) (cid:12) (cid:12) (cid:12)∂ ∂L θ(θ∗ u) − ∂L ∂M θ(θ∗ u)(cid:12) (cid:12) (cid:12) (cid:12)≥ε a(cid:19) ≤2exp(cid:18) −M 8ε2 a(cid:19) . (E27)
u,k u,k
Also a similar inequality holds for the derivative w.r.t θ Now choosing M ≥ 8 log(2(n+1)) gives us the desired
u ε2
a
δa
results by the union bound.
b. Strong convexity-bounds on curvature
It is useful to define the following function on reals,
f(x):=log(1+exp(−2x)). (E28)
Lemma 4. (Smoothnes and Strong convexity of f)
Let, δf(x,ε):=f(x+ε)−(f(x)+εf′(x)). Then if max(|x|,|x+ε|)≤γ,
ε2 exp(−2γ)ε2
≥δf(x,ε)≥ (E29)
2 2
Proof. This lemma can be shown using standard arguments from the strong convexity of f [52]. We can see that for
any |y|≤γ we have,
2 2
f′′(y)= ≥ ≥exp(−2γ). (E30)
1+cosh(2y) 1+cosh(2γ)
From this, it is clear that f is convex in the domain [−γ,γ]. Moreover, the above expression gives that, g(y) :=
f(y) − exp(−2γ)y2 is also a convex function in the same domain. Now from the first-order convexity condition,
2
g(x+ε)≥g(x)+εg′(x), we can find the lower bound on δf.
Similarly, we can see that f′′(y) < 1. This implies that h(y) := y2 −f(y) is also a convex function. From the
2
relation h(x+ε)≥h(x)+εh′(x) we get the upper bound on the δf.
Lemma 5. δL(∆,θ∗)≥ e−4γ||∆ ||2 − 8e−2γη
u 2 \u ∞ ωP23
(cid:80)
Proof. Define local energy E (σ;θ)= θ σ
u k∈Ku k k
n
(cid:88)
L(θ +∆)= E f(E (σ;θ +∆))= E f(E (σ;θ )+E (σ;∆)) (E31)
u u u u u u
σ∼ν σ∼ν
u=1
⟨∇L(θ ),∆⟩= E f′(E (σ;θ
))(cid:88) ∂E u(σ;θ u)
∆ = E f′(E (σ;θ ))E (σ;∆). (E32)
u σ∼ν u u ∂θ uk uk σ∼ν u u u
k̸=u
Now from the definitions in Lemma 4,
δL(∆,θ∗)= E δf(E (σ,θ∗),E (σ,∆)). (E33)
u u u u
σ∼ν
Now from Condition 2 we have, |E (σ,θ∗)| ≤ ||θ∗|| ≤ γ. The same condition will also hold for ||E (σ,θˆ )|| as
u u u 1 u u 1
these constraints are imposed in the optimization.
This implies from Lemma 4,
 
exp(−2γ) exp(−2γ) (cid:88)
δL(∆,θ∗ u)≥
2
σE ∼ν(E u(σ,∆))2 ≥
2
σV ∼a νr  ∆ ukσ k. (E34)
k̸=u
Now for some i̸=u, we can use the law of conditional variances to bound this quantity,
   
(cid:12) (cid:18) (cid:12) (cid:19)
σV ∼a νr  k(cid:88) ̸=u∆ ukσ k≥
σ
\iE
∼ν\i
σi∼V ν(a .|r
σ
\i) k(cid:88) ̸=u∆ ukσ k(cid:12) (cid:12) (cid:12)σ \i=∆2 ui
σ
\iE
∼ν\i
σi∼V ν(a .|r
σ \i)
σ k(cid:12) (cid:12) (cid:12)σ \i (E35)
Now as a consequence of the closeness of conditionals proved in Theorem 1, we can show that the conditional
variance of the metastable state is close to that of the true distribution µ. This is shown in 7. Moreover the
conditional variance w.r.t µ is naturally lower bounded by the finite temperature bound,
(cid:88)
Var [σ |σ ]=1−tanh2( θ∗σ )≥1−tanh2(γ)≥e−2γ. (E36)
i \i ij j
σi∼ν(.|σ \i)
j̸=i
Nowdefineafunctionthatcapturestheerrorincurredinreplacingthemetastabledistributionwiththeequilibrium
distribution in the conditional variance,
(cid:18) (cid:12) (cid:19) (cid:18) (cid:12) (cid:19)
(cid:12) (cid:12)
G(σ \i):= σi∼V ν(a .|r
σ \i)
σ i(cid:12) (cid:12)σ
\i
− σi∼V µ(a .|r
σ \i)
σ i(cid:12) (cid:12)σ
\i
Using this in (E35) we find,
 
(cid:18) (cid:19)
(cid:88)
Var  ∆ ukσ k≥∆2 ui e−2γ − E |G(σ \i)| (E37)
σ∼ν σ∼ν
k̸=u
(E38)
Now by using the result in Lemma 7 we get,
 
(cid:18) (cid:19) (cid:18) (cid:19)
(cid:88) 8η 8η
σV ∼a νr  ∆ ukσ k≥∆2 ui e−2γ − ω
P
=||∆ \u||2 ∞ e−2γ − ω
P
(E39)
k̸=u
(E40)
We have chosen the i here such that ∆2 =||∆ ||2 . Plugging this back in (E34) gives us the desired result.
ui \u ∞24
Lemma6. Foraη−stronglymetastabledistributionandε ,δ ≥0. GivenM ≥ 2γ4 log(1)guaranteeswithprobability
b a ε2
b
δb
at least 1−δ that, δL (∆,θ∗)≥ e−4γ||∆||2 − 8e−2γη −ε
b M u 2 ∞ ωP b
Proof.
M
1 (cid:88)
δL (∆,θ∗)= E δf(E (σ(t),θ∗),E (σ(t),∆)). (E41)
M M σ∼ν u u
t=1
We will bound this using Hoeffding’s inequality. From Lemma 4,
||∆ ||2
0<δf(E (σ,θ∗),E (σ,∆))≤ u 1 ≤2γ2. (E42)
u u 2
Now using Hoeffding inequality,
(cid:18) Mε2(cid:19)
Pr(δL (∆,θ∗)≤δL(∆,θ∗)−ε )≤exp b (E43)
M b 2γ4
So choosing M = 2γ4 log(1) ensures that δL (∆,θ∗)>δL(∆,θ∗)−ε with probability 1−δ . Now using the lower
Mε2
b
δb M b b
bound in Lemma 5 gives us the required lower bound in the lemma.
Lemma 7. (Closeness of conditional variance)
Let µ and ν close in conditionals as defined in Theorem 1. Let f : Rn−1 → R be an arbitrary function. Then the
conditional variances of this random variable under these distributions are also close in the following sense,
(cid:12) (cid:18) (cid:19)(cid:12) (cid:12) (cid:12)
E
ν
(cid:12) (cid:12) (cid:12)f(σ \i) V µar[σ i|σ \i]−V νar[σ i|σ \i] (cid:12) (cid:12) (cid:12)≤ ω8 Pη (cid:12) (cid:12) (cid:12)x∈{−m 1a ,1x }n−1f(x)(cid:12) (cid:12) (cid:12). (E44)
Proof. FirstnoticetheelementaryfactthattheTVupperboundsthedifferenceinanyexpectationvalue,| E x(σ)−
σ∼P
E x(σ)|≤2|P −Q| ||x|| .
TV ∞
σ∼Q
Now for a fixed partial configuration of the spin σ .
\i
Var[σ |σ ]−Var[σ |σ ]=E[σ2|σ ]−E[σ2|σ ] (E45)
i \i i \i i \i i \i
µ ν µ ν
(cid:18) (cid:19)(cid:18) (cid:19)
− E[σ |σ ]−E[σ |σ ] E[σ |σ ]+E[σ |σ ]
i \i i \i i \i i \i
µ ν µ ν
The first term vanishes as σ2 =1. Let |µ(.|σ )−ν(.|σ )| :=δ (σ ). Now using the TV upper bound,
i \i V\u TV i \i
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)V µar[σ i|σ \i]−V νar[σ i|σ \i](cid:12) (cid:12)≤4δ i(σ \i). (E46)
This implies that from Theorem 1
(cid:12) (cid:12) (cid:12) (cid:12)
E ν|f(σ \i)|(cid:12) (cid:12) (cid:12)V µar[σ i|σ \i]−V νar[σ i|σ \i](cid:12) (cid:12) (cid:12)≤4E ν|f(σ \i)|δ i(σ \i)≤ ω8 Pη (cid:12) (cid:12) (cid:12)x∈{−m 1a ,1x }n−1f(x)(cid:12) (cid:12) (cid:12). (E47)
2. Structure learning and recovering magnetic fields
Let θ ={θ ...θ ,θ ,...,θ } be the set of optimization variables associatedwith the edges connected
\u u,1 u,u−1 u,u+1 u,n
to the spin u,
M
θˆ = argmin 1 (cid:88) log(1+exp(−2σ (⟨θ ,σ ⟩+θ )). (E48)
u M u \u \u u
||θ u||1≤γ t=125
First we identify the edges in the true d-sparse graph by the following criterion,
Eˆ ={(u,v)∈[n]×[n] | max(|θˆ |,|θˆ |)>α/2} (E49)
uv vu
(cid:108) (cid:109) (cid:108) (cid:109)
Now choose ε = α/4. This in turn implies that M = 28e8γγ4 log(8n) = 224e8γγ4 log(8n) . This also implies,
ε4 δ α4 δ
(cid:113)
from the condition assumed on α that, ε+4e2γ (1+γ)η ≤ α/2. So from Theorem 1, with probability 1−δ, the
ωP
estimated structure Eˆ matches the true structure of µ. This proves the structure learning result.
Now to recover the magnetic field we solve the following single variable optimization problem,
M
θˆ = argmin 1 (cid:88) log(1+exp(−2σ (⟨ˆθ ,σ ⟩+θ )). (E50)
u M u Eu Eu u
|θu|≤hmax t=1
The same machinery that we used prove Theorem 2 can be used to show that θˆ is close to θ∗.
u u
Define the following loss functions,
M
H (θ )= 1 (cid:88) log(1+exp(−2σ (⟨ˆθ ,σ ⟩+θ )). (E51)
M u M u Eu Eu u
t=1
H(θ )= E log(1+exp(−2σ (⟨ˆθ ,σ ⟩+θ )). (E52)
u
σ∼ν
u Eu Eu u
Also define the curvature of this loss,
∂H(θ )
δH(∆ ,θ )=H(θ +∆ )−(H(θ )+∆ u ) (E53)
u u u u u u ∂θ
u
Now using the same argument that was used to derive (E10), we can show that
δH M(∆ u,θ u∗)≤2h
max(cid:12)
(cid:12) (cid:12) (cid:12)∂H ∂M θ(θ
u∗)(cid:12)
(cid:12) (cid:12) (cid:12). (E54)
u
We will exploit this relation in the same way as done in the proof of Theorem 2.
3. Upperbound on gradient
1
S(x):= (E55)
1+exp(2x)
∂H(θ∗) (cid:16) (cid:17)
u =−2 E σ S ⟨ˆθ ,σ ⟩+θ∗ (E56)
∂θ u σ∼ν u Eu Eu u
Now from Lemma 2
|∂H(θ u∗) |≤2 E (cid:12) (cid:12)S(cid:16) ⟨ˆθ ,σ ⟩+θ∗(cid:17) −S(cid:0) ⟨θ∗ ,σ ⟩+θ∗(cid:1)(cid:12) (cid:12)+ 4η , (E57)
∂θ u σ∼ν (cid:12) Eu Eu u Eu Eu u (cid:12) ω P
4η
≤2|⟨ˆθ −θ∗ ,σ ⟩| sup |S′(x)|+ , (E58)
Eu Eu Eu ω
x∈[−γ,γ] P
4η
≤4||ˆθ −θ∗ || e2γ + , (E59)
Eu Eu 1 ω
P
4η
≤4dεe2γ + . (E60)
ω
P26
(cid:16) (cid:17)
Clearly −1≤σ S ⟨ˆθ ,σ ⟩+θ∗ ≤1. Hence by Hoeffdings inequality,
u Eu Eu u
Pr(cid:18)(cid:12) (cid:12) (cid:12) (cid:12)∂H ∂θ(θ u∗) − ∂H ∂M θ(θ u∗)(cid:12) (cid:12) (cid:12) (cid:12)≥ϵ a(cid:19) ≤2exp(−M 8ϵ2 a) (E61)
u u
Hence with M ≥ 2 log(4), we can ensure with probability at least 1−δ/2 that
ϵ2 δ
a
∂H(θ∗) 4η
| u |≤4dεe2γ + +ϵ (E62)
∂θ ω a
u P
4. Lower bound on curvature
Now the curvature can be bounded using strong convexity arguments as in Lemma 4. For this we first connect δH
to δf,
(cid:16) (cid:17)
δH(∆ ,θ∗)= E f(σ (⟨ˆθ ,σ ⟩+θ ))−f(σ (⟨ˆθ ,σ ⟩+θ∗))−∆ σ f′(σ (⟨ˆθ ,σ ⟩+θ∗)) , (E63)
u u
σ∼ν
u Eu Eu u u Eu Eu u u u u Eu Eu u
= E δf(∆ σ ,θ∗). (E64)
u u u
σ∼ν
Hence from the lower bound in Lemma 4,
e−2hmax e−2hmax
δH(∆ ,θ∗)≥ E (∆ σ )2 = (∆ )2 (E65)
u u 2 σ∼ν u u 2 u
Now from the upper bound in Lemma 4, we have 0<δH(∆ u,θ u∗)≤ ∆ 22 u So from Hoeffding’s inequality,
−8ϵ2 bM
Pr(δH M(∆ u,θ u∗)≤δH(∆ u,θ u∗)−ϵ b)≤e h4 max . (E66)
Hence M ≥ h4 max log(2) ensures that the corresponding lower bound holds w.p greater than 1− δ.
8ϵ2 δ 2
b
5. Magetic field learning guarantee
Nowusingtheboundsongradientandcuravturein(E54)andusingtheunionbound, wehavethefollowingbound
thatholdswithprobability1−δ whenM isgreaterthanmax(cid:16) h4 max log(2), 2 log(4)(cid:17) ,wecanensurewithprobability
8ϵ2 δ ϵ2 δ
b a
at least 1−δ/2 that ,
e−2hmax (cid:18) 4η (cid:19)
∆2 ≤ϵ +2h 4dεe2γ + +ϵ (E67)
2 u b max ω a
P
Now choosing ε 22 h =2e2hmaxϵ b =4h maxe2hmaxϵ a, gives us
(cid:114)
∆ ≤ε +4(cid:112) dεh eγ+hmax +4 ηh maxehmax. (E68)
u h max ω
P
Plugging the definition of ε h in the M values, we see that can be achieved with M
=(cid:108) 27h4
ma εx
4e4hmax(cid:109)
samples.
h