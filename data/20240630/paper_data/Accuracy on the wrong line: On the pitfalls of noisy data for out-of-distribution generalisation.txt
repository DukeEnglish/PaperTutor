Accuracy on the wrong line: On the pitfalls of noisy data for
out-of-distribution generalisation
Amartya Sanyal1, Yaxi Hu1, Yaodong Yu2, Yian Ma3, Yixin Wang4, and Bernhard Schölkopf1
1Max Planck Institute for Intelligent Systems, Tübingen, Germany
2University of California, Berkeley, U.S.A.
3Halıcıoğlu Data Science Institute, University of California San Diego, San Diego, U.S.A.
4University of Michigan, Ann Arbor, U.S.A.
Abstract
“Accuracy-on-the-line” is a widely observed phenomenon in machine learning, where a model’s
accuracy on in-distribution (ID) and out-of-distribution (OOD) data is positively correlated across
differenthyperparametersanddataconfigurations. Butwhendoesthisusefulrelationshipbreakdown? In
this work, we explore its robustness. The key observation is that noisy data and the presence of nuisance
features can be sufficient to shatter the Accuracy-on-the-line phenomenon. In these cases, ID and OOD
accuracy can become negatively correlated, leading to “Accuracy-on-the-wrong-line.” This phenomenon
canalsooccurinthepresenceofspurious(shortcut)features,whichtendtoovershadowthemorecomplex
signal (core, non-spurious) features, resulting in a large nuisance feature space. Moreover, scaling to
larger datasets does not mitigate this undesirable behavior and may even exacerbate it. We formally
prove a lower bound on Out-of-distribution (OOD) error in a linear classification model, characterizing
the conditions on the noise and nuisance features for a large OOD error. We finally demonstrate this
phenomenon across both synthetic and real datasets with noisy data and nuisance features.
1 Introduction
Machine Learning (ML) models often
exhibit a consistent behavior known as
“Accuracy-on-the-line” [22]. This phe- 0.516
nomenon refers to the positive correla- 0.94
tion between a model’s accuracy on In- 0.510
distribution (ID) and OOD data. The 0.92
0.504
positive correlation is widely observed
0.90
across various models, datasets, hyper-
0.498
parameters, and configurations. It sug-
0.60 0.65 0.70 0.930 0.945 0.960
gests that improving a model’s ID perfor- ID Accuracy ID Accuracy
mance can enhance its generalization to
(a) Noisy dataset (b) Noiseless dataset
OODdata. Thisaddressesafundamental
challengeinmachinelearning: extrapolat- Figure 1: Accuracy-on-the-wrong-line behaviour in Noisy dataset
ing knowledge to new, unseen scenarios vs.Accuracy-on-the-line behaviourinNoiselessdatasetinlinearsetting.
byallowingOODperformanceassessment See Section 4 for a description of the setting.
across models without retraining. It also
suggests that modern machine learning does not need to trade off ID and OOD accuracy.
However, does Accuracy-on-the-line always hold? In this work, we explore the robustness of this
phenomenon. Our key observation, supported by theoretical insights, is that noisy data and the presence
1
4202
nuJ
72
]GL.sc[
1v94091.6042:viXra
ycaruccA
DOO
ycaruccA
DOOof nuisance features can shatter the phenomenon, leading to an Accuracy-on-the-wrong-line phenomenon
(Figure 1) with a negative correlation between ID and OOD accuracy. Noisy data is common in machine
learning, as datasets expand and are sourced automatically from the web, introducing label noise through
human annotation [12, 25]. It is also common for modern ML models to obtain zero training error on noisy
training data [46], a phenomenon called noisy interpolation. In this work, we show that when algorithms
memorise noisy data, the correlation between ID and OOD performance can become nearly inverse.
Beyond noisy data, another crucial condition for Accuracy-on-the-wrong-line is the presence of multiple
“nuisance features”, namely features irrelevant to the classification task. Nuisance features are common in
machine learning, as task-relevant features in high-dimensional data frequently lies on a low-dimensional
manifold; see e.g. Brown et al. [8] and Pope et al. [28], This lower intrinsic dimensionality implies that
classification-relevant information is concentrated in a smaller, more manageable subset of the feature space,
rendering the remaining features as “nuisance”.
Even in the direct absence of these nuisance features, this phenomenon can occur due to so-called spurious
features. These are features that are not genuinely relevant to the target task but appear predictive because
of coincidental correlations or dataset biases. Spurious features are often simpler and more easily learned
than non-spurious ones, creating the illusion that data lies on a low-dimensional manifold defined by these
spurious features. This makes other non-spurious features effectively “nuisance”. This illusion often becomes
reality in training and has been observed in a series of works [34, 3, 27, 36]1: Models tend to exploit the
easiestspuriousfeaturesduringtraining, overshadowingthetrue, morecomplexnon-spuriousones. Thisleads
to a large nuisance space that exceeds the true number of nuisance features, as observed in Qiu et al. [29].
A reader might ask: Can scaling
up dataset size resolve the undesirable
Accuracy-on-the-wrong-line phenomenon?
Recent literature on scaling laws [15, 14] 0.70 0.96
as well as uniform-convergence-based re- 0.65 ID 0.94
sults in classical learning theory suggest OOD
0.92
that larger datasets are needed to fully 0.60
benefit from larger models. Even with 0.90 ID
0.55 OOD
noise interpolation, larger datasets usu-
180 240 300 360 100 150 200 250 300
allyimprovegeneralisationerror[5,6,24]. Dataset Size Dataset Size
However, as Figure 2 suggests, we show
(a) Noisy dataset (b) Noiseless dataset
that the answer is no. Scaling can ad-
versely impact OOD error, exacerbating Figure 2: Same setting as Figure 1, increasing dataset size always
the negative correlation between ID and increases ID accuracy irrespective of label noise, but decreases OOD
OODperformance. Asdatasetsizesgrow, accuracy in the presence of label noise.
even a small label noise rate increases the
absolute number of noisy points, significantly impacting OOD error, even if it may not always affect ID error.
Contributions. To summarize, the contributions of this work are as follows: (1) We show that Accuracy-
on-the-wrong-line can occur in practice and provide experimental results on two image datasets. (2) In a
linear setting, we formally prove a lower bound on the difference between per-instance OOD and ID error and
characterise sufficient conditions for it to increase. (3) We argue that these conditions are natural properties
of learned models, especially for noisy interpolation and in the presence of nuisance features.
Related work. Our work builds on the body of work on the Accuracy-on-the-line phenomenon. Miller
et al. [22] first demonstrated empirically that ID and OOD performances exhibit a linear correlation, across
many datasets and associated distribution shifts, while Abe et al. [1] showed a similar positive correlation
for ensemble models. Liang et al. [20] zoomed in on subpopulation shift–a particular type of distribution
shift—and observed a nonlinear "accuracy-on-the-curve" phenomenon. Theoretical analyses by Tripuraneni
et al. [41, 40] supported Accuracy-on-the-line in overparameterised linear models. Baek et al. [4] extended
the concept to "agreement-on-the-line," showing a correlation between OOD and ID agreement for neural
network classifiers. Lee et al. [19] provided theoretical analysis for this phenomenon in high-dimensional
1Thisobservationdatesbacktothe‘urbantanklegend’e.g. seesection8inSchölkopf[33]
2
ycaruccA ycaruccA0.88 0.435
0.488
0.552
0.87 0.420
0.480
0.546
0.86
0.472 0.405
0.540
0.85
0.534 0.464
0.936 0.944 0.952 0.840 0.855 0.870 0.825 0.840 0.855 0.795 0.810 0.825
ID Accuracy ID Accuracy ID Accuracy ID Accuracy
(a) η=0. (b) η=0.15 (c) η=0.2 (d) η=0.25
Figure 3: EachplotshowsOODvsIDaccuracyforvaryinglabelnoiseratesη onthecoloredMNISTdataset. Similar
to Figure 1, the Accuracy-on-the-line phenomenon degrades with increasing amount of label noise.
random features regression. Kim et al. [16] used these insights to develop a test-time adaptation algorithm
for robustness against distribution shift while Eastwood et al. [11] used similar insights to develop a pseudo-
labelling strategy to develop robustness across domain shifts. In contrast to these works, our work focuses on
the robustness of Accuracy-on-the-line, investigating when it breaks and under what conditions.
Other works have also studied the robustness of Accuracy-on-the-line. Wenzel et al. [44] and Teney et al.
[39] empirically suggested that it holds in most but not all datasets and configurations.In particular, Teney
et al. [39] also provided a simple linear example where adding spurious features can differently impact ID
and OOD risks. Kumar et al. [18] theoretically established an ID-OOD accuracy tradeoff when fine-tuning
overparameterised two-layer linear networks. In contrast to all of these works, our work provides both
a theoretical analyses and empirical investigations demonstrating the essential impact of label noise and
nuisance features in breaking Accuracy-on-the-line. Moreover, we state that it leads to a negative correlation
between ID and OOD accuracies, dubbed "Accuracy-on-the-wrong-line." Other lines of work show how label
noise affects adversarial robustness [32, 26] and fairness [45, 43]. However, they are not directly related to
this work.
2 “Accuracy-on-the-wrong-line” in practice
We first show how a combination of practical limitations in modern machine learning can result in Accuracy-
on-the-wrong-line in two real-world computer vision datasets: MNIST [10] and Functional Map of the World
(fMoW) [9]. Then in Section 3, we formalise the necessary conditions and provide a theoretical proof showing
their sufficiency. In Section 4, we conduct synthetic interventional experiments in a simple linear setting to
demonstrate these conditions are indeed sufficient and behave in line with our theoretical results.
2.1 Colored MNIST dataset
We first examine the Colored MNIST dataset, a variant of MNIST, derived from MNIST by introducing
a color-based spurious correlation, where the color of each digit is determined by its label with a certain
probability. Specifically, digits are assigned a binary label based on their numeric value (less than 5 or not),
which is then corrupted with label noise probability η. The color assigned to each digit is thus correlated
with the label with a small probability. A three-layer MLP is then trained on this dataset to achieve zero
training error. The model is subsequently tested on a freshly sampled test set from the same distribution but
without label noise and with a smaller spurious correlation; see Appendix B.3 for more details. The accuracy
on the training distribution is referred to as the ID accuracy, and the accuracy on the test distribution is
referred to as the OOD accuracy.
TheresultsofthisexperimentarepresentedinFigure3. Whentheamountoflabelnoiseislow(Figures3a
and 3b), the ID and OOD accuracy are positively correlated, whereas they become negatively correlated at
higher levels of label noise (Figures 3c and 3d).
3
ycaruccA
DOO
ycaruccA
DOO
ycaruccA
DOO
ycaruccA
DOO0.46
0.60 0.54
0.44 0.57 0.48
0.42 0.54 0.42
0.40 0.51 0.36
0.45 0.60 0.75 0.80 0.84 0.88 0.92 0.6 0.7 0.8
ID Accuracy ID Accuracy ID Accuracy
(a) NoisyInterpolation (b) NoiselessInterpolation (c) Noisywithoutinterpolation
Figure 4: Experiments on the FMoW domain-correlated dataset with label noise. The noisy dataset (left)
shows the Accuracy-on-the-wrong-line phenomenon, while the noiseless dataset (center) shows the Accuracy-
on-the-line phenomenon. When the noisy dataset is not interpolated e.g. due to early stopping (right),
the Accuracy-on-the-line phenomenon persists.
2.2 Functional Map of the World (fMoW) dataset
For the next set of experiments, we use the FMoW-CS dataset designed by Shi et al. [35] based on the
original FMoW dataset [9] in WILDS [17, 30]. The dataset contains satellite images from various parts of the
world and are labeled according to one of 30 objects in the image. Similar to Colored MNIST, FMoW-CS
dataset is constructed by introducing a spurious correlation between the geographic region and the label.
Similar to our previous experiments, we also introduce label noise with a probability of 0.5. For the OOD
test data, we use the original WILDS [17, 30] test set for FMoW. Further details regarding the dataset are
available in Appendix B.4. To obtain various training runs, we fine-tuned ImageNet pre-trained models,
including ResNet-18, ResNet-34, ResNet-50, ResNet-101, and DenseNet121, with various learning rates and
weight decays on the FMoW-CS dataset. We also varied the width of the convolution layers to increase or
decrease the width of each network. In total, we trained more than 400 models using various configurations
and report the results in Figure 4.
Consistent with previous experiments on Colored MNIST, Figure 4a shows that when the data comprises
label noise and training accuracy is 100%, ID and OOD accuracy are inversely correlated. In the absence of
label noise, Figure 4b shows that the two are positively correlated. These two plots only consider models that
fully interpolate the dataset: noisy and noiseless, respectively. To highlight that noisy interpolation is indeed
necessary to break the Accuracy-on-the-line phenomenon, we also plot the experiments for those training
runs where the data is not fully interpolated in Figure 4c. This corresponds to early stopping, stronger
regularizations, as well as smaller widths. Our results show that in this case, the ID and OOD accuracy are
still positively correlated but to a lesser degree than the noiseless setting. We conjecture that this is because
even minimizing the cross-entropy loss on noisy labels contributes to this behavior and is strongest when the
minimisation leads to interpolation.
The results in this section highlight that spurious correlations without label noise are insufficient to
enforce Accuracy-on-the-wrong-line. In Appendix B, we also show evidence (Figure 10) that the presence of
spurious correlations is necessary in these datasets.
3 “Accuracy-on-the-wrong-line” in theory: sufficient conditions
In this section, we present our main theoretical results to isolate the main factors responsible for breaking
the Accuracy-on-the-line phenomenon. First, we define the data distribution µ in Definition 1 and shift
distribution ∆ in Definition 2. The ID error is measured as the expected error on µ and the OOD error is the
expected error on the shifted data i.e. on x+δ where x∼µ and δ ∼∆.
Intuitively, the main property of the distribution µ is that the “signal” and “nuisance” features are
4
ycaruccA
DOO
ycaruccA
DOO
ycaruccA
DOOsupported on disjoint subspaces (S and S respectively) and that the shift ∆ does not affect the signal
d k
features. Then, Theorem 1 lower bounds the difference between OOD and ID error, as a increasing function
of the lower bound on nuisance sensitivity of the learned model. Further, in Proposition 3 , we provide a
theoretical example, where the lower bound on nuisance sensitivity increases with label noise when the label
noise is interpolated. Taken together, Sections 3.3 and 3.4 proves that under high dimensions of nuisance
space, and high label noise interpolation negatively impacts OOD error. We remark that in our theoretical
results, we have avoided defining specific data distribution, distribution shifts, or learning algorithms in order
to show a result on the general phenomenon and highlight the conditions that give rise to it. We leave it to
future work to derive problem and algorithm specific statistical rates of OOD error.
3.1 Data distribution
We model our data distribution µ to have a few signal features and multiple irrelevant (or nuisance) features.
This corresponds to real-world settings where data is usually high dimensional but lies in a low dimensional
manifold. We further simplify the setting by restricting this low-dimensional manifold to the linear subspace
spanned by the first d ∈ Z coordinate basis vectors. Formally, for d,k ∈ Z let S ,S be any two disjoint
d k
subsets of {1...d+k}. Without loss of generality, we assume them to be contiguous i.e. S ={1,...,d} and
d
S ={d+1,...,d+k}.
k
Definition 1. A distribution µ on Rd+k×{−1,1} is called a (S ,S )-disjoint signal distribution with signal
d k
and nuisance support S ,S respectively if there exists a linear separator w ∈Rd+k with its support exclusively
d k
on S and
d
E [I{sign(⟨w,x⟩)̸=y}]=0.
(x,y)∼µ
We define the shift distribution ∆ as only impacting the nuisance features. This corresponds to widely
held assumptions that distribution shifts do not affect the dependence of the label on the signal features. We
define such shifts as S -oblivious shifts in Definition 2.
d
Definition 2. A shift distribution ∆ is called a S -oblivious shift distribution if the marginal distribution
d
∆ on the support S is concentrated fully on 0 , i.e.
Sd d d
∆ (0 )=1.
Sd d
In short, both definitions assume that there are two orthogonal subspaces. For theoretical modelling, we
consider that the signal subspace and the nuisance subspace are exactly disjoint in the standard coordinate
basis. Whilethismaynotholdintheoriginaldataspaceinpractice,thisusuallyholdsinthelatentspaceasa
fewcomponentsinthelatentspacearesufficienttosolvetheproblemathand. Weregardthosecomponentsas
thesignalspaceandtherestasthenuisancesubspace. TheassumptionthattheS -obliviousshiftdistribution
d
has no mass on the signal space reflects a natural assumption about distribution shifts: they do not affect the
causal factors in the data.
3.2 Properties of learned model
The above definitions for ID and OOD data alone do not provide sufficient conditions to break the Accuracy-
on-the-line behavior. These definitions align with realistic settings, such as the sparsity of the true labeling
function and distribution shifts orthogonal to the features in the signal space. Consequently, real-world
experiments on the Accuracy-on-the-line phenomenon already explore these conditions. Therefore, we next
define three conditions on the learned model that we identify as sufficient to break this phenomenon.
Let w ∈ Rd+k be the learned sparse linear classifier with support S. Let µ,∆ be any (S ,S )-disjoint
(cid:98) d k
signal distribution and S -oblivious shift distribution as defined in Definition 1 for some S ,S , and define
d d k
ν =E[∆] as the mean of ∆. Then, we state the following conditions on w.
(cid:98)
Condition C1: “Bounded sensitivity" of wˆ on nuisance subspace assumes there exists M,τ ≥0 s.t.
M ≥ max |wˆ |≥ min |wˆ |≥τ. (C1)
i i
i∈Sk∩S i∈Sk∩S
5Condition C2: “Negative Alignment" of wˆ with mean of shift ν assumes there exists γ >0 s.t.
(cid:80)
wˆ ν
γ ≤− i∈Sk∩S i i . (C2)
∥w ∥
(cid:98)Sk∩S 1
Condition C3: “Small Margin" of wˆ assumes that for all x s.t. ⟨w,x⟩>0, the following holds
(cid:98)
⟨wˆ,x⟩≤τγ|S ∩S|. (C3)
k
3.3 Main theoretical result
Now, we are ready to state the main result. In Theorem 1, we provide a lower bound on the OOD error
corresponding to a fixed x where the randomness is over the sampling of the shift δ ∼∆.
Theorem 1. For any S ,S let D be a (S ,S )-disjoint signal distribution, and ∆ be a S -oblivious shift
d k d k d
distribution where each coordinate is an independent subgaussian with parameter σ.
Then, for any x∈dom(µ) and w ∈Rd+k with support S such that w satisfies Conditions C1, C2, and C3,
(cid:98) (cid:98)
we have Pr (⟨wˆ,x+δ⟩≤0)≥1−e−Γ where
δ
Γ=
|S k∩S|(τγ−C/|Sk∩S|)2
. (1)
2σ2M2
for all x∈dom(µ) where ⟨w,x⟩≥0 and C =max ⟨w,x⟩.
(cid:98) ⟨w(cid:98),x⟩≥0 (cid:98)
Theorem 1 proves that for all (positively) correctly classified points, the probability of misclassification
under the OOD perturbation δ increases with Γ. In particular, Γ scales with the nuisance sensitivity τ and
nuisance density |S ∪S|. Our experiments later show that increasing data size, which leads to lower ID
k
error, in fact increases nuisance density, which as Theorem 1 suggest, leads to larger OOD error.
The proof of the theorem is based on applying a Chernoff-style bound on a weighted combination of
k sub-gaussian random variables; see Appendix A for the full proof. Theorem 1 captures a broad class of
distribution shifts, including bounded and normal distributions. The results can also be extended to other
shifts with bounded moments but we omit them here as they add more mathematical complexity without
additional insights. In particular, under the conditions of Theorem 1, for some σ > 0,ν ∈ Rd+k where ν
satisfies A2, consider either:
• Gaussian Shifts: Each δ for i∈S ∩S is independently distributed as N(ν ,σ2), or
i k i
√
• Bounded Shifts: Each δ for i∈S ∩S satisfies |δ −ν |≤ 3σ.
i k i i
Then, the probability that w misclassifies x which satisfies (C3) under the shift δ is bounded by the same
(cid:98)
expression as in Equation (1).
3.4 Understanding and relaxing conditions
We next argue why these conditions are merely abstractions of phenomena already observed in practice, as
opposed to strong synthetic constraints absent in applications. In addition, we also show how some of these
conditions can be significantly relaxed.
Relaxing Condition (C2) and (C3). Our setting is not restricted to only discussing samples from
one class, balanced classes, or cases where all data points have small margins. In this section, we relax
Condition (C2) and (C3) to allow for imbalanced classes and for some data points to have large margins.
Condition C3 requires that for all data points that are positively classified, the margin of classification is
bounded from above. Note that this is not a limitation of our result. A simple corollary (Corollary 2) states
the proportion of µ for which this holds directly affects the proportion for which the OOD performance is
poor. We use the notation ρ in Equation (C4) to characterise the fraction of the dataset classified positively
6(or negatively, whichever yields a higher ρ) by w with a margin that is less than half of the maximum allowed
(cid:98)
margin.
Condition (C2) requires that the distribution shift should not orthogonal to w. This is not a strict
(cid:98)
requirement, as exact orthogonality of ν with wˆ is a very unlikely setting, and even slight misalignment
will suffice for our result. Here, we show that if the shift distribution is a mixture of multiple components
with a combination of positive and negative alignments, our result extends to that setting. Consider a new
shift distribution ∆′, which is a mixture of two shift distributions ∆ and ∆ with mixture coefficients c
1 −1 1
and c , respectively. Now note that at least one of the two-component shift distributions will likely satisfy
−1
Condition (C2) with w or −w. Assume, ∆ satisfies condition (C2) with γ , and ∆ satisfies the same
(cid:98) (cid:98) 1 1 −1
condition by replacing w with −w for γ .
(cid:98) (cid:98) −1
As shown in Corollary 2, when the distribution becomes more class-imbalanced and a large fraction of
data points have small margins and at least one of the distributions has a large negative alignment γ, the
parameter ρ increases, thereby increasing the probability of misclassification.
Corollary 2. Define S ,S , and µ as in Theorem 1 and ∆′ as described above. Consider any w ∈ Rd+k
d k (cid:98)
with support S such that w satisfies Conditions C1 and C2. Define
(cid:98)
(cid:20) (cid:26) (cid:27)(cid:21)
τγ |S ∩S|
ρ= max Pr I{yˆ⟨w,x⟩≥0}·I yˆ⟨w,x⟩≤ yˆ k . (C4)
yˆ∈{−1,1} x∼µ (cid:98) (cid:98) 2
Then, we have
Pr (⟨wˆ,x+δ⟩≠ ⟨w,x⟩)≥ρ
(cid:88)
c
(cid:18) 1−exp(cid:26) −|S k∩S|τ2γ i2(cid:27)(cid:19)
.
x,δ (cid:98) c i 8σ2M2
i∈{−1,1}
The above result shows how the OOD error adaptively depends on various properties of the learned
classifier and shift distribution. It shows that the OOD error increases with the increase in the density of w in
(cid:98)
thenuisancesubspace,i.e.|S ∩S|,aswellastheratiooftheminimumandmaximumspurioussensitivity τ ,
k M
from Condition C1. Second, the increase in the negative alignment γ increases the OOD error and it depends
on which class has the worse parameters. Finally, we note that 1−ρ upper bounds ID error. Therefore,
while a larger ρ leads to a larger lower bound on the OOD error, it leads to a smaller upper bound on the ID
error— a reflection of the Accuracy-on-the-wrong-line behaviour.
Understanding Condition (C1). Condition (C1) describes the condition that the learned classifier has
moderately large values in its support on the nuisance subspace. We provide an example in Proposition 3 to
show that this naturally occurs when interpolating label noise. Consider a min-ℓ -interpolator that solves the
2
following optimization problem given a dataset (X,Y)∈Rn×d×Rn,
min ∥w∥ s.t. Xw =Y.
w∈Rd 2
Consider a simple linear model with a single signal feature Y = ξ ⊙⟨X,w⋆⟩ where w⋆ = (1,0,...,0).
Here, X is a d-dimensional dataset of size n with signal feature X ∼N(υ1 ,I ) and the remaining d−1
1 n n
nuisance features X ∼ N(0,I ).2 The label noise ξ ∈ Rn follows the distribution π and ⊙ denotes the
2:d n
Hadamard product. When ξ is a Bernoulli random variable on the set {−1,1}, it captures the setting of
uniformly random label flip.
Proposition 3 (Informal). If for d=Ω(logn), some constant C >0, and β ∈(0,1), the noise distribution
π satisfies Pr(cid:2)(cid:13) (cid:13)X+ ξ(cid:13) (cid:13) >C(cid:3) ≥ 1−β, where A+ is the pseudo-inverse of A, then with probability at least
2:d 2
0.9−β, the min-ℓ -interpolator wˆ on the noisy dataset (X,Y) satisfies
2
∥wˆ ∥ =Ω(C).
2:d ∞
2Here,1n denotesanall-onesvectoroflengthn.
745 45 0.88
NNooiissee 0.56
0 00 0. .. .2 12 155 30 30 0.84 0.8
00..11 0.52
00..0055 15 15 0.80 0.6
00..00 160 240 160 240 160 240 160 240 160 240
Dataset Size. Dataset Size Dataset Size Dataset Size Dataset Size.
(a) Nuisance Sensitivity (b) OOD Accuracy (c) ID Accuracy
Figure 5: Figure 5a shows as the amount of label noise increases, nuisance sensitivity as well as the nuisance
density increases faster with larger dataset sizes. This leads to worse OOD accuracy as shown in Figure 5b.
However, ID accuracy still increases with dataset size as shown in Figure 5c.
0.525 0.54 0.70 0.88
0.57
0.510 0.52 0.65 0.84
0.54
0.60
0.495 0.50 0.51 0.80
0.56 0.64 0.72 0.64 0.72 0.80 0.72 0.78 0.84 0.85 0.90 0.92 0.94 0.96
ID Accuracy ID Accuracy ID Accuracy ID Accuracy ID Accuracy
η=0.2 η=0.15 η=0.1 η=0.05 η=0
Figure 6: Accuracy-on-the-line behaviour degrades increasing with increasing amount of label noise.
While Proposition 3 only considers multiplicative label noise for simplicity of the analysis, similar results
also hold for additive label noise models. Proposition 3 also captures the properties of label noise that are
sufficient to increase the sensitivity of the nuisance features. It suggests that a label noise distribution, whose
(noisy) labels are nearly orthogonal to the nuisance subspace (indicated by large C and small β), induces
small nuisance sensitivity. For example, a noiseless setting is equivalent to the noise distribution where
Pr [ξ =1 ]=1. Then, standard concentration bounds on X imply that C must be small while β must
ξ∼µ n 2:d
be large, leading to a vacuous bound. Therefore, Proposition 3 implies that the lower bound on nuisance
sensitivity is much smaller in the noiseless setting.
Next, we verify these properties on a simple learning problem in Section 4. Our experiments show
that increasing dataset size can naturally increase |S ∩S| and τ, thereby leading to a larger OOD error.
k
Conversely, increasing the dataset size will also lead to a lower ID error. Together, this demonstrates an
inverse correlation between ID and OOD error, displaying Accuracy-on-the-wrong-line.
4 Experimental ablation of sufficient conditions in linear setting
In this section, we conduct experimental simulations to corroborate our theory by synthetically varying
conditions (C1), (C2), and (C4) as well as label noise rate and dataset sizes. The data distribution is
300-dimensional with one signal feature and the remain nuisance features, a sparse setting often considered in
the literature; See Appendix B.1 for a detailed discussion of the data distribution. The default label noise
rate is 0.2 unless otherwise mentioned and the default dataset size is 300 unless otherwise mentioned. We
train a ℓ -penalised logistic regression classifier with coefficient 0.1 on varying dataset sizes. In short, our
1
experiments show that all three conditions hold for this learned model in the presence of label noise and
corroborates our theory regarding how these problem parameters affect the OOD and Accuracy-on-the-line
phenomenon.
(C1): Spurious Sensitivity of the Learned Model. We begin by examining the sensitivity of the
learned model w in the nuisance subspace. Condition (C1) states that the non-zero components are bounded
(cid:98)
both from above and below. While regularisation naturally imposes the upper bound, the lower bound is less
common. A key contribution of this work is the demonstration that this occurs under noisy interpolation, i.e.,
models that achieve zero training error in the presence of label noise. Intuitively, when some labels in the
8
ycaruccA
DOO
ytisneD
ecnasiuN
ecnasiuN
naeM
ytivitisneS
ycaruccA
DOO
ycaruccA
DOO
ycaruccA
DI0.72 AAnnggllee
0.9 0.96
00..00
0.8 0.66
0.93 2222..55
0.7 0.60 4455..00
0.90
0.6 Noisy 6677..55
Noiseless 0.54
9900..00
0.00 0.25 0.50 0.75 1.00 0.930 0.945 0.960 0.975 0.64 0.68 0.72
(Assumption A2) ID Accuracy ID Accuracy
(a) OOD Accuracy vs γ (b) Noiseless (c) Noisy
Figure 7: Figure 7a shows that as γ increases, the OOD accuracy decreases for the noisy setting but remains
relatively unchanged for the noiseless setting. Figure 7b shows that a large or small angle (cos(Angle)=γ) does not
have any impact on breaking the Accuracy-on-the-line phenomenon. On the other hand, when angle is close to 0◦ i.e.
w and ν in C2 are fully mis-aligned, the accuracy-on-the-wrong line behaviour is the strongest. The phenomenon
(cid:98)
slowly transforms to Accuracy-on-the-line as the angle approaches 90◦ degrees.
training dataset are noisy, the signal subspace cannot be used to “memorise” them. Consequently, covariates
in the nuisance subspace are necessary to memorise these labels, thereby increasing the magnitude of these
covariates. As the amount of label noise increases, more covariates in the nuisance subspace exhibit this
behaviour. We corroborate this intuition using ℓ -penalised logistic regression in experimental simulations, as
1
shown in Figure 5a.
Figure 5a illustrates that, with higher levels of label noise, the nuisance density and mean nuisance
sensitivity increase more rapidly as the dataset size grows. Theorem 1 predicts that an increase in nuisance
sensitivity leads to poorer OOD accuracy, which is confirmed in Figure 5b (center). However, Figure 5b (left)
demonstrates that this behaviour is not observed in the absence of label noise; OOD accuracy still improves
with an increasing dataset size. Figure 5c reveals that ID accuracy increases with larger datasets, thereby
creating a distinction between the behaviour of ID and OOD accuracy in the presence of label noise. This
distinction underpins the central observation of our paper, as illustrated in Figure 6. For η = 0 (no label
noise), ID and OOD accuracy are linearly correlated as noted in several prior studies [22]. Conversely, as η
increases, the two accuracies become (nearly) inversely correlated, resulting in the Accuracy-on-the-wrong-line
behaviour.
(C2): Mis-alignment of Learned Model with Shift distribution. The next condition on w requires
(cid:98)
that the mean ν of the shift distribution ∆ is misaligned. Mathematically, this requires that the dot product
⟨w,ν⟩ is negative. Intuitively, this ensures that the term ⟨w,δ⟩, where δ ∼∆, is sufficiently negative to flip
(cid:98) (cid:98)
the decision of the classifier. In Theorem 1, the alignment is measured using the quantity γ. In Figure 7a, we
synthetically vary γ (represented by the angle between ν and −w) by controlling the projection of ν on −w
(cid:98) (cid:98)
and evaluate the OOD accuracy in the presence and absence of noise, respectively. The simulation shows
that with increasing alignment between ν and −w, OOD accuracy decreases sharply for the noisy setting but
(cid:98)
remains relatively stable for the noiseless setting.
In Figures 7b and 7c, we show how the Accuracy-on-the-line behaviour is affected by changing the
alignment γ. For the noiseless setting, irrespective of γ, OOD accuracy remains positively correlated with ID
accuracy. However, for the noisy setting, when the alignment is high (i.e., the angle is less than 45◦), we
observe that OOD accuracy is inversely correlated with ID accuracy, but they become more positively
correlated as the angle increases. This highlights the necessity of our second condition, which requires a
misalignment between the shift and the learned parameters. This also highlights that perfect misalignment is
not strictly necessary for breaking Accuracy-on-the-line.
(C4): Significant fraction of points have low margin. The final property of the learned classifier
necessary for the Accuracy-on-the-wrong-line phenomenon is that a significant portion of the distribution,
correctlyclassifiedbyw, hasasmallmargin. ThisiscapturedinC4. Corollary2suggeststhattheprobability
(cid:98)
massofpointsunderdistributionµwhosemargin⟨w,x⟩islessthanγτ|S ∩S|≈γ∥w ∥ isroughlyequal
(cid:98) k (cid:98)Sk∩S 1
to the probability mass of points under µ that are vulnerable to misclassification under the distribution shift.
9
ycaruccA
DOO
ycaruccA
DOO
ycaruccA
DOOPractically, a point classified correctly with a large margin is likely robust to distribution shifts. However, it
is typically the case that not all points are classified with an equally large margin, and some points are closer
to the margin than others. We highlight that as long as this is true, Accuracy-on-the-wrong-line will continue
to hold.
To validate this experimentally, we consider the distribution of ID margin ⟨w,x⟩ for x∼µ and plot its
(cid:98)
CDF in Figure 8 (blue line). Then, we measure the term γτ|S ∩S| and plot it as a vertical red dashed line.
k
The intersection of this red line with the CDF (blue) represents the probability mass of points under µ whose
margin is sufficiently small to be vulnerable to the distribution shift. We plot the empirical OOD error for
this model using the horizontal green dashed line and repeat this experiment for multiple dataset sizes, each
represented in one box in Figure 8. Our simulations clearly show that the theoretically predicted quantity
closely approximates the true OOD error.
0.4
0.2
0.0
0.4
0.2
0.0
0 8 16 0 8 16 0 8 16 0 8 16 0 8 16 0 8 16
ID Margin ID Margin ID Margin ID Margin ID Margin ID Margin
Figure 8: Blue line is the CDF of the ID margin i.e. ⟨w,x⟩ for points x classified positively by w. The red vertical
(cid:98) (cid:98)
line indicates the theoretical quantity in the RHS of Condition (C4) i.e. proportional to τγ|S ∩S|. The green
k
line shows the OOD error. Matching the result of Corollary 2, this simulation shows that the fraction of positively
classified points whose margin is less than τγ|S ∩S| is very close to the true OOD error of that model. Each plot
k
here represents a different run on a different-sized dataset.
5 Implications and conclusion
To summarise, our work argues that interpolation of label noise and presence of nuisance features break the
otherwise positively correlated relationship between ID and OOD accuracy. In support of this argument, we
provide experimental evidence with realistic datasets, theoretical results to isolate the sufficient conditions,
and synthetic simulation to corroborate the theoretical assumptions.
Future Work This raises several questions about the widely prevalent practice of preferring large but noisy
datasets over smaller cleaner datasets. We hope future work will work towards striking the right balance
between size and quality of datasets, keeping in mind their impact on trustworthiness metrics like OOD
accuracy. Proposition 3 shows a sufficient condition for when label noise can increase the nuisance sensitivity;
however this result is restricted to min ℓ interpolators. It is an interesting question to consider what label
2
noise models (e.g. uniform label flip) and inductive biases of the learning algorithm (e.g. min ℓ ) [2, 7] can
p
aggravate or mitigate this phenomenon. It is also interesting to investigate what other factors (e.g. spurious
correlation alone [39]) can lead to similar behaviours. Finally, other works have proposed approaches to
mitigate memorisation of label noise [47, 31], selectively learning signal features [3, 27], unlearning the effect
of manipulated data [13, 38], and improving robustness towards adversarial corruptions [21, 37]. The possible
impact of these techniques on preventing Accuracy-on-the-wrong-line is an interesting line of future work.
10
ytilibaborP
ytilibaborPAcknowledgements
YW is supported in part by the Office of Naval Research under grant number N00014-23-1-2590 and the
National Science Foundation under grant number 2231174 and 2310831. YY is supported by the joint Simons
Foundation-NSF DMS grant #2031899. YM is supported by the NSF awards: SCALE MoDL-2134209,
CCF-2112665 (TILOS), as well as the DARPA AIE program, the U.S. Department of Energy, Office of
Science, and CDC-RFA-FT-23-0069 from the CDC’s Center for Forecasting and Outbreak Analytics.
References
[1] TaigaAbe,EstefanyKellyBuchanan,GeoffPleiss,RichardZemel,andJohnPCunningham. Deepensem-
bles work, but are they necessary? In Conference on Neural Information Processing Systems (NeurIPS),
2022.
[2] MichaelAerni,MarcoMilanta,KonstantinDonhauser,andFannyYang. Stronginductivebiasesprovably
prevent harmless interpolation. In International Conference on Learning Representations (ICLR), 2023.
[3] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv:1907.02893, 2019.
[4] Christina Baek, YidingJiang, Aditi Raghunathan, andJ. Zico Kolter. Agreement-on-the-line: Predicting
the performance of neural networks under distribution shift. In Conference on Neural Information
Processing Systems (NeurIPS), 2022.
[5] Peter L Bartlett, Philip M Long, Gábor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 2020.
[6] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 2019.
[7] Omri Ben-Dov, Jake Fawkes, Samira Samadi, and Amartya Sanyal. The role of learning algorithms in
collective action. 2024.
[8] Bradley CA Brown, Anthony L Caterini, Brendan Leigh Ross, Jesse C Cresswell, and Gabriel Loaiza-
Ganem. Verifying the union of manifolds hypothesis for image data. International Conference on
Learning Representations (ICLR), 2023.
[9] Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
[10] Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine, 2012.
[11] Cian Eastwood, Shashank Singh, Andrei L Nicolicioiu, Marin Vlastelica Pogančić, Julius von Kügelgen,
and Bernhard Schölkopf. Spuriosity didn’t kill the classifier: Using invariant predictions to harness
spurious features. Conference on Neural Information Processing Systems (NeurIPS), 2024.
[12] Benoit Frenay and Michel Verleysen. Classification in the presence of label noise: A survey. IEEE
Transactions on Neural Networks and Learning Systems, 2014.
[13] ShashwatGoel,AmeyaPrabhu,PhilipTorr,PonnurangamKumaraguru,andAmartyaSanyal. Corrective
machine unlearning. arXiv:2402.14015, 2024.
[14] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,
Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically.
arXiv:1712.00409, 2017.
11[15] JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess,RewonChild,ScottGray,
AlecRadford,JeffreyWu,andDarioAmodei. Scalinglawsforneurallanguagemodels. arXiv:2001.08361,
2020.
[16] Eungyeup Kim, Mingjie Sun, Aditi Raghunathan, and Zico Kolter. Reliable test-time adaptation via
agreement-on-the-line. arXiv:2310.04941, 2023.
[17] PangWeiKoh,ShioriSagawa,HenrikMarklund,SangMichaelXie,MarvinZhang,AkshayBalsubramani,
Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian
Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje,
Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild
distribution shifts. In International Conference on Machine Learning (ICML), 2021.
[18] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can
distort pretrained features and underperform out-of-distribution. arXiv:2202.10054, 2022.
[19] Donghwan Lee, Behrad Moniri, Xinmeng Huang, Edgar Dobriban, and Hamed Hassani. Demystifying
disagreement-on-the-line in high dimensions. In International Conference on Machine Learning (ICML),
2023.
[20] Weixin Liang, Yining Mao, Yongchan Kwon, Xinyu Yang, and James Zou. Accuracy on the curve: On
the nonlinear correlation of ML performance between data subpopulations. In International Conference
on Machine Learning (ICML), 2023.
[21] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learning
Representations (ICLR), 2018.
[22] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar,
Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation
between out-of-distribution and in-distribution generalization. In International Conference on Machine
Learning (ICML), 2021.
[23] Kenneth S. Miller. On the inverse of the sum of matrices. Mathematics Magazine, 1981.
[24] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and
Experiment, 2021.
[25] Curtis Northcutt, Anish Athalye, and Jonas Mueller. Confident learning: Estimating uncertainty in
dataset labels. In Journal of Artifical Intelligence Research (JAIR), 2021.
[26] Daniel Paleka and Amartya Sanyal. A law of adversarial risk, interpolation, and label noise. In
International Conference on Learning Representations (ICLR), 2023.
[27] Giambattista Parascandolo, Alexander Neitz, ANTONIO ORVIETO, Luigi Gresele, and Bernhard
Schölkopf. Learning explanations that are hard to vary. In International Conference on Learning
Representations (ICLR), 2021.
[28] Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimen-
sionofimagesanditsimpactonlearning. International Conference on Learning Representations (ICLR),
2021.
[29] GuanWen Qiu, Da Kuang, and Surbhi Goel. Complexity matters: Dynamics of feature learning in the
presence of spurious correlations. In NeurIPS Workshop on Mathematics of Modern Machine Learning,
2023.
12[30] Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar,
Weihua Hu, Michihiro Yasunaga, Henrik Marklund, Sara Beery, Etienne David, Ian Stavness, Wei
Guo, Jure Leskovec, Kate Saenko, Tatsunori Hashimoto, Sergey Levine, Chelsea Finn, and Percy Liang.
Extending the wilds benchmark for unsupervised adaptation. In International Conference on Learning
Representations (ICLR), 2022.
[31] Amartya Sanyal, Philip H. Torr, and Puneet K. Dokania. Stable rank normalization for improved gener-
alization in neural networks and gans. In International Conference on Learning Representations (ICLR),
2020.
[32] Amartya Sanyal, Puneet K. Dokania, Varun Kanade, and Philip Torr. How benign is benign overfitting
? In International Conference on Learning Representations (ICLR), 2021.
[33] Bernhard Schölkopf. Causality for machine learning. 2019. doi: 10.1145/3501714.3501755.
[34] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls
of simplicity bias in neural networks. Conference on Neural Information Processing Systems (NeurIPS),
2020.
[35] YugeShi,ImantDaunhawer,JuliaEVogt,PhilipTorr,andAmartyaSanyal. Howrobustisunsupervised
representation learning to distribution shift? In The Eleventh International Conference on Learning
Representations, 2023.
[36] Sahil Singla and Soheil Feizi. Salient imagenet: How to discover spurious features in deep learning? In
International Conference on Learning Representations (ICLR), 2021.
[37] AmanSinha, HongseokNamkoong, andJohnDuchi. Certifiabledistributionalrobustnesswithprincipled
adversarial training. In International Conference on Learning Representations (ICLR), 2018.
[38] Ryutaro Tanno, Melanie F. Pradier, Aditya Nori, and Yingzhen Li. Repairing neural networks by leaving
the right past behind. In Conference on Neural Information Processing Systems (NeurIPS), 2022.
[39] Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Abbasnejad. Id and ood performance are
sometimes inversely correlated on real-world datasets. In Conference on Neural Information Processing
Systems (NeurIPS), 2023.
[40] Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington. Overparameterization improves robustness to
covariate shift in high dimensions. In Conference on Neural Information Processing Systems (NeurIPS),
2021.
[41] Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington. Covariate shift in high-dimensional random
feature regression. arXiv:2111.08234, 2021.
[42] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science.
2018.
[43] Jialu Wang, Yang Liu, and Caleb Levy. Fair classification with group-dependent label noise. In ACM
conference on fairness, accountability, and transparency (FaccT), 2021.
[44] Florian Wenzel, Andrea Dittadi, Peter Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik Zietlow,
David Kernert, Chris Russell, Thomas Brox, Bernt Schiele, Bernhard Schölkopf, and Francesco Locatello.
Assaying out-of-distribution generalization in transfer learning. In Conference on Neural Information
Processing Systems (NeurIPS), 2022.
[45] Songhua Wu, Mingming Gong, Bo Han, Yang Liu, and Tongliang Liu. Fair classification with instance-
dependent label noise. In Conference on Causal Learning and Reasoning (CLeaR), 2022.
13[46] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Representa-
tions (ICLR), 2017.
[47] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations (ICLR), 2018.
14A Proofs
In this section, we provide the proofs of Theorem 1 and Corollary 2. Then, we state and prove Theorem 5,
which is the full version of Proposition 3.
Theorem 1. For any S ,S let D be a (S ,S )-disjoint signal distribution, and ∆ be a S -oblivious shift
d k d k d
distribution where each coordinate is an independent subgaussian with parameter σ.
Then, for any x∈dom(µ) and w ∈Rd+k with support S such that w satisfies Conditions C1, C2, and C3,
(cid:98) (cid:98)
we have Pr (⟨wˆ,x+δ⟩≤0)≥1−e−Γ where
δ
Γ=
|S k∩S|(τγ−C/|Sk∩S|)2
. (1)
2σ2M2
for all x∈dom(µ) where ⟨w,x⟩≥0 and C =max ⟨w,x⟩.
(cid:98) ⟨w(cid:98),x⟩≥0 (cid:98)
Proof. WLOGconsiderx∈dom(µ)suchthat⟨w,x⟩≥0. Then,definetheeventE :=(cid:8) δ ∈Rd+k :⟨w,x+δ⟩≤0(cid:9)
(cid:98) (cid:98)
for which we need to bound Pr[E]. Decomposing the dot product affected by the shift, ⟨w,x+δ⟩ =
(cid:98)
⟨w,x⟩+⟨w,δ⟩. Given ∆ is S -oblivious, δ contributes only from the coordinates in S ∩S. Then, we can
(cid:98) (cid:98) d k
simplify the probability as
(cid:34) (cid:35)
(cid:88)
Pr[E]=Pr [⟨wˆ,x+δ⟩≤0]=Pr wˆ δ ≤−⟨w,x⟩
δ δ i i (cid:98)
i∈Sk∩S
(cid:34) (cid:35)
(cid:88) (2)
=1−Pr wˆ δ ≥−C
δ i i
i∈Sk∩S
≥1− inf eλC
E(cid:104) eλ(cid:80) i∈Sk∩Swˆiδi(cid:105)
λ≥0
As each δ is independently distributed with subgaussian parameter σ and mean ν , by the properties of
i i
subgaussian random variables, for λ≥0, the moment generating function (MGF) of δ yields
i
E(cid:2) eλδi(cid:3) ≤eλνi+λ2 2σ2
Subsituting this into the Chernoff bound in Equation (2), we obtain
Pr[E]≥1− inf
eλC+λ⟨w(cid:98),ν⟩+λ2(cid:80) i∈Sk∩Sw(cid:98)i2σ2/2
λ≥0 (3)
≥1− inf
eλC+|Sk∩S|(−λτγ+λ2M2σ2)
λ≥0
where the last inequality uses Assumptions A1 and A2. Solving the above optimisation to obtain the optimal
lambda yields
τγ|S ∩S|−C
λ= k
σ2M2
Note that assumption A3 ensures that this term is positive. Substituting this back into Equation (3) and
simplifying the resultant expression yields the following probability bound
(cid:40) (cid:41)
Pr(⟨wˆ,x+δ⟩≤0)≥1−exp
−|S k∩S|(τγ−C/|Sk∩S|)2
.
2σ2M2
Here, we provide a full version of Corollary 2. Specifically, Corollary 2 is a special case of Corollary 4
with c=1/2.
15Corollary 4. Define S ,S , and µ as in Theorem 1 and ∆′ as described above. Consider any w ∈ Rd+k
d k (cid:98)
with support S such that w satisfies Conditions C1 and C2. For any 0≤c≤1 define
(cid:98)
ρ = max Pr [I{yˆ⟨w,x⟩≥0}·I{yˆ⟨w,x⟩≤cτγ |S ∩S|}]. (C4)
c x∼µ (cid:98) (cid:98) yˆ k
yˆ∈{−1,1}
Then, we have
(cid:32) (cid:40) (cid:41)(cid:33)
Pr (⟨wˆ,x+δ⟩≠ ⟨w,x⟩)≥ρ
(cid:88)
c 1−exp
−|S k∩S|τ2γ i2(1−c)2
.
x,δ (cid:98) c i 2σ2M2
i∈{−1,1}
Proof. Let y denote the value of yˆthat achieve ρ , ie.
max c
y = argmaxPr [I{yˆ⟨w,x⟩≥0}·I{yˆ⟨w,x⟩≤cτγ |S ∩S|}].
max x∼µ (cid:98) (cid:98) yˆ k
yˆ∈{−1,1}
For simplicity, we also denote the event I{⟨wˆ,x+δ⟩≠ ⟨wˆ,x⟩} as E . Then, the goal of the proof is to
err
lower bound Pr (E ). Let E denote the event I{y ⟨w,x⟩≥0}·I{y ⟨w,x⟩≤cτγ|S ∩S|}. We
x,δ∼∆′ err max (cid:98) max (cid:98) k
apply law of total probability over the event E and Ec
Pr (E )=Pr (E)Pr (E )+Pr (Ec)Pr (E )
x∼µ,δ∼∆′ err x∼µ x∼µ|E,δ∼∆′ err x∼µ x∼µ|Ec,δ∼∆′ err (4)
≥ρ Pr (E )
c x∼µ|E,δ∼∆′ err
WLOG, we assume y = 1. Then, we rewrite the probability Pr (⟨wˆ,x+δ⟩≠ ⟨wˆ,x⟩) and in-
max x|E,δ∼∆′
voke Theorem 1 to lower bound the term. As y = 1, ⟨wˆ,x⟩ ≥ 0 when event E holds. In other words,
max
⟨wˆ,x⟩≥0forallxinthesupportofµ|E. Hence, bylawoftotalprobabilityoverthethemixturedistributions
∆ and ∆ of the shift random variable δ,
1 −1
Pr (E )=Pr (⟨wˆ,x+δ⟩≤0)
x∼µ|E,δ∼∆′ err x∼µ|E,δ∼∆′ (5)
=c Pr (⟨wˆ,x+δ⟩≤0)+c Pr (⟨wˆ,x+δ⟩≤0)
1 x∼µ|E,δ∼∆1 −1 x∼µ|E,δ∼∆−1
Then,wederivelowerboundsonPr (⟨wˆ,x+δ⟩≤0)andPr (⟨wˆ,x+δ⟩≤0)respectively
x|E,δ∼∆1 x|E,δ∼∆−1
using Theorem 1. By the definition of event E and the fact that y = 1, any x in the support of µ|E
max
satisfies⟨wˆ,x⟩≤cτγ|S ∩S|. WethenemployTheorem1withC =cτγ|S ∩S|andγ =γ tolowerbound
k k +1
Pr (⟨wˆ,x+δ⟩≤0) when ⟨wˆ,x⟩≥0,
x|E,δ∼∆1
(cid:26)
|S
∩S|τ2γ2(1−c)2(cid:27)
Pr (⟨wˆ,x+δ⟩≤0)≥1−exp − k 1 (6)
x|E,δ∼∆1 2σ2M2
Similarly, setting C = cτγ|S ∩S| and γ = γ we can lower bound Pr (⟨wˆ,x+δ⟩≤0) when
k −1 x|E,δ∼∆−1
⟨wˆ,x⟩≥0 by
(cid:26) |S ∩S|τ2γ2 (1−c)2(cid:27)
Pr (⟨wˆ,x+δ⟩≤0)≥1−exp − k −1 (7)
x|E,δ∼∆−1 2σ2M2
Substituting Equation (7) into Equation (5) and then substituting Equation (5) into Equation (4), we obtain
(cid:32) (cid:40) (cid:41)(cid:33)
Pr (⟨wˆ,x+δ⟩≠ ⟨w,x⟩)≥ρ
(cid:88)
c 1−exp
−|S k∩S|τ2γ i2(1−c)2
.
x,δ (cid:98) c i 2σ2M2
i∈{−1,1}
for y =1. The case with y =−1 follows the same analysis.
max max
Now we state the full version of Proposition 3 as Theorem 5 and provide its proof.
16Theorem 5. If d = Ω(logn) and the noise distribution π satisfies Pr [X+ ξ ≥ C] ≥ 1−β for
ξ∼πn,X2:d 2:d
some constant C and β ∈ (0,1). Then, for β ,β ∈ (0,1) and β = O(1/d), with probability at least
1 2 1
0.92−β −dβ −β, the min-ℓ -interpolator wˆ on the noisy dataset (X,Y) satisfies
2 1 2
 
2log n
∥wˆ ∥ ≥0.11− β1 C.
2:d ∞  (cid:16) (cid:113) (cid:17)
(d−1) 1− 6β log β1
2 d−1
Proof. For i ∈ {1,...,d}, let X ∈ Rn denote the ith feature of the data matrix X. For simplicity, let
i
X˜ = X ∈ Rn×(d−1) denote the nuisance covariates, Σ = X X⊤ and Σ˜ = X˜X˜⊤. When d > n, min-ℓ -
2:d 1 1 1 2
interpolator on the noisy dataset (X,Y) has a closed-form solution wˆ =X⊤(cid:0) XX⊤(cid:1)−1 Y. We will show that
the estimated parameters for nuisance features wˆ is lower bounded with high probability.
2:d
wˆ
=(cid:104) X⊤(cid:0) XX⊤(cid:1)−1 Y(cid:105)
2:d
2:d
( =a) X˜⊤(cid:16)
Σ
+Σ˜(cid:17)−1
ξ⊙X
1 1
(cid:32) (cid:33)−1
( =b) X˜T Σ˜−1−
Σ˜−1Σ 1Σ˜−1
(ξ⊙X ) (8)
1+Tr(Σ Σ˜−1) 1
1
Σ˜−1Σ Σ˜−1
=X˜⊤Σ˜−1ξ⊙X −X˜T 1 ξ⊙X
(cid:124) (cid:123)(cid:122) (cid:125)1 1+Tr(Σ 1Σ˜−1) 1
PartI (cid:124) (cid:123)(cid:122) (cid:125)
PartII
where step (a) follows from the definition of Σ ,Σ˜ and Y, and step (b) follows from Lemma 1 [23].
1
Lemma 1 (Inverse of sum of matrices [23]). For two matrices A and B, let g =Tr(BA−1). If A and A+B
are invertible and B has rank 1, then g ̸=−1 and
1
(A+B)−1 =A−1− A−1BA−1.
g+1
We will lower bound Part I and Part II separately using the assumption on the noise distribution µ and
the concentration bound on Gaussian random matrix.
Applying the assumption on the noise distribution and the fact that X˜⊤Σ˜−1 =X+ , we can lower bound
2:d
Part I with probability at least 1−β,
X˜⊤Σ˜−1ξ⊙X =X+ ξ⊙X ≥C⊙X ≥CX . (9)
1 2:d 1 1 1
It remains to upper bound Part II.
(cid:13) (cid:13)
Σ˜−1Σ Σ˜−1 (cid:13) Σ Σ˜−1 (cid:13)
X˜T 1 ξ⊙X ≤X˜⊤Σ˜−1ξ⊙X (cid:13) 1 (cid:13)
1+Tr(Σ 1Σ˜−1) 1 1(cid:13) (cid:13)1+Tr(Σ 1Σ˜−1)(cid:13) (cid:13)
op (10)
(cid:13) (cid:13)
(cid:13) Σ Σ˜−1 (cid:13) (cid:13) (cid:13)
≤C(cid:13) 1 (cid:13) ≤C(cid:13)Σ Σ˜−1(cid:13)
(cid:13)
(cid:13)1+Tr(Σ
1Σ˜−1)(cid:13)
(cid:13)
(cid:13) 1 (cid:13)
op
op
where the last inequality follows from Tr(Σ Σ˜−1)≥0 for positive semi-definite matrices Σ and Σ˜.
1 1
(cid:13) (cid:13)
Then, we derive lower bound and upper bound on the term (cid:13)
(cid:13)
n1Σ 1(cid:13)
(cid:13)
op
and (cid:13)
(cid:13)
(cid:13)(cid:16) n1Σ˜(cid:17)−1(cid:13)
(cid:13)
(cid:13)
,
op
(cid:13) (cid:13) n
(cid:13) (cid:13) (cid:13)n1 Σ 1(cid:13) (cid:13) (cid:13) = n1 X 1⊤X 1 = n1 (cid:88) X 12 i (11)
op i=1
where each X ∼N(0,1).
1i
17Lemma 2 (Tail bound of norm of Gaussian random vector). Let X =(X ,...,X )∈Rn be a vector where
1 n
each X is an independent standard Gaussian random variable, then for some constant C ≥0,
i
(cid:34) n (cid:35)
1 (cid:88)
Pr X2 ≤C ≥1−ne−nC/2
n i
i=1
Thus, with probability 1−β for β ∈(0,1),
1 1
(cid:20) (cid:21) (cid:20) (cid:21)
n 1 2 n
Pr ∥Σ ∥ ≤2log =Pr ∥Σ ∥ ≤ log ≥1−β . (12)
1 op β n 1 op n β 1
1 1
(cid:13) (cid:13)
Let λ i(Σ) denote the ith eigenvalue of a matrix Σ. Then, (cid:13) (cid:13) (cid:13)(cid:16) n1Σ˜(cid:17)−1(cid:13) (cid:13)
(cid:13)
=(cid:16) min iλ i(cid:16) n1Σ˜(cid:17)(cid:17)−1 , we only need
op
(cid:16) (cid:17)
to lower bound min λ 1Σ˜ ,
i i n
(cid:20)(cid:12) (cid:18) (cid:19) (cid:12) (cid:21) (cid:20) (cid:12) (cid:18) (cid:19) (cid:12) (cid:21)
Pr (cid:12) (cid:12) (cid:12)m iinλ i d−1 1Σ˜ −1(cid:12) (cid:12) (cid:12)≤t ≥Pr ∀i,(cid:12) (cid:12) (cid:12)λ i d−1 1Σ˜ −1(cid:12) (cid:12) (cid:12)≤t
(cid:20) (cid:12) (cid:18) (cid:19) (cid:12) (cid:21)
=Pr m iax(cid:12) (cid:12) (cid:12)λ i d−1 1Σ˜ −1(cid:12) (cid:12) (cid:12)≤t
(13)
(cid:34)(cid:13) (cid:13) (cid:115) (cid:35)
( ≥a) Pr (cid:13) (cid:13) (cid:13)d−1 1Σ˜ −I(cid:13) (cid:13)
(cid:13)
≤ 6β 2log (dβ −1
1)
op
(b)
≥ 1−β −(d−1)β
2 1
where the first inequality follows from Weyl’s inequality (Lemma 3), and step (b) follows a corollary of matrix
(cid:113)
Bernstein inequality (Corollary 6) by setting t= 6β log β1 .
2 (d−1)
Lemma 3 (Weyl’s inequality [42]). For any two symmetric matrices A,B with the same dimension,
max|λ (A)−λ (B)|≤∥A−B∥
i i op
i
Corollary 6. Let X ,...,X be independent Gaussian random vectors in Rn with mean 0 and covariance
1 d
matrix I . Then for all t≥0 and β ∈(0,1),
n
(cid:13) (cid:13)1 (cid:88)d (cid:13) (cid:13)  (cid:32) −dt2 (cid:33)
Pr(cid:13)
(cid:13) (cid:13)d
i=1X iXT
i
−I n(cid:13)
(cid:13)
(cid:13)
op
≤t≥1−2nexp
4logn
β
(1+2t/3)
−dβ
Therefore, with probability 1−β −dβ ,
2 1
(cid:13) (cid:13) (cid:13) (cid:13) 2log n
(cid:13)Σ Σ˜−1(cid:13) ≤∥Σ ∥ (cid:13)Σ˜−1(cid:13) ≤ β1 , (14)
(cid:13) 1 (cid:13) 1 op(cid:13) (cid:13) (cid:16) (cid:113) (cid:17)
op op (d−1) 1− 6β log β1
2 d−1
(cid:13) (cid:13)
where the last inequality is obtained by substituting the upper bound for ∥Σ ∥ and (cid:13)Σ˜(cid:13) in Equation (13)
1 op (cid:13) (cid:13)
op
and Equation (12) respectively.
Substituting Equation (14) into Equation (10), we obtain an upper bound on Part II,
X˜T
Σ˜−1Σ 1Σ˜−1
ξ⊙X ≤
2CX 1log βn
1 . (15)
1+Tr(Σ 1Σ˜−1) 1 (d−1)(cid:16) 1−(cid:113) 6β log β1 (cid:17)
2 d−1
18Combining the lower bound on Part I (Equation (9)) and the upper bound on Part II (Equation (15)), we
get the following lower bound in terms of the random vector X,
 
2log n
wˆ ≥1− β1 CX (16)
2:d  (cid:16) (cid:113) (cid:17) 1
(d−1) 1− 6β log β1
2 d−1
Finally, we employ anti-concentration bound of standard normal random variable to lower bound ∥X ∥
1 ∞
to conclude the proof.
By calculation with the cumulative distribution function of the standard normal random variable,
with probability at least 0.92, there exists i such that |X | ≥ 0.1. Therefore, with probability at least
1i
0.92−β −dβ −β,
2 1
 
2log n
∥wˆ ∥ ≥0.11− β1 C.
2:d ∞  (cid:16) (cid:113) (cid:17)
(d−1) 1− 6β log β1
2 d−1
This concludes the proof.
Proof of Lemma 2.
(cid:34) n (cid:35) (cid:34) n (cid:35)
1 (cid:88) (cid:88)
Pr X2 ≤C =Pr X2 ≤nC
n i i
i=1 i=1
≥Pr(cid:2) ∀i,X2 ≤nC(cid:3) =Pr(cid:104) ∀i,X ≥√ nC(cid:105) (17)
i i
(cid:104) √ (cid:105)
=1−Pr ∃i,X ≥ nC
i
≥1−ne−nC/2
where the last inequality follows from Union bound and the tail bound of a standard Gaussian random
variable.
Proof of Corollary 6. We first apply Lemma 2 with C =2logn. That is, with probability at least 1−d, all
β
X ,...,X ∈Rn satisfy
1 d
(cid:20) (cid:21)
n
Pr ∀i∈[d],∥X∥ ≤2log ≥1−dβ.
i 2 β
Applying a standard corollary of Matrix Bernstein inequality3 on X ,...,X concludes the proof.
1 d
B Experimental details
B.1 Data Distribution for simulation using synthetic data
We construct a binary classification task in a 300-dimensional space. The procedure for generating the
training dataset is as follows: Each label y ∈{−1,1} is sampled uniformly at random. The first component
x is sampled from a mixture of two Gaussian distributions with a variance of 0.15, centered at y and 1−y
1
respectively, with mixing proportions of 0.9 and 0.1. As the training dataset size increases, the model’s
ability to learn this feature improves, thereby improving the test accuracy. The remaining 299 dimensions
(x ,...,x ) are drawn from a standard normal distribution with zero mean and a variance of 0.1. They
2 300
3See Theorem 13.5 in the lecture notes: https://www.stat.cmu.edu/~arinaldo/Teaching/36709/S19/Scribed_Lectures/
Mar5_Tim.pdf
19constitute the nuisance subspace, primarily used to memorise label noise. We introduce label noise into
training data by flipping 20% of the labels.
For in-distribution (ID) accuracy evaluation, we generate a fresh set of data points from the initial
distribution, devoid of label noise. Out-of-distribution (OOD) accuracy is evaluated by first constructing the
shift distribution ∆. Assuming w represents the trained linear model, the mean ν of the ∆ distribution for
(cid:98)
each component i for i>2 is set to −0.25sign(w ) with a variance of 10−3. We then simulate a new ID test
(cid:98)i
instance z,y in the usual manner, sample a shift δ ∼∆, and add them z+δ to generate the OOD test point.
All plots related to linear synthetic experiments can be generated in a total of less than two hours on a
Macbook Pro M2.
B.2 Additional results on synthetic linear setting
In this section, we provide new results in Figure 9 where we vary the regularisation strength. The results
show that both ID and OOD accuracy increases with increasing regularisation coefficeint but larger datasets
have a noticeably smaller OOD accuracy in the noisy setting uniformly across all regularisation strengths.
For all other cases, including noiseless OOD and both noisy and noiseless ID, larger datasets perform better.
The results also show that regularisation affects nuisance density and sensitivity as expectedi.e. larger
regularisation leads to lower sensitivity and density. But both the sensitivity and density falls to zero faster
for the noiseless setting compared to the noisy setting.
0.98 DDaattaasseett SSiizzee 0.976
0.80
220000 0.64
0.96
330000 0.968 0.72
440000 0.56 0.94
101 100 101 100 101 100 101 100
Regularisation Coeff. Regularisation Coeff. Regularisation Coeff. Regularisation Coeff.
DDaattaasseett SSiizzee 0.4 50 0.4 50
NNooiissyy
0.2 25 0.2 25
NNooiissyy
0.0 0 0.0 0
101 100 101 101 100 101 100 101 101 100
Regularisation Coeff. Regularisation Coeff. Regularisation Coeff. Regularisation Coeff.
Noisy Noiseless
Figure 9: We show varying the strength of regularisation impacts the ID and OOD accuracy as well as the
spurious sensitivity and density. While both ID and OOD accuracy increases with increasing regularisation
coefficient, larger datasets have a noticeably smaller OOD accuracy in the noisy setting for all regularisation
strengths. In all other settings, larger datasets have a higher accuracy and this is the main factor leading to
the Accuracy-on-the-wrong-line behaviour. Regarding spurious sensitivity and density, for sufficiently large
regularisation both the spurious sensitivity and the density drops to zero much faster for the noiseless setting
than the noisy setting.
B.3 Colored MNIST dataset
As discussed in the main text, this dataset is derived from MNIST by introducing a color-based spurious
correlation. Specifically, digits are initially assigned a binary label based on their numeric value (less than 5
or not), and this label is then corrupted with label noise probability η. To make this set of experiment more
realistic, we use an algorithm that is supposed to be robust to distribution shift. In particular, we construct
domains one with 0.35 and one with 0.7 fraction of the samples with correlated label and colour. Then, we
optimise an average of the losses on these two domains. For test set, the spurious correlation is at 0.1.
20
ycaruccA
DI
ecnasiuN
naeM
ytivitisneS
ycaruccA
DOO
ytisneD
ecnasiuN
ycaruccA
DOO
ecnasiuN
naeM
ytivitisneS
ycaruccA
DI
ytisneD
ecnasiuNA three-layer MLP is then trained on this dataset to achieve zero training error by running the Adam
optimizer for 1000 steps with ℓ regularization. The width of the MLP is varied from 16 to 2048 to generate
2
various runs. The learning rate is set at 0.001. The accuracy on the training distribution is referred to as
the ID accuracy, and the accuracy on the test distribution is referred to as the OOD accuracy. Each set of
runs (multiple seeds etc) was run on a single GPU and took less than 30 minutes for the whole set.
B.4 Functional Map of the World (fMoW) Dataset
The original FMoW dataset [9] contains satellite images from various parts of the world, classified into five
geographical regions: Africa, Asia, America, Europe, and Oceania, and labeled according to one of 30 objects
intheimage. Italsoincludesadditionalmetadataregardingthetimetheimagewascaptured. TheFMoW-CS
dataset is constructed by introducing a correlation between the domain and the label, i.e. only sampling
certain labels for certain domains. We use the domain-label pairing originally used by Shi et al. [35], which
ensures that if the dataset is sampled according to this pairing, the population of each class relative to the
total number of examples remains stable. In this work, we use a spurious correlation level of 0.9, meaning
90% of the training dataset follows the domain-label pairing, while the remaining 10% does not match any
domain-label pairs. The domain-label pairing for FMoW-CS is detailed in Table 1. To simplify the problem
further, we only select five labels instead of all thirty, which is the first in each of the rows.
Similar to our previous experiments, we also introduce label noise with a probability of 0.5. For the OOD
test data, we use the original WILDS [17, 30] test set for FMoW, which essentially creates a distribution
shift by thresholding based on a timestamp; images before that timestamp are ID and images after are OOD.
To obtain various training runs, we fine-tuned ImageNet pre-trained models, including ResNet-18, ResNet-34,
ResNet-50, ResNet-101, and DenseNet121, with various learning rates and weight decays on the FMoW-CS
dataset. We also varied the width of the convolution layers to increase the width of each network. In total,
we trained nearly 400 models using various configurations where each model was trained on a single 48GB
NVIDIA Quadro RTX 6000 with 36 CPUs or a 32GB NVIDIA V100 with 28 CPUs. Each run took between
9 hours and 15 hours depending on problem parameters.
0.450
0.435
0.420
0.48 0.56 0.64
ID Accuracy
Noisy no Spurious correlation
Figure 10: Experiments on the FMoW dataset without spurious correlation shows almost zero correlation
between ID and OOD accuracy.
21
ycaruccA
DOOTable 1: Domain-label pairing for FMoW-CS.
Domain(region) Label
Militaryfacility,multi-unitresidential,tunnelopening,
Asia windfarm,tollbooth,roadbridge,oilorgasfacility,
helipad,nuclearpowerplant,policestation,port
Smokestack,barn,wastedisposal,hospital,water
Europe treatmentfacility,amusementpark,firestation,fountain,
constructionsite,shipyard,solarfarm,spacefacility
Placeofworship,cropfield,dam,tower,runway,airport,electric
Africa substation,floodedroad,bordercheckpoint,prison,archaeologicalsite,
factoryorpowerplant,impoverishedsettlement,lakeorpond
Recreationalfacility,swimmingpool,educationalinstitution,
Americas stadium,golfcourse,officebuilding,interchange,
cardealership,railwaybridge,storagetank,surfacemine,zoo
Single-unitresidential,parkinglotorgarage,racetrack,park,ground
Oceania transportationstation,shoppingmall,airportterminal,airporthangar,
lighthouse,gasstation,aquaculture,burialsite,debrisorrubble
22