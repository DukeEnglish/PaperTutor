Fibottention: Inceptive Visual Representation
Learning with Diverse Attention Across Heads
AliKhaleghiRahimian1 ManishKumarGovind1 SubhajitMaity2 DominickReilly1
ChristianKümmerle1† SrijanDas1† AritraDutta2†
1UNCCharlotte 2UCF
{akhalegh, kuemmerle, sdas24}@charlotte.edu aritra.dutta@ucf.edu
†EqualcontributionasProjectLead
Abstract
Visual perception tasks are predominantly solved by Vision Transformer (ViT)
architectures,which,despitetheireffectiveness,encounteracomputationalbottle-
neckduetothequadraticcomplexityofcomputingself-attention. Thisinefficiency
islargelyduetotheself-attentionheadscapturingredundanttokeninteractions,
reflecting inherent redundancy within visual data. Many works have aimed to
reducethecomputationalcomplexityofself-attentioninViTs,leadingtothede-
velopmentofefficientandsparsetransformerarchitectures. Inthispaper,viewing
throughtheefficiencylens,werealizedthatintroducinganysparseself-attention
strategy in ViTs can keep the computational overhead low. Still, these strate-
giesaresub-optimalastheyoftenfailtocapturefine-grainedvisualdetails. This
observation leads us to propose a general, efficient, sparse architecture, named
Fibottention, for approximating self-attention with superlinear complexity that
isbuiltuponFibonaccisequences. ThekeystrategiesinFibottentioninclude: it
excludesproximatetokenstoreduceredundancy,employsstructuredsparsitybyde-
signtodecreasecomputationaldemands,andincorporatesinception-likediversity
acrossattentionheads. Thisdiversityensuresthecaptureofcomplementaryinfor-
mationthroughnon-overlappingtokeninteractions,optimizingbothperformance
andresourceutilizationinViTsforvisualrepresentationlearning. Weembedour
Fibottention mechanism into multiple state-of-the-art transformer architectures
dedicatedtovisualtasks.Leveragingonly2-6%oftheelementsintheself-attention
heads,FibottentioninconjunctionwithViTanditsvariants,consistentlyachieves
significantperformanceboostscomparedtostandardViTsinninedatasetsacross
threedomains—imageclassification, videounderstanding, androbotlearning
tasks. Wehavemadethecodepubliclyavailableathttps://github.com/Charlotte-
CharMLab/Fibottention.
1 Introduction
Weareinaneraoftransformer-basedarchitectures(largefoundationmodels,e.g. ,GPT[43,44,7],
BERT[19],ALBERT[34],ViT[21],DETR[8],D-DETR[71],CLIP[29],etc.)andtheyoverwhelm
the success of other DNN architectures in many downstream tasks such as object detection and
tracking[22],documentsummarizing[43],languagemodeling[19],videounderstanding[47],and
proteinfolding,tonameafew. Transformers’performancecomesattheexpenseoftrainingthemon
alargecorpusofhigh-qualitydataandextraordinarycomputationalscale. E.g.,thesmallestvariant
oftherecentlargelanguagemodel,Llama-3[2]has8Bparameters;itrequires32GBGPUmemory
to load and 64GB GPU memory to train with state-of-the-art training protocols. With their fast
adoptioninAIandtherapidadvancementoftechnology,therehasbeenagrowingresearchinterestin
Preprint.Underreview.
4202
nuJ
72
]VC.sc[
1v19391.6042:viXradevelopingefficienttransformersthatcanbedeployedandtrainedeffectivelyondiverseedgedevices
ortheInternetofThings(IoT)[1,46,57,42].
Atthecoreofthetransformer-basedarchitecturesarethelayersconsistingofmulti-headself-attention
(MHSA)[30,58,21],inwhichtheinputdata,X ∈RN×dintheformofN inputtokens,eachina
d-dimensionalembeddingspace[4],aremappedvialearnable(d×d)parametermatrices,ortheso-
calledquery,keyandvaluematrices,Q,K,V ∈RN×d. AftersplittingQ,K,V column-wiseintoh
equal-sizedblocks,{Q }h ,{K }h ,{V }h ⊂RN×d/halsoknownasheads,thecomputational
i i=1 i i=1 i i=1
bottleneckofs√uchlayersliesincomputingandprocessingtheunnormalizedself-attentionmatrices,
A =Q K⊤/ d withd =d/h[58,21],whichconsistsofN ×N scaleddotproductsbetween
i i i h h
rowsofQ andK ,respectively. Thevanillavisiontransformer(ViT)[21]architectureisinspiredby
i i
thetransformerencoder[58]. Givenaninputimage,X ∈RH×W×C ofresolutionH ×W,ViTuse
p×ppatchestogenerateN = HW tokens;seeFigure1(a).
p2
Long input sequences characterized by large N contribute to the transformer’s success and pose
quadraticcomputationaloverhead,O(N2)incalculatingtheself-attention. Toharvestefficiency,one
ofthepopularapproachesinvolvesobservingtheattentionmatricesA onlyatasparsesubsetof
i
theirentriesΩ⊂[N]×[N]ofsizes=|Ω|<N2[12,67,5,68,49]. However,thequestforfinding
suitablechoicesforsuchsupportsets,Ω,withafavorabletrade-offbetweenefficacyandefficiency,
isdifficultduetodata,model,andinstancedependenceoftheattentionweightsandtheirsparsity
patterns. Popularsparseattentionstrategiesincludelocalattentionwithslidingwindowsoffixed
windowsize[61,5,45],inwhichonlyinteractionsbetweentokensthatarespatiallyclosetoeach
otherandrandomattention[68,70]areconsidered;[10]usesacertainneighborhoodwhichresembles
diagonalslidingofawindow,[5]usesdilatedwindow.Thefirstwaveofworks[5,67,66,68]propose
toapproximatetheattentionmatrixAbyastructuredsparsematrix. Surprisingly, exceptafew
works[69,70], attemptstodesignefficienttransformersinthevisualdomain(suchasthevision
transformers[21,56])cateredtospecificneedsofvisualrepresentationtasks,remainedrelatively
unaffectedbythesetrendsinthelanguageresearchcommunity.
In this paper, our pursuit of designing an efficient self-attention mechanism for visual tasks is a
combinationoftheknowledgetranslationsfromtheNLPdomainandourexperienceindissecting
self-attentionmechanismsforvisualtasks. Wegainedthefollowingkeyinsights: (i)theprincipal
diagonalinAisindeedtheleastimportantduetothepresenceofredundantinformationinpixel
space;that(ii)structuredsparsity(non-random)isdesirableandtheotherefficientsparsedesigns
[68,49]thatincludedifferentsubandsuper-diagonalsentriesintheattentionmatrixareessential
astheycapturedifferenttokentotokeninteractions; that(iii)inception-likediversity[53]across
theattentionheadsareimportant; andfinally, (iv)visualtasksarevastlydifferentfromlanguage
tasks;althoughnotdesired,visiontaskscanaccountforunderlyingdatasetbiases(e.g.,classification
tasksandvideounderstandingtasksarepronetodifferentdatasetbiases),andtherefore,lowoverlap
betweenattentionheadsisoptimal.
Takentogether,inthiswork,weproposeadilatedslidingwindowedstrategytoextractlocal-level
tokeninformationatdifferentscalesindifferentheadsthateffectivelyboostperformanceonvisual
tasksandcanbeincorporatedintoanyattentionmechanism. First,weproposegeneralsparsesupport
selection mechanism (§3.1) that is equipped with two hyperparameters: (a) a dilation sequence
(f ) ⊂N,whichdeterminesthesequenceofdistancesbetweenindicesoftokensinAthatattendto
n n
eachother,and(b)awindowsize,1≤w ≤N,foragivenattentionhead,A. Buildingonthisgeneral
mechanism,weproposeFibottention—wherethedilationsequenceischosentobetheFibonacci
sequence(§3.2)withamoderatecomputationalcomplexityofO(NlogN). Fibottentioncanbeused
inconjunctionwithstate-of-the-arttransformerarchitecturescateredforvisualrepresentationtasks—
vanillaViT(baseandtiny)[21],CVTbase[63],Swintransformer[37],andTimesFormer[6].
WeextensivelyevaluatetheproposedFibottentiononadiversesetofvisiontasks(§4). Ourfindings
indicatethatViTemployingourproposedFibottentionoutperformtheirbaselinesbyupto+9.4%,
+81%, +6.1% in image classification, video action classification, and robot imitation learning,
respectively. Thisimprovementisachievedbyperformingonlyabout2-6%oflocalinteractionsof
key-querypairsforcomputingself-attention.Furthermore,ourefficientFibottentioncanbeintegrated
intoanyexistingvisiontransformerwithoutsignificantlycompromisingperformance.
22 RelatedWork
Adaptedfromtransformers[58]withcapabilitiesoflong-rangesequencingmodeling,visiontrans-
former(ViT)[21]emergedasapopulararchitectureinvisualunderstandingtasks.WhileViTs[21,56]
particularlythrivedinoutperformingtheirtraditionalcounterpartsinvisualrepresentationtasks,these
architecturessufferfromlargecomputationalrequirements,whichisawell-knownproblemforthe
transformersintendedforlarge-scaledatasets[54].
Theprimarygoalofefficientself-attentiondesignistofindthebestapproximationofQK⊤atthe
expenseofreducedtotalFLOPs(importantly,reducingthequadraticdependenceonN)comparedto
theoriginalself-attention. Theaboveproblemissimilartoastructuredsparserecoveryproblem,
wheretheoriginalmatrixQK⊤isapproximatedbyasparsematrixC(QK⊤)whichneedstohavea
typicalstructureattributedtocapturetheeffectoftheoriginalmatrix,heretheself-attention. Yunet
al.[67]showedthatbymaintainingacertainsparsitypattern,asparseattentionmodelcanuniversally
approximateanysequence-to-sequencefunction.
Inthatattempt,thefirstgenerationofpaperspostulatesthatdiagonalelementsintheattentionmap
and their neighboring inner products of tokens are important [13, 32]. We also note that several
directionshavebeenexploredtowardssparsificationofattentionmatrix[15],eitherindirectlyby
low-rank approximation [28, 60] or directly using selective sampling strategies [68]. In NLP, a
multitudeofworks[25,68,5]concludedthenecessityofglobal-scaletokeninteraction[32]and
local-levelregionaltokeninteractions[32,35]whichresemblesdiagonalslidingofwindow[68]or
dilatedwindow[5]intheattentionmatrixforrobustrepresentationlearning.Insummary,Longformer
[5],BigBird[68],ETC[3],Star-Transformer[25]usesomevariantsofalocal,global,slidingwindow,
dilatedslidingwindowandrandomattentionpattern;seeFigure1-(b). Inthesecondgenerationof
papers,byusingadifferentiablesearch,Shietal.[50]observedthatdiagonalelements,containing
theinteractionsofeachtokentoitselfintheself-attention,QK⊤toberedundantandunnecessary,
and proposed to learn a differentiable attention mask. However, this structured sparsity is less
exploredinViTs,wherethedataishighlyredundantandcontextuallydistinctfromthelanguage
domain. ViTs[21,56],leanedtowardsknowledgedistillation[64],tokenpruningandmerging[37],
neighborhoodattention[27],etc.amongthemostprominentstrategies.ThesearchforbetterViT[56]
architecturesusingthereplicationofglobalandlocal(window-based)tokeninteractionisexplored
bytheuseofconvolutionseitherdirectly[70]orindirectly[69]inducingregionalinductivebias
similartoexistinghybridvisiontransformerarchitectures[17,63]. However, thesearchitectures
exhibitlowerthroughputandlacktherobustnessofViTsinprocessingmultiplemodalities[24].
Adaptive and Dynamic Attention. A few works have considered non-uniform sparse attention
patterns. [62]proposedunstructured,learned,instance-dependentattentionmasks. Thereisalimited
number of works studying varying attention masks or support sets across attention heads. E.g.,
[32]notedthattheperformanceoftransformermodelscanbeimprovedbydisablingattentionfora
subsetofheads. AlthoughLongformer[5]discussestheexplorationofdifferentattentionmasksfor
variousheadsinMHSAanddilatedwindowattentiontechniqueshavebeeninvestigatedinNLP,these
operationshavenotbeenexplicitlyexploredintheiradaptationforvisionapplications[69]. Inthe
visiondomain,SparseViT[11]implementssparsity-awareadaptationtoefficientlyfindtheoptimal
layerwisesparsityconfiguration(orpruning)fordifferentlayers. Recently,iFormer[51]proposed
layersthatmixlocal-levelinformationatdifferentfrequencyrangesbyincludingconvolutionmodules
asparallelheadswhichrelyexclusivelyonversionsmaskedMHSAtoachieveaninception-likeeffect
[53]. Incontrast,wefocusondevelopingconvolution-freearchitecturetoretainitsrobustness.
3 Methodology: DiverseSparseAttentionandFibottention
Shietal.[50]empiricallyobservedthatdiagonalelementsintheself-attention,QK⊤aretheleast
important. Motivatedbythisobservation,andwiththenotionofefficiencyinmind,wedesignan
architecturethatdispensesthediagonalelementsanda(variable)bandofentriesacrossthediagonals
fromeachhead. Instead,ourarchitecturefacilitatesavariabledilationofthewindowineachhead
andcapturesdifferentsubandsuper-diagonalsoftheattentionmatrix.
3.1 SparseAttentionwithWindowedDilationSequences
In the following, we provide a general sparse attention framework that allows formulating and
comparingdifferentsparsitypatternsforattentionheads{A }h . Inparticular,insteadofobserving
i i=1
theattentionmatrixA ateachofitsN2 entries,wecomputeonlythedotproductswhoseindices
i
are supported on a subset Ω ⊂ {1,2,...,N}2, i.e., we can define the sparse attention matrix
3Global for head
Local
(a) (b) (c) (d)
Figure1: (a)Themulti-headself-attention.(b)Ageneralsparseattentioncomputationstrategy.Asequence
ofsparsesupportsets, {Ω }h , whereeachsetselects|Ω| < N2 entriesoftheattentionmatrix. (c)The
i i=1
generalizedmaskingstrategyofFibottentionthatcontrolssparsityofeachattentionmatrixA throughadilated
i
sequence,(f ) ⊂N,andafixedwindowsize,wforeachhead.(d)AnexampleofFibottention.
n n
AΩ ∈RN×N ofthei-thheadcorrespondingtomaskΩas
i
(AΩ
i
)
j,k
=(cid:40) Q( ij √)⊤ dK hi(k) , if(j,k)∈Ω,
(1)
−∞, if(j,k)∈/ Ω;
foranyj,k ∈ [N],whereQ(j) ∈ Rdh andK(k) ∈ Rdh arethej-thqueryvectorandthek-thkey
i i
vectorofthei-thattentionhead,respectively. If⊙denotestheentrywisematrixmultiplication,also
calledHadamardproduct,thiscanbewrittenasAΩ =sign(A )⊙(|A |⊙ι ),whereι ∈RN×N
i i i Ω Ω
is an indicator matrix of the index set Ω that is 1 for indices (j,k) ∈ Ω and −∞, otherwise. In
this work, we study structured support sets that capture both local and global interactions while
ensuringefficientinferenceandtrainingthroughsparsity. Tothisend,weintroducethenotionof
adilationsequence, (f ) ⊂ N, whichdeterminesthesequenceofdistancesbetweenindicesof
n n
tokens that attend to each other. Furthermore, for a given attention head, we fix a window size,
1≤w ≤N,which,independentlyofthedilationsequence,providesanupperboundfortheindex
distancebetweeninteractingtokenindicesinanattentionmatrix.
Giventhesequence,(f ) andparameter,w,wedefinethesupportset,Ω(fn)ofinteractingquery-key
n n w
pairsdilatedby(f ) ofwindowsizewsuchthat
n n
Ω(fn) =(cid:8) (j,k):|j−k|∈{f } ,|j−k|≤w(cid:9) ⊂{1,2,...,N}2.
w n n
WerefertoFigure1-(c)forvisualizingsuchsupportsets;Ω=Ω{fn}representstheeffectivesetof
w
indicesofquery-keypairsforwhichweneedtocalculatethedotproductinagivenattentionheadA .
i
Severaldilationsequenceshavebeenstudiedinbothvisionandlanguagetransformerarchitectures.
Mostcommonly,[12,5,69,26]considereddilationsequences,(f n)
n
=(cn) n∈Nthataremultiples
ofaconstantfactor,c∈N,correspondingtoslidingwindowswithconstantdilationfactorc. While
providing a certain level of efficiency, their attention complexity only reduces from O(Nw) to
O(Nw/c), which is still of order N2 if the window size, W is chosen to be w = O(N). On
the other hand, choosing a small window size, w = O(1) prevents the inclusion of any global
interactions. Dilationpatternsbasedondifferentdilationsequenceshavebeenlessexplored;[35]
studied exponentially dilated sequences giving rise to attention complexities of O(Nlogw) =
O(NlogN). WerefertoourgeneralarchitectureinFigure1(c).
3.2 Fibottention: DiverseSparseAttentionthroughWythoff-FibonacciDilationSequences
Whilesupportsetsderivedfromexponentialdilationsequencesleadtosparseattentionmatrices,
it might happen that crucial query-key interactions are not captured by overly sparse patterns,
deterioratingthequalityoftheresultingMHSArepresentations.
Atthesametime,limitedexperimentalresultsin[32,5,20]indicatethatvaryingsupportsetpatterns
acrossattentionheadscanimprovemodelperformance. Furthermore,state-of-the-artsparseattention
mechanismsaimforadelicatebalancebetweencoveringlocalandglobalinteractions[5,68],anddo
notnecessarilyincludetheinteractionsonthemaindiagonaloftheattentionmatrix[50].
4(a) Multiple Sequence (b) Wythoff Sequence
Figure2: Comparingtheattentionpatternidentityι for(a)multiplessequencef = c·nwith
Ω n
multiples, c = 2,4,6, respectively, and (b) with Wythoff sequence across 3 heads. The central
elementsin(b),shadedinyellowareonlypresentinthemodifiedWythoffsequence.
Motivatedbytheseobservations,wepostulatethatsparseattentionmatriceswithdiversesupport
patternsacrossattentionheadsaredesirableandhavethepotentialtobepartofefficienttransformer
architectures,withoutsacrificingtheeffectivenessofthelearnedrepresentationandmodelaccuracy.
Fibonacci Dilation Sequences. We propose a sparse attention pattern that builds crucially on
(generalized)Fibonaccisequences[52,31]. Thewell-knownFibonaccisequence,(f n) n∈Nisdefined
asthesequenceofintegers(0,1,1,2,3,5,8,13,...)[41]satisfyingthelinearrecurrencerelation
f =f +f , (2)
n+1 n n−1
for each n ≥ 2, where f = 0 and f = 1. Binet’s formula [31] states that the n-th Fibonacci
1 2√ √
numbersatisfiesf =(ϕn−1−ψn−1)/ 5,whereϕ=(1+ 5)/2≈1.618isthegoldenratioand
√ n
ψ =(1− 5)/2. Fromthisformula,itcanbeinferredthatafterinitialslowgrowth,thesequence
growseventuallyexponentiallywithrespecttothebaseϕ.
Similarintegersequencescanbedefinedfromtherecurrence(2)byfixingtheinitialelements,f =
1
a∈Nandf =b∈N.Givenawindowsizew,parameters,a,b∈N,anddenotingthecorresponding
2
generalizedFibonaccisequence,(f ) byFib(a,b),wecandefineacorrespondingsupportsetforan
n n
N ×N attentionmatrixasΩFib(a,b) = (cid:8) (j,k) ∈ {1,...,N}2 : |j−k| ∈ Fib(a,b),|j−k| ≤ w(cid:9) .
w
Anexperimentalablationstudy(seeSection4.2)indicatesthatasimpleFibonacciattentionpattern
canalreadybeadvantageouscomparedtootherdilationsequences.
WythoffArrayanditsProperties. Amongintegersequencesbasedonorder-2linearrecurrence
relations, generalized Fibonacci sequences are attractive for creating attention support sets since
byvaryingaandb,avarietyofintegervaluescanbecoveredwhileretainingthesamelong-term
growth rate (see Lemma 1) as the Fibonacci numbers. Accordingly, we consider the usage of h
different Fibonacci-type sequences, Fib(a ,b ) with different initial values a ,...,a ∈ N and
i i 1 h
b ,...,b ∈ N, giving rise to head-specific attention support sets. Defining also head-specific
1 h
windowsizes,w ,...,w ≤N,weobtainthesupportsetΩ forthei-thattentionheadmatrixA
1 h i i
definedasΩFib(ai,bi)foreachheadindexi=1,...,h.
wi
Withinthisframework,weaimtochoosethesequenceparameters,{a } and{b } suchthatthe
i i i i
following three desiderata are satisfied: (i) the overlap between different attention head support
setsshouldbeminimized,allowingforasemanticspecializationofthecorrespondingheadweights
duringtraining,(ii)theunion,∪h ΩFib(ai,bi)ofsupportsetsshouldbesmalltoretainefficiency,but
i=1 wi
withinthatconstraint,(iii)asmanyrelevantquery-keyinteractionsaspossibleshouldbecapturedby
atleastoneattentionhead.
Asuitable,essentiallyhyperparameter-freechoicecan
bederivedfromtheWythoffarray[40,14,9],which Table 1: Generalized Fibonacci sequences
hadbeenoriginallyintroducedinthecontextofacom- Fib(a ,b )usedbythei-thhead’ssupportset
i i
binatorialgame[65]. TheWythoffarraycanbeconsid- Ω ofFibottention.
i
eredasacollectionofgeneralizedFibonaccisequences
aWyt-m bWyt-m aWyt bWyt
{ foF rib e( aa ci h, ib i ∈)} Ni∈N thaw ti hth avs ep pe rc oifi vac bc lyho ni oce os vea rW i lay pt ,a bn ud tb cW i onyt
-
i 0 i 1 i 1 i 2 3 5 8
1 3 4 7 11 18 29
taineachintegerexactlyonce[40,14]. Inparticular,
2 4 6 10 16 26 42
the i-th row sequence of the Wythoff array is given
3 6 9 15 24 39 63
bythesequence,Fib(aWyt,bWyt)withinitialelements,
4 8 12 20 32 52 84
i i
aWyt =⌊⌊iϕ⌋⌋andbWyt =⌊⌊iϕ⌋ϕ2⌋;seeTable1.
i i
5Fibottention. Based on the above considerations, we define a general, novel, sparse attention
mechanism,calledFibottention,thatisdesignedasadrop-inreplacementoffullself-attentionin
multi-headself-attentionblocks.
InanygivenMHSAlayerwithhheads,foragivenheadindexi∈[h],werestrictthecomputationof
unnormalizedattentionweightsinA ∈RN×N tothesupportset
i
Ω
i
:=ΩF wi ib(aW iyt,bW iyt) =(cid:8) (j,k):|j−k|∈Fib(aW
i
yt,bW
i
yt),|j−k|≤w i(cid:9) , (3)
wherethewindowsizew ofthei-thheadisbasedontwomodel-widehyperparametersw and
i min
w , which are chosen based on insights into the modality of the task and the data distribution.
max
(cid:106) (cid:107)
Specifically,wechoosew ibasedontheformula,w i =w min+ wma hx −− 1wmin(i−1) fori=1,...,h,
whichlinearlyinterpolatesbetweenw ≤N,theminimalwindowsizeboundacrossheads,and
min
the maximal window size bound across heads w satisfying w ≤ w ≤ N. The resulting
max min max
spacingofwindowsizesacrossheadsisdesignedtofurtherdiversifytherepresentationslearned
acrossheadsas,inthecaseofalargedisparitybetweenw andw ,headswithlowerindicesiare
min max
biasedtolearntoencodemorelocalinformation,whereasheadswithw ≈w arebiasedtowards
i max
incorporatingmoreglobalinteractions. Following(1),wedefinetheresultingsparseattentionmatrix
asAΩi =A ⊙ι .
i i Ωi
For transformer architectures with several MHSA layers, we further require that the head-wise
supportsetsareshuffledalongthelayersothatthei-thheadusesthesets,{Ω ,...,Ω }within
π(1) π(h)
Fibottention,whereπ :[h]→[h]isarandompermutationfunction(fixedforeachlayer). Werefer
toAppendixBforaformaloutline.
FibottentionBasedonModifiedWythoffArray. Whileweobservedexcellentperformanceof
vanillaFibottentioninimageclassificationtasks(seeSection4.1),itsperformancedegradesintasks
inotherdomainsduetoitshighdegreeofsparsity, whichmightnotalwayscapturewellenough
importantlocalinteractions. Forsuchcases,weproposeavariantofthissparseattentionmechanism
thatincludestwopredecessorsequenceelementsintoeachWythoffrowsequenceFib(aWyt,bWyt);
i i
following the recurrence (2), we can define new initial sequence elements bWyt-m = bWyt −aWyt
i i i
andaW
i
yt-m = aW
i
yt −bW
i
yt-m andsupportsetsΩ
i
= Ω wFi ib(aW iyt-m,bW iyt-m) foreachheadindexi. Unlike
the original Wythoff array, it is not the case anymore that the resulting sequences contain each
integer only at most once [40, 14]; on the other hand, it can be guaranteed that this modified
Fibottention shares each query-key interaction pair only across at most three heads [14]. The
differencesintheresultingsupportsetpatternsarevisualizedinFigure2andTable1. Wereferto
AppendixBforacomprehensiveoutlinethatincludesbothvariantsofFibottention. Apseudo-code
ofFibottentionusingWythoffandmodifiedWythoffisprovidedinAppendixB.
Inthesupplementarymaterial,weprovideaproofthatthehead-wisecomputationaleffortforboththe
standardandthemodifiedvariantsofFibottentionrequiresthecomputationofonlyO(Nlog(w ))
max
dotproducts;seeLemma2.
4 Experiments: FibottentionforImageClassification
Inthisexperiment,weevaluatetheeffectivenessofFibottentionacrossvariousimageclassification
tasks,demonstrateitsrobustnesswithdifferentVisionTransformers(ViTs),andperformablationsto
justifythedesignchoicesofFibottention.
Datasets. Forimageclassificationtasks,weuseCIFAR-10(C10)[33],CIFAR-100(C100)[33]and
ImageNet-1K(IN-1K)[18]. Additionally, weutilizeatinyversionofIN-1K,referredtoasTiny
ImageNet(Tiny-IN),whichconsistsof200classessampledfromtheIN-1Kdataset. Forevaluation,
wereporttheTop-1imageclassificationaccuracyforallthedatasets.
Training.FortrainingFibottentionwithViTs,weusethetrainingrecipeofDeiT[56].Allourmodels
aretrainedfromscratchusingViT-Base(ViT-B)unlessotherwisestated. Thehyper-parametersw
min
andw inFibottentionaresetto5and65respectively,unlessotherwisestated. Notethatweadd
max
allclasstokeninteractionstotheΩ inourFibottentionimplementation. Allthemodelsaretrained
i
for100epochswithaneffectivesizeof64using4A6000mid-gradeGPUs. NotethatforC10and
C100,theinputimagesareresizedfrom32×32to224×224beforebeingfedtotheViTs.
6Table 2: ViT-B, BigBird, and Fibot- Table3: ExploringtherobustnessofFibottention(Ours)in-
tentionforimageclassification. corporatedintodifferentViTvariantsforimageclassification.
Method C10 C100 Tiny-IN IN-1K ViT-B[56] ViT-T[56] Swin-B[37] CVT-B[63]
Dataset
Vanilla +Ours Vanilla +Ours Vanilla +Ours Vanilla +Ours
ViT-B 83.5 59.3 71.9 75.5
+BigBird 86.3 62.6 71.0 71.5 C10 83.5 89.5 75.4 76.5 81.2 80.9 91.4 91.2
+Fibottention 89.5 64.9 74.1 74.2 C100 59.3 64.8 53.4 54.0 61.0 60.4 67.5 67.3
4.1 Results
Table2presentsacomparisonofFibottentionwithrepresentativebaselines,namelyVanillaViT[56]
andBigBird[68]. WeadaptBigBirdwithinViTsusingslidingwindowattentionwithawidthw =4.
IncontrasttoNLPapplications,wherew =1isapproximately192foraninputsequenceof1024,we
findthatw =4isanoptimalchoiceforvisiontasks. Fibottention consistentlyoutperformsVanilla
ViTby+7.1%,+9.4%and+3%ontheC10,C100,andTiny-INdatasetsrespectivelybymasking98%
oftheself-attentionheadinteractionpairs. Also,ViTintegratedwithFibottentionperformsonpar
withthebaselineViTonthelarge-scaleIN-1Kdataset,whilesignificantlyoutperformingBigBird.
ThisdemonstratesthatViTsuseredundantinformationacrossself-attentionmatrices{A }h ,and
i i=1
thatonly2%oftoken-tokeninteractionsaresufficientandeffectiveforvisualrepresentationlearning.
TheaccuracyimprovementsobtainedbyFibottentioninthesedatasetsareattributedtotheinductive
biasintroducedintotheViTsbylocalizingtheattentionheadmatrices{A }h withinarangeoftheir
i i=1
diagonalelements,withamaximumoffsetofw =65,andthediversityoflearnedrepresentations
max
throughdisjointmasking.
In Table 3, we present the results of Fibottention implemented within various ViT architectures:
ViT-Base(ViT-B),ViT-Tiny(ViT-T),Swinbase[37](Swin-B),andCVTbase[63](CVT-B).The
improvementinViT-TislesspronouncedcomparedtoViT-B,attributedtoViT-T’slowerembedding
dimension and reduced parameterization. Despite Swin and CVT incorporating inductive biases
throughtokenmergingandconvolutionaloperations,respectively,Fibottentionachievesperformance
onparwiththeirbasemodelsbymaskingsubstantialkey-querypairsofattentionvalues,respectively.
ThisshowstherobustnessofFibottentionacrossdifferentViTvariants.
4.2 AblationStudies
Inthissection,weillustratetheeffectivenessofeachdesignchoiceinFibottentiononC10andC100.
Exploring different window sizes. In Figure 3, we plot the classification accuracies of ViT-B
modifiedbyFibottentionfordifferentchoicesofthewindowsizehyperparametersw andw .
min max
Weobservethatincorporatinglocalinformationbyincreasingw =w =1to20,whichresults
min max
inw = w = w beingfixedacrossalltheheads,enhancesclassificationaccuracy. However,
i min max
extendingtheinteractiontomoredistanttokensresultsinadecreaseinaccuracy. Interestingly,our
findingssuggestthatthetokenrepresentationslearnedwithvaryingwindowsizesarecomplementary
and specific to their locality. This is further confirmed by an experiment where a test sample
predictionisdeemedaccurateifatleastonemodelcorrectlypredictsthegroundtruth. Thisapproach
achievesamaximumaccuracyof94.1%,indicatingthateachwindowsizecontributesuniqueand
complementaryinformationtothetokenrepresentationsinViTs.
Selecting dilation patterns to compute attention. In Table 4, we explore various options for
alternativedilationsequences(f ) (withwindowsizew =w =w =N/3forallattention
n n i min max
heads)includingthepopularpowersof2[35],forselectingdiagonalindicesΩ via(3),ifwefix
i
thesequenceacrossallheads. Ourfindingsrevealthatthe(standard)FibonaccisequenceFib(1,1)
emergedasthemosteffectiveoption. Thiscanbeattributedtoitsslowprogression,whichfacilitates
agreaterfocusonlocalinteractionswhilestillincorporatingalimitedextentofglobalinformation.
Masking Ratio (%)
97 81 63.5 35.2 15.4 3.8 0
90 Table4: Ablationofsequencefunctions.
C10
85
Sequences CIFAR-10 CIFAR-100
80
70 Powersof2(2nforn∈N) 86.1 61.6
65 Powersof3(3nforn∈N) 85.3 60.2
60 Squareseries(n2forn∈N) 85.8 61.5
C100
55 Cubeseries(n3forn∈N) 84.6 59.1
2 20 40 80 120 160 196
wmin StandardFibonacciFib(1,1) 87.3 63.2
Figure3: Differentwindowsizes.
7
)%(
ycaruccAMasking Ratio (%)
96.29 98.01 98.74
90 90
Wythoff Baseline
85 85 90
variable N/3
80 80 80
70 70 70
fixed N
65 65 65
60 60 60
55 55 55
2 4 6 8 2 4 6 8 1 2 3 4 5 6 7 8 9 10
wmin wmin
(a) (b) (c)
Figure4:Ablationstudyof(a)impactofdilationwithsequences(f n) n∈N =(c·n) n∈N(multiplesofc)
fixed(dashedlines)andvariable(continuouslines)acrossheadswherew =5h andh∈{1,...,12},
i i
(b)choiceofw maxwithsequences(f n) n∈N =Fib(w min,2·(w min))wherew max =N (dashedlines)
andw =N/3(continuouslines)fixedacrossallheads,and(c)withvariabledilationsequences
max
suchthatFib(i+δ,i+δ)forthei-thheadwherei∈{1,...,12},withvaryingδ,vs. Wythoff. The
blueandgreenlinesindicatetheperformanceonC10andC100respectively.
Impactofdilation. InFigure4a,wepresentaplotofaccuracyversusw forFibottentionusing
min
asequenceofmultipleofw fixedacrossallheads(indicatedbydottedlines)andusingconstant
min
offsetsinthesequence,whichvaryacrossdifferentheads(indicatedbysolidlines). Weobservethat
employingvarieddilatedsequencesincomputingtheattentionmatrixacrossdifferentheadsenhances
thediversityoftheheads,therebyenablingthecaptureofcomplementarylocalinformationinViTs
evenwithahighermaskingratio;forw =2,themaskingratiois85.5%comparedto72.8%in
min
experimentswithfixedsequencesacrossheads.
Choiceofw . InFigure4b,wepresenttheaccuracyversusw forFibottentionusingw =
max min max
N/3andN. Ourfindingsindicatethatthehyper-parameterw =N/3isoptimal,astheregionsof
max
interestinimageclassificationtasksaretypicallyconfinedtolocalizedareaswithintheimagespace.
Consequently,interactionswithdistanttokenscanintroducenoisetotherepresentationofrelevant
objectsintheimages,thusw =N/3helpsfocusattentiononmorepertinenttokeninteractions.
max
Why Wythoff? In Figure 4c, we present the accuracy versus δ, where δ represents the shift in
the initial sequence values of Fib(i + δ,i + δ) of the attention support of the i-th head. The
window hyperparameters are fixed at w = 5 and w = N/3. Interestingly, by employing
min max
WythoffFibonaccisequences,whichencapsulatethemaximumnon-overlappingtokeninteractions
acrossheadsforcomputingtheattentionmatrix,weachievethemaximumaccuracy. Thisapproach
eliminates the need for the extra hyperparameter δ, simplifying the model configuration while
maintainingperformance.
5 Experiments: FibottentioninOtherVisualDomains
TodemonstratethegeneralizabilityofFibottention,weintegrateitintoViTsdesignedforothervisual
perceptiontaskssuchasvideoclassificationandrobotlearning.
5.1 FibottentionforVideoActionClassification
Datasets. WeevaluateFibottentionusingthreeactionrecognitiondatasets: ToyotaSmarthome[16],
Northwestern-UCLAMultiviewActivity3D(NUCLA)[59],andNTURGB+D(NTU)[48]. The
ToyotaSmarthomedatasetcomprises∼16Kvideosacross31classes. Forevaluation,weadhereto
thecross-subject(CS)protocolandthecross-view(CV2)protocol. TheNUCLAdatasetconsistsof
∼1.2Kvideoclipswithsubjectsperforming10differentactionclasses. TheNTUdatasetincludes
57Kvideosof60actions. OurexperimentsonNUCLAandNTUdatasetsutilizethecross-subject
(CS)protocol. Wereportthetop-1actionclassificationaccuracyforallthedatasets.
Training.Forthisexperiment,weemploythedivided-space-timeattentionvariantofTimeSformer[6]
foractionclassification. WeintegrateFibottentionintothespatialattentionmoduleofTimeSformer,
consideringthatthetemporalattentionmodulealreadyprocessesdenseattentionacrossthesame
patchincontiguousframes. FortheimplementationofFibottention,weusebothitsWythoffand
modifiedWythoffvariants.Thehyper-parametersforFibottentionaresettow =1andw =196
min max
8
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccATable5: ShowcasingFibottention’sperformanceinvideoactionclassificationontheSmarthome,
NUCLA,andNTUdatasets. Top-1accuracyisreported.
SmartHome[16] NUCLA[59] NTU[48]
Method
CS CV2 CS CS
TimeSformer[6] 52.2 36.6 32.9 74.8
+BigBird 51.4 40.1 50.9 73.2
+Fibottention(Wythoff) 55.6 38.6 49.3 69.4
+Fibottention(Wythoffmodified) 57.1 42.3 59.6 73.7
exceptforNTUwherew =65. GiventhattheTimeSformerarchitecturefundamentallyresembles
max
aViT-Bwithadditionalattentionalmodules,itisinitializedwithIN-1Kpre-trainedweights. All
videomodelsaretrainedwithabatchsizeof32for15epochs. FortheToyotaSmarthomedataset,
weprocessvideoclipsofsize8×224×224withasamplingrateof1/32,whilefortheNUCLAand
NTUdatasets,weusevideoclipsofsize16×224×224withasamplingrateof1/4.
Results. In Table 5, we compare the action classification results using TimeSformer and other
attentionmechanisms(BigBird,andFibottention)integratedwithinTimeSformer. Weobservethat
FibottentionwithmodifiedWythoffinstantiationoutperformsallbaselinesontheSmarthomeand
NUCLA protocols, utilizing a masking percentage of 94%. Fibottention with modified Wythoff
facilitates increased local interactions among query-key pairs compared to the original Wythoff
sequences, albeit at the expense of a reduced masking ratio (by 1.5%). The modified Wythoff
provesessentialinourvideoexperiments,wherecapturingthetemporalevolutionoflocalpatchesis
criticalforlearningdiscriminativespatiotemporalrepresentations. OntheNTUdataset,TimeSformer
integratedwithFibottentionperformscomparablytothebaselinewhilerequiringonly6%oftoken
interactionsacrosstheattentionheadmatrices. ThisfindingalignswithobservationsfromIN-1K
imageexperiments,owingtotheavailabilityoflarge-scaletrainingvideosinthisdataset.Additionally,
wefindthatsettingw =N/3,whereN isthenumberofspatialtokensperframe,yieldsbetter
max
resultsontheNTUdatasetthanw =N. WehypothesizethatareasonforthisisthatNTU,similar
max
toimagedatasets,isalaboratorydatasetwheretheregionsofinterestareconfinedwithinlocalized
areas. Consequently,restrictingtheupperboundw ofFibottentionminimizestheintroductionof
max
noisyinteractionswithbackgroundtokens. ThisrestrictionisnotapplicablefordatasetslikeNUCLA
andSmarthome,wheresubjectsmayappearanywhereinthevideoperformingactions.
Lift Can PushT Table 6: Performance of Fibottention on be-
havioralcloningforrobotics. Theaveragetask
completionaccuracyisreported.
VisualBackbone Lift Can PushT
ViT-B 0.980 0.960 0.678
+BigBird 1.000 0.880 0.690
+Fibottention(Wythoff) 0.820 0.940 0.630
+Fibottention(WythoffModified) 1.000 0.960 0.720
Figure5: Thedatasetsusedinroboticsexperiments.
5.2 FibottentionforRobotLearning
Datasets. For robotics experiments, we assess the performance of Fibottention for behavioral
cloning[23]inwhichweaimtolearnarobotpolicybytrainingamodelonstate-actionpairsobtained
fromhumanexamples. Weevaluatethreedatasets: CanandLiftfromRobomimic[39],andPushT
fromImplicitBehaviourCloning[23]. InLift,therobotmustliftupacubetoaspecificheight. In
Can,therobotmustmoveacanintoabox. InPushT,therobotmustalignaT-shapedblockwitha
T-shapedoutline. WeprovidevisualsofallthreedatasetsinFigure5.
Training. BuildingupontheCrosswayDiffusion[36]framework,wemodifythearchitectureby
substitutingtheResNetvisualbackbonewithaViT[21],andreplacethestandardViTself-attention
layerswithFibottention. Weemployabatchsizeof64andutilizethebasevariantofViTwithapatch
sizeof8asthevisualbackbone. Forallotherhyper-parameters,includingthenumberoftraining
epochs,wefollow[36].
9Results. WereporttheaveragetaskcompletionaccuracyinTable6andfindthatFibottentionwith
modified Wythoff instantiation leads to improvements over both the baseline ViT and ViT with
BigBirdattention,demonstratingtherobustnessofFibottention. Theseobservationsareconsistent
withthoseinourexperimentsonvideoactionclassification.
6 Conclusion
ThispaperintroducesFibottention,anefficient,robust,andsparsemechanismthatdiversifiesattention
computationacrossheadswhilemaintainingstructuredsparsity. Wefurtherdesignedtwovariantsof
Fibottention —oneiseffectiveforimageclassificationtasks(Wythoff),andtheotheriseffectivein
videodomainsforactionclassificationandrobotimitationlearning(modifiedWythoff). Weused
Fibottentioninconjunctionwithmultiplestate-of-the-arttransformerarchitecturesfabricatedfor
visual representation learning. Finally, we experimented Fibottention across three diverse visual
tasks, outperforming the baselines on small-scale and mid-scale datasets and performing on par
withthebaselineonlarge-scaledatasetswhileutilizingonly2-5%tokeninteractiontocomputethe
multi-headedself-attention. WeenvisionthenextgenerationViTs[55]usingbillionsofinputtokens
shouldusesuchoptimizedarchitecturesforefficienttraining. ThelimitationofFibottentionliesin
itsmarginallyreducedperformanceonlarge-scaledatasetscomparedtoitsbaseline. Futurework
willfocusonstrategiestorecoverthecompromisedaccuracyonlarge-scaledatadistributionand
effortswillbedirectedtowardsoptimizingFibottention’ssparseimplementationasCUDAkernels
forenhancedcomputationalefficiency.
Acknowledgements
ThisworkissupportedinpartbytheNationalScienceFoundation(IIS-2245652). Additionally,this
materialisbaseduponresearchinpartsupportedbytheChateaubriandFellowshipoftheOfficefor
Science&TechnologyoftheEmbassyofFranceintheUnitedStates. WearegratefultoAhmed
HelmyforsupplyingtheessentialGPUs.
References
[1] AbhinavAgarwalla,AbhayGupta,AlexandreMarques,ShubhraPandit,MichaelGoin,Eldar
Kurtic, Kevin Leong, Tuan Nguyen, Mahmoud Salem, Dan Alistarh, et al. Enabling High-
SparsityFoundationalLlamaModelswithEfficientPretrainingandDeployment. arXivpreprint
arXiv:2405.03594,2024.
[2] Meta AI. Introducing Meta Llama 3: The most capable openly available LLM to date.
https://ai.meta.com/blog/meta-llama-3/.
[3] JoshuaAinslie,SantiagoOntanon,ChrisAlberti,VaclavCvicek,ZacharyFisher,PhilipPham,
AnirudhRavula,SumitSanghai,QifanWang,andLiYang.ETC:EncodingLongandStructured
InputsinTransformers.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages268–284,2020.
[4] Mauricio A Alvarez, Lorenzo Rosasco, Neil D Lawrence, et al. Kernels for vector-valued
functions: Areview. FoundationsandTrends®inMachineLearning,4(3):195–266,2012.
[5] IzBeltagy,MatthewE.Peters,andArmanCohan. Longformer:Thelong-documenttransformer.
ArXiv,abs/2004.05150,2020.
[6] GedasBertasius,HengWang,andLorenzoTorresani. Isspace-timeattentionallyouneedfor
videounderstanding? InProceedingsoftheInternationalConferenceonMachineLearning
(ICML),2021.
[7] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. AdvancesinNeuralInformationProcessingSystems(NeurIPS),33:1877–
1901,2020.
[8] NicolasCarion,FranciscoMassa,GabrielSynnaeve,NicolasUsunier,AlexanderKirillov,and
Sergey Zagoruyko. End-to-end object detection with transformers. In Proceedings of the
EuropeanConferenceonComputerVision(ECCV),pages213–229,2020.
10[9] EricChen,AdamGe,AndrewKalashnikov,TanyaKhovanova,EllaKim,EvinLiang,Mira
Lubashev,MatthewQian,RohithRaghavan,BenjaminTaycher,etal. GeneralizingtheWythoff
ArrayandotherFibonacciFactstoTribonacciNumbers. arXivpreprintarXiv:2211.01410,
2022.
[10] TingChen,SimonKornblith,KevinSwersky,MohammadNorouzi,andGeoffreyEHinton. Big
self-supervisedmodelsarestrongsemi-supervisedlearners. AdvancesinNeuralInformation
ProcessingSystems(NeurIPS),33:22243–22255,2020.
[11] Xuanyao Chen, Zhijian Liu, Haotian Tang, Li Yi, Hang Zhao, and Song Han. Sparsevit:
Revisitingactivationsparsityforefficienthigh-resolutionvisiontransformer. InIEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),2023.
[12] RewonChild,ScottGray,AlecRadford,andIlyaSutskever. Generatinglongsequenceswith
sparsetransformers. arXivpreprintarXiv:1904.10509,2019.
[13] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What Does
BERTLookat? AnAnalysisofBERT’sAttention. InProceedingsofthe2019ACLWorkshop
BlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages276–286,2019.
[14] John Conway and Alex Ryba. The Extra Fibonacci Series and the Empire State Building.
MathematicalIntelligencer,38(1),2016.
[15] GonçaloMCorreia,VladNiculae,andAndréFTMartins. Adaptivelysparsetransformers. In
Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessing
andthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),
pages2174–2184,2019.
[16] Srijan Das, Rui Dai, Michal Koperski, Luca Minciullo, Lorenzo Garattoni, Francois Bre-
mond,andGianpieroFrancesca. Toyotasmarthome: Real-worldactivitiesofdailyliving. In
InternationalConferenceofComputerVision(ICCV),2019.
[17] Stéphaned’Ascoli,HugoTouvron,MatthewLeavitt,AriMorcos,GiulioBiroli,andLevent
Sagun. Convit: Improvingvisiontransformerswithsoftconvolutionalinductivebiases. arXiv
preprintarXiv:2103.10697,2021.
[18] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei. Imagenet: Alarge-scale
hierarchical image database. In Conference on Computer Vision and Pattern Recognition
(CVPR),pages248–255,2009.
[19] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingof
DeepBidirectionalTransformersforLanguageUnderstanding. InProceedingsofNAACL-HLT,
pages4171–4186,2019.
[20] JiayuDing,ShumingMa,LiDong,XingxingZhang,ShaohanHuang,WenhuiWang,Nanning
Zheng,andFuruWei. Longnet: Scalingtransformersto1,000,000,000tokens. arXivpreprint
arXiv:2307.02486,2023.
[21] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
JakobUszkoreit,andNeilHoulsby. Animageisworth16x16Words: Transformersforimage
recognitionatscale. InInternationalConferenceonLearningRepresentations,2020.
[22] AritraDutta,SrijanDas,JacobNielsen,RajatsubhraChakraborty,andMubarakShah.Multiview
aerial visual recognition (mavrec): Can multi-view improve aerial visual perception? In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
22678–22690,2024.
[23] PeteFlorence,CoreyLynch,AndyZeng,OscarRamirez,AyzaanWahid,LauraDowns,Adrian
Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning.
ConferenceonRobotLearning(CoRL),2021.
[24] RohitGirdhar,AlaaeldinEl-Nouby,MannatSingh,KalyanVasudevAlwala,ArmandJoulin,
andIshanMisra. Omnimae: Singlemodelmaskedpretrainingonimagesandvideos. arXiv
preprintarXiv:2206.08356,2022.
[25] QipengGuo,XipengQiu,PengfeiLiu,YunfanShao,XiangyangXue,andZhengZhang. Star-
transformer. In Proceedings of the 2019 Conference of the North American Chapter of the
AssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume1(Long
andShortPapers),pages1315–1325,2019.
11[26] AliHassaniandHumphreyShi. Dilatedneighborhoodattentiontransformer. arXivpreprint
arXiv:2209.15001,2022.
[27] AliHassani,StevenWalton,JiachenLi,ShenLi,andHumphreyShi. Neighborhoodattention
transformer. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),pages6185–6194,June2023.
[28] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
preprintarXiv:2106.09685,2021.
[29] QiuhongKe,MohammedBennamoun,SenjianAn,FerdousSohel,andFaridBoussaid.Learning
cliprepresentationsforskeleton-based3dactionrecognition. IEEETransactionsonImage
Processing,27(6):2842–2855,June2018.
[30] YoonKim,CarlDenton,LuongHoang,andAlexanderMRush. Structuredattentionnetworks.
InInternationalConferenceonLearningRepresentations,2016.
[31] ThomasKoshy. FibonacciandLucasNumberswithApplications,volume2. JohnWiley&
Sons,2019.
[32] OlgaKovaleva,AlexeyRomanov,AnnaRogers,andAnnaRumshisky. RevealingtheDark
SecretsofBERT. InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-
guageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing
(EMNLP-IJCNLP),pages4365–4374,2019.
[33] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages.
2009.
[34] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
RaduSoricut. Albert: Alitebertforself-supervisedlearningoflanguagerepresentations. In
InternationalConferenceonLearningRepresentations,2020.
[35] ShiyangLi,XiaoyongJin,YaoXuan,XiyouZhou,WenhuChen,Yu-XiangWang,andXifeng
Yan. Enhancingthelocalityandbreakingthememorybottleneckoftransformerontimeseries
forecasting. AdvancesinNeuralInformationProcessingSystems,32,2019.
[36] XiangLi, VarunBelagali, JinghuanShang, and MichaelS.Ryoo. Crosswaydiffusion: Im-
provingdiffusion-basedvisuomotorpolicyviaself-supervisedlearning. InIEEEInternational
ConferenceonRoboticsandAutomation(ICRA),2024.
[37] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBaining
Guo.Swintransformer:Hierarchicalvisiontransformerusingshiftedwindows.InInternational
ConferenceofComputerVision(ICCV),2021.
[38] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBaining
Guo. Swintransformer: Hierarchicalvisiontransformerusingshiftedwindows. InConference
onComputerVisionandPatternRecognition(CVPR),pages10012–10022,2021.
[39] AjayMandlekar,DanfeiXu,JosiahWong,SoroushNasiriany,ChenWang,RohunKulkarni,
LiFei-Fei,SilvioSavarese,YukeZhu,andRobertoMartín-Martín. Whatmattersinlearning
fromofflinehumandemonstrationsforrobotmanipulation. InarXivpreprintarXiv:2108.03298,
2021.
[40] DavidRMorrison. AStolarskyarrayofWythoffpairs. ACollectionofManuscriptsRelatedto
theFibonacciSequence,38:134–136,1980.
[41] TheFibonacciNumbers. EntryA000045: TheOn-LineEncyclopediaofIntegerSequences
https://oeis.org/A000045.
[42] ZhengQu,LiuLiu,FengbinTu,ZhaodongChen,YufeiDing,andYuanXie. Dota: detectand
omitweakattentionsforscalabletransformeracceleration. InProceedingsofthe27thACM
InternationalConferenceonArchitecturalSupportforProgrammingLanguagesandOperating
Systems,pages14–26,2022.
[43] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal. Improvinglanguage
understandingbygenerativepre-training. OpenAI,2018.
[44] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
12[45] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and
JonathonShlens. Stand-aloneself-attentioninvisionmodels. AdvancesinNeuralInformation
ProcessingSystems,32,2019.
[46] Brendan C Reidy, Mohammadreza Mohammadi, Mohammed E Elbtity, and Ramtin Zand.
EfficientdeploymentoftransformermodelsonedgeTPUaccelerators:Arealsystemevaluation.
InArchitectureandSystemSupportforTransformerModels,2023.
[47] DominickReillyandSrijanDas. Justaddπ! poseinducedvideotransformersforunderstanding
activitiesofdailyliving. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages18340–18350,2024.
[48] AmirShahroudy, JunLiu, Tian-TsongNg, andGangWang. NTURGB+D:ALargeScale
Dataset for 3D Human Activity Analysis. In Conference on Computer Vision and Pattern
Recognition(CVPR),June2016.
[49] Jinghuan Shang, Kumara Kahatapitiya, Xiang Li, and MichaelS Ryoo. Starformer: Trans-
formerwithstate-action-rewardrepresentationsforvisualreinforcementlearning. InEuropean
conferenceoncomputervision,pages462–479.Springer,2022.
[50] HanShi,JiahuiGao,XiaozheRen,HangXu,XiaodanLiang,ZhenguoLi,andJamesTin-Yau
Kwok. Sparsebert: Rethinking the importance analysis in self-attention. In International
ConferenceonMachineLearning,pages9547–9557.PMLR,2021.
[51] ChenyangSi,WeihaoYu,PanZhou,YichenZhou,XinchaoWang,andShuichengYan. In-
ceptiontransformer. AdvancesinNeuralInformationProcessingSystems,35:23495–23509,
2022.
[52] Laurence Sigler. Fibonacci’s Liber Abaci: a translation into modern English of Leonardo
Pisano’sbookofcalculation. SpringerScience&BusinessMedia,2003.
[53] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonShlens,andZbigniewWojna. Re-
thinkingtheinceptionarchitectureforcomputervision. In2016IEEEConferenceonComputer
VisionandPatternRecognition(CVPR),pages2818–2826,2016.
[54] YiTay,MostafaDehghani,DaraBahri,andDonaldMetzler. Efficienttransformers: Asurvey.
ACMComput.Surv.,55(6):1–28,2022.
[55] GeminiTeam. Gemini: Afamilyofhighlycapablemultimodalmodels,2024.
[56] HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,and
HerveJegou. Trainingdata-efficientimagetransformers&distillationthroughattention. In
Proceedingsofthe38thInternationalConferenceonMachineLearning,volume139,pages
10347–10357,2021.
[57] ShikharTuliandNirajKJha. Edgetran: Device-awareco-searchoftransformersforefficient
inferenceonmobileedgeplatforms. IEEETransactionsonMobileComputing,2023.
[58] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.InAdvancesinneuralinformation
processingsystems,pages5998–6008,2017.
[59] JiangWang,XiaohanNie,YinXia,YingWu,andSong-ChunZhu. Cross-viewactionmod-
eling,learning,andrecognition. In2014IEEEConferenceonComputerVisionandPattern
Recognition,pages2649–2656,June2014.
[60] SinongWang,BelindaZLi,MadianKhabsa,HanFang,andHaoMa. Linformer: Self-attention
withlinearcomplexity. arXivpreprintarXiv:2006.04768,2020.
[61] Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, and Bing Xiang. Multi-passage
bert: Agloballynormalizedbertmodelforopen-domainquestionanswering. InProceedings
ofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
5878–5882,2019.
[62] CongWei,BrendanDuke,RuoweiJiang,ParhamAarabi,GrahamWTaylor,andFlorianShkurti.
Sparsifiner: Learningsparseinstance-dependentattentionforefficientvisiontransformers. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
22680–22689,2023.
13[63] HaipingWu,BinXiao,NoelCodella,MengchenLiu,XiyangDai,LuYuan,andLeiZhang.
CVT: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF
InternationalConferenceonComputerVision,pages22–31,2021.
[64] KanWu,JinnianZhang,HouwenPeng,MengchenLiu,BinXiao,JianlongFu,andLuYuan.
Tinyvit: Fastpretrainingdistillationforsmallvisiontransformers. InEuropeanConferenceon
ComputerVision,pages68–85.Springer,2022.
[65] WillemA.Wythoff. Amodificationofthegameofnim. NieuwArch.Wisk,7(2):199–202,1907.
[66] ChulheeYun,SrinadhBhojanapalli,AnkitSinghRawat,SashankReddi,andSanjivKumar.
Aretransformersuniversalapproximatorsofsequence-to-sequencefunctions? InInternational
ConferenceonLearningRepresentations,2019.
[67] ChulheeYun,Yin-WenChang,SrinadhBhojanapalli,AnkitSinghRawat,SashankReddi,and
SanjivKumar. O(n)connectionsareexpressiveenough: Universalapproximabilityofsparse
transformers. AdvancesinNeuralInformationProcessingSystems,33:13783–13794,2020.
[68] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti,
Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird:
Transformers for longer sequences. Advances in Neural Information Processing systems,
33:17283–17297,2020.
[69] PengchuanZhang,XiyangDai,JianweiYang,BinXiao,LuYuan,LeiZhang,andJianfengGao.
Multi-scalevisionlongformer:Anewvisiontransformerforhigh-resolutionimageencoding.In
ProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages2998–3008,
2021.
[70] ZheminZhangandXunGong. Visionbigbird: Randomsparsificationforfullattention. arXiv
preprintarXiv:2311.05988,2023.
[71] XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang,andJifengDai. DeformableDETR:
DeformableTransformersforEnd-to-EndObjectDetection. InProceedingsoftheInternational
ConferenceonLearningRepresentations,2020.
14Appendix
A NoteonGeneralizedFibonacciSequences,theWythoffArray,and
ComputationalComplexity
ToestablishaboundonthecomputationaloverheadofFibottention,first,westateawell-known
generalizationofBinet’sformulatogeneralizedFibonaccisequences,whichprovidesanexplicit,
non-recursivecharacterizationofthen-thsequenceelement.
Lemma 1 (Generalized Binet’s Formula). If Fib(a,b) = (f n) n∈N is the generalized Fibonacci
sequencewithinitialvaluesf =aandf =b,thenitholdsthat
1 2
b−aψ aϕ−b
f = √ ϕn−1+ √ ψn−1
n
5 5
√ √
foreachn≥1,whereϕ=(1+ 5)/2andψ =(1− 5)/2.
Nowwearesettoprovidethehead-wisecomputationaloverheadofthestandardandmodifiedvariant
ofFibottention.
Lemma2. LetN bethenumberoftokensinamulti-headself-attentionblock. If(f ) =Fib(1,1)
n n
isusedasadilationsequencetocreatetheattentionsupportsetΩ=ΩFib(1,1) asin3forwindow
w
sizew,thenthemaskedattentionmatrixAΩcanbecomputedbyevaluatingatmost
√
2N(log( 5w)/log(ϕ)−1)+2
√
dotproductsbetweenqueryandkeyvectors,whereϕ=(1+ 5)/2isthegoldenratio.
Proof. Let(f ,...,f )betheFibonacciindicesforonemask. Wehavef = f +f . Let
1 d k+1 k k−1
f =a andf =b .Letf betheindexofthediagonal.Thusthetotalnumberofinnerproductstobe
1 i 2 i
computedforthediagonalf isgivenby2(N−f)consideringthesymmetricdistributionofdiagonals
intheattentionmatrixA . IfweassumeatotalofD indicesforthediagonals(f ,...,f )there
i 1 D
exist2DdiagonalsintheattentionmatrixA theoverallcomputationaloverheadforself-attentionis
i
dependentonD,leadingtoatotalof(cid:80)D
2(N −f )computations. Wehave
j=1 j
(cid:88)D
2(N −f )=2DN
−2(cid:88)D
f
fD+2−f2==(cid:80)D j=1fj
2DN −2(f −f
)fD+=2=wi
2DN −2w +2b .
j j D+2 2 i i
j=1 j=1
Next,wefindabounddonthelargestsequencesuchthatf ≤w .Wesolvefordsuchthat,f ≤w .
d i d i
UsingBinet’sformula,weobtain:
b−aψ aϕ−b
√ ϕd−1+ √ ψd−1 ≤w .
i
5 5
√
Weknowthat|Ψ|= 1− 5 ≤1. Hence,thesufficientconditionforf ≤w toholdis
2 d i
b−aψ aϕ−b
√ ϕd−1+ √ 1≤w .
i
5 5
Simplifyingtheabovewehave
√
log( 5w +b−aϕ)
d≤ i +1.
logϕ
√ √
Sinceϕ= 1+ 5 anda≥1,wehaved≤ log( 5wi+b) +1.Therefore,thetotalboundisgivenby
2 logϕ
D (cid:32) √ (cid:33)
(cid:88)
2(N −f )=2DN −2w +2b ≤2N
log( 5w i+b)
+1+b−w .
j i i logϕ i
j=1
Hencetheresult.
15B AlgorithmicOutlineforFibottention
Inthissection,weprovidedetailedpseudocodeforFibottention. Algorithm1generatestheFibonacci
sequence with specific constraints. Algorithm 2 produces non-overlapping sequences across all
attentionheads. Finally,Algorithm3demonstrateshowFibottentioncanbeimplementedinMulti-
HeadSelf-Attention(MHSA)tocomputetheattentionmechanism.
Algorithm1GeneratingFibonacciSequencewithConstraint
1: Input: a,b,andw i
2: Output: fib_seq
3: fib_seq ←[a,b]
4: whilefib_seq[−1]<w ido
5: next_num←fib_seq[−1]+fib_seq[−2]
6: fib_seq ←fib_seq∪[next_num]
7: endwhile
8: returnfib_seq
Algorithm2Pseudocodeforgeneratingmaskforallheads
1: Input: L,N,w ,w ,is_modified
min max
2: Output: Ω∈(0,1)h×(N+1)×(N+1)
3: Initialize:√
4: ϕ←(1+ 5)/2
5: a←⌊⌊i×ϕ⌋×ϕ⌋
6: b←⌊⌊i×ϕ⌋×ϕ2⌋
7: foreachheadi∈{1,...,h}do
8: w i ←w min+⌊((i−1)×(w max−w min))/(h−1)⌋
9: Θ←(0)N×N ▷Intermediatetensortoholdthemaskswithouttheclasstoken
10: I ←getFibonacci(a,b,w i) ▷Algorithm1
11: ifis_modifiedandi>1then
12: appendfrom(0,(a−i))toI
13: appendfrom(0,(i−1))toI
14: endif
15: foreacho∈I do
16: foreachj ∈{0,...,N −o}do
17: (Θ) j,j+1 ←1 ▷Uppertriangularmatrix
18: endfor
19: foreachk ∈{o,...,N}do
20: (Θ) k+1,k ←1 ▷Lowertriangularmatrix
21: endfor
22: endfor
23: Ω i ←(1)(N+1)×(N+1)
24: forj ∈{1,...,N}do
25: fork ∈{1,...,N}do
26: (Ω i) j+1,k+1 ←(Θ) j,k
27: endfor
28: endfor
29: endfor
30: Ω←Ω 1×Ω 2×...×Ω h
31: Ω←randomshuffle(L,Ω) ▷ListheLayerofTransformerBlock
32: returnΩ
16Algorithm3PseudocodeforFiboattentioninasingleVisionTransformerBlock
1: Input: X ∈RN+1×d
2: Output: O ∈RN+1×d
3: Parameters: W iQ,W iK,W iV ∈Rd×dh,d h = hd
4: HyperParameters: w ,w ,is_modified
min max
5: ι Ω ←getMask(L,N,h,w min,w max,is_modified) ▷FromAlgorithm2
6: foreachheadi∈{1,...,h}do
7: Q i ←X·W iQ
8: K i ←X·W iK
9: V i ←X·W iV
10: A i ←Q i·K iT
11: AΩ
i
←sign(A i)⊙(|A i|⊙ι Ω[i,:,:])
12: AΩ ←softmax(AΩ)
i i
13: Z i ←(AΩ i ·V i)∈RN+1×dh
14: endfor
15: Z ∈RN+1×(h·dh)
16: O ←Z·WZ,whereWZ ∈R(h·dh)×d
17: returnO
17C FurtherImplementationDetails
Inthissection,weprovideacomprehensiveoutlinefortheimplementationofFibottentionwithina
multi-headself-attentionblockofatransformerarchitecture.
C.1 IntegrationofFibottentioninvariantsofViTs
Fibottention is robust and can be easily integrated into various ViTs. For our experiments, we
implementedFibottentionintwostate-of-the-artViTs: SwinTransformer[37]andCVT[63].
We have adapted the Swin-B architecture by replacing the self-attention within the windows by
Fibottentiononlyinthefirsttwostagesofthemodel. ThelasttwostagesoftheSwin-Bwithlower
complexityowingtothepatchmergingmodules,remainunmodified. Wefollowthestandardtraining
procedureofSwin-B[38].
ConvVIT-Base(CVT-B)[63]consistsofgatedpositionalself-attention(GPSA)andMHSAblocks.
WeapplyFibottentiononlyintheMHSAblocks. WetrainCVT-Bfollowing[63]. Weconductedall
theseexperimentsfor100epochsusingabatchsizeof64on4RTXA6000GPUs.
C.2 ExperimentalConfigurationforImageClassificaion
AlltheexperimentsperformedforImageClassificationbenchmarkingaredetailedin7.
Table7: CIFAR-10,CIFAR-100,andImageNet-1KTrainingSettings[56].
InputSize 224×224
CropRatio 0.9
BatchSize 64
Optimizer AdamW
OptimizerEpsilon 1.0e-06
Momentum 0.9
WeightDecay 0.05
GradientClip 1.0
LearningRateSchedule Cosine
LearningRate 1e-3
WarmupLR 1.0e-6
MinLR 1.0e-5
Epochs 100
DecayEpochs 1.0
WarmupEpochs 5
DecayRate 0.988
ExponentialMovingAverage(EMA) True
EMADecay 0.99992
RandomResize&CropScaleandRatio (0.08,1.0),(0.67,1.5)
RandomFlip Horizontal0.5;Vertical0.0
ColorJittering 0.4
Auto-agumentation rand-m15-n2-mstd1.0-inc1
Mixup True
Cutmix True
Mixup,CutmixProbability 0.5,0.5
MixupMode Batch
LabelSmoothing 0.1
18D FurtherAbaltionStudiesofFibottention
D.1 VisualizationofFibottention
Theattentionmatrixvisualizationspriortotrainingin6arevealalackoffocusalongthediagonals,
indicatingthatthemodelhasnotyetlearnedtodirectitsattentiontowardsspecificimagefeatures.
Incontrast,thevisualizationsforthebaselinemodelViT-BinFig. 6b,after100epochsoftraining,
exhibitmoredefinedpatterns,withthemodelconcentratingitsattentiononcertainimageregions.
Conversely,Fibottention,asshowninFig. 6,appearstoconcentrateitsattentionalongthediagonals,
suggestinganaccumulationofinformationfromhightoken-to-tokeninteractionsatalocallevel.
Head1 Head2 Head3 Head4 Head 1 Hea2d Hea3d Head 4 Head l Head 2 Head 3 Head 4
0.50
0.25
Head 5 Head 6 Head 7 Head 8 Head 5 Hea6d Hea7d Head 8 Head 5 Head 6 Head 7 Head 8 0.00
-0.25
-0.50
Head 9 Hea1d0 Head 11 Head12 Head 9 Head 10 Head1 1 Hea1d2 Head 9 Head 10 Head 11 Head 12 -0.75
-1.00
-1.25
(a) (b) (c)
Figure6:Thisfigureillustratesthevisualizationoftheattentionmatrixforeachhead,aggregatedover
abatchof64images.Thevisualizationsarepresentedforthreedifferentscenarios:(a)beforetraining,
(b)afterthebaselinemodelistrainedfor100epochs,and(c)aftertrainingwithFibottentionfor100
epochs.
D.2 Impactofprincipaldiagonalforattentioncomputation
InTable8,wepresenttheimageclassificationperformanceofFibottentionusingafixedlocalwindow
forcomputingattention,bothwithandwithouttheprincipaldiagonal. Ourfindingsindicatethatlocal
tokeninteractionsalonearesufficientforlearningdiscriminativetokenrepresentations. Theprincipal
diagonalisnotrequiredasitintroducesredundantinformationintothetokenrepresentations.
Table 8: Comparison on performance with a fixed local window w with and without principal
i
diagonalprovingthattheprincipaldiagonalisnotonlyredundant,it,inturn,harmstheperformance,
provingtheclaimsofSparseBERT[50].
w/MainDiagonal w/oMainDiagonal
w
i
MaskRatio C10 C100 MaskRatio C10 C100
2 97.46 86.1 62.0 97.97 85.7 62.2
10 89.57 86.8 62.4 90.08 87.0 63.4
15 84.81 87.5 64.7 85.32 88.0 64.8
20 80.17 87.9 64.9 80.69 88.0 64.9
40 62.94 87.6 64.5 63.45 87.7 65.0
19