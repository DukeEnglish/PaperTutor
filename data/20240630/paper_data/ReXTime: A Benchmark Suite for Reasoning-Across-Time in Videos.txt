REXTIME: A Benchmark Suite for
Reasoning-Across-Time in Videos
Jr-JenChen1 Yu-ChienLiao1 Hsi-CheLin1 Yu-ChuYu1
Yen-ChunChen2 Yu-ChiangFrankWang1
1NationalTaiwanUniversity 2Microsoft
rextime.github.io
Abstract
We introduce REXTIME, a benchmark designed to rigorously test AI models’
abilitytoperformtemporalreasoningwithinvideoevents. Specifically,REXTIME
focusesonreasoningacrosstime,i.e.human-likeunderstandingwhenthequestion
and its corresponding answer occur in different video segments. This form of
reasoning, requiring advanced understanding of cause-and-effect relationships
acrossvideosegments,posessignificantchallengestoeventhefrontiermultimodal
large language models. To facilitate this evaluation, we develop an automated
pipeline for generating temporal reasoning question-answer pairs, significantly
reducingtheneedforlabor-intensivemanualannotations. Ourbenchmarkincludes
921 carefully vetted validation samples and 2,143 test samples, each manually
curatedforaccuracyandrelevance. Evaluationresultsshowthatwhilefrontier
largelanguagemodelsoutperformacademicmodels,theystilllagbehindhuman
performancebyasignificant14.3%accuracygap.Additionally,ourpipelinecreates
atrainingdatasetof9,695machinegeneratedsampleswithoutmanualeffort,which
empiricalstudiessuggestcanenhancetheacross-timereasoningviafine-tuning.
1 Introduction
LargeLanguageModels(LLMs)andMultimodalLargeLanguageModels(MLLMs)havenearly
matched human performance in various language and vision-language tasks [1, 4, 36]. Notably,
frontierMLLMstrainedonweb-scaleproprietarydatasetsshowimpressivevideounderstanding[2].
However,unlikeLLMswhichexcelintextreasoningoverlongsequences,thecause-effectreasoning
inMLLMs,especiallyinunderstandinglongvideoevents,remainsunder-explored. Thiscapabilityis
crucialinroboticsandembodiedagents[5,30,35],healthcareandmedicine[20,50],andlawand
policymaking[20]. Despitetheimportance,currentvideo-languagetaskslikemomentretrieval[13,
21],highlightsdetection[21,34],densevideocaptioning[7,41],andvideoquestionanswering[23,
38]mainlyaddresstext-visualalignment,overlookingdeepertemporalreasoningchallenges.
Inaninitialstudy,weidentifiedacommonshortcominginthemostadvancedMLLMs–theystruggle
withvideoquestionansweringwhenthequestionandanswercorrespondtodifferenttimesegments.
As shown in Fig. 1, the question “How can we cut up the tomato efficiently?” and the answer
“Holdupaplateandsharpentheknifeontheplate.” eachrefertoseparatesegments. Surprisingly,
a simple question like this can challenge leading MLLMs. Therefore, there is a pressing need
forabenchmarktoquantitativelyassessvideotemporalreasoning. Toaddressthis,weintroduce
REXTIME,abenchmarktoevaluateReasoning-Across-Timecapabilitiesforvideoevents.
TodevelopREXTIME,weproposeanLLM-assisteddatagenerationpipelinethatminimizeshuman
effortandcutscostsfrom$300to$135per1,000QApairs. Thebenchmarkincludes921validation
and2143testsamples,eachrigorouslycuratedbyhumanannotators. Empiricalevidenceindicates
thatevenproprietaryfrontierMLLMsareinadequatefortemporalreasoning. Forinstance,humans
Preprint.Underreview.
4202
nuJ
72
]VC.sc[
1v29391.6042:viXraA: To sharpen the knife on the bottom of the plate. Q: Why do we hold up a knife?
Conventional QA
Reasoning Across Time
A: Hold up a plate and sharpen the knife with the bottom the plate. Q: How can we cut up the tomato efficiently?
Human Performance: 88.0%
You can use the unglazed bottom rim of a ceramic plate to sharpen it. 73.7%
GPT-4o
It demonstrates using a plate or flat surface to help guide the knife and cut the tomato into even slices. 68.7%
The video does not provide any information on how to cut tomatoes more efficiently. 68.0%
Slice horizontally through the gap between the chopping board and the bottom of the plate. 63.3%
GPT-4V
It’s important to use a sharp knife for clean cuts and to hold the tomato firmly with a claw grip. 59.7%
Figure1: AREXTIMEexample. Ourbenchmarkspecializesinevaluatingreasoningacrosstime,i.e.
videoQAwhenquestionandanswereachbelongtodifferenttimespans. REXTIMEposesdifficulties
evenforfrontierMLLMs,asindicatedbythelargegaptohuman-levelaccuracy.
canachieve88.0%accuracyonVQAtasks,whereasthetop-performingMLLM,OpenAI’sGPT-4o,
onlyreaches73.7%asshowninFig.1. AnewbenchmarksuchasREXTIMEhasthepotentialto
significantlypropeladvancementsinthisfield–iteffectivelydifferentiatesbetweenmodelcapabilities,
andthestate-of-the-artmodelhasnotyetsaturatedtohuman-levelaccuracy[31].Theadditional9695
unverifiedsamplesprovideatrainingdatasetthathassignificantlyboostedanacademicMLLM’s
temporalreasoningskills,loweringtheentrybarforfutureresearch. Furthermore,weconfirmed
thatREXTIMEprimarilycontainsreasoningacrosstimequestions,withthelowestquestion-answer
overlapintime(QA-mIoU)comparedtoothervideoQAbenchmarks.
Todevelopanefficientandeffectivepipeline,wehaveaddressedtwoprimarychallenges: (1)the
quality-diversitytrade-offinLLMgeneration,and(2)thehighcostofhumanlaborforverification.
Initially,promptingan(M)LLMtogeneratequestion-answerpairsoftenresultsinlogicallyincorrect
responses. While few-shot in-context learning enhances logical correctness, it reduces response
diversity. We address this by moderating the MLLM with specific event attributes and temporal
relations from a structured taxonomy. Additionally, although human verification is necessary to
eliminateresidualerrors,weminimizecostsbyestablishingcriteriathatallowtheMLLMtoself-
assesstheaccuracyofitsgeneratedQAs. Asabonusfeature,weevaluatevideomomentlocalization
toassesswhetheranAImodelaccuratelygroundsitsanswerstothecorrectvideosegments.
Ourcontributionscanbesummarizedasthefollowing:
• REXTIMEisthefirstbenchmarkforcomprehensivevideotemporalreasoningfocusingoncause
andeffectwith2143testsamples,whichfrontierMLLMsstilllagbehindhumanperformance.
• WediscoveracommonweaknesssharedbyfrontierMLLMs–theyreasonpoorlywhenquestion
and answerspan donot overlap. Anewlyproposed measure QA-IoU quantitativelyvalidate
REXTIMEindeedassessAImodels’reasoningacrosstimecapability.
• OurLLM-assisteddatapipelinegenerateshighqualitysampleswithreducedhumanintervention,
saving55%oftheoverallcost. Furthermore,thepuremachinegeneratedtrainingsetisshownto
improvethefinetuningaccuracy,providingastartingpointforfuturestudies.
2 Relatedwork
Temporalreasoningandeventlocalizationinvideos InTable1,wecompareREXTIMEwith
relateddatasetsontemporalreasoningormomentlocalization,highlightingouruniqueness. NEx-
2Table1: Datasetscomparison. REXTIMEcoversfeaturesfromallsimilarvideoQAtasks. Notably,
reasoning-across-timeemphasizesthecauseandeffectunderstandingbetweenvisualevents.
TemporalReasoning
Datasets QA MomentLocalization TrainingData
sequential causal
NExTQA[38] ✓ ✓ ✓
NExTGQA[39] ✓ ✓ ✓
Ego4D-NLQ[14] ✓ ✓ ✓
QVHighlights[21] ✓ ✓
REXTIME ✓ ✓ ✓ ✓ ✓
TQA[38],enhancingvideounderstandingbyexplainingtemporalactions,specializesintemporal
reasoningbutnotmomentlocalization. NExTGQA[39],extendsNExTQAwithover10.5Ktem-
poral grounding labels, revealing models’ inadequacies in grounding answers despite strong QA
performance. Ego4D-NLQ[14]lacksQA,makingitdifficulttoassessmodernAIchatassistants.
QVHighlights[21]featuringover10,000YouTubevideosacrossvariousthemes,aidingsystems
inidentifyingrelevantmomentsandhighlightsinresponsetouserqueries. However, itdoesnot
includetemporalreasoningorQApairs. AnotherrelatedyetorthogonalworkisEgoSchema[27],an
extensionofEgo4D,benchmarkslongvideocomprehensionandintroducesthe“certificatelength”to
measureintrinsictemporalcomplexity.
Querydependmomentretrieval Videomomentretrievalinvolvesretrievingspecificvideoseg-
ments based on user text queries. Proposal-based methods [6, 9, 13, 15, 40, 47] use a two-stage
process: generatecandidateproposalsbyscanningtheentirevideoandthenrankthembasedon
query alignment. In contrast, proposal-free methods [24, 43, 45] directly predict start and end
timestampsoracentertimestampandspanlength. RecentapproachesintegratetheDetectionTrans-
former(DETR)[8],leveragingitshighlightdetectioncapabilities[19,21,28,29]. Whiletheseworks
focusonaligningvisualandtextualcontent,ourresearchemphasizestemporalreasoninginscenarios
withdifferingquestionandanswerspans,requiringadistinctapproach
Groundinglargevideo-languagemodels IntheevolvinglandscapeofMulti-modalLargeLan-
guageModels[4,10,25,36,42,49],significantstrideshavebeenmadeintherealmofvideounder-
standing[22,26,44,46,48],particularlyintheaspectoftemporallocalization[17,18,32,33,37].
VTimeLLM[17]excelswithitsboundary-awaretraining,improvingTemporalVideoGroundingand
DenseVideoCaptioning. Momentor[32],usingtheMoment-10Mdataset,enhancessegment-level
reasoningandlocalization,showcasingfine-grainedtemporalcomprehension. HawkEye[37]focuses
oncomplexvideoswithtime-awareobjectivesandinnovativesegmentrepresentations,achieving
notableperformancegainintemporalvideogrounding. TimeChat[33]usesatimestamp-awareframe
encoderandflexiblevideotokengeneratorforbetterlongvideounderstandingandzero-shottemporal
reasoning. LITA[18]introducestimeandSlowFasttokens[12],significantlyimprovingtemporal
localizationandvideo-textgeneration. Thesemodelscollectivelyadvancetemporalunderstandingof
multimodalAI.Whiletheyclaimadvancedtemporalreasoning,thereisnoquantitativeevaluation. To
bridgethisgap,wedevelopacomprehensivebenchmarkanddatasetspecificallydesignedtoevaluate
andenhancethetemporalreasoningability.
3 Datacollection
We aim to collect video question-answer pairs to assess the reasoning-across-time capability of
multimodalAImodels. Aconversationinvolves“reasoning-across-time”ifthequestion’stimespan
doesnotcompletelyoverlapwiththeanswer’stimespan. Byutilizinglargelanguagemodelsand
largevisionlanguagemodels,wecreatethebenchmark,REXTIME,withmuchlesshumaneffort.
3.1 Selectingvideostoannotate
Weconsidervideosourceswithtime-alignedcaptions(i.e.,captionswithstartandendtimestamps
describingspecificvideosegments)astheyprovidenaturallanguagedescriptionsofvisualevents
3Figure2: Overviewofthedatacollectionpipeline. InstageI,wecollecteventpairsfromtwovideo
sources. InstageII,wescoreandcategorizetheeventpairsintofourrelationtypes. InstageIII,
the(M)LLMgeneratesaquestion-answerpairbyourcarefullywrittenfew-shotdemonstrations. In
stageIV,theLLMself-evaluatesthegeneratedsamplestoreducethehumanverificationcost.
crucialforvideoQA.WeselectActivityNet[7]andQVHighlights[21]datasets,whichmeetthis
criterion,forQAdatacreation. ToensuretheQAsfocusoninterestingeventsandinvolvereasoning
acrosstime, weapplyrule-basedfilteringtoretainonlyvideosthat: (1)containatleasttwonon-
overlappingevents,and(2)haveeventsdenseenoughtocovertheentirevideoduration. Further
detailsonthefilteringprocessareprovidedinthesupplementarymaterial.
3.2 Question-answeringontwoeventsacrosstime
Naivelyfeedingavideoanditstime-alignedcaptionstoanMLLMoftenresultsinlogicallyincorrect
responses. Writingfew-shotdemonstrationsimprovescorrectnessduetoLLMs’strongin-context
learningabilitiesbutunexpectedlyreducesdiversity.Tobalancequalityanddiversity,groundingLLM
generationinspecificvisualeventsandtheirrelationshipsisessential. Weextracteventpairsfrom
captionsandcategorizethemintothreerelationtypes: means-to-an-end,cause-effect,andsequential.
Means-to-an-endreferstooneeventcausinganotherwithsubjectiveintentions,i.e.,“makingadish”
leadingto“choppingtomatoes.” Cause-effectinvolvescausalrelationswithoutapurpose,suchas
“girlfallsdown”causing“girliscrying.” Sequentialeventsarethosewitha“before/after”relation,
whereeventsdonotcompletelyoverlapintime.
Finding candidate event pairs For QVHighlights videos, due to sparsely annotated cap-
tions (events), we use MLLM to find related events given an initial “pivotal event”. We define
acaptionanditsannotatedtimespanasa“pivotalevent”andcropthecorrespondingvideoclipwith
10secondextensionsbeforeandafter. ThisextendedclipisprocessedbyGPT-4Vtodetectboththe
causeleadingtothepivotaleventanditsconsequenteffects.
ForActivityNetvideos,whereevents(captions)aredenser,weuselanguage-onlyGPT-4toextract
event pairs. We prompt the LLM to extract pairs with distinct timestamps and potential causal
relations. Thesepairsarechosenbasedontheirstrongcausalrelationships,ensuringtheeventsare
temporallyseparatedbutintricatelyconnectedintermsofcauseandeffect.
4Event Pairs
Event Pairs
Causality?
Reasoning Across Time Question-Answer
Generated Examples
No Yes
Intentionality?
No Yes
Sequential Cause-Effect Means-to-an-End
Pre-event: Having dinner. Cause: Girl falls down. Means: Chopping tomato.
Post-event: Watching TV. Effect: The girl is crying. End: Making a dish.
Pre-event as question: Cause as question: Means as question:
Q: What do S do after having dinner? Q: What do the girl falling down lead to? Q: Why to chop a tomato?
A: S watches TV. A: The girl cries. A: To make a dish.
Post-event as question: Effect as question: End as question:
Q: What do S do before watching TV? Q: Why is the girl crying? Q: How to make a dish?
A: S has dinner. A: Because she fell down. A: Chop a tomato first.
Figure3: Reasoningacrosstimequestion-answertypespresentstherelationshipandexamples
betweenthethreecategoriesofquestionwegenerated. “Havingdinner/WatchingTV”doesnothave
strongcausalityandisclassifiedinsequential,whichoftenresultsinbefore/afterquestions. “Girls
fallsdown”showsstrongcausalitywith“Thegirliscrying.” butlackshumanintention,isclassified
incause-effect. “Choppingtomato/Makingadish”notonlyhasstrongcausalrelationsbutalso
showssubjectivedeliberation,whichisclassifiedintomeans-to-an-end.
To avoid selecting semantically identical events as candidate pairs, we ask the LLM to output a
similarityscorebetweeneventsandonlyconsiderlesssimilarpairs. FordetailedpromptstoGPT-4V
andGPT-4,pleaseseethesupplementarymaterial.
Eventrelationclassification Weclassifyeventpairsintothethreeaforementionedrelationsusing
thefollowingfourscoringcriteria:
• Directness: Thiscriterionassessesthedirectnessofthecausallinkbetweenevents. Forexample,
“Agirlfallsdown. /Sheiscrying.” scoreshighindirectness,while“Amanhasdinner. /He
watchesTVafterdinner.” scoreslow.
• Necessity: Thiscriterionmeasureswhetherthesecondeventisinevitableduetothefirst,i.e.,if
thesecondeventwouldstilloccurwithoutthefirst. Forexample,“Themarchingbandaligns
inthestreetwithinstruments. /Amanpassesinfrontofthemarchingbandholdingacamera.”
scores high on Directness, but the second event is not necessarily a consequence of the first,
resultinginalowNecessityscore.
• Intentionality: Thiscriterionevaluateswhetheraneventwascarriedoutwithdeliberateintention.
Higher scores are given when there is clear evidence of premeditated action leading to the
outcome. Forexample,“Choptomato. /Makingadish.” scoreshighinIntentionalitybecausethe
humanintentionisclear.
• Purpose: Eveniftheprecedingeventisexecutedwithintention,theresultingeventmaynotalign
withtheoriginalexpectation. WeasktheLLMtospecificallydetectwhethertheintentionhas
beenfulfilled. Forexample,“Addingingredientsintoacup. /Puttingadrinkonthetable.” scores
highinIntentionalitybutlowinPurposebecausetheoriginalgoalwastomakeadrink,notto
placeitonatable.
WeleverageGPT-4toannotatethesefourscores∈[0,1,2,3]foreacheventpair. Therelationcanbe
classifiedusingthefollowingrules: (1)Ifthesumofdirectnessandnecessityscoresisbelow4,they
areinasimplesequentialrelation.1 (2)Ifthesumofintentionalityandpurposeislessthan5,they
areclassifiedasacause-effectrelation. (3)Ifneitheroftheaboveconditionsismet,theeventsarein
ameans-to-an-endrelation. Figure3illustratesthisprocess.
1We further remove the pair if the two events are not consecutive to avoid answer ambiguity, i.e., for
“before/after”questions,weonlyconsidertheimmediatepreceding/followingevent.
5Question-answergeneration TogenerateQApairsfromtheLLM,wecraftedin-contextlearn-
ing[11](ICL)examplesspecifictoeacheventrelation(seetheICLdemonstrationsinthesupplemen-
tarymaterial). Tocreateafairbenchmarkthatcanbeautomaticallyevaluatedwithreliablemetrics,
we made REXTIME a multiple-choice QA task. Thus, we need to generate negative options in
additiontothegroundtruthanswer. Thisiseasilydonewithalanguage-onlyLLM,andthedetailed
promptisprovidedinthesupplementarymaterial.
3.3 Balancingcheapmachinegenerateddataandhigh-qualityhumanannotation
Automatic data verification for cost reduction To ensure a high-quality benchmark, the cor-
rectnessoftheQApairsiscrucial,andalargesamplesizeisneededtoreducevarianceinmodel
evaluation. Therefore,weuseLLMstogenerateextensivedataatalowcost,withhumanjudges
verifyingthecorrectnessoftheoutput,whichisfasterthanmanualQAcreation. Tofurtherreducethe
rejectionrateofLLMresponses,weasktheLLMtoself-verifythelogicalcorrectnessofitsoutputs
forcause-effectandmeans-to-an-endrelationships(forsequentialrelations,thesuccessrateisalready
high). Details of the prompts are provided in the supplementary materials. This step effectively
reducesthehumanverificationworkloadbyfilteringoutpoorsamples. Duetothelowaccessbarrier
ofadvancedLLMs,wegeneratedmoredatathanwecouldmanuallyverify. Unverifieddatasamples
areusedasthetrainingdatasetforREXTIME,servingasajump-startdatasetforfuturemodelsto
tackleourbenchmark.
Mitigatingthemodalitymisalignment Aweaknessofmultiple-choiceQAisthatAImodelscan
learnlanguage-onlyshortcutstoachievehighaccuracy. Toaddressthis,werequiremodelstooutput
thecorrespondingtimespanofthechosenanswer. Astrictermetric,accuracywithIoU@0.5,may
betterreflecttruemultimodalunderstandingability. Oneissueisthattheannotatedcaptiontime
spansfromtheoriginalvideocorpusmaynotbeaccurate. Therefore,werequesthumanannotators
tore-annotatetheeventspans. Theannotatorsareresponsibleforassessingeachquestion-answer
pairtoensurelogicalcoherenceandalignmentwiththevideocontent,andforlabelingthetimespan
oftheanswerevent.
4 Benchmark
4.1 Evaluationmetrics
Toevaluateperformance,weuseaccuracytoassessmultiple-choiceVQA,whereeachquestionhas
fouransweroptions. Additionally,wemeasurethemodel’sabilitytolocalizetheanswereventspan
usingmomentretrievalmetrics,followingLeietal.[21]. WeevaluateRecall@1withIntersection
overUnion(IoU)thresholdsof0.3and0.5atvariousthresholds. Amodelcapableofmultimodal
understandingshouldexcelinbothVQAandlocalization,withaccuracy@IoU≥0.5[39]beinga
keyindicator.
4.2 HowfararefrontierMLLMstosolvingREXTIME?
Table2showstheperformanceofhumansandvariousmulti-modallargelanguagemodels,including
GPT-4V[4],GPT-4o[2],Gemini[36],Claude[1],andReka[3]. ForevaluatingMLLMs,weprompt
the models to predict the time span directly and select the most likely options. Detailed settings
for each model are provided in the supplementary materials. Due to budget constraints and API
querylimits,weusedamini-testsplitof300samples. Human-levelperformanceisincludedtoseta
benchmarkforAImodelsandtoidentifyfuturebenchmarksaturation.
In conclusion, the leading VLLMs can reason across time to some extent, as shown in the VQA
accuracy. ThenewestMLLM,Reka,achieves59.67%,andthebestmodel,GPT-4o,achieves73.67%.
However, these models still lag behind the human-level accuracy of 87.98%. Despite claims of
strongvisioncapabilities,thesemodelsoftenfailtolocalizethecorrectanswerspan,resultingin
significantlylowermIoUcomparedtohumanperformance.
6Table2: Performancesofhumanandfrontiermulti-modallargelanguagemodelsonthemini-
testsplit(300samples). Werandomlysampled100examplesfromeacheventrelationcategory
andevaluatedAPI-basedfrontierMLLMs. ResultsshowthatwhilefrontierMLLMsshowcertain
degreesoftemporalreasoning,theystrugglewithmomentlocalization. Wealsoestimatehuman-level
performance, whereeachquestionisansweredbythreeworkers. Thefindingrevealsthatrecent
MLLMsarestillfarbehindhumansinbothtemporalreasoningVQAandmomentlocalization.
MomentLocalization VQA
Models
R@1 R@1 Accuracy(%) Accuracy(%)
mIoU
(IoU=0.3) (IoU=0.5) @IoU≥0.5
Human 61.11 74.30 62.85 87.98 58.51
GPT-4o[2] 36.28 45.33 34.00 73.67 28.67
Claude3-Opus[1] 23.61 30.67 17.67 68.67 13.67
Gemini-1.5-Pro[36] 28.43 35.67 25.00 68.00 18.33
GPT-4V[4] 26.74 33.33 22.00 63.33 16.67
Reka-Core[3] 27.95 36.33 24.00 59.67 17.00
Table3: Zero-shotperformanceofopensourcemodelsonthetestsplit. Weassessthezero-shot
capabilitiesofstate-of-the-artmomentretrievalmodelsandgroundingvideoLLMs. Wechoosetwo
non-generativevision-languagemodels[24,28]andthreeLLM-basedmethods[17,18,33]with
publiclyavailablecodeandmodelweights. Wecanseeopensourcemodelssignificantlylagbehind
frontierLLMsintemporalreasoningVQA.
MomentLocalization
Models VQA
R@1 R@1
mIoU
(IoU=0.3) (IoU=0.5)
UniVTG[24] 28.07 41.45 26.85 −
CG-DETR[28] 22.39 29.35 16.70 −
VTimeLLM[17] 20.82 31.10 18.30 36.25
TimeChat[33] 11.60 14.25 7.70 38.45
LITA[18] 21.15 28.90 15.90 33.80
4.3 Areacademicandopensourcemodelscompetitive?
Weconsiderbothmomentlocalizationmodels[24,28]andLLM-basedmodels[17,18,33],and
evaluatebothzero-shot(Table3)andfine-tunedperformance(Table4).Akeyobservationisthatmost
currentopen-sourcemodelsstruggletoaccuratelylocalizethegroundtruthmomentinREXTIME.
Comparedtoproprietaryfrontiermodels,thezero-shotVQAaccuracyoftheseopen-sourcemodels
issignificantlylower. ForpureVQAontemporalreasoning,humanscanachieve87.98%accuracy,
thebestproprietaryAPIachieves73.67%,andthebestopen-sourcemodelonlyachieves38.45%
accuracy. Ascontrasted,modelstrainedonourdataset,asshowninTable4,performbetteronthe
momentretrievaltaskcomparedtothebestproprietaryAPI.Thebest-performingmodel,UniVTG,
achievesanmIoUof34.73%,whichiscompetitivewithfrontiermodelsat36.28%. Thisindicates
thatfrontierMLLMsarestillnotwell-equippedformomentretrieval. Lastbutnotleast,wecansee
thataftertrainedonourdataset,VTimeLLMgetsasignificantimprovementfrom36.25%to58.15%
onVQA.ThisresultisevencomparabletoafrontierMLLM–Reka. Similarly,TimeChatimproves
from38.45%to49.35%. Moreover,opensourcegroundinglanguagemodelscangetasignificant
improvementonmomentlocalization. Inconclusion,utilizingourautomaticgenerationpipeline,we
cangeneratetrainingdatabotheffectivelyandefficientlywithlessthan10%ofthemanualannotation
costin(seesupplementaryfordetailedcalculations). Thiscouldserveasagoodstartingpointfor
futuremultimodalmodels’improvementontemporalreasoning.
7Table4: Testsetperformanceofopensourcemodelsafterfinetuning. Theresultsshowthatour
fullyautomaticpipelinemayprovideusefultrainingdatatotechmodelstoreasonacrosstime. We
skipLITA[18]becausetheonlypubliclyaccessiblemodelcontains13Bparameters,whichisbeyond
ourcomputationresourcetofinetune.
MomentLocalization VQA
Models
R@1 R@1 Accuracy(%) Accuracy(%)
mIoU
(IoU=0.3) (IoU=0.5) @IoU≥0.5
UniVTG[24] 34.73 53.55 34.70 − −
CG-DETR[28] 26.60 39.80 22.90 − −
VTimeLLM[17] 30.03 44.05 26.55 58.15 18.30
TimeChat[33] 26.52 40.45 21.90 49.35 11.10
Table 5: Dataset statistics. Our comparison focuses on datasets with both question queries and
momentlocalizationfeatures. Wepresentacomprehensivereportdetailingthenumberoftemporal
reasoningsamplesoneachsplit,certificatelength(C.L.)andQuestion-AnswermeanIntersection
overUnion(QA-mIoU)respectively. Ahigheraveragecertificatelengthindicatesthatamodelneeds
toreasonacrossalongerdurationinavideo. AlowerQA-mIoUindicatessmallerintersectionof
question span and answer span, requiring the model to reason across different time segments in
a video. From the qualitative measures, REXTIME serves as a better benchmark to evaluate the
reasoningacrosstimecapability. (†: OnlycountstemporalreasoningQApairs. Seesupplementary
fordetails.)
#ofReasoningAcrossTimeSamples
Datasets C.L.(s)↑ QA-mIoU(%)↓
Train Val Test
Ego4D-NLQ[14] 2,212† 775† 705† 5.2 85.5
NExTGQA[39] − 1,403† 2,301† 11.7 66.1
REXTIME 9,695 921 2,143 66.0 15.5
4.4 Datasetstatistics
Question-answer intersection of union To quantify “across-time” reasoning, we introduce a
newmeasurecalledQuestion-AnswerIntersectionoverUnion(QA-IoU).QA-IoUiscalculatedby
dividingtheintersectionofthetimespansofthequestionandanswerbytheirunion.AlowerQA-IoU
indicatesagreaterneedforreasoningacrosstime,asitreflectssmallertimeoverlapsbetweenthe
question and answer spans. To excel in a low QA-m(ean)IoU video question-answering task, a
modelmustunderstandthetemporalrelationshipsbetweenevents,presentingsignificantchallenges
tomodernmultimodalAIassistants.
Averagecertificatelengths Mangalametal.[27]definedCertificateLength(C.L.)astheminimal
lengthofthevideosegmentnecessarytoansweragivenquestion. InREXTIME,C.L.correspondsto
theintervalfromtheearlieststarttimestamptothelatestendtimestampofthequestionandanswer
spans. AlongerCertificateLengthrequiresthemodeltoconsideralongersegmenttoanswerthe
question,increasingthedifficultyforAImodels.
Comparison to similar tasks Ego4D-NLQ is a task under the Ego4D Challenge [14] in the
EpisodicMemorycategory.2 Givenavideoclipandanaturallanguagequery,Ego4D-NLQrequires
amodeltolocalizethetemporalwindowwithintheentirevideohistorywheretheanswertothe
question is evident. NExTGQA [39] extends NExT-QA [38] with 10.5k temporal grounding (or
location)labelstiedtotheoriginalQApairs.
WecompareREXTIMEtotheabovetwodatasetsonthenumberofreasoningacrosstimesamples,
certificatelength,andQA-mIoU.AsdepictedinTable5,theaveragecertificatelengthinourdatasetis
2https://ego4d-data.org/docs/challenge/.
8Means_end
Cause Effect
Sequential
Durations (s) Number of Words
Figure4: Datadistribution. Wevisualizethedistributionofourcollectedquestion-answerpairs.
Thepiechartshowstheoverallpercentageofeachrelationcategory. Themiddlehistogramshows
thedistributionofthenumberofwordsinaquestion. Therighthistogramshowsthevideoduration
distribution. The lower number of Cause-Effect samples in ActivityNet can be attributed to the
nature of the dataset, which predominantly features human activities. These activities typically
involvedeliberateactionswithspecificintentions,leadingtoahigherpercentageofMeans-to-an-End
instances.
considerablylongerthaninexistingtasks. Thissuggeststhateffectivelyaddressingourtaskrequires
modelstohavemoreadvancedtemporalreasoningabilities.
ThelowerQA-mIoUinREXTIMEindicatesthatanAImodelneedstofirstlocatethequestionevent
andthenscantherestofthevisualeventsinthevideotoreasonaboutthecorrectanswer. Thisis
morechallengingbecausethereasoningandmomentlocalizationcannotbeeasilydecomposed. For
existingtasks,amodelmostlyneedstolocalizethequestioneventandthenreasonwithinroughlythe
samespanduetothehigherQA-IoU.
NotethatEgoSchema[27],whichalsoposessignificantchallengestomoderndeeplearningsystems,
wouldbemeasuredthelongestcertificatelengthmainlybecauseitsquestionsoftenaskforaverage
statisticsortotalcountsofeventoccurrencesthroughoutthevideo. Sincethisisnotrelatedtoour
focusonlong-distanceeventrelationalreasoning,wedonotincludeitinthetable.
Otherstatistics Figure4providesadditionalanalysisonquestiontypes,thedistributionofques-
tion lengths in words, and video durations. We emphasize that REXTIME is diverse, as simple
“before/after”questionsaccountforlessthan40%ofthedataset, andasignificantportionofthe
questionscontainmorethan15words. Additionally,mostvideosarelongerthan100seconds,posing
achallengingtestforthemultimodalmodel’sabilitytohandlelongsequences.
5 Conclusion
WeproposeREXTIME,acomprehensiveandreliablebenchmarkformultimodalAI,emphasizing
reasoning-across-time and visual event localization in videos, with minimal human labor. We
demonstratethatevenfrontierMLLMsfoundREXTIMEdifficultandfallfarbehindhuman-level
performance. Theautomaticallyconstructedtrainingdatasetfurtherpointsoutapromisingwayfor
futuremodelstoequipthecapability.
Acknowledgement
Weexpressourprofoundgratitudetoallvolunteerswhocontributedtothehumanannotationprocess
forthisproject. Theeffortshavebeenessentialinensuringthedata’saccuracyandreliability,andour
project’ssuccesscouldnothavebeenrealizedwithoutthecommitment. SpecialthankstoSheng-Yu
Huang,I-JiehLiu,Zi-TingChou,BinShihWu,YuChiPai,Yu-ChenCho,WeiWang,YuHsiang
Huang,HsinChenLin,HungKaiChung,KueiChunWang,andKoAnChuforyoursupportand
collaborationpivotalinachievingourproject’sgoals.
References
[1] Theclaude3modelfamily:Opus,sonnet,haiku. Technicalreport,Anthropic,2024. 1,6,7,22
[2] Gpt-4systemcard. Technicalreport,OpenAI,2024. 1,6,7
9
ycneuqerF ycneuqerF[3] Rekacore,flash,andedge: Aseriesofpowerfulmultimodallanguagemodels. Technicalreport,Reka,
2024. 6,7,22
[4] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport. arXiv
preprintarXiv:2303.08774,2023. 1,3,6,7,22
[5] MichaelAhn,AnthonyBrohan,NoahBrown,YevgenChebotar,OmarCortes,ByronDavid,ChelseaFinn,
ChuyuanFu,KeerthanaGopalakrishnan,KarolHausman,etal. Doasican,notasisay: Grounding
languageinroboticaffordances. arXivpreprintarXiv:2204.01691,2022. 1
[6] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell.
Localizingmomentsinvideowithnaturallanguage. InICCV,2017. 3
[7] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A
large-scalevideobenchmarkforhumanactivityunderstanding. InCVPR,2015. 1,4
[8] NicolasCarion,FranciscoMassa,GabrielSynnaeve,NicolasUsunier,AlexanderKirillov,andSergey
Zagoruyko. End-to-endobjectdetectionwithtransformers. InECCV,2020. 3
[9] JingyuanChen,XinpengChen,LinMa,ZequnJie,andTat-SengChua. Temporallygroundingnatural
sentenceinvideo. InEMNLP,2018. 3
[10] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleNFung,andStevenHoi. Instructblip:Towardsgeneral-purposevision-languagemodelswith
instructiontuning. InNeurIPS,2024. 3
[11] QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun,JingjingXu,and
ZhifangSui. Asurveyonin-contextlearning. arXivpreprintarXiv:2301.00234,2022. 6
[12] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video
recognition. InICCV,2019. 3
[13] JiyangGao,ChenSun,ZhenhengYang,andRamNevatia.Tall:Temporalactivitylocalizationvialanguage
query. InICCV,2017. 1,3
[14] KristenGrauman,AndrewWestbury,EugeneByrne,ZacharyChavis,AntoninoFurnari,RohitGirdhar,
JacksonHamburger,HaoJiang,MiaoLiu,XingyuLiu,etal. Ego4d:Aroundtheworldin3,000hoursof
egocentricvideo. InCVPR,2022. 3,8
[15] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell.
Localizingmomentsinvideowithtemporallanguage. InEMNLP,2018. 3
[16] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. LoRA:Low-rankadaptationoflargelanguagemodels. InICLR,2022. 16
[17] BinHuang,XinWang,HongChen,ZihanSong,andWenwuZhu. Vtimellm:Empowerllmtograspvideo
moments. InCVPR,2024. 3,7,8,16
[18] De-AnHuang,ShijiaLiao,SubhashreeRadhakrishnan,HongxuYin,PavloMolchanov,ZhidingYu,and
JanKautz. Lita:Languageinstructedtemporal-localizationassistant. arXivpreprintarXiv:2403.19046,
2024. 3,7,8
[19] JinhyunJang,JunginPark,JinKim,HyeongjunKwon,andKwanghoonSohn. Knowingwheretofocus:
Event-awaretransformerforvideogrounding. InICCV,2023. 3
[20] EmreKıcıman,RobertNess,AmitSharma,andChenhaoTan.Causalreasoningandlargelanguagemodels:
Openinganewfrontierforcausality. arXivpreprintarXiv:2305.00050,2023. 1
[21] Jie Lei, Tamara L Berg, and Mohit Bansal. Detecting moments and highlights in videos via natural
languagequeries. InNeurIPS,2021. 1,3,4,6
[22] KunChangLi,YinanHe,YiWang,YizhuoLi,WenhaiWang,PingLuo,YaliWang,LiminWang,and
YuQiao. Videochat:Chat-centricvideounderstanding. arXivpreprintarXiv:2305.06355,2023. 3
[23] LinjieLi,Yen-ChunChen,YuCheng,ZheGan,LichengYu,andJingjingLiu. Hero:Hierarchicalencoder
forvideo+languageomni-representationpre-training. InEMNLP,2020. 1
10[24] KevinQinghongLin,PengchuanZhang,JoyaChen,ShramanPramanick,DifeiGao,AlexJinpengWang,
RuiYan,andMikeZhengShou. Univtg:Towardsunifiedvideo-languagetemporalgrounding. InICCV,
2023. 3,7,8,16
[25] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,2024.
3
[26] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan. Video-chatgpt:Towards
detailedvideounderstandingvialargevisionandlanguagemodels. arXivpreprintarXiv:2306.05424,
2023. 3
[27] KarttikeyaMangalam,RaiymbekAkshulakov,andJitendraMalik. Egoschema:Adiagnosticbenchmark
forverylong-formvideolanguageunderstanding. InNeurIPS,2024. 3,8,9,17
[28] WonJunMoon, SangeekHyun, SuBeenLee, andJae-PilHeo. Correlation-guidedquery-dependency
calibrationinvideorepresentationlearningfortemporalgrounding. arXivpreprintarXiv:2311.08835,
2023. 3,7,8,16
[29] WonJunMoon,SangeekHyun,SangUkPark,DongchanPark,andJae-PilHeo. Query-dependentvideo
representationformomentretrievalandhighlightdetection. InCVPR,2023. 3
[30] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander
Khazatsky,AnantRai,AnikaitSingh,AnthonyBrohan,etal. Openx-embodiment: Roboticlearning
datasetsandrt-xmodels. arXivpreprintarXiv:2310.08864,2023. 1
[31] PiotrPadlewski,MaxBain,MatthewHenderson,ZhongkaiZhu,NishantRelan,HaiPham,DonovanOng,
KaloyanAleksiev,AitorOrmazabal,SamuelPhua,etal. Vibe-eval:Ahardevaluationsuiteformeasuring
progressofmultimodallanguagemodels. arXivpreprintarXiv:2405.02287,2024. 2
[32] LongQian,JunchengLi,YuWu,YaoboYe,HaoFei,Tat-SengChua,YuetingZhuang,andSiliangTang.
Momentor:Advancingvideolargelanguagemodelwithfine-grainedtemporalreasoning. InICML,2024.
3
[33] ShuhuaiRen,LinliYao,ShichengLi,XuSun,andLuHou. Timechat:Atime-sensitivemultimodallarge
languagemodelforlongvideounderstanding. InCVPR,2024. 3,7,8,16,17
[34] YaleSong,JordiVallmitjana,AmandaStent,andAlejandroJaimes. Tvsum: Summarizingwebvideos
usingtitles. InCVPR,2015. 1
[35] AndrewSzot,MaxSchwarzer,HarshAgrawal,BogdanMazoure,RinMetcalf,WalterTalbott,Natalie
Mackraz,RDevonHjelm,andAlexanderTToshev. Largelanguagemodelsasgeneralizablepoliciesfor
embodiedtasks. InCoRL,2024. 1
[36] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023. 1,3,6,7,22
[37] YueqianWang,XiaojunMeng,JianxinLiang,YuxuanWang,QunLiu,andDongyanZhao. Hawkeye:
Trainingvideo-textllmsforgroundingtextinvideos. arXivpreprintarXiv:2403.10228,2024. 3
[38] JunbinXiao,XindiShang,AngelaYao,andTat-SengChua. Next-qa:Nextphaseofquestion-answeringto
explainingtemporalactions. InCVPR,2021. 1,3,8
[39] JunbinXiao,AngelaYao,YicongLi,andTatSengChua. Canitrustyouranswer? visuallygrounded
videoquestionanswering. InCVPR,2024. 3,6,8
[40] HuijuanXu,KunHe,BryanAPlummer,LeonidSigal,StanSclaroff,andKateSaenko.Multilevellanguage
andvisionintegrationfortext-to-clipretrieval. InAAAI,2019. 3
[41] AntoineYang,ArshaNagrani,IvanLaptev,JosefSivic,andCordeliaSchmid. Vidchapters-7m: Video
chaptersatscale. 2023. 1
[42] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,AnwenHu,
Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with
multimodality. InCVPR,2024. 3
[43] YitianYuan,TaoMei,andWenwuZhu. Tofindwhereyoutalk:Temporalsentencelocalizationinvideo
withattentionbasedlocationregression. InAAAI,2019. 3
11[44] HangZhang,XinLi,andLidongBing. Video-llama:Aninstruction-tunedaudio-visuallanguagemodel
forvideounderstanding. InEMNLP,2023. 3
[45] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. Span-based localizing network for natural
languagevideolocalization. InACL,2020. 3
[46] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,
HongshengLi, andYuQiao. Llama-adapter: Efficientfine-tuningoflanguagemodelswithzero-init
attention. InICLR,2024. 3
[47] SongyangZhang,HouwenPeng,JianlongFu,andJieboLuo. Learning2dtemporaladjacentnetworksfor
momentlocalizationwithnaturallanguage. InAAAI,2020. 3
[48] BinZhu,BinLin,MunanNing,YangYan,JiaxiCui,HongFaWang,YatianPang,WenhaoJiang,Junwu
Zhang,ZongweiLi,etal.Languagebind:Extendingvideo-languagepretrainington-modalitybylanguage-
basedsemanticalignment. InICLR,2024. 3
[49] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing
vision-languageunderstandingwithadvancedlargelanguagemodels. InICLR,2024. 3
[50] MingZhu,AmanAhuja,Da-ChengJuan,WeiWei,andChandanKReddy. Questionansweringwithlong
multiple-spananswers. InEMNLP,2020. 1
12Appendix
A Additionaldocumentationandresources 13
A.1 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
A.2 SocialImpact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
A.3 Datasourcelinks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.4 License . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.5 Authorstatement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.6 Maintenanceplan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.7 Digitalobjectidentifier(DOI) . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.8 Annotationinstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
B Additionalimplementationdetails 14
B.1 SourceDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
B.2 Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
B.3 Costestimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
B.4 Computingresources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
B.5 Trainingdetailsandhyper-parameters . . . . . . . . . . . . . . . . . . . . . . . 15
B.6 CountingtemporalreasoningQAs. . . . . . . . . . . . . . . . . . . . . . . . . 17
B.7 GUI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.8 Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C Additionalexperimentresults 22
C.1 QualitativeResults. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.2 Teaserexamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.3 Opensourceperformanceonminitestset . . . . . . . . . . . . . . . . . . . . . 27
A Additionaldocumentationandresources
A.1 Limitations
Despite these advancements, our dataset does exhibit certain limitations, largely stemming from
inheritedbiasesfromthesourcedatasets:
• Currently,weonlyaddressscenarioswhereboththequestionandtheanswerspanasingletime
duration. Givenaquestion,theannotatedtimespanmustbeasingle,continuousduration,which
mightbelimitingforallscenes.
• Thepresenceofnoisyorinaccurateannotationsinthesourcedatasets,includingcaptionsand
timestamps,posesachallenge.Despiteourefforts,someoftheseerrorscouldnotbeautomatically
filteredout. Theextentofthisissueisdetailedinthequalitativevisualizationconductedbyour
humanreviewers,aspresentedinsupplementary.
• Theaveragedurationofgroundtrutheventsinourdatasetisrelativelylong. Thischaracteristic
hastheunintendedconsequenceofhinderingthemodels’abilitytodetectandanalyzefine-grained
actionswithinshortervideosegments.
Thesedrawbackshighlightareasforpotentialimprovementandindicatethenecessityforongoing
refinementtoensurethecreationofmoreaccurateandunbiasedvideolanguagemodels.
A.2 SocialImpact
Thoughweprovideanassessmentoftemporalreasoningandmomentlocalization,thetypesand
scenediversityarestilllimited. Weinheritthevideoclassesfromthetwosourcevideodatasets,
whichmaynotbesufficientforacomprehensiveassessmentofallkindsoftemporalreasoning. This
limitationcouldintroduceabias.
13Forbothcurateddataandvideodata,theydonotcontainanypersonallyidentifiableinformation.
Besides,someofthevideosamplesinthesourcedatasetsmightbeslightlyuncomfortabledepending
ontheviewer. Forexample,somevideosdiscusstattoosandpiercings,andsomeofthempresent
newsaboutsocialeventsincludingdemonstrationsorwarreports. However,weonlyreleasethedata
ofcuratedquestion-answerandtimespan. Wearenotresponsibleforthereleaseandmaintenanceof
videodata.
A.3 Datasourcelinks
Author’semail: r12942106@ntu.edu.tw
Projectpage: https://rextime.github.io/
Huggingfacedataset: https://huggingface.co/datasets/ReXTime/ReXTime
Github(code,data): https://github.com/ReXTime/ReXTime
Croissant: https://huggingface.co/api/datasets/ReXTime/ReXTime/croissant
A.4 License
OurgenerateddataisreleasedunderCCBY-NC-SA4.0license. OurcodeprovidedinGithubis
releasedunderMITlicense.
A.5 Authorstatement
Astheauthorofthiswork,wetakefullresponsibilityforanyrightsviolations,includingintellectual
propertyrights.Weconfirmthatalldatausedcomplieswithapplicablelicensesandlegalrequirements,
and all external sources have been properly credited and permissions obtained. This statement
acknowledgesouraccountabilityandadherencetorelevantdataandcopyrightregulations.
A.6 Maintenanceplan
Wewillhostandcontinuouslyupdateourdatathroughvariousreleasesources,includingGitHub
(codeanddata),Huggingface(datasets),ourprojectpage,andtheEval.AIchallenge.
A.7 Digitalobjectidentifier(DOI)
YoucanfindthedigitalobjectidentifierinourcitationblockonHuggingfacedatasetpage:
https://huggingface.co/datasets/ReXTime/ReXTime
A.8 Annotationinstruction
Weprovidethelinktotheslidewhichisusedintheannotationprocessasaninstruction. Notethat
theusedlanguageintheslideisChinese. Slide: https://docs.google.com/presentation/d/
1-wgWYaWF-ZIa1YBSyPGc5p5TqTGXorxqkYZhOGIqqBg/edit?usp=sharing
B Additionalimplementationdetails
B.1 SourceDatasets
ActivityNet ActivityNet is a comprehensive large-scale video benchmark designed to advance
the field of human activity recognition by addressing the limitations of current computer vision
algorithms. ActivityNetoffersadiversecollectionofcomplexhumanactivitiesthatreflecteveryday
life. Thedatasetencompasses203distinctactivityclasses,eachwithanaverageof137untrimmed
videos,andfeaturesapproximately1.41activityinstancespervideo. Thisresultsinasubstantialtotal
of849videohours. ActivityNetsupportsvariousevaluationscenarios,includinguntrimmedvideo
classification,trimmedactivityclassification,activitydetectionanddensevideocaptions,makingita
valuableresourceforcomparingandimprovingalgorithmsforhumanactivityunderstanding.
QVHighlights QVHighlights dataset addresses the challenge of detecting video moments and
highlightsbasedonnaturallanguage(NL)queries,anunderexploredareaduetoalackofannotated
14data. Itincludesover10,000YouTubevideosonvarioustopics,eachannotatedwithNLqueries,
relevant moments, and five-point saliency scores. This enables the development and evaluation
ofsystemsfordetectingrelevantmomentsandhighlights. QVHighlightsfocusedonuser-created
lifestylevlogvideosonYouTube. Thesevideos,madebyusersworldwide,showcasevariousevents
andaspectsoftheirlives,includingeverydayactivitiesandtravel. Capturedwithdifferentdevices
(i.e.,smartphones,GoPro)andviewangles(i.e.,first-person,third-person),theypresentsignificant
challengestocomputervisionsystems. Toenhancedatasetdiversity,wealsoincludednewsvideos
withsubstantial“rawfootage”,coveringserioustopicslikenaturaldisastersandprotests. Weused
queriessuchas“dailyvlog”,“travelvlog”,and“newshurricane”toharvestvideosfromYouTube,
selectingtopresultsbetween5-30minuteslong,uploadedafter2016forbettervisualquality,and
filteringoutvideoswithlowviewcountsorhighdislikeratios.Theserawvideoswerethensegmented
into150-secondclipsforannotation.
B.2 Filter
Theinitialstageinvolvesfilteringoutsamplesunsuitableforconversionintoatemporalreasoning
format. Atemporalreasoningconversationsamplerequiresacomplexscenewithsequentialevents
occurringinit. Also,weneedinformationwhichdescribessegmentsindetailinsteadofanoverall
summaryofawholevideo. Lastbutnottheleast,wewantthesamplesourcetocontainasmuch
information as possible. That’s why we need a filter to select a proper sample source. For data
originatingfromQVHighlights,weeliminatesampleswhereinthevideocontentrepresentsasingle,
continuousevent. Specifically,thisreferstovideoswheretheanswerspanencompassestheentire
duration,fromstarttofinish. Also,weexcludesamplesifaqueryhappensseveraltimesinthevideo,
whichindicatesaroutineandrepeatedbehavior. Incontrast,weapplyadistinctsetofcriteriafor
filteringforsamplesfromActivityNet. First,sampleswithaneventdurationexceeding80%ofthe
totalvideolengtharediscarded. Thiscriterionhelpsensureadiverserangeofeventswithineach
video. Second,sampleswherethecumulativedurationofallsegmentsislessthan60%ofthevideo’s
totallengthareregardedasinsufficientlydetailed(“sparsecaptioning”)andarethereforeexcluded.
Thisisduetopotentialinformationdeficitsinsuchsamples. Third,weperformaclusteringofevent
intervals,applyingathresholdof10seconds. Intervalsseparatedbygapsexceedingthisthreshold
areconsidereddiscontinuousandaresegmentedintodistinctgroups. Fromthesegroups,weselect
theonewiththehighesteventcountforthegenerationofquestion-answerpairs,ensuringrichnessin
temporalreasoningcontent.
B.3 Costestimation
TestdatagenerationandVerification Wetake1000samplesasanexample. Onepersoncan
review60samplesperhour. Generating1000sampleswithGPT-4costsabout35$. Ataminimum
hourly rate of 6$, the total cost for 1000 samples, including human verification, is about 135$.
Conversely,creating20naturallanguagequestion-answerpairsforvideocontenttakesaboutone
hour. Thus,generating1000sampleswouldrequire50hours,costing300$intotal. Ourpipelinecan
createvideoQAdatamuchmoreefficiently,atonly45%ofthetotalcost.
Trainingdatageneration Wetake1000samplesasanexample. Generating1000sampleswith
GPT-4costsabout35$. Thetotalcostforgenerating1000trainingsamplesisabout35$. Conversely,
creating20naturallanguagequestion-answerpairsforvideocontenttakesaboutonehour. Thus,
generating 1000 samples would require 50 hours, costing 300$ in total. Our pipeline can create
reasoning-across-timetrainingvideoQAdatamuchmoreefficiently,atabitmorethan10%ofthe
totalcost.
B.4 Computingresources
Allofourfine-tuningexperimentsaredonewithanNvidiaRTX-309024GGPU.
B.5 Trainingdetailsandhyper-parameters
We report the training details and hyper-parameters in this section. Overall, we will follow the
settingprovidedbytheoriginalpapersorofficialGithubsetting. However,tofine-tunegrounding
15video-language models such as [17, 33] on resource as reported in Appendix B.4, we will apply
LoRA[16]fine-tuningandreducebatchsize.
UniVTG[24] Wefollowthesingle-gputrainingscript3providedbyUniVTGofficialimplementa-
tionwithlearningrate1e-4,cliplengths2,batchsize32,epochs200andhiddendimension1024.
Weloadtheweightpre-trainedonseveraldatasetsreleasedbyUniVTGofficialimplementationfor
bothzero-shotmomentretrievalandfine-tuningexperiments.
CG-DETR[28] Weloadtheweightpre-trainedonQVHighlightsreleasedbyCG-DETRofficial
implementationforzero-shotmomentretrieval. Wefollowthesingle-gputrainingscript4provided
byCG-DETRofficialimplementationtotrainonourgenerateddata.
VTimeLLM [17] To evaluate zero-shot performance, we load the stage 3 model weight from
theVTimeLLMofficialimplementation. WeassessmomentretrievalandVQA(VisualQuestion
Answering)performanceseparately. Formomentretrieval, wepromptthemodelwith“Canyou
pinpointwhenand...”followedbythequestionsentence,andextractthetimetokenfromthepredicted
sentence. Forzero-shotVQAevaluations,weconcatenatefouroptionsaftertheprefix“From<ss>to
<ee>,<option>”asfourpredictions,here<ss>and<ee>isgroundtruthspan. Thenwecalculatethe
sequenceprobabilityforeach,andselectthemaximumprobabilityastheVQAprediction.
Forfine-tuningexperiments,wefollowthetuningstrategyprovidedbyVTimeLLM.Startingwith
thestage3modelweight,weaddanewLoRAadapter,tuneonourgeneratedtrainingdataset,and
mergetheadapterduringinference. Weusethehyper-parametersfromtheoriginalpaper: alearning
rateof1e-4,numberofvideoframesof100,LoRArankof64,LoRAalphaof128,trainingfor2
epochs,withabatchsizeof8andgradientaccumulationstepsof16. Forfine-tunedevaluation,we
firstpredictthewholesentencegivenaquestionsentenceandextractthepredictedtimetokens<ss>
and<ee>. Wethenconcatenatethefouroptionsafterthepredictedanswerspan“From<ss>to<ee>,
<option>”asfourpredictions,calculatethesequenceprobability,andchoosethemaximumonefor
VQAandGQA(GroundingVQA)prediction. Hereweprovideapythonpesudoasademonstration:
Pseudocode: Thisisapythonpseudocodefortheassessmentofgroundingmulti-choiceVQA.
def extract_time_token(string):
# string: From ss to ee, the girl is ....
pattern = r"\s+(\d+)\s+to\s+(\d+)"
matches = re.findall(pattern, string)
return matches
def get_predicted_score(logits, labels):
# Get label start index and end index
start_idx, end_idx = ...
scores = nn.CrossEntropyLoss(logits[start_idx:end_idx+1],
\\
labels[start_idx:end_idx+1])
return scores
def concat(question, predicted_time_tokens, option)
# question + ’From ss to ee’ + option.
return question + predicted_time_tokens + option
# Input: question(string), options(string in list, lenght==4)
# Time tokens prediction (Moment localization)
output = model.generate(question)
# Decode to natural language
response = tokenizer.decode(outputs)
# Extract time tokens
3https://github.com/showlab/UniVTG/blob/main/scripts/qvhl_pretrain.sh
4https://github.com/wjun0830/CGDETR/blob/main/cg_detr/scripts/train.sh
16predicted_time_tokens = extract_time_token(response)
# Concatenate predicted_time_tokens with each option.
# From ss to ee, <Option>.
inputs = []
for i in range(4):
inputs.append(concat(question, predicted_time_tokens,
options[i]))
inputs = tokenizer.encode(inputs)
# Multi-choice prediction (VQA)
# input_ids.shape==(4, batch_max_lenghts) for 4 options
output = model(**inputs[’input_ids’])
# Compute the mean of labels sequence log-probability.
scores = get_predicted_score(output[’logits’], inputs[’labels’
])
# Find the one with largest crossentropyloss as predicted
answer
predicted_answer = transition_scores.max()
TimeChat[33] Forthezeroshotsetting,weevaluatethecheckpointsfromtheTimeChatofficial
implementation. WealsoassessmomentretrievalandVQA(VisualQuestionAnswering)separately.
Forthefirsttaskwefollowtheirpromptfortemporalretrieval,andparsemodel’sresponsetoobtain
thetimestampsprediction. Theevaluationprocessforzero-shotVQAforTimeChatisthesameas
thatforVTimeLLM.
Whenfine-tuningTimeChatonourproposeddataset,westartfromfine-tunedcheckpointsprovided
byTimeChatandfollowtheirinstructionfine-tuningsettings. Specifically,weuseLoRAwitharank
of32,alphaof128. Wetrainthemodelwithalearningrateof3e-5,batchsizeof8,andgradient
accumulationstepsof8for3epochs. Thenumberofframesusedineachvideois96. Toevaluatethe
performanceafterfine-tuning,weusethesameevaluationprotocolasthatweusedforVTimeLLM.
B.6 CountingtemporalreasoningQAs
We compare REXTIME with Ego4D-NLQ and NExTGQA. For metrics like average certificate
lengths(C.L.)andquestion-answerintersectionofunion(QA-IoU),wefollowthemethodology
fromMangalametal.[27],manuallyannotatingatleasttwohoursofhumaneffortforeachdataset.
AscreenshotofthelabelingGUItoolisprovidedinAppendixB.7.
To determine the number of reasoning across time samples, we count the total queries with
“before/after”inEgo4D-NLQandthesamplesofthe“temporal”typeinNExTGQA.Weexclude
othercaseswherethequestiontimespancompletelyoverlapswiththeanswertimespan,astheydo
notqualifyas"reasoningacrosstime."
B.7 GUI
Tofacilitateefficientannotation,wehavedevelopedaGradiographicaluserinterface(GUI).5Here
we provide three types of annotation tool, human time span annotation and verification, human
questionspanannotationandhumanperformanceannotation. HereweshowinFig.5. Thefirstone
isforhumantimespanannotation. Theannotatorsareresponsibleforassessingeachquestion-answer
pair to ensure logical coherence and alignment to the video content. Additionally, they need to
providethetimespanoftheanswer,whichwillbeusedasgroundtruthinthefollowing. Thesecond
oneisforhumanquestionspanannotation,givenquestion,answerandanswerspan,theannotators
needtofindaspanwhichisrelevanttothequestionevent. Thisisfortheassessmentofaverage
certificatelengths(C.L.)andquestion-answerintersectionofunion(QA-IoU).Thethirdoneisfor
humanperformanceexperiment,givenquestionandfouroptions,theparticipantsneedtofindnot
only the answer from the four options but also an answer span which is relevant to the selected
5https://www.gradio.app/
171
2
3
1
2
3
4
1
2
3
4
Figure5: WeshowtheGUIsfordifferentannotation/verificationprocesses.
18answer.Thegreenareaindicateswhatanannotatorwillget,andtheorangeareaindicateswhatan
annotatorneedtoanswer.
B.8 Prompts
B.8.1 ActivityNeteventgeneration
Following the steps below to evaluate the causality between two
events in the video.
First, find two events from different timestamps which have
strong causality.
Second, evaluate the causality between the two events according
to the following criteria:
a. Directness: How directly does one event lead to the next?
b. Necessity: Is the subsequent event a necessary consequence
of the previous one?
c. Intentionality: Determine if the first event is deliberately
executed to cause the second event.
d. Purpose: Assess whether the first event is conducted with
the primary goal of leading to the second event.
Scoring Method for criteria a to d (Score 0-3 for each
criterion):
0: Weak causal relationship.
1: Moderate causal relationship.
2: Strong causal relationship.
3. Definite causal relationship.
e. Similarity: Assess whether the two events are just repeated
actions or not.
Scoring Method for criterion e (Score 0-3):
0: Totally different action.
1: Slightly same action with event progression.
2: Partially same action with little event progression.
3: Totally same action without event progression.
Sequential video captioning:
<CAPTIONS>
Provide a brief and concise explanation of the score you give
to each criterion.
<Provide your explanation here>
Finish the result json according to your evaluation:
‘‘‘json{
"event1": "<EVENT1>",
"event1_timestamp": [start, end],
"event2": "<EVENT2>",
"event2_timestamp": [start, end],
"Directness": <DIRECTNESS>,
"Necessity": <NECESSITY>,
"Intentionality": <INTENTIONALITY>,
"Purpose": <PURPOSE>,
"Similarity": <SIMILARITY>
}‘‘‘
B.8.2 QVHighlightseventgeneration
These are frames from a video.
Find out a behavior in the video which is caused by the pivotal
event "<QUERY>" or a behavior which leads to the pivotal
event."
19If there isn’t any behavior that is caused by or leads to the
pivotal event, return "none".
Your response should be in json format as the following.
{
"explain": <A brief explanation according to the video and
instruction>,
"cause": <The behavior leads to the pivotal event>,
"cause-relevant": <Does the cause have strong temporal-
causality with the pivotal event? yes or no.>,
"cause-alignment": <How well the cause is aligned with the
video? high, medium, low>,
"effect": <The behavior caused by the pivotal event>,
"effect-relevant": <Does the effect have strong temporal-
causality with the pivotal event? yes or no.>,
"effect-alignment": <How well the effect is aligned with
the video? high, medium, low>
}
B.8.3 SequentialQAgeneration
Sequential video captioning:
<CAPTIONS>
Find two continuous events in the video captions from different
timestamps.
Construct a temporal related question and answer based on the
two events.
Examples:
(Pre-event) Jack wakes up. (Post-event) Jack brushes his teeth.
Type1. Question (pre-event): What does Jack do after waking up?
Answer (post-event): Jack brushes his teeth.
Type2. Question (post-event): What does Jack do before brushing
his teeth? Answer (pre-event): Jack wakes up.
Provide a brief and concise explanation.
<Your brief explanation here>
Finish the result json according to your explanation:
‘‘‘json{
"pre-event": "<EVENT1>",
"pre-event_timestamp": [start, end],
"post-event": "<EVENT2>",
"post-event_timestamp": [start, end],
"Type1": {
"Question": "<QUESTION>",
"Answer": "<ANSWER>"
},
"Type2": {
"Question": "<QUESTION>",
"Answer": "<ANSWER>"
}
}‘‘‘
B.8.4 Cause-effectQAgeneration
This is a cause-effect relationship. The event "<EVENT1>"
causes the event "<EVENT2>".
Please construct 2 types of questions and answers based on the
cause-effect relationship.
20Examples:
(Cause) A girl falls off a bike. (Effect) She is injured.
Type1. Question (cause): What does the girl falling off the
bike lead to? Answer (effect): She is injured.
Type2. Question (effect): Why is the girl injured? Answer (
cause): She falls off the bike.
Provide a brief and concise explanation.
<Your brief explanation here>
Finish the result json according to your explanation:
‘‘‘json{
"Type1": {
"Question": "<QUESTION>",
"Answer": "<ANSWER>"
},
"Type2": {
"Question": "<QUESTION>",
"Answer": "<ANSWER>"
}
}‘‘‘
B.8.5 Means-to-an-endQAgeneration
This is a means-to-an-end relationship. The event "<EVENT1>" is
a means to achieve the event "<EVENT2>".
Please construct a question and an answer based on the means-to
-an-end relationship.
Examples:
(Means) Mixing flour and water. (End) Make dough.
Type1. Question (end): How do we make dough? Answer (means): By
mixing flour and water.
Type2. Question (means): Why do we mix flour and water? Answer
(end): To make dough.
Provide a brief and concise explanation.
<Your brief explanation here>
Finish the result json according to your explanation:
‘‘‘json{
"Type1": {
"Question": "<QUESTION>",
"Answer": "<ANSWER>"
},
"Type2": {
"Question": "<QUESTION>",
"Answer": "<ANSWER>"
}
}‘‘‘
B.8.6 QAverification
You are tasked with verifying if the question-answer pair is
logically correct.
Provide your explanation of the verification result.
Provide a score from 0 to 3 to indicate the correctness of the
question-answer pair.
Scoring Method:
0: Incorrect question-answer pair.
1: Relevant but not logically correct.
212: Partially correct.
3: Completely correct.
Case 1:
Question: <QUESTION1>
Answer: <ANSWER1>
Case 2:
Question: <QUESTION2>
Answer: <ANSWER2>
Provide a brief and concise explanation.
<Your brief explanation here>
Finish the result json according to your explanation:
‘‘‘json{
"case_1_score": <SCORE>,
"case_2_score": <SCORE>
}‘‘‘
B.8.7 Optionsgeneration
You are tasked with generating high-quality, incorrect options
for a given question-answer pair.
The options should be logically consistent and correct, but
they must be different from the correct answer.
Note that the generated options should not be a interpretations
or variations of the correct answer in any way.
Here are the question-answer pair:
Question: <QUESTION>
Answer: <ANSWER>
Provide a brief and concise explanation.
<Your brief explanation here>
Finish the result json according to your explanation:
‘‘‘json{
"correct_answer": "<ANSWER>",
"wrong_answer_1": "<OPTION1>",
"wrong_answer_2": "<OPTION2>",
"wrong_answer_3": "<OPTION3>"
}‘‘‘
C Additionalexperimentresults
C.1 QualitativeResults
WepresentsixgenerateddataexamplesinFig.6,categorizedintothreetypes:sequential,cause-effect,
andmeans-to-an-end. Foreachcategory,weincludeoneexamplewherethequestioneventhappens
firstandanotherwheretheanswereventhappensfirst.
C.2 Teaserexamples
HereweprovidetheinferencecasedepictedinteaseronGPT-4VAchiametal.[4], Gemini-1.5-
ProTeametal.[36],Claude3-OpusClu[1]andReka-CoreRek[3].
22Figure6: Weshowthequalitativeresultsofgenerateddata.
C.2.1 GPT-4V
NumberofFrames: 50
PromptInput:
According to the video, answer the following questions:
How can we cut the tomato more efficiently?
23Response:
To cut the tomato more efficiently, the video demonstrates a
technique where you can use a plate as a guide to slice the
tomato. Here are the steps based on the images provided:
1. Place the tomato on a chopping board.
2. Cover the tomato with a flat, slightly deep plate,
upside down.
3. Hold the plate firmly in place over the tomato.
4. Using a sharp knife, slice horizontally through the gap
between the chopping board and the bottom of the plate.
This allows you to slice the tomato into even sections
while keeping your fingers safely away from the knife.
5. Carefully lift the plate to reveal the evenly sliced
tomato segments beneath.
This method is particularly useful for slicing multiple
tomatoes quickly and uniformly, and it also helps in
keeping the slices contained, making it cleaner and more
organized.
C.2.2 Gemini-1.5-Pro
NumberofFrames: 50
PromptInput:
According to the video, please describe the video content.
Response:
A man is in a kitchen. He is preparing a meal. He is cutting
tomatoes and there are other ingredients on the counter,
such as meat, green beans, and oil. The man is wearing a
hat and a light blue shirt.
PromptInput:
According to the video, answer the following questions: How can
we cut the tomato more efficiently?
Response:
The video does not provide any information on how to cut
tomatoes more efficiently. The video only shows a person
cutting tomatoes.
C.2.3 Claude3-Opus
NumberofFrames: 20
PromptInput:
According to the video, answer the following questions: How can
we cut the tomato more efficiently?
Response:
24The video demonstrates using a plate or flat surface to help
guide the knife and cut the tomato into even slices more
efficiently. By holding the tomato against the plate while
slicing downward with the knife, it allows for making
consistent, thin slices quickly without the tomato rolling
around on the cutting board. The plate acts as a stable
guide to help control the slicing motion and produce
uniform tomato slices with less effort compared to trying
to slice the tomato freehand.
C.2.4 Reka-Core
NumberofFrames: Unknown
PromptInput:
According to the video, answer the following questions: How can
we cut the tomato more efficiently?
Response:
The video demonstrates a method for slicing a tomato. To cut
the tomato more efficiently, you should start by slicing
off the bottom of the tomato to create a stable base. Then,
slice the tomato horizontally into even rounds, depending
on your preference. If you want to dice the tomato, after
slicing it into rounds, stack the slices and cut across
them to create cubes. It is important to use a sharp knife
for clean cuts and to hold the tomato firmly with a claw
grip to avoid cutting your fingers.
PromptInput:
According to the video, answer the following questions: Why do
we hold up a plate?
Response:
In the video, the act of holding up a plate can have several
purposes. It could be to inspect the plate for cleanliness,
to show the plate to someone else, or to demonstrate a
technique or feature of the plate, such as a logo or a
special design on the bottom. It could also be part of a
cooking demonstration or instructional video where the
person is explaining the next step or ingredient to the
audience.
C.2.5 GPT-4o
NumberofFrames: 50
PromptInput:
According to the video, answer the following questions: How can
we cut the tomato more efficiently?
Response:
Based on the sequence of images from the video, here is how you
can cut a tomato more efficiently:
25**Use a Sharp Knife: Ensure your knife is sharp. The video
shows the person demonstrating the sharpness of the knife
using the edge of a plate.
**Sharpen the Knife**: If your knife is not sharp enough, you
can use the unglazed bottom rim of a ceramic plate to
sharpen it. Run the knife’s edge along the rim at an angle
to hone the blade. This can be a handy trick if you don’t
have a knife sharpener.
**Proper Technique**:
1. Start by halving the tomato.
2. Place the tomato halves on the cutting board with the
flat side down for stability.
3. Slice the tomato into even pieces, keeping your fingers
tucked safely away from the blade.
By ensuring your knife is sharp and using effective slicing
techniques, you can cut the tomato more efficiently and
safely.
26Table6: Zero-shotperformanceonminitestset. Wereportopensourcemodels’zero-shotperfor-
manceontheminitestsetcomparedtothefullset.
MomentLocalization VQA
Models
R@1 R@1 Accuracy(%) Accuracy(%)
mIoU
(IoU=0.3) (IoU=0.5) @IoU≥0.5
UniVTG(zs) 28.07 41.45 26.85 − −
UniVTG(zs,mini) 30.18 42.00 29.33 − −
CG-DETR(zs) 22.39 29.35 16.70 − −
CG-DETR(zs,mini) 22.53 30.00 16.67 − −
VTimeLLM(zs) 20.82 31.10 18.30 36.25 −
VTimeLLM(zs,mini) 19.37 27.67 16.00 37.33 −
TimeChat(zs) 11.60 14.25 7.70 38.45 −
TimeChat(zs,mini) 13.01 16.33 7.00 38.33 −
LITA(zs) 21.15 28.90 15.90 33.80 −
LITA(zs,mini) 24.76 34.33 20.00 35.00 −
Table 7: Fine-tuned performance on mini test set. We report open source models’ fine-tuned
performanceontheminitestsetcomparedtothefullset.
MomentLocalization VQA
Models
R@1 R@1 Accuracy(%) Accuracy(%)
mIoU
(IoU=0.3) (IoU=0.5) @IoU≥0.5
UniVTG(ft) 34.73 53.55 34.70 − −
UniVTG(ft,mini) 34.82 53.00 35.33 − −
CG-DETR(ft) 26.60 39.80 22.90 − −
CG-DETR(ft,mini) 24.98 38.00 20.33 − −
VTimeLLM(ft) 30.03 44.05 26.55 58.15 18.30
VTimeLLM(ft,mini) 29.53 43.67 25.00 54.67 15.67
TimeChat(ft) 26.52 40.45 21.90 49.35 11.10
TimeChat(ft,mini) 27.54 38.00 21.67 52.00 11.33
C.3 Opensourceperformanceonminitestset
Weshowtheperformanceresultsofopensourcemodelsontheminitestset. PleaserefertoTable6
andTable7.
27